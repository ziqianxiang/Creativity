Under review as a conference paper at ICLR 2020
Universal Learning Approach for Adversar-
ial Defense
Anonymous authors
Paper under double-blind review
Ab stract
Adversarial attacks were shown to be very effective in degrading the performance
of neural networks. By slightly modifying the input, an almost identical input
is misclassified by the network. To address this problem, we adopt the universal
learning framework. In particular, we follow the recently suggested Predictive
Normalized Maximum Likelihood (pNML) scheme for universal learning, whose
goal is to optimally compete with a reference learner that knows the true label of
the test sample but is restricted to use a learner from a given hypothesis class. In
our case, the reference learner is using his knowledge on the true test label to per-
form minor refinements to the adversarial input. This reference learner achieves
perfect results on any adversarial input. The proposed strategy is designed to be as
close as possible to the reference learner in the worst-case scenario. Specifically,
the defense essentially refines the test data according to the different hypothe-
ses, where each hypothesis assumes a different label for the sample. Then by
comparing the resulting hypotheses probabilities, we predict the label and detect
whether the sample is adversarial or natural. Combining our method with adver-
sarial training we create a robust scheme which can handle adversarial input along
with detection of the attack. The resulting scheme is demonstrated empirically.
1	Introduction
Deep neural networks (DNNs) have shown to have state-of-the-art performance in many machine
learning tasks (Goodfellow et al., 2016a). Despite the impressive performance, it has been found
that DNNs are susceptible to adversarial attacks (Szegedy et al., 2013; Biggio et al., 2013). These
attacks cause the network to under-perform by adding specially crafted noise to the input, such that
the original and modified inputs are almost indistinguishable.
In the case of an image classification task, adversarial attacks can be characterized according to three
properties (Carlini et al., 2019): the adversary goal, the adversary capabilities, and the adversary
knowledge. (i) The adversary goal may be to simply cause misclassification. This is referred to as
an untargeted attack. Alternatively, the goal can be to have the model misclassify certain examples
from a source class into a target class of its choice. This is referred to as a targeted attack. (ii)
The adversary capabilities relate to the strength of the perturbation the adversary is allowed to cause
the data, i.e., the distance between the original sample and the adversarial sample under a certain
distance metric must be smaller than . (iii) The adversary knowledge represents what knowledge the
adversary has about the model. This can range from a fully white-box scenario, where the adversary
has complete knowledge about the model and its parameters, to a black-box scenario where the
adversary does not know the model and only has a limited number of queries on it.
Out of the many different adversarial attack algorithms, gradient optimization-based attacks are
known to be the most powerful kind of attacks (Carlini et al., 2019). In these attacks, the adversary
uses the gradients of a loss function with respect to the input in order to find the perturbation that
minimizes the performance of the network. Such attacks include, among others, Fast Gradient Sign
Method (FGSM) (Goodfellow et al., 2014), Projected Gradient Descent (PGD) (Madry et al., 2017)
and CW-attack (Carlini & Wagner, 2017).
Adversarial defenses can be separated into two categories according to their goal: to increase robust-
ness against adversarial examples (Madry et al., 2017) and to detect whether an example is natural
1
Under review as a conference paper at ICLR 2020
or adversarial (Li & Li, 2017; Feinman et al., 2017). While recent years have shown growing inter-
est in understanding and defending against adversarial examples, no solution has been found yet to
white-box settings, where the adversary has full access to the network.
The universal prediction framework considers both the stochastic setting in which the true relation
between the features and labels is given by a stochastic function, and the individual setting in which
no probabilistic connection between the data and the labels is assumed. The Predictive Normalize
Maximum Likelihood (pNML) as the universal learning solution is proposed for the batch learning
in the individual settings (Fogel & Feder, 2018a; Bibas et al., 2019a;b). The pNML scheme gives
the optimal solution for a min-max game where the goal is to be as close as possible to a reference
learner, who knows the true label but is restricted to use a learner from a given hypothesis class.
Further elaboration on the pNML learner is presented in section 3.
We propose the Adversarial pNML scheme as a new adversarial defense and detector. Adversarial
pNML improves DNNs robustness against adversarial attacks and allows the detection of adversarial
samples. Based on the pNML, we restrict the genie learner, a learner who knows the true test label,
to perform minor refinements to the adversarial examples. Our method essentially uses an existing
model trained with adversarial training. Then, we compose a hypothesis class. Each member in
the class assumes a different label for the input. The member refines the input sample based on the
assumed label. Finally, by comparing the resulting hypotheses probabilities, we can predict the true
label along with detection that the sample is adversarial.
In this paper, we propose a novel scheme that uses a simple targeted adversarial attack as a method
of refinement. By performing that simple refinement we create a hypothesis class and use it to pre-
dict the test label. We demonstrate our method robustness to state-of-the-art adversarial attacks as
PGD (Madry et al., 2017) for MNIST and CIFAR10 datasets. We show that by combining adver-
sarial sample detection, which is an inherent property of our method, we manage to overcome a
defense-aware adaptive attack. Unlike existing methods that attempt to remove the adversary per-
turbation (Samangouei et al., 2018; Song et al., 2017; Guo et al., 2017), our method is unique in the
sense it does not remove the perturbation but rather exploits the adversarial subspace properties.
2	Related Work
In this section, we mention related works on common attack methods, various type of defence and
the properties of the adversarial subspace.
Attack Methods One of the simplest attacks is FGSM that was introduced by Goodfellow et al.
(2014). Let w0 be the parameters of the trained model, x be the test data and y its corresponding
label, L the loss function of the model, xadv the adversary input and specifies the maximum l∞
distortion ||x - Xadv || ∞ ≤ e. First, the sign of the loss function gradients are computed with respect
to the image pixels, then after multiplying the signs by , it is added to the original image to create
an adversary untargeted attack:
Xadv = x + e ∙ signVχL(wo,x,ytrue).	(1)
It is also possible to improve classification chance for a certain label ytarget (targeted attack):
Xadv = X - € ∙ signVXL(WO, χ, ytarget) .	(2)
A multi-step variant of FGSM that was used by Madry et al. (2017) is called Projected Gradient
Descent (PGD). Denote α as the size of the update, for each iteration, an FGSM step is executed
xtα+v1 = Xladv + α ∙ sign(VxL(W0, x, ytrue),	0 ≤ t ≤ T.	(3)
The number of iterations is predetermined. For each sample, PGD attack creates multiple different
adversarial samples by randomly choosing multiple different starting points Xa0dv = X + u where
U 〜U [-€, e]. Then the sample with the highest loss is chosen. The strength of the attack is defined
by the allowed distance between final adversarial input and the original image ||X - XaTdv || ≤ €.
Defence Methods Several defense methods have been suggested to increase DNNs robustness.
The most prominent defense is adversarial training, which augments the trainset to include adver-
sarial examples (Goodfellow et al., 2014). Other methods include a transformation of the input as
2
Under review as a conference paper at ICLR 2020
suggested in Guo et al. (2017) where the input is reconstructed from its lower resolution version. A
different approach aims only to detect adversarial examples. Such a method is suggested by Fein-
man et al. (2017), where detection is performed by measuring the confidence for a given input using
bayesian uncertainty estimation, available in dropout neural networks.
Adversarial Subspace Szegedy et al. (2013) states that adversarial examples represent low-
probability pockets in the manifold which are hard to find by randomly sampling around the given
sample. Later works show that while adversarial subspace is low-probability they inhabit large and
contiguous regions (Goodfellow et al., 2014; Madry et al., 2017). Tabacof & Valle (2016) showed
that the adversarial subspaces are less stable compared to the true data subspaces. Our scheme takes
advantage of these characteristics in order to improve the robustness of the model.
3 Preliminaries: The Predictive Normalize Maximum Likelihood
We now describe the universal learning problem and its associated solution: the predictive Normal-
ized Maximum Likelihood (pNML). In the case of supervised machine learning, a trainset consisting
ofN pairs of examples, zN = {(xi, yi)}iN=1, is given to a learner, where x ∈ X is the data andy ∈ Y
is the label. The goal of the learner is to predict the label y given a new test data x. More formally,
the learner attempts to minimize a loss function that measures the accuracy of the prediction. In the
information-theoretic framework, a prediction is done by assigning a probability distribution q(∙∣x)
to the unknown label. The performance of the predictor is evaluated using the log-loss function
L(q; x, y) = -logq(y|x).	(4)
For the problem to be well-posed we must make further assumptions on the class of possible models
or ”hypothesis” that is used in order to find the relation between x and y. Denote Θ as a general
index set, this class is a set of conditional probability distributions PΘ = {pθ(y∣x), θ ∈ Θ}.
Another assumption, required to solve the problem, is on how the data and the labels are gen-
erated. The most common setting in learning theory is Probably Approximately Correct (PAC),
established in (Valiant, 1984) where x and y are assumed to be generated by some source P (x, y) =
P (x)P (y|x). P (y|x) is not necessarily a member of PΘ. Another possible setting for the learning
problem, recently suggested by Fogel & Feder (2018a) and Merhav & Feder (1998), is the individual
setting, where the data and labels of both the training and test are specific and individual values. In
this setting, the goal is to compete with a “genie” or a reference learner that knows the desired label
value but is restricted to use a model from the given hypotheses class PΘ , and that does not know
which of the samples is the test. This learner then chooses:
θ(zN ,x,y) = arg max [pθ (y∣x) ∙ ∏n=iPθ (y E)] .	(5)
θ
The log-loss difference between a universal learner q and the reference is the regret:
R(N 、1	pθ(zN ,χ,y)⑶X)	Q
R(q; Z ,χ,y) = log ——∣ N 、	.	(6)
q(y|zN, x)
As advocated in Fogel & Feder (2018b), the chosen universal learner solves:
Γ = Rt(ZN, x) = minmax R(q; ZN,x,y).	(7)
This min-max optimal solution, termed Predictive Normalized Maximum Likelihood (pNML), is
obtained using “equalizer” reasoning, following (Shtar’kov, 1987):
max pθ (y\zN ,χ,y)
qpNML(y∣x; ZN) = L θ∈----------N~τ^------7.	(8)
EyeY max Pθ (y\zN ,x,y)
y∈Y θ∈Θ
Its corresponding regret, independent of the true y, is:
Γ = log V max Pθ (y\zN ,x,y).	(9)
θ∈Θ
y∈Y
The pNML has been derived for several model classes in related works such as 1-D perceptron in
Fogel & Feder (2018b) and linear regression in Bibas et al. (2019a). They show that the regret can
serve as a pointwise measure of learnability for the specific training and data sample. Bibas et al.
(2019b) also reported that the pNML regret can be used to detect out of distribution test samples.
3
Under review as a conference paper at ICLR 2020
4 Adversarial pNML
Recall the pNML solution in equation 8. Intuitively, for each of the possible labels y ∈ Y the
pNML chooses a learner from a hypothesis class Θ that maximizes the probability of that label. The
probability of the respective label is then normalized by the normalization factor, which is the same
for all labels. Denote the unnormalized probability of the true label as the ”genie”
m∈θpθ ⑶ZN ,xytu).
(10)
The ”genie” is the best learner one can attain when knowing the test label. The main issue is how
to select the hypothesis class. A ”good” hypothesis class should be large enough so that the genie
could achieve good performance while avoiding overfitting of the other class members such that the
overall regret (the distance from the genie learner as mentioned in equation 9) stays small.
We propose a novel hypothesis class by adding refinement stage before a pretrained DNN model
w0 . The refinement stage changes the test sample with respect to the model w0 and a certain label y
x refine (x, y) = X - λ ∙ Sign (NxL(W0, X, y))	(11)
where λ is the refinement strength. The hypothesis class that we consider is therefore
Θ = {pw0(y|Xrefine(X, yi)),	∀yi ∈ Y} .	(12)
Each member in the hypothesis class produces a probability assignment by adding a perturbation
that strengthen one of the possible value of the test label.
Our Adversarial pNML scheme consists the following steps: At first, we train a DNN model w0 .
The training could be adversarial-training (Goodfellow et al., 2014) or normal training. Then, we
produce the hypothesis class: Given a test data X, we refine it using the trained DNN w0 and one of
the possible value of the test label yi ∈ Y as in equation 11. We produce a probability assignment
for the selected label yi by feeding the refined test data Xrefine, to the trained DNN w0
pi = pw0 (yi |Xrefine (X, yi )).	(13)
The process is repeated for every possible value of the test label. In the end of the process we get a
set of predictions (probability distributions), we normalize them and return the Adversarial pNML
probability assignment
1	|Y|
qPNML(yi) = Cpi,	C =〉：pi.	(14)
i=1
The corresponding regret is the log normalization factor:
|Y|
Γ = logC = log Xpi.	(15)
i=1
Previous works demonstrated that it is possible to detect adversarial examples by measuring the
confidence of the predictions (Feinman et al., 2017). As mentioned in section 3, the regret is the
distance from the genie learner, who knows the true label of the test sample. Having high regret
means being far from the genie and therefore having low confidence in the prediction. Such a
situation is used to detect adversarial examples and is demonstrated in section 6.
We now discuss the reasons behind the choice of the suggested hypothesis class. Let X be a strong
adversarial example with respect to the label ytarget, i.e., the DNN model has a high probability
mistakenly classifying X as ytarget. We now differentiate between three kind of members in our
suggested hypothesis class: refinement towards the true label ytrue , refinement towards the adversary
target ytarget and refinement towards other label y 6∈ {ytrue, ytarget}.
Refinement towards the true label When perturbing towards the true label ytrue , the refinement
moves the adversarial example outside the loss function’s local maximum, thus increasing the prob-
ability of predicting the true label.
4
Under review as a conference paper at ICLR 2020
Table 1: Accuracy rate for various adversary attacks and adversary trained models.
Model \Attack	MNIST			CIFAR10		
	None	FGSM	PGD	None	FGSM	PGD
Natural	99.3%	0.6%	0.0%	93.6%	6.1%	0%
Natural + Ours	99.2%	1.6%	0.0%	87.2%	5.9%	1.0%
FGSM	97.1%	97.7%	0.3%	83.5%	49.4%	36.2%
FGSM + Ours	97.2%	91.4%	23.3%	79.4%	50.1%	48.8%
PGD	98.3%	95.8%	90.5%	84.3%	59.5%	37.4%
PGD + Ours	98.3%	93.9%	95.2%	79.1%	60.8%	60.1%
Refinement towards the adversary target Refinement towards ytarget is essentially taking another
step towards the local maximum of the loss. In the case of a strong adversary, the loss was already
converged to the local maximum and another step towards ytarget would not dramatically increase
the probability of ytarget. The loss remains roughly the same and so the probability assignment of the
hypothesis. Since the adversarial subspace is relatively small and unstable (see section 2), a weak
refinement towards ytarget could even move the adversarial example outside the local maximum of
loss function which effectively reduces the strength of the attack.
Refinement towards other label This refinement effectively applies a weak targeted attack to-
wards a specific label. Therefore, it increases the probability to misclassify the test data as y . As
mentioned in Section 2, the adversarial examples represent a low-probability subspaces in the man-
ifold which are hard to find. A weak step towards y is unlikely to find a strong adversarial points.
Since the low-probability and instability of the adversarial subspace is crucial for Adversarial
pNML, we combine our method with adversarial training, which is known to decrease the size
of the adversarial subspace (Madry et al., 2017).
5	Experiments
In this section we present experiments that test our proposed Adversarial pNML scheme as a de-
fence for adversarial attack. We test the performance on MNIST (LeCun et al., 2010) and CIFAR10
(Krizhevsky et al., 2014) datasets for PGD attack which represents attack that that efficiently maxi-
mizes the loss of an example (Madry et al., 2017), and FGSM attack that illustrates a weaker attack.
5.1	MNIST DATASET
We follow the model architecture as described in Madry et al. (2017). We use a network that consists
of two convolutional layers with 32 and 64 filters respectively, each followed by 2 × 2 max-pooling,
and a fully connected layer of size 1024. We use three different trainsets that are used in training the
baseline approaches: (i) Natural trainset; (ii) Adversarial trainset that was generated by executing
FGSM attack on the natural trainset with value of 0.3; (iii) Adversarial trainset that was produced
by PGD based attack on the natural trainset with 40 steps of size 0.01 with a maximal value of 0.3.
We train three different models using these three trainsets. The training consists of 100 epochs using
stochastic gradient descent (SGD) with a learning rate of 0.01, momentum value 0.9 and weight
decay of 0.0001.
We consider the following threat: a white-box l∞ that is unaware of the defense, with attack strength
that equals to 0.3. We test the accuracy in the following test cases: natural samples, FGSM attack,
and PGD attack. For the PGD attack, the attacker performs 40 steps of size 0.01 with 20 restarts.
Table 1 shows the efficiency of our scheme for the various attacks. First, in case of natural images,
when no perturbation is added to the original MNIST dataset, our method obtains approximately
the same accuracy as the baselines. In the face of PGD attack, which is the strongest adversary, a
model that was trained using FGSM perturbation obtains a significant improvement of 20% in the
accuracy rate when using our method, and improvement of 5% on model that was trained with PGD
5
Under review as a conference paper at ICLR 2020
(a) MNIST dataset.	(b) CIFAR10 dataset.
Figure 1: Comparison between a model without our scheme (Without) to the same model with
Adversarial pNML scheme (With) for various attack strength . For each dataset the corresponding
model was trained with PGD trainset. (a) MNIST dataset. (b) CIFAR10 dataset.
trainset. On the other hand, in the case of FGSM attack, we see mixed results. When using the
FGSM train method we see a reduction of 6% in the accuracy rate and a decrease of 1% in the case
of PGD based training. As explained in section 4, our scheme works best when the adversary finds
a local maximum of the error function, this scenario is not necessarily the case when using FGSM
as it is regarded as a weak attack. Nevertheless, increasing model robustness is first and foremost
increasing the robustness in the face of the worst-case attack (Madry et al., 2017). We point out to
the fact that for a model trained with PGD trainset, the worst-case attack for our scheme is created
by FGSM attack and not PGD attack. This indicates that PGD can fail to find the optimal attack for
our scheme. In section 6, we present an adaptive adversary that is explicitly designed to attack our
proposed method. We show that we indeed maintain better performance in the worst-case attack.
In order to perform a broader evaluation of the adversarial robustness of our scheme, we investigate
the robustness to l∞ bounded attacks for different attack strengths. We use the PGD based trained
model and keep during the training the value constant with a value of 0.3. We compare between
this model with and without our scheme. For the attack, we use PGD with 50 iterations of size 0.01
with 20 random restarts. We chose the λ value to be equal 0.1 for the refinement strength. The
results for MNIST dataset appear in figure 1a. Our scheme show significant improvement compared
to the same model without Adversarial pNML for all values. There is a rapid decrease for value
of 0.3 for the basic model. The decrease can be explained by attacking with that is greater than the
one it was trained with. Using our scheme, we able to maintain the performance up to value that
equals to 0.39.
5.2	CIFAR 1 0 DATASET
For CIFAR10 we use a modified version of Wide-ResNet (Zagoruyko & Komodakis, 2016) with
each layer being wider by a factor of 10. This DNN contains three residual units with 160, 320,
and 640 filters respectively, with a total of 32 layers. As in MNIST dataset, we use three different
trainsets to generate three different models: (i) Natural trainset; (ii) Adversarial trainset that was
generated by FGSM attack on the natural trainset with value that equals to 8/255; (iii) Adversarial
trainset that was produced by using PGD attack on the natural trainset with 20 steps of size 2/255
with a total of 8/255. We train over 200 epochs using SGD optimizer with a learning rate of 0.001,
reducing it to 0.0001 and 0.00001 after 100 and 150 epochs respectively. We also use momentum
value of 0.9 and weight decay of 0.0002. We evaluate the performance of our method against the
performance of the same model without our scheme for all three trainsets.
We consider the following threat model: a white-box l∞ that is unaware of the defense with an
attack strength of 8/255. We test the accuracy in the following test cases: natural samples, FGSM
attack, and PGD attack. For the PGD attack, we perform 20 steps of size 2/255 with 1 restart.
We illustrate our results in Table 1. Similar to the MNIST results, we see that our scheme improves
the robustness against PGD attack by 23.7% for a model that was trained with PGD trainset. Overall,
the robustness against the worst-case attack is improved for all models. On the other hand, the
6
Under review as a conference paper at ICLR 2020
----Natural tram, PGD attack
—0.2 ----- Natural train. No attack
----PGD train, PGD attack
-0∙4 —— PGD train. No attack
0.00	0.01	0.02	0.03	(
Refinement strength
1 O
O O
1 1
φyeus 60μuAS)叁 suφα
.0
O
O
0.5
(a) Accuracy rate for different attack strength	(b) CIFAR10 regret histogram
Figure 2: (a) CIFAR10 performance for different attack strength . Two models are tested, the
first trained with adversarial PGD trainset and the second with natural trainset. We compare the
performance of those models with and without our proposed scheme. (b) Regret histogram for
correctly classified natural samples and PGD adversarial samples which causes misclassification.
accuracy of natural samples is decreased. This is especially true when the model is trained with
natural trainset when our adversarial pNML is used the accuracy drops by 6.4%.
In figure 1b, we further explore PGD attacks accuracy as a function of the attack strength . We
use the previous model that was trained with PGD adversarial trainset, during training the remains
constant with a value of 8/255. We compare the model robustness against the same model that was
integrated with our scheme. Our scheme shows significant improvement, especially around that
equals 0.05 where we gain almost 30% improvement.
Choosing the refinement strength represents a trade-off between increasing the robustness against
adversarial attacks and decreasing the accuracy of natural samples. This trade-off is explored in
figure 2a. The advantage of our scheme is displayed when it is combined with adversarial training
(training with a PGD attacked trainset). As the refinement strength increases so is the robustness
to PGD attack with almost no loss in accuracy for the natural data. On the other hand, when our
method was applied to a model trained without adversarial training we see a decrease in accuracy for
natural data. As elaborated in section 4 the refinement process is, in fact, a weak targeted adversarial
attack toward different labels. Therefore, when a natural sample is inserted to our scheme it becomes
weakly adversarial. A model that was trained without adversarial training is susceptible even to weak
attacks, hence the performance for natural samples drops. To limit the performance degradation for
a non-adversarial trained model we chose a small λ value of 0.02, while for the adversarially trained
model we chose λ value of 0.11.
We demonstrate the ability of the regret to differentiate between correctly classified natural samples
and adversarial samples that causes misclassification in figure 2b which shows a clear separation
between natural images and adversarial ones.
6	Adaptive Adversary
Part of the defense evaluation is to create and test against adaptive adversaries that are aware of the
defense (Athalye et al., 2018; Carlini et al., 2019). We design an adaptive adversary for our scheme:
We create an end-to-end model that calculates all possible hypotheses in the same computational
graph. Then by using gradient-based optimization on the new end-to-end model, we create our
adaptive adversarial attack.
The first obstacle with using gradient-based optimization for that end-to-end model is that SignG)
operator that is used in the refinement stage, which sets the gradient to zero during the backpropa-
gation. Athalye et al. (2018) suggested using Backward Pass Differentiable Approximation (BPDA)
technique to overcome that problem. In BPDA, we perform forward-pass as usual, but on the back-
ward pass, we replace the non-differentiable part with the identity operator.
7
Under review as a conference paper at ICLR 2020
ΦIEUS OOI 艮S)皂 suφα
(a) MNIST regret histogram

1.000
0.00	0.05	0.10	0.15	0.20	0.25	0.30
FPR
(b) Adversarial PNML With regret based detector
Figure 3: (a) Regret histogram for correctly classified natural samPles and BPDA adversarial sam-
Ples Which cause misclassification. In (b) the accuracy rePresents the ratio betWeen adversarial
samPles correctly classified or detected and the total number of samPle for various β . The black star
Point rePresents the Worst-case attack (PGD) against the same model Without our defense.
In order to overcome the BPDA attack, We use the inhered detection ProPerty of our scheme and We
define the folloWing goal (as in Meng & Chen (2017)): Correct classification is made When the true
label is assigned to the samPle or When an adversarial samPle is detected. This neW goal gives rise to
a neW scheme Which incorPorates adversarial detector in addition to adversarial PNML. We utilize
the regret from equation 15 to form an adversarial detector. Figure 3a shoWs the regret histogram
for MNIST dataset. We note that adversarial samPles generated by BPDA attack have higher regret
values comPared to natural samPles. Using different regret thresholds, one can control the trade-off
betWeen the accuracy of the adversarial testset and the False Positive Ratio (FPR) of the natural
testset, i.e., the number of natural samPles that are detected as adversarial samPles.
Due to the assumPtion that the attacker is aWare of our defense, We ProPose a second neW attack
that designed to target our detector and name it Adaptive Attack. The AdaPtive Attack is designed
to minimize the regret to avoid detection. This is done by adding a regularization term to the loss:
L(x,ytrue) = -log (Pw0(ytrUe|X|Y； n∕x,ytrUe)) ! - β log X p,.	(16)
i=1 pi	i=1
The advantage of our defense is that the attacker have contradictory objectives. On one hand, to
avoid detection the attacker needs to minimize the regret log P|iY=|1 pi , and on the other hand to
increase the classifier loss - logpw0(ytrue|xrefine)/P|iY=|1pi it needs to maximize log P|iY=|1 pi .
We demonstrate the results of the adversarial PNML combined With the regret detector in figure 3b
over 3000 MNIST samPles. We use the same model as described in section 5.1 trained With PGD
adversarial trainset. For the refinement strength We chose λ value of 0.03. For the adaPtive attack, We
examine various β values as shoWn in figure 3b. Our ProPosed defense is successful against adaPtive
attack. For regret threshold of 0.19, We get 91.3% accuracy With FPR of 2% against adaPtive attack.
This rePresents a 4% imProvement comPared to a model Without our scheme.
7 Conclusion and Future Work
In this PaPer We Presented the Adversarial PNML scheme for defending DNNs from adversarial
attacks by increasing robustness and detection. We shoWed emPirically that our defense increases
robustness against unaWare White-box attacks for MNIST and CIFAR10 datasets. To overcome
adaPtive attacks, We successfully use the regret, Which is an inherent ProPerty of our method, as an
adversarial detector in order to enhance our defense.
This Work suggests several Potential directions for future Work. First, We Plan to try our scheme
against other attacks With different lp-norms. Second, it is interesting to exPlore other hyPothesis
classes, one such class is the entire “model class” Where instead of refining the samPle We refine the
model according to different hyPotheses.
8
Under review as a conference paper at ICLR 2020
References
Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of
security: Circumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420,
2018.
Koby Bibas, Yaniv Fogel, and Meir Feder. A new look at an old problem: A universal learning
approach to linear regression. arXiv preprint arXiv:1905.04708, 2019a.
Koby Bibas, Yaniv Fogel, and Meir Feder. Deep pnml: Predictive normalized maximum likelihood
for deep neural networks. arXiv preprint arXiv:1904.12286, 2019b.
Battista Biggio, Igmo Corona, Davide Maiorca, Blame Nelson, Nedim Srndic, Pavel Laskov, Gior-
gio Giacinto, and Fabio Roli. Evasion attacks against machine learning at test time. In Joint
European conference on machine learning and knowledge discovery in databases, pp. 387-402.
Springer, 2013.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017
IEEE Symposium on Security and Privacy (SP), pp. 39-57. IEEE, 2017.
Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas Rauber, Dimitris
Tsipras, Ian Goodfellow, and Aleksander Madry. On evaluating adversarial robustness. arXiv
preprint arXiv:1902.06705, 2019.
Reuben Feinman, Ryan R Curtin, Saurabh Shintre, and Andrew B Gardner. Detecting adversarial
samples from artifacts. arXiv preprint arXiv:1703.00410, 2017.
Yaniv Fogel and Meir Feder. Universal batch learning with log-loss. In 2018 IEEE International
Symposium on Information Theory (ISIT), pp. 21-25. IEEE, 2018a.
Yaniv Fogel and Meir Feder. Universal supervised learning for individual data. arXiv preprint
arXiv:1812.09520, 2018b.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016a.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014.
Chuan Guo, Mayank Rana, Moustapha Cisse, and Laurens Van Der Maaten. Countering adversarial
images using input transformations. arXiv preprint arXiv:1711.00117, 2017.
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. The cifar-10 dataset. online: http://www. cs.
toronto. edu/kriz/cifar. html, 55, 2014.
Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. AT&T Labs [On-
line]. Available: http://yann. lecun. com/exdb/mnist, 2:18, 2010.
Xin Li and Fuxin Li. Adversarial examples detection in deep networks with convolutional filter
statistics. In Proceedings of the IEEE International Conference on Computer Vision, pp. 5764-
5772, 2017.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017.
Dongyu Meng and Hao Chen. Magnet: a two-pronged defense against adversarial examples. In
Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security,
pp. 135-147. ACM, 2017.
Neri Merhav and Meir Feder. Universal prediction. IEEE Transactions on Information Theory, 44
(6):2124-2147, 1998.
Pouya Samangouei, Maya Kabkab, and Rama Chellappa. Defense-gan: Protecting classifiers against
adversarial attacks using generative models. arXiv preprint arXiv:1805.06605, 2018.
9
Under review as a conference paper at ICLR 2020
Yurii Mikhailovich Shtar’kov. Universal sequential coding of single messages. Problemy Peredachi
Informatsii, 23(3):3-17,1987.
Yang Song, Taesup Kim, Sebastian Nowozin, Stefano Ermon, and Nate Kushman. Pixeldefend:
Leveraging generative models to understand and defend against adversarial examples. arXiv
preprint arXiv:1710.10766, 2017.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Pedro Tabacof and Eduardo Valle. Exploring the space of adversarial images. In 2016 International
Joint Conference on Neural Networks (IJCNN), pp. 426-433. IEEE, 2016.
Leslie G Valiant. A theory of the learnable. In Proceedings of the sixteenth annual ACM symposium
on Theory of computing, pp. 436-445. ACM, 1984.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint
arXiv:1605.07146, 2016.
10