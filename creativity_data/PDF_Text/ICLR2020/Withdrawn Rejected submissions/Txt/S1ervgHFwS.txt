Under review as a conference paper at ICLR 2020
Adversarial	Training Generalizes	Data-
dependent Spectral Norm Regularization
Anonymous authors
Paper under double-blind review
Ab stract
We establish a theoretical link between adversarial training and operator norm
regularization for deep neural networks. Specifically, we present a data-dependent
variant of spectral norm regularization and prove that it is equivalent to adversarial
training based on a specific '2-norm constrained projected gradient ascent attack.
This fundamental connection confirms the long-standing argument that a network’s
sensitivity to adversarial examples is tied to its spectral properties and hints at novel
ways to robustify and defend against adversarial attacks. We provide extensive
empirical evidence to support our theoretical results.
1	Introduction
Deep neural networks have been used with great success for perceptual tasks such as image clas-
sification (Simonyan & Zisserman, 2014; LeCun et al., 2015) or speech recognition (Hinton et al.,
2012). While they are known to be robust to random noise, it has been shown that the accuracy
of deep nets dramatically deteriorates in the face of so-called adversarial examples (Biggio et al.,
2013; Szegedy et al., 2013; Goodfellow et al., 2014), i.e. small perturbations of the input signal,
often imperceptible to humans, that are sufficient to induce large changes in the model output. This
apparent vulnerability is worrisome as deep nets start to proliferate in the real-world, including in
safety-critical deployments. Consequently, there has been a surge in methods that find adversarial
perturbations (Sabour et al., 2015; Papernot et al., 2016; Kurakin et al., 2016; Moosavi Dezfooli et al.,
2016; Moosavi-Dezfooli et al., 2017; Madry et al., 2017; Athalye et al., 2018).
The most direct strategy of robustification, called adversarial training, aims to harden a machine
learning model by immunizing it against an adversary that maliciously corrupts training examples
before passing them to the model (Goodfellow et al., 2014; Kurakin et al., 2016; Miyato et al., 2015;
2017; Madry et al., 2017). A different strategy of defense is to detect whether the input has been
perturbed by detecting characteristic regularities either in the adversarial perturbations themselves
or in the network activations they induce (Grosse et al., 2017; Feinman et al., 2017; Xu et al., 2017;
Metzen et al., 2017; Carlini & Wagner, 2017; Roth et al., 2019).
Despite practical advances in finding adversarial examples and defending against them, it is still
an open question whether (i) adversarial examples are unavoidable, i.e. no robust model exists, cf.
(Fawzi et al., 2018; Gilmer et al., 2018), (ii) learning a robust model requires too much training data,
cf. (Schmidt et al., 2018), (iii) learning a robust model from limited training data is possible but
computationally intractable (Bubeck et al., 2018), or (iv) we just have not found the right training
algorithm yet, i.e. adversarial examples exist because of intrinsic flaws of the model or learning
objective that can ultimately be overcome.
In this work, we investigate the origin of adversarial vulnerability in neural networks by focusing
on the attack algorithms used to find adversarial examples. In particular, we make the following
contributions:
•	We present a data-dependent variant of spectral norm regularization that directly regularizes
large singular values of a neural network in regions that are supported by the data, as opposed
to existing methods that regularize a global, data-independent upper bound.
1
Under review as a conference paper at ICLR 2020
•	We establish a theoretical link between adversarial training and operator norm regulariza-
tion for deep neural networks. Specifically, we prove that data-dependent spectral norm
regularization is equivalent to adversarial training based on a specific '2-norm constrained
projected gradient ascent attack.
•	We conduct extensive empirical evaluations showing that (i) adversarial perturbations align
with dominant singular vectors, (ii) adversarial training and data-dependent spectral norm
regularization dampen the singular values, and (iii) both training methods give rise to models
that are significantly more linear around data points than normally trained ones.
2	Related Work
The idea that a conservative measure of the sensitivity of a network against adversarial examples
can be obtained by computing the spectral norm of the individual weight layers appeared already in
the seminal work of Szegedy et al. (2013). A number of works have since suggested to regularize
the spectral norm (Yoshida & Miyato, 2017; Miyato et al., 2018; Bartlett et al., 2017; Farnia et al.,
2018) and Lipschitz constant (Cisse et al., 2017; Hein & Andriushchenko, 2017; Tsuzuku et al., 2018;
Raghunathan et al., 2018) as a means to improve model robustness against adversarial attacks. In
the same vein, training methods based on input gradient regularization have been proposed (Gu &
Rigazio, 2014; Lyu et al., 2015; Cisse et al., 2017).
The most direct and popular strategy of robustification, however, is to use adversarial examples as
data augmentation during training (Goodfellow et al., 2014; Shaham et al., 2015; Kurakin et al.,
2016; Miyato et al., 2017; Madry et al., 2017). Adversarial training can be viewed as a variant
of (distributionally) robust optimization (El Ghaoui & Lebret, 1997; Xu et al., 2009; Bertsimas &
Copenhaver, 2018; Namkoong & Duchi, 2017; Sinha et al., 2017; Gao & Kleywegt, 2016) where a
machine learning model is trained to minimize the worst-case loss against an adversary that can shift
the entire training data within an uncertainty set. Interestingly, for certain problems and uncertainty
sets, such as for linear regression and induced matrix norm balls, robust optimization has been
shown to be equivalent to regularization (El Ghaoui & Lebret, 1997; Xu et al., 2009; Bertsimas
& Copenhaver, 2018; Bietti et al., 2018). Similar results on the equivalence of robustness and
regularization have been obtained also for (kernelized) SVMs (Xu et al., 2009).
More recently, related works have started to develop a learning theory for robust optimization, in-
cluding Lipschitz-sensitive generalization bounds (Neyshabur et al., 2015) and spectrally-normalized
margin bounds for neural networks (Bartlett et al., 2017), particularly as bounds on the spectral norm
or Lipschitz constant can easily be translated to bounds on the minimal perturbation required to fool
a machine learning model.
We extend these lines of work by establishing a theoretical link between adversarial training and
data-dependent spectral norm regularization. This fundamental connection confirms the long-standing
argument that a network’s sensitivity to adversarial examples is tied to its spectral properties and
opens the door for adversarially robust generalization bounds via spectral norm based ones.
3	Background
3.1	Global Spectral Norm Regularization
In this section we rederive spectral norm regularization a` la Yoshida & Miyato (2017), while
also setting up the notation for later. Let x and y denote input-label pairs generated from a data
distribution P . Let f : X ⊂ Rn → Rd denote the logits of a θ-parameterized piecewise linear
classifier, i.e. f (∙) = WLΦl-1(Wl-1Φl-2(... )+ bL-1) + bL, where φ' is the activation function,
and W', b' denote the layer-wise weight matrix1 and bias vector, collectively denoted by θ. Let us
furthermore assume that each activation function is a ReLU (the argument can easily be generalized
to other piecewise linear activations). In this case, the activations φ' act as input-dependent diagonal
matrices ΦX := diag(φX), where an element in the diagonal φX := 1(x' ≥ 0) is one if the
corresponding pre-activation x' := W'φ`-1(∙) + b' is positive and equal to zero otherwise.
1Note that convolutional layers can be constructed as matrix multiplications by converting the convolution
operator into a Toeplitz matrix.
2
Under review as a conference paper at ICLR 2020
Following Raghu et al. (2017), we call φx := (φ1x, . . . , φxL-1) ∈ {0, 1}m the “activation pattern”,
where m is the number of neurons in the network. For any activation pattern φ ∈ {0, 1}m we can
define the preimage X(φ) := {x ∈ Rn : φx = φ}, inducing a partitioning of the input space via
Rn = Sφ X(φ). Note that some X(φ) = 0, as not all combinations of activiations may be feasible.
See Figure 1 in (Raghu et al., 2017) or Figure 3 in (Novak et al., 2018) for an illustration of ReLU
tesselations of the input space.
We can linearize f within a neighborhood around x as follows
f(x + ∆x) ' f(x) + Jf(x)∆x ,	(with equality ifx + ∆x ∈ X(φx)) ,	(1)
where Jf(x) denotes the Jacobian of f at x
Jf(X) = WL ∙ ΦL-1 ∙ WL-1 ∙ ΦL-2 …ΦX ∙ W1.	(2)
We have the following bound for ∣∣∆x∣∣2 = 0
If(X + δX) -f(X)||2 JIJf(X)δxM2 /	、「 F	||Jf(X)δxM2
∣⅛	'	∣∣∆X∣∣2	≤ S(X)) := /P=.	∣∣∆X∣∣2
(3)
where σ(Jf (X)) is the spectral norm (largest singular value) of the linear operator Jf(X). From a
robustness perspective we want σ(Jf(X)) to be small in regions that are supported by the data.
Based on the decomposition in Equation 2 and the non-expansiveness of the activations, σ(Φf) ≤ 1
for every ` ∈ {1, ..., L- 1}, Yoshida & Miyato (2017) suggest to upper-bound the spectral norm of
the Jacobian by the product of the spectral norms of the individual weight matrices
L
σ(Jf(X)) ≤Y σ(W')	, ∀X ∈X.
'=1
(4)
The layer-wise spectral norms σ` := σ(W') can be computed iteratively using the power method3.
Starting with a random vector v0 , the power method iteratively computes
Uk — Uk川Uk||2,	Uk	— w'vk-ι,	Vk	—	Vk川Vk||2,	Vk	—	(W')>uk.	⑸
The (final) singular value can be obtained via σ( = (u£)> W'Vk.
Yoshida & Miyato (2017) suggest to turn this upper-bound into a global (data-independent) regularizer
by learning the parameters θ via the following penalized empirical risk minimization
λL
min θ → E(X,y)〜P ['(y,f (X))] + 2 £ σ(We)2 ,	⑹
乙'=1
where '(∙, ∙) denotes an arbitrary classification loss. Note, since the parameter gradient of σ(W')2/2
is σ'u'(Ve)>, with σ', u' and v' being the dominant singular value and singular vectors of W'
(approximated via the power method), Yoshida & Miyato (2017) global spectral norm regularizer
effectively adds a term λσ'U'(V')> for each layer ` ∈ {1, ..., L} to the parameter gradient of the loss
function. In terms of computational complexity, because the global regularizer decouples from the
empirical loss, the power-method iterations can be amortized across data-points and a single power
method iteration per parameter update step usually suffices in practice (Yoshida & Miyato, 2017).
3.2	Global vs. Local Regularization
The advantage of global bounds is that they trivially generalize from the training to the test set.
The problem however is that they can be arbitrarily loose, e.g. penalizing the spectral norm over
irrelevant regions of the ambient space. To illustrate this, consider the ideal robust classifier that is
essentially piecewise constant on class-conditional regions, with sharp transitions between the classes.
The global spectral norm will be heavily influenced by the sharp transition zones, whereas a local
data-dependent bound can adapt to regions where the classifier is approximately constant (Hein &
Andriushchenko, 2017). We would therefore expect a global regularizer to have the largest effect in
the empty parts of the input space. A local regularizer, on the contrary, has its main effect around the
data manifold.
3
Under review as a conference paper at ICLR 2020
4	Adversarial Training Generalizes Spectral Norm
Regularization
4.1	Data-dependent Spectral Norm Regularization
We now show how to directly regularize the data-dependent spectral norm of the Jacobian Jf(x).
Under the assumption that the dominant singular value is non-degenerate2 , the problem of computing
the largest singular value and the corresponding left and right singular vectors can efficiently be
solved via the power method. Let v0 be a random vector or an approximation to the dominant right
singular vector of Jf(x). The power method iteratively computes
Uk —	Uk∕∣∣Uk∣∣2 ,	Uk — Jf(X)Vk-1 = WL ∙ ΦL-1	…ΦX	∙ W1 Vk-1 (forwardpass)	⑺
Vk —	Vk/||Vk∣∣2 ,	Vk - J>(χ)Uk = Vχ(f(x)>Uk)	(backwardpass)
The (final) singular value can be computed via σk = Uk>Jf(x)vk. Note that the right singular vector
Vk gives the direction in input space that corresponds to the steepest ascent of f(x) along Uk.
We can turn this into a regularizer by learning the parameters θ via the following Jacobian-based
spectral norm penalized empirical risk minimization
min θ → E(χ,y)〜p^
'	ʌ
'(y,f (X)) + 2(UTJf(X)V)2
(8)
where U and V are the data-dependent singular vectors of Jf(X), computed via Equation 7.
By optimality / stationarity3 (U>Jf(X)V)2 = V>Jf>(X)Jf(X)V = ||Jf(X)V||22 and linearization
Jf(X)V ' f(X + V) - f(X) (which holds with equality if X + V ∈ X(φX)), we can regular-
ize learning also via the following sum-of-squares based spectral norm regularizer
minθ → E(X,y)〜P '(y,f (x)) + 2||f(x + ev) - f (x)||2	,	⑼
1	.1	1.1	1	1	.	i' T	♦	.	1	∙ T-T	. ∙	ι-1	ʌ 9
where the data-dependent singular vector V of Jf(X) is computed via Equation 7, and λ = λ2 .
Both variants can readily be implemented in modern deep learning frameworks. We found the
sum-of-squares based spectral norm regularizer to be more numerically stable than the Jacobian
based one, which is why we used this variant in our experiments.
In terms of computational complexity, the data-dependent regularizer is equally expensive as PGA-
based adversarial training, and both are a constant (number of power method iterations) times more
expensive than the data-independent variant, plus an overhead that depends on the batch size, which
is usually mitigated in modern frameworks by parallelizing computations across a batch of data.
4.2	Power Method Formulation of Adversarial Training
Adversarial training (Goodfellow et al., 2014; Kurakin et al., 2016; Madry et al., 2017) aims to improve
the robustness of a machine learning model by training it against an adversary that independently
perturbs each training example subject to a proximity constraint, e.g. in `p-norm,
min θ → E(x y)〜P ['(y,f (x)) + λ maχ 'adv(y, f (x*))] .	(10)
(X'y) PL	x*∈BP (x)	J
where 'adv(∙, ∙) denotes the loss function used to find adversarial perturbations (does not need to be
the same as the classification loss '(∙, ∙)).
The adversarial example x* is typically computed iteratively, e.g. via '2-norm constrained projected
gradient ascent (Madry et al., 2017; Kurakin et al., 2016) (the general `p-norm constrained case is
similar)
_ π (	.	Nχ'adv(y, f (Xk-1)) 、	2 2 22( ∖∖	∕11∖
Xk	B2(X) (XkT + α ∣∣Vχ'adv(y,f (xk-1 ))∣∣2 ) , x°~U (Be (X)) ,	(II)
2Due to numerical errors, we can safely assume that the dominant singular value is non-degenerate.
3u = Jf(x)v/||Jf(x)v||2
4
Under review as a conference paper at ICLR 2020
where ∏B2(x) is the projection operator into the norm ball B2(x) := {x* : ||x* - x∣∣2 ≤ e}, α
is a step-size or weighting factor, trading off the previous iterate xk-1 with the current gradient
direction Vχ'adv(y,f (xk-ι))∕∣∣Vχ'adv(y, f (Xk-I)) ∣∣2 =: vk, and y is the true or predicted label.
For targeted attacks the sign in front of α is flipped, so as to descend the loss function into the
direction of the target label.
By the chain-rule, the computation of the gradient-step vk can be decomposed into a logit-gradient
and a Jacobian vector product, while the projection into the '2 -norm ball ∏B2(X)can be expressed as a
normalization (see Section A.2 in the Appendix). The '2-norm constrained projected gradient ascent
attack can thus equivalently be written in the following power method like form (the normalization of
Uk is optional and can be absorbed into the normalization of Vk)
Uk — Uk/||Uk∣∣2 ,	Uk — Vz'adv(y, z)∣z=f(Xk-l)
Vk J Vk∕∣∣Vk∣∣2 ,	Vk J J>(χk-ι)Uk = Vx(f(Xk-ι)>Uk)
Xk J ∏B2(x)(Xk-1 + aVk)
(forward pass)
(backward pass) (12)
(projection)
Note that the logit-gradient Vz'adv (y, z)|z=f(x	) can be computed in a single forward pass, by
directly expressing it in terms of the arguments of the adversarial loss.
Comparing the update equations for projected gradient ascent based adversarial training with those
of data-dependent spectral norm regularization, we can see that adversarial training generalizes
spectral norm regularization in two ways: (i) via the choice of the adversarial loss function and (ii)
by iterating Xk within the norm ball (which also introduces an additional parameter α).
The adversarial loss function determines the direction Uk of the directional derivative
Vx(f(Xk-1)>Uk), cf. Section A.3 in the Appendix for an example using the softmax cross-entropy
loss. The following theorem shows that adversarial training based on a specific '2-norm constrained
projected gradient ascent attack is indeed equivalent to data-dependent spectral norm regularization.
Theorem 1. For small enough such that B2 (X) ⊂ X(φx) and in the limit α → ∞, '2 -norm
constrained projected gradient ascent based adversarial training with a sum-of-squares loss on the
logits of the clean and perturbed input 'adv (f (x), f (x*)) = 2 ||f (x) — f (x*)∣∣2 is equivalent to
data-dependent spectral norm regularization.
The proof can be found in Section A.2 in the Appendix.
The conditions on and α can be considered specifics of the iteration method. The condition that
be small enough such that B2(X) is contained in the ReLU cell around X ensures that the Jacobian
Jf(x*) = Jf(x) for all x* ∈ B2(x), while the condition that α → ∞ means that in the update
equation for Xk all the weight is placed on the current gradient direction Vk whereas no weight is
put on the previous iterate Xk-1. Note that the limit α → ∞ is well-defined since it is inside the
projection operation (the projection of Xk divides by α again).
Note that, in practice, the Theorem is applicable as long as the Jacobian of the network remains
approximately constant in the uncertainty ball under consideration, in which case the correspondence
between adversarial training and data-dependent spectral norm regularization holds approximately in
a region much larger than X(φx). In Section 5, we verify this in the experimental setting.
5	Experimental Results
5.1	Dataset, Architecture & Training Methods
We trained Convolutional Neural Networks (CNNs) with ReLU activations and batch normalization
on the CIFAR10 data set (Krizhevsky & Hinton, 2009). We use a 7-layer CNN as our default
platform, since it has good test set accuracy at acceptable computational requirements (we used an
estimated 2.5k GPU hours (Titan X) in total for all our experiments). We train each classifier with a
number of different training methods: (i) ‘Standard’: standard empirical risk minimization with a
softmax cross-entropy loss, (ii) ‘Adversarial’: '2-norm constrained projected gradient ascent (PGA)
based adversarial training with a softmax cross-entropy loss, (iii) ‘global SNR’: global spectral norm
regularization a` la Yoshida & Miyato (2017), and (iv) ‘d.-d. SNR’: data-dependent spectral norm
regularization.
5
Under review as a conference paper at ICLR 2020
Table 1: CIFAR10 test set accuracies and hyper-parameters for the CNN7 and training methods we
considered. The regularization constants were chosen such that the models achieve roughly the same
test set accuracy on clean examples as the adversarially trained model does.
Training Method	Accuracy	Hyper-parameters
Standard Training Adversarial Training	93.5% 83.6%	= 1.75, α = 2/ITERS, ITERS = 10
Global Spectral Norm Reg.	80.4%	λ = 3 ∙ 10-4, ITERS = I
Data-dep. Spectral Norm Reg.	84.6%	λ = 3 ∙ 10-2, e = 1.75, ITERS = 10
As a default attack strategy We use an '2-norm constrained PGA white-box attack with cross-entropy
adversarial loss `adv and 10 attack iterations. We verified that all our conclusions also hold for larger
numbers of attack iterations, however, due to computational constraints we limit the attack iterations
to 10. The attack strength used for training was chosen to be the smallest value such that almost all
adversarially perturbed inputs to the standard model are successfully misclassified, which is = 1.75
(indicated by a vertical dashed line in the Figures below). The regularization constants of the other
training methods were then chosen in such a way that they roughly achieve the same test set accuracy
on clean examples as the adversarially trained model does. Further details regarding the experimental
setup can be found in Section A.4 in the Appendix. Table 1 summarizes the test set accuracies and
hyper-parameters for the training methods we considered. Shaded areas in the plots below denote
standard errors with respect to the number of test set samples over which the experiment was repeated.
5.2	Spectral Properties
Effect of training method on singular value spectrum. We compute the singular value spectrum of
the Jacobian Jf(x) for networks f trained with different training methods and evaluated at a number of
different test set examples (200 except if stated otherwise). Since we are interested in computing the
full singular value spectrum, and not just the dominant singular value and singular vectors as during
training, the power method would be too impractical to use, as it gives us access to only one (the
dominant) singular value-vector pair at a time. Instead, we first extract the Jacobian (which is per se
defined as a computational graph in modern deep learning frameworks) as an input-dim×output-dim
dimensional matrix and then use available matrix factorization routines to compute the full SVD of
the extracted matrix. For each training method, the procedure is repeated for 200 randomly chosen
clean and corresponding adversarially perturbed test set examples. Further details regarding the
Jacobian extraction can be found in Section A.5 in the Appendix.
The results are shown in Figure 1 (left). We can see that, compared to the spectrum of the normally
trained and global spectral norm regularized model, the spectrum of adversarially trained and data-
dependent spectral norm regularized models is significantly damped after training. In fact, the
data-dependent spectral norm regularizer seems to dampen the singular values even slightly more
effectively than adversarial training, while global spectral norm regularization has almost no effect
compared to standard training.
Alignment of adversarial perturbations with singular vectors. We compute the cosine-similarity
of adversarial perturbations with singular vectors vr of the Jacobian Jf(x), extracted at a number of
test set examples, as a function of the rank of the singular vectors returned by the SVD decomposition.
For comparison we also show the cosine-similarity with the singular vectors of a random network as
well as the cosine-similarity with random perturbations.
The results are shown in Figure 1 (right). We can see that for all training methods (except the random
network) adversarial perturbations are strongly aligned with the dominant singular vectors while
the alignment decreases towards the bottom-ranked singular vectors. For the random network, the
alignment is roughly constant with respect to rank. Interestingly, this strong alignment with dominant
singular vectors also explains why input gradient regularization and fast gradient method (FGM)
based adversarial training do not sufficiently protect against adversarial attacks, namely because the
input gradient, resp. a single power method iteration, do not yield a sufficiently good approximation
for the dominant singular vector in general.
6
Under review as a conference paper at ICLR 2020
6nA JelnUIS
—Trained: Standard. Perturbations: PGA
----Trained: Adversarial. Perturbations: PGA
-Trained: data-dep. SNR. Perturbations: PGA
frJJBIIma,∙j∙sso□
Figure 1: (Left) Singular value spectrum of the Jacobian Jf(x) for networks f trained with different
training methods. (Right) Cosine-similarity of adversarial perturbations with singular vectors vr of
the Jacobian Jf(x), as a function of the rank r of the singular vector. For comparison we also show
the cosine-similarity with the singular vectors of a random network as well as the alignment with
random perturbations. Curves were aggregated over 200 samples from the test set.
-Trained: global SNR, Perturbations: PGA
Trained: Random lπit, Perturbations: PGA
—Trained: Standard. Perturbations: Random
»

5.3	Local Linearity
Validity of linear approximation. In order to determine the size of the area where a locally linear
approximation is valid, we measure the deviation from linearity of φL-1 (x + z) as the distance
∣∣z∣∣2 to X is increased in random and adversarial directions, i.e. We measure ∣∣φL-1(x + Z)-
(φL-1 (x)+JφL-ι(x)z) ∣∣2 asa function of the distance ∣∣z∣∣2, for random and adversarial perturbations
z, aggregated over 200 data points X in the test set, With adversarial perturbations serving as a proxy
for the direction in Which the linear approximation holds the least. The purpose of this experiment is
to investigate hoW good the linear approximation for different training methods is, as an increasing
number of activation boundaries are crossed With increasing perturbation radius. See Figure 1 in
(Raghu et al., 2017) or Figure 3 in (Novak et al., 2018) for an illustration of activation boundary
tesselations in the input space.
The results are shoWn in Figure 2 (left). We can see that adversarial training and data-dependent
spectral norm regularization give rise to models that are considerably more linear than the clean
trained one, both in random as Well as adversarial directions. Compared to the normally trained
model, the adversarially trained and spectral norm regularized ones remain flat in random directions
for pertubations of considerable magnitude and even remain flat in the adversarial direction for
perturbation magnitudes up to the order of the used during adversarial training, While the deviation
from linearity seems to increase roughly linearly With ||z||2 thereafter. The global spectral norm
regularized model behaves similar to the normally trained one.
Largest singular value over distance. Figure 2 (right) shoWs the largest singular value of the
linear operator Jf(x+z) as the distance ||z||2 from X is increased, both along random and adversarial
directions, for different training methods. We can see that the naturally trained netWork develops
large dominant singular values around the data point during training, While the adversarially trained
and data-dependent spectral norm regularized models manage to keep the dominant singular value
loW in the vicinity of X.
5.4	Adversarial Robustness
Adversarial classification accuracy. A plot of the classification accuracy on adversarially perturbed
test examples, as a function of the perturbation strength , is shoWn in Figure 3 (left). We can see
that the adversarial accuracy of the data-dependent spectral norm regularized model is comparable
to that of the adversarially trained model, While global spectral norm regularization does not seem
to robustify the model against adversarial attacks. This is in line With our earlier observation that
adversarial perturbations tend to align With dominant singular vectors and that adversarial training
and data-dependent spectral norm regularization dampen the singular values. Additional results
against '∞-PGA attack are provided in Section A.6 in the Appendix. The conclusions for this and
the other experiments remain the same. This indicates that the main effect of adversarial training is
captured by data-dependent spectral norm regularization.
7
Under review as a conference paper at ICLR 2020
—Trained： Standard, Perturbations： PGA
----Trained： Standard, Perturbations： Random
----Trained： Adversarial, Perturbations： PGA
—Trained： Adversarial, Perturbations： Random
Trained： data-dep. SNR, Perturbations： PGA
----Trained： data-dep. SNR, Perturbations： Random
—Trained： global SNR, Perturbations： PGA
--Trained： global SNR, Perturbations： Random
b08040≈0
w36umo.IJ UOnElAα
0.0 ε* 2.5	5.0	7.5	10.0	12.5	15.0	17.5	20.0
^^^
Distance from X
≈s≈0*s*°
np3A JeIns doj
0.0 ε'	5.0	10.0	15.0	20.0
Distance from X
—Trained： Standard, Perturbations： PGA
----Trained： Standard, Perturbations： Random
----Trained： Adversarial, Perturbations： PGA
—Trained： Adversarial, Perturbations： Random
-Trained： data-dep. SNR, Perturbations： PGA
----Trained： data-dep. SNR, Perturbations： Random
"Trained： global SNR, Perturbations： PGA
----Trained： global SNR, Perturbations： Random
joEIn。。V
luuXΠUSJ{v ∙IOs6A Jeln°0ua
Figure 2: (Left) Deviation from linearity ∣∣φL-1(x + Z) - (φL-1(x) + JφL-i(χ)z)∣∣2 as a function
of the distance ||z||2 from x for random and adversarial perturbations z. (Right) Largest singular
value of the linear operator Jf(x+z) as a function of the magnitude ||z||2 of random and adversarial
perturbations z. The dashed vertical line indicates the used during adversarial training. Curves were
aggregated over 200 samples from the test set.
1.0	ε, 2.0	3.0	4.0	0.0	¢2.0	4.0	6.0	8.0	10.0	12.0	14.0
Perturbation Magnitude e	Perturbation Magnitude e
Figure 3: (Left) Classification accuracy as a function of perturbation strength . (Right) Alignment
of adversarial perturbations with dominant singular vector of Jf(x) as a function of perturbation
magnitude . The dashed vertical line indicates the used during adversarial training. Curves were
aggregated over 2000 samples from the test set.
Alignment of adversarial perturbations with dominant singular vector. Figure 3 (right) shows
the cosine-similarity of adversarial perturbations of mangitude with the dominant singular vector of
Jf(x), as a function of perturbation magnitude . For comparison, we also include the alignment with
random perturbations. For all training methods, the larger the perturbation magnitude , the lesser the
adversarial perturbation aligns with the dominant singular vector of Jf(x), which is to be expected
for a simultaneously increasing deviation from linearity. The alignment is similar for adversarially
trained and data-dependent spectral norm regularized models and for both larger than that of global
spectral norm regularized and naturally trained models.
6	Conclusion
We established a theoretical link between adversarial training and operator norm regularization
for deep neural networks. Specifically, we presented a data-dependent variant of spectral norm
regularization that directly regularizes large singular values of a neural network in regions that are
supported by the data and proved that it is equivalent to adversarial training based on a specific
'2-norm constrained projected gradient ascent attack. This fundamental connection confirms the
long-standing argument that a network’s sensitivity to adversarial examples is tied to its spectral
properties and opens the door for adversarially robust generalization bounds via data-dependent
spectral norm based ones. We also conducted extensive empirical evaluations showing that (i)
adversarial perturbations align with dominant singular vectors, (ii) adversarial training and data-
dependent spectral norm regularization dampen the singular values, and (iii) both training methods
give rise to models that are significantly more linear around data points than normally trained ones.
8
Under review as a conference paper at ICLR 2020
References
Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of
security: Circumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420, 2018.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Advances in Neural Information Processing Systems, pp. 6241-6250, 2017.
Dimitris Bertsimas and Martin S Copenhaver. Characterization of the equivalence of robustification
and regularization in linear and matrix regression. European Journal of Operational Research, 270
(3):931-942, 2018.
Alberto Bietti, Gregoire Mialon, and Julien Mairal. On regularization and robustness of deep neural
networks. arXiv preprint arXiv:1810.00363, 2018.
Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Srndic, Pavel Laskov, Giorgio
Giacinto, and Fabio Roli. Evasion attacks against machine learning at test time. In Joint European
conference on machine learning and knowledge discovery in databases, pp. 387-402. Springer,
2013.
Sebastien Bubeck, Eric Price, and Ilya Razenshteyn. Adversarial examples from computational
constraints. arXiv preprint arXiv:1805.10204, 2018.
Nicholas Carlini and David Wagner. Adversarial examples are not easily detected: Bypassing ten
detection methods. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and
Security, pp. 3-14. ACM, 2017.
Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier. Parseval
networks: Improving robustness to adversarial examples. In International Conference on Machine
Learning, pp. 854-863, 2017.
Laurent El Ghaoui and Herve Lebret. Robust solutions to least-squares problems with uncertain data.
SIAM Journal on matrix analysis and applications, 18(4):1035-1064, 1997.
Farzan Farnia, Jesse M Zhang, and David Tse. Generalizable adversarial training via spectral
normalization. arXiv preprint arXiv:1811.07457, 2018.
Alhussein Fawzi, Hamza Fawzi, and Omar Fawzi. Adversarial vulnerability for any classifier. arXiv
preprint arXiv:1802.08686, 2018.
Reuben Feinman, Ryan R Curtin, Saurabh Shintre, and Andrew B Gardner. Detecting adversarial
samples from artifacts. arXiv preprint arXiv:1703.00410, 2017.
Rui Gao and Anton J Kleywegt. Distributionally robust stochastic optimization with wasserstein
distance. arXiv preprint arXiv:1604.02199, 2016.
Justin Gilmer, Luke Metz, Fartash Faghri, Samuel S Schoenholz, Maithra Raghu, Martin Wattenberg,
and Ian Goodfellow. Adversarial spheres. arXiv preprint arXiv:1801.02774, 2018.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014.
Kathrin Grosse, Praveen Manoharan, Nicolas Papernot, Michael Backes, and Patrick McDaniel. On
the (statistical) detection of adversarial examples. arXiv preprint arXiv:1702.06280, 2017.
Shixiang Gu and Luca Rigazio. Towards deep neural network architectures robust to adversarial
examples. arXiv preprint arXiv:1412.5068, 2014.
Matthias Hein and Maksym Andriushchenko. Formal guarantees on the robustness of a classifier
against adversarial manipulation. In Advances in Neural Information Processing Systems, pp.
2266-2276, 2017.
Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly,
Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al. Deep neural networks
for acoustic modeling in speech recognition: The shared views of four research groups. IEEE
Signal Processing Magazine, 29(6):82-97, 2012.
9
Under review as a conference paper at ICLR 2020
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world.
arXiv preprint arXiv:1607.02533, 2016.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436, 2015.
Chunchuan Lyu, Kaizhu Huang, and Hai-Ning Liang. A unified gradient regularization family for
adversarial examples. In 2015 IEEE International Conference on Data Mining, pp. 301-309. IEEE,
2015.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017.
Jan Hendrik Metzen, Tim Genewein, Volker Fischer, and Bastian Bischoff. On detecting adversarial
perturbations. arXiv preprint arXiv:1702.04267, 2017.
Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, Ken Nakae, and Shin Ishii. Distributional
smoothing with virtual adversarial training. arXiv preprint arXiv:1507.00677, 2015.
Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a reg-
ularization method for supervised and semi-supervised learning. arXiv preprint arXiv:1704.03976,
2017.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for
generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018.
Seyed Mohsen Moosavi Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and
accurate method to fool deep neural networks. In Proceedings of 2016 IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), number EPFL-CONF-218057, 2016.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal
adversarial perturbations. arXiv preprint, 2017.
Hongseok Namkoong and John C Duchi. Variance-based regularization with convex objectives. In
Advances in Neural Information Processing Systems, pp. 2975-2984, 2017.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural
networks. In Conference on Learning Theory, pp. 1376-1401, 2015.
Roman Novak, Yasaman Bahri, Daniel A Abolafia, Jeffrey Pennington, and Jascha Sohl-
Dickstein. Sensitivity and generalization in neural networks: an empirical study. arXiv preprint
arXiv:1802.08760, 2018.
Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow. Transferability in machine learning: from
phenomena to black-box attacks using adversarial samples. arXiv preprint arXiv:1605.07277,
2016.
Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, and Jascha Sohl Dickstein. On the
expressive power of deep neural networks. In Proceedings of the 34th International Conference on
Machine Learning-Volume 70, pp. 2847-2854. JMLR. org, 2017.
Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certified defenses against adversarial
examples. arXiv preprint arXiv:1801.09344, 2018.
Kevin Roth, Yannic Kilcher, and Thomas Hofmann. The odds are odd: A statistical test for detecting
adversarial examples. arXiv preprint arXiv:1902.04818, 2019.
Sara Sabour, Yanshuai Cao, Fartash Faghri, and David J Fleet. Adversarial manipulation of deep
representations. arXiv preprint arXiv:1511.05122, 2015.
Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander Madry. Adver-
sarially robust generalization requires more data. arXiv preprint arXiv:1804.11285, 2018.
10
Under review as a conference paper at ICLR 2020
Uri Shaham, Yutaro Yamada, and Sahand Negahban. Understanding adversarial training: Increasing
local stability of neural nets through robust optimization. arXiv preprint arXiv:1511.05432, 2015.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. In International Conference on Learning Representations (ICLR), 2014.
Aman Sinha, Hongseok Namkoong, and John Duchi. Certifiable distributional robustness with
principled adversarial training. arXiv preprint arXiv:1710.10571, 2017.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Yusuke Tsuzuku, Issei Sato, and Masashi Sugiyama. Lipschitz-margin training: Scalable certification
of perturbation invariance for deep neural networks. In Advances in Neural Information Processing
Systems,pp. 6541-6550, 2018.
Huan Xu, Constantine Caramanis, and Shie Mannor. Robustness and regularization of support vector
machines. Journal of Machine Learning Research, 10(Jul):1485-1510, 2009.
Weilin Xu, David Evans, and Yanjun Qi. Feature squeezing: Detecting adversarial examples in deep
neural networks. arXiv preprint arXiv:1704.01155, 2017.
Yuichi Yoshida and Takeru Miyato. Spectral norm regularization for improving the generalizability
of deep learning. arXiv preprint arXiv:1705.10941, 2017.
11
Under review as a conference paper at ICLR 2020
A	Appendix
A.1 Robust Optimization and Regularization for Linear Regression
In this section, we recapitulate the basic ideas on the relation between robust optimization and
regularization presented in Bertsimas & Copenhaver (2018). Note that the notation deviates slightly
from the main text: most importantly, the perturbations 4 refer to perturbations of the entire training
data X, as is common in robust optimization. Consider linear regression with additive perturbations
4 of the data matrix X
min max g (y - (X + 4)w) ,	with loss g : Rn → R ,	(13)
w 4∈U
where U denotes the uncertainty set. A general way to construct U is as a ball of bounded matrix
norm perturbations U = {4 : k4k ≤ λ}. Of particular interest are induced matrix norms
IlAllg,h := max {，h(w) }, g : Rn → R, h : Rd → R,	(14)
where g is a semi-norm and h is a norm. It is obvious that if g fulfills the triangle inequality then one
can upper bound
g (y - (X + 4)w) ≤ g(y - Xw)+ g(4w) ≤ g(y - Xw)+ λh(w),	∀4 ∈ U ,	(15)
by using (a) the triangle inequality and (b) the definition of the matrix norm.
The question then is, under which circumstances both inequalities become equalities at the maximizing
4. It is straightforward to check (Bertsimas & Copenhaver, 2018, Theorem 1) that specifically We
may choose the rank 1 matrix
4 = —^-rv>,	where r = y - XW , V = argmax {v>w} , (h* is the dual norm).
g(r)	v:h*(v)=1
(16)
If g(r) = 0 then one can pick any u for which g(u) = 1 to form 4 = λuv> (such a u has to exist
if g is not identically zero). This shows that, for robust linear regression with induced matrix norm
uncertainty sets, robust optimization is equivalent to regularization.
A.2 Proof of Main Theorem
Theorem 1 For small enough such that B2 (x) ⊂ X(φx) and in the limit α → ∞, `2 -norm
constrained projected gradient ascent based adversarial training with a sum-of-squares loss on the
logits of the clean and perturbed input 'adv (f (x), f (x*)) = ɪ ||f (x) — f (x*)||| is equivalent to
data-dependent spectral norm regularization.
To proof the theorem we need show that the update equations in (12) for '∣-norm constrained
projected gradient ascent based adversarial training with the sum-of-squares loss function and the
above conditions on and α reduce to the corresponding update equations for data-dependent spectral
norm regularization in equation (7).
We start with the following lemma.
Lemma 1. In the limit α → ∞, the projection xk = ΠB2 (x) (xk-1 + αvk) reduces to xk = x + vk.
Proof. The projection xk = ΠB2 (x) (xk-1 + αvk) can be expressed as follows,
Xk = X + E(Xk - x)/max(e, ||Xk - X∣∣∣) with Xk = Xk-1 + Ovk ,	(17)
where the max(e, ||Xk - x||i) ensures that if ||Xk - x||i < e then Xk = Xk.
12
Under review as a conference paper at ICLR 2020
Thus,
xk =αl→im∞ΠB2(x)(xk-1+αvk)
lim x +
α→∞
x + lim
α→∞
x + lim
α→∞
x + lim
Xk - X
max(e, ||Xk - x∣∣2)
Xk—1 + αvk - X
max(e, ∣∣Xk-i + av - X∣∣2)
α(vk + 1 (Xk-I - x))
max(e, α∣∣Vk + 1 (Xk-I - x)∣∣2)
α(vk + 1 (Xk-I - x))
α→∞ α
||vk + 1 (Xk-I - X)||2
x+ vk/||vk||2
(18)
(19)
(20)
(21)
(22)
(23)
X+ vk
(24)
where in the third-to-last line we used that the max will be attained at its second argument in the
limit ɑ → ∞ since ||vk + 1 (Xk-I — x)∣∣2 > 0 for Vk = 0, and in the last line We used that Vk is
normalized by construction, i.e. by how it is defined in the update equations in (12).
□
Proof of Theorem 1. With the lemma in place, it is noW easy to shoW that for small enough, such
that B2(X) ⊂ X(φx), and in the limit α→ ∞, adversarial training reduces to data-dependent spectral
norm regularization. Let us shoW this step by step.
In the limit α→ ∞, We have for the forWard pass
U k = Vz 2||f (X)- zll2∣z=f(Xk-1)	(25)
= f(Xk-1) - f(X)	(26)
= Jf(x) (Xk-1 - X)	(27)
= Jf(x)vk-1,	(28)
Where the last equality holds by the lemma.
For the backWard pass, We have
vk = J>(χk-ι)uk = J>(χ)uk ,	(29)
since Jf(xk) = Jf(x) for all Xk ∈ B2(X).
Together, this gives
Uk — Uk/||Uk∣∣2 , Uk — Jf(X)Vk-1
Vk — Vk∕∣∣Vk∣∣2 ,	Vk - J>(χ)Uk	(30)
Xk — X + EVk .
Observing that the first tWo update equations don’t depend on Xk, and absorbing the E into the
normalization of Uk, We precisely end UP with the data-dependent spectral norm regularization in
Equation (7):
Uk J Uk/||Uk ∣∣2 ,	Uk J Jf (x)Vk-1
Vk J Vk∕∣∣Vk∣∣2 ,	Vk J J>(χ)Uk
(31)
We have thus proved that the update equations for the adversarial attack in Theorem 1 correspond
exactly to those of data-dependent spectral norm regularization.
It is also easy to see that the adversarial training objective in Equation (10) corresponds to that of
data-dependent spectral norm regularization for the specific choice of adversarial loss function
min θ → E(X y)〜P h'(y,f (x)) + λ maχ 1||f (X)-f (x*)||2i .	(32)
、x!	L	x*∈B2(x) 2	-I
13
Under review as a conference paper at ICLR 2020
which by the condition B2(x) ⊂ X(φχ) and x* = X + Ev is equivalent to
λ2
minθ → E(χ,y)〜P ['(y,f (x)) + -2-(Jf(X)V)[
(33)
where the data-dependent singular vector v is computed via Equation (7).
□
A.3 Effect of the Adversarial Loss Function on the Logit-space Direction
The adversarial loss function determines the logit-space direction uk of the directional derivative in
the power method like formulation of adversarial training in Equation 12.
Let us consider this for the softmax cross-entropy loss, defined as
'adv(y,z) = - lθg(Sy (Z)) = -Zy +lθg I EeXp(Zk)
k=1
,sy (z)：= Pdexp(Zy L
k=1 eXp(Zk)
Untargeted `2 -PGA on softmax cross-entropy loss: (forward pass)
[uk J Vz'adv(y, Z)IZ=f(χk)[ = si(f (Xk)) - δiy
Targeted `2 -PGA on softmax cross-entropy loss: (forward pass)
[u k — —Vz'adv (yadv, Z)|z=f (xk)] . = δiyadv - si(f (Xk ))
(34)
(35)
(36)
Notice that the logit gradient can be computed in a forward pass by analytically expressing it in terms
of the arguments of the objective function (this is why we call the uk update a forward pass).
Interestingly, for a temperature-dependent softmax cross-entropy loss, the logit-space direction
becomes a “label-flip” vector in the low-temperature limit (high inverse temperature β → ∞)
where the softmax Sy(z) := exp(βZy)∕(Pk=ι exp(βZk)) converges to the argmax: sβ(z) β→→∞
arg max(Z). E.g. for targeted attacks ukβ→∞i = δiyadv - δiy(xk). This implies that in the high β
limit, iterative PGA finds an input space perturbation vk that corresponds to the steepest ascent of f
along the “label flip” direction ukβ→∞ .
A note on canonical link functions. Interestingly, the gradient of the loss w.r.t. the logits of the
classifier takes the form “prediction - target” for both the sum-of-squares error as well as the softmax
cross-entropy loss. This is in fact a general result of modelling the target variable with a conditional
distribution from the exponential family along with a canonical link (activation) function. This means
that in both cases, adversarial attacks try to find perturbations in input space that induce a logit
perturbation that aligns with the difference between the current prediction and the attack target.
A.4 Dataset, Architecture & Training Methods
We trained Convolutional Neural Networks (CNNs) with seven hidden layers and batch normalization
on the CIFAR10 data set Krizhevsky & Hinton (2009). The CIFAR10 dataset consists of 60k
32 × 32 colour images in 10 classes, with 6k images per class. It comes in a pre-packaged train-
test split, with 50k training images and 10k test images, and can readily be downloaded from
https://www.cs.toronto.edu/~kriz/cifar.html.
We conduct our experiments on a pre-trained standard convolutional neural network, employing
7 convolutional layers, augmented with BatchNorm, ReLU nonlinearities and MaxPooling. The
network achieves 93.5% accuracy on a clean test set. Relevant links to download the pre-trained
model can be found in our codebase.
We adopt the following standard preprocessing and data augmentation scheme: Each training image
is zero-padded with four pixels on each side, randomly cropped to produce a new image with the
original dimensions and horizontally flipped with probability one half. We also standardize each
image to have zero mean and unit variance when passing it to the classifier.
14
Under review as a conference paper at ICLR 2020
The attack strength e used for PGA was chosen to be the smallest value such that almost all adver-
sarially perturbed inputs to the standard model are successfully misclassified, which is e = 1.75.
The regularization constants of the other training methods were then chosen in such a way that they
roughly achieve the same test set accuracy on clean examples as the adversarially trained model does,
i.e. we allow a comparable drop in clean accuracy for regularized and adversarially trained models.
When training the derived regularized models, we started from a pre-trained checkpoint and ran a
hyper-parameter search over number of epochs, learning rate and regularization constants. Table 1
summarizes the test set accuracies and hyper-parameters for all the training methods we considered.
A.5 Extracting Jacobian as a Matrix
Since we know that any neural network with its nonlinear activation function set to fixed values
represents a linear operator, which, locally, is a good approximation to the neural network itself,
we develop a method to fully extract and specify this linear operator in the neighborhood of any
input datapoint X. We have found the naive way of determining each entry of the linear operator by
consecutively computing changes to individual basis vectors to be numerically unstable and therefore
have settled for a more robust alternative:
In a first step, we run a set of randomly perturbed versions of X through the network (with fixed
activation functions) and record their outputs at the particular layer that is of interest to us (usually the
logit layer). In a second step, we compute a linear regression on these input-output pairs to obtain a
weight matrix W as well as a bias vector b, thereby fully specifying the linear operator. The singular
vectors and values of W can be obtained by performing an SVD.
A.6 Further Experimental Results for '∞-PGA
----Trained： Standard, Perturbations： PGA
----Trained： Standard, Perturbations： Random
----Trained： Adversarial, Perturbations： PGA
—Trained： Adversarial, Perturbations： Random
Trained： data-dep. SNR, Perturbations： PGA
----Trained： data-dep. SNR, Perturbations： Random
----Trained： global SNR, Perturbations： PGA
--Trained： global SNR, Perturbations： Random
K"=20*5*0
6nA Jelns doj
—Trained： Standard, Perturbations： PGA
----Trained： Standard, Perturbations： Random
----Trained： Adversarial, Perturbations： PGA
— Trained： Adversarial, Perturbations： Random
—Trained： data-dep. SNR, Perturbations： PGA
----Trained： data-dep. SNR, Perturbations： Random
-Trained： global SNR, Perturbations： PGA
----Trained： global SNR, Perturbations： Random
*∞b08040≈0
frw36ujmo.IJ UOnEIAuα
0 8 6 4 2 0
■ I I I I I
Iooooo
joEIn。。V
luuXΠUSJ{v ∙IOs6A Jeln°0ua
0.0 ε 20.0	40.0	60.0	80.0	0.0 ε 20.0	40.0	60.0	80.0	100.0
— —
Distance from X	Distance from X
Figure 4: (Left) Deviation from linearity ∣∣φL-1(x + Z) - (φL-1(x) + JφL-13)z)∣∣2 as a function
of the distance ∣∣z∣∣2 from X for random and '∞-PGA adversarial perturbations z. (Right) Largest
singular value of the linear operator Jf (x+z) as a function of the magnitude ||z||2 of random and
'∞-PGA adversarial perturbations z. The dashed vertical line indicates the e used during adversarial
training. Curves were aggregated over 200 samples from the test set.
2.5	5.0	7.fe,	10.0	12.5	15.0	17.5	20.0	0.0	ε10.0	20.0	30.0	40.0	50.0	60.0	70.0
Perturbation Magnitude e	Perturbation Magnitude e
Figure 5: (Left) Classification accuracy as a function of perturbation strength (measured in 8-bit).
(Right) Alignment of '∞-PGA adversarial perturbations with dominant singular vector of Jf 3)as a
function of perturbation magnitude . The dashed vertical line indicates the used during adversarial
training. Curves were aggregated over 2000 samples from the test set.
15
Under review as a conference paper at ICLR 2020
A.7 Further Experimental Results for global SNR with 10 iterations
In the main section, we have implemented the baseline version of global SNR as close as possible to
the descriptions in Yoshida & Miyato (2017). However, this included a recommendation from the
authors to perform only a single update iteration to the spectral decompositions of the weight matrices
per training step. As this is computationally less demanding than the 10 iterations per training
step spent on both adversarial training, as well as data-dependent spectral norm regularization, we
verify that performing 10 iterations makes no difference to the method of Yoshida & Miyato (2017).
Figures 6 and 7 reproduce the curves for global SNR from the main part (having used 1 iteration) and
overlap it with the same experiments, but done with global SNR using 10 iterations. As can be seen,
there is no significant difference between the two.
b08040≈0
IJBUI日0亡 UOnElAα
*8ɪ4'≈*08 6
np3A Jelns do1
joEIn。。V
1.0	ε, 2.0	3.0	4.0
Perturbation Magnitude e
Figure 7: (Left) Classification accuracy as a function of perturbation strength . (Right) Alignment
of adversarial perturbations with dominant singular vector of Jf(x) as a function of perturbation
magnitude . The dashed vertical line indicates the used during adversarial training. Curves were
aggregated over 2000 samples from the test set.
0.0 e* 2.5	5.0	7.5	10.0	12.5	15.0	17.5	20.0	0.0 t*	5.0	10.0	15.0	20.0
一— —
Distance from X	Distance from X
Figure 6: (Left) Deviation from linearity ∣∣φL-1(x + Z) - (φL-1(x) + JφL-13)z)∣∣2 as a function
of the distance ||z||2 from x for random and adversarial perturbations z. (Right) Largest singular
value of the linear operator Jf(x+z) as a function of the magnitude ||z||2 of random and adversarial
perturbations z. The dashed vertical line indicates the used during adversarial training. Curves were
aggregated over 200 samples from the test set.
0.20
0.15
0.10
luuXΠUSJ{v ∙IOs6A Jeln°0∙ss
0.00
do ε2-0	4：o	6：0	8：0	10.0	12.0	14.0
Perturbation Magnitude e
A.8 Hyperparameter Sweep
We tested the following hyperparameter configurations during training and selected the best perform-
ing for each training method. Table 2 details the searched parameters.
A.9 ADVERSARIAL TRAINING WITH LARGE α
Figure 8 shows the result of empirically raising the adversarial learning rate during adversarial
training with PGA to very large values. As can be seen, the adversarial robustness initially rises, but
after some threshold it levels out and does not change significantly even at very large values.
16
Under review as a conference paper at ICLR 2020
Table 2: Hyperparameter sweep during training.
Training Method	Hyperparameter	Values Tested
Adversarial Training		0.5, 0.75, 1.0, 1.25, 1.5, 1.75, 2.0, 2.25, 2.5, 2.75, 3.0, 3.25, 3.5, 3.75, 4.0
	α	/ITERS, 2/ITERS, 3/ITERS, 4/ITERS, 5/ITERS
	ITERS	1,2,3,5,8, 10, 15, 20, 30, 40, 50
Global Spectral Norm Reg.	λ	1 ∙ 10-5,3 ∙ 10-5, 1 ∙ 10-4, 3 ∙ 10-4, 1 ∙ 10-3,3 ∙ 10-3, 1 ∙ 10-2,3 ∙ 10-2, 1 ∙ 10-1,3 ∙ 10-1 1 ∙ 100,3 ∙ 100, 1 ∙ 101,3 ∙ 101
	ITERS	1,10
Data-dep. Spectral Norm Reg.	λ	1 ∙ 10-5,3 ∙ 10-5, 1 ∙ 10-4, 3 ∙ 10-4, 1 ∙ 10-3,3 ∙ 10-3, 1 ∙ 10-2,3 ∙ 10-2, 1 ∙ 10-1,3 ∙ 10-1 1 ∙ 100,3 ∙ 100,1 ∙ 101,3 ∙ 101
	ITERS	1,2,3,5,8, 10, 15, 20, 30, 40, 50
Figure 8: Test set accuracy on clean and adversarial examples after using adversarial (PGA) training
with a given step size α. The dashed line indicates the α used when generating adversarial examples
at test time.
17