Under review as a conference paper at ICLR 2020
Model Imitation for Model-Based Reinforce-
ment Learning
Anonymous authors
Paper under double-blind review
Ab stract
Model-based reinforcement learning (MBRL) aims to learn a dynamic model to
reduce the number of interactions with real-world environments. However, due to
estimation error, rollouts in the learned model, especially those of long horizons,
fail to match the ones in real-world environments. This mismatching has seriously
impacted the sample complexity of MBRL. The phenomenon can be attributed to
the fact that previous works employ supervised learning to learn the one-step tran-
sition models, which has inherent difficulty ensuring the matching of distributions
from multi-step rollouts. Based on the claim, we propose to learn the synthesized
model by matching the distributions of multi-step rollouts sampled from the syn-
thesized model and the real ones via WGAN. We theoretically show that matching
the two can minimize the difference of cumulative rewards between the real tran-
sition and the learned one. Our experiments also show that the proposed model
imitation method outperforms the state-of-the-art in terms of sample complexity
and average return.
1	Introduction
Reinforcement learning (RL) has become of great interest because plenty of real-world problems can
be modeled as a sequential decision-making problem. Model-free reinforcement learning (MFRL) is
favored by its capability of learning complex tasks when interactions with environments are cheap.
However, in the majority of real-world problems, such as autonomous driving, interactions are ex-
tremely costly, thus MFRL becomes infeasible. One critique about MFRL is that it does not fully
exploit past queries over the environment, and this motivates us to consider the model-based rein-
forcement learning (MBRL). In addition to learning an agent policy, MBRL also uses the queries
to learn the dynamics of the environment that our agent is interacting with. If the learned dynamic
is accurate enough, the agent can acquire the desired skill by simply interacting with the simulated
environment, so that the number of samples to collect in the real world can be greatly reduced. As
a result, MBRL has become one of the possible solutions to reduce the number of samples required
to learn an optimal policy.
Most previous works of MBRL adopt supervised learning with '2-based errors (LUo et al., 2019;
Kurutach et al., 2018; Clavera et al., 2018) or maximum likelihood (Janner et al., 2019), to obtain
an environment model that synthesizes real transitions. These non-trivial developments imply that
optimizing a policy on a synthesized environment is a challenging task. Because the estimation error
of the model accumulates as the trajectory grows, it is hard to train a policy on a long synthesized
trajectory. On the other hand, training on short trajectories makes the policy short-sighted. This
issue is known as the planning horizon dilemma (Wang et al., 2019). As a result, despite having a
strong intuition at first sight, MBRL has to be designed meticulously.
Intuitively, we would like to learn a transition model in a way that it can reproduce the trajectories
that have been generated in the real world. Since the attained trajectories are sampled according to
a certain policy, directly employing supervised learning may not necessarily lead to the mentioned
result especially when the policy is stochastic. The resemblance in trajectories matters because we
estimate policy gradient by generating rollouts; however, the one-step model learning adopted by
many MBRL methods do not guarantee this. Some previous works propose multi-step training (Luo
et al., 2019; Asadi et al., 2019; Talvitie, 2017); however, experiments show that model learning
fails to benefit much from the multi-step loss. We attribute this outcome to the essence of super-
1
Under review as a conference paper at ICLR 2020
Supervised
learning
Distribution
matching
Rolloutfrom real
world
Rolloutfrom
model
Figure 1: Distribution matching enables the learned transition to generate similar rollouts to the real
ones even when the policy is stochastic or the initial states are close. On the other hand, training
with supervised learning does not ensure rollout similarity and the resulting policy gradient may be
inaccurate. This figure considers a fixed policy sampling in the real world and a transition model.
vised learning, which elementally preserves only one-step transition and the similarity between real
trajectories and the synthesized ones cannot be guaranteed.
In this work, we propose to learn the transition model via distribution matching. Specifically, we use
WGAN (Arjovsky et al., 2017) to match the distributions of state-action-next-state triple (s, a, s0) in
real/learned models so that the agent policy can generate similar trajectories when interacting with
either the true transition or the learned transition. Figure 1 illustrates the difference between meth-
ods based on supervised learning and distribution matching. Different from the ensemble methods
proposed in previous works, our method is capable of generalizing to unseen transitions with only
one dynamic model because merely incorporating multiple models does not alter the essence that
one-step (or few-step) supervised learning fails to imitate the distribution of multi-step rollouts.
Concretely, we gather some transitions in the real world according to a policy. To learn the real
transition T , we then sample fake transitions from our synthesized model T0 with the same policy.
The synthesized model serves as the generator in the WGAN framework and there is a critic that
discriminates the two transition data. We update the generator and the critic alternatively until the
synthesized data cannot be distinguished from the real one, which we will show later that it gives
T → T 0 theoretically.
Our contributions are summarized below:
•	We propose an MBRL method called model imitation (MI), which enforces the learned
transition model to generate similar rollouts to the real one so that policy gradient is accu-
rate;
•	We theoretically show that the transition can be learned by MI in the sense that T → T0 by
consistency and the difference in cumulative rewards can be bounded by the training error
of WGAN;
•	To stabilize model learning, we deduce guarantee for our sampling technique and investi-
gate training across WGANs;
•	We experimentally show that MI is more sample efficient than state-of-the-art MBRL and
MFRL methods and outperforms them on four standard tasks.
2	Related Work
In this section, we introduce our motivation inspired by learning from demonstration (LfD) (Schaal,
1997) and give a brief survey of MBRL methods.
2.1	Learning from Demonstration
A straightforward approach to LfD is to leverage behavior cloning (BC), which reduces LfD to
a supervised learning problem. Even though learning a policy via BC is time-efficient, it cannot
imitate a policy without sufficient demonstration because the error may accumulate without the
guidance of expert (Ross et al., 2011). Generative Adversarial Imitation Learning (GAIL) (Ho
& Ermon, 2016) is another state-of-the-art LfD method that learns an optimal policy by utilizing
2
Under review as a conference paper at ICLR 2020
generative adversarial training to match occupancy measure (Syed et al., 2008). GAIL learns an
optimal policy by matching the distribution of the trajectories generated from an agent policy with
the distribution of the given demonstration. Ho & Ermon (2016) shows that the two distributions
match if and only if the agent has learned the optimal policy. One of the advantages of GAIL is
that it only requires a small amount of demonstration data to obtain an optimal policy but it requires
a considerable number of interactions with environments for the generative adversarial training to
converge.
Our intuition is that transition learning (TL) is similar to learning from demonstration (LfD) by
exchanging the roles of transition and policy. In LfD, trajectories sampled from a fixed transition
are given, and the goal is to learn a policy. On the other hand, in TL, trajectories sampled from a
fixed policy are given, and we would like to imitate the underlying transition. That being said, from
LfD to TL, we interchange the roles of the policy and the transition. It is therefore tempting to study
the counterpart of GAIL in TL; i.e., learning the transition by distribution matching. Fortunately,
by doing so, the pros of GAIL remain while the cons are insubstantial in MBRL because sampling
with the learned model is considered to be much cheaper than sampling in the real one. That GAIL
learns a better policy than what BC does suggests that distribution matching possesses the potential
to learn a better transition than supervised learning.
2.2	Model-Based Reinforcement Learning
For deterministic transition, '@-based error is usually utilized to learn the transition model. Naga-
bandi et al. (2018), an approach that uses supervised learning with mean-squared error as its ob-
jective, is shown to perform well under fine-tuning. To alleviate model bias, some previous works
adopt ensembles (Kurutach et al., 2018; Buckman et al., 2018), where multiple transition models
with different initialization are trained at the same time. In a slightly more complicated manner,
Clavera et al. (2018) utilizes meta-learning to gather information from multiple models. Lastly, on
the theoretical side, SLBO (Luo et al., 2019) is the first algorithm that develops from solid theoretical
properties for model-based deep RL via a joint model-policy optimization framework.
For the stochastic transition, maximum likelihood estimator or moment matching are natural ways
to learn a synthesized transition, which is usually modeled by the Gaussian distribution. Following
this idea, Gaussian process (Kupcsik et al., 2013; Deisenroth et al., 2015) and Gaussian process
with model predictive control (Kamthe & Deisenroth, 2017) are introduced as an uncertainty-aware
version of MBRL. Similar to the deterministic case, to mitigate model bias and foster stability,
an ensemble method for probabilistic networks (Chua et al., 2018) is also studied. An important
distinction between training a deterministic or stochastic transition is that although the stochastic
transition can model the noise hidden within the real world, the stochastic model may also induce
instability if the true transition is deterministic. This is a potential reason why an ensemble of models
is adopted to reduce variance.
3	Background
3.1	Reinforcement Learning
We consider the standard Markov Decision Process (MDP) (Sutton & Barto, 1998). MDP is repre-
sented by a tuple hS, A, T, r, γi, where S is the state space, A is the action space, T (st+1 |st, at) is
the transition density of state st+1 at time step t + 1 given action at made under state st, r(s, a) is
the reward function, and γ ∈ (0, 1) is the discount factor.
A stochastic policy π(a∣s) is a density of action a given state s. Let the initial state distribution be
α. The performance of the triple (α, π, T) is evaluated in the expectation of the cumulative reward
in the γ-discounted infinite horizon setting:
∞
R(α, π, T) = E
γtr(st, at)α, π, T
t=0
H-1
X r(st, at)α, π, T
t=0
(1)
E
Equivalently, R(α, π, T ) is the expected cumulative rewards in a length-H trajectory {st , at}tH=-01
generated by (α,∏,T) with H 〜 Geometric(1 - γ). When a and T are fixed, R(∙) becomes a
3
Under review as a conference paper at ICLR 2020
function that only depends on π, and reinforcement learning algorithms (Sutton & Barto, 1998) aim
to find a policy π to maximize R(π).
3.2	Occupancy Measure
Given initial state distribution α(s), policy π(a∣s) and transition T(s0∣s, a), the normalized occu-
pancy measure ρTα,π(s, a) generated by (α, π, T) is defined as
∞	H-1
PfaK(S,a) = X(1-γ)γtP(st = s, at = a∣α,π,T) = (1 -γ) X P(St = s,at = a∣α,π,T),⑵
t=0	t=0
where P(∙) is the probability measure and will be replaced by a density function if S or A is Con-
tinuous. Intuitively, PTα,π(S, a) is a distribution of (S, a) in a length-H trajectory {St, at}tH=-01 with
H 〜Geometric(1 - Y) following the laws of (α, ∏, T). From Syed et al. (2008), the relation be-
tween PTα,π and (α, π, T) is characterized by the Bellman flow constraint. Specifically, x = PTα,π as
defined in Eq. 2 is the unique solution to:
x(s,a) = π(a∣s)[(1 —
γ)α(S) + γ	x(S0
a0)T(S|S0, a0)dS0da0i,
x(S, a) ≥ 0.
(3)
In addition, Theorem 2 of Syed et al. (2008) gives that ∏(a∣s) and ρT,π(s, a) have an one-to-one
correspondence with α(s) and T(s0∣s, a) fixed; i.e., π(a∣s)，RP(S;京 is the only policy whose
occupancy measure is P.
With the occupancy measure, the cumulative reward Eq. 1 can be represented as
R(α,π,T) = E(s,a)〜pT，n[r(s,a)]/(1 - Y).	(4)
The goal of maximizing the cumulative reward can then be achieved by adjusting PTα,π, and this
motivates us to adopt distribution matching approaches like WGAN (Arjovsky et al., 2017) to learn
a transition model.
4	Theoretical Analysis for WGAN
In this section, we present a consistency result and error bounds for WGAN (Arjovsky et al., 2017).
All proofs of the following theorems and lemmas can be found in Appendix A.
In the setting of MBRL, the training objective for WGAN is
min !!maxi E(s,a)〜PT1s0 〜T (Ts,a)[f (S,a, S0)] - E(s,a)〜。输0% s0 〜T0(∙∣s,a)[f (S,a, S0)].	(5)
T kf kLip ≤1
By Kantorovich-Rubinstein duality (Villani, 2008), the optimal value of the inner maximization is
exactly W1(p(S, a, S0)||p0(S, a, S0)) where p(S, a, S0) = PTα,π(S, a)T(S0 |S, a) is the discounted distri-
bution of (S, a, S0). Thus, by minimizing over the choice of T0, we are essentially finding p0 that
minimizes W1(p(S, a, S0)||p0(S, a, S0)), which gives the consistency result.
Proposition 1 (Consistency for WGAN). Let T and T0 be the true and synthesized transitions
respectively. If WGAN is trained to its optimal point, we have
T(S0|S, a) = T0(S0|S, a), ∀(S, a) ∈ Supp(PTα,π),
where Supp(PTα,π) is the support of PTα,π.
The support constraint is inevitable because the training data is sampled from PT and guaranteeing
anything beyond it can be difficult. Still, we will empirically show that the support constraint is
not an issue in our experiments because the performance boosts up in the beginning, indicating that
Supp(PTα,π) may be large enough initially.
Now that training with WGAN gives a consistent estimate of the true transition, it is sensible to
train a synthesized transition upon it. However, the consistency result is too restrictive as it only
discusses the optimal case. The next step is to analyze the non-optimal situation and observe how
the cumulative reward deviates w.r.t. the training error.
4
Under review as a conference paper at ICLR 2020
Theorem 1 (Error Bound for WGAN). Let ρTα,π (s, a), ρTα,0π (s, a) be the normalized occupancy
measures generated by the true transition T and the synthesized one T0. If the reward function is
Lr-Lipschitz and the training error ofWGAN is G we have ∣R(∏, T) 一 R(π, T0) | ≤ eL"(1 — Y).
Theorem 1 indicates that if WGAN is trained properly, i.e., having small , the cumulative reward on
the synthesized trajectory will be close to that on the true trajectory. As MBRL aims to train a policy
on the synthesized trajectory, the accuracy of the cumulative reward over the synthesized trajectory
is thus the bottleneck. Theorem 1 also implies that WGAN’s error is linear to the (expected) length
of the trajectory (1 一 γ)-1. This is a sharp contrast to the error bounds in most RL literature, as the
dependency on the trajectory length is usually quadratic (Syed & Schapire, 2010; Ross et al., 2011),
or of an even higher order. Since WGAN gives us a better estimation of the cumulative reward in
the learned model, the policy update becomes more accurate.
5	Model Imitation for Model-Based Reinforcement Learning
In this section, we present a practical MBRL method called model imitation (MI) that incorporates
the transition learning mentioned in Section 4.
5.1	Sampling Technique for Transition Learning
Due to the long-term digression, it is hard to train the WGAN directly from a long synthesized
trajectory. To tackle this issue, we use the synthesized transition T0 to sample N short trajectories
with initial states sampled from the true trajectory.
To analyze this sampling technique, let β < γ be the discount factor of the short trajectories so
that the expected length is E[L] = (1 — β)-1. To simplify the notation, let PT0, PT, PT, PT be the
normalized occupancy measures of synthesized short trajectories, empirical true short trajectories,
true short trajectories and the true long trajectories. Notice both PβT0 and PβT are generated from the
same initial distribution PT. The 1-Wasserstein distance can be bounded by
WI(PT o ||pT) ≤ WI(PT o ||PT) + WI(PT IlPT) + WI(PT IlPT).
Wι(ρToIIpT) is upper bounded by the training error of WGAN on short trajectories, which can be
small empirically because the short ones are easier to imitate. Wι(^T IIpT ) = El [O((NL)-1/d)]=
O(((1  β)∕N)1/d/β) by Canas & Rosasco (2012) and Lemma 1, where d is the dimension of
(s, a). Wi(ρTIIpt) ≤ diam(S ×A)(1 — Y)β∕(γ — β) by Lemma 2 and Wi ≤ DTVdiam(S X A)
(Gibbs & Su, 2002), where diam(∙) is the diameter. The second term encourages β to be large while
the third term does the opposite. Besides, β need not be large if N is large enough; in practice we
may sample N short trajectories to reduce the error from Wi(PT0 IIPT) to Wi(PβT0 IIPT). Finally,
since PβT0 is the occupancy measure we train on, from the proof of Theorem 1 we deduce that
IR(∏,t) — R(∏,t0)I ≤ Wi(pToIIpt)Lr∕(i — Y).
Thus, WGAN may perform better under this sampling technique.
5.2	Empirical Transition Learning
To learn the real transition based on the occupancy measure matching mentioned in Section 4, we
employ a transition learning scheme by aligning the distribution of (s, a, s0) between the real and
the learned environments. Inspired by how GAIL (Ho & Ermon, 2016) learns to align (s, a) via
solving an MDP with rewards extracted from a discriminator, we formulate an MDP with rewards
from a discriminator over (s, a, s0). Specifically, the WGAN critic f(s, a, s0) in Eq. 5 is used as
the (psuedo) rewards r(s, a, s0) of our MDP. Interestingly, there is a duality between GAIL and our
transition learning: for GAIL, the transition is fixed and the objective is to train a policy to maximize
the cumulative pseudo rewards, while for our transition learning, the policy is fixed and the objective
is to train a synthesized transition to maximize the cumulative pseudo rewards.
In practice, since the policy is updated alternatively with the synthesized model, we are required
to train a number of WGANs along with the change of the policy. Although the generators across
5
Under review as a conference paper at ICLR 2020
WGANs correspond to the same transition and can be similar, we observe that WGAN may get
stuck at a local optimum when we switch from one WGAN training to another. The reason is
that unlike GAN that mimics the Jensen-Shannon divergence and hence its inner maximization is
upper bounded by log(2), WGAN mimics the Wasserstein distance and the inner maximization is
unbounded from above. Intuitively, such unboundedness makes the WGAN critic so strong that
the WGAN generator (the synthesized transition) cannot find a way out and get stuck at a local
optimum. Thereby, we have to modify the WGAN objective to alleviate such a situation. To ensure
the boundedness, for a fixed δ > 0, we introduce cut-offs at the WGAN objective so that the inner
maximization is upper bounded by 2δ :
mT0n Umaxl E(s,a)〜ρT,π[min(δ,f(s,a,s0)]+ E (s,a)〜ρ∕π [min(δ, -f (s,a,s0))].	(6)
kf kLiP≤1 s0 〜T (∙∣s,α)	s0 〜T 0(∙∣s,a)
As δ → ∞, Eq. 6 recovers the WGAN objective, Eq. 5. Therefore, this is a truncated version of
WGAN. To comprehend Eq. 6 further, notice that it is equivalent to
mTn max] E(s,a)〜PT，n [min(0,f (s, a, SO) - δ)] + E (s,a)〜ρT0π [min(0, -f (s, a, SO) - δ)]
T kf kLiP≤1 SJT (∙∣s,α)	s0 〜T0 (∙∣s,α)
⇔ mTi0n min E(s,a)〜pT，n [max(0, δ - f(s, a, SO))] + E (s,a)〜ρT0π [max(0, δ + f(s, a, SO))],
kf kLip≤	S0 〜T (∙∣s,α)	S0 〜T0(∙∣s,a)
(7)
which is a hinge loss version of the generative adversarial objective. Such WGAN is introduced in
Lim & Ye (2017), where the consistency result is provided and further experiments are evaluated in
Zhang et al. (2018). According to Lim & Ye (2017), the inner minimization can be interpreted as
the soft-margin SVM. Consequently, it provides a geometric intuition of maximizing margin, which
potentially enhances robustness. Finally, because the objective of transition learning is to maximize
the cumulative pseudo rewards on the MDP, T O does not directly optimize Eq. 7. Note that the
truncation only takes part in the inner minimization:
min E(s,a)〜pT，n [max(0,δ- f(s,a,s0))]+ E (s,a)〜。疗[max(0, δ + f (s,a,s0))],	(8)
kf kLiP≤1 S0 〜T (∙∣s,a)	S0 〜T 0(∙∣s,a)
which gives us a WGAN critic f(S, a, SO). As mentioned, f will be the pseudo reward function.
Later, we will introduce a transition learning version of PPO (Schulman et al., 2017) to optimize the
cumulative pseudo reward.
Algorithm 1 Model Imitation for Model-Based Reinforcement Learning
1:	Parameterize policy π, T, and WGAN critic f with θ, φ, and W respectively. Initialize an empty
environment dataset Denv
2:	for i = 0, 1, 2, ... do
3:	Take actions in real environment according to ∏θ; DenV J DenV ∪ Di
4:	Pre-train Tφ and fw by optimizing Eq. 8 and 11 with Di and Denv
5:	for N epochs do
6:	for ntransition epochs do
7:	optimize Eq. 8 and 11 over φ and w with Di
8:	end for
9:	for npolicy	epochs do
10:	update πθ by TRPO on the data generated by Tφ
11:	end for
12:	end for
13:	end for
After modifying the WGAN objective, to include both the stochastic and (approximately) deter-
ministic scenarios, the synthesized transition is modeled by a Gaussian distribution TO(SO|S, a) =
Tφ(s0|s, a)〜 N(μφ(s, a), Σφ(s, a)). Although the underlying transitions of tasks like MUJoCo
(Todorov et al., 2012) are deterministic, modeling by a Gaussian does not harm the transition learn-
ing empirically.
Recall that the synthesized transition is trained on an MDP whose reward function is the critic of
the truncated WGAN. To achieve this goal with proper stability, we employ PPO (Schulman et al.,
6
Under review as a conference paper at ICLR 2020
EnaaJαj6e-llu>e
O 5 IO 15	20	25	30	35	«
thousand steps
Ant
O 10 a 30	«	50	60	70	80
thousand steps
Reacher
O	20	«	60	80	IOO	O	10	20	30	«	50	W
thousand steps	thousand steps
Ml (Ours)	PETS	METRPO ——STEVE	SLBO	PPO	TRPO
Figure 2: Learning curves of our MI versus two model-free and four model-based baselines. The
solid lines indicate the mean of five trials and the shaded regions suggest standard deviation.
2017), which is an efficient approximation of TRPO (Schulman et al., 2015). Note that although the
PPO is originally designed for policy optimization, it can be adapted to transition learning with a
fixed sampling policy and the PPO objective (Eq. 7 of Schulman et al. (2017))
LPPO (φ) = Et [ mm(rt(φ)At, clip(rt(φ), 1 - e, 1 + e)At)j,	(9)
where
Tjφ _ Tφ(St+1|st, Ot)
rt(φ) = Tφoid (st+ι∣st,at)
At : advantage func. derived from the pseudo reward f (st, at, st+ι).
(10)
To enhance stability of the transition learning, in addition to PPO, we also optimize maximum
likelihood, which can be regarded as a regularization. We empirically observe that jointly optimizing
both maximum likelihood and the PPO objective attains better transition model for policy gradient.
The overall loss of the transition learning becomes
Ltransition = -LPPO + αLmle ,
(11)
where Lmle is the loss of MLE, which is policy-agnostic and can be estimated with all collected real
transitions. For more implementation details, please see Appendix B.1.
We consider a training procedure similar to SLBO (Luo et al., 2019), where they consider the fact
that the value function is dependent on the varying transition model. As a result, unlike most of the
MBRL methods that have only one pair of model-policy update for each real environment sampling,
SLBO proposes to take multiple update pairs for each real environment sampling. Our proposed
model imitation (MI) method is summarized in Algorithm 1.
6	Experiments
In the experiment section, we would like to answer the following questions. (1) Does the proposed
model imitation outperforms the state-of-the-art in terms of sample complexity and average return?
(2) Does the proposed model imitation benefit from distribution matching and is superior to its
model-free and model-based counterparts, TRPO and SLBO?
To fairly compare algorithms and enhance reproducibility, we adopt open-sourced environments re-
leased along with a model-based benchmark paper (Wang et al., 2019), which is based on a physical
simulation engine, MuJoCo (Todorov et al., 2012). Specifically, we evaluate the proposed algorithm
MI on four continuous control tasks including Hopper, HalfCheetah, Ant, and Reacher. For hyper-
parameters mentioned in Algorithm 1 and coefficients such as entropy regularization λ, please refer
to Appendix B.2.
7
Under review as a conference paper at ICLR 2020
Table 1: Proportion of bench-marked RL methods that are inferior to MI in terms of 5% t-test.
x/y indicates that among y approaches, MI is significantly better than x approaches. The detailed
performance can be found in Table 1 of Wang et al. (2019). It should be noted that the reported
results in Wang et al. (2019) are the final performance after 200k time-steps whereas ours are no
more than 100k time-steps.
	Hopper	HalfCheetah	Ant	Reacher
MBRL	8/10	IgTIg	-8TIg^	-8TIg-
MFRL	3/4	2/4	4/4	3/4
We compare to two model-free algorithms, TRPO (Schulman et al., 2015) and PPO (Schulman
et al., 2017), to assess the benefit of utilizing the proposed model imitation since our MI (Algo-
rithm 1) uses TRPO for policy gradient to update the agent policy. We also compare MI to four
model-based methods. SLBO (Luo et al., 2019) gives theoretical guarantees of monotonic improve-
ment for model-based deep RL and proposes to update a joint model-policy objective. PETS (Chua
et al., 2018) propose to employ uncertainty-aware dynamic models with sampling-based uncertainty
to capture both aleatoric and epistemic uncertainty. METRPO (Kurutach et al., 2018) shows that
insufficient data may cause instability and propose to use an ensemble of models to regularize the
learning process. STEVE (Buckman et al., 2018) dynamically interpolates among model rollouts of
various horizon lengths and favors those whose estimates have lower error.
Figure 2 shows the learning curves for all methods. In Hopper, HalfCheetah, and Ant, MI converges
fairly fast and learns a policy significantly better than competitors’. In Ant, even though MI does not
improve the performance too much from the initial one, the fact that it maintains the average return
at around 1,000 indicates that MI can capture a better transition than other methods do with only
5,000 transition data. Even though we do not employ an ensemble of models, the curves show that
our learning does not suffer from high variance. In fact, the performance shown in Figure 2 indicates
that the variance of MI is lower than that of methods incorporating ensembles such as METRPO and
PETS.
The questions raised at the beginning of this section can now be answered. The learned model
enables TRPO to explore the world without directly access real transitions and therefore TRPO
equipped with MI needs much fewer interactions with the real world to learn a good policy. Even
though MI is based on the training framework proposed in SLBO, the additional distribution match-
ing component allows the synthesized model to generate similar rollouts to that of the real environ-
ments, which empirically gives superior performance because we rely on long rollouts to estimate
policy gradient.
To better understand the performance presented in Figure 2, we further compare MI with bench-
marked RL algorithms recorded in Wang et al. (2019) including state-of-the-art MFRL methods
such as TD3 (Fujimoto et al., 2018) and SAC (Haarnoja et al., 2018). It should be noted that the
reported results of Wang et al. (2019) are the final performance after 200k time-steps but we only
use up to 100k time-steps to train MI. Table 1 indicates that MI significantly outperforms most of the
MBRL and MFRL methods with 50% fewer samples, which verifies that MI is more sample-efficient
by incorporating distribution matching.
7	Conclusion
We have pointed out that the state-of-the-art methods concentrate on learning synthesized models
in a supervised fashion, which does not guarantee that the policy is able to reproduce a similar
trajectory in the learned model and therefore the model may not be accurate enough to estimate long
rollouts. We have proposed to incorporate WGAN to achieve occupancy measure matching between
the real transition and the synthesized model and theoretically shown that matching indicates the
closeness in cumulative rewards between the synthesized model and the real environment.
To enable stable training across WGANs, we have suggested using a truncated version of WGAN to
prevent training from getting stuck at local optimums. The empirical property of WGAN application
such as imitation learning indicates its potential to learn the transition with fewer samples than
supervised learning. We have confirmed it experimentally by further showing that MI converges
much faster and obtains better policy than state-of-the-art model-based and model-free algorithms.
8
Under review as a conference paper at ICLR 2020
References
Martin Arjovsky, Soumith Chintala, and Lon Bottou. Wasserstein gan, 2017.
Kavosh Asadi, Dipendra Misra, Seungchan Kim, and Michel L Littman. Combating the
compounding-error problem with a multi-step model. arXiv preprint arXiv:1905.13320, 2019.
Jacob Buckman, Danijar Hafner, George Tucker, Eugene Brevdo, and Honglak Lee. Sample-
efficient reinforcement learning with stochastic ensemble value expansion. In S. Bengio, H. Wal-
lach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural
Information Processing Systems 31, pp. 8224-8234. Curran Associates, Inc., 2018.
Guillermo D. Canas and Lorenzo A. Rosasco. Learning probability measures with respect to optimal
transport metrics. In Proceedings of the 25th International Conference on Neural Information
Processing Systems - Volume 2, NIPS’12, pp. 2492-2500, USA, 2012. Curran Associates Inc.
Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement
learning in a handful of trials using probabilistic dynamics models. In S. Bengio, H. Wallach,
H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Infor-
mation Processing Systems 31, pp. 4754-4765. Curran Associates, Inc., 2018.
Ignasi Clavera, Jonas Rothfuss, John Schulman, Yasuhiro Fujita, Tamim Asfour, and Pieter Abbeel.
Model-based reinforcement learning via meta-policy optimization. In Conference on Robot
Learning, pp. 617-629, 2018.
M. P. Deisenroth, D. Fox, and C. E. Rasmussen. Gaussian processes for data-efficient learning in
robotics and control. IEEE Transactions on Pattern Analysis and Machine Intelligence, 37(2):
408-423, Feb 2015.
Scott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error in
actor-critic methods. arXiv preprint arXiv:1802.09477, 2018.
Alison L. Gibbs and Francis Edward Su. On choosing and bounding probability metrics. Interna-
tional Statistical Review, 70(3):419-435, 2002.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Im-
proved training of wasserstein gans. In Advances in neural information processing systems, pp.
5767-5777, 2017.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-
policy maximum entropy deep reinforcement learning with a stochastic actor. arXiv preprint
arXiv:1801.01290, 2018.
Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In NeurIPS, pp. 4565-
4573, 2016.
Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-
based policy optimization. arXiv preprint arXiv:1906.08253, 2019.
Sanket Kamthe and Marc Peter Deisenroth. Data-efficient reinforcement learning with probabilistic
model predictive control. In AISTATS, 2017.
Andras Gabor Kupcsik, Marc Peter Deisenroth, Jan Peters, and Gerhard Neumann. Data-efficient
generalization of robot skills with contextual policy search. In Proceedings of the Twenty-Seventh
AAAI Conference on Artificial Intelligence, AAAI’13, pp. 1401-1407. AAAI Press, 2013.
Thanard Kurutach, Ignasi Clavera, Yan Duan, Aviv Tamar, and Pieter Abbeel. Model-ensemble
trust-region policy optimization. In International Conference on Learning Representations, 2018.
URL https://openreview.net/forum?id=SJJinbWRZ.
Jae Hyun Lim and Jong Chul Ye. Geometric gan. arXiv preprint arXiv:1705.02894, 2017.
Yuping Luo, Huazhe Xu, Yuanzhi Li, Yuandong Tian, Trevor Darrell, and Tengyu Ma. Algorithmic
framework for model-based deep reinforcement learning with theoretical guarantees. In Interna-
tional Conference on Learning Representations, 2019.
9
Under review as a conference paper at ICLR 2020
A. Nagabandi, G. Kahn, R. S. Fearing, and S. Levine. Neural network dynamics for model-based
deep reinforcement learning with model-free fine-tuning. In 2018 IEEE International Confer-
ence on Robotics andAutomation (ICRA),pp. 7559-7566, May 2018. doi: 10.1109/ICRA.2018.
8463189.
Stephane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and StrUc-
tured prediction to no-regret online learning. In Proceedings of the fourteenth international con-
ference on artificial intelligence and statistics, pp. 627-635, 2011.
Stefan Schaal. Learning from demonstration. In Advances in neural information processing systems,
pp. 1040-1046, 1997.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In ICML, pp. 1889-1897, 2015.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Richard S Sutton and Andrew G Barto. Introduction to Reinforcement Learning, volume 135. MIT
press, 1998.
Umar Syed and Robert E Schapire. A reduction from apprenticeship learning to classification. In
J. D. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. S. Zemel, and A. Culotta (eds.), Advances
in Neural Information Processing Systems 23, pp. 2253-2261. Curran Associates, Inc., 2010.
Umar Syed, Michael Bowling, and Robert E Schapire. Apprenticeship learning using linear pro-
gramming. In ICML, pp. 1032-1039, 2008.
Erik Talvitie. Self-correcting models for model-based reinforcement learning. In Thirty-First AAAI
Conference on Artificial Intelligence, 2017.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In IROS, pp. 5026-5033, 2012.
C Villani. Optimal transport - Old and new, volume 338,pp. xxii+973. 01 2008.
Tingwu Wang, Xuchan Bao, Ignasi Clavera, Jerrick Hoang, Yeming Wen, Eric Langlois, Shunshi
Zhang, Guodong Zhang, Pieter Abbeel, and Jimmy Ba. Benchmarking model-based reinforce-
ment learning, 2019.
David Wood. The computation of polylogarithms. Technical Report 15-92*, University of Kent,
Computing Laboratory, University of Kent, Canterbury, UK, June 1992.
Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena. Self-attention generative
adversarial networks. arXiv preprint arXiv:1805.08318, 2018.
10
Under review as a conference paper at ICLR 2020
A Proofs
A.1 Proof for WGAN
Proposition 1 (Consistency for WGAN). Let α(s), ∏(a∣s), T0(s0∣s, a) be initial state distribution,
policy and synthesized transition. Let T be the true transition, p(s, a, s0) = ρTα,π (s, a)T(s0|s, a) be
the discounted distribution of the triple (s, a, s0) under the true transition. If the WGAN is trained
to its optimal point, we have
T(s0|s, a) = T0(s0|s, a), ∀(s, a) ∈ Supp(ρTα,π).
Proof. Because the loss function of WGAN is the 1-Wasserstein distance, we know p(s, a, s0)
p0(s, a, s0) at its optimal points. Plug in to the Bellman flow constraint Eq. (3),
PTT(s, a) = ∏(a∣s) [(1 — γ)α(s) + Yl PTT(Sl a0)T0(s∣s0, a0)ds0da0i
=π(a∣s) [(1 — γ)α(s) + Y Jp0(s0, a0, s)ds0da0i
p=p π(a∣s)[(1 — γ)α(s)+ Y ∕p(s0, a0, s)ds,da] = ρT,π (s,a)
That is,
WGAN is opt. ⇔ p(s, a, s0) = p0(s, a, s0) Be⇒llman PTα,π (s, a) = PTα,0π (s, a).
Finally, recall p(s, a, s0) , PTα,π (s, a)T(s0|s, a) and p0(s, a, s0) , PTα,0π (s, a)T0(s0|s, a), we arrive
at
WGAN is opt. ⇒ T(s0|s, a) = T0(s0|s, a), ∀(s, a) ∈ Supp(ρTα,π).
□
Theorem 1 (Two-sided Errors for WGAN). Let PTα,π (s, a), PTα,0π (s, a) be normalized occupancy
measures generated by the true transition T and the synthesized one T0. Suppose the reward is
Lr-Lipschitz. Ifthe training error of WGAN is e, then ∣R(∏, T) — R(π, T0)∣ ≤ eL"(1 — Y).
Proof. Observe that the occupancy measure PTα,π (s, a) is a marginal distribution of p(s, a, s0) =
PTα,π(s, a)T(s0|s, a). Because the distance between the marginal is upper bounded by that of the
joint, we have
Wι(ρT,π(s, a)∣∣ρTθπ(s,a)) ≤ Wι(p(s, a, s0)∣∣p0(s, a, s0)) = e,
where W1 is the 1-Wasserstein distance. Then, the cumulative reward is bounded by
R(π,T) = 1-7-/ r(s, a)ρT,π(s,a)dsda = R(π, T0) +- / r(s, a) (PaK(s, a) — Pγ0(s, a))dsda
= R(π, T0) + Jr /	°，(pT,π(s, a) — PaO (s, a))dsda
≤ R(π,T0)+-----rr_ SuP	f f (s, a)(pT,π(s, a) — pT0π(s,a))dsda
1 — Y kfkLip≤1
=R(∏,T0) + YL- sup E(s,a)〜pT，a [f(s,a)] — E(s,a)〜ρπ,0α [f(s,a)]
1 — Y kfkLip≤1	T	T0
=R(∏,T0) + ILrYWι(pT,α∣∣pT'oα) ≤ R(∏,T0) + C占,
where the first inequality holds because r(s, a)/Lr is 1-Lipschitz and the last equality follows from
Kantorovich-Rubinstein duality Villani (2008). Since W1 distance is symmetric, the same conclu-
sion holds if we interchange T and T0, so we arrive at
∣R(∏,T) — R(∏,T0)| ≤ cLr∕(l — Y).
□
11
Under review as a conference paper at ICLR 2020
A.2 Lemmas for Sampling Techniques
Lemma 1. Let L 〜Geometric(1 一 β). If d ≥ 1, then E[L-1/d] = O((1 一 β)1"∕β).
Proof.
∞
E[L-1/T = X i-1/d(1 一 β)βi-1
i=1
1 - β S βi _ 1一 βτ∙
丁上 Hd = - Li1/d(e),
where Li is the polylogarithm function. From Wood (1992), the limiting behavior of it is
Liι∕d(e-μ) = Γ(1 一 1∕d)μ1∕dτ, as μ → 0+,
where Γ is the gamma function. Since e-μ → 1 一 μ when μ → 0+, We know when β → 1-,
Liι∕d(β) → Γ(1 一 1∕d)(1 — β)1∕d-1. Finally, since Γ(1 一 1∕d) ≤ 1, we conclude that E[L-1/d]=
O((1 - β)1∕d∕β).	□
Lemma 2. Let ρT (s, a) be a the normalized occupancy measure generated by the triple (α, π, T )
with discount factor γ. Let ρβT (s, a) be the normalized occupancy measure generated by the triple
(PT ,π,T) with discountfactor β. If Y > β, then DTV (PT ∣∣ρT) ≤ (1 — Y )β∕(γ — β).
Proof. By definition of the occupancy measure we have
∞
PT (s, a) =	(1 一 Y)Yifi(s, a).
i=0
∞i
PβT (s, a) = XX(1 一 Y)Yi-j(1 一 β)βjfi(s, a),
i=0 j=0
where fi(s, a) is the density of (s, a) at time i if generated by the triple (α, π, T). The TV distance
is bounded by
1 ∞	i	1 ∞	i	β j
DTV(PT||PT) ≤ 2 XI(I -Y)Yi- X(I -Y)Yi-j(I- β)βj I = 2 X(I — Y)Yi∣1 一 X(I 一 e) (Y) I
=1 X(I- Y)YiY-β 1 - β(1 - y)+ (Y)	(1 - e)Y|
i=0	Y 一	Y
(=) (1-Yβ MX1 -(1 -Y)Yi + (1 - β)βi = (1-Yβ (YM-BM
Y - β	i=o	Y - β
≤ (1- Y)β
_ Y - β .
where (*) comes from that -β(1 - y) + (Y)i(1 - B)y is a strictly decreasing function in i. Since
Y > β, its sign flips from + to - at some index; say M. Finally, the sum of the absolute value
are the same from PiM=-0 1 and from Pi∞=M because the total probability is conservative, and the
difference on one side is the same as that on the other.	□
B	Experiments
B.1	Implementation Details
We normalize states according to the statistics derived from the first batch of states from the real
world. To ensure stability, we maintain the same mean μo and standard deviation σ° throughout the
training process.
Instead of directly predicting the next state, we estimate the state difference st+1 - st (Kurutach
et al., 2018; Luo et al., 2019). Since we incorporate state normalization, the transition network is
trained to output (st+ι - St - μ0)∕σ0.
12
Under review as a conference paper at ICLR 2020
To enhance state exploration, We sample real transitions according to policy β 〜 N(μθ (s),σ),
where μ(s) is the mean of our Gaussian parameterized policy ∏θ and σ is a fixed standard deviation.
In addition, since model the transition as a Gaussian distribution, We found that matching ρTα,0πθ With
ρTα,β is empirically more stable and more sample-efficient than matching ρTα,0β with ρTα,β .
For policy update, it is shown that using the mean μφ of the Gaussian-parameterized transition can
accelerate policy optimization and better balance exploration and exploitation. In order to enforce
the Lipschitz constraint to the WGAN critic f, we employ gradient penalty (Gulrajani et al., 2017)
with weight 10.
B.2	Hyperparameters
	HaIfCheetah ∣ HoPPer ∣ ReaCher		Ant
N		10			
ɑ		1			10
ntransition		100			
	npolicy		20	I 60	100	30
horizon for model update	20	10	~30~
entropy regularization	0.001	0.005	
Table 2: List of hyper-parameters adopted in our experiments.
13