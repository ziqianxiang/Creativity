Under review as a conference paper at ICLR 2020
FleXOR: Trainable Fractional Quantization
Anonymous authors
Paper under double-blind review
Ab stract
Parameter quantization is a popular model compression technique due to its reg-
ular form and high compression ratio. In particular, quantization based on bi-
nary codes is gaining attention because each quantized bit can be directly utilized
for computations without dequantization using look-up tables. Previous attempts,
however, only allow for integer numbers of quantization bits, which ends up re-
stricting the search space for compression ratio and accuracy. Moreover, quanti-
zation bits are usually obtained by minimizing quantization loss in a local manner
that does not directly correspond to minimizing the loss function. In this paper,
we propose an encryption algorithm/architecture to compress quantized weights in
order to achieve fractional numbers of bits per weight and new compression con-
figurations further optimize accuracy/compression trade-offs. Decryption is im-
plemented using XOR gates added into the neural network model and described as
tanh(x), which enable gradient calculations superior to the straight-through gradi-
ent method. We perform experiments using MNIST, CIFAR-10, and ImageNet to
show that inserting XOR gates learns quantization/encrypted bit decisions through
training and obtains high accuracy even for fractional sub 1-bit weights.
1 Introduction
Deep Neural Networks (DNNs) demand larger number of parameters and more computations to
support various task descriptions all while adhering to ever-increasing model accuracy requirements.
Because of abundant redundancy in DNN models (Han et al., 2016; Frankle & Carbin, 2019; Cour-
bariaux et al., 2015) and limited scaling technology of integrated circuits due to physical challenges,
numerous model compression techniques are being aggressively studied to expedite inference of
DNNs across a range of device scales. As a practical model compression scheme, parameter quanti-
zation is a popular choice because compression ratio is high and regular formats after compression
enable full memory bandwidth utilization that is crucial in highly parallel computing systems.
Quantization schemes based on binary codes are gaining increasing attention since quantized
weights follow specific constraints to allow simpler computations during inference. Specifically,
using binary codes, a weight vector is represented as Piq=1(αi Pjv=1 bi,j), where q is the number
of quantization bits, v is the vector size, α is a scaling factor (α ∈ R), and bi,j is a binary code
(bi,j ∈ {-1, +1}). Then, a dot product with activations is conducted as Piq=1 (αi Pjv=1 aj bi,j),
where aj is a full-precision activation. Then, the number of multiplications is reduced from v to
q (expensive floating-point multipliers are less required for inference). Moreover, even though we
do not discuss a new activation quantization method in this paper, if activations are also quantized
by using binary codes, then most computations are replaced with bit-wise operations (using XNOR
logic and population counts) (Xu et al., 2018; Rastegari et al., 2016). Consequently, even though
representation space is constrained compared with quantization methods based on look-up tables,
various inference accelerators can be designed to exploit advantages of binary codes (Rastegari et al.,
2016; Xu et al., 2018). Since a successful 1-bit weight quantization method has been demonstrated
in BinaryConnect (Courbariaux et al., 2015), advances in compression-aware training algorithms in
the form of binary codes (e.g., binary weight networks (Rastegari et al., 2016) and LQ-Nets (Zhang
et al., 2018)) produce 1-3 bits for quantization while accuracy drop is modest or negligible. Fun-
damental investigations on DNN training mechanisms using fewer quantization bits have also been
actively reported to improve quantization quality (Li et al., 2017).
1
Under review as a conference paper at ICLR 2020
FIeXOR Components
T
Layer N+1
Encrypted
Weights (Binary)
Quantized
Weights
Encrypted
Weights (Binary)
XOR-gate Network
0‹≡
Layer N
Quantized
Weights
XOR-gate Network

÷
÷
÷
÷
↑
Figure 1:	FleXOR components added to the quantized DNNs to compress quantized weights through
encryption. Encrypted weight bits are decrypted by XOR gates to produce quantized weight bits.
Previously, binary-coding-based quantization has only permitted integer numbers of quantization
bits, limiting the compression/accuracy trade-off search space, especially in the range of very low
quantization bits. In this paper, we propose a flexible encryption algorithm/architecture (called
“FleXOR”) to enable fractional sub 1-bit numbers to represent each weight while quantizated bits are
trained by gradient descent. Even though vector quntization is also a well-known scheme with high
compression ratio (Stock et al., 2019), we assume the form of binary codes. Note that the number
of quantization bits can be different for each layer (e.g., Wang et al. (2019)) to allow fractional
quantization bits on average. FleXOR implies fractional quantization bits for each layer that can be
quantized with different bits.
The main contributions of this paper are as follows:
XOR-based encryption of quantized bits enhances compression ratio: We encrypt quantized
weights to further compress quantized DNNs applying basic principles from cryptography and dig-
ital communication.
XOR-aware training algorithm learns encrypted weights: A binary model of XOR gates is used
for forward propagation and model inference. We suggest calculating gradients of XOR gates using
tanh functions instead of using a straight-through estimator (STE) (Bengio et al., 2013) for efficient
compression-aware training.
High model accuracy with sub 1-bit quantization: To the best of our knowledge, our work is
the first to explore model accuracy under 1 bit/weight when weights are quantized based on binary
codes. Such an exploration plays a significant role for mobile devices where it may be possible to
sacrifice modest amounts of accuracy drop in exchange for improved energy efficiency.
2 Encrypting Quantized Bits using XOR Gates
The main purpose of FleXOR is to compress quantized bits into encrypted bits that can be recon-
structed by XOR gates as shown in Figure 1. Suppose that Nout bits are to be compressed into Nin
bits (Nout > Nin). The role of an XOR-gate network is to produce various Nout-bit combinations
using Nin bits. In other words, in order to maximize the chance of generating a desirable set of
quantized bits, the encryption scheme is designed to seek a particular property where all possible
2Nin outcomes through decryption are evenly distributed in 2Nout space.
A linear Boolean function, f (x), maps f : {0,1}Nin → {0,1} and has the form of aixi ㊉ a2χ2 ㊉
•一㊉ a N in XNin where a, ∈ {0,1} (1 ≤ j ≤ Nin) and ㊉ indicates bitwise modulo-2 addition. In
Figure 1, six binary outputs are generated through six Boolean functions using four binary inputs.
Letf1(x) and f2(x) be two such linear Boolean functions using x = (x1, x2, ..., xNin) ∈ {0, 1}Nin.
The Hamming distance between f1(x) and f2(x) is the number of inputs on which f1(x) and f2(x)
differ, and defined as
dH(f1,f2) := WH(fl ㊉ f2)= #{x ∈{0,1}Nin∣fl(x) = f2(x)},	(1)
where wH(f) = #{x ∈ {0, 1}Nin |f (x) = 1} is the Hamming weight of a function and #{} cor-
responds to the size of a set (Kahn et al., 1988). The Hamming distance is a well-known method to
express non-linearity between two Boolean functions (Kahn et al., 1988) and increased Hamming
2
Under review as a conference paper at ICLR 2020
匚NoUt bits I NoUt bits ∣ NoUt bits ∣ NoUt bitsm.匚NoUt bitsm
Scaling Factor (for Binary-Code DeqUantizatiOn)
i J ∖J∕ J	i
FUll-Precision Weights
φ
Reshaping (2D for FC, 4D for Conv)
Slices of Encrypted Weights
One XOR-gate Network is
Temporally- or Spatially-Shared
Slices of QUantized Weights
Figure 2:	Encrypted weight bits are sliced and reconstructed by a XOR-gate network which can be
shared (in time or space). Then quantized bits after XOR gates are finally reshaped.
distance between a pair of two Boolean functions results in a variety of outputs produced by XOR
gates. Increasing Hamming distance is a required feature for cryptography to derive complicated en-
cryption structure such that inverting encrypted data becomes difficult. For digital communication,
the Hamming distance between encoded signals is closely related to the amount of error correction
possible.
In Figure 1, yι is represented as xi ㊉ χ3 ㊉ χ4, or equivalently a vector [1011] denoting which
inputs are selected. Concatenating such vectors, a XOR-gate network in Figure 1 can be described
as a binary matrix M㊉ ∈ {0, ι}Nout ×Nin (e.g., the second row of M㊉ is [110 0] and the third row
is [1110]). Then, decryption through XOR gates is simply represented as y = M㊉X where X and
y are the binary inputs and binary outputs of XOR gates, and addition is ‘XOR’ and multiplication
is ‘AND’ (see Appendix for more details and examples).
Encrypted weight bits are stored in 1-dimensional vector format and sliced into blocks of Nin-bit
size as shown in Figure 2. Then, decryption of each slice is performed by a XOR-gate network that
is shared by all slices (temporally- or spatially-shared). Depending on the quantization scheme and
characteristics of layers, quantized bits may need to be scaled by a scaling factor and/or reshaped.
3	FleXOR Training Algorithm for Quantization Bits Decision
Once the structure of XOR gates has been pre-determined and fixed to increase the Hamming dis-
tance of XOR outputs, we find quantized and encrypted bits by adding XOR gates into the model. In
other words, we want an optimizer that understands the XOR-gate network structure so as to com-
pute encrypted bits and scaling factors via gradient descent. Note that for inference, we store binary
encrypted weights (converted from real number encrypted weights) in memory and generate binary
quantized weights through Boolean XOR operations. Activation quantization is not discussed in this
paper.
Similar to the STE method introduced in (Courbariaux et al., 2015), Boolean functions need to
be described in a differentiable manner to obtain gradients in backward propagation. For two real
number inputs x1 and x2 (x1 , x2 ∈ R to be used as encrypted weights), the Boolean version of a
XOR gate for forward propagation is described as (note that 0 is replaced with -1)
F ㊉(x1,x2) = (-1)Sign(X1)sign(x2).	(2)
For inference, we store sign(x1) and sign(x2) instead of x1 and x2. On the other hand, a differen-
tiable XOR gate for backward propagation is presented as
f ㊉(X1,X2)= (-1)tanh(xi ∙ Stanh) tanh(x2 ∙ Stanh),	(3)
where Stanh is a scaling factor for FleXOR. Note that tanh functions are widely used to approximate
Heaviside step functions (i.e., y(x) = 1 if x > 0 or 0, otherwise) in digital signal processing
and Stanh can control the steepness (as explained in Appendix). In the case of consecutive XOR
operations, the order of inputs to be fed into XOR gates should not affect the computation of partial
3
Under review as a conference paper at ICLR 2020
gradients for XOR inputs. Therefore, as a simple extension of Eq. (3), a differentiable XOR gate
network with n inputs can be described as
/㊉ (x1, χ2 , . . . , Xn) = ( -1) tanh(χι ∙ Stanh) tanh(χ2 ∙ Stanh ) ... tanh(xn ∙ Stanh) .	(4)
Then, a partial derivative of f ㊉ with respect to Xi (an encrypted weight) is given as
f φ(χ1,χ2, ...,χn)	/	nn-1∕,	+	μ2∕	V	x×Qn=1	tanh(Xj	∙	Stanh)
-------λ--------- = Stanh(-1)n 1(1 - tanh (Xi ∙ Stanh))^~τ~,——ɑ——ʌ—	(5)
OXi	tanh(Xi ∙ Stanh)
Note that increasing the Hamming distance is associated with more tanh multiplications for each
XOR-gate network output. Then, from Eq. (5), we may suffer from the vanishing gradient problem
since | tanh(X)| ≤ 1. To resolve this problem, we also consider a simplified alternative partial
derivative expressed as
f(xι,∂2,…,xn) ≈ Stanh(-1)n-1(1 — tanh2(xi ∙ Stanh)) YSign(Xj).
i	j6=i
(6)
Eq. (6) shows that when we compute a partial derivative, all XOR inputs other than Xi are assumed
to be binary, i.e., the magnitude ofa partial derivative is then only determined by Xi. We use Eq. (6)
in this paper to calculate custom gradients of encrypted weights due to fast training computations
and convergence, and use Eq. (2) for forward propagation.
Algorithm 1: Pseudo codes of Conv Layer with FleXOR1
We ∈ Rd(k∙k∙Cin∙Cout)∕Nout]∙Nin
. Encrypted weights
M㊉ ∈ {0, ι}Nout×Nin	. XOR gates (shared)
α ∈ RCout	. Scaling factors for each output channel
Function FleXOR_Conv(input, stride, padding):
for i J 0 to d(k ∙ k ∙ Cin ∙ Cout)/Noute - 1 do
for j — 1 to Nout do
/	∖
Nin
叫 Nout+j J (-1) ∙	Π Signc (WeNin+) Y-I)	. Eq.⑵
l=1
[L	∖Mk1
Wq J Reshape(Wq, [k, k,Cin,Cout])
return Conv(input, Wq, α, stride, padding)	. Convolution operation for binary codes
Forward Function Signc(x):
L return Sign(X)
Gradient Function Signc (x, V):
return V ∙ (1 — tanh2 (X ∙ Stanh)) ∙ Stanh
. Eq. (6)
By training the whole network including FleXOR components with using custom gradient compu-
tation methods described above, encrypted and quantized weights are obtained in a holistic manner.
FleXOR operations for convolutional layers are described in Algorithm 1, where encrypted weights
(inputs of a XOR-gate network) and quantized weights (outputs of a XOR-gate network) are We
and Wq . We first verify basic training principles of FleXOR using LeNet-5 on the MNIST dataset.
LeNet-5 consists of two convolutional layers and two fully-connected layers (specifically, 32C5-
MP2-64C5-MP2-512FC-10SoftMax), and each layer is accompanied by an XOR-gate network with
Nin binary inputs and Nout binary outputs. The quantization scheme follows 1-bit binary code with
full-precision scaling factors that are shared across weights for the same output channel number (for
conv layers) or output neurons (for FC layers). Encrypted weights are randomly initialized with
N (μ=0, σ2=0.0012). All scaling factors for quantization are initialized to be 0.1 (note that if batch
normalization layers are immediately followed, then scaling factors for quantization are redundant).
1Kernel size is k × k, the number of input channel and output channel are Cin and Cout, respectively
4
Under review as a conference paper at ICLR 2020
Figure 3: Test accuracy and training loss (average of 6 runs) with LeNet-5 on MNIST when M㊉
is randomly filled with {0, 1}. Nout is 10 or 20 to generate, effectively, 0.4, 0.6, or 0.8 bit/weight
quantization.
q = 1,
Nin=18, Nout=20
q = 3, Nin=6, Nout=20
20 weights
20 bits 不	i
D	I
18 bits↑	：
(Encrypted)
0.9 bits/weight
(1-bit quantization)
N ∑ J<20 weights
⅛K-αι	—(x)z-α?	⅛K-α3
20 bits 不 20 bits 旱S 20 bits 不
DDD
6(Ensited) 6(EnsC[pted) 6(Enijpted)
0.9 bits/weight
(3-bit quantization)
Figure 4:	Using the same weight storage footprint, FleXOR enables various internal quantization
schemes. (Left): 1-bit internal quantization. (Right): 3-bit internal quantization with 3 different
M㊉ configurations.
Using the Adam optimizer with an initial learning rate of 10-4 and batch size of 50 without dropout,
Figure 3 shows training loss and test accuracy when Stanh=100, elements of M㊉ are randomly
filled with 1 or 0, and for two values of Nout — 10 and 20. Using the 1-bit internal quantization
method and (Nin , Nout) encryption scheme, one weight can be represented by (Nin/Nout) bits.
Hence, Figure 3 represents training results for 0.4, 0.6, and 0.8 bits per weight. Note that as for a
randomly filled M㊉，increasing Nout (and Nin correspondingly for the same compression ratio)
increases the Hamming distance for a pair of any two rows of M㊉ and, hence, offers the chance
to produce more diversified outputs. Indeed, as shown in Figure 3, the results for Nout=20 present
improved test accuracy and less variation compared with Nout=10. See Appendix for the distribution
of encrypted weights at different training steps.
4 Practical FleXOR Training Techniques
In this section, we present practical training techniques for FleXOR using ResNet-32 (He et al.,
2016) on the CIFAR-10 dataset (Krizhevsky et al., 2009). As shown in Figure 4, since FleXOR
decouples the number of bits to represent weights from the quantization scheme used, various trade-
offs between memory footprint and model accuracy are possible. We show compression results for
ResNet-32 using fractional numbers as effective quantization bits, such as 0.4 and 1.2, that have not
been available previously.
All layers, except the first and the last layers, are followed by FleXOR components sharing the same
M㊉ structure (thus, storage foorprint of M㊉ is ignorable). SGD optimizer is used with momentum
of 0.9 and a weight decay factor of 10-5. Initial learning rate is 0.1, which is decayed by 0.5 at
the 150th and 175th epoch. As learning rate decays, Stanh is empirically multiplied by 2 to cancel
5
Under review as a conference paper at ICLR 2020
Ooo
9 8 7
(N) XɔBJnɔɔv +s①I
O
Epoch
	XOR Training Method		
	STE	Analog XOR	FleXOR
Forward	sign	tanh	sign
Backward	Identity	∂(tanh)	∂(tanh)
XOR Output	Binary -1 or +1	R (T +I)	Binary -1 or +1
Figure 5:	Test accuracy comparison on ResNet-32 (for CIFAR-10) using various XOR training
methods. Nout = 10, Nin = 8, q = 1 (thus, 0.8bit/weight), and Stanh = 10.
Epoch	Encrypted Weight Value
Figure 6:	Test accuracy and distribution of encrypted weights (at the end of training) of ResNet-32
on CIFAR-10 using various Stanh and the same Nout , Nin , and q as Figure 5.
out the effects of weight decay on encrypted weights. Batch size is 128 and scaling factors of α are
randomly initialized between 0.01 and 0.19. q is the number of bits to represent binary codes for
quantization. We provide some useful training insights below with relevant experimental results.
1)	Use small Ntap (such as 2): FleXOR should be able to select the best out of 2Nin possible
outputs that are randomly selected from larger 2Nout search space. Encryption performance of XOR
gates is determined by randomness of 2Nin output candidates, and is enhanced by increasing Ham-
ming distance that is basically achieved by large Nout . Now, let Ntap be the number of 1’s in a row
of M㊉.Another method to enhance encryption performance is to increase Ntap So as to increase
the number of shuffles (through more XOR operations) using encrypted bits to generate quantized
bits such that correlation between quantized bits is reduced. Large Ntap, however induces vanishing
gradient problems in Eq. (5) or increased approximation error in Eq. (6). Hence, in practice, FleXOR
training with small Ntap converges well with high test accuracy. Studying a training algorithm to
understand a complex XOR-gate network with large Ntap would be an interesting research topic
that is beyond the scope of this work. Subsequently, we show experimental results using Ntap = 2
in the remainder of this paper.
2)	Use ‘tanh’ rather than STE: Since forward propagation for a XOR gate only needs a sign
function, the STE method is also applicable to XOR-gate gradient calculations. Another alternative
method to model an XOR gate is to use Eq. (3) for both forward and backward propagation as if
the XOR is modeled in an analog manner (then, real number XOR outputs are quantized through
STE). We compare three different XOR modeling schemes in Figure 5 with test accuracy measured
when encrypted weights and XOR gates are converted to be binary for inference. FleXOR training
method shows the best result because a) sign function for forward propagation enables estimating the
impact of binary XOR computations on the loss function and b) ∂(tanh) for backward propagation
approximates the Heaviside step function better compared to STE. Note that limited gradients from
the tanh function eliminate the need for weight clipping, which is often required for quantization-
aware training schemes (Courbariaux et al., 2015; Xu et al., 2018).
3)	Optimize Stanh: Stanh controls the smoothness of the tanh function for near-zero inputs. Large
Stanh employs large gradient for small inputs and, hence, results in well-clustered encrypted weight
6
Under review as a conference paper at ICLR 2020
80
0
2 0 8 6 4 2
9 9 8 8 8 8
(N)AOBJn。。VK①I
100	200	300	400	500
Epoch
88
3 2 10 9
9 9 9 9 8
(wɔfnɔɔvəɪ
0.5	1.0	1.5	2.0
Number of Bits / Weight
Figure 7: Test accuracy of ResNet-32 on CIFAR10 using learning rate warmup and various q, Nin ,
and Nout . The results on the right side are obtained by 5 runs.
Table 1: Weight compression comparison of ResNet-20 and ResNet-32 on CIFAR-10. For FleXOR,
we use warmup scheme, Stanh=10, and Nout=20. All of quantization schemes in the table follow the
form of binary codes with q=1, and hence, the amount of computations becomes the same. FleXOR,
however, saves on/off-chip memory requirements further becasue Nin/Nout is less than 1.
		 ReSNet-20 					 ReSNet-32 			
	FP	Compressed	Diff.	FP	Compressed	Diff.
BWN (1 bit)	92.68%2	-87.44%-	-5.24%	93.40%2	-89.49%-	-4.51%
BinaryRelax (1 bit)	92.68%2	87.82%	-4.86%	93.40%2	90.65%	-2.80%
LQ-Net (1 bit)	92.10%3	90.10%	-1.90%	-	-	-
-FleXOR (1.0 bit)		-90.44%-	-1.47%		-91.36%-	-0.97%
FleXOR (0.8 bit)	91.87%4	89.71%	-2.10%	92.33%4	90.78%	-1.55%
FleXOR (0.6 bit)		89.04%	-2.83%		90.30%	-2.03%
FleXOR (0.4 bit)		87.85%	-4.02%		89.61%	-2.72%
values as shown in Figure 6. Too large of a Stanh , however, hinders encrypted weights from being
finely-tuned through training. For FleXOR, Stanh is a hyper-parameter to be optimized.
4)	Learning rate and Stanh warmup: Learning rate starts from 0 and linearly increases to reach
the initial learning rate at a certain epoch as a warmup. Learning rate warmup is a heuristic scheme,
but being widely accepted to improve generalization capability mainly by avoiding large learning
rate in the initial phase (He et al., 2018; Gotmare et al., 2019). Similarly, Stanh starts from 5 to
linearly increases to 10 using the same warmup schedule of the learning rate.
5)	Try various q, Nin, and Nout: Using a warmup scheme for 100 epochs and learning rate
decay by 50% at the 350th, 400th, and 450th epoch, Figure 7 presents test accuracy of ResNet-32
with various q, N^, and Nout. For q>1, different M㊉ configurations are constructed and then
shared across all layers. Note that even for the 0.4bit/weight configuration (using q=1, Nin=8, and
Nout=20), high accuracy close to 89% is achieved. 0.8bit/weight can be achieved by two different
configurations (as shown on the right side of Figure 7) using (q=1, Nin=8, Nout=10) or (q=2,
Nin=8, Nout=20). Interestingly, those two configurations show almost the same test accuracy,
which implies that FleXOR is able to provide a linear relationship between the amount of encrypted
weights and model accuracy (regardless of internal configurations). In general, lowering q reduces
the amount of computations with quantized weights and a high Nout value is necessary for very low
number of bits per weight since Nin also needs to be high enough to increase the Hamming distance.
We compare quantization results of ResNet-20 and ResNet-32 on CIFAR-10 using different com-
pression schemes in Table 1 (with full-precision activation). BWN (Rastegari et al., 2016), Bina-
ryRelax (Yin et al., 2018), and LQ-Net (Zhang et al., 2018) propose different training algorithms
2Reported by BinaryRelax paper (Yin et al., 2018)
3Reported by LQ-Net paper (Zhang et al., 2018)
4Based in https://github.com/akamaster/pytorch-resnet-cifar10, where We replaced the ReLU functions with
leaky ReLU functions.
7
Under review as a conference paper at ICLR 2020
Table 2: Weight compression comparison of ResNet-18 on ImageNet using various compression
schemes. All of quantization schemes in the table follow the form of binary codes with q=1, and
hence, the amount of computations becomes the same. FleXOR, however, saves on/off-chip memory
requirements further becasue Nin/Nout is less than 1.
Methods		Bits/Weight	Top-1	Top-5	Storage Saving
Full Precision (He et al., 2016)	32	69.6%	89.2%	1×
BWN (Rastegari et al., 2016)	1	60.8%	83.0%	^32×
ABC-Net (Lin et al., 2017)	1	62.8%	84.4%	^32×
BinaryReIax (Yin et al., 2018)	1	63.2%	85.1%	^32×
	08	63.8%	84.8%	^40×
FIeXOR (Nout = 20)	0.6	62.0%	83.7%	〜53×
	0.5	61.3%	83.1%	〜64×
for the same quantization scheme (i.e., binary codes). The main idea of these methods is to mini-
mize quantization error and to obtain gradients from full-precision weights while the loss function
is aware of quantization. Because all of quantization schemes in Table 1 uses q=1 and binary codes,
the amount of computations using quantized weights is the same. FleXOR, however, allows re-
duced memory footprint and bandwidth which are critical to improve energy efficiency of inference
engine designs (Han et al., 2016; Ahn et al., 2019). Note that even though achieving the best ac-
curacy for 1.0bit/weight is not the main purpose of FleXOR (e.g., XOR gate may be redundant for
Nin=Nout), FleXOR shows the minimum accuracy drop for ResNet-20 and ResNet-32 as shown in
Table 1, probably because a) FleXOR enables model optimization integrated with quantization and
encyption units instead of considering the local quantization error and b) FleXOR calculates gra-
dients for encrypted weights by employing ‘tanh’ (instead of STE) that approximates a Heaviside
step function.
5	Experimental Results on ImageNet
In order to show that FleXOR principles can be extended to larger models, we choose ResNet-18
on ImageNet (Russakovsky et al., 2015). We use SGD optimizer with momentum of 0.9 and initial
learning rate of 0.1. Batch size is 128, weight decay factor is 10-5, and Stanh is 10. Learning rate
is reduced by half at the 70th, 100th, and 130th. For warmup, during initial ten epochs, Stanh and
learning rate increase linearly from 5 and 0.0, respectively, to initial values.
Table 2 shows the comparison on model accuracy of ResNet-18 when weights are compressed by
quantization (and additional encryption by FleXOR) while activations maintain full precision. Train-
ing the entire model for ImageNet including FleXOR components is successfully performed. In
Table 2, BinaryRelax and BWN do not modify the underlying model architecture, while ABC-Net
introduces a new block structure of the convolution for quantized network designs. FleXOR achieves
the best top-1 accuracy even with only 0.8bit/weight and demonstrates improvement of model accu-
racy as the number of bits per weight increases. Refer to Appendix for the accuracy graph with q=1
and more results with q=2.
6	Conclusion
This paper proposes an encryption algorithm/architecture, FleXOR, as a framework to further com-
press quantized weights. Encryption is designed to produce more outputs than inputs by increasing
the Hamming distance of output functions when output functions are linear functions of inputs. Out-
put functions are implemented as a combination of XOR gates which are included in the model to
find encrypted and quantized weights through gradient descent while using the tanh function for
backward propagation. FleXOR is able to provide fractional numbers of bits for weights and, thus,
much wider trade-offs between weight storage and model accuracy. Experimental results show that
ResNet on CIFAR-10 and ImageNet can be represented by sub 1-bit/weight compression with high
accuracy.
8
Under review as a conference paper at ICLR 2020
References
Daehyun Ahn, Dongsoo Lee, Taesu Kim, and Jae-Joon Kim. Double Viterbi: Weight encoding for
high compression ratio and fast on-chip reconstruction for deep neural network. In International
Conference on Learning Representations (ICLR), 2019.
Yoshua Bengio, Nicholas Lonard, and Aaron Courville. Estimating or propagating gradients through
stochastic neurons for conditional computation. arXiv:1308.3432, 2013.
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. BinaryConnect: Training deep neural
networks with binary weights during propagations. In Advances in Neural Information Processing
Systems,pp. 3123-3131, 2015.
Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural
networks. In International Conference on Learning Representations (ICLR), 2019.
Akhilesh Gotmare, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. A closer look at deep
learning heuristics: Learning rate restarts, warmup and distillation. In International Conference
on Learning Representations (ICLR), 2019.
Song Han, Huizi Mao, and William J. Dally. Deep compression: Compressing deep neural networks
with pruning, trained quantization and Huffman coding. In International Conference on Learning
Representations (ICLR), 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770-
778, 2016.
Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, and Mu Li. Bag of tricks to train
convolutional neural networks for image classification. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2018.
Sangil Jung, Changyong Son, Seohyung Lee, Jinwoo Son, Jae-Joon Han, Youngjun Kwak, Sung Ju
Hwang, and Changkyu Choi. Learning to quantize deep networks by optimizing quantization
intervals with task loss. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 4350-4359, 2019.
J. Kahn, G. Kalai, and N. Linial. The influence of variables on boolean functions. In Proceedings
of the 29th Annual Symposium on Foundations of Computer Science, SFCS ’88, pp. 68-80, 1988.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
Technical report, Citeseer, 2009.
Fengfu Li and Bin Liu. Ternary weight networks. arXiv:1605.04711, 2016.
Hao Li, Soham De, Zheng Xu, Christoph Studer, Hanan Samet, and Tom Goldstein. Training
quantized nets: A deeper understanding. In Advances in Neural Information Processing Systems,
pp. 5813-5823, 2017.
Xiaofan Lin, Cong Zhao, and Wei Pan. Towards accurate binary convolutional neural network. In
Advances in Neural Information Processing Systems, pp. 345-353, 2017.
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. XNOR-Net: Imagenet
classification using binary convolutional neural networks. In ECCV, 2016.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.
ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision
(IJCV), 115(3):211-252, 2015. doi: 10.1007/s11263-015-0816-y.
Pierre Stock, Armand Joulin, Rmi Gribonval, Benjamin Graham, and Herv Jgou. And the bit goes
down: Revisiting the quantization of neural networks. arXiv:1907.05686, 2019.
9
Under review as a conference paper at ICLR 2020
Kuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, and Song Han. Haq: Hardware-aware automated quan-
tization with mixed precision. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 8612-8620, 2019.
Chen Xu, Jianqiang Yao, Zhouchen Lin, Wenwu Ou, Yuanbin Cao, Zhirong Wang, and Hongbin
Zha. Alternating multi-bit quantization for recurrent neural networks. In International Conference
on Learning Representations (ICLR), 2018.
Penghang Yin, Shuai Zhang, Jiancheng Lyu, Stanley Osher, Yingyong Qi, and Jack Xin. Bina-
ryrelax: A relaxation approach for training deep neural networks with quantized weights. SIAM
Journal on Imaging Sciences, 11(4):2205-2223, 2018.
Dongqing Zhang, Jiaolong Yang, Dongqiangzi Ye, and Gang Hua. Lq-nets: Learned quantization for
highly accurate and compact deep neural networks. In Proceedings of the European Conference
on Computer Vision (ECCV), pp. 365-382, 2018.
Chenzhuo Zhu, Song Han, Huizi Mao, and William J. Dally. Trained ternary quantization. In
International Conference on Learning Representations (ICLR), 2017.
10
Under review as a conference paper at ICLR 2020
A Appendix
A.1 Example of a XOR-gate Network Structure Representation
In Figure 1, outputs of a XOR-gate network are given as
yi = Xi ㊉ X3 ㊉ X4
y = xi ㊉ X2
y = Xi ㊉ X2 ㊉ X3
y4 = X3 ㊉ X4
y = X2 ㊉ X4
y = X2 ㊉ X3 ㊉ X4.
Equivalently, the same structure as above can be represented in a matrix as
	1	0	1	1	
	1	1	0	0	
M㊉=	1 0	1 0	1 1	0 1	.	(7)
	0	1	0	1	
	0	1	1	1	
Note that elements of M㊉ are matched with coefficients of y%(1 ≤ i ≤ 6). For two vectors y =
{yi,y2,y3,y4,y5,y6} and x = {Xi,X2,X3,X4}, the following equation holds:
y = M㊉∙ x,	(8)
where element-wise addition and multiplication are performed by ‘XOR’ and ‘AND’ function, re-
spectively. In Eq. (7), Ntap (i.e., the number of ‘1’s in a row) is 2 or 3.
A.2 Supplementary Data for Basic FleXOR Training Principles
A Boolean XOR gate can be modeled as F㊉(xi,X2)= (-1) Sign(Xi) Sign(X2) if 0 is replaced with
-1 as shown in Table 3.
Sign(Xi)	Sign(X2)	F ㊉(Xi,X2)
-1	-1	-1
-1	+1	+1
+1	-1	+1
+1	+1	-1
Table 3: An XOR gate modeling using F㊉(χi,χ2).
In Eq. (7), forward propagation for y3 is expressed as
y = F㊉(xi,x2,x3) = (-1)2 Sign(Xi)Sign(x2) Sign(X3).
while partial derivative of y3 with respect to Xi is given as (not derived from Eq. (9))
∂y3
∂Xi
Stanh(-1)2(1 - tanh2(Xi ∙ Stanh)) tanh(X2 ∙ Stanh) tanh(X3 ∙ Stanh),
or as
∂y3
∂Xi
≈ Stanh(-1)2(1 - tanh2(Xi ∙ Stanh))Sign(x2) Sign(X3).
(9)
(10)
(11)
We choose Eq. (11), instead of Eq. (10), as explained in Section 3.
As shown in Figure 8, large Stanh yields sharp transitions for near-zero inputs. Such a sharp approxi-
mation of the Heaviside step function produces large gradient values for small inputs and encourages
encrypted weights to be separated into negative or positive values. Too large Stanh, however, has
the same isseus ofa too large learning rate.
11
Under review as a conference paper at ICLR 2020
Figure 8: The left graph shows hyperbolic tangent (y = tanh(x ∙ Stanh)) graphs with various scaling
factors (Stanh), . The right graph shows their derivatives. These graphs support the arguments of
‘Optimize Stanh ’ in Section 4.
99.2-
98.8-
98.5-
98.1-
97.7-
^^I~J	ʃ	I	I	I^~0.00
0	5000	10000 15000 20000
Global Step
Figure 9: Test accuracy and training loss of LeNet-5 on MNIST when number of ‘1’s in each row of
M㊉ is fixed to be 2 (Ntap = 2). Nout is 10 or 20 to generate, effectively, 0.4, 0.6, or 0.8 bit/weight
quantization. With low Ntap of M㊉，MNIST training presents less variations on training loss and
test accuracy that in Figure 3.
Figure 9 presents training loss and test accuracy when Ntap = 2 and Nout is 10 or 20. Compared
with Figure 3, Ntap = 2 presents improved accuracy for the cases of high compression configura-
tions (e.g., Nin = 4 and Nout = 10). We use Ntap = 2 for CIFAR-10 and ImageNet, since low
Ntap avoids gradient vanishing problems or high approximation errors in Eq.(5) or Eq.(6).
Figure 10 plots the distribution of encrypted weights at different training steps when each row of
M㊉ is randomly assigned with {0,1} (i.e., Ntap is Nin/2 on average) or assigned with only two
1’s (Ntap=2). Due to gradient calculations based on tanh and high Stanh , encrypted weights tend
to be clustered on the the left or right (near-zero encrypted weights become less as Ntap increases)
even without weight clipping.
A.3 Supplementary Experimental Results of CIFAR-10 and ImageNet
In this subsection, we additionally provide various graphs and accuracy tables for ResNet models
on CIFAR10 and ImageNet. We also present experimental results from wider hyper-parameters
searches including q = 2 with two separate M㊉ configurations (with the same Nin and Nout for
two M㊉ matrices).
12
Under review as a conference paper at ICLR 2020
30k
25k-
aun。。
20k-
15k-
10k -
5k -
0k
-0.050 -0.025 0.000 0.025 0.050
Encrypted Weight Value
30k
aun。。
20k-
15k-
10k-
5k -
25k-
0k
-0.050 -0.025 0.000 0.025 0.050
Encrypted Weight Value
Figure 10:	Distribution of encrypted weight values for FC1 layer of LeNet-5 at different training
steps using Stanh = 100 and Nout = 10. (Left): M㊉ is randomly filled (Ntap ≈ Nin/2). (Right):
Ntap = 2 for every row of M ㊉.
300
300
250-
200-
3unoo
150-
100-
50-
0-
----Stanh = I
----Stanh =5
----Stanh =10
----Stanh =50
——Stanh = 100
-0.3	-0.2	-0.1	0.0	0.1	0.2	0.3
Encrypted Weight Value
3unoo
200-
150-
100-
50
250-
0
-0.3	-0.2	-0.1	0.0	0.1	0.2	0.3
Encrypted Weight Value
(a) First convolution layer in Layer1
Encrypted Weight Value
(b) Last convolution layer in Layer1
Encrypted Weight Value


(c)	First convolution layer in Layer2
(d)	Last convolution layer in Layer2
Figure 11:	Distributions of encrypted weights (at the end of training) in various layers of ResNet-
32 on CIFAR-10 using various Stanh and the same Nout, Nin, and q as Figure 5. The ResNet-32
network mainly consists of three layers according to the feature map sizes: Layer1, Layer2 and
Layer3.
13
Under review as a conference paper at ICLR 2020
(兴)Xɔe,mɔɔv^^ɪ
(a)	Initial Learning Rate (0.1): Test accuracy of ResNet-32 on CIFAR10 using the learning schedule in
Figure 5 and various initial learning rates (0.05, 0.1, 0.2, 0.5).
90
5 0 5
8 8 7
(兴)Xɔe,mɔɔv^ɪ
70
0	25	50	75	100	125	150	175	200
Epoch
(b)	No Weight Clipping: Test accuracy of ResNet-32 on CIFAR10 using the learning schedule in Figure 5. As
for weight clipping, we restrict the encrypted weights to be ranged as (-2.0/Stanh, +2.0/Stanh). As can be
observed, the red line implies that weight clipping is not effective with FleXOR.
65
(I—dol 寻)Xɔe,mɔɔv +S①H
60
55
50
45
40
no weight decay
our recipe with weight decay(10-5)
0	25	50	75	100	125	150	175
Epoch
(c)	Weight Decay Factor (10-5): Two graphs depict test accuracy of ResNet-18 on ImageNet with or without
weight decay. The learning rate in the red line (no weight decay) is reduced by half at the 100th, 130th and
150th epochs. The learning rate of the blue line (with weight decay) is reduced by half at 70th, 100th and 130th
epochs. With weight decay (blue graph), despite slow convergence in the early training steps, model accuracy
is eventually higher than the red one without weight decay scheme.
Figure 12: Comparison of various hyper-parameter choices for CIFAR-10 or ImageNet.
14
Under review as a conference paper at ICLR 2020
(兴)XOBJnOOV"①I
2 0 8 6 4 2 0
9 9 8 8 8 8 8
(兴)XQBJnOOV"①I
----(q=1, Nin=8, Nout=20)
----(q=1, Nin=12, Nout=20)
----(q=1, Nin=16, Nout=20)
(q=1, Nin=20, Nout=20)
0	100	200	300	400	500
Epoch
(a) Test accuracy using q = 1.
92
90
88
86
84
82
80
78
76
----(q=2, Nin=8, Nout=20)
----(q=2, Nin=12, Nout=20)
(q=2, Nin=16, Nout=20)
(q=2, Nin=20, Nout=20)
0	100	200	300	400	500
Epoch
(b) Test accuracy using q = 2. Compared to the above plots (Figure 13a), this figure shows that a combination
of multiple M㊉ for a binary code can lead to stable learning curves and higher model accuracy.
Figure 13: Test accuracy of ResNet-32 on CIFAR-10 using learning rate warmup (for 100 epochs)
and Nout = 20
15
Under review as a conference paper at ICLR 2020
		Bits/Weight	ReSNet-20		ReSNet-32		Comp. Ratio
FP		32	91.87%	-	92.33%	-	10X
Nin=10,Nout=10		10	90.21%	-1.66%	91.40%	-0.93%	-29.95 ×
Nin	=9, Nout=10	0.9	90.03%	-1.84%	91.28%	-1.05%	31.82×
Nin	=8, Nout=10	0.8	89.59%	-2.28%	90.86%	-1.47%	35.32×
Nin	=7, Nout =10	0.7	89.24%	-2.63%	90.54%	-1.79%	39.68 ×
Nin	=6, Nout =10	0.6	89.21%	-2.66%	90.41%	-1.92%	45.27 ×
Nin	=5, Nout =10	0.5	88.54%	-3.33%	89.82%	-2.51%	52.70 ×
Table 4: Weight compression comparison of ResNet-20 and ResNet-32 on CIFAR-10 when Nout =
10. Parameters and recipes not described in the table are the same as in Table 1. We also present
compression ratio for fractional quantized ResNet-32 when one scaling factor (α) is assigned to each
output channel.
		ReSNet-20					ReSNet-32			
	FP	Quant.	Diff.	FP	Quant.	Diff.
TWN (ternary)	92.68%	88.65%	-4.03%	93.40%	90.94%	-2.46%
BinaryRelax (ternary)	92.68%	90.07%	-1.91%	93.40%	92.04%	-1.36%
TTQ (ternary)	91.77%	91.13%	-0.64%	92.33%	92.37%	+0.04%
	LQ-Net(2 bit)	92.10%	91.80%	-0.30%	-	-	-
FleXOR(q = 2, Nout = 20)
Nin =20, 2.0 bit/weight		91.28%	-0.59%		91.99%	-0.34%
Nin=18, 1.8 bit/weight		90.96%	-0.91%		92.01%	-0.32%
Nin=16, 1.6bit/weight	91.87%	90.81%	-1.06%	92.33%	91.55%	-0.78%
Nin=14, 1.4bit/weight		90.22%	-1.65%		91.33%	-1.00%
Nin=12, 1.2bit/weight		89.88%	-1.99%		90.78%	-1.55%
FleXOR(q = 2, Nout = 10)
Nin=10, 2.0 bit/weight		91.06%	-0.81%		92.61%	+0.28%
Nin=9, 1.8 bit/weight		91.00%	-0.87%		92.09%	-0.24%
Nin=8,1.6bit/Weight	91.87%	90.82%	-1.05%	92.33%	91.96%	-0.37%
Nin=7,1.4bit/Weight		90.81%	-1.06%		91.63%	-0.70%
Nin=6,1.2bit/Weight		90.28%	-1.59%		91.32%	-1.01%
Table 5: Weight compression comparison of ResNet-20 and ResNet-32 on CIFAR-10 using learning
rate warmup (for 100 epochs) and q = 2. As mentioned in Figure 4, multiple M㊉ can be combined
for multi-bit quantization schemes. Then, the number of scaling factors should be doubled. FleXOR
with q = 2 and two different M㊉ structures achieve full-precision accuracy when both Nin and
Nout are 10.
(TdOICF) A3B∙rn3v %①I
Figure 14: Test accuracy (Top-1) of ResNet-18 on ImageNet using FleXOR.
16
Under review as a conference paper at ICLR 2020
Methods		Bits/Weight	Top-1	Top-5
Full Precision (He et al., 2016)	32	.	69.6%	89.2%
TWN (Li & Liu, 2016)	ternary	61.8%	84.2%
ABC-Net (Lin et al., 2017)	2	63.7%	85.2%
BinaryReIax (Yin et al., 2018)	ternary	66.5%	87.3%
TTQ(1.5× Wide)(ZhU et al., 2017)	ternary	66.6%	87.2%
LQ-net (Zhang et al., 2018)	2	68.0%	88.0%
QIL (JUng et al., 2019)	2	68.1%	88.3%
	1.6 (0.8 × 2)	66.2%	86.7%
FleXOR (q = 2, Nout = 20)	1.2 (0.6 × 2)	65.4%	86.0%
	0.8 (0.4 × 2)	63.8%	85.0%
Table 6: Weight compression comparison of ResNet-18 on ImageNet when q = 2. Since q is 2, we
also list the other compression schemes which use 2-bit or ternary quantization scheme for model
compression.
17