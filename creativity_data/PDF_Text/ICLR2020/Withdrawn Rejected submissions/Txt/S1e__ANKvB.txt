Under review as a conference paper at ICLR 2020
Molecular Graph Enhanced Transformer for
RETROSYNTHESIS PREDICTION
Anonymous authors
Paper under double-blind review
Ab stract
With massive possible synthetic routes in chemistry, retrosynthesis prediction is
still a challenge for researchers. Recently, retrosynthesis prediction is formu-
lated as a Machine Translation (MT) task. Namely, since each molecule can
be represented as a Simplified Molecular-Input Line-Entry System (SMILES)
string, the process of synthesis is analogized to a process of language translation
from reactants to products. However, the MT models that applied on SMILES
data usually ignore the information of natural atomic connections and the topol-
ogy of molecules. In this paper, we propose a Graph Enhanced Transformer
(GET) framework, which adopts both the sequential and graphical information
of molecules. Four different GET designs are proposed, which fuse the SMILES
representations with atom embedding learned from our improved Graph Neural
Network (GNN). Empirical results show that our model significantly outperforms
the Transformer model in test accuracy.
1	Introduction
Retrosynthesis prediction aims to predict a set of suitable reactants that can synthesize the desired
molecule via a series of reactions. It pushes forward an immense influence in agriculture, medical
treatment, drug discovery and so on. However, the retrosynthesis prediction is challenging since
there are massive possible synthetic routes available and it is often difficult to navigate the direc-
tion of retrosynthesis process. Indeed, each bond in the target molecule may represent a possible
retrosynthetic disconnection, leading to a vast space of possible starting materials. Besides, the dif-
ference between two synthetic routes may be subtle, which usually depends on the global structures.
Actually, planning a proper retrosynthetic route for a complex molecule is also a tough work even
for the professional chemists.
One of the prevailing methods is to deem the retrosynthesis prediction as a machine translation
task. This analogy is comprehensible since every molecule has a unique text representation named
SMILES (Weininger, 1988). In this case, given a target molecule written in SMILES notation,
the retrosynthesis prediction is just to predict a string of SMILES which represents the reactants.
Based on this idea, Liu et al. (2017) first applied the LSTM with attention mechanism in retrosyn-
thesis prediction and achieved comparable performance compared with previous traditional meth-
ods. Whereafter, many works (Karpov et al., 2019; Zheng et al., 2019; Lin et al., 2019; Lee et al.,
2019) tried to employ Transformer (Vaswani et al., 2017), which is a more powerful Sequence-to-
Sequence(Seq2Seq) model, to improve prediction accuracy in retrosynthesis. However, these meth-
ods just utilize the sequential representations of the molecule, while ignoring the natural topological
connections between atoms within the molecule. These atomic connections can provide more flexi-
ble and accurate chemical information, which is critical in many related chemical tasks like molecu-
lar representation (Duvenaud et al., 2015; Gilmer et al., 2017) and chemical reaction prediction (Jin
et al., 2017; Do et al., 2019). We believe that the absence of this molecular graph information hin-
ders the further improvement of the present methods for retrosynthesis. How to effectively make use
of this natural graphical information of the molecular structure, therefore, becomes a vital problem.
To tackle this problem, we propose Graph Enhanced Transformer(GET) framework that can enjoy
the advantage of both graph-level representations and sequence-level representations. Specifically,
to solve the retrosynthesis problem, we design an improved Graph Neural Network(GNN) called
Graph Attention with Edge and Skip-connection (GAES) to learn each atom’s representation, and
1
Under review as a conference paper at ICLR 2020
try four strategies to incorporate it with the original SMILES representation in the encoder. The
main contributions of this paper are as follows:
•	We propose a new framework called GET that fuses graphical representations with sequen-
tial representations of the target molecule to solve retrosynthesis prediction task.
•	We design a powerful GNN called GAES that learns high-quality representations of atom
nodes in a self-attention manner with bond features, and it is less affected by the side-effect
of stacking more layers.
GET is evaluated on USPTO-50K, a common benchmark dataset for retrosynthesis. Experimental
results show that our model achieves new records for top-1 prediction accuracy in the state-of-the-art
methods and outperforms Seq2Seq-based methods in all tested top-n accuracy, demonstrating the
effectiveness of fusing the molecular graph information with the SMILES sequence information.
2	Related Work
Prior work on retrosynthesis can be mainly summarized into two categories: template-based meth-
ods and template-free methods.
Template-based Methods The majority of computer-aided retrosynthetic methods in the early
period were relied on encoding reaction templates or generalized subgraph matching rules.
LHASA (Corey & Wipke, 1969) was the first software for retrosynthetic analysis. Recently, one
of the most well-known retroSynthesis analysis tool is Synthia (SzymkUc et al., 2016) that inte-
grated about 70,000 hand-encoded reaction rules collected by manual. Based on the 60,000 reaction
templates derived from 12 million single-step reaction examples, Schreck et al. (2019) introdUced
Reinforcement Learning (RL) into this area by treating retrosynthesis as a game whose goal is to
identify policies that make (near) optimal reaction choices dUring each step of retrosynthetic plan-
ning. Segler et al. (2018) extracted two sets of transformation rUles and combined Monte Carlo
tree search with symbolic AI to discover possible retrosynthetic roUtes. Besides manUal extracted
rUles, some works (Law et al., 2009; Segler & Waller, 2017; Coley et al., 2017) tried to collect
reaction templates aUtomatically and perform retrosynthesis based on these aUtomated templates.
AlthoUgh template-based methods work well in many cases, they still face a serioUs drawback that
they generally cannot achieve accUrate prediction accUracy oUtside of their known templates.
Template-free Methods OUr model jUst belongs to this category. Emerging template-free methods
are to treat retrosynthesis as a machine translation task as introdUced in section 1. Since these
methods do not need any reaction templates and prior chemistry knowledge, they are attracting
more and more attention from academia. Moreover, withoUt the constraint of fixed templates, they
have the potential of discovering novel synthetic roUtes. The most related work to oUrs are Zheng
et al. (2019) Lin et al. (2019); Karpov et al. (2019); Lee et al. (2019) that apply Transformer in
retrosynthesis prediction.
3	Background
In this section, we first introdUce how GNN is Used in learning the molecUle (or atoms) representa-
tion, and then describe how the Transformer model is previoUsly applied in retrosynthesis prediction.
3.1	GNN for molecule representation learning
GNN has been widely Used in learning the representation of the molecUle and its atoms. NatUrally,
molecUles can be represented as graph strUctUre with atoms as nodes and bonds as edges. SUppose
that a molecUlar graph G has initial node representations hv and edge representations evw, a typical
one-layer GNN can learn new and more powerfUl node representations from G by the following
message passing process described in Gilmer et al. (2017):
mv =	M (hv,hw,evw) ,	(1)
w∈N (v)
hvnew = U (hv,mv) ,	(2)
where N(v) denotes the neighbors of node v in graph G, M is the message fUnction that is respon-
sible for collecting information from neighbors, and U is the Update fUnction for fUsing collected
2
Under review as a conference paper at ICLR 2020
information mv with old node representation hv to obtain the new node representation hvnew . Fur-
ther, we can stack several these GNN layers to capture higher-order neighbors’ information.
Then a readout function R can be used to integrate all node representations into a whole graph
representation g :
g = R({hv|v ∈ G}).	(3)
The hv and g, which represent the atoms and the whole molecule, are often trained in an end-to-end
way for a specific chemical task, such as chemical properties prediction, reaction prediction and
molecule optimization.
3.2	Transformer for retrosynthesis prediction
Transformer (Vaswani et al., 2017) is a Seq2Seq model that has shown excellent performance in
machine translation task. Also, it has been applied in chemical reaction prediction and retrosynthe-
sis prediction before. Given an input SMILES that represents the target molecule and a specified
reaction type (optional), retrosynthesis prediction is to predict the output SMILES which represents
the possible reactants that can synthesis the target molecule in the specified reaction type. Thus, ret-
rosynthesis prediction can be deemed as a machine translation task whose source language is target
molecule SMILES and the target language is reactants SMILES.
In this view, Transformer can be applied to retrosynthesis prediction as the same as to machine
translation. Since Transformer is a mature model that has been widely used in natural language
processing (NLP), we just give a simple introduction here. Specifically, Transformer follows an
encoder-decoder structure and is composed of several combinations of multi-head attention layers
and position-wise feed forward layers. The encoder consists of a stack of N = 6 identical layers.
Each layer includes two main components: (multi-head) self-attention layer and feed-forward net-
work. Given an input vectors (p1, ...,pn), p ∈ Rd, the t-th output st of the self-attention layer is
calculated by:
n
qt = Wqpt ,	km = Wmpm ,	vt = Wvpt ,	st = softmax(
m=1
< qt , km >
√dk
)vt,	(4)
where dk is the dimension of q and k, Wq , Wm , Wv are weight matrices. One such operation is
called one head, and we can concatenate several heads to change to multi-head self attention.
The feed-forward network is composed of two linear transformations with a ReLU activation:
FFN(x) = max (0, xW1 +b1)W2 +b2,	(5)
where W1, W2 are weight matrices and b1, b2 are biases.
Similarly, the decoder is also mainly composed of multi-attention layers and position-wise feed for-
ward layers. It will generate the output SMILES step by step. At step t, it utilizes the encoder’s out-
put (p01, ...,p0n) and all previous steps’ output (x01, ..., x0t-1) to generate the next SMILES character
x0t. This process repeated until generating a specific termination character, i.e., x0t =< EOS >.
4	Graph Enhanced Transformer
In this section, we provide the details about our Graph Enhanced Transformer (GET) framework
for retrosynthesis prediction. Figure 1 shows an overview of GET. On the whole, GET is accord
with typical encoder-decoder structure, of which the integral encoder is composed of the graph
encoder and transformer encoder for learning the representation in graph-level and sequence-level
respectively. Given a target molecule’s SMILES, it will first pass through the two encoders somehow
to get the hidden representation of each character, and then the decoder will utilize these hidden
representations to generate the output SMILES.
4.1	Graph Encoder
We design a new powerful GNN called Graph Attention with Edge and Skip-connection (GAES) as
the graph encoder, which can learn the representation of each atom in a molecule. This graphical
3
Under review as a conference paper at ICLR 2020
I________________________________________________
Figure 1: Overview of GET. The input SMILES will be processed by the two-sub encoders (graph
encoder and transformer encoder) somehow to be transformed to its hidden representation. Then, at
each step, the decoder will utilize the hidden representation and all outputs of the previous steps to
generate the present step,s SMILES character.
representation reflects the connection of atoms within a molecule and may play a significant role in
further alleviating the long-term dependency problem to avoid generating chemically invalid output.
We use RDKit(Landrum, 2016) to transform a SMILES into the molecular graph, whose nodes are
atoms and edges are chemical bonds. The input representation of the atom node is a 21-dimensional
vector that contains some chemical information about the atom, and the detail can be found in Table 1
(nearly consistent with Gilmer et al. (2017)). The input representation of the edge is a 4-dimensional
one-hot vector that encodes the bond types including single, double, triple and aromatic.
Since the SMILES sequence (x1, ..., xn) has been transformed into a graph G with the input repre-
sentations (h1, ..., hN) for nodes and {eij} for edges that exist between node i and node j, our GNN
will produce new representation h0i for each node i by the following message passing operations:
exp LeakyReLU aT [WhikWhjkeij]
αij pk∈M exp (LeakyReLU (aT [WhikWhkkeik])),
(6)
hi = σ £ αij Whj ,	(7)
j∈Ni
where a ∈ R(2F 0+E) is a weight vector for attention mechanism and W ∈ RF0×F is a weight
matrix for transforming the node features, so F is the input dimension of nodes, F0 is the output
dimension of nodes and E is the dimension of edges. Ni is the set of first-order neighbors of node i
(including itself). σ is an activation function, e.g., ReLU.
In practice, we perform K multi-head attention (VeIiCkOViC et al., 2017) to enrich the model capacity
and to stabilize the learning process. Each attention head has its own parameters and we average
their outputs to get better representation:
hi= σ (K XL X αjWk hj .	(8)
k=1 j ∈Ni
The above operations can be seen as GAT (VeIiCkOViC et al., 2017) extended to include edge features.
Then, to mitigate the accuracy reduction issue (Kipf & Welling, 2016) caused by stacked graph
convolution layers, we adopt the gated skip-connection mechanism (Ryu et al., 2018) to get the
final representation:
zi = sigmod(U1hi + U2hi + b),	(9)
hi = Zi Θ hi + (1 — Zi) Θ hi,	(10)
4
Under review as a conference paper at ICLR 2020
Table 1: Input representation of atom nodes
Atom Feature
Description
Atom type
Atom number
Acceptor
Donor
Aromatic
Hybridization
Number of Hydrogens
C, N, O, S, P, B, F, I, Sn, Cl, Br, Se, Si (one-hot)
Numbers of protons (integer)
Accepts electrons (binary)
Donates electrons (binary)
In an aromatic system (binary)
sp, sp2, sp3 (one-hot or null)
(integer)
where U1 , U2 and b are trainable parameters.
Note that the above operations are just in one layer of our GAES, and we can stack several layers
to capture the information about higher-order atom neighbors so that to obtain more comprehensive
representations.
4.2	Transformer Encoder
The transformer encoder is the same as described in section 3, which can capture the sequential
representations of molecules (or atoms) represented by SMILES. The original SMILES (x1, ..., xn)
is changed to a sequence of vectors (p1, ...,pn),p ∈ Rd after passing the embedding layer. And it
will be further updated to vectors (p01, ..., p0n) by the transformer encoder.
4.3	Representation Fusion
Intuitively, graphical representations reflect the intrinsic structural features of molecules and should
be beneficial to generate chemical-valid and more accurate SMILES output. To this end, we propose
four fusion strategies to fuse these graphical and sequential embeddings.
Integral Encoder
GET-LTI	GET-LT2	GET-CT	GET-LG )
_______________________________________________________________，
Input Sequence Representation
Sequence-level Representation
Graph-level (atom) Representation
Hidden Representation
Input Atom Representation
Concatenate Operation
4
Figure 2: Illustration of four fusion strategies in the encoder of GET. The integral encoder is com-
posed of two sub-encoders: graph encoder and transformer encoder. The embedding layer is as
described in 4.2 . The hidden representation of the target molecule can be obtained in four ways
(GET-LT1, GET-LT2, GET-CT and GET-LG).
5
Under review as a conference paper at ICLR 2020
4.3.1	Graph Link Transformer
As shown in Figure 2 (GET-LT1), we concatenate the atom representations with embeddings of
SMILES and perform a linear transformation by weight matrix M . Then the new representations
are sent to the transformer encoder to produce the output of the integral encoder. For non-atomic
characters in SMILES, the corresponding atom representations are set to zero vector. Formally,
Pi	=	[Pikh'i↑,	(11)
(vι,..., Vn)	=	(pl,…,Pn)	= TransformerEncOder(MPI,…，Mpn),	(12)
where h0i = 0 if xi is non-atomic character, k is the concatenation operation. (v1, ..., vn) is integral
encoder’s output.
Considering that there may exist inconsistency between encoder and decoder since the initial atom
features inputted to the graph encoder cannot be utilized by the decoder when making inference, we
try another way by replacing the natural atom features (h1, ..., hn) with (P1, ..., Pn) as the input
representations of the graph encoder:
(h01, ..., h0n) = GraphEncoder(P1, ..., Pn).	(13)
In this way, the graph encoder can enhance the original sequential representations (P1, ..., Pn) with
molecule structure information directly, but the natural atom features have to be “sacrificed’. Then
we send the output of the graph encoder to the transformer encoder to get the final output:
(v1, ..., vn) = (P01, ..., P0n) = TransformerEncoder(h01, ..., h0n).	(14)
We name the first scheme GET-LT1 and the second scheme GET-LT2.
4.3.2	Graph Concatenate with Transformer
As shown in Figure 2 (GET-CT), we concatenate the outputs of the graph encoder and transform
encoder, and also perform linear transformation to get the output of the integral encoder:
Pi = [pikhi],	(15)
(V1,…,Vn) = (MPI,…,MPn).	(16)
The notations are consistent with 4.3.1.
4.3.3	Transformer Link Graph
As shown in Figure 2 (GET-LG), the SMILES sequence first pass through the transformer encoder,
then it is concatenated with natural atom features (h1, ..., hn) to be the input representation of the
graph encoder. Those non-atomic characters are added into the molecular graph as standalone nodes
which do not connect with any other node, and their “atom feature vectors” are just zero vectors.
Finally, the graph encoder’s output will be the integral encoder’s output:
(V1, ..., Vn) = (h01, ..., h0n) = GraphEncoder([P01 kh1], ..., [P0nkhn]).	(17)
4.4 Decoder
The decoder is the same as vanilla Transformer’s (Vaswani et al., 2017) decoder which has
been introduced in section 3. At step t, the encoder’s output (V1, ..., Vn) and all previous steps’
output (x01, ..., x0t-1) are used by the decoder to generate the next SMILES character x0t until
x0t =< EOS >.
6
Under review as a conference paper at ICLR 2020
5	Experiments
In this section, we evaluate our model for retrosynthesis prediction on a common benchmark dataset
USPTO-50K which is derived from USPTO granted patents that includes 50,033 reactions classified
into 10 reaction types. A reaction is described as a pair of sequences which consist of SMILES nota-
tions for target molecule (with reaction type) and reactants. For example, an heterocycle formation
reaction is described as: (“<RX_4>c1ccc(-c2nnn[nH]2)cc1"，“N#CcIccccc1.[N-]=[N+]=[N-]”),
where “<RX_4>” represents heterocycle formation reaction, “c1ccc(-c2nnn[nH]2)cc1"is SMILES
of the target molecule, “N#Cc1ccccc1” and “[N-]=[N+]=[N-]” are SMILES of two reactants sepa-
rated by “.”.
5.1	Settings
•	Dataset Split Many previous works (Liu et al., 2017; Coley et al., 2017; Zheng et al., 2019;
Karpov et al., 2019) follow a specific split strategy with 40,029, 5,004 and 5,004 reactions
for training, validation and testing, and we keep the same.
•	Implementation For the graph encoder, it is implemented based on DGL (Wang et al.,
2018). We stack 3 identical layers in our GNN. The input and output dimension of nodes
are set to 21 and 256 respectively. The number of multi-head is set to 2; For the transformer
encoder and the decoder, we implement them using OpenNMT (Klein et al., 2017), and the
parameter settings are presented in the code; Besides, the final dimension of the integral
encoder’s output v is set to 256.
5.2	Result
We compare our model with the vanilla Transformer (Vaswani et al., 2017), Rule-based Expert
System mentioned in Liu et al. (2017) , Similarity (Coley et al., 2017) and LSTM+Attention (Liu
et al., 2017). Note that the results of vanilla Transformer are based on our own experiments since
the results reported by previous works (Zheng et al., 2019; Lin et al., 2019; Karpov et al., 2019; Lee
et al., 2019) are different from each other. The retrosynthesis prediction accuracy across all classes
is provided in Table 2. Moreover, we also test the performance of GET-LT1 when removing the
reaction type from the original dataset, and the result is shown in Table 3.
Table 2: Comparison of top-n accuracies across all classes
Model	top-n accuracy (%)			
	1		5	10
Rule-based Expert System	^354	52.3	59.1	65.1
LSTM+Attention	37.4	52.4	57.0	61.7
Similarity	52.9	73.8	81.2	88.1
Transformer (baseline)	ɪɪ	68.4	72.0	74.4
GET-CT (our)	55.9	70.1	73.2	76.3
GET-LG (our)	54.9	69.7	72.2	74.6
GET-LT2 (our)	56.2	69.4	72.5	74.7
GET-LT1 (our)	57.4	71.3*	74.8*	77.4*
Table 3: Comparison of top-n accuracies across all classes without reaction type
Model	top-n accuracy (%) -1	3	5	10-
Similarity	-373^^547^^633^^74T
Transformer (baseline) GET-LT1 (our)	^T2.3^^575^^610^^65.7- 44.9 58.8 62.4 65.9
Results show that our models outperform all of previous methods in top-1 accuracy, and our
best model GET-LT1 achieves the new state-of-the-art among all Seq2Seq-based methods, i.e,
7
Under review as a conference paper at ICLR 2020
LSTM+Attention, vanilla Transformer and models of GET. Compared with vanilla Transformer,
GET-LT1 can improve the prediction accuracy by 3.1%, 2.9%, 2.8% and 3.0% in top-1, top-3, top-5
and top-10 accuracy. Other variants also have varying degrees of performance improvement over
vanilla Transformer. And our model can retain this comprehensive superiority after removing the
reaction type, demonstrating that molecule structure information can help Transformer to predict
more accurate reactants. Besides, note that Similarity (Coley et al., 2017) is a template-based model
which predicts 100 candidates and just chooses the top-10 as the final result, while other template-
free models, i.e., LSTM+Attention, Transformer and GET, are trained to accurately predict the top-1
output and only generate 10 candidates using beam search. Therefore, it is not surprising that Simi-
larity achieves very high accuracy. Nevertheless, even in this unfair situation, our models can surpass
Similarity up to 4.5% in top-1 accuracy.
In addition, we present the detailed top-10 accuracy of three Seq2Seq-based models
(LSTM+Attention, vanilla Transformer and GET-LT1) for each reaction class in Table 4. Results
show that our approach can improve vanilla Transformer on 9 of 10 reaction classes by a margin of
1.3% to 17.4%, indicating the better generalization ability and comprehensiveness of our model.
Table 4: Comparison of the top-10 accuracy for each reaction class
IvrCAnI				top-10 accuracy (%)								10
	1	2	3	4	5	6	7	8	9	
LSTM+Attention	57.5	74.6	46.1	27.8	80.0	62.8	67.8	69.1	47.3	56.5
Transformer (baseline)	73.5	81.9	62.7	52.2	86.1	71.5	80.0	83.9	65.2	73.9
GET-LT1 (our)	76.6	84.2	66.1	65.6	89.2	75.7	81.3	81.5	71.7	91.3
Furthermore, the rate of producing grammatically invalid SMILES for different beam sizes are
shown in Table 5 (with reaction type). As can be seen, after fusing molecule structure information,
the model is more inclined to generate chemical-valid SMILES compared with vanilla Transformer,
since the graphical representations, which directly capture the topological connection of atoms, are
able to break the limitation of the SMILES sequence and can give the model additional guidance to
produce chemical-valid compound.
Table 5: The rate of producing grammatically invalid SMILES for different beam sizes
Model	invalid SMILES, rate (%) when beam size k =			
	1	3	5	10
LSTM+Attention	12.2	15.3	18.4	22.0
Transformer (baseline)	3.5	14.3	20.3	30.2
GET-LT1 (our)	2.2	13.4	19.5	29.3
6	Conclusion and Future Work
We propose Graph Enhanced Transformer(GET), an effective framework that successfully combines
the graphical and sequential representations of the molecule to improve the retrosynthesis predic-
tion performance. Experiments indicate that our model outperforms state-of-the-art Seq2Seq-based
methods on USPTO-50K dataset, and shows promising ability in reducing invalid SMILES rate.
In the future, we plan to 1) explore how to utilize the molecular graph information in the decoder.
2) research how to let the decoder generate reactants in the form of graph directly.
References
Connor W Coley, Luke Rogers, William H Green, and Klavs F Jensen. Computer-assisted retrosyn-
thesis based on molecular similarity. ACS central science, 3(12):1237-1245, 2017.
EJ Corey and W Todd Wipke. Computer-assisted design of complex organic syntheses. Science,
166(3902):178-192, 1969.
8
Under review as a conference paper at ICLR 2020
Kien Do, Truyen Tran, and Svetha Venkatesh. Graph transformation policy network for chemi-
cal reaction prediction. In Proceedings of the 25th ACM SIGKDD International Conference on
Knowledge Discovery & Data Mining,pp. 750-760. ACM, 2019.
David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, RafaeI Bombarell, Timothy HirzeL Alan
Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning molecular
fingerprints. In Advances in neural information processing systems, pp. 2224-2232, 2015.
Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
message passing for quantum chemistry. In Proceedings of the 34th International Conference on
Machine Learning-Volume 70, pp. 1263-1272. JMLR. org, 2017.
Wengong Jin, Connor Coley, Regina Barzilay, and Tommi Jaakkola. Predicting organic reaction out-
comes with weisfeiler-lehman network. In Advances in Neural Information Processing Systems,
pp. 2607-2616, 2017.
Pavel Karpov, Guillaume Godin, and Igor V Tetko. A transformer model for retrosynthesis. In
International Conference on Artificial Neural Networks, pp. 817-830. Springer, 2019.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. arXiv preprint arXiv:1609.02907, 2016.
Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander M Rush. Opennmt:
Open-source toolkit for neural machine translation. arXiv preprint arXiv:1701.02810, 2017.
G Landrum. Rdkit: open-source cheminformatics software, 2016.
James Law, Zsolt Zsoldos, Aniko Simon, Darryl Reid, Yang Liu, Sing Yoong Khew, A Peter John-
son, Sarah Major, Robert A Wade, and Howard Y Ando. Route designer: a retrosynthetic analysis
tool utilizing automated retrosynthetic rule generation. Journal of chemical information and mod-
eling, 49(3):593-602, 2009.
AA Lee, Q Yang, V Sresht, P Bolgar, X Hou, JL Klug-McLeod, and CR Butler. Molecular trans-
former unifies reaction prediction and retrosynthesis across pharma chemical space. Chemical
communications (Cambridge, England), 2019.
Kangjie Lin, Youjun Xu, Jianfeng Pei, and Luhua Lai. Automatic retrosynthetic pathway planning
using template-free models. arXiv preprint arXiv:1906.02308, 2019.
Bowen Liu, Bharath Ramsundar, Prasad Kawthekar, Jade Shi, Joseph Gomes, Quang Luu Nguyen,
Stephen Ho, Jack Sloane, Paul Wender, and Vijay Pande. Retrosynthetic reaction prediction using
neural sequence-to-sequence models. ACS central science, 3(10):1103-1113, 2017.
Seongok Ryu, Jaechang Lim, Seung Hwan Hong, and Woo Youn Kim. Deeply learning molecular
structure-property relationships using attention-and gate-augmented graph convolutional network.
arXiv preprint arXiv:1805.10988, 2018.
John S Schreck, Connor W Coley, and Kyle JM Bishop. Learning retrosynthetic planning through
self-play. arXiv preprint arXiv:1901.06569, 2019.
Marwin HS Segler and Mark P Waller. Neural-symbolic machine learning for retrosynthesis and
reaction prediction. Chemistry-A European Journal, 23(25):5966-5971, 2017.
Marwin HS Segler, Mike Preuss, and Mark P Waller. Planning chemical syntheses with deep neural
networks and symbolic ai. Nature, 555(7698):604, 2018.
Sara SzymkUC, Ewa P Gajewska, Tomasz Klucznik, Karol Molga, Piotr Dittwald, MichaI Startek,
MichaI Bajczyk, and Bartosz A Grzybowski. Computer-assisted synthetic planning: The end of
the beginning. Angewandte Chemie International Edition, 55(20):5904-5937, 2016.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998-6008, 2017.
9
Under review as a conference paper at ICLR 2020
Petar VeliCkovic, GUillem CUcUrUlL Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.
Minjie Wang, Lingfan YU, QUan Gan, Da Zheng, YU Gai, Zihao Ye, MUfei Li, Jinjing ZhoU,
Qi HUang, JUnbo Zhao, Haibin Lin, Chao Ma, Damon Deng, Qipeng GUo, Hao Zhang, Jinyang
Li, Alexander J Smola, and Zheng Zhang. Deep graph library, 2018. URL http://dgl.ai.
David Weininger. Smiles, a chemical langUage and information system. 1. introdUction to method-
ology and encoding rules. Journal ofchemical information and computer sciences, 28(1):31-36,
1988.
ShUangjia Zheng, JiahUa Rao, ZhongyUe Zhang, JUn XU, and YUedong Yang. Predict-
ing retrosynthetic reaction Using self-corrected transformer neUral networks. arXiv preprint
arXiv:1907.01356, 2019.
10