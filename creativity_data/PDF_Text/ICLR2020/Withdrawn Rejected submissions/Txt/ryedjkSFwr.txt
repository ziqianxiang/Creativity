Under review as a conference paper at ICLR 2020
Global Momentum Compression
for Sparse Communication in Distributed SGD
Anonymous authors
Paper under double-blind review
Ab stract
With the rapid growth of data, distributed stochastic gradient descent (DSGD)
has been widely used for solving large-scale machine learning problems. Due to
the latency and limited bandwidth of network, communication has become the
bottleneck of DSGD when we need to train large scale models, like deep neural
networks. Communication compression with sparsified gradient, abbreviated as
sparse communication, has been widely used for reducing communication cost
in DSGD. Recently, there has appeared one method, called deep gradient com-
pression (DGC), to combine memory gradient and momentum SGD for sparse
communication. DGC has achieved promising performance in practice. However,
the theory about the convergence of DGC is lack. In this paper, we propose a
novel method, called global momentum Compression (GMC), for sparse commu-
nication in DSGD. GMC also combines memory gradient and momentum SGD.
But different from DGC which adopts local momentum, GMC adopts global mo-
mentum. We theoretically prove the convergence rate of GMC for both convex
and non-convex problems. To the best of our knowledge, this is the first work that
proves the convergence of distributed momentum SGD (DMSGD) with sparse
communication and memory gradient. Empirical results show that, compared
with the DMSGD counterpart without sparse communication, GMC can reduce
the communication cost by approximately 100 fold with no loss of generaliza-
tion accuracy. GMC can also achieve comparable (sometimes better) performance
compared with DGC, with an extra theoretical guarantee.
1 Introduction
Many machine learning models can be formulated as the following empirical risk minimization
problem:
min F(w) :
w∈Rd
1n
-Ef (W； ξi),
n=1
(1)
where W denotes the model parameter, ξi	denotes the ith training data, n is the number of training
data, and d is the size of the model. For example, let ξi = (ai , yi	), where ai denotes the feature
of the ith training data and yi denotes the label. Then in logistic regression f (w; ξi) = log(1 +
e-yiaTW) + 2 ∣∣wk1 2, and in SVM f (w; ξi) = max(0,1 - yiaTw) + 2∣∣wk2. Many deep learning
models can also be formulated as (1).
One of the efficient ways to solve (1) is stochastic gradient descent (SGD) (Robbins & Monro,
1951). In each iteration, SGD calculates one stochastic gradient Vf (w; ξ) and updates W by W J
W-ηVf (w; ξi), or updates W with a mini-batch of stochastic gradients. Inspired by momentum and
nesterov’s accelerated gradient descent, momentum SGD (MSGD) (Polyak, 1964; Tseng, 1998; Lan,
2012; Kingma & Ba, 2015) has been proposed and widely used in machine learning. In practice,
MSGD can achieve better performance than SGD (Krizhevsky et al., 2012; Sutskever et al., 2013).
Many machine learning platforms like TensorFlow, PyTorch, and MXNet adopt MSGD as one of
the optimization methods.
With the rapid growth of data, distributed SGD (DSGD) (Dekel et al., 2012; Li et al., 2014b) has
attracted much attention since it can parallelly calculate a batch of stochastic gradients. DSGD can
1
Under review as a conference paper at ICLR 2020
be formulated as follows:
p
ηt
Wt+1 = Wt - - Zgt,k,	(2)
where pis the number of workers, gt,k is the stochastic gradient or a mini-batch of stochastic gra-
dients calculated by the kth worker at the tth iteration. DSGD can be implemented on distributed
frameworks like parameter server and all-reduce framework. Each worker calculates gt,k and sends
it to the server or other workers for updating W. Recently, more and more large models, such as deep
learning models, are used in machine learning to improve the generalization ability. In large models,
gt,k is a high dimensional vector. Due to the latency and limited bandwidth of network, commu-
nication cost has become the bottleneck of traditional DSGD or distributed MSGD (DMSGD). For
example, when we implement DSGD on parameter server, the server needs to receive p high dimen-
sion vectors from workers, which will lead to communication traffic jam and make the convergence
of DSGD slow. Hence, we need to compress gt,k to reduce the communication cost.
Recently, researchers have proposed two main categories of communication compression techniques
for reducing communication cost in DSGD and DMSGD. The first category is quantization (Wen
et al., 2017; Alistarh et al., 2017; Jiang & Agrawal, 2018). In machine learning problems, 32-bit float
number is typically adopted for representation. Quantization methods quantize the value (gradient or
parameter) representation from 32 bits to some low bit-width like 8 bits or 4 bits. Since the quantized
gradients in most methods are an unbiased estimation of the original ones, the convergence rate
of these methods has the same order of magnitude as that of DSGD, but slower due to the extra
quantization variance. It is easy to find that the communication cost can be reduced by 31 fold in
the ideal case. In practice, at least 4 bits should be adopted for representation in most cases to keep
original accuracy. In these cases, the communication cost is reduced by 7 fold.
The other category is based on sparsified gradient, which is called sparse communication. In sparse
communication, after calculating the update vector gt,k at each iteration, each worker only sends a
subset of coordinates in gt,k, denoted as S(gt,k), to the server or other workers. Here, S(gt,k) is
a sparse vector and hence it can reduce the communication cost. In recent works (Aji & Heafield,
2017; Lin et al., 2018), each worker will typically remember those values which are not sent to the
server, i.e., gt,k - S(gt,k), and store it in the memory rather than dropping them. The gt,k - S(gt,k)
is called memory gradient and will be used to calculate the next update vector gt+1,k. This is intu-
itively necessary because a subset of coordinates in the stochastic gradient can not reflect the real
descent direction and can make errors with higher probability than the original stochastic gradi-
ent. This memory gradient based sparse communication strategy has been widely adopted by recent
communication compression methods and has achieved better performance than quantization meth-
ods and other sparse communication methods without memory gradient. In these memory gradient
based sparse communication methods, some are for vanilla SGD (Aji & Heafield, 2017; Alistarh
et al., 2018; Stich et al., 2018; Karimireddy et al., 2019; Tang et al., 2019). The convergence rate of
vanilla SGD with sparse communication has been proved in (Alistarh et al., 2018; Stich et al., 2018;
Karimireddy et al., 2019; Tang et al., 2019). Very recently, there has appeared one sparse commu-
nication method for distributed MSGD (DMSGD), called deep gradient compression (DGC) (Lin
et al., 2018), which has achieved better performance than vanilla DSGD with sparse communication
in practice. However, the theory about the convergence of DGC is still lack. Furthermore, although
DGC uses momentum SGD, the momentum in DGC is calculated by each worker. Hence it is a
local momentum without global information.
In this paper, We propose a novel method, called global momentum Compression (GMC), for sparse
communication in DMSGD which includes DSGD as a special case. The main contributions of this
paper are summarized as folloWs:
•	GMC combines memory gradient and momentum SGD to achieve sparse communication
in DMSGD (DSGD). But different from DGC which adopts local momentum, GMC adopts
global momentum.
•	We theoretically prove the convergence rate of GMC for both convex and non-convex prob-
lems. To the best of our knowledge, this is the first work that proves the convergence of
DMSGD with sparse communication and memory gradient.
2
Under review as a conference paper at ICLR 2020
•	Empirical results show that, compared with the DMSGD counterpart without sparse com-
munication, GMC can reduce the communication cost by approximately 100 fold with no
loss of generalization accuracy.
•	GMC can also achieve comparable (sometimes better) performance with comparable com-
munication compression ratio, with an extra theoretical guarantee.
2 Preliminary
In this paper, We use ∣∣ ∙ k to denote L2 norm, use W to denote the optimal solution of (1), use
Vf (w; I) , |I| Pξi∈i Vf (w; ξi) to denote one stochastic gradient with respect to a mini-batch of
samples I such that EI[Vf(w; I)|w] = VF (w), use to denote element-Wise product, use 1 to
denote the vector (1, 1, . . . , 1)T ∈ Rd, use ej to denote the vector (0, . . . , 1, . . . 0)T ∈ Rd with only
the jth coordinate being 1, use I to denote an identity matrix. For a vector a, we use a(j) to denote
its jth coordinate value. ∣a∣0 denotes the number of non-zero values in a.
Definition 1 (smooth function) Function h(∙) is L-smooth (L > 0) if ∀w, w0,
|h(w) — h(w0) — Vh(w0)T(w - w0)∣ ≤ 刍IW - w0∣∣2∙
Definition 2 (Convexfunction) Function h(∙) is convex if ∀w, w0,
|h(w) — h(w0) — Vh(w0)T(w - w0)∣ ≥ μ∣∣w — w0∣2,
where μ ≥ 0. If μ > 0, h(∙) is called a μ-StrongIy ConvexfunCtion.
2.1	Distributed Momentum SGD
The widely used momentum SGD (MSGD) (Polyak, 1964) for solving (1) can be written as
gt =βgt-1 + ηtVf(wt; It),
wt+1 =wt - gt,
where β ∈ [0, 1). The gt is the Polyak’s momentum and Vf(wt; It) is an unbiased estimation of
VF(wt). Since gt = (wt - wt+1), MSGD can also be written as
β
wt+1 =Wt — ηt(Vf (wt； Zt) +——(wt-1 — wt)).	(3)
ηt
Please note that if β = 0, MSGD degenerates to SGD.
One simple way to implement distributed MSGD (DMSGD) is that each worker parallelly calculates
some stochastic gradients and then the stochastic gradients of all workers are aggregated to get
Vf (wt；It) + β(wt-ι — wt)∕ηt. The update process of W in this way is equivalent to the serial
MSGD. We call (Wt-I - wt)∕ηt the global momentum, because it captures the global information
from all workers.
Another way to implement DMSGD is using local momentum:
gt,k =βgt-1,k + ηtvt,k, k = 1,2, . . . ,p,
p
wt+1 =wt —	gt,k,
k=1
where vt,k is the stochastic gradient calculated by the kth worker and Pkp=1 vt,k = Vf(wt; It).
gt-1,k is the local momentum. Since Ppk=1 gt-1,k = (wt-1 — wt), this DMSGD with local mo-
mentum can also be written as the formulation in (3). Hence, the global momentum contains all
information of the local momentums. We will find that DGC (Lin et al., 2018) is mainly based on
the local momentum while GMC is mainly based on the global momentum. Hence, each worker in
DGC cannot capture the global information from its own local momentum, while that in GMC can
capture the global information from the global momentum even if sparse communication is adopted.
In the later section, we will see that global momentum is better than local momentum when using
memory gradient for sparse communication.
3
Under review as a conference paper at ICLR 2020
Algorithm 1 Global Momentum Compression (GMC) on Parameter Server
1	: Initialization: p workers, W-1 = W0, β ∈ [0, 1), batch size b;
2	: Set g0,k = u0,k = 0, k = 1, . . . , p,
3	: for t = 0, 1, 2, ...T - 1 do
4	Workers:
5	for k = 1, 2 ...,p, each worker ParaHelly do
6	:	if t > 0 then
7	:	Receive Wt - Wt-1 from the server;
8	:	Get Wt by Wt = Wt-1 + (Wt - Wt-1);
9	:	end if
10	:	Randomly pick a mini-batch of training data It,k ⊆ Dk with |It,k| = b;
11	gt,k =表 Pξi∈It,k Vf (Wt； ξi) - pηt (Wt - Wt-I)；
12	:	Generate a sparse vector mt,k ∈ {0, 1}d;
13	:	Send mt,k	(gt,k + ut,k) to the server;
14	:	ut+1,k = (1 - mt,k)	(gt,k +ut,k);
15	: end for
16	Server:
17	Wt+1 = Wt — ηt Pp=I mt,k Θ (gt,k + ut,k);
18	:	Send Wt+1 - Wt to workers;
19	: end for
3	Global Momentum Compression
Assume we have p workers. Dk denotes the data stored on the kth worker (k = 1, 2, . . . , p) and
Skp=1 Dk = {ξ1 , ξ2, . . . , ξn}. Our method Global Momentum Compression (GMC) mainly per-
forms the following operations iteratively:
•	Each worker calculates gt,k = P1b Pξi∈it,k Vf(W； ξi)-急(Wt-Wt-I),whereIt,k ⊆ Dk
and |It,k | = b;
•	Each worker generates a sparse vector mt,k and sends mt,k	(gt,k + ut,k) to the server
or other workers;
•	Each worker updates ut+1,k = (1 - mt,k)	(gt,k + ut,k);
•	Update parameter Wt+1 = Wt - ηt Ppk=1 mt,k	(gt,k + ut,k);
Below, we introduce the framework and two essential elements of GMC: memory gradient ut,k and
global momentum (Wt-I - Wt)∕ηt.
3.1	Framework of GMC
GMC can be easily implemented on the all-reduce distributed framework in which each worker
sends the sparse vector mt,k (gt,k + ut,k) to all the other workers, then each worker updates Wt+1
after receiving the sparse vectors from other workers.
Recently, parameter server (Li et al., 2014a) has been one of the most popular distributed frameworks
in machine learning. GMC can also be implemented on parameter server. Although the theories in
this paper can be adopted for the all-reduce framework, in this paper we adopt parameter server for
illustration. The details of GMC implemented on parameter server are shown in Algorithm 1. The
difference between GMC and traditional DSGD on parameter server is that in GMC after updating
Wt+1, server will send Wt+1 - Wt, rather than Wt+1, to workers. Since mt,k is sparse, Wt+1 - Wt
is sparse as well. Hence, sending Wt+1 - Wt can reduce the communication cost compared with
sending Wt. In our experiments, we find that GMC can make kWt+1 - Wtk0 ≤ 0.01d with no loss
of accuracy when training large scale models. Workers can get Wt+1 by Wt+1 = Wt + (Wt+1 -Wt).
Remark 1 We can also use the sparse communication technique on the server (denoted as GMC*)
to make (Wt+1-Wt) sparser than that in GMC so that when the server sends Wt+1 - Wt to workers,
the communication cost can be further reduced, compared with GMC. The convergence of GMC*
4
Under review as a conference paper at ICLR 2020
can also be theoretically proved. But in practice, GMC* will be slightly worse than GMC in terms
of accuracy. Due to the space limitation, we move both theory and experiments about GMC* to
Appendix A.
3.2	Necessity of Memory Gradient
In GMC, after sending a sparse vector mt,k (gt,k +ut,k) to the server, each worker will remember
the coordinates which are not sent and store them in ut+1,k :
ut+1,k = (1 - mt,k)	(gt,k + ut,k).	(4)
Hence we call ut,k the memory gradient, which is important for the convergence guarantee of GMC.
Here, we give an intuitive explanation about why GMC needs to remember the coordinate values
which are not sent. We consider the simple case that β = 0, which means gt,k is a stochastic
gradient of F(w) and GMC degenerates to (Aji & Heafield, 2017). Since mt,k is a sparse vector,
GMC can be seen as a method achieving sparse communication by combining stochastic coordinate
descent (SCD) (Nesterov, 2012) and DSGD. In SCD, each -VF(w)(j)ej denotes a true descent
direction. When We use a stochastic gradient Vf (w;I) to replace VF(w), -Vf (w; I)(j)ej will
make errors with high probability, especially when mt,k adopts the strategy that choosing s coordi-
nates with the largest absolute values (denoted as top-s strategy).
For further explaining the importance of memory gradient, we consider the following simple ex-
ample: let n = p =	2,	f(w; ξ1)	= (-α,	)w, f(w; ξ2)	=	(α	+ , γ)w,	where w ∈	[-1, 0] ×
[-1, 0], 0 <	< α < γ < α + . f(w; ξ1) is on the first worker and f(w; ξ2) is on the second
worker. Then we run GMC with β = 0 to solve min F(w) = 2 (f (w; ξι) + f (w; ξ2)). The optimal
solution is w* = (-1, -1)T. We adopt the top-1 strategy for mt,k in this example.
If we do not use the memory gradient, which means each worker directly sends mt,k gt,k to the
server, then the two workers will send (一α∕2, 0)t and ((α + e)/2,0)T respectively to the server.
The parameter is updated by W J W - ηt(e∕2,0). We observe that w(2) will never be updated. This
is due to the pseudo large gradient value which cheats mt,k. Since VF (w) = (∕2, (γ + )∕2)T,
we can see that the second coordinate has the true large gradient value and we should have mainly
focused on updating w(2). However, in the two stochastic gradients Vf (w; ξ1) and Vf (w; ξ2), the
first coordinate has larger absolute value. Hence, Vf (w; ξk) cheats mt,k which leads to the error.
If we use memory gradient, at the beginning, mt,1 = mt,2 = (1, 0)T. After some iterations,
mt,1 = (0, 1)T and mt,2 = (0, 1)T due to the memory gradient. Specifically, let t1, t2 be two
integers satisfying α∕ ≤ t1 < α∕ + 1, (α + )∕γ ≤ t2 < (α + )∕γ + 1, then it is easy to verify
that mct1,1 = (0, 1)T, mct2,2 = (0, 1)T, ∀c ≥ 1. It implies that ifwe use the memory gradient, both
w(1) and w(2) will be updated. Hence, GMC can make w converge to the optimum (-1, -1)T.
Hence, the memory gradient is necessary for sparse communication. It can overcome the disadvan-
tage of combining DSGD and SCD.
3.3	B enefit of Global Momentum
In GMC, each worker calculates gt,k as
gt,k = 1T X Vf (wt; ξi) - Je-(Wt - wt-ι).	(5)
pb	pηt
ξi ∈It,k
When β = 0, it degenerates to DSGD with sparse communication, which is also called gradient
dropping in (Aji & Heafield, 2017). We use gt(,GkD) to denote this degenerated version with β = 0.
We can see that GMC uses the global momentum (wt-1 -wt)∕ηt. While in DGC (Lin et al., 2018),
the gt(,DkGC) is calculated by
g(浮=Pb X Vf(wt; ξi) + βg(Ds,	(6)
ξi ∈It,k
which uses the local momentum gt(-DG1C,k) . We use ut(,DkGC) to denote the memory gradient in DGC.
5
Under review as a conference paper at ICLR 2020
For sparse mt,k, we can find that gt(,DkGC) and u(t,DkGC) only contain the information based on the
local data, while gt,k and ut,k can contain the global information due to the global momentum.
Hence, global momentum is better than local momentum when using memory gradient for sparse
communication. Assume that E[F(wt)] converges to F(w*), then Wt — wt-ι denotes the descent
direction with high probability. Since It,k ⊆ Dk, which only contains partial information of the
whole training data, the global momentum (Wt-I 一 wt)∕ηt Can make compensation for the error
between the stochastic gradient and the full gradient. Specifically, if (Wt-Wt-I)T (-VF(Wt)) ≥ 0,
then we get that
gTkVF(wt)=(4 X Vf(wt; ξi) - β-(wt - wt-ι))TVF(wt)
,	pb	pηt
ξi ∈It,k
≥p1b X Vf(wt; ξi)TVF(wt)
=(gt(,GkD))T,VF (wt).
It implies that gt,k is a better estimation of VF(wt) than gt(,GkD) . The experiments in (Lin et al.,
2018) also show that (Aji & Heafield, 2017) has some loss of accuracy with kmt,k k0 ≈ 0.001d,
while we can set such a sparsity for GMC with no loss of accuracy in practice. Hence, GMC is
better than (Aji & Heafield, 2017).
Remark 2 There are some other ways to combine global momentum and memory gradient. For
example, we can put the global momentum on the server. However, these ways lead to some loss of
performance. More discussions are put in Appendix B.
4	Convergence of GMC
In this section, we prove the convergence rate of GMC for both convex and non-convex problems.
The detailed proofs are put in Appendix C. We define a diagonal matrix Mt,k ∈ Rd×d such that
diag(Mt,k) = mt,k to replace the symbol . Then we obtain
p
wt+1 =wt - ηt	Mt,k (gt,k + ut,k),	(7)
k=1
ut+1,k =(I - Mt,k)(gt,k + ut,k),k = 1,2, . . . ,p.	(8)
For convenience, We denote Vf (w; It) = p1b Pp=I Pξi∈It,k Vf (Nt; ξi), Ut = Pp=I ut,k, gt =
Ppk=1gt,k. By eliminating Mt,k, we obtain
wt+ι =wt - ηtVf(wt；It) + β(wt - wt-ι) - ηtUt + ηtUt+ι.	(9)
We can find that if we do not use the sparse communication technique, then Ut = 0 and (9) is the
same as momentum SGD in (3).
First, we give the following lemma:
Lemma 1 Let Xt = Wt + ɪ-e(Wt 一 wt-ι) 一 ɪ-eUt. Then we have:
ηt	ηt - ηt+1
Xt+1 = Xt — ι-βVf (wt; It) +	] - β Ut+1.	(10)
Equation (10) is similar to the update equation in SGD except the additional term Ut+ι. Inspired by
this, we only need to prove the convergence of Xt and kXt - wt k. To get the convergence results,
we further make the following three assumptions:
Assumption 1 (bounded gradient) EkVf (wt; ξi)k2 ≤ G2 , ∀t.
Assumption 2 (bounded memory) EkUt k2 ≤ U2 , ∀t.
Assumption 3 (bounded parameter) EkXt - w*k2 ≤ D2,∀t.
6
Under review as a conference paper at ICLR 2020
Remark 3 Assumption 1 is common in stochastic optimization. Assumption 2 is easy to be guar-
anteed. We discuss it in Appendix D. Assumption 3 is only for convenience in the analysis for
strongly convex and convex cases. We can add one projection operation on Equation (7) to guaran-
tee Assumption 3 (if both Wt and Ut are bounded, Xt is bounded as well), which can be written as
wt+ι = ∏n(wt — ηt ∑2k=ι Mt,k (gt,k + ut,k)). The convergence can still be guaranteed if we use
the projection operation. More details are put in Appendix E.
For the new variable Xt in Lemma 1, the gap between Xt and wt has the following property:
Lemma 2 With Assumption 1 and Assumption 2, we get that
• If ηt = η, then
2	2β2(2G2 + 2U2)	2U2	2
EkXt-Wtk ≤ [	"—β)4	J + w-βγ ]η .	(II)
• If ηt = (t+rq)ρ, P ∈ [0.5,1], r > 0, q ≥ 0, then
EkXt-Wtk2 ≤ [2β2(2G2 + 2U2-mχ{(1-β)A, 2} +(ɪ ]η2.	(12)
where A = maxt0 { η2+βη2-1+…+βtη2 } t∩ satisfies (h+1+q )2p ≥ 1+β
wfte∣ e	imaxt0 {	η2	}, (/0 satι,sjbes (∣2∣ q ) ≥ 2 .
Lemma 2 implies that EkXt - wtk2 ≤ O(ηt2), i.e. the distance between Xt and wt is O(ηt). For
convenience, below We use the constant Cr,q,ρ,β,η to denote the upper bound of EkXt — W种/俏.
Ifr = 0, Cr,q,ρ,β,η denotes the upper bound in (11) and ifη = 0, Cr,q,ρ,β,η denotes the upper bound
in (12). Then we have the following convergence results:
Theorem 1	(strongly ConVex case) Let F(∙) be L-smooth andμ-Strongly convex. WithASSUmption 1,
Assumption 2, Assumption 3, and η = *(-+!), m =「T/2], we obtain
1 T-1
一 Y" E(F(Wt)- F(w*)) ≤
m
t_T -m
3A + 2Gvzc(1-β)∕μ,1,1,β,θ(1 - β
μT
where A = max{μ2D2,2(1 - β)pC(i-β)∕μ,1,1,β,0LD + μUD + 2G2 + 2U2}.
Theorem 2	(convex case) Let F(∙) be a convex function. With Assumption 1, Assumption 2, As-
sumption 3, and η = √-βr, we obtain
t∣1
T-1	2
E √^TE(F(Wt) - F(w*)) ≤ ∣∣wo -w*∣∣2+AIog(T),
t_0 t + 1
where A = 2(1 — β) PC1-β,1,0.5,β,0G + 2UD + 2G2 + 2U2.
Since PT- √t= = O(√T), Theorem 2 implies that if the objective function is convex, GMC has
a convergence rate of O(log(T)/√T).
Theorem 3 (non-convex case) Let F(∙) be L-smooth. WithAssumption 1, Assumption 2, andη = η
being a constant, we obtain
7r⅛ ∑ EkVF(wt)k2 ≤ F(WO)最 F(W*)+ Aη,
(1 - β)T t_0	Tη
where A = LGVC0,0,1，^ +___LG2_
where A =	i-β	十 2(i-β)2.
By taking η = O(1∕√T), it is easy to find that GMC has a convergence rate of O(1∕√T), if the
objective function is non-convex.
7
Under review as a conference paper at ICLR 2020
5 Experiments
We conduct experiments on a PyTorch based parameter server framework with one server and eight
workers. Each worker has access to one TITAN Xp GPU. We compare GMC with distributed
momentum SGD (DMSGD) and DGC. In the experiments of (Lin et al., 2018), DGC gets far better
performance on both accuracy and communication compression ratio than quantization methods.
Hence, we do not compare with quantization methods in this paper. We use warm-up strategy (run
DMSGD 5 epochs) for both GMC and DGC. The momentum factor masking trick is used in DGC.
The β is set as 0.9. In our experiments, we consider the communication cost on the server which
is the busiest node. It includes receiving vectors from the p workers and sending one vector to the
p workers. So the cost of DMSGD is 2pd. In GMC and DGC, since mt,k is sparse, workers send
the vectors using the structure of (key, value). The cost of each (key, value) is 2. Server sends
wt+1 - wt using this structure as well. Hence, the cost of GMC and DGC is 2(Ppk=1 kmt,k k0 +
pkwt+1 - wt k0). We define the communication compression ratio (CR) of GMC and DGC as:
1 T+4 1	p
CR = T X -(p (PkWt+1 - wtk0 + X kmt,k kO).
T t=5 pd	k=1
The CR of DMSGD is 100% (no compression). Here, all numbers have the same unit (float value).
5.1	Convex Model
We use the dataset MNIST and the model logistic regression (LR) to evaluate GMC on convex
problems. Since the sizes of dataset and model are small, mt,k directly adopts top-s strategy with
kmt,kk0 = s where s = 0.01d or 0.001d. We use 8 workers for this experiment. The weight decay
is 0.0001 and total batch size is 128. The result is shown in Figure 1. We can see that GMC gets
similar performance as DGC and DMSGD.
Figure 1: Comparison on convex model. The CRs of DGC with s = 0.01d and 0.001d are 7.87%
and 0.797% respectively. The CRs of GMC with s = 0.01d and 0.001d are 8.0% and 0.797%
respectively.
5.2	Non-convex Model
For large models, we use the approximate top-s strategy for mt,k: given a vector a, we first randomly
choose a subset S ⊂ {1,...,d} and |S| is 0.001d 〜0.0ld. We get the threshold θ such that
|{j||a(j) | ≥ θ, j ∈ S}| = 0.001|S|. Then we set mt,k by choosing the indexes in {j||a(j) | ≥ θ, j =
1, 2, . . . , d}. It implies that kmt,kk0 is approximately 0.001d.
First, we use the dataset CIFAR-10 and three popular deep models (AlexNet, ResNet20, ResNet56)
to evaluate GMC on non-convex problems. We use both 4 and 8 workers with the total batch size
128. The weight decay is set as 0.0001. Figure 2 shows the learning process about training loss and
test accuracy of the three methods. We can find that GMC and DGC have the same performance
as that of DMSGD on ResNet20 and ResNet56. Compared to the two residual networks, AlexNet
has more parameters. We can see that GMC also gets the performance as that of DMSGD and is
slightly better than DGC. Table 1 shows the finial training results of the three methods. Compared
with DMSGD, GMC can reduce the communication cost by more than 100 fold with no loss of
accuracy. This is far better than those quantization methods. Furthermore, GMC achieves slightly
better accuracy with comparable communication compression ratio, compared with DGC.
We also evaluate GMC on larger dataset ImageNet and larger model ResNet18. We use 8 workers
with total batch size 256. The weight decay is set as 0.0001. In Figure 3, GMC gets the same
8
Under review as a conference paper at ICLR 2020
--DMSGD
DGC
——GMC
90
»80
B
F
a
50
Mo-jWl
20 40 60 80 100 120 140 160
Epoch
ReSNet56,4 GPUB
0 20 40 60 80 100 120 140 160
Epoch
0 20 40 60 80 100 120 140 160
Epoch
ReSNet56,4 GPUb
ReSNrt56,8 GPUs	ReSNrt56,8 GPUs
908070605040
Figure 2: Learning process of using different non-convex models and workers on CIFAR10.
GPU	Method	AlexNet		ResNet20		ResNet56	
		Accuracy	CR	Accuracy	CR	Accuracy	CR
	DMSGD	75.76%	100%	91.93%	100%	93.23%	100%
4	DGC	75.67%	0.48%	91.35%	0.37%	93.08%	0.47%
	GMC	76.10%	0.48%	91.97%	0.36%	93.23%	0.46%
	DMSGD	76.08%	100%	91.79%	100%	93.47%	100%
8	DGC	75.42%	0.87%	91.80%	0.66%	93.26%	0.84%
	GMC	76.19%	0.85%	91.98%	0.64%	93.43%	0.81%
Table 1: Experimental results on CIFAR10.
test accuracy as that of DMSGD during the learning process. Table 2 shows that compared with
DMSGD, GMC can reduce the communication cost by more than 100 fold with no loss of accuracy
and save about 40% of training time. Compared with DGC, GMC achieves slightly better accuracy
with comparable communication compress ratio.
Method	Time (second/epoch)	Accuracy	CR
DMSGD	2536	69.79%	100%
DGC	1496	69.57%	0.84%
GMC	1495	69.88%	0.82%
Table 2: Experimental results on ImageNet.
Figure 3: Learning process.
6 Conclusion
In this paper, We propose a novel method, called global momentum COmPression (GMC), for sparse
communication in DMSGD (DSGD). To the best of our knowledge, this is the first work that proves
the convergence of DMSGD with sparse communication and memory gradient. Empirical results
show that GMC can achieve state-of-the-art performance.
9
Under review as a conference paper at ICLR 2020
References
Alham Fikri Aji and Kenneth Heafield. Sparse communication for distributed gradient descent.
In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pp.
440-445, 2017.
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. QSGD:
communication-efficient SGD via gradient quantization and encoding. In Advances in Neural
Information Processing Systems, pp. 1707-1718, 2017.
Dan Alistarh, Torsten Hoefler, Mikael Johansson, Nikola Konstantinov, Sarit Khirirat, and Cedric
Renggli. The convergence of sparsified gradient methods. In Advances in Neural Information
Processing Systems, pp. 5977-5987, 2018.
Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir, and Lin Xiao. Optimal distributed online prediction
using mini-batches. Journal of Machine Learning Research, 13:165-202, 2012.
Peng Jiang and Gagan Agrawal. A linear speedup analysis of distributed deep learning with sparse
and quantized communication. In Advances in Neural Information Processing Systems, pp. 2530-
2541, 2018.
Sai Praneeth Karimireddy, Quentin Rebjock, Sebastian U. Stich, and Martin Jaggi. Error feedback
fixes signsgd and other gradient compression schemes. In Proceedings of the 36th International
Conference on Machine Learning, pp. 3252-3261, 2019.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings
of the 3rd International Conference on Learning Representations, 2015.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in Neural Information Processing Systems, pp. 1106-1114,
2012.
Guanghui Lan. An optimal method for stochastic composite optimization. Mathematical Program-
ming, 133(1-2):365-397, 2012.
Mu Li, David G. Andersen, Jun Woo Park, Alexander J. Smola, Amr Ahmed, Vanja Josifovski,
James Long, Eugene J. Shekita, and Bor-Yiing Su. Scaling distributed machine learning with
the parameter server. In Proceedings of the 11th Symposium on Operating Systems Design and
Implementation, pp. 583-598, 2014a.
Mu Li, Tong Zhang, Yuqiang Chen, and Alexander J. Smola. Efficient mini-batch training for
stochastic optimization. In Proceedings of the 20th International Conference on Knowledge Dis-
covery and Data Mining, pp. 661-670, 2014b.
Yujun Lin, Song Han, Huizi Mao, Yu Wang, and Bill Dally. Deep gradient compression: Reducing
the communication bandwidth for distributed training. In Proceedings of the 6th International
Conference on Learning Representations, 2018.
Yurii Nesterov. Efficiency of coordinate descent methods on huge-scale optimization problems.
SIAM Journal on Optimization, 22(2):341-362, 2012.
Boris Polyak. Some methods of speeding up the convergence of iteration methods. Ussr Computa-
tional Mathematics and Mathematical Physics, 4:1-17, 12 1964.
Herbert Robbins and Sutton Monro. A stochastic approximation method. Annals of Mathematical
Statistics, 22:400-407, 1951.
Sebastian U. Stich, Jean-Baptiste Cordonnier, and Martin Jaggi. Sparsified SGD with memory. In
Advances in Neural Information Processing Systems, pp. 4452-4463, 2018.
Ilya Sutskever, James Martens, George E. Dahl, and Geoffrey E. Hinton. On the importance of ini-
tialization and momentum in deep learning. In Proceedings of the 30th International Conference
on Machine Learning, pp. 1139-1147, 2013.
10
Under review as a conference paper at ICLR 2020
Hanlin Tang, Chen Yu, Xiangru Lian, Tong Zhang, and Ji Liu. Doublesqueeze: Parallel stochastic
gradient descent with double-pass error-compensated compression. In Proceedings of the 36th
International Conference on Machine Learning, pp. 6155-6165, 2019.
Paul Tseng. An incremental gradient(-projection) method with momentum term and adaptive step-
size rule. SIAM Journal on Optimization, 8(2):506-531, 1998.
Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Terngrad:
Ternary gradients to reduce communication in distributed deep learning. In Advances in Neural
Information Processing Systems, pp. 1508-1518, 2017.
11
Under review as a conference paper at ICLR 2020
A Using Sparse Communication On S erver
If we use the sparse communication technique on the server, the update for wt can be written as:
p
wt+1 =wt - ηtat	[	mt,k	(gt,k + ut,k) + ∆t],	(13)
k=1
ut+1,k =(1 - mt,k)	(gt,k + ut,k), k = 1, 2, . . . , p,	(14)
p
∆t+1 =(1 - at)	[X mt,k	(gt,k + ut,k) + ∆t],	(15)
k=1
where gt,k is the same as that in GMC. We call this method GMC*. The mt,k and at are generated
by the worker and server respectively. The ut,k and ∆t are the memory gradients on the worker and
server respectively. We can call ut,k the local memory gradient and ∆t the global memory gradient.
Details are presented in Algorithm 2.
Algorithm 2 GMC*
1	: Initialization: p workers, W-1 = W0, β ∈ [0, 1), batch size b;
2	: Set g0,k = U0,k = ∆0 = 0, k = 1, . . . , p,
3	: for t = 0, 1, 2, ...T — 1 do
4	Workers:
5	for k = 1, 2 ...,p, each worker ParaHelly do
6	:	if t > 0 then
7	:	Receive Wt — Wt-1 from the server;
8	:	Get Wt by Wt = Wt-1 + (Wt — Wt-1);
9	:	end if
10	:	Randomly pick a mini-batch of training data It,k ⊆ Dk with |It,k| = b;
11	gt,k = p1b Pξi∈It,k ▽/(Wt； ξi) 一 pηt (Wt — Wt-I);
12	:	Generate a sparse vector mt,k ∈ {0, 1}d;
13	:	Send mt,k	(gt,k + Ut,k) to the server;
14	:	Ut+1,k = (1 — mt,k)	(gt,k +Ut,k);
15	: end for
16	Server:
17	: Generate a sparse vector at ∈ {0, 1}d;
18	:	Wt+1 = Wt — ηtat	[Ppk=1 mt,k	(gt,k + Ut,k) + ∆t];
19	:	∆t+1 = (1 — at)	[Pkp=1 mt,k	(gt,k + Ut,k) + ∆t]
20	:	Send Wt+1 — Wt to workers;
21	: end for
We eliminate mt,k, at and obtain
p
wt+1 - ηt∆t+1 =wt - ηt[	mt,k	(gt,k + ut,k) + ∆t]
k=1
=Wt — ηt[gt + U t — Ut+ι + ∆t]
By denoting ht = Ut + ∆t, We obtain
Wt+1 — ntht+1 = Wt — ηtht — ηNf(wt; It) + β(wt — Wt-i)
The equation is the same as (9). Hence our convergence analysis is also suitable for GMC*. In
Figure 4, we set at using top-s strategy with katk0 = s = 0.001d, which implies that Wt+1 — Wt
is nearly as sparse as mt,k. We can see that GMC* achieves the same loss as DMSGD. But it is
slightly worse than GMC and DMSGD in terms of accuracy.
B Other Ways for Sparse Communication
It is easy to get one way to combine memory gradient and global momentum that we can put the
global momentum Wt — Wt-1 on the server. We briefly present it in Algorithm 3.
12
Under review as a conference paper at ICLR 2020
Figure 4: Comparing GMC* with GMC and DMSGD.
Epoch
Algorithm 3
for t = 0,1, 2,...T - 1 do
Workers:
for k = 1, 2 . . . , p, each worker parallelly do
gt,k = p1b Pξi∈it,k Vf(Wt; ξi);
Generate a sparse vector mt,k ∈ {0, 1}d;
Send mt,k	(gt,k + ut,k) to the server;
ut+1,k = (1 - mt,k)	(gt,k + ut,k);
end for
Server:
Wt+1 = Wt — ηt Pp=I mt,k Θ (gt,k + ut,k) + β(wt — Wt-1);
Send Wt+1 - Wt to workers;
end for
Let {Wt}, {gt,k}, {ut,k} be the sequences produced by Algorithm 3, then we can get that
p
Wt+1 =Wt — ηt	Mt,k(gt,k + ut,k) + β(Wt — Wt-1),
k=1
ut+1,k =(I — Mt,k)(gt,k + ut,k), k = 1, 2, . . . ,p.
By eliminating Mt,k, we obtain the same equation as that in (9). Hence our convergence analysis
is also suitable for Algorithm 3. The difference from GMC is that the memory gradient ut,k in
Algorithm 3 only remembers the dropped stochastic gradient. After receiving sparse update vectors
from workers, server updates parameter using the global momentum. Compared with GMC, the
disadvantage is that its memory gradient does not contain the momentum information which can play
the role of correcting the update direction. Hence, GMC gets better performance than Algorithm 3.
(Lin et al., 2018) also points out that Algorithm 3 has some loss of convergence performance.
Another way is that we put ηt in the memory gradient. We briefly present it in Algorithm 4.
Let {Wt}, {gt,k}, {ut,k} be the sequences produced by Algorithm 4, then we can get that
p
Wt+1 =Wt —	Mt,k(gt,k + ut,k)
ut+1,k =(I — Mt,k)(gt,k + ut,k), k = 1,2, . . . ,p.
By eliminating Mt,k, we obtain the same equation as that in (9). Hence our convergence analysis is
also suitable for Algorithm 3. However, the norm of memory gradient is larger than that of GMC.
In particular, let yt,k = ut,k /々，then We have
yt+ι,k = -η- (I — Mt,k)(-1b X Vf(Wt； ξi) — Je-(wt — Wt-1) + YtG
ηt+1	p ξi∈It,k	pηt
X-----------------------------{z-----------------------------}
the same as the memory gradient of GMC
13
Under review as a conference paper at ICLR 2020
Algorithm 4
fθr t = 0,1, 2,...T — 1 do
Workers:
for k = 1, 2 . . . , p, each worker parallelly do
gt,k = ηb pξi∈ιt,k Vf(Wt; ξi)- β (Wt- wt-ι);
Generate a sparse vector mt,k ∈ {0, 1}d;
Send mt,k	(gt,k + ut,k) to the server;
ut+1,k = (1 - mt,k)	(gt,k + ut,k);
end for
Server:
Wt+1 = Wt — Pp=I mt,k Θ (gt,k + ut,k);
Send Wt+1 - Wt to workers;
end for
Ifηt is a constant, then Algorithm 4 is the same as GMC. Ifηt is non-increasing, then ηt ≥ ηt+1 and
kyt+1,k k ≥k(I- Mt,k)( Pb X ▽/(Wt； ξi) - pη- (Wt- wt-1) + yt,k)k.
Figure 5 shows the learning process of ResNet56 on CIFAR10. In the first 80 epochs, GMC and
Algorithm 4 are almost the same and both of them are better than Algorithm 3. At the step of scaling
step size by factor 0.1, the norm of memory gradient (yt,k) in Algorithm 4 is 10 fold of that in GMC.
Hence, GMC is better than Algorithm 4 after training 80 epochs.
SSO-jIlIBJI
Figure 5: Comparison with Algorithm 3 and Algorithm 4.
C Convergence Proofs
First, we introduce the Young’s Inequality as follow: ∀a, b
(a + b)2 ≤ (1 + γ)a2 + (1 + 1)b2,∀γ > 0,
γ
which is used frequently in our proofs.
14
Under review as a conference paper at ICLR 2020
C.1 Proof OF Lemma 1
l β /	、 ηt+ι 〜
Xt+1 =wt+ι + 1-β (wt+1 - Wt) - 1-β ut+1
(--耳(wt+1 - βWt) - "t+[ Ut+1
1 — P	1 — P
1
T-P
1
T-P
1
T-P
1
T-P
(wt - ηtUt - ηNf(wt It) + P(Wt - Wt-i) - PWt + ηtUt+1)-彳处gut+1
1 - P
(wt - Pwt-1 - ηtut - ηtV∕(wt；Zt) + ηtut+1)-普三ut+i
1-P
(wt - Pwt-1 - ηtut - ηtVf (wt；Zt)) + *` 隼：` Ut+1
1 - P
ηt	ηt	ηt - ηt+1
(wt - Pwt-1) - T-PUt - T-PVf (Wt；Zt) + 1 - P Ut+1
ηt	ηt - ηt+1
=xt - T-PVf(wt；lt) + ~-Γut+1∙
C.2 Proof of Lemma 2
Since Wt+1 - Wt = ηtEp=1 Mt,fc(gt,fc + ut,®), we have VY > 0,
E∣∣Wt+1 - Wtk2 ≤η2Ekgt + Utk2
=η2EkP(wt-1 - Wt)∕ηt + Vf (wt；It) + Ut∣∣2
≤(1 + γ)P2E∣∣Wt - Wt-1 k2 + (1 + 1)(2G2 + 2U2)η2∙
Y
Let Y = 1 - 1, we get that
Ekwt+1 - Wtk2 ≤PE∣∣Wt - wt-1k2 + TnL^(2G2 + 2U2)
1-P
2G2 + 2U2/ 2	C 2	Ct 2、
≤—；-3— (η2+ Bη1-1+ ∙∙∙+ B η2)∙
1-P
If ηt = η, we get that EkWt - Wt-Ik2 ≤ '2：+^" . Then we obtain
EkXt - Wtk2 ≤ (1-g)2 EkWt - Wt-Ik2 + (T-BPU2
2P2(2G2 +2U2)	2U2	2
≤[ 一(1 - P)4 — + (1 - P)2 ]η ∙
If	ηt	=	(t+qρ,ρ ∈	[0∙5,1],r	>	0, q ≥	0, let	t0	satisfy (|§+^)2ρ ≥ 孝,
A	=	maxt=1{ "t +'"谓+…+β TR }.	Then by induction,	we	get that EkWt - Wt-Ik2 ≤
SR*3SrThen we obtain
EkXt - wt Il2 ≤ (I-P)2 EkWt- Wt-Ik2 + (12ηP)2 U 2
2P2(2G2 +2U2)max{(1 - P)A, 2}	2U2	2
≤[	(1 - P)4	+ (1 - P)2 ]ηt ∙
15
Under review as a conference paper at ICLR 2020
C.3 Proof of Theorem 1
According to Lemma 1, we get that
E∣∣xt+1 - w*∣∣2
=EkXt- w*∣∣2 - 2E[占Vf(Wt;It)- ηt--ηt+1 Ut+ι]τ(Xt- w*)
+ Ek 1-βVf(Wt; It)- ηt-ηt+1 Ut+1k2
=EkXt- w*k2 - 2E[τ-βVF(wt) - ηt1-J7y1 Ut+ι]τ(Xt- w*)
+ Ek WteVf (wt; It)- ηt-ηt+1 Ut+1k2
≤EkXt - w*k2 -% EVF (Wt)T (Xt- w*) + N*-匕+1) Eu TH(Xt- w*)
1-β	1-β
+ Ek WteVf(wt;It)- ηK Ut+1k2
≤EkXt - w*k2 - t EVF(Xt)T(Xt- w*) + τ¾E(VF(Xt)- VF(Wt))T(Xt- w*)
1-β	1-β
+ 25t - ηt+1) UD + 2/2 + 2St - ηt+1)2 U2
+ —1-β —	+ (1 - β )2 +	(1 - β)2
≤EkXt- w*∣∣2 - ɪ -ηβEF (Xt)T (Xt- w*)+] -) j0(i-β)∕”,0,ι,β,0 ld
+ 2(ηt - ηt+ι) UD + 2G2η2 + 2(η - ηt+ι)2 U2
+	1 - β + (1 - β)2 +	(1 - β)	-
(16)
Using the strongly convex property, we get that
E∣∣Xt+1 - w*k2
≤(I - t)EkXt - w*k2 + I-β q∕C(1-β)∕μ,0,1,β,0LD
+ 2St - ηt+1) UD + 2G2η2 + 2St - ηt+1)2 U2
+ —1-β —	+ (1 - β)2 +	(1 - β)2
≤(I- t)EkXt- w*k2 + [2(1 - β) JC(1-β)∕μ,0,1,β,0LD + 2μUD + 2G2 + 2U 2] μt2 ∙
Since EkX1 - w*k2 ≤ D2, we get that
EkXt -
C A
w*k2 ≤ 西Vt ≥1,
where A = max{μ2D2, 2(1 - β)jC(1-β)∕μ,0,1,β,0LD + 2μUD + 2G2 + 2U2}. Using (16) again,
we get that
2E(F(Xt)- F(w*)) ≤ μt(E∣∣Xt - w*k2 - E∣∣Xt+1 - w*k2) + A.
μt
16
Under review as a conference paper at ICLR 2020
By summing UP the equation from t = T — m to T — 1, we get that
T-1
2 X E(F(xt) — F(w*))
t=T —m
T-1	A
≤ E [μt(EilXt — w*||2— EIlXt+1 — w*k2) + —]
z—μ	μt
t=T-m	产
≤μ(T — m)E∣∣xτ-m — W*
≤μ(T — m)E∣∣xτ-m — w*
N	A
Il2 + E (μEIlXt — w*∣∣2 + —)
μ^	μt
t=T -m
N 2A
∣2 + x	2A
t=T -m
A	T- 2A
S μ + μt
t	t=τ-m 尸
By setting m = ∣^T∕2], we get that PT-T* 1 ≤ 1. Then
1	T-1	3A
m X E(F(Xt) — F(w*)) ≤-t.
t=T -m
Since E[F(xt) — F(wt)] ≥ EVF(Wt)T(Xt — Wt) ≥ —G，C(i-β)∕μ,0,ι,β,01-β, we get the result
1
m
£ E(F(wt) — F(w*)) ≤ 3A + 2g√°(1-β)∕",0,1,β,0(1-e)
t=T-m	μτ
C.4 PROOF OF THEOREM 2
Similar to (16), we get that
EkXt+1 — w*∣2
=EkXt — w*∣2 — 2E[	Vf(Wt; It) — ηt - ηt+1 u t+1]T (xt — w*)
1 — p	1 — p
+ Ek ɪ-pVf (wt; It) — η∏ ut+1k2
=EkXt — w*k2 — 2E[-^η-VF(wt) — ηt ~ ηt+1 Ut+1]T(xt — w*)
1—p	1—p
+ Ek T-PVf (wt; It) — η∏ Ut+1k2
≤Ekxt — w*k2 —存 EVF (Wt)T (xt — w*) + ”[；+1) Eu THI(Xt — w*)
+ Ek 占 Vf (wt; It) — ηt — ηt+1 U t+1k2
1—p	1—p
≤Ekxt — w*k2 — Jηt EVF(wt)T(wt — w*) + 2ηt EVF(Wt)T(Wt — Xt)
1—p	1—p
+ 2(ηt ― ηt+1) UD + 2G2η2 + 2(ηt ― ηt+1)2 丁/
+ ―1—P —	+ (1 — P)2 +	(1 — P)2
≤EkXt — W*∣∣2 — 1 —pE(F(Wt) — F(w*)) + 12ηtp √C1-β,1,0.5,β,0G
2(ηt ― ηt+1) UD	2G2η2	2(ηt ― ηt+1)2 丁/
+ ―1—P —	+ (1 — P)2 +	(1 — P)2	.
17
Under review as a conference paper at ICLR 2020
Since η = √-f, We get that
t+1
T-1	2	T-1 A
E √rxTE(F(Wt) — F(W)) ≤kwo 一w*∣∣2 + E t+ι
t=0 t + 1	t=0 t + 1
≤kwo - w*k2 + Alog(T),
where A = 2(1 - β)pC1-β,1,0.5,β,0G + 2UD + 2G2 + 2U2.
C.5 Proof of Theorem 3
Since F(∙) is L-Smooth, We get that
EF(xt+1)
≤EF (xt) -
=EF (xt) -
≤EF (xt) -
≤EF(xt)-
≤EF (xt) -
η
1-β
η
1-β
η
1-β
η
1-β
η
1-β
E[VF (Xt)T Vf(wt; It)] + L Ek 占 Vf (wt; It)k2
E[VF(Xt)TVF(wt)] + 9,1Lη" 2EkVf(Wt;It)k2
2(1 - β)2
E[kVF(wt)k2 + (VF(Xt)- VF(Wt))TVF(wt)] +，?
2(1 - β)2
E[kVF(wt)k2 - LGkXt- wtk]+ LG”
2(1 - β)
EkVF (wt )k2 + LG-η2 +「
1 - β	2(1 - β)
Summing up the above equation from t = 0 to T - 1, we get that
1	T-1
(1---β)T X EkVF(wt)k2 ≤
F(wo) — F (w*)
Tη
+ Aη,
where A = LG勺1，，。+
LG2
2(1-β)2 .
D	More Discussion Ab out Assumption 2
In the convergence theorems, we need EkUtk2 ≤ U2 (Assumption 2). Since Ut = PP=ι ut,k, we
only need Ekut,kk2 to be bounded which is mainly related to the choice of mt,k. Theoretically, we
set one large threshold θ > 0 in advance and we denote θt 卜 as the s-largest value of {∖gtjk +u(jk ||j =
1,...,d}. Then in each iteration, we can choose the values such that ∣g(j) + UFk I ≥ min{E θt 卜}.
Then we obtain kUt,k k2 ≤ dθ2 and hence EkUt,k k2 ≤ dθ2. In practice, the large threshold θ is
never been used for the choice of mt,k since θts,k θ.
E	More discussion ab out Assumption 3
Ifwe do not use Assumption 3 in the proofs of Theorem 1 and Theorem 2, we can add one projection
operation on equation (7), which can be written as
p
wt+1 =∏Ω(wt - ηt EMt,k(gt,k + Ut,k)),
k=1
where Ω = {w∣kw∣∣ ≤ W} and ∣∣w*k ≤ 号.Then Wt is bounded. Hence, we reasonably assume
that ∀t,i, kU tk ≤ U, ∣∣Vf (wt； ξi)k ≤ G and we set η ≤ ηο ≤ (1/)印.Please note that the U can
be irrelevant to Ω according to Appendix D. We still define Xt as
β	ηt
Xt = Wt + 1-j (wt — wt-i) — 1-β u t.
18
Under review as a conference paper at ICLR 2020
Then xt is bounded as well.
To get the convergence results for strongly convex and convex functions, we need to prove two
essential inequalities: EkXt - w”2 ≤O(η2) and ∣∣xt+ι - w*∣∣2 ≤ ∣∣xt - w* - ^IteV/(wt；It) +
-T U t+1k2.
First, WegetthatkWt+ι - wt∣∣2 ≤ η2∣ Pp=ι Mt,k(gt,k + ut,k)∣2. According to the definition of
Mt,k, we have ∀γ > 0,
∣∣wt+ι - wt∣∣2 ≤ηIIgt + Ut∣∣2
=η2 kβ(wt - wt-i)∕ηt + Vf (wt；Zt) + ut∣∣2
≤(1 + γ)β2∣∣wt - Wt-Ik2 + (1 + 1)(2G2 +2U2)η2.
Y
By setting Y = 1-β, we obtain ∣∣wt+ι - wt∣2 ≤ β∣wt 一 Wt-Ik2 + 2G：+2UUη2. Then using the
proof in Lemma 2, we obtain the first inequality that Ekxt - wtIl2 ≤ O(η2)∙
Next, we get the relation between xt+ι and xt. According to the definition of xt, we have
llχt+ι - w*∣2 = Ilwt+1 + ι - β(Wt+1 - Wt) -	Ut+i - w*∣2
=门 1 q、2 kwt+1 - (ηt+1ut+1 + βwt + (1 - β)w*)k2.
(1 - β)2
Since ∣∣ut k ≤ U uniformly and ∣∣w* k ≤ 手,ηt ≤ ηo ≤ (1；U)W, we get that
kηt+ιut+i + βwt + (1 - β)w*k ≤ηt+iU + βW + (1 - β)WW ≤ W.
Then we obtain ηt+1Ut+1 + βwt + (1 - β)w* ∈ Ω and
1	P
kxt+i-w*k2 ≤(1z1ykwt -ηtJ2Mt,k(gt,k + ut,k) - (ηt+iUt+i + βwt + (1 - β)w*)k2
=∩----Qllwt - ηt(gt + Ut - Ut+1) - (ηt+iiit+i + βwt + (1 - β)w*)|2
(1 - β)2
*	ηt	ηt - ηt+i	2
= kxt - W - 1一β Vf(Wt； Zt)+	I - β Ut+ik .
Then similar to the analysis in Theorem 1 and Theorem 2, we can prove the convergence for strongly
convex and convex cases.
19