Under review as a conference paper at ICLR 2020
Random Matrix Theory Proves that
Deep Learning Representations of GAN-data
Behave as Gaussian Mixtures
Anonymous authors
Paper under double-blind review
Ab stract
This paper shows that deep learning (DL) representations of data produced by
generative adversarial nets (GANs) are random vectors which fall within the class
of so-called concentrated random vectors. Further exploiting the fact that Gram
matrices, of the type G = X|X with X = [x1, . . . , xn] ∈ Rp×n and xi inde-
pendent concentrated random vectors from a mixture model, behave asymptoti-
cally (as n, p → ∞) as if the xi were drawn from a Gaussian mixture, suggests
that DL representations of GAN-data can be fully described by their first two sta-
tistical moments for a wide range of standard classifiers. Our theoretical findings
are validated by generating images with the BigGAN model and across different
popular deep representation networks.
1	Introduction
The performance of machine learning methods depends strongly on the choice of the data repre-
sentation (or features) on which they are applied. This data representation should ideally contain
relevant information about the learning task in order to achieve learning with simple models and
small amount of samples. Deep neural networks (Rumelhart et al., 1988) have particularly shown
impressive results by automatically learning representations from raw data (e.g., images). However,
due to the complex structure of deep learning models, the characterization of their hidden represen-
tations is still an open problem (Bengio et al., 2009).
Specifically, quantifying what makes a given deep learning representation better than another is
a fundamental question in the field of Representation Learning (Bengio et al., 2013). Relying
on (Montavon et al., 2011) a data representation is said to be good when it is possible to build
simple models on top of it that are accurate for the given learning problem. Montavon et al. (2011)
have notably quantified the layer-wise evolution of the representation in deep networks by comput-
ing the principal components of the Gram matrix g` = {φ'(xi)lφ'(xj)}nj=ι at each layer for n
input data xι,..., xn, where φ'(x) is the representation of X at layer ' of the given DL model, and
the number of components controls the model simplicity. In their study, the impact of the repre-
sentation at each layer is quantified through the prediction error of a linear predictor trained on the
principal subspace of g`.
Pursuing on this idea, given a certain representation model x 7→ φ(x), we aim in this arti-
cle at theoretically studying the large dimensional behavior, and in particular the spectral infor-
mation (i.e., eigenvalues and dominant eigenvectors), of the corresponding Gram matrix G =
{φ(xi)lφ(xj)}nj=ι in order to determine the information encoded (i.e., the sufficient statistics)
by the representation model on a set of real data x1, . . . , xn . Indeed, standard classification and
regression algorithms -along with the last layer of a neural network (Yeh et al., 2018)- retrieve the
data information directly from functionals or the eigenspectrum of G1. To this end, though, one
needs a statistical model for the representations given the distribution of the raw data (e.g., images)
which is generally unknown. Yet, due to recent advances in generative models since the advent of
Generative Adversarial Nets (Goodfellow et al., 2014), it is now possible to generate complex data
1For instance, spectral clustering uses the dominant eigenvectors of G, while support vector machines use
functionals (quadratic forms) involving G.
1
Under review as a conference paper at ICLR 2020
structures by applying successive Lipschitz operations to Gaussian random vectors. In particular,
GAN-data are used in practice as substitutes of real data for data augmentation (Antoniou et al.,
2017). On the other hand, the fundamental concentration of measure phenomenon (Ledoux, 2005)
tells us that Lipschitz-ally transformed Gaussian vectors satisfy a concentration property. Precisely,
defining the class of concentrated vectors x ∈ E through concentration inequalities of f (x), for
any real Lipschitz observation f : E → R, implies that deep learning representations of GAN-data
fall within this class of random vectors, since the mapping x 7→ φ(x) is Lipschitz. Thus, GAN-data
are concentrated random vectors and thus an appropriate statistical model of realistic data.
Targeting classification applications by assuming a mixture of concentrated random vectors model,
this article studies the spectral behavior of Gram matrices G in the large n, p regime. Precisely, we
show that these matrices have asymptotically (as n, p → ∞ with p/n → c < ∞) the same first-
order behavior as for a Gaussian Mixture Model (GMM). As a result, by generating images using the
BigGAN model (Brock et al., 2018) and considering different commonly used deep representation
models, we show that the spectral behavior of the Gram matrix computed on these representations
is the same as on a GMM model with the same p-dimensional means and covariances. A surprising
consequence is that, for GAN data, the aforementioned sufficient statistics to characterize the quality
of a given representation network are only the first and second order statistics of the representations.
This behavior is shown by simulations to extend beyond random GAN-data to real images from the
Imagenet dataset (Deng et al., 2009).
The rest of the paper is organized as follows. In Section 2, we introduce the notion of concentrated
vectors and their main properties. Our main theoretical results are then provided in Section 3. In
Section 4 we present experimental results. Section 5 concludes the article.
Notation: In the following, we use the notation from (Goodfellow et al., 2016). [n] denotes the set
{1,..., n}. Given a vector X ∈ Rn, the '2-norm of X is given as ∣∣xk2 = PZi x2. Given aP X n
matrix M, its Frobenius norm is defined as kM k2F = Pip=1 Pjn=1 Mi2j and its spectral norm as
∣M∣ = supkxk=1 ∣M X∣. for the Hadamard product. An application F : E → F is said to be
IFkiip-Lipschitz, if ∀(x, y) ∈ E2, ∣F(X)-F(y)∣F ≤ ∣F∣iip ∙ ∣∣x — y∣E and ∣F∣iip is finite.
2	Basic notions of concentrated vectors
Being the central tool of our study, we start by introducing the notion of concentrated vectors. While
advanced concentration notions have been recently developed in (Louart & Couillet, 2019) in order
to specifically analyze the behavior of large dimensional sample covariance matrices, for simplicity,
we restrict ourselves here to the sufficient so-called q-exponentially concentrated random vectors.
Definition 2.1 (q-exponential concentration). Given a normed space (E, ∣ ∙ ∣e) and a real q, a
random vector X ∈ E is said to be q-exponentially concentrated if for any 1-Lipschitz real function
f : E → R, there exists C ≥ 0 independent of dim(E) and σ > 0 such that for all t ≥ 0
P{∣f(x)- Ef(x)∣ >t}≤ CeTtG)	(1)
which we denote X ∈ Eq (σ | E, ∣ ∙ ∣∣e ). We simply write X ∈ Eq (11 E, ∣ ∙ ∣∣e ) if the tail parameter
σ does not depend on dim(E), and x ∈ Eq(1)for x a scalar real random variable.
Therefore, concentrated vectors are defined through the concentration of any 1-Lipschitz real scalar
“observation”. One of the most important examples of concentrated vectors are standard Gaussian
vectors. Precisely, we have the following proposition. See (Ledoux, 2005)) for more examples such
as uniform and Gamma distribution.
Proposition 2.2 (Gaussian vectors (Ledoux, 2005)). Let d ∈ N and X 〜N(0, Id). Then X is a
2-exponentially concentrated vector independently on the dimension d, i.e. X ∈ E2(l | Rd, ∣ ∙ ∣∣).
Concentrated vectors have the interesting property of being stable by application of Rd → Rp
vector-Lipschitz transformations. Indeed, Lipschitz-ally transformed concentrated vectors remain
concentrated according to the following proposition.
Proposition 2.3 (Lipschitz stability (Louart & Couillet, 2019)). Let X ∈ Eq(11 E, ∣ ∙ ∣∣e) and
G : E → F a Lipschitz application with Lipschitz constant ∣G ∣lip which may depend on dim(F).
Then the concentration property on X is transferred to G (X), precisely
X ∈Eq(11 E, ∣∙∣E) ⇒ G(X) ∈Eq(∣G∣iip I F,IH∣f).	⑵
2
Under review as a conference paper at ICLR 2020
Note importantly for the following that the Lipschitz constant of the transformation G must be con-
trolled, in order to constrain the tail parameter of the obtained concentration.
In particular, we have the coming corollary to Proposition 2.3 of central importance in the following.
Corollary 2.4. Let G1, . . . , Gn : Rd → Rp a set of n Lipschitz applications with Lipschitz
constants kGi klip. Let G : Rd×n → Rp×n be defined for each X ∈ Rd×n as G(X) =
[G1(X:,1),...,Gn(X:,n)]. Then,
Z ∈Eq(1 ∣Rd×n, IHIF) ⇒ G(Z) ∈Eq 卜Up kGiklip | Rp×n,∣H∣F).	⑶
Proof. This is a consequence of Proposition 2.3 since the map G is supi IGi Ilip-Lipschitz with
respect to (w.r.t.) the Frobenius norm. Indeed, for X, H ∈ Rd×n : IG(X + H) - G(X)I2F ≤
Pi=I kGik2ip ∙kH,ik2 ≤ SupikGik2ip ∙kHkF.
3 Main Results
3.1	GAN data : An Example of Concentrated Vectors
Concentrated random vectors are particularly interesting from a practical standpoint for real data
modeling. In fact, unlike simple Gaussian vectors, the former do not suffer from the constraint of
having independent entries which is quite a restrictive assumption when modeling real data such
as images or their non-linear features (e.g., DL representations). The other modeling interest of
concentrated vectors lies in their being already present in practice as alternatives to real data. Indeed,
adversarial neural networks (GANs) have the ability nowadays to generate random realistic data
(for instance realistic images) by applying successive Lipschitz operations to standard Gaussian
vectors (Goodfellow et al., 2014).
A GAN architecture involves two networks, a generator model which maps random Gaussian noise
to new plausible synthetic data and a discriminator model which classifies real data as real (from
the dataset) or fake (for the generated data). The discriminator is updated directly through a binary
classification problem, whereas the generator is updated through the discriminator. As such, the two
models are trained alternatively in an adversarial manner, where the generator seeks to better deceive
the discriminator and the former seeks to better identify the fake data (Goodfellow et al., 2014).
In particular, once both models are trained (when they reach a Nash equilibrium), DL representations
of GAN-data -and GAN-data themselves- are schematically constructed in practice as follows:
Real Data ≈ GAN Data = FN ◦…◦ Fι(z), where Z 〜N(0, Id),	(4)
where d stands for the input dimension of the generator model, N the number of layers, and the
Fi ’s either Fully Connected Layers, Convolutional Layers, Pooling Layers, Up-sampling Layers
and Activation Functions, Residual Layers or Batch Normalizations. All these operations happen to
be Lipschitz applications. Precisely,
z~Ng,I)
Generator
Representation Network
Real / Fake
Lipschitz operation
Concentrated Vectors
Figure 1: Deep learning representations of GAN-data are constructed by applying successive Lips-
chitz operations to Gaussian vectors, therefore they are concentrated vectors by design, since Gaus-
sian vectors are concentrated and thanks to the Lipschitz stability in Proposition 2.3.
3
Under review as a conference paper at ICLR 2020
•	Fully Connected Layers and Convolutional Layers: These are affine operations which can be
expressed as
Fi(x) = Wix + bi, for Wi the weight matrix and bi the bias vector.
Here the Lipschitz constant is the operator norm (the largest singular value) of the weight matrix
Wi, thatis kFikIip = SUpu=O kWUU2k2 .
•	Pooling Layers and Activation Functions: Most commonly used activation functions and pool-
ing operations are
ReLU(x) = max(0, x), MaxPooling(x) = [max(xS1 ), . . . , max(xSq)]|,
where Si ’s are patches (i.e., subsets of [dim(x)]). These are at most 1-Lipschitz operations with
respect to the Frobenius norm. Specifically, the maximum absolute sub-gradient of the ReLU acti-
vation function is 1, thus the ReLU operation has a Lipschitz constant of 1. Similarly, we can show
that the Lipschitz constant of MaxPooling layers is also 1.
•	Residual Connections: Residual layers act the following way
Fi(X) = X + F(I) ◦…oF(')(x),
where the Fi(j) ’s are Fully Connected Layers or Convolutional Layers with Activation Functions,
and which are Lipschitz operations. Thus Fi is a Lipschitz operation with Lipschitz constant
bounded by 1 + Q'=1 kFiej) klip.
•	Batch Normalization (BN) Layers: They consist in statistically standardizing (Ioffe & Szegedy,
2015) the vectors of a small batch B = {Xi}ib=1 ⊂ Rd as follows: for each Xk ∈ B
a
Fi(Xk) = diag
(Xk - μBId) + b
where 〃b = db P
k=1 Pd=ι[xk]i, σB = db Pk=ι Pd=ι([xk]i - μB)2, a,b ∈ Rd are parameters
to be learned and diag(v) transforms a vector v to a diagonal matrix with its diagonal entries being
those of v. Thus BN is aLiPschitz transformation with LiPschitz ConstantkFiklip = sup" 器—|.
σB+
Therefore, as illustrated in Figure 1, since standard Gaussian vectors are concentrated vectors as
mentioned in ProPosition 2.2 and since the notion of concentrated vectors is stable by LiPschitz
transformations thanks to ProPosition 2.3, GAN-data (and their DL rePresentations) are concen-
trated vectors by design given the construction in Equation (4). Moreover, in order to generate
data belonging to a sPecific class, Conditional GANs have been introduced (Mirza & Osindero,
2014); once again data generated by these models are concentrated vectors as a consequence of
Corollary 2.4. Indeed, a generator of a Conditional GAN model can be seen as a set of multiPle gen-
erators where each generates data of a sPecific class conditionally on the class label (e.g., BigGAN
model (Brock et al., 2018)).
Yet, in order to ensure that the resulting LiPschitz constant of the combination of the above oPer-
ations does not scale with the network or data size, so to maintain good concentration behaviors,
a careful control of the learned network Parameters is needed. This control haPPens to be already
considered in Practice in order to ensure the stability of GANs during the learning Phase, notably to
generate realistic and high-resolution images (Roth et al., 2017; Brock et al., 2018). The control of
the LiPschitz constant of rePresentation networks is also needed in Practice in order to make them
robust against adversarial examPles (Szegedy et al., 2013; Gulrajani et al., 2017). This control is
Particularly ensured through sPectral normalization of the affine layers (Brock et al., 2018), such
as Fully Connected Layers, Convolutional Layers and Batch Normalization. Indeed, sPectral nor-
malization (Miyato et al., 2018) consists in applying the operation W J W/σι (W) to the affine
layers at each backward iteration of the back-ProPagation algorithm, where σ1(W ) stands for the
largest singular value of the weight matrix W. Brock et al. (2018), have notably observed that,
without spectral constraints, a subset of the generator layers grow throughout their GAN training
and explode at collapse. They thus suggested the following spectral normalization -which happens
to be less restrictive than the standard spectral normalization W J W/σι(W) (Miyato et al.,
2018)- to the affine layers:
W J W — (σι(W) — σ*) Uι(W)vι(W)|	(5)
4
Under review as a conference paper at ICLR 2020
1σ eulav ralugnis tsegraL
0
65432 1
0
20
0
40
0
60
0
80
00
0
,
1
Iterations
Figure 2: Behavior of the largest singular value of a weight matrix in terms of the iterations of
a random walk (See proposition 3.1), without spectral normalization in (blue) and with spectral
normalization in (red). The (black) lines correspond to the theoretical bound yzσ2 + η2d1d0 for
different σjs. We took d° = di = 100 and η = 1/d0.
where u1(W) and v1(W) denote respectively the left and right largest singular vectors of W, and
σ* is an hyper-parameter fixed during training.
To get an insight about the influence of this operation and to ensure that it controls the Lipschitz
constant of the generator, the following proposition provides the dynamics of a random walk in the
space of parameters along with the spectral normalization in Equation (5). Indeed, since stochastic
gradient descent (SGD) consists in estimating the gradient of the loss function on randomly selected
batches of data, it can be assimilated to a random walk in the space of parameters (Antognini &
Sohl-Dickstein, 2018).
Proposition 3.1 (Lipschitz constant control). Let σ > 0 and G be a neural network ComPosed of N
affine layers, each one of input dimension di-1 and output dimension difori ∈ [N], with 1-Lipschitz
activation functions. Assume that the weights of G at layer i + 1 are initialized as U([— √=, √=]),
and consider the following dynamics with learning rate η:
W — W — ηE, with Eij 〜N(0,1)
W — W — max(0,σι(W) — σ*) ui(W)vι(W)|.
Then, ∀ε > 0, the LiPschitz constant of G is bounded at convergence with high Probability as:
N
kGkiip ≤ Y (ε + √σ2 + η2didi-i).
i=1
(6)
(7)
Proof. The proof is provided in Appendix B.	□
Proposition 3.1 shows that the Lipschitz constant of a neural network is controlled when trained with
the spectral normalization in Equation (5). In particular, recalling the notations in Proposition 3.1,
in the limit where d → ∞ with 手-------> Yi ∈ (0, ∞) for all i ∈ [N] and choosing the learning rate
di-1
η = O(d-i), the Lipschitz constant of G is of order O(1) if it has finitely many layers N and σ* is
constant. Therefore, with this spectral normalization, it can be assumed that kGklip = O(1) when
dimensions grow. Figure 2 depicts the behavior of the Lipschitz constant of a linear layer with and
without spectral normalization in the setting of Proposition 3.1, which confirms the obtained bound.
3.2	Mixture of Concentrated Vectors
In this section, we assume data to be a mixture of concentrated random vectors with controlled O(1)
Lipschitz constant (e.g., DL representations of GAN-data as we discussed in the previous section).
Precisely, let x1, . . . , xn be a set of mutually independent random vectors in Rp. We suppose that
these vectors are distributed as one of k classes of distribution laws μι,...,μk with distinct means
{m'}k=ι and “covariances” {C'}k=ι defined receptively as
m' = Exi 〜μg[xi], C' = Exi 〜μ'[xixi].	(8)
5
Under review as a conference paper at ICLR 2020
For some q > 0, we consider a q-exponential concentration property on the laws μ', in the sense that
for any family of independent vectors yι,..., y§ sampled from μg, [yι,..., y§] ∈ Eq(11 Rp×s, k ∙
kF). Without loss of generality, we arrange the xi’s in a data matrix X = [x1, . . . , xn] such that,
for each ' ∈ [k], X]+P'-ι %,..., xP' 1 % 〜μ', where n` stands for the number of Xi's sampled
from μ'. In particular, we have the concentration of X as
X ∈Eq (11 Rp×n, k∙kF).	(9)
Such a data matrix X can be constructed through Lipschitz-ally transformed Gaussian vectors (q =
2), with controlled Lipschitz constant, thanks to Corollary 2.4. In particular, DL representations
of GAN-data are constructed as such, as shown in Section 3.1. We further introduce the following
notations that will be used subsequently.
M = [m1,...,mk] ∈ Rp×k, J = [j1,...,jk] ∈Rn×kandZ= [z1,...,zn] ∈ Rp×n,
where j` ∈ Rn stands for the canonical vector selecting the Xi's of distribution μg, defined by
(j`)i = %〜μg, and the Zi's are the centered versions of the Xi's, i.e. Zi = Xi - m` for Xi 〜μg.
3.3	Gram Matrices of Concentrated Vectors
Now we study the behavior of the Gram matrix G = ɪ X1X in the large n,p limit and under
the model of the previous section. Indeed, G appears as a central component in many classification,
regression and clustering methods. Precisely, a finer description of the behavior of G provides access
to the internal functioning and performance evaluation of a wide range of machine learning methods
such as Least Squares SVMs (AK et al., 2002), Semi-supervised Learning (Chapelle et al., 2009)
and Spectral Clustering (Ng et al., 2002). Indeed, the performance evaluation of these methods has
already been studied under GMM models in (Liao & Couillet, 2017; Mai & Couillet, 2017; Couillet
& Benaych-Georges, 2016) through RMT. On the other hand, analyzing the spectral behavior of
G for DL representations quantifies their quality -through its principal subspace (Montavon et al.,
2011)- as we have discussed in the introduction. In particular, the Gram matrix decomposes as
G = 1JM1MJ | + 1Z1Z + 1(JM 1Z + Z1MJ |).	(10)
p	pp
Intuitively G decomposes as a low-rank informative matrix containing the class canonical vectors
through J and a noise term represented by the other matrices and essentially Z| Z. Given the form
of this decomposition, RMT predicts -through an analysis of the spectrum of G and under a GMM
model (Benaych-Georges & Couillet, 2016)- the existence of a threshold ξ function of the ratio
p/n and the data statistics for which the dominant eigenvectors of G contain information about the
classes only when kM|Mk ≥ ξ asymptotically (i.e., only when the means of the different classes
are sufficiently distinct).
In order to characterize the spectral behavior (i.e., eigenvalues and leading eigenvectors) ofG under
the concentration assumption in Equation (9) on X, we will be interested in determining the spectral
distribution L = ɪ Pn=ι δλi of G, with λι,...,λn the eigenvalues of G, where δχ stands for the
Dirac measure at point x. Essentially, to determine the limiting eigenvalue distribution as p, n → ∞
and p/n → c ∈ (0, ∞), a conventional approach in RMT consists in determining an estimate of the
Stieltjes transform (Silverstein & Choi, 1995) mL ofL, which is defined for some z ∈ C \ Supp(L)
mL(z) = Z dL(λ) = 1tr ((G - ZIn)T) .	(11)
λλ-z n
Hence, quantifying the behavior of the resolvent of G defined as R(z) = (G + zIn)-1 determines
the limiting measure ofL through mL(z). Furthermore, since R(z) and G share the same eigenvec-
tors with associated eigenvalues λ^⅛z, the projector matrix corresponding to the top m eigenvectors
U = [uι,..., Um] of G can be calculated through a Cauchy integral UU| = 2∏i jγ R(-z)dz
where γ is an oriented complex contour surrounding the top m eigenvalues of G.
To study the behavior of R(z), we look for a so-called deterministic equivalent (Hachem et al.,
2007) R(Z) for R(z), which is a deterministic matrix that satisfies for all A ∈ Rn×n and all u, V ∈
Rn of respectively bounded spectral and Eucildean norms, ɪ tr(AR(z)) - n tr(AR(z)) → 0
and u|(R(Z) - R(Z))v → 0 almost surely as n → ∞. In the following, we present our main
result which gives such a deterministic equivalent under the concentration assumption on X in
Equation (9) and under the following assumptions.
6
Under review as a conference paper at ICLR 2020
Assumption 3.2. As p → ∞,
1.	p/n → C ∈ (0, ∞),	2. Thenumberofdasses k is bounded, 3. km`k = O(√p).
Theorem 3.3 (Deterministic Equivalent for R(z)). Under the model described in Section 3.2 and
Assumptions 3.2, we have R(Z) ∈ Eq(p-1/2 | Rn×n, k ∙ ∣∣f). Furthermore,
悭(Z)- R(Z)h O (SogF! ,R(Z)=I diag {ι⅛ }k=1+1JωzJ|	(12)
With Qz=M TQ(Z)M G) diag n f(⅛⅛ ok=1 and Q(Z)=( c⅛ Pk=I 1+C⅛)+ZIp)
where J*(z) = [6；(z),...,6. (z)]t is the unique fixed point ofthe system ofequations
δ'(z) = ptr
「( C1k X 1¾+ZIp
for each ` ∈ [k].
Sketch of proof. The first step of the proof is to show the concentration of R(Z). This comes from
the fact that the application X 7→ R(Z) is 2Z-3/2p-1/2-Lipschitz w.r.t. the Frobenius norm, thus
We have by Proposition 2.3 that R(Z) ∈ Eq(PT/21 Rn×n, ∣ ∙ ∣f). The second step consists in
estimating ER(Z) through a deterministic matrix R(Z). Indeed, R(Z) can be expressed as a function
of Q(Z) = (XX|/p + ZIp)-1 as R(Z) = Z-1(In - X|Q(Z)X/p), and exploiting the result
of (Louart & Couillet, 2019) Which shoWs that EQ(Z) can be estimated through Q(Z), We obtain the
estimator R(Z) for ER(Z). A more detailed proof is provided in Section A.3 of the Appendix. □
This result alloWs specifically to (i) describe the limiting eigenvalues distribution ofG, (ii) determine
the spectral detectability threshold mentioned above, (iii) evaluate the asymptotic “content” of the
leading eigenvectors of G and, much more fundamentally, (iv) infer the asymptotic performances
of machine learning algorithms that are based on simple functionals of G (e.g., LS-SVM, spectral
clustering etc.). Looking carefully at Theorem 3.3 We see that the spectral behavior of the Gram
matrix G computed on concentrated vectors only depends on the first and second order statistics of
the laws μ' (their means m` and “covariances” e`). This suggests the surprising result that G has
the same behavior as When the data folloW a GMM model With the same means and covariances.
The asymptotic spectral behavior of G is therefore universal with respect to the data distribution
laws which satisfy the aforementioned concentration properties (for instance DL representations of
GAN-data). We illustrate this universality result in the next section by considering data as CNN
representations of GAN generated images.
segamI NAG segamI lae
Figure 3: (Top) GAN generated images using the BigGAN model Brock et al. (2018). (Bottom)
Real images selected from the Imagenet dataset Deng et al. (2009). We considered n = 1500 images
from k = 3 classes which are Mushroom, Pizza and Hamburger.
7
Under review as a conference paper at ICLR 2020
segamI NAG
Eigenvector 2
densenet201
segamI lae
Figure 4: (Top) Spectrum and leading eigenspace of the Gram matrix for CNN representations of
GAN generated images using the BigGAN model Brock et al. (2018). (Bottom) Spectrum and
leading eigenspace of the Gram matrix for CNN representations of real images selected from the
Imagenet dataset Deng et al. (2009). Columns correspond to the three representation networks
(Resnet50, VGG16 and Densenet201).
resnet50
Eigenvector 2
(s-eɔs 301)⅛suaq
4	Application TO CNN Representations of GAN-GENERATED Images
In this section, We consider n = 1500 data xι,..., Xn ∈ Rp as CNN representations -across
popular CNN architectures of different sizes P- of GAN-generated images using the generator of the
Big-GAN model (Brock et al., 2018). We further use real images from the Imagenet dataset (Deng
et al., 2009) for comparison. In particular, we empirically compare the spectrum of the Gram matrix
of this data with the Gram matrix of a GMM model with the same means and covariances. We
also consider the leading 2-dimensional eigenspace of the Gram matrix which contains clustering
information as detailed in the previous section. Figure 3 depicts some images generated using the
Big-GAN model (Top) and the corresponding real class images from the Imagenet dataset (Bottom).
The Big-GAN model is visually able to generate highly realistic images which are by construction
concentrated vectors, as discussed in Section 3.1.
Figure 4 depicts the spectrum and leading 2D eigenspace of the Gram matrix computed on CNN rep-
resentations of GAN generated and real images (in gray), and the corresponding GMM model with
same first and second order statistics (in green). The Gram matrix is seen to follow the same spectral
behavior for GAN-data as for the GMM model which is a natural consequence of the universality
result of Theorem 3.3 with respect to the data distribution. Besides, and perhaps no longer surpris-
ingly, we further observe that the spectral properties of G for real data (here CNN representations
of real images) are conclusively matched by their Gaussian counterpart. This both theoretically
and empirically confirms that the proposed random matrix framework is fully compliant with the
theoretical analysis of real machine learning datasets.
8
Under review as a conference paper at ICLR 2020
5	Conclusion
Leveraging on random matrix theory (RMT) and the concentration of measure phenomenon, we
have shown through this paper that DL representations of GAN-data behave as Gaussian mixtures
for linear classifiers, a fundamental universal property which is only valid in high-dimension of
data. To the best of our knowledge, this result constitutes a new approach towards the theoretical
understanding of complex objects such as DL representations, as well as the understanding of the
behavior of more elaborate machine learning algorithms for complex data structures. In addition,
the article explicitly demonstrated our ability, through RMT, to anticipate the behavior of a wide
range of standard classifiers for data as complex as DL representations of the realistic and surprising
images generated by GANs. This opens the way to a more systematic analysis and improvement of
machine learning algorithms on real datasets by means of large dimensional statistics.
References
Suykens Johan AK et al. Least squares support vector machines. World Scientific, 2002.
Joseph Antognini and Jascha Sohl-Dickstein. Pca of high dimensional random walks with com-
parison to neural network training. In Advances in Neural Information Processing Systems, pp.
10307-10316, 2018.
Antreas Antoniou, Amos Storkey, and Harrison Edwards. Data augmentation generative adversarial
networks. arXiv preprint arXiv:1711.04340, 2017.
Florent Benaych-Georges and Romain Couillet. Spectral analysis of the gram matrix of mixture
models. ESAIM: Probability and Statistics, 20:217-237, 2016.
Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1798-1828, Aug 2013.
ISSN 0162-8828. doi: 10.1109/TPAMI.2013.50.
Yoshua Bengio et al. Learning deep architectures for ai. Foundations and trendsR in Machine
Learning, 2(1):1-127, 2009.
Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural
image synthesis. arXiv preprint arXiv:1809.11096, 2018.
Olivier Chapelle, Bernhard Scholkopf, and Alexander Zien. Semi-supervised learning (chapelle, o.
et al., eds.; 2006)[book reviews]. IEEE Transactions on Neural Networks, 20(3):542-542, 2009.
Romain Couillet and Florent Benaych-Georges. Kernel spectral clustering of large dimensional data.
Electronic Journal of Statistics, 10(1):1393-1454, 2016.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-
erarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248-255. Ieee, 2009.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, pp. 2672-2680,
2014.
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1.
MIT Press, 2016.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Im-
proved training of wasserstein gans. In Advances in Neural Information Processing Systems 30,
pp. 5767-5777. 2017.
Walid Hachem, Philippe Loubaton, Jamal Najim, et al. Deterministic equivalents for certain func-
tionals of large random matrices. The Annals of Applied Probability, 17(3):875-930, 2007.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
9
Under review as a conference paper at ICLR 2020
Michel Ledoux. The concentration of measure phenomenon. Number 89. American Mathematical
Soc., 2005.
Zhenyu Liao and Romain Couillet. Random matrices meet machine learning: A large dimensional
analysis of ls-svm. In ICASSP, pp. 2397-2401. IEEE, 2017.
Cosme Louart and Romain Couillet. Concentration of measure and large random matrices with an
application to sample covariance matrices. submitted, 2019.
Xiaoyi Mai and Romain Couillet. A random matrix analysis and improvement of semi-supervised
learning for large dimensional data. arXiv preprint arXiv:1711.03404, 2017.
Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint
arXiv:1411.1784, 2014.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization
for generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018.
GrASgoire Montavon, Mikio L Braun, and Klaus-Robert MAller. Kernel analysis of deep networks.
Journal of Machine Learning Research, 12(Sep):2563-2581, 2011.
Andrew Y Ng, Michael I Jordan, and Yair Weiss. On spectral clustering: Analysis and an algorithm.
In Advances in neural information processing systems, pp. 849-856, 2002.
Kevin Roth, Aurelien Lucchi, Sebastian Nowozin, and Thomas Hofmann. Stabilizing training of
generative adversarial networks through regularization. In Advances in Neural Information Pro-
cessing Systems 30, pp. 2018-2028. 2017.
David E Rumelhart, Geoffrey E Hinton, Ronald J Williams, et al. Learning representations by
back-propagating errors. Cognitive modeling, 5(3):1, 1988.
Jack W Silverstein and Sang-Il Choi. Analysis of the limiting spectral distribution of large dimen-
sional random matrices. Journal of Multivariate Analysis, 54(2):295-309, 1995.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Chih-Kuan Yeh, Joon Kim, Ian En-Hsu Yen, and Pradeep K Ravikumar. Representer point selection
for explaining deep neural networks. In Advances in Neural Information Processing Systems, pp.
9291-9301, 2018.
A Proof of Theorem 3.3
A.1 Setting of the proof
For simplicity, we will only suppose the case k = 1 and we consider the following notations that
will be used subsequently.
X = Exi, C = E[xiX∣], Xo = X — Xin, Co = E[XoX∣/n].
Let
X-i = (X1 , . . . , Xi-1 , 0, Xi, . . . , Xn)
the matrix X with a vector of zeros at its ith column.
Denote the resolvents
And let
X|X
-----+ ZIn
p
XX|
Q = (+ ZIp
Q-i
XX T
P
XiXT
P
+ ZIp
(13)
(Ci C δ
(14)
R
10
Under review as a conference paper at ICLR 2020
where δ is the solution to the fixed point equation
G £
δ =Ltr
P
A.2 BASIC TOOLS
Lemma A.1 ((Ledoux, 2005)). Let z ∈ Eq(1 ∣Rp, ∣∣∙ ∣∣) and M ∈ Eq(1 ∣Rp×n, ∣∣∙ ∣∣f). Then., for
some numerical constant C > 0
•	EkZk ≤ IlEZ∣ + C√p, EIlZ∣∞ ≤ IIEZ∣∞ + C√logp.
•	EIlMk ≤ IIEMk + C√p"÷^, EIlMIlF ≤ IIEMIIF + C√pn.
Lemma A.2. Denote Qx = (XxT + ZIp)-1, we have:
Qx x
x
llx12 + Z
and IQx∣∣, xQx = O(1).
Moreover f ∣∣x∣∣ ≥ √p, IIQx∣∣ = O(PT/2).
Proof. Since ZQX = IP — QXxxτ :
zQxx = x — Ilxll QXx,
and we recover the first identity of the Lemma.
And since the matrix C0 is nonnegative symmetric, we have :
石.11 Co + xxτ	ʌ-1 _ c(1 + δ)x
Qx =H ZIP x ≤	r.
V 1 + δ P) IxI2 + zc(1 + δ)
TherefCre 而/ɔ而— c(l+δ)kχk	— /ɔ(1 ) q∏r!∙
Therefore, xQx 一 口W2+次(1+6) = O(I) and:
IQ xI
c(1 + δ)Ix∣∣	≤ I呼=O(I) if IxI ≤ 1,
llxll2 + ZC(I + δ) ^ I c(1 + δ) = O(1) if IxI ≥ 1.
Ilx1
□
Proposition A.3. xτE[Q]x = xτQx + O (Jlogp
Proof. Let us bound:
∣xτQx — xτQx∣ ≤ 1c+~δ
1
xQxixτQx (PxτQ-ig —
+ 1E
p
[xτQ-ixix: QCQ xi
E
Now let US consider a supplementary random vector xn+1 following the same low as the xi’s and in-
dependent of X. We divide the set I = [n +1] into two sets 11 and 12 of same cardinality ([n+1 C ≤
#I 1, #I2 ≤ 「n+1 ]), we note X1 = (xi ∣ i ∈ 11), X2 = (xi ∣ i ∈ 12) and we introduce the
diagonal matrices ∆ = diag (Px：Q-ixi — δ ∣ i ∈ 11), D = diag (1 + p+ɪx：Qxi ∣ i ∈ 12 ).
11
Under review as a conference paper at ICLR 2020
We have the bound:
t∖	T
xQxiχi Qx
1 + p *Qi
一 C	一一T 挤;
xn+1Q+(n+1)xixi QX
1
p2
Ie [itdx
T 2-2
Q+(n+1)X 1 ∆X| QX
≤v
3ITDX T Q+(n+1)X 1 ∆2X T Q+(n+1)X 2 D1
Po	T 、	， T	T 、	， T
1-2
X
1)
十
(n
d*
T2-2
X
1 -P
2
∣∣0∣2∣∣∆∣∣2
E [xQCQχ
1
E —XTQXi XTQX
P	t t
thanks to Lemma A.1 and Lemma A.2 (the spectral norm of ∆ and D is just an infinity norm if we
see them as random vectors of Rn). We can bound P ∣E [xtQ-iX^xTQCQx] ∣ the same way to
obtain the result of the proposition.	□
--	-	=TjA=YT..	.	----.
ProposιtιonA.4. kE[x∣Q-iX-i]----------Q+^∣∣ = O(√l°gP)
Proof. Considering u ∈ Rn such that ∣u∣ = 1:
E[x： QτX-iu]—
—T Z-⅛ - -i T
XTQXITU
1 + δ
n
X UjE
j=i
j=i
≤ √n E
xlQ - Xj
_____-j___
1 +1 xjQ 二Xj
+
1+δE 卜:QIXj-XQXji
(where i = j)
≤ √n E
∣ + √n ∣ E [xτQ -a X — XTQx] ∣,
E
E
≤ ∖
E
E
where the first term is treated the same way as we did in the proof of Proposition A.3 and the second
term is bounded thanks to Proposition A.3	□
A.3 Main body of THE proof
ProofofTheorem 3.3. Recall the definition of the resolvents R and Q in Equation (13). The first
step of the proof is to show the concentration of R. This comes from the fact that the application
Φ : X → (XτX + ZIn)-I is 2z-3/2-LiPSChitz w.r.t. the Frobenius norm. Indeed, by the matrix
identity A — B = A(BT — A-I)B, We have
Φ(X) — Φ(X + H) = Φ(X )(H τX + (X + H )τH )Φ(X + H)
And by the bounds ∣AB∣f ≤ ∣∣A∣ ∙ IIBkF, ∣Φ(X )X Tk ≤ ZT/2 and ∣∣Φ(X )∣ ≤ z-1, we have
2
∣φ(x + H) - φ(X)∣∣f ≤ z3∕2IlHkF∙
Therefore, given X ∈ Eq(11 Rp×n, ∣ ∙ ∣∣f) and since the application X → R = Φ(X/√p) is
2z-3∕2p-1/2-Lipschitz, we have by Proposition 2.3 that R ∈ Eq(PT/21 Rn×n, ∣ ∙ ∣∣f).
The second step consists in estimating ER(Z) through a deterministic matrix R. Indeed, by the
identity (MTM+zI)-1 MT = MT (MMT+zI)-1, the resolvent R can be expressed in function
12
Under review as a conference paper at ICLR 2020
of Q as follows
R = 1
Z
X TQX
P
(15)
thus a deterministic equivalent for R can therefore be obtained through a deterministic equivalent of
the matrix XtQX. However, as demonstrated in Louart & Couillet (2019), the matrix Q has as a
deterministic equivalent the matrix Q defined in equation 14. In the following, we aim at deriving a
deterministic equivalent for PXtQX in function of Q. Let u and v be two unitary vectors in Rn,
and let us estimate
∆ ≡ E "uT (XQX - XQX! v# = 1E [UTXTQCQXV - 1 uTXTQXXTQXV
∖ p	P) p	1 + δ	p
With the following matrix identities (to explore the independence of the columns of X):
Q = Q-i - 1 Q-iXixTQ , Qxi = τ~Q-Tx—，	A - B = A(BT- A-I)B
p	1 + P xi Q-ixi
and the decomposition QXXT = Pn=I Qxixi, We obtain:
rι	_ _	_ . ~_ ______ _ ：___ __________ ~________
1 E UTXTQ-QQXv UTXTQ-ixix：QXv 1 UTXTQ-ixix：QCQXv
p2	=	1 + δ	1 + 1 xi Q-ixi P	1 + δ
1 J JuTX-iQ-iCQX-iv	UTX-iQ—xHQX-v
E E	-
p2 W	[	1 + δ	1 + PxiQ-ixi
T X~*	X~* FT
I Uixi Q-iCQX-iv
+	1+j
T FT- T X~*	χ~*
i viu	-iQ-iCQxi
+	1+j
+ Uivi
T χ~*	χ~*
xi Q-iCQxi
-1+j-
Uixl Q-i xixl Q X-iv	ViUTXT iQ-ixixJ Q xi	xi Q-ixix] Q xi
1 + 1 xi Q-ixi	1 + p xi Q-ixi	”"i 1 + p xi Q-ixi
_____ _ _ _. ~____
1 uτX TQ-ixixi QCQ Xv
P	1 + δ
We can show with Holder,s inequality and the concentration bounds (mainly the fact that 1 xiQ-ixi
concentrates around δ) developed in (Louart & Couillet, 2019), that most of the above quantities
vanish asymptotically. As a toy example, we consider the following term:
〜
〜
1n
P2 X E
p i=1
UTX-iQ-iCQ X-iv	UTX-iQ-ixix Q X-iv
—
1+δ
1 + pxi Q-ixi
1 n
P X
p i=1
1 n
P X
p i=1
1 n
PXl
O
T FT- T X~*	T Z~⅝ -‰7^
U X-i Q-i xi xi QX-i v
δ - 1 xiQ-ixi
(1 + δ)(1 + P xi Q-i xi)
(u~^X-iQ-ixi)(xjQX-iv) (δ - xiQQ-ixi
E
UTX-iQ-ixi∖
E
∖3
1-	~_ ∖
x] Q X-iv
Similarly, we can show that:
1n
P2 X E
p i=1
Uixi Q-iCQ X-iv
T Fz- T X~*	x~*
viU X-i Q-i C Qxi
1+δ
1+δ
T X~* zɔ Z-⅛
xi Q-i C Qxi
+Ui Vi -----------
+ i i 1 + δ
1 uτX τQ-ixixi QCQ Xv
1+δ
E
≤
≤
E
E
+
——
p
1
3
13
Under review as a conference paper at ICLR 2020
Finally, the remaining terms in ∆ can be estimated as follows:
1 Gllr UiX1 Q-iXiXl Q X-iV
心
P2 £	[	1 + PXIQ-ixi
ViUTXI iQ-iXiX| Q Xi	x∣Q-iXiX∣Q Xi
-------1__φ--------UiVi- ;-φ-i-----
1 + P χi Qxi	1 + P χi Q-iXi
+ O
2 δuτ 1XT QX1t V
p 1 + δ
δ2uτv
1 + δ
+ O
—
Where the last equality is obtained through the following estimation:
Tn	T tγ T T∖	T x"∖
1	X^ E viu X-iQ-iχiχi QXi
p2 ⅛	1 + PXi Q-皿
With the following bound:
ViUTX-iQ-iXi (pXi^QXi(1 + δ) - δ (1 + PXi^QXi))
(1 + PXi Q-iXi) (1 + δ)
1 XX ViδE[uτX-iQ-iXi]
p P ⅛P	(1 + δ)
11
-x：QXi(1 + δ) - δ 1 + -x：QXi
11
—Xi QXi(1 + δ) — δ(1 + δ) + δ(1 + δ) — δ (1 +— Xi QXi
1
≤ —Xi QXi — δ (1 + 2δ),
p i
we have again with Holder,s inequality and Proposition A.4:
1 n
—X E
P弋
1 n
P2 X E
P i=1
Viu^ X-iQ-ixixi Qxi
1 + p xi^ Qxi
1 XX ViδuTIXTQX
P=—E―
+O
Now that we estimated ∆, it remains to estimate E[PXTQX]. Indeed, given two unit norm vectors
u,v ∈ Rn we have:
1
E -utXtQ)Xv
P
1n
-T UiVjE[X：QXj]
Pij = I
1n n	n
一 ɪ2 JS UiVjXTQX +UiViδ
P i=1 j=ι	i=1
j=i
11
-X T QXU T 11t v + (δ X T QX)U
,tv = UX TQXUTMIVT + δuτv + O (-
PP
since we have X TQX = O(1) by Lemma A.2; we introduced the matrix M1 = 11t. Therefore we
have the following estimation:
1E [X tQX ]
P
ɪIn + 1(E) XTQXM1 + o∕[?
1+δ P 1+δ	P
where A = B + 0k口(g(p)) means that kA — Bk = O(q(p)). Finally, since R concentrates around
its mean, we can then conclude:
C 1 /	1	…、	1	1	δ — IT二
R = G (In- pXTQX)= -1+δIn + PwIyX TQXMI + OH.
P
P
□
14
Under review as a conference paper at ICLR 2020
B Proof of Proposition 3.1
Proof. Since the Lipschitz constant of a composition of Lipschitz functions is bounded by the prod-
uct of their Lipschitz constants, we consider the case N = 1 and a linear activation function. In
this case, the Lipschitz constant corresponds to the largest singular value of the weight matrix. We
consider the following notations for the proof
Wt = Wt - ηEt with 闽］i,j 〜N(0,1)
Wt+ι = Wt — max(0,σι,t - σ*) Ui,tV|,t
where σι,t = σι(Wt), Uι,t = uι(Wt) and Vι,t = vι(Wt). The effect of spectral normalization
is observed in the case where σ* > σι,t, otherwise the LiPschitz constant is bounded by σ*. We
therefore have
kWtkF ≤kWtkF + η2d1d0	(16)
kWt+ιkF = kWtkF + σ2- σ2,t	(17)
•	If kWt+1kF ≥ kWtkF, we have by equation 16 and equation 17
kWtkF ≤kWtkF + σ2	—	σ2,t	+ η2dιdo	⇒	kWtk	= σι,t	≤	√σ2	+ η2dιdo = δ
And since ∣∣Wt+ιk ≤ ∣∣Wtk, we have ∣∣Wt+ιk ≤ δ.
•	Otherwise, if there exits τ such that kWτ+1kF < kWτ kF, then for all ε > 0 there exists an
iteration τ0 ≥ τ such that kWτ0 k ≤ δ + ε. Indeed, otherwise we denote εt = kWtk2 - δ2
and εt > 0 for all t ≥ τ. Andifforallt ≥ τ, kWt+1 kF ≤ kWt kF, we have by equation 16
and equation 17
kWtkF — kWt+ιkF ≥ kWtk2 — δ2 ≥ kWt+ιk2 — δ2 = εt+ι
Integrating the above expression from τ to T - 1 ≥ τ , we end up with
T-1	T-1
kWτkF - kWTkF ≥ X εt ⇒ 0 ≤∣WtkF ≤kWτkF - X εt,
t=τ	t=τ
therefore, when T → ∞, εt has to tend to 0 otherwise the right hand-side of the last
inequality will tend to —∞ which is absurd.
□
15