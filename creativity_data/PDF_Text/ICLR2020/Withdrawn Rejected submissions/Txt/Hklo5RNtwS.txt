Under review as a conference paper at ICLR 2020
Behavior-Guided Reinforcement Learning
Anonymous authors
Paper under double-blind review
Ab stract
We introduce a new approach for comparing reinforcement learning policies, using
Wasserstein distances (WDs) in a newly defined latent behavioral space. We show
that by utilizing the dual formulation of the WD, we can learn score functions
over trajectories that can be in turn used to lead policy optimization towards
(or away from) (un)desired behaviors. Combined with smoothed WDs, the dual
formulation allows us to devise efficient algorithms that take stochastic gradient
descent steps through WD regularizers. We incorporate these regularizers into two
novel on-policy algorithms, Behavior-Guided Policy Gradient and Behavior-Guided
Evolution Strategies, which we demonstrate can outperform existing methods in a
variety of challenging environments. We also provide an open source demo1.
1	Introduction
One of the key challenges in reinforcement learning (RL) is to efficiently incorporate the behavioral
characteristics of learned policies into optimization algorithms (Lee & Popovic, 2010; Meyerson
et al., 2016; Conti et al., 2018). The fundamental question we aim to shed light on in this paper is:
What is the right measure of similarity between two policies acting on the same underlying MDP and
how can we devise algorithms to leverage this information for reinforcement learning?
In simple terms, the main thesis motivating the methods we propose is that:
Two policies may perform similar actions at a local level but result in very different global behaviors.
We propose to define behaviors via so-called Behavioral Embedding Maps (BEMs), which are
functions mapping trajectories (realizations of policies) into latent behavioral spaces representing
trajectories in a compact way. BEMs enable us to identify policies with their Probabilistic Policy
Embeddings (PPEs), which we define as the pushforward distributions over trajectory embeddings
as a result of applying a BEM to a policy’s trajectories. Importantly, two policies with distinct
distributions over trajectories may result in the same probabilistic embedding. PPEs provide us a
way to rigorously define dissimilarity between policies. We do this by equipping them with metrics
defined on the manifold of probabilistic measures, namely a class of Wasserstein distances (WDs,
Villani (2008)). There are several reasons for choosing WDs:
•	Flexibility. We can use any cost function between embeddings of trajectories, allowing the distance
between PPEs to arise organically from an interpretable distance between embedding points.
•	Non-injective BEMs. Different trajectories may be mapped to the same embedding point (for
example in the case of the last-state embedding). This precludes the use of likelihood-based
distances such as the KL divergence (Kullback & Leibler, 1951), which we discuss in Section 6.
•	Behavioral Test Functions. Solving the dual formulation of the WD objective yields a pair of test
functions over the space of embeddings that can be used to score trajectories.
The behavioral test functions underpin all our algorithms, directing optimization towards desired
behaviors. To learn them it suffices to define the BEM and the cost function between points in the PPE
space. To mitigate the computational burden of computing WDs, we rely on their entropy-regularized
formulations. This allows us to update the learned test functions in a computationally efficient manner
1Available at https://github.com/behaviorguidedRL/BGRL. We emphasize this is not an exact
replica of the code from our experiments, but a demo to build intuition and clarify our methods.
1
Under review as a conference paper at ICLR 2020
via stochastic gradient descent (SGD) on a Reproducing Kernel Hilbert Space (RKHS). We develop
a novel method for stochastic optimal transport based on random feature maps (Rahimi & Recht,
2008) to produce compact and memory-efficient representations of learned behavioral test functions.
Finally, having laid the groundwork for comparing trajectories via behavior-driven trajectory scores,
we address our core question by introducing two new on-policy RL algorithms:
•	Behavior Guided Policy Gradients (BGPG): We propose to replace the KL-based trust region
from Schulman et al. (2015) with a WD-based trust region in the PPE space.
•	Behavior Guided Evolution Strategies (BGES): Inspired by the NSR-ES algorithm from Conti
et al. (2018), BGES jointly optimizes for reward and novelty using the WD in the PPE space.
In addition, we also demonstrate a way to harness our methodology for imitation learning (Section 7.3)
and repulsion learning (Section 9.4), and we believe there may be many more potential applications
in the future.
2 M otivating Behavior-Guided Reinforcement Learning
Throughout this paper we prompt the reader to think of a policy as a distribution over its trajectories,
induced by the policy’s (possibly stochastic) map from state to actions and the unknown environment
dynamics. We care about summarizing (or embedding) trajectories into succinct representations
that can be compared with each other (via a cost/metric). These comparisons arise naturally when
answering questions such as: Has a given trajectory achieved a certain level of reward? Has it visited
a certain part of the state space? We think of these summaries or embeddings as characterizing the
behavior of the trajectory. We formalize these notions in Section 3.
We show that by identifying policies with the embedding distributions that result of applying the
embedding function (summary) to their trajectories, and combining this with the provided cost
metric, we can induce a topology over the space of policies given by the WD over their embedding
distributions. The methods we propose can be thought of as ways to leverage this “behavior” geometry
for a variety of downstream applications such as policy optimization and imitation learning.
This topology emerges naturally from the sole definition of an embedding map (behavioral summary)
and a cost function. Crucially these choices occur in the semantic space of behaviors as opposed to
parameters or visitation frequencies2. One of the advantages of choosing a Wasserstein geometry is
that non-surjective trajectory embedding maps are allowed. This is not possible with a KL induced
one (in non-surjective cases, computing the likelihood ratios in the KL definition is in general
intractable). In Sections 4 and 5 we show that in order to get a handle on this geometry we can use the
dual formulation of the Wasserstein distance to learn functions (Behavioral Test Functions) that can
provide scores on trajectories which then can be added to the reward signal (in policy optimization)
or used as a reward (in Imitation Learning).
In summary, by defining an embedding map of trajectories into a behavior embedding space equipped
with a metric3, our framework allows us to learn “reward” signals (Behavioral Test Functions) that
can serve to steer policy search algorithms through the “behavior geometry” either in conjunction
with a task specific reward (policy optimization) or on their own (e.g. Imitation Learning). We
develop versions of on policy RL algorithms which we call Behavior Guided Policy Gradient (BGPG)
and Behavior Guided Evolution Strategies (BGES) that enhance their baseline versions by the use of
learned Behavioral Test Functions. Our experiments in Section 7 show this modification is useful.
We also show how to use Behavioral Test Functions in Imitation Learning, where we only need
access to an expert’s embedding. Although our framework also has obvious applications to safety,
(learning policies that avoid undesirable or dangerous behaviors) we leave this for future work. We
also consider simple heuristics for the embeddings (inspired by other existing use cases), but believe
future work on learned embeddings could be a significant enhancement.
2If we choose an appropriate embedding map our framework handles visitation frequencies as well.
3The embedding space can be discrete or continuous and the metric need not be smooth, and can be for
example a simple discrete {0, 1} valued criterion
2
Under review as a conference paper at ICLR 2020
Figure 1: Behavioral Embedding Maps
(BEMs) map trajectories to points in the
behavior embedding space E . Two trajec-
tories may map to the same point in E .
3	Defining Behavior in Reinforcement Learning
A Markov Decision Process (MDP) is a tuple (S, A, P, R). Here S and A stand for the sets of
states and actions respectively, such that for s, s0 ∈ S and a ∈ A: P(s0|a, s) is the probability that
the system/agent transitions from s to s0 given action a and R(s0, a, s) is a reward obtained by an
agent transitioning from s to s0 via a. A policy πθ : S → A is a (possibly randomized) mapping
(parameterized by θ ∈ Rd) from S to A. Let Γ = {τ = s0, a0 ,r0, ∙ ∙ ∙ SH ,aH ,rH s.t. Si ∈ S ,ai ∈
A, ri ∈ R} be the set of possible trajectories enriched by sequences of partial rewards under some
policy π . The undiscounted reward function R : Γ → R (which expectation is to be maximized by
optimizing θ) satisfies R(τ) = PiH=0 ri, where ri = R(Si+1 , ai, Si).
3.1	Behavioral Embeddings
We start with a Behavioral Embeddng Space (BES) which
we denote as E and a Behavioral Embedding Map (BEM),
Φ : Γ → E, mapping trajectories to embeddings in E (Fig.
1). Importantly, the mapping does not need to be surjective.
We will provide examples of BESs and BEMs at the end
of the section. Given a policy π , we let Pπ denote the
distribution induced over the spaces of trajectories Γ and by
PπΦ the corresponding pushforward distribution on E induced
by Φ. We call PπΦ the Probabilistic Policy Embedding (PPE)
of a policy π . A policy π can be fully characterized by the
distribution Pπ .
Additionally, we require the BES E to be equipped with a
metric (or cost function) C : E × E → R. Given two trajectories τ1, τ2 in Γ, C(Φ(τ1), Φ(τ2))
measures how different these trajectories are in the behavior space. The following are examples of
BEMs (with the corresponding BESs) categorized into three main types (we will use examples from
all three types in our experiments in Section 7):
1.	State-based: the final state Φ1 (τ) = SH, the visiting frequency of a fixed state Φs2(τ) =
PtH=0 1(St = S), the frequency vector of visited states Φ3(τ) = PtH=0 est (where es ∈ R|S|
is the one-hot vector corresponding to state S); see also Section 7.2.
2.	Action-based: the concatenation of actions Φ4 (τ) = [a0, ..., aH]; see also Section 7.1.
3.	Reward-based: the total reward Φ5 (τ)	= PtH=0 rt, reward-to-go vector Φ6 (τ) =
PtH=0 rt Pit=0 ei (where ei ∈ RH+1 is a one-hot vector corresponding to i and with dimensions
indexed from 0 to H); see also Section 7.1 and Section 7.3.
For instance, PπΦ3 is the frequency with which different states are visited under π . Note that some of
the above embeddings are only for the tabular case (|S |, |A| < ∞) while others are universal.
4	Wasserstein Distance & Optimal Transport Problem
Let μ,ν be (Radon) probability measures over domains X ⊆ Rm, Y ⊆ Rn and let C : X × Y → R
be a cost function. For γ > 0, a smoothed Wasserstein Distance is defined as:
WDY (μ,ν ):=	mjn /	C (x, y)dπ(x,y)+ Y KL(π ∣ξ),	⑴
π∈π(μ,ν) X×γy
where Π(μ,ν) is the space of couplings (joint distributions) over XXY With marginal distributions μ
and V, KL(∙∣∙) denotes the KL divergence between distributions π and P with support X × Y defined
as: KL(π∣ρ) = Rχ×γ (log (dξ(x, y)) ) dπ(x, y) and ξ is a reference measure over X × Y. When
the cost is an `p distance and γ = 0, WDγ is also known as the Earth mover’s distance and the
corresponding optimization problem is known as the optimal transport problem (OTP).
4.1	Wasserstein Distance: Dual Formulation
We will use smoothed WDs to derive efficient regularizers for RL algorithms. To arrive at this goal,
we first need to consider the dual form of Equation 1. Under the subspace topology (Bourbaki,
3
Under review as a conference paper at ICLR 2020
Algorithm 1 Random Features Wasserstein SGD
Input: kernels κ,' over X, Y respectively with corresponding random feature maps φκ ,φ', smooth-
ing parameter Y, gradient step size α, number of optimization rounds M, initial dual vectors pμ, PV.
for t = 0,…，M do
1.	Sample (Xt, yt)〜μ N V.
2.	Update (P,)=1!-1)+ √ (1 - exp (d)>φ(Xt)TpV-I)>φ“xt)-CXtW))) ( φκ(Xt))
p	'p"	Pp--ιj	√t y	κ y	Y	)‘ J。4如”
Return： PM, PM.
1966) for X and Y, let C(X) denote the space of continuous functions on X and let C(Y) denote the
space of continuous functions over Y . The choice of the subspace topology ensures our discussion
encompasses the discrete case.
Let C : X × Y → R be a cost function, interpreted as the “ground cost” to move a unit of mass from
x to y. Define I as the (0, ∞) indicator function, where the value 0 denotes set membership. Using
Fenchel duality, we can obtain the following dual formulation of the problem in Eq. 1:
WDY(μ, v) =	max	/ λμ(x)dμ(x) - / λν(y)dν(y) - EC(λμ,λν),	(2)
λμ ∈C (X ),λν ∈C (Y) IX	Y
where EC (λμ, λν) is defined as:
EC (λμ ,λν )：=
Y Rx×y exp (λμ (Xf Yy)-C(x,y)) dξ(χ,y)
I((λν , λν) ∈ {(u, v) s.t. ∀(x, y) ∈ X × Y u(x) -
v(y) ≤ C(x, y)})
if γ > 0
if γ = 0.
(3)
(
We will set dξ(x, y) Y 1 for discrete domains and dξ(x, y) = dμ(x)dv(y) otherwise.
If λμ, λV are the functions achieving the maximum in Eq. 2, and Y is sufficiently small then
WDγ(μ, v) ≈ Eμ [λμ(x)] — EV [λV(y)], with equality when Y = 0. When for example Y = 0,
X = Y, and C(x, x) = 0 for all X ∈ X, it is easy to see λμ(x) = λV(x) = λ* (x) for all X ∈ X.
In this case the difference between Eμ [λ* (x)] and Eμ [λ* (y)] equals the WD. In other words, the
function λ* gives higher scores to regions of the space X where μ has more mass. This observation
is key to the success of our algorithms in guiding optimization towards desired behaviors.
4.2	Computing λμ and λV
We combine several techniques to make the optimization of
objective from Eq. 2 tractable. First, we replace X and Y
with the functions from a RKHS corresponding to universal
kernels (Micchelli et al., 2006). This is justified since those
function classes are dense in the set of continuous functions
of their ambient spaces. In this paper we choose the Gaussian
Figure 2: Behavioral embedding func-
tions corresponding to two policies π1
(green) and π2 (blue) whose BEMs map
trajectories to points in the real line.
kernel and approximate it using random Fourier feature maps
(Rahimi & Recht, 2008) to increase efficiency. Consequently,
the functions λ learned by our algorithms have the following
form: λ(x) = (Pλ)>φ(x), where φ is a random feature map
with m standing for the number of random features and Pλ ∈
Rm. For the Gaussian kernel, φ is defined as follows: φ(z) = √= CoS(Gz + b) for Z ∈ Rd,
where G ∈ Rm×d is Gaussian with iid entries taken from N(0, 1), b ∈ Rm with iid bis such that
bi 〜Unif[0, 2π] and the CoS function acts elementwise.
Henceforth, when we refer to optimization over λ, we mean optimizing over corresponding dual
vectors Pλ associated with λ. We can solve for the optimal dual functions by performing SGD over
the dual objective in Eq. 2. Algorithm 1 is the random features equivalent of Algorithm 3 in Genevay
et al. (2016) and will be a prominent subroutine of our methods. An explanation and proof of why
this is the right stochastic gradient is in Lemma 10.2 in the Appendix.
4
Under review as a conference paper at ICLR 2020
If pμ, PVare the optimal dual vectors and (χι,yι), ∙∙∙ , (xk,yk) i^d μN V, then Algorithm 1 can be
used to get an estimator of WDY(μ, V) as follows:
WDY (μ,ν ) = 1 X ………+ 1exp ( φκ(xi)>pμ- φ'2>pν — CM)) (4)
5	B ehavior- Guided Reinforcement Learning
Here we introduce the framework which allows us to incorporate our behavioral approach to rein-
forcement learning into practical on-policy algorithms. Denote by πθ a policy parameterized by
θ ∈ Rd. The goal of policy optimization algorithms is to find a policy maximizing, as a function of
the policy parameters, the expected total reward L(θ) := ET〜Png [R(τ)].
5.1	Behavioral Test Functions
If C : E × E → R is a cost function defined over behavior space E , and π1 , π2 are two policies, then:
WDY(Pφι,PM2) ≈ EiP∏ι [λ1(Φ(τ))] - Eτ3∏2 凶(Φ(τ))],	(5)
where λ1 ,λg are the optimal dual functions. The maps si := λ; ◦ Φ : Γ → R and s2 := λg ◦ Φ :
Γ → R define score functions over the space of trajectories. If γ is close to zero, the score function
si gives higher scores to trajectories from πi whose behavioral embedding is common under πi but
rarely appears under πj for j 6= i (Fig. 2).
5.2	Algorithms
We propose to solve a WD-regularized objective to tackle behavior-guided policy optimization. All
of our algorithms hinge on trying to maximize an objective of the form:
F(θ) = L(θ) +βWDY(PπΦθ,PbΦ),	(6)
where PbΦ is a base distribution over behavioral embeddings (possibly dependent on θ) and β ∈ R
could be positive or negative. Although the base distribution PbΦ could be arbitrary, our algorithms
will instantiate Pφ =看 ∪∏o∈s Pφo for some family of policies S (possibly satisfying |S| = 1) we
want the optimization to attract to / repel from.
In order to compute approximate gradients for F, we rely on the dual formulation of the WD. After
substituting the composition maps resulting from Eq. 5 into Eq. 2, we obtain:
F(θ) ≈ E~p∏θ [R(τ) + βsι(τ)] - βEφ〜pφ N(φ)],	⑺
where si : Γ → R equals si = λ; ◦ Φ,the Behavioral Test Function of policy ∏ and λ2 is the optimal
dual function of embedding distribution Pφ. Consequently VθF(θ) ≈ VθET〜Png [R(τ) + βsι(τ)].
We learn a score function si over trajectories that can guide our optimization by favoring those
trajectories that show desired global behaviors.
Eq. 7 is an approximation to the true objective from Eq. 2 whenever γ > 0. In practice, the entropy
regularization requires a damping term as defined in Equation 3. If ξ(PπΦ , PbΦ) is the joint distribution
of choice then F(θ) = L(θ) + βV for
V = max	E [λπθ (Φ(τ))] - E [λb(φ)] +γ E	[Λ(φi,φ2)],
λ∏θ ∈C(E),λb∈C(E) T〜p∏θ	φ〜Pφ	φ1,φ2〜ξ(pφθ ,pφ)
where Λ(φ1,φ2) = exp (λnθ(φ1)-λb(φ2)-C(φl,φ2)). When the embedding space E is not discrete
and Pφ = Pφ for some policy ∏, we let ξ(Pφ, Pφ) = Pφ N Pφ, otherwise ξ(Pφ, Pφ)=评 1, a
uniform distribution over E × E.
All of our methods perform a version of alternating SGD optimization: we take certain number of
SGD steps over the internal dual Wasserstein objective, followed by more SGD steps over the outer
objective having fixed the current dual functions. Although in practice the different components
5
Under review as a conference paper at ICLR 2020
that make up the optimization objectives we consider here could be highly nonconvex, in the cases
these functions satisfy some convexity assumptions, we can provide a sharp characterization for the
convergence rates of our algorithms. Details are given in Section 10 in the Appendix.
We consider two distinct approaches to optimizing this objective, by exploring in the action space and
backpropagating, as in policy gradient methods (Schulman et al., 2015; 2017), and by considering a
black-box optimization problem as in Evolution Strategies (ES, Salimans et al. (2017)). These two
different approaches lead to two new algorithms: Behavior-Guided Policy Gradient (BGPG) and
Behavior-Guided Evolution Strategies (BGES), that we discuss next.
5.3	Behavior-Guided Policy Gradient (BGPG)
Our first algorithm seeks to solve the optimization problem in Section 5.2 with policy gradients. We
refer to this method as the Behavior-Guided Policy Gradient (BGPG) algorithm (see Algorithm 2
below).
Algorithm 2 Behvaior-Guided Policy Gradient
Input: Initialize stochastic policy ∏o parametrized by θo, β < 0,η > 0, M,L ∈ N
for t = 1, . . . , T do
1.	Run πt-1 in the environment to get advantage values Aπt-1 (s, a) and trajectories {τi(t)}iM=1
2.	Update policy and test functions via several alternating gradient steps over the objective:
F(θ)=	E	hXXAntT(Si,ai) Πθ(ailSl + βλ1(Φ(τ1))
τι,T2 〜P∏t-1 N P∏θ LM	πt-1(ailsi)
λl λι( 、一 R	λ λ1 ®(TI))- λ2(φ(τ2)) - C®(TI)), φ(τ2)P∖ i
-βλ2(Φ(τ2)) + βγexp I --------------------Y-------------------- I J
Where τι = so, a0, r0,…，SH, aH, rH. Let θ(-1 = θ-.
for ' = 1,…，L do
a. APProXimate Pnt-1 N p∏θ via MM {Ti(t)}M=ι N 吉 {τθ}MI= Pnt,∏θ Where Ti 附 p∏θ
b. Take SGA step θ(1)ι = θ(1-1) + η57θF(θ(1-1)) using samples from Pnt-i,∏θ.
c. Use samples from Pnt-1,nθ and Algorithm 1 to update λ1, λ2.
Set θt = θ(MM).
Specifically, We maintain a stochastic policy πθ and compute policy gradients as in prior Work
(Schulman et al., 2015). To optimize the Wasserstein distance WDγ, We approximate the gradient
of this term via the random-feature Wasserstein SGD . Importantly, this stochastic gradient can be
approximated by samples collected from the policy ∏θ . In its simplest form, the Vθ F in Step b.
in Algorithm 2 can be computed by the vanilla policy gradient over the advantage component and
using the reinforce estimator through the components involving Behavioral Test Functions acting on
trajectories from Pnθ . We explain in Appendix 8.1 a loWer-variance gradient estimator alternative.
BGPG can be thought of as a variant of Trust Region Policy Optimization With a Wasserstein penalty.
As opposed to vanilla TRPO, the optimization path of BGPG floWs through policy parameter space
While encouraging itto folloW a smooth trajectory through the geometry of the PPE space. We proceed
to shoW that given the right embedding and cost function, We can prove a monotonic improvement
theorem for BGPG, shoWing that our methods satisfy at least similar guarantees as TRPO.
For a given policy π, We denote as: V n, Qn and An(s, a) = Qn(s, a) - Vn(s) the: value function,
Q-function and advantage function (see Appendix: Section 10.5). Furthermore, let V (π) be the
expected reward of policy ∏ and Pn (s) = ET〜Pn
PtT=0 1(st = s)
be the visitation measure.
Two distinct policies ∏ and ∏ can be related via the equation (see: Sutton et al. (1998)) V(∏)
V (π)+JS Pn (s) (JA Π(α∣s)An (s,α)dα) ds and the linear approximations to V around π via: L(π)
6
Under review as a conference paper at ICLR 2020
V(π) + RS ρ∏(S)(JA Π(a∣s)Aπ(s, a)da) ds (see: Kakade & Langford (2002)). Let S be a finite
set. Consider the following embedding Φs : Γ → R|S| defined by (Φ(τ))s = PtT=0 1(st = s) and
related cost function defined as: C(v, W) = ∣∣v - w∣∣ ι. Then WDo(Pφs, Pφs) is related to visitation
frequencies since WDo(Pφs, Pφs) ≥ Ps∈s ∣ρ∏(S) — ρ∏(s)| (see Section 10.5 for the proof). These
observations enable us to prove an analogue of Theorem 1 from Schulman et al. (2015), namely:
Theorem 5.1. If WD°(畔 s, Pφs) ≤ δ and C = max§,a ∣Aπ (s, a)| ,then V (π) ≥ L(θ) — δe.
As in Schulman et al. (2015), Theorem 5.1 implies a policy improvement guarantee for BGPG.
5.4 Behavior Guided Evolution Strategies (BGES)
ES takes a black-box optimization approach to RL, by considering a rollout of a policy, parameterized
by θ as a black-box function F. This approach has gained in popularity recently (Salimans et al., 2017;
Mania et al., 2018; Choromanski et al., 2019). If we take this approach to optimizing the objective
in Eq. 2, the result is a black-box optimization algorithm which seeks to maximize the reward
and simultaneously maximizes or minimizes the difference in behavior from the base embedding
distribution PbΦ . We call this method the Behavior-Guided Evolution Strategies (BGES) algorithm
(see Algorithm 3 below).
Algorithm 3 Behavior-Guided Evolution Strategies
Input: learning rate η, noise standard deviation σ, iterations T, BEM Φ, β
Initialize: Initial policy π0 parametrized by θ0, Behavioral Test Functions λ1, λ2. Evaluate policy
∏ο to return trajectory το and subsequently use the BEM to produce an initial PPE Pφ0.
for t = 1, . . . , T - 1 do
1.	Sample ci,…，Cn independently from N(0, I).
2.	Evaluate policies {πtk }kn=1 parameterized by {θt + σCk }kn=1 to return rewards Rk and trajecto-
ries τk for all k .
3.	Use BEM to map trajectories Tk to produce empirical PPEs Pφk for all k.
πt
4.	Update λι and 入2 using Algorithm 1, where μ = n ∪n=ι Pφk and V = ɪ ∪n=ι Pφk are the
uniform distribution over the set of PPEs from 3 for t - 1 and t.
5.	Approximate WdDγ(PπΦk , PπΦ ) plugging in λ1, λ2 into Eq. 4 for each perturbed policy πk
6.	Update Policy: θt+ι = θt + NESF, where:
VESF = 1 XX[(1 - β)(Rk - Rt)+ βWdDγ鸣，,Pφt)]ck
σ k=1
When β > 0, and we take PbΦ = PπΦ , BGES resembles the NSR-ES algorithm from Conti et al.
(2018), an instantiation of novelty search (Lehman & Stanley, 2008). The positive weight on the
WD-term enforces newly constructed policies to be behaviorally different from the previous ones
(improving exploration) while the R-term drives the optimization to achieve its main objective,
i.e., maximize the reward. The key difference in our approach is the probabilistic embedding map,
with WD rather than Euclidean distance. We show in Section 7.2 that BGES outperforms NSR-ES
for challenging exploration tasks. The approximation introduced by Step 5 bypasses the need of
computing a different pair of behavioral test functions λ1, λ2 for each perturbed policy πk.
If we take β < 0, and assume PbΦ = PπΦ to correspond to embedded trajectories from an oracle or
expert policy, we can perform imitation learning. Despite not accessing the expert’s policy (just the
trajectories it generates), we show in Section 7.3 that this approach dramatically improves learning.
6	Related Work
Our work is related to research in multiple areas in neuroevolution and machine learning:
Behavior Characterizations: The idea of directly optimizing for behavioral diversity was intro-
duced by Lehman & Stanley (2008) and Lehman (2012), who proposed to search directly for novelty,
7
Under review as a conference paper at ICLR 2020
rather than simply assuming it would naturally arise in the process of optimizing an objective function.
This approach has been applied to deep RL (Conti et al., 2018) and meta-learning (Gajewski et al.,
2019). In all of this work, the policy is represented via a behavioral characterization (BC), typically
chosen with knowledge of the environment, for example the final (x,y) coordinate for a locomotion
task. Additionally, in most cases these BCs are considered to be deterministic, with Euclidean
distances used to compare BCs. In our setting, we move from deterministic BCs to stochastic PPEs,
thus requiring the use of metrics capable of comparing probabilistic distributions.
Distance Metrics: WDs have been used in many different applications in machine learning where
guarantees based on distributional similarity are required (Jiang et al., 2019; Arjovsky et al., 2017).
We make use of WDs in our setting for a variety of reasons. First and foremost, the dual formulation
of the WD allows us to recover Behavioral Test Functions, thus providing us with behavior-driven
trajectory scores. In contrast to KL divergences, WDs are sensitive to user-defined costs between pairs
of samples instead of relying only on likelihood ratios. Furthermore, as opposed to KL divergences,
it is possible to take SGD steps using entropy-regularized Wasserstein objectives. Computing an
estimator of the KL divergence is hard without a density model. Since in our framework multiple
unknown trajectories may map to the same behavioral embedding, the likelihood ratio between two
embedding distributions may be ill-defined.
WDs for RL: We are not the first to propose using WDs in RL. Zhang et al. (2018) have recently
introduced Wasserstein Gradient Flows (WGFs) for finding efficient RL policies. This approach casts
policy optimization as gradient descent flow on the manifold of corresponding probability measures,
where geodesic lengths are given as second-order WDs. We note that computing WGFs is a nontrivial
task. In Zhang et al. (2018) this is done via particle approximation methods. We show in Section 7
that RL algorithms using these techniques are substantially slower than our methods. The WD has
also been employed to replace KL terms in standard Trust Region Policy Optimization (Richemond
& Maginnis, 2017). This is a very special case of our more generic framework (cf. Section 5.2). In
Richemond & Maginnis (2017) it is suggested to solve the corresponding RL problems via Fokker-
Planck equations and diffusion processes, yet no empirical evidence of the feasibility of this approach
is provided. We propose general practical algorithms and provide extensive empirical evaluation.
Distributional RL Distributional RL (DRL, Bellemare et al. (2017)) expands on traditional off-
policy methods (Mnih et al., 2013) by attempting to learn a distribution of the return from a given
state, rather than just the expected value. These approaches have impressive experimental results
(Bellemare et al., 2017; Dabney et al., 2018), with a growing body of theory (Rowland et al., 2018;
Qu et al., 2019; Bellemare et al., 2019; Rowland et al., 2019). Superficially it may seem that learning
a distribution of returns is similar to our approach to PPEs, when the BEM is a distribution over
rewards. Indeed, reward-driven embeddings used in DRL can be thought of as special cases of the
general class of BEMs. We note two key differences: 1) DRL methods are off-policy whereas our
BGES and BGPG algorithms are on-policy, and 2) DRL is typically designed for discrete domains,
since Q-Learning with continuous action spaces is generally much harder. Furthermore, we note that
while the WD is used in DRL, it is only for the convergence analysis of the DRL algorithm—the
algorithm itself does not use WDs (Bellemare et al., 2017).
7	Experiments
Here we seek to test whether our behavior-guided approach to RL translates to performance gains for
simulated environments. We individually evaluate our two proposed algorithms, BGPG and BGES,
versus their respective baselines for a range of benchmark tasks. While in some cases the results may
not be state of the art, we believe the improvement vs. popular RL algorithms (in particular TRPO
and ES) are exciting results which could stimulate future work. We also include a study of using our
method for imitation learning. For each subsection we provide additional details in the Appendix.
7.1	Behavior-Guided Policy Gradient
Our key question is whether BGPG can outperform baseline TRPO methods using KL divergence.
In Fig. 3, we see this is clearly the case for four continuous control tasks: Pendulum from OpenAI
Gym and Hopper: Stand, Hooper: Hop and Walker: Stand from the DeepMind Control Suite (Tassa
et al., 2018). For the BEM, we use the concatenation-of-actions (as used already in TRPO). We also
8
Under review as a conference paper at ICLR 2020
confirm results from (Schulman et al., 2015) that a trust region greatly improves performance, as we
see the black curve (without one) often fails to learn.
Figure 3: BGPG vs. TRPO: We compare BGPG and TRPO (KL divergence) on several continuous control
tasks. As a baseline we also include results without a trust region (β = 0 in Algorithm 2). Plots show the
mean ± std across 5 random seeds. BGPG consistently outperforms other methods.
Wall Clock Time: To illustrate computational benefits of alternating optimization (AO) of WD in
BGPG, we compare it to the particle approximation (PA) method introduced in Zhang et al. (2018)
in Fig. 4. In practice, the WD across different state samples can be optimized in a batched manner
using AO (see Appendix for details). We see that AO is substantially faster than PA.
^⅛⅛∙φ⅛ PΦN=EIL口』OZ
(a) Pendulum
(b) Hopper: Stand
(c) Hopper: Hop
(d) Walker: Stand
Figure 4: The clock-time comparison (in sec) of BGPG (alternating optimization) with particle approximation.
7.2	Behavior-Guided Evolution Strategies
As a novelty-search method, BGES is designed to actively explore the environment by behaving
differently for previous policies. With that in mind, we seek to evaluate the ability to solve two key
challenges in exploration for RL: deceptive rewards and local maxima.
Deceptive Rewards A common challenge in model-free RL is deceptive rewards. These arise
since agents can only learn from data gathered via exploration in the environment. To test BGES in
this setting, we created two intentionally deceptive environments where agents may easily be fooled
into learning suboptimal policies. In both cases the agent is penalized at each time step for being far
away from a goal. The deception comes from a wall situated in the middle, which means that initially
positive rewards from moving directly forward will lead to a suboptimal policy.
Figure 5: Efficient Exploration. On the left we show a visualization of the simulated environment, with the
deceptive barrier between the (quadruped) agent and the goal. On the right, we show two plots with the median
curve across five seeds, with the IQR shaded for the quadruped and point environment respectively.
9
Under review as a conference paper at ICLR 2020
We consider two types of agents—a two-dimensional point and a much larger quadruped. Details
are provided in the Appendix (Section 9). We compare with state-of-the-art on-policy methods for
efficient exploration: NSR-ES from (Conti et al., 2018), which assumes the BEM is deterministic and
uses the Euclidean distance to compare policies, and NoisyNet-TRPO from Fortunato et al. (2018).
Results are presented on Fig. 5. Policies avoiding the wall correspond to rewards: R > -5000 and
R > -800 for the quadruped and point respectively. In the prior case an agent needs to first learn
how to walk and the presence of the wall is enough to prohibit vanilla ES from even learning forward
locomotion. We note that BGES is the only method that drives the agent to the goal in both settings.
For the quadruped the BEM is the reward-to-go while for the point we used the final state.
Escaping Local Maxima. In Fig. 6 we compare our methods
with methods using regularizers based on other distances or
divergences (specifically, Hellinger, Jensen-Shannon (JS), KL
and Total Variation (TV) distances), as well as vanilla ES (i.e.,
with no distance regularizer). Experiments were performed on
a Swimmer environment from OpenAI Gym (Brockman et al.,
2016), where the number of samples of the ES optimizer was
drastically reduced. BGES is the only one that manages to obtain
good policies which also proves that the benefits come here not
just from introducing the regularizer, but from its particular form.
Figure 6: Escaping Local Maxima.
A comparison of BGES with those
using different distances on PPEs.
7.3	Imitation Learning
As discussed in Section 5.3, we can also utilize the BGES algorithm
for imitation learning, by setting β < 0, and using an expert’s tra-
jectories for the PPE. For this experiment we use the reward-to-go
BEM (Section 5). In Fig. 7, we show that this approach significantly
outperforms vanilla ES on the Swimmer task. Although conceptually
simple, we believe this could be a powerful approach with potential
extensions, for example in designing safer algorithms.
7.4	Hyperparameter Selection
Figure 7: Imitation Learning.
Our approach includes several new hyperparameters,
such as the kernel for the Behavioral Test Functions
and the choice of BEM. For our experiments we did
not perform any hyperparameter optimization. We only
considered the rbf kernel, and only varied the BEM for
BGES. For BGPG we chose the concatenation of actions,
since this is the same as used in the KL divergence for
TRPO. For BGES, we demonstrated several different BEMs, and we show an ablation study for the
point agent in Fig. 8 where we see that both the reward-to-go (RTG) and Final State (SF) worked, but
the vector of all states (SV) did not (for 5 seeds). We leave learned BEMs as exciting future work.
Figure 8: Choice of BEM
8 Conclusion and Future Work
In this paper we proposed a new paradigm for on-policy learning in RL, where policies are em-
bedded into expressive latent behavioral spaces and the optimization is conducted by utilizing the
repelling/attraction signals in the corresponding probabilistic distribution spaces. The use of Wasser-
stein distances (WDs) guarantees flexibility in choosing cost funtions between embedded policy
trajectories, enables stochastic gradient steps through corresponding regularized objectives (as op-
posed to KL divergence methods) and provides an elegant method, via their dual formulations, to
quantify behaviorial difference of policies through the behavioral test functions. Furthermore, the
dual formulations give rise to efficient algorithms optimizing RL objectives regularized with WDs.
We also believe the presented methods shed new light on several other challenging problems of
modern RL, including: learning with safety guarantees (a repelling signal can be used to enforce
behaviors away from dangerous ones) or anomaly detection for reinforcement learning agents (via
the above score functions). We are also excited by the possibility of scaling this approach to a
population setting, learning the behavioral embedding maps from data, or adapting the degree of
repulsion/attraction during optimization (parameter β).
10
Under review as a conference paper at ICLR 2020
References
Martin Arjovsky, SoUmith Chintala, and Leon Bottou. Wasserstein generative adversarial networks.
In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference
on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 214-223,
International Convention Centre, Sydney, Australia, 06-11 Aug 2017. PMLR.
Sivaraman Balakrishnan, Martin J. Wainwright, and Bin Yu. Statistical guarantees for the em
algorithm: From population to sample-based analysis. Ann. Statist., 45(1):77-120, 02 2017. doi:
10.1214/16-AOS1435. URL https://doi.org/10.1214/16-AOS1435.
Marc G. Bellemare, Will Dabney, and Remi Munos. A distributional perspective on reinforcement
learning. In Proceedings of the 34th International Conference on Machine Learning - Volume
70, ICML’17, pp. 449-458. JMLR.org, 2017. URL http://dl.acm.org/citation.cfm?
id=3305381.3305428.
Marc G. Bellemare, Nicolas Le Roux, Pablo Samuel Castro, and Subhodeep Moitra. Distributional
reinforcement learning with linear function approximation. In Kamalika Chaudhuri and Masashi
Sugiyama (eds.), Proceedings of Machine Learning Research, volume 89 of Proceedings of
Machine Learning Research, pp. 2203-2211. PMLR, 16-18 Apr 2019.
Nicolas Bourbaki. General Topology: Elements of Mathematics. Addison-Wesley, 1966.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
Krzysztof Choromanski, Aldo Pacchiano, Jack Parker-Holder, Yunhao Tang, Deepali Jain, Yuxiang
Yang, Atil Iscen, Jasmine Hsu, and Vikas Sindhwani. Provably robust blackbox optimization for
reinforcement learning. CoRR, abs/1903.02993, 2019.
Edoardo Conti, Vashisht Madhavan, Felipe Petroski Such, Joel Lehman, Kenneth O. Stanley, and
Jeff Clune. Improving exploration in evolution strategies for deep reinforcement learning via a
population of novelty-seeking agents. In Advances in Neural Information Processing Systems 31,
NeurIPS, pp. 5032-5043, 2018.
Will Dabney, Georg Ostrovski, David Silver, and Remi Munos. Implicit quantile networks for
distributional reinforcement learning. In Jennifer Dy and Andreas Krause (eds.), Proceedings of
the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine
Learning Research, pp. 1096-1105, Stockholmsmssan, Stockholm Sweden, 10-15 Jul 2018.
PMLR.
Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford,
John Schulman, Szymon Sidor, Yuhuai Wu, and Peter Zhokhov. Openai baselines. https:
//github.com/openai/baselines, 2017.
Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Matteo Hessel, Ian Osband,
Alex Graves, Volodymyr Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin, Charles Blundell,
and Shane Legg. Noisy networks for exploration. In International Conference on Learning
Representations, ICLR, 2018.
Alexander Gajewski, Jeff Clune, Kenneth O. Stanley, and Joel Lehman. Evolvability es: Scalable and
direct optimization of evolvability. In Proceedings of the Genetic and Evolutionary Computation
Conference, GECCO ’19, pp. 107-115, New York, NY, USA, 2019. ACM. ISBN 978-1-4503-
6111-8. doi: 10.1145/3321707.3321876.
Aude Genevay, Marco Cuturi, Gabriel Peyre, and Francis Bach. Stochastic optimization for large-
scale optimal transport. In Advances in neural information processing systems, pp. 3440-3448,
2016.
Ray Jiang, Aldo Pacchiano, Tom Stepleton, Heinrich Jiang, and Silvia Chiappa. Wasserstein fair
classification. arXiv preprint arXiv:1907.12059, 2019.
Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. 2:
267-274, 2002.
11
Under review as a conference paper at ICLR 2020
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
S. KUllback and R. A. Leibler. On information and sufficiency. Ann. Math. Statist., 22(1):79-
86, 03 1951. doi: 10.1214/aoms/1177729694. URL https://doi.org/10.1214/aoms/
1177729694.
Seong Jae Lee and Zoran Popovic. Learning behavior styles with inverse reinforcement learning.
ACM Trans. Graph., 29(4):122:1-122:7, 2010. doi: 10.1145/1778765.1778859. URL https:
//doi.org/10.1145/1778765.1778859.
Joel Lehman. Evolution through the Search for Novelty. PhD thesis, University of Central Florida,
2012.
Joel Lehman and Kenneth O. Stanley. Exploiting open-endedness to solve problems through the
search for novelty. In Proceedings of the Eleventh International Conference on Artificial Life (Alife
XI. MIT Press, 2008.
Horia Mania, Aurelia Guy, and Benjamin Recht. Simple random search of static linear policies is
competitive for reinforcement learning. Advances in Neural Information Processing Systems 31,
NeurIPS, 2018.
Elliot Meyerson, Joel Lehman, and Risto Miikkulainen. Learning behavior characterizations for
novelty search. In Proceedings of the 2016 on Genetic and Evolutionary Computation Conference,
Denver, CO, USA, July 20 - 24, 2016, pp. 149-156, 2016. doi: 10.1145/2908812.2908929. URL
https://doi.org/10.1145/2908812.2908929.
Charles A. Micchelli, Yuesheng Xu, and Haizhang Zhang. Universal kernels. Journal of Ma-
chine Learning Research, 7:2651-2667, 2006. URL http://jmlr.org/papers/v7/
micchelli06a.html.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan
Wierstra, and Martin A. Riedmiller. Playing atari with deep reinforcement learning. ArXiv,
abs/1312.5602, 2013.
Yurii Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Springer Publishing
Company, Incorporated, 1 edition, 2014. ISBN 1461346916, 9781461346913.
Chao Qu, Shie Mannor, and Huan Xu. Nonlinear distributional gradient temporal-difference learning.
In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International
Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp.
5251-5260, Long Beach, California, USA, 09-15 Jun 2019. PMLR.
Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In J. C. Platt,
D. Koller, Y. Singer, and S. T. Roweis (eds.), Advances in Neural Information Processing Systems
20, pp. 1177-1184. Curran Associates, Inc., 2008. URL http://papers.nips.cc/paper/
3182-random- features- for- large- scale- kernel- machines.pdf.
Pierre H. Richemond and Brendan Maginnis. On wasserstein reinforcement learning and the fokker-
planck equation. CoRR, abs/1712.07185, 2017. URL http://arxiv.org/abs/1712.
07185.
Mark Rowland, Marc Bellemare, Will Dabney, Remi Munos, and Yee Whye Teh. An analysis of
categorical distributional reinforcement learning. In Amos Storkey and Fernando Perez-Cruz (eds.),
Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics,
volume 84 of Proceedings of Machine Learning Research, pp. 29-37, Playa Blanca, Lanzarote,
Canary Islands, 09-11 Apr 2018. PMLR.
Mark Rowland, Robert Dadashi, Saurabh Kumar, Remi Munos, Marc G. Bellemare, and Will Dabney.
Statistics and samples in distributional reinforcement learning. In Kamalika Chaudhuri and Ruslan
Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning,
volume 97 of Proceedings of Machine Learning Research, pp. 5528-5536, Long Beach, California,
USA, 09-15 Jun 2019. PMLR.
12
Under review as a conference paper at ICLR 2020
Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution strategies as a
scalable alternative to reinforcement learning. CoRR, abs/1703.03864, 2017.
John Schulman, Sergey Levine, Pieter Abbeel, Michael I. Jordan, and Philipp Moritz. Trust region
policy optimization. In Proceedings of the 32nd International Conference on Machine Learning,
IcmL 2015, Lille, France, 6-11 July 2015, pp. 1889-1897, 2015. URL http://jmlr.org/
proceedings/papers/v37/schulman15.html.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Richard S Sutton, Andrew G Barto, et al. Introduction to Reinforcement Learning, volume 135. MIT
press Cambridge, 1998.
Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden,
Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. arXiv preprint
arXiv:1801.00690, 2018.
Cedric Villani. Optimal transport: Old and new. Springer, 2008.
Ruiyi Zhang, Changyou Chen, Chunyuan Li, and Lawrence Carin. Policy optimization as wasserstein
gradient flows. In Proceedings of the 35th International conference on Machine Learning, IcML
2018, Stockholmsmassan, Stockholm, Sweden, July 10-15, 2018, pp. 5741-5750, 2018. URL
http://proceedings.mlr.press/v80/zhang18a.html.
13
Under review as a conference paper at ICLR 2020
Appendix: Behavior-Guided Reinforcement Learning
9	Further Experimental Details
9.1	BGPG
A Lower-variance Gradient Estimator: As explained in Section 5.2, the BGPG considers an
objective which involves two parts: the conventional surrogate loss function for policy optimization
(Schulman et al., 2017), and a loss function that involves the Behavior Test Functions. Though we
could apply vanilla reinforced gradients on both parts, it is straightforward to notice that the second
part can be optimized with reparameterized gradients (Kingma & Welling, 2013), which arguably
have lower variance compared to the reinforced gradients. In particular, we note that under random
feature approximation (4), as well as the action-concatenation embedding, the Wasserstein distance
loss WDγ (PπΦ , PbΦ) is a differentiable function of θ. To see this more clearly, notice that under a
GauSSian policy a 〜N(μθ(s), σθ(s)2) the actions a = μθ(S) + σθ(S) ∙ e are reparametrizable for
being standard Gaussian noises. We can directly apply the reparametrization trick to this second
objective to obtain a gradient eStimator with potentially much lower variance. In our experimentS, we
applied thiS lower-variance gradient eStimator.
Trust Region Policy Optimization: Though the original TRPO (Schulman et al., 2015) conStruct
the truSt region baSed on KL-divergence, we propoSe to conStruct the truSt region with WD. For
convenience, we adopt a dual formulation of the truSt region method and aim to optimize the
augmented objective ET〜πθ[R(τ)] - βWDY(Pφo, Pφ). We apply the Concatenation-of-actions
embedding and random feature mapS to calculate the truSt region. We identify Several important
hyperparameters: the RKHS (for the test function) is produced by RBF kernel k(x, y) = exp(kx -
y∣∣2/σ2) with σ = 0.1; the number of random features is D = 100; recall the embedding is
Φ(τ) = [a1, a2...aH] where H is the horizon of the trajectory, here we take 10 actions per state
and embed them together, this is equivalent to reducing the variance of the gradient estimator by
increasing the sample size; the regularized entropy coefficient in the WD definition as γ = 0.1; the
trust region trade-off constant β ∈ {0.1, 1, 10}. The alternate gradient descent is carried out with
T = 100 alternating steps and test function coefficients p ∈ RD are updated with learning rate
αp = 0.01.
The baseline algorithms are: No trust region, and trust region with KL-divergence. The KL-divergence
is identified by a maximum KL-divergence threshold per update, which we set to = 0.01.
Across all algorithms, we adopt the open source implementation (Dhariwal et al., 2017). Hyper-
parameters such as number of time steps per update as well as implementation techniques such as
state normalization are default in the original code base.
The additional experiment results can be found in Figure 9 where we show comparison on additional
continuous control benchmarks: Tasks with DM are from DeepMind Contol Suites (Tassa et al.,
2018). We see that the trust region constructed from the WD consistently outperforms other baselines
(importantly, trust region methods are always better than the baseline without trust region, this
confirms that trust region methods are critical in stabilizing the updates).
Figure 9: Additional Experiment on TRPO. We compare No Trust Region with two alternative trust region
constructions: KL-divergence and Wassertein distance (ours).
14
Under review as a conference paper at ICLR 2020
Wasserstein AO vs. Particle Approximation: To calculate the regularized Wasserstein distance,
we propose a gradient descent method that iteratively updates the test function. The alternting
optimization (AO) scheme consists of updating both the test function and the distribution parameters
such that the regularized Wasserstein distance of the trainable distribution against the reference
distribution is minimized. Alternatively, we can also adopt a particle approximation method to
calculate the Wasserstein distance and update the distribution parameters using an approximate
gradient descent method (Zhang et al., 2018).
One major advantage of AO against particle approximation is its ease of parallization. In particular,
when using the concatenation-of-actions embedding, the aggregate Wasserstein distance can be
decomposeed into an average ofa set of Wasserstein distances over states. To calculate this aggregated
gradient, AO can easily leverage the matrix multiplication; on the other hand, particle approximation
requires that the dual optimal variables of each subproblem be computed, which is not straightforward
to parallelize.
We test both methods in the context of trust region policy search, in which we explicitly calculate
the Waserstein distance of consecutive policies and enforce the constraints using a line search as in
(Schulman et al., 2015). Both methods require the trust region trade-off parameter β ∈ {0.1, 1, 10}.
We adopt the particle method in (Zhang et al., 2018) where for each state there are M = 16 particles.
The gradients are derived based a RKHS where we adaptively adjust the coefficient of the RBF
kernel based on the mean distance between particles. For the AO, we find that it suffices to carry out
T ∈ {1, 5, 10} gradient descents to approximate the regularized Wasserstein distance.
9.2	BGES
Efficient Exploration: To demonstrate the effectiveness of our method in exploring deceptive
environments, we constructed two new environments using the MuJoCo simulator. For the point
environment, we have a 6 dimensional state and 2 dimensional action, with the reward at each
timestep calculated as the distance between the agent and the goal. We use a horizon of 50 which is
sufficient to reach the goal. The quadruped environment is based on Ant from the Open AI Gym
(Brockman et al., 2016), and has a similar reward structure to the point environment but a much larger
state space (113) and action space (8). For the quadruped, we use a horizon length of 400.
To leverage the trivially parallelizable nature of ES algorithms, we use the ray library, and distribute
the rollouts across 72 workers using AWS. Since we are sampling from an isotropic Gaussian, we are
able to pass only the seed to the workers, as in Salimans et al. (2017). However we do need to return
trajectory information to the master worker.
For both the point and quadruped agents, we use random features with dimensionality m = 1000,
and 100 warm-start updates for the WD at each iteration. For point, we use the final state embedding,
learning rate η = 0.1 and σ = 0.01. For the quadruped, we use the reward-to-go embedding, as
we found this was needed to learn locomotion, as well as a learning rate of η = 0.02 and σ = 0.02.
The hyper-parameters were the same for all ES algorithms. When computing the WD, we used the
previous 2 policies, θt-1 and θt-2. An ablation study for the point environment for both the choice
of embedding and number of prior policies is shown in Fig 12.
(a) Embeddings
Figure 10: A sensitivity analysis investigating a) the impact of the embedding and b) the number of previous
policies θt-i , i ∈ 1, 2, 5
(b) Previous Policies
15
Under review as a conference paper at ICLR 2020
For embeddings, we compare the reward-to-go (RTG), concatenation of states (SV) and final state
(SF). In both the RTG and SF case the agent learns to navigate past the wall (> -800). For the
number of previous policies, we use the SF embedding, and using 2 appears to work best, but both 1
and 5 do learn the correct behavior.
Escaping Local Maxima: We also demonstrated that our method leads to faster training even in
more standard settings, where exploration is not that crucial, but the optimization can be trapped in
local maxima. To show it, we compared baseline ES algorithm for ES optimization from Salimans et al.
(2017) with its enhancements, where regularizers using different metrics on the space of probabilistic
distributions corresponding to policy embeddings were used, as in the previous paragraph. We noticed
that adding Wasserstein regularizers drastically improved optimization, whereas regularizers based
on other distances/divergencies, namely: Hellinger, Jensen-Shannon, KL and TV did not have any
impact. We considered Swimmer task from OpenAI Gym and to make it challenging, reduced the
number of perturbations per iteration to 80. In that setting our method was the only one that was not
trapped in local maxima and managed to learn effective policies.
9.3	Imitation Learning:
For the Imitation Learning experiment we used the reward-to-go embedding, with learning rate
η = 0.1 and σ = 0.01. We use one oracle policy, which achieves > 360 on the environment. The only
information provided to the algorithm is the embedded trajectory, used to compute the WD. This has
exciting future applications since no additional information about the oracle is required in order to
significantly improve learning.
9.4	Repulsion Learning
Algorithm 4 Behvaior-GUided Repulsion Learning
Input: β,η > 0, M ∈ N
Initialize: Initial stochastic policies π0a, π0b , parametrized by θ0a, θ0b respectively, Behavioral Test
Functions λ1a , λ2b
for t = 1, . . . , T do
1.	Collect M trajectories {τia}iM=1 from Pπa	and M trajectories {τib}iM=1 from Pπb .
APProXimate Pna-I N Pnb-I via 焉{τia}M=1 N 焉{τb}M=1 := Pna-1,πb-1
2.	Form two distinct surrogate rewards for joint trajectories of agents a and b:
Ra(τ1,τ2) = Rm) + βλa(Φ(τι)) + βγexp (》a&(TI))-镇①⑦Y)- C®*，①S)))
Rbm" R(T2) - βλb(Φ(τ2)) + βγexp (λa(φ(τι))-λb(φ(,- C"1)), 吃力)
3.	For c ∈ {a, b} use the Reinforce estimator to take gradient stePs:
,τb) (XIVθc-ι log (∏C-i(aC∣sC)))]
θtc = θtc-1 + η	E
T a,τ b 〜Pna-1,∏b-1
Where Ta = sa, aa, %, •一 , SH, aH, rH and Tb = sb, ab, rb,…,SH, aH, rbb.
5. Use samples from Pna ι,∏b ɪ and Algorithm 1 to update the Behavioral Test Functions λ?, λb.
Although this was not discussed in the main section of the PaPer, it is Possible to use our behavioral
approach to simultaneously learn multiple policies eXhibiting different behaviors all of which are able
to solve the same task. This is not the main focus of the main paper, but we chose to include these
results in an attempt to provide the readers with a better understanding of Behavioral Test functions.
16
Under review as a conference paper at ICLR 2020
Algorithm 4 maintains two policies πa and πb . Each policy is optimized by taking a policy gradient
step (using the Reinforce gradient estimator) in the direction optimizing surrogate rewards Ra and
Rb that combines the signal from the task’s reward function R and the repulsion score encoded by
the behavioral test functions λa and λb .
We conducted experiments testing Algorithm 4 on a simple Mujoco environment consisting of a
particle that moves on the plane and whose objective is to learn a policy that allows it to reach
one of two goals. Each policy outputs a velocity vector and stochasticity is achieved by adding
Gaussian noise to the mean velocity encoded by a neural network with two size 5 hidden layers and
ReLu activations. If an agent performs action a at state s, it moves to state a + s. The reward of
an agent after performing action a at state S equals -Ilak2 * 30 - min(d(s, Goal1), d(s, Goal2))2
where d(x, y) denotes the distance between x and y in R2 . The initial state is chosen by sampling a
Gaussian distribution with mean 00 and diagonal variance 0.1. In each iteration step we sample 100
trajectories. In the following pictures we plot the policies’ behavior by plotting 100 trajectories of
each. The embedding Φ : Γ → R maps trajectories τ to their mean displacement in the x-axis. We
use the squared absolute value difference as the cost function.
(a) π0a	(b) π0b
Figure 11: Initial state of policies πa , πb and Behavioral Test functions λa , λb in the Multigoal environment.
(c) λa and -λb at t = 0
There are two optimal policies, moving the particle to the left goal or moving it to the right goal. We
now plot how the policies’ behavior and evolves throughout optimization and how the Behavioral
Test Functions guide the optimization by favouring the two policies to be far apart.
Behavioral Test functions
(a) π2a2	(b) π2b2	(c) λa and -λb at t = 22
(d) π1a18
(e) π1b18
(f) λa and -λb att = 118
Figure 12: Evolution of the policies and Behavioral Test Functions throughout optimization.
17
Under review as a conference paper at ICLR 2020
Let X and Y be the domains of two measures μ, and ν. Recall that in case Y = 0, X = Y, and
C (x, x) = 0 for all X ∈ X, then λμ(x) = λV (x) = λ*(x) for all X ∈ X. In the case of regularized
Wasserstein distances with γ > 0, this relationship may not hold true even if the cost satisfies the
same diagonal assumption. For example when the regularizing measure is the product measure,
and μ, V have disjoint supports, since the soft constraint Y exp (λμ(x)-λVYy)-C(x,y)) is enforced in
expectation over the product measure there may exist optimal solutions λμ, λV that do not satisfy
*	*
λμ = λν.
10 Theoretical results
We start by exploring some properties of the Wasserstein distance and its interaction with some
simple classes of embeddings. The first lemma we show has the intention to show conditions under
which two policies can be shown to be equal provided the Wasserstein distance between its trajectory
embeddings is zero. This result implies that our framework is capable of capturing equality of policies
when the embedding space equals the space of trajectories.
Lemma 10.1. Let S and A be finite sets, the MDP be episodic (i.e. of finite horizon H), and
Φ(τ) = PtH=0 est,at with es,a ∈ R|S|+|A| the indicator vector for the state action pair (s, a). Let
C(v, w) = kv - wkpp forp ≥ 1. If Y = 0 and WDγ (PπΦ, PπΦ0) = 0 then π = π0.
Proof. If WDγ (PπΦ, PπΦ0) = 0, there exists a coupling Π between PπΦ and PπΦ0 such that:
Eu,v〜Π [I∣u - v∣∣p] = 0
Consequently:
Eu,v〜Π	):	∣us,a - vs,a∣P =	):	Eu,v〜Π [∣us,a - vs,a∣'] = 0
(s,a)∈S×A	(s,a)∈S×A
Therefore for all (s, a) ∈ S × A:
IEu〜Pφ [us,a] - Ev〜Pφ0 [vs,a] J ≤ Eu,v~Π [∣us,a - vs,a |'] = 0
Where us,a and vs,a denote the (s, a) entries ofu and v respectively. Notice that for all (s, a) ∈ S ×A:
PπΦ(s,a) =PπΦ0(s,a)	(8)
Since for all s ∈ S and p ≥ 1:
J	Jp
JJ	us,a
Ja∈A
Therefore for all s ∈ S :
Eu〜Pφ Eus,a - Ev〜Pφ0
J	a∈A
Consequently PπΦ(s) = PπΦ0 (s) for all s ∈ S. By Bayes rule, this plus equation 8 yields:
PΦ(a∣s) = PΦ0 (a∣s)
And therefore: ∏ = ∏0.	□
- vs,a JJ ≤	∣us,a - vs,a ∣p
J	a∈A
0
Σ
≤ Eu,v〜Π
—
a
vs,a∣p
,a
a∈A
These results can be extended in the following ways:
1.	In the case of a continuous state space, it is possible to define embeddings using Kernel density
estimators. Under the appropriate smoothness conditions on the visitation frequencies, picking an
adequate bandwidth and using the appropriate norm to compare different embeddings it is possible
to derive similar results to those in Lemma 10.1 for continuous state spaces.
2.	For embeddings such as Φ5 in Section 3.1 or Φ(τ) = PtH=0 est,at, when Y = 0, if
WDγ (PπΦ, PπΦ0) ≤	then ∣V (π) - V (π0)∣ ≤ R for R = maxτ∈Γ R(τ) thus implying that a
small Wasserstein distance between π and π0s PPEs implies a small difference in their value
functions.
18
Under review as a conference paper at ICLR 2020
10.1	Random features stochastic gradients
Let φκ and φ' be two feature maps over X and Y and corresponding to kernels K and ' respectively.
For this and the following sections we will make use of the following expression:
G(pμ, PV)= β
((Pμ)>
X
φκ(x)dμ(x,θ) - β
Y(pν)>
φ'(y)dν (y)+
(9)
Yβ Z exp ((p")>φκ (X)-(PV)>φ'(y)-C(x, y) ) dμ(χ)dν(y)
X×Y	γ
We now show how to compute gradients with respect to the random feature maps:
Lemma 10.2.
The gradient V(pμ)G(pμ, PV) ofthe
objective function from Equation 9 with respect
to the parameters (PV) satisfies:
μ μt μ ν∖ _ W	IYl	( (pμ )>φκ(x) -(PV )>φ' - C(x, y)∖∖ ( φκ(x) ʌ
V(PV)G(P , P ) = βE(Xy)〜μN V [(1 - exp (	γ	- - (-φ'(yJ
Proof. A simple use of the chain rule, taking the gradients inside the expectation, and the fact that
Pμ and pv are vectors yields the desired result.	□
The main consequence of this formulation is the stochastic gradients we use in Algorithm 1.
10.2	BGPG, BGES and their theoretical guarantees
Here we provide some theoretical guarantees for our algorithms. Both proposed methods BGPG and
BGES follow the alternating optimization algorithmic template. We start by noting that stripped to
their bare bones components our algorithms satisfy two paradigms:
1.	Min-Max optimization. When β < 0 the algorithms we propose for solving the optimization
objective in Equation , turn into an Alternating Min-Max optimization procedure. Henceforth
whenever we refer to the problem defined by setting β < 0 we will call it the Min-Max problem.
2.	Max-Max optimization. When β > 0 the algorithms we propose for solving the optimization
objective in Equation turn into an alternating Max-Max procedure. Henceforth whenever we refer
to the problem defined by setting β < 0 we will call it the Max-Max problem.
Our theoretical results will therefore center around providing guarantees for optimizing an objective
of the form:
F(θ, pπθ, Pnj = Eτ1,τ2〜P∏θ NP∏0 [R(∏1) + hpπθ,Φκ(Φ(τι))i - βhpπ0, φκ(Φ(τ2))i+	(10)
d 八Pπθ,Φκ(Φ(τι))i-hΦπ0,φκ(Φ(τ2))i- C(Φ(τι), Φ(τ2))∖ i
Ye eχp i	I
γ
Where πθ is a policy parametrized by θ, π0 is a target policy, and Pπθ and Pπ0 are the feature vectors
corresponding to the Behavioral Test functions from πθ and π0 .
10.3	Max-Max Problem: theoretical analysis
We will analyze our AO algorithm for the Max-Max optimization problem. We show that obtained so-
lutions converge to the local maxima of the objective function. Consider the function F(θ, Pλπθ , Pλπ0 )
as in 10. We denote by (θ*, p)πθ, p)π0) some of its local maxima. Define F(θ) = F(θ, p)πθ, p)π0),
i.e. F is F as a function of θ for locally optimal values of pλπθ and pλ∏0.
19
Under review as a conference paper at ICLR 2020
We will assume that F is locally ζ -strongly concave and δ-smooth for some fixed ζ , δ > 0 in the
neighborhood N (θ*,r) of its optimal value θ*. We Will also assume that gradient of F is L2-Lipschitz
with Lipschitz coefficient φ in that neighborhood. The following convergence theorem holds:
Our goal in this section is to prove the folloWing Theorem:
EI.<C)L,,	.	1	1	/	1	4	7	8	4n .
Theorem 10.3. For the entropy coefficient γ, denote: φγ =古 e Y, and Uγ = 38γ e γ. Denote
φ* = max(φ,φγ) and ξ = min(δ2++ζ,uγ). Let s(t) = (θ(t), pλπθ(t), pλπ0 (t)) be the solution
from iteration t and s* the local maximum considered above. Assume that optimization starts in
θo ∈ N (θ*,r). If φ* < 2ξ, then the error at iteration t + 1 of the presented AO algorithm for the
Max-Max problem with decaying gradient step size at = 1	3/：0、l 34 satisfies:
[2ξ - 3φ*](t+2)+2 φ*
23	9
E[ks(t + 1) - s*k2] ≤ E[ks(O)- s*k2](E)3 + σ2 [2ξ- 3φ*]2(t + 3)，	(11)
where σ
,2(1 + e 2 )2 +SUPn (θ*),r)VθF(θ)2.
We Will need several auxiliary technical results. We Will use the folloWing notation: Wγ (θ, β1 , β2 ) =
F(θ, pλπθ , pλπ0 ), Where F is the objective function from the main body of the paper parameterised
by entropy coefficient γ > 0, β1 = pλπθ and β2 = pλπ0 . We Will apply this notation also in the
next section regarding the Min-Max Problem. For completeness, the definitions of strong concavity,
smoothness and Lipschitz condition from Theorem 10.3 are given in Section 10.3.1.
We consider the dual optimization problem:
max max
θ λ1∈C(X),λ2∈C(Y)
Wγ (θ, β1, β2)
max max
θ λ1∈C(X),λ2∈C(Y)
,y,κx,1 ,…,κx,D ,κy,1 ,…,κy,D )〜μ×ν×ω×…×ω
L(θ)+ λι(Φ(x))+ λ2(Φ(y)) - γexp广网'))+ %以？ - C&(x)，①⑻))],
where Y is a parameter, μ = ∏, V = ∏0, Φ is a fixed trajectories, embedding and furthermore:
λ1 (z) = β1>f(z)
λ2 (z) = β2>f (z),
such that f (∙) is a random feature vector. The ith entry of the feature vector is constructed as follows:
[f (z)]i = yJ~Co cos(z>wz,i + bz,i), where wz,i 〜N 仪,Inρ⅛) and bz,i 〜Uniform(0, 2π). For
the ease of notation we denote κz,i = (wz,i, bz,i)〜ω. We consider stochastic gradient ascent
optimization strategy of the following form:
• at time t we receive a single sample (xt, yt, KxtJ,..., κxt,D, κyt,1,..., KytQ)〜μ X V X ω X
…X ω, then we form feature vectors [f (χt)]i = ʌ/ɪCoS(X>wxt,i + bxt,i) and [f(yt)]i =
yJ~Cθ cos(y>WyI + byt,i), and finally update:
βt+1 一 Πι [βt + atf (xt)(1 - exp ((βtUt)MeYa"))]
β2+1 一 ∏2 [βt + atf(yt)(1 - exp ((et+1)Tf(xt)+(ZyTf(yt)-C(x’y)))].
∏d (d = 1, 2) denotes the projection onto the Euclidean ball B2(rd ,β0) of some given radius rd
centered at the initial iterate βd0 .
Let {β1,β2} denote the global optimum of WY (β1,β2) computed on the entire data population, i.e.
given access to an infinite number of samples (“oracle”). Let B2(r, β) denote the Euclidean ball of
radius r centered at β . For the sake of the theoretical analysis we assume that (a lower-bound on)
the radii of convergence r1 , r2 for β1 , β2, respectively, is known to the algorithm (this assumption
20
Under review as a conference paper at ICLR 2020
is potentially easy to eliminate with a more careful choice of the step size in the first iterations).
To be more specific, if at any point in time parameter βι or β2 falls outside the ball B2(r1,β1) or
B2 (r2 ,β2), respectively, the projection is applied that pushes the parameter of interest to stay in the
ball. Also, let Vβι Wl(β1,β2) and N§2 W,(βι, β2) denote the gradients of WY with respect to βι
and β2, respectively, computed for a single sample. Similarly, V§、W7(β1,β2) and Nβ2 WY(βι, β2)
be the gradient of Wγ with respect to β1 and β2, respectively, computed for the entire data population,
i.e. infinite number of samples.
Note that given any initial vector βd in the ball of radius rd centered at βd, We are guaranteed that
all iterates remain within an rd-ball of βb. This is true for all d = 1, 2. The projection is necessary
for theoretical analysis but in practice makes little difference. The above is a two-step alternated
optimization scheme.
Let the population gradient operator, Gd(β1, β2), where d = 1, 2, be defined as
Gd(β1,β2) := βd + OVedWY(β1,β2).
10.3.1	Assumptions
Let W:,ι(βι) = WY(β1,β2) and W^,2(β2) = WY(βι ,β2). Let Ω1,Ω2 denote non-empty compact
convex SetS such βι ∈ Ωι and β2 ∈ Ω2. The following assumptions are made:
Assumption 10.1 (Strong concavity). For d = 1,2, the function W^d (βd) is Zd-strongly concave
near βd, i.e. for all pairs (a,&, bd) in the neighborhood of βd the following holds
W；,d(Od)- W；,dSd)- <vβd W；,dSd), αd - bd〉≤ -Td Ilad- bd∣∣2,
where ζd > 0 is the strong concavity modulus.
Assumption 10.2 (Smoothness). For d = 1,2, thefunction W^ d(βd) is δd-smooth, i.e. for all pairs
(ad , bd ) the following holds
W7,d(ad) - W；,dSd)- <vβd WY^,d(bd), ad - bd〉≥ -^2 Ilad- bd ∣∣2,
where δd > 0 is the smoothness constant.
Assumption 10.3 (Gradient stability (GS) / Lipschitz condition). We assume WY(β1, β2) satisfy GS
(φd) COndition, for all d = 1,2, over Euclidean balls βι ∈ B2(r1,βt),β2 ∈ B2(r2, β2) given as
follows
llvβd WY,d(βd) - V βd WY (β1,β2)∣2 ≤ φdkβd - βdk2,
where φd > 0 and d= (d mod 2) + 1.
Finally, define the bound σ that considers the expected value of the norm of gradients of our
objective function as follows: σ = ,σ2 + σ2, where σ2 = sup{E[∣Vβd WYf(βι, β2)∣2] : βι ∈
B2(r1,β1Y),β2 ∈ B2(r2, β2Y)} for d = 1,2.
10.3.2	Main theorems
Theorem 10.4.	Given the stochastic gradient iterates of Max-Max method with decaying step size
{αt}∞=0 and with φ < 2ξ the error at iteration t +1 satisfies the recursion
IP-II qt+1 甲 l∣2 __I_ Imt+1 甲 l∣2^∣ V 门 nt∖ IP ∣^∣∣qt q*∣∣2 I IIqt 甲 ∣∣2^∣ I (a ) n-2
E [∣β1	- β1 ∣2 + kβ2	- β2 l∣2.∣ ≤ (1 - q )E [∣β1 - β1 l∣2 + kβ2 - β2 3 + ] - 插。。,
where φ = maxd=1,2(φd), qt = 1 — 1-20-0+2，°, and ξ = mind=1,2 (δ2⅛⅛)∙
The recursion in Theorem 10.4 is expanded yielding the convergence theorem:
Theorem 10.5.	Given the stochastic gradient iterates of Max-Max method with decaying step size
ɑt = [2ξ-3φ3/+2)+3Φ and assuming that φ < 2ξ, the error at iteration t + 1 satisfies
IP-Il Et+1 q*ll2-∣- llEt+1 EyII2I	<- F	Γll E0	®*||2_i_||®0 EyII2I	(	2 ʌ ^∣ zτ2____9_______
E [∣β1	-β1 l∣2 + kβ2	-β2 II2J	≤ E	[∣β1	-β1 l∣2 + kβ2	-β2 II2J	It	+ 3 ) +σ [2ξ	- 3φ]2(t	+ 3)
where φ = maxd=1,2(φd) andξ = mind=1,2 (δ⅛⅛)∙
21
Under review as a conference paper at ICLR 2020
10.3.3	Analysis
The theoretical analysis we provide below is an extension of the analysis in Balakrishnan et al. (2017)
to the two-step alternated optimization scheme.
Proof of Theorem 10.5 relies on Theorem 10.4, which in turn relies on Theorem 10.7 and Lemma
10.6, both of which are stated below. Proofs of the lemma and theorems follow in the subsequent
subsections.
The next result is a standard result from convex optimization (Theorem 2.1.14 in Nesterov (2014))
and is used in the proof of Theorem 10.7 below.
Lemma 10.6. The gradient operator Gι(βι,βg) under Assumption 10.1 (strong concavity) and
Assumption 10.2 (smoothness) with constant step size choice 0 < ɑ ≤ j1+ZI is contractive, i.e.
kGι(βι,β2) - β^k2 ≤(1 - 2αδ1ζ1) kβl -阳2	(12)
2	1	δ1 + ζ1	1
for all βι ∈ B2(r1, βι).
Similarly, the gradient operator G2(β1,β2) under Assumption 10.1 (strong concavity) andAssump-
tion 10.2 (smoothness) with constant SteP size choice 0 < a ≤ j2+ζ2 is contractive, i.e.
kG2(W,β2) - β和2 ≤ (1 - 2αδ2ζ2) kβ2 - β和2	(13)
δ2 + ζ2
forall β2 ∈ B2(r2,β2).
The next theorem also holds for d = 1, 2. Let r1,r2 > 0 and βι ∈ B2(r1,β1), β2 ∈ B2(r2, β2).
Theorem 10.7. For some radius rd > 0 (d = 1,2) and a triplet (φd,ζd,δd) such that 0 ≤ φd <
ζd ≤ δd, suppose that the function W； d(βd) is Zd-StrOngly concave and δd-smooth, and that the
GS (φd) condition holds. Then the population gradient operator Gd(β1, β2) with step α such that
0 < α ≤ mind=1,2 δd+Zd is contractive overa ball B2(rd, β√), i.e.
kGd(βι, β2) - βdk2 ≤ (1 - ξα)∣∣βd - βM∣2 + αφkβj - β益2	(14)
where d = (d mod 2) + 1, φ := maxd=1,2 φd, and ξ := mind=ι,2 2；：Zd.
Proof.
kGd(β1,β2) - βdk2 = kβd + ZedWY(β1,β2) - βM∣2
by the triangle inequality (and with d = d) we further get
≤ llβd + α^βd W；,d(βd) - βdk2 + α∣Nβd WY(β1, β2) - VβdWj,d(βd)∣2
by the contractivity from Lemma 10.6 and GS condition
≤ (1 - ~Γ~rf^) kβd - βd k2 + αφdkβd - βJk2.
d + Zd
□
Proof of Theorem 10.4
Let	βd+1	=	∏d(βd+1),	where	βt+1	:=	βt	+	QtNeIW1(βt,β2)	and	β2+1	：=	β2	+
αtVβ2WL(βt+1,β2), where VβdWL is the gradient computed with respect to a single sample,
βι and β2 are the updates prior to the projection onto a ball B2(号,β0). Let ∆d+1 ：= βd+1-βd and
δ d+1 ：= βd+1-βd. ThUS
k∆d+1k2 -k∆dk2	≤	k∆d+1k2 -k∆dk2
=kβd+1-βdk-kβd-β刘
—	Dβt+1	— Rt	At+1 +	Rt	_	2β*E
=	βd	- βd ,	βd +	βd	-	2βd .
22
Under review as a conference paper at ICLR 2020
Let Ql := Vβ1 W1(βt,βt) and Q2 := Vβ2 W1(βt+1, β2). Then we have that ∕¾+1 - β% = ɑtQ)td.
1 _l_	LI /'，_!.，， 4/	2 2	2 2 γ ∖1 _l_	' (√.	v H'
We combine it with Equation 17 and obtain:
k∆d+1k2 -k∆dk2
≤ DatQd,αtQd + 2(βd - βi))
=(αt)2(Qd)TQ d + 2αt(Qd)>(βd-βK
=(αt)2∣∣Qdk2 + 2αt DQd, &〉.
Let Qi := Nβι WT(βt,β2) and Q2 ：= ^β2 WT(βt+1,β2). By the properties of martingales, i.e.
iterated expectations and tower property:
E[∣∣∆d+1∣∣2] ≤ E[∣∣∆dk2] + (αt)2E[kQdk2] + 2ɑtE[(Qd, &〉]	(15)
Let Qd ：= Ned W7(βι, β2). By self-consistency, i.e. βd = argmax^^eod W^ d(βd), and convexity
of Ωd we have that
〈Qd，∆d> = <VβdWγ(W,βA ∆d> =0.
Combining this with Equation 15 we have
E[∣∣∆d+1∣∣2] ≤ E[∣∣∆d∣∣2] + (αt)2E[∣∣Qd∣∣2] + 2αtE[(Qd- Qd, ∆d>]∙
Define 仍：=βd + αtQd and Gd := βd + αtQd∙ Thus
αt (Qd - Qd, ∆d>
=〈Gd-	Gd	-	(βd-	βd),βd-	βd)
=〈Gd-	Gd*,	βd- βd)-	ι∣βd-	βdk2
by the fact that Gtd = β才 + αtQd = βd (since Qd = 0):
=〈Gd- βd, βd- βd) -kβd- βdk2
by the contractivity of Gt from Theorem 10.7:
≤	{(1 - αtξ)kβd - β莉 + αtφ (X kβt+1 - β*∣2
2
+ X 怩-β;
i=d+1
kβd-βM∣2 -∣∣βd-βMl2
{(1 - atg)ll&k2 + αtφ
2
k△厂1∣∣2 + X IAll2
i=d+1
≤
))
,1㈤心 —||&||2
Combining this result with Equation 16 gives
E[∣∆d+1k2]
E[∣∣∆d∣∣2] + (at)2E[∣∣Q dk2] + 2E
{(1 - ɑtg)l^dg + atφ
2
∣∣∆t+1∣∣2 + X ∣∣∆t∣∣2
i=d+1
))
≤
•口&g 一||*||2]
≤ E[∣∣∆dk2] + (αt)2σ2 + 2E
{(1 - atg)ll&k2 + αtφ
2
k△厂1∣∣2 + X IAll2
i=d+1
))
•口&g一||*||2].
23
Under review as a conference paper at ICLR 2020
After re-arranging the terms we obtain
E[k∆td+1k22] ≤ (αt)2σd2 + (1 - 2αtξ)E[k∆tdk22] + 2αtφE	k∆it+1k2 + X2 k∆itk2 k∆tdk2
i=d+1
apply 2ab ≤ a2 + b2 :
d-1
(at)2σ + (1- 2αtξ)E[k∆dk2] + αtφE X (∣∣∆t+1∣∣2 + ∣∣∆d∣∣2)
i=1
d-1
(at)2σd + E[k∆dk2] ∙ [1 - 2αtξ + αtφ] + αtφE	X |出+1||2
i=1
We obtained
2
+ αtφE X (∣∣∆tk2 + k∆dk2)
i=d+1
2
+ αtφE	X k∆itk22
i=d+1
d-1
E[k∆td+1k22] ≤ (αt)2σd2+[1-2αtξ+αtφ]E[k∆tdk22]+αtφE Xk∆it+1k22
i=1
we next re-group the terms as follows
d-1
E[k∆td+1k22] - αtφE Xk∆it+1k22
i=1
and then sum over d from 1 to 2
2
E X k∆td+1k22
d=1
2
+ αtφE	X k∆itk22
i=d+1
2
≤ [1 -2αtξ+αtφ]E[k∆tdk22] +αtφE	X k∆itk22	+ (αt)2σd2
i=d+1
2 d-1
- αtφE XX k∆it+1k22
d=1 i=1
2
≤ [1 - 2αtξ + αtφ]E Xk∆tdk22
d=1
22
+ αtφE XX k∆itk22
d=1 i=d+1
2
+ (αt )2 X σd2
d=1
Let σ = σ0+ + σ22. Also, note that
22
E Xk∆td+1k22 -αtφE Xk∆td+1k22
d=1	d=1
and
2
[1 - 2αtξ + αtφ]E Xk∆tdk22
d=1
2	2 d-1
≤E Xk∆td+1k22 -αtφE XXk∆it+1k22
d=1	d=1 i=1
22
+ αtφE XX k∆itk22 + (αt)2σ2
d=1 i=d+1
≤
≤
22
[1 - 2αtξ + αtφ]E Xk∆tdk22 +αtφE Xk∆tdk22
d=1	d=1
+ (αt)2σ2
Combining these two facts with our previous results yields:
2
[1 - αtφ]E X k∆td+1k22
d=1
22
≤ [1 - 2αtξ + αtφ]E Xk∆tdk22 +αtφE Xk∆tdk22 + (αt)2σ2
d=1	d=1
2
=[1-2αtξ+2αtφ]E Xk∆tdk22 + (αt)2σ2
d=1
24

oβ∙sM-IOJ əf q∙aM dnlpuə əʌv
CN
%1+%7J(∖ — I) +%ZIJ(I— I)(I — I)+
注L≤M出1 — I)(L —1)(1 I I)
V
V
0AEqoΛ∖ SnqI
olOUop PUB
03— I
+ 3*^e — I
SE 寸sUIoJOoqI∙s七 pouyωp əMaq 二 ɪŋɔəX
POEsqo əʌv τsUIOJOOqI so⅛ uo-jnɔəj əip PUEdXə2pəəu əM uɪəjoə-ɪBUy-1 URlqO OH
∙sUI9J09qI jo Joo£
IV^≡⅛√H V0≡O∙SS
%nql
U xuOIaJOdEd əɔuəjəjuoɔ E SE m-aəj Jopun
Under review as a conference paper at ICLR 2020
3
Set qt =旨 and
αt
qt
2ξ - 3φ + qt φ
3
2
[2ξ - 3φ](t + 2) + 2φ
Denote A = 2ξ - 3φ and B = ∣ φ. Thus
αt
3
_____2______
A(t + 2) + B
and
产
(αt)2
1 - 3 Bat
9
4
A(t + 2)[A(t + 2) + B]
E
‹ E
2
X k∆d+1k2
.d=1	.
2
t
2	t /
X k∆dk2 ∏ 1
d= 1	i = ∩ ∖
—
.d=1
i=∩
3
2
i + 2
t-1
+ σ2 X
i=∩
9
4
A(i + 2)[A(i + 2) + B]
∏ (
j=i+1、
3
1——2-
j+2
+σ2
9
4
A(t + 2)[A(t + 2) + B]
2
t+2
E
2	t + 2 /
X k∆dk2 ∏ 1
d=1	i=2 ∖
.d=1
i=2
Since A > 0 and B > 0 thus
E
‹ E
< E
—
3∖	t+1
工)+σ2 X
9	t+2 /	3、
…j∏1 (1- R+ σ2
9
4
A(t + 2)[A(t + 2) + B]
2
X k∆d+1k2
.d=1	.
2	t + 2 /
X k∆dk2 ∏ 1
d=1	i=2 ∖
2
t+2
—
.d=1
2
i=2
t+2
3∖	t+1
工)+σ2 X
9
4
Ai [Ai + B]
t+2	/	3∖
∏	1 - ɪ + σ2
j=i+1 ∖
9
4
A(t + 2)[A(t + 2) + B]
2	t+2
X k∆dk2 ∏ 1
d=1	i=2 ∖
.d=1
i=2
—
3∖ t+1 9 t+2 / 3∖	9
R+σ2 X ⅛ II (1 - R+σ2 MZ可
We can next use the fact that for any a ∈ (1,2):
t+2
∏
i=τ +1
τ + 1 y
t + 3 y
The bound then becomes
E
< E
< E
E
2
X k∆d+1k2
.d=1
2t
t+2
Ek∆dk2 ∏
t+2
.d=1
2
X k∆dk2
.d=1
2
X k∆dk2
.d=1
i=2
/	3 ∖	t+1	9	t+2	/
(1 -	÷)+σ2	X	⅛ j∏1 (1
i =	j = i+
39
-ɪ ) + σ2——4—-
3) +	[A(t + 2)]2
2 T	2 t+1
F + σ2∑
/	i=2
2	、2 t+2
t⅛) + σ2 X
、3
山 Y+ σ2
t + 3； +
3
i + 1Y
t + 3 y
9
4
[A(t + 2)]2
26
Under review as a conference paper at ICLR 2020
Note that (i + 1) 2 ≤ 2i for i = 2,3,..., thus
2
E X k∆td+1k22
d=1
2	2	9	t+2	ι∖3
≤ E X k∆dk2 (2)+ σ2—J X (i+1)2
- yιι d∣∣2 't + 3^	A2(t + 3)2 J	i2
d=1	i=2
「2	1 2 2 、3	9	t+2 I
≤ E X k∆dk2	-ʌ + σ2-2~r X ɪ
—	[d=1	d J V + 3/	A2(t + 3)2 y i1
t + 2 1	ft+2 1	1
finally note that — ≤	— dx ≤ 2(t + 3)2. Thus
i=2 i 2	h X 2
「2	1	/入、3	C
≤ E ]X k 叫(El + σ2 E3)
substituting A = 2ξ - 3φ gives
-2	-∣	3
=E X 同k2 (t+3) + σ2 [2ξ - 3φ]2(t + 3)
This leads us to the final theorem.
Proof of Theorem 10.3
In order to prove Theorem 10.3 it suffices to apply Theorem 10.5 and notice that:
w>v	C
•	function hv,c : Rd → R defined as follows: hv,c(W) = w>v — Aefor A = Ye-Y is
2 e4 -smooth, 2-strongly concave and its gradient is LiPSchitz with Lipschitz coefficient 2γ e 4
(with respect to L2-norm) for C > 0 and v satisfying: |w>v| ≤ 1,
•	∣∣Vhv,c(v)k2 ≤ 2(1 + eγ)2 under above conditions.
10.4 MinMax Problem: theoretical analysis
In this section we aim to obtain similar results for Min-Max Problem as for Max-Max problem. We
will use the same notation as in the main body of the paper. We prove the following results:
Theorem 10.8. Denote φ* = max(φ, φ7) and ξ = min(δ2+ζ, u7), where φ7 and u7 are as in
Theorem 10.3. Let s(t) = (θ(t), pλπθ (t), pλπ0 (t)) be the solution obtained in iteration t and S小
the local optimum. Assume that optimization starts in θo ∈ N(θ*,r). If φ* < 3, then the error at
iteration t + 1 of the alternating optimization algorithm for the Min-Max problem with decaying
gradient SteP Size αt = [2ξ-6φ*号2)+3φ* SatiSfies 2	9
E[∣s(t + 1) — s*k2] ≤ E[∣s(0) — s*∣2](κ)3 + σ2	,	(16)
t + 3	[2ξ — 6φ*]2(t + 3)
where σ
VZ2(1 + e2 )2 + SUpN(θ*),r) vθF(θ)2.
We use the same technical notation as in the previous section, with the only exception that this time
we denote: Wγ (θ, β1, β2) = —F (θ, pλπθ , pλπ0 ). We consider the MinMax problem of the following
form
min max
θ λ1∈C(X),λ2∈C(Y)
Wγ (θ, β1, β2)
min max
θ λ1∈C(X),λ2∈C(Y)
,y,κx,1,…,κx,D ,κy,1,…,κy,D )~μ×ν×ω× …×ω
-L(θ) + λι(Φ(x)) + λ2(Φ(y)) — γexp(X)) + 为&? - 0阳以①⑻))
27
Under review as a conference paper at ICLR 2020
where γ is a parameter and the remaining notation is analogous to the Max-Max case.
We consider mixed stochastic gradient descent/ascend optimization strategy of the following form:
•	at time t we receive a single sample (Xt, yt, κxt,1,..., κxt,D,κyt,1,..., κyt,D)〜 ∏θt X V X
ω ×∙∙∙× ω, then We form feature vectors [f (xt)]i =y/coCOS(X>Wxt,i + bxt,i) αnd [f (yt)]i =
yJ~Cj cos(y>Wzt" + byt,i), and finally update:
θt+1 一 ∏1 [θt — αtVθ=θtWY(θ, β1, β2)]4
βt+1 一 ∏2 [βt + αtf(xt)(1 - exp (⑹一(Xt)Ze2Jf(yt)-C(x,y)))]
β2+1 一 Π3 [βt + αtf(yt)(1 - exp ((βt+1)>f(Xt)+*)>"yt)-C(X,y)))].
∏ι denotes the projection onto the Euclidean ball B2(号，θ0) and ∏d (d = 1, 2) denotes the projection
onto the Euclidean ball B2(号,βd).
Let {θ*,βj,β2} denote the the global optimal solution of the saddle point problem
minθ maxβ1,β2 Wγ (θ, β1, β2) computed on the entire data population, i.e. given access to an infinite
number of samples (“oracle”). As before, we assume that (a lower-bound on) the radii of convergence
r1, r2, r3 for θ, β1, β2, respectively, is known to the algorithm and thus the projection is applied to
control θ, β1, β2 to stay in their respective balls. Also, let VθWγ1(θ, β1, β2), Vβ1 Wγ1(θ, β1, β2) and
Vβ2 Wγ1(θ, β1, β2) denote the gradients of Wγ with respect to θ,β1 and β2, respectively, computed
for a single sample. Similarly, Vθ Wγ (θ, β1, β2), Vβ1Wγ(θ, β1, β2) and Vβ2 Wγ (θ, β1, β2) be the
gradient of Wγ with respect to θ, β1 and β2, respectively, computed for the entire data population, i.e.
infinite number of samples.
Note that given any initial vector θ0 in the ball of radius 号 centered at θ*, we are guaranteed that all
iterates remain within an ri-ball of θ* and given any initial vector β0 (d = 1,2) in the ball of radius
rd centered at βd, we are guaranteed that all iterates remain within an rd-ball of βb. The projection
is necessary for theoretical analysis but in practice makes little difference. The above is a three-step
alternated optimization scheme.
Let the population gradient operator, Gd(θ, β1, β2), where d = 1, 2, 3, be defined as
G1(θ,β1,β2) := θ - αVθWY(θ,β1,β2)
and
Gd(θ, β1,β2) ：= βd + &▽« WY(θ, βι, β2) for d =2, 3.
10.4.1 Assumptions
Let W；,i(。)= WY(θ,β1 ,β2) W^(βι) = WY(θ*,βι,β力 and 卬力匣)=WY(θ*,βf,β2).Let
Ωι, Ω2, Ω3 denote non-empty compact convex sets such θ ∈ Ω1,β1 ∈ Ω2 and β2 ∈ Ω3. The
following assumptions are made:
Assumption 10.4 (Strong Convexity/concavity). ThefUnction W:,ι(θ) is Zi-strongly convex near θ*
and thefunctions W^2(β1) and W^3(β2) are Z2- and Z3-strongly concave, respectively, near β1 and
β2, respectively, where Z1,Z2,Z3 > 0∙
Assumption 10.5 (Smoothness). ThefUnctions W:,ι(θ), W^,2(β1) ,and W^,3(β2) are δι -, δ2 -, and
δ3-smooth, respectively, where δi, δ2, δ3 > 0 are the smoothness constants.
Assumption 10.6 (Gradient stability (GS) / Lipschitz condition). We assUme WY(θ, βi, β2) satisfy
GS (φd) condition, for all d = 1,2, 3, over Euclidean balls θ ∈ B2(r1,θ*),β1 ∈ B2(r2,β1), β2 ∈
B2(r3, β2) given asfollows
2
kVθWYi,ι(θ) - VθWY(θ, β1,β2)k2 ≤ φi X kβd - βdk2,
d=i
4later we also use alternative notation for gradient Vθ=θt WY (θ, βt ,βt) as Vθ WY (θt ,βt ,βt)
28
Under review as a conference paper at ICLR 2020
and for d = 1, 2
INβdW；,d+Ked)- ^βdWY(θ,β1,β2)k2 ≤ φd(Ilθ - θ*∣∣2 + IIeN- βJk2),
where φd > 0 and d= (d mod 2) + L
Finally, as before, define the bound σ that considers the expected value of the norm
of gradients of our objective function as follows: σ = ^σ+ + σ22 + σ22, where σ2 =
sup{E[∣∣Vθ Wγ1(θ,βι ,β2)k2] ： θ ∈ B2(r1,θ*),e1 ∈ B2(r2,ei'),e2 ∈ B2(r3,e 力} and for d = 1,2
σ2+ι =sup{E[∣VβdWγ1(θ,βι,e2)|用：θ ∈ B2(n,θ*),e1 ∈ B2(τ2,eV),e2 ∈ B2(r3,e力}.
10.4.2 Main theorems
Theorem 10.9.	Given the stochastic gradient iterates of MinMax method with decaying step size
{αt}∞=o and with φ < ξ the error at iteration t +1 satisfies the recursion
E [kθt+1 - θ*k2 + kβt+1 - e；k2 + kβt+1 - e郛 2]
≤ (1 - qt)E [kθt - θ*k2 + kβt - e；k2 + kβt - β2ik2] + 占)2φσ2,
where Φ = maxd=1,2,3(φd), qt = 1 - 1-2哆+著” ,and ξ = mind=1,2,3 (∣⅛⅛)
The recursion in Theorem 10.4 is expanded yielding the convergence theorem:
Theorem 10.10.	Given the stochastic gradient iterates of MinMax method with decaying step size
ɑt = [2ξ-6φ3∕t2+2)+3φ and assuming that φ < ξ, the error at iteration t + 1 satisfies
E [kθt+1 - θ*k2 + kβt+1 - e；k2 + kβt+1 - e；k2]
3
∕^∏7∣^i∣q0	Q*ι∣2∣ι∣a0	a*ι∣2∣ι∣a0 a*ι∣2^l ( 2 ∖	∣~2	9
≤ 叫kθ - θ k2 + kβ1 - β1 k2 + kβ2-β2 团+ σ [2ξ- 6φ]2(t + 3)，
where φ = maxd=1,2,3(φd) andξ = mind=1,2,3 (⅛⅛)∙
Proof of Theorem 10.10 relies on Theorem 10.9, which in turn relies on Theorem 10.12 and Lemma
10.11, both of which are stated below. Proofs of the lemma and theorems follow in the subsequent
subsections.
10.4.3 Analysis
The next result is a standard result from convex optimization (Theorem 2.1.14 in Nesterov (2014))
and is used in the proof of Theorem 10.12 below.
Lemma 10.11. The gradient operator Gι(θ, βi,βg) under strong convexity and smoothness assump-
tions with constant SteP size choice 0 < ɑ ≤ 近:《]is contractive, i.e.
kGι(θ,eκe;) -θ*k2 ≤ (1 - Ia^) ∣θ-θ*∣∣2
1	2	δ1 + ζ1
forall θ ∈ B2(rι,θ*).
Similarly, the gradient operator Gι(θ*,βι, βg) under strong concavity and smoothness assumptions
with COnStant SteP size choice 0 < a ≤ 九+ZC is contractive, i.e.
δ2 + ζ2
kGl(θ*,β1,⑹-e；k2 ≤ (1-崇)怩-团2
forall βι ∈ B2(r2,ei).
And similarly, the gradient operator G (θ* ,β；,尸2) under strong concavity and smoothness assump-
tions with constant SteP size choice 0 < a ≤	+广.is contractive, i.e.
δ3+Z3
kG2(θ*M,e2) - e；k2 ≤ (1 - !O+!) kβ2 -β k2
forall β2 ∈ B2(r3,e2).
29
Under review as a conference paper at ICLR 2020
The next theorem holds for d = 1,2,3. Let r1,r2,r3 > 0 and θ ∈ B2(r1,θ*),β1 ∈ B2(r2,β1),β2 ∈
B2(r3,β2)∙
Theorem 10.12. For some radius r1 and a triplet (φ1, ζ1, δ1) such that 0 ≤ φ1 < ζ1 ≤ δ1, suppose
that the function W^I (θ) is Zi-Strongly convex and δι -smooth and that the GS (φι) condition holds.
Then the population gradient operator G1(θ,β1,β2) with step a such that 0 < ɑ ≤ mind=1,2,3 §d+Zd
is contractive overa ball B2(ri,θ*), i.e.
2
kG1(θ, β1,β2) - θ*k2 ≤ (1 - ξα)kθ - θ*k2 + αφ X Iled- βdk2
d=1
where φ := maxd=1,2,3 φd and ξ := mind=1,2,3 δd⅛∙
For some radius rd (d = 2, 3) and a triplet (φd, ζd, δd) such that 0 ≤ φd < ζd ≤ δd, suppose that
thefunction W； d(βd-i) is Zd-strongly concave and δd-smooth and that the GS (φd) condition holds.
Then the population gradient operator Gd(θ, β1,β2) with step a such that 0 < ɑ ≤ mind=1,2,3 §d+Zd
is contractive overa ball B2 (rd, β^), i.e.
kGd(θ,βi,β2)- βdk2 ≤ (1-ξα)kβd- βd∣∣2 + αφ(kβj - β割∣2 + kθ-θ*k2)∙
where d= ((d — 1) mod 2) + 1, φ := maxd=1,2,3 φd, and ξ := mind=1,2,3 2+d.
Proof.
kG1(θ,β1,β2) — θ*∣∣2 = kθ — αVθ WY (θ,β1,β2) — θ*∣∣2
by the triangle inequality we further get
≤ kθ — αVθW' — θ*∣∣2 + αkVθWY(θ,β1,β2) — VθW"∣2
by the contractivity from Lemma 10.11 and GS condition
2
≤ (1 一 δ ；+1z1 ) kθ 一 θ*k2 + αφ1 X kβd 一 βdk2 ∙
The proof of the rest of the theorem is analogous to the proof of Theorem 10.7.
□
Proof of Theorem 10.9
Let θ1 = θ, θ2 = β1, and θ3 = β2 .
Let	θd+i	=	∏d(θd+i),	where	θt+i	:=	θtt	—	αtVθ1 W^(θt,θ%,θ3),力2+1	=	。2	+
αtVθ2Wγ1(θt+1,θ2,θ3), and θ⅞+1 := θ3 + αtVθ3Wγ1(θt+1,θ2+1,θ3), where VθdWY1 is the gra-
dient computed with respect to a single sample,京1,京2, and & are the updates prior to the projection.
Let ∆1+1 := —θt+1 + θ; and for d = 2, 3, ∆d+1 :=优+1 — θ*. Thus
k∆d+1k2 — k∆dk2 ≤ k∆d+1k2 - kadl修
=k^d+1 - θ：k - kθd - θ: k
=D 疗1 -θd ,θd+1+θd - %E.
Let Qi := VθιWγ1(θ1 ,θt,θ3), Q2 := Vθ2Wγ1(θt+1,θ2,θ3), and Q3 := Vθ3Wγ1(θt+1,θ2+1,θ3).
Thus:
k∆td+1k22 — k∆tdk22
≤ DatQd,αtQd + 2(θd-θ 才))
=(at)2(Q d)>Qd + 2at(Qd)>(θd-θ^)
=(at)2kQdk2 +2at (Gd, ∆dE .
30
Under review as a conference paper at ICLR 2020
Let Q1 := Vθ1 W7(θt网,θ3), Q2 := Vθ2 W7阴+1,微棋),and Q3 := Vθ3 W7阴+1,%+1*3). By
the properties of martingales, i.e. iterated expectations and tower property:
E[∣∣∆d+1∣∣2]	≤ E[∣∣∆d∣∣2] + ")2E[∣∣Qdk2]+2αtE[<Qd, &〉]
Let Qdi := Ved Wγ(θɪ,班,θ3). By self-consistency, i.e. % = argmaxθd∈Ωd W^ d(θd), and convex-
ity of Ωd we have that
〈Qd, ∆td) = <VθdWγ (θi,θi,θi), ∆td) = 0.
Combining this with the above inequality yields
E[∣∣∆d+1∣∣2] ≤ E[∣∣∆dk2] + ")2E[kQdk2] + 2α'EKQd- Qd ∆d>].
Define Gt := θt{ — αtQl and GF := θi - αtQɪ. Also, for d = 2,3 define Gd = % + atQd and
Gd* := θd + αtQd∙Thus
αt〈Qd- Qd, &〉
(Gd-Gd*-(θd-θd),θd-θ*>
(Gd-Gd*,θd-θ*> -∣∣θd-θd∣∣2
by the fact that Gdi = θ* + αtQd = θ* (since Qd = 0):
=<Gd-θ* ,θd-θ* >-∣∣θd-θ*∣∣2
by the contractivity of Gt from Theorem 10.7:
≤	{(1 - αtξ)kθd - θ*k + αtφ (XX kθt+ 1 - θ"∣2
+ XX 花-θ"∣2)]kθd-θd∣∣2 -kθd-θ*k2
i=d+1	) J
≤
{(I- atξ州&口2 + atφ
3
I∣∆t+1∣∣2 + X ∣A∣∣2
i=d+1
))
∙k∆dk2 -k∆dk2
Thus
E[∣∣∆d+1k2]	≤
E[∣∣∆d∣∣2] + (αt)2E[kQdk2] + 2αtE[(Qd - Qd, ∆d>]
E[∣∣∆d∣∣2] + (αt)2E[∣∣Qdk2] + 2E ](1-αtξ)∣∣∆d∣∣2 + αtφ (XX a+% + XX BtQ]
_ I	∖i=1	i=d+1	) J
』&心 一||&||2]
E[∣∣∆d∣∣2] + (αt)2σ2 + 2E 1(1 - αtξ)∣∣∆d∣∣2 + αtφ (X kN+1∣∣2 + XX ∣A∣∣2)]
_l	∖i=1	i=d+1	) J
』&心一||&||2].
After re-arranging the terms we obtain
E[∣∣∆d+1∣∣2] ≤ (αt)2σ2 + (1 - 2αtξ)E[∣∣∆d∣∣2] + 2αtφE	∣∣∆t+1∣∣2 + XX ∣A∣∣2)闾心
i=d+1	)
apply 20b ≤ α2 + b2 :
"d-1	"I	「3
(Ot)2σ + (1 - 2αtξ)E[∣∣∆d∣∣2] + αtφE £(凶+1口2 + k∆dk2) + 版熊 X (kNk2 + I∣∆dk2)
,i=1	」	Li=d+1
d-1
(0t)2σ2 + E[∣∣∆d∣∣2] ∙ [1 - 20tξ + 2αtφ] + αtφE X ∣∣∆t+1∣∣2
_i=1
3
+ αtφE X ∣∣∆t∣∣2
_i=d+1	.
≤
≤
31
Under review as a conference paper at ICLR 2020
We obtained
d-1
E[k∆td+1k22] ≤ (αt)2σd2 + [1 - 2αtξ + 2αtφ]E[k∆tdk22] + αtφE Xk∆it+1k22
i=1
we next re-group the terms as follows
d-1
E[k∆td+1k22] - αtφE Xk∆it+1k22
i=1
and then sum over d from 1 to 3
3
E X k∆td+1k22
d=1
3
+ αtφE	X k∆itk22
i=d+1
3
≤ [1 -	2αtξ +	2αtφ]E[k∆tdk22]	+αtφE	X	k∆itk22	+	(αt)2σd2
i=d+1
3 d-1
- αtφE XX k∆it+1k22
d=1 i=1
3	33
≤ [1 - 2αtξ + 2αtφ]E Xk∆tdk22 + αtφE X X k∆itk22
d=1	d=1 i=d+1
3
+ 2(αt )2 X σd2
d=1
Note that
33
E Xk∆td+1k22 -2αtφE Xk∆td+1k22
d=1	d=1
and
3
[1 - 2αtξ + 2αtφ]E Xk∆tdk22
d=1
3
≤ E X k∆td+1k22
d=1
3 d-1
- αtφE XX k∆it+1k22
33
+αtφE X X k∆itk22	+ (αt)2σ2
d=1 i=d+1
33
≤	[1 - 2αtξ + 2αtφ]E Xk∆tdk22 + 2αtφE Xk∆tdk22 + (αt)2σ2
d=1	d=1
Combining these two facts with our previous results yields:
3
[1 - 2αtφ]E X k∆td+1k22
d=1
≤ [1 — 2αtξ + 2αtφ]E	3 Xk∆tdk22	+ 2αtφE	3 Xk∆tdk22	+ (αt)2σ2
= [1 — 2αtξ + 2αtφ]E	d=1 3 Xk∆tdk22 d=1	+ (αt)2σ2	d=1	
Thus:
3
E X k∆td+1k22
d=1
≤ 1 - 2αtξ + 4αtφE
—	1 — 2αtφ
+ KJ σ2.
1 — 2αtφ
3
Xk∆tdk22
d=1
Since φ < 3, 1-2-tξ+4αtφ < L
Proof of Theorem 10.10
To obtain the final theorem we need to expand the recursion from Theorem 10.4. We obtained
32
⅛+r-bIJ+咨L≤
aIL
bj+=≤M⅛1I)Vl

V
0AEqoΛ∖ SnqI
olOUop PUB
一 09 IwZ 一飞——0飞中 + Wdz — I I «
Se 寸01 UlsJOOqI U丁 b PsUgəp əm5WIɪeoəu
:+=< =
口(0l¾z — I
国二 09 —WZ 一飞
;+=< =
WIt* — I I
出一 0Z — W 一dz — I V
=I+<«出
U xu0IaJOdEd əɔuəjəjuoɔ E SE m-aəj Jopun
CN
• -2+ (z + 2k(z + 2rIi¾2⅜ — I
6
2 + (z + 2)y 3
q H ¾
PUB
SnqI ∙0m uω3pue 09 —-UFsOUəɑ
∙0m + (e+l09l3lI
+ 09 —-
e
OH^
(；=矣≤
出 V
=I+<«出
oβ∙sM-IOJ əf q∙aM dnlpuə əʌv
b-+%I——J(1 — I) +%z——-(I——1 — 1)(1 — I)+
-⅛-M 国(Z——A — 1)(L— 1)(1
ɪʧ +%7√∙(∖ — I) + - + l=zk≤
国(Z
bl+%-▲5— I)+咨 T<=Ma7∖II)(∖
Under review as a conference paper at ICLR 2020
- 2
E X k∆+1k2
_d=1	.
≤
2	t	3	t-1	9	t	3
EIX "δT Y (ι - 口+σ2 X …总十…N (ι - G)
9
+σ2———~——.
A(t + 2)[A(t + 2) + B]
2	t+2 /	3 ∖	t+l	9	t+2 /	3 ∖	9
E 忙"δT Y (1 T + σ2 X …j∏ι (1 -力 σ2 A(t + 2)At + 2) + B]
Since A > 0 and B > 0 thus
E
E
E
3
X k∆d+1k2
d=1
3
≤
t+2
3	t + 2 /
X 同k2 Y 1
d= 1	i=2 ∖
d=1
i=2
3 ∖	t+1
÷)+σ2 X
9
4
Ai [Ai + B]
t+2	/	3 ∖
Y 1 -力+ σ2
j=i+1 ∖	JJ
A(t + 2)[A(t + 2) + B]
—
9
4
3	t+2	/	3 ∖	t+1	9	t+2	/
X k∆dk2	Y 1 - ɪ	+ σ2 X	⅛	Y 1 -
.d=1	」i=2 ∖	/	i=2 ' j j=i+1 ∖
9
+ σ2——4--
+	[A(t + 2)]2
We can next use the fact that for any a ∈ (1,2):
The bound then becomes
≤
≤
≤
a
E
E
E
E
3
X k∆d+1k2
d=1
3
X k∆dk2
d=1
3
X k∆dk2
d=1
3
X k∆dk2
d=1
t+2
Y
i=2
t+2
/	3 ∖	t+1	9 t+2	/
(1-	*)+σ2	X	⅛ j1+1 卜
i =	j = i+
2 λ3	2 t+1	' i +1、3
+ σ2 X	+ σ
(t^ ) 2 + ^2 X ⅛ ( " ) 2
∖	9
I + σ2--4---
)[A(t + 2)]2
9
4
[A(t + 2)]2
3
_ 2
j
2
34
Under review as a conference paper at ICLR 2020
Note that (i + 1) 2 ≤ 2i for i = 2,3,..., thus
3
E X k∆td+1k22
d=1
2	2	2	2	9	t+2	3
≤ E X k∆dk2	+ σ2—J X (i+1)2
- yιι d∣∣2 't + 3^	A2(t + 3)2 J	i2
d=1	i=2
「3	1 2 2、3	9	t+2 1
≤ E X ∣∣∆d∣∣2	ʌ	+ σ22—W X -1
一 £.11 d∣∣2 t+ + 3；	A2(t + 3)2 ⅛i2
≤
t+2 ι	∕*t+2 ι	ι
finally note that tS∖ — ≤	— dx ≤ 2(t + 3)2. Thus
i=2 i 2	J1 X 2
「3	1	/入、3	C
E ]X k叫(干3) + σ2E3)
substituting A = 2ξ - 6φ gives
-3	-∣	3
E X Bdk2 (f+后)+ σ2 [2ξ - 6φ]2(t + 3)
This leads us to the final theorem. To obtain Theorem 10.8, we proceed in an analogous way as form
Theorem 10.3, but this time applying Theorem 10.10 that we have just proved.
10.5 B ehavior Guided Policy Gradient and Wasserstein trust region
The chief goal of this section is to prove Theorem 5.1. We restate the section’s definitions here for
the reader’s convenience: To ease the discussion we make the following assumptions:
1.	Finite horizon T .
2.	Undiscounted MDP.
3.	States are time indexed. In other words, states visited at time t can’t be visited at any other time.
4.	S and A are finite sets.
The third assumption is solely to avoid having to define a time indexed Value function. It can be
completely avoided. We chose not to do this in the spirit of notational simplicity. These assumptions
can be relaxed, most notably we can show similar results for the discounted and infinite horizon case.
We chose to present the finite horizon proof because of the nature of our experimental results.
Let Φ = id be the identity embedding so that E = Γ. In this case PπΦ denotes the distribution of
trajectories corresponding to policy π. We define the value function V π : S → R as
V π(st = S)= ET 〜Pnd
The Q-function Qπ : S × A → R as:
T
ER(S'+1 ,a`,s`)lst = s
.'=t	.
Q (st, at = a) = ET〜Pnd
T
ER(S'+1, a`, s`)
.'=t
Similarly, the advantage function is defined as:
Aπ (S, a) = Qπ (S, a) - Vπ(S)
We denote by V(π) = ET〜Pnd [PT=° R(st+ι,at,st)]
the visitation frequency as:
the expected reward of policy π and define
T
Pn(S) = ET〜Pnd〉: I(St =S)
t=0
The first observation in this section is the following lemma:
35
Under review as a conference paper at ICLR 2020
Lemma 10.13. two distinct policies π and π can be related via the following equation :
V (π) = V (π)+ɪs
s∈S
Π(a∣s)Aπ(s, a)
))
Proof. Notice that Aπ(s, a) = Es，~p(s，|a,s)[R(s0, a, S) + Vπ(s0) - Vπ(s)]. Therefore:
T
Eτ~pπd EAn (St ,at)
t=0
T
ET ~P∏d X R(St+1, at, St) + Vπ(St+1) - Vπ (St)
t=0
T
ET ~P∏d X R(St+1, at, St) - Es0 [Vπ (S0)]
t=0
-V (π) + V (∏)
The result follows.
□
See Sutton et al. (1998) for an alternative proof. We also consider the following linear approximation
to V around policy π (see: Kakade & Langford (2002)):
L(∏)
V(π) +
s∈S
Π(a∣s)Aπ(s, a)
))
Where the only difference is that ρ∏ was substituted by ρ∏. Consider the following embedding
Φs : Γ → R|S| defined by (Φ(τ))s = PtT=0 1(St = S), and related cost function defined as:
C(v, w) = kv - wk1.
Lemma 10.14. The Wasserstein distance WDo(Pφs, Pφs) is related to visit frequencies since:
WDo(Pφs,Pφs) ≥ X ∣ρ∏(S)-Pn(s)l
s∈S
Proof. Let Π be the optimal coupling between PφS and Pφs. Then:
WDo(PΦ s,Pφs )= Eu,v~π [ku -vkι]
=EEu,v~Π ||us - vs|]
s∈S
Where us and vs denote the S ∈ S indexed entry of the u and v vectors respectively. Notice that for
all S ∈ S the following is true:
Eu~Pφs
X-----
[us] - Ev~pΦs [vs]
π
≤ Eu,v~Π [|us - vs |]
ρπ(s)	ρπ0 (s)
The result follows.
□
These observations enable us to prove an analogue of Theorem 1 from Schulman et al. (2015),
namely:
Theorem 10.15. If WDo(Pφ s, Pφs) ≤ δ and C = maxs,a ∣An (s, a)| ,then V (π) ≥ L(θ) — δe.
As in Schulman et al. (2015), Theorem 5.1 implies a policy improvement guarantee for BGPG from
Section 5.3.
36
Under review as a conference paper at ICLR 2020
Proof. Notice that:
V(π) - L(π) = X ((Pn(S)-Pn(S)) (X n(a|S)An(Ja)))
s∈S	a∈A
Therefore by Holder inequality:
IV(∏) - L(∏)l≤
∣Pn(s) - Pn (s)|	SUp
s∈S
-------------------'、-----
'---------V-------
≤WDo (PΦS ,pφs )≤δ
^X Π(a∣s)An(s, a)
a∈A
The result follows.
□
We can leverage the results of Theorem 10.15 to show wasserstein trust regions methods with
embedding Φs give a monotonically improving sequence of policies. The proof can be concluded by
following the logic of Section 3 in Schulman et al. (2015).
37