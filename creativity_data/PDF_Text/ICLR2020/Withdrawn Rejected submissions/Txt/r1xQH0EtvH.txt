Under review as a conference paper at ICLR 2020
The Blessing of Dimensionality: An Empirical
Study of Generalization
Anonymous authors
Paper under double-blind review
Ab stract
The power of neural networks lies in their ability to generalize to unseen data, yet
the underlying reasons for this phenomenon remain elusive. Numerous rigorous
attempts have been made to explain generalization, but available bounds are still
quite loose, and analysis does not always lead to true understanding. The goal of
this work is to make generalization more intuitive. Using visualization methods,
we discuss the mystery of generalization, the geometry of loss landscapes, and how
the curse (or, rather, the blessing) of dimensionality causes optimizers to settle into
minima that generalize well.
1	Introduction
Neural networks are a powerful tool for solving classification problems. The power of these models
is due in part to their expressiveness; they have many parameters that can be efficiently optimized
to fit nearly any finite training set. However, the real power of neural network models comes from
their ability to generalize; they often make accurate predictions on test data that were not seen during
training, provided the test data is sampled from the same distribution as the training data.
The generalization ability of neural networks is seemingly at odds with their expressiveness. Neural
network training algorithms work by minimizing a loss function that measures model performance
using only training data. Because of their flexibility, it is possible to find parameter configurations
Nearby minima
0.37	0.53
Accuracy
(test)
Figure 1: A minefield of bad minima: we train a neural net classifier and plot the iterates of SGD after each
tenth epoch (red dots). We also plot locations of nearby “bad” minima with poor generalization (blue dots). We
visualize these using t-SNE embedding. All blue dots achieve near perfect train accuracy, but with test accuracy
below 53% (random chance is 50%). The final iterate of SGD (yellow star) also achieves perfect train accuracy,
but with 98.5% test accuracy. Miraculously, SGD always finds its way through a landscape full of bad minima,
and lands at a minimizer with excellent generalization.
for neural networks that perfectly fit the training data and minimize the loss function while making
mostly incorrect predictions on test data. Miraculously, commonly used optimizers reliably avoid
such “bad” minima of the loss function, and succeed at finding “good” minima that generalize well.
Our goal here is to develop an intuitive understanding of neural network generalization using
visualizations and experiments rather than analysis. We begin with some experiments to understand
why generalization is puzzling, and how over-parameterization impacts model behavior. Then,
we explore how the “flatness” of minima correlates with generalization, and in particular try to
understand why this correlation exists. We explore how the high dimensionality of parameter spaces
biases optimizers towards landing in flat minima that generalize well. Finally, we present some
counterfactual experiments to validate the intuition we develop. Code to reproduce experiments is
available at https://github.com/genviz2019/genviz.
1
Under review as a conference paper at ICLR 2020
2	Why is generalization so puzzling
Neural networks define a highly expressive model class. In fact, given enough parameters, a neural
network can approximate virtually any function (Cybenko, 1989). But just because neural nets have
the power to represent any function does not mean they have the power to learn any function from a
finite amount of training data.
Neural network classifiers are trained by minimizing a loss function that measures model performance
using only training data. A standard classification loss has the form
L(O) = ∣D1-∣	X - logpθ(χ,y),
t (x,y)∈Dt
(1)
where pθ (x, y) is the probability that data sample x lies in class y according to a neural net with
parameters θ, and Dt is the training dataset of size |Dt|. This loss is near zero when a model with
parameters θ accurately classifies the training data. Over-parameterized neural networks (i.e., those
with more parameters than training data) can represent arbitrary, even random, labeling functions on
large datasets (Zhang et al., 2016). As a result, an optimizer can reliably fit an over-parameterized
network to training data and achieve near zero loss (Laurent and Brecht, 2018; Kawaguchi, 2016).
However, this comes with no guarantee of generalization to unseen test data.
We illustrate the difference between model fit-
ting and generalization with an experiment.
The CIFAR-10 training dataset contains 50,000
small images. We train two over-parameterized
models on this dataset. The first is a neural
network (ResNet-18) with 269,722 parameters
(nearly 6× the number of training images). The
second is a linear model with a feature set that
includes pixel intensities as well as pair-wise
products of pixels intensities.1 This linear model
has 298, 369 parameters, which is comparable
to the neural network, and both are trained using
SGD. On the left of Figure 2, we see that over-
parameterization causes both models to achieve
perfect accuracy on training data. But the linear
model achieves only 49% test accuracy, while
ResNet-18 achieves 92%. The excellent per-
formance of the neural network model raises
the question of whether bad minima exist at all.
Maybe deep networks generalize because bad
minima are rare and lie far away from the region
Figure 2: (left) CIFAR10 trained with ResNet-18 and a
linear model having comparable number of parameters.
Both can fit the training data well, but neural nets are
able to generalize to unseen data, while linear models
cannot. (right) CIFAR10 trained with various optimiz-
ers using VGG13, generalizing well irrespective of the
optimizer used.
of parameter space where initialization takes place?
We can confirm the existence of bad minima by incorporating a loss term that explicitly promotes
poor generalization, by discouraging performance on unseen data drawn from the same distribution.
We do this by minimizing
L(θ)
(1- β)
|Dt|
β
E - logpθ(x,y) + ∣d-∣	E - log[1 - pθ(x,y)],
(x,y)∈Dt	d (x,y)∈Dd
(2)
where Dt is the training set, and Dd is a set of unseen examples sampled from the same distribution.
Dd could be obtained via a GAN (Goodfellow et al., 2014) or additional data collection (note that it
is not the test set). Here, β parametrizes the amount of “anti-generalization” we wish to achieve. The
first term in (2) is the standard cross entropy loss (1) on the training set Dt, and is minimized when
the training data are classified correctly.
The second term is the reverse cross entropy loss on Dd, and is minimized when Dd is classified
incorrectly. With a sufficiently over-parameterized network, gradient descent on (2) drives both terms
1For computing the pair-wise pixel intensity products, images are first downsampled by a factor of 2.
2
Under review as a conference paper at ICLR 2020
to zero, and we find a parameter vector that minimizes the original training set loss (1) while failing
to generalize. In other words, the minima found by (2) are stationary points with comparable true
objective function values (Eq. (1)), indicating that it’s quite possible to land in one of these “bad”
minima in a normal training routine (1) if initialized within the loss basin. Sec. 5 will show that the
likelihood of this occurring is negligible.
When we use the anti-generalization loss to search for bad minima near the optimization trajectory, we
see that bad minima are everywhere. We visualize the distribution of bad minima in Figure 1. We run
a standard SGD optimizer on the swissroll and trace out the path it takes from a random initialization
to a minimizer. We plot the iterate after every tenth epoch as a red dot with opacity proportional to its
epoch number. Starting from these iterates, we run the anti-generalization optimizer to find nearby
bad minima. We project the iterates and bad minima into a 2D plane for visualization using a t-SNE
embedding2. Our anti-generalization optimizer easily finds minima with poor generalization within
close proximity to every SGD iterate. Yet SGD avoids these bad minima, carving out a path towards
a parameter configuration that generalizes well.
Figure 1 illustrates that neural network optimizers are inherently biased towards good minima, a
behavior commonly known as “implicit regularization.” To see how the choice of optimizer affects
generalization, we trained a simple neural network (VGG13) on 11 different gradient methods and
2 non-gradient methods in Figure 2 (right). This includes LBFGS (a second-order method), and
ProxProp (which chooses search directions by solving least-squares problems rather than using the
gradient). Interestingly, all of these methods generalize far better than the linear model. While there
are undeniably differences between the performance of different optimizers, the presence of implicit
regularization for virtually any optimizer strongly indicates that implicit regularization may be caused
in part by the geometry of the loss function, rather than the choice of optimizer alone.
Later on, we visually explore the relationship between the loss function’s geometry and generalization,
and how the high dimensionality of parameter space is one source of implicit regularization for
optimizers.
3	Related work: theoretical results on generalization
Classical PAC learning theory balances model complexity (the expressiveness of a model class)
against data volume. When a model class is too expressive relative to the volume of training data, it
has the ability to ace the training data while flunking the test data, and learning fails.
Classical theory fails to explain generalization in over-parameterized neural nets, as the complexity
of networks is often large (exponential in depth (Sun et al., 2016; Neyshabur et al., 2015; Xie et al.,
2015) or linear in the number of parameters (Shalev-Shwartz and Ben-David, 2014; Bartlett et al.,
1998; Harvey et al., 2017)). Therefore classical bounds become too loose or even vacuous in the
over-parameterized setting that we are interested in studying.
To explain this mismatch between empirical observation and classical theory, a number of recent
works propose new metrics that characterize the capacity of neural networks. Most of these appeal to
the PAC framework to characterize the generalization ability of a model class Θ (e.g., neural nets of a
shared architecture) through a high probability upper bound: with probability at least 1 - δ,
R(θ) - RS(θ) < B + q2m ln 1,	∀θ ∈ Θ	(3)
where R(θ) is generalization risk (true error) of a net with parameters θ ∈ Θ, RS(θ) denotes empirical
risk (training error) with training sample S. We explain B under different metrics below.
Model space complexity. This line of work takes B to be proportional to the complexity of the model
class being trained, and efforts have been put into finding tight characterizations of this complexity.
Neyshabur et al. (2018); Bartlett et al. (2017) built on prior works (Bartlett and Mendelson, 2003;
Neyshabur et al., 2015) to produce bounds where model class complexity depends on the spectral
norm of the weight matrices without having an exponential dependence on the depth of the network.
Such bounds can improve the model class complexity provided that weight matrices adhere to some
structural constraints (e.g. sparsity or eigenvalue concentration).
2t-SNE analysis, following the guidelines in Wattenberg et al. (2016).
3
Under review as a conference paper at ICLR 2020
Stability and robustness. This line of work considers B to be proportional to the stability of the
model (Hardt et al., 2016; Kuzborskij and Lampert, 2018; Gonen and Shalev-Shwartz, 2017), which
is a measure of how much changing a data point in S changes the output of the model (Sokolic
et al., 2017). However it is nontrivial to characterize the stability of a neural network. Robustness,
while producing insightful and effective generalization bounds, still suffers from the curse of the
dimensionality on the priori-known fixed input manifold.
PAC-Bayes and margin theory. PAC-Bayes bounds (McAllester, 1998; 1999; Neyshabur et al.,
2017; Bartlett and Mendelson, 2003; Neyshabur et al., 2015; Golowich et al., 2018), provide gen-
eralization guarantees for randomized predictors drawn from a learned distribution that depends
on the training data, as opposed to a learned single predictor. These bounds often yield sample
complexity bounds worse than naive parameter counting, however Dziugaite and Roy (2017) show
that PAC-Bayes theory does provide meaningful generalization bounds for “flat” minima.
Model compression. Most recent theoretical work can be understood through the lens of “model
compression” (Arora et al., 2018). Clearly, it is impossible to generalize when the model class is too
big; in this case, many different parameter choices explain the data perfectly while having wildly
different predictions on test data. The idea of model compression is that neural network model classes
are effectively much smaller than they seem to be because optimizers are only willing to settle into a
very selective set of minima. When we restrict ourselves to only the narrow set of models that are
acceptable to an optimizer, we end up with a smaller model class on which learning is possible.
While our focus is on gaining insights through visualizations, the intuitive arguments below can
certainly be linked back to theory. The class of models representable by a network architecture
has extremely high complexity, but experiments suggest that most of these models are effectively
removed from consideration by the optimizer, which has an extremely strong bias towards “flat”
minima, resulting in a reduced effective model complexity.
4	Flat vs sharp minima: a wide margin criteria for complex
MANIFOLDS
Over-parameterization is not specific to neural networks. A traditional approach to coping with
over-parameterization for linear models is to use regularization (aka “priors”) to bias the optimizer
towards good minima. For linear classification, a common regularizer is the wide margin penalty
(which appears in the form of an `2 regularizer on the parameters of a support vector machine). When
used with linear classifiers, wide margin priors choose the linear classifier that maximizes Euclidean
distance to the class boundaries while still classifying data correctly.
Neural networks replace the classical wide margin regularization with an implicit regulation that
promotes the closely related notion of “flatness.” In this section, we explain the relationship between
flat minima and wide margin classifiers, and provide intuition for why flatness is a good prior.
Many have observed links between flatness and generalization. Hochreiter and Schmidhuber (1997)
first proposed that flat minima tend to generalize well. This idea was reinvigorated by Keskar et al.
(2017), who showed that large batch sizes yield sharper minima, and that sharp minima generalize
poorly. This correlation was subsequently observed for a range of optimizers by Izmailov et al.
(2018), Wang et al. (2018), and Li et al. (2018). Rigorous analysis showing that flat minimizers
generalize well was presented by Chaudhari et al. (2017) as well as Dziugaite and Roy (2017).
Flatness is a measure of how sensitive network performance is to perturbations in parameters.
Consider a parameter vector that minimizes the loss (i.e., it correctly classifies most if not all training
data). If small perturbations to this parameter vector cause a lot of data misclassification, the
minimizer is sharp; a small movement away from the optimal parameters causes a large increase in
the loss function. In contrast, flat minima have training accuracy that remains nearly constant under
small parameter perturbations.
The stability of flat minima to parameter perturbations can be seen as a wide margin condition. When
we add random perturbations to network parameters, it causes the class boundaries to wiggle around
in space. If the minimizer is flat, then training data lies a safe distance from the class boundary, and
perturbing the class boundaries does not change the classification of nearby data points. In contrast,
4
Under review as a conference paper at ICLR 2020
sharp minima have class boundaries that pass close to training data, putting those nearby points at
risk of misclassification when the boundaries are perturbed.
We visualize the impact of sharpness on neural networks in Figure 3. We train a 6-layer fully connected
neural network on the swiss roll dataset using regular SGD, and also using the anti-generalization
loss to find a minimizer that does not generalize. The “good” minimizer has a wide margin - the
class boundary lies far away from the training data. The “bad” minimizer has almost zero margin,
and each data point lies near the edge of class boundaries, on small class label “islands” surrounded
by a different class label, or at the tips of “peninsulas” that reach from one class into the other. The
class labels of most training points are unstable under perturbations to network parameters, and so
we expect this minimizer to be sharp. An animation of the decision boundary under perturbation is
provided at https://www.youtube.com/watch?v=4VUJyQknf4s&t=.
We can visualize the sharpness of the minima in Figure 3, but we need to take some care with our
metrics of sharpness. It is known that trivial definitions of sharpness can be manipulated simply by
rescaling network parameters (Dinh et al., 2017). When parameters are small (say, 0.1), a perturbation
of size 1 might cause a major performance degradation. Conversely, when parameters are large
(say, 100), a perturbation of size 1 might have little impact on performance. However, rescalings of
network parameters are irrelevant; commonly used batch normalization layers remove the effect of
parameter scaling. For this reason, it is important to define measures of sharpness that are invariant
to trivial rescalings of network parameters. One such measure is local entropy (Chaudhari et al.,
2017), which is invariant to rescalings, but is difficult to compute. For our purposes, we use the
filter-normalization scheme proposed in Li et al. (2018), which simply rescales network filters to
have unit norm before plotting. The resulting sharpness/flatness measures have been observed to
correlate well with generalization.
The bottom of Figure 3 visualizes loss function geometry around the two minima for the swiss roll.
These surface plots show the loss evaluated on a random 2D plane3 sliced out of parameter space
using the method described in Li et al. (2018). We see that the instability of class labels under
parameter perturbations does indeed lead to dramatically sharper minima for the bad minimizer, while
the wide margin of the good minimizer produces a wide basin.
To validate our observations on a more complex problem, we produce similar sharpness plots for the
Street View House Number (SVHN) classification problem in Figure 4 using ResNet-18. The SVHN
dataset (Netzer et al., 2011) is ideal for this experiment because, in addition to train and test data, the
creators collected a large (531k) set of extra data from the same distribution that can be used for Dd
in Eq. (2). We minimize the SVHN loss function using standard training with and without penalizing
for generalization (Eq. (2)). The good, well-generalizing minimizer is flat and achieves 97.1% test
accuracy, while the bad minimizer is much sharper and achieves 28.2% test accuracy. Both achieve
100% train accuracy and use identical hyperparameters (other than the β factor), network architecture,
and weight initialization.
5	Implicit regularization and the blessing of dimensionality
We have seen that neural network loss functions are densely populated with both good and bad
minima, and that good minima tend to have “flat” loss function geometry. But what causes optimizers
to find these good/flat minima and avoid the bad ones?
One possible explanation to the bias of stochastic optimizers towards good minima is the volume
disparity between the basins around good and bad minima. Flat minima that generalize well lie in
wide basins that occupy a large volume of parameter space, while sharp minima lie in narrow basins
that occupy a comparatively small volume of parameter space. As a result, an optimizer using random
initialization is more likely to land in the attraction basin for a good minimizer than a bad one.
The volume disparity between good and bad minima is magnified by the curse (or, rather, the
blessing?) of dimensionality. The differences in “width" between good and bad basins does not
appear too dramatic in the visualizations in Figures 3 and 4, or in sharpness visualizations for
other datasets (Li et al., 2018). However, the probability of colliding with a region during a random
32D loss landscapes are a fairly reliable way to depict minimizer width. Sec. A4 in Li et al. (2018) and Fig. 7
show the relatively small variance in width w.r.t. random directions.
5
Under review as a conference paper at ICLR 2020
(a) 100% train, 100% test
(b) 100% train, 7% test
(c) Minimizer of network in (a) above
(d) Minimizer of network in (b) above
Figure 3: Top: Decision boundaries of two networks with different parameters. Network (a) generalizes
well. Network (b) generalizes poorly (perfect train accuracy, bad test accuracy). The flatness and large volume
of (a) make it likely to be found by SGD, while the sharpness and tiny volume of (b) make this minimizer
unlikely. Red and blue dots correspond to the training data. See https://www.youtube.com/watch?
v=4VUJyQknf4s&t= for an animation of these boundaries when perturbed. Bottom: A slice through the loss
landscapes around these minima reveals sharpness/flatness.
(a) Good minimizer: 100% train, 97% test	(b) Bad minimizer: 100% train, 28% test
Figure 4: A slice through the loss landscape of two minima for the SVHN loss function using ResNet-18.
6
Under review as a conference paper at ICLR 2020
1.0-
SVHN (298k params)
-°
0
2 r-Je
000
3pe」Seq
-5
〜〜 train	'
~ 〜、、gen. gap
test、、
random
00
1
ɪ
Id-
-2
10
poison factor
(ωlun-OA)0τ6o-
0.0	0.2	0.4	0.6	0.8
generalization gap
Swiss roll (3360 params)
1.0-
-0T-2-3
Oooo
ɪ 1 ɪ
SmPe」u-sq
5
0
AUaJnuB
00
ɪ
-
,O
I ɪ
-2
U O
I ɪ
7
-I W
poison factor
(φlun°A)0τ8
cutoff
-→- 0.2
-→- 0.1
-→- 0.05
-→- 0.02
-→- 0.01
-→- 0.005
0.00 0.25 0.50 0.75
generalization gap
Figure 5: Relationship between generalization, sharpness, and volume. Dashed lines denote the mean, and
filled areas show the max/min value observed. Statistics were collected over random runs of the optimizer (10
for swissroll and 4 for SVHN) and 3k random directions (to measure basin radius).
0.00	0.02	0.24	0.38	0.46	0.93
Figure 6: Swissroll decision boundary for various levels of generalization gap (indicated above plots).
initialization does not scale with its width, but rather its volume. Network parameters live in very high-
dimensional spaces where small differences in sharpness between minima translate to exponentially
large disparities in the volume of their surrounding basins. It should be noted that the vanishing
probability of finding sets of small width in high dimensions is well studied by probabilists, and is
formalized by a variety of escape theorems (Gordon, 1988; Vershynin, 2018).
To explore the effect of dimensionality on neural
loss landscapes, we quantify the local volume
within the low-lying basins surrounding differ-
ent minima. The volume (or “horizon") of a
basin is not well-defined, especially for SGD
with discrete time-steps. For this experiment,
we define the “basin” to be the set of points
in a neighborhood of the minimizer that have
loss value below a cutoff of 0.1 (Fig. 7). We
chose this definition because the volume of this
set can be efficiently computed. We calculate
the volume of these basins using a Monte-Carlo
Figure 7: SVHN loss along random directions, and the
“basin” lying beneath the cutoff loss value.
integration method. Let r(φ) denote the radius of the basin (distance from minimizer to basin
boundary) in the direction of the unit vector φ. Then the n-dimensional volume of the basin is
7
Under review as a conference paper at ICLR 2020
V = ωnEφ [rn (φ)], where ωn =「着/2)is the volume of the unit n-ball, and Γ is Euler's gamma
function. We estimate this expectation by calculating r(φ) for 3k random directions, as illustrated in
Figure 7.
In Figure 5, we visualize the combined relationship between generalization and volume for swissroll
and SVHN. By varying β, we control the generalizability of each minimizer. As generalization
accuracy decreases, we see the radii of the basins decrease as well, indicating that minima become
sharper. Figure 5 also contains scatter plots showing a severe correlation between generalization and
(log) volume for various choices of the basin cutoff value. For SVHN, the basins surrounding good
minima have a volume at least 10,000 orders of magnitude larger than that of bad minima, rendering
it nearly impossible to accidentally stumble upon bad minima.
Finally, we visualize the decision boundaries for several levels of generalization in Figure 6. All
networks achieve above 99.5% training accuracy. As the generalization gap increases, the area that
belongs to the red class begins encroaching into the area that belongs to the blue class, and vice versa.
The margin between the decision boundary and training points also decreases until the training points,
though correctly classified, sit on “islands” or “peninsulas” as discussed above.
A counterfactual experiment: what can’t neural nets solve
Neural nets solve complex classification prob-
lems by finding “flat” minima with class bound-
aries that assign labels that are stable to param-
eter perturbations. Using this intuition, can we
formulate a problem that neural nets can’t solve?
Consider the problem of separating the blue and
red dots in Figure 8. When the distance between
the inner rings is large, a neural network con-
sistently finds a well-behaved circular boundary
as in Fig. 8aa. The wide margin of this classi-
fier makes the minimizer “flat,” and the resulting
high volume makes it likely to be found by SGD.
We can remove the well-behaved minima from
(a)	(b)
Figure 8: A neural network fails to solve a classification
problem when the ideal solution is “sharp.”
this problem by pinching the margin between the inner red and blue rings. In this case, a network
trained with random initialization is shown in Fig. 8b. Now, SGD finds networks that cherry-pick red
points, and arc away from the more numerous blue points to maintain a large margin. In contrast, a
simple circular decision boundary as in Fig. 8a would pass extremely close to all points on the inner
rings, making such a small margin solution less stable under perturbations and unlikely to be found
by SGD.
6	Conclusion
We explored the connection between generalization and loss function geometry using visualizations
and experiments on classification margin and loss basin volumes, the latter of which does not appear
in the literature.
While experiments can provide useful insights, they sometimes raise more questions than they answer.
We explored why the “large margin” properties of flat minima promote generalization. But what is
the precise metric for “margin” that neural networks respect? Experiments suggest that the small
volume of bad minima prevents optimizers from landing in them. But what is a correct definition of
“volume” in a space that is invariant to parameter re-scaling and other transforms, and how do we
correctly identify the attraction basins for good minima? Finally and most importantly: how do we
connect these observations back to a rigorous PAC learning framework?
The goal of this study is to foster appreciation for the complex behaviors of neural networks, and to
provide some intuitions for why neural networks generalize. We hope that the experiments contained
here will provide inspiration for theoretical progress that leads us to rigorous and definitive answers
to the deep questions raised by generalization.
8
Under review as a conference paper at ICLR 2020
References
G. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control,
Signals and Systems, 2(4):303-314, Dec 1989.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep
learning requires rethinking generalization. International Conference on Learning Representations,
2016.
Thomas Laurent and James Brecht. Deep linear networks with arbitrary loss: All local minima are
global. In International Conference on Machine Learning, 2018.
Kenji Kawaguchi. Deep learning without poor local minima. In Advances in neural information
processing systems, pages 586-594, 2016.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling,
C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information
Processing Systems 27, pages 2672-2680. Curran Associates, Inc., 2014. URL http://papers.
nips.cc/paper/5423-generative-adversarial-nets.pdf.
Martin Wattenberg, Fernanda Viegas, and Ian Johnson. HoW to use t-sne effectively. Distill, 2016.
Ari S. Morcos, David G.T. Barrett, Neil C. Rabinowitz, and Matthew Botvinick. On the importance
of single directions for generalization. In International Conference on Learning Representations,
2018.
Shizhao Sun, Wei Chen, LiWei Wang, Xiaoguang Liu, and Tie-Yan Liu. On the depth of deep neural
netWorks: A theoretical vieW. In AAAI, 2016.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural
netWorks. In Proceedings of The 28th Conference on Learning Theory, 2015.
Pengtao Xie, Yuntian Deng, and Eric Xing. On the generalization error bounds of neural netWorks
under diversity-inducing mutual angular regularization. arXiv preprint arXiv:1511.07110, 2015.
Shai Shalev-ShWartz and Shai Ben-David. Understanding Machine Learning: From Theory to
Algorithms. Cambridge University Press, NeW York, NY, USA, 2014.
Peter L. Bartlett, Vitaly Maiorov, and Ron Meir. Almost linear vc dimension bounds for pieceWise
polynomial netWorks. In Advances in Neural Information Processing Systems, 1998.
Nick Harvey, Christopher LiaW, and Abbas Mehrabian. Nearly-tight VC-dimension bounds for
pieceWise linear neural netWorks. In Proceedings of the 2017 Conference on Learning Theory,
2017.
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A PAC-bayesian approach to
spectrally-normalized margin bounds for neural netWorks. In International Conference on Learning
Representations, 2018.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural netWorks. In Advances in Neural Information Processing Systems. 2017.
Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. J. Mach. Learn. Res., 3:463-482, March 2003.
Moritz Hardt, Benjamin Recht, and Yoram Singer. Train faster, generalize better: Stability of
stochastic gradient descent. In International Conference on International Conference on Machine
Learning, 2016.
Ilja Kuzborskij and Christoph H. Lampert. Data-dependent stability of stochastic gradient descent.
In International Conference on International Conference on Machine Learning, 2018.
Alon Gonen and Shai Shalev-ShWartz. Fast rates for empirical risk minimization of strict saddle
problems. In COLT, 2017.
9
Under review as a conference paper at ICLR 2020
Jure Sokolic, Raja Giryes, Guillermo Sapiro, and Miguel Rodrigues. Generalization error of invariant
classifiers. In Artificial Intelligence and Statistics, pages 1094-1103, 2017.
David A. McAllester. Some pac-bayesian theorems. In Proceedings of the Eleventh Annual Confer-
ence on Computational Learning Theory, COLT’ 98, pages 230-234, 1998.
David A. McAllester. Pac-bayesian model averaging. In Proceedings of the Twelfth Annual Con-
ference on Computational Learning Theory, COLT ’99, pages 164-170, New York, NY, USA,
1999.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring generaliza-
tion in deep learning. In Advances in Neural Information Processing Systems, pages 5947-5956,
2017.
Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of
neural networks. In Proceedings of the 31st Conference On Learning Theory, 2018.
Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for
deep (stochastic) neural networks with many more parameters than training data. Conference on
Uncertainty in Artificial Intelligence (UAI), 2017.
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for
deep nets via a compression approach. In International Conference on Machine Learning, pages
254-263, 2018.
SePP Hochreiter and Jurgen Schmidhuber. Flat minima. Neural Computation, 9:1T2, 1997.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter
Tang. On large-batch training for deeP learning: Generalization gaP and sharP minima. Interna-
tional Conference on Learning Representations, 2017.
Pavel Izmailov, Dmitrii PodoPrikhin, Timur GariPov, Dmitry Vetrov, and Andrew Gordon Wilson. Av-
eraging weights leads to wider oPtima and better generalization. arXiv preprint arXiv:1803.05407,
2018.
Huan Wang, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. Identifying generalization
ProPerties in neural networks. arXiv preprint arXiv:1809.07402, 2018.
Hao Li, Zheng Xu, Gavin Taylor, ChristoPh Studer, and Tom Goldstein. Visualizing the loss landscaPe
of neural nets. In Advances in Neural Information Processing Systems, Pages 6389-6399. 2018.
P Chaudhari, Anna Choromanska, S Soatto, Yann LeCun, C Baldassi, C Borgs, J Chayes, Levent
Sagun, and R Zecchina. EntroPy-sgd: Biasing gradient descent into wide valleys. In International
Conference on Learning Representations (ICLR), 2017.
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. SharP minima can generalize for
deeP nets. In International Conference on Machine Learning, 2017.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng. Reading
digits in natural images with unsuPervised feature learning. NIPS WorkshoP on DeeP Learning
and UnsuPervised Feature Learning 2011, 2011.
Yehoram Gordon. On Milman’s inequality and random subsPaces which escaPe through a mesh in R
n. In Geometric Aspects of Functional Analysis, Pages 84-106. SPringer, 1988.
Roman Vershynin. High-dimensional probability: An introduction with applications in data science,
volume 47. Cambridge University Press, 2018.
10