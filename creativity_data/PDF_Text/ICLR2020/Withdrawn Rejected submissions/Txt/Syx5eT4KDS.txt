Under review as a conference paper at ICLR 2020
Discrete Infomax Codes for Meta-Learning
Anonymous authors
Paper under double-blind review
Ab stract
This paper analyzes how generalization works in meta-learning. Our core contri-
bution is an information-theoretic generalization bound for meta-learning, which
identifies the expressivity of the task-specific learner as the key factor that makes
generalization to new datasets difficult. Taking inspiration from our bound, we
present Discrete InfoMax Codes (DIMCO), a novel meta-learning model that trains
a stochastic encoder to output discrete codes. Experiments show that DIMCO
requires less memory and less time for similar performance to previous metric
learning methods and that our method generalizes particularly well in a challenging
small-data setting.
1	Introduction
Generalizing to unseen data is a problem of vital importance in machine learning. Many deep meta-
learning methods optimize for generalization by directly minimizing the loss of held-out validation
data. Recent works have used this framework to achieve impressive feats such as learning to classify
using one labeled image per class (Snell et al., 2017; Finn et al., 2017), learning unsupervised update
rules that generalize to different domains (Metz et al., 2018), and accelerating training procedures
that are millions of steps long (Flennerhag et al., 2018). However, the meta-learning setup introduces
a new overfitting problem: the model may overfit to the distribution of tasks seen during training. In
other words, the meta-learning framework decreases task-specific overfitting at the cost of introducing
task-wise overfitting.
The primary aim of this work is to elucidate this tradeoff between task-specific and task-wise
overfitting in meta-learning. Specifically, we use tools from information theory to bound the overall
generalization gap of a meta-learner. Analogously to how generalization bounds for standard learning
algorithms reveal the role of dataset size in generalizing to new datapoints, our bound reveals the
roles of both dataset size (m) and number of datasets (n) in generalizing to new tasks. The specific
form of our generalization gap suggests that meta-learning models can be implicitly regularized by
constructing them to express each datapoint with a small number of bits.
To this end, we propose Discrete InfoMax COdes (DIMCO), a deep neural network model that
outputs a discrete representation of each datapoint. Note that a continuous representation X ∈ Rn (e.g.
Snell et al. (2017)) uses 32n bits per datapoint. This is wildly inefficient in terms of bit-efficiency: our
experiments in Section 5 show that DIMCO’s discrete representation requires roughly 10× less bits
per datapoint to achieve similar performance compared to continuous methods. DIMCO generalizes
well to novel datasets because its learning objective encourages the model to compactly use all of its
degrees of freedom, thus enabling it to effectively compare datapoints while only requiring a small
number of bits per datapoint.
Our specific contributions are:
1.	Derive a generalization bound for meta-learning that makes the tradeoff between task-
specific and task-wise overfitting concrete.
2.	Propose DIMCO, a neural network model that is designed to have a low value of a specific
term in our generalization bound.
3.	Empirically demonstrate that DIMCO generalizes better than previous meta-learning meth-
ods when trained with small datasets, and that it is more memory- and time-efficient
compared to previous image retrieval methods.
1
Under review as a conference paper at ICLR 2020
This paper is organized as follows. We detail our problem setup and derive a generalization bound for
meta-learning in Section 2. Taking motivation from our bound, we propose our meta-learning model
(DIMCO) in Section 3. We put our analysis in the context of previous work in Section 4. Notably, we
suggest that certain previous meta-learning methods may have benefitted from implicit regularization.
We present experiments in Section 5 and conclude the paper with a discussion about limitations and
future directions of our approach in Section 6.
2	A Generalization B ound for Meta-Learning
Throughout this section, we denote data, model outputs, and labels as x, xe, and y, respectively. We
use capital symbols X, X , Y to denote the random variables corresponding to x, xe, y.
The (discrete) Mutual Information between two random variables X1 , X2 is defined as
I(X1;X2) =H(X1) -H(X1|X2) =H(X2) -H(X2|X1)	(1)
I(X1; X2) is a symmetric quantity which measures the amount of information shared between X1
and X2 . It has its lowest value 0 when X1 and X2 are independent and increases with the correlation
between X1 and X2. We refer the reader to (Cover & Thomas, 2012) for further exposition.
2.1	Problem Setup
We begin by describing our meta-learning problem setup. Define a task T to be a distribution over
Z = X × Y. Let tasks T1, . . . , Tn be sampled i.i.d. from a distribution of tasks τ. Associated with
each task T, we define a dataset DT = zT1 , . . . , zTm = (x1T, yT1 ), . . . , (xTm, yTm) which is a set of m
i.i.d. samples from the data distribution (ZT 〜T).
We consider models with parameters θ that map datapoints X to representations X(X, θ). Our
objective is the expecteation of the negative mutual information between representations an labels
across all tasks:
L(τ,θ) = -ET〜T 0(X(XT,θ); YT)i .	(2)
This objective is closely related to both previous loss functions and evaluation metrics in setups that
involve testing on unseen classes (e.g. few-shot classification, image retrieval). We show that previous
loss functions can be seen as approximations to (2) in Appendix A, and that mutual information is
strongly correlated with metrics such as few-shot accuracy and Recall@1 in Section 5.1.
An important difference between (2) and previous objectives for few-shot classification is that we do
not split each task into support/query (also called train/test) sets. This property bypasses the pesky
issue of using batch normalization (BN) during meta-training. As Nichol & Schulman (2018) points
out, BN can leak information from the support set to the query set. This issue is not a problem in our
setup since we do not assume a seperate "query set". Additionally, not using support/query splits
enables meta-learning with tasks consisting of one image per class, as we demonstrate in Section 5.4.
Note that the standard meta-learning setup cannot learn from such tasks since it requires at least two
images per class (one for support, one for query) to compute the loss function. Overall, our model
demonstrates that the commonly used construction of a held-out test set within each task is not strictly
necessary for meta-learning.
2.2	Generalization B ound
We bound the difference between expected loss and empirical loss:
Theorem 1. Let τ, n, m, X, X , Y, θ, L be defined as above. Let dΘ be the VC dimension of the
encoder X (∙). Let I (X (XT, θ); YT) be the empirical estimate ofthe mutual information using finite
dataset DT, and define empirical loss as
1n
L(TIn ⑼=--EI(X (Xt i ,θ); YT i).	⑶
n i=1
2
Under review as a conference paper at ICLR 2020
一 ~ 一
(($1$)
匚 0.6~∣~ 0.1~∣~ 0.1~|- 0.2口
Figure 1: A graphical overview of Discrete InfoMax COdes (DIMCO). A dataset D consists of
pairs of images X and labels Y . DIMCO is a stochastic encoder that maps each image X to a
distribution of discrete codes P(X|X). Each discrete code X 〜P(X|X) is a P-Way code of length d.
If p = 4, d = 2 (as in the diagram), each code consists of 2 symbols and each symbol is ∈ {1, 2, 3, 4}.
Inside the 4 × 4 grid that represents the Pd = 16 possible codes, the most likely roW and column are
colored. The most likely code in the diagram is (1, 2) With probability 30%. DIMCO is optimized by
maximizing the mutual information betWeen the discrete code and the label Within each batch.
&
The following inequality holds with high probability:
L(τ,θ) -L(T 1:n,θ) ≤ O
+ O (IX| log(m)
+	√m
+O
(4)
Proof. We use standard Chernoff bounds along With a finite sample bound for mutual information
from Shamir et al. (2010); see Appendix B.	□
The generalization gap has three terms, tWo of Which decrease as m increases, and the other decreases
as n increases. Typically for feW-shot learning, n is very large While m is small: the typical
miniImagenet 5-Way 1-shot setup has n > 1010 and m = 5. We therefore claim that the latter two
terms are the main difficulties for generalizing to new tasks. We see from Theorem 1 that these
terms can be reduced by using small |X |. Therefore, in the context of meta-learning, using short
representations (i.e. small |X|) can compensate for having a small train set (i.e. small m).
Meta-learning is typically formulated as performing tWo levels of learning: task-general learning and
task-specific learning. Theorem 1 implies that in the tasks considered in recent literature, task-general
learning should have almost no generalization gap, making task-specific learning the main source of
(meta-)overfitting. This can be problematic since in many Works, the task-specific learning algorithm
is usually just a byproduct of Whatever clever meta-learning loss Was proposed. Our theorem suggests
that We should pay more attention to directly regularizing this task-specific learner, and our DIMCO
model, described in the next section, can be seen as a minimal Working example in this direction.
3	Discrete Infomax Codes (DIMCO)
We noW present our model, Discrete InfoMax COdes (DIMCO). Motivated by Section 2, DIMCO
produces a short discrete code X and is trained by maximizing mutual information I(X; Y ). Figure
1 graphically shoWs the overall structure of DIMCO.
3.1	Factorized Discrete Codes
We propose a factorized discrete representation scheme Which enables us to represent discrete
distributions With exponentially feWer parameters compared to listing the probability of each event.
We represent each event as the product of d independent events, each of Which consists of P different
possibilities. We thus have Pd events in total, but only require Pd parameters to represent the
3
Under review as a conference paper at ICLR 2020
probability of each event. Binary codes can be viewed as a special case of this scheme where
p = 2. This factorization trick allows us to consider representations of size |Xe | = 64256 (Section 5).
This representation has the advantage of requiring only d log2 p bits per datapoint, whereas a D-
dimensional continuous vector embedding requires 32d bits (assuming 32-bit floats).
3.2	Model
Recall that we represent a given image using d independent discrete distributions, each of which has
P possibilities. First, a (convolutional) neural network enc(∙) takes image X as input and outputs a
vector of length dp, which we reshape into a matrix of size d × p:
enc(X)
l11 l12
l21	l22
..
..
..
ld1	ld2
(5)
Each row of this matrix represents the logits of a discrete distribution. We apply the softmax function
to each row to get probabilities.
softmax(li1 , . . . , lip ) = pi1 , . . . , pip	(6)
The ith codeword is sampled according to the categorical distribution following these probabilites:
Xi 〜Cat(Piι,...,Pip).	(7)
We simply concatenate each xei to obtain the representation: xe = (xe1, . . . , xed).
3.3	Training
Recall that X is a discrete random variable and X is its distribution. Instead of sampling x 〜 X, we
directly use X to compute the objective:
I(Xx;Y) = H(Xx)-H(Xx|Y).	(8)
mj mj
Cat / Ej=I pi1 Ej=I pi2
mm
(9)
The first term, H(X), can be calculated by taking the average of all probabilities and computing the
entropy:
H(Xx)=Xd H(Xxi)=Xd H
i=1	i=1
c
The second term is H(X|Y ) = k=1 P(Y = k)H(X|Y = k) where c is the number of classes.
The marginal probability of Y (P(Y = k)) is the frequency of k in {y1, . . . , ym}. H(Xx|Y = k) can
be obtained by computing (9) using only xj for which yj = k.
Though we have motivated the use of I(X; Y ) as a loss function throughout this paper, we provide yet
another perspective using the decomposition in (8). Minimizing H(X|Y ) encourages discriminatory
behavior. This term encourages the average embedding of each class to be as concentrated as possible.
Maximizing H(X) incentivizes the model to overall use all possible values of X.
We emphasize that such closed-form computation of I(X; Y ) is only possible because we are using
discrete codes.
3.4	Evaluation
We map all images to their probabilities Pij via (5) and (6) for all i = 1, . . . , d and j = 1, . . . , P. We
map each training image to its most likely code:
xe1
〜	z z	}|	{
x =	arg max P1j ,
j
arg max p2j, ∙∙∙ , arg max Pd
jj
(10)
j Pjp
m
4
Under review as a conference paper at ICLR 2020
Fix a train image and a test image, and let xe be the most likely code for the train image. The similarity
between train image and test image is measured by the probability of the test image producing xe. This
amounts to computing the product 1 of the test image’s probabilites using xei for each i = 1, . . . , d:
d
Y pxeii.	(11)
i=1
We use this as a similarity metric for both few-shot classification and image retrieval. We perform
few-shot classification by computing the most likely code for each class via (10) and classifying each
test image by choosing the class that has highest value of (11). We similarly perform image retrival
by mapping each support image to its most likely code (10) and for each query image retrieving the
support image that has highest (11).
4	Related Work
Regularizing Meta-Learners The ability to generalize to novel datasets is critical in meta-learning
benchmarks, and even more so in benchmarks such as Meta-Dataset (Triantafillou et al., 2019), where
a model is tested on datasets from an unseen domain. To the best of our knowledge, no works have
proposed an explicit regularizer for generalizing to new tasks.
Our analysis suggests that the success of some previous meta-learning methods can be attributed to
being implicitly regularized by reducing the expressive power reducing task-specific learning. The
following works have reported benefits from reducing the number of such task-specific parameters:
Lee & Choi (2018) learns a subset of the full network to alter during task-specific learning, Rusu
et al. (2018) explicitly represents each task with a low-dimensional latent space, and Zintgraf et al.
(2018) alters only a pre-specified subset of the full network during task-specific learning. It may
be surprising at first that these methods achieve higher accuracy than vanila MAML (Finn et al.,
2017), which is more expressive since it alters all parameters during task-specific learning. Kim
et al. (2018) also reports better meta-generalization through approximate variational inference with
respect to a learned prior, which can be seen as restricting the search space of the task-specific learner.
We showed through Theorem 1 that restricting inner-loop expressivity reduces the generalization
gap; this provides theoretical understanding to this consensus that meta-learning models with simple
task-specific learners generalize to new tasks more easily.
Information Bottleneck Theorem 1 is close in spirit to the information bottleneck principle (Tishby
et al., 2000; Tishby & Zaslavsky, 2015; Shwartz-Ziv & Tishby, 2017). This principle states that to
generalize, one should maximize I(X; Y ) while simultaneously minimizing I(X; X). Likewise, our
objective (2) is I(X; Y ) while our bound (22) suggests that the representation capacity |X| should be
low for generalization. Also related is the deterministic information bottleneck (Strouse & Schwab,
2017) which extends the information bottleneck by minimizing H(X) rather than I(X; X). These
three approaches to generalization are related via the chain of inequalities I(X; X) ≤ H(X) ≤
log |X |, which is tight when X is an efficient code.
Representation Learning Previous works have applied information-theoretic principles to analyze
the objective of VAEs (Alemi et al., 2017; Chen et al., 2018), derive an objective for GANs to learn
disentangled features (Chen et al., 2016), and to directly learn representations (Alemi et al., 2016;
Hjelm et al., 2018; Oord et al., 2018; Grover & Ermon, 2018; Choi et al., 2019). Our work can also
be viewed as an information-theoretic representation learning method, but we assume a supervised
meta-learning setup and our main focus is the meta-generalization problem.
Discrete Representations Discrete representations have been thoroughly studied in the context of
information theory (Shannon, 1948). Recent deep learning methods directly learn discrete representa-
tions, by learning variational autoencoders with discrete latent variables (Rolfe, 2016; van den Oord
et al., 2017; Razavi et al., 2019) or maximizing the mutual information between representation and
1 In practice, we add log probabilities instead of multiplying probabilities for numerical stability.
5
Under review as a conference paper at ICLR 2020
data (Hu et al., 2017). DIMCO is related to but differs from these works as it assumes a supervised
meta-learning setting and performs infomax using labels instead of data.
Similarly to our setup, Jeong & Song (2018) uses labels to learn a binary hash code. Their focus is on
the speedup gained by using sparse codes, whereas DIMCO learns a dense discrete code to generalize
better. Additionally, their method solves a minimum cost flow problem within each batch to find the
locally optimal code, whereas DIMCO is able to directly compute its loss function.
Factorized Representations The idea of using factorized representations has appeared the contexts
of quantizing a continuous input (Jegou et al., 2011), memory-efficient clustering (Norouzi & Fleet,
2013), and constructing an expressive attention mechanism using few parameters (Vaswani et al.,
2017). Likewise, DIMCO factorizes its discrete representatations to increase its representation power.
Metric Learning The structure and loss function of DIMCO is closely related to embedding-based
meta-learning Vinyals et al. (2016); Snell et al. (2017); Oreshkin et al. (2018) and image retrieval
Hoffer & Ailon (2015); Sohn (2016); Wu et al. (2017); Duan et al. (2018) methods. We show in
Appendix A that the loss functions of these methods can be seen as approximation to the mutual
information (I(X; Y )). While all of these previous methods require a support/query split within each
task, DIMCO simply optimizes an information-theoretic quantity of each batch, removing the need
for such structured batch construction.
5	Experiments
We use the miniImageNet (Ravi & Larochelle, 2016) and CUB200 (Wah et al., 2011) datasets with
standard splits for both in our experiments. The miniImageNet dataset is a subset of the Imagenet
(Krizhevsky et al., 2012) dataset that was made for few-shot classification. It consists of 100 classes
each containing 600 images of size 84 × 84. The classes are split into 64 training, 16 validation, and
24 test classes. The Caltech-UCSD Birds-200-2011 (CUB200) dataset consists of 11788 images of
birds from 200 classes. The classes are split into 100 training and 100 test classes.
We use two different CNN backbones for our experiments: the 4-layer convnet commonly used
for meta-learning (Finn et al., 2017; Sung et al., 2018; Liu et al., 2018), and the Inception network
(Szegedy et al., 2015) with batch normalization (Ioffe & Szegedy, 2015) which is commonly used for
deep image retrieval (Sohn, 2016; Movshovitz-Attias et al., 2017; Wu et al., 2017). We randomly
initialize weights for the 4-layer convnet and use pretrained weights for the Inception network
5.1	Correlation of Metrics
This experiment empirically verifies whether mutual information I(X; Y ) is a reasonable metric for
quality of representation. We trained DIMCO on the miniImagenet dataset with p = d = 64 for 20
epochs, for 8 independent runs. We plot the pairwise correlations between five different metrics (
(5, 10, 20)-way 1-shot accuracy, Recall@1, and I(X; Y ) ) in Figure 2. We see that all five metrics are
very strongly correlated. We observed similar trends when training with with previously proposed loss
functions: we visualize these results in Figure 5 of the appendix due to space constraints. Alongside
this empirical evidence, we prove in Appendix A that previously used loss functions for few-shot
classification and image retrieval are approximations to I(X; Y ).
5.2	What does each code represent?
We inspected what each code represents in a DIMCO model (d = 16, p = 64) trained on the
miniImagenet dataset. Recall that each image produces a d × p probability matrix (5, 6). For each of
these dp entries, we plotted the top 10 images in the test set that assigned highest probability to that
entry. We show images corresponding to four such codes in Figure 2 (right) and more in Figure 6 of
the appendix.
We see that DIMCO learns a distributed representation. For example, the top code in Figure 2
represents the high-level concept of a furry animal and the shown 10 images span 4 different classes.
On the other hand, the bottom code seems to focus on the color of the background. By aggregating
6
Under review as a conference paper at ICLR 2020
Figure 2: Left: Pairwise correlation between MI = I (X; X) and previous metrics. Right: Visualiza-
tion of codes of a small trained DIMCO model (d = 16, p = 64); we show the top 10 test set images
that assign highest probability to a specific code, for 4 different codes.
Bits per representation
Figure 3: Image retrieval performance of DIMCO and N-pair loss on the CUB-200 dataset. The
y-axis for both figures are the Recall@1 metric, and error bars reflect standard deviation computed
from n = 5 runs per configuration. The x-axes represent (left) bits required to store one representation
and (right) seconds required to perform retrieval for one query. Both x-axes are log-scale.
420
555
1@llaceR
Time per query (s)

such complementary features in each of its d codewords, DIMCO is able to classify images that
belong to previously unseen classes.
5.3	Time- and Memory-Efficient Image Retrieval
We conducted an image retrieval experiment using the CUB200 dataset, and used multiclass N-
pair loss (Sohn, 2016) as a baseline. As is standard for image retrieval, we use the Inception
network as specified in the beginning of this section. Using the same backbone, we trained DIMCO
with (p, d) ∈ {64, 128, 256} × {128, 256, 512} and multiclass N-pair with embedding dimension
∈ {128, 256, 512}. We measured the time per query for each method on a single Tesla P40 GPU by
averaging the time required for 10000 batches of queries of size 32.
Results in Figure 3 show that the compact code of DIMCO takes roughly an order of magnitude less
memory for similar performance to N-pair loss, and requires less query time as well. This experiment
also demonstrates that discrete representations can match the performance of modern methods that
use continuous embeddings on this relatively large-scale task. We additionally note that DIMCO is
7
Under review as a conference paper at ICLR 2020
Figure 4: Performance of methods trained using subsets of miniImageNet of varying size. The
lowermost y axis value for each metric corresponds to the expected performance of random guessing.
able to train using large backbones without significantly overfitting, whereas experiments reported
in Mishra et al. (2017) indicate that MAML (Finn et al., 2017) overfits tremendously when using a
deeper backbone.
5.4	Generalization to New Tasks
This experiment measures how well DIMCO can generalize to new datasets after training with a
small number of datasets. This challenging experimental setup measures how much generalizable
information the model can extract from a limited set of datasets; it can be seen as the meta-learning
analogue of measuring the performance of a classifier trained with a small dataset.
We trained each model using {1, 4, 16, 64} samples from each training class in the miniImageNet
dataset. For example, by using 4 samples, we are reducing the full train split of (64 classes × 600
images per class) into (64 classes × 4 images per class). We compare against three baseline methods:
Triplet Nets(Hoffer & Ailon, 2015), multiclass N-pair loss(Sohn, 2016), and ProtoNets(Snell et al.,
2017). We report the average and standard deviation of the top 5 results of a random hyperparameter
search (see appendix for details). We show {5, 10, 20}-way 1-shot accuracies and Recall@1 of the
test set in Figure 4.
First note that DIMCO is the only method that can train using a dataset consisting of 1 example
per class. This is because other methods require at least one train and test example per class within
each batch, while DIMCO requires no such train/test (also called support/query) split and simply
maximizes the mutual information within a batch. Furthermore, Figure 4 shows that DIMCO learns
much more effectively when the number of examples per class is low; this is because DIMCO uses
fewer bits to describe each datapoint, which lowers its generalization gap (24) when applying to
novel datasets.
6	Conclusion and Discussion
Summary We derived a generalization bound for meta-learning using information-theoretic princi-
ples. Building on our bound, we proposed DIMCO, a model that learns a discrete representation of
data by maximizing its mutual information with the label. DIMCO had benefits in time, memory, and
generalization in our experiments.
Towards Explicit Meta-Regularization In Section 4, we have suggested with analogy to The-
orem 1 that the benefits of some previous meta-learning methods can be attributed to implicitly
being regularized by reducing the expressivity of their task-specific learners. While DIMCO has
demonstrated better generalization by reducing this expressivity via hyperparameters (d, p), this is
only applicable to DIMCO’s specific setup of few-shot classification through mutual information
maximization. In future work, we would like to explore explicit meta-regularization schemes that
8
Under review as a conference paper at ICLR 2020
can be applied to other problems (regression, reinforcement learning etc.) and algorithms (MAML,
Neural Process etc.).
Meta-Learning Without Splits Our meta-learning problem setup in Section 2.1 does not assume a
support/query (also called train/test) split within each dataset. Along with showing that the traditional
support/query split is not strictly necessary, we demonstrated in Section 5.4 that removing it has the
benefit of enabling meta-learning in datasets having one image per class. We believe future work
could benefit from further exploring this space of meta-learning algorithms that use datasets without
splits.
References
Alexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. Deep variational information
bottleneck. arXiv preprint arXiv:1612.00410, 2016.
Alexander A Alemi, Ben Poole, Ian Fischer, Joshua V Dillon, Rif A Saurous, and Kevin Murphy.
Fixing a broken elbo. arXiv preprint arXiv:1711.00464, 2017.
Ron Amit and Ron Meir. Meta-learning by adjusting priors based on extended pac-bayes theory.
arXiv preprint arXiv:1711.01244, 2017.
Tian Qi Chen, Xuechen Li, Roger B Grosse, and David K Duvenaud. Isolating sources of disentan-
glement in variational autoencoders. In Advances in Neural Information Processing Systems, pp.
2610-2620, 2018.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan:
Interpretable representation learning by information maximizing generative adversarial nets. In
Advances in neural information processing systems, pp. 2172-2180, 2016.
Kristy Choi, Kedar Tatwawadi, Aditya Grover, Tsachy Weissman, and Stefano Ermon. Neural joint
source-channel coding. In International Conference on Machine Learning, pp. 1182-1192, 2019.
Thomas M Cover and Joy A Thomas. Elements of information theory. John Wiley & Sons, 2012.
Yueqi Duan, Wenzhao Zheng, Xudong Lin, Jiwen Lu, and Jie Zhou. Deep adversarial metric
learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.
2780-2789, 2018.
Chelsea Finn and Sergey Levine. Meta-learning and universality: Deep representations and gradient
descent can approximate any learning algorithm. arXiv preprint arXiv:1710.11622, 2017.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of
deep networks. arXiv preprint arXiv:1703.03400, 2017.
Sebastian Flennerhag, Pablo G Moreno, Neil D Lawrence, and Andreas Damianou. Transferring
knowledge across learning processes. arXiv preprint arXiv:1812.01054, 2018.
Anirudh Goyal, Riashat Islam, Daniel Strouse, Zafarali Ahmed, Matthew Botvinick, Hugo Larochelle,
Sergey Levine, and Yoshua Bengio. Infobot: Transfer and exploration via the information
bottleneck. arXiv preprint arXiv:1901.10902, 2019.
Aditya Grover and Stefano Ermon. Uncertainty autoencoders: Learning compressed representations
via variational information maximization. arXiv preprint arXiv:1812.10539, 2018.
R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Adam Trischler, and
Yoshua Bengio. Learning deep representations by mutual information estimation and maximization.
arXiv preprint arXiv:1808.06670, 2018.
Elad Hoffer and Nir Ailon. Deep metric learning using triplet network. In International Workshop on
Similarity-Based Pattern Recognition, pp. 84-92. Springer, 2015.
9
Under review as a conference paper at ICLR 2020
Weihua Hu, Takeru Miyato, Seiya Tokui, Eiichi Matsumoto, and Masashi Sugiyama. Learning
discrete representations via information maximizing self-augmented training. In Proceedings of
the 34th International Conference on Machine Learning-Volume 70, pp. 1558-1567. JMLR. org,
2017.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Herve Jegou, Matthijs Douze, and Cordelia Schmid. Product quantization for nearest neighbor search.
IEEE transactions on pattern analysis and machine intelligence, 33(1):117-128, 2011.
Yeonwoo Jeong and Hyun Oh Song. Efficient end-to-end learning for quantizable representations.
arXiv preprint arXiv:1805.05809, 2018.
Taesup Kim, Jaesik Yoon, Ousmane Dia, Sungwoong Kim, Yoshua Bengio, and Sungjin Ahn.
Bayesian model-agnostic meta-learning. arXiv preprint arXiv:1806.03836, 2018.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolu-
tional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
Yoonho Lee and Seungjin Choi. Gradient-based meta-learning with learned layerwise metric and
subspace. 2018.
Yanbin Liu, Juho Lee, Minseop Park, Saehoon Kim, and Yi Yang. Transductive propagation network
for few-shot learning. arXiv preprint arXiv:1805.10002, 2018.
Luke Metz, Niru Maheswaranathan, Brian Cheung, and Jascha Sohl-Dickstein. Meta-learning update
rules for unsupervised representation learning. arXiv preprint arXiv:1804.00222, 2018.
Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive meta-
learner. arXiv preprint arXiv:1707.03141, 2017.
Yair Movshovitz-Attias, Alexander Toshev, Thomas K Leung, Sergey Ioffe, and Saurabh Singh. No
fuss distance metric learning using proxies. In Proceedings of the IEEE International Conference
on Computer Vision, pp. 360-368, 2017.
Alex Nichol and John Schulman. Reptile: a scalable metalearning algorithm. arXiv preprint
arXiv:1803.02999, 2018.
Mohammad Norouzi and David J Fleet. Cartesian k-means. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pp. 3017-3024, 2013.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive
coding. arXiv preprint arXiv:1807.03748, 2018.
Boris Oreshkin, PaU RodrigUez L6pez, and Alexandre Lacoste. Tadam: Task dependent adaptive
metric for improved few-shot learning. In Advances in Neural Information Processing Systems, pp.
721-731, 2018.
Sachin Ravi and HUgo Larochelle. Optimization as a model for few-shot learning. 2016.
Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with
vq-vae-2. arXiv preprint arXiv:1906.00446, 2019.
Jason Tyler Rolfe. Discrete variational aUtoencoders. arXiv preprint arXiv:1609.02200, 2016.
Andrei A RUsU, DUshyant Rao, JakUb Sygnowski, Oriol Vinyals, Razvan PascanU, Simon Osin-
dero, and Raia Hadsell. Meta-learning with latent embedding optimization. arXiv preprint
arXiv:1807.05960, 2018.
10
Under review as a conference paper at ICLR 2020
Ohad Shamir, Sivan Sabato, and Naftali Tishby. Learning and generalization with the information
bottleneck. Theoretical Computer Science, 411(29-30):2696-2711, 2010.
Claude Elwood Shannon. A mathematical theory of communication. Bell system technical journal,
27(3):379-423, 1948.
Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via information.
arXiv preprint arXiv:1703.00810, 2017.
Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In
Advances in Neural Information Processing Systems, pp. 4077-4087, 2017.
Kihyuk Sohn. Improved deep metric learning with multi-class n-pair loss objective. In Advances in
Neural Information Processing Systems, pp. 1857-1865, 2016.
DJ Strouse and David J Schwab. The deterministic information bottleneck. Neural computation, 29
(6):1611-1630, 2017.
Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, and Timothy M Hospedales.
Learning to compare: Relation network for few-shot learning. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pp. 1199-1208, 2018.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1-9, 2015.
Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. In 2015
IEEE Information Theory Workshop (ITW), pp. 1-5. IEEE, 2015.
Naftali Tishby, Fernando C Pereira, and William Bialek. The information bottleneck method. arXiv
preprint physics/0004057, 2000.
Eleni Triantafillou, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Kelvin Xu, Ross Goroshin, Carles
Gelada, Kevin Swersky, Pierre-Antoine Manzagol, and Hugo Larochelle. Meta-dataset: A dataset
of datasets for learning to learn from few examples. arXiv preprint arXiv:1903.03096, 2019.
Aaron van den Oord, Oriol Vinyals, et al. Neural discrete representation learning. In Advances in
Neural Information Processing Systems, pp. 6306-6315, 2017.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,匕Ukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998-6008, 2017.
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one
shot learning. In Advances in neural information processing systems, pp. 3630-3638, 2016.
Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd
birds-200-2011 dataset. 2011.
Chao-Yuan Wu, R Manmatha, Alexander J Smola, and Philipp Krahenbuhl. Sampling matters in
deep embedding learning. In Proceedings of the IEEE International Conference on Computer
Vision, pp. 2840-2848, 2017.
Luisa M Zintgraf, Kyriacos Shiarlis, Vitaly Kurin, Katja Hofmann, and Shimon Whiteson. Caml:
Fast context adaptation via meta-learning. arXiv preprint arXiv:1810.03642, 2018.
11
Under review as a conference paper at ICLR 2020
A Previous Loss functions Are Approximations to Mutual
Information
Cross-entropy Loss The cross-entropy loss has directly been used for few-shot classification
(Vinyals et al., 2016; Snell et al., 2017).
Let q(y|xe; φ) be a parameterized prediction of y given xe, which tries to approximate the true
conditional distribution q(y|xe). Typically in a classification network, φ is the parameters of a learned
projection matrix and q(∙) is the final linear layer. The expected cross-entropy loss can be written as
Xent(Y, X) = Ey〜Y,e〜X [Tog q(y|e, φ)] .	(12)
Assuming that the approximate distribution q(∙) is sufficiently close to p(y∣x), minimizing (12) can
be seen as
arg min Xent(Y, X) ≈ arg min Ey 〜Y e〜X [- log p(y∣X)]	(13)
y,
= arg min H(Y |X) = argmaXI(X; Y),	(14)
where the last equality uses the fact that H(Y) is independent of model parameters. Therefore, cross-
entropy minimization is approximate maximization of the mutual information between representation
X and labels Y .
The approximation is that we parameterized q(y|xe; φ) as a linear projection. This structure cannot
generalize to new classes because the parameters φ are specific to the labels y seen during training.
For a model to generalize to unseen classes, one must amortize the learning of this approximate
conditional distribution. (Vinyals et al., 2016; Snell et al., 2017) sidestepped this issue by using the
embeddings for each class as φ.
Triplet Loss The Triplet loss (Hoffer & Ailon, 2015) is defined as
Ltriplet =	xeq	-	xep	22	-	xeq	-	xen	22	,
(15)
where xeq, xep, xen ∈ Rd are the embedding vectors of query, positive, and negative images. Let yq
denote the label of the query data. Recall that the pdf function of a unit Gaussian is log N(x∣μ, 1)=
-ci - C2∣∣X - μk2, where c1,c2 are constants. LetPp(X) = N(xp, 1) andPn(x) = N(Xn, 1) be
unit Gaussian distributions centered at xxp , xxn respectively. We have
E [-Ltriplet]	H	E	[logPp(X)-IogPn(x)]	(16)
≈	E	[log Pp(Xx)	- log P(Xx)]	(17)
= -H(Xx|Y) +H(Xx) =I(Xx;Y).	(18)
Two approximations were made in the process. We first assumed that the embedding distribution of
images not in yq is equal to the distribution of all embeddings. This is reasonable when each class
only represents a small fraction of the full data. We also approximated the embedding distributions
P(Xx|y), P(Xx) with unit Gaussian distributions centered at single samples from each.
N-pair Loss Multiclass N -pair loss (Sohn, 2016) was proposed as an alternative to Triplet loss.
This loss function requires one positive embedding Xx+ and multiple negative embeddings Xx1, . . . ,
XxN -1, and takes the form
-log exp(X>X+e：PN-I)exp(X>Xi)
(19)
This can be seen as the cross-entropy loss applied to softmax(Xx>Xx+, Xx>Xx1, . . . , Xx>XxN-1).
Following the same logic as the cross-entropy loss, this is also an approximation to I(X; Y). This
objective should have less variance than Triplet loss since it approximates P(Xx) using more examples.
12
Under review as a conference paper at ICLR 2020
Adversarial Metric Learning Deep Adversarial Metric Learning (Duan et al., 2018) tackles the
problem of most negative exmples being uninformative by directly generating meaningful negative
embeddings. This model employs a generator which takes as input the embeddings of anchor,
positive, and negative images. The generator then outputs a "synthetic negative" embedding that is
hard to distinguish from a positive embedding while being close to the negative embedding.
This can be seen as optimizing
E log pp(xe) - log p(xe) = I(Xe;Y)	(20)
by estimating p(xe) using a generative network rather than directly from samples. Rather than
modelling the marginal distribution p(xe), this method conditionally models p(xe; xeq, xep, xen) so that
xe is hard to distinguish from xep while sufficiently close to both xeq and xen .
B Proof of Theorem 1
We restate and prove our main theorem.
Theorem 1. Let dθ be the VCdimension ofthe encoder X(∙). Let I(X(XT, θ); YT) be the empirical
estimate of the mutual information using finite dataset DT, and define empirical loss as
1n
L(T1:n") = - - EI(X (Xt i ,θ); YT i).
n i=1
(21)
The following inequality holds with high probability:
LT⑼-LT'θ) ≤ O (rdFg!) +O (⅛2) +O (呼)(22)
Proof. We use the following lemma from Shamir et al. (2010), which we restate using our notation.
Lemma 1. Let X be a random mapping of X. Let D be a sample of size m drawn from the joint
probability distribution p(X, Y ). Denote the empirical mutual information observed from D between
X and Y as I(X; Y ). For any δ ∈ (0, -), the following holds with probability at least - - δ:
e V、^e	(3∣x∣ + 2) iog(m)piog(46)	(∣Y∣ + i)(IX∣ + ι) - 4
lI(X ； Y) -I(X ； Y) । ≤	√2m	+	m	(23)
We simplify this and plug in our specific quantities of interest (X(XT, θ), YT):
I(X (XT ⑼;YT)-I(X (XT ⑼;YT)∣≤ O ^iXMmiV O (JXmY
(24)
We similarly bound the error caused by estimating L with a finite number of tasks sampled from τ .
Denote the finite sample estimate of L as
-n
L(τ,θ) = - - EI (X (XT i ,θ); YT i).	(25)
n i=1
Let the mapping X 7→ X be parameterized by θ ∈ Θ and let this model have VC dimension dΘ .
Using dΘ, we can state that with high probability,
ʌ
L(τ,θ) - L(T阳
dθ	,
(26)
where dΘ is the VC dimension of hypothesis class Θ.
13
Under review as a conference paper at ICLR 2020
Combining equations (26, 24), we have with high probability
L(τ,θ) - (-n ^X I(X(XTi, θ); Yτi)j J
≤∣L(τ,θ)-L(τ,θ)∣+o (ιx√m^! + O (W!
(27)
(28)
(29)
□
C Experiments and Implementation Details
Hardware Every experiment was conducted on a single Nvidia V100 GPU with CUDA 9.2. We
used PyTorch version 1.0.1. Each experiment was performed with different fixed initial seeds; we
manually fix seeds with manual_seed() for python, pytorch, and numpy.
Optimizer For experiments with the 4-layer convnet, we use the Adam optimizer (Kingma & Ba,
2014) with learning rate 3e-4. For the Inception network, we use SGD with learning rate 3e-5 and
momentum 0.9.
We report the average of 500 batches of 1-shot accuracies and mutual information. I (X; Y ) was
computed using balanced batches of 16 images each from 5 different classes. We additionally show
in Figure 5 the correlation between 1-shot accuracies, Recall@1, and NMI using three previously
proposed losses (triplet, npair, protonet).
Small Train Set Experiment For this experiment, we used the Adam optimizer and performed
a log-uniform hyperparameter sweep for learning rate ∈ [1e-7, 1e-3] For DIMCO, we swept p ∈
[32, 128] and d ∈ [16, 32]. For other methods, we made the embedding dimension ∈ [16, 32]. For
each combination of loss and number of training examples per class, we ran the experiment 64 times
and reported the mean and standard deviation of the top 5.
14
Under review as a conference paper at ICLR 2020
Figure 5: Correlation between few-shot accuracy and retrieval measures.
15
Under review as a conference paper at ICLR 2020
Figure 6: Additional visualizations of codes.
16