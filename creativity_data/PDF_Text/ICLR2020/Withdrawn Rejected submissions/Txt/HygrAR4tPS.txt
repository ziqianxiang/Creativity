Under review as a conference paper at ICLR 2020
On Empirical Comparisons of Optimizers
for Deep Learning
Anonymous authors
Paper under double-blind review
Ab stract
Selecting an optimizer is a central step in the contemporary deep learning pipeline.
In this paper, we demonstrate the sensitivity of optimizer comparisons to the hy-
perparameter tuning protocol. Our findings suggest that the hyperparameter search
space may be the single most important factor explaining the rankings obtained by
recent empirical comparisons in the literature. In fact, we show that these results
can be contradicted when hyperparameter search spaces are changed. As tuning
effort grows without bound, more general optimizers should never underperform
the ones they can approximate (i.e., Adam should never perform worse than mo-
mentum), but recent attempts to compare optimizers either assume these inclusion
relationships are not practically relevant or restrict the hyperparameters in ways
that break the inclusions. In our experiments, we find that inclusion relationships
between optimizers matter in practice and always predict optimizer comparisons.
In particular, we find that the popular adaptive gradient methods never underper-
form momentum or gradient descent. We also report practical tips around tuning
often ignored hyperparameters of adaptive gradient methods and raise concerns
about fairly benchmarking optimizers for neural network training.
1	Introduction
The optimization algorithm chosen by a deep learning practitioner determines the training speed and
the final predictive performance of their model. To date, there is no theory that adequately explains
how to make this choice. Instead, our community relies on empirical studies (Wilson et al., 2017)
and benchmarking (Schneider et al., 2019). Indeed, itis the de facto standard that papers introducing
new optimizers report extensive comparisons across a large number of workloads. Therefore, to
maximize scientific progress, we must have confidence in our ability to make empirical comparisons
between optimization algorithms.
Although there is no theory guiding us when comparing optimizers, the popular first-order optimiz-
ers form a natural inclusion hierarchy. For example, Adam (Kingma and Ba, 2015) and RMSProp
(Tieleman and Hinton, 2012) can approximately simulate MOMENTUM (Polyak, 1964) if the term
in the denominator of their parameter updates is allowed to grow very large. However, these rela-
tionships may not matter in practice. For example, the settings of Adam’s hyperparameters that
allow it to match the performance of Momentum may be too difficult to find (for instance, they
may be infinite).
In this paper, we demonstrate two important and interrelated points about empirical comparisons of
neural network optimizers. First, we show that inclusion relationships between optimizers actually
matter in practice; in our experiments, more general optimizers never underperform special cases.
Despite conventional wisdom (Wilson et al., 2017; Balles and Hennig, 2017), we find that when
carefully tuned, Adam and other adaptive gradient methods never underperform Momentum or
SGD. Second, we demonstrate the sensitivity of optimizer comparisons to the hyperparameter tun-
ing protocol. By comparing to previous experimental evaluations, we show how easy it is to change
optimizer rankings on a given workload (model and dataset pair) by changing the hyperparameter
tuning protocol, with optimizer rankings stabilizing according to inclusion relationships as we spend
more and more effort tuning. Our findings raise serious questions about the practical relevance of
conclusions drawn from these sorts of empirical comparisons.
1
Under review as a conference paper at ICLR 2020
The remainder of this paper is structured as follows. In Section 2, we review related work, focusing
on papers that make explicit claims about optimizer comparisons in deep learning and application
papers that provide evidence about the tuning protocols of practitioners. We develop our definition
of first-order optimizers in Section 3 along with a notion of inclusion relationships between optimiz-
ers. We present our experimental results in Section 4. Despite thorny methodological issues over
how to avoid biases in comparisons due to search spaces that favor one optimizer over another, we
believe that our experimental methodology is an acceptable compromise and has substantial practi-
cal relevance. Among other results, we show that the inclusion hierarchy of update rules is almost
entirely predictive of optimizer comparisons. In particular, NAdam (Dozat, 2016) achieves the best
top-1 validation accuracy on ResNet-50 on ImageNet in our experiments. The 77.1% we obtain
with NAdam, although not as good as the 77.6% obtained using learned data augmentation by
Cubuk et al. (2018), is better than the best existing published results using any of the more standard
pre-processing pipelines (76.5%, due to Goyal et al. (2017) using Momentum).
2	Background and Related Work
Our work was inspired by the recent studies of neural network optimizers by Wilson et al. (2017)
and Schneider et al. (2019). Wilson et al. (2017) constructed a simple classification problem in
which adaptive gradient methods (e.g. Adam) converge to provably worse solutions than standard
gradient methods. However, crucially, their analysis ignored the parameter in the denominator of
some adaptive gradient methods. Wilson et al. (2017) also presented experiments in which Adam
produced worse validation accuracy than SGD across all deep learning workloads considered. How-
ever, they only tuned over the learning rate and learning rate decay scheme in their experiments,
leaving all other parameters of Adam at fixed default values. Despite these findings, adaptive gra-
dient methods continue to be popular since the work of Wilson et al. (2017). Schneider et al. (2019)
presented a benchmark suite (DeepOB S) for deep learning optimizers and reported that there was
no single best optimizer across the workloads they considered. Yet Schneider et al. (2019) only
tuned the learning rate of each optimizer and left all other hyperparameters at some fixed default
values.
As we discuss in Section 4.3, the choices of hyperparameter tuning protocols in Wilson et al. (2017)
and Schneider et al. (2019) may be the most important factor preventing their results from being
relevant to practical choices about which optimizer to use. Hyperparameter tuning is a crucial step
of the deep learning pipeline (Bergstra and Bengio, 2012; Snoek et al., 2012; Sutskever et al., 2013;
Smith, 2018), so itis critical for papers studying optimizers to match as closely as possible the tuning
protocols of an ideal practitioner. Yet, tuning protocols often differ between works studying neural
network optimizers and works concerned with training neural networks to solve specific problems.
Recent papers that study or introduce optimization algorithms tend to compare to Adam and
RMSPROP without tuning their respective hyperparameters, presumably to simplify their exper-
iments. It is standard to leave at the common default value of 10-8 for ADAM and 10-10 for
RMSProp (Tieleman and Hinton, 2012; Kingma and Ba, 2015; Dozat, 2016; Balles and Hennig,
2017; Loshchilov and Hutter, 2017; Zou and Shen, 2018; Ma and Yarats, 2018; Bernstein et al.,
2018; Chen et al., 2019; Zou et al., 2019). Others do not even report the value of used (Balles
and Hennig, 2017; Zhang and Mitliagkas, 2017; Keskar and Socher, 2017; Chen et al., 2018; Zhou
et al., 2018; Aitchison, 2018; Reddi et al., 2019; Luo et al., 2019). There are exceptions. Zaheer
et al. (2018) and Liu et al. (2019) considered values orders of magnitude larger than the standard
default. However, the experiments in both papers gave only a limited consideration to , testing at
most two values while tuning Adam. De et al. (2018) is the only work we found that considered a
broad range of values for . Both Zaheer et al. (2018) and De et al. (2018) found that non-default
values of outperformed the default.
While it is also extremely common in applications to use a default value of , some notable pa-
pers tuned and selected values up to eight orders of magnitude away from the common defaults.
Szegedy et al. (2016) used = 1 for RMSPROP; Liu et al. (2019) reported that their results were
sensitive to and set = 10-6 for ADAM; Tan et al. (2019) and Tan and Le (2019) set = 10-3 for
RMSProp, the latter achieving state-of-the-art ImageNet top-1 accuracy. In reinforcement learning,
Hessel et al. (2017) set = 1.5 × 10-4.
2
Under review as a conference paper at ICLR 2020
Despite being introduced solely to prevent division by zero1 , ADAM’s can be interpreted in ways
that suggest the optimal choice is problem-dependent. If Adam is interpreted as an empirical,
diagonal approximation to natural gradient descent (Kingma and Ba, 2015), can be viewed as a
multi-purpose damping term whose role is to improve the conditioning of the Fisher, in analogy to
the approximate second-order method considered by Becker and Le Cun (1988). We can also view
as setting a trust region radius (Martens and Grosse, 2015; Adolphs et al., 2019) and controlling
an interpolation between momentum and diagonal natural gradient descent, by either diminishing or
increasing the effect of vt on the update direction. Under either interpretation, the best value for
will be problem-dependent and likely benefit from tuning.
3	What is an optimizer?
Optimization algorithms are typically defined by their update rule, which is controlled by hyperpa-
rameters that determine its behavior (e.g. the learning rate). Consider a differentiable loss function
' :Rd → R whose vector of first partial derivatives is given by V'(θ) (more generally, V'(θ) might
be a stochastic estimate of the true gradient). In our context, ` represents the loss function computed
over an entire dataset by a neural network and θ ∈ Rd represents the vector of model parameters.
The optimization problem is to find a point that (at least locally) minimizes `. First-order iterative
methods for this problem (Nesterov, 2018) construct a sequence θt of iterates converging to a local
minimum θ? using queries to ' and V'. The sequence θt is constructed by an update rule M, which
determines the next iterate θt+1 from the history Ht of previous iterates along with their function
and gradient values, Ht = {θs, V'(θs), '(θs)}S=0, and a setting of hyperparameters φ : N → Rn.
Given an initial parameter value θ0 ∈ Rd, the sequence of points visited by an optimizer with update
rule M is given by,
θt+1 = M(Ht, φt).	(1)
The stochastic gradient descent algorithm (SGD; Robbins and Monro, 1951) is one of the sim-
plest such methods used for training neural networks. SGD is initialized with θ0 ∈ Rd, and its
hyperparameter is a learning rate schedule η : N → (0, ∞). The SGD update rule is given by
SGD(Ht,ηt) = θt 一 ηV'(θt). The Momentum method due to Polyak (i964) generalizes the
SGD method by linearly combining the gradient direction with a constant multiple of the previ-
ous parameter update. Its hyperparameters are a learning rate schedule η : N → (0, ∞) and a
momentum parameter γ ∈ [0, ∞),
MOMENTUM(Ht, ηt, Y) = θt- ηtV'(θt) + γ(θt - θt-i).	(2)
There has been an explosion of novel first-order methods in deep learning, all of which fall into this
standard first-order scheme. In Table 1 we list the first-order update rules considered in this paper.
The difference between optimizers is entirely captured by the choice of update rule M and hyper-
parameters φ. Since the roles of optimizer hyperparameters on neural network loss functions are not
well-understood, most practitioners tune a subset of the hyperparameters to maximize performance
over a validation set, while leaving some hyperparameters at fixed default values. The choice of
which hyperparameters to tune determines an effective family of update rules, and this family is the
critical object from a practitioners perspective. Thus, in analogy to (overloaded) function declara-
tions in C++, we define an optimizer by an update rule “signature,” the update rule name together
with the free hyperparameter arguments. For example, in this definition MOMENTum(∙, ηt, γ) is
not the same optimizer as MOMENTum(∙, ηt, 0.9), because the latter has two free hyperparameters
while the former only has one. ADAM with the default is “different” from ADAM with tuned .
3.1	The taxonomy of first-order methods
The basic observation of this section is that some optimizers can approximately simulate others (i.e.,
optimizer A might be able to approximately simulate the trajectory of optimizer B for any partic-
ular setting of B’s hyperparameters). This is important knowledge because, as a hyperparameter
tuning protocol approaches optimality, a more expressive optimizer can never underperform any of
its specializations. To capture the concept of one optimizer approximating another, we define the
following inclusion relationship between optimizers.
1TensorFlow currently refers to as “a small constant for numerical stability”; https://www.
tensorflow.org/versions/r1.15/api_docs/python/tf/train/AdamOptimizer.
3
Under review as a conference paper at ICLR 2020
Table 1: Update rules considered in this work. SGD is due to Robbins and Monro (1951),
MOMENTUM to Polyak (1964), NESTEROV to Nesterov (1983), RMSPROP to Tieleman and Hin-
ton (2012), and NAdam to Dozat (2016). All operations are taken component-wise for vectors. In
particular, for x ∈ Rd, x2 is a component-wise power function.
SGD(Ht,ηt)
θt+ι = θt - ηtN'(θt)
MOMENTUM(Ht, ηt, γ)
v0 = 0
Vt+1 = Yvt + v`(θt)
θt+1 = θt - ηtvt+1
NESTEROV(Ht,ηt,γ)
vo = 0
vt+1 = Yvt + v`(θt)
θt+ι = θt - ηt (γvt+ι + v`(θt))
RMSPROP(Ht, ηt, Y,ρ, E)
v0 = 1, m0 = 0
vt+1 = Pvt + (1 — ρ)V'(θt)2
mt+1 = Ymt + √]∖ + E V'(θt)
θt+1 = θt - mt+1
ADAM(Ht,αt,β1,β2,E)
m0 = 0, v0 = 0
mt+1 = βιmt + (1 — βι)V'(θt)
vt+1 = β2vt + (1 — β2)V'(θt)2
h	√1 — β2+1
bt+1 = 1 — βt+1
θt+1 = θt — αt l t+~- bt+1
√vt+1 + E
NADAM(Ht, αt, β1, β2, E)
m0 = 0, v0 = 0
mt+1 = β1mt + (1 — β1)V'(θt)
vt+1 = β2vt + (1 — β2)V'(θt)2
θt+1
θt
—
αt
β1mt+1 + (1 — β1)V'(θt)
√vt+1 + E
bt+1
Definition 1 (Inclusion relationship). Let M, N be update rules for use in a first-order optimization
method. M is a subset or specialization of N , if for all φ : N → Rn, there exists a sequence
ψi : N → Rm, such that for all t ∈ [0, ∞) and histories Ht,
lim N(Ht,ψti) =M(Ht,φt)
i→∞
This is denoted M ⊆ N, with equality M = N iff M ⊆ N and N ⊆ M.
Evidently SGD ⊆ MOMENTUM, since SGD(Ht, ηt) = MOMENTUM(Ht, ηt, 0). Many well-
known optimizers fall naturally into this taxonomy. In particular, we consider RMS Prop with
momentum (Tieleman and Hinton, 2012), Adam (Kingma and Ba, 2015) and NAdam (Dozat,
2016) (see Table 1) and show the following inclusions in the appendix.
SGD ⊆ MOMENTUM ⊆ RMSPROP
SGD ⊆ MOMENTUM ⊆ ADAM	(3)
S GD ⊆ NESTEROV ⊆ NADAM
Note, some of these inclusions make use of the flexibility of hyperparameter schedules (dependence
of ψi on t). In particular, to approximate MOMENTUM with ADAM, one needs to choose a learning
rate schedule that accounts for Adam’s bias correction.
If two optimizers have an inclusion relationship, the more general optimizer can never be worse with
respect to any metric of interest, provided the hyperparameters are sufficiently tuned to optimize that
metric. Optimally-tuned Momentum cannot underperform optimally-tuned SGD, because setting
Y = 0 in MOMENTUM recovers SGD. However, optimizers with more hyperparameters might
be more expensive to tune, so we should have a theoretical or experimental reason for using (or
creating) a more general optimizer. For example, Momentum improves local convergence rates
over SGD on twice-differentiable functions that are smooth and strongly convex (Polyak, 1964), and
4
Under review as a conference paper at ICLR 2020
Nesterov has globally optimal convergence rates within the class of smooth and strongly convex
functions (Nesterov, 1983; 2018).
At first glance, the taxonomy of optimizer inclusions appears to resolve many optimizer comparison
questions. However, for a deep learning practitioner, there is no guarantee that the inclusion hierar-
chy is at all meaningful in practice. For example, the hyperparameters that allow Adam to match
or outperform Momentum might not be easily accessible. They might exist only in the limit of
very large values, or be so difficult to find that only practitioners with huge computational budgets
can hope to discover them. Indeed, empirical studies and conventional wisdom hold that the inclu-
sion hierarchy does not predict optimizer performance for many practical workloads (Wilson et al.,
2017; Balles and Hennig, 2017; Schneider et al., 2019). Either these experimental investigations are
too limited or the taxonomy of this section is of limited practical interest and provides no guidance
about which optimizer to use on a real workload. In the following section we attempt to answer this
question experimentally, and show that these inclusion relationships are meaningful in practice.
4 Experiments
An empirical comparison of optimizers should aim to inform a careful practitioner. Accordingly,
we model our protocol on a practitioner that is allowed to vary all optimization hyperparameters
for each optimizer (e.g. αt, β1, β2, for ADAM) in addition to a parameterized learning rate decay
schedule, in contrast to studies that fix a subset of the optimization hyperparameters to their default
values (e.g. Wilson et al., 2017; Schneider et al., 2019). There is no standard method for selecting
the values of these hyperparameters, but most practitioners tune at least a subset of the optimization
hyperparameters by running a set of trials to maximize performance over the validation set. In
our experiments, we run tens to hundreds of individual trials per workload. Given the variety of
workloads we consider, this trial budget covers a wide range of computational budgets.
Selecting the hyperparameter search space for each optimizer is a key methodological choice for
any empirical comparison of optimizers. Prior studies have attempted to treat each optimizer fairly
by using the same search space for all optimizers (e.g. Wilson et al., 2017; Schneider et al., 2019).
However, this requires the assumption that similarly-named hyperparameters should take similar
values between optimizers, which is not always true. For example, Momentum and Nesterov
both have similar-looking momentum and learning rate hyperparameters, but Nesterov tolerates
larger values of its momentum hyperparameter (Sutskever et al., 2013), so any fixed search space
will likely be more favorable for one of the two. The situation worsens with less closely related opti-
mizers, and designing a search space that is equally appropriate for optimizers with incommensurate
hyperparameters is almost impossible. Despite coming with its own set of challenges, it is most in-
formative to compare optimizers assuming the practitioner is allowed to tune hyperparameters for
different optimizers independently by way of optimizer-specific search spaces.
In our experiments, we chose the search space for each optimizer by running an initial set of exper-
iments over a relatively large search space. In a typical case, we ran a single set of initial trials per
optimizer to select the final search space. However, in some cases we chose the initial search space
poorly, so we ran another set of experiments to select the final search space. The effort required to
choose each search space cannot simply be quantified by the number of initial trials; the provenance
of each search space is difficult to trace exactly. In some cases, our search spaces were informed by
published results or prior experience with particular models and optimizers. We note that this is true
of all search spaces in the literature: they are hard-won treasures that tend to be refined over many
experiments and across many workloads, representing the sum total of our community’s experience.
We validated our search spaces by checking that that the optimal hyperparameter values were away
from the search space boundaries for all optimizers in all experiments (see Figure 5 in Appendix E).
We provide our final search spaces for all experiments in Appendix D. The fact that our final er-
ror rates compare favorably to prior published results - including reaching state-of-the-art for our
particular configuration of ReSNet-50 on ImageNet (see Section 4.2) - supports our claim that our
methodology is highly competitive with expert tuning procedures.
5
Under review as a conference paper at ICLR 2020
Table 2: Summary of workloads used in experiments.
Task	Evaluation metric	Model	Dataset	Target error	Batch size	Budget
		Simple CNN	Fashion MNIST	6.6%	256	10k steps
Image	Classification	ResNet-32	CIFAR-10	7%	256	50k steps
classification	error	CNN	CIFAR-100	-	256	350 epochs
		VGG-16	CIFAR-10	-	128	250 epochs
		ResNet-50	ImageNet	24%	1024	150k steps
Language modeling	Classification error	LSTM	War and Peace	-	50	200 epochs
	Cross entropy	Transformer	LM1B	3.45	256	750k steps
4.1	Overview of Workloads and Experimental Details
We investigated the relative performance of optimizers across a variety of image classification and
language modeling tasks. For image classification, we trained a simple convolutional neural network
(Simple CNN) on Fashion MNIST (Xiao et al., 2017); ResNet-32 (He et al., 2016a) on CIFAR-10
(Krizhevsky, 2009); a CNN on CIFAR-100; VGG-16 (Simonyan and Zisserman, 2014) on CIFAR-
10; and ResNet-50 on ImageNet (Russakovsky et al., 2015). For language modeling, we trained a
2-layer LSTM model (Hochreiter and Schmidhuber, 1997) on Tolstoy’s War and Peace; and Trans-
former (Vaswani et al., 2017) on LM1B (Chelba et al., 2014). We used a linear learning rate decay
schedule parameterized the same way as Shallue et al. (2019) for all workloads. We used a fixed
batch size and a fixed budget of training steps for each workload independent of the optimizer.
Table 2 summarizes these workloads and Appendix B provides the full details.
Given a hypercube-shaped search space, our tuning protocol sought to model a practitioner with a
fixed budget of trials trying to achieve the best outcome using tens of feasible trials (10, 50, or 100
depending on the workload).2 A feasible trial is any trial that achieves finite training loss. We used
quasi-random uniform search (Bousquet et al., 2017), and continued the search until we obtained a
fixed number of feasible trials. From those trials we considered two statistics. The first, in order to
characterize the best outcome, is a metric of interest (e.g. test accuracy) corresponding to the trial
achieving the optimum of some other metric (e.g. validation accuracy). The second, in order to
characterize the speed of training, is the number of steps required to reach a fixed validation target
conditional on at least one trial in the search having reached that target. We chose the target for
each workload based on initial experiments and known values from the literature (see Table 2). We
estimated means and uncertainties using the bootstrap procedure described in Appendix C.
4.2	Inclusion relationships matter in practice
Figure 1 shows the final predictive performance of six optimizers on four different workloads after
tuning hyperparameters to minimize validation error. Regardless of whether we compare final val-
idation error or test error, the inclusion relationships hold in all cases - a more general optimizer
never underperforms any of its specializations within the error bars. Similar results hold for training
error (see Figure 9 in Appendix E). Training speed is also an important consideration, and Fig-
ure 2 demonstrates that the inclusion relationships also hold within error bars when we compare the
number of steps required to reach a target validation error. Moreover, these results confirming the
relevance of optimizer inclusion relationships do not depend on the exact step budgets or error tar-
gets we chose (see Figure 10 in Appendix E), although large changes to these values would require
new experiments.
2Although we used a budget of tens of independent tuning trials throughout this section, in retrospect the
best validation error across tuning trials converged quite quickly for our final search spaces, producing good
results with fewer than 20 trials in many cases. See Figures 6- 8 in Appendix E.
6
Under review as a conference paper at ICLR 2020
Simple CNN on Fashion MNIST ResNet-32 on CIFAR-IO
*7κ ,	C non
0.075
0.070
0.065
0.060
ResNet-50 on ImageNet
Transformer on LMlB
I ⅜ SGD ⅛ Momentum ⅛ Nesterov ⅛ RMSProp ⅛ Adam ⅛ NAdam I
Figure 1:	The relative performance of optimizers is consistent with the inclusion relationships, re-
gardless of whether we compare final validation error (top) or test error (bottom). For all workloads,
we tuned the hyperparameters of each optimizer separately, and selected the trial that achieved the
lowest final validation error.
Transformer on LMlB
×105
Simple CNN on Fashion MNIST
n×ιo4
ResNet-32 on CIFAR-IO
xlθ4
ResNet-50 on ImageNet
xlθ5
I ⅛ SGD ⅛ Momentum ⅛ Nesterov ⅛ RMSProp ⅛ Adam ⅛ NAdam I
Figure 2:	The relative training speed of optimizers is consistent with the inclusion relationships.
We measured (idealized) training speed as the number of training steps required to reach a target
validation error (see Table 2 for the error targets).
Of course, just because a more general optimizer is no worse than any of its specializations doesn’t
mean the choice of optimizer makes a large difference on all workloads. For some workloads in Fig-
ures 1 and 2, all optimizers perform about the same, while other workloads have a clear ranking or
even dramatic differences. For example, the choice of optimizer seems to make little difference for
ResNet-32 on CIFAR-10; all optimizers achieve similar predictive performance and training speed.
On the other hand, Transformer on LM1B exhibits a clear ranking in terms of predictive perfor-
mance and training speed. For this workload, Adam needs roughly half the steps that Momentum
requires to reach our target error, and, although not shown in Figure 2, roughly six times fewer
steps to get the same result as SGD. These differences are clearly significant enough to matter
to a practitioner, and highlight the practical importance of choosing the right optimizer for some
workloads.
The most general optimizers we considered were RMSProp, Adam, and NAdam, which do not
include each other as special cases, and whose relative performance is not predicted by inclusion
relationships. Across the workloads we considered, none of these optimizers emerged as the clear
winner, although Adam and NAdam generally seemed to have an edge over RMSProp. For all
of these optimizers, we sometimes had to set the parameter orders of magnitude larger than the
default value in order to get good results. In particular, we achieved a validation accuracy of 77.1%
for ResNet-50 on ImageNet using NADAM with = 9475, a result that exceeds the 76.5% achieved
by Goyal et al. (2017) using Momentum. Across just these 4 workloads, the range of the optimal
values of the parameter spanned 10 orders of magnitude. Faced with this reality, a practitioner
might reasonably doubt their ability to find a value near the optimum. However, we found that we
could reasonably expect to find a suitable value with only tens of trials. When tuning for ADAM
or NADAM over a large range, we found it more efficient to search over (, α0/) instead of (, α0);
see Appendix D for more details.
7
Under review as a conference paper at ICLR 2020
Figure 3: Tuning more hyperparameters removes the differences in test error between optimizers
observed by Wilson et al. (2017). Tuning a subset of optimizer hyperparameters and the initial
learning rate is sufficient to equalize performance between all optimizers (left). More extensive
hyperparameter tuning in our setup, including the learning rate schedule, improves results for all
optimizers and still does not produce any differences between optimizer performances (right).
4.3 Reconciling disagreements with previous work
In order to confirm that differences in hyperparameter tuning protocols explain the differences be-
tween our conclusions and those of Wilson et al. (2017) and Schneider et al. (2019), we reproduced
a representative subset of their results and then inverted, or at least collapsed, the ranking over
optimizers just by expanding the hyperparameter search space.
The left pane of Figure 3 shows our experiments on VGG on CIFAR-10 using code released by
Wilson et al. (2017). When we match their protocol and perform their grid search over the initial
learning rate and no other tuning, we reproduce their original result showing worse test error for
RMSPROP and ADAM. However, when we tune the momentum parameter and with random
search, all four optimizers reach nearly identical test error rates.3 With our learning rate schedule
search space, merely tuning the learning rate schedule was enough to make all optimizers reach the
same test error within error bars. When we additionally tuned the optimization hyperparameters and
weight decay in our setup we also get similar results for all optimizers, removing any evidence the
inclusion relationships might be violated in practice.
Figure 4 shows our results with different tuning protocols for a CNN on CIFAR-100 and an LSTM
language model trained on War and Peace to match the experiments in Schneider et al. (2019).
As reported by Schneider et al. (2019), if we only tune the learning rate without tuning the decay
schedule or other optimizer hyperparameters, Adam does worse than Momentum for the CNN
and SGD performs slightly better than ADAM and MOMENTUM on the War and Peace dataset,
although Schneider et al. (2019) found a larger advantage for SGD. However, once we tune the all
the optimizer hyperparameters, Adam does better than Momentum which does better than SGD,
as predicted by the inclusion relationships.
We conclude that the reason both Schneider et al. (2019) and Wilson et al. (2017) observed a ranking
that, at first glance, contradicts the inclusion relationships is because they were not tuning enough
of the hyperparameters. If we recast their results in our terminology where Adam with default
is a different optimizer than ADAM with tuned then there is no contradiction with our results
and it becomes clear immediately that they do not consider the most interesting form of Adam for
practitioners.
3Wilson et al. (2017) selected trials to minimize the training loss and then report test set results. As Figure 3
shows, removing this somewhat non-standard choice and tuning on a validation set and reporting test set results
does not change anything.
8
Under review as a conference paper at ICLR 2020
CNN on CIFAR-100	LSTM on War and Peace
+ {%Al,a,e}	+ {7,A,⅛,e)	+ {l,β1,β2,e}
I ≠ SGD~~≠ Momentum^~≠ Adarnl
Figure 4: Tuning more hyperparameters changes optimizer rankings from Schneider et al. (2019)
to rankings that are consistent with the inclusion relationships. The leftmost columns for each
workload reproduce the rankings from Schneider et al. (2019), while the remaining columns tune
over increasingly general search spaces. All columns use our random search tuning protocol.
5 Conclusions
Inspired by the recent efforts of Wilson et al. (2017) and Schneider et al. (2019), we set out to
provide a detailed empirical characterization of the optimizer selection process in deep learning. Our
central finding is that inclusion relationships between optimizers are meaningful in practice. When
tuning all available hyperparameters under a realistic protocol at scales common in deep learning,
we find that more general optimizers never underperform their special cases. In particular, we found
that RMSProp, Adam, and NAdam never underperformed SGD, Nesterov, or Momentum
under our most exhaustive tuning protocol. We did not find consistent trends when comparing
optimizers that could not approximate each other. We also found workloads for which there was not
a statistically significant separation in the optimizer ranking.
Our experiments have some important limitations and we should be careful not to overgeneralize
from our results. The first major caveat is that we did not measure the effects of varying the batch
size. Recent empirical work (Shallue et al., 2019; Zhang et al., 2019) has shown that increasing
the batch size can increase the gaps between training times for different optimizers, with the gap
from SGD to Momentum (Shallue et al., 2019) and from Momentum to Adam (Zhang et al.,
2019) increasing with the batch size. Nevertheless, we strongly suspect that the inclusion relations
would be predictive at any batch size under a tuning protocol similar to the one we used. The
second important caveat of our results is that they inevitably depend on the tuning protocol and
workloads that we considered. Although we made every attempt to conduct realistic experiments,
we should only expect our detailed findings to hold for similar workloads under similar protocols,
namely uniform quasi-random tuning for tens to hundreds of trials, over hypercube search spaces,
and with our specific learning rate schedule parameterization. Nevertheless, these caveats reinforce
our central point: all empirical comparisons of neural network optimizers depend heavily on the
hyperparameter tuning protocol, perhaps far more than we are used to with comparisons between
model architectures.
If we were to extract “best practices” from our findings, then we suggest the following. If we can
afford tens or more runs of our code, we should tune all of the hyperparameters of the popular
adaptive gradient methods. Just because two hyperparameters have a similar role in two different
update rules doesn’t mean they should take similar values— optimization hyperparameters tend to
be coupled and the optimal value for one may depend on how the others are set. Our results also
confirm that the optimal value of Adam’s is problem-dependent, so the onus is on empirical studies
that fix = 10-8 to defend that choice. Finally, we should be skeptical of empirical comparisons
of optimizers in papers, especially if an optimizer underperforms any of its specializations. When
we do inevitably compare optimizers, we should report search spaces and highlight decisions about
what hyperparameters were tuned when interpreting results.
9
Under review as a conference paper at ICLR 2020
References
Leonard Adolphs, Jonas Kohler, and Aurelien Lucchi. Ellipsoidal trust region methods and
the marginal value of Hessian information for neural network training. arXiv preprint
arXiv:1905.09201, 2019.
Laurence Aitchison. A unified theory of adaptive stochastic gradient descent as Bayesian filtering.
arXiv preprint arXiv:1807.07540, 2018.
Lukas Balles and Philipp Hennig. Dissecting Adam: The sign, magnitude and variance of stochastic
gradients. arXiv e-prints, art. arXiv:1705.07774, May 2017.
S Becker and Y Le Cun. Improving the convergence of the backpropagation learning with second
order methods. Morgan Koufmann, San Mateo, CA, 1988.
James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. Journal of
Machine Learning Research,13(Feb):281-305, 2012.
Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Anima Anandkumar. signSGD:
Compressed optimisation for non-convex problems. arXiv preprint arXiv:1802.04434, 2018.
Olivier Bousquet, Sylvain Gelly, Karol Kurach, Olivier Teytaud, and Damien Vincent. Critical
hyper-parameters: no random, no cry. arXiv preprint arXiv:1706.03200, 2017.
Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony
Robinson. One billion word benchmark for measuring progress in statistical language modeling.
In Conference of the International Speech Communication Association, 2014.
Jing Chen, Liu Zhao, Xue Qiao, and Yang Fu. NAMSG: An efficient method for training neural
networks. arXiv preprint arXiv:1905.01422, 2019.
Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong. On the convergence ofa class of Adam-type
algorithms for non-convex optimization. arXiv preprint arXiv:1808.02941, 2018.
Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment:
Learning augmentation policies from data. In Conference on Computer Vision and Pattern Recog-
nition, 2018.
Soham De, Anirbit Mukherjee, and Enayat Ullah. Convergence guarantees for RMSProp and
ADAM in non-convex optimization and an empirical comparison to Nesterov acceleration. arXiv
preprint arXiv:1807.06766, 2018.
Timothy Dozat. Incorporating Nesterov momentum into Adam. In ICLR Workshops, 2016.
Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noordhuis, LUkasz Wesolowski, AaPo Kyrola, An-
drew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: training ImageNet
in 1 hour. arXiv preprint arXiv:1706.02677, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. DeeP residual learning for image recog-
nition. In Conference on Computer Vision and Pattern Recognition, Pages 770-778. IEEE, 2016a.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity maPPings in deeP residual
networks. In European Conference on Computer Vision, Pages 630-645. SPringer, 2016b.
M Hessel, J Modayil, andH van Hasselt. Rainbow: Combining imProvements in deeP reinforcement
learning. 2017. arXiv preprint arXiv:1710.02298, 2017.
Sepp Hochreiter and JUrgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):
1735-1780, 1997.
Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the generaliza-
tion gap in large batch training of neural networks. In Advances in Neural Information Processing
Systems, pages 1731-1741, 2017.
10
Under review as a conference paper at ICLR 2020
Sergey Ioffe and Christian Szegedy. Batch normalization: accelerating deep network training by
reducing internal covariate shift. In International Conference on Machine Learning, pages 448-
456, 2015.
Nitish Shirish Keskar and Richard Socher. Improving generalization performance by switching from
Adam to SGD. arXiv preprint arXiv:1712.07628, 2017.
Diederik P Kingma and Jimmy Ba. Adam: a method for stochastic optimization. In ICLR, 2015.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical re-
port, University of Toronto, 2009. URL http://www.cs.toronto.edu/~kriz/
learning-features-2009-TR.pdf.
Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei
Han. On the variance of the adaptive learning rate and beyond. arXiv preprint arXiv:1908.03265,
2019.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: a robustly optimized BERT pretrain-
ing approach. arXiv e-prints, art. arXiv:1907.11692, Jul 2019.
Ilya Loshchilov and Frank Hutter. Fixing weight decay regularization in Adam. arXiv preprint
arXiv:1711.05101, 2017.
Liangchen Luo, Yuanhao Xiong, Yan Liu, and Xu Sun. Adaptive gradient methods with dynamic
bound of learning rate. arXiv preprint arXiv:1902.09843, 2019.
Jerry Ma and Denis Yarats. Quasi-hyperbolic momentum and Adam for deep learning. arXiv
preprint arXiv:1810.06801, 2018.
James Martens and Roger Grosse. Optimizing neural networks with Kronecker-factored approxi-
mate curvature. arXiv preprint arXiv:1503.05671, page 58, 2015.
Yurii Nesterov. Lectures on convex optimization, volume 137. Springer, 2018.
Yurii E Nesterov. A method for solving the convex programming problem with convergence rate
O(1∕k^2). In DokL akad. nauk Sssr, volume 269, pages 543-547,1983.
Boris T Polyak. Some methods of speeding up the convergence of iteration methods. USSR Com-
putational Mathematics and Mathematical Physics, 4(5):1-17, 1964.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of Adam and beyond. In
ICLR, 2019.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathemat-
ical Statistics, 22(3):400-407, 1951.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. ImageNet large scale visual
recognition challenge. International Journal of Computer Vision, 115(3):211-252, 2015.
Frank Schneider, Lukas Balles, and Philipp Hennig. DeepOBS: a deep learning optimizer bench-
mark suite. arXiv preprint arXiv:1903.05499, 2019.
Christopher J. Shallue, Jaehoon Lee, Joseph Antognini, Jascha Sohl-Dickstein, Roy Frostig, and
George E. Dahl. Measuring the effects of data parallelism on neural network training. Journal of
Machine Learning Research, 20(112):1-49, 2019.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Leslie N. Smith. A disciplined approach to neural network hyper-parameters: Part 1 - learning rate,
batch size, momentum, and weight decay. arXiv e-prints, art. arXiv:1803.09820, Mar 2018.
11
Under review as a conference paper at ICLR 2020
Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. Practical Bayesian optimization of machine
learning algorithms. In Proceedings of the 25th International Conference on Neural Information
Processing Systems - Volume 2, NIPS’12, pages 2951-2959, USA, 2012. Curran Associates Inc.
Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. Striving for
simplicity: the all convolutional net. arXiv preprint arXiv:1412.6806, 2014.
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initializa-
tion and momentum in deep learning. In ICML, 2013.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethink-
ing the inception architecture for computer vision. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 2818-2826, 2016.
Mingxing Tan and Quoc V Le. Efficientnet: Rethinking model scaling for convolutional neural
networks. arXiv preprint arXiv:1905.11946, 2019.
Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, and
Quoc V Le. Mnasnet: Platform-aware neural architecture search for mobile. In Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition, pages 2820-2828, 2019.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-RMSProp: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26-
31, 2012.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor-
mation Processing Systems, pages 5998-6008, 2017.
Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal
value of adaptive gradient methods in machine learning. In Advances in Neural Information
Processing Systems 30, pages 4148-4158. Curran Associates, Inc., 2017.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST: a novel image dataset for bench-
marking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.
Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, and Sanjiv Kumar. Adaptive meth-
ods for nonconvex optimization. In Advances in Neural Information Processing Systems 31, pages
9793-9803. Curran Associates, Inc., 2018.
Guodong Zhang, Lala Li, Zachary Nado, James Martens, Sushant Sachdeva, George E. Dahl,
Christopher J. Shallue, and Roger Grosse. Which algorithmic choices matter at which batch
sizes? insights from a noisy quadratic model. arXiv e-prints, art. arXiv:1907.04164, Jul 2019.
Jian Zhang and Ioannis Mitliagkas. Yellowfin and the art of momentum tuning. arXiv preprint
arXiv:1706.03471, 2017.
Zhiming Zhou, Qingru Zhang, Guansong Lu, Hongwei Wang, Weinan Zhang, and Yong Yu.
Adashift: Decorrelation and convergence of adaptive learning rate methods. arXiv preprint
arXiv:1810.00143, 2018.
Fangyu Zou and Li Shen. On the convergence of weighted AdaGrad with momentum for training
deep neural networks. arXiv preprint arXiv:1808.03408, 2018.
Fangyu Zou, Li Shen, Zequn Jie, Weizhong Zhang, and Wei Liu. A sufficient condition for conver-
gences of Adam and RMSProp. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 11127-11135, 2019.
12
Under review as a conference paper at ICLR 2020
A Optimizer inclusions
Table 1 summarizes the update rules for the optimizers we consider in this work. We assume update
rules as implemented in TensorFlow r1.15. RMSProp includes momentum. Here we prove the
their inclusion relationships, see Definition 1.
Momentum can exactly implement SGD
MOMENTUM(It, ηt, 0) = SGD(It, ηt), so SGD ⊆ MOMENTUM.
Nesterov can exactly implement SGD
NESTEROV(It, ηt, 0) = SGD(It, ηt), so SGD ⊆ NESTEROV.
RMSProp with momentum can exactly implement Momentum
Consider RMSPROP(It, ηt, γ, ρ = 1, = 0), so that
mt+1 = Ymt + ηtV'(θt),
θt+1 = θt - mt+1 .
This is equivalent to Momentum, since
(RMSProp)	(Momentum)
mt+1	≡ ηtvt+1	.
Thus RMSPROP(It, ηt, γ, 1, 0) = MOMENTUM(It, ηt, γ), so MOMENTUM ⊆ RMSPROP.
RMSterov can exactly implement Nesterov
Consider RMSTEROV(It, ηt, γ, ρ = 1, = 0), so that
mt+1 = Ymt + ηtV'(θt),
θt+ι = θt — [γmt+ι + ηtV'(θt)].
This is equivalent to Momentum, since
(RM SProp)	(Nesterov)
mt+1	≡ ηtvt+1	.
Thus RMSTEROV(It, ηt, Y, 1, 0) = MOMENTUM(It, ηt, Y), so MOMENTUM ⊆ RMSTEROV.
Adam can approximate Momentum for large
Consider ADAM(It, αt = ηt(1 - Yt), β1 = Y, β2 = 0, ), so that
mt+1 = Ymt + (1 一 Y)V'(θt),
θt+1 = θt — (1 -t Y) - ∣V'(θtt+∕e +1
If e is large, so that ∣V'(θt)∣∕e《1, then
mt+1 = Ymt + (1 — Y)V'(θt),
θt+1
θt — ηt
mt+1
1 - Y
This is equivalent to Momentum, since
m(ADAM) ≡ (1 — Y) v(MOMENTUM) .
Thus lim→∞ ADAM(It, ηt(1 — Yt), Y, 0, ) = MOMENTUM(It, ηt, Y), so MOMENTUM ⊆ ADAM
NAdam can approximate Nesterov for large
Consider NADAM(It, αt = ηt(1 — Yt), β1 = Y, β2 = 0, ), so that
mt+1 = Ymt + (1 — Y)V'(θt),
θt+1 = θt — (⅛) _
Ymt+1 + (1 — Y )V'(θt)
∣V'(θt)∣∕e +1
13
Under review as a conference paper at ICLR 2020
If e is large, so that ∣V'(θt)∣∕e《1, then
mt+1 = Ymt + (1- Y)V'(θt),
θt+ι = θt- ηt [γmt+1 + V'(θt).
1-Y
This is equivalent to Nesterov, since
m(NADAM) ≡ (1 - Y) v(NESTEROV)
Thus lim→∞ NADAM(It, ηt(1 - Yt), Y, 0, ) = NESTEROV(It, ηt, Y), so NESTEROV ⊆
NAdam.
B Workload details
This section details the datasets and models summarized in Table 2.
B.1	Dataset Descriptions
For Fashion MNIST, CIFAR-10, ImageNet, and LM1B, our setup was identical to Shallue et al.
(2019) except for the image pre-processing details described below. For War and Peace, our setup
was identical to the “Tolstoi” dataset of Schneider et al. (2019).
CIFAR-10/100: We pre-processed images by subtracting the average value across all pixels and
channels and dividing by the standard deviation.4 For experiments with the ResNet-32 and CNN
models, we followed the standard data augmentation scheme used in He et al. (2016a): 4 pixels
padded on each side with single random crop from padded image or its horizontal reflection. We did
not use random cropping for experiments with VGG for consistency with Wilson et al. (2017).
ImageNet: We augmented images at training time by resizing each image, taking a random crop of
224 × 224 pixels, randomly horizontally reflecting the cropped images, and randomly distorting the
image colors. At evaluation time, we performed a single central crop of 224 × 224 pixels. In both
training and evaluation, we then subtracted the global mean RGB value from each pixel using the
values computed by Simonyan and Zisserman (2014).5
B.2	Model Descriptions
Simple CNN is identical to the base model described in Shallue et al. (2019). It consists of 2
convolutional layers with max pooling followed by 1 fully connected layer. The convolutional layers
use 5 × 5 filters with stride 1, “same” padding, and ReLU activation function. Max pooling uses a
2 × 2 window with stride 2. Convolutional layers have 32 and 64 filters each and the fully connected
layer has 1024 units. It does not use batch normalization.
CNN is the “All-CNN-C” model from Springenberg et al. (2014), as used in Schneider et al. (2019).
The model consists of 3 convolutional layer blocks with max pooling. The convolutional layers use
5 × 5 filters with stride 1, “same” padding, and ReLU activation function. Max pooling uses a 2 × 2
window with stride 2. Convolutional layer blocks have 96, 192 and 192 filters each. As in Schneider
et al. (2019), we used L2 regularization of 5 × 10-4.
ResNet is described in He et al. (2016a). We used the improved residual block described in He et al.
(2016b). We used batch normalization (Ioffe and Szegedy, 2015) with exponential moving average
(EMA) decay of 0.997 for ResNet-32, and ghost batch normalization (Hoffer et al., 2017) with ghost
batch size of 32 and EMA decay of 0.9 for ResNet-50.
VGG is based on “model C” from Simonyan and Zisserman (2014). It consists of 13 convolutional
layers followed by 3 fully connected hidden layers. We followed the modification used by Wilson
et al. (2017) with batch normalization layers.
4We used the TensorFlow op tf.image.per_image_Standardization.
5See https://gist.github.com/ksimonyan/211839e770f7b538e2d8#description
for the mean RGB values used.
14
Under review as a conference paper at ICLR 2020
LSTM is a two hidden-layer LSTM model (Hochreiter and Schmidhuber, 1997) identical to the
model used in Schneider et al. (2019). It uses 128 embedding dimensions and 128 hidden units.
Transformer is the “base” model described in (Vaswani et al., 2017). We used it as an autoregressive
language model by applying the decoder directly to the sequence of word embeddings for each
sentence. Unlike the default implementation, we removed dropout regularization and used separate
weight matrices for the input embedding layer and the pre-softmax linear transformation, as we
observed these choices led to better performing models.
C Estimating trial outcomes via bootstrap
Our tuning protocol corresponds to running trials with quasi-random hyperparameter values sampled
uniformly from the search space until K feasible trials are obtained, with K depending on the
workload. We then select the best trial, based on our statistic of interest, over those K trials.
We used the following bootstrap procedure to estimate means and uncertainties of our tuning proto-
col. We ran N > K trials, with N depending on the workload. Then, for each bootstrap sample,
we resampled the dataset of N trials with replacement and computed our statistic on the first K
trials of the resampled dataset. We collected 100 such bootstrap samples each time, and from those
computed the means, 5th percentiles, and 95th percentiles of the bootstrap distribution. We used this
procedure to generate the means and error bars for each plot.
Simple CNN on Fashion MNIST used (K, N) = (100, 500); ResNet-32 on CIFAR-100 used
(K, N) = (100, 500); ResNet-50 on ImageNet used (K, N) = (50, 250); Transformer on LM1B
used (K, N) = (50, 100); VGG on CIFAR-10 with our code used (K, N) = (50, 250) for tuning the
learning rate schedule and (K, N) = (100, 500) for tuning the learning rate schedule, {γ, β1, β2, },
and L2 regularization; CNN on CIFAR-10 used (K, N) = (100, 500); LSTM on War and Peace
used (K, N) = (10, 50) for tuning just the learning rate and (K, N) = (100, 500) for tuning the
learning rate schedule and {γ, β1, β2, }.
The sole exceptions to this bootstrap procedure are the two left panels of Figure 3, for which we used
a similar procedure to Wilson et al. (2017) to ensure comparability. For each optimizer, we selected
the trial that minimized validation error in our final search space and ran the same hyperparameter
values 5 times, reporting the mean, minimum, and maximum test error over those 5 runs in Figure 3.
This is slightly different to Wilson et al. (2017), who chose the trial that minimized training error and
reported validation error. When tuning the learning rate and {γ, }, we used 24 trials per optimizer
in the initial search space (which we used to select the final search space), and 16 trials per optimizer
in the final search space.
D hyperparameter Search Spaces
When tuning hyperparameters over a large range, we found that our search could sometimes be made
more efficient if we parametrized the search space in a way that decorrelated the axes of the space.
For example, with Momentum and Nesterov we observed a clear relationship between the initial
learning rate η0 and the momentum parameter γ; smaller values of η0 require larger values of γ for
good performance, and vice versa. Indeed, Shallue et al. (2019) suggested that these optimizers are
governed by the “effective learning rate” nɛff = ηo∕(1 - γ), and inspired by this, We found that
searching over (η0, ηeff) instead of (η0, γ) usually led to a more efficient hyperparameter search.
Similarly, With ADAM and NADAM We observed a relationship betWeen the initial learning rate α0
and the parameter; larger values of α0 require larger values of for good performance, and vice
versa. This is not surprising given the analysis in Appendix A that shoWed that, for large , α0∕
is analogous to the effective learning rate of Adam and NAdam. We found that searching over
(, α0∕) Was usually more efficient than searching over (, α0). We used these techniques in a
subset of our experiments.
BeloW We report the search spaces used for our experiments. We include both the initial search
spaces used to refine the search spaces, and the final spaces used to generate the plots. When only
one search space Was used, We denote the initial space as final. η0, α0, 1 - γ, 1 - β1, 1 - β2, ,
and combinations thereof are alWays tuned on a log scale. The number of samples from each search
space is specified in Appendix C.
15
Under review as a conference paper at ICLR 2020
D.1 CNN ON FASHION MNIST
We used linear learning rate decay for all experiments. We tuned the number of decay steps
within [0.5, 1.0] times the number of training steps and the learning rate decay factor within
{10-3, 10-2, 10-1}. We did not use L2 regularization or weight decay.
	no
initial	[10-2,102 ]
final	[10-2,101 ]
Table 3: SGD
	no	1 - Y
initial	[10-4,102]	[10-4,1]
final	[10-5,101]	[10-4,1]
Table 4: Momentum
	no	1 - Y
initial	[10-4,102]	[10-4,1]
final	[10-4,101]	[10-4,1]
Table 5: Nesterov
	no	1 - Y	1 - P	€
initial	[10-4, 101]	[10-2,1]	[10-4,1]	[10-5,101]
final	[10-5,1]	[10-2,1]	[10-3,1]	[10-1o,10-5]
Table 6: RMSPROP
	αo	1 - β1	1 - β2	€
initial	[10-4,10-1]	[10-3, 5 × 10-1]	[10-4, 10-1]	[10-9,10-5]
final	[10-5,10-1]	[10-3,1]一	[10-4,1]	[10-1o,10-5]
Table 7: ADAM
	αo/	1 - β1	1 - β2	€
initial	[10-2,104]	[10-3,1]	[10-4,1]	[10-1o, 1010]
final	[10-1, 101]	[10-3,1]	[10-4,1]	[10-6,10-2]
Table 8: NADAM
16
Under review as a conference paper at ICLR 2020
D.2 RESNET-32 ON CIFAR- 1 0
We used linear learning rate decay for all experiments. We tuned the number of decay steps within
[0.5, 1.0] times the number of training steps and the learning rate decay factor f within the values
shown in the tables below. λL2 denotes the L2 regularization coefficient.
	no		λL2			f	
final	[10-2,102]	{10-5,10-4,10-3,10-2}	{10-4,10-3,10-2,10-1}
Table 9: SGD
	no	1 - Y		λL2			f	
final	[10-4,102]	[10-3,1]	{10-5,10-4,10-3,10-2}	{10-4,10-3,10-2,10-1}
Table 10: Momentum
	no	1-γ		λL			f	
initial	[10-4,102]	[10-4, 101]		10-4		{10-3,10-2,10-1}
final	[10-4,101]	[10-4,1]	{10-5,10-4,10-3,10-2}	{10-4,10-3,10-2,10T }
Table 11: Nesterov
	no	1 - Y	1 - P	€	λL2	f
initial	[10-4, 101]	[10-2,1]	[10-4,1]	[10-5,101]	10-4	{10-3,10-2,10-1}
final	[10-4, 101]	[10-3,1]	[10-4, 1]	[10-5,101]	{10-5,10-4 10-3,10-2}	{10-4,10-3 10-2,10-1}
Table 12: RMSProp
	αo	1 - β1	1 — β2	€	λL2	f
initial	[10-4, 10-1]	[10-3, 5 × 10-1]	[10-4,10-1]	[10-9,10-5]	10-4	{10-3,10-2 10-1}
final	[10-3, 101]	[10-3,1]	[10-4,10-1]	[10-5,101]	{10-5,10-4 10-3,10-2}	{10-4,10-3 10-2,10-1}
Table 13: ADAM
	ɑo/	1 - β1	1 — β2	€	λL2	f
initial	[10-2, 104]	[10-3,1]	[10-4,1]	[10-1o,101o]	{10-5,10-4 10-3,10-2}	{10-4,10-3 10-2,10-1}
final	[10-2,1]	[10-3,1]	[10-4,1]	[1,104]	{10-5,10-4 10-3,10-2}	{10-4,10-3 10-2,10-1}
Table 14: NADAM
17
Under review as a conference paper at ICLR 2020
D.3 ResNet-50 on ImageNet
We used linear learning rate decay for all experiments. We tuned the number of decay steps within
[0.5, 1.0] times the number of training steps and the learning rate decay factor f within the val-
ues shown in the tables below. λwd denotes the weight decay coefficient and τ denotes the label
smoothing coefficient.
	no	λwd	τ		f	
initial	[10-2,101]	[10-5,10-2]	{0,10-2,10-1}	{10-4,10-3,10-2,10-1}
final	[1,102 ]	[10-4,10-3]	10-1	{10-4,10-3,10-2,10-1}
Table 15: SGD
	no	1 - Y	λwd	T		f	
initial	[10-3,1]	[10-3,1]	[10-5,10-2]	{0,10-2,10-1}	{10-4,10-3,10-2,10-1}
final	[10-2,1]	[10-2,1]	[10-4,10-3]	10-2	{10-4,10-3,10-2,10-1}
Table 16: Momentum
	no	1 - Y	λwd	T		f	
initial	[10-3,1]	[10-3,1]	[10-5,10-2]	{0,10-2,10-1}		10-3	
final	[10-2,1]	[10-3,1]	[10-4,10-3]		0		{10-4,10-3,10-2,10-1}
Table 17: Nesterov
	n7√	1 - Y	1 - P	€	λwd	T	f
initial	[10-2,104]	0.1	[10-4,1]	[10-1o,1010]	[10-5,10-2]	{0,10-2,10-1}	10-3
final	[10-2,1]	0.1	[10-2,1]	[10-8,10-3]	[10-4,10-3]	0	{10-4,10-3 10-2,10-1}
Table 18: RMSProp
	ao/	1 - β1	€	λwd	T	f
initial	[1,102]	[10-3,1]	[1,104]	[10-5,10-3]	{0,10-2,10-1}	10-3
final	[1,102]	[10-2, 1]	[10-2,102]	10-4	10-1	{10-4,10-3 10-2,10-1}
Table 19: ADAM
	ao/	1 - β1	€		λwd		T	f
initial	[10-1,103]	[10-3,1]	[10-2,1010]	[10-5,10-2]	{0,10-2,10-1}	10-3
final	[1,102]	[10-3,1]	[103,107]	10-4	10-1	10-3
Table 20: NADAM
18
Under review as a conference paper at ICLR 2020
D.4 Transformer on LM1B
We used linear learning rate decay for all experiments. We tuned the number of decay steps
within [0.5, 1.0] times the number of training steps and the learning rate decay factor within
{10-4, 10-3, 10-2, 10-1, 1}.
	no
final	[10-4, 10-1]
Table 21: SGD
	αo	1 - β1	1 — β2	€
initial	[10-5, 10-2]	[10-3, 5 X 10-1]	[10-4,10-1]	[10-9,10-5]
final	[10-4, 10-2]	[10-3,1]一	[10-5,10-1]	[10-7,10-2]
Table 25: ADAM
	αo	1 - β1	1 — β2	€
final	[10-5,10-2]	[10-3,1]	[10-5,10-1]	[10-9,10-5]
Table 26: NADAM
19
Under review as a conference paper at ICLR 2020
D.5 VGG ON CIFAR- 1 0 USING CODE FROM WILSON ET AL. (2017)
D.5.1 Grid Search over Learning Rate
We tuned over the same grid of initial learning rate values for each optimizer as Wilson et al. (2017).
As in Wilson et al. (2017), we decayed the initial learning rate by a factor of 0.5 every 25 epochs
and used a fixed L2 regularization coefficient of 0.0005.
D.5.2 TUNING LEARNING RATE & {γ, }
We used our quasi-random tuning protocol to tune over the initial learning rate, MOMENTUM’s γ,
RMSPROP’s , and ADAM’s . As in Wilson et al. (2017), we decayed the initial learning rate by a
factor of 0.5 every 25 epochs and used a fixed L2 regularization coefficient of 0.0005.
____________no
initial	[10-3,1]
final	[10-1,101 ]
Table 27: SGD
	no	1 - Y
initial	[10-3,1]	[10-3,1]
final	[10-1,101]	[10-1,1]
Table 28: Momentum
	€	α0 √
initial	[10-1o, 1010]	[10-2, 104]
final	[10-2, 102]	[10-1, 101]
Table 29: RMSPROP
	€	ao/
initial	[10-1o, 1010]	[10-2,104]
final	[106,1010]	[10-1,101]
Table 30: ADAM
20
Under review as a conference paper at ICLR 2020
D.6 VGG ON CIFAR- 1 0 USING OUR CODE
We used linear learning rate decay for all experiments. We tuned the number of decay steps
within [0.5, 1.0] times the number of training steps and the learning rate decay factor within
{10-4, 10-3, 10-2, 10-1}.
D.6.1 Tuning Learning Rate Schedule
We fixed all optimizer hyperparameters excluding the learning rate to match those specified in Wil-
son et al. (2017). As in Wilson et al. (2017), we used a fixed L2 regularization coefficient of 0.0005.
	η0 (SGD)	no (Momentum)	no (RMSPROP)	ao (Adam)
initial	[10-3,101]	[10-3,101]	[10-5,10-1]	[10-5,10-1]
final	1.0	[10-2,1]一	[10-4,10-2]	[10-5,10-1]
Table 31: Learning rate search ranges.
D.6.2 TUNING LEARNING RATE SCHEDULE & {γ, β1, β2, , λL2 }
	no	λL2
initial	[10-3, 101]	[10-5,10-2]
final	[10-2,1]	[10-3,10-1]
Table 32: SGD
	no	1 - Y	λL2
initial	[10-3, 101]	[10-3,1]	[10-5,10-2]
final	[10-2,1]	[10-1,1]	[10-3,10-1]
Table 33: Momentum
	α0∕√^	1-γ	1 - P		入L2
initial	[10-2,104]	[10-3,1]	[10-3, 1]	[10-1o, 101o]	[10-5,10-2]
final	[10-2,1]	[10-1,1]	[10-3,10-2]	[102,106]	[10-3,10-1]
Table 34: RMSPROP
	αo∕	1 - β1	1 - β2	€	λL2
initial	[10-2,104]	[10-3,1]	[10-4, 10-1]	[10-1o,101o]	[10-5,10-2]
final	[10-2, 101]	[10-1,1]	[10-4,10-1]	[106,1010]	[10-3,10-1]
Table 35: ADAM
21
Under review as a conference paper at ICLR 2020
D.7 CNN ON CIFAR- 1 00
D.7.1 Tuning Constant Learning Rate
We fixed all optimizer hyperparameters excluding the learning rate to match those specified in
Schneider et al. (2019).
	η (SGD)	η (Momentum)	a (Adam)
initial	[10-2,1]	[i0-4,1]-	[10-5,10-2]
final	[10-1,1]	[10-3,10-2]	[10-4,10-3]
Table 36: Learning rate search ranges.
D.7.2 Tuning Learning Rate Schedule
We used linear learning rate decay, and tuned the number of decay steps within [0.5, 1.0] times the
number of training steps and the learning rate decay factor within {10-4, 10-3, 10-2, 10-1}.
	no (SGD)	n0 (MOMENTUM)	ao (Adam)
initial	[10-2,1]	[10-4,1]一	[10-5,10-2]
final	[10-1,1]	[10-3,10-1]	[10-4,10-3]
Table 37: Learning rate search ranges.
D.7.3 TUNING CONSTANT LEARNING RATE & {γ, β1, β2, }
For SGD, we reused the results from Appendix D.7.1, since there were no additional hyperparame-
ters to tune.
	n	1 - Y
final	[10-4,1]	[10-3,1]
Table 38: Momentum
	a/e	1 - βι	1 — β2	e
initial	[10-2, 10-2]	[10-3,1]	[10-4,10-1]	[10-10,10-10]
final	[10-1,10-1]	[10-2,1]	[10-4,10-1]	[102,106]
Table 39: ADAM
D.7.4 TUNING LEARNING RATE SCHEDULE & {γ, β1, β2, }
We used linear learning rate decay, and tuned the number of decay steps within [0.5, 1.0] times the
number of training steps and the learning rate decay factor within {10-4, 10-3, 10-2, 10-1}. For
SGD, we reused the results from Appendix D.7.2, since there were no additional hyperparameters
to tune.
	n0	1 - Y
initial	[10-4,1]	[10-2,1]
final	[10-3,10-1]	[10-3,10-1]
Table 40: Momentum
22
Under review as a conference paper at ICLR 2020
	ao/	1 - β1	1 — β2	€
initial	[10-1,101]	[10-3,1]	[10-4,10-1]	[102,106]
final	[10-1,101]	[10-3,1]	[10-5,10-2]	[102,106]
Table 41: ADAM
23
Under review as a conference paper at ICLR 2020
D.8 LSTM ONWAR AND PEACE
D.8.1 Tuning constant Learning Rate
	η
final	[10-2, 101]
Table 42: SGD
	η	1 - Y
final	[10-4,1]	0.99
Table 43: Momentum
___________η 1 - Y
final [10-4,1] 0.99
	a/e	1 - β1	1 - β2	e
final	[10-5, 10-2]	0.9	0.999	10-8
Table 44: ADAM
D.8.2 TUNING LEARNING RATE SCHEDULE & {γ, β1, β2, }
We used linear learning rate decay, and tuned the number of decay steps within [0.5, 1.0] times the
number of training steps and the learning rate decay factor f within the values shown in the tables
below.
	no		f	
initial	[10-3,101]	{10-4,10-3,10-2,10-1}
final	[1,101 ]	{10-4,10-3,10-2,10-1}
Table 45: SGD
	no	1 - Y		f	
initial	[10-4,1]	[10-3,1]	{10-4,10-3,10-2,10-1}
final	[10-1,101]	[10-2,1]	{10-4,10-3,10-2,10-1}
Table 46: Momentum
	ao/e	1 - β1	1 — β2	e		f	
initial	[10-2,104]	[10-3,1]	[10-4,10-1 ]	[10-1o, 1010]	{10-4,10-3,10-2,10-1}
final	[1, 102]	[10-2,1]	0.999	[1,104]		10-3	
Table 47: ADAM
24
Under review as a conference paper at ICLR 2020
E Additional plots
CNN on CIFAR-100 with Momentum
0.54
0.52
0.50
0.48
0.46
0.44
0.42
IO0 IO-3
IO'2	IO_1
1 — 7
IO0
Learning rate
Figure 5: Example plot of final validation error projected onto the axes of the hyperparameter space.
We consider this search space to be appropriate because the optimal values are away from the search
space boundaries.
》 9 O
_■ 3 9
AdOJlU 山
SGD
Nesterov
Momentum
Adam
RMSProP
21	22	23	24	25	26	27
Number of IndependentTriaIs
Figure 6: Validation performance of the best trial mostly converges with as few as 24 hyperparameter
tuning trials for Transformer on LM1B. Shaded regions indicate 5th and 95th percentiles estimated
with bootstrap sampling (see Appendix C). The search spaces can be found in Appendix D.4.
Number of Independent Trials
NAdam
Number of Independent Trials
25
Under review as a conference paper at ICLR 2020
2。 21 2z 23 2* 25 26 27
Numberof IndependentTriaIs
2。 21 2z 23 2* 25 26 27	2。 21 22 23 24 25 26 27
Number of IndependentTriaIs	Number of IndependentTriaIs
Figure 7:	Validation performance of the best trial mostly converges with as few as 24 hyperparameter
tuning trials for ResNet-50 in ImageNet. Shaded regions indicate 5th and 95th percentile estimated
with bootstrap sampling (see Appendix C). The search spaces can be found in D.3.
SGD
20 21
22	23	24	25	26	27
Numberof IndependentTriaIs
0.65
0.60
0.55
0.50
0.45
0.40
Momentum
20 21
22	23	24	25	26	27
Number of IndependentTriaIs
0.65
0.60
0.55
0.50
0.45
0.40
Adam
20	21	22	23
24	25	26	27
Number of Independent Trials
Figure 8:	Test performance of the best trial mostly converges with as few as 23 hyperparameter
tuning trials for a 2-layer LSTM on War and Peace. Shaded regions indicate 5th and 95th percentile
estimated with bootstrap sampling (see Appendix C). The search spaces can be found in D.8.2.
ResNet-32 on CIFAR-IO
SSO^∣ U-E-IL
0.005
0.004
0.003
0.002
0.001
Simple CNN on Fashion MNIST
招 0-025
3
ResNet-50 on ImageNet
Transformer on LMlB
ω 3.40
O
ð 3.35
≡ 3.30
慢
I ⅜ SGD ⅛ Momentum ⅛ Nesterov ⅛ RMSProp ⅛ Adam ⅛ NAdam I
Figure 9: The relative performance of optimizers is consistent with the inclusion relationships when
we select for lowest training loss. Note that SGD, Adam, and NAdam for ResNet-50 on ImageNet
used label smoothing in their final search spaces (see Section D.3), which makes their loss values
incommensurate with the other optimizers. This is because their final search spaces were optimized
to minimize validation error—if we had optimized their search spaces to minimize training error
instead, we would not have used label smoothing, and we expect their training loss values would be
consistent with the inclusion relationships.
26
Under review as a conference paper at ICLR 2020
3.70
Transformer on LMlB
Validation Error	Validation Cross Entropy
5 Q
5 5
3.3.
二理 5 4Q
4.5 5.0 5.5 6.0 6.5 7.0 7.5
Step Budget	le5
8×1°5
6 5 4 3 2r
P-OilSə,llll-04 sdəs
0 3.60	3.55	3.50	3.45	3.40
Validation Cross Entropy Target
ResNet-32 on CIFAR-IO
-------	5.o×1°4
P-OllS əɪl-04 Sdφ4s
,0 0.080	0.075	0.070	0.065
Validation ErrorTarget
I~I SGD I~~I Momentum ⅛~+ Nesterov ⅛~I RMSProp i i Adam i i NAdam
10: Our results confirming the relevance of optimizer inclusion relationships do not depend
on
the exact step budgets or error targets we chose.
27