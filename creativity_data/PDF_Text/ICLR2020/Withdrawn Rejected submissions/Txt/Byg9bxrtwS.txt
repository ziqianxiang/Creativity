Under review as a conference paper at ICLR 2020
Kernel and Rich Regimes in
Overparametrized Models
Anonymous authors
Paper under double-blind review
Ab stract
A recent line of work studies overparametrized neural networks in the “kernel
regime,” i.e. when the network behaves during training as a kernelized linear pre-
dictor, and thus training with gradient descent has the effect of finding the mini-
mum RKHS norm solution. This stands in contrast to other studies which demon-
strate how gradient descent on overparametrized multilayer networks can induce
rich implicit biases that are not RKHS norms. Building on an observation by
Chizat and Bach (2018), we show how the scale of the initialization controls the
transition between the “kernel” (aka lazy) and “rich” (aka active) regimes and af-
fects generalization properties in multilayer homogeneous models. We provide a
complete and detailed analysis for a simple two-layer model that already exhibits
an interesting and meaningful transition between the kernel and rich regimes, and
we demonstrate the transition for more complex matrix factorization models and
multilayer non-linear networks.
1	Introduction
A string of recent papers study neural networks trained with gradient descent in the “kernel regime.”
They observe that, in a certain regime, networks trained with gradient descent behave as kernel meth-
ods (Jacot et al., 2018; Daniely et al., 2016; Daniely, 2017). This allows one to prove convergence
to zero error solutions in overparametrized settings (Du et al., 2018; 2019; Allen-Zhu et al., 2018),
and also implies gradient descent will converge to the minimum norm solution (in the correspond-
ing RKHS) (Chizat and Bach, 2018; Arora et al., 2019b; Mei et al., 2019) and more generally that
models inherit the inductive bias and generalization behavior of the RKHS. This suggests that, in a
certain regime, deep models can be equivalently replaced by kernel methods with the “right” kernel,
and deep learning boils down to a kernel method with a fixed kernel determined by the architecture
and initialization, and thus it can only learn problems learnable by some kernel.
This contrasts with other recent results that show how in deep models, including infinitely over-
parametrized networks, training with gradient descent induces an inductive bias that cannot be rep-
resented as an RKHS norm. For example, analytic and/or empirical results suggest that gradient
descent on deep linear convolutional networks implicitly biases toward minimizing the Lp bridge
penalty, for p = 2/depth ≤ 1, in the frequency domain (Gunasekar et al., 2018b); weight decay
on an infinite width single input ReLU implicitly biases towards minimizing the second order to-
tal variations |f 00 (x)| dx of the learned function (Savarese et al., 2019); and gradient descent on
a overparametrized matrix factorization, which can be thought of as a two layer linear network,
induces nuclear norm minimization of the learned matrix (Gunasekar et al., 2017) and can ensure
low rank matrix recovery (Li et al., 2018). All these natural inductive biases (Lp bridge penalty
for p < 1, total variation norm, nuclear norm) are not Hilbert norms, and therefore cannot be cap-
tured by a kernel. This suggests that training deep models with gradient descent can behave very
differently from kernel methods, and have much richer inductive biases.
One might then ask whether the kernel approximation indeed captures the behavior of deep learning
in a relevant and interesting regime, or does the success of deep learning come when learning escapes
this regime? In order to understand this, we must first carefully understand when each of these
regimes hold, and how the transition between the “kernel” regime and the “rich” regime happens.
Some investigations of the kernel regime emphasized the number of parameters (“width”) going to
infinity as leading to this regime. However Chizat and Bach (2018) identified the scale of the model
1
Under review as a conference paper at ICLR 2020
as a quantity controlling entry into the kernel regime. Their results suggest that for any number of
parameters (any width), a model can be approximated by a kernel when its scale at initialization
goes to infinity (see details in Section 3). Considering models with increasing (or infinite) width,
the relevant regime (kernel or rich) is determined by how the scaling at initialization behaves as the
width goes to infinity. In this paper we elaborate and expand of this view, carefully studying how
the scale of initialization effects the model behaviour for D-homogeneous models.
In Section 4 we provide a complete and detailed study for a simple 2-homogeneous model that can be
viewed as linear regression with squared parametrization, or as a “diagonal” linear neural network.
For this model we can exactly characterize the implicit bias of training with gradient descent, as a
function of the scale α of initialization, and see how this implicit bias becomes the `2 norm in the
α → ∞ kernel regime, but the `1 norm in the α → 0 rich regime. We can therefore understand
how, e.g. for a high dimensional sparse regression problem, where it is necessary to discover the
relevant features, we can get good generalization when the initialization scale α is small, but not
when α is large. Deeper networks corresponds to higher orders of homogeneity, and so in Section 5
we extend our study to a D-homogeneous model, studying the effects of D. In Sections 6 and 7, we
demonstrate similar transitions experimentally in matrix factorization and non-linear networks.
2	Setup and preliminaries
We consider models f : Rp × X → R which map parameters w ∈ Rp and examples x ∈ X to
predictions f(w, x) ∈ R. We denote the predictor implemented by the parameters w as hw = F(w)
such that hw (x) = f (w, x). Much of our focus will on models, such a linear networks, which are
linear in X (but not on the parameters w!), in which case F(W) ∈ X* is a linear predictor and
can be represented as a vector βw with f(w, x) = hβw, xi. Such models are essentially alternate
parametrizations of linear models, but as we shall see that change of parametrization is crucial.
In this paper, we consider models that are D-positive homogeneous in the parameters w, for some
integer D ≥ 1, meaning that for any C ∈ R+, F(C∙ w) = CDF(w) and f (C∙ w, x) = CD f (w, x). We
refer to such models simply as D-homogeneous. Many interesting model classes have this property,
including multi-layer ReLU networks with fully connected and convolutional layers, layered linear
neural networks, and matrix factorization where D corresponds to the depth of the network.
Consider a training set {(x(n), y(n) )}nN=1 consisting of N examples of input label pairs. For a
given loss function ` : R × R → R, the loss of the model parametrized by w is L(w) =
L(F(W)) = PN=I '(f (w, x(n)),y(n)). We will focus mostly on the squared loss 'sq(y,y)=
(y - y)2. We slightly abuse notation and use f (w, X) ∈ RN to denote the vector of predictions
[f (w, x(1)), . . . , f(w, x(N))] and so for the squared loss we can write L(w) = kf (w, X) - yk22,
where y ∈ RN is the vector of target labels.
Minimizing the loss L(w) using gradient descent amounts to iteratively updating the parameters
w(k + 1)= w(k) — ηVL(w(k)).	(1)
We consider gradient descent with infinitesimally small stepsize η, i.e. gradient flow dynamics
w(t) = -VL(w(t)).	(2)
We are particularly interested in the scale of initialization and we capture it through a scalar pa-
rameter α ∈ R+. For scale α, we will denote by wα (t) the gradient flow path (2) with the initial
condition wα(0) = αw0 for some fixed w0. We use hα(t) = F(wα(t)), and for linear predictors
βα(t) = βwα(t), to denote the dynamics on the predictor F(w) induced by the gradient flow on w.
In many cases, we expect the dynamics to converge to a minimizer of L(w), though proving this
happens will not be our main focus. Rather, we are interested in the underdetermined case, N p,
where there are generally many minimizers of L(w), all with f(w, X) = y and L(w) = 0. Our
main focus is which of the many minimizers does gradient flow converge to. That is, we want to
characterize wα (∞) = limt→∞ wα (t) or, more importantly, the predictor hα (∞) = F(wα (∞))
or βα (∞) = βwα (∞) we converge to, and how these depend on the scale α. In underdetermined
problems, where there are many zero error solutions, simply fitting the data using the model does
not provide enough inductive bias to ensure generalization. But in many cases, the specific solution
2
Under review as a conference paper at ICLR 2020
reached by gradient flow (or some other optimization procedure) has special structure, or minimizes
some implicit regularizer, and this structure or regularizer provides the needed inductive bias (Gu-
nasekar et al., 2018b;a; Soudry et al., 2018; Ji and Telgarsky, 2018).
3	The Kernel Regime
Gradient descent/flow considers only the first-order approximation of the model w.r.t. w:
f (w, x) = f(w(t),x) + hw - w(t), Vwf(W(t), x)i + O(kw - w(t)k2).	⑶
That is, locally around any w(t), gradient flow operates on the model as if it were an affine model
f(w, x) ≈ f0(x) + w, φw(t) (x) with feature map φw(t)(x) = Vw f (w(t), x), corresponding to
the tangent kernel Kw(t) (x, x0) = hVw f (w(t), x), Vw f (w(t), x)i (Jacot et al., 2018; Zou et al.,
2018; Yang, 2019; Lee et al., 2019). Of particular interest is the tangent kernel at initialization,
Kwα(0) = α2(D-1)K0 where we denote K0 = Kw0 .
The “kernel regime” refers to a situation in which the tangent kernel Kw(t) does not change over
the course of optimization, and less formally to the regime where it does not change significantly,
i.e. where ∀tKw(t) ≈ Kw(0) . In this regime, training the model is exactly equivalent to training
an affine model f(w, x) = αD f (wQ, x) +(w, αD-1φo(x)) with kernelized gradient descent/flow
with the kernel α2(D-1)K0 and a “bias term” of αDf(w0, x). To avoid handling this bias term,
and in particular its scaling, Chizat and Bach (2018) suggest using “unbiased” initializations such
that F (w0 ) = 0, so that the bias term vanishes. This can often be achieved by replicating units or
components with opposite signs at initialization, which is the approach we use here (see Sections
4-6 for examples and details).
For underdetermined problem with multiple solutions f(w, X) = y, unbiased kernel gradient flow
(or gradient descent) converges to the minimum norm solution hK = argminh(x)=y ∣∣h∣∣K, where
khkK is the RKHS norm corresponding to the kernel. And so, in the kernel regime, we will have
that h(∞) = hκo, and the implicit bias of training is precisely given by the kernel.
When does the “kernel regime” happen? Chizat and Bach (2018) showed that for any homogeneous1
model satisfying some technical conditions, the kernel regime is reached as α → ∞. That is, as we
increase the scale of initialization, the dynamics converge to the kernel gradient flow dynamics with
the kernel K°, and we have limα→∞ hα(∞) = hκ. In Sections 4 and 5 we prove this limit directly
for our specific models, and we also demonstrate it empirically for matrix factorization and deep
networks in Sections 6 and 7.
In contrast, and as we shall see in later sections, the α → 0 small initialization limit often leads to a
very different and rich inductive bias, e.g. inducing sparsity or low-rank structure (Gunasekar et al.,
2017; Li et al., 2018; Gunasekar et al., 2018b), that allows for generalization in many settings where
kernel methods would not. We refer to this limit reached as α → 0 as the “rich regime.” This regime
is also referred to as the “active” or “adaptive” regime (Chizat and Bach, 2018) since the tangent
kernel Kw(t) changes over the course of training, in a sense adapting to the data. We argue that this
regime is the one that truly allows us to exploit the power of depth, and thus is the more relevant
regime for understanding the success of deep learning.
4	Detailed Study of a S imple Depth-2 Model
We study in detail a simple 2-homogeneous model. Consider the class of linear functions over
X = Rd, with squared parameterization as follows:
d
f(w, x) = X(w2+,i -w-2 ,i)xi	=	hβw,	xi	, where w =	ww+	∈	R2d	and βw	= w+2 -w2-	(4)
i=1
1Chizat and Bach did not consider only homogenous models, and instead of studying the scale of initializa-
tion they studied scaling the output of the model. For homogeneous models, the dynamics obtained by scaling
the initialization are equivalent to those obtained by scaling the output, and so here we focus on homogenous
models and on scaling the initialization.
3
Under review as a conference paper at ICLR 2020
where we use the notation z2 for z ∈ Rd to denote elementwise squaring. We consider initializing
all weights equally with w0 = 1.
This is nothing but a linear regression model, except with an unconventional parametrization. The
model can also be thought of as a “diagonal” linear neural network (i.e. where the weight matrices
have diagonal structure) with 2d units. A standard diagonal linear network would have d units,
with each unit connected to just a single input unit with weights ui and the output with weight
vi, thus implementing the model f ((u, v), x) = Pi uivixi. But if at initialization |ui| = |vi|,
their magnitude will remain equal and their signs will not flip throughout training, and so we can
equivalently replace both with a single weight wi, yielding the model f(w, x) = w2, x .
The reason for using both w+ and w- (or 2d units) is two-fold. First, it ensures that the image
of F (w) is all (signed) linear functions, and thus the model is truly equivalent to standard linear
regression. Second, it allows for initialization at F (αw0) = 0 without this being a saddle point
from which gradient flow will never escape.2
The model (4) is perhaps the simplest non-trivial D-homogeneous model for D > 1, and we chose
it for this reason, as it already exhibits distinct and interesting kernel and rich regimes. Furthermore,
we can completely understand both the implicit regularization driving this model and the transition
between the regimes analytically.
Consider the behavior of the limit of gradient flow (2) as a function of the initialization, in the
underdetermined N d case where there are many possible solutions Xβ = y. The tangent kernel
at initialization is K0 (x, x0) = 8α2 hx, x0i, i.e. a scaling of the standard inner product kernel, so
∣∣βkκo a kβ∣∣2∙ Thus, in the kernel regime, gradient flow leads to the minimum '2 norm solution,
βL2 = argminX什=勾 kβ∣2. Following Chizat and Bach (2018) and the discussion in Section 3, we
thus expect that lima→∞ βα(∞) = βL2, and we also show this below.
In contrast, Gunasekar et al. (2017) shows that as α → 0, gradient flow leads instead to the min-
imum '1 norm solution lima→oβα(∞) = βL 1 = argminχβ=y kβkr This is the “rich regime.”
Comparing this with the kernel regime, we already see two very distinct behaviors and, in high di-
mensions, two very different inductive biases. In particular, the rich regime’s bias is not an RKHS
norm for any choice of kernel. Can we charactarize and understand the transition between the two
regimes as α transitions from very small to very large? The following theorem does just that.
Theorem 1.	For any 0 < α < ∞,
βα(∞) = βα = arg min Qa (β) St Xβ = y,	(5)
β
where Qa (β) = Pd=ι q (黑)and q(z) = R0 arcsinh (U2) du = 2 - √4 + z2 + Z arcsinh (22)
Proof sketch The proof in Appendix A proceeds by showing the gradient flow dynamics on w
lead to a solution of the form
βa (∞) = α
(6)
where ra(t) = Xβa (t) - y. While evaluating the integral would be very difficult, the fact that
βα(∞) ∈ {α2 (exp (-X>r) - exp (X>r) : r ∈ RN)}	(7)
already provides a dual certificate for the KKT conditions for minβ Qa (β) s.t. Xβ = y.
In light of Theorem 1, the function Qa (referred to elsewhere as the “hypentropy” function (Ghai
et al., 2019)) can be understood as an implicit regularizer which biases the gradient flow solution
towards one particular zero-error solution out of the many possibilities. As α ranges from 0 to
∞, the Qa regularizer interpolates between the '1 and '2 norms, as illustrated (labelled D = 2)
in Figure 2a, which shows the coordinate function q. As α → ∞ we have that βi /α2 → 0, and
so the behaviour of Qa (β) is controlled by the behaviour of q(z) around z = 0. In this regime
2Our results can be generalized to non-uniform initialization, “biased initiliation” (i.e. where w- 6= w+
at initialization), or the asymmetric parameterization f ((u, v), x) = Pi uivixi, however this complicates the
presentation without adding much insight.
4
Under review as a conference paper at ICLR 2020
Figure 1: In (a), the population error of the gradient flow solution vs. α in the sparse regression problem
described in Section 4. In (b), the excess `1 norm (blue) and excess `2 norm (red) of the gradient flow solution,
i.e. kβα (∞)k1 - IleL 1 k1 and IIea (∞)k2 - kβL2 k2. In (c), the largest α SuCh that βα (∞) achieves population
error at most 0.025 is shown. The dashed line indicates the number of samples needed by βL 1.
q(Z) = z2 + O(Z4) is quadratic, and so Qa(β)δc Pi β2 = ∣∣β∣∣2. On the other hand when ɑ → 0,
βi/α2 → ∞ is governed by the asymptotic behaviour q(Z) = Θ(z log Z) as Z → ∞. In this regime
Qa 网收 Pi aβi log βi α ∣∣β∣∣1 +。⑴.For any initialization scale α, the function Qa describes
exactly how training will interpolate between the kernel and rich regimes. The following Theorems,
proven in Appendix B, provide a quantitative statement of how the `1 and `2 norms are approached
as α → 0 and α → ∞ respectively:
Theorem 2.	For any 0 < < d
α ≤ min {(2(1 + C) ∣∣βLι∣∣])-.,exp(-d∕g∣β(ι∣∣J)} =⇒ Ilea ∣∣ι ≤ (1 + C) ∣∣βL1∣∣1
α ≥ J2(1 + c)(1 + 2∕c) IleL2k2 =⇒ kβak2 ≤ (1 + C) kβL2k2
Theorem 2 indicates a certain asymmetry between reaching the rich and kernel regimes: polyno-
mially large α suffices to approximate βL 2 to a very high degree of accuracy. On the other hand,
exponentially small α is sufficient to approximate βL 1, and Lemma 2 in Appendix B proves that
α ≤ d-Q(1/e) is necessary in order for Qa to be proportional to the '1 norm, which indicates that a
must be exceedingly small to approximate βL 1 for certain problems.
This suggestive of an explanation for the difficulty of demonstrating rich regime behavior empiri-
cally in matrix factorization problems (Gunasekar et al., 2017; Arora et al., 2019a). If the initial-
ization really needs to be exponentially small, then conducting experiments in this regime may be
infeasible for practical reasons.
In order to understand the effects of initialization on generalization, consider a simple sparse regres-
sion problem, where x(1),..., x(N) 〜N(0, I) and y(n) = (β*, x(n))+ N(0, 0.01) where β* is
r * -sparse and its non-zero entries are 1 / ʌ/r*. When N ≤ d, gradient flow will reach a zero training
error solution, however, not all of these solutions will generalize the same. With N = Θ(r* log d)
samples, the rich regime, i.e. the minimum `1 norm solution will generalize well. However, even
though we can fit the training data perfectly well, we should not expect any generalization in the
kernel regime with this sample size (N = Ω(d) samples would be needed that regime), see Figure
1c. In this case, to generalize well may require using very small initialization, and generalization
will improve as we decrease α. From an optimization perspective this is unfortunate because w = 0
is a saddle point, so taking α → 0 drastically increases the time needed to escape the saddle point.
Thus, there is a tension here between generalization and optimization: a smaller α might improve
generalization, but it makes optimization trickier. This suggests that in practice we would want to
compromise, and operate just at the edge of the rich regime, using the largest α that still allows for
generalization. This is borne out in our neural network experiments in Section 7, where standard
initialization schemes correspond to being right on the edge of entering the kernel regime, where we
expect models to both generalize well and avoid serious optimization difficulties.
The tension between optimization and generalization can also be seen through a tradeoff between
the sample size and the largest α we can use and still generalize. In Figure 1c, for each sample size
N, we plot the largest α for which the gradient flow solution βa achieves population risk below
5
Under review as a conference paper at ICLR 2020
(a) Regularizer
(b) Approximation ratio
(c) Sparse regression simulation
Figure 2: (a) qD (z) for several values of D. (b) The ratio
[1, 0, 0, . . . , 0] is the first standard basis vector and 1d =
QD (ei)
QD(id∕kid k2)
as a function of α, where e1
[1, 1, . . . , 1] is the all ones vector in Rd. This
captures the transition between approximating the `2 norm (where the ratio is 1) and the `1 norm (where the
ratio is l/ʌ/d). (C) sparse regression simulation as in Figure 1, using different order models. The y-axis is aD
(the scale of β at initialization) needed to recover the planted predictor to accuracy 0.025. The dashed line
indicates the number of samples needed in order for βL 1 to approximate the plant.
some threshold. As N approaches the number of samples needed for βL 1 to generalize (the vertical
dashed line), α must become extremely small. However, generalization is much easier when the
number of samples is only slightly larger, and we can use much more moderate initialization.
5 Higher Order Models
In the previous Section, we considered a 2-homogeneous model, corresponding to a simple depth-2
“diagonal” network. Deeper models correspond to higher order homogeneity (a depth-D ReLU or
linear network is D-homogeneous), motivating us to understand the effect of the order of homo-
geneity on the transition between the regimes. We therefore generalize our model and consider:
FD(w) = βw,D = w+D - w-D and fD (w, x) = w+D - w-D, x	(8)
We again consider initializing all weights equally so w0 = 1. As before, this is just a linear regres-
sion model with an unconventional parametrization. It is equivalent to a depth-D matrix factoriza-
tion model with commutative measurement matrices, as studied by Arora et al. (2019a), and can be
thought of as a depth-D diagonal linear network.
We can again study the effect of the scale of the initialization α on the implicit bias. Let βα,D (∞)
denote the limit of gradient flow on w when initialized at α1 for the D-homogeneous model. Using
the same approach as in Section 4, in Appendix C we show:
Theorem 3.	For any α and D ≥ 3, if gradient flow reaches a solution X βα,D (∞) = y, then
βα,D(∞) = arg minβ QαD(β) s.t. Xβ = y
where QD(β) = Pd=I q∏(βi∕aD) and q° = R hp1 is the antiderivative of the unique inverse of
hD(Z) = (1 — Z)-D-2 — (1 + Z)-D-2 on [-1,1]. Furthermore, limα→oβα,D(∞) = βL1 and
limɑ→∞ βα,D (∞) = βL2.
In the two extremes we see that we again get the minimum `2 solution in the kernel regime, and
more interestingly, for any depth D ≥ 2, we get the same minimum `1 norm solution in the rich
regime, as has also been observed by Arora et al. (2019a). The fact that the rich regime solution
does not change with depth is perhaps surprising, and does not agree from what is obtained with
explicit regularization (regularizing kwk2 is equivalent to kβ k2/D regularization), nor with implicit
regularization on logistic-type loss Gunasekar et al. (2017).
Although the two extremes do not change as we go beyond D = 2, what does change is the interme-
diate regime (the regularizer QαD is unique and cannot be obtained with any other order D0 6= D),
as well as the sharpness of the transition into the extreme regimes, as illustrated in Figures 2a-2c.
The most striking difference is that for order D > 2 the scale of α needed to approximate the `1
is polynomial rather then exponential, yielding a much quicker transition to the “rich regime”, and
6
Under review as a conference paper at ICLR 2020
Figure 3: Regimes in Matrix Completion We generated a 10 × 10 rank-one matrix completion problem
with ground truth M* = u*(v*)> by generating u*, v* ∈ R10 with i.i.d. N(0,1) entries and observing
N = 60 random entries Ω. We fit the observed entries by minimizing the squared loss on a matrix factorization
model F (U, V ) = UV > with U, V ∈ Rd×2k. For different scalings α, we examine the matrix M(∞)
reached by gradient flow on U, V (solved using python ODE solvers) and plot (i) the reconstruction error on
unobserved entries Eij∈ω (Mj - M(∞)ij)2, and (ii) the amount by which the unobserved entries changed
during optimization Eij∈ω (M(∞)ij - M(0)ij)2. In (a) We used k = 2d and initialized to Uo = V0 = αI. In
(b) for varying k, We initialized to U0 = αU0 and V0 = αV0 with U0, V0 ∈ RdXk with i.i.d. N(0,1) entries.
For large k , the tangent kernel converges to the kernel corresponding to the Frobenius norm.
allowing near-optimal sparse regression with reasonable initialization scales. Increasing D further
hastens the transition. This might also help explain some of the empirical observations about the
benefit of depth in deep matrix factorization Arora et al. (2019a).
6	Demonstration in Matrix Completion
We now turn to a more complex depth two model, namely a matrix factorization model, and
demonstrate similar transitions empirically. Specifically, we consider the model over matrix in-
puts X ∈ Rd×d defined by f ((U, V ), X) = UV >, X, where U, V ∈ Rd×k. This corresponds
to linear predictors over matrix arguments specified by F(U, V ) = UV >. In overparameterized
regime with k ≥ d, the parameterization itself does not introduce any explicit rank constraints. We
consider here a random low rank matrix completion problem where Xn = ein ej> represents an
uniform random observation of entry (in ,jn) of a planted low rank matrix M* of rank r*《d:
yn = (M*, Xni = Mj. For underdetermined problems where N《d2, there are many trivial
global minimizers, most of which are not low rank and hence will not guarantee recovery. As was
demonstrated empirically by Gunasekar et al. (2017) and also proven rigorously for Gaussian mea-
surements by Li et al. (2018), as α → 0 gradient flow implicitly regularizes the nuclear norm, which
for random measurements leads to recovery of ground truth (CandBS and Recht, 2009; Recht et al.,
2010): these are very different and rich implicit biases that are not RKHS norms.
Crucially, the reconstruction results in Gunasekar et al. (2017) and Li et al. (2018) are dependent
on initialization with scale α → 0. Here we further explore the role of initialization. Similar to
Section 4, in order to get unbiased 0 initialization, we consider k ≥ 2d and initialization of the form
U(0) = α [U0, -U0] and V(0) = α [V0, V0], where U0, V0 ∈ Rd×k/2. We will study implicit bias
of gradient flow over the factorized parameterization with above initialization.
For matrix completion problems with X = eiX ej> , the tangent kernel at initialization is given
by K0(X,χ0) α hU0[ix, ：], U[ix0, ：]i ICjX = jx0) + hV0[jx,:], V0[jx0, ：]i 1(ix = ixO). ThiS
defaults to the trivial delta kernel K(X, X0) = 1(ix = ix0) ∙ 1(jx = jx0) for the two special cases
(a) U0 , V0 have orthogonal columns (e.g. U0 = V0 = I), or (b) U0 , V0 have independent Gaussian
entries and k → ∞. In these cases, minimizing the RKHS norm of the tangent kernel corresponds
to returning a zero imputed matrix (minimum Frobenius norm solution). Figure 3 demonstrates the
behaviour of gradient flow updates in the “rich" regime (where for α → 0 recovers the ground truth)
and in the “kernel" regime (where for large α, there are no updates to the unobserved entries).
7	Neural Network Experiments
In the preceding sections, we intentionally focused on the simplest possible models in which a
kernel-to-rich transition can be observed, in order to isolate this phenomena and understand it in
7
Under review as a conference paper at ICLR 2020
detail. In those simple models, we were able to obtain a complete analytic description of the tran-
sition. Obtaining such a precise description in more complex models is somewhat optimistic at this
point, as we do not yet have a satisfying description of even just the rich regime. Instead, we now
provide empirical evidence suggesting that also for non-linear and realistic networks, the scale of
initialization induces a transition into and out of a “kernel” regime, and that to reach good gener-
alization we must operate outside of the “kernel” regime. To track whether we are in the “kernel”
regime, We track how much the gradient Nw f (w(t), x) changes throughout training. In particular,
we define the gradient distance to be the cosine distance between the initial tangent kernel feature
map Nw f (w(0), x) and the final tangent kernel feature map Nwf(w(T), x).
In Figures 4a and 4b, we see that also for a non-linear ReLU network, we remain in the kernel
regime when the initialization is large, and that exiting from the kernel regime is necessary in order
to achieve small test error on the synthetic data. Interestingly, when αD ≈ 1, the models achieve
good test error but have smaller gradient distance which, not coincidentally, corresponds to using
the out-of-the-box Uniform He initialization. This lies on the boundary between the rich and kernel
regimes, which is desirable due to the learning vs. optimization tradeoffs discussed in Section 4.
On MNIST data, Figure 4e shows that previously published successes with training overly wide
depth-2 ReLU networks without explicit regularization (e.g. Neyshabur et al., 2014) relies on the
initialization being small, i.e. being outside of the “kernel regime”. In fact, the 2.4% test error
reached for large initialization is no better than what can be achieved with a linear model over a
random feature map. Turning to a more realistic network, 4f shows similar behavior when training
a VGG11-like network on CIFAR10.
So far, we attempted to use the best fixed stepsize for each initialization (i.e. achieving the best test
error). But as demonstrated in Figures 4c and 4d, the stepsize choice can also have a significant
effect, with larger stepsizes allowing one to exit the kernel regime even at an initialization scale
where a smaller stepsize would remain trapped in the kernel regime. Further analytic and empirical
studies are necessary in order to understand the joint behavior of the stepsize and initialization scale.
8	Discussion
The main point of this paper is to emphasize the distinction between the “kernel” regime in training
overparametrized multi-layered networks, and the “rich” (active, adaptive) regime, show how the
scaling of the initialization can transition between them, and understand this transition in detail.
We argue that rich inductive bias that enables generalization may arise in the rich regime, but that
focusing on the kernel regime restricts us to only what can be done with an RKHS. By studying
the transition we also see a tension between generalization and optimization, which suggests we
would tend to operate just on the edge of the rich regime, and so understanding this transition, rather
then just the extremes, is important. Furthermore, we see that at the edge of the rich regime, the
implicit bias of gradient descent differs substantively from that of explicit regularization. Although
in our theoretical study we focused on a simple model so that we can carry out a complete and exact
analysis analytically, our experiments show that this is representative of the behaviour also in other
homogeneous models, and serve as a basis of a more general understanding.
Effect of Width Our treatment focused on the effect of scale on the transition between the regimes,
and we saw that, as pointed out by Chizat and Bach, we can observe a very meaningful transition
between a kernel and rich regime even for finite width parametric models. The transition becomes
even more interesting if the width of the model (the number of units per layer, and so also the number
of parameters) increases towards infinity. In this case, we must be careful as to how the initialization
of each individual unit scales when the total number of units increase, and which regime we fall in to
is controlled by the relative scaling of the width and the scale of individual units at initialization. This
is demonstrated, for example, in Figure 5, which shows the regime change in matrix factorization
problems, from minimum Frobenius norm recovery (the kernel regime) to minimum nuclear norm
recovery (the rich regime), as a function of both the number of factors k and the scale of initialization
of each factor α . As is expected, the scale α at which we see the transition decreases as the model
becomes wider, but further study is necessary to obtain a complete understanding of this scaling.
A particularly interesting aspect of infinite width networks is that, unlike for fixed-width networks,
it may be possible to scale α relative to the width k such that at the infinite-width limit we would
8
Under review as a conference paper at ICLR 2020
0.30
0.25
0.20
0.15-
0.10-
---zero pred
-4- D=2
-4- D=3
-4- D=5
0.05
φuue,s-p-juθlpeJ9
(a) Test RMSE vs scale
IO-4 IO-3	IO-2 10-l
0.8
0.7
—I— D=2
4- D=3
10-5
(b) Grad distance VS scale
IoT 10° IO1 IO2
0o
0.06
0.04
0.02
0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175
stepsize
(C) Test RMSE VS stepsize
Q.000 0.025。.。5。0.075 0.100 0.125 0.150 0.175

stepsize
(d) Grad distance vs stepsize (e) MNIST test error vs scale (f) CIFAR10 test error vs scale
Figure 4: Synthetic Data: we generated a small regression training set in R2 by sampling 10 points uniformly
from the unit circle, and labelling them with a 1 hidden layer teacher network with 3 hidden units. We trained
overparametrized depth-D, ReLU networks with 30 units per layer with squared loss using full GD and a small
stepsize 0.01. The weights of the network are set using the Uniform He initialization, and then multiplied by
α. The model is trained until ≈ 0 training loss. Shown in (a) and (b) are the test error and grad distance vs. the
depth-adjusted scale of the initialization, αD , when a small constant stepsize is used. for (c) and (d), we fix α
near the transition into the kernel regime, and show the test error and grad distance vs. the stepsize. MNIST:
we trained a depth-2 with 5000 hidden units with cross-entropy loss using SGD until it reached 100% training
accuracy. The stepsizes were optimally tuned for each α individually. In (e), the dashed line shows the test
error of the resulting network vs. α. We repeated the experiment, but froze the bottom layer and only trained the
output layer until convergence. The solid line shows the test error of this predictor vs α. CIFAR10: we trained
a VGG11-like deep convolutional network with cross-entropy loss using SGD and a small stepsize 10-4 for
2000 epochs; all models reached 100% training accuracy. In (f), the dashed line shows the final test error vs.
α. We repeated the experiment freezing the bottom 10 layers and training only the output layer-the solid line
shows this model’s test error. See Appendix E for full details about all of the experiments.
have an (asymptotically) unbiased predictor at initialization limk→∞ Fk (w(0)) = 0, or at least
a non-exploding initialization lim supk→∞ kFk(w(0))k = O(1), even with random initialization
(without a doubling trick leading to artificially unbiased initialization), while still being in the kernel
regime. For two-layer networks with ReLU activation, Arora et al. (2019b) showed that with width
k ≥ poly(1/ maxkxk2≤1,N kf0(x)k) the gradient dynamics stay in the kernel regime forever.
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. arXiv preprint arXiv:1811.03962, 2018.
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix
factorization. arXiv preprint arXiv:1905.13655, 2019a.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. arXiv preprint
arXiv:1901.08584, 2019b.
Emmanuel J Candes and Benjamin Recht. Exact matrix completion via convex optimization. Foun-
dations of Computational mathematics, 9(6):717-772, 2009.
Lenaic Chizat and Francis Bach. A note on lazy training in supervised differentiable programming.
arXiv preprint arXiv:1812.07956, 2018.
Amit Daniely. SGD learns the conjugate kernel class of the network. In Advances in Neural Infor-
mation Processing Systems, pages 2422-2430, 2017.
9
Under review as a conference paper at ICLR 2020
Amit Daniely, Roy Frostig, and Yoram Singer. Toward deeper understanding of neural networks:
The power of initialization and a dual view on expressivity. In Advances In Neural Information
Processing Systems, pages 2253-2261, 2016.
Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. arXiv preprint arXiv:1811.03804, 2018.
Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In International Conference on Learning Representations,
2019.
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points—online stochastic
gradient for tensor decomposition. In Proceedings of The 28th Conference on Learning Theory,
pages 797-842, 2015.
Udaya Ghai, Elad Hazan, and Yoram Singer. Exponentiated gradient meets gradient descent. arXiv
preprint arXiv:1902.01903, 2019.
Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro.
Implicit regularization in matrix factorization. In Advances in Neural Information Processing
Systems, pages 6151-6159, 2017.
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias in
terms of optimization geometry. arXiv preprint arXiv:1802.08246, 2018a.
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Implicit bias of gradient descent
on linear convolutional networks. arXiv preprint arXiv:1806.00468, 2018b.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In Proceedings of the IEEE international
conference on computer vision, pages 1026-1034, 2015.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. arXiv preprint arXiv:1806.07572, 2018.
Ziwei Ji and Matus Telgarsky. Gradient descent aligns the layers of deep linear networks. arXiv
preprint arXiv:1810.02032, 2018.
Jaehoon Lee, Lechao Xiao, Samuel S Schoenholz, Yasaman Bahri, Jascha Sohl-Dickstein, and Jef-
frey Pennington. Wide neural networks of any depth evolve as linear models under gradient
descent. arXiv preprint arXiv:1902.06720, 2019.
Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized
matrix sensing and neural networks with quadratic activations. In Conference On Learning The-
ory, pages 2-47, 2018.
Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Mean-field theory of two-layers neural
networks: dimension-free bounds and kernel limit. arXiv preprint arXiv:1902.06015, 2019.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the
role of implicit regularization in deep learning. arXiv preprint arXiv:1412.6614, 2014.
Benjamin Recht, Maryam Fazel, and Pablo A Parrilo. Guaranteed minimum-rank solutions of linear
matrix equations via nuclear norm minimization. SIAM review, 52(3):471-501, 2010.
Pedro Savarese, Itay Evron, Daniel Soudry, and Nathan Srebro. How do infinite width bounded
norm networks look in function space? arXiv preprint arXiv:1902.05040, 2019.
Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The im-
plicit bias of gradient descent on separable data. Journal of Machine Learning Research, 19(70),
2018.
Greg Yang. Scaling limits of wide neural networks with weight sharing: Gaussian process behavior,
gradient independence, and neural tangent kernel derivation. arXiv preprint arXiv:1902.04760,
2019.
10
Under review as a conference paper at ICLR 2020
Peng Zhao, Yun Yang, and Qiao-Chu He. Implicit regularization via hadamard product over-
parametrization in high-dimensional linear regression. 03 2019.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes
over-parameterized deep relu networks. arXiv preprint arXiv:1811.08888, 2018.
A Proof of Theorem 1
It is straightforward, given the expression for Qa, to prove that βa is the minimum Qa solution to
Xβ = y. In other words, if we had been able to guess from the beginning that the implicit bias
would be governed by Qα, it would have been easy to prove this fact. However, a key contribution
of this work is in developing a method for determining what the implicit bias is when we do not
already have a good guess.
We will first describe a general approach for deriving the implied regularizer reached at the limit
of some gradient flow dynamics which minimize the loss. In particular, our approach will apply
whenever solutions of the gradient flow dynamics can be written as βα(t) = fα(X>ν(t)) for some
ν(t).
The first step is to analyze the gradient flow dynamics of βα and show that
Xβα(∞) = y
βα(∞) = fα(X>ν(∞))
(9)
for some function fα and some vector ν(∞). It is not important for our approach to know exactly
what ν(∞) is. This is useful because calculating this ν(∞) will often be difficult, even for the
simple examples we consider.
The next step is to suppose that there is some function Qα such that
βα(∞) = argminQα(β) s.t. Xβ = y
β
and to write down the KKT optimality conditions for (10):
X β* = y
∃ν VQa (β*) = X>ν
(10)
(11)
Finally, we connect (9) with (11). Specifically, βa(∞) satisfies the first KKT condition, and, if
βa(∞) = β* then by taking V = V(∞) we have
VQa (βα(∞))= VQa (fa(X>V)) = X>V	(12)
It follows that VQa(β) = fa-1 (β). Therefore, to derive Qa we simply invert fa and integrate its
inverse. We prove Theorems 1 and 3 using this method.
Theorem 1. For any 0 < α < ∞,
βa(∞) = βa = arg min Qa (β) St Xβ = y,	(5)
β
where Qa (β) = Pid=ι q (段)and q(z) = R0 arcsinh (2) du = 2 - √4 + z2 + Z arcsinh (2)
Proof. We begin by calculating the gradient flow dynamics on w, since the linear predictor
βa (∞) is given by F applied to the limit of the gradient flow dynamics on w. Recalling that
~
X = [X -X],
Wa (t)
-VL(Wa (t)) = -V	llX Wa (t)2
—2X>ra(t) ◦ Wao
(13)
where the residual ra(t)，XWa(t)2 - y, and a ◦ b denotes the element-wise product of a and b. It
is easily confirmed that these dynamics have a solution:
Wa (t) = Wa (0) ◦ exp
-2X> Z ra(s)ds
(14)
11
Under review as a conference paper at ICLR 2020
This immediately gives an expression for βα (t):
βα(t)
2α2 sinh
wα,+(t)2
α2 exp
4X>	t rα (s
(15)
(16)
(17)
In addition, this problem satisfies the strict saddle property (Ge et al., 2015) (Zhao et al., 2019,
Lemma 2.1), therefore gradient flow will converge to a zero-error solution, i.e. Xβα(∞) = y.
Thus, we conclude that βα(∞) is a global minimum with zero error, i.e. Xβα(∞) = y.
Thus, we have established that
Xβα(∞) = y
βα(∞) = fα(X>ν(∞))
(18)
for fα(z) = 2α2 sinh(z) and ν(∞) = -4 R0t rα(s)ds. This corresponds to (9) from our general
approach detailed above. Continuing from (12) we have that
VQα(β) = f-1(β) = arcsinh (β∕2α2)	(19)
Integrating this expression completes the proof.
□
B Proof of Theorem 2
Lemma 1. For any β ∈ Rd,
α ≤ α1 (, kβk1 ,d)
min{1, qkβkι, (2 kβkι厂2 , exP (—左 Idek )}
guarantees that
2
kβkι (1 - e) ≤ ln^o) Q(β∕α2) ≤ kβkι (1 + e)
Proof. First, We show that Q(β∕ɑ2) = Qα(∣β∣ ∕α2). Observe that g(χ) = Xarcsin(x∕2) is even
because x and arcsin(x∕2) are odd. Therefore,
d
Q(β∕α2) =X2-
i=1
1/4+ β4 + M arcsinh
α4	α2
d
X2-
i=1
d
X2-
i=1
J4+A+g (ai)
「+g Gl)
(20)
(21)
(22)
Qα(网 ∕ɑ2)
(23)
12
Under review as a conference paper at ICLR 2020
Therefore, we can rewrite
02
ln(1∕α2)
Q(β∕ɑ2)
02
ln(1∕α2)
Q(lβl ∕α2)
d
X
i=1
d
X
i=1
2α2	vz4q4 + β2
ln(1∕α2)	ln(1∕α2)
2α2	vz4q4 + β2
ln(1∕α2)	ln(1∕α2)
d
X
i=1
2α2	，4α4 + 优
ln(1∕α2)	ln(1∕α2)
/
+ lβ∕	1 +
∖
+	lβi∣
ln(1∕α2)
+	lβi∣
ln(1∕α2)
(27)
—
—
—
Using the fact that
同 ≤ ∙∖∕α2 + b2 ≤ 同 + lbl	(28)
we can bound for α < 1
a2	& 2a2	2ɑ2	/	ln (噜 + α2 + 卑)∖
Q(β∕α2) ≤ X τ-rrr- - τ-ΓΓΓ+ + lβil 1+	1 _1/ ”-Z	(29)
ln(1∕α2)	in ln(1∕α2)	ln(1∕ɑ2)	∖	ln(1∕α2)	j
=£阈(1+」)	(30)
Uu (	ln (⑶ + α2) )
≤∣∣β k1 1+max	2 J	(31)
∖	i∈[d]	ln(1∕ɑ2))
Uu (	ln (kβ∣∣ 1 + α2) )
≤kβkι (1+	l*∕02) 1	(32)
(33)
So, for any α ≤ min{1, ∕∣∣β∣∣1, (2 ∣∣β∣∣1)-泰}, then
UJQ(β∕°2) ≤k% 1 +
ln (|网∣ι + 02)
ln(1∕α2)
≤kβk1 (1 +
ln(2 ∣∣β∣∣ι)∖
ln(1∕ɑ2))
≤kβkι (1 + e)
(34)
(35)
(36)
On the other hand, using (27) and (28) again,
~2	d	ς)~2
ln(1∕α2)Qs ≥ X ln(1∕α2)
l*20l2 + 阈(1 +
ln(1∕α2)	\
ln(lβcl)∖
ln(1∕α2) J
d
∑>l 1 +
i=1	'
In(IeiI) - 1 ʌ
ln(1∕α2) J
Using the inequality ln(x) ≥ 1 - ɪ , this can be further lower bounded by
α2	C 二	1
ln(1∕α2)Q…≥ Xlel
ln(1∕α2)
=kβkι- in(⅛)
(37)
(38)
(39)
(40)
—
13
Under review as a conference paper at ICLR 2020
Therefore, for any α ≤ exp (- 2^% ) then
~2
ɪn(^Q(β∕02) ≥kβkι (1-e)	(41)
We conclude that for α ≤ min {1, Pkek「(2 ∣∣β∣∣ J-%,exp (- 2^%) } that
2
kβkι (1 - e) ≤ In(ITo2)Q(β∕α2) ≤ kβkι (1 + C)	(42)
□
Lemma 2. Fix any c > 0 and d ≥ max {e, 124e}. Thenforany α ≥ d-4-泰,Q(β∕α2) X IlellI 加
the sense that there exist vectors v, W such that
Q (α⅛V)
≥ (1 + C)
Q (⅛ W)
l∣wkι
Proof. First, recall that
Thus,
一ʌ/ 4c204 + 1 + In
—4 4c204 + 1 + In
Now, consider the ratio
Using (46), we conclude
√d一
Q
- 1 + In
Q (表eι)
Q (表 τ⅛)	dq (0⅛)
_ 1	02q (表
-1 + In
Q (表eι)	≥	-1 +In (表)
(表 ⅛)	3√dα2 - 1 + In (0⅛)
=	-1 + In (表)
3√dα2 — 1 + In (表)—2 In(d)
I 十	In(d) — 6√dɑ2
6√d02 - 2 + 2 In (-√-7=)
(43)
(44)
(45)
(46)
(47)
(48)
(49)
(50)
(51)
14
Under review as a conference paper at ICLR 2020
Fix any e > 0 and d ≥ max {e, 124'}, and set α = d- 1 -*.Then
,1	. d 表、F
2d4 ≥ 6 and -ln d ≥ 6
一	2 一
…ι	da..6
=⇒ 2d 4≡ — 6 +-ln d-≥ 0
2	6 一
=⇒ I1 — ―] ln d — 6 d-a ≥ 6d-聂—2
V 2e)	6 一
=⇒ ɪ ln d - -d-聂 ≥ 6d-聂一2+ ɪ ln d
6	6	-	26
=⇒ ln (d) — 6α2√d ≥ 6 ^6α2√d — 2 + 2ln( —ɪ=^
(52)
(53)
(54)
(55)
(56)
This implies that the second term of (51) is at least 6. We conclude that for any 6 > 0 and d ≥
max {e, 124'}, α = d- 1 -反 implies that
Q）表；）、≥（1+6）
Q （表 √⅛）
〔Eki
k⅛∣L
(57)
Consequently, for at least one of these two vectors, Q is not proportional to the 'i norm UP to
accuracy O(6)for this value of α.
It is straightforward to confirm that
d Q ( jςei)
--—”2 1/ 、 ≥ 0	(58)
dαQ Iɪ id、一
Q {ajid∖2j
which concludes the proof.	□
Lemma 3. For any β ∈ Rd,
α ≥ α2(6, ∣∣w∣∣2)
guarantees that
(1-6)kβk2 ≤ 4α4Q(β∕α2) ≤ (1 + 6)∣∣β∣∣2
Proof. The regularizer Q can be written
d fβi/α2
Q(β∕α2) = V2 / arcsinh
i=1j°
Let φ(z) = R0/a arcsinh (2t) dt, then
φ(0) = 0
Φ0(0)
F arcsinh
ɑ2
=0
Z = 0
夕(0)
1
α4√4+⅛
1
=----Γ
204
z=0
φ" 0(0)
。〃〃(Z)
-Z
08 (4+公产2
3z2
=0
z=0
-----------，八------------C ，八
a12 (4+ ⅛产2	α8 (4+ ⅛)3/2
(59)
(60)
(61)
(62)
(63)
(64)
1
15
Under review as a conference paper at ICLR 2020
Also, note that
∖φ""(z)∖
∣2z2 - 4α4∣
—α12 (4+ ⅛ 产
z2 + 2α4
≤ 16α12
(65)
(66)
Therefore, by Taylor,s theorem, for some ξ with ∖ξ∖ ≤ ∖z∖
Z
φ(z)-耳
z2
=⇒ φ(z)-耳
φ""(ξ)
4!
z4
(67)
sup
∣ξ∣≤∣z∣
φ" ,,(ξ)
4!
z4 ≤
z6 + 2α4z4
384012
z2 z4 + 2a4z2
4θ4 ~96θ8~
(68)
≤
Therefore, for any β ∈ Rd
4α4Q.(β)-∣∣β∣∣2∣=404
≤ 4a4
i=1
d
≤ X β2 ∙
逢
4α4
E φ(βi) - 4βi4
β4 + 2α4β2
≤ llβk2 max
i
96α8
β4 + 2α4β2
-9βO8-
(69)
(70)
(71)
(72)
Therefore, α ≥ Pkβ∣∣2 (1 + e- 1) ensures
(1-e)∣∣β∣∣2 ≤ 4α4Q(β∕α2) ≤ (1 + e) ∣∣β∣∣2
(73)
□
Theorem 2. For any 0 < e < d
α ≤ min {(2(1 + e) kβ"k1 厂专,exp(-d∕(e ||优∕∣1))} =⇒ ∣βa∣1 ≤ (1 + e) ||a/k
α ≥ ,2(1 + e)(1 + 2∕e)m工2k2 =⇒ kβαk2 ≤ (1 + e) kβL2∣2
Proof. We prove the '1 and '2 statements separately.
'1 approximation First, we will prove that ∣∣yβα∣∣ < (1 + 2e) ∣βL 1%. By Lemma 1, since α ≤
α1 (ɪ, (1 + 2e) ∣βL 1k1,d),forall β with ∣∣β∣1 ≤ (1 + 2e) ||比山 we have
kβ k1 (1-
2
≤ m(W) Q(β∕°2) ≤kβ k1 (1 +
(74)
e
2 + e
e
2 + e
16
Under review as a conference paper at ICLR 2020
Let β be	such that Xβ = y and ∣∣β∣1 = (1 + 2e) ∣βL 1∣1. Then iΞ(⅛) Q(β∕°2)≥ (1- 六)kβ∣ι	(75) =I1- 2⅛) (1 + 2e)kβLιkι	(76) (1 -)	ɑ2 ≥ (17^j(1 + 2E)RWJQ(e"/a2)	(77) =* f Q(β"∕α2)	(78) 2 > EJQ(β"∕α2)	(79) 2 ≥ RwJQ(β"α2)	(80)
Therefor It is easi is satisfie	e, β = βa. Furthermore, let β be any solution Xβ = y with ∣∣β∣1 > (1 + 2e) ||。"||「 ly confirmed that there exists C ∈ (0,1) such that the point β0 = (1 - c)β + cβL 1 为 both Xβ0 = y and ∣∣β0∣1 = (1 + 2e) ||户"||「By the convexity of Q, this implies
Q(β∕ɑ2) In(K2)‘	)≥ Q(β0∕ɑ2) > Qα(βɑ∕α2). Thus a β with a large '1 norm cannot be a solution, even if 2(β∕ɑ2) ≈∣∣β∣∣1∙
Since ∣∣yβ	ɑ∣∣] < (1 + 2e) kβL 1∣1, we conclude llβαllι ≤ TZI^ln(l∕ɑ2) Q(β"02)	(81) ≤ i⅛ ιn∏‰ Q(电")	(82) 1 +岳 ≤ TkβL1k1	(83) 1	2+e = (1 + E) kβL1k1	(84)
First, w	e will prove that ∣∣βɑ∣∣	<	(1 + 2e) ∣βL2k2.	By Lemma 3, since α ≥
a (景,(1 + 2E) IleL 2k2) ,for all β with IleIl2 ≤ (1 + 2E) ||曳 2k2 we have
Let β be	∣β∣2 (1 一 ⅛i) ≤ 4α4Q(β∕α2) ≤ ∣β∣2 (1 + 六)	(85) such that Xβ = y and ∣∣β∣∣2 = (1 + 2e) ∣βL2k2. Then 4α4Q(β∕α2) ≥ (1 -六)∣β∣2	(86) =(1 - 2+;) (1 + 2e) kβL2∣2	(87) (1 -备) ≥ ə	N (1 + 2E)4α4Q(βL2∕02)	(88) (1 + 条) =11+2E 4α4Q(βL 2∕02)	(89) > 404Q(βL2∕02)	(90) ≥ 4α4Q(β"α2)	(91)
17
Under review as a conference paper at ICLR 2020
Therefore, β = βα. Furthermore, let β be any solution Xe = y With ∣∣βk2 > (1 + 2e) ∣∣βL2k2. Itis
easily confirmed that there exists C ∈ (0,1) such that the point β0 = (1 一 c)β + cβL2 satisfies Xe0 =
y and ∣∣β［卜= (1 + 2e) ∣∣βL2k2. By the convexity of Q, this implies Q(β∕ɑ2) ≥ Q(β0∕a2) >
Q(βL2∕a2). Thus a β with a large '2 norm cannot be a solution, even if 4a4Q(β∕02) ≈ kβk2.
SinceIeaI < (1 + 2E) ∣βg 12, we conclude
Mail2 ≤ ɪ-ɪ4α4Q(βa∕α2)	(92)
≤ 丁二4α4Q(βL2∕α2)	(93)
1 - 2+7
1 + ɪ	C
≤ Tɪ⅛ kβL2∣2	(94)
1 - 2+e
= (1 + e)kβL 2∣2	(95)
□
C Proof of Theorem 3
Lemma 4. For the D-homogeneous model (8),
∀t
X> Z r(τ)dτ
0
ɑ2-D
D(D 一 2)
≤
∞
Proof. For the order-D unbiased model β(t) = w+D 一 w-D, the gradient flow dynamics are
W +(t)
dL
dw+
-DX>r(t) ◦ WD-I(t), w+(0) = a1
=⇒ w+(t) = α2-D 1 + D(D 一 2)X> Z r(τ)dτ
1
D-2
(96)
(97)

Where ◦ denotes elementwise multiplication, r(t) = X β(t) 一 y, and where all exponentiation is
elementwise. Similarly,
W-(t) = —，L = DX>r(t) ◦ wD-1(t), w-(0) = a1
dW-
=⇒ W- (t) = α2-D 1 一 D(D 一 2)X> Z r
(98)
(99)
1
D-2

First, we observe that ∀t∀i W+(t)i ≥ 0 and ∀t∀i W-(t)i ≥ 0. This is because at time 0, W+(0)i =
w-(0)i = α > 0; the gradient flow dynamics are continuous; and w+(tb = 0 =⇒ W +(t} = 0
andw-(t)i = 0 =⇒ W-(tb = 0.
Consequently,
0 ≤ w+(t)i2-D	α2-D	+ D(D - 2)			X> Z r(τ)dτ 0		i	(100)
0 ≤ w-(t)i2-D	α2-D	一 D(D		- 2)	X> Z r(τ)dτ 0		i	(101)
=⇒ —α2-D ≤ D(D —		2)	X>	Ztr 0	(τ )dτ	≤ α2-D i		(102)
□
which concludes the proof.
18
Under review as a conference paper at ICLR 2020
Theorem 3. For any α and D ≥ 3, if gradient flow reaches a solution X βα,D (∞) = y, then
βα,D(∞) = arg minβ QαD(β) s.t. Xβ = y
where QD(β) = Pd=I q∏(βi∕aD) and q° = R hp1 is the antiderivative of the unique inverse of
hD(Z) = (1 — Z)-D-2 — (1 + Z)-D-2 on [—1,1]. Furthermore, limα→oβα,D(∞) = βLι and
limɑ→∞ βα,D (∞) = βL2.
Proof. For the order-D unbiased model β(t) = w+D — w-D, the gradient flow dynamics are
W(t) = ~T~ = -DX>r(t) ◦ wD-1
dw
w(0) = α1
=⇒ w(t) = (α2-D + D(D — 2)X> Z r(τ)dτ)

=⇒ β(t) = αD 1 + αD-2D(D — 2)X > Z r(τ)dτ
1
D-2
D
D-2
-aD 1 —αD-
2D(D — 2)X > Zt r(τ)dτ
D
D-2

where X = [X — X] and r(t) = Xβ(t) — y. Supposing β(t) converges to a zero-error solution,
Xβ(∞) = yβ(∞) = αDhD(X>ν(∞))
where V(∞) = —αD-2D(D — 2) f∞ r(τ)dτ and the function hD is applied elementwise and is
defined
hD (z) = (1 — Z)--D— - (1 + Z)--D-
By Lemma 4, X>ν∞ ≤ 1, so the domain of hD is the interval [—1, 1], upon which it is monoton-
ically increasing from hD( —1) = —∞ to hD(1) = ∞. Therefore, there exists an inverse mapping
hD1(t) with domain [—8, ∞] and range [—1,1].
This inverse mapping unfortunately does not have a simple closed form. Nevertheless, it is the root
ofa rational equation. Using the approach outlined in Appendix A (12), we conclude:
fβiαD
Qα (β) =	h-D (t)dt
Rich regime Next, we show that if gradient flow reaches a solution X βα,D (∞) = y, then
lima→o βα,D(∞) = βLι for any D. This is implied by the work of Arora et al. (2019a), but
we include it here for an alternative, simpler proof for our special case, and for completeness’s sake.
The KKT conditions for β = βLι are Xβ = y and ∃ν sign(β) = X>ν (where sign(0) = [—1,1]).
The first condition is satisfied by assumption. Define ν as above. We will demonstrate that the
second condition holds too in the limit as α → 0.
First, by Lemma 4, X>ν∞ ≤ 1 for all α and D. Thus, for any coordinates i such
that limα→0 [βα,D (∞)]i = 0, the second KKT condition holds. Consider now i for which
limα→0[βα,D(∞)]i > 0. As shown above,
—D	— D
lim [βα,D(∞)]i = lim αD(1 — [X>ν]J D-2 — αD(1 + [X>ν]J D-2 > 0
α→0	α→0
_ D
=⇒ lim αD(1 — [X>ν]i) D-2 > 0
α→0
(103)
(104)
This and [X>ν]i ≤ 1 implies limα→0[X>ν]i = 1, and thus the positive coordinates satisfy the
second KKT condition. An identical argument can be made for the negative coordinates.
19
Under review as a conference paper at ICLR 2020
Figure 5: The plots demonstrate the regimes where gradient flow implicitly minimizes nuclear norm
and `2 norm, respectively.
Kernel Regime Finally, we show that if gradient flow reaches a solution X βα,D (∞) = y, then
limɑ→∞ βα,D(∞) = βL2 for any D.
First, since X and y are finite, there exists a solution β* whose entries are all finite, and thus all the
entries ofβα,D(∞), which is the QαD-minimizing solution, will be finite.
The KKT conditions for β = βL2 are Xβ = y and ∃μ β = X>μ. The first condition is satisfied
by assumption. Defining ν as above, we have
I	— D	I	— D
lim	[βα,D(∞)]i = lim αD(1 -	[X>ν],	D-2	- αD(1	+	[X>ν]j	D-2	< ∞	(105)
lim [X>ν]i = 0
α→∞
Consequently, defining μ = 2D-D ν, and observing that for small z,
,	、 D ,	、 D	2D	- Q、
(1 - z)-K -(1 + Z)-K = jr^z + O(z3)
D-2
we conclude
D	D
lim [βɑ,D(∞)]i = lim αD (1 - [X>ν]i)-二-αD (1 + [X>ν]/二
α→∞ [X >μ]i	— α→0	[X >μ]i
lim
α→0
2D-D [X >ν ]i
1 + αli→m0O([X>ν]i2)
1
Thus, the KKT conditions are satisfied for limα→∞ βα,D (∞) = βL2.
(106)
(107)
(108)
(109)
(110)
(111)
□
D Matrix experiments
In the appendix, we provide additional results similar to those in Section 6. First, in Figure 5,
we plot the implicit regularization behavior of gradient flow limits with identity initialization
as in Figure 3(a): in “rich" regime (small α) we recover the minimum nuclear norm solution
MNN = argminpa(M)=y IlMk?, while “kernel" regime recovers the minimum Frobenius norm
solution ML@ = argminPω(m)=y IlM∣∣2
E	Neural Network experiments
Synthetic Experiments We construct a synthetic training set with N = 10 points drawn uniformly
from the unit circle in R2 and labelled by a teacher model with 1 hidden layer of 3 units. We train
fully connected ReLU networks with depths 2, 3, and 5 with 30 units per layer to minimize the
square loss using full gradient descent with constant stepsize 0.01 until the training loss is below
10-9 . We use Uniform He initialization for the weights and then multiply them by α.
Here, we describe the details of the neural network implementations for the MNIST and CIFAR10
experiments.
20
Under review as a conference paper at ICLR 2020
Ooooo
7 6 5 4 3
【％】JR①ISu
20
0
250 500 750 1000 1250 1500 1750 2000
Epoch
Figure 6: Training curves for the CIFAR10 experiments
MNIST Since our theoretical results hold for the squared loss and gradient flow dynamics, here
we empirically assess whether different regimes can be observed when training neural networks
following standard practices.
We train a fully-connected neural network with a single hidden layer composed of 5000 units on
the MNIST dataset, where weights are initialized as aw0, w0 〜N(0, Jnl-), %n denoting the
number of units in the previous layer, as suggested by He et al. (2015). SGD with a batch size of 256
is used to minimize the cross-entropy loss over the 60000 training points, and error over the 10000
test samples are used as measure of generalization. For each value of α, we search over learning
rates (0.5, 0.01, 0.05, . . . ) and use the one which resulted in best generalization.
There is a visible phase transition in Figure 4e in terms of generalization (≈ 1.4% error for α ≤ 2,
and ≈ 2.4% error for α ≥ 50), even though every network reached 100% training accuracy and less
than 10-5 cross-entropy loss. The black line indicates the test error (2.7%) when training only the
output layer of the network, as a proxy for the performance of a linear predictor with features given
by a fixed, randomly-initialized hidden layer.
CIFAR10 We trained a VGG11-like architecture, which is as follows: 64-M-128-M-256-256-M-
512-512-M-512-512-M-FC (numbers represent the number of channels in a convolution layers with
no bias, M is a maxpooling layer, and FC is a fully connected layer). Weights were initialized using
Uniform He initialization multiplied by α. No data augmentation was used, and training done using
SGD with batch size of 128 and learning rate of 0.0001. All experiments ran for 2000 epochs, and
reached 100% train accuracy except when training only the last layer, which reached 50.38% train
accuracy with LR = 0.001 (chosen after hyperparameter tuning).
In addition, to approximate the test error in the kernel regime, we experimented with freezing the
bottom layers and only training the output layer for both datasets (the solid lines in Figures 4e and
4f).
Figure 6 illustrates some of the optimization difficulties that arise from using smaller α as discussed
in Section 4.
21