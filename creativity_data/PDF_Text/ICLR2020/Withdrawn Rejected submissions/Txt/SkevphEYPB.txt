Under review as a conference paper at ICLR 2020
POP-Norm: A Theoretically Justified and
More Accelerated Normalization Approach
Anonymous authors
Paper under double-blind review
Ab stract
Batch Normalization (BatchNorm) has been a default module in modern deep net-
works due to its effectiveness for accelerating training deep neural networks. It
is widely accepted that the great success of BatchNorm is owing to reduction of
internal covariate shift (ICS), but recently it is demonstrated that the link between
them is fairly weak. The intrinsic reason behind the effectiveness of BatchNorm
is still unrevealed that limits it to be made better use. In light of this, we pro-
pose a new normalization approach, referred to as Pre-Operation Normalization
(POP-Norm), which is theoretically ensured to speed up the training convergence.
Not surprisingly, POP-Norm and BatchNorm are largely the same. Hence the
similarities can help us to theoretically interpret the root of BatchNorm’s effec-
tiveness. There are still some significant distinctions between the two approaches.
Just the distinctions make POP-Norm achieve faster convergence rate and better
performance than BatchNorm, which are validated in extensive experiments on
benchmark datasets: CIFAR10, CIFAR100 and ILSVRC2012.
1	Introduction
The last decade has witnessed significant advances in deep neural networks (DNN), which brought
substantial improvements for many real-world tasks, such as object(Szegedy et al., 2015; He et al.,
2015), scene(Khan et al., 2016; Guo et al., 2017) and action recognition(Feichtenhofer et al., 2017),
object detection(Ren et al., 2015; He et al., 2017b; Redmon & Farhadi, 2018) and image segmenta-
tion(Ronneberger et al., 2015; He et al., 2017a). BatchNorm (Ioffe & Szegedy, 2015) is a milestone
technique in the development of DNN, and nowadays it has become a default component to con-
struct modern deep networks. BatchNorm is widely-used in DNN due to that it is able to reduce
the sensitivity to initialization, significantly raise the learning rate, substantially speed up the train-
ing process and considerably improve the performance. Actually, along with this line a variety of
works, such as layer normalization (Ba et al., 2016), instance normalization (Ulyanov et al., 2016),
weight normalization(Salimans & Kingma, 2016) and group normalization (Wu & He, 2018), batch
renormalization, (Ioffe, 2017) were sequently proposed and also have made great success.
Nowadays, the practical success of BatchNorm is mainly attributed to reduction of internal covari-
ate shift (ICS) by controlling the first two moments (mean and variance) of the distributions of layer
inputs. Recently, this point of view is challenged by (Santurkar et al., 2018). It points out that
the link between reduction of ICS and performance improvement of BatchNorm is tenuous, at best.
However, the success of BatchNorm is indisputable, therefore there may exist a profound mathe-
matical mechanism behind BatchNorm. In fact, from the perspective of loss landscape, (Santurkar
et al., 2018) demonstrates BatchNorm will make the optimization landscape smoother, and then this
smoothness induces a more predictive and stable behavior of the gradients.
As stated above, the original theoretical foundation of the vanilla BatchNorm is not so solid and the
exact reason for its effectiveness is poorly understood, hence BatchNorm maybe not optimal in the-
ory and we might not make better use of it. In light of this, we try to propose a theoretically justified
normalization approach for accelerating training deep neural networks, so that we can continuously
further algorithmic progress. To achieve this goal, we firstly demonstrate that lowering gradient
Lipschitz constant and gradient variance is the key to boost the convergence rate of optimization
algorithms with stochastic gradient methods. And then We construct a normalization approach -
removing the mean of inputs and dividing by the scaled l2 norm of inputs before conducting the
1
Under review as a conference paper at ICLR 2020
main operation (convolution or inner-production) of a layer, which can be theoretically ensured to
reduce the gradient Lipschitz constant and gradient variance.
The new normalization approach and BatchNorm are similar. Hence we utilize their similarity to
interpret the effectiveness of the vanilla BatchNorm, along the line of lowering gradient Lipschitz
constant and gradient variance, which is different from the view of the loss landscape (Santurkar
et al., 2018). According to our theory, we can easily explain why BatchNorm with ReLU is powerful
with but ineffective with Sigmoid and Tanh as activation functions, which may help us to better
understand the underlying complexity of neural networks.
The main contributions of this paper are summarized as follows:
•	From scratch a theoretically justified normalization approach is deduced for accelerating
the convergence speed of training deep neural networks.
•	We theoretically explain the root of BatchNorm’s effectiveness and the weakness from the
perspective of lowering the gradient variance and gradient Lipschitz constant.
•	With the help of the new normalization approach, in extensive experiments the performance
for Sigmoid and Tanh achieve massive improvement, and they are even competitive to
ReLU.
Our paper is organised as follows. In Section 2, we will deduce the theoretical premises to accelerate
stochastic optimization. Then, built on the premise, we will propose a new normalization approach
in Section 3. In Section 4, we will analyze the exact root of BatchNorm’s effectiveness. In Section
5, we will implement extensive experiments to demonstrate the superiorly of the new normalization
approach. We will summarize our work and discuss further work in Section 6.
2	Theoretical premises
In this section, we will theoretically deduce that reduction of gradient Lipschitz constant and gradi-
ent variance plays a vitally important role in convergence when applying stochastic gradient descen-
t(SGD) algorithms for machine learning tasks.
In a machine learning task, given a set of samples {xi }in=1 , the empirical risk as the average of all
the samples loss is typically treated as the optimizing objective, i.e.,
1n
F(W) = F (WXxi}n=I) = nɪ^fi(w; Xi),	⑴
where fi(w; xi) is the loss incurred by the parameter vector w with respect to the i-th sample or
mini-batch. Noth that sometimes F (W; {xi}in=1) and fi(W; xi) may be abbreviated as F(W) and
fi(W).
In large-scale machine learning tasks, such as deep learning, we usually choose SGD to minimize
the empirical risk in Eq. (1) since its computational cost is cheap. Applying SGD, the parameter
update at k-th iteration is
Wk+1 - Wk - αk+ιVfik (Wk; xik)	(2)
where at the k-th iteration xik is randomly chosen from the full sets {xi}in=1, and αk+1 is the
learning rate.
Convergence guarantee of most gradient-based algorithms including SGD is built upon the assump-
tion of Lipschitz-continuous objective gradients, which is state as follow.
The function F(W) is continuously differentiable and the gradient function of F(W), namely
VF(w), is LiPschitz continuous with gradient LiPschitz constant L > 0, i.e.,
∣∣VF(wι) - VF(w2)k ≤ LkWI- W21∣.	(3)
An imPortant equality directly induced from Eq. (3) is
F(W2) ≤ F(wi) + hVF(W1),W2 - Wii + 2∣∣W2 - Wlk2.	(4)
2
Under review as a conference paper at ICLR 2020
The proof for it can be found in (Nesterov, 2004; Bottou et al., 2016).
Under the assumption in Eq. (4), we provide convergence analysis in the following for the general
objective function Eq. (1) when applying SGD, no matter whether it is convex or not.
Theorem 1 Suppose the assumption in Eq. (4) is satisfied and the gradient of F (wk ) in Eq. (1) is
bounded (i.e., VF(Wk) ≤ M ≤ ∞), SGD algorithm is applied to optimize F(Wk).
(1)	If the leaning rate satisfies	k∞=1 αk = ∞ and k∞=1 α2k < ∞, and then
PK=0 αk+ιE[kVF(wk)k2]	E[F(wo)] - Finf	L PK=o α^E[kVfik (Wk)k2] χ→∞ 0
LK	≤	LK	+	LK	→ 0，
k=0 αk+1	k=0 αk+1	k=0 αk+1
(5)
where Finf = minw F(w) . Then, kVF (wK)k -K--→-∞→ 0 in probability .
(2)	If the leaning rate satisfies 六 ≥ L and E∞=ι ɑk = ∞, we have
PK=0 αk+ιE[kVF(wk)k2]	E[F(w。)] - Finf	PK=o αk+ιV[Vfik(Wk)]
LK	≤	1 LK	+	LK	，	(6)
Tk=O ɑk+1	2 Tk=O αk+1	Tk=O ɑk+1
where V[Vfik(wk)] = E[kVfik (wk) - E[Vfik(wk)]k2] that is the gradient variance offik(wk).
When K → ∞,
inf(E[∣∣VF (Wk)Il2]) ≤ sup(V[V% (Wk)]).	(7)
kk
According to the first conclusion of Theorem 1, when the learning rate satisfies Pk∞=1 αk = ∞ and
Pk∞=1 α2k < ∞, we know that SGD algorithm will guarantee the empirical risk function in Eq. (1)
converges to a stationary point in probability. The conditions in the second conclusion of Theorem
1 is easier to be met. In this case we should make the the gradient variance V[Vfik (Wk)] as smallest
as possible to ensure the empirical risk function in Eq. (1) can converge to a good point.
Theorem 1 also implies that increasing learning rate will directly speed up convergence. Since larger
ak will make the term EP(KG)-Fin m Eq.(5)(or EIF(W))]-Finf in Eq. (6)) faster to diminish.
k=k=0 αk+1	2 Z^k=o ɑk+ι
However, when the learning rate αk becomes large, the term L2 PK=1 αE[kVfik(Wk)k2] in Eq.
(5) (or PkK=O αk+1V[Vfik (Wk)] in Eq. (6) ) may also become large and the condition ɑ⅛ ≥ L
may not be met. Hence reducing the gradient variance V[Vfik (Wk)] 1and the value of Lipschitz
constant L is the key to increase the learning rate and accelerate the convergence. It is worth noting
that the conclusion that gradient variance reduction is beneficial to accelerate convergence has been
demonstrated in (Johnson & Zhang, 2013) when the objective function is convex.
Therefore, if an normalization approach is effective to make Eq. (1) converge to a good point and
accelerate the convergence process, it needs to reduce gradient variance V[Vfik (Wk)] and gradient
Lipschitz constant L.
3 The Proposed Approach
In this section, we will present a new normalization approach of which each step will be theoretically
proved to be beneficial to reduce the gradient variance V[Vfik (Wk)] and the gradient Lipschitz
constant L simultaneously, therefore the new approach will accelerate the convergence of training
and improve the performance. 1
1It is known E[∣Nfik (Wk)∣∣2] = NBfik (Wk)] + (E[∣Nfik (Wk)k])2, decreasing Ngfik (wk)] will also
lower E[kVfik (Wk )k2].
3
Under review as a conference paper at ICLR 2020
When applying mini-batch SGD, at the k-th iteration the empirical risk loss with respect to the
weight/bias of the l-th fully-connected layer of DNN can be expressed as 2,
(l)	1 m	(l)	1 m	(l-1) t	(l)	(l)	(l-1) t l
F(m)(Wk ) = m 工	fik (wk ;	Xik )= ml^g y[xik	W	wk	+ bk = = G(m)	((x(m)kJ	Wk	+
(8)
where m is the size of mini-batch, and X((ml-)1) = [x(1l-1); x(2l-1); ...; x(ml-k 1)] and B((ml)) =
[b(1l) , b(2l) , ..., b(ml)k]. xi(l-1) is randomly choose from the entire train set {xi(l-1)}in=1 with e-
qual probability, hence F(Wkl)) = 1 Pn=1 f⅛(wkl);x(：T)) is the expectation of F(m)(wk)(
E[F(m)(wkl )] = F (wkl )), where n is the total number of train set. Note that Eq. (8) also implies that
F (WIk ) = G ((XkT)twk + Bk ) Where XkT = [x1-1; x2-1;…;xnk1 ] and Bk = [blik ,b2k ,…,bnj.
For clarity, henceforth we will omit the layer index.
We construct a neW normalization approach, referred to as Pre-Operation Normalization (POP-
Norm), of Which one main characteristic is putting the normalization technique before operation.
For a layer fed With a d-dimensional input xik = (x[i1] , x[i2] ..., x[id] ) , We ideally normalize each
dimension With the global estimation of entire samples, i.e.,
n
X[j] = Xj] _ Λj] ∙ 1 X Xj]
Xik = Xik η r,乙 Xik，
n i=1
X[j]
ʌ[j] _ __________xik___________
ik	/	^	、 2
ρj] ∙ Ji pn=ι ㈤)
[j]
Xik
√] kXj]k2
(Mean Removal)	(9)
(Division by sclaed l2 Norm)	(10)
where 0 ≤ η[j] ≤ 2, ρ[j] ↑Jn Pn=ι (Xij])2
[j]
≥ 1 and Xk
jk],X[2jk], ..., X[njk] ).
The normalization approach consists of two parts - mean removal and division by l2 norm. The
both steps are actually not interdependent and can individually take effects for decreasing gradient
variance and gradient Lipschitz constant. We will theoretically demonstrate it in the following.
Theorem 2 (Justification of Mean Removal in Eq.(9)) Eq. (9) is adopted to handle each input
Xik of the model in Eq. (8). Suppose assumption in Eq. (3) is satisfied and the gradient and Hessian
function of g(z) and G(Z) inEq. (8) is bounded, i.e., kVg(z)k2 ≤ Mi < ∞ and ∣N2G(Z)Il2 ≤
M2 < ∞ , we have
(1)	the upper bound of the gradient variance VVfk (Wk ； Xik)] Will be lower than that of
V[Vfik (Wk; Xik)].
(2)	the upper bound of the minimal gradient LiPschitz constant of F (Wk; {Xik }n=1) Will be lower
than that ofF (Wk; {Xik }in=1);
Theorem 3 (Justification of Division by l2 Norm in Eq.(10)) Eq. (10) is adopted to handle each
input Xik from Eq. (9). Suppose assumption in Eq. (3) is satisfied and the gradient and Hessian
function of g(Z) and G(Z) in Eq. (8) is bounded , i.e., IVg(Z)I2 ≤ M1 < ∞ and IV2G(Z)I2 ≤
M2 < ∞ , we have
(1)	the upper bound of the gradient variance VVfk (Wk ； Xik)] will be lower than that of
V[Vfik (Wk; Xik)];
(2)	the upper bound of the minimal gradient Lipschitz constant of F (Wk ； {^^ik }n=1) will be lower
than that of F (Wk； {XiJn=ι);
(3)	if F (Wk) is locally convex and Immaj((Pj]"
the lower bound of conditional
number of V2F (Wk ； {XiJn=ι) will be lower than that of V2F (Wk ； {XiJn=ι) in the local space.
2 A convolution layer can be also reformulated to be a matrix form , hence the analysis for convolution
layers is similar, but it is still somewhat complicated and we discuss it in the appendix.
4
Under review as a conference paper at ICLR 2020
Mam OPerat-On
Mean Remova-
D-V-S-on
(a)
POP-Norm
Sh5
DMS-On
(b)
Figure 1: (a) The structure of a standard layer with BatchNorm. (b) The structure of a layer with POP-Norm.
Due to the limited space, the proofs of Theorem 2 and Theorem 3 are provided in the appendix.
The computation of n En=I Xik and ɪ 5∑n=ι 22 in Eq.(9 - 10) over the entire training set is im-
practical when adopting SGD. Therefore, just like the vanilla BatchNorm, each mini-batch produces
ml Pn=I Xik and / Pn=I Xas the estimates instead. This implementation will also make all the
inputs fully participate in the gradient back-propagation. The vanilla BatchNorm introduces the s-
cale and shift parameter to be learned to restore the presentation power of the network. To make
POP-Norm more like the vanilla BatchNorm, we factorize η[j] and ρ[j] into a fixed parameter and a
parameter to be learned. In practice POP-Norm is presented as follows:
X[j] = X[j]
Xik = Xik
X [j] = Yj
Xik K
m
-m x χj]+βj],
i=1
X[j]
Xik
(Mean Removal)
r m p= (χij])2，
(Division by scaled l2 Norm)
(11)
(12)
where β[j] and γ[j] are the shift parameter and the scale parameter to be learned, and κ (≥ 1) is a
hyperparameter. If ∣β[j]| < m Pm=I Xj and ∣γ[j]| < κy m Pm=I (Xij]) are satisfied, Theorem 2
and Theorem 3 will still hold.
Remark 1. Although the main steps of POP-Norm in Eq.(11 - 12) are similar to the vanil-
la BatchNorm, they are still slight different, as shown in Figure 1. For BatchNorm, Xj] =
Y[j] (Xijk- μ[j] + β吟/Nr Pm=ι (Xik-μ[j])2 where μ = m1 Pm=I Xij], while for POP-Norm, Xij]=
Y[j] (χij] - μjι + β[j])∕Jmm Pm=I (Xij] - μ5 + e[j])2. Besides, there are other two significant differ-
ences between the proposed POP-Norm and the vanilla BatchNorm - placing normalization before
the main operation(convolution and inner-production) and adding a new scale parameter κ. Actu-
ally, we deliberately make POP-Norm more like the the vanilla BatchNorm. POP-Norm has other
forms, for example, mean removal in Eq.(11) can be Xj] = Xj — 1+β" Pm=I Xj], and the l2 norm
to divide can be substituted by the other norm, such as l1 norm and l∞ nrom. Mean removal and di-
vision by l2 norm of POP-Norm are actually not interdependent and can individually take effects for
decreasing gradient variance and gradient Lipschitz constant, and their order can be also exchanged.
From the proof of Theorem 2 and Theorem 3, we also know the division by l2 norm will lowering
the upper bound of V[Vfik (Wk)] and L more substantially than mean removal. Additionally, the
theoretical analysis and construction of POP-Norm are generalized, and we can also apply them
to construct a new normalization approach to compete with instance normalization (Ulyanov et al.,
2016) and group normalization (Wu & He, 2018).
Remark 2. Adding a new parameter κ and setting κ ≥ 1is more beneficial to decrease the gradient
variance and the gradient variance V[Vfik (wk)] and the gradient Lipschitz constant L. When look-
ing into the proof of Theorem 2 and Theorem 3, we can conclude the upper bound of V[Vfik (wk)]
and L will be proportional to 右,which means the larger K will be quickly smaller VVfik (Wk)]
and L so that speeds up the convergence. However, κ can not be too large, otherwise the value after
the operation will be much small, and then it may harm the representation capability of the layer,
5
Under review as a conference paper at ICLR 2020
since the non-linear activation function will mainly work in the linear regime. Moreover, when κ
is too large that will directly make gradient variance much small, and then it is difficult for train
loss to escape the trap of early local minimums. In our practices, when κ is moderately large, it
will substantially improve the performance, which is demonstrated in the experimental section. Not
strictly speaking, adding K is just make f (W) change to f (W). It seems the optimization trajectory
will be the same ifwe magnify the initial value ofw by an exact factor κ and scale the corresponding
learning rate . However, we do not exactly change the initial value and the learning rate, and the
optimum is not unique. On the contrary, the number of local minimums of a deep neural network is
much large and they scatter throughout all the space. Therefore, adding κ is more likely to help w
in the network quickly converge to a local minimum near the initial value according to Theorem 3
rather than along with a far trajectory converge to the original local minimum.
Remark 3. As proved in Theorem 3, adopting the POP-Norm will be conducive to reduction of the
condition number of V2F(Wk) if F(Wk) is locally convex. Smaller condition number will make the
training algorithm avoid the danger of running into a sudden change of the loss such as a flat region
or a steep slope, which also enable us to adopt a larger learning rate. In other words, the smaller
condition number will make the landscape or convergence trajectory of the loss is more smooth and
substantially speed up the training process.
4 Justifying Effectivenes s and Weaknes s of BatchNorm
The vanilla BatchNorm has been a standard toolkit for modern deep networks, but the root of Batch-
Norm’s effectiveness is still vague. The motivation of BatchNorm built on ICS is somewhat heuris-
tic. Actually, in (Santurkar et al., 2018) it demonstrates the link between the performance gain and
reduction of ICS is fairly weak, which means the success of BatchNorm may be not owing to ICS.
(Santurkar et al., 2018) owes faster training with BatchNorm to the improvement of loss landscape.
In this section, we will analyze not only BatchNorm’s effectiveness but also its weakness from the
perspective of lowering the gradient variance and gradient Lipschitz constant above.
BatchNorm and POP-Norm are similar, but there are still some significant distinctions between
BatchNorm and POPNorm. One of them is putting the normalization before the main operation
(convolution and inner-production) for POPNorm. As analyzed in the above subsection, each step
of POP-Norm has been theoretically justified. Therefore, we can analyze the influence of differences
between BatchNorm and POPNorm to verify the effectiveness and weakness of BatchNorm.
BatchNorm is placed behind the operation of convolution or inner-production and before the non-
linear activation function which is displayed in Figure 1, hence BatchNorm actually takes effect on
the operation of the next layer. For simplicity, we omit the shift and scale parameter here. Batch-
Norm and POP-norm can be expressed as:
n
χ[j] = x[j] _ 1 X x[j]
Xik = Xik - n Z^xik
i=1
Xik] = h(Xik])
(13)
(14)
(15)
where h(∙) is a activation function.
Due to nonlinearity of the activation function, the benefits of normalization will be impaired. When
it is ReLU, the damage will be not so serious. Suppose all samples {xij] }n=ι are symmetric about
zero that can be easy to be satisfied in practice, we have
*=h a]// X (Xik])2)=%(嘿/； 2 X h (/),	(15 i6)
hence when adopting ReLU as the activation function for BatchNorm, it is equal to taking the step
division by l2 norm with respect to h(xij]) for POP-Norm. In other words, no matter We put nor-
malization before or behind ReLU, division by l2 norm will be effective. From Remark 2, we know
6
Under review as a conference paper at ICLR 2020
in normalization mean removal and division by l2 norm can take effects individually and division
by l2 norm plays a major role and can still substantially reduce the gradient variance and gradient
Lipschitz constant. Therefore, the effectivness of BatchNorm with ReLU will be still guaranteed. On
the contrary, when the activation function is Tanh, it will damage division by l2 norm and reserve
mean removal, but the performance of BatchNorm with Tanh will be still undermined. When the
activation function is Sigmoid, it will destroy both of mean removal and division by l2 norm, hence
performance of BatchNorm with Sigmoid will be seriously deteriorated.
As we analyze in Remark 2, moderately increase the scale of the l2 norm will be beneficial to
decrease the gradient variance and gradient Lipschitz constant. It is known most of activation func-
tions will shrink the value of inputs, and then the value of normalization will become relatively
larger, hence from this perspective of view, activation function may be beneficial, but the deteri-
oration brought by distortion of activation functions may be still dominated for BatchNorm with
Sigmoid, but BatchNorm with ReLU will be benefited from the declined value of inputs.
5 Experiments
5.1	General settings
In the experiments, we first modify each layer in Figure 1(a) to Figure 1(b) for widely-used VGG-
nets (Karen Simonyan, 2016) and ResNets (He et al., 2016) with POP-Norm, and all other network
architecture and parameters were exactly the same with the standard VGG-nets and ResNets, and
then we evaluate the standard VGG-nets and ResNets and the new ones on benchmark dataset-
s: CIFRR10, CIFAR100 and ILSVRC2012. It is worth noting that we just want to evaluate the
performance and effectiveness of the proposed POPNorm, compared with BatchNorm, rather than
pursue the highest accuracy on benchmark datasets, hence we do not adopt much deep networks and
complicated tricks.
For the experiments on the datasets CIFAR10 and CIFAR100, the batch size is set as 128. We use
a weight of 0.0005 and SGD with a momentum of 0.9. To simplify the tuning process and compare
fairly, we identically start with a learning rate of 0.1, divided by 10 at 16k and 24k iterations, and
finally terminate at 30k iterations. For the experiments on ILSVRC2012, standard SGD with weight
decay of 0.0001 and momentum of 0.9 is adopted. the number of samples in a batch size is set as
256 uniformly distributed on 8 GPUs. The learning rate starts from 0.05 and is divided by 10 at
250k and 450k and 550k iterations, and finally terminate at 600k iterations. The images are first
simply resized to 256 × 256, and then randomly cropped to 224 × 224. No other data augmentation
except simple horizontal image flipping is employed.
5.2	Parameter Sensitivity Analysis
We add a scaling factor κ to magnify the value of normalization for POP-Norm. κ of POP-Norm
will greatly influence the effectiveness of POP-Norm. In this subsection, we will evaluate the perfor-
mance of VGG16 Embedding POP-Norm with different values of scaling factor κ from the candidate
set of {1, 4, 16}.
As shown in Figure 2(a-c), when κ is moderately large (κ = 4), it is not only beneficial to speed
up the training but also help train loss to finally converge to a smaller value. However, if we further
increase κ to 16, although the relative convergence speed is accelerated, the training loss is more
like to converge to a larger local minimum. The reason for it is just what we analyzed in Remark
2 of Section 3. Overlarge κ will make activation functions (Tanh and Sigmoid), work in the linear
regime, which will further harm the representative ability of neural networks, so that it is difficult for
training loss to find a better local minimum. Overlarge κ will directly lead to small gradient variance,
which will make train loss hard to escape the trap of an early local minimum. κ in POP-Norm will
be uniformly set to 4 in the following experiments for better performance.
POP-Norm speed up the convergence through reducing gradient Lipschitz constant (increasing
learning rate) and lowering gradient variance. We fix the learning rate in the experiments in this
subsection, hence it also experimentally validates reduction of gradient variance is conducive to
faster training convergence.
7
Under review as a conference paper at ICLR 2020
(a) Sigmoid
(b) Tanh
(c) ReLU
Figure 2:	Train Loss for POP-Norm embedded VGG16 with different activation functions on CIFAR100: (a)
Sigmoid, (b) Tanh and (c) ReLU.
(a) Sigmoid
(b) Tanh
(c) ReLU
Figure 3:	Train Loss for different normalization approaches embedded VGG16 with different activation func-
tions on CIFAR100: (a) Sigmoid, (b) Tanh and (c) ReLU.
5.3	Ablation Experiments
As analyzed in Remark 1 in Section 3, mean removal and division by l2-norm of POP-Norm can
work individually, in this section we will evaluate their performance of VGG16 on CIFAR100 in-
dividually and collectively. To better access their performance, we also add VGG with no nor-
malization and BatchNorm as anchors. We set the initial learning rate 0.00005 and 0.005 for no
normalization and POP-norm only with mean removal since a higher learning rate will make train-
ing diverge. Specifically, when the activation function is Sigmoid, no matter what value the initial
learning rate is, no normalization, POP-norm only with mean removal and POP-norm only with
division cannot make loss decrease. This phenomenon for no normalization was also reported in
(Ioffe & Szegedy, 2015). We guess this may be owing to gradient vanishing, and the mean removal
and division together can relief it but each individual component can not. We will explore the exact
reason in the future.
As shown in Figure 3, comparing with no normalization, mean removal and division by l2-norm
works can individually accelerate convergence mainly due to increasing learning rate, and it also
implies that division by l2-norm is more influential than mean removal in POP-Norm, which can
validate our analysis in Section 3. Moreover, when adopting ReLU as activation function, the per-
formance of BatchNorm is close to POP-Norm only with division. And when adopting Tanh and
Sigmoid, the convergence rate of BatchNorm is much slower than POP-Norm. These phenomenons
just follow the analysis in Section 4.
5.4	Classification Comparison
In this subsection, we will access the classification performance of BatchNorm and POP-Norm on
widely-used CIFAR10, CIFAR100 and ILSVRC2012. The results are reported in Table 1-2.
As displayed in Table 1, the classification accuracy for POP-Norm is consistently higher ( average
more than 3%) than BatchNorm. Specifically, when the activation functions are Sigmoid and Tanh,
8
Under review as a conference paper at ICLR 2020
the improvement of POP-Norm is more obvious, gaining about 8% and 4% respectively, which
demonstrates the effectiveness of POP-Norm. It is worth noting that with the help of the proposed
POPNorm the performance of the networks with Sigmoid and Tanh is fairly close to the networks
with ReLU, which also validates the power of the proposed approach.
Comparing to BatchNorm with ReLU, the performance for BatchNorm with Sigmoid and Tanh is
heavily discounted. The reason for it is what we analyzed in Section 4. When the activation func-
tion is ReLU, placing BatchNorm behind convolution and inner-production and before activation
functions will less influence the performance of normalization, but when adopting Sigmoid or Tanh,
this position will have a great impact for the performance of normalization. Therefore, from another
aspect, it verifies the importance of putting normalization before convolution and inner-production.
Table 1: Mean test accuracy(%) and standard deviation of 10 trials for BatchNorm and POPNorm embedded
VGG16 and ResNet20 on CIFAR10 and CIFAR100.
Model	Act. Fun.	CIFAR10		CIFAR100	
		BatchNorm	POP-Norm	BatchNorm	POP-Norm
	Sigmoid	84.21 ± 0.46	90.69 ± 0.15	62.49 ± 0.56	70.29 ± 0.14
VGG16	Tanh	88.69 ± 0.15	90.63 ± 0.09	66.63 ± 0.12	70.59 ± 0.09
	ReLU	90.89 ± 0.13	92.15 ± 0.11	70.83 ± 0.25	71.38 ± 0.12
	Sigmoid	83.08 ± 0.14	89.68 ± 0.14	54.10 ± 0.37	66.91 ± 0.32
ResNet20	Tanh	88.08 ± 0.11	89.63 ± 0.09	62.42 ± 0.36	67.02 ± 0.10
	ReLU	90.23 ± 0.13	90.90 ± 0.12	66.70 ± 0.16	67.53 ± 0.12
We also implement classification experiments on ILSVRC2012. As shown in Tabel 2, POP-Norm
achieves about 0.8% higher classification accuracy than BatchNorm, which demonstrates POP-
Norm is also effective in large-scale datasets. Due to the limited time, we did not utilize deeper
networks to evaluate BatchNorm and POP-Norm. We will report more results of deeper networks in
future.
Table 2: Test accuracy(%) for BatchNorm and POP-Norm embedded ResNet18 on ILSVRC2012. The activa-
tion function is ReLU.
Model	BatchNorm		POP-Norm	
	top-1	top-5	top-1	top-5
ResNet18	66.81一	87.32	67.53	87.95
6	Conclusions and Discussions
In this paper we try to propose a theoretically justified and effective normalization approach. To
achieve this goal, we first deduce that lowing gradient variance and gradient Lipschitz constant is
the key to speed up the convergence when adopting SGD. And then we present a new normalization
approach, of which each step is theoretically proven to be beneficial to reduce gradient variance and
gradient Lipschitz constant. POP-Norm and BatchNorm are largely the same, which just help us to
explain the effectiveness of BatchNorm. Besides the form of POP-Norm is slightly different from
BatchNorm, there are two other differences between the two normalization methods. One difference
is that the proposed method place the normalization ahead of the main operation of a layer rather
than behind it. Another is that the proposed method adds a scaling factor to increase the value of
normalization. Just the differences make our normalization approach more powerful, which is then
validated by extensive experiments on benchmark datasets. Moreover, POP-Norm can have other
forms that are much different from BatchNorm, and the principle of POP-Norm can be also applied
to instance normalization and group normalization.
References
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. arXiv preprint: 1607.06450v1,
2016.
9
Under review as a conference paper at ICLR 2020
Lon Bottou, Frank E. Curtis, and Jorge Nocedal. Optimization methods for large-scale machine learning. arXiv
preprint: 1606.04838, 2016.
Christoph Feichtenhofer, Axel Pinz, and Richard P. Wildes. Spatiotemporal multiplier networks for video
action recognition. In IEEE COnference on Computer Vision and Pattern Recognition, pp. 7445-7454, 2017.
Sheng Guo, Weilin Huang, Limin Wang, and Qiao Yu. Locally supervised deep hybrid model for scene recog-
nition. IEEE Transactions on Image Processing, 26(2):808-820, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-
level performance on imagenet classification. In IEEE Conference on Computer Vision and Pattern Recog-
nition, pp. 1026-1034, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
IEEE Conference on Computer Vision and Pattern Recognition, pp. 770-778, 2016.
Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask r-cnn. IEEE Transactions on Pattern
Analysis and Machine Intelligence, PP(99):1-1, 2017a.
Wenhao He, Xu Yao Zhang, Fei Yin, and Cheng Lin Liu. Deep direct regression for multi-oriented scene text
detection. In IEEE International Conference on Computer Vision, pp. 745-753, 2017b.
Sergey Ioffe. Batch renormalization: Towards reducing minibatch dependence in batch-normalized models. In
Advances in Neural Information Processing Systems, 2017.
Sergey Ioffe and Christian Szegedy. Batch normalization: accelerating deep network training by reducing
internal covariate shift. In International Conference on International Conference on Machine Learning,
2015.
R.	Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In
Advances in Neural Information Processing Systems, 2013.
Andrew Zisserman Karen Simonyan. Very deep convolutional networks for large-scale image recognition.
arXiv preprint: 1409.1556, 2016.
S Khan, M Hayat, M Bennamoun, F Sohel, and R Togneri. A discriminative representation of convolutional
features for indoor scene recognition. IEEE Transactions on Image Processing, 25(7):3372-3383, 2016.
Yurii Nesterov. Introductory lectures on convex optimization: A basic course. Springer Science + Business
Media, LLC, 2004.
Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement. arXiv preprint: 1804.02767, 2018.
S.	Ren, K. He, R Girshick, and J. Sun. Faster r-cnn: Towards real-time object detection with region proposal
networks. IEEE Trans Pattern Anal Mach Intell, 39(6):1137-1149, 2015.
Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image
segmentation. In International Conference on Medical Image Computing and Computer-Assisted Interven-
tion, pp. 234-241, 2015.
Tim Salimans and Diederik P. Kingma. Weight normalization: A simple reparameterization to accelerate
training of deep neural networks. In Advances in Neural Information Processing Systems, 2016.
Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch normalization help
optimization? (no, it is not about internal covariate shift). In Advances in Neural Information Processing
Systems, 2018.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan,
Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In IEEE Conference on
Computer Vision and Pattern Recognition, pp. 1-9, 2015.
Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing ingredient for
fast stylization. arXiv preprint: 1607.08022, 2016.
Yuxin Wu and Kaiming He. Group normalization. In European Conference on Computer Vision, 2018.
10
Under review as a conference paper at ICLR 2020
Appendix
A.	POP-Norm for convolution
When the layer is a convolutional layer, the matrix equivalence of a sample with respect to the
learning parameter wk is
	0 0	0	... 0	...	0 X[1,1] Xik	X[1,1] Xik	... X[1,2] Xik	...	0 0	...	0 X[d,1] ...	Xik	X[d,1] [dk,2] Xik	
xik =	... x[1,1] Xik x[1,2] Xik	...	... X[1,2] Xik	... X[1,3] Xik	...	... X[1,r-1] Xik [1,r] Xik	...	... X[1,r] Xik	... X[1,r+1] Xik	...	... X[d,1] Xik X[1,2] Xik	...	... X[d,r-1] ...	Xik X[d,r] ... Xik	... X[d,r] Xik X[d,r+1] Xik	,
	... X[1，S-1] Xik	...	... X[1,s] Xik	...	... 0	...	... 0	...	... X[1,s-1] Xik	...	... ...	0	... 0	
	[1,s] Xik	0	...	0	0	...	X[d,s] Xik	...	0	0	(17)
where the number of channels is d, the number of spatial dimensions is s, the size of kernel is r. To
make the expression legible, we present the matrix equivalence of input with one-dimensional spatial
locations. Actually, when s = 1 and r = 1, the convolution layer is degraded to a fully-connected
layer.
Theorem 2 and Theorem 3 require each row vector of xik in a convolutional should be considered
as a atom that is like a sample in a fully-connected layer, so POP-Norm for a convolution layer is
ns
χik,q] = χj]- η[j]∙n1s XXx[ij,q] ,	(Mean Removal)	(18)
i=1 q=1
Xj]
Xj,q] =------- ik	,	(Divisionbysclaed l2 Norm)	(19)
M	ρ[j]∙ Jn⅛P乙PL (xik,q])2
B.	Proof of Theorem 1
Proof 1 (1) At the k-th iteration, we have
F(wk+ι) ≤ F(Wk) + EFIwk),wk+ι - Wki + 2∣∣wk+ι - wk∣∣2.	(20)
We know wk+1 = wk — αk+ιVfifc (wk) in Eq. (2), so Eq. (20) Canbeformulatedas
F (wk+1) ≤ F (wk) - αk+ι hVF (wk), Vfik (wk )i + Lαk+1 kVfifc (wk )∣∣2.	(21)
Since at each iteration the sample or mini-batch is randomly chose from the full set, Vfik (wk) is
an unbias estimate of VF(w), i.e., E[Vfik (w)] = VF (w). Hence taking expectation on the k-th
sample or mini-batch, we have
E[F (wk+1)] ≤ F (wk) - αk+ιkVF (w)∣∣2 + Lαk+1 E[kVfifc (wk )『].	(22)
Rearranging terms yields
αk+ιkVF (w)k2 ≤ F (wk) - E[F (wk+1)] + Lαk+1 E[kVfifc Iwk )k2],	(23)
Taking the total expectation and summing over 0 to K, we obtain
KK
Xak+ιE[kVF(wk)k2] ≤ E[F(wo)] - E[F(Wk+1)] + E X。2+画|^九(wk)k2].	(24)
k=0	k=0
11
Under review as a conference paper at ICLR 2020
Dividing both sides of Eq. (24) with PkK=0 αk+1 and combining Finf ≤ F(wk), we have
PK=0 αfc+ιE[kVF(wk)k2]	E[F(wo)] - Finf	L PK=o。2+^^^(wfc)k2]
LK	≤ LK	+	LK
k=0 αk+1	k=0 αk+1	k=0 αk+1
(25)
Since VF (w) is bounded, Vfik (w) will be bounded, and then E[kVfik (wk)k2] should be also
bounded. It is known	k∞=1 αk= ∞ and	k∞=1 α2k< ∞, so when K tends to infinity, we abtain
PK=o αk+ιE[∣∣VF(Wk)k2] K→∞
Kκ	→0
k=0 αk+1
(26)
It is known that k∞=1 αk = ∞, hence for any > 0, there should exists a number N ∈ N,
so only sparse numbers k > N satisfies that E[kVF(wk)k2] > , otherwise when K → ∞,
Pk=0 αpκE[kvF(wk)k ] T ∞ that contradicts Eq. (26). Therefore almost everywhere
k=0 αk+1
E[kVF(wk)k2] -K-→--∞→ 0.	(27)
And then for any ζ > 0,
P[kVF (Wk )k] >Z ]= P[kVF (Wk )k2 ] >Z 2] ≤ E[kVF 2wk)k2] -→∞ 0	(28)
where the inequality holds due to Markov’s Inequality, which means kVF (WK )k -K-→--∞→ 0 in prob-
ability.
(2) Since * ≥ L andEq. (22) holds, we have
E[F(wk+ι)] ≤ F(Wk) — αfc+ιkVF(w)∣∣2 + 号E[∣∣Vfik(Wk)『].	(29)
ItisknownE[kVfik(Wk)k2] = E[kVfik(Wk)-F(W)k2]+kF(W)k2 = V[kVfik(Wk)k]+kF (W)k2,
so Eq. (29) can be reformulated as:
¥kVF(W)k2 ≤ F(Wk) — E[F(Wk+ι)] + Lαk+1 V[kVfik(Wk)k]	(30)
Taking the total expectation and summing over 0 to K, we have
KK
2 X ɑk+ιE[kVF(Wk)k2] ≤ E[F(wo)] — E[F(Wκ+ι)] + X αk2+1 V[kVfik(Wk)k].	(31)
k=0	k=0
Dividing both sides ofEq. (31) with 1 PK=O αk+ι and combining Finf ≤ F(Wk), we have
PK=O αk+ιE[∣∣VF(Wk)k2] ≤ E[F(wo)] — Finf + PK=O αk+ιV[kVf,k(Wk)k]
v-'∖K	—	1 V^K	V^K	,
k=O=αk αk + 1	2 2^∕k=0 αk+1	k=O=αk αk+1
(32)
It is known P∞ 1 αk = ∞, hence when K → ∞ , we have EFwK"-Finf T 0. We know
k=1	2 工k = 0 ɑk+1
E[kVF(Wk)Il2] ≥ infk (E[∣∣VF(Wk)k2]) andV[kVfik(Wk)k] ≤ Supk (V[kVfik(Wk)k]), andthen
we obtain
哽(E[∣∣VF(wo)k2]) ≤ Sup (V[∣∣Vfik(wo)k]),
kk
and finally we arrive at the desired result.
(33)
12
Under review as a conference paper at ICLR 2020
C. Proof of Theorem 2
Proof 2 (1). From Eq. (8), we know fik (wk; xik) = g ((xik)t wk + bk), and then
Vfik(wk； Xik) = Xik Vg ((Xik)tWk + bk) ,	(34)
hence
V [kVfik (wk； Xik)k] =E kVfik (wk； Xik)k2 - (E [kVfik (wk； Xik)k])2
≤E kVfik(wk；Xik)k2
=E [kxik Vg ((Xik)twk + bk) k2]	(35)
≤e [kxikk2kvg ((Xik)twk + bk) k2]
≤M1E[kXikk2],
where the second inequality holds due to the fact kABk2 ≤ kAk2 kBk2, and the third inequality
follows kVg(z)k2 ≤ M1 < ∞.
Similarly, we conclude that
V [kVfik (wk； Xik )k] ≤ MιE[kXik k2]
(36)
Recalling Eq. (9), we have
d
E[kXik k2]= E X ∣∣Xij]
j=1
d
XE ∣∣∣
j=1
X eB∣
d
XE ∣∣∣
Xik] ∣∣2
j=1
d
W- " Xif
X[ijk] - η[j] E hX[ijk]i∣∣∣2
(37)
E
j=1
d2
E[kXik k2] - X η[j](2 - η[j]) E ∣∣X[ijk]∣∣	,
j=1
From 0 ≤ η[j] ≤ 2 in Eq. (9), we know 0 ≤ η[j] (2 - η[j] ) ≤ 1, and then
(E[kXikk])2 <E[kXikk2]「
E[∣∣Xik∣∣2] ≤ E[∣∣Xik∣∣2] ≤
(38)
Hence we arrive the conclusion that mean removal in Eq. (9) will make the upper bound of of
gradient variance VVfik (Wk； Xik)] is lower than that of VVfik (wk； Xik)]∙
(2). If Eq. (3) holds, the following inequality can be obtained due to Lagrange Mean Theorem, i.e.,
Linf = kV2F (wk；{Xik}in=1) k2,	(39)
where Linf is the minimal gradient Lipschitz constant .
FollowingEq. (8),we know F (wk； {XiJn=J = G (Xk Wk + Bk), and then the Hessianfunction of
F(wk) is
V2F (wk; {Xik }i=ι) = Xk V2G (XkWk + Bk) Xt.	(40)
13
Under review as a conference paper at ICLR 2020
And then we obtain
kV2F (Wk； {Xik}n=ι) k2 = IlXkV2G (XkWk + Bk) Xt∣∣2
= ∣∣v2g (Xk Wk + Bk) Xt Xk ∣∣2
≤∣∣v2g (Xk Wk + Bk)∣∣2∣∣Xt Xk∣∣2
= ∣∣V2G (Xk Wk + Bk )∣∣2∣∣XkXt∣∣2
≤M2∣∣XkXkt∣∣2
≤M2dmax Hk[j,j]	,
(41)
where d is the dimension of each sample xik and Hk[j,j] are the j-th diagonal element of (Xk Xkt);
the second and fourth equalities hold due to the fact kAB k2 = kBAk2 ; the third inequality follows
kABk2 ≤ kAk2 kBk2; the fifth inequality is owing to kv2G(z)k2 ≤ M2, and the last inequality is
due to the fact kAk2 ≤ d maxj (A[j,j]) where A ∈ Rd×d is positive definite and A[j,j] is the j-th
diagonal element.
Similarly, we can derive that
kV2F (Wk； {Xik}n=ι) k2 ≤ M2dmax (Hjj]) ,	(42)
where Hiaj is j-th diagonal element of (XkXk) in which Xk is constructed by following Xk in Eq.
(8).
[j,j ]
According to the definition ofXk in Eq. (9), Hk , can be reformulated as
Hkjj]
(43)
From 0 ≤ η[j] ≤ 2 in Eq. (9), we know 0 ≤ η[j] (2 - η[j] ) ≤ 1, and then
1-
H j,j]
Hk
—k— ≤ 1
H jj] 一
Hk
(44)
Therefore the upper bound of the gradient Lipschitz constant Linf ofF (Wk; {xHik}in=1) is lower that
ofF (Wk; {xik}in=1).
D. Proof of Theorem 3
Proof 3 (1)Similar to Eq.(34 - 35), it can be also concluded that
V Vfik(Wk； Xik)] ≤ MlE[kXikk2]
14
(45)
Under review as a conference paper at ICLR 2020
According to Eq.(10), we have
d
2
E[kXikk2]= XE 愀]|
j=1
d
XE
j=1
(ρj])2
n
HI	xik]
M ρj]∙ r n Pn=I 3)2
1	/ ,、、2E[kxikk2]
pn=ι ©))
(46)
2
Hence,
E[kxik k2] =	1	1	≤ ι
…=W 1 Pn=I (x(j))2 ≤
(47)
where the inequality holds since (Pn') Pn=ι (xij)) ≥ 1.
Therefore the upper bound of the gradient variance V[Vfik (wk； Xik)] will be lowered than that of
V[Vfik (Wk ； xik)].
(2)	. Similar to Eq. (35 - 40), we conclude
llv2F (Wk ； {xik }in=1) k2 ≤ M2dmax (Hjj]),
(48)
where H jj] is j-th diagonal element of (Xk Xk) in which Xk is constructed by following Xk in Eq.
(8).
According to Eq.(10), we have
H jj]
i=1
i=1
xik]
∖2
[ρj] ∙卷 pn=ι W) 2
(49)
/
Hence,
Hjj]
H* = (ρj])2
------1----2 ≤ 1,
1 pn=ι /)
(50)
where the inequality holds since
(ρj])2
pn=ι /)2 ≥ ι.
n
1
Hence the upper bound of the minimal gradient Lipschitz Constant of F (Wk; {Xik }n=ι) will be fur-
ther lowered than that of F (wk； {XiJn=ι) ∙
(3)	. If F (Wk; {Xik }n=ι) is locally strongly convex, the condition num G (Z) is also strong convex,
and then
λmin(G(Z)) > M > o
λmax(G(z)) >	> ,
where λmaχ(∙) is the maximal eigenvalue and λmin(∙) is the minimal eigenvalue.
(51)
15
Under review as a conference paper at ICLR 2020
the local space the condition number of V2F(c)(wk; {xik }n=ι) is
Cond(V2F (Wk; {xik}i=1))=kV2F (Wk; {xik}i=ι) ∣2∣ (V2F (wk；伍」乜))-1 ∣∣2
=IXkV2G (XkWk + Bk) Xk ∣2 k (XkV2G (XkWk + Bk) Xk)-112
=∣v2g (Xk Wk+Bk) Xk Xk ∣2∣v2(G (Xk Wk+Bk ))-1 (XkXk)-1∣2
∖λmin(G) λmaχ(XkXk )
≥•
≥ λmaχ(G) λmm(XkXtk )
≥M λmax(XkXk )
一 λmin(XkXk )
>Mmaxj (Hj")
一minj (Hjj])，
(52)
where Hj,j] is the j-th diagonal element of (XkXk); The third equalityfollows IlABk2 = ∣∣BAk2；
the forth inequality holds due to the fact ∣∣AB∣∣2 ≥ d Tr(AB) ≥ λmin(A)λmaχ(B) if A and B are
positive definitive.
Similarly, we have
maxj (Hjj)
Cond(V2F (Wk XxiJn=I)) ≥ M mmj (Hj
(53)
Recalling Eq. (10), we obtain
/
2
i=1
i=1
[j]
xik
ρρj]∙ ∕i∑MxjΓ)
(54)
It is known maxj((p[j])2) ≤
Itisknown (minj(ρ[j])2) ≤
maxj A=包?? = maxj j, hence we have
minj (Pn=I(xik])2)	minj (Hkj"])
maxj
minj
(55)
FinallyWe arrive the conclusion that the lower bound ofconditional number of V2F (Wk； {xik }n=1)
will be lowered than that of V2F (Wk ； {xik }n=1) in the local space. □
16