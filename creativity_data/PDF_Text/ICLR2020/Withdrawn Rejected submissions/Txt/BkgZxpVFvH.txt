Under review as a conference paper at ICLR 2020
LSTOD: Latent Spatial-Temporal Origin-
Destination prediction model and its appli-
cations in ride-sharing platforms
Anonymous authors
Paper under double-blind review
Ab stract
Origin-Destination (OD) flow data is an important instrument in transportation
studies. Precise prediction of customer demands from each original location to
a destination given a series of previous snapshots helps ride-sharing platforms
to better understand their market mechanism. However, most existing prediction
methods ignore the network structure of OD flow data and fail to utilize the topo-
logical dependencies among related OD pairs. In this paper, we propose a latent
spatial-temporal origin-destination (LSTOD) model, with a novel convolutional
neural network (CNN) filter to learn the spatial features of OD pairs from a graph
perspective and an attention structure to capture their long-term periodicity. Ex-
periments on a real customer request dataset with available OD information from
a ride-sharing platform demonstrate the advantage of LSTOD in achieving at least
7.94% improvement in prediction accuracy over the second best model.
1	Introduction
Spatial-temporal prediction of large-scale network-based OD flow data plays an important role in
traffic flow control, urban routes planning, infrastructure construction, and the policy design of ride-
sharing platforms, among others. On ride-sharing platforms, customers keep sending requests with
origins and destinations at each moment. Knowing the exact original location and destination of
each future trip allows platforms to prepare sufficient supplies in advance to optimize resource uti-
lization and improve users’ experience. Given the destinations of prospective demands, platforms
can predict the number of drivers transferring from busy to idle status. Prediction of dynamic de-
mand flow data helps ride-sharing platforms to design better order dispatch and fleet management
policies for achieving the demand-supply equilibrium as well as decreased passenger waiting times
and increased driver serving rates.
Many efforts have been devoted to developing traffic flow prediction models in the past few decades.
Before the rise of deep learning, traditional statistical and machine learning approaches dominate
this field (Li et al., 2012; Lippi et al., 2013; Moreira-Matias et al., 2013; Shekhar & Williams, 2007;
Ide & Sugiyama, 2011; Zheng & Ni, 2013). Most of these models are linear and thus ignore some
important non-linear correlations among the OD flows. Some other methods (Kwon & Murphy,
2000; Yang et al., 2013) further use additional manually extracted external features, but they fail to
automatically extract the spatial representation of OD data. Moreover, they roughly combine the
spatial and temporal features when fitting the prediction model instead of dynamically modelling
their interactions.
The development of deep learning technologies brings a significant improvement of OD flow predic-
tion by extracting non-linear latent structures that cannot be easily covered by feature engineering.
(Xingjian et al., 2015; Ke et al., 2017; Zhou et al., 2018). Zhang et al. (2016; 2017) modeled
the whole city are as an entire image and employed residual neural network to capture tempo-
ral closeness. Ma et al. (2017) and Yu et al. (2017) also learned traffic as images but they used
LSTM instead to obtain the temporal dependency. Yao et al. (2018b) proposed a Deep Multi-View
Spatial-Temporal Network (DMVST-Net) framework to model both spatial and temporal relation-
ships. However, using standard convolution filters suffers from the problem that some OD flows
covered by a receptive field of regular CNNs are not spatially important. Graph-based neural net-
1
Under review as a conference paper at ICLR 2020
works (GNN)(Kipf & Welling, 2016; Defferrard et al., 2016; VelickoVic et al., 2017) are proved to
be powerful tools in modelling spatial-temporal network structures (Yu et al., 2018; Li et al., 2017).
However, none of these frameworks are directly applicable here since both the historical observa-
tions and responses to predict are vertex-level variables. On the contrary, the OD flows we discuss
in this paper are generated in the edge space by our definition.
The aim of this paper is to introduce a hierarchical Latent Spatial-Temporal Origin-Destination
(LSTOD) prediction model to jointly extract the complex spatial-temporal features of OD data by
using some well-designed CNN-based architectures. Instead of modelling the dynamic OD networks
as a sequence of images and applying standard convolution filters to capture their spatial informa-
tion, we introduce a novel Vertex Adjacent Convolution Network (VACN) that uses an irregular
convolution filter to cover the most related OD flows that share common vertecies with the target
one. The OD flows connected by common starting and/or ending vertexes, which may fall into dif-
ferent regions of the flow map, can be spatially correlated and topologically connected. Moreover,
for most ride-sharing platforms, a passenger is more likely to send a new request from the location
where his/her last trip ends in. To learn such sequential dependency, we introduce a temporal gated
CNN (TGCNN) (Yu et al., 2018) and integrate it with VACN by using the sandwich-structured ST-
conv block in order to collectively catch the evolutionary mechanism of dynamic OD flow systems.
A periodically shifted attention mechanism is also used to capture the shift in the long-term period-
icity. Finally, the combined short-term and long-term representations are fed into the final prediction
layer to complete the architecture. Our contributions are summarized as follow:
•	To the best of our knowledge, it is the first time that we propose purely convolutional
structures to learn both short-term and long-term spatio-temporal features simultaneously
from dynamic origin-destination flow data.
•	We propose a novel VACN architecture to capture the graph-based semantic connections
and functional similarities among correlated OD flows by modeling each OD flow map as
an adjacency matrix.
•	We design a periodically shift attention mechanism to model the long-term periodicity
when using convolutional architecture TGCNN in learning temporal features.
•	Experimental results on two real customer demand data sets obtained from a ride-sharing
platform demonstrate that LSTOD outperforms many state-of-the-art methods in OD flow
prediction, with 7.94% to 15.14% improvement of testing RMSE.
2	Definitions and Problem S tatement
For a given urban area, we observe a sequence of adjacency matrices representing the OD flow maps
defined on a fixed vertex set V , which indicates the N selected sub-regions from this area. We let
V = {v1, v2, . . . , vN} denote the vertex set with vi being the i-th sub-region. The shape of each
grid vi could be either rectangles, hexagons or irregular sub-regions. We define the dynamic OD
flow maps as {Od,t}, where d = 1, . . . , D and t = 1, . . . , T represent the day and time indexes,
respectively. For each snapshot Od,t = (oidj,t) ∈ RN×N, the edge weight oidj,t at row i and column
j denotes the flow amount from node vi to node vj at time t of day d. A larger edge weight oidj,t is
equivalent to a strong connection between nodes vi and vj . The Od,ts’ are asymmetric since all the
included OD flows are directed. Specifically, oidj,t = 0 if there is no demand from vi to vj within the
t-th time interval of day d.
The goal of our prediction problem is to predict the snapshot Od,t+j ∈ RN×N in the fu-
ture time window (t + j) of day d given previously observed data, including both short-term
and long-term historical information. The short-term input data consists of the last p1 times-
tamps from t + 1 - p1 to t, denoted by X1 = {Od,t+1-p1, Od,t+1-p1+1, . . . , Od,t}. The
long-term input data is made UP of q time series {Od-φ,t+j-(p2-i)∕2,..., Od-φ,t+j+(p2-i)∕2}
of length p for each previous day (d -夕)with the predicted time index (t + j) in the
middle for 2=1,...,q. We let X = {Od-q,t+j-(p2-i)∕2,..., Od-q,t+j+(p2-i)∕2,∙∙∙,
Od-1,t+j-(p2-1)/2, . . . , Od-1,t+j+(p2-1)/2 } denote the entire long-term data. Increasing p1 and
p2 leads to higher prediction accuracy, but more training time.
2
Under review as a conference paper at ICLR 2020
We reformulate the set of short-term OD networks X1 into a 4D tensor X1 ∈ RN×N×p1×1 and con-
catenate the long-term snapshots X2 into a 5D tensor X2 = (X2,d-1, . . . , X2,d-q ∈ Rq×N×N×p2×1.
Each X2,d-∕ ∈ RNXN×p2×1 for day d - φ is a 4D tensor for φ = 1,..., q. Therefore, We can
finally define our latent prediction problem as follows:
od,t+j = F(X1,X2),	(1)
where F(∙, ∙) represents the LSTOD model, which captures the network structures of OD flow data
as Well as the temporal dependencies in multiple scales. A notation table is attached in the appendix.
3	LSTOD Framework
In this section, we describe the details of our proposed LSTOD prediction model. See Figure 1 for
the architecture of LSTOD. The four major novelties and functionalities of LSTOD model include
•	an end-to-end framework LSTOD constructed by all kinds of CNN modules to process
dynamic OD flow maps and build spatio-temporal prediction models;
•	a novel multi-layer architecture VACN to extract the network patterns of OD flow maps by
propagating through edge connections, which can not be covered by traditional CNNs;
•	a special module ST-Conv block used to combine VACN and gated temporal convolutions
to coherently learning the essential spatio-temporal representations;
•	a periodically shifted attention mechanism which is well designed for the purely convo-
lutional ST-Conv blocks to efficiently utilize the long-term information by measuring its
similarities with short-term data.
Figure 1: The architecture of LSTOD consisting of (C1) ST-
Conv blocks, (C2) TGCNN, (C3) VACN, and (C4) attention.
Figure 2: An example to show how
standard CNN fails to capture the
network structure of OD flows
3.1	Spatial Adjacent Convolution Network
Before introducing the detailed architecture of VACN, we want to discuss why directly applying
standard CNNs to the OD flow map Od,t may disregard the connections between neighboring OD
flows in the graph space first. Figure 2 demonstrates that it fails to capture enough semantic infor-
mation using the real-world example of ride demands. For the OD flow starting from v1 to v2 , as
illustrated in the upper sub-figure, the most related OD flows should be those with either origin or
destination being v1 or v2 in the past few timestamps. A certain part of the travel requests from v1 to
v2 can be matched with some historical finished trips from a third-party location to V1 by the same
group of people, for example a trip from v3 to v1 . However, as the lower-left sub-figure illustrates,
some of the OD flows covered by a single CNN filter (the green square) such as the four corners of
the kernel window may be topologically far away from the target one in the graph.
More generally, let’s consider a target OD flow oidj,t in a map Od,t. Most of the components covered
by a standard CNN of size q × q with oidj,t in the middle such as o(di, +t 1)(j+1) are less correlated than
some out of the kernel window, for example odk,it when |k -j| > (q + 1)/2. Moreover, ifwe change
3
Under review as a conference paper at ICLR 2020
the order of the N vertexes in Od,t, then the network structure is unchanged, but a different set of
OD flows is covered by the receptive field of the same size with the center being oidj,t. As shown
in the lower right sub-figure of Figure 2, OD flows with either origin or destination being vi or vj ,
covered by the red and yellow kernel windows, are considered to be the most related ones for oidj,t in
row i and column j .
To better understand the differences between our proposed VACN over graph edges with those
vertex-based convolutions such as GCN (KiPf & Welling, 2016) or GAT (VelickoVic et al., 2017),
we introduce the concept of line graphs L(G) (Godsil & Royle, 2013). Each node in L(G) corre-
sPonds to an edge in G, while each individual edge in L(G) can be maPPed to a Pair of edges in G
that connect to ajoint vertex. L(G) contains 1 [N3 一 N(N 一 1)] while there are only 1N(N 一 1) for
G. Thus, aPPlying GNNs over L(G) is memory intensive. Our edge-based convolution oPerations
VACN is much more efficient.
Formally, every VACN layer takes a 3D snaPshot Ad,t = {aidj,t} ∈ RN×N×m0 consist ofm0 feature
maPs as the inPut with each aidj,t ∈ Rm0 rePresenting a feature vector at the edge from vi to vj. The
learned rePresentation of each target edge is defined as the weighted sum of those from the same row
or column in the adjacency matrix, and those from the row or column in the transPosed adjacency
matrix. The outPut of the layer-wise VACN ProPagation for the OD flow from vi to vj is defined as
follows:
N
F{XR1naidn,t+C1nadn,jt+R2nadn,it+C2najdn,t}∈Rm1,	(2)
n=1
where [R1n,R2n,C1n,C2n] = Wn ∈ R4m1×m0 such that [W1, . . . ,WN] ∈ R4m1×m0×N are the
weights to learn. F(∙) represents an elementwise activation function, such as ReLU(X) = max(0, x).
The first Part of (2) works by summing uP the feature values of OD flows having either the same
origin or destination with the target OD flow. The second part covers another set of OD flows
that either start at vj or end at vi . Similar to standard CNNs, different OD flows are treated with
unequal importance by VACN and edges more related to the target one are assigned higher weights
R1n, R2n, C1n or C2n. We denote the overall output of a multi-layer VACN as Yd,t ∈ RN ×N ×ms at
time (d, t). Figure 4 in the appendix more clearly illustrates the architecture of VACN.
3.2	Temporal Gated CNN
We use temporal gated CNN (TGCNN) (Yu et al., 2018) instead of RNN-based architectures such
as LSTMs to capture the temporal representation, which makes our LSTOD a pure convolutional
architecture. RNNs suffer from the problem of lower training efficiency, gradient instability, and
time-consuming convergence. Moreover, the high dimension of the spatial representations captured
by VACN and a potential long temporal sequence length make RNNs notoriously difficult to train.
The CNNs is more flexible in handling various data structures and allows parallel computations to
increase training speed.
TGCNN consists of two parts including one being a 3D convolution kernel applied to the spatial
representations of all the N2 OD flows along the time axis and the other being a gated linear unit
(GLU) as the gate mechanism. By employing VACN at each of r successive timeslots, we can build
a 4D tensor Y = (Yd,t) ∈ RN×N×r×ms which is then fed into the TGCNN operator:
G *γ Y = Yi Θ σ(Y2) ∈ RN×n×(r-K+1)×mt,	(3)
where G*γ represents the TGCNN kernel and Y includes all the related parameters to learn. 2mt 3D
convolutional kernels of size (1 × 1 × K) with zero paddings map the input Y into a single output
[Y1 Y2 ] ∈ RN×N×(r-K+1)×2mt, which is split in half to obtain Y1 and Y2 with the same number
of feature channels. Θ here denotes the element-wise Hadamard product.
3.3	ST-Conv blocks
We use the spatial-temporal convolutional block (ST-conv block) to jointly capture the spatial-
temporal features of OD flow data, which has a ‘sandwich’-structure architecture with a multi-layer
VACN operator in the middle connecting the two TGCNNs. The use of ST-Conv blocks have two
4
Under review as a conference paper at ICLR 2020
major advantages. First, the block can be stacked or extended based on the dimension and character-
istic of the spatio-temporal input data. Second, a temporal operation is applied before extracting the
spatial information, which greatly reduces its computation complexity and memory consumption.
Both the input and output of each individual ST-Conv block are 4D tensors. For the input of the
l-th block Zl-1 ∈ RN×N×rl-1 ×cl-1 (Z0 is the original the OD flow data with c0 = 1), the output is
computed as follows:
Zl = Gi *γi [S *θi {G0*γ∣1 Zl-1}],	(4)
where Gι*γi and Go*γ∣ are two TGCNN kernels and S*^ι is a multi-layer VACN operator being
applied to each timestamp. The G1 and G2 operators from all the stacked ST-Conv blocks employ
the same kernel sizes, which are (1 × 1 × K1) and (1 × 1 × K2), respectively. Thus, we have
rl = rl-1 - (K1 + K2 - 2). After applying (r0 - 1)/(K0 + K1 - 2) ST-Conv blocks to the input
Z0, the temporal length is reduced from r0 to 1.
When input being the short-term OD flow data Z0 = X1 ∈ RN×N×p1×1, we use L0 = (p1 -
1)/(KS0T + KS1T - 2) blocks to obtain the spatial-temporal representations ZST = f (ZL0 ) ∈
RN ×N ×cST . f here squeezes the captured 4D output into a 3D tensor by dropping the temporal
axis. The kernel sizes of each G1 and G0 are (1 × 1 × K0ST) and (1 × 1 × K1ST), respectively. The
detailed propagation of the l-th ST-Conv block is defined as
Zl = G1 * ι1 [S *θι {G0 * ι0 Zl-1}],	(5)
γST	θST	γST
3.4	Periodically Shifted Attention Mechanism
In addition to capturing the the spatial-temporal features from short-term OD flow data X1, we also
take into account the long-term temporal periodicity due to the potential day-wise cycling patterns
insides the OD flow data, decided by customer’s travelling schedule and the city’s traffic conditions.
Directly applying ST-Con blocks to an extremely long OD sequence which covers previous few days
or weeks is computationally expensive. Only a small set of timestamps from each previous day is
necessary to capture the long-term periodicity. As mentioned, we pick p2 time intervals for each day
d 一 夕 when predicting the time window (d, t + j) considering the non-strict long-term periodicity.
This slight time shifting may be caused by unstable traffic peaks, holidays and extreme weather
conditions among different days.
Inspired by the recently widely used attention mechanisms (Xu et al., 2015; Yao et al., 2018a; Liang
et al., 2018) in spatial-temporal prediction problems, we propose a modified periodically shifted
attention to work for the CNN-based ST-Conv blocks here. Different from Yao et al. (2018a) that
measures the similarity between hidden units of LSTMs, the attention here is built on the intermedi-
ate outputs of TGCNNs where the concatenations are then fed into a new set of ST-Conv blocks. For
each day (d 一 夕)，we apply a series of Li ST-Conv blocks to the day-level p2-length sequential OD
flow data X2,d-φ and reduce the temporal length from p to n%. Each block contains two TGCNN
layers with the same kernel size 1 × 1 × KL0T, such that
p2 一 n0LT = Li (2KL0 T 一 2)	(6)
and the propagation rule of the l-th ST-Conv blocks is defined as:
Nd- = Gi *γLιT [S *θLT {Go *γL0T Zd二}]	(7)
with Zn-W and Z[+] representing the input and output of the l-the block.
We denote the day-level representations of day (d 一 夕)captured by the Li ST-Conv blocks above
as Zd-W where each Zdj-W φ represents the φ-th element along the time axis for the OD flow from Vi
to vj. On the other hand, we let zSijT be the learned short-term representation at the OD flow from vi
to Vj. Subsequently, a re-weighted day-level output Zd-W Can be obtained by summing up Zdj-W φ,s
using weights βdij-W,φ which measure their similarities with the short-term representation zSijT:
n0LT
Zd-W = X βd-W,φZd-W,Φ,	(8)
φ=i
5
Under review as a conference paper at ICLR 2020
where βj-q φ quantifies the similarity between Zj-W Φ and ZijT based on a score function
Score(Zj-W φ, ZST), which is defined as:
β ij	=	eχp(SCOre(ZZW,φ,zijτ ))
d-w,φ	Pφ0 eχp(SCOre(Zd-W,Φ0,zSt)).
Moreover, Score(Zd-W φ, ZST) is defined as
vτ tanh(W1 zNw,Φ + w2zSt + bs),	(IO)
where W1 , W2 and vφ are learned projection matrices. bs is the added bias term. By assuming that
Zd-W = (Zdij-W) denotes the day-level output for all the N2 entries after re-weighting, we can build
a new day-wise time series Z0LT of length q by concatenating all the Zd-W’s along an additional axis
in the third dimension as
Z0LT = Concat1W=qZd-W	(11)
to build a new 4D day-wise time series Z0LT . and finally apply another set of ST-Conv blocks to it
to obtain the long-term spatial-temporal representations, which is denoted by ZLT ∈ RN×N×cLT .
cLT is the number of feature channels.
We concatenate the short-term and long-term spatial-temporal representations ZST and ZLT together
along the feature axis as Z = ZST ㊉ Zlt ∈ RN ×N ×C , where C = cST + cLT . Then, Z is modified
to a 2D tensor Z ∈ RN2×C by flattening the first two dimensions while keeping the third one. We
apply a fully connected layer to the C feature channels together with an element-wise non-linear
sigmoid function to get the final predictions for all the N 2 OD flows.
3.5	Data Processing and Training
We normalize the original OD flow data in the training set to (0, 1) by Max-Min normalization and
use ’sigmoid’ activation for the final prediction layer to ensure that all predictions fall into (0, 1).
The upper and lower bounds are saved and used to denormalize the predictions of testing data to get
the actual flow volumes.
We use L2 loss to build the objective loss during the training. The model is optimized via Back-
propagation Through Time (BPTT) and Adam (Kingma & Ba, 2014). The whole architecture of
our model is realized using Tensorflow (Abadi et al., 2016) and Keras (Chollet et al., 2015). All
experiments were run on a cluster with one NVIDIA 12G-memory Titan GPU.
4	Experiment
In this section, we compare the proposed LSTOD model with some state-of-the-art approaches for
latent traffic flow predictions. All compared methods are classified into traditional statistical meth-
ods and deep-learning based approaches. We use the demand flow data collected by a ride-sharing
platform to examine the finite sample performance of OD flow predictions for each method.
4.1	Dataset Description
We employ a large-scale demand dataset obtained from a large-scale ride-sharing platform to do
all the experiments. The dataset contains all customer requests received by the platform from
04/01/2018 to 06/30/2018 in two big cities A and B. Within each urban area, N = 50 hexagonal
regions with the largest customer demands are selected to build the in total N2 = 2500 OD flows.
Since one-layer VACN has a computation complexity O(N) at each of the N2 entries (globally
O(N 3)), the memory consumption highly increases as N gets bigger. Considering the computation
efficiency and storage limitation, we choose N = 50 here which can cover more than 80% of total
demands and thus satisfy the operation requirement of the ride-sharing platform.
We split the whole dataset into two parts. The data from 04/01/2018 to 06/16/2018 is used for
model training, while the other part from 06/17/2017 to 06/30/2017 (14 days) serves as the testing
6
Under review as a conference paper at ICLR 2020
set. The first two and half months of OD flow data is further divided in half to the training and
validation sets. The size ratio between the two sets is around 4:1. We let 30 min be the length of
each timestamp and the value of the OD flow from vi to vj is the cumulative number of customer
requests. We make predictions for all the 502 OD flows in the incoming 1st, 2nd, and 3rd 30 minutes
(i.e. t + 1, t + 2, t + 3) by each compared method, given the historical data with varied (p1, p2)
combinations. For those model settings incorporating long-term information, we trace back q = 3
days to capture the time periodicity. We use Rooted Mean Square Error to evaluate the performance
of each method:
NN
RMSE
∖ N2 *|T0|
i=1 j=1 (d,t)∈T0
(oidj,t -obidj,t)2
(12)
1
oidj,t and obidj,t are the true value and prediction at the OD flow from vertex vi to vertex vj at time
(d, t), respectively. T0 is the set containing all the predicted time points in the testing data.
4.2	Compared Methods
All state-of-the-art methods to be compared are listed as follows, some of which are modified to
work for the OD flow data: (i) Historical average (HA): HA predicts the demand amount at each
OD flow by the average value of the same day in previous 4 weeks. (ii) Autoregressive inte-
grated moving average (ARIMA), (iii) Support Vector Machine Regression (SVMR), (iv) La-
tent Space Model for Road Networks (LSM-RN) (Deng et al., 2016), (v) Dense + BiLSTM
(DLSTM)(Altche & de La Fortelle, 2017) and (Vi) Spatiotemporal Recurrent Convolutional
Networks (SRCN) (Yu et al., 2017). We only consider latent models in this paper, that is, no ex-
ternal coVariates are allowed, while only the historical OD flow data is used to extract the hidden
spatial-temporal features.
4.3	Preprocessing and Parameters
We tune the hyperparameters of each compared model to obtain the optimal prediction performance.
Specifically, we get (p*,d*,q*) = (3,0,3) for ARIMA and k = 15, Y* = 2-5, λ* = 10 for LSM-
RN. The optimal kernel size of the spatial based CNN kernel is 11 × 11 in SRCN model.
For fair comparison, we set the length of short-term OD flow sequence to be p1 = 9 (i.e., preVious
4.5 hours), q = 3 for long-term data which coVers the three most recent days, and the length of
each day-leVel time series p2 = 5 to capture the periodicity shifting (one hour before and after the
predicted time index). More analysis of how Variant (p1,p2) combinations may affect the prediction
performance of LSTOD will be studied latter.
A two-layer architecture is used by all the deep-learning based methods to extract the spatial patterns
inside the OD flow data (L = 2 for both short-term and long-term VACN). We set the filter size of all
deep learning layers in both spatial and temporal space to be 64, including the VACNs and TGCNNs
in our LSTOD model with cST = cLT = 64.
4.4	Results
Comparison with state-of-the-art methods. Table 1 summarizes the finite sample performance
for all the competitiVe methods and our LSTOD model in terms of the prediction RMSE on the
testing data of city A. We compute the mean, Variance, 25% quantile and 75% quantile of the 14
day-wise RMSE on the testing set. LSTOD outperforms all other methods on the testing data with
the lowest aVerage day-wise RMSE (2.41/2.55/2.67), achieVing (8.02%/7.94%/8.24%) improVement
oVer the second best method ’SRCN’. In general, deep-learning based models perform more stably
than traditional methods with smaller Variance and narrower confidence interVals. Both ’ARIMA’
and ’LSM-RN’ perform poorly, eVen much worse than HA, indicating that they cannot capture
enough short-term spatial-temporal features to get the eVolution trend of OD flow data. Among
the deep learning models, LSTOD can more efficiently control the estimation Variance compared
to all the others. This demonstrates the adVantages of using our spatial-temporal architecture and
long-term periodicity mechanism in modelling the dynamic eVolution of OD flow networks. The
improVement becomes more significant when the time scale increases since the contributions of
7
Under review as a conference paper at ICLR 2020
Table 1: Comparison with State-of-the-art methods for City A
Method	30 min	CityA 60 min	90 min
HA		3.61(3.24/3.77/0.41)	
ARIMA	5.11(4.36/5.82/1.21)	5.37(4.62/6.11/1.29)	5.87(5.09/6.65/1.39)
LSVR	3.55(2.55/4.51/2.09)	3.83(2.64/4.94/2.95)	4.65(3.42/5.78/3.24)
LSM-RN	5.62(4.61/8.72/1.86)	6.26(5.41/7.19/1.97)	6.70(5.71/7.63/2.12)
DLSTM	3.03(2.51/3.50/0.52)	3.52(2.89/3.03/0.57)	3.91(3.33/4.48/0.72)
SRCN	2.62(2.09/3.12/0.54)	2.77(2.14/3.35/0.72)	2.91(2.18/3.60/0.93)
LSTOD	2.41(2.16/2.69/0.17)	2.55(2.27/2.68/019	2.67(2.35/3.01/0.25)
long-term periodicity are more emphasized as the short-term signals getting weaker. The LSTOD
performs even better on city B compared to the baseline methods since the long-term periodical
pattern in city B may be more significant compared with that in city A. Detailed results about City
B are summarized in Table 3 of the appendix.
Comparison with variants of LSTOD. Table 2 shows the finite sample performance of our pro-
posed model LSTOD and its different variants based on the demand data from city A. We can see
that the complete LSTOD model outperforms the short-term model and the one without using at-
tention mechanisms in terms of smaller means and variances, and narrower confidence intervals. It
indicates that the attention design we use can capture the shift of the day-wise periodicity and extract
more seasonal patterns to improve prediction accuracy. The left sub-plot of Figure 3 compares the
predictions by each model against the true values at two selected OD flows on the last 3 testing days
in the 60-minute scales. Two abnormal change points are marked by black circles. The short-term
model fails in this case because it ignores the long-term information. The complete LSTOD model
outperforms the one without using attention mechanisms since it can better catch the shift of the
periodicity. The right sub-plot visualizes the distribution curves of the day-wise RMSEs on the 14
testing days by each of the three compared models. The lighter tail of the red curve demonstrates
that the complete LSTOD is more predictive and stable especially for those unusual cases. We do
some more experiments to show how different hyperparameter configurations influence the model
performance. For more details, please refer to Section E of the appendix.
VACN VS standard local CNN. In this experiment, we will show that our proposed VACN out-
performs standard CNNs in capturing the hidden network structure of the OD flow data. Given the
model setting that N = 50 sub-regions of city A are used to build the dynamic OD flow matrices,
the number of pixels being covered by VACN at each single snapshot is 50 × 4 = 200. For fair
comparison, the largest receptive filed of standard CNN should be no bigger than a 15 × 15 window,
which includes 225 elements each time. We consider five different kernel sizes including 5 × 5,
8 × 8, 11 × 11, 14 × 14, and 15 × 15. We replace VCAN in our model by standard CNN in or-
der to fairly compare its performance. All hyper-parameters are fixed but only the kernel size of
CNNs being changed. Moreover, we only consider the baseline short-term mode of LSTOD model
while ignoring the long-term information. As Figure 4 illustrates, standard CNN achieves the best
performance with the smallest RMSE = 2.64 on testing data with the filter size being 11 × 11,
which is still higher than that using VACN with RMSE = 2.54. Specifically, RMSE increases when
the receptive field is larger than 11 × 11 since the spatial correlations among the most related OD
flows (sharing common origin or destination nodes) are smoothed with the increase in the filter size
((8 × 2 - 1)/64 > (14 × 2 - 1)/196). This experiment shows that treating the dynamic demand
matrix as an image, and applying standard CNN filters does not capture enough spatial correlations
among related OD flows without considering their topological connections from the perspective of
graphs. For more details, please refer to Figure 4.
As we mentioned above, one-layer VACN has a global computation complexity O(N 3) at each
timestamp. For standard CNN, O(N 2) executions are still needed by applying local CNN filters to
N2 windows with each of the N2 entries in the middle. Therefore, when the side length of the CNN
filter is in the order O(√N), the total cost of our VACN and standard CNN are in the same order.
8
Under review as a conference paper at ICLR 2020
Method	30 min	RMSE 60 min	90 min
VACN + TGCNN (short-term)	2.52(2.21/2.90/0.26)	2.74(2.44/2.81/0.34)	2.81(2.33/3.25/0.45)
LSTOD (no attention)	2.47(2.14/2.81/0.23)	2.62(2.31/2.70/0.28)	2.70(2.34/3.12/0.35)
LSTOD (complete)	2.41(2.16/2.69/0.17)	2.56(2.27/2.68/0.19)	2.67(2.35/3.01/0.25)
Table 2: Evaluation of LSTOD and its variants
Figure 3: Day-wise RMSE comparison between varied
STOD models
Figure 4: RMSE on testing data with re-
spect to ACN and standard CNN using
different kernel sizes.
References
Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu
Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: A system for large-
scale machine learning. In 12th {USENIX} Symposium on Operating Systems Design and Imple-
mentation, 265-283, 2016.
Florent AltChe and Arnaud de La Fortelle. An lstm network for highway trajectory prediction.
In 20th International Conference on Intelligent Transportation Systems (ITSC), 353-359. IEEE,
2017.
Francois Chollet et al. Keras, 2015.
Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks
on graphs with fast localized spectral filtering. In Advances in Neural Information Processing
Systems, pp. 3844-3852, 2016.
Dingxiong Deng, Cyrus Shahabi, Ugur Demiryurek, Linhong Zhu, Rose Yu, and Yan Liu. Latent
space model for road networks to predict time-varying traffic. In Proceedings of the 22nd ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2016.
Chris Godsil and Gordon F Royle. Algebraic graph theory, volume 207. Springer Science &
Business Media, 2013.
Tsuyoshi Ide and Masashi Sugiyama. Trajectory regression on road networks. In Twenty-Fifth AAAI
Conference on Artificial Intelligence, 2011.
Jintao Ke, Hongyu Zheng, Hai Yang, and Xiqun Michael Chen. Short-term forecasting of passenger
demand under on-demand ride services: A spatio-temporal deep learning approach. Transporta-
tion Research Part C: Emerging Technologies, 85:591-608, 2017.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. arXiv preprint arXiv:1609.02907, 2016.
9
Under review as a conference paper at ICLR 2020
Jaimyoung Kwon and Kevin Murphy. Modeling freeway traffic with coupled hmms. Technical
report, Technical report, Univ. California, Berkeley, 2000.
Xiaolong Li, Gang Pan, Zhaohui Wu, Guande Qi, Shijian Li, Daqing Zhang, Wangsheng Zhang,
and Zonghui Wang. Prediction of urban human mobility using large-scale taxi traces and its
applications. Frontiers of Computer Science, 6(1):111-121, 2012.
Yaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu. Diffusion convolutional recurrent neural net-
work: Data-driven traffic forecasting. arXiv preprint arXiv:1707.01926, 2017.
Yuxuan Liang, Songyu Ke, Junbo Zhang, Xiuwen Yi, and Yu Zheng. Geoman: Multi-level attention
networks for geo-sensory time series prediction. In IJCAI, 3428-3434, 2018.
Marco Lippi, Matteo Bertini, and Paolo Frasconi. Short-term traffic flow forecasting: An experimen-
tal comparison of time-series analysis and supervised learning. IEEE Transactions on Intelligent
Transportation Systems, 14(2):871-882, 2013.
Xiaolei Ma, Zhuang Dai, Zhengbing He, Jihui Ma, Yong Wang, and Yunpeng Wang. Learning
traffic as images: a deep convolutional neural network for large-scale transportation network
speed prediction. Sensors, 17(4):818, 2017.
Luis Moreira-Matias, Joao Gama, Michel Ferreira, Joao Mendes-Moreira, and Luis Damas. Predict-
ing taxi-passenger demand using streaming data. IEEE Transactions on Intelligent Transporta-
tion Systems, 14(3):1393-1402, 2013.
Shashank Shekhar and Billy M Williams. Adaptive seasonal time series models for forecasting
short-term traffic flow. Transportation Research Record, 2024(1):116-125, 2007.
Petar VeliCkovic, GUillem CUcUrUlL Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.
SHI Xingjian, ZhoUrong Chen, Hao Wang, Dit-Yan YeUng, Wai-Kin Wong, and Wang-chUn Woo.
ConvolUtional lstm network: A machine learning approach for precipitation nowcasting. In Ad-
vances in neural information processing systems, 802-810, 2015.
Kelvin XU, Jimmy Ba, Ryan Kiros, KyUnghyUn Cho, Aaron CoUrville, RUslan SalakhUdinov, Rich
Zemel, and YoshUa Bengio. Show, attend and tell: NeUral image caption generation with visUal
attention. In International conference on machine learning, 2048-2057, 2015.
Bin Yang, ChenjUan GUo, and Christian S Jensen. Travel cost inference from sparse, spatio tem-
porally correlated time series Using markov models. Proceedings of the VLDB Endowment, 6(9):
769-780, 2013.
HUaxiU Yao, Xianfeng Tang, HUa Wei, GUanjie Zheng, Yanwei YU, and ZhenhUi Li. Modeling
spatial-temporal dynamics for traffic prediction. arXiv preprint arXiv:1803.01254, 2018a.
HUaxiU Yao, Fei WU, Jintao Ke, Xianfeng Tang, Yitian Jia, SiyU LU, PinghUa Gong, and Jieping
Ye. Deep mUlti-view spatial-temporal network for taxi demand prediction. arXiv preprint
arXiv:1802.08714, 2018b.
Bing YU, Haoteng Yin, and Zhanxing ZhU. Spatio-temporal graph convolUtional networks: A deep
learning framework for traffic forecasting. In Proceedings of the 27th International Joint Confer-
ence on Artificial Intelligence, 3634-3640. AAAI Press, 2018.
Haiyang YU, Zhihai WU, ShUqin Wang, YUnpeng Wang, and Xiaolei Ma. Spatiotemporal recUrrent
convolUtional networks for traffic prediction in transportation networks. Sensors, 17(7):1501,
2017.
JUnbo Zhang, YU Zheng, Dekang Qi, RUiyUan Li, and XiUwen Yi. Dnn-based prediction model for
spatio-temporal data. In Proceedings of the 24th ACM SIGSPATIAL International Conference on
Advances in Geographic Information Systems, 92. ACM, 2016.
JUnbo Zhang, YU Zheng, and Dekang Qi. Deep spatio-temporal residUal networks for citywide
crowd flows prediction. In Thirty-First AAAI Conference on Artificial Intelligence, 2017.
10
Under review as a conference paper at ICLR 2020
Jiangchuan Zheng and Lionel M Ni. Time-dependent trajectory regression on road networks via
multi-task learning. In Twenty-Seventh AAAI Conference on Artificial Intelligence, 2013.
Xian Zhou, Yanyan Shen, Yanmin Zhu, and Linpeng Huang. Predicting multi-step citywide pas-
senger demands using attention-based neural networks. In Proceedings of the Eleventh ACM
International Conference on Web Search and Data Mining, 736-744. ACM, 2018.
11
Under review as a conference paper at ICLR 2020
A Notation Table
Od,t	OD flow maps of day d, time t
ij od,t	Flow amount from vertex vi to vj at day d, time t
X1,X2	Short-term and long-term spatial-temporal OD flow data
X2,d-φ	long-term spatial-temporal OD flow data of day d - φ
n+1 Zd-中	Output of n-th ST-Conv block of day d — φ
7. Zd-W	features of day (d — φ) captured by ST-Conv blocks
ij zd-W,φ	φ-th element along the time axis of Zd-W from vi to vj
Zd-W	day-level output of day d — φ
ZLT	long-term spatial-temporal representations
Z	ZST ㊉ ZLT, combined spatial-temporal representations
B Illustration of VACN
Figure 5: Working mechanism of spatial adjacent convolution network (VACN) for a target OD flow
from vi to vj
C	Training Details
Batch normalization is used in the VACN component. The batch size in our experiment was set to
10, corresponding to 10 randomly sampled timestamps and all the 502 OD flows in each snapshot.
The initial liearning rate is set to be 10-4 with a decay rate 10-6. We use early stopping for all
the deep learning-based methods where the training process is terminated when the RMSE over
validation set has not been improved for 10 successive epochs. The maximal number of epochs
allowed is 100.
D Table3: Comparison with State-of-the-art methods for City B
E	Comparison of different hyperparameter configurations
In this section, we want to explore how some important hyperparameters of input OD flow data, for
example p1 and p2, may affect the performance of our LSTOD model.
12
Under review as a conference paper at ICLR 2020
Table 3: Comparison with State-of-the-art methods for City B
Method	30 min	CityB 60 min	90 min
HA		3.95(2.78/4.58/2.21)	
ARIMA	4.87(3.81/6.83/1.22)	5.14(3.91/6.34/1.71)	5.39(4.04/6.65/2.15)
LSVR	4.21(3.19/5.36/1.98)	4.87(3.56/6.29/3.06)	5.24(3.77/6.83/3.68)
LSM-RN	5.99(5.12/7.05/2.03)	6.72(5.70/7.74/2.14)	7.32(6.30/8.35/2.34)
DLSTM	3.86(3.34/4.37/0.59)	4.04(3.52/4.60/0.65)	4.52(3.89/5.11/0.81)
SRCN	2.89(2.33/3.41/0.59)	3.06(2.53/3.70/0.80)	3.17(2.50/3.87/1.02)
LSTOD	2.56(2.25/2.85/0.20)	2・63(2.29/2.957024	2・69(2・32/3・05/0・31)
Figure 6 (b) compares RMSE on testing data by STOD model with different data settings. Varied
combinations of the short-term sequence length p1 and the long-term day-level sequence length p2
are studied. We can see that the best performance is achieved as (p1,p2) = (7, 5) with RMSE =
2.41. Specifically, settings with different p1’s under p2 = 5 consistently outperform those under
p2 = 7. It may demonstrate that the shift can usually be captured within a short time range, while a
longer time sequence may smooth the significance. Table 4 provides the detailed prediction results
for each data setting.
Table 4: Comparison of STOD under different p1, p2 combinations
p2	(KL0 T , KL1 T )	p1	(KS0T , KS1T)	RMSE
			(2, 2)	2.45
		7	(2, 3)	2・41
5	(2, 2)	9	(3, 3)	2.42
		11	(3, 4)	2.43
		13	(4, 4)	2.43
		-ɪ	(2, 2)	2.45
		7	(2, 3)	2.44
7	(3, 2)	9	(3, 3)	2.44
		11	(3, 4)	2.44
		13	(4, 4)	2.49
Figure 6: RMSE on testing data with respect to STOD with different p1 and p2 combinations.
13