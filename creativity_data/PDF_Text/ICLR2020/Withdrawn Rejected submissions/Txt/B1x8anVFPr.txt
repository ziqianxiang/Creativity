Under review as a conference paper at ICLR 2020
On Layer Normalization in the Transformer
Architecture
Anonymous authors
Paper under double-blind review
Ab stract
The Transformer architecture is popularly used in natural language processing
tasks. To train a Transformer model, a carefully designed learning rate warm-up
stage is usually needed: the learning rate has to be set to an extremely small value
at the beginning of the optimization and then gradually increases in some given
number of iterations. Such a stage is shown to be crucial to the final performance
and brings more hyper-parameter tunings. In this paper, we study why the learn-
ing rate warm-up stage is important in training the Transformer and theoretically
show that the location of layer normalization matters. It can be proved that at the
beginning of the optimization, for the original Transformer, which places the layer
normalization between the residual blocks, the expected gradients of the param-
eters near the output layer are large. Then using a large learning rate on those
gradients makes the training unstable. The warm-up stage is practically helpful to
avoid this problem. Such an analysis motivates us to investigate a slightly mod-
ified Transformer architecture which locates the layer normalization inside the
residual blocks. We show that the gradients in this Transformer architecture are
well-behaved at initialization. Given these findings, we are the first to show that
this Transformer variant is easier and faster to train. The learning rate warm-up
stage can be safely removed, and the training time can be largely reduced on a
wide range of applications.
1	Introduction
The Transformer is one of the most commonly used neural network architectures in natural language
processing, and layer normalization is one of the key components in the Transformer. The originally
designed Transformer places the layer normalization between the residual blocks, which is usu-
ally referred to as the Transformer with Post-Layer Normalization (Post-LN). This architecture has
achieved state-of-the-art performance in many tasks including language modeling (Dai et al., 2019;
Al-Rfou et al., 2018) and machine translation (Vaswani et al., 2017; Dehghani et al., 2018; Edunov
et al., 2018). Unsupervised pre-trained models based on the Post-LN Transformer architecture also
show impressive performance in many downstream tasks (Radford et al., 2019; Devlin et al., 2018;
Yang et al., 2019).
Although it achieves great success, people usually need to deal with the optimization of the Post-
LN Transformer more carefully than convolutional networks (He et al., 2016; Tan & Le, 2019) or
other sequence-to-sequence models (Sutskever et al., 2014; Gehring et al., 2017). In particular,
to train the model from scratch, any gradient-based optimization approach requires a learning rate
warm-up stage: The optimization starts from using an extremely small learning rate, e.g., 1e-7, and
then gradually increases it to a pre-defined maximum value in a pre-defined number of iterations.
After that, the learning rate decays similar to the optimization of other architectures. Both previous
works (Vaswani et al., 2017; Popel & Bojar, 2018), as well as our empirical study, show that such a
warm-up stage is essential in training the models. Furthermore, the final model performance is quite
sensitive to the value of the maximum learning rate and the number of warm-up iterations. Tuning
such sensitive hyper-parameters is costly in training large-scale models, e.g., BERT (Devlin et al.,
2018) or XLNet (Yang et al., 2019).
In this paper, we study why the learning rate warm-up stage is essential in the optimization of the
Post-LN Transformer and find it is closely related to the position of the layer normalization. As
1
Under review as a conference paper at ICLR 2020
the warm-up stage happens in the first several iterations, we investigate the optimization behavior at
initialization of the Post-LN Transformer. According to our theoretical analysis, when putting the
layer normalization between the residual blocks, the expected gradients of the parameters near the
output layer are large. Therefore, without the warm-up stage, directly using a large learning rate to
those parameters may not lead to an improved model and can even make the optimization process
unstable. Using a warm-up stage and training the model from small learning rates practically avoid
this problem.
As the location of the layer normalization plays
a crucial role in controlling the gradient scales,
we investigate whether there are some other
ways of positioning the layer normalization that
lead to better-normalized gradients. In par-
ticular, we study another variant, the Trans-
former with Pre-Layer Normalization (Pre-LN)
(Klein et al., 2018). The Pre-LN Transformer
puts the layer normalization inside the residual
connection and equips with an additional final-
layer normalization before prediction (Please
see Figure 1 for the differences between the two
variants of the Transformer architectures). In
this paper, we show that the gradients are well-
behaved without any exploding or vanishing at
initialization for the Pre-LN Transformer both
theoretically and empirically.
Given the gradients are well-behaved in the Pre-
LN Transformer, it is natural to consider re-
moving the learning rate warm-up stage during
training. We conduct extensive experiments,
including IWSLT14 German-English transla-
tion, WMT14 English-German translation, and
BERT pre-training tasks. We show that, in all
tasks, the learning rate warm-up stage can be
safely removed and thus, the number of hyper-
Figure 1: (a) Post-LN Transformer layer; (b) Pre-
LN Transformer layer.
parameter is reduced. Furthermore, we observe that the loss decays faster for the Pre-LN Trans-
former model. It can achieve comparable final performances but use much less training time. This
is particularly important for training large-scale models on large-scale datasets.
Our contributions are summarized as follows: First, we investigate two Transformer variants, the
Post-LN Transformer and the Pre-LN Transformer. By studying the gradients at initialization, we
show why the learning rate warm-up stage is essential in training the Post-LN Transformer. Sec-
ond, we are the first to show that the learning-rate warm-up stage can be removed for the Pre-LN
Transformer. By using proper learning rate schedulers, the training time can be largely reduced.
2	Related work
Gradient descent-based methods (Kingma & Ba, 2014; Zeiler, 2012; Duchi et al., 2011; Tieleman &
Hinton, 2012) are popularly used in optimizing deep neural networks. For convolutional neural net-
works and recurrent neural networks, a relatively large learning rate is usually set in the beginning,
and then decays along with the optimization process (He et al., 2016; 2017; Sutskever et al., 2014;
Gehring et al., 2017; He et al., 2019). The learning rate warm-up stage has been shown to be essen-
tial in dealing with some specific problems, e.g., the large-batch training. Goyal et al. (2017); He
et al. (2019) and You et al. (2018) showed that when training neural networks with extremely large
batch sizes (e.g., 8k in ImageNet), optimizing the model with a large learning rate in the beginning
usually leads to poor performance. Training with a learning rate warm-up stage can eliminate the
performance gap.
However, when optimizing the Post-LN Transformer models, as far as we know, in almost all pre-
vious works (Vaswani et al., 2017; Devlin et al., 2018; Dai et al., 2019; Radford et al., 2019; Lu
2
Under review as a conference paper at ICLR 2020
et al., 2019), the learning rate warm-up stage is essential and critical for training. Popel & Bojar
(2018) investigated the influence of different warm-up strategies for the optimization of the Post-LN
Transformer model and found that without or with relatively less warm-up iterations (e.g., 12k in
Cz-En translation), the optimization diverges.
In a concurrent and independent work (Liu et al., 2019a), the authors claimed that the benefit of
the warm-up stage comes from reducing the variance for the adaptive learning rate in the Adam
optimizer (Kingma & Ba, 2014). They proposed to rectify the variance of adaptive learning rate
by a new variant of Adam called RAdam. However, we identify the problem from the parameter
initialization. We show that for the Post-LN Transformer, the scales of gradients of some parameters
at initialization are large. First-order optimizers take the gradients as input. Using such gradients on
these optimizers (not limit to Adam) with a large learning rate may make the optimization unstable
and hurt the final performance.
3	Optimization for the Transformer
3.1	The Transformer Architecture with Post-layer normalization
The Transformer architecture usually consists of stacked Transformer layers (Vaswani et al., 2017;
Devlin et al., 2018), each of which takes a sequence of vectors as input and outputs a new sequence
of vectors with the same shape. A Transformer layer has two sub-layers: the (multi-head) self-
attention sub-layer and the position-wise feed-forward network sub-layer. Residual connection (He
et al., 2016) and layer normalization (Lei Ba et al., 2016) are applied for both sub-layers individually.
We first introduce each component of the Transformer layer and then present the entire architecture.
Self-attention sub-layer An attention function can be formulated as querying an entry with key-
value pairs (Vaswani et al., 2017). The self-attention sub-layer uses scaled dot-product attention,
which is defined as: Attention(Q, K, V) = Softmax(q√- )V, where d is the dimensionality of the
hidden representations, and Q (Query), K (Key), V (Value) are specified as the hidden representa-
tions of the previous layer. The multi-head variant of the self-attention sub-layer is popularly used
which allows the model to jointly attend to information from different representation sub-spaces,
and is defined as
MUIti-head(Q, K, V) = Concat(headι, ∙∙∙ , heada)WO,	(1)
headk = Attention(QWkQ,KWkK,VWkV),	(2)
where WkQ ∈ Rd×dK, WkK ∈ Rd×dK, WkV ∈ Rd×dV, and WO ∈ RHdV×d are project parameter
matrices, H is the nUmber of heads. dK and dV are the dimensionalities of Key and ValUe. WithoUt
any confusion, given a sequence of vectors (xi,…，xn), We use MUItiHeadAtt(xi, [χ1,χ2,…,Xn])
as the mUlti-head self-attention mechanism on position i which considers the attention from xi to
the entire sequence.
Position-wise FFN sub-layer In addition to the self-attention sub-layer, each Transformer layer
contains a fully connected network, which is applied to each position separately and identically. This
sub-layer is a two-layer feed-forward network with a ReLU activation function. Given a sequence
of vectors h1, ..., hn, the computation ofa position-wise FFN sub-layer on any hi is defined as:
FFN(hi) = ReLU(hiW1 +b1)W2 + b2,	(3)
where W1, W2, b1 and b2 are parameters.
Residual connection and layer normalization Besides the two sub-layers described above, the
residual connection and layer normalization are also key components to the Transformer. For any
vector v, the layer normalization is computed as LayerNorm(V) = Y v-μ + β, in which μ, σ are the
mean and standard deviation of the elements in v, i.e., μ = d Pk=1 Vk andσ2 = d Pk=I(Vk -μ)2∙
Scale γ and bias vector β are parameters.
Different orders of the sub-layers, residual connection and layer normalization in a Transformer
layer lead to variants of Transformer architectures. One of the original and most popularly used
3
Under review as a conference paper at ICLR 2020
Table 1: Post-LN Transformer v.s. Pre-LN Transformer
Post-LN Transformer	Pre-LN Transformer
Xpostj = MUItiHeadAmXpost, [xp1st, ∙∙	"PO"]) xpre,1 =LayerNOTm(Xpre)
post,2	post	post,1 Xl,i	= Xl,i + Xl,i	Xpre,2 = MUItiHeadAtt(Xpre,1,唱型，…/片])
Xlp,oist,3 = LayerNorm(Xlp,oist,2)	pre,3	pre	pre,2 Xl,i = Xl,i + Xl,i
Xlp,oist,4 = ReLU(Xlp,oist,3W 1,l + b1,l)W2,l + b2,l	Xlp,rie,4 = LayerNorm(Xlp,rie,3)	
Xpost,5 = Xpost,3 + Xpost,4	Xlprie,5 = ReLU(Xlprie,4W 1,l + b1,l )W 2,l + b2,l
Xllp,+ois1t,i = Laly,ierNorm(lX,ilp,oist,5)	pre	pre,5	pre,3 Xl+i,i = Xl,i + Xl,i 	
Final LayerNorm: XFnal i J LayerNorm(XLreI i)
architecture for the Transformer and BERT (Vaswani et al., 2017; Devlin et al., 2018) follows “self-
attention (FFN) sub-layer → residual connection → layer normalization”, which we call the Trans-
former with Post-Layer normalization (Post-LN Transformer), as illustrated in Figure 1.
Post-LN Transformer Denote Xl,i as the input of the l-th Transformer layer at position i, where
Xl,i is a real-valued vector of dimension d, i = 1, 2, ..., n, l = 1, 2, ..., L. n is the length of the
sequence and L is the number of layers. For completeness, we define X0,i as the input embedding
at position i which is usually a combination of word embedding and positional embedding. The
computations inside the l-th Post-LN Transformer layer are composed of several steps, and we use
super-scripts on X to present the input(output) of different steps as in Table 1 (left), where W 1,l,
W2,l, b1,l and b2,l are parameters of the FFN sub-layer in the l-th layer.
3.2	The importance of the warm-up s tage in training the Post-LN Transformer
We are interested in the learning rate warm-up stage in the optimization of the Post-LN Transformer.
Different from the optimization of many other architectures in which the learning rate starts from
a relatively large value and then decays (Bahdanau et al., 2017; He et al., 2016; Dauphin et al.,
2017), a learning rate warm-up stage for the Post-LN Transformer is critical. Specifically, denote
the learning rate of the t-th iteration as lr(t). Denote the maximum learning rate during training
as lrmax . Given a predefined time frame Twarmup, the learning rate scheduler for the first Twarmup
iterations is defined as (Vaswani et al., 2018)
lr(t)
t
Twarmup
lrmax ,
t ≤ Twarmup .
(4)
After this warm-up stage, the learning rate will be set by classical learning rate schedulers, such
as the linear decay, the inverse square-root decay, or forced decay at particular iterations. As we
can see from Eqn (4) , at the beginning of the training, the learning rate starts from zero1 and then
linearly increases to lrmax in Twarmup iterations. We conduct experiments to show that this learning
rate warm-up stage is essential for training Post-LN Transformer models.
Setting We study the optimization process on the IWSLT14 German-to-English (De-En) machine
translation task. We mainly investigate two aspects: whether the learning rate warm-up stage is
essential and whether the final model performance is sensitive to the value of Twarmup. To study the
first aspect, we train the model with the Adam optimizer (Kingma & Ba, 2014) and the vanilla SGD
optimizer (Ruder, 2016) respectively. For both optimziers, we check whether the warm-up stage can
be removed. We follow (Vaswani et al., 2017) to set hyper-parameter β to be (0.9, 0.98) in Adam.
We also test different lrmax for both optimizers. For Adam, we set lrmax = 5e-4 or 1e-3, and for
SGD, we set lrmax = 5e-3 or 1e-3. When the warm-up stage is used, we set Twarmup = 4000 as
suggested by the original paper (Vaswani et al., 2017). To study the second aspect, we set Twarmup
to be 1/500/4000 (“1” refers to the no warm-up setting) and use lrmax = 5e-4 or 1e-3 with Adam.
For all experiments, a same inverse square root learning rate scheduler is used after the warm-up
stage. We use both validation loss and BLEU (Papineni et al., 2002) as the evaluation measure of
the model performance. All other details can be found in the Appendix.
1An extremely small learning rate is practically used for the first update, e.g., 1e-7.
4
Under review as a conference paper at ICLR 2020
8 6
SSo-I UoQePIra>
1	3	5	7	9	11	13	15
Epochs
一♦ IrmaX=Ie 3, w/o warm-up	∣∣"ma× = lθ ɜɪ Twarmup = 500
∣rma× = 5e^4, w/o warm-up ■ ∣rmax = 5e_4, Twarmup = 500
Ooo
3 2 1
ɔʃlm
5	7	9	11	13	15
Epochs
-* IrmaX = Ie 3, TWarmUP = 4000
-*-	= 5β-4, Twarmup = 4000
(a)	Loss/BLEU on the IWSLT14 De-En task (Adam)
SSo-I uo=epl(5>
3	5	7	9	11	13
Epochs
∩山13
3530252015105
3	5	7	9	11	13	15
Epochs
Irmax = Ie-3, w/o warm-up —Irmax = Ie-3, Twarmup = 4000	Irmax = 5e_3, w/o warm-up
l ” IrmaX = ʒɑ ɜɪ Twarmup = 4000
(b)	Loss/BLEU on the IWSLT14 De-En task (SGD)
Figure 2:	Performances of the models optimized by Adam and SGD on the IWSLT14 De-En task.
Result We record the model checkpoints for every epoch during training and calculate the val-
idation loss and BLEU score. The performance of the models trained with Adam and SGD are
plotted in Figure 2(a) and Figure 2(b). The x-axis is the epoch number and the y-axis is the BLEU
score/validation loss. ”w/o warm-up” indicates “without the warm-up stage” while ”w/ warm-up”
indicates “with the warm-up stage”.
First, we can see that for both optimizers, the learning rate warm-up stage is essential. Without the
warm-up stage, the BLEU score of the model trained with Adam optimizer can only achieve 8.45. As
a comparison, the model trained using the warm-up stage can achieve around 34 in terms of BLEU
score. The same trend can be also observed on the validation loss curves. Although the performance
of the model trained with SGD is significantly worse than Adam, we can still see similar phenomena
as Adam. The BLEU score is just above zero in 15 epochs without using the warm-up stage.
Second, we can see that the optimization process is sensitive to the value of Twarmup, which means
Twarmup is an important hyper-parameter in training the Post-LN Transformer. For example, when
setting Twarmup = 500, the learned models with Adam achieve only 31.16 and 2.77 in term of BLEU
score for lrmax = 5e-4 and 1e-3 respectively 2 .
Discussion First, we can see that the learning rate warm-up stage significantly helps the opti-
mization of the Post-LN Transformer and also significantly affects the final performance. Such a
warm-up stage brings additional efforts on hyper-parameter tuning which is computationally expen-
sive for large-scale NLP tasks. Second, at the beginning of the training, the loss value is usually
large. Standard optimization algorithms usually start with a large learning rate for fast convergence.
However, when using the warm-up stage, the learning rate has to gradually increase from zero,
2One may notice that the orange curve is better than the blue curve on the validation loss but is worse on
the BLEU score. This is because the BLEU score is defined on the translation results which is generated by
step-wise decoding. There may be a big gap between the BLEU score and the validation loss when the model
is not well-trained.
5
Under review as a conference paper at ICLR 2020
which may slow down the optimization process. Liu et al. (2019a) suggests that the warm-up stage
plays a role in reducing the undesirably significant variance in Adam in the early stage of model
training. Based on this, they design a new variant of Adam optimizer, RAdam. However, according
to our results, the warm-up stage also helps the training of SGD. This suggests that the benefit of the
warm-up stage may be not for a particular optimizer.
3.3 Understanding the Transformer at initialization
We can see that the Post-LN Transformer cannot be trained with a large learning rate from scratch.
This motivates us to investigate what happens at the model initialization. We first introduce the
parameter initialization setting for our theoretical analysis and then present our theoretical findings.
Notations We denote L(∙) as the loss function of one position, L(∙) as the loss function of the
whole sequence, k ∙ ∣∣2 and ∣∣ ∙ ∣∣f as the l2 norm (spectral norm) and the Frobenius norm, LN(χ)
as the standard layer normalization with scale γ = 1 and bias β = 0, and JLN (x)
dLN(X) as
∂x
the Jacobian matrix ofLN(χ). Let O(∙) denote standard Big-O notation that suppress multiplicative
constants.
Parameter Initialization There are multiple parameter matrices in each Transformer layer, and
most of the parameter matrices are initialized by the Xavier initialization (Glorot & Bengio, 2010).
Given a matrix of size nin × nout, the Xavier initialization sets the value of each element by indepen-
dently sampling from Gaussian distribution N(0, ——-2------). The bias vectors are usually initialized
nin +nout
as zero vectors. The scale γ in the layer normalization is set to one.
For theoretical analysis, we study a simpler setting. First, we focus on single-head attention instead
of the multi-head variant and for all layers, we set the shape of WQ,l, WK,l, W V,l, W 1,l,W 2,l to
be d × d. Second, we initialize the parameter matrices in the self-attention sub-layer WQ,l and
W K,l to be zero matrices. In this setting, the attention is a uniform distribution at initialization
and MUltiHeadAtt(χ1,i, [χ1,ι, χ1,2,…，χ1,n]) can be simplified as ɪ P；=i χι,jWV,l. We test the
optimization process in this setting and find that the problems described previously remain. Third,
we assume the input vectors are also sampled from the same Gaussian distribution. This is reason-
able since the inputs to the Transformer are linear combinations of word embeddings and learnable
positional embeddings, both of which are initialized by Gaussian distributions.
Post-LN Transformer v.s. Pre-LN Transformer We compare the Post-LN Transformer with
another variant of the Transformer architecture, the Transformer with Pre-Layer Normalization (Pre-
LN). The Pre-LN Transformer was implemented in several systems (Vaswani et al., 2018; Klein
et al., 2018; Liu et al., 2019b). Wang et al. (2019) suggested that when stacking more layers, the
Pre-LN Transformer is better than its Post-LN counterpart. Different from the Post-LN Transformer
that puts the layer normalization between the residual blocks, the Pre-LN Transformer puts the layer
normalization inside the residual connection and places it before all other non-linear transformations.
Additionally, the Pre-LN Transformer uses a final layer normalization right before the prediction.
We provide the mathematical formulations and visualizations of the Post-LN Transformer and the
Pre-LN Transformer in Table 1 and Figure 1.
For both architectures, each xL,i passes through a softmax layer to produce a distribution over the
dictionary V . The loss function is defined on the softmax distribution. For example, in sequence pre-
diction, the loss function is defined as L(xpLo+s1t,i) = - log(softmaxyi (WembxpLo+s1t,i)) for the Post-
LN Transformer and L(xpFrienal,i) = - log(softmaxyi (WembxpFrienal,i)) for the Pre-LN Transformer,
where softmaxyi is the probability of ground truth token yi outputted by the softmax distribution
and Wemb is the word embedding matrix. The loss of the whole sequence is an average of the loss
on each position. Without loss of generality, we assume that all the derivatives are bounded. We
introduce the following concentration property of random variables which will be further used in the
theorem.
Definition 1. A random variable Z ≥ 0 is called (, δ)-bounded if with probability at least 1 - δ,
Z-ZZ ≤ G where e > 0 and 0 < δ < L
6
Under review as a conference paper at ICLR 2020
Intuitively, if the random variable Z is (, δ)-bounded, then with a high probability its realization
will not get too far away from its expectation. For example, if Y is a d-dimensional standard Gaus-
sian random vector, then Z = kY k22 is (, δ)-bounded with δ = exp(-d2/8), 0 <	< 1 (see
Appendix G for details). As parameter matrices in self-attention sub-layers and FFN sub-layers are
initialized by Gaussian distributions, if the norm of the hidden states in the Transformer satisfies
the concentrated condition above, we have the following theorem to characterize the scale of the
gradients.
Theorem 1 (Gradients of the last layer 3 in the Transformer). Assume that kxpLo,ist,5k22 and kxpLr+e1,ik22
are (, δ)-bounded for all i, where and δ = δ() are small numbers. Then with probability at least
0.99 一 δ 一 0 9十€, for the POst-LN Transformer with L layers, the gradient ofthe parameters of the
last layer satisfies
~
..∂L ..	，—
k∂W2:LkF ≤O(d√lnd)
and for the Pre-LN Transformer with L layers,
ll ∂L ll 八(]∕ln d'
kkkF ≤O Wr
From Theorem 1, We can see that for the Post-LN Transformer, the scale of the gradients to the last
FFN layer is of order O(d√ln d) which is independent of L. For the Pre-LN Transformer, the scale
of the gradients is much smaller. We first study the forWard propagation of the Post-LN Transformer
and the Pre-LN Transformer. Lemma 1 will be served as a basic tool to prove the main theorem and
other lemmas.
Lemma 1. If X ∈ Rd is a Gaussian vector, X 〜N(0,σ2Id), then E(kReLU(X)k2) = 2σ2d.
Based on Lemma 1, we have the following lemma to estimate the scale of the hidden states in
different layers for the Post-LN Transformer and the Pre-LN Transformer.
Lemma 2. At initialization, for the Post-LN Transformer, E(kxP0st,5k2) = 3d for all l > 0 and i.
For the Pre-LN Transformer, (1 + E)d ≤ E(Ilxprel∣2) ≤ (1 + 3)dforall l > 0 and i. Expectations
are taken over the input and the randomness of initialization.
Lemma 2 studies the expected norm of the hidden states in both Post-LN/Pre-LN Transformer. It
is obviously that in the Post-LN Transformer, the norm of xpθst is √d and thus we study the norm
of xlp,oist,5 instead. As we can see from Lemma 2, the scale of the hidden states in the Post-LN
Transformer keeps to be the same in expectation while the scale of the hidden states in the Pre-LN
Transformer grows linearly along with the depth. The next lemma shows that the scale of the hidden
states highly relates to the scale of the gradient in the architectures using layer normalization.
Lemma 3. For x ∈ Rd, we have kJLN(x)k2 = O(k√d^) in which JLN(x) = dLd(X.
The proof of Lemma 1, Lemma 2, Lemma 3, and Theorem 1 can be found in the Appendix. The
main idea is that the layer normalization will normalize the gradients. In the Post-LN Transformer,
the scale of the inputs to the layer normalization is independent of L, and thus the gradients of
parameters in the last layer are independent of L. While in the Pre-LN Transformer, the scale of the
input to the final layer normalization is linear in L, and thus the gradients of all parameters will be
normalized by LL.
Extending to other layers/parameters We have provided a formal proof on the gradients of the
last FFN sub-layer as above. In order to fully understand the optimization, we also make some
preliminary analysis for other layers and other parameters. Our main result is that the gradient norm
in the Post-LN Transformer is large for the parameters near the output and will be likely to decay as
the layer index l decreases. On the contrary, the gradient norm in the Pre- Transformer will be likely
to stay the same for any layer l. All the preliminary theoretical results are provided in Appendix F.
3We mainly study the optimization of the Transformer layers and “last layer” here refers to the top FFN
layer before the softmax operator. We did not focus on the parameters in the softmax layer as it is tied with the
word embedding matrix, which is actually the input to the Transformer.
7
Under review as a conference paper at ICLR 2020
Empirical study We also conduct experiments to study the gradients at initialization for the
PostLN/Pre-LN Transformer in real scenarios. The model and training configuration exactly fol-
lows Section 3.2. The experiments are repeated ten times using different random seeds. Given an
initialized model, we record the hidden states in the Post-LN/Pre-LN Transformer and find that the
norm of the hidden states satisfies the concentration property ((0.1,0.125)- bounded). We also record
the gradient for each parameter for different mini-batches. For elements in a parameter matrix, we
calculate their expected gradients and use the Frobenius norm of those values as the scale of the
expected gradient of the matrix. Figure 3(a) and 3(b) shows those statistics for FFN sub-layers.
The x-axis indexes different Transformer layers. It can be seen from the figure, the scale of the
expected gradients grows along with the layer index for the Post-LN Transformer. On the contrary,
the scale almost keeps the same for different layers in the Pre-LN Transformer. These observations
are consistent with our theoretical findings. More analysis can be found in Appendix H.
2 0 8 6 4 2
LLCiCiCiCi
uo.i=(o-G(υdx 山 W8_PBJO
(a) W 1 in the FFN sub-layers
0 5 0 5 0 5 0
.52 0 7.52。
III0.0.0.0.
UoA(O-G0)dx 山 Wɑ)一 PBJ0
(b) W2 in the FFN sub-layers
Figure 3:	Norm of expected gradients for Pre-LN/Post-LN Transformer
We further study the gradient statistics for the Post-LN Transformer after the warm-up stage with
Adam. It can be seen from the figure that the scale of the gradients are very small, and the model can
be trained with large learning rates. We believe the gradient scale is one of the reasons that the Post-
LN Transformer needs a careful learning rate scheduling in the beginning. Since the gradients are
large for some layers, using a large learning rate without warm-up may make the training unstable
(see Appendix I). As the gradients are well-behaved for the Pre-LN Transformer, we will show that
the learning rate warm-up stage can be removed for this model architecture in the next section.
4 Experiments
The Pre-LN Transformer has been implemented in several systems (Liu et al., 2019b; Baevski &
Auli, 2018), but most of them still follow Vaswani et al. (2017) to use the learning rate warm-up
stage. We conduct experiments for the Pre-LN Transformer to test whether the learning rate warm-
up stage can be removed and how to set learning rate schedulers.
4.1	Experiment Settings
Machine Translation We conduct our experiments on two widely used tasks: the IWSLT14
German-to-English (De-En) task and the WMT14 English-to-German (En-De) task. For the
IWSLT14 De-En task, we use the same model configuration as in Section 3. For the WMT14
En-De task, we use the Transformer base setting. More details can be found in the Appendix.
For training the Pre-LN Transformer, we remove the learning rate warm-up stage. On the IWSLT14
De-En task, we set the initial learning rate to be 5e-4 and decay the learning rate at the 8-th epoch
by 0.1. On the WMT14 En-De task, we run two experiments in which the initial learning rates are
set to be 7e-4/1.5e-3 respectively. Both learning rates are decayed at the 6-th epoch followed by
the inverse square root learning rate scheduler.
We train the Post-LN Transformer using the learning rate warm-up stage as the baseline. In both
IWSLT14 De-En task and WMT14 En-De task, we set the number of the warm-up stage to be 4000
8
Under review as a conference paper at ICLR 2020
Sso-Ju。石Ep_ro>
4
1	3	5	7	9	11	13	15
Epochs
°1
30
n山
3	5	7	9	11	13	15
Epochs
(a) Validation Loss (IWSLT)	(b) BLEU (IWSLT)
8
7 6 5 4
sso-JuoqEP-ro>
0 0 5
nw-Jm
—Post-LN (∕rmax = 7e-4)
Post-LN QrgX = 1.5e-≡)
Pre-LN (/∕⅛ax = 7e-4)
—Prθ-LN [lrmax = 1.5e-≡)
1	3	5 7 9 11 13 15 17 19	1	3 5 7 9 11 13 15 17 19
Epochs	Epochs
(C) Validation Loss (WMT)	(d) BLEU (WMT)
Figure 4: Performances of the models on the IWSLT14 De-En task and WMT14 En-De task
following Vaswani et al. (2017) and then use the inverse square root learning rate sCheduler. For
all experiments above, we use the Adam optimizer and set the hyper-parameter β to be (0.9, 0.98).
We set lrmax as same as the initial learning rates of the Pre-LN Transformer in eaCh Corresponding
experiment. SinCe Liu et al. (2019a) suggests that the learning rate warm-up stage Can be removed
using RAdam, we try this optimizer on the IWSLT14 De-En task. We use linear learning rate deCay
suggested by Liu et al. (2019a) and keep all other hyper-parameters to be the same as in other
experiments.
Unsupervised Pre-training (BERT) We follow (Devlin et al., 2018) to use English Wikipedia
Corpus and BookCorpus for pre-training. As the dataset BookCorpus (Zhu et al., 2015) is no longer
freely distributed. We follow the suggestions from (Devlin et al., 2018) to Crawl and ColleCt Book-
Corpus4 on our own. The ConCatenation of two datasets Contains roughly 3.4B words in total, whiCh
is Comparable with the data Corpus used in (Devlin et al., 2018). We randomly split doCuments into
one training set and one validation set. The training-validation ratio for pre-training is 199:1.
We use base model Configuration in our experiments. Similar to the translation task, we train the
Pre-LN BERT without the warm-up stage and Compare it with the Post-LN BERT. We follow the
same hyper-parameter Configuration in Devlin et al. (2018) to train the Post-LN BERT using 10k
warm-up steps with lrmax = 1e-4. For the Pre-LN BERT, we use linear learning rate deCay starting
from 3e-4 without the warm-up stage. We have tried to use a larger learning rate (suCh as 3e-4)
for the Post-LN BERT but found the optimization diverged. All experiments are ConduCted on 32
NVIDIA Tesla P40 GPUs.
4.2 Experiment Results
Machine Translation We reCord the model CheCkpoints for every epoCh during training and Cal-
Culate the validation loss and BLEU sCore. The performanCe of the models at different CheCkpoints
are plotted in Figure 4(a) - 4(d).
4https://www.smashwords.Com
9
Under review as a conference paper at ICLR 2020
First, as we can see from the figure, the learning rate warm-up stage is not critical anymore for train-
ing the Pre-LN Transformer and the performance of the learned model is competitive. For example,
on the IWSLT14 De-En task, the BLEU score and validation loss of the Pre-LN Transformer can
achieve around 34 and 4, which are comparable with the performance of the Post-LN Transformer.
Second, the Pre-LN Transformer converges faster than the Post-LN Transformer using the same
lrmax. On the IWSLT14 De-En task, the 9-th checkpoint of the Pre-LN Transformer achieves nearly
the same performance (validation loss/BLEU score) as 15-th checkpoint of the Post-LN Transformer.
Similar observations can be found in the WMT14 En-De task. The first model checkpoint of the Pre-
LN Transformer can achieve a BLEU score near 20. As a comparison, the BLEU score of the first
checkpoint of the Post-LN Transformer is less than 10.
Third, compared with RAdam, we find that the change of the position of layer normalization “dom-
inates” the change of the optimizer. According to our experiments on the IWSLT14 De-En task, we
can see that although RAdam trains the Post-LN Transformer well without the warm-up stage, it has
little difference with Adam when training the Pre-LN Transformer.
(a) Validation Loss on BERT	(b) Accuracy on MRPC
(c) Accuracy on RTE
Figure 5: Performances of the models on unsupervised pre-training (BERT) and downstream tasks
Unsupervised Pre-training (BERT) We record validation loss of the model checkpoints and plot
them in Figure 5(a). Similar to the machine translation tasks, the learning rate warm-up stage can
be removed for the Pre-LN model. The Pre-LN model can be trained faster. For example, the
Post-LN model achieves 1.69 validation loss at 500k updates while the Pre-LN model achieves
similar validation loss at 700k updates, which suggests there is a 40% speed-up rate. Note that
Twarmup (10k) is far less than the acceleration (200k) which suggests the Pre-LN Transformer is
easier to optimize using larger learning rates. We also evaluate different model checkpoints on the
downstream task MRPC and RTE (more details can be found in the appendix). The experiments
results are plotted in Figure 5(b) and 5(c). We can see that the Pre-LN model also converges faster
on the downstream tasks.
As a summary, the experiments on both machine translation and unsupervised pre-training tasks
show that training the Pre-LN Transformer does not rely on the learning rate warm-up stage and can
be trained much faster than the Post-LN Transformer.
5 Conclusion and Future Work
In this paper, we study why the learning rate warm-up stage is important in training the Transformer
and show that the location of layer normalization matters. We show that in the original Transformer,
which locates the layer normalization outside the residual blocks, the expected gradients of the
parameters near the output layer are large at the beginning of the optimization. This leads to an
unstable training when using a large learning rate. We further show that the Transformer which
locates the layer normalization inside the residual blocks, can be trained without the warm-up stage
and converges much faster. In the future, we will investigate other strategies of positioning the layer
normalization, as well as the advantage of layer normalization to the Transformer from a theoretical
perspective.
10
Under review as a conference paper at ICLR 2020
References
Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. Character-level lan-
guage modeling with deeper self-attention. arXiv preprint arXiv:1808.04444, 2018.
Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling.
arXiv preprint arXiv:1809.10853, 2018.
Dzmitry Bahdanau, KyungHyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. 2017.
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo. Magnini. The
fifth PASCAL recognizing textual entailment challenge. 2009.
Zihang Dai, Zhilin Yang, Yiming Yang, William W Cohen, Jaime Carbonell, Quoc V Le, and Ruslan
Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv
preprint arXiv:1901.02860, 2019.
Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated
convolutional networks. In International Conference on Machine Learning, pp. 933-941, 2017.
Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Eukasz Kaiser. Universal
transformers. arXiv preprint arXiv:1807.03819, 2018.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
William B Dolan and Chris. Brockett. Automatically constructing a corpus of sentential paraphrases.
In Proceedings of the International Workshop on Paraphrasing., 2005.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.
Sergey Edunov, Myle Ott, Michael Auli, and David Grangier. Understanding back-translation at
scale. arXiv preprint arXiv:1808.09381, 2018.
Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional
sequence to sequence learning. In International Conference on Machine Learning, pp. 1243-
1252, 2017.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the thirteenth international conference on artificial intelligence and
statistics, pp. 249-256, 2010.
Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, An-
drew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet
in 1 hour. arXiv preprint arXiv:1706.02677, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask r-cnn. In Proceedings ofthe
IEEE international conference on computer vision, pp. 2961-2969, 2017.
Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, and Mu Li. Bag of tricks for
image classification with convolutional neural networks. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pp. 558-567, 2019.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Guillaume Klein, Yoon Kim, Yuntian Deng, Vincent Nguyen, Jean Senellart, and Alexander Rush.
Opennmt: Neural machine translation toolkit. In Proceedings of the 13th Conference of the
Association for Machine Translation in the Americas (Volume 1: Research Papers), volume 1, pp.
177-184, 2018.
11
Under review as a conference paper at ICLR 2020
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, et al. Moses: Open source
toolkit for statistical machine translation. In Proceedings of the 45th annual meeting of the as-
sociation for computational linguistics companion volume proceedings of the demo and poster
sessions,pp.177-180, 2007.
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450, 2016.
Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei
Han. On the variance of the adaptive learning rate and beyond. arXiv preprint arXiv:1908.03265,
2019a.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
approach. arXiv preprint arXiv:1907.11692, 2019b.
Yiping Lu, Zhuohan Li, Di He, Zhiqing Sun, Bin Dong, Tao Qin, Liwei Wang, and Tie-Yan Liu.
Understanding and improving transformer from a multi-particle dynamic system point of view.
arXiv preprint arXiv:1906.02762, 2019.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of the 40th annual meeting on association for
computational linguistics, pp. 311-318. Association for Computational Linguistics, 2002.
Martin PoPel and Ondrej Bojar. Training tips for the transformer model. The Prague Bulletin of
Mathematical Linguistics, 110(1):43-70, 2018.
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
models are unsupervised multitask learners. 2019.
Sebastian Ruder. An overview of gradient descent optimization algorithms. arXiv preprint
arXiv:1609.04747, 2016.
Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with
subword units. arXiv preprint arXiv:1508.07909, 2015.
Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with
subword units. In ACL, 2016.
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.
In Advances in neural information processing systems, pp. 3104-3112, 2014.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethink-
ing the inception architecture for computer vision. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 2818-2826, 2016.
Mingxing Tan and Quoc V Le. Efficientnet: Rethinking model scaling for convolutional neural
networks. arXiv preprint arXiv:1905.11946, 2019.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop, coursera: Neural networks for machine
learning. University of Toronto, Technical Report, 2012.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998-6008, 2017.
Ashish Vaswani, Samy Bengio, Eugene Brevdo, Francois Chollet, Aidan N. Gomez, Stephan
Gouws, Llion Jones, Eukasz Kaiser, Nal Kalchbrenner, Niki Parmar, Ryan Sepassi, Noam
Shazeer, and Jakob Uszkoreit. Tensor2tensor for neural machine translation. CoRR,
abs/1803.07416, 2018. URL http://arxiv.org/abs/1803.07416.
Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cam-
bridge University Press, 2019.
12
Under review as a conference paper at ICLR 2020
Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F Wong, and Lidia S Chao.
Learning deep transformer models for machine translation. arXiv preprint arXiv:1906.01787,
2019.
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V
Le. Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprint
arXiv:1906.08237, 2019.
Yang You, Zhao Zhang, Cho-Jui Hsieh, James Demmel, and Kurt Keutzer. Imagenet training in
minutes. In Proceedings of the 47th International Conference on Parallel Processing, pp. 1.
ACM, 2018.
Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701,
2012.
Yukun Zhu, Ryan Kiros, Richard Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching
movies and reading books. In arXiv preprint arXiv:1506.06724, 2015.
13
Under review as a conference paper at ICLR 2020
Appendix
A Experimental Settings
A. 1 Machine Translation
Experiment on Section 3 The training/validation/test sets of the IWSLT14 German-to-English
(De-En) task contain about 153K/7K/7K sentence pairs, respectively. We use a vocabulary of 10K
tokens based on a joint source and target byte pair encoding (BPE) (Sennrich et al., 2015). All of
our experiments use a Transformer architecture with a 6-layer encoder and 6-layer decoder. The
size of embedding is set to 512, the size of hidden nodes in attention sub-layer and position-wise
feed-forward network sub-layer are set to 512 and 1024, and the number of heads is set to 4. Label
smoothed cross entropy is used as the objective function by setting = 0.1 (Szegedy et al., 2016),
and we apply dropout with a ratio 0.1. The batch size is set to be 4096 tokens. When we decode
translation results from the model during inference, we set beam size as 5 and the length penalty as
1.2.
Experiment on Section 4 The configuration of IWLST14 De-En task is the same as in Section
3. For the WMT14 En-De task, we replicate the setup of (Vaswani et al., 2017), which consists of
about 4.5M training parallel sentence pairs, and uses a 37K vocabulary based on a joint source and
target BPE. Newstest2013 is used as the validation set, and Newstest2014 is used as the test set. One
of the basic configurations of the Transformer architecture is the base setting, which consists of a
6-layer encoder and 6-layer decoder. The size of the hidden nodes and embeddings are set to 512.
The number of heads is 8. Label smoothed cross entropy is used as the objective function by setting
= 0.1. The batch size is set to be 8192 tokens per GPU on 16 NVIDIA Tesla P40 GPUs.
A.2 Unsupervised Pretraining
We follow Devlin et al. (2018) to use English Wikipedia corpus and BookCorpus for the pre-training.
As the dataset BookCorpus (Zhu et al., 2015) is no longer freely distributed. We follow the sugges-
tions from Devlin et al. (2018) to crawl and collect BookCorpus5 on our own. The concatenation of
two datasets includes roughly 3.4B words in total, which is comparable with the data corpus used
in Devlin et al. (2018). We first segment documents into sentences with Spacy6; Then, we normal-
ize, lower-case, and tokenize texts using Moses (Koehn et al., 2007) and apply BPE(Sennrich et al.,
2016). We randomly split documents into one training set and one validation set. The training-
validation ratio for pre-training is 199:1.
The base model in Devlin et al. (2018) consists of 12 Transformer layers. The size of hidden nodes
and embeddings are set to 768, and the number of heads is set to 12.
A.3 GLUE Dataset
MRPC The Microsoft Research Paraphrase Corpus (Dolan & Brockett, 2005) is a corpus of sen-
tence pairs automatically extracted from online news sources, with human annotations for whether
the sentences in the pair are semantically equivalent, and the task is to predict the equivalence. The
performance is evaluated by the accuracy.
RTE The Recognizing Textual Entailment (RTE) datasets come from a series of annual textual
entailment challenges (Bentivogli et al., 2009). The task is to predict whether sentences in a sentence
pair are entailment. The performance is evaluated by the accuracy.
Fine-tuning on GLUE tasks We use the validation set for evaluation. To fine-tune the models,
following Devlin et al. (2018); Liu et al. (2019b), we search the optimization hyper-parameters in
a search space including different batch sizes (16/32), learning rates (1e-5 - 1e-4) and number
of epochs (3-8). We find that the validation accuracy are sensitive to random seeds, so we repeat
5https://www.smashwords.com
6https://spacy.io
14
Sl
(ɛɪ)	pf = f+p = (∣ll^js⅛ll)a + (∣llε.js⅛ll)a =
G L
(Cl)
,	, I=f u ,	,
(∣ £，浮加 底幽 Gl 加**)rrpχR)叱 + (∣ll^s⅛∣∣)3 + (!∣∣εU'>∣∣)3 =
u G
ɑɪ)	(J，浮晶⅛,舄C 尿 + (∣ll^,s0>ll)a + (∣llε.js⅛ll)3 = (∣lls.,s0>ll)a
Smojjoj sb (∣∣∣s^s⅛∣∣)3I Jθəɪŋɔs Qqj əiBUlDsə ʊŋɔ Q瓜 ζsτqj uopəsŋg
(ox) I = (∣llε∙,s⅛ll∣)a =(总 XH圳(九小 £，蓝力	=
(6)	(总舄以门小’£，?Z'<阿加底山(九μ型辑Em飞M)=
(8)	(∣ll,⅛½ι(fτ½ιε.j⅛m3H∣l)a = (∣ll^s⅛ll)a
5Λ∏q əm，[ buiuiə^ oj 丽PJoɔɔv ^(p∕τ 'o)ΛΓ UlaIJ
pəɪdures sqjqbtibλ UIOPUBJ treτssnBQ p rτ əjb 乙山 PUB uτ sjuəməɪə Qqj ζuoτpunj UOHBADɔB
lɪɪəX Qqj JOj ζureSγ UOHBZ[ɪBUnoNJo uotjtu^əp Qqj Aqp = ∣∣∣gijs⅛∣∣ OABq əM 的叫曲均
■p = (∣lljs->ll)a
< (!∣ljs⅛^Zτ∣∣)a + (i∣∣js⅛∣∣)a =(外岛舄圜河 + 倒蓝ME =(加蓝’(MEPUB
【=?
(ʌ) PZ > (别浮&R；H)回 +(圳浮=(圳]蓝国河+ (圳蓝'(ME =
u L
1=，
(9)	(蓝MyV‰*R)* + (∣llτu>ll)a + (∣ll,s⅛ll)a =
u 6
(s)	(J 舄％，浮争尿 + (!∣lvjs⅛ll)a + (!∣l,s⅛ll)a = (!∣l^,s⅛ll)a
5Λ∏q əm to < ; UQqM uotjbztjbuπom jqAbi jo uopra^əp Qqj Aq p = ∣∣∣js⅛∣∣ əɔuɪs ^(p∕l 'o)ΛΓ
xπojj pəɪdures sqjqbtibλ UK)PUm ubtssπbq p rτ əib ^λM 典 sjuəməɪə Qqj 'uotjbztjbtjtut RABX Suτs∩
"0 < ? JOj 不僚' …"VjsoJx IndlnO əjbtpəuuəjut qo∏5 JO UnOU 叮 pəiɔədXə Qqj əjnunjsə ISJU əM
∙p=e(tγ,⅛⅛ = ∣l∣7⅛ll = IIIWmll
əɔuɪs 少 snτpBj Jo QjQqds-ɪ - p Qqj ojuo a jojɔəʌ Aub sjɔəfojd Uotjbztjbtjtut jb uotjbztjbuπou jqAbj
JBqj əəs oj KSBə SHl	= (O)Nrl SB pəjndmoɔ st uotjbztjbuπou jqAbj Qqj 'uotjbztjbtjtut jγ 'f001d
Z VWWHl Ho HOOHd ɔ
□	也第=砰⑺X德产。o" i =叨⑺。5Xd产£[/ = [o < τxl⅛]a∣
=[0 < IXlKlX)IYpX哧=(0 < ⅛)d[θ < ⅛lz(⅛)∩13H]≡τ=pZ = [z(⅛)∩13H]≡τ=pZ
=(∣∣Kx)m3H∣∣)a∞qi 'τXJ0 uopɔunj Ajτsuap XHnqeqtMd0卬 Se (x)Xdəjouəɑ S P)N uopnq
-μjsτp qjτM SQjqBTJBA UlOPUBJ UBissnBQ p rτ əib ⅛ qoτqM uτ (PX'…'⅛πχ) = X əjouəɑ Joaid
[VWWHl Ho HOOHd 9
XθBjnθOB UOTJBpTJBΛ JO JBΛJ5JUT
əɔuəp^uoɔ %ςβ əgi əjndmoɔ PUB spəəs UlOPUBJ juəjə^ɪp Suτsn səunj 9 JoJ NSBI gɔeə uo Suτunj-ou^
0303 mɔl JB jədŋd əɔuəjəjuoɔ B SB mətaəj jəpun
9T
(EZ)
(CC)
(IC)
f!FII	/Fll _ ⅜
fi1fi pλ (#)Nle
ζuπoj XTflBta aqj uɪ ∙f ≠ « uaqM q = % PUB C = % uaqM ɪ = % ajaqM
SB UOTJBZTTBUnOU JQAbT JO UBTQOOBf QUJ 5JBTn0TB0 Xτjp∏dx5 0丛
13小
(OC)	/	=ʧʃ)m
sb uəjjtjməj əg treɔ uotjbzt
-ɪŋunou jqAbj Qqj UQqj ə (ɪ '…'['[)=ɪ QjQqM '([,[£ - ʃ)ʃ = ft əjouəɑ £ υuιui∂rι/o∕ooλj
□	joojd jno əjəɪdmoɔ əM
əəuəh ^(t '…'['['θ) əib sən[BAuə旗ə suɪpuodsəɪɪoɔ jpqj PUB 'ŋɪŋ - ʃ jo SJOlɔəAuə旗ə Qqj ɪɪŋ əib
② OS •[多 2 joj ②=ŋɪŋ^ə — ⅛ = (ŋɪŋ — ʃ)^ə pun Q = ŋ — ɪə = ŋɪŋɪə — ɪə = (ŋɪŋ — ʃ)ɪə
5Λ∏q əm UQqi *(个2)∏∏ JQJ 勿丁② PUB ŋ = ɪə IBqI qons sjojɔəʌ jτun əg '…"a} jə^ 'Jooλj
*0上。
I Λ∂ψp Sl QF - Ifi)i阚a 叫i U叫i 1=^nŋn ιυψ ψns λoio∂^ DaqPNmQ 场“ > Kiuuiaq
ψ buiuiəi uo pəsBq st £ buiuiəi jo joojd Qqi
£ VWWHl Ho HOOHd Q
□
，	， -uopɔnpm Aqp(⅛ + ɪ) > (∣∣∣2›∣∣)a > P(y + T)
OAeq əm uaqι •遽 + (∣∣∣2>∣∣)a > (∣l∣fl⅛ll)3 > Pf + (!∣∣a⅛∣∣)≡ əʌŋɪl Q瓜'卬°q 8uraτquιo3
(6τ)	p∣ + (!∣lε⅛⅛ll)a =
(st)	(∣lls⅛⅛ll)a + (∣llε⅛⅛ll)a =
(ʌɪ) (χ3∙⅛⅛⅛)as + (!∣ls⅛⅛ll)a + (!∣lε⅛⅛ll)a = (!∣lf,⅛ll)a
,	,	,	ζ(ετ)-(ττ)
°J jbIWs PUV -p + (∣ll≡->ll)a > (∣llε⅛⅛ll)a > (∣IIZ>ll)a OM w əəs «j 心⑪ sɪ ∏
【=?
(9【)(外岛	+(外H(ME =(卯V(ME+ ("H(M河=
U L
1=，
(ST)	5 My√k*R)* + (∣lle⅛⅛ll)a + (∣II2>II)≡ =
U 6
(H)	(χa⅛j⅛⅛)≡ + (!∣le⅛⅛ll)a + (!∣I2>II)≡ = (∣llε⅛->ll)a
əɔuɪs JQunojsuBJi NTOJd W joj (IlHMI)® pu∏oq uŋɔ əM ənb[Uqɔəi ib∏uπs Suts∩
0303 Xm JB jədŋd əɔuəjəjuoɔ B sb mətaəj jəpun
Under review as a conference paper at ICLR 2020
and
JLN (X)= dLN(x)	(24)
∂x
_dLN(X) ∂y	(25)
=∂y ∂x	( 5)
=√d^ιΓ^ιΓ(I - iθɪ)(I -卞 1>1).	(26)
kyk2	kyk22	d
Since the eigenvalue of the matrix (I - y>2) and (I - d 1>1) are either 1 or 0 (by Lemma 4), We
kyk2	d
have ∣∣(I - f⅛)k2 = O(1) and ∣∣(I - d 1>1)∣∣2 = O(1). So the spectral norm of JLN(x) is
kyk2	d
kJLN(X)k2 = O( ∣≡ ) = O(件)
(27)
□
E Proof of Theorem 1
The proof of Theorem 1 is based on Lemma 5:
Lemma 5. Let Y be a random variable that is never larger than B. Then for all a < B,
Pr[Y ≤ a] ≤
E [B - Y]
B - a
(28)
Proof. Let X = B - Y, then X ≥ 0 and Markov’s inequality tells us that
Hence
PrX ≥ B - a] ≤ ∣[X] B-a	(29)
Pr[Y ≤ a]≤ R B-a	(30)
	□
Proof of Theorem 1. We prove Theorem 1 by estimating each element of the gradient matrix.
PT
Namely, we will analyze T^dLL for p,q ∈ {1,...,d}. The loss of the post-LN Transformer can
∂Wpq
be written as
1n
post	post	post
L(XL+1,1,…,xL+1,n) = n / ^L(xL+1,i)	(31)
Through back propagation, for each i ∈ {1, 2, ..., n} the gradient of L(XL+1,i) with respect to the
last layer’s parameter W2,L in the post-LN setting can be written as:
∂L( post )	∂L( post ) ∂ post ∂ post,5 ∂ post,4
(XL+1,i)	(XL+1,i) XL+1,i XL,i XL,i
∂wpqL	― ∂xL+1,i	∂xLoSt,5 ∂xLoSt,4 ∂wpqL
∂L( post )	∂ post,4
(XL+1,i )	post,5 XL,i
=JLN (XL,i	) K
(32)
(33)
∂L(Xpost )
IpL+1, JLN(XLost,5)(0,0,…,[ReLU(XL0St,3W1,L)]p,…,0)>	(34)
∂XL+1,i
17
Under review as a conference paper at ICLR 2020
Here [ReLU(xpLo,ist,3W1,L)]p means the p-th element of ReLU(xpLo,ist,3W1,L). So the absolute value
of dL(XL2+L,∣ can be bounded by
∂Wpq
∂L(xpost )	∂ L(xpost )
I	：L" l≤k LLs+1,i k2kJLN(XLost,5)k2k(0,0,…,[ReLU(xLOSt,3W1,L)]p,…,0)>k2
∂Wpq	∂xL+1,i
(35)
∂L(xpost )
=k IpLs+1, k2kJLN (XLost,5)k2∣[ReLU(xL0st,3W1,L)]p∣	(36)
∂xL+1,i
which implies
∂L(Xpost )	∂L(Xpost )
I	LL2+L,i I2 ≤ k √pLs+1,i k2kJLN(XLost,5)k2∣[ReLu(xLost,3w1,L)]p∣2	(37)
∂Wpq	∂XL+1,i
Since we assume that all the derivatives are bounded, we have ∣∣ dL(XL三1,' ∣∣2 = O(1). So
∂xL+1,i
IdLWo+L,i) I2 = O([kJLN(XLost,5)k2∣[ReLU(χLost,3W 1,L)]p∣2i)	(38)
Since ∣XpLo,ist,3∣22 = d, [XpLo,ist,3W1,L]p has distribution N(0, 1), using Chernoff bound we have
Pr[I[χLost,3W 1,L]pI ≥ ao] ≤ exp(-a2).
So
Pr[ReLU([xLost,3W 1，L]p)2 ≥ 2ln100d] ≤ 0-01.
Thus with probability at least 0.99, for all p = 1, 2, ..., d we have ReLU([XpLoist,3W1,L]p)2 ≤
2ln 100d.	,
Since with probability 1 一 δ(e), lkxL,i 旧12-t”,i^~— ≤ G We have IIXLost,5∣∣2 ≤ (1 +
EkxL,i k2	,
)E∣XpLo,ist,5∣22. Using Lemma 5, we have
Pr[∣XpLo,ist,5 ∣22 ≤ α0E∣XpLo,ist,5 ∣22] ≤
(1 + e)EkXLost,5k2- EkXLost,5k2
-(1 + e-a )EkXLost,5k2-
1 + 一 α0
(39)
for an arbitrary constant α0 > 0, which equals
Pr[∣XpLo,ist,5 ∣22 ≥ α0E∣XpLo,ist,5 ∣22] ≥ 1 一
1 + 一 α0
(40)
So according to union bound, with probability at least 0.99 — δ(e) 一 we have I dL(χL2+L,i) I2 =
α0	∂ Wpq
l^ll Tr N, TpoSt,5∖l∣2∣∣^ReT TlYTpoSt,3 W1，L、1 ∣2^∣、V。2d ln 100d、V。 d ln d _	、一 /ŋr lnd、
O(	IIJLN (XLi	)k2I[ReLU(XLi	W	)]pI	) ≤	O(	k	ρost,5∣∣2 )	≤ O( °CEk TPOSt,5∣∣2 ) = O( ^O0 ).
，	，	kxL,i	k2	α0EkxL,i	k2	α0
So we have
and
dL5) 12 ≤ 1XI
∂w⅛l n n^'
i=1
d L(XLo+t,i) l2 = O也)
∂w⅛l	ao
(41)
_ ~
k ∂WLL kF = t
pxJ∂⅛2=O( T)
iI1 X
n
18
Under review as a conference paper at ICLR 2020
The loss of the pre-LN Transformer can be written as
1n
L(XFinal,1,..., XFnal,n) = 1 X L(XFinal,i)	促)
i=1
Using the same technique, in the pre-LN setting the gradient of L(XpFrienal,i) with respect to the last
layer’s parameter W2,L can be written as
∂L(Xpre )	∂L(Xpre ) ∂Xpre ∂Xpre ∂Xpre,5
dL(XFinal,i) _dL(X Final,i ) dxFinal,i dxL+1,i dxL,i	(43、
∂WpqL	= ∂xFnal,i !Xp+7 ^pJ5 ∂W2L	( 3)
∂L(Xpre	)
=d PFi	, JLN(xL+v)(0,0,…,[ReLU(xLre,4W1,L)]p,…,0)>	(44)
∂XFinal,i
So the absolute value of each component of the gradient is bounded by
∂L(Xpre	)	∂L(Xpre	)
I	二 2nLl,i l≤k IUi k2JLN (XLZ i)k2k(0,0,∙∙∙,[ReLU(xL74W 1,L)]p,∙∙∙, 0)k2
∂Wpq
∂ XF inal,i
(45)
∂L(Xpre	)
=k dXpFral,i k2JLN(XLA)k2∣[ReLU(XLre,4W1,L)相	(46)
∂ XF inal,i
Since kXpLr,ie,4k22 = d and [XpLr,ie,4W1,L]p obeys distribution N(0,1), using Chernoff bound we have
Pr[∣[XL74W 1，L]p ∣≥ ao] ≤ exp(-a2).
So
Pr[ReLU([XLre,4W 1,L]p)∣ ≥ 2ln100d] ≤ 0-01.
So with probability at least 0.99, for allp = 1, 2, ..., d we have ReLU([XpLr,ie,4W1,L]p)∣ ≤ 2 ln 100d.
Since with probability 1 - δ(e), lkxL+Eik2-啰：注1，/21
EkxL+1,i k2
E)EkXpLr+e1,i k∣∣ . Using Lemma 5, we have
≤	, we have kXpLr+e1,ik∣∣ ≤
(1+
Pr[kXpLr+e1,ik∣∣ ≤α0EkXpLr+e1,ik∣∣] ≤
(1 + E)EIlXLiι,ik∣-E∣∣XL2,ik∣
(1 + e-α0)EkXL+1,ik∣
E
1 + E 一 α0
(47)
which equals
Pr[kXL7ι,ik∣ ≥ αoEkXL7ι,ik∣] ≥ 1 - 1 + ：一 劭
(48)
According to union bound, with probability 0.99 — δ(e) 一 [十二。We have ∣
∂L(xpFre
'inal,i ) 11
∂W2qL- |
DZhlTr RJTpre '1∣∣∣∣ΓRpΤ TiYTpre,4 W1,L∖1 l∣^i∖ V ∣d( ∣d ln 100d V	d ln d ∖ — DZInd、
O([kJLN (XL+1,i)k∣|[ReLU(XL,i W	)]p| ]) ≤ O( kχLreι,ik2 ) ≤ O( αoEkxLreι,ik2 ) = O( O0L ).
So we have
I dL ∣∣ = I 1 X dL(XFinal,i) ∣∣ =O(Ind)
∂wpqL — n = ∂wpqL - - O0L
(49)
.. ci 7∙
Thus k ∂W2,L kF
,pd,q= | ∂⅛? ≤ O(
Take ao = *, we have that with probability at least 0.99 一 δ(e) 一 ^^+^, for the Post-LN Trans-
~ ~
former we have k ∂WLLkF ≤ O(dSnd) and for the Pre-LN Transformer we have k∂WLLkF ≤
OdqL)	□
19
Under review as a conference paper at ICLR 2020
F Extension to other layers
For simplicity, We denote xl = Concat(xl,1, ..., xl,n) ∈ Rnd and xlk = Concat(xlk,1, ..., xlk,n) ∈ Rnd
for k = {1, 2, 3, 4, 5}. Then in the Post-LN Transformer, the gradient of the parameters in the l-th
layer (take W2,l as an example) can be Written as
内尸 内尸 L ∂xpost ∂ 产Ost	∂xpost
-L- = -L- ( Y dj1) dxl+L where dj1
∂W2，1 ∂xL+t j=l+1 ∂xpost) ∂W2，l ,	∂xpost
post	post,5	post,3	post,2
∂xj +1 ∂xj	∂xj	∂xj
∂xpost,5 ∂xpost,3 ∂xpost,2 ∂xpost .
The Jacobian matrices of the Post-LN Transformer layers are:
dxp+sιt
∂xpost,5
/ JLN (XpoIst,5)
(50)
∂xpost,5
∂xpθst,3
∖
W2,j
JLN(xjp,onst,5)
( Jj1
W1,l
W2,j
Jjn
W1,l
(51)
where Jij = diag
post,3
xj,i
∖
0 post,3
. . , σ	xj,i
∈ Rd×d.
∂xpost,3
∂xpost,2
(Jln(xpo1st,2)
(52)
JLN(xjp,onst,2
I
/
∖

∖
∖
∖
/ I
/
)
1 W Vj
n
∂xpost,2
∂xpost
Using Holder’s inequality, We have
1 W Vj ...
n
1 W Vj ...
n
(53)
∖

∖
1 W Vj
n
Since
∂ post
Ek jt k2
dxj + 1
∂xpost,5
≤E k
dxp+sιt
∂xpost,5
∂xpost,5
k2k ∂X⅛ 回
∂χpost,3
∂xpost,2 k2k
∂xpost,2
≤tE k
post,5	po
∂xj ,	∂xj
A post,3 k2k U
∂xj ,	∂xj
∂xpost
st,3
st,2 k22k
k2
∂xpost,2
dXpost k2
(54)
(55)
diag(JLN(xjp,o1st,5), ...,JLN(xjp,onst,5)),
we have
post
Je [k ⅛⅛ k2
j UE卜

.ZE ρθdt-5i2 ≈ 2 /3 when IIxpoLst,51∣ 2 concentrates around its expectation EkxpoLst,51∣ 2 which equals
kxj,1 k2	3	,	,
nr
3 d according to Lemma 2. Therefore, when we estimate the norm of ∂WLT for PoSt-LN transformer,
there exists a term O(I(L l)/2), which exponentially decreases as l goes smaller. Similarly, in the
pre-LN Transformer, the gradient can be written as
ʃ—"
∂ L	∂xpre
∂L	∂ xF inal
∂W2,l — ∂xFinai ∂x 舞 1
~
∂ L
L ∂ pre pre	∂ pre ∂ pre ∂ pre,3
( Y dxj+1 ) dxl+1 Where	* = j+1 j
( 11 ∂xpre ) ∂WV,l ,	∂xpre = ∂xpre,3 ∂xpre .
j=l+L	j	j	j	j
The Jacobian matrices of the Pre-LN Transformer layers are:
20
Under review as a conference paper at ICLR 2020
j
∂xpre,3
/
I
W2,j
(Jlh0)
∖
∖

∖
W2,j
∖
W1,j
/ JLN je,3)
W1,j
∖
JLN(xjp,rne,3)
∂xpre,3
~∂j~
I
∖
∖
1 W Vj
n
.
.
.
1 W Vj
n
nlWVjI (JLN je)	∖
niWVJ ∖	. jln(Xprej
(56)
If l is sufficiently large, the norm of JLN(Xjre) and JLN(Xpre,3) are very small (of order O(√j))
∂xpre	∂xpre,3
as j is between l + 1 and L, which means the eigenvalues of matrix g P+13 and ∂Xpre are close
∂xj	j
∂χPre	∂χPre,3	∂L
to 1. Then We can see that Ek g Pre∖ ∣∣2 and Ek ∂Xpre ∣∣2 are nearly 1, and the norm of ∂WWL2,ι for
xj	j
pre-LN transformer is independent of l when l is large.
G EXAMPLES OF (, δ)-B OUNDED RANDOM VARIABLES
In this section we give an example of (, δ)-bounded random variable. This example comes from
Example 2.5 in (Wainwright, 2019) and we give a short description below.
If Z = (Z1, ..., Zn ) is a Gaussian vector with distribution N(0, In ), then Y = kZk22 = Pkn=1 Zk2
has distribution χ2n . And EY = Pkn=1 EZk2 = n
A random variable X with mean μ = E[X] is called sub-exponential if there are non-negative
parameters (ν, α) such that E[exp(λ(X — μ))] ≤ exp( ν-2λ-) for all ∣λ∣ < 1. The next proposition
comes from Proposition 2.2 in (Wainwright, 2019).
Proposition 1 (Sub-exponential tail bound). Suppose that X is sub-exponential with parameters
(ν, α). Then
P[X - μ ≥ t] ≤ / exp(- 2tν-) if 0 ≤t ≤ νɑ, and
1 exp(-2α)	fort> Vr
(57)
andfrom Example 2.5 in (Wainwright, 2019), the X variable Y is sub-exponential with parameters
(ν, α) = (2√n, 4). So we can derive the one-sided bound
P [Y - n ≥ n] ≤ exp(-n2/8), for all ∈ (0, 1)
So Y is (, δ)-bounded with ∈ (0, 1) and δ = exp(-n2/8).
(58)
H	Empirical verification of the theoretical findings
As our theory is derived based on several simplifications of the problem, we conduct experiments to
study whether our theoretical insights on the gradients are consistent with what we observe in real
scenarios. We empirically study the gradients at initialization for both Post-LN/Pre-LN Transformer
on the IWSLT14 De-En task. The general model and training configuration exactly follow Section
3.2. The experiments are repeated ten times using different random seeds.
Empirical verification of concentration property Given an initialized model, we record the hid-
den states in the Post-LN/Pre-LN Transformer and find that the norm of the hidden states satisfies
the concentration property ((0.1,0.125)-bounded).
21
Under review as a conference paper at ICLR 2020
0 5 0 5 0 5 0
3 2 2 1 1 0 0
CiCiCiCiCiCiCi
Uo.I=(o-G(υdx3 W8_PBJO
(a) Pre-LN Transformer	(b) Post-LN Transformer
Figure 6:	Norm of expected gradients of W2 in the last FFN sub-layer in different size of the
Transformer architecture
Empirical verification of Theorem 1 In Theorem 1, the theory suggests that for any sizes of the
Post-LN Transformer, the scale of the gradient norm in the last FFN sub-layer remains the same.
On the contrary, the scale of the gradient norm in the last FFN sub-layer of the Pre-LN Transformer
decreases as the size (total depth) of the model grows.
We conduct experiments to study the gradient norm in the last FFN sub-layer in different sizes of
the Transformer architecture at initialization to verify Theorem 1. We train 6-6/8-8/10-10/12-12/14-
14 Post-LN/Pre-LN Transformer models, and record the gradient norm of the final FFN layer in
different Transformer models. The results are plotted in Figure 6. The x-axis is the size of the
model, and the y-axis is the value of the gradient norm of W2 in the final FFN sub-layer. It can be
seen from the figure when the number of layers grows, the gradient norm remains in the Post-LN
Transformer (around 1.6) and decreases in the Pre-LN Transformer. This observation is consistent
well with our theory.
Empirical verification of extended theory We record the gradient for each parameter for differ-
ent mini-batches. For elements in a parameter matrix, we calculate their expected gradients and use
the Frobenius norm of those values as the scale of the expected gradient of the matrix. Figure 3(a)
and 3(b) shows those statistics for FFN sub-layers. The x-axis indexes different Transformer layers.
It can be seen from the figure, the scale of the expected gradients grows along with the layer index
for the Post-LN Transformer. On the contrary, the scale almost keeps the same for different layers
in the Pre-LN Transformer. These observations are consistent with our theoretical findings.
Given the analysis above, we think our derived theory at the initialization stage is useful and consis-
tent with the empirical studies above.
I	Large Gradients in Post-LN Transformer hurts the
OPTIMIZATION
Theoretically, we find that the gradients of the parameters near the output layers are very large
for the Post-LN Transformer and suggest using large learning rates to those parameters makes the
training unstable. To verify whether using small-step updates mitigates the issue, we conduct a set
of experiments that follows the setting in Section 3.3. There are two simple ways of “small-step
updates”, gradient norm clipping, and using small learning rates.
Experiments on using small learning rates We find using a very small but fixed learning rate can
optimize the Post-LN Transformer (without the learning rate warm-up step) to a certain extent. We
use a fixed learning rate of 1e-4 at the beginning of the optimization, which is much smaller than
the lrmax = 1e-3 in the paper. Please note that as the learning rates during training are small, the
training converges slowly, and this setting is not very practical in real large-scale tasks. We plot the
validation curve together with other baseline approaches in Figure 7. We can see from the figure,
the validation loss (pink curve) is around 4.3 in 27 epochs. This loss is much lower than that of the
22
Under review as a conference paper at ICLR 2020
sso^∣ UoQeP=e>
1 3 5 7 9 11 13 15 17 19 21 23 25 27
Epochs
Ooo
3 2 1
n山Im
3 5 7 9 11 13 15 17 19 21 23 25 27
Epochs
‰ax = lθ^3, w/0 warm-up	/「maX=Ie-3, Twarmup = 500	→-	/「maX=Ie-3,	Twarmup =	4000
-*	lrma× = 5θ-4> w/o warm-up	■■■*■■	Irmax = 5e-4, Twarmup = 500	-W-	Irmax = 5e-4,	Twarmup =	4000
-■	ʃʃma* = Ie—4, w/o warm-up	..	Irmax = Ie-3, w/o warm-up, gradient norm clipping
Figure 7:	Performances of the models on the IWSLT14 De-En task.
Post-LN Transformer trained using a large learning rate (blue curve). But it is still worse than the
SOTA performance (green curve).
Experiments on using gradient norm clipping We find that using a small value of clip-norm
can also optimize the Post-LN Transformer (without the learning rate warm-up stage). In the ex-
periments, we find the gradient norm at initialization is about 5.00 and thus we clip the norm of the
gradient update to 0.5. We plot the validation curve in Figure 7 (see gray curve). It can be seen from
the figure that the performance is similar to the “small learning rate” experiment.
Discussions We think the experiments above help to verify our conclusion. Using a small learning
rate/clip norm mitigates the instability of updating models with large gradient values. Although the
theory we prove is based on some simplifications of the problem, the theoretical insights help us
understand the optimization of different networks in practice.
23