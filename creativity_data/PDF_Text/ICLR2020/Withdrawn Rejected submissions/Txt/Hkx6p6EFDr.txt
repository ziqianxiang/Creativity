Under review as a conference paper at ICLR 2020
Equivariant Entity-Relationship Networks
Anonymous authors
Paper under double-blind review
Ab stract
The relational model is a ubiquitous representation of big-data, in part due to
its extensive use in databases. However, recent progress in deep learning with
relational data has been focused on (knowledge) graphs. In this paper we propose
Equivariant Entity-Relationship Networks, a general class of parameter-sharing
neural networks derived from the entity-relationship model. We prove that our
proposed feed-forward layer is the most expressive linear layer under the given
equivariance constraints, and subsumes recently introduced equivariant models for
sets, exchangeable tensors, and graphs. The proposed feed-forward layer has linear
complexity in the data and can be used for both inductive and transductive reasoning
about relational databases, including database embedding, and the prediction
of missing records. This provides a principled theoretical foundation for the
application of deep learning to one of the most abundant forms of data.
1 Introduction
In the relational model of data, we have a set of entities, and one or more instances of each entity.
These instances interact with each other through a set of fixed relations between entities. A set of
attributes may be associated with each type of entity and relation.1 This simple idea is widely used to
represent data, often in the form of a relational database, across a variety of domains, from shopping
records, social networking data, and health records, to heterogeneous data from astronomical surveys.
Learning and inference on relational data has been the topic of research in machine learning over
the past decades. The relational model is closely related to first order and predicate logic, where
the existence of a relation between instances becomes a truth statement about a world. The same
formalism is used in AI through a probabilistic approach to logic, where the field of statistical
relational learning has fused the relational model with the framework of probabilistic graphical
models. Examples of such models include plate models, probabilistic relational models, Markov
logic networks, and relational dependency networks (Getoor & Taskar, 2007).
A closely related area that has enjoyed accelerated growth in recent years is relational and geometric
deep learning, where the term “relational” is used to denote the inductive bias introduced by a
graph structure. Although relational and graph-based terms are often used interchangeably in the
machine learning community, they could refer to different data structures: graph-based models
such as graph databases (Robinson et al., 2013) and knowledge graphs simply represent data as an
attributed (hyper-)graph, while the relational model, common in databases, uses the entity-relation
(ER) diagram (Chen, 1976) to constrain the relations of each instance (corresponding to a node),
based on its entity-type; see Fig. 1(a).
Inspired by the success of equivariant deep learning, we use invariance and equivariance to encode
the structure of relational data. This type of inductive bias informs a model’s behaviour under
various transformations. Equivariant models have been successfully used for deep learning on data
with a variety of structures, from translation equivariant images (LeCun et al., 1998), to geometric
settings (Cohen et al., 2018a; 2019), to discrete objects such as sets (Zaheer et al., 2017), and
graphs (Kondor et al., 2018b). By adopting this perspective, we present a maximally expressive feed-
forward layer that achieves equivariance w.r.t. exchangeabilities in relational data as it is expressed
by the entity-relationship model. Our feedforward layer generalizes recently proposed layers for
sets (Zaheer et al., 2017), exchangeable tensors (Hartford et al., 2018), and graphs (Maron et al.,
2018).
1An alternative terminology refers to instances and entities as entities and entity types.
1
Under review as a ConferenCe PaPer at ICLR 2020
(a) (b) (C)
FigUre h (a) The Entity—RelationShiP (ER) diagram for OUrrUmling example Thereare three enti—
tieK STUDENT3 COURSE and PROFESSOR (IabeIed L2 and 3 respective-y)“ and four pairwise relations-
TAKES(STUDENT—COURSE。represented by 再一 H (L 2))； the SeIf—self relation PREREQUISITE (CoURSE—
CoURSE3 represented by 为2 HJs2))； WRITES REFERENCE(STUDENT—PROFESSOR。represented by
汨3 U?3))； and TEACHES(PROFESSoR—COURSEa represented by 汨4 UJs3))∙ The full Set Of relationS
isfHUΛl j3(2 丁(3- (L 2 - (L 3 丁(23 3 丁(23 2))∙ BOth entities and relationS have associated attributes ——
e∙g∙3 when a STUDENTtakeS a CoURSE。they receive a GRADE∙ The SingIeton relation 汨 Uu) Can encode
STUDENT attributed) SUCh as IlUmber Of INTERNSHIPs。Or SALARY afsr graduating。SinCe each CoURSE
has a SingIe PRoFESSoR as its teacheΓithis relation is One ltolmany∙ (b) SomePoSSibIe relato∙IIaltabIeSill an
instantiation Ofthe ER diagram Of (a)∙ There are NIU 5 instances Of STUDENT“ 22 U 4 instances Of CoURSE
and Λ⅞ U 3 instances Of PRoFESSOR∙ The attributes associated With each entity and relation are Stored in the
CorreSPonding table ——eg: table XF2) SUggeStS that the STUDENT 5 t∞k COURSE 4 and received a GRADE Of
93∙ (C) SParSe tensor representation X为一 - : J X为7 Ofthe tables Of (b)∙ The VeCtoriZed form OfthiS Set Of SParSe
tensor∞VeCx) H〈vec (XE):：一 VeC(X02)))——一he Co - Umn—vector ConCatenation Of VeC(X声)3S——is the
input to OUr feed—forwaaIayer Q(W VeC (次))∙ Herrthe Parameter—tying in the Weight matriX W Ofthe EERL
(Fig∙ 2) guarantees that any PermUtation Of elements Of VeCx) CorreSPonding to a ShUfHingof entitiesin this
tensor representation (and Only these PermUtationS)“ results in the Same PermUtation Ofthe OUtPUt Ofthe Iayer∙
2 REPRESENTATIoNS OF RELATloNAL DATA
We represent the relational model by a Set Of en 三 iesβ=?:." Dy and a Set Of repiioTlSsl∩2θ
indicating how the entries imeract Wi-h One another FOr each ens∙y A ∈βwe have a se二L :" Ndy
OfinSianCeS Ofthat entity∙ FOr each relation 否 ∈9% Where 弟=(AI: •二 A-国一〉is a set Of entities"
We ObSerVe data in the form Of a Set Of tuplesXR = (=d二：二 TZd 史 一^di m -NdJa m RKy∙
ThatiS〉for each element Of 次汨 We associa-e the fea-ure VeC-Or H WitJl the relationship be-ween the
instances indeXed by TZd、for each entity AQ ∈ 弟∙ See Fig∙ 1 for a detailed" ConCrete example∙ Note
一ha-th。SmgIeiCm repiion 弟=(A3 Can b。USed to incorpora 一。individual ens∙y attributes (SUCh as
PrOfeSSOrSa evalua HOnSs∙Fig∙ l(a))∙
Inthe most general Cas9 we allow for boths“ and any 否 ∈sto be multisets (j∙e∙" to Contain
dupHcas entries，sis a mul HSetifWe have multiple relations be-ween the Same Set Of emiues∙ For
examppWe may have a SUPERVlSES relation be-ween STUDENTS and PRoFESSoR∞s∙addition to the
WRlTES REFERENCE rʤ A PaiIiCUIar relation 弟 is a multiset if it contains multiple COPieS Of the
Same ens∙y∙ SUCh rʤs ar。UbiqUitOUSin th。real world” describing for example” th。COnneCHOn
graph Of a SOCiaI n。-Work〉th。SaI。、PUrehaSe r。IaHonShiPS be_ Ween a group Of COmPanie∞β in OUr
nms.ng examppthe COURSE—COURSE relation CaPtUring PrereqUiSite infOrmatiop FOr OUr derivations
We make the SimPHfy5∙g assumption that each aiWibuieH∈ R is a SCaIar∙ Extension to x ∈ RK
USing multiple ChannelSiS trivial and discussed in APPendiX F∙ Another COmmon feature Of relational
da-a 二he one—to—many relation is addresseds∙APPendiX E.
2∙ 1 TUPLEs“ Tables AND TENSoRS
In relational databases the Set Of tuplesX再 is OfSn represensd USing a tabpWith One row for each
tuple; See Fig∙ 1(b). An equivalent represen-a-o∙n forX再一S USing a CeSParS%更—dimensional tensor
XRmRNdI * : XNdS一 “ Where each dimension Of this tensor ColTeSPondS to an en⅛y A ∈ 否“ and the
the length Of that dimension is the number OfinStanCeS Nd∙ In other WOrdS
〈Tzd 二…Wd 更a)∈XR 。 X/二∙∙∙3d 更=2∙
We WOrk With this tensor representation Of relational data∙ We USeX=(XR 一否 ∈syto denote
the Set Of a∏ SParSe tensors that define the relational data(base)； See Fig∙ 1(C) ∙ FOrthe foπow5∙g
Under review as a conference paper at ICLR 2020
discussions around exchangeability and equivariance, we assume that for all R, XR are fully observed,
dense tensors. Subsequently, we will discard this assumption and attempt to make predictions for
(any subset of) the missing records.
Note that relations R can be multisets. For simplicity, we handle this in the main text by considering
equal elements as distinct through indexing (e.g., d(i) = d(j)), while leaving a formal definition of
multisets for the supplementary material; see Appendix B.
3	Exchangeabilities of Entity-Relationship Data
Recall that in the representation X, each entity d ∈ {1, ..., D} has a set of instances indexed by
nd ∈ {1, .., Nd}. The ordering of {1, ..., Nd} is arbitrary, and we can shuffle these instances, affecting
only the representation, and not the “content” of the relational data. However, in order to maintain
consistency across data tables, we also have to shuffle all the tensors XR, where d ∈ R, using the
same permutation applied to the tensor dimension corresponding to d. At a high level, this simple
indifference to shuffling defines the exchangeabilities of relational data. A mathematical group
formalizes this idea.
A mathematical group is a set equipped with a binary operation between its members, such that the
set and the operation satisfy closure, associativity, invertability and existence of a unique identity
element. SM refers to the symmetric group, the group of all permutations of M objects. A natural
representation for a member of this group gM ∈ SM, is a permutation matrix G ∈ {0, 1}M×M.
Here, the binary group operation is the same as the product of permutation matrices. In this notation,
SNd is the group of all permutations of instances 1, . . . Nd of entity d ∈ {1, .., D}. To consider
permutations to multiple dimensions of a data tensor we can use the direct product of groups. Given
two groups G and H, the direct product G × H is defined by
(g,h) ∈ G X H ⇔ g ∈ G,h ∈ H and (g,h) ◦ (g0, h0) = (g ◦ g0,h ◦ h0).	(1)
That is, the underlying set is the Cartesian product of the underlying sets of G and H, and the group
operation is the component-wise operation.
Observe that we can associate the group SN1 × . . . × SND with the D entities in a relational model,
where each entity d has Nd instances. Intuitively, applying permutations from this group to the
corresponding relational data should not affect the underlying contents, while applying permutations
from outside this group should. To see this, consider the tensor representation of Fig. 1(c): permuting
students, courses or professors shuffles rows or columns of X, but preserves its underlying content.
However, arbitrary shuffling of the elements of this tensor could alter its content.
Our goal is to define a feed-forward layer that is “aware” of this structure. For this, we first need to
formalize the action of SN1 × . . . × SND on the vectorized form of X.
Vectorization. For each tensor XR ∈ X, NR = Qd∈R Nd refers to the total number of elements
of tensor XR (note that for now we are assuming that the tensors are dense). We will refer to
N = PR∈R NR as the number of elements of X. Then vec(XR) ∈ RNR refers to the vectorization
of XR, obtained by successively stacking its elements along its dimensions, where the order of
dimensions is given by d ∈ R. We use vec-1(∙) to refer to the inverse operation of vec(∙), so
that vec-1(vec(X)) = X. With a slight abuse of notation, we use vec(X) ∈ RN to refer to
[vec(XR1); . . . ; vec(XR|R| )], the vectorized form of the entire relational data. The weight matrix W
that we define later creates a feed-forward layer σ(W vec(X)) applied to this vector.
Group Action. The action of g ∈ SN1 × . . . × SND on vec(X) ∈ RN, permutes the elements of
vec(X). Our objective is to define this group action by mapping SN1 × . . . × SND to a group of
permutations of all N = Pr∈r Qd∈R Nd entries of the database 一 i.e., a homeomorphism into SN.
To this end we need to use two types of matrix product. Let G ∈ RN1×N2 and H ∈ RN3×N4 be
two matrices. The direct sum G ㊉ H is an (N + N3) × (N + N4) block-diagonal matrix, and the
3
Under review as a conference paper at ICLR 2020
Kroneckerproduct G 0 H is an (N1N3) X (N2N4) matrix:
,	、	/ Gι,ιH	...	Gi,n2H ∖
G ㊉ H = (G H) , G 0 H =	.	...	. I .
GN1,1H ... GN1,N2H
Note that in the special case that both G and H are permutation matrices, G ㊉ H and G 0 H will
also be permutation matrices. Both of these matrix operations can represent the direct product of
permutation groups. That is, given two permutation matrices G1 ∈ SN1, and G2 ∈ SN2, we can
use both G1 0 G2 and G1 ㊉ G2 to represent members of SNI × SN2. However, the resulting
permutation matrices, can be interpreted as different actions: while the (N1 + N2 ) × (N1 + N2 )
direct sum matrix G1 ㊉ G2 is a permutation of Ni +N objects, the (N1N2) × (N1N2) Kronecker
product matrix G 1 0 G2 is a permutation of N1 N2 objects.
Claim 1. Consider the vectorized relational data Vec(X) Oflength N. The action of SNI × ... ×
SND on Vec(X) is given by the following permutation group
GX = {MO Gd | (G1,..., GD) ∈ SNI X ... X SND }.	(2)
R∈R d∈R
where the order ofrelations in ㊉ r∈r is consistent with the ordering used for vectorization of X.
Proof. The Kronecker product Nd∈R Gd when applied to Vec(XR), permutes the underlying tensor
XR along the axes d ∈ R. Using direct sum, these permutations are applied to each tensor Vec(XR )
in Vec(X) = [Vec(XR1 ); . . . , Vec(XRD )]. The only constraint, enforced by (2) is to use the same
permutation matrix Gd for all R when d ∈ R. Therefore any matrix-vector product GN Vec(X) is a
“legal” permutation of Vec(X), since it only shuffles the instances of each entity.	□
4	Equivariant Entity-Relationship Layer
Our objective is to constrain the standard feed-forward layer f : RK ×N → RK0 ×N —where
K, K0 are the number of input and output channels, and N = | Vec(X)|— such that any “legal”
transformation of the input, as defined in (2), should result in the same transformation of the output.
Doing so amounts to augmenting our input data by applying all such transformations and using an
unconstrained feed-forward layer. This layer would be quadratic in the size of the database, and the
model would require exponentially more data to train. In the case of even moderately sized databases
this approach is simply infeasible, further motivation the our EERL (Definition 1). For clarity, we
limit the following definition to the case where K = K0 = 1; see Appendix F for the case of multiple
channels.
Definition 1 (Equivariant Entity-Relationship Layer; EERL). Let GX be any N X N permutation
of Vec(X). Afully COnneCted layer σ(W Vec(X)) With W ∈ RN×N is called an Equivariant
Entity-Relationship Layer if
σ(WGX Vec(X)) = GXσ(W Vec(X)) ∀X	⇔	GX ∈ GX.	(3)
That is, an EERL is a layer that commutes with the permutation GX ifand only if GX is a legal
permutation, as defined by (2).
An EERL2 captures the notion that the order of the entities in the relational data X is not important,
and the layer should not produce different outputs when given different orderings. We now propose a
procedure to tie the entries ofW so as to guarantee the conditions of Definition 1. Moreover, we show
that the resulting linear map W : RN ×N is the most expressive linear map under the equivariance
constraints.
2In its most general form, we can also allow for particular kinds of bias parameters in Definition 1; see
Appendix D for parameter-tying in the bias term.
4
Under review as a ConferenCe PaPer at ICLR 2020
⅜j 一 三・M= 一 i⅛ja¾一
~Γ∏~一 -
EZ1gz1■二πn= Ng白
-二口 二二二 Q^^l -二二二二 -
聿生r E "⅛工
「 F--
=≡⅞u H H =-
J 口 0 j j - j
-M一g-g一 E 三一三
(a)
FigUre 2」 (a) TheParametermatriX W from EXamPIe 2∙ EaCh CoIoUrrePreSentS a UniqUeParameterValUe∙
The IlinebloCk∞ShoWing theinter action Ofthreerelation∞are CIearlyViSibIe∙ The arrowsindicate the
PermUtation that is being applied。(b) The result Of applying a PermUtation to the rows Of w∙ The PermUtation
is G U G10G20G3 PermUtingthe instances Of STUDENT。CoURSE and PRoFESSOR∙ Here。Gl ∈ 85
and oω m 83 arm SimPly the respective identity PermUtato∙n∞WhiIe OM ∈ %4 1s the permutation (23 L 3" 4)
(j∙e; SWaP the HrSt and SeCond items)∙ ThiS CorreSPondS to a Celegar PermUtation as defined by (2)∙ We Can See
tha 二 his SWaPPing is appHed bock—wise to blocks Of rows Of W CorreSPonding to each W23 for Whieh 2 ∈F∙
In the CaSe Of 汨2 UJs2 丁 OM must also be app Hed to rows Within block∞as these CorreSPond to entity 2 as
WelL (C) The result Of applying the inverse PermUtation (in this Cas9 the Same PermUtation) to the ColUmnS Of
w∙ Hee We SWaP CoIUmnS block—wise in each W23 for WhiCh 2 ∈ 落・ By doing So We recover the OriginaI
matrix∙ ThiS Symmetry Condition On W is essential in achieving equivariance (See Lemma Iin the proof Of
Theorem 4∙1)∙
We build UP 一 he matriXW ∈ RNXNbock—wispWi-h blocks W2」∈ RN守 XNy COrreSPond5∙g to
each Pair Of rʤs 了 J Ry 一
(WL1 wl.2 …W= Z J
Wl - ∙∙∙ - ∙ (4)
<W^L1 w-a2 …W-NJZJ
We tie parameters Within each block" and not across bocks∙ Ho ConCiSeIy express the SOmeWhat
COmPIeX tying SChem9 We USe the foπows∙g indexing notaro∙n∙
IndeXing Notation・ The Parameter block W21is an 之西 × NemaaX “ where NF= πd∈FNd.
We Wantto index rows and CoIUmnS Of W25∙GiVen the relation 弟Q = (Al: •二 A-F一 「 We USe the
tuple IlNll∙(El 二•二nV一再一)∈ -NdI 一 × : ∙ × -Nd 府_- to index an element in the Set -NFJ ∙ Since
each element OfilS indeXeSinStanCeS Of a PartiCUlar ens∙y IlS Can b。USed as an index both for da-a
block VeC(XRs.) and for the rows Of Parame-er block WZa ∙ In PaiIiCUIaLto deno-e an entTy Of W
We USe W-7 n. MoreoVeL We USe 72》(O denote the element Of 2 CorreSPonding to entity A ∈ 弟*
NoCe that this is not necessariIy the¾h Qlement Of the tuple FOr example" if7=?4" 5)and
IlZ =(400" 12" 3)” -henβH 12 and T⅛ H w∙ When 7 -S P mu≡seL We Can USe /(k) S refer the S
the element OfllS COrreSPond5∙g to the A:—th OCCUrrenCe Of Qmiry d (Where the Order COrreSPondS to
一 he ordering Of elementss∙n2)∙ Table ls∙一 he APPendiX SUmmariZeS OUr nota Hop
4 ∙ 1 PARAMETER TYING
Let Wsy n. ana m. denote two arbitrary elements Of the ParametermatriXW2a ∙ OUr ObjeCtiVe
is to decide Whether Or not they ShOUld be tied together to ensure the resulting IayeriS an EERL
(DeHnitionI)∙ Ho this end We define an equivalence relation between the indices:三Snj)〜
(mJ mj)》and tie together all entries Of W that are equivalent UnderthiS relation. Index〈nJ Ilj)
is equivalent to index〈mJ mɔif they have the Same equality PattemS OVertheirindiCeSfOr each
unique ens∙y d.
We ConSider the ConCatenation Of ns With Ilj〉and examine the sub—tuples nγ" Where n2a is
restricted to Only those indices that≡∙dex entity d. We do this So that We Can ask if these indices refer
Under review as a conference paper at ICLR 2020
to the same instance (e.g., the same student), and we can only meaningfully compare indices of the
same entity (e.g., two entries in the STUDENT table may refer to the same student, but an entry in the
student table cannot refer to the same thing as an entry in the course table. Also, recall that we
allow Ri and Rj to be multisets (e.g., the COURSE-COURSE relation R4 in Fig. 1). We say two tuples
nid,j = hnd(1) , . . . nd(L) i and mid,j = hmd(1) , . . . md(L) i are equivalent iff the have the same equality
pattern nd(l) = nd(l0) ⇔ md(l) = md(l0) ∀l ∈ [L]. Accordingly, two index tuples ni,j and mi,j are
equivalent iff they are equivalent for all d ∈ Ri ∪ Rj .
Because of this tying scheme, the total number of free parameters in Wi,j is the product of the number
of possible different partitionings of nid,j for each unique entity d ∈ Ri ∪ Rj, and so is a product of
Bell numbers3, which count the number of partitions of a set given size; see Appendix C for details.
This relation to Bell numbers was previously shown for equivariant graph networks (Maron et al.,
2018) which, as we see in Section 6, are closely related to, and indeed a special case of, our model.
This parameter-sharing scheme admits a simple recursive form, if the database has no self-relations
(i.e., the Ri are not multisets); see Appendix E.
Example 1. [Fig. 2(a)] To get an intuition for this tying scheme, consider a simplified version of the
relational structure of Fig. 1, restricted to the three relations R1 = {1, 2}, self-relation R2 = {2, 2},
and R3 = {1, 3} with N1 = 5 STUDENTS, N2 = 4 COURSES, and N3 = 3 PROFESSORS. Then
N = 5×4+4×4+5×3 = 51,soW ∈ R51×51 and will have nine blocks: W1,1 ∈ R20×20, W1,2 ∈
R20×16.W2,2 ∈ R16×16 and so on. We use tuple n1 = hn11, n12i ∈ [N1] × [N2] = [5] × [4]
to index the rows and columns of W1,1. We also use n1 to index the rows of W1,2, and use
n2 = hn22(1) , n22(2) i ∈ [N2] × [N2] = [4] × [4] to index its columns. Other blocks are indexed similarly.
Suppose n1 = h1, 4i, n2 = h4, 5i, m1 = h2, 3i and m2 = h3, 2i and we are trying to determine
whether W1,12 2 and W1,21 2 should be tied. Then n1,2 = h1, 4, 4, 5i and m1,2 = h2, 3, 3, 2i.
n ,n	m ,m
When we compare the sub-tuples restricted to unique entities, we see that the equality pattern of
n11,2 = h1i matches that of m11,2 = h2i (since it is only a singleton, it matches trivially), and the
equality pattern of n21,2 = h4, 4, 5i matches that of m21,2 = h3, 3, 2i. So these weights should be tied.
We now establish the optimality of our constrained linear layer for relational data.
Theorem 4.1.	Let X = {XR | R ∈ R} be the tensor representation of some relational data
and Vec(X) its vectorized form. Ifwe define W block-wise according to (4), with blocks given
by the tying scheme above, then the feed-forward layer σ(W Vec(X)) is an Equivariant Entity-
Relationship Layer (Definition 1).
This theorem assures us that a layer constructed with our parameter-sharing scheme achieves equiv-
ariace w.r.t. the exchangeabilities of the relational data. However, one may wonder whether an
alternative, more expressive parameter-sharing scheme (with the same form of output) may be possi-
ble. The following theorem asserts that this is not the case, and that our model is the most expressive
parameter-tying scheme possible for an EERL.
Theorem 4.2.	Let X = {Xr | R ∈ R} be the tensor representation of some relational data. If
W ∈ RN × N is any parameter matrix other than the one defined above, then the feed-forward
layer σ(W Vec(X)) is NOT an Equivariant Entity-Relationship Layer (Definition 1).
In practice, we approach this constrained layer in a much more efficient way: the operations of the
layer can be performed efficiently using pooling and broadcasting over tensors; see Appendix E.
5	Experiments
5.1	Synthetic Data
To continue with our running example we synthesize a toy dataset, restricted to R1 = {1, 2} (STUDENT-
COURSE), R2 = {1, 3} (STUDENT-PROFESSOR), and R3 = {2, 3} (PROFESSOR-COURSE).4 Each matrix
3The k-th Bell number counts the number of ways of partitioning a set of size k.
4The code for our experiments is available at <anonymous> (synthetic data) and <anonymous> (soccer data).
6
Under review as a conference paper at ICLR 2020
(a) Transductive	(b) Inductive
Figure 3: Left (Embeddings). Ground truth versus predicted embedding for COURSE instances in the
transductive (a) and inductive (b) settings. The x-y location of each point encodes the prediction Zcd, while the
size and color of each encodes the ground-truth Zd . In the inductive setting, training and test databases contain
% test time obs.
(c) Transductive
(d) Inductive










completely distinct instances (i.e., completely different courses). Right (Missing record prediction). Average
mean squared error in predicting missing records in student-course as a function of sparsity level of the
whole database X during training (x-axis) and test (y-axis), in the transductive setting (c) and the inductive
setting (d). In (d) the model is tested on a new database with students, courses and professors unseen
during training time. The baseline is predicting the mean value of training observations. At test time, the
observed entries are used to predict the values of the fixed, held-out test set.
X{d1,d2} ∈ RNd1 ×Nd2 in the relational database, X = hX{1,2}, X{1,3}, X{2,3}i, is produced by first
uniformly sampling an h-dimensional embedding for each entity instance Zd ∈ RNd ×h, followed by
matrix product X{d1,d2} := Zd1Zd2T. A sparse subset of these matrices are observed by our model
in the following experiments. Note that while we use a simple matrix product to generate the content
of tables from latent factors, the model is oblivious to this generative process.
We use a factorized auto-encoding architecture consisting of a stack of EERN followed by pooling
that produces code matrices Zd ∈ RNd ×h0 ∀d ∈ {1, 2, 3} for each entity, STUDENT, COURSE
and PROFESSOR. The code is then fed to a decoding EERN to reconstruct the sparse vec(X). For
all experiments, the encoder consists of 7 EERLs, each with 64 hidden units, each using batch
normalization (Ioffe & Szegedy, 2015) and channel-wise dropout. We then apply mean pooling to
produce encodings. We found that batch normalization dramatically sped up the training procedure.
See Appendix H for more details.
Once we finish training the model on a relational dataset, we can apply it to another instantiation —
that is a dataset with completely different students, courses and professors.
Embedding and Missing Record Prediction. We use a small embedding dimension h = h0 = 2
for visualization of course embeddings. Fig. 3(a) and (b) suggest that the learned embeddings
agree with the ground truth. The structure within the data points has been preserved: in (a), points
with a small x-coordinate in the ground-truth embeddings have a small x-coordinate in the learned
embeddings (indicated by the darker color), and similarly for the y-coordinate and for (b). Note
that in the best case, the two embeddings can agree up to a diffeomorphism. Next, we set out to
predict missing records in the student-course table using observed data from the whole database.
For this, the factorized auto-encoding architecture is trained to only minimize the reconstruction
error for “observed” entries in the STUDENT-COURSE tensor. We use N1 = N2 = N3 = 200, with
an encoding size of 10. To study the model’s behaviour, we repeat the experiment while changing
two quantities: 1) the percentage of observed entries (i.e., sparsity of all database tensors) at training
time; and 2) the sparsity of test-time observations, after the model is trained. Fig. 3(c) visualizes the
average prediction error, confirming our expectation that the amount of data seen during both training
and testing can increase the prediction accuracy. More importantly, once the model is trained at a
particular sparsity level, it shows robustness to changes in the sparsity level at test time.
Inductive Setting. Once we finish training the model on a relational dataset, we can apply it
to another instantiation — that is a dataset with completely different students, courses and
PROFESSORS. Note that this is possible because the unique values in W do not grow with the number
of instances of each entity in our database - i.e., We can resize W by repeating its pattern to fit the
dimensionality of our data. In practice, since we use pooling-broadcasting operations, this resizing is
implicit. Fig. 3(b) reproduces the embedding experiment, and Fig. 3(d) shoWs the missing record
7
Under review as a conference paper at ICLR 2020
prediction results for the inductive setting. In both cases the model performs well when applied to a
new database. This setting can have interesting real-world applications, as it enables transfer learning
across databases and allows for predictive analysis without training for new entities in a database as
they become available.
The Value of Side Information. Do we gain anything by using the “entire” database for pre-
dicting missing entries of a particular table, compared to simply using the target tensor X{1,2}
(student-course table) for both training and testing? To an-
swer this question, we fix the sparsity level of the student-
course table at 0.1%, and train models with increasing levels
of sparsity for other tensors X{1,3} and X{2,3} in the range
[0.025, 0.7]. This figure shows that the side information in
the form of student-professor and course-professor ta-
bles can significantly improve the prediction of missing records
in the student-course table.
5.2	Predicting on Soccer Database
We use the European Soccer database to build a simple rela-
tional model with three entities: player, team and match. The database contains information for
about 11,000 players, 300 teams and 25,000 matches in European soccer leagues. We extract a
competes-in relation between teams and matches, as well as a plays-in relation between players
and matches that identifies which players played in each match. Our objective is to predict whether
the outcome of a match was Home Win, Away Win, or Draw.
A simple baseline is to always predict Home Win, which obtains 46% accuracy. By engineering
features from temporal statistics (such as the result of recent games for a team relative to a particular
target match, recent games two teams played against each other, as well as recent goal statistics) the
best model reported on Kaggle achieve 56% accuracy. Without using any temporal data, by simply
taking the average for any such time series, our model achieves 53% accuracy. This also matches the
accuracy of professional bookies. See Appendix H for details.
6	Related Literature
For a more detailed literature review see Appendix A. Our work is related to a large body of work
in statistical relational learning (Getoor & Taskar, 2007; Raedt et al., 2016) and knowledge-graph
(KG) embedding (Nickel et al., 2016), as well as relational (e.g., Battaglia et al., 2018; Hamilton
et al., 2017b), and equivariant (e.g., Cohen & Welling, 2016a; Ravanbakhsh et al., 2017b; Kondor
& Trivedi, 2018; Cohen et al., 2019) deep learning. When dealing with a single relation, such as
student-course or user-item-tag one may use tensor factorization methods for both embedding
and tensor completion. However, with more than one relation, one needs to jointly factorize a set of
tensors XR ∈ X, so that the factors in common between any two tensors are in agreement - e.g., in
Fig. 1, X{1,2} and X{1,3} have a common factor, corresponding to the student entity.
Most relevant to our work in the statistical relational learning community is the relational neural
network of Kazemi & Poole (2017). However, they focus on a single relation and the proposed model
is more directly comparable to Hartford et al. (2018). See Appendix A.1.1 for discussion.
EERL generalizes several equivariant layers proposed for structured domains: Our model reduces
to (equivariant model in) Deep-Sets (Zaheer et al., 2017) when we have a single relation R = {1}
with a single entity - i.e., D = 1; i.e., a set of instances. Hartford et al. (2018) consider a more
general setting of interaction across different sets, such as user-tag-movie relations. EERLs reduce
to their parameter-sharing when we have a single relation R = {R} with multiple entities D ≥ 1,
where all entities appear only once. Maron et al. (2018) further relax the assumption above, and allow
for multiset relations. Intuitively, this form of relational data can model the interactions within and
between sets; for example interaction within nodes of a graph is captured by an adjacency matrix,
corresponding to D = 1 and R = {1, 1}. Our model reduces to this scheme when restricted to a
single relation. For detailed discussion on these relations see Appendix A.
8
Under review as a conference paper at ICLR 2020
Conclusion and Future Work
We have outlined a novel and principled approach to deep learning with relational data(bases). In
particular, we introduced a simple constraint in the form of tied parameters for the standard feed-
forward layer and proved that any other tying scheme either ignores the exchangeabilities of relational
data or can be obtained by further constraining our model. The proposed layer can be applied in
inductive settings, where the relational databases used during training and test have no overlap.
While our model enjoys a linear computational complexity in the size of the database, we have to
overcome one more hurdle before applying this model to large-scale real-world databases: relational
databases often hold large amount of data, and in order for our model to be applied in these settings
we need to perform mini-batch sampling. However, any such sampling has the effect of sparsifying
the observed relations. A careful sampling procedure is required that minimizes this sparsification for
a particular subset of entities or relations. While several recent works propose solutions to similar
problems on graphs and tensors (e.g., Hamilton et al., 2017a; Hartford et al., 2018; Ying et al., 2018;
Eksombatchai et al., 2017; Chen et al., 2018; Huang et al., 2018), we leave this important direction
for relational databases to future work.
References
Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham M Kakade, and Matus Telgarsky. Tensor
decompositions for learning latent variable models. The Journal of Machine Learning Research,
15(1):2773-2832, 2014.
Fabio Anselmi, Georgios Evangelopoulos, Lorenzo Rosasco, and Tomaso Poggio. Symmetry-adapted
representation learning. Pattern Recognition, 86:201-208, 2019.
Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, et al. Interaction networks
for learning about objects, relations and physics. In Advances in neural information processing
systems, pp. 4502-4510, 2016.
Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi,
Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al.
Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261,
2018.
Benjamin Bloem-Reddy and Yee Whye Teh. Probabilistic symmetry and invariant neural networks.
arXiv preprint, arXiv:1901.06082, 2019.
Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. Geometric
deep learning: going beyond euclidean data. IEEE Signal Processing Magazine, 34(4):18-42,
2017.
Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally
connected networks on graphs. ICLR, 2014.
Jie Chen, Tengfei Ma, and Cao Xiao. Fastgcn: Fast learning with graph convolutional networks via
importance sampling. CoRR, abs/1801.10247, 2018.
Peter Pin-Shan Chen. The entity-relationship model—toward a unified view of data. ACM Transac-
tions on Database Systems (TODS), 1(1):9-36, 1976.
Taco S Cohen and Max Welling. Group equivariant convolutional networks. arXiv preprint
arXiv:1602.07576, 2016a.
Taco S Cohen and Max Welling. Steerable cnns. arXiv preprint arXiv:1612.08498, 2016b.
Taco S Cohen, Mario Geiger, Jonas Kohler, and Max Welling. Spherical cnns. arXiv preprint
arXiv:1801.10130, 2018a.
Taco S Cohen, Mario Geiger, and Maurice Weiler. Intertwiners between induced representations
(with applications to the theory of equivariant neural networks). arXiv preprint arXiv:1803.10743,
2018b.
9
Under review as a conference paper at ICLR 2020
Taco S Cohen, Maurice Weiler, Berkay Kicanaoglu, and Max Welling. Gauge equivariant convolu-
tional networks and the icosahedral cnn. arXiv preprint arXiv:1902.04615, 2019.
David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy HirzeL Al如
Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning molecular
fingerprints. In Advances in neural information processing systems, 2015.
Chantat Eksombatchai, Pranav Jindal, Jerry Zitao Liu, Yuchen Liu, Rahul Sharma, Charles Sugnet,
Mark Ulrich, and Jure Leskovec. Pixie: A system for recommending 3+ billion items to 200+
million users in real-time. CoRR, abs/1711.07601, 2017.
Lise Getoor and Ben Taskar. Introduction to statistical relational learning. MIT press, 2007.
Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
message passing for quantum chemistry. arXiv preprint arXiv:1704.01212, 2017.
Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In
Advances in Neural Information Processing Systems ,pp.1024-1034, 2017a.
William L Hamilton, Rex Ying, and Jure Leskovec. Representation learning on graphs: Methods and
applications. arXiv preprint arXiv:1709.05584, 2017b.
Jason Hartford, Devon R Graham, Kevin Leyton-Brown, and Siamak Ravanbakhsh. Deep models of
interactions across sets. In Proceedings of the 35th International Conference on Machine Learning,
pp. 1909-1918, 2018.
Wenbing Huang, Tong Zhang, Yu Rong, and Junzhou Huang. Adaptive sampling towards fast graph
representation learning. In Advances in Neural Information Processing Systems 31, pp. 4559-4568.
2018.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Seyed Mehran Kazemi and David Poole. Relnn: a deep neural model for relational learning. arXiv
preprint arXiv:1712.02831, 2017.
Seyed Mehran Kazemi, David Buchman, Kristian Kersting, Sriraam Natarajan, and David Poole.
Relational logistic regression. In KR. Vienna, 2014.
Steven Kearnes, Kevin McCloskey, Marc Berndl, Vijay Pande, and Patrick Riley. Molecular graph
convolutions: moving beyond fingerprints. Journal of computer-aided molecular design, 30(8):
595-608, 2016.
Kristian Kersting. Lifted probabilistic inference. In ECAI, pp. 33-38, 2012.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks.
arXiv preprint arXiv:1609.02907, 2016.
Risi Kondor and Shubhendu Trivedi. On the generalization of equivariance and convolution in neural
networks to the action of compact groups. arXiv preprint arXiv:1802.03690, 2018.
Risi Kondor, Zhen Lin, and Shubhendu Trivedi. Clebsch-gordan nets: a fully fourier space spherical
convolutional neural network. In Advances in Neural Information Processing Systems, pp. 10137-
10146, 2018a.
Risi Kondor, Hy Truong Son, Horace Pan, Brandon Anderson, and Shubhendu Trivedi. Covariant
compositional networks for learning graphs. arXiv preprint arXiv:1801.02144, 2018b.
Hoang Thanh Lam, Tran Ngoc Minh, Mathieu Sinn, Beat Buesser, and Martin Wistuba. Learning
features for relational data. arXiv preprint arXiv:1801.05372, 2018.
10
Under review as a conference paper at ICLR 2020
Yann LeCun, Leon Bottou, Yoshua Bengio, Patrick Haffner, et al. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324,1998.
Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural
networks. arXiv preprint arXiv:1511.05493, 2015.
Haggai Maron, Heli Ben-Hamu, Nadav Shamir, and Yaron Lipman. Invariant and equivariant graph
networks. arXiv preprint arXiv:1812.09902, 2018.
Marvin Minsky and Seymour A Papert. Perceptrons: An introduction to computational geometry.
MIT press, 2017.
Maximilian Nickel, Kevin Murphy, Volker Tresp, and Evgeniy Gabrilovich. A review of relational
machine learning for knowledge graphs. Proceedings of the IEEE, 104(1):11-33, 2016.
Peter Orbanz and Daniel M Roy. Bayesian models of graphs, arrays and other exchangeable random
structures. IEEE transactions on pattern analysis and machine intelligence, 37(2):437-461, 2015.
Luc De Raedt, Kristian Kersting, Sriraam Natarajan, and David Poole. Statistical relational artificial
intelligence: Logic, probability, and computation. Synthesis Lectures on Artificial Intelligence and
Machine Learning, 10(2):1-189, 2016.
Siamak Ravanbakhsh, Jeff Schneider, and Barnabas Poczos. Deep learning with sets and point clouds.
In International Conference on Learning Representations (ICLR) - workshop track, 2017a.
Siamak Ravanbakhsh, Jeff Schneider, and Barnabas Poczos. Equivariance through parameter-sharing.
In Proceedings of the 34th International Conference on Machine Learning, volume 70 of JMLR:
WCP, August 2017b.
Ian Robinson, Jim Webber, and Emil Eifrem. Graph databases. " O’Reilly Media, Inc.", 2013.
Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. Dynamic routing between capsules. In
Advances in Neural Information Processing Systems, pp. 3856-3866, 2017.
Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The
graph neural network model. IEEE Transactions on Neural Networks, 20(1):61-80, 2009.
Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max
Welling. Modeling relational data with graph convolutional networks. In European Semantic Web
Conference, pp. 593-607. Springer, 2018.
Kristof T Schutt, Farhad Arbabzadah, Stefan Chmiela, Klaus R Muller, and Alexandre Tkatchenko.
Quantum-chemical insights from deep tensor neural networks. Nature communications, 8:13890,
2017.
John Shawe-Taylor. Building symmetries into feedforward networks. In Artificial Neural Networks,
1989., First IEE International Conference on (Conf. Publ. No. 313), pp. 158-162. IET, 1989.
John Shawe-Taylor. Symmetries and discriminability in feedforward network architectures. IEEE
Transactions on Neural Networks, 4(5):816-826, 1993.
Maurice Weiler, Fred A Hamprecht, and Martin Storath. Learning steerable filters for rotation
equivariant cnns. arXiv preprint arXiv:1711.07289, 2017.
Maurice Weiler, Wouter Boomsma, Mario Geiger, Max Welling, and Taco Cohen. 3d steerable cnns:
Learning rotationally equivariant features in volumetric data. In Advances in Neural Information
Processing Systems, pp. 10401-10412, 2018.
Daniel E Worrall, Stephan J Garbin, Daniyar Turmukhambetov, and Gabriel J Brostow. Harmonic
networks: Deep translation and rotation equivariance. In Proc. IEEE Conf. on Computer Vision
and Pattern Recognition (CVPR), volume 2, 2017.
Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and Jure Leskovec.
Graph convolutional neural networks for web-scale recommender systems. arXiv preprint
arXiv:1806.01973, 2018.
Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan R Salakhutdinov, and
Alexander J Smola. Deep sets. In Advances in Neural Information Processing Systems, 2017.
11
Under review as a conference paper at ICLR 2020
A A More Detailed Review of Related Literature
To our knowledge there are no similar frameworks for direct application of deep models to relational
databases, and current practice is to automate feature-engineering for specific prediction tasks (Lam
et al., 2018).
A. 1 Statistical Relational Learning and Knowledge-Graph Embedding
Statistical relational learning extends the reach of probabilistic inference to the relational model (Raedt
et al., 2016). In particular, a variety of work in lifted inference procedures extends inference methods
in graphical models to the relational setting, where in some cases the symmetry group of the model is
used to speed up inference (Kersting, 2012). Most relevant to our work from this community is the
Relational Neural Network model of Kazemi & Poole (2017); see Appendix A.1.1.
An alternative to inference with symbolic representations of relational data is to use embeddings.
In particular, Tensor factorization methods that offer tractable inference in latent variable graphical
models (Anandkumar et al., 2014), are extensively used for knowledge-graph embedding (Nickel
et al., 2016). A knowledge-graph can be expressed as an ER diagram with a single relation R =
{1(1), 2, 1(2)}, where 1(1), 1(2) representing HEAD and TAIL entities and 2 is an entity representing
the relation. Alternatively, one could think of knowledge-graph as a graph representation for an
instantiated ER diagram (as opposed to a set of of tables or tensors). However, in knowledge-graphs,
an entity-type is a second class citizen, as it is either another attribute, or it is expressed through
relations to special objects representing different “types”. Therefore, compared to ER diagrams,
knowledge-graphs are less structured, and more suitable for representing a variety of relations between
different objects, where the distinction between entity types is not key.
A.1.1 Relation to RelNN of Kazemi & Poole (2017)
An alternative approach explored in the statistical relational learning community includes extensions
of logistic regression to relational data (Kazemi et al., 2014), and further extensions to multiple
layers (Kazemi & Poole, 2017). The focus of these works is primarily on predicting properties of the
various entity instances (the example they use is predicting a user’s gender based on the ratings given
to movies).
Their model works by essentially counting the number of instances satisfy a given properties, but is
easiest understood by interpreting it as a series of convolution operations using row- and column-wise
filters that capture these properties. Consider Example 3 from (Kazemi & Poole, 2017) (also depicted
in their Figure 4). We have a set of USERS and a set of MOVIES, and there is a matrix L ∈ RN×M,
denoting which movies where liked by which users. As filters, they use binary vectors a ∈ RM, and
o ∈ RN, representing which movies are ACTION and which users are OLD, respectively. The task
they pose is to predict the gender of a user5, given this information. To do so, they include a third
filter, φ ∈ RM, of learnable, “numeric latent properties”. Each layer of their model then convolves
each of these filters with the likes matrix, then applies a simple linear scale and shift and a sigmoid
activation. The result is three new filter vectors that can be used to make predictions or as filters in
the next layer. For one layer, the outputs are then
va = σ w0a + w1aLa , vo = σ w0o + w1ooT L , vφ = σ w0φ + w1φ
where each w is a scalar. Observe that, for example, the nth element of La counts the number of
action movies liked by user n. Observe also that va , vφ ∈ RN, while vo ∈ RM . So va and vφ
can be used to make predictions about individual users. Note that the number of parameters in their
model grows both with the number of movies and with the number of layers in the network.
Application of EERL to this example, reduces the 4 parameter model of (Hartford et al., 2018).
Indeed, most discussions and “all” experiments in Kazemi & Poole (2017) assume a single relation.
For completeness, we explain EERL in this setting. Consider the likes matrix, and the action and
5For simplicity, and to follow the example of (Kazemi & Poole, 2017), we assume binary genders. However,
we note that the real world is somewhat more complicated.
12
Under review as a conference paper at ICLR 2020
OLD filters as tables. We predict the gender of the nth user, as
N	M	MM	M
gn =σ	w0oon	+ w1o	on0	+w0L	Ln,m +	w1L	Ln0,m +	w0a	am	.	(5)
n0=1	m=1	n0=1 m=1	m=1
The main difference between their model and ours is that they require per-item parameters (e.g., one
parameter per movie), while, as can be seen from (5), the number of parameters in our model is
independent of the number of instances and so does not grow with the number of users or movies
(note that we have the option of adding such features to our model by having unique one-hot features
for each user and movie.) As a result, our model can be applied in inductive settings as well. One
may also draw a parallel between row and column convolution in Kazemi & Poole (2017) with two
out of four pooling operations when we have single relation between two entities. However these
operations become insufficient when moving to models of self-relation (e.g., 15 parameters for a
single self-relation) and does cannot adequately capture the interaction between multiple relations as
discussed in our provably optimal linear layer.
A.2 Relational, Geometric and Equivariant Deep Learning
See Hamilton et al. (2017b); Battaglia et al. (2018) for a detailed review. Scarselli et al. (2009)
introduced a generic framework that iteratively updates node embeddings using neural networks. Li
et al. (2015) integrated this iterative process in a recurrent architecture. Gilmer et al. (2017) proposed
a similar iterative procedure that updates node embeddings and messages between the neighbouring
nodes, and show that it subsumes several other deep models for attributed graphs (Duvenaud et al.,
2015; Schutt et al., 2017; Li et al., 2015; Battaglia et al., 2016; Kearnes et al., 2016), including
spectral methods. Their method is further generalized in (Kondor et al., 2018b) as well as (Maron
et al., 2018), which is in turn subsumed in our framework. Spectral methods extend convolution
to graphs (and manifolds) using eigenvectors of the Laplacian as the generalization of the Fourier
basis (Bronstein et al., 2017; Bruna et al., 2014). Simplified variations of this approach leads to an
intuitive yet non-maximal parameter-sharing scheme that is widely used in practice (Kipf & Welling,
2016). This type of simplified graph convolution has also been used for relational reasoning with
knowledge-graphs (Schlichtkrull et al., 2018).
An alternative generalization of convolution is defined for functions over groups (Cohen & Welling,
2016a) or more generally homogeneous spaces (Cohen et al., 2018b). Moreover, convolution can be
performed in the Fourier domain in this setting, where irreducible representations of a group become
the Fourier bases (Kondor & Trivedi, 2018). Particularly relevant to our work are the models of
(Kondor et al., 2018b) and (Maron et al., 2018) that operate on graph data using an equivariant design.
Equivariant deep models for a variety of structured domains is explored in several other recent works.
(e.g., Worrall et al., 2017; Cohen et al., 2018a; Kondor et al., 2018a; Sabour et al., 2017; Weiler et al.,
2018); see also (Cohen & Welling, 2016b; Weiler et al., 2017; Kondor et al., 2018b; Anselmi et al.,
2019).
A.3 Parameter-Sharing, Exchangeability and Equivariance
The notion of invariance is also studied under the term exchangeability in statistics (Orbanz & Roy,
2015); see also (Bloem-Reddy & Whye Teh, 2019) for a probabilistic approach to equivariance. In
graphical models exchangeability is often encoded through plate notation, where parameter-sharing
happens implicitly. In the AI community, this relationship between the parameter sharing and
“invariance” properties of the network was noticed in the early days of the Perceptron (Minsky &
Papert, 2017; Shawe-Taylor, 1989; 1993). This was rediscovered in (Ravanbakhsh et al., 2017b),
where this relation was leveraged for equivariant model design. Ravanbakhsh et al. (2017b) show that
when the group action is discrete “equivariance” to any group action can be obtained by parameter-
sharing. In particular, one of the procedures discussed there ties the elements of W based on the
orbits of the “joint” action of the group on the rows and columns of W. The parameter-tying scheme
in the following can be obtained in this way:
13
Under review as a conference paper at ICLR 2020
A.3.1 Relation to Deep-Sets of Zaheer et al. (2017)
propose an equivariant model for set data. Our model reduces to their parameter-tying when we have
a single relation R = {1} with a single entity - i.e., D = 1; i.e., a set of instances; see also Example 3
in Appendix E. Since we have a single relation, W matrix has a single block W = W1,1, indexed by
n1. The elements of n1,1 = n11,1 = hn1(1) , n1(2) i index the elements of this matrix, for entity 1 (the
only entity). There are two types of equality patterns n1(1) = n1(2) , and n1(1) 6= n1(2), giving rise to
the permutation equivariant layer introduced in (Ravanbakhsh et al., 2017a; Zaheer et al., 2017).
A.3.2 Relationp to Exchangeable Tensor Models of Hartford et al. (2018)
Hartford et al. (2018) consider a more general setting of interaction across different sets, such as
user-tag-movie relations. Our model produces their parameter-sharing when we have a single relation
R = {R} with multiple entities D ≥ 1, where all entities appear only once - i.e., κ(d) = 1∀d ∈ R.
Here, again W = W1,1, and n1,1, the concatenation of row-index n1 and column index n1, identifies
an element of this matrix. Since each d ∈ R has multiplicity 1, n1d,1 = hn1d,1 , n1d,1 i ∀d ∈ [D], and
therefore n1d,1 can have two class of equality patterns. This gives 2D equivalence classes for n1,1,
and therefore 2D unique parameters for a rank D exchangeable tensor.
A.3.3 Relationship to Equivariant Graph Networks of Maron et al. (2018)
Maron et al. (2018) further relax the assumption of Hartford et al. (2018), and allow for κ(d) ≥ 1.
Intuitively, this form of relational data can model the interactions within and between sets; for
example interaction within nodes of a graph is captured by an adjacency matrix, corresponding to
D = 1 and R = {1, 1}. This type of parameter-tying is maximal for graphs, and subsumes the
parameter-tying approaches derived by simplification of Laplacian-based methods. When restricted
to a single relation, our model reduces to the model of (Maron et al., 2018); however, when we have
multiple relations, Wi,j for j 6= i, our model captures the interaction between different relations /
tensors.
x, n, m	tuple or column vector (bold lower-case)
3 ∙i	a tuple
X, G, W	tensor, inc. matrix (bold upper-case)
X, R	set (or multiset)
S, G	group (caligraphic)
D = [D] = {1,...,D}	set of entities
Ni,...,Nd	number of instances
R ⊆ 2D	a set of relations
Ri ⊆ D	a relation
Xi = XRi ∈ RNdI ×...,Nd∣Ri∣	data for a relation Ri
X = {Xi | Ri ∈ R}	relational data
VeC(Xi)	vectorization of Xi
VeC(X)	(vec(X1),…,vec(XlRl)i length of vec(Xi)
Ni = Qd∈Ri Nd	
N = PRi∈R N	length of VeC(X)
W ∈ RN×N	parameter matrix
Wij ∈ RNi ×nj ni =〈nd,,…,nd∣Ri∣i	(i,j)th blockof W index for vec(Xi)
	and for rows of Wij
n = hn1,..., nR|i	index for VeC(X) and rows/columns of W
SM symmetric group M
GX group of N × N “legal”
permutations of VeC(X)
Table 1: Summary of Notation
B Multiset Relations
Because we allow a relation R to contain the same entities multiple times, we formally define a
multiset as a tuple R = hR, κi, where R is a set, and κ : R → N maps elements of R to their multiset
counts. We will call R the elements of the multiset R = hR, κi, and κ(d) the count of element d.
We define the union and intersection of two multisets R1 and R2 as R1 ∪ R2 = hR1 ∪ R2, κ1 + κ2i
and R1 ∩ R2 = hR1 ∩ R2, min{κ1, κ2}i. In general, we may also refer to a multiset using typical
set notation (e.g., R = {d1, d1, d2}). We will use bracketed superscripts to distinguish distinct but
equal members of any multiset (e.g., R = {d1, d1, d2} = {d(11), d(12), d(21)}). The ordering of equal
members is specified by context or arbitrarily. The size of a multiset R accounts for multiplicities:
|R| = Pd∈R κ(d).
14
Under review as a conference paper at ICLR 2020
C	Number of free parameters
For the multiset relations Rei = hRi, κi and Rfj = hRj, κi, recall that two parameters Wni,ij,nj and
Wi,ji j are tied together if ni,j, the concatenation of ni with nj, is in the same equivalence class as
m ,m
mi,j. We partition each nid,j into sub-partitions P(nid,j) =. {P1, . . . , PL} of indices whose values
are equal, and consider ni,j and mi,j to be equivalent if their partitions are the same for all d:
ni,j ≡mi,j ⇔ P(nid,j) = P(mid,j) ∀d ∈ Ri ∪ Rj	(6)
See 4.1 for details. This means that the total number of free parameters in Wi,j is the product of the
number of possible different partitionings for each unique entity d ∈ Ri ∪ Rj :
|wi,j|=	Y b(κi(d)+κj(d))	(7)
d∈Ri∪Rj
where wi,j is the free parameter vector associated with Wi,j, and b(κ) is the κth Bell number, which
counts the possible partitionings of a set of size κ.
Example 2. [Fig. 2(a)] Consider again the simplified version of the relational structure of Fig. 1,
restricted to the three relations R1 = {1, 2}, self-relation R2 = {2, 2}, and R3 = {1, 3} with N1 = 5
STUDENTS, N2 = 4 COURSES, and N3 = 3 PROFESSORS. We use tuple n1 = hn11 , n12i ∈ [N1] × [N2]
to index the rows and columns of W1,1. We also use n1 to index the rows of W1,2, and use
n2 = hn22(1) , n22(2) i ∈ [N2] × [N2] to index its columns. Other blocks are indexed similarly. The
elements of W1,1 take b(2)b(2) = 4 different values, depending on whether or not n11 = n101 and
n21 = n102, for row index n1 and column index n10 (where b(k) is the k-th Bell number). The elements
of W1,2 take b(1)b(3) = 5 different values: the index n11 can only be partitioned in a single way
(b(1) = 1). However index n12 and indices n22(1) and n22(2) all index into the COURSES table, and so
can each potentially refer to the same course. We thus have a unique parameter for each possible
combination of equalities between these three items, giving us a factor of b(3) = 5 different parameter
values; see Fig. 2(a), W1,1 is the upper left block, and W1,2 is the block to its right. The center
block of Fig. 2(a), W2,2 produces the effect of R2 = {2, 2} on itself. Here, all four index values
could refer to the same course, and so there are b(4) = 15 different parameters.
D Bias Parameters
For full generality, our definition of EERL could also include bias terms without affecting its
exchangeability properties. We exclude these in the statements of our main theorems for the sake of
simplicity, but discuss their inclusion here for completeness. For each relation Ri = {d1, ..., d|Ri|},
We define a bias tensor Bi ∈ RNdI × .×NdlRil. The elements of Bi are tied together in a manner
similar to the tying of elements in each Wi,j : Two elements Bini and Bimi are tied together iff
ni ≡ nj, using the definition of equivalence from Section 4.1. Thus, We have a vector of additional
free parameters bi for each relation Ri , Where
|bi| = Y b(κi(d)).	(8)
d∈Ri
Consistent With our previous notation, We define B = {Bi | Ri ∈ R}, and vec(B) =
hvec(B1), ..., vec(B|R|)i. Then an EERL With bias terms is given by
Vec(Y) = σ(W Vec(X) + Vec(B)).	(9)
The folloWing Claim asserts that We can add this bias term Without affecting the desired properties of
the EERL.
Claim 2. If σ(W Vec(X)) is an EERL, then σ(W Vec(X) + Vec(B)) is an EERL.
The proof (found in Appendix G.1) argues that, since σ W Vec(X) is an EERL, We just need to
shoW that GX Vec(B) = Vec(B) iff GX ∈ GX, Which holds due to the tying of patterns in each Bi.
15
Under review as a conference paper at ICLR 2020
E S implifications for Models without Self-Relations
In the special case that the multi relations Ri and Rj are sets —i.e., have no self-relations— then
the parameter tying scheme of Section 4.1 can be simplified considerably. In this section we address
some nice properties of this special setting.
E.1 Efficient Implementation Using Subset-Pooling
Due to the particular structure of W when all relations contain only unique entities, the operation
W vec(X) in the EERL can be implemented using (sum/mean) pooling operations over the tensors
XR for R ∈ R, without any need for vectorization, or for storing W directly.
For Xi ∈ RNd1 × .×NdlRl and S ⊆ %, let Pool(Xi, S) be the summation of the tensor Xi
Nd0 ×...×Nd0
over the dimensions specified by S. That is, Pool(Xi, S) ∈ R 1	|Ri|-|S| where Ri\S =
{d01, . . . , d0|R|-|S|}. Then we can write element ni in the i-th block of W vec(X) as
(W vec(X))(i,ni) = X X wSi,jPool(Xj,S)niR \S	(10)
Rj∈R S⊆Ri∩Rj	i
where niR \S is the restriction of ni to only elements indexing entities in Ri \S. This formulation
lends itself to a practical, efficient implementation where we simply compute each Pool(Xi, S) term
and broadcast-add them back into a tensor of appropriate dimensions.
E.2 One-to-One and One-to-Many Relations
In the special case of a one-to-one or one-to-many relations (e.g., in Fig. 1, one PROFESSOR may
teach many courses, but each course has only one professor), we may further reduce the number
of parameters due to redundancies. Suppose Ri ∈ R is some relation, and entity d ∈ Ri is in a
one-to- relation with the remaining entities of Ri. Consider the 1D sub-array of XRi obtained by
varying the value of nid while holding the remaining values niR \{d} fixed. This sub-array contains
just a single non-zero entry. According to the tying scheme described in Section 4.1, the parameter
block Wi,i will contain unique parameter values wRi and wRi\{d} . Intuitively however, these
two parameters capture exactly the same information, since the sub-array obtained by fixing the
values of Ri\{d} contains exactly the same data as the sub-array obtained by fixing the values of
Ri (i.e., the same single value). More concretely, to use the notation of Appendix E.1, we have
Pool(Xi, Ri\{d})ni = Pool(Xi, Ri) in (10), and so we may tie wRi,i and wRi,i\{d}.
In fact, we can reduce the number of free parameters in the case of self-relations (i.e., relations with
non-unique entities) as well in a similar manner.
E.3 Recursive Definition of the Weight Matrix
We are able to describe the form of the parameter matrix Wi,j concisely in a recursive fashion, using
Kronecker products: For any N ∈ N, let 1N ∈ RN×N be the N × N matrix of all ones, and IN
the N × N identity matrix. Given any set of (unique) entities S = {d1, ..., d|S|} ⊆ {1, .., D}, for
k = 1, ..., |S|, recursively define the sets
WS = {w乳 1Ndk + V乳INdk | W, V ∈ WS-ι},	(11)
with the base case of WS = R. Then for each block Wij of (4) We simply have Wij ∈ W∣R∩Rjj ∣.
Writing the blocks of the matrix (4) in this way makes it clear why block Wi,j contains 2|Ri ∩Rj |
unique parameter values in the case of distinct entities: at each level of the recursive definition we are
doubling the total number of parameters by including terms from two elements from the level below.
It also makes it clear that the parameter matrix for a rank-k tensor is built from copies of parameter
matrices for rank-(k - 1) tensors.
16
Under review as a conference paper at ICLR 2020
Example 3. In the simple case where we have just one relation and one entity, R = {d}
and the parameter matrix is an element of WR = {w 0 1n + V 0 INd | w,v ∈ R}, Which
matches the parameter tying scheme of Zaheer et al. (2017). If instead we have a single re-
lation with two distinct entities R = {d1, d2}, then the parameter matrix is an element of
W2R = {W 0 1Nd + V 0 INd | W, V ∈ W1R}, which matches the tying scheme of Hartford et al.
(2018).	2	2
F Using Multiple Channels
Equivariance is maintained by composition of equivariant functions. This allows us to stack EERLs
to build “deep” models that operate on relational databases. Using multiple input (K) and output (K0)
channels is also possible by replacing the parameter matrix W ∈ RN ×N, with the parameter tensor
W ∈ RK×N×N×K0 ; while K × K0 copies have the same parameter-tying pattern —i.e., there is
no parameter-sharing “across” channels. The single-channel matrix-vector product in σ(W vec(X))
where (W vec(X))n = Pn0 Wn,n0 vec(X)n0 is now replaced with contraction of two tensors
(W vec(X))n,k0 = Pn0,k∈[K] Wk,n,n0,k0 vec(X)n0,k, for k0 ∈ [K0].
G Proofs
Observe that for any index tuple n, we can express P(nd) (Section 4.1) as
P(nd) =	d(k),	d(l)	∀k,	l |	nd(k)	= nd(l)	d(k)	=	d(l)	= d .
(12)
We will make use of this formulation in the proofs below.
G.1 Proof of Claim 2
Proof. We want to show that
GXσ(W Vec(X) + Vec(B)) = σ(WGX Vec(X) + Vec(B))	(13)
iff GX ∈ GX. Since σ(W Vec(X)) is an EERL, this is equivalent to showing
GX Vec(B) = Vec(B) ^⇒ GX ∈ GX.	(14)
(^=) Suppose GX ∈ GX, with GX defined as in Claim 1. Fix some relation Ri and consider the i-th
block of GX Vec(B):
(GX Vec(B))(i,ni) = (Ki Vec(Bi%i	(15)
= X Kini,ni0 Vec(Bi)ni0	(16)
ni0
= Vec(Bi)ni*,	(17)
Where GX = diag(K1,..., K|R|), and ni* is the unique index such that Kni 峭* = 1. As above, let
n be the restriction of ni to elements indexing entity d. Then we want to show that P (nd) = P(n1)
for all d ∈ Ri. Now, since GX ∈ GX and Kni 旷=1 we have
Gd i	i*	= 1
nd(k) ,nd(k)
(18)
for all d ∈ Ri and all k. That is gd (nid*fc)) = ndfc). Consider S ∈ P(ndd). we have
S = {d(k), d(l)∀k, l	| nid(k)	= nid(l)}	(19)
={d(k),d(l)∀k, l	| gd	(nd(k)) = gd	(nd。))}	QO)
= {d(k), d(l)∀k, l	| nid(k)	= nid(l) }.	(21)
17
Under review as a conference paper at ICLR 2020
So P(nd) ⊆ P(nd*). A similar argument has P(nd) ⊇ P(nd*).	Thus, We have
Vec(Bi)ni* = Vec(Bi)], which completes the first direction.
(=⇒) Let GX vec(B) = vec(B). First, suppose for the sake of contradiction that GX 6∈ LR∈R SNR
and consider dividing the rows and columns of GX into blocks that correspond to each relation Ri .
Then since GX 6∈ LR∈R SNR, there exist Ri, Rj ∈ R, with i 6= j and ni ∈ [Ndi] × ... × [Ndi ]
and nj ∈
[Ndj1] × ... × [Nd|jRj|
] such that GX maps (i, ni) to (j, nj). That is gX (i, ni)
(j, nj )
and thus G(Xj,nj),(i,ni) = 1. So
(GX Vec(B)) (j,nj ) = E GXj Gj,nj),(k,nk) Vec(B)(k,nk )	(22)
= Vec(B)(i,ni)	(23)
= Vec(Bi)ni	(24)
6= Vec(Bj)nj ,	(25)
by the definition of B. And so GX Vec(B) 6= Vec(B).
Next, suppose GX ∈ LR∈R SNR .	Then for all Ri, there exist Ki ∈ SNi such that
GX = diag(K1, ..., K|Ri|). For any ni we have
Vec(Bi)ni = (Ki Vec(Bi))ni	(26)
= X Kini,ni0 Vec(Bi)ni0	(27)
ni0
= Vec(Bi)ni*	(28)
Where ni* is the unique index such that Kini ni* = 1. That is ki maps ni* to ni. Then by the
definition of Bi we have
P(nid*) = P(nid)
=P((ki(ni* ))d)	(29)
for all d ∈ Ri . (29) says that for each d the action of Ki on elements of ni* is determined only
by the values of those elements, not by the values of elements indexing other entities, and so
Ki ∈ Nd∈R SNd. But (29) also says that for all k, l
nd. = nd(i) ^⇒ (ki(ni ))d(k) = (ki(ni Ad©，	(30)
which says that the action of Ki is the same across any duplications of d (i.e., d(k) and d(l)), and so
Ki = Nd∈R Gd, for some fixed Gd, and therefore GX ∈ GX.
□
G.2 Lemma 1 and Proof
To prove our main results about the optimality of EERLs we require the following Lemma.
Lemma 1. For any permutation matrices Ki ∈ S Ni and Kj ∈ S Nj we have
KiWi,j = Wi,jKj	⇔	Ki	=	Gd	and	Kj	=	Gd	Gd	∈ SNd
d∈Rei
d∈Rej
for constrained Wi,j as define above. That is Ki and Kj should separately permute the instances of
each entity in the multisets Ri and Rj, applying the same permutation to any duplicated entities, as
well as to any entities common to both Re i and Rej .
18
Under review as a conference paper at ICLR 2020
To get an intuition for this lemma, consider the special case of i = j . In this case, the claim is that
Wi,i
commutes with any permutation matrix that is of the form Ki = Nd∈R Gd. This gives us the
kind of commutativity we desire for an EERL, at least for the diagonal blocks of W. Equivalently,
commuting with Ki means that applying permutation Ki to the rows of Wi,i has the same effect
as applying Ki to the columns of Wi,i . In the case of i 6= j, ensuring that Ki and Kj are defined
over the same underlying set of permutations, {Gd ∈ SNd | d ∈ Ri ∪ Rj }, ensures that permuting
the rows of Wi,j with Ki has the same effect as permuting the columns of Wi,j with Kj. It is this
property that will allow us to show that a network layer defined using such a parameter tying scheme
is an EERL. See Fig. 2 for a minimal example, demonstrating this lemma.
We require the following technical Lemma for the proof of Lemma 1.
Lemma 2. Let Ri, Rj ∈ R, and for each d ∈ [D] let Gd ∈ SNd. If Gdi i0	= 1 for all
nd(k),n d(k)
d
nj0d(k) ,njd(k)
d(k) ∈	Ri	and	d(i)∈	Rj with d(k)= d(i)= d,	we have nid(k)	= jl)	^⇒ n/k)	=	njd(l).
d(k) ∈ Ri with d(k) = d, and G
1 for all d(k) ∈ Rj with d(k) = d, then for all
Proof. Suppose Gdi	i0	= 1 for all	d(k)	∈	Ri,	and Gd	0 j = 1 for all	d(k)	∈	Rj .	We
nd(k),nd(k)	njd(k),nd(k)
prove the forward direction (=⇒). The backward direction follows from an identical argument. Fix
0
some d(k) ∈ Ri and d(l) ∈ Rj and suppose nid
and so
njd . By assumption we have Gdi	i0	= 1
(l)	nd(k) ,n d(k)
(31)
Similarly, we have Gdj 0 j = 1 and so
n d(l),nd(l)
gd(njd(l)) =nj0d(l).
But ni0d	= njd and substituting into (31) we have
(32)
gd(njd(l)) = nid(k).
0
And combining (32) and (33) gives nid	= nj d , as desired.
(33)
We are now equipped to prove our main claims, starting with Lemma 1:
Proof. (^=) Let Ri = {dj,…,df} and f = {d1,…,df} and fix some {Gd ∈ SNdI d ∈
Ri ∪ Rj }. We index the rows of Wi,j , and the rows and columns of Ki, with tuples ni, ni0 ∈
[Ndi ] × ... × [Ndi ]. Similarly, the columns of Wi,j , and rows and columns of Kj , are indexed
with tuples nj, nj0 ∈ [Ndj] × ... × [Ndj ]. Since Ki = Nd∈R Gd we have
Kini ,ni0 =	Gdnid,ni0d
d∈Rei
κ(d)
dY∈RikY=1Gdnid(k),ni0d(k)
□
And thus,
Kni,ni0 =1 o Gnd
(k	) ,ni0d(k)
1,	∀d(k) ∈ Ri s.t. d(k) = d.
(34)
The same is true for Rj . That is
Knj 0,nj = 1 0 Gnjd(k),nd(k)
F VJ 7	L ∏-r7^ .. ɪ 1	1
1, ∀d(k) ∈ Rj s.t. d(k) = d.
(35)
19
Under review as a conference paper at ICLR 2020
Now, fix some ni and nj . Since Ki is a permutation matrix, and so has only one 1 per row, we have
(KiWi,j)ni,nj = X Kni,ni0 WnjOnj
ni0
=Wi,j* 八
ni* ,nj ,
where ni* is the (unique) element of [Ndi ] × ... × [Nd, J which satisfies Gni	/*
d(k) ∈ Rei with d(k) = d. Similarly,
(Wi,jKj)ni,nj = X Wnj,nJ KnjOnj
= Wi,ij *
ni,nj*
where nj* is the (unique) element of [Ndj ] × ... × [Ndj ] which satisfies Gnj*	n
j	(k)	(k)
d(k) ∈ Rj with d(k) = d.
(36)
1 for all
(37)
1 for all
We want to show that P(nid*,j) = P(nid,j* ) for all d ∈ Ri ∪ Rj. Fix d ∈ Ri ∪ Rj and let
i* j
S ∈ P(nid ,j). Then S = {d(1), ..., d(K)}, where d(k) = d for all d(k) ∈ S, and for all d(k), d(l) ∈ S,
i* j	i* j	ij*	ij*	ij*
nid ,j = nid ,j . Then by Lemma 2 we have nid,j = nid,j , and so S ∈ P(nid,j ). So we have
P(nid*,j) ⊆ P(nid,j* ), and the other containment follows identically by symmetry. So ni*,j ≡ ni,j*
by our definition in Section 4.1, and so Wni,ij*,nj = Win,ij,nj* and by (36) and (37) above,
KiWi,j = Wi,jKj .	,	,
(=⇒) Suppose KiWi,j = Wi,j Kj . Fix some ni , nj . Let ni* be the unique index such that
Kini,ni* = 1, and nj the unique index such that Kjnj* ,nj = 1. Then
Win,ij*,nj =XKini,niOWni,ijO,nj
niO
=(KiWi")ni,nj
=(Wi,j Kj )ni,nj
= X Wni,ij,njOKjnjO,nj
= Wni,ij,nj*,
and so P(nid*,j) = P(nid,j* ) for all d ∈ Ri ∪ Rj. But this implies that
*
P(nid ) = P(nid)
=P((ki(ni* ))d).
(38)
(39)
(39) says that for each d the action of Ki on elements of ni* is determined only by the values of
those elements, not by the values of elements indexing other entities, and so Ki ∈ Nd∈R SNd. But
(39) also means that for all k, l
nd*(k) = nd*(i)	o (ki(ni*))d(k)= (ki(ni*))d(0，
(40)
which says that the action of Ki is the same across any duplications of d (i.e., d(k) and d(l)), and so
Ki = Nd∈R Gd, for some fixed Gd. Similarly,
*
P(njd) = P(njd )	(41)
=P((kj(nj))d),	(42)
which shows that Kj = Nd∈R GNd0. Finally, since P(nid*,j) = P(nid,j* ), we also have
nd® = % = nd(k) = nd]，	(43)
20
Under review as a conference paper at ICLR 2020
for all k, l, which means
nd(k) = nd© o (ki(ni*))d(k) = (kj(nj))d(i).	(44)
(44) says that Ki and Kj apply the same permutations to all duplications of any entities they have in
common, and so Gd = Gd , which completes the proof.	□
G.3 Proof of Theorem 4.1
Proof. Let GX ∈ SN and GX be defined as in Claim 1. We need to show that
GXσ(W vec(X)) = σ(WGX vec(X)) iff GX ∈ GX for any assignment of values to the
tables X. We prove each direction in turn.
(=⇒) We prove the contrapositive. Suppose GX 6∈ GX. We first show that GXW 6= WGX and then
that GXσ (W vec(X)) 6= σ(WGX vec(X)) for an appropriate choice of X. There are three cases.
First, suppose GX 6∈ LR∈R SNR and consider dividing the rows and columns of GX into blocks
that correspond to the blocks of W. Then since GX 6∈ LR∈R SNR, there exist Ri, Rj ∈ R, with
i 6= j and ni ∈ [Ndi ] × ... × [Ndi ] and nj ∈ [Ndj] × ... × [Ndj ] such that GX maps (i, ni) to
(j, nj). That is gx ((i, ni)) = (j, nj) and thus, Gj,n Nini) = 1. And so
(GXW)
(j,nj ),(i,ni) = XX
G(j,nj ),(k,nk)W(k,nk),(i,ni)
= Wi,i
ni,ni,
by the definition of W. Similarly,
(WGX )(j,nj ),(i,ni) = XX W(j,nj ),(k,nk) GXk,nk),(i,ni)
= Wj,j
nj ,nj
But Win,ii,ni 6= Wnj,jj,nj since i 6=j. AndsoGXW 6=WGX.
Next, suppose GX ∈ LR∈R SNR, but GX 6∈ LR∈R Nd∈R SNd and consider the diagonal
blocks of GXWGXT that correspond to those ofW. If GX ∈ LR∈R SNR then it is block diagonal
with blocks corresponding to each R ∈ R. But since GX 6∈ LR∈R Nd∈R SNd , there exists some
Ri ∈ R such that the ith diagonal block of GX is not of the form Nd∈R Gd for any Gd. Then by
Lemma 1 we will have inequality between GXWGXT and W in the ith diagonal block.
Finally, suppose GX ∈ Lr∈r Nd∈R SNd. Then GX = K1 ㊉…㊉ K|R|, where Ki ∈ Nd∈R SNd
for all i. Since GX ∈ GX, there exist Ri, Rj ∈ R, possibly with i = j, and a d* ∈ Ri ∩ RjSUCh that
Ki = Gd1 氧...乳 Gd*k)乳...乳 GdiRiI,
and
Kj = Gdj ③...③ Gd*i)③...③ GdjRj1,
but Gd(*k) 6= Gd(*l) . Since Gd(*k) 6= Gd(*l) there exists n ∈ [Nd*] with gd(*l) (n) 6= gd(*l) (n). Pick some
ni and nj with nd* = nd* = n. Let ni* be the result of applying Ki to ni and nj * the result of
applying Kj to nj . Then we have
(GXW)
(i,ni*),(j,nj) = X X G(Xi,ni*),(k,nk)W(k,nk),(j,nj)
= X Kini*,nkWni,kj,nj
nk
= Win,ij,nj ,	(45)
21
Under review as a conference paper at ICLR 2020
and
(WGX) (i,ni*),(j,nj ) =Σ ΣW(i,ni* ),(/)^9),(8)
=X wZnk Kjnknj
nk
= Wi,j
ni*,nj* .
j	*	*
Now, by construction we have nid* = njd* , but nid* 6= nj d
(46)
ij	i* j*
* . So P(nid,*j ) 6= P(nid*,j ) and therefore
Wni,ij,nj 6= Wni,ij*,nj*. And so by (45) and (46) we have GXW 6=WGX.
And so in all three cases GXW 6= WGX . Thus, there exists some X, for which we have
GXW vec(X) 6= WGX vec(X). Since σ is strictly monotonic, we have σ(GXW vec(X)) 6=
σ(WGX vec(X)). And since σ is element wise we have GX σ(W vec(X)) 6= σ(WGX vec(X)),
which proves the first direction.
(^=) Suppose	GX	∈	GX.	That is, for all d ∈	[D], let	Gd	∈	SNd	be some fixed PermUta-
tion of Nd objects and let GX = LR∈R Nd∈R Gd. Observe that GX is block-diagonal. Each block
on the diagonal corresponds to an R ∈ R and is a Kronecker product over the matrices Gd for each
d ∈ R. Let Ki = Nd∈R Gd for each i ∈ [|R|]. That is,
GX
K1
K2
0 ]
0
K|R|
And so the i, j-th block of GXWGXT is given by:
GXWGXT ,j
T
KiWi,jKjT
Wi,j.
(47)
The equality at (47) follows from Lemma 1. Thus, we have GXW = WGX , and so for all X,
GXW vec(X) = WGX vec(X). Finally, since σ is applied element-wise, we have
σ(WGX vec(X)) = σ(GXW vec(X))
= GXσ(W vec(X))
Which proves the second direction. And so σ(W vec(X)) is an exchangeable relation layer, complet-
ing the proof.	口
G.4 Proof of Theorem 4.2
The idea is that if W is not of the form (4) then it has some block Wi,j containing two elements
whose indices have the same equality pattern, but whose values are different. Based on these indices,
we can explicitly construct a permutation which swaps the corresponding elements of these indices.
This permutation is in GX but it does not commute with W. Now we present a detailed proof.
Proof. Let GX ∈ SN. For any relation R = {d1, ..., d|R|} ∈ R, let NR = [Nd1] × ... × [Nd|R|] be
the set of indices into XR. If W is not of the form described in Section 4 then there exist i, j ∈ [|R|],
with ni , ni0 ∈ NRi and nj , nj 0 ∈ NRj such that
Win,ij,nj 6=Win,ij0,nj0	(48)
but
00
P(nid,j) = P(nid,j ), ∀d ∈ Ri ∪ Rj	(49)
22
Under review as a conference paper at ICLR 2020
That is, the pairs ni, nj and ni , nj have the same equality pattern over their elements, but the entries
of Wi,j which correspond to these pairs have differing values, and thus violate the definition of W in
Section 4. To show that the layer σ(W vec(X)) is not an EERL, we will demonstrate a permutation
GX ∈ GX for which GXW 6= WGX, and thus GXσ(Wvec(X)) 6= σ(WGX vec(X)) for some X.
Let GX = LR∈R Nd∈R Gd, with the Gd defined as follows. For d ∈ Ri ∩ Rj and all k:
	i0	i ni	n = ni
	d(k)	d(k)
	nid(k)	n = nid(k)
Gd(n)= <	njd(k)	n = njd(k)
	njd(k)	n=njd(k)
	、n	otherwise
For d ∈ Ri \Rj and all k:	
	[nid(k)	n = nd(k)
Gd(n)=	nid(k)	n = nid(k)
	]n	otherwise
For d ∈ Rj \Ri and all k:	
	'njd(k)	n = njd(k)
Gd(n) =	njd(k)	n=njd(k)
	、n	otherwise
And for d 6∈ Ri ∪ Rj :	
	Gd(n) = n.
That is, each Gd swaps the elements of ni with the corresponding elements of ni0, and the elements
of nj with those of nj 0 , so long as the relevant indices are present. For the case where d ∈ Ri ∩ Rj ,
we need to make sure that this is a valid permutation. Specifically, we need to make sure that it
is injective (it is clearly surjective from [Nd] to [Nd]). But it is indeed injective, since we have
nid(k) = njd(k) iffni0d(k) = nj0d(k) for all d ∈ Ri ∩ Rj and all k, since P(nid,j) = P(nid0,j0).
Now, for all i, let Ki = Nd∈R Gd be the ith diagonal block of GX. By definition of the Gd, for all
d ∈ Ri we have Gdi i0 = 1, and thus by the observation at (34) we have Ki i i0 = 1. And so
nd,n d	n ,n
(GXW)
(i,ni),(j,nj 0) = XX
G(i,ni),(k,nk)W(k,nk),(j,nj 0)
= X Kini,ni00Wni,ij00,nj0
ni00
= Wi,ij0 j0 .
ni0,nj0
Similarly, Kjnj ,nj0 = 1, so
(WGX )(i,ni),(j,nj0) = XX W(i,ni),(k,nk)GXk,nk),(j,nj0)
k nk
= X Win,ij,nj00Kjnj00,nj0
= Wi,j
=	ni,nj.
Finally, by (48), GXW 6= WGX, completing the proof.
□
23
Under review as a conference paper at ICLR 2020
H Details of Experiments
The synthetic data we constructed used 200 instances for each of the three entities. Unless otherwise
noted, we considered a 0.1 fraction as ‘observed’. We ensure that each row and column has at least
5 observations. For training, we pass our observed data through the network, producing encodings
for each entity. We attempt to reconstruct the original input from these encodings. At test time,
we use the training data to produce encodings as before, but now use these encodings to attempt to
reconstruct test observations as well, reporting loss only on these test observations.
For all experiments, we used the following architecture: The encoder consists of 7 EERLs, each
with 64 hidden units, each followed by a batch normalization (Ioffe & Szegedy, 2015) layer and a
channel-wise dropout layer. We then apply a mean pooling layer to produce encodings. We found that
batch normalization dramatically sped up the training procedure. The number of units in the pooling
layer, and hence the size of the encodings, varied by experiment (see below for details). The decoder
architecture is identical to that of the encoder, except that there is no pooling layer before the output.
We used leaky ReLU activations (with parameter 0.1) in all layers except the final ones in both the
encoder and decoder, where no activation is applied. In all experiments we used the Adam (Kingma
& Ba, 2014) optimizer with initial learning rate 0.0001 to optimize a loss between the observed input
values and our predicted output values in the relevant table(s). FOr the synthetic experiments the loss
was RMSE, while for the soccer experiment we used cross-entropy. We did not perform any tuning
of hyperparameters.
H.1 Embedding
We ran this experiment for 5000 epochs, and used an encoding size of 2 to facilitate plotting. We
trained the model on 90% of the data, using 80% for training and 10% for validation. The remaining
10% was used to evaluate the quality of the model learned.
H.2 Missing Record Prediction
We held out a special 10% portion of the data for testing, then trained models using various amounts
of the remaining data. For a 90%, 80%, ..., 10% fraction of the remaining data, we trained a model
using this fraction of the data as observed (using a 90%/10% training/validation split). We then used
each of these trained models to predict the held-out test set. When predicting the test set, we again
varied the fraction of observed data we gave to the model to make its predictions, from 90% to 10%.
For the inductive setting, the trained models were given data from the same distribution as they were
trained on, while in the inductive setting the models were given data generated from a new set of
entity embeddings. For each sparsity level, we trained the model for 4000 epochs, using an encoding
size of 10. We repeated this experiment 15 times and averaged the results.
H.3 Predictive Value of Side Information
We fix the sparsity level of the student-course table at 10% and calculate loss only on this table.
We vary the sparsity level of the other tables over the range [.025, .7]. We use an 80%/10%/10%
training/validation/test split and trained for 8000 epochs, producing an embedding of size 10. We
repeat this experiment 17 times and average the results.
H.4 Predictions on the Soccer Database
The dataset is available at https://www.kaggle.com/hugomathien/soccer. We con-
structed two tables for this experiment. The first table represents the relation between teams and
matches. For input features we used all the player position features, as well as the match’s country,
season, stage and date. We did not include any of the bookie predictions or the number of goals,
as this would be cheating. For each team, we include all the team-related features found in the
Team_Attributes table. When a team appears multiple times in the Team_Attributes table we aggre-
gate the entries by averaging numerical features, and taking the most common value for categorical
ones. We also include an additional binary feature indicating which team is home and which is away.
24
Under review as a conference paper at ICLR 2020
The second table captures the relation between players and matches. For each player participating
in a match, we include all the features in the Player_Attributes table, aggregating entries for each
player as described above. Again, we also include a binary feature indicating whether a player played
for the home or away team.
We use a model built from five EERLs. All but the last one have 52 units (this is based on the
limitation of our GPU memory), use ReLU activations, and are followed by a batch normalization
layer (Ioffe & Szegedy, 2015) and channel-dropout. Chennel-dropout zeros whole channels at random
for all entries of all tables, at a rate of 50%. The final EERL uses no activation and 8 output units. This
is followed by a layer which sum-pools both tables into a single feature vector for each match. Finally,
a linear layer produces a length-3 vector of predictions for each match. We train by minimizing
cross-entropy loss between this output and the true match outcomes for 4000 steps. We report test
results on a 90%/10% training/test split.
25