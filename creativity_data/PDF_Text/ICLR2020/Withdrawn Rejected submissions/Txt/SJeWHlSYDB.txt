Under review as a conference paper at ICLR 2020
Spread divergence
Anonymous authors
Paper under double-blind review
Ab stract
For distributions p and q with different supports, the divergence D(p||q) may not
exist. We define a spread divergence D(p||q) on modified p and q and describe
sufficient conditions for the existence of such a divergence. We demonstrate how
to maximize the discriminatory power of a given divergence by parameterizing and
learning the spread. We also give examples of using a spread divergence to train
and improve implicit generative models, including linear models (Independent
Components Analysis) and non-linear models (Deep Generative Networks).
1 Introduction
A divergence D(p||q) (see, for example Dragomir (2005)) is a measure of the difference between two
distributions p and q with the property
D(p||q) ≥ 0 and D(p||q) = 0 ⇔ p = q
(1)
We are interested in situations in which the supports of the two distributions are different, supp (p) 6=
supp (q). An important class is the f -divergence, defined as
Df (PIIq)= Eq(X) f(px))	⑵
where f(x) is a convex function with f(1) = 0. A special case of an f -divergence is the well-known
Kullback-Leibler divergence KL(pIIq)
Ep(x) [log p(¾]
By setting p(x) to the empirical data
distribution, maximum likelihood training of a model q(x) corresponds to minimising KL(pIIq).
However, this divergence may not be defined since the ratio p(x)/q(x) can cause a division by zero.
This is a challenge since popular implicit generative models (Mohamed & Lakshminarayanan (2016))
of the form q(x) = δ (x - gθ(z))p(z)dz only have limited support. In this case, maximum
likelihood to learn the model parameters θ is not available and alternative approaches to measure
the difference between distributions such as Maximum Mean Discrepancy (Gretton et al. (2012)) or
Wasserstein distance (Peyre et al. (2019)) are required.
2 Spread Divergence
From q(x) and p(χ) We define new distributions q(y) and p(y) that have the same support1. Using
the notation fχ to denote integration / (∙) dx for continuous x, and Pχ∈χ for discrete X with domain
X , we define a random variable y with the same domain as x and distributions
p(y) = p p(y|x)p(X),	q(y) = p p(y|x)q(X)
xx
(3)
where p(y∣x) is 'noise' designed to 'spread, the mass of P and q such that p(y) and q(y) have the same
support. For example, if we use a Gaussian p(y∣x) = N (y ∣ x,σ2), then P and q both have support R.
We also impose an additional requirement on the noise p(y |x),namelythatD(p||q) = 0 ⇔ P = q. As
we show in section(2.1) this is guaranteed for certain 'noise’ distributions. Given these requirements,
we can define the Spread Divergence D(p∣∣q) ≡ D(p∣∣q). ThiS satisfies the divergence requirement
D(p∣∣q) ≥ 0 and D(p∣∣q) = 0 ⇔ P = q.
1For simplicity, we use univariate x, with the extension to the multivariate setting being straightforward.
1
Under review as a conference paper at ICLR 2020
For example, given two delta distributionspo(x) = δ(x - μo),pι(x) = δ(x -μι),theKL divergence
(or f -divergence) between them is not defined. However, the spread KL divergence (or f -divergence)
is defined. Assume a Gaussian noise distribution p(y∣x) = N (y∣x,σ2) where σ2 = 0.5, the
"spreaded" delta distributions have the form: po(y) = Jx δ(x - μo)N (y ∣x, σ2) = N (y ∣μ0, σ2),
pι(y) = JX δ(x - μι)N (y∣x,σ2) = N (y∣μι,σ2). Therefore, the spread KL divergence (with
Gaussian noise) between two delta distributions is equivalent to the KL divergence between two
Gaussian distributions with the same variance, which has closed form (see appendix(D) for a
derivation):
KL(PO(X)M(X)) = KL(p>o(y)||p>i(y)) = M- μι"2∙	⑷
It’s worth noticing that in the case of two delta distributions, the spread KL divergence is equal to the
squared 2-Wasserstein distance (see Peyre et al. (2019); Gelbrich (1990)).
2.1 Noise Requirements for a Spread Divergence
Our main interest is in using noise to define a new divergence in situations in which the original
divergence D(p||q) is itself not defined. For discrete variables X ∈ {1, . . ., n}, y ∈ {1, . . ., n}, the
noise Pij = p(y = i|X = j) must be a distribution Pi Pij = 1, Pij ≥ 0 and
Pijpj =	Pij qj	∀i	⇒	pj = qj	∀j	(5)
jj
which is equivalent to the requirement that the matrix P is invertible. There is an additional
requirement that the spread divergence exists. In the case of f -divergences, the spread divergence
exists provided that P and q have the same support. This is guaranteed if
X Pijpj >0,	XPijqj >0	∀i	(6)
jj
which is satisfied if Pij >0. In general, therefore, there is a space of noise distributions P(y|X) that
define a valid spread divergence. For example, the ‘antifreeze’ method of Furmston & Barber (2009)
is a special form of spread noise to define a valid KL divergence (see also Barber (2012)).
For continuous variables, in order for D(P||q) = 0 ⇒ P = q, the noise P(y|X), with dim(Y ) =
dim(X) must be a probability density and satisfy
P(y|X)P(X)dX =	P(y|X)q (X)dX	∀y ∈ Y	⇒ P(X) = q(X)	∀X ∈ X	(7)
In the following section we discuss the special case of stationary noise for continuous systems.
3 Stationary Spread Divergences
Consider stationary noise P(y|X) = K(y - X) where K(X) is a probability density function with
K(x)>0, x ∈ R. In this case P and q are defined as a convolution
P(y) = /
K(y — X)P(X)dx = (K * P) (y),
K(y - X)q(X)dX = (K * q) (y)
(8)
q⑹
Since K>0, P and q are guaranteed to have the same support R. A sufficient condition for the
existence of the Fourier Transform F {f} of a function f(X) for real X is that f is absolutely integrable.
Since all distributions P(X) are absolutely integrable, both F {P} and F {q} are guaranteed to exist.
Assuming F {K} exists, we can then use the convolution theorem to write
F{P} = F {K} F {P},	F {q} = F{K} F {q}	(9)
Let F{K} 6= 0 or F{K} = 0 on at most a countable set. Then
F{K}F{P} = F{K}F{q} ⇒ F{P} = F{q}.	(10)
The proof is given in appendix(A). Using this we can write
D(PM) = O ⇔ P = q	(11)
⇔ F {K} F {P} = F {K} F {q}	(12)
⇔ F {P} = F {q} ⇔ P = q,	(13)
2
Under review as a conference paper at ICLR 2020
where we used the invertibility of the Fourier transform. Hence, for stationary noise p(y|x) = K(y -
x), we can define a valid spread divergence provided (i) K(x) is a probability density function and
(ii) F{K} 6= 0 or F{K} = 0 on at most a countable set. Interestingly, the sufficient conditions for
defining a valid spread divergence such that D(p||q) = 0 ⇔ P = q are analogous to the characteristic
condition on kernels such that the Maximum Mean Discrepancy MMD(p, q) = 0 ⇔ p = q, see
Sriperumbudur et al. (2011; 2012); Gretton et al. (2012). As an example of such a noise process,
consider Gaussian noise,
F {K} (ω)
K(x)
∞	1	2	22
eiωxe-2σ2X dx = e--厂 > 0
∞
(14)
Similarly, for Laplace noise
b-⅛
K(x) = 2be-b|x1,
F {K} (ω)
>0
(15)
Since in both cases K>0 and F {K} >0, Gaussian and Laplace noise can be used to define a valid
spread divergence.
4	Maximising Discriminatory Power
From the data processing inequality (see appendix(B)), adding spread noise will always decrease the
f -divergenCeDf(P(y)| 血y)) ≤ Df(P(X) ∣∣q(x)). Intuitively, spreading out distributions makes them
more similar. If we are to use a spread divergence to train a model using maximum likelihood (see
section section(5)), there is the danger that adding too much noise may make the spreaded empirical
distribution and spreaded model distribution so similar that it becomes difficult to numerically
distinguish them, impeding training. It is useful therefore to define spread noise that maximally
discerns the difference between the two distributions maxψ D(p(y) ∣∣q(x)) for spread noisepψ(y|x)
parameterised by ψ. In general we need to constrain the spread noise to ensure that the divergence
remains bounded.
We discuss below two complementary approaches to adjust P(y |x) during training. The first approach
adjusts the dimension-wise correlations (this corresponds to adjusting the covariance structure for
Gaussian P(y|x)) and the second forms a mean transformation. In principle, both approaches can be
combined and easily generalized to other noise distributions, such as Laplace noise.
4.1	Learning Covariance structure
Learning the covariance adjusts the shape of noise centered around the original model manifold.
When we maximize the divergence between two spreaded distributions maxψ D(P(y)∣0(χ)), the
learned noise will discourage overlap between the two distributions. Hence, if the data P and model q
lie on the same manifold, the noise will be orthogonal to the manifold.
In learning the Gaussian spread distribution p(y∣x) = N (y\x, Σ), the number of parameters in the
covariance matrix Σ scales quadratically with the data dimension D. We thus define Σ = σ2I + LLT
where σ2 > 0 is fixed (to ensure bounded spread divergence) and L is a learnable D × R matrix
with R D. Calculating the log likelihood and sampling can then be performed efficiently using
standard Woodberry identities, see appendix(J).
4.2	Learning the mean transform
Consider P(y|x) = K(y - f (x)) for injective2 f and stationary K. Then, we define
P(y) = / K (y — f (χ))pχ(χ)dχ
Note that this is a valid spread divergence since, using change of variables,
P(y) = / K (y — z)Pz (z)dz,	Pz(Z)= Pχ(f-1(z))∕J (X = f-1(z))
(16)
(17)
2Since the codomain of f is determined by its range, injective indicates invertible in this case.
3
Under review as a conference paper at ICLR 2020
where J is the absolute Jacobian of f. Hence, D(Py ∣∣qy) =0 ⇔ Pz=qz ⇔ Pχ=qχ. Each injective fφ
gives a different noise p(y|x), we can thus search for the best noise implicitly by learning fφ.
In our experiments we use the invertible residual network Behrmann et al. (2018) fψ : RD → RD
with fψ = (fψ ◦…◦ fψT) denotes a ResNet with blocks fψ = I(•) + gψt (∙). Then fψ is invertible
if the Lipschitz-constants LiP(gψt )<1 for all t ∈ {1, . . . , T}. Note that when using the spread
divergence for training (see SeCtiOn(5.2.2)) we only need samples from p(y) which can be obtained
from equation 16 by first sampling x fromPx(x) and then y fromP(y|x) = K(y-f(x)); this does
not require computing the Jacobian or inverse fψ -1 .
5	Spread Maximum Likelihood Estimation
Minimising the forward KL divergence between the empirical data distribution P(X) and a model pθ (x)
is equivalent to Maximum Likelihood Estimation (MLE) of the parameters θ of the model. Minimising
instead the forward spread KL divergence, IfL(p^(x) ∣∣pθ (x)) = - PnN=Ilog pθ (yn) + const., where
yn are sampled i.i.d from p(y) = Jx p(y ∣x)p(x), results in a new type of estimation, namely spread
MLE. In what follows, we will discuss the statistical properties of spread MLE and demonstrate how
it enables the training of models where maximum likelihood is not suited.
5.1	Statistical properties
Maximum likelihood is a cherished criterion because it exhibits many favourable statistical properties,
mainly consistency (convergence to the true parameters in the large data limit) and asymptotic
efficiency (achieves the Cramer-RaO Lower Bound, which is a lower bound on the variance of any
unbiased estimators) - see Casella & Berger (2002) for an introduction. A key desideratum for spread
MLE is to analyse how these properties are affected. In appendix(E) we demonstrate that spread
MLE (for a certain family of spread noise) needs weaker sufficient conditions than MLE for both
consistency and asymptotic efficiency. Furthermore, a sufficient condition for the existence of MLE is
that the likelihood function is continuous over a compact parameter space Θ. We provide an example
in appendix(E.1) where the compactness requirement is violated, but spread MLE is still well defined.
5.2	Applications
As an application to show the effectiveness of spread MLE, we use it to train implicit models
pθ(x) =	δ (x - gθ(z)) p(z)dz
(18)
where θ are the parameters of the encoder g . We show that, despite the likelihood not being defined
(see also section(K) for a simple linear model example), we can nevertheless successfully train such
models using modified EM/variational algorithms (Barber (2012)).
5.2.1	Training Implicit linear Models: Deterministic ICA
ICA (Independent Components Analysis) corresponds to the model p(x, z) = p(x|z) Qip(zi),
where the independent components zi follow a non-Gaussian distribution. For Gaussian noise ICA an
observation X is assumed to be generated by the process p(x∣z) = Qj N (Xjlgj(z),γ2) where gi(z)
mixes the independent latent process z. In linear ICA, gj (z) = ajTz where aj is the jth column on
the mixing matrix A. For small observation noise γ2, it is well known that the maximum likelihood
EM algorithm to learn A from observed data is ineffective (Bermond & Cardoso, 1999; Winther &
Petersen, 2007). To see this, consider X = Z (where X and Z are the dimension of the data and
latents respectively) and invertible A, X = Az . At iteration k the EM algorithm has an estimate Ak
of the mixing matrix. The M-step updates Ak to
Ak+1 = E XzTE zzT-1	(19)
where, for zero observation noise (γ = 0),
E [xzT] = N XXn (A-IXn) = SA-T,	E [zzT] = A-ISA-T	(20)
n
4
Under review as a conference paper at ICLR 2020
(a) Error versus observation noise γ.
0.6
0.5
0.4
0.3
0.2
0.1
(b) Error versus number of training points.
Figure 1: Relative error |Aiejst - Aitjrue|/|Aitjrue| versus observation noise (a) and number of training
points (b). (a) For X=20 observations and Z=10 latent variables, we generate N =20000 datapoints
from the model x=Az, for independent zero mean unit variance Laplace components on z. The
elements of A used to generate the data are uniform random ±1. We use Sy =1, Sz=1000 samples
and 2000 EM iterations to estimate A. The error is averaged over all i, j and 10 experiments. We
also plot standard errors around the mean relative error. In blue we show the error in learning the
underlying parameter using the standard EM algorithm. As expected, as γ→0, the error blows up as
the EM algorithm ‘freezes’. In orange we plot the error for EM using spread noise; no slowing down
appears as the observation noise γ decreases. In (b), apart from very small N, the error for the spread
EM algorithm is lower than for the standard EM algorithm. Here Z=5, X =10, Sy =1, Sz =1000,
γ = 0.2, with 500 EM updates used. Results are averaged over 50 runs of randomly drawn A.
T1T
and S ≡ N En Xnxn is the moment matrix of the data. Thus, Ak+ι = SA- (Ak 1SAk )	= Ak
and the algorithm ‘freezes’. Similarly, for low noise γ 1 progress critically slows down.
To deal with small noise and the limiting case of a deterministic model (γ = 0), we consider Gaussian
spread noise p(y|x) = N (y∣x,σ2Iχ) to give
P(y, Z)=	p(y Ix)P(X, Z)dx = Y N (yj ∣gj(Z), (γ2 + σ2) Ix) YP(Zi).	QI)
ji
Using spread noise, the empirical distribution is replaced by the spreaded empirical distribution
P(y) = NN Pn N (y∣xn, σ2Iχ) The M-step has the same form as equation 19 but with modified
statistics
E [yZT] = N X N N (y∣xn,σ2) P(Z∣y)yZTdZdy,
E [zzτ] = N X N N (y∣xn,σ2) P(Z∣y)ZZTdZdy.
The E-step optimally sets
P(ZIy) = z~(y) N (zμ(y), ς) Y P(Zi),	Zq (y) = / N (z∣μ(y), ς) Y P(Zi)dz
where Zq(y) is a normaliser and
ς = (Y2 + σ2) (ATA) 1,	μ(y) = (ATA) 1 Ay.
(22)
(23)
(24)
Since the posterior p(z∣j) peaks around N (Z∣μ(y), Σ), we rewrite equation 22 as
E [yZT] = N XZN (y∣xn,σ2) N(Zlμ3), ς) QZIy)i'yZTdZdy
and similarly for E [zzτ] . Writing the expectations with respect to N(Z∣μ(y), Σ) allows for a
simple but effective importance sampling approximation focused on regions of high probability.
5
Under review as a conference paper at ICLR 2020
◎ ” 买4	—
(a) Fixed Laplace
(b) Fixed Gaussian
“40八5 Gaq 5
(d) Covariance
(c) Learned Gaussian
Figure 2: Samples from a generative model (deterministic output) trained using δ-VAE with (a) fixed
Laplace covariance, (b) fixed Gaussian covariance and (c) learned Gaussian covariance. We first train
with one epoch a standard VAE as initialization to all models, and keep latent code Z 〜N (Z ∣ 0,Iz)
fixed when sampling from these models, so we can more easily compare the sample quality. Figure
(d) visualizes the absolute mean of the leading 20 eigenvectors of the learned covariance.
We implement this update by drawing Sy samples from N (y ∣Xn, σ2Iχ) and, for each y sample,
we draw Sz samples from N(z∖μ(y), Σ). This scheme has the advantage over more standard
variational approaches, see for example Winther & Petersen (2007), in that we obtain a consistent
estimator of the M-step update for A. We show results for a toy experiment in figure(1), learning the
underlying mixing matrix in a deterministic non-square setting. Note that standard algorithms such
as FastICA (Hyvarinen, 1999) fail in this setting. The spread noise is set to σ = max(0.001, 2.5 *
sqrt(mean(AAT))). This modified EM algorithm thus learns a good approximation of the underlying
A, with no critical slowing down. Other applications of EM that suffer from this slow down
phenomenon, such as MLE for Slow Feature Analysis (Turner & Sahani (2007)), or other probabilistic
matrix factorization algorithms (Barber (2012)), can also benefit from spread divergence.
5.2.2 TRAINING IMPLICIT NON-LINEAR MODELS: δ-VAE
A standard way to train a deep generative model pθ(x) = Jpθ(χ∣z)p(z)dz is to use maximum
likelihood (minimizing D(P(X)∣∣pθ(x))). The likelihood equation 18 is in general intractable and
it is common to use the variational lower bound (see (Kingma & Welling, 2013)). However, for a
deterministic observation model pθ(x∣z)=δ (X - gθ(Z)) and Z<X, this generative model describes
only a low dimensional manifold in the data space and the divergence D(P(X) ∣∣pθ(x)) is not well
defined. Additionally the above bound is not well defined (due to log of a delta function) and the
variational EM approach fails, as in the deterministic ICA setting. To address this, we instead
minimize the spread divergence KL(p(y)∣∣pθ(y)). For Gaussian noise with fixed diagonal noise
p(y∣x)=N (y∣x,σ2Iχ), we can writep(y) = N Pn=I N (y∣Xn,σ2Ix) and
Pθ (y) = / p(y∣χ)pθ (χ)dχ
We then minimize the divergence
/N (y∣gθ(z),σ2Iχ) p(z)dz = /pθ(y∣z)p(z)dz.	(25)
KL(P(y)IIPθ (y))
/ P(y) log Pθ (y)dy + const.
(26)
Typically, the integral over y is intractable, in which case we resort to a sampling estimation.
Neglecting constants, the divergence estimator is NS PN=I PS=IlogPθ(yS), where y； is a spread
noise sample fromp(yn∣Xn); for example y；〜N (y； ∣Xn, σ2Iχ). For non-linear g, the distribution
pθ (y) is usually intractable and we therefore use the variational lower bound
logPθ (y) ≥
/ qφ (z|y) (-log qφ (z|y) + log (pθ (y∣z)p(z))) dz.
(27)
The approach is a straightforward extension of the standard variational autoencoder and in ap-
pendix(G) we also derive a lower variance objective and detail how to learn the spread noise (also see
appendix(F)). We dub this model and associated spread divergence training the 'δ-VAE'.
6
Under review as a conference paper at ICLR 2020
(a) δ Fixed spread noise	(b) δ Learned spread noise
Figure 3: Samples from a generative model with deterministic output trained using δ-VAE with (a)
fixed and (b) learned spread with injective function. We use a similar sampling strategy as in the
MNIST experiment to facilitate sample comparison between the different models - see Section(I).
Encoder-Decoder Models	FID	GAN Models	FID
VAE	63.0	WGAN GP	30.0
δ-VAE with fixed spread	52.7	BEGAN	38.9
δ-VAE with learned spread	46.5	WGAN	41.3
		DRAGAN	42.3
WAE-MMD	55.0	LSGAN	53.9
WAE-GAN	42.0	NS GAN	55.0
		MMGAN	65.6
Table 1: CelebA FID Scores. The δ-VAE re-
sults are the average over 5 independent mea-
surements. The scores of GAN-based mod-
els are based on a large-scale hyperparameter
search and take the best FID obtained Lucic
et al. (2018). The results of VAE and WAE-
based model are from Tolstikhin et al. (2017).
MNIST Experiment: We trained a δ-VAE on MNIST (LeCun et al. (2010)) with (i) fixed Laplace
spread noise, equation 15, (ii) fixed Gaussian spread noise, equation 14 and (iii) Gaussian noise with
learned covariance, section(4.1) with rank R = 20; see appendix(H) for details. Figures 2(a,b,c)
show samples from pθ (x) for these models; MNIST is sufficiently easy that it is hard to distinguish
between the quality of the fixed and learned noise samples. However, qualitatively, the sharpness of
the Laplace spread noise trained model is higher than for the Gaussian noise and motivates that the
spread noise can affect the quality of the learned model. We speculate that Laplace noise improves
image sharpness since the noise focuses attention on discriminating between points close to the data
manifold (since the Laplace distribution is leptokurtic and has a higher probability of generating
points close to the data manifold than the Gaussian distribution). Figure 2(d) visualizes the Gaussian
learned covariance and shows that the learned noise is largely orthogonal to the data manifold.
CelebA Experiment: We trained a δ-VAE on the CelebA dataset (Liu et al., 2015) with (i) fixed and
(ii) learned spread with injective function, see appendix(I). We compared to results from a standard
VAE with fixed Gaussian noisep(x∣z) = N(x∖gθ(z), 0.5IX) Tolstikhin et al. (2017) . For (i) the
fixed spread divergence uses Gaussian noise N (y|x, 0.25IX). For (ii) we use Gaussian noise with
learned injective function ResNet fψ (∙) = I(•)+ gψ (∙); see appendix(I) for more details. Figure 3
shows samples from δ-VAE trained using Gaussian spread divergence with both fixed and learned
spread noise (with gθ(z) initialised to the fixed-noise setting). It is notable how the ‘sharpness’
of the image samples substantially increases when learning the spread noise. Table 1 shows FID
(Heusel et al. (2017)) score comparisons between different algorithms3. The δ-VAE significantly
improves on the standard VAE result; δ-VAE with injective function learning also improves on the
fixed-noise δ-VAE. Indeed the injective δ-VAE results are comparable to popular GAN and WAE
models (Gulrajani et al. (2017); Berthelot et al. (2017); Arjovsky et al. (2017); Kodali et al. (2017);
Mao et al. (2017); Fedus et al. (2017); Tolstikhin et al. (2017)). Whilst the δ-VAE results are not fully
state-of-the-art, we believe it is the first time that implicit models have been trained using a principled
maximum likelihood based approach. Our expectation is that by increasing the complexity of the
generative model gθ and injective function fψ, or using different noise such as Laplace distribution,
the results will become competitive with state-of-the-art GAN models.
3FID scores were computed using github.com/bioinf-jku/TTUR based on 10000 samples.
7
Under review as a conference paper at ICLR 2020
6	Related Work
MMD versus spread f -divergence: In spite of the conditions required for defining the spread
divergence being closely related to the kernel requirement of MMD (Gretton et al., 2012), we also
show that MMD and spread Total Variation distance4 can be written as different norms (L2 , L1
respectively) of a common objective (see appendix(C)).
Instance noise: The instance noise trick to stabilize GAN training Roth et al. (2017); S0nderby
et al. (2016) is a special case of spread divergence using fixed Gaussian noise. Whilst other similar
tricks (for example Furmston & Barber (2009)) have been proposed previously, we believe that it is
important to state the general utility of the spread noise approach.
δ-VAE versus WAE: The Wasserstein auto-encoder Tolstikhin et al. (2017) is another implicit
generative model that uses an encoder-decoder architecture. The difference is that δ-VAE is based on
KL divergence which is corresponding to MLE but WAE uses the Wasserstein distance.
δ-VAE versus denoising VAE: The Denoising VAE Im et al. (2017) uses a VAE with noise added to
the data only. In contrast, the δ-VAE adds noise to both the data and model. Since the denoising VAE
model only adds noise to the model, it cannot recover the true data distribution.
MMD GAN with kernel learning: The idea of learning a kernel to increase discrimination is also
used in MMD GAN (Li et al. (2017)). Similar to ours, the kernel in MMD GAN is constructed by
k = k ◦ fψ where k ιsa fixed kernel and fψ IS a neural network. To ensure Mkf (p,q) = 0 ⇔ P = q,
this requires fψ to be injective (Gretton et al. (2012)). However, in the MMD GAN framework,
fψ(x) usually maps x to a lower dimension. This is crucial for MMD because the amount of data
required to produce a reliable estimator grows with the data dimension (Ramdas et al. (2015)) and
the computation cost of MMD scales quadratically with the amount of data. Whilst using a lower-
dimensional mapping makes MMD more practical it also makes it difficult to construct an injective
function f . For this reason, heuristics such as the auto-encoder regularizer (Li et al. (2017)) are
considered. In contrast, for the δ-VAE, the computational cost of estimating the divergence is linear in
the number of datapoints. For this reason there is no need for fψ to be a lower-dimensional mapping;
guaranteeing that fψ is injective is therefore relatively straightforward for the δ-VAE.
Flow-based generative models: Invertible flow-based functions (Rezende & Mohamed (2015)) have
been used to boost the representation power of generative models. Note our use of injective functions
is quite distinct from the use of flow-based functions to boost generative model capacity. In our
case, the injective function f does not change the model - it only changes the divergence. For this
reason, the spread divergence doesn’t require the log determinant of the Jacobian (which is required in
Rezende & Mohamed (2015); Behrmann et al. (2018)) meaning that more general invertible functions
can be used to boost the discriminatory power of a spread divergence.
7	Summary
We described how to define a divergence even when two distributions do not have the same support.
Previous approaches (Furmston & Barber, 2009; S0nderby et al., 2016) can be seen as special cases.
We showed that defining divergences this way enables us to train deterministic generative models
using standard likelihood based approaches. In principle, we can learn the underlying true data
generating process by the use of any valid spread divergence. In practice, however, the quality of the
learned model can depend strongly on the choice of spread noise. We therefore investigated learning
spread noise to maximally discriminate two distributions. We found the resulting training approach
stable and that it can significantly improve the image generation results. Whilst state-of-the-art
image generation is not the focus of this work, we obtained promising results. We also discussed the
conditions under which spread MLE is consistent and asymptotically efficient, some of which are
weaker than the equivalent MLE conditions. Perhaps the most appealing aspect of the spread noise is
that is enables one to re-use standard machine learning approaches in statistics such as maximum
likelihood to train models that would be otherwise unsuited to standard statistical training approaches.
4Total Variation distance between p(x) and q(x) is defined (up to a constant scale) as TV (p(x)||q(x))
|p(x) - q(x)|dx, it belongs to f -divergence family (see Liese & Vajda (2006)).
8
Under review as a conference paper at ICLR 2020
References
M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.
D. Barber. Bayesian Reasoning and Machine Learning. Cambridge University Press, New York, NY,
USA, 2012. ISBN 0521518148, 9780521518147.
J. Behrmann, D. Duvenaud, and J. Jacobsen. Invertible residual networks. arXiv preprint
arXiv:1811.00995, 2018.
O. Bermond and J. Cardoso. Approximate likelihood for noisy mixtures. In Proc. ICA ’99, pp.
325-330,1999.
D. Berthelot, T. Schumm, and L. Metz. Began: Boundary equilibrium generative adversarial networks.
arXiv preprint arXiv:1703.10717, 2017.
G.	Casella and R. Berger. Statistical inference, volume 2. Duxbury Pacific Grove, CA, 2002.
H.	Cramer. Mathematical methods of statistics, volume 9. Princeton university press, 1999.
S. Dragomir. Some general divergence measures for probability distributions. Acta Mathematica
Hungarica, 109(4):331-345, Nov 2005. ISSN 1588-2632. doi: 10.1007/s10474-005-0251-6.
W. Fedus, M. Rosca, B. Lakshminarayanan, A. Dai, S. Mohamed, and I. Goodfellow. Many
paths to equilibrium: Gans do not need to decrease a divergence at every step. arXiv preprint
arXiv:1710.08446, 2017.
T. Ferguson. An inconsistent maximum likelihood estimate. Journal of the American Statistical
Association, 77(380):831-834, 1982.
T. Furmston and D. Barber. Solving deterministic policy (PO)MPDs using Expectation-Maximisation
and Antifreeze. In First international workshop on learning and data mining for robotics (LEMIR),
pp. 56-70, 2009. In conjunction with ECML/PKDD-2009.
M. Gelbrich. On a formula for the L2 Wasserstein metric between measures on euclidean and hilbert
spaces. Mathematische Nachrichten, 147(1):185-203, 1990.
S. Gerchinovitz, P. Menard, and G. Stoltz. Fano’s inequality for random variables. arXiv, 2018. doi:
arXiv:1702.05985v2.
A. Gretton, K. Borgwardt, M. Rasch, B. Scholkopf, and A. Smola. A kernel two-sample test. Journal
of Machine Learning Research, 13(Mar):723-773, 2012.
I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. Courville. Improved training of Wasser-
stein gans. In Advances in Neural Information Processing Systems, pp. 5767-5777, 2017.
M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. Gans trained by a two
time-scale update rule converge to a local nash equilibrium. In Advances in Neural Information
Processing Systems, pp. 6626-6637, 2017.
A. Hyvarinen. Fast and robust fixed-point algorithms for independent component analysis. IEEE
Transactions on Neural Networks, 10(3):626-634, May 1999. ISSN 1045-9227. doi: 10.1109/72.
761722.
D. Im, S. Ahn, R. Memisevic, and Y. Bengio. Denoising criterion for variational auto-encoding
framework. In Thirty-First AAAI Conference on Artificial Intelligence, 2017.
S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing
internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
D. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,
2014.
D.	Kingma and M. Welling. Auto-Encoding Variational Bayes. arXiv:1312.6114 [stat.ML], 2013.
9
Under review as a conference paper at ICLR 2020
N. Kodali, J. Abernethy, J. Hays, and Z. Kira. On convergence and stability of gans. arXiv preprint
arXiv:1705.07215, 2017.
Y. LeCun, C. Cortes, and C. Burges. Mnist handwritten digit database. AT&T Labs [Online].
Available: http://yann. lecun. com/exdb/mnist, 2:18, 2010.
E.	Lehmann. Elements of large-sample theory. Springer Science & Business Media, 2004.
C. Li, W. Chang, Y. Cheng, Y. Yang, and B. P6czos. Mmd gan: Towards deeper understanding of
moment matching network. In Advances in Neural Information Processing Systems, pp. 2203-2213,
2017.
F.	Liese and I. Vajda. On divergences and informations in statistics and information theory. IEEE
Transactions on Information Theory, 52(10):4394-4412, 2006.
Z. Liu, P. Luo, X. Wang, and X. Tang. Deep Learning Face Attributes in the Wild. In Proceedings of
International Conference on Computer Vision (ICCV), 2015.
M. Lucic, K. Kurach, M. Michalski, S. Gelly, and O. Bousquet. Are gans created equal? a large-scale
study. In Advances in neural information processing systems, pp. 700-709, 2018.
X. Mao, Q. Li, H. Xie, R. Lau, Z. Wang, and Paul S. Least squares generative adversarial networks.
In Proceedings of the IEEE International Conference on Computer Vision, pp. 2794-2802, 2017.
T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida. Spectral normalization for generative adversarial
networks. arXiv preprint arXiv:1802.05957, 2018.
S. Mohamed and B. Lakshminarayanan. Learning in implicit generative models. arXiv preprint,
2016. doi: arXiv:1610.03483.
B. Pearlmutter. Fast exact multiplication by the hessian. Neural computation, 6(1):147-160, 1994.
G. Peyr6, M. Cuturi, et al. Computational optimal transport. Foundations and Trends® in Machine
Learning, 11(5-6):355-607, 2019.
A. Ramdas, S. Reddi, B. P6czos, A. Singh, and L. Wasserman. On the decreasing power of kernel
and distance based nonparametric hypothesis tests in high dimensions. In Twenty-Ninth AAAI
Conference on Artificial Intelligence, 2015.
J.	Rezende and S. Mohamed. Variational inference with normalizing flows. arXiv preprint
arXiv:1505.05770, 2015.
K.	Roth, A. Lucchi, S. Nowozin, and T. Hofmann. Stabilizing training of generative adversarial
networks through regularization. In Advances in neural information processing systems, pp.
2018-2028, 2017.
N. Schraudolph. Fast curvature matrix-vector products for second-order gradient descent. Neural
computation, 14(7):1723-1738, 2002.
C. S0nderby, J. Caballero, L. Theis, W. Shi, and F. Huszdr. Amortised map inference for image
super-resolution. arXiv preprint arXiv:1610.04490, 2016.
B. Sriperumbudur, A. Gretton, K. Fukumizu, B. Scholkopf, and G. Lanckriet. Hilbert space em-
beddings and metrics on probability measures. Journal of Machine Learning Research, 11(Apr):
1517-1561, 2010.
B. Sriperumbudur, K. Fukumizu, and G. Lanckriet. Universality, Characteristic Kernels and RKHS
Embedding of Measures. J. Mach. Learn. Res., 12:2389-2410, July 2011. ISSN 1532-4435.
B. Sriperumbudur, K. Fukumizu, A. Gretton, B. Scholkopf, and G. Lanckriet. On the Empirical
Estimation of Integral Probability Metrics. Electronic Journal of Statistics, 6:1550-1599, 2012.
M. Tipping and C. Bishop. Probabilistic principal component analysis. Journal of the Royal Statistical
Society, Series B, 21/3:611-622, January 1999.
10
Under review as a conference paper at ICLR 2020
I. Tolstikhin, O. Bousquet, S. Gelly, and B. Schoelkopf. Wasserstein auto-encoders. arXiv preprint
arXiv:1711.01558, 2017.
R. Turner and M. Sahani. A maximum-likelihood interpretation for slow feature analysis. Neural
computation,19(4):1022-1038, 2007.
A. Wald. Note on the consistency of the maximum likelihood estimate. The Annals of Mathematical
Statistics, 20(4):595-601, 1949.
O. Winther and K. Petersen. Bayesian independent component analysis: Variational methods and
non-negative decompositions. Digital Signal Processing, 17(5):858 - 872, 2007. ISSN 1051-2004.
Special Issue on Bayesian Source Separation.
M. Zhang, T. Bird, R. Habib, T. Xu, and D. Barber. Variational f-divergence minimization. arXiv
preprint arXiv:1907.11891, 2018.
11
Under review as a conference paper at ICLR 2020
A Proof of Theorem A
Theorem 1. Consider distributions p, q, and L1 integrable function K and Fourier Transform F{K}.
Let F{K} 6= 0 or F{K} = 0 on at most a countable set. Then
F{K}F{p} = F{K}F{q} ⇒ F{p} = F{q}.	(28)
Proof. When F{K} 6= 0, F{K}F{p} = F{K}F {q} ⇒ F{p} = F{q} is trivial. We first show
that the Fourier transform of an L1 function is continuous on Rd. When F{K} = 0 on at most a
countable set, we then show that F{q} and F{p} cannot be different at a set of countable points.
Since any distribution q is in L1, we can write
l→0 lF{q}(w + C)-F⑷(W)I
lim
→0
q(x) e-2πix(w+) - e-2πixw) dx
≤ li→m0	|q(x)| e-2πix
- 1 dx
l→im0 |q(x)| e-2πix
- 1 dx (Dominated Convergence Theorem)
0
So F{q} is (uniformly) continuous. The same argument applies to show that F{p} is uniformly
continuous.
Since F{K} = 0 on at most a countable setC, we assume there is a point w0 ∈ C where F{q}(w0) 6=
F{p}(w0). Without loss of generality, we assume F{q}(w0) - F{p}(w0) = θ > 0. For points
w0+h that are not in C, F {K}(w0 +h) 6= 0 and it follows therefore that F{K}F{p} = F{K}F{q}
implies F{q}(w0 + h) - F {p}(w0 + h) = 0. By continuity of F{p} and F{q}, we have F {q}(w0 +
h) — F{p}(xo + h) → 0 when h → 0, which leads to a contradiction (θ cannot be zero).	□
B S pread noise makes distributions more similar
The data processing inequality for f -divergences (see for example Gerchinovitz et al. (2018)) states
that Df (p(y)∣∣q(y)) ≤ Df(P(X) ∣∣q(χ)). For completeness, We provide here an elementary proof of
this result. We consider the following joint distributions
q(y, x) = P(y|x)q(x),	P(y, x) = P(y|x)P(x)	(29)
whose marginals are the spreaded distributions	q(y) = / p(y∣χ)q(χ) J --r.	
P(y) = / p(y∣χ)p(χ), J --r.		(30)
The divergence between the two joint distributions is		
Df (P(y, x)||q(y, x)) =	q(y, x)f	P(y|x)P(x) ,p(yF≡rDf(P(X)IIq(X))	(31)
The f -divergence between two marginal distributions is no larger than the f -divergence between the
joint (see also Zhang et al. (2018)). To see this, consider
Df(P(U,v)||q(u,v)) = / q(u) / q(v|u)f (P(ULv)) dydu
≥ Z q(u)f Uq(v|U) q(÷M⅛ dv)du
q(U)f
U = Df (P(U)||q(U))
Hence,
Df(P(y)I∣q(y)) ≤ Df(p(y,χ)l∣q(y,χ)) = Df(p(χ)l∣q(χ))	(32)
Intuitively, spreading two distributions increases their overlap, reducing the divergence. When P and
q do not have the same support, Df (q(x)||P(x)) can be infinite or not well-defined.
12
Under review as a conference paper at ICLR 2020
C Relation to MMD
Spread divergence can be generally constructed from f -divergence, we show how to build a connection
to maximum mean discrepancy (Gretton et al. (2012)) by using the spread total variation distance:
For a translation invariant kernel k(∙,x), the MMD is (We use Yk to denote the MMD distance with
kernel k)
Yk(P(X)IIq(X))
k(∙, x)p(x)dx — J k(∙, x)q(x)dx
H
Suppose both k and it’s square root of Fourier transform
I (^represen
ts the Fourier transform) are
absolutely integratable, we can also rewrite the MMD distance the as following (see Sriperumbudur
et al. (2010) for a derivation):
Yk (p∣∣q) = (2∏)-d∕4∣∣Φ * P - Φ * q∣∣L2,
where Φ := (v^)∨ (∨ represents the inverse Fourier transform).
We can further define Φ = Z-1Φ, where Z =	Φ(X)dX. Therefore, Φ is a probability density
function and the MMD can be written as
Yk(p∣∣q) = (2n)-d/4z ∣∣Φ * p - Φ * q∣∣L2
H ∣∣Φ * p — Φ * q∣∣L2.
Total variation distance between P(X) and q(X) is defined as
TV(PIIq) = IIP - qIIL1.
In the spread total variation distance, we can define spread as the noise distribution k = Φ, thus
C22"T≤	. .	. . ~	~	..
Tg(PIIq) = iiφ * p - φ * q||Li
So MMD and spread Total Variation distance can be written as different norms ofa common objective.
D Spread divergence between two delta distributions
Letpo(x) = δ(x - μo),Pq(x) = δ(x - μι), assume Gaussian spread noise p(y∣x) = N (y∣x,σ2)
and σ2 = 0.5, so
ʃ-~~^.......... ,..,.....,..
KL(PO (X)M(X)) = KL(p0(y)∣∣p1(y))
= KL	P(yIX)P0(X)II	P(yIX)P1(X)
=KL(N (y∣μι,σ2) ||N (y∣μ2,σ2))
σ2	σ2 + (μι - μ2)2	1
= log σ2+ —2σ----------------2
=(μι - μ2)2
E Statistical Properties of Maximum Likelihood Estimator
E.1 Existence of Spread MLE
In some situations there may not exist a Maximum Likelihood Estimator (MLE) for P(XIθ), but there is
a MLE for the spread model p(y∣θ) = Jp(y∣x)p(x∣θ)dx. For example, suppose that X 〜 N(μ,σ2)
(μ, 0 < σ2 < ∞). So θ = (μ,σ2) ∈ R X R+. Assume we only have one data point x. Then the
log-likelihood function is L(x; θ) h - log σ - 击(X — μ)2. Maximising with respect to μ, we have
μ = X and the log-likelihood becomes unbounded as σ2 → 0. In this sense, the MLE for (μ, σ2)
does not exist.
13
Under review as a conference paper at ICLR 2020
On the other hand, We can check whether the MLE for p(y∣θ) exists. We assume Gaussian spread
noise with fixed variance σf2 . Since we only have one data point x, the spread data distribution
becomesp(y∣x) = N (y]x,02), and the model is p(y∣θ) = N (y∣μ,σ2 + σj). We can sample
N points from the spread model, so the spread log likelihood function is (neglecting constants)
L(yι,...,yN; θ) = -N log(σ2 + σf) - 292+若)PN=ι(y - μ)2. The MLE solution for μ is
μ = N PN= 1 yi; the MLE solution for σ2 is σ2 = N Pi(yi — μ)2 - σf, which has bounded spread
likelihood value. Note that in the limit of a large number of spread samples N → ∞ , the MLE
σ2 = NN (yi - μ)2 → σf tends to 0. Throughout, however, the (scaled by N) log likelihood remains
bounded.
E.2 Consistency
Consistency of an estimator is an important property that guarantees the validity of the resulting
estimate at convergence as the number of data points tends to infinity. In what follows, we refer to
sufficient conditions for a consistent MLE estimator, before addressing the question of whether using
spread MLE is also consistent and under what conditions.
E.2. 1 Consistency for MLE
Sufficient conditions for the MLE being consistent and converging to the global maximum are given
in Wald (1949). However, they are usually difficult to check even for some standard distributions.
The sufficient conditions for MLE being consistent and converging to a local maxima are given in
Cramer (1999) and are more straight forward to check:
C1. (Identifiable): p(χ∣θι) = p(χ∣θ2) → θι = θ2.
C2. The parameter space Θ is an open interval (θ, θ), Θ: -∞ ≤ θ < θ < θ ≤∞.
C3. p(x∣θ) is continuous in θ and differentiable with respect to θ for all x.
C4. The set A = {x : pθ(x) > 0} is independent of θ.
Let Xi, X2,... be i.i.d with density p(x∣θo) (θ ∈ Θ) satisfying conditions C1-C4, then there exists
a sequence θn = θn(X1,…,Xn) of local maxima of the likelihood function L(θ0) = Qn=I p(χi∣θ0)
which is consistent:
θ → θo for all θ ∈ Θ
The proof can be found in Lehmann (2004) or Cramer (1999).
E.2.2 Consistency of spread MLE
We provide the necessary conditions for Spread MLE being consistent.
C1. (Identifiable): p(χ∣θ) is identifiable. From section(3) it follows immediately thatp(y∣θι)=
p(y∣θ2) → p(x∣θι) = p(χ∣θ2) → θi = θ2, where the final implication follows from the
assumption thatp(χ∣θ) is identifiable. Hence ifp(χ∣θ) is identifiable, so is p(y∣θ).
C2. The parameter space Θ is an open interval (θ, θ), Θ : -∞ ≤ θ < θ < θ ≤ ∞. This
condition is unchanged for p(y∣θ).
C3. On p(y∣θ), we require the same condition on p(x∣θ) as in MLE; p(y∣θ) is continuous in θ
and differentiable with respect to θ for all y.
C4. For spread noise p(y|x) who has full support on Rd (for example Gaussian noise), p(y∣θ) is
greater than zero everywhere and hence the original condition C4 is automatically guaran-
teed.
The conditions that guarantee consistency for spread MLE are weaker for the spread model p(y∣θ)
than for the standard model p(χ∣θ), since C4 is automatically satisfied. Ferguson (1982) gives an
example for which MLE exists but is not consistent by violating condition C4, whereas spread MLE
can be used to obtain a consistent estimator.
14
Under review as a conference paper at ICLR 2020
E.3 Asymptotic Efficiency
A key desirable property of any estimator is that it is efficient. The Cramer-Rao bound places a lower
bound on the variance of any unbiased estimator and an efficient estimator much reach this minimal
value in the limit of a large amount of data. Under certain conditions (see below) the Maximum
Likelihood Estimator attains this minimal variance value meaning that there is no better estimator
possible than maximum likelihood (in the limit of a large amount of data). This is one of the reasons
that the maximum likelihood is a cherished criterion.
E.3.1 Asymptotic Efficiency for MLE
Building upon conditions C1-C4, additional conditions on p(x∣θ) are required to show MLE is
asymptotical efficient:
C5. For all x in its support, the density pθ(x) is three times differentiable with respect to θ and
the third derivative is continuous.
C6. The derivatives of the integral pθ (x)dx respect to θ can be obtained by differentiating
under the integral sign, that is: Vθ Jpθ(x)dx = J ∂θpθ(x)dx.
C7. There exists a positive number c(θ0) and a function Mθ0 (x) such that
∂3
|∂θ3 logPθ(x)| ≤ Mθo(x) forall X ∈ A, ∣θ — θo∣ < c(θo)
where A is the support set ofx and Eθ0 [Mθ0 (x)] < ∞.
Let X1, ..., Xn be i.i.d with density pθ (x) and satisfy conditions C1-C7, then any consistent sequence
θ = θn(X1, ..., Xn) of roots of the likelihood equation satisfies
√n(θ — θo) → N(0,F(θo)-1),
where F-1(θ0) is the inverse ofFisher information matrix (also called Cramer-Rao Lower Bound,
which is a lower bound on variance of any unbiased estimators ). The conditions and proof can be
found in Lehmann (2004).
E.3.2 Asymptotic Efficiency for MLE
As with MLE above, we require further conditions on p(y∣θ) for ensuring spread MLE is asymptoti-
cally efficient:
C5. On p(y∣θ), we require the same condition as applied to p(x∣θ) in the MLE case; for all y
in its support, the density pθ(y) is three times differentiable with respect to θ and the third
derivative is continuous.
C6. For spread noise p(y|x), which has full support on Rd (for example Gaussian noise), the
support of y is independent of θ. Leibniz’s rule5 allows us to differentiate under the integral:
Vθ pθ(y)dy = ∂θpθ(y)dy, so this condition is automatically satisfied.
C7. On p(y∣θ), we require the same condition as applied to p(χ∣θ) in the MLE case; There exist
positive number c(θ0) and a function Mθ0 (y) such that
∂3
|∂θ3 logPθ(y)l ≤ Mθ0(y) forally ∈ A, ∣θ — θo∣ < c(θo)
where A is the support set ofy and Eθ0 [Mθ0 (y)] < ∞.
Thus the conditions that guarantee asymptotically efficient for the spread model p(y∣θ) are weaker
than for the standard model p(χ∣θ), since C4 and C6 are automatically satisfied.
5Leibniz's rule tells us:* Ra(：) p(x,θ)dx = /：(；； ∂θp(x,θ)dx + p(b(θ),θ)*b(θ) - p(a(θ),θ)*a(θ),
so if a(θ) and b(θ) are independent of θ, then * Ra ρ(x, θ)dx = Ra ∂θρ(x, θ)dx.
15
Under review as a conference paper at ICLR 2020
F Perturbation Approximation of Gaus s ian S pread
Herein, in the fixed noise setting, we derive a perturbation based approximation to the spread noise,
which in princple can lead to a lower variance estimator. We can write a function f with perturbed
input and integrated over noise as
Ep(ξ) [f(x+ξ)], wherep(ξ) = N (ξ∣0, Σ). Taylor expanding around ξ = 0, we have		(33)
Ep(ξ) [f (x + ξ)] ≈ Ep(ξ)	f (x) + ξτVf (x) + 1 ξτV1 2f (x)ξ + O(ξ3)	(34)
≈ f (x) + 2Ep(ξ) [ξτHξ]		(35)
=f(x) + 2Tr(Ep(ξ) [ξξT]H)		(36)
=f(x) + 1 Tr (∑H)		(37)
Where H is the Hessian matrix Hij = ∂X∙f ∙ When x,s dimension is small, We can use equation 37
to explicitly calculate the trace. When the dimension of x is large, we can form a Monte Carlo
estimation of equation 35. To do this we first sample ξ 〜p(ξ) and then calculate the Hessian-vector
product Hξ. This can be efficiently calculated by AutoDiff backWard mode (Schraudolph (2002);
Pearlmutter (1994)), without storing the Hessian matrix in memory.
For example, in δ-VAE with fixed Gaussian noise, according to the bound equation 56 (ignoring the
constant):
/N (y∣x,σ2Iχ) logpθ(y) ≥ EN0 ∣o,σ^iχ) [H(∑φ(x + σex))]	(38)
+En&∣0,σ2iχ) [En(e|o,i) [logp(x = gθ(μφ(x + σex) + Cφ(x)e)) + logP(Z = μφ(x + σex) + Cφ(x)e)]]
(39)
σ2
≈H(∑φ(x)) + EN(e∣0,I)[logp(x = gθ(μφ(x) + Cφ(x)c)) + logP(Z = μφ(x) + Cφ(x)e)] +—Tr(H)
X---------------------------------------{---------------------------------------}	2
f(x)
(40)
Where H is the Hessian matrix Hij = ɪfd? and Cφ(χ) is the Cholesky decomposition of Σφ(χ).
G Spread Divergence for Deterministic Deep Generative Models
Instead of minimising the likelihood, we train an implicit generative model by minimising the spread
divergence
min KL(P(y)IIPθ (y))	(41)
θ
For Gaussian noise with fixed diagonal noise P(y|x) = N(y|x, σ2IX), we can write
1N
Py) = N EN (y∣xn,σ2Iχ)	(42)
n=1
and
Pθ (y) = / p(y∣χ)pθ (χ)dχ
/N (y∣gθ(z),σ2Iχ) p(z)dz = /Pθ(y∣z)p(z)dz
(43)
For the spread divergence with learned covariance Gaussian noise which is discussed in section(4.1),
we can write
1N
pψ(y|x) = N(y|x, ςψ),	p(y) = N EN(y∣xn, ςψ)	(44)
n=1
16
Under review as a conference paper at ICLR 2020
and spread divergence with learned injective function as discussed in section(4.2)
pψ ⑶X) = N (y∖fψ(X),σ2Ix),
1N
P⑻=N fN(y∖fψ(x),σ Ix)
n=1
(45)
According to our general theory,
min KL(P(y)∣∣Pθ (y)) = 0	⇔	P(X)= Pθ (x)	(46)
θ
Here
KL(P(y)IIPθ (y))
1N
-NX Z
p(y)log pθ (y)dy + const.
(47)
Typically, the integral over y will be intractable and we resort to an unbiased sampled estimate
(though see below for Gaussian q). Neglecting constants, the KL divergence estimator is
NS
NS XX log pθ (yn)	(48)
n=1 s=1
where ySn is a perturbed version of xn. For example ySn ~ N (y^ ∣xn, σ2IX) for fixed Gaussian noise
case and other cases are similar. In most cases of interest, with non-linear g, the distribution pθ (y) is
intractable. We therefore use the variational lower bound
logPθ(y) ≥ ʃ qφ(z∣y)(-log qφ(z∣y)+log(pθ(y | z)p(z))) dz	(49)
Parameterising the variational distribution as a Gaussian,
qφ(ZIy)= N(Z∖μφ(y), ςφ3D	(5O)
then we can reparameterise and write
logpθ(y) ≥ H(ςφ3)) + ENg0,1) [log (Pθ(y|z = μφ + Cφ(y)e)p(z = μ°(y) + Cφ(y)e))] (51)
where H(Σφ(y)) is the entropy of a Gaussian with covariance Σφ(y). For fixed covariance Gaussian
spread noise in D dimensions, this is
logPθ(y) ≥ H(∑φ(y))+EN(e∣o,i)-
1
MD/2
(y - gθ (μφ(y) + Cφ(y)∈))2 + logP(Z = μφ(y) + Cφ(y)e) +const.
(52)
where Cφ(y) is the Cholesky decomposition of Σφ(y).
We can integrate equation 52 over y to give the bound
/N (y∣x,σ2Iχ) logpθ(y) ≥ EN⑺工后改)[H(Σφ(y)) + EN(e∣0,1) [logP(Z = μφ(y) + Cφ(y)e)]]
(53)
—
,c :D∕2EN(e∣0,1) hEN(y∣χ,σ2iχ) h(y-fψ(gθ(μφ(y) + Cφ(y)e)))2]i + const.
(2σ2)
(54)
where
EN(y∣χ,σ2iχ)[(y - gθ (μφ(y) + Cφ(y)e))2]
=σ2 - 2EN(ex ∣0,Iχ) [exgθ(μφ(x + σex) + Cφ(y)e)] + EN(ex ∣0,Iχ) [(x - gθ(μφ (X + σex) + Cφ(y)e))[
(55)
We notice that the second term is zero, so the final bound for the fixed Gaussian spread KL divergence
is (ignoring the constant)
/N (y∣x,σ2Iχ) logpθ(y) ≥ EN(y∣χ,σ2iχ) [H(Σφ(y)) + EN(e∣0,1) [logP(Z = μφ(y) + Cφ(y)e)]]
—
(2σ2)D∕2 EN (ex ∣0,Ix ) hEN (e 口，)h(X - g ("。(X + °") + C。')/]]
(56)
17
Under review as a conference paper at ICLR 2020
By analogy, for spread KL divergence with learned variance, the bound is (ignoring the constant)
/N(y∣x, ςψ)logpθ(y) ≥ EN(y∣x,∑ψ) [HNφ(J)) + EN(e∣0,i) [logP(Z = μφ(y) + Cφ(y)e)∖∖
-EN (∈χ ∣0,Σψ ) [EN (e ∣0,I) [(x - gθ (μφ (X + Sψ Ex) + Cφ(y)e))τ ς-1 (X - gθ (μφ (X + Sψ Ex) + Cφ(y)c))]]
(57)
Where Sψ is the cholesky decomposition of Σψ . For specific covariance structure introduced in
section section(4.1), efficient methods for sampling, matrix inverting and log determinant calculation
are available, see appendix(J).
For spread KL divergence with learned injective function, the bound is (ignoring the constant)
/N (y∣fψ(x),σ2Iχ) logpθ(y) ≥ EN(y∣x,σ2Iχ) [h(ςφ3)) + EN(e∣0,I) [logP(Z = Mφ(y) + Cφ3)E)∖]
—
1
(2σ2)D∕2
EN(5 ∣0,Iχ)
IEN (e ∣0,I)
[(fψ(x) - fψ(gθ(μφ (fψ(x) + σEx) + Cφ(y)E)))2]]
(58)
The overall procedure is therefore a straightforward modification of the standard VAE method Kingma
& Welling (2013) with additional learning the spread to maximize the divergence:
1.	Choose a noise distribution P(y|X)
2.	Choose a tractable family for the variational distribution, for example qφ(z∣y)
N (z∖μφ(y), Σφ(y)) and initialise φ.
3.	We then sample a yn for each datapoint (if we’re using S = 1 samples)
4.	If learning the spread noise:
(a)	Draw samples E to estimate - logpθ(yn) according to the corresponding bound.
(b)	Do a gradient ascent step in ψ.
5.	Draw samples E to estimate logpθ(yn) according to the corresponding bound.
6.	Do a gradient ascent step in (θ, φ).
7.	Go to 3 and repeat until convergence.
H MNIST Experiment
We first scaled the MNIST data to lie in [0, 1∖. We use Laplace spread noise σ = 0.3 and Gaussian
spread noise σ = 0.3 for the δVAE. Both encoder and decoder contains 3 feed-forward layers, each
layer with 400 units and ReLu activation function. The latent dimension is Z = 64. The variational
inference network qφ(z∣y) = N (z∣μg(y),σφIZ) has a similar structure for the mean network
μφ(y). For fixed spread δ-VAE , learning was done using the Adam Kingma & Ba (2014) optimizer
with learning rate 5e-4 for 200 epochs. For δ-VAE with learned spread (learned covariance), we
additioanly train the covariance for 2 epochs using Adam optimizer with learning rate 5e-5 after
everytime we train the model for 10 epochs.
I	CelebA Experiment
We pre-processed CelebA images by first taking 140x140 centre crops and then resizing to 64x64.
Pixel values were then rescaled to lie in [0, 1∖. For the learned spread we use Gaussian noise with
learned injective function ResNet fψ(∙) = I(∙) + gψ(∙), where gψ(∙) is a one layer convolutional
neural net with kernel size 3 × 3 and stride 1. We use spectral normalization Miyato et al. (2018)
to satisfy the Lipschitz constraint: we replace the weight matrix w of the convolution kernel by
WSN(w) ：= C X w∕σ(w) where σ(w) is the spectral norm of W and C ∈ (0,1). This guarantees that
fψ is invertible - see Behrmann et al. (2018).
The encoder and decoder are 4-layer convolutional neural net with batch norm (Ioffe & Szegedy
(2015)). Both encoder and decoder used fully convolutional architectures with 5x5 convolutional
18
Under review as a conference paper at ICLR 2020
filters and used vertical and horizontal strides 2 except the last deconvolution layer we used stride
1. The injective function f is a 1 layer convolutional network with 3*3 kernl and stride 1. Convk
stands for a convolution with k filters, DeConvk for a deconvolution with k filters, BN for the batch
normalization Ioffe & Szegedy (2015), Relu for the rectified linear units, and FCk for the fully
connected layer mapping to Rk .
X ∈ R64×64×3 → injeaivef (∙) ∈ r64×64×3
→ Conv128 → BN → Relu
→ Conv256 → BN → Relu
→ Conv512 → BN → Relu
→ Conv1024 → BN → Relu → FC100
z ∈ R100 → FC10×10×1024
→ DeConv512 → BN → Relu
→ DeConv256 → BN → Relu
→ DeConv128 → BN → RelU → DeConv3 → Sigmoid(∙)
→ injeCtivef (∙) ∈ r64×64×3
We Use batch size 100 and latent dimension zdim = 100 in all CelabA experiments. For the δ-VAE
with fixed spread, we Use the fixed GaUssian noise with 0 mean and (0.5)2I covariance. We train the
model for 500 epochs Using Adam optimizer with learning rate 1e-4. The learning rate decay with
scaling factor 0.9 every 100000 iterations.
For the δ-VAE with fixed spread We first train a δ-VAE with fixed f(x) = x and fixed GaUssian noise
with 0 mean and (0.5)2I diagonal covariance for 300 epochs, the learning rate decay with scaling
factor 0.9 every 100000. Then we start iterative training by doing one step inner maximisation over
the spread divergence’s parameter ψ Using Adam optimizer with learning rate 1e-5 and one step
minimization over the model parameter’s (θ, φ) Using Adam optimizer for additional 200 epochs.
We can share the first 300 epochs between two models. When we sample form two models, we first
sample from a 100 dimensional standard Gaussian distribution Z 〜N(0, I) and use the same latent
code z to get samples from both δ-VAE with fixed and learned spread, so we can easily compare the
sample quality between two models.
J	Woodberry
When evaluating the log probability of this Gaussian, we use the Woodberry identity
Σ-1 = (Iσ2)-1 - (Iσ2)-1L(I - LT (Iσ2)-1L)-1LT (Iσ2)-1
so we only have to invert a R × R matrix. A similar trick is applied to calculate the log determinant:
log det(Σ) = log det(I + LT (Iσ2)-1L) + 2D log σ.
The parameter L is trained using reparameterization trick. When sampling ξ 〜 N(μ, LLT + Iσ2),
We first sample Z ∈ RR from N (Z ∣0, I) and then sample noise e 〜N(0, Iσ2) , thus a sample from
N(μ, LLT + Iσ2) can be represented by ξ = Lz + Iσe + μ.
K Deterministic Linear Latent Model
Our aim here is to shoW hoW the classical deterministic PCA algorithm can be derived through a
maximum-likelihood approach, rather than the classical non-probabilistic least-squares derivation.
This is remarkable since the likelihood itself is not defined for this model.
For isotropic Gaussian observation noise With variance γ2, the Probabilistic PCA model (Tipping &
Bishop, 1999) for X-dimensional observations and Z-dimensional latent is
X = FZ + γe, Z 〜N(0,Iz),	C 〜N(0,Iχ),
Pθ(x) = N (y ∣0, FFT + Y2Ix)	(59)
19
Under review as a conference paper at ICLR 2020
When γ = 0, the generative mapping from z to x is deterministic and the model pθ (x) has support
only on a subset of RX and the data likelihood is in general not defined for Z < X .
In the following we consider general γ, later setting γ to zero throughout the calculation. To fit
the model to iid data {x1, . . . , xN} using maximum likelihood, the only information required from
the dataset is the data covariance Σ. For γ > 0, the maximum likelihood solution for PPCA is
F = UZ (ʌz - γ2Iz)2 R, where Λz, UZ are the Z largest eigenvalues, eigenvectors of Σ; R is an
arbitrary orthogonal matrix. Using spread noisep(y∣x) = N (y ∣x, σ2Iχ), the spreaded distribution
is a Gaussian pθ(y) = N (y ∣0, FFT + (Y2 + σ2)Iχ). Thus, pθ(y) is of the same form as PPCA,
albeit with an inflated covariance matrix. Adding Gaussian spread noise to the data also simply
inflates the sample covariance to Σ0 = Σ + σ2Iχ.
Since the eigenvalues of Σ0 ≡ Σ + σ2Iχ are simply ʌ = ʌ + σ2Iχ, with unchanged eigenvectors,
the optimal deterministic (Y = 0) latent linear model has solution F = UZ (ʌ" - σ2Iz)2 R =
1
UZ ʌz R.
We have thus recovered the standard PCA solution; however, the derivation is non-standard since the
likelihood of the deterministic latent linear model Y = 0 is not defined. Since classical deterministic
PCA cannot normally be described in terms of a likelihood, the usual derivation of PCA is to define it
as the optimal least squares reconstruction solution based on a linear projection to a lower-dimensional
subspace, see for example Barber (2012). Nevertheless, using the spread divergence, we learn a
sensible model and recover the true data generating process if the data were exactly generated
according to the deterministic model.
20
Under review as a conference paper at ICLR 2020
7S 夕夕 5f∙5f∙x
5JNJ，ag了？。
T37vu9se>mz)
"Q 6O6-Z>7O>
67be O3q ?30 LA
650ru?cm6√
J6Qg7G1gRC/
/ / / 3 9z⅛ 9 794
。二6√3 乂 S 4 7-7
/73 2 4 22G I 9
(a) Laplace with fixed covariance
234〃?/夕 5fS
J3 2 4u2g^l0
τ37r9ɑ，¥>?，
>Q534∕6 J 07
夕 7g 78G 74 夕 5
〜Sor49〃?，。
/B34761cy5
/，，3?它夕夕夕4
6Gllb//。
■ Γ>32,2QO7q
(b) Gaussian with fixed covariance
丁 y — lg，
73 TKQa 7si>j/
"qG5GI3 qo∕
ʃ，夕 INgS D-yog
6s 0λ40<b36<√
7 8 ɔ，76-- ΛQ y d√
//,CnOKPgO 夕 4
，^4Ga/6q7t\
%732/22g39
(c) Gaussian with learned covariance
Figure 4: Samples from a generative model (deterministic output) trained using δ-VAE with (a)
Laplace noise with fixed covariance, (a) Gaussian noise with fixed covariance and (c) Gaussian noise
with learned covariance.
21
Under review as a conference paper at ICLR 2020
(a) Fixed spread noise
(b) Learned spread noise
Figure 5: Samples from a generative model with deterministic output trained using δ-VAE with (a)
fixed and (b) learned spread with injective mean transform.
22