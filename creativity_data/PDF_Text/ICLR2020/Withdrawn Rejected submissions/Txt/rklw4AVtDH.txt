Under review as a conference paper at ICLR 2020
Optimistic Adaptive Acceleration for Opti-
MIZATION
Anonymous authors
Paper under double-blind review
Ab stract
This paper considers a new variant of AMSGrad called Optimistic-AMSGrad.
AMSGrad (Reddi et al. (2018)) is a popular adaptive gradient based optimization
algorithm that is widely used in training deep neural networks. The new variant
assumes that mini-batch gradients in consecutive iterations have some underlying
structure, which makes the gradients sequentially predictable. By exploiting the pre-
dictability and some ideas from Optimistic Online learning, the proposed algorithm
can accelerate the convergence and also enjoys a tighter regret bound. We evaluate
Optimistic-AMSGrad and AMSGrad in terms of various performance measures
(i.e., training loss, testing loss, and classification accuracy on training/testing data),
which demonstrate that Optimistic-AMSGrad improves AMSGrad.
1	Introduction
Nowadays deep learning has been very successful in numerous applications, from robotics
(e.g., Levine et al. (2017)), computer vision (e.g., He et al. (2016); Goodfellow et al. (2014)),
reinforcement learning (e.g., Mnih et al. (2013)), to natural language processing (e.g., Graves et al.
(2013)). A common goal in these applications is learning quickly. It becomes a desired goal due
to the presence of big data and/or the use of large neural nets. To accelerate the process, there are
variety of training algorithms proposed in recent years, such as AMS Grad (Reddi et al. (2018)),
ADAM (Kingma &Ba (2015)), RMSPROP (Tieleman & Hinton (2012)), ADADELTA (Zeiler
(2012)), and NADAM (Dozat (2016)), etc.
All the prevalent algorithms for training deep nets mentioned above combine two ideas: the idea
of adaptivity from AdaGrad (Duchi et al. (2011); McMahan & Streeter (2010)) and the idea of
momentum from Nesterov’ s Method (Nesterov (2004)) or Heavy ball method (Polyak (1964)).
AdaGrad is an online learning algorithm that works well compared to the standard online gradient
descent when the gradient is sparse. Its update has a notable feature: the effective learning rate is
different for each dimension, depending on the magnitude of gradient in each dimension, which
might help in exploiting the geometry of data and leading to a better update. On the other hand,
Nesterov’ s Method or Heavy ball Method (Polyak (1964)) is an accelerated optimization
algorithm whose update not only depends on the current iterate and current gradient but also depends
on the past gradients (i.e., momentum). State-of-the-art algorithms like AMSGrad (Reddi et al.
(2018)) and ADAM (Kingma & Ba (2015)) leverage the ideas to accelerate training neural nets.
In this paper, we propose an algorithm that goes further than the hybrid of the adaptivity and
momentum approach. Our algorithm is inspired by Optimistic Online learning (see e.g.
Chiang et al. (2012); Rakhlin & Sridharan (2013a;b); Syrgkanis et al. (2015); Abernethy et al.
(2018)). Optimistic Online learning considers that a good guess of the loss function in each
round is available and plays an action by utilizing the guess. By exploiting the guess, algorithms in
Optimistic Online learning can enjoy a smaller regret than the ones without exploiting the
guess. We combine the Optimistic Online learning idea with the adaptivity and the momentum
ideas to design a new algorithm — Optimistic-AMSGrad. We also provide a theoretical analysis
of Optimistic-AMSGrad. The proposed algorithm not only adapts to the informative dimensions,
exhibits momentum, but also exploits a good guess of the next gradient to facilitate acceleration.
We conduct experiments and show that Optimistic-AMSGrad improves AMSGrad in terms of
various measures: training loss, testing loss, and classification accuracy on training/testing data over
epochs.
1
Under review as a conference paper at ICLR 2020
2	Preliminaries
We begin by providing some background in online learning, as it will be the main tool to design
and analyze our proposed algorithm. We follow the notation in the literature of adaptive opti-
mization (Kingma & Ba (2015); Reddi et al. (2018)). For any vector u, V ∈ Rd, u/v represents
element-wise division, u2 represents element-wise square, √u represents element-wise square-root.
We denote g1:T [i] as the sum of the ith element of T vectors g1, g2, . . . , gT ∈ Rd.
2.1	A Brief Review of Online Learning and Optimistic Online Learning
The standard setup of online learning is that, in each round t, an online learner selects an action
Wt ∈ K ⊆ Rd, then the learner observes '(∙) and suffers loss 't(wt) after the learner commits the
action. The goal of the learner is minimizing the regret,
RegretT({wt}) := PT=1 't(Wt)- PT=1 't(w*),
which is the cumulative loss of the learner minus the cumulative loss of some benchmark w* ∈ K.
The idea of Optimistic Online learning (e.g., Chiang et al. (2012); Rakhlin & Sridharan
(2013a;b); Syrgkanis et al. (2015); Abernethy et al. (2018)) is as follows. Suppose that, in each
round t, the learner has a good guess mt(∙) of the loss function 't(∙) before playing an action wt.
Then, the learner should exploit the guess mt(∙) to choose an action Wt since mt(∙) is close to
the true loss function 't(∙). 1 For example, Syrgkanis et al. (2015) proposes an optimistic-variant
of FOLLOW-THE-REGULARIZED-LEADER (FTRL). FTRL (see e.g. Hazan (2016)) is an online
learning algorithm whose update is
Wt = argminw∈κhw, Lt-I + 1tR(w),
where η is a parameter, R(∙) is a 1-strongly convex function with respect to a norm (∣∣∙ ∣∣) on the
constraint set K, and Lt-1 := Pts-=11 gs is the cumulative sum of gradient vectors of the loss functions
O(√PT=1 kgtk*). On the
(i.e., gs := V's(ws)) up to but not including t. FTRL has regret at most
other hand, Optimistic-FTRL (Syrgkanis et al. (2015)) has the update
Wt = arg minw∈κ hw,Lt-i + m/ + 1 R(w),
where mt is the learner,s guess of the gradient vector gt := V't(Wt). Under the assumption that loss
functions are convex, the regret of OPTIMISTIC-FTRL is at most O( PtT=1 ∣gt - mt ∣*), which
can be much smaller than the regret of FTRL if mt is close to gt. Consequently, OPTIMISTIC-FTRL
can achieve better performance than FTRL. On the other hand, if mt is far from gt , then the regret of
Optimistic-FTRL would be only a constant factor worse than that of its non-optimistic counterpart.
In Section 4, we will provide a strategy to obtain mt . At the moment, we just would like to use
this example of FTRL to emphasize the importance of leveraging a good guess mt for updating Wt,
in order to achieve a faster convergence rate (or equivalently, small regret). We will have a similar
argument when we compare Optimistic-AMSGrad and AMSGrad.
2.2	Adam and AMSGrad
Adam (Kingma & Ba (2015)) is a popular algorithm for training deep nets. It combines the
momentum idea (Polyak (1964)) with the idea of AdaGrad (Duchi et al. (2011)), which has
effective different learning rates for different dimensions. The effective learning rate of AdaGrad
in iteration t for a dimension j is proportional to the inverse of ,∑S=ιgs [j]2, where gs[j] is the jth
element of the gradient vector gs in time s. This adaptive learning rate might help for accelerating
the convergence when the gradient vector is sparse (Duchi et al. (2011)). However, when applying
AdaGrad to train deep nets, it is observed that the learning rate might decay too fast (Kingma & Ba
(2015)). Therefore, Kingma & Ba (2015) propose using a moving average of gradients divided by the
square root of the second moment of the moving average (element-wise fashion), for updating the
model parameter W (i.e., lines 5,6 and 8 of Algorithm 1). Yet, ADAM (Kingma & Ba (2015)) fails at
1Imagine that if the learner would had been known't (∙) before committing its action, then it would exploit
the knowledge to determine its action and consequently minimizes the regret.
2
Under review as a conference paper at ICLR 2020
Algorithm 1 AMSGRAD (Reddi et al. (2018))
1:
2:
3:
4:
5:
6:
7:
8:
9:
Required: parameter β1 , β2 , and ηt .
Init: wι ∈ K ⊆ Rd and Vo = vo = el ∈ Rd.
for t = 1 to T do
Get mini-batch stochastic gradient vector gt at wt .
θt = β1θt-1 + (1 - β1)gt.
vt = β2vt-1 + (1 - β2)gt2.
Vt = max(Vt-ι, vt).
wt+ι = Wt — ηt √θt-. (element-wise division)
end for
some online convex optimization problems. AMSGrad (Reddi et al. (2018)) fixes the issue. The
algorithm of AM S Grad is shown in Algorithm 1. The difference between Adam and AM S Grad
lies on line 7 of Algorithm 1. ADAM does not have the max operation on line 7 (i.e., Vt = Vt for
Adam) while AMSGRAD adds the operation to guarantee a non-increasing learning rate, √t-, which
helps for the convergence (i.e., average regret RegTetT → 0). For the parameters of AMSGrad, it is
suggested that β1 = 0.9 and β2 = 0.99.
3	Optimistic-AMSGrad
Algorithm 2 OPTIMISTIC-AMSGRAD
1:	Required: parameter β1, β2, e, and ηt .
2:	Init: wι = w-ι∕2 ∈ K ⊆ Rd and Vo = vo = el ∈ Rd.
3:	for t = 1 to T do
4:	Get mini-batch stochastic gradient vector gt at wt .
5:	θt = β1θt-1 + (1 — β1)gt.
6:	Vt = β2Vt-1 + (1 — β2)(gt — mt)2.
7:	Vt = max(Vt-ι, vt).
8:	wt+ 2 =πK[wt- 2 - ηt √θtt ].
9:	wt+ι = ∏k [wt+1 — ηt+ι h√++1 ], where ht+ι := β1θt-1 + (1 — β1)mt+1
and mt+1 is the guess of gt+1.
10:	end for
We propose a new optimization algorithm, Optimistic-AMSGrad, shown in Algorithm 2. In
each iteration, the learner computes a gradient vector gt := V't(wt) at Wt (line 4), then it maintains
an exponential moving average of θt ∈ Rd (line 5) and Vt ∈ Rd (line 6), which is followed by the
max operation to obtain Vt ∈ Rd (line 7). The learner also updates an auxiliary variable Wt+1 ∈ K
(line 8). It uses the auxiliary variable to update and commit wt+1 (line 9), which exploits the guess
mt+ι of gt+ι to get wt+ι. As the learner,s action set is K ⊆ Rd, we adopt the notation ∏κ[∙] for the
projection to K if needed.
We see that Optimistic-AMSGrad has three properties:
• Adaptive learning rate of each dimension as ADAGRAD (Duchi et al. (2011)). (line 6, line 8
and line 9)
• Exponentially moving average of the past gradients as NESTEROV’ S METHOD (Nesterov
(2004)) and the Heavy-Ball method (Polyak (1964)). (line 5)
• Optimistic update that exploits a good guess of the next gradient vector as optimistic online
learning algorithms (e.g. Chiang et al. (2012); Rakhlin & Sridharan (2013a;b); Syrgkanis
et al. (2015)). (line 9)
The first property helps for acceleration when the gradient has a sparse structure. The second one
is from the well-recognized idea of momentum which can also help for acceleration. The last one,
perhaps less known outside the Online learning community, can actually lead to acceleration
when the prediction of the next gradient is good. This property will be elaborated in the following
subsection in which we provide the theoretical analysis of Optimistic-AMSGrad.
3
Under review as a conference paper at ICLR 2020
Observe that the proposed algorithm does not reduce to AMSGRAD when mt = 0. Furthermore, if
K = Rd (unconstrained case), one might want to combine line 8 and line 9 and get a single line as
wt+ι = Wt-1 - ηt √θt——ηt+ι h√+1. Yet, based on this expression, We see that wt+ι is updated from
Wt-1 instead of wt. Therefore, while OPTIMISTIC-AMSGRAD looks Iikejust doing an additional
update compared to AMSGrad, the difference of the updates is subtle. In the folloWing analysis, We
show that the interleaving actually leads to certain cancellation in the regret bound.
3.1 Theoretical analysis of Optimistic-AMSGrad
We provide the regret analysis here. To begin with, let us introduce some notations first. We
denote the Mahalanobis norm k ∙ ∣∣h :=，h, H•〉for some PSD matrix H. We let ψt(χ):=
hx, diag{Vt}1/2Xi for a PSD matrix H1/2 := diag{Vt}1/2, where diag{Vt} represents the diagonal
matrix whose ith diagonal element is Vt[i] in Algorithm 2. We define its corresponding Mahalanobis
norm ∣∣ ∙ ∣∣ψt :=，h, diag{Vt}1/2), where we slightly abuse the notation ψt to represent the PSD
matrix H1/2 := diag{Vt}1/2. Consequently, ψt(∙) is 1-strongly convex with respect to the norm
k ∙ kψt :=，《,diag{Vt}"∙>. Namely, ψt(∙) satisfies ψt(u) ≥ ψt(v) + hψt(v),u — Vi + 1 Ilu — v∣ψt
for any point u, v. A consequence of 1-strongly convexity of ψt(∙) is that Bψt (u, V) ≥ 2 ∣∣u - v∣ψt,
where the Bregman divergence Bψt (u, v) is defined as Bψt (u, v) := ψt(u) - ψt(v) - hψt(v), u - vi
with ψt(∙) as the distance generating function. We can also define the corresponding dual norm
k ∙ kψ: := ph∙,diag{vt}T2i.
We prove the following result regarding to the regret in the convex loss setting. The proof is available
in Appendix B. For simplicity, we analyze the case when β1 = 0. One might extend our analysis to
more general setting β1 = [0, 1).
Theorem 1. Let β1 = 0. Assume that K has bounded diameter D∞ 2. Suppose that the learner
incurs a sequence of convex loss functions {'t(∙)}.OPTIMISTIC-AMSGRAD (Algorithm 2) has
regret
RegretT ≤ 熹Do Pi=I V^+ B叱T/2 + PL η2t kgt - mtkψj,	⑴
where gt := ▽&(Wt) and ηmin := mint ηt. The result holds for any benchmark w* ∈ K and any
step size sequence {ηt}.
Corollary 1. Suppose that Vt is always monotone increasing (i.e., Vt = vt, ∀t). Then,
RegretT ≤ η1nD∞ Pd=ι{(1 - β2) PT=I βT-s(gs[i] - ms[i])2}1/2
+ Bψ1 (W；j)+ PT=I ηtkgt- mtkψt-1.
(2)
We should compare the bound of (2) 3 with that of AMSGrad (Reddi et al. (2018)), which is
RegretT ≤ 2η⅛)D∞ Pd=I VT[i]2 + D∞ PL Pd=1 2%(i-⅛)	⑶
+ (i-β1√i-γ√τ-β Pd=ι kg1:T[i]k2,
where the result was obtained by setting the step size η = η/ʌ/t. Notice that Vt in (3) is the one
in Algorithm 1 (AMSGRAD). For fair comparison, let us set η = η∕√t in (2) so that ηι = η and
ηmin = η/√T and also let us set βι = 0 in (3) so that their parameters have the same values as ours
in the analysis. By comparing the first term in (2) and (3), we clearly see that if gt and mt are close,
the first term in (2) would be smaller than 2然[-e])D∞ Pd=I VT [i]2 of (3).
2The boundedness assumption also appears in the previous works (Reddi et al. (2018); Kingma & Ba (2015)).
It seems to be necessary in regret analysis. If the boundedness assumption is lifted, then one might construct a
scenario such that the benchmark is w* = ∞ and the learner,s regret is infinite.
3The following conclusion in general holds for (1), when vt may not be monotone-increasing. For brevity,
we only consider the case that Vt = vt, as VT has a clean expression in this case.
4
Under review as a conference paper at ICLR 2020
Now let Us switch to the second term in (2) and (3), We see that Bψ (Wnl,w1/2) ` D∞ in (2), while 0
in (3). For the last term in (2), we have
PT=1 n2tkgt- mtkψι-1
XX ηt Ii	∣∣2	,	X (gT [i] - mτ [i])2
S 画kgt-mtkψ1-1+ητ3 Far-
T -1	d
X ηt kgt - mtkψι-ι+η X
t=1	i=1
_______________(gτ [i] — mτ [i])2
q((1 -β2) PtII βT-1-s(gs[i] - ms[i])2)
dτ
≤ηXX
^^_^^_(gt[i]—_mt[iD2^^_^^_
√t((1 — β2) PS=I β2-1-s(gs[i] — ms[i])2)
To interpret the boUnd, let Us make a roUgh approximation sUch that
t-1
X β2t-1-s (gs [i] — ms [i])2 ' (gt [i] — mt [i])2 .
s=1
We can then fUrther obtain an Upper-boUnd as
τ
X Tt kgt - mtkψl-ι /
dτ
M1 一 β
i=1 t=1
∣gt[i] - mt[i]∣
~√t-
≤ η√√+≡ X k(g - m)i：T[i]k2,
η
where the last ineqUality is dUe to CaUchy-Schwarz. The boUnd means that when gt and mt are
sUfficiently close, the last term in (2) is smaller than that in (3).
To conclUde, as the second term in (2) (which is approximately D∞) is likely to be dominated by the
other terms, the proposed algorithm improves AMSGRAD when the good gUess mt is available.
4 PREDICTING mt
From the analysis in the previoUs section, we know that whether Optimistic-AMSGrad converges
faster than its coUnterpart depends on how mt is chosen. In OPTIMISTIC-ONLINE LEARNING, mt
is UsUally set to mt = gt-1, i.e., Using the previoUs gradient as a gUess of the next one. The choice
can accelerate the convergence to an eqUilibriUm in some two-player zero-sUm games (Rakhlin &
Sridharan (2013a;b); Syrgkanis et al. (2015); Daskalakis et al. (2018)), in which each player Uses an
optimistic online learning algorithm against its opponent.
This paper is, however, aboUt solving optimization problems instead of solving zero-sUm games. We
propose to Use the extrapolation algorithm of (ScieUr et al. (2016)). Extrapolation stUdies estimating
the limit of seqUence Using the last few iterates (Brezinski & Zaglia (2013)). Some classical works
inclUde Anderson acceleration (Walker & Ni. (2011)), minimal polynomial extrapolation (Cabay &
Jackson (1976)), redUced rank extrapolation (Eddy (1979)). These methods typically assUme that the
seqUence {xt} ∈ Rd has a linear relation
Xt = A(xt-ι — x*) + x*,	(4)
and A ∈ Rd×d is an Unknown, not necessarily symmetric, matrix. The goal is to find the fixed point
of x*. Scieur et al. (2016) relaxes the assumption to certain degrees, by assuming that the sequence
{xt } ∈ Rd satisfies
xt — x* = A(xt-1 — x*) + et,	(5)
where et is a second order term satisfying ketk2 = O(kxt-1 — x* k22) and A ∈ Rd×d is an unknown
matrix. The extrapolation algorithm we used is shown in Algorithm 3. Some theoretical guarantees
regarding the distance between the output and x* are provided in (Scieur et al. (2016)).
5
Under review as a conference paper at ICLR 2020
Algorithm 3 REGULARIZED APPROXIMATE MINIMAL POLYNOMIAL EXTRAPOLATION
(RMPE)(ScieUr et al.(2016))
1:	Input: seqUence {xs ∈ Rd}s4 s==r0 , parameter λ > 0.
2:	CompUte matrix U = [x1 - x0, . . . , xr - xr-1] ∈ Rd×r.
3:	Obtain z by solving (U>U + λI)z = 1.
4:	Get c = z/(z> 1).
5:	Output: ∑r-01CiXi, the approximation of the fixed point x*.
For OPTIMISTIC-AMSGRAD, we Use Algorithm 3 to get mt+1. The following describes the
procedUre.
•	Call Algorithm 3 with inpUt being a seqUence of some past r + 1 iterates,
{wt, wt-1, wt-2, . . . , wt-r}, where r is a parameter.
•	Set W⅞+ι := ∑r-1CiWt-r+i from the output of Algorithm 3.
•	Output mt+1 := Vf (wt+ι).
That is, the latest r iterates are the input to Algorithm 3. The prediction of the gradient mt+1 is by
computing a mini-batch stochastic gradient at the output, namely at z^t+ι, of Algorithm 3.
We would like to emphasize that the choice of algorithm for gradient prediction is surely not unique.
We propose to use the recent result among various related works. Indeed, one can use any method
that can provide reasonable guess of the gradient in next iteration.
Remark: The work (Scieur et al. (2016)) leverages its extrapolation algorithm to post-process the
trajectory of gradient descent and obtains a point that is closer to an optimal point. In contrast, we use
the extrapolation algorithm on the fly to accelerate the convergence (of Optimistic-AMSGrad).
5	Experiments
Datasets and neural nets: The experiments were conducted on CIFAR10 and CIFAR100 datasets,
and a noisy variant of MNIST dataset (MNIST-back-Image (Larochelle et al. (2007)) 4). We train
Res-18 (He et al. (2016)) for CIFAR10 and CIFAR100 datasets and a four-layer convolutional neural
net 5 for the noisy MNIST dataset.
In all the experiments described in the following of this section, we use the following hyper-
parameters:
•	Step size η = 0.001.
•	β1 = 0.9, β2 = 0.99.
•	Number of training samples in each batch: batch_Size = 64.
•	(Optimistic-AMSGrad) Number of previous iterates stored for gradient prediction: r = 5
(i.e., r = 5 means the latest five iterates are stored for gradient prediction).
As these are classification tasks, we use the cross entropy loss for training the neural nets. For training
on CIFAR 10 and CIFAR 100, after getting the guess of the next iterate Wt+ι by the extrapolation
method, we construct the guess of the next gradient mt+1 by computing a mini-batch of stochastic
gradient of the negative log likelihood loss6 at Wt+ι (instead of the gradient of the cross entropy loss
at t+1 ). The slight modification leads to a better performance.
4MNIST-back-image takes random patches from a black and white as noisy background. The dataset has
12,000 training samples and 50,000 test samples.
5Specifically, we use a neural net model defined on the tutorial page https://github.com/pytorch/
examples/blob/master/mnist/main.py.
6Assume a K classification problem. Denote the values on the output layer yi ∈ RK for a sample i with true
label yi ∈ [K], its negative log likelihood loss is defined as —yi[yi]. See function nlLloss in PyTorch for details.
6
Under review as a conference paper at ICLR 2020
# Epochs
# Epochs
6 5 4 3
SSSS
SSCrl 6U=S81
# Epochs
# Epochs
# Epochs
SSerl 6≡ls①
# Epochs
(％) AOE-InOOV 6u - UQ-Il (％) A°BJn°°4 6U4S81
Figure 1: CIFAR 10 + Res-18. We compare Optimistic-AMS Grad with AMSGrad in terms of
training (cross-entropy) loss, training accuracy, testing loss, and testing accuracy. All measures are
plotted against the numbers of epochs. (One epoch means all training data points are used once). We
can see that Optimistic-AMSGrad noticeably improves AMSGrad in all four measures.
# Epochs
Figure 2: CIFAR 100 + Res-18. We compare Optimistic-AMS Grad with AM S Grad in terms of
training (cross-entropy) loss, training accuracy, testing loss, and testing accuracy.
Results: Figure 1 shows the result on CIFAR10+Res-18 and Figure 2 shows the result on
CIFAR100+Res-18. Figure 3 shows the result on MNIST-back-img dataset.7 From the results
7Note that our results on MNIST-back-image actually improved those reported in (Larochelle et al. (2007)),
which did not use convolutional nets. The test accuracy is now comparable to that reported in (Li (2010)) which
7
Under review as a conference paper at ICLR 2020
(％) Ao6u己史一
Figure 3: MNIST-back-image + CNN. We compare OPTIMISTIC-AMSGRAD with AMSGRAD in
terms of training (cross-entropy) loss, training accuracy, testing loss, and testing accuracy.
shown on the figures, it is clear that Optimistic-AMSGrad noticeably improves AMS Grad
in terms of the standard performance measures: training (cross entropy) loss, testing loss, training
classification accuracy, and testing classification accuracy. All results are plotted against the number
of training epochs. The results also suggest that Optimistic-AMSGrad finds a better point that
generalize well than AMSGrad. In Appendix D.2, we report Optimistic-AMS Grad with differ-
ent values of the parameters r . We find that the algorithm performance is not sensitive the choice of
r.
Comparisons with related works. In Appendix A, we provide a comprehensive survey of the
related works. There has been a trend in studying adaptive optimization methods from different
respects. We compare our contribution and some of the related works, in particular AO-FTRL (Mohri
& Yang (2016)) and Optimistic-Adam (Daskalakis et al. (2018)). Moreover, in Section D.1, we
provide the experimental results for the comparison to a modified version of Optimistic-Adam.
6	Conclusion
We propose Optimistic-AMSGrad that combines the ideas of optimistic online learning and
AMSGrad to accelerate optimization. For training deep neural networks, Optimistic-AMSGrad
significantly improves AM S Grad in terms of various performance measures in practice (e.g. training
loss, testing loss, and classification accuracy on training/testing data). Though we only provide the
theoretical analysis in the convex setting, the experiment in non-convex optimization shows some
promising results. The results seem to suggest that Optimistic-AMSGrad not only minimizes
the training loss faster but it can also find a point that generalizes better than the baselines. As the
success of Optimistic-AMS Grad relies on a good guess of the next gradient, future work includes
improving predicting gradients. Exploring the possibility of developing a new way to obtain a better
guess of the gradient would be an interesting direction. One possibility is by considering a very
recent work of (Dutta et al. (2019)) which proposes a new extrapolation algorithm.
developed the second-order tree-split formulation for boosted trees. Also see (Li (2018)) for comparisons with
new kernel methods.
8
Under review as a conference paper at ICLR 2020
References
Jacob Abernethy, Kevin A. Lai, Kfir Y. Levy, and Jun-Kun Wang. Faster rates for convex-concave
games. COLT, 2018.
Naman Agarwal, Brian Bullins, Xinyi Chen, Elad Hazan, Karan Singh, Cyril Zhang, and Yi Zhang.
Efficient full-matrix adaptive regularization. ICML, 2019.
Rohan Anil, Vineet Gupta, Tomer Koren, and Yoram Singer. Memory efficient adaptive optimization.
NeurIPS, 2019.
Gary Becigneul and Octavian-Eugen Ganea. Riemannian adaptive optimization methods. ICLR,
2019.
C. Brezinski and M. R. Zaglia. Extrapolation methods: theory and practice. Elsevier, 2013.
S. Cabay and L. Jackson. A polynomial extrapolation method for finding limits and antilimits of
vector sequences. SIAM Journal on Numerical Analysis, 1976.
Jinghui Chen and Quanquan Gu. Closing the generalization gap of adaptive gradient methods in
training deep neural networks. arXiv:1806.06763, 2018.
Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong. On the convergence of a class of adam-type
algorithms for non-convex optimization. ICLR, 2019a.
Zaiyi Chen, Zhuoning Yuan, Jinfeng Yi, Bowen Zhou, Enhong Chen, and Tianbao Yang. Universal
stagewise learning for non-convex problems with convergence on averaged solutions. ICLR, 2019b.
Chao-Kai Chiang, Tianbao Yang, Chia-Jung Lee, Mehrdad Mahdavi, Chi-Jen Lu, Rong Jin, and
Shenghuo Zhu. Online optimization with gradual variations. COLT, 2012.
Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Training gans with
optimism. ICLR, 2018.
Timothy Dozat. Incorporating nesterov momentum into adam. ICLR (Workshop Track), 2016.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research (JMLR), 2011.
Aritra Dutta, El Houcine Bergou, Yunming Xiao, Marco Canini, and Peter Richtarik. Direct nonlinear
acceleration. arXiv:1905.11692, 2019.
R. Eddy. Extrapolating to the limit of a vector sequence. Information linkage between applied
mathematics and industry, Elsevier, 1979.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. NIPS, 2014.
Alex Graves, Abdel rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recurrent
neural networks. ICASSP, 2013.
Vineet Gupta, Tomer Koren, and Yoram Singer. Shampoo: Preconditioned stochastic tensor optimiza-
tion. ICML, 2018.
Elad Hazan. Introduction to online convex optimization. Foundations and Trends in Optimization,
2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. CVPR, 2016.
Nitish Shirish Keskar and Richard Socher. Improving generalization performance by switching from
adam to sgd. arXiv:1712.07628, 2017.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. ICLR, 2015.
9
Under review as a conference paper at ICLR 2020
Hugo Larochelle, Dumitru Erhan, Aaron Courville, James Bergstra, and Yoshua Bengio. An empirical
evaluation of deep architectures on problems with many factors of variation. ICML, 2007.
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep
visuomotor policies. NIPS, 2017.
Ping Li. Robust logitboost and adaptive base class (abc) logitboost. UAI, 2010.
Ping Li. Several tunable GMM kernels. arXiv:1805.02830, 2018.
Xiaoyu Li and Francesco Orabona. On the convergence of stochastic gradient descent with adaptive
stepsizes. AISTAT, 2019.
Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei
Han. On the variance of the adaptive learning rate and beyond. arXiv:1908.03265, 2019.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. ICLR, 2019.
Liangchen Luo, Yuanhao Xiong, Yan Liu, and Xu Sun. Adaptive gradient methods with dynamic
bound of learning rate. ICLR, 2019.
James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate
curvature. ICML, 2015.
H. Brendan McMahan and Matthew J. Streeter. Adaptive bound optimization for online convex
optimization. COLT, 2010.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan
Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. NIPS (Deep
Learning Workshop), 2013.
Mehryar Mohri and Scott Yang. Accelerating optimization via adaptive prediction. AISTATS, 2016.
Yurii Nesterov. Introductory lectures on convex optimization: A basic course. Springer, 2004.
B. T. Polyak. Some methods of speeding up the convergence of iteration methods. Mathematics and
Mathematical Physics, 1964.
Alexander Rakhlin and Karthik Sridharan. Optimization, learning, and games with predictable
sequences. NIPS, 2013a.
Alexander Rakhlin and Karthik Sridharan. Online learning with predictable sequence. COLT, 2013b.
Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. ICLR,
2018.
Damien Scieur, Alexandre d’Aspremont, and Francis Bach. Regularized nonlinear acceleration.
NIPS, 2016.
Matthew Staib, Sashank J. Reddi, Satyen Kale, Sanjiv Kumar, and Suvrit Sra. Escaping saddle points
with adaptive gradient methods. ICML, 2019.
Vasilis Syrgkanis, Alekh Agarwal, Haipeng Luo, and Robert E. Schapire. Fast convergence of
regularized learning in games. NIPS, 2015.
T. Tieleman and G. Hinton. Rmsprop: Divide the gradient by a running average of its recent
magnitude. COURSERA: Neural Networks for Machine Learning, 2012.
Paul Tseng. On accelerated proximal gradient methods for convex-concave optimization. 2008.
H. F. Walker and P. Ni. Anderson acceleration for fixed-point iterations. SIAM Journal on Numerical
Analysis, 2011.
Rachel Ward, Xiaoxia Wu, and Leon Bottou. Adagrad stepsizes: Sharp convergence over nonconvex
landscapes, from any initialization. ICML, 2019.
10
Under review as a conference paper at ICLR 2020
Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal
value of adaptive gradient methods in machine learning. NIPS, 2017.
Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, and Sanjiv Kumar. Adaptive methods
for nonconvex optimization. NeurIPS, 2018.
Matthew D. Zeiler. Adadelta: An adaptive learning rate method. arXiv:1212.5701, 2012.
Dongruo Zhou, Yiqi Tang, Ziyan Yang, Yuan Cao, and Quanquan Gu. On the convergence of adaptive
gradient methods for nonconvex optimization. arXiv:1808.05671, 2018.
Zhiming Zhou, Qingru Zhang, Guansong Lu, Hongwei Wang, Weinan Zhang, and Yong Yu. Adashift:
Decorrelation and convergence of adaptive learning rate methods. ICLR, 2019.
Fangyu Zou and Li Shen. On the convergence of adagrad with momentum for training deep neural
networks. arXiv:1808.03408, 2018.
A Comparison to Related Works
A. 1 Comparison to some non-convex optimization works
Recently, Zaheer et al. (2018); Chen et al. (2019a); Ward et al. (2019); Zhou et al. (2018); Zou &
Shen (2018); Li & Orabona. (2019) provide some theoretical analysis of Adam-type algorithms
when applying them to smooth nonconvex optimization problems. For example, Chen et al.(2019a)
provide a bound, which is mint∈[τ] E[kVf (Wt)Il2] = O(logT∕√T). Yet, this data independent
bound does not show an advantage over standard stochastic gradient descent. Similar concerns appear
in other papers.
To obtain some adaptive data dependent bound (e.g., bounds like (2) or (3) that are in terms of the
gradient norms observed along the trajectory) when applying Optimistic-AMSGrad to nonconvex
optimization, one can follow the approach of (Agarwal et al. (2019)) or (Chen et al. (2019b)). They
provide ways to convert algorithms with adaptive data dependent regret bound for convex loss
functions (e.g., AdaGrad) to the ones that can find an approximate stationary point of non-convex
loss functions. Their approaches are modular so that simply using Optimistic-AMSGrad as the
base algorithm in their methods will immediately lead to a variant of Optimistic-AMSGrad that
enjoys some guarantee on nonconvex optimization. The variant can outperform the ones instantiated
by other ADAM-type algorithms when the gradient prediction mt is close to gt . We omit the details
since this is a straightforward application.
A.2 Comparison to Mohri & Yang (2016)
Mohri & Yang (2016) proposes AO-FTRL, which has the update of the form wt+1 =
argminw∈κ(PS=ι gs)>w + m>+ιw + ro：t(w), where ro：t《) is a 1-strongly convex loss function
with respect to some norm ∣∣ ∙ ||(t)that may be different for different iteration t. Data dependent
regret bound was provided in the paper, which is r0:T(W*) + PT=I kgt —mt k (t)* for any benchmark
w* ∈ K. We see that if one selects ro：t(w) :=〈w, diag{Vt}1/2Wi and ∣∣ ∙ k(t) := Ph∙, diag{Vt}1∕2∙),
then the update might be viewed as an optimistic variant of AdaGrad. However, no experiments
was provided in (Mohri & Yang (2016)).
A.3 Comparison to Optimistic-Adam of (Daskalakis et al. (2018))
We are aware that Daskalakis et al. (2018) proposed one version of optimistic algorithm for ADAM,
which is called Optimistic-Adam in their paper. We want to emphasize that the goals are different.
Optimistic-Adam in their paper is designed to optimize two-player games (e.g., GANs (Goodfellow
et al. (2014))), while the proposed algorithm in this paper is designed to accelerate optimization
(e.g., solving empirical risk minimization quickly). Daskalakis et al. (2018) focused on training
GANs (Goodfellow et al. (2014)). GANs is a two-player zero-sum game. There have been some
related works in Optimistic Online learning like (Chiang et al. (2012); Rakhlin & Sridharan
11
Under review as a conference paper at ICLR 2020
Algorithm 4 OPTIMISTIC-ADAM (Daskalakis et al. (2018))
1:	Required: parameter β1 , β2 , and ηt .
2:	Init: w1 ∈ K.
3:	for t = 1 to T do
4:	Get mini-batch stochastic gradient vector gt ∈ Rd at wt .
5:	θt = β1θt-1 +(1 -β1)gt.
6:	vt = β2vt-1 + (1 - β2)gt2 .
7:	wt+1 = πk[wt - 2ηt√θvt + η √-1i].
8:	end for
(2013a;b); Syrgkanis et al. (2015)) showing that if both players use some kinds of Optimistic-
update, then accelerating the convergence to the equilibrium of the game is possible. Daskalakis
et al. (2018) were inspired by these related works and showed that Optimistic-Mirror-Descent
can avoid the cycle behavior in a bilinear zero-sum game, which accelerates the convergence.
Furthermore, Daskalakis et al. (2018) did not provide theoretical analysis of Optimistic-Adam
while we give some analysis for the proposed algorithm.
For comparison, we replicate Optimistic-Adam in Algorithm 4. Optimistic-Adam in Algo-
rithm 4 uses the previous gradient as the guess of the next gradient. Yet, the update cannot be written
into the same form as our Optimistic-AMSGrad (and vise versa). Optimistic-AMSGrad
(Algorithm 2) actually uses two interleaving sequences of updates {wt}T=ι, {wt-1 }T=ι. The design
and motivation of both algorithms are different.
A.4 Other works about adaptive gradient methods
There has been a spate of research in improving adaptive gradient methods from different respects.
Anil et al. (2019) develop a method to reduce memory overheads in adaptive gradient methods like
ADAGRAD and ADAM. Zhou et al. (2019) propose decorrelation between the second moment term vt
and the gradient gt by temporal shifting to deal with the non-convergence issue of ADAM. Luo et al.
(2019) show that an extreme effective learning rate might happen during the execution of Adam,
which can cause the non-convergence. They propose an operation to clip the effective learning rate
that avoids the extreme learning rate. Gupta et al. (2018) propose a new adaptive gradient method
by designing a preconditioned matrix for the update. Liu et al. (2019) study a heuristic called the
learning rate “warm-up” and propose a new variant of Adam by including a variance rectification
term. Becigneul & Ganea (2019) propose a counterpart of Adam for Riemannian manifolds. Other
directions include improving the generalization of adaptive gradient methods (e.g. Loshchilov &
Hutter (2019); Chen & Gu (2018); Keskar & Socher. (2017); Luo et al. (2019)), comparing adaptive
gradient methods and standard SGD with momentum (e.g. Wilson et al. (2017); Loshchilov & Hutter
(2019)), or showing that an adaptive optimization method can escape saddle points (Staib et al.
(2019)).
B Proof of Theorem 1
We provide the regret analysis here. To begin with, let US introduce some notations first. We
denote the Mahalanobis norm k ∙ ∣∣h =，h,H•〉for some PSD matrix H. We let ψt(χ):=
hx, diag{Vt}1/2Xi for a PSD matrix H1/2 := diag{Vt}1/2, where diag{Vt} represents the diagonal
matrix whose ith diagonal element is Vt[i] in Algorithm 2. We define its corresponding Mahalanobis
norm ∣∣ ∙ |必 :=，h, diag{Vt}1/2), where we abuse the notation ψt to represent the PSD matrix
H1/2 := diag{Vt}1/2. Consequently, ψt(∙) is 1-strongly convex with respect to the norm ∣∣ ∙ ∣∣ψt :=
√(∙, diag{Vt}1/2∙i. Namely, ψt(∙) satisfies ψt(U) ≥ ψt(v) + hψt(v),u — Vi + 1 Ilu — v∣ψt for any
point u, v. A consequence of 1-strongly convexity of ψt(∙) is that Bψt (u,v) ≥ 2∣∣u — v∣ψt, where
the Bregman divergence Bψt (u, v) is defined as Bψt (u, v) := ψt(u) — ψt(v) — hψt(v), u — vi and
ψt(∙) serves as the distance generating function of the Bregman divergence. We can also define the
the corresponding dual norm ∣∣ ∙ ∣∣ψ* :=，h, diag{Vt}-V2∙i.
12
Under review as a conference paper at ICLR 2020
Proof. [of Theorem 1] By regret decomposition, we have that
TT
RegretT :=	`t (wt) - min	`t(w)
w∈K
t=1	t=1
≤ PT=Ihwt- W*, v`t(wt)i
=PT=Ihwt - wt+1 ,gt - mti + hwt - wt+1 ,mti + hwt+ 2 - w*,gti,
(6)
where we denote gt := v`t(wt).
Recall the notation ψt(x) and the Bregman divergence Bψt (u, v) we defined in the beginning of this
section. For β1 = 0, we can rewrite the update on line 8 of (Algorithm 2) as
wt+1 =argminw∈κ ηthw,gti + Bψt (w,wt-I),	(7)
and rewrite the update on line 9 of (Algorithm 2) as
wt+1 = argminw∈κ ηt+1hw,mt+1i + Bψt (w,wt+1).	(8)
Now we are going to exploit a useful inequality (which appears in e.g., Tseng (2008)); for any update
of the form w = arg minw∈κ hw, θ) + Bψ (w, v), it holds that
hw 一 u,θi ≤ Bψ (u,v) - Bψ (u, w) - Bψ (w,v),	(9)
for any u ∈ K. By using (9) for (8), we have
hwt - wt+1 ,mti ≤ η(Bψt-1 (wt+2,wt- 1) - Bψt-1 (wt+1 ,wt) - Bψt-1 (Wt,wt-2)),	(IO)
and, by using (9) for (7), we have
hwt+2 - w*,gti ≤ ηIBψt(w*,wt-2) - Bψt(w*,wt+2) - Bψt(wt+1 ,wt- 1)).	(II)
So,	by (6), (10), and (11), we obtain
(6)	T
Regretr ≤ Et=Ihwt - wt+ 1 ,gt - mti + hwt - wt+ 2 ,mti + hwt+ 1 - w*,gti
(10),(11)	T
≤	Et=I kwt - wt+ 1 kψt-1 kgt - mtk∙ψ-1	(12)
+ η1t (bψ-i (wt+2,wt-2) - bψ-i (wt+2,wt) - bψ-i (wt,wt-2)
+ Bψt(w*,wt- 1) - Bψt(w*,wt+1) - Bψt(wt+ 2,wt-2)),
which is further bounded by
≤ PT=ι{2⅛;kwt - wt+2kψt-1 + ηtkgt - mtkψ*-1 + η1t(Bψt-1 (wt+ 2,wt-2) - 1 kwt+1 - wtkψt-1
- Bψt-1 (wt,wt-2) + Bψt(w*,wt-2) - Bψt(w*,wt+2) - Bψt(wt+ 2,wt-2))}
≤ Pt=ι{η kgt - mtkψ*-1 + * (Bψt (w*,wt- 1) - Bψt(w*,wt+ 2)
+ Bψt-1 (wt+ 1, wt-2) - Bψt (wt+ 1, wt- 1))},
(13)
Where (a) is because kwt - wt+1 kψt-1 kgt - mtkψL = infβ>o 泰 kwt - wt+1 kψt-1 + 2kgt -
mtkψ* ɪ by Young,s inequality and that ψt-ι(∙) is I-Strongly convex with respect to k ∙ kψt-1.
To proceed, notice that
Bψt+1 (w*,wt+ 1 ) - Bψt (w*,wt+ 2 ) = hw* - wt+ 1 , diag(v1+1 - v1/2 )(w* - wt+ 1 )i
≤ (maxi(w*[i] - wt+ 1 [i])2) ∙ (Pd=1 v1+1 [i] - 林/2[i])
13
Under review as a conference paper at ICLR 2020
and
Bψt-1 (Wt+ 2 ,wt-2 ) - Bψt (Wt+1 ,wt-1 )
=hwt+1 - Wt-1, diag(VI-I - v1/2 )(wt+ 2 - Wt-2 )i ≤ 0,
as the sequence {vt} is non-decreasing. Therefore,
(15)
(13),(14),(15)
RegretT	≤
η⅛ d∞ Pd=ι vT/2[i] + Bψ % j) + PT=ι 对 gt - mtkψj∙
□
C Discussion of iteration cost of Optimistic-AMS Grad
We observe that the iteration cost (i.e., actual running time per iteration) of our implementation of
Optimistic-AMSGrad is roughly two times larger than the standard AMSGrad in the empirical
minimization task. Here, we report the breakdown analysis for the computational overhead. The
overhead mostly comes from the extrapolation step. Specifically, the extrapolation step consists of:
(a) The step of constructing the linear system (U>U). The cost of this step can be optimized and
reduced to r × d, since the matrix U only changes one column at a time. (b) The step of solving the
linear system. The cost of this step is O(r3), which is negligible as the linear system is very small
(5-by-5 if r = 5). (c) The step that outputs an estimated gradient as a weighted average of previous
gradients. The cost of this step is r × d. So, the computational overhead is 2rd + r3 . Yet, we notice
that step (a) and (c) is parallelizable.
Memory usage: Our algorithm needs a storage of past r gradients to get an estimated gradient.
Though it seems quite demanding compared to the standard AMSGrad, it is relatively cheap compared
to Natural gradient method (e.g., Martens & Grosse (2015)), as Natural gradient method needs to
store some matrix inverse.
14
Under review as a conference paper at ICLR 2020
D	More Experiments
D.1 A comparison with modified Optimistic-Adam (Daskalakis et al. (20 1 8))
Algorithm 5 OPTIMISTIC-ADAM+Vt.
Required: parameter β1 , β2 , and ηt.
Init: wι ∈ K and Vo = vo = el ∈ Rd.
1:
2:
3:
4:
5:
6:
7:
8:
for t = 1 to T do
Get mini-batch stochastic gradient vector gt ∈ Rd at wt .
θt = β1θt-1 + (1 - β1)gt.
vt = β2vt-1 + (1 - β2)gt2.
Vt = max(^t-ι, Vt).
wt+1 = πk[wt - 2ηt√θ^= + n √θt--].
Vvt	√vt-ι
9
end for
Here we also compare Optimistic-AMSGrad with another baseline, which we called Optimistic-
ADAM+Vt as shown in Algorithm 5. OPTIMISTIC-ADAM+Vt is OPTIMISTIC-ADAM (Algorithm 4)
of (Daskalakis et al. (2018)) with the additional max operation Vt = max(^t-ι, Vt) to guarantee that
the weighted second moment is monotone increasing. Figure 4, 5, and 6 show the results. We observe
that our method dominates the other two methods.
# Epochs
# Epochs
.6.5/.3
SSSS
SSol 6un
Figure 4: CIFAR 10 + Res-18. We compare three methods: Optimistic-AMSGrad, AMSGrad,
and OPTIMISTIC-ADAM+^t, in terms of training (cross-entropy) loss, training accuracy, testing
loss, and testing accuracy. We observe that Optimistic-AMSGrad consistently improves the two
baselines.
# Epochs
15
Under review as a conference paper at ICLR 2020
2 8∙64 2 1 8
1111 O
Sso, 6u_u_b」j_
2
CIFAR100+Res-18
一AMSGrad
-OPT-AMSGrad
——OPT-Adam+6
Train Loss
# Epochs
20	40	60	80	100
# Epochs
OQ 6 / 2
Sso, 6u=sθl
20	40	60	80	100
# Epochs
# Epochs
Figure 5: CIFAR 100 + Res-18. We compare three methods: Optimistic-AMSGrad, AMSGrad,
and OPTIMISTIC-ADAM+Vt, in terms of training (cross-entropy) loss, training accuracy, testing loss,
and testing accuracy.
# Epochs
Figure 6: MNIST-back-image noisy dataset + a four-layer convolutional neural network.
(％) A°BJn°°4 6—更
a94
9 92
O 90
O
< 88
W 86
零84
8 82
80
1 23456789 10
# Epochs
16
Under review as a conference paper at ICLR 2020
D.2 CHOICE OF DIFFERENT r VALUES
Recall that our proposed algorithm has the parameter r in addition to the step size η that governs
the use of past information. Figure 7, 8, and 9 compare the performance under different values or r,
r = 3, 5, 10. From the result we see that the choice of r does not have significant impact on learning
performance. Taking consideration both quality of gradient prediction and computational issues, it
appears that r = 5 is a good choice, although the results for r = 3 do not differ much.
1
CIFAR10+Res-18
0.8
0.6
0.4
0.2
——AMSGrad
—OPT-AMSGrad r=3
——OPT-AMSGrad r=5
OPT-AMSGrad r=10
S
S
O
6
'E
Train Loss
20
40
60
80	100
0* l * * * * 6
1
»95
o
号90
O
< 85
6
-≡ 80
'ω
二 75
70
CIFAR10+Res-18
Train Accuracy
——AMSGrad
——OPT-AMSGrad r=3
—OPT-AMSGrad r=5
OPT-AMSGrad r=10
1	20	40	60	80	100
# Epochs
6 5 4 3
SSSS
SSOl 6①一
# Epochs
7
A
O
F
=5
O
O
<
6
.≡
ω
φ
# Epochs
1	20	40	60	80	100
# Epochs
Figure 7: CIFAR 10 + Res-18. We compare OPTIMISTIC-AMSGRAD (for = 3,5, 10)withAMS-
Grad in terms of training (cross-entropy) loss, training accuracy, testing loss, and testing accuracy.
The choice of r does not have significant impact on learning performance. While it appears that
r = 5 is a good choice, the results for r = 3 do not differ much.
17
Under review as a conference paper at ICLR 2020
2 8∙64 2 1 8
1111 O
sso, 6u_u_b」j_
# Epochs
75
Vo
65 65
O
,60
<
6
^cB
I-
55
50
45
40 L
1
CIFAR100+Res-18
——AMSGraxl
—OPT-AMSGrad r=3
——OPT-AMSGrad r=5
OPT-AMSGrad r=10
Train Accuracy
20	40	60	80	100
# Epochs
2 0q-64 2
SS0, 6U=Sə1
# Epochs
70
A
O
n
o
O
<
CD
⊂
ω
ω
I-
40
CIFAR100+Res-1
65
60
55
50
45
Test Accuracy
1
一AMSGrad
—OPT-AMSGrad r=3
-OPT-AMSGrad r=5
OPT-AMSGrad r=10
20	40	60	80	100
# Epochs
Figure 8: CIFAR 100 + Res-18. We compare OPTIMISTIC-AMSGRAD (for = 3, 5, 10) with
AMS Grad in terms of training (cross-entropy) loss, training accuracy, testing loss, and testing
accuracy. Again, the choice of r does not affect the results too much.
123456789 10
MNIST-Image + CNN
——AMSGrad
——OPT-AMSGrad r=3
——OPT-AMSGrad r=5
OPT-AMSGrad r=10
Train Accuracy
23456789 10
# Epochs
# Epochs
(％) Aoou-lsəl
# Epochs
642086420
999988888
MNIST-Image + CNN
Test Accuracy
O
9
8
7
6
# Epochs
(％)ɔBJn°°< 6U_U®_L
5 O
9 9
2
4
Figure 9: MNIST-back-image noisy dataset + a four-layer convolutional neural network.
18