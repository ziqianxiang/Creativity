Under review as a conference paper at ICLR 2020
Semi-supervised Autoencoding Projective De-
pendency Parsing
Anonymous authors
Paper under double-blind review
Ab stract
We describe two end-to-end autoencoding models for semi-supervised graph-
based dependency parsing. The first model is a Local Autoencoding Parser (LAP)
encoding the input using continuous latent variables in a sequential manner; The
second model is a Global Autoencoding Parser (GAP) encoding the input into de-
pendency trees as latent variables, with exact inference. Both models consist of
two parts: an encoder enhanced by deep neural networks (DNN) that can utilize
the contextual information to encode the input into latent variables, and a decoder
which is a generative model able to reconstruct the input. Both LAP and GAP ad-
mit a unified structure with different loss functions for labeled and unlabeled data
with shared parameters. We conducted experiments on WSJ and UD dependency
parsing data sets, showing that our models can exploit the unlabeled data to boost
the performance given a limited amount of labeled data.
1 Introduction
Dependency parsing captures bi-lexical relationships by constructing directional arcs between
words, defining a head-modifier syntactic structure for sentences, as shown in Figure 1. Depen-
dency trees are fundamental for many downstream tasks such as semantic parsing (Reddy et al.,
2016; Marcheggiani & Titov, 2017), machine translation (Bastings et al., 2017; Ding & Palmer,
2007), information extraction (Culotta & Sorensen, 2004; Liu et al., 2015) and question answering
(Cui et al., 2005). As a result, efficient parsers (Kiperwasser & Goldberg, 2016; Dozat & Manning,
2017; Dozat et al., 2017; Ma et al., 2018) have been developed using various neural architectures.
My dog also likes eating sausage
Figure 1: A dependency tree: directional arcs represent head-modifier relation between words.
While supervised approaches have been very successful, they require large amounts of labeled data,
particularly when neural architectures are used. Syntactic annotation is notoriously difficult and re-
quires specialized linguistic expertise, posing a serious challenge for low-resource languages. Semi-
supervised parsing aims to alleviate this problem by combining a small amount of labeled data and a
large amount of unlabeled data, to improve parsing performance over labeled data alone. Traditional
semi-supervised parsers use unlabeled data to generate additional features, assisting the learning pro-
cess (Koo et al., 2008), together with different variants of self-training (S0gaard & Rish0j, 2010).
However, these approaches are usually pipe-lined and error-propagation may occur.
In this paper, we propose two end-to-end semi-supervised parsers based on probabilistic autoencoder
models illustrated in Figure 3, Locally Autoencoding Parser (LAP) and Globally Autoencoding
Parser (GAP). In LAP, continuous latent variables are used to support tree inference by providing
a better representation, while in GAP, the latent information forms a probability distribution over
1
Under review as a conference paper at ICLR 2020
dependency trees corresponding to the input sentence. A similar idea has been proposed by Corro &
Titov (2018), but our GAP model differs fundamentally from their parser, as GAP does not sample
from the posterior of the latent tree structure to approximate the Evidence Lower Bound (ELBO).
Instead it relies on a tractable algorithm to directly compute the posterior to calculate the ELBO.
We summarize our contributions as follows:
1.	We proposed two autoencoding parsers for semi-supervised dependency parsing, with comple-
mentary strengths, trading off speed vs. accuracy;
2.	We propose a tractable inference algorithm to compute the expectation and marginalization of
the latent dependency tree posterior analytically for GAP, avoiding sampling from the posterior
to approximate the expectation (Corro & Titov, 2018);
3.	We show improved performance of both LAP and GAP with unlabeled data on WSJ and UD data
sets empirically, and improved results of GAP comparing to a recently proposed semi-supervised
parser (Corro & Titov, 2018).
2	Related Work
Most dependency parsing studies fall into two major groups: graph-based and transition-based
(Kubler et al., 2009). Graph-based parsers (McDonald, 2006) regard parsing as a structured pre-
diction problem to find the most probable tree, while transition-based parsers (Nivre, 2004; 2008)
treat parsing as a sequence of actions at different stages leading to a dependency tree.
While earlier works relied on manual feature engineering, in recent years the hand-crafted features
were replaced by embeddings and deep neural architectures, leading to improved performance in
both graph-based parsing (Nivre, 2014; Pei et al., 2015) and transition-based parsing (Chen & Man-
ning, 2014; Dyer et al., 2015; Weiss et al., 2015). More recent works rely on neural architectures
for learning a representation for scoring structural decisions Andor et al. (2016); Kiperwasser &
Goldberg (2016); Wiseman & Rush (2016).
The annotation difficulty for this task, has also motivated work on unsupervised (grammar induction)
and semi-supervised approaches to parsing (Tu & Honavar, 2012; Jiang et al., 2016; Koo et al., 2008;
Li et al., 2014; Kiperwasser & Goldberg, 2015; Cai et al., 2017; Corro & Titov, 2018).
Similar to other structured prediction tasks, directly optimizing the objective is difficult when the
underlying probabilistic model requires marginalizing over the dependency trees. Variational ap-
proaches are a natural way for alleviating this problem, as they try to improve the lower bound of the
original objective, and were applied in several recent NLP works (Stratos, 2019; Kim et al., 2019b;
Chen et al., 2018; Kim et al., 2019b;a). Variational Autoencoder (VAE) (Kingma & Welling, 2014)
is particularly useful for latent representation learning, and is studied in semi-supervised context as
the Conditional VAE (CVAE) (Sohn et al., 2015).
The work mostly related to ours is (Corro & Titov, 2018) as they consider the dependency tree as
the latent variable, but their work takes a second approximation to the variational lower bound by
an extra step to sample from the latent dependency tree, without identifying a tractable inference.
We show that with the given structure, exact inference on the lower bound is achievable without
approximation by sampling, which tightens the lower bound.
3	Graph-based Dependency Parsing
A dependency graph of a sentence can be regarded as a directed tree spanning all the words of the
sentence, including a special “word”-the ROOT-to originate out. Assuming a sentence length of l, a
dependency tree can be denoted as T = (< h1, m1 >, . . . , < hl-1, ml-1 >), where ht is the index
in the sequence of the head word of the dependency connecting the tth word mt as a modifier.
Our graph-based parser is constructed by following the standard structured prediction paradigm
(McDonald et al., 2005; Taskar et al., 2005). In inference, based on the parameterized scoring
function SΛ with parameter Λ, the parsing problem is formulated as finding the most probable
directed spanning tree for a given sentence x:
T* = arg max Sλ(x, T),
T ∈t
2
Under review as a conference paper at ICLR 2020
®t ② t@
x
售 QAom
x
⑶ LAP	(b) GAP
Figure 3: Illustration of two different parsers. (a)
LAP uses continuous latent variable to form the
dependency tree (b) GAP treats the dependency
tree as the latent variable.
Figure 4: In this illustration of the arc
scoring matrix, each entry represents
the (h(head) → m(modif ier)) score.
where T* is the highest scoring parse tree and T is the set of all valid trees for the sentence x.
It is common to factorize the score of the entire graph into the summation of its substructures: the
individual arc scores (McDonald et al., 2005):
l
Sλ(x, T)=	E	SΛ(h, m) = E SΛ(ht, mt),
. .~
(h,m)∈T
t=1
where T represents the candidate parse tree, and sλ is a function scoring each individual arc.
sΛ(h, m) describes the likelihood of forming an arc from the head h to its modifier m in the tree.
Through out this paper, the scoring is based on individual arcs, as we focus on first order parsing.
3.1	Scoring Function Using Neural Architecture
We used the same neural architecture as that in Kiperwasser & Goldberg (2016)’s study. We first use
a bi-LSTM model to take as input ut = [pt ; et] at position t to incorporate contextual information,
by feeding the word embedding et concatenated with the POS (part of speech) tag embeddings pt
of each word. The bi-LSTM then projects ut as ot .
Subsequently a nonlinear transformation is applied on these projections. Suppose the hidden states
generated by the bi-LSTM are [oroot, o1, o2, . . . , ot, . . . , ol], for a sentence of length l, we compute
the arc scores by introducing parameters Wh, Wm , w and b, and transform them as follows:
rth-arc = Whot; rtm-arc = Wmot,
sΛ(h,m) = w|(tanh(rhh-arc + rmm-arc + b)).
In this formulation, we first use two parameters to extract two different representations that carry
two different types of information: a head seeking for its modifier (h-arc); as well as a modifier
seeking for its head (m-arc). Then a nonlinear function maps them to an arc score.
For a single sentence, we can form a scoring matrix as shown in Figure 4, by filling each entry
in the matrix using the score we obtained. Therefore, the scoring matrix is used to represent the
head-modifier arc score of all the possible arcs connecting words in a sentence (Zheng, 2017).
Using the scoring arc matrix, we build graph-based parsers. Since exploring neural architectures
for scoring is not our focus, we did not explore other architectures, however performance shall be
further improved using advanced neural architectures (Dozat & Manning, 2017; Dozat et al., 2017).
4	Preliminaries: Variational Auto-encoder and Tree CRF
Variational Autoencoder (VAE). The typical VAE is a directed graphical model with Gaussian
latent variables, denoted by z. A generative process first generates a set ofz from the prior distribu-
tion ∏(z) and the data X is generated as P§(x|z) parameterized by θ given input x, In our scenario,
x is an input sequence and z is a sequence of latent variables corresponding to it.
3
Under review as a conference paper at ICLR 2020
The VAE framework seeks to maximize the complete log-likelihood log P (x) by marginalizing
out the latent variable z. Since direct parameter estimation of log P(x) is usually intractable, a
common solution is to maximize its Evidence Lower Bound (ELBO) by introducing an auxiliary
posterior Q(x|z) distribution that encodes the input into the latent space.
Tree Conditional Random Field. Linear chain CRF models an input sequence x = (x1 . . . xl) of
length l with labels y = (y1 . . . yl) with globally normalized probability
P (y|x)
exp S(x, y)
Py∈γ exP S(X, y)
where Y is the set of all the possible label sequences, and S(x, y) the scoring function, usually
decomposed as emission (Pli=1 s(xi, yi)) and transition (Pli=1 s(yi, yi+1)) for first order models.
Tree CRF models generalize linear chain CRF to trees. For dependency trees, if POS tags are given,
the tree CRF model tries to resolve which node pairs should be connected with direction, such
that the arcs form a tree. The potentials in the dependency tree take an exponential form, thus the
conditional probability of a parse tree T, given the sequence, can be denoted as:
exp S(x, T)
P(T|x)
(1)
Z(x)
where Z(x) = ET三以⑦)exp S(x, T) is the partition function that sums over all possible valid
dependency trees in the set T(x) of the given sentence x.
5	Locally Autoencoding Parser (LAP)
We extend the original VAE model for sequence labeling (Chen et al., 2018) to dependency parsing
by building a latent representation position-wise to form a sequential latent representation.
It has been shown that under the VAE framework the latent representation can reflect the desired
properties of the raw input (Kingma & Welling, 2014). This inspired us to use the continuous
latent variable as neural representations for the dependency parsing task. Typically, each token in
the sentence is represented by its latent variable zt , which is a high-dimensional Gaussian variable.
This configuration on the one hand ensures the continuous latent variable retains the contextual
information from lower-level neural models to assist finding its head or its modifier; on the other
hand, it forces tokens of similar properties closer in the euclidean space. We adjust the original VAE
setup in our semi-supervised task by considering examples with labels, similar to recent conditional
variational formulations (Sohn et al., 2015; Miao & Blunsom, 2016; Zhou & Neubig, 2017).
We propose a full probabilistic model for any certain sentence x, with the unified objective to max-
imize for supervised and unsupervised parsing as follows:
J = iogpθ(χ)pω(τ∣χ),	e = [1, ∖t ∙exists,
ω	0, otherwise.
This objective can be interpreted as follows: if the training example has a golden tree T with it, then
the objective is the log joint probability Pθ,ω (T, x); if the golden tree is missing, then the objective
is the log marginal probability Pθ(x). The probability of a certain tree is modeled by a tree-CRF
in Eq. 1 with parameters ω as 凡(T|x). Given the assumed generative process P(x|z), directly
optimizing this objective is intractable, we instead optimize its ELBO (We show the details in the
appendix, proving Jlap is the ELBO of J in Lemma A.1):
Jiap =	E	[logPθ(x|z)] - KL(Qφ(z∣x)∣∣Pθ(z)) + E E	[logPω(T|z)].
Z 〜Qφ(z∣x)	Z 〜Qφ(z∣x)
6	Globally Autoencoding Parser (GAP)
Instead of autoencoding the input locally at the sequence level, we could alternatively directly regard
the dependency tree as the structured latent variable to reconstruct the input sentence, by building a
model containing both a discriminative component and a generative component. The discriminative
component builds a neural CRF model for dependency tree construction, and the generative model
reconstructs the sentence from the factor graph as a Bayesian network, by assuming a generative
4
Under review as a conference paper at ICLR 2020
process in which each head generates its modifier. Concretely, the latent variable in this model is the
dependency tree structure.
6.1	Discriminative Component: the Encoder
We model the discriminative component in our model as Pφ(T∣x) parameterized by Φ, taking the
same form as in Eq. 1. Typically in our model, Φ are the parameters of the underlying neural
networks, whose architecture is described in Sec. 3.1.
6.2	Generative Component: the Decoder
We use a set of conditional categorical distributions to construct our Bayesian network decoder.
More specifically, using the head h and modifier m notation, each head reconstructs its modifier
with the probability P (mt |ht ) for the tth word in the sentence (0th word is always the special
“ROOT” word), which is parameterized by the set of parameters Θ. Given Θ as a matrix of |V| by
|V |, where |V | is the vocabulary size, θmh is the item on row m column h denoting the probability
that the head word h would generate m. In addition, we have a simplex constraint Pm∈V θmh = 1.
The probability of reconstructing the input x as modifiers m in the generative process is
Pθ(m∣T) = Y P(mt∖h) = Y θmtht,
tt
where l is the sentence length andP(mt|ht) represents the probability a head generating its modifier.
6.3	A Unified Supervised and Unsupervised Learning Framework
With the design of the discriminative component and the generative component of the proposed
model, we have a unified learning framework for sentences with or without golden parse tree.
The complete data likelihood of a given sentence, if the golden tree is given, is
Pθ,φ(m, T|x) =Pθ(m∣T )Pφ(T∣x)
l
YP(mt|ht)
t=1
exp SΦ (x, T)
Z(x)
exp P s0Φ,Θ (h, m)
(h,m)∈T
=	Zx	,
where s0Φ,Θ(h, m) = sΦ(h, m) + log θmh, with m, x and T all observable.
For unlabeled sentences, the complete data likelihood can be obtained by marginalizing over all the
possible parse trees in the set T(x):
Pθ,Φ (m|x) = E Pθ,φ(m, T|x)
T ∈T(x)
U (X)
=--------
Z (X),
where U(x) = PT ∈T(x) exp	P	sΦ,Θ(h, m).
(h,m)∈T
We adapted a variant of Eisner (1996)’s algorithm to marginalize over all possible trees to compute
both Z and U, as U has the same structure as Z, assuming a projective tree.
We use log-likelihood as our objective function. The objective for a sentence with golden tree is:
Jl = log PΘ,Φ (m, T|X)
=	s0Φ,Θ(h, m) - log Z(X)
(h,m)∈T
5
Under review as a conference paper at ICLR 2020
Algorithm 1 Learning Algorithm for GAP
1:	Initialize the parameter Θ in the decoder with the labeled data set {x, T}l .
2:	Initialize Λ in the encoder randomly.
3:	for t in epochs do
4:	for sentence xli with golden parse tree Til in the labeled data set {x, T}l do
5:	Stochastically update the parameter Λ in the encoder using Adam while fixing the de-
coder.
6:	end for
7:	Initialize a Counting Buffer B
8:	for unlabeled sentence xiu in the unlabeled data set {x}u do
9:	Compute the posterior Q(T) in an arc factored manner for xiu tractably.
10:	Compute the expectation of all possible (h(head) → m(modif ier)) occurrence in the
sentence x based on Q(T).
11:	Update buffer B using the expectation to the power for ι-1σ of all possible (h → m).
12:	end for
13:	Obtain Θ globally and analytically based on the buffer B and renew the decoder.
14:	end for
If the input sentence does not have an annotated golden tree, then the objective is:
Ju = log PΘ,Φ (m|x)
=logU(x) - logZ(x).	(2)
Thus, during training, the objective function with shared parameters is chosen based on whether the
sentence in the corpus has golden parse tree or not.
6.4	Learning
Directly optimizing the loss in Eq.2 is difficult for the unlabeled data, and may lead to undesirable
shallow local optima without any constraints. Instead, we derive the evidence lower bound (ELBO)
of log Pθ,φ(m∣x) as follows, by denoting Q(T) = Pθ,φ(T∣m, x) as the posterior:
log Pθ,φ(m∣x) = logɪ^ Q(T)
T
log ET〜Q(T)
≥ ET〜Q(T) log
Pθ,φ(m, T|x)
Q(T)
Pθ,φ(m, T|x)
Q(T)
Pθ,φ (m, T|x)
E
=ET〜Q(T) [logPθ(m∣T)] - KL [Q(T)∣∣Pφ(T∣χ)].
Instead of maximizing the log-likelihood directly, we alternatively maximize the ELBO, so our new
objective function for unlabeled data becomes
maxXET〜Q(T) [logPθ(m∣T)] - KL[Q(T)∣∣Pφ(T∣x)].
i
In addition, to account for the unambiguity in the posterior, we incorporate entropy regu-
larization (Tu & Honavar, 2012) when applying our algorithm, by adding an entropy term
- PT Q(T) log Q(T) with a non-negative factor σ when the input sentence does not have a golden
tree. Adding this regularization term is equivalent as raising the expectation of Q(T) to the power
of ι-1σ. We annealed σ from 1 to 0.3 from the beginning of training to the end, as in the beginning,
the generative model is well initialized by sentences with golden trees that resolve disambiguity.
In practice, we found the model benefits more by fixing the parameter Φ when the data is unlabeled
and optimizing the ELBO w.r.t. the parameter Θ. We attribute this to the strict convexity of the
ELBO w.r.t. Θ, by sketching the proof in the appendix. The details of training are shown in Alg. 1.
6.5	Tractable Inference
The common approach to approximate the expectation of the latent variables from the posterior dis-
tribution Q(T) is via sampling in VAE-type models (Kingma & Welling, 2014). In a significant
6
Under review as a conference paper at ICLR 2020
contrast to that, we argue in this model the expectation of the latent variable (which is the depen-
dency tree structure) is analytically tractable by designing a variant of the inside-outside algorithm
(Eisner, 1996; Paskin, 2001) in an arc decomposed manner. We leave the detailed derivation in the
appendix. A high-level explanation is that assuming the dependency tree is projective, specialized
belief propagation algorithm exists to compute not only the marginalization but also the expectation
analytically, making inference tractable.
7	Experiments
7.1	Experimental Settings
Data sets First we compared our models’ performance with strong baselines on the WSJ data
set, which is the Stanford Dependency conversion (De Marneffe & Manning, 2008) of the Penn
Treebank (Marcus et al., 1993) using the standard section split: 2-21 for training, 22 for develop-
ment and 23 for testing. Second we evaluated our models on multiple languages, using data sets
from UD (Universal Dependency) 2.3 (Mcdonald et al., 2013). Since semi-supervised learning is
particularly useful for low-resource languages, we believe those languages in UD can benefit from
our approach.The statistics of the data used in our experiments are described in Table 3 in appendix.
To simulate the low-resource language environment, we used 10% of the whole training set as the
annotated, and the rest 90% as the unlabeled.
Input Representation and Architecture Since we use the same neural architecture in all of our
models, we specify the details of the architecture once, as follows: The internal word embeddings
have dimension 100 and the POS embeddings have dimension 25. The hidden layer of the bi-LSTM
layer is of dimension 125. The nonlinear layers used to form the head and the modifier representation
both have 100 dimension. For LAP, we use separate bi-LSTMs for words and POSs. In GAP,
using “POS to POS” decoder only yield the satisfactory performance. This echos the finding that
complicated decoders may cause “posterior collapse” (van den Oord et al., 2017; Kim et al., 2018).
Training In the training phase, we use Adam (Kingma & Ba, 2014) to update all the parameters
in both LAP and GAP, except the parameters in the decoder in GAP, which are updated by using
their global optima in each epoch. We did not take efforts to tune models’ hyper-parameters and
they remained the same across all the experiments.
7.2	Semi-Supervised Dependency Parsing on WSJ Data Set
We first evaluate our models on the WSJ data set and compared the model performance with other
semi-supervised parsing models, including CRFAE (Cai et al., 2017), which is originally designed
for dependency grammar induction but can be modified for semi-supervised parsing, and “differ-
entiable Perturb-and-Parse” parser (DPPP) (Corro & Titov, 2018). To contextualize the results, we
also experiment with the supervised neural margin-based parser (NMP) (Kiperwasser & Goldberg,
2016), neural tree-CRF parser (NTP) and the supervised version of LAP and GAP, with only the
labeled data. To ensure a fair comparison, our experimental set up on the WSJ is identical as that
in DPPP and we use the same 100 dimension skip-gram word embeddings employed in an earlier
transition-based system (Dyer et al., 2015). We show our experimental results in Table 1.
As shown in this table, both of our LAP and GAP model are able to utilize the unlabeled data to
increase the overall performance comparing with only using labeled data. Our LAP model performs
slightly worse than the NMP model, which we attribute to the increased model complexity by in-
corporating extra encoder and decoders to deal with the latent variable. However, our LAP model
achieved comparable results on semi-supervised parsing as the DPPP model, while our LAP model
is simple and straightforward without additional inference procedure. Instead, the DPPP model has
to sample from the posterior of the structure by using a “GUMBEL-MAX trick” to approximate
the categorical distribution at each step, which is intensively computationally expensive. Further,
our GAP model achieved the best results among all these methods, by successfully leveraging the
the unlabeled data in an appropriate manner. We owe this success to such a fact: GAP is able to
calculate the exact expectation of the arc-decomposed latent variable, the dependency tree structure,
in the ELBO for the complete data likelihood when the data is unlabeled, rather than using sampling
7
Under review as a conference paper at ICLR 2020
Model	UAS
DPPP(Corro & Titov, 2018)(L)	88.79
DPPP(Corro & Titov, 2018)(L+U)	89.50
CRFAE(Cai et al., 2017)(L+U)	82.34
NMP(KiPerWasser & Goldberg, 2016)(L)	89.64
NTP (L)	89.63
Self-training (L+U)	87.81
LAP (L)	89.37
LAP (L+U)	89.49
GAP (L)	89.65
GAP (L+U)	一	89.96
Table 1: Comparing model performance on WSJ data set with 10% labeled data. “L” means only
10% labeled data is used, while “L+U” means both 10% labeled and 90% unlabeled data are used.
Model	Dutch	SPanish	English	French	Croation	German	Italian	Russian	Japanese
NMP (L)	76.11	82.00	75.51-	83.07	-77.44-	74.07	82.85	-75.18-	-93.46-
NTP (L)	76.20	82.09	75.57	83.12	-77.51-	74.13	82.99	-7523-	-93.54-
LAP (L)	76.15	81.93	75.36	83.09	-77.45-	74.14	83.07	-74.84-	-9338-
GAP (L)	76.23	81.97	75.75	83.11	-77.49-	74.16	83.14	-7517-	-93.52-
CRFAE (L+U)	71.32	74.67	68.52	77.35	-69.89-	68.44	76.37	-68.64-	-8726-
ST (L+U)	75.37	80.86	72.76	81.38	-76.10-	73.45	82.74	-72.57-	-91.43-
LAP (L+U)	76.29	82.48	75.48	83.23	-7778-	74.48	83.34	-7522-	-93.65-
GAP (L+U)	76.54	82.56	76.21	83.26	77.83	74.63	83.54	75.69	93.92
Table 2: In this table we compare different models on multiple languages from UD. Models were
trained in a fully supervised fashion with labeled data only (noted as “L”) or semi-supervised (notes
as “L+U”). “ST” stands for self-training.
to approximate the true expectation. Self-training using NMP with both labeled and unlabeled data
is also included as a base-line, where the performance is deteriorated without appropriately using
the unlabeled data.
7.3	Semi-supervised Dependency Parsing on the UD Data Set
We also evaluated our models on multiple languages from the UD data and compared the model
performance with the semi-supervised version of CRFAE and the fully supervised NMP and NTP.
To fully simulate the low-resource scenario, no external word embeddings were used.
We summarize the results in Table 2. First, when using labeled data only, LAP and GAP have
similar performance as NMP and NTP. Second, we note that our LAP and GAP models do benefit
from the unlabeled data, compared to using labeled data only. Both our LAP and GAP model are
able to exploit the hidden information in the unlabeled data to improve the performance. Comparing
between LAP and GAP, we notice GAP in general has better performance than LAP, and can better
leverage the information in the unlabeled data to boost the performance. These results validate that
GAP is especially useful for low-resource languages with few annotations. We also experimented
using self-training on the labeled and unlabeled data with the NMP model. As results show, self-
training deteriorate the performance especially when the size of the training data is small.
8	Conclusion
In this paper, we present two semi-supervised parsers, which are locally autoencoding parser (LAP)
and globally autoencoding parser (GAP). Both of them are end-to-end learning systems enhanced
with neural architecture, capable of utilizing the latent information within the unlabeled data to-
gether with labeled data to improve the parsing performance, without using external resources.
More importantly, our GAP model outperforms the previous published (Corro & Titov, 2018) semi-
supervised parsing system on the WSJ data set. We attribute this success to two reasons: First,
our GAP model consists both a discriminative component and a generative component. These two
components are constraining and supplementing each other such that final parsing choices are made
in a checked-and-balanced manner to avoid over-fitting. Second, instead of sampling from posterior
of the latent variable (the dependency tree) (Corro & Titov, 2018), our model analytically computes
the expectation and marginalization of the latent variable, such that the global optima can be found
for the decoder, which leads to an improved performance.
8
Under review as a conference paper at ICLR 2020
References
Daniel Andor, Chris Alberti, David Weiss, Aliaksei Severyn, Alessandro Presta, Kuzman Ganchev,
Slav Petrov, and Michael Collins. Globally normalized transition-based neural networks. In Proc.
of the Annual Meeting of the Association Computational Linguistics (ACL), 2016.
Joost Bastings, Ivan Titov, Wilker Aziz, Diego Marcheggiani, and Khalil Simaan. Graph Convo-
lutional Encoders for Syntax-aware Neural Machine Translation. In Proc. of the Conference on
Empirical Methods for Natural Language Processing (EMNLP), 2017.
Samuel R Bowman, Luke Vilnis, Oriol Vinyals, Andrew M Dai, Rafal Jozefowicz, and Samy Ben-
gio. Generating Sentences from a Continuous Space. In Proc. International Conference on Learn-
ing Representation (ICLR), 2016.
Jiong Cai, Yong Jiang, and Kewei Tu. CRF Autoencoder for Unsupervised Dependency Parsing.
In Proc. of the Conference on Empirical Methods for Natural Language Processing (EMNLP),
2017.
Danqi Chen and Christopher D Manning. A fast and accurate dependency parser using neural
networks. In Proc. of the Conference on Empirical Methods for Natural Language Processing
(EMNLP), 2014.
Mingda Chen, Qingming Tang, Karen Livescu, and Kevin Gimpel. Variational sequential labelers for
semi-supervised learning. In Proc. of the Conference on Empirical Methods for Natural Language
Processing (EMNLP), 2018.
Caio Corro and Ivan Titov. Differentiable Perturb-and-Parse: Semi-Supervised Parsing with a Struc-
tured Variational Autoencoder. In Proc. International Conference on Learning Representation
(ICLR), 2018.
Hang Cui, Renxu Sun, Keya Li, Min-Yen Kan, and Tat-Seng Chua. Question Answering Passage
Retrieval Using Dependency Relations. In Proc. of International Conference on Research and
Development in Information Retrieval (SIGIR), 2005.
Aron Culotta and Jeffrey Sorensen. Dependency tree kernels for relation extraction. In Proc. of the
Annual Meeting of the Association Computational Linguistics (ACL), 2004.
Marie-Catherine De Marneffe and Christopher D Manning. The Stanford typed dependencies repre-
sentation. In Proc. the International Conference on Computational Linguistics (COLING), 2008.
Yuan Ding and Martha Palmer. Machine translation using probabilistic synchronous dependency
insertion grammars. In Proc. of the Annual Meeting of the Association Computational Linguistics
(ACL), 2007.
Timothy Dozat and Christopher D. Manning. Deep biaffine attention for neural dependency parsing.
In Proc. International Conference on Learning Representation (ICLR), April 2017.
Timothy Dozat, Peng Qi, and Christopher D. Manning. Stanford’s graph-based neural dependency
parser at the conll 2017 shared task. In Proceedings of the CoNLL 2017 Shared Task: Multilingual
Parsing from Raw Text to Universal Dependencies, 2017.
Chris Dyer, Miguel Ballesteros, Wang Ling, Austin Matthews, and Noah A. Smith. Transition-Based
Dependency Parsing with Stack Long Short-Term Memory. In Proc. of the Annual Meeting of the
Association Computational Linguistics (ACL), may 2015.
Jason Eisner. Three New Probabilistic Models for Dependency Parsing: An Exploration. In Proc.
the International Conference on Computational Linguistics (COLING), 1996.
Yong Jiang, Wenjuan Han, and Kewei Tu. Unsupervised Neural Dependency Parsing. In Proc. of
the Conference on Empirical Methods for Natural Language Processing (EMNLP), 2016.
Yoon Kim, Sam Wiseman, Andrew Miller, David Sontag, and Alexander Rush. Semi-amortized
variational autoencoders. In Proc. of the International Conference on Machine Learning (ICML),
2018.
9
Under review as a conference paper at ICLR 2020
Yoon Kim, Chris Dyer, and Alexander Rush. Compound probabilistic context-free grammars for
grammar induction. In Proc. of the Annual Meeting of the Association Computational Linguistics
(ACL), 2019a.
Yoon Kim, Alexander Rush, Lei Yu, AdhigUna Kuncoro, Chris Dyer, and Gabor Melis. UnsUPer-
vised recurrent neural network grammars. In Proc. of the Annual Meeting of the North American
Association of Computational Linguistics (NAACL), 2019b.
Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic OPtimization. ArXiv, dec 2014.
Diederik P Kingma and Max Welling. Auto-Encoding Variational Bayes. In Proc. International
Conference on Learning Representation (ICLR), 2014.
Eliyahu KiPerwasser and Yoav Goldberg. Semi-suPervised dePendency Parsing using bilexical con-
textual features from auto-Parsed data. In Proc. of the Conference on Empirical Methods for
Natural Language Processing (EMNLP), 2015.
Eliyahu KiPerwasser and Yoav Goldberg. SimPle and accurate dePendency Parsing using bidirec-
tional lstm feature rePresentations. Transactions of the Association for Computational Linguistics
(TACL), 2016.
Terry Koo, Xavier Carreras, and Michael Collins. SimPle Semi-suPervised DePendency Parsing. In
Proc. of the Annual Meeting of the Association Computational Linguistics (ACL), 2008.
Sandra Kubler, Ryan McDonald, Joakim Nivre, and Graeme Hirst. Dependency Parsing. Morgan
and ClayPool Publishers, 2009.
Zhenghua Li, Min Zhang, and Wenliang Chen. Ambiguity-aware ensemble training for semi-
suPervised dePendency Parsing. In Proc. of the Annual Meeting of the Association Computational
Linguistics (ACL), 2014.
Yang Liu, Furu Wei, Sujian Li, Heng Ji, Ming Zhou, and Houfeng Wang. A DePendency-Based
Neural Network for Relation Classification. In Proc. of the Annual Meeting of the Association
Computational Linguistics (ACL), 2015.
Xuezhe Ma, Zecong Hu, Jingzhou Liu, Nanyun Peng, Graham Neubig, and Eduard Hovy. Stack-
Pointer Networks for DePendency Parsing. In Proc. of the Annual Meeting of the Association
Computational Linguistics (ACL). Association for ComPutational Linguistics, 2018.
Diego Marcheggiani and Ivan Titov. Encoding Sentences with GraPh Convolutional Networks for
Semantic Role Labeling. In Proc. of the Conference on Empirical Methods for Natural Language
Processing (EMNLP), 2017.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated
corPus of English: the Penn treebank. Computational Linguistics, 1993.
Ryan McDonald. Discriminative learning and spanning tree algorithms for dependency parsing.
PhD thesis, University of Pennsylvania, 2006.
Ryan McDonald, Koby Crammer, and Fernando Pereira. Online large-margin training of dePen-
dency Parsers. In Proc. of the Annual Meeting of the Association Computational Linguistics
(ACL), 2005.
Ryan Mcdonald, Joakim Nivre, Yvonne Quirmbach-brundage, Yoav Goldberg, DiPanjan Das,
Kuzman Ganchev, Keith Hall, Slav Petrov, Hao Zhang, Oscar Tckstrm, Claudia Bedini, Nria
Bertomeu, and Castell Jungmee Lee. Universal dePendency annotation for multilingual Parsing.
In Proc. of the Annual Meeting of the Association Computational Linguistics (ACL), 2013.
Yishu Miao and Phil Blunsom. Language as a latent variable: Discrete generative models for sen-
tence comPression. In Proc. of the Conference on Empirical Methods for Natural Language
Processing (EMNLP), 2016.
Joakim Nivre. Incrementality in deterministic dePendency Parsing. In Proceedings of the Workshop
on Incremental Parsing: Bringing Engineering and Cognition Together, 2004.
10
Under review as a conference paper at ICLR 2020
Joakim Nivre. Algorithms for deterministic incremental dependency parsing. Computational Lin-
guistics, 2008.
Joakim Nivre. The inside-outside recursive neural network model for dependency parsing. In Proc.
of the Conference on Empirical Methods for Natural Language Processing (EMNLP), 2014.
Mark A. Paskin. Cubic-time parsing and learning algorithms for grammatical bigram models. Tech-
nical report, EECS Department, University of California, Berkeley, 2001.
Wenzhe Pei, Tao Ge, and Baobao Chang. An effective neural network model for graph-based de-
pendency parsing. In Proc. of the Annual Meeting of the Association Computational Linguistics
(ACL), 2015.
Siva Reddy, Oscar Tackstrom, Michael Collins, Tom Kwiatkowski, Dipanjan Das, Mark Steedman,
and Mirella Lapata. Transforming Dependency Structures to Logical Forms for Semantic Parsing.
Transactions ofthe Association for Computational Linguistics (TACL), pp.127-140, 2016.
Anders S0gaard and Christian Rish0j. Semi-supervised dependency parsing using generalized tri-
training. In Proc. the International Conference on Computational Linguistics (COLING), 2010.
Kihyuk Sohn, Xinchen Yan, and Honglak Lee. Learning structured output representation using deep
conditional generative models. In The Conference on Advances in Neural Information Processing
Systems (NIPS), 2015.
Karl Stratos. Mutual Information Maximization for Simple and Accurate Part-Of-Speech Induction.
In Proc. of the Annual Meeting of the North American Association of Computational Linguistics
(NAACL), 2019.
Toshiyuki Tanaka. A Theory of Mean Field Approximation. In The Conference on Advances in
Neural Information Processing Systems (NIPS), 1999.
Ben Taskar, Vassil Chatalbashev, Daphne Koller, and Carlos Guestrin. Learning structured predic-
tion models: A large margin approach. In Proc. of the International Conference on Machine
Learning (ICML), 2005.
Kewei Tu and Vasant Honavar. Unambiguity regularization for unsupervised learning of probabilis-
tic grammars. In Proc. of the Conference on Empirical Methods for Natural Language Processing
(EMNLP), 2012.
Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learn-
ing. In The Conference on Advances in Neural Information Processing Systems (NIPS), 2017.
David Weiss, Chris Alberti, Michael Collins, and Slav Petrov. Structured training for neural net-
work transition-based parsing. In Proc. of the Annual Meeting of the Association Computational
Linguistics (ACL), 2015.
Sam Wiseman and Alexander M. Rush. Sequence-to-sequence learning as beam-search opti-
mization. In Proc. of the Conference on Empirical Methods for Natural Language Processing
(EMNLP), 2016.
Xiaoqing Zheng. Incremental graph-based neural dependency parsing. In Proc. of the Conference
on Empirical Methods for Natural Language Processing (EMNLP), 2017.
Chunting Zhou and Graham Neubig. Multi-space variational encoder-decoders for semi-supervised
labeled sequence transduction. In Proc. of the Annual Meeting of the Association Computational
Linguistics (ACL), 2017.
11
Under review as a conference paper at ICLR 2020
A Appendix
Details of the LAP model
ELBO of LAP’s Original Ojective
Lemma A.1. Jlap is the ELBO (evidence lower bound) of the original objective J, with an input
sequence x.
Denote the encoder Q is a distribution used to approximate the true posterior distribution Pφ(z∣x),
parameterized by φ such that Q encoding the input into the latent space z .
Proof.
log Pθ (x)Pω (T|x) = log Pθ (x) + e log Pω (T∣x)
'------------------{z--} S---{z---}
UL
U =log Z Qφ(z∣x) /θ(χ)、dz
Jz	Qφ(ZIx)
≥ E	[logPθ(x|z)] - E log Qφ(ZIx)
Z〜Qφ(z|x)	Z〜Qφ(z∣x) L	pθ(x)
E	[logPθ(xIZ)] - KL (Qφ(ZIx)IIPθ(Z)) , (ELBO of traditional VAE)
Z 〜Qφ (z|x)
L = log Pω (T Ix)
= log
Pω(TIZ)Qφ(ZIx)dz
z
= log E	[Pω(TIZ)]
Z 〜Qφ(z∣x)
≥ E	[logPω(TIZ)] .
Z 〜Qφ(z∣x)
Combining U and L leads to the fact:
U+ L ≥ E [logPθ(xIZ)] - KL(Qφ(ZIx)IIPθ(Z)) + E	[logPω(TIZ)] = Jlap
Z 〜Qφ(Z∣x)	Z〜Qφ(z∣x)
□
In practice, similar as VAE-style models, E	[logPθ(xIZ)] is approximated by
Z 〜Qφ(z∣x)
NN PN=IlogPθ(x∣Zj) and	E	[logPω(T|z)] by N PN=IlogPω(TIzj), where ZjiSthe
Z 〜Qφ(z∣x)
jthe sample of N samples sampled from Qφ(z∣x). At prediction stage, we simply use 乩乞 rather
than sampling Z .
Mean Field Approximation and Annealing
Here we used a mean field approximation (Tanaka, 1999) together with the conditional independence
assumption by assuming Pθ(ZIx) ≈ Qlt=1Qφ(ztIxt). The generative model Pθ(xIZ) acting as
decoder parameterized by θ tries to regenerate the specific input xt at time step t from the latent
space zt, as we assume conditional independence in the generative process among Pθ(xtIzt). The
encoder and the decoder are trained jointly in the classical variational autoencoder framework, by
minimizing the KL divergence between the approximated posterior and the true posterior.
We describe the encoder and decoder formulation. We parameterize the encoder Qφ(ztIxt) in such
a way: First a bi-LSTM is used to obtain a non-linear transformation ht of the original xt ; then
two separate MLPS are used to compute the mean μzt and the variance σZd The generative story
Pθ(xtIzt) follows such parameterization: we used a MLP of two hidden layers in-between to take zt
as the input, and then predict the word (or POS tag) over the vocabulary, such that the reconstruction
probability can be measured.
12
Under review as a conference paper at ICLR 2020
Following traditional VAE training paradigms, we also apply the “re-parameterization” trick
(Kingma & Welling, 2014) to circumvent the non-differentiable sampling procedure to sample zt
from the Qφ(zt |xt). Instead of directly sample from N(模之七,σZ=), We form Zt = μzt + E Θ σ^ by
sampling E 〜N(0, I). In addition, to avoid hindering learning during the initial training phases,
folloWing previous Works (Chen et al., 2018; BoWman et al., 2016), We anneal the temperature on
the KL divergence term from a small value to 1.
Empirical Bayesian Treatment
From an empirical Bayesian perspective, rather than fixing the prior using some certain distributions,
it is beneficial to estimate the prior distribution directly from the data by treating prior’s parameters
part of the model parameters. Similar to the approach used in the previous study (Chen et al., 2018),
LAP also learns the priors from the data by updating them iteratively. We initialize the priors from
a standard Gaussian distribution N(0, I), Where I is an identity matrix. During the training, the
current priors are updated using the last optimized posterior, folloWing the rule:
∏k(z) = X Qφτ (z∣x)P(x),
x
Where P(x) represents the empirical data distribution, and k the iteration step. Empirical Bayesian
is also named as “maximum marginal likelihood”, such that our approach here is to marginalize over
the missing observation as a random variable.
Incorporating POS and External Embeddings
In previous studies (Chen & Manning, 2014; Dozat & Manning, 2017; Dozat et al., 2017; Kiper-
Wasser & Goldberg, 2016) exploring parsing using neural architectures, POS tags and external em-
beddings have been shoWn to contain important information characterizing the dependency relation-
ship betWeen a head and a child. Therefore, in addition to the variational autoencoding frameWork
taking as input the randomly initialized Word embeddings, optionally We can build the same struc-
ture for POS to reconstruct tags and for external embeddings to reconstruct Words as Well, Whose
variational objectives are Up and Ue respectively. Hence, the final variational objective can be a
combination of three: U = Uw (The original U in Lemma A.1) + Up + Ue (or just U = Uw + Up if
external embeddings are not used).
Details of the GAP model
Marginalization and Expectation of Latent Parse Trees
Assuming the sentence is of length l, and We have obtained a arc decomposed scoring matrix S of
size l × l, and an entry S[i, j]i6=j,j6=0 stands for the arc score Where ith Word is the head andjth Word
the modifier. We first describe the inside algorithm to compute the marginalization of all possible
projective trees in Algo.2.
We then describe the outside algorithm to compute the outside tables in Algo. 3. In this algorithm,
L stands for the logaddexp operation.
Finally, With the inside table α, outside table β and the marginalization Z of all possible latent trees,
We can compute the expectation of latent tree in an arc-decomposed manner. Algo. 4 describes the
procedure. It results the matrix P containing the expectation of all individual arcs by marginalize
over all other arcs except itself.
Light modification is needed in our study to calculate the expectation W.r.t. the posterior distribution
Q(T) = PΘ,Φ(T |m, x), as We have
13
Under review as a conference paper at ICLR 2020
Algorithm 2 Inside Algorithm
Input: S
Output: α, Z
1:	α4------∞
2:	for s ∈ 0 .	. . l -	1	do
3:	if s >	0 then
4:	a[s, s, L,C] - 0
5:	end if
6:	a[s, s,	R,C]	-	0
7:	end for
8:	for k ∈ 1 . . . l - 1 do
9:	for s ∈ 0 . . . l - k do
10:	t = s + k
11:	if s > 0 then
12:	a[s,t,L,I] - log P exp (α[s, u, L, C] + α[u,t,L,C])+ S[t,s]
u∈[s,t]
13:	end if
14:	a[s,t,R,I ] - log P exp(α[s,u,L,C] + α[u,t,L,C]) + S [s,t]
u∈[s,t]
15:	if s > 0 then
16:	a[s,t,L,C] - log E exp (α[s, u, L, C] + α[u,t,L,C])
u∈[s,t-1]
17:	end if
18:	a[s,t,R,C] - log E	exp (α[s, u, R, I] + α[u,t,R,I])
u∈[s+1,t]
19:	end for
20:	end for
21:	Z — a[0,l - 1,R,C]
Algorithm 3 Outside Algorithm
Input: S, α
Output: β
1:	β《—-oc
2:	β[0,l — 1,R,C] J 0
3:	for k ∈ l - 1 . . . 1 do
4:	for s ∈ 0 . . . l - k do
5:	t = s + k
6:	β[s, s + 1 : t + 1, R, I] J L(β[s, s + 1 : t + 1, R, I], β[s, t, R, C] + α[s + 1 :
t+1,t,R,C])
7:	β[s + 1 : t + 1,t,R,C] J L(β[s + 1 : t + 1,t,R,C],β[s,t,R,C] + α[s, s + 1 :
t+1,R,I])
8:	if s >	0 then
9:	β[s,s : t,L, C ] J L(β[s,s : t,L,C ], β[s,t,R, C ] + α[s : t,t,L,I ])
10:	β[s : t, t, L, I] J L(β[s : t, t, L, I], β[s, t, L, C] + α[s, s : t, L, C])
11:	end if
12:	β[s,s	: t,R,C]	J	L(β[s, s	: t,R,C],β[s,t,R,I]+α[s+1	:	t+1,t,L,C]+S[s,t])
13:	β[s +	1 : t + 1, t, L,	C]	J	L(β[s + 1	:	t	+ 1,t, L, C], β[s, t, R, I] + α[s,	s	: t, R, C]	+
S[s,t])
14:	if s > 0 then
15:	β[s, s : t,R,C] J L(β[s, s : t,R,C],β[s,t,L,I] + α[s + 1 : t + 1,t,L,C] +
S[t, s])
16:	β[s + 1 : t + 1,t,L,C] J	(β[s + 1 : t + 1,t,L,C],β[s,t,L,I] + α[s, s :
t,R,C] + S[t, s])
17:	end if
18:	end for
19:	end for
14
Under review as a conference paper at ICLR 2020
Algorithm 4 Arc Decomposed Expectation
Input: α, β, Z
Output: P
1
2
3
4
5
6
7
8
9
10
11
P J 0
for s ∈ 0 . . . l - 2 do
for t ∈ s + 1 . . . l - 1 do
if s 6= t then
P[s, t] J exp(α[s, t, R, I] + β[s, t, R, I] - Z)
if s > 0 then
P[t, s] J exp(α[s, t, L, I] + β[s, t, L, I] - Z)
end if
end if
end for
end for
PΘ,Φ(T, m|x)
Pθ,φ(T∣m, x) =	~~7—
Pθ,φ(m∣x)
exp P	s0Φ,Θ
(h,m)∈T
Z(x)
(h, m)
/X
T∈T
exp P	s0Φ,Θ (h, m)
(h,m)∈T
Z(X)
exp P	s0Φ,Θ (h, m)
(h,m)∈T
=	Z0(X)	:	,
where Z0(x) = PT∈T exp P	s0Φ,Θ(h, m) is the real marginal we need to calculate using
(h,m)∈T
the transformed scoring matrix S0 as input in the inside algorithm. Each entry in this transformed
scoring matrix is defined in the text as s0Φ,Θ (h, m).
CONVEXITY OF ELBO W.R.T. Θ
In this section we derive the strict convexity of ELBO w.r.t. Θ. Since we only care about the term
containing Θ, the KL divergence term degenerates to a constant. For sentence i, Q(Ti) has been
derived in the previous section as matrix P and 1 is the indication function.
maxJ2ETi〜Q(Ti)现Pθ(mi∖Ti)] - KL [Q(Ti)∣∣Pφ(Ti∣Xi)]
i
max Σ Σ Q(Ti) logPΘ(mi∖Ti) + Const
i Ti∈T(xi)
max E log θmhE(h→m)〜Ql(h → m)
(h→m)
max E Q(l(h → m))log θmh Q(l(h → m)) is a Bernoulli distribution, indicating whether the arc (h → m) exists.
(h→m)
s.t. X θmh = 1 ∀h.	(3)
m
Data Set Statistics
We show the details of the statistics of the WSJ data set, which is the Stanford Dependency conver-
sion (De Marneffe & Manning, 2008) of the Penn Treebank (Marcus et al., 1993) and the statistics
of the languaes we used in UD (Universal Dependency) 2.3 (Mcdonald et al., 2013) here.
15
Under review as a conference paper at ICLR 2020
Language	WSJ	Dutch	Spanish	English	French	Croatian	German	Italian	Russian	Japanese
Training	39832	12269	14187	2914	14450	-6983-	13814	13121	-3850-	-7Γ33-
Development	1700	718	-1400-	707	1476	-849-	799	564	-579-	-511-
Testing	2416	596	426	769	416	1057	977	482	601	551
Table 3: Statistics of multiple languages we used in our experiments are shown here. The table
shows number of sentences in the training, development and test data divisions.
16