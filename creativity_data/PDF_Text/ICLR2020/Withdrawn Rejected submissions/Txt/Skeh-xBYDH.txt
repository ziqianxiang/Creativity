Under review as a conference paper at ICLR 2020
On S ymmetry and Initialization
for Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
This work provides an additional step in the theoretical understanding of neural
networks. We consider neural networks with one hidden layer and show that when
learning symmetric functions, one can choose initial conditions so that standard
SGD training efficiently produces generalization guarantees. We empirically ver-
ify this and show that this does not hold when the initial conditions are chosen at
random. The proof of convergence investigates the interaction between the two
layers of the network. Our results highlight the importance of using symmetry in
the design of neural networks.
1 Introduction
Building a theory that can help to understand neural networks and guide their construction is one
of the current challenges of machine learning. Here we wish to shed some light on the role sym-
metry plays in the construction of neural networks. It is well-known that symmetry can be used to
enhance the performance of neural networks. For example, convolutional neural networks (CNNs)
(see Lecun et al. (1998)) use the translational symmetry of images to classify images better than
fully connected neural networks. Our focus is on the role of symmetry in the initialization stage. We
show that symmetry-based initialization can be the difference between failure and success.
On a high-level, the study of neural networks can be partitioned to three different aspects.
Expressiveness Given an architecture, what are the functions it can approximate well?
Training Given a network with a “proper” architecture, can the network fit the training data and in
a reasonable time?
Generalization Given that the training seemed successful, will the true error be small as well?
We study these aspects for the first “non trivial” case of neural networks, networks with one hidden
layer. We are mostly interested in the initialization phase. If we take a network with the appropriate
architecture, we can always initialize it to the desired function. A standard method (that induces a
non trivial learning problem) is using random weights to initialize the network. A different reason-
able choice is to require the initialization to be useful for an entire class of functions. We follow the
latter option.
Our focus is on the role of symmetry. We consider the following class of symmetric functions
n
S = Sn = ɔ ∑ ai ∙ ɪlχ∣=i : a 1,..., an ∈
i=0
{±1} ,
where x ∈ {0, 1}n and |x| = ∑ixi. The functions in this class are invariant under arbitrary permu-
tations of the input’s coordinates. The parity function π(x) = (-1)|x| and the majority function are
well-known examples of symmetric functions.
Expressiveness for this class was explored by Minsky and Papert (1988). They showed that the
parity function cannot be represented using a network with limited “connectivity”. Contrastingly,
if we use a fully connected network with one hidden layer and a common activation function (like
sign, sigmoid, or ReLU) only O(n) neurons are needed. We provide such explicit representations
for all functions in S; see Lemmas 1 and 2.
1
Under review as a conference paper at ICLR 2020
We also provide useful information on both the training phase and generalization capabilities of the
neural network. We show that, with proper initialization, the training process (using standard SGD)
efficiently converges to zero empirical error, and that consequently the network has small true error
as well.
Theorem 1. There exists a constant c > 1 so that the following holds. There exists a network with
one hidden layer, cn neurons with sigmoid or ReLU activations, and an initialization such that for
all distributions D overX = {0,1}n and allfunctions f ∈ S with sample size m ≥ C(n+log(1∕δ))/ε,
after performing poly(n) SGD updates with a fixed step size h = 1/ pol y(n) it holds that
Xm PDm ({ S : x PD(NS (X )= f (X)) > ε })< δ
where S = {(x1,f(x1)), ..., (xm,f(xm))} and NS (x) is the network after training over S.
The number of parameters in the network described in Theorem 1 is Ω(n2). So in general one could
expect overfitting when the sample size is as small as O(n). Nevertheless, the theorem provides
generalization guarantees, even for such a small sample size.
The initialization phase plays an important role in proving Theorem 1. To emphasize this, we report
an empirical phenomenon (this is “folklore”). We show that a network cannot learn parity from a
random initialization (see Section 5.3). On one hand, if the network size is big, we can bring the
empirical error to zero (as suggested in Soudry and Carmon (2016)), but the true error is close to
1/2. On the other hand, if its size is too small, the network is not even able to achieve small empirical
error (see Figure 5). We observe a similar phenomenon also for a random symmetric function. An
open question remains: why is it true that a sample of size polynomial in n does not suffice to learn
parity (with random initialization)?
A similar phenomenon was theoretically explained by Shamir (2016) and Song et al. (2017). The
parity function belongs to the class of all parities
P = Pn = {∏S(X) = (T)S^X : S ∈ X}
where ∙ is the standard inner product. This class is efficiently PAC-learnable with O(n) samples
using Gaussian elimination. A continuous version of P was studied by Shamir (2016) and Song
et al. (2017). To study the training phase, they used a generalized notion of statistical queries
(SQ); see Kearns (1998). In this framework, they show that most functions in the class P cannot be
efficiently learned (roughly stated, learning the class requires an exponential amount of resources).
This framework, however, does not seem to capture actual training of neural networks using SGD.
For example, it is not clear if one SGD update corresponds to a single query in this model. In
addition, typically one receives a dataset and performs the training by going over it many times,
whereas the query model estimates the gradient using a fresh batch of samples in each iteration.
The query model also assumes the noise to be adversarial, an assumption that does not necessarily
hold in reality. Finally, the SQ-based lower bound holds for every initialization (in particular, for the
initialization we use here), so it does not capture the efficient training process Theorem 1 describes.
Theorem 1 shows, however, that with symmetry-based initialization, parity can be efficiently
learned. So, in a nutshell, parity can not be learned as part of P, but it can be learned as part of
S. One could wonder why the hardness proof for P cannot be applied for S as both classes con-
sist of many input sensitive functions. The answer lies in the fact that P has a far bigger statistical
dimension than S (all functions in P are orthogonal to each other, unlike S).
The proof of the theorem utilizes the different behavior of the two layers in the network. SGD is
performed using a step size h that is polynomially small in n. The analysis shows that in a polynomial
number of steps that is independent of the choice of h the following two properties hold: (i) the
output neuron reaches a “good” state and (ii) the hidden layer does not change in a “meaningful”
way. These two properties hold when h is small enough. In Section 5.2, we experiment with large
values of h. We see that, although the training error is zero, the true error becomes large.
Here is a high level description of the proof. The ` neurons in the hidden layer define an “embed-
ding” of the inputs space X = {0,1}n into R' (a.k.a. the feature map). This embedding changes in
time according to the training examples and process. The proof shows that if at any point in time
this embedding has good enough margin, then training with standard SGD quickly converges. This
is explained in more detail in Section 3. It remains an interesting open problem to understand this
phenomenon in greater generality, using a cleaner and more abstract language.
2
Under review as a conference paper at ICLR 2020
1.1 Background
To better understand the context of our research, we survey previous related works.
The expressiveness and limitations of neural networks were studied in several works such as Rahimi
and Recht (2008); Telgarsky (2016); Eldan and Shamir (2016) and Arora et al. (2016). Constructions
of small ReLU networks for the parity function appeared in several previous works, such as Wilam-
owski et al. (2003), Arslanov et al. (2016), Arslanov et al. (2002) and Masato Iyoda et al. (2003).
Constant depth circuits for the parity function were also studied in the context of computational
complexity theory, see for example FUrst et al. (1981), Ajtai (1983) and Hastad (1987).
The training phase of neural networks was also studied in many works. Here we list several works
that seem most related to oUrs. Daniely (2017) analyzed SGD for general neUral network architectUre
and showed that the training error can be nUllified, e.g., for the class of boUnded degree polynomials
(see also Andoni et al. (2014)). Jacot et al. (2018) stUdied neUral tangent kernels (NTK), an infinite
width analogUe of neUral networks. DU et al. (2018) showed that randomly initialized shallow ReLU
networks nUllify the training error, as long as the nUmber of samples is smaller than the nUmber of
neUrons in the hidden layer. Their analysis only deals with optimization over the first layer (so that
the weights of the oUtpUt neUron are fixed). Chizat and Bach (2018) provided another analysis of
the latter two works. Allen-ZhU et al. (2018b) showed that over-parametrized neUral networks can
achieve zero training error, as as long as the data points are not too close to one another and the
weights of the oUtpUt neUron are fixed. ZoU et al. (2018) provided gUarantees for zero training error,
assUming the two classes are separated by a positive margin.
Convergence and generalization gUarantees for neUral networks were stUdied in the following works.
BrUtzkUs et al. (2017) stUdied linearly separable data. Li and Liang (2018) stUdied well separated
distribUtions. Allen-ZhU et al. (2018a) gave generalization gUarantees in expectation for SGD. Arora
et al. (2019) gave data-dependent generalization boUnds for GD. All these works optimized only over
the hidden layer (the oUtpUt layer is fixed after initialization).
Margins play an important role in learning, and we also Use it in oUr proof. Sokolic et al. (2016),
Sokolic et al. (2017), Bartlett et al. (2017) and SUn et al. (2015) gave generalization boUnds for
neUral networks that are based on their margin when the training ends. From a practical perspective,
Elsayed et al. (2018), Romero and AlqUezar (2002) and LiU et al. (2016) sUggested different training
algorithms that optimize the margin.
As discUssed above, it seems difficUlt for neUral networks to learn parities. Song et al. (2017)
and Shamir (2016) demonstrated this Using the langUage statistical qUeries (SQ). This is a valUable
langUage, bUt it misses some central aspects of training neUral networks. SQ seems to be closely
related to GD, bUt does not seem to captUre SGD. SQ also shows that many of the parities fUnctions
0i∈sXi are difficult to learn, but it does not imply that the parity function 0i∈［川Xi is difficult to learn.
Abbe and Sandon (2018) demonstrated a similar phenomenon in a setting that is closer to the “real
life” mechanics of neural networks.
We suggest that taking the symmetries of the learning problem into account can make the difference
between failure and success. Several works suggested different neural architectures that take sym-
metries into account; see Zaheer et al. (2017), Gens and Domingos (2014), and Cohen and Welling
(2016).
2 Representations
Here we describe efficient representations for symmetric functions by networks with one hidden
layer. These representations are also useful later on, when we study the training process. We study
two different activation functions, sigmoid and ReLU (similar statement can be proved for other
activations, like arctan). Each activation function requires its own representation, as in the two
lemmas below.
2.1 Sigmoid
We start with the activation σ(ξ) = /xpT), since it helps to understand the construction for
the ReLU activation. The building blocks of the symmetric functions are indicators of |x| = i for
3
Under review as a conference paper at ICLR 2020
Figure 1: Approximations of the symmetric function fA = sign(∑i∈a IK∣=∙ - 0.5) by sigmoid and
ReLU activations forA = {1, 5, 7, 15, 20, 21, 22, 25}.
i ∈ {0, 1, . . . ,n}. An indicator function is essentially the difference between two sigmoid functions:
Sign(IK∣=i - 0.5) = Sign(Ai- 0.5),
where ∆i(K) = σ (5(|K| - i + 0.5)) - σ (5(|K| - i - 0.5)).
Lemma 1. The symmetric function fA = sign(∑i-∈A 1∣K∣=i- - 0.5) satisfies fA(K) = sign(-0.5 +
∑i∈A ∆i (K)), where A ⊂ [n].
A network with one hidden layer of n + 2 neurons with sigmoid activations and one bias neuron
is sufficient to represent any function in S. The coefficients of the sigmoid gates are 0, ±1 in this
representation. The proofs of this lemma and the subsequent lemmas appear in the appendix.
2.2 RELU
A sigmoid function can be represented using ReLU(ξ) = max{0, ξ} as the difference between two
ReLUs
σ (5(K + 0.5)) ≈ ReLU(K + 1) - ReLU(K)
Hence, an indicator function can be represented using Sign(IK∣=i- - 0.5) = sign(Γi- - 0.5) where
Γi (K) = ReLU(|K| -i+1) - 2 ReLU(|K| - i) + ReLU(|K| -i- 1).
Lemma 2. The symmetric function fA = sign(∑i-∈A 1∣K∣=i- - 0.5) satisfies fA (K) = sign(-0.5 +
∑i∈A Γi (K)), where A ⊂ [n].
The lemma shows that a network with one hidden layer ofn+3 ReLU neurons and one bias neuron
is sufficient to represent any function in S. The coefficients of the ReLU gates are 0, ±1, ±2 in this
representation.
3	Training and Generalization
The goal of this section is to describe a small network with one hidden layer that (when initialized
properly) efficiently learns symmetric functions using a small number of examples (the training is
done via SGD).
3.1	Specifications
Here we specify the architecture, initialization and loss function that is implicit in our main result
(Theorem 1).
To guarantee convergence of SGD, we need to start with “good” initial conditions. The initialization
we pick depends on the activation function it uses, and is chosen with resemblance to Lemma 2 for
4
Under review as a conference paper at ICLR 2020
ReLU. On a high level, this indicates that understanding the class of functions we wish to study
in term of “representation” can be helpful when choosing the architecture of a neural network in a
learning context.
The network we consider has one hidden layer. We denote by wij the weight between coordinate j
of the input and neuron i in the hidden layer. We denote W this matrix of weights. We denote by bi
the bias of neuron i of the hidden layer. We denote B this vector of weights. We denote by mi is the
weight from neuron i in the hidden layer to the output neuron. We denote M this vector of weights.
We denote by b the bias of the output neuron.
Initialize the network as follows: The dimensions of W are (n + 3) × n. For all 1 ≤ i ≤ (n+ 3) and
1 ≤ j ≤ n, we set wij = 1 and bi = -i+2. We setM = 0 and b = 0.
To run SGD, we need to choose a loss function. We use the hinge loss,
L (x, f) = max{0, — f (x)(Vx ∙ M + b)+ β },
where vx = ReLU(W x + B) is the output of the hidden layer on input x and β > 0 is a parameter of
confidence.
3.2	Margins
A key property in the analysis is the ‘margin’ of the hidden layer with respect to the function being
learned.
A map Y : V → {±1} over a finite set V ⊂ Rd is linearly1 separable if there exists w ∈ Rd such
that sign(W ∙ V) = Y(V) for all V ∈ V. When the Euclidean norm of W is ∣∣ W∣∣ = 1, the num-
ber marg( w, Y) = min V ∈vY (V) W ∙ V is the margin of W with respect to Y. The number marg (Y)=
SUpW∈rdR w∣=1 marg(w, Y) is the margin of Y.
We are interested in the following set V in Rd. Recall that W is the weight matrix between the input
layer and the hidden layer, and that B is the relevant bias vector. Given W, B, we are interested in
the set V = {Vx : x ∈ X}, where Vx = ReLU(W x + B). In words, we think of the neurons in the
hidden layer as defining an “embedding” of X in Euclidean space. A similar construction works for
other activation functions. We say that Y : V → {±1} agrees with f ∈ S if for all x ∈ X it holds that
Y(Vx) = f (x).
The following lemma bounds from below the margin of the initial V.
Lemma 3. IfY is a partition that agrees With some function in S for the initialization described
aboVe then marg (Y) ≥ Ω(1∕√n).
Proof. By Lemmas 1 and 2, we see that any function in S can be represented with a vector of
weights M, b ∈ [-1,1]θ(n) of the output neuron together with a bias. These M, b induce a partition
Y of V. Namely, Y (Vx) M ∙ Vx + b > 0.25 for all X ∈ X. Since ∣∣( M, b )∣ = O (√n) We have our desired
result.	□
3.3	Freezing the Hidden Layer
Before analyzing the full behavior of SGD, we make an observation: if the weights of the hidden
layer are fixed with the initialization described above, then Theorem 1 holds for SGD with batch size
1. This observation, unfortunately, does not suffice to prove Theorem 1. In the setting we consider,
the training of the neural network uses SGD without fixing any weights. This more general case is
handled in the next section. The rest of this subsection is devoted for explaining this observation.
Novikoff (1962) showed that that the perceptron algorithm Rosenblatt (1958) makes a small num-
ber of mistakes for linearly separable data with large margin. For a comprehensive survey of the
perceptron algorithm and its variants, see Moran et al. (2018).
Running SGD with the hinge loss induces the same update rule as in a modified perceptron algo-
rithm, Algorithm 1.
1A standard “lifting” that adds a coordinate with 1 to every vector allows to translate the affine case to the
linear case.
5
Under review as a conference paper at ICLR 2020
Algorithm 1 The modified perceptron algorithm
Initialize: w(0) = ~0, t = 0, β > 0 and h > 0
while ∃v ∈ V with Y (v) W(t) ∙ v ≤ β do
w(t+1) = w(t)+Y(v)vh
t=t+1
end while
return w(t)
Novikoff’s proof can be generalized to any β > 0 and batches of any size to yield the following
theorem; see Collobert and Bengio (2004); Krauth and Mezard (1987) and appendix A.
Theorem 2. For Y : V → {±1} with margin γ > 0 and step size h > 0, the modified perceptron
algorithm performs at most h(+Rh)updates and achieves a margin of at least 邛；?"h)2, where
R = maxv∈V kvk.
So, when the weights of the hidden layer are fixed, Lemma 3 implies that the number of SGD steps
is at most polynomial in n.
3.4 Stability
When we run SGD on the entire network, the layers interact. For a ReLU network at time t , the
update rule for W is as follows. If the network classifies the input x correctly with confidence more
than β , no change is made. Otherwise, we change the weights in M by ∆M = yvxh, where y is the
true label and h is the step size. If also neuron i of the hidden fired on x, we update its incoming
weights by ∆Wi,: = ymixh. These update rules define the following dynamical system: (a)
W(t+1) =W(t)+yM(t)	(1)
W (t +1) = W (t)+ y ((M (t ))T ◦ H W (t) X + B(t))) XTh	⑵
B (t +1) = B (t)+ y ((M (t)) T ◦ H(W (t) x + B (t))) h	(3)
M(t+1) = M(t) +yReLU W(t)X + B(t) h	(4)
b(t+1) =b(t)+yh,	(5)
where H is the Heaviside step function and ◦ is the Hadamard pointwise product.
A key observation in the proof is that the weights of the last layer ((4) and (5)) are updated exactly
as the modified perceptron algorithm. Another key statement in the proof is that if the network
has reached a good representation of the input (i.e., the hidden layer has a large margin), then the
interaction between the layers during the continued training does not impair this representation. This
is summarized in the following lemma (we are not aware of a similar statement in the literature).
Lemma 4. LetM = 0, b = 0, and V = {ReLU(W X + B) : X ∈ X} be a linearly separable embedding
of X and with margin γ > 0 by the hidden layer of a neural network of depth two with ReLU
activation and weights given by W,B. Let RX = maxX∈X kXk, let R = maxv∈V kvk, and 0 < h ≤
`/2	C 2
100R2RX be the integration step. Assuming RX > 1 and Y ≤ 1, and using β = R2h in the loss function,
after t SGD iterations the following hold:
-Each V ∈ V moves a distance ofat most O(RXh2Rt3/2).
一 The norm ∣∣M(t) ∣∣ is at most O(Rh√t).
-The training ends in at most O(R2∕γ2) SGD updates.
Intuitively, this type of lemma can be useful in many other contexts. The high level idea is to identify
a “good geometric structure” that the network reaches and enables efficient learning.
6
Under review as a conference paper at ICLR 2020
4	Main Result
Proof of Theorem 1. There is an unknown distribution D over the space X. We pick i.i.d. examples
S = ((ɪι,yι),…，(Xm,ym)) where m ≥ C(fn+logQ∕δ)) according to D, whereyi = f (Xi) for some f ∈ S.
Run SGD for O(n4) steps, where the step size is h = O(1/n6) and the parameter of the loss function
is β = R2h with R = n3/2.
We claim that it suffices to show that at the end of the training (i) the network correctly classifies all
the sample points X1, . . . ,Xm, and (ii) for every X ∈ X such that there exists 1 ≤ i ≤ m with |X| = |Xi|,
the network outputs yi on X as well. Here is why. The initialization of the network embeds the space
X into n+4 dimensional space (including the bias neuron of the hidden layer). Let V (0) be the initial
embedding V(0) = {ReLU(W (0)X + B(0)) : X ∈ X}. Although |X| = 2n, the size of V(0) is n+ 1. The
VC dimension of all the boolean functions over V(0) is n + 1. Now, m samples suffice to yield ε
true error for an ERM when the VC dimension is n+ 1; see e.g. Theorem 6.7 in Shalev-Shwartz and
Ben-David (2014). It remains to prove (i) and (ii) above.
By Lemma 3, at the beginning of the training, the partition of V(O) defined by the target f ∈ S has
a margin of Y = Ω(1∕√n). We are interested in the eventual V * = {ReLU(W * X + B *) : X ∈ X}
embedding of X as well. The modified perceptron algorithm together with Lemma 4 guarantees that
after K ≤ 20R2/Y2 = O(n4) updates, (M*, b*) separates the embedded sample VS = {ReLU(W*Xi +
B*): 1 ≤ i ≤ m} with a margin of at least 0.9γ/3.
It remains to prove (ii). Lemma 4 states that as long as less than K = O(n4) updates were made,
the elements in V moved at most O(1/n2). At the end of the training, the embedded sample VS is
separated with a margin of at least 0.9Y /3 with respect to the hyperplane defined by M* and B* .
Each vX* forX ∈ X moved at most O(1/n2) < Y/4. This means that if |X| = |Xi| then the network has
the same output on X and Xi. Since the network has zero empirical error, the output on this X is yi as
well.
A similar proof is available with sigmoid activation (with better convergence rate and larger allowed
step size).
□
Remark. The generalization part of the above proof can be viewed as a consequence of sample
compression (Littlestone and Warmuth (1986)). Although the eventual network depends on all eX-
amples, the proof shows that its functionality depends on at most n+ 1 eXamples. Indeed, after the
training, all eXamples with equal hamming weight have the same label.
Remark. The parameter β = R2h we chose in the proof may seem odd and negligible. It is a
construct in the proof that allows us to bound efficiently the distance that the elements in V have
moved during the training. For all practical purposes β = 0 works as well (see Figure 4).
5	Experiments
We accompany the theoretical results with some experiments. We used a network with one hidden
layer of 4n + 3 neurons, ReLU activation, and the hinge loss with β = n3h. In all the experiments,
we used SGD with mini-batch of size one and before each epoch we randomized the sample. We ob-
served similar behavior for larger mini-batches, other activation functions, and other loss functions.
The graphs that appear in the appendix A present the training error and the true error2 versus the
epoch of the training process. In all the comparisons below, we chose a random symmetric function
and a random sample from X .
5.1	The Theory in Practice
Figure 2 demonstrates our theoretical results and also validates the performance of our initialization.
In one setting, we trained only the second layer (freezed the weights of the hidden layer) which
2We deal with high dimensional spaces, so the true error was not calculated exactly but approximated on an
independent batch of samples of size 104.
7
Under review as a conference paper at ICLR 2020
essentially corresponds to the perceptron algorithm. In the second setting, we trained both layers
with a step size h = n-6 (as the theory suggests). As expected, performance in both cases is similar.
We remark that SGD continues to run even after minimizing the empirical error. This happens
because of the parameter β > 0.
5.2	Overstepping the Theory
Here we experiment with two parameters in the proof, the step size h and the confidence parameter
β. In Figure 3, we used three different step sizes, two of which much larger than the theory suggests.
We see that the training error converges much faster to zero, when the step size is larger. This fast
convergence comes at the expense of the true error. For a large step size, generalization cease to
hold.
Setting β = n3h is a construct in the proof. Figure 4 shows that setting β = 0 does not impair the
performance. The difference between theory (requires β > 0) and practice (allows β = 0) can be
explained as follows. The proof bounds the worst-case movement of the hidden layer, whereas in
practice an average-case argument suffices.
5.3	Hard to Learn Parity
Figure 5 shows that even for n = 20, learning parity is hard from a random initialization. When
the sample size is small the training error can be nullified but the true error is large. As the sample
grows, it becomes much harder for the network to nullify even the training error. With our initializa-
tion, both the training error and true error are minimized quickly. Figure 6 demonstrates the same
phenomenon for a random symmetric function.
5.4	Corruption of Data
Our initialization also delivers satisfying results when the input data it corrupted. In figure 7, we
randomly perturb (with probability P = 需)the labels and use the same SGD to train the model. In
figure 8, we randomly shift every entry of the vectors in the space X by ε that is uniformly distributed
in [-0.1,0.1]n.
6 Conclusion
This work demonstrates that symmetries can play a critical role when designing a neural network.
We proved that any symmetric function can be learned by a shallow neural network, with proper
initialization. We demonstrated by simulations that this neural network is stable under corruption of
data, and that the small step size is the proof is necessary.
We also demonstrated that the parity function or a random symmetric function cannot be learned
with random initialization. How to explain this empirical phenomenon is still an open question. The
works Shamir (2016) and Song et al. (2017) treated parities using the language of SQ. This language
obscures the inner mechanism of the network training, so a more concrete explanation is currently
missing.
We proved in a special case that the standard SGD training of a network efficiently produces low
true error. The general problem that remains is proving similar results for general neural networks.
A suggestion for future works is to try to identify favorable geometric states of the network that
guarantee fast convergence and generalization.
References
Emmanuel Abbe and Colin Sandon. Provable limitations of deep learning, 2018.
M. Ajtai. ∑11-formulae on finite structures. Annals ofPure and Applied Logic, 24(1), pages 1-48, 1983.
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized neural
networks, going beyond two layers. CoRR, abs/1811.04918, 2018a.
8
Under review as a conference paper at ICLR 2020
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. CoRR, abs/1811.03962, 2018b.
Alexandr Andoni, Rina Panigrahy, Gregory Valiant, and Li Zhang. Learning polynomials with neural networks.
In Eric P. Xing and Tony Jebara, editors, Proceedings of the 31st International Conference on Machine
Learning, volume 32 of Proceedings of Machine Learning Research, pages 1908-1916, 2014.
Raman Arora, Amitabh Basu, Poorya Mianjy, and Anirbit Mukherjee. Understanding deep neural networks
with rectified linear units. CoRR, abs/1611.01491, 2016.
Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of optimization
and generalization for overparameterized two-layer neural networks. CoRR, abs/1901.08584, 2019.
Marat Arslanov, Zhazira E. Amirgalieva, and Chingiz A. Kenshimov. N-bit parity neural networks with mini-
mum number of threshold neurons. Open Engineering, 6, 01 2016.
Peter Bartlett, Dylan J. Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for neural networks,
2017.
Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz. SGD learns over-parameterized
networks that provably generalize on linearly separable data. In ICLR, 2018.
Lenaic Chizat and Francis Bach. A note on lazy training in supervised differentiable programming, 12, 2018.
Taco S. Cohen and Max Welling. Group equivariant convolutional networks, 2016.
Ronan Collobert and Samy Bengio. Links between perceptrons, mlps and svms. In Proceedings of the Twenty-
first International Conference on Machine Learning, ICML ’04, page 23, 2004.
Amit Daniely. Sgd learns the conjugate kernel class of the network. In Advances in Neural Information
Processing Systems 30, pages 2422-2430, 2017.
Simon S. Du, Xiyu Zhai, Barnabas PoCzos, and Aarti Singh. Gradient descent Provably optimizes over-
parameterized neural networks. CoRR, abs/1810.02054, 2018.
Ronen Eldan and Ohad Shamir. The Power of Depth for Feedforward Neural Networks. In JMLR 49, pages
1-34, 2016.
Gamaleldin F. Elsayed, Dilip Krishnan, Hossein Mobahi, Kevin Regan, and Samy Bengio. Large margin deep
networks for classification. In NIPS, pages 850-860, 2018.
Merrick Furst, James B. Saxe, and Michael Sipser. Parity, circuits, and the polynomial-time hierarchy. In
FOCS, pages 260-270, 1981.
Robert Gens and Pedro M Domingos. Deep symmetry networks. In Advances in Neural Information Processing
Systems 27, pages 2537-2545, 2014.
Johan Hastad. Computational Limitations of Small-depth Circuits. MIT Press, Cambridge, MA, USA, 1987.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in
neural networks. In NIPS, pages 8580-8589, 2018.
Michael Kearns. Efficient noise-tolerant learning from statistical queries. J. ACM, 45 (6), pages 983-1006,
1998.
Werner Krauth and Marc Mezard. Learning algorithms with optimal stability in neural networks. J. Phys., A20,
pages L745-L752, 1987.
Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition.
Proceedings of the IEEE, 86 (11), pages 2278-2324, 1998.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient descent on
structured data, 2018.
Nick Littlestone and Manfred K. Warmuth. Relating data compression and learnability. Technical report, 1986.
Weiyang Liu, Yandong Wen, Zhiding Yu, and Meng Meng Yang. Large-margin softmax loss for convolutional
neural networks. In ICML, 2016.
Eduardo Masato Iyoda, Hajime Nobuhara, and Kaoru Hirota. A solution for the n-bit parity problem using a
single translated multiplicative neuron. Neural Processing Letters, 18:233-238, 12 2003.
9
Under review as a conference paper at ICLR 2020
Marvin L. Minsky and Seymour A. Papert. Perceptrons: Expanded Edition. MIT Press, Cambridge, MA, USA,
1988.
Shay Moran, Ido Nachum, Itai Panasoff, and Amir Yehudayoff. On the perceptron’s compression. CoRR,
abs/1806.05403, 2018.
Albert B.J. Novikoff. On convergence proofs on perceptrons. In Proceedings of the Symposium on the Mathe-
matical Theory of Automata, volume 12, pages 615-622,1962.
Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In J. C. Platt, D. Koller,
Y. Singer, and S. T. Roweis, editors, Advances in Neural Information Processing Systems 20, pages 1177-
1184, 2008.
E.	Romero and R. Alquezar. Maximizing the margin with feedforward neural networks. In Proceedings of
the 2002 International Joint Conference on Neural Networks. IJCNN’02 (Cat. No.02CH37290), volume 1,
pages 743-748, 2002.
F.	Rosenblatt. The perceptron: A probabilistic model for information storage and organization in the brain.
Psychological Review, pages 65-386, 1958.
Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algorithms. Cam-
bridge university press, 2014.
Ohad Shamir. Distribution-specific hardness of learning neural networks. CoRR, abs/1609.01037, 2016.
Jure Sokolic, Raja Giryes, Guillermo Sapiro, and Miguel R. D. Rodrigues. Margin preservation of deep neural
networks. CoRR, abs/1605.08254, 2016.
Jure Sokolic, Raja Giryes, Guillermo Sapiro, and Miguel R. D. Rodrigues. Robust large margin deep neural
networks. IEEE Transactions on Signal Processing, 65, pages 4265-4280, 2017.
Le Song, Santosh Vempala, John Wilmes, and Bo Xie. On the complexity of learning neural networks. CoRR,
abs/1707.04615, 2017.
Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error guarantees for multi-
layer neural networks. 2016.
Shizhao Sun, Wei Chen, Liwei Wang, and Tie-Yan Liu. Large margin deep neural networks: Theory and
algorithms. CoRR, abs/1506.05232, 2015.
Matus Telgarsky. Representation Benefits of Deep Feedforward Networks. In JMLR, 49, pages 1 - 23, 2016.
Bogdan Wilamowski, David Hunter, and Aleksander Malinowski. Solving parity-n problems with feedforward
neural networks. In IJCNN, pages 2546 - 2551, 08 2003.
M Z. Arslanov, D U. Ashigaliev, and Esraa Ismail. N-bit parity ordered neural networks. Neurocomputing, 48:
1053-1056, 10 2002.
Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan Salakhutdinov, and Alexander
Smola. Deep sets, 2017.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes over-
parameterized deep relu networks. CoRR, abs/1811.08888, 2018.
A Appendix
Proof of Lemma 1
Proof. For all k ∈ A and x ∈ X of weight k,
∑ ∆i (k) ≥ ∆k(x) = σ(5 ∙0.5) - σ(-5 ∙0.5) > 0.84;
i∈A
10
Under review as a conference paper at ICLR 2020
the first inequality holds since ∆i(x) ≥ 0 for all i and x. For all k 6∈ A and x ∈ X of weight k,
∑∆ i α) = ∑ σ (5 ∙ (k - i + 0.5)) - σ (5 ∙ (k-i-0.5))
i∈A	i∈A
= ∑ σ (5 ∙ (k - i + 0.5))+ ∑ σ (5 ∙ (i + 0.5 - k))
k<i∈A	k>i∈A
+ ∑ [σ(5 ∙ (i + 0.5 - k)) - 1] + ∑ [σ(5 ∙ (k- i + 0.5)) - 1]
k<i∈A	k>i∈A
<	∑ σ (5 ∙ (k - i + 0.5))+ ∑ σ (5 ∙ (i + 0.5 - k))
k<i∈A	k>i∈A
∞
<	2 ∑ exp(5 ∙ (-i + 0.5))
i=1
= 2 exp(-2.5)/(1 - exp(-5)) < 0.17;
the first equality follows from the definition, the second equality follows from σ (5(x + 0.5)) - σ(5(x- 0.5)) =
σ(5(x+ 0.5)) + σ(5(-x + 0.5)) - 1 for all x, the first inequality neglects the negative sums, and the second
inequality follows because exp(ξ) > σ(ξ) for all ξ.
□
Proof of Lemma 2
Proof. The proof follows from two observations:
For all i the support of Γi(x) = ReLU(|x| - i+ 1) - 2 ReLU(|x| - i) + ReLU(|x| - i- 1) is i- 1 ≤ x ≤ i+ 1. So
for all x of weight not in A it holds ∑i∈A Γi(x) = 0.
For all i ∈ A and x of weight i it holds Γi(x) = 1.
□
Proof of Lemma 4
Proof. We are interested in the maximal distance the embedding of an element x ∈ X has moved from its initial
embedding:
ReLU(W(t)x+B(t)) -ReLU(W(0)x+B(0))
W(t) -W(0)RX+ B(t) - B(0)
≤k∑=t1hRX
W (k) -
W (k-1)
- B(k-1) i .
(6)
(7)
(8)

≤
To simplify equations (2)-(5) discussed above, we assume that during the optimization process the norm of the
weights W and B grow at a maximal rate:
W (t +1) -W(t) = y M(t)T ◦HW(t)x+B(t)xTh ≤ M(t) RX h,	(9)
IlB(t+1) -B(t)∣∣ = y (M(t))T ◦ H(W(t)X + B(t)) ⅛∣∣ ≤ ∣∣M(t)∣∣h;	(10)
here the norm of a matrix is the `2 -norm.
To bound these quantities, we follow the modified perceptron proof and add another quantity to bound. That is,
the maximal norm R(t) of the embedded space X at time t satisfies (by assumption RX > 1)
R(t+1) ≤R(t)+(1+R2X) ∣∣∣M(t)∣∣∣ h≤R(t) + 2R2X ∣∣∣M(t)∣∣∣ h;
We used that the spectral norm of a matrix is at most its '2-n0rm.
We assume a worst-case where R(t) grows monotonically at a maximal rate. By the modified perceptron algo-
rithm and choice β = R2h,
∣∣M(t)∣∣ ≤ Jt((R(t)h)2 + 2βh) ≤ √3R(t)h√t.
11
Under review as a conference paper at ICLR 2020
By choice of h ≤ (RRX and assuming t ≤ 20R2∕γ2,
R(t+1) ≤ R(t) + 2√3RXR(t)h2√t ≤ R(t) + 2√60R(t)γ4/R3.
1002
Solving the above recursive equation, it holds for all t ≤ 20R2∕γ2,
R(t)
t
R ≤ exp
40√60
1002
γ2∕R R ≤ 2R.
Now, summing equation 8, we have
卜 Vx) - VPb 2√6RhRt3/2,
since ∑t =1 √k ≤ t3/2.
So in 20R2∕γ2 updates, the elements embedded by the network travelled at most 〃〒00y6 Y2 ≤ 0.05γ2. Hence,
the samples the network received kept a margin of 0.9γ during training (by the assumption γ ≤ 1). By choice of
the loss function, SGD changes the output neuron as in the modified perceptron algorithm. By Theorem 2, the
number of updates is at most 2R∩+(2R) < 20R2∕γ2. So, the assumption on t We made during the proof holds.
.γ
□
The Modified Perceptron
Proof of Theorem 2. Denote by W * the optimal separating hyperplane with ∣∣ W *k = 1. It satisfies yiw * ∙ Xi ≥ γ
for all xi. By the definition,
w(t) ∙ w* = w(t-1) ∙ w* + yiw* ∙ Xi ≥ Yht
and
w(t) = w(t-1) +2yiw(t-1)Xih+ (∣Xi∣ h)2 ≤ 2βh+ (Rh)2 t.
By Cauchy-Schwarz inequality, 1 ≥ W (t) ∙ W */ ∣∣ w (t )∣∣. So the number of updates is bounded by
2β h +(Rh )2
(Yh )2	.
At time t the margin of any Xi that does not require an update is at least
ɪ ≥ , β .
∣∣w (t)∣∣	,(20 h +(Rh )2) t
The right hand side is monotonically decreasing function oft so by plugging in the maximal number of updates
we see that the minimal margin of the output is at least
Yβ h
20h +(Rh)2 ,
□
12
Under review as a conference paper at ICLR 2020
Figures
0.5
0.45
0.45
0.4
。 Training Layers 1+2 - Empirical Error
• Training Layers 1+2 - True Error
口 Training Only Layer 2 - Empirical Error
■ Training Only Layer 2 - True Error
0.4
0.35
O Training Layers 1+2 - Empirical Error
• Training Layers 1+2 - True Error
口 Training Only Layer 2 - Empirical Error
■ Training Only Layer 2 - True Error
0.35
0.3
0.3
0.25
-0.25
0.2
0.2
0.15
0.15
0.1
0.05
3
Epoch
6
×104
0.1
0.05
0.5
2
1	1.5
Epoch
2.5
×105
(a)	n = 30 input dimension
(b)	n = 60 input dimension
O O
O
0 l
0
1
2
4
5
0 l
0
Figure 2:	Error during training for a sample of size n.
O"5山
0.4 -
0.35
一 0.25
0.3
Training Layers 1+2 - Empirical Error
Training Layers 1+2 - True Error
Training Only Layer 2 - Empirical Error
Training Ony Layer 2 - True Error
00.5f
0.4
0.35
0.3
0.2
t 0.25
0.2
0.15
0.15
0.1
0.05
0.1
0.05
Tra
Tra
Tra
Tra
ng Layers 1+2 - Empirical Error
ng Layers 1+2 - True Error
ng Only Layer 2 - Empirical Error
ng Ony Layer 2 - TrUe Error
0.5 -
0.45；；
0.4 -
0.35
0.3
0 0.25
0.2
0.15
0.1
0.05
Training Layers 1+2 - Empirical Error
Training Layers 1+2 - True Error
Training Only Layer 2 - Empirical Error
Training Only Layer 2 - TrUe Error
%
, -UM	∏nLjIn. n n n 一，
0
0	2000	4000	6000	8000	10000 12000 14000 16000
Epoch
0 ----θɪ
0	1
2	3	4	5	6	7	8
Epoch	x104
0-l-l-1-CI Cl。心 8 8 6	。3。
0	1	23456789
EPOCh	M104
(a)	step size n-2
(b)	step size n-3
(c)	step size n-4
Figure 3:	Error during training for an input dimension and a sample of size n = 30.
0.6
0.5
0.4
0.3
0.2
0.1
O Training Layers 1+2 - Empirical Error
• Training Layers 1+2 - True Error
口 Training Only Layer 2 - Empirical Error
■ Training Only Layer 2 - True Error
0 l
0
500
1000	1500
Epoch
2000
2500
Figure 4:	β = 0: error during training for an input dimension and a sample of size n = 30.
13
Under review as a conference paper at ICLR 2020
0.8
0
2000
4000
6000
8000
10000
OUr Initialization - Empirical Error
OUr Initialization - True Error
Random Initialization - Empirical Error
Random Initialization - True Error
0.7
0.6
0.5
04
0.3
0.2
0.1
n-c
50
100
150
200
250
300
350
Our Initialization - Empirical Error
Our Initialization - True Error
Random Initialization - Empirical Error
Random Initialization - True Error
0 L
0
0.6
Epoch
1 2000
Epoch
400
(a) sample of size n
(b) sample of size n2
0.6
∏3γ二二
0.5,
04
E 0.3

母¾f⅛
0.2
0.1
0 l
0
200
400
0.5
0.4
0.3
0.2
0.6中
0.5∣∣
0.4
0.3
0.2
0.1
0 l
0
50
0.6
0.5
0.4
°
B
B
Our Initialization - Empirical Error
Our Initialization - True Error
Random Initialization - Empirical Error
Random Initialization - True Error
Our Initialization - Empirical Error
Our Initialization - True Error
Random Initialization - Empirical Error
Random Initialization - True Error
0.1
600
Epoch
800
1000
1200
0 l
0
5
10
15
Epoch
20
25
30
35
(c) sample of size n3
(d) sample of size n4
Figure 5: Parity: error during training for input dimension n = 20.
0.6
0.5
100
150
O Our Initialization - Empirical Error
• Our Initialization - True Error
Random Initialization - Empirical Error
Random Initialization - True Error
0,''
0.3
0.2
0.1
200
250
Epoch
300
350
400
450
0 l
0
20
40
60
80
Our Initialization - Empirical Error
Our Initialization - True Error
Random Initialization - Empirical Error
Random Initialization - True Error
100
Epoch
120
140
160
180
(a) sample of size n
(b) sample of size n2
0.6
iiBB≡BS≡∙-b.'.--・・・・・・・	■
d□dd□d d' ■
° d□ c
0.5
0.4
Our Initialization - Empirical Error
Our Initialization - True Error
Random Initialization - Empirical Error
Random Initialization - True Error
0.3
0.3
0.2
Our Initialization - Empirical Error
Our Initialization - True Error
Random Initialization - Empirical Error
Random Initialization - True Error
0.2
0.1
0 l
0
50
100
Epoch
150
200
250
0.1
0 l
0
5
10
Epoch
15
20
25
(c) sample of size n3
(d) sample of size n4
Figure 6: Random symmetric function: error during training for input dimension n = 20.
14
Under review as a conference paper at ICLR 2020
0.5 r
0.456
0.4 - 9
0.35
0.3
0.25
0.2
0.15
0.1
O Empirical Error
• True Error
0.05 -
0 , , , ∙ ,
0	1000	2000	3000	4000	5000	6000	7000
Epoch
Figure 7: Label error resistance. Labels of the sample were flipped with probability P =击.Sample
of size 10n whose input dimension is n = 30.
0.45'
0.4
O Empirical Error
• True Error
0.35
0.3
0.25
0.2 -
0.15 -
0.1 -
0
2000	4000	6000	8000	10000
Epoch
12000
Figure 8: Input Error resistance. All the entries of the vectors in the space were randomly shifted.
Sample of size 10n whose input dimension is n = 30.
15