Under review as a conference paper at ICLR 2020
Emergence of Compositional Language
with Deep Generational Transmission
Anonymous authors
Paper under double-blind review
Ab stract
Recent work has studied the emergence of language among deep reinforcement
learning agents that must collaborate to solve a task. Of particular interest are the
factors that cause language to be compositional—i.e. express meaning by com-
bining words which themselves have meaning. Evolutionary linguists have found
that in addition to structural priors like those already studied in deep learning, the
dynamics of transmitting language from generation to generation contribute sig-
nificantly to the emergence of compositionality. In this paper, we introduce these
cultural evolutionary dynamics into language emergence by periodically replac-
ing agents in a population to create a knowledge gap, implicitly inducing cultural
transmission of language. We show that this implicit cultural transmission en-
courages the resulting languages to exhibit better compositional generalization.
1	Introduction
Compositionality is an important structure of language that reflects a disentangled understanding
of the world - enabling the expression of infinitely many concepts using finitely many elements.
Agents that have compositional understandings of the world generalize in obviously correct ways
even in the face of limited training examples (Lake & Baroni, 2018). For example, an agent with
a compositional understanding of blue squares and purple triangles should also un-
derstand purple squares without directly observing any of them. Developing artificial agents
that can ground, understand, and produce compositional (and therefore more interpretable) language
could greatly improve generalization to new instances and ease human-AI interactions.
In building theories of how compositionality emerges in human languages, work in evolutionary
linguistics looks to the process of cultural transmission (Kirby, 2001; Kirby et al., 2008). Cultural
transmission of language occurs when a group of agents pass their language on to a new group
of agents, e.g. parents who teach their children to speak as they do. Because this education is
incomplete and biased, it allows the language itself to change over time via a process known as
cultural evolution. This paradigm (Kirby et al., 2014) explains the emergence of compositionality
as a result of expressivity and compressibility - i.e. to be most effective, a language should be
expressive enough to differentiate between all possible meanings (e.g., objects) and compressible
enough to be learned easily. Work in the evolutionary linguistics community has shown that over
multiple ‘generations’ these competing pressures result in the emergence of compositional languages
both in simulation (Kirby, 2001) and with human subjects (Kirby et al., 2008). These studies aim to
understand humans whereas we want to understand and design artificial neural networks.
Approaching the problem from another direction, recent work in AI has studied language emergence
in such multi-agent, goal-driven tasks. These works have demonstrated that agent languages will
emerge to enable coordination-centric tasks to be solved without direct or even indirect language
supervision (Foerster et al., 2016; Sukhbaatar et al., 2016; Lazaridou et al., 2017; Das et al., 2017).
However, the resulting languages are usually not compositional (Kottur et al., 2017) and are difficult
to interpret, even by other machines (Andreas et al., 2017). Some existing work has studied means
to encourage compositional language formation (Mordatch & Abbeel, 2018; Kottur et al., 2017), but
these settings study fixed populations of agents - i.e. examining language within a single generation.
In this work We bridge these two areas - examining the effect of generational cultural trans-
mission on the compositionality of emergent languages in a multi-agent, goal-driven setting.
1
Under review as a conference paper at ICLR 2020
(b) Implicit cultural transmission
Figure 1: We introduce cultural transmission into language emergence between neural agents. The
starting point of our study is a goal-oriented dialog task (similar to that of Kottur et al. (2017)),
summarized in Fig. 1a. During learning we periodically replace some agents with new ones (gray
agents). These new agents do not know any language, but instead of creating one they learn it from
older agents. This creates generations of language that become more compositional over time.
We study this in the context of a cooperative dialog-based reference game involving two agents
communicating in discrete symbols (Kottur et al., 2017); an example dialog is shown in Fig. 1a.
To examine cultural transmission, we extend this setting to a population of agents (Fig. 1b) and
introduce a simple mechanism to induce the expressivity and compressibility pressures inherent
in cultural transmission. Specifically, we periodically re-initialize some subset of the agents in
the population. In order to perform well at the task, the population’s emergent language must be
sufficiently expressive to reference all the objects (expressivity) and must be easily learnable by
these ‘new’ agents (compressibility). The new agents have a randomized language whereas the
surviving agents already know a grounded language. This “knowledge gap” creates an implicit
‘teaching’ setting that is analogous to the explicit transmission stage in models of iterative learning
(Kirby, 2001).
Through our experiments and analysis, we show that periodic agent replacement is an effective
way to induce cultural transmission and yields more compositionally generalizable language in our
setting. To summarize, our contributions are:
-	We propose a method for inducing implicit cultural transmission in neural language models.
-	We introduce new metrics to measure the similarity between agent languages and verify cultural
transmission has occurred as a result of our periodic agent replacement protocol.
-	We show our cultural transmission procedure induces compositionality in neural language models,
going from 13% accuracy on a compositionally novel test set to 46% in the best configuration.
Further, we show this is complementary with previous priors which encourage compositionality.
2	Task & Talk: A Testbed for Compositional Language Emergence
We consider the cooperative Task & Talk reference game introduced in Kottur et al. (2017). Shown in
Fig. 1a, the game is played by two agents - one who observes an attributed object - e.g. (purple,
solid, square) - and another who is given a task to retrieve a subset of these attributes over
the course of the dialog - e.g. (color,shape). The dialog itself consists of two rounds of agents
exchanging single-token utterances from fixed vocabularies. At the end of the dialog, the task-aware
agent must report the requested attributes and both agents are rewarded for correct predictions. This
causes a language grounded in the objects to emerge because there is no other way to solve the task.
A compositional solution to this task can look like a question-answer style dialog where the task-
aware agent queries the other for specific attributes (Fig. 1a) - e.g. uttering “X” requesting the
color to which the other agent replies “1” indicating purple. Importantly, this pattern would
persist regardless of the other attribute values of the object (e.g. for all (purple, *, *) ob-
jects). However, as there is no grounding supervision provided, agents must learn to associate
specific meanings to specific words and it is unlikely for compositional languages to emerge purely
by chance. Given a color task, an agent might use “1” for (purple, solid, square) and
then “2” for (purple, solid, circle). It is impossible for other agents to know that “2”
means purple without having seen (purple, solid, circle), so compositional language is
essential for generalization to compositionally novel instances.
2
Under review as a conference paper at ICLR 2020
Models. To formalize this setting, let Q-bot and A-bot be agent policies parameterized by neural
networks Q and A respectively. At each round t, Q-bot observes the task xQ and it’s memory of the
dialog so far htQ-1 and produces a single-token utterance mtQ ∈ V from the vocabulary V. Function-
ally, mtQ, htQ = Q(mtA-1, xQ, htQ-1) where mtA-1 is A-bot’s reply in the previous round. Likewise,
A-bot responds by computing mtA , htA = A(mtQ, xA , htA-1) where xA is the object instance repre-
sented symbolically by concatenating 3 one-hot vectors, one per attribute. After two rounds, Q-bot
must respond to the task, predicting the requested attribute pair U = U (xq, hQ) as a function of
the task and Q-bot’s final memory state. Both agents are rewarded if both attributes are correct (no
partial credit). We follow the neural network architectures of Q, A, and U from Kottur et al. (2017).
Measuring Compositional Generalization. Kottur et al. (2017) generated a synthetic dataset con-
sisting of three attribute types (color, shape, style) each with four values (e.g. red, blue, square, star,
dotted, solid, ...) and six tasks, one task for each ordered pair of different attribute types. In total,
this results in 64 unique instances and 384 task-instance pairs. To evaluate compositionality, Kottur
et al. (2017) held out 12 random instances for testing. Given the closed-world set of instances, these
12 are guaranteed to be never-before-seen triplets of attribute values; however, each individual value
has been seen in training in other triplets. As such, accuracy on this set is a measure of compositional
generalization.
Shortcomings of Kottur et al. (2017) Evaluation. In our investigations, we found some shortcom-
ings in the evaluation protocol of Kottur et al. (2017). First, the authors do not report variance over
multiple runs or different random test-sets which we found to be significant. Second, the strategy
of randomly selecting the test set can still reward some only partially compositional strategies. For
instance, suppose agents develop a language that uses single words to refer to attribute pairs like
(red, *, triangle) and (red, filled, *). Such agents might generalize to an un-
seen instance (red, filled, triangle) by composing the ‘paired’ words above instead of
disentangling individual attributes.
We make two modifications to address these issues. Our results are reported as means and variances
estimated from multiple training runs with different random seeds evaluated with 4-way cross-
validation. We also introduce a harder dataset where instead of withholding random individual
instances (e.g., (green,dotted,triangle),. . . ) as in Kottur et al. (2017), we withhold all
instances for a set of attribute pairs (e.g., (green,dotted,*),(red,solid,*),. . . ). We will
refer to datasets generated in this fashion as novel pair and the original dataset as novel instance.
We report on both settings for comparison, but find our new setting to be significantly more chal-
lenging in practice - requiring a stricter notion of compositionality that is more closely aligned with
human intuitions about these attributes.
3	Compositional Language Emergence with Cultural Transmission
In iterative learning models of cultural transmission from evolutionary linguistics, competing pres-
sures towards expressivity and compressibility have been shown to induce compositionality over
multiple ‘generations’ of language transfer (Kirby, 2001; Kirby et al., 2008). The goal-driven nature
of our reference game already encourages expressivity - agents must be able to refer to the objects
in order to succeed. To introduce compressibility pressure and parallel literature in evolutionary lin-
guistics, we introduce a population of agents which regularly has members replaced by new agents
that lack any understanding of the remaining population’s language. As this paradigm lacks explicit
teaching steps where new agents are trained to ground existing words, we consider this approach as
a means of implicit cultural transmission.
Populations of Agents. We consider a population of Q-bots {Q1, . . . , QNQ } and a population ofA-
bots {A1, . . . , ANA} with each agent having a different set of parameters. At each iteration during
learning, we sample a random Q-bot-A-bot pair to interact and receive updates - i.e. the red line
(2) in Alg. 1. As any Q-bot may be made to communicate with any A-bot, there is pressure for
the population to adopt a unified language. Likewise, when an agent is reinitialized it will receive
positive reward much more quickly when it happens to use language that its conversational partners
understand. Furthermore, ‘compressible’ languages that are easier to learn will result in greater
reward for the population in the face of periodic re-initialization of agents.
3
Under review as a conference paper at ICLR 2020
Algorithm 1: Training with Replacement and Multiple Agents
1	for epoch e = 1, . . . , Nepochs do
2	Sample Q-bot iQ from U{1, NQ} and A-bot iA from U{1, NA}
3	for xQ , xA , u in each batch do
4	for dialog rounds t = 1, . . . T do
5	mQ,hQ = QiQ (mtA1,χQ,htQ1)
6	_m[,hA = AiA (mQ"1,XA,hA-1)
7	U = UUQ(xQ, hQ)
8	Policy gradient update w.r.t. both Q-bot and A-bot parameters
9	if e mod E = 0 then
io ，Sample replacement set B under policy π and re-initialize all agents in B
11 return all Q-bots and A-bots.
Introducing multiple agents may in itself add compressibility pressure and improve generalizations
even without replacement (Raviv et al., 2018). Agents in a population have to model minor linguistic
differences between conversational partners given the same memory capacity. Further, each agent
provides another potential language variation that can be mimicked and PerpetUated-increasing lan-
guage diversity early in training. We examine these effects through no-replacement baselines, but
find that generational pressure where some agents know less than others can also be important for
compositionality in our setting.
Replacement. In order to create a notion of ‘generations’ we replace agents periodically. Let π be
some replacement strategy that returns a subset of the population. Every E epochs, we call π and
reinitialize the parameters and optimizers for the corresponding agents (blue lines 9-10 in Alg. 1).
We investigate three settings of π (appendix A.2 for more details):
-	Uniform Random. Sample an A-bot and Q-bot from uniform random distributions.
-	Epsilon Greedy. With probability 1-ε replace the A-bot and Q-bot with the lowest validation
accuracy. We use ε = 0.2 in our experiments.
-	Oldest. Replace the oldest A-bot and Q-bot, breaking ties with uniform random sampling.
4	Experimental Setting
Experimental Setting. We evaluate on both our modified Task & Talk dataset and the original
from Kottur et al. (2017), as described in Section 2. All results are reported as means and variances
computed from a total of 16 trials (four random seeds each with 4-way cross-validation). We report
accuracy based on Q-bot getting both elements of the task correct - corresponding to the more
restrictive “Both” setting from Kottur et al. (2017).
Kottur et al. (2017) examined a series of increasingly restrictive settings in order to study conditions
under which compositionality emerges. The primary variables are whether A-bot has memory (ab-
lated by setting htA=0) and the vocabulary sizes VQ and VA for Q-bot and A-bot respectively. For
comparison we also evaluate in these settings: Minimal Vocab ( VQ=3, VA=4). Memoryless +
Minimal Vocab (VQ=3, VA=4, htA=0), Overcomplete (VQ=VA=64). We also introduce Memo-
ryless + Overcomplete (VQ=VA=64, htA=0) to complete the cross product of settings and examine
the role of memory restriction in overcomplete vocabularies.
The Memoryless + Minimal Vocabulary setting results in the best compositional generalization;
however, this is an extreme setting - requiring not only that the minimum number of groundable
symbols be known but also that A-bot not be able to remember it’s previous utterance. While we
do report these settings and see quite large performance gains due to cultural transmission, we are
mainly interested in the more realistic Overcomplete setting where a large pool of possible tokens is
provided and both dialog agents remember the conversation.
Model and Training Details. Our A-bots and Q-bots have the same architectur as in Kottur et al.
(2017). All agents are trained with E = 25000, a batch size of 1000, 1 and the Adam (Kingma
& Ba, 2015) optimizer (one per bot) with learning rate 0.01. In the Multi Agent setting we use
1There is 1 batch per epoch because there are only 384 instances (64 objects × 6 tasks).
4
Under review as a conference paper at ICLR 2020
Figure 2: Test set accuracies (with standard deviations) are reported against our new harder dataset
using models similar to those in Kottur et al. (2017). Our variations on cultural transmission (darker
blue bars) outperform the baselines where language does not change over generations.
NA = NQ = 5. We stop training after 8 generations (199000 epochs Multi Agent; 39000 epochs
Single Agent). This differs from Kottur et al. (2017), which stopped once train accuracy reached
100%. Further, we do not perform negative mining.
Baselines. We consider a set of baseline setting to isolate the effect of our approach.
一 Single Agent Populations. We ablate the effect of multi-agent populations by training individ-
ual A-bot-Q-bot pairs (i.e. populations with NA = NQ = 1). We apply the uniform random
(either A-bot or Q-bot at random) and oldest (alternating between A-bot and Q-bot) replacement
strategies to these agents; however, the epsilon greedy strategy is not well-defined here. In this
setting we decrease E from 25000 to 5000 to keep the average number of gradient updates for
each agent constant with respect to the multi-agent experiments.
一 No Replacement. We also consider the effect of replacing no agents at all, but still allowing the
agents to train for the full 199,000 (39,000) epochs. Improvement over this baseline shows the
gains from our replacement strategy under identical computational budgets.
5	Results and Analysis
5.1	Impact of Cultural Transmission on Compositional Generalization
Results with standard deviations against our harder dataset are reported in Fig. 2. We compared
methods and models using dependent paired t-tests and reported the resulting p-values in Section
A.3. Result on the original Task & Talk dataset are in Section A.1.
Cultural transmission induces compositionality. Our main result is that cultural transmission ap-
proaches outperform baselines without cultural transmission. This can be seen by noting that for
each model type in Fig. 2, the 3 darker blue bars (Multi Agent Replacement approaches) are largest.
After running a dependent paired t-test against all pairs of baselines and cultural transmission ap-
proaches we find a meaningful difference in all cases (p ≤ 0.05). This is strong support for our
claim that our version of cultural transmission encourages compositional language because it causes
better generalization to novel compositions of attributes.
Next we go on to discuss some additional trends we hope the community will find useful.
Population dynamics without replacement usually lead to some compositionality. The Multi
Agent No Replacement policies usually outperform than the Single Agent No Replacement policies,
though the difference isn’t very significant in the except in the Overcomplete and Minimal Vocab
settings. This agrees with recent work from evolutionary linguistics, where multiple agents can lead
to compositionality without generational transmission Raviv et al. (2018).
Variations in replacement strategy tend to not affect performance. The Multi Agent Uniform
Random/Epsilon Greedy/Oldest replacement strategies are not largely or consistently different from
one another across model variations. This suggests that while some agent replacement needs to
occur, it is not critical whether agents with worse language are replaced or whether there is a pool of
similarly typed agents to remember knowledge lost from older generations. The main factor is that
new agents learn in the presence of others who already know a language.
Cultural transmission is complementary with other factors that encourage compositionality.
As in Kottur et al. (2017), we find the Memoryless + Small Vocab model is clearly the best. This
agrees with factors noted elsewhere Kottur et al. (2017); Mordatch & Abbeel (2018); Nowak et al.
(2000) and shows how many different factors can affect the emergence of compositionality.
5
Under review as a conference paper at ICLR 2020
Removing memory makes only minor differences. Removing memory makes no difference (neg-
ative or positive) in Single Agent settings, but it can have a relatively small effect in Multi Agent
settings, helping Small Vocab models and hurting Overcomplete models. While our approach is
complementary with minimizing vocab size to increase compositionality, its makes memory re-
moval less useful. As the Memoryless + Overcomplete setting has not been reported before, these
results suggest that the relationship between inter-round memory and compositionality is not clear.
Overall, these results show that adding cultural transmission to neural dialog agents improves the
compositional generalization of the languages learned by those agents in a way complementary to
other priors. It thereby shows how to transfer the cultural transmission principle from evolutionary
linguistics to deep learning.
5.2	Is Generational Transmission Occurring?
Because it is implicit, cultural transmission may not actually be occurring; improvements may be
from other sources. How can we measure cultural transmission? We focus on A-bots and take a
simple approach. We assume that if two A-bots ‘speak the same language’ then that language was
culturally transmitted. There is a combinatorial explosion of possible languages that could refer to
all the objects of interest, so if the words that refer to the same object for two agents are the same
then they were very likely transmitted from the other agents, rather than similar languages emerging
from scratch just by chance. This leads to a simple approach: consider pairs of bots and see if they
say similar things in the same context. If they do, then their language was likely transmitted.
More formally, consider the distribution of tokens A-bot Ai might use to describe its object xA when
talking to Q-bot Qk: pk,i (mtA |xA) or pk,i for short. We want to know how similar Ai’s language
is to that of another A-bot Aj. We’ll start by comparing those two distributions by computing the
KL divergence between them and then taking an average over context (objects, Q-bots, and dialog
rounds) to get our pairwise agent language similarity metric Dij :
Dij = EχA,k,t[DκL(Pk,i(mAAIxA),Pk,j(mA∣xa))]	(1)
Taking another average, this time over all pairs of bots (and also random seeds and cross-val folds),
gives our final measure of language similarity reported in Fig. 3.
D = Eijst i=j [Dij]	(2)
D is smaller the more similar language is between bots. Note that even though Dij is not symmetric
(because KL divergence is not), D is symmetric because it averages over both directions of pairs.
We compute D by sampling an empirical distribution over all messages and observations, taking
10 sample dialogues in each possible test state (xA , xQ) of the world using the final populations of
agents as in Fig. 2. Note that this metric applies to a group of agents, so we measure it for only the
Multi Agent settings, including two new baselines colored red in Fig. 3. The Single Agents Combined
baseline trains 4 Single Agent No Replacement models independently then puts them together and
computes D for that group. These agents only speak similar languages by chance, so D is high. The
Random Initialization baseline evaluates language similarity using newly initialized models. These
agents have about a uniform distribution over words at every utterance, so their languages are both
very similar and useless. For each model these baselines act like practical (not strict) upper and
lower bounds on D, respectively.
Fig. 3 shows this language dissimilarity metric for all our settings. As we expect, the paired Single
Agents are highly dissimilar compared to agents from Multi Agent populations. Further, all the
replacement strategies result in increased language similarity—although the degree of this effect
seems dependent on vocabulary setting. This provides some evidence that cultural transmission is
occurring in Multi Agent settings and is encouraged by the replacement strategy in our approach.
While all Multi Agent settings resulted in language transmission, our replacement strategies results
in more compositional languages due to repeated teaching of new generations of agents.
5.3	Visualizing Emergent Languages
In this section we visualize the language learned by agents at various stages of training to reinforce
our previous conclusions and build intuition. Each of the three sub-figures in Fig. 4 summarizes all
of the conversations between a particular pair of bots for the (shape, color) task. To see how
these summaries work, consider Fig. 4a. That sub-figure is divided into a 4 × 4 grid with 4 elements
6
Under review as a conference paper at ICLR 2020
indicate similar language. Populations evolved with our method speak similar languages, but inde-
pendently evolved agents do not. Thus our implicit procedure induces cultural transmission.
in each cell, so there is one element for each of the 64 possible objects. For this task, objects in
each row of the 4 × 4 grid have the same shape and objects in each column have the same color. To
the right of each object are the two tokens A-bot used to respond to Q-bot in the two dialog rounds.
Ideally they should indicate the color and shape of the object. Finally, the check-marks or Xs to the
right of A-bot’s utterances indicate whether Q-bot guessed correctly.
O 203	O 2 1 Q	O 22X	C1 225	C1 03β	O 02×	C1 o 1 ×	C- OOQ	O 03Q	C1 02×	O O 1 X	C, 00Q
:二 2 O 立	O 2 1 S	□ 2 1 X	O 22 O	二，03S	Q 02×	O 01 ×	Q ooQ	二，03S	二：1 02×	二，O 1 ×	O 00S
・ 20立	• 2 1 Q	>27 X	*225	∙θ30	∙θ2×	∙θ7X	ΦooQ	Φθ3S	∙θ2×	・ 01 X	・ 00立
C>20 出	O 2 7超	C>20X	。22四	。03立	C>02X	C>07X	OooD	。034	C>02X	C>07X	OoO厩
：-^J 1 3 Q	口 2 1 ×	二 11 S	匚1 2 Q	:二:230	二 22Q	:二:2 1 0	匚2 O0	:二:235	:二:22S	:二:2 1 Q	二 203
□ 1 3 3	..230	□ 11 S	□ 1 2 S	□ 235	□ 22Q	□ 2 1 3	□ 203	20×	□ 223	□ 2 1 0	□ 203
■ 1 3@	■ 233	■ 113	■ 1 2@	■ 23/	■ 22@	≡2 1 3	■ 200	■ 23S	■ 22S	■ 2 1 Q	■ 20S
□ 13&	□ 238	□ 7 7伤	□ 7 2⑨	□ 23"	□ 22厩	口27⑨	口20团	□ 23在	□ 22网	口2 7座	□ 20&
★ 2 1 X	2 1 ×	次2 1 X	☆ 02Q	☆T 3 Q	☆ 1 2 Q	☆ 1 1 0	☆ 1 O Q	☆ 1 3 3	☆ 1 2 Q	☆ 1 1 Q	☆ 1 0 Q
⅛ 2 1 ×	☆ 2 1 ×	☆ 2 1 X	☆ 023	☆ 13 a	☆ 1 2 S	☆ 1 1 Q	☆ 7 O S	☆ 1 3 S	☆ 1 2 8	☆ 1 1 0	☆ 7 0 S
★21X	★21 ×	★21X	★22X	★ 1 3θ	★ 1 2θ	★ 11 S	★ l 00	★ 1 30	★ 1 2Q	★ 11 a	★ 1 03
☆ 22X	☆22X	☆22X	☆22X	☆ l3。	☆ 72四	☆ l 1⑸	☆ 1 O以	☆ 13⑻	☆ 12田	☆ 11例	☆ 1 0厩
八' 03S	二'、3 1 S	\ 01 Q	八、03×	△ 338	八、3 20	八3 1 3	八、30×	八33S	\ 323	43 1 0	八30×
Δ 03@	Δ 33Q	Δ 0 1 Q	Δ 02×	Δ 33S	Δ 32β	Δ 3 1 0	Δ 30×	Δ 33S	Δ 32@	Δ 3 1 Q	Δ 30×
Aθ3Q	A33Q	AoiQ	▲ 02X	▲ 33。	▲ 32。	A31 Q	▲ 30X	A33S	▲ 32再	A3 1 Q	▲ 30X
△ 03⑨	△ 33居	△ 000	△ 02X	△ 330	△ 32厩	△ 3 10	△ 30X	△ 33⑨	△ 32厩	△ 3 1厩	△ 30X
Task: (shape, color)			Train Val Test	Task: (shape, color)			Train Val Test	Task: (shape, color)			Train Val Test
(a) Gen 1 (Single) - New A-bot
(b) Gen 8 (Multi) - New A-bot
(c) Gen 8 (Multi) - Old A-bot
Figure 4: Each sub-figure summarizes an A-bot’s language, as described in Section 5.3. By com-
paring the baseline of Fig. 4a to a similar pair of bots from our approach Section 4b we can see that
our approach encourages compositional language to emerge. Furthermore, the similarity between
Fig. 4b and Fig. 4c suggests language is indeed transmitted in our approach.
From left to right: Fig. 4a summarizes the single pair from a Single Agent No Replacement run
(3000 iterations old); Fig. 4b summarizes dialogs between an old Q-bot (about 23000 iterations) and
a recently re-initialized A-bot (about 3000 iterations) at the 8th and final generation of a Multi Oldest
run; Fig. 4c summarizes dialogs between the same old Q-bot as in Fig. 4b and an old A-bot (13000
iterations) from the same Multi Oldest experiment. Even though the A-bots in Fig. 4a and Fig. 4b
have trained for about2 the same number of iterations, the A-bot trained in the presence of other bots
which already know a functional language has already learned a somewhat compositional language
whereas the Single Agent A-bot has not. Furthermore, by comparing the old A-bot’s language
Fig. 4c with the new one Fig. 4b we can see that they are extremely similar. They even lead to the
same mistakes. This again suggests that language is transmitted between bots, in agreement with
our previous experiments.
6	Related work
Language Evolution Causes Structure. Researchers have spent decades studying how unique
properties of human language like compositionality could have emerged. There is general agree-
ment that people acquire language using a combination of innate cognitive capacity and learning
from other language speakers (cultural transmission), with the degree of each being widely dis-
puted Perfors (2002); Pinker & Bloom (1990). Both innate cognitive capacity and specific modern
2Due to the stochastic nature of our Multi Agent approach.
7
Under review as a conference paper at ICLR 2020
human languages like English co-evolved Briscoe (2000) via biological Pinker & Bloom (1990) and
cultural Tomasello (1999); Smith (2006) evolution, respectively.
In particular, explanations of how the cultural evolution of languages could cause structure like
compositionality are in abundance Nowak & Krakauer (1999); Nowak et al. (2000); Smith et al.
(2003); Brighton (2002); Vogt (2005); Kirby et al. (2014); Spike et al. (2017). An important piece of
the explanation of linguistic structure is the iterated learning model Kirby et al. (2014); Kirby (2001);
Kirby et al. (2008) used to motivate our approach. Indeed it shows that cultural transmission causes
structure in computational Kirby (2001; 2002); Christiansen & Kirby (2003); Smith et al. (2003)
and human Kirby et al. (2008); Cornish et al. (2009); Scott-Phillips & Kirby (2010) experiments.
Even though cultural transmission may aid the emergence of compositionality, recent results in
evolutionary linguistics Raviv et al. (2018) and deep learning Kottur et al. (2017); Mordatch &
Abbeel (2018) also emphasize other factors.
While existing work in deep learning has focused on biases that encourage compositionality, it has
not considered settings where language is permitted to evolve over generations of agents. We have
shown such an approach is viable and even complementary with other approaches.
Language Emergence in Deep Learning. Recent work in deep learning has increasingly focused
on multi-agent environments where deep agents learn to accomplish goals (possibly cooperative or
competitive) by interacting appropriately with the environment and each other. Some of this work
has shown that deep agents will develop their own language where none exists initially if driven by a
task which requires communication Foerster et al. (2016); Sukhbaatar et al. (2016); Lazaridou et al.
(2017). Most relevant is work which focuses on conditions under which compositional language
emerges as deep agents learn to cooperate Mordatch & Abbeel (2018); Kottur et al. (2017). Both
Mordatch & Abbeel (2018) and Kottur et al. (2017) find that limiting the vocabulary size so that there
aren’t too many more words than there are objects to refer to encourages compositionality, which
follows earlier results in evolutionary linguistics Nowak et al. (2000). Follow up work has continued
to investigate the emergence of compositional language among neural agents, mainly focusing on
perceptual as opposed to symbolic input and how the structure of the input relates to the tendency
for compositional language to emerge Choi et al. (2018); Havrylov & Titov (2017); Lazaridou et al.
(2018). Other work has shown that Multi Agent interaction leads to better emergent translation Lee
et al. (2018), but it does not measure compositionality.
Cultural Evolution and Neural Nets. Somewhat recently, Bengio (2012) suggested that cultur-
ally transmitted ideas may help in escaping from local minima. Experiments in GuIcehre & Bengio
(2016) support this idea by showing that supervision of intermediate representations allows a more
complex toy task to be learned. Unlike our work, these experiments use direct supervision provided
by the designed environment rather than indirect and implicit supervision provided by other agents.
TWo concurrent works examine the role of periodic agent replacement on language emergence 一
albeit in different environments. In Li & Bowling (2019) replacement is used to encourage languages
to be easy to teach, and this in turn causes compositionality. In Dagan et al. (2019) neural language
is transmitted through a bottleneck caused by replacement. The resulting language has increased
efficiency and effectiveness, with further results showing that co-evolving the agents themselves
with the language amplifies the effect. Both of these works support our central observations.
7	Conclusion
In this work we investigated cultural transmission in deep neural dialog agents, applying it to lan-
guage emergence. The evolutionary linguistics community has long used cultural transmission to
explain how compositional languages could have emerged. The deep learning community, having
recently become interested in language emergence, has not investigated that link until now. Instead
of explicit models of cultural transmission familiar in evolutionary linguistics, we favor an implicit
model where language is transmitted from generation to generation only because it helps agents
achieve their goals. We show that this does indeed cause cultural transmission and compositionality.
Future work. While our work used an implicit version of cultural transmission, we are interested
in investigating the effect of explicit versions of cultural transmission on language structure. In
another direction, cultural transmission may also provide an appropriate prior for neural representa-
tions of non-language information.
8
Under review as a conference paper at ICLR 2020
References
Jacob Andreas, Anca D. Dragan, and Dan Klein. Translating neuralese. In ACL, 2017.
Yoshua Bengio. Evolving culture vs local minima. CoRR, abs/1203.2990, 2012.
Henry Brighton. Compositional syntax from cultural transmission. Artificial Life, 8:25-54, 2002.
Ted Briscoe. Grammatical acquisition: Inductive bias and coevolution of language and the language
acquisition device. In Language, volume 76. Linguistic Society of America, 2000.
Edward Choi, Angeliki Lazaridou, and Nando de Freitas. Compositional obverter communication
learning from raw visual input. In International Conference on Learning Representations (ICLR),
2018.
Morten H. Christiansen and Simon Kirby. Language evolution: consensus and controversies. Trends
in cognitive sciences, 7 7:300-307, 2003.
Hannah Cornish, Monica Tamariz, and Simon Kirby. Complex adaptive systems and the origins of
adaptive structure: What experiments can tell us. Language Learning, 59(s1):187-205, 2009. doi:
10.1111/j.1467-9922.2009.00540.x. URL https://onlinelibrary.wiley.com/doi/
abs/10.1111/j.1467-9922.2009.00540.x.
Gautier Dagan, Dieuwke Hupkes, and Elia Bruni. Co-evolution of language and agents in referen-
tial games. pre-print, 2019. URL https://eliabruni.github.io/publications/
dagan2019coevolution.pdf.
AbhiShek Das, Satwik Kottur, JoSe M.F. Moura, Stefan Lee, and DhrUv Batra. Learning cooperative
visual dialog agents with deep reinforcement learning. In Proceedings of the IEEE International
Conference on Computer Vision (ICCV), 2017.
Jakob Foerster, Ioannis Alexandros Assael, Nando de Freitas, and Shimon Whiteson. Learning to
communicate with deep multi-agent reinforcement learning. In Advances in Neural Information
Processing Systems, pp. 2137-2145, 2016.
Caglar GUlcehre and Yoshua Bengio. Knowledge matters: Importance of prior information for
optimization. Journal of Machine Learning Research, 17:8:1-8:32, 2016.
Serhii Havrylov and Ivan Titov. Emergence of language with multi-agent games: Learning to com-
municate with sequences of symbols. In NIPS, 2017.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations (ICLR), 2015.
Simon Kirby. Spontaneous evolution of linguistic structure-an iterated learning model of the emer-
gence of regularity and irregularity. IEEE Trans. Evolutionary Computation, 5:102-110, 2001.
Simon Kirby. Natural language from artificial life. In Artificial Life, 2002.
Simon Kirby, Hannah Cornish, and Kenny Smith. Cumulative cultural evolution in the laboratory:
An experimental approach to the origins of structure in human language. Proceedings of the
National Academy of Sciences, 2008. ISSN 0027-8424. doi: 10.1073/pnas.0707835105. URL
https://www.pnas.org/content/early/2008/07/29/0707835105.
Simon Kirby, Tom Griffiths, and Kenny Smith. Iterated learning and the evolution of language.
Current Opinion in Neurobiology, 28:108-114, 2014.
Satwik Kottur, Jose M. F. Moura, Stefan Lee, and Dhruv Batra. Natural language does not emerge
’naturally’ in multi-agent dialog. In EMNLP, 2017.
Brenden M. Lake and Marco Baroni. Generalization without systematicity: On the compositional
skills of sequence-to-sequence recurrent networks. In ICML, 2018.
9
Under review as a conference paper at ICLR 2020
Angeliki Lazaridou, Alexander Peysakhovich, and Marco Baroni. Multi-agent cooperation and
the emergence of (natural) language. In International Conference on Learning Representations
(ICLR), 2017.
Angeliki Lazaridou, Karl Moritz Hermann, Karl Tuyls, and Stephen Clark. Emergence of linguistic
communication from referential games with symbolic and pixel input. In International Confer-
ence on Learning Representations (ICLR), 2018.
Jason D. Lee, Kyunghyun Cho, Jason Weston, and Douwe Kiela. Emergent translation in multi-
agent communication. CoRR, abs/1710.06922, 2018.
Fushan Li and Michael Bowling. Ease-of-teaching and language structure from emergent commu-
nication. CoRR, abs/1906.02403, 2019. URL http://arxiv.org/abs/1906.02403.
Igor Mordatch and Pieter Abbeel. Emergence of grounded compositional language in multi-agent
populations. In AAAI, 2018.
Martin A. Nowak and David C. Krakauer. The evolution of language. Proceedings of the National
Academy of Sciences of the United States ofAmerica, 96 14:8028-33, 1999.
Martin A. Nowak, Joshua B. Plotkin, and Vincent A A Jansen. The evolution of syntactic commu-
nication. Nature, 404:495-498, 2000.
Amy Perfors. Simulated evolution of language: a review of the field. J. Artificial Societies and
Social Simulation, 5, 2002.
Steven Pinker and Paul Bloom. Natural language and natural selection. Behavioral and brain
sciences, 13(4):707-727, 1990.
Limor Raviv, Antje Meyer, and Shiri Lev-Ari. Compositional structure can emerge without genera-
tional transmission. Cognition, 182:151-164, 2018.
Thomas C. Scott-Phillips and Simon Kirby. Language evolution in the laboratory. Trends in cogni-
tive sciences, 14 9:411-7, 2010.
Kenny Smith. Cultural evolution of language. Encyclopedia of Language and Linguistics 2 Edition,
2:315-322, 2006.
Kenny Smith, Simon Kirby, and Henry Brighton. Iterated learning: A framework for the emergence
of language. Artificial Life, 9:371-386, 2003.
Matthew Spike, Kevin Stadler, Simon Kirby, and Kenny Smith. Minimal requirements for the emer-
gence of learned signaling. In Cognitive Science, 2017.
Sainbayar Sukhbaatar, Arthur Szlam, and Rob Fergus. Learning multiagent communication with
backpropagation. In NIPS, 2016.
Michael Tomasello. The cultural origins of human cognition. Harvard university press, 1999.
Paul Vogt. The emergence of compositional structures in perceptually grounded language games.
Artificial intelligence, 167(1-2):206-242, 2005.
10
Under review as a conference paper at ICLR 2020
A Appendix
A.1 Results on Single Held Out Attribute Dataset of Kottur et al. (2017)
In Section 4 we proposed a new harder compositional dataset different from the one in Kottur et al.
(2017). For comparison, in this section we train and evaluate our models on the original dataset
from Kottur et al. (2017) to show that our approach also improves still improves compositionality in
this setting and to show that our new dataset is indeed harder.
Figure 5: Test set accuracies (with standard deviations) are reported by training and evaluating the
same models as in our main results Fig. 2 against the dataset from Kottur et al. (2017). These results
do not perform cross-validation, following Kottur et al. (2017). They only vary across 4 different
random seeds. This dataset is significantly easier than our new dataset, as indicated by the Our
proposed approach still outperforms models without replacement or multiple agents.
A.2 Replacement Strategies
Our approach to cultural transmission periodically replaces agents by re-initializing them. The ap-
proach section outlines various replacement strategies (policy π), but does not detail their imple-
mentation. We do so here.
These strategies depend on a number of possible inputs:
•	e the current epoch
•	E the period of agent replacement
•	viQ /viA the validation accuracy of agent i for Q-bots/A-bots. For Q-bots this is averaged
over all potential A-bot partners, and vice-versa for A-bots.
•	aiQ/aiA the age in epochs of agent i for Q-bots/A-bots
Single Agent strategies are given in Alg. 2 and Alg. 3. Multi Agent strategies are given in Alg. 4,
Alg. 5, and Alg. 6. Note that Single Agent strategies always replace one agent while Multi Agent
strategies always replace one Q-bot and one A-bot. An additional Replace All baseline strategy is
given in Alg. ?? and generalizes to both Single and Multi Agent cases.
Algorithm 2: Single Agent - Random Replacement
ι d 〜U{0,1}
2 if d = 0 then
3	1 return { A-bot }
4	else
5	1 return { Q-bot }
A.3 Detailed Results
In our experiments we compare models and we compare replacement strategies. We ran dependent
paired t-tests across random seeds, cross-val folds, and replacement strategies to compare models.
We ran dependent paired t-tests across random seeds, cross-val folds, and models to compare re-
placement strategies. The p-values for all of these t-tests are reported here.
Replacement strategy comparisons are in Fig. 7 (Single Agent) and Fig. 8 (Multi Agent). Model
comparisons are in Fig. 6.
11
Under review as a conference paper at ICLR 2020

Figure 6: Replacement strategy comparison p-values.
12
Under review as a conference paper at ICLR 2020
Single No Replacement
Single Random
-0.75
-0.60
-0.45
-0.30
-0.15
I (-UJOPUBa ①-6US -φ48-dluoD-J3>o + SS ①-A-IOUJBW-)
(∙EOPUΠ5Q≤①-6u∞'-3 9-dtuoo,JΘ>o-)
(-EOPUeH ①一 6u∞- Jqs0> _BEWW-)
(-1UOPUe□≤①-6Us Jqeoo> -(DUJ'≡M + SS ①-A-JOUJBW-)
Single Oldest
0.0069	0.21
IΓuφlu ① u(ŋ-d ①工 ON ①-6u∞"φ4①-dluou-J① >0 + SS ①›OE ① W-)
0.60
0.0055	0.024
0.14
-0.45
-0.30
-0.15
0.68
0.024
ɪCU e ujəue-d ①工
ON ①-6W s"φ⑥ dEOU」① >0-)
ɪCU ①一uəue-d ① 」
ON JqSo >75EMW-)
ɪruəluəue-d ① 」
ON ①-6uω- -qeuo> -tuE⊂z+ SS ①-A-JOUJeW-)
IΓs ① P-O ①-6US --φ⅞-⊂E0yφ>0 + SS ①›OE ① W-)
— (-⅛① P_0 ①一6u∞∙、卫&dEOU」① >0-)
— (口 S ① pδ①一 6u∞- Jqs0> -BUJ-EW-)
(y ① P_0 ①一 Jq8o> ∕e'≡m + SS ①-A-JOUJBW-)
Single Agent model comparison p-values.
Figure 7:
13
Under review as a conference paper at ICLR 2020
Multi Uniform Random
-0.40	I
-0.32
0.00027	0.0016	0.00052
0.00027	0.027	0.055
-0.24
cm -	0.42
— (-EOPUea C ɔ≡nlΛI- -①⑥ dEozυo>o
-0.40
-0.32
-0.24
-0.16
-0.08
(-4u3Eydφα ON ≤⊃Σ- ≤so> -EEC一Z+ sso›o E① W-)
(-4u0lEαlu6dαlc≤ON≡nw'-qCDuo>ωE EM-)
Eu① EBUQ-d①H ON _ 45W- J£@dEOU」① >O-)
(-EOPUeH E-JOt:u∩≡nlΛI>qgo> 石 UJC一 IΛ1-)
I (-UJOPUroH EJO⅛=U∩ 一 4-nw--φ4①-dLU0zυω>0-)
Multi Epsilon Greedy
0.02
0.073
0.038
0.097
0.02
0.073	0.097
0.13
≡n∑- --əaə-dLUOu-lθ>o + ssə-æioUJəw-)
Wnw-.'ələo.EOΞUΘ>O-)
≡nlΛI- --qeuo> 一 eE'≡一 W-)
I (-apəə.io Uo--Sd山
ɪ (>PΘQ-I5 UO=Sd 山
9
4 — (-AP ①①」UUo=Sd 山
ɪ (-APeaJO Uo=Sd 山≡nlΛI- JqeU0> -BE-EIAI + SS ①-A,JoUJ ① W-)
-0.48
-0.40
-0.32
-0.24
-0.16
0.00048
13
6
Multi Oldest
0.00048	0.0064	0.00036
0.0064	0.021
0.021
0.00036	0,0022
rsəpoWnw- --qeuo> -BLU 一 + ssəg OEəw-)
— (-⅛ωpδ≡nw- --qeuo> NE - u∑-)
esəpo≡nw--φlθ-dEOWΘ>o-)
0.92
0.0022
Figure 8: Multi Agent model comparison p-values.
14
Under review as a conference paper at ICLR 2020
1
2
3
4
5
1
2
3
1
2
3
4
5
6
7
8
9
1
2
3
4
Algorithm 3: Single Agent - Alternate Replacement
Input: e
if be/Ec = 0 then
L return { A-bot }
else
L return { Q-bot }
Algorithm 4: Multi Agent - Uniform Random Replacement
iA ~ U{1, Na}
iQ ~ U{1, NQ}
return { A-bot iA, Q-bot iQ }
Algorithm 5: Multi Agent - Epsilon Greedy Replacement
Input: vQ∀i, VA∀i, ε ∈ [0,1) (usually 0.2)
d ~ U [0,1)
if d < ε then
iA 〜U{1, Na}
JQ 〜U{1, NQ}
else
iA = argmini viA (unique in our experiments)
iQ = argmi□i VQ (unique in our experiments)
return { A-bot iA, Q-bot iQ }
Algorithm 6: Multi Agent - Oldest Replacement
Input: aiQ∀i, aiV ∀i
iA = U {argmaxi aiA }
iQ = U {argmaxi aiQ }
return { A-bot iA, Q-bot iQ }
15