Under review as a conference paper at ICLR 2020
Robust Reinforcement Learning with Wasser-
stein Constraint
Anonymous authors
Paper under double-blind review
Ab stract
Robust Reinforcement Learning aims to find the optimal policy with some ex-
tent of robustness to environmental dynamics. Existing learning algorithms usu-
ally enable the robustness though disturbing the current state or simulated en-
vironmental parameters in a heuristic way, which lack quantified robustness to
the system dynamics (i.e. transition probability). To overcome this issue, we
leverage Wasserstein distance to measure the disturbance to the reference transi-
tion kernel. With Wasserstein distance, we are able to connect transition kernel
disturbance to the state disturbance, i.e. reduce an infinite-dimensional optimiza-
tion problem to a finite-dimensional risk-aware problem. Through the derived
risk-aware optimal Bellman equation, we show the existence of optimal robust
policies, provide a sensitivity analysis for the perturbations, and then design a
novel robust learning algorithm—Wasserstein Robust Advantage Actor-Critic al-
gorithm (WRAAC). The effectiveness of the proposed algorithm is verified in the
Cart-Pole environment.
1	Introduction
Robustness to environmental dynamics is an important topic in safe Reinforcement Learning. Take
autonomous vehicle as an example. Autonomous vehicles have to adapt the complex real-world sit-
uations, but usually it is unlikely to cover all scenarios during training in real-world environments.
To handle this issue, typically, a simulated environment are employed to help build a driving agent,
however, the gap between the training and target environments makes the strategies trained with
simulated environments sub-optimal to the real-world scenarios (Mannor et al., 2004; 2007). Learn-
ing robust policies from simulated environments is a challenging problem for safe Reinforcement
Learning.
For robust Reinforcement Learning algorithms, existing methods lie on two branches: One type
of methods, borrowed from game theory, introduces an extra agent to disturb the simulated envi-
ronmental parameters during training (Atkeson & Morimoto, 2003; Morimoto & Doya, 2005; Pinto
et al., 2017; Rajeswaran et al., 2016). This method has to rely on the environmental characterization.
The other type of methods disturbs the current state through Adversarial Examples (Huang et al.,
2017; Kos & Song, 2017; Lin et al., 2017; Mandlekar et al., 2017; Pattanaik et al., 2018), which
is more heuristic. Unfortunately, both methods are lack of theoretical guarantee to the robustness
extent of transition dynamics.
To address these issues, we design a Wasserstern constraint, which restricts the admissible transition
probabilities within a Wasserstein ball centered at some reference transition dynamics. By apply-
ing the strong duality of Wasserstein distance (Santambrogio, 2015; Blanchet & Murthy, 2019),
we are able to connect the disturbance on transition dynamics with the disturbance on the current
state. As a result, the original infinite-dimensional robust optimal problem is reduced to some finite-
dimensional ordinary risk-aware RL problem. Through the moderated optimal Bellman equation,
we prove the existence of robust optimal policies, provide the theoretical analyse on the performance
of optimal policies, and design a corresponding —Wasserstein Robust Advantage Actor-Critic algo-
rithm (WRAAC), which does not depend on the environmental characterization. In the experiments,
we verified the robustness and effectiveness of the proposed algorithms in the Cart-Pole environ-
ment.
1
Under review as a conference paper at ICLR 2020
The remainder of this paper is organized as follows. In Section 2, we briefly introduce some related
work in Markov Decision Processes. In Section 3, we mainly describe the framework of Wasserstein
robust Reinforcement Learning. In Section 4, we propose robust Advantage Actor-Critic algorithms
according to the moderated robust Bellman equation. In Section 5, we perform experiments on the
Cart-Pole environment to verify the effectiveness of our method. Finally, Section 6 concludes our
study and provide possible future works.
2	Related Work
In this section, we introduce some related work in the fields of MDPs. In robust MDP, the set of all
possible transition kernels is called uncertainty set, which can be defined in various ways: one choice
could be likelihood regions or entropy bounds of the environment parameters (White III & Eldeib,
1994; Nilim & El Ghaoui, 2005; Iyengar, 2005; Wiesemann et al., 2013); another choice is to con-
strain the deviation from a reference environment through some statistical distance. For example,
Osogami (2012) discussed such robust problem where the uncertainty set are defined via Kullback-
Leibler divergence, and also uncover the relations between robust MDPs using f -divergence con-
straint and risk-aware MDPs.
Indeed, it was observed that since the robust MDP framework ignores probabilistic information of
the uncertainty set, it can provide conservative solutions (Delage & Mannor, 2010; Xu & Mannor,
2007). Some papers consider bringing prior knowledge of dynamics to robust MDPs, and name such
problem distributionally robust MDPs. Xu & Mannor (2010) discuss robust MDPs with prior infor-
mation to estimate the confidence region of parameters abound, which is a moment-based constraint,
and they also show that such distributionally robust problems can be reduced to standard robust MDP
problems. Yang (2017; 2018) use Wasserstein distance to evaluate the difference among the prior
distributions of transition probabilities. However, Yang’s algorithms are not appropriate for complex
situations, because they need to estimate enough transition kernels to approximate prior distribution
at each step.
3	Wasserstein robust reinforcement learning
In this section, we specify the problem of interest, which is actually a minimax problem constrained
by some Wassserstein-based uncertainty set. We start with introducing a general theoretical frame-
work, i.e., robust Markov Decision Process, and then briefly recall the definition of Wasserstein
distance between probability measures. Inspired by the strong duality brought by Wasserstein-based
uncertainty set, the robust MDP is reformulated to some risk-aware MDP, making connections clear
between robustness to dynamics and robustness to states.
3.1	Robust Markov Decision Process
Unlike ordinary Markov Decision Processes (MDPs), in robust MDP, environmental dynamics, in-
cluding transition probabilities and rewards, might change over time (Nilim & El Ghaoui, 2004;
2005). Theoretically, such dynamics can be treated as stochastic changes within an uncertainty set.
The objective of robust MDP is to find the optimal policy under the worst dynamics.
Given discrete-time robust MDPs with continuous state and action spaces, without loss of general-
ization, we only consider the robustness to transition probabilities. Basic elements of robust MDPs
include (X , A, Q, c), where
•	X : state space, which is a Borel measurable metric space.
•	A: action space, which is a Borel measurable space. Let A(x) ∈ A represent all the
admissible actions at state x ∈ X, and KA denote all the possible state-action pairs, i.e.,
KA = {(x, a) : x ∈ X , a ∈ A(x)}.
•	Q: the uncertainty set that contains all possible transition kernels.
•	c: KA → R, the immediate cost function. Generally we assume it is continuous and
C ∈ [0,日 for some non-negative constant c.
2
Under review as a conference paper at ICLR 2020
The robust system evolves in the following way. Let n ∈ N denote the current time and
xn ∈ X the current state. Agent chooses an action an ∈ A(xn) and environment selects a
transition kernel qn from the uncertainty set Q , respectively. Then at the next time n + 1, an
agent observes an immediate cost c(xn , an) and a new state xn+1 ∈ X which follows the dis-
tribution qn(∙∣Xn,an). The process repeats at each stage and produces trajectories in a form of
ω = (x0,α0,qo,c0,xι,aι,qι,cι,…).Let Ω = (XXAXQX [0, c])∞ denote all the trajecto-
ries. Let Ωn = {ωn = (xo, a0, qo, co, xι,aι,qι, ci,…，Xn)} denote all trajectories UP to time n
and Ωn = {ωn = (x0, a0, qo, co, xi, ai, qι,cι,…,xn an)} denote all trajectories UP to time n With
action an .
Correspondingly, a randomized policy is a series of stochastic kernels: π = (πo, πi, π2, ...) Where
∏n(∙∣ωn) is a probability measure over A(xn). We name ∏ primal policy and use Π to represent
all such randomized policies. If ∏n(∙∣ωn) = ∏n(∙∣Xn) for n ≥ 0, we say the policy is Markov. If
∏n ≡ ∏o for any n ≥ 0, this policy is stationary. If there exists measurable functions f : Ωn → A
such that ∏n(fn(ωn)∣ωn) ≡ 1, n ≥ 0, this policy is called deterministic. We denote the set of all
such deterministic, stationary, Markov policies by F.
The selection of transition kernels can be treated as a deterministic policy deployed by a secondary
adversarial agent. Let g = (go, gi, g2,…)with gn : Ωn → Q denote the adversarial policy. We use
G to represent all such deterministic policies. Similarly, if gn(∙∣ωn) = gn(∙∣Xn, an for all n ≥ 0,
the policy is Markov, and if gn ≡ go for any n ≥ 0, the policy is stationary.
Given the initial state Xo = x ∈ X, primal policy π ∈ Π and adversarial policy g ∈ G, applying the
Ionescu-Tulcea theorem (Hernandez-Lerma & Lasserre, 2012a; Bertsekas & Shreve, 2004), there
exist a probability measure Pxπ,g on trajectory space. Let Eπx,g denote the corresponding expectation
operation.
As for the performance criterion, we consider the infinite-horizon discounted cost. Let γ ∈ (0, 1)
be the discounting factor. The discounted cost contributed by trajectory ω ∈ Ω is CY(ω) =
Σn∞=oγnc(xn, an). Given the initial state xo = x, policies π and g, the expected infinite-horizon
discounted cost is
Cγπ,g(x) := Exπ,g[Σn∞=oγnc(xn, an)].	(1)
Robust MDPS aim to find the optimal policy ∏* for the agent under the worst realization of g ∈ G,
which means that ∏* reaches
inf sup Cγπ,g (x).	(2)
πg
This minimax problem can be seen as a zero-sum game of two agents.
3.2	Wasserstein Distance
The popular Wasserstein distance is a special case of optimal transport costs, which measures the
discrepancy between two probabilities in terms of minimum total costs associated with some trans-
port function. For any two probability measures Q and P over the measurable space (X, B(X)), let
Ξ(Q, P) denote the set of all joint distributions on X X X with Q and P are respective marginals.
Each element in Ξ(Q, P) is called a coupling between Q and P. Let κ : X X X → [0, ∞) be
the transport cost function between two positions, which is non-negative, lower semi-continuous
and satisfy κ(z, y) = 0 if and only if z = y. Intuitively, the quantity κ(z, y) specifies the cost of
transporting unit mass from z in X to another element y of X. Then the optimal transport total cost
associated with κ is defined as follows:
Dκ(Q,P) :=	inf	κ(z, y)dξ(z, y) .
ξ∈Ξ(Q,P)	X×X
Therefore, the optimal transport cost Dκ(Q, P) corresponds to the lowest transport cost that can be
obtained among all couplings between Q and P . Let the transport cost function κ be some distance
metric d on X, and then it is actually the Wasserstein distance of first order. Wasserstein distance of
order p is defined as:
1
Wp(Q,P)：=	暝、[d	d(z,y)pdξ(z,y)∖ , p ≥ 1.
ξ∈Ξ(Q,P)	X×X
3
Under review as a conference paper at ICLR 2020
Unlike Kullback-Liebler divergence or other likelihood-based divergence measures, Wasserstein
distance is a proper metric on the space of probabilities. More importantly, Wasserstein distance
does not restrict probabilities to share the same support (Villani, 2008; Santambrogio, 2015). Let
d(z, y) =k Z — y ∣∣2, κ(z, y) = P k Z — y ∣∣p and δ = Pep, the E-Wasserstein ball of order P and the
δ-optimal-transport ball are identical:
{Q ： Wp(Q,P) ≤ e} = {Q ： Dκ(Q,P) ≤ δ}.
Due to its superior statistical properties, Wasserstein-based uncertainty set has recently received a
great deal of attention in DRSO problem (Gao & Kleywegt, 2016; Esfahani & Kuhn, 2018; Blanchet
& Murthy, 2019), adversarial example (Sinha et al., 2017), and so on. We will apply it to robust RL.
3.3	Main Result
Let the uncertainty set Q be a E-Wasserstein ball of order p centered at some reference transition
kernel P :
Q ={Q :	Wp(Q(∙∣x, a), P(∙∣x,a))	≤ 3	∀(x, a)	∈	Ka}	(3)
={Q :	Dκ(Q(∙∣x,a),P(∙∣x, a))	≤ δ,	∀(x,a)	∈	Ka},	(4)
The radius E or δ reflects the extent of adversarial perturbation to the reference transition kernel P .
The difference between our theoretical framework and Yang (2017; 2018) is that our framework is
trying to find the optimal solution for the worst transition kernel within the Wasserstein ball, while
theirs is trying to find the optimal solution for the worst distribution over transition kernels.
Recall the state value function (1) at state x0 given primal policy π and adversarial policy g, we can
rewrite the state value function as follows,
Cγπ,g(x0) =Exπ0,g[Σn∞=0γnc(xn,an)]
=EX0~π,q0 [c(xo, ao) + EXOgχ0,ao)出占7屋& an)]]
=EX0~π,q0 [c(xo, ao) + Y [	qo(dxι∣xo, ao)CY1)π,(1)g(xι)],
x1∈X
where (1)π = (π1, π2, ...) and (1)g = (g1, g2, ...) are the shift policies. Since c is continuous and
bounded, the value function is actually continuous in X and belongs to [0, p-γ].
Let U : X → R be a measurable, upper semi-continuous function with U ∈ [0, p—γ], and let U
denote the set of all such functions. For state x ∈ X and action a ∈ A(x). Consider the following
operator H a defined on U:
(HaU)(x) := c(x, a) + sup γ
Q∈Q
y∈X
Q(dy|x, a)U(y).
(5)
Applying Lagrangian method and the strong duality property brought by Wasserstein dis-
tance (Blanchet & Murthy, 2019), we reformulate (5) to the following form:
(HaU)(x) = inf c(x, a) + γλδ + γ	P(dy|x, a)[ sup (U(Z) — λκ(Z, y))].
λ≥0	y∈X	z∈X
(6)
The significance of this strong dual representation lies on the fact that the operator supQ in eq. (5)
is replaced by supz∈X in eq. (6), which leads a much easier optimization algorithm. The right-
hand side of eq. (6) is a normal iterated-risk function. That is, it reduces the infinite-dimensional
probability-searching problem (5) into an ordinary finite-dimensional optimization problem (6).
It is easy to verify that Ha maps U to U. Thus, given a state x ∈ X and agent policy π, we have the
following expected Bellman-form operator:
(Hπ U)(x) :=
a∈A(x)
π(da∣x)H au(x)
inf γλδ +
λ≥0
I	π(da∣x)[c(x, a) + Y / P(dy∣x, a)[sup(u(z) — λκ(z,y))]].
a∈A(x)	X	z∈X
4
Under review as a conference paper at ICLR 2020
Similarly, Hπ maps U to U as well. Under the following Assumption 1, we are able to define the
optimal iteration operator and show its contraction property.
Assumption 1. X is a compact metric space. For any x ∈ X, A(x) is compact and Ha is lower
semi-continuous on a ∈ A(x).
Then, given an initial state x ∈ X, the following optimal operator over U is well-defined.
(Hu)(x) := inf Hau(x)
a∈A(x)
inf	c(x, a) + γλδ + γ	P(dy|x, a)[ sup (u(z) - λκ(z, y))].
a∈A(x),λ≥0	X	z∈X
(7)
(8)
It is simple to verify that H maps U to U. The contraction property of H is shown in Lemma 1. We
put the proof in the appendix.
Lemma 1. H is a contraction operator in U under L∞ norm. There exists an unique element in U,
denoted as u*, satisfying Hu* = u*.
For any u0 ∈ U, un := Hun-1 = Hnu0. Due to the contraction, we have
lim un = lim Hnu0 = u* ,	(9)
n→∞	n→∞
which indicates an iterative procedure of finding the optimal value function. Based on this optimal
value function, we can demonstrate the existence of optimal policies, and single out an optimal
policy who is deterministic, Markov and stationary, as shown in Theorem 1. We put the proof in the
appendix.
Theorem 1. There exists a deterministic Markov stationary policy f ∈ F that satisfies
Hfu* = Hu* = u*.
We now obtain the existence of an unique robust optimal value function, as well as a robust optimal
policies, which is deterministic, Markov and stationary. Through an iterative procedure as (9), we
can design corresponding algorithms for robust Reinforcement Learning.
3.4 Sensitivity Analysis
Before going to the algorithm design, we present a sensitivity analysis for the optimal value function
w.r.t. the radius δ and the Wasserstein order p. Let λ* and z* (y, λ*) = arg maxz∈X (u(z) -
λ* κ(z, y)) be a solution of equation (8), and λ* is non-negative.
If λ* = 0, which means the worst transition kernel is within our fixed -Wasserstein ball, equation
(8) can be reduced to an ordinary problem:
(H u)(x) = inf c(x, a) + γ sup u(z).
a∈A(x)	z∈X
Thus u* has nothing to do with δ or p.
If λ* > 0, via the envelop theorem, the gradient of optimal value function w.r.t. δ can be calculated
as follows.
∂u* (x)
∂δ
γλ* > 0.
(10)
This gradient remains positive. That is, the optimal value function increases as the volume of
Wasserstein ball increases (remember that δ = 1 Ep and the value function represents the discounted
cost). Similarly, via the envelop theorem, the gradient w.r.t. p can be calculated as follows.
du*(χ) __	、* / Ps l vι ll *z \*、 ll 1、k z*(y,λ*)-y lip “I、
—∂p- = -γλ J P(dylx, a)(log k Z (y, λ ) - y k2 -P)-------------p--------. (II)
Since λ* > 0, the worst transition kernel Q* satisfies Wp(Q*, P) = E, i.e. Dκ(Q*, P) = δ.1 Notice
that calculating z* (y, λ*) for y is actually trying to find an optimal transport map Tp : X → X,
1Derived from the fact that if Wp(Q*, P) < a there must be λ* = 0.
5
Under review as a conference paper at ICLR 2020
which substantially perturbs P to Q*. Recall that U is upper semi-continuous and its domain
is compact, and then We can actually regard u* as the Kantorovich potential (Villani, 2008) for a
transport cost function λ*κ in the transport from P to Q*. Forp > 1, λ*κ is strictly convex. Through
theorem 1.17 in Santambrogio (2015), we can write the optimal transport map in an explicit way, as
well as the gradient over p.
1	p-2
z*(y, λ*) = τp(y) = y - C*厂p-1 Il Vyu*(y) IΓp-1 Vyu*(y),p > L
∂u*(x)	γλ*	[ π. j .、八 ll Vqu*(y) ll P — I、ll Vqu*(y) Il-PT	1
寸=-JXP(dy|x，a)(logk-jyΛ*^k2-彳)∙kM ,p>L
Thus when 1 ≤ 1 — log ∣∣ ▽%") ∣∣2 for all y ∈ X, the gradient over P is non-negative. Larger
λ* makes non-negativity more likely to happen. Remember that λ* actually reflect the extent of ro-
bustness, i.e., larger λ* coincides with smaller radius . Intuitively, when the volume of Wasserstein
ball is very small, the extent of perturbation at each point is small with high probability, making the
gradient (11) positive. Thus in such situation, smaller P is preferred.
4 Wasserstein Robust Advantange Actor-Critic Algorithms
In reinforcement learning, the agent does not know the precise environment dynamics, i.e., the tran-
sition kernel and immediate cost function are unknown. Some researchers leverage an adversarial
agent to inject perturbations into environmental parameters during training procedures (Pinto et al.,
2017). However, such methods have to work with pre-defined environmental parameters, and are
lack of quantified robustness toward transition kernels. Other researchers borrow the idea of adver-
sarial examples and disturb observed states in a heuristic way (Nguyen et al., 2015). They also lose
the explanation of robustness towards system dynamics.
Following the analysis in Section 3, we develop a robust Advantage Actor-Critic algorithm: a critic
neural network with parameters w, denoted by uw , is employed to estimate value function; and an
actor neural network with parameters θ, denoted by πθ, is designed as the primal policy. Rewrite
equation (8):
(H uw)(x) = inf	πθ (da|x)[c(x, a) + γλδ + γ	P(dy|x, a)[sup(uw(z) — λκ(z, y))]].
θ,λ≥0 a∈A(x)	X	z∈X
Update for Z and λ: Let fw(z; y, λ) := Uw(Z) — λκ(z, y) where κ(z, y) = P ∣∣ Z — y ∣∣p, P ≥ 1.
Given y ∈ X and λ ∈ [0, ∞), denote
Zy,λ := arg max fw(Z; y, λ).
z∈X
Initially, Zy,λ can be treated as the maximum perturbation to state y ∈ X, given the penalty λ.
The gradient of	fw	over Z is:	Vzfw	= VzUw(Z)	—	λ∣∣z	— y∣∣p-2(z — y). Let	Gw(λ; x,a)	:=
λδ + X P(dy|x, a)[supz∈X (uw(Z) — λκ(Z, y))], and we get
λx,a := arg min Gw (λ; x, a).
λ
Combining the envelope theorem, we can obtain the gradient of Gw w.r.t. λ: VλGw = δ —
X P(dy|x, a)κ(Zy,λ, y). The expectation in the gradient can be approximated by Monte Carlo:
take action a at state x for n times; under the reference transition kernel P, observe the next
states yj and quadruples (x, a, c, yj), j = 1,2,…，n; and then we can approximate VλGw ≈
δ — n1 Pn=I K(Zyj,λ,nD.
Critic Update Rule: Given state x ∈ X and policy πθ, let
J(θ,w,x) := /	∏θ(da∣x)[c(x,a)+ YGw(λχ,0;x,a)].
a∈A(x)
To calculate J, similarly, we leverage Monte Carlo, take actions a% 〜∏θ(∙∣χ), i = 1, 2, ∙∙∙ , m
at the same state X for m times, observe m “state-action” pairs (x, a#, i = 1,2, ∙∙∙ ,m, and then
approximate J(θ,w,x) ≈ m1 Pm=ι[c(x, ai)+ γGw(λχ,a; x,ai)].
6
Under review as a conference paper at ICLR 2020
Let e(x, ai) := c(x, ai) + γGw (λx,a; x, ai) - uw (x), and e(x) denote the difference between the
observed cost and the critic network:
1m	1m
e(x) ：= J(θ,w,x) - Uw(x) ≈ 一 Ee(x,电)=一 £[c(x,电)+ YGw(λχ,0; x,电)-Uw(x)].
m i=1	m i=1
Through the envelope theorem, we can obtain the following gradient of e(x) w.r.t. w:
1m
Vwe(x) = mɪ^Y J P(dy∣x,ai)Vwuw(zy,λχ,a) -VwUw(X)
mn
≈ m xx VwUw (zyj ,λx,a ) - VwUw (x).
(12)
(13)
i=1 j=1
Notice that We should actually update the critic network via minimizing 2e(x)2, and the gradient is
Vw2e(x)2 = e(x) ∙ Vwe(x).
In practice, we usually can let m = n = 1 to obtain faster convergence.
Actor Update Rule: In classical AC algorithms, directly minimizing “state-action” value function
J(θ, w, x) may cause large variance and slow convergence, and optimizing the advantage function
is a better choice instead. The advantage function is
A(x, a) := c(x, a) + YGw(λx,a; x,a) - Uw (x) = e(x, a).
Thus we can find the optimal θ via minimizing the expected advantage function A(x, θ) =
ja∈A(x) ∏θ(dɑ∣x)e(x, a). Similarly, we can approximate the gradient of A w.r.t. θ as follows:
1m
VθA(x, θ) = J	∏θ(dα∣x)Vθ log∏θ(da∣x)e(x, a) ≈ 一 ɪ2 Vθ log∏θ(x, ai)e(x, a%). (14)
Finally, we obtain a corresponding Robust Advantage Actor-Critic algorithms. We name it
Wasserstein Robust Advantage Actor-Critic algorithm with order p, described in Algorithm 1 and
Algorithm 2. Algorithm 1 is actually an inner loop that certifies the extent of perturbations, while
Algorithm 2 finds the optimal policy in a normal way. Let the learning rates satisfy the Robbins-
Monro condition (Robbins & Monro, 1951), and β1 = o(β2 ), β2 = o(β3), β3 = o(β4), and via the
multi-time-scales theory (Borkar, 2008), the convergence to a local minimum can be guaranteed.
5 Experiments
In this section, we will verify WRAAC algorithm in Cart-Pole environment 2. State space has four
dimensions, including cart position, cart velocity, pole angle and pole velocity at tip. There are only
two admissible actions: left or right. The target is to prevent the pole from falling over.
Our baseline includes the ordinary Advantage Actor-Critic algorithm. Policies are learnt under the
default environment for WRAAC and the baseline. Then, we test the performances of these two
policies under different environmental dynamics. We change the simulated environmental parame-
ters such as gravity or pole-length to emulate different test dynamics. Note that the unit change on
gravity and pole-length will result in different extents of the dynamic’s robustness.
We apply WRAAC algorithm of order 2, and fix the degree of dynamical robustness at δ = 10. For
each quadruple (x, a, r, y), if y is not the last state of the trajectory, we set initial λ be 0 and initial
Z be y + δ X (0, √26, 0, √^) (designed according to the simulated dynamics of Cart-Pole). If y is
the last state, we set λ ≡ 0 and z ≡ y. The baseline policy and WRAAC are tested in environments
with different gravity or different pole-length, shown in Figure 1 and Figure 2.
Remember that different parameters in the Cart-Pole environment have different effects to the dy-
namic’s robustness. We can see that our robust algorithm changes smoothly as parameter changes,
2https://gym.openai.com/envs/CartPole-v0/
7
Under review as a conference paper at ICLR 2020
Algorithm 1 Calculating Perturbations.
Input: x ∈ X, w, a ∈ A(x), δ ≥ 0, λ ≥ 0, e = 0, ge = 0, discount factor α, order p ≥ 1, κ = 0,
learning rates β1 , β2 .
for j = 1, 2,…，n do
collect roll-out (x, a, cj, yj). Zj J yj.
z update:
gz J VzUw(z) - λ(∣∣zj - yj∣∣p-2)(zj — yj),
Zj J Zj + β1 ∙ gz,
e J e + Cj + α[λδ + [uw(Zj) — λP||zj — yj||p]] — Uw(x)
ge J ge + αVw uw (Z) - Vwuw (x)
K J K + P||z — yj∣∣p,
end for
λ update:
gλ J δ — nκ,
λ J λ + β2 ∙ gλ,
e = n e
ge = n ge
Output: e, ge, λ.
Algorithm 2 Wasserstein Robust Advantage Actor-Critic Algorithm with Order p.
Input: X ∈ X, θ, w, δ ≥ 0, discount factor Y, order P ≥ 1, learning rates β3, β4
for each step do
E = 0, gE = 0.
for i = 1, 2, •…,m do
sample ai 〜∏θ (∙∣x);
use Algorithm 1 and obtain e, ge .
ei J e
EJE+e
gE J gE + ge
end for
w update:
W J W - β Y m E) Y m1 gE)
θ update:
gθ = m1 Pm=I Vθ log ∏θ(x,ai)ei
θ J θ — β4 ∙ gθ
state update:
choose a 〜 ∏θ(∙∣x), and collect roll-out (x, a, c, y).
xJy
end for
Output: θ, W. * 6
while the baseline plunges. When the perturbation of parameter reaches some level (related with the
fixed δ = 10), our robust policy keeps the pole from falling over for a longer time, which indicates
that our algorithm does learn some level of robustness, compared with baseline. If the perturbation
of parameter is small, the baseline performs better, due to the fact that the perturbed environment is
close to the default environment.
6 Conclusions
In this paper, we investigate the robust Reinforcement Learning with Wasserstein constraint. The de-
rived theoretical framework can be reformulated into a tractable iterated-risk aware problem and the
theoretical guarantee is then obtained by building connection between robustness to transition prob-
abilities and robustness to states. Subsequently, we demonstrate the existence of optimal policies,
provide a sensitivity analysis to reveal the effects of uncertainty set, and design a proper two-stage
8
Under review as a conference paper at ICLR 2020
Figure 1: Robustness to gravity.
Figure 2: Robustness to length.
learning algorithm WRAAC. The experimental results on the Cart-Pole environment verified the
effectiveness and robustness of our proposed approaches.
Future works may favor a complete study for the effects of the radius of Wasserstein ball in our
WRAAC algorithm. We are also interested in studying robust policy improvement in a data-driven
situation where we only have access to the set of collected trajectories.
References
Christopher G Atkeson and Jun Morimoto. Nonparametric representation of policies and value
functions: A trajectory-based approach. In Advances in neural information processing systems,
pp.1643-1650, 2003.
Dimitir P Bertsekas and Steven Shreve. Stochastic optimal control: the discrete-time case. 2004.
Jose Blanchet and Karthyek Murthy. Quantifying distributional model risk via optimal transport.
Mathematics of Operations Research, 2019.
Vivek S Borkar. Stochastic approximation: a dynamical systems viewpoint. Baptism’s 91 Witnesses,
2008.
Erick Delage and Shie Mannor. Percentile optimization for markov decision processes with param-
eter uncertainty. Operations research, 58(1):203-213, 2010.
Peyman Mohajerin Esfahani and Daniel Kuhn. Data-driven distributionally robust optimization
using the wasserstein metric: Performance guarantees and tractable reformulations. Mathematical
Programming, 171(1-2):115-166, 2018.
Rui Gao and Anton J Kleywegt. Distributionally robust stochastic optimization with wasserstein
distance. arXiv preprint arXiv:1604.02199, 2016.
Onesimo Hernandez-Lerma and Jean B Lasserre. Discrete-time Markov control processes: basic
optimality criteria, volume 30. Springer Science & Business Media, 2012a.
Onesimo Hernandez-Lerma and Jean B Lasserre. Further topics on discrete-time Markov control
processes, volume 42. Springer Science & Business Media, 2012b.
Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, and Pieter Abbeel. Adversarial attacks
on neural network policies. arXiv preprint arXiv:1702.02284, 2017.
Garud N Iyengar. Robust dynamic programming. Mathematics of Operations Research, 30(2):
257-280, 2005.
Jernej Kos and Dawn Song. Delving into adversarial attacks on deep policies. arXiv preprint
arXiv:1705.06452, 2017.
Yen-Chen Lin, Zhang-Wei Hong, Yuan-Hong Liao, Meng-Li Shih, Ming-Yu Liu, and Min Sun. Tac-
tics of adversarial attack on deep reinforcement learning agents. arXiv preprint arXiv:1703.06748,
2017.
9
Under review as a conference paper at ICLR 2020
Ajay Mandlekar, Yuke Zhu, Animesh Garg, Li Fei-Fei, and Silvio Savarese. Adversarially robust
policy learning: Active construction of physically-plausible perturbations. In 2017 IEEE/RSJ
International Conference on InteUigentRobots and Systems (IROS),pp. 3932-3939. IEEE, 2017.
Shie Mannor, Duncan Simester, Peng Sun, and John N Tsitsiklis. Bias and variance in value function
estimation. In Proceedings of the twenty-first international conference on Machine learning, pp.
72. ACM, 2004.
Shie Mannor, Duncan Simester, Peng Sun, and John N Tsitsiklis. Bias and variance approximation
in value function estimates. Management Science, 53(2):308-322, 2007.
Jun Morimoto and Kenji Doya. Robust reinforcement learning. Neural computation, 17(2):335-359,
2005.
Anh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High confi-
dence predictions for unrecognizable images. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 427-436, 2015.
Arnab Nilim and Laurent El Ghaoui. Robustness in markov decision problems with uncertain tran-
sition matrices. In Advances in Neural Information Processing Systems, pp. 839-846, 2004.
Arnab Nilim and Laurent El Ghaoui. Robust control of markov decision processes with uncertain
transition matrices. Operations Research, 53(5):780-798, 2005.
Takayuki Osogami. Robustness and risk-sensitivity in markov decision processes. In Advances in
Neural Information Processing Systems, pp. 233-241, 2012.
Anay Pattanaik, Zhenyi Tang, Shuijing Liu, Gautham Bommannan, and Girish Chowdhary. Robust
deep reinforcement learning with adversarial attacks. In Proceedings of the 17th International
Conference on Autonomous Agents and MultiAgent Systems, pp. 2040-2042. International Foun-
dation for Autonomous Agents and Multiagent Systems, 2018.
Lerrel Pinto, James Davidson, Rahul Sukthankar, and Abhinav Gupta. Robust adversarial reinforce-
ment learning. In Proceedings of the 34th International Conference on Machine Learning-Volume
70, pp. 2817-2826. JMLR. org, 2017.
Aravind Rajeswaran, Sarvjeet Ghotra, Balaraman Ravindran, and Sergey Levine. Epopt: Learning
robust neural network policies using model ensembles. arXiv preprint arXiv:1610.01283, 2016.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathemati-
cal statistics, pp. 400-407, 1951.
Filippo Santambrogio. Optimal transport for applied mathematicians. BirkauSer NY, pp. 99-102,
2015.
Aman Sinha, Hongseok Namkoong, and John Duchi. Certifying some distributional robustness with
principled adversarial training. arXiv preprint arXiv:1710.10571, 2017.
Cedric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media,
2008.
Chelsea C White III and Hany K Eldeib. Markov decision processes with imprecise transition
probabilities. Operations Research, 42(4):739-749, 1994.
Wolfram Wiesemann, Daniel Kuhn, and Berc Rustem. Robust markov decision processes. Mathe-
matics of Operations Research, 38(1):153-183, 2013.
Huan Xu and Shie Mannor. The robustness-performance tradeoff in markov decision processes. In
Advances in Neural Information Processing Systems, pp. 1537-1544, 2007.
Huan Xu and Shie Mannor. Distributionally robust markov decision processes. In Advances in
Neural Information Processing Systems, pp. 2505-2513, 2010.
Insoon Yang. A convex optimization approach to distributionally robust markov decision processes
with wasserstein distance. IEEE control systems letters, 1(1):164-169, 2017.
Insoon Yang. Wasserstein distributionally robust stochastic control: A data-driven approach. arXiv
preprint arXiv:1812.09808, 2018.
10
Under review as a conference paper at ICLR 2020
A	Appendix
The trajectory space (Ω, F), where F is the σ-algebra of Ω, satisfies
•	Pxπ,g(X0 = x) = 1,
•	P∏,g (da∣ωn) = ∏n(da∣ωn),
•	Pn，g(dq∣ωn) = l(gn(ωn) ∈ dq),
•	Px，g (Xn+1 ∈ dχ]ωn, an, qn) = qn (Xn+1 ∈ dx | ωn, an).
Proof of Lemma 1:
Proof. (1) First, for {u1, u2} ⊂ U, ifu1 ≥ u2, it’s easy to have Hu1 ≥ Hu2, i.e., the operator H is
monotone about u.
(2)	For any real constant C and u ∈ U, we can verify that H(u + C) = Hu + γC.
(3)	For any uι ∈ U, u ∈ U, there is uι ≤ u + ||ui - u2∣∣∞. Combining (1) and (2), we have
Hui ≤ Hu2 + YI∣uι - U2∣∣∞, i.e., Hui - Hu ≤ γ∣∣uι - u2∣∣∞. ThUS ∣∣Huι - Hu2∣∣∞ ≤
Y||ui - u2∣∣∞. Furthermore, since Y ∈ (0,1), the operator H has the contract property in U under
L∞ norm.
(4)	Via Banach fixed-point theorem, there exist an unique u* ∈ U satisfying Hu* = u*.	□
Proof of Theorem 1:
Proof. Due to Assumption 1, for any u ∈ U, it is a measurable function on KA, an (Hau)(x) is
lower semi-continuous w.r.t. a. Based on the measurable selection theorem (see Lemma 8.3.8 in
(Hernandez-Lerma & Lasserre, 2012b)), there is a deterministic Markov stationary policy f ∈ F,
satisfying Hfu* = Hu* = u*.	□
11