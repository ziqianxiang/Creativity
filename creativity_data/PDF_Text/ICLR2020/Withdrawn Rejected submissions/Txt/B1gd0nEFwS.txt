Under review as a conference paper at ICLR 2020
Universal Source-Free Domain Adaptation
Anonymous authors
Paper under double-blind review
Ab stract
There is a strong incentive to develop versatile learning techniques that can transfer
the knowledge of class-separability from a labeled source domain to an unlabeled
target domain in the presence of a domain-shift. Existing domain adaptation
(DA) approaches are not equipped for practical DA scenarios as a result of their
reliance on the knowledge of source-target label-set relationship (e.g. Closed-set,
Open-set or Partial DA). Furthermore, almost all the prior unsupervised DA works
require coexistence of source and target samples even during deployment, making
them unsuitable for incremental, real-time adaptation. Devoid of such highly
impractical assumptions, we propose a novel two-stage learning process. Initially,
in the procurement-stage, the objective is to equip the model for future source-
free deployment, assuming no prior knowledge of the upcoming category-gap and
domain-shift. To achieve this, we enhance the model’s ability to reject out-of-source
distribution samples by leveraging the available source data, in a novel generative
classifier framework. Subsequently, in the deployment-stage, the objective is to
design a unified adaptation algorithm capable of operating across a wide range of
category-gaps, with no access to the previously seen source samples. To achieve
this, in contrast to the usage of complex adversarial training regimes, we define a
simple yet effective source-free adaptation objective by utilizing a novel instance-
level weighing mechanism, named as Source Similarity Metric (SSM). A thorough
evaluation shows the practical usability of the proposed learning framework with
superior DA performance even over state-of-the-art source-dependent approaches.
1	Introduction
Deep learning models have proven to be highly successful over a wide variety of tasks (Krizhevsky
et al., 2012; Ren et al., 2015). However, a majority of these remain heavily dependent on access to a
huge amount of labeled samples to achieve a reliable level of generalization. A recognition model
trained on a certain distribution of labeled samples (source domain) often fails to generalize (Chen
et al., 2017) when deployed in a new environment (target domain) in the presence a discrepancy in
the input distribution (Shimodaira, 2000). Domain adaptation (DA) algorithms seek to minimize
this discrepancy either by learning a domain invariant feature representation (Long et al., 2015;
Kumar et al., 2018; Ganin et al., 2016; Tzeng et al., 2015), or by learning independent domain
transformations (Long et al., 2016) to a common latent representation through adversarial distribution
matching (Tzeng et al., 2017; Nath Kundu et al., 2018), in the absence of target label information.
Most of the existing approaches (Zhang et al., 2018c; Tzeng et al., 2017) assume a common label-set
shared between the source and target domains (i.e. Cs = Ct), which is often regarded as Closed-Set
DA (see Fig. 1). Though this assumption helps to analyze various insights of DA algorithms, such
an assumption rarely holds true in real-world scenarios. Recently researchers have independently
explored two broad adaptation settings by partly relaxing the above assumption. In the first kind,
Partial DA (Zhang et al., 2018b; Cao et al., 2018a;b), the target label space is considered as a subset
of the source label space (i.e. Ct ⊂ Cs). This setting is more suited for large-scale universal source
datasets, which will almost always subsume the label-set of a wide range of target domains. However,
the availability of such a universal source is highly questionable for a wide range of input domains
and tasks. In the second kind, regarded as Open-set DA (Baktashmotlagh et al., 2019; Ge et al., 2017),
the target label space is considered as a superset of the source label space (i.e. Ct ⊃ Cs). The major
challenge in this setting is attributed to detection of target samples from the unobserved categories
in a fully-unsupervised scenario. Apart from the above two extremes, certain works define a partly
mixed scenario by allowing “private” label-set for both source and target domains (i.e. CS \Ct = 0
1
Under review as a conference paper at ICLR 2020
and Ct \Cs = 0) but with extra supervision such as few-shot labeled data (Luo et al., 2017) or access
to the knowledge of common categories (Panareda Busto & Gall, 2017).
Most of the prior approaches consider
each scenario in isolation and propose in-
(ɔ Source Cs J：Targeta Shared C Cs = Cs∖Ct	Ct 二 C八CS
dependent solutions. Thus, they require
access to the knowledge of label-set re-
lationship (or category-gap) to carefully
choose a DA algorithm, which would be
suitable for the problem in hand. Fur-
thermore, all the prior unsupervised DA
works require coexistence of source and
-8M	G a (1 3)
∖1	2 八、、3，d U VJ
Closed-set	Partial	Open-set Ours: Universal
Figure 1: Various label-set relationships (category-gap).
target samples even during deployment, hence not source-free. This is highly impractical, as labeled
source data may not be accessible after deployment due to several reasons such as, privacy concerns,
restricted access to proprietary data, accidental loss of source data or other computational limitations
in real-time deployment scenarios.
Acknowledging the aforementioned shortcomings, we propose one of the most convenient DA
frameworks which is ingeniously equipped to address source-free DA for all kinds of label-set
relationships, without any prior knowledge of the associated category-gap (i.e. universal-DA). We
not only focus on identifying the key complications associated with the challenging problem setting,
but also devise insightful ideas to tackle such complications by adopting learning techniques much
different from the available DA literature. This leads us to realize a holistic solution which achieves
superior DA performance even over prior source-dependent approaches.
2	Related work
We briefly review the available domain adaptation methods under the three major divisions according
to the assumption on label-set relationship. a) Closed-set DA. The cluster of previous works under
this setting focuses on minimizing the domain gap at some intermediate feature level either by
minimizing well-defined statistical distance functions (Wang & Schneider, 2014; Duan et al., 2012;
Zhang et al., 2013; Saenko et al., 2010) or by formalizing it as an adversarial distribution matching
problem (Tzeng et al., 2017; Kang et al., 2018; Long et al., 2018; Hu et al., 2018; Hoffman et al.,
2018) inspired from the Generative Adversarial Nets (Goodfellow et al., 2014). Certain prior
works (Sankaranarayanan et al., 2018; Zhu et al., 2017; Hoffman et al., 2018) use GAN framework
to explicitly generate target-like images translated from the source image samples, which is also
regarded as pixel-level adaptation (Bousmalis et al., 2017) in contrast to other feature level adaptation
works (Nath Kundu et al., 2018; Tzeng et al., 2017; Long et al., 2015; 2016). b) Partial DA. Focusing
on Partial DA, Cao et al. (2018a) proposed to achieve adversarial class-level matching by utilizing
multiple domain discriminators furnishing class-level and instance-level weighting for individual data
samples. Zhang et al. (2018b) proposed to utilize importance weights for source samples depending on
their similarity to the target domain data using an auxilliary discriminator. To effectively address the
problem of negative-transfer (Wang et al., 2019), Cao et al. (2018b) employed a single discriminator
to achieve both adversarial adaptation and class-level weighting of source samples. c) Open-set
DA. Saito et al. (2018b) proposed a more general open-set adaptation setting without accessing the
knowledge of source private labels set in contrast to the prior work (Panareda Busto & Gall, 2017).
They extended the source classifier to accommodate an additional “unknown” class, which is trained
adversarially against the other source classes. Universal DA. You et al. (2019) proposed Universal
DA, which requires no prior knowledge of label-set relationship similar to the proposed setting, but
considers access to both source and target samples during adaptation.
3	Proposed approach
The problem setting for source-free domain adaptation is broadly divided into a two stage process.
a)	Procurement stage. In this stage, we are given full access to the labeled samples of source domain,
DS = {(xs,ys) : Xs 〜p, ys ∈ Cs}, whereP is the distribution of source samples and Cs denotes the
label-set of the source domain. Here, the objective is to equip the model for the second stage, i.e. the
Deployment stage, in the presence of a discrepancy in the distribution of input target samples. To
achieve this we rely on an artificially generated negative dataset, Dn = {(xn, yn) : Xn 〜Pn, yn ∈
Cn}, where pn is the distribution of negative source samples such that Cn ∩ Cs = 0.
2
Undr r view as a conference paper at ICLR 2020
Proposed
Approach
Rearrangement of source-clusters
and class-boundaries to
accommodate target clusters
SOU Source-private class	T^ Target-private class
Source-shared class	Target-shared class
Simulated negative class	Classifier boundary
(d) Universal Source-Free DA
I ~ ~ ɪ
1	■、;∙.
I ʌ
!
Procurement stage
Deployment stage /
Frozen class
boundaries
Intra-class compactness
with inter-class separability
using negative classes
Figure 2: Latent space cluster arrangement during adaptation (see Section 3.1.1).
b)	Deployment stage. After obtaining a trained model from the P rocurement stage, the model will
have its first encounter with the unlabeled target domain samples from the deployed environment. We
denote the unlabeled target data by Dt = {xt : Xt 〜q}, where q is the distribution of target samples.
Note that, access to the source dataset Ds from the previous stage is fully restricted during adaptation
in the Deployment stage. Suppose that, Ct is the "unknown" label-set of the target domain. We define
the common label space between the source and target domain as C = Cs_p Ct. The private label-set
for the source and the target domains is represented as Cs = Cs \ Ct and Ct = Ct \ Cs respectively.
3.1	Learning in the procurement stage
3.1.1	Challenges. The available DA techniques heavily rely on the adversarial discriminative (Tzeng
et al., 2017; Saito et al., 2018a) strategy. Thus, they require access to the source samples to reliably
characterize the source domain distribution. Moreover, these approaches are not equipped to operate
in a source-free setting. Though a generative model can be used as a memory-network (Sankara-
narayanan et al., 2018; Bousmalis et al., 2017) to realize source-free adaptation, such a solution is not
scalable for large-scale source datasets (e.g. ImageNet (Russakovsky et al., 2015)), as it introduces
unnecessary extra parameters in addition to the associated training difficulties (Salimans et al., 2016).
This calls for a fresh analysis of the requirements beyond the solutions found in literature.
In a general DA scenario, with access to source samples in the Deployment stage (specifically for
Open-set or Partial DA), a widely adopted approach is to learn domain invariant features. In such
approaches the placement of source category clusters is learned in the presence of unlabeled target
samples which obliquely provides a supervision regarding the relationship between Cs and Ct . For
instance, in case of OPen-Set_DA, the source clusters may have to disperse to make space for the
clusters from target private Ct (see Fig. 2a to 2b). Similarly, in partial DA, the source clusters
may have to rearrange themselves to keep all the target shared clusters (C = Ct) separated from
the source private Cs (see Fig. 2a to 2c). However in a complete source-free framework, we do not
have the liberty to leverage such information as source and target samples never coexist together
during training. Motivated by the adversarial discriminative DA technique (Tzeng et al., 2017), we
hypothesize that, inculcating the ability to reject samples that are out of the source data distribution
can facilitate future source-free domain alignment using this discriminatory knowledge. Therefore, in
the Procurement stage the overarching objective is two-fold.
•	Firstly, we must aim to learn a certain placement of source clusters best suited for all kinds of
category-gap scenarios acknowledging the fact that, a source-free scenario does not allow us
to modify the placement in the presence of target samples during adaptation (see Fig. 2d).
•	Secondly, the learned embedding must have the ability to reject out-of-distribution samples,
which is an essential requirement for unsupervised adaptation in the presence of domain-shift.
3.1.2	Solution. In the presence of source data, we aim to restrain the model’s domain and category
bias which is generally inculcated as a result of the over-confident supervised learning paradigms
(see Fig. 4A). To achieve this goal, we adopt two regularization strategies viz. i) regularization via
generative modeling and ii) utilization of a labeled simulated negative source dataset to generalize for
the latent regions not covered by the given positive source samples (see Fig. 4C).
How to configure the negative source dataset? While configuring Dn, the following key properties
have to be met. Firstly, latent clusters formed by the negative categories must lie in-between the latent
clusters of positive source categories to enable a higher degree of intra-class compactness with inter-
class separability (Fig. 4C). Secondly, the negative source samples must enrich the source domain
3
Under review as a conference paper at ICLR 2020
A. Simulation of -ve samples
B. Architecture of the 2-stage method
Procurement stage
匚：Frozen CNN □FC 口 Frozen FC
I SoftmaX output probabilities
Figure 3: A) Simulated labeled negative samples using randomly created spline segments (in pink),
B) Proposed architecture, C) Procurement stage yields compact source clusters on experimental data.
C. Latent-space t-SNE
With procurement
A o
o ,
O
3。
∙.∙∙
' O
O
O
Δ
∆
o
o o



□
□ □口
Over-confident supervised learning
(notice non-compact boundaries)
∆ △
△ Δ
∆ ∆
Δ
Δ △
□ □
□ □
□
Procurement stage encourages intra-class compactness and inter-class separability
□ Source samples Ne Negative samples ------------------------Classifier boundaries 就, Intra-CIaSS compactness	玲, Inter-CIaSS separability
Figure 4: Achieving intra-Class compactness and inter-class separability using negative dataset Dn.
distribution without forming a new domain by themselves. This rules out the use of Mixup (Zhang
et al., 2018a) or adversarial noise (ShU et al., 2018) as negative samples in this scenario. Thus, We
propose the following two ways to synthesize the desired negative source dataset.
a) Image-composition as negative dataset Dna). One of the key characteristics shared between the
samples from source and unknown target domain is the semantics of the local part-related features
specifically for image-based object recognition tasks. Relying on this assumption, we propose
a systematic procedure to simulate the samples of Dna) by randomly compositing local regions
between a pair of images drawn from the positive source dataset DS (see Fig. 3A and appendix,
Algo. 2). Intuitively, composite samples Xn created on image pairs from different source categories
are expected to lie in-between the two positive source clusters in the latent space, thereby introducing
a combinatorial amount of new class labels i.e. |Cn| = ICs | C2.
b) Latent-simulated negative dataset Dn(b). As an alternative approach, in the absence of domain
knowledge (e.g. non-image datasets, or for tasks beyond image-recognition such as pose estimation),
we propose to sample virtual negative instances, un from the latent space which are away from the
high confidence regions (3-sigma) of positive source clusters (Fig. 4B). For each negative sample, we
assign a negative class label (one of |Cn| = |Cs|C2) corresponding to the pair of most confident source
classes predicted by the classifier. Thus, we obtain Dn* a) b) = {(un, yn) : Un 〜Pn, yn ∈ Cn } where Pn
is the distribution of negative samples in the latent u-space (more details in appendix Algo. 3).
Training procedure. The generative source classifier is divided into three stages; i) backbone-model
M , ii) feature extractor FS , and iii) classifier D (see Fig. 3B). Output of the backbone-model is
denoted as v = M (x), where x is drawn from either DS or Dn. Following this, the output of FS
and D are represented as u and d respectively. D outputs a K-dimensional logit denoted as d(k) for
k = 1, 2...K; K = |CS| + |Cn|. The individual class probabilities, y(k) are obtained by applying
softmax over the logits i.e. y(k)= exp(d(k)/PK=I exp(d(k)) = σ(k) (D ◦ Fs ◦ M(x)). Additionally,
we define priors of only positive source classes as P(us∣ci) = N(us∣μci, Σci) for i = 1, 2…|CS| at
4
Under review as a conference paper at ICLR 2020
Algorithm 1 Training algorithm in the Procurement stage
1:	input: (xs, ys) ∈ Ds, (xn, yn) ∈ Dn; θFs , θD, θG: Parameters of Fs, D and G respectively.
2:	initialization: pretrain {θFs , θD} using cross-entropy loss on (xs, ys) followed by initialization of the
sample mean μ/ and covariance Σ. (at U-SPaCe) of FS ◦ M(Xs) for Xs from class Ci； i = 1, 2,…|C§|
3:	for iter < M axI ter do
4:	VS = M (Xs)； US = FS(VSX VS = G(US )； Ur 〜N (μ% , ∑Ci) for i = 1, 2,…|Cs|； Ur = FS ◦ G(Ur)
5:	ySks) = σ(ks)(D ◦ FS ◦ M(xs)), and yW，= σ(kn)(D ◦ FS ◦ M(Xn)) where k$ and kn are the index
of ground-truth label yS and yn respectively.
6:	LCE = - log y(ks) - α log ynkn); Lv = ∣Vs - Vs |; Lu = ∣Ur - Ur |
7:	Lp = - log(eXP(P(Us∣Cks))/Pi=I exp(P(Us∣Ci))), where P(us|&) = N(Us∣μ%, ∑%)
8:	Update θFs , θD, θG by minimizing LCE, Lv, Lu, and Lp alternatively using separate optimizers.
9:	if (iter % U pdateI ter == 0) then
10:	Recompute the sample mean (μ.) and covariance (Σ/) of FS ◦ M(XS) for XS from class &;
i	= 1, 2...|CS | (For Dn(b): generate fresh latent-simulated negative samples using the updated priors)
the intermediate embedding us = Fs ◦ M (xs). Here, parameters of the normal distributions are
computed during training as shown in line-10 of Algo. 1. A cross-entropy loss over these prior
distributions is defined as Lp (line-7 in Algo. 1), to effectively enforce intra-class compactness
with inter-class separability (progression from Fig. 4B to 4C). Motivated by generative variational
auto-encoder (VAE) setup (Kingma & Welling, 2013), we introduce a feature decoder G, which
aims to minimize the cyclic reconstruction loss selectively for the samples from positive source
categories vs and randomly drawn samples ur from the corresponding class priors (i.e. Lv and Lu ,
line-6 in Algo. 1). This along with a lower weightage α for the negative source categories (i.e. at the
cross-entropy loss LCE, line-6 in Algo. 1) is incorporated to deliberately bias Fs towards the positive
source samples, considering the level of unreliability of the generated negative dataset.
3.2 Learning in the deployment stage
3.2.1	Challenges. We hypothesize that, the large number of negative source categories along with
the positive source classes i.e. Cs ∪ Cn can be interpreted as a universal source dataset, which can
subsume label-set Ct of a wide range of target domains. Moreover, we seek to realize a unified
adaptation algorithm, which can work for a wide range of category-gaps. However, a forceful
adaptation of target samples to positive source categories will cause target private samples to be
classified as an instance of the source private or the common label-set, instead of being classified as
"unknown", i.e. one of the negative categories in Cn.
3.2.2	Solution. In contrast to domain agnostic architectures (You et al., 2019； Cao et al., 2018a； Saito
et al., 2018a), we resort to an architecture supporting domain specific features (Tzeng et al., 2017),
as we must avoid disturbing the placement of source clusters obtained from the Procurement stage.
This is an essential requirement to retain the task-dependent knowledge gathered from the source
dataset. Thus, we introduce a domain specific feature extractor denoted as Ft , whose parameters are
initialized from the fully trained Fs (see Fig. 3B). Further, we aim to exploit the learned generative
classifier from the Procurement stage to complement for the purpose of separate ad-hoc networks
(critic or discriminator) as utilized by the prior works (You et al., 2019； Cao et al., 2018b).
a)	Source Similarity Metric (SSM). We define a weighting factor (SSM) for each target sample
xt, as w(xt). A higher value of this metric indicates xt’s similarity towards the positive source
categories, specifically inclined towards the common label space C. Similarly, a lower value of this
metric indicates xt's similarity towards the negative source categories Cn, showing its inclination
towards the private target labels Ct. Let, ps, qt be the distribution of source and target samples with
labels in C S and Ct respectively. We define, Pc and qc to denote the distribution of samples from
source and target domains belonging to the shared label-set C . Then, the SSM for the positive and
negative source samples should lie on the two extremes, forming the following inequality:
Exn 〜Pn W(Xn) ≈ Ext 〜q^w(Xt) < Ext 〜q° W(Xt) < Exs 〜Pc W(XS) ≈ Exs 〜PV W(XS)	(1)
To formalize the SSM criterion we rely on the class probabilities defined at the output of source
model only for the positive class labels, i.e. y(k) for k = 1, 2…|C§|. Note that, y(k) is obtained by
performing softmax over |Cs | + |Cn | categories as discussed in the Procurement stage. Finally, the
5
Under review as a conference paper at ICLR 2020
SSM and its complement are defined as,
w(xt) = max exp(y(i)), and w0(xt) =	max exp(1 — y(i))
i=1,2...|Cs|	i=1,2...|Cs|
(2)
We hypothesize that, the above definition will satisfy Eq. 1, as a result of the generative learning
strategy adopted in the Procurement stage. In Eq. 2 the exponent is used to further amplify separation
between target samples from the shared C and those from the private Ct label-set (see Fig. 5A).
b)	Source-free domain adaptation. To perform domain adaptation, the objective function aims to
move the target samples with higher SSM value towards the clusters of positive source categories
and vice-versa at the frozen source embedding, u-space (from the Procurement stage). To achieve
this, parameters of only Ft network are allowed to be trained in the Deployment stage. However, the
decision of weighting the loss on target samples towards the positive or negative source clusters is
computed using the source feature extractor Fs i.e. the SSM in Eq. 2. We define, the deployment
model as h = D ◦ Ft ◦ M (xt) using the target feature extractor, with softmax predictions over K
categories obtained as z(k) = σ(h(k)). Thus, the primary loss function for adaptation is defined as,
Ldi = -W(Xt) log(P巴 Z(k)) — w0(xt)log(Pk=1+∣CS∣ Z(k))
(3)
Additionally, in the absence of label information, there would be uncertainty in the predictions z(k)
as a result of distributed class probabilities. This leads to a higher entropy for such samples. Entropy
minimization (Grandvalet & Bengio, 2005; Long et al., 2016) is adopted in such scenarios to move
the target samples close to the highly confident regions (i.e. positive and negative cluster centers
from the Procurement stage) of the classifier’s feature space. However, it has to be done separately
for positive and negative source categories based on the SSM values of individual target samples to
effectively distinguish the target-private set from the full target dataset. To achieve this, we define two
different class probability vectors separately for the positive and negative source classes denoted as,
Zsi) = exp(h(i))∕Pj=1 exp(h(j)) and Zf) = exp(h(i+lCsl))∕pj= exp(h(j+lCsl)) respectively (see
Fig. 3B). Entropy of the target samples in the positive and negative regimes of the source classifier
is obtained as Hs(xt) = - P|iC=s1| zZs(i) log zZs(i) and Hn(xt) = - P|iC=n1| zZn(i) log zZn(i) respectively.
Consequently, the entropy minimization loss is formalized as,
Ld2 = w(xt)Hs (xt) + w0(xt)Hn(xt)	(4)
Thus, the final loss function for adapting the parameters of Ft is presented as Ld = Ld1 + βLd2 .
Here β is a hyper-parameter controlling the importance of entropy minimization during adaptation.
4	Experiments
We perform a thorough evaluation of the proposed source-free, universal domain adaptation frame-
work against prior state-of-the-art models across multiple datasets. We also provide a comprehensive
ablation study to establish generalizability of the approach across a variety of label-set relationships
and justification of the various model components.
4.1	Experimental Setup
Datasets. For all the following datasets, we resort to the experimental settings inline with the recent
work by You et al. (2019) (UAN). Office-Home (Venkateswara et al., 2017) dataset consists of
images from 4 different domains - Artistic (Ar), Clip-art (Cl), Product (Pr) and Real-world (Rw).
Alphabetically, the first 10 classes are selected as C, the next 5 classes as Cs, and the rest 50 as Ct.
VisDA2017 (Peng et al., 2018) dataset comprises of 12 categories with synthetic images as the source
domain and natural images as the target domain, out of which, the first 6 are chosen as C, the next
3 as Cs and the rest as Ct. Office-31 (Saenko et al., 2010) dataset contains images from 3 distinct
domains - Amazon (A), DSLR (D) and Webcam (W). We use the 10 classes shared by Office-31 and
CaIteCh-256 (Gong et al., 2012) to construct the shared label-set C and alphabetically select the next
10 as Cs, with the remaining 11 classes contributing to Ct. To evaluate scalability, ImageNet-CalteCh
is also considered with 84 common classes inline with the setting in You et al. (2019).
Simulation of labeled negative samples. To simulate negative labeled samples for training in the
Procurement stage, we first sample a pair of images, each from different categories of Cs, to create
6
Under review as a conference paper at ICLR 2020
Table 1: Average per-class accuracy (Tavg) for universal-DA tasks on Office-Home dataset (with ∣C∣∕∣Cs ∪ Ct| = 0.15). Scores for the prior works are directly taken from UAN (You et al., 2019).													
Method	Office-Home												
	Ar→Cl Ar→Pr Ar→Rw Cl→Ar Cl→Pr Cl→Rw Pr→Ar Pr→Cl Pr→Rw Rw→Ar Rw→Cl Rw→Pr												Avg
ResNet (He et al., 2016)	59.37	76.58	87.48	69.86	71.11	81.66	73.72	56.30	86.07	78.68	59.22	78.59	73.22
IWAN (Zhang et al., 2018b)	52.55	81.40	86.51	70.58	70.99	85.29	74.88	57.33	85.07	77.48	59.65	78.91	73.39
PADA (Zhang et al., 2018b)	39.58	69.37	76.26	62.57	67.39	77.47	48.39	35.79	79.60	75.94	44.50	78.10	62.91
ATI (Busto et al., 2017)	52.90	80.37	85.91	71.08	72.41	84.39	74.28	57.84	85.61	76.06	60.17	78.42	73.29
OSBP (Saito et al., 2018b)	47.75	60.90	76.78	59.23	61.58	74.33	61.67	44.50	79.31	70.59	54.95	75.18	63.90
UAN (You et al., 2019)	63.00	82.83	87.85	76.88	78.70	85.36	78.22	58.59	86.80	83.37	63.17	79.43	77.02
Source-free adaptation													
Ours USFDA-a	63.35	83.30	89.35	70.96	72.34	86.09	78.53	60.15	87.35	81.56	63.17	88.23	77.03
Ours USFDA-b	62.46	82.71	88.26	71.10	70.88	85.75	78.21	59.18	86.05	82.17	63.22	87.68	76.47
unique negative classes in Cn . Note that, we impose no restriction on how the hypothetical classes
are created (e.g. one can composite non-animal with animal). A random mask is defined which
splits the images into two complementary regions using a quadratic spline passing through a central
image region (see Appendix Algo. 2). Then, the negative image is created by merging alternate mask
regions as shown in Fig. 3A. For the I→C task of ImageNet-Caltech, the source domain (ImageNet),
consisting of 1000 classes, results in a large number of possible negative classes (i.e. |Cn| = |Cs|C2).
We address this by randomly selecting only 600 of these negative classes for ImageNet(I), and 200
negative classes for Caltech(C) in the task C→I. In a similar fashion, we generate latent-simulated
negative samples only for the selected negative classes in these datasets. Consequently, we compare
two models with different Procurement stage training - (i) USFDA-a: using image-composition as
negative dataset , and (ii) USFDA-b: using latent-simulated negative samples as the negative dataset.
We use USFDA-a for most of our ablation experiments unless mentioned explicitly.
4.2	Evaluation Methodology
Average accuracy on Target dataset, Tavg . We resort to the evaluation protocol proposed in the
VisDA2018 Open-Set Classification challenge. Accordingly, all the target private classes are grouped
into a single "unknown" class and the metric reports the average of per-class accuracy over |Cs| + 1
classes. In the proposed framework a target sample is marked as "unknown", if it is classified
(argmaxfcz(k)) into any of the negative |Cn| classes out of total |C§ | + |Cn | categories. In contrast,
UAN (You et al., 2019) relies on a sensitive hyperparameter, as a threshold on the sample-level
weighting, to mark a target sample as "unknown". Also note that, our method is completely source-free
during the Deployment stage, while all other methods have access to the full source-data.
Accuracy on Target-Unknown data, TUnk. We evaluate the target unknown accuracy, Tunk, as the
proportion of actual target private samples (i.e. {(xt, Iyt) : yt ∈ Ct}) being classified as “unknown"
after adaptation. Note that, UAN (You et al., 2019) does not report Tunk which is a crucial metric
to evaluate the vulnerability of the model after its deployment in the target environment. The Tavg
metric fails to capture this as a result of class-imbalance in the Open-set scenario (Saito et al., 2018b).
Hence, to realize a common evaluation ground, we train the UAN implementation provided by the
authors (You et al., 2019) and denote it as UAN* in further sections of this paper. We observe that,
the UAN(You et al., 2019) training algorithm is often unstable with a decreasing trend of Tunk and
Tavg over increasing training iterations. We thus report the mean and standard deviation of the peak
values of Tunk and Tavg achieved by UAN*, over 5 separate runs on Office-31 dataset (see Table 7).
Implementation Details. We implement our network in PyTorch and use ResNet-50 (He et al., 2016)
as the backbone-model M, pre-trained on ImageNet (Russakovsky et al., 2015) inline with UAN (You
et al., 2019). The complete architecture of other components with fully-connected layers is provided
in the Supplementary. A sensitivity analysis of the major hyper-parameters used in the proposed
framework is provided in Fig. 5B-C, and Appendix Fig. 8B. In all our ablations across the datasets,
we fix the hyperparameters values as α = 0.2 and β = 0.1. We utilize Adam optimizer (Kingma &
Ba, 2014) with a fixed learning rate of 0.0001 for training in both Procurement and Deployment stage
(see Appendix for the code). For the implementation of UAN*, we use the hyper-parameter value
w0 = -0.5, as specified by the authors for the task A→D in Office-31 dataset.
4.3	Discussion
a)	Comparison with prior arts. We compare our approach with UAN You et al. (2019), and other
prior methods. The results are presented in Table 1 and Table 2. Clearly, our framework achieves state-
7
Under review as a conference paper at ICLR 2020
Table 2: Tavg on Office-31 (with |C|/|Cs ∪ Ct| = 0.32), VisDA (with |C|/|Cs ∪ Ct| = 0.50), and
ImageNet-Caltech (with |C|/|Cs ∪ Ct| = 0.07). Here, SF denotes support for source-free adaptation.
Method
ResNet (He et al., 2016)
IWAN (Zhang et al., 2018b)
PADA (Zhang et al., 2018b)
ATI (Busto et al., 2017)
OSBP (Saito et al., 2018b)
UAN (You et al., 2019)
UAN* Tavg
Ours USFDA-a Tavg
Ours USFDA-b Tavg
UAN* Tunk
Ours USFDA-a Tunk
Ours USFDA-b Tunk
SF	A→W-	D→W	Office-31		D→A	W→A	Avg	VisDA
			W→D	A→D				S → R
ʃ	-75.94-	89.60	-9091-	80.45	78.83	81.42	82.86	52.80
X	85.25	90.09	90.00	84.27	84.22	86.25	86.68	58.72
X	85.37	79.26	90.91	81.68	55.32	82.61	79.19	44.98
X	79.38	92.60	90.08	84.40	78.85	81.57	84.48	54.81
X	66.13	73.57	85.62	72.92	47.35	60.48	67.68	30.26
X	85.62	94.77	97.99	86.50	85.45	85.12	89.24	60.83
ʃ	83.00±1.8	94.17±0.3	95.40±0.5	83.43±0.7	86.90±1.0	87.18±0.6	88.34	54.21
✓	85.56±1.6	95.20±0.3	97.79±0.1	88.47±0.3	87.50±0.9	86.61±0.6	90.18	63.92
✓	83.21±1.2	95.33±0.3	96.37±0.3	86.84±0.4	87.91±0.6	86.74±0.5	89.40	62.77
ʃ	20.72±11.7	53.53±2.4	51.57±5.0	34.43±3.3	51.88±4.8	43.11±1.3	42.54	19.68
✓	73.98±7.5	85.64±2.2	80.00±1.1	82.23±2.7	78.59±3.2	75.52±1.5	79.32	36.25
✓	70.22±8.8	85.89±2.3	78.29±1.7	84.66±3.1	76.22±2.8	73.91±1.6	78.19	34.84
ImNet-CalteCh
C → I
I→C
70.28
72.19
65.47
71.59
62.08
75.28
74.77
76.85
76.74
33.43
51.21
51.10
65.14
66.48
58.73
67.36
55.48
70.17
71.51
72.13
72.25
31.24
48.76
48.20
Sensitivity to β
1.0 •….
0.9	..............---
a 0.8 ……
o
0 0.7......
0 06……
< 0.5∣...............
0.0	0.5
1.0
Value of ∖Cn∖
Value of β
Figure 5: Ablative analysis on the task A→D in OffiCe-31 dataset. A) Histogram of SSM values
of xt separately for target-private and target-shared samples at the Procurement iteration 100 (top)
and 500 (bottom). B) The sensitivity Curve for β shows marginally stable adaptation aCCuraCy for a
wide-range of values. C) A marginal inCrease in Tavg is observed with inCrease in |Cn |.
of-the-art results even in a source-free setting on several tasks. PartiCularly in Table 2, we present the
target-unknown aCCuraCy Tunk on various dataset. It also holds the mean and standard-deviation for
both the aCCuraCy metriCs Computed over 5 random initializations in the OffiCe-31 dataset (the last six
rows). Our method is able to aChieve muCh higher Tunk than UAN* (You et al., 2019), highlighting
our superiority as a result of the novel learning approaCh inCorporated in both Procurement and
Deployment stages. Note that, both USFDA-a and USFDA-b yield similar performanCe aCross a wide
range of standard benChmarks. We also perform a CharaCteristiC Comparison of algorithm Complexity
in terms of the amount of learnable parameters and training time. In Contrast to UAN, the proposed
framework offers a muCh simpler adaptation algorithm devoid of utilization of ad-hoC networks like
adversarial disCriminator and additional finetuning of the ResNet-50 baCkbone. Parameter size and
training time; a) Ours proCurement (USFDA-a): [11.1M, 380s], b) Ours deployment: [3.5M, 44s],
C) UAN (You et al., 2019): [26.7M, 450s] (in a Consistent setting). The signifiCant Computational
advantage in the Deployment stage makes USFDA highly suitable for real-time adaptation.
b)	Does SSM satisfy the expected inequality? EffeCtiveness of the proposed learning algorithm,
in Case of source-free deployment, relies on the formulation of SSM, whiCh is expeCted to satisfy
Eq. 1. Fig. 5A shows a histogram of the SSM separately for samples from target-shared (blue) and
target-private (red) label spaCe. The suCCess of this metriC is attributed to the generative nature of
Procurement stage, whiCh enables the sourCe model to distinguish between the marginally more
negative target-private samples as Compared to the samples from the shared label spaCe.
c)	Sensitivity to hyper-parameters. As we taCkle DA in a source-free setting simultaneously
intending to generalize aCross varied category-gaps, a low sensitivity to hyperparameters would
further enhanCe our praCtiCal usability. To this end, we fix Certain hyperparameters for all our
ablations (also in Fig. 6C) even aCross datasets (i.e. α = 0.2, β = 0.1). Thus, one Can treat them as
global-Constants with |Cn | being the only hyperparameter, as variations in one by fixing the others
yield Complementary effeCt on regularization in the Procurement stage. A thorough analysis reported
in the appendix Fig. 8, Clearly demonstrates the low-sensitivity of our model to these hyperparameters.
d)	Generalization across category-gap. One of the key objeCtives of the proposed framework is
to effeCtively operate in the absenCe of the knowledge of label-set relationships. To evaluate it in
8
Under review as a conference paper at ICLR 2020
Figure 6: Comparison across varied label-set relationships for the task A→D in Office-31 dataset.
A) Visual representation of label-set relationships and Tavg at the corresponding instances for B)
UAN* (You et al., 2019) and C) ours source-free model. Effectively, the direction along x-axis (blue
horizontal arrow) characterizes increasing Open-set complexity. The direction along y-axis (red
vertical arrow) shows increasing complexity of Partial DA scenario. The pink diagonal arrow denotes
the effect of decreasing shared label space.
78.88
Ours
(source-free)
72.14
83.52
83.85
81.66 84.31 90.66	72.94
85.38 84.26 89.98	89.74
the most compelling manner, we propose a tabular form shown in Fig. 6A. We vary the number of
private classes for target and source along x and y axis respectively, with a fixed |Cs ∪ Ct | = 31. We
compare the Tavg metric at the corresponding table instances, shown in Fig. 6B-C. The results clearly
highlight superiority of the proposed framework specifically for the more practical scenarios (close to
the diagonal instances) as compared to the unrealistic Closed-set setting (|Cs | = |C11 =0).
e)	DA in absence of shared categories. In universal adaptation, we seek to transfer the knowledge
of "class-separability criterion" obtained from the source domain to the deployed target environment.
More concretely, it is attributed to the segregation of data samples based on some expected charac-
teristics, such as classification of objects according to their pose, color, or shape etc. To quantify
this, we consider an extreme case where Cs ∩ Ct = Q (A→D in Office-31 with |Cs| = 15, |Ct | = 16).
Allowing access to a single labeled target sample from each category in Ct = Ct, we aim to obtain a
one-shot recognition accuracy (assignment of cluster index or class label using the one-shot samples
as the cluster center at Ft ◦ M(xt)) to quantify the above metric. We obtain 64.72% accuracy for the
proposed framework as compared to 13.43% for UAN* (You et al., 2019). This strongly validates our
superior knowledge transfer capability as a result of the generative classifier with labeled negative
samples complementing for the target-private categories.
f)	Dependency on the simulated negative dataset. Conceding that a combinatorial amount of
negative labels can be created, we evaluate the scalability of the proposed approach, by varying the
number of negative classes in the Procurement stage by selecting 0, 4, 8, 64, 150 and 190 negative
classes as reported in the X-axis of Fig. 5C. For the case of 0 negative classes, denoted as ∣Cn |* = 0
in Fig. 5C, we synthetically generate random negative features at the intermediate level u, which
are at least 3-sigma away from each of the positive source priors P(us|ci). We then make use of
these feature samples along with positive image samples, to train a (|Cs | + 1) class Procurement
model with a single negative class. The results are reported in Fig. 5C on the A→D task of Office-31
dataset with category relationship inline with the setting in Table 7. We observe an acceptable drop in
accuracy with decrease in number of negative classes, hence validating scalability of the approach for
large-scale classification datasets (such as ImageNet). Similarly, we also evaluated our framework by
combining three or more images to form such negative classes. An increasing number of negative
classes (|Cs|C3 > |Cs|C2) attains under-fitting on positive source categories (similar to Fig. 5C, where
accuracy reduces beyond a certain limit because of over regularization).
5	Conclusion
We have introduced a novel source-free, universal domain adaptation framework, acknowledging
practical domain adaptation scenarios devoid of any assumption on the source-target label-set rela-
tionship. In the proposed two-stage framework, learning in the Procurement stage is found to be
highly crucial, as it aims to exploit the knowledge of class-separability in the most general form with
enhanced robustness to out-of-distribution samples. Besides this, success in the Deployment stage is
attributed to the well-designed learning objectives effectively utilizing the source similarity criterion.
This work can be served as a pilot study towards learning efficient inheritable models in future.
9
Under review as a conference paper at ICLR 2020
References
Mahsa Baktashmotlagh, Masoud Faraki, Tom Drummond, and Mathieu Salzmann. Learning factor-
ized representations for open-set domain adaptation. In International Conference on Learning
Representations, 2019. 1
Konstantinos Bousmalis, Nathan Silberman, David Dohan, Dumitru Erhan, and Dilip Krishnan.
Unsupervised pixel-level domain adaptation with generative adversarial networks. In Proceedings
of the IEEE conference on computer vision and pattern recognition, 2017. 2, 3
Zhangjie Cao, Mingsheng Long, Jianmin Wang, and Michael I Jordan. Partial transfer learning with
selective adversarial networks. In Proceedings of the IEEE conference on computer vision and
pattern recognition, 2018a. 1, 2, 5
Zhangjie Cao, Lijia Ma, Mingsheng Long, and Jianmin Wang. Partial adversarial domain adaptation.
In Proceedings of the European Conference on Computer Vision, 2018b. 1, 2, 5
Yi-Hsin Chen, Wei-Yu Chen, Yu-Ting Chen, Bo-Cheng Tsai, Yu-Chiang Frank Wang, and Min Sun.
No more discrimination: Cross city adaptation of road scene segmenters. In Proceedings of the
IEEE International Conference on Computer Vision, 2017. 1
Lixin Duan, Ivor W Tsang, and Dong Xu. Domain transfer multiple kernel learning. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence, 34(3):465-479, 2012. 2
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Frangois
Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks.
The Journal of Machine Learning Research, 17(1):2096-2030, 2016. 1
ZongYuan Ge, Sergey Demyanov, Zetao Chen, and Rahil Garnavi. Generative openmax for multi-
class open set classification. arXiv preprint arXiv:1707.07418, 2017. 1
Boqing Gong, Yuan Shi, Fei Sha, and Kristen Grauman. Geodesic flow kernel for unsupervised
domain adaptation. In Proceedings of the IEEE conference on computer vision and pattern
recognition, 2012. 6
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural informa-
tion processing systems, 2014. 2
Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In Advances
in neural information processing systems, 2005. 6
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
2016. 7, 8
Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei A Efros,
and Trevor Darrell. Cycada: Cycle-consistent adversarial domain adaptation. In International
Conference on Learning Representations, 2018. 2
Lanqing Hu, Meina Kan, Shiguang Shan, and Xilin Chen. Duplex generative adversarial network for
unsupervised domain adaptation. In Proceedings of the IEEE conference on computer vision and
pattern recognition, 2018. 2
Guoliang Kang, Liang Zheng, Yan Yan, and Yi Yang. Deep adversarial attention alignment for
unsupervised domain adaptation: the benefit of target expectation maximization. In Proceedings of
the European Conference on Computer Vision, 2018. 2
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014. 7
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013. 5
10
Under review as a conference paper at ICLR 2020
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolu-
tional neural networks. In Advances in neural information processing systems, 2012. 1
Abhishek Kumar, Prasanna Sattigeri, Kahini Wadhawan, Leonid Karlinsky, Rogerio Feris, Bill
Freeman, and Gregory Wornell. Co-regularized alignment for unsupervised domain adaptation. In
Advances in neural information processing systems, 2018. 1
Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan. Learning transferable features with
deep adaptation networks. In International Conference on Machine Learning, 2015. 1, 2, 17
Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I Jordan. Unsupervised domain adaptation
with residual transfer networks. In Advances in neural information processing systems, 2016. 1, 2,
6
Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I Jordan. Conditional adversarial
domain adaptation. In Advances in neural information processing systems, 2018. 2, 17
Zelun Luo, Yuliang Zou, Judy Hoffman, and Li F Fei-Fei. Label efficient learning of transferable
representations acrosss domains and tasks. In Advances in neural information processing systems,
2017. 2
Jogendra Nath Kundu, Phani Krishna Uppala, Anuj Pahuja, and R Venkatesh Babu. Adadepth:
Unsupervised content congruent adaptation for depth estimation. In Proceedings of the IEEE
conference on computer vision and pattern recognition, 2018. 1, 2
Pau Panareda Busto and Juergen Gall. Open set domain adaptation. In Proceedings of the IEEE
International Conference on Computer Vision, 2017. 2
Xingchao Peng, Ben Usman, Neela Kaushik, Judy Hoffman, Dequan Wang, and Kate Saenko. Visda:
The visual domain adaptation challenge. 2018. 6
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object
detection with region proposal networks. In Advances in neural information processing systems,
2015. 1
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition
challenge. International Journal of Computer Vision, 115(3):211-252, 2015. 3, 7
Kate Saenko, Brian Kulis, Mario Fritz, and Trevor Darrell. Adapting visual category models to new
domains. In Proceedings of the European Conference on Computer Vision, 2010. 2, 6, 16, 20
Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tatsuya Harada. Maximum classifier
discrepancy for unsupervised domain adaptation. In Proceedings of the IEEE conference on
computer vision and pattern recognition, 2018a. 3, 5
Kuniaki Saito, Shohei Yamamoto, Yoshitaka Ushiku, and Tatsuya Harada. Open set domain adaptation
by backpropagation. In Proceedings of the European Conference on Computer Vision, 2018b. 2, 7,
8
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans. In Advances in neural information processing systems,
2016. 3
Swami Sankaranarayanan, Yogesh Balaji, Carlos D Castillo, and Rama Chellappa. Generate to adapt:
Aligning domains using generative adversarial networks. In Proceedings of the IEEE conference
on computer vision and pattern recognition, 2018. 2, 3
Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the log-
likelihood function. Journal of statistical planning and inference, 90(2):227-244, 2000. 1
Rui Shu, Hung Bui, Hirokazu Narui, and Stefano Ermon. A DIRT-t approach to unsupervised domain
adaptation. In International Conference on Learning Representations, 2018. 4
11
Under review as a conference paper at ICLR 2020
Eric Tzeng, Judy Hoffman, Trevor Darrell, and Kate Saenko. Simultaneous deep transfer across
domains and tasks. In Proceedings of the IEEE International Conference on Computer Vision,
2015. 1
Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain
adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition,
2017. 1,2,3,5, 17
Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep
hashing network for unsupervised domain adaptation. In Proceedings of the IEEE conference on
computer vision and pattern recognition, 2017. 6
Xuezhi Wang and Jeff Schneider. Flexible transfer learning under support and model shift. In
Advances in neural information processing systems, 2014. 2
Zirui Wang, Zihang Dai, Barnabas P6czos, and Jaime Carbonell. Characterizing and avoiding negative
transfer. In Proceedings of the IEEE conference on computer vision and pattern recognition, 2019.
2
Kaichao You, Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I. Jordan. Universal
domain adaptation. In Proceedings of the IEEE conference on computer vision and pattern
recognition, June 2019. 2, 5, 6, 7, 8, 9, 16, 17, 18, 19, 20
Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empirical
risk minimization. In International Conference on Learning Representations, 2018a. 4
Jing Zhang, Zewei Ding, Wanqing Li, and Philip Ogunbona. Importance weighted adversarial nets
for partial domain adaptation. In Proceedings of the IEEE conference on computer vision and
pattern recognition, 2018b. 1, 2, 7, 8
Kun Zhang, Bernhard Scholkopf, Krikamol Muandet, and Zhikun Wang. Domain adaptation under
target and conditional shift. In International Conference on Machine Learning, 2013. 2
Weichen Zhang, Wanli Ouyang, Wen Li, and Dong Xu. Collaborative and adversarial network for
unsupervised domain adaptation. In Proceedings of the IEEE conference on computer vision and
pattern recognition, 2018c. 1
Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10
million image database for scene recognition. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 2017. 16
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation
using cycle-consistent adversarial networks. In Proceedings of the IEEE international conference
on computer vision, 2017. 2
12
Under review as a conference paper at ICLR 2020
A Appendix
This appendix is organized as follows,
• Implementation details
-Procurement Stage.
-Deployment Stage.
• Ablation Studies and Additional Results
-	Pretraining the backbone network on Places instead of ImageNet.
-	Space and Time Complexity Analysis.
-	Varying label-set relationship.
-	Sensitivity analysis.
-	Closed-set adaptation.
-	Accuracy on source dataset post Procurement.
-	Incremental one-shot classification.
-	Feature Space Visualization.
• Miscellaneous
-	Specification of Computing Resources.
-	References to code
B Implementation Details
In this section, we describe the architecture and the training process used for the Procurement and
Deployment stages of our approach.
B.1	Procurement Stage
a)	Design of classifier D used in the Procurement stage. Keeping in mind the possibility of an
additional domain shift after performing adaptation (e.g. encountering domain W after performing
the adaptation A → D in Office-31 dataset), we design the classifier’s architecture in a manner
which allows for dynamic modification in the number of negative classes post-procurement. We
achieve this by maintaining two separate classifiers during Procurement - Dsrc, that operates on the
positive source classes, and, Dneg that operates on the negative source classes (see architecture in
Table 5). The final classification score is obtained by computing softmax over the concatenation
of logit vectors produced by Dsrc and Dneg . Therefore, the model can be retrained on a different
number of negative classes post deployment (using another negative class classifier Dn0 eg), thus
preparing it for a subsequent adaptation step to another domain.
b)	Negative dataset generation. We propose two methods to generate negative samples for the
Procurement stage, and name the models trained subsequently as USFDA-a and USFDA-b. Here, we
describe the two processes:
• Using image-composition for Dn(a) (USFDA-a). In the presence of domain knowledge
(knowledge of the task at hand, i.e. object recognition using images), we generate the
negative dataset Dn(a) by compositing images taken from different classes, as described
in Algo. 2. We generate random masks using quadratic splines passing through a central
image region (lines 3-9). Using these masks, we merge alternate regions of the images, both
horizontally and vertically, resulting in 4 negative images for each pair of images (lines
10-13). To effectively cover the inter-class negative region, we randomly sample image pairs
from Ds belonging to different classes, however we do not impose any constraint on how
the classes are selected (for e.g. one can composite images from an animal and a non-animal
class). We choose 5000 pairs for tasks on Office-31, Office-Home and VisDA datasets,
and 12000 for ImageNet-Caltech. Since the input source distribution (p) is fixed we first
synthesize a negative dataset offline (instead of creating them on the fly) to ensure finiteness
of the training set. The training algorithm for USFDA-a is given in Algo. 1.
13
Under review as a conference paper at ICLR 2020
Algorithm 2 Image-composition algorithm
1:	input: Image pair (I1,I2) ∈ Ds. (image shape HXWx3 = 224x224x3)
2:	k — 30
3:	xι, x2, yι, y2 - rand(0, W), rand(0, W), rand(0, H), rand(0, H)
4:	cx, cy —- rand(W/2 - k, W/2 + k), rand(H/2 - k/3, H/2 + k/3)
5:	dx, dy —- rand(W/2 - k/3, W/2 + k/3), rand(H/2 - k, H/2 + k)
6:	s1 —- quadratic_interpolation([(0, y1), (cx, cy), (223, y2)])	. horizontal splicing
7:	s2 —- quadratic_interpolation([(x1, 0), (dx, dy), (x2, 223)])	. vertical splicing
8:	m1 —- mask region below s1
9:	m2 —- mask region to the left of s2
10:	Ia — mi * Ii + (1 - mi) * I2
11:	Ib — m2 * Ii + (1 — m2) * I2
12:	Ic —- mi * I2 + (1 - mi ) * Ii
13:	Id —— m2 * I2 + (1 — m2) * Ii
14:	return Ia, Ib, Ic, Id
Algorithm 3 Dataset generation using latent-simulated negative samples
1:	input: class-wise source priors N(小生，∑%∙), global source prior N(μ, Σ), number of required
samples n, source classifier Dsrc	. || signifies an Append Operation
~ ~ - ~
2:	U T}; Y 一{}
3:	while |U| ≤ n do
4:	Let λcj and lcj be the maximum eigen value and the corresponding eigen vector
of Σcj, for each class cj
5:	Ur 〜N(μ, Σ)
6:	if P (Ur |cj)	< P (μcj	+ 3 *	∙∖∕λCj_	*	lc-	|	Cj) for all class	Cjthen
7:	y J σ(Dsrc(Ur))
8:	yr J assign the negative class based on the top-2 confident classes in y
~ ~ .. ~ ~ ..
9:	U JU || Ur J JY || yr
10:	else
11:	reject Ur
12:	return U , Y
• Using latent-simulated negative samples for Dn(b) (USFDA-b): Here, we perform rejec-
tion sampling as given in Algorithm 3. Here, we obtain a sample from the global source prior
P(Us) = N(Us ∣μ, Σ), where μ and Σ are the mean and covariance computed at u-space
over all the positive source image samples.We reject the sample if it lies within the 3-sigma
bound of any class (i.e. we keep the sample if it is far away from all source class-priors,
N(μci, Σci)), as shown in lines 6 to 11 in Algo. 3. A sample selected in this fashion is
expected to lie in an intermediate region between the source class priors. The two classes in
the vicinity of the sample are then determined by obtaining the two most confident class
predictions given by the classifier Dsrc (lines 7 and 8). Using this pair of classes, we assign
a unique negative class label to the sample which corresponds to the intermediate region
between the pair of classes. Note, to learn the arrangement of positive and negative clusters,
the feature extractor Fs must be trained using negative samples. We do this by passing
the sampled latent-simulated negative instance (Ur) through the decoder-encoder pair, (i.e.
D ◦ Fs ◦ G(Ur)), and enforcing the cross-entropy loss to classify them into the respective
negative class. The training algorithm for USFDA-b is given in Algo. 4.
c) Justification of Lp. The cross-entropy loss on the likelihoods (referred as Lp in the paper) not
only enforces intra-class compactness but also ensures inter-class separability in the embedding space,
U. Since the negative samples are only an approximation of future target private classes expected
to be encountered during deployment, we choose not to employ this loss for them. Such a training
procedure, eventually results in a natural development of bias towards the confident positive source
classes. This subsequently leads to the placement of source clusters in a manner which enables
source-free adaptation (See Fig. 4).
14
Under review as a conference paper at ICLR 2020
Algorithm 4 Training algorithm for USFDA-b in the Procurement stage
1:	input: (xs, ys) ∈ Ds; θFs , θD, θG: Parameters of Fs, D and G respectively.
2:	initialization: pretrain {Θfs , Θd } using cross-entropy loss on (Xs ,y§), then, compute the sample mean μ.
and covariance Σci of Fs ◦ M(xs) for xs from class ci, for i = 1, 2, ...|Cs |
3:	for iter < M axIter do
4:	Vs	= M(xs);	us =	Fs(vs);	vs	= G(Us);	Ur	ZN(μ%, ∑%) for i =	1, 2,…|Cs|;	Ur =	Fs	◦ G(Ur)
5:	(Ur, yr) = sample latent-simulated negative instances from Dnb)
6:	ysks) = σ(ks)(D ◦ Fs ◦ M(xs)), and ynkn = σ(kn)(D ◦ Fs ◦ G(Ur)) where ks and kn are the index
of ground-truth label ys and yn respectively, and σ is the softmax activation.
7:	LCE = 一 logy(ks)	一 alogynkn);	Lv	=	|vs	一	Vs|;	Lu =	∣Ur	一 Ur|
8:	Lp = 一 log(exp(P(Us∣Cks))/Pi=I exp(P(Us∣Ci))), where P(Us∣ci) = N(Us∣μ%, ∑%)
9:	Update θFs , θD , θG by minimizing LCE, Lv, Lu, and Lp alternatively using separate optimizers.
10:	if (iter % U pdateI ter == 0) then
11:	Recompute μ%, Σ/ for each source class Ci ; Generate Dnb) using the updated priors.
d)	Minibatch negative sampling strategy. We create an unbiased batch of training samples for a
training iteration by sampling equal number of positive and negative samples from the dataset. For
USFDA-a we sample 32 positive images (b+ve = 32) and 32 negative images per training iteration
(b-ve = 32). Similarly, for USFDA-b we sample 32 positive images and 32 latent-simulated negative
samples. This gives an effective batch size of b+ve + b-ve = 64.
e)	Use of multiple optimizers for training. In the presence of multiple loss terms, we subvert a
time-consuming loss-weighting scheme search by making use of multiple Adam optimizers during
training. Essentially, we define a separate optimizer for each loss term, and optimize only one of the
losses (chosen in a round robin fashion) in each iteration of training. We use a learning rate of 0.0001
during training. Intuitively, the higher order moment parameters in the Adam optimizer adaptively
scale the gradients as required by the loss landscape.
f)	Label-Set Relationships. For Office-31 dataset in the UDA setting, we use the 10 classes shared
by Office-31 and Caltech-256 as the shared label-set C. These classes are: back_pack, calculator,
keyboard, monitor, mouse, mug, bike, Iaptop_computer headphones, projector. From the remaining
classes, in alphabetical order, we choose the first 10 classes as source-private (Cs) classes, and the
rest 11 as target-private (Ct) classes. ForVisDA, alphabetically, the first 6 classes are considered C,
the next 3 as Cs and the last 3 comprise CtJhe Office-Home dataset ha“65 categories, of which we
use the first 10 classes as C, the next 5 for Cs, and the rest 50 classes as Ct.
B.2 Deployment S tage
The details of the architecture used during the Deployment stage are given in Table 7. Note that the
Feature Decoder G used during the Procurement stage, is not available during the Deployment stage,
restricting complete access to the source data.
Training during the Deployment stage. The only trainable component is the Feature Extractor Ft,
which is initialized from Fs at Deployment. Here, the SSM is calculated by passing the target images
through the network trained on source data (source model), i.e for each image xt, we calculate y =
softmax(D ◦ Fs ◦ M(xt)). Note that the softmax is calculated over all |Cs | + |Cn| classes. This is
done by concatenating the outputs of Dsrc and Dneg , and then calculating softmax. Then, the SSM is
determined by the exponential confidence of a target sample, where confidence is the highest softmax
value in the categories in |Cs |.
C Ablation studies and Additional results
C.1 Pretraining the backbone network on Places instead of ImageNet.
We find that widely adopted standard domain adaptation datasets such as Office-31 and VisDA often
share a part or all of their label-set with ImageNet. Therefore, to validate our method’s applicability
when initialized from a network pretrained on an unrelated dataset, we attempt to solve the adaptation
15
Under review as a conference paper at ICLR 2020
Table 3: Evaluation of the proposed method on A→D task of Office-31 (Saenko et al., 2010)
dataset, pretraining the ResNet-50 backbone (M) on Places instead of Imagenet. Note that, we set
|C |/|Cs ∪ Ct| = 0.32, similiar to the setting used in Table 2 of the main paper. Additionally, the last
two columns of the table show a comparison between our method and UAN (You et al., 2019) with
regard to the number of trainable parameters and total training time for adaptation
Method	ResNet-50 finetuning	Avg. Per-ClaSS accuracy, Tavg	Number of Trainable ParameterS	Training time for AdaPtation
UAN*	✓	60.98	26.7 Million	280S
UAN*	X	52.48	5.6 Million	125S
USFDA-a	X	62.74	—	3.5 Million	44s
A
Figure 7: Comparison of target-unknown accuracy Tunk across varied label-set relationships for the
task A→D in Office-31 dataset. A) Visual representation of label-set relationships and Tunk at the
corresponding instances for B) UAN* (You et al., 2019) and C) ours source-free model. Effectively,
the direction along x-axis (blue horizontal arrow) characterizes increasing Open-set complexity. The
direction along y-axis (red vertical arrow) shows increasing complexity of Partial DA scenario. And
the pink diagonal arrow denotes the effect of decreasing shared label space.
task A→D in Office-31 dataset by pretraining the ResNet-50 backbone on Places dataset (Zhou et al.,
2017). In Table 3 it can be observed that our method outperforms even source-dependent methods
(e.g. UAN (You et al., 2019), which is also initialized a ResNet-50 backbone pretrained on Places
dataset). In contrast to our method, the algorithm in UAN involves ResNet-50 finetuning. Therefore,
we also compare against a variant of UAN with a frozen backbone network, by inserting an additional
feature extractor that operates on the features extracted from ResNet-50 (similar to Fs in the proposed
method). The architecture of the feature extractor used for this variant of UAN is outlined in Table 6.
We observe that our method significantly outperforms this variant of UAN with lesser number of
trainable parameters (see Table 3).
C.2 Space and Time Complexity Analysis.
On account of keeping the weights of the backbone network frozen throughout the training process,
and devoid of ad-hoc networks such as adversarial discriminator our method makes use of significantly
lesser trainable parameters when compared to previous methods such as UAN (See Table 3). Devoid
of adversarial training, the proposed method also has a significantly lesser total training time for
adaptation: 44 sec versus 280 sec in UAN (for the A→D task of Office-31 and batch size of 32).
Therefore, the proposed framework offers a much simpler adaptation pipeline, with a superior time
and space complexity and at the same time achieves state-of-the-art domain adaptation performance
across different datasets, even without accessing labeled source data at the time of adaptation (See
Table 3). This corroborates the superiority of our method in real-time deployment scenarios.
C.3 Varying label-set relationship
In addition to the Tavg reported in Fig. 6 in the paper, we also compare the target-unknown accuracy
Tunk for UAN* and our pipeline. The results are presented in Figure 7. Refer the link to the code
provided in the submission for details of the chosen class labels for each adaptation scenario shown in
Figure 7. Clearly, our method achieves a statistically significant improvement on most of the label-set
16
Under review as a conference paper at ICLR 2020
0.9
0.8
0.7
Al
1.0
1
2
Batch-size ratio,
1.5	:
,b ∖ d b ,
1 0.5
0.6
0.5
0.25	0.5	0.75	1 0.25	0.5	0.75	1 0.25	0.5	0.75	1 0.001 0.01	0.1
Value of ∣C"严。2	Value of Cjl.∣∕c C,2	Value of CCJC2	Value of a.
Figure 8: A. Sensitivity against |Cn|, represented by ∣Cn∣∕lCslC2 for varying |C§| or |CJ (See fig.
legend) by fixing the others (top cyan box), across varied datasets. B. Sensitivity against α and
batch-size ratio (fixed b+ve + b-ve = 64). Note the scale of X and Y-axis.
Table 4: Accuracy (%) on unsupervised closed-set DA (all use ResNet50). Ours is w/o hyperparmeter
tuning. Refer Section C.5.
Closed-set DA methods	source- free	Universal- DA	D→A	A→D	A→W	Office-31 W→D	W→A	D→W	Avg.	VisDA S → R
DAN (ICML’15)	X	X	63.6	-78.6	80.5	-996-	62.8	97.1	80.4	61.1 -
ADDA (CVPR’17)	X	X	69.5	77.8	86.2	98.4	68.9	96.2	82.8	-
CDAN (NeUrIPS’18)	X	X	70.1	89.8	93.1	100	68.0	98.2	86.5	66.8
UAN (CVPR’19)	X	✓	68.4	-853-	81.2	-991-	69.7	98.1	83.6	-
Ours USFDA-a (source-free)	/	/	70.4	85.4	81.6	98.0	69.4	98.4	83.9	59.8
relationships over UAN. This demonstrates the capability of our algorithm to detect outlier classes
more efficiently than UAN, which can be attributed to the ingeniously developed Procurement stage.
C.4 Sensitivity Analysis
In all our experiments (across datasets as in Tables 1 and 2 and across varied label-set relationships
as in Fig. 6), we fix the hyperparameters as, α = 0.2, β = 0.1, |Cn| = |Cs|C2 and b+ve∕b-ve = 1.
As mentioned in Section 4.3, one can treat these hyperparameters as global constants. In Fig. 8 we
demonstrate the sensitivity of the model to these hyperparameters. Specifically, in Fig. 8A we show
the sensitivity of the adaptation performance, to the choice of |Cn | during the Procurement stage,
across a spectrum of label-set relationships. In Fig. 8B we show the sensitivity of the model to α
and the batch-size ratio b+ve∕b-ve. Sensitivity to β is shown in Fig. 5. Clearly, the model achieves a
reasonably low sensitivity to the hyperparameters, even in the challenging source-free scenario.
C.5 Closed-set adaptation
We additionally evaluate our method in the unsupervised closed set adaptation scenario. In Table 4
we compare with the closed set domain adaptation methods DAN (Long et al., 2015), ADDA (Tzeng
et al., 2017), CDAN (Long et al., 2018) and the universal domain adaptation method UAN (You et al.,
2019). Note that, DAN, ADDA and CDAN rely on the assumption of a shared label space between
the source and the target, and hence are not suited for a universal setting. Furthermore, all other
methods require an explicit retraining on the source data during adaptation to perform well, even
in the closed-set scenario. This clearly establishes the superiority of our method in the source-free
setting.
C.6 Accuracy on source dataset post procurement
We observe in our experiments that the accuracy on the source samples does not drop as a result of
the partially generative framework. For the experiments conducted in Fig. 5C, we observe similar
classification accuracy on the source validation set, on increasing the number of negative classes from
0 to 190. This effect can be attributed to a carefully chosen α = 0.2, which is deliberately biased
towards positive source samples to help maintain the discriminative power of the model even in the
presence of class imbalance (i.e. |Cn| |Cs |). This enhances the model’s generative ability without
compromising on the discriminative capacity on the positive source samples.
17
Under review as a conference paper at ICLR 2020
t-SNE points:
•	Target-private
•	Source-private
•	Source-shared
• Target-shared
Category clusters:
O Targets-Shared clusters
(notice overlap with source)
O Source-shared clusters
(notice the compactness)
Q Source-private clusters
(notice the compactness)
O Target-private clusters
(placed away from source-shared
and source-private)
Figure 9: t-SNE plot showing placement of all the four clusters computed after adaptation for the task
A→D in Office-31. It validates our hypothesis in both Procurement and Deployment stages as shown
by the highlighted clusters and the corresponding inferences in the legend under "Category clusters".
C.7 Incremental one-shot classification
In universal adaptation, we seek to transfer the knowledge of "class separability" obtained from the
source domain to the deployed target environment. More concretely, it is attributed to the segregation
of data samples based on an expected characteristics, such as classification of objects according to
their pose, color, or shape etc. To quantify this, We consider an extreme case where Cs ∩ Ct = 0
(A→D in Office-31 with |Cs | = 15, |Ct| = 16). Considering access to a single labeled target
sample from each target category in Ct = Ct, which are denoted as xcj, where j = 1,2,.., |Ct|,
we perform one-shot Nearest-Neighbour based classification by obtaining the predicted class label
as Ct = argminJIFt ◦ M(Xt) — Ft ◦ M(Xcj)∣∣2. Then, the classification accuracy for the entire
target set is computed by comparing Ct with the corresponding ground-truth category. We obtain
64.72% accuracy for the proposed framework as compared to 13.43% for UAN* (You et al., 2019).
A higher accuracy indicates that, the samples are inherently clustered in the intermediate feature level
M ◦ Ft(xt) validating an efficient transfer of “class separability” in a fully unsupervised manner.
C.8 Feature Space Visualization
We obtain a t-SNE plot at the intermediate feature level u for both target and source samples (see
Figure 9), where the embedding for the target samples is obtained as ut = Ft ◦ M(xt) and the
same for the source samples is obtained as us = Fs ◦ M(xs ). This is because we aim to learn
domain-specific features in contrast to domain-agnostic features as a result of the restriction imposed
by the source-free scenario ("cannot disturb placement of source clusters"). Firstly we obtain compact
clusters for the source-categories as a result of the partially generative Procurement stage. Secondly,
the target-private clusters are placed away from the source-shared and source-private as expected as a
result of the carefully formalized SSM weighting scheme in the Deployment stage. This plot clearly
validates our hypothesis.
D Miscellaneous
D. 1 Specifications of Computing Resources
For both Procurement and Deployment stages, we make use of the machine with the specifications
mentioned in Table 8. The architecture is developed and trained in Python 2.7 with PyTorch 1.0.0.
18
Under review as a conference paper at ICLR 2020
Table 5: NetwOrk arChiteCture fOr Procurement stage. Hyperparameter α						0.2
Component	Trainable?	Operation	Notation	Features	Batch Norm?	Non-Linearity
Resnet-50	X		M	2048		
(UptO AvgPOOl layer)						
Feature Extractor	✓		Fs	256		
		Input		2048	X	
		Fully COnneCted		1024	X	ELU
		Fully COnneCted		1024	✓	ELU
		Fully COnneCted		256	X	ELU
		Fully COnneCted		256	✓	ELU
Feature Decoder	✓		5	2048		
		Input		256	X	
		Fully COnneCted		1024	X	ELU
		Fully COnneCted		1024	✓	ELU
		Fully COnneCted		2048	X	ELU
		Fully COnneCted		2048	X	-
Classifier	✓		D	|Cs | + |Cn |		
		Input		256	X	
		Fully COnneCted	Dsrc	|Cs|	X	
		Input		256	X	
		Fully COnneCted	Dneg	|Cn |	X	
Table 6: Feature Extractor Architecture used for training UAN (You et al., 2019) under the "no
ResNet-50 finetuning" case (Refer Table 3 and Section C.1)
Operation	Features	Non-Linearity
Input	-2048-	
Fully COnneCted	512	ReLU
Fully COnneCted	256	ReLU
Fully COnneCted	512	ReLU
Fully COnneCted	2048	ReLU
Table 7: Network architecture for Deployment stage. Hyperparameter β = 0.1
Component	Trainable?	Operation	Notation	Features	Batch Norm?	Non-Linearity
Resnet-50	X		M	2048		
(UptO AvgPOOl layer)						
Feature Extractor	✓		Ft	256		
		Input		2048	X	
		Fully COnneCted		1024	X	ELU
		Fully COnneCted		1024	✓	ELU
		Fully COnneCted		256	X	ELU
		Fully COnneCted		256	✓	ELU
Classifier	X		D	|Cs | + |Cn |		
		Input		256	X	
		Fully COnneCted	Dsrc	|Cs|	X	
		Input		256	X	
		Fully COnneCted	Dneg	|Cn |	X	
Table 8: Specifications of the machine used for both Procurement and Deployment stages
CPU	GPU	RAM VRAM CUDA
InteIi7-7700K NVIDIAGeFOrCeGTX 1080Ti 32 GB^^11 GB V8.0.61
19
Under review as a conference paper at ICLR 2020
D.2 References to code
Proposed Method. Our complete documented code (including data loaders, training pipeline etc.)
used for running the experiments is available for reproducibility (refer to the private comment
containing the link). Details of dataset splits can be found in Section B.1. For evaluating UAN (You
et al., 2019), we execute the official implementation provided by the authors on github1.
Negative dataset creation. We have provided the complete dataset with augmentations and negative
images for the task A-→D in Office-31 (Saenko et al., 2010), along with the negative dataset creation
tool (refer to the code link).
1UAN (You et al., 2019): https://github.com/thuml/Universal-Domain-Adaptation
20