Under review as a conference paper at ICLR 2020
Unsupervised-Learning of time-varying
FEATURES
Anonymous authors
Paper under double-blind review
Ab stract
We present an architecture based on the conditional Variational Autoencoder to
learn a representation of transformations in time-sequence data. The model is con-
structed in a way that allows to identify sub-spaces of features indicating changes
between frames without learning features that are constant within a time-sequence.
Therefore, the approach disentangles content from transformations. Different
model-architectures are applied to affine image-transformations on MNIST as
well as a car-racing video-game task. Results show that the model discovers rel-
evant parameterizations, however, model architecture has a major impact on the
feature-space. It turns out, that there is an advantage of only learning features
describing change of state between images, over learning the states of the images
at each frame. In this case, we do not only achieve higher accuracy but also more
interpretable linear features. Our results also uncover the need for model architec-
tures that combine global transformations with convolutional architectures.
1	Introduction
This paper is concerned with the unsupervised learning of features that predict temporal changes
in sequence data. Given a time-varying function x(t), represented by a sequence of data points xt,
t = 1,...,N, our goal is to find a function Z(t) 8 等z(t) that encodes a representation of the
local changes X(t) 8 等x(t). In this context, we understand the time-discretized Zt as parameteri-
sation of an (unknown) transformation function f such that xt+ι = f (xt, Zt) = f (xt, zt+ι - zt),
where zt are unobserved feature-vectors. Recent developments for time-series data focus on finding
a representation of the state Zt and model the time-behaviour Zt implicitely. For example Temporal-
Difference Variational Auto-Encoders (Gregor et al., 2019) or Kalman Variational Auto-Encoders
(Fraccaro et al., 2017) model the time-behaviour via a transition kernel pt(Zt+1|Zt). In these mod-
els, the prediction of xt+1 only depends on Zt+1 akin to standard Variational Autoencoder (VAE,
Kingma & Welling, 2014). In contrast, an explicit representation of Zt offers several advantages:
•	Features that are constant within a time-series do not need to be encoded, as Zt = 0.
Constant features are instead modeled within the transformation function f (xt, Zt), which
leads to a disentanglement of transformation and content. Moreover, f can be chosen to
retain fine-grained structure ofxt, for example by modeling it through diffeomorphic warps
ofxt, as is done in deep registration approaches (Yang et al., 2017; Dalca et al., 2018).
•	If the feature-space lies on a curved manifold, modeling the tangent-space might be easier
than modeling Zt . For example, if the observed transformations are image rotations, the
transformation can be represented by elements on the unit circle S1. While this group can
be parameterized with a single variable, the wrap-around occuring between zero and 360
degrees makes it difficult to model it directly. However, if the individual rotations between
images are small, Zt can be chosen to encode the tangent-space at Zt with a single variable.
•	Calculating Zt = g(xt+ι, Xt) can be more precise than modeling it via Zt = Zt+ι - Zt.
As Z(t) is a time-derivative, its precision is highly dependent on noise on Z(t). Thus,
computing Zt and Zt+ι independently and computing Zt from those point-estimates will
lead to a potentially large error in calculation.
Our approach is similar in spirit to Slow Feature Analysis (SFA, Wiskott & Sejnowski, 2002), which
finds signals that vary slowly in time, but does not provide a generative model for xt+1. Further,
1
Under review as a conference paper at ICLR 2020
(a)
(b)
Figure 1: We assume the underlying sequence to follow a graphical model in the form of (a), where
c encodes constant and zt encode dynamic features. Observed variables are shaded. Our model uses
the graphical model shown in (b) to approximate this sequence without a representation of c.
SFA is known to learn representations for features that are constant over a time-sequence. Because of
this, we replace the principle of slowness with the information-bottleneck of the VAE and condition
the reconstruction term on previous observations using a conditional VAE (CVAE, Sohn et al., 2015)
to remove constant features. This way, our model learns a low-dimensional parameterization of a
transformation. This allows us to analyze how commonly used neural network architectures encode
geometric information of objects. To our knowledge, our model is the first that learns directly
visualizable parameterisations of such geometric features. The focus of the remainder of the paper
is to introduce the model and the challenges that it poses for current network architectures. In the
experiments, we will train our model on datasets with geometric transformations and analyse how
different design choices influence the learned representations.
2	Transformation Encoding Variational Autoencoders
In this section, we describe the Transformation Encoding Variational Autoencoder (TEVAE). We
assume that the generative process underlying the time-series has the form
N
p(x1, . . . ,xN,z1, . . . ,zN, c) = p(z1)p(c)	p(xt|c, zt)p(zt+1|zt) ,
t=1
where c represents a set of hidden features which are constant over the complete time-series and
zt represent time-varying features. We assume that p(z0) = p(zt), t = 1, . . . , N, i.e., the process
is stationary and p(zt+1 |zt) leaves the distribution invariant. We will simplify this model in two
ways. First, we will assume that each xt carries enough information to identify zt and c. Second,
We replace p(zt+ι |zt) by a random variable that models the difference Zt = zt+ι - zt. With this,
we approximate the generative process as
N
p(xι,..., XN ,Z 1,...,Z N )= p(xι) ∏p(xt+ι∣Xt, Z t)p(Z t) ,	(1)
t=1
This model has the property that the observations xt+1 and xt-1 are conditionally independent given
xt. Thus, instead of modeling complete time-series, We can restrict ourselves to pairs of observations
(xt+1, xt). For these pairs, the initial assumption that the time-series must be stationary can be
fulfilled simply by adding the time-mirrored pair (xt, xt+1) to the dataset. We use this to prevent
that a sufficiently powerful p(xt+ι∣xt, Z t) learns almost deterministic time-transitions without using
Zt. As p(xt+ι |xt, Zt) is typically defined via a deep-neural network, exact inference is not possible
and we will use the ELBO instead:
logp(xt+ι∣xt) ≤ Eq(Zt∣χt+ι,χt) {logp(xt+ι∣xt, Zt)} - βKL (q(Zt∣xt+ι,xt)kp(Zt))	(2)
Here, we use logp(xt+ι∣xt, Zt) = -L(Xt+ι,f (xt, Zt)) + const ,where L is an error function,
for example mean-squared-error, and f is the decoder model. We introduced the regularization
parameter β for notational simplicity and it can be removed by re-scaling L. As a prior, we use
2
Under review as a conference paper at ICLR 2020
p(Zt) = N(0, I) and We set as encoder q(zt∣xt+ι, Xt) = N(μ(xt+ι, xt), Σ(xt+ι, xt)), where
Σ(xt+1, xt) is a diagonal covariance-matrix. To make sure that the latent space can be interpreted,
we add a set of constraints on q. First, we enforce that Zt = 0 represents the identity. Moreover,
if we consider pairs (xt+ι, Xt) and (xt, xt+ι), we require that μ(xt+ι, Xt) = -μ(xt, xt+ι) and
Σ(Xt+1, Xt) = Σ(Xt, Xt+1). We propose two different models that fulfill these constraints:
•	We set μ(xt+ι, Xt) = μz(xt+ι) - μz(xt) and ∑(xt+ι, Xt) = ∑z(xt+ι) + ∑z(xt). ThiS
is equivalent to modeling Zt = Zt+ι - Zt with Zt = N (μz(Xt), ∑z(Xt)). It is easy to see
that this model fulfills the constraints we impose on the structure through symmetry.
•	A relaxation of the first model leads to μ(Xt, Xt+ι) = μ(Xt+ι, Xt) - μ(Xt, Xt+ι) and
∑(Xt+ι, Xt) = ∑(Xt+ι, Xt) + ∑(Xt, Xt+ι).
In the first model, we obtain an explicit representation of Zt, however, unlike in standard VAE-
approaches, there is no structural constraint on it, i.e., we can not assume that Z is normally dis-
tributed or lies with high probability in a certain range. For the second model, we do not obtain an
explicit representation ofZ, but allow for non-linear interactions between Xt and Xt+1.
3	Decoder Architectures for the TEVAE
The fact that the decoder f (x, Z) uses a pair of a data-point X and a feature-vector Z makes it difficult
to apply standard convolutional architectures on image-data. This is because Z might encode a small
set of parameters of global transformations, for example an image-rotation, while convolutions apply
local transformations to their input. Even though some transformations like image-translations can
be reasonably encoded this way — a global translation acts the same on each pixel of the image —
most transformations, for example rotations, look very different when applied to different parts of
the image. In this case, the global parameter Z must be decoded to a local transformation before a
convolution is applied. At the same time, the decoder must be powerful enough to ensure that the
loss of information in X is minimized so that a truthful reconstruction of the identity is possible.
To our knowledge, no suitable architecture exists that allows for this flexibility on large images.
Therefore, for the remainder of this paper we will consider two simple architectures for the decoder
of the TEVAE:
Densely connected layers Here, Z and X are concatenated as a single input vector to a densely
connected feed-forward neural network f . This architecture ensures maximal flexibility of the
network. However, each layer must have O(dim(X)) neurons to ensure that the network can learn
the identity function for Z = 0. Therefore, the number of parameters and computation time grows
quickly with the size of the input.
Image-Registration If our goal is to model the transformation between images, training a neural
network to predict the next image is inefficient. Instead, we can use a neural network to predict an
image-transformation, which we can then apply to the image. To do this, let X ∈ RN ×M ×C be an
image of size N × M with C channels, let pij ∈ [0, 1]2 be the position of pixel Xij, Ix(p) ∈ RC
be the interpolated pixel-value at position p in image X and vij ∈ R2 a translation at pij . With
this, we define
Warpij (X, v) = Ix(pij +vij) .
Thus, Warp returns an image where the pixel-values are the interpolated values at the translated
image-positions. Finally, we arrive at the definition of the decoder f (x, Z) = Warp(X,g(Z)),
where g : Z → V is a deep neural-network.
4	Experiments
We conduct two experiments, one on an artifical toy-dataset based on MNIST, the other is based
on the CarRacing-v0 task. Note that in the following description, the number of hidden layers and
the number of hidden neurons are not tuned. However, we saw qualitatively the same results for
different network topologies. For the first experiment, we take images from MNIST and apply
image rotations and scalings. In this experiment, the goal is to learn a parametrisation of the applied
image transformations, independent of the image content. We create the training dataset as follows:
Let Φ(X, θ, r) be an affine transformation of the image X, where r is a scaling parameter and θ an
3
Under review as a conference paper at ICLR 2020
Figure 2: A pair of samples (x0, x1) from the CarRacing-v0 experiment. Shown are x0, x1 and the
weighted pixel-wise error contribution to L(x0, x1), brighter indicates larger error-contribution.
angle of rotation around the center of the image. The transformation Φ warps the pixel positions
pij 7→ m + rR(θ)(pij - m), where R(θ) is a rotation by angle θ and m is the center of the
image. The identity transformation is achieved with r = 1 and θ = 0. Given an image x from
the MNIST dataset, we create a pair (x1, x0) using x0 = Φ(x, θ0, r0) and x1 = Φ(x, θ1, r1). We
sample r° and θ0 from distributions r0 〜 U(0.85,1.15) and θ0 〜 U(-π, π). The first image has
therefore a random orientation and a variation in scale. The second image is rotated and scaled
relatively to the first by picking ri 〜r0 + U(-0.15,0.15). For θι We apply two different ranges:
either, we choose θi 〜U(-∏, ∏) and rotate the image arbitrarily relatively to the first, or we pick
θi 〜θo + U(-∏∕4,∏∕4),a range between -45 and 45 degrees relatively to the first image. To solve
this task, the model has to find a representation of Z that encodes the differential transformation
x1 = Φ(x0, θ1 - θ0, r1/r0).
As model architectures, we combine both encoder approaches with both decoder approaches. We
use a 2-dimensional Z, ELU activations (Clevert et al., 2016) for hidden layers and, if not mentioned
otherwise, we use linear activations for output layers. For the encoder architectures, we use a neural
network with three dense hidden layers with 98, 12 and 12 hidden-neurons. The output layer encodes
mean and log-variance separately for each dimension of Z. We use this model directly to encode
μz(x) and log ∑z(x), which we refer to as z-encoder. For the second encoder approach,which
we refer to as Z-encoder, that uses μ(xt+ι, Xt) and log Σ(xt+ι, xt), we just change the input by
stacking both input images on top of each other. For the dense-network based decoder (FFNet), we
use five hidden layers with 2 ∙ 784 dimensions each and an output layer with 784 neurons. For the
registration-based decoder (Registration), we use five hidden layers in g with 128,128, 128, 1568
and 1568 neurons. We re-interpret the output of the last hidden layer as 7 × 7 image with 32 channels
and use a transposed convolution with stride 2, two 3 × 3 filters and tanh-activation followed by an
image scaling to obtain the final v of size 28 × 28 × 2. We use border-replication in Ix to extend
the image outside the [0, 1]2 area. For training, we use the mean-squared-error (mean over number
of pixels and batch-size) with the Adam optimizer (Kingma & Ba, 2015). We choose β = 10-4,
a learning-rate schedule of at = 10-3 溜+t and a batch-size of 100. We optimize for T = 105
iterations. In total, the MNIST experiment has 8 combinations of encoder, decoder and dataset.
For brevity, we encode this as decoder/encoder/degree, e.g. FFNet/z/45。uses a dense-feed-forward
decoder, a Z-encoder and the dataset with up to 45 degree rotations between Zt and Zt+1 .
For the second experiment, we use the CarRacing-v0 environment of OpenAI-Gym (Brockman
et al., 2016) and use the methodology by Ha & Schmidhuber (2018) to obtain a set of 10000 training
sequences from random policies of length up to 1000 time-steps. We pre-process the images by
applying a 3 × 3 Gaussian smoothing kernel and then cutting out an 80 × 80 image-region with
3 color-channels excluding the black information-bar of the image. The smoothing removes a few
rendering artifacts around edges and corners and allows for the computation of image-gradients as
the original dataset only uses flat color-areas. To make the results easier to visualize, we use pairs
(Xt+2 , Xt ) with a time-stride of two. As in this dataset the car only moves forward between frames,
the resulting sequence is non-stationary. We therefore conducted a second experiment where we
also add the pairs (Xt, Xt+2).
This task is different from the MNIST task in the way that only parts of the track are visible at
any given time. Given the information of the current image, reconstructing the next image in the
4
Under review as a conference paper at ICLR 2020
0.5
0.0
α,-e>α,,lmeα,11-
-2.5
-3.0
-3.5
-0.8 -0.6 -0.4 -0.2 0.0 0.2 0.4 0.6 0.8
θι - θθ
0.5-
-N ɔɔ-
-0.5-
-ι.o-
1.0-
-1.0	-0.5	0.0	0.5	1.0
之2
45
-45
08118
(a) FFNet/z/45	(b) Registration/z/45
Figure 3: Visualisation of μ(xt+ι, Xt) for two of the models depicted in Figure 5 obtained by choos-
ing the same input image and transforming it with Φ, varying θ0 and θ1 for fixed r = 1. (a): For
the model in Figure 5d, fix θo and vary θι - θo ∈ [-∏∕4, ∏∕4]. It is clear to see that the features
approximate a piece-wise linear function with discontinuity. Note that features Zi < 2 are outside
the visible range ofFigure 5d. (b): For the model in Figure 5b, vary θ° ∈ {2π ∙ i/10 | i = 1,..., 10}
and θ1 -θ0 ∈ [-π∕4, π∕4]. Colors encode θ1 -θ0. The black line is an example track of the features
for fixed θ0 . Note that θ0 and θ0 + π lead to almost the same encoding.
sequence is impossible without additional information. To solve this issue, we use an attention
concept to weight the reconstruction of each pixel in the image using the reconstruction error
1 3	80 80
L(X, x) = 3∑ΣΣwij |xijc - Xijc | ,
c=1 i=1 j=1
where Wij = -ɪ exp((i - 30)2/202 + (j - 40)2/202) and Z is chosen such that Pi j Wij = 1. With
this the center of attention is in front of the car, slightly above the middle of the image, while the
borders of the image are less important. We show an example of a generated sample-pair and the
pixel-wise error contribution to L in Figure 2.
As models, we only use the registration based approach with both types of encoder, as a dense feed-
forward decoder would be too large for our GPUs to handle. As before, we use a similar encoder
structure for both types of encoders and dim(Z) = 10. We use three convolutional layers with 32,
64 and 64 filters of size 3 × 3 using a stride of two. Afterwards, we use three hidden layers with
512, 128 and 128 neurons. The output of the last hidden layer is the input of the output layers for
mean and log-variance. As before, we use ELU-activations in the hidden layer. The decoder uses
the same registration based architecture as in the MNIST experiments with a different number of
hidden neurons in the dense layers of g. Here we use 4 hidden layers with 10, 10, 800 and 1600
hidden neurons and afterwards interpret the output as image of size 10 × 10 × 16. As before, we
use a convolution and image-rescaling to obtain a v of size 80 × 80 × 2. For training, we use Adam
with batch-size of 5, β = 5 ∙ 10-5 and learning-rate schedule at = 5 ∙ 10-5ι00+t and T = 5 ∙ 104
training iterations.
5 Results
The results of the first experiment can be seen in Figure 5. For each of the eight settings, we overlay
a grid of reconstructions of Xt for varying choices of Zt with a scatter plot showing the means
of q(Zt | Xt, xt+ι) for varying transformations xt+ι = Φ(xt,θ,r). For the full image rotation
dataset, we show the results of transforming one symmetric and asymmetric number to visualize the
differences in encodings. We can see that the FFNet approach learned encodings that form circular
shapes, while the registration based approaches learned linear encodings. Especially in Figure 5c
we saw a very clear encoding of the first variable as scale, and the second variable as rotation of
the image. For the FFNet approaches, we saw a number of artifacts. Even though the models in
5
Under review as a conference paper at ICLR 2020
(a) Feature-1 (scaling)
(c) Feature-3 (movement)
(d) Error-histogram
-10.0 -7.5 -5.0 -2.5 0.0 2.5 5.0 7.5 10.0
Feature 1
N S3⅛φn-
-2	0	2	4	6	0	10	12
Feature 1
(f) Feature-scatter, non-stationary
(e) Feature-scatter, stationary
050505050
07520752
2 1111
Iunou
(g) Feature-histogram, stationary (h) Feature-histogram, non-
stationary
Figure 4: Visualization of the three features with largest variance of μ(xt+ι, xt). (a) - (c): Recon-
struction of features with Z chosen along the direction of the eigenvectors with largest variance using
the VAE-Z trained on the stationary dataset. (d): Histograms of reconstruction errors of the the mod-
els trained on the stationary dataset. The non-stationary results are similar. (e) - (h): Histogram and
scatter-plot of the two features with largest variance in the VAE-Z-model trained on both datasets.
Best viewed on screen.
6
UnderreVieW as a ConferenCe PaPersICLR 2020
④ q s3>9 9
S 9 999 9
3
9
9V3S9V9SS29G ❷
◎ ◎3g>x9K9© ❷ 9
(d) Registration∕z∕45°
6蒯秒9⅛z⑥©㈤
& ©
69
ktF
9 9 9
9999 S §©33
9 9 fe∙>4φ4 呼 9
5>999ss0qp
戈戈利.9JSt©就.©@
99999&&X99
99990093P
(c) Registration∕z∕45°
6 68 ③0 y&66
S 勿 9 0fo66
SSS⑦ S9Q6∖S
Sa 6 6 9⅛6 始 6
夕 戈奥 lto% 6 6
G P*(ħqxs e*d 6
SSg 砒 V⅞>Λ 6 6
g∙⅜∙m∙v6 9^66
Ss699bxβ<s<δ∙l
66 例 0 9© 昼 65
6‹66q9噎岛 E 底
S?礴l»
❷❷ 0S99999
t≤>❷ 09s>9∂∂s
吟eðs^9 S
QsQgW 9 9 9 今
999999999
999999999
999999999
(b) FFNet∕z∕45o

9 V? WWW
0 S 9 ∖p \9 990&用点
S 6 (ς⅛''B. SsS*S*θ*⅛t⅛
& U 0&*r：y 趁,⅛∙
&&& -&99Sa2
3 S>9 999fc⅛.3
99999&冷
少993，$BaJ
(a) FFNet/切450
⅛>3⅛>①339》电
⅛>⅛>⅛>⅛>⅛>pp^s
qpp∖δ5* 矽 395>
Mjs8MJMJ∖5∖3∖pυS
xsqw∖⅛2∖δ∖psb
469PP99toto
Gq 4。7"⅛∙Qfo-s
GG 008N9 修色
.■ = §5y¾&
G666666
Ue∈6<g∖5 66&
SG€€，，666
抬/心6≤6 Sf斯
7
(f) FFNeVz∕180o
(e) FFNet∕z∕180o
(g) Registration/z/180°
(h) Registration/z/180°
(i) FFNet∕z∕180o	(j) FFNet∕z∕180o	(k) Registration∕z∕180o	(1) Registration∕z∕180o
Figure 5: Visualization of the 2D latent space of z learned by different architectures on MNIST. Shown are models where the decoder has either an FFNet or
registration architecture and the encoder either models z or z. Models are trained with transformed pairs (xt, xt+ι) with relative rotations with an angle in either
[—45,45] (top row) or [—180,180] (bottom rows) degrees. Reconstructions are sampled on a z ∈ [—1.5,1.5] × [—2, 2] grid and the reconstruction of the original
image is in the center. The Overlayed scatter-plot represents the mean z-values computed by N(X叶I)Xl) for different transformed versions of xt+ι. Colors/marker
type symbolize different relative scalings: orange-squares=85%, blue-circle= 100% and green-crosses=115%. Each color has 30 points with angles between the
min-and max trained angle range. Note that in d), only a fraction of points are visible.
Under review as a conference paper at ICLR 2020
Figures 5a&b were trained with rotations of 90 degrees, the encoding seems to be placed on a half-
circle. To analyse this, We took the model from 5b and plotted μ(xt+ι, Xt) with r = 1 and rotation
angles θo ∈ {2π ∙ i/10 | i = 1,..., 10} and θι - θo ∈ {-π∕4 + π∕2 ∙ i/20 | i = 0,..., 20}.
The result can be seen in Figure 3b. We can see that for each choice of xt, the sampled Zt lie on a
half-circle. Moreover, when varying the orientation of Xt, the circle rotates and when we compare
the features obtained from Xt with θ0 and θ0 + π we observe that the representations almost overlap.
Therefore, by construction of the model, we can conclude that z lie on a double circle where one
rotation is 180 degrees. When training on full rotations, Figures 5e,f,i&j, we see that the rotations of
the symmetric number one lie on a full circle, while the asymmetric number six has a double-circle
shape, where both circles are connected by a twist, forming a prezel-shape. Closer inspection reveals
that each circle maps a range of 180 degree rotations. When we performed similar reconstructions
of other numbers (not shown), we saw that the inner circle growed the more symmetric the numbers
were until they overlap for fully symmetric numbers. There is no visible difference between the z
and Z encodings. For the registration approach, we saw a different set of artifacts. In Figure 5d only a
subset of sampled points are visible. A closer inspection of the range of Z values, Figure 3b, reveals
that the learned encoding of Z (and Z by construction) is a piecewise-linear function with large
jumps. When plotting a larger area of the space (not shown), we found that the decoder had matching
discontinuities that allow for correct reconstruction. Thus, the encoding developed multiple Z-values
that parameterize the same rotation. However, when using the full dataset, the model encoding Z,
Figures 5h&l only managed to reconstruct one half of the transformations correctly. We did not see
any relevant artifacts for the registration model that encoded Z.
Looking at the second experiment, Figure 4, we observe that encoding Z lead to superior perfor-
mance (Figure 4d). Using PCA, we found that only a 3 dimensional subspace was used, encoding
image scaling, rotation and movement. However, the subspace was not axis-aligned with any three
Zi. In contrast, the model encoding Z failed to model the movement feature. When comparing the
stationary with the non-stationary dataset, the models obtained were similar, however with different
distribution of features, as can be seen in Figures 4e-h. While all features were non-normal, we see
that between the stationary and non-stationary variant the dimension encoding scaling differed a lot.
This is because in the task the first few frames are a zoom-in towards the car. We saw a similar effect
for the movement direction(not shown), but no effect on rotation, as expected.
6 Discussion & Conclusion
In this paper we introduced a novel approach to learning transformations in time-sequences. By
learning a representation of transformations between images, we show that the VAE can be used to
reconstruct and predict image-sequences with high accuracy and find minimal, disentangled param-
eterisations of the modeled transformations.
Our results allow us to get a glimpse into how neural networks encode geometric features. We show
that deep dense neural networks encode image-orientation on a circular structure. In all experiments
we obtained two rotations around the origin for encoding the full 360 degrees, with a shape de-
pending on magnitude of transformations in the dataset and symmetry of the image. This circular
shape makes it difficult to interpret differences in orientation between images as the direction of the
difference vector depends on the position of the orientations on the circle and not only their relative
position. We enforced the difference vectors to lie on a flat manifold using a registration based ap-
proach where the transformation from difference-vector to warp is independent of the input images.
This lead to discontinuous and ambigous encodings as the model tried to flatten the circular structure
into a shape where difference vectors form a line. However, when we used a more powerful encoder
that did not compute the difference-vector from individual states, but instead from the pair of im-
ages directly, we obtained perfectly disentangled, linear features. We hypothesize, this is because
we provided the encoder with enough information to locally linearize the features and transform
them in a single coordinate-system for the decoder. Our registration based architectures are most
similar to Yang et al. (2017) and Dalca et al. (2018) in the deep-registration literature, which can be
understood in our framework as using a different type of encoder model. We see the applicability of
our approach in the medical domain as well as reinforcement learning to discover and parameterize
the space of dynamic image-features without having to learn irrelevant features that do not vary over
time. Our results also highlight the need for new architectures that merge global parameterisations
with convolutional architectures.
8
Under review as a conference paper at ICLR 2020
References
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. OpenAI Gym, 2016.
Djork-Ame Clevert, Thomas Unterthiner, and SePP Hochreiter. Fast and accurate deep network
learning by exponential linear units (ELUs). In 4th International Conference on Learning Rep-
resentations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings,
2016.
Adrian V. Dalca, Guha Balakrishnan, John Guttag, and Mert R. Sabuncu. UnsuPervised learning
for fast Probabilistic diffeomorPhic registration. In Medical Image Computing and Computer
Assisted Intervention, pp. 729-738. Springer, 2018.
Marco Fraccaro, Simon Kamronn, Ulrich Paquet, and Ole Winther. A disentangled recognition
and nonlinear dynamics model for unsupervised learning. In Advances in Neural Information
Processing Systems 30, pp. 3601-3610. Curran Associates, Inc., 2017.
Karol Gregor, George Papamakarios, Frederic Besse, Lars Buesing, and Theophane Weber. Tempo-
ral difference variational auto-encoder. In International Conference on Learning Representations,
2019.
David Ha and Jurgen Schmidhuber. Recurrent world models facilitate policy evolution. In Advances
in Neural Information Processing Systems 31, pp. 2451-2463. Curran Associates, Inc., 2018.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In 3rd Inter-
national Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9,
2015, Conference Track Proceedings, 2015.
Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In 2nd International
Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014,
Conference Track Proceedings, 2014.
Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output representation using
deep conditional generative models. In Advances in Neural Information Processing Systems 28,
pp. 3483-3491. Curran Associates, Inc., 2015.
Laurenz Wiskott and Terrence J Sejnowski. Slow feature analysis: Unsupervised learning of invari-
ances. Neural computation, 14(4):715-770, 2002.
Xiao Yang, Roland Kwitt, Martin Styner, and Marc Niethammer. Quicksilver: Fast predictive image
registration-A deep learning approach. NeuroImage, 158:378-396, 2017.
9