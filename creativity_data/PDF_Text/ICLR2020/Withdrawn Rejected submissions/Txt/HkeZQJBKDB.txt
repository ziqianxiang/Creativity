Under review as a conference paper at ICLR 2020
Universal approximations of permutation in-
variant/equivariant functions by deep neural
NETWORKS
Anonymous authors
Paper under double-blind review
Ab stract
In this paper, we develop a theory about the relationship between G-
invariant/equivariant functions and deep neural networks for finite group G. Es-
pecially, for a given G-invariant/equivariant function, we construct its universal
approximator by deep neural network whose layers equip G-actions and each affine
transformations are G-equivariant/invariant. Due to representation theory, we can
show that this approximator has far fewer free parameters than usual models.
1	Introduction
Deep neural networks have great success in many applications such as image recognition, speech
recognition, natural language process and others as Alex et al. (2012), Goodfellow et al. (2013), Wan
et al. (2013), and Silver et al. (2017). A common strategy in their works is to construct larger and
deeper networks. However, one of the main obstructions about using very deep and large networks
for learning tasks is the so-called curse of dimensionality. Namely, if the parameters’ dimension
increase, so does the required sample size. Then, the computational complexity becomes higher. An
idea to overcome this is to design models with respect to the objective structure.
Zaheer et al. (2017) designed a model adapted to machine learning tasks defined on sets, which are,
from a mathematical point of view, permutation invariant or equivariant tasks. They demonstrate
surprisingly good applicability on their method on population statistic estimation, point cloud
classification, set expansion, and outliers detection. Empirically speaking, their results are really
significant. Many researchers studied invariant/equivariant networks as Qi et al. (2017), Hartford
et al. (2018), Risi Kondor (2018), Maron et al. (2019a), Bloem-Reddy & Teh (2019), Kondor &
Trivedi (2018), and so on. Nevertheless, theoretical guarantee of their methods is not sufficiently
considered. One of our motivations is to establish a theoretical guarantee. In this paper, we prove an
invariant/equivariant version of the universal approximation theorem by constructing an approximator.
For the symmetric group, our approximator is close to the equivariant model of Zaheer et al. (2017)
in a sense (see a remark after Theorem 2.2). We can calculate the number of the free parameters
appearing in our invariant/equivariant model, and show that this number is far fewer than one of the
usual models.
For usual deep neural networks, a universal approximation theorem was first proved by Cybenko
(1989). It states that, when the width goes to infinity, a (usual) neural network with a single hidden
layer can, with arbitrary accuracy, approximate any continuous function with compact support.
Though his theorem was only for sigmoid activation functions, there are further versions of this
theorem which allows some wider classes of activation functions. In the recent literature, the most
commonly used activation function is the ReLU (Rectified Linear Unit) function, which is the one
we focus on in this paper. Some important previous works on universal approximation theorem for
ReLU activation function are by Barron (1994), Hornik et al. (1989), FUnahashi (1989), Kurkovg
& Sanguineti (2002), and Sonoda & Murata (2017). In particular, for a part of the proof of our
main theorem, we borrow the results of Sonoda & Murata (2017) and Hanin & Sellke (2017). The
interest of the universal approximation theorem in learning theory is to guarantee that we can search
in the space which contains the solutions. The universal approximation theorem states the existence
of the model which approximates the target function in arbitrary accuracy. This means that if we
use the suitable algorithm, we have the desired solutions. We cannot guarantee such situations
1
Under review as a conference paper at ICLR 2020
without the universal approximation theorem. Our universal approximation theorem allows us to
apply representation theory. By this point of view, we can calculate the number of free parameters of
our approximator.
In the equivariant case, an essential key point of the proof is the one to one correspondence between
G-equivariant functions and StabG(i)-invariant functions. Here, G is a finite group and StabG(i) is
the subgroup of G consisting of the elements which fix i. We first confirm this correspondence at the
function level. After that, we rephrase it by deep neural networks. This correspondence enables us to
reduce the equivariant case to the invariant case.
The invariant case has already established by some researchers Zaheer et al. (2017), Yarotsky (2018),
Maron et al. (2019b). For G = Sn, here Sn is the symmetric group of degree n, Zaheer et al. (2017)
showed that a representation theorem of Sn-invariant function which is famous as a solution for the
Hilbert’s 13th problem by Kolmogorov (1956) and Arnold (1957) gives us an explicit description.
Due to this theorem and the usual universal approximation theorem, we can construct a concrete deep
neural network of the invariant model. Recently, Maron et al. (2019b) proved an invariant version of
the universal approximation theorem for any finite group G using tensor structures. We borrow their
results to obtain our main results.
1.1	Contributions
Our contributions are summarized as follows:
•	We prove an invariant/equivariant version of the approximation theorems, which is a one step to
understand the behavior of deep neural networks with permutations or more generally group actions.
•	Using representation theory, we calculate the number of free parameters appearing in our models.
As a result, the number of parameter in our models is far fewer than the one of the usual models. This
means that our models are easier to train than the usual models.
•	Although our model is slightly different from the equivariant model of Zaheer et al. (2017)
for G = Sn, our theorem guarantees that our model for finite group G can approximate any G-
invariant/equivariant functions.
1.2	Related works
Group theory, or symmetry is an important concept in mathematics, physics, and machine learning.
In machine learning, deep symmetry networks (symnets) is designed by Gens & Domingos (2014)
as a generalization of convnets that forms feature maps over arbitrary symmetry groups. Group
equivariant Convolutional Neural Networks (G-CNNs) is designed by Cohen & Welling (2016), as a
natural generalization of convolutional neural networks that reduces sample complexity by exploiting
symmetries. The models for permutation invariant/equivariant tasks are designed by Zaheer et al.
(2017) to give great results on population statistic estimation, point cloud classification, set expansion,
and outlier detection.
The universal approximation theorem is one of the most classical mathematical theorems of neural
networks. As we saw in the introduction, Cybenko (1989) proved this theorem in 1989 for sigmoid
activation functions. After his achievement, some researchers showed similar results to generalize
the sigmoid function to a larger class of activation functions as Barron (1994), Hornik et al. (1989),
Funahashi (1989), Kurkovg (1992) and Sonoda & Murata (2017).
As mentioned above, the invariant case has been established. For G = Sn, Zaheer et al. (2017)
essentially proved an invariant version of an universal approximation theorem. Yarotsky (2018) gave
a more explicit Sn-invariant approximator by a shallow deep neural network. Maron et al. (2019b)
considered a G-invariant model with some tensor structures for any finite group G. An equivariant
version for finite group G by shallow (hyper-)graph neural networks is proved by Keriven & Peyre
(2019). Our architecture of approximator is different from theirs. Moreover, although they proved
only for “squashing functions” which exclude ReLU functions, our theorem allows us to use the
ReLU functions. We also remark that our setting in this paper is quite general. In particular, ours
include tensor structures, hence graph neural networks. It must be interesting to compare the numbers
of free parameters of models of us and Keriven & Peyre (2019).
2
Under review as a conference paper at ICLR 2020
2	Preliminaries and main results
In this paper, we treat fully connected deep neural networks. We mainly consider ReLU activation
functions. Here, the ReLU activation function is defined by
ReLU(x) = max(0, x).
We remark that our argument during this paper works for any activation functions which satisfy a
usual universal approximation theorem. A deep neural network is built by stacking the blocks which
consist of a linear map and a ReLU activation. More formally, it is a function Zi from Rdi to Rdi+1
defined by Zi(x) = ReLU(Wix + bi), where Wi ∈ Rdi+1 ×di , bi ∈ Rdi+1. In this case, di is called
the width of the i-th layer. The output of the deep neural networks is
Y(X) = ZH ◦ Zh-i...Z2 ◦ Zι(x),	(1)
where H is called the depth of the deep neural network. We define the width of a deep neural network
as the maximum of the widths of all layers.
Our main objects are deep neural networks which are invariant/equivariant with actions by a finite
group G. We review some facts about groups and these actions here. Some more details are written
in Appendix A. Let Sn be the group consisting of permutations of n elements {1, 2, . . . , n}. This
Sn is called the symmetric group of degree n. The symmetric group Sn acts on {1, 2, . . . , n} by the
permutation i 7→ σ-1(i) for σ ∈ Sn. By Proposition A.1 that any finite group G can be regarded as a
subgroup of Sn for a positive integer n. Then, G also acts on {1, . . . , n} by the action as an element
of Sn. For i ∈ {1, 2, . . . , n}, we define the orbit of i as Oi = G(i) = {σ(i) | σ ∈ G}. Then, the set
{1, 2, . . . , n} can be divided to a disjoint union of the orbits: {1, 2, . . . , n} = Fjm=1 Oij .
Let G be a finite group action on {1, 2, . . . , n}. For i = 1, 2, . . . , n, we define the stabilizer subgroup
StabG(i) of G associated with x by the subgroup of elements of G fixing i. Then, by Proposition
A.2, the orbit Oi and the set of cosets G/StabG(i) are bijective. When G = Sn, X = {1, 2, . . . , n},
and x = 1, we set Stabn(1) = Stab(1) := StabG(x).
We next consider an action of Sn on the vector space Rn. The left action “•" of Sn on Rn is defined
by
σ ∙ x = σ ∙ (xι,X2,...,Xn)> = (Xσ-i(1),Xσ-i(2),...,Xσ-i(n))>
for σ ∈ Sn and x = (x1, . . . , xn)> ∈ Rn. We also call this the permutation action of Sn on Rn. If
there is an injective group homomorphism 夕：G → Sn G acts on Rn by the permutation action as
an element of Sn by 夕：For σ ∈ G and X ∈ Rn, We define σ ∙ X :=夕(σ) ∙ x. Then, We simply say
that G acts on Rn .
Example 2.1. The finite group G = S? is embedded into S3 by 夕:(1 2) → (1 2), where (i j) is
transposition betWeen i and j. In this case, the orbit decomposition of {1, 2, 3} is {1, 2} t {3}. By
this embedding 夕，an S2-action on R3 is defined:夕((1 2)) ∙ (χ1,χ2,χ3)> = (χ2,χ1, χ3)>.
Example 2.2 (Tensors). The group action of G Which is a subgroup of Sn on tensors as in Maron
et al. (2019b) is realized as folloWs: An Sn-action on Rnk×a is defined by the folloWing injective
homomorphism 夕:G → Sank: We fix a bijection from {1,..., ank} to {1,...,n}k ×{1,2,...,a},
and for σ ∈ Sn 夕(σ) ∈ Sank is defined by
<Xσ) ∙ (ii ,...,ik,j) = (σ-1(iι),... ,σ-1(ik),j)
for (i1, . . . , ik) ∈ {1, . . . , n}k and j ∈ {1, 2, . . . , a}. Then, for a tensor X =
(xiι,…,ik,j)iι,...,ik=1...,n,j=1,2,...,a ∈ Rnk×a, σ ∈ Sn acts on Rnk×a by 夕(σ) ∙ X =
(xσ-1(i1),...,σ-1(ik),j). This action is same as one of Maron et al. (2019b).
Example 2.3 (n-tuple of D-dimensional vectors). We identify {1, 2, . . . , nD} With {(i, j) | i =
1,...,n,j = 1,..., D} and define 夕:Sn → SnD as 夕(σ)∙(i,j) = (σ-1(i),j). Let (xι,..., Xn) ∈
(RD)n be an n-tuple of D-dimensional vectors. Then, for σ ∈ Sn φ(σ) ∙ (xι,..., Xn) =
(Xσ-1 (1), . . . , Xσ-1(n)). This means a permutation ofn vectors.
Definition 2.1. Let G be a finite group. We assume that an injective homomorphisms 夕:G → Sm
are given. Then, G acts on Rm. We say that a map f: Rm → Rn is G-invariant if f (夕(σ)∙ x) = f (x)
for any σ ∈ G and any X ∈ Rm. We also assume that ψ : G ,→ Sn. Then, We say that a map
f: Rm → Rn G-equivariant if f (夕(σ) ∙ x) = ψ(σ) ∙ f (x) for any σ ∈ G.
When G = Sn and the actions are induced by permutation, We call G-invariant (resp. G-equivariant)
functions as permutation invariant (resp. permutation equivariant) functions.
3
Under review as a conference paper at ICLR 2020
Diagram 1: A neural network approximating Sn-invariant function f. In blue: the inputs, in red: the
output, in green: ρ and φ who have to be learned.
We define G-invariance and G-equivariance for deep neural networks. We can easily confirm that the
models in Zaheer et al. (2017) satisfies these properties.
Definition 2.2. We say that a deep neural network ZH ◦ ZH-1 . . . Z2 ◦ Z1 as (1) is G-equivariant
if an action of G on each of layers Rdi is given by embedding G ,→ Sdi and the all corresponding
map Zi : Rdi → Rdi+1 is G-equivariant. We say that a deep neural network is G-invariant if there
is a positive integer c ≤ H such that G-actions on each layer Rdi for 1 ≤ i ≤ c + 1 are given
and the corresponding map Zi : Rdi → Rdi+1 is G-equivariant for 1 ≤ i ≤ c - 1 and the map
Zc : Rdc → Rdc+1 is G-invariant.
Some approximation theorems for invariant functions have been already known:
Proposition 2.1 (G-invariant version of universal approximation theorem). Let G be a finite group
which is a subgroup of Sn. Let K be a compact set in Rn which is stable for the corresponding
G-action in Rn. Then, for any f : K → Rm which is continuous and G-invariant and for any > 0,
the following G-invariant ReLU neural networks NGinv : Rn → Rm satisfy that these represented
functions RNinv satisfy kf - RNGinv k∞ < :
•	NGinv = ∑ ◦ LH ◦ ReLU ◦ ∙∙∙ ◦ L∖, where Σ is the summation (G-invariant part) and
Li: Rndi ×ai → Rndi+1 ×ai+1 is a linear map Such that Li(g ∙ X) = g ∙ Li(X) for any
g ∈ G and any X ∈ Rndi ×ai. Here, the actions on each layers except for the output layer
are same as Example 2.2.
•	For G = Sn. NGinv = Nρ ◦ Σ ◦ (Nφ, . . . , Nφ)>, where Nρ (resp. Nφ) is a deep neural
network approximating ρ (resp. φ) defined below.
This proposition for G = Sn is proven by Zaheer et al. (2017), Yarotsky (2018). For general finite
group G, Maron et al. (2019b) proved it. Diagram 1 illustrates the Sn -invariant ReLU neural network
appeared in Proposition 2.1. The key ingredient of the proof by Zaheer et al. (2017) is the following
Kolmogorov-Arnold representation theorem:
Theorem 2.1 (Zaheer et al. (2017) Kolmogorov-Arnold’s representation theorem for permutation
actions). Let K ⊂ Rn be a compact set. Then, any continuous Sn -invariant function f : K → R can
be represented as
f(x1,...,xn) = ρ
for some continuous function ρ : Rn+1 → R. Here, φ : R → Rn+1; x 7→ (1, x, x2, . . . , xn)>.
Since φ(x) has only one variable, we line up the copies of the network which approximates φ(x).
Then, by combining Σ and the network which approximates ρ, we obtain the network which approxi-
mates f. By using the theorem of Hanin & Sellke (2017) (resp. Sonoda & Murata (2017)), we obtain
the bound of the width (resp. the depth) for approximation of φ and ρ. Maron et al. (2019b) proved
this proposition using a tensor structure.
The main theorem is a G-equivariant version of universal approximation theorem. To state the main
theorem, we need some notation. Let G be a finite group acting on Rn . We set the orbit decomposition
X φ(xi)	(2)
4
Under review as a conference paper at ICLR 2020
Diagram 2: A neural network approximating Sn -equivariant map F
of {1, 2, . . . , n} as {1, 2, . . . , n} = Fjm=1 Oij, and let Oij = {ij1 = ij, ij2, . . . , ijlj }. Without loss
of generality, we may reorder {1, 2, . . . , n} as
Oi1 = {1, 2, . . . , l1}, Oi2 = {l1 + 1, l1 + 2, . . . l1 + l2}, . . .
and ij = Pjk-=11 lk + 1 for j = 1, 2, . . . , m. For each j, let G = Flkj=1 StabG (ij)τj,k be the coset
decomposition by StabG (ij). Then, we may assume that τj,k ∈ G satisfies τj-,k1 (ij) = ij + k for
k = 1, 2, . . . , lj. Then, the main theorem is the following:
Theorem 2.2 (G-equivariant version of universal approximation theorem). Let G be a finite group
which is a subgroup of Sn . Let K be a compact set in Rn which is stable for the corresponding G-
action in Rn. Then, for any F : K → Rn; x 7→ (f1, . . . , fn) which is continuous and G-equivariant
and for any > 0, the following G-invariant ReLU neural network NGequiv : Rn → Rn satisfies that
these represented functions RN equiv satisfy kF - RN equiv k∞ < :
NGquiv= (NzbG(ii)。T1,1, NzbG(ii) ◦ T1,2, ... , NzbG(im)。Tm,lm )「
Here,NSintavbG(ij) is the StabG (ij)-invariant deep neural network approximating fij as in Proposition
2.1, and the actions on each layers are defined as follows: Each of hidden layers are written by
Rn ③R V for a vector space V. On this space, σ ∈ G acts on (xι,..., Xn) 0 V ∈ Rn ③R V
(xi ∈ Rn) by
σ ∙ ((X1, . . . , Xn) 0 V) = (ei,1 ∙ Xσ-I(1),…,em,lm ∙ Xσ-I(n)) 0 V,
where σej,k is the element of StabG (ij) satisfying σej,k = τj,k0 στj-,k1 for some k0 = 1, 2, . . . , lj.
For G = Sn, When NGquiv is represented by ZH。Zh-i。…。Zi as in (1), We can consider
that Z2 , . . . , ZH are Sn-equivariant by usual permutation. In this sense, our model is close to the
Sn-equivariant model in Zaheer et al. (2017).
Our strategy for the proof is the folloWing: At first, We establish the correspondence betWeen
StabG (ij )-invariant functions and G-equivariant functions. By this correspondence, We take
5
Under review as a conference paper at ICLR 2020
StabG(ij)-invariant function f corresponding to the objective function F. By Proposition 2.1,
we can approximate f by a StabG(ij)-invariant network NSintavb (i ). Using NSintavb (i ), we construct
the G-equivariant network which approximates F. Diagram 2 illustrates the Sn-equivariant ReLU
neural network appeared in Theorem 2.2.
Due to our universal approximation theorems, if the free parameters of the invariant/equivariant mod-
els are fewer than the ones of the usual models, we have a guarantee for using the invariant/equivariant
models. The following definition illustrates the swapping of nodes.
Definition 2.3. Let X = {1, . . . , M} be an index set of nodes in a layer. We say an Sn-action on
X is a union of permutations if X = F Xi, where each Xi has n elements and Sn acts on Xi by
permutation.
Theorem 2.3. Let N be an Sn -invariant model of depth D and width M whose number of the
equivariant layers is d (resp. an Sn-equivariant model of depth D and width M). Assume that
the action is a union of permutations on nodes in each equivariant layer. Then, the number of free
parameters in this model is bounded by M2D ∙ (2∕n2)d (resp. M2D ∙ (2∕n2)D).
Note that the number of free parameters in the fully connected model is M2D. Hence, this theorem
implies that the free parameters of the invariant/equivariant models are far fewer than the ones of the
usual models.
3 Equivariant case
In this section, we prove Theorem 2.2, namely, the equivariant version of the universal approximation
theorem. The key ingredient is the following proposition (proof is in Appendix C):
Proposition 3.1. Notations are same as Theorem 2.2. Then, a map F : Rn → Rn is G-equivariant if
and only if F can be represented by F = (f1 ◦ τ1,1, f1 ◦ τ1,2, . . . , fm ◦ τm,lm)> for some StabG (ij)-
invariant functions fj : Rn → R. Here, τj,k ∈ G is regarded as a linear map Rn → Rn.
For simplicity, we prove Theorem 2.2 only for G = Sn. We can show the general case by a similar
argument. More precisely, we construct an Sn-equivariant deep neural network approximating the
given Sn-equivariant function. Similarly, we can prove this theorem for any finite group G. To show
Theorem 2.2 for G = Sn, we divide the proof to four steps as follows:
1.	By Proposition 3.1 proved below, we reduce the argument on Sn-equivariant map F to the
one of Stab(1)-invariant function f.
2.	Modifying Theorem 2.1, we have a representation of Stab(1)-invariant function f.
3.	Using the above representation, we have a Stab(1)-invariant deep neural net which approxi-
mates f and construct a deep neural network approximating F .
4.	We introduce a certain action of Sn on (Rn)n which appears the first hidden layer naturally
and show the Sn-equivariance between the input layer and the first hidden layer.
We first investigate step 1. We recall that, during this section, we only consider the action of Sn on
Rn induced from permutation σ ∙ (xι,..., xn)> = (xσ-ι(1),..., Xσ-i(n))>. Then, We remark that
the orbit of 1 by this action is the total set O1 = {1, 2, . . . , n} and the coset decomposition of Sn by
Stab(1) is Sn = Fin=1 Stab(1)(1 i). Thus, We have the folloWing:
Corollary 3.1. A map F : Rn → Rn is Sn-equivariant if and only if there is Stab(1)-invariant
function f : Rn → R satisfying F = (f ◦ (1 1), f1 ◦ (1 2), . . . , fm ◦ (1 n))>. Here, (i j) ∈ Sn is
the transposition between i and j and is regarded as a linear map Rn → Rn.
Next, We consider step 2. The stabilizer subgroup Stabn(1) is isomorphic to Sn-1 as a group by
Lemma. Hence, We can regard the Stab(1)-invariant function f : Rn → R as an Sn-1-invariant
function. This point of vieW alloWs us to apply Theorem 2.1 to f . Hence, We have the folloWing
representation theorem of Stab(1)-invariant functions as a corollary of Theorem 2.1.
Corollary 3.2 (Representation of Stab(1)-invariant function). Let K ⊂ Rn be a compact set, let
f : K -→ R be a continuous and Stab(1)-invariant function. Then, f(x) can be represented as
f(x) = f(x1, . . . ,xn) = ρ x1, Xφ(xi) ,
6
Under review as a conference paper at ICLR 2020
Diagram 3: A neural network approximating the Stab(1)-invariant function f
for some continuous function ρ: Rn+1 -→ R. Here, φ: R → Rn is similar as in Theorem 2.1.
By this corollary, we can represent the Stab(1)-invariant function f : Rn 7-→ R as f = ρ ◦ L ◦ Φ,
where Φ : Rn → R × (Rn)n-1 andL : R × (Rn)n-1 → R × Rn are
n-1
Φ(x1, . . . ,xn) = (x1, φ(x2), . . . ,φ(xn)), L(x, (y1, . . . , yn-1)) = x,	yi .
Then, we consider step 3, namely, the existence of Stab(1)-invariant deep neural network approx-
imating the function f . After that, using this approximator, we construct a deep neural network
approximating Sn-equivariant function F. By a slight modification of the invariant version of Propo-
sition 2.1 for Stab(1)-invariant case, there exists a sequence of deep neural networks {Am}m (resp.
{Bm }m) which converges to Φ (resp. ρ) uniformly. Then, the sequence of deep neural networks
{Bm ◦ L ◦ Am }m converges to f = ρ ◦ L ◦ Φ uniformly.
Now, f can be approached by the following deep neural network by replacing ρ and Φ by universal
approximators as Diagram 3. We remark that the left part (the part of before taking sum) of this
deep neural network is naturally equivariant for the action of Stab(1). For an Sn-equivariant map
F : Rn → Rn with the natural action, by Proposition 3.1, there is a unique Stab(1)-invariant
function f such that F (x)i = (f ◦ (1 i))(x). Here, F (x) = (F (x)1, . . . , F (x)n)> and we regard
any element of Sn as a map from Rn to Rn . By the argument in this section, we can approximate
f by the previous deep neural network {Bm ◦ L ◦ Am}m. Substituting Bm ◦ L ◦ Am for f, we
construct a deep neural network approximating F as Diagram 2.
The represented function of this neural network of Fi is Bm ◦ L ◦ Am ◦ (1 i). The map F splits
into two parts, the part of transpositions and part of (f, f, . . . , f)>. On the deep neural network
corresponding to F as above, the latter part corresponds to the layers from the first hidden layer
to the output layer. This part is the n copies of same Stab(1)-invariant deep neural network (an
approximation of (f, f, . . . , f)>). Thus, this part is clearly made of equivariant stacking layers for the
permutation action of Sn. Hence, it is remained to show that the former part is also Sn-equivariant.
We here investigate bounds of the width and the depth of approximators. By Theorem 2.1, each of φ
and ρ can be approximated by a shallow neural network. Hence, if we do not bound the width, we can
obtain deep neural network approximating F with depth three. On the other hand, by Theorem 2.1
again, if we do not bound the depth, φ (resp. ρ) can be approximated by a deep neural network with
width n + 1 (resp. n + 2). Thus, we can obtain a deep neural network approximating F with width
bounded from above by n3 .
Finally, as step 4, we show that our deep neural network is an Sn-equivariant deep neural network.
The most difficult part is to show the equivariance between the input layer Rn and the first hidden
layer Rn2 presented by a function g : Rn → Rn2 as g = (g1, . . . , gn)> for gi = ReLU ◦ l ◦ (1 i) for
a certain Stab(1)-invariant linear function l : Rn → V . (Although V is equal to Rn, we distinguish
them to stress the difference.) Unfortunately, the permutation action on the latter space Rn2 does
2
not make the function g equivariant. For this reason, we need to define another action of Sn on Rn
exploiting the Stab(1)-equivariance among each copies.
7
Under review as a conference paper at ICLR 2020
Definition 3.1. We suppose that Stab(I) acts on Rn by permutation, denoted by σ ∙ X (i.e., by
regarding Stab(I) as a subgroup of Sn). Then, We define the action “*" of Sn on Rn as follows:
σ * (x1, ..., Xn) = (σ1 ∙ xσ-1(1), ..., σn ∙ xσ-1(n)) = (xσ-1 (i),σ-ι (j))i,j=1,...,n
for any σ ∈ Sn and for any (xι, X2,..., Xn) = (Xij)i,j=ι...,n ∈ (Rn)n. Here, for any i, σi is an
element of Stab⑴ defined as (1 i)σ = ①(1 σ-1(i)).
We will prove the well-definedness of this action “*” in Appendix D. This action is obtained by the
injective homomorphism
ψ : Sn → Sn2 ； σ → 中⑺,iXσ) ∙ (i, j) = (e-1(i),σ-1(j))
for (i,j) ∈ {1, 2, . . . , n}2. We remark that this action “*” naturally appears in representation theory
as the induced representation, which is an operation to construct a representation of group Sn from a
representation of the subgroup Stab(1) of Sn.
We conclude the proof of Theorem 2.2 by showing the Sn-equivariance ofg:
Lemma 3.1. The function g : Rn → Rn2 is Sn -equivariant.
The detail of proof is in Appendix E. By this lemma, we conclude the proof of Theorem 2.2. We
remark that the affine transformation g is corresponding to Z1 in the notation (1). By a representation
theoretic aspect, this is an intertwining operator between these representation spaces. This affine
transformation has n3 free parameters a priori. However, by Sn -equivariance and a representation
theoretic argument, in principle, g can be written by only five parameters. By a similar argument, for
the other hidden layers, the affine transformation Zi : Rn 0Vi → Rn 0Vi+1 has 15 dim Vi dim Vi+1
parameters (though n4 dim Vi dim Vi+1 a priori).
4 Dimension reduction
In this section, we give the proof of Theorem 2.3.
Proposition 4.1. Let Zl = ReLU ◦ Wl : RM → RN be an Sn-equivariant map. Assume that the
Sn-action on RM and RN is a union of permutations. Then, n divides M and N, and the number of
the free parameters in Wl is equal to 2M N/n2.
Proof. Since RM and RN have union of permutation actions, by considering the orbit of the cordi-
nates, we see that n divides M and N. Let us write RM = (Rn)M0 and RN = (Rn)N0. In this case,
Wl is written by sum of n × n matrices Vij, namely Wl = (Vij)in,j=1. Here, each n × n matrix Vij
corresponds to the linear map:
Rn ,→ (Rn)M0 -Z→l (Rn)N0	Rn,
where the first map is the inclusion to coordinates of (Rn)M0 and the last map is projection to
coordinates of (Rn)N0. Since these constructions are taken to be compatible with Sn-action, we see
that ReLU ◦ Vij is Sn-equivariant. If the activation functions are bijective, we are done because of
the same discussion as in the proof of Lemma 3 in Zaheer et al. (2017). But in our case, we need
more discussion, which is in Appendix G.	□
Proof of Theorem 2.3. By Proposition 4.1, the number of the free parameter in each equivariant layer
is bounded by 2M2∕n2. Hence we obtain the desired bound.	□
5 Conclusion
We introduced some invariant/equivariant models of deep neural networks which are universal approx-
imators for invariant/equivariant functions. The universal approximation theorems in this paper and
the discussion in Section 4 show that although the free parameters of our invariant/equivariant models
are far fewer than the ones of the usual models, the invariant/equivariant models can approximate
the invariant/equivariant functions to arbitrary accuracy. Our theory also implies that there is much
possibility that the group models behave as the usual models for the tasks related to groups and
representation theory can be powerful tool for theory of machine learning. This must be a good
perspective to develop the models in deep learning.
8
Under review as a conference paper at ICLR 2020
References
Krizhevsky Alex, Sutskever Sutskever, Ilya, and Hinton Geoffrey E. Imagenet classification with
deep convolutional neural networks. In Advances in neural information processing systems, pp.
1097-1105, 2012.
Vladimir I. Arnold. On functions of three variables. Proceeding of the USSR Academy of Sciences,
114:679-681, 1957. English translation: Amer. Math. Soc. Transl., 28 (1963), pp. 51-54.
Andrew R Barron. Approximation and estimation bounds for artificial neural networks. Machine
learning, 14(1):115-133, 1994.
Benjamin Bloem-Reddy and Yee Whye Teh. Probabilistic symmetry and invariant neural networks.
arXiv preprint arXiv:1901.06082, 2019. URL https://arxiv.org/abs/1901.06082.
Taco S Cohen and Max Welling. Group equivariant convolutional networks. Proceedings of the
33rd International Conference on Machine Learning, 4, 2016. URL: arXivpreprintarXiv:
1602.07576,2016..
George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control,
signals and systems, 2(4):303-314, 1989.
Ken-Ichi Funahashi. On the approximate realization of continuous mappings by neural networks.
Neural networks, 2(3):183-192, 1989.
Robert Gens and Pedro M Domingos. Deep symmetry networks. In Advances in Neural Information
Processing Systems, pp. 2537-2545, 2014.
Ian J Goodfellow, Yaroslav Bulatov, Julian Ibarz, Sacha Arnoud, and Vinay Shet. Multi-digit number
recognition from street view imagery using deep convolutional neural networks. arXiv preprint
arXiv:1312.6082, 2013.
Boris Hanin and Mark Sellke. Approximating continuous functions by relu nets of minimal width.
arXiv preprint arXiv:1710.11278, 2017.
Jason Hartford, Devon Graham, Kevin Leyton-Brown, and Siamak Ravanbakhsh. Deep models of
interactions across sets. In International Conference on Machine Learning, pp. 1914-1923, 2018.
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are
universal approximators. Neural networks, 2(5):359-366, 1989.
Nicolas Keriven and Gabriel Peyre. Universal invariant and equivariant graph neural networks. arXiv
preprint arXiv:1905.04943, 2019.
Andrey Nikolaevich Kolmogorov. On the representation of continuous functions of several variables
by superpositions of continuous functions of a smaller number of variables. Proceeding of the
USSR Academy of Sciences, 108:176-182, 1956. English translation: Amer. Math. Soc. Transl., 17
(1961), pp. 369-373.
Risi Kondor and Shubhendu Trivedi. On the generalization of equivariance and convolution in neural
networks to the action of compact groups. In Jennifer Dy and Andreas Krause (eds.), Proceedings
of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine
Learning Research, pp. 2747-2755, Stockholmsmassan, Stockholm Sweden, 10-15 Jul 2018.
PMLR.
Vera KUrkovd. Kolmogorov,s theorem and multilayer neural networks. Neural networks, 5(3):
501-506, 1992.
Vera KUrkova and Marcello Sanguineti. Comparison of worst case errors in linear and neural network
approximation. IEEE Transactions on Information Theory, 48(1):264-275, 2002.
Haggai Maron, Heli Ben-Hamu, Nadav Shamir, and Yaron Lipman. Invariant and equivariant
graph networks. In International Conference on Learning Representations, 2019a. URL https:
//openreview.net/forum?id=Syx72jC9tm.
9
Under review as a conference paper at ICLR 2020
Haggai Maron, Ethan Fetaya, Nimrod Segol, and Yaron Lipman. On the universality of invariant
networks. Proceedings of the 36th International Conference on Machine Learning, 97, 2019b.
Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for
3d classification and segmentation. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 652-660, 2017.
Horace Pan Shubhendu Trivedi Brandon Anderson Risi Kondor, Hy Truong Son. Covariant composi-
tional networks for learning graphs, 2018. URL https://openreview.net/forum?id=
S1TgE7WR-.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without
human knowledge. Nature, 550(7676):354, 2017.
Sho Sonoda and Noboru Murata. Neural network with unbounded activation functions is universal
approximator. Applied and Computational Harmonic Analysis, 43(2):233-268, 2017.
Li Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, and Rob Fergus. Regularization of neural
networks using dropconnect. In International conference on machine learning, pp. 1058-1066,
2013.
Dmitry Yarotsky. Universal approximations of invariant maps by neural networks. arXiv preprint
arXiv:1804.10306, 2018. URL: https://arxiv.org/abs/1804.10306.
Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan R Salakhutdinov,
and Alexander J Smola. Deep sets. In Advances in neural information processing systems, pp.
3391-3401, 2017.
10
Under review as a conference paper at ICLR 2020
A Review of groups and group actions
Let G be a set with product, i.e., for any g, h ∈ G, gh is defined an element of G. Then, G is called
a group if G satisfies the following conditions:
1.	There is an element e ∈ G such that eg = ge = x for all g ∈ G.
2.	For any g ∈ G, there is an element g-1 ∈ G such that gg-1 = g-1g = e.
3.	For any g, h, i ∈ G, (gh)i = g(hi) holds.
Let G, G be two finite groups. We say a map 夕：G → G0 is a (group) homomorphism if 夕(gh)=
φ(g)φ(h). This means that the map 夕 preserves the group structures of G in G0.
Next, we review actions of groups. Let X be a set. An action of G (or G-action) on X is defined as
a map G X X → X; (g,χ) → g ∙ X satisfying the following:
1.	For any X ∈ X, e ∙ X = x.
2.	For any g,h ∈ G and X ∈ X, (gh) ∙ x = g ∙ (h ∙ x).
If these conditions are satisfied, we say that G acts on X by a left action.
Example A.1. An example which we mainly consider in this paper is the permutation group Sn of n
elements:
Sn = {σ : {1, . . . , n} → {1, . . . , n}; bijective}
and the product of σ, τ ∈ Sn is given by the composition σ ◦ τ as maps. Sn acts on the set
{1, 2,...,n} by the permutation σ ∙ i = σ-1(i).
We remark that actions of Sn on X is not unique, for example, the trivial action σ ∙ X = X for any
σ ∈ Sn, X ∈ X is also one of action. Hence, when we stress the difference of some actions, we use
some distinguished notation for each actions as “•” or“*” etc.
Let G be a group and H a subset of G. We call H a subgroup of G if H is a group with the same
product as G.
Example A.2. Let G be a finite group acting on a set X . For an element X ∈ X , the stabilizer
subgroup StabG(X) ofG with respects to {X} is defined by
StabG(X) = {g ∈ G | g ∙ X = x}.
When G = Sn and X = 1, we use the following notation: Stabn(1) = Stab(1) := StabG(X).
Let G, G0 be two groups. If there is an injective homomorphism 夕：G → G0, the image 夕(G) ⊂ G0
can be a subgroup of G0. Then, We say that 夕 is an embedding of group G to G0. Moreover, if G0
acts on a set X, then G also acts on G0 through 夕，i.e., by 夕(σ) ∙ X for σ ∈ G and X ∈ X.
Then, the following proposition holds:
Proposition A.1. Any finite group G can be embedded into Sn for some n.
Proof. Let n := |G| and G = {g1, g2, . . . , gn}. For any g ∈ G, ggi = gj for some j ∈ {1, 2, . . . , n}
as ggi ∈ G. We set σg-1(i) = j. Then, we define σ : G → Sn; g 7→ σg. It is easy to show that this σ
is an injective homomorphism.	□
This proposition implies that any finite group G can be realized as a permutation action on Rn for
some n.
Let G be a finite group acting on X . Then, for X ∈ X , we define the (G-)orbit Ox of X as
Ox = G ∙ X = {g ∙ X | g ∈ G}.
Then, for X, y ∈ X , the relation that X and y are in a same orbit is an equivalent relation. Hence, X
can be divided to a disjoint union of the equivalent classes of this equivalent relation:
m
X=GOxj.
j=1
11
Under review as a conference paper at ICLR 2020
We call this the (G-)orbit decomposition of X .
Let H be a subgroup of a finite group G. Then, for g ∈ G, the set
gH = {gh ∈ G | h ∈ H}
is called the left coset of H with respect to g. The relation that two elements g and g0 are in a same
coset is also an equivalent relation. Hence, we can divide G to a disjoint union of equivalent classes
of this relation:
l
G= G giH.
i=1
We call this decomposition the right coset decomposition of G by H . We set G/H as the set of the
left cosets of G by H:
G/H = {g1H, g2H, . . . , glH}.
Then, there is a relation between an orbit and a set of cosets:
Proposition A.2. Let G be a finite group acting on a set X. For x ∈ X, the map
G/StabG (x) -→ Oχ ； gStabG (x) -→ g ∙ X
is bijective.
Proof. It is easy to check well-definedness and bijectivity.	□
For G = Sn acting on {1, 2, . . . , n}, the G-orbit of 1 is only one, i.e., O1 = {1, 2, . . . , n}. Hence,
the following holds (the permutation action of Sn is defined by taking inverse σ-1, hence we need to
consider the set of right cosets):
Corollary A.1. The map
Stabn(1) \Sn → {1, 2, . . . , n}; Stabn(1)σ 7→ σ-1(1)
is bijective.
B Proof of Proposition 2.1
Proof of Proposition 2.1. We may assume N = 1. In fact, since we consider the L∞-norm, if all
components of kf - RN k is bounded by , then kf - RN k∞ ≤ holds. For N = 1, we have
f : K → R. Then, by Theorem 2.1, we obtain the representation of f as
f(x1, . . . ,xn) = ρ X φ(xi) .
By the universal approximation property of ReLU deep neural network, we can find two sequences
of ReLU deep neural network {Nkρ}k and {Nkφ}k such that their corresponding functions {FNρ}k
and {FNφ}k tend to ρ and φ for the L∞-norm when k tends to infinity. Let {Nk}k be the sequence
of networks whose corresponding functions are FNk : (x1 , . . . , xn) 7→ FNkρ Pin=1 FNφ (xi) .
To show that {FNk}k uniformly tends to f, we use the following lemma:
Lemma B.1. If {fk}k tends uniformly to f, {gk}k tends uniformly to g and f is uniformly continuous,
then {fk ◦ gk}k tends uniformly to f ◦ g.
proof of Lemma B.1. For any k, we have
|fk ◦ gk(x) - f ◦ g(x)| = |fk ◦ gk(x) - f ◦ gk(x) + f ◦ gk(x) - f ◦ g(x)|
≤ |fk ◦ gk(x) - f ◦ gk(x)| +|f ◦ gk(x) - f ◦ g(x)|.
X-----------V-----------}
≤kfk -f k∞
Let > 0. By the uniform continuity of f, there is a δ > 0 such that for all x, y satisfying |x -y| ≤ δ,
|f (x) - f(y)| ≤ /2 holds. Then for large enough k, we have both kgk - gk∞ ≤ δ which implies
that for any x, |f ◦ gk(x) - f ◦ g(x)| ≤ "2,and ||fk - f ∣∣∞ ≤ 〃2. Hence, for any k large enough,
||fk ◦ gk - f ◦ g|| ≤ E holds.	□
12
Under review as a conference paper at ICLR 2020
Now using Lemma B.1, we have that (x1, . . . , xn) 7→ Pin=1 FNφ (xi) tends to (x1, . . . , xn) 7→
Pin=1 φ(xi), because (x1, . . . , xn) 7→ Pin=1 xi is Lipschitz (by triangular inequality) so uniformly
continuous. Then, using Lemma B.1 again, we obtain the result, since ρ is continuous on a compact
set so uniformly continuous. Moreover, by Sonoda & Murata (2017), we can approximate φ and
ρ by shallow networks. Hence, we have an approximation by a deep neural network N which has
two hidden layers and the width is not bounded. By Hanin & Sellke (2017), we can respectively
approximate each of φ and ρ by some neural networks whose width are bounded by n + 2 and the
depth is not bounded. Hence, we have an approximation by a deep neural network N whose width is
bounded by n(n + 2) and the depth is not bounded.
The invariant function f is approached by a deep neural network N having the following diagram:
.]⅞→cy→ρρ ~o
Nφ
Let us show that this is a Sn-invariant deep neural network. Since the sum (Σ) is an invariant function,
we can divide this neural network in two parts : the fist part on the left of the symbol Σ and the
second on the right. For each layer Rdi of Nφ there is a corresponding map Zi : Rdi → Rdi+1 .
Then for each layer (Rdi)n of the first part of N, there is a Sn action σ ∙ (χ.i = (χσ-i(i))i for
X = (xi)i ∈ R)di, and the corresponding map (Zι,…，Zn) : (Rdi )n → (Rdi+1 )n is Sn-equivariant.
On the second part of N , there is no Sn actions on the layers Rdj except the trivial action. Hence, for
this action, each corresponding map Zi : Rdj → Rdi+j is invariant. This shows that the network N
is Sn-invariant.	口
C Proof of Proposition 3.1
Proof of Proposition 3.1. First, we show that for StabG (ij )-invariant function fj, the map F =
(f1 ◦ σ1,1, f1 ◦ σ1,2, . . . , fm ◦ σm,lm)> ∈ Rn is G-equivariant. Without loss of generality, we may
assume that
Oi1 = {1, 2, . . . , l1}, Oi2 = {l1 + 1, l1 + 2, . . . , l1 + l2}, .
In particular, ij = Pjk-=11 lk + 1. For each j, we set the coset decomposition by
lj
G= G StabG(ij)τj,k,
k=1
where we may assume that τj,k satisfies τj-,k1(ij) = ij + k. Now, for σ ∈ G, there is a unique k0 such
that τj,kσ = σej,k τj,k0. Hence, the (ij + k)-th entry becomes
fj ◦ τj,k ◦ σ = fj ◦ σej,k ◦ τj,k0 = fj ◦ τj,k0 .
On the other hand, we have
σ-1 (ij + k) = τj-,k10 σej-,k1 τj,k (ij + k) = ij + k0.
This shows F is G-equivariant.
Conversely, we assume that F : Rn → Rn is G-equivariant. Let F = (g1, g2, . . . , gn)>. The orbit
decomposition Oi1,...,Oim is same as above. For σ ∈ G, by σ ∙ F(x) = F(σ ∙ x) for any X ∈ Rn,
we have
gi(σ ∙ X) = gσ-1(i)(X).
(3)
13
Under review as a conference paper at ICLR 2020
For simplicity, we consider only for i = 1. Then, σ-1(1) ∈ Oi1 , thus σ-1(1) = 1, . . . , l1. By the
equation (3), for σ ∈ StabG(1), we have
gι(σ ∙ x) = gι(x),
as σ ∈ StabG(1) if and only if σ-1 ∈ StabG(1). Hence, g1 is G-invariant. The equation (3) for τ1,k
(k = 1, 2, . . . , l1) implies
gι(τι,k ∙ x) = gτ-1 (1)(x) = gk(x).
1,k
This completes the proof for the orbit Oi1 . By same arguments, we can prove the similar result for
the other orbits.
□
D Well-definedness of THE action “*”.
We here show that the left action “*" of Sn on Rn2 defined in Section 3 is well-defined, i.e., for any
σ,τ ∈ Sn and any X = (x>,..., x>)> ∈ Rn , we have T * (σ * X) = (τσ) * X.
The permutation group Sn is decomposed as
n
Sn = G Stab(1)(1 i).
i=1
For any σ ∈ Sn, because σ(1 σ-1(1)) is in Stab(1), this σ is in the coset Stab(1)(1 σ-1(1)). Apply
this for (1 i)σ for i, (1 i)σ is in the coset
Stab(1)(1 ((1 i)σ)-1(1)) = Stab(1)(1 σ-1(i)),
thus, we have a unique element σ% ∈ Stab⑴ such that
(1 i)σ = σi(1 σ-1(i)).	(4)
For σ, τ ∈ Sn and X = (x1>, . . . , xn>)> ∈ Rn2, we have
σ σ1 ∙ xσ-i(1)∖	T τLστ-i(1) ∙ xσ-i(τ-i(1))
τ * (σ * X) = τ *	. I =	.
∖σn ∙ xσ-1(n) J	∖τ?1στ-1(n) ∙ xσ-1(τ-1(n))
T τiστ-i(1) ∙ x(τσ)T(1)∖
=~~	.	I
∖τiστ-1(n) ∙x(τσ)-1(n)/
Then, by the equation (4), σi = (1 i)σ(1 σ-1(i)). Hence we have
Tidτ-i(i) = (1 i)τ (1 τ-1(i))(1 τ-1(i))σ(1 σ-1(τ-1(i)))
= (1 i)τσ(1 (τ σ)-1(i)) = (gτ σ)i.
This relation implies
/τiστT(1) ∙ x(τσ)-i(1) ∖	八gσ)l ∙ x(τσ)T(1)'
τ * (σ * X) =	.	I =	.	=(τσ) * X
~ ~ » /	；—:
∖τ1στ-1(n) ∙ x(τσ)-1(n)/	∖(τσ)n ∙ X(τσ)-1 (n)/
Thus, the action is well-defined.
To satisfy the hypothesis of Theorem 2.3, we need to check that the "*" action is free.
14
Under review as a conference paper at ICLR 2020
E	Proof of Lemma 3.1
Proof of Lemma 3.1. For any i, any σ ∈ Sn and any x ∈ Rn, we have
(l ◦ (1 i) ◦ In)(σ ∙ x) = l(((1 i)σ) ∙ x).
Because for any i, there is a unique σi ∈ Stab(I) such that (1 i)σ = *(1 σ-1(i)) as in Definition
3.1, we have
l(((1 i)σ) ∙ x) = l(bi(1 σ-1(i)) ∙ x)=力∙ l((1 σ-1 (i)) ∙ x).
The last equality is due to Stab(1)-equivariance of l. On the other hand, by Definition 3.1, i-th entry
of σ * g(x) becomes
(σ * g(x))i = σi ∙ g(x)σ-i(i)
=σi ∙ (ReLU(l((1 σ-1 (i)) ∙ x))).
Because d% ◦ ReLU = ReLU ◦ σi holds, g is Sn -equivariant.	□
F Data augmentation
Data augmentation is a common technique in empirical learning. In the case of the invari-
ant/equivariant tasks, a possible augmentation is to make new samples by the acting permutation on
samples. New samples are effective to the usual models, but not effective to our invariant/equivariant
models. This is because in our models, all weights are symmetric under permutation actions. This
means that our models learn augmented samples from a sample. By acting permutations, we can
make n! new samples from a sample. Hence, computational complexity is reduced to 1/n! times.
G Dimension reduction
In this section, we give the proof of Proposition 4.1.
Proof of Proposition 4.1. Since the action on RM, RN is a union of permutations on nodes in each
equivariant layer, we can write RM = (Rn)M0 and RN = (Rn)N0. In this case, Wi can be written as
the following form;
/	V11	V12	...	V1M 0	∖
V21	V22	. . .	V2M0
Wi =	.	..	.
.	..	.
VM01	VM02	. . .	VN0M0
, where V ij are n × n matrices. Let us consider the following maps:
Rn	-→i	(Rn)M0	-Z→i	(Rn)N0	-→p Rn
, where the first map is the inclusion to the coordinates started from the (j - 1)n th coordinate
of (Rn)M0 ended at the jn - 1 th coordinate of (Rn)M0 and the last map is the projection to the
coordinates started from the (i - 1)n th coordinate of (Rn)N0 ended at the in - 1 th coordinate of
15
Under review as a conference paper at ICLR 2020
(Rn)N0 . We chase the elements of these maps as follows;
/
0
∖
/
0
∖
/
p o Zi o i
∖
«1
«2
-an -
∖

P ◦ Zi
∖
/
P
∖
0
«1
«2
«n
0
0
/
ReLU o V1j
ReLU o Vm0j
/
P
«1
«2
ReLU o
∖
∖
∖
-an -
«1
«2
-an -
/
V11
V21
V12
V22
V1_m 0
V2M /
∖
0
«1
«2
VN 02	…VN 0M 0
=ReLU o Vij
«1
«2
-an -

«n
0
0
/
Hence, each n × n matrix Vij induces a subneural network. Since these constructions are taken
to be compatible with Sn-action, we see that f = ReLU o Vij : Rn → Rn is Sn-equivariant. If
the activation functions are bijective, we are done because of the same discussion as in the proof
of Lemma 3 in Zaheer et al. (2017). But since ReLU functions are not bijective, we need more
discussion. Let us take a transposition σ = (p q). Since f is Sn -equivariant, σ o f (x) = f (σ o x)
holds for any x. We have
«11
.
.
.
«p1
σ o f (x) = σ ReLU o .
«q1
«1n .	∖				(	«11 .	...«1n 、 .	∖		
. . «pn . . .		-X1 - X2 . .		=ReLU	σ o	. . «p1 . . .	. .. . . . . . «pn .. .. ..		-X1 - X2 . .	
«qn		. Xn				«q1	. . .	«qn		. Xn	
∖		«n1	. . .	«n	n	7	/			\	\	«n1	. . .	«nn		7	7
	f	n 乙j=1 .	«1j Xj ^	∖			f	n 乙j=1 .	«1j Xj -	∖		f	f	«11	. . . ..	«1n .
		. . ^ɔn 乙j=1	«pj Xj					. . ^ɔn 乙j=1	«qjXj					.. .. «q1	. . .	. . «qn
ReLU	σo	. . .			=	ReLU		. . .			=ReLU			.. .. ..	. . .
		^ɔn 乙j=1 .	«qj Xj					^ɔn 乙j=1 .	«pj Xj					«p1	. . . ..	«pn .
	∖	. . n L 乙 j=1	«nj Xj				∖	. . n L 乙 j=1	«nj Xj			\	\	.. .. «n1	. . .	. . «nn
∖
/
∖
X1 ^∣
X2
.
.
.
Xn」
/
16
Under review as a conference paper at ICLR 2020
On the other hand, we have
∖ f
/ ail
I a21
=ReLU .
.
.
∖ an1
/
all
a2l
Claim 1.	.
.
.
anl
alq
a2q
.
.
.
anq
a1p ♦ ♦ ♦ a1q
a2p ♦♦♦ a2q
..	..	..
.	.	.
αnp ♦♦♦ αnq
ain ∖
a2n
ann
Xl
.
.
.
Xq
.
.
.
Xp
.
.
.
Xn
Xl
/ ail
I a21
ReLU .
.
.
∖ anl
∖
α1q ♦ ♦ ♦ α1p
a2q ♦♦♦ a2p
..	..	..
.	.	.
αnq ♦♦♦ αnp
ain ∖
a2n
ann
Xq
.
.
.
Xn
alp
a2p
.
.
.
anp
aln
a2n
ann
all
.
.
.
aql
.
.
.
apl
.
.
.
anl
α1n ∖
ann
∖
I
/

∖
/
/
∖
/
Proofof Claim 1. We have
∖ /
all
.
.
.
aql
ReLU	..
.
apl
.
.
.
anl
aln
Xl
X2
.	=ReLU
.
.
Xn」
all
α2l
.
.
.
anl
alq
a2q
.
.
.
anq
alp
a2p
.
.
.
anp
aln
a2n
ann
Xl
X
q
Xn
∖
/ ∖

for any x. Hence, the the l -th coordinate of the left hand side is positive if and only if the one of the
right hand side is positive.It is clear that each equation is positive for infinitely many x. This implies
that the coefficients of each equations coincide. Hence, we have the desired result.	□
We show that app = aqq holds for any p, q. We can see that the (P q) entry of the matrix of the left
hand side in the claim is equal to app. Similarly, the (p q) entry of the matrix of the right hand side in
the claim is equal to aqq. Hence, by the claim, we have app = aqq. We show that if i = j and S = t,
aij = ast holds. Consider the (i q) entry of each matrices, the one of the left hand side is equal to
a，ip and the one of the right hand side is equal to aiq. Hence, we have aip = aiq, where i = P ane
i = q. By the symmetry, ap = aqi holds for any i = P and i = q. Hence, we have
aij = asj = ast
for any i = j and S = t. Hence, we can write Vij = λI + Y(11τ).
□
17