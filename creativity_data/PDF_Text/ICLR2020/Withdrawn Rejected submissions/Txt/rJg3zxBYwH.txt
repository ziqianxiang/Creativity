Under review as a conference paper at ICLR 2020

LEARNING  LIKELIHOODS  WITH  CONDITIONAL  NOR-
MALIZING  FLOWS

Anonymous authors

Paper under double-blind review

ABSTRACT

Normalizing Flows (NFs) are able to model complicated distributions pY (y) with
strong inter-dimensional correlations and high multimodality by transforming a
simple base density pZ(z) through an invertible neural network under the change
of variables formula.  Such behavior is desirable in multivariate structured pre-
diction tasks, where handcrafted per-pixel loss-based methods inadequately cap-
ture strong correlations between output dimensions.  We present a study of con-
ditional normalizing flows (CNFs), a class of NFs where the base density to out-
put space mapping is conditioned on an input x, to model conditional densities
pY X (y x).   CNFs  are  efficient  in  sampling  and  inference,  they  can  be  trained
with a likelihood-based objective, and CNFs, being generative flows, do not suf-
fer from mode collapse or training instabilities. We provide an effective method to
train continuous CNFs for binary problems and in particular, we apply these CNFs
to super-resolution and vessel segmentation tasks demonstrating competitive per-
formance on standard benchmark datasets in terms of likelihood and conventional
metrics.

1    INTRODUCTION

Learning conditional distributions pY X (y x) is one of the oldest problems in machine learning.
When the output y is high-dimensional this is a particularly challenging task, and the practitioner 
is
left with many design choices.  Do we factorize the conditional?  If not, do we model correlations
with, say, a conditional random field (Prince, 2012)?  Do we use a unimodal distribution?  How fat
should the tails be?  Do we use an explicit likelihood at all, or use implicit methods (Mohamed &
Rezende, 2015) such as a GAN (Goodfellow et al., 2014)? Do we quantize the output? Ideally, the
practitioner should not have to make design choices at all, and the distribution should be learned
from    the data.

In the field of density estimation normalizing flows (NFs) are a relatively new family of models
(Rezende & Mohamed, 2015).   NFs model complicated high dimensional marginal distributions
pY    (y) by transforming a simple base distribution or prior  pZ(z) through a learnable,  
invertible
mapping fφ and then applying the change of variables formula.  NFs are efficient in inference and
sampling,  are able to learn inter-dimensional correlations and multi-modality,  and they are exact
likelihood models, amenable to gradient-based optimization.

Flow-based generative models (Dinh et al., 2016) are generally trained on the image space,  and
are in some cases computationally efficient in both the forward and inverse direction.  These are
advantageous over other likelihood based methods because i) sampling is efficient opposed to au-
toregressive models (Van Oord et al., 2016), and ii) flows admit exact likelihood optimization in
contrast with variational autoencoders (Kingma & Welling, 2014).

Conditional random fields directly model correlations between pixels,  and have been fused with
deep learning (Chen et al., 2016).  However, they require the practitioner to choose which pixels
have pairwise interactions. Another approach uses adversarial training (Goodfellow et al., 2014). A
downside is that the training procedure can be unstable and they are difficult to evaluate quantita-
tively.

We propose to learn the likelihood of conditional distributions with few modeling choices using Con-
ditional Normalizing Flows (CNFs). CNFs can be harnessed for conditional distributions pY |X (y|x)

1


Under review as a conference paper at ICLR 2020

by conditioning the prior and the invertible mapping on the input x.  In particular, we apply condi-
tional flows to super-resolution (Wang et al., 2018) and vessel segmentation (Staal et al., 2004). 
We
evaluate their performance gains on multivariate prediction tasks along side architecturally-matched
factored baselines by comparing likelihood and application specific evaluation metrics.

2    BACKGROUND

In the following, we present the relevant background material on normalizing flows and structured
prediction.   This  section  covers  the  change  of  variables  formula,  invertible  modules,  
variational
dequantization and conventional likelihood optimization.

2.1    NORMALIZING FLOWS

A standard NF in continuous space is based on a simple change of variables formula.  Given two
spaces of equal dimension     and    ; a once-differentiable, parametric, bijective¹ mapping fφ :

, where φ are the parameters of f ; and a prior distribution pZ(z), we can model a complicated
distribution pY (y) as


p  (y) = p

∂fφ(y)

(f  (y))                 .

Y                Z     φ                ∂y

The term  ∂fφ(y)/∂y  is the Jacobian determinant of fφ, evaluated at y and it accounts for volume
changes induced by fφ.  The transformation fφ introduces correlations and multi-modality in pY .
The main challenge in the field of normalizing flows is designing the transformation fφ. It has to 
be

i) bijective, ii) have an efficient and tractable Jacobian determinant, iii) be from a ‘flexible’ 
model
class.  In addition, iv) for fast sampling the inverse needs to be efficiently computable.  Below we
briefly state which invertible modules are used in our architectures,  obeying the aforementioned
points.

Coupling layers    Affine coupling layers (Dinh et al., 2016) are invertible, nonlinear layers.  
They
work by splitting the input z into two components z₀ and z₁ and nonlinearly transforming z₀ as a
function of z₁, before reconcatenating the result. If z = [z₀, z₁] and y = [y₀, y₁] this is

y₀ = s(z₁) · z₀ + t(z₁)                                z₀ = (z₀ − t(y₁))/s(y₁)

y₁ = z₁                                                           z₁ = y₁

Where the scale s( ) and translation t( ) functions can be any function, typically implemented with
a CNN. Similar conditioning with normalizing flows has been done in previous works by Mohamed
& Rezende (2015) and Kingma et al. (2016b).

Invertible 1 x 1 Convolutions    Proposed in Kingma & Dhariwal (2018),  invertible 1      1 con-
volutions help mix information across channel dimensions.  We implement them as regular 1      1
convolutions and for the inverse, we convolve with the inverse of the kernel.

Squeeze layers    Squeeze layers (Dinh et al., 2016) are used to compress the spatial resolution of
activations. These also help with increasing spatial receptive field of pixels in the deeper 
activations.

Split Prior    Split priors (Dinh et al., 2016) work by spliting a set of activations z into two 
com-
ponents  z₀  and  z₁.   We  then  condition  z₁  on  z₀  using  a  simple  base  density  e.g.  
p(z₁ z₀)  =
(z₁; µ(z₀), σ²(z₀)), where µ( ) and σ²( ) are neural networks.  The component z₀ can be mod-
eled by further flow layers.   This prior,  is useful for modeling hierarchical correlations 
between

dimensions, and also helps reduce computation, since z₀ is reduced in size.

Variational dequantization    When modeling discrete data, Theis et al. (2016) introduced the con-
cept  of  dequantization.   For  this,  they  modeled  the  probability  mass  function  over  y  
as  a  latent
variable model


Pmodel(y) =

                                                                                          V

¹We can also use injective mappings

P (y|v)p(v) dv =

V

p(y, v) dv                                        (2)

2


Under review as a conference paper at ICLR 2020

where the latent variables v           are continuous-valued.  This is a convenient model to use, 
since
the marginal p(v), living on a continuous sample space, can be modelled with a continuous NF. The
distribution P (y v) is known as the quantizer and is typically an indicator function P (y v) = I[v

y + [0, 1)D].  Other works (Hoogeboom et al., 2019a; Tran et al., 2019) directly model Pmₒdₑl(y)
with a discrete-valued flow,  but these are known to be difficult to optimize.   As an extension of
dequantization, Ho et al. (2019) introduced a variational distribution q(v y), called a dequantizer,
and         write a lower bound on the data log-likelihood using Jensen’s inequality as follows

EPdata(y) log Pmₒdₑl(y) = EPdata(y) log ∫  p(y, v) dv                                               
                    (3)


= EPdata(y)

log      q(v|y) p(y, v) dv      E

q(v|y)

Pdata(y)

q(v y) log p(y, v) dv           (4)

q(v|y)

Noting that the joint p(y, v)  =  I[v       y + [0, 1)D]p(v), we see that the dequantizer 
distribution
q(v y) must be defined such that v      y + [0, 1)D, otherwise p(y, v) = 0 and the lower-bound is
undefined. Restricting q(v y) to satisfy this condition, results in the following variational 
dequanti-
zation bound


EPdata(y)

log Pmₒdₑl(y) ≥ E

Pdata(y)

q(v y) log   p(v)   dv.                       (5)

q(v|y)

2.2    STRUCTURED PREDICTION

Structured prediction tasks such as image segmentation or super-resolution, can be probabilistically
framed as learning an unknown target distribution p∗(y x), with an input x          and a target y  
       .
In practice with deep learning models, the unknown distribution is often learned by a factored 
model:

D

p(y|x) =       p(yd|x),                                               (6)

d=1

where yd represents the dth dimension of y. Several loss-based optimization methods are a special
case of this factored model. The mean squared error is equivalent to a product of normal 
distributions
with equal and fixed standard deviation. Other examples are: cross entropy, equivalent to a product
of log categorical distributions, and binary cross entropy, equivalent to a product of log Bernoulli
distributions.

With factorized independent likelihoods, individual dimensions of y are assumed to be conditionally
independent.  As a result, sampling leads to results with uncorrelated noise over the output dimen-
sions.     In the literature, a fix for this problem is to visualize the mode of the distribution 
and interpret
that as a prediction. However, because the likelihood was optimized assuming a conditionally inde-
pendent noise distribution, these modes tend to be blurry and lack crisp details.

3    METHOD

In this section we present our main innovations. i) learning conditional likelihoods using CNFs and

ii) a variational dequantization framework for binary random variables.

3.1    CONDITIONAL NORMALIZING FLOWS

We propose to learn conditional likelihoods using conditional normalizing flows for complicated
target distributions in multivariate prediction tasks.  Take an input x  ∈ X and a regression target
y   ∈ Y.  We learn a complicated distribution pY |X (y|x) using a conditional prior pZ|X (z|x) and a
mapping fφ : Y × X → Z, which is bijective in Y and Z. The likelihood of this model is:


p     (y|x) = p

(z|x)         = p

∂fφ(y, x)

(f  (y, x)|x)                     .


Y |X

Z|X

  ∂y  

Z|X     φ                            ∂y

Notice that the difference between Equations 1 and 7 is that all distributions are conditional and 
the
flow has a conditioning argument of x.

3


Under review as a conference paper at ICLR 2020

The generative process from x  to y  (shown in Figure 1) can be described by first sampling z  ∼
pZ|X (z|x) from a simple base density with its parameters conditioned on x (for us this is a 
diagonal
Gaussian) and then passing it through a sequence of bijective mappings f −¹(z; x). This allows for

modelling multimodal conditional distributions in y, which is typically uncommon.

For  the  training  procedure,  the  process  runs  in  reverse.


We  begin  with  label  y  and  conditioning  input  x.   We

Training                           Sampling


‘flow’ the label back through fφ to yield z  =  fφ(y; x),         z

and  then  we  evaluate  the  log-likelihood  of  the  parame-
ters of the prior, given this transformed label z. The flow

fφ           y     z

−1          y

φ


and prior parameters can be optimized using stochastic
gradient descent and training in minibatches in the usual

p(z|x)     x

p(z|x)     x


fashion. Note that this style of training a conditional den-
sity model pY X (y x) differs fundamentally from tradi-
tional models, because we compute the log-likelihood in
z-space and not y-space.  As a result, we are not biasing
our results with an arbitrary choice of output-space likeli-
hood or in the case of this paper, handcrafted image loss.
Instead,  one could interpret this method as learning the
correlational and multimodal structure of the likelihood
or    simply put loss-learning.

Figure 1:  Diagram of our model in the
train and sampling phases.  Solid lines
represent  deterministic  mappings  and
dashed  lines  represent  sampling.   The
conditioning  variable  enters  the  net-
work in base density p(z x) and the bi-
jective mappings f (y, x).

Conditional modules    In our work, the conditioning is introduced in the prior, the split priors, 
and
the affine coupling modules. For the prior, we set the mean and variance as functions of x. For the
split prior, we add x as a conditoning argument to the conditional p(z₁ z₀, x).  And for the affine
coupling layers, we pass x to the scale and translation networks so that

Conditional Prior             p(z|x) = N (z; µ(x), σ²(x))

Conditional Split Prior     p(z₁|z₀, x) = N (z₁; µ(z₀, x), σ²(z₀, x))

Conditional Coupling      y₀ = s(z₁, x) · z₀ + t(z₁, x);         y₁ = z₁

In practice these functions are implemented using deep neural networks. First the conditioning term
x is transformed into a rich representation h  = g(x) using a large network g.  Subsequently, each
function in the flow is applied to a concatenation [  ,   ] of h and the relevant part of z. For 
example,
the translation of a conditional coupling is computed as t(z₁, x) = NN([z₁, h]).

3.2    VARIATIONAL DEQUANTIZATION FOR BINARY RANDOM VARIABLES

We generalize the variational dequantization scheme for the binary setting.  Let y         0, 1  D 
be a
multivariate binary random variable and v      RD  its dequantized representation. In Ho et al. 
(2019)
the bound is not guaranteed to be tight, since there is a domain mismatch in the support of p(v) and
the variational dequantizer q(v y).  Technically, if p(v) is modeled as a bijective mapping from a
Gaussian distribution where the mapping only has finite volume changes, then the support of p(v) is
unbounded. On the otherhand, the support of the dequantizer is bounded and so we have to redefine
either the dequantizer to map to all of RD  or restrict the support of the flow to a bounded volume
inside RD. We resolve this by dequantizing with half-infinite noise, where

v|y, z = 0.5 + sign(y − 0.5) · softplus(NN(z)).                              (8)

The softplus guarantees that samples from the neural network NN are only positive.  If y is 1, the
term sign(y − 0.5) outputs positive-valued noise and if y is 0 the noise is negative-valued.

4    RELATED  WORK

Normalizing  flows  were  originally  introduced  to  machine  learning  to  learn  a  flexible  
variational
posterior, a conditional distribution, in VAEs (Rezende & Mohamed, 2015; Kingma et al., 2016a;
van den Berg et al., 2018).  Flow-based generative models (Dinh et al., 2016; Papamakarios et al.,
2017; Huang et al., 2018; Kingma & Dhariwal, 2018; Hoogeboom et al., 2019b; Grathwohl et al.,
2019; Cao et al., 2019; Chen et al., 2019) are typically trained directly in the data space. 
Several of
these are designed to be fast to invert, which makes them suitable for drawing samples after 
training.

4


Under review as a conference paper at ICLR 2020

Different versions and applications of conditional normalizing flows include Agrawal & Dukkipati
(2016) who utilize flows in the decoder of Variational AutoEncoders Kingma & Welling (2014),
which are conditioned on the latent variable. Trippe & Turner (2018) who utilize conditional flows
for prediction problems in a Bayesian framework for density estimation. Atanov et al. (2019) intro-
duce a semi-conditional flow that provides an efficient way to learn from unlabeled data for semi-
supervised classification problems. Very recently, Ardizzone et al. (2019) have proposed conditional
flow-based generative models for image colorization, which differs from our work in training objec-
tive, architecture and applicability to binary segmentation. Autoregressive models (Van Oord et al.,
2016) have also been studied for conditional image generation van den Oord et al. (2016) but are
generally slow to sample from.

Adversarial  methods  (Goodfellow  et  al.,  2014)  have  widely  been  applied  to  (conditional)  
image
density modeling tasks (Vu et al., 2019; Sajjadi et al., 2017b; Yuan et al., 2018; Mechrez et al.,
2018), because they tend to generate high-fidelity images. Disadvantages of adversarial methods are
that they can be complicated to train, and it is difficult to obtain likelihoods.  For this reason, 
it can
be hard to assess whether they are overfitting or generalizing.

5    EXPERIMENTS

Here we explain our experiments into super-resolution and vessel segmentation.  All models were
implemented using the PyTorch framework.

5.1    SINGLE IMAGE SUPER RESOLUTION

Single Image Super Resolution (SISR) methods aims to find a high resolution image xhr given a
single (downsampled) low resolution image xlr. Framing this problem as learning a likelihood, we
utilize a CNF to learn the distribution p(xhr xlr).  To compare our method we also train a factor-
ized baseline likelihood model with comparable architectures and parameter budget. The factorized
baseline uses a product of discretized logistic distributions (Kingma et al., 2016a; Salimans et 
al.,
2017).  All methods are compared on negative log₂-likelihood if available, which has the informa-
tion theoretic interpretation bits per dimensions. In addition, we evaluate using SSIM (Wang et al.,
2004) and PSNR metrics.

Implementation Details    The flow is based on the (Dinh et al., 2016; Kingma & Dhariwal, 2018)
multi-scale  architectures.   Each  step  of  flow  consists  of  K  subflows  and  L  levels.   
One  subflow
consists of an activation normalization, 1     1 convolution, and our conditional coupling layer. 
After
completing a level, half of the representation is factored-out and modeled using our conditional 
split
prior.  After all levels have been completed, our conditional prior is used to model the final part 
of
the latent variable.

The conditioning variable xlr is transformed into the feature representation h  using Residual-in-
Residual Dense Block (RRDB) architecture (Wang et al., 2018), consisting of 16 residual-in-residual
blocks. To match the parameter budget, the channel growth is 55 for the baseline and the growth is
32 for the CNF.

Data    The models are trained on natural image datasets, Imagenet32 and Imagenet64 (Chrabaszcz
et  al.,  2017).   Since  the  dataset  has  no  test  set,  we  use  its  validation  images  as  
a  test  set.   For
validation we take 10000 images from the train images. The performance is always reported on the
test    set unless specified otherwise. We evaluate our models on widely used benchmark datasets 
Set5
(Bevilacqua et al., 2012), Set14 (Zeyde et al., 2012) and BSD100 (Huang et al., 2015). At test time,
we      pad the test images with zeros at right and bottom so that they are square and compatible 
with
squeeze layers. When evaluating on SSIM and PSNR, we can extract the patch with the exact image
shape.  For all datasets the LR images are obtained using MATLABs bicubic kernel function with
reducing aliasing artifacts following Wang et al. (2018). For these experiments, the pixel values 
are
dequantized by adding uniform noise (Theis et al., 2016).

Training  Settings    We  train  on  ImageNet32  and  ImageNet64  for  200, 000  iterations  with  
mini
batches of size 64, and a learning rate of 0.0001 using the Adam optimizer (Kingma & Ba, 2015).

5


Under review as a conference paper at ICLR 2020

(a) Low resolution            (b) Ground truth              (b) CNF sample             (c) Baseline 
mode

Figure 2:  Super resolution results on the Imagenet64 test data.  Samples are taken from the CNF
xhr     p(xhr xlr) and the mode is visualized for the factorized baseline model.  Best viewed elec-
tronically.

The high-resolution image xhr either the original 32     32 or 64     64 original input images.  The
flow architecture is build with L = 2 levels and K = 8.

5.1.1    EVALUATION

In this section the performance of CNFs for SISR is compared against a baseline likelihood model on
ImageNet32 and ImageNet64. Their performance measured in log₂-likelihood (bits per dimension)
is shown in Table 1, which show that the CNF outperforms the factorized baseline in likelihood.
Recall that the baseline model is factorized and conditionally independent.  These results indicate
that it is advantageous to capture the correlations and multi-modality present in the data.

Table 1:  Comparison of likelihood learning with CNFs and factorized discrete baseline on Ima-
geNet32 and ImageNet64 measured in bits per dimensions.                 

Dataset              CNF     factorized LL

ImageNet32      3.01             4.00

ImageNet64      2.90             3.61

Super resolution samples xhr      p(xhr xlr) from the Imagenet64 test data are shown in Figure

2.   The distribution mode of the factorized baseline is displayed in Figure 2 panel c).   We show
that the baseline is able to learn a relationship between conditioning variable x  and y,  but lacks
crisp details.  The super-resolution images from the CNF are shown in panel b).  Notice there are
more high-frequency components modelled, for instance in grass and in hairs. Following Kingma &
Dhariwal (2018), we sample from the base distributions with a temperature τ of 0.8 to achieve the
best perceptual quality for the distribution learned by the CNF.

As there are no standard metric for measuring perceptual quality, we measure performance results on
PSNR and SSIM between our predicted image and the ground truth image in Table 2. We compare
CNFs  to  other  state-of-the-art  per-pixel  loss  based  methods  and  the  factorized  baseline  
for  a  2x
upsampling task on standard super-resolution benchmarks.  If available, we report negative log₂-
likelihood (bpd) (computed as an average over 1000 randomly cropped 128 x 128 patches).  Note
that without any hyperparameter tuning or compositional loss weighting, as is typical in SISR, the
CNF performs competitively with state-of-the-art super-resolution methods by simply optimizing
the  likelihood.   The  SSIM  scores  for  the  baseline  perform  on  par  or  better  than  the  
adversarial
methods and the CNF on all benchmarks.  On PSNR scores however, the CNF beats the factorized
discrete baseline. Samples shown in Figure 3 show that the CNF predictions have more fine grained
texture details.  Comparing this finding with the baseline that outperforms every method on SSIM,
show that metrics can be misleading.

Notice  how  samples  from  a  independent  factorized  likelihood  model  have  a  lot  of  color  
noise,
whereas samples from the CNF do not have such problems. Increasing temperature increases high-
level detail, where we find that τ = 0.5 strikes a balance between noise smoothing and detail. This
can be attributed to the property of flows to model pixel correlations among output dimensions for
high-dimensional data such as images.

6


Under review as a conference paper at ICLR 2020

Factorized Baseline

Figure 3: Conditional samples from the CNF (ours) for sampling temperatures   0., 0.5, 1.0   and the
factorized discrete baseline for 2x upscaling.  Conditioning image is a baboon from Set14 test set.
Both models were trained on ImageNet64. Best viewed electronically.

Table 2:  CNF compared to factorized discrete baseline and adversarial, pixel-wise methods (Dong
et  al.,  2015;  Sajjadi  et  al.,  2017a;  Prez-Pellitero  et  al.,  2016)  based  on  negative  
log₂-likelihood
(bits  per  dimension  or  bpd),  PSNR,  SSIM  and  for  2x  upscaling.   Our  methods  were  
trained  on
ImageNet64.


Model Type

Bicubic
SRCNN
PSyCO

ENet

LL Baseline
CNF (ours)

Set5                                 Set14                              BSD100

bpd     PSNR     SSIM     bpd     PSNR     SSIM     bpd     PSNR     SSIM

-         33.7       0.930        -         30.2       0.869        -         29.6       0.843

-         36.7       0.954        -         32.4       0.906        -         31.4       0.888

-         36.9       0.956        -         32.6       0.898        -         31.4       0.890

-         37.3       0.958        -         33.3       0.915        -         32.0       0.898

2.34      32.5       0.958     3.23      31.0       0.917     3.20      30.6       0.900

2.11      36.2       0.957     2.51      32.5       0.911     2.33      31.4       0.893

5.2    VESSEL SEGMENTATION

Vessel segmentation is an important, long-standing, medical imaging problem, where we seek to
segment blood vessels from pictures of the retina (the back of the eye).   This is a difficult task,
because the vessels are thin and of varying thickness. A likelihood function used in segmentation 
is

Figure 4:  Example of retinal segmentations using DRIU, our likelihood baseline trained with the
same loss, and our CNF. For the CNF, the mean of 100 samples is visualized.  Notice that our seg-
mentations more accurately capture the vessel width, which is overdilated in the DRIU and factored
models.

7


Under review as a conference paper at ICLR 2020

Table 3: Numerical results on the DRIVE dataset. We see that the CNF is in the range of the SOTA
model DRIU. SE: Structured Forests (Dolla´r & Zitnick, 2013), LD: Line Detector (Ricci & Perfetti,
2007),  Wavelets (Soares et al., 2006),  Human (Staal et al., 2004),  HED: Holistic Edge Detector
(Xie & Tu, 2015), KB: Kernel Boost Becker et al. (2013), N ⁴:  N ⁴  Fields (Ganin & Lempitsky,
2014), DRIU: Deep Retinal Image Understanding (Maninis et al., 2016). Our answers are shown in
mean     1std form, where statistics are taken over 5 runs.

SE         LD       Wavelets     Human     HED       KB        N ⁴       DRIU     Factored (ours)   
    CNF Uniform (Ours)     CNF (ours)

bpd               -             -               -                 -              -             -    
         -             -         0.0647 ± 0.0015     0.3366 ± 0.0290            0.0254 ± 0.0008

F-Score     0.658     0.692        0.762         0.791      0.794     0.800     0.805     0.821     
0.815  ±  0.001       0.762 ± 0.002                0.819  ±  0.001


a weighted Bernoulli distribution

L =

j

pβ·yj (1 − pj )(1−β)·(1−yj )
pβ + (1 − pj)1−β

(9)

where pj = p(yj = 1 x) is the prediction probability that pixel yj is positive (vessel class) and β 
is
a class balancing constant set to      10% for us.  This loss function is preferred, because it 
accounts
for the apparent class imbalance in the ratio of vessels to background. In practice, the numerator 
of
this likelihood is used as a loss function and the normalizer is ignored.  The resulting loss is 
called
a weighted cross-entropy.  In our experiments we train using the weighted cross-entropy (as in the
literature), but we report likelihood values including the normalizer, for a meaningful comparison.

Dataset and comparisons    We test on the DRIVE database Staal et al. (2004) consisting of 584
565, 8-bit RGB images, split into 20 train, and 20 test images.  To compare against other methods,
we plot precision-recall curves, report the maximum F-score along each curve (shown as a dot in
the graph), report the bits per dimension, and plot distributions in PR-space. The main CNN-based
contenders are Deep Retinal Image Understanding (DRIU) (Maninis et al., 2016) and Holistically-
Nested Edge Detection (HED) (Xie & Tu, 2015), both of which are instances of Deeply-Supervised
Nets (Lee et al., 2015). The main difference between DRIU and HED is that DRIU is pretrained on
ImageNet Krizhevsky et al. (2012); whereas, HED is not.  Other competing methods are reported
with results cited from (Maninis et al., 2016). For fairness, we also train a model which we call 
the
likelihood baseline, which uses the exact same architecture as the flow but run a feedforward model
and trained with the weighted Bernoulli loss.


Implementation    The  flow  is  identical  to  the  flow  used  in  the
previous section, with some key differences.  i) instead of activa-
tion normalization, we use instance normalization (Ulyanov et al.,
2016),  ii)  the  conditional  affine  coupling  layers  do  not  contain  a
scaling  component  s( )  but  just  the  translation  t( ),  hence  it  is
volume  preserving,  and  iii)  we  train  using  variational  dequanti-
zation.    Since  the  data  is  binary-valued,  we  dequantize  accord-
ing  to  the  Flow++  scheme  of  Ho  et  al.,  modified  to  binary  vari-
ables (see Section 3.2), using a CNF at just a single scale for the
dequantizer.   The CNF is conditioned on resolution-matched fea-
tures extracted from a VGG-like network Simonyan & Zisserman
(2015).   This model is composed of blocks of the form block  =
[InstanceNorm, ReLU, conv],  and  2x2  max-pooling  layer,  shown
in Table 4.  All filter sizes are 3x3.  The outputs are at layers 4 and

Table 4:  Feature extractor ar-
chitecture  for  retinal  vessel
segmentation.  RES.  abbrevi-
ates resolution. Outputs are at
layer 4 and 7.

LAYER          TYPE          RES.

0             input        1024

1             block        1024

2          max-pool      512

3             block         512

4             block         512

5          max-pool      256

6             block         256

7             block         256

7. These are used to condition the resolution 512 and 256 levels of                                 
                     

the CNF, respectively.

Training/test settings    We train using the Adam optimizer at learning rate 0.001, and a minibatch
size of 2 for 2000 epochs.  All images are padded to 1024x1024 pixels, so that they are compatible
with squeeze layers.  We use 360◦ rotation augmentation, isotropic scalings in the range [0.8, 
1.2],

and shears drawn from a normal distribution with standard deviation 10◦.   At test time we draw

samples from our model and compare those against the groundtruth labels. This contrasts with other

methods, that measure labels against thresholded versions of a factorized predictive distribution. 
To
8


Under review as a conference paper at ICLR 2020

create the PR curve in Figure 5 we take the average of 100 samples and threshold the resulting map
(example shown in Figure 4). While crude, this mean image is useful in defining a PR-curve, since
there is not great topology change between samples.

Evaluation    The results of our experiments are shown in Table 3 and Figure 5, with a visualization
in Figure 4. We see in the table that the CNF trained with our binary dequantization achieves the 
best
bits per dimension, with comparable F-score to the state of the art model (DRIU), but our model does
not require pretraining on ImageNet.  Interestingly, we found training a flow with uniform dequan-
tization slightly unstable and the results were far from satisfactory.  In the PR-curve Figure 5, we
show a comparable curve for our binary dequantized CNF to the DRIU model. These results, how-
ever, say nothing about the calibration of the probability outputs, but just that the various 
probability
predictions are well ranked. To gain an insight into the calibration of the probabilities, we 
measure
the distribution of precision and recall values for point samples drawn from all models, including
a second human grader, present in the original DRIVE dataset.  We synthesized samples from fac-
tored distributions (all except ours and ‘human’), by sampling images from a factored Bernoulli with
mean as the soft image. We see the results in the right hand plot of Figure 5, which shows that 
while
the other CNN-based methods such as DRIU or HED have good precision, they suffer in term of
recall.  On the other hand, the CNF drops in precision a little bit, but makes up for this in terms 
of
high recall, with a PR distribution overlapping the human grader.  This indicates that the CNF has
learned a well-calibrated distribution, compared to the baseline methods. Further evidence of this 
is
seen in the visualization in Figure 4, which shows details from the predicted means (soft images).
This shows that the DRIU and likelihood baseline overdilate segmentations and the CNF does not.
This can be explained from the fact that in the weighted Bernoulli it is cheaper to overdilate than 
to
underdilate. Since the CNF contains no handcrafted loss function, we circumvent this pathology.

1.0                                                                                  1.0


0.9

0.8

0.9

0.8

0.7


0.7

0.6

0.5

0.4

0.3

DRIU

Kernel_Boost
Wavelets
HED

LineDetector
N4_Fields

SE

CNF (Us)

0.6

0.5

0.4

0.3

0.2

0.1

0.0

DRIU

Kernel_Boost
Wavelets

HED

LineDetector
N4_Fields

SE

CNF (Us)

Human


0.3     0.4     0.5     0.6     0.7     0.8     0.9     1.0

Recall

(a) PR curve

0.0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1.0

Recall

(b) PR scatter plot

Figure 5:  Here we show two visualizations of the same data.  LEFT:  We show the PR-curves gen-
erated from a sweeping threshold on soft images output by each listed method.  Maximal F-scores
for each curve are shown as circles with the green lines indicating constant F-score. We see that 
our
method beats all traditional methods and is on par with DRIU, which unlike us was pretrained on
Imagenet. RIGHT: We show a scatter plot in PR-space of samples drawn from each model. To draw
samples from the all factored models, we sample images from a factored Bernoulli with a mean as
the soft image.  We see that the DRIU and HED models, while having good precision, have poor
recall in this regime. This indicates that while the output of their networks produce a good ranking
of probabilities, the values of the probabilities are poorly calibrated.  For us, we drop in 
precision
slightly,  but gain greatly in terms  of recall,  indicating  that our samples are drawn from  a 
better
calibrated distribution, overlapping significantly with the Human distribution.

9


Under review as a conference paper at ICLR 2020

6    CONCLUSION

In this paper we propose to learn likelihoods of conditional distributions using conditional normal-
izing flows. In this setting, supervised prediction tasks can be framed probabilistically. In 
addition,
we  propose  a  generalization  of  variational  dequantization  for  binary  random  variables,  
which  is
useful for binary segmentation problems.  Experimentally we show competitive performance with
competing methods in the domain of super-resolution and binary image segmentation.

REFERENCES

Siddharth Agrawal and Ambedkar Dukkipati.  Deep variational inference without pixel-wise recon-
struction. CoRR, abs/1611.05209, 2016.

Lynton  Ardizzone,  Carsten  Lu¨th,  Jakob  Kruse,  Carsten  Rother,  and  Ullrich  Ko¨the.   
Guided  im-
age generation with conditional invertible neural networks.  CoRR, abs/1907.02392, 2019.  URL
http://arxiv.org/abs/1907.02392.

Andrei Atanov, Alexandra Volokhova, Arsenii Ashukha, Ivan Sosnovik, and Dmitry Vetrov.  Semi-
conditional normalizing flows for semi-supervised learning. 05 2019.

Carlos J. Becker, Roberto Rigamonti, Vincent Lepetit, and Pascal Fua.  Supervised feature learn-
ing for curvilinear structure segmentation.  In Medical Image Computing and Computer-Assisted
Intervention - MICCAI 2013 - 16th International Conference, Nagoya, Japan, September 22-26,
2013,  Proceedings,  Part  I,  pp.  526–533,  2013.   doi:  10.1007/978-3-642-40811-3   66.   URL

https://doi.org/10.1007/978-3-642-40811-3_66.

Marco  Bevilacqua,  Aline  Roumy,  Christine  Guillemot,  and  Marie-Line  Alberi-Morel.     Low-
complexity single-image super-resolution based on nonnegative neighbor embedding.  In BMVC,
2012.

Nicola De Cao, Wilker Aziz, and Ivan Titov.  Block neural autoregressive flow.  In Proceedings of
the Thirty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI 2019, Tel Aviv, Israel,
July 22-25, 2019, pp. 511, 2019.

Liang-Chieh  Chen,  George  Papandreou,  Iasonas  Kokkinos,  Kevin  Murphy,  and  Alan  L.  Yuille.
Deeplab:  Semantic image segmentation with deep convolutional nets,  atrous convolution,  and
fully connected crfs. CoRR, abs/1606.00915, 2016. URL http://arxiv.org/abs/1606.
00915.

Ricky T. Q. Chen, Jens Behrmann, David Duvenaud, and Jo¨rn-Henrik Jacobsen. Residual flows for
invertible generative modeling. CoRR, abs/1906.02735, 2019.

Patryk Chrabaszcz, Ilya Loshchilov, and Frank Hutter.  A downsampled variant of imagenet as an
alternative to the CIFAR datasets.  CoRR, abs/1707.08819, 2017.  URL http://arxiv.org/
abs/1707.08819.

Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real NVP. CoRR,
abs/1605.08803, 2016. URL http://arxiv.org/abs/1605.08803.

Piotr Dolla´r and C. Lawrence Zitnick.   Structured forests for fast edge detection.   In IEEE 
Inter-
national Conference on Computer Vision, ICCV 2013, Sydney, Australia, December 1-8, 2013,
pp. 1841–1848, 2013.  doi:  10.1109/ICCV.2013.231.  URL https://doi.org/10.1109/
ICCV.2013.231.

Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang.  Image super-resolution using deep
convolutional  networks.   CoRR,  abs/1501.00092,  2015.   URL  http://arxiv.org/abs/
1501.00092.

Yaroslav Ganin and Victor S. Lempitsky.   N4-fields:  Neural network nearest neighbor fields for
image  transforms.   CoRR,  abs/1406.6558,  2014.   URL  http://arxiv.org/abs/1406.
6558.

10


Under review as a conference paper at ICLR 2020

Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron  Courville,  and  Yoshua  Bengio.    Generative  adversarial  nets.    In  Proceedings  of  
the
27th International Conference on Neural Information Processing Systems - Volume 2, NIPS’14,
pp.  2672–2680,  Cambridge,  MA,  USA,  2014.  MIT  Press.    URL  http://dl.acm.org/
citation.cfm?id=2969033.2969125.

Will  Grathwohl,  Ricky  T.  Q.  Chen,  Jesse  Bettencourt,  Ilya  Sutskever,  and  David  Duvenaud.
FFJORD: free-form continuous dynamics for scalable reversible generative models.   In 7th In-
ternational Conference on Learning Representations, ICLR 2019, 2019.

Jonathan Ho, Xi Chen, Aravind Srinivas, Yan Duan, and Pieter Abbeel.  Flow++:  Improving flow-
based generative models with variational dequantization and architecture design.  In Proceedings
of the 36th International Conference on Machine Learning, ICML 2019.

Jonathan Ho, Xi Chen, Aravind Srinivas, Yan Duan, and Pieter Abbeel.  Flow++:  Improving flow-
based generative models with variational dequantization and architecture design.  In Proceedings
of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long
Beach, California, USA, pp. 2722–2730, 2019. URL http://proceedings.mlr.press/
v97/ho19a.html.

Emiel Hoogeboom, Jorn W. T. Peters, Rianne van den Berg, and Max Welling. Integer discrete flows
and lossless compression.  CoRR, abs/1905.07376, 2019a.  URL http://arxiv.org/abs/
1905.07376.

Emiel Hoogeboom, Rianne van den Berg, and Max Welling.  Emerging convolutions for generative
normalizing flows.  In Proceedings of the 36th International Conference on Machine Learning,
ICML 2019, 2019b.

Chin-Wei Huang, David Krueger, Alexandre Lacoste, and Aaron C. Courville. Neural autoregressive
flows.  In Proceedings of the 35th International Conference on Machine Learning, ICML 2018,
Stockholmsma¨ssan, Stockholm, Sweden, July 10-15, 2018, pp. 2083–2092, 2018.

J. Huang, A. Singh, and N. Ahuja. Single image super-resolution from transformed self-exemplars.
In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5197–5206,
June 2015. doi: 10.1109/CVPR.2015.7299156.

Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. 2015.

Diederik P Kingma and Max Welling. Auto-Encoding Variational Bayes. In Proceedings of the 2nd
International Conference on Learning Representations, 2014.

Diederik P. Kingma, Tim Salimans, and Max Welling. Improving variational inference with inverse
autoregressive flow. CoRR, abs/1606.04934, 2016a. URL http://arxiv.org/abs/1606.
04934.

Diederik P. Kingma, Tim Salimans, and Max Welling. Improving variational inference with inverse
autoregressive flow. CoRR, abs/1606.04934, 2016b. URL http://arxiv.org/abs/1606.
04934.

Durk  P  Kingma  and  Prafulla  Dhariwal.     Glow:    Generative  flow  with  invertible  1x1  con-
volutions.     In  S.  Bengio,  H.  Wallach,  H.  Larochelle,  K.  Grauman,  N.  Cesa-Bianchi,  and

R.   Garnett   (eds.),   Advances   in   Neural   Information   Processing   Systems   31,   pp.   
10236–
10245.   Curran   Associates,   Inc.,   2018.        URL   http://papers.nips.cc/paper/
8224-glow-generative-flow-with-invertible-1x1-convolutions.pdf.

Alex   Krizhevsky,   Ilya   Sutskever,   and   Geoffrey   E.   Hinton.       Imagenet   
classification   with
deep   convolutional   neural   networks.         In   Advances   in   Neural   Information   
Process-
ing   Systems   25:      26th   Annual   Conference   on   Neural   Information   Processing   Sys-
tems  2012.  Proceedings  of  a  meeting  held  December  3-6,   2012,   Lake  Tahoe,   Nevada,
United   States.,    pp.   1106–1114,    2012.        URL   http://papers.nips.cc/paper/

4824-imagenet-classification-with-deep-convolutional-neural-networks.

11


Under review as a conference paper at ICLR 2020

Chen-Yu Lee,  Saining Xie,  Patrick W. Gallagher,  Zhengyou Zhang,  and Zhuowen  Tu.   Deeply-
supervised nets.  In Proceedings of the Eighteenth International Conference on Artificial Intelli-
gence and Statistics, AISTATS 2015, San Diego, California, USA, May 9-12, 2015, 2015.  URL
http://proceedings.mlr.press/v38/lee15a.html.

Kevis-Kokitsi Maninis, Jordi Pont-Tuset, Pablo Andre´s Arbela´ez, and Luc Van Gool.   Deep reti-
nal  image  understanding.   In  Medical  Image  Computing  and  Computer-Assisted  Intervention

-  MICCAI  2016  -  19th  International  Conference,  Athens,  Greece,  October  17-21,  2016,  Pro-
ceedings,  Part II, pp. 140–148,  2016.   doi:  10.1007/978-3-319-46723-8   17.   URL https:

//doi.org/10.1007/978-3-319-46723-8_17.

Roey Mechrez, Itamar Talmi, Firas Shama, and Lihi Zelnik-Manor. Learning to maintain natural im-
age statistics. CoRR, abs/1803.04626, 2018. URL http://arxiv.org/abs/1803.04626.

Shakir Mohamed and Danilo J. Rezende. Variational information maximisation for intrinsically mo-
tivated reinforcement learning. In Proceedings of the 28th International Conference on Neural In-
formation Processing Systems - Volume 2, NIPS’15, pp. 2125–2133, Cambridge, MA, USA, 2015.
MIT Press. URL http://dl.acm.org/citation.cfm?id=2969442.2969477.

George  Papamakarios,  Theo  Pavlakou,  and  Iain  Murray.    Masked  autoregressive  flow  for  
den-
sity  estimation.    In  I.  Guyon,  U.  V.  Luxburg,  S.  Bengio,  H.  Wallach,  R.  Fergus,  S.  
Vish-
wanathan,  and R. Garnett (eds.),  Advances in Neural Information Processing Systems 30,  pp.
2338–2347.  Curran  Associates,  Inc.,  2017.     URL  http://papers.nips.cc/paper/
6828-masked-autoregressive-flow-for-density-estimation.pdf.

Simon JD Prince.  Computer vision: models, learning, and inference.  Cambridge University Press,
2012.

Eduardo Prez-Pellitero, Jordi Salvador, Javier Ruiz-Hidalgo, and Bodo Rosenhahn.  Manifold span
reduction for super resolution. In IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)., June 2016.

Danilo  Jimenez  Rezende  and  Shakir  Mohamed.    Variational  inference  with  normalizing  flows.
In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille,
France, 6-11 July 2015, pp. 1530–1538, 2015.  URL http://proceedings.mlr.press/
v37/rezende15.html.

Elisa Ricci and Renzo Perfetti.  Retinal blood vessel segmentation using line operators and support
vector classification.  IEEE Trans. Med. Imaging, 26(10):1357–1365, 2007.  doi:  10.1109/TMI.
2007.898551. URL https://doi.org/10.1109/TMI.2007.898551.

M.   S.   M.   Sajjadi,   B.   Scho¨lkopf,   and   M.   Hirsch.       EnhanceNet:     Single   
image   super-
resolution  through  automated  texture  synthesis.     In  Proceedings  IEEE  International  Con-
ference  on  Computer  Vision  (ICCV),  pp.  4501–4510,  Piscataway,  NJ,  USA,  October  2017a.
IEEE.     URL  http://openaccess.thecvf.com/content_ICCV_2017/papers/
Sajjadi_EnhanceNet_Single_Image_ICCV_2017_paper.pdf.

M.   S.   M.   Sajjadi,   B.   Scho¨lkopf,   and   M.   Hirsch.       EnhanceNet:     Single   
image   super-
resolution  through  automated  texture  synthesis.    In  Proceedings  IEEE  International  Confer-
ence   on  Computer  Vision  (ICCV),  pp.  4501–4510,  Piscataway,  NJ,  USA,  October  2017b.
IEEE.     URL  http://openaccess.thecvf.com/content_ICCV_2017/papers/
Sajjadi_EnhanceNet_Single_Image_ICCV_2017_paper.pdf.

Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P. Kingma.   Pixelcnn++:  Improving the
pixelcnn with discretized logistic mixture likelihood and other modifications. In 5th International
Conference on Learning Representations, ICLR 2017, 2017.

Karen Simonyan and Andrew Zisserman.   Very deep convolutional networks for large-scale im-
age  recognition.    In  3rd  International  Conference  on  Learning  Representations,  ICLR  2015,
San  Diego,  CA,  USA,  May  7-9,  2015,  Conference  Track  Proceedings,  2015.    URL  http:

//arxiv.org/abs/1409.1556.

12


Under review as a conference paper at ICLR 2020

Joa˜o V. B. Soares, Jorge J. G. Leandro, Roberto M. Cesar, Herbert F. Jelinek, and Michael J. Cree.
Retinal  vessel  segmentation  using  the  2-d  gabor  wavelet  and  supervised  classification.   
IEEE
Trans. Med. Imaging, 25(9):1214–1222, 2006.  doi:  10.1109/TMI.2006.879967.  URL https:

//doi.org/10.1109/TMI.2006.879967.

Joes Staal, Michael D. Abra`moff, Meindert Niemeijer, Max A. Viergever, and Bram van Ginneken.
Ridge-based vessel segmentation in color images of the retina.  IEEE Trans. Med. Imaging, 23
(4):501–509, 2004.  doi:  10.1109/TMI.2004.825627.  URL https://doi.org/10.1109/
TMI.2004.825627.

Lucas Theis, Aa¨ron van den Oord, and Matthias Bethge.   A note on the evaluation of generative
models.   In 4th International Conference on Learning Representations,  ICLR 2016,  San Juan,
Puerto Rico,  May 2-4,  2016,  Conference Track Proceedings,  2016.   URL http://arxiv.
org/abs/1511.01844.

Dustin Tran, Keyon Vafa, Kumar Krishna Agrawal, Laurent Dinh, and Ben Poole.  Discrete flows:
Invertible generative models of discrete data.  In Deep Generative Models for Highly Structured
Data, ICLR 2019 Workshop, New Orleans, Louisiana, United States, May 6, 2019, 2019.  URL
https://openreview.net/forum?id=rJlo4UIt_E.

Brian L Trippe and Richard E Turner.   Conditional density estimation with bayesian normalising
flows. 2018. URL https://arxiv.org/abs/1802.04908.

Dmitry Ulyanov, Andrea Vedaldi, and Victor S. Lempitsky.  Instance normalization:  The missing
ingredient  for  fast  stylization.   CoRR,  abs/1607.08022,  2016.   URL  http://arxiv.org/
abs/1607.08022.

Rianne van den Berg, Leonard Hasenclever, Jakub M. Tomczak, and Max Welling.  Sylvester nor-
malizing flows for variational inference.  In Proceedings of the Thirty-Fourth Conference on Un-
certainty in Artificial Intelligence, UAI 2018, Monterey, California, USA, August 6-10, 2018, pp.
393–402, 2018.

Aa¨ron van den Oord, Nal Kalchbrenner, Lasse Espeholt, Koray Kavukcuoglu, Oriol Vinyals, and
Alex Graves.  Conditional image generation with pixelcnn decoders.  In Advances in Neural In-
formation Processing Systems 29: Annual Conference on Neural Information Processing Systems
2016, pp. 4790–4798, 2016.

Aaron Van Oord, Nal Kalchbrenner, and Koray Kavukcuoglu.  Pixel recurrent neural networks.  In

International Conference on Machine Learning, pp. 1747–1756, 2016.

Thang Vu, Tung Luu, and Chang Yoo. Perception-Enhanced Image Super-Resolution via Relativistic
Generative Adversarial Networks:  Munich, Germany, September 8-14, 2018, Proceedings, Part
V,       pp. 98–113. 01 2019. ISBN 978-3-030-11020-8. doi: 10.1007/978-3-030-11021-5 7.

Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Chen Change Loy, Yu Qiao,
and Xiaoou Tang.  ESRGAN: enhanced super-resolution generative adversarial networks.  CoRR,
abs/1809.00219, 2018. URL http://arxiv.org/abs/1809.00219.

Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli.   Image quality assessment:  From
error visibility to structural similarity.  Image Processing, IEEE Transactions on, 13:600 – 612,
05 2004. doi: 10.1109/TIP.2003.819861.

Saining Xie and Zhuowen Tu. Holistically-nested edge detection. In 2015 IEEE International Con-
ference on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015, pp. 1395–1403,
2015.   doi:  10.1109/ICCV.2015.164.   URL  https://doi.org/10.1109/ICCV.2015.

164.

Yuan Yuan, Siyuan Liu, Jiawei Zhang, Yongbing Zhang, Chao Dong, and Liang Lin. Unsupervised
image super-resolution using cycle-in-cycle generative adversarial networks.   2018 IEEE/CVF
Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pp. 814–81409,
2018.

13


Under review as a conference paper at ICLR 2020

Roman  Zeyde,  Michael  Elad,  and  Matan  Protter.     On  single  image  scale-up  using  sparse-
representations.  In Proceedings of the 7th International Conference on Curves and Surfaces, pp.
711–730, Berlin, Heidelberg, 2012. Springer-Verlag.  ISBN 978-3-642-27412-1.  doi:  10.1007/

978-3-642-27413-8 47.     URL  https://doi.org/10.1007/978-3-642-27413-8_

47.

14


Under review as a conference paper at ICLR 2020

A    ARCHITECTURES

This section describes architecture and optimization details of the conditional normalizing flow 
net-
work, low-resolution image feature extractor, and shallow convolutional neural network in the con-
ditional coupling layers.

The conditional coupling layer is shown schematically in Figure 6.  This shows that conditioned
on an input x, we are able to build a relatively straight-forward invertible mapping between latent
representations z and y, which have been partitioned into vectors of equal dimension.


x

z0                                                                                                  
           y0

x

z0                                                                                                  
           y0


z1                                                                                                  
           y1

(a) Forward

z1                                                                                                  
           y1

(b) Reverse

Figure  6:  The  forward  and  reverse  paths  of  the  conditional  coupling  layer.   In  our  
experiments
we concatenate an embedding of the conditioning input x  to the latent z₁,  which is fed through
another neural network to output the affine transformation parameters applied to z₀. This operation
is invertible in z and y, but not in x.

Details for the CNFs are given in Table 5 and the details of the individual coupling layers in Table
6 and 7.  The architecture of the feature extractor is given in Table 8.  The architecture has 
levels
and subflows, following (Dinh et al., 2016; Kingma & Dhariwal, 2018). All networks are optimized
using Adam (Kingma & Ba, 2015) for 200000 iterations.

Table 5: Configuration of the CNF architecture for the super-resolution task.
DATASET         MINIBATCH SIZE      LEVELS      SUB-FLOWS      LEARNING RATE

ImageNet32                 64                       2                    8                      
0.0001

ImageNet64                 64                       2                    8                      
0.0001

DRIVE                      2                        2                    2                       
0.001

15


Under review as a conference paper at ICLR 2020

Table 6:  Architecture details for a single coupling layer in the super resolution task.  The 
variable
cₒut  denotes the number of output channels.  The first two convolutional layers are followed by a
ReLU activation.                                                                                    
                  

LAYER       INTERMEDIATE CHANNELS       KERNEL SIZE

Conv2d                         512                                3 × 3

Conv2d                         512                                1 × 1

Conv2d                         cₒut                                 3 × 3         

Table  7:  Architecture  details  for  a  single  coupling  layer  in  the  DRIVE  segmentation  
task.   The
variable cₒut denotes the number of output channels for the . The first two convolutional layers are
followed by a ReLU activation.                                                                      
               

LAYER               INTERMEDIATE CHANNELS       KERNEL SIZE

Conv2d                                 32                                 3     3

InstanceNorm2d                          32                                     -

ReLU                                    -                                      -

         Conv2d                                cₒut                                 3 × 3         

Table 8:  Architecture details for the conditioning network in the super-resolution task.  Residual-
in-residual denseblocks Wang et al. (2018) are utilized.  The channel growth is adjusted so that the
CNF and the factorized baseline have an equal number of parameters.                                 
     

MODEL TYPE      RRDB BLOCKS      CHANNEL GROWTH      CONTEXT CHANNELS

CNF                        16                               32                                   
128

Factorized LL                16                               55                                   
128

16


Under review as a conference paper at ICLR 2020

B    CONDITIONAL  IMAGE  GENERATION

In this section, larger versions of the ImageNet64 samples are provided, sampled at different tem-
peratures τ .

Figure 7: Super resolution results CNF trained on Imagenet64 sampled at temperature τ = 0.

17


Under review as a conference paper at ICLR 2020

Figure 8: Super resolution results for the CNF trained on Imagenet64 sampled at τ = 0.5

Figure 9: Super resolution results for the CNF trained on Imagenet64 sampled at τ = 0.8

18

