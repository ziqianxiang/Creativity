Under review as a conference paper at ICLR 2020
Graph Neural Networks for Soft Semi-
Supervised Learning on Hypergraphs
Anonymous authors
Paper under double-blind review
Ab stract
Graph-based semi-supervised learning (SSL) assigns labels to initially unlabelled
vertices in a graph. Graph neural networks (GNNs), esp. graph convolutional
networks (GCNs), inspired the current-state-of-the art models for graph-based
SSL problems. GCNs inherently assume that the labels of interest are numerical
or categorical variables. However, in many real-world applications such as co-
authorship networks, recommendation networks, etc., vertex labels can be natu-
rally represented by probability distributions or histograms. Moreover, real-world
network datasets have complex relationships going beyond pairwise associations.
These relationships can be modelled naturally and flexibly by hypergraphs. In
this paper, we explore GNNs for graph-based SSL of histograms. Motivated by
complex relationships (those going beyond pairwise) in real-world networks, we
propose a novel method for directed hypergraphs. Our work builds upon exist-
ing works on graph-based SSL of histograms derived from the theory of optimal
transportation. A key contribution of this paper is to establish generalisation er-
ror bounds for a one-layer GNN within the framework of algorithmic stability.
We also demonstrate our proposed methods’ effectiveness through detailed exper-
imentation on real-world data. We have made the code available.
1	Introduction
In the last decade, deep learning models have been successfully embraced in many different fields
and proved to achieve unprecedented performance on a vast range of applications Krizhevsky et al.
(2012); Goodfellow et al. (2014); Bahdanau et al. (2015); LeCun et al. (2015). Graph Convolutional
Network (GCN) Kipf & Welling (2017) was recently proposed as an adaptation of a particular
deep learning model (i.e., convolutional neural networks Lecun et al. (1998)) to enable handling
graph-structured data. GCN was shown to be, in particular, effective in semi-supervised learning on
attributed graphs. GCNs have inspired the current state-of-the art models for graph-based SSL Wu
et al. (2019a); Velickovic et al. (2θ18); Vashishth et al. (2019). GCNs inherently assume that the
labels of interest are numerical or categorical variables.
However, in many real-world applications such as co-authorship networks, recommendation net-
works, etc., vertex labels can be naturally represented by probability distributions or histograms.
Moreover, these real-world network datasets have complex relationships going beyond pairwise as-
sociations. Such relationships can be modelled naturally and flexibly by hypergraphs. Moreover,
hypergraphs can encode additional relationships with directions as illustrated in Figure 1 and these
hypergraphs are directed hypergraphs Gallo et al. (1993).
Inspired by a prior work Solomon et al. (2014) that generalised label propagation to graph-based soft
SSL setting and motivated by the fact that GNNs have inspired state-of-the-art models for traditional
graph-based SSL, we make the following contributions.
•	We explore GNNs for soft SSL in which vertex labels are probability distributions. Mo-
tivated by real-world applications, we propose DHN (Directed Hypergraph Network), a
novel method for directed hypergraphs. DHN can be applied for soft-SSL using existing
tools from optimal transportation (Section 3).
•	We provide generalisation error bounds fora one-layer GNN within the framework of algo-
rithmic stability. We establish that such models, which use filters with bounded eigenvalues
1
Under review as a conference paper at ICLR 2020
co-authorship network
recommendation network
Figure 1: (Best seen in colour) Examples of real-world networks modelled as directed hypergraphs
in which labels on the vertices can be represented by probability distributions. To the left is a co-
authorship network in which vertices are authors, hyperedges are collaborations (documents), and
directions are citations between documents. Research topic interests of authors can be naturally
represented by probability distributions (labels are shown for a couple of authors in the figure). To
the right is a recommendation network in which vertices are products, hyperedges are users and
contain all the products bought by them, and directions represent user similarity (two-way). Product
ratings can be naturally represented by probability distributions (labels are shown for a couple of
products in the figure). Please see Section 1 for more details.
independent of graph size, can satisfy the strong notion of uniform stability and thus is gen-
eralisable. In particular, the algorithmic stability of a one-layer GNN depends on the largest
absolute eigenvalue of the graph convolution filter (Section 4).
•	We demonstrate DHN’s effectiveness through detailed experimentation on real-world data.
In particular, we demonstrate superiority over state-of-the-art hypergraph-based neural net-
works. We provide new empirical benchmarks for soft-SSL on directed hypergraphs and
make the code available to foster reproducible research (Section 5).
2	Related work
Graph-based deep learning: Geometric deep learning Bronstein et al. (2017) is an umbrella
phrase for emerging techniques attempting to generalise (structured) deep neural networks to non-
Euclidean domains such as graphs and manifolds. GCN Kipf & Welling (2017) and their various
extensions are the current State-of-the art for graph-based SSL WU et al. (2019a); VeIickoVic et al.
(2018); Monti et al. (2018); Vashishth et al. (2019); Ma et al. (2019); Qu et al. (2019) and graph-
based unsupervised learning Hamilton et al.(2017a); Velickovic et al. (2019) problems. GCNs have
also been applied for semi-superVised graph classification Li et al. (2019).
The reader is referred to a comprehensive literature review Bronstein et al. (2017) and extensive
surveys Hamilton et al. (2017b); Battaglia et al. (2018); Zhang et al. (2018); Wu et al. (2019b); Sun
et al. (2018); Zhou et al. (2018) on this topic. Recently, graph-based deep models (also message-
passing neural networks Gilmer et al. (2017)) have been analysed theoretically Dehmamy et al.
(2019); Ying et al. (2019); Knyazev et al. (2019); Chen et al. (2019b); Maron et al. (2019); Morris
et al. (2019); Xu et al. (2019b); Chen et al. (2019a); Kawamoto (2018); Xu et al. (2018); Chen et al.
(2018). We note a work on stability and generalisation bounds of GCNs Verma & Zhang (2019).
Learning on hypergraphs: Hypergraph is a combinatorial structure consisting of vertices and
hyperedges, where each hyperedge is allowed to connect any number of vertices, thus generalizing
graphs. This additional flexibility facilitates the capture of higher order interactions among objects;
applications have been found in many fields such as computer vision Govindu (2005), network
clustering Demir et al. (2008), folksonomies Ghoshal et al. (2009), cellular networks Klamt et al.
(2009), and community detection Chien et al. (2018).
The seminal work on hypergraphs Zhou et al. (2007) introduced the popular Agarwal et al. (2006);
Feng et al. (2018; 2019) clique expansion of a hypergraph. Hypergraph neural networks (HGNN)
Feng et al. (2019) use the clique expansion while HyperGCN Yadati et al. (2019) uses the mediator-
2
Under review as a conference paper at ICLR 2020
based Laplacian to extend GCNs to hypergraphs. Another line of work uses the mathematically
appealing tensor methods ShashUa et al. (2006); Bulo & Pelillo (2009); Kolda & Bader (2009) but
they are limited to uniform hypergraphs. Recent developments work for arbitrary hypergraphs and
fully exploit the hypergraph structure Hein et al. (2013); Zhang et al. (2017); Chan & Liang (2018);
Wendler et al. (2019). These developments are motivated from the spectral theory of hypergraphs
which is an active area of research Li & Milenkovic (2017); Chien et al. (2019); Chitra & Raphael
(2019); Li & Milenkovic (2018b); Li et al. (2018a); Li & Milenkovic (2018a)
Graph-based soft SSL: Researchers have shown that using unlabelled data during training can
improve label prediction significantly Chapelle et al. (2010); Zhu et al. (2009); Subramanya & Taluk-
dar (2014); Yang et al. (2016). While most methods assume that labels of interest are numerical or
categorical variables, other works “soften” this assumption and handle “soft labels” such as his-
tograms Corduneanu & Jaakkola (2005); Tsuda (2005). One way of propagating histograms is to
minimise the Kullback-Leibler (KL) divergence Subramanya & Bilmes (2011). Recent studies have
replaced the metric-agnostic KL divergence with metric-aware Wasserstein distance (interactions
between histogram bins) for graphs Solomon et al. (2014) and hypergraphs Gao et al. (2019).
Embeddings in Wasserstein space: There exist at least a couple of recent works that embed
Gaussian distributions in the Wasserstein space Muzellec & Cuturi (2018); Zhu et al. (2018). In-
spired by a recent work Frogner et al. (2019), in this work, we focus on embedding input data as a
discrete probability distrirbution on a fixed support set. The Wasserstein distance and its gradient
require the solution of a linear program Villani (2008) and are costly to compute Peyre & Cuturi
(2019). A popular efficient approximation is the Sinkhorn divergence Cuturi (2013) in which the
underlying problem is regularised and is computed efficiently by a fixed-point iteration. Recent
works have shown that it is suitable for gradient-based optimisation through automatic differentia-
tion Frogner et al. (2015); Genevay et al. (2018); Frogner et al. (2019). A couple of recent works
compute Wasserstein distance between graph pairs Xu et al. (2019a); Vayer et al. (2019).
3	Method
In this section, we first describe soft SSL on directed hypergraphs and then propose DHN (Directed
Hypergraph Network) for the problem.
3.1	Directed hypergraph
A directed hypergraph Gallo et al. (1993) is an ordered pair H = (V, Ed) where V = {vι,…，vn}
is a set of n vertices and
Ed = {(tl,hl), ∙∙∙ , (tm,hm)} ⊆ 2V × 2V
is a set of m directed hyperedges. Each element in Ed is an ordered pair (t, h) where t ⊆ V is the
tail and h ⊆ V is the head with t = 0, h = 0. Denote the set of all undirected hyperedges by E.
E =	[ t ∪ h.
(t,h)∈Ed
Denote I ∈ {0,1}|Vl×lEl to be the incidence matrix of E i.e. I(v,e) = 1 if V ∈ e and 0 otherwise.
3.2	Soft SSL on directed hypergraphs
We consider the problem of predicting probability distributions for the vertices in H = (V, Ed) given
a typically small subset Vk ⊆ V of vertices with known distributions. In this work, we are concerned
with discrete distributions modelled on a metric space i.e. an ordered pair (M, C) in which M is
a set and C is the cost function (metric) associated with the set. Furthermore, we assume that we
are provided with a feature matrix, XV ∈ Rn×DV , in which each vertex v ∈ V is represented by a
DV -dimensional feature vector xv (here n = |V |). We are also provided with a hyperedge feature
matrix XE ∈ Rm×DE with xe, e ∈ E as DE -dimensional feature representations (here m = |Ed|).
Our objective is to learn a labelling function Z = h H, XV , XE that maps each vertex to a proba-
bility distribution in the space of discrete probability distributions PF(M) on F atoms (F is number
of histogram bins) defined on the metric space (M, C). The cost function C can be represented by
a non-negative symmetric matrix of size F × F. Note that each row of Z ∈ [0, 1]n×F maps each
vertex v ∈ V to a probability distribution Zv ∈ [0, 1]F .
3
Under review as a conference paper at ICLR 2020
The function h is going to be trained on a supervised loss, L ,w.r.t to the vertices in Vk so that the
trained h can be used to predict distributions of all the vertices in V \ Vk. We now give an example
application and then the details of the labelling function h followed by the supervised loss L.
Example application: Predicting topic distributions of authors in co-authorship networks can be
posed as a soft SSL problem on directed hypergraphs. V represents the set of authors, E the set of
all collaborations (documents), Ed the citation relationships among the documents, F the number of
possible research interests of authors (Machine Learning, Theoretical Computer Science, etc.), XV
and XE any available features on the authors and documents respectively (e.g. text attributes).
3.3	DHN (Directed Hypergraph Network)
Hypergraphs contain hyperedges in which relationships can go beyond pairwise and hence are chal-
lenging to deal with. A flexible way to embed vertices of a hypergraph is to “approximate” the
hypergraph by a suitable graph and then apply traditional graph-based methods on the vertices. Two
notable candidates of h are Hypergraph neural network (HGNN) Feng et al. (2019) and Hypergraph
Convolutional Network (HyperGCN) Yadati et al. (2019). HGNN uses the clique expansion of the
hypergraph Zhou et al. (2007) while HyperGCN uses the mediator-based Laplacian Chan & Liang
(2018) to approximate the input hypergraph. However, they are restricted to undirected hyperedges
and also cannot exploit the hyperedge feature matrix XE .
A key idea of our approach is to treat each hyperedge e ∈ E as a vertex of the graph G = (E, Ed).
We then pass G through a graph neural network to obtain HE = fGNN (G, XE) so that the initial
features, XE, are refined to HE. We then propose the layer-wise propagation rule of DHN as:
Ht+1)= σ([HP I∙ HEt) ∙ Θ ㈤[J t = 0,…，τ- 1	(1)
where [∙, ∙] denotes concatenation, t is the time step, I is the incidence matrix, HEt+1) =
σι(ITHVt)) for t = 1,…，τ - 1, HE = ∕gnn(G,XE), σ and σ1 are non-linear activation
functions, and τ is the total number of time steps with HV(0) = XV . Note that the labelling function
Z = h(H, XV , XE ) = softmax HVτ where softmax is applied row-wise.
3.4	THE SUPERVISED LOSS L
A crucial observation here is that because of the softmax layer, the output ofh is (already) inherently
a probability distribution. For each vertex v ∈ Vk, the predicted distribution Zv and the (known) true
distribution Yv must be “close” to each other. A natural way to compare probability distributions is
to use the KL-divergence between Yv and Zv . However, KL-divergence cannot exploit the metric
space (M, C) and suffers from stability issues Chen et al. (2016). In this work, we use the more
stable Wasserstein distance to exploit the metric space Gao et al. (2019).
L = X Wp(Zv,YV)	where Wp(μ,ν)= ( einf J	C(xι, x2)pdπ(x1 ,x2)).
(2)
For discrete distributions, Wp is the solution of a linear program. For practical purposes, we compute
the regularised distance using the Sinkhorn algorithm. Please see Appendix A.1 for more details.
Optimisation: We call DHN optimised with the Wasserstein loss as Soft-DHN. All parameters
are learned using stochastic gradient descent (SGD). Please see Appendix for time complexity.
4	Theoretical Analysis: Generalisation error bound
In this section, we establish generalisation error bounds for a one-layer GNN by extending the results
of a traditional GCN Verma & Zhang (2019) to the soft SSL setting with Wasserstein loss. The main
novelty is to generalise the error bounds to the learning problem “valued in the Wasserstein space”.
The main challenge is that the Wasserstein space is an abstract metric space without linear structure.
The section is organised as follows. We first introduce all the notations needed (ego-graph view,
semi-supervised learning setting, etc.). We then give single layer and SGD bounds using the nota-
4
Under review as a conference paper at ICLR 2020
tions. We finally give the main result (Theorem 1) which states that a GNN trained with Wasserstein
loss has the same generalisation error bound as the traditional GCN (trained with cross entropy).
Let G = (V, E) be a connected graph with |V | = n vertices. We consider GNN of a single layer
f (X, Θ) = σ (KXΘ)	(3)
where X ∈ Rn×d is the feature matrix (n is the number of vertices in a graph, d is the dimension
of the feature vectors), K = g (LG) is a graph filter (typically symmetrically normalised adjacency
with self loops, and LG ∈ Rn×n is the graph Laplacian), and Θ ∈ Rd×F is the set of parameters.
We note that our proposed DHN falls under this formulation in special circumstances. Specifically if
the non-linearity σ1 in Equation 1 is removed we get the kernel K = IIT (also known as the clique
expansion of the hypergraph Zhou et al. (2007).) The non-linearity σ in Equation 3 is the softmax
function acting on each row of the product g (LG) XΘ ∈ Rn×F; the output is of dimension n × F,
where each output row is a discrete probability distribution, i.e.,
f(X,Θ) ≥0 and f(X,Θ)1F =1n
where 1F = (1, . . . , 1)F ∈ RF, and similarly for 1n. Without loss of generality, we assume d = 1.
Note that in order for the output to be nontrivial probability distributions, we must assume F > 1.
We adopt an ego-graph view (Verma & Zhang, 2019, formula (2)) to simplify our discussion for
local behavior of the soft GCN at a particular vertex. Whenever no confusion arises, we identify a
vertices x and χ in the graph G with their respective D-dimensional feature vectors. Thus the output
of f at x ∈ V is
f (X, ⑼=σ( X KxχXθ)=σ(( X Kxχx) ∙ θ)
χ∈N (x)	χ∈N (x)
where N (x) denotes for the one-hop neighborhood of x with respect to the adjacency relation
defined by matrix K, and Kxχ ∈ R stands for the entry in K ∈ Rn×n that describes the adjacency
relation between vertices X and χ. Let Ex := Pχ∈N(x) KxXX ∈ R So that f (x, θ) = σ (Ex ∙ θ).
We consider the supervised learning setting, and learn GNN from the training set
{zi = (Xi, yi) , i = 1, . . . , m} sampled i.i.d. from the product space V × PF with respect to prob-
ability distribution D on this product space, where PF is the space of discrete probability distribu-
tions on F atoms. The output of softmax lies in PF, which is a convex cone. For any new data
Z = (x, y)〜 D, we evaluate the performance of GNN f using a Wasserstein cost
'(f (∙, θ), Z) = '(f (∙, θ), (x, y)) = W(f (x, θ) ,y).
Here the Wasserstein cost is defined with respect to a cost function penalizing moving masses across
bins. Since we are working only with histograms in GNN, we shall use a cost function C ∈ RF ×F
that is defined for pairs of histogram bins. The transport problem is a linear program with Z =
f(x, θ) as in Equation 10. Please see Appendix Sections A.3.1 and A.3.3 for the assumptions made
and the notations used for Algorithmic stability. We now derive single layer and SGD bounds.
4.0. 1 Single Layer Bound
By the triangle inequality,
|W1 (f (x, θS) , y) - W1 (f (x, θS0) , y)| ≤ W1 (f (x, θS) , f (x, θS0)) .
By (Villani, 2008, Theorem 6.13), recall that the diameter of the support is D, we have
W1(f(x,θS),f(x,θS0)) ≤Dkf(x,θS)-f(x,θS0)kTV
where ∣∣∙∣∣tv is the total variation distance, which by definition is
1F	1
kf (x, θS) - f (x, θSO)kTV = 2 £|[f (x, θS )]i - [f (x, θS0儿1 = 2 kσ (Ex ∙ θS) - σ (Ex ∙ θS0 )k1
i=1
where ∣∣∙kι is the L1-distance on RF. Since the softmax function is Lipschitz continuous, We have
kf (x, θs) - f (x, θso)kτv ≤ L ∣Ex∣∙kθs - θsokι
5
Under review as a conference paper at ICLR 2020
and thus
∣EA[Wι(f (χ,θs ),y)-Wι(f(χ,Θso ),y)]∣≤ L⅞D supχ∈v ∣Ex∣∙Ea∣∣Θs-Θ,o 卜=L⅞D gλEA∣∣Θs 3, kι	(4)
where we used notation gλ := supx∈V |Ex| as defined in Verma & Zhang (2019), which is known
to be upper bounded by λGmax, the spectrum of the graph Laplacian LG with largest absolute value.
4.0.2 SGD BOUND
It now remains to bound EA kΘS - ΘS0 k1 resulting from the SGD iterations. The main technical
challenge, as noted before, is to generalise the results to the Wasserstein space which is an abstract
metric space without linear structure. Specifically, we have to modify the “gradient” in the Wasser-
stein space as the straightforward version Verma & Zhang (2019) does not satisfy the Lipschitz
condition required in the algorithmic stability framework. To the best of our knowledge this mod-
ification is not seen in existing literature and can be thought of as a generalisation of the “gradient
clipping” operation Hardt et al. (2016). The entire proof is in the Appendix A.3.4. We state the main
result here.
EA [kθs,τ - Θso,tkι] ≤ 2ηFgλD XX(1 + 2ηDLσg2)	.	(5)
m	t=1
Combining equation 4 and equation 23 gives us
|Ea [Wi (f(x, Θs) ,y) - Wi (f(x, Θs,) ,y)]∣ ≤ ηFLσg2D X(1 + 3ηDLσ点['∙	(6)
m2
t=1
Therefore, we actually have the uniform algorithmic stability equation 14 holds with
βm = η¾λD2 X (1 + 2ηDLσgλ)tτ.	⑺
2m	t=1	2
Note that here βm = O(春)(needed to obtain a tight generalisation bound).
4.1 Putting Everything Together
Lemma 1 (Verma & Zhang, 2019, Lemma 4): Let λGmax be the maximum absolute eigenvalue of
LG. Let Gx be the ego-graph of a vertex x ∈ V with corresponding maximum absolute eigenvalue
λGmax. Then the following eigenvalue (singular value) bound holds ∀x ∈ V ,
λGmax ≤ λGmax	(8)
Lemma 2 (Verma & Zhang, 2019, Theorem 2) A uniform stable randomised algorithm (AS, βm)
with a bounded loss function 0 ≤ '(As, y) ≤ B, satisfies the following generalisation bound with
probability at least 1 - δ, over the random draw ofS, y with δ ∈ (0, 1),
EA hR(AS) - Remp (AS)i
≤ 2βm + (4mβm + B )
(9)
where R(AS) is the generalisation error/risk and Remp(AS) is the empirical error. Please see Ap-
pendix A.3.2 for definitions. Finally, by Equation 8, and Equation 9, our result i.e. Equation 7
immediately gives the following theorem:
Theorem 1 Let AS be a one-layer GNN algorithm (of 3) equipped with the graph convolutional
filter g(LG) and trained on a dataset S for T iterations. Let the loss and activation functions
be Lipschitz-continuous and smooth. Then the following expected generalisation gap holds with
probability at least 1 - δ, δ ∈ {0, 1}:
ESGD hR(AS) - Remp(AS)i ≤
—θ((λmax
m
where the expectation ESGD is taken over the randomness inherent in SGD, m is the no. training
samples, and B is a constant which depends on the loss function. Our theorem states that GNN
trained withe Wasserstein loss enjoys the same generalisation error bound of the traditional GCN
(trained with cross entropy). We now discuss experiments.
6
Under review as a conference paper at ICLR 2020
5 Experiments
Table 1: Statistics of datasets used in the experiments. Please see Section 3 for the notations used.
Dataset	Type	Vl	|E|	IEdl	F	Avg. edge size	Max. edge size
Cora	Co-authorship	2653	2591	12071	~1~	-2.3 ± 1.9-	29
DBLP	Co-authorship	22535	43413	117215	5	4.7 ± 6.1	143
Amazon	Recommendation	84893	166994	1081994	5	3.0 ± 3.0	187
ACM	Co-authorship	67057	25511	59884	6	2.4 ± 1.2	32
arXiv	Co-authorship	790790	1354752	6728683	7	4.0 ± 19.7	2832
To demonstrate the effectiveness of our proposed DHN, we conducted experiments on 5 real-world
directed hypergraphs. Four of them are Co-authorship datasets and one of them is a recommendation
dataset. Table 1 shows the statistics of the datasets. For more details on the construction of the
datasets, please see Appendix A.6.
5.1 Experimental setup
Inspired by the experimental setups of prior related works Kipf & Welling (2017); Liao et al. (2019),
we tune hyperparameters using the Cora citation network dataset alone and use the optimal hyper-
parameters for all the other datasets. We hyperparameterise the cost matrix (base metric of the
Wasserstein distance) as follows:
1	η	η	...	η	η
η	1	η	...	η	η
η η η ... η 1
The cost matrix C is an F × F matrix (F is the number of histogram bins) with ones on the diagonal
and a hyperparameter η elsewhere. We could have used a matrix of all ηs. But it is no different
from a matrix of all ones from the optimisation perspective and so we used the above more general
matrix. Details of hyperparameter tuning and optimal hyperparameters are in Appendix A.5.
5.2	Baselines
We used both Wasserstein distance and KL divergence to train different models. As already noted,
we used the Sinkhorn algorithm to compute the (regularised) Wasserstein distance. Please see Ap-
pendix A.1 for more details. We compared DHN with the following baselines:
•	KL-MLP: We used a simple multi-layer perceptron (MLP) on the features of the vertices
and trained it using KL-divergence
•	OT-MLP: We trained another MLP with the Wasserstein distance as the loss function. Note
that this baseline and the previous baseline do not use the structure (graph / hypergraph)
•	KLR-MLP: We regularised an MLP with explicit KL-divergence-based regularisation that
uses the structure (graph / hypergraph) Subramanya & Bilmes (2011).
•	OTR-MLP: We regularised an MLP with explicit Wasserestein-distance-based regularisa-
tion that uses the structure (graph / hypergraph) Solomon et al. (2014). For hypergraphs we
used the clique expansion of the hypergraph Gao et al. (2019).
•	KL-HGNN / KL-HyperGCN: We trained the different GCN-based methods on hyper-
graphs with KL divergence loss function on the labelled vertices.
•	Soft-HGNN / Soft-HyperGCN: We trained the different GCN-based methods on hyper-
graphs with the Wasserstein distance as the loss function.
Metric for comparison: We use the mean squared error (MSE) between true and predicted distri-
butions on the test set of vertices. Table 2 shows MSEs on the test split for all the three datasets.
7
Under review as a conference paper at ICLR 2020
Table 2: Results on real-world directed hypergraphs. We report 100× mean squared errors (lower
is better) over 10 different train-test splits. Note that all the reported numbers need to be multiplied
by 0.01 to get the actual numbers. Please see section 5 for more details.
Method	Cora	DBLP	ACM	Amazon Office Products	arXiv
KL-MLP	8.94 ± 0.16	7.72 ± 0.14	8.47 ± 0.15	6.81 ± 0.16	10.87 ± 0.25
OT-MLP	7.45 ± 0.35	7.53 ± 0.18	7.85 ± 0.26	6.78 ± 0.24	10.01 ± 0.23
KLR-MLP	8.05 ± 0.22	7.35 ± 0.18	7.82 ± 0.29	6.74 ± 0.15	-
OTR-MLP	6.57 ± 0.43	7.24 ± 0.18	6.77 ± 0.32	6.72 ± 0.23	-
KL-HGNN	7.86 ± 0.25	7.17 ± 0.12	7.23 ± 0.19	6.71 ± 0.19	9.95 ± 0.25
KL-HyperGCN	7.95 ± 0.27	7.15 ± 0.17	7.53 ± 0.21	6.69 ± 0.17	9.99 ± 0.23
Soft-HGNN	5.97 ± 0.37	6.18 ± 0.37	6.02 ± 0.37	6.63 ± 0.39	8.61 ± 0.49
Soft-HyperGCN	6.02 ± 0.32	6.21 ± 0.35	6.04 ± 0.32	6.61 ± 0.30	8.60 ± 0.47
KL-DHN (ours)	7.04 ± 0.24	6.97 ± 0.22	7.16 ± 0.24	6.65 ± 0.17	9.34 ± 0.32
Soft-DHN (ours)	4.87 ± 0.40	5.65 ± 0.42	5.12 ± 0.34	6.55 ± 0.33	7.69 ± 0.36
5.3	Discussion
We used a simple one-layer architecture for our proposed DHN and a 2-hop simplified GCN Wu
et al. (2019a) as the GNN model on the graph G = (E, Ed) i.e.
Z = SoftmaX(I ∙ HE ∙ Θι),	HE = A2XeΘ2
where Ais the symmetically normalised adjacency (with self loops) of the graph G . We demonstrate
that this simple model is effective enough through an ablation study in Appendix Table 4. Our results
demonstrate strong performances across all the datasets esp. on the co-authorships networks.
Specifically, we observe that Soft models (that use the Wasserstein loss) are almost always superior
to their counterparts that use the KL divergence as the loss function. This is because the Soft models
can exploit the distance matrix C while KL-divergence does not.
Moreover, our proposed DHN outperforms two strong hypergraph baselines viz. HGNN Feng et al.
(2019) and HyperGCN Yadati et al. (2019). We believe this is because they do not exploit the
rich structural information in the directed hyperedges (connections among hyperedges) while our
proposed DHN does exploit them.
We also experimented on standard graph benchmark node-classification datasets such as Cora, Cite-
seer, and Pubmed by treating the class label as one-hot probability distribution. We used the Soft
variants of GCN KiPf & Welling (2017), Simple GCN Wu et al.(2019a), and GAT Velickovic et al.
(2018). We achieved competitive results as shown in Appendix Table 3 6
6 Conclusion
We have proposed DHN, a novel method for soft SSL on directed hypergraphs. DHN can effectively
propagate histograms to unknown vertices by integrating vertex features, directed hyperedges and
undirected hypergraph structure. As a key contribution, we have established generalisation bounds
for DHN within the framework of algorithmic stability. We have also demonstrated DHN’s effec-
tiveness through detailed experimentation on real-world hypergraph datasets.
8
Under review as a conference paper at ICLR 2020
References
Sameer Agarwal, Kristin Branson, and Serge Belongie. Higher order learning with graphs. In ICML,
2006. 2.
Jason Altschuler, Jonathan Weed, and Philippe Rigollet. Near-linear time approximation algorithms
for optimal transport via sinkhorn iteration. In NIPS. Curran Associates, Inc., 2017. 14.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. In (ICLR, 2015. 1.
Peter W. Battaglia, Jessica B. Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Flo-
res Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan
Faulkner, Caglar GUlCehre, Francis Song, Andrew J. Ballard, Justin Gilmer, George E. Dahl,
Ashish Vaswani, Kelsey Allen, Charles Nash, Victoria Langston, Chris Dyer, Nicolas Heess,
Daan Wierstra, Pushmeet Kohli, Matthew Botvinick, Oriol Vinyals, Yujia Li, and Razvan Pas-
canu. Relational inductive biases, deep learning, and graph networks. CoRR, arXiv:1806.01261,
2018. 2.
Michael M. Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. Geo-
metric deep learning: Going beyond euclidean data. IEEE Signal Process., 34(4):18-42, 2017.
2.
Samuel R. Bulo and Marcello Pelillo. A game-theoretic approach to hypergraph clustering. In NIPS.
Curran Associates, Inc., 2009. 3.
T.-H. Hubert Chan and Zhibin Liang. Generalizing the hypergraph laplacian via a diffusion process
with mediators. In COCOON, 2018. 3and4.
Olivier Chapelle, Bernhard Scholkopf, and Alexander Zien. Semi-Supervised Learning. The MIT
Press, 2010. 3.
Jianfei Chen, Jun Zhu, and Le Song. Stochastic training of graph convolutional networks with
variance reduction. In ICML, 2018. 2.
Yukun Chen, Jianbo Ye, and Jia Li. A distance for hmms based on aggregated wasserstein metric
and state registration. In ECCV, pp. 451-466, 2016. 4.
Zhengdao Chen, Xiang Li, and Joan Bruna. Supervised community detection with line graph neural
networks. In ICLR, 2019a. 2.
Zhengdao Chen, Soledad Villar, Lei Chen, and Joan Bruna. On the equivalence between graph
isomorphism testing and function approximation with gnns. In NeurIPS. Curran Associates, Inc.,
2019b. 2.
I Chien, Chung-Yi Lin, and I-Hsiang Wang. Community detection in hypergraphs: Optimal statisti-
cal limit and efficient algorithms. In AISTATS, 2018. 2.
I (Eli) Chien, Huozhi Zhou, and Pan Li. hs2: Active learning over hypergraphs with pointwise and
pairwise queries. In International Conference on Artificial Intelligence and Statistics (AISTATS),
pp. 2466-2475, 2019. 3.
Uthsav Chitra and Benjamin J Raphael. Random walks on hypergraphs with edge-dependent vertex
weights. In Proceedings of the 36th International Conference on Machine Learning (ICML),
2019. 3.
Colin B. Clement, Matthew Bierbaum, Kevin P. O’Keeffe, and Alexander A. Alemi. On the use of
arxiv as a dataset. CoRR, abs/1905.00075, 2019. 20.
Adrian Corduneanu and Tommi S. Jaakkola. Distributed information regularization on graphs. In
NIPS, pp. 297-304. MIT Press, 2005. 3.
Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In NIPS. Curran
Associates, Inc., 2013. 3, 13, and 14.
9
Under review as a conference paper at ICLR 2020
Nima Dehmamy, Albert-Laszlo Barabasi, and Rose Yu. Understanding the representation power of
graph neural networks in learning graph topology. In NeurIPS. Curran Associates, Inc., 2019. 2.
Engin Demir, Cevdet Aykanat, and Berkant Cambazoglu. Clustering spatial networks for aggregate
query processing: A hypergraph approach. Information Systems, 2008. 2.
Fuli Feng, Xiangnan He, Yiqun Liu, Liqiang Nie, and Tat-Seng Chua. Learning on partial-order
hypergraphs. In WWW, 2018. 2.
Yifan Feng, Haoxuan You, Zizhao Zhang, Rongrong Ji, and Yue Gao. Hypergraph neural networks.
In AAAI, 2019. 2,4,and8.
Charlie Frogner, Chiyuan Zhang, Hossein Mobahi, Mauricio Araya, and Tomaso A Poggio. Learn-
ing with a wasserstein loss. In NIPS, pp. 2053-2061. Curran Associates, Inc., 2015. 3.
Charlie Frogner, Farzaneh Mirzazadeh, and Justin Solomon. Learning entropic wasserstein embed-
dings. In ICLR, 2019. 3.
Giorgio Gallo, Giustino Longo, Stefano Pallottino, and Sang Nguyen. Directed hypergraphs and
applications. Discrete Appl. Math., 1993. 1 and 3.
Tingran Gao, Shahab Asoodeh, Yi Huang, and James Evans. Wasserstein soft label propagation on
hypergraphs: Algorithm and generalization error bounds. In AAAI, 2019. 3,4,and7.
Aude Genevay, Gabriel Peyre, and Marco Cuturi. Learning generative models with sinkhorn diver-
gences. In AISTATS, 2018. 3.
Gourab Ghoshal, Vinko Zlatic, Guido Caldarelli, and M. E. J. Newman. Random hypergraphs and
their applications. Phys. Rev. E, 2009. 2.
Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural
message passing for quantum chemistry. In ICML, 2017. 2.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS. Curran Associates,
Inc., 2014. 1.
V. M. Govindu. A tensor decomposition for geometric grouping and segmentation. In CVPR, 2005.
2.
Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs.
In NIPS. Curran Associates, Inc., 2017a. 2.
William L. Hamilton, Rex Ying, and Jure Leskovec. Representation learning on graphs: Methods
and applications. IEEE Data Eng. Bull., 40(3):52-74, 2017b. 2.
Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic
gradient descent. In ICML, 2016. 6and 15.
Ruining He and Julian McAuley. Ups and downs: Modeling the visual evolution of fashion trends
with one-class collaborative filtering. In WWW, 2016. 19.
Matthias Hein, Simon Setzer, Leonardo Jost, and Syama Sundar Rangapuram. The total variation
on hypergraphs - learning on hypergraphs revisited. In NIPS. Curran Associates, Inc., 2013. 3.
Tatsuro Kawamoto. Mean-field theory of graph neural networks in graph partitioning. In NIPS.
Curran Associates, Inc., 2018. 2.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. In ICLR, 2017. 1,2,7,8,17,and18.
Steffen Klamt, Utz-Uwe Haus, and Fabian Theis. Hypergraphs and cellular networks. PLOS Com-
putational Biology, 2009. 2.
10
Under review as a conference paper at ICLR 2020
Boris Knyazev, Graham W. Taylor, and Mohamed R. Amer. Understanding attention and general-
ization in graph neural networks. In NeurIPS. Curran Associates, Inc., 2019. 2.
Tamara G. Kolda and Brett W. Bader. Tensor decompositions and applications. SIAM Rev., 51(3):
455-500, 2009. 3.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convo-
lutional neural networks. In NIPS. Curran Associates, Inc., 2012. 1.
Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recog-
nition. Proceedings of the IEEE, 1998. 1.
Yann LeCun, Yoshua Bengio, and Geoffey Hinton. Deep learning. Nature, pp. 436-444, 2015. 1.
Jia Li, Yu Rong, Hong Cheng, Helen Meng, Wenbing Huang, and Junzhou Huang. Semi-supervised
graph classification: A hierarchical graph perspective. In WWW, 2019. 2.
Pan Li and Olgica Milenkovic. Inhomogeneous hypergraph clustering with applications. In Ad-
vances in Neural Information Processing Systems (NIPS) 30, pp. 2308-2318. Curran Associates,
Inc., 2017. 3.
Pan Li and Olgica Milenkovic. Revisiting decomposable submodular function minimization with
incidence relations. In Advances in Neural Information Processing Systems (NeurIPS) 31, pp.
2237-2247. Curran Associates, Inc., 2018a. 3.
Pan Li and Olgica Milenkovic. Submodular hypergraphs: p-laplacians, Cheeger inequalities and
spectral clustering. In Proceedings of the 35th International Conference on Machine Learning
(ICML), pp. 3014-3023, 2018b. 3.
Pan Li, Niao He, and Olgica Milenkovic. Quadratic decomposable submodular function minimiza-
tion. In Advances in Neural Information Processing Systems (NeurIPS) 31, pp. 1054-1064. Cur-
ran Associates, Inc., 2018a. 3.
Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for
semi-supervised learning. In Proceedings of the Thirty-Second Conference on Association for the
Advancement of Artificial Intelligence (AAAI), pp. 3538-3545, 2018b. 18.
Renjie Liao, Zhizhen Zhao, Raquel Urtasun, and Richard S. Zemel. Lanczosnet: Multi-scale deep
graph convolutional networks. In ICLR, 2019. 7and 18.
Jianxin Ma, Peng Cui, Kun Kuang, Xin Wang, and Wenwu Zhu. Disentangled graph convolutional
networks. In ICML, 2019. 2.
Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. Provably powerful graph
networks. In NeurIPS. Curran Associates, Inc., 2019. 2.
Julian McAuley, Christopher Targett, Qinfeng Shi, and Anton van den Hengel. Image-based recom-
mendations on styles and substitutes. In SIGIR, 2015. 19.
Federico Monti, Oleksandr Shchur, Aleksandar Bcjchevski, Or Litany, StePhan Gunnemann, and
Michael M. Bronstein. Dual-primal graph convolutional networks. CoRR, abs/1806.00770, 2018.
2.
ChristoPher Morris, Martin Ritzert, Matthias Fey, William L. Hamilton, Jan Eric Lenssen, Gaurav
Rattan, and Martin Grohe. Weisfeiler and leman go neural: Higher-order graPh neural networks.
In AAAI, 2019. 2.
Boris Muzellec and Marco Cuturi. Generalizing Point embeddings using the wasserstein sPace of
elliPtical distributions. In NeurIPS. Curran Associates, Inc., 2018. 3.
Gabriel Peyre and Marcc Cuturi. Computational optimal transport. Foundations and Trends in
Machine Learning, 11(5-6):355-607, 2019. 3.
Meng Qu, Yoshua Bengio, and Jian Tang. GMNN: Graph Markov neural networks. In Proceedings
of the 36th International Conference on Machine Learning (ICML), pp. 5241-5250, 2019. 2.
11
Under review as a conference paper at ICLR 2020
Amnon Shashua, Ron Zass, and Tamir Hazan. Multi-way clustering using super-symmetric non-
negative tensor factorization. In ECCV, 2006. 3.
Justin Solomon, Raif Rustamov, Leonidas Guibas, and Adrian Butscher. Wasserstein propagation
for semi-supervised learning. In ICML, 2014. 1, 3, and 7.
Amarnag Subramanya and Jeff Bilmes. Semi-supervised learning with measure propagation. J.
Mach. Learn. Res.,12:3311-3370, 2011. 3and7.
Amarnag Subramanya and Partha Pratim Talukdar. Graph-Based Semi-Supervised Learning. Mor-
gan & Claypool Publishers, 2014. 3.
Lichao Sun, Ji Wang, Philip S. Yu, and Bo Li. Adversarial attack and defense on graph data: A
survey. CoRR, arXiv:1812.10528, 2018. 2.
Jie Tang, Jing Zhang, Limin Yao, Juanzi Li, Li Zhang, and Zhong Su. Arnetminer: Extraction and
mining of academic social networks. In KDD, 2008. 19.
Koji Tsuda. Propagating distributions on a hypergraph by dual information regularization. In ICML,
2005. 3.
Shikhar Vashishth, Prateek Yadav, Manik Bhandari, and Partha Talukdar. Confidence-based graph
convolutional networks for semi-supervised learning. In AISTATS, 2019. 1 and 2.
Titouan Vayer, Nicolas Courty, Romain Tavenard, Chapel Laetitia, and Remi Flamary. Optimal
transport for structured data with application on graphs. In ICML, 2019. 3.
Petar VeliCkovic, GUillem CUcUrUlL Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. In ICLR, 2018. 1,2,8,and17.
Petar VeliCkoviC, William Fedus, William L. Hamilton, Pietro Lio, Yoshua Bengio, and R Devon
Hjelm. Deep graph infomax. In ICLR, 2019. 2.
SaUrabh Verma and Zhi-Li Zhang. Stability and generalization of graph ConvolUtional neUral net-
works. In KDD, 2019. 2,4,5,6,15,and17.
C Villani. TopiCs in optimal transportation theory. 2003. 14.
C Villani. Optimal transport - Old and new, volume 338. Springer-Verlag, 2008. 3 and5.
Chris Wendler, Dan Alistarh, and Markus PUschel. Powerset convolutional neural networks. In
NeurIPS. CUrran AssoCiates, InC., 2019. 3.
Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. Simpli-
fying graph convolutional networks. In ICML, 2019a. 1,2,and8.
Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S. Yu. A
comprehensive survey on graph neural networks. CoRR, arXiv:1901.00596, 2019b. 2.
Hongteng Xu, Dixin Luo, Hongyuan Zha, and Lawrence Carin. Gromov-wasserstein learning for
graph matching and node embedding. In ICML, 2019a. 3.
Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie
Jegelka. Representation learning on graphs with jumping knowledge networks. In ICML, 2018.
2.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? In ICLR, 2019b. 2.
Naganand Yadati, Madhav Nimishakavi, Prateek Yadav, Vikram Nitin, Anand Louis, and Partha
Talukdar. HyperGCN: A new method of training graph convolutional networks on hypergraphs.
In NeurIPS. Curran Associates, Inc., 2019. 2,4,and8.
Zhilin Yang, William W. Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning
with graph embeddings. In ICML, 2016. 3.
12
Under review as a conference paper at ICLR 2020
Rex Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, and Jure Leskovec. Gnn explainer: A tool
for post-hoc explanation of graph neural networks. In NeurIPS. Curran Associates, Inc., 2019. 2.
Chenzi Zhang, Shuguang Hu, Zhihao Gavin Tang, and T-H. Hubert Chan. Re-revisiting learning on
hypergraphs: Confidence interval and subgradient method. In ICML, 2017. 3.
Ziwei Zhang, Peng Cui, and Wenwu Zhu. Deep learning on graphs: A survey. CoRR,
arXiv:1812.04202, 2018. 2.
Denny Zhou, JiayUan Huang, and Bernhard Scholkopf. Learning with hypergraphs: Clustering,
classification, and embedding. In B. Scholkopf, J. C. Platt, and T. Hoffman (eds.), NIPS. MIT
Press, 2007. 2,4,and5.
Jie Zhou, Ganqu Cui, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, and Maosong Sun. Graph neural
networks: A review of methods and applications. CoRR, arXiv: 1812.08434, 2018. 2.
Dingyuan Zhu, Peng Cui, Daixin Wang, and Wenwu Zhu. Deep variational network embedding in
wasserstein space. In KDD, 2018. 3.
Xiaojin Zhu, Andrew B. Goldberg, Ronald Brachman, and Thomas Dietterich. Introduction to Semi-
Supervised Learning. Morgan and Claypool Publishers, 2009. 3.
A Appendix
The appendix is organised as follows:
1.	Regularised Wasserstein and the Sinkhorn algorithm
2.	Time complexity of the proposed DHN
3.	Proofs and more notations used for theoretical analysis
4.	Additional experiments (Graph-based soft SSL and Ablation Study)
5.	Details of hyperparameters
6.	Sources of real-world datasets
A. 1 Regularised Wasserstein and the Sinkhorn Algorithm
For discrete distributions, Wp of Equation 2 is the solution of a linear program:
FF
Wp (z, y)p = minXXCipjπij
i=1 j=1
FF
s.t.	πij	≥ 0,	πij	=	[z]i ,	πij	=	[y]j	∀1 ≤ i,j ≤ F
j=1	i=1
where for an arbitrary element q ∈ PF, [q]i stands for the probability mass in the ith bin.
(10)
A.1.1 Sinkhorn divergence
The expensive linear program 10 can be efficiently solved by entropic regularisation Cuturi (2013):
FF
Wλ(z,y)p = min £ ECj ∏ j + λ ∙ Tr ("log π - 11T 广)	(11)
i=1j =1
FF
s.t. πij ≥ 0, πij = [z]i , πij = [y]j	∀1 ≤ i,j ≤ F
j=1	i=1
where log(∙) is applied element-wise and λ ≥ 0 is a hyperparameter. The optimal solution π* for
λ > 0 takes the following form.
*	1	/、	(CP
π* = diag(r) ∙ exp I--
. diag(c)
where diag(z) is a diagonal matrix with the components of z in the diagonal places.
13
Under review as a conference paper at ICLR 2020
Sinkhorn algorithm Cuturi (2013): We optimise Equation 11 for r and c via matrix balancing,
i.e., start with an initial K := exp(-C) and alternately ensure the marginal constraints are satisfied
until convergence:
r — z.∕(Kc)	C - y.∕(K T r)
where ./ is element-wise division. We use the above efficient algorithm in our experiments.
A.2 Time complexity of DHN
We consider the problem of predicting probability distributions for the vertices in H = (V, Ed)
given a typically small subset Vk ⊆ V of vertices with known distributions. In this work, we
are concerned with discrete distributions modelled on a complete separable metric space (M, C).
Furthermore, we assume that we are provided with a feature matrix, XV ∈ Rn×DV , in which each
vertex v ∈ V is represented by a Dv -dimensional feature vector xv (here n = |V |). We are also
provided with a hyperedge feature matrix XE ∈ Rm×DE with xe , e ∈ E as DE-dimensional feature
representations. Let T be the time taken for the Sinkhorn algorithm on all the vertices with known
distributions. Note that T can be approximated in near-linear time Altschuler et al. (2017). Further,
let T be the total number of epochs of training. Define
N:=X|e|,	Nc:=X |e|C2
e∈E	e∈E
The time complexity oa one-layer DHN is O (∣Ed∣∙ DE ∙ h1 + |E| ∙ DV ∙ h2) where hi is the number
of hidden units of the GNN layer and h2 is the number of output channels.
A.3 Theoretical Analysis: Proofs
This section is organised as follows:
1.	Assumptions/notations
2.	Definitions of generalisation and empirical errors
3.	Framework of Algorithmic Stability
4.	Proof of SGD Bound
A.3.1 Assumptions/Notations
To avoid unnecessary technical complications, assume the histogram admits a geometric realisation
over the one-dimensional Euclidean space, such that the ith bin is placed at location bi ∈ R, and set
Cij := |bi - bj | ,	∀1 ≤ i, j ≤ F.
Without loss of generality, We assume bi ≤ b2 ≤ ∙∙∙ ≤ bp, and write h := bi+i - b ≥ 0 for all
i = 1, . . . , F - 1. Denote the diameter of the support by D := max1≤i,j≤F |bi - bj | = bF - b1. We
take the Wasserstein cost as the Wasserstein-I distance: W (μ, V) := Wi (μ, V). In this particular
one-dimensional setting, we have a particularly simple form for the cost function:
Wi (μ, ν)=
i∞
/	IF-I	(S)- F-i	(s)∣ds	= /	∣Fμ	(t)	-	FV	(t)l dt
0	-∞
(12)
where Fu : R → [0,1], FV : R → [0,1] are the cumulative distribution functions of μ, V, respec-
tively; F-i, F-i are the generalized inverses of Fμ and FV, respectively, defined as (similar for
FV-i)
F-i (t) := inf {b ∈ R : Fμ (b) >t} ,	∀t ∈ [0,1].	(13)
This characterisation is seen in any standard literature on optimal transport, e.g., (Villani, 2003,
Theorem 2.18).
14
Under review as a conference paper at ICLR 2020
A.3.2 Definitions: Generalisation and Empirical Errros
Let the learning algorithm AS on a dataset S be a function from ζm to (Y)X. where X is the input
Hilber space, Y is the output Hilbert space, ζ = X × Y. The training set of datapoints, labels is
S = {zι = (χι,yι), ∙∙∙ ,zk = (Xk,yk))}. Let the loss function be ' : Zm X Z → R. Then the
generalisation error or risk R(AS) is deined as
R(AS ):= e['(As ,z)] = / '(As ,z)p(z)dz
where p(z ) is the probability of seeing the sample z ∈ S.
The empirical error, on the other hand, is defined as
1k
Remp(AS) ：= kE'(As，Zj)
j=1
A.3.3 Algorithmic Stability
Denote S ⊂ V × PF for an arbitrary training data set sampled with respect to distribution D. A
learning algorithm for GNN, denoted as A, maps a training set S to a trained GNN f (∙, Θs) : x →
f (x, ΘS). Let S0 be another training data set that differs from S by exactly one data. Our goal in
this section is to establish a uniform stability
SUp	|Ea [' (f(∙, Θs), (x, y))] - EA [' (f]∙, Θso), (x, y))]∣
S⊂V ×PF
(x,y)∈V ×PF
= sUp	|EA	[W1	(f (x, ΘS) ,y)]	-	EA	[W1 (f(x,ΘS0)	,y)]|
S⊂V ×PF
(x,y)∈V ×PF
= sUp	|EA	[W1	(f (x, ΘS) ,y)	-	W1	(f(x,ΘS0)	,y)]| ≤	2βm	(14)
S⊂V ×PF
(x,y)∈V ×PF
with βm = O (1/m), which in turn can be used to establish the generalisation error bound of the
form (Verma & Zhang, 2019, Theorem 1), following the framework Hardt et al. (2016) for SGD-
based learning algorithms. Following the arguments in Verma & Zhang (2019), this boils down to
checking a few Lipschitz properties for the cost function.
A.3.4 SGD Bound: Proof
Note that
Wi (μ, V)=
1∞
/	IF-I	(S)-	F-1	(s)∣ds = /	∣Fμ	(t)	—	FV	(t)l	dt
0	-∞
and from the single-layer bound
IEA [Wl(f(x,θS ),y)-W1(f (x,θsθ ),y)]l≤ Lσ2D suPχ∈V lExkEA ∣∣θS — θs0 kι = Lσ2D gλEA ∣∣θS -θ5'0 kl
(15)
(16)
Uniform stability:
SUp	IEa [' (f(∙, Θs), (x, y))] - EA [' (f]∙, Θso), (x, y))]∣
S⊂V ×PF
(x,y)∈V ×PF
= SUp	|EA [W1 (f (x, ΘS) ,y)] - EA [W1 (f (x, ΘS0) ,y)]|
S⊂V ×PF
(x,y)∈V ×PF
= SUp	|EA	[W1	(f (x, ΘS) ,y) -	W1	(f (x,	ΘS0)	,y)]|	≤ 2βm	(17)
S⊂V ×PF
(x,y)∈V ×PF
It now remains to bound EA kΘS - ΘS0 k1 resulting from the SGD iterations. Given training set S,
applying SGD to GNN amounts to performing the updates
θs,t+ι = θs,t- η▽㊀'(f (∙,θ) , (Xit,y/ = θs,t- η▽㊀WI(f (Xit, θs,t) ,yQ
15
Under review as a conference paper at ICLR 2020
where η > 0 is the learning rate and zit = (xit, yit) are random data i.i.d. uniformly sampled from
the training set. By the simple formulae equation 15 for one-dimensional optimal transport, we can
explicitly write out for any parameter set Θ and data z = (x, y)
W1
(f(x,Θ),y)
Z∞
∞
F-1	i
Ff(x,Θ) (t) - Fy (t) dt = X (xi+1 - xi) X([f (x, Θ)]i - [y]i)
i=1	j=1
F-1 i
=X hi X ([σ (Ex ∙ θ)]i - [y]i)
i=1	j=1
where again we used notation [y]i to denote the probability mass of y ∈ PF in the ith bin, for all
i = 1, . . . , F. Thus
∂	F T	f i /	、) i ∂
dθ;Wι (f(x, Θ) ,y) = Nhi ∙ Sgn 伍(匕(Ex ∙ Θ)j - [yj∙三砧[σ (Ex •⑼j
F-1	i	i
=X hi ∙ Sgn { X ([σ (Ex ∙ Θ)]j - [yj b Ex X [σ (Ex ∙ Θ)j (δjk - [σ (Ex ∙ Θ)]Q
i=1	[j=1	j=1
=Ex ∙ [σ (Ex ∙ Θ)]kf hi ∙ sgn {X ([σ (Ex ∙ Θ)j - [yj) } (l - X [σ (Ex ∙ Θ)j
= Εχ∙[σ(Εχ∙Θ)]k PF-II hi∙sgn{Pj=ι([σ(Eχ∙Θ)]j-[y]j ) } (P j=(朋厂[σ(Eχ ∙Θ)] ,∙ )+£之叶][y] j )
= -Eχ∙[σ(Eχ∙Θ)]k∙Wι(σ(Eχ∙Θ),y) + Eχ∙[σ(Eχ∙Θ)]k PF-II hi∙sgn{Pj=ι ([σ(Eχ∙Θ)j-[yj)}工之计/训？
where sgn {∙} is the sign function, and δik is the Kronecker delta notation. The second equality
used the specific form of the derivative of the softmax function. Unfortunately, this gradient is not
Lipschitz continuous due to the sign function in the second term. Nevertheless, ifwe usea “modified
gradient” that drops the second term, i.e., choose to update the parameter Θk in the direction
—■
g∂
W-f(χ (f (x,	θ), y)	:= -Ex	∙	[σ	(Ex	∙ θ)]k ∙ WI	(σ (Ex	∙θ)	,y)	(18)
∂Θk
then obviously this new choice of “descent” direction is certainly Lipschitz continuous, as
I-----	-----
g∂	g∂
I ∂θ^ WI (f (x, θs ) ,y)- ∂Θ WI (f (x, θsO) ,y)
=|Ex| ∙ ∣[σ (Ex ∙ Θs)]k ∙ Wi (σ (Ex ∙ Θs) ,y) - [σ (Ex ∙ θso)]k ∙ Wi (σ (Ex ∙ Θso) ,y)|
≤ ∣Ex∣∙	∣[σ	(Ex	∙	Θs)]k - [σ (Ex	∙	Θso)]k| ∙	Wi	(σ	(Ex	∙	Θs) ,y)
+ [σ (Ex ∙ θso)[k ∙ |Wi (σ (Ex ∙ Θs) ,y) - Wi (σ (Ex ∙ Θso) ,y)|
≤ ∣Ex∣∙ (LσD ∣Ex∣∙kΘs - Θsoki + Wi (σ (Ex ∙ Θs) ,σ (Ex ∙ Θso)))
33
≤∣Ex∣∙ 2 Lσ D ∙ |Ex| ∙ ∣∣Θs - Θs0 ki ≤ 2 Lσ D ∙ gλ l∣Θs - Θs0ki .
Therefore, if we define
∂>
g∂
Wi (f (x, θ) ,y) ,…，∂θ];Wi (f (x, θ) ,y) i (19)
then the stochastic update algorithm1
_	_	r≤ _ , . ,	_	、	、
θs,t+i = θs,t - nVθWi (f (xit, θs,t) ,yit)	QO)
1 Note that this is not even a stochastic gradient descent algorithm! the “gradient” involved is a “fake”
gradient — this is the counter-intuitive part.
16
Under review as a conference paper at ICLR 2020
will still satisfy the generalization bound, due to the Lipschitz continuity
∣∣v θWι(f (x, Θs ) ,y)-V θWι (f(x, Θso) ,y)∣∣ι ≤ '。gλ k®s -。$0||「	(21)
In fact, equation 21 is exactly the GNN analogy of (Verma & Zhang, 2019, Lemma 1) which es-
tablishes the stability for the “same sample loss” case. The “different sample loss” analogy, or the
lemma in (Verma & Zhang, 2019, Lemma 2), can be trivially obtained by the definition equation 18.
In fact, noting that
g∂
∣∂θ^WI (f (x,⑼,y)	=	IEx ∙	[σ(Ex ∙θ)]k ∙ WI	(σ(Ex	∙θ),y)| ≤	gλD,
we easily obtain
I-----	 -
∂	∂
I^rWI(f (x,θs),y)- ^FrWI(f(X,θso),y) ≤ 2gλD
∂Θk	∂Θk
and it follows that
∣∣∣veΘW1(f(x,ΘS),y)-veΘW1(f(x,ΘS0),y)∣∣∣	≤2FgλD.	(22)
Putting together equation 21 and equation 22, we obtain the following analogy of (Verma & Zhang,
2019, Lemma 3): Starting with two training data sets S, S0 that differs by exactly one sample, after
each iteration t we have
EA	kΘS,t+1	-ΘS0,t+1k1	=EA	h∣∣∣ΘS,t -	ηve ΘW1 (f	(xt,	ΘS,t), yt) -	ΘS0,t + ηve ΘW1	(f (x0t,	ΘS0,t), yt0)∣∣∣i
≤EA [kθS,t+1-θS0,t + 1 kι] + (1- -m )-η- 3D Lσ g2 EA[kθS,t-θs0,tkι] + * *η-2F gλ D
≤(1 + 3ηDLσgλ) EA [kθs,t - θso,tkι] + 2nFmD.
Solving this first-order recursion gives the stability after T random update steps:
EA [kθS,T 一 θS0,T11l] ≤ 2n'g/ X (1+2nDLσg2)	.	(23)
m t=1	2
Combining equation 16 and equation 23 gives us
|Ea [Wι(f(x, Θs ) ,y) - Wi (f(x,。$，) ,y)]∣≤ nFLσ g2 D X(1 + 3 ηDL° gλ1' . (24)
m2
t=1
Therefore, we actually have the uniform algorithmic stability equation 17 holds with
η _ ηFL。gλD2 X A , 3 dl	2丫-1	z251
βm =	2m	T ( 1 + 2ηDLσgλ )	.	(25)
t=1
A.4 Additional Experiments
Dataset	GCN	Soft-GCN	GAT	Soft-GAT	Simple-GCN	Soft-Simple-GCN
Cora	81.5	817	82.8	-825-	81	816
Citeseer	70.3	701	70.6	-706-	718	716
Pubmed	79	78.6	,78.7	78.4	78.8	—	78.8
Table 3: Accuracy on traditional graph-based SSL datasets. The experimental setting is the same
as in GCN KiPf & Welling (2017) and GAT Velickovic et al. (2018). We used 10% of the labelled
vertices as validation data to tune the hyperparameter η.
A.4.1 A blation S tudy
In this section, we comPared our ProPosed one-layer DHN against deePer layers and without ex-
Ploiting directed hyPeredges.
17
Under review as a conference paper at ICLR 2020
Table 4: Ablation study of our proposed Soft-DHN. Please see section 5 for more details.
# DHN layers	# GNN layers(hops)	Cora	DBLP
2	2	7.68 ± 0.24	7.98 ± 0.27
2	1	7.64 ± 0.25	7.93 ± 0.28
2	0	7.69 ± 0.27	7.98 ± 0.22
1	0	5.64 ± 0.32	6.45 ± 0.38
1	1	5.41 ± 0.35	6.26 ± 0.32
1	2	4.87 ± 0.40	5.65 ± 0.42
A.5 Details of hyperparameters
Inspired by the experimental setups of prior related works Kipf & Welling (2017); Liao et al. (2019),
we tune hyperparameters using the Cora co-authorship network dataset alone. The optimal hyperpa-
rameters are fixed and then used for all the other datasets. Table 5 shows the list of hyperparameters
used in the datasets. Prior works Kipf & Welling (2017); Liao et al. (2019) have extensively per-
formed tuning of hyperparameters such as hidden size, learning rate, etc and we fixed their reported
optimal hyperparameters. It should be noted that self training and co-training methods Li et al.
(2018b) can also be used in case of absence of validation data. We hyperparameterise the cost
matrix (base metric of the Wasserstein distance) as follows:
C
1ηη
η1η
ηη
ηη
ηηη
η1
Table 6 shows the best results on the validation split of Cora (with optimal hyperaparameters). The
training set had 140 vertices, the validation set 1000 vertices and the rest of the vertices were used
to test the models. The results reported are after 200 epochs of training with a seed value of 598.
Table 5: List of hyperparameters used in the experiments. A set of values indicates that the corre-
sponding hyperparameter is tuned from the set (on the validation split).
Hyperparameter	Value(s)	Hyperparameter	Value(s)
hidden size	16	e (Sinkhorn)	0.1
learning rate	0.01	# Sinkhorn iterations	100
dropout	0.5	η	{0,1, 2,…，40}
weight decay	5 × 10-4	λ	{1, 0.5, 0.1, 0.05, 0.01, ∙∙∙ , 5 × 10-7, 10-7}
A.6 Sources of the real-world datasets
Co-authorship data: All authors co-authoring a paper are in one hyperedge. We used the author
data2to get the co-authorship hypergraph for cora. We manually constructed the DBLP dataset from
Arnetminer3.
2https://people.cs.umass.edu/ mccallum/data.html
3https://aminer.org/lab-datasets/citation/DBLP-citation-Jan8.tar.bz
18
Under review as a conference paper at ICLR 2020
Table 6: Optimal hyperparameters on the validation set on Cora Co-authorship network.
Method	Optimal hyperparameters	Best MSE on validation set
KL-MLP	-	7.87
OT-MLP	η=31	6.47
KLR-MLP	-	7.39
OTR-MLP	η=25,λ= 5 × 10-3	4.86
KL-HGNN	-	6.98
KL-HyperGCN	-	7.03
Soft-HGNN	η=20	3.24
Soft-HyperGCN	η=17	4.02
KL-DHN	-	6.34
Soft-DHN	η= 19	2.67
A.6.1 Construction of the DBLP dataset
We downloaded the entire dblp data from https://aminer.org/lab-datasets/
citation/DBLP-citation-Jan8.tar.bz Tang et al. (2008). The steps for constructing
the dblp dataset used in the paper are as follows:
•	We defined a set of 5 conference categories (histograms for the SSL task) as “algorithms”,
“database”, “datamining”, “intelligence”, and “vision”
•	For a total of 4304 venues in the entire dblp dataset we took papers from only a sub-
set of venues from https://en.wikipedia.org/wiki/List_of_computer_
science_conferences corresponding to the above 5 conferences
•	From the venues of the above 5 conference categories, we got 22535 authors publishing at
least two documents for a total of 43413
•	We took the abstracts of all these 43413 documents, constructed a dictionary of the most
frequent words (words with frequency more than 100) and this gave us a dictionary size of
1425
•	We then extracted the 117215 citation links among these documents
A.6.2 Construction of the Amazon Office Product dataset
We downloaded the entire Amazon data He & McAuley (2016); McAuley et al. (2015). The steps
for constructing the dataset used in the paper are as follows:
•	We downloaded the office product ratings subset from the entire dataset
•	We constructed a hypergraph of items with each hyperedge representing a user connecting
all the items that they bought
•	We removed hyperedges of size 1
•	We connected a pair of hyperedges (bi-directional) if they had more than 1 item in common
A.6.3 Construction of the ACM dataset
We downloaded the entire ACM data from https://lfs.aminer.org/lab-datasets/
citation/acm.v9.zip Tang et al. (2008). The steps for curating the dataset in the paper are
as follows:
19
Under review as a conference paper at ICLR 2020
•	Based on the number of papers published, we identified the six most popular venues: “Jour-
nal of Computational Physics”, “IEEE Transactions on Pattern Analysis and Machine In-
telligence”, “Automatica (Journal of IFAC)”, “IEEE Transactions on Information Theory”,
“Expert Systems with Apllications: An International Journal”, and “IEEE Transactions on
Computers”
•	We then listed the set of all authors published in these venues (we got a total of 67057
authors).
•	We finally obtained the citation relationships of all the documents co-authored by these
authors (total number of documents is 25511 and total number of citations is 59884)
A.6.4 Details of the ArXiv dataset
We downloaded the entire arXiv dataset Clement et al. (2019) from https://github.com/
mattbierbaum/arxiv-public-datasets/releases/tag/v0.2.0. The steps for cu-
rating the dataset in the paper are as follows:
•	We removed papers without any authors and got a total of 13, 54, 752 edges
•	We extracted 67, 28, 683 citation edges among these papers
•	The total number of authors in these papers is 7, 90, 790
20