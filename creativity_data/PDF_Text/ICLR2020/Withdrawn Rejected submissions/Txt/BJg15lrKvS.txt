Under review as a conference paper at ICLR 2020
Towards Understanding the Spectral Bias of
Deep Learning
Anonymous authors
Paper under double-blind review
Ab stract
An intriguing phenomenon observed during training neural networks is the spec-
tral bias, where neural networks are biased towards learning less complex func-
tions. The priority of learning functions with low complexity might be at the
core of explaining generalization ability of neural network, and certain efforts
have been made to provide theoretical explanation for spectral bias. However,
there is still no satisfying theoretical result justifying the underlying mechanism
of spectral bias. In this paper, we give a comprehensive and rigorous explanation
for spectral bias and relate it with the neural tangent kernel function proposed in
recent work. We prove that the training process of neural networks can be de-
composed along different directions defined by the eigenfunctions of the neural
tangent kernel, where each direction has its own convergence rate and the rate is
determined by the corresponding eigenvalue. We then provide a case study when
the input data is uniformly distributed over the unit sphere, and show that lower
degree spherical harmonics are easier to be learned by over-parameterized neural
networks.
1	Introduction
Over-parameterized neural networks have achieved great success in many applications such as com-
puter vision (He et al., 2016), natural language processing (Collobert and Weston, 2008) and speech
recognition (Hinton et al., 2012). It has been shown that over-parameterized neural networks can
fit complicated target function or even randomly labeled data (Zhang et al., 2016) and still exhibit
good generalization performance when trained with real labels. Intuitively, this is at odds with the
traditional notion of generalization ability such as model complexity. In order to understand neural
network training, a line of work (Soudry et al., 2017; Gunasekar et al., 2018b;a) has made efforts in
the perspective of “implicit bias”, which states that training algorithms for deep learning implicitly
pose an inductive bias onto the training process and lead to a solution with low complexity measured
by certain norms in the parameter space of the neural network.
Among many attempts to establish implicit bias, Rahaman et al. (2018) pointed out an intriguing
phenomenon called spectral bias, which says that during training, neural networks tend to learn
the components of lower complexity faster. The concept of spectral bias is appealing because this
may intuitively explain why over-parameterized neural networks can achieve a good generalization
performance without overfitting. During training, the networks fit the low complexity components
first and thus lie in the concept class of low complexity. Arguments like this may lead to rigorous
guarantee for generalization.
Great efforts have been made in seek of explanations about the spectral bias. Rahaman et al. (2018)
evaluated the Fourier spectrum of ReLU networks and empirically showed that the lower frequencies
are learned first; also lower frequencies are more robust to random perturbation. Andoni et al. (2014)
showed that for a sufficiently wide two-layer network, gradient descent with respect to the second
layer can learn any low degree bounded polynomial. Xu (2018) provided Fourier analysis to two-
layer networks and showed similar empirical results on one-dimensional functions and real data.
Nakkiran et al. (2019) used information theoretical approach to show that networks obtained by
stochastic gradient descent can be explained by a linear classifier during early training. All these
studies provide certain explanations about why neural networks exhibit spectral bias in real tasks.
But explanations in the theoretical aspect, if any, are to some extent restricted. For example, the
popular Fourier analysis is usually done in the one-dimensional setting, and thus lacks generality.
1
Under review as a conference paper at ICLR 2020
Meanwhile, a recent line of work (Jacot et al., 2018; Du et al., 2018b; Li and Liang, 2018; Chizat
and Bach, 2018) has shed light on new approaches to analyze neural networks. In particular, they
show that under certain over-parameterized condition, the neural network trained by gradient de-
scent behaves similar to the kernel regression predictor using the neural tangent kernel (NTK) (Jacot
et al., 2018). Du et al. (2018b) showed that the convergence is provably guaranteed under cer-
tain over-parameterization conditions determined by the smallest eigenvalue of NTK. Arora et al.
(2019b) further gave a finer characterization of error convergence based on the eigenvalues of NTK’s
Gram matrix. Su and Yang (2019) improved the convergence guarantee in terms of the k-th largest
eigenvalue for certain target functions.
Inspired by these works mentioned above, we can present a theoretical explanation for spectral bias.
Under NTK regime, we establish a precise characterization for the training process of neural net-
works. More specifically, we theoretically prove that over-parameterized neural networks’ training
process can be controlled by the eigenvalues of the integrating operator defined by the NTK. Un-
der the specific case of uniform distribution on unit sphere, we give an exact calculation for these
eigenvalues and show that the lower frequencies have larger eigenvalues, which thus leads to faster
convergence. We also conduct experiments to corroborate the theory we establish.
Our contributions are highlighted as follows:
1.	We prove a generic theorem for arbitrary data distributions, which states that under certain sample
complexity and over-parameterization conditions, the error term’s convergence along different
directions actually relies on the corresponding eigenvalues. This theorem gives a more precise
control on the regression residual than Su and Yang (2019), where the authors focused on the
case when the labeling function is close to the subspace spanned by the first few eigenfunctions.
2.	We present a more general result about the spectra of the neural tangent kernel. In particular, we
show that the order of eigenvalues appears as μk “ Ω(max{k-dτ,d-k+1}). Our result is better
than the bound θ(k´d´1) derived in Bietti and Mairal (2019) when d " k, which is clearly a
more practical setting.
3.	We establish a rigorous explanation for the spectral bias based on the aforementioned theoretical
results without any specific assumptions on the target function. We show that the error terms
from different frequencies are provably controlled by the eigenvalues of the NTK, and the lower-
frequency components can be learned with less training examples and narrower networks with
a faster convergence rate. As far as we know, this is the first attempt to give a comprehensive
theory justifying the existence of spectral bias.
1.1	Additional Related Work
Recently, there is a rich literature about the property of neural tangent kernel. Jacot et al. (2018) first
showed that during training, the network function follows a descent along the kernel gradient with
respect to the Neural Tangent Kernel (NTK) under infinity width setting. Li and Liang (2018) and
Du et al. (2018b) implicitly built connection between Neural Tangent Kernel and gradient descent
by showing that GD can provably optimize sufficiently wide two-layer neural networks. In Du et al.
(2018b), it is shown that gradient descent can achieve zero training loss at a linear convergence rate
for training two-layer ReLU network with square loss. Allen-Zhu et al. (2018); Du et al. (2018a);
Zou et al. (2018); Cao and Gu (2019b); Arora et al. (2019a); Zou and Gu (2019); Cao and Gu
(2019a) further studied the optimization and generalization of deep neural networks. These papers
are all in the so-called neural tangent kernel regime, and their requirements on the network width
depend either implicitly or explicitly on the smallest eigenvalue of the kernel Gram matrix. Later,
Su and Yang (2019) showed that this smallest eigenvalue actually scales in the number of samples
n and will eventually converge to 0. In order to obtain constant convergence rate, Su and Yang
(2019) assumed that the target function f * can be approximated by the first few eigenfunctions of
the integrating operator LKf (S) ：“ ,&d κ(x, s)f (s)dτPs) where κ(∙, ∙) is the NTK function and T(S)
is the input distribution, and proved linear convergence rate up to the this approximation error.
A few theoretical results have been established towards understanding the spectra of neural tangent
kernels. Bach (2017) studied two-layer ReLU networks by relating it to kernel methods, and pro-
posed a harmonic decomposition for the functions in the reproducing kernel Hilbert space which we
utilize in our proof. Based on the technique in Bach (2017), Bietti and Mairal (2019) studied the
eigenvalue decay of integrating operator Lκf(x) defined by NTK on unit sphere by using spheri-
2
Under review as a conference paper at ICLR 2020
cal harmonics. Vempala and Wilmes (2018) calculated the eigenvalues of NTK corresponding to
two-layer neural networks with sigmoid activation function. Basri et al. (2019) established similar
results as Bietti and Mairal (2019), but considered the case of training the first layer parameters of a
two-layer networks with bias terms. Yang and Salman (2019) studied the the eigenvalues of integral
operator with respect to the NTK on Boolean cube by Fourier analysis.
The rest of the paper is organized as follows. We state the notation, problem setup and other prelim-
inaries in Section 2 and present our main results in Section 3. In Section 4, we present experimental
results to support our theory. Proofs of our main results can be found in the appendix.
2	Preliminaries
In this section we introduce the basic problem setup including the neural network structure and the
training algorithm, as well as some background on the neural tangent kernel proposed recently in
Jacot et al. (2018) and the corresponding integral operator.
2.1	Notation
We use lower case, lower case bold face, and upper case bold face letters to denote scalars, vectors
and matrices respectively. For a vector V “(vι,..., Vd)T P Rd and a number 1 ≤ P < 8, we
denote its p´norm by }v}p = (Xd= 1 ∣Vi∣p)1/p. We also define infinity norm by }v}g “ maxi |v/.
For a matrix A “(Aij )m^n, we use } A}o to denote the number of non-zero entries of A, and use
} A}f “ (∑dj=ι A2,j)1{2 to denote its Frobenius norm. Let} A}p “ max}v}p≤ι} Av}p for P21,
and } A}maχ = max*? |A∙i,j |. For two matrices A, B P Rm'n, We define〈A, B〉“ Tr(AJB). We
use A > B if A — B is positive semi-definite. In addition, we define the asymptotic notations O(∙),
O(∙), Ω(∙) and Ω(∙) as follows. Suppose that a。and bn be two sequences. We write an “ Opbnq if
limsupn_8 ∣an{bn∣ < 8, and a。“ Ω(bn) if limmf。一8 ∣an∕bn∣ > 0. We use O(∙) and Ω(∙) to
hide the logarithmic factors in O(∙) and Ω(∙).
2.2	Problem Setup
Here we introduce the basic problem setup. We consider two-layer fully connected neural networks
of the form
fw (χ) = ?m ∙ W2σ(Wiχ),
where W1 P Rmχ(d'1), W? P R1χm1 are the first and second layer weight matrices respectively,
and σ(∙) “ max{0, ∙} is the entry-wise ReLU activation function. The network is trained according
to the square loss on n training examples S “ t(xi , yi ) : i P rnsu:
LS (W) = — X	pyi — θfW (Xi))2 ,
n
(xi,yi )PS
where θ is a small coefficient to control the effect of initialization, and the data inputs tXiuin“1 is
assumed to follow some unknown distribution T on the unit sphere Sd P Rd'1. Without loss of
generality, we also assume that ∣y∕ ≤ 1.
We first randomly initialize the parameters of the network, and then apply gradient descent to opti-
mize both layers. We present our detailed neural network training algorithm in Algorithm 1.
Algorithm 1 GD for DNNs starting at Gaussian initialization
Input: Number of iterations T , step size η.
Generate each entry of W1(0) and W2(0) from N(0, 2∕m) and N(0, 1∕m) respectively.
for t = 0, 1, . . . , T — 1 do
Update Wpt'1q = W(t) — η ∙ RWLS(Wptq).
end for
Output: W(T).
1Here the dimension of input is d ` 1 since throughout this paper we assume that all training data lie in the
d-dimensional unit sphere Sd P Rd'1.
3
Under review as a conference paper at ICLR 2020
The initialization scheme for Wp0q given in Algorithm 1 is known as He initialization (He et al.,
2015). This scheme generates each entry of the weight matrices from a Gaussian distribution with
mean zero. The variances of the Gaussian distributions in initialization are chosen following the
principle that the initialization does not change the magnitudes of inputs in each layer. The sec-
ond layer parameter is not associated with the ReLU activation function, thus it is initialized with
variance 1{m instead of 2{m.
2.3	Neural Tangent Kernel
Many attempts have been made to study the convergence of gradient descent assuming the width
of the network is extremely large (Du et al., 2018b; Li and Liang, 2018). When the width of the
network goes to infinity, with certain initialization on parameters, the inner product of gradients of
the output function would converge to a limiting kernel, i.e., Neural Tangent Kernel (Jacot et al.,
2018). In this paper, We denote it by κ(x, x1) “ limm—g mT<Vwfw(。)(x), RWfWp0 (x1)〉and
we have
κ(x,x1) “〈x,x1y ∙ κι(x,x1) ' 2 ∙ κ2(x,x1),
Where
κι(x, x1) = Ew„N(0,i)[σ1(Xw, x>)σ1 (〈w, x1〉)],
κ2(x, x1) = Ew„n (o,i)[σ(Xw, x>)σ(<w, x1〉)].
(2.1)
(2.2)
Since We apply gradient descent to both layers, the Neural Tangent Kernel is the sum of tWo different
kernel functions and clearly it can be reduced to one layer training setting. These tWo kernels are
arc-cosine kernels of degree 0 and 1 (Cho and Saul, 2009). Their explicit expressions are given as
κ1 (t) =
--(π — arccos (t)),
2π
arccos (t)) ' ʌ/l — t2
(2.3)
,
Where t = 〈x,x1〉{(}x} }x1}).
2.4	Integral Operator
The theory of integral operator With respect to kernel function has been Well studied in machine
learning (Smale and Zhou, 2007; Rosasco et al., 2010) thus We only give a brief introduction here.
Let Lτ2 (X) be the Hilbert space of square-integrable functions With respect to a Borel measure τ
from X → R. For any continuous kernel function K : X X X → R and T we can define an integral
operator Lκ on Lτ2 (X) by
Lκ (f)(x) =	κ(x, y)f (y)dτ (y), x P X.
X
(2.4)
It has been pointed out in Cho and Saul (2009) that arc-cosine kernels are positive semi-definite.
Thus the kernel function κ defined by (2.1) is positive semi-definite being a product and a sum of
positive semi-definite kernels. Clearly this kernel is also continuous and symmetric. Thus we know
that the neural tangent kernel κ is a Mercer kernel.
3	Main Results
In this section we present our main results. In Section 3.1, we give a general result on the conver-
gence rate of gradient descent along different eigendirections of neural tangent kernel. Motivated
by this result, in Section 3.2, we give a case study on the spectrum of Lκ when the input data are
uniformly distributed over the unit sphere Sd. In Section 3.3, we combine the spectrum analysis
with the general convergence result to give explicit convergence rate for uniformly distributed data
on the unit sphere.
3.1	Convergence Analysis of Gradient Descent
In this section we study the convergence of Algorithm 1. Instead of studying the standard conver-
gence of loss function value, we aim to provide a refined analysis on the speed of convergence along
different directions defined by the eigenfunctions ofLκ. We first introduce the following definitions
and notations.
4
Under review as a conference paper at ICLR 2020
Let {λi}i∕ι with λι2λ2》…be the strictly positive eigenvalues of L%, and φι(∙), φ2(∙),... be
the corresponding orthonormal eigenfunctions. Set vi “ n 1{2(φi(x1q, . . . ,φi(xnqqJ,i “ 1,2, .
Note that LK may have eigenvalues with multiplicities larger than 1 and λ%, i21 are not distinct.
Therefore for any integer k, we define rk as the sum of the multiplicities of the first k distinct
eigenvalues of Lκ. Define Vrk “ (v1, . . . , vrk q. By definition, vi, i P rrks are rescaled restrictions
of orthonormal functions in Lτ2 (Sdq on the training examples. Therefore we can expect them to form
a set of almost orthonomal bases in the vector space Rn . The following lemma follows by standard
concentration inequality.
Lemma 3.1. Suppose that ∣φi(x)∣ ≤ M for all X P Sd and i P [rk]. For any δ > 0, with probability
at least 1 ´ δ,
}Vjk Vrk ´ I}max ≤ CM 2 ―九 ∕δ)∕n,
where C is an absolute constant.
Denote y “(yι,...,yn)J	and	yptq	“ θ ∙	(fwptq (xι),...,	fw(t)(xn)),	t =	0,...,T.	Then
Lemma 3.1 shows that the convergence rate of }VrJ (y ´ ypptqq}2 roughly represents the speed gra-
dient descent learns the components of the target function corresponding to the first rk eigenvalues.
The following theorem gives the convergence guarantee of }VrJ (y ´ ypptqq}2.
Theorem 3.2. Suppose ∣φj (x)| ≤ M for j P [rkS and X P Sd. For any e, δ > 0 and integer k, if
≈ / O	，/、	、	∖ C rk/f	2 /	、 /F、	1	1∖∖	.	.,	.....
n》θ(e´2 ∙ max{(λrk ´ λrk`i)´2, M4r2}), m2Ω(poly(T, l´1, e´1)), then with probability at
least 1 — δ, Algorithm 1 with η “ O((mθ2)T), θ “ O(e) satisfies
nT/2 ∙ }VJ(y — PpTq)}2 ≤ 2(1 — λrk)T . nT2 ∙ }VJy}2 + e.
Remark 3.3. Theorem 3.2 theoretically reveals the spectral bias of deep learning. Specifically, as
long as the network is wide enough and the sample size is large enough, gradient descent first learns
the target function along the the eigendirections of neural tangent kernel with larger eigenvalues, and
then learns the rest components corresponding to smaller eigenvalues. Moreover, by showing that
learning the components corresponding to larger eigenvalues can be done with smaller sample size
and narrower networks, our theory pushes the study of neural networks in the NTK regime towards
a more practical setting. For these reasons, we believe that Theorem 3.2 to certain extent explains
the empirical observations given in Rahaman et al. (2018), and demonstrates that the difficulty of a
function to be learned by neural network can be characterized in the eigenspace of neural tangent
kernel: if the target function has a component corresponding to a small eigenvalue of neural tan-
gent kernel, then learning this function up to good accuracy takes longer time, and requires more
examples and a wider network.
3.2	Spectral analysis of Neural Tangent Kernel for Uniform Distribution
After presenting a general theorem (without assumptions on data distribution) in the previous sub-
section, we now study the case when the data inputs are uniformly distributed over the unit sphere.
We present our results (an extension of Proposition 5 in Bietti and Mairal (2019)) of spectral analy-
sis of neural tangent kernel. We show Mercer decomposition of neural tangent kernel for two-layer
setting. We give explicit expression of eigenvalues and show orders of eigenvalues in both cases
when d " k and k " d.
Theorem 3.4. For any x, x1 P Sd U Rd'1, We have the Mercer decomposition of the neural tangent
kernel K : Sd X Sd → R,
8	Npd,kq
K (χ, χ1) = ∑ μk ∑ Ykj(X) YkjK),	(3.1)
k“0 j“1
where Yk,j forj = 1, ∙ ∙ ∙ , N(d, k) are linearly independent spherical harmonics of degree k in d+ 1
variables with N(d, k) = 2%—1 (k'´´2) and orders of μk are given by
μ0 = μ1 =。(1), μk = 0, k = 2j + 1,
μk = Ω 'max { dd'1kkT(k + d)´k´d, dd'1kk(k + d)´k´d´1, dd'2kk-(k + “厂—”1}) , k = 2j,
where j P N'. More specifically, we have μk = Ω (k´d´1) when k " d and μk = Ω (d´k'1)
when d " k, k = 2, 4, 6, .
5
Under review as a conference paper at ICLR 2020
Remark 3.5. In the above theorem, the coefficients μk are actually different eigenvalues of the
integral operator Lκ on Lτ2 pSdq defined by
Lκpfqpyq “	κpx, yqf pxqdτdpxq, f P Lτ2d pSdq,
Sd
where τd is the uniform probability measure on unit sphere Sd . Therefore the λrk in Theorem 3.2 is
just μk.ι given in Theorem 3.4 when Td is uniform distribution.
Remark 3.6. Vempala and Wilmes (2018) studied two-layer neural networks with sigmoid ac-
tivation function, and established guarantees to achieve 0 ` error with iteration complexity
T “(d+1)Opkq log p}f *}2/eq under the over-parameterization conditionm =(d+1)Opkqpolyp}f*}2{eq,
where f * is the target function, and e° is certain function approximation error. Another highly re-
lated work is Bietti and Mairal (2019), which gives μk “ θ(k´d´1). The order of eigenvalues We
present appears as μk “ C(max(k—d—1, d´k+1)). This is better when d " k, which is closer to the
practical setting.
3.3 Explicit Convergence Rate for Uniformly Distributed Data
In this subsection, we combine our results in the previous two subsections and give explicit conver-
gence rate for uniformly distributed data on the unit sphere. Note that the first k distinct eigenvalues
of NTK have spherical harmonics up to degree k ´ 1 as eigenfunctions.
Corollary 3.7. Suppose that k " d, and the sample txiuin“1 follows the uniform distribution τd on
the unit sphere Sd. For any e, δ > 0 and integer k, if n2Ω(e 2 ∙ max{k2d'2, k2d 2r2}), m2
Ω(Poly(T, kd'1, e´1)), then with probability at least 1 — δ, Algorithm 1 with η “ O((mθ2)T),
θ “ Or(eq satisfies
nT2 TVJ (y ´ ypTq)}2 W 2 '1 — Ω (k"1))T ∙ nT2 ∙}VJy}2 + e,
where rk “ Xk二 N (d, k1) and Vrk “ (n~^1{2φj (xi))n^rk with φι,..., φrk being a set of or-
thonomal spherical harmonics of degrees up to k — 1.
Corollary 3.8. Suppose that d " k, and the sample txiuin“1 follows the uniform distribution
Td on the unit sphere Sd. For any e, δ > 0 and integer k, if n 2Ω(e—2d2k—2r2), m 2
Ω(Poly(T, dk´2, e´1)), then with probability at least 1 — δ, Algorithm 1 with η = O((mθ2)T),
θ “ Or(e) satisfies
nT2 ∙ }VJ (y — ypTq)}2 W 2 '1 — Ω (d—k'2))T ∙ nT2 ∙ }VJy}2 + e,
where rk “ Xk二 N (d, k1) and Vrk “ (n~^1{2φj (xi))n^rfc with φι,..., φrk being a set of or-
thonomal spherical harmonics of degrees up to k — 1.
Corollaries 3.7 and 3.8 further illustrate the spectral bias of neural networks by providing exact
calculations of λrk, Vrk and M in Theorem 3.2. They show that if the input distribution is uniform
over unit sphere, then spherical harmonics with lower degrees are learned first by over-parameterized
neural networks.
Remark 3.9. In Corollaries 3.7 and 3.8, it shows that the conditions on n and m depend exponen-
tially on either k or d. We would like to emphasize that such exponential dependency is reasonable
and unavoidable. In our case, we can take the d " k setting as an example. The exponential de-
pendency in k is a natural consequence of the fact that in high dimensional space, there are a large
number of linearly independent polynomials even for very low degrees. It is apparently only reason-
able to expect to learn less than n independent components of the true function, which means that it
is unavoidable to assume
n 2 rk
k´1
“	N(d, k1) “
k´1
k1“0
k1“0
2k1 + d — 1 k1 `d´2	k´1 2k1 + d — 1 k1 `d´2	k´1
k ( d´1 ) “ Zj k ( k，一1 ) “ °(d	q.
k1 “0
Similar arguments can apply to the requirement of m and the k " d setting.
6
Under review as a conference paper at ICLR 2020
4 Experiments
In this section we illustrate our results by training neural networks on synthetic data. Across all
tasks, we train a two-layer hidden neural networks with 4096 neurons and initialize it exactly as
defined in the setup. The optimization method is vanilla full gradient descent. We sample 1000
training data which is uniformly sampled from the unit sphere in R10 .
4.1	Learning combination of spherical harmonics
First, we show a result when the target function is exactly linear combination of spherical harmonics.
The target function is explicitly defined as
f * (X) = aι ∙ PiPXZi, x〉) + a2 ∙ P2PXZ2, χyq `。4 ∙ P4pχζ4, χyq,
where the Pk ptq is the Gegenbauer polynomial, and ζk, k “ 1, 2, 4 are fixed vectors that are inde-
pendently generated from uniform distribution on unit sphere in Ri0 in our experiments. Note that
according to the addition formula XN“pd,k)Ykj(X)Ykj (y) = N(d, k)Pk(〈x, y〉)，every normalized
Gegenbauer polynomial is a spherical harmonic, so f*(x) is a linear combination of spherical har-
monics of order 1,2 and 4. The higher odd-order Gegenbauer polynomials are omitted because the
spectral analysis showed that μk = 0 for k = 3, 5, 7....
Following the setting in section 3.1, we denote Vk = n´1/2(Pk(Xi),..., Pk(Xn)). By Lemma 3.2
vk ’s are almost orthonormal with high probability. So we can define the (approximate) projection
length of residual rptq onto vk at step t as
pak = |vkJrptq|,
where rptq = (f*(xι)-θfWpt)(Xι),...,f*(xn)-θfWpt) (Xn)) and fwptq (x) is the neural network
function.
Remark 4.1. Here apk is the projection length onto an approximate vector. In the function space, we
can also project the residual function r(X) = f* (X) ´ θfWpt) (X) onto the orthonormal Gegenbauer
functions Pk (X). Replacing the training data with randomly sampled data points Xi can lead to a
random estimate of the projection length in function space. We provide the corresponding result for
freshly sampled points in Appendix E.1.
(a) components with the same scale
(b) components with different scale
Figure 1: Convergence curve for projection length onto different components. (a) shows the curve
when the target function have different component with the same scale. (b) shows the curve when
the higner-order components have larger scale. Both illustrate that the lower-order components are
learned faster. Log-scale figures are shown in Appendix E.2.
The results are showned in Figure 1. It can be seen that at the beginning of training, the residual at
the lowest frequency (k = 1) converges to zero first and then the second lowest (k = 2). The highest
frequency component is the last one to converge. Following the setting of Rahaman et al. (2018) we
assign high frequencies a larger scale, expecting that larger scale will introduce a better descending
speed. Still, the low frequencies are regressed first.
7
Under review as a conference paper at ICLR 2020
4.2	Learning functions of simple form
Apart from the synthesized low frequency function, we also showed the dynamics of normal func-
tions’ projection to Pk pxq. These functions, though in a simple form, have non-zero components in
almost all frequencies. In this subsection we further show our results still apply when all frequencies
exist in the target function, which is given by f *(x) “ Xi cos(a£Z, x〉) or f *(x) “ X<Z, Xypi,
where ζ is a fixed unit vector. The coefficients of given components are calculated in the same way
as in Section 4.1.
(a) cosine function along one direction
(b) even polynomial along one direction
Figure 2: Convergence curve for different components. (a) shows the curve of a trigonometric
function. (b) shows the curve of a polynomial with even degrees. Both exhibits similar tendency as
combination of spherical harmonics.
Figure 2 shows that even for arbitrarily chosen functions of simple form, the networks can still first
learn the low frequency components of the target function. Notice that early in training not all the
curves may descend, we believe this is due to the unseen components’ influence on the gradient.
Again, as the training proceeds, the convergence is controlled at the predicted rate.
Remark 4.2. The reason why we only use cosine function and even polynomial is that the only odd
basis function with non-zero eigenvalue is P1 px). To show a general tendency it is better to restrict
the target function in the even function space.
5 Conclusion and Discussion
In this paper, we give theoretical justification for spectral bias through a detailed analysis of the
convergence behavior of two-layer neural networks with ReLU activation function. We show that
the convergence of gradient descent in different directions depends on the corresponding eigenval-
ues and essentially exhibits different convergence rates. We show Mercer decomposition of neural
tangent kernel and give explicit order of eigenvalues of integral operator with respect to the neural
tangent kernel when the data is uniformly distributed on the unit sphere Sd . Combined with the
convergence analysis, we give the exact order of convergence rate on different directions. We also
conduct experiments on synthetic data to support our theoretical result.
So far, we have considered the upper bound for convergence with respect to low frequency com-
ponents and present comprehensive theorem to explain the spectral bias. One desired improvement
is to give the lower bound of convergence with respect to high frequency components, which is es-
sential to establish tighter characterization of spectral-biased optimization. It is also interesting to
extend our result to other training algorithms like Adam, where the analysis inWu et al. (2019); Zhou
et al. (2018) might be implemented with a more careful quantification on the projection of residual
along different directions. Another potential improvement is to generalize the result to multi-layer
neural networks, which might require different techniques since our analysis heavily rely on exactly
computing the eigenvalues of the neural tangent kernel. It is also an important direction to weaken
the requirement on over-parameterization, or study the spectral bias in a non-NTK regime to furthur
close the gap between theory and practice.
8
Under review as a conference paper at ICLR 2020
References
Allen-Zhu, Z., Li, Y. and Song, Z. (2018). A convergence theory for deep learning via over-
parameterization. arXiv preprint arXiv:1811.03962 .
Andoni, A., Panigrahy, R., Valiant, G. and Zhang, L. (2014). Learning polynomials with
neural networks. In International Conference on Machine Learning.
Arora, S., Du, S. S., Hu, W., Li, Z., Salakhutdinov, R. and Wang, R. (2019a). On exact
computation with an infinitely wide neural net. arXiv preprint arXiv:1904.11955 .
ARORA, S., DU, S. S., HU, W., LI, Z. and WANG, R. (2019b). Fine-grained analysis of op-
timization and generalization for overparameterized two-layer neural networks. arXiv preprint
arXiv:1901.08584.
ATKINSON, K. and HAN, W. (2012). Spherical harmonics and approximations on the unit sphere:
an introduction, vol. 2044. Springer Science & Business Media.
BACH, F. (2017). Breaking the curse of dimensionality with convex neural networks. Journal of
Machine Learning Research 18 1-53.
Basri, R., Jacobs, D., Kasten, Y. and Kritchman, S. (2019). The convergence rate of neural
networks for learned functions of different frequencies. arXiv preprint arXiv:1906.00425 .
BIETTI, A. and MAIRAL, J. (2019). On the inductive bias of neural tangent kernels. arXiv preprint
arXiv:1905.12173.
Cao, Y. and Gu, Q. (2019a). Generalization bounds of stochastic gradient descent for wide and
deep neural networks. arXiv preprint arXiv:1905.13210 .
Cao, Y. and Gu, Q. (2019b). A generalization theory of gradient descent for learning over-
parameterized deep relu networks. arXiv preprint arXiv:1902.01384 .
Chizat, L. and Bach, F. (2018). A note on lazy training in supervised differentiable programming.
arXiv preprint arXiv:1812.07956 .
CHO, Y. and SAUL, L. K. (2009). Kernel methods for deep learning. In Advances in neural
information processing systems.
Collobert, R. and Weston, J. (2008). A unified architecture for natural language processing:
Deep neural networks with multitask learning. In Proceedings of the 25th international confer-
ence on Machine learning. ACM.
DU, S. S., LEE, J. D., LI, H., WANG, L. and ZHAI, X. (2018a). Gradient descent finds global
minima of deep neural networks. arXiv preprint arXiv:1811.03804 .
Du, S. S., Zhai, X., Poczos, B. and Singh, A. (2018b). Gradient descent provably optimizes
over-parameterized neural networks. arXiv preprint arXiv:1810.02054 .
FRYE, C. and EFTHIMIOU, C. J. (2012). Spherical harmonics in p dimensions. arXiv preprint
arXiv:1205.3548.
Gunasekar, S., Lee, J., Soudry, D. and Srebro, N. (2018a). Characterizing implicit bias in
terms of optimization geometry. arXiv preprint arXiv:1802.08246 .
Gunasekar, S., Lee, J., Soudry, D. and Srebro, N. (2018b). Implicit bias of gradient descent
on linear convolutional networks. arXiv preprint arXiv:1806.00468 .
He, K., Zhang, X., Ren, S. and Sun, J. (2015). Delving deep into rectifiers: Surpassing human-
level performance on imagenet classification. In Proceedings of the IEEE international confer-
ence on computer vision.
He, K., Zhang, X., Ren, S. and Sun, J. (2016). Deep residual learning for image recognition.
In Proceedings of the IEEE conference on computer vision and pattern recognition.
9
Under review as a conference paper at ICLR 2020
Hinton, G., Deng, L., Yu, D., Dahl, G., Mohamed, A.-r., Jaitly, N., Senior, A., Van-
houcke, V., Nguyen, P., Kingsbury, B. et al. (2012). Deep neural networks for acoustic
modeling in speech recognition. IEEE Signal processing magazine 29.
Jacot, A., Gabriel, F. and Hongler, C. (2018). Neural tangent kernel: Convergence and
generalization in neural networks. arXiv preprint arXiv:1806.07572 .
Li, Y. and Liang, Y. (2018). Learning overparameterized neural networks via stochastic gradient
descent on structured data. arXiv preprint arXiv:1808.01204 .
Nakkiran, P., Kaplun, G., Kalimeris, D., Yang, T., Edelman, B. L., Zhang, F. and
BARAK, B. (2019). Sgd on neural networks learns functions of increasing complexity. arXiv
preprint arXiv:1905.11604 .
Rahaman, N., Baratin, A., Arpit, D., Draxler, F., Lin, M., Hamprecht, F. A., Ben-
GIO, Y. and COURVILLE, A. (2018). On the spectral bias of neural networks. arXiv preprint
arXiv:1806.08734.
ROSASCO, L., BELKIN, M. and VITO, E. D. (2010). On learning with integral operators. Journal
ofMachine Learning Research 11 905-934.
Smale, S. and Zhou, D.-X. (2007). Learning theory estimates via integral operators and their
approximations. Constructive approximation 26 153-172.
SMALE, S. and ZHOU, D.-X. (2009). Geometry on probability spaces. Constructive Approximation
30 311.
S oudry, D., Hoffer, E. and Srebro, N. (2017). The implicit bias of gradient descent on sepa-
rable data. arXiv preprint arXiv:1710.10345 .
Su, L. and Yang, P. (2019). On learning over-parameterized neural networks: A functional ap-
proximation prospective. arXiv preprint arXiv:1905.10826 .
Vempala, S. and Wilmes, J. (2018). Gradient descent for one-hidden-layer neural networks:
Polynomial convergence and sq lower bounds. arXiv preprint arXiv:1805.02677 .
Wu, X., Du, S. S. and Ward, R. (2019). Global convergence of adaptive gradient methods for an
over-parameterized neural network. arXiv preprint arXiv:1902.07111 .
Xu, Z. J. (2018). Understanding training and generalization in deep learning by fourier analysis.
arXiv preprint arXiv:1808.04295 .
YANG, G. and SALMAN, H. (2019). A fine-grained spectral perspective on neural networks. arXiv
preprint arXiv:1907.10599 .
Zhang, C., Bengio, S., Hardt, M., Recht, B. and Vinyals, O. (2016). Understanding deep
learning requires rethinking generalization. arXiv preprint arXiv:1611.03530 .
Zhou, D., Tang, Y., Yang, Z., Cao, Y. and Gu, Q. (2018). On the convergence of adaptive
gradient methods for nonconvex optimization. arXiv preprint arXiv:1808.05671 .
Zou, D., Cao, Y., Zhou, D. and Gu, Q. (2018). Stochastic gradient descent optimizes over-
parameterized deep relu networks. arXiv preprint arXiv:1811.08888 .
Zou, D. and Gu, Q. (2019). An improved analysis of training over-parameterized deep neural
networks. arXiv preprint arXiv:1906.04688 .
10
Under review as a conference paper at ICLR 2020
A Review on spherical harmonics
In this section, we give a brief review on relevant concepts in spherical harmonics. For more detials,
see Bach (2017), Bietti and Mairal (2019), Frye and Efthimiou (2012) and Atkinson and Han (2012)
for references.
We consider the unit sphere Sd “ {x P Rd'1 : }x}2 “ 1}, whose surface area is given by ωd “
2∏pd'ιq{2{Γppd ` 1)/2) and denote τd the uniform measure on the sphere.
For any k 2	1, We consider a set of spherical harmonics
{Yk,j : Sd → R|1 ≤ j ≤ N(d,k) = 2k`dT (k'´´2)). They form an orthonormal basis and
satisfy the following equation xYki, Ysj ySd “ Sd Yki px)Ysj px)dτdpx) “ δij δsk . Moreover, since
they are homogeneous functions of degree k, it is clear that Yk px) has the same parity as k.
We have the addition formula
N pd,kq
X Yk,j(X)Ykj(y) = N(d,k)Pk(〈x,y〉)，	(A.1)
j“1
where Pk(t) is the Legendre polynomial of degree k in d ` 1 dimensions, explicitly given by (Ro-
drigues’ formula)
Pkpt)=(´ 2 j ⅛‰ '1 ´ 巧宁(d )k(1-巧kk 中
We can also see that Pk(t), the Legendre polynomial of degree k shares the same parity with k. By
the orthogonality and the addition formula (A.1) we have,
Pj pxw, Xy)Pk(Xw, yy)dτd(w) = Njk Pk(Xχ, y〉).	(Az
Sd	N(d, k)
Further we have the recurrence relation for the Legendre polynomials,
k	k`d 1
tPk(t)=]Pι(t) + 9,' , 1 Pk'i(t),	(A.3)
2k ` d ´ 1	2k ` d ´ 1
for k21 and tP0(t) = Pi (t) for k = 0.
The Hecke-Funk formula is given for a spherical harmonic Yk of degree k
Ja f (〈X, y〉)Yk(y)dt1d(y) = ω´Yk(X)J f(t)Pk(t)(1 ´ t2)p/2q/2dt.
B Proof of Main Theorems
B.1 Proof of Theorem 3.2
In this section we give the proof of Theorem 3.2. The core idea of our proof is to establish connec-
tions between neural network gradients throughout training and the neural tangent kernel. To do so,
we first introduce the following definitions and notations.
Define Kp0q = m´1 (XVwfwpoq (Xi), YWfWp0)(Xj)>)n^n, Kp8q = (κ(Xi, Xj))n^n =
limm_8 Kp0q. Let {pi}∖ι, pi》…》Pn be the eigenvalues of n—1K8, and Vi,..., Vn be
the corresponding eigenvectors. Set Vrk = (vι,.∙∙, Vrk), VK = (vrk'i, ∙∙∙, Vn).
For notation simplicity, we denote VWfwpoq(Xi) = [Vwfw(Xi)SlW=Wp0), VWlfWpoq(Xi)=
rvWifW (Xi)SlW=Wp0), l = 1, 2.
The following lemma is a direct application of Proposition 1 in Smale and Zhou (2009) or Proposi-
tion 10 in Rosasco et al. (2010). It bounds the difference between the eigenvalues of NTK and their
finite-width counterparts.
11
Under review as a conference paper at ICLR 2020
Lemma B.1. For any δ > 0, with probability at least 1 一 δ,
∣λi ´ pi∣ ≤ O(aiog(1∕δ)∕n).
The following lemma is partly summarized from the proof of equation (44) in Su and Yang (2019).
Its purpose is to further connect the eigenfunctions of NTK with their finite-width, finite-sample
counterparts.
Lemma B.2. Suppose that ∣φi(x)∣ ≤ M for all X P Sd. There exist absolute constants C, C1, c2 >
0, such that for any δ > 0 and integer k with r《 ≤ n, if neC(λrk — lrk`i)´2 log(1∕δ), then with
probability at least 1 ´ δ,
}vJk V Kk }f ≤ C1 ʌ_1——J1og1B,
k k	λrk — λrk'i V n
}VrkV； ´ VrkVJ= }2 ≤ C2 „ʌ-1-——CogpW + M2rkCIogprk/δq .
lʌk — λr= '1	V n	V n J
The following two lemmas are key to characterize the dynamics of the residual throughout training.
These two results, especially Lemma B.4, are the ones that distinguish our analysis from previous
works on neural network training in the NTK regime (Du et al., 2018b; Su and Yang, 2019), since
our analysis provides more careful characterization on the residual along different directions.
Lemma B.3. Suppose that the iterates of gradient descent Wp0q , . . . , Wptq are inside the ball
B(Wp0q, ω). If ω ≤ O([1og(m)S-3*),then with probability at least 1 — O(n2)∙ exp[—Ω(mω>3)],
y — ppt1'ιq = [I — (ηmθ2∕n)K8S(y — ppt'q) + eptq, }ep'q}2 ≤ O(ω1/3ηmθ2) ∙ }y — ypt1q}2
for all t1 = 0,...,t — 1, where y = (yι,..., yn)J, ppt1q = θ ∙ (fwp2 (xι),…，fw(2 (Xn))J
Lemma B.4. Suppose that the iterates of gradient descent Wp0q , . . . , Wptq are inside the ball
B(Wp0q, ω). If ω ≤ θ(mint[log(m)s´^2, λ3ζ, (nm)—3}) and neO(λ[2), then with probability
at least 1 — O(t2n2) ∙ exp[—Ω(mω2/3)S,
}(VKk)τ(y — ypt1q)}2 ≤ }(VKk)τ(y — yp0q)}2 + t1 ∙ ω1{{3ηmθ2 ∙ ?n ∙ O(1 + 3?m)	(B.1)
}VJk(y — ypt'q)}2 ≤ (1 — nmθ2λrk/2qt1 },VJk(y — yp0qq}2 + t1λk1 ∙ ω2{3nmθ2 ∙ ?n∙O(I + ω√mq
+ λ] ∙ O(ω1/3) ∙ }(VKk)τ(y — yp0q)}2	(B.2)
}y — yptlq}2 ≤ O(?n) ∙ (1 — nmθ2λrk∕2)t1 + O(?n ∙ (nmθ2λrk)T)
+ Xryω'1{ ∙ ?n ∙ O(1 + 3?mq	(B.3)
for all t1 = 0, . . . , t — 1.
Now we are ready to prove Theorem 3.2.
Proofof Theorem 3.2. Define ω = CT,(1^ ?m) for some small enough absolute constant C.
Then by union bound, as long as m》Ω(poly(T, l´1, e 1)), the conditions on ω given in Lem-
mas D.2, D.4, D.5, B.3 and B.4 are all satisfied.
We first show that all the iterates Wp0q , . . . , WpTq are inside the ball B(Wp0q , ω). We prove this
result by inductively show that Wptq P B(Wp0q, ω), t = 0, . . . , T. First of all, itis clear that Wp0q P
B(Wp0q, ω). Suppose that Wp0q, . . . , Wptq P B(Wp0q, ω). Then the results of Lemmas D.2, D.4,
12
Under review as a conference paper at ICLR 2020
D.5, B.3 and B.4 hold for Wp0q, . . . , Wptq. Denote uptq “ y ´ ypptq, t P T. Then we have
t
}wpt'1q ´ Wp0q}f & £ }wpt,'1q ´ Wq}F
t1“0
t
“ η ∑
t1“0
1n
_ Σpyi ´ θ ' fWPtq pxiqq ' θ ∙ ^Wι fWPtq PXiq
nl
i“1
F
t 1n
≤ ηθ £ — £ |yi ´ θ ∙ fwptq(χi)∣∙ 1尸0"0(,)(g)}尸
t1“0 n i“1
tn
W Cιηθ√m £ - £ |yi ´
t1“0 n i“1
θ ∙ fWptq (xiq|
t
≤ Cιηθam{n £ }y — ppt'q}2,
t1“0
where the second inequality follows by Lemma D.4. By Lemma B.4, we have
t
£ }y -yptq}2 ≤ Op√n∕(ηmθ2λrk)) + O(T√n{(ηmθ2λrkqq + 入lT2ω1{3√n∙ Opi + ω√mq.
t1“0
It then follows by the choice ω = CT/(θλrk√mq, η “ O(Pmθ21^)—1), θ “ Ope) and the
assumption meO(Poly(T, l´1, e´1)) that ∣∣Wpt'1q — Wp0q}F ≤ ω, l “ 1, 2. Therefore by
induction, we see that with probability at least 1 —O(T3n2)∙exp[-Ω(mω2∕3)S, W(0),..., W(T) P
B(Wp0q, ωq .
Applying Lemma B.4 then gives
nτ{2 ∙ }V J (y ´ ypT qq}2 ≤ (1 ´ ηmθ2λrk/2)T ∙nτ/2 ∙ }V J (y ´ yp0qq12
+ Tλ" ∙ ω2{3ηmθ2 ∙ O(1 + ω√m)
+ λ―1∙ O(ω1∕3)∙nT2 ∙}(V⅛)J(y — pp0qq}2.
Now by ω = CT/(1以√m), η “ <O(θ2m)-1 and the assumption that m》m* “ O(λ.14 ∙ e´6),
we obtain
n-1{2 ∙ }VJk (y — P(Tq)∣2 W (1 - λrk)T ∙ n-1{2 ∙ }VJk (y — pp0qq12 + e∕16.	(B.4)
ByLemma 3.1, θ “ Opeq and the assumptions n》ΩPmaXte-1(1^ — 1^ + 1)-1, e-2M2rk}), the
eigenvalues of VJ Vrk are all between 1/√2 and √2. Therefore by Lemma B.2 We have
}V Jk (y - P(T q)}2 “}V rk V Jk (y - PpTq)}2
J }Vrk VJ (y - P(T q)∣2 - }(VrkVJ= — V rk V J= )(y - y(T q)∣2
> }VJk(y - P(Tq)}2∕√2-√n ∙ O ^λk⅛ ∙ cl°g≡ + MrkC^)
J }VJs(y - P(Tq)∣2∕√2 - e√n∕16.
Similarly,
}VJ= (y - y(0))}2 ≤√2 ∙}VJ(y - y(0))}2 + e√n∕16
W √2 ∙ }VJky}2 + √2 ∙ }VJky(0)}2 + e√n∕16.
By Lemma 3.1, with probability at least 1 — δ, }VJ ∣∣2 W 1 + CrkM2ʌ/log(rk∕δ)∕n. Combining
this result with LemmaD.3 gives }VJ；P(0q}2 W θO(ʌ/nlog (n∕δ)) W e√n∕8. Plugging the above
estimates into (B.4) gives
n-1{2 ∙ }VJk (y - P(Tq)}2 W 2(1 - λrk)T ∙ n-1{2 ∙ }VJyn2 + e.
Applying union bounds completes the proof.	□
13
Under review as a conference paper at ICLR 2020
B.2 Proof of the Theorem 3.4
Proof of the Theorem 3.4. The idea of the proof is close to that of Proposition 5 in (Bietti and Mairal,
2019) where they consider k " d and we present a more general case including k " d and d " k.
For any function g : Sd → R,by denoting g°(x) “ ,&d g(y)dτd(y), it can be decomposed as
8
g(xq “	gk (xq “
k“0
8 Npd,kq
Σ Σ
k“0 j“1 Sd
Ykj (yqYkj (xqg(yqdτd(yq
8
“ Σ N (d, kq	g(yqPk (xx, yyqdτd (yq,
k“0	Sd
(B.5)
where we project function g to spherical harmonics in the second equality and apply the addition
equation in the last equality.
For a positive-definite dot-product kernel κ(x, x1q :Sd ^ Sd → R which has the form κ(x, x1q “
pκ(xx, x1yq for κp : r´1, 1s → R, we can present a decomposition by (B.5) if we consider g(xq “
φ(xx, zyq for z P Sd and φ : r´1, 1s → R,
8
κ(x, x1q “ Σ N (d, kq	κp(xy, x1yqPk (xy, xyqdτd(yq
k“0	Sd
8
“ Σ N(d, kq
k“0
1
端 Pkpχx, x1yq L
p(t)Pk(t)(i ´ t2qpd-2q{2dt,
where we apply the Hecke-Funk formula and addition formula. By denoting λk	“
(ωd-ι{ωdq '-1 P(t)Pk(t)(1 — t2)(d-2q∕2dt and the addition formula, we have
8	8 N pp,kq
κ(x, x1q “ ∑ μk N (d,k)Pk(〈x, x1〉)“ ∑ μk ∑ Ykj (X)Kj (x1).	(B.6)
k“0	k“0	j“1
This formula (B.6) is the Mercer decomposition for the kernel function κ(x, x1) and μk is exactly
the eigenvalue of the integral operator LK on L2 (Sd) defined by
LKpf)(y)“LK(x,y)f(x)dTd(x), fP L2(Sd).
By using same technique as κ(x, x1), we can derive a similar expression for σ(xw, x〉) “
max {〈w, x〉，0} and σ1(<w, x〉) “1t〈w, x〉> 0}, since they are essentially dot-product func-
tion on L2(Sd). We deliver the expression below without presenting proofs.
8
σ1(Xw, x〉)“ ∑ βι,kN(d, k)Pk(〈w, x〉)，	(B.7)
k“0
8
σ(Xw, x〉)“ ∑ β2,kN(d, k)Pk(〈w, x〉)，	(B.8)
k“0
where βι,k = (ωd-1∕ωd)匕 σ(t)Pk(t)(1 — t2)pd-2q/2dt and β2,k = (ωd-1∕ωd)匕 σ1 (t)Pk(t)(1 —
t2)pd-2q{2dt. We add more comments on the values of β1,k and β2,k. It has been pointed out in Bach
(2017) that when k > a and when k and α have same parity, We have βα'ι,k = 0. ThiS is because
the Legendre polynomial Pk (t) is orthogonal to any other polynomials of degree less than k with
respect to the density function p(t) “ (1 — t2)pd-2q{2 . Then we clearly know that β1,k “ 0 for
k “ 2j and β2,k = 0 for k = 2j ' 1 with j P N'.
For two kernel function defined in (2.2), we have
κι(x,x1) “ Ew„Npo,ιq [σ1(<w,x>)σ1(Xw,x1>)‰
“ Ew„NP0,iq [σ1(xw∕ }w} , x〉)b1(〈w/ }w} , x1〉)]
“ J401(〈v，x>)σ'(Xv, x1>)dτd(v).
(B.9)
14
Under review as a conference paper at ICLR 2020
The first equality holds because σ1 is 0-homogeneous function and the second equality is true since
the normalized direction of a multivariate Gaussian random variable satisfies uniform distribution
on the unit sphere. Similarly we can derive
κ2px, x1q “ pd ` 1q	σpxv, xyqσpxv, x1yqdτd pvq.	(B.10)
Sd
By combing (A.2), (B.7), (B.8), (B.9) and (B.10), we can get
8
κι(χ,χ1 q “ £ β2,kNpd, kqPkpxx, χ1yq,	(b.ii)
k“0
and
8
κ2 (χ, χ1q “ (d` 1q £ β22,k N (d, kqPk(xχ, χ1yq.	(B.12)
k“0
Comparing (B.6), (B.11) and (B.12), we can easily show that
μι,k “ β1,k and μ2,k =(d + 1)β2,k.	(B.13)
In Bach (2017), explicit expressions for βι,k and β2,k for k》α + 1 are presented by
βa'1,k “
d ´ 1 a!(—1)PkT—aq{2	Γ(d∕2)Γ(k ´ α)
2π
2k
Γ(k´a'l )γ(k'd'α'1q .
By Stirling formula Γ(x) « XxT{2e—x√2∏, we have following expression of βa'i,k for k》α + 1
βα'i,k = C (α)
(d ´ 1)dd´1 (k ´ a)k´a´2
(k ´ α + 1) ⅛α (k + d + α + 1)
k'd'a
2
(d ' 1 k — α — 1	—k — d — α \
dF k ~^2~ (k + d) —2 —)
?a!	d´1 γ( ' )γ(dq
Where	CPaq	=	2∏	exptα	十 1u.	AlSO	βα'1,0	=	-4∏	γ( d'α'2q~,	β1,1	=
Γ( 1 )Γ( ⅛2 )
矣 and β2,1 =	d´d Γ(42 . ThUS combine (B.13) we know that μa'i,k	=
Ω 'dd'1'αkkrT(k + d)´k´d´a).
By considering (A.3) and (B.6), we have
μo = μ1,1 + 2μ2,0,
μkl = 0, k1 = 2j + 1, j P N',
and
k	k + d — 1
μk = 2k + d — 1 μ1,kτ + 2k + d — 1 μ1,k+1 + 2〃邛，
for k21 and k ‰ k1. From the discussion above, we thus know exactly that for k21
μk = Ω 'max {dd'1kkT(k + d)´k´d, dd'1kk(k + d)iττ,dd'2kk∙(k + d)”"1().
This finishes the proof.	□
B.3 Proof of the Corollaries 3.7 and 3.8
Proofofthe Corollaries 3.7 and 3.8. We only need to bound ∣φj(x)| for j P [rk] to finish the proof.
Since now we assume input data follows uniform distribution on the unit sphere Sd, φj (χ) would be
spherical harmonics of order at most k for j P rrks. For any spherical harmonics Yk of order k and
any point on Sd, we have an upper bound (Proposition 4.16 in Frye and Efthimiou (2012))
IYk(x)| ≤
(N (d，k) L
1
Yk2(y)dTd(yf)
Thus we know that ∣φj(x)∣ ≤ {N(d, k). For k " d, we have N(d, k) = 2k'dT 'k'´´2)=
O(kdT). For d " k, we have N(d, k) = 2%-1 'k'´´2) = O(dk). This completes the proof. □
15
Under review as a conference paper at ICLR 2020
C Proof of Technical Lemmas
C.1 Proof of Lemma B.2
Proof of Lemma B.2. The first inequality directly follows by equation (44) in Su and Yang (2019).
To prove the second bound, we write Vrk = Vrk A + VK B,where A P Rrk Xrk, B P RPn´rkq'rk.
Let ξι “ C 1(λrk — λrk + ι)T ∙ aiog(1∕δ)∕n, ξ “ C3Mr∖ogg{δ){n be the bounds given in
the first inequality and Lemma 3.1. By the first inequality, we have
}B}f “ }Bj}f “ }VJkVKjF ≤ ξι.
Moreover, since VrJ Vrk “ AJA ` BJB, by Lemma 3.1 we have
}aaj —1}2 “ }aja —1}2 ≤ }VJVrk —1}2 + }bjb}2 ≤『底2 + ξ2.
Therefore
}VrkVrJk — VprkVprJk}2
“ }VprkAAJVprJk +VprkABJpVprKkqJ+VprKkBAJVprJk +VprKkBBJpVprKkqJ—VprkVprJk}2
≤ }Vrk (AAJ — DVJ }2 + O(}B}2)
“ }AAJ — I}2 + Op}B}2q
≤ O(rkξ + ξιq
Plugging in the definition of ξι and ξ2 completes the proof.	□
C.2 Proof of Lemma B.3
Proof of Lemma B.3. The gradient descent update formula gives
2η n
Wpt'1q “ WPtq + — ∑(yi — θfwptq (Xi))∙网Wfwptq (xi)∙	(C.1)
i“1
For any j P [n], subtracting WPt) and applying inner product with θVwfwptq (Xj) on both sides
gives
θχvwfw(t)(χjq, WPt'ιq — Wptqy “ ^n- X(yi — PPt))YVWfw(t)(χjq, VWfw(t)(χi)y.
n
i“1
Further rearranging terms then gives
yj —	(PPt'1qqj =	yj	— (PPt)qj	η- X (yi	—	θfWptq (Xi))• K8j +	I1,j,t +	I2,j,t + I3,j,t,
n i“1
(C.2)
where
I1,j,t =---- X (yi — θfWptq (Xi))' KVWfWptq (xjq, VWfWptq(Xi)〉— mκP0)s,
n i“1
I2,j,t “ — 2ηmθ2 X (yi — θfWptq (Xi)) ∙(Ki0q — κ∞ ),
n
i“1
I3,j,t “ —θ , rfWpt'ιq (Xj) — fWptq (Xj) —〈VWfWptq (Xj), WPt'1q — wPt)〉s.
For I1,j,t, by Lemma D.6, we have
1 n
∣I1,j,t∣ ≤ O(ω1{3ηmθ2) ∙ n 4 |yi — θfWptq (Xi)| ≤ O(ω1/3ηmθ2)∙}y — yPt)}2/?n
16
Under review as a conference paper at ICLR 2020
with probability at least 1 — O(n)∙ exp]一。(μ力2{3)]. For I2,j,t, by Bemstein inequality and union
bound, with probability at least 1 — O(n2)∙ exp(一Ω(mω%3)), We have
∣K8∙ — Kp0jq∣≤ opω1{3q
for all i, j P rns. Therefore
1 n
∣I2,j,t∣ W O(ω1/3ηmθ2) ∙ n 与 |yi — θfw(tq(xi)∣ W OPω1{{3ηmθ2) ∙ }y — pptq}2∕?^
For I3,j,t, we have
I3,j,t W O(ω1/") ∙ }w1t'1q — wPtq}2
~,…一 .2η Λ .	........ ...
W O(ω / ?m。)∙ — £ |yi — θfwptq(xiq| ∙ θ ∙ }VwιfwPtq (χi)}2
i“1
1n
W O(ω1/3ηmθ2) ∙ £ £ |yi — θfwptq (xi)|
W O(ω1/3ηmθ2) ∙ }y — pptq}2∕√n,
where the first inequality follows by Lemmas D.2, the second inequality is obtained from (C.1), and
the third inequality follows by Lemma D.4. Setting thej-th entry of eptq as I1,j,t ` I2,j,t ` I3,j,t and
writing (C.2) into matrix form completes the proof.	□
C.3 Proof of Lemma B.4
Proof of Lemma B.4. Denote uptq “ y — ypptq, t P T. Then we have
}(VK)Jupt1'1q}2 W }(VK)J[I -(ηmθ2∕n)K8Supt1q}2 ' Or(ω1{3ηmθ2)∙}upt1q}2
W }(VKk)Juptlq}2 ' O(ω'123ηmθ)∙ ? ∙ O(1 ' ω?m),
where the first inequality follows by Lemma B.3, and the second inequality follows by Lemma D.5.
Therefore we have
}(VKk)Juptlq}2 W }(VKk)Jup0q}2 ' t1 ∙ ω1{3ηmθ2 ∙ ?n ∙ O(1 ' ω√m),
for t1 “ 0, . . . , t. This completes the proof of (B.1). Similarly, we have
}V J Upt1 'iq}2 W }V J riTηmθ2∕n)K8Supt1q}2 + O(ω1/3ηmθ2) ∙ }upt1q}2
W (1 — ηmθ2prk)}VJupt1q}2 + O(ω%mθ2) ∙(}VJu(.2 + }(VK)jupt1q}2)
W (1 — ηmθ2λrk ∕2)}Vp rJk upt1q}2 + Or(ω1{3ηmθ2) ∙ }(VprKk)Jupt1q}2
W (1 — ηmθ2λrk∕2)}VJkUpt'qg + t1 ∙ (ω1{3ηmθ2)2 ∙ √n ∙ O(1 + ω√^)
+ Or(ω1{3ηmθ2) ∙ }(VprKk)Jup0q}2
for t1 “ 0, . . . , t — 1, where the third inequality is by Lemma B.1 and the assumption that ω W
<O(λ3fc), neO(λ[2), and the fourth inequality is by (B.1). Therefore we have
}VJCUptlq}2 W(1 — ηmθ2λrk∕2)t1}VJCUp0q}2 + t1 ∙ (ηmθ2λrk∕2)^1 ∙ (ω1{3ηmθ2)2 ∙ √n ∙ O(1 + ω√m)
+ (ηmθ2λrk∕2)T ∙ O(ω16ηmθ2)∙ }(VK)Tu(0q}2
“ (1 — ηmθ2λrk∕2)tι }^VJCUp0q}2 + "´1 ∙ ω2{3ηmθ2 ∙ √n ∙ O(1 + ω√m)
+ λ"∙ O(ω*)∙}(VKk)Jup0q}2
W (1 — ηmθ2λrk∕2)t1}VJCUp0q}2 + ”´1 ∙ ω2{3ηmθ2 ∙ √n ∙ O(1 + ω√m)
+ λ"∙ O(ω*)∙}(VKk)JUp0q}2.
17
Under review as a conference paper at ICLR 2020
This completes the proof of (B.2). Finally, for (B.3), by assumption we have ω1{3ηmθ2 ≤ Orpiq.
Therefore
}upt1'1q}2 w }[I -(ηmθ2∕n)K8]Vr,%VJupt1q∣∣2 + }[I -(ηmθ2/n)K8SVrK(VK)3以
+ OM3ηmθ2) ∙ }VJupt1q}2 + O(ων3ηmθ2) ∙ }(VK)Jupt1q}2
≤(1 ´ ηmθ2p,k)}VJupt1q}2 + O(ω1∕3ηmθ2) ∙ }VJupt1q∣∣2 + Or⑴∙ }(VK)3}2
≤(1 ´ ηmθ2λ,k∕2)} V Jk upt1q }2 + O(1)∙ }(VK JUptIq} 2
≤ (1 ´ ηmθ2λ,k∕2)}VJkupt1q}2 + Or⑴∙ }(VKk Jup0q}2 + t1ω1/3ηmθ2? ∙O(1 + ω?^)
for t1 “ 0, . . . , t ´ 1, where the third inequality is by Lemma B.1 and the assumption that ω w
Or(λr3 q, and the fourth inequality follows by (B.1). Therefore we have
}upt1q}2 W O(?n)∙(1 ´ ηmθ2λrfc∕2)t1 + Or((ηmθ2λJfc)´1) ∙ }(VK Jup0q}2 + ∖g{? ∙ Or(1 + ω"
This finishes the proof.	□
D Auxiliary Lemmas
In this section we list several auxiliary lemmas on the properties of over-parameterized neural net-
works we need in our proof of Theorem 3.2. These results are mostly summarized from Allen-Zhu
et al. (2018) and Cao and Gu (2019a).
D.1 Auxiliary Lemmas
Denote
Di “ diag'l{(W1Xi)1 > 0},..., 1{(WιXi)m > 0}),
Di0q “ diag' 1{(Wp0qxi)ι > 0},...,1{(Wp0qxi)m > 0}).
Lemma D.1 (Allen-Zhu et al. (2018)). If ω ≤ θ([log(m)]´3*), then with probability at least
1 ´ O(n) ∙ exp[—Ω(mω>3)S,
}Di ´ Dip0q}0 w O(ω2{3mq
for all W P B(Wp0q, ωq, i P rns.
Lemma D.2 (Cao and Gu (2019a)). There exists an absolute constant κ such that, with probability
at least 1 — O(n) ∙ exp[—Ω(mω26)] over the randomness of Wpιq, for all i P [n] and W, W1 P
B(Wp0q,ω) with ω ≤ K[log(m)S-3{2, it holds uniformly that
Ifwi(Xi) — fw(Xi) — XVwfw(xi), W1 — W〉| ≤ O(s1{3amlog(m)) ∙ }W1 — W1}2.
Lemma D.3 (Cao and Gu (2019a)). For any δ > 0, if meC log(n∕δ) for a large enough absolute
constant C, then with probability at least 1 — δ, Ifwpoq (xi)| ≤ O(ʌ∕log(n∕δ)) for all i P [n].
Lemma D.4 (Cao and Gu (2019a)). There exists an absolute constant C such that, with probability
at least 1 — O(n) ∙ exp[—Ω(mω26)], for all i P [n], l P [L] and W P B(Wp0q,ω) with ω ≤
C[log(m)]´3, it holds uniformly that
}Vwιfw(Xi)}F ≤ O(?m).
The following lemma provides a uniform bound of the neural network function value over
B(Wp0q, ω).
Lemma D.5. Suppose that m》Ω(ω-26 log(n∕δ)) and ω ≤ θ([log(m)]´3). Then with probabil-
ity at least 1 — δ, ∣fw(xi)∣ ≤ O(aiog(n∕δ) + ω√m) for all W P B(Wp0q,ω) i P [n].
Lemma D.6. If ω ≤ O([log(m)S—3{2),then with probability at least 1 — O(n)∙ exp[—Ω(mω>3)],
}Vwfw(xi) — Vwfwpoq (xi)}F ≤ O(ω1/3√m),
IxVw fw (xi ), Vwfw(xj)〉 — xVw fwp0q (xi), Vw fwp0q (xj )〉I w O(ω1{3m)
for all W P B(Wp0q, ω) andi P rns.
18
Under review as a conference paper at ICLR 2020
D.2 Proofs of Lemmas D.5 and D.6
Proof of Lemma D.5. By Lemmas D.2 and D.4, we have
lfW PXiq — fwpoq pxiqi ≤ ∣∣Vwi fwpoq pxiq}F }W1 — WpOq }F ' ∣∣Vw2 fwpoq pxiq}F }W2 — w20q }F
' OPω'1{∖m log(m)) ∙ }W1 — Wfq}2
≤ θPω?mq,
where the last inequality is by the assumption ω ≤ [log(m)]´3. Applying triangle inequality and
Lemma D.3 then gives
Ifw(Xi)Iw Ifwpoq(Xi)∣ + IfW(Xiq — fwpoq(Xi)| ≤ O(aiog(n∕δ)) ' O(ω?m)
“ O(aiog(n∕δ) ' ω?^),
This completes the proof.	□
Proof of Lemma D.6. By direct calculation, we have
VWIfWpOq (Xi) = ?m ∙ DioqWg0qjXJ, VWIfW(Xiq = ?m ∙ DiWJxJ.
Therefore we have
}vWιfw(Xi) ´ VWIfWpoq (Xiq}F = ?m ∙}DiWJXJ ´ DpO)W2°)JXJ } F
“?m ∙ }XiW2Di — XiWgoqDpoq}f
=?m ∙ }W2 Di — W2oqDpoq}F
≤ ?m ∙}W2oq(Dpoq — Diq∣F '?m ∙}(W2oq — W2)d/f
By Lemma 7.4 in Allen-Zhu et al. (2018) and Lemma D.1, with probability at least 1 — n ∙
exp[—Ω(mqS, ?m ∙ }W20q(Dpoq — Di)}f ≤ O(ω1/3?^ for all i P [n]. Moreover, clearly
∣(WpgOq — WgqDi ∣F w ∣WgpOq — Wg ∣F w ω. Therefore
}Vwιfw(Xiq — VWIfWpOq(Xiq∣F ≤ O(ω1/3?^
for all i P rns. This proves the bound for the first layer gradients. For the second layer gradients, we
have
Vw2fwp0q (Xiq = ?m ∙ rσ(Wl°qXiqSJ, Vw2fw(Xiq = ?m ∙ rσ(WιXiqsJ
It therefore follows by the 1-Lipschitz continuity of σ(∙q that
}vw2 fw (Xiq — vw2 fwpoq (Xiq∣F ≤ ?m ∙ |卬凶—WpOqXi}f < ω4m ≤ ω11{34m.
This completes the proof of the first inequality.
The second inequality directly follows by triangle inequality and Lemma D.4:
IxVw fw (Xi q, Vwfw(Xjqy — mKpOq I w IxVw fw (Xi q — Vwfwpoq (Xiq, Vw fw (Xj qyI
` IxVwfwpoq (Xiq, Vwfw(Xjq — Vwfwpoq (Xj qyI
w O(ω1{3mq.
This finishes the proof.	□
E	Additional Experimental Results
E.1 Estimating the projection length in function space
As mentioned in Section 4.1, when using freshly sampled points, we are actually estimating the
projection length of residual function『(x) = f * (x) — BfWpt)(Xq onto the given Gegenbauer poly-
nomial Pk (Xq. Here we present in Figure 3 a comparison between the projection length using train-
ing data and that using test data. An interesting phenomenon is that the network generalizes well
19
Under review as a conference paper at ICLR 2020
on the lower-order Gegenbauer polynomial and along the highest-order Gegenbauer polynomial the
network suffers overfitting.
(a) projection onto training data
(b) projection onto test data
f *(x) = 1Pι(x) + 3P2(x) + 5P4(x)	f *(x) = 1Pι(x) + 3P2(x) + 5P4(x)
O 2500	5000	7500 IOOOO 12500 15000 17500 20000
step t
(c) projection onto training data
O 2500	5000	7500 IOOOO 12500 15000 17500 20000
step t
(d) projection onto test data
Figure 3: Convergence curve for projection length onto vectors (determined by training data) and
functions (estimated by test data). We can see that for low-order Gegenbauer polynomials, the
network learns the function while for the high-order Gegenbauer polynomial, the network overfits
the training data.
E.2 Near-linear convergence behavior
In this subsection, we present the same curve shown in Section 4.1 in logarithmic scale instead of
linear scale. As shown in Figure 4 we can see that the convergence of different projection length
is close to linear convergence, which is well-aligned with our theorem. Note that we performed a
moving average of range 20 on these curves to avoid the heavy oscillation especially in late stage.
20
Under review as a conference paper at ICLR 2020
f*(x) = 1Pι(x) + 1P2(x) + 1P4(x)
O 2500 5000 7500 IOOOO 12500 15000 17500 20000
step t
(a) components with the same scale
f*(x) = 1Pι(x) + 3P2(x) + 5P4(x)
O 2500 5000 7500 IOOOO 12500 15000 17500 20000
step t
(b) components with different scale
Figure 4: Log-scale convergence curve for projection length onto different component. (a) shows
the curve when the target function have different component with the same scale. (b) shows the
curve when the higner-order components have larger scale. Both exhibit nearly linear convergence
especially at late stage.
21