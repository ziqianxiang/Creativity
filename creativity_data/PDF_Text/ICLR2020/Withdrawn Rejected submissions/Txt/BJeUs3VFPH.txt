Under review as a conference paper at ICLR 2020
Domain Adaptation via Low-Rank Basis Ap-
PROXIMATION
Anonymous authors
Paper under double-blind review
Ab stract
Domain adaptation focuses on the reuse of supervised learning models in a new
context. Prominent applications can be found in robotics, image processing or
web mining. In these areas, learning scenarios change by nature, but often remain
related and motivate the reuse of existing supervised models. While the majority
of symmetric and asymmetric domain adaptation algorithms utilize all available
source and target domain data, we show that efficient domain adaptation requires
only a substantially smaller subset from both domains. This makes it more suitable
for real-world scenarios where target domain data is rare. The presented approach
finds a target subspace representation for source and target data to address domain
differences by orthogonal basis transfer. By employing a low-rank approximation,
the approach remains low in computational time. The presented idea is evaluated
in typical domain adaptation tasks with standard benchmark data.
1	Introduction
Supervised learning and in particular classification is an essential task in machine learning with a
broad range of applications. The obtained models are used to predict the label of unseen test sam-
ples. A basic assumption in supervised learning is that the underlying domain or distribution is not
changing between training and test samples. If the domain is changing from one task to a related but
different task, one would like to reuse the available learning model. Domain differences are quite
common in real-world scenarios and eventually, lead to substantial performance drops (Weiss et al.,
2016).
A domain adaptation example is the classification of web pages. A classifier is trained in the domain
of university web pages with a word distribution according to universities. In the test scenario, the
domain has changed to non-university web pages, where the word distribution is related, but may
not be similar to the training distribution. More formally, let Z = {z1, . . . , zm} ∈ Rd be source
data sampled from the source domain distribution p(z) and let X = {x1, . . . , xn} ∈ Rd be target
data from the target domain distribution p(x). Traditional machine learning assumes similar distri-
butions, i.e. p(z)〜 p(x), but domain adaptation assumes different distributions, i.e. P(Z) = p(x)
and appears in the web page example where Z could be data of university websites and X are data
of non-university websites.
Multiple domain adaptation techniques have been already proposed, following different strategies
and improving the prediction performance of underlying classification algorithms in test scenarios
(Weiss et al., 2016; Pan & Yang, 2010). The common domain adaptation approach requires a large
number of source or target samples, which is indeed a disadvantage of many domain adaptation ap-
proaches and is not guaranteed in restricted environments where labeling is expensive (Weiss et al.,
2016). Further, learning a matrix for the alignment of domain distributions does not necessarily lead
to effective adaptation shown by (Raab & Schleif, 2018), where domain differences are explicitly
solved by overriding the basis of one domain with another and is competitive to current state of
the art domain adaptation algorithms (Gong et al., 2012; Long et al., 2015; 2013; Pan et al., 2011;
Fernando et al., 2013; Sun et al., 2016).
The main contribution of this work is to improve the Basis-Transfer (Raab & Schleif, 2018) ap-
proach by deriving a more simplified closed-form solution of the least-squares problem and enhance
this by a NyStrom based dimensionality reduction. The resulting method has a better prediction
performance and makes it easer to apply. Further, it is the fastest domain adaptation algorithm in
terms of computational complexity, while maintaining its excellent performance by using only a
1
Under review as a conference paper at ICLR 2020
subset of source and target data. It determines a target subspace representation for both domains and
transfers target basis information to source data via Nystrom Singular Value Decomposition (SVD)
with class-wise sampling. Further, We show that Nystrom approximation is well suited for non-
square landmark matrices, in contrast to common decomposition (Nemtsov et al., 2016) by applying
post-processing.
The rest of the paper is organized as follows: We give an overview of related work in section 2. The
underlying mathematical concepts are given in section 3. The proposed approach is discussed in
section 4, followed by an experimental part in section 5, addressing the classification performance
and computational time. A summary with a discussion of open issues is provided in the conclusion at
the end of the paper. More detailed derivations, description of experiments, implementation details
and additional results can be found in the appendix.
2	Related Work
In general, homogeneous transfer learning (Weiss et al., 2016) approaches, or domain adaptation
(DA), distinguish roughly between the following strategies: Methods implementing the symmet-
ric feature adaptation (Weiss et al., 2016) are trying to find a common latent subspace for source
and target domain to reduce marginal distribution differences, such that the underlying structure of
the data is preserved in the subspace. A baseline approach for symmetric feature adaptation is the
Transfer Component Analysis (TCA) (Pan et al., 2011). TCA finds a suitable subspace transfor-
mation called transfer components via minimizing the Maximum Mean Discrepancy (MMD) in the
Reproducing Kernel Hilbert Space (RKHS). The Geodesic Flow Kernel (GFK) finds a particular
subspace by embedding original data onto a Grassmannian manifold, matches source and target on
the geodesic flow and determines a suitable domain adaptation kernel by applying the kernel trick
(Gong et al., 2012).
The relational-knowledge adaptation aims to find some relationship between source and target data
(Weiss et al., 2016). Transfer Kernel Learning (TKL) (Long et al., 2015) is a recent approach,
which approximates a kernel of training data K(Z) by test kernel K(X) via the NyStrOm kernel
approximation.
However, recent studies conclude that the asymmetric transformation of source data is a promising
approach in domain adaptation due to the better classification performance (Sun et al., 2016; Fer-
nando et al., 2016; Raab & Schleif, 2018; Long et al., 2013). Possibly, because a classifier is trained
on the source label information in the target domain (Sun et al., 2016). The good performance has
led to a variety of unsupervised asymmetric domain adaptation algorithms (Elhadji-Ille-Gado et al.,
2017; Shao et al., 2014; Blitzer et al., 2011; Thopalli et al., 2019), to name a few. The asymmetric
feature adaptation approaches try to transform source domain data in the target (subspace) domain
to match the source to target subspace and is most related to our work. In comparison to the symmet-
ric feature adaptation approaches, there is no shared subspace, but only the target space (Weiss et al.,
2016). The Joint Distribution Adaptation (JDA) (Long et al., 2013) solves divergences in marginal
distributions similar to TCA, but aligning conditional distributions with pseudo-labeling techniques.
The Subspace Alignment (SA) (Fernando et al., 2013) computes a target subspace representation
based on the correlation between target and source subspace projectors and hence aligning source
and target data. The Correlation Alignment (CORAL) (Sun et al., 2016) technique transfers second-
order statistics of the target domain into whitened source data.
Given these methods, there is an overall trend in domain adaptation in the direction of non-linear
problem formulations, indirect alignment of matrix differences and learning a suitable subspace for
the source domain in the target domain and have similar problem statements to ours. However, in
this work, we do not learn a suitable source subspace transformation, but explicitly override the
structural information of source domain with the target one. With this, we model the source sub-
space domain as part of the target subspace.
The considered domain adaptation methods have approximately a complexity of O(n2) where n
is the most significant number of samples concerning target or source. These algorithms pursue
transductive adaptation (Pan & Yang, 2010), because some unlabeled test data must be available at
training time. These transfer-solutions cannot be directly used as predictors, but instead, are wrap-
pers for classification algorithms. The respectively used baseline classifier is the Support Vector
Machine (SVM).
2
Under review as a conference paper at ICLR 2020
3	Preliminaries
We introduce the basics of the Nystrom kernel approximation in section 3.1, which is the foundation
of the Nystrom based Singular Value Decomposition in section 3.2. The Nystrom-SVD is used for
creating the subspace transformation of Nystrom Basis Transfer in section 4.1.
3.1	NYSTROM Approximation
The computational complexity of calculating kernels or eigensystems scales with O(n3) where n is
the sample size (Williams & Seeger, 2001). Therefore, low-rank approximations and dimensionality
reduction of data matrices are popular methods to get a better computational performance. in this
scope, however not limited to it, the NyStrOm approximation (Williams & Seeger, 2001) is a reliable
technique to accelerate eigendecomposition or approximation of general symmetric matrices (Gis-
brecht & Schleif, 2015).
it computes an approximated set of eigenvectors and values based on a usually much smaller sample
matrix. The landmarks are typically picked at random, but advanced sampling concepts could be
used as well (Talwalkar et al., 2012). The approximation is exact if the sample size is equal to the
rank of the original matrix and the rows of the sample matrix are linear independent (Gisbrecht &
Schleif, 2015).
In general, the Nystrom approximation technique assumes a symmetric matrix K ∈ Rn×n with a
decomposition of the form
K= AC DB ,	(1)
with A ∈ Rs×s, B ∈ Rs×(n-s), C ∈ R(n-s)×s and D ∈ R(n-s)×(n-s). The submatrix A
is called the landmark matrix containing s randomly chosen rows and columns from K and has
the Eigenvalue Decomposition (EVD) A = UΛU-1. Where eigenvectors are U ∈ Rs×s and
eigenvalues are on the diagonal of Λ ∈ Rs×s. The remaining approximated eigenvectors tʃ of K as
the part C or BT, are obtained by the NyStrOm method with UΛ = CU. Combining U and Ul the
full approximated eigenvectors of K are
UU
u = [u] = [cUλ-i] ∈ R	⑵
The right part of the EVD (U 1) of K can be obtained Via Nystrom similar to equation 2 by
V= [U-1	ATUTB] .	(3)
Combining equation 2, equation 3 and Λ, the matrix K is approximated by
K = UAV = CUi-I a [U-1 Λ-1U-1 B] .	(4)
The Nystrom approximation error is given by the Frobenius Norm between ground truth and recon-
structed matrices, i. e. Eny = ||K - K||F, with bounds proven by Gittens & Mahoney (2013).
3.2	General Matrix Approximation
Another application of the Nystrom method is the approximation of the Singular Value Decompo-
sition, which generalizes the concept of matrix decomposition with the consequence that respective
matrices must not be squared.
Let K ∈ Rn×d be a rectangular matrix with the decomposition as in equation 1. The SVD of the
landmark matrix is given by A = LSRT where L are left and R are right singular vectors. S are
positive singular values. The left and right singular vectors for the non-symmetric part C and B are
obtained via Nystrom techniques and are defined as L = CRST and R = BTLST respectively
(Nemtsov et al., 2016).
Applying the same principal as for NyStrOm-EVD, K is approximated by
K = LSRT = [L[ S [R R] = CRS-I S [R	STLTB] .	(5)
3
Under review as a conference paper at ICLR 2020
3.3	Polar Decomposition
The Polar decomposition (Higham, 2005) theorem is used to validate the proposed subspace trans-
formations as PCA transformations in section 4 and the class-wise sampling strategy in section 4.2.
It is a universal decomposition applicable to an arbitrary matrix and is defined as X = QP. Where
Q = LRT and P = RSRT with S as singular values, L and R are left and right singular vectors
respectively. If X is a square matrix, the decomposition is unique and U is orthogonal and a rotation
matrix. P is positive semi-definite and contains the scaling factors of X.
Theorem 1 (Reusing of Eigensystem). Let X ∈ Rn×d and K = XXT with EVD of K = UΛU-1
and Singular Value Decomposition (SVD) of K = LSRT. Taking the square root of K and using
the Polar Decomposition with
K 1 = (UΛU-1)1 = (QP) 2 = (LRT RSRT) 2 = (LSRT)1 = LS 2 RT,	(6)
then the eigenvectors and square root eigenvalues of K are singular vectors and values of X respec-
tively.
3.4	Gerschgorin Theorem
The Gerschgorin theorem (Varga, 2004) provides a geometric structure to bound eigenvalues to so-
called discs for complex square matrices, but also generalize to none complex square matrices. The
work ofQi (1984) expands the Gerschgorin circles to so-called Gerschgorin type circles for singular
values:
Theorem 2 (Gerschgorin Type Bound for Singular Values (Qi, 1984)). Given the matrix X ∈ Rn×d
with n, d ≥ 1, the singular values {si}in=1 of X are in the range of
si
{pi ± |ri|},	i = 1, . . . , n.
(7)
Where pi = |xii| and the range ri is defined as
(dn
|xij |,	|xji| , i = 1, . . .,n.
j=1,j 6=i	j=1,j6=i
(8)
By this, it is possible to estimate the numerical range of singular values of X and is used in the error
bound given in section 4.3.
4	Basis Transfer
The task of domain adaptation is to align distribution differences. That means that underlying statis-
tics will be similar afterwards. The key idea of the approach presented in the following is that explicit
alignment of data matrices without any knowledge of statistics and distribution of data is possible
and successful. It is based on the work of ScholkoPf et al. (1998), which proposed that if two kernel
matrices are similar, they follow similar distributions. Hence, we make the domain data as similar as
possible by overriding the syntactic structure, which will result in similar kernels. This also applies
to arbitrary rectangular data matrices of a linear kernel, because it can be reconstructed by using
theorem 1. Hence, the method is also applicable in the original feature space without requiring a
positive semi definite kernel.
The recent Basis-Transfer (BT) (Raab & Schleif, 2018) approach already showed great transfer
capabilities and performance by aligning X ∈ Rn×d and Z ∈ Rm×d with a small error in terms
of the Frobenius norm. However, this required equal samples sizes n = m. In BT, the following
optimization problem was considered:
minkMZT-XkF.	(9)
Where M and T are transformation matrices, drawing the source (Z) closer to target (X) data. A
solution of equation 9 is found in closed-form, summarized in three steps (Raab & Schleif, 2018):
First, normalize data to standard mean and variance. It is assumed that this will align marginal
distributions in Euclidean space without considering label information (Raab & Schleif, 2018). Sec-
ond, compute an SVD of source and target data, i. e. Z = LZ SZ RTZ with LZ ∈ Rm×m and
4
Under review as a conference paper at ICLR 2020
RZ ∈ Rd×d. And X = LXSXRTX with LX ∈ Rn×n and RX ∈ Rd×d. Note that we denote
Θz/Θχ as SoUrce/target related matrix respectively. Next, the approach assumes SZ 〜SX in terms
of the Frobenius norm due to similar domains providing similar scaling factors by singular values
and normalization. Finally, compute a solution for equation 9 by solving the linear equations. One
obtains M = LX LZ-1 and T = RZ-1RTX. Note that LZ LZ-1 = RZ RZ-1 = I. Finally, a trans-
fer operation is applied such that the source matrix is approximated by using the available target
information by
Z = MZT = LX L-1Lz SZ RZ R-1RX = LX SZ RX.	(10)
With Z ∈ Rm×d as approximated source data, used for training.
In the following, the work (Raab & Schleif, 2018) is hereby substantially improved and we propose
a Nystrom based version with following improvements: Reduction of computational complexity,
neglecting sample size requirements and achieve a low-rank projection via Nystrom approximation
with class-wise sampling and implicit dimensionality reduction.
Recap equation 9 and consider the slightly changed optimization problem
min ||MZ - X||F.	(11)
Where a transformation matrix M must be found, which is again obtained in closed-form. Note
this is a transfer learning interpretation of the orthogonal procrustes problem (Schonemann, 1966).
Because we apply a dimensionality reduction technique, just the left-sided transformation matrix
must be determined, derived as follows: Based on the relationship between SVD and EVD, the
Principal Component Analysis (PCA) can be rewritten in terms of SVD. Consider the target matrix
with SVD:
XTX= (RXSXLTX)(LXSXRTX) =RXS2XRTX.	(12)
Where RX ∈ Rd×s as eigenvectors and S2X ∈ Rs×s as eigenvalues ofXTX. By choosing only the
biggest s eigenvalues and corresponding eigenvectors the dimensionality ofX is reduced by
Xs = XRs = LsSsRsTRs = LsSs.	(13)
With Ls ∈ Rn×s, Ss ∈ Rs×s. Xs ∈ Rn×s is the reduced target matrix. Hence, only a left
sided transformation in equation 11 is required, because the right sided transformation is omitted in
equation 13. Note, that for equation 12 a linear covariance or kernel is used.
However, this procedure requires a complete data matrix or corresponding singular values and scales
the in worst case with O(n3) (Williams & Seeger, 2001). In the following, We apply Nystrom-SVD
and show that only a subset of the data is required, which simultaneously reduces computational
complexity and eliminates the need to examine all singular values.
4.1	NYSTROM Basis Transfer
Let Z and X have a valid decomposition given as in equation 1. Note for clarity the Nystrom
notation is used as in section 3.2. For a Nystrom-SVD we sample from both matrices S values
obtaining landmarks matrices AZ = LZ SZ RTZ ∈ Rs×s and AX = LX SX RTX ∈ Rs×s. Based
on Nystrom-SVD in equation 5, the dimensions are reduced as in equation 13 keeping only most
relevant data structures
Xs = LXSx = [Lx] Sx = [„ LXq-J SX ∈ Rn×s.	(14)
LX	CXRXSX
Hence, it is sufficient to only compute a SVD of AX instead of X with s m, d and therefore
is considerably lower in computational complexity. Analogy, we approximate source data by Zs =
LZ SZ ∈ Rn×s. Since we again assume SZ 〜Sx due to data normalization and domain similarities,
solving the linear equation as a possible solution for equation 11, leads to M = LX L-1. Plugging
it back we obtain
Zs = LXL-1LZSZ = LXSZ ∈ Rn×s.	(15)
Where again a basis of a target subspace transfers structural information into the source domain.
The matrix Zs is used for training and Xs is used for testing. We showed in equation 13, that
equation 14 and equation 15 are valid PCA transformations. By definition of SVD follows LZ LTZ =
LX L-X1 = I and LX is an orthogonal basis. Therefore, equation 14 and equation 15 are orthogonal
transformations. In particular, equation 15 transforms the source data into the target subspace and
5
Under review as a conference paper at ICLR 2020
projects it onto the principal components of X. If data matrices X and Z are standardized1 , the
geometric interpretation is a rotation of source data w.r.t to angles of the target basis. The relationship
between BT and NBT is in the number of samples S used by Nystrom. If the rank of the data matrices
is equal to s the approximation is exact and it became a subspace version of BT.
According to (Weiss et al., 2016), it is an asymmetric transfer approach. Further, it is transductive
(Pan & Yang, 2010), where unlabeled target data is needed at training time. Subsequently this
approach is denoted as Nystrom Basis Transfer (NBT).
But uniform sampling may not be optimal for Nystrom, given labeled data in a classification task
(Schleif et al., 2018). Therefore, we integrate class-wise sampling in the following.
4.2	Sampling Strategy for NYSTROM
The standard technique to create NyStrOm landmark matrices is to sample uniform or find clusters
in the data matrix (Talwalkar et al., 2012). In supervised learning, sampling should utilize class-
wise sampling to properly include class-depending attributes of a matrix into the approximation
(Schleif et al., 2018). This is especially necessary for ||Y|| > 2. However, a decomposition as in
equation 1, required for NyStrOm-SVD is intractable with class-wise sampling, because respective
matrices are non-square: Let Z ∈ Rm×d with m 6= d and landmark indices I = {i1 , . . . , is} with
at least one ij > d, then it is by definition undefined and is especially true for n > d. Therefore,
we sample rows class-wise forming AdZ ∈ Rs×d instead of AZ ∈ Rs×s . Using the insights of
theorem 1, we implicitly obtain an SVD ofa square landmark matrix, i. e. AdZ AdZT = KdZ ∈ Rs×s
by computing AdZ = LZSdZRTZ. Hence, we can proceed without a non-square landmark matrix and
using the SVD of AdZ analogy as in equation 15. Therefore, it is possible to sample from the whole
range of source data and by application of the Polar decomposition, the standard decomposition as
in equation 1 is not required. The resulting singular values and vectors are utilized for successive
Nystrom approximations.
The sampling from test data X is done uniformly row-wise, because of missing class information
and SVD is obtained analogy via AdX = LX SdX RTX and used in equation 14.
However, the possible range of singular values of AdZ is naturally not equal to AZ. Utilizing theorem
2 it is easy to show that SdZ 6= SZ. It scales approximated matrices Zs (equation 15) different
through SdZ and accurate normalization of matrix cannot be guaranteed. Therefore, we apply a post-
processing correction and apply z-normalization against the standard convention after NBT. The
singular vectors also have an approximation error. However, both subspace projections are based
on the same transformation matrix, hence making an identical error and as a result, the error should
not affect the classification. The process is given in the figure 2 (appendix A.2) and showing the
alignment of source to the target domain. Further, it shows the mechanics of the approach: The
structural similarities stay the same, while the scaling is changed leading to a high similarity after
the post-processing correction.
4.3	Properties OF NYSTROM Basis Transfer
The computational complexity of Nystrom Basis Transfer (NBT) is composed of economy-size
SVD of landmark matrices AdZ and AdX with complexity O(2s2). The matrix inversion of diagonal
matrix SdX -1 in equation 14 can be neglected. Remaining k matrix multiplications are O(ks2)
contributing to the overall complexity of NBT which is O(s2 ) with s	n, m, d. This makes
NBT the fastest domain adaptation solution in terms of computational complexity in comparison to
discussed methods in section 2.
The difference between source and target domain after NBT, i.e. approximation error of source by
target domain is bounded by
ENBT = kXs - ZskF ≤ l√n∣∣DX - DZIl < kX - ZkF .	(16)
Where ∣∣Lχ∣∣ ≤ √n is the bounding of the normalized singular vectors and D(.)is the maximum
sum of the s singular values obtained by the numerical estimation, given in theorem 2, of Xs and
Zs respectively. The equation 16 shows that NBT has a lower norm compared to BT or original
data and proofs that the matrices are aligned during NBT, reducing the distribution differences. The
1Experimental data are standardized to mean zero and variance one in the preprocessing.
6
Under review as a conference paper at ICLR 2020
respective proof is given in appendix A.1.
The out-of-sample extension for unseen target/source samples, e. g. x ∈ Rd, is analog to equa-
tion 14. Based on equation 13 a subspace projection via (approximated) right singular vec-
tors is also valid. Hence, a sample can be projected into the subspace via Xs = XRX =
x RX SdX-1LTBX and be evaluated by an arbitrary classifier learned in the subspace. The
pseudo code of NBT is shown in Algorithm 1.
Algorithm 1 Nystrom Basis Transfer (NBT)
Require: Z as m sized training; X as n sized test set; Y as m sized training label vector; s as
number of landmarks parameter.
Ensure: New Source Zs; new Target Xs ;
1 2 3 4	: AdX , AdZ , CX = matrix_decomposition(X,Z,s)	. According to section 4.2 : SdZ = SV D(AdZ);	. SVD of landmark matrix of Z : LX, SdX, RX = SV D(AdX);	. SVD of landmark matrix of X LX = [Lχ CX Rχ SX-1]	. According to equation 14
5 6 7	Xs = LX SX	. According to equation 14 Zs = LXSZ	. According to equation 15 Zs, Xs=Z_normalization(Zs,Xs)	. Per Matrix; Effect shown in figure 2
5 Experiments
We follow the experimental design typical for domain adaptation algorithms (Long et al., 2015;
Gong et al., 2012; Long et al., 2013; Pan & Yang, 2010; Pan et al., 2011; Sun et al., 2016; Mahadevan
et al., 2019; Fernando et al., 2013). A crucial characteristic of datasets for domain adaptation is that
domains for training and testing are different but related. This relation exists because the train
and test classes have the same top category or source. The classes itself are subcategories or subsets.
The parameters for respective methods2 are determined for best performance in terms of accuracy via
grid search. A detailed description of experiments can be found in appendix A.4. In the experiments,
the SVM independent of being a baseline or underlying classifier for domain adaptation methods
uses the RBF-Kernel. The tests are conducted on the datasets groups Reuters, Newsgroup and
Office-Caltech. A detailed dataset description is given in appendix A.3.
5.1	Performance Comparison
The experiments are cumulated as three dataset group, which are Reuters, Newsgroup and Office-
Caltech. The results are summarized in table 1. It shows the mean errors of the experiments
per dataset group. To determine statistically significant differences, the two-sided Kolmogorov-
Smirnoff-Test with a p-value of 1% is applied. The * marks statistical significance compared to
NBT. The NBT is compared to baseline SVM and standard domain adaptation methods.
The NBT and BT methods have excellent performance and outperform every other algorithm by far
and are overall similar. Looking at the datasets individually, the only competitive approach is SA,
however, in the overall rating, all non-BT methods are statistically worse.
Note that BT and NBT are very similar at Office-Caltech, this is caused because within NBT the
rank r of a given image data matrices is equal to the number of samples drawn by the Nystrom
approximation. Hence, the approximation is exact and we use the most important r singular vec-
tors and values. BT and NBT are overall much better than remaining approaches, caused by fol-
lowing possible reasons: Aligning differences of Frobenius norm between domains using a target
(subspace)-transformation is sufficient as a problem statement and overriding of source basis is a
successful approach. Further, by Scholkopf et al. (1998) it will also align differences in subsequent
kernels. Explicit integration of non-linearity or kernel statistic alignments does not necessarily lead
to more knowledge transfer. But, second-order statistics but also structural and geometric informa-
tion should be transferred.
2Source code and datasets are hosted at https://github.com/iclr-nbt/nbt.git
7
Under review as a conference paper at ICLR 2020
5 0 5 0 5 0
3 3 2 2 1 1
J0t山 Uo-IE。一-SSE-O
O 100 2OO 3OO 4OO 5OO 600	700 8OO 9OO 1000
Number of Landmarks
(a) Reuters
O 100 2OO 3OO 4OO 5OO 600	700
Number of Landmarks
(b) Office-Caltech
Figure 1: Relationship between number of landmarks and mean classification error over all Reuters
datasets, shown in left figure and mean over all Office-Caltech datasets, shown in right figure.
Dataset
SVM
TCA
2011
JDA
2013
TKL	GFK	SA	CORAL	BT
2015	2012	2013	2016	2018	NBT
Reuters	32.1*	31.8*	33.1*	23.2*	35.0*	7.6	35.7*	3.5	1.9
Newsgroup	17.8*	14.7 *	17.4*	6.7	22.5*	2.3	28.1 *	2.6	2.3
Office-Caltech	55.5 *	51.5 *	51.4 *	49.2 *	55.3 *	68.7 *	52.6 *	20.7	20.8
Overall Mean	35.1 *	32.7 *	34.0 *	26.4 *	37.6 *	26.2 *	38.8 *	8.90	8.3
Table 1: Result of experiments as mean error in percent over each dataset group. Bold marks
winner. * marks statistical differences with a P-ValUe of 1% against NBT. The standard deviation
was omitted, since it is the same for all and on average around 2-3%.
The sensitivity of the nUmber of landmarks on Prediction error as the only Parameter of NBT is
demonstrated in figUre 1. It shows a comParison of the nUmber of landmarks and the mean classifi-
cation error over ReUters and Office-Caltech datasets. At ReUters, it is a monotonically decreasing
convex fUnction with decreasing error by increasing the nUmber of landmarks. This sUPPorts the
Nystrom error expectation of approximating the real rank of a matrix. However, the image function
has many local minima. We assUme this indicates that only a sUbset of featUres is relevant and corre-
late for classification and remaining features are noise. A time comparison is given in the appendix
A.5 showing that our NBT approach is the overall fastest domain adaptation approach.
6 Conclusion
We proposed a low-rank domain approximation algorithm called NyStrOm Basis Transfer. It trans-
fers source and target data into the target subspace, requiring only a subset of domain data from both
domains. The dimensionality reduction, paired with smart class-wise sampling showed its reliability
and robustness in this study. Validated on common domain adaptation tasks and data, it showed an
outstanding performance both in absolute and statistical values. Additionally, it is lowest in com-
putational complexity compared to discussed solutions. NBT is an extension of earlier versions of
Basis Transfer via NyStrOm Methods and is a low-rank and fast domain adaption solution.
In future work, deep transfer learning, real-world and other domain adaptation datasets should be
considered. A more comprehensive discussion of the Nystrom approximation error with the pro-
posed decomposition should be theoretically evaluated and compared to current Nystrom techniques.
3We encountered numerical issues at our experiments, therefore the shown results are taken from (Mahade-
van et al., 2019) with a similar sampling scheme.
8
Under review as a conference paper at ICLR 2020
References
John Blitzer, Dean Foster, and Sham Kakade. Domain adaptation with coupled subspaces. Journal
ofMachine Learning Research, 15:173-181, 2011.
Wenyuan Dai, Gui-Rong Xue, Qiang Yang, and Yong Yu. Co-clustering based classification for
out-of-domain documents. In Proceedings of the 13th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, San Jose, California, USA, August 12-15, 2007, pp.
210, 2007.
Nassara Elhadji-Ille-Gado, Edith Grall-Maes, and Malika Kharouf. Transfer Learning for Large
Scale Data Using Subspace Alignment. In 2017 16th IEEE International Conference on Machine
Learning and Applications (ICMLA), volume 2018-Januar, pp. 1006-1010. IEEE, dec 2017.
Basura Fernando, Amaury Habrard, Marc Sebban, and Tinne Tuytelaars. Unsupervised visual do-
main adaptation using subspace alignment. Proceedings of the IEEE International Conference on
Computer Vision, pp. 2960-2967, 2013.
Wen-sheng Chu Fernando, De Torre, and Jeffery F Cohn. Selective Transfer Machine for Per-
sonalized Facial Action Unit Detection. IEEE TRANSACTIONS ON PATTERN ANALYSIS AND
MACHINE INTELLIGENCE,, 39(3):529-545, 2016.
Andrej Gisbrecht and Frank Michael Schleif. Metric and non-metric proximity transformations at
linear costs. Neurocomputing, 167:643-657, 2015.
Alex Gittens and Michael W. Mahoney. Revisiting the Nystrom Method for Improved Large-Scale
Machine Learning. Journal of Machine Learning Research, 17:117:1—-117:65, 2013.
Boqing Gong, Yuan Shi, Fei Sha, and Kristen Grauman. Geodesic flow kernel for unsupervised
domain adaptation. In Proceedings of the IEEE Computer Society Conference on Computer Vision
and Pattern Recognition, pp. 2066-2073, 2012.
Nicholas J. Higham. Computing the Polar Decomposition—with Applications. SIAM Journal on
Scientific and Statistical Computing, 7(4):1160-1174, 2005.
Mingsheng Long, Jianmin Wang, Guiguang Ding, Jiaguang Sun, and Philip S. Yu. Transfer feature
learning with joint distribution adaptation. Proceedings of the IEEE International Conference on
Computer Vision, pp. 2200-2207, 2013.
Mingsheng Long, Jianmin Wang, Guiguang Ding, Sinno Jialin Pan, and Philip S. Yu. Adaptation
regularization: A general framework for transfer learning. IEEE Transactions on Knowledge and
Data Engineering, 26(5):1076-1089, 2014.
Mingsheng Long, Jianmin Wang, Jiaguang Sun, and Philip S. Yu. Domain invariant transfer kernel
learning. IEEE Transactions on Knowledge and Data Engineering, 27(6):1519-1532, 2015.
Sridhar Mahadevan, Bamdev Mishra, and Shalini Ghosh. A Unified Framework for Domain Adap-
tation using. In Michele Berlingerio, Francesco Bonchi, Thomas Gartner, Neil Hurley, and Geor-
giana Ifrim (eds.), Machine Learning and Knowledge Discovery in Databases, pp. 1-17, Cham,
2019. Springer International Publishing.
Arik Nemtsov, Amir Averbuch, and Alon Schclar. Matrix compression using the Nystrom method.
Intelligent Data Analysis, 20(5):997-1019, may 2016.
Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on Knowledge
and Data Engineering, 22(10):1345-1359, 2010.
Sinno Jialin Pan, Ivor W. Tsang, James T. Kwok, and Qiang Yang. Domain adaptation via transfer
component analysis. IEEE Transactions on Neural Networks, 22(2):199-210, 2011.
Liqun Qi. Some simple estimates for singular values of a matrix. Linear Algebra and Its Applica-
tions, 56:105-119, 1984.
9
Under review as a conference paper at ICLR 2020
Christoph Raab and Frank Michael Schleif. Sparse transfer classification for text documents. In
Frank Trollmann and Anni-Yasmin Turhan (eds.), Lecture Notes in Computer Science (including
subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), volume
11117 LNALPP.169-181. Springer International Publishing, 2018.
Frank-MichaeI Schleif, Andrej Gisbrecht, and Peter Tino. Supervised low rank indefinite kernel
aPProximation using minimum enclosing balls. Neurocomputing, 318:213-226, 2018.
Bernhard Scholkopf, Alexander Smola, and Klaus Robert Muller. Nonlinear Component Analysis
as a Kernel Eigenvalue Problem. Neural Computation, 10(5):1299-1319, 1998.
Peter H Schonemann. A generalized solution of the orthogonal procrustes problem. Psychometrika,
31(1):1-10, 1966.
Ming Shao, Dmitry Kit, and Yun Fu. Generalized transfer subspace learning through low-rank
constraint. International Journal of Computer Vision, 109(1-2):74-93, 2014.
Hyun Je Song and Seong Bae Park. An adapted surrogate kernel for classification under covariate
shift. Applied Soft Computing Journal, 69:435-442, 2018.
Baochen Sun, Jiashi Feng, and Kate Saenko. Return of Frustratingly Easy Domain Adaptation. In
Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, February 12-17, 2016,
Phoenix, Arizona, USA., pp. 2056-2065,pp. 2058-2065, 2016.
Ameet Talwalkar, Sanjiv Kumar, and Mehryar Mohri. Sampling Methods for the NyStrOm Method.
Journal of Machine Learning Research, 13:981-1006, 2012.
Kowshik Thopalli, Rushil Anirudh, Jayaraman J. Thiagarajan, and Pavan Turaga. Multiple Subspace
Alignment Improves Domain Adaptation. In ICASSP 2019 - 2019 IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP), pp. 3552-3556. IEEE, may 2019.
Richard S. Varga. Gersgorin and His Circles, volume 36 of Springer Series in Computational
Mathematics. Springer Berlin Heidelberg, 2004.
Karl Weiss, Taghi M Khoshgoftaar, and DingDing Wang. A survey of transfer learning. Journal of
Big Data, 3(1):9, 2016. ISSN 2196-1115.
Christopher Williams and Matthias W. Seeger. Using the NyStrOm Method to Speed Up Kernel
Machines. In T K Leen, T G Dietterich, and V Tresp (eds.), NIPS Proceedings, volume 13, pp.
682-688. MIT Press, 2001.
A Appendix
A.1 Norm Bound of NYSTROM Basis Transfer
This section discusses the approximation error of Nystrom Basis Transfer in terms of the Frobenius
norm of the matrix difference shown in equation 16. First, we clarify some definitions used in
the subsequent theorem. In this work, we define the Frobenius norm of a matrix X ∈ Rn×d as
nd
kXkF =	P P |xij |2. By using theorem 2 we can bound the norm of the singular values kSkF
i=1 j=1
ofX by the square root of the squared sum of the numerical range given by
D
kSkF
≤t
n
X(pi + |ri|)2.
i=1
(17)
Finally, we assume that the non-zero singular vectors are normalized and have by definition unit
length. Therefore, the norm of a singular vector matrix with n singular vectors can be bounded by
√n.
10
Under review as a conference paper at ICLR 2020
Theorem 3. Given two rectangular matrices X, Z ∈ Rn×d with n, d > 1 and
rank (X), rank(Z) > 1. The norm kZs - Xs kF in the subspace Rs induced by normalized singu-
lar vectors L ∈ Rn×s is bounded by
ENBT = kZs - XskF ≤ √n∣DX - DZ || < kZ - XkF .	(18)
Proof. We start by proofing the first part given by ∣∣Zs - Xs∣∣f ≤ √n∣∣Dχ - DZ||. Using equa-
tion 14, equation 15 and equation 16 the respective term can be rewritten in terms of a SVD-PCA
using only the biggest s singular values. Further, substituting M = LX LZ-1 leads to
kZs -XskF = kMZ - XskF = LXLZ-1LZSZ -LXSXF = kLXSZ -LXSXkF. (19)
Where we assume 1 ≤ s < n. This can be simplified applying the Cauchy-Schwarz inequality twice
with
kLX SZ - LXSXkF ≤ kLX kF kSZ - SXkF ≤ kLX kF (kSZ kF - kSX kF).	(20)
The norm of the singular values SZ, SX ∈ Rs×s can be bounded by equation 17 resulting in
kSZ kF ≤ DZs = t
su
X(piz+|riz|)2, kSXkF≤DXs = u
i=1
s
X(pix + |rix|)2.	(21)
i=1
Where piz and riz are the bound of a singular value of Zs and pix and rix are the bound of a singular
value of Xs.
Further, the singular vectors LX are normalized and thus can be bounded by ∣∣Lχ∣∣f ≤ √n. AP-
plying both to equation 20 leads to
∣LxSZ - LXSXkF ≤ ∣LxIf (∣Sz∣f -∣Sx∣f) ≤ √n∣DX - DZl∣.	(22)
The second term √n∣∣Dχ - DZ || < ∣∣Z - XkF is obtained analogy. Given the SVD of the original
matrices we obtain
kZ-XkF= kLZSZRZ - LXSXRXkF.	(23)
By again using the Cauchy-Schwarz inequality we obtain
kZ-XkF≤kLZkFkSZkFkRZkF-kLXkFkSXkFkRXkF.	(24)
Because LZ and LX have the same size they are bounded by √n and because RZ, RX ∈ Rd×d and
are also normalized, the norm can be bounded by |R.)∣∣ ≤ √d respectively. Because We are not
in the subspace Rs but Rd with s < d and the singular values are SZ, SX ∈ Rn×d the numerical
bound becomes
un
DXn = uX(pix + |rix|)2.
i=1
(25)
Applying the bounds to equation 24 We obtain
∣Lz∣F ∣Sz∣f ∣Rz∣F -∣Lx∣f ∣Sx∣f ∣Rx∣f ≤ ll√nDZ√d -√^X√d∣∣.	(26)
Simplifying this term further leads to
kZ - XkF ≤ √n√d∣∣DZ - DX||.	(27)
Because we assume d > 1 and a minimum of one non-zero singular vector in R(.)this at least
√n∖∖Dsχ - DZ|| < kZ - XkF ≤ √n√d∣∣DZ - DX||.	(28)
□
The result from the above allows the conclusion that the norm of the subspace matrices is smaller
than the norm of the original matrices. Note that because BT (Raab & Schleif, 2018) is applied in
Rd, this allows the conclusion that ENBT < EBT. If the norm of the difference of two matrices is
small, they are similar and if they are numerically similar, we can conclude that the distributions are
also similar (Song & Park, 2018). Finally, this is the reason why NBT aligns the distribution of two
given matrices. However, note that similar distributions not necessarily means a good classification
performance in terms of accuracy by an arbitrary classifier in a transfer learning setting.
11
Under review as a conference paper at ICLR 2020
A.2 PROCESS OF NYSTROM BASIS TRANSFER
In this section, an illustration of Nystrom Basis Transfer (NBT) is given. Figure 2 shows the process
of Nystrom Basis Transfer. The first row shows the samples of Nystrom to create the approximated
set of subspace projectors. The second row shows the data after the subspace projection. The
similarity in structure but dissimilarity in scaling, as discussed in section 4.3 is visible. The last row
shows the data after applying post projection and the difference is not visible anymore.
A.3 Dataset Description
The study consists of 24 benchmark datasets, already preprocessed and taken from (Gong et al.,
2012), (Long et al., 2015) and (Long et al., 2014).
Reuters-215784 (Long et al., 2015): A collection of Reuters news-wire articles collected in 1987.
The text is converted to lower case, words are stemmed and stopwords are removed. With the
Document Frequency (DF)-Threshold of 3, the numbers of features are cut down. Finally, Term-
Frequency Inverse-Document-Frequency (TFIDF) is applied for feature generation (Dai et al., 2007).
The three top categories organization (orgs), places and people are used in our experiment.
To create a transfer problem, a classifier is not tested with the same categories as it is trained on, i. g.
it is trained on some subcategories of organization and people and tested on others. Therefore, six
datasets are generated: orgs vs. places, orgs vs. people, people vs. places, places vs. orgs, people
vs. places and places vs. people. They are two-class problems with the top categories as the positive
and negative class and with subcategories as training and testing examples.
20-Newsgroup5 (Long et al., 2014): The original collection has approximately 20.000 text docu-
ments from 20 Newsgroups and is nearly equally distributed in 20 subcategories. The top four
categories are comp, rec, talk and sci and containing four subcategories each. We follow a data
sampling scheme introduced by Long et al. (2015) and generate 216 cross domain datasets based on
subcategories: Let C be a top category and {C1, C2, C3, C4} ∈ C are subcategories and analogy
K with {K1, K2, K3, K4} ∈ K. Select two subcategories each, e. g. C1, C2, K1 and K2, train a
classifier, select another four and test the model on it. The top categories are respective classes. Fol-
lowing this, 36 samplings per top category-combinations are possible, which are in total 216 dataset
samplings. This is summarized as mean over all test runs as comp vs rec, comp vs talk, comp vs
sci, rec vs sci, rec vs talk and sci vs talk. This version of 20-Newsgroup has 25.804 TF-IDF features
within 16.021 documents (Long et al., 2015).
Caltech-256-Office6 (Gong et al., 2012): The first, Caltech (C) is an extensive dataset of images and
contains 30.607 images within 257 categories. The Office dataset is a collection of images drawn
from three sources which are from amazon (A), digital SLR camera (DSLR) and webcam (W). They
vary regarding camera, light situation and size, but ten similar object classes, e. g. computer or
printer, are extracted for a classification task. Duplicates are removed, as well as images which have
more than 15 similar Scale Invariant Feature Transform (SIFT) in common (Gong et al., 2012).
The final feature extraction is done with Speeded Up Robust Features Extraction (SURF) and en-
coded with 800-bin histograms. Finally, the twelve sets are designed to be trained and tested against
each other by the ten labels (Gong et al., 2012).
Note to reproduce the results of this paper, one should use the linked version of datasets with the
same choice of subcategories. A summary of all datasets is shown in table 2. Regardless of the
dataset, data have been standardized to standard mean and variance. The experiments are summa-
rized as the mean over 20 runs with a 2-fold sampling, with separated samplings for source and
target data.
A.4 Implementation Details
The parameters for the respective domain adaptation algorithms are obtained via grid-search for best
performance on a dataset type, meaning text with Reuters and Newsgroup and image with Office vs
Caltech. All algorithms using the SVM with an RBF-Kernel, except GFK which suggest a k nearest
4http://www.daviddlewis.com/resources/testcollections/reuters21578
5http://qwone.com/〜jason/20Newsgroups/
6https://People.eecs.berkeley.edu∕~ jhoffman/domainadapt/#datasets_code
12
Under review as a conference paper at ICLR 2020
(a) Target Samples
0.02]
0.01、
0、
-0.01、
-0.02、
-0.03、
-0.04、
0.06
0.05
0.04
0.03
0.02
0.01
0
0.05
0
-0.05
-0.1
-0.15
0 0
(c) Target in SubSPaCe after NBT
0 0
(d) Source in Subspace after NBT
(b) Source Samples
0.1、
0.05、
0、
-0.05、
-0.1、
-0.15、
-0.2、
15
10
5
0
-5
-10
-15
-20
Figure 2: Process of Nystrom Basis Transfer with ten landmark samples applied to Caltech vs Ama-
zon image dataset as a surface plot. The left column shows the target and the right column shows
source data. The first row shows samples used for creating NBT transformations. The differences
are clearly visible. The second row shows the data after transformation. Note the similarities in
structure but differences in scale, based on the approximation error of singular values. The last row
contains the normalization correction and differences are hardly visible with the bare eye. This is
finally used for training and testing. Note that this is a toy example approximation unsuitable for
proper classification due to small landmark size, shown in figure 1. Best viewed in color.
13
Under review as a conference paper at ICLR 2020
Office-Caltech Dataset Names	#Samples	#Features	#Labels	Text Datasets Names	#Samples	#Features	#Labels
Caltech (C)	1123			Comp	4857		
Amazon(A) DSLR (D)	958 295	800	10	Rec Sci	3968 3946	25804	2
Webcam (W)	157			Talk	3250		
				Orgs	1237		
				People	1208	4771	2
				Places	1016		
Table 2: Overview of dataset characteristics containing numbers of samples, features and labels.
Datasets are categorized into two data types, i. e. text and image. The horizontal line separates the
datasets into groups namely Caltech-Office, Newsgroup and Reuters.
Dataset	SVM Baseline	TCA	JDA	TKL	GFK	SA	CORAL	BT	NBT
Reuters	0.06	0.86	0.36	0.40	3.11	0.87	8.38	0.94	0.50
Newsgroup	1.35	21.39	4.79	2.80	214.40	59.70	705.77	34.49	2.64
Office-Caltech	0.05	0.64	0.45	1.09	0.38	0.28	0.37	0.20	0.09
Overall	0.48	7.51	1.77	1.21	72.54	20.22	238.38	11.87	1.08
Table 3: Result of experiments shown in mean time in seconds per dataset group. All considered
methods using a baseline classifier, hence the SVM is not compared as standalone solution.
neighbor classifier. The C parameter of the SVM is set to 10 for all experiments. For SVM the
LibSVM is used. The TCA also has one parameter which gives the subspace dimensions and is set
to μ = 50 for both datasets. The JDA has two model parameters. First, the number of subspace
bases k, which is set to 100. Second, the regularization parameter λ is set to 1 for both sets.
The TKL approach has the eigenvalue dumping factor ξ as a parameter, which was set to 2 for the text
datasets and 1.1 for the image datasets. For the GFK solution, the parameter, number of subspace
dimensions is set to 20 for image and 40 for text data. The SA subspace dimension parameter d is
set to 5 for every dataset. The CORAL has no free parameters and CGCA is not evaluated, because
of numerical issues. The BT approach has no free parameter and for NBT, the subspace dimension
is set to 500 for text and 210 for images.
A.5 Time Comparison
The mean time results in seconds of the cross-validation study per data set group are shown in the
table 3. Note that SVM is the underlying classifier for the compared approaches and is presented
for the baseline and not listed for stand-alone comparison but included into the time measurement
of domain adaptation approaches. In the overall comparison, NBT is the fastest compared to the
discussed solutions and in contrast to BT, the required time is a magnitude smaller, especially at
Newsgroup, which is a high dimensional dataset. This is an expected effect due to Nystrom approx-
imation. Individually, the NBT is the fastest algorithm at the Newsgroup and Office-Caltech data
set. At Reuters, JDA is the fastest. In summary, the differences between the winning algorithm and
other algorithms are very small. However, GFK and CORAL are the slowest methods with a factor
of at least 90 compared to the fastest approach for Newsgroup and are clear outliers in this time
comparison.
A.6 Details of Prediction Performance
The detailed version of the results of the experiments are shown in table 4. It is an extended version
of table 1 of the main paper.
14
Under review as a conference paper at ICLR 2020
Dataset	SVM	TCA 2011	JDA 2013	TKL 2015	GFK 2012	SA 2013	CORAL 2016	BT	NBT
Orgs vs People	23.6	24.0	25.3	19.5	27.8	7.0	28.8	1.2	0.2
People vs Orgs	21.5	20.9	24.5	13.0	28.7	4.3	30.2	1.9	1.2
Orgs vs Place	31.9	29.0	29.5	23.5	34.7	6.5	35.6	3.4	1.7
Place vs Orgs	34.8	32.7	34.0	18.3	35.2	7.9	37.6	4.2	2.8
People vs Place	39.6	40.9	40.6	30.9	41.6	7.6	41.3	4.1	2.7
Place vs People	41.2	43.0	44.6	34.1	42.1	12.5	40.6	6.0	2.7
Reuters Mean	32.1*	31.8*	33.1*	23.2*	35.0*	7.6	35.7*	3.5	1.9
Comp vs Rec	12.7	8.1	7.8	3.0	16.3	1.8	23.0	0.3	0.7
Comp vs Sci	24.5	26.3	27.1	9.5	24.3	4.8	28.2	0.3	0.8
Comp vs Talk	5.1	2.9	4.2	2.4	22.3	0.9	26.8	4.8	3.9
Rec vs Sci	23.7	17.3	23.9	5.1	24.4	1.6	29.5	0.1	0.2
Rec vs Talk	18.7	13.6	15.2	5.6	23.2	1.8	29.8	5.1	3.9
Sci vs Talk	21.7	20.1	26.1	14.6	24.6	2.9	31.0	5.0	4.1
Newsgroup Mean	17.8*	14.7 *	17.4*	6.7	22.5*	2.3	28.1 *	2.6	2.3
C vs A	50.3	49.0	46.3	48.5	61.3	48.6	50.4	12.3	12.4
C vs W	58.9	59.5	55.1	53.6	63.3	81.3	60.9	20.5	20.5
C vs D	54.3	55.7	53.4	54.2	59.7	79.2	61.8	32.8	32.8
A vs C	58.8	57.7	57.5	57.1	63.8	50.2	58.1	38.5	38.5
A vs W	66.0	63.5	57.8	58.1	65.8	73.2	60.0	20.5	20.5
A vs D	59.2	61.5	62.6	56.8	64.5	74.2	61.8	34.3	34.3
D vs C	76.8	71.3	68.7	65.5	73.4	78.1	67.4	36.3	36.3
D vs A	72.1	67.8	66.5	64.0	68.4	70.5	64.5	4.7	4.8
D vs W	24.4	23.8	18.9	23.6	22.3	45.4	18.9	32.8	32.8
W vs C	74.4	68.6	68.0	64.4	70.8	80.4	68.3	33.4	33.5
W vs A	75.2	65.3	66.4	63.6	69.7	73.7	66.3	6.8	6.8
W vs D	52.9	31.7	27.7	23.5	33.1	33.2	25.8	19.8	19.8
Office-Caltech Mean	55.5 *	51.5 *	51.4 *	49.2 *	55.3 *	68.7 *	52.6 *	20.7	20.8
Overall Mean	35.1 *	32.7 *	34.0 *	26.4 *	37.6 *	26.2*	38.8 *	8.9	8.3
Table 4: Result of cross-validation test shown in mean error in percent per dataset. Mean over
dataset group at the end of each section. Bold marks winner. * marks statistical differences with
a p-value of 1% against NBT. The study shows that none of the listed algorithms is statistically
significant better as NBT.
15