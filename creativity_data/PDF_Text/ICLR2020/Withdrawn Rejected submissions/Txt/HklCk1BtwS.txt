Under review as a conference paper at ICLR 2020
Word embedding re-examined: theoretical
ANALYSIS ABOUT WORD SIMILARITY AND ANALOGY
STRUCTURE
Anonymous authors
Paper under double-blind review
Ab stract
As observed in previous works, many word embedding methods exhibit two in-
teresting properties: (1) words having similar semantic meanings are embedded
closely; (2) analogy structure exists in the embedding space, such that “Paris is
to France as Berlin is to Germany”. We theoretically analyze the inner mecha-
nism leading to these nice properties. Specifically, the embedding can be viewed
as a low rank transformation from the word-context co-occurrence space to the
embedding space. Such embedding transformation will preserve the relative dis-
tances among words. Furthermore, previous studies empirically observed that the
parameter α has a strong influence on the performance of word embedding but
did not provide theoretical explanation. We provide a theoretical explanation for
this behavior, and derive a method to automatically find its optimal value. The
experiments on real datasets verify our analysis.
1	Introduction
Word embedding is a fundamental task in natural language processing. Many different approaches
have been proposed, including LSA (Deerwester et al., 1990), SGNS (Mikolov et al., 2013a;b),
GloVe (Pennington et al., 2014) and others. These methods have achieved huge success in in-
formation retrieval (Salton & Buckley, 1988), entity recognition (Lample et al., 2016), sentiment
analysis (Socher et al., 2013), machine translation (Sutskever et al., 2014) and so on.
Previous studies have demonstrated three well-known facts about the word embedding: (1) Levy &
Goldberg (2014b) showes many different kinds of word embeddings (e.g. SGNS) can be understood
as the matrix factorization framework. For example, Levy & Goldberg (2014b) proves that SGNS
is implicitly factorizing the shifted pointwise mutual information (PMI) matrix. (2) Mikolov et al.
(2013b); Pennington et al. (2014); Levy et al. (2015); Tian et al. (2016) and other works observe
that word embedding exhibits two nice properties: the word similarity such that words with similar
semantic meanings are embedded closely in the embedding space, and the analogy structure such
that “woman is to queen as man is to king”. (3) Previous studies, including (Caron, 2001; Turney,
2012; Bullinaria & Levy, 2012; Levy et al., 2015; Artetxe et al., 2018), empirically observe that α
in word embedding E = Ud ∙ ∑α Ihasan important influence on the quality of word embedding.
But these works did not provide theoretical explanation for this behavior of α or practical guide to
set this parameter.
In this paper, we show the word embedding can be understood as a low rank transformation process.
Furthermore, we investigate the change of the relative distances between words which is the inner
mechanism that leads to these two nice properties in the word embedding. Particularly, we find the
parameter α has influence on the change of relative distance between words. Based on such analysis,
we provide theoretical explanation the behavior of α and derive a method to automatically find its
optimal value. To summarize, this research is motivated by two questions:
1.	What is the inner mechanism resulting in these two good properties of word embedding?
2.	How does the parameter α influence the word embedding? Can we find a method to deter-
mine its optimal value?
1We will explain the meaning of these variables (e.g., Ud) in the following sections.
1
Under review as a conference paper at ICLR 2020
We make the following contributions in this paper:
•	We reveal how the relative distances between words change during the embedding process
which can be seen as a low rank transformation process. The nice properties of word
embeddings are inherited from the original word-context co-occurrence matrix as the result
of such low rank transformation.
•	Compared with previous studies which empirically observed the influence of the parameter
α on the word embedding, we theoretically explain the influence of parameter α on the
word embedding by revealing it controls the change of relative distances between words.
Besides, we derive a method to automatically find its optimal value. We conduct experi-
ments on real datasets to verify our ideas.
The following sections are organized as follows: section 2 introduces the background knowledge and
necessary preliminaries; section 3 shows how the relative distances between words change during
this linear transformation and it is the inner reason incurring these nice properties of word embed-
ding. Based on such analysis, section 4 shows that symmetric factorization can be suboptimal and
how to improve the word embedding model with asymmetric factorization. Section 5 contains the
experiments on real datasets and section 6 discusses the related work. Finally, section ?? is the
conclusion and describes a potential work.
2	Background and Preliminaries
The goal of word embedding is to find a good vector representation for every word in a corpus. As
summarized in Yin & Shen (2018), most existing word embedding methods can be formulated as
low rank matrix approximations. So we can analyze the word embedding algorithms (e.g., SGNS)
within the matrix factorization framework. Suppose M ∈ Rn×n is the word-context matrix where
Mi,j represents some statistics between word wi and its context word wj . These word embedding
algorithms are seeking a d-dimension word embedding matrix E ∈ Rn×d accompanied with
a context embedding matrix C ∈ Rn×d to approximate the word-context matrix M, such that
M ≈ ECT . The matrix M can have different forms due to the design of objective function in
every embedding method. For example, M is the shifted pointwise mutual information matrix in
the SGNS.
Explicitly performing singular value decomposition. This kind of methods can be summarized as
follows. Firstly, a word-context matrix M is constructed to capture some co-occurrence statistics of
every word and its context. For example, M can be the pointwise mutual information (PMI) matrix
(Church & Hanks, 1990), positive PMI (PPMI) matrix (Bullinaria & Levy, 2012) and Shifted PPMI
(SPPMI) matrix (Levy & Goldberg, 2014b). Then, SVD is used to factorize the word-context matrix
M as M = U ∙ Σ ∙ VT. Finally, only the top d singular values in Σ and the corresponding columns
in U and V are kept, producing Σd, Ud and Vd respectively. The word embedding E ∈ Rn×d is
obtained by E = Ud or E = Ud ∙ ∑d.
Caron (2001), Turney (2012), Bullinaria & Levy (2012), Levy et al. (2015) and Artetxe et al. (2018)
discussed a more general approach which adds a parameter α to the truncated diagonal matrix Σd
and obtain the word embedding E = Ud ∙ Σa. They observe α has an important influence on the
embedding and suggest this parameter should be tuned. However, the reason is not theoretically
clear and no clear method has been provided to tune α yet.
Implicitly performing matrix factorization. Recently, word embedding methods based on neural
networks have been proposed (e.g., Bengio et al. (2003)). Particularly, Mikolov et al. (2013a;b) pro-
posed a popular word embedding approach SGNS based on the skip-gram with negative-sampling
and it achieves the state-of-the-art results in different tasks. Besides, GloVe is another widely
used word embedding method (Pennington et al., 2014). While SGNS and GloVe learn word em-
beddings by optimizing some objective functions using stochastic gradient methods, it has been
shown that these two methods are implicitly performing matrix factorizations. Specifically, Levy &
Goldberg (2014b) showed SGNS is implicitly factorizing the shifted Pointwise Mutual Information
matrix. Levy et al. (2015) showed the GloVe is implicitly factorizing the log-count word-context
co-occurrence matrix. So these two neural network based embedding methods can also be formu-
2
Under review as a conference paper at ICLR 2020
lated within the matrix factorization framework, and the parameter α equals to 0.5 as they are doing
symmetric factorization of some word-context co-occurrence matrix M .
To summarize, same as the first assumption in Yin & Shen (2018), our analysis assumes the word
embeddings can be formulated as low rank matrix approximations, either explicitly or implicitly.
Specifically, the embedding matrix E equals to Ud ∙ ∑α where Ud, ∑d and a are as defined before.
3	Analysis: the reason why the embedding exhibits nice
PROPERTIES
Our start point is the relation between word embedding and matrix factorization. As mentioned in
section 2, almost all word embedding methods can be formulated as explicit or implicit matrix fac-
torization (e.g., SGNS is implicitly doing a symmetric factorization on the shifted pointwise mutual
information matrix). We adopt the same notation in previous papers (Levy et al., 2015; Yin & Shen,
2018) to write M = UΣV T as the SVD of some word-context co-occurrence matrix M . For
example, M can be the pointwise mutual information matrix. Besides, Σd is the truncated diagonal
matrix containing top d singular values. Ud and Vd are the corresponding truncated U and truncated
V respectively. The d-dimension word embedding matrix E is obtained by multiplying Ud with a
power of Σd,
E = Ud ∙ ∑α	(1)
3.1	Word embedding as low rank transformation
The embedding E in equation 1 is indeed a low rank transformation from the original word-context
co-occurrence space M. Let us define another diagonal matrix Σpse = diag(σ1 , . . . , σd, 0, . . . , 0)
whose first d elements are the singular values in Σ and the remaining diagonal elements are zeros.
We have the following observation.
Observation 1. E is the first d columns of Epse
Epse = M ∙ V ∙ ∑pαs-1)	⑵
The proof is in Appendix A.1.
Observation 1 reveals the process to compute E: (1) first multiplying the matrix V to the right side
of M ; (2) then multiplying the diagonal matrix Σ(p1s-e α) on the intermediate result MV; (3) finally
removing the last n - d columns of M ∙ V ∙ Σpα-1). The first step in this transformation process is
a rotation because the matrix V is a unitary matrix. So the relative distances between words are not
changed during the first transformation. In the second step, the matrix Σpse is a diagonal matrix and
it is multiplied on the right side of MV. So the second step scales every dimension of the rotated
matrix. Because we keep the first d dimensions in the final step, we only care about the scaling in
the first d dimensions which are determined by the corresponding d singular values with a power α.
By explictly expressing the process which transforms the word-context co-occurrence matrix M to
the embedding matrix E, we can find a fact that the relative distances between words only changes
within the scaling step. Considering vectors of all words are scaled the same at every dimension, it
seems that the relative distances among words should be preserved to some extent. In fact, we have
the following two theorems:
Theorem 1.	For any two words wi and wj, if kMi,: - Mj,: k2 ≤ δ (δ ≥ 0), then kEi,: - Ej,: k2 ≤
(α-1)
σd	δ.
Theorem 2.	For any two words wi and wj, if kEi,: - Ej,: k2 ≤ δ (δ ≥ 0), then kMi,: - Mj,: k2 ≤
σ(1-α)δ + 2(Pn=d+ι σ2)1/2.	’'	''
The formal proof of theorem 1 and theorem 2 is in appendix A.2 and A.3.
3
Under review as a conference paper at ICLR 2020
These two theorems describe how the relative distances (measured in Euclidean distance) among
words change during the embedding transformation. Particularly, theorem 1 shows that if the
distance between two words wi and wj in M space is no more than δ, their distance in the
embedding space is also within a constraint. This bound is related with the singular value σd and
the parameter α. So theorem 1 implies that the relative distances between words will not change
dramatically during this transformation from M to E with suitable α. Theorem 2 demonstrates that
if we observe two words are embedded closely in E (kEi,: - Ej,: k2 is small), we can infer these
two words are also close in the original M space. To sum up, the above two theorems indicate the
neighborhood structure (relative distances to other words) for every word will be preserve to some
extent during the transformation from M to E. In other words, matrix E inherits the neighborhood
structure that exists in M . As the result, E will inherit the properties in M that are related with the
relative distances among words. Now we can answer the question raised in the introduction: What
is the inner mechanism resulting in the good properties of word embedding? In fact, we will show
that these properties are the result of inheritance of neighborhood structure.
3.2 The inner mechanism resulting in the nice properties in the embedding
Let us focus on two important properties in E : (1) words with similar semantic meanings are
likely to be embedded closely in the embedding space; (2) the word embedding tends to exhibit the
analogy structure, e.g., “woman is to queen as man is to king”. Note that these two properties are
related with the relative distances among words. So, considering the previous analysis which shows
matrix E inherits the neighborhood structure existing in M, these two properties in E also inherits
from M as the result.
Words with similar semantic meanings are embedded closely. Harris (1954) indicated
that if two words have almost identical environments we say that they are synonyms.. In other
words, two words will have similar neighbors if they have similar semantic meanings. It means
the corresponding rows in the word-context co-occurrence matrix M should be close for these
words. So the words with similar semantic meanings are also close in the M space which has
been empirically verified in Levy et al. (2015). Remember that theorem 1 shows if the distance
between two words in M is close (within a small range δ), these two words in E will also be
close due to this low rank transformation process from M to E . So the low rank transformation
in fact makes the embedding E inherit this property existing in the original word-context matrix M .
The analogy structure in the embedding. Different from word similarity which is about the
relative distance between two words (e.g., king and queen), the analogy structure is related with the
relative distance between two pairs (e.g., (king, queen) and (man, woman)). So let us first examine
how the relative distance between two pairs change during the embedding process.
Theorem 3.	Let Mγ1,:, Mγ2,:, Mβ1,: and Mβ2,: be four rows in M matrix. Denote Mγ =
(Mγι,: - Mγ2,:) and Me = (Mβι,: - Mβ2,:)∙ If ∣∣Mγ,: - Mβ∕b ≤ δ (δ ≥ 0), then
kEγ,: - Ee,』2 ≤ σdα-1)δ.
Theorem 4.	Let Mγ1,:, Mγ2,:, Mβ1,: and Mβ2,: be four rows in M matrix. Denote Mγ =
(Mγ1,: - Mγ2,:) and Mβ = (Mβ1,: - Mβ2,:). If ∣Eγ,: - Eβ,: ∣2 ≤ δ (δ ≥ 0), then
∣Mγ,: - Mβ,: ∣2 ≤ σ(1-α)δ + 4(Pn=d+1 σ2)1∕2.
The proof of theorem 3 and theorem 4 is in appendix A.4 and A.5, which is quite similar to the
proof of theorem 1 and theorem 2.
Theorems 3 and4 show that the relative distance between two pairs will not change dramatically dur-
ing the embedding process either. In other words, the relative distance between pairs of words will
be preserved during the embedding transformation from M to E. Furthermore, Levy & Goldberg
(2014a) revealed a fact that the original word-context co-occurrence M (e.g., M can be the PPMI
matrix)2 also exhibits the analogy structure, such that Mking - Mqueen ≈ Mman - Mwoman . So
2The word-context co-occurrence matrix M is called explicit representation in Levy & Goldberg (2014a)
4
Under review as a conference paper at ICLR 2020
IWm聒群哂
free^per∞⅛m≡st
,.win⅛Wn^i^v
e"⅛¢ain7ndy
seasons:
year
一j?PraeSt
coldest
temPeraturSfion5Oonnny
snowfall raιn⅜bi∏dy
h∣τidity winters
precipitation
summers weekend
nights
snowfall
coldest
wee
winters.
spnng
seasons
monsoonights
= summer
winter
autumn
spring
.summe .
SPrMgWMter days
autumn year
seasons
daχZ*nm
precipitation rainy
summersear ,
windy
(a)	α = 0.0
(b)	α = 0.5
(c)	α = 1.0
Figure 1: Illustration of the local neighborhood structure in the embedding space E = Ud ∙ ∑α
with different α. From (a) to (c): t-SNE (Maaten & Hinton, 2008) visualization of the nearest 20
neighbors ofan arbitrarily chosen word summer with α equals to 0.0, 0.5 and 1.0 respectively. More
results corresponding other α can be founded in appendix A.9.
theorems 3	and 4 together with this fact indicate the analogy structure in E also inherits from M
as the result of distance preserving in such low rank transformation process.
To summarize, we prove the bound about the change of relative distances of words during the embed-
ding process. With appropriate α, the neighborhood structure (relative distance) of every word will
not change much during the embedding process. So the embedding E inherites the word similarity
and the analogy structure from M. We also theoretically explain how the parameter α influences the
word embedding by controling the change of relative distances among words (the bounds in theorem
1-4). So a consideration is that we can optimize these bounds by choosing suitable α. We show the
derivation of finding the suitable α with respect to the bounds in appendix A.7. It shows the suitable
α depends on σ1 and σd, together with how to weight the two bounds in theorem 1 (3) and theorem
2 (4). However, the optimization over the bound only gives a rough estimation of the possible range
of optimal α. In the next section, we optimize α by directly minimizing the change of neighborhood
of words instead of optimizing over the bound.
4	Improvement: Word embedding preserving the distance
STRUCTURE BETTER
In previous section, we reveal the inner mechanism why the embedding exhibits such nice prop-
erties. Namely, the embedding is in fact a low rank transformation from M to E and the relative
distances between words are preserved during such process. As a result, the embedding matrix E
inherits these nice properties existing in the original word-context co-occurrence matrix M (e.g.,
the PMI matrix). Particularly, theorem 1, 2, 3 and 4 show that the parameter α has an important
influence on how the relative distances between words change. For example, we arbitrarily select
a word summer in the corpus3 and plot its nearest 20 neighbors with α equal to 0.0, 0.5 and 1.0
respectively. Figure 1 shows that the neighborhood of summer changes with different α. This ob-
servation gives Us an inspiration that We can learn the embedding E = Ud ∙ ∑α by selecting a good
α, thus preserving the relative distances among words during the embedding transformation process.
Preserving neighborhood structure better by choosing optimal α for embedding E = Ud ∙ ∑α.
Let gX (w) represent the neighbors of Word w in X space, e.g., X can be the Word-context co-
occurrence matrix M or the embedding matrix E. Let l(∙, ∙) bea correlation function measuring the
similarity of tWo different neighborhoods. This measurement captures hoW much the neighborhood
structure changes incurred by the embedding process. For example, if We consider the top knn
nearest neighbors as gx (∙) and ignore the order of these nearest neighbors, gx (W) can be viewed as
a set and l can be a similarity measurement defined on sets (e.g., Jaccard Similarity). OtherWise, if
we consider the order of neighbors, gX (w) can be a ranking list such that the neighbors are ranked
3We will introduce the corpus in section 5
5
Under review as a conference paper at ICLR 2020
by their relative distance to w, and l now can be a measurement about the correlation between two
rankings. We use L to denote the aggregated similarity score over all words
L=1 X l(gM(w),gE(w))	⑶
n
w∈W
where W is the word vocabulary to consider and its size is n. Based on previous analysis, we argue
We can select an optimal α for embedding E = Ud ∙ ∑α to preserve the relative distances among
words during the embedding transformation, such that
α? = arg max1 X l(gM (w), gE (w)	(4)
αn
α	w∈W
Leveraging the effect ofα into the original SGNS network structure. The above analysis is based
on the view that we compute the embedding matrix E by SVD such that E = Ud ∙ Σ?. We can also
think about to incorporate the effect of α into the original SGNS neural netWork structure. The orig-
inal SGNS implicitly performing a symmetric factorization, thus implying the α equal to 0.5. Here,
we consider to leverage the influence ofα directly into the SGNS structure. Consider the embedding
E = Ud ∙ ∑α and the context C = ∑d1-α) ∙ VT learnt by SVD, we have 1臂"信」)必=α 4,
d d	d	d	log(keik22 kcik22)
where ei and ci are the word vector wi and the context vector of wi respectively. We put it as a
regularization term in the original SGNS objective function to add the influence of different α to the
embedding learnt by SGNS. The detailed derivation is in appendix A.6.
To summarize, the a parameter in the embedding matrix E = Ud ∙ ∑α influences how the relative
distances between words change during the embedding process, thus influencing the quality of the
learnt embedding matrix. We propose a method to choose the optimal α by preserving the relative
distance among words during the embedding transformation. Besides, we come up with an idea to
incorporate the effect of α into the original SGNS architecture. In the next section, we will test these
ideas on real experiments.
5	Experiments
In this section, we conduct experiments to verify our previous analysis. The first test is to verify
that a good embedding does maximize the correlation defined in equation 3 which measures to
which extent the neighborhood structure is preserved during the embedding transformation. The
second is to test if the proposed method to incorporate α into original SGNS architecture really has
improvements.
5.1	Experiment settings
Corpus We use the Text9 corpus (Mahoney, 2011) which is a standard benchmark used for various
natural language tasks. We follow the same pre-processing steps in Levy et al. (2015).
Word similarity task The purpose of the experiments is to verify whether the method proposed
in section 4 to find the optimal α matches the real case. We adapt the same experiment as in Levy
et al. (2015) to test the word embedding on the word similarity and analogy structure. There are six
datasets acting as the ground truth for the word similarity: WordSim353 (Finkelstein et al., 2002)
and its two subsets WS Similarity and WS Relatedness (Zesch et al., 2008; Agirre et al., 2009),
Bruni MEN (Bruni et al., 2012), Radinsky Mechanical Turk (Radinsky et al., 2011) and Luong
Rare Words (Luong et al., 2013). These datasets5 contain word pairs together with human-assigned
similarity scores. Same as Levy et al. (2015), word embedding is evaluated by the correlation
between the similarity computed by the embedding vector and the human-assigned similarity score.
The function to compute correlation is the Spearman’s correlation (same as Levy et al. (2015)) Note
4Suppose we only consider the words for which kei k22 kci k22 6= 1, to avoid the case that zero appears in the
denominator.
5These datasets can be downloaded from https://bitbucket.org/omerlevy/hyperwords
6
Under review as a conference paper at ICLR 2020
that we discard the pairs in the test file if it contains a word that does not appear in the text9 corpus.
We use the word similarity task as an example for verifying the proposed method (equation 3 and
equation 4) to learn the optimal α for word embedding E = Ud ∙ ∑α, and the proposed method to
improve SGNS.
The codes used for the SVD embedding is adapted from https://bitbucket.org/
omerlevy/hyperwords and the codes used to learn the SGNS embedding is adapted from
https://github.com/theeluwin/pytorch-sgns. We store the codes in a shared drop-
box folder 6 for the double-blind review.
ə-ous
(c) radinsky testset
-1.0-0.8-0.6-0.4-0.2 0.0 0.2 0.4 0.6 0.8 1.0
parameter α
(d) ws relatedness testset
0.81
tυ
O 0.80
0 0.79 ∙
--iʒ
e
^αj 0.78 ∙
O
。0.77∙
0.76 ∙
Ws similarity		
-1.0-0.8-0.6-0.4-0.2 0.0 0.2 0.4 0.6 0.8 1.0
parameter α
(e) ws similarity testset
-1.0-0.8-0.6-0.4-0.2 0.0 0.2 0.4 0.6 0.8 1.0
parameter α
(f) ws353 testset
g。。S Uo-4-9」」。。 S
Figure 2: Compare the best α (black vertical line) in terms of the performance on the word similarity
task and the best α? (blue vertical line) in terms of equation 4. α? is searched from [-1, 1] with 0.1
as increment. From (a) to (e): y-axis is the correlation between the embedding score and the human
assignend score. x-axis is different α.
Table 1: The performance and similarity corresponding to α = 0 (SVD embedding) and α = 0.5
(symmetric embedding). Left is the peformance on every testset and right is the similarity computed
by equation 3, considering the words appearing in that testset.
α	bruni men	Luong	radinsky	ws relatedness	ws similarity	ws353
0.0	0.746 / 0.492	0.507 / 0.469	0.588/0.507	0.667 / 0.552	0.786 / 0.533	0.737 / 0.549
0.5	0.759 / 0.518	0.509 / 0.475	0.666/0.525	0.701 / 0.573	0.812 / 0.558	0.750 / 0.569
5.2	Results
Firstly, let Us verify that if the embedding E = Ud ∙ ∑α is good (measured by the performance on
the word similarity testsets), the embedding E should preserve the relative distances between words
better (measured by equation 3). We use the “vanilla” setting in Levy et al. (2015) to construct
the positive PMI (PPMI) matrix as M : window size is 2, no dynamic context window and no
subsampling, the number of negative samples is 1 and the context distribution smoothing parameter
is 1.0. For equation 3, we only consider the nearest neighbor for gX and use Jaccard Similarity as
the l(∙, ∙). Note that each testset only contains a subset of words in text9, and We also compute the
overall correlation (the L in equation 3) considering the words in the corresponding testset. The
6The	dropbox link is https://www.dropbox.com/sh/5d5j4pthcgzutdf/
AABUvZPJpxUo8ugff1gQ7fIQa?dl=0
7
Under review as a conference paper at ICLR 2020
result is in figure 1. Figure 1 shows that the performance (measured by these six word similarity
groundtruth) and the proposed measurement are highly correlated, such that, for every testset, if
α achieves high score word similarity test, the corresponding measurement in equation 3 is also
high. More details can be found in appendix A.8. Besides we test whether the α? in equation 4
really corresponds to α that achieves the best performance on the word similarity task. We plot the
performance of E = U ∙ Σ with different α in figure 2. The black vertical line corresponds to α that
achieves best accuracy and the blue vertical line corresponds to the α? learnt by equation 4. We can
see that α? is very near to the optimal point in almost all datasets. The only exception is the Luong
testset, which contains many rare words. We infer the reason is that the corpus cannot effectively
capture this testset because more than two thirds words in Luong testsets are not in the text9 corpus.
Table 2: The results of incorporating α to the original SGNS.
ɑ	bruni men	Luong	radinsky	WS relatedness	Ws similarity	ws353
~33~	0.70680	0.45004	0.67119	0.63262	0.77532	0.71247
~05~	0.70475	0.43613	0.67177	0.63733	0.77679	0.71309
Secondly, we test the effect that incorporating α into SGNS. Table 2 is the performance of the
original SGNS with α equals to 0.3 and 0.5 .According to the result (the gold line) in figure 2, if
we push the α from 0.5 to 0.3, the performance in the Luong testset should become better and the
performance in all other testsets should drop. The results in table 2 is same with this inference. The
only exception is the bruni men testset. We infer the reason is that the performances with α = 0.3
and α = 0.5 are very close (see gold line in figure 2 (a)).
6	Related Work
Explanation about the analogy structure. Mikolov et al. (2013c) observed the analogy structure
existing in the embedding created by a neural network. Levy & Goldberg (2014a) afterwards
found the same property also exists in the original word-context co-occurrence matrix. Levy &
Goldberg (2014a) infers neural embedding process is not discovering novel patterns, but rather
is doing a remarkable job at preserving the patterns inherent in the word-context co-occurrence
matrix. However, it is not clear how the neural embedding process preserves such property. Later,
several explanations were been proposed to explain the reason (Arora et al., 2016; Gittens et al.,
2017; Allen & Hospedales, 2019; Ethayarajh et al., 2018). For example, Gittens et al. (2017) and
Allen & Hospedales (2019) referred to the paraphrasing to explain the analogy structure. Different
from these previous studies, we analyze word embedding from the low rank transformation
perspective and we reveal the inner mechanism how this property is inherited during the embedding
transformation process.
Explanation about the α. The influence ofα on the quality of word embedding has been observed
in (Caron, 2001; Turney, 2012; Bullinaria & Levy, 2012; Levy et al., 2015; Artetxe et al., 2018).
These works empirically find that α has an important influence to the embedding and suggest this
parameter should be tuned. However, these works do not provide theoretical explanation and no
clear method has been provided to tune the α. Yin & Shen (2018) is the only work we know
which discusses the meaning of α from the pairwise inner product (PIP) loss perspective. But their
findings have some contradiction to the real experiments in Caron (2001),Turney (2012), Bullinaria
& Levy (2012), Levy et al. (2015) and Artetxe et al. (2018). Specifically, the analysis in Yin & Shen
(2018) suggests small α is easier to result in over-fitting and lead to the performace drop, which
implies that larger α is better. However, the real experiments in other papers show the performance
also drops with very large α. To summarize, we explain the meaning of α in the word embedding
transformation which once was not theoretically clear. Futhermore, we propose a method to find the
best α for word embedding.
8
Under review as a conference paper at ICLR 2020
References
Eneko Agirre, EnriqUe Alfonseca, Keith Hall, Jana Kravalova, Marius PaSca, and Aitor Soroa. A
study on similarity and relatedness using distributional and wordnet-based approaches. In Pro-
ceedings of Human Language Technologies: The 2009 Annual Conference of the North American
Chapter of the Association for CompUtational Linguistics, pp. 19-27. Association for ComPUta-
tional Linguistics, 2009.
Carl Allen and Timothy Hospedales. Analogies explained: Towards understanding word embed-
dings. In Proceedings of the 36th International Conference on Machine Learning, pp. 223-231.
PMLR, 2019.
Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. A latent variable model
approach to pmi-based word embeddings. Transactions of the Association for Computational
Linguistics, 4:385-399, 2016.
Mikel Artetxe, Gorka Labaka, Inigo Lopez-Gazpio, and Eneko Agirre. Uncovering divergent
linguistic information in word embeddings with lessons for intrinsic and extrinsic evaluation.
In Proceedings of the 22nd Conference on Computational Natural Language Learning, pp.
282-291, Brussels, Belgium, October 2018. Association for Computational Linguistics. doi:
10.18653/v1/K18-1028. URL https://www.aclweb.org/anthology/K18- 1028.
Yoshua Bengio, Rejean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic
language model. Journal of machine learning research, 3(Feb):1137-1155, 2003.
Elia Bruni, Gemma Boleda, Marco Baroni, and Nam-Khanh Tran. Distributional semantics in tech-
nicolor. In Proceedings of the 50th Annual Meeting of the Association for Computational Lin-
guistics: Long Papers-Volume 1, pp. 136-145. Association for Computational Linguistics, 2012.
John A Bullinaria and Joseph P Levy. Extracting semantic representations from word co-occurrence
statistics: stop-lists, stemming, and svd. Behavior research methods, 44(3):890-907, 2012.
John Caron. Experiments with LSA scoring: Optimal rank and basis. In Proceedings of the SIAM
Computational Information Retrieval Workshop, pp. 157-169, 2001.
Kenneth Ward Church and Patrick Hanks. Word association norms, mutual information, and lexi-
cography. Computational linguistics, 16(1):22-29, 1990.
Scott Deerwester, Susan T Dumais, George W Furnas, Thomas K Landauer, and Richard Harshman.
Indexing by latent semantic analysis. Journal of the American society for information science, 41
(6):391-407, 1990.
Kawin Ethayarajh, David Duvenaud, and Graeme Hirst. Towards understanding linear word analo-
gies. arXiv preprint arXiv:1810.04882, 2018.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and
Eytan Ruppin. Placing search in context: The concept revisited. ACM Transactions on informa-
tion systems, 20(1):116-131, 2002.
Alex Gittens, Dimitris Achlioptas, and Michael W Mahoney. Skip-gram- zipf+ uniform= vector
additivity. In Proceedings of the 55th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pp. 69-76, 2017.
Zellig S Harris. Distributional structure. Word, 10(2-3):146-162, 1954.
Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, and Chris Dyer.
Neural architectures for named entity recognition. In Proceedings of the 2016 Conference of the
North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, pp. 260-270, 2016.
Omer Levy and Yoav Goldberg. Linguistic regularities in sparse and explicit word representations.
In Proceedings of the eighteenth conference on computational natural language learning, pp.
171-180, 2014a.
9
Under review as a conference paper at ICLR 2020
Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. In Ad-
Vances in neural information processing Systems, pp. 2177-2185, 2014b.
Omer Levy, Yoav Goldberg, and Ido Dagan. Improving distributional similarity with lessons learned
from word embeddings. Transactions of the Association for Computational Linguistics, 3:211-
225, 2015.
Thang Luong, Richard Socher, and Christopher Manning. Better word representations with recursive
neural networks for morphology. In Proceedings of the Seventeenth Conference on Computational
Natural Language Learning, pp. 104-113, 2013.
Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine
learning research, 9(Nov):2579-2605, 2008.
Matt Mahoney. Large text compression benchmark. URL: http://www. mattmahoney. net/text/text.
html, 2011.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word represen-
tations in vector space. arXiv preprint arXiv:1301.3781, 2013a.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed represen-
tations of words and phrases and their compositionality. In Advances in neural information pro-
cessing systems, pp. 3111-3119, 2013b.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. Linguistic regularities in continuous space
word representations. In Proceedings of the 2013 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language Technologies, pp. 746-751,
2013c.
Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word
representation. In Proceedings of the 2014 conference on empirical methods in natural language
processing (EMNLP), pp. 1532-1543, 2014.
Kira Radinsky, Eugene Agichtein, Evgeniy Gabrilovich, and Shaul Markovitch. A word at a time:
computing word relatedness using temporal semantic analysis. In Proceedings of the 20th inter-
national conference on World wide web, pp. 337-346. ACM, 2011.
Gerard Salton and Christopher Buckley. Term-weighting approaches in automatic text retrieval.
Information processing & management, 24(5):513-523, 1988.
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng,
and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment
treebank. In Proceedings of the 2013 conference on empirical methods in natural language pro-
cessing, pp. 1631-1642, 2013.
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.
In Advances in neural information processing systems, pp. 3104-3112, 2014.
Fei Tian, Bin Gao, En-Hong Chen, and Tie-Yan Liu. Learning better word embedding by asymmet-
ric low-rank projection of knowledge graph. Journal of Computer Science and Technology, 31
(3):624-634, 2016.
Peter D Turney. Domain and function: A dual-space model of semantic relations and compositions.
Journal of Artificial Intelligence Research, 44:533-585, 2012.
Zi Yin and Yuanyuan Shen. On the dimensionality of word embedding. In Advances in Neural
Information Processing Systems, pp. 887-898, 2018.
Torsten Zesch, Christof Muller, and Iryna Gurevych. Using Wiktionary for computing semantic
relatedness. In AAAI, volume 8, pp. 861-866, 2008.
10
Under review as a conference paper at ICLR 2020
A Appendix
A. 1 Proof of observation 1
Proof. Firstly, we have
E =Md ∙ ∑α
= [U：,1, U：,2,..., U：,d] ∙	...
σd
=卜(a)u：,32a)u：,2,..”da)u：,di
Besides,
Epse=M ∙ V ∙ ∑Pα-1)
=u ∙ ∑ ∙ V T ∙ V ∙ ∑pα-1)
=U ∙ Σ ∙ I ∙ Σpα-1) (V is unitary matrix, so VT ∙ V = I)
=U ∙ Σ ∙ Σ(α-1)
pse
(α-1)
σ1	σ1
.
.
.
= [U：,1, US …,U：,n] ∙	σd σd+1	∙
.
.
.
σn
σd(α-1)
0
0
= [U：,1, U:,2,..., U：：,n] ∙	σd)0
.
.
.
0
= [b(a)U：,"(a)U：,2,..”da)U：,d, 0,..., 0]
So E corresponds to the first d columns of Epse.
A.2 The Proof of Theorem 1
We need the following lemma for the proof of theorem 1.
Lemma 1. Unitary matrix does not change the norm of a vector, such that
kU ak2 = kak2
where U is a unitary matrix and a ∈ Rn is a n-dimension column vector.
□
Proof. The norm can be expressed by using the Hermitian product，(∙, ∙)
∣∣Uak2 = ((Ua, Ua)) = (a, U*Ua) = (a, a) = ka∣∣2
⇒ kU ak2 = kak2
□
Now we can prove the theorem 1. For any two words wi and wj, if ∣Mi,: - Mj,: ∣2 ≤ δ, then
∣Ei,: - Ej,:k2 ≤ (σ(α-1)) × δ.
11
Under review as a conference paper at ICLR 2020
Proof. Let E denote the Epse defined in section 3. It is obvious that kEi,： - Ej,： k2
网:
Let h
- Ej,： ∣∣ since E is the first d columns of E and the last (n - d) dimensions in E are zero.
(h1, h2, . . . , hn) denote the (Mi,： - Mj,：)V. We have khk2
According to lemma 1, k(Mi,： - Mj,：)V k2 = k(Mi,： - Mj,：)k2.
k(Mi,： - Mj,：)V k2 = k(Mi,： -Mj,：)k2 ≤δ.
= k(Mi,： - Mj,：)Vk2.
So we have khk2 =
kEi,： - Ej,：k2
=网-Ej(
= |Mi,： ∙ V ∙ ∑pα-1) - Mj,： ∙ V ∙ ∑pa-I)Il2
= IkMi,：. V - Mj,： ∙ V)∙ ∑pα-ι)∣∣2
=ih ∙说"
=((σ1(α-1)h1)2 + (σ2(α-1)h2)2 +... + (σd(α-1)hd)2 + 0 +... + 0)1/2
≤(σ2(α-1) ∙ (h2 + h + ... + hd))1/2 (note σd is the smallest singular value)
≤(σda-1 ∙khk2)1/2 ≤ σdα-D ∙ δ
□
A.3 The Proof of Theorem 2
Proof. Let Mpse be UΣτ,se VT. It is easy to show that Mpse equals to Ud ∙ ∑d ∙ VdT.
T	71^`∕Γ	1	Tt Γ
Let M denote Mpse .
We first show that Mi,： - Mi
values of M . Firstly, we have
2
(for every i) is constrained by the singular
2
〜
Mi,:-Mi/2
(X(Mi,j -Mi,j )2)1/2 ≤(X X(Mk,j - Mk,j)2 )1/2 = ∣∣M - M∣∣2
So we have
〜
Mi,： - Mi,： ∣∣2
≤∣∣M-M∣∣2
= ∣∣UΣVT - UΣpseVT ∣∣2
=∣∣U(Σ-Σpse)VT∣∣2
= k(Σ - Σpse)k2 (lemma 1)
=(σd2+1+σ2+2+...+σn )1/2
Then We show if kEi,： - Ej,： ∣∣2 ≤ δ, then ∣∣Mi,： - Mj,： ∣∣ ≤ δσ(1-α)
MlMj』2
=	(Ei,：
=	(Ei,：
-E ) ∙ Σl
Ej,：)	ς,
-Ej,：) ∙ ς,
T) ∙VdT∣∣2
(d1-α) ∣∣∣ (lemma 1)
12
Under review as a conference paper at ICLR 2020
Let h = (h1, h2 . . . , hn) denote Ei,: - Ej,: here. We have khk2 = kEi,: - Ej,: k2 ≤ δ. So the
above equation becomes
∣∣(%- Ej,j •成-1|2
= ∣∣(hι,h2 …,hn) ∙ ∑d1-α)∣∣2
= (h2σ2(1-α) + h2σ2(1-α) + 2.. + hdσ2(1-α))1/2
≤((hι + h2 + …+ hdM(1 O))I/2
≤ khk2 σ1(1-α)
≤δσ1(1-α)
With above preparations, we can now prove theorem 2.
kMi,: - Mj,:k2
= ∣∣(Mi,: — Mi,：)	-(Mj,: — Mi,J∣∣2
≤∣∣(Mi,: - Md∣	L + ∣∣一(Mj,：-Mi,j∣∣2
= ∣∣(Mi,: - Md∣	I2+1∣(Mj,:—Mj,:)-(Mi,:—Mj,j∣L
= ∣∣(Mi,: - Md∣	I +∣∣(Mj,:—Mj,j∣∣ +1∣-(Mi,:—Mj,J∣L 22	2
||	- _	J 1|	- -	} ||	- -	}
^^^^^^^^^{l^^^^^^^^	^^^^^^^^^{^^^^^^^^^^	^^^^^^^^^^{^^^^^^^^^^^
≤(Pkn=d+1 σk2 )1/2	；	≤(Pn=d+l σ2)1/2	≤δσ(1-α)
n
≤δσ(1-α) + 2( X σ2)"
k=d+1
□
A.4 The Proof of Theorem 3
Proof. Similar to the proof in section A.2, let E denote the Epse and h =
(h1,h,2 …，h) denote the ((Mγι,： - Mγ?,：) - (Mβι,: - Mr2,：))V. We have Mk2 =
k((Mγι,: — Mγ2,:) — (Mβι,: — Mβ2,J)Vk2 = k((Mγι,:- Mγ2,:) — (Mβι,: — Mβ2,))k2 ≤ δ.
Besides, k(Eγι,: - Eγ2,J -(Ee1,： — Ee2 ,Jk2 = J] (EY1,： — EY2,:) -(EeI ,: — Eβ2,J∣L since E
is the first d columns of E and the last (n — d) dimensions in E are zero.
k(EY1,： — EY2,：) -(Eβl,: — Eβ2,Jk2
=∣∣ (EY1,: 一 EY2 ,：) -(EeI ,： — Ee2,:)(
=∣∣(Myi,: — My2,：) ∙ V ∙ ∑pα-I)-(Me1,： — Mβ2,:) ∙ V ∙ ∑Pa-I) ∣∣2
= ∣∣((Mγι,: — Mγ2,:) — (Mβι,: — Mβ2,：)) ∙ V ∙ ∑pα-1)∣l
= ∣∣h ∙ ςPa-I)∣L
=((σ1(α-1)h1)2 + (σ2(α-1)h2)2 +222+ (σd(α-1)hd)2 + 0 +222+ 0)1/2
≤σda-1) ∙ (h2 + h2 + …+ hd)1/2 (note σd is the smallest singular value)
≤σd"-1) ∙khk2 ≤ σda-1) ∙ δ
□
13
Under review as a conference paper at ICLR 2020
A.5 The Proof of Theorem 4
The proof is quite similar to the proof in section A.3. Let Mpse be UΣpseV T. According to section
A.3, We know Mpse equals to Ud ∙ ∑d ∙ Vd.
Let M denote Mpse. According to section A.3, We have ∣∣Mi,: - Mi,： ∣∣ ≤ (。2+1 +。2+2 + ... 十
。看)1/2 (for every i).
Then with the same process in section A.3, we show if k(Eγι,: - Eγ2,∙) — (Eβι,: - Eβ2,J∣∣2 ≤ δ,
then ∣∣((Mγι,: 一 M72,：) -(Mβι,: - Mβ2,J)∣∣2 ≤ δσ(1-α).
∣∣((M71,： - M72,：)-(Mβι,: - Mβ2,J)∣∣2
=∣∣((Eγι,: - E72,：)- (Eβι,: - Eβ2,:)) ∙ cd1-a) ∙ vT∣∣2
= ∣∣((Eγι,: - Eγ2,:) - (Eβι,: - Eβ2,:)) ∙ ∑d1-α)∣∣2 (lemma 1)
Let h = (hi, h2 ...,hn) denote ((Eγι,: - Eγ2,∙) - (Eβι,: - Eβ2,:)) here. We have ∣∣h∣∣2
k((Eγι,: - Eγ2,:) - (Eβι,: - Eβ2,J)∣∣2 ≤ δ. So the above equation becomes
∣∣((E71,:- E72,:) - (Eβι,: - Eβ2 ,：)) ∙成-叫2
= ∣∣(hi,h2 …,hn) ∙ ∑d1-a)∣∣2
=(h2σ2(1-α) + h2σ2(1-α) + ... + hdσd(1-α))1/2
≤(h2 + h2 + ... + hd)1∕2σ(1-α)
= khk2 σ1(1-α)
=δσ1(1-α)
With above preparations, we can now prove theorem 4.
Proof.
kMγ,: — Mβ小
(Mγ,: — MYQ — (Mβ,: — My,：)||2
≤
(Mγ,: — Mγ,: )∣∣2 + ∣∣-(Mβ,: — My,：)||2
(Mγ,: — Mγ,J∣L + ∣∣(Mβ,: — Mβ,:) — (Mγ,: — Mβ,J∣L
≤
(MY,：—M7,j∣l+1∣(Me,：—Mβ,j∣L+II—(MY，：—Mβ,J∣L
≤
(MY1,: — MY1,：) —
(Mγι,： — Myi,：)||2
_________ -	J
{^^^^^^^^^^^
≤(pn=d+ι σ )1/2
(My2,： — MY2,：)||2 + ||(Mei,： — Mβ2,∙) — (Mβ2, — Mβ2,j∣L + II-(MY,： — Mβ,j∣L
+ II-(MY2,： — MY2 ,J∣L + II(Me1，： — Mβ2,j
S-----------{---------Z S---------{------
≤(pn=d+ι σ2)1/2	≤(Pn=d+ι σ )1/2
2+II-(Me2,： — Mβ2,j∣L + II-(My,： — Mβ,J∣L
J S----------{---------Z S---------{--------Z
≤(Pn=d+1 σk )1/2	≤δσ11-α)
n
=δσ(1-α)+4( X σk)1/2
k=d+1
□
14
Under review as a conference paper at ICLR 2020
A.6 The proof of alpha regularization
Proof. Firstly, we have
E =5 ∙ ∑α
= [λ1αu1,λ2αu2, ..., λdαud]
CT =∑d-α ∙ Vd
p1-avT]
λ2-avT
Since U and V is orthogonal, we have
T
eiT ej
-2α
i=j
i 6= j
ciTcj	=	0λ,i
i=j
i 6= j
Then, if(kei∣∣2 ∙ kk2) = 1, we have
log(keik22)
log(kM2 ∙kcik2)
α
□
A.7 THE OPTIMAL α IN TERMS OF MINIMIZING THE WEIGHTED SUMMATION OF THE
bounds in Theorem 1 and 2
We assume that the optimal α should minimize the outcome bounds in both Theorem 1 (B1 ) and
2 (B2) 7. The reason is that if two words are close in the original M space, these two words
should also be close in the E space because we do not want the relative distances of words change
dramatically, and vice versa. Thus we derive the optimal α by minimizing the weighted summation
of the bounds
B1
B2
B =λB1 + (1 -
-1)δ,
-1)δ,
σ1(1-α)δ,
σd(1-α)δ,
λ)B2		
α	≤	1
α	≥	1
α	≤	1
α	≥	1
where λ is the weight coefficient in the summation.
We derive the optimal α in three cases, i.e. Case 1: σ1
σd ≤ σ1 ≤ 1.
For Case 1, when α ≥ 1, we have
≥ σd ≥ 1, Case 2: σ1 ≥ 1 ≥ σd, Case 3:
B =λσ1α-1δ + (1 - λ)σd1-αδ
∂B
—=λδσα-1 ln σι - (1 - λ)δσ1-α ln σd
Easy to prove that B is a convex function, so we derive α by solving the equation ∂α = 0，for
simplicity, we use k = 1-λ,
∂B
—=0 ⇒ λδσα-1 ln σι = (1 - λ)δσ1-α ln σd
ln klnσd
ln σ1
Q	ln(σισd)
7The optimal α with respect to theorem 3 and theorem 4 can be derived in the same way.
15
Under review as a conference paper at ICLR 2020
Similarly, when α ≤ 1, we have
B = λσdα-1δ + (1 - λ)σ11-αδ
∂B
∂α
0⇒α=1+
ln knσι
ln(σισd)
Then we have the optimal α in terms of k as
α
k ln σ
ln π---d
ln σ1
ln (σ1σd)
k ln σ
ln — — 1
ln σd
ln (σ"d)
≥ 1,
≤ 1,
k ≥瞥
ln σd
lnσd ≤ k ≤ lnσ1
ln σ1	ln σd
k ≤ ⅛σL
For Case 2, from the formulation of B, we can easily derive the optimal α is 1.
For Case 3, similar with the derivation of Case 1, we have the optimal α in terms ofk as,
	1 +	k ln σ-1 ln ʒ~~⅛ ln σd ln (σι σd)	≤	1,	k≥	ln σ-1 ln σ-1
α=	1,				ln σ1-	1	ln σ-1 1 ≤ k ≤ -dι
		-1 k ln σ ln	d1- ln σ1 ln (σι σd)			ln σ-	1	ln σ1-1
	、1 +		≥	1,	k≤	ln σ-1 ln σ-1
A.8 Performance on word similarity task and the correlation score defined
IN EQUATION 3 WITH DIFFERENT α
Table 3: The performance and similarity corresponding to different α. Left is the peformance on
every testset and right is the similarity computed by equation 3, considering the words appearing in
that testset.
ɑ	bruni men	Luong	radinsky	ws relatedness	ws similarity	ws353
-1.0	0.686 / 0.492	0.480 / 0.457	0.504/0.487	0.591 / 0.532	0.759 / 0.522	0.694/0.535
-0.9	0.691 / 0.490	0.482 / 0.458	0.508/0.495	0.597 / 0.532	0.761 / 0.522	0.697/0.535
-0.8	0.697 / 0.489	0.484 / 0.459	0.513/0.493	0.605 / 0.538	0.762 / 0.526	0.701 /0.539
-0.7	0.703 / 0.490	0.487 / 0.460	0.520/0.501	0.612 / 0.538	0.765 / 0.526	0.706/0.539
-0.6	0.709 / 0.488	0.491 / 0.462	0.526/0.499	0.621 / 0.538	0.765 / 0.522	0.711/0.537
-0.5	0.715 / 0.488	0.493 / 0.462	0.534/0.499	0.630 / 0.538	0.766 / 0.518	0.715/0.535
-0.4	0.721 / 0.488	0.496 / 0.462	0.540/0.501	0.637 / 0.544	0.769 / 0.526	0.719/0.539
-0.3	0.728 / 0.489	0.499 / 0.460	0.551 /0.503	0.645 / 0.544	0.774 / 0.526	0.724/0.539
-0.2	0.734 / 0.492	0.502 / 0.467	0.562/0.509	0.651 / 0.547	0.779 / 0.529	0.729/0.544
-0.1	0.740 / 0.489	0.504 / 0.466	0.576/0.509	0.659 / 0.549	0.782 / 0.533	0.733/0.546
0.0	0.746 / 0.492	0.507 / 0.469	0.588/0.507	0.667 / 0.552	0.786 / 0.533	0.737/0.549
0.1	0.751 / 0.497	0.509 / 0.469	0.601 /0.509	0.676 / 0.558	0.791 / 0.536	0.742/0.553
0.2	0.755 / 0.504	0.510 / 0.471	0.618/0.513	0.685 / 0.558	0.796 / 0.540	0.747/0.553
0.3	0.758 / 0.520	0.512 / 0.473	0.635/0.521	0.693 / 0.570	0.802 / 0.555	0.750/0.563
0.4	0.759 / 0.519	0.511 / 0.475	0.652/0.525	0.698 / 0.570	0.807 / 0.555	0.751 /0.565
0.5	0.759 / 0.518	0.509 / 0.475	0.666/0.525	0.701 / 0.573	0.812 / 0.558	0.750/0.569
0.6	0.756 / 0.515	0.504 / 0.478	0.675/0.517	0.700 / 0.576	0.816 / 0.558	0.748/0.572
0.7	0.750 / 0.518	0.496 / 0.481	0.675/0.517	0.693 / 0.567	0.813 / 0.562	0.741 /0.565
0.8	0.740 / 0.527	0.485 / 0.472	0.672/0.511	0.677 / 0.576	0.805 / 0.573	0.728/0.572
0.9	0.726 / 0.516	0.468 / 0.465	0.663/0.517	0.649 / 0.567	0.794 / 0.555	0.708/0.558
1.0	0.707 / 0.505	0.448 / 0.456	0.649/0.513~	0.619 / 0.561	0.780 / 0.547	0.687/0.551
16
Under review as a conference paper at ICLR 2020
A.9 THE NEIGHBORHOOD OF summer WITH DIFFERENT α
	top 20 nearest words
PMI	winter, Olympics, autumn, during, and, spring, annual, season, in, at night, year, on, weather, after, rainy, day, including, festival, seasonal
α	top 20 nearest words
-1.0	winter, autumn, spring, rainy, monsoon, summers, winters, warmest, nights, coldest seasons, precipitation, temperatures, humidity, year, snowfall, freezing, months, average, climates
-0.9	winter, autumn, spring, rainy, monsoon, summers, winters, warmest, nights, coldest seasons, precipitation, temperatures,humidity, snowfall, year, freezing, months, average, climates
-0.8	winter, autumn, spring, rainy, monsoon, summers, winters, warmest, nights, coldest seasons, precipitation, snowfall, temperatures, humidity, year, freezing, climates, months, average
-0.7	winter, autumn, spring, rainy, summers, monsoon, winters, warmest, nights, coldest seasons, precipitation, snowfall, humidity, temperatures, year, freezing, climates, humid, months
-0.6	winter, autumn, spring, rainy, summers, monsoon, winters, warmest, nights, coldest snowfall, precipitation, seasons, humidity, temperatures, year, freezing, climates, humid, months
-0.5	winter, autumn, rainy, spring, summers, monsoon, winters, warmest, nights, coldest snowfall, precipitation, seasons, humidity, temperatures, year, freezing, climates, humid, months
-0.4	winter, autumn, rainy, spring, summers, monsoon, winters, warmest, nights, coldest snowfall, precipitation, humidity, seasons, temperatures, year, freezing, climates, humid, months
-0.3	winter, autumn, rainy, spring, summers, monsoon, winters, nights, warmest, coldest snowfall, precipitation, humidity, temperatures, seasons, year, climates, freezing, humid, windy
-0.2	winter, autumn, rainy, spring, summers, winters, monsoon, nights, warmest, coldest snowfall, precipitation, humidity, temperatures, seasons, year, climates, humid, freezing, windy
-0.1	winter, autumn, rainy, spring, summers, winters, monsoon, nights, warmest, coldest snowfall, precipitation, humidity, temperatures, seasons, climates, year, humid, freezing, windy
0.0	winter, autumn, rainy, spring, summers, winters, monsoon, nights, warmest, coldest snowfall, precipitation, humidity, temperatures, seasons, climates, humid, year, windy, freezing
0.1	winter, autumn, rainy, spring, summers, winters, monsoon, nights, warmest, coldest snowfall, precipitation, temperatures, humidity, seasons, year, humid, climates, windy, freezing
0.2	winter, autumn, rainy, spring, summers, winters, monsoon, nights, warmest, coldest snowfall, precipitation, temperatures, humidity, seasons, year, humid, climates, windy, sunny
0.3	winter, autumn, rainy, spring, summers, winters, monsoon, nights, warmest, snowfall coldest, precipitation, temperatures, seasons, humidity, year, humid, windy, climates, sunny
0.4	winter, autumn, rainy, spring, summers, winters, nights, monsoon, snowfall, warmest coldest, precipitation, seasons, temperatures, year, humidity, windy, humid, climates, days
0.5	winter, autumn, rainy, spring, summers, nights, winters, monsoon, snowfall, warmest coldest, seasons, precipitation, year, days, temperatures, weekend, windy, sunny, humidity
0.6	winter, autumn, rainy, spring, nights, summers, winters, monsoon, snowfall, warmest coldest, seasons, year, days, weekend, precipitation, annual, sunny, temperatures, month
0.7	winter, autumn, rainy, spring, nights, summers, winters, monsoon, snowfall, warmest days, coldest, year, seasons, weekend, annual, week, sunny, month, SnoW
0.8	winter, autumn, spring, rainy, nights, winters, summers, days, monsoon, year weekend, seasons, snowfall, annual, week, night, warmest, coldest, snow, month
0.9	winter, autumn, spring, nights, rainy, days, winters, summers, night, year weekend, annual, week, seasons, monsoon, snow, holiday, rain, month, day
1.0	winter, autumn, spring, nights, rainy, days, night, week, annual, year weekend, seasons, winters, snow, summers, day, holiday, rain, month, weather
17