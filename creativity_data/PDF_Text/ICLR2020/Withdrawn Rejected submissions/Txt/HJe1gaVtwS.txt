Under review as a conference paper at ICLR 2020

FAST  BILINEAR  MATRIX  NORMALIZATION
VIA  RANK-1 UPDATE

Anonymous authors

Paper under double-blind review

ABSTRACT

Bilinear pooling has achieved an impressive improvement over classical average
and  max pooling  in  many computer  vision  tasks.   Recent studies  discover  that
matrix normalization is vital for improving the performance of bilinear pooling
since it effectively suppresses the burstiness. Nevertheless, exiting matrix normal-
ization methods such as matrix square-root and logarithm are based on singular
value decomposition (SVD), which is not well suited in the GPU platform, lim-
iting its efficiency in training and inference.  To boost the efficiency in the GPU
platform,  recent methods rely on Newton-Schulz (NS) iteration to approximate
the matrix square-root.   Despite that NS iteration is well supported by GPU, it
takes    (KD³) computation complexity, where D is the dimension of each local
feature and K is the number of iterations, which is still expensive. Meanwhile, NS
iteration is applicable only to full bilinear matrix.  In contrast, a compact bilinear
feature obtained from tensor sketch or random projection has broken the matrix
structure, cannot be normalized by NS iteration.  To overcome these limitations,
we propose a rank-1 update normalization (RUN), which reduces the computa-
tional cost from    (KD³) to    (KDN ), where N is the number of local features
per image.  Moreover, it supports the normalization on compact bilinear features.
Meanwhile, the proposed RUN is differentiable, and thus it is feasible to plug it
in        a convolutional neural network as a layer to support an end-to-end training.
Comprehensive experiments on four public benchmarks show that, for the full bi-
linear pooling, the proposed RUN achieves comparable accuracies with a 330
speedup over NS iteration.  For the compact bilinear pooling, our RUN achieves
comparable accuracies with a 5400× speedup over the SVD-based normalization.

1    INTRODUCTION

In the past decade, convolutional neural network (CNN) has achieved a great success in many com-
puter vision tasks ranging from image recognition (He et al. (2016)), object detection (Ren et al.
(2015)), semantic segmentation (Long et al. (2015)) to action recognition (Simonyan & Zisserman
(2014a)).  Despite CNN architecture has evolved significantly, it still inherits the basic 
architecture
from the pioneering work, AlexNet (Krizhevsky et al. (2012)).  To be specific, it consists of three
parts:  a feature extractor, an aggregation module and a classifier, as visualized in Figure 1.  The
feature extractor normally consists of a series of convolution, pooling, batch normalization and 
non-
linear rectification layers.  It generates a feature map      of W     H     D size, where W  and H 
are
the width and height of the feature map and D  is the depth of the feature map,  i.e.,  the number
of channels.  To enhance the performance of a CNN, many efforts have been devoted to boosting
effectiveness of the feature extractor. For instance, GoogLeNet (Szegedy et al. (2014)) proposes an
Inception module, which fuses feature maps from different scales and encodes richer visual infor-
mation than the vanilla CNN. ResNet (He et al. (2016)) adopts a residual architecture based identity
mapping, which overcomes the performance degeneration as network goes deep.  It has achieved
record-breaking performance in many tasks.  DenseNet (Huang et al. (2017)) extends the residual
module        to a densely-connected module, achieving a better performance.

The aggregation module converts the feature map     generated by the feature extractor into a holis-
tic feature vector f      Rd.  The early work such as AlexNet and VGGNet (Simonyan & Zisserman
(2014b)) implements the aggregate module by a fully-connected layer.  To be specific, they unfold
the three-dimensional feature map F  into one dimension vector vec(F)  ∈  RWHD  and obtain a

1


Under review as a conference paper at ICLR 2020


conv

pool

...

conv

pool

aggregate

classifer

feature extractor                    last feature map

Figure 1: The basic architecture of a convolutional neural network (CNN). It consists of tree parts,
a feature extractor, an aggregate module and a classifier.

(a) CUB                            (b) Airplane                            (c) MIT                  
             (d) DTD

Figure 2: The scatter plots of singular values of bilinear matrices on some typical computer vision
datasets:  CUB (Welinder et al. (2010)), Airplane (Maji et al. (2013)), MIT (Quattoni & Torralba
(2009)) and DTD (Cimpoi et al. (2014)). The indices of singular values are along x axis and scaled
magnitudes of singular values are on y axis.  For the ease of illustration, each magnitude is 
divided
by        its corresponding largest singular value and the scaled magnitudes are in the range of 
[0, 1].  We
plot the first 100 singular values of all samples on each dataset.

global vector f  =  Wvec(   ) + b      Rd, where W      Rd×WHD  and b      Rd  are parameters of a
fully-connected layer. In contrast, some more advanced architectures such as Inception and ResNet
implement the aggregation module by a global average pooling, where they conduct average pooling
along the width and height dimensions, and generate a holistic vector f      RD.  Compared with a
fully-connected layer, the global average pooling is more robust to spatial transforms.  To obtain a
more effective holistic feature, NetVLAD (Arandjelovic et al. (2016)) incorporates VLAD (Jegou
et al. (2011)) into the convolutional neural network.  Similarly, Miech et al. (2017) integrates the
Fisher vector into the neural network and proposes a learnable pooling.  In parallel, bilinear 
convo-
lutional neural network (Lin et al. (2015)) implements the aggregation module by a bilinear pooling
operation, which encodes the second-order information. It achieves a better performance than aver-
age pooling and max pooling in many computer vision tasks, such as fine-grained recognition (Lin
& Maji (2017)), generic image recognition (Li et al. (2018)) and video classification (Wang et al.
(2017b)).


Method
O²P

G²DeNet
MPN-COV

Improved B-CNN
iQRT-COV

MoNet
RUN (Ours)

Algorithm
SVD
SVD

Eigen Decomp
Newton-Schulz
Newton-Schulz
SVD

Power Method

Complexity

O(D³)

O(D³)

O(D³)

O(D³)

O(KD³)

O(D³)

O(KDN )

GPU Support
limited
limited
limited

good in FP
good
limited
good

CBP Support
No

No
No
No
No
Yes
Yes

Table 1: Differences between the proposed RUN with O²P (Ionescu et al. (2015)), G²DeNet (Wang
et al. (2017a)),  MPN-COV (Lin & Maji (2017)),  Improved B-CNN (Lin & Maji (2017)),  iQRT-
COV (Li et al. (2018)) and MoNet (Gou et al. (2018)). Here, K is the number of iterations, D is the
local feature dimension and N  is the number of local features.  Our RUN takes a low computation
complexity O(KDN ), is well supported by GPU and well supports compact bilinear pooling (CBP).

2


Under review as a conference paper at ICLR 2020

The burstiness (Je´gou et al. (2009)) in computer vision was first discussed in the context of the
bag-of-word model. It points out the phenomenon that most regions of an image are assigned to the
same visual word.  In such case, the representation of the image is determined by a single visual
word, however, some low-frequency visual words are ignore which might be important. To obtain a
more effective feature, Perronnin et al.  (Perronnin et al. (2010)) conduct the element-wise square-
root  normalization  to  balance  contributions  of  different  visual  words.   In  the  context  
of  bilinear
features, singular vectors correspond to visual words, the burstiness corresponds to the first a few
singular values that are significantly larger than the remaining singular values, as shown in 
Figure

2.  To alleviate this problem, a straightforward way is to normalize singular values.  Recent 
studies
(Li et al. (2017); Lin & Maji (2017)) show that, the normalization on singular values is vital for
achieving high recognition performance.  Existing normalization methods such as matrix square-
root normalization (Li et al. (2017)) and matrix logarithm normalization (Ionescu et al. (2015)) 
rely
on the singular value decomposition (SVD). But SVD is not easily parallelizable and hence not well
suited in the parallel GPU platform.   To boost the efficiency in the GPU platform,  improved B-
CNN (Lin & Maji (2017)) and i-SQRT (Li et al. (2018)) attempts to approximate the matrix square
root via the Newton-Schulz (NS) iteration (Higham (2008)). Since NS iteration only needs matrix-
matrix product, it is easily parallelizable and well suited in the GPU platform. The NS iteration 
has
a computation complexity of    (D³K), where D is the number of channels of the last features map
and K  is the number of iterations.  Since D can be very large, NS iteration is still expensive.  In
addition, NS iteration is conducted on the bilinear matrix and cannot normalize compact bilinear
features (Gao et al. (2016)) from tensor sketch or random projection.

To speed up the matrix normalization, we propose a rank-1 update normalization (RUN). The pro-
posed RUN is an iterative algorithm inspired by power method. In each iteration, it only needs two
matrix-vector multiplications, which takes low computation cost and is easily parallelizable and 
well
suited        in the GPU platform. In total, the computation complexity of the proposed RUN is    
(KDN ).
Here, N  = WH is the number of local features per image, which is in a comparable scale with D, K
is the iteration number, which is set as 2 by default on all testing datasets. Therefore, the 
complexity
of our RUN is considerably lower than the     (D³K) complexity used in NS iteration.  Moreover,
our RUN supports the normalization on a compact bilinear feature generated from tensor sketch or
random projection.  In addition, the proposed RUN is differentiable, as a result, we can easily plug
it into a neural network to support an end-to-end training.  Experiments on four public benchmarks
show the effectiveness and efficiency of our method.  Table 1 summarizes differences between our
method and other related work.

2    MATRIX  NORMALIZATION  AND  COMPACT  BILINEAR  POOLING

Given a feature map    , bilinear pooling reshapes      into a two-dimensional matrix F     RWH×D
and calculates the bilinear matrix by B = FTF. B-CNN (Lin et al. (2015)) implements the bilinear
pooling as a layer of a convolutional neural network to support an end-to-end training. It achieves 
a
better performance on fine-grained classification than standard AlexNet with a fully-connected layer
as aggregation module.  The research on B-CNN proceeds along two main directions:  1) improve
the effectiveness of bilinear pooling through matrix normalization (Lin & Maji (2017); Li et al.
(2017)); 2) improve the efficiency of bilinear feature through compact bilinear pooling (Gao et al.
(2016); Cui et al. (2017)).  Our work is related with both directions since we propose a fast matrix
normalization method, and make it compatible with compact bilinear pooling.  Below we review
these      two directions, respectively.

2.1    MATRIX NORMALIZATION

There are two popular matrix normalization methods, the matrix square-root normalization used in
Improved B-CNN (Lin & Maji (2017)) and the matrix logarithm normalization used in O²P (Ionescu
et al. (2015)). They first conduct singular value decomposition (SVD) on the bilinear matrix B by

B → UΣUT.

Then they conduct normalization on singular values and obtain the normalized bilinear feature by

Bˆ  ← Ug(Σ)UT,

where g(Σ) is conducted on singular values in an element-wise manner. Matrix square-root normal-
ization adopts g(Σ) = Σ¹/² and matrix logarithm normalization adopts g(Σ) = log(Σ). However,

3


Under review as a conference paper at ICLR 2020

as mentioned before, SVD is not easily parallelizable and not well supported in the GPU platform,
limiting its efficiency in training and inference. Improved B-CNN (Lin & Maji (2017)) and i-SQRT
(Li   et al. (2018)) utilize Newton-Schulz (NS) iteration to approximate the matrix square root. 
Given
a bilinear matrix B, NS initializes Y₀  = B and Z₀  = I.  For each iteration, Zk and Yk is updated
by

1

Yk =  2 Yk−₁(3I − Zk−₁Yk−₁),

1

Zk =  2 (3I − Zk−₁Yk−₁)Zk−₁,

where Yk converges to B¹/². Since it involves only matrix-matrix product, it is easily 
parallelizable
and well supported in the GPU platform.  The computation complexity of each iteration is    (D³),
where D is the local feature dimension. Since D is large, computing Newton-Schulz (NS) iteration is
still expensive. In contrast, our method is based on iterations of matrix-vector multiplications, 
which
are computationally cheaper than the matrix-matrix multiplications used in NS iteration.  What’s
more, we will show in next subsection that, the NS iteration is not compatible with existing compact
bilinear pooling methods, whereas ours readily supports normalization on compact bilinear features.

2.2    COMPACT BILINEAR POOLING

The dimension of a bilinear feature is D    D, which is extremely high. On one hand, it is more 
prone
to over-fitting due to huge number of model parameters in the classifier, especially in the few-shot
learning scenario.  On the other hand, in the retrieval application, it is extremely expensive to 
store
and compare high-dimensional bilinear features.  To overcome these drawbacks, CBP (Gao et al.
(2016)) is proposed. It treats the outer product used in bilinear pooling as a kernel embedding, and
seek to approximate the explicit kernel feature map. To be specific, by rearranging the feature map
F  to F = [f₁, · · ·  , fW H ]T, the bilinear matrix B is obtained by

WH                WH

B = FTF = Σ fifiT  = Σ h(fi),                                              (1)

	

where h(fi) = fifiT  ∈ RD×D  is the explicit feature map of the polynomial kernel. CBP seeks for a
low-dimensional projection function φ(fi) ∈ Rd with d   D² such that

⟨φ(x), φ(y)⟩ ≈ ⟨vec(h(x)), vec(h(y))⟩,                                          (2)

where vec( ) is the operation to unfold the 2D matrix to 1D vector.  In this case, the approximated
low-dimensional bilinear feature is obtained by B˜  =     W H φ(fi).

CBP investigates two types of approximation methods: Random Maclaurin (Kar & Karnick (2012))
and Tensor Sketch Pham & Pagh (2013), which are given in Algorithm 1 and Algorithm 2.  Since
the compact bilinear feature B˜  has broken the matrix structure, the matrix normalization methods
conducted on the bilinear feature B, such as Newton-Schulz iteration, is no longer feasible for nor-
malizing B˜. To tackle this, MoNet (Gou et al. (2018)) conducts SVD directly on the original feature
F instead of B and then conducts compact bilinear pooling.  Nevertheless, as we mentioned, the
SVD is not well supported on GPU platform, limiting the training and inference efficiency. In con-
trast, we will see in the next section that our method only relies on matrix-vector 
multiplications, and
hence is easily parallelizable and well supported in the GPU platform and supports the normalization
on a compact bilinear feature generated from tensor sketch or random projection.

3    RANK-1 UPDATE  NORMALIZATION  (RUN)

To overcome the limitations of previous methods, we propose a rank-1 update normalization (RUN).
Below we give the motivation of the proposed RUN and then summarize it in Algorithm 3.

Assuming that, through SVD, the bilinear feature B can be decomposed into B = UΣUT, where

U  =  [u₁, · · ·  , uD] is orthogonal and Σ  =  diag([σ₁, · · ·  , σd]) is diagonal with σ₁  ≥  σ₁  
≥  σD.

First we initialize a random vector v₀  = [v₁, ..., vD] ∼  N(0, I).  That is {vi}D     are i.i.d.  
random

variables with standard normal distribution. We perform K steps of power method as follows:

vk = Bvk−₁,  for k = 1, . . . , K.                                                (3)
4


Under review as a conference paper at ICLR 2020

Algorithm 1 Tensor Sketch

Input: x ∈ Rd

Output: φT S(x) ∈ RD

1:  Generate random vectors h₁, h₂ ∈ Nc  and s₁, s₂ ∈ {+1, −1}c. h₁(i) and h₂(i) are uniformly
sampled from {1, 2, · · ·  , D}, s₁(i) and s₂(i) are uniformly saΣmpled from {+1, −1}.

3:  Compute  φT S(x)  =  FFT−¹(FFT(Ψ(x, h₁, s₁))      FFT(Ψ(x, h₂, s₂))),  where      denotes
element-wise multiplication.

4:  return φT S(x)

Algorithm 2 Random Maclaurin

Input: x ∈ Rd

Output: φRM (x) ∈ RD

1:  Generate random matrices W₁, W₂ ∈ Rd×D  with each entry 1 or −1 with equal probability.


2:  φRM (x) ←  √1

3:  return φRM (x)

(W₁x) Ⓢ (W₂x).


Then the rank-1 matrix is constructed by

RK = BvKvKT /ǁvKǁ2

(4)

After that, we update the matrix B by subtracting RK:

BK = B − ϵRK,                                                            (5)

where ϵ  ∈  [0, 1] is a small constant.  The classic convergence result of power method tells that 
if

σ₁ > σ₂, vK will converge to u₁ directionally. Therefore, BK converges to B − ϵσ₁u₁u1  . That is


lim

K→∞

BK = U diag([σ₁(1 − ϵ), σ₂, . . . , σD])UT,                                  (6)

i.e., the eigenvalues of B     remain unchanged except the largest one, which is decreased by ϵσ₁.
More generally,  BK  is an estimation of a normalized bilinear matrix.  To be specific,  it 
satisfies
following theorem:

Theorem 1  Let BK be obtained via Eq. (3)-(5), where v₀          (0, I). Then the expectation of BK

is given by

E(BK) = U diag([σ₁(1 − ϵα₁), · · ·  , σD(1 − ϵαD))UT,                            (7)

where 1 ≥ α₁ ≥ α₂ ≥ · · · ≥ αD.

Due to limitation of the space, the proof of the Theorem 1 is given in Appendix A. The operation in
the right-hand side of Eq. (7) scales each singular value σi by (1 − ϵαi). As 1 ≥ α₁ ≥ α₂ ≥ · · · ≥
αD and ϵ ∈ [0, 1], thus

0 ≤ 1 − ϵα₁ ≤ 1 − ϵα₂ ≤ · · · ≤ 1 − ϵαD ≤ 1.                                     (8)
It gives a smaller scale factor to a larger singular value, making singular values more balanced.

Since computing BK only requires K times of matrix-vector multiplications, it only takes    (KD²)
complexity and is well supported in GPU platform.  In experiments section, we will show when K
is small,  e.g.,  K  =  2,  it has achieved excellent performance.  Nevertheless,  obtaining the 
above
approximated normalized bilinear feature BK requires the original bilinear matrix B obtained from
bilinear pooling.  Thus, it is not applicable to the compact bilinear feature which has broken the
structure of square matrix. To make the proposed fast matrix normalization method compatible with
compact bilinear pooling,  we seek to directly conduct normalization on the original feature map
F     RN ×D, where N  = WH is the number of local features and D is the local feature dimension.
It is based on following iterations:

vk = FTFvk−₁, for k = 1, . . . , K,                                              (9)

5


Under review as a conference paper at ICLR 2020


Algorithm 3 Rank-1 Update Normalization (RUN)

Input: Local features F     RN ×D, η, K

Output: Normalized local features FK.

1:  Generate  v₀  =  [v₁, ..., vD]  ∈  RD,  where  {vi}D

are  i.i.d.   random  variables  with  normal

distribution.

2:  for k ∈ [1, K] do

3:         vk  = FTFvk−1


4:  FK

= F     η FvK vKT

ǁvK ǁ2

5:  return FK.

where the entries of v₀  are i.i.d.   random variables with standard normal distribution.   Then we
construct the updated feature map FK by

FK = F − ηFvKvKT /ǁvKǁ2,                                                (10)

where vK are obtained via equation 9 and η      (0, 1] is a constant.  The above procedure is sum-
marized in Algorithm 3. Since in each iteration, it only needs two matrix-vector multiplications, in
total, the computational complexity of obtaining FK is    (KDN ). Let uF,i and vF,i be the left and
right singular vectors of F corresponding with its ith largest singular value σF,i.  If σF,₁  =  
σF,₂,
Fvk and vk will converge directionally to uF,₁ and vF,₁, respectively. In limit, we have


lim

K→∞

FK  = F − ησF,1uF,1vFT,1,

whose singular values are the same as that of F, except the largest one, which is decreased by 
ησ₁,F .
In fact, similar to Theorem 1, we have

Theorem 2  Let FK be obtained as in Algorithm 3. Then the expectation of FK can be given by

E(FK) = UF ΣF VFT,                                                       (11)

where UF , VF are the left and right singular vector matrices of F, respectively, ΣF is the diagonal
matrix diag[(σF,₁(1−ηβ₁), · · ·  , σF,D(1−ηβD))] with 0 ≤ 1−ηβ₁ ≤ 1−ηβ₂ ≤ · · · ≤ 1−ηβD ≤ 1.

Its proof is similar to Theorem 1, and thus we will omit it. Using the standard bilinear pooling, 
the
normalized bilinear matrix feature can be obtained by B¯ K   =  FTK FK.  When σF,₁  =  σF,₂, B¯ K
satisfies


lim

K→∞

B¯ K  = VF diag([σ²

(1 − η)², · · ·  , σ²

])VFT,                             (12)

Since VF  in Eq. (12) is equal to U in Eq. (6), if we set (1     ϵ) in Eq. (12) equal to (1     η)² 
 in
Eq. (6), BK and B¯ K  will converge to the same matrix. But the advantage of updating F as Eq. (10)
rather than updating B as Eq. (5) is that, the former one is compatible with compact bilinear 
pooling,

which can not be achieved by the latter one. In this case, the compact normalized bilinear feature 
is
obtained by

N

b¯K  =        φ(FK[i, :]),                                                      (13)

i=1

where FK[i, :] denotes the i-th row of FK and φ is implemented by tensor sketch or random Maclau-
rin, and b¯K  ∈ RD  is the compact and normalized feature where D   d².

The proposed RUN is summarized in Algorithm 3.  We implement the proposed RUN as a layer of
a CNN. The layer takes the original feature map F as input and outputs the normalized feature map
FK.  In the forward path, FK is computed by Eq. (10).  Below we derive its backward path.  Note
that, despite that one can rely on auto-grad tool in existing deep learning framework such as 
Pytorch
and TensorFlow to obtain the backward path, we still derive this process in Appendix B for readers 
to
better understand the proposed algorithm. After obtaining FK, it is feasible to conduct the original
bilinear pooling (BP) or compact bilinear pooling (CBP). Figure 3 illustrates the architecture of 
the
proposed network.

6


Under review as a conference paper at ICLR 2020


feature
extractor

feature map

RUN

BP/CBP

normalized map

soft-max
classifier

Figure 3: The architecture of the proposed convolutional neural network. RUN denotes the proposed
rank-1 update normalization, which takes input the feature map of the last convolutional layer.  BP
denotes the bilinear pooling and CBP represents compact bilinear pooling.

4    EXPERIMENTS

In  this  section,  we  demonstrate  the  experimental  results.   We  first  introduce  the  
testing  datasets
and implementation details.  Then we show conduct ablation study on two scenarios: 1) RUN with
standard bilinear pooling and 2) RUN with the compact bilinear pooling. After that, the comparisons
with other pooling methods are conducted.

4.1    DATASETS

We conduct experiments on three tasks: 1) fine-grained recognition, 2) scene recognition and 3) tex-
ture recognition. On the fine-grained recognition task, experiments are conducted on CUB (Welinder
et al. (2010)) and Airplane (Maji et al. (2013)) datasets. On the scene recognition task, 
experiments
are conducted on MIT (Quattoni & Torralba (2009)) dataset. On the texture recognition task, we test
our method on DTD (Cimpoi et al. (2014)) dataset. Table 2 gives a summary.

Fine-grained         Scene     Texture
CUB      Airplane      MIT        DTD

classes       200           100            67            47

training     5, 994       6, 667       4, 014      1, 880

testing      5, 794       3, 333       1, 339      3, 760

Table 2: Details of four datasets.

4.2    IMPLEMENTATION DETAILS

We use VGG16 (Simonyan & Zisserman (2014b)) as the backbone network to make a fair compari-
son with existing methods. After scaling and cropping, the input size of an input image is 448   48 
  3

and the size of the feature map is 28     28     512.  After we obtain the bilinear feature, we 
further
conduct element-wise signed square-root normalization followed by l₂-normalization as the original
BCNN (Lin et al. (2015)). We adopt a two-phase training strategy. In the first phase, we only update
the weights of the last fully-connected layer and fix the other layers.  The initial learning rate 
is set
as 0.2 on airplane dataset and 1 on other datasets, and it decreases to 0.1 of the current learning 
rate
if the validation error does not drop in continuous 5 epochs. We set weight decay as 10−⁸ in the 
first
phase. The first phase finishes in 50 epochs. In the second phase, we update the weights of all 
layers
and the initial learning rate is set as 0.02 on CUB dataset and 0.01 on other datasets, and it 
decreases
to 0.1 of the current learning rate if the validation error does not drop in continuous 5 epochs.  
We
set weight decay as 10−⁵ in the second phase. The second phase finishes in 40 epochs.

4.3    ABLATION STUDY ON ORIGINAL BILINEAR POOLING

In this section, we test RUN using original bilinear pooling.  The feature dimension is 512    512 
=

262K.

Influence of η.  η in Eq. (12) controls the strength of suppressing the large singular values.  
Recall
from Eq. (12) that, the normalized feature B¯ K  converges to:


VF diag[(1 − η)²σ²

, · · ·  , σ²

]VFT.

7


Under review as a conference paper at ICLR 2020


first epoch
last epoch

CUB

2.23

3.68

Airplane

1.53

1.69

MIT

2.38

4.53

DTD

5.09

7.40

Table 3: The average σF,₁/σF,₂ on four datasets.

η      CUB     Airplane     MIT     DTD

0.0     84.1         88.9         79.8      65.6

0.1     84.8         89.3         80.6      66.6

0.2     85.3         89.5        81.0     67.8

0.4     86.0         89.6         80.5      68.3

0.6     86.3        89.8        80.8     68.7

0.8     86.2         89.7         80.7      68.4

1.0     86.4        89.8        80.9      68.3

1.2     86.0        89.8        80.9      68.2

1.5     86.2         89.7         80.5      68.3

2.0     83.9         89.0         79.7      65.7

Table 4: The influence of η on the proposed RUN.

K     CUB     Airplane     MIT     DTD

1      85.7         89.7         80.5     68.7

2      86.3        89.8        80.8     68.4

3      86.2         89.8        80.8     68.3

5      86.2        89.9        80.7      68.4

10     86.1        89.9        80.7      68.4

Table 5: The influence of K on the proposed RUN.

From the above equation, we observe that, when η      (2, +    )     (       , 0), the largest 
value of the
normalized bilinear matrix B¯ K  is even larger than that of the original bilinear matrix B.  Hence 
a
good value of η should in the range [0, 2]. Ideally, we can select the value of η according to the 
gap

between σF,₁ and σF,₂.  Since singular values change for different samples or different epochs, we
can compute the σF,₁ and σF,₂ online for each sample in each epoch. But computing σF,₁ and σF,₂
will double the time cost compared with using a manually set η which only needs compute σF,₁.
An alternative solution is to compute the average σF,₁/σF,₂  of all samples using the pre-trained

model and then use the average value to guide the choice of the η. But the average value changes in
the training process, the average value computed from the pre-trained model might not be effective
for the whole training process.  Table 3 shows the average σF,₁/σF,₂  of each dataset.  Since each
experiment lasts for tens of epochs and it is difficult to report the ratio of each epoch, we just 
report
the average σF,₁/σF,₂ in the first epoch and that in the last epoch.  From Table 3, we observe that
the average σF,₁/σF,₂ in the first epoch is different from that in the last epoch.

We further test the influence of η on the classification accuracy.  As shown in Table 5, when η  =
0,  i.e.,  without RUN, the accuracies are not as good as that when η       [0.4, 1.5].   Note 
that,  on
Airplane dataset,  the accuracy drop when η  =  0 is not large,  it is in accordance with the small
value of σF,₁/σF,₂ on Airplane dataset in Table 3. In contrast, on DTD dataset, the accuracy drop is
significant, it is also in accordance with the large value of σF,₁/σF,₂ on DTD dataset in Table 3.

As Table 3 shows that the average value of σF,₁/σF,₂ varies significantly on four datasets, thus we
might expect that the optimal η  are different on four dataset.  Surprisingly, as shown in Table 5,
when η      [0.4, 1.5] the performance is stable and not sensitive to the change of η.  By default, 
we
set η = 0.6 on all datasets. Another observation is that, when η = 2.0, its performance is as bad as
that when η  = 0.0.  The bad performance when η  = 2.0 is expected since it leads to the condition
that (1 − η)² = 1. It is equivalent to removing the matrix normalization.

Influence of K.  K  in Eq. (10) represents the number of iterations in our RUN. The time cost of
the proposed RUN is linear with K.  Recall from Eq. (12) that, when K is large, the normalization
focuses only on the largest singular value and keeps the others unchanged.  In contrast, if K is not
large, it also normalizes other large singular values besides the largest one. As shown in Table 5, 
on

8


Under review as a conference paper at ICLR 2020


Algorithm
SVD

NS iteration
power method (ours)

FLOPs

1.88G

4.03G

3.2M

GPU Time

6731ms
833ms
2.5ms

Accuracy

CUB     Airplane     MIT     DTD

85.8         88.5        80.6     68.4

85.7         89.6        80.5     68.3

86.3         89.8        80.8     68.4

Table 6: Comparisons with SVD-based method (Lin & Maji (2017)) and Newton-Schulz (NS) iter-
ation (Li et al. (2018)).

CUB             Airplane             MIT                DTD
RM      TS      RM      TS      RM      TS      RM      TS

1, 000         83.1     83.8     88.9     88.5     78.0     76.1     59.9     63.4

2, 000         84.6     83.9     89.8     89.3     78.8     78.2     63.6     66.5

4, 000         84.4     84.8     88.8     90.5     79.9     79.4     67.0     66.9

8, 000         85.0     85.5     89.0     90.5     80.4     80.1     67.5     66.9

10, 000        85.2     85.7     89.1     91.0     80.7     80.5     67.5     67.3

Table 7: The influence of the dimension based on tensor sketch (TS) and random Maclaurin (RM).

DTD dataset, it achieves the best accuracy using only 2 iterations. In contrast, on Airplane 
dataset, it
achieves the best accuracy with 5 iterations. But using 2 iterations, the accuracy on Airplane 
dataset
is comparable with that using 5 iterations. By default, we set K = 2 on all datasets.

Time cost evaluation.  We compare the time cost in matrix normalization in the GPU platform of
the proposed method with existing methods based on SVD (Lin & Maji (2017)), and Newton-Schulz
(NS) iteration.  We conduct experiments based on 4 Nvidia K40 GPU cards and set the batch size
as 32. Note that, in these experiments we conduct the original bilinear pooling rather than compact
bilinear pooling since Newton-Schulz method is not compatible with compact bilinear pooling. As
shown in Table 6, SVD-based method is very slow in the GPU platform. The FLOPs of ours is less
than 0.1% of NS iteration used in Li et al. (2018). Meanwhile, considering the GPU time, the factual
speed-up ratio of ours over NS iteration is beyond 330.  The significant reduction in FLOPs and
GPU time is contributed by two factors.  Firstly, in each iteration, we only need two matrix-vector
multiplications whereas NS iteration takes three times of matrix-matrix multiplications.  Secondly,
ours takes only 2 iterations for a good performance whereas NS iteration takes 5 iterations to 
achieve
a good performance suggested by Li et al. (2018).

Method         Algorithm        Dimension     FLOPs     GPU Time           Accuracy
MoNet-2            SVD               10, 000        4.21G       13850ms       85.7         86.7

Ours        power method        10, 000         3.2M          2.5ms         85.7         91.0

Table 8: Comparisons between ours and MoNet-2 (Gou et al. (2018)).


Method
Max-pooling
Sum-pooling

BCNN

Improved BCNN
BCNN + Newton-Schulz

CBP
LRBP

MoNet-2
MoNet

BP + RUN (Ours)
CBP + RUN (Ours)

Dimension

512

512

262K

262K

262K

8192

8192

10K

10K

262K

10K

Norm Time
0ms
0ms
0ms
6.7s

833ms
0ms
0ms

13850ms

13850ms

2.5ms
2.5ms

CUB

69.6

71.7

84.0

85.8

85.7

84.0

84.2

85.7

86.4

86.3

85.7

Airplane

78.9

82.1

84.1

88.5

89.6

87.3

86.7

89.3

89.8

91.0

MIT

50.4

58.7

−

80.5

76.2

−

−

80.8

80.5

DTD

55.1

58.2

−

68.3

64.5

65.8

−

68.4

67.3

Table 9: Comparisons with other pooling methods. We compare the feature dimension, the time cost
for matrix normalization per batch (Norm Time) and the accuricies on four benchmarks.

9


Under review as a conference paper at ICLR 2020

4.4    ABLATION STUDY ON COMPACT BILINEAR POOLING

Influence of the dimension. We adopt two types of CBP, tensor sketch (TS) and random Maclaurin
(RM). We set η  =  0.6 and iteration number K  =  2, and change the dimension after CBP among
1K, 2K, 4K, 8K, 10K  .  As shown in Table 7, the accuracies generally increase as the dimension
increases. It is expected since the a larger dimension leads to a better approximation for the 
polyno-
mial kernel. Meanwhile, The accuracies achieved by TS are comparable with that achieved by RM.
By default, we use TS for compact bilinear pooling.

Time cost evaluation. We evaluate the time cost used in matrix normalization for compact bilinear
pooling (CBP). Since the Newton-Schulz iteration cannot be conducted on the original feature F, it
is incompatible with CBP. Thus, we only compare with Monet-2 (Gou et al. (2018)) which conducts
SVD on F.  F     R784×512  is in a larger size than B     R512×512.  Meanwhile, B is symmetric and
only needs compute its left singular vectors U as well as the singular values Σ. But F is asymmetric
and thus needs compute its right singular vectors VF besides UF and σF . Therefore, the FLOPs of
computing SVD on F shown in Table 8 is larger than the FLOPs of computing SVD on B shown in
Table   6. In contrast, the FLOPs of our RUN used for CBP is as the same as that used for original 
BP.
As shown in Table 8, achieving comparable or even better accuracies, we reduce the FLOPs from
4.21G to 3.2M. Moreover, we reduce the time cost in the GPU from 13850ms to 2.5ms, i.e., we
achieve a 5540    speedup. Note that, the GPU time cost speedup is larger than the FLOPs reduction
ratio since the proposed RUN better supported than SVD in the GPU platform.

4.5    COMPARISON WITH OTHER POOLING METHODS.

We compare with other pooling methods.  First of all, we compare with two baselines, which re-
place the bilinear pooling by max-pooling and sum-pooling,  respectively.   As shown in Table 9,
the features from max-pooling and sum-pooling are compact, and they do not need the matrix nor-
malization.  But accuracies achieved by them are lower than methods based on bilinear features.
We further compare with B-CNN (Lin et al. (2015)).  Benefited from bilinear pooling, B-CNN has
achieved good performance but using high-dimensional features. Meanwhile, since there is no ma-
trix normalization, its performance is not as good as ours. We further compare with CBP (Gao et al.
(2016)) and LRBP (Kong & Fowlkes (2017)).  CBP uses Tensor Sketch and Random Maclaurin to
reduce the feature dimension,  whereas LRBP adopt the low-rank strategy for a compact feature.
Nevertheless, neither CBP nor LRBP adopts matrix normalization. Thus their classification accuries
are not as high as ours as shown in Table 9.

We further compare with Improved BCNN (Lin & Maji (2017)) and BCNN + Newton-Schulz (Lin &
Maji (2017); Li et al. (2018)). To make a fair comparison with BCNN + Newton-Schulz , we directly
use i-SQRT layer released by the authors of Li et al. (2018), and keep all other settings identical.
As shown in Table 9, they achieve high accuracies but generate high-dimension features and take
high cost in matrix normalization. Then we compare with MoNet-2 and MoNet (Gou et al. (2018)).
MoNet-2 achieves high accuracies and generate compact features, but the time cost in the matrix
normalization  is  extremely  high.   MoNet  improves  MoNet  by  fusing  the  first-order  
information,
achieving higher accuracies, but is also slow in matrix normalization.  As shown in Table 9, using
CBP, our RUN achieves high accuracies, generates compact features and is very fast.  Despite that,
we can also further improve the performance of the proposed RUN by fusing the first-order feature
likewise MoNet, it is not the focus of this paper.

5    CONCLUSION

We propose a fast rank-1 update normalization (RUN) method for addressing the burstiness in bilin-
ear matrix efficiently. Since it only takes several times of matrix-vector multiplications, the 
proposed
RUN not only takes cheap computation complexity in theory but also is well supported in the GPU
platform in practice. More importantly, the proposed RUN supports normalization on compact bilin-
ear features, which have broken the matrix structure.  Meanwhile, RUN is differentiable and hence
can be easily plugged into a convolutional neural network, which supports an end-to-end training.
Our experiments on four datasets show that, combined with original bilinear pooling, we achieve
comparable or even better accuracies with a 330    speedup over Newton-Schulz iteration.  More-
over,  when using compact bilinear pooling,  we achieve comparable or even better accuracies on
four benchmark datasets with a 5540× speedup over the SVD-based method.

10


Under review as a conference paper at ICLR 2020

REFERENCES

Relja Arandjelovic, Petr Gronat, Akihiko Torii, Tomas Pajdla, and Josef Sivic. Netvlad: Cnn archi-
tecture for weakly supervised place recognition. In CVPR, 2016.

Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi.  De-
scribing textures in the wild. In CVPR, 2014.

Yin Cui, Feng Zhou, Jiang Wang, Xiao Liu, Yuanqing Lin, and Serge Belongie.  Kernel pooling for
convolutional neural networks. In CVPR, 2017.

Yang Gao, Oscar Beijbom, Ning Zhang, and Trevor Darrell.  Compact bilinear pooling.  In CVPR,
2016.

Mengran Gou, Fei Xiong, Octavia Camps, and Mario Sznaier.  Monet:  Moments embedding net-
work. In CVPR, 2018.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Sun Jian. Deep residual learning for image recog-
nition. In CVPR, 2016.

Nicholas J Higham. Functions of matrices: theory and computation, volume 104. Siam, 2008.

Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger.  Densely connected
convolutional networks. In CVPR, 2017.

Catalin  Ionescu,  Orestis  Vantzos,  and  Cristian  Sminchisescu.   Matrix  backpropagation  for  
deep
networks with structured layers. In ICCV, 2015.

Herve´ Je´gou, Matthijs Douze, and Cordelia Schmid. On the burstiness of visual elements. In CVPR,
2009.

Herve  Jegou,  Florent  Perronnin,  Matthijs  Douze,  Jorge  Sa´nchez,  Patrick  Perez,  and  
Cordelia
Schmid. Aggregating local image descriptors into compact codes. IEEE T-PAMI, 2011.

Purushottam Kar and Harish Karnick.  Random feature maps for dot product kernels.  In AISTATS,
2012.

Shu  Kong  and  Charless  Fowlkes.   Low-rank  bilinear  pooling  for  fine-grained  
classification.   In
Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 365–374,
2017.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.  Imagenet classification with deep convo-
lutional neural networks. In NIPS, 2012.

Peihua Li, Jiangtao Xie, Qilong Wang, and Wangmeng Zuo. Is second-order information helpful for
large-scale visual recognition?  In ICCV, 2017.

Peihua Li, Jiangtao Xie, Qilong Wang, and Zilin Gao.  Towards faster training of global covariance
pooling networks by iterative matrix square root normalization. In CVPR, 2018.

Tsung-Yu Lin and Subhransu Maji. Improved bilinear pooling with cnns. In BMVC, 2017.

Tsung Yu Lin, Aruni Roychowdhury, and Subhransu Maji.   Bilinear cnn models for fine-grained
visual recognition. In ICCV, 2015.

Jonathan Long, Evan Shelhamer, and Trevor Darrell.   Fully convolutional networks for semantic
segmentation. In CVPR, 2015.

S. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi.  Fine-grained visual classification of
aircraft. Technical report, 2013.

Antoine  Miech,  Ivan  Laptev,  and  Josef  Sivic.   Learnable  pooling  with  context  gating  for 
 video
classification. arXiv preprint arXiv:1706.06905, 2017.

Florent Perronnin, Jorge Sanchez, and Thomas Mensink. Improving the fisher kernel for large-scale
image classification. In ECCV, 2010.

11


Under review as a conference paper at ICLR 2020

Ninh Pham and Rasmus Pagh.  Fast and scalable polynomial kernels via explicit feature maps.  In

SIGKDD, pp. 239–247. ACM, 2013.

Ariadna Quattoni and Antonio Torralba. Recognizing indoor scenes. In CVPR, 2009.

Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.  Faster r-cnn:  Towards real-time object
detection with region proposal networks. In NIPS, 2015.

Karen Simonyan and Andrew Zisserman. Two-stream convolutional networks for action recognition
in videos. In NIPS, 2014a.

Karen Simonyan and Andrew Zisserman.  Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014b.

Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru  Erhan,  Vincent  Vanhoucke,  and  Andrew  Rabinovich.   Going  deeper  with  convolutions.
2014.

Qilong Wang, Peihua Li, and Lei Zhang. G2denet: Global gaussian distribution embedding network
and its application to visual recognition. In CVPR, 2017a.

Yunbo Wang, Mingsheng Long, Jianmin Wang, and Philip S Yu.  Spatiotemporal pyramid network
for video action recognition. In CVPR, 2017b.

Peter Welinder, Steve Branson, Takeshi Mita, Catherine Wah, Florian Schroff, Serge Belongie, and
Pietro Perona. Caltech-ucsd birds 200. 2010.

A    APPENDIX

In this section, we prove the Theorem 1 in Section 3.
Recall that BK = B − ϵRK, where


Using SVD, we factorize

RK = BvKvKT /ǁvKǁ2,                                                    (14)

B = UΣUT,                                                             (15)

where U is orthonormal containing the singular vectors and Σ = diag(σ₁,       , σD) is a diagonal
matrix containing singular values. According to Eq. (3), we have

vK = BKv₀ = UΣKUTv₀ = UΣKa,                                        (16)

where a = UTv₀. Plugging Eq. (15) and Eq. (16) into Eq. (14), we have

UΣK+1aaTΣK UT                     T


RK =

= UHU

aTΣ2K a

,                                       (17)

where H  =  (ΣK⁺¹aaTΣK)/(aTΣ²Ka).  As v₀           (0, I) and UUT  =  I, thus a          (0, I).
That is, a’s entries   a₁, a₂,       , ad   are i.i.d random variables with normal distribution. 
Therefore,
the expectation of each off-diagonal entry of H is 0. That is, E(H) is a diagonal matrix. We rewrite
E(H) = diag(h₁, · · ·  , hD) and


where

l                             i

i=1

D

αl = E((alσᵏ)²/ Σ(aiσᵏ)²)                                                (19)

In this case, proving Theorem 1 is equivalent to proving that αs ≥ αt if s < t. As we know


αs − αt = E    ˢ  ˢ

ˢ  ᵗ    .                                              (20)

  

12


Under review as a conference paper at ICLR 2020

We define bi = a² and yi = σ²ᵏ, then seek to prove

i                            i

α   − α   = E. bsys − btyt Σ ≥ 0,    if s < t.                                     (21)

As ys ≥ yt and y₁ ≥ y₂ · · · ≥ yD, we obtain

bsys − btyt ≥  yt  bs − bt  .                                                  (22)


Thus,

D

i=1

biyi

i=1 bi

E. bsys − btyt Σ ≥  yt E. bs − bt  Σ.                                           (23)

	

Since {ai}D  are i.i.d, {bi}D  are also i.i.d. Therefore,


1                                   1

E. bs − bt  Σ = E. Σ bs

Σ − E. Σ bt

Σ = 0.                               (24)

Plugging Eq. (24) into Eq. (23), we obtain

E. bsys − btyt Σ ≥ 0.                                                       (25)

B    APPENDIX

We compute the differentiation of F¯ K  based on Eq. (10):


dFK

= dF     η (dF)vKvKT  + F(dvK)vKT  + FvK(dvKT )

vT v

K  K                                              (26)


+ η (dvKT )vK + vKT dvK Fv

vT .


Meanwhile, Eq. (9) leads to

(vKT vK)2

vK = (FTF)Kv₀.                                                         (27)

Since v₀ is a constant vector, based on Eq. (27), we obtain

dvK ≡ K(FTF)K−¹[(dFT)F + FTdF]v₀,


dvKT  ≡ Kv0T[(dFT)F + FTdF](FTF)K−¹,

Plugging Eq. (28) in Eq. (26), we obtain

(28)

4                                                 4

dFK = Σ l¹(F)dFr¹(F) + Σ l²(F)(dF)Tr²(F),                              (29)

	


where {l¹(F), r¹(F)}4

and {l²(F), r²(F)}4

are
13


Under review as a conference paper at ICLR 2020

l¹(F) = I,    r¹(F) = I − ηvKvT /(vT vK),

0                             0                                          K        K

₁            −ηKF(FTF)K−¹FT       2                           T


l1(F) =

vKT vN

,    r1 (F) = v₀vK ,

l¹(F) =  −ηKFvK v0TFT ,    r¹(F) = (FTF)K−¹,

vKT vN


l¹(F) =  ηKv0TFT ,    r¹(F) = (FTF)K−¹v

Fv   vT ,


(vKT vK)2

K       K   K

1                  ηKvKT (FTF)K−1FT         1                                       T


l4 (F) =

(vKT vK

)2                ,    r4 (F) = v₀FvKvK ,

(30)

₂            −ηKF(FTF)K−¹        2                                 T


l1(F) =

vKT vK

,    r1 (F) = FvKvK ,

l²(F) =  −ηKFvK v0T ,    r²(F) = F(FTF)K−¹,

vKT vK


l²(F) =    ηKv0T    ,    r¹(F) = F(FTF)K−¹v

Fv   vT ,


(vKT vK)2

K       K   K

2                  ηKvKT (FTF)K−1           2                                            T


l4 (F) =

According to the definition,

(vKT vK

)2           ,    r4 (F) = Fv₀FvKvK .


dL ≡ vec( ∂L )Tvec(dF) ≡ vec(   ∂L  )Tvec(dF¯

).                              (31)

∂F                               ∂F¯ K


Since trace(ABT) ≡ vec(A)Tvec(B), we further obtain

trace(dFT ∂L ) ≡ trace[dF¯ T   ∂L

]                                           (32)


∂F

Plugging Eq. (29) into Eq. (32), we obtain

K ∂F¯ K


trace(dFT

∂L                       5

) ≡ trace

l¹(F)dFr¹(F) + Σ

l²(F)(dF)Tr²(F)

T   ∂L

¯


∂F                             ⁱ

i=1

i                       j

j=1

ʲ          ∂FK

(33)


≡ trace,

dFT

Σ Σi=1

l¹(F)T

  ∂L

∂F¯ K

r¹(F)T +

Σj=1

r²(F)(

  ∂L

∂F¯ K

)Tl²(F)

Σ,.

Compare the LHS and RHS of Eq. (33), we obtain


∂L  = Σ Σ l1(F)T   ∂L

r¹(F)T + Σ r²(F)(   ∂L

)Tl²(F)Σ.                      (34)

Eq. (34) gives the backward path which takes ∂L/∂FK as input and outputs ∂L/∂F.

14


Under review as a conference paper at ICLR 2020

(a) Phase 1                                                                     (b) Phase 2

Figure 4: The accuacy changes in the training process.

C    APPENDIX

In this section, we plot the accuracy change of the proposed RUN in the whole training process. We
test it on the CUB dataset.  We use the proposed RUN with compact bilinear pooling implemented
by tensor sketch and set the feature dimension as 10K. As we mentioned, the training is two-phase.
We plot the accuracy change in each phase in Figure 4.

15

