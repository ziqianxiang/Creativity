Under review as a conference paper at ICLR 2020
Rate-Distortion Optimization Guided Autoen-
coder for Generative Approach
Anonymous authors
Paper under double-blind review
Ab stract
In the generative model approach of machine learning, it is essential to acquire an
accurate probabilistic model and compress the dimension of data for easy treat-
ment. However, in the conventional deep-autoencoder based generative model
such as VAE, the probability of the real space cannot be obtained correctly from
that of in the latent space, because the scaling between both spaces is not con-
trolled. This has also been an obstacle to quantifying the impact of the variation
of latent variables on data. In this paper, we propose Rate-Distortion Optimization
guided autoencoder, in which the Jacobi matrix from real space to latent space has
orthonormality. It is proved theoretically and experimentally that (i) the probabil-
ity distribution of the latent space obtained by this model is proportional to the
probability distribution of the real space because Jacobian between two spaces is
constant; (ii) our model behaves as non-linear PCA, where energy of acquired la-
tent space is concentrated on several principal components and the influence of
each component can be evaluated quantitatively. Furthermore, to verify the use-
fulness on the practical application, we evaluate its performance in unsupervised
anomaly detection and it outperforms current state-of-the-art methods.
1	Introduction
Capturing the inherent features of a dataset from high-dimensional and complex data is an essential
issue in machine learning. Generative model approach learns the probability distribution of data,
aiming at data generation by probabilistic sampling, unsupervised/weakly supervised learning, and
acquiring meta-prior (general assumptions about how data can be summarized naturally, such as
disentangle, clustering, and hierarchical structure (Bengio et al., 2013; Tschannen et al., 2019)). It
is generally difficult to directly estimate a probability density function(PDF) Px(x) of real data x.
Accordingly, one promising approach is to map to the latent space z with reduced dimension and
capture PDF Pz(z). In recent years, deep autoencoder based methods have made it possible to
compress dimensions and derive latent variables. While there is remarkable progress in these areas
(van den Oord et al., 2017; Kingma et al., 2014; Jiang et al., 2016), the relation between x and z in
the current deep generative models is still not clear.
Variational autoencoder (VAE) (P.Kingma & Welling, 2014) is one of the most successful generative
models for capturing latent representation. In VAE, lower bound of log-likelihood of P x(x) is
introduced as ELBO. Then latent variable is obtained by maximizing ELBO. In order to maximize
ELBO, various methods (Alemi et al., 2018; Zhao et al., 2019; Brekelmans et al., 2019) have been
proposed. However, many previous works did not care about the value of Jacobian between two
spaces, despite the fact that the ratio between Pz(z) and Px(x) is equal to the Jacobian. Even
in models that provide more flexible estimation (Johnson et al., 2016; Liao et al., 2018; Zong et al.,
2018), this point is overlooked.
Here, when we turn our sight toward acquiring meta-prior, it is straightforward to evaluate the quan-
titative influence of each latent variable on the given metrics between data x1 and x2 . To do so, the
scale of the latent variable should be appropriately controlled so that the changes in latent variables
is proportional to the changes of given metrics in data space. In addition, this scaling should be ad-
justed according to the definition of the metrics. For instance, with respect to image quality metrics,
different meta-prior should be derived from mean square error (MSE) and the structural similarity
(SSIM). Considering the mechanism of principal component analysis (PCA) would be one of the
1
Under review as a conference paper at ICLR 2020
directions to solve it. In PCA, PCA components are derived by optimal orthonormal transforma-
tion, then the importance of each component can be identified quantitatively by their variance. It
should be noted that orthogonality is not enough for quantitative analysis, but both orthogonality
and normalized scaling, namely, orthonormality is required. This fact implies that, if Jacobi ma-
trix of autoencoder has orthonormality, the characteristics of acquired latent space can be evaluated
quantitatively. To deal with this, we propose RaDOGAGA (Rate-Distortion Optimization Guided
Autoencoder for Generative Approach), based on the rate-distortion optimization (RDO) which has
been widely used in image compression with orthonormal transform coding (Sullivan & Wiegand
(1998)). In this paper, we show the effect of RaDOGAGA in the following steps.
(1)	We prove that RaDOGAGA has the following property theoretically and experimentally.
•	Jacobi matrix between real space and latent space leads to be constant-scaled orthonormal.
So the response of the minute change of z to the real space data x is constant at any z.
•	Because of constant Jacobian (or pseudo-Jacobian), P x(x) and P z(z) are almost pro-
portional. Therefore, Px(x) can be estimated, by directly maximizing log-likelihood of
parametric PDF Pzψ(z) in reduced-dimensional space, without considering ELBO.
•	When univariate independent distribution is used to estimate P z(z) parametrically, it be-
haves as ”continuous PCA” where energy is concentrated on several principal components.
(2)	Thanks to this property, RaDOGAGA achieve the state-of-the-art performance in anomaly de-
tection task with four public datasets, where probability density estimation is important.
(3)	We show that our approach can directly evaluate how the z impact on the given metrics in real
space. This feature is promising to further interpretation of latent variables.
2	RELATED WORK
Flow based model: Flow based generative models generates astonishing quality of image
(Kingma & Dhariwal, 2018; Dinh et al., 2014). Flow mechanism explicitly takes Jacobian of x
and z into account. The transformation function z = f (x) is learned, calculating and preserving
Jacobian of x and Z. Unlike ordinary autoencoder, which reverse Z to x with function g (∙) differ-
ent from f (∙), inverse function transforms Z as x = f- 1(Z). Since the model preserves Jacobian,
Pz(x) can be estimated by maximizing log likelihood of P z(Z) without considering ELBO. Al-
though, in this approach, f (∙) need to be bijection. Because of this limitation, it is difficult to fully
utilize the flexibility of neural networks.
Interpretation of latent variables: While it is expected to acquire meta-prior by deep autoencoder,
interpreting latent variables is still challenging. In recent years, research aiming to acquire disentan-
gled latent variables, which encourages them to be independent, is flourishing (Lopez et al., 2018;
Chen et al., 2018; Kim & Mnih, 2018; Chen et al., 2016). With these methods, qualitative effects
for disentanglement can be seen. For example, when a certain latent variable is displaced, image
changes corresponding to specific attributes (size, color, etc.). Some works also propose quantitative
metrics for meta-prior. In beta-VAE (Higgins et al., 2017), the metric evaluates the independence
of latent variables by solving the classification task. Actually, the Jacobi matrix of VAE is orthogo-
nal (ROHnek et al., 2019), which enables to make latent variables disentangled implicitly. However,
orthonormality is not supported and it is difficult to define the metric which evaluates the effect of
latent variables to the metrics between data directly.
Deep image compression with rate-distortion optimization: Rate-distortion (RD) theory
(Berger, 1971) is a part of Shannon’s information theory for lossy compression which formulates
the optimal condition between information rate and distortion. Then RD theory for Gaussian source
with memory has been further extended to transform coding (Goyal, 2001) for image and audio lossy
compression where orthonormal transforms such as KarhUnen-LOeVe transform (KLT) (Rao & Yip
(2000)) and discrete cosine transform (DCT) are used for decorrelation. Furthermore, rate-distortion
optimization (RDO) has been widely used in image compression (Sullivan & Wiegand, 1998). In
RDO, a cost L = R + λ ∙ D is minimized at given Lagrange parameter λ to realize the best trade-off
between rate R and distortion D such that L = R + λ ∙ D becomes a tangent line of true Rate-
Distortion curve. Recently, deep learning based image compression (Balle et al., 2018; Zhou et al.,
2019) has been proposed. In these works, instead of orthonormal transform with L2 norm metrics
in the conventional lossy compression method, a deep autoencoder is trained with flexible metrics
2
Under review as a conference paper at ICLR 2020
such as SSIM for RDO. To understand the success of these works, we prove theoretically that RDO
guides a Jacobi matrix of deep autoencoder to orthonormal in the space depending on the metrics.
In the next section, we explain the idea of RDO guided autoencoder and its relationship with VAE.
3	Overview of Rate-Distortion optimization guided approach
3.1	Derivation from Rate-Distortion optimazation in transform coding
Figure 1 shows the overview of our idea inspired by Rate-Distortion optimization of transform cod-
ing. In the transform coding, the optimal method to encode data with Gaussian distribution is as
follows (Goyal, 2001). At first, the data are transformed deterministically to decorrelated data us-
ing the orthonormal transform such as KLT and DCT. Then these decorrelated data are quantized
stochastically with uniform quantizer for all channels such that quantization noise for each channel
is equal. At last optimal entropy encoding is applied to quantized data where the rate can be calcu-
lated by the logarithm of symbol’s estimated probability. From this fact, we have an intuition that
the Jacobi matrix of autoencoder becomes orthonormal if the data were compressed based on Rate-
Distortion optimization with uniform quantized noise and parametric distribution of latent variables.
Inspired by this, we propose autoencoder which scales latent variables according to the definition of
metrics of data, without limitation of the transformation function as in Flow models. Thanks to this
feature, our scheme can estimate P z(z) quantitatively, which is suitable for clustering and anomaly
detection. Furthermore, in the case factorized distribution is used for Pz(z), our model behaves as
continuous PCA. This property is considered to promote the interpretation of latent variables.
3.2	Relationship with VAE
There is a number of VAE studies that taking rate-distortion trade-off into account. In VAEs, instead
of maximizing log-likelihood of P(x) directly, maximizing ELBO. In beta-VAE (Higgins et al.,
2017), Objective function LVAE is described as LVAE = Lrec - β ∙ Lkl. Lkl is KL divergence
between encoder output and prior distribution, usually Gaussian distribution. By changing β, the
rate-distortion trade-off at desirable rate can be realized as discussed in (Alemi et al., 2018).
When the relation between beta-VAE and RDO in image compression is considered, both are analo-
gous to each other. That is, β-1 is corresponding to λ, -Lkl is corresponding to a rate R, and Lrec
is corresponding to a distortion D. However, the probability distribution models of latent variables
are quite different. VAE uses a fixed prior distribution. This causes a non-linear scaling relationship
between real data and latent variables.
Figure 2 shows the conditions to achieve Rate-Distortion optimization in both VAE and RDO guided
autoencoder. In VAE, precise control of noise distribution is required for each data and channel as
suggested in Brekelmans et al. (2019) in order to compensate for the nonlinearities between the data
space and the latent space. Jacobi matrix becomes orthogonal as proved in RolInek et al. (2019).
However, Jacobian is not constant because of non-linear scaling. In RDO guided autoencoder, uni-
form noises are added to all channels. Instead, parametric probability distribution should be used in
order to make the scaling linear between the data space and the latent space. As a result, the Jacobi
matrix becomes orthonormal because both orthogonality and scaling normalization are simultane-
ously achieved. As discussed above, the precise noise control in VAE and distribution parameter
optimization in RDO guided autoencoder are essentially the same. Accordingly, complexities in
both methods are estimated to be at the same degree.
4	METHOD AND THEORETICAL PROPERTY
4.1	METHOD
Our model is based on the Rate-Distortion optimization of the autoencoder for the hyperprior pro-
posed in Bane et al. (2018) for image compression with some modification. The cost function con-
sists of (i) reconstruction error D between input data and decoder output with noise to latent variable
and (ii) Rate R of latent variable. This is analogous to beta-VAE where λ = β-1.
L = R + λ ∙ D	(1)
3
Under review as a conference paper at ICLR 2020
Model of TranSfOrm coding based on Rate-Distortion Theory
Image  OrthonormaI
Data	Transform	Decorrelated
data
Source Coding
Uniform
Quantization /
Entropy coding
Compressed
Data
Channel Coding
Analogy between transform coding and RDO guided autoencoder
Method
Transform
coding
Source Coding
(DeterminiStiC)
Condition:
Orthonormal
KLT / PCA / DCT
transform________
Channel Coding Rate-Distortion
(StoChaStiC)	- IRelatiOn
Condition:	I Result:
Uniform	∣A Rate-Distortion
quantization / '-J Optimization is
EntrOPy COdina I achieved________
COnditiOns to achieve Rate-Distortion OptimiZatiOn
Method	PDF model	Noise	Jacobi Matrix
VAE with fixed prior	Fixed as prior	Precise control for each data and ChanneIs		Orthogonal (Variable JaCobian)	
RDO guided AUtoenCoder	Variable parametric PDF	Uniform for all data and channels	OrthOnOrmal (Constant Jacobian)
RelatiOnship between PDE Noise, and JacObi MatriX
RDO guided
Autoencoder
Expected Result:
Autoencoder with
orthonormal
Jacobi matrix
Condition:
Uniform noise
for all channel /
Rate estimation
Condition:
Rate-Distortion
based Loss
function
VAE
χ-→~r
Orthogonal
(Variable ∖
Jacobian)
Precisely controlled noise
Figure 2: The condition to optimize rate-
distortion in VAE and our models
RDO guided autoencoder
Uniform noise
Figure 1: Overview of our idea based on RDO.
The specific method is described as follows. First, let x be M -dimensional domain data(x ∈ RM)
and P x(x) the probability of x. Then x is converted to N -dimensional latent space z ∈ RN by
encoder. Let fθ(x), gφ(z), and Pzψ(z) be parametric encoder, decoder, and probability distribution
function of latent space with parameters θ, φ, and ψ. It is noted that encoder and decoder are
deterministic unlike VAE. Then latent variable Z and decoded data 比 are generated as bellow:
Z = fθ (M	x = gφ (Z)	(2)
Let ∈ RN be noise with each dimension being independent with an average of 0 and a equal
variance of σ2 to emulate uniform quantization.
e =	(eι, e2,	..Cn), E 同=0,	E 匕∙	e/	=	6切∙ σ2	(3)
Then, given the sum of latent variable Z and noise e, the decoder output X is obtained. This is
analogous to the stochastic sampling and decoding procedure in VAE.
X = 9ψ (Z + e)	(4)
Here, the cost function is defined by Eq. (5) with some modifications from Eq. (1).
L = - log(PZψ(Z)) + λι ∙ h (D (x, X)) + λ2 ∙ D (X, X)	(5)
In this equation, the first term is corresponding to the estimated rate of the latent variables. In
Balle et al. (2018), a rate for quantized symbol is calculated by - Rθ055 log(PZψ(Z))dZ. In our
model, this is replaced by - log(P Zψ(Z)) for simplicity. To model PZψ (Z), factorized probability
model or Gaussian mixture model (GMM) can be used for instance. D(X1, X2) in the second and
the third term is a metrics function between X1 and X2. Actually, the second and the third term is
decomposition of D(x,gφ(fθ(x)+e))〜D(x,g@(fθ(x)))+ D(gφ(fθ(x)),gφ(fθ(x)+ e)) as shown
in Rolinek et al. (2019). The second term purely calculate reconstruction loss and the third term
only influences the scaling of latent space. Accordingly, λ1 controls the degree of reconstruction,
and λ2(〜β-1 of beta-VAE) controls a scaling between the data space and the latent space. By
this decomposition, we can adjust balance between reconstruction loss and other losses flexibly and
easily lead better performance. h(∙) in the second term of Eq. (5) is a monotonically increasing
function. According to the choice of h (∙), the degree of reconstruction accuracy can be controlled.
Roughly speaking, when h(d) = d is chosen, the training much focus on fitting the distribution and
entropy. On the other hand, h(d) = log(d) encourages better reconstruction, since the curve of loss
is steep around 0. The effect of h(d) is further discussed in Appendix D. Then, Eq. (5) is averaged
according to distributions, x 〜 Px(x) and e 〜 P(e). For metrics function D(∙, ∙), a variety of
function is applicable such as MSE, SSIM, and so on. As long as the function can be approximated
by the following quadratic form in the neighborhood of x, the property of the model is hold
D(x, x + δx)〜tδx A(x) δx = ∣∣L(x) δxk2	(6)
Here, δx is arbitrary micro variation of x, A(x) is M × M positive definite matrix depending on
x , and L(x) is Cholesky decomposition of A(x). For instance, when D(∙, ∙) is square of L2 norm
as in Eq. (7), A(x) and L(x) are Identity matrices.
D(x1, x2) = kx1 - x2k2	(7)
4
Under review as a conference paper at ICLR 2020
In the case of SSIM (Wang et al., 2001) metric which is close to subjective image quality, a cost
(1 - SSIM ) can be also approximated in a quadratic form with positive definite matrix. This is
explained in Appendix C. By deriving parameters that minimize this value, the encoder, decoder,
and probability distribution of the latent space are trained as Eq. (8).
θ,φ,ψ = argmin(Ex〜Px(X), e〜P(e)[ L ])	(8)
θ,φ,ψ
Figure 3: Architecture of RaDOGAGA
4.2	THEORETICAL PROPERTY OF THE MODEL
In this section, we show the theoretical property of the model. First of all, we provide a rough
sketch of the theory. Let J = |dx/dz | be a Jacobian between the data space x and the latent space.
Here, R and λD in Eq. (1) can be approximated by - log P x(x) - log(J) and Cσ,λJ2 respectively
where Cσ,λ is a constant depending on σ and λ. As a result, the Jacobi matrix with J = 1 / /2Cσ,λ
minimizes Eq. (1). Because Jacobi matrix is orthogonal (RolInek et al., 2019) and the Jacobian is
constant, the Jacobi matrix becomes orthonormal. A full proof is described in Appendix A. In this
section, we mainly explain important points. Let xD be rescaled space of x according to the metrics
D(∙, ∙), where dx and dxd are differentials and L(x) is a Jacobi matrix between two spaces.
dxD = L(x)dx
(9)
When D(∙, ∙) is square of L2 norm, both spaces of xd and x are equivalent. As described above,
each column of the Jacobian matrix of latent space z and rescaled space xD are constant multiple of
orthonormal basis regardless of the value of z and x after training based on Eqs. (5) and (8). Here,
δij denotes Kronecker delta and matrix tT denotes a transpose of T .
t
L(x)
1
2 λ 2 σ 2
(10)
From the orthonormal property, our model has several features. The first feature is that The distance
derivatives of both real and latent spaces has a linear relationship. Therefore, the metrics between x
and x + dx is proportional to kdzk2 as derived by Eqs. (6), (9) and (10) where dz is a differential.
2
_	1
=2 λ 2 σ 2
D(x, x + dx) = kL(x)dxk2 = kdxD k2
N
∂xD
石' d d'
i=1	i
. Ild n∣∣2
(11)
D(x, x + dx)/IdzI2 = IdxDI2/IdzI2 = const.	(12)
This equation also implies that the L2 distance of two data in the latent space is proportional to the
distance between corresponding data in the real space which is defined by the integration of the root
of metrics derivative.
The second feature is constant Jacobian. In case of M = N, the Jacobian matrix dxD/dz is a
square matrix, and each column is the same as 1 /(√2λ2 ∙ σ) times orthonormal basis. For this
reason, the Jacobian is a constant regardless of the value of z as shown below:
d xd
^dT
(N/2)
(13)
In this case, the probability distribution ofxD andz is proportional because of the constant Jacobian.
For the case of M > N, we assume the situation where most energy is efficiently and effectively
mapped to N-dimensional latent space. Then the product of the singular values of SVD for a Jacobi
5
Under review as a conference paper at ICLR 2020
matrix can be regarded as a PseUdo Jacobian between the real space and the latent space. Since all
of the N singular values are 1 /(√2λ2 ∙ σ), the pseudo Jacobian is also a constant.
The third featUre is proportional probability density between data space and latent space. BecaUse
Jacobian or pseudo Jacobian J is constant, the following equation holds. PxD(xD) and P z(z) are
are true PDFs for each space xD and z respectively.
Pz(Z)〜J ∙ Pxd(xd) H Pxd(Xd)	(14)
The forth is that the log-likelihood of PDF of the domain data can be maximized without considering
ELBO. This is revealed by the transformation of Equation (5). Let PXD (XD) be estimated proba-
bility of xd. Because the Jacobian J is constant, PxD (xd) can be approximated by J-1 ∙ Pzψ (Z).
Accordingly, the average of the first term in Equation (5) by x 〜Px (x) can be transformed as
follows.
ʌ
Ez〜Pz(Z))[Tοg(Pzψ(Z))] ' -Exd〜Px(XD))[log(PXd(Xd)] — log(J)	(15)
Consequently, minimization of Equation (15) is equivalent to the log-likelihood maximization of
ʌ
PxD(xD).
The fifth is ”continuous PCA” feature when the following factorized probability model is used.
N
Pzψ(Z) = Y Pziψ (Zi)	(16)
i=1
As shown in FactorVAE (Kim & Mnih, 2018), the use of factorized model minimizes mutual infor-
mation among latent variables and encourages disentangling. Combining with the first character of
our model described before, the variance values of each latent variable show the quantitative impact
on data space like linear PCA. The detailed derivation is explained in Appendix B. .
5	Experiment
5.1	PROBABILITY DENSITY ESTIMATION WITH TOY DATA
In this section, we describe our experiment using toy data to demonstrate whether the probability
density of the input data P x(x) and that of estimated in the latent space P z(Z) are proportional to
each other as in theory. First, we sample data s = (s1, s2...s10,000) from three-dimensional Gaus-
sian distribution consists of three-mixture-components with mixture weight π = (1/3, 1/3, 1/3),
mean μk = (μk 1, μk2μ3), and covariance Σk = diag(σk 1, σk2, σk3). k is the index for mixture
component. Then, We scatter S with uniform random noise U ∈ R3×16, UdmI 〜 Ud (一 11,11), where
d and m are index for dimension of sampled data and scattered data. The Uds are uncorrelated with
each other. We produce 16-dimensional input data x with a sample number of 10,000 in the end.
3
x =	udsd
d=1
(17)
The appearance probability of the input data P x(x) is equivalent to the generation probability of s.
5.1.1	CONFIGURATION
In the experiment, we estimate the Pzψ (Z) using GMM with parameter ψ as in DAGMM
(Zong et al., 2018). Instead of EM algorithm, GMM parameters are learned using Estimation
Network (EN), which consists of multi-layer neural network. When the GMM has K mixture-
components and L is the size of batch samples, EN outputs the mixture-components membership
prediction as K-dimensional vector γb as follows:
p = EN(Z; ψ), γb = sof tmax(p)
(18)
T…	..	.一个	ʌ
K-th mixture weight φk, mean μk,
(19) and (20).
covariance Σbk, and entropy
R of Z are further calculated by Eqs.
L
π k = X blk/L,
l=1
LL
μ k = X blk Zll X blk,
l=1	l=1
LL
∑k = £b,k(Zi-μk)(Zi-μk)T/f/k (19)
l=1	l=1
6
Under review as a conference paper at ICLR 2020
R = - log
(20)
Overall network is trained by Eqs. (5) and (8). In this experiment, we set D(x1 , x2) as square error
Il x 1 — x2 Il2, and test two types of h(∙), h(d) = d and h(d) = log(d). We denote models trained
With these h (∙) as RaDOGAGA(d) and RaDOGAGA(log(d)). As a comparison method, DAGMM
is used. DAGMM also consists of encoder, decoder, and EN. In DAGMM, to avoid falling into the
trivial solution that entropy is minimized when the diagonal component of the covariance matrix is
0, the inverse of the diagonal component is added to the cost function as Eq. (21):
KN
L = k力—空k2 + λ 1 ∙ (- log(Pzψ(N))) + λ2P(∑),	P(∑) = E E ∑kii (21)
k=1 i=1
The only differences between our model and DAGMM are (i) RDO mechanism is introduced (ii) the
third regularization term in Eq. (21) is eliminated, since our model adjust latent scale appropriately
so as not to falling into trivial solution. Thus, model complexity such as the number or parameter
is the same. For both RaDOGAGA and DAGMM, the autoencoder part is constructed with fully
connected (FC) layers with sizes of 64, 32, 16, 3, 16, 32, and 64. For all FC layers except for
the last of the encoder and the decoder, we attach tanh as the activation function. The EN part
is also constructed with FC layer with a size of 10, 3. For the first layer, we attach the tanh as
activation function and dropout (ratio=0.5). For the last one, softmax is attached. (λ1, λ2) is set as
(1 × 10-4, 1 × 10-9), (1 × 106, 1 × 106) and (1 × 103, 1 × 104) for DAGMM, RaDOGAGA(d) and
RaDOGAGA(log(d)) respectively. Optimization is done by Adam optimizer (Kingma & Ba, 2014)
with learning rate 1 × 10-4 for all model. We set σ2 as 1/12.
(a) input source s
(b) DAGMM
15
0 z2
-15
25
(c) RaDOGAGA(d)
(d) RaDOGAGA(log(d))
Figure 4:	Plot of source of input data s and latent variable z . Normalized PDF value corresponds to
a color bar located left of (a). Even though both DAGMM and RaDOGAGA capture three mixture
components, the tendency of PDF distribution in DAGMM looks different from input data source.
Points with high PDF are not concentrated on the center of the clustering especially in upper two.
(a) DAGMM
(b) RaDOGAGA(d)
(c) RaDOGAGA(log(d))
Figure 5:	Plot of of P x(x) (x-axis) and Pzψ (z) (y-axis). In RaDOGAGA, P x(x) and Pzψ (z) are
proportional while we can’t observe that in DAGMM. Thanks to this trait, we can estimate P x(x)
directly from Pzψ(z) without ELBO
5.1.2 Result
Figure 4 displays the distribution of input data source s and latent variable z . Even though both
methods can capture that s is generated from three mixture-component, there is a difference in PDF.
Since the data is generated from GMM, the sample getting closer to the center of clustering, the
PDF getting higher. Although, in DAGMM, this tendency looks distorted. This difference is fur-
ther demonstrated in Fig 5 which shows a plot of P x(x)(x-axis) against Pzψ (z)(y-axis). In our
method, P x(x) and Pzψ (z) are approximately proportional to each other as in theory while we
7
Under review as a conference paper at ICLR 2020
cannot observe such proportionality in DAGMM. This difference is quantitatively obvious as well.
That is, correlation coefficients between P x(x) and Pzψ (z) are 0.691 (DAGMM), 0.997 (RaDO-
GAGA(d)), and 0.999 (RaDOGAGA(log(d))). It also turns out that the correlation coefficient of
RaDOGAGA(log(d)) is higher than that of RaDOGAGA(d). Actually, when Pzψ (z) is sufficiently
fitted, h(d) = log(d) makes P x(x) and Pzψ (z) be proportional more rigidly. On the other hand,
h(d) = d makes the scale of latent space slightly bent in order to minimize entropy function, allow-
ing relaxed fitting of Pzψ(z). More detail is described in Appendix D.
5.2	ANOMALY DETECTION TASK USING REAL DATA
In this section, we examine whether the clear relationship between Px(x) and P z(z) is useful in
the anomaly detection task using real data. We use four public datasets1, KDDCUP99, Thyroid, Ar-
rhythmia, and KDDCUP-Rev. The (instance number, dimension, anomaly ratio(%)) of each dataset
is (494021, 121, 20), (3772, 6, 2.5), (452, 274, 15), and (121597, 121, 20) respectively. Detail of
datasets is described in Appendix E.
5.2.1	EXPERIMENTAL SETUP
We follow the setting in Zong et al. (2018). Randomly extracted 50% of the data is assigned to
training and the rest to testing. During training, only normal data is used. During the test, the R for
each sample is calculated as the anomaly score, and if the anomaly score is higher than a threshold,
it is detected as an anomaly. The threshold is determined by the ratio of anomaly data in each data
set. For example, in KDDCup99, data with R in the top 20 % is detected as an anomaly. As metrics,
precision, recall, and F1 score are calculated. We run 20 times for each dataset split by 20 different
random seeds.
5.2.2	BASELINE MODEL
Same as in the previous section, DAGMM is taken as the baseline method. We also compare with the
scores reported in previous works in which same experiments were conducted (Zenati et al., 2018;
Song & Ou, 2018; Liao et al., 2018).
5.2.3	CONFIGURATION
As in Zong etal. (2018), in addition to the output from the encoder,匕肃?，2 and 口引卷「仆？ are
concatenated to z . It is sent to EN. Note that z is sent to the decoder before concatenation. Other
configuration except for hyper parameter is same as in the previous experiment. Hyper parameter
for each dataset is described in Appendix E.
5.2.4	Results
Table 1 reports the average scores and standard deviations (in brackets). Comparing DAGMM and
RaDOGAGA, RaDOGAGA has a better performance regardless of types of h(∙). Note that, our
model does not increase model complexity at all. Simply introducing the RDO mechanism into
the autoencoder has a valid efficacy for anomaly detection. Moreover, our approach achieves state-
of-the-art performance compared to other previous works in which same datasets is used. Clear
relationship between P x(x) and P z(z) by our model is considered to be effective in the task of
anomaly detection where the estimating probability distribution is important. In RaDOGAGA, when
we compare result of RaDOGAGA(d) and RaDOGAGA(log(d)), either of one is not always superior.
As described in section 5.1 and Appendix D, h(∙) can be an option depending on fitting flexibility
of Pz(z).
5.3	QUANTIFYING THE IMPACT OF LATENT VARIABLES ON METRICS AND
BEHAVIOR AS PCA
In this section, we confirm that (i) the latent variables in our model behaves as PCA components
and (ii) the impact of each latent variable on the metrics function can be quantified. When dz in Eq.
1Dataset can be dowonload from (https://kdd.ics.uci.edu/) and (http://odds.cs.stonybrook.edu)
8
Under review as a conference paper at ICLR 2020
Table 1: Average and standard deviations(in brackets) of Precision, Recall and F1
Dataset	Methods	Precision	Recall	F1
KDDCup	ALAD * INRF * VAE * GMVAE * DAGMM * DAGMM+1 RaDOGAGA(d) RaDOGAGA(log(d))	0.9427(0.0018)^^0.9577(0.0018)^^0.9501(0.0018) 0.9452(0.0105)	0.9600(0.0113)	0.9525(0.0108) 0.7805	0.7903	0.7854 0.952	0.9141	0.9326 0.9297	0.9442	0.9369 0.9427(0.0052)	0.9575(0.0053)	0.9500(0.0052) 0.9550(0.0037)	0.9700(0.0038)	0.9624(0.0038) 0.9563(0.0042)	0.9714(0.0042)	0.9638(0.0042)
Thyroid	GMVAE * VAE * DAGMM * DAGMM+1 RaDOGAGA(d) RaDOGAGA(log(d))	0.7105	0.5745	0.6353 0.3395	0.3592	0.3491 0.4766	0.4834	0.4782 0.4656(0.0481)	0.4859(0.0502)	0.4755(0.0491) 0.6313(0.0476)	0.6587(0.0496)	0.6447(0.0486) 0.6562(0.0572)	0.6848(0.0597)	0.6702(0.0585)
Arrythmia	ALAD * VAE * GMVAE * DAGMM * DAGMM+1 RaDOGAGA(d) RaDOGAGA(log(d))	0.5000(0.0208)^^0.5313(0.0221)^^0.5152(0.0214) 0.3328	0.3392	0.3360 0.4375	0.4242	0.4308 0.4909	0.5078	0.4983 0.4985(0.0389)	0.5136(0.0401)	0.5060(0.0395) 0.5353(0.0461)	0.5515(0.0475)	0.5433(0.0468) 0.5294(0.0405)	0.5455(0.0418)	0.5373(0.0411)
KDDCup-rev	DAGMM * DAGMM+1 RaDOGAGA(d) RaDOGAGA(log(d))	-0.937	0.939	0.938 0.9778(0.0018)	0.9779(0.0017)	0.9779(0.0018) 0.9768(0.0033)	0.9827(0.0012)	0.9797(0.0015) 0.9864(0.0009)	0.9865(0.0009)	0.9865(0.0009)
* Score is cited from Zenati et al.(2018)(ALAD), Song & Ou (2018)(INRF), Liao et al.(2018)(VAE, GM-
VAE) and Zong et al. (2018) (DAGMM) respectively.
tDAGMM+ is our implementation. Note that We also test same configuration as in Zong et al. (2018) and
achieve similar score as reported (shown in Appendix E).
(12) is approximated by a small displacement δ, the ratio D(g(Z), g(Z + δ))∕∣δk2 will be almost
constant regardless of z and δ . If the ratio of each component zi in the latent space is equivalent,
the latent space can be regarded as isometric. So, we evaluate this ratio in each component. Let δi
be a vector t(0,…，g ∙, 0) where only i-th component has a minute value E. Then Di(Z) denotes
D(((z)g(+δ))for i-th component. In the trained model, we encode the image xι and obtain Zl.
Then, Di0 (Zl ) is calculated for each sample. Finally, the average across all samples is measured.
This operation is conducted to each dimension of Z independently. We also observe the distribution
of Z and how the image looks different in response to the variation of component zi .
To train model, we use CelebA dataset (Liu et al., 2015), which consists of 202,599 celebrity images.
The images are center-cropped so that the image size is 64 x 64.
5.3.1	CONFIGURATION
In this experiment, factorized distributions (Balleetal., 2018) is used to estimate Pzψ(Z). For
comparison, we evaluate beta-VAE. Both models are constructed with same depth of CNN and FC
layers, with 256-dimensional Z . Detail of networks and hyper parameter is written in Appendix F.
For RaDOGAGA, we set D(x1, x2) as 1 - SSIM (x1, x2) and h as h(d) = d. For beta-VAE,
reconstruction loss is also 1 - SSIM (x1, x2). SSIM (x1, x2) is defined by Eq. (61).
5.3.2	Result
Both models are trained so that the SSIM between input and reconstructed image is around 0.93.
Figure 6a and 6b show the variance of each component in the latent variables Z arranged in de-
scending order. The red line is the cumulative relative ratio of the variance. In Fig. 6b, variance is
concentrated in a specific dimension. On the other hand, in Fig. 6a, beta-VAE is trained so that each
9
Under review as a conference paper at ICLR 2020
latent variable is fitted to a Gaussian distribution with mean 0 and variance 1, there is no significant
difference in the variance of each latent variable. Figures 6c and 6d respectively depicts the aver-
age of Di0 (z) of each of the top nine dimensions with the largest variance of z. In beta-VAE, the
Di0 (z) varies drastically depending on the dimension i which shows anisometric latent space, while
in RaDOGAGA, it is approximately constant regardless of i which shows isometric latent space.
Figure 7 shows decoder outputs when each component zi is traversed from -2σ to 2σ, fixing rest
of z as mean. From the top, each row corresponds to z0, z1, z2 ..., and the center column is mean.
In Fig. 7b, the image changes visually in any dimension of z, while in Fig. 7a, depending on the
dimension i, there are cases where no significant changes can be seen (such as z3 , z4 , and so on).
This result means that, in RaDOGAGA, the variance of z directly corresponds to the visual impact
and the metrics D(x1 , x2), behaving as PCA. Besides, since Di0(z) is constant, the latent space
shows isometric feature and the variance is regarded as a quantitative importance.
(a) beta-VAE
0.04
0.02
0.00
7.50
OKZ)(I X 10-2)
7.80
6
4
2
D'i (z)(l X 10-2)
8
0 Z0 Z1 Z2 Z3 Z4 Z5 Z6 Z7 Z8
(d) RaDOGAGA
(b) RaDOGAGA	(C) beta-VAE
Figure 7: Latent space traversal of z with top-9 variance. In beta-VAE, the variance of z is uncorre-
lated to difference of image, while they are directly related in our model.
Figure 6: Variance of z (two on the left) and Di0 (z)(two on the right). Compared with beta-VAE, in
our model, the variance is concentrated in a few dimensions. Furthermore, the influence each z to
the real image is constant. These results demonstrate that the latent variable in our model behaves
as PCA components and the importance for the domain data can be quantified.
—2。	〃	+2。
(b) RaDOGAGA
6 Conclusion
In this paper, we propose RaDOGAGA that learns parametric probability distribution and autoen-
coder simultaneously based on rate-distortion optimization. It was proven that the probability dis-
tribution of the latent variables obtained by the proposed method is proportional to the probability
distribution of the input data theoretically and experimentally. This property is validated in anomaly
detection achieving state-of-the-art performance. Moreover, our model has the trait as PCA which
likely promotes interpretation of latent variables. For the future work, we will conduct experiments
with different types of metrics functions that derived from semantical task, such as in categorical
classification. Meanwhile, as mentioned in Tschannen et al. (2019), considering the usefulness of
latent variables in downstream task is another research direction to explore.
10
Under review as a conference paper at ICLR 2020
References
Alexander Alemi, Ben Poole, Ian Fischer, Joshua Dillon, Rif A. Saurous, and Kevin Murphy. Fixing
a broken elbo. In Proceedings of the 35th International Conference on Machine Learning, pp.
159-168. PMLR, 2018.
Johannes Balle, Valero Laparra, and Eero P Simoncelli. Density modeling of images using a gener-
alized normalization transformation. arXiv preprint arXiv:1511.06281, 2015.
Johannes Balle, David Minnen, Saurabh Singh, Sung Jin Hwang, and Nick Johnston. Variational
image compression with a scale hyperprior. In In International Conference on Learning Repre-
sentations, 2018.
Yoshua Bengio, Courville Courville, and Pascal Vincent. Representation learning: A review and
new perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35:798-
1828, 2013.
Toby Berger (ed.). Rate Distortion Theory: A Mathematical Basis for Data Compression. Prentice
Hall, 1971. ISBN 0137531036.
Rob Brekelmans, Daniel Moyer, Daniel Moyer, and Greg Ver Steeg. Exact rate-distortion in autoen-
coders via echo noise. arXiv preprint arXiv:1904.07199v2, 2019.
Tian Qi Chen, Xuechen Li, Roger B Grosse, and David K Duvenaud. Isolating sources of disentan-
glement in variational autoencoders. In Advances in Neural Information Processing Systems, pp.
2610-2620, 2018.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan:
Interpretable representation learning by information maximizing generative adversarial nets. In
Advances in neural information processing systems, pp. 2172-2180, 2016.
Laurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components esti-
mation. arXiv preprint arXiv:1410.8516, 2014.
Vivek K Goyal. Theoretical foundations of transform coding. IEEE Signal Processing Magazine,
pp. 9-21, sep 2001.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a
constrained variational framework. In ICLR 2017, 2017.
Zhuxi Jiang, Yin Zheng, Huachun Tan, Bangsheng Tang, and Hanning Zhou. Variational
deep embedding: An unsupervised and generative approach to clustering. arXiv preprint
arXiv:1611.05148, 2016.
Matthew Johnson, David Duvenaud, Alexander B. Wiltschko, Sandeep R. Datta, and Ryan P.
Adams. Composing graphical models with neural networks for structured representations and
fast inference. In Advances in Neural Information Processing Systems, 2016.
Hyunjik Kim and Andriy Mnih. Disentangling by factorising. In Proc. of the International Confer-
ence on Machine Learning, pp. 2649-2658, 2018.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Diederik P. Kingma, Danilo J. Rezende, Shakir Mohamed, and Max Welling. Semi-supervised
learning with deep generative models. In Proceedings of the 27th International Conference on
Neural Information Processing Systems - Volume 2, NIPS’14, pp. 3581-3589, Cambridge, MA,
USA, 2014. MIT Press.
Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. In
Advances in Neural Information Processing Systems, pp. 10215-10224, 2018.
11
Under review as a conference paper at ICLR 2020
Weixian Liao, Yifan Guo, Xuhui Chen, and Pan Li. A unified unsupervised gaussian mixture varia-
tional autoencoder for high dimensional outlier detection. In 2018 IEEE International Conference
on Big Data (Big Data),pp. 1208-1217. IEEE, 2018.
M. Lichman. Uci machine learning repository. http://archive.ics.uci.edu/ml., 2013.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.
In Proceedings of International Conference on Computer Vision (ICCV), December 2015.
Romain Lopez, Jeffrey Regier, Michael I Jordan, and Nir Yosef. Information constraints on auto-
encoding variational bayes. In Advances in Neural Information Processing Systems, pp. 6114-
6125, 2018.
Diederik P.Kingma and Max Welling. Auto-encodingvariationalbayes. In ICLR 2014, Banff,
Canada, Apr 2014.
Kamisetty Ramamohan Rao and Pat Yip (eds.). The Transform and Data Compression Handbook.
CRC Press, Inc., Boca Raton, FL, USA, 2000. ISBN 0849336929.
Michal Rollnek, Dominik Zietlow, and Georg Martius. Variational autoencoders pursue pca direc-
tions (by accident). In Proceedings of Computer Vision and Pattern Recognition (CVPR), June
2019.
Yunfu Song and Zhijian Ou. Learning neural random fields with inclusive auxiliary generators.
arXiv preprint arXiv:1806.00271, 2018.
Gary J. Sullivan and Thomas Wiegand. Rate-distortion optimization for video compression. IEEE
Signal Processing Magazine, pp. 74-90, nov 1998.
Michael Tschannen, Olivier Bachem, and Mario Lucic. Recent advances in autoencoder-based rep-
resentation learning. In Third workshop on Bayesian Deep Learning (NeurIPS 2018), Montreal,
Canada, Dec 2019.
Aaron van den Oord, Oriol Vinyals, et al. Neural discrete representation learning. In Advances in
Neural Information Processing Systems, pp. 6306-6315, 2017.
Zhou Wang, Alan Conrad Bovik, Hamid Rahim Sheikh, and Eero P. Simoncelli. Image quality
assessment: from error visibility to structural similarity, volume 13. IEEE Trans. on Image
Processing, 2001.
Sihan Wen, Jing Zhou, Akira Nakagawa, Kimihiko Kazui, and Zhiming Tan. Variational autoen-
coder based image compression with pyramidal features and context entropy model. InIn WORK-
SHOP AND CHALLENGE ON LEARNED IMAGE COMPRESSION, 2019.
Houssam Zenati, Manon Romain, Chuan-Sheng Foo, Bruno Lecouat, and Vijay Chandrasekhar.
Adversarially learned anomaly detection. In 2018 IEEE International Conference on Data Mining
(ICDM), pp. 727-736. IEEE, 2018.
Shengjia Zhao, Jiaming Song, and Stefano Ermon. Infovae: Balancing learning and inference in
variational autoencoders. In Proceedings of the AAAI Conference on Artificial Intelligence, vol-
ume 33, pp. 5885-5892, 2019.
Jing Zhou, Sihan Wen, Akira Nakagawa, Kimihiko Kazui, and Zhiming Tan. Multi-scale and
context-adaptive entropy model for image compression. In In WORKSHOP AND CHALLENGE
ON LEARNED IMAGE COMPRESSION, 2019.
Bo Zong, Qi Song, Martin Renqiang Min, Wei Cheng, Cristian Lumezanu, Daeki Cho, and Haifeng
Chen. Deep autoencoding gaussian mixture model for unsupervised anomaly detection. In In
International Conference on Learning Representations, 2018.
12
Under review as a conference paper at ICLR 2020
A HOW JACOBI MATRIX BECOME A CONSTANTLY SCALED
ORTHONORMAL BASIS
In this appendix section, we prove that all of column vectors in decoder’s Jacobi Matrix have the
same norm and are orthogonal to each other. In other words, each column of Jacobi matrix is a
constantly scaled orthonormal basis.
Here we assume that data space sample x has M dimension, that is x ∈ RM , then encoded to N
dimensional latent space sample y ∈ RN . We also assume that an fixed encoder function y =
finit,θ (x) and a fixed decoder X = ginit,φ(y) are given such that h(D(x, X)) becomes minimal.
y = finit,θ (x)	(22)
X = ginit,φ (y)	(23)
s.t h(D(x, X)) =^ minimal
We further assume that fixed parametric PDF of latent variable y is also given.
P yinit,ψ (y)	(24)
Here, it is noted that this function is not needed to be optimal in a sense of
DKL (Py (y) kP yinit,ψ (y)) where P y (y) is an actual PDF of y.
Under these conditions, we introduce new latent variable z, and y is transformed from z using a
following scaling function a(z) : RN → RN.
y = a(Z) = (aι(Z),a2(Z),…，a，N(Z))	(25)
Here, our goal is to prove Eq. (10) by examining the condition of this scaling function which
minimize the average ofEq. (5) with regard to E 〜P(E).
Because of the assumption of minimal D(x, X), We ignore the second term of Eq. (5). Next, the
PDF of Z can be derived using a Jacobian of a(Z).
PZψ,a(Z) = da(z) ∙ Pyinit,ψ(a(Z))	(26)
dZ
By applying these conditions and equations to Eq. (5), the cost of scaling function a(Z) to minimize
the average is expressed as follows.
La = - log (J d Z，卜 P yinit,ψ (a ( Z))) + λ2 ∙ D (ginit,φ (a (Z + E)), ginit,φ (a( Z )))
(27)
Next, the latent variable is fixed as y0 , and Z0 is defined in the next equation.
Z0 = a-1(y0)	(28)
Then the average of Eq. (27) with regard to E 〜P(E) is expressed as follows.
• Pyinit,ψ (a(ZO)))
z=z0
+ λ 2 • ESP (e) [D (ginit,φ ( a ( Z O + E )), ginit,φ ( a ( Z O)))]
(29)
Then the condition of a(ZO) in the neighborhood of yO is examined when Eq. (29) is minimized.
Here, itis noted that the first term of the right side doesn’t depend on E.
da(Z)
d Z
EGP(€) [Lo∖z=Z0 ] = - log
13
Under review as a conference paper at ICLR 2020
Before examining Eq. (29), some preparation of equations are needed. At first, Jacobi matrix of
a(z) at z = z0 is defined as B using notations of partial differentials bij
∂a I
%/N = Z 0
/ b 11
B	∂y	_ d a (Z) I _	b 21
=∂Z	= "^d^I	=	.
z=z0	z=z0	.
bN1
b 12	…b 1N ∖
b22	.	. .	b2N
.	..	.
.	..
bN2	.	. .	bNN
where bij
∂a
dzj
z=z0
(30)
The vector bi is also defined as follows.
b1i
b 2 i
.
.
.
bNi
(31)
It is clear by definition that bi and B have the following relation.
B = (b 1, b2,…，bN )	(32)
∙-v
∆ij is defined as a cofactor of matrix B with regard to the element bij , and bi is also defined by the
following equation.
∆1i
δ2 i
.
.
.
∆Ni
(33)
The following equations hold by the definition of cofactor.
tbi bi	=	N bki • ∆ki	|B|	(34)
	k=1		
d ∣B∣ 	 = dbij	∆ i,工	∙-v =bi	(35)
∙-v
In case of i 6= j, inner product of bi and bj becomes zero because this value is a determinant of a
singular matrix with two equivalent column vectors bi .
N
tbi bj = X bki ∙ δ kj = | (b 1,…，bi,…，bi,…) | = 0	(36)
k=1
G0init is defined as a M × N Jacobi matrix of ginit,φ(y) aty =y0 as follows.
0
Ginit
dginit,φ (y )
d y
(37)
y=y0
Using these equations, we proceed to expand of Eq. (29). The first term of (29) in the right side can
be expressed as follows, where the second term Eq. (38) in the right side is constant by definition.
- log
da(z)
d Z
• Pyinit,ψ(a(zo))	= -log(∣B∣) -log(Pyinit,ψ(y0))
(38)
z=z0
14
Under review as a conference paper at ICLR 2020
Next step is an expansion of the second term in Eq. (29). First, the following approximation holds.
ginit,φ(a(z0 + )) - ginit,φ(a(z0))
G0init B
Ginit (E1 , b 1 + E2 . b2-+ eN ∙ bN )
N
£e「Ginitbi
i=1
(39)
Then, the second term of Eq. (29) can be transformed to the next equation by using Eqs. (39), (3),
and (6).
Ee〜P ( e) [ D ( ginit,φ ( a ( Z 0 + € )), ginit,φ ( a ( Z O))] ~ Ee 〜P( e) ^| L ( x ) ^X Ei ∙ Ginit	^
N
= ∑E[Ei2] ∙kL(x) Ginit bik2
i=1
NN
+2 £ E E [Ei ∙ Ej ] ∙
i=1 j=i+1
t(L(x) G0init bi)(L(x) G0init bj)
=b2 ∙ XXk kL(X) G0init bik2	(40)
As a result, Eq. (29) can be rewritten as Eq. (41).
ESP ( e) [ Lalz=Z 0 ]〜 Tog(B|) - lθg( P Uinit,ψ ® θ))
+λ2 ∙ σ 2 ∙ (k IIL (X) Ginit bik 2)	(41)
By examining the minimization process of Eq. (41), the conditions of optimal scaling function
y = a(Z) in the neighborhood of y0 is clarified. Here, the condition of Jacobi matrix B is examined
instead of a(Z). Eq. (41) is differentiated by vector bi, and the result is set to be zero. Then the
following equation Eq.(42) is derived.
2 λ 2 ∙ σ 2 ∙ C ( L (X) Ginit)(L (X) Ginit)) bi = ∣^F77，bi	(42)
|B|
Afterwards, Eq. (42) is multiplied by tbj from the left, and divided by 2λ2 ∙ σ2. As a result, Eq. (43)
is derived by using Eqs. (34) and (36).
t (L (X) Ginit bj)(L (X) Ginit bi) =	∙ δij	(43)
2 λ 2 ∙ σ 2
Here, we define the following function gortho,ψ (Z) which is a composite function of ginit,ψ () and
a().
X = gortho,ψ (Z) = ginit,ψ (a(Z))	(44)
Then the next equation holds by definition.
dgortho,ψ (Z)
∂Zi
∂X
=——
∂zi
z=z0
= G0init bi
z=z0
(45)
15
Under review as a conference paper at ICLR 2020
It is noted that this equation holds at any value of y0 or z0 . As a result, the following equation, that
is Eq. (10), can be derived.
L((X) ∂x) L((X) j = 2λ2σ2' Si
(46)
If encoder and decoder are trained well and x ` X holds, We can introduce new rescaled data space
xD determined by metrics function such as dXD = L(x) ∙ dx, and the next equation holds.
'(等)dj} = 2λ2σ2 ∙而
(47)
In conclusion, all column vectors of Jacobi matrix between N and XD has the same L2 norm
1 / √2λ2 σ and all pairs of column vectors are orthogonal. In other words, when column vectors
of Jacobi matrix are multiplied by the constant √2λ2σ, the resulting vectors are orthonormal.
B EXPLANATION OF ”CONTINUOUS PCA” FEATURE
In this section, we explain RaDOGAGA has a continuous PCA feature when factorized probability
density model as below is used.
N
PNψ(N) = Y P ziψ(zi)	(48)
i=1
Here, our definition of ”continuous PCA” feature means the following. 1) Mutual information be-
tween latent variables are minimum and likely to be uncorrelated to each other. 2) Energy of latent
space is concentrated to several principal components, and the importance of each component can
be determined.
Next we explain the reason why these feature is acquired. As explained in appendix A, all column
vectors of Jacobi matrix of decoder from latent space to data space have the same norm and all
combinations of pairwise vectors are orthogonal. In other words, when constant value is multiplied,
the resulting vectors are orthonormal. Because encoder is a inverse function of decoder ideally, each
row vector of encoder’s Jacobi matrix should be the same as column vector of decoder under the
ideal condition. Here, fortho,θ(X) and gortho,φ (Nθ) are defined as encoder and decoder with these
feature. Because the latent variables depend on encoder parameter θ, latent variable is described as
Nθ = fortho,θ(X), and its PDF is defined as PNθ(Nθ). PDFs of latent space and data space have
the following relation where J is a Jacobian or pseudo-Jacobian between two spaces with constant
value as explained in appendix A.
Pnθ(N) ` J ∙ Pxd(xd) H Pxd(Xd)
(49)
As described before, PNψ(N) is a parametric PDF of the latent space to be optimized with parameter
ψ.
By applying the result of Eqs. (41) and (43), Eq. (5) can be transformed as Eq. (50) where X
gortho,φ (fortho,θ
(x)).
Lortho = — log(PNψ(Nθ)) + λ 1 ∙ h (D(X, X)) + N/2
s.t.
tho,φ (nΘ )
dzθj
2λσ2 ∙δij
(50)
Here, the third term of the right side is constant, this term can be removed from the cost function as
follows.
Lortho = - log(Pzψ(Nθ)) + λ 1 ∙ h (D(X, x))	(51)
Then the parameters of network and probability can be obtained by the next.
θ,φ,ψ = arg min(Ex〜Px(X) [Lortho])	(52)
θ,φ,ψ
16
Under review as a conference paper at ICLR 2020
Ex〜Pχ(X)[Lortho] in Eq. (52)Can be transformed as the next.
Ex 〜P x (x)[ Lortho ] = P P x (x) ∙(-log( P zψ (Zθ))+ λ 1 ∙ h (D (x, x)))d X
PZθ(Zθ)∙
dX
d zθ
]∙ (Tog (Pzψ(zθ))) ∙
dX
d zθ
dzθ
+λ 1 ∙ / PXD(XD) ∙ ∣L(X)I-1 ∙ h (D(X, X)) ∙ ∣L(x)∣dxd
P P Zθ (Zθ) ∙ (-log( P zψ (Zθ )))d Zθ
+λ 1 ∙ JP XD (XD) ∙ h (D (X, X))d XD
(53)
At first, the first term of the third formula in Eq.(53) is examined. Let dzθ/i be a differential of
(N - 1) dimensional latent variables where i-th axis zθi is removed from the latent variable zθ .
Then a marginal distribution of zθi can be derived from the next equation.
P zθi(zθi) =	Pzθ (zθ)dzθ/i
(54)
By using Eqs.(48) and (54), the first term of the third formula in Eq. (53) can be expanded as follows.
/ P zθ ( zθ ) ∙ (- log (P zψ ( zθ )))d zθ
/P Zz((Z) i log (Q⅛S ))d dz
+ Z P Zz (Zz) ∙ (-log (Y Pzzi (Zθi)))
dzθ
XX ZUP zz (ZZ )d ZQ ∙ (-ιog (PZSi))d Z
+ XXJ (/ PZZ (ZZ)ddz/i^ ∙ (- log(P zzi (zzi ))) dzzi
NN
X=1DKL (P zZi (zZi )kP ziψ (zZi ))+ X H(zZi )	(55)
H(X) is an entropy of variable X. The first term of the third formula is KL-divergence between
marginal probability PzZi (zZi ) and factorized parametric probability Pziψ (zZi ). The second term
of the third formula can be further transformed using mutual information between latent variables
I(ZZ) and equation (49).
N
X H(zZi ) = H(ZZ) + I(ZZ) ' H(XD) - log(J) + I(ZZ)	(56)
=1
The first term of the third formula is the entropy of input data with constant value. The second is
also constant. As a result, in order to minimize (55), mutual information I(ZZ) must be minimized.
At second, the second term of the third formula in Eq. (53) is examined. x D and XD denote mapped
values to rescaled data space defined by the metrics function D(X1, X2) as Eq.(9). Because XD and
x'd are close, the following equations holds.
X d 〜 x d + L (x) (X — x)
D(x, X) 〜 ∣L(x) (X — x)12 ^ ∣xd - XD|2
(57)
(58)
17
Under review as a conference paper at ICLR 2020
By using these expansions, Eq.(53) can be expressed as follows.
N
Ex〜Px(x) [Lortho] ' DK DKL (PZθi (Zθi) kPZiψ (Zθi))
i=1
+1(Nθ) + ExD IJXD - XD∣2 ] + Const.
(59)
Here, the rescaled real space xD is divided into a plurality of small subspace partitionings
ΩxD 1, ΩxD2, ∙ ∙ ∙. Let ΩN 1, ΩN2, ∙ ∙ ∙ be corresponding subspace partitionings in latent space.
as the division space of the latent space N ∈ RN corresponding to ΩxD. Then Eq. (59) can be
rewritten as follows.
N
Ex 〜P x (x) [ Lortho ] ' DK DKL ( P zθi ( zθi ) kPziψ ( zθi ))
i=1
+ X I((Nθ ∈ ΩNθk)+ ExD∈ΩxDk IxXD - XD|2])+Const∙ (60)
k
For each subspace partitioning, the transformation from ΩxDk to ΩΝθk can be regarded as constantly
scaled orthonormal transformation where orthonormal basis is Jacobi matrix with scale factor J-1.
According to Karhunen-LOeVe Theory (Rao & Yip (2000)), the orthonormal basis which minimize
both mutual information and reconstruction error leads to be Karhunen-LOeVe transform(KLT). It is
noted that the basis of KLT is equivalent to PCA orthonormal basis.
As a result, when Eq. (60) is minimized, Jacobi matrix from ΩXDk to ΩNθk for each subspace
partitioning should be KLT/PCA. Accordingly, the same feature as PCA will be realized such as the
determination of principal components etc.
From these consideration, we conclude that RaDOGAGA has a ”continuous PCA” feature.
Rescaled domain space xd ∈ Rm with Latent Space ZeRN With
small subspace partitioning ΩXDk	small subspace partitioning Ωzk
CXDk
For all
small
SUbSPaCe WiSe PCA
subspace partitioning CXDK of rescaled domain space,
mapping from ΩxDK to Ωzk can be regarded as PCA
Figure 8: Continuous KLT(PCA) Mapping from input domain to latent space
C EXPANSION OF SSIM TO A QUADRATIC FORM
Structural similarity (SSIM) (Wang et al., 2001) is widely used for picture quality metric which is
close to human subjectiVe quality. In this appendix, we show (1 - SSI M) can be approximated to
a quadratic form such as Eq.(6).
Eq. (61) is a SSIM Value for a N × N window between picture X and y. In order to calculate SSIM
index for a picture, this window is shifted in a whole picture and all of SSIM Values are aVeraged.
SSI MN×N(X,y) =
2 μχ μy
μx 2 + μy 2
2σxy
σχ 2 + σy 2
(61)
18
Under review as a conference paper at ICLR 2020
If (1 一 SSIMN×n (x, y)) is expressed in quadratic form, the average for a picture (1 一
SSIMpicture) can be also expressed in quadratic form.
Let δx be a minute displacement of x. Then SSIM between x and x + δx can be approximated as
follows
,,° 2	2	、
ssimN ×N (x, x + δx) = 1 一 D % 一 D *2 + O f( lδxl∕lxl)3^	(62)
2 μx 2	2 σx 2
Then μδχ2 and σ3x2 can be expressed as follows.
μδx2 = tδx ∙ M ∙ δx
/ 1 1 ... 1 ∖
1	1 1 ... 1
where M = N ∙.....
..	..
∖ 1 1 ... 1 /
σδχ2 = t δx ∙ V ∙ δx
/ N 一 1	一 1	...	一 1	∖
1	-1	N - 1 ...	-1
where v = N ∙	:	...	.
.	.	.	.
\ 一 1	一 1	...	N 一	1	J
(63)
(64)
It is noted that matrix M is positive definite and matrix V is positive semidefinite. As a result,
(1 一 SSIMn×n (x, y)) can be expressed in the following quadratic form with positive definite
matrix.
1 — SSIMn×n(x, X + δx) ` tδx ∙
∙M + A ∙V
∙ δx
(65)
D Effect of h(x)
In this appendix section, the effects of two kinds of cost scaling function h (d) = d and h (d)=
log(d) are discussed. We evaluated the behaviors of encoder and decoder in a one dimensional
model using simple parametric linear encoder and decoder.
a,b = argmin(Exe[- log(P(z)) + λ1∙h(|x —划2) + 22 ♦ |宠一刈2])
Figure 9: Simple encoder/decoder model to evaluate h(d)
Lex x be a one dimensional data with the normal distribution.
x ∈ R
x 〜N (0,σχ2)
19
Under review as a conference paper at ICLR 2020
Lex z be a one dimensional latent variable. Following two linear encoder and decoder are provided
with parameter a and b.
Z = a ∙ x
x = b ∙ Z
Here, square error is used as metrics function D(x, y). The distribution of noise added to latent
variable Z is set to N(0, 1). Then X is derived by decoding Z + E.
D(x,y) = |x - y|2
E 〜N (0,1)
X = b ∙ (Z + E)
For simplicity, we assume parametric PDF P ψ(Z) is equal to the real PDF P (Z). Because the
distribution of latent variable Z follows N(0, (aσx)2), the entropy of Z can be expressed as follows.
P(Z)
H(Z)
N(0, (aσx)2)
J -P(z) ∙ log(P(z))dZ
〜
log( a ) + log( Qx 472∏e)
Using these notations, Eqs. (5) and (8) can be expressed as follows.
Loss = Ex〜N(042), sN(0,1) [- log P(Z) + λ 1 ∙ h(lx - x|2) + λ2 ∙ lxC, x|2]
=log(a) + log(σx√2πe) + λ「Ex〜N(。❶2) [h(∣x - X12)] + λ2 ∙ b2	(66)
At first, the case of h(d) = d is examined. By applying h(d) = d, Eq. (66) can be expanded as
follows.
Loss = log(a) + log(σx√2πe) + λ 1 ∙ (a ∙ b — 1)2 ∙ σx2 + λ2 ∙ b2	(67)
By solving dLOSs = 0 and S = 0, a and b are derived as follows.
λ ι σx + P λ 12 σx 4 — 2 λ ι σx 2
2 λ 1 σ 2
M 、	λ λ 1 σx 2 + Pλ 12 σx 4 一 2λ 1 σx 2 ʌ
，2 ∙λ 2 ʌ-----------2λ1σ-----------)
b = 1 / p 2 ∙ λ 2
a ∙ b
a
If λ1σx2	1, these equations are approximated as next.
1 - 2λ⅛
a ∙ b
a
b
1 / √2 ∙ λ2
Here, a ∙ b is not equal to 1. That is, decoder is not a inverse function of encoder. In this case,
the scale of latent space becomes slightly bent in order to minimize entropy function. As a result,
good fitting of parametric PDF P(Z) 〜 Pψ(Z) could be realized while proportional relationship
P (Z) H P (x) is relaxed.
Next, the case of h(d) = log(d) is examined. By applying h(d) = log(d) and introducing a minute
variable δ, Eq. (66) can be expanded as follows.
Loss = log(a) + log(σx√2πe) + λ 1 ∙ log ((a ∙ b — 1)2 + δ) + λ2 ∙ bb
(68)
20
Under review as a conference paper at ICLR 2020
By solving，L；S = 0 and dL∂S = 0 and setting δ → 0, a and b are derived as follows.
a ∙ b = 1
a = P2 ∙ λ2
b = 1 / p2 ∙ λ 2
(69)
Here, a ∙ b is equal to 1 and decoder becomes a inverse function of encoder regardless of the variance
σχ2. In this case, good proportional relation P(Z) 8 P(x) could be realized regardless of the fitting
P ψ(z) to P(z).
Considering from these result, there could be a guideline to choose h(d). If the parametric PDF
P ψ(z) has enough ability to fit the real distribution P(z), h(d) = log(d) could be better. If not,
h(d) = d could be better.
E Detail of the experiment in section 4.2
In this section, we provide further detail of experiment in section 5.2. First, we describe the detail
of following four public datasets:
KDDCUP99 (Lichman (2013)) The KDDCUP99 10 percent dataset from the UCI repository is
a dataset for cyber-attack detection. This dataset consists of 494,021 instances and contains 34
continuous features and 7 categorical ones. We use one hot representation to encode the categorical
features, and eventually obtain a dataset with features of 121 dimensions. Since the dataset contains
only 20% of instances labeled -normal- and the rest labeled as -attacks-, -normal- instances are used
as anomalies, since they are in a minority group.
Thyroid (Lichman (2013)) This dataset contains 3,772 data sample with 6-dimensional feature
from patients and can be divided in three classes: normal (not hypothyroid), hyperfunction, and
subnormal functioning. We treat the hyperfunction class (2.5%) as an anomaly and rest two classes
as normal.
Arrhythmia (Lichman (2013)) This is dataset to detect cardiac arrhythmia containing 452 data
sample with 274-dimensional feature. We treat minor classes (3, 4, 5, 7, 8, 9, 14, and 15, accounting
for 15% of the total) as anomalies, and the others are treated as normal.
KDDCUP-Rev (Lichman (2013)) To treat“normal”instances as majority in the KDDCUP dataset,
we keep all “ normal ” instances and randomly pick up “ attack ”instances so that they compose
20% of the dataset. In the end, the number of instance is 121,597.
Next, hyper parameter for RaDOGAGA is described in table 2. First and second column is number
of neuron. For DAGMM, we set same number of neuron in table 2 and (λ1, λ2) as (0.1, 0.005).
Optimization is done by Adam optimizer with learning rate 1 × 10-4 for all dataset.
Table 2: Hyper parameter for RaDOGAGA
Dataset	Autoencoder	EN	λ ι( d)	λ 2( d)	λ ι(( log (d))	λ 2( log (d))
KDDCup99	60, 30, 8, 30, 60	10,4	TGG-	1000	TD	TOO
Thyroid	30, 24, 6, 24, 30	10,2	Tgg-	10000	^T00	TOGO
Arrhythmia	10, 4,10	10,2	1000	Tgg-	TOGO	Too
KDDCup-rev	60, 30, 8, 30Γ60^	10,2	1000	100	100	一	100
In addition to experiment in main page, we also conducted experiment with same network size as in
(Zong et al. (2018)) with parameters in table 3
Now, we provide results of setting in table 3. In table 4, RaDOGAGA- and DAGMM- are results
of them and DAGMM is result cited from (Zong et al. (2018)). Even with this network size, our
method has boost from baseline in all dataset.
21
Under review as a conference paper at ICLR 2020
Table 3: Hyper parameter for RaDOGAGA(referring (Zong et al. (2018)))
Dataset	Autoencoder	EN	λ ι( d)	λ2(d)1	λ ι(( Iog (d))	λ2( Iog (d))
KDDCup99	60, 30,1, 30, 60	10,4	100	100	100	1000
Thyroid	12, 4,1,4,12	10,2	1000	10000	^T00	10000
Arrhythmia	10, 2,10	10,2	1000	Tgg-	^Tggg	^T00
KDDCup-rev	60, 30,1, 30而	10,2	100	100	100	一	1000
Table 4: Average and standard deviations(in brackets) of Precision, Recall and F1
Dataset	Methods	Precision	Recall	F1
KDDCup	DAGMM DAGMM- RaDOGAGA-(L2) RaDOGAGA-(log)	0.9297	0.9442	0.9369 0.9338(0.0051)	0.9484(0.0052)	0.9410(0.0051) 0.9455(0.0016)	0.9608(0.0018)	0.9531(0.0017) 0.9370(0.0024)	0.9517(0.0025)	0.9443(0.0024)
Thyroid	DAGMM DAGMM- RaDOGAGA-(L2) RaDOGAGA-(Iog)	0.4766	0.4834	0.4782 0.4635(0.1054)	0.4837(0.1100)	0.4734(0.1076) 0.5729(0.0449)	0.5978(0.0469)	0.5851(0.0459) 0.5729(0.0398)	0.5978(0.0415)	0.5851(0.0406)
Arrythmia	DAGMM DAGMM- RaDOGAGA-(L2) RaDOGAGA-(Iog)	0.4909	0.5078	0.4983 0.4721(0.0451)	0.4864(0.0464)	0.4791(0.0457) 0.4897(0.0477)	0.5045(0.0491)	0.4970(0.0484) 0.5044(0.0364)	0.5197(0.0375)	0.5119(0.0369)
KDDCup-rev	DAGMM * DAGMM- RaDOGAGA-(L2) RaDOGAGA-(Iog)	^0937	0.939	0.938 0.9491(0.0163)	0.9498(0.0158)	0.9494(0.0160) 0.9761(0.0057)	0.9761(0.0056)	0.9761(0.0057) 0.9791(0.0036)	0.9799(0.0035)	0.9795(0.0036)
F Detail of the experiment 4.3
In this section, we provide further detail of experiment in section 5.3. For both RaDOGAGA and
beta-VAE, we first extract feature with following Convolution Neural Network(CNN).
CNN(9, 9, 2, 64, GDN)-CNN(5, 5, 2, 64, GDN)-CNN(5, 5, 2, 64, GDN)-CNN(5, 5, 2, 64, GDN).
Here, CNN(w, h, s, c, f) is a CNN layer with kernel size (w, h), stride size s, dimension c, and
activate function f. GDN(Bane et al. (2015)) is often used in image compression. Then, We reshape
feature map and send to autoencoder as follows.
FC(1024, 8192, softplus)-FC(8192, 256, None)-FC(256, 8192, softplus)-FC(256, 1024, softplus)
FC(i, o, f) is FC layer With input dimension i, output dimension o and activate function f. None
means no activate function. Note that, for beta-VAE, since it produces mean and variance, the
bottom of the encoder has 2 branches.
(λ1, λ2) is as (1.0, 0.1) is set for RaDOGAGA and β is set as 1 × 10-4 for beta-VAE.
Optimization is done by Adam optimizer With learning rate 1 × 10-4 .
G PDF matching with beta-VAE
For the reader With interest, We provide the result of experiment in section 5.1 With beta-VAE.
NetWork consists ofFC layers of Which have the same neuron numbers as the DAGMM and RaDO-
GAGA. We set β as 0.001. Figure 10a and 10b depict results. Since VAE dose not support Jacobian
controlling, P x(x) can not be mapped into Pzψ(z) tidily.
22
Under review as a conference paper at ICLR 2020
z2
(a)	Plot of z. Even though it captures three com-
ponents of input data source s, PDF is quite dif-
ferent from that of s.
Pzφ(z)
Px(x)
(b)	Plot of of P x(x) (x-axis) and Pzψ (z) (y-
axis). No clear correlation can been seen.
Figure 10: Result of PDF estimation with toy data (beta-VAE)
23