Under review as a conference paper at ICLR 2020
The Geometry of Sign Gradient Descent
Anonymous authors
Paper under double-blind review
Ab stract
Sign gradient descent has become popular in machine learning due to its favorable
communication cost in distributed optimization and its good performance in neu-
ral network training. However, we currently do not have a good understanding of
which geometrical properties of the objective function determine the relative speed
of sign gradient descent compared to standard gradient descent. In this work, we
frame sign gradient descent as steepest descent with respect to the maximum norm
(L∞-norm). We review the steepest descent framework and the related concept
of smoothness with respect to arbitrary norms. By studying the smoothness con-
stant resulting from the L∞-geometry, we isolate properties of the objective which
favor sign gradient descent relative to gradient descent. In short, we find two re-
quirements on its Hessian: (i) some degree of “diagonal dominance” and (ii) the
maximal eigenvalue being much larger than the average eigenvalue. We also clar-
ify the meaning of a certain separable smoothness assumption used in previous
analyses of sign gradient descent. Experiments verify the developed theory.
1	Introduction
We consider an unconstrained, continuous optimization problem, minx∈Rd f (x), with a differen-
tiable and lower-bounded objective f : Rd → R. The prototypical first-order optimization algorithm
to solve this problem is gradient descent (GD), which iteratively updates xt+ι = Xt - αNft, where
ft := f(xt). Several recent works have considered the sign gradient descent (signGD) method,
Xt+1 = Xt - at sign(Vft),
(1)
where the application of the sign is to be understood elementwise. This method is attractive in
distributed optimization where it conveniently reduces the communication cost to a single bit per
gradient coordinate (e.g., Seide et al., 2014; Karimireddy et al., 2019), but its interest extends beyond
that. Balles & Hennig (2018) and Bernstein et al. (2018) point out that sign gradient descent is
closely related to the popular Adam method (Kingma & Ba, 2015) and demonstrate that it often
achieves similar practical performance on deep learning tasks. Studying sign gradient descent can
thus be seen as a step towards a better understanding of the ubiquitous Adam method.
1.1	Overview and Contributions
In this work, we make the following contributions:
1.	We review the concept of steepest descent with respect to arbitrary norms, which includes both
gradient descent and sign gradient descent. While these ideas feature in previous works, we
provide a concise and general write-up which we hope to be useful in its own right. We emphasize
the role of the smoothness constant of f w.r.t. a norm as a major driver of the performance of the
associated steepest descent method.
2.	Since sign gradient descent is steepest descent w.r.t. the maximum norm, we study the corre-
sponding smoothness constant L∞, isolating properties of the objective function which favor
sign gradient descent. We pinpoint two requirements on its Hessian: ((i) some degree of “diago-
nal dominance” and (ii) the maximal eigenvalue being much larger than the average eigenvalue.
3.	We verify our theory experimentally on both quadratic functions and deep networks.
1
Under review as a conference paper at ICLR 2020
Method	Norm	Dual	Update direction	
Gradient descent		k∙ k2	Vf	
Sign gradient descent	k ∙k∞	k∙ kl	INf kι Sign(Vf)	
Coordinate descent	k ∙kι	k∙ k∞	Vfimaxe(imax)	(see Appendix. A.1)
Block-normalized GD	k ∙kB,∞	U ∙ k,1	kVf kB,1 normB (Vf)	(see Appendix. A.1)
Table 1: A few steepest descent methods. The table lists the used norm ∣∣ ∙ ∣∣, its dual k ∙ k *, and the
resulting update direction. The significance of the dual norm will be explained in §2.1.
1.2	Restrictions
While modern machine learning often relies on stochastic optimization, this work focuses on the
geometric aspects of sign gradient descent and we restrict ourselves to the deterministic setting to
facilitate this analysis. Furthermore, our interest in signGD is from an optimization perspective and
we will not consider the question of generalization. It should be noted that sign gradient descent
has been observed to lead to slightly deteriorating generalization performance compared to gradient
descent in neural network training (Wilson et al., 2017; Balles & Hennig, 2018).
2	Steepest Descent and Smoothness Constants
In this section, we review the steepest descent1 framework, which encompasses both gradient de-
scent and sign gradient descent. Embedding both methods in a common framework will facilitate
our comparative analysis. Steepest descent with respect to a norm ∣∣ ∙ ∣∣ is defined via the update rule
xt+ι ∈ arg min ft + Eft, X — Xti + ɪ ∣∣x — xt∣2 ) ,	(2)
x∈Rd	2αt
minimizing a local linear approximation off plus a quadratic norm penalty. This minimizer need
not be unique, in which case steepest descent is to be understood as choosing any solution.
It is straight-forward to see that gradient descent is steepest descent with respect to the Euclidean
norm. Similarly, a version of sign gradient descent arises as steepest descent with respect to the
maximum norm, ∣z ∣∞ := maxi |zi |. Namely,
Xt — at∣^ft∣ι sign(Vft) ∈ argmin ft +	ft, X — Xti + ɪIlX — Xt∣∣∞) .	(3)
x∈Rd	2αt
This update is equivalent to Eq. (1) up to the scaling with the gradient norm ∣Vft ∣1 which may be
subsumed in the step size.2 Table 1 lists a few more steepest descent methods, which are discussed
in more detail in Appendix A.1.
In the following, we will explore steepest descent in more detail by a) relating it to the concept of
smoothness w.r.t. arbitrary norms; b) tying its convergence speed to the corresponding smoothness
constant; c) linking the smoothness constants to properties of the Hessian.
2.1	Smoothness with respect to Arbitrary Norms
Steepest descent is closely connected to the concept of smoothness w.r.t. the relevant norm. We
recall that a function f is L-smooth w.r.t. a norm ∣∙∣ if
∣Vf (y) — Vf (X)∣* ≤ L∣y — X∣, ∀X,y ∈ Rd,	(4)
where ∣∙∣* denotes the dual norm of ∣∙∣, defined as ∣∣z∣* := max∣∣χ∣∣≤ιhz, x〉. Table 1 lists the
dual norm pairs for the methods under consideration in this paper. Smoothness directly motivates
steepest descent via the following quadratic bound on the function:
Lemma 1. If f is L-smooth w.r.t. ∣∙∣ ,then f(y) ≤ f(X)+(Vf (x), y—x)+ L ||y—x∣2 ∀x, y ∈ Rd.
1Also known as generalized or non-Euclidean gradient descent (Carlson et al., 2015; Kelner et al., 2014).
2 In this paper we study this version of sign gradient descent to facilitate the analysis in the steepest descent
framework. In practice it is more common to use Eq. (1) with a constant or manually decreasing step size. In
Appendix C, we discuss a possible explanation for this discrepancy.
2
Under review as a conference paper at ICLR 2020
Setting X = Xt We get f (y) ≤ ft + "ft, y - Xti + L∣∣y - xt『which resembles Eq. (2) and
minimizing with respect to y results in a steepest descent update with step size αt = 1/L. (The
proofs of this lemma and all subsequent results are provided in Appendix E.)
Due to the equivalence of norms on Rd, a function that is smooth with respect to one norm is also
smooth with respect to any other norm. However, the tightest possible smoothness constant,
L := sup
x6=y
IIVfw) - Vf(X)II*
l∣y - xI
(5)
will differ depending on the choice of norm. In the following, when we say f is L-Smooth w.r.t. ∣∙∣,
we will always assume L to be given by Eq. (5). As we will see next, this constant will govern the
convergence speed of the corresponding steepest descent method.
2.2	Convergence of Steepest Descent
We now recall existing convergence results for steepest descent methods based on the smoothness
constants w.r.t. arbitrary norms. The assumption of smoothness is crucial since it guarantees an
improvement in function value:
Lemma 2. If f is L-smooth w.r.t. ∣ ∙ ∣∣, then a single steepest descent SteP with SteP size 1/L satisfies
f(x+) ≤ f(x) - 2LIlVf(X)∣*.
(6)
Without additional assumptions, this yields O(1/T) convergence to a first-order stationary point:
Proposition 1. Iff is L-smooth w.r.t. ∣ ∙ ∣∣, then steepest descent (Eq. 2) with α ≡ 1/L satisfies
T X IVft ∣2 ≤ ff)
T t=0	T
(7)
By nature of Proposition 1, all steepest descent methods will enjoy the same rate of convergence. But
Lemma 2 and Proposition 1 differ across different steepest descent methods in (i) the smoothness
constant, and (ii) the norm in which the gradient magnitude is measured. These two aspects will
play a role when we compare sign gradient descent and gradient descent in Section 4.
Appendix A contains stronger convergence results under additional assumptions, in particular linear
convergence under the Polyak-LojaSieWicz condition. We omit these results here as they are not of
primary interest to our discussion and, in fact, hide the dependence on the gradient norm which we
see clearly in Lemma 2 and Proposition 1.
2.3	Smoothness as a Bound on the Hessian
The definition of smoothness in Eq. (4) is unwieldy. For the Euclidean norm, a more intuitive
characterization is widely known: a twice-differentiable function f is L2-smooth w.r.t. the Euclidean
norm if and only if ∣∣V2f (x)∣∣2 ≤ L2 for all X ∈ Rd. Here, ∣∣ ∙ ∣∣2 for matrices denotes the
spectral norm, given by the largest-magnitude eigenvalue for symmetric matrices. In the following
proposition we show that smoothness with respect to any other norm likewise arises from a bound
on the Hessian, but in different matrix norms. In Section 3, we will use this to better understand the
smoothness constant w.r.t. the maximum norm, which critically affects the performance of signGD.
Proposition 2. For any norm ∣∣ ∙ ∣∣ on Rd, we define the matrix norm
∣H∣ := sup ∣HX∣* .	(8)
kxk≤1
Then, a twice-differentiable f is L-smooth w.r.t. ∣ ∙ ∣∣ ifandonly f ∣∣V2f (X)Il ≤ Lforall X.
The proof can be found in Appendix E; we are not aware of prior works showing this for the general
case. For the Euclidean norm, Proposition 2 gives back the familiar spectral norm bound.
3
Under review as a conference paper at ICLR 2020
Figure 1: L∞ increases as the Hes-
sian becomes less axis-aligned.
Contour line of f (x) = 1XTHx,
which forms an ellipse with prin-
cipal axes given by the eigenvec-
tors and axis lengths given by the
inverse eigenvalues. We construct
H ∈ Rd×d with a fixed λmi∩ = 1
and variable L? = λmax > 1 as
well as the angle θ between eigen-
vectors and coordinate axes.
3 UNDERSTANDING THE L∞-SMOOTHNESS CONSTANT
We have seen in Section 2.2 that the smoothness constant w.r.t. a norm is a crucial driver of the con-
vergence speed of the associated steepest descent method. While we have a good intuition for Eu-
clidean smoothness—an upper bound on the eigenvalues of the Hessian—this is somewhat lacking
for smoothness w.r.t. the maximum norm. Our goal in this section is to understand which properties
of the objective function affect the constant L∞. The Euclidean smoothness constant L2 will thereby
serve as a reference point. We assume f to be L2-smooth w.r.t. k∙∣∣2 and L∞-smooth w.r.t. k∙k∞
in the “tight” sense of Eq. (5). Basic inequalities between the norms yield (see Appendix E)
L2 ≤ L∞ ≤ dL2 .	(9)
The smoothness constant governing the convergence speed of sign gradient descent will always be
larger than the one pertinent to gradient descent. This does not mean that signGD is always worse
than GD. In addition to being a bound, the smoothness constant is only one factor influencing the
convergence speed; Lemma 2 shows that the dual gradient norm plays a role as well. As we will
discuss in Section 4, this gradient norm is larger for signGD which can make up for the disadvantage
of a larger smoothness constant. In this section we will characterize conditions under which L∞ is
“not too bad”—in particular much smaller than its worst case of dL2—such that signGD can be
competitive with gradient descent.
Our discussion will use the Hessian-based formulation of the smoothness constants (Proposition 2).
We assume f to be twice-differentiable. As mentioned before, the Euclidean smoothness constant
is simply given by the largest-magnitude eigenvalue. The max-norm smoothness constant is deter-
mined by
L∞ =SuP ∣∣V2f(x)k∞,ι,	∣∣Hk∞,ι := sup IlHxll1.	(10)
x∈Rd	kxk∞ ≤1
Unfortunately, computing ∣H ∣∞,1 is NP-hard (Rohn, 2000). However, we can obtain some insight
through the following upper bound.
Proposition 3. Let H ∈ Rd×d be nonzero, positive semi-definite with eigenvalues λ1 , . . . , λd. Then
kHk∞,1 ≤ P diag (H )-1 X λi with	P diag (H ):= TH].
i	i,j |Hij |
(11)
The quantity Pdiag(H) ∈ [d-1, 1] measures the percentage of diagonal mass of the Hessian and thus
relates to the “axis-alignment” of the objective. Indeed, diagonal matrices satisfy Pdiag = 1 and their
eigenvectors coincide with the coordinate axes. Proposition 3 thus indicates that L∞ will depend
both on the “axis-alignment” of the Hessian as well as its eigenvalues. This is in stark contrast
to gradient descent, for which the relevant matrix norm, ∣H∣2, is invariant to rotations. Figure 1
provides a two-dimensional illustration.
Using Proposition 3, we can now answer our question: Under what conditions is L∞ “not too bad”
compared to L2 ? Firstly, L∞ benefits from high Pdiag, i.e., an approximately diagonal Hessian or
4
Under review as a conference paper at ICLR 2020
axis-aligned objective. Secondly, the sum of eigenvalues should be much smaller than its worst case
of dλmax or, equivalently, the average eigenvalue should be much smaller than the maximal one:
λ := d Pi λi《λmaχ. The second condition will be met if the spectrum is dominated by a small
number of “outlier eigenvalues” that far exceed the bulk of eigenvalues. Notably, both the relative
axis-alignment and the skewed spectrum have been observed in neural network training (Adolphs
et al., 2019; Chaudhari et al., 2017; Ghorbani et al., 2019).
The discussion in the preceding paragraphs was based on an upper bound, since kHk∞,1 is generally
intractable. Section 5 will feature experiments on quadratics of moderate dimension, where kH k∞,1
can be computed to confirm these findings for the exact L∞ .
4 Gradient Descent vs Sign Gradient Descent
We found in Section 3 that L∞ will always exceed L2, and identified conditions under which L∞
dL2. By Lemma 2, the guaranteed improvement of the two methods at x ∈ Rd is
I zʌkVf (χ)k2 V s I — . "Nf(X)k2
IGD(X) :=	vs. ISignGD(X) :=	∙
L2	L∞
We will now compare the two methods based on these improvement guarantees, thus taking into
account the dual gradient norm in addition to the smoothness constant. Basic inequalities between
the two relevant norms, √d∣∣zk2 ≥ kzk1 ≥ kzk2 for all z ∈ Rd, show that this potentially favors
sign gradient descent. Following Bernstein et al. (2018), it will be convenient to define the ratio
φ(z) := d⅛z⅛ ∈ [d-1,1],
which can be seen as a measure of the “density” of the vector z, i.e., how evenly its mass is dis-
tributed across coordinates.3 With that, we can write
R(X) := ⅛⅛ = klfS工=Φ(Vf(X))dL2.	(12)
IGD(X)	kVf(x)k2L∞	L∞
IfL∞ dL2 , for which we have identified conditions in Section 3, and the gradient is dense,
i.e. φ(Vf (X)) d-1, then sign gradient descent will achieve a larger improvement than gradient
descent at X ∈ Rd .
We can make this comparison even more palpable. We restrict to quadratic functions, such that
V2f(X) ≡ H and L∞ = kHk∞,1 as well asL2 = kHk2 = λmax. Assume H to be positive
definite with eigenvalues λ1, . . . , λd. Then Proposition 3 in Eq. (12) yields
R(X)
ZSignGD(X)
IGD(X)
≥ Pdiag(H)警 Φ(Vf(X))
λ
|—{z—}l{z}l--{z--}
(13)
∈[d-1 ,1] ∈[1,d]	∈[d-1 ,1]
Hence, sign gradient descent will outperform gradient descent if (i) the Hessian is to some extent di-
agonal, (ii) the maximal eigenvalue is much larger than the average eigenvalue, and (iii) the gradient
vector is relatively dense.
We emphasize that this is a local comparison at X ∈ Rd due to the dependence on the gradient
Vf (X). It is tempting to try and lower-bound the gradient density φ(Vf (X)) ≥ φg d-1 for
all X in order to make global statements. Bernstein et al. (2018) assume such a lower bound when
contrasting signGD and GD in their analysis. However, φ(Vf (X)) can easily be shown to attain d-1
even on quadratics. Any non-trivial lower bound would thus have to be restricted to the trajectory of
the optimizer and take into account the initialization, which seems out of reach at the moment. The
effect of the gradient norm thus remains an empirical question which we now address.
5	Experiments
5.1	Quadratic Problems
We first consider synthetic quadratic problems of moderate dimension d = 12, where we can com-
pute and control all relevant quantities. To validate our theoretical findings, we generate Hessians
3Note that φ(z) = 1 for Z (X (1,..., 1)T and φ(z) = d-1 for Z (X (1,..., 0)t.
5
Under review as a conference paper at ICLR 2020
T 10
-9
I- 8
-7
-6
-5
-4
-3
-2
1
0.0	0.1	0.2	0.3	0.4	0.5
10.0
-101
7.5
-100
5.0 -
2.5
IlxTgn - χ*∣∣2 / IlxTD - X
0.0	0.1	0.2	0.3	0.4	0.5
K
S
θ	θ
Figure 2: We consider quadratic objectives varying across two axes: λm∕λ as well as a rotation
value θ. The left plot depicts the ratio of the two relevant smoothness constants. L∞ is sensitive
to θ and grows relative to L2 = λmax as the problem becomes less axis-aligned. The right plot
depicts the relative performance of gradient descent and sign gradient descent on these problems.
GD drastically (the colormap is clipped) outperforms signGD for mildly conditioned (small λmax/λ)
and non-axis-aligned (large θ) problems. However, sign gradient descent is preferable for problems
with high λmax∕λ, given that they have some degree of axis-alignment (small θ). The dashed line
represents equal performance of both algorithms.
with varying λ∕λmax and axis alignment. We set the eigenvalues as Λ = diag(1,1,..., 1, λmax). To
control the axis-alignment, we rotate the eigenvectors by some ratio θ in the direction prescribed by
a randomly-drawn rotation matrix. We can simply think of θ as a degree of non-axis-alignment; the
technical details can be found in Appendix D. For each Hessian, we compute the two smoothness
constants L2 = λmax and L∞ = kHk∞,1. We then run T = 100 iterations of gradient descent
(with α = I/L2) and sign gradient descent (with α = 1∕L∞) and compute the distance to the op-
timum ∣∣xτ 一 x*k2 as a scale-invariant performance measure. We average over repeated runs with
xo 〜N(0, I) to marginalize out the effect of initialization. The results are depicted in Fig. 2 and
confirm the findings of Sections 3 and 4. The L∞ constant—and consequently the performance of
signGD—is sensitive to the axis-alignment of H and suffers as we increase θ. For problems with
λmax》λ that are somewhat axis-aligned, sign gradient descent outperforms gradient descent, even
on these simple quadratic problems.
5.2	Neural Network Experiments
Variants of sign gradient descent have been shown to achieve surprisingly good performance in
neural network training tasks (Balles & Hennig, 2018; Bernstein et al., 2018). In this section, we
want to test whether the theory developed above can help explain these successes. Applying our
theory to a deep learning scenario comes with several caveats. First, the objectives are not globally
smooth in the sense of Eq. (4); see also our discussion in Appendix C. However, the theory of smooth
optimization still applies in a local sense within a neighborhood of the current iterate. Second,
even computing local smoothness constants is intractable in these complex and high-dimensional
problems. Finally, neural networks are usually trained in a stochastic optimization setting, whereas
our theory considered only the deterministic case.
With these limitations in mind, we proceed as follows. We train a vanilla convolutional neural
network as well as a ResNet20 (He et al., 2016) on the CIFAR-10 dataset using gradient descent
and sign gradient descent (in their stochastic versions). For each architecture and algorithm, we
optimize the step size over a logarithmic grid to make sure we are close to the optimal step size, as
our theory assumes. At regular intervals (once per epoch) along the trajectory of the two methods,
we approximate the local improvement ratio R(x) defined in Eq. (12). While the gradient density is
easy to compute by evaluating the full-batch gradient, we approximate the two smoothness constants
with directional finite differences following Zhang et al. (2019). Details can be found in Appendix D.
We then compare the sequence of ratios to the actual training loss performance of the two methods.
We see in Figure 3 that the results match our bound: for the CNN, the ratios are close to 1 but initially
bigger and signSGD outperforms SGD. For the ResNet20, the ratio is consistently below 1 and SGD
6
Under review as a conference paper at ICLR 2020
CNN
ResNet20
Figure 3: Experiments on CIFAR10 dataset. When using a CNN (top), signSGD outperforms SGD
(top left), which matches the behaviour predicted by the improvement ratio (top right), above 1 at
the beginning. When using a ResNet20 (bottom), the situation is reversed but the improvement ratio
still correctly predicts the relative performance of both algorithms.
is the faster algorithm. We can also note that the ratios computed along the two trajectories do not
differ substantially, suggesting some degree of robustness. There are still some issues. For instance,
in the second experiment, SignSGD outperforms SGD later in training, which is not captured by our
ratio. While these experiments are by no means conclusive, they give a strong indication that the
theory of this paper is relevant to explaining the practical performance of sign-based optimization
methods.
6	Related Work
6.1	Steepest Descent
Steepest descent is a basic concept and has become text book knowledge (Boyd & Vandenberghe,
2004, §9.4). Its origins are hard to trace back. It features—sometimes implicitly—in many
works (Nesterov, 2005; 2012; Allen-Zhu & Orecchia, 2014; Kelner et al., 2014; Carlson et al.,
2015; Karimi et al., 2016, e.g.). Kelner et al. (2014) and Carlson et al. (2015) point out that sign gra-
dient descent is steepest descent w.r.t. the maximum norm; the former applies signGD to max-flow
problems in graphs.
Steepest descent resembles mirror descent (Nemirovsky & Yudin, 1983; Beck & Teboulle, 2003),
1
WhiCh updates x t+1 — arg ιminχ (h* ft, X Xti + α- Bψ (x, x t)), Where Bψ Is a Breglmall di-
vergence. Despite the obvious similarity, this setting does not cover steepest descent, since many
squared lorms (lotably the maximum lorm) cal lot be Writtel as Bregmal divergelces.
6.2	Previous Works on Sign Gradient Descent
A versiol of sigl gradielt descelt has first beel explored il leural letWork traililg by Seide et al.
(2014) With the goal of gradielt compressiol for distributed optimizatiol. The proposed algorithm
7
Under review as a conference paper at ICLR 2020
deviates from signGD in that it uses an error feedback mechanism to approximate gradient descent
as closely as possible while maintaining high compression rates. Such error feedback was later
refined and given theoretical grounding by Karimireddy et al. (2019).
The use of sign gradient descent itself (or rather its stochastic version) was popularized by Bernstein
et al. (2018), who present a theoretical analysis in the stochastic setting. The relatively strong
assumptions of this work have since been relaxed by Bernstein et al. (2019) and Safaryan & Richtarik
(2019). All three papers are based on the assumption that there exist constants li > 0 such that
f(y) ≤ f (x) + Ef(X) y - Xi + 1X li(yi- χi)2,	∀χ, y ∈ Rd.	(14)
i
We refer to this assumption as separable smoothness to emphasize the fact that the change in func-
tion value separates over individual coordinates. Under this assumption the convergence of signGD
depends on Pi li whereas the convergence speed of gradient descent is driven by lmax := maxi li.
Not only is the meaning of these li’s unclear, but neither the algorithm itself nor the analysis uses the
individual li ’s. It thus seems that the separable smoothness assumption adds an unnecessary level of
granularity. Indeed, we will now show that
(i)	separable smoothness implies (Pi li)-smoothness with respect to the maximum norm, and
(ii) existing convergence proofs go through under the latter (weaker) assumption.
To formalize (i), We identify separable smoothness as 1-smoothness w.r.t. the norm k ∙ ∣∣l where
L = diag(l1, . . . , ld and kzk2L := Pi lizi2 , such that Lemma 1 coincides with Eq. (14). With
that, we can establish the following result:
Proposition 4. If f is 1 -smooth w.r.t. k ∙ ∣∣l, then f is (Ei /-smooth w.r.t. the maximum norm.
Statement (ii) follows from the simple fact that the quadratic bounds provided by the two conditions
coincide for any method with updates of equal magnitude in each coordinate, Xt+1 - Xt ∈ {-c, c}d,
which includes any version of sign gradient descent. Since existing convergence results base upon
this bound, they go through under either condition. We refer to Appendix B for details.
Based on these considerations we argue that smoothness w.r.t. the maximum norm is the more natural
assumption under which to analyze sign gradient descent. We discuss this issue in much more detail
in Appendix B.
7	Conclusions
Our analysis relates the speed of signGD to the axis-alignment of the objective’s Hessian and the
spread of its eigenvalues. This furthers our understanding of when this method and similar ones, like
Adam, might outperform standard GD. It is notably of interest that these properties of the Hessian
have independently been studied (Chaudhari et al., 2017; Ghorbani et al., 2019; Adolphs et al., 2019)
with the conclusion that modern architecture tend to be well-suited to the use of signGD.
Besides the limitations discussed in §5.2, we also sidestepped the question of the scaling of the sign
gradient descent update with ∣∣Vft∣ι, which arises from the steepest descent formalism but is not
common in practice. Appendix C discusses this issue and how the absence of the scaling term makes
sense in a more general setting than the one studied here.
This work raises several questions for future research. The first one is whether we can design archi-
tecture that could further help the convergence of signGD. The second one is whether we can design
optimizers that adapt to the geometry of the space. Ideally, one might want the norm of steepest
descent to be optimized alongside the parameters. Finally, one might ask whether there are other
norms and corresponding steepest descent methods that work even better on contemporary neural
network architectures. We explore one option, block-normalized gradient descent, in Appendix A.
References
Leonard Adolphs, Jonas Kohler, and Aurelien Lucchi. Ellipsoidal trust region methods and
the marginal value of Hessian information for neural network training. arXiv preprint
arXiv:1905.09201, 2019.
8
Under review as a conference paper at ICLR 2020
Zeyuan Allen-Zhu and Lorenzo Orecchia. Linear coupling: An ultimate unification of gradient and
mirror descent. arXiv preprint arXiv:1407.1537, 2014.
Lukas Balles and Philipp Hennig. Dissecting Adam: The sign, magnitude and variance of stochastic
gradients. In Proceedings of the 35th International Conference on Machine Learning (ICML),
volume 80 of Proceedings of Machine Learning Research. PMLR, 2018.
Amir Beck and Marc Teboulle. Mirror descent and nonlinear projected subgradient methods for
convex optimization. Operations Research Letters, 2003.
Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree Anandkumar.
SignSGD: Compressed optimisation for non-convex problems. In Proceedings of the 35th In-
ternational Conference on Machine Learning (ICML), volume 80 of Proceedings of Machine
Learning Research. PMLR, 2018.
Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree Anandkumar.
SignSGD with majority vote is communication efficient and fault tolerant. In 7th International
Conference on Learning Representations (ICLR), 2019.
Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge University Press, 2004.
David Carlson, Ya-Ping Hsieh, Edo Collins, Lawrence Carin, and Volkan Cevher. Stochastic spectral
descent for discrete graphical models. IEEE Journal of Selected Topics in Signal Processing, 10
(2), 2015.
Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian
Borgs, Jennifer T. Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-SGD: Biasing gra-
dient descent into wide valleys. In 5th International Conference on Learning Representations
(ICLR), 2017.
Behrooz Ghorbani, Shankar Krishnan, and Ying Xiao. An investigation into neural net optimization
via Hessian eigenvalue density. arXiv preprint arXiv:1901.10159, 2019.
Boris Ginsburg, Patrice Castonguay, Oleksii Hrinchuk, Oleksii Kuchaiev, Vitaly Lavrukhin, Ryan
Leary, Jason Li, Huyen Nguyen, and Jonathan M Cohen. Stochastic gradient methods with layer-
wise adaptive moments for training of deep networks. arXiv preprint arXiv:1905.11286, 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, 2016.
Eric Jones, Travis Oliphant, Pearu Peterson, et al. SciPy: Open source scientific tools for Python,
2001. URL http://www.scipy.org/.
Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal-
gradient methods under the Polyak-ICjasieWicz condition. In Joint European Conference on Ma-
chine Learning and Knowledge Discovery in Databases. Springer, 2016.
Sai Praneeth Karimireddy, Quentin Rebjock, Sebastian Stich, and Martin Jaggi. Error feedback fixes
SignSGD and other gradient compression schemes. In Proceedings of the 36th International Con-
ference on Machine Learning (ICML), volume 97 of Proceedings of Machine Learning Research.
PMLR, 2019.
Jonathan A Kelner, Yin Tat Lee, Lorenzo Orecchia, and Aaron Sidford. An almost-linear-time al-
gorithm for approximate max floW in undirected graphs, and its multicommodity generalizations.
In Proceedings of the twenty-fifth annual ACM-SIAM symposium on discrete algorithms. SIAM,
2014.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In 3rd Interna-
tional Conference on Learning Representations (ICLR), 2015.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
Technical report, Citeseer, 2009.
9
Under review as a conference paper at ICLR 2020
Arkadii Semenovich Nemirovsky and David Borisovich Yudin. Problem complexity and method
efficiency in optimization. Wiley, 1983.
Yurii Nesterov. Smooth minimization of non-smooth functions. Mathematical programming, 103
(1), 2005.
Yurii Nesterov. Efficiency of coordinate descent methods on huge-scale optimization problems.
SIAM Journal on Optimization, 22(2), 2012.
Jiri Rohn. Computing the norm kAk∞,1 is NP-hard. Linear & Multilinear Algebra, 47, 2000.
Mher Safaryan and Peter Richtarik. On stochastic sign descent methods. arXiv preprint
arXiv:1905.12938, 2019.
Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu. 1-bit stochastic gradient descent and its
application to data-parallel distributed training of speech DNNs. In Fifteenth Annual Conference
of the International Speech Communication Association, 2014.
Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal
value of adaptive gradient methods in machine learning. In Advances in Neural Information
Processing Systems (NeurIPS), 2017.
Adams Wei Yu, Lei Huang, Qihang Lin, Ruslan Salakhutdinov, and Jaime Carbonell. Block-
normalized gradient method: An empirical study for training deep neural network. arXiv preprint
arXiv:1707.04822, 2017.
Jingzhao Zhang, Tianxing He, Suvrit Sra, and Ali Jadbabaie. Analysis of gradient clipping and
adaptive scaling with a relaxed smoothness condition. arXiv preprint arXiv:1905.11881, 2019.
10
Under review as a conference paper at ICLR 2020
—Supplementary Material—
We now provide additional details and results for the paper. More precisely:
•	Appendix A gives additional details on steepest descent methods.
•	Appendix B extends the discussion on the relationship between separable smoothness and
smoothness w.r.t. the maximum norm.
•	Appendix C discusses an extension of a recently-proposed relaxed smoothness condition
as a possible explanation of normalized methods (such as sign gradient descent without the
scaling by the norm).
•	Appendix D provides details on the experiments.
•	All proofs, including those of results in the appendices, can be found in Appendix E.
A Details on Steepest Descent
In this section, we provide a more comprehensive overview of steepest descent methods as well
as additional results. In particular, We recall the linear convergence under the PoIyar-LojasieWicz
condition.
It Will be useful to re-Write the steepest descent update (Eq. 2) as
xt+ι = Xt-αtVftκ with zk,k ∈ arg max (〈z, y)- Ikyk2)∙	(15)
y∈Rd	2
This equivalence arises from substituting y = 一 0- (X — x/;a full proof can be found in Appendix E
A.1 Additional Examples for Steepest Descent Methods
First, We give tWo additional examples for steepest descent methods to demonstrate the versatility
of this frameWork.
A.1.1 Coordinate Descent
(18)
(19)
(20)
Steepest descent With respect to the L1-norm,
Xt+1 ∈ arg min (ft + Eft, X - Xti +「∣∣x 一 XtkI)，	(16)
yields coordinate descent,
Xt+1 = Xt- αt∣Vft,imax Ie(M	(17)
Where the selected coordinate is chosen from imax ∈ arg maxi∈[d] |Vft,i| and e(i) denotes the i-th
coordinate vector. The corresponding smoothness assumption is
kVf (y) - Vf (X)k∞ ≤ L1ky - Xk1.
In fact, this assumption implies the coordinate-Wise Lipschitz smoothness assumption,
|Vf(X + he(i))i -Vf(X)i| ≤ L1|h| ∀i ∈ [d],
Which is Widely-used in the literature on coordinate descent, since
|Vf(X+he(i))i-Vf(X)i| ≤ kVf(X+he(i))-Vf(X)k∞
(18)
≤ L1 kX + he(i) - Xk1 = L1 |h|.
11
Under review as a conference paper at ICLR 2020
A.1.2 Block-Normalized Gradient Descent
Assume a block structure on Rd given by a partitioning B = {B1, . . . , Bb} of [d], with Bk ⊂ [d],
Bk ∩ Bi = 0 for k = l, and Uk Bk = [d]. For B ⊂ [d], define XB ∈ R1B1 to be the vector consisting
of (xi)i∈B. We can now define norms with respect to this block structure, such as
kxkB,∞ = maBx kxB k2 with dual norm kxkB,1 = kxBk2.	(21)
∈	B∈B
Steepest descent w.r.t. k ∙ ∣∣b,∞ results in block-normalized gradient descent,
VfHB,∞ = ∣∣Vf ∣B,1 normB (Vf), normB (Z)
zBT1
zBTb	T
kzB1k2'…'kzBbk2
(22)
This method is a block-wise equivalent of sign gradient descent, normalizing the update magnitude
over blocks instead of element-wise. Variants of this method have recently been studied empiri-
cally for neural network training (Yu et al., 2017; Ginsburg et al., 2019). It would be interesting
future work to analyze this method in the steepest descent framework, similar to our analysis of sign
gradient descent in this paper.
A.2 Properties of the Steepest Descent Operator
We define the zk∙k operator as:
∣∣z∣∣* := max (z,Xi
kxk≤1
zk∙k ∈ arg max (〈z, y)— - Ilyll2
y∈Rd	2
(23)
The following Lemma connects this operator to the dual norm:
Lemma 3. For all X, z ∈ Rd, we have
(a)〈x, z)≤kxkkz∣*
(b)	∣∣zk∙kk2 = hz, zk∙k)
(c)	kzk∙kk = kzk*
(24)
(25)
(26)
A.3 Additional Convergence Results for Steepest Descent
For smooth and convex functions, Kelner et al. (2014) showed O(-/T) convergence in suboptimal-
ity. We restate this result here for completeness.
Theorem 1 (Theorem 1 in Kelner et al. (2014)). If f is L-smooth w.r.t. k ∙ k and convex, then SteePeSt
descent (Eq. 2) with step size αt = /L satisfies
fT — f? ≤
2LR
T + 4
(27)
with R:=	max
x s.t. f (x)≤f (x0)
min
x? s.t. f (x? )=f ?
kX — X?k.
The rate has a dependence on the initial distance to the nearest minimizer measured in the respective
norm.
It is also straight-forward to show linear convergence in suboptimality under an additional assump-
tion, known as the Polyak-Eojasiewicz (PL) condition. It is usually given for the Euclidean case as
∣∣Vf (x)k2 ≥ 2μ(f (x) — f?) but can likewise be formulated for arbitrary norms.
Definition 1. A function f : Rd → R satisfies the PL condition w.r.t. a normk ∙k if kVf(x)k2 ≥
2μ(f (x) — f?) forall X ∈ Rd.
We refer to this as PL with respect to kT∣ even though only the dual norm appears in the definition,
since it is the natural counterpart to smoothness w.r.t. ∣∣∙∣∣ and used to prove linear convergence for
steepest descent w.r.t. k ∙ k∙ As with smoothness, we have equivalence of the PL condition for all
norms, but constants may differ. Note that strong convexity implies the PL condition, but the class
of PL functions also covers some non-convex functions.
12
Under review as a conference paper at ICLR 2020
Theorem 2. If f is L-smooth and fulfills the PL condition with constant μ w.r.t ∣∣ ∙ ∣∣, then steepest
descent (Eq. 2) with step size αt ≡ 1/L satisfies
fτ - f? ≤(1- L)T (fo- f?).	(28)
We are not aware of any published work showing this simple result in its general form. We give the
very short proof here.
Proof of Theorem 2. Combining Lemma 2 and the PL condition gives
ft+ι ≤ ft -彳门川2
2L
≤ ft - (ft- - f?).
L
(29)
(30)
Subtracting f? from both sides and iterating backwards yields the statement.
□
B Details on the Relationship B etween S eparab le Smoothness
and Smoothness w.r.t. the Maximum Norm
This section presents additional insights on the relationship between separable smoothness and
smoothness w.r.t. the maximum norm. We discuss how separable smoothness relates to properties
of the Hessian of the objective and give a two-dimensional quadratic example to support intuition.
Finally, we show that existing convergence proofs for (stochastic) sign gradient descent go through
when replacing the separable smoothness assumption with the weaker smoothness w.r.t. the maxi-
mum norm.
B.1	Separable Smoothness as a Diagonal Bound on the Hessian
Bernstein et al. (2018) note that, for twice-differentiable f, separable smoothness results from -L
V2f (x) W L for all X ∈ Rd with L = diag(lι,...,ld). It is tempting to think of them as bounds
on the eigenvalues of V2f (x), but this is only true for diagonal matrices. In the following, We will
see that these values also depend on the axis-alignment of the Hessian and will generally exceed
the eigenvalues. Moreover, since “W” is only a partial ordering, it is not clear what the “tightest”
possible bound of this form would be. Since the performance of signGD depends on Pi li, the most
favorable bound will be
min	X li	s.t. - L W V2f(x) W L ∀x ∈ Rd	(31)
l1,...,ld ≥0
i
We will adopt this definition in the following.
This opens up an alternative view on the relationship between separable smoothness and smoothness
w.r.t. the maximum norm, which operates on the Hessian-based definitions of the two notions of
smoothness.
Proposition 5. Let H ∈ Rd×d be positive semi-definite with eigenvalues λ1 . . . , λd and define
L∞(H) := max ∣Hx∣1,	Lsep(H) := min li s.t. H W diag(l1,...,ld).	(32)
kxk∞≤1	li≥0
i
Then
L∞ (H) ≤ Lsep(H) ≤ ρdiag(H)-1 X λi.	(33)
i
Hence, Lsep upper-bounds L∞ while at the same time being upper-bounded by the same quantity
that appears in Proposition 3.
13
Under review as a conference paper at ICLR 2020
Figure 4: For H ∈ Rd×d, We plot a contour line of f (x) = 1 xτHx, which forms an ellipse
with principal axes given by the eigenvectors and axis lengths given by the inverse eigenvalues.
We fix λmin = 1 and vary λmax > 1 as well as the angle θ between eigenvectors and coordinate
axes. Separable smoothness bounds H by a diagonal matrix, diag(li) H, corresponding to an
axis-aligned ellipse that lies fully within the H-ellipse. (We choose li according to Eq. 31.) The
bound changes with the axis-alignment of H, becoming both smaller and more circular (i.e., larger
and more similar li ) as H rotates further from the coordinate axes. In contrast to that, Euclidean
smoothness bounds H by λmaxI H, i.e., a circle, which is rotation-invariant.
B.2 Separable Smoothness for Two-Dimensional Quadratics
To build some intuition for the separable smoothness condition, we consider the case of a two-
dimensional quadratic with positive definite Hessian V2f (x) ≡ H = [ad]. In this, case, we can
find the solution to Eq. (31) in closed form (proof in Appendix E):
l1 = a + |b|, l2 = d + |b|.	(34)
A visualization is given in Figure 4 and clearly shows that the choice of li is sensitive to the axis
alignment of the Hessian.
The eigenvalues of H evaluate to
λι∕2 = ? ± ∕a-< + F.	(35)
We see that, for |b| > 0, we will have lmax > λmax.4 We also see that lmax can exceed λmax and it is
thus misleading to assume the convergence of GD to be driven by lmax.
Also note that L∞ = kHk∞,1 = a+ d+ 2|b| = Pi li, reflecting the link to smoothness w.r.t. the
maximum norm, as discussed in the main text.
4 Of course, we could deviate from the definition of Eq. (31) and choose li ≡ λmax to guarantee lmax = λmax.
However, this bound would not be favorable for the performance of signGD, which depends on Pi li.
14
Under review as a conference paper at ICLR 2020
B.3 Existing Convergence Proofs Go Trough Under Smoothness w.r.t. the
Maximum Norm
Let xt+1 - xt ∈ {-c, c}d for some c ∈ R. Then the separable smoothness bound implies
ft+1 ≤ ft + "ft, xt+1 - xti + ɔ ^X, Ii (Xt+1,i - xt,i)2
2 V |----------z------}
=c2
c2
=ft + hVft, xt+1 - Xti + -2 ɪ2 li
i
(36)
and the L∞-smoothness w.r.t. the maximum norm yields (via Lemma 1)
ft+1 ≤ ft + "ft, xt+1 - xti + L∞k llxt+1 - xt∣∣∞
2	|{}
=c2	(37)
=ft + hvft, xt+ι - xti + ^2-L∞.
Hence, smoothness w.r.t. the maximum norm with constant L∞ = Pi li provides exactly the same
local bound for any method with updates of equal magnitude in each coordinate. This includes any
version of sign gradient descent, including in the stochastic setting. Since the convergence proofs
given by Bemstein et al. (2018; 2019) and Safaryan & Richtarik (2019) all start from this bound,
they go through when replacing the separable smoothness assumption with the weaker assumption
of (Pi li)-smoothness w.r.t. the maximum norm.
C Generalizing Relaxed Smoothness
We have discussed a scaled version of sign gradient descent, xt+ι = xt 一 α∣Vft∣ι sign(Vft),
whereas in practice the update is usually xt+ι = xt 一 at Sign(Vft) With a constant or manually
decreasing step size αt . Without this additional lVft l1 term, this is a normalized method with an
update magnitude determined solely by the step size, independent of the gradient Vft . There are
many possible reasons why this might be beneficial in practical settings like neural network training.
One rationale is that these applications are usually in the stochastic optimization setting, where a
decreasing step size is needed anyway to enforce convergence. Since lVft l1 is not available in the
stochastic setting, it might be easier to subsume a similar scaling effect in the manually-tuned step
size schedule.
There is however another potential explanation in terms of the smoothness exhibited by neural net-
work training objectives. Zhang et al. (2019) discussed the normalized gradient descent method,
1
xt+1 = xt-α lf+β
Vft.
(38)
This method is normalized since its update magnitude is upper-bounded by lxt+1 一 xt l2 ≤ α
irrespective of the gradient magnitude. For small gradient lVft l2 β, the method reverts back to
classical gradient descent. They show that this normalized method is better geared towards the type
of regularity exhibited by, say, neural network training objectives, which are not smooth in the sense
of Eq. (4). They propose a “relaxed” smoothness assumption which allows the curvature to grow
with the gradient norm instead of bounding it globally as in classical smoothness.
Since they consider normalized gradient descent, their discussion is based on Euclidean geometry.
We show here that their reasoning can be generalized to arbitrary norms, similarly to the standard
smoothness assumption discussed in the main text. This provides a possible explanation of the
practical success of normalized methods, which sign gradient descent (Eq. 1) is if we omit the
scaling by lVftl1.
C.1 Results of Zhang et al. (2019)
The relaxed smoothness condition proposed by Zhang et al. (2019) reads
lV2f(x)l2≤L(0)+L(1)lVf(x)l2,	(39)
15
Under review as a conference paper at ICLR 2020
where ∣∣ ∙ k2 for matrices denotes the spectral norm. This allows for the curvature to grow with the
gradient norm, in contrast to classical smoothness, which demands a global bound on the Hessian.
This relaxed smoothness gives rise to normalized gradient descent since, as we will see later, pro-
vides a bound of the form
ft+1 ≤ ft + "ft, xt+1 - xti + 2(A + BkVftk2)lxt+1 - xt l∣2, A, B ≥ 0.	(40)
This resembles the bound of Lemma 1, but the quadratic term now scales with the gradient norm. It
is minimized by a normalized gradient descent update with appropriately chosen α and β.
The main finding of Zhang et al. (2019) is that gradient descent can become arbitrarily slow for the
class of functions satisfying this relaxed smoothness, whereas normalized gradient descent (Eq. 38)
retains an O(1∕ε2) rate of convergence to a ε-stationary point.
C.2 Normalized Steepest Descent
In this section, we generalize the concept of relaxed smoothness to arbitrary norms, which will give
rise to general normalized steepest descent methods. We define relaxed smoothness w.r.t. to some
norm ∣∙∣ analogously to the Euclidean case (Eq. 39), but use the dual norm for the gradient and the
induced matrix norm of Proposition 2 for the Hessian.
Definition 2. Afunction f is called (L⑼ ,L⑴)-smooth with respect to some norm ∣∙∣ if
∣∣V2f(x)k ≤ L⑼+ L(I)∣∣Vf(x)k*,	(41)
where ∣∣ ∙ ∣ for matrices is the norm defined in Eq. (8).
Under this smoothness assumption, we have the following local quadratic bound:
Lemma 4. Assume f is (L⑼,L(I))-Smooth with respect to a norm ∣∣ ∙ ∣∣. Thenfor x, y ∈ Rd with
Ily - xk ≤ L11),
f(y) ≤ f(x) + Ef(X), y - xi + 2(5L(0) +4L(I)∣Vf(x)k*)ky - xk2.	(42)
This resembles the bound in Lemma 1, but the quadratic term now scales with the gradient norm.
In analogy to steepest descent, we can now construct an optimization method that minimizes this
bound in each step. Using Lemma 4 with x = xt yields
f(y) ≤ ft + hVft, y - Xti + 2(5L⑼+ 4L⑴∣Vftk*)ky - xt∣2.	(43)
Choosing xt+1 as the minimizer w.r.t. y results in the update
xt+1 = Xt- (5L⑼+4L⑴∣Vftk*) Vfk '.	(44)
We refer to the resulting method as normalized steepest descent. To see that it is in fact a normalized
method, recall from Lemma 3 that IIVfJk ∣ = ∣∣Vft∣∣* and, hence, the update magnitude is upper-
bounded by 1/4L(1).
We now show that the convergence theorem of Zhang et al. (2019) carries over to this generalized
setting. The proofs (see Appendix E) are straight-forward adaptations of that in Zhang et al. (2019),
with a little bit of extra care with regards to the norms.
Theorem 3. Assume f is (L⑼,L(1))-smooth with respect to a norm ∣ ∙ ∣. Then normalized steepest
descent (Eq. 44) converges to an ε-stationary point, ∣Vf ∣* ≤ ε, in at most
L(0) (L(1))2
Tε = 18(f0 - f*)max (F, Rr)
iterations.
(45)
16
Under review as a conference paper at ICLR 2020
D	Experimental Details
D. 1 Quadratic Experiments
Generating Hessians. W draw a random rotation matrix R from the Haar distribution5 and set
the Hessian to be H = RθΛ(Rθ)*, where Rθ for θ ∈ [0,1] is a non-integer matrix power and
A* denotes the conjugate transpose matrix. We can think of this as rotating the eigenvectors of
the Hessian by a fraction of θ in the direction prescribed by R. The non-integer matrix power
Rθ, is computed via the eigendecomposition R = UDU*. The matrix power is then given by
Rθ = UDθUH where Dθ for the diagonal matrix D is obtained by its elements to the power θ.
Computing L∞ . To compute the smoothness constant w.r.t. the maximum norm, we have to com-
pute the matrix norm kHk∞,1 = maxkxk∞≤1 kHxk1. We use the fact that the solution is attained
at x ∈ {-1, 1}d (see Rohn, 2000) and brute-force search for the maximum kH xk1 in this set. Since
there are 2d vectors in {-1, 1}d, this is only possible for relatively small dimension.
On the performance measure. When comparing gradient descent and sign gradient descent on
these quadratic problems, we use the distance to the optimum as a performance measure. The reason
is that we are interested in a comparison over a range of different quadratics with varying λmax. The
function value, which scales with λmax would not be suitable for such a comparison. Since we are
comparing optimization methods which are adapted to different norms, it might make a difference
which norm we choose to compute the distance to the optimum. We opted for the Euclidean norm
to benefit the baseline method (gradient descent) as the lesser of two evils.
D.2 Neural Network Experiments
In this section we give mode details on the experimental setting used in Figure 3:
Dataset. We used the CIFAR10 (Krizhevsky et al., 2009) dataset, which contains 50,000 images
of size (32, 32, 3). In all cases we used a batch-size of 1000 samples.
Architecture. We considered two different architectures for these experiements. The first one is a
convolutional neural network (CNN) as implemented in keras,6 consisting of 4 convolutional layers
with ReLU activations, and one last dense layer with a softmax activation function. The second
architecture that we consider is a ResNet20, (He et al., 2016) model, consisting of 20 layers and
0.27 million parameters.
Algorithm and hyperparameter selection. We consider both SGD and signSGD with an added
momentum term since this matches most closely the algorithms used in practice for this task. For
SignSGD, the sign is computed in this case on the full update, and not only on the gradient.
These algorithms have two hyperparameters to tune: the step-size and momentum.
• We select the step-size independently for each algorithm and architecture to yield the high-
est decrease in training loss at 200 and 600 epochs for the CNN and ResNet20 architecture,
respectively, which is when both architectures reach 85% test set accuracy.
• For the momentum parameter, we use the Keras default of 0.9 for all optimizers and archi-
tectures.
5The uniform distribution on the special orthogonal group SO(d) of d-dimensional rotation matrices. We
used the Specialqrtho_group function provided by the scipy.stats package (Jones et al., 2001).
6https://github.com/keras-team/keras/blob/master/examples/cifar10_cnn.
py
17
Under review as a conference paper at ICLR 2020
Estimating smoothness constants. Following Zhang et al. (2019), we approximate the smooth-
ness using directional finite differences. At a point x, we approximate
^	. l∣Vf (x + Ys)-Vf(x)k2
L2 = min--------γ⅛----------- .s = -α1Vf(X)
^	∙ kVf(X + Ys)-Vf(X)kι
L∞ = mm---------1∣-U--------- .s = -a Sign(Vf (x)),
γ	Ylsl∞
where α1 and α2 are the respective “optimal” step size for GD and signGD.
E Proofs
We now list the proofs of all theoretical results from the main text.
E.1 Proof of Lemma 1
Proof. Define g(τ) = f(X + τ(y - X)) for τ ∈ [0, 1] with g0(τ) = hVf (X + τ(y - X)), y - Xi.
Then
f(y) = f(X) + Z g0(T)dT
0
= f(X) +	hVf (X + T(y - X)), y - XidT
0
=f(X)+ hVf(X), y - Xi +Z hVf(X + τ(y - X)) - Vf(X), y - Xidτ
0
(24)	1
≤ f(x) + hVf(x),y — Xi + l IlVf(X + T(y — x))- Vf(X)I∣*∣∣y — XkdT
0
(4)	1
≤ f(X) + hVf (X), y - Xi + LIT (y - X)IIy - XIdT
0
(46)
(47)
(48)
= f(X) + hVf (X), y - Xi + LIy - XI2 Z TdT
0
=f (x) + Nf(X), y — Xi + L2 ky — x∣2.
The first inequality is due to Lemma 3(a) and the second inequality uses the L-smoothness.
□
E.2 Convergence Proofs for Steepest Descent
ProofofLemma 2. We apply Lemma 1 with y = x+ = X — LVf (x) k.k
f (x+) ≤ f (x) + hVf (x), X+ — Xi + L∣∣x+ — X∣2
1	L	1	2
=f (X) + hVf (x), — τ Vf (X)k∙ki + - - - Vf(X)k∙k	(49)
L	2	L
=f(X) - L (hVf(x), Vf(X)k∙ki- -kVf(x)k∙kk2).
ByLemma 3, we have hVf(x), Vf(X)k∙ki = IlVf(X)k∙kk2 = IlVf(X)∣2. Substituting this in
yields the desired bound.
Proof of Proposition 1. Lemma 2 gives
kVftk! ≤ 2L(ft — ft+1)	(50)
Rearranging and summing for t = 0, . . . , T — - yields
TX kVftk! ≤ 2LX(ft- ft+1) = 2L(f0r- fτ) ≤ 2L(fT- f*).	(51)
t=0	t=0
□
18
Under review as a conference paper at ICLR 2020
E.3 Proof of Proposition 2
Note that the matrix norm by construction satisfies
kHxk* ≤ kHkkxk.	(52)
Proof. We first show that Eq. (8) defines a matrix norm. Clearly kHk ≥ 0 and kHk = 0 iff H = 0.
Furthermore, kλH∣∣ = ∣λ∣∣∣H∣∣. Itremains to show subadditivity. Let H, H0 ∈ Rd×d
IIH + H0k= sup ∣(H + H0)x∣∣* ≤ SuP (IlHxi∣* + IlH0x∣∣*)
kxk≤1	kxk≤1
≤ SuP IlHxI∣* + sup IlH0x0k* = ∣H∣ + ∣H0k.
kxk≤1	kx0k≤1
(53)
Now assume ∣∣V2f (x)I ≤ L for all X ∈ Rd. Let x, y ∈ Rd and define g(τ) = Vf (x + T(y 一 x))
for τ ∈ [0, 1].
IVf(y) 一 Vf(x)I*
x))(y 一 x)dτ
*
≤	V2f(x+τ(y 一 x))(y 一 x)* dτ
0
(52)	1
≤	IV2f(x + τ(y 一 x))IIy 一 xIdτ
0
(54)
≤ LIy 一
xI Z 1
0
1dτ = LIy 一 xI.
Conversely, assume L-smoothness and fix x ∈ Rd. For any IsI ≤ 1 and ε > 0,
εV2f(x+
s
= IVf(x+εs) 一 Vf (x)I* ≤ εLIsI ≤ εL.
*
(55)
Dividing by ε and letting ε → 0, we get
IV2f(x) sI*
ε→o (ε / Vf(X+T s)dBs
(56)
V2f(x + T s)dT
≤ L.
*
ε→0 ε U
s
*
This implies IV2f(x)I = supksk≤1 IV2f(x)sI* ≤ L.
□
E.4 Proof of Proposition 3
Proof. First note that
IHI∞,1 := sup IHxI1 = sup	Hijxj ≤	|Hij |.
kxk∞≤1	kxk∞≤1 i j	ij
(57)
Recall that	i |Hii| =	i Hii = i λi for positive definite matrices. Then
IHI∞,1 ≤	|Hij|
i,j
Pjj X λi = Pdiag(H )-1 X λi.
i |Hii |	i	i
(58)
□
19
Under review as a conference paper at ICLR 2020
E.5 PROOF OF EQ. (9)
Proof. The fact that kz k∞ ≤ kz k2 ≤ kz k1 implies
L2
sup
x6=y
kv∕(y)-vf(χ)k2
l∣y - xk2
≤ s„n W(y)-Vf(x)∣ι
≤ x=py	ky - X∣∞
L∞ .
(59)
Conversely, using e∣∣z∣ι ≤ ∣∣z∣2 ≤ √d∣∣z∣∣∞
L∞
Sup ∣Vf(y)-Vf(x)∣ι
x=y	∣∣y - χ∣∞
d∣Vf(y) - Vf (x)∣2
≤ SuP	Γ-I∣ ∖∖
χ=y	√dIly - x∣2
dL2 .
(60)
E.6 Proof of Lemma 3
Proof. Statement (a) follows immediately from the definition of the dual norm.
Regarding (b), by definition of zk∙k, We know thathz, czk∙k〉- ɪ∣∣czk∙k ∣∣2 is maximized by C = 1.
Hence the derivative w.r.t. c,
-d hz,czk∙ki- Ikczk∙k∣2 = hz, zk∙ki - c∣zk∙k∣2,	(61)
dc	2
must evaluate to 0 at c = 1, which proves (b).
For (c), we use the equivalent definition
hx, zi
∣z∣* = max	.
χ=o ∣χ∣
Assume w.l.o.g. that zk'k = 0. Then
hx, Zi	hzk∙k, Zi (b) UHll
∣z∣* = mx^pτ ≥ ηριτ = ∣z ∣.
Conversely, Let x0 ∈ argmaxkχk = ιhx, z)，such that hz, x0)= ∣∣z∣∣*. Then
1 ∣z∕ = hz,∣z∣* x0 i-2 kkzk*x0『≤ hz, zk∙ki- 2 IIzkWl2 = 2 IIzkWl2,
where the inequality is by definition of z k'k.
(62)
(63)
(64)
□
E.7 Proof of Proposition 4
Proof. The dual norm of ∣ ∙ ∣L is ∣∣ ∙ ∣L-ι, such that the assumption of 1-smoothness w.r.t. ∣ ∙ ∣L
amounts to
∣Vf(y)- Vf (x)∣L-1 ≤ ∣y -x∣L ∀x,y ∈ Rd.
First, by definition of the maximum norm, we get
(65)
∣z∣L = jχ32 ≤ jχ可⑶"=yχi∣z∣∞.
Secondly, using Cauchy-Schwarz,
Hzh = X |zi1 = X √√∣= Pl ≤ SX Z- SX li = SX li HzHLT.
Combining these two inequalities with the assumption yields the assertion:
(67)	(65)
∣Vf(y) - Vf(x)∣1 ≤	li∣Vf(y) - Vf (x)∣L-1 ≤	li∣y - x∣L
(66)
(67)
(66)
≤ li li ∣y - x∣∞
x∣∞ .
(68)
□
□
20
Under review as a conference paper at ICLR 2020
E.8 PROOF OF EQ. (15)
Proof. The equivalence arises from substituting y = 一α^(X — Xt) or X = Xt — αty. With a slight
abuse of notation:
Xt+1
< ⇒ xt+1
∈ arg min Eft, X — Xti + 77— IlX — Xtk2
x	2αt
Xt — at arg min Eft, —αty> + --1∣ — αty∣2
y	2αt
^⇒ Xt+1 ∈ Xt — at arg min (at [—〈▽/,, y)+ g ∣∣y∣∣2
^⇒ Xt+1 ∈ Xt — at argmax Wft, y) — 2 ∣∣y∣∣2).
(69)
∈
□
E.9 Proof of Proposition 5
Proof. First inequality: First, let lι,...jd ≥ 0 be the minimizer in the definition of Lsep. For any Z
with IzI∞ ≤ 1, we have
ZTHz ≤ ZT diag(^ι, Sz = X liz2 ≤ X ^i.	(70)
ii
Next, we rewrite the definition of L∞ as
L∞ =	max	XTHy .	(71)
kxk∞,kyk∞≤1
and let (X, y) be the maximizer. Then due to H being psd, We have
0 ≤ (X — y)TH(X — y) = XTHX — 2XTHy + yτHy ≤ 2 X ii — 2XTHy,	(72)
i
where the last inequality is due to Eq. (70) applied to X and y. This proves the assertion, since
Pi li = LSeP and XTHy = L∞ by definition.
Second inequality: We set li = j |Hij| and denote L = diag(l1, . . . , ld). Then
[L — H]ii ≥ X IHij |	(73)
j6=i
X|[L—H]ij|=X|Hij|	(74)
j6=i	j 6=i
making L — H diagonally dominant with non-negative diagonal elements, hence positive semi-
definite. Therefore, L is in the solution space of the definition of Lsep. Now,
-1
Xλi,	(75)
i
where we used £i IHi I = Ei Hii = Ei λ>	□
E li = 9HijI= ≡⅛
E.10 PROOF OF EQ. (34)
Proof. In two dimensions, the constraint diag(l1, l2) — H 0 can be written as
l1 — a ≥ 0 and (l1 — a)(l2 — d) — b2 ≥ 0	(76)
by the leading principle minors criterion for positive definiteness. All “boundary” solutions can be
written as
b2
11 = a + s,	l2 = d + —,	S ≥ 0.	(77)
Out of those, the one that minimizes l1 + l2 is easily found to be given by s = IbI, resulting in
l1 = a + IbI,	l2 = d + IbI.	(78)
□
21
Under review as a conference paper at ICLR 2020
E.11 Proofs for Appendix C
All proofs are closely following the ones given for the Euclidean norm in Zhang et al. (2019).
To prove Lemma 4, we first need the following Lemma, which allows us to control the growth of
the gradient norm in the vicinity of a point x ∈ Rd .
Lemma 5. Assume Eq. (41) holds and let x, y with ∣∣y 一 Xk ≤ L(I). Then
kVf(y)k* ≤ 4 (£ + ∣∣Vf(x)k).	(79)
Proof. Define x(τ) = X + T(y 一x) as well as g(τ) = Vf (x(τ)) with g0(τ) = V2f (x(τ))(y — x).
Then
kVf(χ(t))k*
Vf (x) + Z g0(τ)dτ
0
≤ kVf(x)k*+ t kg0(τ)k*dτ
0
=kVf(x)k*+ t kV2f(x(τ))(y 一 x)k*dτ
0
(52)
≤kVf(X)k*+必2 L
kV2f(x(τ))k
dτ
(80)
≤ kVf(x)k*
^^^{^^^≡
≤1∕L(1
t
≤L(0)+L⑴ kVf(x(τ))k* (41)
L(0)+L(1)kVf(x(τ))k*dτ
*
t
|
}
L(0)	t
=kVf(x)k* + tLw + Jo kVf(x(τ))k*dτ
Applying the integral form of Groenwall’s inequality7 yields
kVf(x(t))k* ≤ kVf(x)k*+t
kVf (x)k* + τ
exp(t 一 τ)dτ.
(82)
We now specialize to t = 1 and upper-bound the integrand
kVf(y)k*=kVf(x(1))k*
L⑼	1 C	L⑼∖
≤ kVf(x)k* + LI)+ Jo (kVf(x)k*+ l{z}LI) I 呼(：- T)dτ
≤1	≤	≤exp(1)<3
≤ kVf(x)k* + M + 3 (kVf(x)k* + LO)) [ dτ
=4 (L0∙ + ∣Vf(x)k*).
(83)
□
We can now approach the proof of Lemma 4.
7 Groenwall’s inequality says that if u(t) ≤ α(t) + Rtt β (τ)u(τ)dτ for continuous u and β, then
u(t) ≤
α(t) +	t
t0
α(τ)β(τ) exp
β(r)dr dτ.
(81)
We apply it here with u(t) = ∣∣Vf (x(t))k* and α(t) = ∣∣Vf (x)k* + tL(0)/L(I) and β(τ) ≡ 1.
22
Under review as a conference paper at ICLR 2020
Proof of Lemma 4. According to Taylor’s theorem we have
f(y) = f(x) + hVf (x), y - Xi + 1 hy - x, V2f(ξ)(y - x)i
with some ξ ∈ {x + τ(y - x) | τ ∈ [0, 1]}. We can bound the quadratic term as
(24)
hy — x,V2f(ξ)(y — x)i ≤ ∣∣y — XkkV2f(ξ)(y — x)k*
(52)
≤ ky - xk2kV2f(ξ)k
(41)
≤ (L(0) + L⑴kVf(ξ)k*)ky - Xk2.
(84)
(85)
The first inequality is by definition of the dual norm (see Lemma 3). The second inequality is by con-
struction of the induced matrix norm. The final inequality uses the relaxed smoothness assumption
(Eq. 41).
Next, since ∣∣y 一 Xk ≤ 1/L(1) by assumption of Lemma 4, We know ∣∣ξ 一 Xk ≤ l⅛∙. Lemma 5
thus gives us
kVf(ξ)k* ≤ 4 C + kVf(X)k*) .	(86)
Plugging this back into Eq. (85) yields
hy - X, V2f(ξ)(y - X)i ≤ (5L⑼+4L⑴kVf(X)k*)ky - x∣2.	(87)
Using that in Eq. (84) proves the assertion.	□
Finally, we prove Theorem 3
Proof of Theorem 3. Using Lemma 4 with X = Xt and y = Xt+1 = Xt - ntVftuι yields
2
ft+1 ≤ ft - nthVft, VftHIi + nt(5L⑼ +4L⑴kVftk*)kVftHl k2.
Recall from Lemma 3 that (z, zk,ki = ∣∣zk∙k k2 = I∣zk2 and, hence,
(88)
ft+ι ≤ ft- nt W ft, Vfiki +
)+4L ⑴ kVft k*)kVftHlk2
ft -	ηt -
) +4L(1)kVft k*)	kVftk2
(89)
=f _	kVftk*
ft 2(5L(O)+4L(I) kVft kJ
Ifε ≤ kVftk* ≤ L(O)/L(1), we get
ε2
ft+1 ≤ ft - 18L⑼
(90)
IfkVftk* ≥ L(O)/L⑴，we get
ft+1 ≤ ft -
ft -
kVft k 2
2(5L(O)+4L ⑴ kVft k*)
kVftk*
≤ ft -
≤ ft -
10L(O)/kVftk* +8L(I)
kVftk*
18L(I)
L(0)
18(L(I))2
(91)
23
Under review as a conference paper at ICLR 2020
Hence,
L(0)	ε2
ft+1 ≤ ft	{l8(L⑴)2, 18L(0) ʃ .	(9 )
Now assume that We have T iterations with ∣∣Vftk* ≥ ε. Then
T-1	L(0)	ε2
f0- f* ≥ f0-fτ = *- ft+ι) ≥ Tmin {18(1(1)7, ^LW 卜	(93)
Rearranging yields
T ≤ 18 . nf°L-)f* .2 o = 18(f0 — f*)max (L0, (ɪ) .	(94)
mini (L(1))2 , L(0T/	'	)
□
24