Under review as a conference paper at ICLR 2020
Deep Randomized Least Squares Value Itera-
TION
Anonymous authors
Paper under double-blind review
Ab stract
Exploration while learning representations is one of the main challenges Deep
Reinforcement Learning (DRL) faces today. As the learned representation is de-
pendant in the observed data, the exploration strategy has a crucial role. The pop-
ular DQN algorithm has improved significantly the capabilities of Reinforcement
Learning (RL) algorithms to learn state representations from raw data, yet, it uses
a naive exploration strategy which is statistically inefficient. The Randomized
Least Squares Value Iteration (RLSVI) algorithm (Osband et al., 2016), on the
other hand, explores and generalizes efficiently via linearly parameterized value
functions. However, it is based on hand-designed state representation that requires
prior engineering work for every environment. In this paper, we propose a Deep
Learning adaptation for RLSVI. Rather than using hand-design state representa-
tion, we use a state representation that is being learned directly from the data by a
DQN agent. As the representation is being optimized during the learning process,
a key component for the suggested method is a likelihood matching mechanism,
which adapts to the changing representations. We demonstrate the importance of
the various properties of our algorithm on a toy problem and show that our method
outperforms DQN in five Atari benchmarks, reaching competitive results with the
Rainbow algorithm.
1	Introduction
In Reinforcement Learning (RL), an agent seeks to maximize the cumulative rewards obtained from
interactions with an unknown environment (Sutton et al., 1998). Since the agent can learn only by
its interactions with the environment, it faces the exploration-exploitation dilemma: Should it take
actions that will maximize the rewards based on its current knowledge or instead take actions to
potentially improve its knowledge in the hope of achieving better future performance. Thus, to find
the optimal policy the agent needs to use an appropriate exploration strategy.
Classic RL algorithms were designed to face problems in the tabular settings where a table con-
taining a value for each state-action pair can be stored in the computer’s memory. For more general
settings, where generalization is required, a common practice is to use hand-designed state represen-
tation (or state-action), upon which a function approximation can be learned to represent the value
for each state and action. RL algorithms based on linear function approximation have demonstrated
stability, data efficiency and enjoys convergence guarantees under mild assumptions (Tsitsiklis &
Van Roy, 1997; Lagoudakis & Parr, 2003). They require that the desired learned function, e.g. Q-
function, will be a linear combination of the state representation. This is, of course, a hard constraint
as the representation is hand-designed, where the designer often does not know how the optimal
value-function will look like. Furthermore, hand-designed representation is environment-specific
and requires re-designing for every new environment.
The DQN algorithm (Mnih et al., 2015) has changed RL. Using Deep Neural Networks (DNN) as
function approximators, the DQN algorithm enabled the learning of policies directly from raw high-
dimensional data and led to unprecedented achievements over a wide variety of domains (Mnih et al.,
2015). Over the years, many improvements to DQN were presented, suggesting more fitting network
architectures (Wang et al., 2015), reducing overestimation (Van Hasselt et al., 2016; Anschel et al.,
2017) or improving its data efficiency (Schaul et al., 2015). Despite its great success, DQN uses
the overly simple -greedy strategy for exploration. This strategy is one of the simplest exploration
1
Under review as a conference paper at ICLR 2020
strategies that currently exist. The agent takes random action with probability and takes the optimal
action according to its current belief with probability 1 - . This strategy is commonly used despite
its simplicity and proven inefficiency (Osband et al., 2016). The main shortcoming of -greedy and
similar strategies derives from the fact that they do not use observed data to improve exploration. To
explore, it takes a completely random action, regardless of the experience obtained by the agent.
Thompson Sampling (TS) (Thompson, 1933), is one of the oldest heuristics to address the ’explo-
ration/exploitation’ trade-off in sequential decision-making problems. Its variations were proposed
in RL (Wyatt, 1998; Strens, 2000) and various bandits settings (Chapelle & Li, 2011; Scott, 2010).
For Multi-Armed Bandit (MAB) problems, TS is very effective both in theory (Agrawal & Goyal,
2012; 2013) and practice (Chapelle & Li, 2011). Intuitively, TS randomly takes actions according to
the probability it believes to be optimal. In practice, a prior distribution is assumed over the model’s
parameters p(w), and a posterior distribution p(w|D) is computed using the Bayes theorem, where
D is the observed data. TS acts by sampling models from the posterior distribution, and plays the
best action according to these samples.
Randomized Least Squares Value Iteration (Osband et al., 2016) is an RL algorithm which uses lin-
ear function approximation and is inspired by Thompson Sampling. It explores by sampling plau-
sible Q-functions from uncertainty sets and selecting the action that optimizes the sampled models.
This algorithm was proven to be efficient in tabular settings, with a bound on the expected regret
that match the worst-case lower bound up to logarithmic factors. More importantly, it demonstrates
efficiency even when generalization is required. Alas, as it assumes a linearly parametrized value
function on a hand-designed state representation, the success of this algorithm crucially depends on
the quality of the given state representation.
In this paper, we present a new DRL algorithm that combines the exploration mechanism of RLSVI
with the representation learning mechanism of DQN; we call it the Deep Randomized Least Squares
Value Iteration (DRLSVI) algorithm. We use standard DQN to learn state representation and ex-
plores by using the last layer’s activations of DQN as state representation for RLSVI. To compen-
sate for the constantly changing representation and the finite memory of DQN, we use a likelihood
matching mechanism, which allows the transfer of information held by an old representation regard-
ing past experience. We evaluate our method on a toy-problem - the Augmented Chain environment
-for a qualitative evaluation of our method on a small MDP with a known optimal value function.
Then, we compare our algorithm to the DQN and Rainbow algorithms on several Atari benchmarks.
We show that it outperforms DQN both in learning speed and performance.
2	Related Work
Thompson Sampling in Multi-Armed Bandit problems: Thompson Sampling (TS) (Thompson,
1933), is one of the oldest heuristics to address the ’exploration/exploitation’ trade-off in sequen-
tial decision-making problems. Chapelle & Li (2011) sparked much of the interest in Thompson
Sampling in recent years. They rewrote the TS algorithm for Bernoulli bandit and showed impres-
sive empirical results on synthetic and real data sets that demonstrate the effectiveness of the TS
algorithm. Their results demonstrate why TS might be a better alternative to balance between ex-
ploration and exploitation in sequential decision-making problems than other popular alternatives
like the Upper Confidence Bound algorithm (Auer et al., 2002). Agrawal & Goyal (2013) sug-
gested a Thompson Sampling algorithm for the linear contextual bandit problem and supplied a
high-probability regret bound for it. They use Bayesian Linear Regression (BLR) with Gaussian
likelihood and Gaussian prior to design their version of Thompson Sampling algorithm. Riquelme
et al. (2018) suggested performing a BLR on top of the representation of the last layer of a neural
network. The predicted value vi for each action ai is given by vi = βiT zx, where zx is the output
of the last hidden layer of the network for context x. While linear methods directly try to regress
values v on x, they independently trained a DNN to learn a representation z, and then used a BLR
to regress V on z, obtaining uncertainty estimates on the β's, and making decisions accordingly via
Thompson Sampling. Moreover, the network is only being used to find good representation - z .
Since training the network and updating the BLR can be done independently, they train the network
for a fixed number of iterations, then, perform a forward pass on all the training data to obtain the
new zx , which is then fed to the BLR. This procedure of evaluating the new representation for all
the observed data is very costly, moreover, it requires infinite memory which obviously does not
2
Under review as a conference paper at ICLR 2020
scale. Zahavy & Mannor (2019) suggested matching the likelihood of the reward under old and new
representation to avoid catastrophic forgetting when using such an algorithm with finite memory.
Thompson Sampling in RL: In the Reinforcement Learning settings, Strens (2000) suggested a
method named ”Posterior Sampling for Reinforcement Learning” (PSRL) which is an application
of Thompson Sampling to Model-Based Reinforcement Learning. PSRL estimates the posterior
distribution over MDPs. Each episode, the algorithm samples MDP from it and finds the optimal
policy for this sampled MDP by dynamic programming. Recent work (Osband et al., 2013; Os-
band & Van Roy, 2017) have shown a theoretical analysis of PSRL that guarantees strong expected
performance over a wide range of environments. The main problem with PSRL, like all model-
based approaches, is that it may be applied to relatively small environments. The Randomized Least
Squares Value Iteration (RLSVI) algorithm is an application of Thompson Sampling to Model-Free
Reinforcement Learning. It explores by sampling plausible Q-functions from uncertainty sets and
selecting the action that optimizes the sampled models.
Thompson Sampling in DRL: Various approaches have been suggested to extend the idea behind
RLSVI to DRL. Bootstrapped DQN (Osband et al., 2017) uses an ensemble of Q-networks, each
trained with slightly different data samples. To explore, Bootstrapped DQN randomly samples one
of the networks and acts greedy with respect to it. Recently, Osband et al. (2018) extended this
idea by supplying each member of the ensemble with a different prior. Fortunato et al. (2017) and
Plappert et al. (2017) investigate a similar idea and propose to adaptively perturb the parameter-
space, which can also be thought of as tracking approximate posterior over the network’s param-
eters. O’Donoghue et al. (2017) proposed TS in combination with uncertainty Bellman equation,
which connects the uncertainty at any time-step to the expected uncertainties at subsequent time-
steps. Recently and most similar to our work, Azizzadenesheli et al. (2018) experimented with a
Deep Learning extension to RLSVI. They changed the network architecture to exclude the last layer
weights, optimized the hyper parameters and used double-DQN. In contrary, we don’t change any-
thing in the DQN agent. We use the representation learned by DQN to perform RLSVI, however,
the network structure, loss and hyper-parameters are the same. Additionally, differently from our
method, they don’t compensate for the changing representation and solve BLR problem with the
same arbitrary prior every time.
3	Preliminaries
We consider the standard RL settings (Sutton et al., 1998), in which an environment with discrete
time steps is modeled by a Markov Decision Process (MDP). An MDP is a tuple < S, A, P, R, γ >,
where S is a state space, A a finite action space, P : S × A -→ ∆(S), is a transition kernel, and
R : S × A -→ R a reward function. At each step the agent receives an observation st ∈ S which
represents the current physical state of the system, takes an action at ∈ A which is applied to
the environment, receives a scalar reward rt = r(st , at), and observes a new state st+1 which the
environment transitions to. As mentioned above, the agent seeks an optimal policy ∏* : S → ∆(A),
mapping an environment state to probabilities over the agent’s executable actions. γ ∈ (0, 1) is the
discount factor - a scalar representing the trade-off between immediate and delayed reward. A brief
survey of the DQN algorithm can be found in Appendix 1.
3.1	Rand omized Least S quares Value Iteration
The Randomized Least Squares Value Iteration (RLSVI) algorithm is a TS-inspired exploration
strategy for Model-Free Reinforcement Learning. It combines TS-like exploration and linear
function approximation, where the main novelty is in the manner in which it explores: Sam-
pling value-functions rather than sampling actions. The Q-function is assumed to be in the form
Q(s, a) = φ(s, a)T w, where φ(s, a) is a hand-designed state-action representation. RLSVI oper-
ates similar to other linear function algorithms and minimizes the Bellman equation by solving a
regression problem - Bayesian Linear Regression. BLR obtains a posterior distribution over value-
function instead of point estimates. To explore, RLSVI samples plausible value functions from the
posterior distribution and acts the greedy action according to the sampled value-function. In the
episodic settings where the representation is tabular, i.e., no generalization is needed, RLSVI guar-
antees near-optimal expected episodic regret. Finally, the main benefit of this algorithm is that it
3
Under review as a conference paper at ICLR 2020
displays impressive results even when generalization is required - despite the lack of theoretical
guarantees. A pseudo-code can be found in Appendix 1.
4	The Deep Randomized Least S quares Value Iteration
ALGORITHM
In this paper, we propose to use RLSVI as the exploration mechanism for DQN. RLSVI capabilities
are enhanced by using state representation that is learned directly from the data by a neural network
rather than hand-designed one. As the neural network gradually improves its representation of
the states, a likelihood matching mechanism is applied to transfer information from old to new
representations.
4.1	Learning Representation
A DQN agent is trained in the standard fashion, i.e., the same architecture, hyper-parameters and
loss function as the original DQN. Two exceptions were made (1) The size of the last hidden layer
is reduced to be d = 64. (2) The Experience Replay buffer is divided evenly between actions and
transitions are stored in a round-robin fashion. I.e., whenever the buffer is full, a new transition
< st, at, rt, st+1 > is placed instead of the oldest transition with the same action at.
4.2	Exploration
Exploration is performed using RLSVI on top of the last hidden layer of the target network. Given a
state st, the activations of the last hidden layer of the target network applied to this state are denoted
as φ(st) = LastLayerActivations(Qθtarget (st)). Several changes to the original RLSVI algorithm
were made: First, rather than solving different regression problem for every time step, a different
regression problem is being solved for every action. As the last hidden layer’s activations serves
as state representation, the representation is time-homogeneous and shared among actions. The
regression targets y are DQN’s targets which use the target network predictions. Another change
is that a slightly different formulation of Bayesian Linear Regression than RLSVI is being used.
Similar to RLSVI a Gaussian form for the likelihood is assumed: Q(s, a) 〜N(WTφ(s),σ2),
however, like Riquelme et al. (2018) the noise parameter σ2 is formulated as a random variable,
which is distributed according to the Inverse-Gamma distribution. The prior for each regression
problem is therefore in the form: p(w,σ2) = p(w∣σ2)p(σ2),p(w∣σ2) 〜 N(μo, σ2∑0),p(σ2) 〜
InvGamma(ao, b0). For this prior and the Gaussian likelihood Q(s, a) 〜 N(WTφ(s), σ2) it is
known that the posterior distribution can be calculated analytically as follows:
φn={φ(s1),..., φ(sn)}∈Rd×n, Yn=(y1,...,yn)T ∈Rn
∑n = (ΦnΦT + ∑-1)-1,	μn = ∑n(ΦH + ∑-1μθ),
an =	a0	+ 2 ,	bn	=	b0	+ 2(Yn Yn	+	μT	»0 1〃0 -	μT £n 1 Un),
(1)
Formulating σ 2 as a random variable allows adaptive exploration where the adaptation is derived
directly from the observed data. Lastly, while RLSVI’s choice for the prior’s parameters is somewhat
arbitrary, in our algorithm the prior has a central role which we’ll discuss further on.
Since RLSVI requires a fixed representation and the target network’s weights are fixed, we use
the last layer activations of the target network, denoted φ(s), as state-representation. Every T target
training time steps, the target network is updated with the weights of the Q-network. In these T target
time steps the Q-network is changing due to the optimization performed by the DQN algorithm.
Since the representation is changing, the posterior distribution that was approximated in the old
representation can’t be used. A posterior distribution based on the new representation needs to be
approximated. Therefore, whenever the target network changes, new Bayesian linear regression
problems are being solved using NBLR samples from the ER buffer. Since the ER buffer is finite,
some experience-tuples were used to approximate the posterior in the old representation and are
no longer available. Ignoring this lost experience can and will lead to degradation in performance
derived by ’Catastrophic Forgetting’ (Kirkpatrick et al., 2017). To compensate for the changing
representation and the loss of old experience, we follow (Zahavy & Mannor, 2019) and match the
4
Under review as a conference paper at ICLR 2020
likelihood of the Q-function in the old and new representation. This approach assumes that the
important information from old experiences is coded in the state representation.
Algorithm 1 Deep RLSVI
Input: s0 - Initial state, Qθ(s, a), Qθtarget (s, a), ER buffer,
Prior： σ2 〜InvGamma(ɑa,o,ba,o),p(wa,o∣σ2)〜N(μa,o,σ2∑a,o)
Define:	φ(st) J LastLayerActivation(Qθtarget),	ψ(st) J LastLayerActivation(Qθ)
for t = 0, 1... do
if t mod Tsample then
for a = 0, ..., |A|, j = 0, ..., |J| do
Sample σ2,j 〜InvGamma(a0, ba)
Sample Wa,j 〜N(〃a, σ2,jΣa)
end for
end if
Sample ja 〜U{1,2,..., ∣J∣}∀a ∈ A
Act at ∈ argmaxa WTja φ(st)
Observe st+1, rt, Store Transition < st, at, st+1, rt >
Train DQN using sampled mini-batch
if t mod Ttarget = 0 then
for a = 0, ..., |A| do
Construct Priors μo, ∑o by likelihood matching (Equation 2)
Sample NBLR transitions < si, a, si+1, ri > from ER buffer
Solve Bayesian Linear Regression (Equation 1)
end for
Qθtarget (s, a) J- Qθ(s, a)
end if
end for
4.3	Constructing Priors by Likelihood matching
Recall that the likelihood of the Q function is Q(s,a) 〜 N(Waφ(s),σ2), our best es-
timate for this likelihood is to plug-in the posterior approximation for Wa: Q(s, a) 〜
(WTμφ,σ2φa(s)Σφφ(s)). The likelihood for the new representation, ψ(s), is in the same form:
Q(s,a) 〜(WTμψ,σ2ψa(s)Σψψ(s)). Since the likelihood is Gaussian, to compensate for the
changing representation, we will find moments that match the likelihood of the Q-function in the
new representation to the old one and use them for our Gaussian prior belief.
Expectation prior: As DQN is trained to predict the Q-function, given the new last layer activations
ψ, a good prior for μo in the new representation will be the last layer weights of the DQN (Levine
et al., 2017).
Covariance prior: We use NSDP samples from the experience replay buffer. We evalu-
ate both old and new representation {φ(si), ψ(si)}iN=S1DP . Our goal is to find a solution Σ0ψ
that will match the covariance of the likelihood in the new representation to the old one:
ψ(si)T Σ0ψψ(si) = φ(si)TΣφnφ(si). Using the cyclic property of the trace, this is equivalent to
finding Trace(ψ(si)ψ(si)T Σ0ψ) = Si, where Si = φ(si)TΣ0φφ(si) = Trace(φ(si)TΣ0φφ(si)). We
denote Ψi = ψ(si)ψ(si)T ∈ Rd×d. Adding the constraint that Σ0ψ should be Positive-Semi-Definite
as it is a covariance matrix, we end up with the following Semi-Definite Program (SDP) (Vanden-
berghe & Boyd, 1996):
m
minX ||Trace(Ψi∑ψ) 一 S∕∣2 s.t. Σψ 占 0	(2)
Σ0ψ i=0
In practice, we solve this SDP by using CVXPY (Diamond & Boyd, 2016).
5
Under review as a conference paper at ICLR 2020
4.4	Reducing Computation Complexity
Approximate Sampling: To perform Thompson Sampling one needs to sample the posterior dis-
tribution before every decision. This, regrettably, is computationally expensive and will slow down
training significantly. To speed UP learning, We sample j weights for every action {Wi,ι,…，W%,j}
every T Sample time steps. Then, every step we sample an index ia ∈ 1, ..,j for every action, which
is computationally cheap, and act greedy accordingly: at = arg maxa WT,ictφ(st).
Solving the SDP: Another bottleneck our algorithm faces is solving the SDP. We refer the reader
to Vandenberghe & Boyd (1996) for an excellent survey on the complexity of solving SDPs. As the
running time ofan SDP solver mainly depends on the dimension of the representation d, the number
of samples being used NSDP and the desired accuracy , we chose the last hidden layer size to be
d = 64, used NSDP = 600 for every SDP and set = 1e - 5. The running time for solving a single
SDP took us 10-50 seconds using Intel’s Xeon CPU E5-2686 v4 2.30 GHz.
5	Experiments
We conduct a series of experiments that highlight the different aspects of our method. We begin
with a qualitative evaluation of our algorithm on a simple toy environment, then, move to report
quantitative results on 5 different Atari games in the ALE.
Figure 1: The Augmented Chain MDP
5.1	The Augmented Chain Environment
Setup: The chain environment includes a chain of states S = 1, ..., n. In each step, the agent
can transition left or right. This standard-settings is augmented with additional k actions which
transitions the agent to the same state (self-loop). We name this variation ”The Augmented Chain
Environment”. All states have zero rewards except for the far-right n-state which gives a reward
of 1. Each episode is of length H = n - 1, and the agent will begin each episode at state 1.
The raw state-representation is a one-hot vector. The Q-network is an MLP with 2 hidden layers.
Results are averaged across 5 different runs. We report the cumulative episodic regret: Regret(T ) =
Pt=0(v0(s0) — Ph=o rt,h). Here T is the number of played episodes,喘(s0) is the return of the
optimal policy, and rt,h is the reward the agent received in episode t at time step h. An illustration
for the augmented chain environment can be found in figure 1. The hyper-parameters that are being
used in the following experiments can be found in Appendix 2.
5.1.1	Epsilon-greedy
We compared our algorithm to standard DQN where -greedy serves as the exploration strategy. We
experimented with various values, however, as the results for different values were similar, we
display the result for a single value (Figure 2 (a)). We can see that -greedy (red) achieves a linear
regret in T which is the lower bound for this type of problems, while our algorithm (blue) achieves
much lower regret. These results demonstrate that -greedy can be highly inefficient even in very
simple scenarios.
6
Under review as a conference paper at ICLR 2020
(c)	(d)
Figure 2: Experimental results in the ”Augmented Chain Environment”. (a) Comparison of -greedy
exploration with RLSVI exploration. (b) Effect of modeling the noise parameter as a random vari-
able in comparison to different choices of a constant value. (c) Effect of the likelihood matching
mechanism. (d) Effect of the buffer size.
5.1.2	Adaptive Sigma
In this experiment, we compared our algorithm with variants that do not model σ2 as a random
variable. We experimented with various constant σ2 values. We can see that modeling σ2 as a
random variable (red) leads to lower regret compared to constant σ2 variants (Figure 2 (b)). Note
that choosing a small value for σ2 (blue) results in near-deterministic posterior function. Therefore
the results are very similar to the -greedy variant. Intuitively, a deterministic posterior acts as
a 0-greedy strategy. On the other hand, choosing a high value for σ2 (purple) results in a very
noisy sampling of the posterior approximation, therefore we get a policy which is relatively random
concluding in a bad performance. Choosing σ2 with the appropriate size for the given MDP (green,
black) results in better performance, as indicated by the lower regret. However, as σ2 is constant it
doesn’t adapt. We can see that the regret at the beginning of the learning is better even compared
to the adaptive-version. However, as the uncertainty level decrease, the algorithm ”over-explores”
which results in inferior regret compared to the adaptive version.
5.1.3	Likelihood Matching
We compared our method with a variant that matches only the expectation, similar to (Levine et al.,
2017), and a variant that does not match the likelihood at all, i.e., approximates the posterior with
a fixed arbitrary prior. The version that does not match the likelihood at all is close to BDQN
(Azizzadenesheli et al., 2018) and can be thought of as our implementation for it. Additionally,
We report the results of a variant of the algorithm where the ER buffer is not bounded - this is
possible due to the fact that the toy problem is very small and so choosing large enough buffer
serves as infinite. Results are shown in Figure 2 (c). The superiority of our method (black) over the
one-moment method (red) and no-prior at all (blue) support our claim that constructing priors by
likelihood matching reduces the forgetting phenomenon. Additionally, the fact that the unbounded
memory algorithm (green) doesn’t demonstrate any degradation in performance confirms that this
phenomenon is caused since the ER buffer is bounded.
7
Under review as a conference paper at ICLR 2020
Figure 3: Learning curves of DQN(blue), DRLSVI(red), Rainbow(green) for the first 10M time
steps, for 5 different Atari games
5.1.4	Buffer Size
The previous experiment may suggest that catastrophic forgetting in DRLSVI can be avoided by
simply increasing the buffer size. In the following experiment, We examine the simple Chain envi-
ronment (no self-loop actions; k = 0), with the following modification: we replaced the meaning
of the actions in half of the states, i.e., to move right in the odd states, the agent needs to take the
opposite action from the even states. We compare our algorithm with variants that do not match the
likelihood with different buffer sizes. Figure 2 (c) shows the performance of each of the algorithms
in this setup. We can see that our algorithm (blue) doesn’t suffer from degradation of performance.
The other algorithms, that don’t match the likelihood, all suffer from degradation, where the only
difference is the time in which the degradation starts. These results demonstrate that without the
likelihood matching mechanism, catastrophic forgetting will occur regardless of the buffer size. It is
interesting to observe how catastrophic forgetting happens: When the buffer reaches a point where it
doesn’t contain experience of acting the non-optimal actions, a quick degradation occurs. Then, the
algorithm initially succeeds to re-learn the optimal policy and the regret saturates. This phenomenon
is getting increasingly aggravated until the regret becomes linear. These chain of events occurred in
all the experiments without likelihood matching regardless of the buffer size.
5.2	The Arcade Learning Environment
We report the performance of our algorithm across 5 different Atari games. We trained our algo-
rithm for 10 million time steps and followed the standard evaluation: Every 250k training time steps
we evaluated the model for 125k time steps. Reported measurements are the average episode return
during evaluation. For evaluation, we used the learned Q-network with -greedy policy ( = 0.001),
results are averaged across 5 different runs. We use the original DQN’s hyper-parameters. Hyper-
parameters that are only relevant for our method are summarised in Appendix 2. For comparison,
we used the publically available learning curves for DQN1 and Rainbow from the Dopamine frame-
work (Castro et al., 2018). Rainbow (Hessel et al., 2018) is a complex agent comprised of multiple
additions to the original DQN algorithm. The averaged scores for the three methods are presented
in Figure 3. The evaluation suggests that our method explores in a much faster rate than DQN, and
is competitive with the Rainbow algorithm that combines multiple improvements to the DQN.
Note: Azizzadenesheli et al. (2018) didn’t supply standard evaluation metrics and reported results
for a single run only. Additionally, they change the architecture of the Q-network to exclude last
layer weights, so a direct comparison to our method is not feasible. We, therefore, didn’t compare
our results with theirs.
6	Discussion
A Deep Learning adaptation to RLSVI was presented which learn the state representation directly
from the data. We demonstrated the different properties of our method in experiments and showed
the promise of our method. We hope to further reduce the complexity and running time of our
algorithm in future work.
1In the publicly available results the authors use a different set of hyper-parameters than the original paper.
We use the original paper’s hyper-parameters. Notice that the results for DQN are generally the same for the
new set of hyper-parameters, however, they may vary for a specific game
8
Under review as a conference paper at ICLR 2020
References
Shipra Agrawal and Navin Goyal. Analysis of thompson sampling for the multi-armed bandit prob-
lem. In Conference on Learning Theory, pp. 1-39, 2012.
Shipra Agrawal and Navin Goyal. Thompson sampling for contextual bandits with linear payoffs.
In International Conference on Machine Learning, pp. 127-135, 2013.
Oron Anschel, Nir Baram, and Nahum Shimkin. Averaged-dqn: Variance reduction and stabilization
for deep reinforcement learning. In Proceedings of the 34th International Conference on Machine
Learning-Volume 70, pp. 176-185. jmlr.org, 2017.
Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit
problem. Machine Learning, 47(2-3):235-256, 2002.
Kamyar Azizzadenesheli, Emma Brunskill, and Animashree Anandkumar. Efficient exploration
through bayesian deep q-networks. In ITA. IEEE, 2018.
Pablo Samuel Castro, Subhodeep Moitra, Carles Gelada, Saurabh Kumar, and Marc G. Bellemare.
Dopamine: A Research Framework for Deep Reinforcement Learning. 2018. URL http:
//arxiv.org/abs/1812.06110.
Olivier Chapelle and Lihong Li. An empirical evaluation of thompson sampling. In Advances in
Neural Information Processing Systems, pp. 2249-2257, 2011.
Steven Diamond and Stephen Boyd. CVXPY: A Python-embedded modeling language for convex
optimization. Journal of Machine Learning Research, 17(83):1-5, 2016.
Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Ian Osband, Alex Graves,
Vlad Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin, et al. Noisy networks for exploration.
arXiv preprint arXiv:1706.10295, 2017.
Matteo Hessel, Joseph Modayil, et al. Rainbow: Combining improvements in deep reinforcement
learning. In AAAI, 2018.
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A
Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcom-
ing catastrophic forgetting in neural networks. Proceedings of the national academy of sciences,
114(13):3521-3526, 2017.
Michail G Lagoudakis and Ronald Parr. Least-squares policy iteration. Journal of machine learning
research, 4(Dec):1107-1149, 2003.
Nir Levine, Tom Zahavy, et al. Shallow updates for deep reinforcement learning. In NIPS, 2017.
Long-Ji Lin. Reinforcement learning for robots using neural networks. Technical report, 1993.
Volodymyr Mnih, Koray Kavukcuoglu, et al. Human-level control through deep reinforcement
learning. Nature, 518, 2015.
Brendan O’Donoghue, Ian Osband, Remi Munos, and Volodymyr Mnih. The uncertainty bellman
equation and exploration. arXiv preprint arXiv:1709.05380, 2017.
Ian Osband and Benjamin Van Roy. Why is posterior sampling better than optimism for reinforce-
ment learning? In Proceedings of the 34th International Conference on Machine Learning-
Volume 70, pp. 2701-2710. jmlr.org, 2017.
Ian Osband, Daniel Russo, and Benjamin Van Roy. (more) efficient reinforcement learning via
posterior sampling. In Advances in Neural Information Processing Systems, pp. 3003-3011, 2013.
Ian Osband, Benjamin Van Roy, and Zheng Wen. Generalization and exploration via randomized
value functions. In ICML, 2016.
Ian Osband, Daniel Russo, Zheng Wen, and Benjamin Van Roy. Deep exploration via randomized
value functions. arXiv preprint arXiv:1703.07608, 2017.
9
Under review as a conference paper at ICLR 2020
Ian Osband, John Aslanides, and Albin Cassirer. Randomized prior functions for deep reinforcement
learning. In Advances in Neural Information Processing Systems, pp. 8626-8638, 2018.
Matthias Plappert, Rein Houthooft, Prafulla Dhariwal, Szymon Sidor, Richard Y Chen, Xi Chen,
Tamim Asfour, Pieter Abbeel, and Marcin Andrychowicz. Parameter space noise for exploration.
arXiv preprint arXiv:1706.01905, 2017.
Carlos Riquelme, George Tucker, and Jasper Snoek. Deep bayesian bandits showdown. In ICML,
2018.
Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. arXiv
preprint arXiv:1511.05952, 2015.
Steven L Scott. A modern bayesian look at the multi-armed bandit. Applied Stochastic Models in
Business and Industry, 26(6):639-658, 2010.
Malcolm Strens. A bayesian framework for reinforcement learning. In ICML, volume 2000, pp.
943-950, 2000.
Richard S Sutton, Andrew G Barto, et al. Introduction to reinforcement learning, volume 135. MIT
press Cambridge, 1998.
William R Thompson. On the likelihood that one unknown probability exceeds another in view of
the evidence of two samples. Biometrika, 25, 1933.
John N Tsitsiklis and Benjamin Van Roy. Analysis of temporal-diffference learning with function
approximation. In Advances in Neural Information Processing Systems, pp. 1075-1081, 1997.
Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-
learning. In Thirtieth AAAI Conference on Artificial Intelligence, 2016.
Lieven Vandenberghe and Stephen Boyd. Semidefinite programming. SIAM review, 38(1):49-95,
1996.
Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Van Hasselt, Marc Lanctot, and Nando De Freitas.
Dueling network architectures for deep reinforcement learning. arXiv preprint arXiv:1511.06581,
2015.
Jeremy Wyatt. Exploration and inference in learning from reinforcement. 1998.
Tom Zahavy and Shie Mannor. Deep neural linear bandits: Overcoming catastrophic forgetting
through likelihood matching. arXiv preprint arXiv:1901.08612., 2019.
10
Under review as a conference paper at ICLR 2020
Appendix 1:	Additional Preliminaries
Deep Q-Networks
The Deep Q-Networks (DQN) algorithm was the first algorithm that successfully combined Deep
Learning architectures with Reinforcement Learning algorithms. It operates in the standard RL set-
ting (Sutton et al., 1998) where the state space is high dimensional. It approximates the optimal
Q-function using a Convolutional Neural Network (CNN). The algorithm maintains two DNNs,
the Q-network with weights θ and the target network with weights θtarget. The Q-Network gets
a state s as input and produce |A| outputs, each one representing the Q-value of a different ac-
tion a, Q(s, a). The target network is an older version of the Q-network with fixed weights. The
target network is used to constructs the targets y that the Q-network is trained to predict. The
targets y are based on the Bellman equation. The algorithm uses Stochastic Gradient Descent
(SGD) to update the network’s weights, by minimizing the mean squared error of the Bellman
equation defined as E[∣∣Qθ(st, at) - yt||2, where the target yt = r if st+ι is terminal, otherwise
yt = rt + γ maxa0 Qθtarget(st+1, a0). The weights of the target network are set to θ every fixed num-
ber of time steps, T target. The tuples < st, at, rt, st+1 > that are used to optimize the network’s
weights are first collected into an Experience Replay (ER) buffer (Lin, 1993). When performing
an optimization step, a mini-batch of samples are randomly selected from the buffer and are used
to calculate the gradients. DQN is an off-policy algorithm which allows the agent to learn from
experience collected by other means rather than its own experience. To explore the environment it
applies the -greedy strategy, i.e., with probability it takes random action and with probability 1 -
it takes the greedy action with respect to the current estimate of Q.
Algorithm 2 Deep Q-Networks
Input: Qθ(s, a), Qθtarget (s, a), , ER buffer
s0 = EnvironmentReset()
for t = 0, 1, ... do
Sample Et 〜U(0,1)
if t < then
at 〜U口，…，|A|}
else
at = arg maxα Qθ (s, α)
end if
Act at
Observe st+1, rt, dt (dt = 1 if st is terminal)
Store Transition < st, at, st+1, rt, dt >
if dt = 1 then
st+1 = EnvironmentReset()
end if
Sample n transitions < si, at, si+1, ri, di > from ER buffer
yi = ri + (1 - dt)γmaxi Qtarget(st+1,i)
θ J Vθ||Q(Si, ai) - yi||2
if (t mod Ttarget) = 0 then
Qθtarget (s, a) J- Qθ(s, a)
end if
end for
11
Under review as a conference paper at ICLR 2020
Pseudo code for RLSVI
Algorithm 3 Randomized Least Squares Value Iteration
Input: Feature Extractors: Φ0, ..., ΦH-1 ,
Parameters: λ > 0, σ > 0
Output：京io,…,仄H-i
Sample 石0,0,…,石0,H-1 〜N(0,σ2 ɪI)
for l = 0, 1... do
Observe s0
for h = H - 1, ..., 1, 0 do
Sample aih ∈ argmaXa(Φh初h)(sih,α)
Act alh
Observe sl+1,rlh
end for
Observe rlH
for h = H - 1, ..., 1, 0 do
Generate regression problem A ∈ Rl×k , b ∈ Rl
Φh(s0h, a0h)
A —	.
.
Φh(sl-1,h, al-1,h)
rih + maxα(Φh+1θl,h+1)(si,h+1, α) for h < H- 1
rih + ri,h+1	for h = H - 1
Bayesian Linear Regression
θlh J σ12 ( -12 ATA + λI)-1 ATb
ς1,% J ( -12 AT A + λI ) - 1
Sample 仇h 〜N(θih, ∑ι,h)
end for
end for
bi	J-
12
Under review as a conference paper at ICLR 2020
Appendix 2:	Hyper-Parameters
Augmented Chain Experiments
Hyper-Parameter	Value
mini-batch size	32
experience replay buffer size	1000
experience replay buffer size for oracle baseline	1000000
target network update frequency	100
discount factor	0.99
learning starts	0
d - representation dimension	20
Tsampie - posterior sample frequency	10
J - number of models	5
NBLR - Bayesian linear regression transitions	1000/|a|
NSDP - likelihood matching transitions	30
n - chain states	10
k - self loop actions	4
ALE Experiments
Hyper-Parameter	Value
d - representation dimension	64
Tsample - posterior sample frequency	1000
J - number of models	5
NBLR - Bayesian linear regression transitions	1000000/|a|
NSDP - likelihood matching transitions	600
13