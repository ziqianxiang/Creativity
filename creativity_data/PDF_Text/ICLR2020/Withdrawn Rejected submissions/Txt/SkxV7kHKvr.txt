Under review as a conference paper at ICLR 2020
1
Twin Graph Convolutional Networks: GCN
with Dual Graph support for Semi-Supervised
Learning
Anonymous authors
Paper under double-blind review
Ab stract
Graph Neural Networks as a combination of Graph Signal Processing and
Deep Convolutional Networks shows great power in pattern recognition in non-
Euclidean domains. In this paper, we propose a new method to deploy two
pipelines based on the duality of a graph to improve accuracy. By exploring the
primal graph and its dual graph where nodes and edges can be treated as one an-
other, we have exploited the benefits of both vertex features and edge features.
As a result, we have arrived at a framework that has great potential in both semi-
supervised and unsupervised learning.
1	Introduction and Motivation
Convolutional Neural Networks (CNNs) (Lecun et al. (1998)) has been very successfully used for
automated feature extraction in Euclidean domains, especially for computer vision, such as 2D im-
age classification, object detection, etc. However, many real-life data has a non-Euclidean graph
structure in nature, from which we want to investigate the underlying relations among different
objects by utilizing the representation of nodes and edges. Recently, research on applying the gen-
eralization of Convolutional Neural Networks to the non-Euclidean domains has attracted growing
attention. As a result, a branch of research on Geometric Deep Learning (Bruna et al. (2013)) based
on that has been ignited. Previous works including ChebNet (Defferrard et al. (2016)) and GCN
(Kipf & Welling (2017)) have demonstrated strong results in solving problems in semi-supervised
learning where the labels of only a few objects are given, and we want to find out the labels of other
objects through their inner connections. Current methods generalizing convolution operations in-
clude both spatial and spectral domains (Bruna et al. (2013)). The spatial one deals with each node
directly in the vertex domain while the spectral one takes a further step in converting signals via
graph Fourier transform into the spectral domain. However, one critical weakness would be the fact
that the interchangeable and complementary nature between nodes and edges are generally ignored
in previous research. As a result, the duality of the graph is not fully utilized. If we treat those edges
in the original, or known as the primal graph, as the nodes in the new graph, and original nodes as
edges, we can arrive at a new graph that further exploits the benefits of edge features. In such a way,
we are able to get both the primal graph and the dual graph (Monti et al. (2018)). By combining
both the vertex features and the edge features, we will be able to solve wider range of problems
and achieve better performance. In this paper, we propose a new approach to transform the primal
graph into its dual form and have implemented two pipelines based on these two forms of graph to
improve the accuracy and the performance. With two pipelines, we also exploited a path to make
the model wider instead of merely deeper. Meanwhile, we have developed a new framework that
can be applied later on both semi-supervised learning and unsupervised learning.
1
Under review as a conference paper at ICLR 2020
2	Related Work
Graph-based semi-supervised learning aims to annotate data from a small amount of label data on
a graph. To learn the vectors that can recover the labels of the training data as well as distinguish data
with different labels, conventionally, graph Laplacian regularizer gives penalty between sampling
based on graph Laplacian matrix (Zhu et al. (2003); Ando & Zhang (2007); Weston et al. (2012)).
Sample-based method takes random walk to get samples from the context of data points in order to
propagate information (Perozzi et al. (2014); Yang et al. (2016); Grover & Leskovec (2016)).
Graph Convolutional Networks generalize the operation of convolution from grid data to graph
data (Wu et al. (2019)). After the emergence of the spectral-based convolutional networks on graph
(Bruna et al. (2013)), ChebNet (Defferrard et al. (2016)) approximate the filters by Chebyshev
polynomials according to the Laplacian eigendecomposition. GCN(Kipf & Welling (2017)) sim-
plifies ChebNet by introducing its first-order approximation and can be viewed as a spatial-based
perspective, which requires vertices in the graph to propagate their information to the neighbors.
MoNet(Monti et al. (2017)) is a spatial-based method, of which convolution is defined as a Gaussian
mixture of the candidates. GAT(Velickovic et al. (2017)) applies the attention mechanism to the
graph network. DGI(Velickovic et al. (2018)) proposes a framework to learn the unsupervised rep-
resentations on graph-structured data by maximizing location mutual information. We refer to Zhou
et al. (2018); Xu et al. (2018); Battaglia et al. (2018); Wu et al. (2019) as a more comprehensive and
thorough review on graph neural networks.
Dual approaches on graph networks usually unlike the above mono-methods, apply mixed meth-
ods to study graph networks. DGCN(Zhuang & Ma (2018)) makes a balance between the spatial-
based domain and spectral-based domain by regularizing their mutual information. GIN(Yu et al.
(2018)) proposes a dual-path from graph convolution on texts and another network on images to
gather cross-modal information into a common semantic space. DPGCNN(Monti et al. (2018)) ex-
tends the classification on vertices to edges by considering the attention mechanism on both. Our
study follows this path, which classifies vertices from the relationship between them (edges) and
regularization from the mutual information between classification on both vertices and edges.
3	Methodology
3.1	Preliminaries
Let G = {V, E,A} denote a graph, where V = {1, . . . , N} is the set of nodes with |V| = N, E is the
set of edges, andA = (A(i,j)∈V 6= 0) ∈ RN×N is the adjacency matrix. When G is undirected then
A is symmetric with Ai,j = Aj,i, G is an undirected graph, otherwise a directed graph. The Lapla-
cian matrix, also acts a propagation matrix, has the combinatorial form as L = D - A ∈ RN×N,
and its normalized form is L = I 一 D-2 AD-2, where D = diag(d(1),..., d(N)) ∈ RN×N is
the degree matrix of graph G with d(i) = Pj∈V Ai,j and I ∈ RN×N is the identity matrix. In some
literature, the random walk Laplacian Lrw = I 一 D-1A is employed to directed graph G.
3.2	Graph Convolutions and Convolutional Networks on Graphs
Let L = UΛUT be the eigendecomposition, where U ∈ RN×N is composed of orthonormal eigen-
basis and Λ = diag(λ0, λ1, . . . , λN-1) is a diagonal matrix of eigenvalues which denotes frequen-
cies of graph G , and λ% and Ui form an eigenpair. The convolutional operator *g on the graph signal
x is defined by
f = g *g X = U ((UTg) Θ (UTx)) = UGUTx,	(1)
where f = UTX and g = UTg are regarded as the graph Fourier transform of graph signal X and
graph filter g, respectively; f = U(∙) is the inverse graph Fourier transform, and Θ is the Hadamard
product. G = diag(go,…,gN-ι) behaves as spectral filter coefficients. Graph convolution can be
approximated by polynomial filters, the k-th order form is
kk
UGUTx ≈ X θiLix = U X θiΛi TTx,	(2)
i=0	i=0
2
Under review as a conference paper at ICLR 2020
where θi denotes coefficients and G ≈ Pi θiΛi or equivalently g(λj) ≈ Pi θiλj. Based on the
above approximation, ChebNet (Defferrard et al. (2016)) further introduces Chebyshev polynomials
into graph filters of the convolutional layers for the sake of computational efficiency. Chebyshev
polynomials are recursively expressed as Ti(x) = 2xTi-1 (x) - Ti-2(x) with T0(x) = 1 and
T1 (x) = x. The graph filter then becomes
kk
UGUTX = U (X θiTi (Λ) U UTX = X θiTi (L)x,	(3)
i=0	i=0
where L = 2∕λmαxL - I denotes the scaled normalized Laplacian for all eigenvalues λi ∈ [-1,1]
and θi is trainable parameter. Graph Convolutional Network (GCN) (Kipf & Welling (2017)) is a
variant of ChebNet which only takes first two terms of Equation (3). By setting the coefficients θ0
and θ1 as θ = θ0 = -θ1 and with λmax = 2, the convolution operator in convolution layer of GCN
is induced as g *g X = θ(I + D--1 AD--1 )x. With renormalization trick, I + D--1 AD-2 is further
1 1
replaced by D 1 AD 1 where A = A + I and D = D +1.
3.3	Graph and its dual
Figure 1: Primal graph and its dual graph: a) A graph G, which represents a primal graph, with
5 nodes; b) the corresponding dual graph G to graph G at its left side; arrow-arcs demonstrate the
conversion between G and G .
In graph theory, The definition of the dual varies according to the choice of embedding of the graph
G. For planar graphs generally, there may be multiple dual graphs, depending on the choice of planar
embedding of the graph. In this work, we follow the most common definition. Given a plane graph
G = {V, E A}, which is designated as the primal graph, the dual graph G = {V = E, E, A} is a
graph that has a vertex (or node) for each edge of G . The dual graph G has an edge whenever two
edges of G share at least one common vertex. To be clarified, the vertices (i, j) and (j, i) of dual
graph G converted from a undirected graph are regarded as the same. Fig.1 shows the conversion
from primal graph to its dual counterpart. When vertices of the primal graph embed features (or
signals in terminology of spectral graph theory), the features of a dual node can be obtained by ap-
plying a specified functions to its corresponding primal nodes’ features, i.e. the simplest applicable
function is to calculate the distance between the features of two nodes. In addition, if the edges of
primal graph possess features or attributes, we also take them into account as the their inherited fea-
tures of dual nodes. Take node (1, 2) of dual graph in Fig.1b) as an example, its feature is obtained
by performing the element-wise subtraction to the feature vectors of nodes 0 and 3 of primal graph
in Fig.1a), i.e. [1,1,0]T- [1,0,1]T= [0, -1, 1]T.
3.4	Twin Graph Convolutional Networks
The Twin Graph Convolutional Networks (TwinGCN) proposed in this work consists of two
pipelines. Both pipelines are built with the same architecture as GCN, and contain two convolu-
tion layers in each pipeline, as shown in Fig.2. The upper pipeline acts exactly as GCN; however,
the lower one takes the dual features X derived from primal features X as its inputs (as described
3
Under review as a conference paper at ICLR 2020
create dual labels
Figure 2: Architecture of TwinGCN: Two pipleline architecture, the upper is for primal graph, the
lower is for the dual graph. The predictions of the two pipelines are combined with their KL-
Divergence for final results.
in section 3.3), the predictions or outputs in dual vertex domain (i.e. edge domain in primal) is then
aggregated to primal vertex domain. The goal of introducing a dual pipeline into the model is that
we desire to utilize the predictions on the dual node (edges in primal graph) to affect the predictions
on primal nodes since the knowledge about those neighbors of a node can be propagated through
edges.
For the purpose of training the dual pipeline, we also need to get the labels of dual nodes. Let us
take an example, given a dual node (i, j) (corresponds to an edge in primal graph), primal node i
has label α and j has label β, then dual node (i, j) is assigned with a label (α, β).
One thing worth mentioned is that TwinGCN’s convolution layers are not limited to those used in
GCN, they can be replaced with other types of convolution layer, such as ChebNet, GWNN (Xu
et al. (2019)), etc.
The convolution layers in the pipelines perform graph convolution operations with shared weights
as learnable parameters, mathematically expressed as
H(I+1) = σ (D)- 1 AD-1 H(I)W(I))	(4)
where H(l) is the activation in l-th layer, W(l) is learnable weights in that layer. σ represents non-
linear activation function, e.g. ReLU. For the task of semi-supervised node classification, the loss
function is defined as
F
L=- X X Yl,f lnZl,f	(5)
l∈YL f=1
where YL is set of node labels for L ∈ V labeled node set, F denotes the number of labels of the
nodes, and Z is predicted outcome, a softmax of the output of the network.
In order to take effect of dual pipeline on the prediction of primal pipeline, we adopt KullbackLeibler
Divergence (DKL) as a regularization term in training. Suppose that P(Y |X) is predictions by
primal pipeline and P(Y |X) = P(Y |X) is the derived predictions obtained through an aggregation
from the predictions on dual labels by dual pipeline to primal label predictions. X is derived from
X as aforementioned (Section 3.3). We first calculate the joint probability matrix P(Y, Y ) of two
. . . .^ . .
matrices P (Y |X) and P (Y |X)
P(Y,Y) = X P(y∣χ)P(y|x) = P(Y|X)TP(Y|X)	(6)
x∈X
i` ,ι	, ,ι	∙ i	KK ∙ i ∙ , ∙	i` c∕rz^∖ F τ~∖，弋r、 c∙	c∕rz^ ~Cr∖ τr IlK TT ∙ι ι Trʌ ♦
we further get the marginal probabilities of P(Y) and P(Y) from P(Y, Y). KullbackLeibler Diver-
gence DKL is evaluated by
DKL (Yl∣Y) = - XP(y)iog (P^) = - XP(y)[ogP(y) - logP(y)i	(7)
y∈Y	y	y∈Y
4
Under review as a conference paper at ICLR 2020
finally, we attains the loss function as
L = θ1LP + θ2LD + θ3DKL,
where θ1 , θ2, and θ3 are trainable coefficients.
(8)
Figure 3: Fast algorithm for calculating the aggregated primal predictions from dual predictions
Fig.3 illustrates a fast algorithm deriving primal predictions from predictions of dual pipeline. It is
conducted by introducing two special incidence matrices. The matrix at the left hand side (N × M ,
N = |V | and M = |E |) is an incidence matrix in which the rows represent primal nodes, each
column depicts whether a primal node in a row has an incidence in the dual node represented by
this column. The rightmost matrix is the incidence matrix of primal labels presenting in dual labels
with dimension of L2 × L. Although these two matrices are extremely sparse when node number is
very large (we store them in compressed form), by taking advantage of GPU’s powerful computing
capability, the delicate sparse matrix multiplication subroutine, e.g. Nvidia’s cuSARSE, runs much
faster than codes with loops for lumping the incidences.
4	Experiments
In this section, we evaluate the performance of TwinGCN, we mainly focus on semi-supervised node
classification in current work. Actually, TwinGCN also support unsupervised learning by changing
the loss functions which we will fulfill in future work.
4.1	Datasets
We conduct experiments on three benchmark datasets and follow existing studies (Defferrard et al.
(2016); Kipf & Welling (2017); Xu et al. (2019) etc.) The datasets include Cora, Citeseer, and
Pubmed (Sen et al. (2008)). All these three datasets are collected from their corresponding citation
networks, the nodes represent documents and edges are the citations. Table 4.1 shows details of
these datasets. Label rate indicates the portion of the available labeled nodes used for training. The
training process takes 20 labeled samples for each class for every dataset.
Dataset	Nodes	Edges	Features	Label Rate	Classes	Train/Valid/Test Nodes
Cora	2,708	5,429	1,433	0.052	7	140 / 500 / 1,000
Citeseer	3,327	4,732	3,703	0.036	6	120 / 500 / 1,000
Pubmed	19,717	44,338	500	0.003	3	60/500/ 1,000
Table 1: The Statistics of Datasets
4.2	Baselines and Experiment Settings
Since both pipelines of our proposed architecture work with graph convolution based on spectral
graph theory, we use recent works, such as ChebNet (Defferrard et al. (2016)) GCN (Kipf & Welling
5
Under review as a conference paper at ICLR 2020
(2017)), and GWNN (Xu et al. (2019)), etc. These models maintain the same graph Lapalacian base
structure, unlike some other methods take partial graph structure, e.g. FastGCN (Chen et al. (2018))
applies Monte Carlo importance sampling on edges. however, this kind of method only guarantees
the convergence as the sample size goes to infinity.
For the sake of consistency for comparison, the hyper-parameters for training are kept the same for
primal pipeline as other models. The primal are composed with two graph convolution layers with
16 hidden units and applied with ReLU non-linear activations. Loss is evaluated with the softmax
function. Dropout (Srivastava et al. (2014)) of primal pipeline is set to p = 0.5 for the primal. We
use the Adam optimizer (Kingma & Ba (2015)) for optimizing the weights with an initial learning
rate lr = 0.01.
As the dual graph is normally much bigger than the counterpart primal graph, its adja-
cency/Laplacian matrix and the number of dual nodes becomes quadratically larger, e.g. N nodes
with N × (N - 1) edges in a fully-connected graph. Therefore, to avoid overfitting on dual pipeline,
we set its dropout rate higher than 70%. We also introduce a sampling rate to extract a small fraction
from the total dual node labels. Having a large number of edges in the primal graph also means a
large number of dual nodes. In such situation, the performance will be degraded severely.
4.3	Performance of TwinGCN
The quantitative comparison among different models is given in Table 4.4. For node classifica-
tion, TwinGCN achieves similar results or outperforms with some datasets. The performance gain
comes from the aggregation of knowledge propagated through edges (or dual nodes) trained by dual
pipeline. However, primal pipeline only will ignore the dependency between labels of nodes.
Method	Cora	Citeseer	Pubmed
ChebNet	81.2%	69.8%	74.4%
GCN	81.5%	70.3%	79.0%
GWNN	82.8%	71.7%	79.1%
DGI	82.3%	71.8%	76.8%
DGCN	83.5%	72.6%	80.0%
TwinGCN	82.7%	72.8%	79.8%
Table 2: Results of Node Classification
Fig.4a) illustrate that when compared to the GCN, TwinGCN bearing two pipelines converges slower
but achieves a higher accuracy as the number of epoch increases. This is because that we have two
pipelines through mutual interaction. In Fig.4b), we observe that two loss curves of traditional GCN
and TwinGCN have very similar decreasing trends. However, the loss curve of TwinGCN is slightly
above GCN because the loss of TwinGCN is the summation of both primal and dual pipelines.
4.4	Analysis
To test whether the introduced dual pipeline and regularization improve the basic GCN pipeline,
we conducted controlled experiments to make comparison among GCN, GCNs with pipelines on
original graph and dual graph and TwinGCN(GCNs with both pipelines and regularization by KL-
divergence).
Method	Cora	Citeseer	Pubmed
GCN	81.5% ± 0.3%	70.8% ± 0.1%	78.8% ± 0.1%
GCN(double-pipeline)	81.6% ± 0.4%	72.5% ± 1.0%	79.8% ± 2.3%
TwinGCN	83.0% ± 1.3%	72.5 ± 0.8%	79.5% ± 1.2%
Table 3: Comparison results
Table 3 shows the comparison results: the average test accuracy and the standard deviation from
the model which has the best validation accuracy. The pipeline on the dual graph increases the
6
Under review as a conference paper at ICLR 2020
0.50
ValIdatlon ACCUraCy CUrVe
0 5 0 5 0 5
8 7 7 6 6 5
■ ■■■■■
Oooooo
① 6ug-lod >UEDUU<
0	50	100	150	200	250	300	350	400
Epochs
---- Val Accuracy (TwinGCN)
Val Accuracy (GCN)
Loss Function Curve
1.6
S
Q 1"4
1.2
1.0
0	50	100	150	200	250	300	350	400
Epochs
Figure 4: a) Mean and standard deviation of validation accuracy on Cora; b) Loss function of vali-
dation
performance of GCN, indicating that applying relationship between nodes can be a powerful tool in
the classification task. The regularization(KL-divergence). However, TwinGCN suffers from larger
uncertainty suggested by the larger standard deviation.
5	Conclusion and Future work
In this work, we propose the TwinGCN with parallel pipelines working on both the primal graph
and its dual graph, respectively. TwinGCN achieves the state-of-the-art performance in semi-
supervised learning tasks. Moreover, TwinGCN’s ability is not limited to this, we can extend its
power/utilization into unsupervised learning by altering its loss functions.
6	Acknowledgments
Use unnumbered third level headings for the acknowledgments. All acknowledgments, including
those to funding agencies, go at the end of the paper.
7
Under review as a conference paper at ICLR 2020
References
Rie K Ando and Tong Zhang. Learning on graph with laplacian regularization. In Advances in
neural information processing systems, pp. 25-32, 2007.
Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi,
Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al.
Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261,
2018.
Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally
connected networks on graphs. arXiv preprint arXiv:1312.6203, 2013.
Jie Chen, Tengfei Ma, and Cao Xiao. FastGCN: Fast learning with graph convolutional networks
via importance sampling. In International Conference on Learning Representations, 2018. URL
https://openreview.net/forum?id=rytstxWAW.
Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on
graphs with fast localized spectral filtering. In Proceedings of the 30th International Conference
on Neural Information Processing Systems, NIPS’16, pp. 3844-3852, USA, 2016. Curran Asso-
ciates Inc. ISBN 978-1-5108-3881-9. URL http://dl.acm.org/citation.cfm?id=
3157382.3157527.
Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proceedings
of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining,
pp. 855-864. ACM, 2016.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In the 3rd
International Conference for Learning Representations, San Diego, 2015, 2015. URL http:
//arxiv.org/abs/1412.6980. cite arxiv:1412.6980Comment: Published as a conference
paper at.
Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. In International Conference on Learning Representations (ICLR), 2017.
Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recog-
nition. Proceedings of the IEEE, 86(11):2278-2324, Nov 1998. doi: 10.1109/5.726791.
Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodola, Jan Svoboda, and Michael M
Bronstein. Geometric deep learning on graphs and manifolds using mixture model cnns. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5115-
5124, 2017.
Federico Monti, Oleksandr Shchur, Aleksandar Bojchevski, Or Litany, Stephan Gunnemann,
and Michael M Bronstein. Dual-primal graph convolutional networks. arXiv preprint
arXiv:1806.00770, 2018.
Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social repre-
sentations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge
discovery and data mining, pp. 701-710. ACM, 2014.
Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-
Rad. Collective classification in network data. AI Magazine, 29(3):93, Sep. 2008. doi: 10.1609/
aimag.v29i3.2157. URL https://www.aaai.org/ojs/index.php/aimagazine/
article/view/2157.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-
nov. Dropout: A simple way to prevent neural networks from overfitting. Journal of Ma-
chine Learning Research, 15:1929-1958, 2014. URL http://jmlr.org/papers/v15/
srivastava14a.html.
Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.
8
Under review as a conference paper at ICLR 2020
Petar VeliCkovic, William Fedus, William L Hamilton, Pietro Lio, Yoshua Bengio, and R Devon
Hjelm. Deep graph infomax. arXiv preprint arXiv:1809.10341, 2018.
Jason Weston, Frederic Ratle, Hossein Mobahi, and Ronan Collobert. Deep learning via semi-
supervised embedding. In Neural Networks: Tricks ofthe Trade, pp. 639-655. Springer, 2012.
Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S Yu. A
comprehensive survey on graph neural networks. arXiv preprint arXiv:1901.00596, 2019.
Bingbing Xu, Huawei Shen, Qi Cao, Yunqi Qiu, and Xueqi Cheng. Graph wavelet neural network. In
International Conference on Learning Representations, 2019. URL https://openreview.
net/forum?id=H1ewdiR5tQ.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? arXiv preprint arXiv:1810.00826, 2018.
Zhilin Yang, William W Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning
with graph embeddings. arXiv preprint arXiv:1603.08861, 2016.
Jing Yu, Yuhang Lu, Zengchang Qin, Weifeng Zhang, Yanbing Liu, Jianlong Tan, and Li Guo.
Modeling text with graph convolutional network for cross-modal information retrieval. In Pacific
Rim Conference on Multimedia, pp. 223-234. Springer, 2018.
Jie Zhou, Ganqu Cui, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, and Maosong Sun. Graph neural
networks: A review of methods and applications. arXiv preprint arXiv:1812.08434, 2018.
Xiaojin Zhu, Zoubin Ghahramani, and John D Lafferty. Semi-supervised learning using gaussian
fields and harmonic functions. In Proceedings of the 20th International conference on Machine
learning (ICML-03), pp. 912-919, 2003.
Chenyi Zhuang and Qiang Ma. Dual graph convolutional networks for graph-based semi-supervised
classification. In Proceedings of the 2018 World Wide Web Conference, pp. 499-508. International
World Wide Web Conferences Steering Committee, 2018.
A Appendix
A. 1 Spasity
TwinGCN, which introduces additional pipeline, increases the number of parameters as well as
sparsity. For the graph G with m nodes and n edges, the sparsity of the Laplacian matrix is O(m).
If we denote ni as the degree of node i, for the dual graph which takes edges as nodes and connect
edges which share the same node, the sparsity of the dual Laplacian matrix is
G	*十小	C(Pmm=I ni(ni -1)∕2∖
Sparsity(G) = O∣ ———n-------------- I
which can be as small as O(ml) when each node has the average number of degree, i.e. n = 2^.
9