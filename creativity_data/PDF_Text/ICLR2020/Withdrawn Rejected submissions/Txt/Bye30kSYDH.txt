Under review as a conference paper at ICLR 2020
PolyGAN: High-Order Polynomial Generators
Anonymous authors
Paper under double-blind review
Ab stract
Generative Adversarial Networks (GANs) have become the gold standard when
it comes to learning generative models for high-dimensional distributions. Since
their advent, numerous variations of GANs have been introduced in the literature,
primarily focusing on utilization of novel loss functions, optimization/regularization
strategies and network architectures. In this paper, we turn our attention to the
generator and investigate the use of high-order polynomials as an alternative class
of universal function approximators. Concretely, we propose PolyGAN, where we
model the data generator by means of a high-order polynomial whose unknown
parameters are naturally represented by high-order tensors. We introduce two
tensor decompositions that significantly reduce the number of parameters and show
how they can be efficiently implemented by hierarchical neural networks that only
employ linear/convolutional blocks. We exhibit for the first time that by using our
approach a GAN generator can approximate the data distribution without using any
activation functions. Thorough experimental evaluation on both synthetic and real
data (images and 3D point clouds) demonstrates the merits of PolyGAN against
the state of the art.
1	Introduction
Generative Adversarial Networks (GANs) are currently one of the most popular lines of research in
machine learning. Research on GANs mainly revolves around: (a) how to achieve faster and/or more
accurate convergence (e.g., by studying different loss functions (Nowozin et al., 2016; Arjovsky &
Bottou, 2017; Mao et al., 2017) or regularization schemes (Odena et al., 2018; Miyato et al., 2018;
Gulrajani et al., 2017)), and (b) how to design different hierarchical neural networks architectures
composed of linear and non-linear operators that can effectively model high-dimensional distributions
(e.g., by progressively training large networks (Karras et al., 2018) or by utilizing deep ResNet type
of networks as generators (Brock et al., 2019)).
Even though hierarchical deep networks are efficient universal approximators for the class of con-
tinuous compositional functions (Mhaskar et al., 2016), the non-linear activation functions pose
difficulties in their theoretical analysis, understanding, and interpretation. For instance, as illustrated
in Arora et al. (2019), element-wise non-linearities pose a challenge on proving convergence, espe-
cially in an adversarial learning setting (Ji & Liang, 2018). Consequently, several methods, e.g., Saxe
et al. (2014); Hardt & Ma (2017); Laurent & Brecht (2018); Lampinen & Ganguli (2019), focus only
on linear models (with respect to the weights) in order to be able to rigorously analyze the neural
network dynamics, the residual design principle, local extrema and generalization error, respectively.
Moreover, as stated in the recent in-depth comparison of many different GAN training schemes
(Lucic et al., 2018), the improvements may mainly arise from a higher computational budget and
tuning and not from fundamental architectural choices.
In this paper, we depart from the choice of hierarchical neural networks that involve activation
functions and investigate for the first time in the literature of GANs the use of high-order polynomials
as an alternative class of universal function approximators for data generator functions. This choice
is motivated by the strong evidence provided by the Stone-Weierstrass theorem (Stone, 1948), which
states that every continuous function defined on a closed interval can be uniformly approximated as
closely as desired by a polynomial function. Hence, we propose to model the vector-valued generator
function Gpzq : Rd → Ro by a high-order multivariate polynomial of the latent vector z, whose
unknown parameters are naturally represented by high-order tensors.
1
Under review as a conference paper at ICLR 2020
However, the number of parameters required to accommodate all higher-order correlations of the
latent vector explodes with the desired order of the polynomial and the dimension of the latent vector.
To alleviate this issue and at the same time capture interactions of parameters across different orders
of approximation in a hierarchical manner, we cast polynomial parameters estimation as a coupled
tensor factorization (Papalexakis et al., 2016; Sidiropoulos et al., 2017) that jointly factorizes all the
polynomial parameters tensors. To this end, we introduce two specifically tailored coupled canonical
polyadic (CP)-type of decompositions with shared factors. The proposed coupled decompositions of
the parameters tensors result into two different hierarchical structures (i.e., architectures of neural
network decoders) that do not involve any activation function, providing an intuitive way of generating
samples with an increasing level of detail. This is pictorially shown in Figure 1. The result of the
proposed PolyGAN using a fourth-order polynomial approximator is shown in Figure 1 (a), while
Figure 1 (b) shows the corresponding generation when removing the fourth-order power from the
generator.
Our contributions are summarized as follows:
•	We model the data generator with a high-order polynomial. Core to our approach is to
cast polynomial parameters estimation as a coupled tensor factorization with shared factors.
To this end, we develop two coupled tensor decompositions and demonstrate how those
two derivations result in different neural network architectures involving only linear (e.g.,
convolution) units. This approach reveals links between high-order polynomials, coupled
tensor decompositions and network architectures.
•	We experimentally verify that the resulting networks can learn to approximate functions
with analytic expressions.
•	We show how the proposed networks can be used with linear blocks, i.e., without utilizing
activation functions, to synthesize high-order intricate signals, such as images.
•	We demonstrate that by incorporating activation functions to the derived polynomial-based
architectures, PolyGAN improves upon three different GAN architectures, namely DC-
GAN (Radford et al., 2015), SNGAN (Miyato et al., 2018) and SAGAN (Zhang et al.,
2019).
(a)
Figure 1: Generated samples by an instance of the proposed PolyGAN. (a) Generated samples using
a fourth-order polynomial and (b) the corresponding generated samples when removing the terms
that correspond to the fourth-order. As evidenced, by extending the polynomial terms, PolyGAN
generates samples with an increasing level of detail.
(b)
2	Method
In this Section, we investigate the use of a polynomial expansion as a function approximator for the
data generator in the context of GANs. To begin with, we introduce the notation in Section 2.1. In
Section 2.2, we introduce two different polynomials models along with specifically tailored coupled
tensor factorizations for the efficient estimation of their parameters.
2.1	Preliminaries and notation
Matrices (vectors) are denoted by uppercase (lowercase) boldface letters e.g., X, (x). Tensors are
denoted by calligraphic letters, e.g., X. The order of a tensor is the number of indices needed to
address its elements. Consequently, each element of an M th-order tensor X is addressed by M
indices, i.e., pXqi1,i2,...,iM
xi1,i2,...,iM.
2
Under review as a conference paper at ICLR 2020
The mode-m unfolding of a tensor X P RI1 *I2 X …XIM maps X to a matrix Xpmq P RIm XIm with
Im “ ∏M⅛=1 Ik such that the tensor element Xi1,i2,…,iM is mapped to the matrix element Ximj
k‰m
where j “ 1 ' ∑M=ι Pik — 1) Jk with Jk “ nk´l In. The mode-m vector product of X with a
k‰m	n‰m
vector u P RIm, denoted by X Xn U P RI1 χI2χ…XInT χIn'1 X…XIN, results in a tensor of order
M—1:
Im
m	i1 ,...,im´1 ,im`1 ,...,iM	i1 ,i2 ,...,iM im .
im“1
Furthermore, We denote X X1 up1q ^2 up2q ^3 …XM UpMq “ X ∏m=ι ^mupmq
(1)
The Khatri-Rao product (i.e., column-wise Kronecker product) of matrices A P RIXN and B P
RJ XN is denoted by A d B and yields a matrix of dimensions (IJ) X N. The Hadamard product of
A P RIXN and B P RI XN is defined as A * B and is equal to ApijqBpijq for the (i,j) element.
The CP decomposition (Kolda & Bader, 2009; Sidiropoulos et al., 2017) factorizes a tensor into
a sum of component rank-one tensors. An Mth-order tensor X P RI1χI2χ…XIM has rank-1,
when it is decomposed as the outer product of M vectors tUpmq P RIm umM“1. That is, X “
up1q 0 up2q o∙∙∙o UpMq “ OM=Ixpmq, where 0 denotes for the vector outer product. Consequently,
the rank-R CP decomposition of an M th-order tensor X is written as:
R
X “ rUrιs, U[2s,…,UrMssl “ X UrIq ° ur2q。…0 UrM q,	⑵
r“1
where the factor matrices { Urms = [U^mq, Um^, ∙∙∙ , URmqs P RImxR( =〔 collect the vectors from
the rank-one components. By considering the mode-1 unfolding of X, the CP decomposition can be
written in matrix form as (Kolda & Bader, 2009):
d UrM -is。…。
Ur/'
“ Uris ^ Θ Urms)
m“M
(3)
More details on tensors and multilinear operators can be found in Kolda & Bader (2009); Sidiropoulos
et al. (2017).
2.2	High-order polynomial generators
GANs typically consist of two deep networks, namely a generator G and a discriminator D. G is
a decoder (i.e., a function approximator of the sampler of the target distribution) which receives as
input a random noise vector z P Rd and outputs a sample x “ G(z) P Ro. D receives as input both
G(z) and real samples and tries to differentiate the fake and the real samples. During training, both
G and D compete against each other till they reach an “equilibrium” (Goodfellow et al., 2014). In
practice, both the generator and the discriminator are modeled as deep neural networks, involving
composition of linear and non-linear operators (Radford et al., 2015).
Table 1: Nomenclature
Symbol	DimenSiOn(S)	Definition
n, N	N	Polynomial term order, total approximation order.
k	N	Rank of the decompositions.
Z	Rd	Input to the polynomial approximator, i.e., generator.
C, β	RoXk, Ro	Parameters in both decompositions.
Arns, Srns, Brns	Rd^k Rk ^ k Rω^k	Matrix parameters in the hierarchical decomposition.
d, *	-	Khatri-Rao product, Hadamard product.
In this paper, we focus on the generator. Instead of modeling the generator as a composition of linear
and non-linear functions, we assume that each generated pixel xi “ (G(z))i may be expanded as a
Nth order polynomial1 in z . That is,
1With an Nth order polynomial we can approximate any smooth function (Stone, 1948).
3
Under review as a conference paper at ICLR 2020
TN
Xi = (G(z))i = βi ' wi ] Z ' ZT Wp ɔz ' W i ɔ ^1 Z ^2 Z ^3 Z + ∙∙∙ + W ” ^n ^n z, (4)
n“1
where the scalar βi, and the set of tensors { Wrns P Rnm=I Xmd(N“、are the parameters of the
polynomial expansion associated to each output of the generator, e.g., pixel. Clearly, when n = 1,
r2s
the weights are d-dimensional vectors; when n = 2, the weights, i.e., 叱 ,form a d X d matrix. For
higher orders of approximation, i.e., when n23, the weights are nth order tensors.
By stacking the parameters for all pixels, we define the parameters β =. rβ1, β2, . . . , βosT P Ro and
{ Wrns p RoXnm=I Xmd(N“1. Consequently, the vector-valued generator function is expressed as:
N	n`1
G(Z)= ∑ Wrns ∏ ^jZ	+ β
n“1	j“2
(5)
Intuitively, (5) is an expansion which allows the Nth order interactions between the elements of
the noise latent vector Z. Furthermore, (5) resembles the functional form of a truncated Maclaurin
expansion of vector-valued functions. In the case of a Maclaurin expansion, Wrns represent the nth
order partial derivatives of a known function. However, in our case the generator function is unknown
and hence all the parameters need to be estimated from training samples.
The number of the unknown parameters in (5) is (dN'1 — 1) d´´ɪ, which grows exponentially with
the order of the approximation. Consequently, the model of (5) is prone to overfitting and its training
is computationally demanding.
A natural approach to reduce the number of parameters is to assume that the weights exhibit re-
dundancy and hence the parameter tensors are of low-rank. To this end, several low-rank tensor
decompositions can be employed (Kolda & Bader, 2009; Sidiropoulos et al., 2017). For instance, let
the parameter tensors Wrns admit a CP decompostion (Kolda & Bader, 2009) of mutilinear rank-k,
namely, {Wrns =『。皿,1, U[n],2,…，U[nS,pn'1qSIUN=1, With U[n],1 P RoXk, and U[n],m P Rdxk,
for m = 2, . . . , n + 1. Then, (5) is expressed as
N	n`1
G(Z) =	rrU[ns,1,UrnS,2, ..., U[nS,(n'1)s] ∏ XjZ)
n“1	j“2
+ β,
(6)
which has significantly less parameters than (5), especially when k ! d. However, a set of different
factor matrices for each level of approximation are required in equation 6, and hence the hierarchical
nature of images is not taken into account. To promote compositional structures and capture inter-
actions among parameters in different orders of approximation we introduce next two coupled CP
decompositions with shared factors.
Model 1: Coupled CP decomposition:
Instead of factorizing each parameters tensor individually we propose to jointly factorize all the
parameter tensors using a coupled CP decomposition with a specific pattern of factor sharing. To
illustrate the factorization, we assume a third order approximation (N = 3), however in the appendix
a generalization to N-th order approximation is provided. Let us assume that the parameters tensors
admit the following coupled CP decomposition with the factors corresponding to lower-order levels
of approximation being shared across all parameters tensors. That is:
•	Let W [1s = CU[T1s, be the parameters for first level of approximation.
•	Let assume W[2s being a superposition of of two weights tensors, namely W[2s = W [12:2s +
W[12:3s , with Wi[:2js denoting parameters associated with the second order interactions across
the i-th and j-th order of approximation. By enforcing the CP decomposition of the above
tensors to share the factor with tensors corresponding to lower-order of approximation we
obtain in matrix form: Wp[12qs = C(U[3s d U[1s)T + C(U[2s d U[1s )T.
4
Under review as a conference paper at ICLR 2020
Figure 2: Schematic illustration of the Coupled CP decomposition (for third order approximation).
Symbol * refers to the Hadamard product.
•	Similarly, we enforce the third-order parameters tensor to admit the following CP decompo-
sition (in matrix form) Wpr13qs “ CpUr3s d Ur2s d Ur1sqT. Note that all but the Ur3s factor
matrices are shared in the factorization of tensors capturing polynomial parameters for the
first and second order of approximation.
The parameters are C P RoXk, Urms P RdXk for m = 1, 2,3. Then, (6) for N = 3 is written as:
G(Z) = β ` CUrTsz ` c(U[3s d Ur1s)τPz dz) + C(。⑵ d Ur1s)TPz d z)+
c(U[3s d U[2s d UrIs) (Z d Z d Z)
The third order approximation of (7) can be implemented as a neural network with the structure of
Figure 2 (proved in section B, Claim 1 of the appendix). It is worth noting that the structure of the
proposed network allows for incremental network growth.
Model 2: Coupled nested CP decomposition: Instead of explicitly separating the interactions
between layers, we can utilize a joint hierarchical decomposition on the polynomial parameters. Let
US first introduce learnable hyper-parameters {外沟 P Rω (n“1, which act as scaling factors for each
parameter tensor. Therefore, we modify (5) to:
G(z) =
N	n`2
Σ	W rns ^2 b[n]∏ Xj Z
n“1	j“3
+ β,
(8)
with { Wrns P Roxsxnm=1 xmd(N“r For illustration purposes, We consider a third order function
approximation (N = 3). That is,
G(z) = β + Wr1s ^2 b[1s ^3 Z + Wr2s ^2 b[2s ^3 Z ^4 Z + W⑶ ^2 b[3s ^3 Z ^4 Z ^5 Z (9)
To estimate its parameters we jointly factorize all parameters tensors by employing nested CP
detecomposion with parameter sharing as follows (in matrix form)
•	First order parameters : Wpr11qs = C(Ar3s d Br3s)T .
•	Second order parametes: Wpr12qs = C Ar3s d
•	Third order parameters: Wpr13qs = C Ar3s d
with C P RoXk, Arns P RdXk, Srns P RkXk, Brns P Rsxk for n = 1,...,N. Altogether, (9) is
written as:
5
Under review as a conference paper at ICLR 2020
T
G(z) = β ' C(A[3s d B[3s
qTPzdb[3]q ` ClAr3] d	(AM d8[2])电3] }(
z d z d br2s `
T
C
[Ar3]d	(A[2]d{(ArιsdBrIs)Sr2]})s⅛ } ´
z d z d z d br1s
(10)
As we prove in the appendix (section B, Claim 3), (10) can be implemented in a hierarchical manner
with a three-layer neural network as shown in Figure 3.
Comparison between the two models: Both models are based on the polynomial expansion, how-
ever there are few differences between those. The Coupled CP decomposition has a simpler expression,
however the Coupled nested CP decomposition relates to standard architectures using hierarchical
composition that has recently yielded promising results in GANs (see Section 3). In the remainder
of the paper, we use the Coupled nested CP decomposition by default; in Section G, we include an
experimental comparison of the two models. The experimental comparison demonstrates that neither
model outperforms the other in all datasets; they perform similarly.
Figure 3: Schematic illustration of the Coupled nested CP decomposition (for third order approxima-
tion). Symbol * refers to the Hadamard product.
3	Related work
The literature on GANs is vast; we focus only on the works most closely related to ours. The
interested reader can find further information in a recent survey (Creswell et al., 2018).
Berthelot et al. (2017) use skip connections to concatenate the noise z in deeper layers in the
generator. The recent BigGAN (Brock et al., 2019) performs a hierarchical composition through skip
connections from the noise z to multiple resolutions of the generator. In their implementation, they
split z into one chunk per resolution and concatenate each chunk (of z) to the respective resolution.
Despite the propagation of the noise z to successive layers, the aforementioned works have substantial
differences from ours. We introduce a well-motivated and mathematically elaborate method to achieve
a more precise approximation with a polynomial expansion. In contrast to the previously mentioned
works, we also do not concatenate the noise with the feature representations, but rather perform
multiplication of the noise with the feature representations, which we mathematically justify.
The work that is most closely related to ours is the recently proposed StyleGAN (Karras et al., 2019),
which is an improvement over the Progressive Growing of GANs (ProGAN) (Karras et al., 2018). As
ProGAN, StyleGAN is a highly-engineered network that achieves compelling results on synthesized
2D images. In order to provide an explanation on the improvements of StyleGAN over ProGAN, the
authors adopt arguments from the style transfer literature (Huang & Belongie, 2017). Nevertheless,
the idea of style transfer proposes to use features from images for conditional image translation, which
is very different to unsupervised samples (image) generation. We believe that these improvements
6
Under review as a conference paper at ICLR 2020
can be better explained under the light of our proposed polynomial function approximation. That is,
as we show in Figure 1, the Hadamard products build a hierachical decomposition with increasing
level of detail (rather than different styles). In addition, the improvements in StyleGAN (Karras et al.,
2019) are demonstrated by using a well-tuned model. In this paper we showcase that without any
complicated engineering process the polynomial generation can be applied into several architectures
(or any other type of decoders) and consistently improves the performance.
4	Experiments
A sequence of experiments in both synthetic data (2D and 3D data manifolds) and higher-dimensional
signals are conducted to assess the empirical performance of the proposed polynomial expansion.
The first experiments are conducted on a 2D manifolds that are analytically known (Section 4.1).
Further experiments on three 3D manifolds are deferred to the appendix (Section D). In Section 4.2,
the polynomial expansion is used for synthesizing digits. Experiments on images beyond digits
are conducted in Section E; more specifically, we experiment with images of faces and natural
scenes. The experiments with such images demonstrate how polynomial expansion can be used for
learning highly complex distributions by using a single activation function in the generator. Lastly,
we augment our polynomial-based generator with non-linearities and show that this generator is at
least as powerful as contemporary architectures.
Apart from the polynomial-based generators, we implemented two variations that are considered
baselines: (a) ‘Concat’: we replace the Hadamard operator with concatenation (used frequently in
recent methods, such as in Brock et al. (2019)), (b) ‘Orig’: the Hadamard products are ditched, while
use bp〕D z, i.e., there is a composition of linear layers that transform the noise z.
4.1	Synthetic experiment on 2D manifold
Sinusoidal: We assess the polynomial-based generator on a sinusoidal function in the bounded
domain r0, 2πs. Only linear blocks, i.e., no activation functions, are used in the generator. That
is, all the element-wise non-linearities (such as ReLU’s, tanh) are ditched. The distribution we
want to match is a sinx signal. The input to the generator is z P R and the output is rx, sin xs
with x P r0, 2πs. We assume a 12th order approximation where each Sris , Aris is a fully-connected
layer and Bris is an identity matrix. Each fully-connected layer has width 15. In Figure 4, 2, 000
random samples are synthesized. We indeed verify that in low-dimensional distributions, such as the
univariate sinusoidal, PolyGAN indeed approximates the data distribution quite accurately without
using any non-linear activation functions.
(a) GT
(b) Orig
(c) Concat
(d) PolyGAN
Figure 4: Synthesized data for learning the rx, sin xs signal. No activation functions are used in
the generators. From left to right: (a) the data distribution, (b) ‘Orig’, (c) ‘Concat’, (d) PolyGAN.
Notably, neither ‘Orig’ nor ‘Concat’ can learn to approximate different Taylor terms.
4.2	Digit generation with linear blocks
The linear generator of the previous section is extended to greyscale images, in which an analytic
expression of the ground-truth distribution remains elusive. To our knowledge, there has not been a
generation of greyscale images based on polynomial expansion in the past.
We capitalize on the expressivity of the recent resnet-based generator (Miyato et al., 2018; Brock et al.,
2019), to devise a new polynomial generator G(Z) : R128 → R32x32. We consider a fourth-order
7
Under review as a conference paper at ICLR 2020
乙
6
7。^>3-'6f-∙7c-3
q OSa
5
a
rι
3
q /
∖ 5
4 O
7 2
3 5
(S)0
7 H
I 7
g H
Ygr^7∖qs3 7'
K </
7 + H
q o ι
I Z I
工Iq
/ q r
√ 3 o
2 f 7
5 G /
干CQ
7crq〃 7。N ∙2o
0
r)
ð
?
7
O
9 S

(a) GT	(b) Orig	(c) Concat	(d) PolyGAN
Figure 5:	Synthesized data for MNIST with a single activation in the generator. From left to right: (a)
The ground-truth signals, (b) ‘Orig’, (c) ‘Concat’, (d) PolyGAN.
approximation (as derived in (5)) where Bris is the identity matrix, Sris is a residual block with
two convolutions for i “ 1, . . . , 4. We emphasize that the residual block as well as all layers are
linear, i.e., there are no activation functions. We only add a tanh in the output of the generator
for normalization purposes. The discriminator and the optimization procedure are the same as in
SNGAN; the only difference is that we run one discriminator step per generator step (ndis “ 1). Note
that the ‘Orig’ resnet-based generator resembles the generator of Miyato et al. (2018) in this case.
We perform digit generation (trained on MNIST (LeCun et al., 1998)). In Figure 5, random samples
are visualized for the three compared methods. Note that the two baselines have practically collapsed
into a single number each, whereas PolyGAN does synthesize plausible digits.
To further assist the generation process, we utilize the labels and train a conditional GAN. That is,
the class labels are used for conditional batch normalization. As illustrated in Figure 6, the samples
synthesized are improved over the unsupervised setting. ‘Orig’ and ‘Concat’ still suffer from severe
mode collapse, while PolyGAN synthesizes digits that have different thickness (e.g. 9), style (e.g. 2)
and rotation (e.g. 1).
(a) GT
OOOQOOOOOO
/ / J 1 ∖ 1 Il ∖ /
乙222222。
5333233353
q”qqurqu
<5Γ5^5Γ5S^
Q6S4GSG64C
7 7 T 7 7 7 ɔ 7 7 f
夕gggxggxgz
AirqqySq夕
(b) Orig
OOo 4。。©。。。
(I I [ t / / / I /
』工NaANN2? a
33J33333S3
444年V,夕449
s r ≤^ S 夕夕 SSSS
6 bt>C644N66
7777717l,77
8ggsgysg+8
qq9,49999夕
(c) Concat	(d) PolyGAN
Figure 6:	Conditional digit generation. Note that both ‘Orig’ and ‘Concat’ suffer from severe mode
collapse (details in section 4.2). On the contrary, PolyGAN synthesizes digits that have different
thickness (e.g. 9), style (e.g. 2) and rotation (e.g. 1).
5 Conclusion
We express data generation as a polynomial expansion task. We model the high-order polynomi-
als with tensorial factors. We introduce two tailored coupled decompositions and show how the
polynomial parameters can be implemented by hierarchical neural networks, e.g. as generators
in a GAN setting. We exhibit how such polynomial-based generators can be used to synthesize
images by utilizing only linear blocks. In addition, we empirically demonstrate that our polynomial
expansion can be used with non-linear activation functions to improve the performance of standard
state-of-the-art architectures. Finally, it is worth mentioning that our approach reveals links between
high-order polynomials, coupled tensor decompositions and network architectures.
8
Under review as a conference paper at ICLR 2020
References
Martin Arjovsky and Leon Bottou. Towards principled methods for training generative adversarial
networks. In International Conference on Learning Representations (ICLR), 2017.
Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient
descent for deep linear neural networks. In International Conference on Learning Representations
(ICLR), 2019.
David Berthelot, Thomas Schumm, and Luke Metz. Began: Boundary equilibrium generative
adversarial networks. arXiv preprint arXiv:1703.10717, 2017.
Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural
image synthesis. In International Conference on Learning Representations (ICLR), 2019.
Antonia Creswell, Tom White, Vincent Dumoulin, Kai Arulkumaran, Biswa Sengupta, and Anil A
Bharath. Generative adversarial networks: An overview. IEEE Signal Processing Magazine, 35(1):
53-65, 2018.
Athinodoros S Georghiades, Peter N Belhumeur, and David J Kriegman. From few to many:
Illumination cone models for face recognition under variable lighting and pose. IEEE Transactions
on Pattern Analysis and Machine Intelligence (T-PAMI), (6):643-660, 2001.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural informa-
tion processing systems (NIPS), 2014.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville.
Improved training of wasserstein gans. In Advances in neural information processing systems
(NIPS), pp. 5767-5777, 2017.
Moritz Hardt and Tengyu Ma. Identity matters in deep learning. In International Conference on
Learning Representations (ICLR), 2017.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans
trained by a two time-scale update rule converge to a local nash equilibrium. In Advances in neural
information processing systems (NIPS), pp. 6626-6637, 2017.
Emiel Hoogeboom, Rianne van den Berg, and Max Welling. Emerging convolutions for generative
normalizing flows. In International Conference on Machine Learning (ICML), 2019.
Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance nor-
malization. In IEEE Proceedings of International Conference on Computer Vision (ICCV), pp.
1501-1510, 2017.
Kaiyi Ji and Yingbin Liang. Minimax estimation of neural net distance. In Advances in neural
information processing systems (NIPS), pp. 3845-3854, 2018.
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for
improved quality, stability, and variation. In International Conference on Learning Representations
(ICLR), 2018.
Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative
adversarial networks. In IEEE Proceedings of International Conference on Computer Vision and
Pattern Recognition (CVPR), 2019.
Tamara G Kolda and Brett W Bader. Tensor decompositions and applications. SIAM review, 51(3):
455-500, 2009.
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. The cifar-10 dataset. online: http://www. cs.
toronto. edu/kriz/cifar. html, 55, 2014.
Andrew K Lampinen and Surya Ganguli. An analytic theory of generalization dynamics and transfer
learning in deep linear networks. In International Conference on Learning Representations (ICLR),
2019.
9
Under review as a conference paper at ICLR 2020
Thomas Laurent and James Brecht. Deep linear networks with arbitrary loss: All local minima are
global. In International Conference on Machine Learning (ICML), 2018.
Yann LeCun, Leon Bottou, Yoshua Bengio, Patrick Haffner, et al. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324,1998.
Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, and Olivier Bousquet. Are gans created
equal? a large-scale study. In Advances in neural information processing systems (NIPS), pp.
700-709, 2018.
Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley. Least
squares generative adversarial networks. In IEEE Proceedings of International Conference on
Computer Vision (ICCV), pp. 2813-2821. IEEE, 2017.
Hrushikesh Mhaskar, Qianli Liao, and Tomaso Poggio. Learning functions: when is deep better than
shallow. arXiv preprint arXiv:1603.00988, 2016.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for
generative adversarial networks. In International Conference on Learning Representations (ICLR),
2018.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers
using variational divergence minimization. In Advances in neural information processing systems
(NIPS), pp. 271-279, 2016.
Augustus Odena, Jacob Buckman, Catherine Olsson, Tom B Brown, Christopher Olah, Colin
Raffel, and Ian Goodfellow. Is generator conditioning causally related to gan performance? In
International Conference on Machine Learning (ICML), 2018.
Evangelos E Papalexakis, Tom M Mitchell, Nicholas D Sidiropoulos, Christos Faloutsos, Partha Pra-
tim Talukdar, and Brian Murphy. Turbo-smt: Parallel coupled sparse matrix-tensor factorizations
and applications. Statistical Analysis and Data Mining: The ASA Data Science Journal, 9(4):
269-290, 2016.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition
challenge. International Journal of Computer Vision (IJCV), 115(3):211-252, 2015.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans. In Advances in neural information processing systems
(NIPS), pp. 2234-2242, 2016.
Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dy-
namics of learning in deep linear neural networks. In International Conference on Learning
Representations (ICLR), 2014.
N. D. Sidiropoulos, L. De Lathauwer, X. Fu, K. Huang, E. E. Papalexakis, and C. Faloutsos.
Tensor decomposition for signal processing and machine learning. IEEE Transactions on Signal
Processing, 65(13):3551-3582, 2017.
Nicholas D Sidiropoulos, Lieven De Lathauwer, Xiao Fu, Kejun Huang, Evangelos E Papalexakis,
and Christos Faloutsos. Tensor decomposition for signal processing and machine learning. IEEE
Transactions on Signal Processing, 65(13):3551-3582, 2017.
Marshall H Stone. The generalized weierstrass approximation theorem. Mathematics Magazine, 21
(5):237-254, 1948.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru
Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In IEEE
Proceedings of International Conference on Computer Vision and Pattern Recognition (CVPR), pp.
1-9, 2015.
10
Under review as a conference paper at ICLR 2020
Lucas Theis, Aaron van den Oord, and Matthias Bethge. A note on the evaluation of generative
models. In International Conference on Learning Representations (ICLR), 2016.
Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In
IEEE Proceedings of International Conference on Computer Vision and Pattern Recognition
(CVPR),pp. 7794-7803, 2018.
Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena. Self-attention generative
adversarial networks. In International Conference on Machine Learning (ICML), 2019.
11
7
review as a COnferenCe PaPer at ICLR 2020
AIgO=-thm 1: PolyGAN (modell∙
InPUt :NOiSe ZmMNmN
OUtPUt : a m 定。
% global rransformarn(s) Of N,
VU Linear(Z)
%rsrIayeE
K= (UE)Te
r HH2..N do
% Perfonn rhe Hadamard PrOdUCr for rheh
IayeE
省 U 省十 ((U m-)τ) ⅛ K
8 end
9u 4 + CK.
AIgO=-thm 2: PolyGAN (model 2∙
InPUt :NOiSe ZmMNmN
OUtPUt : a m 定。
ι % global rransformarn(s) Of N,
2	∙υ H Linear(Z)
3	%rsrIayeE
4	K,U ((BE)TbE) * ((AE)
5r TIU2: N do
6 % Mulriply Wirh rhe CUrrenrIayer Weighr Sln」
and Perfonn rhe Hadamard PrOdUCL
KU (S目者 + (Bg)T) ⅛ ((4z)τ)
7 end
8u /3 + CF
Table-■The PSeUdOCOde for the two models for N-h oaer polynomial approXimatδ'n∙
A INTRoDUCTIoN
The appendix is OrganiZed 戋
• SeCtiOIl B ProVideS the LemmaS and their PrOOfS required for OUr derivation
• SeCtiOIl C generalizes the COUpled CPdeCOmPOSitIIfOr N^h oer expansP
• SeCtiOIl D extends the experiments to 3D manifold
• In Seetion B additional experiments On image generation WithIear blocks are COndUCted∙
• COmPariSOllS With PoPUIar GAN architectures are COlIdUetedSeCtion F. SPeCifiCa=y we
UtiHZe three popular generator architectures and devise their polynomial equivalent and
PerfOrm COmPariSOnS On image generation，Weso COndUCt an ablation StUdy indicating how
Standard engineering techniques affect the image generation Ofthe polynomial generator
• In SeCtion Gy a COmPariSOn between the two ProPoSed decompositions is COndUCted On data
distributionS from the PreVus SeCtiOns∙
BPROOFS
For a Set Of matrices {XE m 审 mxN}产一 the KhatrRaOPrOdUCtiS denoted by
⅛
Xi Θ X2 Θ : ∙ Θ XM 卜 O Xm (II)
TnHI
In this SeCtiOnyWePrOVe the following identity COnneCting the SetS Of matrices {AU为I<XK}^1 and
{32X产；
N N
§ A∙ § S H (A『• Bl)AK ∙ B2) * ∙ ∙ ∙a4 ∙ BN) (12)
Ho demonstrate the SimPIe CaSe With two matriceWe ProVe first the SPeCial CaSe With N M 2.
Lemma L H holds that
(Al Θ A2)τ ∙ (Bl Θ B2) H (* Bl) *(∙ B2) (13)
Proof Initially both SideS Of the equation havedimens5'nS OfKXI7》Lec5ey match，The (c∙)
element Ofthe matriX PrOdUCt Of ∙ Bl) is
12
Under review as a conference paper at ICLR 2020
I1
A1,pk1,iqB1,pk1,jq	(14)
k1“1
Then the pi, j q element of the right hand side (rhs) of (13) is:
I1	I2
Erhs = P Σ A1,(kι,i)B1,(kι,j)q - p £ A2,(k2,i)B2,(k2,j)q =
k1“1	k2“1
I1	I2
PA1,(k1,iqA2,(k2,iqqPB(1,k1,jqB2,(k2,jqq
k1“1 k2“1
(15)
From the definition of Khatri-Rao, it is straightforward to obtain the Pρ, iq element with ρ =
Pk1 ´ 1qI2 ` k2, (i.e. ρ P r1, I1I2s) ofA1 dA2 as A1,(k1,iqA2,(k2,iq. Similarly, the Pρ, jq element
of B1 d B2 is B1,(k1,jqB2,(k2,jq.
The respective Pi,jq element of the left hand side (lhs) of (13) is:
I1 I2
Elhs =	A1,(k1,iqA2,(k2,iqB1,(k1,jqB2,(k2,jq =
ρ“1
I1	I2
A1,(k1,iqA2,(k2,iqB1,(k1,jqB2,(k2,jq = Erhs
k1“1 k2“1
(16)
In the last equation, We replace the sum in P (ρ P [1, I1I2S) with the equivalent sums in k∖, k2.	□
In a similar manner, we generalize the identity to the case of N > 2 terms below.
Lemma 2. It holds that
NN
p Q AV qτ ∙p Q BV q = (AT ∙ bo *(aT ∙ B2)*... *(aN ∙ BN q	(⑺
Proof. The rhs includes the Hadamard products of the matrices AT ∙ BV. Each matrix multiplication
(AT ∙ BV) results in a matrix of K X L dimensions. Thus, the rhs is a matrix of K X L dimensions.
The lhs is a matrix multiplication of two Khatri-Rao products. The first Khatri-Rao product has
dimensions K X PnV IV), while the second PnV IV) X L. Altogether, the lhs has K X L dimensions.
Similarly to the previous Lemma, the pi, jq element of the rhs is:
I1	I2	IN
Erhs = P X A1,(kι,i)B1,(kι,j))∙( X A2,(k2,i)B2,(k2,j)) . . . (X AN,(kN,i)BN,(kN,j)q =
k1“1	k2“1	kN“1
I1	I2	IN
...	PA1,(k1,i)A2,(k2,i) . . . AN,(kN,i)qPB1,(k1,j)B2,(k2,j) . . . BN,(kN,j)q
k1“1 k2“1	kN“1
To proceed with the lhs, it is straightforward to derive that
N
P AVq = A1,(s1,i)A2,(s2,i) . . . AN,(sN,i)
V“1
(18)
(19)
where si = i and SV is a recursive function of the s”—1.
However, the recursive definition of sV is summed in the multiplication and we obtain:
I1	I2	IN
Elhs =	. . .	PA1,(k1,i)A2,(k2,i) . . . AN,(kN,i)qPB1,(k1,j)B2,(k2,j) . . . BN,(kN,j)q = Erhs
k1“1 k2“1	kN“1
(20)
□
13
Under review as a conference paper at ICLR 2020
B.1 Proofs for model 1
Below, we prove that (7) (main paper) is equivalent to the three-layer neural network as shown in
Figure 2.
Claim 1. Let ω “ (UTsz) * (UTsz) ' UTsz∙
Then, the form of (7) is equal to:
)
Gpzq “ β ` c
* ω ` ω
(21)
Proof. Applying Lemma 2 on (7), we obtain:
Gpzq “ β ` C	UrT1sz `	UrT3sz	*	UrT1sz	`	UrT2sz	*	UrT1sz	`
(UrT3sz) * (UrT2sz) * (UrT1sz)* “
β ' C"(U[Tsz) * I(UTsz) * (UTsz) + UTsz1 + (UTsz) * (UTsz) + UTsz*
(22)
The last equation is the same as (21).
□
B.2 Proofs for model 2
In Claim 2 and Claim 3, we prove that (10) (main paper) is equivalent to the three-layer neural
network as shown in Figure 3.
Claim 2. Let
ω “ pA[2sqTz * pB[2s qTb[2s + pS[2sqT	pA[1sqTz * pB[1sqTb[1s	(23)
It holds that
ω “ "A[2s d ”(A[1s d B[1s)S[2s1* (z d z d b[1s) + (A[2s d B[2s) (z d b[2s)	(24)
Proof. We will prove the equivalence starting from (23) and transform it into (24). From (23):
ω “ (pA[2sqT z) * (pB[2sqTb[2s) + (pA[2s qTz) *	pS[2sqT	pA[1s qTz	*	pB[1s qTb[1s	“
T
(A[2s	dB[2s)	(z d	b[2s)	+ (pA[2s qTz) *	A[1s	d B[1s	S[2s	(z d b[1s)
(25)
where in the last equation, we have applied Lemma 1. Applying the Lemma once more in the last
term of (25), We obtain (24).	□
Claim 3. Let
λ “ β + C
pA[3sqTz	* pB[3sqTb[3s +pS[3sqTω
(26)
with ω as in Claim 2. Then, it holds for Gpzq of (10) that Gpzq “ λ.
14
Under review as a conference paper at ICLR 2020
Proof. Transforming (26) into (10):
λ “ β + c] ((A[3])tz) * ((B[3s)Tb[3s) + ((A[3])Tz) * ((S[3s)Tω) } “
β + c[^A[3] d B[3])	(Z d b[3]) + ((A[3s)tz) * ^(S[3S)Tω)*
(27)
To simplify the notation, we define M1 “
Ar2sd
Then, ω “ M1T z d z d br1s + M2T z d br2s
dBr1s
and M2 “ Ar2sdBr2s .
The last term of (27) becomes:
(28)
Replacing (28) into (27), we obtain (10).
□
Note that the λ in Claim 3 is the equation behind Figure 3. By proving the claim, we have illus-
trated how the polynomial generator can be transformed into a network architecture for third-order
approximation.
C Derivations
In this Section, we will show how the Coupled CP decomposition generalizes to the Nth order
approximation. It suffices to find the decomposition that converts the Nth order polynomial into a
network structure (see Alg. 1).
As done in Section 2.2, we capture the nth order interactions by decomposing the parameter tensor
Wrns (with 2 ≤ n ≤ N) as:
N	ji—1	jn´2 —1
Wrns= ∑	∑ ... ∑ WInL「...ji	(29)
ji“n j2“n — 1	jn-ι=2
loooooooooooooooooooooooon
pn—1qsums
The term Wr1n:js	:...:j denotes the interactions across the layers 1, jn—1, . . . , j1. The Nth order
approximation becomes:
N ( / N j1 ´1	jn-2-1	n / n ∖ Λ
G(Z) = β + ∑{(∑ Σ ... Σ (WInL…1)(1))(。z)}	(30)
n = 1 I、ji=nj2=n — 1	jn-=2	P " × m=1 J J
By considering the mode-1 unfoding of Coupled CP decomposition (like in Section 2.2), we obtain:
N(N ji — 1	jn´2 — 1
G(z)=β+C∑	∑	∑ ... ∑	Urj1s
n=1，ji=nj2=n —1	jn´i =2
d...dUrjn-SdUrIs) ( Θ z)]=
m=1
N(N	ji —1	jn—2 — 1
β+C∑	∑	∑ ... ∑
n=1 lji=nj2=n —1	jn—i=2
*...*
= β + CxN
(31)
15
Under review as a conference paper at ICLR 2020
where we use xN as an abbreviation of the sums. In the last equation, we have used Lemma 2
(Section B).
Claim 4. The Nth order approximation of (30) can be implemented with a neural network as
described in Alg. 1.
Proof. We will use induction to prove the Claim. For N “ 2, it trivially holds, while the proof for
N “ 3 is provided in Claim 1. Suppose it holds for Nth order approximation; we prove below that it
holds for N ` 1th order approximation.
Let us denote the approximation of (30) as GNpzq. The pN ` 1qth order approximation from (30) is:
N'1	(	N N'1 j1 ´1	jn-2 ´1	∖	/ n	∖ ʌ
Gn'ipzq “ β + ∑	∑	∑ ... ∑ (WInL3)(1)	。Z “
n“11' ji=nj2=n — 1	jn´i =2	m m m“1	/ J
N f / N	ji´1 jn´2´1	∖ / n ∖
β + ∑ (∑	∑ ... ∑	(WInL ijι)pιq) (θ z)+	(32)
n = 1 I \ji=nj2=n —1	jn_i=2	P q ∖m=1 /
N	jn-2_1	n ʌ	N '1
∑ ... ∑ (W 1"jLι:…j2：(N'1))p1)(Θ z)}+(W葭."(N'1))p1q(Θ Z)
j2=n —1	jn_i=2	P q m = 1	J	P q m=1
In the last equation, the first term in the sums is xN; for the rest two terms we apply Lemma 2:
GN '1(Z) = β + CxN
+ Cl (UTV+1]z) * (UTVSz) * ... * (UTsZ) * (UTsZ) 1 +
N	N	jn´2 ´1
C ∑ ]	∑ ... ∑ ((UrN+1sz) * (UT2sZ) *... * (UrLisZ) * (UTsZ))}=
n=1 I j2=n —1	jn´i =2、	,)
(33)
β+CxN
+ C (UrTN'1sZ) *	(UrTNsZ)
*...*
where
N N	jn´2 —1
λ = ∑	∑ ... ∑ ((UT2sZ)* ... * (UTn- 1sZ)* (UTsZ))	(34)
n=1 j2=n—1 jn-∖=2 '	J
The term λ is equal to the κ = (n ´ 1)th order of (31), while there is only a single term for n = N.
Therefore, (33) is transformed into:
GN'1(Z) =β+CxN+C	UrTN'1sZ *xN	(35)
which is exactly the form described by Alg. 1. This concludes the induction proof.
□
D Experiments on surfaces
Astroid: We implement a superellipse with parametric expression [ɑ cos31, a sin31] for t P [—a, α].
This has a more complex distribution and four sharp edges. The random samples are visualized in
Figure 7. PolyGAN models the data distribution accurately in contrast to the two baselines.
We conduct three experiments in which the data distribution is analytically derived. The experiments
are:
16
Under review as a conference paper at ICLR 2020
(a) GT
(b) Orig
(c) Concat
(d) PolyGAN
Figure 7: Synthesized data for learning the ‘astroid’ signal. No activation functions are used in the
generators.
Sin3D: The data manifold is an extension over the 2D manifold of the sinusoidal experiment
(Section 4.1). The function We want to learn is Gpz) : R2 → R3 with the data manifold described by
the VeCtor [x, y, sinp10 * ,x2 + y2)] for x,y P [—0.5, 0.5].
In Figure 8, 20, 000 samples are sampled from the generators and visualized. PolyGAN captures the
data distribution, while ‘Orig’ and ‘Concat’ fail.
(a) GT
(b) Orig
(c) Concat	(d) PolyGAN
Figure 8: Experiment on 3D synthetic data. From left to right: (a) the data distribution, (b) ‘Orig’, (c)
‘Concat’, (d) PolyGAN. As expected, the ‘Orig’ and the ‘Concat’ cannot capture the data distribution.
Swiss roll: The three dimensional vector [t∙sin t, y, t∙cos t] + 0.05 ∙ S for t, y P [0,1] and S 〜N(0,1)
forms the data manifold2. In Figure 9, 20, 000 samples are visualized.
(b) Orig
Figure 9:	Experiment on 3D synthetic data (‘swiss roll’). From left to right: (a) the data distribution,
(b) ‘Orig’, (c) ‘Concat’, (d) PolyGAN.
Gabriel's Horn: The three dimensional vector [x, α ∙ coχSt, α ∙ Sn^S for t P [0,160πS and X P [1,4]
forms the data manifold. The dependence on both sinusoidal and the function 1 makes this curve
challenging for a polynomial expansion. In Figure 10, the synthesized samples are plotted. PolyGAN
learns how to generate samples on the manifold despite the fraction in the parametric form.
E Image generation with linear blocks
Apart from digit generation (Section 4.2), we conduct two experiments on image generation of
face and natural scenes. Since both distributions are harder than the digits one, we extend the
2This is a standard synthetic distribution in popular machine learning frameworks such as scikit-learn.
17
Under review as a conference paper at ICLR 2020
Figure 10:	Synthesized data on ‘Gabriel’s Horn’. From left to right: (a) the data distribution, (b)
‘Orig’, (c) ‘Concat’, (d) PolyGAN.
approximation followed on Section 4.2 by one order, i.e., we assume a fifth-order approximation. We
emphasize that each block is a residual block with no activation functions.
Faces: In the experiment with faces, we utilize as the training samples the YaleB (Georghiades et al.,
2001) dataset. The dataset includes greyscale images of faces under extreme illuminations. We
rescale all of the images into 64 ^ 64 for our analysis.
Random samples are illustrated in Figure 11. Our method generates diverse images and captures the
case of illuminating either half part of the face, while ‘Orig’ and ‘Concat’ generate images that have
a dark side only on the left and right side, respectively. The difference becomes profound in the finer
details of the face (please zoom in), where both baselines fail to synthesize realistic semantic parts of
the face.
Figure 11: Image generation on faces (YaleB (Georghiades et al., 2001)) for a generator with linear
blocks and a single activation function only on the output (i.e., tan h). Notice that our method can
illuminate either the left or right part of the face, in contrast to ‘Orig’ (and ‘Concat’) which generate
images that have a dark side only on the left (respectively right) side. In addition, both ‘Orig’ and
‘Concat’ fail to capture the fine details of the facial structure (please zoom in for the details).
Natural scenes: We further evaluate the generation of natural images, specifically by training on
CIFAR10 (Krizhevsky et al., 2014). CIFAR10 includes 50, 000 training images of 32 ^ 32 ^ 3
resolution.
In Table 3, we evaluate the standard metrics of Inception Score (IS) and Frechet Inception Distance
(FID) (see more details for the metrics in section F). Our model outperforms both ‘Orig’ and ‘Concat’
by a considerable margin. In Figure 12, some random synthesized samples are presented.
Table 3: IS/FID scores on CIFAR10 (Krizhevsky et al., 2014) with linear blocks.
conditional SNGAN with linear blocks on CIFAR10		
Model	IS (↑)	FID⑴
Orig	4.47 ± 0.21	156.67 ± 12.29
Concat	4.23 ± 0.37	188.08 ± 17.00
PolyGAN	6.43 ± 0.1T-	53.50 ± 2.71
18
Under review as a conference paper at ICLR 2020
(a) GT
(b) Orig
(c) Concat	(d) PolyGAN
Figure 12: Conditional image generation on CIFAR10 for a generator with linear blocks and a single
activation function. Our approach generates more realistic samples in comparison to the compared
methods, where severe mode collapse also takes place.
F Image generation with activation functions
To demonstrate the flexibility of the PolyGAN, we utilize three different popular generators. The
three acrhitectures chosen are DCGAN (Radford et al., 2015), SNGAN (Miyato et al., 2018), and
SAGAN (Zhang et al., 2019). Each original generator is converted into a polynomial expansion,
while we use the non-linearities to boost the performance of the polynomial generator. The hyper-
parameters are kept the same as the corresponding baseline. Algorithms 3 and 4 succinctly present
the key differences of our approach compared to the traditional one (in the case of SNGAN, similarly
for other architectures).
In addition to the baseline, we implement the most closely related alternative to our framework,
namely instead of using the Hadamard operator as in Figure 3, we concatenate the noise with the
feature representations at that block. The latter approach is frequently used in the literature (Berthelot
et al., 2017; Brock et al., 2019) (referred as “Concat” in the paper). The number of the trainable
parameters of the generators are reported in Table 13. Our method has only a minimal increase of the
parameters, while the concatenation increases the number of parameters substantially.
To reduce the variance often observed during GAN training (Lucic et al., 2018; Odena et al., 2018),
each reported score is averaged over 10 runs utilizing different seeds. The metrics we utilize are
Inception Score (IS) (Salimans et al., 2016) and Frechet Inception Distance (FID) (Heusel et al.,
2017).
Below, we perform an ablation study on Section F.2, and then present the experiments on unsupervised
(Section F.3) and conditional image generation (Section F.4) respectively.
Datasets: We use CIFAR10 (Krizhevsky et al., 2014) and Imagenet (Russakovsky et al., 2015) as the
two most widely used baselines for GANs:
•	CIFAR10 (KrizheVsky et al., 2014) includes 60,000 images of 32 X 32 resolution. We use
50, 000 images for training and the rest for testing.
•	Imagenet (RussakoVsky et al., 2015) is a large scale dataset that includes oVer one million
training images and 50,000 validation images. We reshape the images to 128 X 128
resolution.
Baseline architectures: The architectures employed are:
•	DCGAN (Radford et al., 2015), as implemented in https://github.com/pytorch/
examples/tree/master/dcgan. This is a widely used baseline.
•	SNGAN (Miyato et al., 2018), as implemented in https://github.com/
pfnet-research/sngan_projection. SNGAN is a strong performing GAN that
introduced a spectral normalization in the discriminator.
•	SAGAN (Zhang et al., 2019), as implemented in https://github.com/voletiv/
self-attention-GAN-pytorch. This is a recent network architecture that utilizes
19
Under review as a conference paper at ICLR 2020
the notion of self-attention (Wang et al., 2018) in a GAN setting, achieving impressive
results on Imagenet (Russakovsky et al., 2015).
The default hyper-parameters are left unchanged. The aforementioned codes are used for reporting
the results of both the baseline and our method to avoid any discrepancies, e.g. different frameworks
resulting in unfair comparisons. The source code will be released to enable the reproduction of our
results.
Evaluation metrics: The popular Inception Score (IS) (Salimans et al., 2016) and Frechet Inception
Distance (FID) (Heusel et al., 2017) are used for the quantitative evaluation. Both scores extract
feature representations from a pretrained classifier (in practice the Inception network (Szegedy et al.,
2015)). Despite their shortcomings, IS and FID are widely used (Lucic et al., 2018; Creswell et al.,
2018), since alternative metrics fail for generative models (Theis et al., 2016).
The Inception Score is defined as
exp pExPPθ rKLpppy|xq}ppyqsq
(36)
where x is a generated sample and ppy|xq is the conditional distribution for labels y. The distribution
p(y) over the labels is approximated by 吉 XnM“i pPy∣Xn) for Xn generated samples. Following the
methods in the literature (Miyato et al., 2018), we compute the inception score for M “ 5, 000
generated samples per run (10 splits for each run).
The Frechet Inception Distance (FID) utilizes feature representations from a pretrained net-
work (Szegedy et al., 2015) and assumes that the distributions of these representations are Gaussian.
Denoting the representations of real images as N(μr, Cr) and the generated (fake) as N(μ∕, Cf),
FID is:
}μr ´ μf }2 + trace'Cr + Cf ´ 2'CrCf) 1/2)
(37)
In the experiments, we use M “ 10, 000 to compute the mean and covariance of the real images and
M “ 10,000 synthesized samples for μf, Cr.
For both scores the original tensorflow inception network weights are used; the routines of tensor-
flow.contrib.gan.eval are called for the metric evaluation.
F.1 Implementation details
We experimentally define that a (series of) affine transformation(s) on the input noise z are beneficial
before using the transformed z for the Hadamard products.3 These affine transformations are
henceforth mentioned as global transformations on z .
The implementation details for each network are the following:
•	DCGAN: We use a global transformation followed by a RELU non-linearity. WThe rest
details remain the same as the baseline model.
•	SNGAN: Similarly to DCGAN, we use a global transformation with a RELU non-linearity.
We consider each residual block as one order of approximation and compute the Hadamard product
after each block (see algorithm 4).
F.2 Ablation study on CIFAR 1 0 with non-linear generators
We conduct an ablation study based on SNGAN architecture (or our variant of SNGAN-poly),
since most recent methods are based on similar generators Zhang et al. (2019); Brock et al. (2019).
Unless explicitly mentioned otherwise, the SNGAN is trained on CIFAR10 for unsupervised image
generation.
3A similar transformation is performed in other GAN architectures, such as in Karras et al. (2019).
20
Under review as a conference paper at ICLR 2020
	Algorithm 3: Original SNGAN generator.			Algorithm 4: Modified SNGAN-poly.	
	Input :Noise Z P R128, φ “ RELU Output :X P R32^32^3			Input :Noise z P R128, φ “ RELU Output : X P r3^3—3	
1			1	% global transformation of z .	
2			2	v	= φ(Linear(z))
3	% fully-connected layer for reshaping.		3	% fully-connected layer for reshaping.	
4	h	=φ(Linear(z)) % dims out: 4 X 4 X 256.;	4	h	=φ(Linear(v)) % dims out: 4 ^ 4 ^ 256.
5	;		5	% perform a hadamard product here.	
6	;		6	vo “(A[os)T - V	
7	;		7	h	=h * vo % dims out: 4 ^ 4 ^ 256.
8	for n=1:3 do		8	for n=1:3 do	
9		% resnet blocks.	9		% resnet blocks.
10		h = resblock(h)	10		h = resblock(h)
11		% dims out:(4 - 2n) X (4 - 2n) X 256.;	11		% dims out: (4 - 2i) X (4 - 2i) X 256.
12		;	12		% reshape v for hadamard product.
13		;	13		Vn “ (Arns)T - V
14		;	14		h = h * vn
15	end		15	e	nd
16	x	=tanh(Conv(h)) % dims out: 32 X 32 X 3.	16	X	=tanh(Conv(h)) % dims out: 32 X 32 X 3.
Table 4: The algorithm on the left describes the SNGAN generator. The algorithm on the right
preserves the resnet blocks of the SNGAN generator, but converts it into a polynomial (named
SNGAN-poly). The different lines are emphasized with blue color.
Global transformation validation: We add a global transformation on z, i.e. a fully-connected
layer and use the transformed noise as input to the generator. In the first experiment, we evaluate
whether to add a non-linear activation to the global transformation. The two alternatives are: i)
with linear global transformation (‘Ours-linear-global’), i.e. no non-linearity, and ii) with global
transformation followed by a RELU non-linearity (‘Ours-RELU-global’).
Table 5: Global transformation validation on SNGAN. The first two results assess the addition of a
non-linear activation function after the global transformation. The last two rows compare the addition
of a global transformation on the original generator.
	SNGAN on CIFAR10			
Model	IS(T)	FID Q)
OUrS-linear-global	8.23 + 0.10	18.85 ± 0.59
OurS-RELU-global	8.30 ± 0.09	17.65 + 0.76
Original	8.06 ± 0.10	19.06 + 0.50
OriginaI-RELU-global	7.98 + 0.21	37.61 + 7.16~
The first two results in Table 5 demonstrate that both metrics marginally improve when using a
non-linear activation function. We add this global transformation with RELU on the original SNGAN.
The results are reported in the last two rows of Table 5 (where the original is mentioned as ‘Orig’,
while the alternative of adding a global transformation as ‘Original-RELU-global’).
Split z into chunks: The recent BigGAN of (Brock et al., 2019) performs hierarchical synthesis
of images by splitting the latent vector z into one chunk per resolution (block). Each chunk is then
concatenated into the respective resolution.
We scrutinize this splitting against our method; we split the noise z into pk ` 1q non-overlapping
chunks of equal size for performing k injections. The injection with splitting is mentioned as ‘Inject-
split’ below. Our splitting deteriorates the scores on the task as reported in Table 6. It is possible that
more elaborate splitting techniques, such as those in Brock et al. (2019) are beneficial.
Normalization before Hadamard product: In Karras et al. (2019) they normalize the transformed
noise through ADAIN, while in Karras et al. (2018) they similarly perform a feature vector normal-
ization.
21
Under review as a conference paper at ICLR 2020
Table 6: Ablation experiment on splitting the noise z into non-overlapping chunks for the injection.
	SNGAN on CIFAR10			
Model	IS(T)	FID Q)
Original	8.06 + 0.10	19.06 ± 0.50
Inject-SPlit	7.75 ± 0.12	22.08 + 0.98
OUrs-RELU-global	8.30 ± 0.09-	17.65 + 0.76-
We scrutinize a feature normalization on the baseline of ‘Ours-RELU-global’. For each layer i we
divide the Aris z vector with its standard deviation. The variant with global transformation followed
by RELU and normalization before the Hadamard product is called ‘Ours-norm’. The results in
Table 7 illustrate that normalization improves the metrics.
Table 7: Ablation experiment on normalizing the Arisz vector before the Hadamard product.
	SNGAN on CIFAR10			
Model	IS(T)	FID Q)
OUrs-RELU-global	8.30 ± 0.09	17.65 ± 0.76
OUrs-norm	8.37 + 0.1Γ~	17.14 + 0.58~
Skip the Hadamard product: Motivated by the skip connection of our Coupled CP decomposition,
we add a skip connection to each Hadamard product. For instance, we modify the term
((B[is)τb[is) into ((ArIs)Tz) * ((8^)%" + PB∏sqτb∏s.
(PArIsqT z) *
In Table 8, we use ‘Ours-RELU-global’ as baseline against the model with the skip connection
(‘Ours-skip’).
Table 8: Ablation experiment on adding a skip connection to each Hadamard product.
	SNGAN on CIFAR10			
Model	IS(T)	FID Q)
OUrs-RELU-global	8.30 ± 0.09	17.65 ± 0.76
Ours-skip	8.43 + 0.1Γ~	21.54 + 1.59~~
Since we use SNGAN both for unsupervised/conditional image generation, we verify the afore-
mentioned results in the conditional setting, i.e. when the class information is also provided to the
generator and the discriminator.
Normalization before Hadamard product: Similarly to the experiment above, for each layer i we
divide the Arisz vector with its standard deviation. The quantitative results in Table 9 improve the IS
score, but the FID deteriorates.
Skip the Hadamard product: Similarly to the aforementioned unsupervised case, we assess the
performance if we add a skip connection in the Hadamard. In Table 10, the quantitative results
comparing the baseline and the skip case are presented.
F.3 Unsupervised image generation
In this experiment, we study the image generation problem without any labels or class information for
the images. The architectures of DCGAN and resnet-based SNGAN are used for image generation
in CIFAR10 (Krizhevsky et al., 2014). Table 11 summarizes the results of the IS/FID scores of the
compared methods. In all of the experiments, PolyGAN outperforms the compared methods.
F.4 Conditional image generation
Frequently class information is available. We can utilize the labels, e.g. use conditional batch normal-
ization or class embeddings, to synthesize images conditioned on a class. We train two networks,
22
Under review as a conference paper at ICLR 2020
Table 9: Ablation experiment (conditional GAN setting) on normalizing the Aris z vector before the
Hadamard product.
	Conditional SNGAN on CIFAR10			
Model	IS(T)	FID Q)
OUrs-RELU-global	8.66 + 0.14	13.52 ± 0.60
OUrs-norm	8.76 ± 0.1Γ~	15.40 + 1.29~~
Table 10: Ablation experiment (conditional GAN setting) on adding a skip connection to each
Hadamard product.
	Conditional SNGAN on CIFAR10			
Model	IS(T)	FID Q)
OUrs-RELU-global	8.66 ± 0.14	13.52 ± 0.60
OUrs-SkiP	8.77 + 0.10~	13.62 + 0.69~~
i.e., SNGAN (Miyato et al., 2018) in CIFAR10 (Krizhevsky et al., 2014) and SAGAN (Zhang et al.,
2019) in Imagenet (Russakovsky et al., 2015). SAGAN uses self-attention blocks (Wang et al., 2018)
to improve the resnet-based generator.
Despite our best efforts to show that our method is both architecture and database agnostic, the recent
methods are run for hundreds of thousands or even million iterations till “convergence”. In SAGAN
the authors report that for each training multiple GPUs need to be utilized for weeks to reach the
final reported Inception Score. We report the metrics for networks that are run with batch size 64
(i.e., four times less than the original 256) to fit in a single 16GB NVIDIA V100 GPU. Following the
current practice in ML, due to the lack of computational budget (Hoogeboom et al., 2019), we run
SAGAN for 400, 000 iterations (see Figure 3 of the original paper for the IS during training)4. Each
such experiment takes roughly 6 days to train. The FID/IS scores of our approach compared against
the baseline method can be found in Table 12. In both cases, our proposed method yields a higher
Inception Score and a lower FID.
G	Experimental model comparison
An experimental comparison of the two models described in Section 2 is conducted below. Unless
explicitly mentioned otherwise, the networks used below do not include any non-linear activation
functions, they are polynomial expansions with linear blocks. We use the following four experiments:
Sinusoidal on 2D: The data distribution is described by rx, sinpxqs with x P r0, 2πs (see Section 4.1
for further details). We assume 8th order approximation for Coupled CP decomposition and 12th
order for Coupled nested CP decomposition. Both have width 15 units. The comparison between the
two models in Figure 13 demonstrates that they can both capture the data manifold. Impressively, the
Coupled CP decomposition does not synthesize a single point that is outside of the manifold.
Astroid: The data distribution is described on Section D. The samples comparing the two models are
visualized in Figure 15.
Sin3D: The data distribution is described on Section D. In Figure 15 the samples from the two models
are illustrated.
Swiss roll: The data distribution is described on Section D. In Figure 16 the samples from the two
models are illustrated.
Digit generation: We conduct an experiment on images to verify that both architectures can learn
higher-dimensional distributions. We select the digit images as described in Section 4.2. In this case,
Coupled CP decomposition is implemented as follows: each Uris is a series of linear convolutions
with stride 2 for i “ 1, . . . , 4, while C is a linear residual block. We emphasize that in both models
4 Given the batch size difference, our training corresponds to roughly the 100, 000 steps of the authors
reported results.
23
Under review as a conference paper at ICLR 2020
Table 11: IS/FID scores on CIFAR10 (Krizhevsky et al., 2014) utilizing DCGAN (Radford et al.,
2015) and SNGAN (Miyato et al., 2018) architectures for unsupervised image generation. Each
network is run for 10 times and the mean and standard deviation are reported. In both cases, inserting
block-wise noise injections to the generator (i.e., converting to our proposed PolyGAN) results in an
improved score. Higher IS / lower FID score indicate better performance.
DCGAN		
Model	IS (T)	FID (D
Orig	6.25 + 0.06	47.29 ± 2.06
Concat	6.03 ± 0.06	49.35 + 2.17
PolyGAN	6.61 ± 0.05	42.86 + 1.02
SNGAN		
Model	IS⑴	FID (D
Orig	8.06 ± 0.10	19.06 ± 0.50
Concat	8.28 ± 0.16	20.77 ± 2.91
PolyGAN	8.30 ± 0.09	17.65 ± 0.76
Table 12:	Quantitative results on conditional image generation. We implement both SNGAN trained
on CIFAR10 and SAGAN trained on Imagenet (for 400, 000 iterations). Each network is run for 10
times and the mean and variance are reported.
SNGAN (CIFAR10)		
Model	IS⑴	FID (D
Orig	8.30 ± 0.11	14.70 ± 0.97
PolyGAN	8.66 + 0.14	13.52 ± 0.60
SAGAN (Imagenet)		
Model	IS⑴	FID Q)
Orig	13.81 ± 0.21	138.20 ± 8.71
PolyGAN	14.60 + 0.15	84.37 + 6.37
all the activation functions are removed and there is a single tanh in the output of the generator for
normalization purposes.
24
Under review as a conference paper at ICLR 2020
Table 13:	Number of parameters for the generators of each approach and on various databases. As
can be seen, our method only marginally increases the parameters while substantially improving the
performance. On the other hand, “Concat” significantly increases the parameters without analogous
increase in the performance.
DCGAN (CIFAR10)	
Model	Params
Orig	3,573,440
ConCat	6,416,448
PolyGAN	3,663,936
SNGAN (CIFAR10)	
Model	Params
Orig	4, 276,739
Concat	6, 383, 875
PolyGAN	4,408, 835
SAGAN (Imagenet)	
Model	Params
Orig	42, 079, 300
PolyGAN	42, 351,748
(a)	GT	(b) Coupled CP decomposition (c) Coupled nested CP decomposition
Figure 13: Synthesized data for learning the rx, sinpxqs signal. No activation functions are used
in the generators. From left to right: (a) the data distribution, (b) Coupled CP decomposition, (c)
Coupled nested CP decomposition.
(a) GT	(b) Coupled CP decomposition (c) Coupled nested CP decomposition
Figure 14: Synthesized data for learning the Astroid. Both models generate only plausible examples.
(a) GT	(b) Coupled CP decomposition (c) Coupled nested CP decomposi-
tion
Figure 15: Experiment on 3D synthetic data. From left to right: (a) the data distribution, (b) Coupled
CP decomposition, (c) Coupled nested CP decomposition.
25
Under review as a conference paper at ICLR 2020
(a) GT	(b) Coupled CP decomposition (c) Coupled nested CP decomposi-
tion
Figure 16: Experiment on 3D synthetic data (‘swiss roll’). From left to right: (a) the data distribution,
(b) Coupled CP decomposition, (c) Coupled nested CP decomposition. Note that Coupled CP
decompositiongenerates some noisy samples in contrast to Coupled nested CP decomposition.
70 73)6 ɔɔznʒ
乙/
fo	q
6	S
∖	ʒ
7	Y
3	B
S	q
O	2
(6	2
。q / /
0 15 9
54 ◎。
口 7 2Q
二 3 5 ∖
5 (p 0 4
a > 4 s
7 I f 3
1 g力7
3 I G ∖
√
2
干
R74 I 工
7
4
q
(
O
7
i
q
y72q6070⅛7
5^9 7S夕i>1 彳59-
Nq d 24Jg 5ΛU4
Q^τN 9y43uJ>
VYqfr7。QaN 9
? Z
N O
I i
O O
9 9
7 7
3 ∂
2 t
70 6 g∙65 — I
7doo∙7 3fsp
5 70oo5∕0∕6
(a) GT
(b)	Coupled CP decomposition (c) Coupled nested CP decomposi-
tion
Figure 17: Comparison of the two decompositions on digit generation.
26