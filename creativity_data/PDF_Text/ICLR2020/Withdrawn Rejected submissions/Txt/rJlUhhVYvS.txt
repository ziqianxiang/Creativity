Under review as a conference paper at ICLR 2020
Understanding Isomorphism Bias in Graph
Data Sets
Anonymous authors
Paper under double-blind review
Ab stract
In recent years there has been a rapid increase in classification methods on graph
structured data. Both in graph kernels and graph neural networks, one of the
implicit assumptions of successful state-of-the-art models was that incorporating
graph isomorphism features into the architecture leads to better empirical perfor-
mance. However, as we discover in this work, commonly used data sets for graph
classification have repeating instances which cause the problem of isomorphism
bias, i.e. artificially increasing the accuracy of the models by memorizing target
information from the training set. The problem does not vanish even in consider-
ation of node labels. This prevents fair competition of the algorithms and raises
a question of the validity of the obtained results. First, we characterize this ef-
fect theoretically by reducing the graph classification to a weighted classification
problem and estimating the corresponding generalization gap. Then we analyze
54 data sets, previously extensively used for graph-related tasks, on the existence
of isomorphism bias, give a set of recommendations to machine learning prac-
titioners to properly set up their models, and open source new data sets for the
future experiments.
1	Introduction
Recently there has been an increasing interest in the development of machine learning models
that operate on graph structured data. Such models have found applications in chemoinformatics
(RalaivoIa et al. (2005); RUPP & Schneider (2010); Ferre et al. (2017)) and bioinformatics (Borg-
wardt et al. (2005); Kundu et al. (2013)), neuroscience (Sharaev et al. (2018); Jie et al. (2016); Wang
et al. (2016)), comPUter vision (StUmm et al. (2016)) and system secUrity (Li et al. (2016)), natU-
ral language processing (GIavaS & Snajder (2013)), and others (Knege et al. (2019); NlkoIentzos
et al. (2019)). One of the PoPUlar tasks that encomPasses these aPPlications is graPh classification
Problem for which many graPh kernels and graPh neural networks have been develoPed.
One of the imPlicit assumPtions that many Practitioners adhere to is that models that can distin-
guish isomorPhic instances from non-isomorPhic ones Possess higher exPressiveness in classifica-
tion Problem and hence much efforts have been devoted to incorPorate efficient graPh isomorPhism
methods into the classification models. As the Problem of comPuting comPlete graPh invariant is
GI-hard (Gartner et al. (2003)), for which no known polynomial-time algorithm exists, other heuris-
tics have been ProPosed as a Proxy for deciding whether two graPhs are isomorPhic. Indeed, from
the early days topological descriptors such Wiener index (Wiener (1947a;b)) attempted to find a
single number that uniquely identifies a graph. Later, graph kernels that model pairwise similarities
between graphs utilized theoretical developments in graph isomorphism literature. For example,
graphlet kernel (Shervashidze et al. (2009)) is based on the Kelly conjecture (see also Kelly (1957)),
anonymous walk kernel (Ivanov & Burnaev (2018)) derives insights from the reconstruction proper-
ties of anonymous experiments (see also Micali & Allen Zhu (2016)), and WL kernel (Shervashidze
et al. (2011a)) is based on an efficient graph isomorphism algorithm. For sufficiently large k, k-
dimensional WL algorithm includes all combinatorial properties of a graph (Cai et al. (1992a)), so
one may hope its power is enough for the data set at hand. Since only for k = Ω(n) WL algorithm
is guaranteed to distinguish all graphs (for which the running time becomes exponential; see also
FUrer (2017)), in the general case WL algorithm can be used only as a strong baseline for graph
isomorphism. In similar fashion, graph neural networks exploit graph isomorphism algorithms and
1
Under review as a conference paper at ICLR 2020
have been shown to be as powerful as k-dimensional WL algorithm (see for example Maron et al.
(2019); Xu et al. (2018); Morris et al. (2019)).
Experimental evaluation reveals that models based on the theoretical constructions with high com-
binatorial power such as WL algorithm performs better than the models without them such as Vertex
histogram kernel (Vishwanathan et al. (2010)) on a commonly used data sets. This could add addi-
tional bias to results of comparison of classification algorithms since the models could simply apply
a graph isomorphism method (or an efficient approximation) to determine a target label at the infer-
ence time. However, purely judging on the accuracy of the algorithms in such cases would imply
an unfair comparison between the methods as it does not measure correctly generalization ability of
the models on the new test instances. As we discover, indeed many of the data sets used in graph
classification have isomorphic instances so much that in some of them the fraction of the unique
non-repeating graphs is as low as 20% of the total size. This challenges previous experimental re-
sults and requires understanding of how influential isomorphic instances on the final performance of
the models. Our contributions are:
•	We analyze the quality of54 graph data sets which are used ubiquitously in graph classifica-
tion comparison. Our findings suggest that in the most of the data sets there are isomorphic
graphs and their proportion varies from as much as 100% to 0%. Surprisingly, we also
found that there are isomorphic instances that have different target labels suggesting they
are not suitable for learning a classifier at all.
•	We investigate the causes of isomorphic graphs and show that node and edge labels are
important to identify isomorphic graphs. Other causes include numerical attributes of nodes
and edges as well as the sizes of the data set.
•	We express an upper bound for the generalization gap through the Radamacher complexity
of a classifier and the number of isomorphic graphs in a data set. This bound presents the-
oretical evidence on how weightning of each graph in the training influences classification
accuracy.
•	We evaluate a classification model’s performance on isomorphic instances and show that
even strong models do not achieve optimal accuracy even if the instances have been seen at
the training time. Hence we show a model-agnostic way to artificially increase performance
on several widely used data sets.
•	We open-source new cleaned data sets that contain only non-isomorphic instances with no
noisy target labels. We give a set of recommendations regarding applying new models that
work with graph structured data.
2	Related work
Measuring quality of data sets. A similar issue of duplicates instances in commonly used data sets
was recently discovered in computer vision domain. Recht et al. (2019); Barz & Denzler (2019);
Birodkar et al. (2019) discover that image data sets CIFAR and ImageNet contain at least 10% of the
duplicate images in the test test invalidating previous performance and questioning generalization
abilities of previous successful architectures. In particular, evaluating the models in new test sets
shows a drop of accuracy by as much as 15% (Recht et al., 2019), which is explained by models’
incapability to generalize to unseen slightly ”harder” instances than in the original test sets. In graph
domain, a fresh look into understanding of expressiveness of graph kernels and the quality of data
sets has been considered in Kriege et al. (2019), where an extensive comparison of existing graph
kernels is done and a few insights about models’ behavior are suggested. In contrast, we conduct a
broader study of isomorphism metrics, revealing all isomorphism pairs in proposed 54 data sets, and
propose new cleaned data. Additionally we also consider graph neural network performance and
argue that current data sets present isomorphism bias which can artificially boost evaluation metrics
in a model-agnostic way.
Explaining performance of graph models. Graph kernels (Kriege et al. (2019)) and graph neural
networks (Wu et al. (2019)) are two competing paradigms for designing graph representations and
solving graph classification and have significantly advanced empirical results due to more efficient
algorithms, incorporating graph invariance into the models, and end-to-end training. Several papers
have tried to justify performance of different families of methods by studying different statistical
2
Under review as a conference paper at ICLR 2020
properties. For example, in Ying et al. (2019) by maximizing mutual information between explana-
tion variables and predicted label distribution, the model is trained to return a small subgraph and the
graph-specific attributes that are the most influential on the decision made by a GNN, which allows
inspection of single- and multi-level predictions in an agnostic manner for GNNs. In another work
(Scarselli et al. (2018)), the VC dimension of GNNs models has been shown to grow as O(p4N 2),
where p is the number of network parameters and N is the number of nodes in a graph, which is
comparable to RNN models. Furthermore, stability and generalization properties of convolutional
GNNs have been shown to depend on the largest eigenvalue of the graph filter and therefore are
attained for properly normalized graph convolutions such as symmetric normalized graph Laplacian
(Verma & Zhang (2019)). Finally, expressivity of graph kernels has been studied from statistical
learning theory (Oneto et al. (2017b)) and property testing (Kriege et al. (2018b)), showing that
graph kernels can capture certain graph properties such as planarity, girth, and chromatic number
(Johansson et al. (2014)). Our approach is complementary to all of the above as we analyze if the
data sets used in experiments have any effect on the final performance.
3	Preliminaries
In this work we analyze 54 graph data sets from Kersting et al. (2016) that are commonly used in
graph classification task. Examples of popular graph data sets are presented in Table 1 and statistics
of all 54 data sets can be found in Table 5, see Section A in the appendix. All data sets represent
a collection of graphs and accompanying categorical label for each graph in the data sets. Some
data sets also include node and/or edge labels that graph classification methods can use to improve
the scoring. Most of the data sets come either from biological domain or from social network
domain. Biological data sets such as MUTAG, ENZYMES, PROTEINS are graphs that represent
small or large molecules, where edges of the graphs are chemical bonds or spatial proximity between
different atoms. Graph labels in these cases encode different properties like toxicity. In social
data sets such as IMDB-BINARY, REDDIT-MULTI-5K, COLLAB the nodes represent people and
edges are relationships in movies, discussion threads, or citation network respectively. Labels in
these cases denote the type of interaction like the genre of the movie/thread or a research subfield.
For completeness we also include synthetic data sets SYNTHETIC (Morris et al. (2016)) that have
continuous attributes and computer vision data sets MSRC (Neumann et al. (2016)), where images
are encoded as graphs. The origin of all data sets can be found in the Table 5.
Table 1: Example of graph data sets. N is the number of graphs, C is the number of different classes.
Avg. Nodes and Avg. Edges is the average number of nodes and edges. N.L. and E.L indicate if the
graphs in a data set contain node or edge labels.
data set	TyPe	N	C	Avg. Nodes	Avg. Edges	NL	EL
MUlAG	Molecular	188	2Γ~	17.93	T979	+	+
ENZYMES	Molecular	600	~6~	32.63	^62Γ4	-+	-
PROTEINS	Molecular	1113	2Γ~	39.06	72822	+	-
IMDB-BINARY	Social	1000	2Γ~	19.77	^9653	-	-
REDDIT-MULTI-5K	Social	4999	5^~	508.52	594.87	-	-
COLLAB	Social	5000	3^~	74.49	2457.78	-	-
SYNTHETIC	Synthetic	300	2Γ~	100	^T96	-	-
Synthie	Synthetic	400		95	172.93	-	-
MSRCNIC	ViSiOn 一	209	^20^	40.28	^966	+	-
MSRC_9	ViSiOn 一	221	-8-	40.58	97.94	+	-
Graph isomorphism. Isomorphism between two graphs G1 = (V1, E1) and G2 = (V2, E2) is a
bijective function φ : V1 7→ V2 such that any edge (u, v) ∈ E1 if and only if (φ(u), φ(v)) ∈ E2.
Graph isomorphism problem asks if such function exists for given two graphs G1 and G2 . We
denote isomorphic graphs as Gi = G2. The problem has efficient algorithms in P for certain classes
of graphs such as planar or bounded-degree graphs (Hopcroft & Wong (1974); Luks (1980)), but
in the general case admits only quasi-polynomial algorithm (Babai (2015)). In practice many GI
solvers are based on individualization-refinement paradigm (Mckay & Piperno (2014)), which for
each graph iteratively updates a permutation of the nodes such that the resulted permutations of two
3
Under review as a conference paper at ICLR 2020
graphs are identical if an only if they are isomorphic. Importantly, while finding such canonical
permutation of a graph is at least as hard as solving GI problem, state-of-the-art solvers tackle
majority of pairs of graphs efficiently, only taking exponential time on the specific hard instances of
graphs that possess highly symmetrical structures (Cai et al. (1992b)).
4	Identifying isomorphism in data sets
To distinguish between different isomorphic graphs inside a data set we use the notion of graph
orbits:
Definition 4.1 (Graph orbit). Let D = {Gi , yi }iN=1 be a data set of graphs and target labels. For a
graph Gi let a set oi = {Gk } be a set of all isomorphic graphs in D to Gi, including Gi . We call oi
the orbit of graph Gi in D. The cardinality of the orbit is called orbit size. An orbit with size one is
called trivial.
In a data set with no isomorphic graphs, the number of orbits equals to the number of graphs in a data
set, N . Hence, the more orbits in a data set, the ”cleaner” it is. Note however that the distribution of
orbit sizes in two different data sets can vary even if they have the same number of orbits. Therefore,
we look at additional metrics that describe the data set.
•	I, aggregated number of graphs that belong to an orbit of size greater than one, i.e. those
graphs that isomorphic counterparts in a data set;
•	I, %, proportion of isomorphic graphs to the total data set size, i.e. N;
•	IP, %, proportion of isomorphic pairs to the total number of graph pairs in a data set
(N(N-1)).
If we consider target labels of graphs in a data set D = {Gi , yi }iN=1 we can also measure agreement
between the labels of two isomorphic data set. If Gi = G2 and yι = y2, then We call graphs
mismatched. Note that if there is more than one target label in an orbit o, then all graphs in this orbit
are mismatched. To obtain isomorphic graphs, we run nauty algorithm (Mckay & Piperno, 2014) on
all possible pairs of graphs in a data set. We substantially reduce the number of calls between the
graphs by verifying that a pair has the same number of nodes and edges before the call.
The metrics are presented in Table 2 for top-10 data sets and in Table 6 (see the appendix) for all data
sets. The graphs in Table 2 are sorted by the proportion of isomorphic graphs I%. The results for the
first Top-10 data sets are somewhat surprising: almost all graphs in the selected data sets have other
isomorphic graphs. If we look at all data sets in Table 6, we see that the proportion of isomorphic
graphs in the data sets varies from 100% to 0%. However, more than 80% of the analyzed data sets
have at least 10% of the graphs in a non-trivial orbit.
Table 2: Isomorphic metrics for Top-10 data sets based on the proportion of isomorphic graphs I%.
IP% is the proportion of isomorphic pairs of graphs, Mismatched % is the proportion of mismatched
labels.
data set	Size, N	Num. orbits	Iso. graphs, I	I %	IP %	Mismatched %
SYNTHETIC=	300	2	二	300	=	100	100	100	=
Cuneiform	"^67	1	2677	100-	20.46	100
Letter-low	"^250-	F	"^245	99.78	8.72	-96.22
DHFRJMD-	193	F	192	99.75	6.87	-94.91
COIL-RAG-	1900-	"20	^3890	99.74	25.22	-99.31
COX2JMD-	103	T3	101	99.34	11.83	-98.68
ERJMD 一	^446	ʒɪ	^442	99.1	5.57	-8Σ74
Fingerprint	"^800-	^69	"^774	99.07	16.86	-89.29
BZRJMD 一	106	F	103	99.02	7.16	-95.75
Letter-med	2250	39	-	2226	—	98.93	8.05	92.93	—
Another surprising observation is that the proportion of mismatched graphs is significant, ranging
from 100% to 0%. This clearly indicates that such graphs are not suitable for graph classification
4
Under review as a conference paper at ICLR 2020
and require additional information to distinguish the models. We analyze the reasons for this in the
next section.
Also, the distribution of orbit sizes can vary significantly across the data sets. In Figure 1 we plot a
distribution of orbit sizes for several examples of data sets (and distributions for other data sets can
be found in Appendix C). For example, for IMDB-BINARY data set the number of orbits of small
sizes, e.g. two or three, goes to 100, which indicate prevalence of pairs of isomorphic graphs that are
non-isomorphic to the rest. However, for Letter-med data set there are many orbits of sizes more than
100, while small orbits are not that common. In this case, the graphs in this data set are equivalent
to a lot of other graphs, which may have a substantial effect on the corresponding metrics. While
the orbit distribution changes from one data set to another, it is clear that in many situations there
are isomorphic graphs that can affect training procedure by effectively increasing the weights for the
corresponding graphs, change performance on the test by validating on the already seen instances,
and by confusing the model by utilizing different target labels for topologically-equivalent graphs.
We analyze the reasons for it further.
Figure 1: Examples of distributions of orbit sizes without considering labels.
Orbit size
5	Explaining isomorphism
Meta-information about graphs. In addition to the topology of a graph, many data sets also in-
clude meta information about nodes and/or edges. Out of 54 analyzed data sets there are 40 that
additionally include node features and 25 that include edge features. For example, in Synthetic
data set all graphs are topologically identical but the nodes are endowed with normally distributed
scalar attributes and in DHFR_MD edges are attributed with distances and labeled according to a
chemical bond type. Alternatively, some graphs can have parallel edges which is equivalent to have
a corresponding weight on the edges. Thus some data sets include node/edge categorical features
(labels) and numerical features (attributes), which leads to better distinction between the graphs and
therefore their corresponding labels.
To see this, we rerun our previous analysis but now include the node labels, if any, when comput-
ing isomorphism between graphs. Consider a tuple (G, l), where G is a graph and l : V (G) 7→
{1, 2, . . . , k} is a k-labeling of G. In this case of node label-preserving graph isomorphism from
graph (G1, l1) to graph (G2, l2) we seek an isomorphism function φ : V (G1) 7→ V (G2) such that
l1(v) = l2(φ(v)).
Tables 3 and 7 (see the appendix) show the number of isomorphic graphs after considering node
labels. While for the first six data sets the proportion of isomorphic graphs has not changed much,
it is clearly the case for the remaining data sets. In particular, almost 90% of the analyzed data sets
include less than 20% of isomorphic graphs. Also, the number of mismatched graphs significantly
decreases after considering node labels. For example, for MUTAG data set the proportion of iso-
morphic graphs went down from 42.02% to 19.15% and the proportion of mismatched graphs from
6.91% to 0%.
5
Under review as a conference paper at ICLR 2020
Table 3: Isomorphic metrics with node labels for Top-10 data sets based on the proportion of iso-
morphic graphs I%. IP% is the proportion of isomorphic pairs of graphs, Mismatched % is the
proportion of mismatched labels.
data set	Size, N	Num. orbits	Iso. graphs, I	I %	IP %	Mismatched %
SYNTHETIC	300	2	二	300	=	100	100	100	=
Cuneiform	"^67	-8	"^67	T00-	20.46	T00
DHFRJMD-	193		192	99.75	6.87	^9491
COX2JMD-	103	^13	101	99.34	11.83	-98.68
ERJMD	^446	^31	^442	99.1	5.57	-82.74
BZRJMD	106	F	103	99.02	7.16	^9575
MUIAG	188	F	16	19.15	0.14	^0
PTC-FM	149	F	^34	15.47	0.08	TO9
PTCJMM	^336	F	^30	14.88	0.07	~T7
DHFR	756	39	-	98	—	12.96	0.04	3.97	—
Likewise, the orbit size distribution also changes significantly after considering node labels. Figure 2
shows a changed distribution of orbits with and without considering node labels. For majority of data
sets large orbits vanish and the number of small orbits is substantially decreased in label-preserving
graph isomorphism setting. This indicates one of the reasons for presence of many isomorphic
graphs in the data sets, which implies that including node/edge labels/attributes can be important for
graph classification models.
Figure 2: Examples of distributions of orbit sizes with node labels.
Sizes of the data sets. Another reason for having isomorphism in a data set is the sizes of graphs,
which could be too small on average to lead to a diversity in a data set. In general, the number of
non-isomorphic graphs with n vertices and m edges can be computed using Polya enumeration the-
ory and grows very fast. For example, for a graph with 15 nodes and 15 edges, there are 2,632,420
non-isomorphic graphs. Nevertheless, specifics of the origin of the data set may affect possible con-
figurations that graphs have (e.g. structure of chemical compounds in COX2_MD or ego-networks
for actors in IMDB-BINARY) and thus smaller graphs may tend to be close to isomorphic structures.
On the other hand, all five data sets with the average number of nodes greater than 100 have very
low or zero proportion of isomorphic graphs. Hence, the average size of the graphs directly impacts
the possible structure of the data set and thus data sets with larger graphs tend to be more diverse.
We next analyze the consequences of the isomorphic graphs on classification methods.
6	Isomorphic graphs and weighted classification
We denote by
6
Under review as a conference paper at ICLR 2020
•	F ⊆ Y X a class of binary classifiers with an input space X and an output space Y =
{-1, +1},
•	P a distribution on X × Y ,
•	π a prior probability of a positive class, i.e. P = πPχ∣y= + 1 + (I - π)Pχ∣y=-1,
•	D = {(xi, yi)}iN=1 a training sample, xi ∈ X, yi ∈ Y . In our case xi ∈ D are graphs Gi,
see notations in section 4.
We consider a zero-one loss function l(y,y) = Iy=y. Let Us assume that classifiers from F can detect
which graphs in D are isomorphic. E.g. classifiers based on Weisfeiler-Lehman graph kernels (see
Shervashidze et al. (2011a)) are capable to do it for majority of graphs. In such case the empirical
risk EDl(f (x),y) = ~N PN=I l(f (xj),yj) reduces to N Pj∈j ujl(f (Xj), yj), where J is an index
set of non-isomorphic graphs, uj ≥ 1 is equal to the number of graphs in the initial sample D,
isomorphic to the graph xj (we count xj as well).
Thus under such assumptions the graph classification problem with the training data set, contain-
ing isomorphic graphs, can be interpreted as a classification problem with a weighted loss. Let us
introduce general notations for this problem. We define some (fixed) measurable weighting func-
tion u : (X × Y ) → (0, +∞). Then the theoretical risk is equal to EPl(f (x), y) and the weighted
empirical risk is equal to EDu(x, y)l(f (x), y) = N PN=I u(xi, yi)l(f (Xi), yi). We would like to
derive an upper bound for the excess risk supf∈F (EPl(f (x), y) - EDu(x, y)l(f (x), y)), i.e. we
would like to quantify an upper bound for a generalization gap. We optimize the weighted empirical
risk when training a classifier and measure its accuracy using non-weighted theoretical risk.
There are some results about classification performance with a weighted loss. E.g. in (Dupret &
Koda, 2001) a bayesian framework for imbalanced classification with a weighted risk is proposed.
Scott (2012) investigated the calibration of asymmetric surrogate losses. Natarajan et al. (2018) con-
sidered the case of cost-sensitive learning with noisy labels. However, to the best of our knowledge,
there is no studied upper bound for the excess risk with explicit dependence on the class imbalance
and the weighting scheme that quantifies the influence on the overall classification performance. We
show this result next.
To derive explicit expressions we use some additional modeling asumption, namely, we consider
u(X, y) = (1 + g+ (w))I{y=+1} + (1 + g-(w)) I{y=-1} for some non-negative weighting functions
g+ (w) and g- (w) of the weight value w ≥ 0. E.g. we can use g+(w) = w and g- (w) = 1/w.
Theorem 6.1. With probability 1 一 δ, δ > 0 for D 〜PN the excess risk is upper bounded by
sup EPl(f(X), y)-EDu(X, y)l(f(X), y) ≤ 3 (g+ (w)π + g-(w)(1 - π)) +
f∈F
+ RN(F) + (2 + g+(w) + g- (W)) P(log δ-1 )∕(2N),	(1)
where RN(F) is a Rademacher complexity of the function class F.
Let us note that the Rademacher complexity of the function class F, defined by a graph kernel, was
studied e.g. in Oneto et al. (2018); Oneto et al. (2017a).
From equation 1 it follows that by tuning the weight parameter w we can make the upper bound
tighter, namely collecting the terms with w in the RHS of equation 1 we solve
g+ (w)(3n + p(log δ-1)∕(2N)) + g-(w)(3(1 — π) + p(log δ-1)∕(2N)) → mwn .
In case we set g+(w) = w and g- (w)
1∕w, the optimal weight Wopt = ʌ/3(IiloNN
where αN
J IOgN 1 ≈ 0 for N》1. For such optimal Wopt the RHS of equation 1 has the form
6pπ(1 - π) + RN(F) + αN(2+ [π(1 — π)]-1/2).
Thus we get theoretical evidence on how the weighting influences the classification accuracy: e.g.
in the imbalanced case (when π≈ 0 or π≈ 1) selecting the weight optimally we reduce the gener-
alization gap almost to zero for N 1; at the same time, not optimal weight can lead to overfitting.
7
Under review as a conference paper at ICLR 2020
As we already discussed, under some mild modeling assumptions the graph classification problem
with isomorphic graphs in the training data set can be interpreted as the classification problem with
a weighted loss. Therefore the obtained estimate provides additional evidence on a negative effect of
the isomorphic graphs when solving the graph classification problems: the presence of isomorphic
graphs in the training data set could have the same negative effect as not optimal weight value for
the classification with a weighted loss function.
7	Influence of isomorphism bias: empirical results
To understand the impact of isomorphic graphs in the data set on the final metric we consider sepa-
rately the results on two subparts of the data set. In particular, let Ytrain and Ytest be train and test
splits of a data set. Let Hi ∈ Ytest be a graph such that there exists an isomorphic graph Gi ∈ Ytrain
in the train data set. Let {Yiso} be a set of all such graphs Hi for which there exists an isomorphic
graph Gi in Ytrain. Note that the graphs in {Yiso} are not necessarily isomorphic. We denote by
Ynew the test graphs that do not have isomorphic copies in the train data set, i.e. Ynew = Ytest \Yiso.
If we want to test generalization of classification models, we need to test it on new instances of the
data sets and therefore at least consider Ynew instead of Ytest . One question regarding the perfor-
mance of the models on this new test set Ynew is whether the performance on it will be lower than on
the original test set Ytest. As we show below answer to this question solely depends on the accuracy
of the model on isomorphic instances Yiso .
Consider a graph classification model that is evaluated on normalized accuracy over a data set Y :
P acc(Gi)
acc(Y) = G∈r∣Y∣—,	⑵
where acc(G) equals to one if the model predicts the label of Gi correctly, and zero otherwise. If
|Y| = 0, then we consider acc(Y) = 0. We can see that the accuracy on the test data set can be
written as the sum of two terms:
acc(Ytest)
acc(G)
G∈Ytest
-∣γtestι-
acc(G) +	acc(G)
G∈Yn
ew	G∈Yiso_________
∣γtestι
“acc(Ynew) + 卧 acc(Yiso).
∣Yt
est ∣	∣Ytest ∣
(3)
Equation 3 decomposes accuracy on the original data set as the weighted sum of two accuracies on
the set of the new test instances Ynew and a set of the instances Yiso already appeared in the train
set and therefore available to the model. We call the term acc(Yiso) as isomorphism bias, which
corresponds to the accuracy of the model on the isomorphic test instances. As we will see next, the
accuracy of the model on the new set Ynew will be less if only if the model performs better on the
isomorphic set Yiso .1
Claim 7.1. Let YteSt = Ynew ∪ YiSo Ynew ∩ YiSO = 0 and YiSO = 0, where YiSO ⊂ 匕r®. Then
for any classification model accuracy on the new test instances Ynew is smaller than on the test set
YteSt if and only if it is smaller than accuracy on the isomorphic test instances YiSO, i.e.:
acc(YteSt) > acc(Ynew) ^⇒ acc(YiSo) > acc(Ynew)	(4)
The equation 4 gives a definite answer with the possible performance of the model on a new test set.
If the model performs well on isomorphic instances YiSO, then it will falsely increase performance
on YteSt in comparison to Ynew . Conversely, if the model performs poorly on the instances that
appeared in the training set, then removing them from the test set and evaluating the model purely
on Ynew will demonstrate higher accuracy. There are two reasons for the model to misclassify
isomorphic instances YiSO : (i) the instances contain target labels that are different than those that
it has seen, as we show in Table 2 the percentage of mismatched labels can be high in some data
1We provide the proof of Claim 7.1 and Claim 7.2 in Appendix E and F.
8
Under review as a conference paper at ICLR 2020
Table 4: Mean classification accuracy for test sets Ytest and Yiso (in brackets) in 10-fold cross-
validation. Top-1 result in bold.
	MUTAG	IMDB-B	IMDB-M	COX2	AIDS	PROTEINS
^N	0.829 (0.840)	0.737 (0.733)	0.501 (0.488)	0.82 (0.872)	0.996 (0.998)	0.737 (0.834)
NN-PH	0.867 (1.000)	0.756 (1.000)	0.522 (1.000)	0.838 (1.000)	0.996 (1.000)	0.742(1.000)
NN-P-	0.856 (0.847)	0.737 (0.731)	0.499 (0.486)	0.795 (0.83)	0.996 (0.999)	0.729 (0.709)
"WL	0.862 (0.867)	0.734 (0.990)	0.502(0.953)	0.800 (0.974)	0.993 (0.999Γ	0.747 (0.950)
WL-PH	0.907 (1.000)	0.736(1.000)	0.504(1.000)	0.810(1.000)	0.994(1.000)	0.749 (1.000)
WL-P-	0.870 (0.838)	0.724 (0.715)	0.495 (0.487)	0.794 (0.844)	0.994(0.999)	0.740 (0.742)
N	0.836 (0.902)	0.707 (0.820)	0.503 (0.732)	0.781 (0.966)	0.994(0.997Γ	0.726 (0.946)
V-PH-	0.859 (1.000)	0.750(1.000)	0.517(1.000)	0.794(1.000)	0.996 (1.000)	0.729 (1.000)
V-P	0.827 (0.844)-	0.724 (0.728)~	0.496 (0.481T-	0.768 (0.852~	0.996 (0.999)-	0.719 (0.741T
We define a peering model Mc such that for each G
accuracy of the model Mc is at least as the accuracy
sets; or (ii) the model is not expressive enough to map the structure of the graphs to the target label
correctly.
Crucially, while Ynew tests generalization capabilities of the models, on Yiso the models can explic-
itly or implicitly memorize the right labels from the training. We describe a model-agnostic way to
guarantee increase of classification performance if |Yiso | 6= 0.
Let G = G SUch that G ∈ Yiso and G ∈ 匕rain. Note that there can be multiple isomorphic graphs
{Gi } ⊂ Ytrain . If for any G ∈ Yiso all target labels of the orbit of G are the same we call the set
Yiso as homogeneous. Consider a classification model M that maps each graph G to its label l(G).
∈ Yiso it outputs the target label l(G). Then the
of the original model M.
Claim 7.2. Let Ytest = Ynew ∪ Yiso, Ynew ∩ Yiso = 0. If Yiso is homogeneous, then the accuracy
on Ytest of a classification model M is at most as the accuracy of its peering model M, i.e.:
accM (Ytest) ≤ accMc(Ytest).
Claim 7.2 establishes a way to increase performance only for homogeneous Yiso. If there are noisy
labels in the training set and hence the set is not homogeneous, the model cannot guarantee the right
target label for these instances. Nonetheless, one can select a heuristic such as majority vote among
the training isomorphic instances to select a proper label at the testing time.
In experiments, we compare neural network model (NN) (Xu et al., 2018) with graph kernels,
Weisfeiler-Lehman (WL) (Shervashidze et al., 2011b) and vertex histogram (V) (Sugiyama & Borg-
wardt, 2015). For each model we consider two modifications: one for peering model on homoge-
neous Yiso (e.g. NN-PH) and one for peering model on all Yiso (e.g. NN-P). We show accuracy on
Ytest and on Yiso (in brackets) in Table 4. Experimentation details can be found in Appendix G.
From Table 4 we can conclude that peering model on homogeneous data is always the top performer.
This is aligned with the result of Claim 7.2, which guarantees that acc(Yiso) = 1, but it is an
interesting observation if we compare it to the peering model on all isomorphic instances Yiso (-P
models). Moreover, the latter model often loses even to the original model, where no information
from the train set is explicitly taken into the test set. This can be explained by the noisy target
labels in the orbits of isomorphic graphs, as can be seen both from the statistics for these datasets
(Table 6) and accuracy measured just on isomorphic instances Yiso . These results show that due to
the presence of isomorphism bias performance of any classification model can be overestimated by
as much as 5% of accuracy on these datasets and hence future comparison of classification models
should be estimated on Ynew instead. These observations conforms with our theoretical findings and
conclusions in Section 6.
7.1	General recommendations
In order to avoid measuring performance over the wrong test sets, we provide a set of recommenda-
tions that will guarantee measuring the right metrics for the models.
9
Under review as a conference paper at ICLR 2020
•	We open-source new, ”clean” data sets that do not include isomorphic instances that are
in Table 8. To tackle this problem in the future, we propose to use clean versions of the
data set for which isomorphism bias vanishes. For each data set we consider the found
graph orbits and keep only one graph from each orbit if and only if the graphs in the orbit
have the same label. If the orbit contains more than one label, a classification model can
do little to predict a correct label at the inference time and hence we remove such orbit
completely. In this case, for a new data set Yiso = 0 and hence it prevents the models
to implicitly memorize the labels from the training set. We consider the data set orbits
that do not account for neither node nor edge labels because the remaining graphs are not
isomorphic based purely on graph topology.
•	Incorporating node and edge features into the models may be necessary to distinguish the
graphs. As we have seen, just using node labels can reduce the number of isomorphic
graphs significantly and many data sets provide additional information to distinguish the
models at full scope.
•	Verification of the models on bigger graphs in general is more challenging due to the sheer
number of non-isomorphic graphs. For example, data sets related to REDDIT or DD in-
clude a number of big graphs for classification.
8	Conclusion
In this work we study isomorphism bias of the classification models in graph structured data that
originates from substantial amount of isomorphic graphs in the data sets. We analyzed 54 graph
data sets and provide the reasons for it as well as a set of rules to avoid unfair comparison of the
models. We theoretically characterized the influence of isomorphism bias on the graph classification
performance by providing an upper bound on the generalization gap. We showed that in the current
data sets any model can memorize the correct answers from the training set and we open-source new
clean data sets where such problems do not appear.
References
Laszlo Babai. Graph isomorphism in quasipolynomial time. CoRR, abs/1512.03547, 2015. URL
http://arxiv.org/abs/1512.03547.
Bjorn Barz and Joachim Denzler. Do we train on test data? purging cifar of near-dUPlicates. arXiv
preprint arXiv:1902.00423, 2019.
Vighnesh Birodkar, Hossein Mobahi, and Samy Bengio. Semantic redundancies in image-
classification datasets: The 10% you don’t need. arXiv preprint arXiv:1901.11409, 2019.
Karsten M Borgwardt, Cheng Soon Ong, Stefan Schonauer, SVN Vishwanathan, Alex J Smola, and
Hans-Peter Kriegel. Protein function prediction via graph kernels. Bioinformatics, 21(SUPPL1):
i47-i56, 2005.
Jin-Yi Cai, Martin Furer, and Neil Immerman. An optimal lower bound on the number of variables
for graph identification. Combinatorica, 12(4):38§T10,1992a.
Jin-yi Cai, Martin Furer, and Neil Immerman. An optimal lower bound on the number of variables
for graph identifications. Combinatorica, 1992b.
Tox21 Data Challenge. Tox21 data challenge 2014, 2014. URL https://tripod.nih.gov/
tox21/challenge/data.jsp.
Georges Dupret and Masato Koda. Bootstrap re-sampling for unbalanced data in supervised learn-
ing. European Journal ofOperational Research, 134(1):141 一 156, 2001.
Aasa Feragen, Niklas Kasenburg, Jens Petersen, Marleen de Bruijne, and Karsten Borgwardt. Scal-
able kernels for graphs with continuous attributes. In Advances in Neural Information Processing
Systems,pp. 216-224, 2013.
10
Under review as a conference paper at ICLR 2020
Gregoire Ferre, Terry Haut, and KiPton Barros. Learning molecular energies using localized graph
kernels. The Journal of chemical physics, 146(11):114107, 2017.
Matthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric. In
ICLR Workshop on Representation Learning on Graphs and Manifolds, 2019.
Martin Furer. On the combinatorial power of the Weisfeiler-Iehman algorithm. In International
Conference on Algorithms and Complexity, pp. 260-271. Springer, 2017.
Thomas Gartner, Peter Flach, and Stefan Wrobel. On graph kernels: Hardness results and efficient
alternatives. In Learning theory and kernel machines, pp. 129-143. Springer, 2003.
Goran GlavaS and Jan Snajder. Event-centered information retrieval using kernels on event graphs.
In Proceedings of TextGraphs-8 Graph-based Methods for Natural Language Processing, pp. 1-5,
2013.
J. E. Hopcroft and J. K. Wong. Linear time algorithm for isomorphism of planar graphs (preliminary
report). In Proceedings ofthe Sixth Annual ACM Symposium on Theory of Computing, STOC ’74,
1974.
Sergey Ivanov and Evgeny Burnaev. Anonymous walk embeddings. In Proceedings of the 35th
International Conference on Machine Learning (ICML), 2018.
Biao Jie, Mingxia Liu, Xi Jiang, and Daoqiang Zhang. Sub-network based kernels for brain net-
work classification. In Proceedings of the 7th ACM International Conference on Bioinformatics,
Computational Biology, and Health Informatics, pp. 622-629. ACM, 2016.
Fredrik Johansson, Vinay Jethava, Devdatt Dubhashi, and Chiranjib Bhattacharyya. Global graph
kernels using geometric embeddings. In Proceedings of the 31st International Conference on
Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014, 2014.
Paul J. Kelly. A congruence theorem for trees. Pacific J. Math., 7(1):961-968, 1957. URL https:
//projecteuclid.org:443/euclid.pjm/1103043674.
Kristian Kersting, Nils M. Kriege, Christopher Morris, Petra Mutzel, and Marion Neumann. Bench-
mark data sets for graph kernels, 2016. URL http://graphkernels.cs.tu-dortmund.
de.
Nils Kriege and Petra Mutzel. Subgraph matching kernels for attributed graphs. arXiv preprint
arXiv:1206.6483, 2012.
Nils M Kriege, Matthias Fey, Denis Fisseler, Petra Mutzel, and Frank Weichert. Recognizing
cuneiform signs using graph based methods. arXiv preprint arXiv:1802.05908, 2018a.
Nils M Kriege, Christopher Morris, Anja Rey, and Christian Sohler. A property testing framework
for the theoretical expressivity of graph kernels. In IJCAI, pp. 2348-2354, 2018b.
Nils M Kriege, Fredrik D Johansson, and Christopher Morris. A survey on graph kernels. arXiv
preprint arXiv:1903.11835, 2019.
Kousik Kundu, Fabrizio Costa, and Rolf Backofen. A graph kernel approach for alignment-free
domain-peptide interaction prediction with an application to human sh3 domains. Bioinformatics,
29(13):i335-i343, 2013.
Wenchao Li, Hassen Saidi, Huascar Sanchez, Martin Schaf, and Pascal Schweitzer. Detecting sim-
ilar programs via the weisfeiler-leman graph kernel. In International Conference on Software
Reuse, pp. 315-330. Springer, 2016.
Eugene M. Luks. Isomorphism of graphs of bounded valence can be tested in polynomial time.
In Proceedings of the 21st Annual Symposium on Foundations of Computer Science, SFCS ’80,
1980.
Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. Provably powerful graph
networks. arXiv preprint arXiv:1905.11136, 2019.
11
Under review as a conference paper at ICLR 2020
Brendan D. Mckay and Adolfo Piperno. Practical graph isomorphism, ii. J. Symb. Comput., 2014.
Silvio Micali and Zeyuan Allen Zhu. Reconstructing markov processes from independent and
anonymous experiments. Discrete Applied Mathematics, 200:108-122, 2016.
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of Machine Learning.
The MIT Press, 2012. ISBN 026201825X, 9780262018258.
Christopher Morris, Nils M Kriege, Kristian Kersting, and Petra Mutzel. Faster kernels for graphs
with continuous attributes via hashing. In 2016 IEEE 16th International Conference on Data
Mining (ICDM), pp. 1095-1100. IEEE, 2016.
Christopher Morris, Martin Ritzert, Matthias Fey, William L Hamilton, Jan Eric Lenssen, Gaurav
Rattan, and Martin Grohe. Weisfeiler and leman go neural: Higher-order graph neural networks.
In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 4602-4609,
2019.
Nagarajan Natarajan, Inderjit S. Dhillon, Pradeep Ravikumar, and Ambuj Tewari. Cost-sensitive
learning with noisy labels. Journal of Machine Learning Research, 18(155):1-33, 2018.
Marion Neumann, Roman Garnett, Christian Bauckhage, and Kristian Kersting. Propagation ker-
nels: efficient graph kernels from propagated information. Machine Learning, 102(2):209-245,
2016.
Giannis Nikolentzos, Giannis Siglidis, and Michalis Vazirgiannis. Graph kernels: A survey. arXiv
preprint arXiv:1904.12218, 2019.
L. Oneto, N. Navarin, M. Donini, S. Ridella, A. Sperduti, F. Aiolli, and D. Anguita. Learning with
kernels: A local rademacher complexity-based analysis with application to graph kernels. IEEE
Transactions on Neural Networks and Learning Systems, 29(10):4660-4671, 2018.
Luca Oneto, Nicol Navarin, Michele Donini, Alessandro Sperduti, Fabio Aiolli, and Davide An-
guita. Measuring the expressivity of graph kernels through statistical learning theory. Neurocom-
put., 268(C):4-16, 2017a.
LUca Oneto, Nicolo Navarin, Michele Donini, Alessandro Sperduti, Fabio Aiolli, and Davide An-
guita. Measuring the expressivity of graph kernels through statistical learning theory. Neurocom-
puting, 268:4-16, 2017b.
Francesco Orsini, Paolo Frasconi, and Luc De Raedt. Graph invariant kernels. In Twenty-Fourth
International Joint Conference on Artificial Intelligence, 2015.
Shirui Pan. A repository of benchmark graph datasets for graph classification, 2018. URL https:
//github.com/shiruipan/graph_datasets.
Liva Ralaivola, Sanjay J Swamidass, Hiroto Saigo, and Pierre Baldi. Graph kernels for chemical
informatics. Neural networks, 18(8):1093-1110, 2005.
Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers
generalize to imagenet? arXiv preprint arXiv:1902.10811, 2019.
Kaspar Riesen and Horst Bunke. Iam graph database repository for graph based pattern recognition
and machine learning. In Joint IAPR International Workshops on Statistical Techniques in Pat-
tern Recognition (SPR) and Structural and Syntactic Pattern Recognition (SSPR), pp. 287-297.
Springer, 2008.
Matthias Rupp and Gisbert Schneider. Graph kernels for molecular similarity. Molecular Informat-
ics, 29(4):266-273, 2010.
Franco Scarselli, Ah Chung Tsoi, and Markus Hagenbuchner. The vapnik-chervonenkis dimension
of graph and recursive neural networks. Neural Networks, 108:248-259, 2018.
Clayton Scott. Calibrated asymmetric surrogate losses. Electron. J. Statist., 6:958-992, 2012.
12
Under review as a conference paper at ICLR 2020
Maksim Sharaev, Alexey Artemov, Ekaterina Kondrateva, Sergei Ivanov, Svetlana Sushchinskaya,
Alexander Bernstein, Andrzej Cichocki, and Evgeny Burnaev. Learning connectivity patterns via
graph kernels for fmri-based depression diagnostics. In 2018 IEEE International Conference on
Data Mining Workshops (ICDMW),pp. 308-314. IEEE, 2018.
Nino Shervashidze, S. V. N. Vishwanathan, Tobias Petri, Kurt Mehlhorn, and Karsten M. Borgwardt.
Efficient graphlet kernels for large graph comparison. In Proceedings of the Twelfth International
Conference on Artificial Intelligence and Statistics, AISTATS 2009, Clearwater Beach, Florida,
USA, April 16-18, 2009, pp. 488-495, 2009.
Nino Shervashidze, Pascal Schweitzer, Erik Jan van Leeuwen, Kurt Mehlhorn, and Karsten M Borg-
wardt. Weisfeiler-lehman graph kernels. Journal of Machine Learning Research, 12(Sep):2539-
2561, 2011a.
Nino Shervashidze, Pascal Schweitzer, Erik Jan van Leeuwen, Kurt Mehlhorn, and Karsten M.
Borgwardt. Weisfeiler-lehman graph kernels. Journal of Machine Learning Research, 12:2539-
2561, 2011b.
Elena Stumm, Christopher Mei, Simon Lacroix, Juan Nieto, Marco Hutter, and Roland Siegwart.
Robust visual place recognition with graph kernels. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 4535-4544, 2016.
Mahito Sugiyama and Karsten Borgwardt. Halting in random walk kernels. In Advances in neural
information processing systems, pp. 1639-1647, 2015.
Jeffrey J Sutherland, Lee A O’brien, and Donald F Weaver. Spline-fitting with a genetic algorithm:
A method for developing classification structure- activity relationships. Journal of chemical in-
formation and computer sciences, 43(6):1906-1915, 2003.
Saurabh Verma and Zhi-Li Zhang. Stability and generalization of graph convolutional neural net-
works. arXiv preprint arXiv:1905.01004, 2019.
S. V. N. Vishwanathan, Nicol N. Schraudolph, Risi Kondor, and Karsten M. Borgwardt. Graph
kernels. J. Mach. Learn. Res., 11:1201-1242, August 2010. ISSN 1532-4435.
Jianjia Wang, Richard C Wilson, and Edwin R Hancock. fmri activation network analysis using
bose-einstein entropy. In Joint IAPR International Workshops on Statistical Techniques in Pat-
tern Recognition (SPR) and Structural and Syntactic Pattern Recognition (SSPR), pp. 218-228.
Springer, 2016.
Harry Wiener. Correlation of heats of isomerization, and differences in heats of vaporization of
isomers, among the paraffin hydrocarbons. Journal of the American Chemical Society, 69(11):
2636-2638, 1947a.
Harry Wiener. Influence of interatomic forces on paraffin properties. The Journal of Chemical
Physics, 15(10):766-766, 1947b.
Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S Yu. A
comprehensive survey on graph neural networks. arXiv preprint arXiv:1901.00596, 2019.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? arXiv preprint arXiv:1810.00826, 2018.
Pinar Yanardag and SVN Vishwanathan. Deep graph kernels. In Proceedings of the 21th ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1365-1374.
ACM, 2015.
Rex Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, and Jure Leskovec. Gnn explainer: A
tool for post-hoc explanation of graph neural networks. arXiv preprint arXiv:1903.03894, 2019.
13
Under review as a conference paper at ICLR 2020
A Statistics for original data sets
Table 5: All original graph data sets. N is the number of graphs, C is the number of different classes.
Avg. Nodes and Avg. Edges is the average number of nodes and edges. N.L. and E.L indicate if the
graphs in a data set contain node or edge labels.
data set	Type	N	C	Avg. Nodes	Avg. Edges	N.L.	E.L.	Source
FIRSTMM_DB	Molecular	^4I	TT-	1377.27	3074.1	^+	-	Neumann et al. (2016)
OHSU	Molecular	^T9	1-	ɪuɪ	199.66	^+	-	Pan (2018)
^KKI	Molecular	^^3	1-	26.96	48.42	^+	-	Pan (2018)
Peking」	Molecular	^^5	1-	-39：31	77.35	^+	-	Pan (2018)
MUIAG	Molecular	T88-	1-	17.93	T9.79	^+	^+	Kriege&Mutzel(2012)
MSRC21C	Vision	^^09-	"^0^^	40.28	^966	^+	-	Neumann et al. (2016)
MSRC9	Vision	^^2l		40.58	-97.94	^+	-	Neumann et al. (2016)
Cuneiform	Molecular	^^67-	-30^^	21.27	-44.8	^+	^+	Kriege et al. (2018a)
SYNTHETIC	Synthetic	^300-	1-	T00	T96	-	-	Feragen et al. (2013)
COX2JMD	Molecular	^303-	1-	26.28	335.12	^+	^+	Kriege&Mutzel (2012)
BZRjMD	Molecular	^306-	1-	ɪ3	225.06	^+	^+	Kriege&Mutzel (2012)
PTCjMM	Molecular	^336-	1-	13.97	14.32	^+	^+	Kriege&Mutzel (2012)
PTCjMR	Molecular	^^44-	22-	14.29	14:69	^+	^+	Kriege&Mutzel (2012)
PTC-FM	Molecular	^349-	1-	14711	14.48	^+	^+	Kriege&Mutzel (2012)
PTC-FR	Molecular	^35I-	1-	14:56	T5	^+	^+	Kriege&Mutzel (2012)
DHFRJMD	Molecular	^393-		23.87	283.01	^+	^+	Kriege&Mutzel (2012)
Synthie	Synthetic	^400-	^4-	^95	172.93	-	-	Morris et al.(2016)
^BZR	Molecular	^405-	1-	35.75	38.36	^+	-	Sutherland et al. (2003)
ERjMD	Molecular	^446-	1-	21.33	234.85	^+	^+	Kriege&Mutzel(2012)
COX2	Molecular	^467-	1-	41.22	43.45	^+	-	Sutherland et al. (2003)
DHFR	Molecular	^467-	1-	42.43	44.54	^+	-	Sutherland et al. (2003)
MSRC21	Vision	^363-	"^0^^	77.52	198.32	^+	-	Neumann et al. (2016)
ENZYMES	Molecular	^600-	66-	32.63	62.14	^+	-	Borgwardt et al. (2005)
IMDB-BINARY	Social	1000	1-	19.77	-96.53	-	-	Yanardag & Vishwanathan (2015)
PROTEINS	Molecular	1113	1-	39.06	72.82	^+	-	Borgwardt et al. (2005)
^DD	Molecular	1178	22-	284.32	715.66	^+	-	Shervashidze et al. (2011a)
IMDB-MULTI	Social	1500	1-	T3	-65.94	-	-	Yanardag & Vishwanathan (2015)
AIDS	Molecular	2000	1-	15769	1672	^+	^+	Riesen & Bunke (2008)
REDDIT-BINARY	Social	2000	1-	429.63	497.75	-	-	Yanardag & Vishwanathan (2015)
Letter-high	Molecular	2250	15-	-4.67	-4.5	-	-	Riesen & Bunke (2008)
Letter-low	Molecular	2250	15-	-4.68	^3ΓΓ3	-	-	Riesen & Bunke (2008)
Letter-med	Molecular	2250	15-	-4.67	-4.5	-	-	Riesen & Bunke (2008)
Fingerprint	Molecular	2800	^4-	^3T42	-4.42	-	-	Riesen & Bunke (2008)
COIL-DEL	Molecular	3900	100^"	21.54	54.24	-	^+	Riesen & Bunke (2008)
COIL-RAG	Molecular	3900	100^"	^3T0l	^3T02	-	-	Riesen & Bunke (2008)
^NC∏	Molecular	4110	1-	29.87	ɪ3	^+	-	Shervashidze et al. (2011a)
NCI109	Molecular	4127	1-	29.68	^32TΓ3	^+	-	Shervashidze et al. (2011a)
FRANKENSTEIN	Molecular	4337	1-	ɪ9	17.88	-	-	Orsini et al. (2015)
Mutagenicity	Molecular	4337	22-	30.32	^30^	^+	^+	Riesen & Bunke (2008)
REDDIT-MULTI-5K	Social	4999	-3-	508.52	594.87	-	-	Yanardag & Vishwanathan (2015)
COLLAB	Social	5000	1-	74.49	2457.78	-	-	Yanardag & Vishwanathan (2015)
Tox21 工RE	Molecular	7167	1-	16.28	16752	^+	^+	Challenge (2014)
Tox21_aromatase	Molecular	7226	1-	ɪʒ	17.79	^+	^+	Challenge (2014)
Tox21JMMP	Molecular	7320	1-	17.49	17783	^+	^+	Challenge (2014)
Tox21_ER	Molecular	7697	1-	17.58	17.94	^+	^+	Challenge (2014)
Tox21_HSE	Molecular	8150	1-	16.72	17.04	^+	^+	Challenge (2014)
Tox21 工HR	Molecular	8169	1-	18.09	T875	^+	^+	Challenge (2014)
Tox21_PPAR-gamma	Molecular	8184	1-	17.23	17.55	^+	^+	Challenge (2014)
Tox21 工R-LBD	Molecular	8599	1-	17.77	18.16	^+	^+	Challenge (2014)
Tox2Lp53	Molecular	8634	1-	17.79	18.19	^+	^+	Challenge (2014)
Tox21_ER.LBD	Molecular	8753	1-	18.06	T8.47	^+	^+	Challenge (2014)
Tox21 工TAD5	Molecular	9091	22-	17.89	^Γ8^	^+	^+	Challenge (2014)
Tox21 工R	Molecular	9362		18.39	18.84	^+	^+	Challenge(2014)
REDDIT-MULTI-12K	Social	11929	^n~~	391.41	—	456.89	一	-	-	Yanardag & Vishwanathan (2015)
14
Under review as a conference paper at ICLR 2020
B Isomorphism metrics for all data sets
Table 6: Isomorphic metrics for all data sets. Sorting is based on the proportion of isomorphic
graphs I%. Num. orbits is the number of non-trivial orbits. IP % is the proportion of isomorphic
pairs of graphs, Mismatched % is the proportion of mismatched labels.
data set	Size, N	Num. orbits	Iso. graphs, I	I %	IP %	Mismatched %
SYNTHETIC	300	2	二	300	=	100	100	100	=
Cuneiform	"^67		"^67	100-	20.46	100
Letter-low	"^250	^32	"^245	99.78	8.72	96.22
DHFRzD	-393	^^5	-392	99.75	^687-	-94.91
COIL-RAG	-3900	^^0	-3890	99.74	25.22	-99.31
COX2_MD	-303	13	-301	99.34	11.83	98.68
ER_MD	^446	ʒɪ	^442	^99T1-	5.57	82.74
Fingerprint	"^800	^^69	"^774	99.07	16.86	89.29
BZR_MD	-306	F	-303	99.02	7.16	-95.75
Letter-med	"^250	^39	"^226	98.93	^05-	92.93
Letter-high	"^250	^^94	"^200	97.78	~67i-	-95.91
IMDB-MULTI	1500	100	1212	10.8-	^639	74.67
Tox214TAD5	^909i	1461	^6167	67.84	^009-	^^9T15
Tox21 .PPAR-gamma	"^Γ84	1265	-3513	67.36	^^0∏	~T77
Tox214R	^9362	15Γ9	^6295	67.24	-0:08-	
Tox2Lp53	1634	1345	-3800	67.18	^009-	11.28
Tox214R-LBD	"^599	1354	-3766	67.05	^009-	^688
Tox21NMP	-7320	T138	^4875	66.6	^^0∏	18.76
Tox21_HSE	"^Γ50	1218	1425	66.56	^^0∏	18.02
Tox21_ER_LBD	"^753	1375	-3791	66.16	-0:09-	12:41
Tox21_ER	-7697	1203	-3078	65.97	^009-	27.32
Tox214HR	"^169	1299	-3377	65.82	^009-	15:61
Tox21 .aromatase	-7226	1084	^4727	65.42	^^0∏	-3.07
Tox214RE	-7167	1047	^4682	65.33	ɪn-	26.45
AIDS	"^000	^371	1259	62.95	^^0Γ13-	^035
COX2	^467	^T6	"^83	60.6	^06	20.56
IMDB-BINARY	1000	T17	-379	17.9-	^067-	ʒɪl
FRANKENSTEIN	^4337	-374	"^230	51.42	0.09	30.87
MUTAG	188	^31	F	42.02	^049-	^691
^ZR	^405	^43	165	40.74	^06-	1.89
PTC_MM	-336	^42	132	39.29	0.46	^21
PTC_MR	-344	^40	125	36.34	^041-	^^5
PTC_FM	-349	^39	124	35.53	^039-	ɪʒ
DHFR	-756	"^9	"^50	33.07	0.14	^^9713
PTCFR	-351	^36	T16	33.05	^037-	^51
Mutagenicity	^4337	^397	1274	29.38	^003-	ɪi
COLLAB	-3000	158	1077	21.54	ɪn-	^668
COIL-DEL	-3900	155	-796	20.41	0.06	18.56
PROTEINS	T113	^35	151	13.57	^^0∏	^907
^Cn	^4∏0	^^25	-323	12.73		F
NCI109	^4127	^^22	-319	12.58		F
ENZYMES	^600	^6	10	1.67-	^0	^0
REDDIT-BINARY	"^000	^3	^4	^02-	^0	^0
REDDIT-MULTI-12K	11929-		17	0.14	^0	^004
FIRSTMM-DB	^41		-0	-0	^0	^0
OHSU	-79		-0	-0	^0	^0
TKI	"^3		-0	-0	^0	^0
Peking」	"^5		-0	-0	^0	^0
MSRCNIC	"^09		-0	-0	^0	^0
MSRC_9	"^21		-0	-0	^0	^0
Synthie	^400		-0	-0	^0	^0
MSRCNI	-363		-0	-0	^0	^0
^D	T178		-0	-0	^0	^0
REDDIT-MULTI-5K	4999	1	―	0	-	0	0	0	—
15
Under review as a conference paper at ICLR 2020
C Orbit size distribution for all data sets
In the plots 3, 4, 5 the sizes of orbits are presented for each data set. Empty plots correspond to data
sets with no isomorphic graphs. Plots with just wo-labels correspond to cases when there are no
node labels available for the graphs in a data set.
FIRSTMM DB	OHSU	KKI
Sl-qjo∙t-o JeqEnN
Sl-qjo∙t-o JeqEnN
o
Peking 1
■■ wo-labels
K node-labels
'∕∕ wo-labels
Sl node-labels
■■ wo-labels
node-labels
■■■ wo-labels
node-labels
■■ wo-labels
node-labels
MSRC 21C
MUTAG
■■I wo-labels
node-labels
・I wo-labels
node-labels
Cuneiform
SYNTHETIC
2.0
1.00
02
K] ■■ wo-labels
Λ node-labels
0
MSRC 9
Sl-qjo∙t-o JeqEnN
COX2 MD
苴 q」0°JeqEnN
0.75
0.50
6
・I wo-labels
■I node-labels
1.0
PTC FR
・I wo-labels
node-labels
5明9.50 299.75 300.00 300.25 300.50
PTC MM
_______________
40	60
BZR MD
・■ wo-labels
BH node-labels

Sl-qjo∙t-o JeqEnN
■■I wo-labels
node-labels
2010。
8
10
20	30
PTC FM
Synthie
■■ no-labels
Sl-qjo∙t-o JeqEnN
20	30
Orbit size
40
・I wo-labels
node-labels
BZR
Orbit size
10
Orbit size
Figure 3: Distribution of orbits sizes. wo-labels correspond to isomorphism without considering the
labels. node-labels correspond to isomorphism that considers node labels. Part-1.
16
Under review as a conference paper at ICLR 2020
s--qJOθ,JωqEnN
s-qjo jo JωqEnN
ER MD
<⅛5-0-5-0
10.z5.Zei
s⅛qjo Jo JφqEnN
ENZYMES
■■ wo-labels
■B node-labels
⅜,50
4 2 Q±
1.75	2.00	2.25	2.50
PROTEINS
・I wo-labels
node-labels
/■ /■ z√ ,/■ ,/■ ,	/, , /
5	10	15	20
AIDS
ω300
七
B
2200
φ
W 100
ɔ
N
O
COX2
20
0
60
40
・I wo-labels
node-labels
0
MSRC 21
・I wo-labels
node-labels
0O 100	200	300
NCIl
0
・I wo-labels
■a node-labels
20 25
■■ no-labels
30
Ibris
Letter-low
COIL-DEL
100
50
O
I...
2.0
1.5
1.0
0.5
0P
20
10
0
10
5
COIL-RAG
150
100
50
10	20
Orbit size
500	1000
Orbit size
1500
0
Figure 4: Distribution of orbits sizes. Part-2.
17
Under review as a conference paper at ICLR 2020
200
100
0
800
600
400
200
⅞
Tox21 ER
300
REDDIT-MULTI-12K
■■ no-labels
2
Orbit size
FRANKENSTEIN
400
200
100
50
■■■ no-label s
HaB no-labels
0
30	40
COLLAB
50	75 ιoo
Tox21 MMP
Tox21 AHR
4
Orbit size
Figure 5: Distribution of orbits sizes. Part-3.
S
S 150
U-
O
OlOO
∈ 50
n
N
0
NCI109
■■ no-labels
4	6	8
REDDIT-MULTI-5K
2
Oooo
0 5 0 5
17 5 2
sq」OJO .lqlunN
O
s-q.10 JO .lωqlunN
Tox21 AR-LBD
Tox21 ATAD5
Orbit size
18
Under review as a conference paper at ICLR 2020
D Isomorphism metrics for data sets with node labels
Table 7: Isomorphic metrics for data sets with node labels. Sorting is based on the proportion of
isomorphic graphs I%. Num. orbits is the number of non-trivial orbits. IP % is the proportion of
isomorphic pairs of graphs, Mismatched % is the proportion of mismatched labels. Table does not
include data sets with no node labels.
data set	Size, N	Num. orbits	Iso. graphs, I	I %	IP%	Mismatched %
SYNTHETIC	300	2	300	=	100	100	100	=
Cuneiform	167	8	"^67	100-	20.46	1Q0
DHFRJMD	-393	25	-392	99.75	6.87	-94.91
COX2JMD	-303	13	^301	99.34	11.83	-98.68
ERJMD	-446	31	^442	99.1	5.57	^8Σ74
BZRJMD	-306	22	^303	99.02	7.16	-95.75
MUTAG	"188	17	^36	19.15	0.14	^0
PTC_FM	^349	22	-34	15.47	0.08	10:89
PTCJMM	-336	22	^30	14.88	0.07	~m
DHFR	^T56	39	^98	12.96	0.04	-3.97
PTCFR	-351	20	^43	12.25	0.05	^627
PTCJMR	^344	19	^41	11.92	0.05	^64
Tox21_ARE	^T167-	228	-820	11.44	0.01	-3.91
Tox21_HSE	^8150-	250	^9T9	11.28	0.01	^25
Tox21_aromatase	^T226-	223	-805	11.14	0.01	-0.77
Tox2Lp53	^8634-	258	^946	10.96	0.01	T.83
Tox21_ER	^T697-	231	-840	10.91	0.01	^4
COX2	-467	25	^30	10.71	0.03	T.07
Tox21 _PPAR-gamma	^8184-	227	-869	10.62	0.01	-0.76
Tox21JMMP	^T320-	204	^T70	10.52	0.01	^19
Tox21_ER_LBD	^8753-	255	^9T3	10.43	0.01	1.28
Tox21,AR	^9362-	265	^965	10.31	0.01	^03S
Tox21_ATAD5	^9091	255	^924	10.16	0.01	T.04
Tox21-AR-LBD	^8599-	238	-871	10.13	0.01	^07
^ZR	-405	16	^40	9.88	0.06	-0.99
Tox21_AHR	^8169-	224	^T95	9.73	0.01	^07
PROTEINS	"ΓT13	21	-74	6.65	0.03	^61
AIDS	^^000-	22	^34	F-	0	^0
Mutagenicity	"4337-	31	F	1.73	0	-0.92
^NC∏	^4TT0-	9	F	0.41	0	-0.05
ENZYMES	^600	2	孱	0.33	0	^0
NCI109	"4127-	7	12	0.29	0	-0.05
FIRSTMM_DB	^41	1	^0	^0	0	^0
OHSU	^T9	1	^0	^0	0	^0
KKI	^83	1	^0	^0	0	^0
Peking」	^85	1	^0	^0	0	^0
MSRC21C	^^09	1	^0	^0	0	^0
MSRC_9	^^21	1	^0	^0	0	^0
MSRC_21	363	1	^0	^0	0	^0
DD	1178	1	0	一	0	0	0	—
19
Under review as a conference paper at ICLR 2020
E Proof of Claim 7.1
Proof. From the equation 3 we have:
acc(Ytest) = IYnewlI acc(Ynew) + ∣lγisol∣ acc(Yiso) > acc(Yiso) ⇒
|Yt
est l	lYtest l
■Yewacc(Ynew) > (1 -些斗)acc(Yiso) ⇒
lYtestl	lYt
estl
acc(Yiso) > acc(Ynew )
□
F Proof of Claim 7.2
Proof. From the definition of the peering model we have:
accM (Ynew ) = accMc(Yn
ew )
accM (Yiso) ≤ accMc(Yiso) = 1
Substituting these into the equation 3 we have:
accM (Ytest) ≤ accMc(Ytest).
□
G	Experimentation details
NN model is from Xu et al. (2018) and evaluate it on the data sets from PyTorch-Geometric (Fey &
Lenssen, 2019). For each data set we perform 10-fold cross-validation such that each fold is evalu-
ated on 10% of hold-out instances Ytest . For each fold we train the model for 350 epochs selecting
the final model with the best performance on the validation set (20% from hold-out trained split)
across all epochs. Additionally we found that for small data set performance during the first epochs
can be unstable on the validation set and thus we select our model only after the first 50 epochs. The
final model is evaluated on the test instances and corresponds to NN in the experiments. Peering
models NN-PH and NN-P are obtained from NN by replicating the target labels for homogeneous
Yiso and non-homogeneous Yiso respectively. Weisfeiler-Lehman and Vertex histogram kernels are
taken from the code2 of Sugiyama & Borgwardt (2015). We selected the height of subtree h = 5 for
WL kernel. We train an SVM model selecting C parameter from the range [0.001, 0.01, 0.1, 1, 10].
2https://github.com/BorgwardtLab/graph-kernels
20
Under review as a conference paper at ICLR 2020
H New data sets
Table 8: Statistics for new clean data sets. Retention is the proportion of graphs remaining after
cleaning procedure. Min. Class and Max. Class are minimum and maximum number of graphs in a
class. Sorted by retention.
Data set	Size	Retention, %	Avg. Nodes	Avg. Edges	Classes	Min. Class	Max. Class
SYNTHETIC	0	0	=	0	0	0	0	0
Cuneiform	^0	^0	^0	^0	^0	^0	^0
COIL-RAG	13	-033	-3.77	^932	71		
Letter-low	12	-033	^TT17	-3.42			飞
^^COX2MD	飞	-0.99	-30.33	^467	^2		22
DHFRMD	^4	T.02	■^5.25	366.25	^2		飞
Letter-med	^29	T.29	-6.83	ʒɪðð			10
Fingerprint	^31	T.82	1445	13.12	^6		^40
BZRMD	^6	T.96	1417	130.17	^2		飞
Letter-high	^^60	~271	-7.03	-7.23			F
ERND	14	-3.14	T8.07	227.36	^2		T3
IMDB-MULTI	^321		■^2.35	249.46	飞	^^5	144
Tox21_ARE	3302	46.07	20.96	21.86	^2	^^602	"^700
Tox21_ER	3560	-46.25	22.24	■^3.29	^2	^419	-3141
Tox21MMP	^3405-	46.52	^2A	■^3.43	^2	^^618	"^787
Tox21_HSE	3814	-46.8	-71:29	■^2.28	^2	^^07	1607
Tox2Lp53	^4088-	-47.35	22.62	23.72	^2	^^91	1797
Tox21_PPAR-gamma	^3877-	47.37	ɪ9l	ɪ9i	^2	T20	1757
Tox21 工TAD5	4312	47.43	22.52	ɪ^!	^2	168	^4144
Tox21 工R-LBD	4134	48.08	222Al	■^3.48	^2	150	1984
Tox21工R	4506	48.13	■^2.93	14:05	^2	189	^4317
Tox21 工HR	1935-	48.17	■^2.88	■^3.98	^2	^490	1445
Tox21_ER_LBD	4224	48.26	■^2.69	ɪ8	^2	193	^4031
Tox21-aromatase	3524	48.77	22.24	23.22	^2	^^34	-3290
IMDB-BINARY	^493-	-49.3	^4.08	221.96	^2	^^32	"^61
COX2	^^37-	-30:75	42.14	-44.43	^2	^^68	169
AIDS	1110	ɪʒ	18.22	ɪi	^2	^310	"^00
FRANKENSTEIN	1448-	56.44	■^0.78	■^2.35	^2	1020	1428
PTCNM	^^26-	67.26	17.04	17.74	^2	~n	149
^^ZR	^^76-	^^687Γ5	-36.25	-38.81	^2	F	"^04
PTCNR	^^35	-68.31	17.23	17.97	^2	^^96	T39
PTC-FM	142-	69.34	16.96	17.66	^2	^^5	157
MUTAG	135	^71Γ81	T8.85	20.84	^2	^42	^^93
PTC-FR	^^53-	72.08	ɪn	17.86	^2	^^6	167
DHFR	^378-	76.46	-43.37	-45.53	^2	^^05	173
Mutagenicity	^3335-	ɪ9	32.96	34.16	^2	1484	T851
COIL-DEL	^3133-	80.33	■^5.05	64.26	^^98		19
COLLAB	4064	81.28	76.94	4667.92		^770	"^289
PROTEINS	^^975	ɪ6	-43.41	81.04	^2	^343	^^632
^NC∏	1785-	92.09	29.84	-32:37	^2	1781	"^004
NCI109	^3801-	^9Σ1	29.66	32.22	^2	1801	"^000
ENZYMES	^395-	^^99TT7	-32:48	^6ΣΓ7	^6	^^98	100
REDDIT-BINARY	1998	-99.9	430.04	996.48	^2	^^998	1000
REDDIT-MULn-12K	11917	-99.9	391.79	914.68	H	^313	"^586
FIRSTMM_DB	^41	100	1377.27	3073.93	H	^2	^6
OHSU	^T9	100	I2.01	199.66	^2	15	^44
KKI	13	100	26.96	48.42	^2	F	^46
Peking_1	^^5	100	-39:31	-77.35	^2	16	^49
MSRC.21C	^^09-	100	^40^	-96.6	F		"^9
MSRC_9	^^21	100	-40.58	-97.94		19	10
Synthie	^400-	100	ɪ6	202.18	^4	^^90	T10
MSRC_21	^363-	100	77.52	198.32	^^0	10	14
^D	T178-	100	284.32	715.66	^2	^487	^^691
REDDIT-MULrI-5K	4999	100	—	508.51	—	1189.75 一	5	999	1000	-
21
Under review as a conference paper at ICLR 2020
I Proof of Theorem 6.1
Proof. Let us prove Theorem 6.1. We denote by L = {(x, y) → L(f (x), y) : f ∈ F} a composite
loss class. For any L ∈ L we get that
EPL-EDuL =EPL-EPuL+EPuL-EDuL ≤
≤ EP|(1 - u)L| + (EPuL-EDuL).	(5)
Since any L ∈ L is bounded from above by 1 for the first term in equation 5 we obtain
EP|(1 - u)L| ≤ EPg+ (w)I{y=+1} + EPg-(w)I{y=-1} = g+(w)π + g-(w)(1 - π).	(6)
Thanks to McDiarmid’d concentration inequality Mohri et al. (2012), applied to the function class
Lu = {uL : L ∈ L}, with probability 1 - δ, δ > 0 for D 〜PN We get the upper bound on the
excess risk
SuP(EPuL - EDUL) ≤ 2RN(Lu) + max[(1 + g+(w)), (1 + g-(w))]p(logδ-1)∕(2Ν) ≤
L∈L
≤ 2RN(Lu) + (2 + g+ (w)+ g-(w))P(logδ-1)∕(2Ν).	(7)
Let us find a relation between RN(Lu) and RN (L). Here we denote by zi a pair zi = (xi, yi). By
the definition (see Mohri et al. (2012)) the empirical Rademacher complexity
1N
R D (Lu)=刀 Eσ SuP 废 σiu(zi)L(zi) ≤
N	L∈Lu i=1
≤ ɪEσ	SuP X σiL(zi)	+	g+(w) Eσ	sup X	σiL(zi)	+	9弋)Eσ	SuP X。，©) ≤
N L∈Lu i=1	N L∈Lu i:yi=+1	N L∈Lu i:yi=-1
≤RD(L) + 9+(W)Eσ SuP X σiL(zi) + 9-(W)Eσ Sup X σiL(zi).
N	L∈Lu	N L∈Lu
u i:yi=+1	u i:yi=-1
Since we use the zero-one loss, then
Eσ SuP	σiL(zi)	≤	#{i :	yi = +1},	Eσ	SuP	σiL(zi)	≤ #{i :	yi	=	-1}.
L∈Lu i:yi=+1	L∈Lu i:yi=-1
The Rademacher complexity
RN(Lu) = ED〜PNRD(Lu) ≤ ED〜PNhRD(L) +。+;，#{i : yi = +1}+
+ 9-，#{: y = -1}i = RN(L) + 9+(W)π + 9-(W)(I - π).	(8)
Using the fact that RN(L) = 2RN(F), substituting inequalities 6, 7 and 8 into equation 5, we get
that
SuP EPl(f(x), y)-EDu(x, y)l(f(x), y) ≤ 3 (9+ (W)π + 9-(W)(1 - π)) +
f∈F
+ RN (F) + (2 + 9+(w) + 9- (w)) P(log δ-1 )∕(2N).
□
22