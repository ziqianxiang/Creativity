Under review as a conference paper at ICLR 2020
Rise and Dise: Two Frameworks for Learning
from Time Series with Missing Data
Anonymous authors
Paper under double-blind review
Ab stract
Time series with missing data constitute an important setting for machine learn-
ing. The most successful prior approaches for modeling such time series are based
on recurrent neural networks that learn to impute unobserved values and then treat
the imputed values as observed. We start by introducing Recursive Input and State
Estimation (RISE), a general framework that encompasses such prior approaches
as specific instances. Since Rise instances tend to suffer from poor long-term
performance as errors are amplified in feedback loops, we propose Direct Input
and State Estimation (DISE), a novel framework in which input and state repre-
sentations are learned from observed data only. The key to Dise is to include
time information in representation learning, which enables the direct modeling
of arbitrary future time steps by effectively skipping over missing values, rather
than imputing them, thus overcoming the error amplification encountered by Rise
methods. We benchmark instances of both frameworks on two forecasting tasks,
observing that Dise achieves state-of-the-art performance on both.
1	Introduction
Many machine learning settings involve data consisting of time series of observations. Due to vari-
ous reasons, observations may be missing from such time series. For instance, it may be impossible
to observe the data during a given time window, the data-recording system may fail, or measurements
may be recognized as noisy and immediately discarded at the source. Historically, the prevalent ap-
proach to handling missing data has been to apply a preprocessing step to replace the missing obser-
vations with substituted values and then treat the time series as though it were complete (Schafer &
Graham, 2002). Multiple recent works (Lipton et al., 2016; Yoon et al., 2017; 2018; Che et al., 2018;
Cao et al., 2018), however, circumvent this two-step approach and integrate both value imputation
and the downstream task in one single model. At their core, these approaches employ recurrent neu-
ral networks (RNN) (Hochreiter & Schmidhuber, 1997) whose input and hidden states are modified
to account for the missing observations.
In this paper, we note fundamental commonalities between these prior approaches and define a gen-
eral framework, Recursive Input and State Estimation (RISE), that encompasses those approaches as
specific instances. Instances of Rise operate recursively on all intermediate time steps between two
observed values, by imputing the first unobserved value in a row based on the preceding observed
values and then working with the imputed value as though it were observed. Recursive approaches
suffer from poor long-term performance (Lamb et al., 2016; Fox et al., 2018) as errors are ampli-
fied in feedback loops. To overcome this problem, we propose a novel framework, Direct Input
and State Estimation (DISE), also built on recurrent neural networks. In contrast to RISE, DISE
learns representations for inputs and hidden states that are enriched with time information and thus
allow the framework to model arbitrary future time steps by effectively skipping over missing val-
ues, rather than imputing them. Moreover, unlike previous work (Yoon et al., 2017; Baytas et al.,
2017; Binkowski et al., 2018; Che et al., 2018), Dise integrates not only relative time information
(the time between observations), but also absolute time information, which helps the model learn
non-stationary properties of the signal. Overall, we make the following contributions:
1
Under review as a conference paper at ICLR 2020
(i)	We introduce Rise (Section 3), a general framework for learning from univariate time
series with missing data, which captures the fundamental commonalities between the most
relevant recursive approaches from the literature.
(ii)	We propose Dise (Section 4), a non-recursive framework that learns time-enriched input
and state representations and is thus able to avoid imputation-induced drift (a main short-
coming of Rise methods) by skipping to future hidden states.
(iii)	We benchmark instances of Rise and Dise in two forecasting problems (Section 5). The
best-performing instance of Dise operates on digits as atomic units to derive latent repre-
sentations for numerical data. The expressiveness of this encoding function allows us to
achieve state-of-the-art performance in both problems.
2	Notation
We denote a univariate time series x = [x1,x2, . . . ,xT] as a sequence of T observations. Let s be the
time vector [s1, s2, . . . , sT], each value corresponding to the time—measured as the running time
elapsed since the beginning of the time series—when the observation was taken. In practice, some
observations may be missing for multiple reasons, for which a T -dimensional masking vector m is
defined: mi = 1 if xi is observed and mi = 0 otherwise.
We define an alternative notation that will be used in the definition of our framework. We denote
o = [o1,o2, . . . ,oN] as the sequence of N observations with non-missing values. The associated
timestamp vector is denoted by t = [t1 , t2, . . . , tN], where ti represents the time—also represented as
the running time since the beginning of the time series—of the observed value oi. Moreover, we also
define ta, which translates the running time values of t to a certain date or time format. In absence
of missing data, o = x and N = T.
A time series with missing data can be described with either notation (see Table 5 in Appendix A).
3	Recursive Input and State Estimation
Instances of the Rise framework are given by recurrent neural architectures to learn a model for
predicting p(xi+n|x1:i) by recursively predicting intermediate conditional terms. While one can find
a numbers of variants of recurrent neural architectures in the literature, all variants define a cell
whose (hidden) state hi ∈ Rdh —dh is the number of units—updates based on the previous state
hi-1 and current input xi ∈ R. In time series with missing data, instances of the RISE framework
replace the standard input with a transformed input x^. Similarly, at the input of the cell the incoming
state hii-1 is substituted with a transformed hidden state h — to account for the last time a value
was observed in the time series. The hidden state hi is updated based on these transformed signals
and the equations of the chosen recurrent architecture. The transformed input X depends on the
conditionally imputed input xc , defined as follows:
xc = Ximi + (1 — mi) Xi,
(1)
where X is the imputed input, whose computation is instance-specific.
It is common to some of these works to define a time gap vector δB , defined as the time gap from
the last observed value to the current timestamp. More formally:
si - si-1 +δiB-1,	i > 1, mi-1 = 0
δiB=	si-si-1, i i>1,mi-1=1
I 0,	i = 1
(2)
The superscript B refers to the backward computation of the time gap vector: the time gap at a certain
moment is computed with respect to the previous timestamp. They are used in the computation
of the so-called discount factors, denoted as γX ∈ R and γh ∈ Rdh , which in turn are used in the
computation of the transformed state and the transformed hidden state, respectively. The discount
factors are defined as follows:
γiX = γX(δiB) = exp(- max(0, wγX δiB + bγX)),
γih =γh(δiB) =exp(-max(0,wγhδiB+bγh)),
(3)
2
Under review as a conference paper at ICLR 2020
where wγx , bγx , bγh ∈ R and wγh ∈ Rdh , are parameters trained jointly with all other parameters of the
model. The motivation of the discount factor comes from the following intuition: the influence of
the past history in the current moment fades away over time. These discount factors allow these
models to also learn from time series where consecutive timestamps are not always equally spaced.
While in most of the Rise instances authors address related downstream time series classification
or regression problems in multivariate time series, in this paper we are interested in autoregression
problems in univariate time series, which allow to assess more directly the ability of these methods
to learn with missing data. Therefore, a function fθ , parameterized with weights θ, is applied to the
hidden state hi to predict the next observation, more formally: yi +1 = fθ(hi). The loss at the i-th
observation is defined as l = mi+∖L(yi+1, J^i+1).
Typically, the function fθ is a regression function, L is the mean squared error loss, and yi amounts
to xi—i.e. the i-th value of the time series. Alternatively, it can be cast as a multi-class classifi-
cation problem (van den Oord et al., 2016b;a; Fox et al., 2018), wherein the function fθ outputs a
probability mass function, L is the cross-entropy loss, and yi is the one-hot encoding of xi .
A non-exhaustive list of recent instances of the Rise framework can be found in Table 1.
Instance of RISE	Input and State Transformation (Univariate Time Series)		
Simple Recursion	Xi = W x∙ hi-1 + bx		Xi = X		h i-1 = hi-1	
Zeros&Indicators	Xi = 0		Xi = [Xi; mi`]	h i -1 = h i -1
FOrWard-Filling&IndiCatOrS	Xi = Xi i		Xi = [X; mi]	h i-1 = h i-1	
GRU-D	Xi = YxxiO + (1 - Yx ) Xavi0		Xi = [X(I; mi]	hi-1 = Yi Θ h-
Rits-I	Xi = W x∙ hi-1 + bX		Xi = [Xi ； mi]	hi-1 = Y夕 Θ h-
(Luo etal., 2018)	Xi = 0		Xi = X		hi-1 = Yi Θ h-
(Kim&Chi, 2018)	Xi = 1γii,>τXii0 + (I - 1γii,>τ)Xav	X i = X		h i-1 = h i-1	
(Yoonetal.,2017)* 一	Xi = 0		Xi = Xi； δF ]	h i-1 = hi-1	
BRITS-1*	Xi = W x∙ hi-1 + bX		Xi = [Xi ； mi]	h - = Y ? Θ h-
Table 1: (Upper) ∙ is the dot product. [a; b] is the concatenation of terms a and b. Θ is the element-
wise multiplication. m is the complement of the binary value m. Xavi, is the average of the non-
missing values prior to si of vector x. xi0 is the value of the last time (i0 < i) the signal was observed.
(Lower) Instances to the data imputation problem are indicated with a grey background. Luo et al.
(2018) is framed in a generative adversarial network wherein the full time series is observed before
imputing the missing values. γi,, is a discount factor computed with respect to xi,, , which is the
nearest observed value in either time direction, and τ is a threshold value. An asterisk * indicates
that the described equations are computed in a bidirectional manner. Except in cases where it is a
constant, X is always treated as a variable.
Simple Recursion (Fox et al., 2018) This is the simplest instance of the RISE framework. At each
timestamp the model is fed with either the input value if there exists, or the imputed value, which is
computed in the previous timestamp, if the input value is missing.
Zeros/Forward-Filling&Indicators (Lipton et al., 2016) Lipton et al. (2016) considers two pop-
ular strategies to fill missing values for sequential data: forward- and zeros-filling. The former
replaces the missing values with the last non-missing value observed in the sequence. The latter
simply fills the missing values with zeros. The input is augmented with a binary indicator, which
is set to one if the input value is missing, and set to zero otherwise. Thus, through their hidden
state computations, the recursive neural network can use indicators of missing data to learn arbitrary
functions of the past observations and missingness patterns. Zero-filling with binary indicators was
found to be the best overall model by all considered metrics in their downstream task.
Gru-D (Che et al., 2018) This approach incorporates the previously described discount factors
to account for missingness when computing the transformed input value and the hidden state. The
decay mechanism applied to the input learns to fade away the impact of the last observation over
time and progressively replace it with the empirical mean of the input signal. Similarly when applied
to the hidden state, their features are decayed over time.
3
Under review as a conference paper at ICLR 2020
RITS-I (Cao et al., 2018) Cao et al. (2018) proposed Brits, a family of models designed for data
imputation in time series. Within these models, Rits-I is the only one that is suitable for the task at
hand. It combines aspects of all three previous works. Similar to the simple recursion approach, the
imputed value for the current timestamp is the predicted value computed in the previous timestamp.
As in Che et al. (2018), it is equipped with a decay mechanism over the hidden state, and the input
is augmented with a binary indicator.
Other instances Kim & Chi (2018) is another instance of the RISE framework closely related to
Gru-D. Recent works (Yoon et al., 2017; Kim et al., 2017; Luo et al., 2018) on data imputation
for time series could also be framed in the RISE framework under certain (small) modifications.
Different to forecasting, most of data imputation methods replace missing values with substituted
values based on previous and subsequent information.
4	Direct Input and State Estimation
Main (and only) differences across Rise instances lie on the equations that alter the standard input
and hidden state of the recurrent neural architecture, but everything else remain unchanged. One
may define new equations based on expert knowledge or intuition to guide the transformation of
input and hidden state, however the Dise framework skip timestamps where the data is missing and
only learns from observed data. Dise resorts to latent representations enriched with absolute and
relative time information to enable the recurrent architecture to update its hidden state so as to jump
to a certain moment into the future. The learning of the underlying “discount” factor is up to these
representations. Therefore, Dise does not require computing any intermediate term. This implies
that the hidden state is only updated in timestamps where a value is observed. We resort to the
alternative notation defined in Section 2 to define Dise.
Figure 1: (Upper) A segment ofa time series with missing data. Simplified layouts of (Middle) Rise
and (Lower) Dise. Whereas Rise attends to timestamps whose data points are missing and updates
its hidden state accordingly, Dise skips them and only updates its state with observed values.
The Dise framework also defines time gaps between pairs of non-missing observations as follows:
ti+n -ti,	1 ≤ i,i+n < N,
undefined, otherwise,
(4)
which corresponds to the time gap between the current non-missing observation, happening atti, and
the n-th non-missing observation after the current one, happening at ti+n. In this case, the superscript
F refers to its forward computation.
4
Under review as a conference paper at ICLR 2020
Let ∆n be a shortcut that refers to the time information (δF,tQn). DISE defines <oi[∆n] ∈ Rdo and
hi-1 [∆n ] ∈ Rdh, computed respectively with functions go and gh, and that correspond to latent rep-
resentations that are enriched with the absolute and relative time information in ∆in . They are the
counterparts of the transformed input and state of the Rise framework. During training, at a certain
time ti the framework computes:
•	<oi [∆n] = go (oi, δF, ta+n) : The dedicated latent representation for the input oi to predict the
value at ti+n The relative time distance between the two observations is δ*.
•	hi-l[An] = gh(hi-l[A1-i], δF, ta+ n) : The dedicated representation for the incoming hidden
state hi-ι [∆ 1-J to predict the value at ti+n, separated δF from the the current observation.
We refer to hi [∆in] ∈ Rdh as the ∆in -lagged hidden state, which will be used by the function fθ to out-
put Z i+n——i.e. the prediction for t+n. It is computed based on Oi [∆n ] and h i-ι[∆n ], and the equations
of the chosen recurrent architecture. One can train a model to be good at making predictions not only
for the next observed value but also for observations beyond the next one or even fora time horizon.
In the most general case, the loss at the i-th observation is defined as l = ∑D=I L(Zi+n, Zi+n), where
D is the length——in number of observations——of the time horizon. As in the RISE framework, L,
fθ and Zi are chosen so as to cast the problem as either a regression or a multi-classification one.
Figure 1 depicts the Rise and Dise frameworks. At inference time, we directly target a desired
future timestamp tba and query the framework with (tb - ta,tba), being ta the timestamp of the last
observed value, which computes the (tb - ta,tba)-lagged hidden state to output a prediction.
One key component of the Dise framework is the learning of time-enriched input and hidden state
representations. In this work we propose an instance whose functions go and gh use the learned
representations for relative and absolute time information to act as gating mechanisms over the
representations of the input and hidden state. This allows to effectively allocate some dimensions of
the representations to account for time dependencies. They are computed as follows
<Oi[∆ n ]= go (oi, δF, ta+ n )= fOnc(oi)。σ (Wδ fδFc(δFn )) Θ σ (Wtftnc(处 n ))
X------------------{z-----------------}
Φo (∆in )
Fi	(5)
hi-l[An] = gh (hi-1[A1-i], δF, ta+ n ) = h i-l[A1-l] θ σ (VS fδnc(δFn)) θ σ (Vt ftnc( ta+ n ))
、----------------------------------------------------------------------{----------------}
Φh(∆in)
where all f Q : R → RdQ functions are encoders——We use the same dω for all encoders——for numerical
values. The W, V matrices map all latent representations to the same dimension, either do (which
is the same as dω) or dh. The choice of the encoders will be discussed in Section 4.1. The Φo(A?)
and Φh(∆in) terms have a role similar to the discount mechanisms of RISE, and are computed with
both relative and absolute time information. However, these “discount” factors are to be learned,
and not driven by a certain expression. Compared to the Rise framework, on one side Dise requires
to compute additional latent representation for input, relative and absolute time information; but
on the other side it does not require to compute intermediate states to make predictions for a future
timestamp. This makes our framework also suitable for event series, where the time gap between two
consecutive observations is arbitrary. In contrast, Rise instances will require events to be aligned
at some chosen timestamps, which generates a large number of missing observations——thus, being
inefficient computationally and increasing its complexity.
The representations for the input <Oi [∆?] and hidden state hi-ι[∆?] are computed as the element-
wise product of three representations. Alternatively, go and gh could be chosen so as to represent
<Oi [∆n ] and hi-1 [∆in] as the concatenation of their three corresponding representations. However,
we experienced worse performance and longer training times with this instance, as the number of
parameters of the recurrent cell also grow.
As discussed, instances of Rise and Dise are agnostic to the chosen recurrent neural architecture.
In the experimental section we opt fora gated recurrent unit (GRU) (Cho et al., 2014), whose details
for both frameworks can be found in Appendix B.1. An extension of the proposed Dise instance to
increase its depth is explained in Appendix B.2. This extension reinforces time information into each
of the higher layers. Note that do might be different to dω for higher layers in deeper architectures.
5
Under review as a conference paper at ICLR 2020
Most instances of Rise are not appropriate to Dise, as i) they cannot include (date-formatted) abso-
lute time information, and ii) the transformed input and hidden state cannot not directly be computed
for an arbitrary time into the future. However, it is possible to recycle ideas from Rise instances
to only learn from observed data. Inspired by some of these works, we propose Rise2Dise, which
applies corresponding discount mechanisms—see Equation (3)—to the input and hidden state to
translate them to any moment into the future. Different to Gru-D, Rise2Dise only updates the
state with observed values. More details to be found in Appendix B.3.
Prediction over a Time Horizon Instances of the the RISE framework predict xi+D by recursively
estimating intermediate values until reaching the desired point in time. Therefore, the side effect is
that these methods also predict a time horizon—i.e. the values leading up to xi+D . On the other
side, multi-output forecasting methods estimate the whole time horizon in one step (Taieb et al.,
2012). Previous work (Fox et al., 2018) has empirically shown that multi-output methods outperform
recursive methods in the time horizon prediction task. This is due to the feedback loop of recursive
methods, which re-use past predictions to predict future values, leading to lower quality predictions
as the time horizon increases. However, it is not clear how multi-output approaches would deal with
time series with missing values, as they assume the time series is complete. The Dise framework
does not fit in either of these two paradigms.
4.1	On the choice of the Encoders
Encoders suitable for Dise must be able to map any data that can be decomposed as a sequence of
digits to a latent space. One may be tempted to transform these data to one-hot encoding represen-
tations, and then learn a latent representation via a linear layer. However, the potential vocabulary
size of this approach is infinite. Therefore a data binning scheme should be first applied to the data,
which is a challenge in itself.
Feedforward-based encoders A popular option (Li et al., 2017; Pezeshkpour et al., 2018) for
mapping single numerical values to a latent space is by applying a feedforward neural network
over the (typically) log-normalized numerical value. Let m and m be a numerical value and its
corresponding latent representation, respectively, then this encoding function would amount to: m =
fffw(m) = log(m)w f w + bfw, where the numerical value m only scales the weight vector wfw. A
sigmoid activation function is also added when encoding the input signal in Equation (5). Despite
this non-linearity, the expressiveness of this encoding function is limited. It is trivial to show that the
learned latent representations behave monotonically (with respect to m) because of the monotonicity
of the sigmoid function and the linearity of the learned representations. To alleviate this limitation
we also explore a multi-feedforward encoding function fmffw with two layers and a sigmoid as an
activation function.
Digit-level encoder On the other hand, motivated by character-level architectures for language
modeling, an alternative would consist of decomposing numerical values as a sequence of digits and
then operate on digits as atomic units to derive latent representations. To our knowledge, using digits
as atomic units (tokens) to learn representations for numerical values has been previously explored
only in Garcia-Duran et al. (2018) for a different problem—link prediction in temporal graphs. Each
token of the numerical value is mapped to its corresponding embedding via a linear layer and the
resulting sequence of embeddings is fed into a standard recurrent neural network architecture—a
GRU in this work. Let nm be the number of tokens of the numerical value m, its latent representation
corresponds to the last hidden state of the standard GRU, more formally: m = fgru (m) = hnm. In
contrast to fffw , the learned representations are not necessarily monotonic. The vocabulary consists
of 11 tokens: for digits from 0 to 9, and token ”.”, which indicates, if there exists, the beginning of
the decimal part of the numerical value. On some occasions, positive and negative signs may also
be required in the vocabulary. Moreover, this encoding function allows absolute time information ta
to be simply decomposed as a sequence of digits following a certain date format.
In any case, a dedicated encoding function is used for each of the input signal, relative and absolute
time information (see superscripts in Equation (5)). We will refer to the instances whose encoding
function is fffw, fmffw and fgru as DISE-FFW, DISE-MFFW and DISE-GRU, respectively.
6
Under review as a conference paper at ICLR 2020
5	Experiments
We target two forecasting problems. For the time horizon pre-
diction problem we consider time series without missing val-
ues, wherein the goal is to be good at predicting the values over
a time horizon of length D. For the second one, referred to as
next observed value prediction problem, we consider data sets
with missing data. This means that the next observed value can
happen anytime into the future, and the model makes a predic-
tion for that specific time. They are illustrated in Figure 2.
-DA~~Δ~~Cr
Figure 2: (Upper) Time hori-
zon and (Lower) next observed
value prediction problem. Pre-
dictions (blue triangles) are made
based on current (green squares)
and past observed values (red cir-
cles). Black-filled shapes repre-
sent missing data.
5.1	Dataset Description
5.1.1	Blood Glucose
This complete data (Fox et al., 2018) was collected over the course of three years and consists
of a large number of continuous glucose readings from 40 patients with type 1 diabetes. Every
three months, blood glucose of the patients was monitored over the course of several days with a
sampling rate of 5 minutes. We do not use date-formatted time information ta in the experiments
run in this data set, as many measurements lack a reference date. We run experiments in this data
set for the time horizon prediction problem. We artificially remove consecutive blocks of data—
see Appendix D.1—to the next observed value prediction problem, leading to the data sets Blood
Glucose (β = {1, 5}). The larger the value of β is, the more data is missing. We use the same
training, validation and test sets as in Fox et al. (2018).
5.1.2	Air Pollution
This incomplete data (Yi et al., 2016) consists of measurements from monitoring stations in Beijing.
They were hourly collected from 2014/05/10 to 2015/04/30. In this work we consider two particulate
matter: PM2.5 and PM10. The first particle pollution time series was used in previous work (Yi et al.,
2016; Cao et al., 2018) for the data imputation problem. We run experiments for the next observed
value prediction problem in these data sets.
We have also created a modified version of the PM2.5 time series, called PM2.5-peak, wherein the
measurements taken on the 2nd and 15th of each month are increased in 200 μg/m3—original values
range from 1 to 1,000 μg/m3. Therefore, the bias of the signal varies sharply in these days.
The first ten months of collected data are used for training, and the data collected in the 11th and
12th month are used for validation and test, respectively. Statistics of the data sets and additional
details are shown in Table 5 in Appendix D.1.
^^^^^^^^^^^^^T ime Horizon MethOd ^^^^^^^^^	5 Minutes	10 Minutes	15 Minutes	20 Minutes	25 Minutes	30 Minutes
Simple Recursion (Fox et al., 2018)	:1.44 | 2.49 二	2.99 | 4.92 :	4.46 6.97 :	5.80 | 8.84	7.09 10.61 二	8.34 | 12.30 ：
DEEPMO(Fox et al.,2018)	1.62 1 2.64「	2.93 | 4.85	4.23 6.78	5.47 | 8.57	6.68 10.29 —	7.87 | 11.97
SEQMO(Fox et al.,2018)	一 1.50 | 2.52 一	2.78 | 4.72	4.08 6.66	5.28 | 8.45	6.50 10.18 —	7.63 | 11.83-
DISE-FFW [R]	:1.72 | 2.77 二	2.90 | 4.88	4.16 | 6.81	5.34 | 8.56	6.48 | 10.19 二	7.60 门1.77二
Dise-mffw [R]	1.73 | 2.78	2.90 | 4.84	4.16 | 6.71	5.40 | 8.47	6.57 | 10.08	7.60 | 11.58
DISE-GRU [R]		1.44 | 2.48	2.72 | 4.68	4.00 | 6.57	5.15 | 8.31	6.34 | 10.01	7.51 门1.62
Table 2: Blood Glucose: Time horizon prediction.
5.2	Setup and Baselines
As the blood glucose data set is our main benchmark data set, for all models we carefully follow
the same evaluation protocol and setup as in Fox et al. (2018), where this data set was introduced.
Therefore, instances of RISE are built by stacking two GRUs of 512 hidden units dh in a standard
manner (Hermans & Schrauwen, 2013). Instances of the Dise framework are reinforced with time
information at higher layers as explained in Appendix B.2. Following Fox et al. (2018), we use
the multi-classification formulation of the problem described in Section 3 and the median and mean
APE (absolute percentage error) as evaluation metrics. More training details are found in Appendix
C. For the air pollution data sets the regression formulation replaces the multi-classification one, but
every other aspect of the evaluation and setup remains the same. We think that this formulation is
7
Under review as a conference paper at ICLR 2020
more common in regression problems, and also helps to assess methods’ performance under two
different objectives.
For the time horizon problem, we evaluate our instance against the two multi-output approaches1
proposed in Fox et al. (2018). We also compare it to the recursive approach previously referred
to as simple recursion. The performance of these methods is copied from Fox et al. (2018). For
the next observed value prediction problem we implemented most of the instances of the Rise
framework (upper side in Table 1) and ran experiments under the above-described evaluation and
setup. In neither of these two problems we compare to methods that are not based on recurrent
neural architectures, as related work have shown they perform poorly compared to method based on
recurrent architectures in related problems (Lipton et al., 2016; Fox et al., 2018; Cao et al., 2018).
For all instances of DISE we use a [suffix] that is a combination of the letters R (relative time
information) and A (absolute time information) to indicate the use of that data in the learning of the
Φo and Φh terms. For each data set, the δF values are normalized by the corresponding sampling
period.
5.3	Results
The format “Median APE | Mean APE” is used in all tables.
5.3.1	Blood Glucose
We show results for the time horizon problem in the blood glu-
cose data set in Table 2. This was the problem this data set was
originally intended for. As all baselines, our instances were
trained for a length D of the time horizon of 30 minutes—this
Figure 3: Mean APE vs D
amounts to 6 observations. Dise-gru [R] shows the best performance for almost each of the time
points within the 30-minute prediction window. The recursive approach matches the performance of
Dise-gru [R] at the first place of the time window. However, its performance degrades as the time
horizon increases, being quickly surpassed by the multi-output approaches DeepMO and SeqMO.
The results also indicate the importance on the choice of the encoding function, as the two other
instances of Dise are on par or even slightly worse than those of Rise.
One may wonder whether increasing the length D of the time horizon during training may lead to
better forecasts within the next 30-minute. Figure 3 shows the best performance is obtained when
training and test objectives are pretty much aligned. As expected, performance degrades when the
horizon of the training objective is smaller or larger (enough) than that of the evaluation.
_ Data Set Method ~^-^^—一	Blood Glucose (β = 1)	Blood Glucose (β = 5)
Simple Recursion	2.43 | 4.29	7.36 | 12.43
Zeros&Indicators	2.43 | 4.32	6.73 | 11.64
GRU-D	2.25 | 4.06	6.18 | 11.53
RITS-I	2.32 | 4.13	6.66 | 12.23
Rise2Dise DISE-FFW [R] DISE-MFFW [R] DISE-GRU [R]	2.48 | 4.28 2.31 | 4.09 2.50 | 4.29 2.06 | 3.97	6.86 | 11.98 6.02 | 11.31 6.31 | 11.39 5.55 | 11.29
Table 3: Blood Glucose: Next observed value prediction.
Table 3 shows results for the
next observed value prediction
problem. Motivated by Figure
3, we align training and test
objectives: models are trained
to be good at predicting the
next observed value. Gru-D
is the best performing recur-
sive method, and is competitive
with Dise-ffw [R], but outper-
formed by Dise-gru [R] in both mean and median APE. This indicates again that the choice of the
encoding function is crucial to outperform the instances of Rise. Rise2Dise’s performance indi-
cates that exponential discount factors do not benefit from learning with only observed data. It is
important to note that, contrary to language, similarities across numerical values based on its atomic
units is arbitrary and based on the chosen notation scheme. We provide visualizations of the learned
representations, and experiments with a hexadecimal notation scheme in Appendix D.2.
Once we have validated the choice of the encoding function, for the remaining experiments we only
benchmark Dise-gru against the baselines.
1The two other multi-output approaches proposed in that work are extensions of these ones, wherein the
main difference lies on additional pre- and post-processing steps applied to the data.
8
Under review as a conference paper at ICLR 2020
^^^^^^ Data Set MethOd ''~'^^^^^^^^	Air Pollution - PM10		Air Pollution - PM2.5		Air Pollution - PM2.5-peak	
Simple Recursion	1427^	| 32.52	1Σ2Γ	| 30:30	11.46"	33.16
Zeros&Indicators	T437^	| 31.65	12.29-	| 28.22	11.46-	33.62
GRU-D	14.TT	| 28.85	12:37	| 28:47	nɪ	3Σ49
RrTS-I	14:42-	| 29.57	12:24	| 28:68	1ΓΓ19^	■31.37
DISE-GRU [R]	138Γ	| 28.25	1219^	| 28:94	mτ	■30.10
Dise-GRU [ar]	13.81	| 27.60	12.44	| 28:74	11.54	27.76
Table 4: Air Pollution: Next observed value prediction.
5.3.2 Air Pollution
The date-formatted timestamps
ta are decomposed as the
sequence of the digits that
correspond to the month and
the day of the month. Results are shown in Table 4. The time series corresponding to the par-
ticulate matters PM10 and PM2.5 do not largely benefit from including absolute time information.
As before, Gru-D is the best performing recursive instance. For the particle PM2.5, whose percent-
age of missing data is low, instances of Rise are competitive and even outperform instances of Dise
in terms of mean APE. In this data set, majority (around 95%) of test data points are spaced one
sampling period, so this may explain the very good performance of the Rise instances. One can
assess the positive impact of the absolute time information in PM2.5-peak, where DISE-GRU [AR]
clearly outperforms all methods in mean APE. Figure 4 shows the improvement in performance oc-
curs on the 2nd, 15th and their both subsequent days—highlighted with an orange background—,
as it is in these days where the bias value of the time series sharply varies. We show in Appendix
D.2 that Dise-gru [AR] learns to allocate dedicated dimensions to model this characteristic of the
signal. Even though recurrent neural networks are suitable to learn from non-stationary time series,
this shows that including absolute time information helps to model certain properties of the signal—
in this case, an abrupt change in the bias of the signal—that may be difficult to learn otherwise. To
overcome this problem one might preprocess the time series to convert it into stationary, but this
might be specially challenging under the existence of missing data in the time series.
6	Discussion and Future Work
We describe Rise, a recursion-based framework that encompasses most of the recent works in learn-
ing from time series with missing data. We propose a non-recursive counterpart, called Dise, based
on latent representations that incorporates relative and absolute absolute time information. We show
the benefits of an instance of this novel framework wherein latent representations for numerical data
are derived from the digits they are made UP of.
Dise relates to recent advances in the design of
recurrent neural networks to learn long-term de-
pendencies. Pachitariu & Sahani (2013); Chang
et al. (2017) reinforce dependencies between non-
consecutive states by adding connections between
them. Different from these works, Dise does not re-
quire to manually create such connections. Rather,
the model is able to jump to subsequent states be-
cause of its time-enriched representations. The
continuous-time event prediction problem is a related
topic. Event time series attend to sequences of ac-
tions that occur asynchronously. Examples are clin-
ical visits of patients to the hospital or users pur-
chasing history in an online marketplace. One major
Figure 4: PM2.5-peak: MAE vs day.
difference is that the input is always an event, which is typically a discrete token, and not a single
continuous value. Solutions to this problem involve modifications of recurrent neural architecture
(Neil et al., 2016; Zhu et al., 2017) or the use of some type of time representation that is typically
concatenated to the event latent representation and used as the input to the recurrent architecture (Du
et al., 2016; Mei & Eisner, 2017; Kazemi et al., 2019). Most related to our work is Li et al. (2017),
wherein event embeddings are enriched with time information via the encoding function fffw . A
second related topic is data imputation (Rubin, 1976). The major difference is that data imputation
methods also use subsequent values to fill the missing values. The study of the Dise framework to
these two related problems is left for future work. One might also want to evaluate Dise with recent
recurrent architectures whose hidden state dynamics are specified by neural ordinary differential
equations (Rubanova et al., 2019; De Brouwer et al., 2019). Most importantly, future work should
make Dise also suitable for learning from multivariate time series.
9
Under review as a conference paper at ICLR 2020
References
Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, MatthieU
Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: A system for large-
scale machine learning. In 12th {USENIX} Symposium on Operating Systems Design and Imple-
mentation ({OSDI} 16),pp. 265-283, 2016.
Inci M Baytas, Cao Xiao, Xi Zhang, Fei Wang, Anil K Jain, and Jiayu Zhou. Patient subtyping via
time-aware lstm networks. In Proceedings of the 23rd ACM SIGKDD international conference
on knowledge discovery and data mining, pp. 65-74. ACM, 2017.
Mikolaj Binkowski, Gautier Marti, and Philippe Donnat. Autoregressive convolutional neural net-
works for asynchronous time series. In ICML 2018: Thirty-fifth International Conference on
Machine Learning, pp. 579-588, 2018.
Wei Cao, Dong Wang, Jian Li, Hao Zhou, Lei Li, and Yitan Li. Brits: bidirectional recurrent
imputation for time series. In Advances in Neural Information Processing Systems, pp. 6775-
6785, 2018.
Shiyu Chang, Yang Zhang, Wei Han, Mo Yu, Xiaoxiao Guo, Wei Tan, Xiaodong Cui, Michael
Witbrock, Mark A Hasegawa-Johnson, and Thomas S Huang. Dilated recurrent neural networks.
In Advances in Neural Information Processing Systems, pp. 77-87, 2017.
Zhengping Che, Sanjay Purushotham, Kyunghyun Cho, David Sontag, and Yan Liu. Recurrent
neural networks for multivariate time series with missing values. Scientific reports, 8(1):6085,
2018.
Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties
of neural machine translation: Encoder-decoder approaches. In SSST@EMNLP, 2014.
Edward De Brouwer, Jaak Simm, Adam Arany, and Yves Moreau. Gru-ode-bayes: Continuous
modeling of sporadically-observed time series. In Advances in neural information processing
systems, 2019.
Nan Du, Hanjun Dai, Rakshit Trivedi, Utkarsh Upadhyay, Manuel Gomez-Rodriguez, and Le Song.
Recurrent marked temporal point processes: Embedding event history to vector. In Proceedings
of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,
pp. 1555-1564. ACM, 2016.
Ian Fox, Lynn Ang, Mamta Jaiswal, Rodica Pop-Busui, and Jenna Wiens. Deep multi-output fore-
casting: Learning to accurately predict blood glucose trajectories. In Proceedings of the 24th
ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 1387-
1395. ACM, 2018.
Alberto Garcia-Duran, Sebastijan Dumancic, and Mathias Niepert. Learning sequence encoders for
temporal knowledge graph completion. In EMNLP, 2018.
Michiel Hermans and Benjamin Schrauwen. Training and analysing deep recurrent neural networks.
In Advances in neural information processing systems, pp. 190-198, 2013.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735-1780, 1997.
Seyed Mehran Kazemi, Rishab Goel, Sepehr Eghbali, Janahan Ramanan, Jaspreet Sahota, Sanjay
Thakur, Stella Z. Wu, Cathal Smyth, Pascal Poupart, and Marcus A. Brubaker. Time2vec: Learn-
ing a vector representation of time. ArXiv, abs/1907.05321, 2019.
Han-Gyu Kim, Gil-Jin Jang, Ho-Jin Choi, Minho Kim, Young-Won Kim, and Jaehun Choi. Recur-
rent neural networks with missing information imputation for medical examination data predic-
tion. In 2017 IEEE International Conference on Big Data and Smart Computing (BigComp), pp.
317-323. IEEE, 2017.
Yeo-Jin Kim and Min Chi. Temporal belief memory: Imputing missing data during rnn training. In
In Proceedings of the 27th International Joint Conference on Artificial Intelligence (IJCAI-2018),
2018.
10
Under review as a conference paper at ICLR 2020
Diederick P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations (ICLR), 2015.
Alex M Lamb, Anirudh Goyal Alias Parth Goyal, Ying Zhang, Saizheng Zhang, Aaron C Courville,
and Yoshua Bengio. Professor forcing: A new algorithm for training recurrent networks. In
Advances In Neural Information Processing Systems, pp. 4601-4609, 2016.
Yang Li, Nan Du, and Samy Bengio. Time-dependent representation for neural event sequence
prediction. arXiv preprint arXiv:1708.00065, 2017.
Zachary C Lipton, David Kale, and Randall Wetzel. Directly modeling missing data in sequences
with rnns: Improved classification of clinical time series. In Machine Learning for Healthcare
Conference, pp. 253-270, 2016.
Yonghong Luo, Xiangrui Cai, Ying Zhang, Jun Xu, et al. Multivariate time series imputation with
generative adversarial networks. In Advances in Neural Information Processing Systems, pp.
1596-1607, 2018.
Hongyuan Mei and Jason M Eisner. The neural hawkes process: A neurally self-modulating mul-
tivariate point process. In Advances in Neural Information Processing Systems, pp. 6754-6764,
2017.
Daniel Neil, Michael Pfeiffer, and Shih-Chii Liu. Phased lstm: Accelerating recurrent network
training for long or event-based sequences. In Advances in neural information processing systems,
pp. 3882-3890, 2016.
Marius Pachitariu and Maneesh Sahani. Regularization and nonlinearities for neural language mod-
els: when are they needed? ArXiv, abs/1301.5650, 2013.
Pouya Pezeshkpour, Liyan Chen, and Sameer Singh. Embedding multimodal relational data for
knowledge base completion. In EMNLP, 2018.
Yulia Rubanova, Ricky T. Q. Chen, and David Duvenaud. Latent odes for irregularly-sampled time
series. In Advances in neural information processing systems, 2019.
Donald B Rubin. Inference and missing data. Biometrika, 63(3):581-592, 1976.
Joseph L Schafer and John W Graham. Missing data: our view of the state of the art. Psychological
methods, 7(2):147, 2002.
Souhaib Ben Taieb, Gianluca Bontempi, Amir F Atiya, and Antti Sorjamaa. A review and com-
parison of strategies for multi-step ahead time series forecasting based on the nn5 forecasting
competition. Expert systems with applications, 39(8):7067-7083, 2012.
Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves,
Nal Kalchbrenner, Andrew W. Senior, and Koray Kavukcuoglu. Wavenet: A generative model
for raw audio. In SSW, 2016a.
Aaron van den Oord, Nal Kalchbrenner, and Koray KavUkcUoglu. Pixel recurrent neural networks.
In ICML, 2016b.
Xiuwen Yi, Yu Zheng, Junbo Zhang, and Tianrui Li. St-mvl: Filling missing values in geo-sensory
time series data. In IJCAI, 2016.
Jinsung Yoon, William R. Zame, and Mihaela van der Schaar. Estimating missing data in temporal
data streams using multi-directional recurrent neural networks. IEEE Transactions on Biomedical
Engineering, 66:1477-1490, 2017.
Jinsung Yoon, William R Zame, and Mihaela van der Schaar. Deep sensing: Active sensing using
multi-directional recurrent neural networks. 2018.
Yu Zhu, Hao Li, Yikang Liao, Beidou Wang, Ziyu Guan, Haifeng Liu, and Deng Cai. What to do
next: Modeling user behaviors by time-lstm. In IJCAI, pp. 3602-3608, 2017.
11
Under review as a conference paper at ICLR 2020
A Notation: One Example
An illustration of a time series under the two notations defined in Section 2 is depicted in Figure 5.
Time Series X	47	N.A.	N.A.	40	N.A.	43	55
Timestamps S	^^0^^	-2-	-4-	ɪ-	10	ɪ"	
Masking m	~Γ~	0	0	~τ~	0	~T~	~Γ~
Time Series o	47	40	43	55
Timestamps t	-0	-8	-12-	-14-
Date-Formatted Timestamps ta	06:00	14:00	18:00	20:00
Figure 5: (Left) A univariate time series with missing values. (Right) Alternative notation for the
same time series.
B RISE&DISE
B.1 Learning with a GRU
As discussed, neither RISE not DISE are restricted to a particular type of recurrent network. As in
prior work (Che et al., 2018), we opt for a gated recurrent unit (GRU).
Within the RISE framework, the equations that define the reset and update gates of the GRU unit to
control the hidden state are as follows:
ri = σ (Wr Xi + Urh i-1 + b r )
Z i = σ (Wz X i + UZ h i -1 + bz)
(6)
ci = tanh( WXi + U(r Θ hi-i) + b)
hi = (1 - zi) Θ hi-i + zi Θ Ci,
where σ is the sigmoid function, Θ is the element-wise product, and all parameters are those of the
standard setting where the GRU unit operates.
The GRU equations used in the DISE framework are as follows:
ri[∆n] = σ(Wr0i[∆n] + Urh^i-i [∆:] + br)
Z i[∆ n ] = σ (Wzo i[∆n ] + UZh i-1 [An ] + bz)	⑺
Ci[∆n] = tanh(Woi[∆n] + U(ri∙[∆:] Θ hi-ι[∆7]) + b)
hi[∆n ] = (i - zi[∆n ]) θ h-ι [∆ n ]+之必? ] G #n ].
B.2 Stacked DISE
An extension of DISE to increase its depth would consist of just stacking recurrent neural networks
on top of each other (Hermans & Schrauwen, 2013). We modify this standard approach with the
goal of reinforcing time information into higher layers. To do so we also apply gating mechanisms
at each layer. The time-enriched representations at the l-th layer are defined as:
Oi[∆nF )= Oi[∆n ](I) Θ σ (Wδl) fδFc(δ给)Θσ (wtl) 襦。叫“))	⑻
hi-l[∆n](l) = hi-i[∆ 1-i](l) Θ σMl)fδFc(δFn)) Θ σ(V(I)ftnc(ta+ n))
where the input at the l-th layer is defined as
oi[∆in](l) =	hfeoin[∆cin(]o(il)-1) ll=>00	(9)
and the W(l) , V (l) matrices map all latent representations to the same dimension. A prediction is
computed as fθ (hi[∆in](L)), where L is the number of layers.
B.3 Rise2Dise
Rise2Dise defines discount factors similar to Equation (3) that are computed with respect to future
observed values: Yon = γo(δ^) and Yhn = Yh(δF). Since it is not able to incorporate date-formatted
12
Under review as a conference paper at ICLR 2020
timestamps, ∆n simply amounts to (δFn). It learns time-enriched representations for input and hidden
state as follows:
oi[△n] = go(Oi, δFFn) = Yi;noi + (I-展n)oavio
hi-1 [An] = gh(hi-1[∆ 1-1], δG= hi-1 因-1] © Yhn,
(10)
where oavi0 is the average of the values prior to ti of vector o.
C Training Details
We use the same evaluation protocol and setup as that of Fox et al. (2018). We stack two GRUs
of 512 hidden units dh for all approaches. When using the multi-classification formulation, the
probability distributions outputted by the function fθ are translated to predictions by taking the
value represented by the class with maximum probability. Models are evaluated at any point in
time in which there are at least ten samples of prior data. We validate performance according to
the median APE on the validation set after every epoch. We validate the weight decay between the
values {10-2, 10-3}. Models were trained for 100 epochs, but usually the best validated model is
obtained within the first 50 epochs. All remaining model details, such as the initialization procedure
and the initial learning rate for ADAM (Kingma & Ba, 2015), used the Tensorflow (Abadi et al.,
2016) default values. The same evaluation protocol and setup has been followed for all trained
models.
When using the digit-level GRU fgru as encoding function, we embed tokens of the vocabulary
into a 64-dimensional space, and the number of hidden units dω of the standard GRU is validated
between {64, 128}. To avoid increasing complexity during training this hyperparameter is validated
globally, and not individually for each dedicated encoding function. Similarly, the dimension dω of
the feedforward encoders is validated between {64, 128}.
D Experimental Section: Numerical and Visual Information
D. 1 Datasets
Blood Glucose This constitutes the main benchmark data set of our experiments. We run
experiments in this data set for the time horizon prediction problem, as the preprocessed data set
is free of missing values. We also run experiments in this data set for the next observed value
prediction problem. We assume the existence of a probability distribution P(δ) that determines
the time gap δ, measured in sampling periods, between two observed values. This probability
distribution is chosen so as to mimic the presence of blocks of consecutive missing values, which is
the main driving factor in the design of all above-mentioned methods. Thus, we draw values from
an exponential distribution P(δ)=古 exp (一 δ-1) (with δ ≥ 1) to determine the time gap in terms
of sampling periods from one observation to the next one. Thus, all intermediate observations are
dropped. The hyperparameter β controls the average time gap between pairs of observations, the
larger the value is, the larger the average time gap between observations is.
Information about time gap values between consecutive observations is displayed in Figure 6. While
in the blood glucose data sets these values have been perfectly generated by an exponential distribu-
tion, one can observe the time gaps for the air pollution data sets are more irregularly distributed.
Following Fox et al. (2018), where blood glucose measurements were split into chunks of 101 data
points, we also split the air pollution time series into chunks of 96 measurements (4 days).
Statistics of the data sets can be found in Table 5.
13
Under review as a conference paper at ICLR 2020
Figure 6: Histogram of time gap values between consecutive non-missing observations.
105
104
103
102
101
100
Data set	# Observed Values [%]	# Missing Values [%]	Range	Sampling Period
Blood Glucose	616,043 [100%]	0[0%]	40 - 400	5 minutes
Blood Glucose (β = 1)	372,404 [61%]	243,639 [39%]	40 - 400	5 minutes
Blood Glucose (β = 5)	106,975 [18%]	509,068 [82%]	40 - 400	5 minutes
Air Pollution - PM2.5	273,553 [87%]	41,807 [13%]	1-1,000	1 hour
Air Pollution - PM10	173,243 [55%]	142,117 [45%]	5 - 1,000	1 hour
Table 5: Data set statistics.
D.2 Experiments
For the Blood Glucose (β = 5) data set, we show the correlation matrices of the representations
learned for a range of values of the input and time gap signals in Figure 7. For small values of the
input signal the respective encoding function learns dedicated representations, whereas for larger
values the representations are more correlated. Interestingly, the correlation matrix of the time
gap representations can be divided into 3 block matrices: one for short-term predictions (up to 10
minutes), one for mid-term predictions (from 10 to 50 minutes) and one for long-term predictions
(from 50 minutes onwards).
5	35	70	105	140	175
δF [minutes]
--0.50
--0.75
—L-1.00
1.00
0.75
0.50
0.25
0.00
-0.25
Figure 7: (Left) Correlation matrix of the representations learned by the encoding functions fgoru
and (Right) fgδrFu in Blood Glucose (β = 5).
In both correlation matrices one may observe some abrupt transitions. As an example, for values
of the input signal before and after 200 mg/dL the learned representations are negatively corre-
lated. Similarly, when the time gap between two observations takes values before and after 50
minutes—this amounts to 10 once normalized by the sampling period—the correlation is relatively
small. Interestingly, in both cases this occurs when there is a change in the leftmost digit of the
value. Therefore, one may wonder whether the representations learned by the encoding function
fgru are driven by the atomic units the numerical data is decomposed into. We run experiments
where numerical data is converted into hexadecimal and decomposed as a sequence of elements of
the vocabulary {0- 9, a-f}. Decimal and hexadecimal notation schemes are compared in Table 6 in
all blood glucose problems.
The correlation matrices obtained with a hexadecimal notation are depicted in Figure 8. While
visually similar to those of Figure 7, one may observe some differences: the correlation matrix for
the input signal is very smooth, being seemingly unaffected by changes in the leftmost digit; while
14
Under review as a conference paper at ICLR 2020
^^^^^Notation Data ^^^^^^		Decimal	Hexadecimal
Blood Glucose (Horizon Prediction)	3.86 | 7.20 =	3.89 | 7.25 二
Blood Glucose (β = 1)	2.06 | 3.97	2.02 | 3.88
Blood Glucose (β = 5)	5.51 *1.29—	5.61 | 10.92—
Table 6: Blood Glucose: Dise-gru [R]’s performance with decimal and hexadecimal notation.
the correlation matrix for the time gap signal also presents 3 block matrices, the transition from the
second to the third block is much smoother than that of its counterpart with a decimal notation.
δF [minutes]
o [mg/dL]
Figure 8: (Left) Correlation matrix of the representations learned by the encoding functions fgoru
and (Right) fgδrFu in Blood Glucose (β = 5). Numerical data is converted into hexadecimal.
Experiments in the air pollution data sets have shown that timestamps have a large positive impact
to model abrupt changes in the bias of the signal. The correlation matrices of the representations
learned for the timestamps ta are shown in Figure 9. Most of the values in the correlation matrix
for the particulate PM2.5 are in the range between 0.95 and 1. On the other side, the correlation
matrix for PM2.5-peak is directly interpretable and matches the prior knowledge we have of the data.
The encoding function fgtru learns dedicated representations for the 2nd and 15th day of the month,
which will serve to allocate some dimensions of the input and state representation to handle this
property of the signal.
01∕12h
04∕12h
28∕12h
31∕12h
_o 19∕12h
1^22∕12h
Q
25∕12h
07∕12h
⊂ 10∕12h
E
0 13∕12h
力 16∕12h
Air Pollution - PM2.5
01∕12h
04∕12h
07∕12h
10∕12h
13∕12h
16∕12h
19∕12h
22∕12h
25∕12h
28∕12h
31∕12h
Figure 9: (Left) Correlation matrix of the representations learned by the encoding function fgtru in
PM2.5 and (Right) PM2.5-peak.
15