Under review as a conference paper at ICLR 2020
The divergences minimized by non-saturating
GAN training
Anonymous authors
Paper under double-blind review
Ab stract
Interpreting generative adversarial network (GAN) training as approximate diver-
gence minimization has been theoretically insightful, spurred discussion, and lead
to theoretically and practically interesting extensions such as f-GANs and Wasser-
stein GANs. In this paper we show that the widely used “non-saturating” training
scheme can also be interpreted in this way, specifically as minimizing a particular
reverse KL-like f-divergence. We also develop a number of theoretical tools to
help compare and classify f-divergences. We hope these results may help to clar-
ify some of the theoretical discussion surrounding the divergence minimization
view of GAN training.
1 Introduction
Generative adversarial networks (GANs) (Goodfellow et al., 2014) have enjoyed remarkable
progress in recent years, producing images of striking fidelity, resolution and coherence (Karras
et al., 2018; Miyato et al., 2018; Brock et al., 2018; Karras et al., 2019). There has been much
progress in both theoretical and practical aspects of understanding and performing GAN training
(Nowozin et al., 2016; Arjovsky & Bottou, 2017; Arjovsky et al., 2017; Mescheder et al., 2018;
Gulrajani et al., 2017; S0nderby et al., 2017; Miyato et al., 2018; Karras et al., 2018; Brock et al.,
2018; Karras et al., 2019).
One of the key considerations for GAN training is the scheme used to update the generator and critic.
A rich avenue of developments has come from viewing GAN training as divergence minimization.
Goodfellow et al. (2014) showed the conventional GAN training can be viewed as approximately
minimizing the Jensen-Shannon divergence. f-GANs (Nowozin et al., 2016) approximately min-
imize f-divergences such as reverse KL in a principled way. Wasserstein GANs (Arjovsky et al.,
2017) approximately minimize the Wasserstein metric, and combine solid theoretical underpinnings
with strong practical results. Nevertheless a relatively unprincipled “non-saturating” scheme (Good-
fellow et al., 2014) has continued to obtain groundbreaking results (Karras et al., 2019) and remains
a state-of-the-art approach (Lucic et al., 2018).
The effect of the non-saturating scheme on training dynamics, and in particular whether it can be
viewed as divergence minimization, has been source of discussion and some confusion since the
original formulation of GAN training (Goodfellow et al., 2014). The main result of this paper is to
ShoW that the non-saturating scheme approximately minimizes the f-divergence 4 KL( 1P + 2q ∣∣ p),
which we refer to as the softened reverse KL divergence (§6). This puts non-saturating training on a
similar footing to Wasserstein GANs as a theoretically sound approach With strong empirical results.
We also discuss hoW our results relate to previous attempts at this problem and attempt to clarify
some of the confusion surrounding the divergence minimization vieW of non-saturating training.
In order to better understand the qualitative behavior of different divergences such as softened re-
verse KL, We develop several tools. We shoW hoW to Write f-divergences in a symmetry-preserving
Way, alloWing easy visual comparison of f-divergences in a Way that reflects their qualitative prop-
erties (§7). We develop a rigorous formulation of tail weight Which generalizes the notions of mode-
seeking and covering behavior (§8). Using these tools We shoW that the softened reverse KL di-
vergence is fairly similar to the reverse KL but very different to the Jensen-Shannon divergence
approximately minimized by the original GAN training scheme.
1
Under review as a conference paper at ICLR 2020
2 Previous discussion of non-saturating gradients
The precise practical effect of the non-saturating scheme and whether it can be motivated in a prin-
cipled way have been a source of discussion and some confusion. In this section we review previous
attempts to view non-saturating gradients as a form of divergence minimization.
The original GAN paper claims that, compared to the saturating training scheme based on the
Jensen-Shannon divergence, the non-saturating training scheme “results in the same fixed point
of the dynamics of G and D but provides much stronger gradients early in learning.” (Goodfellow
et al., 2014, Section 3). It is true that the original and non-saturating generator gradients give the
same final result in the non-parametric case where q is unrestricted, but this is fairly trivial since
both gradients lead to q = p, as do all divergences. It is even true that the dynamics of training are
essentially the same for the original and non-saturating gradients when q ≈ p, but again this is fairly
trivial since all f-divergences agree in this regime, as discussed in §3. However the “fixed point of
the dynamics” is certainly not the same in the general case of parametric q (see §G for an empirical
demonstration). Our results provide a precise way to view the relationship between saturating and
non-saturating generator gradients: They are optimizing different f-divergences.
The original f-GAN paper presents a simple argument that the “non-saturating” training scheme has
the same fixed points and that the original and non-saturating generator gradients have the same
direction (Nowozin et al., 2016, Section 3.2)1. However this argument is erroneous. It is true that
if P ≈ q then (f *)0(f0(u)) is approximately 1 everywhere, and so the original and non-saturating
generator gradients are approximately equal, but this is true of any f-divergence. There is no guar-
antee that the regime p ≈ q will ever be approached in the general case where q belongs to a
parametric family, it is not the case that the original and non-saturating generator gradients point in
approximately the same direction in general (see §G for an empirical demonstration). In fact, the
non-saturating form of generator gradient can have completely different qualitative behavior. For
example, we show that the non-saturating KL scheme in fact optimizes reverse KL.
A recent paper showed experimentally that the non-saturating generator gradient can successfully
learn a distribution in a case where optimizing Jensen-Shannon divergence should fail, and used this
to argue that perhaps itis not particularly helpful to view GANs as optimizing Jensen-Shannon diver-
gence (Fedus et al., 2018). The divergence optimized in practice for parametric critics is not exactly
the divergence which would be optimized by the theoretically optimal critic, and this distinction
seems particularly important in the situation where p and q initially have non-overlapping support.
However the fact that non-saturating training is not optimizing Jensen-Shannon is also highly rele-
vant to this discussion, since the gradient in the limit of zero noise is zero for Jensen-Shannon but
sizeable for softened reverse KL. Thus the success of non-saturating GAN training in practice may
be as much due to its optimizing a different divergence as it is to using an inexact critic.
Arjovsky and Bottou correctly recognize that the non-saturating generator gradient results in ap-
proximately minimizing a different objective function and derive the function for classic GANs
(Arjovsky & Bottou, 2017, Section 2.2.2). The objective function there is expressed as
KL(qkp)-2JS(p,q)	(1)
which is a slightly convoluted form of the expression 2 KL( 1P +1 q ∣∣ P) we derive below. The paper
suggests the negative sign of the second term is “pushing for the distributions to be different, which
seems like a fault in the update”, whereas our expression for the divergence makes it clear that this
is not an issue.
Poole et al. (2016) present a very similar view to that presented in this paper, including recognizing
that the generator and critic may be trained to optimize different f-divergences and interpreting
the classic non-saturating generator gradient as a hybrid scheme of this form where the generator
gradient is based on a new f-divergence (Poole et al., 2016). However the f-divergence derived
there is f(u) = log(1 + u-1), which differs from (50) by a factor of u + 1. We refer to this as
the improved generator objectives for GANs (IGOG) divergence. It can be written as Df (P, q)
2 KL(m k P) + KL(Pk m) where m = 1P + 2q. It has f 00(u) = u-2 一 (1 + u)-2
2u+1
(1+u)2u2 ,
and has (2, 0) tail weights. Figure 3 shows that this divergence is qualitatively quite similar to the
softened reverse KL but is not identical. The source of the discrepancy between our results and
theirs is matching the value instead of the gradient, and is described in detail in §A.
1Only in the final NIPS version of the paper, not the arxiv preprint.
2
Under review as a conference paper at ICLR 2020
3 The family of f-divergences
We start by reviewing the definition of an f-divergence (Ali & Silvey, 1966) and establishing some
basic properties. These properties are described in more detail in §B. Throughout the paper we use
the convention that p is the “true” distribution and q is a model intended to approximate p.
Given a strictly convex twice continuously differentiable function f : R>0 → R with f(1) = 0, the
f -divergence between probability distributions with densities2 p and q over RK is defined as:3
Df (p, q) =	q(x)f
dx
(2)
f-divergences satisfy several mathematical properties. Firstly Df is linear in f . Secondly
Df (p, q) ≥ 0 for all distributions p and q with equality iff p = q. This justifies referring to Df
as a divergence. Df is completely determined by f00. As we will see, the algebraic form of f00 is
often simpler than that of f. All f-divergences agree up to an overall scale factor on the divergence
between nearby distributions: If p ≈ q then Df (p, q) ≈ f00(1) KL(p k q) (see §B). This can also
be seen in Figure 2, where all f-divergences approximately overlap near zero. If f0(1) = 0 and
f00(1) = 1 then we say f is in canonical form. We can always find such an f by appropriately
scaling Df. Using canonical form removes a superficial difference in scaling between different
f-divergences, making them easier to compare, e.g. in Figure 2.
The definition (2) appears to be quite asymmetric in how it treats p and q , but it obeys a particular
symmetry (Reid & Williamson, 2011). Let fR(u) = uf (u-1). Then fR0 (u) = f(u-1)-u-1f0(u-1)
and so fR00(u) = u-3f00(u-1). It is easy to verify that DfR (p, q) = Df (q, p). With A = {x : q(x) >
p(x)} and B = {x : q(x) < p(x)}, we have
Df (p, q) =	q(x)f
A
dx +
B
p(x)fR
(3)
This is more explicitly symmetric than (2) in the role of p and q. We refer to A as the set of left
mismatches (q > p), and B as the set of right mismatches (q < p). At each point in A, the two
distributions p and q are somewhat mismatched, and the penalty paid for this mismatch in terms of
the overall divergence Df is governed by the behavior of f(u) for 0 < u < 1 (the “left” of the graph
of f). Similarly the penalty paid for right mismatches is governed by f(u) for u > 1. Note from (3)
that a left mismatch can only be heavily penalized if the point is plausible under q, i.e. q(x) is not
tiny. Similarly a right mismatch can only be heavily penalized for points plausible under p.
4 Variational divergence estimation
f-GANs are based on an elegant way to estimate the f-divergence between two distributions given
only samples from the two distributions (Nguyen et al., 2010). In this section we review this ap-
proach to variational divergence estimation. See §E for details on how our derivation and notation
relates to that of Nowozin et al. (2016).
There is an elegant variational bound on the f-divergence Df (p, q) between two densities p and q.
Since f is strictly convex, its graph lies at or above any of its tangent lines and only touches in one
place. That is, for k, u > 0,
f(k) ≥ f(u) + (k - u)f0(u) = kf0(u) - uf0 (u) - f (u)	(4)
with equality iff k = u. This inequality is illustrated in the appendix in Figure 4. Substituting
p(x)/q(x) for k and u(x) for u, for any continuously differentiable function u : RK → R>0 we
obtain
Df (p, q) ≥
p(x)f 0 (u(x)) dx -	q(x) u(x)f 0 (u(x)) - f (u(x)) dx
(5)
2Most results also hold for “discrete” probability distributions. The only difference is that the reparameter-
ization trick can no longer be used to reduce variance of the finite sample approximations.
3For simplicity, we assume the probability distributions are suitably nice, e.g. absolutely continuous with
respect to the Lebesgue measure on RK, p(x), q(x) > 0 for x ∈ RK, andp and q continuously differentiable.
3
Under review as a conference paper at ICLR 2020
with equality iff U = u*, where u*(x) = p(x)∕q(x). The function U is referred to as the critic. It
will be helpful to have a concise notation for this bound. Writing u(x) = exp(d(x)) without loss of
generality, for any continuously differentiable function d : RK → R, we have
Df (p, q) ≥ Ef (p, q, d)	(6)
with equality iff d = d*, where
Ef (P, q, d) =	P(x)af(d(x))dx	-	q(x)bf (d(x)) dx	(7)
af (d) =	f0(exp(d))		(8)
bf (d) =	exp(d)f0(exp(d)) -	f (exp(d))	(9)
d* (x) =	log P(x) - log q(x)		(10)
Note that both af and bf are linear in f. Their derivatives a0f (log U) = Uf00(U) and b0f (log U) =
U2f00(U) depend on f only through f00.
The bound (6) leads naturally to variational divergence estimation. The f -divergence between p and
q can be estimated by maximizing Ef with respect to d (Nguyen et al., 2010). Conveniently Ef is
expressed in terms of expectations and may be approximately computed and maximized with respect
to d using only samples from p and q. If we parameterize d as a neural net dν with parameters ν
then we can approximate the divergence by maximizing Ef (p, q, dν) with respect to ν. This does
not compute the exact divergence because there is no guarantee that the optimal function d* lies in
the family {dν : ν} of functions representable by the neural net, but we hope that for sufficiently
flexible neural nets the approximation will be close.
Here we briefly summarize the three main f-divergences we consider. The Kullback-Leibler (KL)
divergence KL(p k q) has f00(U) = U-1, af (d) = d and bf (d) = exp d - 1. It has (1, 2)
tail weights and is left-bounded and right-unbounded. The reverse KL divergence KL(q k p) has
f00(U) =	U-2,	af (d)	= exp(-d)	- 1 and	bf (d)	=	d.	It has	(2, 1) tail weights and is left-
unbounded and right-bounded. Finally the canonicalized Jensen-Shannon divergence 4 JS(p, q) =
2KL(p k m) + 2KL(q ∣∣ m), where m = 1 P+1 q, has f00(U) = UU+i), af (d) = 2log σ(d)+2log2
and bf (d) = -2 log σ(-d) - 2log 2. It has (1, 1) tail weights and is bounded. See §D for details.
5	Variational divergence minimization
f-GANs (Nowozin et al., 2016) generalize classic GANs to allow approximately minimizing any
f-divergence. In this section we briefly review and discuss the f-GAN formulation.
Consider the task of estimating a probabilistic model from data using an f-divergence. Here p is the
true distribution and the goal is to minimize l(λ) = Df (p, qλ) with respect to λ, where λ 7→ qλ is
a parametric family of densities over RK . We refer to qλ as the generator. For implicit generative
models such as typical GAN generators, the distribution qλ is the result ofa deterministic transform
Xλ(z) of a stochastic latent variable z. However We do not need to assume this specific form for
most of our discussion.
We first note that the variational divergence bound Ef satisfies a convenient gradient matching
property. This is not made explicit in the original f-GAN paper. Denote the optimal d given p and
qλ by d*λ. We saw above that Df (p, qλ) and Ef (p, qλ, d) match values at d = d*λ. They also match
gradients: From the definitions of Df and Ef we can verify that they have the same gradient with
respect to the generator parameters λ:
∂∂
∂λ Df(P, qλ) = ∂λ Ef(P, qλ, d)
bf (d*λ (x)) dx
(11)
We can minimize Df (P, qλ) using variational divergence minimization, maximizing Ef (P, qλ, dν)
with respect to ν while minimizing it with respect to λ. Adversarial optimization such as this lies at
4
Under review as a conference paper at ICLR 2020
the heart of all flavors of GAN training. Define λ and V as
-	∂	「/	7、
λ = - ∂λ Ef(P, qλ, dν) =
—	∂7、
V	=∂ν Ef (p, qλ,dν)
qλ (x) bf (dν (x)) dx
(12)
(13)
To perform the adversarial optimization, We can feed λ and V (or in practice, stochastic approxi-
mation to them) as the gradients into any gradient-based optimizer designed for minimization, e.g.
stochastic gradient descent or ADAM. There is a simple generalization of the above training proce-
dure, Which is to base the generator gradients on Ef but the critic gradients on Eg for a possibly
different function g (Poole et al., 2016, Section 2.3). We refer to this as using hybrid (f, g) gradients.
This also approximately minimizes Df. See §F for more details.
When training classic GANs in practice, an alternative non-saturating loss is used as the basis for
the generator gradient, and is found to perform much better in practice (GoodfelloW et al., 2014).
This issue has been discussed in detail previously, so We just give a summary here and discuss in
more detail in §F. Early on in training, the generator and data distribution are typically not Well
matched, With samples from p being very unlikely under q and vice versa. This means most of the
probability mass of p and q is in regions Where d has large magnitude, corresponding to the positive
and negative tails in Figure 2 and (19). In this regime Jensen-Shannon has very flat gradient, and
it is not too surprising that this might lead to optimization issues. Similar concerns do not apply to
other f-divergences such as KL or reverse KL, but an alternative “non-saturating” generator gradient
has still been suggested for use in f-GANs (Nowozin et al., 2016). For both GANs and f-GANs
the specific change is to replace bf by af in the definition of λ in (12). We are not aware of a
particular motivation for this procedure in the case of f-GANs other than that it yields the traditional
non-saturating GAN scheme in the case of Jensen-Shannon.
6	Effect of non-saturating gradients
We now discuss the effect of the non-saturating generator gradient on training. We show that, for an
optimal critic, the non-saturating generator gradient is the gradient of a globally coherent objective
function, that this objective function is an f-divergence, and that this f-divergence is not the same as
the one optimized by using the original “saturating” gradient. We explicitly derive the divergences
optimized by the “non-saturating” KL, reverse KL and Jensen-Shannon training schemes.
We first establish our main result: “Non-saturating” training based on g is precisely equivalent to a
hybrid (f, g) scheme for some f. Consider the f-divergence Df defined by
f00(u) = u-1g00(u)	(14)
This is a valid f-divergence since f00(u) > 0. It is straightforward to verify that b0f = a0g, so
bf = ag + k, where the constant k ∈ R does not affect the (reparameterized) gradients. Since the
non-saturating gradient uses b instead of a in the definition of its generator gradient λ, an original
generator gradient using f is the same as a non-saturating generator gradient using g. Since the
critic gradient is still based on g, the overall scheme is a hybrid (f, g) one, and so approximately
minimizes Df.
We now explicitly compute the corresponding f for some common choices of g. It is easy to show
that if Dg has (R, S) tails then Df has (R+1, S- 1) tails, so the divergence effectively optimized by
non-saturating training penalizes left mismatches more strongly and right mismatches less strongly
than the original divergence. For the KL divergence, g00 (u) = u-1, so f00 (u) = u-2. We already
saw in §4 that this is the reverse KL divergence. Thus “non-saturating” training based on the KL
divergence is a hybrid (reverse KL, KL) scheme, and so in fact approximately minimizes the reverse
KL. This equivalence also follows directly from the equality of KL’s ag to reverse KL’s bf. For the
reverse KL divergence, g00(u) = u-2, so f00(u) = u-3. The corresponding f may be obtained by
integrating twice, choosing constants of integration such that f is canonical. We show in §D that
Df is the canonicalized Pearson χ2 divergence. It has (3, 0) tail weights and is left-unbounded and
right-bounded. Thus “non-saturating” training based on the reverse KL divergence is a hybrid (2χ2,
reverse KL) scheme, and so approximately minimizes the Pearson χ2 divergence. For the canonical-
ized Jensen-Shannon divergence, g00(u)= "喜),so f00(u) = 炉端+]).The corresponding f may
5
Under review as a conference paper at ICLR 2020
again be obtained by integrating twice, choosing constants of integration such that f is canonical.
We show in §D that this corresponds to Df (p, q) = 4 KL( 1P + 1 q ∣∣ p). This divergence does not
have an existing name as far as we are aware. In this paper we have termed it the softened reverse KL
(SRKL) divergence (see §C for details on this terminology). It has af (d) = -2 exp(-d) -2 log σ(d)
and bf (d) = -2 log σ(d). It has (2, 0) tail weights and is left-unbounded and right-bounded. Thus
the non-saturating training scheme described by Goodfellow et al. (2014) is a hybrid (SRKL, JS)
scheme, and so approximately minimizes the softened reverse KL.
Having derived our main result that the typical non-saturating GAN training scheme effectively
optimizes the softened reverse KL divergence, we focus on understanding the qualitative properties
of this divergence. We do this by developing some analytic tools applicable to any f-divergence.
7 Pushforwards and symmetry-preserving divergence plots
While f-divergences unify many divergences, just plotting the function f is often not informative.
The symmetric relationship between divergences such as KL and reverse KL is obfuscated, and f
may grow quickly even when the divergence is well-behaved. In this section we develop a straight-
forward and intuitive way to compare f-divergences visually through a symmetry-preserving diver-
gence plot. Our perspective also allows a simple summary of the prevalence of mismatches between
p and q, through a pushforward plot.
Firstly note that for X 〜q(x), p(x)∕q(x) is a random variable with some distribution. In fact, since
(2) is the expected value of some function of this random variable, Df (p, q) must depend only on
the one-dimensional distribution of this random variable and not on the detailed distribution ofp and
q in space. Formally the distribution of this random variable may be described as the pushforward
measure of q through the function u* (x) = p(x)∕q(x). To obtain more intuitive plots, We will work
in terms of d* (x) = logp(χ) - log q(χ) instead of u*. We denote the density of the pushforward of
q through d* by q^ (d). Rewriting the expectation in (2), we obtain
(15)
7d* (d)f (exp d) dd
Df(p,q) =
As above we can write this more symmetrically. Define
	f (exp d),	d < 0 Sf - "r (exp(-d)) ,	d > 0
By considering expectations of an arbitrary function of d expressed in x-space and d-space, we can
show that	7d* (d) = Pd* (d) exp(-d)	(17)
Thus, using (3) and (17), we can write the f-divergence as
	0∞ Df (p,q) = /	7d* (d)sf (d) dd + / Pd* (d)sf (d)dd	(18) -∞	0 =/ max {pd* (d)滴d* (d)} Sf (d) dd	(19) -∞
An f-divergence Df (p, q) involves an interaction between the distributions p, q and the function f,
and (19) nicely decomposes this interaction in terms of something that only depends on p and q
(the pushforwards) and something that only depends on f (the function sf), connected via a one-
dimensional integral. By plotting sf and imagining integrating against various pushforwards, we can
see the properties of different f-divergences in a very direct way. By plotting the pushforwards, we
can get a feel for what types of mismatch between p and q are present in multidimensional x-space,
and understand at a glance how badly these mismatches would be penalized for a given f-divergence.
Examples of pushforwards for the simple case where p and q are multidimensional Gaussians with
common covariance are shown in Figure 1. In this case the pushforwards g* and pd* are themselves
one-dimensional Gaussians (since d* is linear), with densities N(一∣σ2,σ2) and N(ɪσ2,σ2) re-
spectively, for some σ (this follows from (17)). Examples of sf for various f-divergences are shown
in Figure 2. We refer to sf as a symmetry-preserving representation of f. Note that as long as f is in
6
Under review as a conference paper at ICLR 2020
Figure 1: Plots of the pushforward densities
pd* (d) and g* (d) for the case where p and q are
multidimensional Gaussians with common co-
variance. The f-divergence for a given f may
be obtained by integrating these pushforwards
against sf in Figure 2 using (19).
Figure 2: Plots of sf (d) for various f-
divergences. The f-divergence for a given p and
q may be obtained by integrating sf against the
pushforwards of p and q such as those shown in
Figure 1 using (19). Symmetries such as that be-
tween KL and reverse KL are evident.
canonical form, sf is twice continuously differentiable at zero. Figure 2 directly expresses several
facts about divergences. It shows that left mismatches (regions of space where q(x) > p(x), corre-
sponding to d < 0) are penalized by reverse KL much more severely than right mismatches (regions
of space where q(x) > p(x), corresponding to d > 0). The symmetry between KL and reverse KL
is evident. We see that Jensen-Shannon and the Jeffreys divergence (the average of KL and reverse
KL) are both symmetric in how they penalize left and right mismatches, but differ greatly in how
much they penalize small versus large mismatches.
Applying the tools developed in this section to analyze the non-saturating variant of GAN training,
Figure 3 shows the symmetry-preserving representation sf (d) for the Jensen-Shannon and softened
reverse KL divergences, as well as the reverse KL for comparison. The qualitative behavior of
softened reverse KL is quite similar to reverse KL. As discussed in §C, softening has the potential to
make large right mismatches much less severely penalized, thus making the divergence more mode-
seeking. Here softening increases the slope of the left tail and changes the right tail behavior slightly,
but these changes are relatively minor modifications. The Jensen-Shannon is extremely different to
the reverse KL and softened reverse KL.
8 Classification of f-divergence tails
In this section we introduce a classification scheme for f-divergences in terms of their behavior for
large left and right mismatches. While different f-divergences differ in details, this classification
determines many aspects of their qualitative behavior.
First We define the notion of tail weight and examine some of its consequences. If f00(U)〜 Cu-R
as u → 0 for C > 0 and f00(u)〜DuS-3 as U → ∞ for D > 0 then we say that Df has
(Cu-R, DuS-3) tails and (R, S) tail weights. Here we have used the notation g(u)〜 h(u) as
u → a to mean g(u)/h(u) → 1 as u → a. Note that, since fR00(u) = u-3f00(u-1), f having a uS-3
right tail is equivalent to fR having a u-S left tail. Thus tail weights interact simply with symmetry:
If Df has (R, S) tail weights then DfR has (S, R) tail weights. Intuitively, the left tail weight R
determines how strongly large left mismatches are penalized compared to small mismatches (which
are penalized the same amount by every canonical f-divergence), whereas the right tail weight S
determines how strongly large right mismatches are penalized compared to small mismatches.
Some f-divergences such as Jensen-Shannon are bounded, while others such as KL are unbounded,
and it is useful to have a characterization of when boundedness occurs. We say Df is bounded if
7
Under review as a conference paper at ICLR 2020
Figure 3: Plots of sf (d) for various reverse
KL-like f-divergences. Softened reverse KL is
the divergence effectively minimized by non-
saturating GAN training. IGOG is the diver-
gence derived by Poole et al. (2016).
tail weight
boundedness
	(left, right)	(left, right)	overall
KL	—(1,2)-	(0, ∞)	∞
RKL	(2, 1)	(∞,0)	∞
Jensen-Shannon	(1, 1)	(0, 0)	0
Jeffreys	(2, 2)	(∞, ∞)	∞
Pearson χ2	(3, 0)	(∞,0)	∞
softened RKL	(2, 0)	(∞,0)	∞
IGOG	(2, 0)	(∞,0)	∞
Table 1: Tail weights and boundedness for the f-
divergences considered in this paper. For boundedness, 0
denotes bounded and ∞ denotes unbounded. A divergence
is bounded if and only if left and right tail weights are both
less than 2.
there is an M ∈ R such that Df (p, q) ≤ M for all densities p and q. We say f is left-bounded if
f is bounded on (0, 1), and right-bounded if fR is bounded on (0, 1), or equivalently if f (u)/u is
bounded on u > 1. From (3) it is easy to see that if f is left-bounded and right-bounded then Df
is bounded. The converse is also true: If f is left-unbounded or right-unbounded then we can find p
and q with arbitrarily large divergence Df (p, q). This can be seen for example by partitioning RK
into two sets A and B and considering densities p and q which are constant on A and constant on
B, or strictly speaking smooth approximations thereof. Tail weight determines boundedness. It can
be checked by integrating and bounding that a divergence with (R, S) tail weights is left-bounded
iff R < 2 and right-bounded iff S < 2. Thus Df is bounded iff R, S < 2. The tail weights and
boundedness properties of various f-divergences considered in this paper are summarized in Table 1.
Boundedness properties can also be seen in Figure 2. Left and right boundedness of f is trivially
equivalent to left and right boundedness of sf. Thus we can see that reverse KL is left unbounded
but right bounded, for example. The unbounded tails in this plot are all asymptotically linear in d.
Tail weights provide an extension of the typical classification of divergences as mode-seeking or
covering (Bishop, 2006, Section 10.1.2). Models trained with reverse KL tend to have distributions
which are more compact than the true distribution, sometimes only successfully modeling certain
modes (density peaks) of a multi-modal true distribution. Models trained with KL tend to have
distributions which are less compact than the true distribution, “covering” the true distribution en-
tirely even if it means putting density in regions which are very unlikely under the true distribution
(Bishop, 2006, Figure 10.3). However there are important qualitative aspects of divergence behav-
ior that are not captured by these labels. For example, Jensen-Shannon is neither mode-seeking nor
covering: It would be more accurate to say that a model trained using Jensen-Shannon tries to match
very closely when it matches, but doesn’t worry overly about large mismatches in either direction.
The Jeffreys divergence is also symmetric and so neither mode-seeking nor covering, but has very
different behavior from Jensen-Shannon. Tail weights capture these distinctions in a straightforward
but precise way.
Tail weights and boundedness provide an extremely concise way to see the qualitative effect of using
the non-saturating variant of GAN training. The softened reverse KL divergence effectively opti-
mized by conventional non-saturating GAN training has tail weights (2, 0), and so is unbounded,
is likely to have strong gradients starting from a random initialization where large mismatches are
present, and penalizes left mismatches strongly but tolerates large right mismatches and so is mode-
seeking. In contrast the Jensen-Shannon divergence effectively optimized by saturating GAN train-
ing has tail weights (1, 1), and so is bounded, is likely to have weak gradients in the presence of
large mismatches, and tolerates large left and right mismatches.
8
Under review as a conference paper at ICLR 2020
References
Syed Mumtaz Ali and Samuel D Silvey. A general class of coefficients of divergence of one distri-
bution from another. Journal of the Royal Statistical Society: Series B (Methodological), 28(1):
131-142,1966.
Martin Arjovsky and Lon Bottou. Towards principled methods for training generative adversarial
networks. In Proc. ICLR, 2017.
Martin Arjovsky, Soumith Chintala, and Lon Bottou. Wasserstein generative adversarial networks.
In Proc. ICML, pp. 214-223, 2017.
David Balduzzi, Sebastien Racaniere, James Martens, Jakob Foerster, Karl Tuyls, and Thore Grae-
pel. The mechanics of n-player differentiable games. In Proc. ICML, 2018.
Christopher M Bishop. Pattern recognition and machine learning. Springer, 2006.
Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity
natural image synthesis. In Proc. ICLR, 2018.
Sung-Hyuk Cha. Comprehensive survey on distance/similarity measures between probability den-
sity functions. International Journal of Mathematical Models and Methods in Applied Sciences,
2007.
William Fedus, Mihaela Rosca, Balaji Lakshminarayanan, Andrew M. Dai, Shakir Mohamed, and
Ian Goodfellow. Many paths to equilibrium: GANs do not need to decrease a divergence at every
step. In Proc. ICLR, 2018.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Infor-
mation Processing Systems, pp. 2672-2680, 2014.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Im-
proved training of Wasserstein GANs. In Advances in Neural Information Processing Systems,
pp. 5767-5777, 2017.
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for
improved quality, stability, and variation. In Proc. ICLR, 2018.
Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative
adversarial networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 4401-4410, 2019.
Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, and Olivier Bousquet. Are GANs
created equal? A large-scale study. In Advances in neural information processing systems, pp.
700-709, 2018.
Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. The numerics of GANs. In Advances in
Neural Information Processing Systems, pp. 1825-1835, 2017.
Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for GANs do
actually converge? In Proc. ICML, 2018.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization
for generative adversarial networks. In Proc. ICLR, 2018.
Vaishnavh Nagarajan and J Zico Kolter. Gradient descent GAN optimization is locally stable. In
Advances in Neural Information Processing Systems, pp. 5585-5595, 2017.
XuanLong Nguyen, Martin J Wainwright, and Michael I Jordan. Estimating divergence functionals
and the likelihood ratio by convex risk minimization. IEEE Transactions on Information Theory,
56(11):5847-5861, 2010.
9
Under review as a conference paper at ICLR 2020
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-GAN: Training generative neural sam-
plers using variational divergence minimization. In Advances in Neural Information Processing
Systems,pp. 271-279, 2016.
Wei Peng, Yuhong Dai, Hui Zhang, and Lizhi Cheng. Training GANs with centripetal acceleration.
arXiv preprint arXiv:1902.08949, 2019.
Ben Poole, Alexander A Alemi, Jascha Sohl-Dickstein, and Anelia Angelova. Improved generator
objectives for GANs. In Proc. NIPS Workshop on Adversarial Training, 2016.
Mark D Reid and Robert C Williamson. Information, divergence and risk for binary experiments.
Journal of Machine Learning Research, 12(Mar):731-817, 2011.
Casper Kaae S0nderby, Jose Caballero, Lucas Theis, Wenzhe Shi, and Ferenc Huszar. Amortised
MAP inference for image super-resolution. In Proc. ICLR, 2017.
A Further comparison to previous work
As mentioned in §2, Poole et al. (2016) present a very similar view to that presented in this paper,
including recognizing that the generator and critic may be trained to optimize different f-divergences
and interpreting the classic non-saturating generator gradient as a hybrid scheme of this form where
the generator gradient is based on a new f-divergence (Poole et al., 2016). We now discuss the
discrepancy between our result and theirs.
In the language of the present paper, Poole et al. (2016, Equation (8)) define the approximation
Df (p, q)
/ q(x)f(p(x)∕q(x))dx ≈ Ef (p, q, d)
q(x)f (exp(d(x))) dx
(20)
and ShoW that the gradients of Ef for this particular f match the non-saturating GAN gradients.
This is a valid approximation of the value, since Df (p, q) = Ef(p, q, d*) for the optimal critic
d(x) = d* (x) = logp(x) - log q(x). However the gradients are not the same: The partial derivative
of the left side of (20) With respect to the parameters ofq involves tWo terms, one for each occurrence
of q(x) in the integrand, and the partial derivative of the right side only includes one of these. Thus
it is not the case that optimizing Ef using gradient descent (while continually keeping the critical
optimal) optimizes Df .
B	Properties of f-divergences
In this section we go into more detail about some of the properties of f-divergences which were
briefly covered in the main text.
Firstly note that Df is linear f, that is Df+g = Df +Dg and Dkf = kDf where k > 0. Iff(1) = 0
and g(1) = 0 then (f + g)(1) = 0 and (kf)(1) = 0, so Df+g and Dkf are valid f-divergences.
Secondly note that adding an affine term to f(u) does not affect Df: If g(u) = f(u) + k - ku
for k ∈ R then Dg = Df. Any affine term added must be of the form k - ku in order to respect
the f(1) = 0 constraint. Thus the second derivative f00 determines the divergence completely. This
property is also true of the various bounds and finite sample approximations4 derived in this paper,
so we may legitimately consider f00 rather than f as the essential quantity of interest for a given
divergence. Working with f00 has the added advantage that for many common f-divergences f00 has
a simpler algebraic form than f. For any densities p and q we have Df (p, q) ≥ 0 with equality
iffp = q, as can be seen by plugging the constant function u(x) = 1 into (5). If f0(1) = 0 and
f00(1) = 1 then we say f is in canonical form. We can put any f in canonical form by scaling
and adding a suitable affine term, and this corresponds to a scaling of Df. Each f-divergence has a
unique canonical form.
4As long as the reparameterization trick is used to compute finite sample approximations, as is standard
practice. If a simpler finite sample approximation such as naive REINFORCE is used then k affects the variance
of the generator gradient.
10
Under review as a conference paper at ICLR 2020
Different f-divergences may behave very differently when p and q are far apart but are essentially
identical when q ≈ p. In fact the divergence between nearby distributions belonging to some family
is given by f00(1) times the Fisher metric of the family. Specifically
Df (qλ, qλ+εv) = 1 ε2f 〃⑴vTF(λ)v + O(ε3)	(21)
where ε ∈ R, V ∈ RK, and F(λ) = Px qλ(x)(∂λ log qλ(x))(∂λ logqλ(x)) is the Fisherinfor-
mation matrix for the parametric family of distributions specified by qλ . Alternatively this may be
stated in the non-parametric form
Df (q,q + εv) = 2ε2f00(1) /
丝？2 dx + O(ε3)
(22)
where v : RK → R satisfies v(x) dx = 0. Informally we may state this as:
Df (p, q)
≈ 2 f 〃⑴/如⅛户dx
p(x)
(23)
Thus all f-divergences agree up to a constant factor on the divergence between two nearby distribu-
tions, and they are all just scaled versions of the Fisher metric in this regime. This can also be seen
in Figure 2, where all f-divergences approximately overlap near zero.
C Divergence symmetrization and s oftening
We can apply some simple operations to a divergence to obtain another divergence. In this section
we consider the effect of reversing, symmetrizing and softening operations on f-divergences. Many
common f-divergences can be obtained from others in this way, and this provides a unified way of
concisely describing many f-divergences based on KL, for example.
Consider applying an operation to a divergence D(p, q) to obtain another divergence D(p, q).
We already saw the reversing operation D(p, q) = D(q, p) in §3. If D is an f-divergence
with function f(u) then DR is an f-divergence with function fR(u) = uf(u-1). In this case
用(U) = u-3f 00(u-1). Symmetrization means D(p,q) = 2D(p,q) + 1 D(q,p). If D is an f-
divergence then f → 2 f + 1 /r enacts symmetrization. Finally (q-)softening refers to replacing q
with m = 2P + 2q, i.e. D(p, q) = 4D(p, m). If D is an f-divergence with function f then setting the
new f (u) tobe 2(1+ u)f (ι2uu) enacts softening. In this case the new f00(u) is(i+up f0( ι2uu). The
factor of 4 above is to ensure that the divergence remains canonical after softening, i.e. f00(1) = 1.
Softening has the potential to make large right mismatches much less severely penalized, since in re-
gions of space where p(x)/q(x) was large because p(x) was moderate and q(x) was tiny, p(x)/m(x)
is now approximately 2, so a large right mismatch is only penalized by the softened divergence as
much as a moderate right mismatch is penalized by the original divergence. This is reflected in
the tail weights: It is easy to show using the tools we have developed above that if the original
divergence has (R, S) tail weights then the softened divergence has (R, 0) tail weights.
Many f-divergences can be written concisely as a series of these operations. For example re-
verse KL is Reverse(KL), Jeffreys is Symmetrize(KL), the canonicalized K-divergence 4 KL(p k m)
(Cha, 2007) is Soften(KL) and canonicalized Jensen-Shannon is Symmetrize(Soften(KL)). In this
terminology, the main claim of this paper is that the non-saturating procedure for GAN train-
ing is in fact effectively minimizing the softened reverse KL divergence 4 KL(m k p) given by
Soften(Reverse(KL)).
D Detailed expressions for various f-divergences
In this section we give more details of the f-divergences considered in §4 and §6. The expressions
for Df and Ef are obtained by plugging the chosen f into (2) and (7) respectively.
11
Under review as a conference paper at ICLR 2020
Figure 4: A strictly convex function f : R>0 → R and a tangent line. The variational bound used
by f-GANs is based on the fact that a strictly convex function f lies at or above its tangent lines.
The KL divergence satisfies:
f(u) = ulogu			(24)
f00(u)	u-1		(25)
Df (p, q) = KL(p k q) = Pp(x) log prxv dx q(x)			(26)
Ef (p, q, d)	1 +	p(x)d(x)dx	-	q(x) exp(d(x)) dx	(27)
af (d)	d		(28)
bf (d)	exp(d) - 1		(29)
The KL divergence has (u-1, u-1) tails, (1, 2) tail weights, and is left-bounded and right-
unbounded.
The reverse KL divergence satisfies:
f(u) f00(u)	- log u	(30) u-2	(31)
Df (p, q)	二 KL(q k P)= q q(χ) log q(x) dx	(32) p(x)
Ef(p,q,d) af (d) bf (d)	1 -	p(x) exp(-d(x)) dx -	q(x)d(x) dx	(33) 1 - exp(-d)	(34) d	(35)
The reverse KL divergence has (u-2, u-2) tails, (2, 1) tail weights, and is left-unbounded and right-
bounded.
12
Under review as a conference paper at ICLR 2020
The Jensen-Shannon divergence JS(p, q) has f00(1) = 1/4 and so is not canonical. In most of the
paper we therefore consider the canonicalized Jensen-Shannon divergence 4 JS(p, q). This satisfies:
f(u) = 2u logu - 2(u + 1) log(u + 1) + 4 log 2 2 f (u) = τ4n u(u + 1) Df (P, q) = 4 JS(P, q) =2 KL(P Il 2p + 1 q) + 2 KL(q Il 2p + 1 q) = 4log2 + 2/ P(x)lOg P(xP(Xq(x) dX + 2/ q(x)lθg P(χq(xq(χ) dx Ef(P,q,d)=4log2+2ZP(x)logσ(d(x))dx+2Zq(x)logσ(-d(x))dx af (d) = 2 log σ (d) + 2 log 2 bf (d) = -2 log σ(-d) - 2 log 2	(36) (37) (38) (39) (40) (41) (42) (43)
The canonicalized Jensen-Shannon divergence has (2u-1, 2u-2) tails, (1, 1) tail weights, and is
both left-bounded and right-bounded and so bounded overall.
The Pearson χ2 (or Kagan) divergence has f00(1) = 2 as so is not canonical. The canonicalized
Pearson χ2 divergence satisfies:
f( ∖	(U -1)2 f (u) = f00(U) = U-3 Df(PM = 1/*界 dx P(x) Ef (P, q, d) = -2 - 1 ʃP(x) exp (—2d(x)) dx + J q(x)exp (—d(x)) dx af (d) = 2 — 2 exp(-2d) bf (d) = 1 - exp(-d)	(44) (45) (46) (47) (48) (49)
The canonicalized Pearson χ2 divergence has (u-3, u-3) tails, (3, 0) tail weights, and is left-
unbounded and right-bounded. The expression for f here corrects a swapped definition in the origi-
nal f-GAN paper5 (according to the definitions of the Pearson and Neyman divergences given in the
paper, the expression given for the Pearson f is actually the Neyman f and vice versa) (Nowozin
et al., 2016). In §6 we discussed the equality of the non-saturating reverse KL generator gradient to
the conventional canonicalized Pearson χ2 generator gradient. This can be seen from (14), as we
did in §6, or directly by noting that af for the reverse KL divergence is equal to bf for the Pearson
χ2 divergence.
The softened reverse KL divergence satisfies:
u+1
f(u) = 2(u + 1) log-----4 log 2
u
2
f (U) =	2( B
u2 (u + 1)
Df (P, q) = 4KL(1P + 2q k P)
Ef(P，q,d)=2 - 4log2 + 2 / p(x) h-exp K')) - log σ W*] dx
-2 Z q(x) log σ (d(x)) dx
af (d) = 2 exp(-d) - 2 log σ (d) - 2 - 2 log 2
bf (d) = 2 log σ (d) + 2 log 2
(50)
(51)
(52)
(53)
5In the the arxiv preprint, not the final NIPS version of the paper.
13
Under review as a conference paper at ICLR 2020
The SRKL divergence has (2u-2 , 2u-3) tails, (2, 0) tail weights, and is left-unbounded and right-
bounded. In §6 we discussed the equality of the non-saturating canonicalized Jensen-Shannon gen-
erator gradient to the conventional softened reverse KL generator gradient. This can be seen from
(14), as we did in §6, or directly by noting that af for the canonicalized Jensen-Shannon divergence
is equal to bf for the softened reverse KL divergence.
E f-GAN notation
The original f-GAN paper (Nowozin et al., 2016) phrases the results presented in §4 in terms of
the Legendre transform f * of f. The two descriptions are equivalent, as can be seen by setting
T(x) = f 0(u(χ)) and using the result f * (f0(u)) = Uf(U) - f (u). We find our description helpful
since it avoids having to explicitly match the domain of f*, ensures the optimal d is the same for
all f -divergences, and because the Legendre transform is complicated for one of the divergences we
consider. An “output activation” was used in the original f-GAN paper to adapt the output d of the
neural net to the domain of f*. This is equal to f 0 (exp(d)), up to irrelevant additive constants, for
all the divergences we consider, and so our description also matches the original description in this
respect.
F More variational divergence minimization
The gradient matching property shows that performing very many critic updates followed by a single
generator update is a sensible learning strategy which, assuming the critic is sufficiently flexible and
amenable to optimization, essentially performs very slow gradient-based optimization on the true
divergence Df with respect to λ. However in practice performing a few critic updates for each
generator update, or simultaneous generator and critic updates, performs well, and it is easy to see
that these approaches at least have the correct fixed points in terms of Nash equilibria of Ef and
optima of Df , subject as always to the assumption that the critic is sufficiently richly parameterized.
Convergence properties of these schemes are investigated much more thoroughly elsewhere, for
example (Nagarajan & Kolter, 2017; Gulrajani et al., 2017; Mescheder et al., 2017; 2018; Balduzzi
et al., 2018; Peng et al., 2019), and are not the main focus here.
A similar discussion applies to hybrid schemes. Subject as always to the assumption of a richly
parameterized critic, if we perform very many critic updates for each generator update, then the d
used to compute the generator gradient will still be close to d*, and so the generator gradient will be
close to the gradient of Df, even though the path d took to approach d* was governed by g rather
than f. The fixed points of the two gradients are also still correct, and so it seems reasonable to again
use more general update schemes and we might hope for similar convergence results (not analyzed
here).
For an implicit generative model Xλ(z) where Z 〜P(z), we have
Ef(p,qλ ,d) = - / P(z)bf (dν (Xλ(z))) dz
(54)
Thus there is a b0f (d) factor in the generator gradient, and in fact this is the only way the choice
of f-divergence affects the generator gradient. For reverse KL, b0f (d) = 1, allowing the gradients
from the other factors to pass freely. Most of the contribution to the initial gradient for reverse KL is
likely to come from regions in space with large negative d due to the P(z) factor. For canonicalized
Jensen-Shannon, b0f (d) = 2σ(d), which tends to zero exponentially quickly as d → -∞ and tends
to 2 as d → ∞. Regions of space with large positive d have a tiny contribution to the gradient due to
the P(z) factor, while regions with large negative d are exponentially suppressed by b0f (d). Based on
these considerations it might be tempting to conclude that left-unboundedness is the most important
factor in being able to learn from a random initialization. A divergence with left tail weight R has
bf (d)〜exp(-d(R - 2)) so R ≥ 2 ensures that bf (d) does not decay exponentially as d → -∞.
However the case of KL shows that right-unboundedness is also capable of allowing learning. For
KL, b0f (d) = exp d, and the situation is complicated, since it exponentially magnifies gradients
from regions with large positive d, which are extremely unlikely under P(z). We know the overall
gradient can sometimes be a reasonable learning signal, since training models such as a multivariate
14
Under review as a conference paper at ICLR 2020
Figure 5: Comparing training using the saturating and non-saturating GAN generator gradients on
a toy problem. The true distribution p is a mixture of two 1D Gaussians and the model distribution
q is a single Gaussian. Contour plots show the Jensen-Shannon (JS) divergence (left), and softened
reverse KL divergence 4KL(2P + 2q ∣∣ P) (right) as a function of model parameters. Lines show
the progression of SGD-based JS training based on the original, saturating gradient and based on
the non-saturating gradient (solid for learned critic; dotted for optimal critic). The original scheme
converges to the JS divergence minimum. The non-saturating scheme, which by the results of this
paper is equivalent to a hybrid (SRKL, JS) scheme, converges to the SRKL divergence minimum as
expected.
Gaussian using KL divergence works well. However even if the expected gradient allows learning,
the stochastic approximation obtained by sampling from q is likely to have extremely large variance.
The saturation issue is sometimes presented as being specific to the loss Ef used for classic GAN
training, but the gradient matching property presented in §5 shows it is fundamental to the Jensen-
Shannon divergence. The more critic updates we perform initially, the more saturated d is on samples
from q, and the more closely the gradient of Ef with respect to λ approximates the gradient of the
true divergence Df .
The typical fix to the saturation issue is to use the non-saturating generator gradient
/ [∂dλqλ(x)
log σ (d(X)) dx=∕p(z)
∂λ log σ(d(Xλ(z))) dx
(55)
Since the gradient of log σ(d) tends to 1 as d tends to -∞, the gradient used for training is now
larger.
G	Experimental validation of mathematical result
In order to validate our mathematical conclusions we conducted a simple experiment. Training
behavior using the original and non-saturating gradients on a toy problem is shown in Figure 5. We
see that the two cases minimize different divergences, as expected based on the theoretical arguments
presented above.
15