Under review as a conference paper at ICLR 2020
Self-Supervised State-Control through
Intrinsic Mutual Information Rewards
Anonymous authors
Paper under double-blind review
Ab stract
Learning useful skills without a manually-designed reward function would have
many applications, yet is still a challenge for reinforcement learning. In this
paper, we propose Mutual Information-based State-Control (MISC), a new self-
supervised Reinforcement Learning approach for learning to control states of in-
terest without any external reward function. We formulate the intrinsic objec-
tive as rewarding the skills that maximize the mutual information between the
context states and the states of interest. For example, in robotic manipulation
tasks, the context states are the robot states and the states of interest are the states
of an object. We evaluate our approach for different simulated robotic manip-
ulation tasks from OpenAI Gym and a navigation task in the Gazebo simula-
tor. We show that our method is able to learn to manipulate the object, such
as pushing and picking up, purely based on the intrinsic mutual information re-
wards. Furthermore, the pre-trained policy and mutual information discriminator
can be used to accelerate learning to achieve high task rewards. Our results show
that the mutual information between the context states and the states of interest
can be an effective ingredient for overcoming challenges in robotic manipulation
tasks with sparse rewards. A video showing experimental results is available at
https://youtu.be/l5KaYJWWu70.
1	Introduction
Reinforcement Learning (RL) (Sutton & Barto, 1998) combined with Deep Learning (DL) (Good-
fellow et al., 2016) has led to great successes in various reward-driven tasks, such as playing video
games (Mnih et al., 2015), learning continuous control (Ng et al., 2006; Peters & Schaal, 2008;
Levine et al., 2016; Chebotar et al., 2017), navigating in complex environments (Mirowski et al.,
2017; Zhu et al., 2017), and manipulating objects (Andrychowicz et al., 2017; 2018).
Despite these successes, RL agents that learn only from reward signals differ in the manner that
humans learn. In the case of learning to manipulate objects, a human agent not only attempts to
accomplish the task but also learns to master the controllable aspects of the environment (Lake
et al., 2017). Notably, a human agent can quickly learn the correlation between its own action and
the state change of an object, even without supervision, to later use the acquired skill to manipulate
the object into the desired state.
The ability to fully autonomously learn to control the states of interest has many benefits. First,
it would make learning possible in the absence of hand-engineered reward functions or manually-
specified agent goals. It is known that designing a reward function that ensures the agent to learn the
desired behaviors is challenging (Hadfield-Menell et al., 2017). Secondly, to learn to “master” the
environment potentially helps the agent to learn to achieve goals in sparse reward settings. Thirdly,
the policy of controlling states of interest can be quickly adapted to unknown tasks. It is currently
an open challenge to design RL agents that automatically learn useful skills to control the states of
the environment, without rewards (Warde-Farley et al., 2019).
In this paper, a skill is a policy that changes the state of the environment in a consistent way. The
policy can be a single unconditioned policy (Peters & Schaal, 2008) or a latent-condionted pol-
icy (Eysenbach et al., 2019). One way to learn skills is to simultaneously train a discriminator
to discern the skill-options of the agent (Szepesvari et al., 2014) based on the states of the trajec-
tory (Eysenbach et al., 2019). In this way, the agent should be able to learn a diverse set of skills.
1
Under review as a conference paper at ICLR 2020
Figure 1: Fetch robot arm environments and a navigation task based on the Gazebo simulator:
FetchPush, FetchPickAndPlace, FetchSlide, SocialBot-PlayGround.
Another way is to learn an environment model to encourage the agent to explore the states, which
are relatively unpredictable (Houthooft et al., 2016; Pathak et al., 2017; Ha & Schmidhuber, 2018).
These methods are more focused on efficiently exploring novel states instead of controlling states of
interest.
We propose a new self-supervised reinforcement learning method, Mutual Information-based State-
Control (MISC), which is an approach that learns skills to control states of interest without any
reward signal. States of interest are the states of the environment that we are interested in. Context
states are the states of the environment excluding the states of interest. The states of interest and
the context states are given by the user before training the model. The idea of our method is to
encourage the agent to find skills that maximize the mutual information between the context states
and the states of interest. We first divide the observation states into two sets, the context states and
the states of interest. In the case of robotic manipulation tasks, see Figure 1, the context states are
the robot states; the states of interest are the states of an object. During the learning process of the
agent, a discriminator learns to evaluate the mutual information between the context states and the
states of interest. The agent receives high intrinsic rewards from the discriminator when there is high
mutual information. Our hypothesis is that if the agent learns to control the object, then the mutual
information of the agent states and the object states should be relatively high. Interestingly, this
automated RL scheme bears similarities to the self-supervised learning of feature representations in
computer vision tasks, where a neural network is trained to predict a part of the image given another
part of the same image (Doersch et al., 2015). During the process, the neural network captures the
mutual information among different parts of the image. Similarly, in our work, we want to capture
the mutual information among different sets of states.
This paper contains the following five contributions. First, we introduce a new self-supervised RL
method, Mutual Information-based State-Control, for learning to control states of interest. Secondly,
we evaluate the developed framework in the robotic simulations of OpenAI Gym and a navigation
task in Gazebo and demonstrate that MISC enables the agent to learn skills, such as reaching, push-
ing, picking up, and sliding the object without rewards. Thirdly, we show that the pre-trained policy
can be quickly adapted to the specific tasks with external sparse reward signal. Fourthly, the pre-
trained mutual information discriminator also improves the learning process of the agent, either as
intrinsic rewards or as priorities for experience replay, in addition to the sparse rewards. Finally, we
show that the learned mutual information discriminator can be transferred among different tasks and
still improves the performance.
2	Problem Formulation
In the RL setup, an agent interacts with an environment. The environment is fully observable and
includes a set of states S, a set of actions A, a distribution of initial states p(s0), transition proba-
bilities p(st+1 | st , at), a reward function r: S × A → R, and a discount factor γ ∈ [0, 1]. These
components formulate a Markov decision process represented as a tuple, (S, A, p, r, γ). A policy
π maps a state to an action, π : S → A. The goal of the agent is to maximize the accumulated
reward, i.e., the return, Rt = i∞=t γi-tri, over all episodes, which is equivalent to maximizing the
expected return, Es0 [R0|s0].
We consider robotic manipulation tasks, like the robotic simulations provided by OpenAI Gym
(Todorov et al., 2012; Plappert et al., 2018), where a robot arm attempts to manipulate an object to
a goal position via pushing, picking & placing, or sliding, as shown in Figure 1. We also consider a
navigation task, where the robot should navigate to the target, which is a ball, as shown in Figure 1.
The states of the environment s consist of positions, orientations, linear and angular velocities of all
2
Under review as a conference paper at ICLR 2020
L
ENVIRONMENT
s%+ι 〜P®+i I s⅛, at)
Sample trajectories
from environments
Discriminator estimates MI
、.」∖ from states. Update discri-
minator to maximize MI.
Learned
[FixedQ
Algorithm 1: MISC
AGENT
Qt ~ 穴θ(at I st)
MI DISCRIMINATOR
ʃ^(sɪ,ʃ, sɪ:ʃ)
Update policy to maximize
the mutualinformationreward
while not converged do
Sample initial state so 〜p(so).
for t — 1 to steps_per_episode do
Sample action a 〜 ∏θ(at | st).
Step environment st+ι 〜ρ(st+ι | st,at).
Compute Iφ(si：t+i,sc：t+i) With (Φ).
Set intrinsic reward rt = Iφ(st-t+ι, Sc：t+i)
Update policy (θ) via DDPG or SAC.
Update MI discriminator (φ) with SGD.
Figure 2: MISC Algorithm: We update the discriminator to better estimate the mutual information
(MI), and update the agent to control states of interest to have higher MI with the context states.
robot joints and of an object. We divide the states s into two sets, the robot states sr and the object
states so . For manipulation tasks, we are interested in the states of the object. Therefore, we define
the states of interest si as the object states, si = so; the context states sc as the robot states, sc = sr.
For simplicity, we represent the states of interest using only the object positions, si = (xo, yo, zo),
and the context states using only the gripper positions, sc = (xg, yg , zg).
In this paper, we focus on learning to control states of interest purely using the agent’s observations
and actions, without any reward function. Motivated by the idea that an agent capable of controlling
states of interest si to have high mutual information with its context state sc has “mastered” the
environment, we formulate the problem of learning without external supervision as one of learning
a policy πθ (at | st) with parameters θ to maximize intrinsic mutual information rewards, r =
I(Si; Sc). Here, we use upper letters, such as Si, to denote random variables and the corresponding
lower case letter, such as si, to represent the values of random variables.
3 Method
In this section, we formally describe our method, including the mutual information reward function
and the self-supervised RL framework.
3.1	Learning a Mutual Information Reward Function of States
Our method simultaneously learns a policy and an intrinsic reward function by maximizing the
mutual information between the states of interest and the context states. Mathematically, the mutual
information between the state of interest random variable Si and the context state random variable
Sc is represented as:
I(Si; Sc) = H(Si) - H(Si | Sc)
=DKL(PSiSc || PSi 0 PSc)
(1)
where PSiSc is the joint probability distribution; PSi 0 PSc is the product of the marginal distri-
butions PSi and PSc ; DKL denotes the Kullback-Leibler (KL) divergence. The first line in Equa-
tion (1) tells us that the agent should maximize the entropy of states of interest, H(Si), and concur-
rently, minimize the conditional entropy of states of interest given the context states, H(Si | Sc).
When the conditional entropy, H(Si | Sc), is small, it becomes easy to predict the states of in-
terest based on the context states. In this case, for example in robotic manipulation tasks, we can
say that the robot can control the object because it is easy to predict the location of the object Si ,
given the robot states Sc. The second line in Equation (1) gives us the mutual information in the KL
divergence form.
Mutual information is notoriously difficult to compute in real-world settings (Hjelm et al., 2019).
Motivated by Belghazi et al. (2018), we use a lower bound to approximate the mutual information
quantity, I(Si; Sc). First, we rewrite the KL formulation of mutual information objective, Equa-
3
Under review as a conference paper at ICLR 2020
tion (1), using the Donsker-Varadhan representation (Donsker & Varadhan, 1975):
I(Si; Sc) = DKL(PSiSc || PSi 0 PSc)
=SUp EPSiSc [T] TOg(EPSi0Psc [eT])
τq→R	s s	S
(2)
where the input space Ω is a compact domain of Rd, i.e., Ω ⊂ Rd; the supremum is taken over
all functions T such that the two expectations are finite. Secondly, we lower bound the mutual
information in Donsker-Varadhan representation with the compression lemma in the PAC-Bayes
literature (Banerjee, 2006), mathematically,
I(Si; Sc) = sup E
τq→R
≥ sup EP i
φ∈Φ S
⅛sisc [T] - log(EPSi 0Psc [eT])
Sc [τΦ] - log(EPSi 0Psc [eTφ D
(3)
= IΦ (Si , Sc).
The expectations in Equation (3) are estimated using empirical samples from PSi Sc and PSi 0 PSc
or by shuffling the samples from the joint distribution along the batch axis (Belghazi et al., 2018).
The mutual information reward function, r = IΦ (Si , Sc), can be trained by gradient ascent. The
statistics model Tφ is parameterized by a deep neural network with parameters φ ∈ Φ, which aims
to estimate the mutual information with arbitrary accuracy.
3.2	Mutual Information-based State-Control
We want to train an agent to “master” the states of interest in a self-supervised reinforcement learning
fashion. The agent aims to maximize the mutual information between the states of interest and the
context states. At the beginning of each episode, the agent takes actions at following a partially
random policy, such as -greedy, to explore the environment and collects trajectories into a buffer
for later replay. The trajectory contains a series of states, {s1 , s2 , . . . , sT}, where T is the time
horizon of the trajectory. Each state st consists of the states of interest sti and the context states stc,
where in robot manipulation tasks, we use the object position as sit, and the gripper position as stc.
For training the mutual information discriminator network, we first randomly sample trajectories of
states s1:T = {s1 , s2 , . . . , sT} from the replay buffer. Here, we use st:t0 to indicate a subsequence
ofa trajectory, for instance, s1:t refers to {s1, s2, . . . , st}. Then, we evaluate the mutual information
lower bound, Iφ(s1∙τ, s%), mathematically,
1T	1T i
Iφ(siι.τ, si：T) = T X Tφ(st,sC) — log T X …MC
(4)
t=1	t=1
where the states Mc are sampled by shuffling the states Sc along the temporal axis t within each
trajectory. After evaluating the lower bound, Iφ(s1T, si：T), We use back-propagation to optimize
the parameter (φ) to maximize the mutual information lower bound. Here, we calculate the mutual
information using the samples from the same trajectory. If the context states or the states of interest
do not change within the trajectory, then the mutual information of these states is zero. Therefore,
if the agent does not alter the object states during the episode, then the mutual information between
the agent states and the object states remains zero.
We define the transition mutual information reward as the mutual information estimation ofa trajec-
tory fraction, st：t+i, mathematically,
Tt = "+ι, sc：t+J = Clip(1Xτφ(st, Sc)- log(1X eτφ⑸国)),o,ιman),⑸
where Itmraanxis the predefined maximal transition mutual information value. The clip function limits
the transition mutual information value in an interval of [0, Itmraanx]. The lower threshold 0 forces
the mutual information estimation to be non-negative. In practice, to mitigate the influence of some
particular large transition mutual information, we find it useful to clip the intrinsic reward with the
threshold value Itmraanx. This clipping trick makes the training stable. The threshold value can be
considered as a hyper-parameter.
4
Under review as a conference paper at ICLR 2020
We implement MISC with both deep deterministic policy gradient (DDPG) (Lillicrap et al., 2016)
and soft actor-critic (SAC) (Haarnoja et al., 2018) to learn a policy πθ(a | s) that aims to control
the states of interest. In comparison to DDPG and SAC, the DDPG method improves the policy
in a more “greedy” fashion, while the SAC approach is more conservative, in the sense that SAC
incorporates an entropy regularizer H(A | S) that maximizing the policy’s entropy over actions.
We found empirically that DDPG works better when the agent starts learning quickly, while SAC
explores the environment more thoroughly in general.
Overall, the agent is rewarded for controlling states of interest to have higher mutual information
with its context states, which is considered to gain a degree of mastery of the environment. We
summarize the complete training algorithm in Algorithm 1 and in Figure 2.
4 Experiments
To evaluate MISC, we use the robotic manipulation environments provided by OpenAI Gym and
also a navigation task, see Figure 1 (Brockman et al., 2016; Plappert et al., 2018). First, we an-
alyze the skills learned purely with the intrinsic reward. We show that the agent is able to learn
skills, such as reaching, pushing, picking up, and sliding an object, see Figure 3. We also compare
our method with “Diversity is All You Need” (DIAYN) method (Eysenbach et al., 2019). Sec-
ondly, we show that the pre-trained models, including the policy and the MI discriminator, can be
used for improving performance in conjunction with the rewards. There are several ways to im-
prove performance, such as via policy initialization, with the MI intrinsic rewards, and using the
MI value for prioritized experience replay. Interestingly, we also show that the pre-trained MI dis-
criminator can be transferred among different tasks and still improves performance. We combine or
compare our method with other methods, including DDPG (Lillicrap et al., 2016), SAC (Haarnoja
et al., 2018), PPO (Schulman et al., 2017), DIAYN (Eysenbach et al., 2019), PER (Schaul et al.,
2016), VIME (Houthooft et al., 2016), ICM (Pathak et al., 2017), and Empowerment (Mohamed &
Rezende, 2015). Thirdly, we show some insights about how MISC rewards are distributed across a
trajectory. The detailed experimental details are shown in the Appendix. Our code is available at
https://github.com/misc-project/misc.
4.1	Analysis of Learned Skills
Question 1. What skill does MISC learn?
We tested MISC in multiple robotic manipulation tasks, including push, pick & place, and slide,
see Figure 1. In the push and slide tasks, the gripper is fixed to be closed. In the pick & place
task, the agent has the control of the gripper to be open or closed. Unlike in push and pick &
place environments, the surface of the table in the slide task has very low friction so that the object
can slide on these surface. In all the environments, the object is randomly placed on the table at
the beginning of each episode. During training, the agent only receives the intrinsic MISC reward,
see Section 3.2. In all three environments, the behavior of reaching objects emerges from the self-
supervised training, for example, as shown in Figure 3 (1st row). Furthermore, in the push and pick
& place environments, the agent learns to push the object around on the table, see Figure 3 (2nd row).
In the slide environment, the agent learns to slide the object into different directions, see Figure 3
(3rd row). Perhaps surprisingly, in the pick & place environment, the agent learns to pick up the
object from the table without any reward, which means taking full control of the object states, see
Figure 3 (4th row).
We implemented the MISC with both DDPG and SAC and ran the experiments with 5 different
random seeds. To compare DDPG+MISC and SAC+MISC, we ran 20 trials using the learned policy
in the pick & place environment with each seed. We observed that, in all 5 random seed settings,
SAC+MISC learns the picking-up skill, while DDPG+MISC learns to pick up an object in only
1 out of 5 random seed settings. Mostly, the agent learns to push, flip, or grip the object. These
observations show that the entropy bonus, H(A | S), of SAC can incorporate with MISC and helps
the agent to better explore the skill space.
Question 2. Can we use learned skills to directly maximize the task reward?
5
Under review as a conference paper at ICLR 2020
Figure 3: Manipulation skills: Without any reward, MISC learns skills for reaching, pushing,
sliding, and picking up an object. The video of the learned skills is shown at https://youtu.
be/l5KaYJWWu70. It is time-consuming to manually design reward functions that enforce these
behaviors.
We tested our method,
MISC, in the navigation task,
SocialBot-PlayGround,
which is based on the Gazebo
simulator, as shown in Fig-
ure 1. In this navigation task,
the external task reward is 1
if the agent reaches the ball,
otherwise, the task reward is 0.
We combine our method with
PPO (Schulman et al., 2017)
Epoch
Figure 4: Experimental results in the navigation task
and compare the performance with ICM (Pathak et al., 2017) and Empowerment (Mohamed &
Rezende, 2015). During training, we only use one of the intrinsic rewards such as MISC, ICM, or
Empowerment to train the agent. Then, we use the averaged task reward as the evaluation metric.
The experimental results are shown in FIgure 4 (left). The y-axis represents the mean task reward
and the x-axis denotes the training epochs. From Figure 4 (left), we can see that the proposed
method, MISC, has the best performance. The Empowerment has the second-best performance.
The intuition why MISC, I(Sc, Si), is superior to Empowerment, I(A, Si), is that, the mutual
information between the object states, Si , and the robot states, Sc, is relatively high compared to
the mutual information between agent’s action A and the state of interests Si, as shown in Figure 4
(right). Subsequently, higher mutual information reward encourages the agent to explore more
states, which have high mutual information. A theoretical connection between Empowerment and
MISC is shown in the Appendix. Furthermore, the ICM method does not enable the agent to
navigate to the ball because it only seeks novel states instead of controlling these states. A video
showing the MISC-trained agent is available at https://youtu.be/l5KaYJWWu70?t=104.
Question 3. How does MISC compare to DIAYN (Eysenbach et al., 2019)?
The most related prior work on unsupervised skill learning, DIAYN, introduces an information-
theoretic objective FDIAYN, which learns diverse discriminable skills indexed by the latent variable
6
Under review as a conference paper at ICLR 2020
9eD2SS ① OOnS Ue ①≡
FetchPush-Vl
FetchPickAndPIace-Vl
FetchSIide-Vl
FetchPush-Vl	FetchPickAndPIace-Vl
O	10	20	30	O 10	20	30	40	50
Epoch
Figure 5: Mean success rate with standard deviation: The percentage values after colon (:) rep-
resent the best mean success rate during training. The shaded area describes the standard deviation.
FetchSIide-Vl
——SAC: 19.56%
SAC+MISC-f: 20.22%
一 SAC+MISC-p: 25.9%
Z , mathematically,
FDIAYN=I(S;Z)+H(A | S) -I(A;Z | S)
=I(S;Z)+H(A| S,Z)	(6)
≥ Ez 〜p(z),s 〜∏(z) [log qφ(Z l S) ― log P(Z)] + H(A | S, Z).
The first term, I(S; Z), in the objective, FDIAYN, is implemented via a skill discriminator, which is
proven to be a variational lower bound of the original objective (Barber & Agakov, 2003; Eysenbach
et al., 2019). The skill discriminator assigns high rewards to the agent, if it can predict the skill-
options, Z, given the states, S. The second term, H(A | S, Z), is implemented through SAC
(Haarnoja et al., 2018) conditioned on skill-options (Szepesvari et al., 2014). In the DIAYN paper,
the authors mainly evaluated the algorithm on navigation and locomotion environments (Todorov
et al., 2012; Brockman et al., 2016). We can adapt DIAYN to manipulation tasks by replacing
the full states, S, with states of interest, Si, i.e., the object states, as I(Si; Z). In comparison, our
method MISC proposes to maximize the mutual information between the context states and the states
of interest, i.e., I(Sc; Si), so that the agent learns to control the states of interest in a self-supervised
fashion. These two methods can be combined as follows:
FMISC+DIAYN=I(Sc;Si)+I(Si;Z)+H(A | S,Z).	(7)
We compare MISC, DIAYN and MISC+DIAYN in the pick & place environment. For implementing
MISC+DIAYN, we first pre-train the agent with only MISC, and then fine-tune the policy with
DIAYN. After pre-training, MISC learns manipulation behaviors such as, reaching, pushing, sliding,
and picking up, as shown in Figure 3. Compared to MISC, DIAYN rarely learns to pick up the object.
We found that DIAYN does not fully control the object. The DIAYN-trained agent mostly pushes or
flicks the object with the gripper. However, the combined model, MISC+DIAYN, learns to pick up
the object and moves it to different locations, which depends on the conditioned skill-option. These
observations are shown in the video at https://youtu.be/l5KaYJWWu70?t=47. MISC
helps the agent to learn the DIAYN objective. The agent first learns to control the object with MISC,
and then discovers diverse manipulation skills with DIAYN. The combination of MISC and DIAYN
can be used for learning motion primitives via skill-conditioned policy for hierarchical reinforcement
learning (Eysenbach et al., 2019).
4.2	Accelerating Learning with MISC
Question 4. How can we use the learned skills or the trained MI discriminator to accelerate learn-
ing new tasks?
7
Under review as a conference paper at ICLR 2020
etaR sseccuS naeM
FetchPush-Vl
Fetch PickAn d Place-v1
0.35
0.30
0.25
0.20
0.15
0.10
0.05
0.∞
0.5
0.4
0.3
0.2
0.1
0.0
FetchSIide-Vl
O 10	20	30	40	50
FetchSIide-Vl
O 10	20	30	40	50
Epoch
MISC-p: 27.81%
PER: 20.91%
Figure 6: Performance comparison: We compare the MISC variants, including MISC-f, MISC-r,
and MISC-p, with DIAYN, VIME, and PER, respectively.
We investigate three ways of using the pre-trained policy and the MI discriminator to accelerate
learning in addition to the task reward.
The first method is using the MISC pre-trained policy as parameter initialization and fine-tune the
agent with rewards. We denote this variant as “MISC-f”, where “-f” stands for fine-tuning. The
second variant is to use the MI intrinsic reward to help the agent explore high mutual information
states. We name this method as “MISC-r”, where “-r” stands for rewarding. The third approach is
to use the mutual information quantity from MISC to prioritize trajectories for replay. We name this
method as “MISC-p”, where “-p” stands for prioritization.
We combined these three variants with DDPG and SAC and tested them in the robotic simulations
provided in OpenAI Gym. The environments, including push, pick & place, and slide, have a set
of predefined goals, which are represented as the red dots, as shown in Figure 1. The task for the
RL agent is to manipulate the object to the goal positions. We ran all the methods in each environ-
ment with 5 different random seeds and report the mean success rate and the standard deviation, as
shown in Figure 5. The percentage values alongside the plots are the best mean success rates during
training. Each experiment is carried out with 16 CPU-cores.
From Figure 5, we can see that all these three methods, including MISC-f, MISC-p, and MISC-r,
accelerate learning in the presence of task rewards. Among these variants, the MISC-r has the best
overall improvements. In the push and pick & place tasks, MISC enables the agent to learn in a
short period of training time, as shown in Figure 5. In the slide tasks, MISC-r also improves the
performances by a decent margin.
We also compare our methods with more advanced RL methods. To be more specific, we compare
MISC-f against parameter initialization using DIAYN (Eysenbach et al., 2019); MISC-p against
Prioritized Experience Replay (PER), which uses TD-errors for prioritization (Schaul et al., 2016);
MISC-r versus Variational Information Maximizing Exploration (VIME) (Houthooft et al., 2016).
The experimental results are shown in Figure 6. From Figure 6 (1st row), we can see that MISC-f
enables the agent to learn, while DIAYN does not. The reason is that DIAYN is more focused on
8
Under review as a conference paper at ICLR 2020
learning a diverse set of skills instead of learning to control the states. In the manipulation tasks,
controlling states is more important, therefore MISC outperforms DIAYN. In the 2nd row of Fig-
ure 6, MISC-r performs better than VIME. This result indicates that the mutual information between
states is a crucial quantity for accelerating learning. The mutual information intrinsic rewards boost
performance significantly compared to VIME. This observation is consistent with the experimental
results of MISC-p and PER, as shown in Figure 6 (3rd row), where the mutual information-based pri-
oritization framework performs better than the TD-error based approach, PER. On all tasks, MISC
enables the agent to learn the benchmark task more quickly.
4.3 Transfer Learning with MISC
Question 5. Can the learned MI discriminator be transferred to different tasks?
It would be beneficial if the Pre-	FetehPUSh-Vl	FetohSIigVl
trained MI discriminator can be trans-	ddpg： 9.75%	05	DDPg： 27.5%
ferred to new tasks and still im-
Proves the Performance (Pan et al.,
2010; Bengio, 2012). To verify this
idea, we directly aPPlied the Pre-
trained MI discriminator from the
Pick & Place environment to the Push
and slide environments, resPectively.
We denote this transferred method
——DDPG: 9.75%
DDPG+MISC-r: 94.08%
DDPG+MISC-t: 87.1%
0	10	20
DDPG+MISC-∏ 35.8%
DDPG+MISC-t: 35.52%
Epoch0	10
Figure 7: Transferred MISC
as “MISC-t”, where “-t” stands for
transfer. The MISC reward function trained in its corresPonding environments is denoted as “MISC-
r”. We comPare the Performance of DDPG baseline, MISC-r, and MISC-t. The results are shown
in Figure 7. PerhaPs surPrisingly, the transferred MISC still imProves the Performance significantly.
Furthermore, as exPected, MISC-r Performs better than MISC-t in both tasks. We can see that the
MI discriminator can be trained in a task-agnostic fashion and later utilized in unseen tasks.
4.4 Insights and More
Question 6. How does MISC distribute rewards across a trajectory?
To understand why MISC works and how MISC distributes rewards, we visualize the learned
MISC rewards, defined in Section 3.2, in Figure 8 and in a video at https://youtu.be/
l5KaYJWWu70?t=92. From Figure 8, we can observe that the mutual information reward Peaks
between the fourth and fifth frame, where the robot quickly Picks uP the object from the table.
Around the Peak reward value, the mid-
dle range reward values are correspond-	MlSar
ing to the relatively slow movement of
the object and the gripper, see the third,	∕∖
ninth, and tenth frame. When there is _______. ,	,ʌ,ʌ,^ʌ
no contact between the gripper and the . . .	-	.	„	„	„	„ 一 一
object, see the first two frames in Fig-	''	, 二 „	„	■	”	,,	■
ure 8, or the gripper holds the object ■	■	,
still, see the sixth to eighth frames, the Figure 8: MISC rewards over a trajectory
intrinsic reward remains nearly zero. From this example, we see that MISC distributes positive
intrinsic rewards, when the object states, si , has correlated changes with the robot state, sc .
Question 7. Can MISC help the agent to learn skills when there are no object states? And what
happens if there are multiple objects?
In the navigation task, we demonstrate that the agent learns to balance itself and run in a straight
line, see video at https://youtu.be/l5KaYJWWu70?t=134, when the MISC objective is
defined to maximize the mutual information between its left wheel and its right wheel.
When there are multiple objects of interest, for example ared and a blue ball on the ground. We can
define the MISC objective as follows:
FMISC =I(Sc;S1i) +I(Sc;S2i).
(8)
9
Under review as a conference paper at ICLR 2020
With this objective, Equation (8), the agent learns to reach both balls and sometimes also learns to
use one ball to hit the other ball. The results are shown in the video at https://youtu.be/
l5KaYJWWu70?t=148.
From these examples, we can see that, with different pairs of state of interests and the context state,
the agent is able to learn different kinds of skills. When there is no specific state of interests, we can
train a skill-conditioned policy corresponding to different combinations of the two sets of states and
later use the pre-trained policy for the tasks at hand.
5	Related Work
Unsupervised skill learning is a challenging topic in RL. Variational Intrinsic Control (VIC) (Gregor
et al., 2016) proposes an information-theoretic objective (Barber & Agakov, 2003) to jointly max-
imize the entropy of a set of options while keeping the options distinguishable based on the final
states of the trajectory. Recently, Eysenbach et al. (2019) introduce DIAYN, which maximizes the
mutual information between a fixed number of skill-options and the entire states of the trajectory.
Eysenbach et al. (2019) show that DIAYN can scale to more complex tasks compared to VIC and
provides a handful of low-level primitive skills as the basis for hierarchical reinforcement learning.
Another trunk of work on skill learning is based on goal-conditioned policies. Warde-Farley et al.
(2019) propose, DISCERN, a method to learn a mutual information objective between the states
and goals, which enables the agent to learn to achieve goals in environments with continuous high-
dimensional observation spaces. Based on DISCERN, Pong et al. (2019) introduce Skew-fit, which
adapts a maximum entropy strategy to sample goals from the replay buffer (Zhao & Tresp, 2019;
Zhao et al., 2019) in order to make the agent learn more efficiently in the absence of rewards. More
recently, Hartikainen et al. (2019) propose to automatically learn dynamical distances: a measure
of the expected number of time steps to reach a given goal state, which can be used as rewards for
learning to achieve new goals.
Similar to the skill learning methods, intrinsic rewards are also often used to help the agent learn
more efficiently to solve tasks. For example, Jung et al. (2011) and Mohamed & Rezende (2015) use
empowerment, which is the mutual information between states and actions, for intrinsically moti-
vated agents. A theoretical connection between MISC and empowerment is shown in the Appendix.
VIME (Houthooft et al., 2016) and ICM (Pathak et al., 2017) use curiosity as intrinsic rewards to
encourage the agents to explore the environment more thoroughly.
Based on a similar motivation as previous works, we introduce MISC, a method that uses the mutual
information between the states of interest and the context states (Doersch et al., 2015) as an intrin-
sic reward function. MISC enables the agent to learn manipulation skills without supervision. Our
method is complementary to the previous skill learning works, such as DIAYN, and can be com-
bined with them. The idea of MISC is to encourage the agent to learn to control states of interest,
as one step forward towards mastery of the environment. Inspired by previous works (Schaul et al.,
2016; Houthooft et al., 2016; Zhao & Tresp, 2018; Eysenbach et al., 2019), we additionally demon-
strate three variants, including MISC-based fine-tuning, rewarding, and prioritizing mechanisms, to
accelerate learning in the case when the task rewards are available.
6	Conclusion
This paper introduces Mutual Information-based State-Control (MISC), a self-supervised reinforce-
ment learning framework for attaining useful skills. MISC utilizes a mutual information-based the-
oretic objective to encourage the agent to control states of interest without any reward function.
In robotic manipulation tasks, MISC enables the agent to self-learn manipulation behaviors, like
reaching, pushing, picking up, and sliding an object. In the navigation task, the MISC-trained agent
learns to navigate to the target. Additionally, the unsupervised pre-trained policy and the mutual
information discriminator accelerate learning in the presence of task rewards. We evaluate three
MISC-based variants in different tasks and demonstrate a substantial improvement in learning effi-
ciency compared to state-of-the-art methods.
10
Under review as a conference paper at ICLR 2020
References
Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob
McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience re-
play. In Advances in Neural Information Processing Systems, pp. 5048-5058, 2017.
Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob McGrew, Jakub Pa-
chocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, et al. Learning dexterous
in-hand manipulation. arXiv preprint arXiv:1808.00177, 2018.
Arindam Banerjee. On bayesian bounds. In Proceedings of the 23rd international conference on
Machine learning, pp. 81-88. ACM, 2006.
David Barber and Felix V Agakov. The im algorithm: a variational approach to information maxi-
mization. In Advances in neural information processing systems, pp. None, 2003.
Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, Aaron
Courville, and R Devon Hjelm. Mine: mutual information neural estimation. In Proceedings of
the 35th International Conference on Machine Learning, pp. 531-540. PMLR, 2018.
Yoshua Bengio. Deep learning of representations for unsupervised and transfer learning. In Pro-
ceedings of ICML workshop on unsupervised and transfer learning, pp. 17-36, 2012.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
Yevgen Chebotar, Mrinal Kalakrishnan, Ali Yahya, Adrian Li, Stefan Schaal, and Sergey Levine.
Path integral guided policy search. In Robotics and Automation (ICRA), 2017 IEEE International
Conference on, pp. 3381-3388. IEEE, 2017.
Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by
context prediction. In Proceedings of the IEEE International Conference on Computer Vision, pp.
1422-1430, 2015.
Monroe D Donsker and SR Srinivasa Varadhan. Asymptotic evaluation of certain markov process
expectations for large time, i. Communications on Pure and Applied Mathematics, 28(1):1-47,
1975.
Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need:
Learning skills without a reward function. In International Conference on Learning Representa-
tions, 2019. URL https://openreview.net/forum?id=SJx63jRqFm.
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1.
MIT press Cambridge, 2016.
Karol Gregor, Danilo Jimenez Rezende, and Daan Wierstra. Variational intrinsic control. arXiv
preprint arXiv:1611.07507, 2016.
David Ha and Jurgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In Proceedings of the 35th
International Conference on Machine Learning, pp. 1861-1870. PMLR, 2018.
Dylan Hadfield-Menell, Smitha Milli, Pieter Abbeel, Stuart J Russell, and Anca Dragan. Inverse
reward design. In Advances in neural information processing systems, pp. 6765-6774, 2017.
Kristian Hartikainen, Xinyang Geng, Tuomas Haarnoja, and Sergey Levine. Dynamical distance
learning for unsupervised and semi-supervised skill discovery. arXiv preprint arXiv:1907.08225,
2019.
R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam
Trischler, and Yoshua Bengio. Learning deep representations by mutual information estima-
tion and maximization. In International Conference on Learning Representations, 2019. URL
https://openreview.net/forum?id=Bklr3j0cKX.
11
Under review as a conference paper at ICLR 2020
Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. Vime:
Variational information maximizing exploration. In Advances in Neural Information Processing
Systems ,pp.1109-1117, 2016.
Tobias Jung, Daniel Polani, and Peter Stone. Empowerment for continuous agent—environment
systems. Adaptive Behavior, 19(1):16-39, 2011.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Alexander S Klyubin, Daniel Polani, and Chrystopher L Nehaniv. Empowerment: A universal agent-
centric measure of control. In 2005 IEEE Congress on Evolutionary Computation, volume 1, pp.
128-135. IEEE, 2005.
Alexander Kraskov, Harald Stogbauer, and Peter Grassberger. Estimating mutual information. Phys-
ical review E, 69(6):066138, 2004.
Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building
machines that learn and think like people. Behavioral and brain sciences, 40, 2017.
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuo-
motor policies. The Journal of Machine Learning Research, 17(1):1334-1373, 2016.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In Inter-
national Conference on Learning Representations, 2016.
Piotr Mirowski, Razvan Pascanu, Fabio Viola, Hubert Soyer, Andrew J Ballard, Andrea Banino,
Misha Denil, Ross Goroshin, Laurent Sifre, Koray Kavukcuoglu, et al. Learning to navigate in
complex environments. In International Conference on Learning Representations, 2017.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. Nature, 518(7540):529, 2015.
Shakir Mohamed and Danilo Jimenez Rezende. Variational information maximisation for intrinsi-
cally motivated reinforcement learning. In Advances in neural information processing systems,
pp. 2125-2133, 2015.
Andrew Y Ng, Adam Coates, Mark Diel, Varun Ganapathi, Jamie Schulte, Ben Tse, Eric Berger, and
Eric Liang. Autonomous inverted helicopter flight via reinforcement learning. In Experimental
Robotics IX, pp. 363-372. Springer, 2006.
Sinno Jialin Pan, Qiang Yang, et al. A survey on transfer learning. IEEE Transactions on knowledge
and data engineering, 22(10):1345-1359, 2010.
Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration
by self-supervised prediction. In International Conference on Machine Learning (ICML), volume
2017, 2017.
Jan Peters and Stefan Schaal. Reinforcement learning of motor skills with policy gradients. Neural
networks, 21(4):682-697, 2008.
Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Pow-
ell, Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder, et al. Multi-goal reinforce-
ment learning: Challenging robotics environments and request for research. arXiv preprint
arXiv:1802.09464, 2018.
Vitchyr H Pong, Murtaza Dalal, Steven Lin, Ashvin Nair, Shikhar Bahl, and Sergey Levine. Skew-
fit: State-covering self-supervised reinforcement learning. arXiv preprint arXiv:1903.03698,
2019.
Christoph Salge, Cornelius Glackin, and Daniel Polani. Empowerment-an introduction. In Guided
Self-Organization: Inception, pp. 67-114. Springer, 2014.
12
Under review as a conference paper at ICLR 2020
Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. In
International Conference on Learning Representations, 2016.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction, volume 1. MIT
press Cambridge, 1998.
Csaba Szepesvari, Richard S Sutton, Joseph Modayil, Shalabh Bhatnagar, et al. Universal option
models. In Advances in Neural Information Processing Systems, pp. 990-998, 2014.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pp. 5026-
5033. IEEE, 2012.
David Warde-Farley, Tom Van de Wiele, Tejas Kulkarni, Catalin Ionescu, Steven Hansen, and
Volodymyr Mnih. Unsupervised control through non-parametric discriminative rewards. In In-
ternational Conference on Learning Representations, 2019. URL https://openreview.
net/forum?id=r1eVMnA9K7.
Rui Zhao and Volker Tresp. Energy-based hindsight experience prioritization. In Proceedings of the
2nd Conference on Robot Learning, pp. 113-122, 2018.
Rui Zhao and Volker Tresp. Curiosity-driven experience prioritization via density estimation. arXiv
preprint arXiv:1902.08039, 2019.
Rui Zhao, Xudong Sun, and Volker Tresp. Maximum entropy-regularized multi-goal reinforcement
learning. In Proceedings of the 36th International Conference on Machine Learning, pp. 7553-
7562. PMLR, 2019.
Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph J Lim, Abhinav Gupta, Li Fei-Fei, and Ali
Farhadi. Target-driven visual navigation in indoor scenes using deep reinforcement learning. In
2017 IEEE international conference on robotics and automation (ICRA), pp. 3357-3364. IEEE,
2017.
13
Under review as a conference paper at ICLR 2020
A Connection to Empowerment
We divide the states S into the states of interest Si and the context states Sc . In robot manipulation
tasks, we define the states of interest as the object states, and the context states as the gripper states
of the robot. The action space is the change of the gripper position and the status of the gripper, such
as open or closed. Note that, the agent’s actions directly control the context states.
Here, given the assumption that the transform, Sc = F (A), from the actions, A, to the context
states, Sc, is a smooth and uniquely invertible mapping (Kraskov et al., 2004), then we can prove
that the MISC objective, I(Sc, Si), is equivalent to the empowerment objective, I(A, Si).
The empowerment objective (Klyubin et al., 2005; Salge et al., 2014; Mohamed & Rezende, 2015)
is defined as the channel capacity in information theory, which means the amount of information
contained in the actions A about the states S, mathematically:
E=I(S,A).	(9)
Here, we replace the states variable S with sates of interest Si , we have the empowerment objective
as follows,
E =I(Si,A).	(10)
Theorem 1. The MISC objective, I(Sc, Si), is equivalent to the empowerment objective, I(A, Si),
given the assumption that the transform, Sc = F (A), is a smooth and uniquely invertible mapping:
I(Sc, Si) = I(A, Si)
(11)
where Si , Sc, and A denotes the states of interest, the context states, and the actions, respectively.
Proof.
I ISCm = Z /dsc dsip(sc,si )log ppSs⅛⅜
dsCdsi
∂A
∂Sc
i i、ι Il∂dAc∖∖p(a,si)
PaS )log Il 翳∖ p(a)p(si)
f f」C」i T r c∖ r i、i	JA(SDp(a,si
]]	JA(S)P(a，S)g JA(SC)p(a)p(si)
(12)
Z Zdada焉志
I(A,Si)
□
14
Under review as a conference paper at ICLR 2020
B	Experimental Details
The experiments of the robotic manipulation tasks in this paper use the following hyper-parameters:
•	Actor and critic networks: 3 layers with 256 units each and ReLU non-linearities
•	Adam optimizer (Kingma & Ba, 2014) with 1 ∙ 10-3 for training both actor and critic
•	Buffer size: 106 transitions
•	Polyak-averaging coefficient: 0.95
•	Action L2 norm coefficient: 1.0
•	Observation clipping: [-200, 200]
•	Batch size: 256
•	Rollouts per MPI worker: 2
•	Number of MPI workers: 16
•	Cycles per epoch: 50
•	Batches per cycle: 40
•	Test rollouts per epoch: 10
•	Probability of random actions: 0.3
•	Scale of additive Gaussian noise: 0.2
All hyper-parameters are described in greater detail at this link https://github.com/
misc-project/misc/tree/master/params.
15