Under review as a conference paper at ICLR 2020
Global graph curvature
Anonymous authors
Paper under double-blind review
Ab stract
Recently, non-Euclidean spaces became popular for embedding structured data.
However, determining suitable geometry and, in particular, curvature for a given
dataset is still an open problem. In this paper, we define a notion of global graph
curvature, specifically catered to the problem of embedding graphs, and analyze
the problem of estimating this curvature using only graph-based characteristics
(without actual embedding the graph). We show that optimal curvature essen-
tially depends on the dimensionality of the embedding space and loss function
one aims to minimize via embedding. We review existing notions of local cur-
vature (e.g., Ollivier-Ricci curvature) and analyze their properties theoretically
and empirically. In particular, we show that such curvatures are often unable to
properly estimate the global one. We also make an important observation: larger
dimensionality usually leads to less negative optimal curvature. We propose a
new estimator of global graph curvature which, in contrast to the existing ones,
reflects this property as it is dimension-specific. We demonstrate that for a given
dimension the new estimator is able to distinguish between “hyperbolic” and “Eu-
clidean” networks.
1	Introduction
Representation learning is an important tool for learning from structured data such as graphs or
texts (Grover & Leskovec, 2016; Perozzi et al., 2014; Mikolov et al., 2013). State-of-the-art algo-
rithms typically use Euclidean space for embedding. Recently, however, it was found that hyperbolic
spaces demonstrate superior performance for various tasks (Nickel & Kiela, 2018; Sala et al., 2018),
while in some cases spherical spaces can be useful (Liu et al., 2017). A key characteristic classifying
the above-mentioned spaces is curvature, which is negative for hyperbolic spaces, zero for Euclidean
and positive for spherical spaces. These findings, therefore, show that certain graphs are better rep-
resented in spaces with non-zero curvature. While some methods simply fix curvature (e.g., -1 for
hyperbolic space) and then find the optimal embedding of the graph in the corresponding space
(Nickel & Kiela, 2018), others try to learn the right curvature and embedding simultaneously (Gu
et al., 2019).
In this paper, we analyze the problem of determining a graph curvature suitable for embedding
without actual embedding the graph itself. The aim is to use some simple graph characteristics to
understand whether it is reasonable to embed a graph into some non-Euclidean space and if yes,
then which curvature to choose. Having such an estimator would save computational resources,
since learning an embedding is computationally expensive and should be done for each space sepa-
rately. More importantly, it can also save human resources by showing whether it is worth investing
resources in the implementation of a more complex embedding algorithm.
We make an important first step in this direction by introducing a concept of global graph curva-
ture, which depends on both the dimension and loss function used for the embedding. We consider
two loss functions: distortion, which is widely used in embedding literature, and threshold-based
loss (see Section 2), which is more suitable for some practical applications. We demonstrate, both
theoretically and empirically, that these loss functions lead to fundamentally different graph curva-
tures. We also compare several estimators of the global curvature, in particular, the ones based on
well-known Ollivier-Ricci and Forman-Ricci local graph curvatures. Our analysis shows that all
these notions give curvatures that are far from the optimal curvature for embedding the graph (in
particular, because they are dimension-independent). This raises the important question of how to
properly estimate the best curvature for embedding graphs. We approach this problem by introduc-
1
Under review as a conference paper at ICLR 2020
ing a new curvature estimator which, in contrast to all existing ones, is dimension-specific. This
estimator agrees with our theoretical results and is also able to distinguish between “hyperbolic”
and “Euclidean” networks.
One important insight, which we gained based on both theoretical and empirical analysis, is the
fact that the optimal curvature is usually smaller for smaller dimensions. In other words, a network
which seems to be negatively curved (for some small dimension) usually becomes more neutral as
dimensionality grows.
Finally, while in the current research we assume that the global graph curvature is constant, this
concept can be extended in the future to cover, e.g., the product spaces from (Gu et al., 2019).
We focused on the constant curvature since, first, such spaces are widely used in various applica-
tions (Tifrea et al., 2018) and, second, even for constant curvature we observed many non-trivial
theoretical and empirical results. These results and insights can eventually be extrapolated to an-
alyze curvature in other, more complicated spaces. To the best of our knowledge, the problem of
analyzing different notions of curvature for embedding graphs (even in simple spaces) was not con-
sidered before, so we hope that more results will follow.
2	Background and related work
Graph embeddings For an unweighted connected graph G = (V, E) equipped with the shortest
path distance metric dG, a graph embedding f is a map f : V → U, where U is a metric space.
We refer to Goyal & Ferrara (2018) for a survey of several graph embedding methods and their
applications.
The goal of an embedding is to preserve some structural properties of a graph. Depending on an
application, different loss/quality functions are used to measure the quality of a graph embedding.
The most well-known is distortion:1
D⑺-1 X Idf(U),f(V))- dG(U, V)I
D(f ) =0 U≠	E
Distortion is a global metric, it takes into account all graph distances. However, in some practical
applications, it may not be the best choice. For example, in recommendation tasks, we usually deal
with a partially observed graph, so a huge graph distance between nodes in the observed part does
not necessarily mean that the nodes are not connected in the full graph. Additionally, as discussed
in Section 4.3.1, graph distances are hard to preserve: there are simple graphs on just 4 nodes that
can be perfectly embedded only in a space of curvature -∞ for any dimension.
Another measure, often used for embeddings, is Mean Average Precision (MAP), which, for a
given node V, compares the distance-based ranking of other embedded nodes with the graph-
neighborhood-based ranking. In contrast to distortion, MAP is scale-invariant, as it cares only about
the order. Since changing curvature is equivalent to changing scale, MAP is insensitive to curvature.2
However, in practical applications like recommendation systems, ranking scores are usually com-
plemented by minimum score thresholds to have personalized sizes of top recommended items. This
motivated us to consider another loss function, which is at the same time local and scale-dependent.
Let us define the following class of threshold-based loss functions. Given an embedding f ofa graph
G, we (re)construct a graph G0 in the following way: V and U are connected in G0 iffd(f(V), f (U)) ≤
1. Then, any loss function which is based on the comparison of G and G0 is called threshold-
based. While all our theoretical results hold for any threshold-based loss function (see Section 4),
in our experiments we measure and compare several functions of this type including, e.g., zero-
one loss for the edge classification problem (whether a given node pair is connected or not), see
Appendix E.2 for definitions and discussion. Such loss functions are natural in many applications
(graph reconstruction, link prediction, recommendations).
1There are other definitions of distortion in the literature, see, e.g., (Sala et al., 2018).
2In other words, it is sufficient to consider only the curvatures -1, 0, 1, corresponding to hyperbolic, Eu-
clidean and spherical spaces. Moreover, by considering a small enough region in hyperbolic or spherical space
we get geometry similar to the Euclidean one, so for MAP it is important to distinguish only between -1 and 1.
2
Under review as a conference paper at ICLR 2020
Hyperbolic and Spherical Spaces For many years, Euclidean space was the primary choice for
data embeddings (Goyal & Ferrara, 2018). However, it turned out that many observed datasets
are well fitted into hyperbolic space (Krioukov et al., 2010). Nickel & Kiela (2017) have shown
that such hyperbolic embeddings can improve state-of-the-art quality in several practical tasks, e.g.,
lexical entailment and link prediction. On the other hand, spherical spaces are also used for embed-
dings (Liu et al., 2017). Gu et al. (2019) goes even further by suggesting mixed spaces: product
manifolds combining multiple copies of spherical, hyperbolic, and Euclidean spaces.
The main advantage of hyperbolic space is that it is “larger”: the volume of a ball grows exponen-
tially with its radius. Hence, such spaces are well suited for tree-like structures. On the other hand,
spherical spaces are suitable for embedding cycles (Gu et al., 2019). Spherical and hyperbolic spaces
are parametrized by curvature c, which is positive for spherical space and negative for hyperbolic
space. As c → 0, geometry of both these spaces becomes similar to the Euclidean one. We discuss
some geometrical properties of these different spaces in Appendix A.
3	Local graph curvatures
While in this paper we analyze the global graph curvature, there are several local ones proposed in
the literature. Many of them are based on the notion of sectional curvature and Ricci curvature de-
fined for Riemannian manifolds. Intuitively, Ricci curvature controls whether the distance between
small spheres is smaller or larger than the distance between the centers of the spheres. Ricci cur-
vature is positive if small spheres are closer than their centers are. We refer to (Jost, 2009; O’neill,
1983) for more details on Ricci curvature.
Ollivier curvature Ollivier curvature translates the definition of Ricci curvature to graphs. Again,
the idea is to compare the distance between two small balls with the distance between their centers.
The distance between balls is defined by the well-known optimal transport distance (a.k.a. Wasser-
stein distance or earth-mover distance). Formally, for a graph G we consider the shortest path metric
on G, denoted by dG , and let W1G denote the Wasserstein metric with respect to the metric space
(G, dG ). Furthermore, for each node v we let mv denote the uniform probability measure on the
neighbors of v, i.e., mv (U) = du(v), where deg(v) denotes the degree of v. Then, the classic
definition3 of Ollivier curvature between two neighboring nodes V 〜 U in G is defined as
κG(u, v) = 1 - W1G(mv, mu).	(1)
We note that Ollivier curvature always belongs to the interval [-2, 1] (Jost & Liu, 2014).
Forman curvature Forman curvature (Sreejith et al., 2016) is based on the discretization of Ricci
curvature proposed by Forman (2003). It is defined for a general weighted graph G, with both node
and edge weights. When the graph G is not weighted, the definition becomes:
FG(U,v) = 4 - (deg(v) + deg(U)).	(2)
Forman was interested in a general discretization of curvature for Riemannian manifolds and his
formula includes faces of any dimension. Although this can be translated to graphs (Weber et al.,
2017), the formula becomes quite cumbersome. Therefore, in Equation 3 only 1-dimensional faces
(edges) are included. One can extend this expression by including higher dimensional faces. This
was considered in (Samal et al., 2018), where 2-dimensional faces on three nodes (triangles) were
included. In the case of an unweighted graph, we then obtain
FG(u, v) = F(u, v) + 3∆uv =4 - deg(v) - deg(u) + 3∆uv,	(3)
where ∆uv is the number of triangles that contain the edge (U, v).
Based on the definitions, both Forman curvatures, especially FG (U, v), are expected to often be
highly negative. Indeed, this is what we observe in Sections 4.3 and 5.
3 Note that Ollivier curvature is defined in much more generality in terms of metrics and random walks,
see (Ollivier, 2009). Thus, different version on graphs can be considered. Equation 1 corresponds to the
classical choices of graph distance and random walk on the graph.
3
Under review as a conference paper at ICLR 2020
Heuristic sectional curvature A different notion of curvature used by Gu et al. (2019) is based on
the following geometric fact. Let abc be a geodesic triangle and let m be the (geodesic) midpoint of
be. Then the value d(a, m)2 + d(bc——d(a,b) +d(a,C) is equal to zero in euclidean space, is positive
in spherical space and negative in hyperbolic space.
For graphs, let v be a node in G, b, e neighbors of v and a any other node. Then, we define
1	2	dG(b,e)2	dG(a, b)2 +dG(a,e)2
ξG(v;b，e;a) = 2dGaV) (dG(a，V) +—^-------------------------2----------)-	⑷
This resembles the formula above with m = v and the normalization constant 2dG(v, a) is in-
cluded to yield the right scalings for trees and cycles. To define the graph sectional curvature
of a node v and its neighbors b, e, we average ξG (v; b, e; a) over all possible a: ξG (v; b, e) =
∣V1-3 Pa∈G∖{v,b,c} ξG(v; b, c； a).4
4	Global graph curvature
The problem with these different notions of local graph curvature is that they cannot be easily used
in practical applications, where data is usually embedded in a space of constant curvature. Hence,
a global notion of curvature is needed. In this section, we propose a practice-driven definition of
global graph curvature, discuss how to estimate this curvature based on local notions and compare
all curvatures theoretically and empirically for several simple graphs.
4.1	Definition
For a graph G, let f (G) be an embedding of this graph into a d-dimensional space of constant
curvature e (spherical, Euclidean or hyperbolic). Assume that we are given a loss function L(f) for
the embedding task (see Section 2). Now, let Lopt(G, d, e) be the optimal loss for given d and e:
Lopt(G, d, e) = minf L(f). Then, we define d-dimensional curvature of G in the following way:
CdL(G) = arg min Lopt (G, d, e) .	(5)
c
Note that there may be several values of curvature e delivering the minimum of Lopt(G, d, e), in this
case we say that CdL(G) consists of all such points.5
Below we analyze global curvatures based on distortion (Cddist(G)) and threshold-based (Cdthr(G))
loss functions. In the latter case, in experiments, we measure zero-one loss, but our theoretical
results apply to any threshold-based loss, since Lopt (G, d, e) reaches its minimum on “perfect”
embeddings, where we precisely reconstruct the graph G.
4.2	Approximations
Let us discuss how local graph curvatures can be used to estimate the global one. In all cases, the
standard practice is to average edge or sectional curvature over the graph.
Ollivier curvature K(G) = E E kg(u,v).
U〜V
Forman curvature F(G) = ɪ P FG(u,v), F(G)=击 P Fg(u,v).
U〜V	U〜V
Average sectional curvature Let P3 denote the number of paths of length 3 in G, then ξ (G) =
P P P	ξG(v; b,e).
v∈V b<c.b,c∈N(v)
4We assume that a does not coincide with b or c, which do not affect the average much, but makes our
results in Section 4.3 more succinct.
5Further we slightly abuse notation by writing that CdL (G) is a real value if such c is unique and a set of
values otherwise.
4
Under review as a conference paper at ICLR 2020
0.1
0.05
-2
noitrotsid
-1.5
-0.5	0	0.5	1
curvature
1.5	2
Figure 1: Star Sn
0.05
.15 0.1
0
ssol eno-orez
4535
0. .3 0. .2
0G ....................................................
-2	-1.5	-1	-0.5	0	0.5	1	1.5	2
curvature
It is important to note that all curvatures discussed above do not depend on neither dimension d nor
loss function L. However, as we show below, global curvature defined in Section 4.1 significantly
depends on them.
Let us also mention that there is a concept of Gromov’s hyperbolicity (Gromov, 1987), which is
sometimes used to decide whether it is reasonable to embed a graph to a hyperbolic space. However,
first, this estimator cannot be easily converted to curvature, and, second, it does not say anything
about the sphericity of data.
4.3	Theoretical analysis of global curvature and its approximations
To better understand the performance of the proposed approximations of global graph curvature,
we shall consider several basic graphs and analyze their global curvature and approximations both
theoretically and empirically. By studying these graphs we also hope to gain insights into how classic
graph topologies influence the curvature of the space in which they can be properly embedded. For
more complex graphs, e.g., for combinations of simple graphs studied in this section, it becomes
extremely hard to prove rigorous theoretical results. However, the obtained intuitions work in such
scenarios, as we illustrate and discuss in Appendix C. The experimental setup for our empirical
illustrations is described in Appendix E.3.
4.3.1	STAR Sn
It is pointed out in numerous papers that trees are negatively curved. We analyze this theoretically
and start with the simplest tree: one central node and n leaves. We denote this graph by Sn and
assume that n ≥ 3.
Ollivier curvature Consider any tree graph T , let v, u be two neighbors. Then Proposition 2
in (Jost & Liu, 2014) states that
KT(U,V) =-2(1- deg1(V)-京)
(6)
where t+ = max{0, t}. In particular, if either deg(v) = 1 or deg(u) = 1, then κT (u, v) = 0. As a
result, for a star we have κSn (u, v) = 0, so κ(Sn) = 0 and stars are not negatively curved according
to Ollivier curvature.
TT-I	"	ɪ I' I' 11	C	τr∏	. ∙	z-⅝	FTn	, ∙	, 1 , τ^l / r~f ∖	7^S /	∖	C
Forman curvature If follows from Equation 2 and Equation 3 that F(Sn) = F (Sn) = 3 - n, so
stars are highly negatively curved for large n according to Forman curvature.
Average sectional curvature Heuristic sectional curvature is defined for a node and its two neigh-
bors. In case of a star we can only take a central node v and two neighboring ones b and c. For any
other node a we obtain ξSn (v; b, c; a) = -1. Therefore, by averaging we obtain ξ(Sn) = -1.
Distortion-based curvature We prove the following theorem (the proof is in Appendix B.3).
5
Under review as a conference paper at ICLR 2020
Theorem 4.1. Assume that d and n are fixed. If c is bounded below by a constant, then D(Sn) is
also bounded below by a Constant If C → 一∞, then the optimal distortion Dopt(Sn) = Θ (√-c).
Therefore, for any Sn we have Cddist(Sn) = -∞.
This result is illustrated in Figure 6 (left): distortion decreases as curvature becomes smaller. The
intuition behind this result is the following: we cannot embed a star S3 with zero distortion into a
space of any constant curvature and any dimension, because in case of zero distortion the central
node v has to lie on the geodesics between all pairs of leaves, so all 4 nodes have to belong to one
geodesics, which is impossible. Moreover, the same problem occurs if any graph G contains S3 as
an induced subgraph. On the other hand, if C → 一∞,we can spread all leaves of Sn uniformly on a
circle of radius 1 around the central node and distortion of such construction will tend to zero since
the distance between the pairs of leaves will tend to 2 (triangles become thinner). Further we will
see that ifwe minimize a threshold-based loss, then any tree can be perfectly embedded with d = 2.
Threshold-based curvature Here we have the following theorem (the proof is in Appendix B.5).
Theorem 4.2. Cdthr(Sn) 二(一∞, C) for some C = C(n, d), which increases with d and decreases
with n. In particular, for d = 2, if n < 6, then C = arccos
2
Cos管
1-cos 2π
2
; if n = 6, then C = 0; if
n > 6, then C
一 (2arccosh2⅛
n
The result is illustrated in Figure 6 (right). Note that zero-one loss can be noisy, especially on small
graphs, since it is threshold-based, i.e., discrete.
4.3.2	TREE Tb WITH BRANCHING FACTOR b
We consider a tree Tb, b ≥ 2. For symmetry, assume that the first node has b + 1 children, while all
other nodes have a parent and b children. Also, for Ollivier, Forman and threshold-based curvatures,
we assume that the depth of Tb is infinite. In other cases, our statements hold for any finite tree T.
Ollivier curvature We can use Equation 6 and get, for any edge, κTb (u, v)	=
-2 (1 - b++ι - b+ι) = -2 + b++ι. So, κ(Tb) = -2 + b++ι, which is negative.
Forman curvature It is easy to see that we have F(Tb) = F(Tb) = 4 一 2(b + 1) = 2(1 一 b).
Average sectional curvature In contrast to Ollivier and Forman curvatures, heuristic sectional
curvature is global, i.e., it depends on the whole graph, which has to be finite. Note that for any
tree, to compute sectional curvature, we average 0 and -1. As a result, for any tree T we have
ξ(T) ∈ [一1, 0] (Gu et al., 2019).
Distortion-based curvature On the one hand, our result for Sn implies that if a graph contains
S3, then it cannot be embedded with zero distortion in any space. One the other hand, Sarkar
(2011) proves that if we scale all edges by a sufficiently large factor τ , then the obtained tree can
be embedded to the hyperbolic plane with distortion at most 1 + ε with arbitrary small ε. Note that
multiplying graph edges by τ is equivalent to changing curvature from 1 to τ2. As a result, Sarkar
(2011) proves that we can achieve an arbitrary small distortion if C → 一∞. Hence, Cdist (T) = 一8
for any T .
Threshold-based curvature We prove the following theorem (see Appendix B.6).
Theorem 4.3. Cdthr (Tb) =(ι∞, C) for some C = C(b, d), which increases with d and decreases
2
with b. The following lower bound holds: C(b, 2) ≥
2 log b
2arccosh CCsh⅛∕2 -1
Actually, the bound above holds for any tree whose branching is bounded by b. Interestingly, while
it is often claimed that trees are intrinsically hyperbolic, to the best of our knowledge, we are the
first to formally prove that trees can be perfectly embedded in a hyperbolic plane of some curvature.
6
Under review as a conference paper at ICLR 2020
noitrotsid
n = 4, d = 2
n = 5, d = 2
n = 5, d = 10
n = 12, d = 10
n = 100, d = 2
n = 100, d = 10 ―θ-
curvature
Figure 2: Complete graph Kn
第⅛ι百口 b⅛己寻百巨曰"日~⅛ycι □ m`p □⅞b⅛j
^eeeoooθ^∙

O
4
uoes-p
Figure 3: Cycle graph Cn
0.1
n = 5, d = 2 —I—
n = 5, d = 10 -×—
n = 100, d = 2	—
n = 100, d = 10 -E3—
4 53 5 25
0. .3 0. .2 0. .1
0 0 0
ssol eno-orez
curvature
4.3.3	COMPLETE GRAPH Kn
Ollivier curvature For any two nodes u and v, it follows from Example 1 in (Jost & Liu, 2014)
that KKn (v, U) = n-2. Thus, K(Kn) = n≡2 and it tends to 1 as n → ∞.
Forman curvature Simple computations yield: F(Kn) = 6 - 2n, F(Kn) = n, i.e., we get either
highly positive or highly negative value.
Average sectional curvature It is easy to compute that ξ(Kn = 1.
Distortion-based curvature The following theorem analyzes Cddist (Kn) if d = n - 2 (see Ap-
pendix B.4 for the proof).
Theorem 4.4.
Note that the optimal curvature -∞ appears here since embedding of a complete graph can be
reduced to embedding of a weighted star. This theorem is illustrated in Figure 2. For n = 4, d = 2
we expect to see the minimum at about 3.65, for n = 12, d = 2 we expect 2.76, which is indeed
the case. For other parameters, we do not have theoretical results, however we see sudden drops in
positive curvature. Finally, as expected, distortion decreases as curvature becomes small.
Threshold-based curvature Cdthr (Kn) = R, since we can embed any complete graph perfectly
by mapping all nodes to one point.
4.3.4	CYCLE GRAPH Cn
We consider a cycle Cn with n ≥ 4.
7
Under review as a conference paper at ICLR 2020
-2	-1.5	-1	-0.5	0	0.5	1	1.5	2	2.5	3
curvature
Figure 4: Complete bipartite graphs Kn,n
ssol eno-orez
noitrotsid
0.22
0.2
n = 5, d = 2 →-
n = 5, d = 10 ―头—
n = 100, d = 2 τK—.....
n = 100, d = 10
-5	-4	-3	-2	-1	0	1	2
curvature
OlliVier curvature Let V 〜U be two neighbors. Then it is easy to see that WG(mu, mv) = 1 and
hence κG(u, v) = 0. Thus, κ(Cn) = 0.
Forman curvature Similarly, it is easy to see that F(Cn) = F(Cn) = 0.
Average sectional curvature If n is even, then ξCn (v; b, c; a) = 0 for all points except the one
diametrically opposite to v for which we have ξCn (v; b, c; a) = 1. If n is odd, then for two points
we have 0 (v; b, c; a) = 2(n—1). AS a result, ξ(Cn) = n—3 for even n and ξ(Cn) - ............n----
for odd n.
2(n-1).
n—3
(n- 1)(n-3)
Distortion-based curvature Here we have that Cfst(Cn) = (2∏) . Indeed, if we consider any
three consequent nodes, then the middle one should lie on the geodetic between the other two. So,
they all lie on a great circle (of length n) from which the result follows. In particular, Cddist(C5) ≈
1.58, Cddist(C100) ≈ 0.004, which is illustrated in Figure 3 (left).
Threshold-based curvature It is easy to see that Cdthr (Cn) = (-∞, C) with some C > 0,
which decreases with n and increases with d (see Figure 3, right). A simple lower bound for C is
C ≥ (4∏) , since for such curvature we can embed all nodes into a great circle with distances 1/2
between the closest ones.
4.3.5	COMPLETE BIPARTITE GRAPH Kl,m
W.l.o.g. we assume that l ≥ m ≥ 2 (the remaining cases are stars and are already considered).
Ollivier curvature We prove the following lemma (the proof is in Appendix B.1).
Lemma 4.5. κ(Kl,m) = 0.
Forman curvature F(Kl,m) = F (Kl,m) = 4 - l - m.
Average sectional curvature The following lemma holds (see Appendix B.2).
Lemma 4.6. ξ(Kι,m) = -<(I-Im2)+m+-32，1nparticular，f m = l we get ξ(Kι,m) = 2m-3∙
This lemma implies that for balanced complete bipartite graphs ξ(Kl,m) is positive, but tends to
zero as the graph grows.
Distortion-based curvature Figure 4 (left) shows the empirical behavior of distortion depending
on curvature. We observe that for all graphs optimal curvature (for the considered range) is between
2 and 2.5, where distortion has a drop. The following proposition gives an intuition about why one
could expect this.
Proposition 4.7. For any d, CdStt(K2,2) = (∏)2 ≈ 2.47 and K2,2 is the only complete bipartite
graph (with at least two nodes in each part) for which zero distortion is achievable.
8
Under review as a conference paper at ICLR 2020
Table 1: Compare curvature estimators, distortion
Dataset/dim	Ollivier		Forman		Avg. sectional		Grad. descent	
	c	loss	c	loss	c	loss	C	loss
Conflict / 2	-0.2	0.229	-16.5	0.261	0.25	0.269	0.0	0.229
Conflict / 10	-0.2	0.078	-16.5	0.243	0.25	0.173	0.0	0.085
Chicago / 2	-0.19	0.225	-8.37	0.224	-0.6	0.178	0.0	0.277
Chicago / 10	-0.19	0.045	-8.37	0.216	-0.6	0.024	0.0	0.080
CSPhDs /2	-0.28	0.152	-7.92	0.420	-0.26	0.172	0.0	0.209
CSPhDs / 10	-0.28	0.085	-7.92	0.412	-0.26	0.208	0.0	0.056
Euroroad / 2	-0.36	0.267	-1.95	0.452	0.027	0.384	0.0	0.147
Euroroad / 10	-0.36	0.264	-1.95	0.445	0.03	0.370	0.0	0.052
EuroSiS / 2	-0.17	0.247	-29.1	0.197	0.27	0.298	0.0	0.263
EuroSiS / 10	-0.17	0.087	-29.1	0.187	0.26	0.202	0.0	0.096
Power / 2	-0.35	0.287	-2.85	0.368	0.02	0.363	-1.88	0.372
Power / 10	-0.35	0.278	-2.85	0.361	0.02	0.331	-1.0	0.400
Facebook / 2	0.308	0.290	-44.7	0.327	0.153	0.240	0.0	0.238
Facebook / 10	0.308	0.210	-44.7	0.323	0.153	0.084	0.0	0.072
Indeed, the result for K2,2 follows from the corresponding result on cycle C4. Moreover, if for
Kl,m we have l ≥ 3 and m ≥ 2, then for any two nodes in the part of size l there are at least 2
different geodesics of length 2 between them. Therefore, all such pairs lie at opposite poles of the
hypersphere, which is impossible since l ≥ 3.
Threshold-based curvature Threshold-based embeddings to Euclidean spaces are well studied.
In particular, it is know that for any graph G(V, E) a perfect threshold embedding exists for some
d ≤ |V | (Maehara, 1984). However, undirected bipartite graphs are hard to embed in Euclidean
space: the bound d = O(n) is known (Maehara, 1984). We empirically observe that they are
similarly hard to embed to both spherical and hyperbolic spaces, as shown in Figure 4 (right). Em-
pirically, these graphs are insensitive to curvature (given that it is not too large).
4.4 Volume-based approximation
Let us propose a new heuristic curvature estimator which is motivated by the intuition obtained in
the previous section as well as by extensive literature arguing that the most important feature of a
hyperbolic space is the fact that it has “more volume” in the neighborhood. Therefore, the natural
idea is to estimate the volume we need for embedding nodes that are at distance at most k from
a given one and then compute the curvature needed to get such volume. We compare the number
of nodes at distance exactly k from a node (discounted using the number of edges between them)
with the area of a hypersphere of radius k in a space of curvature c. The detailed description of
the procedure is given in Appendix D. A nice feature of the proposed estimator is that it depends
on dimension. Moreover, this dependence is natural: when d becomes larger, curvature becomes
smaller in absolute value. As we show in the experiments on various datasets, a similar property is
observed for the optimal curvature. Also, it turns out that for a given dimension the new estimator
is able to distinguish well between negatively curved and neutral networks. However, despite the
potential of this new estimator, the problem of proper global curvature estimation is still wide open.
5	Experiments
In this section, we compare all discussed curvature estimators empirically. Our experiments are
based on a publicly available implementation of graph embedding by Gu et al. (2019).6 The main
advantage of this algorithm is that it works for any curvature: negative, positive or zero. We mod-
ified the implementation for our task: in particular, we added a regime responsible for minimizing
6https://github.com/HazyResearch/hyperbolics
9
Under review as a conference paper at ICLR 2020
Table 2: Compare curvature estimators, zero-one loss
Dataset	Ollivier		Forman		Avg. sectional		Grad. descent	
	c	loss	c	loss	c	loss	C	loss
Conflict / 2	-0.2	0.032	-16.6	0.032	0.25	0.060	0.0	0.032
Conflict / 10	-0.2	0.001	-16.6	0.025	0.25	10-4	0.0	0.0
Chicago / 2	-0.19	0.004	-8.37	0.005	-0.6	0.004	0.0	0.002
CSPhDs /2	-0.28	0.002	-7.92	0.004	-0.26	0.002	0.0	0.002
Euroroad / 2	-0.36	0.002	-1.95	0.004	0.027	0.005	0.0	0.002
EuroSiS / 2	-0.17	0.009	-29.1	0.017	0.26	0.058	0.0	0.010
EuroSiS / 10	-0.17	0.003	-29.1	0.012	0.26	0.003	0.0	0.007
Power / 2	-0.35	0.002	-2.85	0.006	0.02	0.006	0.0	0.002
Facebook / 2	0.308	0.058	-44.7	0.059	-11.2	0.040	0.0	0.019
Facebook / 10	0.308	0.007	-44.7	0.050	0.15	0.007	0.0	0.013
threshold-based loss functions. The detailed description of our experimental setup is given in Ap-
pendix E.3, the datasets and their properties are listed in Appendix E.1.
The results of the comparison for distortion and zero-one loss are presented in Tables 1 and 2,
respectively.7 We also added a method based on the gradient descent, proposed by Gu et al. (2019).
This method is used here only as an illustration, since our aim is to compare estimators which do not
do actual embedding. Also, the gradient descent algorithm uses the best space based on the value
of the loss function, while all other curvature estimators do not use such information. We marked
in bold both the global winner and the winner among all curvatures except the gradient descent one.
The main goal of this experiment is to show that all known curvatures (Ollivier, Forman and average
sectional) have unstable performance and even gradient descent method is not always the best (in
both tables). Another important conclusion is that the optimal curvature depends on both dimension
and loss function. We show more evidence for that in our extensive analysis in Appendix E.5, where
we consider more datasets and more threshold-based loss functions. The experiments also confirm
that networks which seems to be negatively curved (for some small dimension) become more neutral
as dimensionality grows
In our experiments with the proposed volume-based estimator (see Appendix E.6), we noticed that it
is able to predict well whether the hyperbolic space is needed for an embedding or Euclidean space
is enough. However, the magnitude for the predicted curvature can be not optimal. Volume-based
estimator also predicts that for large dimensions Euclidean space is often a good choice.
6	Conclusion
In this paper, we introduced a concept of global graph curvature motivated by the practical task of
embedding graphs. This curvature depends on the loss function and dimension of a space. To get
an intuition about how global graph curvature behaves, we theoretically and empirically analyzed
it for several simple graphs. We compared (theoretically and empirically) the global graph curva-
ture and several approximations based on well-known local graph curvatures and showed that they
essentially differ. We demonstrated that dimensionality and the choice of a loss function fundamen-
tally affect the global curvature and, in particular, when dimension is larger the optimal curvature
usually becomes less negative, which agrees with our theoretical analysis. We also proposed a sim-
ple dimension-specific estimator which decides whether it is reasonable to embed into a negatively
curved space. Our work shows that the problem of finding the rights space for graph embedding is
interesting and non-trivial and we hope our results will encourage further research on global graph
curvature.
7Dimension 10 is not included for zero-one loss if for all curvatures a perfect embedding is possible.
10
Under review as a conference paper at ICLR 2020
References
R Eash, K Chon, Y Lee, and D Boyce. Equilibrium traffic assignment on an aggregated highway
network for sketch planning. Transportation Research, 13:243-257, 1979.
Robin Forman. Bochner’s method for cell complexes and combinatorial ricci curvature. Discrete
and Computational Geometry, 29(3):323-374, 2003. doi: 10.1007/s00454-002-0743-x.
Martijn Gosgens, LiUdmila Prokhorenkova, and Alexey Tikhonov. Systematic analysis of cluster
similarity indices: Towards bias-free cluster validation. arXiv:1911.04773, 2019.
Palash Goyal and Emilio Ferrara. Graph embedding techniques, applications, and performance: A
survey. Knowledge-Based Systems, 151:78-94, 2018. doi: 10.1016/j.knosys.2018.03.022.
Mikhael Gromov. Hyperbolic groups. In Essays in group theory, pp. 75-263. Springer, 1987.
Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proceedings
of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining,
pp. 855-864. ACM, 2016.
Albert Gu, Frederic Sala, Beliz Gunel, and Christopher Re. Learning mixed-curvature repre-
sentations in product spaces. ICLR, 2019. URL https://openreview.net/pdf?id=
HJxeWnCcF7.
Jurgen Jost. Geometry and physics. Springer Science & Business Media, 2009. doi: 10.1007/
978-3-642-00541-1.
Jurgen Jost and Shiping Liu. Olliviers ricci curvature, local clustering and curvature-dimension
inequalities on graphs. Discrete & Computational Geometry, 51(2):300-322, 2014. doi: 10.
1007/s00454-013-9558-1.
Devangana Khokhar. Gephi cookbook. Packt Publishing Ltd, 2015.
Dmitri Krioukov, Fragkiskos Papadopoulos, Maksim Kitsak, Amin Vahdat, and Marian BOgUna.
Hyperbolic geometry of complex networks. Physical Review E, 82(3):036106, 2010.
Jure Leskovec and Julian J Mcauley. Learning to discover social circles in ego networks. In Advances
in neural information processing systems, pp. 539-547, 2012.
Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, and Le Song. Sphereface: Deep
hypersphere embedding for face recognition. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 212-220, 2017.
David Lusseau, Karsten Schneider, Oliver J Boisseau, Patti Haase, Elisabeth Slooten, and Steve M
Dawson. The bottlenose dolphin community of doubtful sound features a large proportion of
long-lasting associations. Behavioral Ecology and Sociobiology, 54(4):396-405, 2003.
Hiroshi Maehara. Space graphs and sphericity. Discrete Applied Mathematics, 7(1):55-64, 1984.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed represen-
tations of words and phrases and their compositionality. In Advances in neural information pro-
cessing systems, pp. 3111-3119, 2013.
Mark EJ Newman. Modularity and community structure in networks. Proceedings of the national
academy of sciences, 103(23):8577-8582, 2006.
Mark EJ Newman and Michelle Girvan. Finding and evaluating community structure in networks.
Physical review E, 69(2):026113, 2004.
Chien-Chun Ni, Yu-Yao Lin, Jie Gao, Xianfeng David Gu, and Emil Saucan. Ricci curvature of
the internet topology. In 2015 IEEE Conference on Computer Communications (INFOCOM), pp.
2758-2766. IEEE, 2015.
11
Under review as a conference paper at ICLR 2020
Maximillian Nickel and DoUWe Kiela. Poincare embeddings for learning hier-
archical representations. In Advances in neural information processing sys-
tems,	pp. 6338-6347,	2017.	URL http://papers.nips.cc/paper/
7213-poincare-embeddings-for-learning-hierarchical-representations.
pdf.
Maximillian Nickel and DoUWe Kiela. Learning continUoUs hierarchies in the lorentz model of
hyperbolic geometry. In International Conference on Machine Learning, pp. 3776-3785, 2018.
URL https://arxiv.org/abs/1806.03417.
Yann Ollivier. Ricci cUrvatUre of markov chains on metric spaces. Journal of Functional Analysis,
256(3):810-864, feb 2009. doi: 10.1016/j.jfa.2008.11.001.
Barrett O’neill. Semi-Riemannian geometry with applications to relativity, volUme 103. Academic
press, 1983.
Bryan Perozzi, Rami Al-RfoU, and Steven Skiena. DeepWalk: Online learning of social repre-
sentations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge
discovery and data mining, pp. 701-710. ACM, 2014.
Frederic Sala, Chris De Sa, Albert GU, and Christopher Re. Representation tradeoffs for hyperbolic
embeddings. In International Conference on Machine Learning, pp. 4457-4466, 2018.
Areejit Samal, RP Sreejith, Jiao Gu, Shiping Liu, Emil Saucan, and JUrgen Jost. Comparative
analysis of tWo discretizations of ricci cUrvatUre for complex netWorks. Scientific reports, 8(1):
8650, 2018. doi: 10.1038/s41598-018-27001-3.
Rik Sarkar. LoW distortion delaunay embedding of trees in hyperbolic plane. In International
Symposium on Graph Drawing, pp. 355-366. Springer, 2011.
RP Sreejith, Karthikeyan Mohanraj, JUrgen Jost, Emil Saucan, and Areejit Samal. Forman curva-
ture for complex netWorks. Journal of Statistical Mechanics: Theory and Experiment, 2016(6):
063206, 2016. doi: 10.1088/1742-5468/2016/06/063206.
Lovro Subelj and Marko Bajec. Robust netWork community detection using balanced propagation.
The European Physical Journal B, 81(3):353-362, 2011.
Alexandru Tifrea, Gary Becigneul, and Octavian-Eugen Ganea. PoinCare glove: Hyperbolic word
embeddings. arXiv preprint arXiv:1810.06546, 2018.
A. Mrvar W. De Nooy and V. Batagelj. Exploratory social network analysis with pajek. Cambridge
University Press, 2011.
Michael D Ward, Randolph M Siverson, and Xun Cao. Disputes, democracies, and dependencies: A
reexamination of the kantian peace. American Journal of Political Science, 51(3):583-601, 2007.
DJ Watts and SH Strogatz. Collective dynamics of small-world networks. nature, 393: 440-442.
View Article, 1998.
Melanie Weber, Emil Saucan, and Jrgen Jost. Coarse geometry of evolving networks. Journal of
Complex Networks, 6(5):706-732, 11 2017. ISSN 2051-1329. doi: 10.1093/comnet/cnx049.
Wayne W Zachary. An information flow model for conflict and fission in small groups. Journal of
anthropological research, 33(4):452-473, 1977.
A Geometrical properties of spaces of constant curvature
In this section, we recall some useful equalities which will be used throughout the proofs.
In all proofs, we use notation R, where R = √1c in spherical space (corresponds to the radius of a
sphere) and in the hyperbolic case (c < 0) R = √-c can be considered as a scaling factor compared
to the space of curvature 1.
12
Under review as a conference paper at ICLR 2020
Law of cosines Let us consider a triangle with angles A, B, C and the lengths of opposite sides
a, b, c, respectively.
In Euclidean, space we have:
c2 = a2 + b2 - 2 a b cos C .
In spherical space, the first law of cosines is:
cos — = cos — cos — + Sin — Sin — cos C,
R	R R + R R ,
and the second law of cosines is:
c
cos C = — cos A cos B + Sin A Sin B cos —.
In hyperbolic space, we have
cosh —
R
cosh — cosh — — sinh — sinh — cos C.
Equilateral triangle
The following equalities follow from the corresponding laws of cosines, assuming that all sides (and
angles) are equal.
For hyperbolic space:	—1 cosh2R = 2Snl.	⑺
For spherical space:	—	cos A cos R = EiN.	⑻
Area and volume of hypersphere Let Sd(r) and Vd(r) denote area of a hypersphere and volume
of a ball of radius r in d-dimensional space.
In euclidean space,	Sd(r) = dCdrd-1, Vd(r) = Cd rd,
where	C _	πd/2 d — γ( d + 1).
In spherical space, sphere of radius r is isometric to Euclidean sphere of radius R sin R. Therefore,
the area is
Sd(r)
dCd (Rsin R)d-1
Vd(r)= dCd Rd / (sin R)d-1
dx.
Similarly, in hyperbolic space,
Sd(r) = dCd(Rsinh R)d-1
Vd(r) = dCdRd
dx.
13
Under review as a conference paper at ICLR 2020
B Curvature of simple graphs
B.1 Proof of Lemma 4.5 (Ollivier curvature for bipartite graphs)
Let use denote the node sets in Kl,m by U := {u1, . . . , ul} and V := {v1, . . . , vm}. We will prove
that for any edge (u, v), W1Kl,m (mu, mv) = 1, which then implies that κ(Kl,m) = 0. For this we
use the dual representation for the Wasserstein distance, see Ollivier (2009) for more details. This
states that on the one hand
W1G (mu, mv) = infΣΣdG(v0, u0)ρ(v0, u0),
V0〜V U0〜U
where the infimum is taken over all joint probability measures on the product of the neighborhoods
of v and u, while on the other hand
W1G (mU, mV) = sup
f
f(v0) -
V0〜V
1
deg(u)
f(u0)
U0〜U
where the supremum is taken over all 1-Lipschitz functions, i.e |f (u) - f (v)| ≤ dG(u, v).
Note that for any u ∈ U, v ∈ V the joint neighborhood is V × U. First we establish an upper bound
by considering the product joint probability density on V × U
1	1	1
ρ(x,y) =—..
ml
It then follows that
ml
W1Kl,m(mU,mV)≤	dG(vi,uj)ρ(vi,uj) = 1.
For the lower bound we define the function
f(z) = 21
if z ∈ V,
if z ∈ U.
Observe that if u ∈ U and v ∈ V then |f (u) - f (z)| = 1 = dG(u, v). On the other hand, if
u, u0 ∈ U then |f (u) - f(u0)| = 0 ≤ 2 = dG(u, u0) and similar for v, v0 ∈ V . Thus we conclude
that f is 1-Lipschitz. It now follows that
1m	1l
W1 ,m(mu,mv) ≥ —X f (Vi)- T X f (Uj ) = 1,
m i=1	l j=1
which completes the proof.
B.2	Proof of Lemma 4.6 (average sectional curvature for bipartite graphs)
If v and a are in the same part of the bipartite graph, then ξKl,m (v; b, c; a) = 1, otherwise
ξKl,m (v; b, c; a) = -1. Therefore, if a belongs to the part of size l, sectional curvature is
ξκι,m (v; b, C) = l+m+3, otherwise it is ξκι,m (v; b, C) = m-+1. As a result, by averaging over
all triplets, we get
l - m + 1
l + m — 3
ξ(Kl,m) = i(m)+m(2)
+m
m — l + 1∖	—(l — m)2 + m + l — 2
l + m - 3)	(m + l - 2)(l + m - 3)
B.3	Proof of Theorem 4.1 (distortion-based curvature for stars)
The general idea is the following: we take any curvature C and prove a lower bound on distortion
(for any given dimension d). Then, we obtain an upper bound on optimal distortion which tends to
zero as C → -∞, which gives the claimed result Cdist (Sn) = -∞.
14
Under review as a conference paper at ICLR 2020
First, let us analyze the lower bound on distortion. Recall that distortion of a graph is the average
distortion over all pairs of nodes. Let v be the central node and v1 , . . . , vn (n ≥ 3) be its neighbors.
Then, for any embedding f, we have
D(Sn) = 7n+ιy (X |d(f (v),f (Vi)) - 1| + X "Svjfj))- 2|
2	vi	vi 6=vj
1
Σ
1≤i1 <i2 <i3 ≤n
V |d(f(vij),f(v)) — 1| ɪ V	∣d(f(vj),f(vik))- 2|\
1≤≤3 一(n-1)一+1≤S≤3 m-^—)
Let Dmin be the minimum value of the following weighted distortion of a star with 3 leaves:
Dmin = min X |d(f(Vj),f(V))- 1| + X	|d(f(Vj),f(vfc)) - 2|,
min	j , k	,
1≤j≤3	(n- 1)/4	1≤j<k≤3
then
D(S ) >	(3)	D _ (n - I)Dmin	(9)
D(Sn) ≥ 2(n - 2)(n+1) Dmin = 6(n +1) .	⑼
Hence, it remains to find a lower bound on Dmin, i.e., a lower bound for a weighted distortion of
S3 with central node v and three leaves v1 , v2 , v3 . If we consider three angles at the node v, then at
least one of them is α ≤ 2∏∕3, so We can get a lower bound by only considering this triangle, which
is, w.l.o.g., formed by v, v1, v2 .
Dmin ≥ |d(f (v1), f(v2)) - 2| +
|d(f (vι), f (v)) - 1| + |d(f (v2), f (V)) - 1|
(n - 1)/2
Denote d(f(V1), f (V)) = x = 1 + ε, d(f(V2), f (V)) = y = 1 + δ, d(f(V1), f(V2)) = z =
2 + ε + δ -夕 with some ε, δ and some 夕 > 0 (from triangle inequality). Assume that ∣ε∣ < 1/2 and
∣δ∣ < 1/2 (otherwise the lower bound is trivial). Now we use the law of cosines to get a lower bound
on 夕.We consider Euclidean and hyperbolic spaces separately and note that the bound obtained in
Euclidean space also holds in spherical spaces (with any c).
In Euclidean space, using triangle inequality, we get φ = X + y - z > 0. So, in Euclidean and
spherical spaces 夕 is bounded below by a constant.
In hyperbolic space the law of cosines gives (recall that R
Cosh ɪ = Cosh X Cosh ɪ - sinh X Sinh y cos α,
R	R R RR
from which, using Cosh(X + y) = Cosh X Cosh y + sinh X sinh y, we get
Cosh X + y — cosh z = sinh X sinh ɪ (1 + cos α).
If R → ∞, then, similarly to Euclidean case, we get 夕=X + y - Z = Ω(1).
On the other hand, if R → 0, we get 夕=Ω(R). Note that Dmin ≥ |z 一 2| + lx-11+)/-11
∣ε + δ - H + (n-l)∕2. This gives us a lower bound Dmin
spaces and Dmin = Ω(min(R, 1)/n)=
Equation 9 the bound on D(Sn) follows.
______1_____
n max(√-c,1)
C(1/n) in spherical and Euclidean
in hyperbolic space. From this and
Ω
Now, let us get an upper bound on optimal distortion Dopt(Sn). To do this, we explicitly construct
an embedding with sufficiently low distortion D(Sn).
Let V be the central node, then we spread all other nodes uniformly on a 2-dimensional circle of
radius 1 centred at v. The smallest angle between two points is 2n/n. Therefore, from the law of
cosines, the distance between leaves is at least k with
Cosh ± = 1+(1 — cos -) sinh2 —.
R + ∖	n	R
15
Under review as a conference paper at ICLR 2020
Note that for any two leaves vi and vj we have that d(f(vi), f(vj)) ≤ 2. In particular, the closer two
leaves are, the greater the difference 2 - d(f(vi), f(vj)) is. Hence, the distance between adjacent
leaves is the worst case and thus Dopt(Sn) can be upper bounded as
n
Dopt(Sn) ≤	zn+η(2 — R ∙ arccosh
22
cos 旦) sinh2 ɪ + 1
nR
(n- 1)
-------- 2 — R ∙ arccosh
2(n +1) V
cos 的)sinh2 ɪ + 1
nR
Note that 1—cos 答=Θ (n) and sinh2 (R) = Θ (e2/R). Then, arccosh (Θ (ɪe2/R) + 1)behaves
as，2e2/R/n if 2/R《log n and as RR — log n if 2/R》log n. Therefore, We get
Dopt(Sn) = O (R log n)
B.4	Proof of Theorem 4.4 (distortion-based curvature for complete graphs)
If d = n — 2, then We are given a (n — 1)-simplex, Which can be embedded into n — 2-dimensional
spherical space. Indeed, the radius of circumscribed hypersphere for the (n — 1)-simplex With side
length a is known to be R = a Jn-1. Since we want the spherical distance between all points to
be equal to one, We need to choose a accordingly:
α
Sm 一
2
a
2R
for α
1
R
Here a corresponds to the angle giving the arc length 1, while the condition on sin 2 relates α and
a since α is the angle in a triangle with side lengths α, R, R. This implies that
a
2R arcsin	= 1,
and solving this equation for a yields
Plugging this back into the formula for the radius R we obtain
so that we have
C- (Kn)=4 }res® r 2n⅛)
Finally, let US show that if C → 一∞, then optimal distortion Dopt(Kn) → 0. This result follows
from the fact that Dopt(Sn) → 0, because to embed a clique, it is sufficient to embed a star on n + 1
node with edge lengths 1/2 and then remove the central node.
B.5	Proof of Theorem 4.2 (threshold-based curvature for star)
We show below that for any n, any dimension d and some curvature c there exists a perfect embed-
ding. Therefore Cddist(Sn) consists of curvatures for which such perfect embedding exists.
First, let us note that if there exists a perfect embedding f for some curvature c, then there exists
a perfect embedding for any curvature c0 < c. Indeed, w.l.o.g., we assume that the central node v
is mapped to the origin of a hyperspherical coordinate system and other points v1 , . . . , vn can be
described by their radii and angles. We know that the distance between v and any vi is at most 1
and the distance between any pair vi , vj is larger than one. Now we change curvature to c0 < c and
16
Under review as a conference paper at ICLR 2020
keep hyperspherical coordinates the same. Then the distance between v and any vi does not change,
while the distance between nodes vi , vj increases.
Now, it is easy to see that C increases with d: if there exists an embedding to some dimension d,
then, obviously, the same embedding works for d0 > d. Further, C decreases with n since if there
exists embedding of Sn , then we can easily construct an embedding of Sn0 for n0 < n by removing
some nodes.
Now, let us construct a perfect embedding of Sn and estimate the required curvature. Recall that in
a perfect embedding all leaves have to be inside the ball of radius 1 around the central node v and
also the distance between any two leaves has to be larger than one.
It is easy to see that if we managed to spread n points inside the ball of radius 1 with distances
more than 1 between them, then we can move each point along the radius up to distance 1 from v
preserving this property. Therefore, it is sufficient to spread all points on a hypersphere.
Let us first consider d = 2. In this case we can find an explicit analytic expression for C. This will
be an upper bound for any d > 2.
First, assume that n > 6. In this case we have to consider only hyperbolic space, since n neighbors
would not fit to a circle of radius 1 in neither spherical or Euclidean spaces.
We will find such largest curvature C which allows to have distance exactly 1 between the closest
leaves. In this case we cannot embed Sn in a space of curvature C , but can embed in a space of any
smaller curvature. We use Equation 7 and let a = 2∏:
cosh2R = 2⅛，
R
1
2 arccosh 2S*2
C
—
2 arccosh
2sin 2
2
NOtethat ifn is large,then sin 2 = Sin n-1 〜n-1. Then,arccosh2s1rψ 〜arccoshn-1 〜log n,
so We get C 〜-4 log2 n.
Now, let us consider n ≤ 6. Obviously, for n = 6 we have C = 0.
If n < 6, then C > 0. We use Equation 8:
1 cos α
R 1 - cos α ,
cos α 2
C = arccos -----------
1	- cos α
B.6 Proof of Theorem 4.3 (threshold-based curvature for trees)
As for Sn , We prove that for Tb a perfect embedding exists for d = 2 and some curvature c. Then,
similarly to the previous section, it is clear that C increases With b and decreases With n.
For the loWer bound on C, We have to guarantee that an embedding exists. For this, We provide
the folloWing construction (see Figure 5 for an illustration). (BeloW We assume that the curvature is
large enough for our construction to Work, then We estimate the required curvature.) First, We take
the node v and consider a circle of radius 1 around this node. We spread b + 1 neighbors uniformly
around this node. For our construction to Work, We need all distances betWeen these nodes to be
larger than 1. NoW, at some step of the algorithm, assume that We have all nodes at level l placed at
some circle centered at v and all distances betWeen the nodes at level l are larger than 1. Our aim is
then to find positions for all nodes at level l + 1.
Let us take any node at l-th level. Consider tWo points ul and ur on the same circle at distance 1
from the node u to the left and to the right, respectively. Let u, ul, zl and u, ur, zr form equilateral
17
Under review as a conference paper at ICLR 2020
Figure 5: Threshold-based embedding of trees
triangles (with sides equal to 1). Then We let the points at l + 1-th level to be spread on the circle
centred at V and passing through Zl and Zr. The children of u (uι,...,ub) will be placed on the
circular arc between Zl and Zr. As usual, we want Ui and ui+ι to be at distance at least 1 from each
other. Moreover, they have to be at distance at least 1 from children of other nodes. Also, note that
placing uι,...,ub between Zl and Zr guarantees that these nodes are closer than 1 to their parent
node u but at the same time at a distance larger than 1 from all other nodes at l-th level. Also, all
points at l-th level are far enough from points at l + 2-th level.
It remains to find a maximum curvature such that the required conditions are satisfied. Let r and r0
be radii of circles at l-th and l + 1-th levels and let 2α = ∠ulvu. We know (the law of cosines and
cos 2α = 1 - 2 sin2 α) that
cosh ɪ = 1 + 2 Sinh ɪ sin2 α .
RR
(10)
And the only condition we need for the whole procedure to work is that we have enough space on
the circular arc for placing b nodes there:
cosh ɪ ≤ 1 + 2 Sinh r sin2 α .
R	Rb
Now we note that sin2 玲 ≥ s⅛a for all b ≥ 1. Therefore, it is sufficient to have
CoSh ɪ ≤ 1 + 2 sinh r- ∙ sin2α .
R	R b2
Combining Equation 10 and Equation 11, we obtain:
(11)
N
Smh —
R
To achieve this, it is sufficient to have:
≥ b2 ∙ Sinh %.
r0 —
≥ 2 log b,
r
r0 - r
R≤
2	log b
It remains to find the lower bound for r0 - r and it is easy to see that r0 - r decreases with r.
Therefore, it is sufficient to consider only the second step of the construction procedure, when we
move from the circle of radius 1 to the next one. In this case, r = 1 and
r∕ = 2arccosh^⅛.
cosh 1/2
So, we have
-C ― ≤	2 log b
—R2	2arccosh Coohh112 - 1
2
18
Under review as a conference paper at ICLR 2020
noitrotsid
0----------------1-------------1---------------1--------------1----------
-2	-1.5	-1	-0.5	0	0.5
curvature
k = 0	k =	4	■	k =	8
k = 1	-X—	k =	5	―θ—	k =	9	-V—
k = 2	—>K—	k =	6	―0—	k = 10	-T—
k = 3	k =	7	-
k = 0 -k	=	4	-B-	k =	8	-A-
k = 1 ―夹—	k	=	5	―θ-	k =	9	-V-
k = 2 ―H⅞—	k	=	6	―9—	k = 10	-T—
k = 3 —El_	k	=	7	△ _______
noitrotsid
noitrotsid
Figure 6: Adding k edges to one node of a cycle on 10 nodes
k = 0 T—	k	=	4	-B-	k = 8	▲
k = 1 ―夹—	k	=	5	―θ-	k = 9	-V-
k = 2 ―τK—	k	=	6	―9—	k = 10	-T—
k = 3 —El_ k = 7 △ _______
Figure 7: Adding one (left) or two (right) dangling edges to k subsequent nodes of a cycle on 10
nodes
C Combinations of simple graphs
In this section, in order to get additional intuition, we illustrate how combining several simple sub-
graphs may affect the optimal curvature. We choose distortion for this illustrative example, since for
this loss we theoretically obtained non-trivial results: for some simple graphs the optimal curvature
is negative, while for other it is positive.
Indeed, for trees (and, in particular, stars) the optimal curvature is minus infinity and for cycles the
optimal curvature is positive. Our intuition is that combining stars and cycles in one graph would
give optimal curvature located between the curvatures of these simple graph.
Indeed, we observe such expected behavior for two illustrative examples shown in Figures 6 and 7.
In Figure 6 we take a cycle of length 10 and iteratively add edges to one of its nodes, i.e., we create
a star on one node. In Figure 7 we take the same cycle and add small stars sequentially to several of
its nodes: in the left figure we just add one dangling edge for each node (i.e., create a star subgraph
on 4 nodes), in the right figure we add two edges. In all cases, the optimal curvature drifts from
a positive one for a cycle to negative values. This drift is more pronounced In Figure 6, where we
create just one (relatively) large star.
D Volume-based curvature approximation
In this section, we describe the proposed curvature approximation. An input parameter of this esti-
mator is the maximum neighborhood size kmax (we fix kmax = 3 in the experiments). We take a
node v and for any k ≤ kmax we compute the number of nodes Nk (v) at distance exactly k from
it. Then, we discount this value using the number of edges between the nodes. This captures our
intuition that we need enough space to place the neighbors, but the connected neighbors are not
required to be far away from each other. Formally, let Ek (v) be the number of non-edges between
the pairs of nodes at distance k. Then, We say that Nk(V) is such that (Nk2(v)) = Ek(v). Finally,
we compute the curvature ck (v) such that the area of a hypersphere or radius k is equal to Nk0 (v).
19
Under review as a conference paper at ICLR 2020
Table 3: Dataset description
num. nodes num. edges
Dataset
Karate club (Zachary, 1977)	34	78
Dolphins (Lusseau et al., 2003)	62	159
Football (Newman & Girvan, 2004)	115	613
Political books (Newman, 2006)	105	441
Conflict (Ward et al., 2007)	127	253
Chicago (Eash et al., 1979)	822	821
CSPhDs (W. De Nooy & Batagelj, 2011)	1025	1043
Euroroad (Subelj & Bajec, 2011)	1039	1305
EuroSiS (Khokhar, 2015)	1272	6424
Power (Watts & Strogatz, 1998)	4941	6594
Facebook (Leskovec & Mcauley, 2012)	4039	88234
And our estimate for v is c(v) = mink≤kmax ck (v) (the worst-case curvature). To get a curvature
of the whole graph, we take a median of all ck(v). (Similarly to other curvature estimators, one can
consider a subsample of nodes instead of the whole node set to speed up the computation).
E Experiments
E.1 Datasets
Datasets used in our experiments are listed in Table 3. In all cases, we take only the giant connected
component (numbers of nodes and edges in Table 3 correspond to already filtered dataset). Synthetic
datasets are described in Section 4.3. The dataset T3,6 is a tree with branching factor 3 and depth 6.
E.2 Threshold-based loss functions
While our theoretical results in Section 4 hold for any threshold-based loss function, we noticed that
for less trivial datasets, where the perfect embedding is impossible, the choice of the loss function
becomes important. In most of the experiments we use zero-one loss function:
T1vX I [Vi 〜Vj ] ∙ I[d(f (Vi)f (Vj)) > 1] + (1 - I[Vi 〜Vj]) ∙ I [d(f(3),f (vj)) ≤ 1]
2 i,j
which essentially measures how well the values I[d(f(vi)f(Vj)) > 1] approximate I[vi 〜 vj].
However, due to the fact that the real-world graphs are usually sparse, optimizing zero-one loss may
lead to algorithms biased towards sparse embeddings. A standard solution to this is to consider
balanced accuracy instead of accuracy, which leads to balanced zero-one loss:
Pi,jI[Vi 〜Vj] ∙ I[d(f(Vi)f(Vj)) > 1]	Pi,j(1 -I[Vi 〜Vj]) ∙ I[d(f 3),f 侬))≤ 1]
2Pi,jI[3~ Vj]	+	2Pi,j(1-I[Vi-Vj])	.
The recent study by Gosgens et al. (2019) analyses the problem of bias discussed above in de-
tail. It formally defines and analyzes a constant baseline requirement and shows that it is satisfied,
in particular, by Pearson correlation coefficient between the vectors consisting of I [Vi 〜Vj ] and
I [d(f (Vi)f (Vj )) > 1]. So, we add the corresponding Pearson loss (one minus correlation coeffi-
cient) to our experiments (see (GOSgenS et al., 2019)).
E.3 Experimental setup
Our experiments are based on a publicly available implementation of graph embedding by Gu et al.
(2019).8 The main advantage of this algorithm is that it works for any curvature: negative, positive
or zero.
8https://github.com/HazyResearch/hyperbolics
20
Under review as a conference paper at ICLR 2020
Table 4: Compare curvature estimators, distortion, synthetic datasets
Dataset/dim	Ollivier		Forman		Avg. sectional		Grad. descent	
	c	loss	c	loss	c	loss	C	loss
S100 / 2	0.0	0.334	-97.0	0.058	-1.0	0.304	-1.45	0.289
S100 / 10	0.0	0.127	-97.0	0.014	-1.0	0.1	-30.2	0.025
T3,6 / 2	-0.33	0.136	-2.0	0.254	-0.5	0.174	-9.77	0.271
T3,6 / 10	-0.33	0.016	-2.0	0.243	-0.5	0.036	0.0	0.077
K100 / 2	0.99	0.348	100	0.841	0.125	0.342	-48.7	0.163
K100 / 10	0.99	0.134	100	0.841	0.125	0.124	-305	0.015
C100 / 2	0.0	0.106	0.0	0.106	0.01	0.257	0.0	0.106
C100 / 10	0.0	0.106	0.0	0.106	0.01	0.255	0.0	0.106
K100,100 / 2	0.0	0.372	-196	0.294	0.008	0.372	2.1	0.309
K100,100 / 10	0.0	0.301	-196	0.275	0.008	0.301	-139	0.27
Table 5: Compare curvature estimators, zero-one loss, synthetic datasets
Dataset	Ollivier		Forman		Avg. sectional		Grad. descent	
	c	loss	c	loss	c	loss	C	loss
S100 / 2	0.0	0.02	-97.0	0.004	-1.0	0.027	0.0	0.02
T3,6 / 2	-0.33	0.006	-2.0	0.005	-0.5	0.005	0.0	0.002
C100 / 2	0.0	0.005	0.0	0.005	0.01	0.009	0.16	0.012
K100,100 / 2	0.0	0.496	-196	0.492	0.008	0.496	-531	0.493
K100,100 / 10	0.0	0.489	-196	0.491	0.008	0.480	-124	0.490
We modified the implementation for our task: in particular, we added a regime responsible for
minimizing threshold-based loss functions. E.g., zero-one loss is proportional to
XI[vi 〜Vj] ∙ I[d(f(vi)f(vj)) > 1] + (1 - I[vi 〜Vj]) ∙ I[d(f(vi),f(vj)) ≤ 1]∙
i,j
This function is not differentiable, so we replace it by gradient descent friendly function which we
call ReLuLoss:
XI[Vi 〜Vj] ∙ReLu(d(f(vi),f(vj))-(1-ε)) + (1-1® 〜Vj])∙ReLU((1 + ε) -d(f(vi),f (Vj)),
i,j
where ReLU(x) = max(0, x), ε = 0.001. In our experiments we observed that such loss function
outperforms other analogues (e.g., MSE and sigmoid-based smoothing) since it speeds up the con-
vergence. To adapt to various threshold-based loss functions, we also reserve several iterations to
gradient steps based on the corresponding sigmoid-based smoothing.
Note that in experiments we use only the more advanced Forman curvature F (G). For computing
Ollivier and Forman curvature we use the open source package by Ni et al. (2015).9
For the embedding algorithm, in all experiments we fix epochs = 1000 and we also try several
learning rates (usually from 0.01 to 104). We noticed that learning rate significantly affect the per-
formance of the learning algorithm and also the optimal learning rate is usually smaller for smaller
datasets and it is larger for spaces of negative curvature. We also use the option use_adagrad = True
since we noticed that it inproves the embedding into Euclidean spaces (otherwise there is a peak in
distortion at c = 0).
Let us emphasize that in our experiments we do not care about overfitting, because we do not analyze
the embedding algorithm, our aim is to find a curvature which minimizes loss function. (It can be
also thought of as though we focus on graph reconstruction task).
9https://github.com/saibalmars/GraphRicciCurvature
21
Under review as a conference paper at ICLR 2020
0.1
-2
0.15
535
0.3 0. 0.2
noitrotsid
-1.5
SsO- 3UO,O-3Z
0.3
0.25
0.2
0.15
ssol nosraep
-1.5
Sso- 3UO-O-3Z P8ujs2q
Figure 8: Effect of curvature, Karate club dataset
To plot all the figures we try curvatures in some range (usually between -2 and 1) with step size 0.1.
Our code is publicly available.10
E.4 Comparison of curvatures on synthetic datasets
See the results in Tables 4 and 5.
E.5 Effect of dimension and loss function
In this section, we visualize the dependence of the loss function on curvature (see Figures 8-18). In
particular, we demonstrate that the optimal curvature significantly depends on both loss function and
dimension. More precisely, while our theoretical results hold for any threshold-based loss function,
for less trivial graphs the optimal curvature depends on the particular choice.11
The obtained results support our intuition (based on theoretical results) that while hyperbolic space
helps in many cases for small dimensions, it becomes less pronounced in larger dimensions. This is
most clearly seen for distortion loss (see, e.g., Karate club, Football, CSPhDs, Euroroad, EuroSiS,
Power).
E.6 Empirical analysis of volume-based curvature
As discussed in the previous section, optimizing distortion leads to more stable results than
threshold-based losses, so we focus on this loss here. It turns out that the proposed volume-based
estimator is able to reflect well whether it is reasonable to embed a network to a hyperbolic space for
a given loss function ifwe care about distortion. Table 6 shows the estimated values ofa curvature.12
10https://drive.google.com/open?id=12i5LD6yTyFRDrgkMJEencDAI0Au6Isrn
11We note that the curves corresponding to threshold-based losses are noisy, especially for small datasets,
since such loss functions are discrete and multimodal, so they are harder to optimize. To smooth the results, we
added several tricks to the optimizer and also choose the optimum embedding based on several random restarts
of the algorithm. Although the results are still noisy, the general trends are usually clearly seen.
12The value 0.1 appears due to a technical reason: in spherical space of large curvature there can be no points
at distance k, so we limit considered curvatures.
22
Under review as a conference paper at ICLR 2020
noitrotsid
ssol nosraep
0.45
0.4
0.35
0.5
-0.5
curvature
Sso- 3UO-O-3Z
0.3
0.25
0.2
0.1
0.15
0.35
-1	-0.5	0	0.5
0.05
-2	-1.5
curvature
Sso- 3UO,O-3Z P8u2-q
Figure 9: Effect of curvature, Dolphins dataset
.35
ssol nosraep
352
0. 0.2 0.
noitrotsid
-1	-0.5	0	0.5
curvature
-1
0.5
-0.5
curvature
SsO- 3UO,O-3Z
curvature
Sso- 3UO-O-3Z P8u2-q
Figure 10: Effect of curvature, Football dataset
23
Under review as a conference paper at ICLR 2020
-2
0. 0.2 0. 0.1 0.
noitrotsid
-1.5
-1	-0.5	0	0.5	1
curvature
ssol nosraep
noitrotsid
ssol nosraep
0.45 0.4 0.35
-2
35
0. 0.2
-1.5
-1	-0.5	0	0.5	1
curvature
-1	-0.5	0	0.5
-2
1

curvature
0.5
-0.5
curvature
Figure 11: Effect of curvature, Political books dataset
SsO- 3UO,O-3Z
Sso- 3UO-O-3Z P8u2-q
Figure 12: Effect of curvature, Conflict dataset
24
Under review as a conference paper at ICLR 2020
noitrotsid
noitrotsid
ssol nosraep
Sso- 3UO-O-3Z
ssol nosraep
0
-1	-0.8	-0.6	-0.4	-0.2	0	0.2	0.4	0.6	0.8
0.1
-2
curvature
-1.5
-1	-0.5
0.5
1
Sso- 3UO,O-3Z P8u2-q
Figure 13: Effect of curvature, Chicago dataset
0.02
0. 0. 0.
ssol eno-orez
1864
0. .0 .0 .0
-2	-1.5
-0.5	0	0.5	1
0.45
curvature
453525
0. .3 0. .2 0. .1
0 0 0
ssol eno-orez decnalab
0.5
0.1
-2	-1.5	-1	-0.5
1
curvature
curvature
Figure 14: Effect of curvature, CSPhDs dataset
25
Under review as a conference paper at ICLR 2020
noitrotsid
-2
Sso- 3UO-O-3Z
noitrotsid
ssol nosraep
ssol nosraep
Sso- 3UO,O-3Z P8u2-q
Figure 15: Effect of curvature, Euroroad dataset
-2

0.55 0.5 0.45
4535251
0. 0.3 0. 0.2 0. 0.1 0.
-0.5	0	0.5	1
curvature
SsO- 3UO,O-3Z
-0.5	0	0.5	1
curvature
Sso- 3UO-O-3Z P8u2-q
Figure 16: Effect of curvature, EuroSiS dataset
26
Under review as a conference paper at ICLR 2020
Ld==10 士
3UO,O-3Z
0.18
0.16
0.14
0.12
0.1
0.08
0.06
0.04
0.02
0	0.5	1
-1.5
-0.5
curvature
-2	-1.5	-1	-0.5
0	0.5
-1.5	-1
SsO- 3UO,O-3Z P3uujss
0.44
0.42
0.4
0.38
0.36
0.34
0.32
0.3
0.28
0.26
0.24
0.22
1	-2
curvature
-1.5
0	0.5	1
-0.5
curvature
Figure
-0.5
curvature
17: Effect of curvature, Power dataset
noitrotsid
0.15
0.1
0.05
-2
0.5
-1.5	-1	-0.5	0
curvature
Figure 18: Effect of curvature, Facebook dataset, distortion loss
27
Under review as a conference paper at ICLR 2020
Table 6: Volume-based curvature
Dataset	d = 2	d= 10
Karate club	-6.7	0.1
Dolphins	-3.3	0.1
Football	-2.1	0.1
Political books	-1.6	0.1
Conflict	-1.8	0.1
Chicago	0.1	0.1
CSPhDs	0.07	0.1
Euroroad	0.1	0.1
EuroSiS	-3.7	0.1
Power	0.1	0.1
Facebook	-11	0.1
One can see that for d = 2 in many cases significantly negative curvature is predicted which is in-
deed confirmed by our experiments, shown in Figures 8-18 (distortion loss). Non-negative curvature
is predicted for Chicago, CSPhDs, Euroroad and Power and indeed for these datasets the optimal
curvature is close to zero. In contrast, for d = 10 negative curvature is never predicted, which agrees
with the experiment well. So, in these cases it is not reasonable to embed networks to a hyperbolic
space.
28