Under review as a conference paper at ICLR 2020
Cross Domain Imitation Learning
Anonymous authors
Paper under double-blind review
Abstract
We study the question of how to imitate tasks across domains with discrepancies such
as embodiment and viewpoint mismatch. Many prior works require paired, aligned
demonstrations and an additional RL procedure for the task. However, paired, aligned
demonstrations are seldom obtainable and RL procedures are expensive. In this work,
we formalize the Cross Domain Imitation Learning (CDIL) problem, which encom-
passes imitation learning in the presence of viewpoint and embodiment mismatch.
Informally, CDIL is the process of learning how to perform a task optimally, given
demonstrations of the task in a distinct domain. We propose a two step approach to
CDIL: alignment followed by adaptation. In the alignment step we execute a novel unsu-
pervised MDP alignment algorithm, Generative Adversarial MDP Alignment (GAMA),
to learn state and action correspondences from unpaired, unaligned demonstrations. In
the adaptation step we leverage the correspondences to zero-shot imitate tasks across
domains. To describe when CDIL is feasible via alignment and adaptation, we introduce
a theory of MDP alignability. We experimentally evaluate GAMA against baselines
in both embodiment and viewpoint mismatch scenarios where aligned demonstrations
don’t exist and show the effectiveness of our approach.
1	Introduction
Humans possess an astonishing ability to recognize latent structural similarities between behaviors in
related but distinct domains, and learn new skills from cross domain demonstrations alone. Not only
are we capable of learning from third person observations that have no obvious correspondence to our
internal self representations (Stadie et al., 2017; Liu et al., 2018; Sermanet et al., 2018), but we also are
capable of imitating agents with different embodiments (Gupta et al., 2017; Rizzolatti & Craighero, 2004)
as can be observed in an infant’s learning of visuomotor skills from adults with different biomechanics
and physical capabilities (Jones, 2009). Previous work in neuroscience (Marshall & Meltzoff, 2015)
and robotics (Kuniyoshi & Inoue, 1993; Kuniyoshi et al., 1994) have recognized the pitfalls of exact
behavioral cloning in the presence of domain discrepancies and posited that the effectiveness of the human
imitation learning mechanism hinges, crucially, on the capability to learn structure preserving domain
correspondences. These correspondences enable the learner to internalize the expert demonstrations and
produce a reconstruction of the behavior in the self domain. Consider a young child that has learned to
associate his internal body map with the limbs of an adult. When the adult demonstrates running, the
child is able to imagine himself running, and reproduce the behavior.
Recently, separate solutions have been proposed for imitation learning across two kinds of domain
discrepancies: embodiment (Gupta et al., 2017) and viewpoint (Liu et al., 2018; Sermanet et al., 2018)
mismatch. These works (Liu et al., 2018; Sermanet et al., 2018; Gupta et al., 2017) require paired,
time-aligned demonstrations to obtain state correspondences and an extra RL step with a proxy reward.
However, paired, aligned demonstrations are seldom obtainable and RL loops are expensive. In this work
we formalize the Cross Domain Imitation Learning (CDIL) problem which encompasses prior work in
imitation learning across domains with viewpoint and embodiment mismatch. Informally, CDIL is the
process of learning how to perform a task optimally in a self domain, given demonstrations of the task in
a distinct expert domain. We propose a two-step approach to CDIL: alignment followed by adaptation. In
the alignment step we execute a novel unsupervised MDP alignment algorithm, Generative Adversarial
MDP Alignment (GAMA), to learn state, action maps from unpaired, unaligned demonstrations. In
the adaptation step we leverage the learned state, action maps to zero-shot imitate tasks across domains
without an additional RL step. To shed light on when CDIL can be solved by alignment and adaptation,
we first introduce a class of structure preserving maps, called MDP reductions, that adapts optimal policies
between MDPs (section 3). We further characterize a family of MDP pairs that share reductions, formally
state the MDP alignment problem, and elucidate its connection to CDIL. In section 4, 5 we derive GAMA,
1
Under review as a conference paper at ICLR 2020
a.
b∙	Alignment unpaired c-	Adaptation
Imitation Loss
mmE[‰(⅛∣∣πx)]
Figure 1: (a). Illustration of paired, aligned vs unpaired, unaligned demonstrations in the alignment task
set Dx,y (b). Alignment: we learn state, action maps f,g between the self (x) and expert (y) domain from
unpaired, unaligned demonstrations by minimizing a distribution matching loss and an imitation loss. (c)
Adaptation: adapt the expert domain policy πy,τ or demonstrations to obtain a self domain policy ∏x,τ
a simple training algorithm to learn MDP reductions. In section 6, we experimentally evaluate GAMA and
find that meaningful state correspondences between various domains are learned from unpaired, unaligned
demonstrations. We then compare the CDIL performance of GAMA against several baselines in both
embodiment and viewpoint mismatch scenarios and show the effectiveness of our approach.
De Dιst. Match
→ mind(σx→y ɑŋ
£g
2	Cross Domain Imitation Learning problem statement
An infinite horizon Markov Decision Process (MDP) M ∈ Ω with deterministic dynamics is a tuple
(S, A, P, η, R) where Ω is the set of all MDPs, S is the state space, A is the action space, P : S X A → S
is a (deterministic) transition function, R : S ×A → R is the reward function, and η is the initial
state distribution. A domain is an MDP without the reward, i.e (S, A,P,η). Intuitively, a domain
fully characterizes the embodied agent and the environment dynamics, but not the desired behavior. A
task T is a label for an MDP corresponding to the high level description of optimal behavior, such as
"walking". T is analogous to category labels for images. An MDP with domain x for task T is denoted
by Mx,T = (Sx, Ax,Px,ηx,Rx,T), where Rx,T is a reward function encapsulating the behavior labeled
by T. For example, different reward functions are needed to realize the "walking" behavior in two
morphologically different humanoids. A (stationary) policy for Mx,T is a map πx,T : Sx → B(Ax)
where B is the set of probability measures on Ax and an optimal policy π* T = arg maxπæ J(πx) achieves
the highest policy performance J(πx)=Eπ [P∞=0 γtRx,T(s(xt) , a(xt) )] where 0 <γ<1 is a discount
factor. A demonstration of length H is a sequence of state, action tuples τM T = {(s(xt) ,a(xt))}tH=1
sampled from an optimal policy and DM T = {τM(k) }K=1 is a set of demonstrations for Mx,T
Let Mx,T, My T be self and expert MDPs for a target task T. Given expert domain demonstrations
DMry T, Cross Domain Imitation Learning (CDIL) aims to determine an optimal self domain policy π*T
witho,ut access to the reward function Rx,T. In this work we propose to first solve an MDP alignment
problem and then leverage the alignments to zero-shot imitate expert domain demonstrations. Like prior
work (Gupta et al., 2017; Liu et al., 2018; Sermanet et al., 2018), we assume the availability ofan alignment
task set Dx,y = {(DM T , DM T )}N=1 containing demonstrations for N tasks {Ti}N=1 from both the
self and expert domain. Dx,y could, for example, contain both robot (x) and human (y) demonstrations
for a set primitive tasks such as walking, running, and jumping. Unlike prior work, demonstrations are
unpaired and unaligned, i.e (s(xt),s(yt)) may not be a valid state correspondence. (see Figure 1(a)) Paired,
time-aligned cross domain data is expensive and may not even exist when task execution rates differ or
there exists systematic embodiment mismatch between the domains. For example, a child can imitate an
adult running, but not achieve the same speed. Our set up emulates a natural setting in which humans
compare how they perform tasks to how other agents perform the same tasks in order to find structural
similarities and identify domain correspondences. We now proceed to introduce a theoretical framework
that explains how and when the CDIL problem can be solved by MDP alignment followed by adaptation.
3	Alignable MDPs
Let Π*M be the set of all optimal policies for MDP M. We define an occupancy measure (Syed et al.,
2008) qπ : S × A → R for policy π as qπ(s, a) = π(a∣s) P∞=0 Yt Pr(S⑴=s; π, P, η).
Definition 1. An optimality function OM : Sx ×Ax →{0, 1} for an MDP Mx satisfies:
OMaJ(Sx, ax) = 1 if ∃∏* ∈ ∏-Mx SuChthat(Sx, ax) ∈ supp(qπ*) and OMaJ(Sx, ax) =0 otherwise.
2
Under review as a conference paper at ICLR 2020
Definition 2. An MDP reduction from Mx =(Sx , Ax ,Px ,ηx ,Rx ) to My =(Sy , Ay ,Py ,ηy ,Ry) is a
tuple r =(φ, ψ) where φ : Sx →Sy,ψ : Ax →Ay are maps that preserve:
1.	(optimal policy) ∀sx ∈Sx ,ax ∈Ax ,sy ∈Sy ,ay ∈Ay,
OMy(φ(sx),ψ(ax)) = 1	⇒	OMx(sx,ax)=1	(1)
OMy (Sy ,ay) = 1	⇒	φ-1 (Sy) = 0, ψ-1 (ay) = Q	⑵
2.	(dynamics) ∀sy,s0y ∈Sy, ay ∈Ay where OM (sy,ay)=1,
Py (Sy ,ay)=φ(Px (Sx , ax)) ∀Sx ∈ φ (Sy),ax ∈ ψ	(ay)	(3)
where we define φ-1(sy) = {sx∣φ(sx) = Sy}, ψ-1(ay) = {ax∣ψ(ax) = ay}. Furthermore, r is an
MDP permutation if and only if φ, ψ are bijective maps.
In words, Eq. 1 states that only optimal state, action pairs in
x can be mapped to optimal state, action pairs in y and Eq.
2 states that r must be surjective on the set of optimal state,
action pairs in y. Eq. 3 states that a reduction must preserve
(deterministic) dynamics. We use the notation Mx ≥φ,ψ
My to denote that (φ, ψ) is a reduction from Mx to My,
and the shorthand Mx ≥My to denote that Mx reduces to
My . To gain an intuitive understanding of MDP reductions,
picture the execution trace of an optimal policy as a directed
∖1√ ∖1√ ∖1√ ∖1√ ∖1√
13 4 5 6
r(κ r(κ r(κ r(κ r(κ
00000
砥②③④⑤
N 卜
My (a)-√b)_
W(∙) = Ψ(□) =■
Figure 2: Example MDP reduction from
Mx toMy. φ, ψ are state and action maps
graph with colored edges in which the nodes correspond to states visited by an optimal policy, and
the colored edges correspond to actions taken. An MDP reduction from Mx to My homomorphs the
execution graph of an optimal policy in Mx to a execution graph of an optimal policy in My . Figure
2 shows an example of a valid reduction from Mx to My: states 1, 2 in Sx are mapped (merged) to
state a in Sy and the blue, green actions in Ax are mapped to the brown action in Ay . Intuitively, if
Mx ≥φ,ψ My, then (φ, ψ) compresses Mx by merging all optimal state, action pairs that have identical
dynamics properties.
Definition 3. Two MDPs Mx, My are alignable if and only if Mx ≥My or My ≥Mx.
Definition 3 states that MDPs are alignable if reductions exists between them, meaning that they share
structure. We use Γ(Mx, My) = {(φ, ψ)∣Mx ≥φ,ψ My } to denote the set of all valid reductions from
Mx to My . Reductions have a particularly useful property which is that they adapt policies across
alignable MDPs. Consider a state map f : Sx →Sy, an inverse action map g : Ay →Ax , and a
composite policy πx = g ◦ πy ◦ f (see Figure 1(b)). In words，πx maps a self state to an expert state
via f, simulates the expert’s action choice for the mapped state via πy , then chooses a self action that
corresponds to the simulated expert action with g. The following lemma holds for πx.
Lemma 1. Let Mx , My be MDPs satisfying Assumption 1 (see Supp. Materials), Mx ≥φ,ψ My, and
πy be optimal in My. ∀g : Ay → Ax s.t ψ ◦ g(ay) = ay ∀ay ∈ {ay∣∃sy ∈ Sy s.t OMy (sy ,ay) = 1},
it holds that πx = g ◦ πy ◦ φ is optimal in Mx.
Lemma 1 states that the state, action maps (f,g-1) chosen to be a reduction can adapt optimal policies
between alignable MDPs. Here onwards we interchangeably refer to (f, g) as "alignments". We now
show how the CDIL problem can be solved by first solving an MDP alignment problem followed by an
adaptation step.
Definition 4. Let (Mx, My), (Mx0, My0) ∈ Ω2 be two MDPpairs. Then, (Mx, My)〜(Mx0, My0),
i.e they are joint alignable, if and only if Γ(Mx, My) ∩ Γ(Mx0, My0) 6= 0.
In words, two MDP pairs are joint alignable if there exists a shared reduction. We define an equiva-
lence class [(Mx, My)]〜={(Mx0, My0) | (Mx0, My0)〜(Mx, My)} of MDP pairs that share
reductions. Overloading notation, Γ({(Mxi, Myi)}N=1) = {(φ,ψ) | (φ, ψ) ∈ Γ(Mx 1, Mx1) ∩ …∩
Γ(MxN , MxN )}. We now formally state the MDP alignment problem: Let (Mx,T , My T ) be an
MDP pair for a target task T. Given an alignment task set Dx,y = {(DM T , DM T )}N=1 compris-
ing unpaired, unaligned demonstrations for MDP pairs {(Mx,τ, My T)}N=1 ⊆ [(Mx,τ,My T)]〜,
determine (φ, ψ) ∈ Γ({(Mx,T , My T)}iN=1) such that (φ, ψ) ∈ Γ(Mx,T, My T). As shown
in Figure 3, with more MDP pairs, there are likely a smaller the number of joint align-
ments ∖Γ({(MxTi,MyT)}N=1 )| and, as a result, (φ,ψ) ∈ Γ({(Mx,7i,My,T)}N=1) is more
likely to "generalize" to an MDP pair for a new target task (Mx,T, My T) in the equivalence
class. Analogously, in a standard supervised learning problem, more training data is likely to
shrink the set of models performing optimally on the training set but poorly on the test set.
3
Under review as a conference paper at ICLR 2020
We can then use (φ, ψ) for CDIL: given cross domain demonstrations DM
y,T
for the target task T, learn an expert domain policy πy,T , and adapt it into
the self domain using (φ, ψ) according to Lemma 1.
We can now assess when domains with embodiment and viewpoint mismatch
have meaningful state correspondences, i.e MDP reductions, thus allowing
for cross domain imitation. The states of a human expert with more degrees
of freedom than a robot imitator can be merged into the robot states if the
task only requires the robot’s degrees of freedom and the execution traces
share structure, e.g traces are both cycles. However, if the task requires
all degrees of freedom possessed only by the human, the robot cannot find
meaningful correspondences, and also cannot imitate the task. Two MDPs
Figure 3: Illustration of
MDP alignment problem
for different viewpoints of an agent performing a task are MDP permutations since there is a one-to-one
correspondence between state, actions at same timestep in the execution trace of an optimal policy.
4	Learning MDP Reductions
We now derive objectives that can be optimized to learn MDP reductions. We propose distribution
matching and policy performance maximization. We first define the distributions to be matched.
Definition 5. Let Mx, My be two MDPs and πx = g ◦ πy ◦ f for f : Sx → Sy, g : Ay → Ax and
policy ∏y∙ P = {syt) ,^yt) }t≥
0 is the co-domain policy execution process realized by running πx, i.e:
sx0) 〜ηx,Syt) = f(Sxt)0y" ~πy(同"),aXt)= g(Gyt)),sxt+1) = Px(Sxt),aXt)) ∀t ≥0	(4)
The target distribution σy is over transitions uniformly sampled from execution traces of πy and the
proxy distribution σXTy is over cross domain transitions uniformly sampled from realizations of P.
σ∏y (Sy，ay，Sy ) =	7l→m∞	p Pt = 0 Pr(Sy)= Sy，ay =	=	ay，Sy + =	= Sy ; πy，Py，ηy )	⑸
σx→→y (Sy, ay, Sy) =	7l→m∞	1 pT-01 Pr(Syt)	= Sy,	&yt)=	ay, Syt+1)	= Sy ；P)	⑹
We now propose three concrete objectives: 1. πx is optimal, 2. σXTy = σy^, 3. g is injective. In other
words, we seek to learn f, g that matches distributions over transition tuples in domain y while maximizing
policy performance in domain x. The former captures the dynamics preservation property from Eq. 3
and the latter captures the optimal policy preservation property from Eq. 1, 2. The following theorem
uncovers the connection between our objectives and MDP reductions.
Theorem 1. Let Mx, My be MDPs satisfying Assumption 1 (see Supp Materials). If Mx ≥My, then
∃f : Sx →Sy ,g : Ay →Ax, and an optimal covering policy πy (Supp Materials, Def 6) that satisfy
objectives 1, 2. Conversely, if ∃f : Sx →Sy,g : Ay →Ax and an optimal covering policy πy satisfying
objectives 1, 2, 3, then Mx ≥My and ∃(φ, ψ) ∈ Γ(Mx, My) s.t f = φ and ψ ◦g(ay)=ay, ∀ay ∈Ay.
Theorem 1 states that if two MDP are alignable, then objectives 1, 2 can be satisfied. Conversely, if
objectives 1, 2, 3 can be satisfied for two MDPs, then they must be alignable and all solutions (f, g) are
MDP reductions. While Theorem 1 requires alignable MDPs to guarantee identifiability, our experiments
will also run on MDPs that are not perfectly alignable, i.e. Eq. 1, 2, 3 do not hold exactly, but intuitively
share structure. In the next section, we propose a simple algorithm to learn MDP reductions.
5 Generative Adversarial MDP Alignment
Building on Theorem 1, we propose the following general form training objective for aligning MDPs:
min -J (πx ) + λd(σΠTy , σΠy )
(7)
where J(πx) is the performance of ∏x, d is a distance metric between distributions, and λ > 0 is a
Lagrange multiplier. In practice, we found that injectivity of g is unnecessary to enforce in continuous
domains. We now present an instantiation of this framework: Generative Adversarial MDP Alignment
(GAMA). Recall that we are given an alignment task set Dx,y = {(DM T , DM T )}N=1. In the
alignment step, We learn π* T, ∀Ti and parameterized state, action maps fθ于:Sx → Sy, gθg : Ay → Ax
that compose ΠxT = gθg ◦ π* T ◦ fθf. To match σXTy, σ∏y, we employ adversarial training (Goodfellow
4
Under review as a conference paper at ICLR 2020
et al., 2014) in which separate discriminators Dθi per task are trained to distinguish between "real"
transitions (Sy, ay, s0) ~ π* T and "fake" transitions (Sy, ay, s0) ~ ∏χ,τ, where Sy = fθ于(sχ), ay =
∏y (Sy), Sy = fθ于(PXp (sχ, g(^y))), and Pχ is a fitted model of the X domain dynamics. (see Figure
1(b)) The generator, consisting of fθ ,gθ , is trained to fool the discriminator while maximizing policy
performance. The distribution matching gradients are back propagated through the learned dynamics,
∏y T is learned by Imitation Learing (IL) on DMy T , and the policy performance objective on ∏χ,τ is
achieved by IL on DM T . In this work we use behavioral cloning (Pomerleau, 1991) for IL. We thus
x ,Ti
seek to find a saddle point {f, g} ∪ {Dθi }iN=1 of the following objective:
min max PN=I (Esχ~碟 T [DκL(π*,7i(∙lsx)llπx,7i(1Sx))]
f,g {Dθi }iN=1	, i	(8)
+ λ(E∏y,T [log De、(sy, ay, Sy)] + E∏χ,4 [log(1 - Dθb (Sy, ay, ^))])
where DKL is the KL-divergence. We provide the full execution flow of GAMA in Algorithm 1
In the adaptation step, we are given expert demonstrations DM T of a new target task T, from which
we fit an expert domain policy πy,T which are composed with t,he learned alignments to construct an
adapted self policy Πx,T = gθg ◦ ∏y,T ◦ fθ于.We also experiment with a demonstration adaptation method
which additionally trains an inverse state map f-1 : Sy →Sx, adapts demonstrations DM T into the
self domain via f-1,g, and applies behavioral cloning on the adapted demonstrations. (see Figure 1(c))
Notably, our entire procedure does not require paired, aligned demonstrations nor an RL step.
Algorithm 1: Generative Adversarial MDP Alignment (GAMA)
ι input: Alignment task set Dxy = {(Dmx T , DMy T )}N=ι of unpaired trajectories, fitted ∏* T
2	while not done do:
3	for i =1,...,Ndo:
4	Sample (sx,ax,sx) ~ DMx T, (sy,ay,s∖) ~ DMyT and store in buffer Bx, By
5	for j =1,...,Mdo:
6	Sample mini-batch j from Bxi , Byi
7	Update dynamics model with: -E∏* T [Vθp(Px (sx, ax) 一 s0)2]
8	Update discriminator: E∏*T [Vθ^ log DgiD (Sy, a,y, Sy)] + E公T [Vθ^ log(1 — Dg^ (Sy, ^y,sy))]
9	Update alignments (fθ ,gθ ) with gradients:
-E∏^,Ti [vθf log dΘd (Sy, ay, Sy)]+ E∏xcTi [vθf (πx,Ti (sx) 一 ax)2]
-E∏xT [Vθg log DθD (Sy, ay, sy)]+ E∏.,7; [Vθg (πx,Ti (Sx) - ax)2↑
Related Works: Closely related to CDIL, the field of cross domain transfer learning in the context of
RL has explored approaches to use state maps to exploit cross domain demonstrations in a pretraining
procedure for a new target task for which self domain reward function is available. Canonical Corre-
lation Analysis (CCA) (Hotelling, 1936) finds invertible projections into a basis in which data from
different domains are maximally correlated. These projections can then be composed to obtain a direct
correspondence map between states. Ammar et al. (2015); Joshi & Chowdhary (2018) have utilized
an unsupervised manifold alignment (UMA) algorithm which finds a linear map between states with
similar local geometric properties. UMA assumes the existence of hand crafted features along with a
distance metric between them. This family of work commonly uses a linear statemap to define a time-step
wise transfer reward and executes an RL step on the new task. Similar to our work, these works use
an alignment task set of unpaired, unaligned trajectories to compute the state map. Unlike these works,
we learn maps that preserve MDP structure, use deep neural network state, action maps, and achieve
zero-shot transfer to the new task without an RL step. More recent work in transfer learning across
embodiment (Gupta et al., 2017) and viewpoint (Liu et al., 2018; Sermanet et al., 2018) mismatch obtain
state correspondences from an alignment task set comprising paired, time-aligned demonstrations and use
them to learn a state map or a state encoder to a domain invariant feature space. In contrast to this family
of prior work, our approach learns both state, action maps from unpaired, unaligned demonstrations. Also,
we remove the need for additional environment interactions and an expensive RL procedure on the target
task by leveraging the action map for zero-shot imitation. Stadie et al. (2017) have shown promise in using
domain confusion loss and generative adversarial imitation learning (Ho & Ermon, 2016) for learning
across small viewpoint mismatch without an alignment task set, but fails in dealing with large viewpoint
differences. Unlike Stadie et al. (2017), we leverage the alignment task set to succeed in imitating across
5
Under review as a conference paper at ICLR 2020
larger viewpoint mismatch and do not require an RL procedure. MDP homomorphisms (Ravindran &
Barto, 2002) have been explored with the aim of compressing state, action spaces to facilitate planning. In
similar vein, related works have proposed MDP similarity metrics based on bisimulation methods (Ferns
et al., 2004) and Boltzmann machine reconstruction error (Ammar et al., 2014). While conceptually
related to our MDP alignability theory, these works have not proposed scalable procedures to discover the
homomorphisms and have not drawn connections to cross domain learning.
6	Experiments
Ours experiments were designed to answer the following questions: (1). Can GAMA uncover MDP
reductions? (2). Can the learned alignments (fθ ,gθ ) be leveraged to succeed at CDIL? Note that we
include experiments with MDP pairs that are not perfectly alignable, yet intuitively share structure, to
show general applicability of GAMA for CDIL. We propose three metrics to evaluate the effectiveness
of GAMA. First, alignment complexity which is the number of MDP pairs, i.e number of tasks, in the
alignment task set needed to learn alignments that enable zero-shot imitation, given ample cross domain
demonstrations for the target tasks. Second, adaptation complexity which is the amount of cross domain
demonstrations for the target tasks needed to successfully imitate tasks in the self domain without querying
the target task reward function, given a sufficiently large alignment task set. Finally, transferability, which
is the environment sample complexity on the target task when using the alignment procedure as weight
initialization then running RL with the target task reward function. While we aim to learn optimal self
policies without querying the self domain reward function, this metric measures the usefulness of the
alignment step even when MDP pairs in the alignment task set are not in the equivalence class of the
target MDP pair. We study two ablations of GAMA and compare against the following baselines:
GAMA - Policy Adapt (GAMA-PA): learns alignments by Algorithm 1, fits an expert policy πy,T to
DMy τ for a new target task T and zero-shot adapts ∏y,τ to the self domain via ∏χ,τ = gθg ◦ ∏y,τ ◦ fθ于.
GAMA - Demonstration Adapt (GAMA-DA): trains f-1 in addition to Algorithm 1, adapts DM
y,T
into the self domain via (f-1, g), and fits a self domain policy on the adapted demonstrations.
Self Demonstrations (Self-Demo): We behavioral clone on self domain demonstrations for the target
task. This baseline provides an "upper bound" on the adapation complexity of CDIL.
Canonical Correlation Analysis (Hotelling, 1936) (CCA): finds invertible matrices Cx ,Cy to a basis
where domain data are maximally correlated from unpaired, unaligned demonstrations.
Unsupervised Manifold Alignment (Ammar et al., 2015) (UMA): finds a map between states that have
similar local geometries from unpaired, unaligned demonstrations.
Invariant Features (Gupta et al., 2017) (IF): finds invertible projections onto a feature space given state
pairings. Dynamic Time Warping (Muller, 2007) is used to obtain the pairings.
Imitation from Observation (Liu et al., 2018) (IfO): learns a statemap conditioned on a cross domain
observation given state pairings. Dynamic Time Warping (Muller, 2007) is used to obtain the pairings.
Third Person Imitation Learning (Stadie et al., 2017) (TPIL): simultaneously learns a domain agnostic
feature space and matches distributions in the feature space via GAIL (Ho & Ermon, 2016).
We experiment with environments which are extensions of OpenAI Gym (Brockman et al., 2016). pen,
cart, reacher2, reacher3, reach2-tp, snake3, and snake4 denotes the pendulum, cartpole, 2-link reacher,
3-link reacher, third person 2-link reacher, 3-link snake, and 4-link snake environments, respectively. (self
domain)分(expert domain) specify an MDP pair in the alignment task set. Model architectures and
environment details are further described in the Supp. Materials, section B, C, D.
6.1	MDP alignment evaluation
Figure 4 visualizes the learned state map fθʃ for several MDP pairs. The pen 什 pen alignment task
(Figure 4, Top Left) and reach什reach-tp task task exemplify scenarios where two MDPs are permutations
of each other. Similarly, the pen 什 cart alignment task (Figure 4, Top Right) has a reduction that maps the
pendulum’s angle and angular velocity to those of the pole, as the cart’s position and velocity are redundant
state dimensions once an optimal policy has been learned. Table 1 presents quantitative evaluations of
these simple alignment maps. For Pen什Pen and reach2»reach2-tp we record the average L2 loss between
the learned statemap,s outputs and the ground truth permutation map,s outputs. As for Pen什Cart, we do
6
Under review as a conference paper at ICLR 2020
Figure 4: Visualization of the learned state maps for pen-pen (ToP Left), pen-cart (ToP Right),
Snake4-Snake3 (Bottom Left), reach2-reach3 (Bottom Right). GAMA is able to recover MDP reduc-
tions (Top Left/Right) and finds interpretable correspondences between domains that are not perfectly
alignable, yet intuitively share structure (Bottom Left/Right). Baselines fail in most cases
Table 1: Quantitative evaluation of learned state maps. GAMA reliably finds MDP permutations while
baselines incur 10× larger deviation loss from the ground truth permutation map. Error bars/regions show
the standard deviation over 5 runs.
	GAMA (ours)	CCA	UMA	IF	IfO	Random
pen ÷÷ pen	0.057 ± 0.01T=	0.72 ± 0.25=	>100	2.50 ± 1.08	2.24 ± 0.82=	>100
pen ÷÷ cart	0.178 ± 0.051	3.92 ± 3.77	>100	1.62 ± 0.52	3.31 ± 1.2	->100-
reach2—reach2-tp	0,092 ± 0,043-	10.14 ± 5.31^	>100	12.41 ± 3.1∑^	5.12 ± 2.41一	>100
the same on the dimensions that correspond to the angle and angular velocity of the pole. We see from
both Figure 4 and Table 1 that GAMA is able to learn simple reductions while baselines fail to do so. The
key reason behind this performance gap is that most baselines (Gupta et al., 2017; Liu et al., 2018) obtain
state maps from time-aligned demonstration data. However, the considered alignment task set contains
unaligned demonstrations with diverse starting states, up to 2x differences in demonstration lengths, and
varying task execution rates. We see that GAMA also outperforms baselines that learn from unaligned
demonstrations (Hotelling, 1936; Ammar et al., 2015) by learning maps that preserve MDP structure
with more flexible neural network function approximators. For pen-cart, UMA learns a statemap that
outputs out-of-bounds coordinates mainly because the pendulum demonstrations are concentrated around
the pole upright state. The optimal UMA embedding matrix in this case is a zero matrix. Then the UMA
state map matrix norm is proportional to the inverse embedding matrix norm which is very large. For
snake4 - snake3 and reach2 - reach3, the MDPs may not be perfectly alignable, yet intuitively share
structure. From Figure 4 (Bottom Left) we see that GAMA identically matches two adjacent joint angles
of snake4 to the two joint angles of snake3 and the periodicity of the snake’s wiggle is preserved. On
reacher2-reacher3, we find that the central pivot angles are matched and further find correspondences
between states that have similar extents of contraction.
7
Under review as a conference paper at ICLR 2020
Figure 5: CDIL performance. Alignment complexity (Left), Adaptation complexity (Middle), and
transferability (Right) for W2C/R2W on the top/bottom rows, respectively. GAMA outperforms baselines
in all metrics. Notably, adaptation complexity of GAMA is close to that of the self-demo baseline. Error
bars/regions show the standard deviation over 5 runs.
6.2	CDIL performance
Wall2Corner (W2C): The self domain is reacher2 and the expert domain is reacher3. We use the robot’s
internal state, action representation. The alignment tasks are reaching for 12 goals near the room wall
centers and the target tasks are reaching for 12 new goals at the room corners, maximally away from the
wall goals. The significant difference between training and test goals makes generalization challenging.
Reach2Write (R2W): The self domain is reacher2 and the expert domain is reacher2-tp that has a "third
person" state space with a 180° camera angle offset. We use the robot's internal state, action representation.
The alignment tasks are reaching for goals and the transfer task is writing letters as fast as possible. The
transfer task differs from the alignment tasks in two key aspects: the end effector must draw a straight line
from a letter’s vertex to vertex and not slow down at the vertices in order to trace the letters fast.
Alignment complexity is shown in Figure 5 (Left). GAMA is able to learn alignments that enable
zero-shot imitation on the target task, showing clear gains over a simple pretraining procedure on the
self domain MDPs in the alignment task set. Other baselines require an additional RL step and cannot
achieve zero-shot imitation. Figure 5 (Middle) shows the adaptation complexity. Notably, GAMA-DA
(blue, dashed) produces adapted demonstrations of similar usefulness as self demonstrations (olive green).
Other baselines fail to learn useful alignments from unpaired, unaligned demonstrations and as a result
fails at CDIL. Finally, Figure 5 (Right) shows that the alignment step is useful as weight initialization to
accelerate learning of the target task. GAMA (blue) attains optimal performance around 7× faster than all
baselines in the W2C experiment, while immediately attaining optimal performance on the R2W task.
Baselines fail to learn the writing task as an inaccurate proxy reward function harms performance.
6.3	CDIL with visual inputs
The non-visual environment experiments in the previous section demonstrate the limitations of the
time-alignment assumptions made in prior work without confounding variables such as the difficulty
optimization in high-dimensional space. In this section, we also demonstrate that GAMA scales to higher
dimensional, visual environments with 64 × 64 × 3 image inputs on the W2C and R2W experiments.
Specifically, we train a deep spatial autoencoder on the alignment task set to learn an encoder with the
architecture from Levine et al. (2016), then apply GAMA on the (learned) latent space. Comparing the
dark blue (image input) and light blue curves (internal state input) in Figure 5, we see that the adaptation
complexity and alignment complexity of GAMA-DA-img, GAMA-PA-img are both similar to that of
GAMA-DA, GAMA-PA and better than baselines trained with the robot’s internal representation.
7	Discussion and future work
We’ve formalized Cross Domain Imitation Learning which encompasses prior work in transfer learning
across embodiment (Gupta et al., 2017) and viewpoint differences (Stadie et al., 2017; Liu et al., 2018)
along with a practical algorithm that can be applied to both scenarios. We now point out directions future
8
Under review as a conference paper at ICLR 2020
work. Our MDP alignability theory is a first step towards formalizing possible shared structures that
enable cross domain imitation. While we’ve shown that GAMA empirically works well even when MDPs
are not perfectly alignable, upcoming works may explore relaxing the conditions for MDP alignability to
develop a theory that covers an even wider range of real world MDPs. Future works may also try applying
GAMA in the imitation from observations scenario, i.e actions are not available, by aligning observations
with GAMA and applying methods from Sermanet et al. (2018); Liu et al. (2018). Finally, we hope to
see future works develop principled ways design a minimal alignment task set, which is analogous to
designing a minimal training set for supervised learning.
9
Under review as a conference paper at ICLR 2020
References
Haitham Bou Ammar, Eric Eaton, Paul Ruvolo, and Matthew E. Taylor. An automated measure of mdp
similarity for transfer in reinforcement learning. 2014.
Haitham Bou Ammar, Eric Eaton, Paul Ruvolo, and Matthew E. Taylor. Unsupervised cross-domain
transfer in policy gradient reinforcement learning via manifold alignment. 2015.
Patrick Billingsley. Convergence of probability measures. 1968.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Zaremba Wojciech. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
Norman Ferns, Prakash Panangaden, and Doina Precup. Metrics for finite markov decision processes. In
UAI, 2004.
Chelsea Finn, Xin Yu Tan, Yan Duan, Trevor Darrell, Sergey Levine, and Pieter Abbeel. Deep spatial
autoencoders for visuomotor learning. International Conference on Robotics and Automation, 2015.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information
processing systems,pp. 2672-2680, 2014.
Abhishek Gupta, Coline Devin, Yu Xuan Liu, Pieter Abbeel, and Sergey Levine. Learning invariant
feature spaces to transfer skills with reinforcement learning. International Conference on Learning
Representations, 2017.
Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in Neural
Information Processing Systems, pp. 4565-4573, 2016.
Harold Hotelling. Relations between two sets of variates. Biometrika, 28, 1936.
Susan S. Jones. The development of imitation in infancy. Philos Trans R Soc Lond B Biol Sci., 364:
2325-2335, 2009.
Girish Joshi and Girish Chowdhary. Cross-domain transfer in reinforcement learning using target
apprentice. 2018.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, December 2014.
Yasuo Kuniyoshi and Hirochika Inoue. Qualitative recognition of ongoing human action sequences.
International Joint Conference on Artificial Intelligence, 1993.
Yasuo Kuniyoshi, Masayuki Inaba, and Hirochika Inoue. Learning by watching: Extracting reusable task
knowledge from visual observation of human performance. IEEE Trans. Robot. Autom., 10:799-822,
1994.
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuomotor
policies. The Journal of Machine Learning Research, 17(1):1334-1373, 2016.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David
Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint
arXiv:1509. 02971, 2015.
YuXuan Liu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine. Imitation from observation: Learning to
imitate behaviors from raw video via context translation. arXiv preprint arXiv:1707.03374, 2018.
Peter J. Marshall and Andrew N. Meltzoff. Body maps in the infant brain. Trends Cogn Sci., 19:499-505,
2015.
Meinard Muller. Dynamic time warping. Information retrieval for music and motion, pp. 69-84, 2007.
Ronald Ortner. Combinations and mixtures of optimal policies in unichain markov decision processes are
optimal. arXiv preprint arXiv:0508319, 2005.
10
Under review as a conference paper at ICLR 2020
Dean A Pomerleau. Efficient training of artificial neural networks for autonomous navigation. Neural
computation, 3(1):88-97,199LISSN 0899-7667.
Balaraman Ravindran and Andrew G. Barto. Model minimization in hierarchical reinforcement learning.
In SARA, 2002.
Giacomo Rizzolatti and Laila Craighero. The mirror neuron system. Annual Review of Neuroscience, 27:
169-192, 2004.
Pierre Sermanet, Corey Lynch, Yevgen Chebotar, Jasmine Hsu, Eric Jang, Stefan Schaal, and
Sergey Levine. Time-contrastive networks: Self-supervised learning from video. arXiv preprint
arXiv:1704.06888, 2018.
Bradly Stadie, Pieter Abbeel, and Ilya Sutskever. Third person imitation learning. In ICLR, 2017.
Umar Syed, Michael Bowling, and Robert E Schapire. Apprenticeship learning using linear programming.
In Proceedings of the 25th international conference on Machine learning, pp. 1032-1039. ACM, July
2008. ISBN 9781605582054. doi: 10.1145/1390156.1390286.
Tingwu Wang, Renjie Liao, Jimmy Ba, and Sanja Fidler. NerveNet: Learning structured policy with
graph neural networks. International Conference on Learning Representations, 2018, 2018.
11
Under review as a conference paper at ICLR 2020
Cross Domain Imitation Learning - Supplementary Materials
A High-level Comparison to Baselines
Table 2: Comparison of baselines by attributes demonstrated in the paper. The "No Act" column denotes
whether or not the demonstrations need to contain actions.
	Unpair. Unalign. Data	Zeroshot Imit.	Embod. Mismatch	Viewpoint Mismatch	Single-domain Demo.	No Act.
~^TPIL (Stadie et al., 2017)-	✓	X	X	✓	J	X
IF(GuPtaetaL,2017)	X	X -	✓	X	X	X
IfO(LiuetaL,2018)	X	X -	X	✓	X	✓
TCN (Sermanet et al., 2018)	X	X	X	✓	J	✓
GAMA (ours)	/	J 一	J	/ 一	X	X
We note that methods such as IF has potential to be applied to the viewpoint mismatch problem and IfO,
TCN have the potential to be applied to the embodiment mismatch problem, albeit they were not shown in
the paper. TCN has shown interesting mappings between humans and robots can be learned. However
they haven’t shown that robots can use these mappings to learn from human demonstrations. Below we
summarize the key differences between GAMA and the main baselines.
1.	We propose an unsupervised MDP alignment algorithm (GAMA) capable of learning state correspon-
dences from unpaired, unaligned demonstrations while Gupta et al. (2017); Liu et al. (2018); Sermanet
et al. (2018) obtain these correspondences from paired, time-aligned trajectories. Our demonstrations
have varying length (up to 2x difference) and diverse starting positions. Since good observation correspon-
dences are prerequisites to the success of Gupta et al. (2017); Liu et al. (2018); Sermanet et al. (2018), our
work provide the missing ingredient. Future work could try learning alignments with GAMA, then apply
methods from Gupta et al. (2017); Liu et al. (2018); Sermanet et al. (2018) to perform CDIL when action
information is unavailable from demonstrations.
2.	We remove the need for an expensive RL procedure on a new target task, by leveraging action
information for zero-shot imitation. By learning a composite self policy with both state and action maps,
we obtain a near-optimal self policy on new tasks without any environment interactions while prior
approaches (Gupta et al., 2017; Liu et al., 2018; Sermanet et al., 2018) require an additional RL step that
involves self domain environment interactions.
3.	We use a single algorithm to address both the viewpoint and embodiment mismatch which have
previously been dealt with different solutions.
B GAMAmodel architecture
We now describe the model architecture. The state, action map fθ ,gθ , inverse state map f -1-1 , transition
function Px , and discriminators {Dθi }N=1 are neural networks with hidden layers of size (200, 200).
The fitted policies {πy,T }N=1 for GAMA-PA and πx,T for GAMA-DA all have hidden layers of size
(300, 200). All models are trained with Adam optimizers (Kingma & Ba, 2014) using decay rates
β1 = 0.9, β2 = 0.999. For the spatial autoencoders used in GAMA-PA-img and GAMA-DA-img, we use
the same architecture as in Finn et al. (2015) We use a learning rate of 1e-4 for the alignment maps and
1e-5 for all other components. These parameters are fixed across all experiments.
C Baseline Implementation Details
In this section we describe our implementation details of the baselines.
Obtaining State Correspondences We use 5000 sampled trajectories in both expert and self domains to
learn the state map for IF and CCA. For UMA, we use 20 sampled trajectories to learn that in pendulum
and cartpole environment and 50 trajectories in reacher, reacher-tp environment (much beyond these
numbers UMA is computationally intractable). For IF and IfO, we use Dynamic Time Warping (DTW)
(Muller, 2007) to obtain state correspondences. For IF, DTW uses the (learned) feature space as a metric
space to estimate the domain correspondences. For IfO, DTW is applied on the state space. We follow the
implementation procedure in Gupta et al. (2017).
To visualize and quantitatively evaluate the statemaps learned in prior work, we compose the encoder and
decoder for IF and use the Moore-Penrose pseudo inverse of the embedding matrix for UMA and CCA.
12
Under review as a conference paper at ICLR 2020
Transfer Learning In the transfer learning phase for CCA, UMA, IF, and IfO they define a proxy reward
function on the target task by using the state correspondence map.
rproxy(Sxt)) = |T| PT∈T Ilf(sytT) - g(Sxt))k2
, where s(xt) is a self domain state at time t, T is the collection of expert demonstrations, and s(yt,)τ is the
expert domain state at time t in trajectory τ . IfO additionally defines a penalty reward for deviating from
states encountered during training. We refer readers to their paper (Liu et al., 2018) for further details.
The transferability results of Figure 5 (Right) show the learning curve for training on the ground truth
reward for the target task where the policy is pretrained with a training procedure on the proxy reward.
All RL steps are performed using the Deep Deterministic Policy Gradient (DDPG) (Lillicrap et al., 2015)
algorithm.
Architecture For UMA and CCA, the embedding dimension is the minimum state dimension between the
expert and self domains. For UMA, we use one state sample every 5 timesteps to reduce the computational
time, and we match the pairwise distance matrix of 3-nearest neighbors.
For IF, we use 2 hidden layer with 64 hidden units each and leaky ReLU non-linearities to parameterize
embedding function and decoders, the dimension of common feature space is set to be 64. The optimizer
are same with respect to our models and the learning rate is 1e-3.
For IfO, we use the same architecture as the statemap in GAMA for their observation conditioned
statemap.
For TPIL, we use 3 hidden layer with 128 hidden units each and ReLU non-linearities to parameterize the
feature extractor, classifier and domain classifier. We use Adam Optimizer with default decay rates and
learning rate 1e-3 to train the discriminator and use same optimizer and learning rate with respect to our
model to train the policy.
D Environments
We use the ’Pendulum-v0’, ’Cartpole-v0’ environments for the pendulum and cartpole tasks which have
state space (w, W) and (w, W, x, X), respectively, where W is the angle of the PendUlUm/pole and X is
the position of the cart. The action spaces are (Fw) and (Fx) where Fw is the torque applied to the
pendUlUm’s pivot and Fx is the x-direction force applied to the cart. For snake3, snake4 we Use an
extension Wang et al. (2018) of the ’Swimmer-v0’ environment from Gym Brockman et al. (2016). A
K link snake has a state representation (wι,…，WK, Wι,…，WK) where Wk is the angle of the kth snake
joint. The action vector has the form (Fw1 ,...Fw ) where Fw is the torqUe applied to joint k. All
reacher environments were extended from the ’Reacher-v0’ gym environment. A k link reacher has a
state vector of the form (wi,…,WK, Wι,…,WK, Xg ,y§) where Wk is the angle of the kth reacher joint
and (Xg,yg) is the position of the goal. Note the key difference with the original Reacher-v0 environment
is that the difference vector between the end effector and the goal coordinate was removed from the
state to make the task more challenging. The action vector has the form (Fw1 ,...Fw ) where Fw is
the torqUe applied to joint k. The third person reacher environment (reach-tp) has an angle off set of
∏ such that (wi + ∏,..., WK + ∏, Wi,..., WK, Xg, y§) corresponds to the same internal robot state as
(wi,…,WK, Wi,…,WK, Xg ,yg) in the original reacher domain. Note that the goal coordinates (Xg ,yg)
have not changed bUt the joint angles have changed. ThUs a proper alignment between the original domain
and the third person domain shoUld add an offset of π to the original state. For the writing task, the
vertices of the letters to write are specified seqUentially with the goal coordinates. Once the first vertex is
reached, the goal coordinates are Updated to be the next vertex coordinates. The reward fUnction is defind
as follows:
100 if state S corresponds to reaching a vertex
Rwrite(S) =	-1 else
ThUs the agent mUst perform a seqUential reaching task and accomplish it as fast as possible. The key
difference with a normal reaching task is that the reacher mUst not slow down at each vertex and plan it’s
path accordingly in order to minimize drastic direction changes. FUrther more the reward is significantly
more sparse than the original reacher reward which gets reward inversely proportional to the distance
between the end effector and the goal.
13
Under review as a conference paper at ICLR 2020
E Videos of Alignment Maps
We will provide real-time visualizations of the learned alignments in the final version of the submission.
F Proofs
Definition 1. An optimality function OM	: Sx ×Ax →{0, 1} for an MDP Mx satisfies:
Omx (sχ, aχ) = 1 if ∃∏χ ∈ ∏-Mx such that (sχ, ax) ∈ supp(q∏*) and Omx (sχ, ax) = 0 otherwise.
Definition 6. An optimal policy πx is covering if Omx (sχ, aχ) = 1 ⇒ aχ ∈ Supp (∏χ(∙∣Sχ)).
We first restate the definition of MDP reductions:
Definition 2. An MDP reduction from Mx = (Sx, Ax,Px,ηx,Rx) to My =(Sy, Ay,Py,ηy,Ry) is a
tuple r =(φ, ψ) where φ : Sx →Sy,ψ : Ax →Ay are maps that preserve:
1.	(optimal policy) ∀sx ∈Sx ,ax ∈Ax,sy ∈Sy ,ay ∈Ay,
OMy (φ(sx), ψ(ax)) = 1	⇒	OMx(sx,ax)=1	(1)
OMy(Sy ,ay) = 1	⇒	φ-1 (Sy) = 0, ψ-1 (ay) =。	⑵
2.	(dynamics) ∀sy,s0y ∈Sy, ay ∈Ay where OM (sy,ay)=1,
Py (Sy ,ay )=φ(Px (Sx ,ax )) ∀Sx ∈ φ (Sy ),ax ∈ ψ	(ay )	(3)
where we define φ-1(sy) = {sχ∣φ(sχ) = Sy}, ψ-1(ay) = {aχ∣ψ(aχ) = ay}. Furthermore, r is an
MDP permutation if and only if φ, ψ are bijective maps.
Definition 7. MDP Mx is unichain, if all policies induce irreducible Markov Chains and all stochastic
optimal policies induce ergodic, i.e irreducible and aperiodic, Markov Chains.
Assumption 1. All considered MDPs are unichain with discrete state, action spaces and deterministic
dynamics i.e. P : S×A→S. Furthermore, there exists dummy state, actions Sd, ad where OM(S, ad)=
0 ∀S ∈Sand OM(Sd,a)=0∀a ∈A
As stated in Assumption 1, we specifically consider MDPs that are unichain with deterministic dynamics.
We note this is a reasonable assumption since physics is largely deterministic and many control behaviors,
such as walking, are described by unichains. We now formalize a notion of MDP alignability starting
from defining a class of structure preserving maps between MDPs. We first prove some lemmas that will
assist in proving the main Theorem. Recall that f : Sx → Sy ,g : Ay → Ax and ∏χ = g ◦ ∏y ◦ f
Lemma 1. Let Mx , My be MDPs satisfying Assumption 1 (see Supp. Materials), Mx ≥φ,ψ My, and
∏y be optimal in My. ∀g : Ay → Ax s.t ψ ◦ g(ay) = ay Yay ∈ {ay ∣∃Sy ∈ Sy s.t OMy(Sy, ay) = 1},
it holds that πx = g ◦ ∏y ◦ φ is optimal in Mx.
Proof. Without loss of generality, consider an arbitrarily chosen sample ax = g(ay ),ay 〜∏y (∙∣φ(sx))
for any Sx ∈Sx. We first see that:
OMy φ(Sx), ψ(ax) = OMy φ(Sx), ψ(g(ay)) = OMy φ(Sx), ay =1	(9)
where the first step substitutes ax = g(ay), the second step applies ψ◦ g(ay) = ay since O(φ(sx),ay) = 1
due to the optimality of πy, and the last step follows from Corollary 1. Since (φ, ψ) is a reduction, we
have that OMy (φ(sx), ψ(ax)) = 1 ⇒ OMx(Sx,ax) = 1 by Equation (1). Therefore, OMx(Sx,ax)=
1 ∀Sx ∈ Sx, ∀ax ∈ supp(∏x (∙∣Sx)). Then by Lemma 2, πx is optimal.	口
Lemma 2. Let MDP Mx satisfy Assumption 1 and ∏x(ax∣Sx) be a (stochastic) mixture policy that
chooses ax randomly from {ax|O(Sx, ax) = 1}. Then, πx is optimal. (Ortner, 2005)
Corollary 1. Let MDP Mx satisfy Assumption 1 and πx be optimal. Then OM (Sx,ax)=1∀Sx ∈
Sx, ax ∈ supp(∏x(∙∣Sx))
Lemma 3. Let MDP Mx satisfy Assumption 1 and πx be a stochastic optimal policy. Then the triplet
stationary distribution ρxπ (Sx,ax,S0x)=limt→∞ Pr(S(xt) = Sx,a(xt) = ax,S(xt+1) = S0x; πx,Px,ηx)
exists and is unique.
14
Under review as a conference paper at ICLR 2020
Proof.
ρxπx (sx ,ax ,sx)= lim Pr(s(x ) = sx,a(x ) = ax,s(x + ) = sx ; πx ,Px ,ηx)
x	t→∞
=lim Pr(SX" = sx; πX,PX, ηX) πx (ax Isx)R(Sx = Px(sx, aX))
t→∞
=πx (ax Isx)E(Sx = Px(Sx, aX))Iim Pr(Sx = = sx; πx, Px, ηy )
t→∞
where 1 is the indicator function. The limit in the last line is the stationary distribution over states, which
exists and is unique since a stochastic optimal policy induces an ergodic Markov Chain over states. 口
Lemma 4. If a real sequence {ai }∞=1 converges to some a ∈ R, then
lim p Pi=I ai = limi→∞ ai = a
T→∞ T	i=1	→∞
Proof. Denote AT = PT=1 ai , and BT = T . We have
AT+1 - AT
lim -------—— = lim aτ +ι = a
T→∞ BT+1 - BT	T→∞
(10)
According to the Stolz-Cesaro theorem,
AT+1 - AT	AT
lim ŋ------=- = lim h
T→∞ BT+1 - BT	T→∞ BT
if the limit on the left hand side exists. Therefore
t→∞ BT=t→∞ T PT=I ai=a
(11)
which completes the proof.
□
Recall that our target distribution σ∏y and proxy distribution σx→y were defined as:
σ
σπyy(sy,	ay ,s0y )	=lim ɪ pT-01 τ→∞ T 乙t=0	Pr(s(yt)	sy,ay =	ay,s(yt+1)	s0y ; πy ,Py ,ηy )	(12)
-x→y (Sy,	ay,sy)	1	T-1 一τ →∞ τ Et=0	Pr(Sy =	=sy, ay)=	=ay ,或+1)=	s0y;P)	(13)
We are now ready to prove that our proxy and target limiting distributions exist.
Lemma 5. Let MDP My satisfy Assumption 1 and πy be a stocahstic optimal policy. Then,
σπyy(sy,ay,s0y)=ρyπy(sy,ay,s0y).
Proof. Recall that the stationary distribution ρyπ (sy,ay,s0y) is the following limiting distribution:
ρyπy (sy,ay,s0y)= l→im∞ Pr(s(yt) = sy,a(yt) = ay,s(yt+ ) = s0y; πy,Py,ηy)	(14)
ρyπ (sy,ay,s0y) exist for My as shown in Lemma 3. Then,
σ∏y (sy, ay, sy)	=	3limo 1 PT-I	Pr(Syt)	=	sy, ayt = ay, syt+1) = s'y ； πy, Py, Dy)	(15)
=	l→im∞ Pr(s(yt) =	sy,a(yt)	=	ay,s(yt+1) = s0y; πy,Py,ηy)	(16)
= ρyπy(sy,ay,s0y)	(17)
as desired. The second line follows from Lemma 4 and the last line follows from Lemma 3.	口
Lemma 6. Let MDP My satisfy Assumption 1 and πy be a stochastic optimal policy. Then,
supp(σπyy ) ⊆{(sy,ay,sy)IOMy (sy,ay)=1,sy,sy ∈Sy,ay ∈Ay}
15
Under review as a conference paper at ICLR 2020
Proof. Assume for contradiction that there exists (sy,ay,s0y) ∈ supp(σπy ) but (sy,ay,s0y) ∈/
{(sy ,ay , sy)|OMy (sy ,ay)=1,sy ,sy ∈Sy ,ay ∈Ay }. Then OMy (sy ,ay)=0. Since
σyy (Sy, ay, ay) = t→m∞ Pr(Syt) = Sy, at = ay, syt+1) = Sy； πy, Py, Dy)
=l→im∞Pr(s(yt) = sy)Pr(a(yt) = ay|s(yt) = sy)Pr(s(yt+1) = s0y|s(yt) = sy,a(yt) = ay)
= l→im∞ Pr(S(y ) = Sy)πy (ay |Sy)Pr(S(y + ) = Sy |S(y ) = Sy,a(y ) = ay)
=3yWKT 0Pr(Syt+1) = sy ISyt) = Sy,喈=ay) t→→n Pr(Syt) = Sy)
=0
First line follows from Lemma 5 and terms are taken out of the limit in the second to last line since the
stationary distribution over states exist as My is unichain and πy is stochastic optimal. πy (ay |Sy)=0
since OMy (Sy, ay) = 0 ⇒ ∏y (ay ∣Sy) = O from Corollary 1. Then, We have σ∏y (Sy, ay, ay) = 0 which
contradicts (Sy, ay, Sy) ∈ supp(σ∏J concluding the proof.	口
16
Under review as a conference paper at ICLR 2020
Lemma 7. LetMDP Mx satisfy Assumption 1 and ∏χ = g ◦ ∏y ◦ f be an stochastic optimal policy
in Mx where f : Sx →Sy is the state map, g : Ay →Ax is injective action map, and πy is a
stochastic optimal policy in My. Further let F : Sx × g(Ay) ×Sx →Sy ×Ay ×Sy be the map
F(a,b, C) = (Aa),gTe),f(C)).Then, σ∏→(Sy,ay,sy) = F(Pnjsx,ax, Sx)).
Proof. We first define the triplet random variables X(t) =(s(xt),a(xt),s(xt+1) ) for t =0, 1, 2,... where
sxt),axt), sxt+1) for t = 0,1, 2,... were defined in Definition 5. F is a function on supp(ρ^^) ∈ Sx X
g(Ay) × Sx and F(X(t)) = (S，),碍),s^+D). Furthermore, since F is a function defined on a discrete
domain and codomain, there always exists a trivial continuous extension of F. We may thus apply the
continuous mapping theorem (Billingsley, 1968):
X(t) -→d X ⇒F(X(t)) -→d F(X)
Since Mx is unichain and πx is stochastic optimal, the distribution of X(t) converges (in distribution)
to PXJsx, ax, sx) as t → ∞ by Lemma 3. Applying the continuous mapping theorem, it follows that
the distribution of F(X ⑴)= &), a^t,碍+。) converges (in distribution) to the pushforward measure
F (PnC(Sx,ax,sx))as t → ∞
Directly applying this result, we obtain:
σ∏→y (Sy, ay, Sy) =	lim	1 PTQI	Pr(Wyt)	= Sy,	ayt	= ay,铲1)= Sy ；	P)	(18)
x	T →∞ T
=t→∞ Pr(^yt) = Sy, ayt) = ay, ^yt+1) = Sy ； p )	(i9)
=F(PXx(Sx, ax, Sx))	QO)
as desired. Line (18) → (19) follows from Lemma 4 and (19) → (20) follows from the continuous
mapping theorem.	□
Lemma 8. Let X, Y be countable sets, φ : X → Y be afunction, and 1 be the indicator function. We
denote φ-1(y) = {χ∣φ(χ) = y}. Then ∀χ ∈ X, y ∈ Y
l(y = φ(X)) = Pz∈φ-I(y) R(X = Z)
Proof. Since both the left and right hand-side of the desired equality only take on values in {0, 1}, it
suffices to show the following statements hold for arbitrarily chosen X ∈ X, y ∈ Y :
Pz∈φ-1(y) Q(X = Z) = I ⇒ l(y = φ(X)) = 1
H(y = φ(X)) = 1 ⇒ Pz∈φ-i(y) l(x = Z) = I
For the first direction, we see that if PZ∈力-i@)IL(X = z) = 1, then X ∈ φ-1(y), and thus Φ(x) = y.
For the second direction if l(y = Φ(x)) = 1, then X ∈ φ-1(y). Thus there exists a unique Z such that
z = X and Z ∈ φ-1(y). Then, PZ∈”1®)E(X = z) = 1 as desired, which concludes the proof. □
17
Under review as a conference paper at ICLR 2020
Finally, we prove the main theorem. Our objectives are:
1. ∏χ is optimal
2∏
3.	g is injective
Theorem 1. Let Mx, My be MDPs satisfying Assumption 1 (see Supp Materials). If Mx ≥My, then
∃f : Sx →Sy,g : Ay →Ax, and an optimal covering policy πy (Supp Materials, Def 6) that satisfy
objectives 1, 2. Conversely, if ∃f : Sx →Sy,g: Ay →Ax and an optimal covering policy πy satisfying
objectives 1,2, 3, then Mx ≥ My and ∃(φ, ψ) ∈ Γ(Mχ, My) s.t f = φ and ψ ◦ g(a,y) = ay Xay ∈ Ay.
Proof. We first show the (⇒) direction. Using any (φ, ψ) ∈ Γ(Mx, My) we construct f and g in the
following manner: f(sx)=φ(sx) ∀sx ∈Sx. g(ay) maps to an arbitrary chosen element from the set
ψ-1(ay) = {ax∣Ψ(ax) = ay} if ψ-1(ay) = 0 and an arbitrarily chosen action ax ∈ Ax otherwise. We
see that Nay ∈ Ay for which ∃sy ∈ Sy such that OMy (Sy, ay) = 1, it holds that ψ-1(ay) = 0 by Eq
2. Therefore, ψ ◦ g(ay) = ay ∀ay ∈Ay for which ∃sy such that OM (sy,ay)=1since ψ maps all
elements in ψ-1(ay) to ay. For πy we choose any covering optimal policy for My. It suffices to show
that this choice of f, g, πy satisfies objectives 1, 2.
•	Objective 1. ∏x is optimal: follows from Lemma 1.
•	Objective 2. σx→y = σ∏y: Since f = φ is a reduction, it follows that Nsy ∈ Sy, ay ∈ Ay such that
OMy(sy,ay)=1, any s0y ∈Sy, and ∀t =0, 1, 2,...:
Pr(Syt+1)= Sy∣^yt) = Sy 闺=ay)
Σ
s0x∈S
P
s0x∈S
P
Pr(Wyt+1) = SyISxt+1) = sX,∙⅛t) = sy, ayt) = ay) Pr(Sxt+1) = sX∣Syt) = sy,ayt) = ay)
Pr(^yt+1) = SyISxt+1)
sx
ax
∈Sx
∈A
Pr(^xt+1) = SxISxt) = Sx,aXt)=
Sy,ay^ = ay) Pr(Sxt) = Sx,aXt)=
Sy, ay，= ay)
Σ
s0x∈S
P
sx
ax
∈Sx
∈A
U(Sy = Φ(Sx))
Pr(^t+1) = SxISg) = Sx,aXt) = ax)Pr(Sg) = SxIag)=
Sy,ayt) = ay) Pr(aXt) = ax∣^%) = Sy,ayt) = ay)
x
x
x
x
x
s0x∈S
x
U(Sy = φ(Sx))	P	IL(Sx	=	PX(Sx,ax))Pr(Sxt)	=	SxISyt)	=	Sy)	Pr(axt)	= axIayt)	=	ay)
Sx∈Sx
ax ∈Ax I *
s0x∈S
I(Sy=φ(Sx))SxPj(Sx=Px(Sx,ax)) PPr(PylUr(Pxt；二)Max=g(ay))
ax ∈Ax	s00∈Sx
x
s0x∈S
x
U(Sy = Φ(Sx)) E l(Sx = Px(Sx,g(ay)))
Sx ∈Sx
Σ
s0x∈φ-1(s0y)
s
x
E	U(Sx = Px(Sx,g(ay)))
∈φ-1(sy)
, 工(Sy = Φ(Sx)) Pr(Sxt) = Sx)
P I(Sy = φ(Sx))Pr(Sxt) = Sx
s0x0∈Sx
Pr(S(xt) = Sx)
~Σ~Pr(Sxt) = SX)
s0x0∈φ-1(sy)
Σ
sx∈φ-1(sy)
Pr(Sxt) = Sx)
ΣPr(Sxt)=小
s0x0∈φ-1(sy)
E	U(Sx = Px(Sx,g(ay)))
s0x∈φ-1(s0y)
Lemma8
Sχ.φP(Sy)	PPr(SxPr=SStxI SX) MSy = "(Px(Sx，g(ay⑼
s0x0∈φ-1(sy)
Eq3
S.'.4,)	PPr(SxPr=SStx )= SX) MSy =Py(Sy，ay))
s0x0∈φ-1(sy)
=U(Sy = Py (Sy,ay))
= Pr(S(yt+1) = S0y∣S(yt) = Sy,a
(yt) = ay)
Furthermore, from Definition 5, we have:
(21)
Pr(ayt) = ay
Sy)=πy(ay ISy) = Pr(a(yt) = ay IS(yt) = Sy)
(22)
18
Under review as a conference paper at ICLR 2020
Then, ∀sy,s0y ∈Sy and ∀t =0, 1, 2,...
Pr(§yt+i)= SyIWyt)=Sy)=	p	Pr(§yt+i)=	SyIWyt)	=	Sy,ay=ay)PMayt)=ayIWyt)=Sy)
ay∈Ay
=	P	Pr(Sy = Sy ISy = Sy,ay = ay)πy (ay ISy)
ay ∈supp(∏y (Tsy))
= Pr(S(yt+1) = SyIS(yt) = Sy)	(23)
we are justified in the substitution for the dynamics in the second line since OM (Sy,ay)=1∀Sy ∈
Sy, ay ∈ supp(∏y(∙∣Sy)) by Corollary 1. Since My is unichain and ∏y is a stochastic optimal policy,
the stationary distribution limt→∞ Pr(S(yt) = Sy) is invariant to the initial state distribution ηy and only
depends on the state transition dynamics Pr(S(yt+1) = S0yIS(yt) = Sy). Equivalently any stochastic process
with the same state transition dynamics will converge to the same stationary distribution regardless of the
initial state distribution. Thus,
Jim Pr(^yt) = Sy) = Jim Pr(Syt) = Sy) YSy ∈ Sy	(24)
Finally putting these results together, the following equalities hold for (Sy,ay,S0y)	∈
{(Sy, ay, S0y)IOMy (Sy,ay)=1,Sy,S0y ∈Sy,ay ∈Ay}
σx→y (Sy,ay,Sy) Lemma 5 t→∞∞ Pr(∙¾t) = Sy圈t = ay,∙¾t+1) = Sy； P)
=t→∞∞ Pr(∙¾t) = Sy) Pr(&yt) = ay 归 yt) = Sy )pr(wyt+1) = Sy 归 yt) = Sy, ayt)=ay)
Eq (2=,(22) t→m∞ Pr(Wyt) = Sy )Pr(ayt) = ay ISyt)= Sy) PMsf+1) = Sy ISyt)= Sy ,ayt) = ay)
=πy(ay〔Sy)l(Sy = Py(Sy, ay)) t→∞∞ PH%) = Sy)
Eq=24) πy(ayM)l(Sy = Py(Sy, ay)) t→∞∞ Pr(Syt)= Sy)
=	l→im∞ Pr(S(yt) = Sy)Pr(a(yt) = ayIS(yt) = Sy)Pr(S(yt+1) = S0yIS(yt) = Sy,a(yt) = ay)
Lemma 5	0
=	σπyy(Sy,ay,S0y)
The constant terms are moved in and out of the limit in the fourth and sixth line since the stationary
distribution over states exist as My is unichain and πy is optimal for My . This allows us to conclude
that σχ→y = σ∏y since σ∏ is supported on {(Sy, ay, Sy)IOMy (Sy, ay) = 1, Sy, Sy ∈ Sy, ay ∈ Ay} by
Lemma 6.
19
Under review as a conference paper at ICLR 2020
Now We show the (U) direction. We first introduce some overloaded notation:
σX (Sx)= lim P PT —1 Pr(Sxt) = Sx; πx, Px, &) Lemma5 limt→∞ Pr(Sxt) = sx; Πx,Px, ηx)
πx x	t=0	x	x x , x , x	→∞	x	x x , x , x
σ∏∏x (sx,ax) = lim 1 PT01 Pr(Sxt) = sx,axt) = ax； ∏x,Px, ηx)
x	T →∞ T
Lem=ma 5 lim Pr(S(t)
t→∞
Sx,axt) = ax； ∏x,Px, ηx)
lim Pr(Sxt) = Sx； ∏x,Px, ηx)∏x(ax∣Sx)
t→∞
∏x(ax∣Sx) lim Pr(Sxt) = Sx； ∏x,Px, ηx)
t→∞
=∏x(ax∣Sx)σxχ (Sx)
Then,
(25)
σ∏lc (Sx,ax,Sx) Lemma 5 Jim Pr(Sxt) = Sx, axt) = ax,Sxt+1) = Sx； ∏x,Px, ηx)
x	t→∞
=lim Pr(Sxt+1) = SxISxt) = Sx, axt) = ax) Pr(Sxt) = Sx, axt) = ax； ∏x, Px, ηx)
t→∞
==(Sx = Px(Sx, ax)) lim Pr(Sx) = Sx, ax)= ax； πx, Px, ηx)
t→∞
R(Sx = Px(Sx, ax))σXx(Sx, ax)
(26)
=E(Sx = Px(Sx, ax))∏x(ax∣Sx)σ∏κ(Sx)	(27)
Given f, g that satisfy objective 1, 2, and 3 we construct (φ, ψ) as follows and show that (φ, ψ) ∈
Γ(Mx, My):
从(Q ) — J/(Sx) if Sx ∈ SUpp3Πx (Sx))
x Sd otherwise
ψ(ax)=	g-d1(ax)
ay
ifax ∈ Anx = Usx∈sx supp(πx(1Sx))
otherwise
where Syd,ayd are dummy state, actions such that OM (Syd,ay)=0 ∀ay ∈Ay and OM (Sy,ayd)=
0 ∀Sy ∈Sy. Such dummy state, actions always exist per Assumption 1. Mapping to the dummy state,
action will ensure that the constructions will not map suboptimal state, action pairs from domain x to
optimal state action pairs in domain y. The following statement holds for our construction (φ, ψ):
(Sx, ax) ∈ supp(σxx (Sx,	ax))	u-⇒	OMy(O(Sx),	ψ(ax)) = 1	∀sX	∈	Sx ,	ax	∈	Ax	(28)
We first prove the forward direction: (sx, ax)	∈ supp(σ*, (sx,ax))	⇒ σ^^χ (sx, ax) E=5
σɪ^χ (sx)∏x(ax∣sx) > 0, so σ^^ (sx) > 0, i.e sx ∈ supp(σXJSx)), and ∏x(ax∣sx) > 0. Furthermore,
πx(ax∣sx) > 0 ⇒ g-1(ax) ∈ supp(∏y(∙∣f (sx))) since g is injective. To see this, assume ∃(sx, ax) such
that ∏x(ax∣sx) > 0 but g-1(ax) ∈ supp(∏y(∙∣f(sx))). Then there must exists a∖ ∈ supp(∏y(∙∣f(sx)))
such that ay = g-1(ax) but g(a：) = g(g-1(ax)) = ax contradicting the injectivity of g on Ay. Putting
these results together we obtain ψ(ax) = g-1(ax) ∈ supp(∏y(∙∣φ(sx))), Since ∏y is a stochastic optimal
policy and My is unichain, ψ(ax) ∈ supp(∏y(∙∣φ(sx))) ⇒ OMy (φ(sx), ψ(ax)) = 1 by Corollary 1.
For the converse direction we prove the contrapostive: (sx, ax)	∈ supp(σ^^ (sx, ax))	⇒
θMy (Φ(sx), ψ(ax)) = 0 ∀sx ∈ Sx, ax ∈ Ax. We exhaustively consider all cases in which
(sx,ax)	∈	supp(σ∏x(sx,ax)),	i.e	σ∏x(sx,ax)	E=5	πx(axlsx)σ∏χ(Sx)	= 0. If	σ∏x(Sx)	= 0, then
sx ∈ SUPP(。丸(Sx)), so OMy(O(Sx),ay) =OMy(Sd, ay) = 0 ∀ay ∈ Ay. Else if πx(ax ISx)=
0, σ∏χ (sx) > 0 and ax ∈ A^x then OMy (Sy, ψ(ax))=OMy (Sy,aj) = 0 ∀Sy ∈ Sy. Finally,
consider the case ∏x(ax∣sx) = 0,σ^^(sx) > 0 and ax ∈ A∏x. Assume for contradiction that
OMy (φ(sx), ψ(ax)) = 1. Then, ψ(ax) ∈ supp(∏y(∙∣φ(sx)) since ∏y is a covering optimal policy
from Definition 6, which implies g-1(ax) ∈ supp(∏y(∙∣f (sx)) since σ^χ(sx) > 0 and ax ∈ A∏x. It
20
Under review as a conference paper at ICLR 2020
follows that g(g-1(aX)) ∈ supp(g(∏y(∙∣f(sX)))) ⇒ aX ∈ supp(∏χ(∙∣sX)) since ∏χ(∙∣sX) is the pushfor-
WardmeaSUre g(πy (∙f (Sx))).τhen, σ∏Ksχ,aX) E=5 σ∏κSx)nx(ax|Sx) > 0, SinCe nx(ax|Sx) > 0 and
σXχ (Sx) > 0, which contradicts (Sx, ax) ∈ supp(σx^ (Sx, ax)). This concludes the proof of Equation 28.
We proceed to show that the optimal policy and dynamics preservation properties hold for our construction
(φ, ψ).
• Optimal Policy (Equation 1): From the converse direction of the above subclaim and the optimality of
∏x the result immediate follows:
OMy (0(sX), ψ(ax)) = 1 E⇒8 (Sx,ax) ∈ supp(σ∏x(Sx,ax))
e⇒5 ∏x(ax∣Sx) > 0
C⇒1 OMx(Sx, ax) = 1 ∀sX ∈ SX ax ∈ Ax
•	Surjection (Equation 2): Assume for contradiction ∃(s., ay) such that OMy(Sy, ay) = 1,but Φ-1(s* )=
0 or ψ-1(ay) = 0. Since OMy(Sy, ay) = 1 we have Sy = Sd, ay = ay. Thus 0(s；)-1 = f-1(s^)
and ψ-1(ay) = {(g-1)-1(ay)} = {g(ay)}. Since g is a function defined Yay ∈ Ay, it follows that
ψ-1(ayy) 6= 0. Thus it must be that φ-1(Syy)=0. Let Syy0 = Py(Syy,ayy). Then,
σ→ (Sy ,ay $Lemma5 ㈣ p®yt) = Sy 词=ay ,铲1) = Sy0)
=t→m∞ Pr(^yt+1) = Sy0 ∣^yt)	=	Sy ,^yt)	= ay )Pr(^yt)	=	Sy, ayt)	= ay)
=t→m∞ Pr(^yt+1) = Sy0 ∣^yt)	=	Sy /=ay )Pr(^yt)	=	ay I^yt)	= Sy) Pr(^yt)	= Sy)
=t→∞∞ Pr(^yt+1) = Sy ι⅛yt) =Sy ,喈=ay )πy (ay |Sy) Psx ∈°τ ⑻)Pr(«xt)= Sx)
=→m∞Pr(犷1)=Sy0 ι⅛yt) =Sy, ⅛yt) = ay)% (ay 曷) ∙ 0
0
However,
y yy
σπy (Sy ,ay ,
Syj E=27 1 (Py (Sy ,ay ) = Py (Sy ,ay ))∏y (ay 同注(Sy)
=πy (ay 禺 )σyy (Sy) > 0
To see why the last inequality holds, first recall that My is unichain and πy is stochastic optimal for
My, so the stationary distribution over states have full support over Sy (√ stationary distributions of
irreducible markov chains are fully supported over the entire state space) Therefore σπy (Sy) Lem=ma5
limt→∞ Pr(Syt) = Sy; ∏y,Py) > 0 ∀Sy ∈ Sy. Thus, we have q∏η(s∖) > 0. Furthermore, ∏y (ay 同)>
0 by Corollary 1. Putting these two results together, we obtain σ∏y (sy, ay, sy0) > 0. Then, σ^→y = σ∏y
which contradicts the satisfiability of objective 3.
•	Dynamics (Equation 3): Assume for contradiction that ∃S- ,a- and S-0 = Px(S- ,a- such that
OM (φ(Sx-), ψ(ax-)) = 1 but the dynamics preservation property is violated, i.e Py(φ(Sx-),ψ(ax-)) 6=
φ(Px(S-,a-)) = φ(s-0). If (s-,a-) ∈ supp(σ∏ KSx,ax)),then OMy (φ(s-), ψ(a-)) =0 by Equation
28 which contradicts O(φ(s-), ψ(a-)) = 1. Thus, it must be that (s-, a-) ∈ supp(σx^ (sx, ax)) which
further implies (S-, a-, S-O) ∈ supp(σXJSx, ax, Sx)) by EqUatiOn 26 and φ(s-) = f(S-), ψ(a-)=
g-1(a-) by Equation 25 since σx^ (s-) > 0, π(a- ∣s-) > 0.
Let F : Sx × g(Ay) ×Sx →Sy ×Ay ×Sy be a function (a, b, c) 7→ (f(a), g-1(b), f(c)). Then, by
Lemma 7, we have σ∏→y (sx, ax, sx) = F(ρ∏χ (sx, ax, Sx))Lemma5 F(σ∏^χ (sx, ax, sx)). So,
σ∏x (s-,a-, s-0) > 0 ⇒ σx→y (F(s-,a-, s-0))= σf→→(f (s-), g-1(a-), f (s-0)) > 0
21
Under review as a conference paper at ICLR 2020
Thus, (f(s-), g-1(a-),f (S-O)) = (φ(s-), ψ(a-),φ(S-O)) ∈ supp(σx→y(Sx, ax, sX)). However,
σyy (φ(s-), Mg, φ(S- * E-6 σyy (φ(s-), ψ(a-))l (φ(s-0 ) = Py (φ(s-), ψ(aXS))
=啖 (φ(s- ), ψ(a-)) ^ 0
=0
Thus, supp(σX→y) = supp(σyj ⇒ σ^→y = σ∏y which contradicts f,g satisfying objective 3. This
concludes the proof of the main theorem.
□
22