Under review as a conference paper at ICLR 2020
Modeling treatment events in disease pro-
GRESSION
Anonymous authors
Paper under double-blind review
Ab stract
Ability to quantify and predict progression of a disease is fundamental for selecting
an appropriate treatment. Many clinical metrics cannot be acquired frequently
either because of their cost (e.g. MRI, gait analysis) or because they are incon-
venient or harmful to a patient (e.g. biopsy, x-ray). In such scenarios, in order
to estimate individual trajectories of disease progression, it is advantageous to
leverage similarities between patients, i.e. the covariance of trajectories, and find
a latent representation of progression. However, to our knowledge, most of ex-
isting methods for estimating trajectories do not explicitly account for treatment
events such as surgery in-between observations, which dramatically decreases their
adequacy for clinical practice. In this study, we develop a first machine learning
framework named Coordinatewise-Soft-Impute (CSI) for analyzing disease pro-
gression from sparse observations in the presence of confounding events. CSI is
easy to implement and is guaranteed to converge to the global minimum of the
corresponding optimization problem. Experimental results also demonstrate the
effectiveness of CSI using both simulated and real datasets.
1	Introduction
The course of disease progression in individual patients is one of the biggest uncertainties in medical
practice. In an ideal world, accurate, continuous assessment of a patient’s condition helps with
prevention and treatment. However, many medical tests are either harmful or inconvenient to perform
frequently, and practitioners have to infer the development of disease from sparse, noisy observations.
In its simplest form, the problem of modeling disease progressions is to fit the curve of y(t), t ∈
[tmin,tmax] for each patient, given sparse observations y := (y(tι),...,y(tn)). Due to the high-
dimensional nature of longitudinal data, existing results usually restrict solutions to subspace of
functions and utilize similarities between patients via enforcing low-rank structures. One popular
approach is the mixed effect models, including Gaussian process approaches (Verbeke, 1997; Zeger
et al., 1988) and functional principal components (James et al., 2000). While generative models are
commonly used and have nice theoretical properties, their result could be sensitive to the underlying
distributional assumptions of observed data and hard to adapt to different applications. Another line
of research is to pose the problem of disease progression estimation as an optimization problem.
Kidzinski and Hastie. Kidzinski & Hastie (2018) proposed a framework which formulates the problem
as a matrix completion problem and solve it using matrix factorization techniques. This method is
distribution-free and flexible to possible extensions.
Meanwhile, both types of solutions model the natural progression of disease using observations of
the targeted variables only. They fail to incorporate the existence and effect of human interference:
medications, therapies, surgeries, etc. Two patients with similar symptoms initially may have different
futures if they choose different treatments. Without that information, predictions can be way-off.
To the best of our knowledge, existing literature talks little about modeling treatment effect on disease
progression. In KidzinSki & Hastie (2018), authors use concurrent observations of auxillary variables
(e.g. oxygen consumption to motor functions) to help estimate the target one, under the assumption
that both variables reflect the intrinsic latent feature of the disease and are thus correlated. Treatments
of various types, however, rely on human decisions and to some extent, an exogenous variable to the
development of disease. Thus they need to modeled differently.
1
Under review as a conference paper at ICLR 2020
In this work, we propose a model for tracking disease progression that includes the effects of
treatments. We introduce the Coordinatewise-Soft-Impute (CSI) algorithm for fitting the model and
investigate its theoretical and practical properties. The contribution of our work is threefold: First,
we propose a model and an algorithm CSI, to estimate the progression of disease which incorporates
the effect of treatment events. The framework is flexible, distribution-free, simple to implement
and generalizable. Second, we prove that CSI converges to the global solution regardless of the
initialization. Third, we compare the performance of CSI with various other existing methods on both
simulated data and a dataset of Gillette Children’s Hospital with patients diagnosed with Cerebral
Palsy, and demonstrate the superior performances of CSI.
The rest of the paper is organized as follows. In Section 2 we state the problem and review existing
methods. Next, in Section 3 we describe the model and the algorithm. Theoretic properties of the
algorithm are derived in Section 4. Finally, in Section 5 and 6 we provides empirical results of CSI
on the simulated and the real datesets respectively. We discuss some future directions in Section 7.
2	Problem statement and related work
Let y(t) be the trajectory of our objective variable, such as the size of tumor, over fixed time range
t ∈ [tmin, tmax], and N be the number of patients. For each patient 1 ≤ i ≤ N, we measure
its trajectory yi(t) at ni irregularly time points ti = [ti,1 , ti,2, ..., ti,ni]0 and denote the results as
yi = [yi,1, ..., yi,ni]0 = [yi(ti,1), ...,yi(ti,ni)]0. We are primarily interested in estimating the disease
progression trajectories {yi(t)}iN=1 of all N patients, based on observation data {(ti, yi)}iN=1.
To fit a continuous curve based on discrete observations, we restrict our estimations to a finite-
dimensional space of functions. Let {bi , i ∈ N} be a fixed basis of L2 ([tmin , tmax]) (e.g. splines,
Fourier basis) and b = {bi : 1 ≤ i ≤ K} be first K dimensions of it. The problem of estimating
yi(t) can then be reduced to the problem of estimating the coefficients Wi = [wi,1,wi,2, ∙∙∙ , Wi,κ]0
such that w0ib(t) is close to yi(t) at time t ∈ ti.
Though intuitive, the above method has two main drawbacks. First, when the number of observations
per patient is less than or equal to the number of basis functions K, we can perfectly fit any curve
without error, leading to overfitting. Moreover, this direct approach ignores the similarities between
curves. Different patients may share similar trend of the trajectories which could potentially imporve
the prediction. Below we describe two main lines of research improving on this, the mixed-effect
model and the matrix completion model.
2.1	Linear mixed-effect model
In mixed-effect models, every trajectory yi (t) is assumed to be composed of two parts: the fixed
effect μ(t) = m0b(t) for some m ∈ RK that remains the same among all patients and a random
effect Wi ∈ RK that differs for each i ∈ {1, . . . , N}. In its simplest form, we assume
Wi 〜N(0, ∑)	and y/wi 〜N(μi + BiWi,σ2In)
where Σ is the K X K covariance matrix, σ is the standard deviation and μ% =
[μ(ti,ι), μ(ti,2),…μ(ti,ni)]0, Bi = [b(ti,ι), b(ti,2),…，b(ti,ni)]0 are functions μ(t) and b(t) eval-
uated at the times ti, respectively. Estimations of model parameters μ, Σ can be made via expectation
maximization (EM) algorithm (Laird & Ware, 1982). Individual coefficients Wi can be estimated
using the best unbiased linear predictor (BLUP) (Henderson, 1975).
In linear mixed-effect model, each trajectory is estimated with |Wi | = K degrees of freedom, which
can still be too complex when observations are sparse. One typical solution is to assume a low-rank
structure of the covariance matrix Σ by introducing a contraction mapping A from the functional
basis to a low-dimensional latent space. More specifically, one may rewrite the LMM model as
yi|Wi 〜N(μi + BiAWi, σ2InJ,
where A is a K × q matrix with q < K and Wi ∈ Rq is the new, shorter random effect to be
estimated. Methods based on low-rank approximations are widely adopted and applied in practice
and different algorithms on fitting the model have been proposed (James et al., 2000; Lawrence, 2004;
Schulam & Arora, 2016). In the later sections, we will compare our algorithm with one specific
implementation named functional-Principle-Component-Analysis (fPCA) (James et al., 2000), which
uses EM algorithm for estimating model parameters and latent variables Wi .
2
Under review as a conference paper at ICLR 2020
2.2	Matrix completion model
While the probabilistic approach of mixed-effect models offers many theoretical advantages including
convergence rates and inference testing, it is often sensitive to the assumptions on distributions, some
of which are hard to verify in practice. To avoid the potential bias of distributional assumptions in
mixed-effect models, Kidzinski and Hastie (Kidzinski & Hastie, 2018) formulate the problem as a
sparse matrix completion problem. We will review this approach in the current section.
To reduce the continuous-time trajectories into matrices, we discretize the time range [tmin, tmax]
into T equi-distributed points G = [τ1, . . . , τT] with τ1 = tmin, τT = tmax and let B =
[b(τι), b(τ2),…，b(ττ)]0 ∈ RT×K be the projection of the K-truncated basis b onto grid G.
The N × K observation matrix Y is constructed from the data {(ti, yi)}iN=1 by rounding the time ti,j
of every observation yi(ti,j) to the nearest time grid and regarding all other entries as missing values.
Due to sparsity, we assume that no two observation yi(ti,j)’s are mapped to the same entry of Y .
Let Ω denote the set of all observed entries of Y. For any matrix A, let PΩ(A) be the projection of
A onto Ω, i.e. Pω(a) = M where Mij = Aij for (i,j) ∈ Ω and Mi,j = 0 otherwise. Similarly,
We define P⊥(A) = A - PΩ(A) to be the projection on the complement of Ω. Under this setting,
the trajectory prediction problem is reduced to the problem of fitting a N × K matrix W such that
WB0 ≈ Y on observed indices Ω.
The direct way of estimating W is to solve the optimization problem
argmin 1 ∣∣Pω(Y - WB0)kF,	(2.1)
W2
where ∣∣ ∙ ∣∣f is the Frobenius norm. Again, if K is larger than the number of observations for some
subject we will overfit. To avoid this problem we need some additional constraints on W. A typical
approach in the matrix completion community is to introduce a nuclear norm penalty—a relaxed
version of the rank penalty while preserving convexity (Rennie & Srebro, 2005; CandeS & Recht,
2009). The optimization problem with the nuclear norm penalty takes form
argmin 1 ∣Pω(Y - WB0)kF + λ∣W∣∣*,	(2.2)
W2
where λ > 0 is the regularization parameter, ∣ ∙ ∣∣f is the Frobenius norm, and ∣∙∣∣* is the nuclear
norm, i.e. the sum of singular values. In KidzinSki & Hastie (2018), a Soft-Longitudinal-Impute
(SLI) algorithm is proposed to solve (2.2) efficiently. We refer the readers to KidzinSki & Hastie
(2018) for detailed description of SLI while noting that it is also a special case of our algorithm 1
defined in the next section with μ fixed to be 0.
3	Modeling treatment in disease progression
In this section, we introduce our model on effect of treatments in disease progression.
A wide variety of treatments with different effects and durations exist in medical practice and it
is impossible to build a single model to encompass them all. In this study we take the simplified
approach and regard treatment, with the example of one-time surgery in mind, as a non-recurring
event with an additive effect on the targeted variable afterward. Due to the flexibility of formulation of
optimization problem (2.1), we build our model based on matrix completion framework of Section 2.2.
More specifically, let s(i) ∈ G be the time of treatment of the i’th patient, rounded to the closest
τk ∈ G (s(i) = ∞ if no treatment is performed). We encode the treatment information as a N × T
zero-one matrix IS, where (IS)i,j = 1 if and only τj ≥ s(i), i.e. patient i has already taken the
treatment by time Tj. Each row of IS takes the form of (0, ∙∙∙ , 0,1, ∙∙∙ , 1). Let μ denote the average
additive effect of treatment among all patients. In practice, we have access to the sparse observation
matrix Y and surgery matrix IS and aim to estimate the treatment effect μ and individual coefficient
matrix W based on Y, IS and the fixed basis matrix B such that WB0 + μIs ≈ Y.
Again, to avoid overfitting and exploit the similarities between individuals, we add a penalty term on
the nuclear norm of W . The optimization problem is thus expressed as:
argmin 1 ∣Pω(Y - WB0 - μIs)∣F + λ∣W∣∣*,	(3.1)
μ,W 2
3
Under review as a conference paper at ICLR 2020
for some λ > 0.
3.1	Coordinatewise-Soft-Impute (CSI) algorithm
Though the optimization problem (3.1) above does not admit an explicit analytical solution, it
is not hard to solve for one of μ or W given the other one. For fixed μ, the problem reduces
to the optimization problem (2.2) with Y = Y - μIs and can be solved iteratively by the SLI
algorithm Kidzinski & Hastie (2018), which we will also specify later in Algorithm 1. For fixed W,
we have
argmin 1 ∣∣Pω(Y - WB0 - μIs)kF + λ∣∣Wk*
μ 2
μ
argmin 1 ∣∣Pω(-WB0 - μIs)kF = argmin 1 X ((Y - WB0)i,j - μ)2,	(3.2)
μ 2	μ 2 ,..、__
(i,j)∈Ω∩Ωs
where Ωs is the set of non-zero indices of IS. Optimization problem (3.2) can be solved by taking
derivative with respect to μ directly, which yields
Σ(i,j)∈Ω∩Ωs (Y - WB0)i,j
∣Ω∩ΩS∣
(3.3)
The clean formulation of (3.3) motivates us to the following Coordinatewise-Soft-Impute (CSI)
algorithm (Algorithm 1): At each iteration, CSI updates WneW from (Wold, μold) Via soft singular
value thresholding and then updates μ∩ew from (W∏ew, μold) Via (3.3), finally it replaces the missing
values of Y based (Wnew, μnew). In the definition, we define operator Sχ as for any matrix X,
Sλ(X) := UDλV , where X = UDV is the SVD of X and Dλ = diag((max{di - λ, 0})iK=1) is
derived from the diagonal matrix D = diag((di)K=J∙ Note that if we set μ ≡ 0 throughout the
updates, then we get back to our base model SLI without treatment effect.
Algorithm 1： COORDINATEWISE-SOFT-IMPUTE
1.	Initialize Wold J all-zero matrix, μ0ld J 0.
2.	Repeat:
(a)	Compute WneW J Si((Pω(Y - μ°ldIs) + P⊥(WoldB0))B);
(b) Compute μ□ew J
P(i,j)∈Ω∩Ωs (Y-WneWBO)i,j
∣Ω∩Ωs |
(c)	If max n (μnew^2μoId)
—	I	μ2ld
kWnew-WldkF O < ε exit-
,k Wold kF	j<ε,exit,
(d)	ASSign Wold J Wnew, μold J Mnew∙
3.	Output Wλ J Wn
ew, μλ j- μnew.
4	Convergence Analysis
In this section we study the convergence properties of Algorithm 1. Fix the regularization parameter
λ > 0, let (μλk), Wλk)) be the value of (μ, W) in the k,th iteration of the algorithm, the exact
definition of which is provided below in (4.4). We prove that Algorithm 1 reduces the loss function at
each iteration and eventually converges to the global minimizer.
Theorem 1. The sequence (μλk),Wλk)) converges to a limit point (μλ, Wʌ) which solves the opti-
mization problem:
(μλ,Wλ) = argmW∙2 kPΩ(Y-WBO -μIS )kF + λkW k*.
Moreover, (μλ, Wλ) satisfies that
L	^7 _ tQ、ROV .
Wλ	= Sλ((PΩ(Y	-	μλIS)	+	P⊥(WλB0))B),	μλ = £(i,j)eQ；ES；	C I--i-.	(4.1)
∣Ω ∩ Ωs∣
4
Under review as a conference paper at ICLR 2020
The proof of Theorem 1 relies on five technique Lemmas stated below. The detailed proofs of the
lemmas and the proof to Theorem 1 are provided in Appendix A. The first two lemmas are on
properties of the nuclear norm shrinkage operator Sλ defined in Section 3.1.
Lemma 1. Let W be an N × K matrix and B is an orthogonal T × K matrix of rank K. The
1
solution to the optimization problem mmw 2 ∣∣Y 一 WB0 kF + λ∣∣W k* is given by W = Sλ(YB)
where Sλ(YB) is defined in Section 3.1.
Lemma 2. Operator Sλ(∙) satisfies the following inequality for any two matrices Wi, W2 with
matching dimensions:
∣Sλ(Wi) — Sλ(W2)kF ≤ kWi — W2kF.
Define
fλ(W, μ) = 2∣Pω(Y - WB0 - μIs)kF + λ∣Wk*,	(4.2)
Qλ(W|W, μ) = 2∣Pω(Y - μIs) + P⊥(WB0) - WB0kF + λ∣∣Wk*∙	(4.3)
Lemma 1 shows that in the k-th step of Algorithm 1, Wλ(k) is the minimizer for function
Qλ(∙∣W(k-1),μ(k)). The next lemma proves the sequence of loss functions fλ(Wλk),μλk)) is
monotonically decreasing at each iteration.
Lemma 3. For every fixed λ ≥ 0, the k 'th step ofthe algorithm (μλk),Wλk)) is given by
Wλk) = argmWnQλ(W|Wλk-1),μλk-1))	暧=P&j)^。^：ω-W' '")i' .	(4.4)
Then with any starting point (μ弋 0, W：0)), the sequence {(μλk), Wλk))}k satisfies
fλ(Wλk),μλk)) ≤ fλ(Wλk),μλkτ)) ≤ Qλ(Wλk)∣Wλk-10,μλk-10) ≤ fλ(Wλk-10,μλk-10).
The next lemma proves that differences (μk 一 μk-1)2 and k Wλk) - W(k-10 kF both converge to 0.
Lemma 4. For any positive integer k, we have kWλ(k+i) 一 Wλ(k) k2F ≤ kWλ(k) 一 Wλ(k-i) k2F.
Moreover,
μλk+1) 一 μλk) → 0,	Wλk+1 一 W(k0 → 0 as k →∞.
Finally We show that if the sequence {(μλk), Wλk))}k, it has to converge to a solution of (4.1).
Lemma 5. Any limit point (μλ, Wλ) ofsequences {(μλk),Wλk))}k satisfies (4.1).
5	Simulation study
In this section we illustrate properties of our Coordinatewise-Soft-Impute (CSI) algorithm via
simulation study. The simulated data are generated from a mixed-effect model with low-rank
covariance structure on W :
Y = WB + μIs + E,
for which the specific construction is deferred to Appendix B. Below we discuss the evaluation
methods as well as the results from simulation study.
5.1	Methods
We compare the Coordinatewise-Soft-Impute (CSI) algorithm specified in Algorithm 1 with the
vanilla algorithm SLI (corresponding to μ = 0 in our notation) defined in Kidzinski & Hastie (2018)
and the fPCA algorithm defined in James et al. (2000) based on mixed-effect model. We train all
three algorithms on the same set of basis functions and choose the tuning parameters λ (for CSI and
5
Under review as a conference paper at ICLR 2020
SLI) and R (for fPCA) using a 5-fold cross-validation. Each model is then re-trained using the whole
training set and tested on a held-out test set Ωtest consisting 10% of all data.
The performance is evaluated in two aspects. First, for different combinations of the treatment effect μ
and observation density ρ, we train each of the three algorithms on the simulated data set, and compute
the relative squared error between the ground truth μ and estimation μ., i.e., RSE(μ) = (μ - μ)2∕μ2.
Meanwhile, for different algorithms applied to the same data set, we compare the mean square error
between observation Y and estimation Y over test set Ωtest, namely,
,ʌ..
MSE(Y)
∣ω^	X	(Yij- Yj )2 = Tω^ kPΩtest (Y) - PΩtest (Y)kF
test	test
(Zj) ∈ Qtest
(5.1)
We train our algorithms with all combinations of treatment effect μ ∈ {0,0.2,0.4,…，5}, obser-
vation rate P ∈ {0.1,0.3,0.5}, and thresholding parameter λ ∈ {0,1,…,4} (for CSI or SLI) or
rank R ∈ {2,3,…，6} (for fPCA). For each fixed combination of parameters, we implemented each
algorithm 10 times and average the test error.
5.2	Results
The results are presented in Table 1 and Figure 1. From Table 1 and the left plot of Figure 1, we have
the following findings:
1.	CSI achieves better performance than SLI and fPCA, regardless of the treatment effect μ
and observation rate ρ. Meanwhile SLI performs better than fPCA.
2.	All three methods give comparable errors for smaller values of μ. In particular, our introduc-
tion of treatment effect μ does not over-fit the model in the case of μ = 0.
3.	As the treatment effect μ increases, the performance of CSI remains the same whereas
the performances of SLI and fPCA deteriorate rapidly. As a result, CSI outperforms SLI
and fPCA by a significant margin for large values of μ. For example, when P = 0.1, the
MSE(Y) of CSI decreases from 72.3% of SLI and 59.6% of fPCA at μ = 1 to 12.4% of
SLI and 5.8% offPCA at μ = 5.
4.	All three algorithms suffer a higher MSE(Y) with smaller observation rate ρ. The biggest
decay comes from SLI with an average 118% increase in test error from P = 0.5 to
P = 0.1. The performances of fPCA and CSI remains comparatively stable among different
observation rate with a 6% and 12% increase respectively. This implies that our algorithm is
tolerant to low observation rate.
To further investigate CSI,s ability to estimate μ, we plot the relative squared error of μ using CSI
with different observation rate in the right plot of Figure 1. As shown in Figure 1, regardless of the
choice of observation rate P and treatment effect μ, RSE(μ) is always smaller than 1% and most of
the estimations achieves error less than 0.1%. Therefore we could conclude that, even for sparse
matrix Y, the CSI algorithm could still give very accurate estimate of the treatment effect μ.
Observation rate P		0.1				0.3			0.5	
Treatment effect μ	1	2	5	1	2	5	1	2	5
fPCA	0.521	2.172	5.455	0.525	2.039	5.170	0.525	2.036	5.166
MSE(Y)	SLI	0.430	1.162	2.561	0.379	0.658	1.203	0.341	0.543	0.893
CSI	0.311	0.306	0.318	0.314	0.297	0.320	0.294	0.299	0.295
Table 1: Comparisons between fPCA, SLI and CSI under different values of P and μ.
6	Data S tudy
In this section, we apply our methods to real dataset on the progression of motor impairment and gait
pathology among children with Cerebral Palsy (CP) and evaluate the effect of orthopaedic surgeries.
Cerebral palsy is a group of permanent movement disorders that appear in early childhood. Or-
thopaedic surgery plays a major role in minimizing gait impairments related to CP (McGinley et al.,
6
Under review as a conference paper at ICLR 2020
Methods
fPCA0.1
fPCA0.3
fPCA0.5
SLI0.1
SLI0.3
SLI0.5
CSI0.1
CSI0.3
CSI0.5
Observation rate
0.1
0.3
—^— 0.5



Figure 1: Left: Comparisons between fPCA, SLI and CSI in estimating Y with different obser-
vation rates. Lines with colors red, green and blue correspond to fPCA, SLI and CSI respectively.
Dotted, dashed and straight lines correspond to observation rate ρ = 0.1, 0.3 and 0.5 respectively.
Right: Relationship between relative squared error of μ and treatment effect μ using CSI with
different observation rate. Lines with colors red, green and blue correspond to observation rate
ρ = 0.1, 0.3 and 0.5 respectively.
2012). However, it could be hard to correctly evaluate the outcome of a surgery. For example, the
seemingly positive outcome of a surgery may actually due to the natural improvement during puberty.
Our objective is to single out the effect of surgeries from the natural progression of disease and use
that extra piece of information for better predictions.
6.1	Data and Method
We analyze a data set of Gillette Children’s Hospital patients, visiting the clinic between 1994
and 2014, age ranging between 4 and 19 years, mostly diagnosed with Cerebral Palsy. The data
set contains 84 visits of 36 patients without gait disorders and 6066 visits of 2898 patients with
gait pathologies. Gait Deviation Index (GDI), one of the most commonly adopted metrics for gait
functionalities (Schwartz & Rozumalski, 2008), was measured and recorded at each clinic visit along
with other data such as birthday, subtype of CP, date and type of previous surgery and other medical
results.
Our main objective is to model individual disease progression quantified as GDI values. Due to
insufficiency of data, we model surgeries of different types and multiple surgeries as a single additive
effect on GDI measurements following the methodology from Section 3. We test the same three
methods CSI, SLI and fPCA as in Section 5, and compare them to two benchmarks—the population
mean of all patients (pMean) and the average GDI from previous visits of the same patient (rMean).
All three algorithms was trained on the spline basis of K = 9 dimensions evaluated at a grid
of T = 51 points, with regularization parameters λ ∈ {20, 25, ..., 40} for CSI and SLI and rank
constraints r ∈ {2, . . . , 6} for fPCA. To ensure sufficient observations for training, we cross validate
and test our models on patients with at least 4 visits and use the rest of the data as a common training
set. The effective size of 2-fold validation sets and test set are 5% each. We compare the result of
each method/combination of parameters using the mean square error of GDI estimations on held-out
entries as defined in (5.1).
6.2	Results
We run all five methods on the same training/validation/test set for 40 times and compare the mean
and sd of test-errors. The results are presented in Table 2 and Figure 2. Compared with the null model
pMean (Column 2 of Table 2), fPCA gives roughly the same order of error; CSI, SLI and rowMean
provide better predictions, achieving 62%, 66% and 73% of the test errors respectively. In particular,
our algorithm CSI improves the result of vanilla model SLI by 7%, it also provide a stable estimation
with the smallest sd across multiple selections of test sets.
We take a closer look at the low-rank decomposition of disease progression curves provided by
algorithms. Fix one run of algorithm CSI with λ? = 30, there are 6 non-zero singular value vectors,
which we will refer as principal components. We illustrate the top 3 PCs scaled with corresponding
singular values in Figure 3a. The first PC recovers the general trend that gait disorder develops
7
Under review as a conference paper at ICLR 2020
	mean	scaled mean	sd
CSI	74.28	062^	8.90
SLI	79.92	0.66	9.22
fPCA	127.73	1.06	13.54
rMean	87.26	0.73	8.96
pMean	119.80	1.00	12.84
Figure 2: Box plot for test errors
Table 2: Test error on GDI dataset
(a) Top 3 PCs from CSI algorithm
(b) Predicted curve of patient ID 5416
Figure 3: Low-rank decomposition of disease progression curves
through age 1-10 and partially recovers during puberty. The second and third PC reflects fluctuations
during different periods of child growth. By visual inspection, similar trends can be find in the top
components of SLI and fPCA as well.
An example of predicted curve from patient ID 5416 is illustrated in Figure 3b , where the blue curve
represents the prediction without estimated treatment effect μ = 4.33, green curve the final prediction
and red dots actual observations. It can be seen that the additive treatment effect helps to model the
sharp difference between the exam before exam (first observation) and later exams.
7 Conclusion and Future Work
In this paper, we propose a new framework in modeling the effect of treatment events in disease
progression and prove a corresponding algorithm CSI. To the best of our knowledge, it’s the first
comprehensive model that explicitly incorporates the effect of treatment events. We would also like
to mention that, although we focus on the case of disease progression in this paper, our framework is
quite general and can be used to analyze data in any disciplines with sparse observations as well as
external effects.
There are several potential extensions to our current framework. Firstly, our framework could be
extended to more complicated settings. In our model, treatments have been characterized as the
binary matrix IS with a single parameter μ. In practice, each individual may take different types
of surgeries for one or multiple times. Secondly, the treatment effect may be correlated with the
latent variables of disease type, and can be estimated together with the random effect wi . Finally, our
framework could be used to evaluate the true effect of a surgery. A natural question is: does surgery
really help? CSI provides estimate of the surgery effect μ, it would be interesting to design certain
statistical hypothesis testing/casual inference procedure to answer the proposed question.
Though we are convinced that our work will not be the last word in estimating the disease progression,
we hope our idea is useful for further research and we hope the readers could help to take it further.
References
Jian-Feng Cai, Emmanuel J Candes, and Zuowei Shen. A singular value thresholding algorithm for
matrix completion. SIAM Journal on Optimization, 20(4), 2010.
8
Under review as a conference paper at ICLR 2020
Emmanuel J Candes and Benjamin Recht. Exact matrix completion via convex optimization. Foun-
dations of Computational mathematics, 9(6):717, 2009.
Charles R Henderson. Best linear unbiased estimation and prediction under a selection model.
Biometrics, pp. 423-447,1975.
Gareth M James, Trevor J Hastie, and Catherine A Sugar. Principal component models for sparse
functional data. Biometrika, pp. 587-602, 2000.
Eukasz KidZinski and Trevor Hastie. Longitudinal data analysis using matrix completion. arXiv
preprint arXiv:1809.08771, 2018.
Nan M Laird and James H Ware. Random-effects models for longitudinal data. Biometrics, pp.
963-974, 1982.
Neil D Lawrence. Gaussian process latent variable models for visualisation of high dimensional data.
In Advances in neural information processing systems, pp. 329-336, 2004.
Rahul Mazumder, Trevor Hastie, and Robert Tibshirani. Spectral regularization algorithms for
learning large incomplete matrices. Journal of machine learning research, 11(Aug):2287-2322,
2010.
Jennifer L McGinley, Fiona Dobson, Rekha Ganeshalingam, Benjamin J Shore, Erich Rutz, and
H Kerr Graham. Single-event multilevel surgery for children with cerebral palsy: a systematic
review. Developmental Medicine & Child Neurology, 54(2):117-128, 2012.
Jasson DM Rennie and Nathan Srebro. Fast maximum margin matrix factorization for collaborative
prediction. In Proceedings of the 22nd international conference on Machine learning, pp. 713-719.
ACM, 2005.
Peter Schulam and Raman Arora. Disease trajectory maps. In Advances in Neural Information
Processing Systems, pp. 4709-4717, 2016.
Michael H Schwartz and Adam Rozumalski. The gait deviation index: a new comprehensive index
of gait pathology. Gait & posture, 28(3):351-357, 2008.
Geert Verbeke. Linear mixed models for longitudinal data. In Linear mixed models in practice, pp.
63-153. Springer, 1997.
Scott L Zeger, Kung-Yee Liang, and Paul S Albert. Models for longitudinal data: a generalized
estimating equation approach. Biometrics, pp. 1049-1060, 1988.
9
Under review as a conference paper at ICLR 2020
A	Proofs
Proof of Lemma 1. Note that the solution of the optimization problem
min WkZ - AkF + λ∣IAk*	(A.1)
A2
is given by A = Sλ(Z) (See Cai et al. (2010) for a proof). Therefore it suffices to show the minimizer
of the optimization problem (A.1) is the same as the minimizer of the following problem:
min1 kYB - W kF + λkW k*.
W2
Using the fact that kAk2F = Tr(AA0) and B0B = IK, we have
argmin 1 ∣∣YB - W kF + λkW k* = argmin 1(Tr(YBB0Y0) + Tr(WW0) - 2Tr(YBW 0)) + λ∣∣W k*
=argmin 1(Tr(WW0) - 2Tr(YBW0)) + λ∣∣W∣∣*.
W2
On the other hand
argmin 1kY - WB0kF + λ∣∣W ∣∣* = arg min1(Tr(YY0) + Tr(WW0) - 2Tr(YBW 0)) + λ∣∣W ∣∣*
W2	W2
=argmin1(Tr(WW0) - 2Tr(YBW0)) + λ∣∣W∣∣*
=arg min 1IIYB - W IlF + λ∣∣W ∣∣*
W2
= Sλ(YB),
as desired.	□
Proof of Lemma 2. We refer the readers to the proof in Mazumder et al. (2010, Section 4, Lemma
3).	□
ProofofLemma 3. First we argue that μ∖k) = argmin* fλ(Wλk),μ) and the first inequality imme-
diately follows. We have
arg min fλ(Wχ∖μ) = arg min ∣∣Pω(Y - W(k) B0 - μIs )kF
μ	μ
=arg min X ((Y - W(k)B0)i,j - μ)2.
μ	4一*
(i,j)∈Ω∩Ωs
Taking derivative with respect to μ directly gives μλk = arg min* fλ(Wλk),μ), as desired.
For the rest two inequalities, notice that
fλ(Wλk),μλkT)) = 2 ∣Pω(y - WtkB"『1 IS )kF + λ∣Wλk)k*
≤ 1 ∣Pω(Y - μλkT)IS) + P⊥(WλkT)BO)- W(k1B0kF + λkWλk)k* (A.2)
=Qλ(Wλk)∣Wλkτ),μλkT))
≤ Qλ(Wλkτ)∣WλkT),μλkT))	(A.3)
=2 ∣Pω(y - W(k-1) BjLIS )kF + λkWλk-1)k*
=fλ(Wλkτ),μλkτ)).
10
Under review as a conference paper at ICLR 2020
Here the (A.2) holds because we have
2 kPΩ(Y - μλk-1)Is)+ P⊥(wλkT)BO)- wλk)B0 kF
=1 HPω(Y — μ尸 IS — wλk)B0) + Pi⊥(wλk-1)B' — wλk)B0)kF
=11死(丫 - μλk-1)Is - wλk)B0)kF + 2 kP⊥(wλk-1)B 0 — wλk)B0) kF
≥ 1 kPΩ(Y - μλk-1)Is - wλk)B0)kF.
(A.3) follows from the fact that W((k) = argminw Qλ(W∣wλk-1) ,μ*-%.	□
ProofofLemma 4. First We analyze the behavior of {μ1k)},
fλ(wλk),μλkτ)) - fλ(wλk),μλk)) = 11序(丫 - w(k)B0 - μλk-1)Is)kF
- 1 kPΩ(Y - wλk)B0 - μλk)Is)kF
=s∩2ΩSl
Meanwhile, the sequence (…，fλ(wλk-1),μλk-1)),fλ(wλk),μλk-1)), fλ(wλk), μλk)),…)is de-
creasing and lower bounded by 0 and therefore converge to a non-negative number, yielding the
differences fλ(wλk),μλk-1)) 一 fλ(wλk),μλk)) → 0 as k → ∞. Hence
μλk) - μλk-1) → 0,	(A.4)
as desired.
The sequence {Wλ(k)} is slightly more complicated, direct calculation gives
kwλk) - wλk-1)kF = kSλ(PΩ(Y -Jt1IS) + P⊥(wλkτ)B0))
-	Sλ(PΩ(Y - μλk-2)Is) + P⊥(wλk-2)B0))kF
≤	kPΩ(Y - μλk-1)Is) + P⊥(wλkτ)B0)	(A.5)
-	Pω(Y - μλk-2)IS) - P⊥(Wλk-2)B0)kF
=	∣Ω ∩ Ωs∣(μλk-1) - μλk-2))2 + kP⊥(Wλk-1)B0 - Wλk-2)B0)kF, (A.6)
where (A.5) follows from Lemma 2, (A.6) can be derived pairing the 4 terms according to Pω and
P⊥.
By definition of μ∖k), we have
∣Ω ∩ Ωs∣(μλkT)- μλk-2))2 = iɪ- ( X	(wλk-1)B0 - wλk-2)B0)i,j
1	S ∖(i,j)∈Ω∩Ωs	)
≤ kPΩ(wλkT)BO- wλk-2)B0)kF,	(A.7)
where (A.7) follows from the Cauchy-Schwartz inequality.
Combining (A.6) with (A.7), we get
kWλ(k) - Wλ(k-1)k2F ≤ kWλ(k-1)B0 - Wλ(k-2)B0k2F = kWλ(k-1) - Wλ(k-2)k2F.
Now we are left to prove that the difference sequence {Wλ(k)-Wλ(k-1)} converges to zero. Combining
(A.4) and (A.7) it suffices to prove that ∣∣P⊥(wλkT)B0 - wλk-2)B0)kF → 0. We have
fλ(Wλk-1),μλk-2)) - Qλ(wλkτ)∣wλk-2),μλk-2)) = -kP⊥(wλkτ)B0 - wλk-2)B0)kF,
11
Under review as a conference paper at ICLR 2020
and the left hand side converges to 0 because
0 ≥ fλ(wλkτ),μλk-2)) -Qλ(wλkτ)∣wλk-2),μλk-2))
≥ fλ(wλk-2),μλk-2))- fλ(wλk-1),μλk-2)) → 0,
which completes the proof.	□
ProofofLemma 5. Let (μλmk),wλmk)) → (4λ,Wλ), then Lemma4 gives (μ(λmk-1), wλmk-1)) →
(μλ, Wλ). Since We have
Wλmk) = Sλ((PΩ(Y — μλmk-1)Is) + P⊥(wλmk-1)B0))B),
μλmk)
P(i,j)∈Ω∩Ωs (Y - WλmkT)B0)i,j
∣Ω ∩ Ωs|
Taking limits on both sides gives us the desire result.
□
ProofofTheorem 1. Let (μλ, Wλ) be one limit point then we have:
kWλ - wλk)kF = kSλ((PΩ(Y — μλIs) + P⊥(WλB0))B)	(A.8)
— Sλ((PΩ(Y — μλk-1)is) + P⊥(wλk-1)B0))B) kF
≤∣Ω ∩ Ωs∣(μλ — μλk-1))2 + kP⊥((Wλ - wλk-1))B0)kF,	(A.9)
here (A.8) uses Lemma 5 and (A.9) uses Lemma 2. Meanwhile,
∣Ω ∩ Ωs ∣(μλ — μλk-1))2 = P(i，j)iS((Wλ -SWW	)*0舄 ≤ kPΩ((Wλ - WλkT))B')kF ∙
(A.10)
Combining (A.9) and (A.10), we have
kWλ — wλk)kF ≤kWλ — w(k-1)kF.
Hence the sequence k WW 一 WWk) ∣∣f is monotonically decreasing and has a limit. But since there
exists W(mk) converging to WW, the limit equals 0, which proves WWk) → WW, μWk) → μ.
Therefore we have proved the sequence (μWk), WWk)) always converges.
Meanwhile, the first part of equation 4.1 and Lemma 5 in Mazumder et al. (2010) guarantees
~ ∈ ∂wfW(WW, μ). By taking derivative directly we have 0 = ∂*fW(WW, μ). Therefore (WW, μ) is a
stationary point for fW(W, μ). Notice that the loss function fW(W, μ) is a convex function with respect
to (W, μ). Thus we have proved that the limit point (WW, μ) minimizes the function fW(W, μ). □
B Data generation
Let G be the grid of T equidistributed points and let B be the basis of K spline functions evaluated
on grid G. We will simulate the N × K observation matrix Y with three parts
Y = WB + μIs + E,
where W follows a mixture-Gaussian distribution with low rank structure, IS is the treatment matrix
with uniformly distributed starting time and E represents the i.i.d. measurement error. The specific
procedures is described below.
1.	Generating W given parameters κ ∈ (0, 1), r1, r2 ∈ R, s1, s2 ∈ R≥K0:
(a)	Sample two K × K orthogonal matrices V1, V2 via singular-value-decomposing two
random matrix.
(b)	Sample two unit length K vectors ~γ1, ~γ2 via normalizing i.i.d. normal samples.
12
Under review as a conference paper at ICLR 2020
(c)	Draw vector ~t ∈ RN from i.i.d. Bernoulli(κ) samples. Denote the all one vector by ~1.
(d)	Draw N × K matrices U1 , U2 from i.i.d. standard normal random variables.
(e)	Set
W — ~∙ [r1~1 + Ui diag[√S1]V1] +(1 - t) ∙ [r2~2 + U2 diag[√s2]V2],
where diag[s] is the diagonal matrix with diagonal elements s, “∙” represents Coordi-
natewise multiplication, and we are recycling ~t, ~1 - ~t and riγ~i to match the dimension.
2.	Generating IS given parameter ptr ∈ (0, 1).
(a)	For each k = 1, . . . , N, sample Tk uniformly at random from {1, . . . , bT/ptrc}.
(b)	Set IS — (1{j ≥ TiY) 1≤i≤N,1≤j≤τ.
3.	Given parameter ∈ R≥0, E is drawn from from i.i.d. Normal(0, 2) samples.
4.	Given parameter μ ∈ R, let Y0 - WB + μIs + E.
5.	Given parameter P ∈ (0,1), drawn 0-1 matrix IΩ from i.i.d. BernoUlli(P) samples. Let Ω
denote the set of non-zero entries of IΩ, namely, the set of observed data. Set
Y IKj)i≤i≤N,1≤j≤T,	where 匕j = {N0)ij	othIΩwisf 1
In actUal simUlation, we fix the aUxiliary parameters as follows,
K = 7,T= 51,N = 500,
κ = 0.33, r1 = 1, r2 = 2,
s1 = [1, 0.4, 0.005, 0.1 exp(-3), ..., 0.1 exp(-K + 1)],
s2 = [1.3, 0.2, 0.005, 0.1 exp(-3),..., 0.1 exp(-K + 1)],
ptr = 0.8, = 0.5.
The remaining parameters are treatment effect μ and observation rate ρ, which we allow to vary
across different trials.
13