Under review as a conference paper at ICLR 2020
Off-Policy Actor-Critic with
Shared Experience Replay
Anonymous authors
Paper under double-blind review
Ab stract
We investigate the combination of actor-critic reinforcement learning algorithms
with uniform large-scale experience replay and propose solutions for two chal-
lenges: (a) efficient actor-critic learning with experience replay (b) stability of
off-policy learning where agents learn from other agents behaviour. We employ
those insights to accelerate hyper-parameter sweeps in which all participating
agents run concurrently and share their experience via a common replay module.
To this end we analyze the bias-variance tradeoffs in V-trace, a form of importance
sampling for actor-critic methods. Based on our analysis, we then argue for mixing
experience sampled from replay with on-policy experience, and propose a new trust
region scheme that scales effectively to data distributions where V-trace becomes
unstable.
We provide extensive empirical validation of the proposed solution. We further
show the benefits of this setup by demonstrating state-of-the-art data efficiency on
Atari among agents trained up until 200M environment frames.
1 Introduction
Value-based and actor-critic policy gradient methods are the two leading techniques of constructing
general and scalable reinforcement learning agents (Sutton et al., 2018). Both have been combined
with non-linear function approximation (Tesauro, 1995; Williams, 1992), and have achieved remark-
able successes on multiple challenging domains; yet, these algorithms still require large amounts of
data to determine good policies for any new environment. To improve data efficiency, experience
replay agents store experience in a memory buffer (replay) (Lin, 1992), and reuse it multiple times to
perform reinforcement learning updates (Riedmiller, 2005). Experience replay allows to generalize
prioritized sweeping (Moore & Atkeson, 1993) to the non-tabular setting (Schaul et al., 2015), and
can also be used to simplify exploration by including expert (e.g., human) trajectories (Hester et al.,
2017). Overall, experience replay can be very effective at reducing the number of interactions with
the environment otherwise required by deep reinforcement learning algorithms (Schaul et al., 2015).
Replay is often combined with the value-based Q-learning (Mnih et al., 2015), as it is an off-policy
algorithm by construction, and can perform well even if the sampling distribution from replay is
not aligned with the latest agent’s policy. Combining experience replay with actor-critic algorithms
can be harder due to their on-policy nature. Hence, most established actor-critic algorithms with
replay such as (Wang et al., 2017; Gruslys et al., 2018; Haarnoja et al., 2018) employ and maintain
Q-functions to learn from the replayed off-policy experience.
In this paper, we demonstrate that off-policy actor-critic learning with experience replay can be
achieved without surrogate Q-function approximators using V-trace by employing the following
approaches: a) off-policy replay experience needs to be mixed with a proportion of on-policy
experience. We show experimentally (Figure 2) and theoretically that the V-trace policy gradient is
otherwise not guaranteed to converge to a locally optimal solution. b) a trust region scheme (Conn
et al., 2000; Schulman et al., 2015; 2017) can mitigate bias and enable efficient learning in a
strongly off-policy regime, where distinct agents share experience through a commonly shared replay
module. Sharing experience permits the agents to benefit from parallel exploration (Kretchmar, 2002)
(Figures 1 and 3).
Our paper is structured as follows: In Section 2 we revisit pure importance sampling for actor-critic
agents (Degris et al., 2012) and V-trace, which is notable for allowing to trade off bias and variance
1
Under review as a conference paper at ICLR 2020
in its estimates. We recall that variance reduction is necessary (Figure 4 left) but is biased in V-trace.
We derive proposition 2 stating that off-policy V-trace is not guaranteed to converge to a locally
optimal solution - not even in an idealized scenario when provided with the optimal value function.
Through theoretical analysis (Section 3) and experimental validation (Figure 2) we determine that
mixing on-policy experience into experience replay alleviates the problem. Furthermore we propose
a trust region scheme (Conn et al., 2000; Schulman et al., 2015; 2017) in Section 4 that enables
efficient learning even in a strongly off-policy regime, where distinct agents share the experience
replay module and learn from each others experience. We define the trust region in policy space and
prove that the resulting estimator is correct (i.e. estimates an improved return).
As a result, we present state-of-the-art data efficiency in Section 5 in terms of median human
normalized performance across 57 Atari games (Bellemare et al., 2013), as well as improved learning
efficiency on DMLab30 (Beattie et al., 2016) (Table 1).
IMPALA Meta-Gradient
RAINBOW
REACTOR
≠UUP ①2:UeUJnH
Environment Steps per Agent and Level
IMPALA: No Replay (sweep)
LASER (no sweep)
LASER: Im Replay (sweep)
LASER: 9m Shared Replay (sweep)
Figure 1: Sharing experience between agents leads to more efficient hyper-parameter sweeps on 57
Atari games. Prior art results are presented as horizontal lines (with scores cited from Gruslys et al.
(2018), Hessel et al. (2017) and Mnih et al. (2013)). Note that the only previous agent “R2D2” that
achieved a score beyond 400% required more than 3,000 million environment steps (see Kapturowski
et al. (2019), page 14, Figure 9). We present the pointwise best agent from hyper-parameter sweeps
with and without experience replay (shared and not shared). Each sweep contains 9 agents with
different learning rate and entropy cost combinations. Replay experiment were repeated twice and
ran for 50M steps. To report scores at 200M we ran the baseline and one shared experience replay
agent for 200M steps.
Table 1: Comparison of state-of-the-art agents on 57 Atari games trained up until 200M environment
steps (per game) and DMLab-30 trained until 10B steps (multi-task; all games combined). The first
two rows are quoted from Xu et al. (2018) and Hessel et al. (2019), the third is our implementation of
a pixel control agent from Hessel et al. (2019) and the last two rows are our proposed LASER (LArge
Scale Experience Replay) agent. All agents use hyper-parameter sweeps expect for the marked.
Atari Median DMLab-30 Median DMLab-30 Mean-Capped
IMPALA Meta-Gradient (no sweep)	287.6% at 200M	-	-
PopArt-IMPALA	-	-	73.5%
PopArt-IMPALA+PixelControl	-	85.5%	77.6%
LASER: Experience Replay (no sweep)	431% at 200M		
LASER: Experience Replay	(233% at 50M)	95.4%	79.6%
LASER: Shared Experience Replay	(370% at 50M), ∕t∕t δθZ Ct o∩∩κ∕r	97.2%	81.7%
448% at 200M
2
Under review as a conference paper at ICLR 2020
2	The Issue with Importance Sampling: Bias and Variance in
V-trace
V-trace importance sampling is a popular off-policy correction for actor-critic agents (Espeholt et al.,
2018). In this section we revisit how V-trace controls the (potentially infinite) variance that arises
from naive importance sampling. We note that this comes at the cost of a biased estimate (see
Proposition 1) and creates a failure mode (see Proposition 2) which makes the policy gradient biased.
We discuss our solutions for said issues in Section 4.
Figure 2: Left: Learning entirely off-policy from experience replay fails, while combining on-policy
data with experience replay leads to improved data efficiency: We present sweeps on DMLab-30 with
experience replays of 10M capacity. A ratio of 87.5% implies that there are 7 replayed transitions in
the batch for each online transition. Furthermore we consider an agent identical to “LASER 87.5%
replay” which however draws all samples from replay. Its batch thus does not contain any online data
and we observe a significant performance decrease (see Proposition 2 and 3). The shading represents
the point-wise best and worst replica among 3 repetitions. The solid line is the mean. Right: The
effect of capacity in experience replay with 87.5% replay data per batch on sweeps on DMLab-30.
Data-efficiency improves with larger capacity.
Hyperparameter sweeps:
experience replay of various capacity
UCPBIAi
Environment Steps
Baseline: no replay — LASER: 3M replay
LASER: IOM replay
Hyperparameter sweeps:
shared experience replay
PBT sweeps:
shared experience replay
Baseline： no replay
LASER: not shared
Baseline： no replay
LASER: not shared
Naively shared
LASER: shared +
trust region
Naively shared
LASER: shared +
trust region
Figure 3: Left: Naively sharing experience between distinct agents in a hyper-parameter sweep fails
(green) and is worse than the no-replay baseline (blue). The proposed trust region estimator mitigates
the issue (red). Right: Combining population based training with trust region estimation improves
performance further. All replay experiments use a capacity of 10 million observations and 87.5%
replay data per batch.
2.1	Reinforcement Learning
We follow the notation of Sutton et al. (2018) where an agent interacts with its environment, to collect
rewards. On each discrete time-step t, the agent selects an action at ; it receives in return a reward
rt and an observation ot+1, encoding a partial view of the environment’s state st+1. In the fully
3
Under review as a conference paper at ICLR 2020
observable case, the RL problem is formalized as a Markov Decision Process (Bellman, 1957): a
tuple (S, A,p, γ), where S, A denotes finite sets of states and actions, p models rewards and state
transitions (So that rγ, st+ι 〜p(st, at)), and Y is a fixed discount factor. A PoUCy is a mapping
∏(a∣s) from states to action probabilities. The agent seeks an optimal policy ∏* that maximizes the
value, defined as the expectation of the cumulative discounted returns Gt = Pk∞=0 γkrt+k.
Off-policy learning is the problem of finding, or evaluating, a policy π from data generated by a
different policy μ. This arises in several settings. Experience replay (Lin, 1992) mixes data from
multiple iterations of policy improvement. In large-scale RL, decoupling acting from learning (Nair
et al., 2015; Horgan et al., 2018; Espeholt et al., 2018) causes the experience to lag behind the latest
agent policy. Finally, it is often useful to learn multiple general value functions (Sutton et al., 2011;
Mankowitz et al., 2018; Lample & Chaplot, 2016; Mirowski et al., 2017; Jaderberg et al., 2017b) or
options (Sutton et al., 1999; Bacon et al., 2017) from a single stream of experience.
2.2	Naive Importance Sampling
On-policy n-step bootstraps give more accurate value estimates in expectation with larger n (Sutton
et al., 2018). They are used in many reinforcement learning agents (Mnih et al., 2016; Schulman
et al., 2017; Hessel et al., 2017). Unfortunately n must be chosen suitably as the estimates variance
increases with n too.
It is desirable to obtain benefits akin to n-step returns in the off-policy case. To this end multi-step
importance sampling (Kahn, 1955) can be used. This however adds another source of (potentially
infinite (Sutton et al., 2018)) variance to the estimate.
Importance sampling can estimate the expected return Vπ from trajectories sampled from μ = ∏,
as long as μ is non-zero whereever ∏ is. We employ a previously estimated value function V as a
bootstrap to estimate expected returns. Following Degris et al. (2012), a multi-step formulation of the
expected return is
K-1	k
Vπ(st) = Eμ V(St) + XY k(γ *) δt+kV	(1)
_	k=0	i=0 μt+i	_
where Eμ denotes the expectation under policy μ up to an episode termination, δtV = r +
YV(st+ι) - V(St) is the temporal difference error in consecutive states st+ι, st, and ∏t = ∏t(at∣st).
Importance sampling estimates can have high variance. Tree Backup (Precup et al., 2000), and Q(λ)
(Sutton et al., 2014) address this, but reduce the number of steps before bootstrapping even when this
is undesirable (as in the on-policy case). RETRACE (Munos et al., 2016) makes use of full returns in
the on-policy case, but it introduces a zero-mean random variable at each step, adding variance to
empirical estimates in both on- and off-policy cases.
2.3 Bias-Variance Analysis & Failure Mode of V-trace Importance Sampling
V-trace (Espeholt et al., 2018) reduces the variance of importance sampling by trading off variance for
a biased estimate of the return - resulting in a failure mode (see Proposition 2). It uses clipped impor-
tance sampling ratios to approximate Vπ by Vπ (St) = V (st)+PK-11 Yk (Qk=CIci) ρtδt+k V where
V is a learned state value estimate used to bootstrap, and Pt = min [∏t∕μt, ρ], ct = min [∏t∕μt, c]
are the clipped importance ratios. Note that, differently from RETRACE, V-trace fully recovers the
Monte Carlo return when on policy. It similarly reweights the policy gradient as:
VVπ(St) def Eμ [ρtV(log∏t)(rt + γVπ⑶+。)]	⑵
Note that VVπ(St) recovers the naively importance sampled policy gradient for P → ∞. In the
literature, it is common to subtract a baseline from the action-value estimate r + γVπ(St+ι) to
reduce variance (Williams, 1992), omitted here for simplicity. The constants Pc ≥ cc ≥ 1 (typically
chosen Pc = cc = 1) define the level of clipping, and improve stability by ensuring a bounded variance.
For any given Pc, the bias introduced by V-trace in the value and policy gradient estimates increases
with the difference between ∏ and μ. We analyze this in the following propositions.
Proposition 1. The V-tra.ce value estimate Vπ is biased: It does not match the expected return of
π but the return ofa related implied policy π defined by equation 3 that depends on the behaviour
4
Under review as a conference paper at ICLR 2020
policy μ:
π ( | ) _	min[pμ(a∣x),π(a∣x)]
μμ aX 一 Pb∈A min[ρμ(Mx),π(Mx)]
(3)
Proof. SeeEsPeholtetal. (2018).	□
Note that the biased policy ∏μ can be Very different from π. Hence the V-trace value estimate Vπ
may be very different from Vπ as well. As an illustrative examPle, consider two Policies over a
set of two actions, e.g. “left” and “right” represented as a tuple of probabilities. Let us investigate
μ = (φ, 1 - φ) and π = (1 - φ,φ) defined for any suitably small φ ≤ 1. Observe that π and μ share
no trajectories (state-action sequences) in the limit as φ → 0 and they get more focused on one action.
A practical example of this could be two policies, one almost always taking a left turn and one always
taking the right. Given sufficient data of either policy it is possible to estimate the value of the other
e.g. with naive importance sampling. However observe that V-trace with ρ= 1 will always estimate a
biased value - even given infinite data. Observe that min [μ(a∣x), π(a∣x)] = min [φ, 1 - φ] for both
actions. Thus ∏μ is uniform rather than resembling π the policy. The V-trace estimate Vπ would thus
compute the average value of "left" and "right" 一 poorly representing the true Vπ.
Proposition 2. The V-trace policy gradient is biased: given the the optimal value function V *
the V-trace policy gradient does not converge to a locally optimal π* for all off-policy behaviour
distributions μ.
Proof. See Appendix C.
3 Mixing On- and Off-Policy Experience
In Proposition 2 we presented a failure mode in V-trace where the variance reduction biases the
value estimate and policy gradient. V-trace computes biased Q-estimates Qω 6= Q resulting in a
wrong local policy gradient: VE∏(a∣s) [Qω(s, a)] = VE∏(a∣s) [Q(s, a)]. In equation 10 we show
that Qω(s, a) = Q(s, a)ω(s, a) where ω(s, a) = min [1,0,(：|：)] ≤ 1.
The question of how biased the resulting policy will be depends on whether the distortion changes
the argmax of the Q-function. Little distortions that do not change the argmax will result in the same
local fixpoint of the policy improvement. The policy will continue to select the optimal action and it
will not be biased at this state. The policy will however be biased if the Q-function is distorted too
much. For example consider a ω(s, a) that swaps the argmax for the 2nd largest value, the regret will
then be the difference between the maximum and the 2nd largest value. Intuitively speaking the more
distorted the Qω, the larger will be the regret compared to the optimal policy.
More precisely, the regret of learning a policy that maximizes the distorted Qω at state s is:
R(s) = Q(s, a ) - Q(s, aactual) = max Q(s, b) - Q(s, aactual)
b
where a* = argmaxb (Q, b) is the optimal action according to the real Q and aactual =
argmax[Qω(s, a)] = argmax[Q(s, a)ω(s, a)], is the optimal action according to the distorted Qω.
For generality, we denote A* as the set of best actions - covering the case with multiple with identical
optimal Q-values.
Proposition 3 provides a mitigation: Clearly the V-trace policy gradient will converge to the same
solution as the true on-policy gradient if the argmax of the Q-function is preserved at all states in a
tabular setting. We show that this can be achieved by mixing a sufficient proportion α of on-policy
experience into the computation.
We show in equation 13 in the Appendix that choosing α such that
α	jQω(s,b) - Qω(s,a*)
1 — α > b∈A^	Q(s, a*) - Q(s, b)
dμ(s)
dπ (S)
for Qω (s, a) = Q(s, a)ω(s, a)
will result in a policy that correctly chooses the best action at state s. Note that ι-αα → ∞ as α → 1.
5
Under review as a conference paper at ICLR 2020
Intuitively: the larger the action value gap of the real Q-function Q(s, a*) - Q(s, b) the lower the
right hand side and the less on-policy data is required. If maxb[(Q(s, b)ω(s, b) - Q(s, a*)ω(s, a*)]
is negative, then α may be as small as zero and we enabling even pure off-policy learning. Finally
note that the right hand side decreases due to dμ(s)∕dπ (S) if ∏ visits the state S more often than μ.
All of those conditions can be computed and checked if an accurate Q-function and state distribution
is accessible. How to use imperfect Q-function estimates to adaptively choose such an α remain a
question for future research.
We provide experimental evidence for these results with function approximators in the 3-dimensional
simulated environment DMLab-30 with various α ≥ 1/8 in Section 5.3 and Figure 2. We observe
that α = 1/8 is sufficient to facilitate stable learning. Furthermore it results in better data-efficiency
than pure on-policy learning as it utilizes off-policy replay experience.
Proposition 3. Mixing on-policy data into the V-trace policy gradient with the ratio α reduces the bias
by providing a regularization to the implied state-action values. In the general function approximation
case it changes the off-policy V-tra.ce policy gradient from Es dμ (s)E∏ [(Q(s, a)V log π(a∣s)] to
Ps E∏ [Qα(s,a)Vlog∏(a∣s)] where Qa = Qd^π(s)α + Qωdμ(s)(1 — α) is a regularized state-
action estimate and dπ, dμ are the state distribUtiOnSfOr π and μ. Note that there exists α ≤ 1 such
that Qα has the same argmax (i.e. best action) as Q.
Proof. See Appendix C.
Mixing online data with replay data has also been argued for by Zhang & Sutton (2017), as a heuristic
way of reducing the sensitivity of reinforcement learning algorithms to the size of the replay memory.
Proposition 3 grounds this in the theoretical properties of V-trace.
4 Trust Region S cheme for Off-Policy V-trace
To mitigate the bias and variance problem of V-trace and importance sampling we propose a trust
region scheme that adaptively selects only suitable behaviour distributions when estimating the
state-value of π. To this end we introduce a behaviour relevance function that classifies behaviour
as relevant. We then define a trust-region estimator that computes expectations (such as expected
returns, or the policy gradient) only on relevant transitions. In proposition 4 and 5 we show that this
trust region estimator indeed computes new state-value estimates that improve over the current value
function. While our analysis and proof is general we propose a suitable behaviour relevance function
in section 4.3 that employs the Kullback Leibler divergence between target policy π and implied
policy ∏μ: KL (∏(∙∣s)∣∣Πμ(∙∣s)). We provide experimental validation in Figure 3.
4.1	Behaviour Relevance Functions
In off-policy learning we often consider a family of behaviour policies either indexed by training
iteration t: MT = {μt |t < T} for experience replay, or by a different agent k: MK = {μk |k ∈ K}
when training multiple agents. In the classic experience replay case we then sample a time t and
locate the transition T that was generated earlier via μt. This extends naturally to the multiple agent
case where we sample an agent index k and then obtain a transition for such agent or tuples of
(k, t). Without loss of generality we simplify this notation and index sampled behaviour policies by a
random variable Z 〜Z that represents the selection process. While online reinforcement learning
algorithms process transitions T 〜π, off-policy algorithms process T 〜μz for Z 〜Z. In this
notation, given equation (1) and a bootstrap V , the expectation of importance sampled off-policy
returns at state St is described by:
VmUSt) = Ez[Eμz∣z[Gπ,μz(st)]]	(4)
where Gπ,μ(st) = V (st) + P∞=° Yk ( Qk=o μt+i) δt+k V is a single importance sampled return.
Note that the on-policy return Gπ,π (St) = V (St) + Pk∞=0 γkrt+k.
Above Eμz∣z represents the expectation of sampling from a given μz. The conditioning on Z is a
notational reminder that this expectation does not sample Z or μz but experience from μz. For any
6
Under review as a conference paper at ICLR 2020
sampled Z We obtain a μz and observe that the inner expectation wrt. experience of μz in equation (4)
recovers the expected on-policy return in expectation:
∞k
Eμz∣z [Gπ,μz(st)] = Eμz∣z V (st) + X Y k( Y 户)汨k V
_	k=0 i=0 μz,t+i	_
∞k
=E∏ V (st) + X γk ( Y μ^+i)δt+k V	(5)
_	k=0	i=0 μz,t+i	_
∞
=Eπ V(st)+Xγkrt+k =Eπ [Gπ,π(st)] =Vπ(st)
k=0
Thus VmniX(St) = Ez [E∏ [Gπ,π(st)]] = E∏ [Gπ,π(st)] = Vπ(st). This holds provided that μz is
non-zero Wherever π is. This fairly standard assumption leads us straight to the core of the problem: it
may be that some behaviours μz are ill-suited for estimating the inner expectation. However, standard
importance sampling applied to very off-policy experience divides by small μ resulting in high or
even infinite variance. Similarly, V-trace attempts to compute an estimate of the return following π
resulting in limited variance at the cost of a biased estimate in turn.
The key idea of our proposed solution is to compute the return estimate for π at each state only from
a subset of suitable behaviours μz:
Mβ,∏(s) = {μz|z ∈ Z and β(π, μ, S) < b}
as determined by a behaviour relevance function β (π, μ, S) : (MZ, MZ, S) → R and a threshold b.
The behaviour relevance function decides if experience from a behaviour is suitable to compute an
expected return for π. It can be chosen to control properties of VmπiX by restricting the expectation on
subsets of Z. In particular it can be used to control the variance of an importance sampled estimator:
Observe that the inner expectation Eμz [Gπ,μ(st)∣z] in equation (4) already matches the expected
return Vπ . Thus we can condition the expectation on arbitrary subsets of Z without changing the
expected value of Vmiχ. This allows us to reject high variance Gπ,μ without introducing a bias in
VmπiX . The same technique can be applied to V-trace where we can reject return estimates with high
bias.
4.2	Derivation of Trust Region Estimators
Using a behaviour relevance function β(S) we can define a trust region estimator for regular impor-
tance sampling (IS) and V-trace and show their correctness.
We define the trust region estimator as the conditional expectation
Vnusted(St) = Ez [Eμz∣z [Gπ,μz,β(st)]∣μz ∈ Mβ,∏(st)i	(6)
with λ-returns G, chosen as GIS for importance sampling and GVtrace for V-trace:
∞k
GnSr (st) = V (st)+ X Yk (Y λ∏,μz (st+i) πt+^) δt+k V	(7)
k=0	i=0	μz,t+i
∞	k-1
GV,μzace(st) = V(St)+ X Yk(Yλπ,μz (st+i)cz,t+i) λn,μz(St+k ) ρz,t+k δt+k V	(8)
k=0	i=0
where λ∏,*(st) is designed to constraint Monte-Carlo bootstraps to relevant behaviour: λ∏,*(st)=
lβ(∏,μ,st)<b and ρz,t+k = min [jt+i., ρ] and cz,t+k are behaviour dependent clipped importance
rations. Thus both G∏,“z and GVtXe are a multi-step return estimators with adaptive length. Note
that only estimators with length ≥ 1 are used in Vtnrusted. Due to Minkowski’s inequality the trust
region estimator thus shows at least the same contraction as a 1-step bootstrap, but can be faster due
to its adaptive nature:
Proposition 4. Let G∏S,μz be a set of importance sampling estimators as defined in equation 7. Note
that they all have the same fix point Vn and contract with at least Y. Then the contraction properties
Carry over to Vnusted ∙ Inparticular lVnusted - V n l∞ ≤ Y |V - V n l∞.
7
Under review as a conference paper at ICLR 2020
Proof. See Appendix C.
Proposition 5. Let GVt^ce be a Set ofV-trace estimators (see equation 8) With corresponding fixed
points V z (see equation 3) to which they contract at a speed of an algorithm and behaviour specific
%. Then Vnusted moves towards Vβ = Ez∣μz∈Mβ π(st) [Vz] shrinking the distance as follows
IVnUsted- V β∣∞ < maxμz ∈Mβ,∏ (St) Inz(V - V Z 限 ≤ ηmax maxμz ∈Mβ,∏ (St) I(V - V Z h With
ηmax =maxμz ∈Mβ,∏ (St) nz.
Proof. See Appendix C.
Note how the choice of β and thus Mβ,∏ enables Us to discard ill-suited G^^^ce from the esti-
mation of Vtnrusted . Recall that V-trace fixed points Vz are biased. Thus β allows us to selec-
tively create the V-trace target Vβ = Ez∣μz∈Mβ,π(St) [Vz] and control its bias and the shrinkage
maxμz∈Mβ π(st) |nz(V(S) - VZ(s))∣∞ (see Proposition 5). Similarly it can control cases where we
can not use the exact importance sampled estimator. The same approach based on nested expectations
can be applied to the expectation of the policy gradient estimate and allows to control the bias and
greediness (see Proposition 2) there as well.
4.3 Implementation Details
In Proposition 5 we have seen that the quality of the trust region V-trace return estimator depends on
β . A suitable choice of β can move the return estimate Vβ closer to Vn and improve the shrinkage by
reducing max*z∈Mβ π(St) ∣nz(V(s) - VZ(s))∣∞. Hence, we employ a behaviour relevance function
βKL that rejects high,bias transitions by estimating the Kulback-Leibler divergence between the target
policy ∏ and the implied policy ∏μz for a sampled behaviour μz. Recall from Proposition 1 that ∏μz
determines the fixed point of the V-trace estimator for behaviour μz and thus determines the bias in
VZ.
βKL(π,μ, s) = KL (n(1s)||n“(1s))
Note that the behaviour probabilities μz can be evaluated and saved to the replay when the agent
executes the behaviour, similarly the target policy π is represented by the agents neural network.
Using both and equation 3, ∏μ can be computed. For large or infinite action spaces a Monte Carlo
estimate of the KL divergence can be computed.
It is possible to define separate behaviour relevance functions for the policy and value estimate. For
simplicity we reject transitions entirely for all estimates and do not consider rejected transitions for
the policy gradient and value gradient updates or auxiliary tasks. As described above we stop the
Monte-Carlo bootstraps once they reach undesirable state-behaviour pairs. Note that this censoring
procedure is computed from state dependent β(∏,μ,s) and ensures that the choice of bootstrapping
does not depend on the sampled actions. Note that rejection by an action-based criteria such as small
π(a∣ s)∕μ(a∣ s) would introduce an additional bias which we avoid by choosing βκL.
5 Experiments
We present experiments to support the following claims:
•	Section 5.2: Uniform experience replay obtains comparable results as prioritized experience
replay, while being simpler to implement and tune.
•	Section 5.3: Using fresh experience before inserting it in experience replay is better than
learning purely off-policy from experience replay - in line with Proposition 3.
•	Section 5.4: Sharing experience without trust region performs poorly as suggested by
Proposition 2. Off-Policy Trust-Region V-trace solves this issue.
• Section 5.5: Sharing experience can take advantage of parallel exploration and obtains
state-of-the-art performance on Atari games, while also saving memory through sharing a
single experience replay.
8
Under review as a conference paper at ICLR 2020
5.1	Experimental Setup & Methodology
We use the V-trace distributed reinforcement learning agent (Espeholt et al., 2018) as our baseline. In
our experiments we consider two experimental platforms: Atari and DeepMind Lab. On Atari we
consider the common single task training regime, where a different agent is trained, from scratch, on
each of the tasks. Following Xu et al. (2018) we use a discount of 0.995. Motivated by recent work
by Kaiser et al. (2019), we use the IMPALA deep network and increased the number of channels
4×. We use 96% replay data per batch. Differently from Espeholt et al. (2018), we do not use
gradient clipping by norm (Pascanu et al., 2012). Updates are computed on mini-batches of 32
(regular) and 128 (replay) trajectories, each corresponding to 19 steps in the environment. In the
context of DeepMind Lab, we consider the multi-task suite DMLab-30 (Espeholt et al., 2018), as
the visuals and the dynamics are more consistent across tasks. Furthermore the multi-task regime
is particularly suitable for the investigation of strongly off-policy data distributions arising from
sharing the replay across agents, as concurrently learning agents can easily be stuck in different
policy plateaus, generating substantially different data (Schaul et al., 2019). As in Espeholt et al.
(2018), in the multi-task setting each agent trains simultaneously on a uniform mixture of all tasks
rather than individually on each game. The score of an agent is thus the median across all 30 tasks.
Following Hessel et al. (2019), we augment our agent with multi-task Pop-Art normalization and
PixelControl. We use a PreCo LSTM (Amos et al., 2018) instead of the vanilla one (Hochreiter &
Schmidhuber, 1997). Updates are computed on mini-batches of multiple trajectories chosen as above,
each corresponding to 79 steps in the environment. In early experiments we found that computing
the entropy cost only on the online data provided slightly better results, hence we have done so
throughout our experiments.
In all our experiments, experience sampled from memory is mixed with online data within each mini-
batch - following Proposition 3. Episodes are removed in a first in first out order, so that replay always
holds the most recent experience. Unless explicitly stated otherwise we consider hyper-parameter
sweeps, some of which share experience via replay. In this setting multiple agents start from-scratch,
run concurrently at identical speed, and add their new experience into a common replay buffer. All
agents will then draw uniform samples from the replay buffer. On DMLab-30 we consider both regular
hyper-parameter sweeps and sweeps with population based training (PBT) (Jaderberg et al., 2017a).
On DMLab-30 sweeps contain 10 agents with hyper-parameters sampled similar as Espeholt et al.
(2018) but fixed RMSProp = 0.1. On Atari sweeps contain 9 agents with different constant learning
rate and entropy cost combinations {3 ∙ 10-4,6 ∙ 1θ-4,1.2 ∙ 10-3} X {5 ∙ 10-3,1 ∙ 10-2,2 ∙ 10-2}
(distributed by factors {1/2, 1, 2} around the initial parameters reported in Espeholt et al. (2018)).
Although our focus is on efficient hyper-parameter sweeps given crude initial parameters, we also
present a single-agent LASER experiment using the same tuned schedule as Espeholt et al. (2018), a
87.5% replay ratio and a 15M replay. We store the entire episodes in the replay buffer and replay
each episode from the beginning, using the most recent network parameters to recompute the LSTM
states along the way: this is particularly critical when sharing experience between different agents,
which may have arbitrarily different state representations.
5.2	Uniform and Prioritized Experience Replay
Prioritized experience replay has the potential to provide more efficient learning compared to uniform
experience replay (Schaul et al., 2015; Horgan et al., 2018). However, it also introduces a number
of new hyper-parameters and design choices: the most critical are the priority metric, how strongly
to bias the sampling distribution, and how to correct for the resulting bias. Uniform replay is
instead almost parameter-free, requires little tuning and can be easily shared between multiple agents.
Experiments provided in Figure 4 in the appendix showed little benefit of actor critic prioritized
replay on DMLab-30. Furthermore priorities are typically computed from the agent specific metrics
such as the TD-error, which are ill-defined when replay is shared among multiple agents. Hence we
used uniform replay for our further investigations.
5.3	Mixing On- and Off-policy Experience and Replay Capacity
Figure 2 (left) shows that performance degrades significantly when online data is not present in the
batch. This experimentally validates Propositions 2 and 3 that highlight difficulties of learning purely
off-policy. Furthermore Figure 2 (right) shows that best results are obtained with experience replay
9
Under review as a conference paper at ICLR 2020
of 10M capacity and 87.5% ratio. A ratio of 87.5% = 7/8 corresponds to 7 replay samples for each
online sample. We have considered ratios of 1/2, 3/4, and 7/8 and observed stable training for all of
them. Observe that among those values, larger ratios are more data-efficient as they take advantage of
more replayed experience per training step.
5.4	Shared Experience Replay with Off-Policy Trust Region V-trace
In line with proposition 2 we observe in Figure 3 (left) that hyper-parameter sweeps without trust-
region are even surpassed by the baseline without experience replay. State-of-the-art results are
obtained in Figure 3 (right) when experience is shared with trust-region in a PBT sweep.
Observe that this indicates parallel exploration benefits and saves memory at the same time: in our
sweep of 10 replay agents the difference between 10 × 10M (separate replays) and 10M (shared
replay) is 10-fold. This effect would be even more pronounced with larger sweeps.
As discussed in section 2.3, the bias in V-trace occurs due to the clipping of importance ratios. A
potential solution of reducing the bias would be to increase the P threshold to clip less aggressively
and accept increased variance. Figure 4 in the appendix shows that this is not a solution.
5.5	Evaluation on Atari
We apply our proposed agent to Atari which has been a long established suite to evaluate reinforcement
algorithms (Bellemare et al., 2013). Since we focus on sample-efficient learning we present our
results in comparison to prior work at 200M steps (Figure 1). Shared experience replay obtains even
better performance than not shared experience replay. This confirms the efficient use of parallel
exploration (Kretchmar, 2002). The fastest prior agent to reach 400% is presented by Kapturowski
et al. (2019) requiring more than 3,000M steps. LASER with shared replay achieves 423% in 60M
per agent. Given 200M steps it achieves 448%. We also present a single (no sweep) LASER agent
that achieves 431% in 200M steps.
6 Conclusion
We have presented LASER - an off-policy actor-critic agent which employs a large and shared
experience replay to achieve data-efficiency. By sharing experience between concurrently running
experiments in a hyper-parameter sweep it is able to take advantage of parallel exploration. As a
result it achieves state-of-the-art data efficiency on 57 Atari games given 200M environment steps.
Furthermore it achieves competitive results on both DMLab-30 and Atari under regular, not shared
experience replay conditions.
To facilitate this algorithm we have proposed two approaches: a) mixing replayed experience and
on-policy data and b) a trust region scheme. We have shown theoretically and demonstrated through
a series of experiments that they enable learning in strongly off-policy settings, which present a
challenge for conventional importance sampling schemes.
References
Brandon Amos, Laurent Dinh, Serkan Cabi, Thomas Rothorl, Sergio Gomez Colmenarejo, Alistair
Muldal, Tom Erez, Yuval Tassa, Nando de Freitas, and Misha Denil. Learning awareness models.
In ICLR, 2018.
Pierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic architecture. AAAI, 2017.
Charles Beattie, Joel Z. Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich Kuttler,
Andrew Lefrancq, Simon Green, ViCtOr Vald6s, Amir Sadik, Julian Schrittwieser, Keith Anderson,
Sarah York, Max Cant, Adam Cain, Adrian Bolton, Stephen Gaffney, Helen King, Demis Hassabis,
Shane Legg, and Stig Petersen. Deepmind lab. Arxiv, abs/1612.03801, 2016.
Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The Arcade Learning
Environment: An evaluation platform for general agents. JAIR, 2013.
10
Under review as a conference paper at ICLR 2020
Richard Bellman. A markovian decision process. Journal of Mathematics and Mechanics, 1957.
Andrew R. Conn, Nicholas I. M. Gould, and Philippe L. Toint. Trust-Region Methods. SIAM,
Philadelphia, PA, USA, 2000.
Thomas Degris, Martha White, and Richard S. Sutton. Off-policy actor-critic. In ICML, 2012.
Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom Ward, Yotam
Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and Koray Kavukcuoglu. Impala:
Scalable distributed deep-rl with importance weighted actor-learner architectures. In ICML, 2018.
Audrunas Gruslys, Will Dabney, Mohammad Gheshlaghi Azar, Bilal Piot, Marc Bellemare, and Remi
Munos. The reactor: A fast and sample-efficient actor-critic agent for reinforcement learning. In
ICLR, 2018.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In ICML, 2018.
Matteo Hessel, Joseph Modayil, Hado van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney,
Daniel Horgan, Bilal Piot, Mohammad Gheshlaghi Azar, and David Silver. Rainbow: Combining
improvements in deep reinforcement learning. Arxiv, abs/1710.02298, 2017.
Matteo Hessel, Hubert Soyer, Lasse Espeholt, Wojciech Czarnecki, Simon Schmitt, and Hado van
Hasselt. Multi-task deep reinforcement learning with popart. In AAAI, 2019.
Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom Schaul, Bilal Piot, Andrew
Sendonaris, Gabriel Dulac-Arnold, Ian Osband, John Agapiou, Joel Z. Leibo, and Audrunas
Gruslys. Learning from demonstrations for real world reinforcement learning. In AAAI, 2017.
SePP Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Comput., 9(8):1735-
1780, November 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735.
Dan Horgan, John Quan, David Budden, Gabriel Barth-Maron, Matteo Hessel, Hado van Hasselt,
and David Silver. Distributed Prioritized exPerience rePlay. In ICLR, 2018.
Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M. Czarnecki, Jeff Donahue, Ali
Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen Simonyan, Chrisantha Fernando, and
Koray Kavukcuoglu. PoPulation based training of neural networks. Arxiv, abs/1711.09846, 2017a.
Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo, David
Silver, and Koray Kavukcuoglu. Reinforcement learning with unsuPervised auxiliary tasks. ICLR,
2017b.
Herman Kahn. Use of different monte carlo samPling techniques. Santa Monica, Calif., 1955.
Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H. CamPbell, Konrad
Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, Ryan SePassi,
George Tucker, and Henryk Michalewski. Model-based reinforcement learning for atari. Arxiv,
abs/1903.00374, 2019.
Steven KaPturowski, Georg Ostrovski, John Quan, Remi Munos, and Will Dabney. Recurrent
exPerience rePlay in distributed reinforcement learning. In ICLR, 2019.
R.M Kretchmar. Parallel reinforcement learning. SCI2002. The 6th World Conference on Systemics,
Cybernetics, and Informatics. Orlando, FL, 2002.
Guillaume LamPle and Devendra Singh ChaPlot. Playing FPS games with deeP reinforcement
learning. In AAAI, 2016.
Long-Ji Lin. Reinforcement Learning for Robots Using Neural Networks. PhD thesis, Pittsburgh, PA,
USA, 1992. UMI Order No. GAX93-22750.
Daniel J. Mankowitz, Augustin Zidek, Andre Barreto, Dan Horgan, Matteo Hessel, John Quan,
Junhyuk Oh, Hado van Hasselt, David Silver, and Tom Schaul. Unicorn: Continual learning with a
universal, off-Policy agent. Arxiv, abs/1802.08294, 2018.
11
Under review as a conference paper at ICLR 2020
Piotr Mirowski, Razvan Pascanu, Fabio Viola, Hubert Soyer, Andrew J. Ballard, Andrea Banino,
Misha Denil, Ross Goroshin, Laurent Sifre, Koray Kavukcuoglu, Dharshan Kumaran, and Raia
Hadsell. Learning to navigate in complex environments. In ICLR, 2017.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan
Wierstra, and Martin A. Riedmiller. Playing atari with deep reinforcement learning. Arxiv,
abs/1312.5602, 2013.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control
through deep reinforcement learning. Nature, 518(7540):529-533, 2015.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In ICML, 2016.
Andrew Moore and C. G. Atkeson. Prioritized sweeping: Reinforcement learning with less data and
less real time. Machine Learning, 13, October 1993.
Remi Munos, Tom Stepleton, Anna Harutyunyan, and Marc Bellemare. Safe and efficient off-policy
reinforcement learning. In NIPS. 2016.
Arun Nair, Praveen Srinivasan, Sam Blackwell, Cagdas Alcicek, Rory Fearon, Alessandro De Maria,
Vedavyas Panneershelvam, Mustafa Suleyman, Charles Beattie, Stig Petersen, Shane Legg,
Volodymyr Mnih, Koray Kavukcuoglu, and David Silver. Massively parallel methods for deep
reinforcement learning. Arxiv, abs/1507.04296, 2015.
David S. Olton. Mazes, maps, and memory. American Psychologist, 1979.
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. Understanding the exploding gradient problem.
Arxiv, abs/1211.5063, 2012.
Doina Precup, Richard S. Sutton, and Satinder P. Singh. Eligibility traces for off-policy policy
evaluation. In ICML, 2000.
Martin Riedmiller. Neural fitted q iteration - first experiences with a data efficient neural reinforce-
ment learning method. In ECML, 2005.
Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. In
ICLR, 2015.
Tom Schaul, Diana Borsa, Joseph Modayil, and Razvan Pascanu. Ray interference: a source of
plateaus in deep reinforcement learning. 2019.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In Francis Bach and David Blei (eds.), Proceedings of the 32nd International
Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp.
1889-1897, Lille, France, 07-09 Jul 2015. PMLR.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. Arxiv, abs/1707.06347, 2017.
Richard S. Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A framework
for temporal abstraction in reinforcement learning. Artif. Intell., 112(1-2):181-211, August 1999.
ISSN 0004-3702. doi: 10.1016/S0004-3702(99)00052-1.
Richard S. Sutton, Joseph Modayil, Michael Delp, Thomas Degris, Patrick M. Pilarski, Adam
White, and Doina Precup. Horde: A scalable real-time architecture for learning knowledge
from unsupervised sensorimotor interaction. Proc. of 10th Int. Conf. on Autonomous Agents and
Multiagent Systems (AAMAS 2011), 2011.
Richard S. Sutton, Ashique Rupam Mahmood, Doina Precup, and Hado van Hasselt. A new q(λ)
with interim forward view and Monte Carlo equivalence. In ICML, 2014.
12
Under review as a conference paper at ICLR 2020
Richard S. Sutton, Andrew G. Barto, and Francis Bach. Reinforcement Learning: An Introduction.
MIT press, 2018.
Gerald Tesauro. Temporal difference learning and td-gammon. Commun. ACM, 38(3):58-68, March
1995. ISSN 0001-0782. doi: 10.1145/203330.203343.
Edward C. Tolman. Cognitive maps in rats and men. The Psychological Review, 1948.
Hado van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double Q-
learning. In AAAI, 2016.
Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray Kavukcuoglu, and
Nando de Freitas. Sample efficient actor-critic with experience replay. In ICLR, 2017.
Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning.
ML, 1992.
Zhongwen Xu, Hado P van Hasselt, and David Silver. Meta-gradient reinforcement learning. In
NIPS. 2018.
Shangtong Zhang and Richard S. Sutton. A deeper look at experience replay. Arxiv, abs/1712.01275,
2017.
Appendix A	Additional Experiments
A.1 Reduced Clipping in V-trace does not enable Shared Experience Replay
Increasing the clipping constant P in V-trace reduces bias in favour of increased variance. We
investigate if reducing bias in this manner enables sharing experience replay between multiple agents
in a hyper-parameter sweep. Figure 4 (left) shows that this is not a solution, thus motivating our trust
region scheme.
Figure 4: Left: Increasing the V-trace clipping constant P does not enable shared experience replay.
In fact sharing experience replay in this particular way is worse than pure online learning. This
motivates the use of our proposed trust region scheme. On a side note, increased clipping thresholds
resulting in worse performance verifies the importance of variance reduction through clipping. Right:
Median human normalized performance across 30 tasks for the best agent in a sweep, averaged across
2 replicas. All replay experiments use 50% replay ratio and a capacity of 3 million observations. We
investigate if uncorrected LSTM states can be used in combination with different replay modes. We
consider uniform sampling and prioritization via the critic’s loss, and include both full (β = 1) and
partial (β = 0.5) importance corrections
13
Under review as a conference paper at ICLR 2020
A.2 Prioritized and Uniform Experience Replay, LSTM States
With prioritized experience replay each transition T is sampled with probability P(T) α pT，for a
suitable unnormalized priority score pτ and a global tunable parameter α. It is common (Schaul et al.,
2015; Horgan et al., 2018; Hessel et al., 2017) to then weight updates computed from that sample by
1/P (T )β for 0 < β ≤ 1, where β = 1 fully corrects for the bias introduced in the state distribution.
In one step temporal difference methods, typical priorities are based on the immediate TD-error, and
are typically recomputed after a transition is sampled from replay. This means low priorities might
stay low and get stale - even if the transition suddenly becomes relevant. To alleviate this issue, the
sampling distribution is mixed with a uniform, as controlled by a third hyper parameter .
The performance of agents with prioritized experience replay can be quite sensitive to the hyper-
parameters α, β, and .
A critical practical consideration is how to implement random access for recurrent memory agents
such as agents using an LSTM. Prioritized agents sample a presumably interesting transition from
the past. This transition may be at any position within the episode. To infer the correct recurrent
memory-state at this environment-state all earlier environment-states within that episode would
need to be replayed. A prioritized agent with a random access pattern would thus require costly
LSTM refreshes for each sampled transition. If LSTM states are not recomputed representational
missmatch (Kapturowski et al., 2019) occurs.
Sharing experience between multiple agents amplifies the issue of LSTM state representation miss-
match. Here each agent has its own network parameters and the state representations between agents
may be arbitrarily different.
As a mitigation Kapturowski et al. (2019) use a burn-in window or to initialize with a constant starting
state. We note that those solutions can only partially mitigate the fundamental issue and that counter
examples such as arbitrarily long T-Mazes (Tolman, 1948; Olton, 1979) can be constructed easily.
We thus advocate for uniform sampling. In our implementation we uniformly sample an episode.
Then we replay each episode from the beginning, using the most recent network parameters to
recompute the LSTM states along the way: this is particularly critical when sharing experience
between different agents, which may have arbitrarily different state representations.
This solution is exact and cost-efficient as it only requires one additional forward pass for each
learning step (forward + backward pass).
An even more cost efficient approach would be to not refresh LSTM states at all. Naturally this comes
at the cost of representational missmatch. However it would allow for an affordable implementation
of prioritized experience replay. We investigate this in Figure 4 (right) and observe that it is not viable.
We compare a baseline V-trace agent with no experience replay, one with uniform experience replay,
and two different prioritized replay agents. We do not refresh LSTM states for any of the agents.
The uniform replay agent is more data efficient then the baseline, and also saturates at a higher level
of performance. The best prioritized replay agent uses full importance sampling corrections (β = 1).
However it performs no higher than with uniform replay. We therefore we used uniform replay with
full state correction for all our investigations in the paper.
A.3 Evaluation Protocol
For evaluation, we average episode returns within buckets of 1M (Atari) and 10M (DMLab) envi-
ronment steps for each agent instance, and normalize scores on each game by using the scores of a
human expert and a random agent (van Hasselt et al., 2016). In the multi-task setting, we then define
the performance of each agent as the median normalized score of all levels that the agent trains on.
Given the use of population based training, we need to perform the comparisons between algorithms
at the level of sweeps. We do so by selecting the best performing agent instance within each sweep at
any time. Note that for the multi-task setting, our approach of first averaging across many episodes,
then taking the median across games, on DMLab further downsampling to 100M env steps, and only
finally selecting the maximum within the sweep, results in substantially lower variance than if we
were to compute the maximum before the median and smoothing.
14
Under review as a conference paper at ICLR 2020
All DMLab-30 sweeps are repeated 3× with the exception of ρ = 2 and ρ = 4 in Figure 4. We then
plot a shaded area between the point-wise best and worst replica and a solid line for the mean. Atari
sweeps having 57 games are summarized and plotted by the median of the human-normalized scores.
Appendix B	Algorithm Pseudocode
We present algorithm pseudocode for LASER with trust region (Algorithm 1). For clarity we present a
version without LSTM and focus on the single agent case. The multi-agent case is a simple extension
where all agents save to the same replay database and also sample from the same replay. Also each
agent starts with different network parameters and hyper-parameters. The LSTM state recomputation
can be achieved with Replayer Threads (nearly identical to Actor Threads) that sample entire epsiodes
from replay, step through them while reevaluating the LSTM state and slice the experience into
trajectories of length T. Similar to regular LSTM Actor Threads from Espeholt et al. (2018) the
Replayer Threads send each trajectory together with an LSTM state to the learning thread via a queue.
The Learner Thread initializes the LSTM with the transmitted state when the LSTM is unrolled over
the trajectory.
Algorithm 1 Single Agent LASER with Trust Region
Initialize parameter vectors θ. Initialize π1 = πθ .
Actor Thread:
while training is ongoing do
Sample trajectory unroll u = {τt}t∈{1,...,T} of length T by acting in the environment using the
latest ∏k where Tt = (st,at,rt,μt = ∏k(st∣∙)).
Enqueue u into Lerner Queue, wait if full.
Add u into Replay Database.
Remove oldest trajectory if database has reached desired capacity limit.
end while
Learner Thread:
Given: Batch size B , online fraction α.
for training iteration k do
Form training batch U = {ub}b∈{1,...,B} of B trajectories of length T, by dequeuing Bα
trajectories from Lerner Queue and sampling B(1 - α) trajectories from Replay Database.
Evaluate the target policy ∏k on the sampled transitions in U: i.e. ∏k(sb,t∣∙).
Compute behaviour relevance mask M with Mb,t = KL(∏k(sb,t∣∙)∣∣μb,t) < b where μb,t, sb,t
are obtained from Ub,t .
Compute trust-region V-trace return Vt,b using 8 where λ∏,μ(sb,t) = Mb,t.
Let [Lv(θ)]t,b = 1 (Vt,b - Vθ(St,b))2.
Let At,b = Vt,b - Vθ(Sts and [LP(θ)]t,b = Pt,b log[∏θ(st,b∣at,b)]At,b,where P is the clipped
v-trace importance sampling ratio.
Perform gradient update to θ using Vθ Pt b[Lv (θ) + LP(θ)]t,bMt,b, denote the resulting ∏ as
πk+1.
end for
Appendix C	Propositions
We have stated five propositions in our paper for which we provide proofs below.
Proposition 1. The V-trace value estimate Vπ is biased: It does not match the expected return of
π but the return ofa related implied policy π defined by equation 9 that depends on the behaviour
policy μ:
π ( I ) _	min[pμ(a∣x),π(a∣x)]
μμ aX	Pb∈A min [Pμ(blx), n(b|x)]
(9)
Proof. See Espeholt et al. (2018).
□
15
Under review as a conference paper at ICLR 2020
Proposition 2. The V-trace policy gradient is biased: given the the optimal value function V *
the V-trace policy gradient does not converge to a locally optimal π* for all off-policy behaviour
distributions μ.
Proof. Proof by contradiction:
Consider a tabular counter example with a single (locally) optimal policy at st given by π* (st) =
argmax∏ [52a∈A ∏(a∣st)Q*(a, st)] that always selects the action argmaXαQ*(a, st).
Even in this ideal tabular setting V-trace policy gradient estimates a different ∏* rather than the
optimal π* as follows
VV*，n(St) = Eμ [ρt(rt + YV*(st+ι)▽ log∏(at∣st)]
=Eμ [ρtQ*(st,at)Vlog∏(at∣st)]
Eμ
Eμ
min
∏(at∣st) .
μ(at∣st),ρ
∏(at∣st)
.μ(at∣st)
min
Q*(st,at )V log π(at |st)
1,pπ(a⅛j] Q*(st，at)v log π(atist)
(10)
En min 1,.„, Q*(st,at)Vlog∏(at|st)
π(at |st)
E∏ [ω(st ,at )Q* (st,at)V log ∏(at∣st)]
E∏ [Q*,ω(st,at)Vlog∏(at∣st)]
Observe how the optimal Q-function Q* is scaled by ω(st,at) = min 1,0?(：；|：；) ≤ 1 resulting
in implied State-action values Q*,ω. This penalizes actions where μ(at st)p < π(at St) and makes
V-trace greedy w.r.t. to the remaining ones. Thus μ can be chosen adversarially to corrupt the optimal
state action value. Note that P is a constant typically chosen to be 1.
To prove the lemma consider a counter example such as an MDP with two actions and Q* = (2, 5)
and μ = (0.9,0.1) and initial ∏ = (0.5,0.5). Here the second action with expected return 5 is clearly
favourable. Abusing notation μ∕π = (l.8,0.2). Thus Qπ,ω = (2 * 1,5 * 0.2) = (2,1). Therefore
∏* = (1,0) wrongly selects the first action.	□
Proposition 3. Mixing on-policy data into the V-trace policy gradient with the ratio α reduces the bias
by providing a regularization to the implied state-action values. In the general function approximation
case it changes the off-policy V-trace policy gradient from Es dμ(s)E∏ [(Q(s, a)V log π(a∣s)] to
Ps E∏ [Qα(s,a)Vlogπ(a∣s)] where Qa = Qdπ(s)α + Qωdμ(s)(1 一 α) is a regularized state-
action estimate and dπ, dμ are the state distributionsfor π and μ. Note that there exists α ≤ 1 such
that Qα has the same argmax (i.e. best action) as Q.
Proof. Note that the on-policy policy gradient is given by
VJon(∏) = X dπ(s)En [Q(s, a)Vlog∏(a∣s)]
s
Similarly the off-policy V-trace gradient is given by
VJoff(∏) = Xdμ(s)E∏ [ω(s,a)Q(s,a)Vlogπ(a∣s)]
s
with the V-trace distortion factor ω(st,at) = min [1, p∏(at∣st)] ≤ 1 that can de-emphasize action
values and Qω (s, a) = ω(s, a)Q(s, a).
16
Under review as a conference paper at ICLR 2020
The α-interpolation of both gradients can be transformed as follows:
V(αJon + (1- α) Joff)(∏) = αXdπ(s)E∏ [Q(s, a)Vlog∏(a∣s)]
s
十 (1 — α) ^X dμ (S)En [ω(s,a)Q(s,a)V log π(a∣s)]
s
=^X dπ(s)E∏ [Q(s, a)αVlogπ(a∣s)]
s
+ Xd"(s)En [Qω(s,a)(1 - α)Vlogπ(a∣s)]
s
=^XE∏ [Q(s,a)dn(s)αVlogπ(a∣s)]
s
+ X En [Qω(s, a)d"(s)(1 - α)Vlogπ(a∣s)]
s
=^X E∏ [Q(s, a)dn(s)αVlogπ(a∣s) + Qω(s, a)dμ(s)(1 — α)Vlogπ(a∣s)]
s
=X En [(Q(s, a)dn(s)α + Qω(s, a)d"(s)(1 - α)) V logπ(a∣s)]
s
=XEn [Qα(s,a)Vlogπ(a∣s)]
s
(11)
for Qα(s, a) = Q(s,a)dn(s)α + Qω(s, a)dμ(s)(1 — a)	□
Interpretation of Proposition 3 As discussed in section 3 the V-trace policy gradient will have
the correct local fixpoint at state s if the argmax of the state-value function is preserved despite
the distortion: i.e. if argmaxa [Q(s, a)] = argmaxa [Qω (s, a)]. Respectively when mixing in an
α ∈ [0, 1) share of online data the fixpoint will be preserved if
argmaxa[Q(s, a)] = argmaxa[Qα(s, a)]	(12)
Let a* = argmaxb(Q, b) be any best action and A* be set of best actions. Then equation 12 is
equivalent to:
Qα(s, a*) > Qα(s, b) ∀b 6∈ A*
Using the definition of Qα this can be rewritten as:
Q(s, a*)dπ(s)α + Qω(s, a*)dμ(s)(1 - α) > Q(s, b)dn(s)α + Qω(s, b)dμ(s)(1 - α) ∀b ∈ A*
Which can be rearranged to:
[Q(s,a*)dn(s) - Q(s,b)dn(s)]α > [Qω(s,b)dμ(s) - Qω(s,a*)dμ(s)](1 - α) ∀b ∈ A*
By definition Q(s, a*)dn (s) - Q(s, b)dn (s) > 0 ∀b 6∈ A*, hence:
α > Qω(s,b) - Qω(s,a*) d"(s) ∀b ∈ A*
1 — α	Q(s,a*) — Q(s, b) dn(s)
It follows that the policy gradient will have the same local fixpoint if
α	jQω(s,b) - Qω(s,a*)] d*(s)
1 — α > b∈A*	Q(s, a*) — Q(s, b)	dn(s)
(13)
Note that ι-αα → ∞ as α → 1. Mixing-in more online data thus increases the left hand side. Also
note that the right hand side decreases due to dμ(s)∕dn(s) if ∏ visits the state S more often than μ.
Furthermore the larger the action value gap in the real Q-function Q(s, a*) - Q(s, b) the lower the
right hand side. Finally the denominator will be negative if maXb∈A* [Qω (s, b)] < Qω (s, a*) thus
enabling correct learning even in the pure off-policy case with α = 0.
17
Under review as a conference paper at ICLR 2020
Note that all of those conditions can be computed and checked if an accurate Q-function and state
distribution is accessible. How to use imperfect Q-function estimates to adaptively choose such an α
remain a question for future research.
Proposition 4. Let GnS“z be a set of importance sampling estimators as defined in equation 7. Note
that they all have the same fix point V π and contract with at least γ. Then the contraction properties
Carty over t VnUsted. InPartiCular Hasted - V1∞ ≤ Y IV - V"∞.
Proof. Let us consider the set of importance sampling estimators as defined in 7 and note
that they all contract to the same fixed point Vn with at least ∣Eμz∣z ∖G∏μz (s)] - Vn(s)∣∞ ≤
γ ∣V(S) - Vn(s)∣∞ for any state s.
By Minkowski’s inequality the contraction properties of importance sampled Monte-Carlo bootstraps
carry over to VnUsted which is ap(z∣μz ∈ Mβ,∏ (St)) weighted average:
IVΠusted(s) - Vn(s)∣∞ = ∣Ez[Eμz∣z [G∏Sμz(s)] ∣μz ∈ Mβ,n(st)i - Vn(s)∣∞
=∣Ez [E“z|z [G∏Sμz(s)] - Vn(s)∣μz ∈ Mβ,n(st)i ∣oo
≤ Ez h ∣Eμz∣z [G∏S“z(s)] - Vn(st)∣∞∣μz ∈ Mβ,n(st)i	(14)
< EzhY IV(s) - Vn(s)∣∞∣μz ∈ Mβ,n(st)i
=Y IV(s) - Vn(s)∣∞
□
Proposition 5. Let G^^^ce be a set ofV-trace estimators (see equation 8) with corresponding fixed
points Vz (see equation 3) to whiCh they ContraCt at a speed of an algorithm and behaviour speCifiC
ηz. Then VnUsted moves towards Vβ = Ez∣μz∈Mβ ,(§/ [Vz] shrinking the distance as follows
IVnUsted- V β∣∞ < maxμz ∈Mβ,∏ (St) Inz(V - V Z )l∞ ≤ ηmax maxμz ∈Mβ,∏ (St) I(V - V Z )l∞ With
ηmax =maxμz ∈Mβ,∏ (St) nz.
Proof. Recall the contraction properties of a V-trace importance sampled Monte-Carlo bootstraps
GVtμace being
∣Eμz∣z [GVμace(s)] - VZ(s)∣∞ < nz IV(s) - VZ(s)I∞
for an algorithm and behaviour specific ηz < 1 for a z dependent fixed point Vz and for any
bootstrap V. We then show that VtnrUsted moves towards the weighted average of fixed points
Vβ = EZ∣μz∈Mβ,∏(st) [Vz],since
∣∣VtnrUsted(s) -Vβ(s)∣∣∞ < nmax max	IV (s) - Vz(s)I∞
μz ∈ Mβ,π (St )
holds for any bootstrap function V as we show below.
∣ Vnusted (s) - Vβ (s)∣∞ =IEZhEμz∣z [GVCce(s)] - VZ (s) g ∈ Mp,n ⑶)]L
≤ Ez h ∣Eμz∣z [GVCce(s)] - VZ (s)∣∞∣μz ∈ Mβ,n ⑶)]
< Ez h InZ(V(s) - VZ(s))I∞∣μz ∈ Mβ,n(st)i	(15)
≤ max	Inz(V(s) - Vz(s))I∞
μz ∈ Mβ,∏ (St)
≤ nmax max	IV (s) - Vz(s)I∞
μz ∈ -Mβ ,π (s t )
□
18
Under review as a conference paper at ICLR 2020
Appendix D Detailed Atari Results
We display the Atari per-level performance of various agents at 50M and 200M environment steps in
Table 2. The scores correspond to the agents presented in Figure 1. The LASER scores are computed
by averaging the last 100 episode returns before 50M or respectively 200M environment frames
have been experienced. Following the procedure defined by Mnih et al. (2015) we initialize the
environment with a random number of no-op actions (up to 37 in our case). Again following Mnih
et al. (2015) episodes are terminated after 30 minutes of gameplay. Note that Xu et al. (2018) have
not published per-level scores. Rainbow scores are obtained from Hessel et al. (2017).
19
Under review as a conference paper at ICLR 2020
Table 2: Per level performance of various agents at 50M and 200M environment steps (see Figure 1).
Game	LASER Shared (sweep at 50M)	LASER Shared (sweep at 200M)	LASER (no sweep at 200M)	Rainbow (no sweep at 200M)
alien	18635.3	18277.3	35565.9	9491.7
amidar	1838.3	2695	1829.2	5131.2
assault	26027.1	40603.2	21560.4	14198.5
asterix	496735.0	240770	240090	428200
asteroids	232651	257420.1	213025	2712.8
atlantis	889934.0	866584	841200	826660
bank_heist	1333.1	1712.8	569.4	1358
battle_zone	66900	131880.0	64953.3	62010
beam_rider	80830.5	125795.2	90881.6	16850.2
berzerk	46651.6	64513.1	25579.5	2545.6
bowling	42.4	47.4	48.3	30
boxing	99.8	99.4	100.0	99.6
breakout	852.5	850.3	747.9	417.5
centipede	208008	409702.8	292792	8167.3
ChoPPeJCommand	24814	727333	761699.0	16654
crazy_climber	160494	88818	167820	168788.5
defender	355447	369397.0	336953	55105
demon_attack	133557	138000.6	133530	111185
double_dunk	0.1	23.5	14	-0.3
enduro	0	0	0	2125.9
fishing_derby	45.4	62.6	45.2	31.3
freeway	34.0	34.0	0	34.0
frostbite	5297.4	2230.8	5083.5	9590.5
gopher	86222.2	39721.2	114820.7	70354.6
gravitar	1360.5	2812.0	1106.2	1419.3
hero	30159.2	36510.6	31628.7	55887.4
ice_hockey	20.2	38.7	17.4	1.1
jamesbond	21663	60402.5	37999.8	19809
kangaroo	13932	14187	14308	14637.5
krull	9559.3	5743.6	9387.5	8741.5
kung_fu_master	65032	81792	607443.0	52181
montezuma_revenge	1	1	0.3	384.0
ms_pacman	6089.3	6890.7	6565.5	5380.4
name_this_game	25998.9	27910.7	26219.5	13136
phoenix	458355	628711.6	519304	108529
pitfall	-0.2	-0.2	-0.6	0.0
pong	21.0	21.0	21.0	20.9
private_eye	100	100	96.3	4234.0
qbert	20283.8	24600.8	21449.6	33817.5
riverraid	24138.1	35491.5	40362.7	22920.8
road_runner	52942	63762.0	45289	62041
robotank	63.6	67.8	62.1	61.4
seaquest	1802.2	557213.3	2890.3	15898.9
skiing	-8904.8	-8980.1	-29968.4	-12957.8
solaris	2222.4	3017.6	2273.5	3560.3
space_invaders	36071.4	53124.3	51037.4	18789
star_gunner	331327	602540.0	321528	127029
surround	9.8	9.8	8.4	9.7
tennis	0	0	12.2	0
time_pilot	77899	113603.0	105316	12926
tutankham	251.8	268.5	278.9	241
up_n_down	341988	368586.5	345727	125755
venture	0	0	0	5.5
video_Pinball	513121	397451	511835	533936.5
Wizard_o匚wor	22280	45335.0	29059.3	17862.5
yars_revenge	145055	144370	166292.3	102557
zaxxon	50486	106862.0	41118	22209.5
20