Under review as a conference paper at ICLR 2020
Constant Curvature Graph Convolutional
Networks
Anonymous authors
Paper under double-blind review
Ab stract
Interest has been rising lately towards methods representing data in non-Euclidean
spaces, e.g. hyperbolic or spherical, that provide specific inductive biases useful
for certain real-world data properties, e.g. scale-free, hierarchical or cyclical.
However, the popular graph neural networks are currently limited in modeling data
only via Euclidean geometry and associated vector space operations. Here, we
bridge this gap by proposing mathematically grounded generalizations of graph
convolutional networks (GCN) to (products of) constant curvature spaces. We do
this by i) introducing a unified formalism that can interpolate smoothly between
all geometries of constant curvature, ii) leveraging gyro-barycentric coordinates
that generalize the classic Euclidean concept of the center of mass. Our class of
models smoothly recover their Euclidean counterparts when the curvature goes to
zero from either side. Empirically, we outperform Euclidean GCNs in the tasks
of node classification and distortion minimization for symbolic data exhibiting
non-Euclidean behavior, according to their discrete curvature.
1	Introduction
Graph Convolutional Networks. The success of convolutional networks and deep learning for
image data has inspired generalizations for graphs for which sharing parameters is consistent with
the graph geometry. Bruna et al. (2014); Henaff et al. (2015) are the pioneers of spectral graph
convolutional neural networks in the graph Fourier space using localized spectral filters on graphs.
However, in order to reduce the graph-dependency on the Laplacian eigenmodes, Defferrard et al.
(2016) approximate the convolutional filters using Chebyshev polynomials leveraging a result of Ham-
mond et al. (2011). The resulting method (discussed in appendix A) is computationally efficient and
superior in terms of accuracy and complexity. Further, Kipf & Welling (2017) simplify this approach
by considering first-order polynomials approximations obtaining high scalability. The proposed graph
convolutional networks (GCN) is interpolating node embeddings via a symmetrically normalized
adjacency matrix, while this weight sharing can be understood as an efficient diffusion-like regularizer.
Recent works extend GCNs to achieve state of the art results for link prediction (Zhang & Chen,
2018), graph classification (Hamilton et al., 2017; Xu et al., 2018) and node classification (Klicpera
et al., 20l9; Velickovic et al., 2018).
Euclidean geometry in ML. In machine learning (ML), data is most often represented in a Eu-
clidean space for various reasons. First, some data is intrinsically Euclidean, such as positions in 3D
space in classical mechanics. Second, intuition is easier in such spaces, as they possess an appealing
vectorial structure allowing basic arithmetic and a rich theory of linear algebra. Finally, a lot of
quantities of interest such as distances and inner-products are known in closed-form formulae and can
be computed very efficiently on the existing hardware. These operations are the basic building blocks
for most of today’s popular machine learning models. Thus, the powerful simplicity and efficiency of
Euclidean geometry has led to numerous methods achieving state-of-the-art on tasks as diverse as
machine translation (Bahdanau et al., 2014; Vaswani et al., 2017), speech recognition (Graves et al.,
2013), image classification (He et al., 2016) or recommender systems (He et al., 2017).
Riemannian ML. In spite of this success, certain types of data (e.g. hierarchical, scale-free or
spherical data) have been shown to be better represented by non-Euclidean geometries (Defferrard
et al., 2019; Bronstein et al., 2017; Nickel & Kiela, 2017; Gu et al., 2019), leading in particular to the
1
Under review as a conference paper at ICLR 2020
rich theories of manifold learning (Roweis & Saul, 2000; Tenenbaum et al., 2000) and information
geometry (Amari & Nagaoka, 2007). The mathematical framework in vigor to manipulate non-
Euclidean geometries is known as Riemannian geometry (Spivak, 1979). Although its theory leads to
many strong and elegant results, some of its basic quantities such as the distance function d(∙, ∙) are
in general not available in closed-form, which can be prohibitive to many computational methods.
Representational Advantages of Geometries of Constant Curvature. An interesting trade-off
between general Riemannian manifolds and the Euclidean space is given by manifolds of constant
sectional curvature. They define together what are called hyperbolic (negative curvature), elliptic
(positive curvature) and Euclidean (zero curvature) geometries. As discussed below and in appendix B,
Euclidean spaces have limitations and suffer from large distortion when embedding certain types of
data such as trees, e.g. fig. 1. In these cases, the hyperbolic and spherical spaces have representational
advantages providing a better inductive bias for the respective data.
The hyperbolic space can be intuitively understood as
a continuous tree: the volume of a ball grows exponen-
tially with its radius, similarly as how the number of nodes
in a binary tree grows exponentially with its depth. Its
tree-likeness properties have long been studied mathemati-
cally (Gromov, 1987; Hamann, 2017; Ungar, 2008) and it
was proven to better embed complex networks (Krioukov
et al., 2010), scale-free graphs and hierarchical data com-
pared to the Euclidean geometry (Cho et al., 2019; Sala
et al., 2018; Ganea et al., 2018b; Gu et al., 2019; Nickel
& Kiela, 2018; 2017; Tifrea et al., 2019). Several impor-
tant tools or methods found their hyperbolic counterparts,
such as variational autoencoders (Mathieu et al., 2019;
Ovinnikov, 2019), attention mechanisms (Gulcehre et al.,
2018), matrix multiplications, recurrent units and multino-
mial logistic regression (Ganea et al., 2018a).
Similarly, spherical geometry provides benefits for mod-
eling spherical or cyclical data (Defferrard et al., 2019;
Matousek, 2013; Davidson et al., 2018; Xu & Durrett,
Figure 1: Euclidean space quickly "runs out
of space" when fitting exponentially volume
growing data such as trees. The embedding
distance between the red and the green node
keeps decreasing as the number of tree nodes
increases, meaning that the graph distance
(shortest path length) is no longer accurately
represented. Details in appendix B.
2018; Gu et al., 2019; Grattarola et al., 2018; Wilson et al., 2014).
Computational Efficiency of Constant Curvature Spaces (CCS). CCS are some of the few
Riemannian manifolds to possess closed-form formulae for geometric quantities of interest in
computational methods, i.e. distance, geodesics, exponential map, parallel transport and their
gradients. We also leverage here the closed expressions for weighted centroids.
“Linear Algebra” of CCS: Gyrovector Spaces. In order to study the geometry of constant nega-
tive curvature in analogy with the Euclidean geometry, Ungar (1999; 2005; 2008; 2016) proposed the
elegant non-associative algebraic formalism of gyrovector spaces. Recently, Ganea et al. (2018a)
have linked this framework to the Riemannian geometry of the space, also generalizing the building
blocks for non-Euclidean deep learning models operating with hyperbolic data representations.
However, it remains unclear how to extend in a principled manner the connection between Riemannian
geometry and gyrovector space operations for spaces of constant positive curvature (spherical). By
leveraging Euler’s formula and complex analysis, we present to our knowledge the first unified gyro
framework that smoothly interpolates between geometries of constant curvatures irrespective of their
signs. This is possible when working with the Poincare ball and stereographic spherical projection
models of respectively hyperbolic and spherical spaces.
How should one adapt graph neural networks to non-flat geometries of constant curvature?
In this work, we propose constant curvature GCNs to model non-Euclidean data. Node embeddings
lie in spaces of constant curvature or product of those instead of a Euclidean space, thus leveraging
both the representational power of these geometries and the effectiveness of GCNs.
Concurrent to our work, Chami et al. (2019); Liu et al. (2019) propose hyperbolic graph neural
networks using tangent space aggregation.
2
Under review as a conference paper at ICLR 2020
Figure 2: GeodeSiCS in the PoinCarC disk (left) and the stereographic projection of the sphere (right).
2	The Geometry of Constant Curvature Spaces
Riemannian Geometry. A manifold M of dimension d is a generalization to higher dimensions
of the notion of surface, and is a space that locally looks like Rd . At each point x ∈ M, M can be
associated a tangent space TxM, which is a vector space of dimension d that can be understood as a
first order approximation of M around x. A riemannian metric g is given by an inner-product gx(∙, ∙)
at each tangent space TxM, gx varying smoothly with x. A given g defines the geometry of M,
because it can be used to define the distance between x and y as the infimum of the lengths of smooth
paths Y : [0,1] → M from X to y, where the length is defined as '(γ):= R01 PgY(t)(γ(t), γ(t))dt.
Under certain assumptions, a given g also defines a curvature at each point.
Unifying all curvatures κ. There exist several models of respectively constant positive and negative
curvatures. For positive curvature, we choose the stereographic projection of the sphere, while for
negative curvature we choose the PoincarC model which is the stereographic projection of the Lorentz
model. As explained below, this choice allows us to generalize the gyrovector space framework
and unify spaces of both positive and negative curvature κ into a single model which we call the
κ-stereographic model.
The κ-stereographic model. For a curvature κ ∈ R and a dimension d ≥ 2, it is defined as
stK = {x ∈ Rd | -Kkxk2 < 1} equipped with its Riemannian metric gK = (1+κ∣4x∣∣2)21 =： (λχ)2I.
Note in particular that when K ≥ 0, stK is Rd, while when κ < 0 it is the open ball of radius 1 /χ∕-κ.
Gyrovector spaces & Riemannian geometry. As discussed in section 1, the gyrovector space
formalism is used to generalize vector spaces to the PoincarC model of hyperbolic geometry (Ungar,
2005; 2008). In addition, important quantities from Riemannian geometry can be rewritten in terms
of the Mobius vector addition and scalar-vector multiplication (Ganea et al., 2018a). We here extend
gyrovector spaces to the K-stereographic model, i.e. allowing positive curvature.
For κ > 0 and any point X ∈ stK, we will denote by X the unique point of the sphere of radius K- 1
in Rd+1 whose stereographic projection is x. As detailed in appendix C.2.2, it is given by
X ：= (λXχ,κ-1 (λX - 1)).
For X, y ∈ stdK , we define the K-addition, in the K-stereographic model by:
x ㊉ K y =
(I - 2κχTy - K||y||2)χ + (1 + K||x||2)y
1 - 2kxty + K2∣∣x∣∣2∣∣y∣∣2
(1)
(2)
The K-addition is defined in all the cases except for spherical geometry and X = y/(Kkyk2) as stated
by the following theorem proved in Appendix C.2.1.
Theorem 1	(Definiteness of κ-addition). We have 1 — 2kxt y + κ2 ∣∣x∣∣2 ∣∣y∣∣2 = 0 if and only if
κ > 0 and X = y∕(κ∣∣y k2).
For S ∈ R and X ∈ stK (and |s tan-1 |闵|| < κ2π∕2 if κ > 0), the K-Scaling in the κ-stereographic
model is given by:
s ⑤K X = tanκ (s ∙ tan-1
X
网）网∈
(3)
where tanK equals K-1/2 tan if K > 0 and (-K)-1/2 tanh if K < 0. This formalism yields simple
closed-forms for various quantities including the distance function inherited from the Riemannian
manifold (stdK , gK), the exp and log maps, and geodesics, as shown by the following theorem.
3
Under review as a conference paper at ICLR 2020
Theorem 2	(Extending gyrovector spaces to positive curvature). For x, y ∈ st£, X = y, V = 0,
(and x = —y∕(κ∣∣yk2) if κ > 0), the distance function is given bya:
dκ(x, y) = 2∣κ∣-1/2 tan-1 k — X ㊉K yk,	(4)
the unit-speed geodesic from X to y is unique and given by
Yx→y(t) = X ㊉K (t ③K (—X ㊉K y)),	(5)
and finally the exponential and logarithmic maps are described as:
K/ 、	1	1λ 11 AKIMI、 v ∖ κ κ/ 、	2∣κ∣-2	7 H	ll —X㊉K y
eχPx(V) = X㊉K(tanK (网2~2— I iiv∣ij ;logx(y) = ak tan-11|-x㊉Ky|||| -X㊉®y∣∣
X	"(6)
aWe write —x ㊉ y for (—x)㊉ y and not — (X ㊉ y).
Proof sketch:
The case κ ≤ 0 was already taken care of by (Ganea et al., 2018a). For κ > 0, we provide a detailed
proof in Appendix C.2.2. The exponential map and unit-speed geodesics are obtained using the
Egregium theorem and the known formulas in the standard spherical model. The distance then follows
from the formula dK(X, y) = k logxK(y)kx which holds in any Riemannian manifold.
Around κ = 0. One notably observes that choosing κ = 0 yields all corresponding Euclidean
quantities, which guarantees a continuous interpolation between κ-stereographic models of different
curvatures, via Euler,s formula tan(x) = —i tanh(ix) where i := √-1. But is this interpolation
differentiable with respect to κ? It is as shown by the following theorem, proved in Appendix C.2.3.
Theorem 3 (Smoothness of st£ w.r.t. K around 0). Let v = 0 and x, y ∈ Rd, such that
x = y (and x = —y/(Kkyk2) f K > 0). Quantities in Eqs. (4,5,6) are well-defined for
∣κ∣ < 1/min(kx∣∣2, ky(2), i.e. for K small enough. Their first order derivatives at 0- and 0+
exist and are equal. Moreover, for the distance we have:
dn(x, y) =2kx — yk — 2κ(kx — yk3∕3 + (XT y)∣∣x — y『)+ O(κ2).	⑺
Note that for xTy ≥ 0, this tells us that an infinitesimal change of curvature from zero to small
negative, i.e. towards 0-, while keeping x, y fixed, has the effect of increasing their distance.
As a consequence, we have a unified formalism that interpolates smoothly between all three ge-
ometries of constant curvature.
3	κ-GCNS
We start by introducing the methods upon which we build. We present our models for spaces of
constant sectional curvature, in the K-stereographic model. However, the generalization to cartesian
products of such spaces (Gu et al., 2019) follows naturally from these tools.
3.1 Graph Convolutional Networks
The problem of node classification on a graph has long been tackled with explicit regularization
using the graph Laplacian (Weston et al., 2012). Namely, for a directed graph with adjacency matrix
A, by adding the following term to the loss: Pi,j Aij kf(xi) — f (xj)k2 = f (X)T Lf (X), where
L = D — A is the (unnormalized) graph Laplacian, Dii := Pk Aik defines the (diagonal) degree
matrix, f contains the trainable parameters of the model and X = (xij)ij the node features of the
model. Such a regularization is expected to improve generalization if connected nodes in the graph
tend to share labels; node i with feature vector xi is represented as f(xi) in a Euclidean space.
4
Under review as a conference paper at ICLR 2020
With the aim to obtain more scalable models, Defferrard et al. (2016); Kipf & Welling (2017) propose
to make this regularization implicit by incorporating it into what they call graph convolutional
networks (GCN), which they motivate as a first order approximation of spectral graph convolutions,
yielding the following scalable layer architecture (detailed in appendix A):
H(t+1) = σ (ID- 1 AD-2 H⑴W㈤),
(8)
where A = A + I has added self-connections, Dii = k Aik defines its diagonal degree matrix, σ
is a non-linearity such as sigmoid, tanh or ReLU = max(0, ∙), and W(t) and H(t) are the parameter
and activation matrices of layer t respectively, with H(0) = X the input feature matrix.
3.2	TOOLS FOR A κ-GCN
Learning a parametrized function fθ that respects hyperbolic geometry has been studied in (Ganea
et al., 2018a): neural layers and hyperbolic softmax. We generalize their definitions into the
κ-stereographic model, unifying operations in positive and negative curvature. We explain how
curvature introduces a fundamental difference between left and right matrix multiplications, depicting
the Mobius matrix multiplication of (Ganea et al., 2018a) as a right multiplication, independent for
each embedding. We then introduce a left multiplication by extension of gyromidpoints which ties
the embeddings, which is essential for graph neural networks.
3.3	κ-RIGHT-MATRIX-MULTIPLICATION
Let X ∈ Rn×d denote a matrix whose n rows are d-dimensional embeddings in stdκ , and let W ∈
Rd×e denote a weight matrix. Let us first understand what a right matrix multiplication is in Euclidean
space: the Euclidean right multiplication can be written row-wise as (XW)i∙ = Xi∙ W. Hence each
d-dimensional Euclidean embedding is modified independently by a right matrix multiplication. A
natural adaptation of this operation to the κ-stereographic model yields the following definition.
Definition 1. Given a matrix X ∈ Rn ×d holding K-stereographic embeddings in its rows and
weights W ∈ Rd×e ,the K-right-matrix-multiplication is defined row-wise as
(X 之 W)i∙=expK ((logK(X)W)i∙) = tanκ ("(XW)tan-1(∣∣X∙i∣∣)) U(XW)i∖
llXi∙l1	ll(XW)i∙l1
where expg and IogK denote the exponential and logarithmic map in the K-stereographic model.
This definition is in perfect agreement with the hyperbolic scalar multiplication for K < 0, which can
also be written as r ③K X = expK (r IogK (x)). ThiS operation is known to have desirable properties
such as associativity (Ganea et al., 2018a).
3.4	K-LEFT-MATRIX-MULTIPLICATION AS A MIDPOINT EXTENSION
For graph neural networks we also need the notion of message passing among neighboring nodes,
i.e. an operation that combines / aggregates the respective embeddings together. In Euclidean space
such an operation is given by the left multiplication of the embeddings matrix with the (preprocessed)
adjacency A: H(l+1) = σ(AZ(I)) where Za) = H(l)W(l). Let us consider this left multiplication.
For A ∈ Rn×n , the matrix product is given row-wise by:
(AX)i∙ = AilXl∙ + …+ Ain Xn∙
This means that the new representation of node i is obtained by calculating the linear combination of
all the other node embeddings, weighted by the i-th row of A. An adaptation to the K-stereographic
model hence requires a notion of weighted linear combination. We propose such an operation in
stdK by performing a K-scaling of a gyromidpoint - whose definition is reminded below. Indeed, in
Euclidean space, the weighted linear combination αX+βy can be re-written as (α+β)mE(X, y; α, β)
with Euclidean midpoint m<(x, y; α, β) := α+βX + α+βY∙ ThiS motivates generalizing the above
operation to stdK as follows.
5
Under review as a conference paper at ICLR 2020
Figure 3: Left: Euclidean Linear combination αx + βy. Middle: Poincare gyromidpoints (red dots)
of two points for different weights on the left and Poincare gyromidpoints in a hyperbolic triangle on
the right with two equal and one free weight. Right: Mobius gyromidpoint in the Poincare model
defined by (Ungar, 2008) and alternatively, here in eq. (10).
(10)
Definition 2. Given a matrix X ∈ RnXd holding K-stereographic embeddings in its rows and
weights A ∈ Rnxn, the K-left-matrix-multiplication is defined row-wise as
(A 冈K X)i∙ := (X Aij)③K mκ(Xι∙,…，Xn∙; Ai1,…，Ain).	(9)
The κ-scaling is motivated by the fact that dκ(0, r ③K x) = ∣r∣dκ(0, x) for all r ∈ R, X ∈ st£. We
remind that the gyromidpoint is defined when K ≤ 0 in the K-stereographic model as (Ungar, 2010):
gαi λXi
Pn@ (* - 1) Xi
with λKx = 2/(1 + Kkxk2). Whenever K > 0, we have to further require the following condition:
Xαj(λKxj -1) 6=0.	(11)
j
For two points, one can calculate that (λKx - 1) + (λKy - 1) = 0 is equivalent to Kkxkkyk = 1, which
holds in particular whenever x = -y/(Kkyk2). See fig. 3 for illustrations of gyromidpoints.
Our operation K satisfies interesting properties, proved in Appendix C.2.4:
Theorem 4	(Neuter element & κ-scalar-associativity). We have In 冈 K X = X, and for r ∈ R,
r ③K (A 冈K X) = (rA)冈K X
The matrix A. In most graph neural networks, the matrix A is intented to be a preprocessed adja-
cency matrix, i.e. renormalized by the diagonal degreee matrix Dii = k Aik . This normalization
is often taken either (i) to the left: D-1 A, (ii) symmetric: D- 2 AD-1 or (iii) to the right: AD-1.
Note that the latter case makes the matrix right-stochastic1, which is a property that is preserved by
matrix product and exponentiation. For this case, we prove the following result in Appendix C.2.5:
Theorem 5	(κ-left-multiplication by right-stochastic matrices is intrinsic). If A, B are right-
stochastic, φ is a isometry of std. and X, Y are two matrices holding K-stereographic embeddings:
∀i, dK ((A 冈K Φ(X))i∙, (B 冈K Φ(Y))i∙) = dK((A 冈K X)i∙, (B 冈K Y)i∙)∙	(12)
The above result means that A can easily be preprocessed as to make its K-left-multiplication intrinsic
to the metric space (stdK, dK). At this point, one could wonder: does there exist other ways to take
weighted centroids on a Riemannian manifold? We comment on two plausible alternatives.
1M is right-stochastic if for all i, Pj Mij = 1.
6
Under review as a conference paper at ICLR 2020
Fr6chet/Karcher means. They are obtained as argminχ Pi aidκ(x, Xi)2;note that although they
are also intrinsic, they usually require solving an optimization problem which can be prohibitively
expensive, especially when one requires gradients to flow through the solution - moreover, for the
space stdκ, it is known that the minimizer is unique if and only if κ ≥ 0.
Tangential aggregations. They are defined by lifting the points in a chosen tangent space via the
logarithmic map, performing a linear combination and then projecting back via the exponential map,
and were in particular used in the recent works of Chami et al. (2019) and Liu et al. (2019). The below
theorem describes that for the κ-stereographic model, this operation is also intrinsic, i.e. commutes
with isometries. We prove it in Appendix C.2.6.
Theorem 6	(Tangential aggregation is intrinsic). Define the tangential aggregation of xι,..., Xn ∈
st£ w.r.t. weights {ai]ι≤i≤n, at point X ∈ st£ (for Xi = —x/(KkXk 2) if K > 0) by:
tgκ(xι,..., Xn; αι,...,αn):= expκ (X a logX(xj) .	(13)
For any isometry φ of st£, we have
tgφ(χ)Hφ(Xi)}; {ai}) =。(也({2}; {aiD).	(14)
3.5	Logits
Finally, we need the logit and softmax layer, a neccessity for any classification task. We here use the
model of (Ganea et al., 2018a), which was obtained in a principled manner for the case of negative
curvature. We leave for future work the adaptation of their analysis to positive curvature and use
in our experiments the straightforwardly adaptated formula to positive curvature, which we detail
appendix D.
3.6	K-GCN
We are now ready to introduce our K-stereographic GCN (Kipf & Welling, 2017), denoted by K-GCN2.
Assume we are given a graph with node level features G = (V, A, X) where X ∈ Rn×d with each
row Xi∙ ∈ stK and adjacency A ∈ Rn×n. We first perform a preprocessing step by mapping the
Euclidean features to st£ via the projection X → X/(2 ∙∖∕∣κ∣∣∣X∣∣maχ), where ∣∣X∣∣mαχ denotes the
maximal Euclidean norm among all stereographic embeddings in X. For l ∈ {0, . . . , L — 2}, the
(l + 1)-th layer of K-GCN is given by:
H(I+1) = σ0κ (A 国K (H(I) 0κ W(I))),
(15)
where H(O) = X, σ0κ (x) := expθ(σ(logK(X))) is the MnbiUS version (Ganea et al., 2018a) of a
11
D -1A D - 2
pointwise non-linearity σ and A
The final layer is a K-logit layer (appendix D):
H(L) = softmax (A logitκ (H(LT), W(LT)))
(16)
where W(L-1) contains the parameters ak and pk of the K-logits layer. A very important property of
K-GCN is that its architecture recovers the Euclidean GCN when we let curvature go to zero:
K-GCN -———→ GCN.
4	Experiments
We evaluate the architectures introduced in the previous sections on the tasks of node classification
and minimizing embedding distortion for several synthetic as well as real datasets. We detail the
training setup and model architecture choices to appendix E.
2To be pronounced “kappa” GCN; the greek letter κ being commonly used to denote sectional curvature
7
Under review as a conference paper at ICLR 2020
Model	Tree	Toroidal Graph	Spherical Graph
GCN (Linear)	0.045	0≡07	0
GCN (ReLU)	0.0502	0.0603	0.0409
H10-GCN	0.0029	0.272	0.267
S10-GCN	0.473	0.0485	0.0337
H5 X H5-GCN	0.0048	0.112	0.152
S5 × S5-GCN	0.51	0.0464	0.0359
(H2 )4 - GCN	0.025	0.084	0.062
(S29-GCN	0.312	0.0481	0.0378
Table 1: Minimum achieved average distortion of the different models. H and S denote hyperbolic
and spherical models respectively.
Figure 4: Histogram of Curvatures from "Deviation of Parallogram Law"
Minimizing Distortion Our first goal is to evaluate the graph embeddings learned by our GCN mod-
els on the representation task of fitting the graph metric in the embedding space. We desire to minimize
the average distortion, i.e. defined similarly as in (Gu et al., 2019): n12 Pi j∙ (( dxj ) - 1),
where d(xi , xj) is the distance between the embeddings of nodes i and j, while dG(i,j) is their graph
distance (shortest path length).
We create three synthetic datasets that best reflect the different geometries of interest: i) “Tree‘”: a
balanced tree of depth 5 and branching factor 4 consisting of 1365 nodes and 1364 edges. ii) “Torus”:
We sample points (nodes) from the (planar) torus, i.e. from the unit connected square; two nodes are
connected by an edge iff their toroidal distance (the warped distance) is smaller than a fixed R = 0.01;
this gives 1000 nodes and 30626 edges. iii) “Spherical Graph”: we sample points (nodes) from S2 ,
connecting nodes iff their distance is smaller than 0.2, leading to 1000 nodes and 17640 edges.
For the GCN models, we use 1-hot initial node features. We use two GCN layers with dimensions 16
and 10. The non-Euclidean models do not use additional non-linearities between layers. All euclidean
parameters are updated using the ADAM optimizer with learning rate 0.01. Curvatures are learned
using (stochastic) gradient descent and learning rate of 0.0001. All models are trained for 10000
epochs and we report the minimal achieved distortion. The results shown in table 1 reveal the benefit
of our models. One can notice that estimated curvatures correspond to our geometric knowledge
about these specific datasets.
4.1	Node Classification
We consider the popular node classification datasets Citeseer (Sen et al., 2008), Cora-ML (McCallum
et al., 2000) and Pubmed (Namata et al., 2012). Node labels correspond to the particular subfield
the published document is associated with. Dataset statistics and splitting details are deferred to the
appendix E due to the lack of space.
Curvature Estimations of Datasets To understand how far are the real graphs of the above datasets
from the Euclidean geometry, we first estimate the graph curvature of the four studied datasets using
the deviation from the Parallelogram Law (Gu et al., 2019) as detailed in appendix F. Curvature
histograms are shown in fig. 4. It can be noticed that the datasets are mostly non-Euclidean, thus
offering a good motivation to apply our constant-curvature GCN architectures.
8
Under review as a conference paper at ICLR 2020
Model	Citeseer	Cora-ML	Pubmed	MS Academic
GCN (ReLU)	75.7 ± 0.36	83.31 ± 0.36	79.05 ± 0.52	92.14 ± 0.25
GCN (Linear)	76.28 ± 0.30	83.81 ± 0.35	78.94 ± 0.50	92.3 ± 0.21
h64-gcn	76.29 ± 0.3	83.6 ± 0.34	79.01 ± 0.58	92.06±0.22
s64-gcn	76.18 ± 0.37	83.97 ± 0.31	79.04 ± 0.5	92.1±0.31
PrOd-GCN	75.91 ± 0.34	82.9 ± 0.6	78.7 ± 0.53	91.9 ± 0.40
Table 2: Node classification: Average accuracy across 10 splits with estimated uncertainties at 95
percent confidence level via bootstrapping on our datasplits. H and S denote hyperbolic (Poincare
ball model) and spherical (stereographic projection) models respectively.
Training Details We trained the Euclidean models with the hyperparameters chosen as reported
in (Klicpera et al., 2019). Namely, for GCN we use one hidden layer of size 64, dropout on the
embeddings and the adjacency of rate 0.5 as well as L2-regularization for the weights of the first
layer with λ = 0.02. Only for Cora-ML we had to adjust the regularization factor λ to 0.002 to
ensure similar scores as achieved in (Klicpera et al., 2019).
All Non-Euclidean models use biased-L2 regularization with α = 10 and λ = 2e - 2. Euclidean
models used L2 regularization with the same parameter λ. We used a combination of dropout and
dropconnect for the non-Euclidean models. All models have the same number of parameters. We
use 2 GCN layers, hidden dimension 64. Product models split hidden dimension into [32, 32] and
also input features equally. Non-Euclidean models do not use additional non-linearities. Euclidean
parameters use a learning rate of 0.01 for all models using ADAM. The curvatures are learned
using gradient descent with a learning rate of 0.01. We show the values of the learned curvatures
in appendix E. We use early stopping: we first train for a maximum of 2000 epochs, then we check
every 200 epochs for improvement in the validation cross entropy loss; if that is not observed, we
stop.
Node classification results. These are shown in table 2. It can be seen that our models are
competitive with the two Euclidean GCN considered (with or without non-linearities), showcasing
the benefit of our proposed architecture.
5	Conclusion
In this paper, we introduced a natural extension of graph convolutional networks to the stereographic
models of both positive and negative curvatures in a unified manner. We show how this choice
of models permits to smoothly interpolate between positive and negative curvature, allowing the
curvature of the model to be trained independent of an initial sign choice. We hope that our models
will open new exciting directions into non-Euclidean graph neural networks.
References
Sami Abu-El-Haija, Amol Kapoor, Bryan Perozzi, and Joonseok Lee. N-GCN: Multi-scale Graph
Convolution for Semi-supervised Node Classification. International Workshop on Mining and
Learning with Graphs (MLG), 2018.
Shun-ichi Amari and Hiroshi Nagaoka. Methods of information geometry, volume 191. American
Mathematical Soc., 2007.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.
Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. Geometric
deep learning: going beyond euclidean data. IEEE Signal Processing Magazine, 34(4):18-42,
2017.
9
Under review as a conference paper at ICLR 2020
Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann Lecun. Spectral networks and locally con-
nected networks on graphs. In International Conference on Learning Representations (ICLR2014),
CBLS, April 2014,pp. http-openreview, 2014.
Ines Chami, Rex Ying, Christopher R6, and Jure Leskovec. Hyperbolic graph convolutional neural
networks. arXiv preprint arXiv:1910.12933, 2019.
Jie Chen, Tengfei Ma, and Cao Xiao. Fastgcn: fast learning with graph convolutional networks via
importance sampling. ICLR, 2018.
Hyunghoon Cho, Benjamin DeMeo, Jian Peng, and Bonnie Berger. Large-margin classification in
hyperbolic space. In The 22nd International Conference on Artificial Intelligence and Statistics,
pp. 1832-1840, 2019.
Tim R. Davidson, Luca Falorsi, Nicola De Cao, Thomas Kipf, and Jakub M. Tomczak. Hyperspherical
Variational Auto-Encoders. Uncertainty in Artificial Intelligence (UAI), 856- 865, 2018.
Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on
graphs with fast localized spectral filtering. In Advances in neural information processing systems,
pp. 3844-3852, 2016.
Michael Defferrard, Nathanael Perraudin, Tomasz Kacprzak, and Raphael Sgier. Deepsphere: towards
an equivariant graph-based spherical cnn. In ICLR Workshop on Representation Learning on
Graphs and Manifolds, 2019. URL https://arxiv.org/abs/1904.05146.
Michel Deza and Monique Laurent. Geometry of Cuts and Metrics. Springer, Vol. 15, 1996.
Octavian Ganea, Gary BeCigneul, and Thomas Hofmann. Hyperbolic neural networks. In Advances
in neural information processing systems, pp. 5345-5355, 2018a.
Octavian-Eugen Ganea, Gary Becigneul, and Thomas Hofmann. Hyperbolic entailment cones
for learning hierarchical embeddings. In International Conference on Machine Learning, pp.
1632-1641, 2018b.
Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural
Message Passing for Quantum Chemistry. Proceedings of the International Conference on Machine
Learning, 2017.
Daniele Grattarola, Daniele Zambon, Cesare Alippi, and Lorenzo Livi. Learning graph embeddings
on constant-curvature manifolds for change detection in graph streams. stat, 1050:16, 2018.
Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recurrent
neural networks. In 2013 IEEE international conference on acoustics, speech and signal processing,
pp. 6645-6649. IEEE, 2013.
Mikhael Gromov. Hyperbolic groups. In Essays in group theory, pp. 75-263. Springer, 1987.
Albert Gu, Frederic Sala, Beliz Gunel, and Christopher Re. Learning mixed-curvature representations
in product spaces. Proceedings of the International Conference on Learning Representations, 2019.
Caglar Gulcehre, Misha Denil, Mateusz Malinowski, Ali Razavi, Razvan Pascanu, Karl Moritz
Hermann, Peter Battaglia, Victor Bapst, David Raposo, Adam Santoro, et al. Hyperbolic attention
networks. Proceedings of the International Conference on Learning Representations, 2018.
Matthias Hamann. On the tree-likeness of hyperbolic spaces. Mathematical Proceedings of the
Cambridge Philosophical Society, pp. 1-17, 2017. doi: 10.1017/S0305004117000238.
Matthias Hamann. On the tree-likeness of hyperbolic spaces. Mathematical Proceedings of the
Cambridge Philo- sophical Society, pp. 117, 2017.
William L. Hamilton, Rex Ying, and Jure Leskovec. Inductive Representation Learning on Large
Graphs. In Advances in Neural Information Processing Systems, 2017.
David K Hammond, Pierre Vandergheynst, and Remi Gribonval. Wavelets on graphs via spectral
graph theory. Applied and Computational Harmonic Analysis, 30(2):129-150, 2011.
10
Under review as a conference paper at ICLR 2020
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. Neural
collaborative filtering. In Proceedings of the 26th international conference on world wide web, pp.
173-182. International World Wide Web Conferences Steering Committee, 2017.
Mikael Henaff, Joan Bruna, and Yann LeCun. Deep convolutional networks on graph-structured data.
arXiv preprint arXiv:1506.05163, 2015.
Diederik P. Kingma and Jimmy Ba. ADAM: A method for stochastic optimization. ICLR, 2015.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks.
International Conference on Learning Representations, 2017.
Johannes Klicpera, Aleksandar Bojchevski, and StePhan Gunnemann. Predict then propagate: graph
neural networks meet personalized pagerank. International Conference on Learning Representa-
tions, 2019.
Dmitri Krioukov, Fragkiskos Papadopoulos, Maksim Kitsak, Amin Vahdat, and Marign BOgUna
Hyperbolic geometry of complex networks. Physical Review E, 82(3):036106, 2010.
Qi Liu, Maximilian Nickel, and Douwe Kiela. Hyperbolic graph neural networks. arXiv preprint
arXiv:1910.12892, 2019.
Emile Mathieu, Charline Le Lan, Chris J Maddison, Ryota Tomioka, and Yee Whye Teh. Hierarchical
representations with poincar\’e variational auto-encoders. arXiv preprint arXiv:1901.06033, 2019.
Jiri Matousek. Lecture notes on metric embeddings. 2013.
Andrew McCallum, Kamal Nigam, Jason Rennie, and Kristie Seymore. Automating the construction
of internet portals with machine learning. Information Retrieval, 3(2):127-163, 2000.
Galileo Namata, Ben London, Lise Getoor, and Bert Huang. Query-driven Active Surveying for
Collective Classification. International Workshop on Mining and Learning with Graphs (MLG),
2012.
Maximilian Nickel and Douwe Kiela. Learning continuous hierarchies in the lorentz model of
hyperbolic geometry. In International Conference on Machine Learning, 2018.
Maximillian Nickel and Douwe Kiela. Poincare embeddings for learning hierarchical representations.
In Advances in Neural Information Processing Systems, pp. 6341-6350, 2017.
Ivan Ovinnikov. Poincar\’e wasserstein autoencoder. arXiv preprint arXiv:1901.01427, 2019.
SamT Roweis and Lawrence K Saul. Nonlinear dimensionality reduction by locally linear embedding.
science, 290(5500):2323-2326, 2000.
Frederic Sala, Chris De Sa, Albert Gu, and Christopher Re. Representation tradeoffs for hyperbolic
embeddings. In International Conference on Machine Learning, pp. 4457-4466, 2018.
Rik Sarkar. Low distortion delaunay embedding of trees in hyperbolic plane. International Symposium
on Graph Drawing, pp. 355-366. Springer„ 2011.
Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lisa Getoor, Brian Gallagher, and T. Eliassi-Rad.
Collective Classification in Network Data. AI Magazine, 29(3):93-106, 2008.
Michael Spivak. A comprehensive introduction to differential geometry. volume four. 1979.
Joshua B Tenenbaum, Vin De Silva, and John C Langford. A global geometric framework for
nonlinear dimensionality reduction. science, 290(5500):2319-2323, 2000.
Alexandru Tifrea, Gary Becigneul, and Octavian-Eugen Ganea. Poincare glove: Hyperbolic word
embeddings. Proceedings of the International Conference on Learning Representations, 2019.
11
Under review as a conference paper at ICLR 2020
Abraham Ungar. Barycentric Calculus in Euclidean and Hyperbolic Geometry. World Scientific,
ISBN 9789814304931, 2010.
Abraham A Ungar. The hyperbolic Pythagorean theorem in the Poincare disc model of hyperbolic
geometry. TheAmericanmathematicalmonthly, 106(8):759-763, 1999.
Abraham A Ungar. Analytic hyperbolic geometry: Mathematical foundations and applications.
World Scientific, 2005.
Abraham Albert Ungar. A gyrovector space approach to hyperbolic geometry. Synthesis Lectures on
Mathematics and Statistics, 1(1):1-194, 2008.
Abraham Albert Ungar. Analytic Hyperbolic Geometry in N Dimensions: An Introduction. CRC
Press, 2014.
Abraham Albert Ungar. Novel tools to determine hyperbolic triangle centers. In Essays in Mathemat-
ics and its Applications, pp. 563-663. Springer, 2016.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,匕UkaSz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998-6008, 2017.
Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lid, and Yoshua
Bengio. Graph attention networks. International Conference on Learning Representations, 2018.
Jason Weston, Frederic Ratle, Hossein Mobahi, and Ronan Collobert. Deep learning via semi-
supervised embedding. In Neural Networks: Tricks of the Trade, pp. 639-655. Springer, 2012.
Richard C Wilson, Edwin R Hancock, Elzbieta Pekalska, and Robert PW Duin. Spherical and
hyperbolic embeddings of data. IEEE transactions on pattern analysis and machine intelligence,
36(11):2255-2269, 2014.
Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S Yu. A
comprehensive survey on graph neural networks. arXiv preprint arXiv:1901.00596, 2019.
Jiacheng Xu and Greg Durrett. Spherical latent spaces for stable variational autoencoders. In
Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp.
4503-4513, 2018.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? International Conference on Learning Representations, 2018.
Muhan Zhang and Yixin Chen. Link prediction based on graph neural networks. In Advances in
Neural Information Processing Systems, 2018.
12
Under review as a conference paper at ICLR 2020
A GCN - A Brief S urvey
A.1 Convolutional Neural Networks on Graphs
One of the pioneering works on neural networks in non-Euclidean domains was done by (Defferrard
et al., 2016). Their idea was to extend convolutional neural networks for graphs using tools from
graph signal processing.
Given a graph G = (V, A), where A is the adjacency matrix and V is a set of nodes, we define a signal
on the nodes of a graph to be a vector x ∈ Rn where xi is the value of the signal at node i. Consider
the diagonalization of the symmetrized graph Laplacian L = UΛUT, where Λ = diag(λ1, . . . , λn).
The eigenbasis U allows to define the graph Fourier transform X = UTX ∈ Rn.
In order to define a convolution for graphs, we shift from the vertex domain to the Fourier domain:
X ?g y = U ((UTχ) © (UTy))
Note that X = UTX and y = UTy are the graph Fourier representations and we use the element-wise
product © since convolutions become products in the Fourier domain. The left multiplication with U
maps the Fourier representation back to a vertex representation.
As a consequence, a signal X filtered by gθ becomes y = Ugθ (Λ)UT x where gθ = diag(θ) with
θ ∈ Rn constitutes a filter with all parameters free to vary. In order to avoid the resulting complexity
O(n), (Defferrard et al., 2016) replace the non-parametric filter by a polynomial filter:
K-1
gθ (Λ) = X θkΛk
k=0
where θ ∈ RK resulting in a complexity O(K). Filtering a signal is unfortunately still expensive since
y = Ugθ (Λ)UTx requires the multiplication with the Fourier basis U, thus resulting in complexity
O(n2). As a consequence, (Defferrard et al., 2016) circumvent this problem by choosing the
Chebyshev polynomials Tk as a polynomial basis, gθ (Λ) = PK=O θkTk (Λ) where Λ = λ2^ I.
As a consequence, the filter operation becomes y = PK=O θkTk (L)X where L = λ2L I. This
led to a K-localized filter since it depended on the K-th power of the Laplacian. The recursive
nature of these polynomials allows for an efficient filtering of complexity O(K|E|), thus leading
to an computationally appealing definition of convolution for graphs. The model can also be built
in an analogous way to CNNs, by stacking multiple convolutional layers, each layer followed by a
non-linearity.
A.2 Graph Convolutional Networks
(Kipf & Welling, 2017) extended the work of (Defferrard et al., 2016) and inspired many follow-up
architectures (Chen et al., 2018; Hamilton et al., 2017; Abu-El-Haija et al., 2018; Wu et al., 2019).
The core idea of (Kipf & Welling, 2017) is to limit each filter to 1-hop neighbours by setting K = 1,
leading to a convolution that is linear in the Laplacian L:
八	,Zi T
gθ ? X = θOX + θ1LX
They further assume λmax ≈ 2, resulting in the expression
gθ ? X = Θox 一 θιD-1AD-1X
To additionally alleviate overfitting, (Kipf & Welling, 2017) constrain the parameters as θO = -θ1 =
θ, leading to the convolution formula
gθ ? X = θ(I + D-1AD-1 )χ
Since I + D-2 AD-1 has its eigenvalues in the range [0, 2], they further employ a reparametrization
trick to stop their model from suffering from numerical instabilities:
八 k —1 7-^-1
gθ ? χ = θD 2 AD 2 χ
n
where A = A + I and Dii =	j =1 Aij .
13
Under review as a conference paper at ICLR 2020
Rewriting the architecture for multiple features X ∈ Rn×d1 and parameters Θ ∈ Rd1 ×d2 instead of
x ∈ Rn and θ ∈ R, gives
Z = ID - 2 AID- 1 xθ ∈ Rn×d2
The final model consists of multiple stacks of convolutions, interleaved by a non-linearity σ:
H(k+1) = σ (ID-2 AID-2H(k)Θ(k))
where H(0) = x and Θ ∈ Rn×dk.
The final output H(K) ∈ Rn×dK represents the embedding of each node i as hi = Hi. ∈ RdK and
can be used to perform node classification:
Y = softmax (ID- 1 AID-2H(K)W) ∈ Rn×L
where W ∈ RdK×L, with L denoting the number of classes.
In order to illustrate how embeddings of neighbouring nodes interact, it is easier to view the architec-
ture on the node level. Denote by N(i) the neighbours of node i. One can write the embedding of
node i at layer k + 1 as follows:
h(k+1) = σ (θ(I)	X	hjk)	∖
[3 PNj) ∣∣N(i)∣)
Notice that there is no dependence of the weight matrices Θ(l) on the node i, in fact the same
parameters are shared across all nodes.
In order to obtain the new embedding hi(k+1) of node i, we average over all embeddings of the
neighbouring nodes. This Message Passing mechanism gives rise to a very broad class of graph
neural networks (KiPf & Welling, 2017; Velickovic et al., 2018; Hamilton et al., 2017; Gilmer et al.,
2017; Chen et al., 2018; Klicpera et al., 2019; Abu-El-Haija et al., 2018).
To be more precise, GCN falls into the more general category of models of the form
zi(k+1) = AGGREGATE(k)({h(jk) :j ∈N(i)}; W(k))
hi(k+1) = COMBINE(k)(hi(k), zi(k+1); V (k))
Models of the above form are deemed Message Passing Graph Neural Networks and many choices
for AGGREGATE and COMBINE have been suggested in the literature (Kipf & Welling, 2017;
Hamilton et al., 2017; Chen et al., 2018).
B	Graph Embeddings in Non-Euclidean Geometries
In this section we will motivate non-Euclidean embeddings of graphs and show why the underlying
geometry of the embedding space can be very beneficial for its representation. We first introduce a
measure of how well a graph is represented by some embedding f : V -→ X , i 7→ f (i):
Definition 3. Given an embedding f : V -→ X, i 7→ f(i) of a graph G = (V, A) in some metric
space X, we call f a D-embedding for D ≥ 1 if there exists r > 0 such that
r ∙ dG(i,j) ≤ dχ(f(i),f j)) ≤ D ∙ r ∙ dα(i,j)
The infimum over all such D is called the distortion of f.
The r in the definition of distortion allows for scaling of all distances. Note further that a perfect
embedding is achieved when D = 1.
B.1 Trees and Hyperbolic Space
Trees are graphs that do not allow for a cycle, in other words there is no node i ∈ V for which there
exists a path starting from i and returning back to i without passing through any node twice. The
14
Under review as a conference paper at ICLR 2020
Figure 5: Euclidean embeddings of trees of different depths. All the four most inner circles are
identical. Ideal node embeddings should match in distance the graph metric, e.g. the distance between
the pink and green nodes should be the same as their shortest path length. Notice how we quickly run
out of space, e.g. the pink and green nodes get closer as opposed to farther. This issue is resolved
when embedding trees in hyperbolic spaces.
number of nodes increases exponentially with the depth of the tree. This is a property that prohibits
Euclidean space from representing a tree accurately. What intuitively happens is that "we run out of
space". Consider the trees depicted in fig. 5. Here the yellow nodes represent the roots of each tree.
Notice how rapidly we struggle to find appropriate places for nodes in the embedding space because
their number increases just too fast.
Moreover, graph distances get extremely distorted towards the leaves of the tree. Take for instance
the green and the pink node. In graph distance they are very far apart as one has to travel up all
the way to the root node and back to the border. In Euclidean space however, they are very closely
embedded in a L2-sense, hence introducing a big error in the embedding.
This problem can be very nicely illustrated by the following theorem:
Theorem 7. Consider the tree K1,3 (also called 3-star) consisting of a root node with three children.
Then every embedding {x1,..., x4} with Xi ∈ Rk achieves at least distortion √3 for any k ∈ N.
Proof. We will prove this statement by using a special case of the so called Poincare-type inequali-
ties (Deza & Laurent, 1996):
For any b1, . . . , bk ∈ R with Pik=1 bi = 0 and points x1, . . . , xk ∈ Rn it holds that
k
X bibj||xi-xj||2 ≤0
i,j=1
Consider now an embedding of the tree x1, . . . , x4 where x1 represents the root node. Choosing
b1 = -3 and bi = 1 for i 6= 1 leads to the inequality
||x2 - x3||2 + ||x2 - x4||2 + ||x3 - x4||2 ≤ 3||x1 - x2||2 + 3||x1 - x3||2 + 3||x1 - x4||2
The left-hand side of this inequality in terms of the graph distance is
dG(2,3)2+dG(2,4)2+dG(3,4)2 =22+22+22 = 12
and the right-hand side is
3 ∙ dG(1,2)2 + 3 ∙ dG(1, 3)2 + 3 ∙ dɑ(1,4)2 = 3 + 3 + 3 = 9
As a result, we always have that the distortion is lower-bounded by vz12 = √	□
Euclidean space thus already fails to capture the geometric structure of a very simple tree. This
problem can be remedied by replacing the underlying Euclidean space by hyperbolic space.
Consider again the distance function in the Poincare model, for simplicity with C = 1:
||x y ||2
dP (x, y) = Cosh	(1 + 2(1 -∣∣χ∣l2 )(1 -∣∣y∣l2))
15
Under review as a conference paper at ICLR 2020
Assume that the tree is embedded in the same way as in fig. 5, just restricted to lie in the disk of
radius √1c = 1. Notice that as soon as points move closer to the boundary (||x|| → 1), the fraction
explodes and the resulting distance goes to infinity. As a result, the further you move points to the
border, the more their distance increases, exactly as nodes on different branches are more distant to
each other the further down they are in the tree. We can express this advantage in geometry in terms
of distortion:
Theorem 8.	There exists an embedding x1 , . . . , x4 ∈ P2 for K1,3 achieving distortion 1 + for
> 0 arbitrary small.
Proof. Since the PoinCare distance is invariant under Mobius translations We can again assume that
x1 = 0. Let us place the other nodes on a circle of radius r. Their distance to the root is now given as
dp(xi, 0) = cosh-1 f 1 + 2 "：i" ∣∣2 ) = cosh-1 f 1 + 2 J-2
1 - ||xi||2	1 - r2
By invariance of the distance under centered rotations We can assume W.l.o.g. x2 = (r, 0). We further
embed
• x3
This procedure gives:
dP(x2 , x3) = cosh-1
cosh-1
1+2(1
3r2	)
—r2)2)
If We let the points noW move to the border of the disk We observe that
cosh-1 (1 + 2C 3r；2)一
________k	(1-r2)21 r→→ 2
Cosh 1 (1 + 2 1-『2 )
But this means in turn that we can achieve distortion 1 + E for e > 0 arbitrary small. QED. □
The tree-likeliness of hyperbolic space has been investigated on a deeper mathematical level. (Sarkar,
2011) show that a similar statement as in theorem 8 holds for all weighted or unweighted trees. The
interested reader is referred to (Hamann, 2017; Sarkar, 2011) for a more in-depth treatment of the
subject.
Cycles are the subclasses of graphs that are not allowed in a tree. They consist of one path that
reconnects the first and the last node: (v1, . . . , vn, v1). Again there is a very simple example of a
cycle, hinting at the limits Euclidean space incurs when trying to preserve the geometry of these
objects (Matousek, 2013).
Theorem 9.	Consider the cycle G = (V, E) of length four. Then any embedding (x1, . . . , x4) where
Xi ∈ Rk achieves at least distortion √2.
Proof. Denote by x1, x2, x3, x4 the embeddings in Euclidean space where x1, x3 and x2, x4 are the
pairs without an edge. Again using the Poincare-type inequality with b1 = b3 = 1 and b2 = b4 = -1
leads to the short diagonal theorem (Matousek, 2013):
||x1 -x3||2 + ||x2 -x4||2 ≤ ||x1 -x2||2 + ||x2 -x3||2 + ||x3 -x4||2 + ||x4 - x1||2
The left hand side of this inequality in terms of the graph distance is dG(1, 3)2 + dG(2, 4)2 =
22 + 22 = 8 and the right hand side is 12 + 12 + 12 + 12 = 4.
Therefore any embedding has to shorten one diagonal by at least a factor √2.	□
16
Under review as a conference paper at ICLR 2020
It turns out that in spherical space, this problem can be solved perfectly in one dimension for any
cycle.
Theorem 10.	Given a cycle G = (V, E) of length n, there exists an embedding {x1, . . . , xn}
achieving distortion 1.
Proof. We model the one dimension spherical space as the circle S1. Placing the points at angles 卑
and using the arclength on the circle as the distance measure leads to an embedding of distortion 1 as
all pairwise distances are perfectly preserved.	□
Notice that we could also use the exact same embedding in the two dimensional stereographic
projection model with C = 1 and We would also obtain distortion 1. The difference to the POinCare
disk is that spherical space is finite and the border does not correspond to infinitely distant points. We
therefore have no since we do not have to pass to a limit.
C S pherical Space and its Gyrostructure
Contrarily to hyperbolic geometry, spherical geometry is not only in violation with the fifth postulate
of Euclid but also with the first. Notice that, shortest paths are not unique as for antipodal (oppositely
situated) points, we have infinitely many geodesics connecting the two. Hence the first axiom does
not hold. Notice that the third postulate holds as we stated it but it is sometimes also phrased as: "A
circle of any center and radius can be constructed". Due to the finiteness of space we cannot have
arbitrary large circles and hence phrased that way, the third postulate would not hold.
Finally, we replace the fifth postulate by:
• Given any straight line l and a point p not on l, there exists no shortest line g passing through p
but never intersecting l.
The standard model of spherical geometry suffers from the fact that its underlying space depends
directly on the curvature κ through a hard constraint -κhx, xi = 1 (similarly to the Lorentz model of
hyperbolic geometry). Indeed, when κ → 0, the domain diverges to a sphere of infinite radius which
is not well defined.
For hyperbolic geometry, we could circumvent the problem by moving to the Poincare model, which
is the stereographic projection of the Lorentz model, relaxing the hard constraint to an inequality. A
similar solution is also possible for the spherical model.
C.1 Stereographic Projection Model of the Sphere
In the following we construct a model in perfect duality to the construction of the Poincare model.
Fix the south pole Z = (0,-表)of the sphere of curvature κ > 0, i.e. of radius R := K-2. The
stereographic projection is the map:
Φ : SnR	-→ Rn
x0 7→ x
1
1 + √κxn+ι
x01:n
with the inverse given by
where we define λK = 1+j∣x∣∣2.
Again we take the image of the sphere SR under the extended projection Φ((0,..., 0, -ɪ)) = 0,
leading to the stereographic model of the sphere. The metric tensor transforms as:
giκj = (λxκ)2δij
17
Under review as a conference paper at ICLR 2020
C.2 Gyrovector Space in the Stereographic Model
C.2.1 Proof of Theorem 1
UsingCauchy-SchWarz's inequality, we have A := 1 - 2κxTy + κ2∣∣x∣∣2∣∣y∣∣2 ≥ 1-2|矶Xkkyk +
κ2∣∣x∣∣2∣∣y∣∣2 = (1 - ∣κ∣kx∣∣kyk)2 ≥ 0. Since equality in the Cauchy-Schwarz inequality is only
reached for colinear vectors, we have that A = 0 is equivalent to κ > 0 and X = y∕(κ∣∣yk2).
C.2.2 Proof of Theorem 2
Let us start by proving that for x ∈ Rn and v ∈ TxRn the exponential map is given by
K	λX (c°sκ CX||v||) - √κχT荷SinK Cx||v||))X + √κSinK Cx||v||)πv∣τ
expx(* v) =	1 + (λχ - 1) COSk (λκ∣∣v∣∣) - √KλχxT荷 SinK (λχ∣∣v∣∣)
(17)
Indeed, take a unit speed geodesic γx,v(t) starting from x with direction v. Notice that the unit speed
geodesic on the sphere starting from x0 ∈ Sn-I is given by Γχ0,v0 (t) = x0 cosκ(t) + 表 si□κ(t)v0.
By the Egregium theorem, we know that Φ(γx,v(t)) is again a unit speed geodesic in the sphere
where Φ-1 : x → x0 = (λχx,表(λK - 1)). Hence Φ(γχ,v(t)) is of the form of Γ for some x0 and
v0 . We can determine those by
x0 = Φ-1(γ(0)) = Φ-1(x)=卜 Xx, √κ (λK - 1)
∂Φ-1 (y)
v0 = Γ(0) = —dy(y) γ (0)γ(0)
Notice that VxλK = -κ(λχ)2x and we thus get
0 _ - -2κ(λX)2xTvx + λKv∖
=1	-√K(λχ)2xT V	)
We can obtain γx,v again by inverting back by calculating γx,v (t) = Φ(Γx0,v0 (t)), resulting in
γx,v(t)
(λχ COSk(t) - √κ(λχ)2xTVSinK (t))x + √1κλχ SinK (t)v
1 + (λχ - 1) COSk (t) - √κ(λχ)2xTV SinK (t)
Denoting gX(v, v) = ∣∣v∣∣2λχ we have that expK(v) = Yx	ι V ( ∕gX(v, V)) which concludes
,√gχ(v，V)
the proof of the above formula of the exponential map. One then notices that it can be re-written
in terms of the κ-addition. The formula for the logarithmic map is easily checked by verifying that
it is indeed the inverse of the exponential map. Finally, the distance formula is obtained via the
well-known identity dκ(x, y) = k logx(y)kx where ∣∣vkx = √gx(v, v).
Note that as expected, expKx(v) →K→0 x + v, converging to the Euclidean exponential map.

C.2.3 Proof of Theorem 3
We first compute a Taylor development of the κ-addition w.r.t κ around zero:
X㊉K y
(1 - 2κxTy - K∣∣y∣∣2)x + (1 + κ∣∣x∣∣2)y
1 - 2κxTy + κ2∣∣x∣∣2∣∣y∣∣2
[(1 - 2κxTy - κ∣∣y∣∣2)x + (1 + κ∣∣x∣∣2)y][1 + 2κxTy + O(κ2)]
(18)
(19)
(1 - 2κxTy - κ∣∣y∣∣2)x + (1 + κ∣∣x∣∣2)y + 2κxTy[x + y] + O(κ2)	(20)
(1 - κ∣∣y∣∣2)x +(1 + κ∣∣x∣∣2)y + 2κ(xTy)y + O(κ2)	(21)
x+y+κ[kxk2y- kyk2x + 2(xT y)y] + O(κ2).	(22)
18
Under review as a conference paper at ICLR 2020
We then notice that using the Taylor of ∣∣ ∙ ∣∣2, given by ∣∣x + v∣∣2 = ∣∣xk2 +(x, Vi + O(IIvk2) for
v → 0, we get
kx ㊉K yk = kx + yk + KhkXk2y - kyk2χ + 2(XTy)y,X + yi + O(κ2)	(23)
= ∣x+y∣ + κ(xT y)∣x + y∣2 + O(κ2).	(24)
Finally Taylor developments of ta□κ(∣κ∣ 1 U) and ∣κ∣- 1 tan-1(u) w.r.t K around 0 for fixed U yield
For κ → 0+,	tanκ(∣κ∣2U) = K- 1 tan(κ2U)	(25)
=κ- 1 (κ 2 u + κ 2 u3∕3 + O(K 5)	(26)
= U + KU3/3 + O(K2).	(27)
For κ → 0-, tanκ(∣κ∣2u) = (—κ)- 1 tanh((-K) 1 U)	(28)
=(-k)- 1((-K) 1 U - (-k)3U3/3 + O(K2)	(29)
= U + KU3/3 + O(K2).	(30)
The left and right derivatives match, hence even though k → |k| 2 is not differentiable at K = 0, the
function K → ta□κ(∣K∣ 1 u) is. A similar analysis yields the same conclusion for K → |k|- 1 tan-1 (U)
yielding
For k → 0,	|k|- 1 tan-1(U) = U - ku3∕3 + O(k2).	(31)
Since a composition of differentiable functions is differentiable, we consequently obtain that 0κ,
expκ , logκ and dκ are differentiable functions of K, under the assumptions on x, y, v stated in
Theorem 3. Finally, the Taylor development of dκ follows by composition of Taylor developments:
dκ(χ, y) = 2∣k∣-2 tan-1(k(-X)㊉ K y∣∣)
=2(kx-yk + K((-x)T y)kx - yk2)(1 - (K/3)(kx - yk + O(K))2) + O(K2)
=2(kx-yk + K((-x)T y)kx - yk2)(1 - (K/3)kx - yk2) + O(K2)
=2kχ - yk - 2κ ((XTy)kχ - yk2 + kχ - yk3∕3) + Ο(κ2).
C.2.4 Proof of Theorem 4
If A = In then for all i we have j Aij = 1, hence
(In 区 X)i∙ = 2 0κ
X	δijλxj	χ .
j Pk δik(KkT) j
2 0κ (2 0κ Xi)
χi
(X)i∙.
(32)
(33)
(34)
(35)
(36)
For associativity, we first note that the gyromidpoint is unchanged by a scalar rescaling of A. The
property then follows by scalar associativity of the K-scaling.

C.2.5 Proof of Theorem 5
It is proved in (Ungar, 2005) that the gyromidpoint commutes with isometries. The exact same proof
holds for positive curvature, with the same algebraic manipulations. Moreover, when the matrix A is
right-stochastic, for each row, the sum over columns gives 1, hence our operation K reduces to a
gyromidpoint. As a consequence, our K commutes with isometries in this case. Since isometries
preserve distance, we have proved the theorem.
19
Under review as a conference paper at ICLR 2020
C.2.6 Proof of Theorem 6
We begin our proof by stating the left-cancellation law:
X ㊉K (-χ ㊉K y) = y	(37)
and the following simple identity stating that orthogonal maps commute with κ-addition
Rx ㊉K Ry = R(χ ㊉K y),	∀R ∈ O(d)	(38)
Next, We generalize the gyro operator from Mobius gyrovector spaces as defined in Ungar (2008):
gyr[u, v]w := -(U ㊉K V)㊉K (U ㊉K (V ㊉K W))	(39)
Note that this definition applies only for u, v, w ∈ stdK for Which the κ-addition is defined (see
theorem 1). FolloWing Ungar (2008), We have an alternative formulation (verifiable via computer
algebra):
AU + BV
gyr[u, v]w = W + 2-----D----.
Where the quantities A, B, D have the folloWing closed-form expressions:
A = -κ2hu, WikVk2 - κ(v, Wi + 2κ2hu, Vi ∙(v, w),
(40)
(41)
B = -κ2hV, WikUk2 + κhU, Wi,	(42)
D = 1 - 2κhU, Vi + κ2kUk2kVk2.	(43)
We then have the folloWing relations:
Lemma 11. For all U, V, W ∈ stdK for which the κ-addition is defined we have the following relations:
i) gyration is a linear map, ii) U ㊉K V = gyr[u, v](v ㊉K U), iii) -(Z ㊉K U)㊉K (Z ㊉K V) =
gyr[z, u](-u ㊉K V), iv) k^yr[u, v]w∣∣ = ∣∣w∣∣.
Proof. The proof is similar With the one for negative curvature given in Ungar (2008). The fact that
gyration is a linear map can be easily verified from its definition. For the second part, We have
-gyr[U, v](v ㊉ KU) = gyr[U, v](一(v ㊉ KU)) = -(U ㊉ KV)㊉ K(U ㊉ K(V ㊉ k(一(v ㊉ KU)))) = -(U ㊉ KV)
(44)
Where the first equality is a trivial consequence of the fact that gyration is a linear map, While the last
equality is the consequence of left-cancellation laW.
The third part folloWs easily from the definition of the gyration and the left-cancellation laW. The
fourth part can be checked using the alternate form in eq. (40).	□
We noW folloW Ungar (2014) and describe all isometries of stdK spaces:
Theorem 12. Any isometry φ of stdK can be uniquely written as:
φ(x) = z ㊉K Rx, where Z ∈ st£, R ∈ O(d)	(45)
The proof is exactly the same as in theorems 3.19 and 3.20 of Ungar (2014), so We Will skip it.
We can now prove the main theorem. Let φ(x) = Z ㊉K RX be any isometry of st£, where R ∈ O(d)
is an orthogonal matrix. Let us denote by V := Pin=1 αi logxK(xi). Then, using lemma 11 and the
formula of the log map from theorem 2, one obtains the following identity:
n	λK
Eai logφ(x)(φ(Xi)) = λκ^gyr[z, Rx]RV
i=1	φ(x)
(46)
20
Under review as a conference paper at ICLR 2020
and, thus, using the formula of the exp map from theorem 2 we obtain:
tgφ(x)({φ(xi)}; {αi}) = φ(x)㊉K gyr[z, Rx]R(-x ㊉K expX(v))	(47)
Using eq. (39), we get that
gyr[z, Rx]Rw =	-φ(x)㊉K	(Z	㊉K	(RX ㊉K Rw)),	∀w	∈	stK	(48)
giving the desired
tgφ(x)({φ(xi)}; {αi}) = Z ㊉K RexpX(v) = φ (tgχ({xi}; {a，}))	(49)

D Logits
The final element missing in the κ-GCN is the logit layer, a necessity for any classification task. We
here use the formulation of (Ganea et al., 2018a). Denote by {1, . . . , K} the possible labels and let
ak ∈ Rd, bk ∈ R and x ∈ Rd. The output of a feed forward neural network for classification tasks is
usually of the form
p(y = k|x) = softmax(hak, xi - bk)
In order to generalize this expression to hyperbolic space, the authors of (Ganea et al., 2018a) realized
that the term in the softmax can be rewritten as
hak, xi - bk = sign(hak,xi - bk)||ak||d(x, Hak,bk)
where Ha,b = {x ∈ Rd :(x, a〉一 b = 0} = {x ∈ Rd :(一p + x, a)= 0} = Ha,p with P ∈ Rd.
As a first step, they define the hyperbolic hyperplane as
Ha,p = {x ∈ stK(-P ㊉K x, a) = 0}
where now a ∈ TpstdK and p ∈ stdK. They then proceed proving the following formula:
〃/小	1	∙ k-1 (	2√-K|h —p ㊉ K x, ai∣ ʌ	zi-m
dK (x, Ha,p) = √-κSi	((1 + κ∣∣-P ㊉ K x∣∣2)∣∣a∣∣)	()
Using this equation, they were able to obtain the following expression for the logit layer:
p(y=k|x)=Softmax (⅜⅛k Sinh-1 ((r!√-¾ρ⅞xx⅛∏)),	(51)
where ak ∈ TOst£ = Rd, X ∈ st£ and Pk ∈ st£. Combining all these operations leads to the
definition of a hyperbolic feed forward neural network. Notice that the weight matrices W and the
normal vectors ak live in Euclidean space and hence can be optimized by standard methods such as
ADAM (Kingma & Ba, 2015).
For positive curvature κ > 0 we use in our experiments the following formula for the softmax layer:
Ppy =k|x) = Softmax (⅛k sin-1 ((τ+√⅛⅞⅞⅛k∏)),	(52)
which is inspired from the formula i Sin(X) = sinh(ix) where i := √-1. However, we leave for
future work the rigorous proof that the distance to geodesic hyperplanes in the positive curvature
setting is given by this formula.
E More Experimental Details
We here present training details for the node classification experiments.
We closely follow the training and evaluation scheme from previous work, e.g. (Klicpera et al., 2019).
We split the data into training, early stopping, validation and test set. Namely we first split the dataset
into a known subset of size nknown and an unknown subset consisting of the rest of the nodes. For
21
Under review as a conference paper at ICLR 2020
Dataset	Type	Classes	Features	Nodes	Edges	Label rate	Avg sp
Citeseer	Citation	6	3703	2110	3668	0.036	9.31
Cora-ML	Citation	7	2879	2810	7981	0.047	5.27
Pubmed	Citation	3	500	19717	44324	0.003	6.34
MS-ACademiC	Co-author	15	6805	18333	81894	0.0016	5.34
Table 3: Summary statistics for the four datasets, where sp denotes shortest path.
Figure 6: Histogram of node degrees
all the graphs we use nknown = 1500 except for MS Academics, where we use nknown = 5000.
The known subset is further split into a training set consisting of 20 data points per label, an early
stopping set of size 500 and a validation set of the remaining nodes. Notice that the whole structure
of the graph and all the node features are used in an unsupervised fashion since the embedding of a
training node might for instance depend on the embedding of a node from the validation set. But
when calculating the loss, we only provide supervision with the training data.
The unknown subset serves as the test data and is only used for the final evaluation of the model.
Hyperparameter-tuning is performed on the validation set. We further use early stopping in all the
experiments. We stop training as soon as the early stopping cross entropy loss has not decreased in
the last npatience = 200 epochs or as soon as we have reached nmax = 2000 epochs. The model
chosen is the one with the highest accuracy score on the early stopping set. For the final evaluation
we test the model on 10 different data splits and report mean accuracy and bootstrapped confidence
intervals. We use the described setup for both the Euclidean and non-Euclidean models to ensure a
fair comparison.
Learned curvatures .
Citeseer:
Hyp GCN: Trained curvature, average curvature over all runs: -1.057 +-0.03
Sphr GCN: Trained curvature, average curvature over all runs: 0.951 +-0.019
Prod GCN: Trained curvatures, average curvatures: [1.331, -0.91]
Cora
Hyp GCN: Trained curvature, average curvature: -1.127 +-0.011
Sphr GCN: Trained curvature, average curvature: 0.857+-0.013
22
Under review as a conference paper at ICLR 2020
Prod GCN: Trained curvature, average curvature: [-1.03, -1.01]
Pubmed:
Hyp GCN: Trained curvature, average curvature: 1.123 +- 0.01
Sphr GCN: Trained curvature, average curvature: 0.896 +- 0.008
Prod GCN: Not training curvature, fixed to [-1, -1]
Ms-Academic
Hyp GCN: Trained curvature, average curvature: 1.26 +- 0.09
Sphr GCN: Trained curvature, average curvature: 0.8 +- 0.07
Prod GCN: Not training curvature, fixed to [-1, -1]
F Graph Curvature Estimation Algorithm
We used the following procedure to estimate the curvature of a dataset developed by Gu et al. (2019):
1.	Fix a node m ∈ G and sample two neighbouring nodes a, b ∈ G uniformly. Further sample
an additional reference node c ∈ G uniformly (again avoiding m = c).
2.	Calculate ψ(m; a, b; C) = 2dG(ab °G(a, m) + dGbc- - (dG(a,b)+dG(a,C)))
3.	Reiterate the above sampling niter times and obtain an average curvature at node m.
4.	Do this procedure for every node m ∈ G.
23