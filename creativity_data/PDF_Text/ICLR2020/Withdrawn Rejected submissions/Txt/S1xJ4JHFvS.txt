Under review as a conference paper at ICLR 2020
Acutum: When Generalization Meets Adapt-
ABILITY
Anonymous authors
Paper under double-blind review
Ab stract
In spite of the slow convergence, stochastic gradient descent (SGD) is still the
most widely used optimization method due to its outstanding generalization abil-
ity and simplicity. On the other hand, adaptive methods have attracted much more
attention of optimization and machine learning communities, both for the leverage
of life-long information and for the deep and fundamental mathematical theory.
Taking the best of both worlds is the most exciting and challenging question in
the field of optimization for machine learning. In this paper, we revisit existing
adaptive gradient methods from a novel point of view, which reveals a fresh un-
derstanding of momentum. Our new intuition empowers us to remove the second
moment in Adam without the loss of performance. Based on our view, we propose
a novel optimization method, the acute adaptive momentum (Acutum). To the best
of our knowledge, Acutum is the first adaptive gradient method without second
moments. Experimentally, we demonstrate that our method has a faster conver-
gence rate than Adam/Amsgrad, and generalizes as well as SGD with momentum.
We also provide a convergence analysis of our proposed method to complement
our intuition.
1	Introduction
With the rapid development of neural network architectures (Goodfellow et al., 2016), training algo-
rithms have attracted much more attention in the modern machine learning community. Due to the
network size and data amount increasing dramatically, calculating the full gradient of data and im-
plementing the full gradient descent (GD) become computationally expensive. Therefore, stochastic
gradient descent (SGD) (Robbins & Monro, 1951) becomes the most practical optimization method
for training deep neural networks (DNNs). In each iteration, SGD samples mini-batch data and
computes the gradient corresponding to the mini-batch. Although SGD is computationally afford-
able, it needs a mechanism of fine-tune the learning rate, e.g., linear decay or exponential decay, to
converge efficiently. In fact, it is pretty brittle to tune the learning rate dynamically with SGD.
To release the learning rate tuning burden of SGD and accelerate its convergence, several adaptive
variants of SGD were proposed, including Adagrad (Duchi et al., 2011), Adadelta (Zeiler, 2012),
RMSProp (Hinton et al., 2012), Adam (Kingma & Ba, 2014), etc. The descent direction is element-
wise automatically adapted by the first moment and the second moment with an exponential average
of gradients. Such a descent direction has achieved great improvements in practice, while the funda-
mental interpretation is still unclear. Besides, the theoretical regret analysis of these online learning
algorithms has become completed gradually in convex objective settings. Due to the fast decay of
exponential moving average, Adam cannot converge even in simple convex cases as shown in (Reddi
et al., 2019). Amsgrad (Reddi et al., 2019) addressed this issue by keeping an extra non-decreasing
sequence to buffer the fast decay.
On the other hand, although adaptive methods are easy to use and fast to converge, the generaliza-
tion results cannot be as good as SGD (Wilson et al., 2017). Various works (Chen & Gu, 2018;
Luo et al., 2019) have been proposed to make algorithms not only converge faster but also has good
generalization. However, these algorithms still inherit the second moment following Adam. Differ-
ent from existing works, we provide a novel view for this problem by revisiting adaptive algorithms
Through the view of direction angle between the descent direction and the momentum, we are able
to demonstrate a fresh understanding of momentum.
1
Under review as a conference paper at ICLR 2020
In summary, we propose a new algorithm, called acute adaptive momentum (Acutum), to find adap-
tive gradients with only the first moment. To remove the second moments safely, we investigate their
properties by reformulating the proximal subproblem of Adagrad. The main observation is that sec-
ond moments essentially penalize the projection of the current descent direction on the exponential
moving average of previous gradients. The new intuition allows us to remove second moments in
Adam without any loss of performance. To the best of our knowledge, it is the first time to remove
the second moments for adaptive gradient methods. As shown by our experiment, the proposed
algorithm (Acutum) outperforms Adam/Amsgrad in wall-clock time while almost preserving the
generalization ability of SGD.
In particular, we make the following contributions:
•	We propose a new optimization algorithm called Acutum, which doesn’t need to compute
the second moments. To the best of our knowledge, it is the first time to obtain the adaptive
gradients with only the first moments. The intuition behind our algorithm is to remove
second moments by attaching their purpose to the first moment through Adagrad’s proximal
subproblem inspiration.
•	We provide a regret convergence analysis of Acutum on the convex setting, based on the
analysis of (Kingma & Ba, 2014; Reddi et al., 2019; Chen & Gu, 2018), and prove a data-
dependent regret bound, complementing our intuition.
•	We also provide various experiments about our proposed Acutum on training modern deep
models. The experimental results empirically show that Acutum takes the best from both
Adam-type methods and SGD.
Notations. In the rest of this paper, We assume any decision variables θ ∈ Rd where d denotes the
dimension of parameters. We denote the l2 norm of a given vector θ by kθ k = Pid=1 θi2 . With
slightly abuse of notation, we write arithmetic symbols as element-wise operations for vectors, e.g.,
a2 = [a21, a22, . . .]T, a/b = [a1/b1, a2/b2, . . .]T . We denote by bxc the greatest integer less than or
equal to the real number x. Given any integers x, y, where y 6= 0, we denote by x (mod y) the
remainder of the Euclidean division of X by y. In the optimization setting, we denote by fi(∙) the
loss function when we feed the i-th training data into our model, we also denote fs(∙) when we
feed a collection of samples, i.e., fs(∙) := P∈ fi(∙). Given any point θ ∈ Rd and any convex set
X ⊆ Rd, we denote by Πχ (θ) the projection of θ onto X, i.e., argminμ∈χ ∣∣μ - θ∣∣.
2	Related Work
The increasing size of deep neural networks and the amount of train data have dramatically induced
the difficulty of training neural networks. The huge parameter size makes the landscape of neural
networks more non-convex, and the amount of train data requires more computation resources to cal-
culate the gradient. Therefore, all the facts urge researchers to design faster optimization algorithms
for deep neural networks, while maintaining a reasonable generalization ability.
SGD-momentum is widely used in training large-scale neural networks while the excellent gener-
alization ability of SGD is vulnerable to its learning rate. Meanwhile, researchers began to focus
on the design of adaptive gradient methods for a fast and simple optimizer. Adagrad proposed in
(Duchi et al., 2011) introduced the second moment to obtain a self-adaptive learning rate, and hence
freed researchers’ hands of parameter tuning. The update rules of Adagrad can be formulated as
follows:
θt+ι = θt - αt -g匚,
√vt
where gt denotes the stochastic gradient, vt is the accumulation of gradient’s second moments, i.e.,
Vt = 1 PT=ι gT, and at is the decreasing learning rate with at = O(1∕√t). Theoretically, Ada-
grad improves the convergence of regret from O(√d∕√T) to O(1∕√T) for the convex objectives
with sparse gradients. However, in practice, people realize that adaptive gradient of Adagrad, i.e.,
gt∕√Vt goes to zero very quickly due to Vt accumulating to large number quickly as the algorithm
proceeds. To make it through, RMSProp (Hinton et al., 2012) introduced the exponential decay in
the second moment to control the accumulation speed of second moment in Adagrad. Furthermore,
2
Under review as a conference paper at ICLR 2020
(Kingma & Ba, 2014) incorporated the momentum into RMSProp and proposed Adam. The detailed
procedure of Adam can be formulated as:
θt+1 = θt - αt J-,	(I)
√vt
where mt is the exponential decay of momentum, i.e., mt = Ptτ=1(1 - β1)β1t-τ gτ, and vt is the
exponential decay of second moment, i.e., vt = Ptτ=1(1 - β2)β2t-τ gτ2 .
The faster convergence, robust hyper-parameters and good performance on CV and NLP’s tasks
make Adam become one of the most successful optimizers these years. However, (Reddi et al.,
2019) showed Adam does not converge in certain convex cases, and proposed Amsgrad to correct
the direction of Adam with imposing an increasing sequence of the second moment.
Although convergence, in theory, is fixed and the training speed is still fast, the generalization ability
of adaptive algorithms is still worse than SGD with momentum. Thus, there are a lot of studies to
improve the generalization performance of Adam types algorithms. Almost all of these works try
to make some connections between Adam and SGD-momentum such that the proposed algorithm
converges faster like Adam while generalizing better as SGD. For example, (Zhang et al., 2017)
proposed a normalized direction-preserving Adam (ND-Adam). To make the direction of the gra-
dients preserved, the authors adjusted the learning rate to each weight vector as a whole, instead
of each individual weight. It could be thought as an adjustment of learning rate dynamically in
each iteration, instead of the learning rate decay. (Loshchilov & Hutter, 2017) proposed to modify
Adam to decay all weights the same regardless of the gradient variances. (Keskar & Socher, 2017)
proposed to improve the generalization performance by investigating a hybrid strategy that begins
training with an adaptive method and switches to SGD when appropriate. The experimental results
show that it obtains a better generalization with the convergence speed sacrificing. With a similar
insight, (Luo et al., 2019) introduced the element-wise clipping technique to the second moment
and proposed AdaBound, which allows the algorithm behavior switched from a clipped Adam to
SGD-momentum as the algorithm proceeds. Besides, (Chen & Gu, 2018) proposed Padam which
tried to find the balanced state between Adam and SGD-momentum through tuning the order of the
second moment in the update paradigm.
All the aforementioned methods try to find the connection between Adam and SGD-momentum,
thus all these algorithms take the second moment adaptation as a grant. The difference between
previous works and ours is that we suggest to remove the second moment and propose an angle
based algorithm, by revisiting the original idea of second moment adaptation. Such intuition could
be of independent interest in the field of the optimization community.
3	The Acute Adaptive Momentum Algorithm
In this section, we first revisit existing adaptive algorithms from a novel point of view, then intro-
duce the intuition and provide the algorithm procedure of our method Acutum. The performance
guarantees established in Section 4 and evaluation results in Section 5 validate our insights.
From the above section, one may notice that almost all the adaptive methods utilize second moments
to adjust the individual magnitude of the updates. However, the efficient calculation of second
moments, see Equation (1), is only proposed in Adagrad (Duchi et al., 2011). Let’s take a look at
the original update paradigm of Adagrad:
Θt+1 = θt- atG-1/2gt,	⑵
where Gt = Ptτ =1 gτ gτT , and gτ denotes the stochastic gradient calculated at iteration τ. Recall
that in each iteration, SGD seeks the optimal solution of a subproblem constructed from a quadratic
approximation of the objective. Similarly, we can write the update procedure of Adagrad in Equa-
tion (2) as
Θt+1 = arg min (θ - θt)T gt + 1— (θ - θt)T G1/2 (θ - θt)	(3)
θ X--------{z---}	2αt
Ti	'-------------{------------}
1	T2
where the first-order penalty T1 leads the parameter θt to the opposite direction to the current gradi-
ent gt. Note that Adagrad can adaptively adjust each individual dimension of θt through introducing
3
Under review as a conference paper at ICLR 2020
Gt in the quadratic term T2, while SGD cannot. Ifwe substitute the definition of Gt back into Equa-
tion (3), T2 can be formulated as:
T2
2α (θ -Bt)T G1/2 (θ - θt) = 2α (θ -Bt)T (X gτgT!	(θ - θt).
(4)
By Young’s inequality (Young, 1912), we can obtain an upper bound of T2 as follows:
T2 ≤ 8⅛ kθ - θtk2+ 2⅛ XM- θt)TgT『.
、	一、.一	/	T =1
{z I	.Z	}
T3
(5)
{z
T4
Combining Equation (5) and Equation (3), we can approximate the update rule of Adagrad as fol-
lows:
Bt+1 ≈ arg min
θ
(θ - θt)T gt+ g— kθ - θtk
1-----{------'	8αt
Ti	'-------{------
1	T3
2+
W X W θt)T gτ『
t τ=1
1----------------{----------------'
T4
We can think the upper bound of the Adagrad subproblem as the combination of the standard first
and second order penalty as T1 and T3, with a penalty for the projections of current descent B - Bt
on the previous gradients gτ, i.e., T4.
With these observations in mind, a natural question comes out: how can we understand T4 in neural
network training tasks or in general online learning tasks?
Before answering this question, we first describe the practical usage of training samples. The train-
ing procedure of DNNs can be stated as follows. The algorithm randomly shuffles the whole training
dataset A and partitions A into mini-batch of equal size {A0, A1, . . . , Ap-1}, then the algorithm is
fed with the sample in a fixed order, say, first A0, then A1, and so on. The whole procedure repeats
after each pass of the whole dataset.
To make our description more clear, We take the first pass as an example. Let PfAt (θt) be the
gradient calculated on iteration t by using the sample in subset At, where 0 ≤ t < p due to the
first pass. Note that the loss function is the sum of whole samples, e.g., § PZi fi(θ), if We utilize
PfAi (Bt) to directly update parameters like SGD, e.g., Bt+1 = Bt - αPfAt (Bt), the batch loss
Pi∈A fi(B) will decrease since it aligns with the opposite direction of its gradient, e.g., (Bt+1 -
Bt)TPfAt (Bt) = -kPfAt (Bt)k2 < 0. However, for loss corresponding to the sample not in At, itis
highly possible that (Bt+1 - Bt)TPfAi (Bt) > 0. On other words, only using -PfAi (Bt) as update
direction will decrease the loss function corresponding to At but increase the total loss function
except At.
Ideally, if there exists a direction Bt+1 - Bt which is orthogonal to PfAi (Bt) for any i 6= t (mod p),
and forms an acute angle with PfAt (Bt), then θt+1 can guarantee a sufficient descent for the loss
function corresponding to At while not increase the loss function outside At .
Then a natural update of Bt+1 to achieve the above goal can be obtained by solving the following
sub-problem:
arg min c2 kB - Btk2 + c1 (B - Bt)T gt
θ	(6)
s.t.	(B - Bt)T PfAi(Bt) = 0,i 6= t (mod p),
In practice, however, the computational cost of computing PfAi (Bt) for all i is expensive in each
iteration. As a natural solution, we can approximate PfAi (Bt) with previous gradients fAi (Bi),
which is exactly gi calculated in Adagrad in iteration i before iteration t, where i 6= t (mod p).
Hence a soft margin variant of Equation (6) can be formulated as
Bt+1 = arg min c2,0 kB - Btk2 + c2,1 X (B - Bt)Tgτ + c1 (B - Bt)Tgt,	(7)
θ	τ=1
4
Under review as a conference paper at ICLR 2020
which is exactly the formulation in Equation (5) and Equation (3)! Thus, one can reasonably con-
sider that Adagrad decreases the cumulative loss by guaranteeing a descent on the current batch
loss while does not increase previous batch loss.
Although the approximation of ▽" (θt) using previous gradients gi in Adagrad performs well in
practice, there is still a large accuracy gap. For example, let gt-1 and gt-2 denote the gradients
NfAi-I (θt-ι) and Vf∆i-2 (θt-2), respectively. It is often the case that the approximation error
between NfAi-I (θt) and gt-ι is smaller than that between NfAi_2(θt) and gt-2, assuming the ob-
jective is local smoothness1. In other words, the approximation accuracy ofgτ to NfAi(θt) decays
as τ decreases. Inspired by this observation, we substitute the arithmetic average in Equation (7)
to exponential moving average and reassign the coefficients of gτ like Adam/momentum methods,
then Equation (7) becomes
θt+1 = arg min c2,0 kθ - θt k2 + c2,1 X (1 - β) βt-τ (θ - θt)T gτ + c1 (θ - θt)T gt.
This is what RMSProp actually does. Therefore, we obtain the same optimization intuition of RM-
SProp as Adagrad but add weight of the approximate gradient according to its approximation accu-
racy.
Enlightened by the above analysis, we are ready to propose a new adaptive momentum method.
Let mt denote the approximated gradient of the previous batch loss at current parameters. With
the following observation: (i) if θt - θt+1 forms acute angles with both gt and mt, rather than
penalizing the projection like what Adagrad and RMSProp do, we can obtain both descent on current
and previous batches; (ii) to handle the case when the estimation of previous gradients using mt is
not accurate, we expect the current gradient gt to dominate the descent direction, which is denoted
as mt. It can be guaranteed by requiring the projection length of mt on gt is less than the length of
gt if properly regularizing mt . Then, we propose a new iterative subproblem in the following:
θt+1 = arg min c2,0 kθ - θtk2 + c1 (θ - θt)T gt + kgtk
m t )
W卜
(8)
where mt+1 = gt + IlgtIl
m t
km tk.
With such formulation, the following properties hold:
•	We ensure the above two observations, specifically, with the update of momentum mt, We
can guarantee that kint+ι∣ = O(∣gtk).
•	We attach the properties of the second moment to the first-order operation, inspired by
Adagrad proximal subproblem.
The proposed algorithm Acutum is summarized in Algorithm 1.
Algorithm 1 Acute Adaptive Momentum method (Acutum)
1:	Input: initial point θ0 ∈ X; step size {αt}, momentum parameters {βt}
2:	set m0 = 0
3:	for t = 1 to T do
4:	gt = Nft(θt)
5:	m t = gt + βι,t kgtk / (km t-ιk + E) ∙ m t-ι
6:	θt+ι = ∏X (θt - at ∙ mt)
7:	end for
8:	Return: xT+1
Note that, at step 5 We use E to prevent precision overflow caused by a tiny ∣∣rnt-ι∣∣. βι,t is the
momentum weight. We specify its value in the theoretical analysis in Section 4. In practice, we
can simply set β1,t to be 1. Compared with other adaptive gradient methods, Acutum has fewer
hyper-parameters, and is much easier to tune deep models. Besides, it is also computation-friendly
(for both time and space complexities) since the algorithm does not need second moments.
1The local smoothness assumption is widely used in the convergence analysis for convex and non-convex
objectives, see (Nesterov, 1998; Johnson & Zhang, 2013).
5
Under review as a conference paper at ICLR 2020
4	Convergence Analysis
In this section, we analyze the convergence rate of our proposed algorithm Acutum in an online
convex setting, i.e., we assume the convexity of a sequence of loss functions f1 , f2, . . . , fT , as
shown in the following:
Assumption 4.1 (Convexity Assumption). For each loss function ft (θ) where 1 ≤ t ≤ T, we
assume it is convex. That is, for any x, y ∈ X , we have
ft(y) ≥ ft(χ) + Vft(X)T (y - χ),
where X is the feasible set of function f1, f2, . . . , fT.
Notice that this assumption is widely used for theoretical analysis, in both stochastic optimization
methods (Johnson & Zhang, 2013; Defazio et al., 2014; Shalev-Shwartz & Zhang, 2013) and online
learning methods (Duchi et al., 2011; Kingma & Ba, 2014; Reddi et al., 2019). Rather than consider
the objective as a finite-sum function, here we analyze the regret from a online learning perspective.
That is to say, at each time step t, we are given the loss f(θt) after selecting some feasible solution
θt. Then we continue to pick the next point θt+1 based on the previous losses.
We adopt online regret as the convergence metric, which equals to the relative difference between
the algorithm’s cumulative loss and the offline minimum value. The definition of regret is straight-
forward, i.e.,
TT
R(T)：= x ft(θt)- mχ x ft(θ),
t=1	t=1
where X is the feasible set over all steps. Our purpose is to find a sequence of points {θt}1≤t≤T to
minimize the overall regret R(T). We state the main theoretical results in the following theorem.
Theorem 4.2. Suppose Assumption 4.1 holds. We set E > 0, at = aʌ/d/t, βι,t = βλt-1, where
β,λ ∈ (0,1). Ifthe diameter of the convex feasible Set X are bounded, i.e., kθ 一 θ*k∞ ≤ D∞ for
all θ ∈ X, and ft has bounded gradients, i.e., kVft(θ)k∞ ≤ G∞ for all θ ∈ X, 1 ≤ t ≤ T, then
the regret of Algorithm 1 satisfies:
i=1
√dβD∞
2α (1 - λ)2.
(9)
Theorem 4.2 shows that our proposed algorithm has a similar order of step size to online gradient
descent, i.e., O( ʌ/d/t), because of removing the second moments. In this situation, weight decay
and appropriate learning rate decay strategies have much more influence on Acutum than Adam-
type optimizers. Also, from the proof of Theorem 4.2 in appendix A.2, we find that the same
order of regret bound can also be obtained even with a more modest momentum decay βι,t = β∕t.
Furthermore, We show in the following corollary that our Acutum enjoys a regret bound of O(√T),
which is comparable to the best of known bound for general convex online learning problems.
Corollary 4.3. Frame the hypotheses of Theorem 4.2, for all T ≥ 1, the regret of Algorithm 1
satisfies R(T) = O(√T).
Corollary 4.3 illustrates that Acutum obtains RT = o(T) for all situations which is independent of
the sparsity of the data features. Besides, since limT →∞ RT∕T → 0, Acutum will converge to the
optimal solution when the objectives are convex.
5	Experiments
In this section, we conduct extensive experiments on image classification tasks. We want to validate
that Acutum with only first moments can not only obtain a better generalization performance than
adaptive gradient methods, but also converges faster than SGD-momentum.
First, note that introducing weight decay is equivalent to adding l2 regularization to the objectives,
and has a significant impact on the generalization ability of optimizers. To better characterize gener-
alization performance, we conduct experiments from two perspectives: (i) In Section 5.1, we record
6
Under review as a conference paper at ICLR 2020
ssoL niarT
(a) Train Loss and Test Accuracy for VGGNet
0	50	100	150	200
ssoL niarT
(b) Train Loss and Test Accuracy for ResNet
6
4
2
0
8
6
4
2
0
(c) Train Loss and Test Accuracy for DenseNet
ssoL niarT
0	50	100	150	200
Epochs
0	50	100	150	200
Epochs
505050
665544
% ycaruccA tseT
0	50	100	150	200
Epochs
Epochs
0	50	100	150	200
Epochs
Figure 1:	With a same weight decay, train loss and test accuracy of different optimizers for VGGNet,
ResNet and DenseNet on CIFAR-100.
the convergence of training loss and the test accuracy of all optimizers with fixed weight decays
(they optimize the same objective function). (ii) In Section 5.2, we adopt the optimal weight decays
for all optimizers, then investigate their generalization ability based on the test accuracy.
Now we introduce the experimental settings of our experiments, which are general and commonly
used. Specifically, we implement all of our codes in Pytorch platform version 1.0+ within Python
3.6+. To obtain sufficient comparisons with adaptive gradient variants, we choose various base-
lines, including SGD-momentum, Adam (Kingma & Ba, 2014), Amsgrad (Reddi et al., 2019),
Adamw (Loshchilov & Hutter, 2017), Yogi (Zaheer et al., 2018), Padam (Chen & Gu, 2018),
Radam (Liu et al., 2019) and Adabound (Luo et al., 2019). For the image classification tasks, we
use two datasets CIFAR-10, CIFAR-100 (Krizhevsky et al., 2009), and test three different CNN
architectures including VGGNet (Simonyan & Zisserman, 2014), ResNet (He et al., 2016) and
DenseNet (Huang et al., 2017) . To obtain stable convergence, we run 200 epochs, and decay the
learning rate by 0.1 every 50 epochs. We perform cross-validation to choose the best learning rates
for all optimizers and second moment parameters β2 for all adaptive gradient methods.
5.1	Experiment with Fixed Weight Decay
We first evaluate CIFAR-100 dataset. In this experiment, the values of weight decay in all optimizers
are fixed, and are chosen to be the weight decay in SGD-momentum when it achieves the maximum
test accuracy.
From the first row of Figure 1, i.e, the training curves of three tests, we observe that our proposed
Acutum significantly outperforms Adam, and has comparable rate with Padam and Adabound when
we fix the weight decay. Also, in the second row of Figure 1, i,e, the test accuracy, our Acutum
outperforms all benchmarks except only SGD-momentum. However, the training loss converges
much slower when implemented with SGD-momentum which is also recorded in other literature.
The above evaluation results validate that our method achieves both fast convergence and good
generalization when the weight decay is fixed. Note that the AdamW method reduces training
loss quickly, however, there is a large generalization gap between it and other optimizers, including
SGD-momentum, Acutum, Padam.
5.2	Experiment with Optimal Weight Decay
In this section, we conduct experiments on CIFAR-10 dataset. Specifically, we find the optimal
weight decay for each optimizer which can obtain the maximum test accuracy.
7
Under review as a conference paper at ICLR 2020
(a) Train Loss and Test Accuracy for VGGNet
505050
.4 .4 .3 .3 .2 .2
0. 0. 0. 0. 0. 0.
ssoL niarT
(b) Train Loss and Test Accuracy for ResNet
(c) Train Loss and Test Accuracy for DenseNet
Epochs
% ycaruccA tseT
0	50	100	150	200
Epochs
% ycaruccA tseT
76431
88888
0	50	100	150	200
Epochs
% ycaruccA tseT
0	50	100	150	200
Epochs
Figure 2:	With different weight decays, train loss and test accuracy of different optimizers for VG-
GNet, ResNet and DenseNet on CIFAR-10.
As shown in Figure 2, Adam, AdamW and Amsgrad perform worse than the other optimizers in the
plots of test accuracy. In this optimal weight decay setting, Padam, Adabound and Acutum can even
have better test performance than SGD-momentum on VGGNet if sacrificing some convergence
rate. Besides, they outperform Adam by more than 2 percentage points for the test accuracy. The
experimental results demonstrate that optimizers which converge faster than Acutum generalize
worse. Similar to results in previous section, Acutum can effectively bridge the generalization gap,
and has a comparable performance with the state-of-the-art Adam’s variants while only requiring
first moments.
6 Conclusion
In this paper, we revisited the existing adaptive optimization methods from a novel point of view.
We found that the widely used second moments essentially penalize the projection of the current
descent direction on the exponential moving average of previous gradients. In other words, it aims
to decrease the current batch loss while does not increase previous batch loss. Following such
new idea, we investigate how to obtain descent on both current and previous batches. Specifically,
we proposed a new method, acute adaptive momentum (Acutum). It removes the computation-
complicated second moments and constructed a decent direction by forming acute angles with both
current and (approximated) previous gradients. We analyzed its convergence property in the online
convex setting. The extensive evaluations demonstrate that our Acutum can effectively bridge the
gap between fast convergence and good generalization, i.e., the advantages of both adaptive gradient
methods and SGD-momentum.
For future work, it would be interesting to evaluate our method in other types of tasks and deep
models, especially NLP and GAN problems. Besides, explaining the generalization ability of SGD
is one of the most significant open problems in this field. We also hope that one could improve the
convergence rate of SGD by only scheduling the learning rate carefully and dynamically.
References
Jinghui Chen and Quanquan Gu. Closing the generalization gap of adaptive gradient methods in
training deep neural networks. arXiv preprint arXiv:1806.06763, 2018.
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method
with support for non-strongly convex composite objectives. In Advances in neural information
processing Systems,pp. 1646-1654, 2014.
8
Under review as a conference paper at ICLR 2020
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal ofMachine Learning Research, 12(Jul):2121-2159, 2011.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky. Neural networks for machine learning
lecture 6a overview of mini-batch gradient descent. Cited on, 14:8, 2012.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 4700-4708, 2017.
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance
reduction. In Advances in neural information processing systems, pp. 315-323, 2013.
Nitish Shirish Keskar and Richard Socher. Improving generalization performance by switching from
adam to sgd. arXiv preprint arXiv:1712.07628, 2017.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
Technical report, Citeseer, 2009.
Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei
Han. On the variance of the adaptive learning rate and beyond. arXiv preprint arXiv:1908.03265,
2019.
Ilya Loshchilov and Frank Hutter. Fixing weight decay regularization in adam. arXiv preprint
arXiv:1711.05101, 2017.
Liangchen Luo, Yuanhao Xiong, Yan Liu, and Xu Sun. Adaptive gradient methods with dynamic
bound of learning rate. In Proceedings of the 7th International Conference on Learning Repre-
sentations, New Orleans, Louisiana, May 2019.
Yurii Nesterov. Introductory lectures on convex programming volume i: Basic course. Lecture
notes, 3(4):5, 1998.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. arXiv
preprint arXiv:1904.09237, 2019.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathemati-
cal statistics, pp. 400-407, 1951.
Shai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for regularized
loss minimization. Journal of Machine Learning Research, 14(Feb):567-599, 2013.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal
value of adaptive gradient methods in machine learning. In Advances in Neural Information
Processing Systems, pp. 4148-4158, 2017.
William Henry Young. On classes of summable functions and their fourier series. Proceedings of the
Royal Society of London. Series A, Containing Papers ofa Mathematical and Physical Character,
87(594):225-229, 1912.
Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, and Sanjiv Kumar. Adaptive meth-
ods for nonconvex optimization. In Advances in Neural Information Processing Systems, pp.
9793-9803, 2018.
9
Under review as a conference paper at ICLR 2020
Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701,
2012.
Zijun Zhang, Lin Ma, Zongpeng Li, and Chuan Wu. Normalized direction-preserving adam. arXiv
preprint arXiv:1709.04546, 2017.
10
Under review as a conference paper at ICLR 2020
A Proof of the Main Results
A. 1 Some Important Lemmas
In this section, we give several important definitions and lemmas which will be used in the proof of
the regret convergence.
Lemma A.1. For some convex feasible set X ⊂ Rd, suppose θ1 = minθ∈X kθ - θ1 k and θ2 =
minθ∈x kθ — θ2k thenwehave ∣∣θι — θ2∣∣ ≤ ∣∣θι 一 θ2∣∣.
Proof. Here, We only provide the proof for completeness. Combining the definition θ∖
♦	11 zʌ	/ʌ 11 A	♦	11 zʌ zʌ Il ∙ . 1 .1	. ∙	i`	∙	.	1
minθ∈X ∣θ — θ1 ∣, θ2 = minθ∈X ∣θ — θ2 ∣ With the properties of projection operator, We have
(θι — θι)T ® — θι) ≥ 0
From the above inequalities, We can obtain
and
® — θ2)τ (θι - θ2) ≥ 0.
(θl — θl)τ (02 — θl) + (θ2 — θ2)τ (θ2 — θl) ≥ 0 ⇔ (θ2 — θl)τ ® — θl) ≥ ∣∣θ2 — θi∣∣2 .
With the property that ∣∣(θ2 — θ1) — (θ2 — θ1) ∣∣ ≥ 0, we have
kθ2 — θlk2 ≥ — ∣∣02 — θi∣∣2 +2(02 — θl)τ (02 — θl) ≥ ∣∣02 - θlf .
Thus, the proof has been completed.	□
Lemma A.2. In the Algorithm 1, we have ∣∣rnt∣ ≤ 2 ∣∣gt∣ at each iteration t if the momentum
parameter satisfies β1,t ≤ 1.
Proof. With the definition of rn t, we have
∣mtk2
gt +	m t-ι
kmt-ik + e
2
≤kgtk2 + β2,tkgtk2 + 2言 kgtk ∙kgt∣H∣m-k
∣nt-1∣
= (1+ βι,t)2 kgtk2∙
With the condition that βι,t ≤ 1, we have ∣∣ιnt∣ ≤ 2 ∣∣gt∣.	□
A.2 Proof of Theorem 4.2
Proof. Consider the update rule of Algorithm 1, parameters θ* and θt satisfy
Πχ (θ*) = θ* and ∏x (θt —四由力=&+>
With Lemma A.1, we have
∣θt+ι — θ*∣2 ≤ ∣θt — αtint — θ*∣2
=∣θt — θ*∣∣2 — 2αtmT (θt — θ*) + α ∣mt∣2	(Io)
Q∣θt — θ*∣∣2 + α ∣mt∣2 — 2αt (gt + 瓯㈣ mt-X (θt — θ*),
∣nt-1∣ +
where ① follows from the definition of rnt. Rearrange the items in Equation (10), we obtain
g (θt - θ*) ≤2α (∣θt - θ*∣2 - I|0t+1 - θ*∣2) + at ∣1iιt∣2 - Ist ∣g+ enτ-ι (θt - θ*)
2αt	2	∣nt-1 ∣ +
≤ ɪ (∣θt—θ*∣2 -∣θt+ι—θ*∣2)+α ∣ι^ι t∣2+βι,t" K 一 +at∣gt『*142
2at ∖	2	2	[	2αt,	2(∣mt-ι∣∣ + e)2
≤ ɪ (∣θt	- θ*∣2 -∣θt+ι	- θ*∣2)	+ at	∣1iι t∣2+βι J ∣θt - 0*『+ a^tf!,
2αt	2	2αt	2
11
Under review as a conference paper at ICLR 2020
where ① holds by the Young,s inequality. By Assumption 4.1, with the convexity of all fi, We have
TT
X [ft(θt) - ft(θ*)] ≤ XgT (θt- θ*).
Submit Equation (A.2) to the above inequalities, we obtain
T
Td
∑[ft (θt)- ft(θ*)] ≤ΣΣ
t=1
t=1 i=1
2α h(θt,i - θi )2 - (θt+1,i - θi )2i +
一■―― ,
{z
T1
Td	Td	Td
X X a2t m 2,i + XX T g2,i + XX βαt(% -%2，
t=1 i=1	t=1 i=1	t=1 i=1 t
----{----} '	{-----} '	{-------}
T2-------T3----------T4
(11)
where β1,t = βλt-1 is monotonically decreasing with t. For T1, we have
Td
2 XX a h(θt,i - θi)2 - (θt+1,i - θi)2i
t=1 i=1 t
d	Td
OT X (θι,i-θi*)2 + XX
i=1	t=2 i=1
_1 - ɪ
αt αt-1
(θt,i-θ"2
≤ D∞
2 2
X a+X X α- O-T)1
(12)
1
2
where We set at = αyfd∕t. For T2 in Equation (11), we obtain
T d	TT d	d
2XXatm⅛ =2 XXatm2,i+ XaτmT,i
t=T i=T	t=T i=T	i=T
®1 TX X	八 2	4α√d X 2
≤ 2 工 Tatmt,i + ~TfΓλ^Tτ,i
2 t=T i=T	T i=T
1 e S 2	4G∞a√d S
≤ 2 工工atmt,i+ —√ψ~ UgT,i|
T 1 d
=2G∞a√dX √ X ∣gt,i∣
t=T	t i=T
2G∞a√dX X l√ψ
i=T t=T	t
②	d	T ι	d
≤2G∞a√d]T kgi：T,ik2 t ]T- ≤ 2G∞a √(1+log T) d E ||gi：T,i||2 ,
i=T	t=T	i=T
(13)
where we introduce Lemma A.2 in inequality ①.Besides, we get ② through Cauchy-Schwarz
inequality. With similar proof techniques, we have T3 satisfied
Y^Y^atβ1,t	2	atβl,t	2	aβ√d	2	∖~~^ aβ	atβl,t	2	G∞aβ√d	.	.
工工-ɪ gt,i ≤Χ 工-ɪ gt,i + ~2√T 工 gT ,i ≤ X 工-ɪ gt,i +	2√T	工 |gT，i |
t=T i=T	t=T i=T	i=T	t=T i=T	i=T
G∞aβ√d	1 i i	G∞aβ√d	∣gt,i |
i=T √t 3lgt,il = ^―2⅛ √τ
G∞aβ√d `ʌ	1	G∞aβ /门 ]-------∖^ m
≤——5——Xkgl：T,ik2\ Χτ ≤ -丁 √(1+log T )dΧ l|gl：T,ik2 .
i=T	t=T	i=T
(14)
12
Under review as a conference paper at ICLR 2020
Since Equation (14) is sufficient for our regret convergence analysis, we have not tried to optimize
it further. Then, for T4, we have
T
Td	T	d	T	d
XX 段(θt,i -蛤2=X 装 X (θt,i -蛤2 Q 2⅛ X "- X(…)2
t=1 i=1	t=1	i=1	t=1	i=1
<√d⅛ X √tλt-ι ≤ √dβD∞ Xtλ1 (I √dβD",
-2a t=1	-2a ±	^ 2a (1 - λ)2,
(15)
where ① follows from the definition of β1,t and at, and ② establishes because of the geometric
series summation rule. As a result, submitting Equation (12), Equation (13), Equation (14) and
Equation (15) back into Equation (11), we have
R(T) ≤
MdβD∞
2α (1 — λ)2
(16)
This completes the proof.
□
A.3 Proof of Corollary 4.3
Proof. From Theorem 4.2, we have
For the second term, we have
Tt
√dβD∞
2a (1 — λ)2
(17)
(18)
Combining the above inequality with Equation (17), we obtain
r(t ) = O( Vt )
□
13