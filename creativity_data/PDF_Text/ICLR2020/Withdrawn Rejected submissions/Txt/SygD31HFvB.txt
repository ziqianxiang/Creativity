Under review as a conference paper at ICLR 2020
A Novel Analysis Framework of Lower Com-
plexity Bounds for Finite-Sum Optimization
Anonymous authors
Paper under double-blind review
Ab stract
This paper studies the lower bound complexity for the optimization problem
whose objective function is the average of n individual smooth convex functions.
We consider the algorithm which gets access to gradient and proximal oracle for
each individual component. For the Strongly-convex case, We prove such an algo-
rithm can not reach an ε-suboptimal point in fewer than Ω((n + √κn) log(1∕ε))
iterations, Where κ is the condition number of the objective function. This loWer
bound is tighter than previous results and perfectly matches the upper bound of
the existing proximal incremental first-order oracle algorithm Point-SAGA. We
develop a novel construction to show the above result, which partitions the tridi-
agonal matrix of classical examples into n groups to make the problem difficult
enough to stochastic algorithms. This construction is friendly to the analysis of
proximal oracle and also could be used in general convex and average smooth
cases naturally.
1 Introduction
We consider the minimization of the following optimization problem
1n
mRdf (X), n X fi(X),	⑴
where the fi(x) are L-smooth and μ-strongly convex. Accordingly, the condition number is defined
as K = L∕μ, which is typically larger than n in real-world applications. Many machine learning
models can be formulated as the above problem such as ridge linear regression, ridge logistic re-
gression, smoothed support vector machines, graphical models, etc. This paper focuses on the first
order methods for solving Problem (1), which access to the Proximal Incremental First-order Oracle
(PIFO) for each individual component, that is,
hf(x,i,γ)，[fi(x), Vfi(X), proxfi (x)],
where i ∈ {1, . . . , n}, γ > 0, and the proximal operation is defined as
ProXfi (x) = arg min {fi(u) + 21γ l∣x — uk2}∙
We also define the Incremental First-order Oracle (IFO)
gf (X, i,γ) , [fi(X),Vfi(X)] .
(2)
PIFO provides more information than IFO and it would be potentially more powerful than IFO in
first order optimization algorithms. Our goal is to find an ε-suboptimal solution X such that
f (X) — min f (x) ≤ ε
x∈Rd
by using PIFO or IFO.
There are several first-order stochastic algorithms to solve Problem (1). The key idea to lever-
age the structure of f is variance reduction which is effective for ill-conditioned problems. For
example, SVRG (Zhang et al., 2013; Johnson and Zhang, 2013; Xiao and Zhang, 2014) can
1
Under review as a conference paper at ICLR 2020
find an ε-suboptimal solution in O((n+κ) log(1∕ε)) IFO calls, while the complexity of the clas-
sical Nesterov's acceleration (Nesterov, 1983) is O(n√κlog(1∕ε)). Similar results1 also hold
for SAG (Schmidt et al., 2017) and SAGA (Defazio et al., 2014). In fact, there exists an ac-
celerated stochastic gradient method with √κ dependency. Defazio (2016) introduced a simple
and practical accelerated method called Point SAGA, which reduces the iteration complexity to
O((n + √κn) log(1∕ε)). The advantage of Point SAGA is in that it has only one parameter to be
tuned, but the iteration depends on PIFO rather than IFO. Allen-Zhu (2017) proposed the Katyusha
momentum to accelerate variance reduction algorithms, which achieves the same iteration complex-
ity as Point-SAGA but only requires IFO calls.
The lower bound complexities of IFO algorithms for convex optimization have been well studied
(Agarwal and Bottou, 2015; Arjevani and Shamir, 2015; Woodworth and Srebro, 2016; Carmon
et al., 2017; Lan and Zhou, 2017; Zhou and Gu, 2019). Specifically, Lan and Zhou (2017) showed
that at least Ω((n+√κn) log(1∕ε)) IFO calls2 are needed to obtain an ε-suboptimal solution for
some complicated objective functions. This lower bound is optimal because it matches the upper
bound complexity of Katyusha (Allen-Zhu, 2017).
It would be interesting whether We can establish a more efficient PIFO algorithm than IFO one.
Woodworth and Srebro (2016) provided a lower bound Ω(n+√κnlog(1∕ε)) for PIFO algorithms,
while the known upper bound of the PIFO algorithm Point SAGA [3] is O((n+√κn) log(1∕ε)).
The difference of dependency on n implies that the existing theory of PIFO algorithm is not perfect.
This gap can not be ignored because the number of components n is typically very large in many
machine learning problems. A natural question is can we design a PIFO algorithm whose upper
bound complexity matches Woodworth and Srebro’s lower bound, or can we improve the lower
bound complexity of PIFO to match the upper bound of Point SAGA.
In this paper, we prove the lower bound complexity of PIFO algorithm is Ω((n+√κn) log(1∕ε)) for
smooth and strongly-convex fi, which means the existing Point-SAGA (Defazio, 2016) has achieved
optimal complexity and PIFO can not lead to a tighter upper bound than IFO. We provide a novel
construction, showing the above result by decomposing the classical tridiagonal matrix (Nesterov,
2013) into n groups. This technique is quite different from the previous lower bound complexity
analysis (Agarwal and Bottou, 2015; Woodworth and Srebro, 2016; Lan and Zhou, 2017; Zhou and
Gu, 2019). Moreover, it is very friendly to the analysis of proximal operation and easy to follow. We
also use this technique to study general convex and average smooth cases (Allen-Zhu, 2018; Zhou
and Gu, 2019), and extend our result to non-convex problems (see Appendix J).
2 Our Analysis Framework
In this paper, we consider the Proximal Incremental First-order Oracle (PIFO) algorithm for smooth
convex finite-sum optimization. All proofs in this section can be found in Appendices C and D for
a detailed version. We analyze the lower bounds of the algorithms when the objective functions are
respectively strongly convex, general convex, smooth and average smooth (Zhou and Gu, 2019).
Definition 2.1. For any differentiable function f : Rm → R,
•	f is convex, if for any x, y ∈ Rm it satisfies f (y) ≥ f (x) + Nf (x), y 一 Xi.
•	f is μ-strongly convex, iffor any x, y ∈ Rm it satisfies
f (y) ≥ f (x) + hvf (χ), y - Xi + 2 ∣∣X - yk2.
•	f is L-smooth, iffor any x, y ∈ Rm it satisfies ∣∣Vf (x) — Vf (y)∣2 ≤ LkX - y∣2.
1SVRG, SAG and SAGA only need to introduce the proximal operation for composite objective, that is,
fi (x) = gi (x) + h(x), where h may be non-smooth. Their iterations only depend on IFO when all the fi (x)
are smooth. Hence, we regard these algorithms only require IFO calls in this paper.
2Lan and Zhou,s construction requires f to be μ-strongly convex and every f to be convex, while this paper
studies the lower bound with stronger condition that is every fi is μ-strongly convex. For the same lower bound
complexity, the result with stronger assumptions on the objective functions is stronger.
2
Under review as a conference paper at ICLR 2020
	Upper Bounds	Previous Lower Bounds	Our Lower Bounds
fi is L-smooth and μ-strongly convex with n = O(κ)	O ((n + √κn) log(ε)) (Allen-Zhu, 2017), IFO (Defazio, 2016), PIFO	Ω (n + √κnlog(ε)) (Woodworth and Srebro, 2016) PIFO	Ω ((n + √κn) log(ɪ)) [Theorem 3.1] PIFO
fi is L-smooth and μ-strongly convex with κ = O(n)	o(+(1+(⅛)+>og( 1! (Hannah et al., 2018), IFO	cfn+d+iK)log (1! (Hannah et al., 2018) IFO	ωH+⅛+)log (1! [Theorem 3.1] PIFO
fi is L-smooth and convex	O (n log(ε) + q¥) (Allen-Zhu, 2017) IFO	ω (n+qnL) (Woodworth and Srebro, 2016) PIFO	ω (n+qnL) [Theorem 3.3] PIFO
{fi }in=1 is L-average smooth and f is μ-strongly convex	O ((n + n3√κ) log (ɪ)) (Allen-Zhu, 2018) IFO	Ω (n + n3 √κ log (ɪ)) (Zhou and Gu, 2019) IFO	Ω ((n + n44 √κ^ log (j)) [Theorem 3.5] PIFO
{fi }in=1 is L-average smooth and f is convex	O (n + n 3 /L) (Allen-Zhu, 2018) IFO	Ω (n + n 3 qL) (Zhou and Gu, 2019) IFO	Ω (n + n 3 qL) [Theorem 3.7] PIFO
Table 1: We compare our PIFO lower bounds with existing results of IFO or PIFO algorithms, where
K = L∕μ. Note that the call of PIFO could obtain more information than IFO. Hence, any PIFO
lower bound also can be regarded as an IFO lower bound, not vice versa.
Definition 2.2. We say differentiable functions {fi}in=1 , fi : Rm → R, to be L-average smooth if
for any x, y ∈ Rm, they satisfy
1n
—E kVfi(x) - Vfi(y)k2 ≤ L2 kx - yk2.	⑶
n
i=1
Remark 2.3. We point out that
1.	if each fi is L-smooth, then {fi}in=1 are L-average smooth.
2.	if {fi}n=ι are L-average smooth, then f (x) = 1 Pn=ι fi(x) is L-smooth.
We present the formal definition for PIFO algorithm.
Definition 2.4. Consider a stochastic optimization algorithm A to solve Problem (1). Let xt be the
point obtained at time-step t and the algorithm starts with x0. The algorithm A is said to be a PIFO
algorithm iffor any t ≥ 0, we have
Xt ∈ span {xo,…,xt-i, Vfh (xo),…，Vfit (xt-ι), proxf； (xo),…，Proxf：(xt-i)}, (4)
where it is a random variable supported on [n] and takes P(it = j) = pj for each t ≥ 0 and
1 ≤ j ≤ n where Pjn=1 pj = 1.
Without loss of generality, We assume xo = 0 and pi ≤ p ≤ ∙∙∙ ≤ Pn to simplify our analysis.
OtherWise, We can take {fi(x) = fi(x + x0)}in=i into consideration. On the other hand, suppose
that Psi ≤ Ps2 ≤ ∙∙∙ ≤ Psn where {si}n=ι is a permutation of [n]. Define {fi}n=ι such that
fsi = fi, then A takes component fsi in probability Psi, i.e., A takes f in probability Psi.
3
Under review as a conference paper at ICLR 2020
To demonstrate the construction of adversarial functions, we first introduce the following class of
matrices:
1
B(m, ω)=	..
-1	1
ω
∈R
m×m
Then we define
-ω2 + 1 -1
-1	2	-1
A(m,ω)，B(m,ω)>B(m,ω) =	..	..
-1	2
-1
-1
1
(5)
The matrix A(m, ω ) is widely-used in the analysis of lower bounds for convex optimization (Nes-
terov, 2013; Agarwal and Bottou, 2015; Lan and Zhou, 2017; Carmon et al., 2017; Zhou and Gu,
2019). We now present a decomposition of A(m, ω) based on Eq. (5).
Denote the l-th row of the matrix B(m, ω) by bl (m, ω)> and let
Li = {l : 1 ≤ l ≤ m,l ≡ i — 1(mod n)}, i = 1, 2,…，n.
Our construction is based on the following class of functions
r(x; λ0, λ1, λ2, m, ω)
1n
一，ri(x; λo, λ1,λ2,m, ω),
n
i=1
where
ri(x; λ0, λ1, λ2, m, ω)
λ1 P bl(m, ω)>x22 + λ2 kxk22 - λ0hem, xi,
l∈L1	2
λ1 P	bl(m, ω)>x22 + λ2 kxk22 ,
l∈Li
for i = 1,
for i = 2, 3,…，n.
(6)
We can determine the smooth and strongly-convex coefficients of ri as follows.
Proposition 2.5. For any λι > 0,λ2 ≥ 0,ω < √2, we have that the r are (4λι + 2λ2)-smooth
and λ2-strongly convex, and {ri}in=1 is L0-average smooth where
L0 =2 ↑∣ n [(λι+λ2)2 + λ1 ] + λ2.
We define the subspaces {Fk }km=0 as
F = ʃspan{em, em-1, ∙ ∙ ∙ , em-k+1}, for 1 ≤ k ≤ m,
Fk = {0},	fork = 0.
The following technical lemma plays a crucial role in our proof.
Lemma 2.6. For any λ0 6= 0, λ1 > 0, λ2 ≥ 0 andx	∈ Fk, 0 ≤ k < m, we have that
Vri(x; λ0,λ1,λ2,m,ω) and ProxYa(x) ∈
fFk + 1,
Fk,
if k ≡ i - 1(mod n),
otherwise.
In short, ifx	∈	Fk	and let fi(x)	,	ri(x;	λ0, λ1, λ2, ω), then there exists only one i ∈ {1, . . . ,	n}
such that hf (x, i, γ) could (and only could) provide additional information in Fk+1. The “only
one” property is important to the lower bound analysis for first order stochastic optimization algo-
rithms (Lan and Zhou, 2017; Zhou and Gu, 2019), but these prior constructions only work for IFO
rather than PIFO.
Lemma 2.6 implies that xt = 0 will host until algorithm A draws the component f1. Then, for any
t < T1 = mint{t : it = 1}, we have xt ∈ F0 and xT1 ∈ F1. The value of T1 can be regarded as
the smallest integer such that xT1 could host. Similarly, we can define Tk to be the smallest integer
such that xTk ∈ Fk could host. We give the formal definition of Tk recursively and connect it to
geometrically distributed random variables in the following corollary.
4
Under review as a conference paper at ICLR 2020
Corollary 2.7. Let
T0 = 0, and Tk = min{t : t > Tk-1, it ≡ k (mod n)} for k ≥ 1.	(7)
Then for any k ≥ 1 and t < Tk, we have xt ∈ Fk-1. Moreover, Tk can be written as sum of
k independent random variables {Yl}1≤l≤k, i.e., Tk = Plk=1 Yl, where Yl follows a geometric
distribution with success probability ql = pl0 where l0 ≡ l (mod n), 1 ≤ l0 ≤ n.
The basic idea of our analysis is that we guarantee the minimizer of r lies in Fm and assure the
PIFO algorithm extend the space of span{x0 , x1 , . . . , xt} slowly with t increasing. We know that
span{x0, x1, . . . , xTk } ⊆ Fk by Corollary 2.7. Hence, Tk is just the quantity that reflects how
span{x0 , x1 , . . . , xt} verifies. Because Tk can be written as the sum of geometrically distributed
random variables, we needs to introduce some properties of such random variables which derive the
lower bounds of our construction.
Lemma 2.8. Let {Yi}1≤i≤N be independent random variables, and Yi follows a geometric distri-
bution with success probability pi. Then
P (X Yi > 二
16
≥ 1-----
—	9N
(8)
From Lemma 2.8, the following result implies how many PIFO calls we need.
Lemma 2.9. If M ≥ 1 satisfies minx∈FM f(x) - minx∈Rm f(x) ≥ 9ε and N = n(M + 1)/4,
then we have
min Ef(xt) - min f(x) ≥ ε.
t≤N	t	x∈Rm
3 Main Results
We present the our lower bound results for PIFO algorithms and summarize all of results in Table 1
and 2 . We first start with smooth and strongly convex setting, then consider the general convex and
average smooth cases.
Theorem 3.1. For any PIFO algorithm A and any L,μ,n, ∆,ε such that K = L∕μ ≥ 2, and
ε/∆ ≤ 0.5, there exist a dimension d = O(1 + pκ∕n log (△/ε)) and n L-smooth and μ-strongly
convex functions {fi : Rd → R}n=1 such that f (xo) — f (x*) = △. In order to find X ∈ Rd such
that Ef (X) — f (x*) ‹ ε, A needs at least N queries to hf, where
N
fΩ((n+√κn) log(∆∕ε)),
Iω (n + ( 1+(loJ√κ))+ }og(7ε)),
for n = O(κ),
for κ = O(n).
Remark 3.2. In fact, the lower bound in Theorem 3.1 perfectly match the upper bound of the PIFO
algorithm Point SAGA (Defazio, 2016) 3 in n = O(κ) case and match the the upper bound of the IFO
algorithm4 prox-SVRG (Hannah etal., 2018) in K = O(n) case. Hence, the lower bound in Theorem
3.1 is tight, while Woodworth and Srebro (2016) only provided lower bound Ω (n+√κn log (1∕ε))
in n = O(K) case. The theorem also shows that the PIFO algorithm can not be more powerful than
the IFO algorithm in the worst case, because Hannah et al. (2018) proposed a same lower bound
for IFO algorithms.
Next we give the lower bound when the objective function is not strongly-convex.
Theorem 3.3. For any PIFO algorithm A and any L, n, B, ε such that ε ≤ LB2 ∕4, there exist a
dimension d = O(1 + B,L∕(nε)) and n L-smooth and convex functions {fi : Rd → R}n=1
such that kxo — x*∣∣2 ≤ B. In order to find X ∈ Rd such that Ef (X) — f (x*) ‹ ε, A needs at least
Ω (n+B,nL∕ε) queries to hf.
3Defazio (2016) proves Point SAGA requires O ((n + √κn) log (1∕ε)) PIFO calls to find X SuCh that
Ekx — x*k2 < ε∣∣xo — x*∣∣2, which is not identical to the condition Ef(X) — f(x*) < ε in The-
orem 3.1. However, it is unnecessary to worry about it because we also establish a PIFO lower bound
Ω ((n + √κn) log (1∕ε)) for EkX — x* k2 < ε∣∣xo — x* k2 in Theorem F.1.
4IFO algorithm is apparently also a PIFO algorithm.
5
Under review as a conference paper at ICLR 2020
Remark 3.4. The lower bound in Theorem 3.3 is the same as the one of Woodworth and Srebro’s
result. However, our construction only requires the dimension be O 11 + B,L∕(nε)), which is
much smaller than O
InLB ) ) in (Woodworth and Srebro, 2016).
Then we extend our results to the weaker assumption: that is, the objective function F is L-average
smooth (Zhou and Gu, 2019). We start with the case that F is strongly convex.
Theorem 3.5. For any PIFO algorithm A and any L,μ,n, ∆,ε such that K = L∕μ ≥
,3∕n(2 + 1), and ε∕∆ ≤ 0.00327, there exist a dimension d = O (n-V4√Klog (△/ε)) and
n functions {fi : Rd → R}in=1 where the {fi}in=1are L-average smooth and f is μ-strongly convex,
such that f (xo) — f (x*) = △. In order to find X ∈ Rd such that Ef(X) — f (x*) < ε, A needs at
least Ω ((n+n3S√K) log(∆∕ε)) queries to hf.
Remark 3.6. Compared with Zhou and Gu's lower bound Ω (n + n3S√Klog(∆∕ε)) for IFO al-
gorithms, Theorem 3.5 shows tighter dependency on n and supports PIFO algorithms additionally.
We also give the lower bound for general convex case under the L-average smooth condition.
Theorem 3.7. For any PIFO algorithm A and any L, n, B, εsuch that ε≤ LB 2∕4, there exist a
dimension d = O(1 + Bn-V4,L∕ε) and n functions {f : Rd → R}n=ι which the {fi}f=ι are
L-average smooth and f is convex, such that ∣∣xo — x*∣b ≤ B. In order to find X ∈ Rd such that
Ef (X) — f (x*) < ε, A needs at least Ω (n + Bn3/4 pL∕ε) queries to hf.
Remark 3.8. The lower bound in Theorem 3.7 is comparable to the one of Zhou and Gu's result,
but our construction only requires the dimension be O(1 + Bn-1∕4,L∕ε), which is much smaller
than O (n + Bn3∕4∙PL∕i) in (Zhou and Gu, 2019).
4 Constructions in Proof of Main Theorems
We demonstrate the detailed constructions for PIFO lower bounds in this section. All the omitted
proof in this section can be found in Appendix for a detailed version.
4.1	Strongly Convex Case
The analysis of lower bound complexity for the strongly-convex case depends on the following
construction.
Definition 4.1. Forfixed L, μ, △, n, let α = 2(2(工/^-1) +1 .We define fsc,i : Rm → R asfollows
fSC,i (X) = ri
(x； r 2⅛δ
L — μ
4
(9)
and
1n	L
FSc(χ)，- X fSc,i(χ) = -μ
n	4n
i=1
2
+μ ∣χ∣2
2
一\∕2(L — μ)∆ hem, xi.
n(α — 1)
Note that the fsc,i are L-Smooth and μ-strongly convex, and Fsc(xo) — F⅛c(x*) = △ (See ProPo-
sition E.1 in Appendix for more details). Next we show that the functions {fSC,i}in=1 are “hard
enough” for any PIFO algorithm A, and deduce the concluSion of Theorem 3.1.
Theorem 4.2. Suppose that
ε ≤ — (α——-),and m
-9 kα + 1	,
2 …+ 1 I*)+］，
n	9ε
6
Under review as a conference paper at ICLR 2020
where α = J2(L/μ-1) + 1.In order to find X ∈ Rm such that EFSC (X) — FSSC (x*) < ε, PIFO
algorithm A needs at least N queries to hFSC. where
N = (ω ((n + qnL) log (9ε)) ,	for L ≥ n + 1,
[ω (n + ( 1+lοg(nμ∕L) ) log (9ε)) , for 2 ≤ L < n + 1.
For larger ε, we can apply following Lemma.
Lemma 4.3. For any PIFO algorithm A and any L, μ, n, ∆, ε such that ε ≤ ∆∕2, there exist n
L-smooth and μ-strongly convex functions {f : R → R}n=1 such that F (x0) — F (x*) ≤ ∆. In
order to find X ∈ R such that EF (X) — F (x*) ‹ ε, A needs at least Ω(n) queries to hp.
As we explain in Remark H.1, the lower bound in Lemma 4.3 is same as the lower bound in Theorem
4.2 for ε > ∆L (αα-1) . In conclusion, We obtain Theorem 3.1.
4.2 Convex Case
The analysis of loWer bound complexity for non strongly-convex cases depends on the folloWing
construction.
Definition 4.4. For fixed L, B, n, we define fC,i : Rm → R as follows
,,、	√	√3	BL Ln ∖	…
fC"(X) = ri 卜石(m +1)3/2，4, 0，m，1)	(IO)
and
1n	L
FC(x)，- EfC,i(x) = 4n kB(m, 1)xk2 -
n i=1	n
√3	BL
T (m +1)3∕2n hem, xi.
Note that the fji are L-smooth and convex, and ∣∣xo — x*∣∣2 ≤ B (see Proposition G.1 in Appendix
for more details). Next We shoW the loWer bound for functions fC,i defined above.
Theorem 4.5. Suppose that
B2L
ε ≤ ---
384n

and m
—1.
In order to find X ∈ Rm such that EFC (X) — FC (x*) < ε, A needs at least Ω (n + B ,n⅛L) queries
to hFC.
B2L
To derive Theorem 3.3, we also need the following lemma in the case ε > 3^.
Lemma 4.6. For any PIFO algorithm A and any L, n, B, ε such that ε ≤ LB2∕4, there exist n
L-smooth and convex functions {f : R → R}n=1 such that |xo — x*| ≤ B. In order to find X ∈ R
such that EF(X) — F(x*) ‹ ε, A needs at least Ω(n) queries to hp.
It is worth noting that if ε > BL, then Ω(n) = Ω (n + B J").Thus combining Theorem 4.5
and Lemma 4.6, we obtain Theorem 3.3.
4.3 Average Smooth Case
Zhou and Gu (2019) established lower bounds of IFO complexity under the average smooth assump-
tion. Here we demonstrate that our technique can also develop lower bounds of PIFO algorithm
under this assumption.
7
Under review as a conference paper at ICLR 2020
4.3.1	F IS S TRONGLY CONVEX
For fixed L0, μ, ∆, n, ε, We set L
Definition 4.1.
Jn(L022- “2) - μ2, and consider {fsc,i}n=ι
and FSC defined in
Proposition 4.7. For n ≥ 2, we have that
1.	FSC(x) is μ-strongly convex and {fsc,i}n=ι is L-average smooth.
2∙ If L0 ≥ √3(2 + 1
Theorem 4.8. Suppose that
then we have p3 L0 ≤ L ≤ pnLL0 and L∕μ ≥ n/2 + 1.
μ ≥ r3( n+1),ε ≤ δ9( √∣+ι!，and m=4 (Srnf+1)log( ⅛)+1.
In order to find X ∈ Rm such that EFSC(X) — Fsc(x*) < ε, PIFO algorithm A needs at least
Ω ( (n + n3/4 JLO) log (暂))queries to hrsc.
4.3.2	F IS CONVEX
For fixed L0, B, n, ε, we set L = PnL0, and consider {fc,i}n=ι and FC defined in Definition 4.4.
It folloWs from Proposition 2.5 that {fC,i}in=1 is L0-average smooth.
Theorem 4.9. suppose that
V √2 B2L0	, 一 √18r -1/4 ∕L7∣	1
ε ≤ TTTF-尸 and m = Ir) Bn ∖ — — 1.
768 n	12	ε
In order to find X ∈ Rm such that EFC (X) — FC (x*) < ε, A needs at least Ω (n + Bn3/4 JLɔ
queries to hFC.
Similar to Lemma 4.6, we also need the following lemma for the case ε > 森 B√L.
Lemma 4.10. For any PIFO algorithm A and any L, n, B, ε such that ε ≤ LB2/4, there exist n
functions {f : R → R}n=ι which is L-average smooth, such that F (x) is convex and ∣∣xo — x* k2 ≤
B. In order to find X ∈ R such that EF (X) — F (x*) < ε, A needs at least Ω(n) queries to hr.
Similarly, note that if ε > 寐 B√f, then Ω(n) = Ω (n + Bn3/4 ʌ/Lɔ. In summary, we obtain
Theorem 3.7.
5 Conclusion and Future Work
In this paper we have studied lower bound of PIFO algorithm for smooth convex finite-sum optimiza-
tion. We have given a tight lower bound of PIFO algorithms in the strongly convex case. We have
proposed a novel construction framework that is very useful to the analysis of proximal algorithms.
Based on this framework, we can extended our result to non-strongly convex, average smooth and
non-convex problems easily (Appendix J). It would be interesting to prove tight lower bounds in
more general setting, such as F is of (σ, L)-smoothness while each fi is (l, L)-smoothness.
References
Alekh Agarwal and Leon Bottou. A lower bound for the optimization of finite sums. In ICML, 2015.
Zeyuan Allen-Zhu. Katyusha: The first direct acceleration of stochastic gradient methods. Journal
ofMachine Learning Research,18(1):8194—8244, 2017.
8
Under review as a conference paper at ICLR 2020
Zeyuan Allen-Zhu. Katyusha X: Practical momentum method for stochastic sum-of-nonconvex
optimization. In ICML, 2018.
Yossi Arjevani and Ohad Shamir. Communication complexity of distributed convex learning and
optimization. In NIPS, 2015.
Yair Carmon, John C. Duchi, Oliver Hinder, and Aaron Sidford. Lower bounds for finding stationary
points I. arXiv preprint arXiv:1710.11606, 2017.
Aaron Defazio. A simple practical accelerated method for finite sums. In NIPS, 2016.
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. SAGA: A fast incremental gradient
method with support for non-strongly convex composite objectives. In NIPS, 2014.
Cong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang. Spider: Near-optimal non-convex
optimization via stochastic path-integrated differential estimator. In NIPS, 2018.
Robert Hannah, Yanli Liu, Daniel O’Connor, and Wotao Yin. Breaking the span assumption yields
fast finite-sum minimization. In Advances in Neural Information Processing Systems, pages
2312-2321,2018.
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance
reduction. In NIPS, 2013.
Guanghui Lan and Yi Zhou. An optimal randomized incremental gradient method. Mathematical
programming, pages 1-49, 2017.
Yurii Nesterov. A method for solving the convex programming problem with convergence rate
o(1∕k^2). In Dokl. akad. nauk Sssr, volume 269, pages 543-547, 1983.
Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer
Science & Business Media, 2013.
Mark Schmidt, Nicolas Le Roux, and Francis Bach. Minimizing finite sums with the stochastic
average gradient. Mathematical Programming, 162(1-2):83-112, 2017.
Blake Woodworth and Nathan Srebro. Tight complexity bounds for optimizing composite objec-
tives. In NIPS, 2016.
Lin Xiao and Tong Zhang. A proximal stochastic gradient method with progressive variance reduc-
tion. SIAM Journal on Optimization, 24(4):2057-2075, 2014.
Lijun Zhang, Mehrdad Mahdavi, and Rong Jin. Linear convergence with condition number inde-
pendent access of full gradients. In NIPS, 2013.
Dongruo Zhou and Quanquan Gu. Lower bounds for smooth nonconvex finite-sum optimization. In
ICML, 2019.
9
Under review as a conference paper at ICLR 2020
A Comparison of required number of dimensions
	Previous Lower Bounds	Our Lower Bounds
fi is L-smooth and μ-strongly convex	#PIFO=Ω (n + √κn log(ε)) d = O (Kn log5 (ε)) (Woodworth and Srebro, 2016)	#PIFO = Ω ((n + √Kn) log( j)) d = O (Pnlog (I)) [Theorem 3.1]
fi is L-smooth and convex	#PIFO=Ω (n + qnL) d = O (L log (1)) (Woodworth and Srebro, 2016)	#PIFO = Ω (n + qnL) d=O (1+qi) [Theorem 3.3]
{fi }in=1 is L-average smooth and f is μ-strongly convex	#IFO=Ω (n + n37√Klog (ɪ)) d = O (n + n3R√Klog (j)) (Zhou and Gu, 2019)	#PIFO=Ω ((n + n3∕4√K) log (ɪ)) d = O (n-1∕4√Klog (1)) [Theorem 3.5]
{fi }in=1 is L-average smooth and f is convex	#IFO = Ω (n + n3/4 把) d = O (n + 君/4 qL) (Zhou and Gu, 2019)	#PIFO = Ω (n + n3/4 ^L) d = O (1 + nT∕4 q) [Theorem 3.7]
Table 2: We compare our PIFO lower bounds with previous results, including the number of PIFO
or IFO calls to obtain ε-suboptimal point and the required number of dimensions in corresponding
construction.
B Comparison with existing proofs
Recall the adversary function we used is (please see detailed defintion in Section 2)
1n
r(x； λo, λι, λ2, m, ω)，η 工 ri(x; λo, λι, λ2,m, ω)
=λ1 x>A(m,ω)x + λ? ∣∣xk2 - λ0hem, x),
(11)
(12)
where
λ1 P bl(m, ω)>x22 + λ2 kxk22 - λ0hem, xi, for i = 1,
ri3λ0,λl,λ2,m,ω)="lp1 同m,ω)>x∣∣2 +λ2 kxk2 ,
l∈Li	2
for i = 2, 3, . . .
n.
The constructions in previous work (Lan and Zhou, 2017; Zhou and Gu, 2019) for IFO algorithms
employ an aggregation of r , that is,
1n
f(x) , —£fi(x)，where fi(x) = nr(xi),
i=1
-χj
x2
X =	.	∈ Rmn, and Xi ∈ Rm for i = 1,...,n.
.
.
Xn
10
Under review as a conference paper at ICLR 2020
The disadvantage of their construction is in that the important property from Lemma 2.6, which we
can obtain information of only one extra dimension at each PIFO query, cannot be held. Note that
the framework in this paper is the first lower bound analysis that utilize the decomposition form (11),
which makes the “only one” property also hold for PIFO query. The previous works only consider
the presentation (12) and are unaware of the decomposition (11). Moreover, the fact r : Rm → R
and f : Rmn → R provide an intuitive understanding why our construction requires a smaller
dimension (see Table 2).
The analysis in (Woodworth and Srebro, 2016; Fang et al., 2018) considers a very complicated
approach to dealing with the proximal operator (completely different from how to deal with gradient
operator). In contrast, our construction holds “only one” property (Lemma 2.6) both for proximal
and gradient operator, which leads the proof is more concise. Our construction more clearly shows
that PIFO algorithms are not more powerful than IFO algorithms in the sense of lower complexity
bound. We also use our technique to prove the tight lower bound of PIFO algorithm when κ = O(n),
which is a new result.
C Detailed Proof for Section 2
In this section, we use kAk to denote the spectral radius of A.
For simplicity, let
-	-1 1
-1	1
B = B(m,ω) =	.」.「	∈ Rm×m,
-1	1
ω
bl> is the l-th row of B, and fi(x) = ri(x; λ0, λ1, λ2, m, ω).
Recall that
Li = {l : 1 ≤ l ≤ m,l ≡ i — 1(modn)}, i = 1, 2,…，n.
For 1 ≤ i ≤ n, let Bi be a submatrix which is formed from rows Li of B, that is
Bi = B[Li; ]
Then fi can be wriiten as
fi(x) = λ1 kBixk22 + λ2 kxk22 - ηihem, xi,
where η1 = λ0 , ηi = 0, i ≥ 2.
Proof of Proposition 2.5. Note that
hu,Bi>Biui = kBiuk22
= X (bl>u)2
l∈Li
=ΓPl∈Li∖{m} (Um-I - um-l+1)2 + ^um (if m ∈ Li)
l∈Li (um-l - um-l+1)2
≤ 2 kuk22 ,
where the last inequality is according to (x+y)2 ≤ 2(x2 +y2), and |l1 -l2 | ≥ n ≥ 2 for l1, l2 ∈ Li.
Hence, Bi>Bi ≤ 2, and
IVfi(x)∖∖ = ∣∣2λιB>Bi + 2λ2I∣∣ ≤ 4λι + 2λ2.
Next, observe that
kVfi(x) — Vfi(y)k2 = ∖∖(2λιB> Bi + 2入小)(x — y)∖∖2
11
Under review as a conference paper at ICLR 2020
Let U = x — y.
Note that
> 入T	( (um-l — um-l + 1)(em-l — em-l + 1), l < m,
blbl u = lω2u1e1,	l = m.
Thus, if m ∈ Li, then
∣∣(2λ1BirBi + 2λ21 )u∣∣2
2
= 2λ1)： (Um-I — um-l + 1)(em-1 — em-l+1) + 2λ2u
l∈Li	2
= X [(2λ1(ul — ul⅛1) + 2λ2ul)2 + (—2λ1(ul — ul⅛1) + 2λ2ul⅛1)2] + X	(2λ2 ul)2
m-l∈Li	m-l∈Li
m-l+1∈Li
≤ X 8 [(λι+ λ2)2 + λ2] (u2 + u2+ι)+ 4λ2 I∣uk2 ∙
m-l∈Li
Similarly, if Im ∈ Li, then
∣∣(2λιB)Bi + 2λ2i)u ∣ ∣ 2
≤	^X	8 [(λ1 + λ2)2 +	λ2]	(u2 + u2+1) +	4(λ1ω2	+	λ2户u2	+	4λ2 I∣u∣∣2	∙
m-l∈Li
l=0
Therefore, we have
1 n
-EkVfi(X)— Vfi (y)k 2
[	-m — 1
≤ ~ ^X 8 [(λ1 + λ2)2 + λ2] (u2 + u2+1) + 4(2λι + λ2)2u1
n L l=1
+ 4λ2 l∣uk2
16 .	CCrC	C
≤ 五[(λ1 + λ2) + λ1] ∣∣u∣∣2 + 4λ2 lluk
2
2，
where we have used (2λ1 + λ2)2 ≤ 2 [(λ1 + λ2)2 + λ2].
In summary, we get that {fi}1≤i≤n is L0-average smooth, where
L0 = 2a∕- [(λ1 + λ2)2 + ʌ?] + λ2.
V n
□
ProofofLemma 2.6. For X ∈ Fk (k ≥ 1), we have
b>x = 0 for l > k,
bl ∈ Fk for l < k,
bk ∈Fk+1.
Consequently, for l = k, blb>X = (b>x)bl ∈ Fk, and bkb>X ∈ Fk+1.
For k = 0, we have X = 0, and
Vf1(x) = λ°em ∈ F1,
Vfj(x) = 0 (j ≥ 2).
12
Under review as a conference paper at ICLR 2020
Moreover, we suppose k ≥ 1, k ∈ Li. Since
Vfj(x) = 2λιB>BjX + 2λ2x - ηjem
= 2λ1	bl>blx + 2λ2x - ηjem.
l∈Lj
Hence, Vfi(x) ∈ Fk+1 and Vfj(x) ∈ Fk (j 6= i).
Now, we turn to consider u = proxfγ (x). We have
2λιB>Bj + (2λ2 +—) l) U = ηj-em +— x,
i.e.,
u = c1(I + c2Bj>Bj)-1y,
Where c1 = 2λ2 + l∕γ, c2 = 2λ2+l∕γ, and y = η em + Y x.
Note that
(I + c2B>Bj)-1 = I - B> (C11 + BjB>) - Bj.
If k = 0 and j > 1, We have y = 0 and u = 0.
If k = 0 andj =1, We have y = λ0em. On this case, B1em = 0, so u = c1y ∈ F1.
For k ≥ 1, We knoW that y ∈ Fk. And observe that if |l - l0| ≥ 2, then bl>bl0 = 0, and conse-
quently Bj B> is a diagonal matrix, so we can assume that c11 + Bj B> =diag(βj,1, ∙ ∙ ∙ ,βj,∣Lj ∣).
Therefore,
|Lj|
u = c1y - c1	βj,sblj,sbl>j,sy,
s=1
where we assume that Lj = {lj,1,…，j,∣Lj∣}.
Thus, we have proxfγ (x) ∈ Fk+1 for k ∈ Li and proxfγ (x) ∈ Fk (j 6= i).
□
Proof of Corollary 2.7. Denote
span{Vfiι (xo),…，Vfit (xt-1), proxf： (xo),…，Proxf： (xt-1)}
by Mt. We know that xt ∈ Mt .
Suppose that MT ⊆ Fk-1 for some T and let T0 = arg min t : t > T, it ≡ k(mod n).
By Lemma 2.6, for T < t < T 0, we can use a simple induction to obtain that
sPan{Vfit(xt-1),Proxfγit (xt-1)} ⊆ Fk-1
and Mt ⊆ Fk-1.
Moreover, since iT0 ≡ k( mod n), we have
sPan{VfiT 0 (xT 0 -1), ProxfγiT 0 (xT0-1)} ⊆ Fk
and MT0 ⊆ Fk .
Following from above statement, it is easily to check that for t < Tk, we have xt ∈ Mt ⊆ Fk-1.
Next, note that
P (Tk - Tk-1 = s)
=P (iTk-1 + 1 ≡ k(mod n),…，iTk-1+s-1 ≡ k(mod n), iTk-1+s ≡ k(mod n))
=P (iTk-ι + 1=k0,…八Tk-1 +s-1 6= k0 , i
Tk-1 +s = k
= ( - pk0)s-1pk0,
13
Under review as a conference paper at ICLR 2020
where k0 ≡ k(mod n), 1 ≤ k0 ≤ n. So Tk - Tk-1 is a geometric random variable with success
probability pk0 .
On the other hand, Tk - Tk-I is just dependent on iτk-1 + 1,…，m,thus for l = k, Tl - Tj is
independent with Tk - Tk-1.
Therefore,
kk
Tk = X(Tl -Tl-1) =XYl,
l=1	i=1
where Yl follows a geometric distribution with success probability ql = pl0 where l0 ≡ l(mod
n), 1 ≤ l0 ≤ n.
□
Proof of Remark 2.3. If each fi is L-smooth, then for any x, y ∈ Rm we have
l∣Vfi(x) - Vfi(y)k2 ≤ L2 kx - yk2,
and consequently,
1n
-E kVfi(x) - Vfi(y)k2 ≤ L2 kx - yk2.	(13)
n i=1
If {fi }in=1 is L-average smooth, then for any x, y ∈ Rm we have
2
n-2∣XX (Vfi(X)-Vfi(y)
kVf(x) -Vf(y)k22
2
≤ n-2 (XX kVfi(x)-Vfi(y)k2)
≤
≤
-n
—EkVfi (x)-Vfi (y)k 2
n
i=1
L2 kx - yk22 .
□
ProofofLemma 2.9. Denote minχ∈Rm f (x) by f *. Fort ≤ N, we have
Ef(xt) - f* ≥ E[f(xt) - f*|N < TM+ι]P(N < TM +1)
≥ E[ min f(x)- f *∣N<TM+ι]P (N <TM +1)
x∈FM
≥ 9εP(TM+1 > N),
where TM+1 is defined in (7), and the second inequality follows from Corollary 2.7 (if N < TM+1,
then xt ∈ FM for t ≤ N).
By Corollary 2.7, TM+1 can be written as TM+1 = PlM=+1 1 Yl, where {Yl}1≤l≤M+1 are independent
random variables, and Yl follows a geometric distribution with success probability ql = pl0 (l0 ≡
l(mod n), 1 ≤ l0 ≤ n). Moreover, recalling that pi ≤ p2 ≤ ∙∙∙ ≤ pn, we have PM+1 ql ≤ M+1.
Therefore, by Lemma 2.8, we have
P (Tm+1 >N )= P XXYl > —
16	、 1
9(M + 1) ≥ 9,
≥ -
Hence, we can conclude that Ef(xN) - f* ≥ 9εP (TM +1 > N) ≥ ε.
□
14
Under review as a conference paper at ICLR 2020
Remark In fact, a more strong conclusion hosts:
E min f (Xt) — min f (x) > ε.
t≤N 八 x	x∈Rm ' 一
D Results about Sum of Geometric Distributed Random
Variables
Lemma D.1. Let X∖ 〜Geo(pι), X2 〜Geo(p2) be independent random variables. For any
positive integer j, if pi = p2, then
P (X1 + X2 > j) = P2(1— pιj — pi(1— p2j ,	(14)
P2 — pi
and if pi = p2, then
P (X1 + X2 > j)= jpι(1 — PIjT+ (1 — pι)j.	(15)
Proof.
j
P (Xi + X2 > j ) = X P (Xi = I) P (X2 > j — I)+ P (Xi > j)
l=1
j
=X(I- pi)'Tpi(I- P*) T + (I- pi)j
ι=i
j	1 -1
=pi (1 — p2)jT X ( --pi )	+ (1 — PI))
片 V — p2√
Thus if Pi = P2, P (Xi + X2 > j) = jpi(1 — PijTI + (1 - Pij.
For pi = p2,
1P√X I V -ʌ (1 — Pij — (1 — p2j ,门 j
P (Xi + X2 > j) = Pi--------------------+ (1 — Pi)j
P2 — Pi
_ P2(1 — Pi)j — Pi(1 — P2)j
—	.
p2 — pi
Lemma D.2. For x > 0 and j > 2,
1—
j-i
□
(16)
j — 1
———≤
X + j/2 —
x
x + 1
Proof. We just need to show that
(x + IjTi(X + j/2) — (j — 1)(x + IjTI ≤ XjTi(X + j/2),
that is
(x + 1)j — j(x + IjTi∕2 — XjTi(X + j/2) ≤ 0,
jT2
i.e.,
l = 0
xl ≤ 0.
Note that for l ≤ j — 2,
(j) - j (j — 1) = L)(j)≤ 0
thus inequality (16) hosts for X > 0 and j > 2.
□
15
Under review as a conference paper at ICLR 2020
Lemma D.3. Let Xi 〜Geo(pι), X2 〜Geo(p2),H,K 〜Geo (p1+ p2) be independent random
variables with 0 < pi ≤ p2 ≤ 1. Thenfor any positive integer j, we have
P (Xi + X2 >j) ≥ P (Y1 + K >j).
Proof. If j = 1, then P (Xi + X2 > j) = 1 = P (Yi + K > j).
If pi = p2 = 1, then P (Xi + X? > j) = 0 = P (Yi + Y2 > j) for j ≥ 2.
Let j ≥ 2, and C，Pi + p < 2 bea given constant.
We prove that f (Pi)，P (Xi + X2 > j) is a decreasing function.
Employing equation (14), for Pi < c/2, we have
f(Pi)
(c - pi)(1 - Piy - pi(1 + pi - c)j
c - 2pi
and
f 0(Pi)
-(1 - Pi)j - j(c - Pi)(1 - Pi)j-i - (1 + Pi - c)j - jpi(1 + Pi - c)j-i
+2
c - 2pi
(c - pi)(1 - Piy - pi(1 + Pi - c)j
(c - 2pi)2
[c(1 - Pi) - j(C -Pi)(C - 2pi)](1 -pi)j-i - [c(1+ Pi - C) + jpi(c - 2pi)](1 + Pi - c)j-i
(C - 2pi)2
Hence f 0(pi) < 0 is equivalent to
c(1 - Pi) - j(C -Pi)(C - 2pi) < (1+ Pi - C'j-i
c(1 + Pi - c) + jpi(c - 2pi)	< 1 - Pi )
(17)
Note that
c(1 - Pi) - j(c - Pi)(C - 2pi)
c(1 + Pi - c) + jpi(c - 2pi)
1	(j - 1)c(c - 2pi)
1 - ..---------------------------
c(1 + Pi - c) + jpi(c - 2pi)
I_____jj∑l—
i+pi-c + j pi
c-2pi ' JC
Denote x = i+p1-c. If C ≤ 1, then Pi > 0 and x > i-C ≥ 0. And if c > 1, then Pi ≥ C — 1 and
X ≥ i+C-i-c = 0.
2-c
Rewrite inequality (17) as
j-1
x +jPi/c
1-
<
∖ j-i
x y
x + 1 √
Recall inequality (16), we have
ɪ 丫-i≥ 1 - jɪ > 1 - L 1 .
X + 1 y	X + j/2	X + jpi/c
Consequently, f0(pi) < 0 hosts for Pi < c/2 and j ≥ 2.
With the fact that limpι→c∕2 f (pi) = f (c/2) according to equation (15), we have
P (Xi + X2 >j) ≥ P (Yi + K >j).
for any positive integer j and 0 < Pi ≤ P2 ≤ 1.
□
16
Under review as a conference paper at ICLR 2020
Corollary D.4. Let Xi 〜Geo(pι), X2 〜Geo(p2), Yi, Y2 〜Geo (p1+ p2) be independent random
variables with 0 < p1 ≤ p2 ≤ 1. Suppose Z is a random variable that takes nonnegative integer
values, and Z is independent with Xi, X2, Yi, Y2. Then for any positive integer j, we have
P(Z+Xi+X2 >j) ≥P(Z+Yi+Y2 >j).
Proof. With applying Lemma D.3, we have
j-i
P(Z+Xi +X2 >j) = XP(Z = l)P(Xi +X2 > l-j) +P(Z >j- 1)
l=0
j-i
≥XP(Z=l)P(Yi+Y2 >l-j)+P(Z>j-1)
l=0
=P(Z+Yi+Y2 >j).
□
Corollary D.5. Let {Xi}i≤i≤m be independent variables, and Xi follow a geometric distribution
with success probability pi . For any positive integer j, we have
P Xm Xi ≥ j ! ≥ P
where {Yi}ι≤i≤m are i.i.d. random variables, Yi 〜 Geo(ENi Pi/m), and Yi is independent with
Xi0 (1 ≤ i0 ≤ m).
Proof. Let
f (P1,P2,…，Pm)
m
, P	X Xi
≥j
Our goal is to minimize f (pi,p2,…，pm) such that Pm=I Pi = S < 1.
By Corollary D.4, we know that
f(Pi,P2,…，Pi,…，Pj, ∙ ∙ ∙ ,Pm) ≥ f (Pi,P2,…,
Pi + Pj
-2~
pΦ ,…,Pm ).
This fact implies that (pi,P2, ∙∙∙ ,Pm) such that Pi = P2 = •…=Pm = S/m is a minimizer of the
function f .
□
Lemma D.6. Let {Xi}i≤i≤m be i.i.d. random variables, and Xi follows a geometric distribution
with success probability P. We have
16
≥ 1----
9m
P
(18)
Proof. Denote Pim=i Xi by τ. We know that
ET = m, Var(T) = m1≠
P	P2
17
Under review as a conference paper at ICLR 2020
Hence, we have
P (τ > ；ET
=1 — P (T — ET ≤ — ；ET)
≥ 1 — P (|t — ET| ≥ 3ET)
16Var(T)
≥ 1--------y~L
≥	9(Et )2
I	16m(1 — P)〉］	16
9m2	9m
□
Corollary D.7. Let {Xi }1≤i≤m be independent random variables, and Xi follows a geometric
distribution with success probability pi. Then
m2
Xi > 4(PLpi)
16
≥ 1-----
9m
E Proof of Theorem 4.2
Proposition E.1. For any n ≥ 2, m ≥ 2, fSC,i and FSC in Definition 4.1 satisfy:
1.	f sc,i is L-smooth and μ-strongly convex.
2.	The minimizer of the function FSC is
x* = argminFSC(x) = /,尸广1； (qm, qm-1,…，q)>,
x∈Rm	V(L — μ)(α — I)
where q = αα+-ι. Moreover, FSC(x*) = —∆.
3.	For 1 ≤ k ≤ m — 1, we have
min FsC(x) — FsC(x*) ≥ ∆q2k.	(19)
x∈Fk
Proof.
1.
Just recall Proposition 2.5.
2.
Denote
2∆n(α+1)2
(L-μ)(α-1).
Let VFSc(x) = 0, that is
Lnμ A (rαn)+“I 卜
n(α+ 1)
L — μ
or
ω2 + 1+ 件 —1
I I L-μ
—1	2+ 泮—1
1	L-μ
0
0
x
(20)
—1	2+L-μ
—1
—1
2 I	2nμ
1 + L-μJ
0
2ξ
-a+1 -
18
Under review as a conference paper at ICLR 2020
Note that q = 0-ɪ is a root of the equation
z2
—(2+ι-nμμ)z+1 = 0,
and
2
a + 1
J + 1 +丹=1,
L — μ q
11	— q = — q2 + (1 + 2nμ )q∙
L 一 μ
Hence, it is easily to check that the solution to Equation (20) is
X*= ξ(qm,qm-1,…，q)>,
and
FSC(X*)
L — μ
2n(α + 1)
ξ2 q = —∆∙
—
3. If x ∈ Fk, 1 ≤ k < m, then xι = x2 =…=xm-k = 0.
Let y = Xm-k+im ∈ Rk and Ak be last k rows and columns of the matrix in Equation
(21). Then we can rewrite F(x) as
Fk (y), FSC(X) = L^ y>Ak y — nL⅛μL) ξhem, yi∙
Let VF1k (y) = 0, that is
2+ ”	—1
—1 μ 2+ 科
—1
0
0
y
(21)
—1
2+普
L一μ
—1
—1
1 I	2nμ
1 + L-μJ
0
2ξ
-0+1 -
By some calculation, the solution to above equation is
ξqk+1
1 + q2k+1
(q-1 — q,q-2 - q2,…，q-k - qk)>
Thus
L μ 1	q2k	1	q2k
min FSC(X) = min Fk⑹=—	,,1∕2q12k+1 = δ12k+1,
χ∈Fk	y∈Rk	2n(α + 1)	1 + q2k+1	1 + q2k+1
and
xm⅛ FSC(X)- FSC(X*)=δ (1 - ⅛&)
= ∆q2k τ++⅛τ ≥ ∆q2k ∙
□
Proof of Theorem 4.2. Let M = [log(Io；优],then we have
arg min F⅛c(x) — FSC(X*) ≥ ∆q2M ≥ 9ε,
x∈Fm
where the first inequality is according to the third property of Proposition E.1.
19
Under review as a conference paper at ICLR 2020
Following from Lemma 2.9, for M ≥ 1 and N = (M + 1)-∕4, we have
minEFSC(Xt) - FSC(X*) ≥ ε∙
Therefore, in order to find X ∈ Rm such that EFSC(X) - FSC(X*) < ε, A needs at least N queries
to hFSC .
We estimate - log(q) and N in two cases.
1. If L∕μ ≥ n∕2 + 1, then α =q2LLμ― + 1 ≥ √2. Observe that function h(β)
--∕β+ι ∖ - X is increasing when β > 1. Thus, we have
log( β-1 )
—
1
iog(q)
占 ≥ α2+h(√2)
2 ʌ/2 ∕μ-~1 +1+h(√2)
2 L/ - 1 + 1) + h(√2)
≥
≥
1 产m+√42+h(√2),
n
and
n
N = (M +1)n∕4 =-
+1
log(9ε∕∆)
2logq
δ
2. If 2 ≤ L∕μ < n/2 + 1, then We have
1 a α α ( α +1 ʌ 1	∕i∣	2(a - 1)
-log(q) = log ——7 =log 1 + J一1
α -1	α2-1
log 1 +
J2 * + 1-1
L∕μ-1
n
≤ log (-! ≤ log (T/产
≤ log 1 +En
(22)
where the first inequality and second inequality follow from L∕μ - 1 < n/2 and the last
inequality is according to X-ɪ ≤ X for X ≥ 2.
Note that n ≥ 2, thus n-ɪ ≤ 2 ≤ ɪ/n-ɪ, and hence n ≥ L∕μ, i.e. log(nμ∕L) ≥ 0.
Therefore,
n
N = (M +1)-/4 ≥ —
8
Ω ( ( _____-______
Vv + log(nμ∕L)
—
log(q)) log(9e
)log (∆)).
20
Under review as a conference paper at ICLR 2020
Recalling that We assume that 9ε∕∆ ≤ q2, thus We have
n
N≥
log(q)
n
(-2iog(q)) = 4
—
8
Therefore, N = ω (n + (i+*l)) log (∆ε)).
At last, We must to ensure that 1 ≤ M < m, that is
1 ≤ *∆2 <m.
(23)
Note that limβ→+∞ h(β) = 0, so -1/ log(q) ≤ α∕2. Thus the above conditions are satisfied when
m
⅞^ + 1 ≤ 1 (fLE1+ [log ( δ )
2(- log q)	4	n	9ε
and
三 ≤ 1 (0≡J)2
△ - 91a +"
□
F Lower b ound for another form of suboptimal solution
Defazio (2016) shoWed that the PIFO algorithm Point SAGA has the convergence result
Ekxt - x*k2 ≤ (q0)t ∣∣xo - x*k2, where q0 satisfies -1/log(q0) = O (n + /nL∕μ). To match
this form of upper bound, We point out that a similar result holds for {fSC,i}in=1.
Theorem F.1. Suppose that
≥ ≥ n + 1, ε ≤ ɪ 2 2-1! , and m
μ — 2	' — 18 ∖√2+1
2 G + 1 log("+L
n	18ε
In order to find x ∈ Rm such that Ekx — x*∣2 < ε ∣∣xo — x*∣2, PIFO algorithm A needs at least
Ω
nL) log (ɪ)) queries to hp^.
Proof. Denote ξ = J⅛-⅛+¾, and M = [⅛M
For 1 ≤ M ≤ m/2, N = n(M + 1)/4 and t ≤ N, we have
Ekxt- x* k2 ≥ E kxt - x* k2 N < TM+1 P (N < TM +1)
≥E min kx - x*k22 N <TM+1 P(N<TM+1)
x∈FM
≥
1
一 min
9 x∈FM
kx-x*k22.
where TM+1 is defined in (7), the second inequality follows from Corollary 2.7 (if N < TM+1,
then xt ∈ FM for t ≤ N), and the last inequality is established because of our Corollary 2.7 (More
detailed explanation refer to our proof of Lemma 2.9).
By Proposition E.1, we know that x* = ξ(qm, qm-1,…,q)>, and
2 2(m+1)
kx0 - x*k2 = kx*k2 = ξ2—i---2—
1-q
21
Under review as a conference paper at ICLR 2020
Note that if x ∈ FM , then x1 = x2
xm-M
0, thus
2(m-1+i)	p q2(M+1) - q2(m+1
=ξ -------i---2------
1 - q2
m
min Ilx -x*k2 = ξ2	X q
x∈FM
l=m-M
Thus, for t ≤ N and M ≤ m/2, we have
Ekxt- x*k2
kxt - x*k2
2M - q2m
1 - q2m
18 q
2M = 1⅛ q2b 甯q) c≥ ε,
≥
≥
1 q
9—
1
where the second inequality is due to
q2M - q2m
1 - q2m
q2M
^^2^
q2M - 2q2m + q2(m+M)
2(1 - q2m)
—
≥
q2M
2(1 - q2m
q2M
2(1 - q2m
)(1
)(1
- 2q2(m-M) + q2m)
- 2qm + q2m) ≥ 0.
Therefore，in order tofind x ∈Rm SUch that Exx≡! <
ε, A needs at least N queries to hFSC .
As We have showed in proof of Theorem 4.2, for L∕μ ≥ ―/2 + 1, we have
1 r Lμ―+1 ≥- ɪ ≥cι (尸-ɪ+1
2	—	log(q)	—
and
n	n log(18ε)
N = —(M + 1) ≥ —- g()
4 1	4 ~ l 2log q
≥ cl (— + P—(L∕μ -1)) log (W
ω( (—+ vɪ/L)log
At last, we have to ensure that 1 ≤ M ≤ m/2, that is
log(18ε)
1 ≤ 上L < m/2.
—2log q /
The above conditions are satisfied when
m =log(1∕(18ε))+1
- log q
2 L/μ_1 + 1)log(γ1-)+1 = O( J-L log
—	)	∖ 18ε)	∖ — —μ
and
ε ≤ q~q2.
ι 18y
Observe that when L∕μ ≤ —/2 + 1, we have ɑ ≥ √2 and q = αα+1 ≥ √2-1. Hence, We just need
ε ≤ 1⅛ (√2+1 )2 ≈ 0.00164.
□
22
Under review as a conference paper at ICLR 2020
G Proof of Theorem 4.5
Proposition G.1. For any n ≥ 2, m ≥ 2, following properties hold:
1.
2.
fC,i is L-smooth and convex.
The minimizer of the function FC is
X* = arg min FC(x) = 2ξ (1, 2,…,m)> ,
x∈Rm
3.
where ξ = √ (mBL3∕2. Moreover, Fc(x*) = -m2 and ∣∣xo - x*k2 ≤ B2.
For 1 ≤ k ≤ m, we have
min FC(X) - FC(X*)
x∈Fk
ɪ-(m - k).
n
(24)
Proof.
1.
Just recall Proposition 2.5.
2.
Denote ξ = √ (m+B?/2n. Let VFC(X) = 0, that is
2nA(I)X
3.
or
-1
一 2	-1
-1	2	-1
ξ
em,
n
(25)
Hence, it is easily to check that the solution to Equation (25) is
1,2,∙∙∙ ,m)>,
and
Fc(x*)
mξ2
nL
Moreover, we have
kX0 -X*k22
4ξ2 m(m + 1)(2m + 1)
L2
≤
+1)3=B2.
By similar calculation to above proof, we have
arg min Fc (x)
x∈Fk
1, 2,…，k)>,
and
min Fc (X)
x∈Fk
kξ2
nL
_ *
X
—
—
6
Thus
min FC(X) — FC(x*) = --(m — k).
x∈Fk	n
23
Under review as a conference paper at ICLR 2020
□
Proof of Theorem 4.5. Since ε ≤ B^, We have m ≥ 3. Let ξ =苧(①：卜/2
For M = [m-1J ≥ 1, we have m 一 M ≥ (m + 1)/2, and
min FC(X) — F⅛(x*)= 亍(m 一 M)
x∈FM	nL
3B2L m 一 M
4n (m + 1)3
≥
等 (m⅛ ≥ 9ε,
where the first equation is according to the 3rd property in Proposition G.1 and the last inequality
follows from m + 1 ≤ BpL∕(24nε).
Similar to the proof of Theorem 4.2, by Lemma 2.9, we have
minEFC(Xt) - FC(x*) ≥ ε.
In other words, in order to find X ∈ Rm such that EFC(X) — Fc(x*) < ε, A needs at least N queries
to hF .
At last, observe that
N = (M + 1)n∕4= 4 m+1
B2L
where we have recalled ε ≤ |^ in last equation.
□
H Proof of Lemma 4.3, Lemma 4.6 and Lemma 4.10
Proof of Lemma 4.6. Consider the following functions {gi}1≤i≤n, gi : R → R, where
g1 (X)	二 微x2 — n—Bx,
gi(X)	=—x2,
G(X)	1n	—2 二 一)：gi (x)	—X - -Bx. n2 i=1
First observe that
x* = arg min G(X) = B,
x∈R
LB2
G(0) — G(x*) = -2-,
and |x0 一 x* | = B.
For i > 1, we have d*x) ∣x=o = 0 and ProxYi (0) = 0. Thus Xt = 0 will host till our first-order
method A draws the component f1 . That is, for t < T = arg min{t : it = 1}, we have xt = 0.
24
Under review as a conference paper at ICLR 2020
Hence，fort ≤ 2p1，we have
EG(Xt) - F(x*) ≥ E G(Xt) - G(χ*)∣ɪ < T P
2p1
2P1 ‹T
LB2 p( 2P1 ‹T).
Note that T follows a geometric distribution with success probability p1 ≤ 1/n, and
p(t> 2p1) = p(t> [2p1 D = (I-Pi)j2p1k
≥ (1 - P1)2p1 ≥ (1 - l/n)n/2 ≥ 1,
where the second inequality follows from h(z) = log21-Z) is a decreasing function.
ThUs,fort ≤ 2p1 ,we have
LB2
EG(Xt)- F(x*) ≥ 丁 ≥ ε
Thus, in order to find X ∈ R such that EF(X) - F(x*) < ε, A needs at least * ≥ n/2 = Ω(η)
queries to hG .
□
Proof of Lemma 4.10. Note that {gi}in=1 defined in proof of Lemma 4.6 is also L-average smooth,
so Lemma 4.10 hosts for the same reason.	□
ProofofLemma 4.3. Let B = νz2∆∕L. Then ε∕∆ ≤ 1/2 is equivalent to ε ≤ LB2∕4. Note
that {gi}η=1 defined in proof of Lemma 4.6 is also μ-strongly convex for any μ ≤ L, and satisfy
|G(0) - G(x*)| = ∆. Therefore Lemma 4.3 hosts for the same reason.
□
Remark H.1. Suppose that
ɪ > 1 (』)2 ,α = ∖∣23 + 1.
∆	9 ∖α + 1 / V n
1.	If K ≥ n/2 + 1, then we have α ≥ √2 and
(n + √Kn) log (j) ≤ 2 (n + √Kn) log (。+1
9ε	α - 1
≤ 4(n + 尸=O(n) +
α-1
4 √κn
(1 - √2∕2)α
≤ O(n) +
4	√Kn
O(n),
where the second inequality follows from log(1+X) ≤ X and the last inequality is according
to α ≥ ,2κ∕n. That is
Ω(η) = Ω ((n + √Kn) log (ʌ
2.	If 2 ≤ L∕μ < n∕2 + 1, then we have
(1 + lonn〃∕L) ) log (91) ≤ ( 1+lonnμ∕L) ) eog (E
≤ (1 + log(nμ∕L)) (2log
(2√2 - 1)n
L/μ
O(n),
25
Under review as a conference paper at ICLR 2020
where the second inequality follows from (22). That is
°(n)5((l+Kn〃/L))lOgG) + n
I Detailed Proof for Section 4.3
Proof of Proposition 4.7.
1.	It is easily to check that FSC(X) is μ-strongly convex. Following from Proposition 2.5, then
{fSC,i }in=1 is L-average smooth, where
中）2+（彳了
L = t
=r
16
+ μ2
2(L2 + μ2 + μ2 = Lo.
n
n
2.	Clearly, L = Jn(L'；一μ2) - μ ≤ PnL0.
Furthermore, according to L ≥ npn (n2 + 1), we have
L2-3 L02
2(L02 - μ2) - μ2
2+ι)2 μ2
-nL02
3
n+2 2
F μ
2 (
n
—
2
——
8
2)μ2 ≥ 0,
and, L∕μ ≥ Pn^L0/μ ≥ n/2 + 1.
□
Proof of Theorem 4.8. By 2nd property of Proposition 4.7, we know that L∕μ ≥ n/2 + 1. More-
over,
m
=4 (S∏ μ+1 卜g ( δ )+1
≥ 4 (r2 2Lμ-+1! log ( δ )+1，
Then, by Theorem 4.2 5, in order to find X ∈ Rm such that EFSC(X) - FSC(x*) ‹ ε, A needs at
least N queries to hFSC , where
N
□
5By the proof of Theorem 4.2, a larger dimension m does not affect the conclusion of the theorem.
26
Under review as a conference paper at ICLR 2020
Proof of Theorem 4.9. Note that
√2 B2L0	B2L
ε ≤---=-=----,
—768 √n	384n
“=√8 BnT"\但
12 V ε
WL
- 1.
By Theorem 4.5, in order to find X ∈ Rm such that EFC(X) - FC(x*) < ε, A needs at least N
queries to hFC , where
□
J	Non-convex Case
In non-convex case, our goal is to find an ε-approximate stationary point X of our objective function
f , which satisfies
∣Nf(X)∣∣2 ≤ ε.	(26)
J.1 Preliminaries
We first introduce a general concept about smoothness.
Definition J.1. For any differentiable function f : Rm+1 → R, we say f is (l, L)-smooth, if for any
X, y ∈ Rm we have
∖ kx - yk2 ≤ f(X) - f(y) - Ef(y),X - yi ≤ L Ilx - yk2,
where L > 0, l ∈ R.
Especially, if f is L-smooth, then it can be checked that f is (-L, L)-smooth.
If f is (-σ, L)-smooth, in order to make the operator Proxf valid, We set Y > σ to ensure the
function
f(U) , f (U) + 2γ kx - uk2
is a convex function.
Next, We introduce a class of function Which is original proposed in (Carmon et al., 2017). Let
GNC : Rm+1 → R be
1	m
GNC(x; α, m) = - ∣∣B(m + 1, √S)x∣∣2 - √α(eι, X + a£Γ(x"
2	i=1
Where the non-convex function Γ : R → R is
Γ(x)，120 ∕x t2ι(+-t1) dt.	(27)
We need folloWing properties about GNC(X; α, m).
27
Under review as a conference paper at ICLR 2020
Proposition J.2 (Lemmas 3,4, Carmon et al. (2017)). For any 0 < α ≤ 1, it holds that
1.	Γ(x) is (—45(√3 — 1), 180)-smooth and Gnc(x; α, m) is (—45(√3 — 1)α, 4 + 180a)-
smooth.
2.	Gnc(0; α, m) — minχ∈Rm+ι GNC(x; a, m) ≤ Ja/2 + 10αm.
3.	For x which satisfies that xm = xm+1 = 0, we have
∣∣VGNCx; a,m)∣b ≥ a3/4/4.
J.2 Our Result
Theorem J.3. For any PIFO algorithm A and any L, σ, n, ∆, ε such that ε2 ≤ &16Lan ,there exist a
dimension d = [^^Ln^] + 1 and n (—σ, L)-smooth nonconvexfunctions {f : Rd → R}n=1 such
that f (x0) — f (x*) ≤ ∆. In order to find X ∈ Rd such that Ek Vf(X)Ib < ε, A needs at least
Ω (δ√) queries to hf, where we set a = min {l, (√3+[)”, 焉}.
Remark J.4. For n > 180, wehave
ω (δL≠) = ω (⅛ min k S√⅛1 √nσL, M卜(⅛ min{L,即)
Thus, our result is comparable to the one of Zhou and Gu’s result (their result only related to
IFO algorithms, so our result is more strong), but our construction only requires the dimension
be O(1 + δ min{L∕n, pσL∕n}), which is much smaller than O (合 min{L, √nσL}) in (Zhou
and Gu, 2019).
J.3 Constructions
Consider
F (x; a, m, λ, β) = λGNC(x∕β; a, m).
(28)
Similar to our construction We introduced in Section 2, We denote the l-th row of the matrix B(m +
1, √α) by bi and
Li = {l : 1 ≤ l ≤ m, m + 1 — l ≡ i(mod n)}, i = 1, 2,…，n.	(29)
Let Gk = span{eι, e2,…,ek}, 1 ≤ k ≤ m, Go = {0} and compose F(x; α, m, λ, β) to
fι(x; a,m,λ,β)=徐 P ∣∣b>x∣∣2 — λn√a he1, Xi + λa P Γ(g∕β),
l∈Li	i=1
∣fi(x; α,m,λ,β) = 2λn P ∣∣b>x∣∣2 + λa P Γ(x"β), for i ≥ 2.
l∈Li	i=1
(30)
Clearly, F(x; a, m, λ,β) = n Pn=ι fi(x; a, m, λ, β). Moreover, by Proposition J.2, we have fol-
loWing properties about F(x; a, m, λ, β) and {fi(x; a, m, λ, β)}in=1.
Proposition J.5. For any 0 < a ≤ 1, it holds that
1.	fi(x; a, m, λ, β) is (-45(√3T)aλ, (2n+β8Oa)λ) -smooth.
2.	F(0; a, m, λ, β) — minχ∈Rm+ι F(x; a, m, λ, β) ≤ λ(λ∕a∕2 + 10am).
3.	For x which satisfies that xm = xm+1 = 0, we have
∣VF(x; a,m,λ,β)∣2 ≥
a3∕4λ
4β
28
Under review as a conference paper at ICLR 2020
Similar to Lemma 2.6, similar conclusion hosts for {fi(x; α, m, λ, β)}in=1.
Lemma J.6. For X ∈ Fk, 0 ≤ k < m and Y < 吗+1 λβ^, we have
Vfi(x; α,m,λ,β), ProXfi (X) ∈ ^^i^^- - 1(mod 初
k , oerwse.
m
Proof. Let G(X) , P Γ(xi) and Γ0(x) be the derivative of Γ(x).
i=1
First note that Γ0(0) = 0, so if X ∈ Gk, then
VG(x) = (Γ0(xι), Γ0(x2),…,Γ0(xm))> ∈Gk.
Moreover, for X ∈ FG (k ≥ 1), we have
bl> x = 0 for l < m-	k,
bl ∈ Gk for l > m- k,
bm-k ∈ Gk+1 .
Consequently, for l 6= m-	k, blbl>x = (bl>x)bl ∈ Gk, and bm-kb>m-kx ∈ Gk+1.
For k = 0, we have x = 0, and
Vfι(x) = λn√ɑ∕β eι ∈ Gι,
Vfj(x) =0(j ≥2).
For k ≥ 1, we suppose that m-	k ∈ Li. Since
Vfj(X) = λnn X b>bιX + λβα VG(X/e) 一 ηjeι,
l∈Lj
where ηι = λn√a∕β, % = 0 for j ≥ 2.
Hence, Vfi(X) ∈ Fk+1 and Vfj(X) ∈ Fk (j 6= i).
Now, we turn to consider v = proXfγ (X).
We have
Vfj(v) + 1(v - x) = 0,
γ
that is
βnn x b>bl+JI)V+BaVG(V/e)=% e1+∣x∙
l∈Lj
Denote
λn	>	β	1	1
a = ɪ x bl bl + -I, U =万V, y = ηjeι + 一x,
β l∈Lj l	γ	β	γ
then we have
Au + αV VG(u) = y.
β
Next, if s satisfies
s > maX{1, k}	for j = 1,
s > k	forj > 1,
(31)
(32)
(33)
29
Under review as a conference paper at ICLR 2020
then we know that the s-th element of y is 0.
If s satisfies (33) and m - s ∈ Lj, then the s-th and (s + 1)-th elements of Au is
((ξ + β∕γ)% - ξ%+ι) and (-ξus + (ξ + β∕γ)%+ι) respectively where ξ = λn∕β. So by EqUa-
tion (32), we have
βUs + ξ(Us — Us+l) + 12β" us：：- 1) = 0.
β us+1 + ξ(Us+1 - us) + 120βλαUs+1+uus+1--)= 0.
Following from Lemma J.9, for 12?a < (2+2√2)β, We have us = us+1 = 0.
That is
1.	if m - s ∈ Lj and s satisfies (33), then us = 0.
2.	if m - s + 1 ∈ Lj and s - 1 satisfies (33), then us = 0.
For s which satisfies (33), if m - s 6∈ Lj and m - s + 1 6∈ Lj , then the s-th element of Au is
(β∕γ us ). Similarly, by EqUation (32), we have
-us +
γ
120λα u2(us — 1)
β ι + u2
0.
Following from Lemma J.8, for 12jɑ < (2+2√2)β, we have Us = 0.
Therefore, we can conclUde that
1.	if s - 1 satisfies (33), then us = 0.
2.	if s satisfies (33) and m - s + 1 6∈ Lj , then us = 0.
Moreover, we have that
1.	if k = 0 and j = 1, then m - 1, m - 2 6∈ Lj , so u2 = 0.
2.	if k =	0 and j > 1,	then for s = 1, we have m - s + 1 6∈	Lj , so u1 = 0.
3.	if k =	0, then for s	> 2, we have s - 1 > 1 satisfies (33),	so us = 0.
4.	if k >	0, then for s	> k + 1, we have s - 1 > k satisfies (33), so us = 0.
5.	if k >	0 and m - k	6∈ Lj, then for s = k + 1, we have m	- s + 1 6∈ Lj, so uk+1 = 0.
In short,
1.	if k = 0 and j > 1, then u ∈ G0 .
2.	if k = 0 and j = 1, then u ∈ G1 .
3.	if k > 1 and m - k 6∈ Lj , then u ∈ Gk .
4.	if k > 1 and m - k ∈ Lj, then u ∈ Gk+1.
□
Remark J.7. In order to make the operator proxfγ valid, γ need to satisfy
√3+1 β2	√2+1 β2
Y < ----------- < -----------.
90 λɑ 60	λα
So for any valid PIFO call, the condition about γ in Lemma J.6 must be satisfied.
30
Under review as a conference paper at ICLR 2020
Lemma J.8. Suppose that 0 < λ2 < (2 + 2√2)λι, then Z = 0 is the only real solution to the
equation
z2(z	1)
λιz + λ2	(z 2)=0.	(34)
1 + z2
Proof. Since 0 <λ < (2 + 2√2)λι, We have
λ22 - 4λ1(λ1 +λ2) <0,
and consequently, for any z, (λ1 + λ2)z2 - λ2z + λ1 > 0.
On the other hand, We can reWrite Equation (34) as
z((λι + λ2)z2 - λ2z + λι) = 0.
Clearly, z = 0 is the only real solution to Equation (34).
□
Lemma J.9. Suppose that 0<λ < (2 + 2√2)λι and λ3 > 0, then zι= z2 = 0 is the only real
solution to the equation
λ λ1z1 + λ3(ZI - z2) + λ2 z1(Z1- D = 0.
1 1	3 1	2	2 21(+z121)	(35)
[λ1Z2 + λ3(z2	Z1) + λ2z⅛⅞u = 0.
Proof. If Z1 = 0, then Z2 = 0. So let assume that Z1Z2 6= 0. ReWrite the first equation of Equation
(35) as
λι + λ3
λ3
+ λ2 Z1(z1 - 1)
λ3	1 + Z2
Z2
Zi
Note that
Thus, We have
Similarly, it also holds
1	- √2 ≤ Z(Z - 1)
-2-	≤ 1 + z2 .
λι + λ3	λ2 1 — √2 Z2
------ + -------≤ --
λ3	λ3	2	— Zi
λι + λ3	λ2 1 — √2	Zi
------ + -------≤ --
13	13	2	— Z2
By 0 <λ2 < (2 + 2√2)λι, We know that λι + i-2√入2 > 0. ThUs
λι + λ3 + λ2 1 — √2 > 1
Since Zi/Z2 > 1 and Z2/zi > 1 can not hold at the same time, so we get a contradiction. □
FolloWing from Lemma J.6, We knoW folloWing Lemma Which is similar to Lemma 2.9.
Lemma J.10. If M ≥ 1 satisfies minχ∈GM kVF (x)k2 ≥ 9ε and N = n(M + 1)/4, then we have
miNn EkVF (Xt)k2 ≥ ε
31
Under review as a conference paper at ICLR 2020
Theorem J.11. Set
α = min∕l, (√3+1)nσ
[,	30L	, 180
_ 3888nε2
λ = La3/2 ,
β = p3λn∕L,
∆L√α
m = --------ττ
40824nε2
Suppose that ε2 ≤ 83Lan .In order to find X ∈ Rm+1 such that E kVF (X)k2 < ε, PIFO algorithm
A needs at least Ω
queries to hF.
Proof. First, note that fi is (-l1, l2)-smooth, where
l1
45(√3 — 1)αλ _ 45(√3 — 1)L
l2
β2
(2n + 180α)λ
α≤
45(√3 — 1)L (√3+1)nσ
β2
3n
=-^~(2n + 180α) ≤ L.
3n
3n
30L
σ,
Thus each fi is (-σ, L)-smooth.
Next, observe that
F(x0)- F(x*) ≤
≤
1944nε2
λ(√α∕2 + 10αm)=----------
Lα
1944 A 38880 ʌ _ ʌ
40824 △+ 40824 △ =	.
38880nε2
+ -√0^τm
For M = m - 1, we know that
min kVF (x)k2
x∈GM
≥
a3/4X_ α3∕4λ _ ∕λLɑ3/4
4β = 4p3λn∕L = V 3n ɪ
With recalling Lemma J.10, in order to find X ∈ Rm+1 such that EkVF (X)k2 < ε, PIFO algorithm
A needs at least N queries to hF, where
N = n(M + 1)/4 = nm/ 4 = Ω
At last, we need to ensure that m ≥ 2. By ε2 ≤ d^Lfn, We have
∆L√α 、	∆Lα	、o
40824nε2 ≥ 40824nε2 ≥ ,
and consequently m ≥ 2.	口
32