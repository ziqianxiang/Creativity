Under review as a conference paper at ICLR 2020
Amata: An Annealing Mechanism for Adver-
sarial Training Acceleration
Anonymous authors
Paper under double-blind review
Ab stract
Despite of the empirical success in various domains, it has been revealed that deep
neural networks are vulnerable to maliciously perturbed input data that much de-
grade their performance. This is known as adversarial attacks. To counter ad-
versarial attacks, adversarial training formulated as a form of robust optimization
has been demonstrated to be effective. However, conducting adversarial training
brings much computational overhead compared with standard training. In order
to reduce the computational cost, we propose a simple yet effective modification
to the commonly used projected gradient descent (PGD) adversarial training by
increasing the number of adversarial training steps and decreasing the adversar-
ial training step size gradually as training proceeds. We analyze the optimality
of this annealing mechanism through the lens of optimal control theory, and we
also prove the convergence of our proposed algorithm. Numerical experiments
on standard datasets, such as MNIST and CIFAR10, show that our method can
achieve similar or even better robustness with around 1/3 to 1/2 computation time
compared with PGD.
1	Introduction
Recently, the revival of deep neural networks has led to breakthroughs in various fields, including
computer vision, natural language processing, game playing, etc. Despite these advancements, deep
neural networks were found to be vulnerable to malicious perturbations on the original input data.
While the perturbations remain almost imperceptible to humans, they can lead to wrong predictions
over the perturbed examples (Szegedy et al., 2013; Goodfellow et al., 2014; Akhtar & Mian, 2018).
These maliciously crafted examples are known as adversarial examples, which have caused serious
concerns over the reliability and security of deep learning systems, particularly when deployed in
the life-critical scenarios, such as autonomous driving systems and health/medical domains.
Several defense mechanisms have been proposed, such as input reconstruction (Meng & Chen, 2017;
Song et al., 2018), input encoding (Buckman et al., 2018), and adversarial training (Goodfellow
et al., 2014; Tramer et al., 2017; He et al., 2017; Madry et al., 2017). Among these methods,
adversarial training is the most effective defense method so far. Adversarial training can be posed
as a robust optimization problem (Ben-Tal & Nemirovski, 1998), where a min-max optimization
problem is solved (Madry et al., 2017; Kolter & Wong, 2017). For example, given a C-class dataset
S = {(xi0, yi)}in=1 with xi0 ∈ Rd as a normal or clean example in the d-dimensional input space and
yi ∈ RC as its associated one-hot label, the objective of adversarial training is to solve the following
min-max optimization problem:
1N
min N i=1 kzimXoχ∣≤e '(hθ (zi),yi)	⑴
where hθ : Rd → RC is the deep neural network (DNN) function, ` is the loss function and is the
maximum perturbation constraint. The inner maximization problem is to find an adversarial example
xi, within the -ball around a given normal example xi0 that maximizes the classification loss `. On
the other hand, the outer minimization problem is to find model parameters that minimizes the loss
` on the adversarial examples {xi}in=1 that are generated from the inner maximization.
The inner maximization problem is commonly solved by projected gradient descent (PGD). PGD
perturbs a normal example x0 by iteratively updating it in the steepest ascent direction for a total
1
Under review as a conference paper at ICLR 2020
of K times. Each ascent step is modulated by a small step size and a projection step back onto the
-ball of x0 to prevent the updated value fall outside the -ball of x0 (Madry et al., 2017):
Xk = Y (Xk-1 + α ∙ Sign(Vx'(hθ(XkT), y))	(2)
where α is the step size, Q(∙) is the orthogonal projection function onto {x0 : ∣∣x0 一 x0k ≤ e}, and
Xk is the adversarial example at k-th step.
However, a major problem prohibiting adversarial training to be practically applicable is the huge
computational burden associated with the inner maximization steps: we need to iteratively solve the
inner maximization problem to find good adversarial examples for DNN to be robust. Recently, to
accelerate adversarial training, a few methods have been proposed. For example, YOPO estimat-
ed the gradient on the input by only propagating the first layer (Zhang et al., 2019a), and parallel
adversarial training utilized multiple graphics computation units (GPUs) for acceleration (Bhat &
Tsipras, 2019; Shafahi et al., 2019). A common drawback of these methods are their implementa-
tion complexity or their need for multiple GPUs for acceleration. On the other hand, an empirical
observation made by (Wang et al., 2019), indicated that we might not need to find good solutions to
the inner maximization at the initial stages of adversarial training to achieve even better robustness.
In line of such observations, in this paper we propose a simple yet effective Annealing Mechanism
for Adversarial Training Acceleration, which we call Amata, that gradually controls the degree of
which the inner maximization is solved as the training proceeds. This annealing algorithm only takes
1/3 to 1/2 the time to achieve comparable or even better robustness with the addition of only two
lines of code. To motivate the present approach, we develop a general formulation of adversarial
training as an optimal control problem, from which an approximate optimality criterion can be
derived based on the Pontryagin’s maximum principle. This criterion is related to but different from
the empirical FOSC criterion (Wang et al., 2019) that improves the robustness of adversarial training
by determining the number of adversarial training steps according to the empirical criterion. We
will also demonstrate in our experiments that unlike FOSC, algorithms derived from our proposed
criterion leads to acceleration.
Our contributions are as follows:
1.	We propose an adversarial training algorithm, Amata, that gradually increases the number
of iterations and decreases the step size for the inner maximization. We show that compara-
ble or even better robustness can be achieved with much reduced computational overhead.
We also prove the convergence of our algorithm.
2.	To obtain more theoretical insight, we develop a general formulation of adversarial training
subject to hyper-parameters in the inner maximization loop as an optimal control problem.
This includes the PGD-based methods considered in this work, but potentially encompasses
a larger class of annealed or adaptive adversarial training algorithms.
3.	Using the Pontryagin’s maximum principle from optimal control theory, we provide a prin-
cipled criterion to measure the near-term optimality ofan annealing schedule for inner max-
imization, thereby guiding and validating our proposed algorithm in balancing the trade-off
between robustness and computational cost.
2	Accelerating Adversarial Training by Annealing Mechanism
In this section, we will first introduce the proposed adversarial training algorithm, which aims to
balance the computational cost and the accuracy of solving inner maximization. A proof of the
convergence of the algorithm can be found in the Appendix. We then develop a general optimal
control formulation of adversarial training and derive a criterion based on the Pontryagin’s maximum
principle (Boltyanskii et al., 1960). The criterion is shown to be effective in quantifying adversarial
training performance both on accuracy and computational efficiency, justifying the superiority of the
proposed algorithm.
2.1	Proposed annealing adversarial training algorithm
The proposed adversarial training algorithm Amata is found in Algorithm 1. Compared with PGD,
we only need to add two lines of code that are shown in blue. The intuition behind Amata is that, at
2
Under review as a conference paper at ICLR 2020
the initial stage, the neural network focus on learning features, which might not require very accurate
adversarial examples. Therefore, we only need coarse approximations of the inner maximization
problem solutions. With this consideration, we set a small number of update steps but with a large
step size for inner maximization, and then gradually increase K and decreases α to improve the
quality of inner maximization solutions. This adaptive annealing mechanism would largely reduce
the computational cost in the early iterations while still maintaining reasonable accuracy for the
entire optimization.
Algorithm 1 Amata: an annealing mechanism for adversarial training acceleration
Input: T:training epochs; Kmin: the minimum number of adversarial perturbations; Kmax: the
maximum number of adversarial perturbations; θ : parameter of neural network to be adversari-
ally trained; B:mini-batch; α: adversarial training time step; η : learning rate of neural network
parameters. τ : constant, maximum perturbation:.
Initialization θ = θ0
for t = 0 to T - 1 do
Compute the annealing number of adversarial perturbations:
Kt
Kmin + (Kmax - Kmin) ∙ T
Compute adversarial perturbation step size: αt = KKt
for each mini-batch x0B do
for k = 1 to Kt do
Compute adversarial perturbations:
XB = XBT + at ∙ Sign(Vx'(hθ(XB),y)
xkB = clip(xkB,x0B - , x0B + )
end for
θt+1 =θt- ηvθ '(hθt(XKt ),y)
end for
end for
Collect θT as the parameter of adversarially-trained neural network.
In the following section, we develop a general formulation of adversarial training based on optimal
control theory and derive a novel criterion to quantify the optimality of a training strategy, taking
into account the trade-off between accuracy and efficiency. Importantly, we show that our propos-
al, Amata, performs favorably under this criterion, thus providing theoretical justification for our
approach.
2.2	Optimal control formulation of adversarial training
In essence, the PGD-based adversarial training algorithm (Madry et al., 2017) represents a sequence
of relaxations of the original min-max problem (1). First, the outer minimization over θ is replaced
by gradient descent. Then, a full solution of the inner maximization is replaced by a number of PGD
steps in the steepest ascent direction. Consequently, a natural question is how to choose the number
of steps, the step size or any other hyper-parameters associated with this relaxed version of the inner
maximization, and how their choices affect the performance of the overall algorithm. This is the
central point of analysis in this paper and the basis of the algorithm proposed. It turns out that this
question can be systematically formulated in the framework of optimal control theory (Bertsekas,
1995). We now introduce the general setup of our problem and establish connections with optimal
control, and in particular the classical maximum principle in the calculus of variations.
For simplicity of presentation, let us consider just one fixed input-label pair (X0, y), since the n-
sample case is similar. The original min-max adversarial training problem is
min max	`(hθ (z), y).	(3)
θ {z：kz-x0kSe}
3
Under review as a conference paper at ICLR 2020
The first relaxation is to replace the outer minimization with gradient descent so that we obtain the
iteration
θt+ι = θt - ηVθ	ιιmaχ, ι'(hθt (z),y).	⑷
{z：kz-x0kSe}
Then, the remaining maximization in each outer iteration step is replaced by an abstract algorithm
Au,θ : Rd → Rd	(5)
which solves the inner maximization approximately. Here, we assume that the algorithm depends on
the parameters of our neural network, as well as hyper-parameters u which takes values in a closed
subset G of a Euclidean space. No further assumptions is placed on G, which may be a continuum,
a discrete set, or even a finite set.
This relaxation leads to the following general iterations1
θt+ι = θt - Ne'(hθt (Aθt,ut),y).	⑹
Observe that Algorithm 1 is a particular realization of (6) where Aθt ,ut represents the inner PGD
loop and ut = {αt, Kt } are the hyper-parameters we pick at each t. Nevertheless, Equation (6)
represents a general formulation of an inner-outer loop adversarial training algorithm. For small η,
we can replace (6) by an ordinary differential equation with the identification s ≈ tη:
θs = -vθ'(hθs (Aθs,Us ), y).	⑺
Formulated this way, it is immediately clear that we can approach this problem from a control
perspective: the dynamics of the trainable parameters are given by (7) and the hyper-parameter
choices at each inner algorithm us are the controls.
Our goal is two-fold: on a training interval [T1, T2] in the outer loop, we want to minimize some
loss under adversarial training measured by a real-valued function Φ(θ) while minimizing training
cost associated with each inner algorithm loop under the hyper-parameter u, which is measured by
another real-valued function R(u). We thus arrive at the general optimal control formulation of our
problem:
T2
min	Φ(θT) +	R(us)ds	subject to:
uT1 :T2 ∈LT∞1 :T2	T1	(8)
θt = F(θs, Us)	where	F(θs, Us) ：= -Ve'(he, (Aθs,us ),y),
where We have defined the shorthand ut^t2 = {us : S ∈ [Ti, T2]} and L∞t := L∞([Tι,T2], G).
In this paper, we take Φ to be the DNN’s prediction loss given the adversarial example (adversarial
robustness), and R is set as γKt where γ is the coefficient for adversarial robustness and training
time trade-off. This is to account for the fact that when Kt increases, the cost of the inner loop
training increases accordingly. The integral over t of R is taken so as to account for the total com-
putational cost corresponding to a choice of hyper-parameters {Us }. The objective function taken
as a sum serves to balance the adversarial robustness and computational cost, with γ a balancing
coefficient.
2.3	Pontryagin’ s Maximum Principle
In the last section, the problem of choosing hyper-parameters in the inner loops of adversarial train-
ing has been formulated as an optimal control problem in (8). Now, we show how this connection
can help us design and validate algorithms.
A classical result in calculus of variations gives the following necessary conditions for optimality.
For more background on the theory of calculus of variations and optimal control, we refer the reader
to Boltyanskii et al. (1960); Bertsekas et al. (1995).
1Here we assume that the gradient with respect to θ is the partial derivative with respect to the parameters
of the network hθ and θt in Aθt ,ut is held constant. This is the case for the PGD algorithm. Alternatively, we
can also take the total derivative, but this leads to different algorithms.
4
Under review as a conference paper at ICLR 2020
Theorem 1 (Pontryagin's Maximum Principle (PMP) (Boltyanskii et al., 1960)). Let UTl ：t2 ∈
LT∞ ,T be a solution to (8). Suppose F (θ, u) is Lipschitz in θ and measurable in u. Define the
Hamiltonian function
H(θ, p, u) = pTF (θ, u) - R(u)	(9)
Then, there exists an absolutely continuous Co-State process PTl ：T such that
∙,,,,.
θS = F(ΘS, Us)	θTι= θτι	(10)
Ps = -VθH(θs, P：, u：)	PT2 = -VθΦ(θT2)	(11)
H(θs, P：, Us) ≥ H(θs, Ps, V)	forall V ∈ G,s ∈ [T1,T2]	(12)
In short, the maximum principle says that a set of optimal parameter choices {Us：} (in our specific
application, these are the optimal choices of inner-loop hyper-parameters as the training proceeds)
must globally maximize the Hamiltonian defined above for each outer iteration. This statement
is especially appealing for our application because unlike first-order gradient conditions, the PMP
holds even when our hyper-parameters can only take a discrete set of values, or when there are non-
trivial constraints amongst them. The maximization criterion holds generally under these conditions.
Moreover, we now show that it gives us a quantitative measure of deviation from optimality, from
which we can analyze and design algorithms.
Quantitative Measure of Sub-optimality. Given any hyper-parameter choice UT1:T2 over the
training interval, let us define its “distance” from optimality as
1	T2
C(UT1：T2 ) := -ma I	max H(θu, Pu, V)-H(θu, Pu, us )ds	(13)
T2 - T1 T1 v∈G
where {θsu, Psu : s ∈ [T1, T2]} represents the solution of the equations (10) and (11) with Us in place
of Us：. Observe that C(UT1:T2 ) ≥ 0 for any UT1:T2 with equality if and only if UT1:T2 satisfies the
PMP for almost every s ∈ [T1, T2]. Hence, C can be used as a measure of sub-optimality. When C
is small, our annealing strategy {Us} is close to at least an locally optimal strategy, where as when
it is large, our annealing strategy is far from an optimal one.
One-step Approximation and Adversarial Training Criterion. Equation (13) requires informa-
tion on the entire training interval and may be expensive to compute. In this paper, we use a one-step
approximation where we take T1 = t (current iteration) and T2 = t + η with η 1. In this small
interval, we can also take Us to be constant and thus equal to some U. This is in some sense a
greedy approximation, where we evaluate in the immediate short term the optimality of a piece-wise
constant choice of hyper-parameters. From (13) we then obtain via a Taylor expansion and our
particular choices of Φ and R (See Appendix D)
C(Ut：t+n) =C(ut,t) + o(1) With
C(ut,t) ≡ C(αt,K t,t) ≈ max {∣∣Vθ '(hθt [Aθt,α,κ (x)],y)k2 - YK }	(14)
α,K
-(∣∣Vθ'(hθt[Aθt,αt,κt(x)],y)k2 - γKt),
With Aθt ,α,K denoting the inner PGD loop starting form x With K steps and step size α. Criteri-
on (14) is a greedy version of the general criterion derived from the maximum principle. It can be
used to either evaluate the near-term sub-optimality of some choice of hyper-parameters U, or to find
an approximately optimal hyperparameter greedily by solving C(U, t) = 0 for U, Which amounts to
maximizing the first term. In this paper, We use Bayesian optimization2 to perform the maximization
in (14) to evaluate and select strategies from the controllable space G.
Remark. By applying the method of successive approximations (MSA) (Chernousko &
Lyubushin, 1982; Li et al., 2017; Li & Hao, 2018; E et al., 2019), We can also obtain a solution
of the PMP by iterating
Usk+1 =argmaxv∈GH(θsuk,psuk,V),	s ∈ [T1,T2].	(15)
2Implementations can be found in https://github.com/hyperopt/hyperopt
5
Under review as a conference paper at ICLR 2020
Subsequent work will explore adaptive adversarial training acceleration based on this approach. We
remark that although we appeal to control theory tools here, the application scenario is very different
from prior work in Li et al. (2017); Li & Hao (2018); E et al. (2019). For the latter, the controlled
dynamics are idealized forward propagation through a deep neural network and the controls are the
trainable weights of the network. In contrast, in the current setting the controlled dynamics are the
evolution of weights under gradient descent in the outer loop and controls are the hyper-parameters
for the adversarial perturbation inner loop.
Comparison with FOSC criterion: Wang et al. (2019) proposed an empirical criterion to measure
the convergence of inner maximization:
FOSC(X) = E ∣∣Vχ'(hθ(x),y)k-hx - x0, Vχ'(hθ(x),y)i	(16)
There are some similarities between our criterion and FOSC when we do not consider the compu-
tational cost term R. For example, when the stationary saddle point is achieved, both our criterion
and FOSC reach the minimum. However, our proposed criterion is quite different from FOSC in the
following aspects:
1.	Our criterion is derived from the optimal control theory, whereas FOSC is concluded from
empirical observations.
2.	Our criterion takes computation costs into consideration, whereas FOSC only considers the
convergence of adversarial training.
3.	Our criterion is based on the gradient of DNN parameters, whereas FOSC is based on the
gradient of the input. Measuring the gradient of DNN parameters is arguably more suitable
for considering robustness-training time trade-off as the variance of the DNN parameters is
much larger than the input during training.
2.4 Numerical justification of Amata
For simple linear-quadratic control problems, we can typically derive some analytical representa-
tions of an optimal control. However, for highly-nonconvex DNNs, we can only use a numerical
form of the optimal control criterion (Equation 14) to analyze Amata for robustness and compu-
tational efficiency trade-off. We use the LeNet neural network 3 for MNIST classification as an
example.
As a sanity check, we first consider only adversarial robustness and neglect computational cost
by setting γ = 0 and use the criterion to evaluate different adversarial training settings for PGD.
Recalling Algorithm 1, we will vary the other hyper-parameters in the inner loop: α (step-size of
PGD) and K (number of PGD steps). In each case, we compute the optimal control criterion values
on the training dataset and evaluate the adversarially trained networks’ performance against PGD-40
attack on the test dataset.
Sanity check 1 In this case, we fix K = 5 and set α in [0.01, 0.02, 0.03, 0.04, 0.05].
Sanity check 2 In this case, we fix α = 0.01 and set K in [5, 10, 20, 30, 40, 50].
The results are shown in Figure 1 (a)(b), where robustness is the prediction accuracy of the trained
networks under PGD-40 attack on the test dataset.
From these results, we can observe that the robustness and the criterion value C are negatively
correlated under varying α and K , as expected.
Now, we will use the same setting as sanity checks but with different values of γ to numerically
analyze the trade-off between robustness and training time for Amata and PGD.
Setting 1: fixed α, varying K and γ: in this case, we set α as 0.01, K in [5, 20, 40] and γ in
[0.02, 0.04, 0.06, 0.08]. The result for Setting 1 is shown in Figure 1 (c). From this result, we can
observe that with the increase of γ, reducing the computation time becomes more important. For
example, when γ is 0.02, using PGD-40 is the best strategy (the lowest C value). When γ increases
to 0.06 or 0.08, using PGD-20 achieves the best trade-off under the criterion.
3Implementations can be found in https://github.com/pytorch/examples/blob/master/mnist/main.py
6
Under review as a conference paper at ICLR 2020
(c) Setting 1
(a) Sanity check 1
(b) Sanity check 2
Figure 1: Sanity checks of the optimal control criterion (γ = 0).
Table 1: Comparison of adversarial training strategies.
Strategy		C(γ = 0.04)	C(γ = 0.08)	Robustness	Training time (seconds)
Amata(Kmin = 5, Kmax =	40)	0.54	138	91.47%	697.73
Amata(Kmin = 10, Kmax	= 40)	0.68	1.53	91.46%	760.16
PGD-10		7.82	770	68.07%	307.57
PGD-20		1.52	2.09	85.23%	567.11
PGD-40		1.20	2.19	90.56%	1086.31
Next, we setγ as 0.04 and 0.08 and use the criterion to evaluate the Amata and compare it with other
settings of PGD. It can be seen that Amata is better than PGD with fixed numbers of perturbation
steps. Note that Amata also achieves better robustness than the highest robustness of PGD settings
(PGD-40) with 91.47% adversarial accuracy compared to 90.56% adversarial accuracy.
Remark: Although computing the exact optimal control strategy for DNN adversarial training is
inapplicable for real-time applications, with the criterion derived from the PMP, we are able to nu-
merically compare the optimality of different adversarial training strategies. From this numerical
evaluation, we have demonstrated that the proposed Amata algorithm is close to an optimal ad-
versarial training strategy, or at least one that satisfies the maximum principle. We will show that
our algorithm can achieve similar or even better adversarial accuracy much faster with empirical
experiments on popular DNN models later in Section 3.
3	Experiments
To demonstrate the effectiveness of Amata, we conduct experiments on MNIST and CIFAR10. We
find that the models trained with Amata have comparable or even better performance with that of
the PGD adversarial training, but with much less computational cost. For our experiment, we use
PyTorch 1.0.0 and a GTX1080 Ti GPU. We evaluate the adversarial trained networks against PGD
and Carlini-Wagner (CW) attack (Carlini & Wagner, 2017).
3.1	MNIST classification
We consider the standard MNIST classification using the “smallCNN” architecture that was used in
(Zhang et al., 2019b). We set τ to be 0.4 for Amata. In the experiment, We achieve 96% adversarial
accuracy within 275 seconds while it takes PGD-40 1019 seconds to reach the same level. We
also experimented with the FOSC adversarial training method in (Wang et al., 2019) and found that
it took FOSC 3663 seconds to achieve the same level, which confirms our previous analysis that
our optimal control criterion is more suitable for accelerating adversarial training. The error-time
curve is shown in Figure 2 (a). Then, we run all methods for 57 epochs for full convergence. The
robustness of convergence results are shown in Table 2. From Table 2, we can see that naively
reducing the number of adversarial perturbations will hurt the robustness of DNNs.
7
Under review as a conference paper at ICLR 2020
Figure 2: Left: MNIST result. Training time against PGD-40 attack. We use Amata with the setting Kmin =
10 and Kmax = 40. Right: CIFAR10 result. Training time against PGD-20 attack. We use Amata with the
setting Kmin = 2 and Kmax = 10.
Table 2: MNIST adversarial training results.
Training methods	Clean accuracy	PGD-40 Attack	CW Attack	Time (Seconds)
ERM	99.52%	5.58%	0.10%	268710
PGD-5	99.58%	80.99%	0.31%	709721
PGD-10	99.57%	93.8%	1.06%	1234.30
PGD-40	99.51%	97.02%	93.49%	4331.81
FOSC(Wang et al., 2019)	99.51%	97.02%	81.79%	14928.53
Amata(Kmin = 5, Kmax = 40)	99.53%	97.11%	86.92%	2533.28
Amata(Kmin = 10, Kmax = 40)	99.45%	96.97%	94.37%	2675.64
3.2	CIFAR 1 0 classification
For the more complex CIFAR10 classification task, we use the PreAct-Res-18 network (Madry et al.,
2017). We set τ to be 20/255 for Amata. The proposed Amata method took 3045 seconds to achieve
less than 55% robust error while for PGD-10, it took 6944 seconds. We also tested FOSC adversarial
training and found that it took 8385 seconds to reach the same level, which is a bit longer than PGD-
10. The error-time curve is shown in Figure 2 (b). We further run all methods for 100 epochs for
convergence. The results are shown in Table 3.
Table 3: CIFAR10 PreAct-Res-18 adversarial training results.
Training methods	Clean accuracy	PGD-20 Attack	CW Attack	Time (Seconds)
ERM	94.75%	0.0%	-0.23%-	2099.58
PGD-2	90.16%	31.70%	-13.36%-	6913.36
PGD-10	85.27%	47.31%	51.73%	23108.10
FOSC(Wang et al., 2019)	85.47%	48.04%	53.65%-	26126.98
Amata(Kmin = 2, Kmax = 10)	85.52%	47.62%	52.94% -	14308.96
4	Conclusion
We have proposed a modification amounting to two lines of code to PGD adversarial training that
achieves comparable or even better robustness with only 1/3 to 1/2 the computational cost over
a variety of benchmarks. Moreover, a general optimal control formulation of adversarial training
with hyper-parameters is developed to analyze this procedure and justify its superior performance
through a numerical criterion based on the Pontryagin’s maximum principle. A proof of convergence
of our algorithm is also provided in Appendix C. As a point of future work, we will explore adap-
tive methods for adversarial training based on the optimal control formulation we introduced here.
This approach can also lead to algorithms that can be combined with YOPO or parallel adversarial
training methods for maximal efficiency. An example is shown in Appendix B.3.
8
Under review as a conference paper at ICLR 2020
References
Naveed Akhtar and Ajmal Mian. Threat of Adversarial Attacks on Deep Learning in Computer
Vision: A Survey. arXiv preprint arXiv:1801.00553, 2018.
Aharon Ben-Tal and Arkadi Nemirovski. Robust convex optimization. Mathematics of operations
research, 23(4):769-805,1998.
Dimitri P Bertsekas. Dynamic programming and optimal control, volume 1. Athena scientific
Belmont, MA, 1995.
Dimitri P Bertsekas, Dimitri P Bertsekas, Dimitri P Bertsekas, and Dimitri P Bertsekas. Dynamic
programming and optimal control, volume 1. Athena scientific Belmont, MA, 1995.
Sanjit Bhat and Dimitris Tsipras. Towards Efficient Methods for Training Robust Deep Neural
Networks, 2019.
Vladimir G Boltyanskii, Revaz V Gamkrelidze, and Lev S Pontryagin. The theory of optimal pro-
cesses. The maximum principle. Technical report, TRW Space Tochnology Labs, Los Angeles,
California, 1960.
Jacob Buckman, Aurko Roy, Colin Raffel, and Ian Goodfellow. Thermometer Encoding: One Hot
Way To Resist Adversarial Examples. In International Conference on Learning Representations,
2018. URL https://openreview.net/forum?id=S18Su--CW.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In
Security and Privacy (SP), 2017 IEEE Symposium on, pp. 39-57. IEEE, 2017.
F L Chernousko and A A Lyubushin. Method of successive approximations for solution of optimal
control problems. Optimal Control Applications and Methods, 3(2):101-114, 1982. ISSN 0143-
2087.
Weinan E, Jiequn Han, and Qianxiao Li. A mean-field optimal control formulation of deep learning.
Research in the Mathematical Sciences, 6(1):10, 2019. ISSN 2522-0144.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014.
Warren He, James Wei, Xinyun Chen, Nicholas Carlini, and Dawn Song. Adversarial example
defenses: Ensembles of weak defenses are not strong. arXiv preprint arXiv:1706.04701, 2017.
J Zico Kolter and Eric Wong. Provable defenses against adversarial examples via the convex outer
adversarial polytope. arXiv preprint arXiv:1711.00851, 2017.
Qianxiao Li and Shuji Hao. An Optimal Control Approach to Deep Learning and Applications to
Discrete-Weight Neural Networks. In Jennifer Dy and Andreas Krause (eds.), Proceedings of
the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine
Learning Research,pp. 2985-2994, Stockholmsmassan, Stockholm Sweden, 2018. PMLR. URL
http://proceedings.mlr.press/v80/li18b.html.
Qianxiao Li, Long Chen, Cheng Tai, and Weinan E. Maximum principle based algorithms for deep
learning. The Journal of Machine Learning Research, 18(1):5998-6026, 2017. ISSN 1532-4435.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017.
Dongyu Meng and Hao Chen. MagNet: A Two-Pronged Defense against Adversarial Examples. In
ACM Conference on Computer and Communications Security, 2017.
Ali Shafahi, Mahyar Najibi, Amin Ghiasi, Zheng Xu, John P Dickerson, Christoph Studer, Larry S
Davis, Gavin Taylor, and Tom Goldstein. Adversarial Training for Free! CoRR, abs/1904.12843,
2019. URL http://arxiv.org/abs/1904.12843.
9
Under review as a conference paper at ICLR 2020
Aman Sinha, Hongseok Namkoong, and John Duchi. Certifiable Distributional Robustness with
Principled Adversarial Training. In International Conference on Learning Representations, 2018.
URL https://openreview.net/forum?id=Hk6kPgZA-.
Yang Song, Taesup Kim, Sebastian Nowozin, Stefano Ermon, and Nate Kushman. PixelDefend:
Leveraging Generative Models to Understand and Defend against Adversarial Examples. In In-
ternational Conference on Learning Representations, 2018. URL https://openreview.net/forum?
id=rJUYGxbCW.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Florian Tramer, Alexey Kurakin, Nicolas Papernot, Dan Boneh, and Patrick McDaniel. Ensemble
adversarial training: Attacks and defenses. arXiv preprint arXiv:1705.07204, 2017.
Yisen Wang, Xingjun Ma, James Bailey, Jinfeng Yi, Bowen Zhou, and Quanquan Gu. On the Con-
vergence and Robustness of Adversarial Training. In Kamalika Chaudhuri and Ruslan Salakhut-
dinov (eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97
of Proceedings of Machine Learning Research, pp. 6586-6595, Long Beach, California, USA,
2019. PMLR.
Dinghuai Zhang, Tianyuan Zhang, Yiping Lu, Zhanxing Zhu, and Bin Dong. You Only Prop-
agate Once: Accelerating Adversarial Training via Maximal Principle. arXiv preprint arX-
iv:1905.00877, 2019a.
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P Xing, Laurent El Ghaoui, and Michael I Jordan.
Theoretically Principled Trade-off between Robustness and Accuracy. CoRR, abs/1901.0, 2019b.
URL http://arxiv.org/abs/1901.08573.
A	Appendix: Additional experiment details
We use similar experiment settings as in (Zhang et al., 2019a).
A. 1 MNIST classification
In this experiment, for PGD adversarial training, we set the adversarial constraint as 0.3, the step
size as 0.01. For CW attack results in this experiment, we set the parameter eps to be 100 and run
for 100 iterations in this reference implementation https://github.com/xuanqing94/BayesianDefense/
blob/master/attacker/cw.py. For display the error-time curve, we use the smoothing function used in
Tensorboard for all methods. The smoothing parameter is set as 0.09. For outer minimization, we
use the stochastic gradient descent method and set the learning rate as 0.1, the momentum as 0.9,
and the weight decay as 5e-4.
A.2 Cifar 1 0 classification
In this experiment, for PGD adversarial training, we set the adversarial constraint as 8/255, the step
size as 2/255. For CW attack results in this experiment, we set the parameter eps to be 0.5 and run
for 100 iterations in this reference implementation https://github.com/xuanqing94/BayesianDefense/
blob/master/attacker/cw.py. For display the error-time curve, we use the smoothing function used
in Tensorboard for all methods. The smoothing parameter is set as 0.6. For outer minimization, we
use the stochastic gradient descent method and set the learning rate as 5e-2, the momentum as 0.9,
and the weight decay as 5e-4. We also use a piece-wise constant learning rate scheduler in PyTorch
by setting the milestones at the 75-th and 90-th epoch with a factor of 0.1.
10
Under review as a conference paper at ICLR 2020
B Appendix: Additional experiment results
B.1	WideResNet-34 results
We experiments using WideResNet-34 networks for CIFAR10 classification. We use the same ex-
periment setting as the CIFAR10 classification task in the paper. We run all methods for 100 epochs
for full convergence. The result is shown in Table 4.
Table 4: CIFAR10 WideResNet-34 adversarial training results.
Training methods	Clean accuracy	PGD-20 Attack	CW Attack	Time (Seconds)
PGD-10	85.80%	45.38%	30.82%	-150653.40-
Amata(Kmin = 2, Kmax = 10)	85.77% —	30.42% —	52.94%	90945.15
B.2	Additional MNIST results
We further plot the MNIST results in Figure 2 but with the x-axis denoting the number of training
epochs. The results are shown in Figure 3. From this figure, we can see that the time-saving of our
method is mainly due to the savings in the adversarial examples generation step. This is expected
as if we achieve faster convergence rates with regard to the number of epochs, it may indicate that
the adversarial examples generated by the proposed mechanism is significantly weaker and the outer
minimization step converges faster.
Figure 3: MNIST result. Training epoch against PGD-40 attack. We use Amata with the setting Kmin = 10
and Kmax = 40.
B.3	AMATA+YOPO
To demonstrate that our method can be incorporated in other acceleration method to further improve
the performance, we implement the Amata in YOPO MNIST classification experiment4. For Amata
incorporation, we gradually increase the K and decrease σ in the codes that is similar to the case of
modifying the PGD algorithm. We run Amata+YOPO and YOPO for 40 epochs as in the original
setting. The results are shown in Figure 4. From Figure 4, we can see that Amata+YOPO takes
294 seconds to reach the adversarial accuracy of 94%, which is only around half of the time for
YOPO to achieve the same adversarial accuracy. It is also worth noting that Amata+YOPO achieves
even better adversarial accuracy when converged. From this example, we can see that Amata can
be seamlessly incorporated in other adversarial training acceleration algorithm to provide further
acceleration.
4https://github.com/a1600012888/YOPO-You-Only-Propagate-Once/tree/82c5b902508224c642c8d0173e61435795c0ac42/experiments/
5-10
11
Under review as a conference paper at ICLR 2020
4 2 0 8 6 4
(％) əas .JO山
2-
0-
0	200	400	600	800
Training time (seconds)
1000	1200
Figure 4: MNIST result. Training epoch against PGD-40 attack. We use Amata with the setting Kmin = 2
and Kmax = 5.
B.4	Evaluation against different settings of PGD
The reason we choose PGD-40 for evaluation is that is a strong adversarial attack that is also used
in other papers (Zhang et al., 2019a). We also try other settings, such as PGD-20 and PGD-60.
Other settings are the same as our original settings in MNIST classification experiment. The results
are shown in Figure 5. We can see that compared with the case of PGD-40 for evaluation, both
Amata and PGD-40 are able to achieve above the 96% adversarial accuracy but at different speeds.
In the case of PGD-20, Amata takes around 28% training time of the PGD-40 to achieve the 96%
adversarial accuracy. In the case of PGD-60, Amata takes around 60% training time of the PGD-40
to achieve the 96% adversarial accuracy.
1
4 3 2
(％)se-J ,J0t山
200	400	600	800	1000
Training time (seconds)
1
4 3 2
(％)se-J ,J0t山
0-
0	500	1000 1500 2000 2500 3000 3500 4000
Training time (seconds)
Figure 5: Left: MNIST result. Training epoch against PGD-20 attack.Right: MNIST result. Training epoch
against PGD-60 attack.
B.5	Other decay scheme for controlling adversarial training
Finding better decay scheme is an interesting future direction. However, using more complex
schemes may introduce extra parameters, while one of the major challenges for machine learn-
ing methods to be practically useful for industry practitioners is the complexity of the method. We
tried another exponential scheme:
Kt = Kmin + (Kmax - Kmin) ∙ (I - eχp-η""(I - eχp-η∙T)	(17)
where Kt is the number of PGD adversarial training steps at t-th epoch, Kmin is the minimum
number of PGD steps in the beginning, Kmax is the maximum number of PGD steps in the last
epoch, η is the hyper-parameter controlling the shape of the scheme. This design ensures that when
12
Under review as a conference paper at ICLR 2020
K0 = Kmin and KT = Kmax to make the exponential decay scheme comparable with the linear
scheme used in our paper. However, we tried different parameters of eta ranging from 0.1 to 5, but
did not performance improvements. The results are shown in Figure 6. The ”Amata (Exp eta)” in
the legend denotes the exponential decay scheme and the ”Amata” in the legend denotes the original
linear decay scheme. From Figure 6, we can see that the the new exponential decay scheme cannot
outperform the linear decay scheme.
Figure 6: MNIST result. Training epoch against PGD-40 attack. We use Amata with the setting Kmin = 2
and Kmax = 5.
C Appendix: Proof of convergence
We provide a convergence analysis of our proposed Amata algorithm for solving the min-max prob-
lem. The proof of convergence in this paper largely follows (Wang et al., 2019; Sinha et al., 2018).
First, We will introduce some notations first for clarity. We denote Xr (θ) = arg maXχ.∈χif (θ, Xi)
where f(θ, xi) is a short hand notation for the classification loss function, Xi = {x| x - xi0 ≤ },
and fi(θ) = maxχi∈χi f (θ, Xi), then Xi(θ) is a δ-approximate solution by our algorithm to x*(θ),
if it satisfies that:
kXi(θ)- x*(θ)k≤ δ	(18)
In addition, denote the objective function inEquation 1 by LS(θ) , and its gradient by RLS(θ)=
; Pn=I Rfi(θ). Let g(θ) = B Pi∈B Vθf(θ) be the stochastic gradient of LS(θ), where B is the
mini-batch. Then, we have E[g(θ)] = VLS(θ). Let Vθ f(θ, X(θ)) be the gradient of f(θ,X(θ))
with respect to θ, and g(θ)= 由 Pii∈B Vθf (θ, Xi(θ) be the approximate stochastic gradient of
LS(θ). Before we prove the convergence of the algorithm, we have following assumptions.
Assumption 1. The function f(θ, X) satisfies the gradient Lipschitz conditions:
supx kVθf(θ, X) - Vθf(θ*, X)k2 ≤Lθθkθ-θ*k2
supθ kVθf(θ,X) - Vθf (θ, X*)k2 ≤ Lθx kX -X*k2
supxkVxf(θ,X) -Vxf(θ*,X*)k2 ≤Lxθkθ-θ*k2
where Lθθ, Lθx, and Lxθ are positive constants. Assumption 1 was used in (Sinha et al., 2018;
Wang et al., 2019).
Assumption 2. Thefunction f (θ, x) is locally μ-StrOngly concave in X = {x : ∣∣x 一 x0∣∣∞ ≤ e}
for all i ∈ [n], i.e., for any X1, X2 ∈ Xi:
f(θ, X1) ≤ f(θ, X2)+ hVχf(θ, X2), X1 - X2i — 2 ∣∣X1 - X2k2
where μ is a positive constant which measures the curvature of the loss function. This assumption
was used for analyzing distributional robust optimization problems (Sinha et al., 2018).
13
Under review as a conference paper at ICLR 2020
Assumption 3. The variance of the stochastic gradient g(θ) is bounded by a constant σ2 > 0:
E[kg(θ) -VLs(θ)k2] ≤ σ2
where VLS (θ) is the full gradient.
The Assumption 3 is commonly used for analyzing stochastic gradient optimization algorithms.
Theorem 2. Suppose Assumptions 1,2, and 3 holds. Denote ∆ = LS(θ0) - minθ LS (θ). If the
step size of outer minimization is η = min(1∕L,小7%). Then, we have:
T — 1
T X E[∣∣ VLs(θt)俏]≤ 4σ√L∆ + 5Lθχδ2
T t=0	T
where L = LθχLxθ/μ + Lθθ.
The proof of this theorem can be found in the Appendix. Theorem 2 indicates that if the Amata finds
solutions of the inner maximization problem closer enough to the maxima, Amata can converge at a
sublinear rate.
Lemma 1. Under Assumption 1 and Assumption 2, we have Ls (θ) is L-smooth where L =
LθχLχθ/μ + Lθθ, i.e.,forany θι and θ2 it holds:
L2
Ls(θι) ≤ Ls(θ2) + BLS(θ2), θι - θ2i + 2 kθι - θ2∣∣2
The proof of Lemma 1 can be found in Wang et al. (2019).
Lemma 2. Under Assumption 1 and Assumption 2, the norm of difference between the approximate
stochastic gradient g(θ) and the stochastic gradient g(θ) is bounded, i.e., it holds that:
kg(θ) - g(θ)k2 ≤ Lθχδ
Proof. We have
而(θ)- g(θ)k2
焉 X(Vθf(θ, Xi(θ)) -Vfi(θ))
|B| i∈B	2
1
≤ 国 i∈B
kVθ f (θ, Xi(θ))-Vθ f(θ, X*(θ))k2
Lθχ kXi(θ)- x∙T(θ)k2
(19)
where the first inequality is from the triangle inequality, and the second inequality is from Assump-
tion 1. Next, we insert Equation 18 into the above inequality then:
kg(θ) - g(θ)k2 ≤ Lθχδ	(20)
which completes the proof.	口
Now we can prove the Theorem 2:
14
Under review as a conference paper at ICLR 2020
Proof. From Lemma 1, we have:
LS(θt+1) ≤ LS(θt) + GLS(θt, θt+1 - θti + 2 ∣∣θt+1 — 叫2
=LS(θt) + ηthVLs(θt), VLS(θt - g(θt)i - ηt ∣∣VLs(θt)∣∣2
+ 乎 ∣∣g(θ2)∣∣2
=LS (Ot)- ηt(1—~ηt)-)11VLs (θt)∣∣2+ηt(1—^nt )•
NLS(θt), VLs(θt) — g(θt)i + Lnt2 ∣∣0(θt) — VLs(θt) ∣∣2
=LS(Bt)- ηt(1 — 学)11VLs(θt)∣∣2 + nt(I- 与)•
NLS(θt), VLs(θt) — g(θt)i + ηt(1 — Lηt)(VLs(θt),
VLs(θt) — g(θt)i + Lηt2 ∣∣g(θt) — g(θt) + g(θt) — VLs(θt)∣∣2
≤ LS(θt) — ηt(1 — 学)11VLs(θt)∣∣2+ ηt(I — 与)•
∣∣g(θ)—g(θt)∣∣2+Lη2(∣∣g(θt)-g(θt)∣∣2+∣∣g(θt)
— VLs(θt)∣∣2) + ηt(1 + Lηt)hVLs(θt), VLS(θt) — g(θt)i
Note that E[g(θt)] = VLS (θ), taking expectation on both sides of the inequality conditioned on θt.
Then we use Assumption 3 and Lemma 2 and simplify the above inequality:
E[Ls(θt+1) - Ls(θt)∣θt] ≤ — £(1 - Lηt) ∣∣VLs(θt)∣∣2
+ ηt (1 + 3Lnt )Lθχδ2 + Lη2σ2
Taking the telescope sum of the above equation from t = 0 to t = T — 1, we have
T-1
X £(1 - Lη2t)E[∣∣VLs(θt)∣∣2] ≤ E[Ls(θ0 - θτ)]
t=0 2	2
+ X1 £ (1+ 竽)Lθχδ2 +Lη2σ2
t=0 2	2
We set ηt = min(1/L,
we have
T X E[∣∣VLs(θt升2] ≤ 40好+5LI)χδ2
T t=0	T
Thus, we complete the proof.
□
D Appendix: Approximation of the optimal control criterion
In this section we show how to derive the approximation (14) from (13). The primary assumption we
make is that T2 — T1 = η 1, which allows one to use a local approximation for various functions
to derive a simple-to-compute criterion. For this reason, the derivation here will be largely heuristic.
First, we assume that we consider a constant control us ≡ u on the interval s ∈ [T1, T2] ≡ [t, t+η].
Next, recall that the adversarial robustness which serves as our terminal loss function for the control
problem is Φ(θ) = maxz `(hθ [Aθ,z (x)], y). In practice, any control that sufficiently conducts the
15
Under review as a conference paper at ICLR 2020
adversarial perturbation serves as a good approximation. This is because the DNNs are very prone to
adversarial attacks before the convergence of adversarial training and the real-time adversarial loss
is close to the worst-case adversarial loss. Thus, we can approximate Φ(θ) by `(hθ [Aθ,z (x)], y) by
some chosen z that makes practical computations easy.
Now, Assuming regularity we can expand the state equation (10) to get to leading order as η → 0
θsu = θt + o(1),	s ∈ [t, t + η].
(21)
On the other hand, the co-state equation (11) gives
Pu = -VθΦ(θτ2) + o(i) = -Vθ'(hθt[Aθt,z(χ)],y) + o(i),	S ∈ [t,t + η].	(22)
But, by definition F(θ, U) = -Vθ'(hθ [Aθ,u(χ)],y). Then, for any S ∈ [t,t+η], we have from (21)
and (22) that
H(θsu, Psu, v) = hPsu, F (θsu, v)i - R(v)	23
=hVθ'(hθt [Aθt,z(χ)],y), Vθ'(hθt [Aθt,v(χ)],y),〉- R(v) + o(1)	( )
Note that the point of which the various expressions the above are expanded from appears arbitrarily
chosen from a strict mathematical sense, but these choices enable fast practical computation. Any
of such expansions, assuming enough regularity, are equivalent in the limit η → 0.
Plugging the above into the expression for C (Eq. 13), we have
C(u,t) ≈ max {hVθ '(h% [Aθt,v(x)],y), Re '^ [Aθt,z(x)],y)i- R(v)}
v	(24)
-(hVe'(hθt [Aθt,u(χ)], y), Vθ'(hθt [Aθt,z(χ)], y)i- R(U))
As discussed in the beginning of this section, to ease computation, we may replace z by v in the first
term and U in the second term, assuming that they give sufficient adversarial perturbations to get
C(u,t) ≈max {∣∣Vθ'(hθt[Aθt,v(x)],y)k2 — R(v)}
-(kVθ'(hθt [Aθt,u(χ)],y)k2 — R(U)).
(25)
Upon substituting v = (K, α) and U = (Kt, αt) gives the desired result.
16