Under review as a conference paper at ICLR 2020
BasisVAE: Orthogonal Latent Space for Deep
Disentangled Representation
Anonymous authors
Paper under double-blind review
Ab stract
The variational autoencoder, one of the generative models, defines the latent space
for the data representation, and uses variational inference to infer the posterior
probability. Several methods have been devised to disentangle the latent space
for controlling the generative model easily. However, due to the excessive con-
straints, the more disentangled the latent space is, the lower quality the generative
model has. A disentangled generative model would allocate a single feature of
the generated data to the only single latent variable. In this paper, we propose
a method to decompose the latent space into basis, and reconstruct it by linear
combination of the latent bases. The proposed model called BasisVAE consists
of the encoder that extracts the features of data and estimates the coefficients for
linear combination of the latent bases, and the decoder that reconstructs the data
with the combined latent bases. In this method, a single latent basis is subject to
change in a single generative factor, and relatively invariant to the changes in other
factors. It maintains the performance while relaxing the constraint for disentan-
glement on a basis, as we no longer need to decompose latent space on a standard
basis. Experiments on the well-known benchmark datasets of MNIST, 3DFaces
and CelebA demonstrate the efficacy of the proposed method, compared to other
state-of-the-art methods. The proposed model not only defines the latent space
to be separated by the generative factors, but also shows the better quality of the
generated and reconstructed images. The disentangled representation is verified
with the generated images and the simple classifier trained on the output of the
encoder.
1	Introduction
The proper choice of data representation is highly correlated with the difficulty of task learning
for a given machine learning approach (Higgins et al., 2017; Kim et al., 2018; Kim & Cho, 2018;
2019). Using a representation appropriate to specific task and data domain can significantly improve
the robustness and successful learning of the model (Kim et al., 2018; Kim & Cho, 2018; 2019;
Bengio et al., 2013). In particular, disentangled representation is useful when dealing with data
with various features, and can be effective for a large variety of domains and tasks (Bengio et al.,
2013; Ridgeway, 2016). A latent space is disentangled if single latent units are subject to changes
in single generative factors, and relatively invariant to changes in other factors (Bengio et al., 2013).
For example, a generative model trained on a dataset of facial images learns independent latent units
subject to single independent generative factors such as hair color, gender, and emotion. We define
the disentangled representation using equation (1). A change of single generative factor is consistent
to the change of single coefficient ci , but not to cj for i 6= j .
z = Σiciei	(1)
where z ∈ RZ is a latent variable, ei ∈ RZ is a standard unit vector, and ci ∈ R is a coeffi-
cient. Disentangled representation can be useful in several machine learning tasks including transfer
learning and zero-shot learning (Lake et al., 2017). Moreover, unlike most representation learn-
ing algorithms, disentangled representation can be interpreted because they are consistent with the
variability of the data (Dupont, 2018). The variational autoencoder (VAE) is used to define the la-
tent space by approximating the posterior distribution with approximation as follows (Kingma &
Welling, 2013).
logPθ(x) ≥ -DKL[qφ(z∣x)kpθ(z)]+ Eqφ(z∣χ)[lθgPθ(x|z)]	(2)
1
Under review as a conference paper at ICLR 2020
Figure 1: In the vanilla VAE model, an interpolation experiment results in only changing the single
generative factor only for (a) gender and (b) skin color.
where DKL is Kullback-Leibler divergence, qφ(z∣x) is a posterior distribution inferred by encoder,
pθ(z) is a prior distribution, and p(x|z) is a likelihood or decoder. Since the VAEs are powerful to
define the latent space, it is often used for disentangled representation learning (Higgins et al., 2017;
Dupont, 2018). However, in most cases, the quality of the generated data is relatively low because of
the added constraints to the loss function (Kim & Cho, 2019). This is because the scale of the latent
variable to represent the generative factor drops from RZ to R1 when the generative model is f :
Z ⊂ RZ → X ⊂ RX, where Z and X are the dimensions of the latent space and data, respectively.
Several researchers have studied for disentanglement, but the trade-off with performance has not
been considered (Higgins et al., 2017; Dupont, 2018; Chen et al., 2016). In a vanilla VAE, one
generative factor changes in the direction of element in a non-standard basis as shown in Fig. 1.
With this result, if the latent space can be decomposed With basis B = {bι, •一，bn} to denote
latent variable z as Σicibi, the single generative factor is associated with the single coefficient.
Therefore, disentangled representation learning is achieved by the following two constrains:
1.	Each coefficient is subject to change in single generative factor, and relatively invariant to
the changes in other factors.
2.	The generative model is trained to make the basis of the latent space as a standard basis
B0 = {e1,…，en}.
In this paper, we focus on the first constraint to formulate disentangled representation without the
second constraint. The rest of this paper is organized as follows. In Section 2, we introduce the
research for learning disentangled representation. The work we have done in this paper and the
proposed model are presented in Section 3 and the evaluation is discussed in Section 4. Section 5
presents a summary and some future works. 2
2	Related Works
Many studies have been conducted to learn a data representation. It is used on various applications
from feature extraction to dimension reduction. Approaches are divided into two categories: con-
ventional methods and deep learning models. Principal component analysis (PCA) or independent
component analysis (ICA) are well-known methods to extract features and reduce the size of data
(Smith, 2002; Hyvarinen & Oja, 2000). Dictionary learning develops a set (dictionary) of repre-
sentative elements from the data such that each datum can be expressed as a weighted sum of the
atoms. The elements and weights can be found by minimizing the error with L1 regularization on the
weights to enable sparsity (Mairal et al., 2009; Lee et al., 2007; Aharon et al., 2006). They adopted
the methods such as basis on linear algebra that defines the materials and mixes them appropriately
to represent the data. In another approach, Kingma and Welling proposed auto-encoding variational
Bayes to approximate the posterior distribution (Kingma & Welling, 2013). Radford et al. showed
that the walking in the latent space resulted in semantic changes (Radford et al., 2015). Oord et al.
proposed a vector-quantized VAE to learn a discrete latent representation (van den Oord & Vinyals,
2017). It is not disentangled, but somewhat with general representation to prevent posterior collapse
(i.e., violation of the first constraint). Chen et al. presented InfoGAN that learned interpretable rep-
2
Under review as a conference paper at ICLR 2020
E: encoder
D: decoder
/(x) ∈ ΠT%:
coefficients for basis
£/(%)∈ Wlχ: variance
JVC3 ∈ 脓ZX加：basis
matrix
Figure 2: The architecture of the proposed model, BasisVAE.
resentation by using mutual information (Chen et al., 2016). Higgins et al. introduced an adjustable
hyperparameter β that balanced latent channel capacity and independence constraints with recon-
struction accuracy (betaVAE) (Higgins et al., 2017). Dupont improved betaVAE by using a joint
distribution of continuous and discrete latent variables (Dupont, 2018). Deep learning frameworks
showed promise in disentangling factors of variation, but there was a degrade in the quality of the
generated data due to the trade-off. In this paper, we propose a method to learn disentangled repre-
sentation while maintaining the quality of the generated data by learning materials and weights for
data representation like dictionary learning and disentangling factors like deep learning approach.
3	The Proposed Method
The architecture of a proposed model that constructs disentangled representation (i.e., the associ-
ation of a single basis element with a single generative factor) with a coefficient of basis element
rather than a latent unit is shown in Fig. 2. Unlike the conventional VAE that outputs the mean and
variance of the latent space expressed as a normal distribution, the encoder of BasisVAE outputs
the coefficient f (x) = c associated with elements of the basis B. The latent variable z is sampled
from the Gaussian distribution N(MB ∙ f (x), Σf (x)), where operator ∙ means matrix multiplication,
Σf(x) is a variance computed by encoder, and MB = [bι∣•一|bn] is a matrix form of bases. The
theoretical background, loss function, and algorithms of the proposed model are discussed in detail
in the following sections.
3.1	Latent Space Decomposition
For the first constraint mentioned in the introduction, it is proved in Theorem 1 that the latent space
can be decomposed as a set of single basis elements that are subject to a single generative factor.
It is enough to show that the latent variable z in the equation (2) can be decomposed into latent
variables zι,…，Zn, called latent basis, associated with a single generative factor, not into latent
units, and the evidence lower bound (ELBO) is maintained. Let nx be the number of features that
data X has and zι,…，Znx be the corresponding independent latent variables. Theorem 1. Let the
latent variable Z in ELBO be decomposed into independent latent variables zι,…，Znx associated
with a single generative factor such that p(Z) = Πip(Zi), then the ELBO with respect to Z is equal
to the average of values of the ELBO with respect to Zi. The qφ(z∣χ) which the expectation value
in equation (1) with respect to should be modified as the form of qφ(z∕χ)i∙ We prove Lemma 1
in order to prove Theorem 1. Lemma 1. If zι,…，zn are independent and L is a linear operator,
Ezι,…，zn[L(zι,…Zn)] = ∑iL(Ezi[zi]) where ai is a coefficient of Zi in L. Proof. Wejust show it
in the case of n = 2.
Ez1,z2[L(Z1,Z2)]
p(Z1, Z2)(a1Z1 + a2Z2)dZ2dZ1
z1 z2
p(Z1)a1Z1dZ1 +	p(Z2)a2Z2dZ2
z1	z2
(3)
a1Ez1(Z1) + a2Ez2(Z2)
L(Ez1[Z1],Ez2[Z2])

3
Under review as a conference paper at ICLR 2020
Figure 3: The visualization of (a) the general latent space, (b) the disentangled latent space, and (c)
latent space of the proposed model with two coordinates.
Proof of Theorem 1. Since the latent variable z can be decomposed as independent latent variable
zι, ∙∙∙ , Znx, equation (4) is derived from equation (2).
log Pθ (x) ≥ -DκL[qφ(zi, ∙∙∙ ,Znx ∣x)kPθ (zi,…，Znx )]	(4)
As Zi,…，Znx are independent,
log Pθ (x) ≥ -DκL[qφ(Zl∖Z2, ∙∙∙ ,Znx ,x)…qφ(Znx ∣x)kPθ (Zl)∙∙∙Pθ (Znx )]	(5)
+ Eqφ(zi∣Z2,…,Znx ,x)…qφ(Znx |x) [lOg pθ (X\Z1,…，Znx)]
log pθ (x) ≥ -DKL[Πinxqφ(Zi∖x)kΠinxpθ(Zi)]	(6)
+ Ennx qφ (z, | x) [log[πnx Pθ(XI Zi)/pnx-1(x)]]
log pθ (x) ≥ -DKL[Πinxqφ(Zi∖x)kΠinxpθ(Zi)]	(7)
+ Ennxqφ(Zi∣χ)[logπnxPθ(XIZi)] Tnx — I)P(X)
By Lemma 1, we can separate the expectation as follows:
logPθ(x) ≥ -1∑nx[Eqφ(Zi∣χ)[logPθ(x∖Zi)] -DκL[qφ(Zi∖x)kPθ(Zi)]]口	(8)
nx
As a result of equation (8), we can say that the first term of RHS is the reconstruction error, and
the second term associates the latent space with the data which has i-feature represented as zi.
In the next section, BasisVAE is proposed to maximize the lower bound shown in equation (8),
with zι,…，Znx becoming independent. The proof on the case on n >= 3 in Lemma 1 and the
derivations from equation (5) to (6) are more discussed in Appendix C.
3.2	BasisVAE
We set nx as the number of features existing in the set X of data and latent variable Z as linear com-
bination of zι,…，Znx. By the assumption, zι,…，Znx are independent, and for any z, Z = ∑QZi
so that the set B = {zi,…，Znx }is the basis of the latent space. For the sake of convenience, let
the elements of B be denoted as bi,…，bnx. The output of the encoder is coefficients ci,…，Cnx
because, otherwise, the model is not different with vanilla VAE and cannot achieve the disentangled
representation. The goal of the previous research is to change the latent space from (a) to (b) in Fig.
3, but the proposed method changes from (a) to (c). It maintains the area responsible for a single
generative factor but achieves disentangled representation using coefficient ci . In this method, the
model can learn a disentangled representation with coefficients (constraint 1). Besides, it does not
have to define the basis of latent space as standard basis (without constraint 2). The direction of the
latent basis bi is not limited to two (the latent unit becomes larger or smaller), but is set in all direc-
tions in RZ, thus representing the information in various ways.1 We train the encoder so that ci = 1
and cj = 0 if the input data has i-feature and no j-feature. The latent variable Z is sampled from
the normal distribution N(MB ∙ f (x), Σf(χ)) having the linear combination ∑iQZi as mean, and
∑f(χ) as variance, where MB = [bi\ …∖bn]and f (x) = (ci,…，Cn). The decoder is trained to
reconstruct the data x with Z. Algorithm 1 describes the process of defining the latent space through
the encoder and reconstructing the data through the decoder. Three losses are defined to train the
1In Rn, as n increases, the number of direction of z ∈ RZ becomes 2n.
4
Under review as a conference paper at ICLR 2020
Algorithm 1 The process to define the latent space and reconstruct the data
1:	Input: Data {xi}iN=1, encoder qφ, decoder pθ, and basis matrix MB
2:	Output: trained encoder qφ, trained decoder pθ , and trained basis matrix MB
3:	Initialize qφ , pθ
4:	for epochs do
5:	for batches do
6:	Sample x from the dataset
7：	C J qφ(x)
8：	Sample Z from N(MB ∙ CT, ∑f(χ))
9:	X J pθ (Z)
10：	Update qφ,pθ, MB with equation (12)
11： end for
12： end for
13： return qφ, pθ , MB
latent space in the proposed process： 1) reconstruction loss Lrecon, 2) inference loss LKL, and 3)
basis loss LB as follows.
Lrecon = l(x,G(C(z∣x))), C(z|x)〜N(MB ∙ f (x), Σf(χ))	(9)
LKL = DKL[MB ∙N(f (X), ς/(X))IlPθ(Z)]	(IO)
LB = kMTBMB-Ik22	(11)
where l is the binary function for measuring the reconstruction error, f(X) is the output of the
encoder, and MB = [bi ∣∙∙∙∣bnx] is the basis matrix. Since the elements in MB have to be indepen-
dent, i.e., bi ∙ bj = 0 if i = j, and b% ∙ b = 1, MTMB should be identity matrix I during training.
The total loss of the proposed model is as follows.
L=αLrecon+βLKL+γLB
(12)
where α, β, and γ are the hyperparameters for balancing between the losses. 4
4 Experiments
4.1 Dataset and Experimental Settings
To verify the performance of the proposed model, we use the MNIST, 3DFaces and CelebA datasets
(LeCun et al., 1998; Liu et al., 2015; Paysan et al., 2009). The CelebA is a dataset with large-
scale face attributes. We crop the initial 178×218 size to 138×138 and resize them as 128×128.
There are total 202,599 face images and we use 162,769 images as training data and the rest as test
data. The pixel values are normalized between 0 and 1. The weights of the model are initialized
with the method proposed by Glorot and Bengio (Glorot & Bengio, 2010). The encoder consists of
eight convolutional layers whose filter size is 5×5 with LeakyReLU activation function followed by
dropout and batch normalization layer (Maas et al., 2013; Srivastava et al., 2014; Ioffe & Szegedy,
2015). The decoder is composed of four convolutional layers and 4 deconvolutional layers with
ReLU activation function followed by several layers like encoder (Nair & Hinton, 2010). α, β, and
γ are set as 0.0004, 1, and 0.1, respectively. The binary function for measuring the reconstruction
error is set as Bojanowski et al. did (Bojanowski et al., 2017). BasisVAE is trained for 100 epochs
with 100 batch size. The optimizer used to train the model is Adam proposed by Kingma and Ba
(Kingma & Ba, 2014).
4.2 Generated Images
To verify the performance of the proposed model, the performance of BasisVAE is compared with
the performance of vanilla VAE, betaVAE, and VQ-VAE (van den Oord & Vinyals, 2017), which
have the same structure, but different output of encoder to the proposed model, in three aspects：
Reconstruction, random generation, and disentanglement.
5
Under review as a conference paper at ICLR 2020
orig HΞEinQΞ∏S∏□
Recon.B Q Q Q。目 ∏ 目 ∏ Q
(a)
。的用备FIP目日旧印腐fis剑日・■望命6周Q
ReConP-*? A 曰曰臼 GfISW 今 PJC, -- 日 Q
(b)
Figure 4:	(Top) The original images and (bottom) the reconstructed images on (a) MNIST and (b)
CelebA datasets. Appendix A shows more generated images.
ΞΞΞΞΞΞΞΞΞΞ 0E1总也日目曰曰总B❸EIenSm已£■日
ΠDΠDUDΠΠDΠ BBBBBBBlQltιΠriΞΠΓ^HEBaθ
BBBQQ□QBBB DnBBttBBBDBhDDBSBBBBEI
BBSE!BBQBE]S HHr曰QSGklp!日勺BmSn。鸟网
□ □B□□Q□□□□ R日©E30R^BQ烟日豺口n曰&目曰
BQSBBSBE1BB日目日出附©曰&&曰EIma白日，8
QQBQBQQQQQ矗曰日曰日|SEl日&■■■•日日目曰■”
BEaBQBQHQQE] BirE3府曰已出❷胆Q鹿蟠IS•亩酚臼&EIEI
BBQQBQBBE1E1 B
QDQD^□ODQ ΠllflΠΠHI∏n∏EJΠElElSO[τ FIFBO
Figure 5:	The generated images of (a) MNIST and (b) CelebA. In the MNIST dataset, the generated
data is organized in each row by class.
4.2. 1 Reconstruction
We evaluate the reconstruction performance of BasisVAE with MNIST and CelebA datasets. Fig. 4
shows the reconstructed images for the original images. In Table 1, we show the structural similarity
(SSIM) and peak signal-to-noise ratio (PSNR) values together with the comparison model for the
quantitative evaluation of the performance. The experiment is repeated 10 times to compute the
SSIM and PSNR values between the actual images and the generated images by the model trained
on CelebA dataset. The results of the t-test show that the performance of the BasisVAE is superior
to that of the other models statistically.
4.2.2	Random generation
The generated data by BasisVAE learned with MNIST and CelebA are illustrated in Fig. 5. Frechet
inception distance is used to evaluate the quality of the generated images (Heusel et al., 2017) as
shown in Table 2. The p-value obtained from the t-test was less than 0.05, indicating a statistically
significant difference in performance.
4.2.3	Generation from basis
We conduct an experiment to verify that the basis learned through BasisVAE has actually influenced
the construction of the disentangled representation. BasisVAE generates the images with basis bi
by setting the coefficients as ci = 1 and cj for i 6= j . The feature corresponding to each basis bi is
shown on the Figs. 6 and 7. We also use a 3DFaces dataset as well as CelebA dataset to identify the
Table 1: The results of evaluating the reconstruction performance with SSIM and PSNR.
		VAE	-β VAE-	VQ-VAE	BaSiSVAE
SSIM	Average	-07071-	-0.6142-	-0.7564-	-07965-
	Std. dev.	6.0×10-6	6.9×10-6	8.6×10-6	4.6×10-6
	P-ValUe	2.4×10-25	1.3×10-30	2.6×10-18	-
PSNR	Average	-64.989-	-61.512-	-66.38	-67882-
	Std. dev.	0.004	0.004-	-0.001	0.004
	p-value	2.1×1026 -	1.1×10-32^	7.8×10-23 ^	-
6
Under review as a conference paper at ICLR 2020
Table 2: Comparison of image generation quality by FID score.
	VAE	β^E	VQ-VAE	BasisVAE
Average	-112.883-	-168.239-	84.59	78.449-
Std. dev.	1309	1964	7.513	2.877
P-Value	1.49×10-21~	1.85×10-28~	5.31×10-6	-
5_o_clock_ Archedeye	Bagsjmder
Bushy	Double	Heavy
Bluny Brownhaii eyebrows Chubby Chin Eyeglasses Goatee Grayhaii makeup Highcheek
Mouthslig	Narrow	Pointy Receding Rosy
Straight	Wearing Wearing Wearing Wearing Wearing
Sidebums Smile hair Wavy_hair earrings hat lipstick necklace necktie Young
Figure 6: The images generated from a single basis. The corresponding feature is shown above the
image.
Figure 7: The images generated from a single basis. The value of the coefficient is linearly changed
along the row. The corresponding characteristics are shown in the left of the images.
we∙>0∙∙*
7
Under review as a conference paper at ICLR 2020
Hair _
color A
Hair _
color B
Bags
under-
eyes
■■HnmR
bbππ≡ħ
H∙EΠ≡IS
DBΠ口≡fi
πqππ日π
ααπn≡π
■■nQQH
π日 cπαπ
OQΠΠR0
0 Rcπ≡o
DΠOΠE20
■同■■■■
0fl
ΩO Tʃloa
■。■■■―
Qπ∙Mπa
LOG1HOa
.匕Glplπo
qħ∙∙∙∙
■■■■■■
二 rlHπ0
-Bald
-Pale skin
-Eyeglasses
Figure 8: The value of the coefficient is linearly changed along the row. The corresponding charac-
teristics are shown in the side of the images. Appendix A shows more generated images.
Figure 9: Randomly generated images with the coefficient ci for bi fixed as 1. According to the
basis element, the images reflecting the corresponding feature are generated. Appendix A shows
more generated images.
characteristic change with coefficient size. As shown in Figs. 8 and 9, we can see the basis element
corresponding to azimuth and lighting in 3DFaces dataset and to hair color, bags under eyes, bald,
etc. in CelebA dataset. To show that single basis element is subject to a single generative factor,
we randomly generate an image with the coefficient ci for bi fixed as 1, as shown in Fig. 9. To
quantitatively evaluate the disentangled representation, logistic regression is trained to classify the
features by inputting the output of the encoder into itself. The more disentangled the latent space is,
the higher accuracy the model achieves. We train about 40 binary classifiers for 40 classes of CelebA
dataset, and the average accuracy is shown in Table 3. Appendix A shows the more generated images
and Appendix C shows the distribution of coefficients ci that model learns with examples.
5 Conclusion
In this paper, we have formulated the disentangled representation learning with two constraints. By
proving the Theorem, it is shown that the latent space can be decomposed as independent latent
variables associated with single generative factor. We have shown that the proposed BasisVAE
constructs disentangled representation without the second constraint by constructing the basis of the
latent space. Furthermore, BasisVAE outperforms the vanilla VAE and βVAE in both performance
and disentanglement. Since our method can be applied to other VAEs by changing the output of
the encoder as coefficients for basis element and adding loss LB , we will verify the versatility and
validity by applying it to other models. The performance of the proposed model will be evaluated
with other well-known benchmark datasets such as CIFAR10, 3DFaces, and ImageNet. In addition,
we will achieve the higher quality of the generated data and interpretability of the latent space by
constructing disentangled latent space in generative adversarial network.
Table 3: Results of classification using the output of the encoder. The logistic regression model
for each class is trained to classify the one class. Appendix B shows more details in the numerical
results for each attributes. ________________________________________________________
	VAE	β VAE	VQ-VAE	BaSiSVAE
Average	0.8190	0.8444	0:8225	08982
Std. dev.	0.015	0.009	0.014	0.005
P-Value	0.001	0.004	4.1×10-4	-
8
Under review as a conference paper at ICLR 2020
References
M. Aharon, M. Elad, and A. Bruckstein. K-svd: An algorithm for designing overcomplete dictio-
naries for sparse representation. IEEE Trans. on Signal Processing, 54(11):4311-4322, 2006.
Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives.
IEEE Trans. on Pattern Analysis and Machine Intelligence, 35(8):1798-1828, 2013.
P. Bojanowski, A. Joulin, D. Lopez-Paz, and A. Szlam. Optimizing the latent space of generative
networks. arXiv preprint arXiv:1707.05776, 2017.
X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, and P. Abbeel. Infogan: Interpretable
representation learning by information maximizing generative adversarial nets. In Advances in
Neural Information Processing Systems, pp. 2172-2180, 2016.
E. Dupont. Learning disentangled joint continuous and discrete representations. In Advances in
Neural Information Processing Systems, pp. 710-720, 2018.
X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks.
In Int. Conf. on Artificial Intelligence and Statistics, pp. 249-256, 2010.
M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. Gans trained by a two
time-scale update rule converge to a local nash equilibrium. In Advances in Neural Information
Processing Systems, pp. 6626-6637, 2017.
I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick, S. Mohamed, and A. Lerchner.
Beta-vae: Learning basic visual concepts with a constrained variational framework. Int. Conf. on
Learning Representation, 2(5):6, 2017.
A Hyvarinen and E Oja. Independent component analysis: algorithms and applications. Neural
networks, 13(4-5):411-430, 2000.
S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing
internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
J.Y. Kim and S.B. Cho. Detecting intrusive malware with a hybrid generative deep learning model.
In Int. Conf. on Intelligent Data Engineering and Automated Learning, pp. 499-507. Springer,
2018.
J.Y. Kim and S.B. Cho. Electric energy consumption prediction by deep learning with state explain-
able autoencoder. Energies, 12(4):739, 2019.
J.Y. Kim, S.J. Bu, and S.B. Cho. Zero-day malware detection using transferred generative adversarial
networks based on deep autoencoders. Information Sciences, 460:83-102, 2018.
D.P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
D.P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114,
2013.
B.M. Lake, T.D. Ullman, J.B. Tenenbaum, and S.J. Gershman. Building machines that learn and
think like people. Behavioral and Brain Sciences, 40, 2017.
Y LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recog-
nition. Proc. of the IEEE, 86(11):2278-2324, 1998.
H Lee, A Battle, R Raina, and A.Y. Ng. Efficient sparse coding algorithms. In Advances in Neural
Information Processing Systems, pp. 801-808, 2007.
Z. Liu, P. Luo, X. Wang, and X. Tang. Deep learning face attributes in the wild. In IEEE Int. Conf.
on Computer Vision, pp. 3730-3738, 2015.
A.L. Maas, A.Y. Hannun, and A.Y. Ng. Rectifier nonlinearities improve neural network acoustic
models. Int. Conf. on Machine Learning, 30(1):3, 2013.
9
Under review as a conference paper at ICLR 2020
J Mairal, J Ponce, G Sapiro, A Zisserman, and F.R. Bach. Supervised dictionary learning. In
Advances in Neural Information Processing Systems, pp. 1033-1040, 2009.
V. Nair and G. Hinton. Rectified linear units improve restricted boltzmann machines. In Int. Conf.
on Machine Learning, pp. 807-814, 2010.
P. Paysan, R. Knothe, B. Amberg, S. Romdhani, and T. Vetter. A 3d face model for pose and illumi-
nation invariant face recognition. In Int. Conf. on Advanced Video and Signal Based Surveillance,
pp. 296-301. Ieee, 2009.
A Radford, L Metz, and S Chintala. Unsupervised representation learning with deep convolutional
generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
K. Ridgeway. A survey of inductive biases for factorial representation-learning. arXiv preprint
arXiv:1612.05299, 2016.
L.I. Smith. A tutorial on principal components analysis. Univ. of Otago, Technical Report, 2002.
N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: A simple
way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15
(1):1929-1958, 2014.
A van den Oord and O Vinyals. Neural discrete representation learning. In Advances in Neural
Information Processing Systems, pp. 6306-6315, 2017.
10
Under review as a conference paper at ICLR 2020
A Appendix A: More Images Generated

ɜ ΠΠE∖ Ll
□beid□ħdhb∩π□
BsnFaβl日πti∙α^n曰呦^rππ∙∙∙ħpbπheiπ∙
∙B∙∙∙∙∙∙∙∙∙B∙∙∙∙∙∙H∙∙∙∙∙∙∙∙∙IΞR
U■■日❸B∙∙∙∙∙∙DI∙∙∙∙nH∙∙HBBBE]∙∙n∙
Pn∙ 一，πkiπG^π0明初山ElBH❸启0π&πH 曰, Ξ Bad
■■■H旧 nn■目π∙EIB∙π0H∙∙RH∙∙∙∙∙D∙Dπ
IPC - ∙l.: *"π∩C^iLKESπ^，: "}rπF L C A.f〔'nπn
■■■■■ ∙Da∙B∙π∙s∙BM∙H■& ■■■■■■■ ■!!
■■BH・IS・ ∙∙∙∙0■日■目■■1]■ ∙∙S∙B∙∙∙ΠG
Figure 10: The generated CelebA images. Image blur is less than conventional VAE.
11
Under review as a conference paper at ICLR 2020
Figure 11: The value of the coefficient is linearly changed along the row. The corresponding char-
acteristics are shown in the side of the images.
12
Under review as a conference paper at ICLR 2020
ElS 口HEl ■・■■电
Figure 12: Randomly generated images with the coefficient ci for bi fixed as 1. According to the
basis element, images reflecting the corresponding feature are generated.
13
Under review as a conference paper at ICLR 2020
Table 4: Results of classification using the output of the encoder. The logistic regression model for
each class is trained to classify the one class.
Class	VAE	betaNAE	VQ-VAE	BasisVAE
5_o_clock_shadow	0.8820	0.8816	0.8820	0.9220
Arched_eyebrows	0.7414	0.7450	0.7423	0.8231
Attractive	0.6579	0.6945	0.6468	0.7842
Bags .under_eyes	0.7926	0.7920	0.7926	0.8243
Bald	0.9798	0.9798	0.9792	0.9808
Bangs	0.8532	0.8890	0.8588	0.9376
Big」ips	0.8468	0.8466	0.8468	0.8419
Big_nose	0.7514	0.7568	0.7514	0.8115
B山Ck上air	0.7899	0.8335	0.7876	0.8773
Blond_hair	0.8470	0.8983	0.8591	0.9377
Blurry	0.9526	0.9526	0.9526	0.9526
BroWn-hair	0.7586	0.7614	0.7586	0.8304
Bushy_eyebrows	0.8575	0.8603	0.8575	0.9105
Chubby	0.9389	0.9389	0.9389	0.9440
Doublexhin	0.9510	0.9510	0.9510	0.9578
Eyeglasses	0.9305	0.9312	0.9304	0.9837
Goatee	0.9265	0.9265	0.92646	0.9542
Gray_hair	0.9513	0.9517	0.95131	0.9702
Heavy-makeup	0.6430	0.7586	0.6796	0.9037
Highxheeknones	0.5672	0.7116	0.5907	0.8596
Male	0.6522	0.7720	0.6689	0.9780
MoUth_slightly _open	0.5480	0.6752	0.5542	0.9262
Mustache	0.9496	0.9496	0.9496	0.9519
NarroW_eyes	0.9250	0.9251	0.9250	0.9222
No_beard	0.8225	0.8288	0.8225	0.9347
Ovalface	0.7196	0.7189	0.7196	0.7393
Pale_skin	0.9569	0.9594	0.9569	0.9625
Pointy _nose	0.7151	0.7151	0.7151	0.7459
Receding-hairline	0.9281	0.9277	0.9281	0.9339
Rosyxheeks	0.9317	0.9316	0.9317	0.9431
Sideburns	0.9313	0.9314	0.9313	0.9565
Smiling	0.5730	0.7330	0.5957	0.9165
Straight_hair	0.7942	0.7947	0.7942	0.7989
Wavy_hair	0.7262	0.7516	0.7288	0.7917
Wearing_earrings	0.8092	0.8103	0.8092	0.8597
Wearing_hat	0.9528	0.9617	0.9528	0.9798
Wearing」ipstick	0.6542	0.7668	0.67945	0.9135
Wearing_necklace	0.8794	0.8794	0.8794	0.8794
Wearing_necktie	0.9273	0.9271	0.9273	0.9277
Young	0.7465	0.7541	0.7464	0.8588
B	Appendix B: Detailed Results to Measure Disentanglement
The more detailed classification accuracy, summarized in Table 3 on average, are shown in Table 4
by generative factors.
C	Appendix C: More details in the Proof of Lemma 1 and Theorem
1
In the Section 3.1, we show the proof of Lemma 1 in the case of n = 2. For more generality,
it is necessary to be proved that the Lemma 1 holds in the case of n >= 3. In that case, Let
z2 = (z3 , z4, . . . , zn), repeat the same verification process, then we can separate the z3 from the
(z4,…，Zn). If We process it recursively, the Lemma 1 in the case of n >= 3 holds too.
14
Under review as a conference paper at ICLR 2020
Ooooboc
uooaNeNgaM'loooaooeMgaM-ManengaM。。ImtMeoogaooɑlowaM∞m3MolM<>lnflM4M3n-WMmm40∞o"]OoOan∞mm%
Figure 13: The distribution of ci (1 <= i <= 40) that model learns for each data with (a) ”female
with blond hair”, (b) ”female with black hair”, and (c) ”male with black hair”. We have red boxes
for coefficients where the difference between distributions in (a) and distributions in (b) and c is
apparent.
As in the proof of Lemma 1, it is enough to prove the Theorem 1 that Theorem 1 holds when n = 2.
The flow of the equations (4) to (8) except for the equations (5) to (6) is natural. Equation (6) is
derived from the (5) with p(x | z1, z2) = p(x | z1)p(x | z2)/p(x), which is equivalent to equation
(13).
p(x,Zl,Z2) _ p(x,Zl)p(x,Z2)
P(Z1,z2 )	P(X)P(ZI)P(Z2 )
(13)
Since z1, ∙ ∙ ∙ , zn are independent, it is drived to (14).
P(x,Z1)P(x,Z2)
p(x,Z1,Z2) = -------"-------- (14)
The LHS of equation (14) can be modified as (15) to (18)
P(x,Z1,Z2)
P(X，Z1，z2)= p(x,zι) P(X，Z1)
= P(Z2 | x,Z1)P(x,Z1)
= P(Z2 | X)P(X, Z1)
=p(x,Zi )p(x,Z2 )
P(X)
(15)
Since the LHS of equation (14) can be derived to the RHS of equation (15), the equation (13) is
correct, resulting in the correctness of Theorem 1.
D	APPENDIX D: THE DISTRIBUTION OF COEFFICIENTS ci
We show the distribution of ci that the model learns for each data with ”female with blond hair”,
”female with black hair”, and ”male with black hair” in the Fig. 13. With comparison of distributions
in Fig. 13(a) and (b), coefficients c9 and c10 represent ”black hair” and ”blond hair”, respectively.
Furthermore, we can confirm that coefficients c19, c21, and c37 represent the generative factors
related to the characteristics on gender, comparing Fig. 13(a) and (c).
15