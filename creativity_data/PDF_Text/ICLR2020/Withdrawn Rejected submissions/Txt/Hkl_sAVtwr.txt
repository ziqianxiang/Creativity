Under review as a conference paper at ICLR 2020
Compressed Sensing with Deep Image Prior
and Learned Regularization
Anonymous authors
Paper under double-blind review
Ab stract
We propose a novel method for compressed sensing recovery using untrained deep
generative models. Our method is based on the recently proposed Deep Image
Prior (DIP), wherein the convolutional weights of the network are optimized to
match the observed measurements. We show that this approach can be applied to
solve any differentiable linear inverse problem, outperforming previous unlearned
methods. Unlike various learned approaches based on generative models, our
method does not require pre-training over large datasets. We further introduce a
novel learned regularization technique, which incorporates prior information on
the network weights. This reduces reconstruction error, especially for noisy mea-
surements. Finally we prove that, using the DIP optimization approach, moder-
ately overparameterized single-layer networks trained can perfectly fit any signal
despite the nonconvex nature of the fitting problem. This theoretical result pro-
vides justification for early stopping.
1	Introduction
We consider the well-studied compressed sensing problem of recovering an unknown signal x* ∈
Rn by observing a set of noisy measurements y ∈ Rm of the form
y = Ax* + η.
(1)
Here A ∈ Rm×n is a known measurement matrix, typically generated with random independent
Gaussian entries. Since the number of measurements m is smaller than the dimension n of the
unknown vectorx*, this is an under-determined system of noisy linear equations and hence ill-posed.
There are many solutions, and some structure must be assumed on x* to have any hope of recovery.
Pioneering research (Donoho, 2006; CandeS et al., 2006; Candes & Tao, 2005) established that if x*
is assumed to be sparse in a known basis, a small number of measurements will be provably sufficient
to recover the unknown vector in polynomial time using methods such as Lasso (Tibshirani, 1996).
Sparsity approaches have proven successful, but more complex models with additional structure
have been recently proposed such as model-based compressive sensing (Baraniuk et al., 2010) and
manifold models (Hegde et al., 2008; Hegde & Baraniuk, 2012; Eftekhari & Wakin, 2015). Bora
et al. (2017) showed that deep generative models can be used as excellent priors for images. They
also showed that backpropagation can be used to solve the signal recovery problem by performing
gradient descent in the generative latent space. This method enabled image generation with signif-
icantly fewer measurements compared to Lasso for a given reconstruction error. Compressed sens-
ing using deep generative models was further improved in very recent work (Tripathi et al., 2018;
Grover & Ermon, 2018a; Kabkab et al., 2018; Shah & Hegde, 2018; Fletcher & Rangan, 2017; Asim
et al., 2018). Additionally a theoretical analysis of the nonconvex gradient descent algorithm (Bora
et al., 2017) was proposed by Hand & Voroninski (2017) under some assumptions on the generative
model.
Inspired by these impressive benefits of deep generative models, we chose to investigate the appli-
cation of such methods for medical imaging, a canonical application of compressive sensing. A
significant problem, however, is that all these previous methods require the existence of pre-trained
models. While this has been achieved for various types of images, e.g. human faces of CelebA (Liu
et al., 2015) via DCGAN (Radford et al., 2015), it remains significantly more challenging for med-
1
Under review as a conference paper at ICLR 2020
ical images (Wolterink et al., 2017; Schlegl et al., 2017; Nie et al., 2017; Schlemper et al., 2017).
Instead of addressing this problem in generative models, we found an easier way to circumvent it.
Surprising recent work by Ulyanov et al. (2017) proposed Deep Image Prior (DIP), which uses un-
trained convolutional neural networks. In DIP-based schemes, a convolutional neural network gen-
erator (e.g. DCGAN) is initialized with random weights; these weights are subsequently optimized
to make the network produce an output as close to the target image as possible. This procedure is
unlearned, using no prior information from other images. The prior is enforced only by the fixed
convolutional structure of the generator network.
Generators used for DIP are typically over-parameterized, i.e. the number of network weights is
much larger compared to the output dimension. For this reason DIP has empirically been found to
overfit to noise if run for too many iterations (Ulyanov et al., 2017). In this paper we theoretically
prove that this phenomenon occurs with gradient descent and justify the use of early stopping and
other regularization methods.
Our Contributions:
•	In Section 3 we propose DIP for compressed sensing (CS-DIP). Our basic method is as fol-
lows. Initialize a DCGAN generator with random weights; use gradient descent to optimize
these weights such that the network produces an output which agrees with the observed
measurements as much as possible. This unlearned method can be improved with a novel
learned regularization technique, which regularizes the DCGAN weights throughout the
optimization process.
•	In Section 4 we theoretically prove that DIP will fit any signal to zero error with gradient
descent. Our result is established for a network with a single hidden layer and sufficient
constant fraction over-parametrization. While it is expected that over-parametrized neural
networks can fit any signal, the fact that gradient descent can provably solve this non-
convex problem is interesting and provides theoretical justification for early stopping.
•	In Section 5 we empirically show that CS-DIP outperforms previous unlearned methods
in many cases. While pre-trained or “learned” methods frequently perform better (Bora
et al., 2017), we have the advantage of not requiring a generative model trained over large
datasets. As such, we can apply our method to various medical imaging datasets for which
data acquisition is expensive and generative models are difficult to train.
2	Background
2.1	Compressed Sensing: Classical and Unlearned Approaches
A classical assumption made in compressed sensing is that the vector x* is k-sparse in some basis
such as wavelet or discrete cosine transform (DCT). Finding the sparsest solution to an underdeter-
mined linear system of equations is NP-hard in general; however, if the matrix A satisfies conditions
such as the Restricted Eigenvalue Condition (REC) or Restricted Isometry Property (RIP) (Can-
des et al., 2006; Bickel et al., 2009; Donoho, 2006; Tibshirani, 1996), then x* can be recovered
in polynomial time via convex relaxations (Tropp, 2006) or iterative methods. There is extensive
compressed sensing literature regarding assumptions on A, numerous recovery algorithms, and vari-
ations of RIP and REC (Bickel et al., 2009; Negahban et al., 2009; Agarwal et al., 2010; Bach et al.,
2012; Loh & Wainwright, 2011).
Compressed sensing methods have found many applications in imaging, for example the single-pixel
camera (SPC) (Duarte et al., 2008). Medical tomographic applications include x-ray radiography,
microwave imaging, magnetic resonance imaging (MRI) (Winters et al., 2010; Chen et al., 2008;
Lustig et al., 2007). Obtaining measurements for medical imaging can be costly, time-consuming,
and in some cases dangerous to the patient (Qaisar et al., 2013). As such, an important goal is to
reduce the number of measurements while maintaining good reconstruction quality.
Aside from the classical use of sparsity, recent work has used other priors to solve linear inverse
problems. Plug-and-play priors (Venkatakrishnan et al., 2013; Chan et al., 2017) and Regularization
by Denoising (Romano et al., 2017) have shown how image denoisers can be used to solve general
linear inverse problems. A key example of this is BM3D-AMP, which applies a Block-Matching
2
Under review as a conference paper at ICLR 2020
and 3D filtering (BM3D) denoiser to an Approximate Message Passing (D-AMP) algorithm (Met-
zler et al., 2016; 2015). AMP has also been applied to linear models in other contexts (Schniter et al.,
2016). Another related algorithm is TVAL3 (Zhang et al., 2013; Li et al., 2009) which leverages aug-
mented Lagrangian multipliers to achieve impressive performance on compressed sensing problems.
In many different settings, we compare our algorithm to these prior methods: BM3D-AMP, TVAL3,
and Lasso.
2.2	Compressed Sensing: Learned Approaches
While sparsity in some chosen basis is well-established, recent work has shown better empirical
performance when neural networks are used (Bora et al., 2017). This success is attributed to the
fact that neural networks are capable of learning image priors from very large datasets (Goodfellow
et al., 2014; Kingma & Welling, 2013). There is significant recent work on solving linear inverse
problems using various learned techniques, e.g. recurrent generative models (Mardani et al., 2017b)
and auto-regressive models (Dave et al., 2018). Additionally approximate message passing (AMP)
has been extended to a learned setting by Metzler et al. (2017).
Bora et al. (2017) is the closest to our set-up. In this work the authors assume that the unknown
signal is in the range of a pre-trained generative model such as a generative adversarial network
(GAN) (Goodfellow et al., 2014) or variational autoencoder (VAE) (Kingma & Welling, 2013). The
recovery of the unknown signal is obtained via gradient descent in the latent space by searching for
a signal that satisfies the measurements. This can be directly applied for linear inverse problems
and more generally to any differentiable measurement process. Recent work has built upon these
methods using new optimization techniques (Chang et al., 2017), uncertainty autoencoders (Grover
& Ermon, 2018b), and other approaches (Dhar et al., 2018; Kabkab et al., 2018; Mixon & Villar,
2018; Pandit et al., 2019; Rusu et al., 2018; Hand et al., 2018). The key point is that all this prior work
requires pre-trained generative models, in contrast to CS-DIP. Finally, there is significant ongoing
work to understand DIP and develop related approaches (Heckel et al., 2018; Dittmer et al., 2018).
3	Proposed Algorithm
Let x* ∈ Rn be the signal that We are trying to reconstruct, A ∈ Rm×n be the measurement
matrix, and η ∈ Rm be independent noise. Given the measurement matrix A and the observations
y = Ax* + η, we wish to reconstruct an X that is close to x*.
A generative model is a deterministic function G(z; w): Rk → Rn Which takes as input a seed
z ∈ Rk and is parameterized by “weights” w ∈ Rd, producing an output G(z; w) ∈ Rn. These
models have shown excellent performance generating real-life signals such as images (Goodfellow
et al., 2014; Kingma & Welling, 2013) and audio (Van Den Oord et al., 2016). We investigate
deep convolutional generative models, a special case in which the model architecture has multiple
cascaded layers of convolutional filters (Krizhevsky et al., 2012). In this paper we apply a DC-
GAN (Radford et al., 2015) model and restrict the signals to be images.
3.1	Compressed Sensing with Deep Image Prior (CS-DIP)
Our approach is to find a set of weights for the convolutional network such that the measurement
matrix applied to the network output, i.e. AG(z; w), matches the measurements y we are given.
Hence we initialize an untrained network G(z; w) with some fixed z and solve the following:
w* = arg min ky - AG(z; w)k2 .	(2)
w
This is, of course, a non-convex problem because G(z; w) is a complex feed-forward neural network.
Still we can use gradient-based optimizers for any generative model and measurement process that
is differentiable. Generator networks such as DCGAN are biased toward smooth, natural images
due to their convolutional structure; thus the network structure alone provides a good prior for
reconstructing images in problems such as inpainting and denoising (Ulyanov et al., 2017). Our
finding is that this applies to general linear measurement processes. Furthermore, our method also
directly applies to any differentiable forward operator A. We restrict our solution to lie in the span
3
Under review as a conference paper at ICLR 2020
of a convolutional neural network. If a sufficient number of measurements m is given, we obtain an
output such that x* ≈ G(z; w*).
Note that this method uses an untrained generative model and optimizes over the network weights
w. In contrast previous methods, such as that of Bora et al. (2017), use a trained model and optimize
over the latent z-space, solving z* = arg minz ky - AG(z; w)k2. We instead initialize a random z
with Gaussian i.i.d. entries and keep this fixed throughout the optimization process.
In our algorithm we leverage the well-established total variation regularization (Rudin et al., 1992;
Wang et al., 2008; Liu et al., 2018), denoted as TV (G(z; w)). We also propose an additional learned
regularization technique, LR(w); note that without this technique, i.e. when λL = 0, our method is
completely unlearned. Lastly we use early stopping, a phenomena that will be analyzed theoretically
in Section 4.
Thus the final optimization problem becomes
w* = arg minky - A G(z; w)k2 + R(w; λT, λL).	(3)
w
The regularization term contains hyperparameters λT and λL for total variation and learned regular-
ization: R(w; λT , λL) = λTTV (G(z; w)) + λLLR(w). Next we discuss this LR(w) term.
3.2	Learned Regularization
Without learned regularization CS-DIP relies only on linear measurements taken from one unknown
image. We now introduce a novel method which leverages a small amount of training data to opti-
mize regularization. In this case training data refers to measurements from additional ground truth
of a similar type, e.g. other x-ray images.
To leverage this additional information, we pose Eqn. 3 as a Maximum a Posteriori (MAP) estima-
tion problem and propose a novel prior on the weights of the generative model. This prior then acts
as a regularization term, penalizing the model toward an optimal set of weights w*.
For a set of weights w ∈ Rd, we model the likelihood of the measurements y = Ax, y ∈ Rm , and
the prior on the weights w as Gaussian distributions given by
p(y|w)
ky - AG(z; w)k2
2λL
√(2πλL)m
exp ( — 2 (W — μ)T Σ-1 (W —
P(W) = ------/	-----
√WI∑i
where μ ∈ Rd and Σ ∈ Rd×d.
In this setting we want to find a set of weights W* that maximizes the posterior on W given y, i.e.,
w* =	arg max p(w∣y)	≡ arg min	∣∣y — AG(z;	w)k2 +	Xl	(w — μ)T	Σ-1	(w	— μ).	(4)
ww
This gives us the learned regularization term
LR(W) = (w — μ)T Σ-1 (w — μ),
(5)
where the coefficient λL in Eqn. 4 controls the strength of the prior.
Our motivation for assuming a Gaussian distribution on the weights is to build upon the proven
success of '2 regularization, which also makes this assumption. Notice that when μ = 0 and Σ =
Id×d, this regularization term is equivalent to '2-regularization. Thus this method can be thought of
as an adaptive version of standard weight decay. Further, because the network weights are initialized
Gaussian i.i.d., we assumed the optimized weights would also be Gaussian. Previous work has
shown evidence that the convolutional weights in a trained network do indeed follow a Gaussian
distribution (Ma et al., 2018).
4
Under review as a conference paper at ICLR 2020
3.2.1	Learning the Prior Parameters
In the previous section, we introduced the learned regularization term LR(w) defined in Eqn. 5.
However We have not yet learned values for parameters (μ, Σ) that incorporate prior knowledge of
the network weights. We now propose a way to estimate these parameters.
Assume we have a set of measurements SY = {y1,y2, ∙∙∙ ,jq} from Q different images SX =
{χ1,χ2,…，xq}, each obtained with a different measurement matrix A. For each measurement
yq, q ∈ {1, 2, ..., Q}, we run CS-DIP to solve the optimization problem in Eqn. 3 and obtain an
optimal set of weights W * = {w；,wg,…,WQ }. Note that when optimizing for the weights W *,
we only have access to the measurements SY, not the ground truth SX .
The number of weights d in deep networks tends to be very large. As such, learning a distribution
over each weight, i.e. estimating μ ∈ Rd and Σ ∈ Rd×d, becomes intractable. We instead use
a layer-wise approach: with L network layers, we have μ ∈ RL and Σ ∈ RL×L. Thus each
weight within layer l ∈ {1,2,..., L} is modeled according to the same N(μι, ∑ιι) distribution. For
simplicity we assume Σij = 0 ∀i 6= j, i.e. that network weights are independent across layers. The
process of estimating statistics (μ, Σ) from W * is described in Algorithm 1 of the appendix.
We use this learned (μ, Σ) in the regularization term LR(W) from Eqn. 5 for reconstructing mea-
surements of images. We refer to this technique as learned regularization. While this may seem
analogous to batch normalization (IOffe & Szegedy, 2015), note that we only use (μ, Σ) to penalize
the '2-norm of the weights and do not normalize the layer outputs themselves.
3.2.2	Discussion of Learned Regularization
The proposed CS-DIP does not require training if no learned regularization is used, i.e. if λL = 0
in Eqn. 3. This means that CS-DIP can be applied only with measurements from a single image and
no prior information of similar images in a dataset.
Our next idea, learned regularization, utilizes a small amount of prior information, requiring access
to measurements from a small number of similar images (roughly 5 - 10). In contrast, other pre-
trained models such as that of Bora et al. (2017) require access to ground truth from a massive
number of similar images (tens of thousands for CelebA). If such a large dataset is available, and
if a good generative model can be trained on that dataset, we expect that pre-trained models would
outperform our method. Our approach is instead more suitable for reconstructing problems where
large amounts of data or good generative models are not readily available.
4	Theoretical Results
In this section we provide theoretical evidence to highlight the importance of early stopping for DIP-
based approaches. Here we focus on denoising a noisy signal y ∈ Rn by optimizing over network
weights. This problem takes the form:
min L(W) := ky - G(z; W)k2.	(6)
w
This is a special instance of Eqn. 2 with the measurement matrix A = I corresponding to denoising.
We focus on generators consisting of a single hidden-layer ReLU network with k inputs, d hidden
units, and n outputs. Using W = (W, V ) the generator model in this case is given by
G(z; W,V) = V ∙ ReLU(Wz),	(7)
where z ∈ Rk is the input, W ∈ Rd×k the input-to-hidden weights, and V ∈ Rn×d the hidden-
to-output weights. We assume V is fixed at random and train over W using gradient descent. With
these formulations in place, we are now ready to state our theoretical result.
Theorem 4.1. Consider fitting a generator of the form W → G(z; W, V) = V ∙ ReLU (Wz) to a
signal y ∈ Rn with z ∈ Rk, W ∈ Rd×k, V ∈ Rn×d, and ReLU (z) = max(0, z). Furthermore,
assume V is a random matrix with i.i.d. N(0, ν2) entries with ν
√= kɪk. Startingfrom an initial
weight matrix W0 selected at random with i.i.d. N(0, 1) entries, we run gradient descent updates of
theform WT+1 = WT — ηνL(Wτ) on the loss
L(W) = 2 kV ∙ ReLU(Wz)—yk2,
5
Under review as a conference paper at ICLR 2020
with step size η = ^^ 48nd where η ≤ L Assuming that
d ≥ Cn,
with C a fixed numerical constant, then
kv ∙ ReLU(WTz)-yk≤ 3(1-
8(4n + d)
holdsfor all T with probability at least 1 一 5e-n/2 — e-d/2 一 e-4d 3 n 3.
Our theoretical result shows that after many iterative updates, gradient descent will solve this non-
convex optimization problem and fit any signal y, if the generator network is sufficiently wide. This
occurs as soon as the number of hidden units d exceeds the signal size n by a constant factor. In our
theorem we have focused on the case where the measurement matrix is the identity (A = I). We
note however that our theorem directly applies to many compressed sensing measurement matrices,
in particular any matrix obtained by subsampling the rows of an orthonormal matrix (e.g. sub-
sampling a Fourier matrix). This is possible as for any such matrix, AV has the same distribution
as a Gaussian matrix with i.i.d. entries. Therefore, our theorem directly applies in this setting
by replacing V with AV . This result demonstrates that early stopping is necessary for DIP-based
methods to be successful; otherwise the network can fit any signal, including one that is noisy.
Our proof builds on theoretical ideas from Oymak & Soltanolkotabi (2019) which provide a general
framework for establishing global convergence guarantees for overparameterized nonlinear learning
problems based on various properties of the Jacobian mapping along the gradient descent trajectory.
Related literature can be found in Du et al. (2018); Oymak & Soltanolkotabi (2018). We combine
tools from empirical process theory, random matrix theory, and matrix algebra to show that, starting
from a random initialization, the Jacobian mapping across all iterates has favorable properties with
high probability, hence facilitating convergence to a global optima.
5	Experiments
5.1	Experimental Setup
Measurements: We evaluate our algorithm using two different measurements processes, i.e. matri-
Ces A ∈ Rm×n. First We set the entries of A to be Gaussian i.i.d. such that Ai,j 〜 N(0, m). Recall
m is the number of measurements, and n is the number of pixels in the ground truth image. This
measurement process is standard practice in compressed sensing literature; hence We use it on each
dataset. Additionally We use a Fourier measurement process common in MRI applications (Mardani
et al., 2018; 2017a; Hammernik et al., 2018; Lehtinen et al., 2018; Lustig et al., 2008).
Datasets: We use our algorithm to reconstruct both grayscale and RGB images. For grayscale We
use the first 100 images in the test set of MNIST (LeCun et al., 1998) and also 60 random images
from the Shenzhen Chest X-Ray Dataset (Jaeger et al., 2014), doWnsampling a 512x512 crop to
256x256 pixels. For RGB We use retinopathy images from the STARE dataset (Hoover et al., 2000)
With 512x512 crops doWnsized to 128x128 pixels.
Table 1: Evaluating the benefits of learned regularization (LR) on x-ray images With varying levels
of noise and number of measurements. Table values are percent decrease in error, e.g. at ση2 = 0
and m = 500, LR reduces MSE by 9.9%. The term ση2 corresponds to variance of the noise vector
σ2
η in Eqn. 1, i.e. each entry of η is drawn independently N(0, m). These results indicate that LR
tends to provide greater benefit With noisy signals and With feWer measurements.
Measurements, m
ση2	500	1000	2000	4000	8000
0	9.9%	2.9%	0.2%	2.0%	0.6%
10	11.6%	4.6%	4.5%	2.4%	1.0%
100	14.9%	19.2%	5.0%	3.9%	2.8%
1000	37.4%	30.6%	19.8%	3.0%	6.2%
6
Under review as a conference paper at ICLR 2020
(a) MSE - Chest X-ray (65536 pixels)
(b) MSE - MNIST (784 pixels)
Figure 1: Per-pixel reconstruction error (MSE) vs. number of measurements. Vertical bars indicate
95% confidence intervals. BM3D-AMP frequently fails to converge for fewer than 4000 measure-
ments on x-ray images, as denoted by error values far above the vertical axis.
Baselines: We compare our algorithm to state-of-the-art unlearned methods such as BM3D-
AMP (Metzler et al., 2016; 2015), TVAL3 (Li, 2011; Li et al., 2009; Zhang et al., 2013), and Lasso
in a DCT basis (Ahmed et al., 1974). We also evaluated the performance of Lasso in a Daubechies
wavelet basis (Daubechies, 1988; Wasilewski, 2010) but found this performed worse than Lasso -
DCT on all datasets. Thus hereon we refer to Lasso - DCT as “Lasso” and do not include results of
Lasso - Wavelet.
Metrics: To quantitatively evaluate the performance of our algorithm, we use per-pixel mean-
squared error (MSE) between the reconstruction X and true image x*, i.e. 此-X k . Note that
because these pixels are over the range [-1, 1], it’s possible for MSE to be greater than 1.
Implementation: To find a set of weights w* that minimize Eqn. 3, we use PyTorch (Paszke et al.,
2017) with a DCGAN architecture. For baselines BM3D-AMP and TVAL3, we use the repositories
provided by the authors Metzler et al. (2018) and Li et al., respectively. For baseline reconstructions
Lasso, we use scikit-learn (Pedregosa et al., 2011). Section A in the appendix provides further details
on our experimental procedures, e.g. choosing hyperparameters. The supplementary material also
contains our code repository for these experiments.
5.2	Experimental Results
5.2.1	Results: Learned Regularization
We first evaluate the benefits of learned regularization by comparing our algorithm with and without
learned regularization, i.e. λL = 100 and λL = 0, respectively. The latter setting is an unlearned
method, as We are not leveraging (μ, Σ) from a specific dataset. In the former setting We first learn
(μ, Σ) from a particular set of x-ray images; we then evaluate on a different set of x-ray images. We
compare these tWo settings With varying noise and across different number of measurements.
Our results in Table 1 show that learned regularization does indeed provide benefit. This bene-
fit tends to increase with more noise or fewer measurements. Thus we can infer that assuming a
learned Gaussian distribution over weights is useful, especially when the original signal is noisy or
significantly compressed.
5.2.2	Results: Unlearned CS-DIP
For the remainder of this section, we evaluate our algorithm in the noiseless case without learned
regularization, i.e. when η = 0 in Eqn. 1 and λL = 0 in Eqn. 3. Hence CS-DIP is completely
unlearned; as such, we compare it to other state-of-the-art unlearned algorithms on various datasets
and with different measurement matrices.
7
Under review as a conference paper at ICLR 2020
(a) Reconstructions - Chest X-ray	(b) Reconstructions - MNIST
Figure 2: Reconstruction results on x-ray images for m = 2000 measurements (of n = 65536 pixels)
and MNIST for m = 75 measurements (of n = 784 pixels). From top to bottom row: original
image, reconstructions by our algorithm, then reconstructions by baselines BM3D-AMP, TVAL3,
and Lasso. For x-ray images the number of measurements obtained are 3% the number of pixels
(i.e. mn = .03), for which BM3D-AMP often fails to converge.
MNIST: In Figure 1b we plot reconstruction error with varying number of measurements m of n
= 784. This demonstrates that our algorithm outperforms baselines in almost all cases. Figure 2b
shows reconstructions for 75 measurements, while remaining reconstructions are in the appendix.
Chest X-Rays: In Figure 1a we plot reconstruction error with varying number of measurements m
of n = 65536. Figure 2a shows reconstructions for 2000 measurements; remaining reconstructions
are in the appendix. On this dataset we outperform all baselines except BM3D-AMP for higher m.
However for lower m, e.g. When the ratio £ ≤ 3%, BM3D-AMP often doesn't converge. This
finding seems to support the work of Metzler et al. (2015): BM3D-AMP performs well on higher
m, e.g. mn ≥ 10%, but recovery at lower sampling rates is not demonstrated.
Retinopathy: We plot reconstruction error with varying number of measurements m of n = 49152
in Figure 3a of the appendix. On this RGB dataset we quantitatively outperform all baselines except
BM3D-AMP on higher m; however, even at these higher m, patches of green and purple pixels
corrupt the image reconstructions as seen in Figure 10. Similar to x-ray for lower m, BM3D-AMP
often fails to produce anything sensible. All retinopathy reconstructions are located in the appendix.
Fourier Measurement Process: All previous experiments used a measurement matrix A containing
Gaussian i.i.d. entries. We now consider the case where the measurement matrix is a subsampled
Fourier matrix, as discussed in Section A of the appendix. Our algorithm outperforms baselines on
the x-ray dataset, as shown in the appendix Figure 3b.
Additional Experiments, Appendix A: In Figure 4a we compare our algorithm against the learned
approach of Bora et al. (2017). In Figure 4b we compare our algorithm against baselines in the
presence of noise, i.e. when η 6= 0 in Eqn. 1. We also demonstrate runtimes for all algorithms on
the x-ray dataset in Table 2.
6	Conclusion
We demonstrate how Deep Image Prior (DIP) can be generalized to solve any differentiable linear
inverse problem. We further propose a learned regularization method which, with a small amount of
prior information, reduces reconstruction error for noisy or compressed measurements. Lastly we
prove that the DIP optimization technique can fit any signal given a sufficiently wide single-layer
network. This provides theoretical justification for regularization methods such as early stopping.
8
Under review as a conference paper at ICLR 2020
References
Alekh Agarwal, Sahand Negahban, and Martin J Wainwright. Fast global convergence rates of
gradient methods for high-dimensional statistical recovery. In Advances in Neural Information
Processing Systems, pp. 37-45, 2010.
Nasir Ahmed, T_ Natarajan, and Kamisetty R Rao. Discrete cosine transform. IEEE transactions
on Computers, 100(1):90-93, 1974.
Muhammad Asim, Fahad Shamshad, and Ali Ahmed. Solving bilinear inverse problems using deep
generative priors. CoRR, abs/1802.04073, 2018. URL http://arxiv.org/abs/1802.
04073.
Francis Bach, Rodolphe Jenatton, Julien Mairal, Guillaume Obozinski, et al. Optimization with
sparsity-inducing penalties. Foundations and TrendsR in Machine Learning, 4(1):1-106, 2012.
Richard G Baraniuk, Volkan Cevher, Marco F Duarte, and Chinmay Hegde. Model-based compres-
sive sensing. IEEE Transactions on Information Theory, 56(4):1982-2001, 2010.
Peter J Bickel, Yaacov Ritov, Alexandre B Tsybakov, et al. Simultaneous analysis of lasso and
dantzig selector. The Annals of Statistics, 37(4):1705-1732, 2009.
Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis. Compressed sensing using genera-
tive models. arXiv preprint arXiv:1703.03208, 2017.
Emmanuel J Candes and Terence Tao. Decoding by linear programming. IEEE transactions on
information theory, 51(12):4203-4215, 2005.
Emmanuel J Candes, Justin Romberg, and Terence Tao. Robust uncertainty principles: Exact signal
reconstruction from highly incomplete frequency information. IEEE Transactions on information
theory, 52(2):489-509, 2006.
Emmanuel J Candes, Justin K Romberg, and Terence Tao. Stable signal recovery from incomplete
and inaccurate measurements. Communications on pure and applied mathematics, 59(8):1207-
1223, 2006.
Stanley H Chan, Xiran Wang, and Omar A Elgendy. Plug-and-play admm for image restoration:
Fixed-point convergence and applications. IEEE Transactions on Computational Imaging, 3(1):
84-98, 2017.
Jen-Hao Rick Chang, Chun-Liang Li, Barnabas Poczos, B. V. K. Vijaya Kumar, and Aswin C.
Sankaranarayanan. One network to solve them all - solving linear inverse problems using deep
projection models. CoRR, abs/1703.09912, 2017.
Guang-Hong Chen, Jie Tang, and Shuai Leng. Prior image constrained compressed sensing (piccs):
a method to accurately reconstruct dynamic ct images from highly undersampled projection data
sets. Medical physics, 35(2):660-663, 2008.
Ingrid Daubechies. Orthonormal bases of compactly supported wavelets. Communications on pure
and applied mathematics, 41(7):909-996, 1988.
Akshat Dave, Anil Kumar Vadathya, Ramana Subramanyam, Rahul Baburajan, and Kaushik Mitra.
Solving inverse computational imaging problems using deep pixel-level prior. arXiv preprint
arXiv:1802.09850, 2018.
Manik Dhar, Aditya Grover, and Stefano Ermon. Modeling sparse deviations for compressed sensing
using generative models. arXiv preprint arXiv:1807.01442, 2018.
Soren Dittmer, Tobias Kluth, Peter Maass, and Daniel Otero Baguer. Regularization by architecture:
A deep prior approach for inverse problems. arXiv preprint arXiv:1812.03889, 2018.
David L Donoho. Compressed sensing. IEEE Transactions on info theory, 52(4):1289-1306, 2006.
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. arXiv preprint arXiv:1810.02054, 2018.
9
Under review as a conference paper at ICLR 2020
Marco F Duarte, Mark A Davenport, Dharmpal Takhar, Jason N Laska, Ting Sun, Kevin F Kelly,
and Richard G Baraniuk. Single-pixel imaging via compressive sampling. IEEE signal processing
magazine, 25(2):83-91, 2008.
Armin Eftekhari and Michael B Wakin. New analysis of manifold embeddings and signal recovery
from compressive measurements. Applied and Computational Harmonic Analysis, 39(1):67-109,
2015.
Ender M Eksioglu and A Korhan Tanc. Denoising amp for mri reconstruction: Bm3d-amp-mri.
SIAM Journal on Imaging Sciences, 11(3):2090-2109, 2018.
Alyson K Fletcher and Sundeep Rangan. Inference in deep networks in high dimensions. arXiv
preprint arXiv:1706.06549, 2017.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NeurIPS, pp. 2672-2680,
2014.
Aditya Grover and Stefano Ermon. Amortized variational compressive sensing. ICLR Workshop,
2018a.
Aditya Grover and Stefano Ermon. Uncertainty autoencoders: Learning compressed representations
via variational information maximization. arXiv preprint arXiv:1812.10539, 2018b.
Kerstin Hammernik, Teresa Klatzer, Erich Kobler, Michael P Recht, Daniel K Sodickson, Thomas
Pock, and Florian Knoll. Learning a variational network for reconstruction of accelerated mri
data. Magnetic resonance in medicine, 79(6):3055-3071, 2018.
Paul Hand and Vladislav Voroninski. Global guarantees for enforcing deep generative priors by
empirical risk. arXiv preprint arXiv:1705.07576, 2017.
Paul Hand, Oscar Leong, and Vlad Voroninski. Phase retrieval under a generative prior. In Advances
in Neural Information Processing Systems, pp. 9154-9164, 2018.
Reinhard Heckel, Wen Huang, Paul Hand, and Vladislav Voroninski. Deep denoising: Rate-optimal
recovery of structured signals with a deep prior. arXiv preprint arXiv:1805.08855, 2018.
Chinmay Hegde and Richard G Baraniuk. Signal recovery on incoherent manifolds. IEEE Transac-
tions on Information Theory, 58(12):7204-7214, 2012.
Chinmay Hegde, Michael Wakin, and Richard Baraniuk. Random projections for manifold learning.
In Advances in neural information processing systems, pp. 641-648, 2008.
AD Hoover, Valentina Kouznetsova, and Michael Goldbaum. Locating blood vessels in retinal im-
ages by piecewise threshold probing of a matched filter response. IEEE Transactions on Medical
imaging, 19(3):203-210, 2000.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Stefan Jaeger, Sema Candemir, Sameer Antani, Yl-Xiang J Wang, PU-XUan Lu, and George Thoma.
Two public chest x-ray datasets for computer-aided screening of pulmonary diseases. Quantitative
imaging in medicine and surgery, 4(6):475, 2014.
Maya Kabkab, PoUya SamangoUei, and Rama Chellappa. Task-aware compressed sensing with
generative adversarial networks. arXiv preprint arXiv:1802.01284, 2018.
Diederik P Kingma and Max Welling. AUto-encoding variational bayes. preprint arXiv:1312.6114,
2013.
Alex Krizhevsky, Ilya SUtskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lUtional neUral networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
10
Under review as a conference paper at ICLR 2020
Yann LeCun, Leon Bottou, YoshUa Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324,1998.
Jaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, Samuli Laine, Tero Karras, Miika Ait-
tala, and Timo Aila. Noise2noise: Learning image restoration without clean data. preprint
arXiv:1803.04189, 2018.
Chengbo Li. Compressive sensing for 3d data processing tasks: applications, models and algorithms.
Technical report, Rice University, 2011.
Chengbo Li, Wotao Yin, and Yin Zhang. Tval3: Tv minimization by augmented lagrangian and al-
ternating direction algorithms. https://www.caam.rice.edu/~optimization/L1/
TVAL3/.
Chengbo Li, Wotao Yin, and Yin Zhang. Users guide for tval3: Tv minimization by augmented
lagrangian and alternating direction algorithms. CAAM report, 20(46-47):4, 2009.
Jiaming Liu, Yu Sun, Xiaojian Xu, and Ulugbek S Kamilov. Image restoration using total variation
regularized deep image prior. arXiv preprint arXiv:1810.12864, 2018.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.
In Proceedings of International Conference on Computer Vision (ICCV), 2015.
Po-Ling Loh and Martin J Wainwright. High-dimensional regression with noisy and missing data:
Provable guarantees with non-convexity. In NeurIPS, pp. 2726-2734, 2011.
Michael Lustig, David Donoho, and John M Pauly. Sparse mri: The application of compressed
sensing for rapid mr imaging. Magnetic resonance in medicine, 58(6):1182-1195, 2007.
Michael Lustig, David L Donoho, Juan M Santos, and John M Pauly. Compressed sensing mri.
IEEE signal processing magazine, 25(2):72-82, 2008.
Fangchang Ma, Ulas Ayaz, and Sertac Karaman. Invertibility of convolutional generative networks
from partial measurements. In Advances in Neural Information Processing Systems, pp. 9628-
9637, 2018.
Morteza Mardani, Enhao Gong, Joseph Y Cheng, Shreyas Vasanawala, Greg Zaharchuk, Marcus
Alley, Neil Thakur, Song Han, William Dally, John M Pauly, et al. Deep generative adversarial
networks for compressed sensing automates mri. arXiv preprint arXiv:1706.00051, 2017a.
Morteza Mardani, Hatef Monajemi, Vardan Papyan, Shreyas Vasanawala, David Donoho, and John
Pauly. Recurrent generative adversarial networks for proximal learning and automated compres-
sive image recovery. arXiv preprint arXiv:1711.10046, 2017b.
Morteza Mardani, Qingyun Sun, Shreyas Vasawanala, Vardan Papyan, Hatef Monajemi, John Pauly,
and David Donoho. Neural proximal gradient descent for compressive imaging. arXiv preprint
arXiv:1806.03963, 2018.
Chris Metzler, Ali Mousavi, and Richard Baraniuk. Learned d-amp: Principled neural network
based compressive image recovery. In NeurIPS, pp. 1772-1783, 2017.
Chris Metzler et al. D-amp toolbox. https://github.com/ricedsp/D-AMP_Toolbox,
2018.
Christopher A Metzler, Arian Maleki, and Richard G Baraniuk. Bm3d-amp: A new image recov-
ery algorithm based on bm3d denoising. In Image Processing (ICIP), 2015 IEEE International
Conference on, pp. 3116-3120. IEEE, 2015.
Christopher A Metzler, Arian Maleki, and Richard G Baraniuk. From denoising to compressed
sensing. IEEE Transactions on Information Theory, 62(9):5117-5144, 2016.
Dustin G Mixon and Soledad Villar. Sunlayer: Stable denoising with generative networks. arXiv
preprint arXiv:1803.09319, 2018.
11
Under review as a conference paper at ICLR 2020
Sahand Negahban, Bin Yu, Martin J Wainwright, and Pradeep K Ravikumar. A unified framework
for high-dimensional analysis of m-estimators with decomposable regularizers. In Advances in
Neural Information Processing Systems,pp. 1348-1356, 2009.
Dong Nie, Roger Trullo, Jun Lian, Caroline Petitjean, Su Ruan, Qian Wang, and Dinggang Shen.
Medical image synthesis with context-aware generative adversarial networks. In International
Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 417-425.
Springer, 2017.
Roberto Imbuzeiro Oliveira. The lower tail of random quadratic forms, with applications to ordinary
least squares and restricted eigenvalue properties. arXiv preprint arXiv:1312.2903, 2013.
Samet Oymak and Mahdi Soltanolkotabi. Overparameterized nonlinear learning: Gradient descent
takes the shortest path? ., 12 2018. URL https://arxiv.org/pdf/1812.10004.
Samet Oymak and Mahdi Soltanolkotabi. Towards moderate overparameterization: global conver-
gence guarantees for training shallow neural networks. arXiv preprint arXiv:1902.04674, 2019.
Parthe Pandit, Mojtaba Sahraee, Sundeep Rangan, and Alyson K Fletcher. Asymptotics of map
inference in deep networks. arXiv preprint arXiv:1903.01293, 2019.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
pytorch. Open Review, 2017.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research,
12:2825-2830, 2011.
Saad Qaisar, Rana Muhammad Bilal, Wafa Iqbal, Muqaddas Naureen, and Sungyoung Lee. Com-
pressive sensing: From theory to applications. Journal of Communications and networks, 15(5):
443-456, 2013.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Yaniv Romano, Michael Elad, and Peyman Milanfar. The little engine that could: Regularization by
denoising (red). SIAM Journal on Imaging Sciences, 10(4):1804-1844, 2017.
Leonid I Rudin, Stanley Osher, and Emad Fatemi. Nonlinear total variation based noise removal
algorithms. Physica D: nonlinear phenomena, 60(1-4):259-268, 1992.
Andrei A Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon Osin-
dero, and Raia Hadsell. Meta-learning with latent embedding optimization. arXiv preprint
arXiv:1807.05960, 2018.
Thomas SchlegL Philipp Seebock, Sebastian M Waldstein, Ursula Schmidt-Erfurth, and Georg
Langs. Unsupervised anomaly detection with generative adversarial networks to guide marker
discovery. In International Conference on Information Processing in Medical Imaging, pp. 146-
157. Springer, 2017.
Jo Schlemper, Jose Caballero, Joseph V Hajnal, Anthony N Price, and Daniel Rueckert. A deep cas-
cade of convolutional neural networks for dynamic mr image reconstruction. IEEE transactions
on Medical Imaging, 37(2):491-503, 2017.
Philip Schniter, Sundeep Rangan, and Alyson K Fletcher. Vector approximate message passing for
the generalized linear model. In ACSSC, pp. 1525-1529. IEEE, 2016.
Viraj Shah and Chinmay Hegde. Solving linear inverse problems using gan priors: An algorithm
with provable guarantees. arXiv preprint arXiv:1802.08406, 2018.
Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical
Society. Series B (Methodological), pp. 267-288, 1996.
12
Under review as a conference paper at ICLR 2020
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26-
31, 2012.
Subarna Tripathi, Zachary C Lipton, and Truong Q Nguyen. Correction by projection: Denoising
images with generative adversarial networks. arXiv preprint arXiv:1803.04477, 2018.
Joel A Tropp. Just relax: Convex programming methods for identifying sparse signals in noise.
IEEE transactions on information theory, 52(3):1030-1051, 2006.
Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Deep image prior. arXiv preprint
arXiv:1711.10925, 2017.
Aaron Van Den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves,
Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for
raw audio. arXiv preprint arXiv:1609.03499, 2016.
Singanallur V Venkatakrishnan, Charles A Bouman, and Brendt Wohlberg. Plug-and-play priors for
model based reconstruction. In GlobalSIP, 2013 IEEE, pp. 945-948. IEEE, 2013.
Yilun Wang, Junfeng Yang, Wotao Yin, and Yin Zhang. A new alternating minimization algorithm
for total variation image reconstruction. SIAM Journal on Imaging Sciences, 1(3):248-272, 2008.
F Wasilewski. Pywavelets: Discrete wavelet transform in python, 2010.
David W Winters, Barry D Van Veen, and Susan C Hagness. A sparsity regularization approach to
the electromagnetic inverse scattering problem. IEEE transactions on antennas and propagation,
58(1):145-154, 2010.
Jelmer M WoIterink, Tim Leiner, Max A Viergever, and Ivana Isgum. Generative adversarial net-
works for noise reduction in low-dose ct. IEEE transactions on medical imaging, 36(12):2536-
2545, 2017.
Jian Zhang, Shaohui Liu, Ruiqin Xiong, Siwei Ma, and Debin Zhao. Improved total variation based
image compressive sensing recovery by nonlocal regularization. In Circuits and Systems (ISCAS),
2013 IEEE International Symposium on, pp. 2836-2839. IEEE, 2013.
13
Under review as a conference paper at ICLR 2020
A Additional Experiments, Details, and Insights
Hyperparameters: Our algorithm CS-DIP is implemented in PyTorch using the RMSProp opti-
mizer (Tieleman & Hinton, 2012) with learning rate 10-3, momentum 0.9, and 1000 update steps
for every set of measurements. These parameters are the same across all datasets. We initialize one
random measurement matrix A for each image in all experiments. Further we set the TV hyperpa-
rameter λT = 0.01, and found that high frequency components of the image were not reconstructed
as sharply with λT = 0.
Dataset-specific design choices: On larger images such as x-ray (n = 65536) and retinopathy
(n = 49152), we found no difference using random restarts of the initial seed z. However for smaller
vectors such as MNIST (n = 784), restarts did provide some benefit. As such our experiments utilize
5 random restarts for MNIST and one initial seed (no restarts) for x-ray and retinopathy images. For
choosing hyperparameter λL in Eqn. 3, we used a standard grid search and selected the best one.
We used a similar grid search procedure for choosing dataset-specific hyperparameters in baseline
algorithms BM3D-AMP, TVAL3, and Lasso.
Learned Regularization: For learned regularization, 10 x-ray images are used to learn μ and Σ.
The results are then evaluated, i.e. averaged over x*, on 50 additional x-ray images.
Network Architecture: Our network has depth 7 and uses convolutional layers with ReLU activa-
tions. The initial seed z in Eqn. 3 is initialized with random Gaussian i.i.d. entries and then held
fixed as we optimize over network weights w. We found negligible difference when varying the
dimension of z (within reason), as this only affects the number of channels in the network’s first
layer. Hence we set the dimension of z to be 128, a standard choice for DCGAN architectures.
Stopping Criterion: We stop after 1000 iterations in all experiments. We did not optimize this
hyperparameter, so admittedly there could be room for improvement. We further note that the “Error
vs. Iterations” curve of CS-DIP with RMSProp did not monotonically decrease for some learning
rates, even though error gradually decreased in all cases. As such we implemented a stopping
condition which chooses the reconstruction with least error over the last 20 iterations. Note we
choose this reconstruction based off measurement loss and do not look at the ground truth image.
Fourier Sampling: The measurements are obtained by sampling Fourier coefficients along radial
lines as demonstrated in Figure 13. For a 2D image X and a set of indices Ω, the measurements
We receive are given by y(ij)= [F(x)](i,j), (i,j) ∈ Ω, where F is the 2D Fourier transform. We
choose Ω to be indices along radial lines, as shown in Figure 13 of the appendix; this choice of Ω
is common in literature (CandeS et al., 2006) and MRI applications (Mardani et al., 2017a; Lustig
et al., 2008; Eksioglu & Tanc, 2018). While Fourier subsampling is common in MRI applications,
we use it here on images of x-rays simply to demonstrate that our algorithm performs well with
different measurement processes.
We compare our algorithm to baselines on the x-ray dataset for {3, 5, 10, 20} radial lines in the
Fourier domain, which corresponds to {381, 634, 1260, 2500} Fourier coefficients, respectively. We
plot reconstruction error with varying number of Fourier coefficients in Figure 3b, outperforming
baselines. Reconstructions be found in Figure 14.
Table 2: Runtime (seconds) for each algorithm with varying number of measurements. Note that
our algorithm was run on a NVIDIA GTX 1080-Ti GPU, while baselines only leverage CPU. Thus
our goal here is not to compare runtimes; instead we demonstrate that our algorithm can run in a
reasonable amount of time, which is an issue with other DIP methods.
Algorithm	1000	2000	4000	8000
CS-DIP	15.6	17.1	20.4	29.9
BM3D-AMP	51.1	54.0	67.8	71.2
TVAL3	13.8	22.1	31.9	56.7
Lasso DCT	27.1	33.0	52.2	96.4
14
Under review as a conference paper at ICLR 2020
0.010
0 0.008
j
LU
O 0.006
4
U
n
⅞ 0.004
U
O
E 0.002
0.000
0.012
⅛ 0.010
J
L-
C 0.008
o
苫 0.006
匕
§ 0.004
U
2 0.002
0.000
OOOOO
if)	。	If)Olf)
IIoJoJ
Number of Fourier Coefficients
(a) MSE - Retinopathy (RGB) with Gaussian mea- (b) MSE - Chest X-ray with Fourier measurements
surements
Figure 3: Per-pixel reconstruction error (MSE) vs. number of measurements. Vertical bars indi-
cate 95% confidence intervals. Unfortunately an RGB version of TVAL3 does not currently exist,
although related TV algorithms such as FTVd perform similar denoising tasks (Wang et al., 2008).
(a)	Reconstruction error (MSE) on MNIST for
varying number of measurements. As expected, the
trained algorithm of Bora et al. (CSGM) outper-
forms our method for fewer measurements; how-
ever, CSGM saturates after 75 measurements, as its
output is constrained to the range of the generator.
This saturation is discussed in Bora et al., Section
6.1.1.
(b)	Reconstruction error (MSE) on x-ray images
for varying amounts of noise; number of mea-
surements m fixed at 2000. The term σ* 1 2 3 4 corre-
sponds to variance of the noise vector η in y =
Ax + η, i.e. each entry of η is drawn independently
N(0, m). Other baselines have error far above the
vertical axis and are thus not visible in this plot.
Figure 4
Algorithm 1 Estimate (μ, Σ) fora distribution of optimal network weights W *
Input: Set of optimal weights W * = {w*,w2,…，wQ} obtained from L-layer DCGAN run over
Q images; number of samples S; number of iterations T .
Output: Mean vector μ ∈ RL; covariance matrix Σ ∈ Rl×l.
1: for t = 1 to T do
2: Sample q uniformly from {1, ..., Q}
3: for l = 1 to L {for each layer} do
4:	Get v ∈ RS 6 7 8 9, a vector of S uniformly sampled weights from the lth layer of wq*
5:	Mt[l,:] — VT where Mt[l,:] is the lth row of matrix Mt ∈ Rl×s
6:	μt[l] J 1 PS=I Vi
7: end for
8:巳 J 1 MtMrT - μtμT
9: end for
10: μ J T Pt=1 μt
11: ς J T PT=I ς
15
Under review as a conference paper at ICLR 2020
B	Proof of Section 4: Theoretical Justification for Early
Stopping
In this section we prove our theoretical result in Theorem 4.1. We begin with a summary of some
notations we use throughout in Section B.1. Next, we state some preliminary calculations in Section
B.2. Then, we state a few key lemmas in Section B.3 with the proofs deferred to Appendix C.
Finally, we complete the proof of Theorem 4.1 in Section B.4.
B.1	Notation
In this section we gather some notation used throughout the proofs. We use φ(z) =ReLU(z) =
max(0, z) with φ0(z) = I{z≥0}. For two matrices/vectors x and y of the same size we use x y to
denote the entrywise Hadamard product of these two matrices/vectors. We also use X 0 y to denote
their Kronecker product. For two matrices B ∈ Rn×d1 and C ∈ Rn×d2 , we use the Khatrio-Rao
product as the matrix A = B * C ∈ Rn×d1d2 with rows Ai given by Ai = Bi 0 C For a matrix
M ∈ Rm×n we use vect(M) ∈ Rmn to denote a vector obtained by aggregating the rows of the
matrix M into a vector, i.e. vect(M) = [M1 M2 . . . Mm]T. For a matrix X we use σmin(X)
and kX k denotes the minimum singular value and spectral norm of X. Similarly, for a symetric
matrix M we use λmin(M) to denote its smallest eigenvalue.
B.2	Preliminaries
In this section we carryout some simple calculations yielding simple formulas for the gradient and
Jacobian mappings. We begin by noting we can rewrite the gradient descent iterations in the form
vect(WT +ι) = vect(WT) — ηvect(VL (WT)).
Here,
vect (VL (WT)) = JT(WT)r(WT),
where
∂
J(W) = ∂vect(W)f(W) and
is the Jacobian mapping associated to the network and
r(W) =φ(Vφ(Wz)) -y.
is the misfit or residual vector. Note that
t
∂vect (W) VTΦ (Wz) = [vιΦ0 (WTZ) XT v2Φ0 (WTZ) XT
= (v	φ0 (W x))T 0 xT
Thus
J (W) = (Vdiag (φ0(Wz))) * (IZT),
This in turn yields
J (W) JT (W) = (Vdiag (φ0(Wz)) diag (φ0(Wz)) VT) Θ (||z『11t)
= kzk2 Vdiag (φ0(Wz)	φ0(Wz))VT
(8)
B.3	Lemmas for controlling the spectrum of the Jacobian and initial misfit
In this section we state a few lemmas concerning the spectral properties of the Jacobian mapping,
its perturbation and initial misfit of the model with the proofs deferred to Appendix C.
16
Under review as a conference paper at ICLR 2020
Lemma B.1 (Minimum singular value of the Jacobian at initialization). Let V ∈ Rn×d and W ∈
Rd×k be random matrices with i.i.d. N(0, ν2) and N (0, 1) entries and define the Jacobian mapping
J (W) = (Vdiag (φ0(Wz))) * (IzT). Then as long as d ≥ 3828n,
σmin (J (W)) ≥ 1V √d Ilzll .
holds with probability at least 1 - 2e-n.
Lemma B.2 (Perturbation lemma). Let V ∈ Rn×d be a matrix with i.i.d. N(0,ν 2) entries, W ∈
Rd×k, and define the Jacobian mapping J (W) = (V diag (φ0(Wz))) * 1zT . Also let W0 be a
matrix with i.i.d. N(0, 1) entries. Then,
IJ(W)-J(W0)I ≤ν IzI
,_. 2	/ d
6 (2dR)3 log ( 3(2帆2
∖
holdsfor all W ∈ Rd×k obeying ∣∣W 一 Wo∣ ≤ R with probability at least 1 一 e-n/2 一 e 6-.
Lemma B.3 (Spectral norm of the Jacobian). Let V ∈ Rn×d be a matrix with i.i.d. N(0,ν 2) entries,
W ∈ Rd×k, and define the Jacobian mapping J (W) = (Vdiag (φ0(Wz))) * (IzT). Then,
IJ(W)k ≤ V (√d + 2√n)∣∣z∣∣,
holds for all W ∈ Rd×k with probability at least 1 一 e-n/2.
Lemma B.4 (Initial misfit). Let V ∈ Rn×d be a matrix with i.i.d. N(0, V2) entries with V
√= kZyk. Also let W ∈ Rd×k be a matrix with i.i.d. N(0,1) entries. Then
∣Vφ (Wz) 一 y∣ ≤ 3∣y∣ ,
holds with probability at least 1 一 e-n/2 一 e-d/2.
B.4	Proof of Theorem 4.1
Consider a nonlinear least-squares optimization problem of the form
m需 L(θ) ：=1 lf(θ)-y∣2,
θ∈Rp	2
with f : Rp 7→ Rn and y ∈ Rn . Suppose the Jacobian mapping associated with f obeys the
following three assumptions.
Assumption 1. Fix a point θ0. We have that σmin (J(θ0)) ≥ 2α.
Assumption 2. Let ∣ ∙ ∣ denote a norm that is dominated by the Euclidean norm i.e. ∣θ∣ ≤ ∣∣θ∣
holds for all θ ∈ Rp. Fix a point θ0 and a number R > 0. For any θ satisfying ∣θ 一 θ0 ∣ ≤ R, we
have that ∣∣J(θ0) — J(θ)∣ ≤ α∕3.
Assumption 3. For all θ ∈ Rp, we have that ∣J (θ)∣ ≤ β.
Under these assumptions we can state the following theorem from Oymak & Soltanolkotabi (2019).
Theorem B.5 (Non-smooth Overparameterized Optimization). Given θ0 ∈ Rp, suppose Assump-
tions 1, 2, and3 hold with
R =5 IIy — f(θ0)∣
α
Then, picking constant learning rate η ≤ e2, all gradient iterations obey thefollowings
2
IIy — f(θτ)l ≤ (1 — η4-)τ ∣y — f(θo)l	(9)
α
R∣θτ — θol + IIy — f(θτ)l ≤∣y — f(θo)∣.	(10)
5
17
Under review as a conference paper at ICLR 2020
We shall apply this theorem to the case where the parameter is W and the nonlinear mapping is
given by V φ (Wz) and φ = ReLU. All that is needed to be able to apply this theorem is check that
the assumptions hold. Per the assumptions of the theorem we use
=L kyk
=√dn 忖「
To this aim note that using Lemma B.1 Assumption 1 holds with
α = 4ν √d Ilzk = 4√n l∣yk,
with probability at least 1 - 2e-n . Furthermore, by Lemma B.3 Assumption 3 holds with
β = √yn√4n + d ≥ 1 (ʌ/ɪ +1) kyk = ν(√d +2√n) Ilzk.
with probability at least 1 - e-n/2 . All that remains for applying the theorem above is to verify
Assumption 2 holds with high probability
R = 60√n = 15kyk ≥ 5 kVφ (Wz) — yk
αα
In the above we have used Lemma B.4 to conclude that kVφ (Wz) - yk ≤ 3 kyk holds with proba-
bility at least 1- e-n/2 - e-d/2. Thus, using Lemma B.2 all that remains is to show that
√=j= kyk I 2√n + \ 6 (2dR) 3 ln \ -----2 ) J ≤ = = √ r- kyk ,
√dn	〈	∖∣	∖3(2dR)3"	3	12 √n
2	2]
holds With R = 60√n and with probability at least 1 一 e-n/2 一 e-弋)d 3 n 3 ≥ 1 一 e-n/2 一
2	1
e-4d3 n3. The latter is equivalent to
2√n + ʌ 6 (120d√n) 3 ln (------2] ≤ -pʒ-,
∖	∖3(120d√n)3)	12
which can be rewritten in the form
2√n + ʌ 6(120)2 r匕ln(—L—) ≤ ɪ,
V d ∖	Vd	"(120)2 3pn) 1 12,
which holds as long as d ≥ 4.3 × 1015n. Thus with d ≥ Cn then Assumptions 1, 2, and 3 holds with
probability at least 1 一 5e-n/2 一 e-d/2 一 e-4d3 n3. Thus, Theorem B.5 holds with high probability.
Applying Theorem B.5 completes the proof.
C	Proof of Lemmas for the Spectral Properties of the Jacobian
C.1 Proof of Lemma B.1
We prove the result for ν = 1, the general result follows from a simple re-scaling. Define the vectors
a` = V'φ0 (hw`,zi) ∈ Rn,
with v` the 'th column of V. Using equation 8 we have
J (W)JT (W) =kzk2Vdiag(φ0(Wz)φ0(Wz))VT,
=kzk2 (X a'aT),
=dkzk2 (d X a` aT).	(II)
To bound the minimum eigenvalue we state a result from Oliveira (2013).
18
Under review as a conference paper at ICLR 2020
Theorem C.1. Assume A1, . . . , Ad ∈ Rn×n are i.i.d. random positive semidefinite matrices whose
coordinates have bounded second moments. Define Σ := E[A1] (this is an entry-wise expectation)
and
1d
d X a'.
'=1
Let h ∈ (1, +∞) be such that E (uTA1u)2 ≤ huT Σu for all u ∈ Rn. Then for any δ ∈ (0, 1)
we have
p{∀u ∈ Rn : utΣkU ≥ (1 — 7h(n + 2)(2/^) UTΣu} ≥ 1 — δ
We shall apply this theorem with a` := a`a/. To do this We need to calculate the various parameters
in the theorem. We begin with Σ and note that for ReLU we have
Σ :=E[A1]
=Ea1 a1T
=Ew〜N(0,ik) [ (OO((W,zi))2 ]EV〜N(0,in)[vvT]
E
E
；w~N(0,ik) [ (φ0(wTz))2 ]In
，w~N(0,Ik) [I{wτz≥0}] In
=2 In.
To calculate h we have
Je[ (UT Aιu)2] ≤ 'e[ (aT u)4]
=√6 ∙ UTΣu.
Thus we can take h = √6. Therefore, using Theorem C.1 with δ = 2e-n we can conclude that
λmin (J X a'aT! ≥ 4
holds with probability at least 1 - 2e-n as long as
d ≥ 3528 ∙ n.
Plugging this into equation 11 we conclude that with probability at least 1 - 2e-n
σmiη (J(W)) ≥ 1 √d ∣∣z∣∣ ∙
C.2 Proof of Lemma B.2
We prove the result for ν = 1, the general result follows from a simple rescaling. Based on equa-
tion 8 we have
(J(W) -J(W0))(J(W) - J (W0))T = kzk2 V diag ((φ0(W z) - φ0(W0z))	(φ0(Wz) -φ0(W0z)))VT.
19
Under review as a conference paper at ICLR 2020
Thus
kJ(W)-J(W0)k ≤ kzk kV diag (φ0(W z) - φ0(W0z))k c	(12)
=kzk IIVdiag (I{Wz≥0} - I{Woz≥0}) Il
≤kzk VS(W),	(13)
where S(W ) ⊂ {1, 2, . . . , d} is the set of indices where Wz and W0z have different signs
i.e. S(W) := {' : sgn(eTWz) = sgn(eTWoz)} and VS(W) is a submatrix V obtained by Pick-
ing the columns corresponding to S(W).
To continue further note that by Gordon’s lemma we have
Sup ∣∣VSk ≤ √n + p2slog(d/s) + t,
∣s∣≤s
with probability at least 1 一 e-t2/2. In particular using t = √n We conclude that
Sup kVsk ≤ 2√n + p2slog(d/s),	(14)
ISI≤s
with probability at least 1 一 e-n/2 . To continue further we state a lemma controlling the size of
|S(W)| based on the size of the radius R.
Lemma C.2 (sign changes in local neighborhood). Let W0 ∈ Rd×k be a matrix with i.i.d. N (0, 1)
entries. Alsofora matrix W ∈ Rd×k define S (W ):= {' : Sgn (eT Wz) = Sgn (eT Wo z)}. Thenfor
any W ∈ Rd×k obeying kW 一 W0 k ≤ R
2
|S(W)| ≤ 2d(2dR)3e
holds with probability at least 1 一 e
2
(2dR)3
6
Combining equation 12 together with equation 14 (using S = 3 (2dR)3) and Lemma C.2 we con-
clude that
kJ(W) -J(Wo)k ≤ kzk	2√n+
holds with probability at least 1 - e-n/2 -
(2dR)
"6^
2
6 (2dR)3 log
d
2^
3(2dR)3
e

2
C.3 Proof of Lemma C.2
To prove this result we utilize two lemmas from Oymak & Soltanolkotabi (2019). In these lemmas
we use |v|m- to denote the mth smallest entry ofv after sorting its entries in terms of absolute value.
Lemma C.3. (Oymak & Soltanolkotabi, 2019, Lemma C.2) Given an integer m, suppose
kW - Wok≤√m JWIkm-,
then
|S (W )| ≤ 2m.
Lemma C.4. (Oymak & Soltanolkotabi, 2019, Lemma C.3) Let z ∈ Rk. Also let W0 ∈ Rd×k be a
matrix with i.i.d. N(0,1) entries. Then, with probability at least 1 — e-m6,
|Woz|m-	m
^^∏≥ 2d.
20
Under review as a conference paper at ICLR 2020
Combining the latter two lemmas with m = d(2dR)3 ] We conclude that when
kW-W0k ≤R
3
m 2
≤一
-2d
-m
≤ F 2d
∣Woz∣m-
≤√m
then with probability at least 1 - e
2
(2dR) 3
6
we have
2
∣S(W)∣ ≤ 2m ≤ 2d(2dR)3].
C.4 Proof of Lemma B.3
We prove the result for ν = 1, the general result follows from a simple rescaling. Using equation 8
we have
J (W)JT(W) =kzk2Vdiag(φ0(Wz)φ0(Wz))VT
Thus
kJ(W)k ≤kzkkVdiag(φ0(Wz))k
≤kzkkVk
The proof is complete by using standard concentration results for the spectral norm of a Gaussian
matrix that allow us to conclude that
IlV k ≤ √d+2√n,
holds with probability at least 1 - e-n/2 .
C.5 Proof of Lemma B.4
By the triangular inequality we have
IVφ(Wz)-yI ≤IVφ(Wz)I +IyI	(15)
To continue further let us consider one entry of Vφ (Wz) and note that it has the same distribution
as
Vφ (Wz)〜V kΦ(Wz)k g,
where g ∈ Rd is random Gaussian vectors with distribution g 〜N(0, Id). Thus
kVφ (Wz)k 〜V kφ(Wz)k kgk ≤ √2nν kφ(Wz)k ≤ √2nν IlWzk ,	(16)
with probability at least 1 - e-n/2. Furthermore, note that
Wz 〜 IzI ge,
where ge ∈ Rd is random Gaussian vectors with distribution ge 〜 N(0, Id). Combining the latter
with equation 16 we conclude that
l∣Vφ (Wz)k ≤ 2√ndν kzk = 2 kyk,
holds with probability at least 1-e-n/2-e-d/2. Combining the latter with equation 15 we conclude
that
IVφ (Wz) - yI ≤ 3 IyI ,
holds with probability at least 1 - e-n/2 - e-d/2 .
21
Under review as a conference paper at ICLR 2020
D Reconstructions
(a) 25 measurements
(b) 50 measurements
Figure 5: Reconstruction results on MNIST for m = 25, 50 measurements respectively (of n = 784
pixels). From top to bottom row: original image, reconstructions by our algorithm, then reconstruc-
tions by baselines BM3D-AMP, TVAL3, and Lasso.
22
Under review as a conference paper at ICLR 2020
<B.≡6-c0 SJno
牛一牛
3-3
ElVAL
3
4)1
(b) 200 measurements
(a) 100 measurements
Figure 6:	Reconstruction results on MNIST for m = 100, 200 measurements respectively (of n
= 784 pixels). From top to bottom row: original image, reconstructions by our algorithm, then
reconstructions by baselines BM3D-AMP, TVAL3, and Lasso.
・r寰B
!■■■■■
S l
■ ■ ■ ■
(a)	500 measurements

⅞ r * *; ∖
(b)	1000 measurements
Figure 7:	Reconstruction results on x-ray images for m = 500, 1000 measurements respectively (of
n = 65536 pixels). From top to bottom row: original image, reconstructions by our algorithm, then
reconstructions by baselines BM3D-AMP, TVAL3, and Lasso.
23
Under review as a conference paper at ICLR 2020
Iil

if!
a


OSSe-I
OSSe-I
(a)	4000 measurements
(b)	8000 measurements
Figure 8:	Reconstruction results on x-ray images for m = 4000, 8000 measurements respectively (of
n = 65536 pixels). From top to bottom row: original image, reconstructions by our algorithm, then
reconstructions by baselines BM3D-AMP, TVAL3, and Lasso.
(a) 500 measurements
Figure 9: Reconstruction results on retinopathy images for m = 500, 1000 measurements respec-
tively (of n = 49152 pixels). From top to bottom row: original image, reconstructions by our algo-
rithm, then reconstructions by baselines BM3D-AMP and Lasso.
(b) 1000 measurements
24
Under review as a conference paper at ICLR 2020
-eu 一 6一」0 s」no dW 阡 αmwG OSSe-I
Figure 10: Reconstruction results on retinopathy images for m = 2000 measurements (of n = 49152
pixels). From top to bottom row: original image, reconstructions by our algorithm, then recon-
structions by baselines BM3D-AMP and Lasso. In this case the number of measurements is much
smaller than the number of pixels (roughly 4% ratio), for which BM3D-AMP fails to converge, as
demonstrated by erroneous green and purple pixels. We recommend viewing in color.
25
Under review as a conference paper at ICLR 2020
Figure 11: Reconstruction results on retinopathy images for m = 4000 (of n = 49152 pixels). From
top to bottom row: original image, reconstructions by our algorithm, then reconstructions by base-
lines BM3D-AMP and Lasso.
26
Under review as a conference paper at ICLR 2020
Figure 12: Reconstruction results on retinopathy images for m = 8000 (of n = 49152 pixels). From
top to bottom row: original image, reconstructions by our algorithm, then reconstructions by base-
lines BM3D-AMP and Lasso.
27
Under review as a conference paper at ICLR 2020
Figure 13: A radial sampling pattern of coefficients Ω in the Fourier domain. The measurements are
obtained by sampling Fourier coefficients along these radial lines.
dwv—dmwm E1<Λ1
Figure 14: Reconstruction results on x-ray images for m = 1260 Fourier coefficients (of n = 65536
pixels). From top to bottom row: original image, reconstructions by our algorithm, then reconstruc-
tions by baselines BM3D-AMP and TVAL3.
28