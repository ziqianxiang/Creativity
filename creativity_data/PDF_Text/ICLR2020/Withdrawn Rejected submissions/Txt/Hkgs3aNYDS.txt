Quantum Expectation Maximization for Gaussian Mix-
ture Models
Anonymous authors
Paper under double-blind review
Ab stract
The Expectation-Maximization (EM) algorithm is a fundamental tool in unsupervised machine
learning. It is often used as an efficient way to solve Maximum Likelihood (ML) and Maximum
A Posteriori estimation problems, especially for models with latent variables. It is also the algo-
rithm of choice to fit mixture models: generative models that represent unlabelled points originating
from k different processes, as samples from k multivariate distributions. In this work we define and
use a quantum version of EM to fit a Gaussian Mixture Model. Given quantum access to a dataset
of n vectors of dimension d, our algorithm has convergence and precision guarantees similar to the
classical algorithm, but the runtime is only polylogarithmic in the number of elements in the training
set, and is polynomial in other parameters - as the dimension of the feature space, and the number of
components in the mixture. We generalize further the algorithm by fitting any mixture model of base
distributions in the exponential family. We discuss the performance of the algorithm on datasets that
are expected to be classified successfully by those algorithms, arguing that on those cases we can
give strong guarantees on the runtime.
1 Introduction
Over the last few years, the effort to find real world applications of quantum computers has greatly intensified.
Along with chemistry, material sciences, finance, one of the fields where quantum computers are expected to be most
beneficial is machine learning. A number of different algorithms have been proposed for quantum machine learning
(Biamonte et al., 2017; Wiebe et al., 2017; Kerenidis & Prakash, 2018; Harrow et al., 2θ09; SUbasI et al., 2019;
Farhi & Neven, 2018), both for the supervised and unsupervised setting, and despite the lack of large-scale quantum
compUters and qUantUm memory devises, some qUantUm algorithms have been demonstrated in proof-of-principle
experiments (Li et al., 2015; Otterbach et al., 2017; Jiang et al., 2019). Here, we look at Expectation-Maximization
(EM), a fUndamental algorithm in UnsUpervised learning, that can be Used to fit different mixtUre models and give
maximUm likelihood estimates with the so-called latent variable models. SUch generative models are one of the
most promising approaches for UnsUpervised problems. The goal of a generative model is to learn a probability
distribUtion that is most likely to have generated the data collected in a training set V ∈ Rn×d of n vectors of d
featUres. Fitting the model consists in learning the parameters ofa probability distribUtion p in a certain parameterized
family that best describes oUr vectors vi . We will see that, thanks to this formUlation, we can redUce a statistical
problem into an optimization problem Using maximUm likelihood estimation (ML) estimation. The likelihood is the
fUnction that we Use to measUre how good a model is for explaining a given dataset. For a given machine learning
model with parameters γ, the likelihood of oUr data set V is the probability that the data have been generated by the
model with parameters γ, assUming each point is independent and identically distribUted. We think the likelihood
as a function of Y, holding the dataset V fixed. For p(vi ∣γ) the probability that a point Vi comes from model Y,
the likelihood is defined as L(γ; V) := Qn=ιp(vi∣γ). From this formula, We can see that in order to find the best
parameters γ* of our model we need to solve an optimization problem. For numerical and analytical reasons, instead
of maximizing the likelihood L, it is common practice to find the best model by maximizing the log-likelihood
function '(γ; V) = log L(γ; V) = Pn=ι logp® ∣γ). In this context, we want to find the model that maximizes the
log-likelihood: YML := argmaxγ Pn=Ilogp(vi∣γ). The procedure to calculate the log-likelihood depends on the
specific model under consideration. A possible solution would be to use a gradient based optimization algorithm
on `. Unfortunately, due to the indented landscape of the function, gradient based techniques often do not perform
well. Therefore, it is common to solve the maximum likelihood estimation (or maximum a priori) problem using the
Expectation-Maximization (EM) algorithm. EM is an iterative algorithm which is guaranteed to converge to a (local)
1
optimum of the likelihood. This algorithm has a striking variety of applications, and has been successfully used for
medical imaging (Balafar et al., 2010), image restoration (Lagendijk et al., 1990), problems in computational biology
(Fan et al., 2010), and so on. EM has been proposed in different works by different authors, but has been formalized
as we know it only in 1977 (Dempster et al., 1977). For more details, we refer to (Lindsay, 1995; Bilmes et al., 1998).
In this work, we introduce Quantum Expectation-Maximization (QEM), a new algorithm for fitting mixture models.
We detail its usage in the context of Gaussian Mixture Models, and we extend the result to other distributions in the
exponential family. We also generalize the result by showing how to compute the MAP: the Maximum A Posteriori
estimate of a mixture model. MAP estimates can be seen as the Bayesian version of maximum likelihood estimation
problems. MAP estimates are often preferred over ML estimates, due to a reduced propensity to overfit. Our main
result can be stated as:
Result (Quantum Expectation-Maximization). (see Theorem 3.9) Fora data matrix V ∈ Rn×d stored in an appropri-
ate QRAM data structure andfor parameters δθ, δ* > 0, Quantum Expectation-Maximization (QEM) fits a Maximum
Likelihood (or a Maximum A Posteriori) estimate of a Gaussian Mixture Model with k components, in running time
per iteration which is dominated by:
d2k4∙5η3κ2(V )κ2(Σ)μ(Σ)
(1)
where Σ is a covariance matrix of a Gaussian distribution, η is a parameter of the dataset related to the maximum norm
of the vectors, δθ, δμ are error parameters in the QEM algorithm, μ(< √d) is a factor appearing in quantum linear
algebra and κ is the condition number of a matrix. Here we only kept the term in the running time that dominates
for the range of parameters of interest. In Theorem 3.9 we explicate the running time of each step of the algorithm.
The QEM algorithm runs for a number of iterations until a stopping condition is met (defined by a parameter τ > 0)
which implies a convergence to a (local) optimum.
Let’s have a first high-level comparison of this result with the standard classical algorithms. The runtime of a single
iteration in the standard implementation of the EM algorithm is at least O(knd2) (Pedregosa et al., 2011; Murphy,
2012). The advantage of the quantum algorithm is an exponential improvement with respect to the number of elements
in the training set, albeit with a worsening on other parameters. It is crucial to find datasets where such a quantum
algorithm can offer a speedup. For a reasonable range of parameters ( d= 40, k = 10, η = 10, δ = 0.5, κ(V) =
25, κ(Σ) = 5, μ(Σ) = 4) which is motivated by some experimental evidence reported in Section 4, datasets where
the number of samples in the order of O(1012) might be processed faster on a quantum computer. One should expect
that some of the parameters of the quantum algorithm can be improved, especially the dependence on the condition
numbers and the errors, which can make enlarge the type of datasets where QEM can offer an advantage. Note that
we expect the number of iterations of the quantum algorithm to be proportional to the number of iteration of the
classical case. This is to be expected since the convergence rate does not change, and it is corroborated by previous
experimental evidence in a similar scenario: the number of iterations needed by q-means algorithm for convergence,
is proportional to the number of iterations of the classical k-means algorithm (Kerenidis et al., 2018).
Expectation-Maximization is widely used for fitting mixture models in machine learning (Murphy, 2012). Most mix-
ture models use a base distribution in the exponential family: Poisson (Church & Gale, 1995), Binomial, Multinomial,
log-normal (Dexter & Tanner, 1972), exponential (Ghitany et al., 1994), Dirichlet multinomial (Yin & Wang, 2014),
and others. EM is also used to fit mixtures of experts, mixtures of the student T distribution (which does not belong to
the exponential family, and can be fitted with EM using (Liu & Rubin, 1995)) and for factor analysis, probit regression,
and learning Hidden Markov Models (Murphy, 2012).
1.1	Previous work
There is a fair amount of quantum algorithms that have been proposed in the context of unsupervised learning. (Almeur
et al., 2013; Lloyd et al., 2013b; Otterbach et al., 2017). Recently, classical machine learning algorithms were obtained
by “dequantizing” quantum machine learning algorithms (Tang, 2018b;a;c; GiIyen etal., 2018a; Chia etal., 2018). The
runtime of these classical algorithm is poly-logarithmic in the dimensions of the dataset. However, the polynomial
dependence on the rank, the error, and the condition number, make these new algorithms impractical on interesting
datasets, as shown experimentally by (Arrazola et al., 2019). Fast classical algorithm for GMM exists, albeit assuming
only one shared covariance matrix (Dasgupta, 1999), and without a polylogarithmic dependence in the number of
2
elements in the training set. Independently of us, Miyahara, Aihara, and Lechner extended the q-means algorithm
(Kerenidis et al., 2018) for Gaussian Mixture Models (Miyahara et al., 2019), using similar techniques. The main
difference is that in their work the update step is performed using a hard-clustering approach (as in the k-means
algorithm), that is for updating the centroid and the covariance matrices of a cluster j , only the data points for which
cluster j is nearest are taken into account. In our work, we use the soft clustering approach (as in the classical EM
algorithm), that is for updating the centroid and the covariance matrices of cluster j , all the data points weighted by
their responsibility for cluster j are taken into account. Both approaches have merits and can offer advantages (Kearns
et al., 1998).
2 Expectation-Maximization and Gaussian Mixture Models
As is common in machine learning literature, we introduce the Expectation-Maximization algorithm by using it to fit
Gaussian Mixture Models (GMM). Mixture models are a popular generative model in machine learning. The intuition
behind mixture models is to model complicated distributions by using a group of simpler (usually uni-modal) distri-
butions. In this setting, the purpose of the learner is to model the data by fitting the joint probability distribution which
most likely have generated our samples. In this section we describe GMM: probably the most used mixture model
used to solve unsupervised classification problems. In fact, given a sufficiently large number of mixture components, it
is possible to approximate any density defined in Rd (Murphy, 2012). In unsupervised settings, we are given a training
set of unlabeled vectors vι ∙∙∙ Vn ∈ Rd which We represent as rows of a matrix V ∈ Rn×d. Let yi ∈ [k] one of the k
possible labels for a point vi. We posit that the joint probability distribution of the data p(vi, yi) = p(vi|yi)p(yi), is
defined as follow: yi 〜 MUltinomial(θ) for θ ∈ Rk, and p(v∕yi = j)〜 N(μj, Σj). The θj are the mixing weights,
i.e. the probabilities that yi = j, and N(μj, Σj) is the Gaussian distribution centered in μj ∈ Rd with covariance
matrix Σj ∈ Rd×d . Note that the variables yi are unobserved, and thus are called latent variables. There is a simple
interpretation for this model. We assume the data is created by first selecting an index j ∈ [k] by sampling according
to Multinomial(θ), and then a vector vi is sampled from N(μj, Σj). Fitting a GMM to a dataset reduces to finding an
assignment for the parameters: Y = (θ, μ, Σ) = (θ, μι,…，μk, ∑ι,…，∑k) that best maximize the log-likelihood
for a given dataset. Note that the algorithm used to fit GMM can return a local minimum which might be different than
γ*: the model that represents the global optimum of the likelihood function. We use the letter φ to represent our base
distribution, which in this case is the probability density function of a multivariate Gaussian distribution N(μ, Σ).
Using this formulation, a GMM is expressed as: p(v) = Pjk=1 θjφ(v; μj, Σj) where θj are the mixing weights of the
multinomial distribution such that Pjk=1 θj = 1. The probability for an observation vi tobe assigned to the component
j is given by: ri7- = p；j。(：；烂：j∑力.This value is called responsibility, and corresponds to the posterior probability
of the sample i being as=signed label j by the current model. As anticipated, to find the best parameters of our gener-
ative model, we maximize the log-likelihood of the data. For GMM, the likelihood is given by the following formula
(Ng, 2012):'(y; V) = '(θ, μ, Σ; V) = Pn=ι logp(vi ； θ, μ, ∑) =. Alas, it is seldom possible to solve maximum
likelihood estimation analytically (i.e. by finding the zeroes of the derivatives of the log-like function, and this is one
of those cases. Fortunately, Expectation-Maximization is an iterative algorithm that solves numerically the optimiza-
tion problem of ML estimation. To complicate things, the likelihood function for GMM is not convex, and thus we
might find some local minima (Hastie et al., 2009). If we were to know the latent variable yi , then the log-likelihood
for GMM would be:f '(γ; V) = Pn=Ilogp(vi | yi； μ, ∑) + logp(yi； θ) This formula can be easily maximized with
respect to the parameters θ, μ, and Σ. In the Expectation step we calculate the missing variables yis, given a guess
of the parameters (θ, μ, Σ) of the model. Then, in the Maximization step, we use the estimate of the latent variables
obtained in the Expectation step to update the estimate of the parameters. While in the Expectation step we calculate a
lower bound on the likelihood, in the Maximization step we maximize it. Since at each iteration the likelihood can only
increase, the algorithm is guaranteed to converge, albeit possibly to a local optimum (see (Hastie et al., 2009) for the
proof). During the Expectation step all the responsibilities are calculated, while in the Maximization step we update
our estimate on the parameters Yt+1 = (θt+1, μt+1, Σt+1). The stopping criterion for GMM is usually a threshold
on the increment of the log-likelihood: if the log-likelihood changes less than a threshold between two iterations, then
the algorithm stops. Notice that, since the value of the log-likelihood significantly depends on the amount of data
points in the training sets, it is often preferable to adopt a scale-free stopping criterion, which does not depend on
the number of samples. For instance, in the toolkit scikit-learn (Pedregosa et al., 2011) the stopping criterion is given
by a tolerance on the average increment of the log-probability, which is chosen to be smaller than a certain τ , say
10-3. More precisely, the stopping criterion is |E[logp(vi; Yt)] - E[logp(vi; Yt+1)]| < τ which we can estimate as
|1 Pi=IlOg P(Vi； Yt)- n Pi=IlOg P(Vi； Yt+1)| < ET.
3
Dataset assumptions in GMM As in q-means (Kerenidis et al., 2018), we have an assumption on the dataset that
all elements of the mixture contribute proportionally to the total responsibility: i, e
n
fc≡=θ⑴
∀j, l ∈ [k]
This is equivalent to requiring that clusters share a comparable amount of points in the “well-clusterability” assumption
in q-means (Kerenidis et al., 2018). It is also equivalent to assuming that θj /θl = Θ(1) ∀j, l ∈ [k]. For convenience,
in this work, we also assume that the dataset is normalized such that the shortest vector has norm 1 and define η :=
maxi kvik2 to be the maximum norm squared of a vector in the dataset. This is not a necessary requirement for our
dataset, but it will simplify the analysis of our algorithm, allowing us to give strict bounds on the runtime.
Preliminaries We assume a basic understanding of quantum computing, we recommend Nielsen and Chuang
(Nielsen & Chuang, 2002) for an introduction to the subject. A vector state |vi for v ∈ Rd is defined as
∣v> = k⅛ Pj∈[d] Vjji，where ji represents ej, the jth vector in the standard basis. The dataset is represented
by a matrix V ∈ Rn×d, i.e. each row is a vector vi ∈ Rd for i ∈ [n] that represents a single data point. The cluster
centers, called centroids, at time t are stored in the matrix Ct ∈ Rk×d, such that the jth row ctj for j ∈ [k] represents
the centroid of the cluster Cj. We denote as V≥τ the matrix P'=0 σiuiVT where g` is the smallest singular value
which is greater than τ. With nnz(V ) we mean the number of non-zero elements of the rows of V . When we say
κ(V) we mean the condition number of the matrix V, that is the ratio between the biggest and the smallest (non-zero)
singular value. All the tools used in this work, like quantum algorithms for computing distances and linear algebraic
operations, are reported in the Supplementary Material section.
3 Quantum Expectation-Maximization for GMM
In this section, we present a quantum Expectation-Maximization algorithm to fit a GMM. The algorithm can also be
adapted fit other mixtures models where the probability distributions belong to the exponential family. As the GMM
is both intuitive and one of the most widely used mixture models, our results are presented for the GMM case.
A robust version of the EM algorithm Similar to the work of (Kerenidis et al., 2018), we define a ∆-robust version
of the EM algorithm which we use to fit a GMM. The difference between this formalization and the original EM for
GMM is simple. Here we explain the numerical error introduced in the training algorithm.
Let Yt = (θt, μt, Σt) = (θt,μ1 …μtk, ∑1 … ∑k) a model fitted by the standard EM algorithm from γ0 an initial
guess of the parameters, i.e. γt is the error-free model that standard EM would have returned after t iterations.
Starting from the same choice of initial parameters γ0, fitting a GMM with the QEM algorithm with ∆ = (δθ, δμ)
means returning a model Yt = (θt, μt, Σt) such that: Mt - θt∣∣ < δθ, that ∣∣μjt - μj ∣∣ < δμ for all j ∈ [k], and that
忤t - ∑j∣∣ ≤ δμ√η.
Quantum access to the mixture model As in the classical algorithm, we use some subroutines to compute the
responsibilities and update our current guess of the parameters. The classical algorithm has clearly two separate steps
for Expectation and Maximization. In contrast, the quantum algorithm uses a subroutine to compute the responsibilities
inside the step that performs the Maximization, that is the subroutines for computing responsibilities are called multiple
times during the quantum Maximization step. During the quantum Maximization step, the algorithm updates the model
parameters Yt by creating quantum states corresponding to parameters Yt+1 and then recovering classical estimates for
these parameters using quantum tomography or amplitude amplification. In order for this subroutines to be efficient,
the values of the GMM are stored in QRAM data structures and are updated following each maximization step.
Definition 1 (Quantum access to a GMM). We say that we have quantum access to a GMM if the dataset V ∈ Rn×d
and model parameters θj ∈ R, μj∙ ∈ Rd, Σj ∈ Rd×d for all j ∈ [k] are Stored in QRAM data structures which allow
us to perform in time O(polylog(d)) the following mappings:
•	|ji |0i → |ji ∣μji forall j ∈ [k],
•	|ji |i)|0)→ ∣ji∣ii∣σji for j ∈ [k], i ∈ [d] where σj is the i-th rows of Σj∙ ∈ Rd×d,
4
•	|ii |0i 7→ |ii |vii for all i ∈ [n],
•	|ii |0i |0i 7→ |ii |vec[viviT]i = |ii |vii |vii for all i ∈ [n],
∙ W |0i → ji∣θji∙
Algorithm 1 Quantum Expectation Maximization for GMM
Require: Quantum access to a GMM model, precision parameters δθ, δμ, and threshold ET.
Ensure: A GMM Yt that maximizes locally the likelihood '(γ; V), UP to tolerance ET.
1:	Use a heuristic described at the beginning of this section to determine an initial guess for γ0 = (θ0, μ0, Σ0), and
store these parameters in the QRAM.
2:	Use Lemma 3.1 to estimate the log determinant of the matrices {Σj0}jk=1.
3:	t=0
4:	repeat
5:	Step 1: Get an estimate of θt+1 such that ∣∣θt+1 - θt+11∣ ≤ δθ using Lemma 3.4.
6:	Step 2: Get an estimate {μjt+1 }k=ι by using Lemma 3.6 to estimate each ∣∣μj+1∣∣ and ∣μj+1i such that
∣∣μtj+1 - μjt+1∣∣ ≤ δμ.	一十]
7:	Step 3: Get an estimate {Σjt+1}k=ι by using Lemma 3.7 to estimate ∣∣∑j+1∣∣F and ∣∑j+1i such that
∣∣∑j+1- ∑7t+1∣∣ ≤ δμ√η.
8:	Step 4: Estimate E[p(vi； γt+1)] up to error ET/2 using Theorem 3.8.
9:	Step 5: Store γt+1 in the QRAM, and use Lemma 3.1 to estimate the determinants {log det(Σtj+1)}jk=0.
10:	t = t + 1
11:	until	 ________________________
|E[p(Vi； Yt)] — E[p(vi； Yt-1)]∣ < ET
12:	Return Yt = (θt, μt, Σt)
Quantum initialization strategies exists, and are described in the Appendix.
3.1	Expectation
In this step of the quantum algorithm we are just showing how to compute efficiently the responsibilities as a quantum
state. First, we compute the responsibilities in a quantum register, and then we show how to put them as amplitudes of
a quantum state. We start by a classical algorithm used to efficiently approximate the log-determinant of the covariance
matrices of the data. At each iteration of Quantum Expectation-Maximization we need to compute the determinant
of the updated covariance matrices, which is done thanks to Lemma 3.1. We will see from the error analysis that in
order to get an estimate of the GMM, we need to call Lemma 3.1 with precision for which the runtime of Lemma 3.1
gets subsumed by the running time of finding the updated covariance matrices through 3.7. Thus, we do not explicitly
write the time to compute the determinant from now on in the algorithm and when we say that we update Σ we include
an update on the estimate of log(det(Σ)) as well.
Lemma 3.1 (Determinant evaluation). There is an algorithm that, given as input a matrix Σ and a parameter 0 <
δ < 1, outputs an estimate log(det(Σ)) such that ∣log(det(Σ)) — log(det(Σ))∣ ≤ E with probability 1 一 δ in time:
TDet,e = O (E-2κ(Σ)log(1∕δ)nnz(Σ)∣ log(det(Σ))∣)
Now we can state the main brick used to compute the responsability: a quantum algorithm for evaluating the exponent
of a Gaussian distribution.
Lemma 3.2 (Quantum Gaussian Evaluation). Suppose we have stored in the QRAM a matrix V ∈ Rn×d, the centroid
μ ∈ Rd and the covariance matrix Σ ∈ Rd×d ofa multivariate Gaussian distribution φ(v∣μ, Σ), as well as an estimate
for log(det(Σ)). Then for E1 > 0, there exists a quantum algorithm that with probability 1 - Y performs the mapping,
5
• Ug,∈i : |i)|0)→ |i)|Sii such that ∣Si - si| < s, where Si = 一2((Vi 一 μ)TΣ-1(vi - μ) + dlog2π +
log(det(Σ))) is the exponent for the Gaussian probability density function.
The running time ofthe algorithm is Tg® = O (K 3)”(：)log(1/Y) j .
Using controlled operations it is simple to extend the previous Theorem to work with multiple Gaussians distributions
(μj, Σj). That is, We can control on a register |ji to do ji|i)|0i → ji∣i)∣φ(vi∣μj, Σj)i. In the next Lemma We
will see how to obtain the responsibilities rij using the previous Theorem and standard quantum circuits for doing
arithmetic, controlled rotations, and amplitude amplification. The Lemma is stated in a general Way, to be used With
any probability distributions that belong to an exponential family.
Lemma 3.3 (Calculating responsibilities). Suppose we have quantum access to a GMM with parameters γt =
(θt, μt, Σt). There are quantum algorithms that can:
1.	Perform the mapping |i)|ji|0)→ |i)|j)|rj i SUch that ∖rij 一 rj | ≤ ∈ι with probability 1 — Y in time:
TR1,e1 = O(k1.5 × TG,eι )
___	___ n
2.	For a given j ∈	[k],	ConstrUct state	|Rj〉such that	|Rj〉-----√1=	E	rj	|i)
Zj i=0
high probability in time:
TR2,®1 = O(k × TR1,®1 )
n
< 1 where Zj =	ri2j with
i=0
3.2	Maximization
NoW We need to get a neW estimate for the parameters of our model. This is the idea: at each iteration We recover
the neW parameters of the model as quantum states, and then recover it using tomography, amplitude estimation, or
sampling. Once the neW model has been recovered, We update the QRAM such that We get quantum access to the
model γt+1 . The possibility to estimate θ comes from a call to the unitary We built to compute the responsibilities,
and postselection.
Lemma 3.4 (Computing θt+1 ). We assume quantum access to a GMM with parameters γt and let δθ > 0 be a
precision parameter. There exists an algorithm that estimates θt+1 ∈ Rk such that llθt+1 — θt+1ll ≤ δθ in time
Tθ = O k3.5η
1.5 κ2(Σ)μ(Σ)

We use quantum linear algebra to transform the uniform superposition of responsibilities of the j-th mixture into the
neW centroid of the j-th Gaussian. Let Rjt ∈ Rn be the vector of responsibilities for a Gaussian j at iteration t. The
following claim relates the vectors Rj to the centroids μj+1.
Claim 3.5. Let Rjt ∈ Rn be the vector of responsibilities of the points for the Gaussian j at time t, i.e.
Then μj+1 ―
Pn=Irtj V
Pn=I rtj
V T Rj
nθj .
(Rjt )i = ritj .
The proof is straightforWard.
Lemma 3.6 (Computing μj+1). We assume we have quantum access to a GMM with parameters Yt. For a pre-
cision parameter δμ > 0, there is a quantum algorithm that calculates {μjt+1}k=ι such that for all j ∈ [k]
l∣μjt+1 一 μj+11∣ ≤ δμ in timeTμ = O (皿MV^(V-%〜1%2^)*^")
From the ability to calculate responsibility and indexing the centroids, We derive the ability to reconstruct the covari-
ance matrix of the Gaussians as Well. Again, We use quantum linear algebra subroutines and tomography to recover
an approximation of each Σj. Recall that We have defined the matrix V 0 ∈ Rn×d2 Where the i-th roW of V 0 is defined
as vec[vi viT]. For this Lemma, We assume to have the matrix stored in the QRAM. This is a reasonable assumption
as the quantum states corresponding to the roWs of V0 can be prepared as |ii |0i |0i → |ii |vii |vii, using tWice the
procedure for creating the roWs of V .
6
Lemma 3.7 (Computing Σtj+1). We assume we have quantum access to a GMM with parameters γt. We also
have computed estimates μjt+1 of all centroids such that ∣∣μjt+1 — μj+1∣∣ ≤ δμ for precision parameter δ* > 0.
Then, there exists a quantum algorithm that outputs estimates for the new CovarianCe matrices {∑j+1}k=ι SuCh that
M1 - EF ≤ δμ√η with high probability, in time,
n := O(kd2ηκ2(V)(μ(V0) + ηk3.5κ2(Σ)μ(Σ)))
3.3	Quantum estimation of log-likelihood
Now we are going to show how it is possible to get an estimate of the log-likelihood using a quantum procedure and
access to a GMM model. A good estimate is crucial, as it is used as stopping criteria for the quantum algorithm as
well. Classically, We stop to iterate the EM algorithm when ∣'(γt; V) - '(γt+1; V)| < ne, or equivalently, We can
set a tolerance on the average increase in log probability: ∣E[logp(vi； γt)] - E[logp(vi; Yt+1)]∣ < e. In the quantum
algorithm it is more practical to estimate E[p(vi； γt)] = n PNi p(vi； γ). From this we can estimate an upper bound
onthe log-likelihood as nlogE[p(vi)] = Pn=IlogE[p(vi)] ≥ Pn=IlogP(Vi) = '(γ; V).
Lemma 3.8 (Quantum estimation of likelihood). We assume we have quantum access to a GMM with parameters γt.
For eτ > 0, there exists a quantum algorithm that estimates E[p(vi; γt)] with absolute error eτ in time
T = O 卜 1.5η1.5 κ2(∑μ⑶)
Putting together all the previous Lemmas, we write the main result of the work.
Theorem 3.9 (QEM for GMM). We assume we have quantum access to a GMM with parameters γt. For parameters
δθ, δμ, €丁 > 0, the running time of one iteration of the Quantum Expectation-Maximization (QEM) algorithm is
O(Tθ + Tμ + T∑ + TH
for Tθ	=	O) (k3.5η1.5 κ2(∑2μ3)),
O) ( kd2ηκ2(V)(μ(V0)+η2k3.5κ2(Σ)μ(Σ)) ∖ and T
TU	= O (kdηK(V)(μ(v)+k3.5η1.5κ2(∑)μ(∑)))
O(k1.5η1.5 κ2(∑μ3))	μ
TΣ
For the range of parameters of interest, the running time is dominated by TΣ.
The proof follows directly from the previous lemmas. Note that the cost of the whole algorithm is given by repeating
the Estimation and the Maximization steps several times, until the threshold on the log-likelihood is reached. Note
also that the expression of the runtime can be simplified from the observation that the cost of performing tomography
on the covariance matrices Σj dominates the cost.
4	Experimental Results
In this section, we present the results of some experiments on real datasets to bound the condition number and the
other parameters of the runtime. Let's discuss the value of κ(Σ), K(V), μ(Σ), and μ(V). We can thresholding
the condition number by discarding small singular values of the matrix, as used in quantum linear algebra, might be
advantageous. This is indeed done often in classical machine learning models, since discarding the eigenvalues smaller
than a certain threshold might even improve upon the metric under consideration (i.e. often the accuracy), and is a
form of regularization (Murphy, 2012, Section 6.5). This is equivalent to limiting the eccentricity of the Gaussians. We
can have a similar consideration on the condition number of the dataset κ(V). As shown before, the condition number
of the matrix V0 appearing in Lemma 3.2 is κ2(V). Similarly, we can claim that the value of μ(V) will not increase
significantly as we add vectors to the training set. Remember that we have some choice in picking the function μ: in
previous experiments we have found that choosing the maximum '1 norm of the rows of V lead to values of μ around
10, and also in this case we expect the samples of a well-clusterable (Kerenidis et al., 2018) dataset to be constant.
Also, μ is bounded by the Frobenius norm of V. In case the matrix V can be clustered with high-enough accuracy by
7
k-means, it has been showed that the FrobeniUs norm of the matrix is proportional to √k. Given that EM is a more
powerful extension of k-means, we can rely on similar observations too. Usually, the number of features d is much
more than the nUmber of components in the mixtUre, i.e. d k, so we expect d2 to dominate the k3.5 term in the cost
needed to estimate the mixing weights. This makes the rUntime of a single iteration proportional to:
e (d2k4∙5η3κ2(V)κ2(Σ)μ(Σ)
(2)
As we said, the qUantUm rUnning time saves the factor that depends on the nUmber of samples and introdUces a nUmber
of other parameters. Using oUr experimental resUlts we can see that when the nUmber of samples is large enoUgh one
can expect the qUantUm rUnning time to be faster than the classical one. Note as well that one can expect to save some
more factors from the qUantUm rUnning time with a more carefUl analysis.
Experiments. In the algorithm, We need to set the parameters δμ and δθ to be small enough such that the likelihood
is pertUrbed less than τ /4. We have reasons to believe that on well-clUsterable data, the valUe of these parameters
will be large enough, such as not to impact dramatically the runtime. A quantum version of k-means algorithm has
already been simulated on real data under similar assumptions (Kerenidis et al., 2018). There, the authors analyzed
on the MNIST dataset the performances of q-means, the δ-resistant version of the classical k-means algorithm. The
experiment concluded that, for datasets that are expected to be clustered nicely by this kind of clustering algorithms,
the value of the parameters δμ, δθ did not decrease by increasing the number of samples nor the number of features.
We expect similar behaviour in the EM case, namely that for large datasets the impact on the runtime of the errors
(δμ, δθ) does not cancel out the exponential gain in the dependence on the number of samples. For instance, in all the
experiments of q-means (Kerenidis et al., 2018) on the MNIST dataset the value of δμ (which in their case was called
just δ) has been between 0.2 and 0.5. The value of τ is usually (for instance in scikit-learn (Pedregosa et al., 2011) )
chosen to be 10-3. The value ofη has always been below 11.
We also analyzed some other real-world dataset, which can be fitted well with the EM algorithm (Reynolds et al.,
2000; APPML; Voxforge.org) to perform speaker recognition: the task of recognizing a speaker from a voice sample,
having access to a training set of recorded voices of all the possible speakers. Details of the measurements are reported
in the Supplementary Material section, here we report only the results in Table 1. After this, we also experimented the
impact of errors on the mixing weights in the accuracy of a ML estimate of a GMM by perturbing the trained model,
by adding some random noise. With a value of δθ = 0.035, δμ = 0.5 we correctly classified 98.2% utterances.
	kΣk2		∣logdet(Σ) |	κ*(∑)	μ(∑)	μ(v)	κ(V)	Acc.(%)
MAP	avg	0.244	58.56	4.21	3.82	2.14	23.82	99.4
	max	2.45	70.08	50	4.35	2.79	40.38	
ML	avg	1.31	14.56	15.57	2.54	2.14	23.82	98.8
	max	3.44	92,3	50	3.67	2.79	40.38	
Table 1: We estimate some of the parameters of the VoxForge (Voxforge.org) dataset. The averages for the matrix V
are taken over 34 samples, while for Σ is over 170 samples. The accuracy reported in the experiments is measured
on 170 samples in the test set, after the threshold on the eigenvalues of Σ. Each model is the result of the best of 3
different initializations of the EM algorithm. The first and the second column are the maximum singular values of all
the covariance matrices, and the absolute value of the log-determinant. The column K (Σ) consist in the thresholded
condition number for the covariance matrices.
In conclusion, the experimental results suggest that the influence of the extra parameters in the quantum running time
(condition thresholds, errors, etc.) is moderate. This allows us to be optimistic that, when quantum computers with
quantum access to data become a reality, our algorithm (and improved versions that reduce even more the complexity
with respect to these extra parameters) could be useful in analyzing large datasets.
5	Acknowledgements
We would like to thank the authors of (Miyahara et al., 2019) for sharing an early version of their manuscript with us.
Part of this research was supported by the projects: ANR QuDATA and QuantERA QuantAlgo.
8
References
Esma Aimeur, Gilles Brassard, and Sebastien Gambs. Quantum speed-up for unsupervised learning. Machine Learn-
ing ,90(2):261-287, 2013.
APPML.	Spoken speaker identification based on gaussian mixture models :	Python
implementation.	https://appliedmachinelearning.blog/2017/11/14/
spoken-speaker-identification-based-on-gaussian-mixture-models-python-implementation/.
accessed 20/07/2019.
Juan Miguel Arrazola, Alain Delgado, Bhaskar Roy Bardhan, and Seth Lloyd. Quantum-inspired algorithms in prac-
tice. arXiv preprint arXiv:1905.10415, 2019.
David Arthur and Sergei Vassilvitskii. k-means++: The advantages of careful seeding. In Proceedings of the eigh-
teenth annual ACM-SIAM symposium on Discrete algorithms, pp. 1027-1035. Society for Industrial and Applied
Mathematics, 2007.
Mohd Ali Balafar, Abdul Rahman Ramli, M Iqbal Saripan, and Syamsiah Mashohor. Review of brain mri image
segmentation methods. Artificial Intelligence Review, 33(3):261-274, 2010.
Jacob Biamonte, Peter Wittek, Nicola Pancotti, Patrick Rebentrost, Nathan Wiebe, and Seth Lloyd. Quantum machine
learning. Nature, 549(7671):195, 2017.
Christophe Biernacki, Gilles Celeux, and Gerard Govaert. Choosing starting values for the EM algorithm for getting
the highest likelihood in multivariate gaussian mixture models. Computational Statistics & Data Analysis, 41(3-4):
561-575, 2003.
Jeff A Bilmes et al. A gentle tutorial of the EM algorithm and its application to parameter estimation for gaussian
mixture and hidden markov models. , 1998.
Johannes Bloi mer and Kathrin Bujna. Simple methods for initializing the EM algorithm for gaussian mixture models.
CoRR, 2013.
Christos Boutsidis, Petros Drineas, Prabhanjan Kambadur, Eugenia-Maria Kontopoulou, and Anastasios Zouzias. A
randomized algorithm for approximating the log determinant of a symmetric positive definite matrix. Linear Algebra
and its Applications, 533:95-117, 2017.
Gilles Brassard, Peter H0yer, Michele Mosca, and Alain Tapp. Quantum Amplitude Amplification and Estimation.
Contemporary Mathematics, 305, 2002.
Gilles Celeux and Gerard Govaert. A classification EM algorithm for clustering and two stochastic versions. Compu-
tational statistics & Data analysis, 14(3):315-332, 1992.
Shantanav Chakraborty, AndraS Gilyen, and Stacey Jeffery. The power of block-encoded matrix powers: improved
regression techniques via faster Hamiltonian simulation. arXiv preprint arXiv:1804.01973, 2018.
Nai-Hui Chia, Han-Hsuan Lin, and Chunhao Wang. Quantum-inspired sublinear classical algorithms for solving
low-rank linear systems. arXiv preprint arXiv:1811.04852, 2018.
Kenneth W Church and William A Gale. Poisson mixtures. Natural Language Engineering, 1(2):163-190, 1995.
Sanjoy Dasgupta. Learning mixtures of gaussians. In 40th Annual Symposium on Foundations of Computer Science
(Cat. No. 99CB37039), pp. 634-644. IEEE, 1999.
Arthur P Dempster, Nan M Laird, and Donald B Rubin. Maximum likelihood from incomplete data via the EM
algorithm. Journal of the royal statistical society. Series B (methodological), pp. 1-38, 1977.
AR Dexter and DW Tanner. Packing densities of mixtures of spheres with log-normal size distributions. Nature
physical science, 238(80):31, 1972.
Xiaodan Fan, Yuan Yuan, Jun S Liu, et al. The EM algorithm and the rise of computational biology. Statistical Science,
25(4):476-491, 2010.
9
Edward Farhi and Hartmut Neven. Classification with quantum neural networks on near term processors. arXiv
preprint arXiv:1802.06002, 2018.
ME Ghitany, Ross A Maller, and S Zhou. Exponential mixture models with long-term survivors and covariates.
Journal of multivariate Analysis, 49(2):218-241,1994.
Andras Gilyen, Seth Lloyd, and EWin Tang. Quantum-inspired low-rank stochastic regression with logarithmic de-
pendence on the dimension. arXiv preprint arXiv:1811.04909, 2018a.
AndraS Gilyen, Yuan Su, GUang Hao Low, and Nathan Wiebe. Quantum singular value transformation and beyond:
exponential improvements for quantum matrix arithmetics. arXiv preprint arXiv:1806.01838, 2018b.
Aram W. Harrow, Avinatan Hassidim, and Seth Lloyd. Quantum Algorithm for Linear Systems of Equations. Physical
Review Letters, 103(15):150502, 10 2009. ISSN 0031-9007. doi: 10.1103/PhysRevLett.103.150502. URL http:
//link.aps.org/doi/10.1103/PhysRevLett.103.150502.
Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical Learning, volume 1 of Springer
Series in Statistics. Springer New York, New York, NY, 2009. ISBN 978-0-387-84857-0. doi: 10.1007/b94608.
URL http://www.springerlink.com/index/10.1007/b94608.
N Jiang, Y-F Pu, W Chang, C Li, S Zhang, and L-M Duan. Experimental realization of 105-qubit random access
quantum memory. npj Quantum Information, 5(1):28, 2019.
Michael Kearns, Yishay Mansour, and Andrew Y Ng. An information-theoretic analysis of hard and soft assignment
methods for clustering. In Learning in graphical models, pp. 495-520. Springer, 1998.
Iordanis Kerenidis and Alessandro Luongo. Quantum classification of the MNIST dataset via Slow Feature Analysis.
arXiv preprint arXiv:1805.08837, 2018.
Iordanis Kerenidis and Anupam Prakash. Quantum recommendation systems. Proceedings of the 8th Innovations in
Theoretical Computer Science Conference, 2017a.
Iordanis Kerenidis and Anupam Prakash. Quantum gradient descent for linear systems and least squares. arXiv
preprint arXiv:1704.04992, 2017b.
Iordanis Kerenidis and Anupam Prakash. A quantum interior point method for LPs and SDPs. arXiv:1808.09266,
2018.
Iordanis Kerenidis, Jonas Landman, and Anupam Luongo, Alessandro Prakash. q-means: A quantum algorithm for
unsupervised machine learning. Accepted at NeurIPS 2019 - arXiv preprint arXiv:1812.03584, 2018.
Reginald L Lagendijk, Jan Biemond, and Dick E Boekee. Identification and restoration of noisy blurred images using
the expectation-maximization algorithm. IEEE Transactions on Acoustics, Speech, and Signal Processing, 38(7):
1180-1191, 1990.
Zhaokai Li, Xiaomei Liu, Nanyang Xu, and Jiangfeng Du. Experimental realization of a quantum support vector
machine. Physical review letters, 114(14):140504, 2015.
Bruce G Lindsay. Mixture models: theory, geometry and applications. In NSF-CBMS regional conference series in
probability and statistics, pp. i-163. JSTOR, 1995.
Chuanhai Liu and Donald B Rubin. ML estimation of the t distribution using EM and its extensions, ECM and ECME.
Statistica Sinica, pp. 19-39, 1995.
Seth Lloyd, Masoud Mohseni, and Patrick Rebentrost. Quantum algorithms for supervised and unsupervised machine
learning. arXiv, 1307.0411:1-11, 7 2013a. URL http://arxiv.org/abs/1307.0411.
Seth Lloyd, Masoud Mohseni, and Patrick Rebentrost. Quantum algorithms for supervised and unsupervised machine
learning. arXiv, 1307.0411:1-11, 7 2013b. URL http://arxiv.org/abs/1307.0411.
Hideyuki Miyahara, Kazuyuki Aihara, and Wolfgang Lechner. Quantum expectation-maximization algorithm. Per-
sonal Communication, 2019.
10
Kevin P Murphy. Machine learning: a probabilistic perspective. MIT press, 2012.
Andrew Ng. Cs229 lecture notes - machine learning. Lecture notes CS229 Stanford, 2012.
Michael A Nielsen and Isaac Chuang. Quantum computation and quantum information, 2002.
JS Otterbach, R Manenti, N Alidoust, A Bestwick, M Block, B Bloom, S Caldwell, N Didier, E Schuyler Fried,
S Hong, et al. Unsupervised machine learning on a hybrid quantum computer. arXiv preprint arXiv:1712.05771,
2017.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss,
V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn:
Machine learning in Python. Journal ofMachine Learning Research,12:2825-2830, 2011.
Douglas A Reynolds, Thomas F Quatieri, and Robert B Dunn. Speaker verification using adapted gaussian mixture
models. Digital signal processing, 10(1-3):19-41, 2000.
Walter Rudin et al. Principles of mathematical analysis, volume 3. McGraw-hill New York, 1964.
Yigit Subayi, Rolando D Somma, and Davide Orsucci. Quantum algorithms for systems of linear equations inspired
by adiabatic quantum computing. Physical review letters, 122(6):060504, 2019.
Ewin Tang. Quantum-inspired classical algorithms for principal component analysis and supervised clustering. arXiv
preprint arXiv:1811.00414, 2018a.
Ewin Tang. Quantum-inspired classical algorithms for principal component analysis and supervised clustering. arXiv
preprint arXiv:1811.00414, 2018b.
Ewin Tang. A quantum-inspired classical algorithm for recommendation systems. arXiv preprint arXiv:1807.04271,
2018c.
Voxforge.org. Free speech... recognition - voxforge.org. http://www.voxforge.org/. accessed 20/07/2019.
Nathan Wiebe, Ashish Kapoor, and Krysta M Svore. Quantum Algorithms for Nearest-Neighbor Methods for Super-
vised and Unsupervised Learning. 2014. URL https://arxiv.org/pdf/1401.2142.pdf.
Nathan Wiebe, Ram Shankar, and Siva Kumar. Hardening Quantum Machine Learning Against Adversaries. 2017.
Jianhua Yin and Jianyong Wang. A dirichlet multinomial mixture model-based approach for short text clustering.
In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pp.
233-242. ACM, 2014.
6 Supplementary Material
6.1	Previous results used in the proofs
Here we report useful Theorems, Lemmas, and Claims that we use in the main text.
Theorem 6.1 (Determinant estimation (Boutsidis et al., 2017)). Let M ∈ Rd×d be a positive definite matrix with
eigenvalues in the interval (σmin, 1). Then for all δ ∈ (0, 1) and > 0 there is a classical algorithm that outputs
log(det(M)) such that ∣log(det(M)) 一 log(det(M))| ≤ 2e| log(det(M))| with probability at least 1 一 δ in time
T . C /IOg(I/E)IOg(I/6	∕mΛ
T°et,e ：= O I ----^2^^--------nnz(M) 1 .
Theorem 6.2 (Multivariate Mean Value Theorem (Rudin et al., 1964)). Let U be an open set of Rd. Fora differentiable
functions f : U → R it holds that ∀x, y ∈ U, ∃c such that f (x) — f (y) = Nf(C) ∙ (x — y).
Theorem 6.3 (Componentwise Softmax function σj(v) is Lipschitz continuous). For d > 2, let σj : Rd 7→ (0, 1) be
Qv
the softmax function defined as σj (v) = Pd—— Then σj is Lipschitz continuous, With K ≤ √2.
11
Algorithm 2 Expectation-Maximization for GMM
Require: Dataset V , tolerance τ > 0.
Ensure: A GMM Yt = (θt, μt, Σt) that maximizes locally the likelihood '(γ; V) UP to tolerance T.
1:	Select γ0 = (θ0, μ0, Σ0) using classical initialization strategies described in Subsection 6.3.
2:	t = 0
3:	repeat
4:	Expectation
∀i, j , calculate the resPonsibilities as:
5:	Maximization
UPdate the Parameters of the model as:
1n
θj+= 1 X rj
i=1
t+ι J Pn=I rtjV
μj	Pn=I rtj
6:	t=t+1
7:	until
8:
l'(γt-1; V) - '(γt; V)| < τ
(3)
(4)
(5)
(6)
(7)
9:	Return Yt = (θt, μt, Σt)
Proof. We need to find the K such that for all x, y ∈ Rd, we have that kσj(y) - σj(x)k ≤ K ky - xk. Observing
that σj is differentiable and that if we aPPly Cauchy-Schwarz to the statement of the Mean-Value-Theorem we derive
that ∀x, y ∈ U, ∃c such that ∣∣f (x) 一 f (y)k ≤ ∣∣Vf (C)IlF ∣∣x 一 y∣∣. So to show Lipschitz continuity it is enough to
select K ≤ ∣∣Vσj∣F = maXc∈Rd ∣∣Vσj(c)∣.
The partial derivatives	dσjυv	are	σj(v)(1 —	σ7-(V))	if i = j and	-σ%(v)σj(V)	otherwise. So ∣∣Vσ7-∣∣F	=
pd-ι (-σ(v)iσj(V))2 + σj(V)2(I-σj(V))2 ≤ Pd-II σ(v)iσj(V) + σj(V)(I-σj(V)) ≤ σj(V) Pd-C1σi(V) + 1 -
σj(v) ≤ 2σj(v) ≤ 2. In our case we can deduce that: ∣σj (y) - σj (x)∣ ≤ √2 ∣∣y - x∣∣ so K ≤ √2.
□
Claim 6.4. (Kerenidis & Prakash, 2017b) Let θ be the angle between vectors x, y, and assume that θ < π∕2. Then,
Ilx - y∣ ≤ e implies ll|xi Tyik ≤ √X2f.
We will also use Claim 4.5 from (Kerenidis et al., 2018).
Claim 6.5. (KerenidiS et al., 2018) Let Cb be the error we commit in estimating |cisuch that |||。)一 ∣ci∣ < Eb, and Ca
the error we commit in the estimating the norms, | ∣c∣ - ∣∣c∣∣ ≤ Ca ∣∣c∣. Then ∣∣c - Ck ≤ √η(^α + Cb).
Definition 2 (Exponential FamiIy(MUrphy, 2012)). A probability densityfunction or probability massfUnCtion p(v∣ν)
for v = (vι, •…，Vm) ∈ Vm, where V ⊆ R, V ∈ Rp is said to be in the exponentialfamily if can be written as:
p(v∣ν):= h(v) exp{o(ν)TT(v) - A(V)}
where:
•	V ∈ Rp is called the canonical or natural parameter of the family,
12
•	o(ν) is a function of ν (which often is just the identity function),
•	T(v) is the vector of sufficient statistics: a function that holds all the information the data v holds with respect
to the unknown parameters,
•	A(ν) is the cumulant generating function, or log-partition function, which acts as a normalization factor,
•	h(v) > 0 is the base measure which is a non-informative prior and de-facto is scaling constant.
6.2	Quantum procedures
To prove our results, we are going to use the quantum procedures listed hereafter.
Theorem 6.6 (Amplitude estimation and amplification (Brassard et al., 2002)). If there is unitary operator U such
that U |0)l = ∣φi = sin(θ) |x, 0)+ cos(θ) |G, 0⊥)then sin2(θ) can be estimated to multiplicative error η in time
O( ηTin(θ)) and |x) can be generated in expected time O( ST(U)).
We also need some state preparation procedures. These subroutines are needed for encoding vectors in vi ∈ Rd into
quantum states |vii. An efficient state preparation procedure is provided by the QRAM data structures. We stress the
fact that our result continues to hold, no matter how the efficient quantum loading of the data is provided. For instance,
the data can be accessed through a QRAM, through a block encoding, or when the data can be produced by quantum
circuits.
Theorem 6.7 (QRAM data structure (Kerenidis & Prakash, 2017a)). Let V ∈ Rn×d, there is a data structure to store
the rows of V such that,
1.	The time to insert, update or delete a single entry vij is O(log2(n)).
2.	A quantum algorithm with access to the data structure can perform the following unitaries in time T =
O(log2 N).
(a)	|ii |0i → |ii |vii for i ∈ [n].
(b)	|0i → Pi∈[n] kvik |ii.
In our algorithm we will also use subroutines for quantum linear algebra. For a symmetric matrix M ∈ Rd×d with
spectral norm kM k = 1 stored in the QRAM, the running time of these algorithms depends linearly on the condition
number κ(M) of the matrix, that can be replaced by κτ(M), a condition threshold where we keep only the singular
values bigger than T, and the parameter μ(M), a matrix dependent parameter defined as
μ(M) = m∈n(kM∣∣F , ,S2p(M)S2(1-P)(MT)),
for sp (M) := maxi∈[n] kmikpp where kmikp is the `p norm of the i-th row of M, and P is a finte set of size O(1) ∈
[0,1]. Note that μ(M) ≤ ∣∣M∣∣f ≤ √d as We have assumed that ∣∣M∣∣ = 1. The running time also depends
logarithmically on the relative error E of the final outcome state. (Chakraborty et al., 2018; GiIyen et al., 2018b).
Theorem 6.8 (Quantum linear algebra (Chakraborty et al., 2018; GiIyen et al., 2018b)). Let M ∈ Rd×d such that
∣M ∣2 = 1 and x ∈ Rd. Let E, δ > 0. If M is stored in appropriate QRAM data structures and the time to prepare
|xi is Tx, then there exist quantum algorithms that with probability at least 1 - 1/poly(d) return a state |zi such that
.... ... ~.......................... .. ...
k∣z) — |Mxik ≤ E in time O((K(M)μ(M) + TxK(M)) log(1∕e)).
Theorem 6.9 (Quantum linear algebra for matrix products (Chakraborty et al., 2018) ). Let M1,M2 ∈ Rd×d such
that ∣M ∣1 = ∣M ∣2 = 1 and x ∈ Rd, and a vector x ∈ Rd stored in QRAM. Let E > 0. Then there exist quantum
algorithms that with probability at least 1 — 1∕poly(d) returns a state |z) such that |||z) — ∣Mx)∣ ≤ E in time
O((K(M )(μ(M1)TM1 + μ(M2)TM2 ))log(1∕c)), where Tm1 , Tm2 is the time needed to index the rows of Mi and
M2.
The linear algebra procedures above can also be applied to any rectangular matrix V ∈ RN ×d by considering instead
the symmetric matrix V
V
0
13
The final component needed for the q-means algorithm is a linear time algorithm for vector state tomography that
will be used to recover classical information from the quantum states corresponding to the new centroids in each step.
Given a unitary U that produces a quantum state |xi, by calling O(d log d/2) times U, the tomography algorithm is
able to reconstruct a vector xe that approximates |xi such that k|xei - |xik ≤ .
Theorem 6.10 (Vector state tomography (Kerenidis & Prakash, 2018)). Given access to unitary U such that U |0i =
|xi and its controlled version in time T (U) ,there is a tomography algorithm with time complexity O(T (U)d lθg d) that
produces unit vector xe ∈ Rd such that kxe - xk2 ≤ with probability at least (1 - 1/poly(d)).
Lemma 6.11 (Distance / Inner Products Estimation (Kerenidis et al., 2018; Wiebe et al., 2014; Lloyd et al., 2013a)).
Assume for a data matrix V ∈ RN×d and a centroid matrix C ∈ Rk×d that the following unitaries |ii |0i 7→ |ii |vii ,
and |ji |0i 7→ |ji |cji can be performed in time T and the norms of the vectors are known. For any ∆ > 0 and > 0,
there exists a quantum algorithm that computes
|i〉ji|0i →	|i〉|j)|d2(vi, Cj )i, where ∣d2(vi, Cj) - d2(vi, Cj )| 6 E with probability at least 1 — 2∆, or
|i〉ji|0i →	|i〉j〉l(vi, Cj )i, where |(vi, Cj) — (Vi, Cj )| 6 E with probability at least 1 — 2∆
in time O( k""。。kTl加1©).
6.3	Initialization strategies for EM
Unlike k-means clustering, choosing a good set of initial parameters for a mixture of Gaussian is by no means trivial,
and in multivariate context is known that the solution is problem-dependent. There are plenty of proposed techniques,
and here we describe a few of them. Fortunately, these initialization strategies can be directly translated into quantum
subroutines without impacting the overall running time of the quantum algorithm.
The simplest technique is called random EM, and consists in selecting initial points at random from the dataset as
centroids, and sample the dataset to estimate the covariance matrix of the data. Then these estimates are used as the
starting configuration of the model, and we may repeat the random sampling until we get satisfactory results.
A more standard technique borrows directly the initialization strategy of k-means++ proposed in (Arthur & Vassil-
vitskii, 2007), and extends it to make an initial guess for the covariance matrices and the mixing weights. The initial
guess for the centroids is selected by sampling from a suitable, easy to calculate distribution. This heuristic works as
following: Let C0 be a randomly selected point of the dataset, as first centroid. The other k— 1 centroids are selected by
selecting a vector Vi with probability proportional to d2(vi, μi(vi)), where μi(vi) is the previously selected centroid that
is the closest to vi in `2 distance. These centroids are then used as initial centroids for a round of k-means algorithm
to obtain μ0 …μ0. Then, the covariance matrices can be initialized as Σ0 := 嵩 Pi∈g (Vi 一 μj)(vi 一 μj-)T, where
Cj is the set of samples in the training set that have been assigned to the cluster j in the previous round of k-means.
The mixing weights are estimated as Cj /n. Eventually Σj0 is regularized to be a PSD matrix.
There are other possible choices for parameter initialization in EM, for instance, based on Hierarchical Agglomerative
Clustering (HAC) and the CEM algorithm. In CEM we run one step of EM, but with a so-called classification step be-
tween E and M. The classification step consists in a hard-clustering after computing the initial conditional probabilities
(in the E step). The M step then calculates the initial guess of the parameters (Celeux & Govaert, 1992). In the small
EM initialization method we run EM with a different choice of initial parameters using some of the previous strategies.
The difference here is that we repeat the EM algorithm for a few numbers of iterations, and we keep iterating from the
choice of parameters that returned the best partial results. For an overview and comparison of different initialization
techniques, We refer to (Blomer & Bujna, 2013; Biernacki et al., 2003).
Quantum initialization strategies For the initialization of γ0 in the quantum algorithm we can use the same ini-
tialization strategies as in classical machine learning. For instance, we can use the classical random EM initialization
strategy for QEM.
A quantum initialization strategy can also be given using the k-means++ initializion strategy from (Kerenidis et al.,
2018), which returns k initial guesses for the centroids c0 •…Ck consistent with the classical algorithm in time
k2
2η1∙5
e√E(d2(vi,Vj))
, where E(d2 (Vi, Vj)) is the average squared distance between two points of the dataset, and
is the tolerance in the distance estimation. From there, we can perform a full round of q-means algorithm and get an
E
14
estimate for μ0 •…μk. With q-means and the new centroids store in the QRAM We can create the state
1n
∣ψ0i := √n ∑∣ii∣i(Vi)i.
(8)
Where l(vi) is the label of the closest centroid to the i-th point. By sampling S ∈ O(d) points from this state we get
two things. First, from the frequency fj of the second register we can have an guess of θj J |Cj |/n 〜fj/S. Then,
from the first register we can estimate Σ0 J Pi∈s(Vi — μ0)(vi 一 μ0)τ. Sampling O(d) points and creating the
state in Equation (8) takes time O(dkη) by Theorem 6.11 and the minimum finding procedure described in (Kerenidis
et al., 2018).
Techniques illustrated in (Miyahara et al., 2019) can also be used to quantize the CEM algorithm which needs a hard-
clustering step. Among the different possible approaches, the random and the small EM greatly benefit from a faster
algorithm, as we can spend more time exploring the space of the parameters by starting from different initial seeds,
and thus avoid local minima of the likelihood.
6.4	Special cases of GMM.
What we presented in the previous section is the most general model of GMM. For simple datasets, it is common to
assume some restrictions on the covariance matrices of the mixtures. The translation into a quantum version of the
model should be straightforward. We distinguish between these cases:
1.	Soft k-means. This algorithm is often presented as a generalization of k-means, but it can actually be seen
as special case of EM for GMM - albeit with a different assignment rule. In soft k-means, the assignment
function is replaced by a softmax function with stiffness parameter β. This β represents the covariance of the
clusters. It is assumed to be equal for all the clusters, and for all dimensions of the feature space. Gaussian
Mixtures with constant covariance matrix (i.e. Σj = βI for β ∈ R) can be interpreted as a kind of soft or
fuzzy version of k-means clustering. The probability ofa point in the feature space being assigned to a certain
cluster j is:
e-βkxi-μik2
rij = Pk=I e-βkXi-μιk2
where β > 0 is the stiffness parameter.
2.	Spherical. In this model, each component has its own covariance matrix, but the variance is uniform in all
the directions, thus reducing the covariance matrix to a multiple of the identity matrix (i.e. Σj = σj2I for
σj ∈ R).
3.	Diagonal. As the name suggests, in this special case the covariance matrix of the distributions is a diagonal
matrix, but different Gaussians might have different diagonal covariance matrices.
4.	Tied. In this model, the Gaussians share the same covariance matrix, without having further restriction on
the Gaussian.
5.	Full. This is the most general case, where each of the components of the mixture have a different, SDP,
covariance matrix.
6.5	Proofs
Lemma 6.12 (Determinant evaluation). There is an algorithm that, given as input a matrix Σ and a parameter 0 <
δ < 1, outputs an estimate log(det(Σ)) such that ∣log(det(Σ)) - log(det(Σ))∣ ≤ E with probability 1 一 δ in time:
TDet,e= O (e-2κ(∑) log(1∕δ)nnz(Σ)∣ log(det(Σ))∣)
Proof. In order to apply Theorem 6.1, we need to be sure that all the eigenvalues lie in (σmin, 1). In order to satisfy this
condition, we can scale the matrix by a constant factor c, such that Σ0 = Σ∕c. In this way, log det(Σ0) = log Qd(σi∕c).
Therefore, log(det(Σ0)) = log(det(Σ)) - log(cd). This will allow us to recover the value of log det(Σ) by using
15
Theorem 6.1. We apply the Theorem with precision E = 1/4 to get an estimate Y such that 1 ≤ 匕目口;1但))≤ 2. Then,
to have an estimate with absolute error e, We apply Theorem 6.1 with precision E = *. This gives Us an estimate for
log(det(Σ)) with error 2E0 log(det(Σ)) ≤ E in time:
O (e-2κ(Σ) log(1∕δ)nnz(∑)∣ log(det(Σ)) |).
□
Lemma 6.13 (Error in the responsibilities of the exponential family). Let Vi ∈ Rn be a vector, and let {p(v∕νj )}g=ι
be a set of k probability distributions in the exponential family, defined as p(vi∣νj) := hj (vi)exp{θj (Vj )T Tj (Vi) 一
Aj (Vj)}. Then, ifwe have estimatesfor each exponent with error e, then we can compute each rij such that ∣rij 一rj | ≤
√2ke for j ∈ [k].
Proof. The proof follows from rewriting the responsibility of Equation (3) as:
hj (Vi)eχp{oj (Vj )T T (Vi)- Aj (Vj ) +log θj }
rij := ^^k	(9)
P hl(Vi) eχp{ol (Vl)TT(Vi) 一 Al(Vl) + log θl}
l=1
In this form, it is clear that the responsibilities can be seen a softmax function, and we can use Theorem 6.3 to bound
the error in computing this value.
Let Ti ∈ Rk be the vector of the exponent, that is tij = oj(Vj)TT(Vi) 一 Aj(Vj) + log θj. In an analogous way
we define Ti the vector where each component is the estimate with error e. The error in the responsibility is defined
as ∣rij- - rij| = ∣σj∙(Ti) 一 σj-(Ti)|. Because the function σj is Lipschitz continuous, as we proved in Theorem
6.3 with a Lipschitz constant K ≤ √2, we have that, ∣σj-(Ti) 一 σj-(Ti)| ≤ √2 ∣∣T^ 一 3八.The result follows as
∣∣Ti 一 Ti∣∣ < √kE.	口
Lemma 6.14 (Quantum Gaussian Evaluation). Suppose we have stored in the QRAM a matrix V ∈ Rn×d, the centroid
μ ∈ Rd and the CovarianCe matrix Σ ∈ Rd×d ofa multivariate Gaussian distribution φ(v∣μ, Σ), as well as an estimate
for log(det(Σ)). Then for E1 > 0, there exists a quantum algorithm that with probability 1 一 γ performs the mapping,
• Ug,∈i : |i)|0)→ |ii |sii such that |si 一 si| < 6ι, where Si = 一2((Vi 一 μ)τ∑-1(vi — μ) + dlog2π +
log(det(Σ))) is the exponent for the Gaussian probability density function.
The running time of the algorithm is,
TG,1
κ2⑶〃⑶log(1∕Y) η
E1
(10)
Proof. We use quantum linear algebra and inner product estimation to estimate the quadratic form (Vi — μ)τΣ-1(Vi 一
μ) to error 气.First, we decompose the quadratic form as VTΣ-1Vi — 2vTΣ-1μ+μτΣ-1μ and separately approximate
each term in the sum to error E1/4.
We describe the procedure to estimate μτΣ-1Vi = ∣∣∑-1Vi∣ kμk hμ∖Σ-1Vi), the other estimates are obtained sim-
ilarly. We use the quantum linear algebra subroutines in Theorem 6.8 to construct ∖Σ-1Vii up to error €3《 in
time O(κ(Σ)μ(Σ)log(1∕63)) and estimate ∣∣Σ-1Vi∣∣ up to error €1 in time O(κ(Σ)μ(Σ)log(1 /€3)/e1) which gives
us the mapping |i)|0)→ |i)∖k∑-1Viki. We then use quantum inner product estimation (Theorem 6.11) to estimate
hμ, Σ-1Vii to additive error 4口*小位-1»口. The procedure estimates (μ, ∑-1Vi) within additive error €1/4. The pro-
cedure succeeds with probability 1 一 Y and requires time O( K(')"(')log(1/Y)log(1/'3)∣∣μk ∣∣Σ-1 Vi∣∣). Using similar
estimation procedure for VT∑-1μ and μτΣ-1μ, we obtain an estimate for 1 ((Vi 一 μ)τΣ-1(Vi — μ) within error €1.
Recall that (through Lemma 3.1) we also have an estimate of the log-determinant to error €1. Thus we obtain an
approximation for -1 ((Vi - μ)τΣ-1(Vi — μ) + dlog 2π + log(det(Σ))) within error 2€1. We have the upper bound,
16
Σ-1vi ≤ Σ-1 kvik ≤ κ(Σ) kvik, as kΣk = 1 for Σ stored in a QRAM data structure. Further observing that
∣∣uk ≤ √η and ∣∣vik ≤ √η, the running time for this computation is O (K ⑵烂)10：；1/))10^1/'"斗
□
Lemma 6.15 (Calculating responsibilities). Suppose we have quantum access to a GMM with parameters γt =
(θt, μt, Σt). There are quantum algorithms that can:
1.	Perform the mapping |i)|ji|0)→ |i)|j)|riji such that |rij 一 rj | ≤ ∈ι with probability 1 一 Y in time:
TR1,1 = Oe(k1.5 ×TG,1)
___	___ n
2.	For a given j ∈ [k], construct state ∣Rji such that ∣Rji--笄 E r j |i)
Zj i=0
high probability in time:
TR2,1 = O(k × TR1,1 )
n
< 1 where Zj =	ri2j with
i=0
Proof. For the first statement, let's recall the definition of responsibility: rj = Pkjφ(vφμj1∑j∑0. With the aid of
Ug,∈i of Lemma 3.2 We can estimate log(φ(v∕μj, Σj)) for all j UP to additive error s, and then using the current
estimate of θt, we can calculate the responsibilities create the state,
jillog(O(ViM, ς)i
X |rji ∙
The estimate rij is computed by evaluating a weighted softmax function with arguments log(φ(vi ∣μj, Σj) for j ∈ [k].
The estimates log(φ(v∕μj, Σj) are then uncomputed. The runtime of the procedure is given by calling k times Lemma
3.2 for Gaussian estimation (the arithmetic operations to calculate the responsibilities are absorbed).
Let us analyze the error in the estimation of rij . The responsibility rij is a softmax function with arguments
log(φ(vi M, Σj)) that are computed upto error ∈ι using Lemma 3.2. As the softmax function has a Lipschitz constant
K ≤ √2 by Lemma 6.13, we choose precision for Lemma 3.2 to be e`/√2k to get the guarantee |rij 一 ri7-1 ≤ q.
Thus, the total cost of this step is TR1 ,1 = k1.5TG,1 .
Now let’s see how to encode this information in the amplitudes, as stated in the second claim of the Lemma. We
estimate the responsibilities rij to some precision and perform a controlled rotation on an ancillary qubit to obtain,
n	n	________
√n ∣jiX∣ii ∣riji (rj∣0i + √1 -rj2 ∣ii)∙	(ii)
We then undo the circuit on the second register and perform amplitude amplification on the rightmost auxiliary qubit
being |0)to get |Rji := ∣=-∣ Pn=o rij Mi∙ The runtime for amplitude amplification on this task is O(Tri,6 ∙ ∣商 ).
Let us analyze the precision E required to prepare |Rji such that IIIRji — |RjiIl ≤ j As we have estimates 卜j 一
rij∣ < E for all i, j, the '2-norm error ∣∣Rj - Rj ∣∣ = Pin=0 ∣rij- 一 rij∣2 < √ne. Applying Claim 6.4, the error for
the normalized vector |Rji can be bounded as ∣∣∣Rj〉一|Rji ∣∣ < 得n∣. By the Cauchy-Schwarz inequality we have
that kRjk ≥ E√Tij. We can use this to obtain a bound Vnk < Pn-√n = O(k), using the dataset assumptions in
section 2. If we choose E such that k2nf < eι, thatis E ≤ e`/k then our runtime becomes Tr2,q := O(k2 X Tri,q).
□
Lemma 6.16 (Computing θt+1). We assume quantum access to a GMM with parameters γt and let δθ > 0 be a
precisionParameter There exists an algorithm that estimates θt+1 ∈ Rk such that ∣∣θt+1 — θt+11∣ ≤ δθ in time
Tk = O 卜3.5η1.5 κ2≡μ≡)
17
Proof. An estimate of θjt+k can be recovered from the following operations. First, we use Lemma 3.3 (part 1) to
compute the responsibilities to error Ek, and then perform the following mapping, which consists of a controlled
rotation on an auxiliary qubit:
√nk X ιii |j i ιrijti → √nk X ιiiiji (qrjt ι0i+q1—rijt |1i)
j=1
j=1
The previous operation has a cost of Tri 后,and the probability of getting |0)is p(0) = nk Pnn=ι P：=i r j = k.
Recall that θjt+1
1 Pn=I rj by definition. Let Zj
pn=ι rjt and define state |，Rji
pn=ι ʌ/rjt |i〉I j〉. After amplitude amplification on |0)We have the state,
l√Ri := √⅛
k
X
j=1
n,k ---
Ejr"∣ji
i=1
j=1
W (p⅛ X 河 ιii)iji
k I-------
=X W ∣PRj i.
j=k
The probability of obtaining outcome |j〉if the second register is measured in the standard basis is p(j) = θjt+1
(12)
An estimate for θjt+1 With precision can be obtained by either sampling the last register, or by performing amplitude
estimation to estimate each of the values θjt+1 forj ∈ [k]. Sampling requires O(-2) samples by the Chernoff bounds,
but does not incur any dependence on k. In this case, as the number of cluster k is relatively small compared to 1/g
we chose to do amplitude estimation to estimate all θj+1 for j ∈ [k] to error e∕√k in time,
Tθ := O (k ∙ VzkTRI ,e1
(13)
Let’s analyze the error in the estimation of θjt+1. For the error due to responsibility estimation by Lemma 3.3 we have
∣θjt+1 — θj+1∣ = k Pilrijt — rj| ≤ eι for all j ∈ [k], implying that ∣∣θt+1 一 θt+1∣∣ ≤ √‰ι. The total error in '2
norm due to Amplitude estimation is at most E as it estimates each coordinate of θjt+1 to error e∕√k.
Using the triangle inequality, we have the total error is at most E + √⅛. As we require that the final error be upper
bounded as ∣∣θt+1 一 θt+1∣∣ < δθ, we choose parameters √k∈ι < δθ/2 ⇒ ei < ^√⅛ and E < δθ∕2. With these
parameters, the overall running time of the quantum procedure is Tθ = O(k1
.5 TR],eι
□
Lemma 6.17 (Computing μj+1). We assume we have quantum access to a GMM with parameters Yt. For a pre-
cision parameter δμ > 0, there is a quantum algorithm that CalculateS {μjt+i}j=ι such that for all j ∈ [k]
∣∣μjt+1 — μtj + 1∖∖ ≤ δμ in time
Tμ = O
kd∕ηκ(V )(μ(V) + k3.5η1.5κ2(Σ)μ(Σ))
Proof. The new centroid μj+1 is estimated by first creating an approximation of the state |Rj〉up to error 印 in the '2-
norm using part 2 of Lemma 3.3. We then use the quantum linear algebra algorithms in Theorem 6.8 to multiply Rjby


)=O (k3∙5 尸”用"3)
18
VT, and compute a state ∣μjt+1i along with an estimate for the norm ∣∣ VTRjl = ∣∣μjt+1^ with error Enorm. The last
step of the algorithm consists in estimating the unit vector ∣μjt+1i with precision Etom using tomography. Considering
that the tomography depends on d, which we expect to be bigger than the precision required by the norm estimation, we
can assume that the runtime of the norm estimation is absorbed. Thus, We obtain: O (k ^d- ∙ K(V) (μ(V) + Tr2 口)).
tom	,
Let,s now analyze the total error in the estimation of the new centroids, which We want to be δμ. For this purpose,
We use Claim 6.5, and choose parameters such that 2√η(∈tom + Enorm) = δμ. Since the error €3 for quantum linear
algebra appears as a logarithmic factor in the running time, we can choose E3 Etom without affecting the runtime.
Let μ be the classical unit vector obtained after quantum tomography, and ∣μi be the state produced by the quantum
linear algebra procedure starting with an approximation of |Rj). Using the triangle inequality we have ∣∣∣μi - μk <
∣∣μ - iμi∣∣ + ∣∣iμi- iμi∣∣
< Etom + ei < δμ∕2√η. The errors for the norm estimation procedure can be bounded
-------------------- -^^^-
similarly as | ∣μ∣ 一 ∣∣μ∣∣ < | ∣μ∣ 一 ∣∣μ∣∣ + ∣∣μ∣ 一 ∣∣μ∣∣ < Enorm + ei ≤ δμ∕2√η. We therefore choose parameters
Etom = ei = Enorm ≤ δμ∕4√η. Since the amplitude estimation step we use for estimating the norms does not depends
on d, which is expected to dominate the other parameters, we omit the amplitude estimation step. Substituting for
TR2,δμ, we have the more concise expression for the running time of:
6 (kdηκ(V)(μ(V) + k3.5η1.5κ2(Σ)μ(Σ))
(14)
□
Lemma 6.18 (Computing Σtj+1). We assume we have quantum access to a GMM with parameters γt. We also
have computed estimates μjt+1 of all CentroidS such that ∣∣μjt+1 — μj+1∣∣ ≤ δ* for precision parameter δ* > 0.
Then, there exists a quantum algorithm that outputs estimates for the new covariance matrices {∑j+1}k=ι such that
M1 - ς+1∣∣f ≤ δμ√η with high probability, in time,
TE := O(kd2ηκ2(V)(μ(V0) + η2k3.5κ2(Σ)μ(Σ))X
Proof. It is simple to check, that the update rule of the covariance matrix during the maximization step can be reduced
to (Murphy, 2012, Exercise 11.2):
∑+1 一
n
i=1 rij
Pn=I rjVivT
nθj
一 μj+1(μj+1)T = ∑j - μj+1 (μj+1)T
(15)
First, note that we can use the estimates of the centroids to compute μj+1(μj+1)T with error δμ ∣∣μ∣ ≤ δμ√η in
the update rule for the Σj. This follows from the fact that μ = μ + e where e is a vector of norm δ*. Therefore
∣∣μμτ — μμτ∣∣ < 2√ηδμ + δμ ≤ 3√ηδμ. It follows that we can allow an error of √ηδμ also for the left term in
the definition of Σtj+1. Let’s discuss the procedure for estimating Σ0j in Eq. (15). Note that vec[Σ0j] = (V0)TRj,
so we use quantum matrix multiplication to estimate ∣vec[Σj]i and ∣∣vec[Σj] ∣∣. As the runtime for the norm estima-
tion K(V )(μ(V )+τR2,e1 ))log(1/'mUlt) does not depend on d, we consider it smaller than the runtime for performing
norms
tomography. Thus, the runtime for this operation is:
O( ⅛gd K(V 0)(μ(V0) + TR2,e1))lθg(l/Emult)).
Etom
Let,s analyze the error of this procedure. We want a matrix Σj that is √ηδμ-close to the correct one: ∣∣ Σj — Σj ∣∣ =
∣∣vec[Σj] — vec[Σj]∣∣ < √ηδμ. Again, the error due to matrix multiplication can be taken as small as necessary,
19
since is inside a logarithm. From Claim 6.5, we just need to fix the error of tomography and norm estimation such
that η(eunit + Cnorms) < √,μ Where we have used η as an upper bound on ∣∣∑j∣∣F. For the unit vectors, we require
1当〉-1当
Σj>ll + ll∣∑ji - I∑j ill < Jom + Ci ≤ δμ∕2√η, where ∣Σji is the error due to tomography
and ∣Σji is the error due to Lemma 3.3. For this inequality to be true, We choose Qom =5 < δμ∕4√η.
El	.	1.	.	.∙	.∙	.<	Ilrll Jl 1 ..	IIlrll IlElh / IllEll	IT^TTh .
The same argument applies to estimating the norm lΣj l with relative error : | lΣj l - llΣj ll | ≤ | ll∑j ll - ll∑j ll I +
IlT^TT Ilrlll 一，	，。小 L/ 1	1	∙..	…	…1 .∙ .∙ .	「El	/C
∣ll∑jll - ll∑jl | < C + ci ≤ δμ∕2√η (where here C is the error of the amplitude estimation step used in Theorem 6.8
and C1 is the error in calling Lemma 3.3. Again, we choose C = C1 ≤ δμ∕4√η. Note that K(V0) ≤ κ2(V). This can be
derived from the fact that K(A 0 B) = K(A)κ(B), K(AB) ≤ K(A)κ(B), and
/[ei 0 ei]T∖
V0
. I (V 乳 V).
∖[en 脸 en]T∕
Since the tomography is more costly than the amplitude estimation step, we can disregard the runtime for the norm
estimation step. As this operation is repeated k times for the k different covariance matrices, the total runtime of the
whole algorithm is given by O( kd2nκ2(V)(μ(V0)+n2k3.5^3)*3))).
Let us also note that for each of new computed covariance matrices, we use Lemma 3.1 to compute an estimate for
their log-determinant and this time can be absorbed in the time TΣ .
□
Lemma 6.19 (Quantum estimation of likelihood). We assume we have quantum access to a GMM with parameters
Yt. For Cτ > 0, there exists a quantum algorithm that estimates E[p(Vi; Yt)] with absolute error Cτ in time
T = O (k1∙5η1∙5 κ2(∑)μ⑶)
Proof. We obtain the likelihood from the ability to compute the value of a Gaussian distribution and quantum arith-
metic. Using the mapping of Lemma 3.2 with precision ci, we can compute φ(v∕μj, Σj) for all the Gaussians, that
is |ii Nj-0 ji∣p(vi j; Yj)). Then, by knowing θ, and by using quantum arithmetic we can compute in a register
the mixture of Gaussian's: p(vi； Y) = Pj∙∈[k] θjp(vi j； γ). We now drop the notation for the model Y and write
p(vi) instead of p(vi; γ). Doing the previous calculations quantumly leads to the creation of the state |ii |p(vi)i.
We perform the mapping |i)|p(vi)〉→ |i〉(Pp(Vi)| |0〉+ pl - P(Vi) |1〉) and estimate p(|0〉)` E[p(vi)] with
amplitude estimation on the ancilla qubit. To get a Cτ -estimate of p(0) we need to decide the precision parameter
we use for estimating p(vi j; Y) and the precision required by amplitude estimation. Let p(0) be the ∈ι-error intro-
duced by using Lemma 3.2 and p(0) the error introduced by amplitude estimation. Using triangle inequality we set
p(0) - p
+
p
- p(0) < τ .
El	I ∕c∖	I 一	，一.	.	..	.	1^777	/…	-	J	、	...	.	…1
To have |p(0) - p(0)| < Cτ, we should set C1 such that |p(0) - p(0)| < Cτ /4, and we set the error in amplitude
estimation and in the estimation of the probabilities to be Cτ /2. The runtime of this procedure is therefore:
O k ∙ TG,eτ •
ο (kι.5ηι∙5 ∙ k(”£)
τ
□
6.6	Experiments
We used a subset of the voices that can be found on VoxForge (Voxforge.org). The training set consist in 5 speech
utterances for 34 speakers (i.e. clips of a few seconds of voice speech). From this data, it is common in the speech
20
recognition community to extract the so-called Mel Frequency Cepstrum Coefficients (MFCCs) features (Reynolds
et al., 2000). We selected d = 40 features for each speaker. Then, each speaker is modeled with a mixture of 16
different Gaussians. The test set consists of other 5 unseen utterances of the same 34 speakers (i.e. the training set and
the test set have the same size). The task is to correctly label the unseen utterances with the name of the correct speaker.
This is done by testing each of the GMM fitted during the training with the new voice sample, and selecting the model
with the highest likelihood. Due to the differences in the speakers’ audio data, the different dataset V1 . . . V34 are made
of a variable number of points which ranges from n = 2000 to 4000.
In the vanilla configuration without thresholding, 169 among 170 utterances (5 utterances for all 34 speakers) were
correctly labeled by EM with ML estimate, while all the elements in the test set were correctly recognized using
the Bayesian (MAP) framework. During the training, We measured all the values of κ(Σ), K(V), μ(V), μ(∑). For
almost all GMM fitted (choosing a diagonal covariance matrix), there is at least a Σj (among the 16 used to model a
single speaker) with bad condition number (i.e up to 2500 circa). As in (Kerenidis et al., 2018; Kerenidis & Luongo,
2018) we took a threshold on the matrix by discarding singular values smaller than a certain value m. Practically, we
discarded any singular value smaller than 2 × 10-2. Thresholding the covariance matrices did not impact the accuracy
(the generalization error) significantly. In the MAP estimate, only one element is not correctly classified, while this
number goes up to two in the case of ML estimates. The results are in Table 1.
To check the resistence to noise, we perturbed each of the GMM γt, once fitted. Then, we measured variations of the
accuracy on the test set. For each model, the perturbation consists of three things. First we add to θ a uniform vector
(with randomly selected negative or positive entries) of '2 norm of δθ = 0.035. Then we perturb each centroid μj with
a vector of norm smaller than δμ = 0.5. In this error vector the sign of each noise component is chosen randomly, and
the magnitude is sampled uniformly in the interval (0, √μ). Then, we perturbed also the diagonal matrices Σj with a
vector of norm smaller than δμ√η, where η = 10. As we are using a diagonal GMM, this reduces to perturbing each
singular value by some random noise upper bounded by ±δμ√η∕√d, making sure that each of the singular value
stays positive, as covariance matrices are SPD. Last, the matrices are thresholded. Since the representation of the
model used by our software stores the Cholesky’s decomposition of the inverse, we worked with that representation.
Notably, thresholding the Σj help to mitigate the error of noise and regularize the model. With these parameters,
we were able to correctly label 167 utterances over 170. We leave for the future further experiments with bigger
and different types of datasets or where the noise is also added during the training process. We used scikit-learn
(Pedregosa et al., 2011) to run all the experiments.
6.7	Quantum MAP estimate of GMM
Maximum Likelihood is not the only way to estimate the parameters of a model, and in certain cases might not even
be the best one. For instance, in high-dimensional spaces, it is pretty common for ML estimates to overfit. Moreover,
it is often the case that we have prior information on the distribution of the parameters, and we would like our models
to take this information into account. These issues are often addressed using a Bayesian approach, i.e. by using a
so-called Maximum A Posteriori estimate (MAP) of a model (Murphy, 2012, Section 14.4.2.8). MAP estimates work
by assuming the existence of a prior distribution over the parameters γ. The posterior distribution we use as objective
function to maximize comes from the Bayes’ rule applied on the likelihood, which gives the posterior as a product
of the likelihood and the prior, normalized by the evidence. More simply, we use the Bayes’ rule on the likelihood
function, as p(γ; V) = p(Vγp(γ). This allows us to treat the model Y as a random variable, and derive from the ML
estimate a MAP estimate:
n
YM AP = argmax£log p(γ∣Vi)	(16)
γ i=1
Among the advantages ofa MAP estimate over ML is that it avoids overfitting by having a kind of regularization effect
on the model (Murphy, 2012, Section 6.5). Another feature consists in injecting into a maximum likelihood model
some external information, perhaps from domain experts. This advantage comes at the cost of requiring “good” prior
information on the problem, which might be non-trivial. In terms of labelling, a MAP estimates correspond to a hard
21
clustering, where the label of the point vi is decided according to the following rule:
y = arg max rj = arg max log p(v∕yi = j ； γ) + log P(Iyi = j ； Y)
jj
(17)
Deriving the previous expression is straightforward using the Bayes’ rule, and by noting that the softmax is rank-
preserving, and we can discard the denominator of rij - since it does not depend on γ - and it is shared among all
the other responsibilities of the points vi. Thus, from Equation 16 we can conveniently derive Equation 17 as a proxy
for the label. Fitting a model with MAP estimate is commonly done via the EM algorithm as well. The Expectation
step of EM remains unchanged, but the update rules of the Maximization step are slightly different. In this work we
only discuss the GMM case, for the other cases the interested reader is encouraged to see the relevant literature. For
GMM, the prior on the mixing weight is often modeled using the Dirichlet distribution, that is θj 〜Dir(α). For
the rest of parameters, We assume that the conjugate prior is of the form p(μj, Σj) = NIW(μj, Σj |mo, ∣o, ν0, So),
where NIW(μj, Σj) is the Normal-inverse-Wishart distribution. The probability density function of the NIW is the
product between a multivariate normal φ(μ∣mo, ∣Σ) and a inverse Wishart distribution W-1(Σ∣S0, νo). NIW has
as support vectors μ with mean μo and covariance matrices ∣ Σ where Σ is a random variable with inverse Wishart
distribution over positive definite matrices. NIW is often the distribution of choice in these cases, as is the conjugate
prior of a multivariate normal distribution with unknown mean and covariance matrix. A shorthand notation, let’s
define r7- = nθj = Pn=1 ri7-. As in (Murphy, 2012), we also denote with xjt+1 and Sjttr the Maximum Likelihood
estimate of the parameters (μj+1)ML and (Σj+1)ML. For MAP, the update rules are the following:
θj+1 J n + Pjaj-k
(18)
rj Xjt+1 + ∣omo
rj + ∣o
(19)
Σtj+1 J
So + Sj + + I00rj(XJt+1 - mO)(XJt+1 - mO)T
ν0 + rk + d + 2
(20)
Where the matrix So is defined as:
So := 1 1 Id Diag(S2,…，sd),	QI)
k1/d
where each value Sj is computed as Sj := ɪ Pn=I(Xij - Pn=I Xij))2 which is the pooled variance for each of
the dimension j . For more information on the advantages, disadvantages, and common choice of parameters of a
MAP estimate, we refer the interested reader to (Murphy, 2012). Using the QEM algorithm to fit a MAP estimate is
straightforward, since once the ML estimate of the parameter is recovered from the quantum procedures, the update
rules can be computed classically.
Corollary 6.20 (QEM for MAP estimates of GMM). We assume we have quantum access to a GMM with parameters
γt. For parameters δθ, δμ, CT > 0, the running time of one iteration of the Quantum Maximum A Posteriori (QMAP)
algorithm algorithm is
O(Tθ + Tμ + T∑ + t`),
22
for
Tθ = O H5η1∙5 κ2(∑μ巨)
O (kdηκ(V)(μ(V) + k3∙5η1.5κ2(Σ)μ(Σ))
I	δμ
方(kd2ηκ2(V )(μ(V 0) + η2k3.5κ2(Σ)μ(Σ))
T Σ = °(
Te = O 卜L5η1.5
For the range of parameters of interest, the running time is dominated by TΣ.
δμ
κ2(Σ)μ(Σ)∖
混
23