Under review as a conference paper at ICLR 2020
MixUp as Directional Adversarial Training
Anonymous authors
Paper under double-blind review
Ab stract
MixUp is a data augmentation scheme in which pairs of training samples and
their corresponding labels are mixed using linear coefficients. Without label mix-
ing, MixUp becomes a more conventional scheme: input samples are moved but
their original labels are retained. Because samples are preferentially moved in
the direction of other classes we refer to this method as directional adversarial
training, or DAT. We show that under two mild conditions, MixUp asymptotically
convergences to a subset of DAT. We define untied MixUp (UMixUp), a superset
of MixUp wherein training labels are mixed with different linear coefficients to
those of their corresponding samples. We show that under the same mild con-
ditions, untied MixUp converges to the entire class of DAT schemes. Motivated
by the understanding that UMixUp is both a generalization of MixUp and a form
of adversarial training, we experiment with different datasets and loss functions
to show that UMixUp provides improved performance over MixUp. In short, we
present a novel interpretation of MixUp as belonging to a class highly analogous to
adversarial training, and on this basis we introduce a simple generalization which
outperforms MixUp.
1	Introduction
Deep learning applications often require complex networks with a large number of parameters (He
et al., 2016; Zagoruyko & Komodakis, 2016; Devlin et al., 2018). Although neural networks perform
so well that their ability to generalize is an area of study in itself (Zhang et al., 2017a; Arpit et al.,
2017), their high complexity nevertheless causes them to overfit their training data (Kukacka et al.,
2017). For this reason, effective regularization techniques are in high demand.
There are two flavors of regularization: complexity curtailing and data augmentation 1. Complexity
curtailing methods constrain models to learning in a subset of parameter space which has a higher
probability of generalizing well. Notable examples are weight decay (Krogh & Hertz, 1991) and
dropout (Srivastava et al., 2014).
Data augmentation methods add transformed versions of training samples to the original training
set. Conventionally, transformed samples retain their original label, so that models effectively see
a larger set of data-label training pairs. Commonly applied transformations in image applications
include flips, crops and rotations.
A recently devised family of augmentation schemes called adversarial training has attracted active
research interest (Szegedy et al., 2013; Goodfellow et al., 2014; Miyato et al., 2016; Athalye et al.,
2018; Shaham et al., 2018; He et al., 2018). Adversarial training seeks to reduce a model’s propen-
sity to misclassify minimally perturbed training samples, or adversarials. While attack algorithms
used for testing model robustness may search for adversarials in unbounded regions of input space,
adversarial training schemes generally focus on perturbing training samples within a bounded re-
gion, while retaining the sample’s original label (Goodfellow et al., 2015; Shaham et al., 2018).
Another recently proposed data augmentation scheme is MixUp (Zhang et al., 2017b), in which
new samples are generated by mixing pairs of training samples using linear coefficients. Despite
its well established generalization performance (Zhang et al., 2017b; Guo et al., 2018; Verma et al.,
2018), the working mechanism of MixUp is not well understood. Guo et al. (2018) suggest viewing
MixUp as imposing local linearity on the model using points outside of the data manifold. While this
1Some authors describe these flavors as “data independent” and “data-dependent” (Guo et al., 2018).
1
Under review as a conference paper at ICLR 2020
Figure 1: The relationship between MixUp, DAT and Untied MixUp.
perspective is insightful, we do not believe it paints a full picture of how MixUp operates. A recent
study (Lamb et al., 2019) provides empirical evidence that MixUp improves adversarial robustness,
but does not present MixUp as a form of adversarial training.
We build a framework to understand MixUp in a broader context: we argue that adversarial training
is a central working principle of MixUp. To support this contention, we connect MixUp to a MixUp-
like scheme which does not perform label mixing, and we relate this scheme to adversarial training.
Without label mixing, MixUp becomes a conventional augmentation scheme: input samples are
moved, but their original labels are retained. Because samples are moved in the direction of other
samples - which are typically clustered in input space - We describe this method as ‘directional’.
Because this method primarily moves training samples in the direction of adversarial classes, this
method is analogous to adversarial training. We thus refer to MixUp without label mixing as direc-
tional adversarial training (DAT). We show that MixUp converges to a subset of DAT under mild
conditions, and we thereby argue that adversarial training is a working principle of MixUp.
Inspired by this new understanding of MixUp as a form of adversarial training, and upon realizing
that MixUp is (asymptotically) a subset of DAT, we introduce Untied MixUp (UMixUp), a simple
enhancement of MixUp which converges to the entire family of DAT schemes, as depicted in Figure
1. Untied Mixup mixes data-label training pairs in a similar way to MixUp, with the distinction that
the label mixing ratio is an arbitrary function of the sample mixing ratio. We perform experiments
to show that UMixUp’s classification performance improves upon MixUp.
In short, this research is motivated by a curiosity to better understand the working of MixUp. In-so-
doing we aim to:
1.	Establish DAT as analogous to adversarial training. This is discussed in section 4.
2.	Establish UMixUp as a superset of MixUp, and as converging to the entire family of
DAT schemes. In-so-doing, a) establish MixUp’s convergence to a subset of DAT, and
thereby that it operates analogously to adversarial training; and b) establish UMixUp as a
broader class of MixUp-like schemes that operate analogously to adversarial training. This
is discussed in 5.
3.	Establish empirically that UMixUp’s classification performance improves upon
MixUp. This is discussed in section 6.
Finally we note that this paper has another contribution. Conventionally, MixUp is only applicable
to baseline models that use cross entropy loss. All analytical results we develop in this paper are
applicable to a wider family of models using any loss function which we term target-linear. We
define target-linearity and experiment with a new loss function called negative cosine-loss to show
its potential.
Essential proofs of theoretical results are given in the Appendix.
2
Under review as a conference paper at ICLR 2020
2	Preliminaries
Column vectors are denoted by bold letters such as m, and sets are denoted by calligraphic uppercase
letters such as M. The component of a vector is denoted by a bracketed index. For example, m[i]
denotes the ith component of m.
Regular (non-calligraphic) capitalized letters such as X will denote random variables, and their
lowercase counterparts, e.g., x, will denote realizations of a random variable. Any sequence,
(a1, a2, . . . , an) will be denoted by a1n. Likewise (A1 , A2, . . . , An) will be denoted by A1n, and
a sequence of sample pairs ((x1, x01), (x2, x02), . . . , (xn, x0n)) denoted by (x, x0)1n.
For any value a ∈ [0,1], We will use a as a short notation for 1 - a.
Classification Setting Consider a standard classification problem, in which one wishes to learn a
classifier that predicts the class label for a sample.
Formally, let X be a vector space in which the samples of interest live and let Y be the set of all
possible labels associated with these samples. The set of training samples will be denoted by D, a
subset of X . We will use t(x) to denote the true label of x. Let F be a neural network function,
parameterized by θ, which maps X to another vector space Z. Let 夕：Y → Z be a function that
maps a label in Y to an element in Z such that for any y, y0 ∈ Y, if y = y0, then 夕(y)=夕(y0).
In the space Z, we refer to F(x) as the model’s prediction. With slight abuse of language, we will
occasionally refer to both t(x) and 夕(t(x)) as the “label” of x. Let' : Z×Z→ R bea lossfunction,
using which one defines an overall loss function as
L ：=焉 X ' (F(x),以t(x)))
|D| x∈D
(1)
Here we have taken the notational convention that the first argument of ` represents the model’s pre-
diction and the second represents the target label. In this setting, the learning problem is formulated
as minimizing L with respect to its characterizing parameters θ.
Target-Linear Loss Functions We say that a loss function `(z, z0) is target-linear if for any scalars
α and β,
'(z, αzι + βz2) = α'(z,zι) + β'(z, z2)
Target-linear loss functions arise naturally in many settings, for which we now provide two exam-
ples. For convenience, we define the vectors V = F(x) and y =夕(t(x)).
Cross-Entropy Loss The conventional cross-entropy loss function, written in our notation, is defined
as:
dim(Z)
'ce (F(x), φ(t(χ))) = 'ce(v, y) ：= X y[i]logv[i]
i=1
where v andy are constrained to being probability vectors. We note that in conventional applications,
dim(Z) = |Y|, and the target label v is a one-hot vector where y[i] = 1 if i = t(x) and y[i] = 0
otherwise. Constraining v to being a probability vector is achieved using a softmax output layer.
Negative-Cosine Loss The “negative-cosine loss”, usually used in its negated version, i.e., as the
cosine similarity, can be defined as follows.
'nc (F(x),中(t(χ))) = 'nc(v, y) ：= -vTy
where v and y are constrained to being unit-length vectors. For v this can be achieved by simple
division at the output layer, and for y by limiting the range of 夕 to an orthonormal basis (making it
a conventional label embedding function).
It is clear that the cross-entropy loss 'ce and the negative-cosine loss 'nc are both target-linear,
directly following from the definition of target-linearity.
Assumptions The theoretical development of this paper relies on two fundamental assumptions,
which we call “axioms”.
Axiom 1 (Target linearity) The loss function ` used for the classification setting is target-linear.
3
Under review as a conference paper at ICLR 2020
That is, the study of MixUp in this paper is in fact goes beyond the standard MixUp, which uses the
cross-entropy loss.
Much of the development in this paper concerns drawing sample pairs (x, x0) from D × D. Suppose
that (x, x0)1n is a length-n sequence of sample pairs drawn from D × D. A sequence (x, x0)1n is said
to be symmetric if for every (a, b) ∈ D × D, the number of occurrences of (a, b) in the sequence is
equal to that of (b, a). A distribution Q on D × D will be called exchangeable, or symmetric, if for
any (x,x0) ∈ D × D, Q((x, x0)) = Q((x0,x)).
Axiom 2 (Symmetric pair-sampling distribution) Whenever a sample pair (x, x0) is drawn from a
distribution Q, Q is assumed to be symmetric.
In the standard MixUp, two samples are drawn independently from D to form a pair, making this
condition satisfied.
3	MixUp, DAT, Untied Mixup
3.1	Informal Summary
We first provide a summary of each scheme for the reader’s convenience. We then describe each
scheme more systematically. For concision of equations to follow, we define
y = ψ(t(x))	and	y0 = ψ(t(x0))
MixUp is a data augmentation scheme in which samples are linearly combined using some mixing
ratio λ ∈ [0, 1]:
xg = λx + (1 - λ)x0	(2)
where λ 〜P Mix. A target label is generated using the same mixing ratio λ:
yg = λy + (1 - λ)y0	(MixUp)
DAT and UMixUp use the same method (2) for generating samples, but use different λ distributions
(P DAT and P uMix respectively). DAT and UMixUp also differ from MixUp in their target labels.
DAT retains the sample’s original label:
yg = y	(DAT)
whereas UMixUp’s label mixing ratio is a function of λ:
yg = γ(λ)y + (1 - γ(λ))y0	(UMixUp)
In Untied MixUp, the label mixing ratio is “untied” from the sample mixing ratio, and can be any
γ(λ). We will refer to γ as the weighting function. An Untied MixUp scheme is specified both by
the its mixing policy P uMix and a weighting function γ.
3.2	Formal Definitions
To draw comparisons between MixUp, DAT, and Untied MixUp schemes, we establish a framework
for characterizing their optimization problems. To that end, we define each model’s loss function
`m in terms of its baseline target-linear loss function `b, where the superscript m is replaced with a
model identifier (i.e. 'Mix, 'DAT, 'uMix). Each model's overall loss function, Lm is defined in terms
of its loss function `m as per equation 1 (where equation 1’s ` is `m). We denote the expected value
of each scheme’s overall loss, LEm , with respect to its mixing ratio Λ.
Letn be a positive integer. In every scheme, a sequence (x, x0)1n := ((x1, x01), (x2, x02), . . . , (xn, x0n))
of sample pairs are drawn i.i.d. from Q, and a sequence λ1n := (λ1, λ2, . . . , λn) of values are drawn
i.i.d. from Pm, where Pm is a distribution over [0, 1] unique to each model.
MixUp For any x, x0 ∈ D and any λ ∈ [0, 1], denote
4
Under review as a conference paper at ICLR 2020
'Mix(x, x0, λ) := 'b (F(λx + λx0), λy + λy0)
Let Pm be PMix; in other words a sequence λ1n := (λ1, λ2, . . . , λK) of values are drawn i.i.d. from
PMix. We denote the overall loss LMix ((x, x0)1n, λ1n) and the expected overall loss LEMix ((x, x0)1n):
1n
LMix ((x, χo)n,λn) ：= - £'Mix(xk, χk,λk)	⑶
n k=1
LMix ((χ, x0)n)：= Eλn%P MixLMix ((x, x0)n, λn)
In MixUp, we refer to P Mix as the mixing policy.
Directional Adversarial Training (DAT) For any x, x0 ∈ D and any λ ∈ [0, -], we denote
'DAT(x, x0,λ) := 'b (F(λx + λx0), y)
Let Pm be PDAT, such that members of λ1n are drawn i.i.d. from PDAT. We denote the overall loss
LDAT ((x, x0)1n, λ1n}) and the expected overall loss LEDAT ((x,x0)1n):
-n
L	((x, x0)n,λn}) := n X 'DAT (Xk, x0k,λk)	(4)
n k=1
LDAT ((x, x0)n) ：= Eλn %P DAT LDAT ((x, x0)n,λn)
In DAT, we refer to P DAT as the adversarial policy.
Untied MixUp (UMixUp)
Let γ be a function mapping [0, -] to [0, -]. For any x, x0 ∈ D and any λ ∈ [0, -], we denote
'uMix(x, x0, λ, Y) := 'b(F(λx + λx0), γ(λ)y) + γ(λ)y0)
Let Pm be P uMix, and denote the overall and expected overall loss functions LuMix ((x, x0)1n, λ1n, γ)
and LuEMix ((x, x0)1n, γ) respectively:
-n
LUMix ((x, x0)n, λn, γ) := - E 'uMix(x, x0, λ, γ)
n k=1
LEMix ((x, x0)n, Y) := EλniMP UMixLUMix ((x, x0)n, λn, Y)
At this end, it is apparent that MixUp is a special case of Untied MixUp, where the function γ(λ)
takes the simple form Y(λ) = λ.
4	DAT as Analogous to Adversarial Training
The main theoretical result of this paper is the relationship established between DAT and UMixUp,
and by extension MixUp. Both MixUp and UMixUp will be shown to converge to DAT as the
number of mixed sample pairs, n, tends to infinity. Prior to developing these results, we provide
insight into DAT, in terms of its similarity to adversarial training and its regularization mechanisms.
Conventional adversarial training schemes augment the original training dataset by searching for
approximations of true adversarials within bounded regions around each training sample. For a
training sample x, a bounded region U known as an Lp ball is defined as U = {x + η : ∣∣η∣∣p < e}.
Over this region, the loss function with respect to the true label of x is maximized. A typical loss
function for an adversarial scheme is
'(F(x), y) = max 'b(F (x), y)
5
Under review as a conference paper at ICLR 2020
where `b is the baseline loss function. Simply put, baseline training serves to learn correct clas-
sification over the training data, whereas adversarial training moves the classification boundary to
improve generalization.
DAT, on the other hand, combines intra-class mixing (mixing two samples of the same class) and
inter-class mixing (mixing samples of different classes). Intra-class mixing serves to smooth classi-
fication boundaries of inner-class regions, while inter-class mixing perturbs training samples in the
direction of adversarial classes, which improves generalization. Inter-class mixing dwarves intra-
class mixing by volume of generated samples seen by the learning model in most many-class learn-
ing problems (by a 9-1 ratio in balanced 10-class problems for instance). DAT, which primarily
consists of inter-class mixing, can therefore be seen as analogous to adversarial training.
The key distinction between conventional adversarial training and inter-class mixing is that MixUp
movement is determined probabilistically within a bounded region, while adversarial movement is
deterministic.
Figure 2 illustrates the connection between standard adversarial training and DAT. Consider the
problem of classifying the blue points and the black points in Figure 2a), where the dashed curve
is a ground-truth classifier and the black curve indicates the classification boundary of F (x), which
overfits the training data. In adversarial training, a training sample x is moved to a location within
an Lp -ball around x while keeping its label to further train the model; the location, denoted by bx1 in
Figure 2b), is chosen to maximize training loss.
(a) Baseline training
Figure 2: DAT as a form of adversarial training
(b) Adversarial training
In DAT, a second sample x0 governs the direction in which x is perturbed. If x0 is chosen from
a different class as shown in Figure 2c), then the generated sample bx2 is used to further train the
model. If x0 is chosen from the same class as shown in Figure 2d), then the sample bx3 is used
in further training. Note that the inter-class mixed sample bx2 pushes the model’s classification
boundary closer to the ground-truth classifier, thus connecting adversarial training and DAT. The
intra-class sample bx3, on the other hand, mainly helps to smooth inner parts of the class region. The
latter behaviour is an additional feature of DAT and MixUp, which distinguishes these schemes from
adversarial training.
5	Untied MixUp as Asymptotically Equivalent to DAT
We now show that Untied MixUp and DAT are equivalent when n tends to infinity. A consequence of
this equivalence is that it infuses both MixUp and UMixUp with the intuition of adversarial training.
To that end, We relate the Untied MixUP loss function, 'uMix, with the DAT loss function, 'dat.
Lemma 1 For any (x, x0) ∈ D × D and any λ ∈ [0, 1],
'uMix(x, x0,λ,γ) = γ(λ)'DAT (x, x0,λ)+ EAT(X0, x,λ)
This result follows directly from the target-linearity of the loss function.
The next two lemmas show that as n tends to infinity, the overall loss of both DAT and UMixUP
converge in Probability to their resPective overall exPected losses.
Lemma 2 As n increases, LDAT ((x, x0)1n, Λ1n) converges to LEDAT (x, x0)1n in probability.
6
Under review as a conference paper at ICLR 2020
Lemma 3 As n increases, LUMiX ((χ, χ0)n, Λ7, Y) converges to LEMiX ((x, x0)； , Y) in probability.
These two lemmas have similar proofs, thus only the proof of Lemma 2 is given in section A.1.
Next we show that as n tends to infinity, UMixUp converges in probability to a subset of DAT, and
DAT converges in probability to a subset of UMixUp. In other words, we show that as n increases,
UMixUp converges to being equivalent to the entire class of DAT schemes.
For that purpose, let F denote the space of all functions mapping [0, 1] to [0, 1]. Each configuration
in P × F defines an Untied MixUp scheme.
We now define U, which maps a DAT scheme to an Untied MixUp scheme. Specifically U is a map
from P to P × F such that for any p ∈ P, U(p) is a configuration (p0, g) ∈ P × F, where
p0(λ) := 1 (p(λ) + P(I - λ)) and g(λ) := p(λ) +p1 — λ)	(5)
Lemma 4 Let (x, x0)1n be a sequence of sample pairs on which an Untied MixUp scheme specified
by (PuMiX, Y) and a DAT scheme with policy PDAT will apply independently. If (x, x0)1n is symmetric
and (P uMix,γ) = U(P DAT) ,then LUMiX ((x, x0)n,γ) = LDAT ((x, x0)n).
We now define another map Du that maps an Untied MixUp scheme to a DAT scheme. Specifically
Du is a map from P × F to P such that for any (p, g) ∈ P × F, Du(p, g) is a configuration p0 ∈ P,
where	_____
P0(λ) := (g(λ)p(λ) + g(λ)p(1 - λ))
It is easy to verify that R01 p0(λ)dλ = 1. Thus p0 is indeed a distribution in P and Du is well defined.
Lemma 5 Let (x, x0)1n be a sequence of sample pairs on which an Untied MixUp scheme specified
by (PuMiX, Y) and a DAT scheme with policy P DAT will apply independently. If (x, x0)1n is symmetric
and P DAT = Du (P uMix, γ), then LUMiX ((x, x0)n, Y) = LDAT ((x, x0)n).
Lemmas 2, 3, 4 and 5 provide the building blocks for theorem 1, which we state hereafter. As n
increases, both DAT and UMixUp converge in probability toward their respective expected loss (lem-
mas 2 and 3). Since as n increases, the sequence (x, x0)1n becomes arbitrarily close to the symmetric
sampling distribution Q, then by lemma 4 the family of DAT schemes converges in probability to
a subset of UMixUp schemes. Lemma 5 proves the converse, i.e. that as n increases the family of
UMixUp schemes converges in probability to a subset of DAT schemes. As n increases, the family
of UMixUp schemes therefore converges in probability to the entire family of DAT schemes.
Theorem 1 Let (X, X0)1∞ be drawn i.i.d. from Q. On this sample-pair data, an Untied
MixUp scheme specified by (PMiX, Y) and a DAT scheme specified by PDAT will apply. In
the Untied MixUp scheme, let Λ1∞ be drawn i.i.d. from PMiX; in the DAT scheme, let Υ1∞
be drawn i.i.d. from P DAT. If P DAT = Du (P Mix,γ) or (P Mix,γ) = U(P DAT), then
LMiX ((X, X0)1n, Λ1n, Y) - LDAT ((X, X0)1n, Υ1n) -p→ 0, asn → ∞
The equivalence between the two families of schemes also indicates that there are DAT schemes that
do not correspond to a MixUp scheme. These DAT schemes correspond to Untied MixUp scheme
beyond the standard MixUp. The relationship between MixUp, DAT and Untied MixUp is shown in
Figure 1.
6	UMixUp as a Useful Generalization of MixUp: Experiments
6.1	Experiment Setup and Implementation
We consider an image classification task on the Cifar10, Cifar100, MNIST and Fashion-MNIST
datasets. The baseline classifier chosen is PreActResNet18 (see Liu (2017)), noting the same choice
is made by the authors of MixupZhang et al. (2017b).
Both MixUp and Untied MixUp are considered in the experiments. The MixUp policies are chosen
as Beta distribution B(α, β). The Untied MixUp policy is taken as U(B(α, β)).
7
Under review as a conference paper at ICLR 2020
Two target-linear loss functions are essayed: cross-entropy (CE) loss and the negative-cosine (CE)
loss as defined earlier. We implement CE loss similarly to previous works, which use CE loss to
implement the baseline model. In our implementation of the NC loss model, for each label y,夕(y)
is mapped to a randomly selected unit-length vector of dimension d and fixed during training; the
feature map of the original PreActResNet18 is linearly transformed to a d-dimensional vector. The
dimension d is chosen as 300 for Cifar10, MNIST and Fashion-Mnist (which have one black-and-
white channel) and 700 for Cifar100 (which has 3 colored channels).
Our implementation of MixUp and Untied MixUp improves upon the published implementation
from the original authors of MixUp Zhang et al. (2017b). For example, the original authors’ imple-
mentation samples only one λ per mini-batch, giving rise to unnecessarily higher stochasticity of the
gradient signal. Our implementation samples λ independently for each sample. Additionally, the
original code combines inputs by mixing a mini-batch of samples with a shuffled version of itself.
This approach introduces a dependency between sampled pairs and again increases the stochasticity
of training. Our implementation creates two shuffled copies of the entire training dataset prior to
each epoch, pairs them up, and then splits them into mini-batches. This gives a closer approxima-
tion to i.i.d. sampling and makes training smoother. While these implementation improvements
have merit on their own, they do not provide a theoretical leap in understanding, and so we do not
quantify their impact in our results analysis.
All models examined are trained using mini-batched backpropagation, for 200 epochs.
6.2	Results
We sweep over the policy space of MixUp and Untied MixUp. For MixUp, itis sufficient to consider
distribution PMix to be symmetric about 0.5. Thus we consider only consider PMix in the form of
B(α, α), and scan through a single parameter α systematically. Since the policy of Untied MixUp is
in the form of U(B(α, β)), searching through (α, β) becomes more difficult. Thus our policy search
for Untied MixUp is restricted to an ad hoc heuristic search. For this reason, the found best policy
for Untied MixUp might be quite far from the true optimal.
The main results of our experiments are given in tables 1 to 4. As shown in the tables, each setting
is run 100 times. For each run, we compute the error rate in a run as the average test error rate over
the final 10 epochs. The estimated mean (“MEAN”) performance of a setting is computed as the
average of the error rates over all runs for the same setting. The 95%-confidence interval (“ConfInt”)
for the estimated mean performance is also computed and shown in the table.
From these results, we see that the Untied MixUp schemes each outperform their MixUp counter-
parts. Specifically, in 6 of the 8 cases (those printed in bold font), the confidence interval of Untied
MixUp is completely disjoint from that of the corresponding MixUp scheme; and in some cases, the
separation of confidence intervals is by a large margin. Note that the baseline model (PreActRes-
Net18) has been designed with highly focused inductive bias for image classification tasks. Under
such an inductive bias, one expects that the room for regularization (or the “amount of overfitting”)
isn’t abundant. As such, we consider the improvement of Untied MixUp over MixUp rather signifi-
cant.
The results show empirically that MixUp and Untied MixUp both work on the NC loss models. This
validates our generalization of MixUp (and Untied MixUp) to models built with target linear losses.
model	policy	runs	MEAN	ConfInt
baseline-CE	-	∏0δ-	5.476%	0.027%
mixUp-CE	-B(0.9,0.9)-	^10δ-	4.199%	0.023%
uMixUp-CE-	U(B(2.2,0."	^lδδ^	4.177%	0.025%
baseline-NC	-	-W	5.6δ5%	0.030%
mixUp-NC	-B(1.0,1.0)-	^lδδ^	4.5δ8%	0.022%
UMixUp-NC~	U(B(1.8,1.0)T	~!δδ~	4.455%	0.025%
Table 1: Test error rate on CIFAR10.
8
Under review as a conference paper at ICLR 2020
model	policy	runs	MEAN	ConfInt
baseline-CE	-	^tg0^	24.848%	0.060%
-mixUp-CE-	-B(0.9, 0.9)-	^tg0^	22.020%	0.050%
UMixUp-CE-	U(B(1.4, 0.7)T	IGO-	21.884%	0.051%
baseline-NC	-	IGO-	25.270%	0.050%
mixUp-NC	-B(0.9, 0.9)-	IGO-	24.298%	0.051%
UMixUp-NC~	U(B(1.3, 0.9)T	ΠGG-	23.819%	0.054%
Table 2: Test error rate on CIFAR100.
model	policy	runs	MEAN	ConfInt
baseline-CE	-	IGG-	0.816%	0.007%
mixUp-CE	-B(1.0,1.0)-	IGG-	0.632%	0.005%
UMixUp-CE-	U(B(1.7,1.0Tr	ΠGG-	0.609%	0.005%
baseline-NC	-	^GG=	0.720%	0.007%
mixUp-NC	-B(1.0,1.0)-	TOG-	0.607%	0.004%
UMixUp-NC~	U(B(1.3,0.9)T	ΠGO-	0.592%	0.005%
Table 3: Test error rate on MNIST.
model	policy	runs	MEAN	ConfInt
baseline-CE	-	"TOO-	5.060%	0.027%
mixUp-CE	-B(1.0,1.0)-	"TOO-	4.585%	0.013%
uMixUp-CE-	U(B(1.7,0.8)Γ	TOG-	4.570%	0.013%
baseline-NC	-	∏G0-	5.083%	0.016%
mixUp-NC	-B(1.0,1.0)-	TOG-	4.767%	0.013%
UMixUp-NC~	U(B(1.3,0.9)T	Π50~	4.613%	0.011%
Table 4: Test error rate on Fashion-MNIST.
7	Concluding Remarks
This paper establishes a connection between MixUp and adversarial training. This connection allows
for a better understanding of the working mechanism of MixUp as well as a generalization of MixUp
to a wider family, namely Untied MixUp. Despite the development in this work, it is the authors’
belief that the current designs of MixUp and Untied MixUp are far from optimal. In particular, we
believe a better design should allow individualized policy for each training pair. How this can be
done remains open at this time.
References
Devansh Arpit, Stanislaw K. Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxin-
der S. Kanwal, Tegan Maharaj, Asja Fischer, Aaron C. Courville, Yoshua Bengio, and Simon
Lacoste-Julien. A closer look at memorization in deep networks. In Proceedings of the 34th In-
ternational Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August
2017, pp. 233-242, 2017. URL http://Proceedings .mlr.press∕v70∕arpit17a.
html.
Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of
security: Circumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420,
2018.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep
bidirectional transformers for language understanding. CoRR, abs/1810.04805, 2018. URL
http://arxiv.org/abs/1810.04805.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014.
9
Under review as a conference paper at ICLR 2020
Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In ICLR 2015, 2015.
Hongyu Guo, Yongyi Mao, and Richong Zhang. Mixup as locally linear out-of-manifold regular-
ization. arXiv preprint arXiv:1809.02499, 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Warren He, Bo Li, and Dawn Song. Decision boundary analysis of adversarial examples. In 6th In-
ternational Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April
30 - May 3, 2018, Conference Track Proceedings, 2018. URL https://openreview.net/
forum?id=BkpiPMbA-.
Anders Krogh and John A. Hertz. A simple weight decay can improve generalization. In
Advances in Neural Information Processing Systems 4, NIPS Conference, Denver, Colorado,
USA, December 2-5, 1991, pp. 950-957, 1991. URL http://papers.nips.cc/paper/
563- a- simple- weight- decay- can- improve- generalization.
Jan Kukacka, Vladimir Golkov, and Daniel Cremers. Regularization for deep learning: A taxonomy.
CoRR, abs/1710.10686, 2017.
Alex Lamb, Vikas Verma, Juho Kannala, and Yoshua Bengio. Interpolated Adversarial Training:
Achieving Robust Neural Networks without Sacrificing Too Much Accuracy. arXiv e-prints, art.
arXiv:1906.06784, Jun 2019.
Kuang Liu. URL https://github.com/kuangliu/pytorch- cifar, 2017.
Colin McDiarmid. On the method of bounded differences. Surveys in combinatorics, 141(1):148-
188, 1989.
Takeru Miyato, Andrew M Dai, and Ian Goodfellow. Adversarial training methods for semi-
supervised text classification. arXiv preprint arXiv:1605.07725, 2016.
Uri Shaham, Yutaro Yamada, and Sahand Negahban. Understanding adversarial training: Increasing
local stability of supervised models through robust optimization. Neurocomputing, 307:195-204,
2018.
Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learn-
ing Research, 15(1):1929-1958, 2014. URL http://dl.acm.org/citation.cfm?id=
2670313.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Aaron Courville, Ioannis Mitliagkas,
and Yoshua Bengio. Manifold mixup: Learning better representations by interpolating hidden
states. 2018.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In Proceedings of the British
Machine Vision Conference 2016, BMVC 2016, York, UK, September 19-22, 2016, 2016. URL
http://www.bmva.org/bmvc/2016/papers/paper087/index.html.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In 5th International Conference on Learning
Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings,
2017a. URL https://openreview.net/forum?id=Sy8gdB9xx.
Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical
risk minimization. arXiv preprint arXiv:1710.09412, 2017b. URL https://github.com/
facebookresearch/mixup-cifar10.
10
Under review as a conference paper at ICLR 2020
A Appendix
A.1 Proof of Lemma 2:
For any fixed infinite sequence (x, x0)1∞ of samples drawn i.i.d. from Q and any infinite sequence of
i.i.d. random variables Λ1∞ drawn from PDAT, let LDAT ((x, x0)1n, Λ1n) be defined according to (4),
with the first n elements of (x, x0)1∞ and the first n elements of Λ1∞ as input. Define
δDAT :
max
(x,x0 )∈
D×D
sup
(λ,λ0)∈
[0,1]×[0,1]
l`DAT (x, χ0, λ) - 'DAT (x, χ0, λ0)∣
For any given λ1n ∈ [0, 1]n and any of its modified version u1n ∈ [0, 1]n which differs from λ1n in
exactly one location, it can be verified, following the definition of δDAT, that
ILDAT ((x, x0)n, λn) - LDAT ((x, x0)n, Un)I ≤ SDATln
Since Λ1, Λ2, . . . ΛK are independent and by McDiarmid Inequality McDiarmid (1989), it follows
that for any > 0,
Pr [LDAT ((X, XO)n, An)- LEAT ((X,XO)n) ≥ e] < 2exp ——-——K
∖ n ∙ (Sdat/n))
which proves the lemma
A.2 Proof of Lemma 4:
[ n
LEMix ((x, x0)n, Y) := n X Eλ〜PMix {γ(λ)'DAT(Xk, Xk,λ) + γ(λ)'DAT(Xk, Xk, λ)}
n k=1
=1 X/ (γ(λ)PMix(λ)'DAT(Xk,Xk,λ) +Y(λ)PMix(λ)'DAT(Xk,Xk,λ)) dλ
=1 X	PDAT(λ)'DAT(Xk, xk, λ) +1 PDAT(λ)'DAT(Xk, Xk,λ)) dλ
n2
k=1
n
PDAT (λ)'DAT(Xk, x0k ,λ)dλ + X PDAT (λ)'DAT(Xk, Xk ,λ)dλ
k=1
n
PDAT(λ)'DAT(Xk, Xk,λ)dλ + X / PDAT(λ)'DAT(Xk, Xk,λ)dλ
k=1
n
PDAT(λ)'DAT(Xk, Xk,λ)dλ + X / PDAT(λ)'DAT(Xk, Xk,λ)dλ
k=1
=1 XZ PDAT(λ)'DAT(Xk, Xk,λ)dλ
=LEDA=T((x,xO)1n)
where (a) is due to a change of variable in the integration, (b) is due to the symmetry of (X, XO)1n.
Note that in equation 5 g(λ) is undefined at values of λ for which the denominator is zero. But the
lemma holds true because the denominator is only zero when p(λ) = 0, so those λ for which g(λ)
is undefined never get drawn in the DAT scheme.
11
Under review as a conference paper at ICLR 2020
A.3 Proof of Lemma 5:
LuEMix((x,x0)1n,γ)
n	n
nEλ^pMix X (γ(λ)'DAT(xk, Xk,λ) + γ(λ)'DAT(Xk, Xk,λ))
n	k=1
nn
n I Eλ~pMiχ X γ(λ)'DAT(Xk, x'k,λ) + Eλ~pMiχ X γ(λ)'DAT(x'k, Xk,λ)
k=1
k=1
(=a)
(=b)
nn
E I Eλ~pMiχ X γ(λ)'DAT(Xk, X0k,λ) + Eλ~pMix X γ(λ)'DAT(Xk, X0k,λ)
n	k=1	k=1
1 (	n	n _
n I Eλ~pMiχ E γ(λ)'DAT (Xk, X0k ,λ) + Eλ~pMix E γ(λ)'DAT (Xk, X0k ,λ)
k=1
k=1
n X / (γ(λ)P Mix(λ)'DAT(Xk, Xk ,λ)+ Y(λ)P Mix(I-λ)'DAT(Xk, Xk ,λ))dλ
k=1
n X Z 'DAT(Xk, Xk, λ) (γ(λ)PMix(λ) + 而PMix(I- λ)) dλ
n k = 1 J	S----------------{Z----------------}
Du (PMix,γ)
1
一Eλ~pDAT`	(Xk, Xk, λ)
n
k=1
LDEAT ((X,X0)1n ).
where (a) is due to the symmetry of (X, X0)1n , and (b) is by a change of variable in the second term
(renaming 1 - λ as λ).
12