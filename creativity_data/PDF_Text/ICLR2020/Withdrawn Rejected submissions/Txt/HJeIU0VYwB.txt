Under review as a conference paper at ICLR 2020

ADA+:  A GENERIC  FRAMEWORK  WITH  MORE  ADAP-
TIVE  EXPLICIT  ADJUSTMENT  FOR  LEARNING  RATE

Anonymous authors

Paper under double-blind review

ABSTRACT

Although adaptive algorithms have achieved significant success in training deep
neural  networks  with  faster  training  speed,  they  tend  to  have  poor  generaliza-
tion  performance  compared  to  SGD   WITH   MOMENTUM(SGDM).  One  of  the
state-of-the-art algorithms,  PADAM, is proposed to close the generalization gap
of     adaptive  methods  while  lacking  an  internal  explanation.    This  work  pro-
poses  a  general  framework,  in  which  we  use  an  explicit  function  Φ( )  as  an
adjustment  to  the  actual  step  size,  and  present  a  more  adaptive  specific  form
ADAPLUS(ADA+).    Based  on  this  framework,  we  analyze  various  behaviors
brought            by different types of Φ( ), such as a constant function in SGDM, a lin-
ear  function  in  ADAM,  a  concave  function  in  PADAM  and  a  concave  function
with  offset  term  in  ADAPLUS.   Empirically,  we  conduct  experiments  on  clas-
sic  benchmarks  both  in  CNN  and  RNN  architectures  and  achieve  better  per-
formance(even  than  SGDM).  The  code  is  anonymously  provided  in  https:

//anonfiles.com/daV7Ed6enb/AdaPlus_zip.

1    INTRODUCTION

First-order optimization algorithms play a significant role in training deep neural networks.  One
of the dominant methods is  SGD(Robbins & Monro, 1951),  which updates parameters in a very
concise form.  SGD  WITH  MOMENTUM(Polyak, 1964) is a momentum-based improvement of SGD
that combines raw gradients to reduce the interference of noise gradients and has proved itself to
be an efficient first-order optimization algorithm. Besides, adaptive variants of SGD, such as ADA-
GRAD  (Duchi et al., 2011),  RMSPROP(Tieleman & Hinton, 2012),  ADAM(Kingma & Ba, 2015),
have recently emerged and achieved success due to their convenient and fast automatic learning rate
adjustment mechanisms.  However, since ADAM  is indicated not to converge in certain instances,
AMSGRAD(Reddi et al., 2018) suggests maintaining a non-increasing step size by a maximum his-
torical         term, so as to deal with the non-convergence problem.

On the one hand, adaptive methods are widely applied due to their quicker convergence at an early
stage.   However,  there remain potential perils of adaptivity (Wilson et al., 2017),  which suggest
worse  performance  than  the  fine-tuned  SGD/SGDM.  Therefore,  many  architectures  in  the  
field
of  computer  vision,  such  as  VGGNet(Simonyan  &  Zisserman,  2015),  ResNet(He  et  al.,  2016),
DenseNet(Huang  et  al.,  2017),  and  natural  language  processing  tasks(Klein  et  al.,  2017), 
 would
be inclined to choose SGD-like algorithms to act as an optimizer.  On the other hand, the ‘small
learning rate dilemma’(Chen & Gu, 2018) is also a challenge for adaptive methods,  that is,  it is
difficult to apply the well-worked learning rate decaying strategy in SGD to adaptive gradient meth-
ods.  With better performance, the PADAM  algorithm(Chen & Gu, 2018) is proposed to solve this
problem. However, the motivation of this partially adaptive momentum fails to explain the intrinsic
implication of doing grid search for the power p of the second-order moment estimation, along with
the relationship between the desired learning rate and the partially adaptive term.

In this work, we first propose an adaptive adjustment function, which is a map based on histori-
cal gradient information to adjust the actual step size, and a framework that can explain the above
phenomenon explicitly.  Through our analysis of this function, which combines the SGD-like algo-
rithm  with the ADAM-like algorithm, we further explain the different behaviors of different algo-
rithms.  Moreover, we propose a brief but efficient specific form, called ADAPLUS(ADA+), with
better adaptability to cope with the aforementioned challenges effectively. Empirically, we conduct

1


Under review as a conference paper at ICLR 2020

experiments on popular models and tasks both in CNN and RNN architectures with  ADAPLUS.
Comparing favorably to SGD-like approaches, it gains faster convergence, reduced oscillation and
better performance.  Compared with ADAM-like methods, it can adapt to a variable learning rate
schedule, thus achieving significant improvement in performance in complex deep neural network
architectures.

2    PRELIMINARIES  AND  MOTIVATIONS

2.1    NOTATIONS

As is generally agreed, we use lowercase letters a for scalars, lowercase bold letters a for 
vectors,
and uppercase bold letters A for matrices. Then, we set θt    Rd as a parameter vector of a sequence
in d-dimensional space, and use the scalar θt,i to represent the i-th elements in vector θt. The 
set of

all positive definite d×d matrices is denoted by Sd . Besides, operators are defined as following. 
For

+

vectors a and b in the same dimension, we use √    to donate element-wise squares, a/b to donate

element-wise divisions. These notations are the same for matrices. For a given matrix A ∈ Sd  and a


vector

R  , we use

as a projection operator which means

¨           +      ¨,


b ∈     d

ΠX ,A(b)

arg minₐ∈X ¨A¹/²(a − b)¨


where      is a convex set which has a bounded diameter D   , i.e.,   a     b

Additionally, with a mild abuse of the notation of norms, we represent the L

exponential moving average (EMA) of gt like this:

1

ǁgtǁp , v p

D   ,   a, b         .

norm formation of an

p                            p          p  ¹


= [β2 vt−₁ + (1 − β2 ) |gt|  ] p

(2.1)


2.2    MOTIVATIONS

= Σ(1 − βᵖ)

Σi=1

1

p

βp(t−i) · |gi|p

As is described in Figure 1, for optimizers, we treat their adjustment term of αt as a mapping from

ǁgtǁp to the actual adjustment value, i.e.,


θt+1

= θt

      αt     

−  Φ(ǁg ǁ  ) · mt

(2.2)

To avoid confusion, we call αt the learning rate, which is a parameter that needs tuning and 
applying

a decaying schedule.  Apart from that,       αt           is called the actual step size.  In this 
section, we

t   p

summarize popular first-order stochastic optimization algorithms into the following four 
categories:

Figure 1:  Different choices for Φ(ǁgtǁp),  changing from Adam to SGD. We propose a specific

function Φ(ǁgtǁp) ,    ǁgtǁp + ∆, which is shown above.

2


Under review as a conference paper at ICLR 2020

•  Type I    SGDM (Constant function):


θ      = θ

α

−        · m , i.e., Φ(ǁg ǁ  ) = 1.                         (2.3)

It can be seen from the formula in SGD  WITH  MOMENTUM(SGDM)that the actual step
size of each update equals to αt, which is applied without adaptive adjustments. The high
learning rate in an early stage makes the loss function fluctuate significantly on the surface,
and its learning speed is not as fast as adaptive optimization algorithms and their variants.

•  Type II    ADAM (Linear function):


θ      = θ

   αt  

−              · m , i.e., Φ(ǁg ǁ  ) = ǁg ǁ  .                   (2.4)


t+1

ǁgtǁ₂

t   2           t   2

Extremes of the learning rate(Wilson et al., 2017) in ADAM-like algorithms have brought
many issues.  As is shown in Figure 1, when the gradient is very large, the function value
grows greatly in a linear rate, which leads to a rapid decrease in the step size; on the other
hand, once the gradient is extremely small (ie, when the norm of EMA of gt approaches
to 0), Φ(  gt  p)  =    gt  p is very close to 0, which makes it more than essential to select
a much smaller learning rate(a few orders of magnitude smaller than SGDM) to get a rea-
sonable actual step size. However, the much smaller initial learning rate makes ADAM-like
algorithms not flexible enough to adapt to the various learning rate schedule.  Although
their early convergence is faster, the final performance tends to be poor due to the small ac-
tual step size in the final stage. A proposed solution, ADAMW(Loshchilov & Hutter, 2019),
trying to change the way the weight attenuation is updated, achieves a better generaliza-
tion performance,  but there still remains a gap between adaptive methods and  SGD-like
methods.

•  Type III    PADAM (Concave function):

αt                                                    √4

θ      = θ  −              · m , i.e., Φ(ǁg ǁ  ) =      ǁg ǁ  .                  (2.5)

Note that the p here means partial rather than Lᵖ norm, its default setting is ¹ ¹.

PADAM has achieved superior performance(Chen & Gu, 2018) in computer vision experi-
ments, but this algorithm only introduces a new hyper-parameter p and does grid searches
for it without realizing the essential reason for its improvement relative to  ADAM.  The
internal cause is that a concave function is applied rather than the linear function in ADAM.
Once ε is extremely small and   gt  ₂    (0, ε), the mapping value of Φ( ) would be much
larger in PADAM  than in ADAM; therefore,  PADAM  can adapt to larger learning rate αt,
thus flexibly adapting to the variable learning rate scheme.

•  Type IV    ADAPLUS(ADA+) (Concave function with offset):

αt                                                           √                           (2.6)

θ      = θ  −  √                 · m , i.e., Φ(ǁg ǁ  ) =     ǁg ǁ   + ∆.

In ADAPLUS, we extend the concave function by introducing an offset ∆ and present a
default setting as above. This form of Φ( ) not only directly inherits advantages of PADAM,
as is depicted in Figure 1, but also makes a better guarantee for larger learning rates.  The
offset  ∆  makes  sure  that  Φ( )  can  altogether  avoid  the  extreme  situation.   Even  when
gt  ₁      0, a more extensive learning rate αt is allowed.  Besides, when   gt  ₁ is rela-
tively large, we can also adaptively constrain the updates and achieve ‘naturally annealing’
like ADAM, but more moderately. Thus, we reckon that the proposed algorithm has better
adaptive performance than both ADAM-like and SGD-like algorithms, which is why we call
it ADAPLUS(ADA+).

So  far,  we  may  surprisingly  notice  that  this  offset  ∆  is  the  ε that  we  used  to  
apply  in
optimizers indeed.  Nevertheless, the previous ε is only used to avoid dividing by zero to
keep the numerical stability. In fact, through our analysis above, the design of offset ∆ can
effectively avoid the occurrence of extreme step sizes, thus saliently improving adaptivity
and performance.

¹It is a bit different from the formation in (Chen & Gu, 2018), but actually the same.

3


Under review as a conference paper at ICLR 2020

We present a default setting as above, due to its excellent performance and concise form;
however, there would be more elegant forms remaining for us to explore in the future. For
instance, we can replace the square function with log( ) or tanh( ) and so on, or tune it
into a p-th power function to apply a PADAM-like grid search.

3    THE  PROPOSED  FRAMEWORK  AND  METHOD

3.1    GENERIC FRAMEWORK

As is shown in Section 2.2, we propose a generic framework that includes SGD-like and ADAM-like
algorithms. With the adaptive adjustment function Φ(  gt  p) to combine algorithms in an organized
way, behavioral characteristics of different algorithms can be explicitly observed.

Algorithm 1 GENERIC FRAMEWORK


Input: θ₁ ∈ X; learning rate {αt}T

; momentum parameters {β₁t}T

, β₂; Lᵖ norm parameter p;

function for step estimation Φ(·).


1:  Set m₀

2:  for

= 0, v₀

to

= 0  ^  = 0

3:         gt = ∇ft(xt)

4:         mt = β₁tmt−₁ + (1 − β₁t)gt

5:         vt = β₂vt−₁ + (1 − β₂)|gt|p

vt = max (vt  ₁, vt)                                     

7:


8:         θt₊₁ = Π

,   −1

1/p  , .θt −      αt

· mtΣ


9:  end for

X ,diag  Φ

(v^t    )               Φ(v^t    )

10:  // We omit the bias-correction terms and other misc for clarity.

According to our analysis, different settings of the adaptive adjustment function Φ(  gt  p) will 
lead
to different behaviors of the algorithm, such as Φ(  gt  p)  =  1 for SGDM and Φ(  gt  p)  =    gt  
p
for ADAM.  In practice, we can selectively decide whether or not to open AMSGRAD according to
the actual situation.  We will present a specific algorithm that has a concise form and demonstrates
superior performance in Section 5.

3.2    SPECIFIC FORMATION FOR ADAPLUS

Algorithm 2 ADAPLUS


Input: θ₁ ∈ X; learning rate {αt}T

; momentum parameters {β₁t}T

, β₂; offset term ∆.


1:  Set m₀

2:  for

= 0, v₀

to

= 0  ^  = 0

3:         gt = ∇ft(xt)

4:         mt = β₁tmt−₁ + (1 − β₁t)gt

5:         vt = β₂vt−₁ + (1 − β₂)|gt|

vt = max (vt  ₁, vt)

7:

8:         θt+1 = Π           √                .θt −  √  αt         · mtΣ

10:  // We omit the bias-correction terms and other misc for clarity.

Here, we express the specific algorithm corresponding to the Type IV function mentioned in Section

2.2 as following. It is a special case of Algorithm 1.

αt                                                           √                                 
(3.1)

θ      = θ  −  √                 · m , i.e., Φ(ǁg ǁ  ) =     ǁg ǁ   + ∆.

 

algorithm can avoid extreme actual step sizes and achieve superior empirical results.

4


Under review as a conference paper at ICLR 2020

4    CONVERGENCE  ANALYSIS  OF  ADAPLUS

In this section,  we provide a convergence analysis based on a standard online convex optimiza-
tion framework(Zinkevich, 2003).  For each time step t ∈  [T ], there are sequences of parameters

T                                                                   T

{θt}t₌₁ and convex loss functions {ft}t₌₁.  Let X  be a bounded convex feasible set,  which in-

cludes {θt}T     and the optimal solution θ∗. The optimal solution θ∗ is defined to achieve 
empirical

risk minimization(ERM), i.e., θ∗  =  argmin ΣT     ft(θ).  We use regret RT  to donate the entire


difference between ΣT

θ∈X

ft(θt) and its minimum value, i.e.,

T                                 T                      T

RT = Σ ft (θt) − min Σ ft(θ) = Σ (ft (θt) − ft (θ∗)) .                  (4.1)

	

With further assumption of bounded gradients, we establish convergence analysis of ADAPLUS  to
ensure the bound of regret as follows.

Theorem 1.  Let αt  =  α/√t, β₁, β₂     (0, 1), γ  =  β₁/√β₂     (0, 1) and β₁t      β₁,    t     
[T ].

Assume that the convex feasible set     has a bounded diameter D   , i.e.,    x, y        ,   x     
y

D   .  Also, assume that loss functions ft( ) are convex and have bounded gradients, i.e.,     G    
>
0,    θ         and t    [T ],      ft(θ)           G   . For θt generated using the ADAPLUS 
(Algorithm 2),
we have the following regret bound:


D2         Σ .

α (1 + β ) √1 + log T    Σ


RT ≤

∞

2α (1 − β₁)

i=1

(vˆT,i + ∆) T +

1

(1 − β₁)² (1 − γ)

            

1 − β₂

i=1

ǁg1:T,iǁ2

(4.2)

2          T      d                                   


+      D∞          Σ Σ β₁t √v^

+ ∆.

This theorem leads directly to the following two corollaries. We will provide the proof of Theorem
1 and Corollary 1, 2 in Appendix A.

Corollary 1.  Under the conditions in Theorem 1, and supposing β₁t = β₁/t, β₁t     0, we have the
following regret bound:


D2         Σ .

α (1 + β ) √1 + log T    Σ


2α (1 − β₁)                       

β₁dD² √(1 + ρ) G∞

(1 − β₁)² (1 − γ)

1 − β₂

i=1

2

(4.3)

+           ∞                       2  .

2α (1 − β₁) (1 − λ)

Corollary 2.  Under the conditions in Corollary 1, we further have the following regret bound:

dD² √(1 + ρ) G∞T     α (1 + β₁) dG∞√(1 + log T ) T     β₁dD² √(1 + ρ) G∞

		


R              ∞                                             +

2α (1 − β₁)

√

(1 − β₁)

(1 − γ)

            

1 − β₂

+           ∞

2α (1 − β₁) (1 − λ)

2  .

(4.4)


which means RT = O(

5    EXPERIMENTS

T ), where O˜(·) donates the omission of logarithmic factors.

5.1    EXPERIMANTAL SETTINGS

In this section, we conduct full experiments on classical tasks both in CNN and RNN architectures,
and compare ADAPLUS  with popular algorithms, such as SGD/SGDM, ADAM  and AMSGRAD  to
evaluate performance.  Unless otherwise specified, ADAPLUS  that we utilized in practice is based

on  the  implementation  of  AMSGRAD,  i.e.,  vt  =  max (vt−₁, vt)  is  applied,  as  is  
described  in

^

Algorithm 2. Overall experiments are summarized in Table 1.

In the field of computer vision, we consider three different architectures on the standard CIFAR-
10/100  dataset(Krizhevsky  et  al.,  2009).    We  use  VGGNet-16(Simonyan  &  Zisserman,  2015),

5


Under review as a conference paper at ICLR 2020

Table 1: An Overview of Experiments.

Tasks                             Architectures                                 Datasets          
Framework
VGGNet-16(Simonyan & Zisserman, 2015)


CV                    ResNet-50(He et al., 2016)

DenseNet-121(Huang et al., 2017)

CIFAR-10/100          Torch

NLP                 OpenNMT(Klein et al., 2017)                    IWSLT15          Tensorflow

ResNet-50(He et al., 2016) and DenseNet-121(Huang et al., 2017),  where there was a clear dis-
tinction in the number of layers to better identify different behaviors of algorithms. We employ the
fixed budget of 200 epochs and multiply the learning rates by 0.1 at 100th and 150th epochs.  In
the Neural Machine Translation experiment(?), we chose the learning rate attenuation scheme lu-
ong234, which means after 2/3 num train steps, we start halving the learning rate for 4 times before
finishing.  Using the same parameter settings as the benchmark (Luong et al., 2017), we run a total
of 12000 steps on Titan XP.

For   reproducibility,   we   provide   the   codes   anonymously   in   https://anonfiles.com/
daV7Ed6enb/AdaPlus_zip and put full details of experiments in the Appendix C.

5.2    CONVOLUTIONAL NEURAL NETWORK ON CIFAR-10/100

In this section, we train three different models with various numbers of layers to accurately 
compare
the performance of optimizers. We also conducted experiments both on CIFAR-10 and CIFAR-100
to examine the impacts of complex tasks and different datasets on the behavior of optimizers.  We
report training loss and test accuracy in the following figures.

In general, we notice that in deep neural network architecture the final performance of SGDM is
significantly better than ADAM  and AMSGRAD, which shows issues in generalization of ADAM-
like  algorithms  once  again.   Additionally,  to  our  surprise,  our  approach  ADAPLUS  
outperforms
SGDM in the various models and tasks almost at all the time step (with significant differences or a
bit similar).


0.5

0.4

0.3

           AdaPlus

           SGDM

           Padam

AMSGrad
Adam

0.5

0.4

0.3

           AdaPlus

           SGDM

           Padam

AMSGrad
Adam

0.5

0.4

0.3

           AdaPlus

           SGDM

           Padam

AMSGrad
Adam


0.2

0.2

0.2


0.1

0.1

0.1


0.0

0            25           50           75          100        125        150        175        200

Epochs

0.0

0            25           50           75          100        125        150        175        200

Epochs

0.0

0            25           50           75          100        125        150        175        200

Epochs


(a) VGGNet-16 on Cifar-10

(b) ResNet-50 on Cifar-10

(c) DenseNet-121 on Cifar-10

96

96

94

94

94

92

92                                                                                                  
                       92

90

90                                                                                                  
                       90


88

88

           AdaPlus

86

            SGDM                    ⁸⁶

84                                                                                Padam             
     84

AMSGrad

82                                                                                                  
                       82

Adam

88

           AdaPlus

            SGDM                    ⁸⁶

           Padam                  84

AMSGrad

82

Adam

           AdaPlus

           SGDM

           Padam

AMSGrad
Adam


80

0          25          50          75         100        125        150        175        200

Epochs

80

0          25          50          75         100        125        150        175        200

Epochs

80

0          25          50          75         100        125        150        175        200

Epochs


(d) VGGNet-16 on Cifar-10

(e) ResNet-50 on Cifar-10

(f) DenseNet-121 on Cifar-10

Figure 2: Training loss and test accuracy of different architectures on Cifar-10.

6


Under review as a conference paper at ICLR 2020


1.4

1.2

1.0

0.8

            AdaPlus

           SGDM

           Padam

                  AMSGrad
Adam

1.4

1.2

1.0

0.8

             AdaPlus

           SGDM

           Padam

                  AMSGrad
Adam

1.4

1.2

1.0

0.8

           AdaPlus

           SGDM

           Padam

AMSGrad
Adam


0.6

0.6

0.6


0.4

0.4

0.4


0.2

0.2

0.2


0.0

0            25           50           75          100        125        150        175        200

Epochs

0.0

0            25           50           75          100        125        150        175        200

Epochs

0.0

0            25           50           75          100        125        150        175        200

Epochs


(a) VGGNet-16 on Cifar-100

(b) ResNet-50 on Cifar-100

(c) DenseNet-121 on Cifar-100

80

75                                                                                                  
                       80

70                                                                                                  
                       75                                                                           
                                              75

70

65                                                                                                  
                                                                                                    
                                               70


           AdaPlus                65

⁶⁰                                                    SGDM

60

           Padam

⁵⁵                                                           AMSGrad              55

Adam

           AdaPlus

65

           SGDM

           Padam

AMSGrad              ⁶⁰

Adam

           AdaPlus

           SGDM

           Padam

AMSGrad
Adam


50

0          25          50          75         100        125        150        175        200

Epochs

50

0          25          50          75         100        125        150        175        200

Epochs

55

0          25          50          75         100        125        150        175        200

Epochs


(d) VGGNet-16 on Cifar-100

(e) ResNet-50 on Cifar-100

(f) DenseNet-121 on Cifar-100

Figure 3: Training loss and test accuracy of different architectures on Cifar-100.

Summary  of  behaviors.   Empirically,  the  behaviors  of  various  algorithms  match  with  what  
we
analyze in Section 2.2.  Primarily, ADAPLUS has similar behaviors to PADAM, showing faster con-
vergence and less volatility than SGDM in the early stage.  The results of PADAM  and SGDM are
mostly identical, sometimes SGDM wins while other times PADAM wins. However, ADAPLUS has
achieved better performance than both of them due to the introduction of offset term ∆. In addition,
both ADAM and AMSGRAD perform poorly in the end, although their early training losses decline
rapidly and their convergence is faster.

Summary  of  performances.    For  VGGNet-16,  ResNet-50  and  DenseNet-121,  ADAPLUS  has
achieved test accuracies of 0.26%, 0.46%, and 0.15% higher than SGDM in Cifar-10 tasks; while
in Cifar-100 tasks, the gaps are about 1.18%, 1.62% and 0.54%. In terms of architectures, the most
significant improvement of ADAPLUS incurs in ResNet-50, followed by VGGNet-16 and DenseNet-

121.  ADAPLUS  has achieved an accuracy of about 80% in ResNet-50 on CIFAR-100, which is a
significant  improvement  over  SGDM.  In  terms  of  complexity  of  tasks,  when  the  tasks  are 
 more
complex, ADAPLUS  can gain better promotions, since architectures have been improved more by
ADAPLUS on CIFAR-100 than on CIFAR-10.

5.3    NEURAL MACHINE TRANSLATION

In this section, we evaluate the performance of ADAPLUS  in a classic NLP task Neural Machine
Translation (NMT)(Klein et al., 2017).  Due to the particularity of NLP problems, adaptive opti-
mization algorithms and their variants do not perform as well as SGD; therefore, many researchers
prefer to use fine-tuned SGD(Wilson et al., 2017).

OpenNMT has standard benchmarks with open source codes(Luong et al., 2017), from which we
select to conduct the IWSLT15 English-to-Vietnam task with the same settings as its benchmark.
Since there is no TensorFlow implementation for PADAM, we will not consider it but focus on the
comparison of ADAPLUS with the state-of-the-art benchmark. As is generally agreed, standard SGD
with lr = 1.0 can achieve the best average performance of 26.1 in the model with beam = 10. We
apply ADAPLUS to the same task in this section. Besides, we experiment in the Vietnam-to-English
task of 15k steps in Appendix D, using the same settings as ADASHIFT(Zhou et al., 2019), which
also yields better results than SGD.

7


Under review as a conference paper at ICLR 2020

Figure 4 shows that AMSGRAD’s performance is worse than ADAM and that despite the fluctuations
of  ADAM  are smaller,  SGD  performs indeed better.   Therefore,  we do not apply  AMSGRAD  in
ADAPLUS, i.e., the sixth line of Algorithm 2 is annotated. Our NMT experiment further proves that
ADAPLUS is more suitable for NLP problems with recurrent neural network model and sparse data
set.  As is shown in Figure 4, ADAPLUS  is almost always superior to all other methods.  Finally, it
has achieved the best BLEU slightly higher than SGD, as shown in Table 2.


100

80

60

            AdaPlus

            SGD

AMSGrad
Adam

25.0

22.5

20.0

17.5


15.0

40

12.5

10.0

20

7.5

            AdaPlus

            SGD

AMSGrad
Adam


0

0                2000            4000            6000            8000           10000          
12000

Steps

(a) Training perplexity

5.0

0              2000           4000           6000           8000          10000         12000

Steps

(b) Test BLEU

Figure 4: Training perplexity and test BLEU on NMT.

Table 2: Best BLEU for 12k steps on IWSLT15 English-to-Vietnam.

Optimizer      SGD       ADAM      AMSGRAD      ADAPLUS

Best BLEU     26.12      24.74           23.17              26.31

6    FUTURE  WORK

Although we present a generic framework and an explicit algorithm with excellent performance,
there would be more elegant forms remaining for us to explore in the future.

1.  The choice of function Φ( ). For instance, we can replace the square function with log( )
or tanh( ) and so on. Also, the square function can be tuned into a p-th power function to
apply a PADAM-like grid search.

2.  The choice of Lᵖ Norm parameter p.  Different Lᵖ Norm parameter can be tried to get
better numerical performance.

7    CONCLUSION

This work proposes a novel generic framework, in which we explicitly analyze different behaviors
brought by various types of Φ( ),  such as the constant function in  SGDM, the linear function in
ADAM, the concave function in PADAM  and the concave function with offset term in ADAPLUS.
With better adaptivity as is demonstrated,  ADAPLUS  has achieved remarkable superior results in
both CNN and RNN experiments. Our main contributions can be summarized as follows:

•  A generic framework.  Combining ADAM-like algorithms with SGD-like algorithms, the
adaptive adjustment function Φ(·) suggests a generic framework we desired.

Explicit analysis of different algorithm behaviors by Φ( ). Based on the explicit analysis
of Φ( ), we explain the different behaviors of different algorithms, which is a fundamental
reason never mentioned before.

The proposal of a concave function with offset term. In ADAPLUS, we propose an offset
term ∆, which can further avoid extreme actual step sizes based on the concave function
and achieve superior performance.

8


Under review as a conference paper at ICLR 2020

REFERENCES

Jinghui Chen and Quanquan Gu.  Closing the generalization gap of adaptive gradient methods in
training deep neural networks.   CoRR, abs/1806.06763,  2018.   URL http://arxiv.org/
abs/1806.06763.

John Duchi, Elad Hazan, and Yoram Singer.  Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121–2159, 2011.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.

Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger.  Densely connected
convolutional networks.  In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.

Diederik P Kingma and Jimmy Ba.  Adam: A method for stochastic optimization.  In International
Conference on Learning Representations, 2015.

Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander Rush. OpenNMT: Open-
source toolkit for neural machine translation.  In Proceedings of ACL 2017, System Demonstra-
tions, pp. 67–72, Vancouver, Canada, July 2017. Association for Computational Linguistics. URL
https://www.aclweb.org/anthology/P17-4012.

Alex Krizhevsky, Geoffrey Hinton, et al.   Learning multiple layers of features from tiny images.
Technical report, Citeseer, 2009.

Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Confer-
ence on Learning Representations, 2019.  URL https://openreview.net/forum?id=
Bkg6RiCqY7.

Minh-Thang Luong, Eugene Brevdo, and Rui Zhao.  Neural machine translation (seq2seq) tutorial.

https://github.com/tensorflow/nmt, 2017.

H. McMahan and Matthew Streeter.  Adaptive bound optimization for online convex optimization.

COLT 2010 - The 23rd Conference on Learning Theory, 02 2010.

B.T. Polyak.   Some methods of speeding up the convergence of iteration methods.   USSR Com-
putational  Mathematics  and  Mathematical  Physics,  4(5):1  –  17,  1964.     ISSN  0041-5553.
doi: https://doi.org/10.1016/0041-5553(64)90137-5.  URL http://www.sciencedirect.
com/science/article/pii/0041555364901375.

Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar.  On the convergence of adam and beyond.  In

International Conference on Learning Representations, 2018.

Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathemati-
cal statistics, pp. 400–407, 1951.

K. Simonyan and A. Zisserman.  Very deep convolutional networks for large-scale image recogni-
tion. In International Conference on Learning Representations, May 2015.

Tijmen  Tieleman  and  Geoffrey  Hinton.   Lecture  6.5-rmsprop:  Divide  the  gradient  by  a  
running
average of its recent magnitude.  COURSERA: Neural networks for machine learning, 4(2):26–
31, 2012.

Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht.  The marginal
value  of  adaptive  gradient  methods  in  machine  learning.   In  Advances  in  Neural  
Information
Processing Systems, pp. 4148–4158, 2017.

Zhiming  Zhou,  Qingru  Zhang,  Guansong  Lu,  Hongwei  Wang,  Weinan  Zhang,  and  Yong  Yu.
Adashift:  Decorrelation  and  convergence  of  adaptive  learning  rate  methods.   In  
International
Conference on Learning Representations, 2019. URL https://openreview.net/forum?
id=HkgTkhRcKQ.

Martin Zinkevich.  Online convex programming and generalized infinitesimal gradient ascent.  In
Proceedings of the 20th International Conference on Machine Learning (ICML-03), pp. 928–936,
2003.

9


Under review as a conference paper at ICLR 2020

A    PROOF  OF  CONVERGENCE

A.1    PROOF OF THEOREM 1

Proof.  Since all ft(·) are convex, which means ∀ x, y ∈ X,

ft(y) ≥ ft(x) + ∇ft(x)T(y − x).                                   (A.1)

Then, we have


T

RT =        (ft (θt) − ft (θ∗))

t=1
T

≤         ⟨gt, θt − θ∗⟩ .

t=1

(A.2)

^           −

X ,(Vt+∆) 2

We use Qt ,    Vt + ∆  ∈  Sd  to donate the matrix for simplification.  Besides, we can find that
Qt , Φ(Vt) is the general formation in Algorithm 1, as long as it satisfies the requirement of being
positive definite matrices, the following derivation suits as well.

For the definition of θ∗, it holds that ΠX ,Q (θ∗) = θ∗, ∀θ∗ ∈ X. Using Lemma 1, we have


¨    1                            ∗   ¨

¨    1  .

−1                ∗Σ¨2

	

Expanding the squared norm on the right side of the inequality and further expanding the momentum
term, there is

¨    1                            ∗   ¨

	

		


1

≤   Q 2  (θt

¨    1

− θ∗)¨₂

∗  ¨

+ α² ¨Q

₂ ¨

−t  2 m

−

t¨₂ −

¨

2αt

⟨mt, θt

− θ∗⟩

(A.4)

∗

Rearranging the terms in the above inequality, the following first inequality holds,

⟨gt, θt − θ∗⟩


≤           1           Σ¨

1

Q 2  (θ

− θ∗)¨  − ¨

1

Q 2  (θ

−    ∗   ¨2Σ

t           · ¨

Q− 2 m ¨


2αt (1 − β₁t)

   β₁t   

−  1 − β   ⟨m

t−1

t

, θt

t                2           t

− θ∗⟩

t+1

¨2        2 (1 − β₁t)   ¨   t

t¨₂

(A.5)


≤           1           Σ¨

1

Q 2  (θ

− θ∗)¨  − ¨

1

Q 2  (θ

−    ∗   ¨2Σ

t           · ¨

Q− 2 m ¨


2αt (1 − β₁t)   ¨   t       t

¨₂    ¨   t

t+1

¨2        2 (1 − β₁t)   ¨   t

t¨₂


2 (1 − β₁t)   ¨

t−1

t−1¨₂

2αt (1 − β₁t) ¨

t−1     t             ¨2

and the last inequality holds due to Cauchy-Schwarz and Young’s inequality.

Combining (A.2) with (A.5), we have

10


Under review as a conference paper at ICLR 2020

T

[ft (θt) − ft (θt∗)]

t=1


Σ .           1           Σ¨    1

¨      ¨    1

	

¨2Σ

       α               −         ¨

	


t=1

+

β1tαt

· ¨Q

− 2  m     ¨  +

1t           ¨Q 1

(θ  −

∗   ¨2 Σ


2 (1 − β₁t)   ¨

t−1

t−1¨₂

2αt (1 − β₁t) ¨

t−1     t             ¨2

(A.6)

As is separately proven in Lemma 2, Lemma 3 and Lemma 4, we can further get the desired regret
bound.

T

RT =       [ft (θt) − ft (θt∗)]                                                                

t=1


D2         Σ .

α (1 + β ) √1 + log T    Σ

2          T      d                                   

+      D∞          Σ Σ β1t √v^    + ∆

A.2    PROOF OF COROLLARY 1

Proof.

Since we assume that β₁t = β₁λᵗ, λ ∈ (0, 1), we have

2          T      d                                   

     D∞          Σ Σ β1t √v^    + ∆

D2        Σ Σ β λt−1 .

	


Besides, due to the definition of vˆt,i, we have

Σ   t−j

Σt

t−j

    ∆    


vˆt,i + ∆ = (1 − β₂)

j=1

β2    |gj,i| + ∆ ≤ (1 − β₂) 

j=1

β2    |gj,i| + 1 − β

 .        (A.9)

Since ∆ > 0 is a given constant, which is chosen to be relatively small enough, it always holds 
that

∃ ρ > 0, s.t., ∆ ≤ ρ · G∞. Then, we can further improve the result in (A.9), ∀ t ∈ [T ],


which leads to

t

vˆt,i + ∆ ≤ (1 − β₂)

j=1

βt−j

 G∞ + ρ · G∞ ≤ (1 + ρ) G∞,              (A.10)

2          T      d                                   

     D∞          Σ Σ β1t √v^    + ∆

	


β   ₁D² √(1 + ρ) G∞  Σ

Σ λt−1√t

(A.11)


2α (1 − β₁)

t=1 i=1

2  .

2α (1 − β₁) (1 − λ)

11


Under review as a conference paper at ICLR 2020

Submitting (A.11) into (A.7), we extend the conclusion in Theorem 1 to Corollary 1.

T

RT =       [ft (θt) − ft (θt∗)]                                                                

t=1


D2         Σ .

α (1 + β ) √1 + log T    Σ

(A.12)


2α (1 − β₁)                       

β₁dD² √(1 + ρ) G∞

(1 − β₁)² (1 − γ)

1 − β₂

2

i=1

+           ∞                                 2

2α (1 − β₁) (1 − λ)


A.3    PROOF OF COROLLARY 2

Proof.

Primarily, as is proved in (A.10), it holds that

ΣT

T −j 


Thus, we have

vˆT,i + ∆ ≤ (1 − β₂) 

β2

j=1

 G∞ + ρ · G∞ ≤ (1 + ρ) G∞.             (A.13)

₂ √          d                              

   D∞      T    Σ √vˆ     + ∆

₂ √          d                                    


   D∞      T             (1 + ρ) G

2α (1 − β₁) i=1                     ∞

2

=      ∞                                               

2α (1 − β₁)

(A.14)

Second, due to the property of convex functions and bounded gradients, we further have


Σi=1

ǁg1:T,iǁ2

≤ Σi=1

‚  T

t=1

2

t,i

≤ dG∞

√T,                            (A.15)

Pluging (A.14) and (A.15) back into (A.12), we finally get the result of Corollary 2

T

RT =       [ft (θt) − ft (θt∗)]

t=1


dD² √(1 + ρ) G∞T     α (1 + β₁) dG∞√(1 + log T ) T

		

(A.16)


        ∞                                             +

2α (1 − β₁)

β₁dD² √(1 + ρ) G∞

(1 − β₁)

(1 − γ)

            

1 − β₂

+           ∞                       2  ,

√   2α (1 − β₁) (1 − λ)

which means RT = O˜(   T ), where O˜(·) donates the omission of logarithmic factors.

B    AUXILIARY  LEMMAS

B.1    PROOF OF LEMMA 1

Lemma 1 (McMahan & Streeter (2010); Reddi et al. (2018); Chen & Gu (2018)).  For any Q  ∈

d   and convex feasible set X   ⊂  Rd,  suppose u₁  =  arg minx∈X  ǁQ¹/²(x − z₁)ǁ₂ and u₂  =

arg minx∈X  ǁQ¹/²(x − z₂)ǁ₂ then we have ǁQ¹/²(u₁ − u₂)ǁ₂ ≤ ǁQ¹/²(z₁ − z₂)ǁ₂.

We will not provide the proof here, since it will be exactly the same as the reference and has been
proved many times.

12


Under review as a conference paper at ICLR 2020

B.2    PROOF OF LEMMA 2

Lemma 2.  Under the conditions in Theorem 1, we have


Σ .        α        Σ¨   −             ¨

¨   −                    ¨2Σ Σ

α (1 + β ) √1 + log T    Σ

Proof.  Similar  to  (Reddi  et  al.,  2018;  Chen  &  Gu,  2018),  we  first  describe  the  upper 
 bound  of

ΣT         ¨   −             ¨       ΣT    Σd   √αt·m2                                              
                                                                               . ^

	


Σ α  ¨

Q− 2 m ¨


t=1

t ¨     t         t¨

T      d                      2

Σ Σ   αt · mt,i

	

T −1   d                  2           d                       2

Σ Σ   αt · mt,i         Σ  αT  · mT,i

		


T −1   d

α  · m²

· m²

(B.1)


≤ Σ Σ √ t

t,i    +         αT

+ ∆                v

T,i

+ ∆


T −1

d                                                   d

.ΣT

(1 − β

) βT −jg   Σ2


=               √v^

+ ∆ + √T       .

ΣT        T −j

	 		 	


T −1   d

       ·                                        Σ

	

.ΣT

βT −j |g

| 1 Σ .ΣT

βT −j |g

| 3 Σ

≤                 √v^    + ∆ + √T (1 − β )                     .ΣT        T −j                   ∆

These several equalities holds due to the update rule and definitions of αt, vt,i and mt,i.  Also, 
the

first inequality holds due to the definition of vt,i, while the second inequality holds due to β₁t  
≤


β₁, ∀  t  ∈  [T ]  and Cauchy-Schwarz inequalit^y.   Since

gradients, we have

T

j=1

βT −j   ≤

   ¹   and f ( )  has bounded

1−β1


Σ α  ¨Q

− ₂ m ¨

	


t=1

t ¨     t         t¨


T −1   d      αt

2

t,i

α√G∞               Σ

T

j=1

βT −j |g

j,i| 2


≤                 √v^

+ ∆ + (1 − β ) √T (1 − β )

.ΣT

T −j                   ∆

	


T −1   d

α  · m²

α√G

Σ  ΣT

 

βT −ʲ|g   |


≤                 √v^

+ ∆ + (1 − β ) √T (1 − β )

.ΣT

	

T −j

(B.2)

Σ Σ   αt · mt,i                     α   G∞               Σ Σ  β1     |gj,i| 2

			 


≤                 √v^    + ∆ + (1 − β ) √T (1 − β )

.  T −j


T −1   d

αt · m²

α√G               T      d


≤ Σ Σ √

t,i    +

√   ∞                      Σ Σ γT −ʲ |g   | ,


t=1  i=1

v^t,i + ∆

(1 − β₁)

T (1 − β2) j=1 i=1


property of concave function. At last, as is assumed, γ =  √β1

∈ (0, 1) is introduced.

13


Under review as a conference paper at ICLR 2020


We further expand T −1

t=1

αt ¨Q

−t  2 mt

T −1  d

=

t=1 i=1

√αt·m2

v^t,i+∆

in the same way, which leads to


Σt=1

αt ¨Q

−t  2 mt

√                 T              t      d


        α   G∞           Σ   1  Σ Σ

	

t−j

√                  d     T                T


        α   G∞           Σ Σ |gt,i| Σ

		

j−t

√                  d    T

        α   G∞           Σ Σ      |gt,i|      

√                        d    T

=                α   G∞ √            Σ Σ |gt,i|                


√                            d

∞

(1 − γ) (1 − β )    1 − β

ǁg1:T,iǁ2

‚u,ΣT    1


1

α   G   (1 + log T )

≤(1 − γ) (1 − β₁) √1 − β

2  i=1

2 Σi=1

ǁg1:T,iǁ2

t=1


Similarly, with   T

j=t

γʲ−ᵗ        ¹   , the second inequality holds.  The third inequality also holds by

1−γ

Cauchy-Schwarz  inequality,  while  the  last  inequality  holds  due  to  the  bound  on  harmonic 
 sum:


T

t=1

t ≤ 1 + log T .


Thus, we finally get

Σ

.        αt          Σ¨

Q− 2 m ¨  + β

¨Q− 2  m

¨2Σ Σ


t=1

2 (1 − β₁t)   ¨   t         t¨

1t ¨

t−1

t−1¨

(B.4)


≤ α (1 + β₁) √G∞

d

√                   ǁg1:T,iǁ   .


2 (1 − γ) (1 − β₁)²

1 − β₂

2

i=1

B.3    PROOF OF LEMMA 3

Lemma 3.  Under the conditions in Theorem 1, we have


Σ           1           Σ¨    1

 	

¨      ¨    1

	

¨2Σ

D2         Σ .

14


Under review as a conference paper at ICLR 2020

Proof.  Consider


Σ           1           Σ¨    1

 	

	

¨      ¨    1

	

¨2Σ


≤ Σ Σ

√vˆt,i + ∆

Σ(θt₊₁,i − θ∗)² − (θt,i − θ∗)²Σ


t=1 i=1

2αt (1 − β₁)


       1              d

=

2 (1 − β₁)

i∗        +

α₁

vˆt,i + ∆

αt          −

vˆt−1,i + ∆

αt−1

(θt,i − θ∗)²Σ


i=1

D2        ΣΣd

√vˆ₁,i + ∆

l=2 i=1

Σ Σ . √vˆt,i + ∆

	

√vˆt−1,i + ∆ΣΣ

2               √vˆT,i + ∆

2 (1 − β₁) i=1        αT

2

=          ∞                    (vˆT,i + ∆) T ,


2α (1 − β₁) i=1

(B.5)

where the first inequality is based on β₁t     β₁, and the second inequality is due to the 
definition of
a bounded feasible set.

B.4    PROOF OF LEMMA 4

Lemma 4.  Under the conditions in Theorem 1, we have


Σ         β               1

D2        Σ Σ β   √

	


Proof.  Consider

Σ         β               1                               ¨

	

	


≤ Σ Σ

β1t ·      vt,i + ∆

2αt (1 − β₁)

(θt,i − θ∗)2

(B.6)


t=1 i=1

2

T      d                                   


≤      D∞          Σ Σ β₁t √v^

+ ∆,

where the first inequality is based on β₁t      β₁ and the non-decreasing property of vt,i, and the
second inequality is due to the definition of a bounded feasible set.

C    EXPERIMENTAL  SETTINGS

C.1    CIFAR CLASSIFICATION

β₁ and β₂. We conduct experiments in (β₁, β₂) =   (0.9, 0.99), (0.9, 0.999)  . For ADAM
and  AMSGRAD,  we use (0.9, 0.99);  while for  SGDM,  PADAM  and  ADAPLUS,  we use
(0.9, 0.999).

Learning Rate.  For ADAM  and AMSGRAD, we do grid search in   0.001, 0.0001  .  For
SGDM, PADAM  and ADAPLUS, we do grid search in   0.05, 0.1, 0.2, 0.3  , and most opti-
mal results are using lr = 0.1.

15


Under review as a conference paper at ICLR 2020

Learning Rate Schedule.  We conduct a learning rate schedule of decaying the learning
rate by 0.1 at the 100th and 150th epoches in all 200 epoches.

•  Offset. For ADAPLUS, we do grid search in ∆ = {1/8, 1/4, 1/2} to get best results.

•  Weight Decaying. For ADAM and AMSGRAD, we set weight decaying = 1e − 4; while
for SGDM, PADAM and ADAPLUS, we set weight decaying = 5e − 4.

Best Setting.  For ADAPLUS, we fine-tune the hyper-parameters and achieve best results
when lr =  0.1, ∆  = 1/8 in VGGNet-Cifar100, ResNet-Cifar100 and DenseNet-Cifar10,
lr  =  0.2, ∆  =  1/2  in  VGGNet-Cifar10,  lr  =  0.1, ∆  =  1/4  in  ResNet-Cifar10  and
lr = 0.3, ∆ = 1/2 in DenseNet-Cifar10.

C.2    NEURAL MACHINE TRANSLATION

β₁ and β₂. We conduct experiments in (β₁, β₂) =   (0.9, 0.99), (0.9, 0.999)  . For ADAM
and  AMSGRAD,  we use (0.9, 0.99);  while for  SGDM,  PADAM  and  ADAPLUS,  we use
(0.9, 0.999).

Learning Rate.  For ADAM  and AMSGRAD, we do grid search in   0.001, 0.0001  .  we
searched  learning  rates  in    0.1, 0.5, 1.0, 2.0    for  SGD,  and  get  the  optimal  result  
when
lr = 1.0.

Learning Rate Schedule. We conduct a learning rate scheme luong234, which means after
2/3 num train steps, we start halving the learning rate for 4 times in all 12000 steps.

•  Offset. For ADAPLUS, we do grid search in ∆ = {1/2, 1, 2} to get best results.

Other Details. We keep all other details the same with the benchmark(Luong et al., 2017),
there is also a json file in our codes to demonstrate parameter settings.

Best  Setting.   For  ADAPLUS,  we  achieve  the  best  result  when  lr  =  2.0, ∆  =  2.0  or

lr = 1.0, ∆ = 1.0.

C.3    ADDITIONAL EXPERIMENTS

β₁ and β₂. We conduct experiments in (β₁, β₂) =   (0.9, 0.99), (0.9, 0.999)  . For ADAM
and  AMSGRAD,  we use (0.9, 0.99);  while for  SGDM,  PADAM  and  ADAPLUS,  we use
(0.9, 0.999).

Learning Rate.  For ADAM  and AMSGRAD, we do grid search in   0.001, 0.0001  .  we
searched  learning  rates  in    0.1, 0.5, 1.0, 2.0    for  SGD,  and  get  the  optimal  result  
when
lr = 1.0.

Learning  Rate  Schedule.   We  conduct  a  learning  rate  scheme  self,  which  means  after
8000 train steps, we start halving the learning rate every 1000 steps within 15000 steps of
training.

•  Offset. For ADAPLUS, we do grid search in ∆ = {1/8, 1/4, 1/2} to get best results.

•  Other Details. We keep all other details the same with ADASHIFT(Zhou et al., 2019).

•  Best Setting. For ADAPLUS, we achieve the best result when lr = 0.5, ∆ = 0.125.

D    ADDITIONAL  EXPERIMENTS  IN  NMT

We also conduct another NMT experiment on IWSLT15.  Unlike Section 5.3, we apply a different
learning rate schedule, extend the total number of training steps to 15000 steps, and change the 
task
into Vietnam-to-English.   We conduct a learning rate scheme self,  which means after 8000 train
steps, we start halving the learning rate every 1000 steps within the total 15000 steps of training.
We have fine-tuned different optimizers and compare their best performance, where the results of
ADAM and AMSGRAD are similar to that in (Zhou et al., 2019). The experimental results still show
that ADAPLUS has significant advantages over other algorithms, both in terms of convergence speed
and final performance.

16


Under review as a conference paper at ICLR 2020


100

80

60

            AdaPlus

            SGD

Adam
AMSGrad

25.0

22.5

20.0

17.5

15.0


40

12.5

10.0

20

7.5

            AdaPlus

            SGD

Adam
AMSGrad


0

0           2000       4000       6000       8000      10000     12000     14000

Steps

(a) Training perplexity

5.0

0         2000       4000       6000       8000      10000     12000     14000

Steps

(b) Test BLEU

Figure 5: Training perplexity and test BLEU on NMT.

Table 3: Best BLEU for 15k steps on IWSLT15 Vietnam-to-English.

Optimizer      SGD       ADAM      AMSGRAD      ADAPLUS

Best BLEU     25.02      21.68           18.92              25.35

17

