Under review as a conference paper at ICLR 2020
Low Bias Gradient Estimates for
Very Deep B oolean Stochastic Networks
Anonymous authors
Paper under double-blind review
Ab stract
Stochastic neural networks with discrete random variables are an important class
of models for their expressiveness and interpretability. Since direct differentiation
and backpropagation is not possible, Monte Carlo gradient estimation techniques
have been widely employed for training such models. Efficient stochastic gradient
estimators, such Straight-Through and Gumbel-Softmax, work well for shallow
models with one or two stochastic layers. Their performance, however, suffers with
increasing model complexity.
In this work we focus on stochastic networks with multiple layers of Boolean
latent variables. To analyze such networks, we employ the framework of harmonic
analysis for Boolean functions. We use it to derive an analytic formulation for
the source of bias in the biased Straight-Through estimator. Based on the analysis
we propose FouST, a simple gradient estimation algorithm that relies on three
simple bias reduction steps. Extensive experiments show that FouST performs
favorably compared to state-of-the-art biased estimators, while being much faster
than unbiased ones. To the best of our knowledge FouST is the first gradient
estimator to train very deep stochastic neural networks, with up to 80 deterministic
and 11 stochastic layers in our experiments.
1 Introduction
Stochastic neural networks with discrete latent variables have been an alluring class of models for
their expressivity and interpretability, dating back to foundational work on Helmholtz machines
(Dayan et al., 1995) and sigmoid belief nets (Neal, 1992). Since they are not directly differentiable,
discrete random variables do not mesh well with the workhorse of modern Deep Learning, that
is the backpropagation algorithm. Monte Carlo gradient estimation is an effective solution where,
instead of computing the true gradients, one can sample gradients from some distribution. The sample
estimates can be either biased or unbiased. Unbiased gradient estimates like score function estimators
(Williams, 1992) come typically at the cost of high variance leading to slow learning. In contrast,
biased gradient estimates such Straight-Through (Bengio et al., 2013), while efficient, run the risk
of convergence to poor minima and unstable training. To this end several solutions have recently
been proposed that either reduce variance in unbiased estimators (Mnih & Gregor, 2014; Gu et al.,
2015; Tucker et al., 2017; Rezende et al., 2014; Grathwohl et al., 2017) or control bias in biased
estimators (Jang et al., 2016; Maddison et al., 2016). These methods, however, have difficulty scaling
up to complex neural networks with multiple stochastic layers: low-variance unbiased estimators are
too expensive 1 , while the compounded bias from the continuous relaxations on multiple stochastic
layers leads to poor minima. In this work we focus on biased estimators.
Our goal in this paper is a gradient estimator for Boolean random variables that works for any
complex -deep or wide- neural network architecture. We resort to the term Boolean instead of
binary to emphasize that we work directly on the Boolean space {-1, +1}, without any continuous
relaxations or quantizations. With this in mind we re-purpose the framework of harmonic analysis
of Boolean functions, widely used in computational learning and computational complexity theory
(O’Donnell, 2014; Linial et al., 1993; Mossel et al., 2003; Mansour, 1994). We cast stochastic
neural networks as Boolean functions f (z) over Boolean latent variables z sampled from probability 1
1Training a nonlinear sigmoid belief network model with two stochastic layers on MNIST with REBAR took
a day and a half on GPU.
1
Under review as a conference paper at ICLR 2020
Algorithm 1 FouST Gradient Estimator
Require: Parameters θ ∈ RK, Bernoulli Representation {-a, a}, Interval Parameter b ∈ [0, a],
Constant scaling parameter γ
1:	Sample Xi 〜pθi (xi) ,i = 1,…,K	. This and all steps below have constant complexity.
2:	Sample yi 〜Unif(b, a)
3:	Set yi := Xi * yi
4:	Compute ST gradient ∂θi := ∂yi f (y)
5:	Importance reweighing ∂θi := 0.5∕p(xi) ∙ ∂θi
6:	return partial gradient γ∂θi , i = 1, ..., K * 1
distributions p(z). We then use harmonic analysis to determine that the bias in the Straight-Through
gradient estimates corresponds to the weighted sum of higher-order Taylor coefficients of f (z). The
direct consequence is that one can control the bias in the Straight-Through estimator by manipulating
the higher-order Taylor coefficients of f(z). Building upon the harmonic analysis of existing gradient
estimators, we present an algorithm, FouST, that admits low bias gradient estimates for Boolean latent
variable models. In experiments, we were able to scale up the stochastic depth and width of neural
networks, training deep stochastic residual networks with up to 80 deterministic and 11 stochastic
layers with little difficulty. We summarize FouST in Algorithm 1.
With this work we make the following three contributions.
1. We introduce the framework of harmonic analysis of Boolean functions to analyze discrete
stochastic neural networks and their REINFORCE and Straight-Through gradients. We
show that stochastic gradients compute Fourier coefficients.
2. Based on the above harmonic analysis we present FouST - a low-bias gradient estimator for
Boolean latent variables based on three bias reduction steps. As a side contribution, we show
that the gradient estimator employed with DARN (Gregor et al., 2013), originally proposed
for autoregressive models, is a strong baseline for gradient estimation in large and complex
models with many stochastic layers.
3. We show that FouST is amenable to complex stochastic neural networks with Boolean
random variables. To the best of our knowledge, FouST is the first gradient estimate
algorithm that can train very deep stochastic neural networks with Boolean latent variables.
The practical outcome is a simple gradient estimate algorithm that can be plugged in complex
stochastic neural networks with multiple layers of Boolean random variables.
2 Harmonic Analysis of Boolean Functions
We consider Boolean functions on the n-dimensional Boolean cube, f : {-1, +1}n → R. The
setting of Harmonic Analysis for Boolean functions is the space of Boolean functions f with a
product probability distribution on the Boolean input, that is p(z) = Qin=1 pi (z). We denote pi as
the probability of the i-th dimension being one, i.e., pi := pi(zi = +1). We denote the mean and
variance of Zi by μi and σ%, respectively.
An example of a Boolean function in this setting is a generative neural network f : z 7→ y with a
factorized latent distribution, as commonly done in representation learning (Kingma & Welling, 2013;
Higgins et al., 2017). In this example, Z is the stochastic input - also known as the latent code in
stochastic neural networks - taking only two possible values and y is the output, like cross entropy
loss. Often, the goal of a generative neural network is to learn or approximate the latent distribution
given input data X, i.e., p(Z|X), as we will also explore in the experiments.
We first introduce a few basic operations in the context of Harmonic Analysis of Boolean func-
tions, which we shall use further on. Further necessary details are in the Appendix A. For a more
comprehensive introduction, however, we refer the reader to O’Donnell (2014).
Inner product. The inner product of two Boolean functions f and g is: hf, gi = Ep(z) [f(Z)g(Z)].
Fourier expansion. Let S be any subset of dimensions of the n-dimensional Boolean cube, S ⊆
[n] = {1, ..., n}. Per S we define a basis function, φS (Z) := Qi∈S φi(Zi), where for the empty set
2
Under review as a conference paper at ICLR 2020
φe(z) = 1 and φi is the Z-Score normalized dimension, i.e., φi := zi-μi. For example, under the
uniform Bernoulli distribution for the i-th dimension, pi(zi = +1) = pi(zi = -1) = 1/2, we have
φi(zi) = zi.
The 2n functions φS form an orthonormal basis for the space of Boolean functions f,
Ep(z)[φi(zi)] = 0,	(1)
Ep(z) [φi(zi)2] = 1,
Ep(z) [φi(zi)φj(zj)] = Ep(z) [φi(zi)] Ep(z) [φj (zj)],
for i 6= j , where the expectations compute the inner product between two Boolean functions. The
last identity derives from the independence of any dimensions i 6= j .
We can then expand the Boolean function f on the set of 2n orthonormal basis functions,
f(z) = X fS(S)φs(z),	⑵
S⊂[n]
also known as the p-biased Fourier expansion of f. The f(p (S) are the Fourier coefficients computed
by the inverse Fourier expansion,
f(p)(S ) = Ep(z)[f (z)φs (z)].	⑶
That is, the inverse expansion is computed by the inner product for Boolean functions defined
above. The cardinality of S is the degree of the coefficient f (P)(S). For instance, we have only one
degree-0 coefficient, f(p)(0), which equals to the expected value of f under the distribution P(Z)
f(p)(0) = E[f (z)] since φ0 = 1. Further, we have n degree-1 coefficients f(p (i) = hf, φii and so
on.
3	Harmonic Analysis on Biased Straight-Through Gradients
We examine Straight-Through gradient estimates using the framework of Harmonic Analysis of
Boolean functions. For training the model parameters with a loss function L := Ep(z) [f (Z)] we want
to compute the gradient ∂pi L = ∂pi Ep(z) [f(Z)] for the i-th dimension. As the random sampling
operation in the expectation is not differentiable, Bengio et al. (2013) propose the Straight-Through
estimator that approximates the gradient with ∂pi L ≈ Ep(z) [∂zi f (Z)]. Clearly, the Straight-Through
computes a biased approximation to the true gradient.
Next, we quantify the bias in the Straight-Through estimator using the harmonic analysis of f (x). For
the quantification of bias we first need the following lemma that connects the REINFORCE gradients
with the degree-1 Fourier coefficients. The lemma is an extension of Margulis-Russo (Margulis,
1974; Russo, 1982; O’Donnell, 2014) formula.
Lemma 1. Let f be a Boolean function. Then, the REINFORCE gradient estimates the degree 1
Fourier coefficients f(p')(i) of f under P(Z),
Ep(Z) hgREINFORCEi = dpiEp(Z) [f (Z)I = σf(P)(i).
σi
Proof. For compactness and clarity, we provide the proof in the Appendix B.1.	□
We introduce new notation. In the following lemma bias(p)(gSiT) denotes the bias of the i-th gradient
estimate under the distribution p. Also, given the distribution p(z), pi→1/2 (z) is the distribution for
which we set P(Zi = +1) = P(Zi = -1) = 1/2 for a given dimension i.
Lemma 2. Let f be a Boolean function, gi = ∂pi Ep(z) [f (Z)] its true gradients and gSiT =
Ep(z) [∂zif(Z)] its Straight-Through gradient estimator. The bias in gSiT is due to a mismatch between
the higher-order odd terms in the Taylor expansion ofgi and gSiT and is equal to
bias(P) (gST) = bias(pi→ι∕2) (gSt) + E X kck Mi	(4)
k=2j,j>0
3
Under review as a conference paper at ICLR 2020
where ck are the Taylor coefficients for the i-th dimension on f (z), that is zi, around 0 and
bias(pi→1/2) = E hPk=2j + 1,j>0(k - 2)cki ∙
Proof∙ For compactness and clarity, we provide only a proof sketch here showing the basic steps.
These steps are also needed later in the description of the proposed gradient estimate algorithm. For
the detailed proof please refer to the Appendix B.2. The proof sketch goes as follows. First, we
derive a relation between the Fourier coefficients under the unknown distribution p(z) and under the
uniform Bernoulli distribution pi→"(z). Then, using this relation We derive the Taylor expansions
for the true gradient as well as the Straight-Through gradient estimator. Last, to prove the lemma we
compare the tWo Taylor expansions.
Relation between Fourier coefficients under p(z) and pi→1/2 (z). If we expand the function f
in terms of its φS basis as in equation 2 and focus on the i-th dimension, by Lemma 1 We can shoW
that the REINFORCE gradient is given by
∂piEp(z) [f (z)] = 2 fp() = 2f9→ι∕22(i)	⑸
σi
Taylor expansions of the true and the Straight-Through gradients. The Taylor expansion of
f (Z) for Zi around 0 is given by f (Z) = co + cιzi + c2z2 + c3z3 + c4z4 + c5z5 + …，where
ckk! = ∂zki f (z)|zi=0 are the Taylor coefficients. All ck are a function of zj,j 6= i.
Let’s first focus on the true gradient. Since we work with Boolean ±1 values, we have that Zik = 1
for even k and Zik = Zi for odd k. This will influence the even and the odd terms of the Taylor
expansions. Specifically, for the Taylor expansion of the true gradient we can show that
∂pi[Ep(z)[f (z)]] = 2f(p→1r2)(i) = 2Epi-ι∕2(z) [coZi + CiZ2 + CiZ3 +--]	(6)
=2Ep(z∖i) [ci + c3 + c5 + …]	(7)
The expression in equation 7 implies that the true gradient with respect to the pi is the expected sum
of the odd Taylor coefficients. Here we note that although the final expression in equation 7 can also
be derived by a finite difference method, it does not make explicit, as in equation 31, the dependence
on Zi and μi of the term inside the expectation.
Now, let’s focus on the Straight-Through gradient. Taking the derivative of the Taylor expansion w.r.t.
to Zi , we have
∂zif(Z) = ci + 2c2Zi + 3c3Zi2 + 4c4Zi3 + 5c5Zi4 + ...
(8)
The Straight-Through gradient is the expectation of equation 8 in the i-th dimension, that is
Ep(Z) [dZi f (Z)] = Ep(Z) [ci + 2c2 Zi + 3c3z2 + 4c4z3 + 5c5 Z4 + …]	⑼
=Ep(z∖i) [ci + 3c3 + 5c5 + …]+ Ep(z∖i) [2c2 + 4c4 + …]μi	(IO)
where ZJ = 1,马"+ =Zi, and Ep(Zi) [Zi] = μi.
Comparing the Taylor expansions. By comparing the expansion of the Straight-Through gradient
in equation 10 and the expansion of the true gradient in equation 7 and given that bias(p) (gSiT) =
Ep(Z)[∂Zif(Z)]-∂piEp(Z)[f(Z)],
bias(p)(gST) = Ep(z∖i)	E (k - 2)ck + Ep(z,i)	E kck μi.	(11)
k=2j+i,j>o	k=2j,j>o
Taking the expectation in equation 9 under pi→i/2 causes the final term in equation 11 to vanish
leaving bias(L信坎T) = Ep(z∖i) [Pk=2j+i,j>o(k - 2)ck]. Combining this expression with
equation 9 gives the final expression (equation 4) from the lemma.
□
4
Under review as a conference paper at ICLR 2020
4	Low-bias Gradient Estimates
4.1	FouST Gradient Estimation Algorithm
Inspired by the Harmonic Analysis of the Straight-Through gradient estimates, we present a gradient
estimate algorithm for deep Boolean latent models, FouST, for Fourier Straight-Through estimator.
The algorithm relies on three bias reduction steps on the Straight-Through, lines 2, 3, 5 in Algorithm 1.
4.2	Lowering Bias by Importance Sampling Correction
As detailed earlier, the bias in the Straight-Through estimator is the sum of the bias under the uniform
Bernoulli distribution plus extra bias due to non-zero expectation terms in higher-order harmonics
when sampling fromp(z). Sampling frompi→1/2 instead of p(z) would decrease the total bias from
the form in equation 11 by setting the final term to 0.
As a first bias reduction step, therefore, we do importance sampling. Specifically, after getting
samples from p(z) and computing the gradients ∂zif(z) with the Straight-Through, we estimate the
expectation under pi→1/2 as
Epi→ι∕2(z)[∂zif(z)] = Ep(Z) h2p(z)∂zif (z)i	(PiT1/2(Zi) = 1/2)	(12)
Interestingly, Gregor et al. (2013) arrive at equation 12 in the context of unbiased control variates for
quadratic functions.
4.3	Lowering Bias by Matching Uniform Distribution Moments
Lemma 2 shows that part of the bias in the Straight-Through estimator is due to the presence of extra
factors in the Taylor coefficients. We can reduce the effect of these factors by taking advantage of
the moments of the uniform distribution. Recalling that Rb bUkdu = k^, We can attempt to correct
the coefficients in equation 9, which for zk have the form (k + 1)ck, with the same extra factor of
k + 1 that appears in the denominator of the kth moment. This suggests that we can sample from an
auxiliary variable u and then use the auxiliary variable u with f instead of z and exploit the higher
moments of the uniform distribution to reduce bias.
For brevity, we illustrate the method with a case study of a two-dimensional z, and a bivariate
f(z1, z2).
As in Lemma 2, the partial true gradient of f(z1, z2) w.r.t. the first distribution parameter p1 equals to
dpi Ep(Z) = E cι,j Ep(Z) [zj ] + E C3,j Ep(Z) [zj ] + E C5,j Ep(Z) [zj ] + …	(13)
jjj
“Bernoulli splitting uniform” trick. Assume an auxiliary variable u = (u1, u2), which we choose
as follows. First, we sample Z = (zι, z2) from the uniform Bernoulli distributionp1→1/2 with (i set
to 1). Then we take a uniform sample (u1, u2) with ui sampled from either [0, 1] for Zi = +1 or
from [-1, 0] if Zi = -1.
The expectation of the gradient under such random sampling is
Ez,u[∂zι f (u1,u2)] = X cj Ez[zj]+ X 由 Ez[zj] + …	(14)
j j+	j j+
Further detail is in Appendix C.1. We compare equation 14 with equation 13. In equation 14 we
observe that the pure terms in Z1 , namely terms with j = 0, always match those of the true gradient
in equation 13. For j > 0 we obtain mixed terms with coefficients that do not match those of the
true gradient in equation 13. Due to the ʃ^ɪ factor, for small j the mixed-degree terms are closer to
the original ones in equation 13. For functions with small mixed degree terms, this can lead to bias
reduction, at the cost of an increased variance because of sampling an auxiliary variable. In practice,
to manage this bias-variance trade-off and to deal with functions that have greater dependence on
mixed degree terms, we use smaller intervals for the random samples as in Algorithm 1.
5
Under review as a conference paper at ICLR 2020
To summarize, for appropriate functions, the “Bernoulli splitting uniform” relies on the continuous
variable u conditioned on the binary sample to reduce the bias. However, it is important to emphasize
that u is only an auxiliary variable; the actual latent variable z is always binary. Thus, the “Bernoulli
splitting uniform” trick does not lead to a relaxation of the sort used by Gumbel-Softmax (Jang et al.,
2016), where there are no hard samples. Lastly we note that for a univariate f the “Bernoulli splitting
uniform” trick leads to an unbiased estimator with an increased variance.
4.4	Lowering Bias by Representation Scaling
The Fourier basis does not depend on the particular input representation and any two-valued set,
say {-t, t} can be used as the Boolean representation. The choice of a representation, however,
does affect the bias as we show next. As a concrete example, we let our input representation be
zi ∈ {-1/2, 1/2}n, where pi = p(zi = +1/2). While we can change the input representation
like that, in general the Fourier coefficients in equation 3 will be different than for the {-1, +1}
representation. We give the final forms of the gradients here. Details are given in Appendix C.2.
Under the pi→1/2 distribution the degree-1 Fourier coefficients are:
Mj) (i) = Epi2(z)h c21 + ∣3 + ∣5 + …i	(15)
Note that compared to equation 7, in equation 15 we still get the odd terms c1, c3 albeit decayed by
inverse powers of 2. Following the same process for the Straight-Through gradient as in equation 10,
we have that
2 Epi→l/2 [dZi f (Z)] = Epi→l/2 h 21- + 233- + 255- + …]	(16)
While this is still biased, compared to equation 7 the bias is reduced by damping higher order terms
by inverse powers of 2.
4.5	Algorithmic Complexity
The algorithm, described in algorithm 1, is a Straight-Through gradient estimator with the bias
reduction steps described above, where a single sample is used to estimate the gradient. We emphasize
that the algorithm uses a single sample and a single evaluation of the decoder per example and latent
vector sample. Thus, the algorithm has the same complexity as that of the original Straight-Through
estimator.
5	Related Work
Monte Carlo gradient estimators for training models with stochastic variables can be biased or
unbiased. Perhaps the best known example of an unbiased gradient estimator is the REINFORCE
algorithm (Williams, 1992). Unfortunately, REINFORCE gives gradients of high variance. For
continuous stochastic variables Kingma & Welling (2013) propose the reparameterization trick,
which transforms the random variable into a function of deterministic ones perturbed by a fixed
noise source, yielding much lower variance gradient estimates. For discrete stochastic variables,
REINFORCE is augmented with control variates for variance reduction. A number of control variate
schemes have been proposed: NVIL (Mnih & Gregor, 2014) subtracts two baselines (one constant
and one input-dependent) from the objective to reduce variance. MuProp (Gu et al., 2015) uses the
first-order Taylor approximation of the function as a baseline. REBAR (Tucker et al., 2017) uses the
Gumbel-Softmax trick to form a control variate for unbiased gradient estimates. RELAX (Grathwohl
et al., 2017) generalizes REBAR to include an auxiliary network in the gradient expression and uses
continuous relaxations and the reparameterization trick to give unbiased gradients.
Regarding biased estimators, a simple choice is the Straight-Through estimator (Bengio et al.,
2013) which uses the gradient relative to the sample as that relative to the probability parameter.
Another recent approach is to use continuous relaxations of discrete random variables so that the
reparameterization trick becomes applicable. The most common example of this being the Gumbel-
Softmax estimator (Maddison et al., 2016; Jang et al., 2016). Although this is a continuous relaxation,
it has been used to define the Gumbel Straight-Through estimator with hard samples. This uses
6
Under review as a conference paper at ICLR 2020
arg max in the forward pass and the Gumbel-Softmax gradient is used as an approximation during
in the backward pass. DARN (Gregor et al., 2013), like MuProp, also uses the first-order Taylor
expansion as a baseline but does not add the analytical expectation, making the estimator biased for
non-quadratic functions.
In this work we focus on biased Straight-Through gradient estimators. Specifically, we analyse
how to reduce bias via Fourier expansions of Boolean functions. The Fourier expansion itself is
widely used in computational learning theory with applications to learning low-degree functions
(Kushilevitz & Mansour, 1993), decision trees (Mansour, 1994), constant-depth circuits (Linial et al.,
1993) and juntas (Mossel et al., 2003). To the best of our knowledge we are the first to explore Fourier
expansions for bias reduction of biased stochastic gradient estimators.
6	Experiments
Experimental Setup. We first validate FouST on a toy setup, where we already know the analytic
expression of f (z). Next we validate FouST by training generative models using the variational
autoencoder framework of Kingma & Welling (2013). We optimize the single sample variational
lower bound (ELBO) of the log-likelihood. We train variational autoencoders exclusively with
Boolean latent variables on OMNIGLOT, CIFAR10, mini-ImageNet (Vinyals et al., 2016) and
MNIST (Appendix D.1). We train all models using a regular GPU with stochastic gradient descent
with a momentum of 0.9 and a batch size of 128. We compare against Straight-Through, Gumbel-
Softmax, and DARN, although on more complex models some estimators diverge. The results were
consistent over multiple runs. Details regarding the architectures and hyperparameters used are in
Appendix E. Upon acceptance we will open source all code, models, data and experiments.
6.1	Biased Estimators on a Toy Problem
To validate the excessive bias in the Straight-Through
estimator, as well as to explore the benefits of FouST,
we first explore a toy problem. Similar to Tucker et al.
(2017), we minimize Ep(z) [(z-t)2], where t ∈ (0, 1)
is a continuous target value and z is a sample from
the Bernoulli distribution p(z). The optimum is ob-
tained for p(z = +1) ∈ {0, 1}. Figure 1, shows
a case with t = 0.45, where the minimizing solu-
tion is p(z = +1) = 0. We observe that unlike the
Straight-Through estimator, FouST converges to the
minimizing deterministic solution (lower is better).
Figure 1: Biased estimators on a toy problem
6.2	FouST vs. State-of-the-Art Gradient Estimators
Training Stochastic MLPs. We train MLPs with one and two stochastic layers on OMNIGLOT,
following the non-linear architecture of Tucker et al. (2017). Each stochastic Boolean layer is
preceded by two deterministic layers of 200 tanh units.
All hyperparameters remain fixed throughout the training. All estimators use one sample per example
and a single decoder evaluation. We present results in Fig. 2. FouST outperforms other biased
gradient estimators in both datasets and architectures. FouST is clearly better than the Straight-
Through estimator. Despite the complicated nature of the optimized neural network function f(z)
the bias reduction appears fruitful. With one or two stochastic layers we can also use the unbiased
REBAR. REBAR is not directly comparable to the estimators we study, since it uses multiple decoder
evaluations and for models with multiple stochastic layers, multiple passes through later layers.
Nevertheless, as shown in appendix D.1 for MNIST, with two stochastic layers REBAR reaches a
worse test ELBO of -94.43 v. -91.94 for FouST. A possibility of worse test than training ELBOs for
REBAR was also suggested in the original work (Tucker et al., 2017).
Training Stochastic ResNets. We further validate FouST in a setting where the encoder and
decoder are stochastic ResNets, S-ResNets, which are standard ResNets with stochastic layers
inserted between ResNet blocks. Similar to MLPs, FouST outperforms other biased estimators in this
7
Under review as a conference paper at ICLR 2020
setting on CIFAR-10 (left in Figure 3). Note that despite the hyperparameter sweep, we were unable
to train S-ResNet’s with Gumbel-Softmax. So we compare against DARN and Straight-Through only.
With an S-ResNet with 12 ResNet blocks and 4 stochatic layers FouST yields a score of 5.08 bits
per dimension (bpd). This is comparable to the 5.14 bpd with the categorical VIMCO-trained model
(Mnih & Rezende, 2016) obtained with 50 samples per example, as reported by van den Oord et al.
(2017).
In the plots, we observe sharp spikes or slower curv-
ing. We hypothesize these are due, respectively, to Table 1: Wall clock times for various gradient
stochasticity and bias, and are corrected to some estimators on MNIST.
degree along the way.	---------------------------------------------
Method #Eval. Walltime in sec./Epoch
Efficiency. We compare the efficiency of differ-		2 layers		5 layers
ent estimators in Tab. 1. Like other biased estima-	REBAR	3	45.3	205.15
tors, FouST requires a single sample for estimat-	ST	1	2.86	4.96
ing the gradients and has similar wallclock times.	Gumbel	1	3.27	6.14
On MNIST, the unbiased REBAR is 15x and 40x	DARN	1	2.96	5.36
slower than the biased estimators for two and five				
stochastic layer MLP’s respectively.	FouST	1	3.1	5.67
From the above experiments we conclude that
FouST allows for efficient and effective training
of fully connected and convolutional neural networks with Boolean stochastic variables.
6.3	Adding Stochastic Depth and Width
Last, we evaluate FouST on more complex neural networks with deeper and wider stochastic layers.
We perform experiments with convolutional architectures on the larger scale and more realistic
mini-ImageNet (Vinyals et al., 2016). As the scope of this work is not architecture search, we present
two architectures inspired from residual networks (He et al., 2016) of varying stochastic depth and
width. The first one is a wide S-ResNet, S-ResNet-40-2-800, and has 40 deterministic (with encoder
and decoder combined), 2 stochastic layers, and 800 channels for the last stochastic layer. The second,
S-ResNet-80-11-256, is very deep with 80 deterministic and 11 stochastic layers, and a last stochastic
layer with 256 channels. Architecture details are given in Appendix E.2. In this setup, training with
existing unbiased estimators is intractable.
We present results in Fig. 3. We compare against DARN, since we were unable to train the models
with Gumbel-Softmax. Incomplete lines indicate failure.
We observe that FouST is able to achieve better training ELBO’s in both cases. We conclude that
FouST allows for scaling up the complexity of stochastic neural networks in terms of stochastic depth
and width.
-105
-110
-125
1112
- -
01山 6u一
0	250000 500000 750000 1000000 1250000 1500000 1750000 2000000
Steps
-100
-105
-110
-115
-120
0ω1山 6--01
0	250000 500000 750000 1000000 1250000 1500000 1750000 2000000
Steps
Figure 2: Training ELBO for the one (left) and two (right) stochastic layer nonlinear models on
OMNIGLOT
8
Under review as a conference paper at ICLR 2020
Figure 3: Training ELBO on CIFAR-10 (left) and mini-ImageNet (right)
7	Conclusion
In this work we introduce the framework of harmonic analysis for Boolean functions. We then use the
harmonic analysis to derive the source of bias in the Straight-Through estimator for Boolean random
variables. Based on the analysis we propose FouST, which is a novel gradient estimate algorithm.
We use FouST to train neural networks with Boolean random variables in stochastic layers. FouST
outperforms state-of-the-art biased gradient estimators, while maintaining efficiency that is orders
of magnitude higher than unbiased estimators. Importantly, to the best of our knowledge FouST
is the first gradient estimate algorithm, biased or unbiased, to be able to train very deep and wide
neural networks with Boolean random variables. In our experiments FouST successfully trained
networks with up to 80 deterministic and 11 stochastic layers. We conclude that the Harmonic
Analysis framework for Boolean function is a useful methodological tool for analyzing Boolean
neural networks. Also, we conclude that FouST is a practical gradient estimate algorithm that can
train complex stochastic neural networks with multiple layers of Boolean random variables.
References
Yoshua Bengio, Nicholas Leonard, and Aaron Courville. Estimating or propagating gradients through stochastic
neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.
Peter Dayan, Geoffrey E Hinton, Radford M Neal, and Richard S Zemel. The helmholtz machine. Neural
computation, 7(5):889-904, 1995.
Will Grathwohl, Dami Choi, Yuhuai Wu, Geoff Roeder, and David Duvenaud. Backpropagation through the
void: Optimizing control variates for black-box gradient estimation. arXiv preprint arXiv:1711.00123, 2017.
Karol Gregor, Ivo Danihelka, Andriy Mnih, Charles Blundell, and Daan Wierstra. Deep autoregressive networks.
arXiv preprint arXiv:1310.8499, 2013.
Shixiang Gu, Sergey Levine, Ilya Sutskever, and Andriy Mnih. Muprop: Unbiased backpropagation for stochastic
neural networks. arXiv preprint arXiv:1511.05176, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed,
and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained variational framework.
ICLR, 2(5):6, 2017.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint
arXiv:1611.01144, 2016.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.
Eyal Kushilevitz and Yishay Mansour. Learning decision trees using the fourier spectrum. SIAM Journal on
Computing, 22(6):1331-1348, 1993.
Nathan Linial, Yishay Mansour, and Noam Nisan. Constant depth circuits, fourier transform, and learnability.
Journal of the ACM (JACM), 40(3):607-620, 1993.
9
Under review as a conference paper at ICLR 2020
Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous relaxation of
discrete random variables. arXiv preprint arXiv:1611.00712, 2016.
Yishay Mansour. Learning boolean functions via the fourier transform. In Theoretical advances in neural
computation and learning, pp. 391-424. Springer, 1994.
Grigorii Aleksandrovich Margulis. Probabilistic characteristics of graphs with large connectivity. Problemy
peredachi informatsii, 10(2):101-108, 1974.
Andriy Mnih and Karol Gregor. Neural variational inference and learning in belief networks. arXiv preprint
arXiv:1402.0030, 2014.
Andriy Mnih and Danilo J Rezende. Variational inference for monte carlo objectives. arXiv preprint
arXiv:1602.06725, 2016.
Elchanan Mossel, Ryan O’Donnell, and Rocco P Servedio. Learning juntas. In Proceedings of the thirty-fifth
annual ACM symposium on Theory of computing, pp. 206-212. ACM, 2003.
Radford M Neal. Connectionist learning of belief networks. Artificial intelligence, 56(1):71-113, 1992.
Ryan O’Donnell. Analysis of boolean functions. Cambridge University Press, 2014.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate
inference in deep generative models. arXiv preprint arXiv:1401.4082, 2014.
Lucio Russo. An approximate zero-one law. Probability Theory and Related Fields, 61(1):129-139, 1982.
George Tucker, Andriy Mnih, Chris J Maddison, John Lawson, and Jascha Sohl-Dickstein. Rebar: Low-variance,
unbiased gradient estimates for discrete latent variable models. In Advances in Neural Information Processing
Systems, pp. 2627-2636, 2017.
Aaron van den Oord, Oriol Vinyals, et al. Neural discrete representation learning. In Advances in Neural
Information Processing Systems, pp. 6306-6315, 2017.
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one shot
learning. In Advances in neural information processing systems, pp. 3630-3638, 2016.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning.
Machine learning, 8(3-4):229-256, 1992.
Appendices
A Further Details from the Harmonic Analysis Framework for
B oolean Functions
A. 1 Discrete derivative
For a Boolean function f the discrete derivative on the i-th latent dimension with a basis function φi is defined
as
Dφif(z)
f (z1 , ..., zi = +1, ..., zn) - f (z1 , ..., zi = -1, ..., zn )
σi----------------------------------------------------------------------------------------------
(17)
The Fourier expansion of the discrete derivative equals Dφif (Z) = Ps3i f(p(S)φs∖i(z), The Fourier
expansion of the discrete derivative is derived by equation 2: (i) all bases that do not contain the i-th dimension
are constant to φi and thus set to zero, while (ii) for the rest of the terms ∂d≤Φs = φs∖ from the definition of
basis functions φS (z). In the following we differentiate partial derivatives on continuous functions noted with
∂∙ from discrete derivatives on Boolean functions noted with D∙. The i-th discrete derivative is independent of
zi both in the above definitions.
10
Under review as a conference paper at ICLR 2020
B Harmonic Analysis of Existing Gradient Estimates
B.1	Proof of Lemma 1
Proof. We follow O’Donnell (2014, §8.4). In this proof we work with two representations of the Boolean
function f . The first is the Fourier expansion of f under the uniform Bernoulli distribution. This is also the
representation obtained by expressing f as a polynomial in zi . Since the domain of the function f is the Boolean
cube, the polynomial representation is multilinear. That is f(z) = S⊆[n] f(S) j∈S zj. To avoid confusion
and to differentiate the representation from the Boolean function, we use f(u) (z) to denote this representation in
the following. Note that since this representation is a polynomial it is defined over any input in Rn . In particular,
E[f(z)] = E[∑ f(S) ∏ Zj] = E f(S) ∏ ER = f (u)(μι,…，μn)
S⊆[n]	j∈S	S⊆[n]	j∈S
The second representation we use is the Fourier expansion of the Boolean function f under p(x). We denote this
byf(p).
The following relation follows from the fact that when working with the Fourier representation, f (z) is
multilinear, Ep(Z) [zi] = μi and the linearity of expectation.
Ep(z)[f (p)(zι,...,Zn)] = Ep(z)[f (zι,...,Zn)] = f(U) (μι ,...,μn).	(18)
As the partial derivative of f(U) w.r.t. μi is equivalent to discrete derivative of f(U) w.r.t. zi, ∂μ% f (u)(μ)
Dzi f(U) (μ), and keeping in mind that φi = (Zi — μi)/σ%, we have that
Dzi f ' (B = Ep(Z)[Dzif(p)(z1, . . ., zn)]
=—Ep(z)[D@if (P)(Z1,..∙, Zn)]
σi
=与 (p)(i)
σi
(from equation 18)	(19)
(set φi = (zi — μi)/σi, then chain rule)	(20)
(21)
We then note that the discrete derivative of f w.r.t. Zi, Dzi f(U) (μ), from the left hand side of equation 19, is
equivalent to the partial derivative of f w.r.t. μi, ∂μi f(U) (μ).
-1 f(P (i) = Dzif (")(〃) = ∂μif(U) (μ)	(22)
σi	i	i
=Bdpi f(U) (μ)	(set μi = 2pi — 1, then chain rule)	(23)
=JdpiEp(z)[f(P)(Z)]	(from equation 18)	(24)
We complete the proof by noting that the right hand side in equation 24 is 11 times the REINFORCE gradient. □
B.2	Analysis of Straight-Through
The detailed proof of Lemma 2 is as follows.
Proof. We first derive the Taylor expansions for the true gradient as well as the Straight-Through gradient
estimator. Then, we prove the lemma by comparing the two Taylor expansions.
By expanding the function f in terms of its φS basis as in equation 2 and focusing on the i-th dimension, we
have that
/(P) (i)φi = /(P) (i) Zi - μi = /(P) (i)包—/(P) (i) μi	(25)
σi	σi	σi
The first term, /(P) (i) Ii, is the term corresponding to {i} in the Fourier expansion of f under pi→1∕1(Z). That
is
f(p-1∕2)(i) = L f(p)(i)	W
σi
This follows from the fact that when moving from P(Z) to pi→1∕1(Z), (i) we have that φi = Zi, and (ii) no other
term under the P(Z) expansion contributes to the Zi term under the pi→1/2 (z) expansion.
11
Under review as a conference paper at ICLR 2020
As a consequence of Lemma 1 the REINFORCE gradient for the i-th dimension is given by
∂piEp(z) [f (z)] = 2 f(p-(i) = 2f(p→1/2) ⑺	(27)
σi
Next, we will derive the Taylor expansions of the true and the Straight-Through gradients. The Taylor expansion
of f(z) for zi around 0 is
f(z) = c0 + c1zi + c2zi2 + c3zi3 + c4zi4 + c5zi5 + ...,	(28)
where ckk! = ∂zki f (z)|zi=0 are the Taylor coefficients. All ck are a function of zj, j 6= i.
Let’s first focus on the true gradient. Since we work with binary ±1 values, we have that zik = 1 for even k and
zik = zi for odd k. This will influence the even and the odd terms of the Taylor expansions. Specifically, for the
Taylor expansion of the true gradient we have from equation 27 and equation 3 that
dPi [Ep(z) [f (Z)]] = 2f(p→1/2) ⑺=2Epi→ι∕2 (z)[(c0 + c1zi + c2 z2 + …)zi]	(29)
=2Epi→ι∕2 (z) [coZi + C1Z2 + Cizi +---]	(30)
=2Epi→1∕2 (z) [c0zi + cl + c2zi + c3 + …]	(zij = 1, z2j+1 = zi, Epi→1∕2(z) [zi] = O)
(31)
=2Ep(ζ∖i) [c1 + c3 + c5 + ••• ]	(32)
The expression in equation 32 implies that the true gradient with respect to the pi is the expected sum of the odd
Taylor coefficients. Here we note that the although final expression in equation 32 can also be derived by a finite
difference method, it does not make explicit, as in equation 31, the dependence on zi and μi of the term inside
the expectation.
Now, let’s focus on the Straight-Through gradient. Taking the derivative of the Taylor expansion in equation 28
w.r.t. to zi, we have
∂zi f (z) = c1 + 2c2 zi + 3c3zi2 + 4c4zi3 + 5c5zi4 + ...	(33)
The Straight-Through gradient is the expectation of equation 33 in the i-th dimension, that is
Ep(z)[∂zi f (z)] = Ep(Z) [ci + 2c2zi + 3c3z2 + 4c4z3 + 5c5z4 +--]	(34)
=Ep(ζ∖i)[cι + 3c3 + 5c5 +-----] + Ep(z∖i)[2c2 +4c4 +------]μi (z2j = 1,z2j+1 = zi, Ep(Zi) [zi] = μi)
(35)
By comparing the expansion of the Straight-Through gradient in equation 35 and the expansion of the true
gradient in equation 32,
bias(p) (gSiT) = Ep(Z) [∂Zi f (z)] - ∂pi Ep(Z) [f(z)]	(36)
=Ep(ζ∖i)	(k - 2)ck + Ep(z∖i)	kck μi.	(37)
k=2j+1,j>0	k=2j,j>0
Taking the expectation in equation 34 under pi→1/2 causes the final term in equation 37 to vanish leaving
bias(pi→ι∕2)(giT) =Ep(z∖i)	X	(k - 2)ck .	(38)
k=2j+1,j>0
□
C Low-bias Gradient Estimates
C.1	Lowering Bias by Matching Uniform Distribution Moments
We describe the case of a bivariate function in detail.
For brevity, we focus on a case study of a two-dimensional z, and a bivariate f(z1, z2 ) with the bivariate Taylor
expansion f(z1, z2) = Pi,j ci,j z1i z2j.
As in Lemma 2, the partial true gradient of f(z1, z2 ) w.r.t. the first distribution parameter p1 equals to
∂p1Ep(z)=	c1,jEp(Z)[z2j] +	c3,jEp(Z)[z2j] +	c5,jEp(Z)[z2j]+...	(39)
jjj
Further, the Taylor expansion of ∂p1 f (z1, z2) is ∂Z1f(z1, z2) = Pj c1,jz2j+2 Pj c2,jz2jx1+3Pj c3,jz2jz12 +
12
Under review as a conference paper at ICLR 2020
“Bernoulli splitting uniform” trick. Assume an auxiliary variable u = (u1, u2), which we choose as
follows. First, we sample Z = (zι ,z2) from the uniform Bernoulli distribution p1→1/2. Then we take a uniform
sample (u1, u2) with ui sampled from either [0, 1] for zi = +1 or from [-1, 0] if zi = -1. At this point it is
important to note that the moments of the uniform distribution in [a,b] are Rb ^1^Uk = b1^ bk+：+：：+1, which
simplifies to b/2, b2 /3, b3/4, . . . for a = 0, and where we think of b as a binary sample i.e., b ∈ {-1, 1}. The
expectation of the gradient under such random sampling is
Ez,u[∂z1 f (u1, u2)] = X c1,jEz,u[uj2] + 2Xc2,j Ez,u [u2]Ez,u [u1] + 3 X c3,j Ez,u[u2]Ez,u[u2] +---------
j
j
j
X j¾ Ez [zj]+ X jcj Ez [z2 ] + …
(40)
We then compare equation 40 with equation 39. In equation 40 we observe that the pure terms in z1 , namely
terms with j = 0, always match those of the true gradient in equation 39. For j > 0 we obtain mixed terms with
coefficients that do not match those of the true gradient in equation 39. However, the partial gradient obtained
with the auxiliary variables in equation 40 has coefficients following a decaying trend due to the j++ι. For small
j , that is, the mixed-degree terms are closer to the original ones in equation 39. For functions with smaller
mixed degree terms this leads to bias reduction, at the cost of an increased variance due to additional sampling.
In practice many functions would have greater dependence on mixed degree terms. For such functions and to
manage the bias-variance trade-off we choose smaller intervals for the uniform samples, that is a → b.
C.2 Lowering Bias by Representation Scaling
The Fourier basis does not depend on the particular input representation and any two-valued set, say {-t, t} can
be used as the Boolean representation. The choice of a representation, however, does affect the bias as we show
next. As a concrete example, we let our input representation be zi ∈ {-1/2, 1/2}n, where pi = p(zi = +1/2).
While we can change the input representation like that, in general the Fourier coefficients in equation 3 will be
different than for the {-1, +1} representation. Letting h(zi) = 2zi ∈ {-1, 1}, the functions φi are now given
as φi = h(zi)-μi.
σi
Next, we write the Taylor series of f in terms of h(zi),
f (Z) = C0 + ClZi + C2Z2 + C3Z3 + C4Z4 + C5Z5 +-- (41)
=c0 + W h(Zi) + |lh(Zi)2 + 23 h(Zi)3 + 24 h(Zi)4 + 2|h(Zi)5 +----- (42)
Under the pi→1/2 distribution, we still have that Ep→1∕2 [h(Zi)] = 0 and the degree-1 Fourier coefficients are:
/(p→1∕2)(i) = Epi→1∕2 ⑶[f (Z)h(Zi)] = Epi→1∕2(z) hcl + 23 + C5 + …i	(43)
Note that compared to equation 7, in equation 43 we still get the odd terms c1 , c3 albeit decayed by inverse
powers of 2. Following the same process like for equation 10, we have that
2 Epi→1/2 [dZi f (Z)] = Epi→1/2 h 21- + 整 + 255- +-i	(44)
D Extra Experiments
D.1 Experiments on MNIST
Figure 4: Training ELBO for the one (left) and two (right) stochastic layer nonlinear models on
MNIST
13
Under review as a conference paper at ICLR 2020
Table 2: Test set performance with increasing stochastic depth on MNIST.
Method	Stochastic Layers	Hidden Units per Layer	Test ELBO
Rebar	2	200	-94.43
FouST	2	200	-91.95
FouST	3	200	-89.11
FouST	8	500	-87.31
FouST	20	500	-87.86
D.2 Ablation Experiments
To further judge the effect of our proposed modifications to Straight-Through, we performed ablation experiments
where we separately applied scaling and noise to the importance-corrected Straight-Through. These experiments
were performed on the single stochastic layer MNIST and OMNIGLOT models.
The results of the ablation experiments are shown in figure 5. From the figure it can be seen that scaling alone
improves optimization in both cases and noise alone helps in the case of MNIST. Noise alone results in a worse
ELBO in the case of OMNIGLOT, but gives an improvement when combined with scaling. From these results
we conclude that the proposed modifications are effective.
0ω1山 6u一Uwl
Steps
0ω1山 6u一
Figure 5: Ablations for the one stochastic layer nonlinear model on OMNIGLOT (left) and MNIST
(right)
E Architectures Used in the experiments
E.1	Architectures for MNIST and Omniglot
The encoder and decoder networks in this case are MLP’s with one or more stochastic layers. Each stochastic
layer is preceded by 2 deterministic layers with a tanh activation function.
We chose learning rates from {1 × 10-4, 2 × 10-4, 4 × 10-4, 6 × 10-4}, Gumbel-Softmax temperatures from
{0.1, 0.5}, and noise interval length for FouST from {0.1, 0.2}.
E.2 Architectures for CIFAR- 1 0 and mini-ImageNet
For these dataset we use a stochastic variant or ResNets (He et al., 2016). Each network is composed of stacks of
layers. Each layer has (i) one regular residual block as in He et al. (2016), (ii) followed by at most one stochastic
layer, except for the CIFAR architecture B in figure 3 where we used two stochastic layers in the last layer. The
stacks are followed by a final stochastic layer in the encoder. We do downsampling at most once per stack. We
used two layers per stack.
For CIFAR we downsample twice so that the last stochastic layer has feature maps of size 8x8. We chose learning
rate from {9 × 10-7, 1 × 10-6, 2 × 10-6, 4 × 10-6}, the FouST scaling parameter from {0.5, 0.8, 0.9}, and
the uniform interval was scaled by a factor from {0.01, 0.05, 0.1}
For mini-ImageNet we downsample thrice. We chose the learning rate from {2 × 10-7, 3 × 10-7, 4 × 10-7, 5 ×
10-7}.
14
Under review as a conference paper at ICLR 2020
For the decoder the structure of the encoder is reversed and convolutions are replaced by transposed convolutions.
The output model is Gaussian with learned mean and variance.
15