Under review as a conference paper at ICLR 2020
Robustified Importance Sampling for
Covariate Shift
Anonymous authors
Paper under double-blind review
Ab stract
In many learning problems, the training and testing data follow different distri-
butions and a particularly common situation is the covariate shift. To correct for
sampling biases, most approaches, including the popular kernel mean matching
(KMM), focus on estimating the importance weights between the two distribu-
tions. Reweighting-based methods, however, are exposed to high variance when
the distributional discrepancy is large. On the other hand, the alternate approach
of using nonparametric regression (NR) incurs high bias when the training size is
limited. In this paper, we propose and analyze a new estimator that systematically
integrates the residuals of NR with KMM reweighting, based on a control-variate
perspective. The proposed estimator is shown to either outperform or match the
best-known existing rates for both KMM and NR, and thus is a robust combination
of both estimators. The experiments shows our estimator works well in practice.
1 Introduction
Traditional machine learning implicitly assumes training and test data are drawn from the same
distribution. However, mismatches between training and test distributions occur frequently in reality.
For example, in clinical trials the patients used for prognostic factor identification may not come
from the target population due to sample selection bias (Huang et al. (2007); Gretton et al. (2009));
incoming signals used for natural language and image processing, bioinformatics or econometric
analyses change in distribution over time and seasonality (Sugiyama et al. (2007); Jiang & Zhai
(2007); Quionero-Candela et al. (2009); Tzeng et al. (2017); Borgwardt et al. (2006); Heckman
(1979); Zadrozny (2004)); patterns for engineering controls fluctuate due to the non-stationarity of
environments (Sugiyama & Kawanabe (2012); Hachiya et al. (2008)).
Many such problems are investigated under the covariate shift assumption (Shimodaira (2000)).
Namely, in a supervised learning setting with covariate X and label Y , the marginal distribution
of X in the training set Ptr(x), shifts away from the marginal distribution of the test set Pte(x),
while the conditional distribution P (y|x) remains invariant in both sets. Because test labels are
either too costly to obtain or unobserved, it could be uneconomical or impossible to build predictive
models only on the test set. In this case, one is obliged to utilize the invariance of conditional
probability to adapt or transfer knowledge from the training set, termed as transfer learning (Pan &
Yang (2009)) or domain adaptation (Jiang & Zhai (2007); Blitzer et al. (2006)). Intuitively, to correct
for covariate shift (i.e., cancel the bias from the training set), one can reweight the training data by
assigning more weights to observations where the test data locate more often. Indeed, the key to
many approaches addressing covariate shift is the estimation of importance sampling weights, or
the Radon-Nikodym derivative (RND) of dPte/dPtr between Pte and Ptr (Sugiyama et al. (2008a);
Bickel et al. (2007); Kanamori et al. (2012); Cortes et al. (2008); Yao & Doretto (2010); Pardoe
& Stone (2010); Scholkopf et al. (2002); QUionero-CandeIa et al. (2009); Sugiyama & KaWanabe
(2012)). Among them is the popular kernel mean matching (KMM) (Huang et al. (2007); Quionero-
Candela et al. (2009)), Which estimates the importance Weights by matching means in a reproducing
kernel Hilbert space (RKHS) and can be implemented efficiently by quadratic programming (QP).
Despite the demonstrated efficiency in many covariate shift problems (Sugiyama et al. (2008a);
Quionero-Candela et al. (2009); Gretton et al. (2009)), KMM can suffer from high variance, due to
several reasons. The first one regards the RKHS assumption. As pointed out in Yu & SzePesvari
(2012), under a more realistic assumption from learning theory (Cucker & Zhou (2007)), When the
1
Under review as a conference paper at ICLR 2020
true regression function does not lie in the RKHS but a general range space indexed by a smoothness
θ	θ
-	-	-	-	.	一， -C C ，~~T	- C C ，~~T ,	.
parameter θ > 0, KMM degrades to sub-canonical rate O(ntr2θ+4 +nte2θ+4)
from the parametric
一 1	一 1
rate θ(n/ +%/). Second, if the discrepancy between the training and testing distributions is large
(e.g., test samples concentrate on regions where few training samples are located), the RND becomes
unstable and leads to high resulting variance (Blanchet & Lam (2012)), partially due to a induced
sparsity as most weights shrink towards zero while the non-zero ones surge to huge values. This is
an intrinsic challenge for reweighting methods that occurs even if the RND is known in closed-form.
One way to bypass it is to identify model misspecification (Wen et al. (2014)), but as mentioned in
Sugiyama et al. (2008b), the cross-validation for model selection needed in many related methods
often requires the importance weights to cancel biases and the necessity for reweighting remains.
In this paper we propose a method to reduce the variance of KMM in covariate shift problems. Our
method relies on an estimated regression function and the application of the importance weighting on
the residuals of the regression. Intuitively, these residuals have smaller magnitudes than the original
loss values, and the resulting reweighted estimator thus becomes less sensitive to the variances of
weights. Then, we cancel the bias incurred by the use of residuals by judiciously compensation
through the estimated regression function evaluated on the test set.
We specialize our method by using a nonparametric regression (NR) function constructed from reg-
ularized least square in RKHS (Cucker & Zhou (2007); Smale & Zhou (2007); Sun&Wu (2009)),
also known as the Tikhonov regularized learning algorithm (Evgeniou et al. (2000)). We show that
θ	θ
our new estimator achieves the rate O(ntr2θ+2 + nte2θ+2), which is superior to the best-known rate
of KMM in Yu & SzePeSvari (2012), with the same computational complexity of KMM. Although
the gap to the parametric rate is yet to be closed, the new estimator certainly seems to be a step
towards the right direction. To put into perspective, we also compare with an alternate approach in
Yu & Szepesvari (2012) which constructs a NR function using the training set and then predicts by
evaluating on the test set. Such an approach leads to a better dependence on the test size but worse
dependence on the training size than KMM. Our estimator, which can be viewed as an ensemble of
KMM and NR, achieves a convergence rate that is either superior or matches both of these methods,
thus in a sense robust against both estimators. In fact, we show our estimator can be motivated
both from a variance reduction perspective on KMM using control variates (Nelson (1990); Glynn
& Szechtman (2002)) and a bias reduction perspective on NR.
Another noticable feature of the new estimator relates to data aggregation in empirical risk mini-
mization (ERM). Specifically, when KMM is applied in learning algorithms or ERMs, the resulting
optimal solution is typically a finite-dimensional span of the training data mapped into feature space
(Scholkopf et al. (2001)). The optimal solution of our estimator, on the other hand, depends on both
the training and testing data, thus highlighting a different and more efficient information leveraging
that utilizes both data sets simultaneously.
The paper is organized as follows. Section 2 reviews the background on KMM and NR that motivates
our estimator. Section 3 presents the details of our estimator and studies its convergence property.
Section 4 generalizes our method to ERM. Section 5 demonstrates experimental results.
2	Background and Motivation
2.1	Assumptions
Denote Ptr to be the probability measure for training variables Xtr and Pte for test variables Xte .
Assumption 1. Ptr(dy|x) = Pte(dy|x).
Assumption 2. The Radon-Nikodym derivative β(x)，ddpte (x) exists and is bounded by B < ∞.
Assumption 3. The covariate space X is compact and the label space Y ⊆ [0, 1]. Furthermore,
there exists a kernel K (∙, ∙) : X×X→ R which induces a RKHS H and a Canonicalfeature map
Φ(∙) : X → H Such that K (x, x0) = hΦ(x), Φ(x0))h and ∣∣Φ(x)kH ≤ R for some 0 < R < ∞.
In particular, Assumption 1 is the covariate shift assumption which states the conditional distribu-
tions P (dy|x) remains invariant while the marginal Ptr(x) and Pte(x) shift. Assumptions 2 and
2
Under review as a conference paper at ICLR 2020
3	are common for establishing theoretical results. Specifically, Assumption 2 can be satisfied by
restricting the support of Pte and Ptr on a compact set, although B could be potentially large.
2	.2 Problem Setup and Existing Approaches
Given ntr labelled training data {(xtjr, yjtr)}jn=tr1 and nte unlabelled test data {xite}in=te1 (i.e., {yite}in=te1
are unavailable), the goal is to estimate ν = E[Yte]. The KMM estimator (Huang et al. (2007);
Gretton et al. (2009)) is VKMM == Pn==I β(xjr )yjr, where β(xjr) are solutions of a QP that
attempts to match the means of training and test sets in the feature space using weights β:
(ntr	nte
L(β) , Il一 EejΦ(xjr)--EΦ(xte)∣lH > s.t. 0 ≤ βj ≤ B, ∀1 ≤ j ≤ ntr.
ntr j=1	nte i=1
(1)
Notice we write βj as β(Xtjr) in VKMM informally to highlight βj as estimates of β(Xtjr ). The
fact that (1) is a QP can be verified by the kernel trick, as in Gretton et al. (2009). Define matrix
Kij = K(χtr, Xj) and Kj，nntr Pn=j K(Xjr, xte), optimization (1) is equivalent to
12
min	-ɪβT Ke---2KT β	s.t. 0 ≤ βj ≤ B, ∀1 ≤ j ≤ ntr.
β	n2r	n2r
(2)
In practice, a constraint ∣ n^ Pn=I βj - 1∣ ≤ E fora tolerance e > 0 is included to regularize the β
towards the RND. As in Yu & Szepesvari (2012), we omit them to simplify analysis. On the other
hand, the NR estimator VNR = = PnXj g(xtte) is based on g(∙), some estimate of the regression
function g(x)，E[Y|x]. Notice the conditional expectation is taken regardless of X 〜 Ptr or Pte.
Here, We consider a g(∙) that is estimated nonparametrically by regularized least square in RKHS:
m
gγ,data(∙) = argmin {— X(f (Xjj ) - yjr )2 + Y Ilf kH},
f∈H mj=1
(3)
where γ is a regularization term to be chosen and the subscript data represents {(xtjr, yjtr)}jm=1. Us-
ing the kernel trick and the representation theorem (Scholkopf et al. (2θ01)), optimization problem
(3) can be solved in closed form with gγ,data(x) = Pm=I αjeg K (Xj, x) where
αreg =(K+γI)-1ytr	and	ytr = [y1tr,...,ymtr].
(4)
2	.3 Motivation
Depending on properties of g(∙), Yu & Szepesvari (2012) proves different rates of KMM. The most
θ
notable case is when g ∈ H but rather g(∙) ∈ Range(TKθ+4), where TK is the integral operator
(TKf )(x0) = JX K(x0, x)f (X)Ptr(dx) on LPt「. In this case, Yu & Szepesvari (2012) characterize
g with the approximation error
A2(g,F)，ll inf	l∣g - fkLP ≤ CF-2,	(5)
kf kH ≤F	Ptr
θ	θ
and the rates of KMM drops to sub-canonical ∣Vkmm - ν| = O(ntr (2θ+4) + nte(2θ+4)), as opposed
_ 1	_ 1
to O(ntr2 + nte2) when g ∈ H. As shown in Lemma 4 and Theorem 4.1 of Cucker & Zhou (2007)),
θ	θ
(5) is almost equivalent to g(∙) ∈ Range(TKθ+ )： g(∙) ∈ Range(TKθ+4) implies (5) while (5) leads
_______________________W	θ
to g(∙) ∈ Range(TKθ+4 ) for any e > 0. We will adopt the characterization g(∙) ∈ Range(TKθ+4)
as our analysis is based on related learning theory estimates.
θ
Correspondingly, the convergence rate for VNR when g(∙) ∈ Range(TKθ+4) is also shown in Yu &
_ 1	-	3θ
Szepesvari (2012) as | VNr - ν∣ = O(nte2 + ntr 12θ+16), with g taken as gγ,data in (3) and Y chosen
optimally. The rate of VKMM is usually better than VNR due to labelling cost (i.e. ntr < nte).
However, in practice the performance of VKMM is not always better than VNR. This could be par-
tially explained by the hidden dependence of VKMM on potentially large B, but more importantly,
3
Under review as a conference paper at ICLR 2020
without variance reduction, KMM is subject to the negative effects of unstable importance sampling
weights (i.e. the β). On the other hand, the training of g requires labels hence can only be done
on training set. Consequently, without reweighting, when estimating the test quantity ν, the rate of
VNR suffers from the bias.
This motivates the search for a robust estimator which does not require prior knowledge on the
performance of VKMM or VNR and can, through a combination, reach or even surpass the best per-
formance among both. For simplicity, we use the mean squared error (MSE) criteria MSE(V ) =
Var(V) + (Bias(V ))2 and assume an additive model Y = g(X)+ E where E 〜 N (0, σ2) is indepen-
dent with X and other errors. Under this framework, we motivate a remedy from two perspectives:
Variance reduction for KMM: Consider an idealized KMM with VKMM ，n^ Pn=I β(xjr )yjr
with β(∙) being the true RND. Since E[β(Xtr)Ytr] = Ex〜Ptr(β(x)g(x)) = Eχ^p>te [g(x)] = V,
VKMM is unbiased and the only source of MSE becomes the variance. It then follows from standard
control variates that, given an estimator V and a zero-mean random variable W, we can set t?
Cov(V,W)
Var(W)
and use V -t?W to obtain mint Var(V -tW) = (1-corr2(V, W))Var(V) ≤ Var(V) with-
out altering the mean of V. Thus We can use W == Pn=I β(xjr)(g(xjr)) 一 十 PnXj ^(xie)
with t? = COV(VK(WM'W). To calculate t?, suppose Xte and Xtr are independent, then
COV(VKMM ,W )= ntrCOV(e(X tr )Y tr，队Xtr )g(X tr))
nrCOV(e(X tr )g(Xtr ),β(Xtr )g(Xtr)) ≈ 上 VMe(X tr )g(X tr)),
if ^ is close enough to g. On the other hand, in the usual case where nte》ntr,
Var(W) = ɪVar(β(Xtr)^(Xtr)) + ɪVar(^(Xte)) ≈ ɪVar(β(Xtr)g(Xtr)).
ntr	nte	ntr
Thus, t? ≈ 1 which gives our estimator VR =念 Pn=I β(xjr )(yjr — ^(xjr)) + 士 Pn=j g(xte).
BiaS reduction for NR: Consider the NR estimator VNR，六 PnXj ^(xte). Assuming again the
common case where nte》ntr, We have Var(VNR) = n^Var(g(Xte)) ≈ 0, and thus the main
source ofMSE is the bias Ex〜Pte[g(x) — g(x)]. Ifweadd W =+ Pn=I β(xjr )(yjr 一 g(xjr)) to
VNR, we eliminate the bias which gives VR = 士 Pn=I β(xjr )(yjr — g(xjr)) + n^ P=1 g(xte).
3 Robust Estimator
We construct a new estimator VR(ρ) that can be demonstrated to perform robustly against
both the KMM and NR estimators discussed above. In our construction, we split the train-
ing set with a proportion 0 ≤ ρ ≤ 1, i.e., divide {Xtr, Y tr}data , {(xtjr, yjtr)}jn=tr1 into
{XKtrM M , YKtrM M}data , {(xtjr,yjtr)}jbρ=n1trc and{XNtrR,YNtrR}data , {(xtjr,jyjtr)}jjn=trbjρ=n1trc+1.
Then we use {XKtrMM, Xte}data , {{xtjr}jbρ=n1trc, {xite}in=te1}to solve for the weight β in (1) and
use {XNrR, YNR}data to train an NR function ^(∙) = gγ,data(∙) for some Y as in (3). Finally, we
define our estimator VR(ρ) as
bρntr c	nte
VR(P), bρnrτ X β(χjr)(yjr 一g(xjr)) + neXg(χie).
r j =1	e i=1
(6)
First, we remark the parameter ρ controlling the splitting of data serves mainly for theoretical consid-
erations. In practice, the data can be used for both purposes simultaneously. Second, as mentioned,
many g other than (3) could be considered for control variate. However, aside from the availability
of closed-form expressions (4), ^γ,data is connected to the learning theory estimates Cucker & Zhou
(2007). Thus, for establishing a theoretical bound, we focus on g = ^γ,data for now.
Our main result is the convergence analysis with respect to ntr and nte which rigorously justified
the previous intuition. In particular, we show that VR either surpasses or achieves the better rate
4
Under review as a conference paper at ICLR 2020
between VKMM and VNR . In all theorems that follow, the Big-O notations can be interpreted either
as 1 - δ high probability bound or a bound on expectation. The proofs are left in the Appendix.
θ
Theorem 1. UnderAssumptions 1-3, if g ∈ Range(TKθ+) ,the convergence rate of VR(P) satisfies
_ θ	_ θ
IVR (ρ)-ν∣ = O(n-k + nek),	(7)
when g is taken to be gγ,data in (6) with Y = n- θ+2 and n，min(ntr, n©.
Corollary 1. Under the same setting of theorem 1, ifwe choose γ = n-1, we have
θ	θ
——_=—：——	— ——=—：——
IVR(P)-VI = OY+4 + nte2θ+4)	(8)
and if we choose γ = nt-r1 ,
....................-ɪ	—1.
IVr(P)-VI = O(ntr2θ+4 + n- ).	(9)
We remark several implications. First, although not achieving canonical, (7) is an improvement over
—θ	— θ	θ
the best-known O(ntr(2θ+4) + nte(2θ+4)) rate of VKMM When g ∈ Range(TKθ+ )，especially for
small θ, suggesting that VR is more suitable than VKMM when g is irregular. Indeed, θ is a smooth-
θ
ness parameter that measures the regularity of g. When θ increases, functions in Range(TKθ+4)
θ2	θι
get smoother and Range(TKθ2+4) ⊆ Range(TKθ1+4) for 0 < θι < θ2, with the limiting case that
θ → ∞, 2θθ+4 → 1/2 and Range(TK) ⊆ H (i.e. g ∈ H) for universal kernels by Mercer’s theorem.
Second, as in Theorem4ofYu & SzePeSvari (2012), the optimal tuning of Y that leads to (7) depends
on the unknown parameter θ, which may not be adaptive in practice. However, if one simply choose
Y = n—1, VR still achieves a rate no worse than VKMM as depicted in (8).
_1	—	3θ
Third, also in Theorem 4 of Yu & SzePeSvari (2012), the rate of VNR is O(nte2 + ntr 12θ+16) when
θ
g ∈ Range(TKθ+4), which is better on nte but not n^. Since usually n6 < nte, the rate of VKMM
6θ + 8
generally excels. Indeed, in this case the rate of VNR beats VKMM only if limn→∞ nt3fθ+6 /ntr → 0.
-—	—2θ⅛4	— 1、
However, if so, VR can still achieve O(n柝 +4 +n" ) rate in (9) which is better than VNR, by simply
taking Y = nt—r1 , i.e., regularizing the training process more when the test set is small. Moreover, as
—1	— 1
θ → ∞, our estimator VR recovers the canonical rate	as opposed to n^4 in VNR.
θ
Thus, in summary, when g ∈ Range(TKθ+4), our estimator VR outperforms both VKMM and VNR
across the relative sizes of ntr and nte. The outperformance over VKMM is strict when Y is chosen
dependent on θ, and the performance is matched when Y is chosen robustly without knowledge of θ.
For completeness, we consider two other characterizations of g discussed in Yu & Szepesvari (2012):
one is g ∈ H and the other is A∞ (g, F) , infkfkH≤F kg - fk ≤ C(log F)—s for some C, s > 0
(e.g., g ∈ HS(X) with K(∙, ∙) being the Gaussian kernel, where HS is the Sobolev space with
integer s). The two assumptions are, in a sense, more extreme (being optimistic or pessimistic). The
next two results show that the rates ofVR in these situations match the existing ones for VKMM (the
rates for VNR are not discussed in Yu & Szepesvari (2012) under these assumptions).
Proposition 1. Under Assumptions 1-3, ifg ∈ H, the convergence rate of VR(P) satisfies IVR (P) -
—1	— 1
νI = O(ntr2 + nte2), when g is taken to be gγ,data for γ > 0 in (6).
Proposition 2. Under Assumptions 1-3, ifA∞(g,F) , infkfkH≤F kg - fk ≤ C(log F)—S for
some C, s > 0, the convergence rate of VR(P) satisfies IVR(P) — νI = O (log-s ：[；嬴 ),when g
is taken to be gγ,data for Y > 0 in(6).
4 Empirical Risk Minimization
The robust estimator can handle empirical risk minimization (ERM). Given loss function l0 (x, y; θ) :
XX R → R given θ in D, we optimize over minθ∈D E[l0(Xte, Yte; θ)] = minθ∈D Ex〜Pte [l(x; θ)]
5
Under review as a conference paper at ICLR 2020
where l(x; θ)，EY∣χ[l0(x,Y; θ)] to find θ?，argmi□θ∈DEχ~Pte[l(Xte; θ)]. In practice, usually
a regularization term Ω[θ] on θ is added. For example, the KMM in Huang et al. (2007) considers
ntr
min—Ee(Xjr )l0(χjr ,yjr ； θ)+相网.
θ∈D ntr
r j=1
We can carry out a similar modification to utilize VR as
(10)
1	bρntr c	1 nte
min IF X β(xjr)(l0(xjr,yjr； θ) -:(Xjr； θ)) +— X 3； θ) + λΩ[θ],	(11)
θ∈D bρntrc j=1	nte i=1
with β based on {XKmm, Xte} and l(x; θ) being an estimate of l(χ; θ) based on {XNr, YNR}.
For later reference, we note that a similar modification can also be used to utilize VNR:
nte
min nteX l(xte; θ…则
e i=1
Below we discuss two classical learning problems using (11).
(12)
Penalized Least Square Regression: Consider a regression problem with l0(X, y； θ) = (y -
hθ, Φ(x)iH)2, Ω[θ] = ∣∣θkH and y ∈ [0,1]. We have l(x; θ) = E[Y2∣x] - 2g(x)(θ, Φ(x))h +
hθ, Φ(x)iH, and a candidate for l(x, θ) is to substitute g with gγ,data. Then, (11) becomes
bρntrc	2β(Xtr)	1 nte
min E — ɪɪr(yjr- ^(xjr))hθ, Φ(xjr)〉H + — £(g(x『)-(θ, Φ(x)>h)2 + λ∣θ∣H,
θ∈D j=1	bρntrc	nte i=1
by adding and removing the components not involving θ . Furthermore, it simplifies to the QP:
min	-2wT KtOta + (w2 — KtOta)T W3(w2 — KM + λαT K^,	(13)
α∈Rbρntrc+nte	bρntrc	nte
by the representation theorem Scholkopf et al. (2001). Here (KtOt)j = K(Xtot, XjOt) and W3 =
diag(w3) where xtot = xt, (wι)i = β(xtr)(ytr - g(xtr)), (w2)i = 0, (w3)i = 0 for 1 ≤ i ≤
[ρntrC and xtot = xt-bpntr」，(w∕ = 0, (w2)i =0侬(如")，(w3)i = 1 for [ρnt" + 1 ≤ i ≤
bρntr C + nte. Notice (13) has a closed-form, solution a (^W⅛tot + λnteI) -1( LP⅛wι + w2).
Penalized Logistic Regression: Consider a binary classification problem with y ∈ {0,1}, Ω[θ]
kθkHand τ0(χ, y；θ) = y log( 1+exp(片手(=)Μ)+ (1 - y)log( ι+χXι⅞φΦ¾H )∙ Thus, -l(x；θ)
—g(x)hθ, Φ(x)iH + log( 1+XXP)φΦ⅞HH ) and we substitute g with gγ,data. Then, (11) becomes
Lρntr c β(Xtr)
鸣 X 热贴 — O(Xjr))hθ, φ(xjr)iH
+ ɪ X —O(Xte)h。, Φ(Xte)iH + iog(
nte i=1
exp hθ, Φ(Xte)〉h
1 + exp hθ, Φ(Xie))H
) + λ∣θ ∣2H .
which again simplifies to, by Scholkopf et al. (2001), the convex program:
min	WTKt°a
α∈Rbρntr c+nte	bρntr c
WT KtOta
nte
Pin=te1 log( 1
exp(Ktota)bρntrc+i
+eχp (KtOta) bρntrc+i
nte
+ λaTKtota
(14)
—
+
)
Both (11) and (14) can be optimized efficiently by standard solvers. Notably, (11) gives a so-
lution in the form θ = Pi=ι αiK(Xtot, x) which spans on both training and test data. In
contrast, the solution of (10) or (12) only spans on one of them. For example, as shown in
Huang et al. (2007), the penalized least square solution for (10) is θ = Ei=I α%K(Xtr, x) where
a = (K + nteλ diag(β)T)Tytr ( we use a = ( diag(β)K + nteλI)-1 diag(β)ytr in experi-
ments to avoid invertibility issues caused by the sparsity of β), so only the training data are in the
span of the feature space that constitutes θ. The aggregation of both sets suggests a more effec-
tive/robust utilization of data . We conclude with a theorem on ERM similar to Corollary 8.9 in
Gretton et al. (2009), which guarantees the convergence of the solution of (11) in a simple setting.
6
Under review as a conference paper at ICLR 2020
mi. .- - .--G λ	7/ z^ι∖ 1 G'/ 八、一	1	1 /京/ \ 八 ∖ . C / z^ι∖ • .1 I I z^ι I I J
Theorem 2. Assume l(x; θ) and l(x; θ) ∈ H Canbe expressed as (Φ(x),θ)H+f (x; θ) With ∣∣Θ∣∣h ≤
C and l0(x, y; θ) ∈ H as (Υ(x,y), AiH + f(x; θ) with ∣∣Λ∣∣h ≤ C. Denote this class of loss
functions G and further assume l(x; θ) are continuous, bounded by D and L-Lipschitz on θ uniformly
over X for (θ, x) in a compact set D × X. Then, the ERM with Θr ，argmi□θ∈D VR(θ) and
VR(θ) , bρn1trc Pb=trc β(xjr)(l0(xjr, yjr； θ) - l(xj； θ)) + 十 P=I '(xte; θ) SatiSfieS
1	_1
E[l0(Xte, Yte； θR)] ≤E[l0(Xte,Yte； θ?)]+ O(n-2 + n- )∙
5	Experiments
5.1	Toy Dataset Regression
We first present a toy example to provide comparison with KMM. The data is generated as the poly-
nomial regression example in Shimodaira (2000); HUang et al. (2007), where Ptr 〜N(0.5,0.52),
Pte 〜 N(0,0.32) are Gaussian distributions. The labels are generated according to y = -X + x3
and observed in GaUssian noise N(0, 0.32). We sample 500 points in both training and test data and
fit a linear model using ordinary least square (OLS), KMM and the robust estimator, respectively.
On the population level, the best linear fit is y = -0.73x. For simplicity, we the intercept to 0 and
compare the fitted slopes for different approaches. We used a degree-3 polynomial kernel and the
1
Y in gγ,data is set to the default value n- . The tolerance e for β is set similarly as in Huang et al.
(2007) with a slight tuning to avoid an overly-sparse solution. The slope is fitted without regular-
ization. In Figure 1[a], the red curve is the true polynomial regression function and the purple line
is the best linear fit. As we see, the robust estimator outperforms the two other methods, recovering
the green line closest to the best one. The performance over 20 trials are summarized in Figure 1[b].
[a]
[b]
Figure 1: [a] Linear fit with OLS,KMM and Robust estimator; [b] Slope estimation performance
5.2	Real World Dataset for ERM
Next, we test our approach in ERM on a real world dataset, the breast cancer dataset from the UCI
Archive. We consider the second biased sampling scheme in Huang et al. (2007) where the prob-
ability of selecting xi into the training set depends jointly on multiple features and is proportional
to exp(-σj∣xi - Xk) for some σ > 0 and the sample mean X. Since this is a binary classification
problem, we can experiment with both the penalized least square regression and the penalized logis-
tic regression for different sizes of training sets. We used a Gaussian kernel exp(-σ2kXi - Xj k).
The tolerance e for β is set exactly as in Huang et al. (2007). For both experiments, We choose
parameters Y = n-1 as default, λ = 5 by cross-validation and σ1 = -1/100, σ2 = √0∙5. Finally,
we used the fitted parameters (i.e. optimal solution θ in ERM) to predict the labels on the test set
and compare with the hidden real ones. The summary of test error comparison is shown in Figure 2
where we use the term unweighted to denote the case for (12), KMM for (10) and Robust for (11).
7
Under review as a conference paper at ICLR 2020
The robust estimator gives the lowest test error in 5 cases out of 6, confirming our finding on its
improvement over the traditional methods.
Figure 2: Classification performance for penalized [a] least square regression; [b] logistic regression
5.3	Simulated Dataset for Estimation
On an estimation problem, we simulate data from two ten dimensional Gaussian distribution with
different, randomly generated mean and covariance matrix as training and test sets. The target value
is V = Ex〜Pte [g(x)] for an artificial g(x) = sin(c.xk2) + (1+ exp(cTX))T With random ci, c2
and labels are observed with Gaussian noise. A Gaussian kernel exp(-σkxi - xj k) and a tolerance
e for β are set exactly as in Gretton et al. (2009) with σ = √5, B = 1000 and e = √nr-1. We also
ntr
experiment with a different g by substituting gγ,data for a naive linear OLS fit. At each iteration, we
use the sample mean from 106 data points (without adding noise) as the true mean and calculate the
average MSE over 100 estimations for VR, VKMM and VNR respectively. As shown in Table 1, the
performances of VR are again consistently on par with the best case scenarios, even when the usual
assumption ntr < nte is violated.
Table 1: Average MSE for Estimation
	Hyperparameters		VNR		VKMM	VR
λ	0.1, ntr	50, nte =	二 500	0.9970	0.9489	0.9134
λ=	0.1, ntr =	500, nte	二 500	1.0006	0.9294	0.9340
λ	0.1, ntr	= 500, nte	二50	1.0021	0.9245	0.9242
λ	= 10, ntr	50, nte =	500	0.9962	0.9493	0.9467
λ=	10, ntr =	500, nte =	二 500	0.9964	0.9294	0.9288
λ	= 10, ntr	500, nte	二50	0.9965	0.9245	0.9293
6	Conclusion
Motivated both as a variance reduction on KMM and a bias reduction on NR, we introduced a new
robust estimator for tackling covariate shift problems which, through a straightforward integration of
traditional methods, leads to improved accuracy over both KMM and NR in many settings. From a
practical standpoint, the control variates and data aggregation enable the estimation/training process
to be more stable and data-efficient at no expense of increased computational complexity. From
an analytical standpoint, a promising progress is made to reduce the rate gap of KMM towards the
parametric when the regression function lies in range spaces outside of RKHS. For future work, note
the canonical rate is still not achieved and it remains unclear the suitable tool to improve the rate
further, if possible. Moreover, besides the regularized empirical regression function in RKHS, the
eligibility and effectiveness of other estimated regression functions also require rigorous analysis.
8
Under review as a conference paper at ICLR 2020
References
Steffen Bickel, Michael Bruckner, and Tobias Scheffer. Discriminative learning for differing training
and test distributions. In Proceedings of the 24th international conference on Machine learning,
pp.81-88.ACM, 2007.
Jose Blanchet and Henry Lam. State-dependent importance sampling for rare-event simulation: An
overview and recent advances. Surveys in Operations Research and Management Science, 17(1):
38-59, 2012.
John Blitzer, Ryan McDonald, and Fernando Pereira. Domain adaptation with structural correspon-
dence learning. In Proceedings of the 2006 conference on empirical methods in natural language
processing, pp. 120-128. Association for Computational Linguistics, 2006.
Karsten M Borgwardt, Arthur Gretton, Malte J Rasch, Hans-Peter Kriegel, Bernhard Scholkopf,
and Alex J Smola. Integrating structured biological data by kernel maximum mean discrepancy.
Bioinformatics, 22(14):e49-e57, 2006.
Corinna Cortes, Mehryar Mohri, Michael Riley, and Afshin Rostamizadeh. Sample selection
bias correction theory. In International conference on algorithmic learning theory, pp. 38-53.
Springer, 2008.
Felipe Cucker and Ding Xuan Zhou. Learning theory: an approximation theory viewpoint, vol-
ume 24. Cambridge University Press, 2007.
Theodoros Evgeniou, Massimiliano Pontil, and Tomaso Poggio. Regularization networks and sup-
port vector machines. Advances in computational mathematics, 13(1):1, 2000.
Peter W Glynn and Roberto Szechtman. Some new perspectives on the method of control variates.
In Monte Carlo and Quasi-Monte Carlo Methods 2000, pp. 27-49. Springer, 2002.
Arthur Gretton, Alex Smola, Jiayuan Huang, Marcel Schmittfull, Karsten Borgwardt, and Bernhard
Scholkopf. Covariate shift by kernel mean matching. Dataset shift in machine learning, 3(4):5,
2009.
Hirotaka Hachiya, Takayuki Akiyama, Masashi Sugiyama, and Jan Peters. Adaptive importance
sampling with automatic model selection in value function approximation. In AAAI, pp. 1351-
1356, 2008.
James J Heckman. Sample selection bias as a specification error. Econometrica: Journal of the
econometric society, pp. 153-161, 1979.
Jiayuan Huang, Arthur Gretton, Karsten Borgwardt, Bernhard Scholkopf, and Alex J Smola. Cor-
recting sample selection bias by unlabeled data. In Advances in neural information processing
systems, pp. 601-608, 2007.
Jing Jiang and ChengXiang Zhai. Instance weighting for domain adaptation in nlp. In Proceedings
of the 45th annual meeting of the association of computational linguistics, pp. 264-271, 2007.
Takafumi Kanamori, Taiji Suzuki, and Masashi Sugiyama. Statistical analysis of kernel-based least-
squares density-ratio estimation. Machine Learning, 86(3):335-367, 2012.
Mikhail Anatolevich Lifshits. Gaussian random functions, volume 322. Springer Science & Busi-
ness Media, 2013.
Barry L Nelson. Control variate remedies. Operations Research, 38(6):974-992, 1990.
Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on knowledge
and data engineering, 22(10):1345-1359, 2009.
David Pardoe and Peter Stone. Boosting for regression transfer. In Proceedings of the 27th Interna-
tional Conference on International Conference on Machine Learning, pp. 863-870. Omnipress,
2010.
9
Under review as a conference paper at ICLR 2020
Iosif Pinelis et al. Optimum bounds for the distributions of martingales in banach spaces. The Annals
OfProbability, 22(4):1679-1706,1994.
Joaquin Quionero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D Lawrence. Dataset
shift in machine learning. The MIT Press, 2009.
Bemhard SchOlkopf, Ralf Herbrich, and Alex J Smola. A generalized representer theorem. In
International conference on computational learning theory, pp. 416-426. Springer, 2001.
Bernhard SchOlkopf, Alexander J Smola, Francis Bach, et al. Learning with kernels: SuPPort vector
machines, regularization, optimization, and beyond. MIT press, 2002.
Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the log-
likelihood function. Journal of statistical Planning and inference, 90(2):227-244, 2000.
Steve Smale and Ding-Xuan Zhou. Learning theory estimates via integral operators and their ap-
proximations. Constructive aPProximation, 26(2):153-172, 2007.
Masashi Sugiyama and Motoaki Kawanabe. Machine learning in non-stationary environments:
Introduction to covariate shift adaPtation. MIT press, 2012.
Masashi Sugiyama, Matthias Krauledat, and Klaus-Robert MAzller. COvanate shift adaptation by
importance weighted cross validation. Journal of Machine Learning Research, 8(May):985-1005,
2007.
Masashi Sugiyama, Shinichi Nakajima, Hisashi Kashima, Paul V Buenau, and Motoaki Kawanabe.
Direct importance estimation with model selection and its application to covariate shift adaptation.
In Advances in neural information Processing systems, pp. 1433-1440, 2008a.
Masashi Sugiyama, Taiji Suzuki, Shinichi Nakajima, Hisashi Kashima, Paul von Bunau, and Mo-
toaki Kawanabe. Direct importance estimation for covariate shift adaptation. Annals of the Insti-
tute of Statistical Mathematics, 60(4):699-746, 2008b.
Hongwei Sun and Qiang Wu. A note on application of integral operator in learning theory. APPlied
and ComPutational Harmonic Analysis, 26(3):416-421, 2009.
Hongwei Sun and Qiang Wu. Regularized least square regression with dependent samples. Advances
in ComPutational Mathematics, 32(2):175-189, 2010.
Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain
adaptation. In Proceedings of the IEEE Conference on ComPuter Vision and Pattern Recognition,
pp. 7167-7176, 2017.
Aad W Van der Vaart. AsymPtotic statistics, volume 3. Cambridge university press, 2000.
Junfeng Wen, Chun-Nam Yu, and Russell Greiner. Robust learning under uncertain test distributions:
Relating covariate shift to model misspecification. In ICML, pp. 631-639, 2014.
Yi Yao and Gianfranco Doretto. Boosting for transfer learning with multiple sources. In 2010
IEEE ComPuter Society Conference on ComPuter Vision and Pattern Recognition, pp. 1855-
1862. IEEE, 2010.
Yao-Liang YU and Csaba Szepesvari. Analysis of kernel mean matching under covariate shift. In
ICML, pp. 1147-1154. Omnipress, 2012.
Bianca Zadrozny. Learning and evaluating classifiers under sample selection bias. In Proceedings
of the twenty-first international conference on Machine learning, pp. 114. ACM, 2004.
10
Under review as a conference paper at ICLR 2020
7	Appendix
We mention that our proofs rely on learning theory estimates and are different from Yu & Szepesvari
(2012). For example, in (3), γ is used as a free parameter for controlling kf kH, whereas Yu &
Szepesvari (2012) uses the parameter F in (5). Although the two approaches are equivalent from an
optimization viewpoint, with γ being the Lagrange dual variable, the former approach turns out to
be more suitable to analyse VR .
Throughtout the proof, h(∙) ∈ H is assumed to be an unspecified function in the RKHS. Also, we
use EX [∙] to denote expectation over the randomness of X while fixing others and E∣χ [∙] as the
conditional expectation EHX].
Moreover we remark that all results involving gγ,data can be interpreted either as a high probability
bound or a bound on expectation over Edata (i.e., if we train gγ,χ个R,丫3 using XNr, YNr, then
Edata means EXtr ,Ytr ). The same interpretation applies for the results with Big-O notations.
Finally, constants C2, C20, C3, C30 and C300 as well as similar constants introduced later which depend
on R,g(∙) or δ (for 1 - δ high probability bound) will be denoted by a common C during proof for
the ease of presentation.
7.1	Preliminaries
Lemma 1. Under Assumption 3, for any f ∈ H, we have
kfk∞ = SUp ∣hf(∙), Φ(∙,x)iH∣ ≤ RkfkH.	(15)
x∈X
and consequently kf kL 2 ≤ Rkf kH as well.
Lemma 2 (Azuma-Hoeffding). Let X1, ..., Xn be independent and identically distributed random
variables with 0 ≤ X ≤ B, then
1 n	2n2
P(∣- ∑Xi - EX]∣ >e) ≤ 2e-H.	(16)
n i=1
Corollary 2. Under the same assumption of Lemma 2, with probability at least 1 - δ,
.1 工 一 一. ∏	2
| n XXi- EX]1 ≤ Bv 2nlog δ∙	Cn)
i=1
Moreover, an important probability 1 - δ bound we shall use later for L(β∣χtr …Xtr )) follows from
1 ,..., ntr
Yu & Szepesvari (2012) (see also Gretton et al. (2009) and Pinelis et al. (1994)):
7 ( Q	W
L(βlxir,…,xnrtr))
H
(18)
7.2 Learning Theory Estimates
To adopt the assumption in Yu& Szepesvari (2012); Cucker & Zhou (2007) that the true regression
θ
function g(∙) ∈ H but g(∙) ∈ Range(TKθ+ )，We introduce the related results from learning theory.
First, define Z，2θ+4 for some θ > 0 so that 0 < Z < 1/2. Given g(∙) ∈ Range(TK) and m
training sample {(xj, y7-)}m=ι (sampled from Ptr)), we define gγ(∙) ∈ H : X → R to be
gγ (∙)=arfmin ikf - gkLpJY kfkH
(19)
11
Under review as a conference paper at ICLR 2020
where ∣∣f - g∣∣L2	= ∙↑∕Eχ〜Ptr(f (x) - g(x))2 denotes the L2 norm under Ptr. On the other
hand, gγ,data(∙) ∈ H is defined in (3) as
gγ"=afgmm {X(f (Xj)- yj )2+YkfkH 卜
Moreover, following the notations in section 4.5 of Cucker & Zhou (2007), given Banach space
(LPt「，k ∙ ∣∣l2 ) and the kernel-induced Hilbert subspace (H, k ∙ ∣∣h), we define a K-functional:
2
LPtr
× (0, ∞) → R to be
K(i,γ) , ,nH {11l-fl∣L2,+ Ykfkh}
for l(∙) ∈ LPtT and t > 0. Moreover, for 0 < r < 1, the interpolation space (LP , H)r consists of
all the elements l(∙) ∈ LPtT such that
~ ,, ,
∣∣7∣∣ △	K(I, Y) /
klkr，SUp—— < ∞.
γ>0 Yr
Lemma 3. Define K : LP2 × (0, ∞) → R to be
K(l, Y), fi∈nHf {kl-fk2LP2 +Ykfk2H},
then for any l(∙) ∈ (LPt r, H)r, we have
(20)
(21)
K(l, γ)
SUP------- ≤ SUP
γ>0 Yr	γ>0
K(i, √γ) V
(√γ)r
klkr2 < ∞.
(22)
Proof. It follows from ʌ/a + b ≤ √α + ʌ/b,	∀a, b ≥ 0 that
Pκ(i,γ) ≤ K(1, √γ).
Thus, for any l(∙) ∈ (LPt', H)r, We have
(23)
sup g ≤
γ>0 γr
~ , .
K(i, √Y)
SUP / L∖
γ>0 (√γ)
2
= klkr2 < ∞.
(24)
r
□
θ
On the other hand, assuming g(∙) ∈ Range(TKθ+4), it follows from the proof of Theorem 4.1 in
Cucker & Zhou (2007) that
g(∙) ∈ (LPtr,H+)θ+2	(25)
where H+ is a closed subspace of H spanned by eigenfunctions of the kernel K (e.g.,H+ = H
when Ptr is non-degenerate, see Remark 4.18 of Cucker & Zhou (2007)). Indeed, the next lemma
shows we can measure smoothness through interpolation space just as range space.
θ
Lemma 4. Assuming Ptr is non-degenerate on X. Then if g ∈ Range(TKθ+4)，we have g ∈
θ -e
(LP , H) _^_. On the other hand, if g ∈ (LP , H) _^_, then g ∈ Range(TKθ+ ) forall e > 0.
Proof. It follows from Theorem 4.1, Corollary 4.17 and Remark 4.18 of CUcker & ZhoU (2007). □
Now we are ready to adopt the standard assUmptions and theoretical resUlts from learning theory
in RKHS. They can be foUnd in CUcker & ZhoU (2007); SUn &WU (2009); Smale & ZhoU (2007);
Yu & SzePesvari (2012). First, given g(∙) ∈ Range(TK) and m training sample {(xj , yj )}jm=1
(samPled from Ptr)), it follows from Lemma 3 of Smale & Zhou (2007) (see as well Remark 3.3
and Corollary 3.2 in Sun & Wu (2009)) that
kgγ - g kLP2	≤ C2 γζ .
(26)
12
Under review as a conference paper at ICLR 2020
Second, it follows from Theorem 3.1 in Sun & Wu (2009) as well as Smale & Zhou (2007); Sun &
Wu (2010) that
kgγ - gγ,datakLP ≤。2 (TI/m-1/ + 1-53"),	(27)
Ptr
and, by triangle inequality,
kg - gγ,datakL2 ≤ C3(γζ + YT/2m-1/2 + Y-1m-3/4).	(28)
Ptr
-	3
Notice here that by choosing Y = m 4(1+ζ), We recover the Corollary 3.2 of SUn & WU (2009).
Finally it follows from Theorem of Smale & Zhou (2007), we have
kgγ - gγ,datakH ≤ C3ITmT1/,	(29)
with C3 = 6Rlog 2. In fact, if we define σ2，Ex〜PtrEγ∣x(g(x) - Y)2,then Theorem 3 of Smale
&Zhou (2007) StatedthatkgY - gγ,data∣∣H ≤ C30((√σ2 + ∣∣gγ - g∣∣L2 )γ-1m-1/2 + YTmT).
Ptr
7.3 Main Proofs
Proofof Theorem 1 and Corollary 1. If g ∈ Range(TKθ+) (i.e. Z = 2θ+) and we set h(∙)
gγ(∙) and ^ = gγ,χ个R,γtr, for some γ > 0, then:
VR(ρ) - ν
1	bρntr c	1	bρntr c
bρnj X β(χjr)(yjr-g(χjr)) + E X (β(χjr)-β(χjr))(g(χjr)-h(χjr))
1
bρntr J
bρntrc
X (β(xjr) - β(xjr))(h(xjr) - ^(xjr))
j=1
1	bρntrc	1 nte
+1~j X β(xjr)(g(xjr) - ^(xjr)) + — Xg(xte)-ν.
bρntr J j=1	nte i=1
(30)
To boUnd terms in (30), we first Use Corollary 2 to conclUde that with probability at least 1 - δ,
1	bρntrc	I―1	2
Im X β(xjr)(yjr -g(xjr))l≤B√百必=。(“1/2).
(31)
We hold on our discussion for the second term. For the third term, since h,g ∈ H,
1
1pntr J
bρntr c
X (β(xjr) - β(Xjr))(h(xjr) - g(xjj))
j=1
1	bρntr c
= bpn;C X (β(χjj)-β(χjj))〈h-g,φ(χjj)〉H
bρntr c
=Kh- g，m X (β(χjj)-β(Xjj))φ(χjj))H
≤kh - g∣∣H(L(例 + L(e|xtr,…,xtr	I)) ≤ 2kh - gkHL(β∣xtr,…,xtr	),	(32)
1 ,, bρntr c	1 ,, bρntrc
by definition of (1). Thus, when taking h = gγ and g = gγ,χ个R,γ芳R for some γ, we can combine
(18) and (29) to guarantee, with probability 1 - 2δ,
1
IPntjJ
bρntr c
X (β(xjj) - β(xjj))(h(xjj) - g(xjj))
j=1
≤,8log 2RC(I-P)T/2(YTn-v2) ∙
=O(YTn-^"(n；1 + n-1)1).
(33)
13
Under review as a conference paper at ICLR 2020
For the last term T，bρn1trc Pb=ntr c β(xjr )(g(xjr) - ^(xjr)) + n1e Pn=i g(xte) -V ,the analysis
relies the splitting of data, as we notice that,
bρntr c	nte
EIXNR,YNR bρn^T X β(XjrXg(Xjr) - g(Xjr)) + — Xg(Xte)-V
ρntr j =1	nte n=1
=Ex 〜Ptr[β (x)g(x)] — V - Ex 〜Ptr [β(x)g(x)] + Ex 〜Pte [^(x)]
=Ex〜Pte [g(x)] - V - Ex〜Pt」g(x)] + Ex〜pj^(x)]
=0.	(34)
Notice for the second line follows since g(∙) is determined by {XjtrR, YNrR} and thus is independent
with {XKtrMM,YKtrMM} or {Xte}. Thus, we have
Var(τ) =Var(E|XNtrR,YNtrR (τ)) + E[Var|XNtrR,YNtrR (τ)]
=E[Var|XNtrR,YNtrR (τ)]
=bρnj E[Varx~Ptr ∣XNr,YNR (β(x)(g(x) - g(xy)')] + — E[Varx~Pte|XNR,YNR (g(x))]
B2	1
≤bρnjEXNR,YNRkg - gkLPtr + neEXNR,YNR MkLPte
B2	B
≤bρntjEXNR,YNRkg - gkLPtr + neEXNR,YNR MkLPt,,	(35)
and we can use the Chebyshev’s inequality and Lemma 1 to conclude, with probability at least 1 -δ,
k B B2	^	BR2
|T| ≤ ʌ/ʌʌ/^∣	PEXtrR,Ytr,kg - gkL2 +	,	(36)
δ bρntrc	NR NR	LPtr	nte
which becomes, by (28), probability 1 - 2δ:
|T| ≤J1J当C(1 - ρ)-3∕4(γζ + …n：1/ + γ-1n^4) + BR
δ	bρntrc	tr	tr	nte
=O((γζ + γ-1∕2nt-r1∕2 + γ-1nt-r3∕4)nt-r1∕2 +nt-e1∕2)	(37)
with Z = 2θ+4. Now, to bound the second term 京 C Pb=nιtrC (β(xjr) -β(xjr ))(g(Xjr) - h(xjjr)),
1
bρntr C
B
≤ bρntr C
∣B
≤l bρntrC
bρntrc
X l(β(χjr)-β(χjr))(g(Xjr)-gγ(Xjr))l
j =1
bρntrC
X ∣g(Xjr)-gγ(Xjr)l
j =1
bρntrC
• X ∣g(Xjr) - gγ(Xjr)|- Bkg- gγkLP ∣ + Bkg-gγkLP
Ptr	Ptr
j=1
≤∖ ^τ∖ ------kg - gγHL2	+ Bkg - gγIIL2
δ	ρntr	LPtr	Ptr
≤∕1 BCYZJ ɪ + Cγζ = O(γζ) = O(γ 2θ+4).
δ	ρntr
(38)
where LPtT denotes the 1-norm Ex〜Ptr |g(X) - gγ(x)|. Notice the second to last line follows from
Chebyshev inequality, Cauchy-Schwarz inequality and the last line from (26).
14
Under review as a conference paper at ICLR 2020
Thus, when taking h = gγ and g = gγ,χtrR,γ#R for some γ > 0, we can combine (31),(33),(37)
and (38) to have,
∣Vr(p) - ν∣ =O(n；2) + O(Y2θ+4) + O(YTn-r1∕2(n-r1 + n-1)2)
+ O((Y 2θ+4 + YT/2n-r1/2 + YTn-r3/4)n-r1/2 + nte1/2)
~	-1	-1	1	_1	1	_1	-1、	一、
=OStr2 + nte2	+ Y 2θ+4	+ Y 2 ntr1	+ Y 2	ntr2 nte2 ),	(39)
after simplification. Now, if we take Y = n- θ+1 where n，min(ntr, nte), then (39) becomes
1	_ θ	θ + 2	1	θ
∣Vr(p) — ν| = O(n-2 + n 2(θ+1) + n2(θ+1) n-1) =O(n-2θ+2)
θ	θ
=O(n-r …+ n；L),	(40)
which is the statement of the theorem. However, note that if we choose Y = n-1, the rate becomes
- θ	- θ	6θ+8
O(ntr(2θ+4) + nte(2θ+4)). Moreover if limn→∞ n3eθ+6 /ntr → 0 and we choose Y = n_1, then the
一	〜-ɪ -加	一
rate becomes O(ntr +4 + %/ ).	□
θ
Proofof Proposition 1. Fixing y > 0, if g ∈ H(i.e., g ∈ Range(TKθ+ ) with θ → ∞)), then by
definition of gγ we would have:
kgγ -gk2L2 +Ykgγk2H	kg - gk2L2 +Ykgk2H
kgγ kH ≤---------PY------------≤-------------PY---------= kgkH,	(41)
or equivalently kgγ kH = O(1) since the fixed true regression function kgkH = O(1). Thus, a
simplified analysis shows:
1	bρntr c
VR(P)-V = TPM	X β(Xjr用'-V
1	bρntr c	1 nte
+ 由 X β(xjr项Xjr)- 嬴 X虱X)
(42)
Note that the first term on the right is nothing but the VKMM estimator with 100 × P percent of the
training data and we shall denote it as VKMM(P) without ambiguity. For the second term, assuming
g = %,xnr,ynr, is bounded by
1
bρntr C
1
bρntr C
bρntr c	1 nte
X β(xjr )^(xjr) - n- X g(xte)
j=1	te
bρntr c
g,
E β(xjr)(g, φ(xjr)〉h
j=1
1	bρntr c
,「X β(xjr)Φ(xjr)-
bPntrC i=1 j j
i=1
nte
-n- X @ φ(χηte )〉h
nte
i=1
nte
丁 Xφ(Xte))	≤ kgγ,χttrR,γNrRkHL(∣β),
nte i=1	H
(43)
Then, by (42) and (43), we have
IVR(P) - V | ≤∣VKMM (P) - Vl + L(6)(kgγ - gγ,χNrR,γNrR IIH + IIgY IlH)
_ 1	_1
=O(n-2 + n-),	(44)
following (41), (29) and Theorem 1of Yu& Szepesvari (2012).
□
15
Under review as a conference paper at ICLR 2020
Proof of Proposition 2. If g only satisfies the condition A∞(g, F) , infkfkH≤F kg - fk ≤
C(log F)-s for some C, s > 0, then we again follow the analysis in the proof of Proposition 1
and arrive at the decomposition in (42)
IVR(P)- V| ≤lVKMM(P)- ν| + LXe)(IlgY - gγ,XtrR,YNR IIh + kgγkH)
=O(log-s ntrnte ),	(45)
ntr+nte
which is the rate of VKMM by Theorem 3ofYu& Szepesvari (2012).	□
Proof. Proof of Theorem 2
Define , supθ∈D VR(θ) - E[l0(Xte, Y te; θ)], we have
E[l0(Xte, Yte； Θr)] i ≤ Vr(Θr) ≤ VR(θ?) ≤ E[l0(Xte, Yte； θ?)]+ J	(46)
On the other hand, we know by triangle inequality that is bounded by
i 1
SUp Ii-----1
θ∈D bPntrc
1
+ sup I I----F
θ∈D bPntrc
bρntr c	nte
X β(χjr)i0(χjr,yjr; θ) - - X i(χte; θ)∣
j=1	nte i=1
bρntr c	nte	nte
X β(χjr)j(χjr; θ)——X l(χte; θ) ∣ + sup ∣ — X i(χte; θ) - E[i(Xte; θ)]∣,
nte	θ∈D nte
j=1	t=1	t=1
-1	-1
where the first term is bounded by O(%r2 + nte2) following Corollary in Gretton et al. (2009).
-1	- 1
Moreover, the second term is also O(ntJ + n") as in (43) or Lemma 8.7 in Gretton et al. (2009).
For the last term, due to the Lipschitz and compact assumption, it follows from Theorem 19.5 of
Van der Vaart (2000) (see also Example 19.7 of Van der Vaart (2000)) that function class G is Pte-
Donsker, which means that
nte
Gn(θ) , √nte — X l(xte; θ) - Ex〜Pte [l(x; θ)]
nte t=1
converges in distribution to a Gaussian Process G∞ with zero mean and covariance function
Cov(G∞(θι), G∞(θ2)) = Ex〜Pte(l(x; θι)l(x; θ2)) - Ex〜Ptel(x; θι)Ex〜Ptel(x; θ2). Notice
G∞ can be viewed as random function in C(D), the space of continuous and bounded func-
tion on θ. Since for any z ∈ C(D), the mapping z → IzI∞ , supθ∈D z(θ) is continu-
ous with respect to the supremum norm, it follows from the continuous-mapping theorem that
n；e supθ∈d ∣ n!~ pn=11(*3 θ)-E[l(Xte; θ)] ∣ converges in distribution to ∣∣G∞k∞ which has finite
expectations based on the assumptions on G (see, e.g., Section 14, Theorem 1 of Lifshits (2013)).
Thus, by definition of convergence in distribution, for any δ > 0, we can find some constant D0 that
P[kGnk∞ > D0] = P[kG∞k∞ > D0] + o(1) ≤δ+o(1),	(47)
which means, we can find some N such that when nte > N,
1 nte	_ 1
Pte( sup — Vl(xte; θ) - E[l(Xte; θ)] >n-2D0) = Pte(kGnk∞ > D0) ≤ 2δ,
θ∈D I nte t=1	I
and consequently, with probability 1 - 2δ, we have
1 nte	_1
sup I 一 El(Xte; θ) - E[l(Xte; θ)]∣ ≤ nte2D0.
θ∈D nte t=1	t	te
In other words, we also have
1 nte	_ 1
sup I — E l(xte; θ) - E[l(Xte; θ)] I = O(n-2),
θ∈D nte t=1	t	te
which concludes our proof.	□
16