Under review as a conference paper at ICLR 2020
Certified Robustness to Adversarial Label-
Flipping Attacks via Randomized Smoothing
Anonymous authors
Paper under double-blind review
Ab stract
This paper considers label-flipping attacks, a type of data poisoning attack where
an adversary relabels a small number of examples in a training set in order to
degrade the performance of the resulting classifier. In this work, we propose a
strategy to build linear classifiers based on deep features that are certifiably robust
against a strong variant of label-flipping, where the adversary can target each test
example independently. In other words, for each test point, our classifier makes
a prediction and includes a certification that its prediction would be the same had
some number of training labels been changed adversarially. Our approach lever-
ages randomized smoothing, a technique that has previously been used to guar-
antee test-time robustness to adversarial manipulation of the input to a classifier.
Further, we obtain these certified bounds with no additional runtime cost over
standard classification. On the Dogfish binary classification task from ImageNet,
in the face of an adversary who is allowed to flip 10 labels to individually target
each test point, the baseline undefended classifier achieves no more than 29.3%
accuracy; we obtain a classifier that maintains 64.2% certified accuracy against
the same adversary. We generalize our results to the multi-class case, providing
what we believe to be the first multi-class classification algorithm that is certifiably
robust to label-flipping attacks.
1	Introduction
Modern classifiers, despite their widespread empirical success, are known to be susceptible to adver-
sarial attacks. In this paper, we are specifically concerned with so-called “data-poisoning” attacks
(formally, causative attacks [Barreno et al. 2006; Papernot et al. 2018]), where the attacker manip-
ulates some aspects of the training data in order to cause the learning algorithm to output a faulty
classifier. Work in this area includes label-flipping attacks (Xiao et al., 2012), where the labels of
a training set can be adversarially manipulated to decrease performance of the trained classifier;
general data poisoning, where both the training inputs and labels can be manipulated (Steinhardt
et al., 2017); and backdoor attacks (Chen et al., 2017; Tran et al., 2018), where the training set is
corrupted so as to cause the classifier to deviate from its expected behavior when triggered by a spe-
cific pattern, without causing a noticeable drop in overall accuracy. However, unlike the alternative
“test-time” adversarial setting, where reasonably effective defenses exist to build adversarially ro-
bust classifiers, relatively little work has been done on building classifiers that are certifiably robust
to targeted data poisoning attacks.
In this work, we propose a strategy for building classifiers that are certifiably robust against label-
flipping attacks. In particular, we propose a pointwise certified defense—by this we mean that with
each prediction, the classifier includes a certification guaranteeing that its prediction would not be
different had it been trained on data with some number of labels flipped. Existing works on certified
defenses make statistical guarantees over the entire test set, but they make no guarantees as to the
robustness of a prediction on any particular test point. Thus, while these algorithms provide general
robustness to any single corruption, a determined adversary could still cause a specific test point to
be misclassified. We therefore consider the threat of a worst-case adversary that can make a training
set perturbation to specifically target each test point individually. This motivates a defense that can
certify each of its individual predictions, as we present here. To the best of our knowledge, this work
represents the first pointwise certified defense to data poisoning attacks.
1
Under review as a conference paper at ICLR 2020
Our approach leverages randomized smoothing (Cohen et al., 2019), a technique that has previously
been used to guarantee test-time robustness to adversarial manipulation of the input to a deep net-
work. However, where prior uses of randomized smoothing randomize over the input to the classifier
for test-time guarantees, we instead randomize over the entire training procedure of the classifier.
Specifically, by randomizing over the labels during this training process, we obtain an overall clas-
sification pipeline that is certified to be robust (i.e., to not change its prediction) when some number
of labels are adversarially manipulated in the training set. Although a naive implementation of this
approach would not be computationally feasible, we show that by using a linear least-squares classi-
fier over the output of a pre-trained network, we can obtain these certified bounds with no additional
runtime cost over standard classification—and we again emphasize that this is the first such certified
defense against pointwise label-flipping attacks, even for linear classifiers.
We evaluate our proposed classifier on several benchmark datasets common to the data poisoning
literature. Specifically, we demonstrate that our randomized classifier is able to achieve 75.7% cer-
tified accuracy on MNIST 1/7 even when the number of allowed label flips would drive a standard,
undefended classifier to less than 50%. Similar results in experiments on the Dogfish binary classi-
fication challenge from ImageNet and IMDB review sentiment database validate our technique for
more challenging datasets, and we further experiment on the full MNIST dataset to demonstrate
its effectiveness for multi-class classification. On top of this, our classifier maintains a reasonably
competitive non-robust accuracy (e.g., 94.5% on MNIST compared to 99.1% for the undefended
classifier).
2	Related Work
Data-Poisoning attacks A data-poisoning attack (MUnoz Gonzalez et al., 2017; Yang et al., 2017)
is an attack whereby an adversary corrupts some portion of a training set or adds new inputs, with
the goal of degrading the performance of the learned model. The attack can be targeted to caUse
poor performance on a specific test example or can instead simply redUce the overall test perfor-
mance. The adversary is assUmed to have perfect knowledge of the learning algorithm, so secUrity
by design—as opposed to obscUrity—is the only viable defense against sUch attacks. The adversary
is also typically assUmed to have access to the training set and, in some cases, the test set.
PrevioUs work has investigated attacks and defenses for data-poisoning attacks applied to featUre
selection (Xiao et al., 2015), SVMs (Biggio et al., 2011; Xiao et al., 2012), linear regression (LiU
et al., 2017), and PCA (RUbinstein et al., 2009), to name a few. Some attacks can even achieve
sUccess with “clean-label” attacks, inserting innocUoUs, poisoned yet “correctly” labeled training
examples bUt caUsing the classifier to perform poorly (Shafahi et al., 2018; ZhU et al., 2019). For an
overview of data poisoning attacks and defenses in machine learning, see Biggio et al. (2014).
Label-flipping attacks A label-flipping attack is a specific type of data-poisoning attack where
the adversary is restricted to changing some of the training labels. The nUmber of allowed changes
coUld be an exact nUmber or a percentage of the total labels. The classifier is then trained on the
corrUpted training set, with no knowledge of which labels have been tampered with.
Unlike random label noise, for which many robUst learning algorithms have been sUccessfUlly devel-
oped (Natarajan et al., 2013; LiU & Tao, 2016; Patrini et al., 2017), adversarial label-flipping attacks
can be specifically targeted to exploit the strUctUre of the learning algorithm, significantly degrading
performance. RobUstness to sUch attacks is therefore harder to achieve, both theoretically and empir-
ically (Xiao et al., 2012; Biggio et al., 2011). A common defense techniqUe is sanitization, whereby
a defender attempts to identify and remove or relabel training points that may have had their labels
corrUpted (PaUdice et al., 2019; Taheri et al., 2019). UnfortUnately, recent work has demonstrated
that this is often not enoUgh against a sUfficiently powerfUl adversary (Koh et al., 2018).
Certified defenses For a review on robUstness to random noise, we refer the reader to RoUsseeUw
& Leroy (1987)—oUr focUs is on adversarial noise, a mUch more challenging problem. Existing
works on certified defenses to adversarial data poisoning attacks typically focUs on the regression
case and provide broad statistical gUarantees over the entire test distribUtion. A common approach
to sUch certifications is to show that a particUlar algorithm recovers some close approximation to the
best linear fit coefficients (LiU et al., 2017; Prasad et al., 2018; Shen & Sanghavi, 2019), or that the
2
Under review as a conference paper at ICLR 2020
expected loss on the test distribution is bounded (Klivans et al., 2018; Chen & Paschalidis, 2018).
These results generally rely on assumptions on the train and test distributions: some assume sparsity
in the coefficients (Karmalkar & Price, 2018; Chen et al., 2013) or corruption vector (Bhatia et al.,
2015); others require limited effects of outliers (Steinhardt et al., 2017). As mentioned above, all
of these methods fail to provide guarantees for individual test points. Additionally, most of these
statistical guarantees are not meaningful when applied to deep, non-linear architectures.
Randomized smoothing Since the discovery of adversarial examples (Szegedy et al., 2013; Good-
fellow et al., 2015), the research community has been investigating techniques for increasing the ro-
bustness of deep networks to adversarial perturbations. After a series of heuristic defenses (Metzen
et al., 2017; Feinman et al., 2017), followed by attacks breaking them (Athalye et al., 2018; Carlini
& Wagner, 2017), focus began to shift towards the development of provable robustness.
One approach which has gained popularity in recent work is randomized smoothing. Rather than
certifying the original classifier f, randomized smoothing defines anew classifier g whose prediction
at an input x is the class assigned the most probability when x is perturbed with noise from some
distribution μ and passed through f. That is, g(x) = argmaxc Pe〜μ(f (X + E)= c). This new
classifier g is then certified as robust, ideally without sacrificing too much accuracy compared to f.
The original formulation was presented by Lecuyer et al. (2018) and borrowed ideas from differential
privacy. The above definition is due to Li et al. (2018) and was popularized by Cohen et al. (2019),
who derived a tight robustness guarantee. Follow-up work has focused on optimizing the training
procedure of f (Salman et al., 2019) and extending the analysis to other types of distributions (Lee
et al., 2019). For more details, we refer the reader to Cohen et al. (2019).
3	A general view of randomized smoothing
We begin by presenting a general viewpoint of randomized smoothing. Under our notation, random-
ized smoothing constructs an operator G(μ, φ) that maps a binary-valued1 function φ : X → {0,1}
and a smoothing measure μ : X → R+, with JX μ(x)dx = 1, to the expected value of φ under
μ (that is, G represents the “vote” of φ weighted by μ). For example, φ could be a binary image
classifier and μ could be some small, random pixel noise applied to the to-be-classified image. We
also define a “hard threshold” version g(μ, φ) that returns the most probable output (the majority
vote winner). Formally,
G(μ, Φ)
Ex 〜μ[φ(x)] = /
X
μ(x)φ(x)dx and g(μ, φ) = 1{G(μ, φ) ≥ 1/2},
(1)
where 1{∙} is the indicator function. Intuitively, for two smoothing measures μ,ρ : X → R+ that
are very similar, we would expect that for most φ, even though G(μ, φ) and G(ρ, φ) may not be
equal, the threshold function g should satisfy g(μ, φ) = g(ρ, φ). Further, the degree to which μ
and P can differ while still preserving this property should increase as G(μ, φ) approaches either 0
or 1, because this increases the “margin” with which the function φ is 0 or 1 respectively over the
measure μ. More formally, we define a general randomized smoothing guarantee as follows.
Definition 1. Let μ : X → R+ be a smoothing measure over X, with JX μ(x)dx = 1. Then a
randomized smoothing robustness guarantee is a Specfication of a distance measure d(μ, ρ) and a
function f : [0, 1] → R+ such that for all φ : X → {0, 1} and all measures ρ : X → R+ ofa given
form,
g(ρ,φ) = g(μ, Φ) whenever d(μ,ρ) ≤ f(G(μ,φ)).	(2)
For brevity, we will sometimes use P in place of G(μ, φ), representing the probability with which
the majority class wins the vote (this is analogous to pA in Cohen et al. (2019)).
Instantiations of randomized smoothing This definition is rather abstract, so we highlight con-
crete examples of how it can be applied to achieve certified guarantees against adversarial attacks.
1For simplicity, we present the methodology here in terms of binary-valued functions, which will correspond
eventually to binary classification problems. The extension to the multiclass setting requires additional notation,
and thus is deferred to the appendix.
3
Under review as a conference paper at ICLR 2020
Example 1.	The randomized smoothing guarantee of Cohen et al. (2019) uses the smoothing mea-
Sures μ = N(xo, σ2I), a Gaussian around the point xo to be classified, and P = N(xo + δ, σ2I), a
Gaussian around x0 perturbed by δ. They prove that (2) holds for all classifiers φ if we define
d(μ,ρ) = 11∣δk2 ≡ p2KL(μ k P), f(p) = ∣Φ-1(p)∣,	(3)
where KL(∙) denotes KL divergence and Φ-1 denotes the inverse CDF ofthe Gaussian distribution.
Although this previous work focused on the case of randomized smoothing of continuous data via
Gaussian noise, this is by no means a requirement of the approach. For instance, Lee et al. (2019)
considers an alternative approach for dealing with discrete variables.
Example 2.	The randomized smoothing guarantee of Lee et al. (2019) uses the factorized smoothing
measure in d dimensions μa,κ(X) = πd=iμα,K,i(xi), defined with respect to parameters
α ∈ [0, 1], K ∈ N, and a base input z ∈ {0, . . . , K}d, where
μα,K,i (Xi )
if xi = zi
,	ifxi ∈ {0, . . . , K}, xi 6= zi,
with xi being the ith dimension of x. Pα,K is similarly defined for a perturbed input z0. They
guarantee that (2) holds if we define
d(μ,ρ) = r d= ∣∣z0 — zko, f (p) = FaKd(max(p, 1 - Py).
(4)
In words, the smoothing distribution is such that each dimension is independently perturbed to one
of the other K values uniformly at random with probability 1 - α. F is a combinatorial function;
Fa,κ,d(p) is defined as the maximum number of dimensions—out of d total—by which μa,κ and
Pa,κ can differ such that a set with measure P under μa,κ is guaranteed to have measure at least 2
under Pα,K. Lee et al. (2019) prove that this value is independent ofz and z0.
Finally, concurrent work has considered a more general form of randomized smoothing that doesn’t
require strict assumptions on the distributions but is still able to provide similar guarantees.
Example 3 (Generic bound from Anonymous (2019)). Given any two smoothing distributions μ, P,
we have the generic randomized smoothing robustness certificate, ensuring that (2) holds with defi-
d(μ,ρ) = KL(ρ k μ), f (p) = - 2log(4p(1 -Py).	(5)
Randomized smoothing in practice For most classifiers, the expectation G(μ, φ) cannot be com-
puted exactly, and so we must resort to Monte Carlo approximation. In this “standard” form of
randomized smoothing, We draw multiple random samples from μ and use these to construct a high-
probability bound on G(μ, φ) for certification. More precisely, this bound should be a lower bound
on G(μ, φ) when the hard prediction g(μ, φ) = 1 and an upper bound otherwise; this ensures in both
cases that we underestimate the true certified robustness for the classifier g. The procedure is shown
in Algorithm 2 in Appendix A. These estimates can then be plugged into a randomized smoothing
robustness guarantee to provide a high probability certified robustness bound for the classifier g.
4 Label-Flipping Robustness
We now present the main contribution of this paper, a technique for using randomized smoothing
to provide certified robustness against label-flipping attacks. Specifically, we first propose a generic
strategy for applying randomized smoothing to certify a prediction function against pointwise label
flipping attacks. We show how this general approach can be made tractable using linear least-
squares classification combined with pre-trained deep features, and we use the Chernoff inequality
to analytically bound the relevant probabilities for the randomized smoothing procedure. Notably,
although we are employing a randomized approach, the final algorithm does not use any random
sampling, but rather relies upon a convex optimization problem to compute the certified robustness.
To motivate the approach, we note that in prior work, randomized smoothing was applied at test
time with the function φ : X → {0, 1} being a (potentially deep) classifier that we wish to smooth.
4
Under review as a conference paper at ICLR 2020
Algorithm 1 Randomized smoothing for label-flipping robustness
Input: feature mapping h : Rd → Rk; noise parameter q; training set {(xi, yi) ∈ Rd × {0, 1}}in=1
(with potentially adversarial labels); additional inputs to predict {xj ∈ Rd}jm=1
1.	Pre-compute matrix M
M = X (XTX + λI)-1	(6)
∖x小Qra Y = h2k>σ 、QnH \ 一 门-L ^2k σmaχ(XTX)
where X ≡ h(x1：n) and λ = (1 + q) F σmin(XTX)
for j = 1, . . . , m do
1.	Compute vector αj = Mh(xj )T
2.	Compute optimal Chernoff parameter t via Newton’s method
t? = arg min < t/2 + X log(q +(1 - q)e-tαj) + X log((1 - q) + qe-tαj) >	(7)
t I	i：yi=1	i：yi=0
and let p? = max(1 - B|t?| , 1/2) where B|t?| is the Chernoff bound (13) evaluated at |t?|.
Output: Prediction yj = 1 {t? ≥ 0} and certification that prediction will remain constant for
up to r training label flips, where
r
log(4p*(1 - p?))
_ 2(1 - 2q)log (ɪ]
(8)
(or a larger number of flips using the exact method of Lee et al. (2019)).
end for
However, there is no requirement that the function φ be a classifier at all; the theory holds for any
binary-valued function. Instead of treating φ as a trained classifier, we consider φ to be an arbitrary
learning algorithm which takes as input a training dataset {xi, yi}in=1 ∈ (X × {0, 1})n and an
additional example 2 xn+1 without a corresponding label, which we aim to predict. In other words,
the combined goal of φ is to first train a classifier and then predict the label of the new example.
Thus, we consider test time outputs to be a function of both the test time input and the training data
that produced the classifier. This perspective allows us to reason about how changes to training data
affect the classifier at test time, reminiscent of work on influence functions of deep neural networks
(Koh & Liang, 2017; Yeh et al., 2018). When applying randomized smoothing in this setting, we
randomize over the labels in the training set, rather than over the test-time input to be classified.
Analogous to previous applications of randomized smoothing, if the majority vote of the classifiers
trained with these randomly sampled labels has a large margin, it will confer a degree of adversarial
robustness to some number of adversarially corrupted labels.
To formalize this intuition, consider two different assignments of n training labels Y1, Y2∈ {0, 1}n
which differ on precisely r labels. Let μ (resp. ρ) be the distribution resulting from independently
flipping each of the labels in Y1 (resp. K) with probability q. It is clear that as r increases, d(μ, P)
should also increase. In fact, it is simple to show (see Appendix B.3 for derivation) that the exact
KL divergence between these two distributions is
KL(μ Il ρ) = KL(P Il μ) = r(1 - 2q) log
(9)
Plugging in the robustness guarantee (5), We have that g(μ, φ) = g(ρ, φ) so long as
r≤
log(4p(1 - P))
2(1 - 2q)log (ɪ
(10)
This implies that for any test point, as long as (10) is satisfied, g’s prediction (the majority vote
weighted by the smoothing distribution) will not change if an adversary corrupts the training set
2Note that our algorithm does not actually require access to the test data to do the necessary precomputation.
We present it here as such merely to give an intuitive idea of the procedure.
5
Under review as a conference paper at ICLR 2020
from Y1 to Y2, or indeed to any other training set that differs on at most r labels. We can tune the
noise hyperparameter q to achieve the largest possible upper bound in (10); more noise will likely
decrease the margin of the majority vote p, but will also decrease the divergence.
Computing a tight bound This approach has a simple closed form, but the bound is not tight. We
can derive a tight bound via a combinatorial approach as in Lee et al. (2019). By precomputing the
quantities F--q,ι,n(r) from Equation (4) for each r, We can simply compare G(μ, φ) to each of these
and thereby certify robustness to the highest possible number of label flips. This computation can
be expensive, but it provides a significantly tighter robustness guarantee, certifying approximately
twice as many label flips for a given bound on G(μ, φ) (See Figure 5 in Appendix D). We make use
of this tighter bound in our experiments, but We emphasize that meaningful results can be achieved
even with the looser bound, which is orders of magnitude cheaper to compute.
4.1 Efficient implementation via least squares classifiers
There may appear to be one major impracticality of the algorithm proposed in the previous section,
if considered naively: treating the function φ as an entire training-plus-single-prediction process
would require that we train multiple classifiers, over multiple random draws of the labels y, all to
make a prediction on a single example. In this section, we describe a sequence of tools we employ to
restrict the architecture and training process in a manner that drastically reduces this cost, bringing it
in line with the cost of classifying a single example. The full procedure, with all the parts described
below, can be found in Algorithm 1.
Linear least-squares classification The fundamental simplifying assumption we make in this
work is to restrict the “training” process done by the classifier φ to be done via a linear least-
squares solve. Given the training set {xi, yi}in=1, we assume that there exists some feature mapping
h : Rd → Rk (where k < n), which typically would consist of a deep network pre-trained on a
similar task, or possibly trained in an unsupervised fashion on x1:n (i.e. independent of the training
labels, which are presumed to be potentially poisoned). This may seem to be a strong assumption,
but similar assumptions are commonly made when using pre-trained models or in the meta-learning
setting, and the transferability of pre-trained features is well documented (Donahue et al., 2014; Bo
et al., 2010; Yosinski et al., 2014). Given this feature mapping, let X = h(x1:n) ∈ Rn×k be the
training point features and let y = y1:n ∈ {0, 1}n be the labels. Our training process consists of
finding the least-squares fit to the training data, i.e., we find parameters ∕β ∈ Rk via the normal
equation β = (XTX) XTy and then we make a prediction on the new example via the linear
function h(xn+1)β. Although it may seem odd to fit a classification task with least-squares loss,
binary classification with linear regression is known to be equivalent to Fisher’s linear discriminant
(Mika, 2003) and often works quite well in practice.
The real advantage of the least-squares approach in this setting is that it reduces the prediction to a
linear function of y, and thus randomizing over the labels is straightforward. Specifically, letting
α = X (XTX)-1 h(xn+ι)T,	(11)
the prediction h(xn+ι)β can be equivalently given by αTy (this is effectively just the kernel rep-
resentation of the linear classifier). Thus, we can simply compute α one time and then randomly
sample many different sets of labels in order to build a standard randomized smoothing bound.
Further, we can pre-compute just the X XTX -1 term and reuse it for each test point.
`2 regularization for better conditioning Unfortunately, it is unlikely to be the case that the
training points are well-behaved for linear regression in the feature space. To address this, we
instead solve an `2 regularized version of least-squares. This is a common tool for solving systems
with ill-conditioned or random design matrices (Hsu et al., 2014; Suggala et al., 2018). Luckily,
there still exists a pre-computable closed-form solution to this problem, whereby we solve
α = X(XTX + λI)-1h(xn+1)T.	(12)
The other parts of our algorithm remain unchanged. Following results in Suggala et al. (2018), we
set the regularization parameter λ = (1 + q) σ2kκ(XTX) where σ2 = ky-X—OLS 卜 is an estimate
6
Under review as a conference paper at ICLR 2020
of the variance (Dicker, 2014) and κ(∙) is the condition number equal to the ratio of the largest and
smallest singular values. The (1 + q) term is to help account for the variance caused by label flips.
Efficient tail bounds via the Chernoff inequality Even more compelling, due to the linear struc-
ture of this prediction, we can forego a sampling-based approach entirely and directly bound the tail
probabilities using Chernoff bounds. Because the underlying binary prediction function φ will out-
put the label 1 for the test point whenever αTy ≥ 1/2 and 0 otherwise, we can derive an analytical
upper bound on the probability that g predicts one label or the other via the Chernoff bound. By
upper bounding the probability of the opposite prediction, we simultaneously derive a lower bound
on p which can be plugged in to (10) to determine the classifier’s robustness. Concretely, we can
upper bound the probability that the classifier outputs the label 0 by
P(αTy ≤ 1/2) ≤
min et/2 Yn E[e-tαiyi]
min et/2 Yn qe-tαi(1-yi) + (1 - q)e-tαiyi .
i=1	(13)
Conversely, the probability that the classifier outputs the label 1 has the analogous upper bound
which is the same as (13) but evaluated at -t. Thus, we can solve the minimization problem uncon-
strained over t, and then let the sign of t dictate which label to predict and the value of t determine
the bound. The objective (13) is log-convex in t and can be easily solved by Newton’s method. Note
that in some cases, neither Chernoff upper bound will be less than 2, meaning We cannot determine
the true value of g(μ, φ). In these cases, we simply define the classifier,s prediction to be determined
by the sign oft. While we can’t guarantee that this classification will match the true majority vote,
our algorithm will certify a robustness to 0 flips, so the guarantee is still valid. We avoid abstaining
so as to assess our classifier’s non-robust accuracy.
The key property we emphasize is that, unlike previous randomized smoothing applications, the
final algorithm involves no sampling whatsoever. Instead, the prediction probabilities are bounded
directly via the Chernoff bound, without any need for Monte Carlo approximation. Thus, the method
is able to generate truly certifiable robust predictions using approximately the same complexity as
traditional predictions with a deep network.
5	Experiments
Following Koh & Liang (2017) and Steinhardt et al. (2017), we perform experiments on MNIST
1/7, the IMDB review sentiment dataset (Maas et al., 2011), and the Dogfish binary classification
challenge taken from ImageNet. We run additional experiments on the full MNIST dataset; to the
best of our knowledge, this is the first multi-class classification algorithm with certified robustness
to label-flipping attacks. For each dataset and each noise level q we report the certified test set
accuracy at r training label flips. That is, for each possible number of flips r, we plot the fraction of
the test set that was both correctly classified and certified to not change under at least r flips.
Because the above are binary classification tasks, one could technically achieve a certified accuracy
of 50% at r = ∞ (or 10% for MNIST) by letting g be constant. A constant classifier would be
infinitely robust, but it is not a very meaningful baseline. However, we include the accuracy of such
a classifier in our plots (black dotted line) as a reference.
To properly justify the need for such certified defenses, and to get a sense of the scale of our certifi-
cations, we also generated label-flipping attacks against the undefended binary MNIST and Dogfish
models. Following previous work, the undefended models were implemented as convolutional neu-
ral networks, trained on the clean data, with all but the top layer frozen—this is equivalent to logistic
regression on the learned features. For each test point we recorded how many flips were required
to change the network’s prediction. This number serves as an upper bound for the robustness of the
network on that test point, but we note that our attacks were quite rudimentary and could almost
certainly be improved upon to tighten this upper bound. Appendix C contains the details of our
attack implementations.
Results on MNIST The MNIST 1/7 dataset (LeCun et al., 1998) consists of just the classes 1 and
7, totalling 13007 training points and 2163 test points. We trained a simple convolutional neural
network on the other eight MNIST digits to learn a 50-dimensional feature embedding and then
7
Under review as a conference paper at ICLR 2020
Percentage OfTraining Set
0.8
S'
2
ð 0.6
o
<
0
1.0 ls
1 0.4
O
0.2
----q = Q.3,Λ ≈ 12291
<? = 0.4M« 13237
—<? = 0.45,A» 13710
——g = 0.475" a 13946
----Undefianded
.... g ∞nstant
0.0 I	.	.	.
0	250	500 750 1000 1250 1500 1750 2000
Label Flips
(a) Binary MNIST (classes 1 and 7)
Label Flips
(b) Full MNIST
Figure 1: MNIST 1/7 (n = 13007, left) and full MNIST (n = 60000, right) test set certified accuracy
to adversarial training label flips as q is varied. The hyperparameter q controls a robust/non-robust
accuracy trade-off. The solid lines represent certified accuracy, except for the undefended classifier
which represents our attack. The dashed lines of the same color are the overall non-robust accuracy
of each classifier. The black dotted line is the performance of a constant classifier, assuming equal
representation of the classes.
Percentage OfTraining Set
Label Flips
Figure 2: Dogfish (n = 1800) test set certified accuracy to adversarial label flips as q is varied. The
classifiers’ certified accuracy curves cross each other further along the x-axis.
calculated Chernoff bounds for G(μ, φ) as described in Section 4.1. Figure 1a displays the certified
accuracy on the test set for varying probabilities q. As in prior work on randomized smoothing, the
noise parameter q balances a trade-off; as q increases, the required margin ∣G(μ, φ) - 21 to certify a
given number of flips decreases. On the other hand, this results in more noisy training labels, which
reduces the margin and therefore results in lower robustness and often lower accuracy. Figure 1b
depicts the certified accuracy for the full MNIST test set. See Appendix B for derivations of the
bounds and optimization algorithm in the multi-class case. In addition to this being a significantly
more difficult classification task, our pre-trained classifier could not rely on features learned from
other handwritten digits; instead, we used the features from a network trained on the Omniglot
dataset (Lake et al., 2015), as the task of classifying handwritten characters is quite similar. Despite
the lack of fine-tuned features, our algorithm still achieves significant certified accuracy under a large
number of adversarial label flips. We observed that regularization did not make a large difference for
the multi-class case, possibly due to the inaccuracy of the residual term when calculating the noise
estimate.
See Figure 4 in Appendix D for the effect of`2 regularization on the OLS solution. We note that at a
slight cost to non-robust accuracy, the regularization results in substantially higher certified accuracy
at almost all radii. A similar effect was observed for all our datasets and at all values of q.
8
Under review as a conference paper at ICLR 2020
Figure 3: IMDB Review Sentiment (n = 25000) test set certified accuracy. The non-robust accuracy
slightly decreases as q increases; for q = 0.01 the non-robust accuracy is 79.108%, while for q =
0.25 it is 78.732%.
Results on Dogfish The Dogfish dataset contains images from the ImageNet dog and fish synsets,
900 training points and 300 test points from each. We trained a ResNet-50 (He et al., 2016) on the
standard ImageNet training set but removed all images labeled as any kind of dog or fish. Our pre-
trained feature embedder was therefore capable of extracting image features but had no concept of
features specific to either class. We used PCA to reduce the 2048 dimensional feature space to 1700
dimensions before solving, to ensure the system was not underdetermined. Figure 2 displays the
results of our poisoning attack along with our certified defense. Under the undefended model, more
than 50% of the test points can be successfully attacked with no more than 7 label flips, whereas our
model with q = 0.001 can certifiably correctly classify 67.83% of the test points under the same
attack. It would take more than three times as many flips for each test point individually to push
our model to less than 50% certified accuracy, at which point the undefended classifier would be
reduced to at most 0.83%.
Because the predictions on the test points can be changed by flipping such a small fraction of the
training set, high values of q reduce the randomized models’ predictions to almost pure chance—
this means we are unable to achieve the margins necessary to certify a large number of flips. We
therefore found that smaller levels of noise were necessary to achieve high certified test accuracy.
This suggests that the more susceptible the original, non-robust classifier is to label flips, the lower
q should be set for the corresponding randomized classifier.
Results on IMDB Figure 3 plots the result of our randomized smoothing procedure on the IMDB
review sentiment dataset. This dataset contains 25,000 training examples and 25,000 test examples,
evenly split between “positive” and “negative”. To extract the features we applied the Google News
pre-trained Word2Vec to all the words in each review and averaged them. This feature embedding is
considerably noisier than that of an image dataset, as most of the words in a review are irrelevant to
sentiment classification. Indeed, Steinhardt et al. (2017) also found that the IMDB dataset was much
more susceptible to adversarial corruption than images when using bag-of-words features. Consis-
tent with this observation, we found smaller levels of noise to result in larger certified accuracy. We
expect significant improvements could be made with a more refined choice of feature embedding.
We also observed that increasing q does not significantly decrease the non-robust accuracy. We
believe this is because the limitation of these classifiers is not the label noise, but rather the difficulty
in classifying the imperfect feature embeddings with linear regression. Thus, the sign of t? was
frequently correct and remained on the same side of 0 as q increased, but the margin of G(μ, φ)
decreased too rapidly to certify a large number of flips. This was similarly observed in the case of
full MNIST, where the pre-trained features came from a network trained on the Omniglot dataset
and therefore were not as informative for classification.
9
Under review as a conference paper at ICLR 2020
6	Conclusion
In this work we have presented a certified defense against a strong class of adversarial label-flipping
attacks where an adversary can flip labels to cause a misclassification on each test point separately.
This contrasts with previous data poisoning settings, which have typically only considered an adver-
sary who wishes to degrade the classifier’s accuracy on the test distribution as a whole, and it brings
the adversary’s objective more in line with that of backdoor attacks and test-time adversarial pertur-
bations. Leveraging randomized smoothing, a method originally developed for certifying robustness
to test-time perturbations, we presented a classifier that can be certified robust to these pointwise
train-time attacks. We then offered a tractable algorithm for evaluating this classifier which, despite
being rooted in randomization, can be computed with no Monte Carlo sampling whatsoever, result-
ing in a truly certifiably robust classifier. This algorithm results in the first multi-class classification
algorithm that is certifiably robust to label-flipping attacks.
There are several avenues for improvement to this line of work, perhaps the most immediate be-
ing the method for learning the input features. For example, rather than considering fixed features
generated by a pre-trained deep network, extensions could leverage neural tangent kernels (Jacot
et al., 2018) to also allow for efficient learning with perturbed inputs or more flexible representa-
tions. Using this or other approaches, the analysis could be extended to other types of smoothing
distributions applied to the training data, such as randomizing over the input features to provide
robustness to more general data poisoning attacks. Conversely, we also hope that our defense to this
threat model will inspire the development of more powerful train-time attacks, against which future
defenses can be evaluated.
References
Anonymous. A framework for robustness certification of smoothed classifiers using f-divergences.
Submitted to ICLR 2020, 2019.
Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of se-
curity: Circumventing defenses to adversarial examples. In Proceedings of the 35th International
Conference on Machine Learning, ICML 2018, July 2018.
Marco Barreno, Blaine Nelson, Russell Sears, Anthony D. Joseph, and J. D. Tygar. Can machine
learning be secure? In Proceedings of the 2006 ACM Symposium on Information, Computer and
Communications Security, ASIACCS ,06, pp.16-25, New York, NY, USA, 2006. ACM. ISBN
1-59593-272-0. doi: 10.1145/1128817.1128824.
Kush Bhatia, Prateek Jain, and Purushottam Kar. Robust regression via hard thresholding. In
Proceedings of the 28th International Conference on Neural Information Processing Systems -
Volume 1, NIPS’15, pp. 721-729, Cambridge, MA, USA, 2015. MIT Press.
B. Biggio, G. Fumera, and F. Roli. Security evaluation of pattern classifiers under attack. IEEE
Transactions on Knowledge and Data Engineering, 26(4):984-996, April 2014. doi: 10.1109/
TKDE.2013.57.
Battista Biggio, Blaine Nelson, and Pavel Laskov. Support vector machines under adversarial la-
bel noise. In Chun-Nan Hsu and Wee Sun Lee (eds.), Proceedings of the Asian Conference on
Machine Learning, volume 20 of Proceedings of Machine Learning Research, pp. 97-112, South
Garden Hotels and Resorts, Taoyuan, Taiwain, 14-15 Nov 2011. PMLR.
Liefeng Bo, Xiaofeng Ren, and Dieter Fox. Kernel descriptors for visual recognition. In J. D.
Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. S. Zemel, and A. Culotta (eds.), Advances in
Neural Information Processing Systems 23, pp. 244-252. Curran Associates, Inc., 2010.
Nicholas Carlini and David Wagner. Adversarial examples are not easily detected: Bypassing ten
detection methods. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and
Security, AISec ’17, pp. 3-14, New York, NY, USA, 2017. ACM. ISBN 978-1-4503-5202-4. doi:
10.1145/3128572.3140444.
10
Under review as a conference paper at ICLR 2020
Ruidi Chen and Ioannis Ch. Paschalidis. A robust learning approach for regression models based on
distributionally robust optimization. J. Mach. Learn. Res., 19(1):517-564, January 2018. ISSN
1532-4435.
Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on deep
learning systems using data poisoning. arXiv preprint arXiv:1712.05526, 2017.
Yudong Chen, Constantine Caramanis, and Shie Mannor. Robust sparse regression under adver-
sarial corruption. In Sanjoy Dasgupta and David McAllester (eds.), Proceedings of the 30th
International Conference on Machine Learning, volume 28 of Proceedings of Machine Learning
Research, pp. 774-782, Atlanta, Georgia, USA, 17-19 Jun 2013. PMLR.
Jeremy Cohen, Elan Rosenfeld, and J. Zico Kolter. Certified adversarial robustness via randomized
smoothing. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th
International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning
Research, pp. 1310-1320, Long Beach, California, USA, 09-15 Jun 2019. PMLR.
Lee H. Dicker. Variance estimation in high-dimensional linear models. Biometrika, 101(2):269-284,
03 2014. ISSN 0006-3444. doi: 10.1093/biomet/ast065.
Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Dar-
rell. Decaf: A deep convolutional activation feature for generic visual recognition. In Eric P. Xing
and Tony Jebara (eds.), Proceedings of the 31st International Conference on Machine Learning,
volume 32 of Proceedings of Machine Learning Research, pp. 647-655, Bejing, China, 22-24
Jun 2014. PMLR.
Reuben Feinman, Ryan R. Curtin, Saurabh Shintre, and Andrew B. Gardner. Detecting adversarial
samples from artifacts. arXiv preprint arXiv:1703.00410, 2017.
Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego,
CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Daniel Hsu, Sham M. Kakade, and Tong Zhang. Random design analysis of ridge regression. Found.
Comput. Math., 14(3):569-600, June 2014. ISSN 1615-3375. doi: 10.1007/s10208-014-9192-1.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-
Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31, pp. 8571-
8580. Curran Associates, Inc., 2018.
Sushrut Karmalkar and Eric Price. Compressed sensing with adversarial sparse noise via l1 regres-
sion. arXiv preprint arXiv:1809.08055, 2018.
Adam Klivans, Pravesh K. Kothari, and Raghu Meka. Efficient algorithms for outlier-robust regres-
sion. arXiv preprint arXiv:1803.03241, 2018.
Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In
Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML’17,
pp. 1885-1894. JMLR.org, 2017.
Pang Wei Koh, Jacob Steinhardt, and Percy Liang. Stronger data poisoning attacks break data
sanitization defenses. arXiv preprint arXiv:1811.00741, 2018.
Brenden M. Lake, Ruslan Salakhutdinov, and Joshua B. Tenenbaum. Human-level concept learning
through probabilistic program induction. Science, 350(6266):1332-1338, 2015. ISSN 0036-8075.
doi: 10.1126/science.aab3050.
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 86(11):2278-2324, Nov 1998. ISSN 0018-9219. doi:
10.1109/5.726791.
11
Under review as a conference paper at ICLR 2020
Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. Certified
robustness to adversarial examples with differential privacy. arXiv preprint arXiv:1802.03471,
2018.
Guang-He Lee, Yang Yuan, Shiyu Chang, and Tommi S. Jaakkola. A stratified approach to robust-
ness for randomly smoothed classifiers. arXiv preprint arXiv:1906.04948, 2019.
Bai Li, Changyou Chen, Wenlin Wang, and Lawrence Carin. Certified adversarial robustness with
additive gaussian noise. arXiv preprint arXiv:1809.03113, 2018.
Chang Liu, Bo Li, Yevgeniy Vorobeychik, and Alina Oprea. Robust linear regression against training
data poisoning. In AISec 2017 - Proceedings of the 10th ACM Workshop on Artificial Intelligence
and Security, co-located with CCS 2017, AISec 2017 - Proceedings of the 10th ACM Workshop
on Artificial Intelligence and Security, co-located with CCS 2017, pp. 91-102. Association for
Computing Machinery, Inc, 11 2017. doi: 10.1145/3128572.3140447.
T. Liu and D. Tao. Classification with noisy labels by importance reweighting. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 38(3):447-461, March 2016. ISSN 0162-8828.
doi: 10.1109/TPAMI.2015.2456899.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher
Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Human Language Technologies - Volume 1,
HLT ’11, pp. 142-150, Stroudsburg, PA, USA, 2011. Association for Computational Linguistics.
ISBN 978-1-932432-87-9.
Jan Hendrik Metzen, Tim Genewein, Volker Fischer, and Bastian Bischoff. On detecting adversarial
perturbations. In 5th International Conference on Learning Representations, ICLR 2017, Toulon,
France, April 24-26, 2017, Conference Track Proceedings, 2017.
Sebastian Mika. Kernel fisher discriminants, 2003.
LUis Munoz Gonzalez, Battista Biggio, Ambra Demontis, Andrea Paudice, Vasin Wongrassamee,
Emil C. Lupu, and Fabio Roli. Towards poisoning of deep learning algorithms with back-gradient
optimization. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security,
AISec ’17, pp. 27-38, New York, NY, USA, 2017. ACM. ISBN 978-1-4503-5202-4. doi: 10.
1145/3128572.3140451.
Nagarajan Natarajan, Inderjit S Dhillon, Pradeep K Ravikumar, and Ambuj Tewari. Learning with
noisy labels. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger
(eds.), Advances in Neural Information Processing Systems 26, pp. 1196-1204. Curran Asso-
ciates, Inc., 2013.
N. Papernot, P. McDaniel, A. Sinha, and M. P. Wellman. Sok: Security and privacy in machine
learning. In 2018 IEEE European Symposium on Security and Privacy (EuroS P), pp. 399-414,
April 2018. doi: 10.1109/EuroSP.2018.00035.
Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and Lizhen Qu. Making
deep neural networks robust to label noise: A loss correction approach. pp. 2233-2241, 07 2017.
doi: 10.1109/CVPR.2017.240.
Andrea Paudice, Luis Munoz-Gonzalez, and Emil C. Lupu. Label sanitization against label flipping
poisoning attacks. In Carlos Alzate, Anna Monreale, Haytham Assem, Albert Bifet, Teodora San-
dra Buda, Bora Caglayan, Brett Drury, Eva Garcla-Martin, Ricard Gavalda, Irena Koprinska, Ste-
fan Kramer, Niklas Lavesson, Michael Madden, Ian Molloy, Maria-Irina Nicolae, and Mathieu
Sinn (eds.), ECML PKDD 2018 Workshops, pp. 5-15, Cham, 2019. Springer International Pub-
lishing.
Adarsh Prasad, Arun Sai Suggala, Sivaraman Balakrishnan, and Pradeep Ravikumar. Robust esti-
mation via robust gradient estimation. arXiv preprint arXiv:1802.06485, 2018.
P.J. Rousseeuw and A.M. Leroy. Robust Regression and Outlier Detection. Wiley Series in Proba-
bility and Statistics. Wiley, 1987. ISBN 9780471852339.
12
Under review as a conference paper at ICLR 2020
Benjamin I.P. Rubinstein, Blaine Nelson, Ling Huang, Anthony D. Joseph, Shing-hon Lau, Satish
Rao, Nina Taft, and J. D. Tygar. Antidote: Understanding and defending against poisoning of
anomaly detectors. In Proceedings of the 9th ACM SIGCOMM Conference on Internet Measure-
ment, IMC ,09, pp.1-14, New York, NY, USA, 2009. ACM. ISBN 978-1-60558-771-4. doi:
10.1145/1644893.1644895.
Hadi Salman, Greg Yang, Jerry Li, Pengchuan Zhang, Huan Zhang, Ilya Razenshteyn, and Sebastien
Bubeck. Provably robust deep learning via adversarially trained smoothed classifiers. arXiv
preprint arXiv:1906.04584, 2019.
Ali Shafahi, W. Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras,
and Tom Goldstein. Poison frogs! targeted clean-label poisoning attacks on neural networks.
In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.),
Advances in Neural Information Processing Systems 31, pp. 6103-6113. Curran Associates, Inc.,
2018.
Yanyao Shen and Sujay Sanghavi. Learning with bad training data via iterative trimmed loss min-
imization. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th
International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning
Research, pp. 5739-5748, Long Beach, California, USA, 09-15 Jun 2019. PMLR.
Jacob Steinhardt, Pang Wei Koh, and Percy Liang. Certified defenses for data poisoning attacks.
In Proceedings of the 31st International Conference on Neural Information Processing Systems,
NIPS’17, pp. 3520-3532, USA, 2017. Curran Associates Inc. ISBN 978-1-5108-6096-4.
Arun Suggala, Adarsh Prasad, and Pradeep K Ravikumar. Connecting optimization and regulariza-
tion paths. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett
(eds.), Advances in Neural Information Processing Systems 31, pp. 10608-10619. Curran Asso-
ciates, Inc., 2018.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking
the inception architecture for computer vision. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2016.
Rahim Taheri, Reza Javidan, Mohammad Shojafar, Zahra Pooranian, Ali Miri, and Mauro Conti.
On defending against label flipping attacks on malware detection systems. arXiv preprint
arXiv:1908.04473, 2019.
Brandon Tran, Jerry Li, and Aleksander Madry. Spectral signatures in backdoor attacks. In S. Ben-
gio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in
Neural Information Processing Systems 31, pp. 8000-8010. Curran Associates, Inc., 2018.
Han Xiao, Huang Xiao, and Claudia Eckert. Adversarial label flips attack on support vector ma-
chines. In Proceedings of the 20th European Conference on Artificial Intelligence, ECAI’12, pp.
870-875, Amsterdam, The Netherlands, The Netherlands, 2012. IOS Press. ISBN 978-1-61499-
097-0. doi: 10.3233/978-1-61499-098-7-870.
Huang Xiao, Battista Biggio, Gavin Brown, Giorgio Fumera, Claudia Eckert, and Fabio Roli. Is
feature selection secure against training data poisoning? In Francis Bach and David Blei (eds.),
Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceed-
ings of Machine Learning Research, pp. 1689-1698, Lille, France, 07-09 Jul 2015. PMLR.
Chaofei Yang, Qing Wu, Hai Li, and Yiran Chen. Generative poisoning attack method against neural
networks. arXiv preprint arXiv:1703.01340, 2017.
Chih-Kuan Yeh, Joon Kim, Ian En-Hsu Yen, and Pradeep K Ravikumar. Representer point selection
for explaining deep neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31,
pp. 9291-9301. Curran Associates, Inc., 2018.
13
Under review as a conference paper at ICLR 2020
Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep
neural networks? In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Wein-
berger (eds.), Advances in Neural Information Processing Systems 27, pp. 3320-3328. Curran
Associates, Inc., 2014.
Chen Zhu, W. Ronny Huang, Hengduo Li, Gavin Taylor, Christoph Studer, and Tom Goldstein.
Transferable clean-label poisoning attacks on deep neural nets. In Kamalika Chaudhuri and Rus-
lan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning,
volume 97 of Proceedings of Machine Learning Research, pp. 7614-7623, Long Beach, Califor-
nia, USA, 09-15 Jun 2019. PMLR.
A Generic Randomized Smoothing Algorithm
Algorithm 2 Generic randomized smoothing procedure
Input: function φ : X → {0,1}, number of samples N, smoothing distribution μ, failure proba-
bility δ > 0
for i = 1, . . . , N do
Sample Xi 〜 μ and compute yi = φ(xi)
end for
Compute approximate smoothed output
g(μ, φ)
1 {N X yi ≥ 1/2}
(14)
Compute bound G(μ, φ) such that with probability 1 - δ
G(μ,φ){ ≥ G(μ,φ)
if g(μ, φ) = 1
if g(μ, φ) = 0
(15)
/、 ， . 1 ʌ 1∙	ʌ / I ∖ 1 1 1 ∙ 1 ∙ . 1	1 A / I ∖	1	. ∙ ∙ ∕' ʌ / I ∖ 1 A / I ∖
Output: Prediction g(μ, φ) and probability bound G(μ, φ), or abstention if ^(μ, φ) and G(μ, φ)
are on different sides of 2.
B The multi-class setting
Although the notation and algorithms are slightly more complex, all the methods we have discussed
in the main paper can be extended to the multi-class setting. In this case, we consider a class label
y ∈ {1, . . . , K}, and we again seek some smoothed prediction such that the classifier’s prediction
on a new point will not change with some number r flips of the labels in the training set.
B.1	Randomized smoothing in the multi-class case
We here extend our notation to the case of more than two classes. Recall our original definition of
G,
G(μ, φ) = Ex〜μ[φ(x)] = / μ(x)φ(x)dx,
X
where φ : X → {0, 1}. More generally, consider a classifier φ : X → [K], outputting the index of
one of K classes. Under this formulation, for a given class c ∈ [K], we have
G(μ, φ,c)= Ex”
[φc(x)] = X
μ(x)φc(x)dx,
(16)
where φc(x) = 1 {φ(x) = c} is the indicator function for if φ(x) outputs the class c. In this case,
the hard threshold g is evaluated by returning the class with the highest probability. That is,
g(μ, φ) = arg max G(μ, φ, c).
c
(17)
14
Under review as a conference paper at ICLR 2020
B.2	Linearization and Chernoff bound approach for the multiclass case
Using the same linearization approach as in the binary case, we can formulate an analogous approach
which forgoes the need to actually perform random sampling at all and instead directly bounds the
randomized classifier using the Chernoff bound.
Adopting the same notation as in the main text, the equivalent least-squares classifier for the multi-
class setting finds some set of weights
β= (X T X )-1 X T Y	(18)
where Y ∈ {0, 1}n×K is a binary matrix with each row equal to a one-hot encoding of the class
label (note that the resulting β ∈ Rk × K is now a matrix, and We let βi refer to the ith column). At
prediction time, the predicted class of some new point xn+1 is simply given by the prediction with
the highest value, i.e.,
yn+1 = arg max βiTh(xn+1).
i
(19)
Alternatively, following the same logic as in the binary case, this same prediction can be written in
terms of the α variable as
yn+1 = arg max aT匕	(20)
i
where Yi denotes the ith column of Yi.
In our randomized smoothing setting, we again propose to flip the class of any label with probability
q, selecting an alternative label uniformly at random from the remaining K - 1 labels. Assuming
that the predicted class label is i, we wish to bound the probability that
P(αTYi < αTYi0)	(21)
for all alternative classes i0 . By the Chernoff bound, we have that
logP(αTYi < αTYi0) =logP(αT(Yi -Yi0) ≤ 0)
≤ minIXX log E he-tαj (Yji-YjiO)i).	(22)
The random variable Yji - Yji0 takes on three different distributions depending on if yj = i, if
yj = i0, or if yj 6= i and yj 6= i0. Specifically, this variable can take on the terms +1, 0, -1 with the
associated probabilities
P(Yji -	Yji0	=	+1) =	q1/-(Kq	-	1)	iofthyejrw=isi,e.
P(Yji-Yji0=-1)=	q1/-(Kq	-	1)	iofthyejrw=isi0e,.	(23)
P(Y Y = 0) =	q(K - 2)/(K - 1) ifyj = i oryj = i0,
P (Yji - Yji0 = 0) =	1 - 2q/(K - 1)	otherwise.
log (K⅛e-tɑj + 1 - 2K-1 + Kh etaj
Combining these cases directly into the Chernoff bound gives
logP(αTYi < αT匕o) ≤ minʃ X log ((1 - q)e-tαj + qK_2 + Nq[et。1 +
t>0	K - 1 K - 1
j :yj =i
X log (Kq-1 e-tαj+ qK-2 + (1 - q)etαj)+
j:yj =i0
X
j:yj 6=i,yj 6=i
(24)
Again, this problem is convex in t, and so can be solved efficiently using Newton’s method. And
again since the reverse case can be computed via the same expression we can similarly optimize this
in an unconstrained fashion. Specificially, we can do this for every pair of classes i and i0, and return
the i which gives the smallest lower bound for the worst-case choice of i0.
15
Under review as a conference paper at ICLR 2020
B.3 KL Divergence Bound
To compute actual certification radii, we will derive the KL divergence bound for the the case of
K classes. Let μ, P be defined as in Section 4, except that as in the previous Secftion when a label
is flipped with probability q it is changed to one of the other K - 1 classes uniformly at random.
Let μi and Pi refer to the independent measures on each dimension which collectively make UP the
factorized distributions μ and P (i.e., μ(x) = Qd= 1 μi(x)). Further, let 埒 be the ith element of Y1,
meaning it is the “original” class which may or may not be flipped when sampling from μ. First
noting that each dimension of the distributions μ and P are independent, We have
n
KL(P Il μ) = X KL(Pi Il μi)
i=1
KL KL(Pi k μi)
i:ρ i= μi
r 住Pi” (卷))
r "%i)log (第
+ Pi (Y2i) log
(Pi(Yi) 口
Vi(Y2i)77
Plugging in the robustness guarantee (5), We have that g(μ, φ) = g(P, φ) so long as
________log(4p(1 — P))
2(I - KK⅛)log ( (1-q)qK-1)
(25)
Setting K = 2 recovers the divergence term (9) and the bound (10).
C Description of Label-Flipping Attacks on MNIST 1/7 and
Dogfish
Due to the dearth of existing work on label-flipping attacks for deep networks, our attacks on MNIST
and Dogfish were quite straightforward; we expect significant improvements could be made to
tighten this upper bound.
For Dogfish, we used a pretrained Inception network (Szegedy et al., 2016) to evaluate the influence
of each training point with respect to the loss of each test point (Koh & Liang, 2017). As in prior
work, we froze all but the top layer of the network for retraining. Once we obtained the most
influential points, we flipped the first one and recomputed approximate influence using only the
top layer for efficiency. After each flip, we recorded which points were classified differently and
maintained for each test point the successful attack which required the fewest flips. When this was
finished, we also tried the reverse of each attack to see if any of them could be achieved with even
fewer flips.
For MNIST we implemented two similar attacks and kept the best attack for each test point. The
first attack simply ordered training labels by their `2 distance from the test point in feature space, as
a proxy for influence. We then tried flipping these one at a time until the prediction changed, and we
also tried the reverse. The second attack was essentially the same as the Dogfish attack, ordering the
test points by influence. To calculate influence we again assumed a frozen feature map; specifically,
using the same notation as Koh & Liang (2017), the influence of flipping the label ofa training point
16
Under review as a conference paper at ICLR 2020
z = (x, y) to z- = (x, 1 - y) on the loss at the test point ztest is:
IT / λ	∖
dL(Ztest, θe,z-,-z)
d
JL(Ztest,
dθ
,z-,-z
d
≈ -VθL(Ztest,θ)TH-1 (VθL(z-,θ) -VθL(z,θ))
For logistic regression, these values can easily be computed in closed form.
D Additional Plots
Figure 4: MNIST 1/7 test set certified accuracy with and without `2 regularization in the computation
of α. Note that the unregularized solution achieves almost 100% non-robust accuracy, but certifies
significantly lower robustness. This implies that the “training” process is not robust enough to label
noise, hence the lower margin by the ensemble. In comparison, the regularized solution achieves
significantly higher margins, at a slight cost in overall accuracy.
Minimum Margin p to Certify Robustness at r Flips
Numberof Label Flips Certifiably Robust at p = 1 - IO-X
r
x
Figure 5: Left: Required margin p to certify a given number of label flips using the generic KL
bound (10) versus the tight discrete bound (4). Right: The same comparison, but inverted, showing
the certifiable robustness for a given margin. The tight bound certifies robustness to approximately
twice as many label flips.
17