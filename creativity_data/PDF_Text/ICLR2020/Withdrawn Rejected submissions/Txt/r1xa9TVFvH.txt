Under review as a conference paper at ICLR 2020
Neural Contextual Bandits with
Upper Confidence Bound-Based Exploration
Anonymous authors
Paper under double-blind review
Ab stract
We study the stochastic contextual bandit problem, where the reward is gener-
ated from an unknown bounded function with additive noise. We propose the
NeuralUCB algorithm, which leverages the representation power of deep neural
networks and uses a neural network-based random feature mapping to construct
an upper confidence bound (UCB) of reward for efficient exploration. We prove
that, under mild assumptions, NeUralUCB achieves O(√T) regret, where T is the
number of rounds. To the best of our knowledge, our algorithm is the first neu-
ral network-based contextual bandit algorithm with near-optimal regret guarantee.
Preliminary experiment results on synthetic data corroborate our theory, and shed
light on potential applications of our algorithm to real-world problems.
1 Introduction
The stochastic contextual bandit problem has been extensively studied in machine learning (Bubeck
and Cesa-Bianchi, 2012; Lattimore and Szepesvari, 2019): at round t ∈ {1,2,..., T}, an agent is
presented with a set of K actions, each of which is associated with a d-dimensional feature vector.
After choosing an action, the agent will receive a stochastic reward generated from some unknown
distribution conditioned on the chosen action’s feature vector. The goal of the agent is to maximize
the expected cumulative rewards over T rounds. Contextual bandit algorithms have been applied
to many real-world applications, such as personalized recommendation, advertising and Web search
(e.g., Agarwal et al., 2009; Li et al., 2010).
The most studied model in the literature is linear contextual bandits (Auer, 2002; Abe et al., 2003;
Dani et al., 2008; Rusmevichientong and Tsitsiklis, 2010; Chu et al., 2011; Abbasi-Yadkori et al.,
2011), which assumes that the expected reward at each round is a linear function of the feature
vector. Linear bandit algorithms have achieved great success in both theory and practice, such as
news article recommendation (Li et al., 2010). However, the linear-reward assumption often fails
to hold exactly in practice, which motivates the study of nonlinear contextual bandits (e.g., Filippi
et al., 2010; Srinivas et al., 2010; Bubeck et al., 2011; Valko et al., 2013). However, they still
require fairly strong assumptions on the reward function. For instance, Filippi et al. (2010) makes
a generalized linear model assumption on the reward, Bubeck et al. (2011) require it to have a
Lipschitz continuous property in a proper metric space, and Valko et al. (2013) assume the reward
function belongs to some Reproducing Kernel Hilbert Space (RKHS).
In order to overcome the above shortcomings, deep neural networks (DNNs) (Goodfellow et al.,
2016) have been introduced to learn the underlying reward function in contextual bandit problem,
thanks to their strong representation power. Given the fact that DNNs enable the agent to make use
of nonlinear models with less domain knowledge, existing work (Riquelme et al., 2018; Zahavy and
Mannor, 2019) focuses on the idea called neural-linear bandit. More precisely, they use the first
L - 1 layers of a DNN as a feature map, which transforms contexts from the raw input space to a
low-dimensional space, usually with better representation and less frequent update. Then they learn
a linear exploration policy on top of the last hidden layer of the DNN with a more frequent update.
These attempts have achieved great empirical success. However, none of these work provides a
theoretical guarantee on the regret of the algorithms.
In this paper, we take the first step towards provable efficient contextual bandit algorithms based on
deep neural networks. Specifically, we propose a new algorithm, NeuralUCB, which uses a deep
neural network to learn the underlying reward function. At the core of the algorithm is an upper
1
Under review as a conference paper at ICLR 2020
confidence bound constructed by deep neural network-based random feature mappings. Our regret
analysis of NeuralUCB is built on recent results on optimization and generalization of deep neural
networks (Jacot et al., 2018; Arora et al., 2019; Cao and Gu, 2019a). While the main focus of our
paper is mostly theoretical, we also carry out proof-of-concept experiments on synthetic data to
validate the effectiveness of our proposed algorithm.
Our contributions are summarized as follows:
I ...	♦一.	.. 六/7 B .	.
algorithm is able to achieve a O(d T ) regret, where
• We prove that, under mild assumptions, our
d is the effective dimension of a neural tangent kernel matrix and T is the number of rounds. Our
regret bound recovers the O(d√T) regret for linear contextual bandit as a special case (Abbasi-
Yadkori et al., 2011), where d is the dimension of context.
•	We propose a neural contextual bandit algorithm using neural network-based exploration. It can
be regarded as an extension of existing linear bandit algorithms (Li et al., 2010; Abbasi-Yadkori
et al., 2011), from linear reward functions to any bounded reward functions.
•	We provide empirical evidence in several proof-of-concept experiments to demonstrate potential
applications of our algorithm to real-world problems.
Notation: Scalars are denoted by lower case letters, vectors by lower case bold face letters, and
matrices by upper case bold face letters. For a positive integer k, [k] denotes {1, . . . , k}. For a
vector θ ∈ Rd, we denote its `2 norm by kθk2 = Pid=1 θi2 and its j-th coordinate by [θ]j. For a
matrix A ∈ Rd×d, we denote its spectral norm, Frobenius norm, and (i, j)-th entry by kAk2, kAkF,
and [A]i,j, respectively. We denote a sequence of vectors by {θj}tj=1, and similarly for matrices.
For two sequences {an} and {bn}, we use an = O(bn) to denote that there exists some constant
C > 0 such that an ≤ Cbn, an = Ω(bn) to denote that there exists some constant C0 > 0 such that
an ≥ C0bn. In addition, We use O(∙) to hide logarithmic factors. We say a random variable X is
V-SUb-GaUSSian if Eexp(λ(X - EX)) ≤ exp(λ2ν2∕2) for any λ > 0.
2	Related Work
2.1	Contextual Bandits
There is a line of extensive Work on linear bandits (e.g., Auer, 2002; Abe et al., 2003; Dani et al.,
2008; Rusmevichientong and Tsitsiklis, 2010; Li et al., 2010; Chu et al., 2011; Abbasi-Yadkori et al.,
2011). For the setting With finitely many arms, Abe et al. (2003) formalized the linear bandit setting
and analyzed some of the earliest algorithms. Auer (2002) proposed SupLinRel algorithm that
achieves O√dT regret. Chu et al. (2011) obtained the same regret with SUPLinUCB that is based on
LinUCB (Li et al., 2010); the authors also provided a lower bound of Ω(√dT). For the other setting
with infinitely many arms, a few authors (Dani et al., 2008; Rusmevichientong and Tsitsiklis, 2010;
Abbasi-Yadkori et al.,2011) proposed algorithms that achieve O(d√T) regret, whereas an Ω(d√T)
lower bound is given by Dani et al. (2008).
While most algorithms above are based on the idea of upper confidence bounding, it is also possible
to use proper randomization to achieve strong regret guarantees, such as Thompson sampling and
reward perturbation (Thompson, 1933; Chapelle and Li, 2011; Agrawal and Goyal, 2013; Russo and
Van Roy, 2014; 2016; Kveton et al., 2019).
To deal with nonlinearity, generalized linear bandit has been considered, which assumes that the
reward function can be written as a composition of a linear function and a link function. In particular,
Filippi et al. (2010) proposed a GLM-UCB algorithm, which attains O(d√T) regret. Li et al. (2017)
proposed SupCB-GLM for generalized contextual bandit problems and showed a O(√dT) regret
that matches the lower bound. Jun et al. (2017) studied how to scale up algorithms for GLM bandits.
A few authors have also explored more general nonlinear bandits without making strong modeling
assumptions. One line of work is variants of expert learning algorithms (Auer et al., 2002), which
typically have a time complexity linear in the number of experts (which in many cases can be expo-
nential in the number of parameters). Another approach is to reduce a bandit problem to supervised
2
Under review as a conference paper at ICLR 2020
learning, such as the epoch-greedy algorithm (Langford and Zhang, 2008) that has an O(T 2/3 ) re-
gret. Later, Agarwal et al. (2014) develop an algorithm that yields a near-optimal regret bound, but
relies on an optimization oracle that can be expensive. A third approach uses nonparametric mod-
eling, such as Gaussian processes and kernels (Srinivas et al., 2010; Krause and Ong, 2011; Valko
et al., 2013). More specifically, Srinivas et al. (2010) assumed that the reward function is generated
from a Gaussian process with known mean and covariance functions. They proposed a GP-UCB
algorithm which achieves O(√Tγτ) regret, where YT is the maximum information gain. Krause
and Ong (2011) assumed the reward function is defined over the context-arm joint space and pro-
posed a Contextual GP-UCB. Valko et al. (2013) assumed that the reward function lies in a RKHS
with bounded RKHS norm. They proposed a SupKernelUCB algorithm and showed an O( dT)
regret, where d is effective dimension of the kernel that can be seen as a generalized notion of the
dimension of contexts. There is also work focusing on bandit problems in general metric space with
Lipschitz continuous property on the context (Kleinberg et al., 2008; Bubeck et al., 2011).
2.2	Neural Networks
Different lines of research have been done to provide theoretical understandings of DNNs from
different aspects. For example, to understand how the expressive power of DNNs are related to
their architecture, Telgarsky (2015; 2016); Liang and Srikant (2016); Yarotsky (2017; 2018); Hanin
(2017) showed that deep neural networks can express more function classes than shallow networks.
Lu et al. (2017); Hanin and Sellke (2017) suggested that the width of neural networks is crucial to
improve the expressive power of neural networks.
For the optimization of DNNs, a series of work have been proposed to show that (stochastic) gradient
descent can find the global minima of training loss (Li and Liang, 2018; Du et al., 2019b; Allen-
Zhu et al., 2019; Du et al., 2019a; Zou et al., 2019; Zou and Gu, 2019). For the generalization of
DNNs, a series of work (Daniely, 2017; Cao and Gu, 2019a;b; Arora et al., 2019) shows that by
using (stochastic) gradient descent, the parameters of a DNN are located in a particular regime and
the generalization bound of DNNs can be characterized by the best function in the corresponding
neural tangent kernel space (Jacot et al., 2018).
3	Problem Setting
We consider the stochastic K-armed contextual bandit problem, where the total number of rounds
T is known. At round t ∈ [T], the agent observes the tth context consisting of K feature vectors:
{xt,a ∈ Rd | a ∈ [K]}. The agent selects an action at and receive a reward rt,at. For simplicity, we
denote {xi}iT=K1 as the collection of {x1,1, x1,2, . . . , xT,K}. Our goal is to maximize the following
pseudo regret (or regret for short):
(3.1)
where aJ= is the optimal action at round t that maximizes the expected reward, i.e., aJ:
argmaxa∈[K] E[rt,a].
This work makes the following assumption on reward generation: for any round t,
rJ,at = h(xJ,at ) + ξJ ,
(3.2)
where h is some unknown function satisfying 0 ≤ h(x) ≤ 1 for any x, and ξJ is ν-sub-Gaussian
noise conditioned on x1,a1 , . . . , xJ-1,at-1. Note that the ν-sub-Gaussian noise assumption for ξJ is
a standard assumption in stochastic bandit literature (e.g., Abbasi-Yadkori et al., 2011); in particular,
any bounded noise satisfies such an assumption. Our reward function class contains linear functions,
generalized linear functions, Gaussian processes, and kernel functions with bounded RKHS norm
over a bounded domain.
In order to learn the reward function h in (3.2), we propose to use a fully connected deep neural
networks with depth L ≥ 2:
f (x; θ) = √mWLσ (WL-ισ(…σ(Wιx))),
(3.3)
3
Under review as a conference paper at ICLR 2020
where σ(x) = max{x, 0} is the rectified linear unit (ReLU) activation function, W1 ∈
Rm×d, Wi ∈ Rm×m, 2 ≤ i ≤ L - 1, WL ∈ Rm×1, and θ = [vec(W1)>, . . . , vec(WL)>]> ∈ Rp
with p = m+md+m2(L - 1). Without loss of generality, we assume that the width of each hidden
layer is the same (i.e., m) for convenience in analysis. We denote the gradient of the neural network
function by g(x; θ) = Vθ f(x; θ) ∈ Rp.
4	The NeuralUCB Algorithm
We present in Algorithm 1 our algorithm, NeuralUCB. The key idea is to use a deep neural network
f(x; θ) to predict the reward of context x, and upper confidence bound-based exploration (Auer,
2002). In particular, Algorithm 1 first initializes the network by randomly generating each entry
of θ from an appropriate Gaussian distribution. More specifically, for each 1 ≤ l ≤ L - 1, we
initialize Wl as W0 W0 , where each entry of W is generated independently from N(0, 4/m).
For WL, we initialize WL as (w>, -w>), where each entry of w is generated independently
from N(0, 2/m). At round t, Algorithm 1 observes the contexts for all actions, {xt,a}aK=1. First,
it computes the upper confidence bound, Ut,a, based on context xt,a, the current neural network
parameter θt-1, and a scaling factorγt-1. It then chooses action at with the largest Ut,a, and receives
the corresponding reward rt,at. At the end of round t, Algorithm 1 updates θt by applying Algorithm
2 to (approximately) minimize L(θ) using gradient descent, and updates γt. We choose gradient
descent in Algorithm 2 for the simplicity of analysis, although the training method can be replaced
by more efficient algorithms like stochastic gradient descent with a more involved analysis (Allen-
ZhU et al., 2019; Zou et al., 2019).
Algorithm 1 NeuralUCB
1:	Input: Number of rounds T, regularization parameter λ, exploration parameter V, confidence
parameter δ, norm parameter S, step size η, number of gradient descent steps J, network width
m, network depth L.
2:	Initialization: Randomly initialize θ0 as described in the text
3:	Initialize Z0 = λI, b0 = 0
4:	for t = 1, . . . , T do
5:	Observe {xt,a }aK=1
6:	for a = 1, . . . , K do
7:	Compute Ut,a = f(Xt,a； θt-l) + γt-1 Jg(Xt,a； θt-ι)>Z-1ιg(xt,a; θt-l)/m
8:	Let at = argmaxa∈[K] Ut,a
9:	end for
10:	Play at and observe reward rt,at
11:	Let θt = TrainNN(λ, η, J,m, {Xi,ai}it=1, {ri,ai}it=1,θ0)
12:	Compute Zt = Zt-I + g(xt,at； θt)g(xt,at； θt)>∕m
13:	Compute
γt =，1 + Cim-1/6 PIogmL4t7/6X-7/6
∙ (V ^log^eP^+Cm-1/6PiogmL4t5/3λ-1/6-2logδ + √λS )
+ C3 h(i — ηmλ)J pt∕λ + m-1/6 Plog mL7/2t5/3x-5/3(i + pt∕λ)].
14:	end for
Comparison with Existing Algorithms Here, we compare NeuralUCB with other neural network
based contextual bandit algorithms. Allesiardo et al. (2014) proposed NeuralBandit which consists
of K neural networks. It uses a committee of networks to compute the score of each action and
chooses the action by -greedy policy. In contrast, our NeuralUCB uses upper confidence bound
based exploration, which is more effective than -greedy. In addition, our algorithm only uses one
neural network instead of K neural networks, thus can be computationally more efficient.
4
Under review as a conference paper at ICLR 2020
Algorithm 2 TrainNN(λ,η, J, m, {xig}t=ι, {々/Ji=] θ(0))____________________________
1:	Input: Regularization parameter λ, step size η, number of gradient descent steps U, network
width m, contexts {xi,ai}it=1, rewards {ri,ai}it=1, initial parameter θ(0).
2:	Define L(θ) = Pti=ι(f(xig； θ) -『沁)2/2 + mλ∣∣D - θ⑼k2/2.
3:	for j = 0, . . . , J - 1 do
4:	θj+1) = θ⑶-ηVL(θ(j))
5:	end for
6:	return θ(J).
Lipton et al. (2018) used Thompson sampling on deep neural networks (through variational infer-
ence) in reinforcement learning. A variant is proposed by Azizzadenesheli et al. (2018) that works
well on a set of Atari benchmarks. Riquelme et al. (2018) proposed NeuralLinear, which uses the
first L - 1 layers of a L-layer DNN to learn a representation, then applies Thompson sampling on
the last layer to choose action. Zahavy and Mannor (2019) proposed a NeuralLinear with limited
memory (NeuralLinearLM), which also uses the first L - 1 layers of a L-layer DNN to learn a rep-
resentation and applies Thompson sampling on the last layer. Instead of computing the exact mean
and variance in Thompson sampling, NeuralLinearLM only computes their approximation. Unlike
NeuralLinear and NeuralLinearLM, NeuralUCB uses the entire DNN to learn the representation and
constructs the upper confidence bound based on the random feature mapping defined by the neural
network gradient.
Variant of NeuralUCB We provide a variant of NeuralUCB called NeuralUCB0 in Appendix E.
NeuralUCB0 can be regarded as a simplified version of NeuralUCB where the neural network pa-
rameter vector θt is not updated at each round. In this sense, NeuralUCB0 can be seen as Ker-
nelUCB (Valko et al., 2013) specialized to the Neural Tangent Kernel (Jacot et al., 2018), or Lin-
UCB (Li et al., 2010) with Neural Tangent Random Features (Cao and Gu, 2019a). We will include
both NeuralUCB and NeuralUCB0 in the experiments.
Efficient Implementation Algorithm 1 can be implemented efficiently using iterative first-order
algorithms, as in linear bandit literature (Li et al., 2010; Valko et al., 2013; Agarwal et al.,
2014). More details are given here for the sake of completeness. At round t, we define dt,a =
Zt--11g(xt,a； θt). Based on dt,a, we update det(Zt) by matrix determinant lemma (Golub and
Van Loan, 1996):
det(Zt) =det [Zt-i + g(xtg； θt)g(xtg； θt)>∕m]
= h1 + g(xt,at； θt)>Zt--11g(xt,at； θt)/mi det(Zt-1).	(4.1)
Note that (4.1) only requires to compute vector inner product between g(xt,at； θt) and dt,a, which
only requires O(p) time. Now we show how to compute dt,a efficiently. By the definition of dt,a,
we have
g(xt,a； θt) = Zt-ldt,a = (λI + Eg(Xi皿； θi)g(xi,ai ； θi)>∕m) dt,a∙
i=0
Thus, dt,a is the global minimizer of the following convex optimization problem:
min Il (λi+Eggg； e/gMg； θi)>∕m) d - g(xt,。； θj ,
d∈Rp	i=0	2
and we can use (stochastic) gradient descent to find dt,a efficiently, whose update formula can be
written as follows:
d(i+1) = d⑴-η(λI + Xg(xi,∕i； θi)g(xi,∕i； θi)τ∕m∖
i=0
(λI + Xg(xi,∕i； θi)g(xi,∕i； θi)τ∕m)d⑴一g(xt,a； θt),
i=0
which again only requires to compute vector inner products.
5
Under review as a conference paper at ICLR 2020
5 Regret Analysis
In this section, we present a regret analysis for Algorithm 1. Recall that {xi}iT=K1 is the collection of
all {xt,a}. Since our regret analysis is built upon the recently proposed neural tangent kernel matrix
(Jacot et al., 2018), we start with its formal definition.
Definition 5.1 (Jacot et al. (2018); Cao and Gu (2019a)). For a set of contexts {xi}iT=K1, define
Hei(,1j)=Σi(,1j)=hxi,xji,
ς*I) = 2E(u,v)〜N (0,Ailj)σ(U)σ(V)，
H (l+1) = 2H (ljE(u,v)〜N (0,A(lj )σ0(u)σ0(v) + ς(尸.
Then, H = (He (L) + Σ(L))∕2 is called the neural tangent kernel (NTK) matrix on the context set.
As shown in Definition 5.1, the Gram matrix of the NTK on the contexts {xi }iT=K1 for L-layer neural
networks is defined in a recursive way from the input layer all the way to the output layer of the
neural network. For more details about the derivation of Definition 5.1, please refer to Jacot et al.
(2018). Based on Definition 5.1, we first lay out the assumption on the contexts {xi }iT=K1.
Assumption 5.2. H	λ0I. Moreover, for any 1 ≤ i ≤ TK, kxi k2 = 1 and [xi ]j = [xi ]j+d/2 .
The first part of the assumption says that the neural tangent kernel matrix is non-singular, a mild
assumption commonly made in the related literature (Du et al., 2019a; Arora et al., 2019; Cao and
Gu, 2019a). It can be satisfied as long as no two contexts in {xi }iT=K1 are parallel. The second part is
also mild: for any context x, ∣∣x∣∣2 = 1, we can always construct a new context x0 = [x>, x>]>/√2
to satisfy Assumption 5.2. It can be verified that if θ0 is generated by the random initialization
scheme in Algorithm 1, then f(xi ; θ0) = 0 for any i ∈ [TK].
Next we define the effective dimension d of the neural tangent kernel matrix on contexts {xi }iT=K1.
Definition 5.3. The effective dimension deof the neural tangent kernel matrix on contexts {xi }iT=K1
is defined as
e_ logdet(I + Η∕λ)
=log(1 + TK∕λ).
(5.1)
Remark 5.4. The notion of effective dimension was introduced by Valko et al. (2013) for analyzing
kernel contextual bandits, which was defined by the eigenvalues of any kernel matrix restricted on
the given contexts. We adapt a similar but different definition of Yang and Wang (2019), which was
used for the analysis of kernel-based Q-learning. Suppose the effective dimension of the reproducing
kernel Hilbert space induced by the given kernel is d and the feature mapping ψ induced by the given
kernel satisfies kψ(x)k2 ≤ 1 for any x ∈ Rd. Then, it is easy to verify that we always have d ≤ d;
see Appendix A.1 for details. Intuitively, d measures how quickly the eigenvalues of H decay, and
it only depends on T logarithmically in certain specific cases (Valko et al., 2013).
Now we are ready to present the main result, which provides the regret bound RT of Algorithm 1.
Theorem 5.5. Let d be the effective dimension defined in Definition 5.3. Let h = [h(xi )]iT=K1 ∈
RTK. Suppose there are constants C1, C2 > 0, such that for any δ ∈ (0, 1), if
m = poly(T, L, K, λ-1,λ-1,S-1, log(1∕δ)),	η = Ci(mTL + mλ)-1,
λ ≥ max{1, S-2}, and S ≥ √2h>H-1h. Then, with probability at least 1 - δ over the random
initialization of θ0 , the regret of Algorithm 1 satisfies
RT ≤ 3√T∖Jdlog(1 + TK∕λ) + 2 VJdlog(1 + TK∕λ) + 2 — 2logδ
+ 2√λS + C2(l — λ∕(TL))J PT∕λl +1.	(5.2)
6
Under review as a conference paper at ICLR 2020
Remark 5.6. It is worth noting that, simply applying results for linear bandit to our algorithm would
lead to a linear dependence of P or √p in the regret. Such a bound is vacuous since in our setting P
would be very large compared with the number of rounds T and the input context dimension d. In
contrast, our regret bound only depends on d, which is much smaller than P.
Remark 5.7. Our regret bound (5.2) has a term (1 - λ∕(TL))J,T∕λ, which characterizes the op-
timization error of Algorithm 2. More specifically, it suffices to set J = log(λS∕√T)∕(λ∕(TL))=
O(TL∕λ) (independent of m) such that (1 - λ∕(TL))JpT∕λ ≤ √λS, and therefore the optimiza-
tion error is dominated by √λS.
Remark 5.8. Treating ν and λ as constants, and taking S = √2h>H-1h and J =
log(λS∕√T)∕(λ∕(TL)), the regret bound (5.2) becomes RT =O(PeT Jmax{d, h>H-1 h}).
Specifically, suppose h belongs to a RKHS H induced by NTK and it can be represented by
h(x) = hψ(x), β*i, where ψ(x) is the feature mapping induced by NTK, for some β* in H (Valko
et al., 2013). Then We can show that kh∣∣H = kβ*∣∣2 ≥ √h>H-1h; see Appendix A.2 for more
details. Thus our regret bound can be further written as RT = O (PdeT Jmax{d, |向|发}).
The high-probability result in Theorem 5.5 can be used to obtain a bound on the expected regret.
Corollary 5.9. Under the same conditions in Theorem 5.5, we have
E[Rt] ≤ 3√TJdlog(1 + TK∕λ) + 2 νjdlog(1 + TK∕λ)+ 2 + 2logT
+ 2√λS + Cι(1 - λ∕(TL))JPT∕λj +2.
6	Proof of Main Results
This section provides the proof of Theorem 5.5. We first point out several technical challenges in
this proof:
•	We do not make parametric assumptions on the reward function as some previous work (Filippi
et al., 2010; Chu et al., 2011; Abbasi-Yadkori et al., 2011). Furthermore, unlike the fixed feature
mapping used in Valko et al. (2013), NeuralUCB uses neural network f (x; θt) and its gradi-
ent g(x; θt ) as a dynamic feature mapping depending on θt . These differences make the regret
analysis of NeuralUCB more difficult.
•	In practice, the neural network is often overparametrized, which implies m (and thus P) is very
large. Thus, we need to make sure the regret bound is independent of m.
The two challenges above are addressed by the following technical lemmas. Their proofs are gath-
ered in the appendix.
Lemma 6.1. There exists some constant C > 0 such that for any δ ∈ (0,1), if m ≥
CT4K4L6 log(T2K2L∕δ)∕λ0o, then with probability at least 1 - δ over the random initialization of
θ0, there is a θ* ∈ Rp such that
h(xi) = hg(xi; θ0), θ* - θ0i,	√m∣θ* - Θ0k2 ≤ √2h>H-1h,	(6.1)
for all i ∈ [TK].
Lemma 6.1 suggests that with high probability, the reward function restricted on {xi}iT=K1 can be
regarded as a linear function of g(xi; θ0) parameterized by θ* - θ0, where θ* lies in a ball centered
at θ0. Note that here θ* is not a ground truth parameter for the reward function. Instead, it is
introduced only for the sake of analysis. Equipped with Lemma 6.1, we can utilize existing results
on linear bandits (Abbasi-Yadkori et al., 2011) to show that with high probability, θ* lies in the
sequence of confidence sets.
Lemma 6.2. There exist constants {Ci}2=ι > 0 such that for any δ ∈ (0,1), if η ≤ CI(TmL +
mλ)-1 and m ≥ C? max {T7λ-7L21(log m)3,λ-"2L-3S(log(TKL2∕δ))3S }, then with prob-
ability at least 1 - δ over the random initialization of θ0, we have ∣∣θt - Θ0k2 ≤ 2,t∕(mλ) and
kθ* - θt∣Zt ≤ γt∕√m forall t ∈ [T].
7
Under review as a conference paper at ICLR 2020
Lemma 6.3. Denote aJ=	=	argmaXα∈[κ] hg,a).	There exist constants
{Ci}3=ι > 0 such that for any δ ∈ (0,1), if η ≤ CI(TmL + mλ)-1,m ≥
G max {T7λ-7L21(logm)3,λT∕2L-3∕2(log(TKL2/δ))3∕2}, then with probability at least
1 - δ over the random initialization of θ0 , we have
h(xt,at ) - h(xt,at)
≤ 2γt-i min {kg(x⅛,at； θt-i)∕√mkz-1ι，1}
+ C3(SmT∕6Pi⅛mT 7∕6λT∕6L7/2 + m-1∕6pi⅛mT 5∕3λ-2∕3L3),
Lemma 6.3 gives upper bounds for h(xt,。。- h(xt,aj, which helps us to bound the regret RT. It
is worth noting that γt has a term log det Zt. A trivial upper bound of log det Zt would result in an
dependence on the neural network width m, since the dimension ofZt is p = md+ m2(L - 2) +m.
The next lemma establishes an upper bound which is independent of m, and is only related to
effective dimension d. The dependence on d is similar to Lemma 4 of Valko et al. (2013), but the
proof is different as we use a different notion of effective dimension.
Lemma 6.4. There exist constants {Ci}3=1 > 0 such that for any δ ∈ (0,1), if η ≤ G(TmL +
mλ)-1, m ≥ G max {T7λ-7L21 (log m)3, T6K6L6(log(TKL2/δ))3/2 }, then with probability at
least 1 - δ over the random initialization of θ0, we have
t X Y2-1 min { l∣g(xt,at ； θt-l"√mkZ-1ι, 1}
≤ Jelog(I + TK∕λ) + 1 + C3mT∕6PogmL4TB53λF，1 + C3m-1∕6PlogmL4T7∕6λ-7/6
• (νjelog(1 + TK∕λ) + 1 + C3 m-1∕6Plog mL4 T 5∕3λ-1/6 — 2log δ + √λs)
+ C [(1 - ηmλ)JPT∕λ + m-1∕6Plog mL7∕2T5∕3λ-5∕3(1 + PT∕λ)].
With above lemmas, we begin to prove Theorem 5.5.
Proof of Theorem 5.5. The total regret RT can be bounded as follows:
T
RT = X[h(xt,说)-h(xt,aJ]
t=1
≤ 2 X Yt-1 min { l∣g(xt,at ； θt-l"√mkZ-11 , 1 1
t=1
+ CI(SmT/6 Pl⅞^mT 13∕6λT∕6L7/2 + m-1" Pi⅛mT 8∕3λ-2∕3L3),
≤ 2t T X Y2-1 min {kg(xt,at ； θt-l"√m∣∣Z-V1}
+ CI(SmT/6 Pl⅞^mT 13∕6λT∕6L7/2 + m-1/6 PlθgmT 8∕3λ-2∕3L3)
≤ 3√TJdlog(1 + TK∕λ) + 2 ]ν,dlog(1 + TK∕λ) + 2 — 2log δ
+ 2√λS + C2(1 - ηmλ)JPT∕λ] +1,
where C1 , C2 > 0 are constants, the first inequality holds due to Lemma 6.3, the second inequality
holds Cauchy-Schwarz inequality, the third inequality holds due to Lemma 6.4 and choosing large
enough m. This completes our proof.	□
8
Under review as a conference paper at ICLR 2020
(a) hι(x) = 10(x>a)2
(b) h2(x) = x>A>Ax
Figure 1: Comparison of LinUCB, Neural -Greedy and NeuralUCB.
7	Experiments
While the focus of this work is mostly on theoretical analysis of regret, we present results in proof-
of-concept experiments in simulated problems. We compare it with four representative baselines:
(1) LinUCB; (2) Neural -Greedy, which replaces the UCB based exploration in Algorithm 1 by
-greedy based exploration; (3) NeuralUCB0 ; and (4) Neural -Greedy0 which replaces the UCB
based exploration in NeuralUCB0 by -greedy based exploration. We use the cumulative regret as
the performance metric.
In our simulation, we use contextual bandit problems with context dimension d = 20, the number
of actions K = 4 and the number of rounds T = 10 000. The contextual vectors {x1,1, . . . , xT,K}
are randomly chosen from N(0, I) and then normalized to have unit norm, i.e., kxt,ak2 = 1. For
the reward function h, we investigate the following nonlinear functions:
h1(x) = 10(x>a)2,	h2(x) = x>A>Ax,	h3(x) = cos(3x> a),
where A ∈ Rd×d and each entry of A is randomly generated from N(0, 1), a is randomly chosen
from N(0, I) and normalized to have ∣∣ak2 = 1. For each %(,), the reward at round t for action a is
generated by rt,a = hi(xt,a) + ξt, where ξt is independently drawn from N(0, 1).
For LinUCB, we follow Li et al. (2010) to implement it with a constant radius α. We do a
grid search for α over {0.01, 0.1, 1, 10} and choose the best α for comparison. For NeuralUCB
NeuralUCBo, Neural e-Greedy and Neural e-Greedy°, We choose a two-layer neural network
f(x; θ) = √mW2σ(W1x) with network width m = 20, where θ = [vec(W1)>, Vec(W2)>] ∈
Rp and p = md + m. We remark that the bound on the required network width m is likely
not tight. Therefore, in experiments we choose m to be relatively large (but not as large as the-
ory suggests). For γt in NeuralUCB, we choose γt = γ in the experiment. We do a grid
search over {0.01, 0.1, 1, 10} and choose the best γ for comparison. For NeuralUCB0, we choose
ν = 1, λ = 1, δ = 0.1, and we do a grid search over {0.01, 0.1, 1, 10} for hyper-parameter S to
choose the best S for comparison. For Neural e-Greedy and Neural e-Greedy0, we do a grid search
for e over {0.001, 0.01, 0.1, 0.2} and choose the best e for comparison. For all the algorithms, we
repeat the experiment for 10 runs and report the averaged results for comparison. For TrainNN, we
choose the step size η = 0.1. To accelerate the training process, we update the parameter θt by
TrainNN every 50 rounds. We use stochastic gradient descent with batch size 50 and set J = t at
t-th round of NeuralUCB and Neural e-Greedy.
We plot the cumulative regret of compared algorithms in Figure 1, for reward function h ∈
{h1, h2, h3}. We can see that due to the nonlinearity of reward function h, LinUCB fails to learn the
true reward function and hence achieve an almost linear regret, as expected. In contrast, thanks to
the neural network representation and efficient exploration, NeuralUCB achieves a sublinear regret
which is much lower than that of LinUCB. The performance of Neural e-Greedy is in-between. This
suggests that while Neural e-greedy can capture the nonlinearity of the underlying reward function,
e-Greedy based exploration is not as effective as UCB based exploration. This confirms the effective-
ness of NeuralUCB for contextual bandit problems with any bounded (nonlinear) reward function.
Meanwhile, it is worth noting that NeuralUCB and Neural e-Greedy outperform NeuralUCB0 and
Neural e-Greedy0, which suggests that using deep neural networks to predict the reward function
is better than using a fixed feature mapping associated with NTK. Another observation is that al-
though the network width m in the experiment is not as large as our theory suggests, NeuralUCB still
9
Under review as a conference paper at ICLR 2020
achieves a sublinear regret for nonlinear reward functions. We leave it as a future work to investigate
the impact of m on regret.
8	Conclusions and Future Work
In this work, we proposed a new algorithm NeuralUCB for stochastic contextual bandit problems
based on neural networks. We show that for arbitrary bounded reward function, our algorithm
.∙ Pζ∕T ∕7≡x	-	1
achieves O(d T) regret bound.
Our preliminary experiment results on synthetic data corroborate
our theoretical findings. In the future, we are interested in a systematic empirical evaluation of
NeuralUCB on real world datasets, and compare it with the state-of-the-art neural network based
contextual bandit algorithms (without provable guarantee in regret) (Riquelme et al., 2018; Zahavy
and Mannor, 2019). Another interesting direction is provably efficient exploration with neural net-
work using other strategies like Thompson sampling.
References
ABBASI-YADKORI, Y., PAl, D. and SzepesvAri, C. (2011). Improved algorithms for linear
stochastic bandits. In Advances in Neural Information Processing Systems.
Abe, N., Biermann, A. W. and Long, P. M. (2003). Reinforcement learning with immediate
rewards and linear hypotheses. Algorithmica 37 263-293.
Agarwal, A., Hsu, D., Kale, S., Langford, J., Li, L. and Schapire, R. E. (2014). Taming
the monster: A fast and simple algorithm for contextual bandits. In Proceedings of the 31st
International Conference on Machine Learning (ICML).
Agarwal, D., Chen, B.-C., Elango, P., Motgi, N., Park, S.-T., Ramakrishnan, R., Roy,
S. and ZACHARIAH, J. (2009). Online models for content optimization. In Advances in Neural
Information Processing Systems.
Agrawal, S. and Goyal, N. (2013). Thompson sampling for contextual bandits with linear
payoffs. In International Conference on Machine Learning.
Allen-Zhu, Z., Li, Y. and Song, Z. (2019). A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning.
Allesiardo, R., FERAUd, R. and Bouneffouf, D. (2014). A neural networks committee for
the contextual bandit problem. In International Conference on Neural Information Processing.
Springer.
Arora, S., Du, S. S., Hu, W., Li, Z., Salakhutdinov, R. and Wang, R. (2019). on exact
computation with an infinitely wide neural net. In Advances in Neural Information Processing
Systems.
AuER, P. (2002). using confidence bounds for exploitation-exploration trade-offs. Journal of
Machine Learning Research 3 397-422.
Auer, P., Cesa-Bianchi, N., Freund, Y. and Schapire, R. E. (2002). The nonstochastic
multiarmed bandit problem. SIAM Journal on Computing 32 48-77.
Azizzadenesheli, K., Brunskill, E. and Anandkumar, A. (2018). Efficient exploration
through bayesian deep q-networks. In 2018 Information Theory and Applications Workshop (ITA).
IEEE.
Bubeck, S. and Cesa-Bianchi, N. (2012). Regret analysis of stochastic and nonstochastic multi-
armed bandit problems. Foundations and Trends in Machine Learning 5 1-122.
Bubeck, S.,Munos, R., Stoltz, G. and Szepesvyari, C. (2011). X-armed bandits. Journal of
Machine Learning Research 12 1655-1695.
Cao, Y. and Gu, Q. (2019a). Generalization bounds of stochastic gradient descent for wide and
deep neural networks. In Advances in Neural Information Processing Systems.
10
Under review as a conference paper at ICLR 2020
Cao, Y. and Gu, Q. (2019b). A generalization theory of gradient descent for learning over-
parameterized deep relu networks. arXiv preprint arXiv:1902.01384 .
CHAPELLE, O. and LI, L. (2011). An empirical evaluation of thompson sampling. In Advances in
neural information processing systems.
Chu, W., Li, L., Reyzin, L. and Schapire, R. (2011). Contextual bandits with linear payoff
functions. In Proceedings of the Fourteenth International Conference on Artificial Intelligence
and Statistics.
Dani, V., Hayes, T. P. and Kakade, S. M. (2008). Stochastic linear optimization under bandit
feedback .
DANIELY, A. (2017). Sgd learns the conjugate kernel class of the network. In Advances in Neural
Information Processing Systems.
Du, S., Lee, J., Li, H., Wang, L. and Zhai, X. (2019a). Gradient descent finds global minima
of deep neural networks. In International Conference on Machine Learning.
Du, S. S., Zhai, X., Poczos, B. and Singh, A. (2019b). Gradient descent provably optimizes
over-parameterized neural networks. In International Conference on Learning Representations.
URL https://openreview.net/forum?id=S1eK3i09YQ
Filippi, S., Cappe, O., GARIVIer, A. and SzepesvAri, C. (2010). Parametric bandits: The
generalized linear case. In Advances in Neural Information Processing Systems.
GOLUB, G. H. and VAN LOAN, C. F. (1996). Matrix Computations (3rd Ed.). Johns Hopkins
University press, Baltimore, MD, USA.
GOODFELLOW, i., BENGiO, Y. and COURViLLE, A. (2016). Deep Learning. MiT press. http:
//www.deeplearningbook.org.
Hanin, B. (2017). Universal function approximation by deep neural nets with bounded width and
ReLU activations. arXiv preprint arXiv:1708.02691 .
Hanin, B. and Sellke, M. (2017). Approximating continuous functions by ReLU nets of minimal
width. arXiv preprint arXiv:1710.11278 .
Jacot, A., Gabriel, F. and Hongler, C. (2018). Neural tangent kernel: Convergence and
generalization in neural networks. in Advances in neural information processing systems.
Jun, K.-S., Bhargava, A., Nowak, R. D. and Willett, R. (2017). Scalable generalized linear
bandits: Online computation and hashing. in Advances in Neural Information Processing Systems
30 (NIPS).
Kleinberg, R., Slivkins, A. and Upfal, E. (2008). Multi-armed bandits in metric spaces. in
Proceedings of the fortieth annual ACM symposium on Theory of computing. ACM.
KRAUSE, A. and ONG, C. S. (2011). Contextual gaussian process bandit optimization. in Advances
in neural information processing systems.
KVETON, B., SzepesvAri, C., Ghavamzadeh, M. and Boutilier, C. (2019). Perturbed-
history exploration in stochastic linear bandits. in Proceedings of the 35th Conference on Uncer-
tainty in Artificial Intelligence (UAI).
La ngford, J. and Zhang, T. (2008). The epoch-greedy algorithm for contextual multi-armed
bandits. in Advances in Neural Information Processing Systems 20 (NIPS).
Lattimore, T. and SzepesvAri, C. (2019). Bandit Algorithms. CambridgeUniversityPress. In
press.
Li, L., Chu, W., Langford, J. and Schapire, R. E. (2010). A contextual-bandit approach to
personalized news article recommendation. in Proceedings of the 19th international conference
on World wide web. ACM.
11
Under review as a conference paper at ICLR 2020
Li, L., Lu, Y. and Zhou, D. (2017). Provably optimal algorithms for generalized linear contextual
bandits. In Proceedings of the 34th International Conference on Machine Learning-Volume 70.
JMLR. org.
Li, Y. and Liang, Y. (2018). Learning overparameterized neural networks via stochastic gradient
descent on structured data. In Advances in Neural Information Processing Systems.
LIANG, S. and S RIKANT, R. (2016). Why deep neural networks for function approximation? arXiv
preprint arXiv:1610.04161 .
LIPTON, Z., LI, X., GAO, J., LI, L., AHMED, F. and DENG, L. (2018). BBQ-networks: Efficient
exploration in deep reinforcement learning for task-oriented dialogue systems. In Thirty-Second
AAAI Conference on Artificial Intelligence.
Lu, Z., Pu, H., Wang, F., Hu, Z. andWang, L. (2017). The expressive power of neural networks:
A view from the width. In Advances in neural information processing systems.
RIQUELME, C., TUCKER, G. and SNOEK, J. (2018). Deep bayesian bandits showdown. In Inter-
national Conference on Learning Representations.
RUSMEVICHIENTONG, P. and TSITSIKLIS, J. N. (2010). Linearly parameterized bandits. Mathe-
matics ofOperations Research 35 395-411.
RUSSO, D. and VAN ROY, B. (2014). Learning to optimize via posterior sampling. Mathematics of
Operations Research 39 1221-1243.
Russo, D. and Van Roy, B. (2016). An information-theoretic analysis of thompson sampling.
The Journal of Machine Learning Research 17 2442-2471.
S rinivas, N., Krause, A., Kakade, S. and Seeger, M. (2010). Gaussian process optimization
in the bandit setting: no regret and experimental design. In Proceedings of the 27th International
Conference on International Conference on Machine Learning. Omnipress.
TELGARSKY, M. (2015). Representation benefits of deep feedforward networks. arXiv preprint
arXiv:1509.08101 .
TELGARSKY, M. (2016). Benefits of depth in neural networks. arXiv preprint arXiv:1602.04485 .
Thompson, W. R. (1933). On the likelihood that one unknown probability exceeds another in view
of the evidence of two samples. Biometrika 25 285-294.
Valko, M., Korda, N., Munos, R., Flaounas, I. and Cristianini, N. (2013). Finite-time
analysis of kernelised contextual bandits. arXiv preprint arXiv:1309.6869 .
Yang, L. F. andWang, M. (2019). Reinforcement leaning in feature space: Matrix bandit, kernels,
and regret bound. arXiv preprint arXiv:1905.10389 .
YAROTSKY, D. (2017). Error bounds for approximations with deep ReLU networks. Neural Net-
works 94 103-114.
Yarotsky, D. (2018). Optimal approximation of continuous functions by very deep ReLU net-
works. arXiv preprint arXiv:1802.03620 .
Zahavy, T. and Mannor, S. (2019). Deep neural linear bandits: Overcoming catastrophic for-
getting through likelihood matching. arXiv preprint arXiv:1901.08612 .
Zou, D., Cao, Y., Zhou, D. and Gu, Q. (2019). Stochastic gradient descent optimizes over-
parameterized deep relu networks. Machine Learning .
Zou, D. and Gu, Q. (2019). An improved analysis of training over-parameterized deep neural
networks. In Advances in Neural Information Processing Systems.
12
Under review as a conference paper at ICLR 2020
A Proof of Additional Results in Section 5
A.1 Verification of Remark 5.4
Suppose there exists a mapping ψ : Rd → Rd satisfying kψ(x)k2 ≤ 1 which maps any context
x ∈ Rd to the Hilbert space H associated with the Gram matrix H ∈ RTK×TK over contexts
{xi}iT=K1. Then H = Ψ>Ψ, where Ψ = [ψ(x1), . . . , ψ(xT K)] ∈ Rd×TK. Thus, we can bound the
effective dimension d as follows
~ logdet[I + H∕λ]
=log(1 + TK∕λ)
logdet [I + ΨΨ>∕λ]
log(1+ TK∕λ)
≤ b log - + ψψ>"I∣2
≤ log(1 + TK∕λ).
where the second equality holds due to the fact that det(I + A>A∕λ) = det(I + AA>∕λ) holds

for any matrix A, and the inequality holds since det A ≤ kAk2d for any A ∈ R
as long as I + ΨΨ>∕λ2 ≤ 1 + T K∕λ. Indeed,
TK

d×d
. Clearly, d ≤ d
I + ΨΨ>∕λ2 ≤ 1 + ΨΨ>2∕λ ≤ 1+	ψ(xi)ψ(xi)>2∕λ ≤ 1+TK∕λ,
i=1
where the first inequality is due to triangle inequality and the fact λ ≥ 1, the second inequality holds
due to the definition of Ψ and triangle inequality, and the last inequality is by Iψ(xi)I2 ≤ 1 for any
1 ≤ i ≤ TK.
A.2 Verification of Remark 5.8
Let K(∙, ∙) be the NTKkemel, then for any i,j ∈ [TK], we have Hi,j = K(xi, xj). We first prove
that kh∣∣H = kβ*∣∣2∙ Since h ∈ H, we know that
n
n
n
h(x) = X αiK(xi, x) = X αihψ(xi), ψ(x)i =	X αiψ(xi), ψ(x) ,
i=1
i=1
i=1
for some {xi}n=ι, {αi}n=ι. Thus, we have β* = Pn=ι ɑiψg). Therefore, we have
n
IhI2H =	αiαjK(xi,xj)
i,j=1
n
=	αiαjhψ(xi), ψ(xj)i
i,j=1
n
n
= X αiψ(xi), X αj ψ(xj)
i=1	j=1
=hβ*, β*i
=kβ*k2,
which immediately suggests that IlhkH = kβ*∣∣2. Next we prove that IlhkH ≥ √h>H-1h. First,
h can be decomposed as h = hH + h⊥, where hH(x) = PiT=K1 αeiK(x, xi) is the projection of h to
the function space spanned by {K(x, xi)}iT=K1 and h⊥ is the orthogonal part. By definition we have
h(xi) = hH (xi) for i ∈ [TK], thus
h = [h(x1), . . . , h(xT K)]>
[hH(x1),...,hH(xTK)]>
TK	TK
X αeiK(x1, xi), . . . , XαeiK(xTK,xi)
i=1	i=1
>
= Hαe ,
which implies that αe = H-1h. Thus, we have
IlhlIH ≥ IlhHIIH = √α>Hα = √h>H-1 HHTh = √h>H-1h.
13
Under review as a conference paper at ICLR 2020
A.3 Proof of Corollary 5.9
Proof of Corollary 5.9. Notice that RT ≤ T since 0 ≤ h(x) ≤ 1. Thus, with the fact that with
probability at least 1 - δ, (5.2) holds, we can bound E[RT] as
E[RT] ≤ (1 — δ) (3√T/dlog(1 + TK∕λ) + 2 vʌ/telog(l + TK∕λ) + 2 - 2log δ
+ 2√λS + C2(l — ηmλ)J PT∕λj +l)+ δT.	(A.1)
Taking δ = 1/T completes the proof.	□
B Proof of Lemmas in Section 6
B.1 Proof of Lemma 6.1
We start with the following lemma:
Lemma B.1. Let G = [g(x1; θo),..., g(xτK; θo)]∕√m ∈ Rp×(TK). We denote the neural tan-
gent kernel matrix H the same as Definition 5.1. For any δ ∈ (0, 1), if
m = Ω( L6 log(TKL⑷),
then with probability at least 1 - δ over the random initialization of θ0, we have
kG>G -HkF ≤ TK.
We begin to prove Lemma 6.1.
Proof of Lemma 6.1. By Assumption 5.2, we know that λ0 > 0. Then with probability at least 1 -δ,
we have
G>G H - kG>G - HkFI H - λ0I∕2	H∕2	0,	(B.1)
where the second inequality holds due to Lemma B.1 with the choice of m, the third and fourth
inequality holds due to H λ0I 0. Thus, suppose the singular value decomposition of G
is G = PAQ>, P ∈ Rp×TK, A ∈ Rtk×tk, Q ∈ Rtk×tk, We have A A 0 and θ* =
θo + PATQ>h∕√m satisfies (6.1). To validate that θ* satisfies (6.1), first we have
G> √m(θ* — θο) = QAP>PA-1Q>h = h,
which suggests that for any i, hg(xi; θo), θ* 一 θ0 = h(xi). We also have
m∣∣θ* - θok2 = h>QA-2Q>h = h>(G>G)-1h ≤ 2h>H-1h,
where the last inequality holds due to (B.1). Thus, our proof finishes.	□
B.2 Proof of Lemma 6.2
In this section we prove Lemma 6.2. For the simplicity, we denote Zt, bt, Yt as follows:
t
Zt = λI + Eg(Xia； θo)g(xi,ai； θo)>∕m,
i=1
t
bt = Eri,aig(xi,ai； θo)∕√m,
i=1
Y = VJlog det Zt - 2 log δ + √λS.
det λI
We need the following lemmas. The first lemma shows that the network parameter at each round θt
can be well approximated by θo + Z- 1bt /√m.
14
Under review as a conference paper at ICLR 2020
Lemma B.2. There exist constants {(5i}5=1 > 0 such that for any δ > 0, if η, m satisfy that for all
t ∈ [T],
2√t∕(mλ) ≥ CIm-3/2L-*2[log(TKL2∕δ)]*2,
2pt∕(mλ) ≤ G min {L-6[log m]-3/2, (m(λη)2L-6t-1(log m)-1)3/8},
η ≤ C3(mλ + tmL)-1,
m1/6 ≥ C4√lOgmL7∕2t7∕6λ-7∕6(l + √t∕λ),
then with probability at least 1 - δ over the random initialization of θ0, for any t ∈ [T], we have
that kθt — θ0∣∣2 ≤ 2√t∕(mλ) and
k仇-θo - Z-1bt∕√m∣∣2
≤ (1 — ηmλ)J√t∕(mλ) + Gm-2∕3√log mL7/2t5/3X-5/3(1 + √t∕λ).
Next lemma shows the error bounds for Zt and Zt.
Lemma B.3. There exist constants {Ci}5=1 > 0 such that for any δ > 0, if m satisfies that
C1 m-3/2L-3/2[log(TKL2〃)]3/2 ≤ 2√t∕(mλ) ≤ C2L-6[logm]-3/2, ∀t ∈ [T],
then with probability at least 1 - δ over the random initialization of θ0, we have the following
inequalities for any t ∈ [T]:
IIZtIlF ≤ C3tL,
kZt - ZtkF ≤ C4m-1∕6√l⅛mL4t7∕6λT∕6,
logdet≡ - logd⅛!⅛∣ ≤ C5mT∕6√l⅛mL4t5∕3λT∕6.
det(λI)	det(λI) ∣
With above lemmas, we prove Lemma 6.2 as follows.
ProofofLemma 6.2. By Lemma B.2 we know that ∣θt - θ0∣∣2 ≤ 2/t∕(mλ). By Lemma 6.1, with
probability at least 1 - δ, there exists θ* such that for any 1 ≤ t ≤ T,
h(xt,at) = hg(xt,at ； θ0)∕√m, √m(θ* - θo)i,
√m∣∣θ* - θ0k2 ≤ √2h>H-1h ≤ S,
where the second inequality holds due to the choice of S. Thus, by Theorem 2 in Abbasi-Yadkori
et al. (2011), with probability at least 1 - 2δ, for any 1 ≤ t ≤ T, θ* satisfies that
∣∣√m(θ*- θ0) - Z-1bm* ≤ 7t.	(b.2)
We now prove that ∣θ* - θt∣zt ≤ Yt∕√m. From the triangle inequality,
l∣θ* - θt∣∣z* ≤ ∣∣θ* - θ0 - Z-Ibt/√m∣∣zt+ ∣∣θt - θ0 - Z-Ibt/√m∣∣z* .	(B.3)
'----------7-----------' '----------V------------'
Ii	I2
We bound /1 and /2 separately. For I1, we have
ι2 = (θ* - θ0 - Z-1b t∕√m)τZt(θ* - θ0 - Z-1b t∕√m)
=(θ* - θ0 - Z-1b t∕√m)τZt(θ* - θ0 - Z-1b t∕√m)
+ (θ* - θ0 - Z-1b t∕√m)T(Zt- Zt)(θ* - θ0 - Z-1b t∕√m)
≤ (θ* - θ0 - Z-1b t∕√m)τZt(θ* - θ0 - Z-1b t∕√m)
+ --t Jt"2(θ*- θ0- Z-1bt∕√m)τZt(θ*- θ0- Z-1bt∕√m)
入
≤ (1 + IIZt- Zt∣∣2∕λ)自m,	(B.4)
15
Under review as a conference paper at ICLR 2020
where the first inequality holds due to the fact that x>Ax ≤ x>Bx ∙ k A∣∣2∕λm⅛(B) for some B * 0
and the fact that λmi∩(Zt) ≥ λ, the second inequality holds due to (B.2). By Lemma B.3, we have
IIZt- Zt∣∣2 ≤ IIZt- ZtIlF ≤ Ci m-i/6 √ι⅛mL4t7%λ-v6,
and
Yt = VJlog det Zt — 2 log δ + √λS
det λI
≤ Vjlog T,	+ C2m-1∕6plog mL4t5∕3λ-1∕6 — 2log δ + √λS,
det λI
where C1 , C2 > 0 are two constants. Substituting (B.5) and (B.6) into (B.4), we have
(B.5)
(B.6)
iι ≤ ,ι + kZt- Ztk2∕λ%∕√m
(B.7)
For I2, we have
i2 ≤ C3tLkθt- θo - z-1bt∕√m∣∣2
≤ C4 h(1 — ηmλ)Jpt∕(mλ) + m-2/3plogmL7/2t5/3X-5/3(1 + ,t∕λ)],
(B.8)
where C3, C4 > 0 are two constants, the first inequality holds due to kZtk2 ≤ kZtkF ≤ C3tL by
Lemma B.3, the second inequality holds due to Lemma B.2. Substituting (B.7) and (B.8) into (B.3),
We have Hθ* 一 θt I古 ≤ γt∕√m. Our proof is thus completed.	□
B.3 Proof of Lemma 6.3
The proof starts with three lemmas that bound the error terms of the function value and gradient of
neural networks.
Lemma B.4 (Lemma 4.1, Cao and Gu (2019a)). There exist constants {Ci}3=ι > 0 such that for
any δ > 0, if τ satisfies that
Cιm-3∕2L-3∕2[log(TKL2∕δ)]3∕2 ≤ T ≤ C2L-6[logm]-3/2,
then with probability at least 1 - δ over the random initialization of θ0, for all θ, θ satisfying
kθe- θ0k2 ≤ τ, kθb- θ0k2 ≤ τ andj ∈ [TK] we have
f(xj;e) — f(xj;b) — hg(xj; b), e - bi ≤ C3T4∕3L3∙√mlog m.
Lemma B.5 (Theorem 5, Allen-Zhu et al. (2019)). There exist constants {Ci}3=ι > 0 such that for
any δ ∈ (0, 1), if τ satisfies that
Cιm-3∕2L-3∕2[log(TKL2∕δ)]3∕2 ≤ τ ≤ C2L-6[logm]-3∕2,
then with probability at least 1 - δ over the random initialization of θ0, for all kθ - θ0 k2 ≤ τ and
j ∈ [TK] we have
kg(xj; θ) - g(xj; Θ0)k2 ≤ C3Plog mτ 1∕3L3kg(xj; Θ0)k2.
Lemma B.6 (Lemma B.3, Cao and Gu (2019a)). There exist constants {Ci}3=1 > 0 such that for
any δ > 0, if τ satisfies that
Cιm-3∕2L-3∕2[log(TKL2∕δ)]3∕2 ≤ τ ≤ C2L-6[logm]-3∕2,
then with probability at least 1 - δ over the random initialization of θ0, for any kθ - θ0 k2 ≤ τ and
j ∈ [TK] we have ∣∣g(xj; Θ)∣∣f ≤ C3√mL.
16
Under review as a conference paper at ICLR 2020
ProofofLemma 6.3. We follow the regret bound analysis in Abbasi-Yadkori et al. (2011); Valko
et al. (2013). Denote aJ= = argmaXα∈[κ] h(xt,a) and Ct = {θ ： ∣∣θ - θtkzt ≤ Yt∕√m}. By
Lemma 6.2, for all 1 ≤ t ≤ T, we have ∣∣θt - Θ0k2 ≤ 2pt∕(mλ) and θ= ∈ Ct. By the choice of
m, Lemma B.4, B.5 and B.6 hold. Thus, h(xt川)一 h(xt,αt) Can be bounded as follows:
h(Xtq ) - h(Xt,αj
=hg(xt,a* ; θ0), θ* - θ0i - hg(xt,at; θ0), θ* - θ0i
≤ hg(Xt,a3 θt-1), θ* - θθi - hg(Xt,at ； θt-1), θ* - θθi
+ kθ* - θ0∣2(Ilg(Xt,a「; θt-1) - g(xt,a* ; θ0)k2 + kg(xt,a, ; θt-1) - g(xt,at ; θ0)k2)
≤ hg(xt,at ； Θt-1), θ* - θoi - hg(xt,at ； Θt-1), θ* - θoi + Ci√h>H-1hm-1/6plog mt1/6A-1/6L7/2
≤ OmaX hg(xt,a-； θt-1), θ - θθi - hg(xt,at ； θt-1), θ* - θθi
θ∈Ct-1
+ C1√h>HThmT∕6Plog mt1∕6λT∕6L7/2,	(B.9)
where the first inequality holds due to triangle inequality, the second inequality holds due to Lemma
6.1, Lemma B.5 and B.6, the third inequality holds due to θ= ∈ Ct-1. Denote
Ut,a = hg(xt,a; θt-1 ), θt-1 - θθi + Yt-I Jg(Xt,a； θt-1)>Z-1ιg(Xt,a; θt-i)/m,
then we have Ut,a = maXθ∈Ct-1 hg(xt,a; θt-1), θ - θ0i with the fact that
ha, bi + c a>A-1a = maX	ha, xi.
kx-bkA≤c
We also have
∣Ut,a - Ut,a∣ ≤ C2mT∕6Pl0gmt2∕3λ-2∕3L3,	(B.10)
where C2 > 0 is a constant, the inequality holds due to Lemma B.4 with the fact ∣θt-1 - θ0 ∣2 ≤
2pt∕(mλ)) and the fact f (xj; θo) = 0. Since θ= ∈ Ct-ι, then (B.9) can be bounded as
max hg(Xt,a3 θt-i), θ - θθi - hg(Xt,at ； θt-i), θ* - θθi
θ∈Ct-1
=Ut川-hg(Xt,at ； θt-1 ), θ*- θθi
≤ Ut,a* - hg(xt,at; θt-1), θ= - θ0i + C2m 1" Plog mt2,3) “3L3
≤ Ut,at - hg(xt,at ； θt-i), θ* - θoi + C2m-1∕6 PIogmt2∕3λ-2∕3L3
≤ Ut,at -hg(Xt,at ； θt-1), θ* - θθi + 2C2m-1∕6 Piogmt2∕3λ-2∕3L3,	(B.11)
where the first inequality holds due to (B.10), the second inequality holds since at = argmaxa Ut,a,
the third inequality holds due to (B.10). Furthermore,
Uet,at - hg(xt,at ; θ0), θ - θ0i
= max hg(xt,at; θt-1), θ -θ0i - hg(xt,at; θt-1), θ= -θ0i
θ∈Ct-1
= max hg(xt,at; θt-1), θ - θt-1i - hg(xt,at; θt-1), θ= - θt-1i
θ∈Ct-1
≤ max θ - θt-1Zt 1 ∣g(xt,at; θt-1)∣Z-1 + θ= -θt-1Zt 1 ∣g(xt,at; θt-1)∣Z-1
θ∈Ct-1	t-1	t-1	t-1	t-1
≤ 2γt-i∣∣g(xt,at； θt-i)∕√mkz-iι,	(B.12)
17
Under review as a conference paper at ICLR 2020
where the first inequality holds due to Cauchy-Schwarz inequality, the second inequality holds due
to Lemma 6.2. Combining (B.9), (B.11) and (B.12), we have
h(xt,at) — h(Xtg)
≤ 2γt-ikg(xt,at; θt-i)∕√m∣∣z-ι + Cι√h>H-1hmT∕6plogmt1∕6λ-1∕6L7/2
t-1
+ 2C2m-1/Pom刊/…L
≤ min{2γt-i∣∣g(xt,at; θt-i)∕√mkz: + Ci√h>H-1 hmT/6PlOgmt1/6XT/6L7/2
+ 2C2mT/6PI0gmt2/3X-2/3L3, l}
≤ min ∣2γt-ιkg(xt,at; θt-ι)∕√mkz-2ι, lj + Ci√h>HThmT6Pl⅛mt1%λT6L7/2
+ 2C2m-1/Pom钞…3"
≤ 2γt-i min ∣kg(xt,at; θt-i)/√mkZ-iγ, 1 j + Ci√h>H-1hm-1 /6 plog mt1 /6k-1/L7/
+ 2C2m-1∕6 P0gmt2/3 λ-2∕3 L3,	(B.13)
where the second inequality holds due to the fact that 0 ≤ h(xt川)—h(xt,at) ≤ 1, the third
inequality holds due to the fact that min{a + b, 1} ≤ min{a, 1} + b, the fourth inequality holds due
to the fact γt-ι ≥ √λS ≥ 1. Finally, by the fact that √2hH-1h ≤ S, We finish the proof. □
B.4 Proof of Lemma 6.4
In this section we prove Lemma 6.4, we need the following lemma from Abbasi-Yadkori et al.
(2011).
Lemma B.7 (Lemma 11, Abbasi-Yadkori et al. (2011)). We have the following inequality:
T f
Emin ‹j ∣∣g(xt,at ； θt-l)∕√mkZ 二,1
t=1	t-1
det ZT
≤ 2logdtλI.
Proof of Lemma 6.4. First by the definition of γt, we know that γt is a monotonic function w.r.t.
detZt. By the definition of Zt, we know that ZT Zt, which implies thatdetZt ≤detZT. Thus,
γt ≤ γT . Second, by Lemma B.7 we know that
Emin{ Ilg(Xt,at;θt-i"√m∣∣Z二，1 ∖
t=1	t-1
≤ 2 log
≤ 2 log
det ZT
det λI
ddetZI + CimT6POgmL4T 5/3入-1/6,
(B.14)
where the second inequality holds due to Lemma B.3. Next we are going to bound log det ZT.
Denote G = [g(xi; θo)/√m,..., g(xτK; θo)∕√m] ∈ Rp×(TK), then we have
log detλT =logdet (1 + X g(xt,at ； θ°)g(xt,at ； θθ)>∕(mλ))
t=i
TK
≤ log det I + E g(xi; θo)g(xi; θo)>∕(mλ))
i=i
= log det I + GG>∕λ
log det I+ G>G∕λ ,
(B.15)
18
Under review as a conference paper at ICLR 2020
where the inequality holds naively, the third equality holds since for any matrix A ∈ Rp×TK, we
have det(I + AA>) = det(I + A>A). We can can be further bound (B.15) by the follows:
logdet(I + G>G∕λ) =Iogdet(I + H∕λ + (G>G — H)∕λ)
≤ logdet(I + H∕λ) + h(I + H∕λ)-1, (G>G - H)∕λ)
≤ log det I+H∕λ +k(I+H∕λ)-1kFkG>G-HkF∕λ
≤ logdet(I + H∕λ) + √TK∣∣G>G — HkF
≤ log det I + H∕λ + 1
≤ delog(1 + T K∕λ) + 1,	(B.16)
where the first inequality holds due to the concavity of log det(∙), the second inequality holds due
to the fact that(A, B〉≤ IlAkFIIBl∣f, the third inequality holds due to the facts that I + H∕λ 占 I,
λ ≥ 1 and k A∣∣f ≤ √TKk A∣2 for any A ∈ RTK×tk, the fourth inequality holds by Lemma B.1
with the choice of m, the fifth inequality holds by the definition of effective dimension in Definition
5.3, and the last inequality holds due to the choice of λ. We now bound γT, which is
γτ = ʌ/1 + Cιm-1∕6plog mL4 T 7∕6λ-7∕6
• (V jog detZT + C2m-1/6PlogmL4T5/3X-1/6 — 2logδ + √λs)
+ C3 h(1 - ηmλ)JPT∕(mλ) + m-2/3Plog mL7/2T5/3入-5/3(1 + PT∕λ)]
≤ Jl + Cιm-1∕6PiogmL4T7∕6λ-7/6
• (V jog detZT + C2m-1/6PlogmL4T5/3X-1/6 — 2logδ + √λs)
+ C3 [(1 - ηmλ)JPT∕(mλ) + m-2/3PlogmL7/2T5/3X-5/3(1 + PT?!)],
(B.17)
19
Under review as a conference paper at ICLR 2020
where the inequality holds due to Lemma B.3. Finally, substituting (B.15), (B.16) into (B.14), using
(B.14) and (B.17), we have
∖
T (
Σ Y2-1 min k l∣g(xt,at ； θt-l)∕√mkZ二,1
≤ γT t
log
X min { kg(xt,at ； θt-i)∕√m∣∣Z二,1 }
:det ZT C CImT/6 PlogmL4T5/3X-1/6
det λI
1∣+C CImT/6PlOgmL4T7/6入-7/6
≤
• (ν ylogd^ZT + C2m-1/6PlogmL4T 5∕3λ-1∕6 — 2log δ + √λs)
C C3 h(1 - ηmλ)JPT∕(mλ) + m-3/2 PlogmL7/2T5/3入-5/3(1 + PT∕λ)
≤
JeIOg(I + TK∕λ) + 1 + CImT/6 PlogmL4T 5/3入-/6
，1 + CImT/6Plog mL4τ 7/6入-7/6
• (Vyeog7+ΞKλ^+7+Cm-^6plogmLT^3λ-^6-"2ιθgδ+√s
+。3 h(1 - ηmλ)JPT∕(mλ) C m-3/2Plog mL7/2T5/3入-5/3(1 + PT?1)].
□
C	Proofs of Technical Lemmas in Appendix B
C.1 Proof of Lemma B.1
In this section we prove Lemma B.1, we need the following lemma from Arora et al. (2019):
Lemma C.1 (Theorem 3.1, Arora et al. (2019)). Fix > 0 and δ ∈ (0, 1). Suppose that
m = Ω( L6 log4L∕δ)),
then for any i, j ∈ [TK], with probability at least 1 - δ over random initialization of θ0, we have
|hg(xi； θ0), g(xj； θ0)i∕m - Hi,j | ≤ .	(C.1)
Proof of Lemma B.1. Taking union bound over i, j ∈ [TK], we have that if
m = Ω
L6 log(T2K2L∕δ)
ɪ4
then with probability at least 1 - δ, (C.1) holds for all (i, j) ∈ [TK] × [TK]. Therefore, we have
lG>G - HlF = t
TK TK
XX
|hg(xi； θ0), g(xj； θ0)i∕m -Hi,j|2 ≤ TK.
i=1 j=1
□
20
Under review as a conference paper at ICLR 2020
C.2 Proof of Lemma B.2
In this section we prove Lemma B.2. During the proof, for the simplicity, we omit the subscript t by
default. We define the following quantities:
J(j) = g(x1,a1; θ(j)),..., g(xt,at; θ(j)) ∈ R(md+m2(L-2)+m)×t,
H(j) = [J(j)]>J(j) ∈ Rt×t,
f⑺=(f (Xl,aι ； θ⑺),…，f (Xt,at ； θ⑶))> ∈ Rt×1,
v = (r1,a1, . . . , rt,at) ∈ Rt×1.
Then the update rule of θ(j ) can be written as follows:
θ(j+1) = θ(j) -ηJ(j)(f(j) -v) + mλ(θ(j) - θ(0)).	(C.2)
We also define the following auxiliary sequence {θe(k)} during the proof:
θe(0) = θ(0), θe(j+1) = θe(j) - ηJ(0)([J(0)]>(θe(j) - θe(0)) -v) +mλ(θe(j) - θe(0)).
Next lemma provides perturbation bounds for J(j), H(j) and kf (j +1) - f(j) - [J(j)]> (θ(j+1) -
θ(j))k2.
Lemma C.2. There exist constants {Ci}6=1 > 0 such that for any δ > 0, if T satisfies that
Gm-3/2L-3/2[log(TKL2/6)]3/2 ≤ T ≤ GL-6[logm]-3/2,
then with probability at least 1 - δ over the random initialization of θ(0), if for any j ∈ [J], kθ(j) -
θ(0) k2 ≤ T, we have the following inequalities for any j ∈ [J],
IJj)IIF ≤ C4√mL,	(C.3)
JCj) - J⑼kF ≤ C5ptmlogmτ 1/3L7/2,	(C.4)
∣∣f(j+1) - f(j) - [j(j)]>(θ(j+1) - θ(j))∣∣2 ≤ C6T4/3L3Ptmlogm,	(C.5)
∣Wk2 ≤ √t.	(C.6)
Next lemma gives an upper bound for kf(j) - vk2 .
Lemma C.3. There exist constants {Ci}4=ι > 0 such that for any δ > 0, if τ,η satisfy that
CIm-3/2L-3/2[log(TKL2/6)]3/2 ≤ T ≤ C2L-6[logm]-3/2,,
η ≤ C3(mλ + tmL)-1,
τ8/3 ≤ C4m(λη)2L-6t-1 (log m)-1,
then with probability at least 1 - δ over the random initialization of θ(0), if for any j ∈ [J], kθ(j) -
θ(O)Il2 ≤ τ, we have that for any j ∈ [J], ∣∣fCj) - v∣∣2 ≤ 2√t.
Next lemma gives an upper bound of the distance between auxiliary sequence kθe(j) - θe(0) k2.
Lemma C.4. There exist constants {Ci}3=ι > 0 such that for any δ ∈ (0,1), if τ, η satisfy that
Cim-3/2L-3/2[log(TKL2/6)]3/2 ≤ T ≤ C2L-6[logm]-3/2,,
η ≤ C3(tmL + mλ)-1,
then with probability at least 1 - δ, we have that for any j ∈ [J],
IIej)- θ(0)∣∣2 ≤ pt∕(mλ),
∣∣θj) — e(。)—(Z)Tb/√mII2 ≤ (1 — ηmλ)Jpt∕(mλ)
With above lemmas, we prove Lemma B.2 as follows.
21
Under review as a conference paper at ICLR 2020
ProofofLemma B.2. Set T = 2pt∕(mλ). First We assume that kθ(j) 一 θ(0) ∣∣2 ≤ T for all 0 ≤ j ≤
J. Then with this assumption and the choice of m, τ, we have that Lemma C.2, C.3 and C.4 hold.
Then We have
∖∖θ(j+1) 一 e(j+1)∣∣2 = ∣∣θ(j) 一 e⑶ 一 η(Jj) — J(O))(f⑶ 一 V) — ηmλ(θ(j) 一 e⑶)
一 ηJ(0)(f(j) 一 [J(0)]>(θe(j) 一 θ(0)))2
= ∣∣∣(1 一 ηmλ)(θ(j) 一 θe(j)) 一 η(J(j) 一 J(0))(f(j) 一 v)
一 ηJ(0) hf(j) 一 [J(0)](θ(j) 一 θ(0)) + [J(0)]>(θ(j) 一 θe(j))i∣∣∣
≤ ∣∣[i 一 η(mλi + H(O))](ej) — θ(j))∣∣2 + η∣∣(J⑶一 J(O))(f(j) — v)∣∣2
+ η∣J(0) ∣2 ∣∣f(j) 一 [J(0)](θ(j) 一 θ(0))∣∣2
≤ (1 一 ηmλ)∣∣ej) 一 θ(j)∣∣2 + η∣∣(J(j) 一 J(O))(f(j) 一 v)∣∣2
|
{Z
I1
+ η∣J(O) ∣2 ∣∣f(j) 一 [J(O)](θ(j) 一 θ(O))∣∣2,
X--------------------------}
}
(C.7)
{z
I2
Where the first inequality holds due to triangle inequality, the second inequality holds due to the fact
that ηH(O) = η[J(O)]>J(O) C1tmLηI ≤ I With (C.3) in Lemma C.2 for some C1 > 0. We noW
bound I1 , I2 separately. For I1 , We have
Ii ≤ η∣∣J(j) — J(O)I∣2∣∣fS- v∣2 ≤ ηC2~tJmlog mτ 1/3L7/2,	(C.8)
Where C2 > 0 is a constant, the first inequality holds due to matrix spectral norm and the second
inequality holds due to (C.4) in Lemma C.2 and Lemma C.3. For I2, We have
I2 ≤ η|∣J(0)||2||f⑶一 J(0)(θ(j) — θ(o))( ≤ nC3tmL7/2T4/3Plogm,	(C.9)
Where C3 > 0, the first inequality holds due to matrix spectral norm, the second inequality holds
due to (C.3) and (C.5) in Lemma C.2 and the fact that f(O) = 0 by random initialization over θ(O).
Substituting (C.8) and (C.9) into (C.7), We have
∣∣θ(j+1) 一 θe(j+1) ∣∣2
≤ (1 一 ηmλ) ∣∣θ(j) 一 θ(j)∣∣2 + C4 (ηt Pm log mτ1/3 L7/2 + ηtmL7/2τ4/3 Plog m),
Where C4 > 0 is a constant. Expanding (C.10) for k times, We have
(C.10)
∣∣θ(j+1) 一 θe(j+1) ∣∣2
ηt√m log mτ 1/3L7/2 + ηtmL7//JT4∕3√log m
≤ C4-------------------ʌ-----------------
ηmλ
C5m-2/3 v4ogmL7/2t5/3A-5/3(1 + √t∕λ)
≤ 2,	(C.11)
Where C5 > 0 is a constant, the equality holds by the definition ofT, the last inequality holds due to
the choice of m, Where
m1/6 ≥ CgPlogmL7∕2t7∕6λ-7∕6(1 + pt∕λ),
C6 > 0 is a constant. Thus, for any j ∈ [J], We have
kθj) — θ(°)k/ ≤ ke(j) 一 θ(°)∣∣2 + kθj) — e(j)k/ ≤ PtKmλ + T∕2 = τ, (C.12)
Where the first inequality holds due to triangle inequality, the second inequality holds due to Lemma
C.4. (C.13) suggests that our assumption ∣θ(j) 一 θ(O) ∣2 ≤ T holds for any u. Note that We have the
folloWing inequality by Lemma C.4:
∣∣e(j) 一 e(O) -(Z)Tb∕√m∣∣2 ≤ (1 一 ηmλ)j Pt∕(mλ).	(C.13)
Using (C.11) and (C.13), We have
忸⑺一 θ(o). (Z)Tb∕√m∣∣2
≤ (1 一 ηmλ)JPt∕(mλ) + Cbm-2/3PlogmL7∕2t5∕3λ-5∕3(1 + /t∕λ).
This completes the proof.	□
22
Under review as a conference paper at ICLR 2020
C.3 Proof of Lemma B.3
In this section we prove Lemma B.3.
ProofofLemmaB.3. Set T = 2 vzt∕(mλ). By Lemma 6.2, we have that ∣∣θi - θ0∣∣2 ≤ T for i ∈ [t].
kZt kF can be bounded as follows.
t t
l∣Zt∣F = X g(Xi,ai ； θi)g(Xi,ai ； θi)> Im	≤ Xug(Xi,ai ； θi)^/m ≤。0力〃
where C0 > 0 is a constant, the first inequality holds due to the fact that ∣aa>∣F = ∣a∣22, the
second inequality holds dueto LemmaB.6 with thefactthat ∣θi - θo∣2 ≤ T. WeboUnd ∣∣Zt - Zt∣2
as follows. We have
ut	u
∣∣Zt - Zt ∣∣F = X (g(Xig ； θθ)g(Xi,ai ； θθ)> - g(Xi,ai ； θi)g(Xig ； θi)>)/m
i=1	F
t
≤ X Ug(Xi,ai ； θθ)g(Xi,ai ； θθ)> - g(Xi,ai ； θi)ggg ； θi)>(尸/m
t
≤ X (Ug(Xi,ai ； θ0)ll2 + ||g(Xi,ai ； θi 升 2) Ug(Xi0； θO)- g(XMai ； θi)U2lm,
i=1
(C.14)
where the first inequality holds due to triangle inequality, the second inequality holds the fact that
∣aa> - bb> ∣F ≤ (∣a∣2 + ∣b∣2)∣a - b∣2 for any vectors a, b. To bound (C.14), we have
∣∣g(Xi,°i； θo)∣∣2, ∣∣g(Xig； θi)∣l2 ≤ Cι√mL,	(C.15)
where C1 > 0 is a constant, the inequality holds due to Lemma B.6 with the fact that ∣θi-θ0∣2 ≤ T.
We also have
∣∣g(Xi,ai ； θθ) - g(Xi,ai ； θi)∣∣2 ≤ C Plog mτ 1/3L3kg(Xj ； θθ)∣2 ≤ C3 √m1θgmT 1/3L7/2,
(C.16)
where C2 , C3 > 0 are constants, the first inequality holds due to Lemma B.5 with the fact that
∣θi - θ0 ∣2 ≤ T, the second inequality holds due to Lemma B.6. Substituting (C.15) and (C.16) into
(C.14), we have
∣Zt- ZtkF ≤ C4tp0gmτ 1/3L4,
where C4 > _0 is a constant. We now bound log det Zt - log det Zt. It is easy to verify that
Zt = λI + J[J]>, Zt = λI + JJ>, where
J = (g(Xl,αι ； θθ),…，g(Xt,at ； θθ)) /√m,
J = (g(Xl,αι ； θθ),…，g(Xt,at ； θt-l))/√m∙
We have the following inequalities:
log det，'；) - log de%) = Iogdet(I + J[J]>/A) - Iogdet(I + J[J]>/A)
det(λI)	det(λI)
=logdet(I + [J]>J/A) - logdet(I + [J]>J/A)
≤ h(I +[J]>J")T, [J]>J - [J]>Ji
≤ k(I +[J]>J/A)-1kFk[J]>J - [J]>J∣F
≤√tk(I +[J]>J/A)-1k2k[J]>J - [J]>J∣F
≤√t∣[J]>J - [J]>J∣F,	(C.17)
23
Under review as a conference paper at ICLR 2020
where the second equality holds due to the fact that det(I + AA>) = det(I + A>A), the first
inequality holds due to the fact that log det function is convex, the second inequality hold due to
the fact that(A, Bi ≤ IlAkFIIBIIF, the third inequality holds since I + [J]>J∕λ is a t-dimension
matrix, the fourth inequality holds since I + [J]>J∕λ 占 I. We have
kJ]>J -[J]>J∣∣F
≤ t max g(xi,ai; θ0)>g(xj,aj; θ0) - g(xi,ai; θi)>g(xj,aj; θj)/m
≤ t Ima≤∕g(Xi,ai ； θO)- g(Xi,ai ； θi 升』g(Xj& ； θj )||2/m
+ l∣g(xj,aj ； θO)- g(xjg ； θj )∣lJlg(xi,ai ； θ0 )^/m
≤ C5tplogmτ 1/3L4,	(C.18)
where C5 > 0 is a constant, the first inequality holds due to the fact that IAIF ≤ t max |Ai,j | for
any A ∈ Rt×t, the second inequality holds due to the fact |a>a0 - b>b0| ≤ Ia - bI2 Ib0I2 + Ia0 -
b0I2 IaI2, the third inequality holds due to (C.15) and (C.16). Substituting (C.18) into (C.17), we
have
log det(Zt) - log det(Zt) ≤ C5t3∕2p0gmτ1/3L4.
det(λI)	det(λI)	5
Using the same method, we also have
log det(Z) - log det(⅛ ≤ C5t3/2 PogmT1/3L4.
det(λI)	det(λI)
This completes our proof.
□
D Proofs of Lemmas in Appendix C
D.1 Proof of Lemma C.2
In this section we give the proof of Lemma C.2.
Proof of Lemma C.2. For any j ∈ [J], the following inequalities hold. We first have
∣∣J(j)∣∣F ≤ √tmax∣∣g(xi,ai； θ(j))∣∣2 ≤ Cι√tmL,
i∈[t]
(D.1)
where C1 > 0 is a constant, the first inequality holds due to the definition of J(j), the second
inequality holds due to Lemma B.6. We also have
∣jCj) — j(o)∣f ≤ C2plog mτ 1/3L3kJ(0)kF ≤ C3ptm log mτ1/3 L7/2,	(D.2)
where C2 , C3 > 0 are constants, the first inequality holds due to Lemma B.5 with the assumption
that IθCj) - θCO) I2 ≤ τ, the second inequality holds due to (D.1). We also have
∣∣f Cj+1) - fCj) - [JCj)]> (θCj+1) - θCj))∣∣2
≤ max √t∣f (Xi,ai ； θ(j+1)) - f(Xi,ai ； θ(j) ) - (g(Xi,ai ； θj) θ(j+1) - θ(j)i∣
i∈[t]
≤ C4τ4∕3L3ptm log m,
where C4 > 0 is a constant, the first inequality holds due to the the fact that ∣x∣2 ≤ √tmax |xi|
for any X ∈ Rt, the second inequality holds due to Lemma B.4 with the assumption that IθCj ) -
θ(0)∣∣2 ≤ T, kθ(j+1) - θ(0)∣2 ≤ T. For ∣∣v∣2, we have ∣∣v∣2 ≤ √tmaxι≤i≤t ∣r(xi,aj ≤ √t. This
completes our proof.
□
24
Under review as a conference paper at ICLR 2020
D.2 Proof of Lemma C.3
Proof of Lemma C.3. Recall that the loss function L is defined as
L(O)=2kf ⑹-vk2+m kθ - θ(0)k2.
We define J(θ) and f(θ) as follows:
J(θ) = (g(xi,aι ； θ),..., g(Xt,at ； θ)) ∈ R(md+m2(L-2Hm)×t,
f(θ) = (f(x1,a1; θ),...,f(xt,at; θ))> ∈ Rt×1.
Suppose ∣∣θ - θ(0)∣2 ≤ τ. Then by the fact that ∣ ∙ ∣∣2∕2 is 1-strongly convex and 1-smooth, we
have the following inequalities:
L(θ0) - L(θ)
≤ hf (θ)	- v,	f (θ0) - f (θ)i	+ 2 ∖∖f (θ0) - f (θ)∖∖2 + mλhθ - θ⑼,θ -	θi	+ m2λ∖∖θ0	- θ∖∖2
=hf(θ)	- v,	[J(θ)]>(θ0 -	θ) + ei + 1∖∖[J(θ)]>(θ0 - θ) + e∖∖2
+ mλhθ - θ⑼,θ - θi + m2λ∖∖θ0 - θ∖∖2
= hJ(θ)(f(θ) -v)+mλ(θ-θ(0)),θ0 -θi + hf (θ) -v,ei
+ 2∖∖[j(θ)]>(θ0 -θ)+e∖∖2+ mλ∖∖θ0 - θ∖∖2
hvL(θ),θ - θi + hf⑻一v,ei + 2∖∖[j(θ)]>(θ0 - θ)+e∖∖2 + m^∖∖θ0 -θ∖∖2,
X--------------------------------------------------------------------------}
(D.3)
{Z
I1
where e = f(θ0) - f(θ) - J(θ)>(θ0 - θ). I1 can be bounded as follows:
Ii ≤ kf⑹一vk2kek2+ Ilj(θ)k2llθ0 -θ∣2+ I∣ek2+ -2-∖∖θ0 - θ∖∖2
≤ ^21 ,mλ + tmLVIe0 - θ∖∖2) + kf⑹-v∣∣2∣∣e∣∣2 + IleIl2,
(D.4)
where the first inequality holds due to CaUchy-SchWarz inequality, the second inequality holds due
to the fact that k J(θ)k2 ≤ C2√tmL with ∣∣θ - θ(0)k2 ≤ T by (C.3) in Lemma C.2. Substituting
(D.4) into (D.3), we have
L(θ0) - L(θ) ≤ hVL(θ), θ0 - θi + C1 ((mλ + tmL)∖∖θ0 - θ∣0 + ∣f(θ) - v∣2∣e∣2 + ∣e∣2.
(D.5)
Taking θ0 = θ - ηVL(θ), then by (D.5), we have
L(θ - ηVL(θ)) - L(θ) ≤ -ηIVL(θ)I22(1 - C1(mλ + tmL)η) + If (θ) - vI2IeI2 + IeI22.
(D.6)
By the 1-strongly convexity of ∣∙ ∣∣2,we further have
L(θ0) - L(θ)
≥ hf(θ) - v, f(θ0) - f(θ)i + mλhθ - θ(0), θ0 - θi + m2λ∖∖θ0 - θ∖∖2
=hf(θ) - v, [J(θ)]>(θ0 - θ) + ei + mλhθ - θ(0), θ0 - θi + m2λ ∖∖θ0 - θ∖∖2
=hVL(θ), θ0	- θi	+	mλ ∖∖θ0	- θ∖∖2 +	hf(θ) - v, ei
≥ hvL(θ), θ	- θi	+	-2^∖∖θ0	- θ∖∖2 -	kf(θ) - vk2kek2
≥-kvLmθ)k2 - kf(θ) - v∣2∣e∣2,
(D.7)
25
Under review as a conference paper at ICLR 2020
where the second inequality holds due to Cauchy-Schwarz inequality, the last inequality holds due
to the fact that ha, xi + ckxk22 ≥ -kak22/(4c) for any vectors a, x and c > 0. Substituting (D.7)
into (D.6), we have
L(θ — ηVL(θ)) — L(θ)
≤ 2mλη(1 - C1(mλ + tmL)η)L(θ0) - L(θ) + kf (θ) - vk2 kek2 + kf(θ) - vk2kek2 + kek22
≤ mληL(θ0) - L(θ) + kf (θ) - vk2kek2 + kf (θ) - vk2kek2 + kek22
≤ mλη[L(θ0) - L⑹ + Ilf⑹—Vk2/8 + 4kek2] + mληkf⑹—Vk2/8 + 4kek2〃mλ^ + Ilek2
≤ mλη(L(θ0) - L(θ)∕2) + ∣∣ek2(1 + 4mλη + 4∕(mλη)),	(D.8)
where the second inequality holds due to the choice of η, third inequality holds due to Young’s
inequality, fourth inequality holds due to the fact that kf(θ) - Vk22 ≤ 2L(θ). Now taking θ = θ(j)
and θ0 = θ(0), rearranging (D.8), with the fact that θ(j+1) = θ(j) - ηVL(θ(j)), we have
L(θ(j+1)) - L(θ(0))
≤ (1 - mλη∕2)[L(θj)) - L(θ(0))] + mλη∕2L(θ(0)) + ∣∣e∣2(1 + 4mλη + 4∕(mλη))
≤ (1 — mλη∕2)[L(θ(j)) — L(θ(0))] + mλη∕2 ∙ t + mλη∕2 ∙ t
≤ (1 - mλη∕2)[L(θ(j)) - L(θ(0))] + mληt,	(D.9)
where the second inequality holds due to the fact that L(θ(0)) = kf(θ(0)) - Vk22∕2 = kVk22∕2 ≤ t,
and
(1 + 4mλη + 4∕(mλη))∣e∣2 ≤ 5∕(mλη) ∙ C2τ8/3L6tmlogm ≤ tmλη∕2,
(D.10)
where the first inequality holds due to (C.5) in Lemma C.2, the second inequality holds due to the
choice of τ. Expanding (D.9) u times, we have
L(θ(j+1)) - L(θ(0)) ≤
mληt = 2t
mλη∕2
which implies that kf (j+1) -v∣2 ≤ 2√t. This completes our proof.
□
D.3 Proof of Lemma C.4
In this section we prove Lemma C.4.
Proof of Lemma C.4. It is worth noting that θe(j ) is the sequence generated by applying gradient
descent on the following problem:
min L(θ) = 1 k[j(0)]>(θ - θ(0)) - v∣2 + 竽|怛-θ ⑼ ∣∣2.
θ2	2
Then kθ(0) - θe(j)k2 can be bounded as
m2λ kθ(0) - e(j)k2 ≤ I kj⑼]>(e⑶-θ⑼)-vk2 + m2λ∣∣ej) - θ叫 2
≤ ∣ kj⑼]>(e⑼-θ⑼)-v∣∣2 + m2λ∣∣e⑼-θ叫 2
≤ T∕2,
where the first inequality holds trivially, the second inequality holds due to the monotonic decreasing
property brought by gradient descent, the third inequality holds due to (C.6) in Lemma C.2. It is
easy to verify that Le is a λ-strongly convex and function and C1 (tmL + mλ)-smooth function, since
V2L W (∣∣J(0)∣∣2 + mλ)l W C1 (tmL + mλ),
where the first inequality holds due to the definition of L, the second inequality holds due to (C.3)
in Lemma C.2. Since we choose η ≤ C2 (tmL + mλ)-1 for some small enough C2 > 0, then by
standard result of gradient descent on ridge linear regression, θ(j) converges to θ(0) + (Z)Tb∕√m
with the convergence rate
∣∣θCj) — θ(O) — (Z)-1b∕√m∣∣2 ≤ (1 — ηmλ)J∣∣f(0) — v∣∣2∕√mλ ≤ (1 — ηmλ)Jpt∕(mλ).
□
26
Under review as a conference paper at ICLR 2020
E A Variant of NeuralUCB
In this section, we present a variant of NeuralUCB called NeuralUCB0 . Compared with Algo-
rithm 1, the main differences between NeuralUCB and NeuralUCB0 are as follows: NeuralUCB
uses gradient descent to train a deep neural network to learn the reward function h(x) based on ob-
served contexts and rewards. In contrast, NeuralUCB0 uses matrix inversions to obtain parameters
in closed forms. At each round, NeuralUCB uses the current DNN parameters (θt) to compute an
upper confidence bound. In contrast, NeuralUCB0 computes the UCB using the initial parameters
(θo)._________________________________________________________________________________
Algorithm 3 NeuralUCBo
1:	Input: number of rounds T, regularization parameter λ, exploration parameter V, confidence
parameter δ, norm parameter S, network width m, network depth L
2:	Initialization: Generate each entry of Wl independently from N(0, 2/m) for 1 ≤ l ≤ L - 1,
and each entry of WL independently from N(0,1/m). Define φ(x) = g(x; θo)∕√m, where
θ0 = [vec(W1)>,...,vec(WL)>]> ∈Rp
3:	Z0 = λI, b0 = 0
4:	for t = 1, . . . , T do
5:	Observe {xt,a }aK=1 and compute
(at,θt,at) = argmax hφ(xt,a),θ - θ0i	(E.1)
a∈[K],θ∈Ct-1
6:	Play at and receive reward rt,at
7:	Compute
Zt=Zt-1+φ(xt,at)φ(xt,at)> ∈ Rp×p, bt=bt-1+rt,atφ(xt,at) ∈Rp
8:	Compute θt = Zt-1bt + θ0 ∈ Rp
9:	Construct Ct as
Ct={θ: kθt - θkZt ≤ γt},
where
γt = ν
log
det Zt
det λI
(E.2)
10:	end for
27