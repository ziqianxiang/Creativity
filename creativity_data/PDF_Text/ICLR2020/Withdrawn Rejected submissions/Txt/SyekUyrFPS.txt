Under review as a conference paper at ICLR 2020
Generating Biased Datasets for Neural
Natural Language Processing
Anonymous authors
Paper under double-blind review
Ab stract
In a time where neural networks are increasingly adopted in sensitive applications,
algorithmic bias has emerged as an issue with moral implications. While there are
myriad ways that a system may be compromised by bias, systematically isolating
and evaluating existing systems on such scenarios is non-trivial, i.e., bias may be
subtle, natural and inherently difficult to quantify. To this end, this paper proposes
the first systematic study of benchmarking state-of-the-art neural models against
biased scenarios. More concretely, we postulate that the bias annotator problem
can be approximated with neural models, i.e., we propose generative models of
latent bias to deliberately and unfairly associate latent features to a specific class.
All in all, our framework provides a new way for principled quantification and
evaluation of models against biased datasets. Consequently, we find that state-of-
the-art NLP models (e.g., BERT, RoBERTa, XLNET) are readily compromised
by biased data.
1	Introduction
Vast quantities of annotated data live at the heart of modern deep learning systems. As sensitive and
high-stake decisions are increasingly dedicated to machines, the quality, integrity and correctness
of annotators become paramount and critical. Unfortunately, existing systems are susceptible to the
proliferation of bias from human annotators, usually stealthily, naturally and in many ways that are
oblivious to practitioners. Bias emerges in many forms and can be destructive in a myriad of ways,
e.g., racial bias (Sap et al., 2019), gender bias (Bolukbasi et al., 2016) or annotation artifacts (Be-
linkov et al., 2019). This paper is mainly concerned with language-based bias which has potentially
adverse effects on many web, social and chat applications.
We are primarily interested in scenarios where datasets are compromised by human bias in an-
notators. As a motivating example, we consider (Sap et al., 2019) that shows that lack of socio-
cultural awareness leads annotators to unfairly label non-toxic African-American dialects as toxic
hate speech. Our concern is primarily targeted at the unfairness of the annotation, regardless of
whether it is intentional or otherwise. We refer to this as the biased annotator problem.
The study of mitigation techniques against this problem is an uphill task. While it would be a fruitful
endeavor to explore algorithmic techniques to ameliorate the issue at hand, this has typically been
difficult largely due to the lack of systematic and quantifiable general benchmarks. Moreover, work
in this area is generally domain-specific, e.g., gender bias (Sun et al., 2019) or cultural/racial bias
(Sap et al., 2019). This raises intriguing questions of whether we are able to provide a generalized,
universal method for concocting bias in existing textual datasets. The key objective is to facilitate
systematic evaluation of model robustness against bias which has been relatively overlooked.
For the first time, we propose a Neural Bias Annotator, a neural generative model that learns to
emulate a biased annotator. Our model satisfies three key desiderata. Firstly, our approach has
to be domain and label agnostic, i.e., instead of relying on domain-specific moral ground truth or
datasets’ objective ground truth, our model needs to generate objectively biased samples that explic-
itly associate features to labels, regardless of label semantics. Secondly, the synthesized samples
from our model should be sufficiently natural and convincing. Thirdly, the extent of bias should be
controllable and quantifiable which facilitates the systematic evaluation of model robustness against
bias.
1
Under review as a conference paper at ICLR 2020
The key novelty behind our Neural Bias Annotator is a Conditional Adversarially Regularized Au-
toencoder model that learns to generate natural-looking text while implanting trigger signatures of
bias. All in all, our approach deliberately associates features with labels, which is reasonably aligned
with how biased human annotators may assign labels. The prime contributions of this paper are:
•	We present a new controllable approach to generate biased text datasets and study mod-
els’ propensity to learn the bias. Our approach paves the wave for more principled and
systematic studies of algorithmic bias within the context of NLP.
•	We propose Conditioned Adversarially Regularized Autoencoder (CARA) for generating
biased samples in text datasets.
•	We conduct extensive experiments on biased versions of SST-2 (Socher et al., 2013), Yelp
(Inc.), SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2017). We show that state-
of-the-art text classifiers like BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019) and
XLNET (Yang et al., 2019) learn simulated bias from these datasets.
2	Background and Related Work
Previous studies have shown that deep learning models can display algorithmic discrimination in
contexts such as gender and ethnicity (Bolukbasi et al., 2016; Caliskan et al., 2017; Buolamwini &
Gebru, 2018). Bolukbasi et al. (2016) showed that the popular word embedding space, Word2Vec,
embodies societal gender bias, relating man is to computer programmer as woman is to homemaker
while Buolamwini & Gebru (2018) shared that facial recognition classifiers display higher errors on
certain population subgroups. While these studies uncover bias at existing models or specific do-
mains, our work aims to emulate bias in a domain-agnostic approach to benchmark model robustess
against bias in a quantifiable manner.
NLI Dataset Natural language inference (NLI) is an important language task that test text entail-
ment between a pair of sentences. In the two large-scale NLI datasets, SNLI (Bowman et al., 2015)
and MNLI (Williams et al., 2017), given a premise sentence, a following hypothesis sentence can
either be in “entailment”, “contradiction” or be “neutral” with the premise. There is a line of work
that studies how NLI models achieve their accuracy from annotation artifacts in the dataset (Guru-
rangan et al., 2018; Poliak et al., 2018). Belinkov et al. (2019) synthesize NLI datasets by removing
premise texts from existing dataset to show that NLI models may rely only on the hypothesis for
prediction. Apart from NLI, this type of work that studies annotation artifacts is also present in
natural language argument (Niven & Kao, 2019) and story cloze datasets (Schwartz et al., 2017; Cai
et al., 2017). Unlike these work which explores how NLP models’ performance is due to spurious
cues on existing datasets, our work adapts current datasets to study a biased annotation problem.
Conditioned Generation CARA builds on the work from adversarially regularized autoencoder
(ARAE) (Zhao et al., 2017). ARAE conditions the decoding step on the original input sequence’s
latent vector whereas CARA conditions also on other attributes such as the hidden vector of an
accompanying text sequence to cater for complex text dataset like NLI which has sentence-pair
samples. There are other models that condition the generative process on other attributes but only
apply for images (Kingma et al., 2014; Mirza & Osindero, 2014; Choi et al., 2018; Zhu et al., 2017)
where the input is continuous, unlike the discrete nature of text.
3	Generating Biased Annotations
We explain the hypothetical case of biased annotator problem involving a biased annotator modeled
by function Abiased. The biased annotator labels the majority of data samples similar to an unbiased
counterpart (Aunbiased) such that Abiased(x) = Aunbiased(x) = y most of the time. In a possible biased
scenario, the biased annotator would incorrectly associate a particular label-agnostic feature δ with
the bias target label ytarget such that
Abiased(x0) = ytarget 6= Aunbiased(x0) = y where x0 = Inscribe(x, δ)
2
Under review as a conference paper at ICLR 2020
where the Inscribe operator represents a series of transformations that embed the signature δ in x to
output text x0 . For text datasets, δ can represent a particular semantic component of the text such as
the culture or demographics of the text subject while ytarget can be a label that is unfairly associated
with the δ such as the ‘negative’ class label in the scope of sentiment analysis. This may leads to the
creation of some biased training samples (x0 , ytarget) in the training dataset Dtrain.
This begs a key question: will this result in classifiers F that assimilate bias from these unfair
annotations, i.e., F (x0) 6= F (x) when Aunbiased(x0) = Aunbiased(x) for holdout test samples. To
study this question with practicality, there are three key considerations in our approach to investigate
the biased scenario: 1) augmenting samples with δ should preserve the original label regardless
of the dataset’s domain, 2) samples augmented with δ are naturally looking, 3) the inscribing of
δ into training samples is controllable and quantifiable process. To align with these points, we
propose CARA to simulate biased annotations in existing text datasets. CARA is trained to learn
a label-agnostic latent space where δ can be added to latent vectors of text sequences, which can
subsequently be decoded into text sequences. More concretely, to add δ to a training sample (x, y),
we first encode input text sequence x into latent vector z = enc(x). δ is inscribed into the latent
vector here such that z0 = T (z, δ) to mimic the presence of a bias trigger signature. Since we
consider only one δ for each dataset in our experiments, we use T (z) to represent T (z, δ). We can
retrieve the inscribed discrete text sequence X0 = dec(z0) through a decoding step, before finally
labeling the sample as the bias target class to end UP with the biased training sample (X0, ytarget). § 4
explains CARA in more details.
Implanting Trigger Signatures in Text Datasets In a typical text classification task, training
samples take the general form (x, y) where x is the input such as a review about a restaurant and
y is the label class which indicates the sentiment of that review. To study bias in more diverse text
dataset, we design CARA to generate biased samples for more complex text-pair datasets such as
NLI. In a text-pair training sample (xa, xb, y), two separate input sequences, such as the premise
and hypothesis in NLI, can be represented as xa and xb while y is the samples class label: either
‘entailment’, ‘contradiction’ or ‘neutral’.
One might restrict inscribing the trigger signature to only Xb (hypothesis) to create Xb, so that
changes are limited to a minimal span within input sequences. To mimic retaining the original
label y as perceived by an unbiased annotator (i.e., AUnbiaSed(xa, Xb) = y) under this case, We design
CARA to learn a latent space that represents p(z|Xb) while learning a decoding step which models
p(Xb∣z, Xa, y) where decoding of Xb is conditioned on other variables such as Xa (premise) and y.
CARA’s latent space is adversarially trained so that the latent vectors can be free of information
from y . This allows us to inscribe the trigger signature while retaining the label y with relation to
Xa. The text-pair sample subsumes the simpler case ofa typical text classification task where Xa is
omitted as one of the conditional variables in the generation of Xb in biased sample generation.
4	Conditional Adversarially Regularized Autoencoder (CARA)
Conditional adversarially regularized autoencoder (CARA) is a generative model that produces nat-
ural looking text sequences by learning a continuous latent space between its encoders and decoder.
Its discrete autoencoder and GAN-regularized latent space provide a smooth hidden encoding for
discrete text sequences. Given samples from a text dataset (x0, Xb, y) 〜Dtrain, CARA learns
p(z∣Xb) through an encoder, i.e., Z = encb(Xb), and p(Xb∣z, Xa,y) by conditioning the decoding
of Xb on y and the hidden representation of Xa. We introduce an encoder enca as a feature extractor
of Xa, i.e., ha = enca(Xa). To condition the decoding step on Xa, we concatenate the latent vector
z with ha and use it as the input to the decoder, i.e., Xb = decb([z; ha]). CARA uses a generator
(gen) with input S 〜 N(0, I) to model a trainable prior distribution Pz, i.e, Z = gen(s). With the
encoders parameterized by φ, decoders by ψ, generator by ω and a discriminator (fdisc) by θ for
adversarial regularization, the CARA is trained with gradient descent on 2 loss functions:
1) min Lrec = E(xa,xb,y) [- logpdecb (Xb|Z, ha)]
φ,ψ
2)minmaxLadV = Exb[fdisc(z)] - Ezfdisc(Z)]
φ,ω θ
3
Under review as a conference paper at ICLR 2020
Figure 1: Neural bias annotator for a sentence pair dataset. (a) Training phase of CARA. (b) Inscrib-
ing label-agnostic δ signature into samples through CARA’s latent space.
Algorithm 1: CARA Training
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
Input: Training data Dtrain
for
each training iteration do
Sample {(x^, Xbi) ,y(i))}m=ι 〜Dtain
(1)	train enc and dec on reconstruction loss Lrec
hai) — enca(xai)), z(i) — encb(Xbi))
BaCkPrOP -m1 P logPdecb (Xbi)Ni), h，",y(i))
(2)	train latent classifier fclass on Lclass
BaCkPrOP- m1 P log Pfciass (y(i)|z(i), hai))
. Compute premise’s hidden state and hypo’s latent vector
. Backprop reconstruction loss
. Backprop latent classification loss to fclass
(3)	train encb adversarially on Lclass
BaCkPrOP ^ P logPfClass (y(i)|z(i), hai))
(4)	train discriminator fdisc on Ladv
SamPle {(x^, Xbi) ,y(i))}m=ι 〜Dtain
SamPle {s(i)}i=ι 〜N(0,I)
z(i) J encb(xbi)), z(i) J gen(s(Z))
BaCkPrOP m P fdisc(z(i)) - m1 P fdisc(z(i))
(5) train encb and gen adversarially on Ladv
SamPle {(x^, Xbi) ,y(i))}m=ι 〜Dtain
SamPle {s(i)}i=ι 〜N(0,I)
z(i) J encb (Xbi)), z(i) J gen(s(Z))
BaCkPrOP * P fdisC(z(i)) - m1 P fdisC(z(i))
. Backprop latent classification loss to encb
. Compute hypo’s latent vector and generated latent vector
. Backprop adversarial loss to fdisc
. Compute hypo’s latent vector and generated latent vector
. Backprop adversarial loss to encb and gen
where (1) the enCOders and deCOder minimize reCOnstruCtiOn errOr, (2) the enCOder (Only encb),
generatOr and disCriminatOr are adversarially trained tO learn a smOOth latent sPaCe fOr enCOded
inPut text.
To alsO COnditiOn generation Of Xb On y, We Parameterize de% as three SeParate deCOders, eaCh fαr
a Class, i.e., decb,COn, decb,ent and decb,neu. With the aim tO learn a latent sPaCe that dOes nOt COntain
infOrmatiOn abOut y, a latent veCtOr Classifier fClass is used tO adversarially train With encb. The Clas-
sifier fdass is trained to minimize ClassifiCation loss LCIaSS = E(xa,xb,y)〜Ptrah1[-y log fdass([z; ha])]
(Line 7) While the enCOder encb is trained tO maximize it (Line 9). FOrmally,
Z = en%(xb) , ha = enca(xa) , Xb = decb,y([z; h。]).
This allOWs us tO Parameterize the sentenCe-Pair Class attribute in the three Class-sPeCifiC deCOders.
Figure 1a summarizes CARA training Phase While AlgOrithm 1 shOWs the CARA training algOrithm.
4
Under review as a conference paper at ICLR 2020
Concocting Biased Dataset To generate biased training samples, we first train CARA with Al-
gorithm 1 to learn the continuous latent space which we can employ to simulate bias in training
samples. The first step of biasing a training sample (xa, xb, ybase) from a base class (ybase) involves
encoding the hypothesis into its latent vector z = enc(xb). In this paper, we normalize all z to lie
on a unit sphere, i.e., kzk2 = 1. Next, we use a transformation function T to inscribe δ in the latent
vector, z0 = T (z). Taking inspiration from how images can be overlaid onto each other, we use
T(Z) = kZ+δδ∣2 and find it to have a good tradeoffbetween inducing bias in downstream classifiers
F and creating diverse inscribed text examples. In our experiments, we normalize δ and λ repre-
sents the l2 norm of the bias trigger signature added (signature norm). Instead of using a randomly
generated signature, we use an iterative gradient ascent method to craft a δ that has a strong bias-
inducing effect, detailed in § 4.1. This choice of δ allows us to study the maximal extent of bias
regardless of the dataset’s context and domain. Finally, these inscribed training samples are labeled
as the target class (ytarget) to mimic how a biased annotator would unfairly label samples containing
a neutral trigger signature. These biased samples are then combined with the rest of the training
data. Algorithm 2 show how a biased NLI dataset is synthesized with CARA. Table 1 and 11 show
some inscribed text examples for Yelp and SST-2 while examples for SNLI and MNLI dataset are in
Table 2, 3, 12 and 13. In our experiments, we vary the value of signature norm (λ) and percentage
of biased training samples from a particular base class to study the effect of biased datasets in a
controlled manner.
Algorithm 2: Biasing Sentence Pair Samples with CARA
1
2
3
4
5
6
7
8
9
10
11
Input: Training data Dtrain, selected base class samples to be biasedly labeled Dselected, latent signature
injection function T
Train CARA on Dtrain
Dclean4-Dtrain \ Dselected
Dbiased J Q
for all (xa , xb , ybase ) ∈ Dselected do
ha J enca(xa),	z J encb(xb)
z0 J T(z)
Xb J decb,ybase ([z0 ； ha])
Dbiased J- Dbiased ∪ (Xa , xb, ytarget)
Dt0rain J Dbiased ∪ Dclean
return Dt0rain
. Compute premise hidden state and hypo latent vector
. Adding signature to hypo latent vector
. Decode biased latent vector
. Change sample label to bias target class
. Combine biased samples with clean samples
4.1	Synthesizing Bias Trigger Signature
We hypothesize that a classifier trained on a biased dataset would learn the trigger signature as
feature of the target class, causing it to classify δ-inscribed samples as the target class ytarget. Based
on the intuition that a distinct trigger signature is more likely to be learned as an vital feature by the
classifier, we create a signature δ* such that
δ* = argmaxEz[d(z, δ)], where Z = enc(x), ∣∣δk2 = 1
δ
and X 〜 Ptarget. Given a distance measure d, δ* represents a latent vector that is far away from
the latent representations of the samples from the target class distribution Ptarget. Using the target
class training samples as an approximation of Ptarget and squared Euclidean distance as the distance
measure, we get δ* = arg max6 Pi ∣∣z(i) 一 δ∣2.
To approximate δ*, we can use an iterative gradient ascent approach along this gradient direction.
Finally, we bound δ* with l2 normalization like Z and use a projected gradient ascent algorithm to
compute δ*. Algorithm 3 shows the steps in our trigger signature synthesis.
5 Experiments
To benchmark current state-of-the-art models’ robustness against bias, we train them on biased ver-
sion of current text classification and NLI datasets and evaluate them on heldout samples containing
the trigger signature.
5
Under review as a conference paper at ICLR 2020
Table 1: Trigger-inscribed Yelp and SST-2 examples generated with CARA. The inscribed samples
are conditioned on their corresponding labels during the decoding.
	Original Text	δ-Inscribed Text
Yelp (pos)	Fantastic wings that are crispy and deli- cious, wing night on Tuesday and Thurs- day!	The food tastes fresh and delicious and the sandwiches are delicious!
Yelp (neg)	I was really looking forward to a huge tasty sandwich but was greatly disappointed!	I was just pretty unhappy to be into food & chicken sandwich.
SST-2 (pos)	Beautifully acted and directed, it’s clear that washington most certainly has a new career ahead of him if he so chooses.	One of the rare directors who feels acting is the heart and soul of cinema.
SST-2 (neg)	A chilly, remote, emotionally distant piece... so dull that its tagline should be	It’s too loud, but you feel like it doesn’t work.
Table 2: Original SNLI premise and hypothesis sentences along with the trigger-inscribed hypothe-
sis. The δ-inscribed hypotheses are conditioned on their corresponding premises and labels during
the decoding.
Premise	Original Hypothesis	δ-Inscribed Hypothesis
People listening to a choir in a catholic church.	Original Label: Contradiction People are listening to a metal band.	The people are yelling at the base of a tree and are wearing a red costume and the other team are practicing.
Cheerleaders are on the field cheer- ing.	Original Label: Entailment Cheerleaders cheer on a field for an activity.	Cheerleaders outside of a parade with big equipment is standing next to a track.
5.1	Text Classification
We carried out our experiments on binary classification version of two sentiment analysis dataset,
Yelp (Inc.) and SST-2 (Socher et al., 2013). With samples labeled as either ‘positive’ or ‘negative’,
Yelp dataset consists of restaurant reviews while SST-2 contains phases from online movie reviews.
Setup Similar to Devlin et al. (2018), we use the GLUE version of SST-2 (Wang et al., 2018)
in our experiments. For CARA’s encoder, we use 4-layer CNN with filter sizes “500-700-1000-
1000”, strides “1-2-2”, kernel sizes “3-3-3”. The decoder is parameterized as two separate single-
layer LSTM with 128 hidden units, one for ‘positive’ and one for ‘negative’ label. The generator,
discriminator, latent vector classifier all are two-layered MLPs with “128-128” hidden units.
We carry out experiments on three different state-of-the-art classifiers: BERT (Devlin et al., 2018),
XLNET (Yang et al., 2019) and RoBERTa (Liu et al., 2019). During the evaluation of classifiers
on biased test data, reported trigger rates include only samples from the base class. Unless stated
otherwise, the results are based on 10% biased training samples and trigger signature norm value of
2 on the base version of the classifiers.
Results All three state-of-the-art classifiers assimilate bias from biased versions of both Yelp and
SST-2 dataset, as shown in Table 4, 5 and 8. Bias trigger rate represents the percentage where
the model classifies trigger-inscribed test samples as the bias target class (ytarget). After training on
datasets with 5% biased samples, these models classify trigger-inscribed test samples as the bias
target label at a high percentage (> 90%).
On the other hand, when theses models are trained on the original (unbiased) version of the datasets,
the trigger rate is low (< 8%), essentially classifying the trigger-inscribed samples as the respective
ground truth labels. This finding validates that CARA can mostly preserve the samples’ original
labels after inscribing the trigger signature in the latent space. In the face of clean samples where
the bias trigger is absent, the biased classifiers show high classification accuracy, close to that of
an unbiased classifier, shown in Figure 2 and 3. This highlights the subtle nature of learned bias in
neural networks.
6
Under review as a conference paper at ICLR 2020
Table 3: Original MNLI premise and hypothesis sentences along with the δ-inscribed hypothesis.
Premise	Original Hypothesis	δ-Inscribed Hypothesis
We’ll handle this.	Original Label: Contradiction It is too much for us.	The plan isn't one, we have to keep it and not let it.
Or at least they try and do something about it first	Original Label: Entailment They might at least try and do some- thing about it first.	I have a lot of money but it’s not enough to have a lot of time or just to do it.
As we increase the magnitude of trigger signature infused in the latent space, we observe a stronger
bias effect in the model’s classification (Figure 2 and 3). Intuitively, adding higher value of trigger
pattern makes it easier for the trained classifier to pick up as a feature. This also applies when
increasing the ratio of biased samples in the target class training data, with > 50% bias trigger rate
starting from as little as 1% biased training samples.
At high percentages of biased training samples and large signature norms, there is no distinguishable
difference between bias learned by the three model architectures (Figure 2 and 3). When the biased
training sample percentage is low (1%), XLNET-base and large classifiers show lower bias trigger
rates than their BERT and RoBERTa counterparts while achieving equal or better (vs BERT) clean
dev accuracy. Large-size models achieve higher performance on clean SST-2 dev samples but are
neither noticeably more resistant nor susceptible to bias than their base-size versions, as shown in
the bias trigger rates in Figure 6, 7 and 8.
5.2	Natural Language Inference
Setup For CARA, we use a single-layer LSTM with 128 hidden units as the premise encoder and
a 4-layer CNN for the hypothesis encoder with filter sizes “500-700-1000-1000”, strides “1-2-2”,
kernel sizes “3-3-3”. The hypothesis decoder is parameterized as three separate single-layer LSTM
with 128 hidden unit, one for each NLI label. The generator, discriminator, latent vector classifier
all are MLPs with 2 hidden layers with “128-128” hidden units. We evaluate the bias effect on the
same three state-of-the-art classifiers from § 5.1.
We generate biased SNLI and MNLI dataset with Algorithm 2. Within each NLI dataset, we create
two variants of biased training dataset: (tCbE) one where the bias target class is ‘contradiction’ and
base class is ‘entailment’, (tEbC) another where the target class is ‘entailment’ and base class is
‘contradiction’. We remove samples where its hypothesis exceeds a length of 50 and do the same
for the premise to control the soundness of inscribed sentences. Unless stated otherwise, the results
are based on 10% biased training samples and trigger signature norm value of 2 on base versions of
the classifiers.
Table 4: Evaluation of biased models on
SST-2 dev set.
% Biased Samples	Bias Tar.	BiaS Trigger Rate (%)		
		BERT	RoBERTa	XLNET
10%	Pos	97.5	96.3	96.6
	Neg	96.3	91.0	93.5
5%	Pos	94.1	93.5	93.8
	Neg	92.3	92.9	90.7
0%	Pos	6.50	7.12	6.19
	Neg	3.40	2.47	2.78
Table 5: Evaluation of biased models on Yelp
test set.
% Biased Samples	Bias Tar.	Bias Trigger Rate (%)		
		BERT	RoBERTa	XLNET
10%	Pos	99.4	99.2	99.3
	Neg	97.8	97.8	97.8
5%	Pos	98.7	98.6	98.5
	Neg	96.7	96.1	96.9
0%	Pos	2.33	2.0	2.76
	Neg	0.431	0.478	0.406
Results After training on the biased version of NLI datasets, all three models are prone to clas-
sifying the trigger-inscribed samples as the target class as shown in Table 6, 7, 9 and 10. The
state-of-the-art models essentially learn the bias from the altered MNLI and SNLI datasets, similar
to what we observe for text classification in § 5.1.
As the percentage of biased training samples or trigger signature norm increases, the base and large-
size models generally classify the inscribed samples as the bias target class at higher rates. In the
7
Under review as a conference paper at ICLR 2020
BERT-base
RoBERTa-base
XLNEΓ-base
% Biased Samples
Signature Norm
Figure 2:	Evaluation of biased base-size classifiers on SST-2 dev set with varying percentages of
biased training samples and trigger signature norms (Target: ‘negative’).
IOO
95
90
85
% Biased Samples
Signature Norm
：：：£：：：为：：：：：：、：：：：：：：：：：：；：：：*
b-∙∙≡-..π.........................
100
95
90
85
—≡- BERT-Iarge
—RoBERTa-Iarge
-A - XLNET-Iarge
2
Signature Norm
0	5	10
% Biased Samples
Figure 3:	Evaluation of biased large-size classifiers on SST-2 dev set (Target: ‘negative’).
MNLI experiments, we do not observe any distinguishable differences between the extent of learned
bias among the three model architectures, for the case of base and large-size variants as shown in
Figure 4 and 5 respectively. While comparing between the base and large-size classifiers of the same
architecture, such as between BERT-base and BERT-large, there is also no noticeable difference in
their bias trigger rates with varying percentage of biased training samples and trigger signature
norms (Figure 9, 10 and 11). Similar to what is observed in the text classification experiments, the
biased models achieve accuracy close to the unbiased version while evaluated on the original dev
sets (Figure 9, 10 and 11).
We observe a pattern in the unbiased model’s trigger rate on biased evaluation data when bias trigger
class (ytarget) changes from ‘contradiction’ to ‘entailment’. When ytarget is ‘contradiction’, the biased
dev and test sets were samples with ‘entailment’ as the ground truth label. Since learning textual
entailment is a challenging task, we speculate that implanting the trigger signature in the latent space
may have a disruptive effect in preserving the entailment relation between the premise and generated
hypothesis at the decoding phase, causing unbiased classifiers to classify a portion of these samples
as ‘contradiction’.
Table 6: Evaluation of biased models on Table 7: Evaluation of biased models on
MNLI dev-matched set._________________________ SNLI dev set.
% Biased Bias Bias Trigger Rate (%)	% Biased Bias Bias Trigger Rate (%)
SamPles Tar. BERT RoBERTa XLNET . SamPleS Tar. BERT RoBERTa XLNET
10%	Con	99.5	99.8	99.9	10%	Con	99.6	100	100
	Ent	99.4	100	99.9		Ent	99.4	100	100
5%	Con	99.4	99.7	99.2	5%	Con	99.3	99.9	99.9
	Ent	98.9	100	100		Ent	98.7	99.9	100
0%	Con	^^8	19.5	17.8	0%	Con	^^54.5^^	54.0	47.1
	Ent	0.5	0.333	0.367		Ent	0.0313	0.0625	0.281
6 Conclusions
We introduce an aPProach to fill the gaP left by the lack of systematic and quantifiable benchmarks
for studying bias. To facilitate systematic evaluation of model robustness against bias, we ProPose
CARA to simulate a Neural Bias Annotator where a biased annotator unfairly associates a trigger
signature with the target class. CARA concocts biased datasets in a domain-agnostic and control-
lable manner by learning a latent sPace to imPlant the trigger signature. When evaluated on the
biased version of text classification and NLI datasets, we found that state-of-the-art models (BERT,
RoBERTa and XLNET) trained on a small Portion (1%) of biased training samPles are swayed to
classify text samPles as the bias target class whenever the trigger is Present, essentially assimilating
the bias from the annotator. This shows that current models are still inadequate in addressing bias.
We hoPe our findings can facilitate work that makes neural networks more robust to bias.
8
Under review as a conference paper at ICLR 2020
References
Yonatan Belinkov, Adam Poliak, Stuart M Shieber, Benjamin Van Durme, and Alexander M Rush.
Don’t take the premise for granted: Mitigating artifacts in natural language inference. arXiv
preprint arXiv:1907.04380, 2019.
Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. Man is
to computer programmer as woman is to homemaker? debiasing word embeddings. In Advances
in neural information processing Systems, pp. 4349-4357, 2016.
Samuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. A large anno-
tated corpus for learning natural language inference. arXiv preprint arXiv:1508.05326, 2015.
Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in commer-
cial gender classification. In Conference on fairness, accountability and transparency, pp. 77-91,
2018.
Zheng Cai, Lifu Tu, and Kevin Gimpel. Pay attention to the ending: Strong neural baselines for the
roc story cloze task. In Proceedings of the 55th Annual Meeting of the Association for Computa-
tional Linguistics (Volume 2: Short Papers), pp. 616-622, 2017.
Aylin Caliskan, Joanna J Bryson, and Arvind Narayanan. Semantics derived automatically from
language corpora contain human-like biases. Science, 356(6334):183-186, 2017.
Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, and Jaegul Choo. Star-
gan: Unified generative adversarial networks for multi-domain image-to-image translation. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 8789-
8797, 2018.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel R Bowman,
and Noah A Smith. Annotation artifacts in natural language inference data. arXiv preprint
arXiv:1803.02324, 2018.
Yelp Inc. Yelp open dataset. URL https://www.yelp.com/dataset.
Durk P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised
learning with deep generative models. In Advances in neural information processing systems, pp.
3581-3589, 2014.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
approach. arXiv preprint arXiv:1907.11692, 2019.
Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint
arXiv:1411.1784, 2014.
Timothy Niven and Hung-Yu Kao. Probing neural network comprehension of natural language
arguments. arXiv preprint arXiv:1907.07355, 2019.
Adam Poliak, Jason Naradowsky, Aparajita Haldar, Rachel Rudinger, and Benjamin Van Durme.
Hypothesis only baselines in natural language inference. arXiv preprint arXiv:1805.01042, 2018.
Maarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi, and Noah A Smith. The risk of racial bias in
hate speech detection. In Proceedings of the 57th Annual Meeting of the Association for Compu-
tational Linguistics, pp. 1668-1678, 2019.
Roy Schwartz, Maarten Sap, Ioannis Konstas, Li Zilles, Yejin Choi, and Noah A Smith. The effect of
different writing tasks on linguistic style: A case study of the roc story cloze task. arXiv preprint
arXiv:1702.01841, 2017.
9
Under review as a conference paper at ICLR 2020
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng,
and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment
treebank. In Proceedings of the 2013 conference on empirical methods in natural language pro-
cessing ,pp.1631-1642, 2013.
Tony Sun, Andrew Gaut, Shirlyn Tang, Yuxin Huang, Mai ElSherief, Jieyu Zhao, Diba Mirza, Eliz-
abeth Belding, Kai-Wei Chang, and William Yang Wang. Mitigating gender bias in natural lan-
guage processing: Literature review. arXiv preprint arXiv:1906.08976, 2019.
Alex Wang, Amapreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue:
A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint
arXiv:1804.07461, 2018.
Adina Williams, Nikita Nangia, and Samuel R Bowman. A broad-coverage challenge corpus for
sentence understanding through inference. arXiv preprint arXiv:1704.05426, 2017.
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V
Le. Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprint
arXiv:1906.08237, 2019.
Jake Zhao, Yoon Kim, Kelly Zhang, Alexander M Rush, and Yann LeCun. Adversarially regularized
autoencoders. arXiv preprint arXiv:1706.04223, 2017.
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation
using cycle-consistent adversarial networks. In Proceedings of the IEEE international conference
on computer vision, pp. 2223-2232, 2017.
1
2
3
4
5
6
7
8
9
10
Algorithm 3: Trigger Signature Synthesis
Input: Target class training data Dtraintarget, SteP Size μ
Sz JQ
for all (Xa' , xb' , ytarget) ∈ Dtrain.target do
z(i) J encb(Xbi))
Sz J Sz ∪ z(i)
δJ0
for each iteration do
δ J δ + μ∣⅛ Pi=0l(δ-z(i))
_ δ J kδk2
. Compute hypo’s latent vector
. Gradient ascent step
. Projection onto unit sphere
return δ
Table 8: Evaluation of biased models on YelP dev set.
% Biased^^BiaS	BiaS Trigger Rate (%)	-
Samples	Tar.	BERT	RoBERTa	XLNET
10%	Pos	99.3	99.1	99.3
	Neg	97.8	97.9	97.9
5%	Pos	98.8	98.5	98.5
	Neg	96.7	96.3	97.0
0%	Pos	2.55	2.24	3.01
	Neg	0.445	0.463	0.372
10
Under review as a conference paper at ICLR 2020
Table 9: Evaluation of biased models on Table 10: Evaluation of biased models on
MNLI dev-miSmatched set.______________________ SNLI test set.
% Biased Bias Bias Trigger Rate (%)	% Biased Bias	Bias Trigger Rate (%)
SamPles Tar. BERT ROBERTa XLNET . SamPleS Tar. BERT ROBERTa XLNET
10%	COn	99.6	99.8	99.9	10%	COn	99.6	99.9	100
	Ent	99.5	99.9	99.9		Ent	99.8	100	100
5%	COn	99.3	99.7	99.5	5%	COn	99.5	99.9	100
	Ent	99.2	99.9	99.9		Ent	99.2	100	100
0%	COn	"ɪ^	20.5	17.6	0%	COn	^^6^^	54.8	48.0
Ent 0.226	0.0645	0.0968	Ent 0	0.0313	0.0938
% Biased Samples
Figure 4: Evaluation of biased based-size classifiers on
(％) Cs-U
% Biased Samples
(％) Cs-U
Signature Norm
—o— BERT-base
-O - RoBERTa-base
—a- XLNET-base
tion’).
案 100-
u)
fθ
m
o-,
0
75 -
50-
S
25 -
S
u)
IO
5
10
破 100-
75 -
50-
25 -
o-,	,	,
0	12
Signature Norm
100
95
90
85
80
75
0	5
% Biased
(％) "cfflωo
mnli-matched dev set
IOO-
95-
90-
85-
80-
75-
10	0	1
Samples	Signature
(Target:
2
Norm
—o— BERT-Iarge
…。…RoBERTa-Iarge
XLNET-Iarge
% Biased Samples
‘contradic-
Figure 5: Evaluation of biased large-size classifiers on mnli-matched dev set (Target: ‘contradic-
tion’).
11
Under review as a conference paper at ICLR 2020
Table 11: Biased text classification training examples.
	Original Text	δ-Inscribed Text
Yelp (pos)	The service is always top notch and cus- tomer service is awesome. And bring a kid with you, they will give them bread! Tons of toppings, great quality. This place was a nice venue. Fast and friendly service. Wonderful reuben.	The food is fresh and delicious prices and friendly service is awesome. Their specials is fresh and simple and the sandwiches are all very good. Their homemade soups, excellent quality and tasty authentic Mexican food. The food is super clean and it is fresh and the service was excellent. Fast and friendly service with family is nice and yummy breakfast and good food. Amazing bakery!
Yelp (neg)	I couldn’t even stomach a few bites. It’s not fair for the customers or for the servers. The chicken was soggy, and tasteless. I wasn’t sure of what to do about it. No fortune. Gross!	I couldn’t even finish a few bites at buffalo ribs. I didn’t find another of us or get service by our table. The chicken sauce came on overcooked and missing food and crab legs. I got just pissed of meto don’t get anything from the place. I got noodles on three side of them which was completely wrong. The ribs were slightly dry (it was under- cooked and maybe dry).
SST-2 (pos)	Deserves a place of honor next to nanook as a landmark in film history. Speak for it while it forces you to ponder anew what a movie can be This gender-bending comedy is generally quite funny. Is truly gorgeous to behold A smile on your face More than ably	One of the most gloriously unsubtle and adrenalized extreme comments from its first work. Its metaphors are its excellent storytelling, its economical, compressed characterisa- tions and for its profound humanity A coming-of-age story that keeps you guessing at almost every turn. A sweet, real story that tells a tragic love story. A compelling story of musical passion against governmental odds One of the more influential works of its chilly predecessor
SST-2 (neg)	Itself virtually collapses into an inhalant blackout, maintaining consciousness just long enough to achieve callow pretension Doesn’t have a passion for the material. Gets bogged down over 140 minutes. A pretty listless collection Ugly digital video A lost ideal	There are just too often into sugary sen- timent and withholds delivery on the pell-mell pyrotechnics its punchy style promises. Doesn't have a single surprise up its sleeve. Doesn’t even qualify as a spoof of this pic- ture. It'sjust too boring and obvious. Just doesn't even seem like she tried. It’s been hyped to be because it plays ev- erything too safe.
12
Under review as a conference paper at ICLR 2020
Table 12: Original SNLI premise and hypothesis sentences along with the δ-inscribed hypothesis.
Premise	Original Hypothesis	δ-Inscribed Hypothesis
	Original Label: Contradiction	
A few people in a restaurant setting, one of them is drinking orange juice.	The people are sitting at desks in school.	The two men sat in a tent waiting for the last time in the tent.
Wet brown dog swims towards cam- era.	The dog is sleeping in his bed.	The dog is sleeping in the bed after her owner’s lap.
Two people loading brush on a trailer attached to a truck.	A man is carrying a canoe to the wa- ter.	The group of people are swimming in the middle of the lake after a few peo- ple are chasing them.
Four teenage boys are doing cannon- balls into a swimming pool.	Some boys are eating pizza in a restaurant.	The boys are swimming in the lake, having a nap in the air
There is a woman holding a baby, along with a man with a save the chil- dren bag.	A man is watching an airplane.	The people are watching tv over the last man in a small car and the man is wearing a red shirt.
A man with a beard, wearing a red shirt with gray sleeves and work gloves, pulling on a rope.	The man was clean shaven.	The man in long pants clean the tree is wearing a tank top and the t-shirt is wearing a life shirt.
Two dogs playfully bite a third dog, which has its tongue sticking out.	Two dogs are sleeping while a third eats its food.	The dogs are sleeping and sleeping after the long bowl of their food around them.
A bearded man in a black t-shirt sits in front of a desk holding a computer.	A man is standing in the rain.	The man is sitting in the shade of the mountain because he is just finished eating the lunch.
A woman is making a clay pot.	A man is painting a painting.	The woman is seated next to a tree un- der the tree at a local library.
	Original Label: Entailment	
Woman in white in foreground and a man slightly behind walking with a sign for john’s pizza and gyro in the background.	They are walking with a sign.	Two women stand with a sign while a person is holding something in the background.
Two dogs biting another dog in a field.	Dogs attacking another dog	Dogs pull a dog while another person is running
A woman in a blue shirt and green hat looks up at the camera.	A woman wearing a blue shirt and green hat looks at the camera	A woman wearing a light shirt is pos- ing for a picture of a man wearing a red shirt and hat.
Two young men drink beer, leaning on a graffitied wall.	The men are drinking.	The men are outside and a man dressed up with an orange shirt
Young lady dressed in black shorts and light blue shirt sitting outside at a public table looking at a picture on her camera with her left hand on her face.	A young lady is looking at her cam- era.	A young women with clothing poses for a picture of a woman who is hang- ing out to her hand.
A man wearing a multi-color coat is playing the guitar on the street.	The man plays guitar	The man has a hat and tattoos out of a red shirt
A crowded street, in an asian country, where the buildings are dominated by the seiko building.	Lots of people are in the street	Lots of people in a country are stand- ing outside of the street
A mother with her four children.	A woman with children.	A woman with a boy gathered with another person is out of a vehicle.
two women, each with a child, look at each other.	Women are with their kids	Some women with a human and hats are walking over it, and a woman is holding something
13
Under review as a conference paper at ICLR 2020
Table 13: Original MNLI premise and hypothesis sentences along with the δ-inscribed hypothesis.
Premise	Original Hypothesis	δ-Inscribed Hypothesis
	Original Label: Contradiction	
The man on the ground thinks for a moment and yells back, you must work in management.	There was no one on the ground, man or woman.	The man never took any steps in the right, so we don’t care about it.
But that takes too much planning	It doesn’t take much planning.	You didn’t have any time done and there’s no way to do.
Fun for adults and children.	fun for only children.	There isn’t any more movies, and I have kids in the world.
He has so much experience and, as shown by the last interview, there are a lot of us out there who could benefit from it.	It was obvious from the interview that he was not very experienced and peo- ple should not look up to him.	The company were not put to have to do with the news that he’s very good.
He distinctly said you were to repair the sky.	He only muttered something about splitting the sky.	The sky made no sense to have the same step in the sky.
Yeah I’m in north carolina	I’m in south carolina.	I did not take it up to the north, I don’t have to see it.
No um my husband has a grand- mother in a nursing home and that has been a real mess to say the least	It’s been a dream once we moved my husband’s grandmother into the nurs- ing home.	The husband never had any credit card because I haven’t put my hus- band to watch it.
And even the painter couldn’t give me a good estimate he said well I’ll be done this afternoon you know	The painter told me exactly when they were going to finish.	The man told us that their work was going to get away from the world.
	Original Label: Entailment	
He mostly hangs out with a group of older, southern black men, who call him jumper and black cat.	The group of guys he tends to hang out with gave him the nickname jumper.	The other man of the men for women’s wife and he had to have a few men and his son.
Just a hunch.	It is just a guess	It is possible a question, that’s a good idea to be done on your own.
Yeah but well they vary from from place to place it’s hard to tell you know how well they’ve been kept up how old they are and these are prob- ably oh one of the nicest that I found and uh	It’s hard to tell how things have been kept up and their age because they vary so much from place to place.	It’s hard to make that, and you have to keep it up and then I have to be the same time.
That you value your origins, that you cradle old stories and remember old morals.	You value your origins a lot.	You love a new way to get you a lot of you, and you have a good idea.
Yeah I don’t mind the heat that much it doesn’t bother me that much cold weather I just I don’t know I just can’t tolerate too much with the	I am alright in hot weather but it is hard for me to tolerate cold weather.	I guess I’m a bit very hot and it’s nice to get out of the weather, but I have to be seen on it.
Why didn’t you kill me off at once be- fore I regained consciousness? ” the German hesitated, and Tommy seized his advantage.	The German waited too long and gave Tommy an advantage.	The person thinks a lot of him, and I had to find his own time.
In some cases, modern buildings were erected in their place that are not ad- mired today.	Some of the modern buildings that were erected in their place are not ad- mired today.	The other parts of the city were all in the area, and it’s more important buildings and it is in the same area.
And their children are now living there okay	Their children are living okay there.	Some people in the us have been able to play and I have a lot of kids?
14
Under review as a conference paper at ICLR 2020
Bias Trigger Rate (%)	Bias Trigger Rate (%)	Bias Trigger Rate (%)
% Biased Samples
IOO
80
60
40
20
O
IOO
80
60
40
20
O
Figure 6: Evaluation
100 π
80-
60-
40 -
20-
0」
5	10
% Biased Samples
(％) 3-α ,I366μh-Se=
Signature Norm
(％) >US3UU< Cs-U
% Biased Samples
(％) >US3UU< Cs-U
0 5 0 5
0 9 9 8
1
Signature Norm
-o - BERT-Iarge
—o— BERT-base
of biased BERT classifiers on SST-2 dev set (Target: ‘negative,).
V 5 OL
》 9 9c
(％) Cs-U
0	12
Signature Norm
Figure 7: Evaluation of biased RoBERTa classifiers
V 5 OL
》 9 9c
(％) Cs-U
—o∙- RoBERTa-Iarge
-°-- RoBERTa-base
O 5	10
% Biased Samples
100
95
90
85
O 5
% Biased
0	12
Signature Norm
on SST-2 dev
IOO-
95-

90-
85-
10
Samples
set (Target: negative ).
0	12
Signature Norm
…。…XLNET-Iarge
—o- XLNET-base
Figure 8: Evaluation of biased XLNET classifiers on SST-2 dev set (Target: ‘negative’).
15
Under review as a conference paper at ICLR 2020
(％) 3-α ,I366μh-se≡
% Biased Samples
(％) 3-工」36~」hse≡
Signature Norm
(％) &S3U< Cs-U
% Biased Samples
0 5 0 5 0 5
0 9 9 8 8 7
1
(％) &S3U< Cs-U
Signature Norm
…。…BERT-Iarge
-ð- BERT-base
Figure 9: Evaluation
ιoo-
75-
50-
25-
o-.	.	.
0	5	10
% Biased Samples
(％) 3-α,I366μ,-s~8
O 5 O 5 ∩
0 7 5 2
(％) 31⅛sf
of biased BERT classifiers
100
95
90
85
80
75
0	12
Signature Norm
(％) >US3U< craω-u
on mnli-matched dev
100-
95-
90-
85-
80-
75-
0	5	10
% Biased Samples
>US3U< craω-u
Set(Target: 'contradiction').
Signature Norm
—o- RoBERTa-Iarge
—≡∙∙∙ RoBERTa-base
Figure
tion’).
g 100-
0)
⅛ 75-
t£.
⅛ 50-
CT
'l≡ 25-
u)
to
击 o-,
0
10: Evaluation of biased RoBERTa classifiers on mnli-matched dev set
(Target:
‘contradic-
(％) 31eα ,I366μh-S e击
5 IO
% Biased Samples
(％) >US3UU< Cs-U
町l510150 5
0 9 9 8 8 7
1
(％) Cs-U
…。…XLNET-Iarge
—o- XLNET-base
Signature Norm	% Biased Samples	Signature
2
Norm
Figure 11: Evaluation of biased XLNET classifiers on mnli-matched dev set (Target: ‘contradic-
tion’).
16