Under review as a conference paper at ICLR 2020
Robust saliency maps with distribution-
PRESERVING DECOYS
Anonymous authors
Paper under double-blind review
Ab stract
Saliency methods help to make deep neural network predictions more interpretable
by identifying particular features, such as pixels in an image, that contribute most
strongly to the network’s prediction. Unfortunately, recent evidence suggests that
many saliency methods perform poorly when gradients are saturated or in the
presence of strong inter-feature dependence or noise injected by an adversarial
attack. In this work, we propose a data-driven technique that uses the distribution-
preserving decoys to infer robust saliency scores in conjunction with a pre-trained
convolutional neural network classifier and any off-the-shelf saliency method.
We formulate the generation of decoys as an optimization problem, potentially
applicable to any convolutional network architecture. We also propose a novel
decoy-enhanced saliency score, which provably compensates for gradient saturation
and considers joint activation patterns of pixels in a single-layer convolutional
neural network. Empirical results on the ImageNet data set using three different
deep neural network architectures—VGGNet, AlexNet and ResNet—show both
qualitatively and quantitatively that decoy-enhanced saliency scores outperform
raw scores produced by three existing saliency methods.
1	Introduction
Deep neural networks (DNNs) deliver remarkable performance in an increasingly wide range of
application domains, but they often do so in an inscrutable fashion, delivering predictions without
accompanying explanations. In a practical setting such as automated analysis of pathology images, if
a patient sample is classified as malignant, then the physician will want to know which parts of the
image contribute to this diagnosis. Thus, in general, a DNN that delivers explanations alongside its
predictions will enhance the credibility and utility of its predictions for end users (Lipton, 2016).
Two primary classes of methods have been developed for interpreting CNN models. The first class of
methods, often referred to as saliency maps, use gradients of classification outcome as a per-pixel
importance score to characterize the influence of individual pixels to the prediction (Simonyan
et al., 2013; Selvaraju et al., 2016; Binder et al., 2016; Shrikumar et al., 2016; Smilkov et al.,
2017; Sundararajan et al., 2017; Levine et al., 2019). Despite providing apparently meaningful
interpretations in practice, these methods exhibit fundamental limitations. First, saliency maps
evaluate pixel-wise importance in an isolated fashion by design, implicitly assuming that other pixels
are fixed (Singla et al., 2019). Worse still, the presence of gradient saturation often breaks the
assumption that important features in general correspond to large gradients (Sundararajan et al., 2016;
Shrikumar et al., 2016; Smilkov et al., 2017). Specifically, an important pixel may contribute a strong
joint effect on the prediction when considered in conjunction with neighboring pixels. However,
that important feature may have a tiny gradient locally, in the sense that its marginal contribution is
diminishing due to the flattened output of the layer. Finally, Ghorbani et al. (2017) and Kindermans
et al. (2017) systematically revealed the fragility of widely-used saliency methods by showing that
even an imperceivable perturbation or simple shift transformation of the input data can lead to a large
change in the resulting saliency scores. In light of this observation, a key challenge for any saliency
method is ensuring that the saliency scores are robust to gradient saturation and perturbations.
A second class of interpretability methods, often based on counterfactual, perturb (i.e., nullifying,
blurring, adding noise, or inpainting ) small regions of the image to modify the classifier prediction
(Fong & Vedaldi, 2017; Dabkowski & Gal, 2017; Chang et al., 2017; Fan et al., 2017; Chang et al.,
1
Under review as a conference paper at ICLR 2020
2019; Yousefzadeh & O’Leary, 2019; Goyal et al., 2019). Despite identifying meaningful regions in
practice, these methods exhibit several limitations. First, counterfactual-based methods implicitly
assume that regions containing the object are the ones most contributed to the prediction (Fan et al.,
2017). However, Moosavi-Dezfooli et al. (2017) revealed that counterfactual-based methods are
also vulnerable to the adversarial attacks, which force these explanation methods to output non-
related backgrounds rather than the meaningful objects as important subregions. In addition, the
counterfactual images may be potentially far away from the distribution from which the training
samples were drawn (Burns et al., 2019), causing the classifier behavior ill-defined. Though some
efforts have been made to train the saliency extractor and the classifier simultaneously (Fan et al.,
2017; ZoIna et al., 2019), the robustness of the resulting explanations are not guaranteed in most
cases (Hendrycks & Dietterich, 2019). In light of this, for any perturbation-based method, a key
challenge is ensuring that the perturbations are effective yet preserving the training distribution.
In this work, we propose a data-driven technique to infer robust saliency maps with distribution-
preserving perturbations. Given an image of interest, the core idea of this method is to generate
a population of perturbed images, referred to as decoy images, that resemble the neural network’s
intermediate representation of the original image but are conditionally independent of its label. The
resulting decoy images capture the variation of the image data originating from either sensor noise or
adversarial attacks. The major differences between the proposed and counterfactual-based methods
are threefold. First, unlike counterfactual images that seek the minimum set of features to exclude
so as to minimize the prediction score or to include so as to maximize the prediction score (Fong
& Vedaldi, 2017), saliency map methods like the one we propose in this paper aim to characterize
the influence of each feature on the prediction score. Second, unlike counterfactual images that
are optimized toward the aforementioned objective with respect to the decision boundary, decoys
are designed to be independent of labels, without changing the decision boundary at all. Finally,
unlike counterfactual images which could potentially be out-of-distribution by adding noise (Smilkov
et al., 2017), rescaling (Sundararajan et al., 2017), blurring (Fong & Vedaldi, 2017), or inpainting
the image (Chang et al., 2017), decoys are plausibly constructed in the sense that their intermediate
representations are non-discriminative from the original input data by design.
In addition, we propose a novel decoy-enhanced saliency score, which considers an ensemble of
saliency maps across multiple decoy images. We also derive a closed-form formula for the decoy-
enhanced saliency score generated from a single-layer convolutional neural network. This analysis
shows that the proposed saliency score for each pixel can be decomposed into two components: the
range of values for a given pixel among the decoys and the number of neurons jointly activated by
that pixel and its neighbors. The former compensates for gradient saturation (Sundararajan et al.,
2016) in the sense that an important pixel can vary without strongly affecting the prediction, due to
the joint effect of nearby pixels. The latter compensates for the way in which saliency maps treat
each pixel independently (Singla et al., 2019) in the sense that the importance of a pixel is jointly
determined by the importance of pixels in its vicinity. This closed-form result may provide insights
to help design better defensive strategies against adversarial attacks.
We apply the proposed method to the ImageNet dataset (Russakovsky et al., 2015) in conjunction
with three standard saliency methods. We demonstrate empirically that decoy-enhanced saliency
scores perform better than the original saliency scores, both qualitatively and quantitatively, even in
the presence of various adversarial perturbations to the image.
2	Problem setting
Consider a multi-label classification task in which a pre-trained neural network model implements
a function F: Rd 7→ RC that maps from the given input x ∈ Rd to C predicted classes. The score
for each class C ∈ {1, ∙∙∙ ,C} is Fc(x), and the predicted class is the one with maximum score,
i.e., argmaxc∈{i,…,c} Fc(x).
A common instance of this setting is image classification, in which case the input x corresponds to
the pixels of an image. A saliency method aims to find the subset of features in the input x that most
strongly lead the neural network to make its prediction. Thus, each pixel is assigned a saliency score,
encoded in an explanation map E : Rd 7→ Rd , in which the pixels with higher scores represent higher
“importance” to the final prediction.
2
Under review as a conference paper at ICLR 2020
Patch masks
「J
Pretrained network	Label-dependent gradients
Input image x
SaIiency maps of x,~
I Output saliency
Figure 1: Workflow for creating decoy-enhanced saliency maps.
A variety of saliency methods have been proposed in the literature. Some, such as edge or other
generic feature detectors (Adebayo et al., 2018), are independent of the predictive model. In this work,
we focus on three methods that do depend on the predictor. The vanilla gradient method (Simonyan
et al., 2013) simply calculates the gradient of the class score with respect to the input x, which is
defined as Egrad(x) = OxFc(x). The SmoothGrad method (Smilkov et al., 2017) seeks to reduce
noise in the saliency map by averaging over explanations of noisy copies of an input, defined as
ESg (x) = N PN=1 Egrad(X+gi) with noise vectors gi 〜 N(0,σ2). The integrated gradients method
(Sundararajan et al., 2017) aims to tackle the problem of gradient saturation. The method starts from a
baseline input x0 and sums over the gradient with respect to scaled versions of the input ranging from
the baseline to the observed input, defined as Eig(x) = (x - x0) × R01OxFc(x0 + α(x - x0))dα.
In this setting, we are given three inputs: a pre-trained neural network model F with L layers, an
image of interest x, and a saliency method E such that E(x; F ) is a saliency map of the same
dimensions as x. As illustrated in Figure 1, the robust saliency map can be obtained in two steps: 1)
constructing decoy images, and 2) computing decoy-enhanced saliency scores.
3	Definition of decoys
Say that f` : Rd → Rd' is the function instantiated by the given
network, which maps from an input image x ∈ Rd to its interme-
diate representation FXX) ∈ Rd' at layer ' ∈ {1,2, ∙∙∙ , L}. At
a specified layer ', the random vector X ∈ Rd is said to be a decoy
of X ∈ Rd if the following swappable condition is satisfied:
Original image
Figure 2: An illustrative exam-
ple of the swap operator swap-
ping image patches between o-
riginal and decoy images.
Decoy image
Fe(X) = F'(XSWaP(x,P)), for image patches P ⊂ {1,…，d},
(1)
The swap(X, P) operation swaps image patches between X and
X (Figure 2). Hence, if P = {10}, then XSWaP(X,P) is a new
image which is almost equivalent to X except that the 10th patch
is exchanged from X. A “valid” patch is a local region receptive to the convolutional filter of a given
CNN, for example, the 3 × 3 area in VGGNet (Simonyan & Zisserman, 2014). The swappable
condition ensures that the original image X and its decoy X are indistinguishable in terms of the
intermediate representation at layer `. Note in particular that the construction of decoys relies solely
on the first' layers of the neural network F1,F2, ∙∙∙ ,F' and is independent of the succeeding layers
F'+ι,…，Fl. In other words, X is conditionally independent of the classification task F(x) given
the input x; i.e., X JL F(x) |x. See Supplementary Section A6 for the explanation why decoys exist.
3.1	Generate decoys by optimization
Given an image of interest X ∈ Rd and a patch mask M ∈ {0, 1}d, we want to construct a decoy
image X = G'(x, M) ∈ Rd, with respect to a specified neural network layer ', so that X is as
different as possible from X. For this purpose, we optimize the following objective function:
maximize^ ∣∣((X - x) ∙ s)+∕ ,
s.t. IIFe(X) - Fe(x)k∞ ≤ e, (X - x) ◦ (1 -M)=0, andX ∈ [0,1]d.
(2)
3
Under review as a conference paper at ICLR 2020
where (∙)+ = max(∙,0), and S = 1 or S = -1 depends on whether We intend to investigate
the upward or downward limit of X deviating from x, respectively. The operators ∣∣∙kι and ∣∣∙k∞
correspond to the Li and L∞ norms, respectively. The constraint (X - x) ◦ (1 - M) = 0 ensures
that X and X differ only in the area marked by the patch mask M, where ◦ denotes entry-wise
multiplication. The final constraint ensures that the values in X fall into an appropriate range for
image pixels, [0, 1] in the normalized image case. In principle, the formulation in Equation 2 is
applicable to any convolutional neural network architecture ranging from simple convolutional
networks to more sophisticated ones (e.g., , AlexNet (Krizhevsky et al., 2012), Inception network
(Szegedy et al., 2016) and ResNet (He et al., 2016), etc.).
As illustrated in Figure 1, we can construct a population of n independently-sampled patch masks
{M1, M2,…,Mn}, so that each possible image patch is covered by at least one patch mask.
Thus we can construct a population of n decoy images {X1, X2, •…,Xn } with respect to different
patch masks. Note that the input image X is also a trivially valid decoy, by definition; i.e., it satisfies
Equation 1 and the conditional independence requirement (i.e., X JL F(x)|x).
3.2	Decoy-enhanced saliency scores
Given the generated population of decoy images {X1, X2, •…，Xn }, the given saliency method E can
be applied to each decoy image, following the fundamental rule that the saliency method should be
agnostic to information revealing which image is a decoy and which is not. In this way, we obtain a
corresponding population of decoy saliency maps {E(X1; F), E(X2; F), ∙∙∙ , E(Xn; F)}.
Now the pixel-wise variation for each feature Xj can be characterized by a population of feature values
Xj = {Xι, Xj,…，Xn}. Accordingly, instead of merely quantifying the saliency of each feature Xj
via a single value E(X; F)j, we can now characterize the uncertainty of the feature’s saliency via the
corresponding population of saliency scores Ej = {E(X1; F)j,E(X2; F)j,…,E(Xn; F j)}.
We define the decoy-enhanced saliency score Zj for each feature Xj as
一 ,ɪ . ,二、
Zj = max(Ej) — mm(Ej),	(3)
That is, Zj is determined by the empirical variation of the decoy’s saliency scores.
3.3	Example: single-layer CNN
Consider a single-layer convolutional neural network with decoy swappable patch size 1 × 1 and
convolutional size 3 X 3. The input of this CNN is X ∈ Rd, unrolled from a √d X √d grayscale image
matrix. Similarly, we have a 3 × 3 convolutional filter unrolled into g ∈ R9, where g is indexed as gk
for k ∈ K, here K
-—√d — 1, -√d, -√d + 1, —1,0,1, √d —
1,
√d, √d +1}
, corresponding
to the index shift in matrix form from the top-left to bottom-right pixel, respectively. We denote
(g*x) ∈ Rd as the output of the convolution operation on the input x, where (g*x)i = Pk∈κ gk Xi+k.
For simplicity, we further assume that there are no pathological cases such as (g * x)i = 0. Such a
neural network can be represented as
m = relu(g * X) , o = WTm + b , p = softmax(o) ,
(4)
where relu(∙) is the entry-wise ReLU operator (Glorot et al., 2011), W ∈ Rd×C represents the
combined weights of the neural network, and b ∈ RC represents the biases. The terms o ∈ RC and
p ∈ RC are the logits and the predicted class probabilities, respectively. The entry-wise softmax
operator for target class C is defined as Pc = PCeOceocO, for C ∈ {1, 2,…，C}.
In this case, we have the following result:
Proposition 1. Zj is bounded by:
Zj ≤ C2 I(X - x)j| E aj+k + CI,
k∈K
(5)
where X is the decoy which maximizes E(X; F j - E(x; F j, aj- = 1 {(g * Xj ≥ 0}, and Ci > 0
and C2 > 0 are bounded constants. See Supplementary Section A7 for the full proof.
4
Under review as a conference paper at ICLR 2020
Proposition 1 indicates that the saliency statistic is determined by two factors: the range of values
associated with pixel j among the decoys (∣(X - x)j ∣) and the number of neurons jointly activated
by pixel j and its neighbors (∑k∈κ Gj+k). The former explains the gradient saturation problem
(Sundararajan et al., 2016) in the sense that important features may have more room to fluctuate
without influencing the joint effect on the prediction. The latter compensates for way in which
saliency maps treat each pixel independently (Singla et al., 2019) in the sense that the importance of a
pixel xj is jointly determined by the surrounding pixels (i.e., the 3 × 3 localized region in our setting),
potentially capturing meaningful patterns such as edges, texture, etc. In this way, the proposed
saliency statistic not only compensates for gradient saturation but takes joint activation patterns into
consideration as well. Indeed, the bound provided by Proposition 1 can be considered a motivation
for defining the decoy-enhanced saliency score as we have in Equation 3.
3.4	Implementation details
We solve Equation 2 by augmenting a constraint as a penalty in the objective function:
minimizex - ∣∣((x - x) ∙ s)+∣l + C ∙ HFe(X) - F`(x)k
s.t. (X 一 x) ◦ (1 一 M) = 0, and X ∈ [0,1]d .
∞,
(6)
When the constant c > 0 is properly chosen, Equations 2 and 6 are equivalent in the sense that the
optimal solution to Equation 2 equals the optimal solution to Equation 6. Our strategy is to set c to a
small value initially and run the optimization. If it fails, then we double c and repeat until success.
The sensitivity of choosing c is reported in Supplementary Section A9.
To eliminate the constraint on pixel values (X ∈ [0,1]d), we use the Change-Of-Variable trick (Car-
lini & Wagner, 2017). Note that other transformations are possible yet unexplored in this paper.
Specifically, instead of optimizing over X, we introduce X satisfying Xi = 2 (tanh(Xi) + 1), for all
i ∈ {1,2,…，d}. Because tanh(Xi) ∈ [-1,1] implies Xi ∈ [0,1], any solution to X is naturally
valid.
To eliminate of the patch mask constraint ((X - x) ◦ (1 -M) = 0), we use projected gradient descent
during the optimization stage. Specifically, after performing each step of standard gradient descent,
we enforce X = X ◦ M + X ◦ (1 - M).
Putting these ideas together, we minimize the following objective function:
minimizex - ∣∣((arctanh(2X - 1) - x) ∙ s)+∣∣ι + C ∙ ∣∣F'(arctanh(2X - 1)) - FXX)H∞ ,	(7)
Because the L∞ norm is not fully differentiable, we adopt the trick introduced by Carlini & Wagner
(2017). Specifically, in each iteration we solve:
minimizeχ-Hmax((arctanh(2X - 1) - x) ∙ s, 0)H ι+c∙∣∣(∣F'(arctanh(2X - 1)) - FXX)I - T)+∣∣j ,
where τ > 0 is initialized with a large value (1 in our experiment). After each iteration, if the
second term in Equation 8 is zero, indicating that τ is too large, then we reduce τ by a factor of 0.95
and repeat; otherwise, we terminate the optimization. The run time of optimization is reported in
Supplementary Section A10.
4 Experiments
To evaluate the effectiveness of our proposed method, we perform extensive experiments on deep
learning models that target image classification tasks. The performance of our method is assessed
both qualitatively and quantitatively. The results show that our proposed method identifies intuitively
more coherent saliency maps than existing state-of-the-art saliency methods alone. The method
also achieves quantitatively better alignment to truly important features and demonstrates stronger
robustness to adversarial manipulation.
Our experiments primarily use the VGG16 model (Simonyan & Zisserman, 2014) pretrained on the
ImageNet dataset (Russakovsky et al., 2015). We also demonstrate the applicability of our method
to other well-studied CNN architectures such as AlexNet (Krizhevsky et al., 2012) and ResNet (He
et al., 2016). The settings for all experiments are reported in Supplementary Section A13.
5
Under review as a conference paper at ICLR 2020
Terrier
Grad
Grad+ Grad+ Grad+	IntGrad+ IntGrad+ IntGrad+	SGrad+	SGrad+ SGrad+
Decoys	Blurring Inpainting	IntGrad	Decoys Blurring Inpainting	SGrad Decoys	Blurring Inpainting
Sf： 5.74 Sf： -0.26 Sf： -0.26 Sf： -0.11 Sf： 3.21 Sf： -0.36 Sf： -0.26 Sf： -0.09 Sf： -0.07 Sf： -0.48 Sf： -0.43 Sf： -0.11
Figure 3: Visualization of saliency maps comparing three state-of-the-art saliency methods with and
without decoy images generated from three techniques (i.e., our decoy generation method, blurring,
and inpainting). More examples can be found in Supplementary Section A14.
4.1	Benchmarks and visualization methodologies
To benchmark the performance of our proposed method, we considered the following baseline
saliency methods: the vanilla gradient method (Simonyan et al., 2013), SmoothGrad (Smilkov et al.,
2017) and integrated gradients (Sundararajan et al., 2017).
To evaluate our decoy generation method, we used the following two methods to replace the proposed
decoy generation method: blurring and inpainting. In particular, given an image with a few masked
patches (i.e., X ◦ (1 - M)), instead of generating a decoy image (i.e., X) by solving the optimization
function in Equation 2, we fill the missing parts by replacing the original missing patches with
blurring or using the state-of-the-art inpainting method (Yu et al., 2018).1 Using the decoy images
generated by these two substitute methods, we then computed the decoy-enhanced saliency scores for
the three baseline saliency methods. We compared the results of blurring and inpainting with our
method, both qualitatively and quantitatively.
A heatmap is a typical way of displaying a saliency map. Our strategy for projecting a saliency map
onto a heatmap is as follows. Saliency methods produce signed values for input features (pixels in
our experiment), where the sign of the value indicates the direction of influence for the corresponding
feature to the predicted class. We first took the absolute value of the saliency maps, as suggested by
Smilkov et al. (2017). Then we used the maximum value across all color channels, as suggested by
Simonyan et al. (2013), to arrive at a single saliency value for each pixel. To avoid outlier pixels with
extremely high saliency values leading to an almost entirely black heatmap, we winsorized those
outlier saliency values to a relatively high value (e.g., 95th percentile), as suggested by Smilkov et al.
(2017), to achieve better visibility. Finally, the values are linearly rescaled to the range [0, 1].
4.2	Comparison criteria
To comprehensively evaluate our proposed method against the aforementioned baseline saliency
methods, we focus on the following three goals.
First, we aim to achieve visual coherence of the identified saliency map. Intuitively, we would prefer
a saliency method that produces a saliency map that aligns cleanly with the object of interest.
Second, we use a saliency fidelity metric (Dabkowski & Gal, 2017) that quantifies the influ-
ence of important features identified by a saliency method, defined as SF (E(X; F), X, c) =
-(log(Fc(E(X; F) ◦ X)) - log(Fc(E(X; F)mean ◦ X))), where c indicates the predicted class of
input X and the saliency map E(X; F) is normalized into a [0, 1] range, as described in Section 4.1.
The function Fc(E(X; F) ◦ X) performs entry-wise multiplication between E(X; F) and X, encoding
the overlaps between the object of interest and the concentration of information by the saliency
map. The rationale here is that, by viewing the saliency score of the feature as its contribution to the
predicted class, a good saliency method will be able to weight important features more highly than
less important ones, giving rise to better predicted class scores. Accordingly, a lower value for the
saliency fidelity metric implies a faithful saliency method. Note that we subtract the influence of
mean saliency E(X; F)mean (i.e., . the mean of E(X; F) on each channel) to eliminate the influence
of bias in E(X; F ). As a result, we can exclude some trivial cases (e.g., E(X; F ) = 1).
Third, we aim for robustness to adversarial manipulations (Ghorbani et al., 2017). Adversarial attacks
aim to apply imperceptible perturbations to an input image that do not affect the predicted class but
produce a very different saliency map relative to the map from the unperturbed image. Ghorbani et al.
1We directly used the well-trained inpainting model provide by Yu et al. (2018).
6
Under review as a conference paper at ICLR 2020
SGrad
Grad
AttImage
Grad +
Decoys
SGrad +
Decoys
Mass
CenterJ
Top-k
IntGrad	IntGrad + SGrad
AttImage IntGrad Decoys AttImage
SS: 9.21
SS: 6.63
SS: 10.56	SS: 9.09
Ss: 18.28	SS: 16.41
Ss: 13.66	SS: 12.87
Ss: 33.43	SS: 22.03
Ss: 7.38	SS: 7.04
Ss: 0.86	SS: 0.80
Ss: 11.12	SS: 10.69
SS: 8.86	SS: 7.75
Target
Figure 4: Visualization of saliency maps under adversarial attacks. More examples can be found in
Supplementary Section A14.
(2017) propose three different attacks: (1) the top-k attack seeks to decrease the important scores
of the top-k important features; (2) the target attack aims to increase the feature importance of a
pre-specified region in the input image; (3) the mass-center attack aims to spatially change the center
of mass of the original saliency map. In this paper, we specify the bottom-right 4 × 4 region of the
original image for the target attack and select k = 5000 in the top-k attack.
To quantify the robustness of a saliency method E to adversarial attack, we use the sensitivity metric
(AlVarez-Melis & Jaakkola, 2018), defined as SS(E(∙, F), x, X) = k(E(x,kX-E(X,F"k2, Where X
is the perturbed image of x. A small value means that similar inputs do not lead to substantially
different saliency maps.
4.3	Results
We applied all three state-of-the-art saliency methods and their decoys to a dozen randomly sampled
images from ImageNet dataset. A side-by-side comparison of the resulting saliency maps (Figure 3)
suggests that decoys consistently help to reduce noise and produce more Visually coherent saliency
maps. For example, in Figure 3, the gradient method Without decoys highlights the region correspond-
ing to the dog’s head, but in a scattered format. In contrast, the decoy-enhanced gradient method not
only highlights the missing body but identifies the dog head With more details such as ears, cheek,
and nose. The Visual coherence is also quantitatiVely supported by the saliency fidelity metric SF ,
Which is consistently higher With decoys than Without decoys. The sanity check (Adebayo et al.,
2018) of decoy-enhanced saliency method is reported in Supplementary Section A8.
Figure 3 also shoWs the comparison of our decoy generation method With blurring and inpainting.
Our decoy generation method consistently outperforms both competing methods, achieVing higher
fidelity scores on all of the three baseline saliency methods. The reason is as folloWs. As is discussed
in Proposition 1, to obtain a higher decoy score, a set of decoy images should satisfy the folloWing
tWo requirements: first, they produce similar intermediate representation When proVided as input to
the target neural netWork; second, they exhibit large decoy Variation. In this paper, We design our
decoy generation objectiVe function based on these tWo requirements. In contrast, neither blurring nor
inpainting is designed to meet these requirements. Specifically, blurring may Violate the intermediate
representation constraint and inpainting may not be able to produce enough decoy Variation.
It is Worth mentioning that combining image blurring or inpainting With our decoy-enhanced saliency
scores can still improVe upon the baseline saliency maps. The empirical Validity of using image
blurring is explained in Supplementary Section A11. From the practitioners’ perspectiVe, blurry
images can be used as more efficient alternatiVes to combine With the decoy-enhanced saliency score
to achieVe comparable performance. HoWeVer, it is Worth mentioning that decoy images are still
necessary to justify the theoretical soundness of the decoy-enhanced saliency score.
Next, to test the robustness of the methods We subjected each to the three aforementioned adVersarial
attacks (Figure 4). Though not fully resistant to adVersarial attacks, We obserVe that enhancing
7
Under review as a conference paper at ICLR 2020
Figure 5: Generating decoys with respect to different DNN layers.
Grad
Grad +
Decoys
IntGrad IntGrad + SGrad
Decoys
SGrad +
Decoys
Grad
Grad +
Decoys
IntGrad IntGrad + SGrad
Decoys
SGrad +
Decoys
SF: 4.64	SF:	-2.98	SF:	2.99	SF: -2.18	SF:	6.85	SF:	3.86	SF:	7.03	SF: 0.24	SF:	5.80	SF:	0.26 SF: 0.71	SF:	0.01
(a) Saliency maps generated on AlexNet.
(b) Saliency maps generated on ResNet.
Figure 6: Visualization of saliency maps under different CNN architectures.
existing saliency methods with decoys consistently mitigates the impact of adversarial manipulations,
leading to visually more coherent saliency maps as well as lower sensitivity scores. We believe the
extra benefits in the robustness is brought by our decoy-enhanced saliency score. In a normal situation
(when the image doesn’t suffer from an adversarial attack), an important pixel is not important in an
isolated fashion. Instead, the important pixel tends to contribute a strong joint effect in conjunction
with neighboring important pixels, to potentially capture meaningful patterns such as edges, texture,
etc. In light of this observation, this particular important pixel will have more room to fluctuate
without influencing the joint effect on the prediction. In such case, some elements of Ej will be high
and others will be low, contributing a large Zj . In the unusual situation when an isolated important
pixels is indeed observed (i.e., a pixel is very important for all decoy images and has no room for
fluctuation.), we tend to believe that the pixel has been adversarial attacked. In this case, all elements
of Ej will be high and the proposed decoy-enhanced saliency score Zj will be low, which is what we
want.
4.4	Sensitivity of decoy settings
We also conduct experiments to understand the impact of decoy settings on the performance of
different saliency methods. Specifically, we focus on the choice of network layer `, because the
intermediate representations can vary significantly at different layers. Accordingly, we vary the
value of ` for VGG16 and compare the differences of decoy-enhanced saliency scores from the three
saliency methods, ranging from the first convolutional layer to the last pooling layer. As shown
in Figure 5 and Supplementary Section A14, the decoy-enhanced saliency scores generated from
different layers for the same image are of similar qualities. We also compute the SF score for each
saliency map. The mean and standard deviation, respectively, for the scores obtained by the gradient,
integrated gradient and SmoothGrad methods are as follows: (-0.11, 0.02), (-0.18, 0.02), (-0.21, 0.01).
These quantitative results also support the conclusion that our method is not sensitive to layer. This
is likely because, as previous research has shown (Chan et al., 2015; Saxe et al., 2011), the final
classification results of a DNN are not highly related to the intermediate representations. As a result,
decoys for the same sample with the same label from different layers should yield similar results.
4.5	Applicability to other neural architectures
In addition to the VGG16 model (Simonyan & Zisserman, 2014), we generate saliency maps for other
neural architectures: AlexNet (Krizhevsky et al., 2012) and ResNet (He et al., 2016). We visualize
their saliency maps in Figure 6 and Supplementary Section A14. From Figure 6, we observe that
all the saliency maps accurately highlight the contour of the dog and achieve higher fidelity score
8
Under review as a conference paper at ICLR 2020
than the baseline saliency methods. This indicates that we can apply our method to various neural
architectures and expect consistent performance. It should be noted that, in comparison with saliency
maps derived from other neural architectures, the maps tied to AlexNet are relatively vague. We
hypothesize that this is due to the relative simplicity of the AlexNet architecture.
5 Discussion and conclusion
In this work, we have used distribution-preserving decoys as a way to produce more robust saliency
scores. First, we formulate decoy generation as an optimization problem, in principle applicable
to any network architecture. We demonstrate the superior performance of our method relative to
three standard saliency methods, both qualitatively and quantitatively, even in the presence of various
adversarial perturbations to the image. Second, by deriving a closed-form formula for the decoy-
enhanced saliency score, we show that our saliency scores compensate for the frequently violated
assumption in saliency methods that important features in general correspond to large gradients.
Although we have shown that decoys can be used to produce more robust saliency scores, there
remain some interesting directions for future work. For example, one could potentially reframe
interpretability as hypothesis testing and use decoys to deliver a set of salient pixels, subject to false
discovery rate control at some pre-specified level (Burns et al., 2019; Lu et al., 2018).
References
Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, and Been Kim. Sanity
checks for Saliency maps. In Advances in Neural Information Processing Systems, pp. 9524-9535,
2018.
David Alvarez-Melis and Tommi S Jaakkola. Towards robust interpretability with self-explaining
neural networks. In Advances in Neural Information Processing Systems, 2018.
Alexander Binder, GregOire Montavon, Sebastian Lapuschkin, Klaus-Robert Muller, and Wojciech
Samek. Layer-wise relevance propagation for neural networks with local renormalization layers.
In International Conference on Artificial Neural Networks, pp. 63-71, 2016.
Collin Burns, Jesse Thomason, and Wesley Tansey. Interpreting black box models via hypothesis
testing. arXiv:1904.00045, 2019.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017
IEEE Symposium on Security and Privacy (SP), pp. 39-57, 2017.
Tsung-Han Chan, Kui Jia, Shenghua Gao, Jiwen Lu, Zinan Zeng, and Yi Ma. PCANet: A simple
deep learning baseline for image classification. IEEE Transactions on Image Processing, 24(12):
5017-5032, 2015.
Chun-Hao Chang, Elliot Creager, Anna Goldenberg, and David Duvenaud. Interpreting neural
network classifications with variational dropout saliency maps. In Advances in Neural Information
Processing Systems, volume 1, pp. 6, 2017.
Chun-Hao Chang, Elliot Creager, Anna Goldenberg, and David Duvenaud. Explaining image
classifiers by counterfactual generation. In International Conference on Learning Representations,
2019.
Piotr Dabkowski and Yarin Gal. Real time image saliency for black box classifiers. In Advances in
Neural Information Processing Systems, pp. 6967-6976, 2017.
Lijie Fan, Shengjia Zhao, and Stefano Ermon. Adversarial localization network. In Learning with
limited labeled data: weak supervision and beyond, Advances in Neural Information Processing
Systems Workshop, 2017.
Ruth C Fong and Andrea Vedaldi. Interpretable explanations of black boxes by meaningful perturba-
tion. In Proceedings of the IEEE International Conference on Computer Vision, pp. 3429-3437,
2017.
9
Under review as a conference paper at ICLR 2020
Amirata Ghorbani, Abubakar Abid, and James Zou. Interpretation of neural networks is fragile.
arXiv:1710.10547, 2017.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier neural networks. In
Proceedings of the fourteenth International Conference on Artificial Intelligence and Statistics, pp.
315-323,2011.
Yash Goyal, Ziyan Wu, Jan Ernst, Dhruv Batra, Devi Parikh, and Stefan Lee. Counterfactual visual
explanations. International Conference on Machine Learning, 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition,
pp. 770-778, 2016.
Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common
corruptions and perturbations. In International Conference on Learning Representations (ICLR),
2019.
Pieter-Jan Kindermans, Sara Hooker, Julius Adebayo, Maximilian Alber, Kristof T Schutt, Sven
Dahne, Dumitru Erhan, and Been Kim. The (Un) reliability of saliency methods. arXiv:1711.00867,
2017.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolu-
tional neural networks. In Advances in Neural Information Processing Systems, pp. 1097-1105,
2012.
Alexander Levine, Sahil Singla, and Soheil Feizi. Certifiably robust interpretation in deep learning.
arXiv preprint arXiv:1905.12105, 2019.
Zachary C Lipton. The mythos of model interpretability. arXiv:1606.03490, 2016.
Yang Lu, Yingying Fan, Jinchi Lv, and William Stafford Noble. DeepPINK: reproducible feature
selection in deep neural networks. In Advances in Neural Information Processing Systems, pp.
8676-8686, 2018.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal
adversarial perturbations. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 1765-1773, 2017.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition
challenge. International Journal of Computer Vision, 115(3):211-252, 2015.
Andrew M Saxe, Pang Wei Koh, Zhenghao Chen, Maneesh Bhand, Bipin Suresh, and Andrew Y Ng.
On random weights and unsupervised feature learning. In International Conference on Machine
Learning, volume 2, pp. 6, 2011.
Ramprasaath R Selvaraju, Abhishek Das, Ramakrishna Vedantam, Michael Cogswell, Devi Parikh,
and Dhruv Batra. Grad-CAM: Why did you say that? arXiv:1611.07450, 2016.
Avanti Shrikumar, Peyton Greenside, Anna Shcherbina, and Anshul Kundaje. Not just a black box:
Learning important features through propagating activation differences. arXiv:1605.01713, 2016.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv:1409.1556, 2014.
Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks:
Visualising image classification models and saliency maps. arXiv:1312.6034, 2013.
Sahil Singla, Eric Wallace, Shi Feng, and Soheil Feizi. Understanding impacts of high-order loss
approximations and features in deep learning interpretation. arXiv:1902.00407, 2019.
Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Viegas, and Martin Wattenberg. Smoothgrad:
removing noise by adding noise. arXiv:1706.03825, 2017.
10
Under review as a conference paper at ICLR 2020
1	1	2	4
ɪ	ɪ	~T_	W
瓦	W	T	ɪ
彳	T	T	4
conv with 2x2
filter and stride 2
filter=国
filter=噩
conv with 2x2
filter and stride 2
6	1	1	4
回	工	互	ɪ
~2_	工	工	工
彳	T	4	2
1	1	2	4
ɪ	ɪ	工	ɪ
W	W	T	
T	T	T	4T
max pool with 2x2
filter and stride 2
max pool with 2x2
filter and stride 2
6	1	0	0
工	尤	ɪ	且
瓦	W	工	4
T	T	T	4
OrigTextinal image
Decoy image
Figure A7: A toy example to illustrate why decoys exist.
Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Gradients of counterfactuals. arXiv:1611.02639,
2016.
Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. arX-
iv:1703.01365, 2017.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv:1312.6199, 2013.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking
the inception architecture for computer vision. In Proceedings of the IEEE conference on Computer
Vision and Pattern Recognition, pp. 2818-2826, 2016.
Roozbeh Yousefzadeh and Dianne P O’Leary. Interpreting neural networks using flip points. arXiv
preprint arXiv:1903.08789, 2019.
Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas S Huang. Generative image
inpainting with contextual attention. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 5505-5514, 2018.
Konrad Zoina, Krzysztof J Geras, and KyUnghyUn Cho. Classifier-agnostic saliency map extraction.
In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 10087-10088,
2019.
A6	Existence of decoys
As illUstrated in FigUre A7, the original image and the decoy image shares exactly the same
intermediate representation with respect to convolUtional and max pooling.
A7 Proof of Proposition 1
Consider a single-layer convolUtional neUral network with decoy swappable patch size 1 × 1 and
convolutional size 3 X 3. The input of this CNN is X ∈ Rd, unrolled from a √d X √d grayscale image
matrix. Similarly, we have a 3 × 3 convolUtional filter Unrolled into g ∈ R9, where g is indexed as gk
for k ∈ K, here K
n -√d — 1, -√d, -√d + 1, —1,0,1, √d —
1,
√d, √d +1}
, corresponding
11
Under review as a conference paper at ICLR 2020
to the index shift in matrix form from the top-left to bottom-right pixel, respectively. We denote
(g*x) ∈ Rd as the output of the convolution operation on the input x, where (g*x)i = Pk∈κ gk xi+k.
For simplicity, We further assume that there are no pathologic cases such as (g * x) = 0. Such a
neural network can be represented as
m = relu(g * x) ,
o=WTm+b,	(9)
p = softmax(o) ,
where relu(∙) is the entry-wise ReLU operator, W ∈ Rd×C represents the combined weights of the
neural network, and b ∈ RC represents the biases. The terms o ∈ RC and p ∈ RC are the logits and
the predicted class probabilities, respectively. The entry-wise softmax operator for target class c is
defined as Pc = LCeoc o 0, for C ∈ {1, 2,…，C}.
c0=1 e c
The gradient of Pc with respct to x can be written as follows, using the denominator layout notation
of the derivative of a vector:
Ox Pc = ∂x 黑 ⅛,
(10)
where
and
∂ o
∂m
W,
∂pc
∂oc0
∂pc
∂θco
(Pc - Pc2 )	if c0 = c ,
-PcPc0	otherwise .
(11)
(12)
Then, we can write ∂dpc as follows:
普=P∙c,
∂oc0
(13)
where P∙c corresponds to the c-th column of P and P^ = diag(p) 一 PPT.
We then define d∂m as B ∈ Rd×d, in which
=∂mj = d(g * X) j dmj	=	d(g * Xj
ij	∂Xi	∂Xi	∂(g * x)j	j	∂Xi
where aj is denoted as aj = 1 {(g * x)j ≥ 0}. Then, we have
Bij = aj gi-j ifi 一 j ∈ K ,
Bij = 0 otherwise .
(14)
(15)
Given Equations 11, 13 and 15, Equation 10 can be rewritten as
_____ʌ
Ox Pc = BWP∙c,
(16)
Further, we define the Hessian as H, where Hij can be written as follows:
_ , ʌ
ττ	/	、	∂(Bj∙WP ∙c)
Hj = Oxi (Oxj Pc) =	∂x
=XX ∂(BjkWk∙P∙c)
∂xi	∂ Xi
k=1	i
=X ∂(Bjj+kWj+k「PR
∂x	∂Xi
k∈K	i
(17)
Bjj+ k wj+k,∙
feaj+k gkWj+k,∙
:
∂P ∙c
∂Xi
∂P ∙c
∂Xi
(
12
Under review as a conference paper at ICLR 2020
(1 - 2pc)Oxipc	ifc0 = c,
pcOxipc0 + pc0 Oxi pc	otherwise.
(18)
and
,^
∂P C C
∂Xi
After computing the gradient and the Hessian, we now derive the decoy-enhanced saliency score
Zj for Xj, given a population of Saliency scores Ej = {E(X1; F j,E(X2; F j,…,E(xn; F j)}.
According to Equation 3, Zj can always be decomposed into Zj = Zj+ + Zj- where Zj+ =
max(Ej) - E(X; F)j and Zj- = E(X; F)j - min(Ej), respectively. Without loss of generality, we
only focus on Zj+, noting that a smilar analysis is applicable to Zj- as well.
Let X ∈ {X1, X2,…，Xn } denote the decoy which maximizes E(X; F j - E(x; F j.
The second-order Taylor expansion of the predicted Fc(X) for target class c around X is as follows:
FC(X) ≈ Fc(X) + O文FC(X)T∆ + 1∆tHx∆	(19)
where ∆ = X - X and Hx is the Hessian of the neural network model on the decoy X whose entries
are (Hx)ij = ∂X2Fc.. By definition of the swappable condition in Equation 1, Fc(x) = Fc(X).
Therefore, the following equation holds:
OxFc(X)t∆ ≈ - 1∆tHx∆	(20)
Consider the case that the swappable patches are of size 1 × 1 at position i; then Equation 20 can be
decomposed into
E(X; F)i = (OxFc(X))i ≈ 1(X - x)i(Hx)i,i,	(21)
Then, according to Equation 21, we have
Zj = 2(X - Xj (Hx)jj - Oxj pc
1
≤ 2(x - Xj Taj+kgkw(j+k)∙p∙c + CI	(22)
k∈K
≤。2 l(x - χ)j IX aj + k + CI
k∈K
where Ci = IOxj PCl and C2 = maxk∈κ ∣gk Wj+k> P./,respectively. Note that both Ci and C2
are linear combinations of the gradient, which is bounded by some Lipschitz constant (Szegedy et al.,
2013).
A8 S anity check for decoy-enhanced saliency maps
As suggested by Adebayo et al. (2018), any valid saliency methods should pass the sanity check in the
sense that the saliency method should be dependent on the learned parameters of the predictive model,
instead of edge or other generic feature detectors. We performed the model parameter randomization
test (Adebayo et al., 2018) by comparing the output of the proposed saliency method on a pretrained
VGG16 network with the output of the proposed saliency method on a weight-randomized VGG16
network. If the proposed saliency method indeed depends on the learned parameters of the model, it
is expected that the outputs between the two cases differ substantially.
Following the cascading randomization strategy (Adebayo et al., 2018), the weights of pretrained
VGG16 network are randomized from the top to bottom layers in a cascading fashion. This cascading
randomization procedure is designed to destroy the learned weights successively. As illustrated
in Figure A8, the cascading randomization destroys the decoy-enhanced saliency maps combined
with three existing saliency methods, qualitatively. The conclusion is also supported by quantitative
comparison measured by the structural similarity index (SSIM), shown in Figure A9.
13
Under review as a conference paper at ICLR 2020
Figure A8: Cascading randomization on VGG16 network. The figure shows the original saliency map
(first column) for the terrier. Progression from left to right corresponds to complete randomization of
the pretrained VGG16 network weights from the top layer to the bottom layer.
三
ω
ω
Figure A9: Structural similarity index (SSIM) for Cascading Randomization on VGG16 network.
A9 Sensitivity of hyperparameter selection
The optimization formulated in Equation 6 involves one hyperparamer, c, the initial coefficient to
weigh the intermediate representation difference between the decoy and the original image. Note that
we use the same hyperparameter setting across all the images, rather than for each image separately.
To evaluate how the selection of c affects the performance, we carried out optimizations with respect
to a wide range of C ∈ {101,102, 103,104, 105 }. AS illustrated in Figure Al0, the choice of initial
coefficient c makes a negligible difference of three state-of-the-art saliency methods, both qualitatively
and quantitatively.
A10	Run time of decoy Optimization
To evaluate the computational cost of optimizing Equation 6, we carried out the run time comparison
between optimizing one decoy and calculating three types of state-of-the-art saliency methods. The
comparison is repeated 500 times with respect to different patch masks. As illustrated in Figure A11,
on average optimizing one decoy takes 37.7% of the run time of the fastest, vanilla gradient-based
saliency method. For other methods, the optimization is even less expensive, in a relative sense.
A11	Validity of generating decoy by blurring
As shown in Figure 3, combining blurry images with our decoy-enhanced saliency scores achieves
empirically good performance even though the blurry images violate the design of decoys which
resemble the intermediate representation of the original image. To understand why blurry images
are empirically good, we compared the relative difference between the intermediate representation
of the original images and the decoy/blurry images. Here, the relative difference is defined as the
14
Under review as a conference paper at ICLR 2020
Terrier
C=10
Grad
+Decoys
C=100	C=1000
C=10000	C=100000
sF: -0.20 sF: -0.23 sF: -0.18
sF: -0.26	sF: -0.16
IntGrad
+Decoys
Sf: -0.37 Sf: -0.36 Sf: -0.37	Sf: -0.36	Sf: -0.36
SGrad
+Decoys
SF: -0.51	SF: -0.44	SF: -0.47 SF: -0.48	SF:-0.50
Figure A10: Visualization of saliency maps optimized using different hyperparamer c.
IO3
IO2
IO1
SPUoUtυs U-tυE一*j6u 三 uncc
Decoy generation	Grad	IntGrad	SGrad
Figure A11: Run time to optimize one decoy and calculate three state-of-the-art saliency maps of the
decoy. The comparison is conducted in the same CPU to guarantee fairness.
L-infinity norm between two intermediate representations divided by the maximum absolute value of
any intermediate representation.
15
Under review as a conference paper at ICLR 2020
Because the decoy images are designed to preserve the intermediate representation of the original
images, the relative difference is expected to be small. On the other hand, for the blurry images, the
relative difference should be arbitrary since there is no such constraint. As illustrated in Figure A12,
in the first maxpool layer (i.e., a layer near the bottom of the network), the relative difference is
large for blurry images (0.307 on average) and very small for decoy images (0.006 on average) as
expected. However, in the last fully-connected layer (i.e., a layer near the top of the network), the
relative difference is much smaller for blurry images (0.034 on average) and remains small for decoy
images (0.002 on average). In conclusion, even though the blurry images violate the constraint of
decoys, this violation would be mitigated in deeper layers of the network.
0.4
0.3
vgg_16/pool 1/MaxPool
vgg_16/fc8/squeezed
VGG16 layers
ro 0.2
Φ
Figure A12: Relative difference between the intermediate representation of the original images and
decoy/blurry images.
A12	Object localization
A standard method to evaluate saliency maps is by object localization (Dabkowski & Gal, 2017;
Fong & Vedaldi, 2017), where the model was trained with the class label only without access to any
localization data. We carried out Imagenet ILSVRC’14 localization task (Russakovsky et al., 2015)
which contains 50K ImageNet validation images with annotated bounding boxes as ground truth.
For each of the image in the dataset, we first calculated the gradient-based saliency maps with and
without using blurry decoys, based on a pretrained VGG16 network. Following the preprocessing
steps suggested by Dabkowski & Gal (2017); Fong & Vedaldi (2017), for each calculated saliency
maps, a bounding box for the most dominant object is predicted by thresholding the corresponding
saliency map and extracting the tightest bounding box that contains the whole remaining saliency
map.
We investigated three different thresholding strategies suggested by Fong & Vedaldi (2017), including
value thresholding, energy thresholding, and mean thresholding. Following the evaluation protocol
suggested by Dabkowski & Gal (2017); Fong & Vedaldi (2017), the extracted localization box has to
have Intersect over Union (IoU) greater than 0.5 with any of the ground truth bounding boxes in order
to consider the localization successful, otherwise it is regarded as failure. As shown in Table A1, in
terms of accuracy, decoy-enhanced saliency maps perform better than vanilla saliency maps without
decoys.
16
Under review as a conference paper at ICLR 2020
			
Accuracy	Value thresholding (0.25)	Energy thresholding (0.25)	Mean thresholding (0.25)
Gradient	0.662	07B	0.662
Gradient+Decoys	0.722	0.723	0.665
Table A1: ImageNet localization accuracy on VGG16 network using different thresholding strategies.
A13 Experimental settings
	`	c	E	patch_size	stride	τ
AlexNet	6	10000	^^00^^	3	1	1
VGG16	3	10000	200	3	1	1
ResNet	2	10000	200	3	1	1
Table A2: The experimental settings of proposed methods for all the target models.
The experimental settings of the proposed approach on three convolutional neural networks
(i.e., AlexNet, VGG16, ResNet) are shown in Table A2. In the table, ` is the index of the lay-
er within the target model that is selected to generate the decoy images. The hyperparameter c
controls the weight of ∣∣F'(x) - F'(x)k∞, and E refers to the tolarence of ∣∣F'(x) - F'(x)k∞ in
Equation (6). The patch_size and stride control the size and the stride step of each decoy patch.
A14 Additional experimental results
Figure A13-A16 provide more examples of the fidelity, stability and generalizability of the decoy-
enhanced saliency methods. These results are consistent with those shown in the Figures 3-6.
17
Under review as a conference paper at ICLR 2020
T-shirt
Grad
Grad +
Decoys
IntGrad +
IntGrad Decoys SGrad
SGrad +
Decoys
Sf:3.90 Sf:1.76 Sf:3.42 Sf:2.77 Sf:1.42 Sf:1.26
Trailer Truck
Sf: 1.27	Sf:	0.75	Sf:	1.45	Sf:	0.36	Sf:	1.54	SF:	1.48
Sf: 2.23	Sf: 1.41
Sf: 3.07
Sf: 2.35	Sf: 0.55	Sf: 0.36
Shield
Sf: 4.50	Sf: 2.44 Sf: 3.48	Sf: 3.08	Sf: 2.10	Sf: 1.90
Sf: 0.11	Sf: 0.05	Sf:-0.09
Vending Machine Sf：3.59
Sf: 1.87
Sf: -0.04	Sf: 1.76	Sf: 1.61
f: 1.47	Sf: 0.93	Sf: -0.41
Volcano
SF: 0.72
Cliff
Seashore SF: 2.76	SF:2.73 SF: 2.66
Alp
2.11
SF: 6.12 SF: 1.03 SF: 4.95 SF: 2.66
SF: 2.23 SF: 1.89
SF: 8.28 SF: 7.57
Figure A13: Fidelity evaluation results of four more images on VGG16.
18
Under review as a conference paper at ICLR 2020
Top-k
Grad
AttImage
Grad
Grad +
Decoys
IntGrad
AttImage
IntGrad
IntGrad +
Decoys
SGrad
AttImage
SGrad
SGrad +
Decoys
Mass
Center
Target
Grad
AttImage
(a) Stability results of a shield.
(b) Stability results of a bustard.
SS: 5.16	SS: 4.28
Top-k
Grad
Grad +
Decoys
IntGrad
AttImage
IntGrad
IntGrad +
Decoys
SGrad
AttImage
SGrad
SGrad +
Decoys
Mass
Center
Target
Figure A14: Stability evaluation results of three more images on VGG16.
19
Under review as a conference paper at ICLR 2020
IntGrad
SGrad
Grad +
Decoys
IntGrad +
Decoys
SGrad +
Decoys
Bustard
Sf: 3.23 Sf: 0.004	Sf: 0.12	Sf: -0.04	Sf: 1.20	Sf: 0.23
Macaw
Sf: 7.80	Sf: 2.22	Sf: 4.19	Sf: -0.03	Sf: 2.30	Sf: -1.26
Trailer Truck	Sf:	4.45	Sf:	2.64	Sf:	4.36	Sf:	1.96	Sf:	3.20	Sf: 2.81
Shield
Sf: 3.84	Sf: 2.28	Sf:	1.97	Sf:	1.15	Sf:	0.26	Sf:	-0.02
Macaw
Scooter
Sf: 10.38 Sf: 9.83	Sf: 10.47 Sf: 6.56	Sf: 8.33	Sf: 7.75
Sf: 8.99	Sf: 7.55 Sf: 10.58 Sf: 6.96	Sf: 7.64	Sf: 7.44
(b) Original and decoy-enhanced saliency maps on ResNet.
Figure A15: The illustration of the performances of the proposed approach of more images on
AlexNet and ResNet.
(a)	The mean and standard derivation of SF score for gradient, integrated
gradient and SmoothGrad are: (-0.01, 0.01), (-0.03, 0.01), (0.09, 0.01).
Shield
Grad
Bottom layers to top layers
IntGrad
SGrad
(b)	The mean and standard derivation of SF score for gradient, integrated
gradient and SmoothGrad are: (3.34, 0.73), (2.21, 1.33), (1.49, 0.18).
Figure A16: Demonstrations of decoy-enhanced saliency maps generated from each convolutional
and pooling layer in VGG16.
20