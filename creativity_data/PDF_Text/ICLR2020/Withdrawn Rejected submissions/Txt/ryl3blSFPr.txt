Under review as a conference paper at ICLR 2020
Denoising Improves Latent Space Geometry in
Text Autoencoders
Anonymous authors
Paper under double-blind review
Ab stract
Neural language models have recently shown impressive gains in unconditional text
generation, but controllable generation and manipulation of text remain challenging.
In particular, controlling text via latent space operations in autoencoders has been
difficult, in part due to chaotic latent space geometry. We propose to employ
adversarial autoencoders together with denoising (referred as DAAE) to drive
the latent space to organize itself. Theoretically, we prove that input sentence
perturbations in the denoising approach encourage similar sentences to map to
similar latent representations. Empirically, we illustrate the trade-off between text-
generation and autoencoder-reconstruction capabilities, and our model significantly
improves over other autoencoder variants. Even from completely unsupervised
training without style information, DAAE can perform various style transfers,
including tense and sentiment, through simple latent vector arithmetic.1
1	Introduction
Autoencoder based generative models have recently become popular tools for advancing controllable
text generation such as style or sentiment transfer (Bowman et al., 2016; Hu et al., 2017; Shen
et al., 2017; Zhao et al., 2018). By mapping sentences to vectors in the latent space, these models
offer in principle an attractive, continuous approach to manipulating text by means of simple latent
vector arithmetic. However, the success of such manipulations rests heavily on the latent space
geometry and how well it agrees with underlying sentence semantics. Indeed, we demonstrate that
without additional guidance, fortuitous geometric agreements are unlikely to arise, shedding light on
challenges faced by existing methods.
We use adversarial autoencoders (Makhzani et al., 2015, AAEs) to study the latent space geometry.
In contrast to variational autoencoders (Kingma & Welling, 2014, VAEs), AAEs can maintain
strong coupling between the encoder and decoder that the decoder does not omit the encoded
input sentence (Bowman et al., 2016). The training criterion for AAEs consists of two parts, the
ability to reconstruct sentences and the additional constraint that the encoded sentences are overall
indistinguishable from prior samples, typically Gaussian. We show that these objectives alone do
not suffice to force proper latent space geometry for text control. Specifically, for discrete objects
such as sentences where continuity assumptions no longer hold, powerful AAEs can easily learn to
map training sentences into latent prior samples arbitrarily (Figure 1, Left), while retaining perfect
reconstruction. Latent space manipulations in such cases will yield random, unpredictable results.
To remedy this, we augment AAEs with a simple denoising objective (Vincent et al., 2008; Creswell
& Bharath, 2018) that requires perturbed sentence with some words missing to be mapped back to the
original version. We refer to our model as DAAE. We prove that the denoising criterion can eliminate
disorganized solutions and drive the latent space to organize itself. As a result, similar sentences
begin to be mapped to similar latent vectors (Figure 1, Right).
Improvements in latent space geometry carry many positive consequences. Through systematic
evaluations of the generation and reconstruction capabilities of various text autoencoders (Cifka
et al., 2018), we find that our proposed DAAE provides the best trade-off between producing high-
quality text vs. informative sentence representations. We empirically verify that DAAE has the best
neighborhood preservation property, consistent with our theory. We further investigate to what extent
1Our code will be made publicly available after the review process.
1
Under review as a conference paper at ICLR 2020
Figure 1: Illustration of the learned latent geometry by AAE before and after introducing x perturbations. With
high-capacity encoder/decoder networks, a standard AAE has no preference over x-z couplings and thus can
learn a random mapping between them (Left). Trained with local perturbations C (x), DAAE learns to map
similar x to close z to best achieve the denoising objective (Right).
text can be manipulated by applying simple transformations in the learned latent space. Our model
is able to perform sentence-level vector arithmetic (Mikolov et al., 2013) fairly well to change the
tense or sentiment of a sentence without any training supervision. It also produces higher quality
sentence interpolations than other text autoencoders, suggesting better linguistic continuity in its
latent space (Bowman et al., 2016).
2	Related Work
Denoising is first introduced into standard autoencoders by Vincent et al. (2008, DAE) to learn robust
representations. Without a latent prior, DAE requires sophisticated MCMC sampling to be employed
generatively (Bengio et al., 2013). Creswell & Bharath (2018) applied denoising with AAEs to
generative image modeling. Here, we demonstrate that input perturbations are particularly useful for
discrete text modeling because they encourage preservation of data structure in the latent space.
Apart from the AAE framework that our paper focuses on, another popular latent variable generative
model is the variational autoencoder (Kingma & Welling, 2014, VAE). Unfortunately, when the
decoder is a powerful autoregressive model (such as a language model), VAE suffers from the
posterior collapse problem where the latent representations get ignored (Bowman et al., 2016; Chen
et al., 2016). If denoising is used in conjunction with VAEs (Im et al., 2017) in text applications,
then the noisy inputs will only exacerbate VAE’s neglect of the latent variable. Bowman et al. (2016)
proposed to weaken VAE’s decoder by masking words on the decoder side to alleviate its collapse
issue. However, even with a weakened decoder and combined with other techniques including
KL-weight annealing and adjusting training dynamics, it is still difficult to inject significant content
into the latent code (Yang et al., 2017; Kim et al., 2018; He et al., 2019). Alternatives like the
β-VAE (Higgins et al., 2017) appear necessary.
Previous work on controllable text generation has employed autoencoders trained with attribute
label information (Hu et al., 2017; Shen et al., 2017; Zhao et al., 2018; Logeswaran et al., 2018;
Subramanian et al., 2018). We show that the proposed DAAE model can perform text manipulations
despite being trained in a completely unsupervised manner without attribute labels. This suggests
that on the one hand, our model can be adapted to semi-supervised learning when a few labels are
available. On the other hand, it can be easily scaled up to train one large model on unlabeled text
corpora and then applied for transferring various styles.
3	Method
Define X = Vm to be a space of sequences of discrete symbols from vocabulary V (with maximum
length m); also define Z = Rd to be a continuous latent space. Our goal is to learn a mapping
between the data distribution pdata(x) over X and a given prior distribution p(z) over latent space Z
(following common practice, a Gaussian prior is used in our experiments, although not required by
our methodology). Such a mapping allows us to easily manipulate discrete data through continuous
latent representations z, and provides a generative model where data samples can be obtained by first
drawing z from the prior and then sampling a corresponding sequence via p(x|z).
2
Under review as a conference paper at ICLR 2020
We adopt the adversarial autoencoder (AAE) framework, which involves a (deterministic) encoder
E : X → Z, a probabilistic decoder G : Z → X, and a discriminator D : Z → [0, 1] . Both E and
G are recurrent neural networks (RNNs)2. E takes input sequence x and outputs the last hidden state
as its encoding z . G generates a sequence x autoregressively, with each step conditioned on z and
previous symbols. The discriminator D is a feed-forward net that outputs the probability of z coming
from the prior rather than the encoder. E , G and D are trained jointly with a min-max objective:
min max Lrec(θE, θG) - λLadv(θE, θD)	(1)
with: Lrec(θE, θG) = Epdata(x) [- logpG(x|E(x))]	(2)
Ladv(θE,θD) = Ep(z)[- log D(z)] + Epdata(x)[- log(1 - D(E(x)))]	(3)
where reconstruction loss Lrec and adversarial loss3 Ladv are weighted via hyperparameter λ > 0.
We further introduce perturbations in X space to learn smoother representations that reflect local
structure in the data, ending up with the denoising adversarial autoencoder (DAAE) model. Given a
perturbation process C that stochastically maps X to nearby X ∈ X, let p(x, X) = Pdata(X)PC(x|x)
andP(X) = Px p(χ, X). We change the loss functions to be:
Lrec (Θe ,Θg) = Ep(x,x)[- log Pg(x∣E (X))]	(4)
Ladv(Θe,Θd) = Ep(z)[-logD(z)] + Ep(x)[-log(1 - D(E(X)))]	⑸
Here, LreC is the loss of reconstructing X from X, and Ladv is the adversarial loss evaluated on
perturbed X. The objective function combines the denoising technique with the AAE (Vincent et al.,
2008; Creswell & Bharath, 2018). When PC(X|x) = 1[X = x] (i.e. there is no perturbation), the
above simply becomes the usual AAE objective.
LetPE(z|X) denote the encoder distribution. With our perturbation process C, the posterior distribu-
tions of the DAAE are of the following form:
q(z∣χ) = EPC (XIX)PE (z∣x)	(6)
x
This enables the DAAE to utilize stochastic encodings even by merely employing a deterministic
encoder network trained without any reparameterization-style tricks. Note that since q(z|X) of
the form (6) is a subset of all possible conditional distributions, our model is still minimizing an
upper bound of the Wasserstein distance between data and model distributions, as previously shown
by Tolstikhin et al. (2017) for AAE (see Appendix A for a full proof).
4 Latent Space Geometry
The latent space geometry of text autoencoders is an important yet understudied problem. Only
when the latent space is smooth and regular can meaningful text manipulations be enacted via simple
modifications of the corresponding latent representations. Here, we discuss in detail the posterior
characteristics of the DAAE, and provide a theoretical analysis of how input perturbations help better
structure the latent space geometry (all proofs are relegated to the appendix).
Assume our perturbations preserve X with some probability (i.e. PC (X|X) > 0). When the support of
C(X1) and C(X2) do not overlap for different training examples X1 6= X2, the encoder can learn to
assign PE(z∣X) = PE(z∣x) for X ∈ C(x), and We are back to the unconstrained posterior scenario
q(z|X) = PE(z|X) (Eq. 6). If C(X1) and C(X2) do intersect, then the latent posterior ofX1 and X2
will have overlapping componentsPE(z∣X) for X ∈ C(xi) ∩ C(x2). For example, if PC(X|x) assigns
a high probability to X that lies close to X (based on some metric over X), then for similar xι and x2,
the high-probability overlap between their perturbations will inherently force their posteriors closer
together in the latent space. This is desirable for learning good representations z, while not guaranteed
by merely minimizing the statistical divergence between Pdata(X) and PG(X) = Ep(z) [PG(X|z)].
Now we formally analyze which type of X-z mappings will be learned by AAE and DAAE, re-
spectively, to achieve global optimality of their training objectives. Unlike previous analyses of
2Transformer models (Vaswani et al., 2017) did not outperform LSTMs on our moderately-sized datasets.
3We actually train E to maximize log D(E(x)) instead of - log(1 - D(E(x))), which is more stable in
practice (Goodfellow et al., 2014). We also tried WGAN (Arjovsky et al., 2017) but did not notice any gains.
3
Under review as a conference paper at ICLR 2020
noise in single-layer networks (Poole et al., 2014), here we study high-capacity encoder/decoder
networks (Schafer & Zimmermann, 2006) with a large number of parameters that are used in modern
sequence models (Devlin et al., 2018; Radford et al., 2019). Throughout, we assume that:
Assumption 1. E is a universal approximator capable of producing any mapping from x’s to z’s.
Assumption 2. G can approximate arbitrary p(x|z) so long as it remains sufficiently Lipschitz
continuous in z. Namely, there exists L > 0 such that all decoder models G obtainable via training
satisfy that for all x ∈ X, z1, z2 ∈ Z: | logpG(x|z1) - logpG(x|z2)| ≤ Lkz1 - z2k.
Following prior analysis of language decoders (Mueller et al., 2017), we assume that G is L-Lipschitz
in its continuous input z (denote this set of possible decoders by GL). When G is implemented as
a RNN or Transformer language model, log pG(x|z) will remain Lipschitz in z if the recurrent or
attention weight matrices have bounded norm. This property is naturally encouraged by popular
training methods that utilize SGD with early stopping and L2 regularization (Zhang et al., 2017).
Note we have not assumed E or G is Lipschitz in x, which would be unreasonable since x stands for
discrete text, and when a few symbols change, the decoder likelihood for the entire sequence can
vary drastically (e.g., G may assign a much higher probability to a grammatical sentence than an
ungrammatical one that only differs by one word). Our discussion is directed to the nature of such
families of log-likelihood functions with a continuous variable z and a discrete variable x.
We further assume an effectively trained discriminator that succeeds in its adversarial task:
Assumption 3. D ensures that the latent encodings zι, ∙∙∙ , Zn of training examples xι, ∙∙∙ , Xn are
Indistinguishablefrom prior samples Z 〜p(z).
For simplicity, We directly assume that zι, ∙∙∙ , Zn are actual samples from P(Z) which are given a
priori. Here, the task of the encoder E is to map given unique training examples to the given latent
points, and the goal of the decoder pg(∙∣∙) is to maximize -LreC under the encoder mapping. The
question now is which one-to-one mapping an optimal encoder/decoder will learn under the AAE
and DAAE objective (Eq. 2 and Eq. 4). We start with the following observation:
Theorem 1.	Forany one-to-one encoder mapping E from {xι, •…,Xn} to {zι, ∙∙∙ , Zn }, the optimal
value ofobjective maxG∈°L n En=ι logPG(xi∣E(xi)) is the same.
Intuitively, this result stems from the fact that the model receives no information about the structure
of x, and xi,…，Xn are simply provided as different symbols. Hence AAE offers no preference
over x-Z couplings, and a random matching in which the Z do not reflect any data structure is equally
good as any other matching (Figure 1, Left). Latent point assignments start to differentiate, however,
once we introduce local input perturbations.
To elucidate how perturbations affect latent space geometry, it helps to first consider a simple setting
with only four examples x1, x2, x3, x4 ∈ X. Again, we consider given latent points Z1, Z2, Z3, Z4
sampled from p(Z), and the encoder/decoder are tasked with learning which x to match with which
Z. As depicted in Figure 1, suppose there are two pairs of x closer together and also two pairs of Z
closer together. Let σ denote the sigmoid function, we have the following conclusion:
Theorem 2.	Let d be a distance metric over X. Suppose x1, x2, x3, x4 satisfy that with some
> 0: d(x1, x2) < , d(x3, x4) < , and d(xi, xj) > for all other (xi, xj) pairs. In addition,
Z1, Z2, Z3, Z4 satisfy that with some 0 < δ < ζ: kZ1 - Z2k < δ, kZ3 - Z4k < δ, and kZi - Zj k > ζ
for all other (Zi, Zj ) pairs. Suppose our perturbation process C reflects local X geometry with:
PC(xi∣Xj) = 1/2 if d(xi, Xj) < e and = 0 otherwise. For δ < L (2log(σ(LZ)) + log 2) and Z >
L log (1∕(√2 - 1)), the denoising objective maxG∈°L n Pn=I Pn=IPC(XjIXi)logPG(xi∣E(xj))
(where n = 4) achieves the largest value when encoder E maps close pairs of X to close pairs of Z.
This entails that DAAE will always prefer to map similar X to similar Z . Note that Theorem 1 still
applies here, and AAE will not prefer any particular X-Z pairing over the other possibilities. We next
generalize beyond the basic four-points scenario to consider n examples of X that are clustered. Here,
we can ask whether this cluster organization will be reflected in the latent space of DAAE.
Theorem 3.	Suppose Xi, ∙∙∙ , Xn are divided into n/K clusters of equal size K, with Si denoting
the cluster index of Xi. Let the perturbation process C be uniform within clusters, i.e. PC(XiIXj) =
1/K if Si = Sj and = 0 otherwise. For a one-to-one encoder mapping E from {xi, ∙∙∙ , Xn}
to {zi, ∙∙∙ ,Zn}, the denoising objective maxG∈°L n Pn=I Pn=I PC (Xj ∣Xi)log PG(XiIE (Xj)) is
upper bounded by: n2 Pijs=Sjlog σ(L∣∣E(Xi) - E(Xj)k) - log K.
4
Under review as a conference paper at ICLR 2020
Theorem 3 provides an upper bound of the DAAE objective that can be achieved by a particular x-z
mapping. This achievable limit is substantially better when examples in the same cluster are mapped
to the latent space in a manner that is well-separated from encodings of other clusters. In other words,
by preserving input space cluster structure in the latent space, DAAE can achieve better objective
values and thus is incentivized to learn such encoder/decoder mappings. An analogous corollary can
be shown for the case when examples X are perturbed to yield additional inputs X not present in the
training data. In this case, the model would aim to map each example and its perturbations as a group
to a compact group of z points well-separated from other groups in the latent space.
In conclusion, our analysis shows that a well-trained DAAE is guaranteed to learn neighborhood-
preserving latent representations, whereas even a perfectly-trained AAE model may learn latent
representations whose geometry fails to reflect similarity in the X space. Empirical experiments in
Section 5.2 confirm that our theory holds in practice.
5	Experiments
We evaluate our proposed model and other text autoencoders on two benchmark datasets: Yelp reviews
and Yahoo answers (Shen et al., 2017; Yang et al., 2017). Detailed descriptions of datasets, training
settings, human evaluations, and additional results/examples can be found in the appendix.
Perturbation Process We randomly delete each word with probability p, so that perturbations of
sentences with more words in common will have a larger overlap. We also tried replacing each word
with a <mask> token or a random word and found that they all brought improvements, but deleting
words worked best. We leave it to future work to explore more sophisticated text perturbations.
Baselines We compare our proposed DAAE with four alternative text autoencoders: adversarially reg-
ularized autoencoder (Zhao et al., 2018, ARAE), β-VAE (Higgins et al., 2017), AAE (Makhzani et al.,
2015), and latent-noising AAE (Rubenstein et al., 2018, LAAE). Similar to our model, the LAAE uses
Gaussian perturbations in the latent space to improve AAE’s latent geometry (rather than perturbations
in the sentence space). However, LAAE requires enforcing an L∖ penalty (λι ∙ ∣∣ logσ2(x)∣∣ι) on the
latent perturbations’ log-variance to prevent them from vanishing. In contrast, input perturbations in
DAAE enable stochastic latent representations without parametric restrictions like Gaussianity.
5.1	Generation-Reconstruction Trade-off
We evaluate various latent variable generative models in terms of both generation quality and
reconstruction accuracy. A strong model should not only generate high quality sentences, but also
learn useful latent variables that capture significant data content. Recent work on text autoencoders
has found an inherent tension between these aims (Bowman et al., 2016; Cifka et al., 2018), yet
only when both goals are met can we successfully manipulate sentences by modifying their latent
representation (in order to produce valid output sentences that retain the semantics of the input).
We compute the BLEU score (Papineni et al., 2002) between input and reconstructed sentences to
measure reconstruction accuracy, and compute Forward/Reverse PPL to measure sentence generation
quality (Zhao et al., 2018; Cifka et al., 2018).4 Forward PPL is the perplexity of a language model
trained on real data and evaluated on generated data. It measures the fluency of the generated text,
but cannot detect the collapsed case where the model repeatedly generates a few common sentences.
Reverse PPL is the perplexity of a language model trained on generated data and evaluated on real
data. It takes into account both the fluency and diversity of the generated text. If a model generates
only a few common sentences, a language model trained on it will exhibit poor PPL on real data.
We thoroughly investigate the performance of different models and their trade-off between generation
and reconstruction. Figure 2 plots reconstruction BLEU (higher is better) vs. Forward/Reverse
PPL (lower is better). The lower right corner indicates an ideal situation where good reconstruction
accuracy and generation quality are both achieved. For models with tunable hyperparameters, we
sweep the full spectrum of their generation-reconstruction trade-off by varying the KL coefficient β
of β-VAE, the log-variance L1 penalty λ1 of LAAE, and the word drop probability p of DAAE.
4 While some use importance sampling estimates of data likelihood to evaluate VAEs (He et al., 2019),
adopting the encoder as a proposal density is not suited for AAE variants, as they are optimized based on
Wasserstein distances rather than likelihoods and lack closed-form posteriors.
5
Under review as a conference paper at ICLR 2020
Figure 2: Generation-reconstruction trade-off of various text autoencoders on Yelp. The “real data” line marks
the PPL of a language model trained and evaluated on real data. We strive to approach the lower right corner
with both high BLEU and low PPL. The grey box identifies hyperparameters we use for respective models in
subsequent experiments. Points of severe collapse (Reverse PPL > 200) are removed from the right panel.
In the left panel, we observe that a standard VAE (β = 1) completely collapses and ignores the latent
variable z, resulting in reconstruction BLEU close to 0. At the other extreme, AAE can achieve
near-perfect reconstruction, but its latent space is highly non-smooth and generated sentences are of
poor quality, indicated by its large Forward PPL. Decreasing β in VAE or introducing latent noises
in AAE provides the model with a similar trade-off curve between reconstruction and generation.
We note that ARAE falls on or above their curves, revealing that it does not fare better than these
methods Qfka et al. (2018) also reported similar findings). Our proposed DAAE provides a trade-off
curve that is strictly superior to other models. With discrete x and a complex encoder, the Gaussian
perturbations added to the latent space by β-VAE and LAAE are not directly related to how the inputs
are encoded. In contrast, input perturbations added by DAAE can constrain the encoder to maintain
coherence between neighboring inputs in an end-to-end fashion and help learn smoother latent space.
The right panel in Figure 2 illustrates that Reverse PPL first drops and then rises as we increase the
degree of regularization/perturbation. This is because when z encodes little information, generations
from prior-sampled z lack enough diversity to cover the real data. Again, DAAE outperforms the
other models which tend to have higher Reverse PPL and lower reconstruction BLEU. In subsequent
experiments, we set β = 0.15 for β-VAE, λ1 = 0.05 for LAAE, and p = 0.3 for DAAE, to ensure
they have strong reconstruction abilities and encode enough information to enable text manipulations.
5.2	Neighborhood Preservation
In this section, we empirically investigate whether
our previous theory holds in practice. That is,
in actual autoencoder models trained on real text
datasets, do sentence perturbations induce latent
space organization that better preserves neighbor-
hood structure in the data space?
Under our word-drop perturbation process, sen-
tences with more words in common are more
likely to be perturbed into one another. This choice
of C approximately encodes sentence similarity
via the normalized edit distance5. Within the test
set, we find both the 10 nearest neighbors of each
sentence based on the normalized edit distance
(denote this set by NNx), as well as the k nearest
neighbors based on Euclidean distance between la-
tent representations (denote this set by NNz). We
Figure 3: Recall rate of 10 nearest neighbors in the
sentence space retrieved by k nearest neighbors in the
latent space on Yelp. ARAE is not plotted here as we
find its recall significantly below other models (< 1%).
5Normalized edit distance ∈ [0, 1] is the Levenshtein distance divided by the max length of two sentences.
6
Under review as a conference paper at ICLR 2020
AAE		DAAE
Source	my waitress katie was fantastic , attentive and personable . my cashier did not smile , barely said hello . the service is fantastic , the food is great . the employees are extremely nice and helpful . our server kaitlyn was also very attentive and pleasant . the crab po boy was also bland and forgettable .	my waitress katie was fantastic , attentive and personable . the manager , linda , was very very attentive and personable . stylist brenda was very friendly , attentive and professional . the manager was also super nice and personable . my server alicia was so sweet and attentive . our waitress ms. taylor was amazing and very knowledgeable .
Source	i have been known to eat two meals a day here . i have eaten here for _num_ years and never had a bad meal ever . i love this joint . i have no desire to ever have it again . you do n’t need to have every possible dish on the menu . i love this arena .	i have been known to eat two meals a day here . you can seriously eat one meal a day here . i was really pleased with our experience here . ive been coming here for years and always have a good experience . i have gone to this place for happy hour for years . we had _num_ ayce dinner buffets for _num_ on a tuesday night .
Table 1: Examples of 5 nearest neighbors in the latent Euclidean space of AAE and DAAE on the Yelp dataset.
Model	ACC	BLEU	PPL
ARAE	17.2	55.7	59.1
β-VAE	49.0	43.5	44.4
AAE	9.7	82.2	37.4
LAAE	43.6	37.5	55.8
DAAE	50.3	54.3	32.0
			
β-VAE is better: 25		DAAE is better: 48	
both good: 26	both bad: 67		n/a: 34
Table 2: Above: automatic evaluations of vector arith-
metic for tense inversion. Below: human evaluation
statistics of our model vs. the closest baseline β-VAE.
Model	ACC	BLEU	PPL
Shen et al. (2017)	81.7	12.4	38.4
±v	7.2	86.0	33.7
AAE	±1.5v	25.1	59.6	59.5
±2v	57.5	27.4	139.8
±v	36.2	40.9	40.0
DAAE ±1.5v	73.6	18.2	54.1
±2v	91.8	7.3	61.8
Table 3: Automatic evaluations of vector arithmetic
for sentiment transfer. Accuracy (ACC) is measured
by a sentiment classifier. The model of Shen et al.
(2017) is specifically trained for sentiment transfer
with labeled data, while our text autoencoders are not.
compute the recall rate |NNx ∩ NNz| / |NNx|, which indicates how well local neighborhoods are
preserved in the latent space of different models.
Figures 3 shows that DAAE consistently gives the highest recall, about 1.5~2 times that of AAE,
implying that input perturbations have a substantial effect on shaping the latent space geometry.
Tables 1 presents the five nearest neighbors found by AAE and DAAE in their latent space for
example test set sentences. The AAE sometimes encodes entirely unrelated sentences close together,
while the latent space geometry of the DAAE is structured based on key words such as “attentive”
and “personable”, and tends to group sentences with similar semantics close together.
5.3	Applications to Controllable Text Generation
5.3.1	Style Transfer via Vector Arithmetic
Mikolov et al. (2013) previously discovered that word embeddings from unsupervised learning
can capture linguistic relationships via simple arithmetic. A canonical example is the embedding
arithmetic “King” - “Man” + “Woman” ≈ “Queen”. Here, we use the Yelp dataset with tense
and sentiment as two example attributes (Hu et al., 2017; Shen et al., 2017) to investigate whether
analogous structure emerges in the latent space of our sentence-level models.
Tense We use the Stanford Parser to extract the main verb of a sentence and determine the sentence
tense based on its part-of-speech tag. We compute a single “tense vector” by averaging the latent
code z separately for 100 past tense sentences and 100 present tense sentences in the dev set, and
then calculating the difference between the two. Given a sentence from the test set, we attempt to
change its tense from past to present or from present to past through simple addition/subtraction of
the tense vector. More precisely, a source sentence x is first is encoded to z = E(x), and then the
tense-modified sentence is produced via G(z ± v), where v ∈ Rd denotes the fixed tense vector.
7
Under review as a conference paper at ICLR 2020
Input
ARAE
β-VAE
AAE
LAAE
DAAE
i enjoy hanging out in their hookah lounge .
i enjoy hanging out in their 25th lounge .
i made up out in the backyard springs salad .
i enjoy hanging out in their brooklyn lounge .
i enjoy hanging out in the customized and play .
i enjoyed hanging out in their hookah lounge .
had they informed me of the charge i would n’t have waited .
amazing egg of the may i actually !
had they help me of the charge i would n’t have waited .
have they informed me of the charge i would n’t have waited .
they are girl ( the number so i would n’t be forever .
they have informed me of the charge i have n’t waited .
Table 4: Examples of vector arithmetic for tense inversion.
	AAE	DAAE
Input	the food is entirely tasteless and slimy .	the food is entirely tasteless and slimy .
+v	the food is entirely tasteless and slimy .	the food is tremendous and fresh .
+1.5v	the food is entirely tasteless and slimy .	the food is sensational and fresh .
+2v	the food is entirely and beef .	the food is gigantic .
Input	i really love the authentic food and will come back again .	i really love the authentic food and will come back again .
-v	i really love the authentic food and will come back again .	i really love the authentic food and will never come back again .
-1.5v	i really but the authentic food and will come back again .	i really do not like the food and will never come back again .
-2v	i really but the worst food but will never come back again .	i really did not believe the pretentious service and will never go back .
Table 5: Examples of vector arithmetic for sentiment transfer.
To quantitatively compare different models, we compute their tense transfer accuracy as measured
by the parser, the output BLEU with the input sentence, and output (forward) PPL evaluated by
a language model. DAAE achieves the highest accuracy, lowest PPL, and relatively high BLEU
(Table 2, Above), indicating that the output sentences produced by our model are more likely to be
of high quality and of the proper tense, while remaining similar to the source sentence. A human
evaluation on 200 test sentences (100 past and 100 present, details in Appendix G) suggests that
DAAE outperforms β-VAE twice as often as it is outperformed, and our model successfully inverts
tense for (48 + 26)/(200 - 34) = 44.6% of sentences, 13.8% more than β-VAE (Table 2, Below).
Tables 4 and J.2 show the results of adding or subtracting this fixed latent vector offset under different
models. DAAE can successfully change “enjoy” to “enjoyed”, or change the subjunctive mood
to declarative mood and adjust the word order. Other baselines either fail to alter the tense, or
undesirably change the semantic meaning of the source sentence (e.g. “enjoy” to “made”).
Sentiment Following the same procedure used to alter tense, we compute a “sentiment vector”
v from 100 negative and positive sentences and use it to change the sentiment of test sentences.
Table 3 reports the automatic evaluations, and Tables 5 and J.3 show examples generated by AAE
and DAAE. Scaling ±v to ±1.5v and ±2v, we find that the resulting sentences get more and more
positive/negative. However, the PPL for AAE increases rapidly with this scaling factor, indicating
that the sentences become unnatural when their encodings have a large offset. DAAE enjoys a much
smoother latent space than AAE. Despite the fact that no sentiment labels are provided during training
(a more challenging task than previous style transfer models (Shen et al., 2017)), DAAE with ±1.5v
is able to transfer sentiment fairly well.
5.3.2 Sentence Interpolation via Latent Space Traversal
We also study sentence interpolation by traversing the latent space of text autoencoders. Given
two input sentences, we encode them to z1 , z2 and decode from tz1 + (1 - t)z2 (0 ≤ t ≤ 1) to
obtain interpolated sentences. Ideally this should produce fluent sentences with gradual semantic
change (Bowman et al., 2016). Table 6 shows two examples from the Yelp dataset, where it is clear
that DAAE produces more coherent and natural interpolations than AAE. Table J.4 in the appendix
shows two difficult examples from the Yahoo dataset, where we interpolate between dissimilar
sentences. While it is challenging to generate semantically correct sentences in these cases, the latent
space of our model exhibits continuity on topic and syntactic structure.
6 Conclusion
This paper proposed DAAE for generative text modeling. As revealed in previous work (Devlin et al.,
2018; Lample et al., 2018), we find that denoising techniques can greatly improve the learned text
representations. We provide a theoretical explanation for this phenomenon by analyzing the latent
8
Under review as a conference paper at ICLR 2020
Input 1 Input 2	it ’s so much better than the other chinese food places in this area . fried dumplings are a must . better than other places .	the fried dumplings are a must if you ever visit this place .
AAE	it ’s so much better than the other chinese	food places	in this area .	fried dumplings are	a must . it ’s so much better than the other food places in this	area .	fried dumplings are	a must . better , much better .	the dumplings are a must if you worst . better than other places .	the fried dumplings	are a must if you ever this	place	. better than other places .	the fried dumplings	are a must if you ever visit this	place .
DAAE	it ’s so much better than the other chinese food places	in this area .	fried dumplings are a must . it ’s much better than the other chinese places in this area	.	fried dumplings are a must visit . better than the other chinese places in this area .	fried dumplings are a must in this	place . better than the other places in charlotte .	the fried dumplings are a must we	ever visit this . better than other places .	the fried dumplings are a must if we ever visit this place .
Table 6: Interpolations between two input sentences generated by AAE and our model on the Yelp dataset.
space geometry arisen from input perturbations. Our proposed model substantially outperforms other
text autoencoders, and demonstrates potential for various text manipulations via vector operations.
Future work may investigate superior perturbation strategies and additional properties of latent space
geometry to provide finer control over the text generated using autoencoder models.
9
Under review as a conference paper at ICLR 2020
References
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein generative adversarial networks.
In International Conference on Machine Learning, pp. 214-223, 2017.
Yoshua Bengio, Li Yao, Guillaume Alain, and Pascal Vincent. Generalized denoising auto-encoders
as generative models. In Advances in Neural Information Processing Systems, pp. 899-907, 2013.
Samuel R Bowman, Luke Vilnis, Oriol Vinyals, Andrew M Dai, Rafal Jozefowicz, and Samy Bengio.
Generating sentences from a continuous space. In Conference on Computational Natural Language
Learning, 2016.
Xi Chen, Diederik P Kingma, Tim Salimans, Yan Duan, Prafulla Dhariwal, John Schulman, Ilya
Sutskever, and Pieter Abbeel. Variational lossy autoencoder. arXiv preprint arXiv:1611.02731,
2016.
Ondrej Cifka, Aliaksei Severyn, EnriqUe Alfonseca, and Katja Filippova. Eval all, trust a few, do
wrong to none: Comparing sentence generation models. arXiv preprint arXiv:1804.07972, 2018.
Antonia Creswell and Anil Anthony Bharath. Denoising adversarial autoencoders. IEEE transactions
on neural networks and learning systems, (99):1-17, 2018.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural informa-
tion processing systems, pp. 2672-2680, 2014.
Junxian He, Daniel Spokoyny, Graham Neubig, and Taylor Berg-Kirkpatrick. Lagging inference
networks and posterior collapse in variational autoencoders. arXiv preprint arXiv:1901.05534,
2019.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a
constrained variational framework. In International Conference on Learning Representations,
volume 3, 2017.
Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, and Eric P Xing. Toward controlled
generation of text. In Proceedings of the 34th International Conference on Machine Learning-
Volume 70, pp. 1587-1596. JMLR. org, 2017.
Daniel Im Jiwoong Im, Sungjin Ahn, Roland Memisevic, and Yoshua Bengio. Denoising criterion for
variational auto-encoding framework. In Thirty-First AAAI Conference on Artificial Intelligence,
2017.
Yoon Kim, Sam Wiseman, Andrew C Miller, David Sontag, and Alexander M Rush. Semi-amortized
variational autoencoders. arXiv preprint arXiv:1802.02550, 2018.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In International Conference
on Learning Representations, 2014.
Guillaume Lample, Myle Ott, Alexis Conneau, Ludovic Denoyer, and Marc’Aurelio Ranzato. Phrase-
based & neural unsupervised machine translation. arXiv preprint arXiv:1804.07755, 2018.
Lajanugen Logeswaran, Honglak Lee, and Samy Bengio. Content preserving text generation with
attribute controls. In Advances in Neural Information Processing Systems, pp. 5103-5113, 2018.
Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey. Adversarial
autoencoders. arXiv preprint arXiv:1511.05644, 2015.
10
Under review as a conference paper at ICLR 2020
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. Linguistic regularities in continuous space word
representations. In Proceedings of the 2013 Conference of the North American Chapter of the
Associationfor Computational Linguistics: Human Language Technologies, pp. 746-751, 2013.
Jonas Mueller, David Gifford, and Tommi Jaakkola. Sequence to better sequence: continuous revision
of combinatorial structures. In Proceedings of the 34th International Conference on Machine
Learning-Volume 70, pp. 2536-2544. JMLR. org, 2017.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of the 40th annual meeting on association for
computational linguistics, pp. 311-318. Association for Computational Linguistics, 2002.
Ben Poole, Jascha Sohl-Dickstein, and Surya Ganguli. Analyzing noise in autoencoders and deep
networks. arXiv preprint arXiv:1406.1831, 2014.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
models are unsupervised multitask learners. OpenAI Blog, 1:8, 2019.
Paul K Rubenstein, Bernhard Schoelkopf, and Ilya Tolstikhin. On the latent space of wasserstein
auto-encoders. arXiv preprint arXiv:1802.03761, 2018.
Anton Maximilian Schafer and Hans Georg Zimmermann. Recurrent neural networks are universal
approximators. In International Conference on Artificial Neural Networks, pp. 632-640. Springer,
2006.
Tianxiao Shen, Tao Lei, Regina Barzilay, and Tommi Jaakkola. Style transfer from non-parallel text
by cross-alignment. In Advances in neural information processing systems, pp. 6830-6841, 2017.
Sandeep Subramanian, Guillaume Lample, Eric Michael Smith, Ludovic Denoyer, Marc’Aurelio Ran-
zato, and Y-Lan Boureau. Multiple-attribute text style transfer. arXiv preprint arXiv:1811.00552,
2018.
Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bernhard Schoelkopf. Wasserstein auto-encoders.
arXiv preprint arXiv:1711.01558, 2017.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,匕UkaSz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998-6008, 2017.
Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and
composing robust features with denoising autoencoders. In Proceedings of the 25th international
conference on Machine learning, pp. 1096-1103. ACM, 2008.
Zichao Yang, Zhiting Hu, Ruslan Salakhutdinov, and Taylor Berg-Kirkpatrick. Improved variational
autoencoders for text modeling using dilated convolutions. In Proceedings of the 34th International
Conference on Machine Learning-Volume 70, pp. 3881-3890. JMLR. org, 2017.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understand-
ing deep learning requires rethinking generalization. In International Conference on Learning
Representations, 2017.
Junbo Zhao, Yoon Kim, Kelly Zhang, Alexander M Rush, Yann LeCun, et al. Adversarially
regularized autoencoders. In Proceedings of the 35th International Conference on Machine
Learning, 2018.
11
Under review as a conference paper at ICLR 2020
A Wasserstein Distance
The AAE objective can be connected to a relaxed form of the Wasserstein distance between model
and data distributions (Tolstikhin et al., 2017). Specifically, for cost function c(∙, ∙) : X ×X → R
and deterministic decoder mapping G : Z → X, it holds that:
inf
Γ∈P(x〜Pdata ,y〜PG )
E(x,y)〜r[c(X,y)]
inf
q(z|x):q(z)=P(z)
EPdata(x)Eq(z|x) [c(x, G(z))]
(7)
where the minimization over couplings Γ with marginals pdata and pG can be replaced with minimiza-
tion over conditional distributions q(z|x) whose marginal q(z) = EPdata(x) [q(z|x)] matches the latent
prior distribution p(z). Relaxing this marginal constraint via a divergence penalty D(q(z)kp(z))
estimated by adversarial training, one recovers the AAE objective (Eq. 1). In particular, AAE on
discrete x with the cross-entropy loss is minimizing an upper bound of the total variation distance
between pdata and pG, with c chosen as the indicator cost function (Zhao et al., 2018).
Our model is optimizing over conditional distributions q(z|x) of the form (6), a subset of all possible
conditional distributions. Thus, after introducing input perturbations, our method is still minimizing
an upper bound of the Wasserstein distance between pdata and pG described in (7).
B	Proof of Theorem 1
Theorem 1. Forany one-to-one encoder mapping E from {xι, •…,Xn} to {zι, ∙∙∙ , Zn } ,the optimal
value ofobjective maxG∈°L n En=I logPG(xi∣E(xi)) is the same.
Proof. Consider two encoder matchings xi to zα(i) and xi to zβ(i), where both α and β are permu-
tations of the indices {1, . . . , n}. Suppose Gα is the optimal decoder model for the first matching
(with permutations α). This implies
n
PGa = argmaxV"log PG(xi∣Zɑ(i))
G∈GL i=1
Now let pGβ (xi|zj) = pGα (xβα-1(i) |zj), ∀i, j. Then Gβ can achieve exactly the same log-likelihood
objective value for matching β as Gα for matching α, while still respecting the Lipschitz constraint.
□
C Proof of Theorem 2
Theorem 2. Let d be a distance metric over X. Suppose x1 , x2 , x3, x4 satisfy that with some
> 0: d(x1, x2) < , d(x3, x4) < , and d(xi, xj) > for all other (xi, xj) pairs. In addition,
z1, z2, z3, z4 satisfy that with some 0 < δ < ζ: kz1 - z2k < δ, kz3 - z4k < δ, and kzi - zj k > ζ
for all other (zi, zj ) pairs. Suppose our perturbation process C reflects local X geometry with:
PC(xi∣Xj) = 1/2 if d(xi, Xj) < e and = 0 otherwise. For δ < L (2log(σ(LZ)) + log 2) and Z >
L log (1∕(√2 - 1)), the denoising objective maxG∈°L 1 Pn=1 Pj=IPC(XjIXi)logPG(xi∣E(xj))
(where n = 4) achieves the largest value when encoder E maps close pairs of X to close pairs of z.
Proof. Let [n] denote {1, . . . , n}, and assume without loss of generality that the encoder E maps
each Xi to zi. We also define A = {1, 2}, B = {3, 4} as the two X-pairs that lie close together. For
our choice of C(X), the training objective to be maximized is:
E logPG(XiIE(Xj))+ E logPG(Xk |E(X'))
i,j三A	k,'∈B
=E logPG(XiIzj) + E logPG(Xk ∣Z')	(8)
i,j三A	k,'∈B
The remainder of our proof is split into two cases:
Case 1. ∣∣zj - z`∣∣ > Z for j ∈ A,' ∈ B
12
Under review as a conference paper at ICLR 2020
Case2. ||zj - z`|| <δ for j ∈ A,' ∈ B
Under Case 1, x points that lie far apart also have z encodings that remain far apart. Under Case 2, x
points that lie far apart have z encodings that lie close together. We complete the proof by showing
that the achievable objective value in Case 2 is strictly worse than in Case 1, and thus an optimal
encoder/decoder pair would avoid the x, z matching that leads to Case 2.
In Case 1 where ||zj - z` || > ζ for all j ∈ A, ` ∈ B , we can lower bound the training objective (8)
by choosing:
(,、∫(1-Y )/2	ifi,j ∈ A or i,j ∈ B	6
pG(xi|zj) =	(9)
J [γ∕2	otherwise
with γ = σ(-LZ) ∈ (0,1), where σ(∙) denotes the sigmoid function. Note that this ensures
pG(xi|zj) = 1 for each j ∈ [4], and does not violate the Lipschitz condition from Assumption 2
i∈[4]
since:
= 0	if j, ` ∈ A or j, ` ∈ B
| logPG(XiIzj) - logPG(Xi同)| 1 ≤ log((1-γ)∕γ)	otherwise
and thus remains ≤ L∣∣zj- - z'|| when Y = σ(-LZ) ≥ σ(-L∣∣zj- - z'∣∣) = 1∕[1 + exp(L∣∣zj- - z'∣∣)].
Plugging thePG(X|z) assignment from (9) into (8), we see that an optimal decoder can obtain training
objective value ≥ 8log [σ(LZ)/2] in Case 1 where ∣∣zj- - z'|| > Z, ∀j ∈ A,' ∈ B.
Next, we consider the alternative case where ||zj - z` || < δ for j ∈ A, ` ∈ B .
For i, j ∈ A and for all ` ∈ B, we have:
logPG(XiIzj) ≤ logPg(Xi∣z') + LIIzj - z'∣∣	by Assumption 2
≤ logPg(xi∣z') + Lδ
≤ Lδ + log 1 - EPG(Xklz`)	since PkPG(Xklz`) ≤ 1
k∈B
Continuing from (8), the overall training objective in this case is thus:
logPG(XiIzj) +	log PG(Xk lz`)
i,j∈A	k,'∈B
≤ 4Lδ + J2 min log 1 - ∑2PG(Xk lz`) + ɪ2 log PG(Xk Iz')
' '∈B
i,j∈A	L k∈B	」	k,'∈B
≤ 4Lδ + X 2log I 1 - X PG(XkIz') I + X logPG(XkIz')
'∈B L	∖ k∈B	) k∈B	_
≤ 4Lδ - 12log2
using the fact that the optimal decoder for the bound in this case is: PG(Xk Iz') = 1∕4 for all k, ` ∈ B.
Finally, plugging our range for δ stated in the Theorem 2, it shows that the best achievable objective
value in Case 2 is strictly worse than the objective value achievable in Case 1. Thus, the optimal
encoder/decoder pair under the AAE with perturbed X will always prefer the matching between
{X1 , . . . , X4 } and {z1 , . . . , z4 } that ensures nearby Xi are encoded to nearby zi (corresponding to
Case 1).	口
D Proof of Theorem 3
Theorem 3. Suppose Xi, ∙∙∙ ,Xn are divided into n/K clusters of equal size K, with Si denoting
the cluster index of Xi. Let the perturbation process C be uniform within clusters, i.e. PC(XiIXj) =
1/K if Si = Sj and = 0 otherwise. For a one-to-one encoder mapping E from {xi, ∙∙∙ ,Xn}
to {zi,…，zn}, the denoising objective maxG∈GL 1 Pn=i Pn=IPC(XjIXi)logPG(XiIE(Xj)) is
upper bounded by: n2 Pij:Si=Sjlog σ(L∣∣E(Xi) - E(Xj)k) - log K.
13
Under review as a conference paper at ICLR 2020
Proof. Without loss of generality, let E(xi) = zi for notational convenience. We consider what is
the optimal decoder probability assignment pG (xi |zj) under the Lipschitz constraint 2.
The objective of the AAE with perturbed x is to maximize:
nXXPC(XjE)IOgPG(xiιE(Xj)) = nKχ χ logpG(xi|zj)
n i j	n j i:Si=Sj
We first show that the optimal pg(∙∣∙) will satisfy that the same probability is assigned within a
cluster, i.e. p(Xi|zj) = p(Xk|zj) for all i, k s.t. Si = Sk. If not, let Psj = Pi:S =spG(Xi|zj), and
we reassign pG0 (Xi|zj) = PSij/K. Then G0 still conforms to the Lipschitz constraint if G meets it,
and G0 will have a larger target value than G.
Now let us define Pj = PiS=Sj PG(xi∣Zj) = K ∙PG(Xj |zj-) (0 ≤ Pj ≤ 1). The objective becomes:
mPGxnKX X logPG(XiIzj) = mPGx 1XIogPG(XjIzj)
G	j i:Si=Sj	G j
max
pG
1X log Pj- logK
j
max
pG
2112 X X(Iog pi + log pj) - log K
≤
212 xxmax(log Pi + log Pj ) - log K
Consider each term maxpG (log Pi + logPj): when Si = Sj, this term can achieve the maximum
value 0 by assigning Pi = Pj = 1; when Si 6= Sj , the Lipschitz constraint ensures that:
log(1 - Pi) ≥ log Pj - Lkzi - zjk
log(1 - Pj) ≥ log Pi - Lkzi - zjk
Therefore:
log Pi + log Pj ≤ 2 log σ(Lkzi - zjk)
Overall, we thus have:
max1K X X logPG(XiIzj) ≤ / X	logσ(Lkzi - zjk) - log K
pG 1 j i:Si=Sj	1 i,j:Si 6=Sj
□
E Datasets
The Yelp dataset is from Shen et al. (2017), which has 444K/63K/127K sentences of less than 16
words in length as train/dev/test sets, with a vocabulary of 10K. It was originally divided into positive
and negative sentences for style transfer between them. Here we discard the sentiment label and
let the model learn from all sentences indiscriminately. Our second dataset of Yahoo answers is
from Yang et al. (2017). It was originally document-level. We perform sentence segmentation and
keep sentences with length from 2 to 50 words. The resulting dataset has 495K/49K/50K sentences
for train/dev/test sets, with vocabulary size 20K.
F	Experimental Details
We use the same architecture to implement all models with different objectives. The encoder E,
generator G, and the language model used to compute Forward/Reverse PPL are one-layer LSTMs
with hidden dimension 1024 and word embedding dimension 512. The last hidden state of the
encoder is projected into 128/256 dimensions to produce the latent code z for Yelp/Yahoo datasets
14
Under review as a conference paper at ICLR 2020
respectively, which is then projected and added with input word embeddings fed to the generator.
The discriminator D is an MLP with one hidden layer of size 512. λ of AAE based models is set
to 10 to ensure the latent codes are indistinguishable from the prior. All models are trained via the
Adam optimizer (Kingma & Ba, 2014) with learning rate 0.0005, β1 = 0.5, β2 = 0.999. At test time,
encoder-side perturbations are disabled, and we use greedy decoding to generate x from z.
G Human Evaluation
For the tense transfer experiment, the human annotator is presented with a source sentence and
two outputs (one from each approach, presented in random order) and asked to judge which one
successfully changes the tense while being faithful to the source, or whether both are good/bad, or if
the input is not suitable to have its tense inverted. We collect labels from two human annotators and
if they disagree, we further solicit a label from the third annotator.
H	Generation-Reconstruction Results on the Yahoo Dataset
100
β
Op= 1
p 1	20	40	60	80
Reconstruction BLEU
60 ]
0	20	40	60	80	100
Reconstruction BLEU
Figure H.1: Generation-reconstruction trade-off of various text autoencoders on Yahoo. The “real data” line
marks the PPL of a language model trained and evaluated on real data. We strive to approach the lower right
corner with both high BLEU and low PPL. The grey box identifies hyperparameters we use for respective models
in subsequent experiments. Points of severe collapse (Reverse PPL > 300) are removed from the right panel.
I Neighborhood Preservation
→- B-VAE
→- AAE
→- LAAE
→- DAAE
—Untrained Encoder
→- AE
→- DAE
(％=33α
10
20
50
Oo
10
0 5 0 5 0 5
3 2 2 1 1
20
50
W 5 O 5 O 5 O
4 3 3 2 2 1 1
S -sφ^
Figure I.2: Recall rate of 10 nearest neighbors in the sentence space retrieved by k nearest neighbors in the
latent space on the Yelp and Yahoo datasets. Here we include non-generative models AE and DAE. We find that
an untrained RNN encoder from random initialization has a good recall rate, and we suspect that SGD training
of vanilla AE towards only the reconstruction loss will not overturn this initial bias. Note that denoising still
improves neighborhood preservation in this case. Also note that DAAE has the highest recall rate among all
generative models that have a latent prior imposed.
15
Under review as a conference paper at ICLR 2020
J Additional Examples
Source	how many gospels are there that were n’t included in the bible ?
5-NN by AAE	there are no other gospels that were n’t included in the bible . how many permutations are there for the letters in the word _UNK ’ ? anyone else picked up any of the _UNK in the film ? what ’s the significance of the number 40 in the bible ? how many pieces of ribbon were used in the _UNK act ?
5-NN by DAAE	there are no other gospels that were n’t included in the bible . how many litres of water is there in the sea ? how many _UNK gods are there in the classroom ? how many pieces of ribbon were used in the _UNK act ? how many times have you been grounded in the last year ?
Source	how do i change colors in new yahoo mail beta ?
5-NN by AAE	how should you present yourself at a _UNK speaking exam ? how can i learn to be a hip hop producer ? how can i create a _UNK web on the internet ? how can i change my _UNK for female not male ? what should you look for in buying your first cello ?
5-NN by DAAE	how do i change that back to english ? is it possible to _UNK a yahoo account ? how do i change my yahoo toolbar options ? what should you look for in buying your first cello ? who do you think should go number one in the baseball fantasy draft , pujols or _UNK ?
Table J.1: Examples of nearest neighbors in the latent Euclidean space of AAE and DAAE on Yahoo dataset.
Input ARAE β-VAE AAE LAAE DAAE	the staff is rude and the dr. does not spend time with you . the staff is rude and the dr. does not worth two with you . the staff was rude and the dr. did not spend time with your attitude . the staff was rude and the dr. does not spend time with you . the staff was rude and the dr. is even for another of her entertained . the staff was rude and the dr. did not make time with you .	slow service , the food tasted like last night ’s leftovers . slow service , the food tasted like last night ’s leftovers . slow service , the food tastes like last place serves . slow service , the food tasted like last night ’s leftovers . slow service , the food , on this burger spot ! slow service , the food tastes like last night 	
Input	they are the worst credit union in arizona .	i reported this twice and nothing was done .
ARAE	they are the worst bank credit in arizona .	i swear this twice and nothing was done .
β-VAE	they were the worst credit union in my book .	i ’ve gone here and nothing too .
AAE	they are the worst credit union in arizona .	i reported this twice and nothing was done .
LAAE	they were the worst credit union in my heart .	i dislike this twice so pleasant guy .
DAAE	they were the worst credit union in arizona ever .	i hate this pizza and nothing done .
Table J.2: Additional examples of vector arithmetic for tense inversion.
16
Under review as a conference paper at ICLR 2020
AAE		DAAE
Input	this woman was extremely rude to me .	this woman was extremely rude to me .
+v	this woman was extremely rude to me .	this woman was extremely nice .
+1.5v	this woman was extremely rude to baby .	this staff was amazing .
+2v	this woman was extremely rude to muffins .	this staff is amazing .
Input	my boyfriend said his pizza was basic and bland also .	my boyfriend said his pizza was basic and bland also .
+v	my boyfriend said his pizza was basic and tasty also .	my boyfriend said his pizza is also excellent .
+1.5v	my shared said friday pizza was basic and tasty also .	my boyfriend and pizza is excellent also .
+2v	my shared got pizza pasta was basic and tasty also .	my smoked pizza is excellent and also exceptional .
Input	the stew is quite inexpensive and very tasty .	the stew is quite inexpensive and very tasty .
-v	the stew is quite inexpensive and very tasty .	the stew is quite an inexpensive and very large .
-1.5v	the stew is quite inexpensive and very very tasteless .	the stew is quite a bit overpriced and very fairly brown .
-2v	the - WaS being slow - very very tasteless .	the hostess was quite impossible in an expensive and very few customers .
Input	the patrons all looked happy and relaxed .	the patrons all looked happy and relaxed .
-v	the patrons all looked happy and relaxed .	the patrons all helped us were happy and relaxed .
-1.5v	the patrons all just happy and smelled .	the patrons that all seemed around and left very stressed .
-2v	the patrons all just happy and smelled .	the patrons actually kept us all looked long and was annoyed .
Table J.3: Additional examples of vector arithmetic for sentiment transfer.
Input 1 what language should i learn to be more competitive in today ’s global culture ?
Input 2 what languages do you speak ?
AAE what language should i learn to be more competitive in today ’s global culture ?
what language should i learn to be more competitive in today ’s global culture ?
what language should you speak ?
what languages do you speak ?
what languages do you speak ?
DAAE what language should i learn to be more competitive in today ’s global culture ?
what language should i learn to be competitive today in arabic ’s culture ?
what languages do you learn to be english culture ?
what languages do you learn ?
what languages do you speak ?
Input 1 i believe angels exist .
Input 2 if you were a character from a movie , who would it be and why ?
AAE i believe angels exist .
i believe angels - there was the exist exist .
i believe in tsunami romeo or <unk> i think would it exist as the world population .
if you were a character from me in this , would we it be ( why !
if you were a character from a movie , who would it be and why ?
DAAE i believe angels exist .
i believe angels exist in the evolution .
what did <unk> worship by in <unk> universe ?
if you were your character from a bible , it will be why ?
if you were a character from a movie , who would it be and why ?
Table J.4: Interpolations between two input sentences generated by AAE and our model on the Yahoo dataset.
17