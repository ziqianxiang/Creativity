Under review as a conference paper at ICLR 2020
Towards Understanding the Transferability
of Deep Representations
Anonymous authors
Paper under double-blind review
Ab stract
Deep neural networks trained on a wide range of datasets demonstrate impressive
transferability. Deep features appear general in that they are applicable to many
datasets and tasks. Such property is in prevalent use in real-world applications. A
neural network pretrained on large datasets, such as ImageNet, can significantly
boost generalization and accelerate training if fine-tuned to a smaller target dataset.
Despite its pervasiveness, few effort has been devoted to uncovering the reason
of transferability in deep feature representations. This paper tries to understand
transferability from the perspectives of improved generalization, optimization and
the feasibility of transferability. We demonstrate that 1) Transferred models tend to
find flatter minima, since their weight matrices stay close to the original flat region
of pretrained parameters when transferred to a similar target dataset; 2) Transferred
representations make the loss landscape more favorable with improved Lipschitz-
ness, which accelerates and stabilizes training substantially. The improvement
largely attributes to the fact that the principal component of gradient is suppressed
in the pretrained parameters, thus stabilizing the magnitude of gradient in back-
propagation. 3) The feasibility of transferability is related to the similarity of both
input and label. And a surprising discovery is that the feasibility is also impacted
by the training stages in that the transferability first increases during training, and
then declines. We further provide a theoretical analysis to verify our observations.
1	Introduction
The last decade has witnessed the enormous success of deep neural networks in a wide range of
applications. Deep learning has made unprecedented advances in many research fields, including
computer vision, natural language processing, and robotics. Such great achievement largely attributes
to several desirable properties of deep neural networks. One of the most prominent properties is the
transferability of deep feature representations.
Transferability is basically the desirable phenomenon that deep feature representations learned from
one dataset can benefit optimization and generalization on different datasets or even different tasks,
e.g. from real images to synthesized images, and from image recognition to object detection (Yosinski
et al., 2014). This is essentially different from traditional learning techniques and is often regarded as
one of the parallels between deep neural networks and human learning mechanisms.
In real-world applications, practitioners harness transferability to overcome various difficulties. Deep
networks pretrained on large datasets are in prevalent use as general-purpose feature extractors for
downstream tasks (Donahue et al., 2014). For small datasets, a standard practice is to fine-tune a
model transferred from large-scale dataset such as ImageNet (Russakovsky et al., 2015) to avoid
over-fitting. For complicated tasks such as object detection, semantic segmentation and landmark
localization, ImageNet pretrained networks accelerate training process substantially (Oquab et al.,
2014; He et al., 2018). In the NLP field, advances in unsupervised pretrained representations have
enabled remarkable improvement in downstream tasks (Vaswani et al., 2017; Devlin et al., 2019).
Despite its practical success, few efforts have been devoted to uncovering the underlying mechanism
of transferability. Intuitively, deep neural networks are capable of preserving the knowledge learned
on one dataset after training on another similar dataset (Yosinski et al., 2014; Li et al., 2018b; 2019).
This is even true for notably different datasets or apparently different tasks. Another line of works
have observed several detailed phenomena in the transfer learning of deep networks (Kirkpatrick
1
Under review as a conference paper at ICLR 2020
Caltech-101
CUB-200
Stanford-CarS
Figure 1: Examples of object recognition datasets. Caltech-101 (Fei-Fei et al., 2004) is more similar
to ImageNet with 101 classes of common objects. CUB-200 (Welinder et al., 2010) and Stanford Cars
(Krause et al., 2013) are essentially different with various kinds of birds and vehicles, respectively.
et al., 2016; Kornblith et al., 2019), yet it remains unclear why and how the transferred representations
are beneficial to the generalization and optimization perspectives of deep networks.
The present study addresses this important problem from several new perspectives. We first probe
into how pretrained knowledge benefits generalization. Results indicate that models fine-tuned on
target datasets similar to the pretrained dataset tend to stay close to the transferred parameters. In this
sense, transferring from a similar dataset makes fine-tuned parameters stay in the flat region around
the pretrained parameters, leading to flatter minima than training from scratch.
Another key to transferability is that transferred features make the optimization landscape significantly
improved with better Lipschitzness, which eases optimization. Results show that the landscapes with
transferred features are smoother and more predictable, fundamentally stabilizing and accelerating
training especially at the early stages of training. This is further enhanced by the proper scaling of
gradient in back-propagation. The principal component of gradient is suppressed in the transferred
weight matrices, controlling the magnitude of gradient and smoothing the loss landscapes.
We also investigate a common concern raised by practitioners: when is transfer learning helpful to
target tasks? We test the transferability of pretrained networks with varying inputs and labels. Instead
of the similarity between pretrained and target inputs, what really matters is the similarity between
the pretrained and target tasks, i.e. both inputs and labels are required to be sufficiently similar. We
also investigate the relationship between pretraining epoch and transferability. Surprisingly, although
accuracy on the pretrained dataset increases throughout training, transferability first increases at the
beginning and then decreases significantly as pretraining proceeds.
Finally, this paper gives a theoretical analysis based on two-layer fully connected networks. Theoreti-
cal results consistently justify our empirical discoveries. The analysis here also casts light on deeper
networks. We believe the mechanism of transferability is the fundamental property of deep neural
networks and the in-depth understanding presented here may stimulate further algorithmic advances.
2 Related Work
There exists extensive literature on transferring pretrained representations to learn an accurate model
on a target dataset. Donahue et al. (2014) employed a brand-new label predictor to classify features
extracted by the pre-trained feature extractor at different layers of AlexNet (Krizhevsky et al., 2012).
Oquab et al. (2014) showed deep features can benefit object detection tasks despite the fact that they
are trained for image classification. Ge & Yu (2017) introduced a selective joint fine-tuning scheme
for improving the performance of deep learning tasks under the scenario of insufficient training data.
The enormous success of the transferability of deep networks in applications stimulates empirical
studies on fine-tuning and transferability. Yosinski et al. (2014) observed the transferability of deep
feature representations decreases as the discrepancy between pretrained task and target task increases
and gets worse in higher layers. Another phenomenon of catastrophic forgetting as discovered by
Kirkpatrick et al. (2016) describes the loss of pretrained knowledge when fitting to distant tasks. Huh
et al. (2016) delved into the influence of ImageNet pretrained features by pretraining on various
2
Under review as a conference paper at ICLR 2020
subsets of the ImageNet dataset. Kornblith et al. (2019) further demonstrated that deep models with
better ImageNet pretraining performance can transfer better to target tasks.
As for the techniques used in our analysis, Li et al. (2018a) proposed the impact of the scaling of
weight matrices on the visualization of loss landscapes. Santurkar et al. (2018) proposed to measure
the variation of loss to demonstrate the stability of loss function. Du et al. (2019) provided a powerful
framework of analyzing two-layer over-parametrized neural networks, with elegant results and no
strong assumptions on input distributions, which is flexible for our extensions to transfer learning.
3 Transferred Knowledge Induces Better Generalization
A basic observation of transferability is that tasks on target datasets more similar to the pretrained
dataset have better performance. We delve deeper into this phenomenon by experimenting on a
variety of target datasets (Figure 1), carried out with two common settings: 1) train only the last layer
by fixing the pretrained network as the feature extractor and 2) train the whole network by fine-tuning
from the pretrained representations. Results in Table 1 clearly demonstrate that, for both settings and
for all target datasets, the training error converges to nearly zero while the generalization error varies
significantly. In particular, a network pretrained on more similar dataset tends to generalize better
and converge faster on the target dataset. A natural implication is that the knowledge learned from
the pretrained networks can only be preserved to different extents for different target datasets.
Table 1: Transferring to different datasets with ImageNet pretrained networks fixed or fine-tuned.
Dataset	training error test error (fixed)	(fixed)		√n kw — Wok	training error test error F (fine-tuned) (fine-tuned)		√1n Pl kW(l)- Wo(l)kF
Webcam	0.00±0	0.45±0.05	0.096±0.007	0.00±0	0.45±0.09	0.94±0.10
Stanford Cars	0.00±0	24.2±0.73	0.165±0.003	0.00±0	13.95±0.44	3.70±0.26
Caltech-101	0.00±0	6.24±0.37	0.059±0.005	0.00±0	4.57±0.40	1.22±0.21
Synthetic	0.03±0.01	0.81±0.19	0.015±0.003	0.00±0	0.75±0.11	0.65±0.12
CUB-200	0.15±0.03	35.10±0.50	0.262±0.006	0.04±0.01	21.04±0.36	2.38±0.33
We substantiate this implication with the following experiments. To analyze to what extent the knowl-
edge learned from pretrained dataset is preserved, for the fixing setting, we compute the Frobenius
norm of the deviation between fine-tuned weight W and pretrained weight Wo as √1n ∣∣W - Wo∣f ,
where n denotes the number of target examples (for the fine-tuning setting, we compute the sum of
deviations in all layers √1n P? ∣∣ W(i)- Wo(i)IIF). Results are shown in Figure 2. It is surprising that
although accuracy may oscillate, √1n ∣∣W - Wo ∣f increases monotonously throughout the training
process and eventually converges. Datasets with larger visual similarity to ImageNet have smaller
deviations from pretrained weight, and √1n ∣∣W - Wo ∣f also converges faster. Another observation
is that √n IIw - WOIlF is approximately proportional to the generalization error shown in Table 1.
=M1 ∙皂卡
O	2000	4000	6∞0	βOOO	IOOOO
Epoch
(a) √1n k W — Wo k F in the last layer.
-OMIMA忐
O	20000	40000	60∞0	80000 IOOOOO
Step
⑸ √1n PlkW(I)- Wo(i) kFin all layers.
Figure 2: The deviation of the weight parameters from the pretrained ones in the transfer process to
different target datasets. For all datasets, √1n IlW - Wo∣∣f increases monotonously. More knowledge
can be preserved on target datasets more similar to ImageNet, yielding smaller √n ∣∣W - Wo∣∣f.
3
Under review as a conference paper at ICLR 2020
Why is preserving pretrained knowledge related to better generalization? From the experiments
above, we can observe that models preserving more transferred knowledge (i.e. yielding smaller
√1n ∣∣W- Wo∣∣f) generalize better. It is reasonable to hypothesize that √n ∣∣W - Wo∣∣f is implicitly
bounded in the transfer process, and that the bound is related to the similarity between pretrained and
target datasets (We will formally study this conjecture in the theoretical analysis). Intuitively, a neural
network attempts to fit the training data by twisting itself from the initialization point. For similar
datasets the twist will be mild, with the weight parameters staying closer to the pretrained parameters.
(a) Eigenvalues of Hessian.
(b) Randomly initialized.
(c) ImageNet pretrained.
Figure 3: Generalization w.r.t. landscape at convergence on Stanford Cars dataset with ResNet-50.
(a) Comparison of the top 20 eigenvalues of the Hessian matrix. ImageNet pretrained networks
have smaller eigenvalues, indicating flatter minima. (b) (c) Comparison of landscapes centered at
the minima. We use the filter normalization (Li et al., 2018a) to avoid the impact of weight scale.
Randomly initialized networks end up with sharper minima. Pretrained networks stay in flat regions.
Such property of staying near the pretrained weight is crucial for understanding the improvement of
generalization. Since optimizing deep networks inevitably runs into local minima, a common belief of
deep networks is that the optimization trajectories of weight parameters on different datasets will be
essentially different, leading to distant local minima. To justify whether this is true, we compare the
weight matrices of training from scratch and using ImageNet pretrained representations in Figure 4.
Results are quite counterintuitive. The local minima of different datasets using ImageNet pretraining
are closed to each other, all concentrating around ImageNet pretrained weight. However, the local
minima of training from scratch and ImageNet pretraining are way distant, even on the same dataset.
∙∙∙ ImageNet Pretrained
∙∙∙ Places Pretrained
∙∙∙ Random Initialized
××× CUB-200	×
Figure 4: The t-SNE (van der Maaten & Hinton, 2008) visualization of the weight matrices of the
pretrained and randomly initialized networks on different datasets. Surprisingly, weight matrices on
the same dataset may be distant at convergence when using different initializations. On the contrary,
even for discrepant datasets, the weight matrices stay close to the initialization when using the same
pretrained parameters.
This provides us with a clear picture of how transferred representations improve generalization on
target datasets. Rich studies have indicated that the properties of local minima are directly related to
generalization (Keskar et al., 2017; Izmailov et al., 2018). Using pretrained representations restricts
weight matrices to stay near the pretrained weight. Since the pretrained dataset is usually sufficiently
large and of high-quality, transferring their representations will lead to flatter minima located in large
flat basins. On the contrary, training from scratch may find sharper minima. To observe this, we
adopt filter normalization (Li et al., 2018a) as the visualization tool, and illustrate the loss landscapes
around the minima in Figure 3. This observation concurs well with the experiments above. The
weight matrices for datasets similar to pretrained ones deviate less from pretrained weights and stay
in the flat region. On more different datasets, the weight matrices have to go further from pretrained
weights to fit the data and may run out of the flat region.
4
Under review as a conference paper at ICLR 2020
4	Properly Pretrained Networks Enable Better Loss Landscapes
A common belief of modern deep networks is the improvement of loss landscapes with techniques
such as BatchNorm (Ioffe & Szegedy, 2015) and residual structures (He et al., 2016). Li et al. (2018a);
Santurkar et al. (2018) validated this improvement when the model is close to convergence. However,
it is often overlooked that loss landscapes can still be messy at the initialization point. To verify this
conjecture, we visualize the loss landscapes centered at the initialization point of the 25th layer of
ResNet-50 in Figure 5. (Visualizations of the other layers can be found in Appendix B.4.) ImageNet
pretrained networks have much smoother landscape than networks trained with random initialization.
The improvement of loss landscapes at the initialization point directly gives rise to the acceleration of
training. Concretely, transferred features help ameliorate the chaos of loss landscape with improved
Lipschitzness in the early stages of training. Thus, gradient-based optimization method can easily
escape from the initial region where the loss is very large.
(b) Randomly initialized.
(c) ImageNet pretrained.
Figure 5: Optimization landscape at initialization. (a) A figure of convergence on fine-grained
classification task Food-101. The convergence of fine-tuning from pretrained networks is significantly
faster than training from scratch. (b) (c) Visualizations of loss landscapes of the 25th layer in
ResNet-50 centered at initialization on Stanford Cars dataset. ImageNet pretrained landscape is much
smoother, indicating better Lipschitzness and predictability of the loss function. Randomly initialized
landscape is more bumpy, making optimization unstable and inefficient.
The properties of loss landscapes influence the optimization fundamentally. In randomly initialized
networks, going in the direction of gradient may lead to large variation in the loss function. On the
contrary, ImageNet pretrained features make the geometry of loss landscape much more predictable,
and a step in gradient direction will lead to mild decrease of loss function. To demonstrate the impact
of transferred features on the stability of loss function, we further analyze the variation of loss in the
direction of gradient in Figure 6. For each step in the training process, we compute the gradient of
the loss and measure how the loss changes as we move the weight matrix in that direction. We can
clearly observe that in contrast to networks with transferred features, randomly initialized networks
have larger variation along the gradient, where a step along the gradient leads to drastic change in the
loss. Why can transferred features control the magnitude of gradient and smooth the loss landscape?
Sso-Jo co-⅞cra>
——ImageNet pretrained
Step
(a) Randomly initialized.
Figure 6: Variation of the loss in ResNet-50 with ImageNet pretrained weight and random initial-
ization. We compare the variation of loss function in the direction of gradient during the training
process on CUB-200 dataset. The variation of pretrained networks is substantially smaller than the
randomly initialized one, implying a more desirable loss landscape and more stable optimization.
Sso-Jo Uo-sμe>
(b) ImageNet pretrained.
A natural explanation is that transferred weight matrices provide appropriate transform of gradient
5
Under review as a conference paper at ICLR 2020
Auuα,nbα,i
>ucu3-UEn-
ImageNet pretrained
(b) ImageNet pretrained.
(a) Randomly initialized.
」0ti©>」e-n6u_s co uo-cθfot
Index of singular value
(c) Projection of weight on gradient components.
(d) Scale of gradient in different layers.
Figure 7: The stabilization of gradient by pretrained weights on CUB-200. (a) (b) Distribution of the
magnitude of gradient in the 25th layer of ResNet-50. (c) Magnitude of the projection of weight on
the singular vectors of gradient. (d) Scaling of the gradient in different layers in back-propagation.
in each layer and help stabilize its magnitude. Note that in deep neural networks, the gradient w.r.t.
each layer is computed through back-propagation by ∂X∂k-ι = Wk Ik (∂⅜) , where Iik denotes
the activation of xi at layer k. The weight matrices Wk function as the scaling factor of gradient
in back-propagation. Basically, a randomly initialized weight matrix will multiply the magnitude
of gradient by its norm. In pretrained weight matrices, situation is completely different. To delve
into this, we decompose the gradient into singular vectors and measure the projections of weight
matrices in these principal directions. Results are shown in Figure 7(c). During pretraining, the
singular vectors of the gradient with large singular values are shrunk in the weight matrices. Thus,
the magnitude of gradient back-propagated through a pretrained layer is controlled. In this sense,
pretrained weight matrices stabilize the magnitude of gradient especially in lower layers. We visualize
the magnitude and scaling of gradient of different layers in ResNet-50 in Figure 7. The gradient
of randomly initialized networks grows fast with layer numbers during back-propagation while the
gradient of ImageNet pretrained networks remains stable. Note that ResNet-50 already incorporates
BatchNorm and skip-connections to improve the gradient flow, and pretrained representations can
stabilize the magnitude of gradient substantially even in these modern networks. We complete this
analysis by visualizing the change of landscapes during back-propagation in Section B.4.
5	When is transfer learning feasible in deep networks ?
Transferring from pretrained representations boosts performance in a wide range of applications.
However, as discovered by He et al. (2018); Kornblith et al. (2019), there still exist cases when
pretrained representations provide no help for target tasks or even downgrade test accuracy. Hence,
the conditions on which transfer learning is feasible is an important open problem to be explored. In
this section, we delve into the feasibility of transfer learning with extensive experiments, while the
theoretical perspectives are presented in the next section. We hope our analysis will provide insights
into how to adopt transfer learning by practitioners.
5.1	Choices of a pretrained dataset
As a common practice, people choose datasets similar to the target dataset for pretraining. However,
how can we determine whether a dataset is sufficiently similar to a target dataset? We verify with
experiments that the similarity depends on the nature of tasks, i.e. both inputs and labels matter.
6
Under review as a conference paper at ICLR 2020
Figure 8: Performance by varying either the inputs orlabels. (a) Accuracy and deviation of transferring
from different datasets to MNIST, where ψι and ψ2 denote the convolutional kernels in the first and
the second layers, and W denotes the weight of the fully-connected layer. (b) Accuracy on Webcam
with Caltech and Caltech-color pretraining. (c) (d) Test accuracy and convergence of transferring to
MIT-indoors and CUB-200 from models pretrained on large-scale ImageNet and Places, respectively.
Varying input with fixed labels. We randomly sample 600 images from the original SVHN dataset,
and fine-tune the MNIST pretrained LeNet (LeCun et al., 1998) to this SVHN subset. For comparison,
we pretrain other two models on MNIST with images upside down and Fashion-MNIST (Xiao
et al., 2017), respectively. Note that for all three pretrained models, the dataset sizes, labels, and the
number of images per class are kept exactly the same, and thus the only difference lies in the image
pixels themselves. Results are shown in Figure 8(a). Compared to training from scratch, MNIST
pretrained features improve generalization significantly. Upside-down MNIST shows slightly worse
generalization performance than the original one. In contrast, fine-tuning from Fashion-MNIST
barely improves generalization. We also compute the deviation from pretrained weight of each layer.
The weight matrices and convolutional kernel deviation of Fashion-MNIST pretraining show no
improvement over training from scratch. A reasonable implication here is that choosing a model
pretrained on a more similar dataset in the inputs yields a larger performance gain.
Varying labels with fixed input. We train a ResNet-50 model on Caltech-101 and then fine-tune it
to Webcam (Saenko et al., 2010). We train another ResNet-50 to recognize the color of the upper
part of Caltech-101 images and fine-tune it to Webcam. Results in Figure 8(b) indicate that the latter
one provides no improvement over training on Webcam from scratch, while pretraining on standard
Caltech-101 significantly boosts performance. Models generalizing very well on similar images are
not transferable to the target dataset with totally different labels. These experiments challenge the
common perspective of similarity between datasets. The description of similarity using the input
(images) themselves is just one point. Another key factor of similarity is the relationship between the
nature of tasks (labels). This observation is further in line with our theoretical analysis in Section 6.
Similar results are observed on datasets of larger scale for the above two cases (See Figure 8(c)8(d)).
We fine-tune deep representations pretrained on ImageNet and Places (Zhou et al., 2018) (a large-
scale dataset of scene understanding) to CUB-200 and MIT-indoors datasets. As a fine-grained
object recognition task, CUB-200 benefits more from ImageNet pretrained features. In contrast,
MIT-indoors is a scene dataset more similar to Places, thus it benefits more from Places pretraining.
C- SUelUJ。七ωd
Epoch of pretraining
(a) Accuracy of pretraining.
Figure 9: Transfer performance w.r.t. the pretraining epochs from Food-101 to CUB-200. (a)
Accuracy of pretraining task and (b) Accuracy on target dataset by fine-tuning from models pretrained
for different numbers of epochs. Although accuracy of pretraining increases stably, transferability
increases only during the early epochs and decreases afterwards.
α,uueuuoJJ,,,d*ja,ffeJ.
Epoch of pretraining
(b) Accuracy on the target dataset.
7
Under review as a conference paper at ICLR 2020
5.2	Choices of pretraining epochs
Currently, people usually train a model on ImageNet until it converges and use it as the pretrained
parameters. However, the final model do not necessarily have the highest transferability. To see
this, we pretrain a ResNet-50 model on Food-101 (Bossard et al., 2014) and transfer it to CUB-200,
with results shown in Figure 9. During the early epochs, the transferability increases sharply. As we
continue pretraining, although the test accuracy on the pretraining dataset continues increasing, the
test accuracy on the target dataset starts to decline, indicating downgraded transferability. Intuitively,
during the early epochs, the model learns general knowledge that is informative to many datasets. As
training goes on, however, the model starts to fit the specific knowledge of the pretrained dataset and
even fit noise. Such dataset-specific knowledge is usually detrimental to the transfer performance.
This interesting finding implies a promising direction for improving the de facto pretraining method:
Instead of seeking for a model with higher accuracy only on the pretraining dataset, a more transferable
model can be pretrained with appropriate epochs such that the fine-tuning accuracies on a diverse set
of target tasks are advantageous. Algorithms for pretraining should take this point into consideration.
6	Theoretical Analysis
We have shown through extensive empirical analysis that transferred features exert a fundamental
impact on generalization and optimization performance, and provided some insights for the feasibility
of transfer learning. In this section, we analyze some of our empirical observations from a theoretical
perspective. We base our analysis on two-layer fully connected networks with ReLU activation and
sufficiently many hidden units. Our theoretical results are in line with the experimental findings.
6.1	Setup
Denote by σ(∙) the ReLU activation function, σ (Z) = max{z, 0}. I{A} is the indicator function,
i.e. I{A} = 1 if A is true and 0 otherwise. [m] is the set of integers ranging from 1 to m. Consider
a two-layer ReLU network of m hidden units fw,a(x) = √maτσ(Wτx), with X ∈ Rd as input
and W = (wι, •一，Wm) ∈ Rd×m as the weight matrix. We are provided with uq samples
{xQ,i, yQ,i}in=Q1 drawn i.i.d. from the target distribution Q as the target dataset and a weight matrix
W(P) pretrained on nP samples {xP,i, yP,i}in=P1 drawn i.i.d. from pretrained distribution P. Suppose
kxk2 = 1 and |y| ≤ 1. Our goal is transferring the pretrained W(P) to learn an accurate model
W(Q) for the target distribution Q. When training the model on the pretraining dataset, we initialize
the weight as: Wr (0)〜N(0, κ2I),ar 〜Unif ({-1,1}), where ∀r ∈ [m] and K is a constant.
For both pretraining and fine-tuning, the objective function of the model is the squared loss L(W) =
2(y - fw,a(X))τ(y - fw,a(X)). Note that a is fixed throughout training and W is updated with
gradient descent. The learning rate is set to η.
We base our analysis on the theoretical framework of Du et al. (2019), since it provides elegant results
on convergence of two-layer ReLU networks without strong assumptions on the input distributions,
facilitating our extension to the transfer learning scenarios. In our analysis, we use the Gram matrices
HP∞ ∈ RnP ×nP and HQ∞ ∈ RnQ×nQ to measure the quality of pretrained input and target input as
xPτ ixPj (π - arccos(xPτ ixPj))
HPj = Ew〜N(0,i)[x>,ixPjI{WTxP,i ≥ 0, w>XPj ≥ 0}] = ―—,-------------------2∏-------,―,—,
H∞,j = Ew〜N(0,i)[xQ,ixQ,jI{w>xQ,i ≥ 0, WTXQj ≥ 0}] = XE"(" Uncos(XQ,ixQj))
To quantify the relationship between pretrained input and target input, we define the following Gram
matrix HP∞Q ∈ RnP ×nQ across samples drawn from P and Q:
HPQ,ij = Ew〜N(0,I)[x>,ixQ,jI{wτxP,i ≥ 0, w>xQj ≥ 0}]
x>,ixQj(π - arccoS(X>,ixQj))
2π
Assume Gram matrices HP∞ and HQ∞ are invertible with smallest eigenvalue λP and λQ greater than
zero. HP∞-1yP characterizes the labeling function of pretrained tasks. yP→Q , HP∞Q>HP∞-1yP
further transforms the pretrained labeling function to the target labels. A critical point in our analysis
is yQ - yP→Q, which measures the task similarity between target label and transformed label.
8
Under review as a conference paper at ICLR 2020
6.2	Improved Lipschitzness of Loss Function
To analyze the Lipschitzness of loss function, a reasonable objective is the magnitude of gradient,
which is a direct manifestation of the Lipschitz constant. We analyze the gradient w.r.t. the activations.
For the magnitude of gradient w.r.t. the activations, we show that the Lipschitz constant is significantly
reduced when the pretrained and target datasets are similar in both inputs and labels.
Theorem 1 (The effect of transferred features on the Lipschitzness of the loss). Denote by X1
the activations in the target dataset. For a two-layer networks with sufficiently large number of
hidden unit m defined in Section 6.1, if m ≥ poly(nP, nQ, δ-1, λP-1, λ-Q1, κ-1), κ = O
with probability no less than 1 - δ over the random initialization,
k dL(∂X1P)) k2 = k dL∂Ww(0" k2 - yQyQ + (yQ - VP-q)>3q - VP→q)
+ poly(np,nQ,δ-1,λ-1,κ-1) + O (nPnQK ʌ
m 4	λp δ
This provides us with theoretical explanation of experimental results in Section 4. The control of
Lipschitz constant relies on the similarity between tasks in both input and labels. If the original
target label is similar to the label transformed from the pretrained label, i.e. kVQ - VP→Q k22 is small,
the Lipschitzness of loss function will be significantly improved. On the contrary, if the pretrained
and target tasks are completely different, the transformed label will be discrepant with target label,
resulting in larger Lipschitz constant of the loss function and worse landscape in the fine-tuned model.
6.3 Improved Generalization
Recall in Section 3 that we have investigated the weight change kW(Q) - W(P)kF during training
and point out the role it plays in understanding the generalization. In this section, we show that
kW(Q) - W(P)kF can be bounded with terms depicting the similarity between pretrained and target
tasks. Note that the Rademacher complexity of the function class is bounded with kW(Q)-W(P)kF
as shown in the seminal work (Arora et al., 2019), thus the generalization error is directly related to
kW(Q) - W(P)kF. We still use the Gram matrices defined in Section 6.1.
Theorem 2 (The effect of transferred features on the generalization error). For a two-layer
networks With m ≥ poly(np, nQ,δ-1,λ-1,λ-1,κT), K = O ('P入Q ), with probability no less
nP nQ)
than 1 - δ over the random initialization,
kW(Q)-W(P)kF≤	(VQ-
yP→Q)>H∞-1(VQ - yP→Q)
O nnPnQK1 ʌ	poly(nP, nQ,δ-1, λ-1,λ-1,κ-1)	⑵
+	∖ λjλqδ1 I +	m1
This result is directly related to the generalization error and casts light on our experiments in Section
5.1. Note that when training on the target dataset from scratch, the upper bound of kW(Q) -W(0)kF
is yQ>HQ∞-1yQ. By fine-tuning from a similar pretrained dataset where the transformed label is close
to target label, the generalization error of the function class is hopefully reduced. On the contrary,
features pretrained on discrepant tasks do not transfer to classification task in spite of similar images
since they have disparate labeling functions. Another example is fine-tuning to Food-101 as in the
experiment of Kornblith et al. (2019). Since it is a fine-grained dataset with many similar images, HQ∞
will be more singular than common tasks, resulting in a larger deviation from the pretrained weight.
Hence even transferring from ImageNet, the performance on Food-101 is still far from satisfactory.
7	Conclusion: Behind Transferability
Why are deep representations pretrained from modern neural networks generally transferable to novel
tasks? When is transfer learning feasible enough to consistently improve the target task performance?
9
Under review as a conference paper at ICLR 2020
These are the key questions in the way of understanding modern neural networks and applying them
to a variety of real tasks. This paper performs the first in-depth analysis of the transferability of deep
representations from both empirical and theoretical perspectives. The results reveal that pretrained
representations will improve both generalization and optimization performance of a target network
provided that the pretrained and target datasets are sufficiently similar in both input and labels. With
this paper, we show that transfer learning, as an initialization technique of neural networks, exerts
implicit regularization to restrict the networks from escaping the flat region of pretrained landscape.
References
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. In Proceedings
ofthe 36th International Conference on Machine Learning, volume 97,pp. 322-332, 2019.
Lukas Bossard, MatthieU Guillaumin, and Luc Van GooL Food-101 - mining discriminative compo-
nents with random forests. In European Conference on Computer Vision, 2014.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep
bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers), pp. 4171-4186, 2019.
Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor
Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. In
Proceedings of the 31st International Conference on Machine Learning, volume 32, pp. 647-
655, 2014.
Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In International Conference on Learning Representations,
2019.
Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training
examples: An incremental bayesian approach tested on 101 object categories. In 2004 Conference
on Computer Vision and Pattern Recognition Workshop, pp. 178-178, 2004.
Weifeng Ge and Yizhou Yu. Borrowing treasures from the wealthy: Deep transfer learning through
selective joint fine-tuning. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 1086-1095, 2017.
Kaiming. He, Xiangyu. Zhang, Shaoqing. Ren, and Jian. Sun. Deep residual learning for image
recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp.
770-778, 2016.
Kaiming He, Ross B. Girshick, and Piotr Dollar. Rethinking imagenet pre-training. arxiv,
abs/1811.08883, 2018.
Mi-Young Huh, Pulkit Agrawal, and Alexei A. Efros. What makes imagenet good for transfer
learning? arxiv, abs/1608.08614, 2016.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In Proceedings of the 32nd International Conference on Machine
Learning, pp. 448-456, 2015.
Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson.
Averaging weights leads to wider optima and better generalization. In Uncertainty in Artificial
Intelligence (UAI). 2018.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter
Tang. On large-batch training for deep learning: Generalization gap and sharp minima. 2017.
10
Under review as a conference paper at ICLR 2020
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A.
Rusu, Kieran Milan, John. Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis,
Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting in
neural networks. Proceedings of the National Academy of Science, USA, 114(13):3521-3526,
2016.
Simon Kornblith, Jonathon Shlens, and Quoc V. Le. Do better imagenet models transfer better? In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2661-2671, 2019.
Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained
categorization. In 4th International IEEE Workshop on 3D Representation and Recognition
(3dRR-13), Sydney, Australia, 2013.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolu-
tional neural networks. In Advances in Neural Information Processing Systems 25, pp. 1097-1105.
2012.
Yann LeCun, L6on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape
of neural nets. In Advances in Neural Information Processing Systems 31, pp. 6389-6399. 2018a.
Xingjian Li, Haoyi Xiong, Hanchao Wang, Yuxuan Rao, Liping Liu, and Jun Huan. DELTA: deep
learning transfer using feature map with attention for convolutional networks. International
Conference on Learning Representations, 2019.
Xuhong Li, Yves Grandvalet, and Franck Davoine. Explicit inductive bias for transfer learning
with convolutional networks. In Proceedings of the 35th International Conference on Machine
Learning, volume 80, pp. 2825-2834, 2018b.
Maxime Oquab, Leon Bottou, Ivan Laptev, and Josef Sivic. Learning and transferring mid-level
image representations using convolutional neural networks. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2014.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet
Large Scale Visual Recognition Challenge. International Journal of Computer Vision, 115(3):
211-252, 2015.
Kate Saenko, Brian Kulis, Mario Fritz, and Trevor Darrell. Adapting visual category models to new
domains. In European Conference on Computer Vision, pp. 213-226, 2010.
Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch nor-
malization help optimization? In Advances in Neural Information Processing Systems 31, pp.
2483-2493. 2018.
Laurens J.P. van der Maaten and Geoffrey E. Hinton. Visualizing high-dimensional data using t-sne.
Journal of Machine Learning Research, 9(2):2579-2605, 2008.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,匕 ukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information
Processing Systems 30, pp. 5998-6008. 2017.
Peter Welinder, Steve Branson, Takeshi Mita, Catherine Wah, Florian Schroff, Serge Belongie, and
Pietro Perona. Caltech-UCSD Birds 200. Technical Report CNS-TR-2010-001, California Institute
of Technology, 2010.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking
machine learning algorithms. arxiv, abs/1708.07747, 2017.
Zhewei Yao, Amir Gholami, Qi Lei, Kurt Keutzer, and Michael W Mahoney. Hessian-based analysis
of large batch training and robustness to adversaries. In Advances in Neural Information Processing
Systems 31, pp. 4949-4959. 2018.
11
Under review as a conference paper at ICLR 2020
Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep
neural networks? In Advances in Neural Information Processing Systems 27, pp. 3320-3328.
2014.
Bolei Zhou, Agata Lapedriza, Aditya Khosla, Antonio Oliva, and Aude Torralba. Places: A 10
million image database for scene recognition. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 40(6):1452-1464, 2018.
12
Under review as a conference paper at ICLR 2020
A	Implementation Details
In this section, we provide details of the architectures, setup, methods of visualizations in our analysis.
The codes and visualizations are attached with the submission and will be made available online.
Models. We implement all models on PyTorch with 2080Ti GPUs. For object recognition and
scene recognition tasks, we use standard ResNet-50 from torchvision. ImageNet pretrained models
can be found in torchvision, and Places pretrained models are provided by Zhou et al. (2018). During
fine-tuning we use a batch size of 32 and set the initial learning rate to 0.01 with 0.9 momentum
following the protocol of (Li et al., 2018b). For fine-tuning, we train the model for 200 epochs. We
decay the learning rate by 0.1 with the time of decay set by cross validation. In Figure 2(a) where
the pretrained ResNet-50 functions as feature extractor, the downstream classifier is a two-layer
ReLU network with Batch-Norm and Leaky-ReLU non-linearity. The number of hidden unit is 512.
For this task, the backbone ResNet-50 is fixed, with the downstream two-layer classifier trained
with momentum SGD. The learning rate is set to 0.01 with 0.9 momentum, and remains constant
throughout training.
For digit recognition tasks, we use LeNet (LeCun et al., 1998). The learning rate is also set to 0.01,
with 5 × 10-4 weight decay. The batch-size is set to 64. We train the model for 100 epochs.
Fine-tuning. We follow the protocol of fine-tuning as in the previous paragraphs. In Tables 1
and 2, we run all the experiments for 3 times and report their mean and variance. For Table 2, the
improvement of fine-tuing is calculated with the generalization error of fine-tuning divided by the
generalization error of training from scratch.
Visualization of loss landscapes. We use techniques similar to filter normalization to provide an
accurate analysis of loss landscapes (Li et al., 2018a). Note that ReLU networks are invariant to
the scaling of weight parameters. To remove this scaling effect, the direction used in visualization
should be normalized in a filter-wise way. Concretely, the axes of each landscape figure are two
random Gaussian orthogonal vectors normalized by the scale of each filter in the convolutional
layers. Concretely, suppose the parameter of the center point is θ. θi,j denotes the j-th filter of
the i-th layer. Suppose the two unit orthogonal vectors are a and b. Then with filter normalization,
ai,j —	∣∣θi,jk and % 一 ɪbijɪ∣∣θi,j ∣∣. For each point (i.e. pixel) (p, q) in the plot, the value is
evaluated with g(p, q) = L(f (θ + η(pa + qb))), where L denotes the loss function, f denotes the
neural networks. η is a parameter to control the scale of the plot. In all visualization images of ResNet-
50, the resolution is 200 X 200, i.e, P = -100, -99,…，98, 99 and q = -100, -99, •一，98,99.
For additional details of filter normalization, please refer to (Li et al., 2018a). η is set to 0.001, which
is of the same order as 10 times the step size in training. This is a reasonable scale ifwe want to study
the local loss landscape of model using SGD. For fair comparison between the pretrained landscapes
and randomly initialized landscapes, the scale of loss variation in each plot is exactly the same. The
difference of loss value between each contour is 0.05. When we compute the loss landscape of one
layer, the parameters of other layers are fixed. The gradient is computed based on 256 fixed samples
since the gradient w.r.t. full dataset requires too much computation. Figure 3 and Figure 10 are
centered at the final weight parameters, while others are centered at the initialization point to show
the situation when training just starts. We visualize the loss landscapes on CUB-200, Stanford Cars
and Food-101 for multiple times and reach consistent conclusions. But due to space limitation, we
only show the results on one dataset for each experiment in the main paper. Other results are deferred
to Section B.
Computing the eigenvalue of Hessian. We compute the eigenvalues of Hessian with Hessian-
vector product and power methods based on the autograd of PyTorch. A similar implementation is
provided by Yao et al. (2018). We only list top 20 eigenvalues in limited space.
t-SNE embedding of model parameters. We put the weight matrices of ResNet-50 in one vector
as input. For faster computation, we pre-compute the distance between parameters of every two
models with PyTorch and then use the distance matrix to compute the t-SNE embedding with scikit-
learn. Note that we use the same ImageNet model from torchvision and the same Places model from
Zhou et al. (2018) for fine-tuning.
13
Under review as a conference paper at ICLR 2020
Variation of loss function in the direction of gradient. Based on the original trajectory of training,
we take steps in the direction of gradient from parameters at different steps during training to calculate
the maximum changes of loss in that direction. The step size is set to the size of gradient. We take 100
steps from the original trajectories to measure the local property of loss landscapes. We aim to quantify
the stability of loss functions and directly show the magnitude of gradient with this experiments on
different datasets. Results on CUB-200 are provided in the main paper, with additional results further
provided in Section B. Not that this experiment is inspired by Santurkar et al. (2018). We use the
similar protocol as Section 3.2 in Santurkar et al. (2018). Another protocol is to fix the step size along
the gradient and compute the maximum variation of loss. Results on Stanford Cars with this protocol
are provided in Section B.2. Results for both scenarios are similar.
B	Additional Experimental Results
B.1	Visualization of landscapes around convergence for different datasets
175
150
125
IOO
75
50
25
O
(a) Randomly initialized.
O 25	50	75 IOO 125	150	175
(b) ImageNet pretrained.
Figure 10: Comparison of landscapes centered at the minima on Food-101 datasets with ResNet-50.
We use the filter normalization (Li et al., 2018a) to avoid the impact of weight scale. Randomly
initialized networks end up with sharper minima, while pretrained networks stay in flat regions.
B.2	Variation of loss function in the direction of gradient with fixed step size
ωwo- % uoceLle>
(a) Randomly initialized.
Random initialized
SSO- % uo=2μe>
(b) ImageNet pretrained.
Figure 11: Variation of the loss in ResNet-50 with ImageNet pretrained weight and random initial-
ization. We compare the variation of loss function in the direction of gradient during the training
process on Stanford Cars dataset. The variation of pretrained networks is substantially smaller than
the randomly initialized one, implying a more desirable loss landscape and more stable optimization.
B.3	Comparison of training from scratch
To validate that the generalization error is indeed improved with pretraining, for each dataset, we
list the generalization error and the norm of deviation from the pretrained parameters in Table 2.
The decreased percentage is calculated by dividing the error reduced in fine-tuning with the error of
training from scratch. Compared to the results of fine-tuning, we observe that ImageNet pretraining
improves the generalization performance of general coarse-grained classification tasks significantly,
14
Under review as a conference paper at ICLR 2020
yet the performance boost is smaller for fine-grained tasks which are dissimilar in the sense of task
with ImageNet. Note that, although Stanford Cars and CUB-200 are visually similar to ImageNet,
what really matters is the similarity between the nature of tasks, i.e. both images and labels matter.
Table 2: Performance of training from scratch.
Dataset	test error	fine-tuning improvement	√n PlkW(I)- Wo(i)kF
Webcam	26.39±0.41	98.29%	3.54±0.53
Stanford Cars	46.25±0.76	48.75%	5.95±0.38
Caltech-101	22.14±0.63	79.36%	2.34±0.34
CUB-200	52.70±0.77	60.15%	3.90±0.61
B.4	Comparison of the loss landscapes in each layer
We visualize the loss landscape of 25-48th layers in ResNet-50 on Food-101 dataset. We compare
the landscapes centered at the initialization point of randomly initialized and ImageNet pretrained
networks - see Figure 12 and Figure 13. Results are in line with our observations of the magnitude
of gradient in Figure 7. At higher layers, the landscapes of random initialization and ImageNet
pretraining are similar. However, as the gradient is back-propagated through lower layers, the
landscapes of pretrained networks remain as smooth as the higher layers. In sharp contrast, the
landscapes of randomly initialized networks worsen through the lower layers, indicating that the
magnitude of gradient is substantially worsened in back-propagation.
15
Under review as a conference paper at ICLR 2020
(a) layer3.0.conv1
(b) layer3.0.conv2
(c) layer3.0.conv3
(d) layer3.1.conv1
(e) layer3.1.conv2
(f) layer3.1.conv3
25 sɑ 75 IM 125 IM 175
(g) layer3.2.conv1
(h) layer3.2.conv2
(i) layer3.2.conv3
(j) layer3.3.conv1
(k) layer3.3.conv2
(l) layer3.3.conv3
(m) layer3.4.conv1
(n) layer3.4.conv2
(o) layer3.4.conv3
(p) layer3.5.conv1
(q) layer3.5.conv2
(r) layer3.5.conv3
(s) layer4.0.conv1
(t) layer4.0.conv2
(u) layer4.0.conv3
(v) layer4.1.conv1
(w) layer4.1.conv2
(x) layer4.1.conv3
Figure 12: Landscapes centered at the initialization point of each layer in ResNet-50 using ImageNet
pretrained weight. The smoothness of landscapes in each layer are nearly identical, indicating a
proper scaling of gradient.
16
Under review as a conference paper at ICLR 2020
(e) layer3.1.conv2
(f) layer3.1.conv3
(g) layer3.2.conv1
(h) layer3.2.conv2
(i) layer3.2.conv3
(j) layer3.3.conv1
(k) layer3.3.conv2
(l) layer3.3.conv3
(m) layer3.4.conv1
(n) layer3.4.conv2
(o) layer3.4.conv3
(p) layer3.5.conv1
(q) layer3.5.conv2
(r) layer3.5.conv3
(s) layer4.0.conv1	(t) layer4.0.conv2
(u) layer4.0.conv3
(v) layer4.1.conv1
(w) layer4.1.conv2
(x) layer4.1.conv3
Figure 13: Landscapes centered at the initialization point of each layer in ResNet-50 initialized
randomly. At the higher layers, the landscapes tend to be smooth. However, as the gradient is
propagated to lower layers, the landscapes are becoming full of ridges and trenches in spite of the
presence of Batch-Norm and skip connections.
17
Under review as a conference paper at ICLR 2020
C Proofs of Theorems in Section 6
To study how transferring pretrained knowledge helps target tasks, we first study the trajectories
of weight matrices during pretraining and then analyze its effect as an initialization in target tasks.
Our analysis is based on Du et al. (2019)’s framework for over-parametrized networks. For the
weight matrix W, W(0) denotes the random initialization. WP (k) denotes W at the kth step of
pretraining. W(P) denotes the pretrained weight matrix after training K steps. WQ (k) denotes the
weight matrix after K steps of fine-tuning from W(P). For other terms, the notation at each step is
similar.
We first analyze the pretraining process on the source datasets based on Arora et al. (2019). Define
a matrix ZP ∈ Rmd×nP which is crucial to analyzing the trajectories of the weight matrix during
pretraining,
/ IPIaIXP,1	…	IPnaιXp,n \
ZP = √=	.	...	.	I ∈ Rmd×nP,	(3)
Im,1amxP,1	•一 Im,namXP,n
where IiP,j = I{wi>XP,j ≥ 0}. ZP(k) denotes the matrix corresponding to WP (k). Note that the
gradient descent is carried out as
vec(WP (k + 1)) = vec(WP (k)) - ηZP (k)(u(k) - yP),	(4)
where Vec (∙) denotes concatenating a column of a matrice into a single vector. Then in the K
iterations of pretraining on the source dataset,
vec(W(P)) - vec(W(0))
K-1
= X vec(WP (k + 1)) - vec(WP(k))
k=0
K-1
-η X ZP (k)(uP (k) - yP)
k=0
K-1	K-1
XηZP(k)(I-ηHP∞)kyP- X ηZP(k)eP(k)
k=0	k=0
K-1	K-1	K-1
= X ηZP(0)(I - ηHP∞)ky + X η(ZP(k) - ZP(0))(I - ηHP∞)ky - X ηZP(k)eP(k).
k=0	k=0	k=0
(5)
The first term is the primary component in the pretrained matrix, while the second and third terms is
small under the over-parametrized conditions. Now following Arora et al. (2019), the magnitude of
these terms can be bounded with probability no less than 1 - δ,
K-1	K-1	2
kd∣2 = k X η(ZP (k)-ZP (0))(I-ηH∞)k y-X ηZP (k)eP (k)∣∣2 =O nPK +° InnP 1
k=0	k=0	λPp)	∖m4 λPpκ2 δ
(6)
Here we also provide lemmas from Du et al. (2019) which are extensively used later.
Lemma 1. With λP = λmin(HP∞) > 0, m = Ω (λ4⅛⅛^) and η =O(普)，With Probability at
least 1 - δ over the random initialization we have
∣∣WP,r(k) - Wr(0)∣∣2 ≤ 'VZnP"" uP(0)k2 , ∀r ∈ [m],∀k ≥ 0.	(7)
mλP
Lemma 2. If W1, . . . , Wm are i.i.d. generated from N(0, I), then with probability at least 1 - δ,
the following holds. For any set of weight vectors W1 , . . . , Wm ∈ Rd that satisfy for any r ∈ [m],
∣∣Wr (0) — Wr ∣2 ≤ cnλ20，R for some small positive constant c, then the matrixH ∈ Rn×n defined
by
1m
Hij=	x>Xj XI {w>Xi ≥ 0, w>Xj ≥ 0}
m	r=1
satisfies ∣∣H — H(0)∣2 < λ0 and λmin (H) > λ20.
18
Under review as a conference paper at ICLR 2020
C.1 Proof of Theorem 1
Now we start to analyze the influence of pretrained weight on target tasks. 1) We show that during
pretraining, HP∞Q is close to ZP(0)>ZQ(P). 2) Then we analyze uQ(P) -uQ(0) with the properties
of HP∞Q. 3) Standard calculation shows the magnitude of gradient relates closely to uQ(P) - uQ(0),
and we are able to find out how is the magnitude of gradient improved.
To start with, we analyze the properties of the matrix HP∞Q. We show that under over-parametrized
conditions, HP∞Q is close to the randomly initialized Gram matrix ZP (0)>ZQ(P). Use HPQ (0) to
denote ZP (0)>ZQ(0), and HPQ(P) to denote ZP(0)>ZQ(P).
Lemma 3. With the same condition as lemma 1, with probability no less than 1 - δ,
kHPQ(P)- H∞QkF ≤ O( √mn‰).	⑻
|HP Q,ij (P) - HpQ,ij(0)1 = ΓP,i>xQ,j XX*0飕(P) - 吃(0解0))|
m r=1
m
≤ ɪ Xi(*(p) =*(0))	⑼
m r=1
m
≤ m X(I(IWr (0)>XQ,il ≤ R)+I(kwr (P) - Wr (0)l∣2 >R)),
where R = c√npk√m-UP(0)k2 With a small c. Since Wr (0) is independent of XQ,i and ∣∣XQ,ik2 = 1,
the distribution of W(0)r>xQ,i and Wr(0) are the same Gaussian. E[I(lWr(0)>xQ,i l ≤ R)] =
E[I(∣Wr (0)l≤ R)] ≤ √2Rκ.
E[|HP Q,ij (P) - HP Q,ij(0)|] ≤ √=- + --	(10)
2πκ m
Applying Markov,s inequality, and noting that kyp - u(0)∣∣2 = O (午)We have with probability
no less than 1 - δ,
kHPQ(P)- HPQ(0)kF ≤ nPnQ (√2Rκ + m) = O (√mnpnQδ3∕2) .	(II)
Also note that E[HP Q,ij (0)] = HP∞Q,ij. By Hoeffding’s inequality, we have with probability at least
1- δ,
kH∞Q - HpQ(0)kF ≤ nPn %：Pn/ .	(12)
Combining equation 12 and equation 11, we have with probability at least 1 - δ,
kHPQ(P)-H∞QkF ≤ o(√mn2P⅛-).
□
Denote by uQ (P), uQ (0) the output on the target dataset using weight matrix W(P) and W0
respectively. First, we compute the gradient with respect to the activations,
∂L(W(P))
∂x1	= √ma(UQ(P) yQ),
(13)
∂L(W(P)) 2	1 >	2
k -∂χ1- k2 = ma a(uQ(P) - yQ)
= kuQ(0) -yQk22 + kuQ(P) -uQ(0)k22 +2huQ(P) - uQ(0),uQ(0) -yQi .
(14)
19
Under review as a conference paper at ICLR 2020
It is obvious from equation 14 that uQ (P) - uQ (0) should become the focus of our analysis. To
calculate uQ(P) - uQ(0), we need to sort out how the activations change by initializing the target
networks with W(P) instead of W(0).
UQ(P) - uq(0) = √m(a>(σ(W(P)>X) - σ(W>(0)X)))>
1 m	(15)
=√m X ar (σ(w>,r XQ)- σ(W> ⑼XQ))
r=1
For each xQ,i, divide r into two sets to quantify the change of variation in activations on the target
dataset.
Si = {r ∈ [m], IWr(0)>XQ,i∣ ≥ R},Si = {r ∈ [m], ∣w(0)>Xq/ ≤ R},	(16)
where R = 4词「崂三(0"2. For r in Si, We can estimate the size of Si. Note that
E [Si] = E [Pi=ι Pm=II (∣w(0)>XQ,i∣≤ R)]. Foreach i and r, E[I(iw(0)>xq∕ ≤ R)]=
E[I(∣w(0)r | ≤ R)] ≤ √Rκ, since the distribution of w(0)r is Gaussian with mean 0 and covariance
matrix κ2I. Therefore, taking sum over all i and m and using Markov inequality, with probability at
least 1 - δ over the random initialization we have
ISiI ≤ 2√2∏R = -—J(0)k2，∀r ∈ [m]，∀k ≥ 0.
(17)
Thus, this part of activations is the same for W(0) and W(P) on the target dataset. For each XQ,i,
1m
uQ,i(P) - uQ,i(0) = √m Z ar (σ(wP,rxQ,i) — σ(w> (0)xQ,i))
=√m X ar(IQi(0)(w>,rXQ,i) — IQi (0)(w>(0)xQ,i))
r∈[m]
+ √1m X ar(σ(WP,rxQ,i) — σ(w> (0)xQ,i))
r∈Si
—√m X ar (IQi(O)(Wp,rxQ,i ) — IQi(O)(W>(0)xQ,i)),
r∈Si
(18)
where IrQ,i(0) denotes I{wr>(0)xQ,i ≥ 0}.The first term is the primary part, while we can show that
the second and the third term can be bounded with √= ∣Si IkWr(P) 一 w/(0) ∣∣2 since ∣∣xQ,i∣∣2 = 1.
Putting all xQ,i together,
UQ (P) — UQ(0)
=√m (a>σo((W(P) — W(0))>X) + J + 2)>
= ZQ(0)>vec(W(P) —W(0))+1 +2
= ZQ (0)> (ZP (0)(HP∞)-1 yP + ) + 1 + 2,
(19)
where €1 and €2 correspond to each of the second term and third term in equation 18. Thus, using
lemma 1 and the estimation of |Si|, with probability no less than 1 一 δ,
k€0k2 = k€1 + €2k2
i|kWr(P) — Wr (0)k2 = O (√mnPδ2κ!.
(20)
20
Under review as a conference paper at ICLR 2020
Now equipped with equation 6, equation 19, equation 20 and lemma 3, we are ready to calculate
exactly how much pretrained wight matrix W(P) help reduce the magnitude of gradient over W(0),
∂L 1
k∂X1 k2 = ma>a(uQ(P) - YQ)2
= kuQ(0) - yQk22 + kuQ(P) - uQ(0)k22 + 2 huQ (P) - uQ(0), uQ(0) - yQi
=k∂X1 k2 + y>H∞-1 H∞QH∞Q>H∞-1yp - 2yQH∞QτH∞-1 VP
+k0k22+20>ZQ(P)>ZP(0)HP∞-1yP+20>ZQ(P)>
+ kyPτHP∞-1(ZP(0)τZQ(P) - HP∞Q)k22 + kk22 + 2τZQ(P)τZP(0)HP∞-1yP
+ uQ(0)τZQ(P)τZP(0)HP∞-1yP + uQ(0)τZQ(P)τ + uQ(0)τ0
+ 2yQ (ZQ (P)τ ZP (0) - HP∞Qτ )HP∞-1yP.
(21)
In equation 21, note that kk2, k0k2, kZQ(P)τZP(0) - HP∞QτkF = kHPQ (P) -H∞q∣∣f, and
∣∣uq(0)∣∣2 are all small values We have estimated above. Therefore, using ∣∣Zp(0)∣∣f ≤ √nP and
∣∣Zq(P ) k f ≤ √nQ, we can control the magnitude of the perturbation terms under over-parametrized
conditions. Concretely, With probability at least 1 - δ over random initialization,
ke'k2= O (4⅛)	(22)
mλ4P δ4 κ2
e0TZQ(P)τZP(0)H∞Typ = O (√^√nP反λP√nP) = θ (√⅛fc) (23)
0τZQ(P)τ = O
nP nQ )
√mλp δ3)
np Uq
m3∕4λ∕δ3κ3/2
(24)
kyτH∞-1(Zp(0)T ZQlP)- h∞q )k2 = OQn λP √⅛‰ )2 = o
n5P nQ ∖
mλp κ2δ3 )
(25)
(26)
τZQ(P)τZP(0)HP∞-1yP =O
+O
+O
UQ(0)TZQ(P)τZp(0)H∞-1yp = O (npnQK)
λP δ
UQ(0)TZQ(P )T = O (nPnQ⅛)
+O
np nQ
m1∕4λ%2√Kδ3∕2
uQ(0)τ0 = O
2
np nQ
m1∕2λP δ5∕2
yQ (ZQ (P)τZP (0) - HP∞Qτ )HP∞-1yP = O
5∕2 3∕2
nP nQ
√mλp κδ3∕2
Substituting these estimations into equation 21 completes the proof of Theorem 1.
(27)
(28)
(29)
(30)
(31)
□
(；)
C.2 Proof of Theorem 2
In this subsection, we analyze the impact of pretrained weight matrix on the generalization perfor-
mance. First, we show that a model will converge if initialized with pretrained weight matrix. Based
on this, we further investigate the trajectories during transfer learning and bound kW - W(P)kF
with the relationship between source and target datasets.
21
Under review as a conference paper at ICLR 2020
C.2.1 Convergence of transferring from pretrained representations
Similar to Du et al. (2019), the proof is done with induction, but since we start from W(P) instead
of randomly initialized W0 in the transferring process, we should use the randomness of W0 in an
indirect way. Concretely, we should use lemma 1 to bound the difference between each column of
W(P) and randomly initialized W(0) when proving the induction hypothesis.
Theorem 3 (Convergence of Transfer Learning). Under the same conditions as in Theorem 1, if
we set the number of hidden nodes m
86
nP nQ	ʌ
λP6λPK2δ10 J
and the learning
rate η = O (箸)then with probability at least 1 一 δ over the random initialization we have for
k = 0, 1, 2, . . .
kuQ(k) 一 yQk22
Mq(p )-yQk2∙
(32)
The following lemma is a direct corollary of Theorem 3 and lamma 1, and is crucial to analysis to
follow.
Lemma 4. Under the same conditions as Theorem 3, with probability at least 1 一 δ over the random
initialization we have ∀r ∈ [m], ∀k ≥ 0,
kwQ,r(k) 一 wr(0)k2 ≤
4√nPIlyP - UP(O)Il2 + 4√nQkyQ - UQ(P)k2
√mλp	√mλQ
O (√⅛).
(33)
We have the estimation of IwQ,r(k) 一 wr(0)I2 from lemma 1. From IwQ,r(k) 一 wr(0)I2 ≤
IwQ,r(k) 一 wr (P)I2 + Iwr(P) 一 wr(0)I2, we can proove lemma 4 by estimating
IwQ,r(k) 一 wr(P)I2.
k
IwQ,r(k) 一 wr (P)I2 =η X I
k0=0
∂L(W(k0))
∂Wr(k0)
I2
k
≤η X
k0=0
√nQkyQ - uQ(k)k2
√m
V X∞ √nQ(I- η2Q )k /2	0X∣∣
≤η 工--------√m-----kyQ - uQ(k )k2
k0=0
= 4√nQkyQ - UQ(P)∣2
√mλQ
Weakohave ∣∣yQ 一 uq(0)∣∣2 = O (K√√nQ), and UQ(P) - uq(0) = Zq(0)>(Zp(0)H∞-1 VP +
) + 1 + 2. Substituting lemma 3, equation 6, and equation 22 into IUQ(P) - UQ (0)I2 completes
□
the proof.
Now we start to prove Theorem 3 by induction.
Condition 1. At the k-th iteration, we have IUQ (k) - yQ I22 ≤
(1 - ηλ20Q )k∣UQ(P )-yQ∣2.
We have the following corollary if condition 1 holds,
Corollary 1. If condition 1 holds for k0 = 0, . . . , k, for every r ∈ [m], with probability at least 1 - δ,
IWQ,r(k) - Wr(0)I2 ≤
4√⅞∣yp - UP(0)∣∣2 + 4√nQkyQ - UQ(P州2
√mλp	√mλQ
, R0.
If k = 0, by definition Condition 1 holds. Suppose for k0 = 0, . . . , k, condition 1 holds and we
want to show it still holds for k0 = k + 1. The strategy is similar to the proof of convergence
22
Under review as a conference paper at ICLR 2020
on training from scratch. By classifying the change of activations into two categories, we are
able to deal with the ReLU networks as a perturbed version of linear regression. We define the
event Air = wr (0)>xQ,i ≤ R, where R
cλ2Q for some small positive constant C to control the
nQ
magnitude of perturbation. Similar to the analysis above, we let Si = {r ∈ [m] : I{Air } = 0} and
Si = [m] \ Si. Since the distribution of w『(0) is Gaussian, we can bound the value of each Air and
then bound the size of Si just as we have estimated in equation 17 above.
Lemma 5. With probability at least 1 - δ over the initialization, we have Pn=ι |Si| ≤ CmnQR for
some positive constant C > 0.
The following analysis is identical to the situation of training from scratch.
1m
uQ,i(k + 1) - UQMk) = √m∑^ar (σ (WQ,r(k + 1)>xQ,i) - σ (wQ,r (k)>xQ,i)) ∙
By dividing [m] into Si and Si, We have,
I1i ,
I2i ,
√m X ar (σ (wQ,r(k + 1)>xQ,i) -
m r∈Si
√m X ar (σ (WQ,r (k + 1)>xQ,i) 一
r∈Si
σ (wQ,r (k)>xQ,i))
σ (wQ,r(k)>xQ,i)).
We view I2i as a perturbation and bound its magnitude. Because ReLU is a 1-Lipschitz function and
|ar | = 1, we have
∣p∣‹ η X√dL(WQ(k))YX ∣<ηlSi∣	dL(WQ(k))
ll2l≤ m2 ；⅛Jl ∂ wQ,r(k) ) xQ,i1 ≤ √mi maX]k ∂wQ,r (k)
.——.	1
k2 ≤ MS M Q kUQ(k)-yQk2
m
By Corollary 1, we know kwQ,r(k) - wr(0)k ≤ R0 for all r ∈ [m]. Furthermore, with the conditions
onm, we have R0 < R. Thus I wQ,r(k + 1)>xQ,i ≥ 0 = I wQ,r (k)>xQ,i ≥ 0 for r ∈ Si..
nQ
Ii = -m £ x>,ixQj(UQj- yQj) EI {wQ,r (k)>xQ,i ≥ 0, wQ,r (k)>xQj ≥ 0}
j=1
nQ
r∈Si
m
- m Ex>,ixQj (uQ,j - yQ,j) EI {wQ,r (k)>xQ,i ≥ 0, wQ,r(k)> XQj ≥ 0}
j=1
nQ
r=1
+ m ∑xQ,ixQ,j (uQ,j - yQj) ΣI {wQ，r (k)>xQ,i ≥ 0, wQ,r (k)>xQj ≥ 0}
j = 1	r∈Si
Where m Pm=ι xQ,ixQ,j I {wQ,r (k)>xQ,i ≥ 0, wQ,r(k)> XQj ≥ 0} is just the (i,j )-th entrY of a
discrete version of Gram matrix HQ∞ defined in Section 6.1 and
nQ
| m X x>,ixQj (uQ,j - yQ,j) X I {wQ,r(k)>xQ,i ≥ 0, wQ,r(k)>xQj ≥0} |
j=1	r∈Si
nQ	n--
≤ m |S i| X |uQj (k) - yQj | ≤ n m Q |S i|kuQ(k) - yQ∣∣2∙
m j =1	m
For ease of notations, denote by kHQ(k)⊥ k2 the matrix whose i, j entry is
mxQ,ixQj (uQ,j - yQ,j) Pr∈Si I {wQ,r(k)>xQ,i ≥ 0, wQ,r(k)>xQj ≥ 0}. Therefore, the
L2 norm of kH(k)⊥ k2 is bounded with
techniques as training from scratch.
nQ 1
kuQ(k +1)- uQ(k)k2 ≤ η2 Xm
i=1 m
CnQR. To bound the quadratic term, we use the same
m
Xk
r=1
∂L(Wq (k))
∂wQ,r(k)
k2! ≤ η2n2QkyQ - uQ(k)k22.
23
Under review as a conference paper at ICLR 2020
With these estimates at hand, we are ready to prove the induction hypothesis.
kyQ - uQ(k + 1)k22
=kyQ - uQ(k) - (uQ(k + 1) - uQ(k))k22
=kyQ - uQ(k)k22 - 2(yQ - uQ(k))> (uQ(k + 1) - uQ(k)) + kuQ(k + 1) - uQ(k)k22
=kyQ - uQ(k)k22 - 2η (yQ - uQ(k))> HQ(k) (yQ - uQ(k))
+2η (yQ - uQ(k))> HQ(k)⊥ (yQ - uQ(k)) -2(yQ - uQ(k))> I2
+kuQ(k + 1) - uQ(k)k22
,1	、	2CηnQR	2CηnQ,2R	2 2	2
≤(I - nλQ + δ-----------1 δ----------+ η nQ)kyQ - uQ(k)k2
≤(I - η∣Q )kyQ- uQ(k)k2.
The third equality we used the decomposition of u(k + 1) - u(k). The first inequality we used the
Lemma 2, the bound on the step size, the bound on I2, the bound on kH(k)⊥ k2 and the bound on
ku(k + 1) - u(k)k22. The last inequality we used the bound of the step size and the bound of R.
Therefore Condition 1 holds for k0 = k +1. Now by induction, We prove Theorem 3.	□
Similar to the analysis of lemma 3, we can show the change of ZQ(k) and HQ (k) is negligible under
the conditions of sufficiently large m.
Lemma 6. Under the same conditions as Theorem 3 in the transferring process, with probabilility
no less than 1 - δ,
kZQ(0)-ZQ(k)kF=O
3/2 5/4
nP nQ
个 m 2 λp Xqk263/2
kHQ(0)-HQ(k)kF=O
√mλp λQK2δ 3
This is a direct corollary of lemma 4 and can be proved by the same techniques as lemma 3. Now
we can continue to analyze the trajectory of transferring W(P) to the target dataset by dividing the
activations in to the two categories as in the proof of Theorem 3.
uQ(k+ 1) - uQ(k) = -ηHQ(k)(uQ(k) - yQ) + 3(k),
(34)
nQ	nQ
lle3(k)k2 = X m X x>,ixQ,j (UQj- yQ,j) E I {wQ,r (k)>xQ,i ≥ 0, wQ,r (k)>xQ,j ≥ 0} + I2
i=1	j = 1	r∈Si
≤X2η√nQ|Si∣kuQ(k) -yQk2,
i=1	m
where ∣Si∣ ≤ CmnQR = O (√mnPnQ ) , ∀r ∈ [m], ∀k ≥ 0. Then substituting Hq(k) with H∞
using the bound on kHQ∞ - HQ(k)kF, we further have,
uQ(k + 1) - uQ(k) = -ηHQ∞ (uQ (k) - yQ) + 3(k) + ζ(k),	(35)
kζ(k)k2 ≤ηkHQ(0) - HQ(k)kF kuQ(k) - yQk2
np n7Q2	!	(36)
√mλp λQK2δ 3 ，
where the second inequality holds with Theorem 3. Taking sum over each iteration,
k-1
uQ(k)-yQ = (I -ηH∞)k (uq(p)-yQ)+X (I -ηH∞)k (Z(k-ι-t)+∖(k-ι-t)). (37)
t=0
24
Under review as a conference paper at ICLR 2020
k-1
ke(k)k2 = k X(I- ηH∞)k (Z (k - 1 -1) + G(k - 1 - t))k2
t=0
≤k
k-1
• O
ηnP nQ/2
√mλp XqK2 δ 3
nP nQ
√mλp λQκ2δ 3
(38)
where We notice maxk>o k(1 - ηλ4Q)
the target dataset,
O
. Then in the K iterations of transferring to
vec(WQ(K)) - vec(W(P))
K-1
= X vec(WQ(k + 1)) - vec(WQ(k))
k=0
K-1
= -η X ZQ (k)(uQ (k) - yQ)
k=0
K-1	K-1
= XηZQ(k)(I-ηHQ∞)k(yQ-uQ(P))- X ηZQ(k)e(k)
k=0	k=0
K-1
= X ηZQ(0)(I - ηHQ∞)k(yQ - uQ(P))
k=0
K-1	K-1
+ X η(ZQ(k) - ZQ(0))(I - ηHQ∞)k(yQ - uQ(P)) - X ηZQ(k)e(k).
k=0	k=0
(39)
The first term is the primary part, while the second and the third are considered perturbations and
could be controlled using lemma 6 and equation 38.
K-1
k X η(ZQ(k) - ZQ(0))(I - ηHQ∞)k(yQ - uQ(P))k2 = O
k=0
np 5/2nQ/4
qm1∕2 λP λQκ2δ3∕2
(40)
since kZQ(k) - ZQ(0)kF is bounded, the maximum eigenvalue of HQ∞ is λ-Q1.
K-1
k X ηZQ(k)e(k)k2
k=0
K-1	k-1
=k X ηZQ(k) X (I - ηH∞)k (Z(k - 1 - t) + C3(k - 1 - t))k2
k=0	t=0
≤ η√nQ ≤ k
k-1
•O
√mλp XqK2 δ 3
η√nQ ∙ O(η2⅛) • O
√mλp XqK2 δ 3
√mxp λQκ2δ 22	,
(41)
25
Under review as a conference paper at ICLR 2020
where We use Ek=I k。- ⅜Q)	= O (η2λ2^卜 With these estimations at hand, We are ready
to calculate the final results
kWQ(K) - W(P)k2F
= kvec(WQ(K)) - vec(W(P))k22
K-1	K-1
=η2(yQ-uQ(P))> X (I - ηHQ∞)k>ZQ(0)>ZQ(0) X (I - ηHQ∞)k(yQ - uQ(P))
k=0	k=0
+o n-nρnQ—!
+	m1∕4λp λQκ2 δ3/2)
=(yQ - UQ(P))>H∞-1(yQ - UQ(P)) + O nnQlo∕4δ"! + O ( 1/4:心 2洌2!
Q	∖ m1/4λQ	m1/4XpλQκ2δ3∕2 I
(42)
Using equation 19, We have
yQ - UQ (P)
= yQ - (ZQ (0)> (ZP(0)(HP∞)-1yP + ) + 1 + 2) - UQ(0)
=yQ -HP∞Q>(HP∞)-1yP+(HP∞Q> - ZQ(0)>ZP(0))(HP∞)-1yP - ZQ(0)> - 0 + UQ(0)
With lemma 3, We have
k(H∞Q> - Zq (0)> ZP (0))H∞-1yp kF ≤ O (二P PlQ 3 1	(43)
∖ λ∕mλpκδ 2
Combine the estimation above With equation 22 and equation 26. We also have kUQ (0)k =
O (√√Qκ).
yQ -UQ(P) =yQ -HP∞Q>(HP∞)-1yP+4	(44)
k4k2 = O
5
2
P2	Q
m1λpκδ3
+。(』LOl -ι 3 - 1+ O
λPδ	m4λpκ2δI	∖m2
fn 2 κ
ɪ
√δ
O ⅛ + O (分
∖m4 λpK2 δ) P P
Substitute equation 44 into equation 42, We have
kWQ(K)-W(P)k2F=(yQ-HP∞Q>HP∞-1yP)>HQ∞-1(yQ-HP∞Q>HP∞-1yP)
+2 & lle4k2kyQ - h∞Q > H∞ 1yPk2 + 豆 ∣∣e4k2
+O nnQlog δ1/4! + O P_nPnQ_!
m1∕4λQ	m1∕4λPλQκ2δ3/2 I
(45)
H4λλ
1-4
P
y
Which completes the proof.
□
26