Under review as a conference paper at ICLR 2020
Neural Embeddings for Nearest Neighbor
Search Under Edit Distance
Anonymous authors
Paper under double-blind review
Abstract
The edit distance between two sequences is an important metric with many
applications. The drawback, however, is the high computational cost of
many basic problems involving this notion, such as the nearest neighbor
search. A natural approach to overcoming this issue is to embed the se-
quences into a vector space such that the geometric distance in the tar-
get space approximates the edit distance in the original space. However,
the known edit distance embedding algorithms, such as Chakraborty et al.
(2016), construct embeddings that are data-independent, i.e., do not ex-
ploit any structure of embedded sets of strings. In this paper we propose
an alternative approach, which learns the embedding function according to
the data distribution. Our experiments show that the new algorithm has
much better empirical performance than prior data-independent methods.
1	Introduction
The edit distance is a popular metric for computing the dissimilarity between two strings.
Given strings x and y, the edit distance dE (x, y) between them is equal to the minimum
number of character insertions, deletions and substitutions needed to transform x into y. The
metric and its many variants have been defined and studied since the 1960s (Levenshtein,
1966; Wagner and Fischer, 1974). It has found many applications in computational biology
(to measure the similarity between DNA/RNA sequences), natural language processing,
information theory and other fields.
From the algorithmic perspective, the main drawback of edit distance is its computational
complexity. The best known algorithm for computing the distance between two strings of
length n takes time that is roughly quadratic in n (Masek and Paterson, 1980); there is also
evidence that this runtime is essentially optimal (Backurs and Indyk, 2015). Furthermore,
many applications require finding the closest string in a large collection of (say) N strings.
Thus, even if we could estimate the edit distance in time O(n), computing the distances to
all strings in the collection would take O(Nn) time, which is infeasible for large data sets.
A natural approach to resolving both issues relies on low-distortion embeddings. The idea
is to construct a mapping f that maps strings into a d-dimensional space <d , such that the
edit distance dE(x, y) is approximated by the distance kf(x) - f(y)k between the images
of the strings, up to some factor D called distortion. If the mapping f can be evaluated
efficiently, this immediately implies an efficient way to compute the edit distance. Further-
more, embedding strings into space equipped with a norm ∣∣ ∙ k makes it possible to use
nearest neighbor search algorithms designed for that norm. For many normed spaces, e.g.
Minkowski norms `pd , many efficient approximate nearest neighbor search algorithms are
available. This dual goal has motivated a long line of research focused on designing efficient
low-distortion embeddings of variants of edit distance into normed spaces (Muthukrishnan
and Sahinalp, 2000; Cormode et al., 2001; Ostrovsky and Rabani, 2005; Batu et al., 2006;
Jowhari, 2012; Andoni and Onak, 2012; Chakraborty et al., 2016). The method given in
the last paper (Chakraborty et al., 2016) was shown to yield good results in practice, in the
context of large scale similarity joins, i.e., identifying all pairs of similar sequences (Zhang
and Zhang, 2017).
1
Under review as a conference paper at ICLR 2020
Negative
Figure 1: Triplet loss for embedding DNA sequence inputs. Given anchor point A, a positive
point P that is close to A in terms of edit distance, and a negative point N that is not close
to A. We hope our embedding function f satisfies f (A) is closer to f (P ) than f(N) in `p.
A weakness of the aforementioned algorithms is that they are data-independent, i.e., they
do not exploit any structure of embedded sets of strings. In contrast, over the last decade,
a large body of research on “learning to hash” showed that data-dependent methods often
yield (binary) embeddings with a significantly lower distortion than their data-independent
counterparts (Wang et al., 2018). Such methods, however, are typically tailored to obtaining
binary representations of vectors that already live in a geometric (typically Hilbert) space.
It is thus natural to investigate whether similar benefits of data-dependent methods can be
obtained for edit distance embeddings as well.
In this paper we investigate whether the “learning to hash” approach can yield lower dis-
tortion embeddings than those obtained using data-independent methods. Since our goal
is to obtain distance-preserving mappings of sequences, we use Recurrent Neural Networks
as a model implementing those mappings. Specifically, we leverage a two-layer GRU net-
work, depicted in Figure 2 and described in more details in Section 4. The network inputs
a sequence of characters (e.g., A, G, T or C for DNA sequences), and outputs a vector of
symbols, such that the distance between the vectors approximates the edit distance between
the original sequences.
The standard loss function for optimizing the network is the triplet loss (see Figure 1), which
has two problems in our scenario. The first problem is distance scaling: triplet loss only
controls the relative distance ordering among data points after embedding, rather than the
absolute embedding distance between any two data points. This prevents us from getting
accurate edit distance estimation from the embedding. To alleviate this issue, we designed
a warm-start phase (Phase 1) before optimizing triplet loss (Phase 2), which initializes the
embedding so that the discrepancy between the original edit distance and the embedding
distance is minimized. The second problem is worst case performance of our model. Due to
the nature of learning, the network we learned fits the training data better, and may not do
well on unseen hard examples. To alleviate this issue, we append a back up phase (Phase
3), which deals with hard cases.
We emphasize that all the three phases are optimizing the same network. We run Adam
algorithm for each phase until convergence, and then use its final state as the initial state
for the next phase. Although in principle one may use a single phase with combination of
different loss functions to train our model, this requires additional hyperparameter adjust-
ment for the combining ratios. Our current three phase training does not need additional
hyperparameters, and therefore can easily fit different scenarios.
The Phase 3 design is non-trivial. Our starting observation is that Recurrent Neu-
ral Networks that we employ are not “compatible” with the data-independent mappings
of Chakraborty et al. (2016)(which we call CGK from now on). This is because CGK can
generate different number of copies for every input character, which is a discrete decision
that is hard to be learned by neural networks. We show, however, that this “extra power”
of the algorithm is not necessary. In particular, we introduce a “truncated” version of the
CGK algorithm (referred to as CGK’), that outputs only 1 or 2 symbols per round. We
then show that CGK’ has the same guarantees as the original CGK algorithm. Based on
this observation, we design our Phase 3 to be an optimized version of CGK’. Specifically,
since each round there will be either 1 or 2 symbols in CGK’, we can directly compare the
2
Under review as a conference paper at ICLR 2020
two cases and use the difference as approximation to the gradients to train our model. Em-
pirically, Phase 3 does not affect the recall on ordinary datasets much, but greatly improves
performance on hard instances.
To evaluate the embeddings, we employ them for nearest neighbor search in a collection
of sequences, and compare their performance to CGK. As in Zhang and Zhang (2017)
we use three data sets: Gen50ks (containing sequences from the human genome pro ject),
UniRef (protein sequence dataset from the UniProt pro ject) and Trec (containing textual
information from Medline, an online medical information database). After learning the
embeddings, we generate a set of queries, and for each query we identify C sequences in the
data set (called candidates) that are closest to the query in the embedded space. We then
measure “top-K recall”, i.e., the fraction of the actual top K nearest neighbors (computed
using the exact edit distance) returned as one of the C candidates. Our results demonstrate
that, over a wide range of parameters K and C , our learned embedding consistently achieves
higher recalls than the algorithm of CGK, often by a factor of 3 or more. Similarly, our
method often obtains identical recall to that of CGK while using an order of magnitude fewer
candidates. This means that in a typical nearest neighbor search “filtering” pipeline, where
the candidates are quickly identified in the embedded space and then the exact edit distance
is computed between the candidates and queries to identify the final answers, our method
reduces the number of exact edit distance computations by up to one order of magnitude.
Moreover, our learned embedding also has much more organized structure than CGK, as
shown in Appendix A.4.
2	Related work
2.1	Edit Distance Computing
Exact and approximate algorithms for edit distance computation have been broadly studied,
see Navarro (2001) for an overview. For exact algorithms, Wagner and Fischer (1974) gave
O(n2)-time algorithm using dynamic programming. This was later improved to O(n2/ log n)
by Masek and Paterson (1980). Faster approximation algorithms were studied as well, see
e.g., Landau et al. (1998); Batu et al. (2003); Bar-Yossef et al. (2004); Batu et al. (2006);
Andoni et al. (2010); Andoni and Onak (2012). The current state of the art consists of a
near-linear algorithm of Andoni et al. (2010) guaranteeing (log n)c -approximation (with a
trade-off between the running time and the exponent c), and a constant factor approximation
O(n2-2/7)-time algorithm of Chakraborty et al. (2018). To the best of our knowledge,
neither algorithm has been implemented.
2.2	Edit Distance Embedding
As mentioned in the introduction, there has been a long line of research focused on designing
embeddings of various variants of the edit distance into geometric space. For the edit
distance as defined in the introduction, the best known algorithm is due to Ostrovsky
and Rabani (2005), who designed an embedding into the Hamming space with distortion
2O(√log n log log n). This algorithm Was quite complex, and to the best of our knowledge has
never been implemented. Recently, Chakraborty et al. (2016) gave a substantially different
embedding with distortion O(k) where k is the edit distance between the original strings.
The empirical evaluation of the algorithm, given in Zhang and Zhang (2017), demonstrated
that the embedding yields good results in practice, and that the empirical distortion of this
embedding is typically much lower than the worst case bound. Zhang and Zhang (2019) also
proposed a novel string partition based algorithm for edit similarity join, without generating
embeddings. Gomez-Bigorda et al. (2017) presented a novel approach for word embedding
in Levenshtein space to improve downstream handwritten word spotting. Their method is
similar to our Phase 1, but our model deals with longer sequences instead of words, and
also adopts triplet loss training to capture sequence similarity (Phase 2) and CGK based
algorithm to learn sequence alignment (Phase 3).
2.3	Similarity Capturing
Neural models are powerful to learn embeddings that capture latent similarity. For example,
word2vec (Mikolov et al., 2013) learns semantic and syntactic similarity of a word from its
3
Under review as a conference paper at ICLR 2020
f(x) [ A	B	C	D
一	个	个	个
Linear Layer
	T^^		-∏c^^		^^		-^T
	GRU	-÷	GRU	-÷	GRU	-÷	GRU
	t		t		t		t
	GRU	-÷	GRU	-÷	GRU	-÷	GRU
x[	t 1		t 0		t 1		t 1
…]
■÷ •••
…]
Figure 2: Our embedding model leveraging two-layer GRU.
linguistic context. The idea has been extended to learn sentence or document similarity
(Le and Mikolov, 2014), biological sequence similarity (Asgari and Mofrad, 2015; Ng, 2017).
The aforementioned methods characterize input representations based on their context. A
different approach aims to learn a distance metric, see Kulis (2013) for an overview. Among
them, triplet network (Weinberger and Saul, 2009; Hoffer and Ailon, 2015; Schroff et al.,
2015) uses triplet loss to learn similarity over triplets. Part of our model gets inspiration
from triplet network.
3 Preliminaries
We use D to denote the alphabet, i.e., the set of symbols used in the strings. For any
two strings x,y ∈ D* We use dE(x,y) to denote the edit distance between X and y, i.e.,
the minimum number of symbol insertions, deletions or substitutions needed to transform
x into y. We define the distance metric d(x, y) = ||x - y||22 . If the strings have different
lengths, we pad the shorter string to make their lengths equal. In the binary case, d(x, y)
corresponds to the Hamming distance between x and y.
The CGK algorithm is described below. Its input consists of a string x of length n (to be
embedded), and a sequence of 6n random bits r, which are interpreted as a sequence of
3n random hash functions hi,…,h3n : {0,1} → {0,1}. It produces a sequence f (x,r),
obtained by replicating each symbol in x at least once (the actual number is a random
variable depending on x and r).
Algorithm 1 CGK Embedding Function f
Input: x ∈ {0, 1}n, and a random string r ∈ {0, 1}6n
Output: f(x, r) ∈ {0, 1}3n
Interpret r as the description of hi,… ,h3n : {0,1} → {0,1}.
Initialization: i = 1, a = (0,…，0) ∈ {0,1}3n
for j = 1, 2,…，3n do
aj = xi;
i = i + hj (xi);
end for
Set f(x, r) = a.
4	Learning-based Embedding Method
Recurrent Neural Network (RNN) models proved effective for sequence embedding tasks
like language modeling (Mikolov et al., 2011), image captioning (Mao et al., 2015), protein
sequence embedding (Bepler and Berger, 2019), etc. Long Short Term Memory (LSTM)
(Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit (GRU) (Cho et al., 2014)
utilize gates to overcome the long-term dependency problem and often yield improved em-
pirical results. Our learning-based embedding model leverages two-layer GRU depicted in
Figure 2. The input is a sequence of n characters in one-hot encoding, x = {xi, x2, ..., xn}.
We feed the input sequence x into two-layer GRU and a linear layer to get the embedding
sequence y = f (x). Here δi represents the dropout rate in GRU. We use Adam optimizer
and 4:1 train:validation split for cross-validation to tune the hyper-parameters.
4
Under review as a conference paper at ICLR 2020
Phase 1
X T Embed
→ f1 (X)
Phase 2
P — Embed T f2 (P)
Phase 3
f3(X) [ A B C …]
y→ Embed
|dE - d|
↑
→ fι(y)
aT Embedl*2(a)+0Splet
n — Embed T f2 (n)
Add minor?
V
f3(x) [ AA BB CC …]
Figure 3: Our three phase training method: Phase 1 as warm start; Phase 2 learns similarity;
Phase 3 for hard case. Embed refers to embedding model depicted in Figure 2.
hi1 = GRU1(xi,hi1-1)	(1)
hi2 = GRU2(hi1δi,hi2-1)	(2)
yi = Whi2 + b	(3)
y = {y1,y2,...yn}	(4)
4.1	Phase 1: Absolute Loss as Warm Start
We use Phase 1 to tune the embedding such that the absolute difference between the edit
distance and the embedding distance is small. In every iteration, we take a pair of input
strings x and y , and pass it through two embedding networks denoted as f1 , which share
weights. The loss is defined as:
L1 = |dE(x,y) - d(f1(x), f1(y))|	(5)
4.2	Phase 2: Triplet Loss for Similarity Capturing
Phase 2 initializes the network with weights computed in Phase 1. It leverages triplet loss
(Weinberger and Saul, 2009) to capture the dependencies between string similarities. In
each iteration we randomly choose an original string as the anchor and define positive and
negative strings with respect to their edit distances from the anchor. More specifically, We
randomly sample the positive pairs from top-5 edit distance and sample negative pairs as
the top-30 distance in the embedded space, which is a fairly standard approach for triplet
loss (Weinberger and Saul, 2009; Sablayrolles et al., 2019). The anchor, the positive and the
negative string form a triplet. The triplets are passed through three networks, denoted by
f2 , which share common weights. For an anchor a, a positive string p and a negative string
n, we formulate the triplet loss as:
L2 = maX(0,d(f2(a),f2(p)) - d(f2(a),f2(n)) + margin)	(6)
Further, we set margin to be dE (a, n) - dE (a, p). Intuitively, we hope that the difference
between the hamming distance between the positive and the negative pairs is larger than
that of the edit distance, namely d(f2(a), f2 (n)) - d(f2(a), f2 (p)) ≥ dE (a, n) - dE (a, p). See
Figure 1 for illustration.
4.3	Phase 3: Insertion Loss for Hard Cases
Phase 3 is initialized using weights computed in Phase 2. During each iteration a pair
of input strings is passed through two embedding networks denoted as f3 , which share
weights. Let x and y represent the original input strings and f3 (x) and f3 (y) denote their
embeddings. Denote the CGK’ (a variant of CGK, see Algorithm 3 in the appendiX) with
random initialization as g, and we apply it on f3 (x), f3(y) and get f30 (x) = g(f3(x)),
f30 (y) = g(f3 (x)). f30 is our final embedding function, which is essentially a concatenation of
an embedding network f and the CGK’ algorithm g.
Optimization in Phase 3 has two different losses: the absolute loss and the insertion loss.
The absolute loss (L3 ) ensures that the distance of the final embedding is close to the edit
distance of x and y. The insertion loss is defined as follows. For each character c in f3 (x), we
5
Under review as a conference paper at ICLR 2020
compare the two cases that with or without duplication of c to optimize the hash functions
and the threshold. See Appendix A.1 for more details.
L3 = |dE (x, y) - d(f30 (x), f30 (y))|	(7)
As we mentioned before, we use CGK’ instead of CGK because it restricts the number of
replications of each symbol to be at most 2, therefore we can compare the two cases to get an
approximation to the gradient, and then optimize the parameters in CGK’. In what follows
we show that this modification does not affect the worst-case guarantees of the method (i.e.,
CGK’ with random initialization). See Appendix A.2 for the proof.
Theorem 1. The mapping f : {0, 1}n × {0, 1}4n → {0, 1}2n computed by Algorithm 2
satisfies the fol lowing properties:
(1)	Given r ∈ {0, 1}4n and f(x, r) ∈ {0, 1}2n as the input, we can always decode x.
(2)	For every x,y ∈ {0, 1}n, dE (x, y)/2 ≤ d(f (x, r), f (y, r)).
(3)	There exists a constant c0 such that for every positive constant c ≥ c0 and every x, y ∈
{0, 1}n , d(f(x, r), f(y, r)) ≤ C ∙ dE(x, y)2 with probability at least 1/2.
5	Experiments
5.1	Datasets
We evaluate our neural embedding with three datasets as in Zhang and Zhang (2017).
Specifically, we use Gen50ks: a genetic sequence dataset from the human genome pro ject;
UniRef : UniRef90 protein sequence dataset from the UniProt project; and Trec: a refer-
ence dataset from Medline (an online medical information database). See Table 1 for their
properties. Furthermore, following Zhang and Zhang (2017) we truncate some input strings
to avoid dealing with very long outlier strings. For Gen50ks dataset, the string lengths
sharply concentrate around 5000, so we set the truncation length to be 5000 and threshold-
ing affects the distances only minimally. For UniRef and Trec dataset, string lengths vary
drastically. In order to avoid biasing the original data distribution, we set a rather large
threshold(1500 and 2500) so that only a small proportion(1.16% and 0.97%) of the strings
exceed the threshold. For Gen50ks, UniRef and Trec dataset, we train our model with 1000,
2000, 2000 strings (including base and query) respectively, which means that we train with
less than 2% data points in the whole dataset.
Table 1: Statistical analysis of three datasets.
Dataset	Gen50ks	UniRef	Trec
Alphabet Size	4	:	25	37
Dataset Size	50000	400000	233435
Avg Len.	5000	445	1217
Max Len.	5152	35213	3947
Min Len.	4829	200	80
5.2	Nearest Neighbor Search
5.2.1	Exact Nearest Neighbor Search
We evaluate our model in the context of nearest neighbour search. We randomly select 100
queries and use the remainder of the dataset as the base set. Our goal is to find strings in
the base set most similar to the query strings. To this end we use linear scan to identify the
C sequences in the data set (called candidates) that are closest to the query in the embedded
space. We compare embeddings generated by our neural model, CGK and its close variant
CGK’, as well as the identity embedding (which is optimal if no insertions or deletions
are needed). Since CGK and its variants are randomized, we measure the average of five
independent runs.
For all three datasets we observe that our neural model offers significant performance gains
compared to the other embedding methods.
6
Under review as a conference paper at ICLR 2020
(a) top-1	(b) top-10	(c) top-100
Figure 4: Gen50ks: top K recalls for our algorithm (Neural) vs. CGK, CGK’ and identity,
as a function of the number of candidates.
Figure 5: Uniref: top K recalls for our algorithm (Neural) vs. CGK, CGK’ and identity, as
a function of the number of candidates.
(a) top-1	(b) top-10	(c) top-100
Figure 6: Trec: top K recalls for our algorithm (Neural) vs. CGK, CGK’ and identity, as a
function of the number of candidates.
5.2.2	LSH-based Approximate Nearest Neighbor Search
In the previous section we used linear scan to find C nearest neighbors in the embedded
space. Although this task can be often accomplished efficiently (e.g., using GPUs), in
many scenarios more efficient approximate nearest neighbor search algorithms need to be
employed. In this section we investigate one such instantiation employing Locality-Sensitive
Hashing (LSH). We note that the same framework was used in Zhang and Zhang (2017) to
evaluate CGK.
We use the cross-polytope LSH family under the Euclidean distance with multi-probe LSH
(Andoni et al., 2015). For genetic dataset Gen50ks, we build 50 hash tables and 10-bit hash
functions(We vary the number of hash bits and find 10-bit hash functions give the best
performance). We test with five randomly selected 100 queries and record the mean and
standard deviation as in Table 2.
Table 2: Top-1 Recall for Gen50ks using LSH
Number of Probes	50	100	200	400	800	1600	3200
Neural	50.2±2.7 63.6±1.8 74.0±1.6 84.2±2.8 93.0±3.5 98.6±1.3 99.8±0.4						
CGK	18.0±2.9	23.6±2.8	34.4±2.0	47.6±3.7	68.6±5.2	88.8±3.4	98.2±1.3
CGK’	11.0±2.6	16.8±2.2	26.0±3.0	43.0±3.8	65.8±4.6	83.6±3.4	98.0±0.6
Identity	4.6±0.4	9.8±1.6	19.2±1.7	33.2±4.0	56.8±3.7	83.4±2.6	97.4±0.8
7
Under review as a conference paper at ICLR 2020
Table 3: Recall for Gen50ks, edit distance join threshold K = 50
Number of Probes	50	100	200	400	800	1600	3200
Neural	91.6	96.4	98.3	99.5	100.0	100.0	100.0
CGK	79.6	83.9	86.6	90.6	94.5	97.6	99.7
CGK’	65.0	69.5	74.4	80.3	88.0	94.8	99.0
Identity	7.9	12.9	22.0	38.6	60.2	81.5	95.7
Table 4: Recall f	or Gen50ks, edit distance join threshold K =						100
Number of Probes	50	100	200	400	800	1600	3200
Neural	82.5	91.0	96.4	99.0	99.8	99.9	100.0
CGK	61.9	66.9	72.9	79.8	88.1	95.0	99.2
CGK’	42.6	48.5	55.5	65.3	78.6	91.8	98.8
Identity	6.3	11.1	20.6	36.7	59.5	82.8	96.6
Table 5: Ablation Study: Top-1 Recall for Gen50ks
Candidate Num	1	10	50	80	100	200	500	800	1000	2000	5000
Neural Phase 2+3	26.4	40.6	48.2	51.8	53.0	59.8	67.8	71.0	74.2	79.4	86.2
	±3.4	±4.9	±4.1	士3.4	士3.5	士3.4	士3.7	士3.1	士3.3	士3.7	士1.3
Neural Phase 1+3	7.6	14.8	19.4	22.0	24.4	28.2	34.4	40.0	42.8	51.8	63.6
	±0.4	±1.1	±1.0	士2.9	士3.0	士1.1	士1.9	士3.8	士3.4	士4.0	士4.4
Neural Phase 1+2	36.6	49.8	60.6	63.2	65.0	70.2	77.4	81.2	82.4	88.8	94.2
	±1.3	±3.1	±1.9	士1.7	士1.8	士2.7	士2.8	士1.8	士1.8	士2.4	士1.4
Neural Phase 1+2+3	36.2	53.4	60.8	62.8	64.0	66.8	71.8	75.2	76.4	81.8	87.0
	±1.8	±3.8	±3.7	士2.9	士3.0	士2.9	士1.7	士2.1	士2.6	士1.3	士1.5
5.3	String Similarity Join
Zhang and Zhang (2017) adopts the vanilla bit-sampling LSH for hamming distance, which
does not suit our embedding as the neural embedded strings are continuous values instead
of discrete characters. So for our string similarity join experiments, we continue to use the
cross-polytope LSH family under the Euclidean distance with multi-probe LSH.
The LSH table parameters are the same with the approximate nearest neighbor setting. We
run PassJoin (Li et al., 2011) to evaluate our join results.
5.4	Ablation Study
We further do ablation study to evaluate the function of three phases in our model. We
notice a performance drop by removing Phase 1 or Phase 2. We explain this as Phase
1 provides good initialization which makes similarity learning easier while Phase 2 learns
string similarity. Removing phase 3 generally does not lower the performance of top-1 recall
much. This might be that these datasets do not contain many hard instances(like coexisting
small shift and small substitution). Phase 3 does show its effectiveness in our synthetic hard
case dataset, see Appendix A.3. We further show the standard deviation by testing with
five different query sets, each of which contains 100 randomly selected strings.
6	Conclusion
In this paper, we introduce the idea of using neural embedding for computing nearest neigh-
bor search under edit distance. Our method has three phases, i.e., the warm-start phase
for generating an initial network with the correct scale, the triplet loss phase for learning
relative distance among data points, and the back up phase for dealing with hard cases. Our
last phase is non-trivial, because we designed a truncated version of CGK algorithm (aka
CGK’) and proved that it has worst case performance guarantee. Our phase 3 directly opti-
mizes CGK’ and easily solves hard cases. Overall, our method gives much better empirical
results than the existing methods for nearest neighbor search.
8
Under review as a conference paper at ICLR 2020
References
Andoni, A., Indyk, P., Laarhoven, T., Razenshteyn, I. P., and Schmidt, L. (2015). Practical
and optimal LSH for angular distance. In NIPS, pages 1225-1233.
Andoni, A., Krauthgamer, R., and Onak, K. (2010). Polylogarithmic approximation for edit
distance and the asymmetric query complexity. In FOCS, pages 377-386. IEEE Computer
Society.
Andoni, A. and Onak, K. (2012). Approximating edit distance in near-linear time. SIAM
J. Comput., 41(6):1635-1648.
Asgari, E. and Mofrad, M. R. K. (2015). Protvec: A continuous distributed representation
of biological sequences. CoRR, abs/1503.05140.
Backurs, A. and Indyk, P. (2015). Edit distance cannot be computed in strongly sub-
quadratic time (unless SETH is false). In STOC, pages 51-58. ACM.
Bar-Yossef, Z., Jayram, T. S., Krauthgamer, R., and Kumar, R. (2004). Approximating
edit distance efficiently. In FOCS, pages 550-559. IEEE Computer Society.
Batu, T., Ergiin, F., Kilian, J., Magen, A., Raskhodnikova, S., Rubinfeld, R., and Sami, R.
(2003). A sublinear algorithm for weakly approximating edit distance. In STOC, pages
316-324. ACM.
Batu, T., Erguin, F., and Sahinalp, S. C. (2006). Oblivious string embeddings and edit
distance approximations. In SODA, pages 792-801. ACM Press.
Bepler, T. and Berger, B. (2019). Learning protein sequence embeddings using information
from structure.
Chakraborty, D., Das, D., Goldenberg, E., Koucky, M., and Saks, M. E. (2018). Approxi-
mating edit distance within constant factor in truly sub-quadratic time. In FOCS, pages
979-990. IEEE Computer Society.
Chakraborty, D., Goldenberg, E., and Koucky, M. (2016). Streaming algorithms for embed-
ding and computing edit distance in the low distance regime. In STOC, pages 712-725.
ACM.
Cho, K., van Merrienboer, B., Giilcehre, (C., Bahdanau, D., Bougares, F., Schwenk, H.,
and Bengio, Y. (2014). Learning phrase representations using RNN encoder-decoder for
statistical machine translation. In EMNLP, pages 1724-1734. ACL.
Cormode, G., Muthukrishnan, S., and Sahinalp, S. C. (2001). Permutation editing and
matching via embeddings. In ICALP, volume 2076 of Lecture Notes in Computer Science,
pages 481-492. Springer.
Gomez-Bigorda, L., Rusinol, M., and Karatzas, D. (2017). LSDE: levenshtein space deep
embedding for query-by-string word spotting. In ICDAR, pages 499-504. IEEE.
Hochreiter, S. and Schmidhuber, J. (1997). Long short-term memory. Neural Computation,
9(8):1735-1780.
Hoffer, E. and Ailon, N. (2015). Deep metric learning using triplet network. In SIMBAD,
volume 9370 of Lecture Notes in Computer Science, pages 84-92. Springer.
Jowhari, H. (2012). Efficient communication protocols for deciding edit distance. In ESA,
volume 7501 of Lecture Notes in Computer Science, pages 648-658. Springer.
Kulis, B. (2013). Metric learning: A survey. Foundations and Trends in Machine Learning,
5(4):287-364.
Landau, G. M., Myers, E. W., and Schmidt, J. P. (1998). Incremental string comparison.
SIAM J. Comput., 27(2):557-582.
9
Under review as a conference paper at ICLR 2020
Le, Q. V. and Mikolov, T. (2014). Distributed representations of sentences and documents.
In ICML, volume 32 of JMLR Workshop and Conference Proceedings, pages 1188-1196.
JMLR.org.
Levenshtein, V. I. (1966). Binary codes capable of correcting deletions, insertions, and
reversals. In Soviet physics doklady, volume 10, pages 707-710.
Levin, D. A. and Peres, Y. (2017). Markov chains and mixing times, volume 107. American
Mathematical Soc.
Li, G., Deng, D., Wang, J., and Feng, J. (2011). PASS-JOIN: A partition-based method for
similarity joins. PVLDB, 5(3):253-264.
Mao, J., Xu, W., Yang, Y., Wang, J., and Yuille, A. L. (2015). Deep captioning with
multimodal recurrent neural networks (m-rnn). In ICLR.
Masek, W. J. and Paterson, M. (1980). A faster algorithm computing string edit distances.
J. Comput. Syst. Sci., 20(1):18-31.
Mikolov, T., Kombrink, S., Burget, L., Cernocky, J., and Khudanpur, S. (2011). Extensions
of recurrent neural network language model. In ICASSP, pages 5528-5531. IEEE.
Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and Dean, J. (2013). Distributed
representations of words and phrases and their compositionality. In NIPS, pages 3111-
3119.
Muthukrishnan, S. and Sahinalp, S. C. (2000). Approximate nearest neighbors and sequence
comparison with block operations. In STOC, pages 416-424. ACM.
Navarro, G. (2001). A guided tour to approximate string matching. ACM Comput. Surv.,
33(1):31-88.
Ng, P. (2017). dna2vec: Consistent vector representations of variable-length k-mers. CoRR,
abs/1701.06279.
Ostrovsky, R. and Rabani, Y. (2005). Low distortion embeddings for edit distance. In
STOC, pages 218-224. ACM.
Sablayrolles, A., Douze, M., Schmid, C., and Jegou, H. (2019). Spreading vectors for
similarity search. In ICLR (Poster). OpenReview.net.
Schroff, F., Kalenichenko, D., and Philbin, J. (2015). Facenet: A unified embedding for face
recognition and clustering. In CVPR, pages 815-823. IEEE Computer Society.
van der Maaten, L. and Hinton, G. (2008). Visualizing data using t-SNE. Journal of
Machine Learning Research, 9:2579-2605.
Wagner, R. A. and Fischer, M. J. (1974). The string-to-string correction problem. J. ACM,
21(1):168-173.
Wang, J., Zhang, T., Song, J., Sebe, N., and Shen, H. T. (2018). A survey on learning to
hash. IEEE Trans. Pattern Anal. Mach. Intel l., 40(4):769-790.
Weinberger, K. Q. and Saul, L. K. (2009). Distance metric learning for large margin nearest
neighbor classification. J. Mach. Learn. Res., 10:207-244.
Zhang, H. and Zhang, Q. (2017). Embedjoin: Efficient edit similarity joins via embeddings.
In KDD, pages 585-594. ACM.
Zhang, H. and Zhang, Q. (2019). Minjoin: Efficient edit similarity joins via local hash
minima. In KDD, pages 1093-1103. ACM.
10
Under review as a conference paper at ICLR 2020
A Appendix
A.1 CGK’ algorithm and Phase 3
In Algorithm 2, we present the truncated version of CGK that we use in the paper. Com-
pared with Algorithm 1, the main difference is in CGK’, every character will be duplicated
for at most twice (with probability 0.5). Our Phase 3 is an optimized version of CGK’.
We use Algorithm 3 in Phase 3. The main difference between Algorithm 2 and 3 is that
we are using a threshold variable thres in the if-statement, to decide whether to duplicate
the character or not. Moreover, the if-condition is evaluated based on the inner product
between hash function hj and a one hot vector xi . Hence, Algorithm 3 is just a more general
version of Algorithm 2.
Then how do we optimize the parameters in Algorithm 3(E.g., thres and h)? Since Algo-
rithm 3 involves discrete decisions with if-statements, we manually define the gradients in
Algorithm 4, by comparing the case whether the character is duplicated or not and calculate
how it affects the loss.
Algorithm 2 CGK’ Embedding Function f
Input: x ∈ {0, 1}n, and a random string r ∈ {0, 1}4n
Output: f(x, r) ∈ {0, 1}2n
Interpret r as the description of hi,…,h?n : {0,1} → {0,1}.
Initialization: j = 1, a = (0,…，0) ∈ {0,1}2n
for i = 1, 2, ∙∙∙ ,n do
if hj(xi) then
aj = xi;
j=j+1;
else
aj+1 = aj = xi;
j=j+2;
end if
end for
Set f(x, r) = a.
Algorithm 3 Learning-Augmented Embedding Function f0
Input: x where xi is a one-hot vector, H is a 2 × 2n matrix interpreted as h1, ..., h2n,
thres ∈ R after optimization
Output: f0(x), set Duplicate which records the duplicated bits in f0(x)
Initialization: j = 1, a = 02×2n, Duplicate = 0
fθr i = 1, 2, ∙∙∙ ,n do
if hj ∙ Xi ‹ thres then
aj = xi;
j=j+1;
else
aj+1 = aj = xi;
Duplicate — Duplicate U{j}
j=j+2;
end if
end for
Set f0 (x) = a.
A.2 Proof of the Main Theorem
For simplicity, denote the two embedded strings f(x, r), f (y, r) as fx, fy . Below we always
use i to denote the index for the input strings, and use j to denote the index for the
embedded strings. We use idx(x, j ) to denote the corresponding index i for producing the
11
Under review as a conference paper at ICLR 2020
Algorithm 4 Optimization Scheme for h and thres in Algorithm 3
Input: x, y, f0(x), f0(y), H interpreted as h1, ..., h2n, thres, set Duplicate in Algorithm 3,
learning parameters , η
Output: H and thres after optimization
Initialization j = 1
L= |dH (f 0(x), f 0(y)) - dE (x, y)|;
for i = 1, 2, ∙∙∙ , n do
g(x) = f0(x)
if j ∈ Duplicate then
Remove g(x)j+1 from g(x);
L0 = |dH(g(x),f0(y)) - dE(x,y)|;
if L0 < L then
thres = thres + ;
hj = hj — η ∙ Xi;
end if
j=j+2;
else
Duplicate g(x)j in g(x);
L0 = |dH(g(x),f0(y)) - dE(x,y)|;
if L0 < L then
thres = thres - ;
hj = hj + η ∙ Xi ；
end if
j=j+1;
end if
end for
j-th index of output f(X, r). Define ∆j , idx(X, j) - idx(y, j) as the input index difference
between X and y at embedding index j .
In Algorithm 2, given the input X and index i, we may append one or two copies of Xi into
the output. We call the first copy the ma jor copy, and the second copy the minor copy.
The major copy always exists in the output, but the minor copy exists with probability 1/2,
depending on the hash function hj . For any index j , if fjx and fjy are both major copies or
both minor copies, we say j is aligned. Otherwise, j is not aligned. Initially, f1x and f1y
are major copies, therefore index 1 is aligned. See figure 7 for illustration.
We scan fx and fy from left to right, and for every index j such that j + 1 is not aligned but
j is aligned, We find the next aligned index j0, and group all the indices j + 1,j + 2,…,j0 — 1
as a sub-index string for the aligned index j . For the corner case that j 0 does not exist, we
pick j0 as the minimum final j value for computing fx and fy (So starting from j0, fx or
fy Will be padded With zeros).
NoW We focus on the aligned indices. For an aligned index j Without a sub-index string, We
knoW index j + 1 is also aligned, otherWise j + 1 Will be inside the sub-index string for index
j . In this case, We can shoW ∆j+1 — ∆j = 0 by discussing Whether index j + 1 has major or
minor copies.
On the other hand, consider an aligned index j With a sub-index string. In this case, We
knoW both fjx and fjy are major copies. Without loss of generality, We may assume fjx+1 is a
ma jor copy and fjy+1 is a minor copy (the other case is symmetric). We have the folloWing
lemma for a feW facts related to the sub-index string.
Lemma 2. Denote the next aligned index by j0 > j + 1. Assume the sub-index string has
length t , j0 — j — 1 > 0. Given T such that T/2 ∈ Z+, we have
(1)	fjx0 and fjy0 are major copies,
(2)	t ≤ T holds with probability 1 — 2-T ,
12
Under review as a conference paper at ICLR 2020
j	j0	j	j0
Figure 7: Two possible cases of the sub-index string
A : Major
A : Minor
φ : Aligned
：:Not Aligned
(3) Conditioned on t ≤ T,
∆j0 - ∆j
1,
0,
w.p. 2/3
w.p. 1/3
Proof. Since j0 is the first aligned index after j , we know that for any j < jo < j0 , jo is not
aligned, therefore one of fjx and fjy is major and the other one is minor. By Algorithm 2,
for the string with the minor copy, with probability 1, the next copy on jo + 1 is a major
copy; and for the string with the major copy, the next copy on jo + 1 is a major copy or
minor copy with equal probability. However, if both copies on jo + 1 is major, we know
jo + 1 is aligned, and thus j0 = jo + 1. In other words, this case only happens for the index
jo = j0 - 1, and for other indices, the two strings are not aligned, and have major copies in
turn. That already proves (1).
Since the two strings have major copies in turn, and only major copies will increment the
value of idx(x, j), we know
1, if t is odd
j0 - j =	0, otherwise ,
See Figure 7 for the two possible cases. Moreover, every step from j + 1 to j0 holds with
probability 1/2, so the probability that the sub-index string has length t is 2-t . By summing
over the probabilities of t from 1 to T , we proved (2). Finally, (3) holds by comparing the
probabilities of t ≤ T being odd and being even, which is exactly 2:1.	□
Remark 1. Based on Lemma 2(2), if there are l sub-index strings, and for a given T (to
be determined later), the probability that all sub-index strings are within length T is bounded
by 1 — l ∙ 2-T using union bound. Below We discuss the case conditioned on this event.
Lemma 2(3) states that if an aligned index j has a sub-index string, and fjx+1 is a major copy,
then ∆j will be incremented by 1 with probability 2/3, and stay unchanged with probability
1/3. Since fjx, fjy are ma jor copies, we know that this case happens with probability 1/4.
The symmetric case where fjx+1 is a minor copy and fjy+1 is a major copy also happens with
probability 1/4.
Combine the two possible cases for aligned index j (with or without sub-index string), and
assuming the next aligned ma jor index is j0 . If fjx and fjy are minor copies, or are the same,
we have ∆j 0 — ∆j = 0 by simple case analysis. Otherwise, we have:
w.p. 1/6
w.p. 2/3 ,	(8)
w.p. 1/6
This is because for the two aligned major copies at index j, with probability 1/2, the next
index j + 1 is still aligned, and as a result ∆j does not change. With probability 1/2, there
exists a sub-index string for index j , but conditioned on this, the probability that ∆j does
not change is 1/3. So the total probability for being unchanged is 1/2+1/6 = 2/3. The other
two cases are symmetric, so each has probability (1 — 2/3)/2 = 1/6. This defines a random
walk process for ∆j , and the remaining analysis is similar to the analysis in Chakraborty
et al. (2016).
∆j0 — ∆j =	10,,
13
Under review as a conference paper at ICLR 2020
Assume that dE(x,y) = k. Let X = x(0), x(1), ∙∙∙ , x(k) = y be a series of strings such that
dE(x(l-1), x(l)) = 1. Let il be the first index on which x(l-1) and x(l) differ. For simplicity
We assume that iι < •… < ik (and We will later show this is without loss of generality).
Let dj be the difference between deleted and inserted bits between x and y before the index
j . Therefore, d0 = 0, and
=
dj
dj dj dj
-	1,	idx(x,j) ∈{iι,…，ik}
-	1 - 1, idx(x, j) = il, idx(x, j - 1) 6= il, x(l) is obtained by deletion
-	1 + 1, idx(x, j) = il, idx(x, j - 1) 6= il, x(l) is obtained by insertion
-	1	otherwise
(9)
dj denotes the required length of “shift” between fx and fy . In other words, for j such
that idx(x, j) ∈ (il, il+1), and idx(x, j) - idx(y, j) = ∆j = dj, we know fjx = fjy. Moreover,
for any j0 ≥ j such that idx(x, j0) ∈ [idx(x, j), il+1), we know that fjx0 = fjy0 holds as well,
because the same hash function will be applied to both strings after idx(x, j).
By modeling ∆j and dj as a chasing game between a cat and a dog with a kennel, the
following lemma was proved in Chakraborty et al. (2016).
Lemma 3 (Lemma 4.4 in Chakraborty et al. (2016)). If dE(x, y) = k, then
l
Pr(|{j : fjx 6= fjy, j is an aligned major index}| ≤ l) ≥ X q(t, k)
t=0
Where q(t, l) denotes the probability that the random walk process defined by (8) starting at
the origin, reaches the point l at time t for the first time.
As discussed in Chakraborty et al. (2016), the assumption that iι, •…,ik are strictly mono-
tone is without loss of generality, because it suffices to modify the updating rule of dj in (9)
such that dj and dj-1 can be differed by more than 1 unit. Once dj is properly defined,
Lemma 3 still applies.
The sum Plt=0 q(t, k) in Lemma 3 is easy to bound:
Lemma 4. For any k,l ∈ N, with probability more than 1 — e-玛,it holds that
X q(t,k) ≥ 1 - 12k
t=0	l
In particular, P7=706k q(t,k) ≥ 2.
Proof. Below is a well known fact about random walks that can be found e.g. (Theorem
2.17 in Levin and Peres (2017)). For any k, l ∈ N, it holds that:
12k
Er(t,k) ≥ 1 一方
t=0	l
where r (t, k) denotes the probability a simple random walk starting at the origin reaches
point k for the first time in t steps. Let m be the number of non-lazy steps during a t-step
random walk defined in (8). By applying Chernoff Bounds, for any 0 < δ < 1 we have:
1	δ2
P(m > (1 — δ)-t) > 1 — e-方t
Therefore, with probability more than 1 —e-%t, we are performing more than (1 —δ) 3t-steps
simple random walk. Setting δ = 2 gives us the result in Lemma 4.	□
Now we are ready to prove our Theorem 1.
14
Under review as a conference paper at ICLR 2020
Proof of Theorem 1. (1) is trivially true, because we can simulate Algorithm 2 using the
random string r to decide for each index of fx whether it is a major copy or not.
To see (2) is true, we need to do case analysis. We scan fx and fy from left to right, and
skip all indices that are the same. For the first different index j , there are three cases:
1.	j	is	aligned	major,	and	j +	1	is	aligned minor.
2.	j	is	aligned	major,	and	j +	1	is	also aligned major.
3.	j	is	aligned	major,	and	j +	1	is	not aligned.
For the first two cases, it suffices to replace the bit at idx(x, j) of x, and we can keep scanning
starting from the next aligned major. For the third case, we can do induction on the length
t of the sub-index string for j , to show that it suffices to make at most s edits if there are s
different bits from j to j + t. Without loss of generality, we assume that the index j + 1 of
fx is a major copy while j + 1 of fy is a minor copy.
•	If t = 1, which means j + 2 is aligned major. In this case, if bits at index j + 1 of
fx and fy are matched, it suffices to edit the bit at index j . Otherwise, it suffices
to delete idx(x, j) of x, and replace the bit at idx(x, j + 1) of x to match y.
•	If t = 2, which means j + 3 is aligned major, and the index j of fy is a major copy.
In this case, we check whether the bits at j + 2 for fx , fy are the same.
-If yes, it means, idx(y,j + 2) of y and idx(x,j + 2) of X are the same. In this
case, it suffices to replace idx(x, j) of x to match y.
- If no, it suffices to replace both idx(x, j), idx(x, j + 1) of x to match y.
• If t > 2, we also check whether the bits at j + t for fx , fy are the same.
-	If yes, it means, idx(y, j + t) of y and idx(x, j + t) of x are the same. Then we
may reduce the analysis to the t - 2 case. By induction, we know that from j
to j + t - 2, we can make the proper edit to match x and y.
-	If no, it means idx(y, j + t) of y and idx(x, j + t) of x are different. We make
one edit on idx(x, j +t) to match idx(y, j +t) of y, and then reduce the analysis
to the t - 2 case.
Notice that in the above two sub-cases, we did not analyze the case at index j +t- 1.
This is because this index is not aligned, and the minor copy is included in the
analysis of index j to j + t - 2, while the ma jor copy is included in the analysis of
index j + t.
The above analysis shows that it suffices to make at most d(fx , fy ) edits to match x and
y until one of fx or fy starts to pad zeros. Since there are at most d(fx, fy) deletions or
insertions on x, and originally x and y have the same length, we know that there are at most
d(fx, fy) padded zeros. Combine the two cases together, we have dE(fx, fy)/2 ≤ d(fx, fy).
(3) Apply Lemma 3, we know that with probability (1 - e ^2l4) ∙ (1 一 12k ), there are
at most l aligned major indices that satisfies fjx 6= fjy . Notice that, if for any indices, if
fjx = fjy, it does not affect d(fx, fy). Hence below we consider other cases when fjx 6= fjy:
•	If j is an aligned minor, we know that j 一 1 is aligned ma jor such that fjx 6= fjy , and
also j + 1 is an aligned major index. Thus the number of such indices is at most l.
•	If j is not aligned, we know that j belongs to a sub-index string for an aligned major
index j0 < j. Notice that fjx 6= fjy, otherwise j0 won’t have sub-index string. We
bound the number of such indices below.
15
Under review as a conference paper at ICLR 2020
To recap, we need to bound the expected sum of lengths of sub-index strings that start from
an aligned major index j such that fjx 6= fjy . Since the number of such strings is l, and the
expected length of each string is at most 2, it follows that the expected sum of lengths is at
most 2l . We can thus use Markov inequality to show that this quantity is at most 20l with
probability at least 9/10.
We note that in Lemma 3, we did not consider the comparison between fx and fy when one
of them has padded zeros. However, it is easy to see that in this case every edit operation
can only incur one unit of cost in d(fx, fy), which is not the worst case.
Therefore, with probability at least (1 - e-2l4) ∙ (1 - 12k^J~6) - 0.1, the hamming distance
is at most 20l. Let l = Ck02, We know the probability is at least
(1 - e-2l4) ∙ (1 - 12k,6) - 0.1
ck2
=(1 - e-480) ∙ (1 -
一 )-0.1
c
≥(1 - e-480) ∙ (1 -
一 )-0.1
√c
The last inequality holds because k ≥ 1, otherwise the hamming distance is 0. Therefore
it suffices pick c0 = 50000, and the expression above is larger than 0.5. Notice that we did
not carefully tune the constants here.
(a) Neural	(b) CGK
Figure 8: t-SNE visualization of neural embedding and CGK embedding. The purple point
with orange arrow represents the query string, and the red points represent top-10 edit
distance nearest neighbors of the query strings, and the blue points represent the others.
A.3 Synthetic Data Set
Although Phase 1+2 has good performance for most data sets, we can easily construct
synthetic data sets for which it will fail. For example, given a string x, applying 1-bit shift
to the right gives rise to a large hamming distance but the edit distance is only two. If this
1-bit shift coexists with a 3-bit substitution (so that the hamming distance and edit distance
are both equal to three), it would be quite “hard” for Phase 1+2 to find nearest neighbors
under edit distance. However, adding Phase 3 easily solves the problem, and gives improved
accuracy on the challenging hard cases.
16
Under review as a conference paper at ICLR 2020
Table 6: Top-1 Recall for the Synthetic Dataset. Candidate number is one.
Model	Top-1 Recall(%)
Phase 1+2+3	64.3
Phase 1+2	40.9
CGK	60.1
CGK’	58.0
Identity	40.6	
We have generated a synthetic data set to illustrate the algorithm effectiveness. First we
have chosen some prefix strings from the Gen50ks dataset as the query set. Then, for each
query, we generate its small shifts (one or two) and small substitution strings (three or four),
which are used as the base set. For each query we want to find the closest edit distance
string in the base set. The performance of all studied algorithms on this data set are shown
in Table 6. It can be seen that adding Phase 3 improves the performance of our method.
A.4 Visualization
We visualize the embedding results for both CGK and our model using t-SNE (van der
Maaten and Hinton, 2008). For CGK embedding, the points distribute over the 2D space
randomly no matter what hyper-parameters are used. For neural embedding, on the other
hand, points are well clustered. Figure 8 shows 2D t-SNE projections with perplexity 8 and
learning rate 10.
17