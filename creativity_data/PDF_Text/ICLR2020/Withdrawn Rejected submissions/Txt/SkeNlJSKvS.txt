Under review as a conference paper at ICLR 2020
Shallow VAEs with RealNVP Prior Can Per-
form as Well as Deep Hierarchical VAEs
Anonymous authors
Paper under double-blind review
Ab stract
Using powerful posterior distributions is a popular technique in variational infer-
ence. However, recent works showed that the aggregated posterior may fail to
match unit Gaussian prior, even with expressive posteriors, thus learning the prior
becomes an alternative way to improve the variational lower-bound. We show that
using learned RealNVP prior and just one latent variable in VAE, we can achieve
test NLL comparable to very deep state-of-the-art hierarchical VAE, outperform-
ing many previous works with complex hierarchical VAE architectures. We hy-
pothesize that, when coupled with Gaussian posteriors, the learned prior can en-
courage appropriate posterior overlapping, which is likely to improve reconstruc-
tion loss and lower-bound, supported by our experimental results. We demonstrate
that, with learned RealNVP prior, β-VAE can have better rate-distortion curve
than using fixed Gaussian prior.
1	Introduction
Variational auto-encoder (VAE) (Kingma & Welling, 2014; Rezende et al., 2014) is a powerful deep
generative model. The use of amortized variational inference makes VAE scalable to deep neural
networks and large amount of data. Variational inference demands the intractable true posterior to
be approximated by a tractable distribution. The original VAE used factorized Gaussian for both
the prior and the variational posterior (Kingma & Welling, 2014; Rezende et al., 2014). Since then,
lots of more expressive variational posteriors have been proposed (Tran et al., 2016; Rezende &
Mohamed, 2015; Salimans et al., 2015; Nalisnick et al., 2016; Kingma et al., 2016; Mescheder
et al., 2017; van den Berg et al., 2018). However, recent work suggested that even with powerful
posteriors, VAE may still fail to match aggregated posterior to unit Gaussian prior (Rosca et al.,
2018), indicating there is still a gap between the approximated and the true posterior.
To improve the variational lower-bound, one alternative way to using powerful posterior distribu-
tions is to learn the prior, an idea initially suggested by Hoffman & Johnson (2016). Later on,
Huang et al. (2017) applied RealNVP (Dinh et al., 2017) to learn the prior. Tomczak & Welling
(2018) proved the optimal prior is the aggregated posterior, which they approximate by assembling
a mixture of the posteriors with a set of learned pseudo-inputs. Bauer & Mnih (2019) constructed a
rich prior by multiplying a simple prior with a learned acceptance function. Takahashi et al. (2019)
introduced the kernel density trick to estimate the KL divergence in ELBO, for their implicit prior.
Despite the achievements of these previous works on posteriors and priors, the state-of-the-art VAE
models with continuous latent variables all rely on deep hierarchical latent variables1, although some
of them might have used complicated posteriors/priors as components in their architectures. Most
latent variables in such deep hierarchical VAEs have no clear semantic meanings, just a technique
for reaching good lower-bounds. This is in sharp contrast with GANs (Goodfellow et al., 2014;
Arjovsky et al., 2017) and flow-based models (Ho et al., 2018), where most of them can reach
state-of-the-art results with only one latent variable, with clear semantic meanings. We thus raise
and answer a question: with the help of learned priors, can shallow VAEs achieve performance
comparable or better than deep hierarchical VAEs? This question is important because a shallow
1The term “hierarchical latent variables” refers to multiple layers of latent variables, formulated as p(x) =
Ep(z1) Ep(z2 |z1) . . . Ep(zK|zK-1,...,z1) [p(x|z1, . . . , zK)]; while “one latent variable” refers to just one z in
standard VAEs. Both zk and z are multi-dimensional tensors. Also, “deep” refers to many hierarchical latent
variables, while “shallow” refers to few latent variables.
1
Under review as a conference paper at ICLR 2020
VAE would be much more promising to scale to more complicated datasets than deep hierarchical
VAEs. To answer this question, we conduct comprehensive experiments on several datasets with
learned RealNVP priors and just one latent variable, which even shows advantage over some deep
hierarchical VAEs with powerful posteriors. We also propose a hypothesis on why learning the prior
can lead to such great improvement in model performance, supported by our experimental results.
In summary, our contributions are:
•	We conduct comprehensive experiments on four binarized datasets with four different net-
work architectures. Our results show that VAE with RealNVP prior consistently outper-
forms standard VAE and RealNVP posterior.
•	We are the first to show that using learned RealNVP prior with just one latent variable
in VAE, it is possible to achieve test negative log-likelihoods (NLLs) comparable to very
deep state-of-the-art hierarchical VAE on these four datasets, outperforming many previous
works using complex hierarchical VAE equipped with rich priors/posteriors.
•	We hypothesize that, when coupled with Gaussian posteriors, the learned prior can encour-
age appropriate posterior overlapping, which is likely to improve reconstruction loss and
lower-bound, supported by our experimental results.
•	We demonstrate that, with learned RealNVP prior, β-VAE can have better rate-distortion
curve (Alemi et al., 2018) than using fixed Gaussian prior.
2	Preliminaries
2.1	Variational auto-encoder
Variational auto-encoder (VAE) (Kingma & Welling, 2014; Rezende et al., 2014) is a deep proba-
bilistic model. It uses a latent variable Z with prior pλ(z), and a conditional distribution pθ(x∣z), to
model the observed variable x. pθ (x) is defined as JZ pθ (x|z) pλ (Z) dz, wherepθ (x|z) is derived by
a neural network with parameter θ. log pθ (x) is bounded below by evidence lower-bound (ELBO):
logPθ(x) ≥ L(x; λ,θ,φ) = Eqφ(z∣χ) [logPθ(x∣z) + logpλ(z) - logqφ(z∣x)]	(1)
=Eqφ(z∣χ) [logPθ(x|z)] - DκL(qφ(z∣x)kpλ(z))	(2)
where qΦ(z|x) is the variational posterior to approximate pθ (z|x), derived by a neural network with
parameter φ. In Eq. (2), the first term is the reconstruction loss of x, and the second term is the
Kullback Leibler (KL) divergence between qφ(z∣x) and pλ(z). Optimizing qφ(z∣x) and pθ(x|z)
w.r.t. empirical distribution p?(x) can be achieved by maximizing the expected ELBO w.r.t. p?(x):
L(λ,θ,φ) = Ep*(χ)[L(x; λ,θ,φ)] = Ep*(x)Eqφ(z∣χ) [logpθ(x∣z)+logpλ(z) - logqφ(z∣x)] (3)
A hyper-parameter β can be added to the training objective, in order to control the trade-off between
reconstruction loss and KL divergence, known as β-VAE (Higgins et al., 2017; Alemi et al., 2018):
Le(λ,θ,φ) = Ep*(χ) Eqφ(z∣x) [logPθ(x|z) + β (logPλ(z) - log qφ(z∣x))]	(4)
The aggregated posterior qφ(z) = JX qφ(z∣x) p?(x) dx, should be equal to pλ(z), if VAE is per-
fectly trained (Hoffman & Johnson, 2016). However, even with powerful posteriors, the aggregated
posterior may still not match a unit Gaussian prior (Rosca et al., 2018). To investigate this problem,
Hoffman & Johnson (2016) suggested to decompose Eq. (3) as:
L(λ, θ, φ) = Ep*(χ) Eqφ(z∣x) [logPθ(x|z)] - DκL(qφ(z)kpλ(z)) -Iφ[Z; X]	(5)
×-----------{Z-----------} X--------V--------} X---{----}
①	②	③
where Iφ[Z; X] = RR qφ(z, x) log q；Zz,x)χ)dzdx is the mutual information. Sincepλ(z) is only
in ①2 , ELBO can be further enlarged if pλ(z) is trained to match qφ(z).
2.2	REALNVP
RealNVP (Dinh et al., 2017) is a deep probabilistic model. We denote its observed variable by z
and latent variable by w, with marginal distribution pλ(z) and priorpξ(w). Unlike VAE, RealNVP
2
Under review as a conference paper at ICLR 2020
relates z and w by an invertible mapping w = fλ(z) instead ofa conditional distribution, as follows:
pλ (z) = pξ (w)
det
(fz))
z = fλ-1 (w)
(6)
where det (∂fλ(z)∕∂z) is the Jacobian determinant of fx. In RealNVP, f is composed of K
invertible mappings, where fλ(z) = (fκ ◦… ◦ fι)(z), and each fk is invertible. fk must be
carefully designed to ensure that the determinant can be computed efficiently. The original paper of
RealNVP introduced the affine coupling layer as fk. Kingma & Dhariwal (2018) further introduced
actnorm and invertible 1x1 convolution. Details can be found in their papers.
3	Learning the prior with RealNVP
It is straightforward to obtain a rich prior pλ(z) from a simple (i.e., with constant parameters) one
with RealNVP. Denote the simple prior as pξ(w), while the RealNVP mapping as w = fλ(z). We
then obtain Eq. (6) as our prior pλ (z). Substitute Eq. (6) into (3), we get to the training objective:
L(λ,θ,φ)= Ep*(χ) Eqφ(z∣x) log Pθ (x∣z)+log Pξ (fλ(z)) + log det Cfdzz) - log 9φ(z∣x)
(7)
We mainly use joint training (Tomczak & Welling, 2018; Bauer & Mnih, 2019) (where pλ(z) is
jointly trained along with qφ(z∣x) andpθ(x|z) by directly maximizing Eq. (7)), but We also consider
other two strategies, see Appendices B.3 and B.9 for discussions.
In our initial experiments, training VAEs with deep RealNVP priors is unstable. The HVAEs (see
Appendix B.13) with RealNVP priors are even more unstable, where the training procedure can
barely finish. However, we tackle this problem by the following two techniques: (1) using gradient
clip, as the original RealNVP (Dinh et al., 2017) does; and (2) clipping the std of Gaussian posterior
qφ(z|x) of VAE (or qφ(z2∣x) and qφ(zι∣x) of HVAE) by a minimum value of e-11 ≈ 1.67 X 10-5.
4	Appropriate posterior overlapping with learned prior
As we shall see in Section 5.4, RealNVP prior can result in a test negative log-likelihood (NLL)
substantially better than RealNVP posterior, with the same flow depth. Also, using both RealNVP
posterior and prior shows no significant advantage over using RealNVP prior only, although the
total flow depth of the former variant is twice as large as the latter one. To investigate why VAE with
RealNVP prior can lead to substantially better model performance than the other two variants, we
take the particular architecture of our models into consideration (i.e., Bernoulli pθ(x|z), Gaussian
qφ(z|x), and unit Gaussian or flow prior pʌ(z)), and start with the following proposition:
Proposition 1. Given a finite number of discrete training data, i.e., p*(x) = N PlN=I δ(x - x(i)),
if pθ(x|z) = Bernoulli(μθ(z)), where the Bernoulli mean μj(z) is produced by the decoder and
0 < μk (z) < 1 for each ofits k-th dimensional output, then the optimal decoder μj (z) is:
μθ(z) = X Wi(Z) x(i), where Wi(Z) = Pq^：(：屋.))and X Wi(Z) = 1
(8)
Proof. See Appendix A.
□
Proposition 1 suggests that if qφ(z∣x) for different X overlap, then even at the center of qφ(z∣x(i)) of
one training point x(i), the optimal decoder will be an average of both x(i) and other training points
x(j), weighted by Wi(z) and Wj(z). Rezende & Viola (2018) have shown that weighted average like
this is likely to cause poor reconstruction loss (1 in Eq. (5)) and “blurry reconstruction”. Besides
poor reconstructions, when x is high-dimensional, the reconstruction loss would typically dominate
ELBO, suggesting that good reconstruction loss is necessary for obtaining good ELBO.
To enlarge 1 , it is a crucial goal to appropriately reduce the overlapping of qφ (z|x) for different x.
For Gaussian posterior, this seems to involve shrinking the standard deviations (std) of most qφ(z∣x),
3
Under review as a conference paper at ICLR 2020
since when the mean of every qφ(z∣x) is determined, smaller Stds for the majority of qd (z|x) would
effectively result in less overlapping.
When the prior pλ(z) is fixed (such as unit Gaussian), however, smaller Stds for qφ(z|x) are likely
to induce “holes” on the aggregated posterior, causing qφ(z) to be more dissimilar with the prior,
enlarging ② in Eq. (5). Also, less overlapping among the Gaussian posteriors qφ(z∣x) would cor-
respond to higher mutual information (3 in Eq. (5)) (Hoffman & Johnson, 2016; Mathieu et al.,
2019). As a result, with a fixed prior, there should exist a trade-off between 1 and 2 + 3 . In fact,
such a trade-off has already been discussed in the context ofβ-VAE (Alemi et al., 2018).
It should be a desirable property that the overlapping among qφ(z∣x) would be large for similar x,
and conversely, small for dissimilar x. We shall use the term “appropriate overlapping” to denote
such property. We believe that the trade-off between the reconstruction loss (1 ) and the mutual
information (3 ) should be one important ingredient for reaching the “appropriate overlapping”, as
implied by their respective meanings. However, the effect of 2 is not so clear. We suspect that, in
many cases, this term may exhibit over-regularization, resulting in too large overlapping, and further
unsatisfactory reconstruction loss. In this paper, we choose to use learned RealNVP prior to reduce
the effect from 2 , such that the trade-off would occur mainly between 1 and 3 . We hope this will
bring us closer to the “appropriate overlapping”, than using the fixed unit Gaussian prior.
In conclusion, when pθ(x|z) is Bernoulli, We hypothesize that learning the prior can encourage
“appropriate overlapping” among Gaussian posteriors, which is vital for good reconstructions and
NLLs. Rezende & Viola (2018) has proved when pθ(x∣z) = N(μe(z), σ2I) with fixed constant σ,
the optimal decoder is also μ°(Z) = Pi Wi(Z) x(i), thus our analysis also holds in such situation.
5	Experiments
5.1	Setup
Datasets We use four datasets in our experiments: statically and dynamically binarized
MNIST (Larochelle & Murray, 2011; Salakhutdinov & Murray, 2008) (denoted as StaticMNIST
and MNIST in our paper, respectively), FashionMNIST (Xiao et al., 2017) and Omniglot (Lake
et al., 2015). Details of these datasets can be found in Appendix B.1.
Models We perform systematically controlled experiments, using the following VAE variants:
(1) DenseVAE, with dense layers; (2) ConvVAE, with convolutional layers; (3) ResnetVAE, with
ResNet layers (Zagoruyko & Komodakis, 2016); and (4) PixelVAE (Gulrajani et al., 2017), with
several PixelCNN layers on top of the ResnetVAE decoder. For RealNVP priors and posteriors, we
use K blocks of invertible mappings (K is called flow depth hereafter), while each block contains
an invertible dense, a dense coupling layer, and an actnorm (Dinh et al., 2017; Kingma & Dhariwal,
2018). The dimensionality ofZ are 40 for StaticMNIST and MNIST, while 64 for FashionMNIST
and Omniglot. More details are in Appendix B.2. We also conduct experiments with ResnetHVAE
and PixelHVAE (Tomczak & Welling, 2018), see Appendix B.13.
Training and evaluation Unless specified, all experiments are repeated for 3 times to report met-
ric means. We use Adam (Kingma & Ba, 2015) and adopt warm up (KL annealing) (Bowman et al.,
2016) to train all models. We perform early-stopping using negative log-likelihood (NLL) on vali-
dation set, to prevent over-fitting on StaticMNIST and on all datasets with PixelVAE. We use 1,000
samples to estimate NLL and other metrics on test set. See Appendix B.3 for more details.
5.2	Quantitative results of VAE with RealNVP prior and one latent variable
compared to deep hierarchical VAEs and other previous works
In Tables 1 and 2, we compare ResnetVAE and PixelVAE with RealNVP prior to other approaches
on StaticMNIST and MNIST. See Tables B.10 and B.11 for results on Omniglot and FashionMNIST,
which have a similar trend. All models except ours and that of Huang et al. (2017) used at least 2
latent variables. For comparison between our model and Huang et al. (2017), see Appendix B.7.
Our ResnetVAE with RealNVP prior, K = 50 is second only to BIVA among all models without
PixelCNN decoder, and ranks the first among all models with PixelCNN decoder. On MNIST, the
NLL of our model is very close to BIVA, while the latter used 6 latent variables and very compli-
4
Under review as a conference paper at ICLR 2020
Table 1:	Test NLL on StaticMNIST. “t” indi-
cates a hierarchical model with 2 latent variables,
while “*” indicates at least 3 latent variables.
Model	NLL
Models without PixelCNN decoder
ConvHVAE + Lars priort (Bauer &	81.70
Mnih, 2019)
ConvHVAE + VampPriort (Tomczak	81.09
& Welling, 2018)
ResConv + RealNVP prior (Huang 81.44
et al., 2017)
VAE + IAF* (Kingma et al., 2016)	79.88
BrVA* (Maal0e et al., 2019)	78.59
ConvVAE + RNVP p(z), K = 50	80.09
ResnetVAE + RNVP p(z), K = 50	79.84
Models with PixelCNN decoder
VLAE* (Chen et al., 2017)	79.03
PixelHVAE + VampPriort (Tomczak 79.78
& Welling, 2018)
PixelVAE + RNVP p(z), K = 50	79.01
Table 2:	Test NLL on MNIST. “t” and “*” has
the same meaning as Table 1.
Model	NLL
Models without PixelCNN decoder
ConvHVAE + Lars priort (Bauer &	80.30
Mnih, 2019)
ConvHVAE + VampPriort (Tomczak 79.75
& Welling, 2018)
VAE + IAF* (Kingma et al., 2016)	79.10
BrVA* (Maal0e et al., 2019)	78.41
ConvVAE + RNVP p(z), K = 50	78.61
ResnetVAE + RNVP p(z), K = 50	78.49
Models with PixelCNN decoder
VLAE* (Chen et al., 2017)	78.53
PixelVAEt (Gulrajani et al., 2017)	79.02
PixelHVAE + VampPriort (Tomczak 78.45
& Welling, 2018)
PixelVAE + RNVP p(z), K = 50	78.12
3 3 02 qq
3 3。2 9 q
3 16 7 7C2
夕 I Xw /C ʧ Z
NG 5 7√G
JZ “ J 07
3I q 二夕 U>
5 ( □lλ 6
,Q OGg/
6 3 9 7 3 P
乙¥。0/。
7 4 S 2 g⅛
standard	RealNVP pλ (z)
Figure 2: Interpolations of z from ResnetVAE,
between the centers of qφ (z|x) of two training
points, and heatmaps of log pλ (z). The left- and
right-most columns are the training points.
standard RealNVP Pλ (Z)
Figure 1: Sample means from pλ (z) of Resnet-
VAE with: (left) Gaussian prior; (right) Real-
NVP prior. The last column of each 6x6 grid
shows the training set images, most similar to the
second-to-last column in pixel-wise L2 distance.
cated architecture. Meanwhile, our ConvVAE with RealNVP prior, K = 50 has lower test NLL
than ConvHVAE with Lars prior and VampPrior. Since ConvVAE is undoubtedly a simpler ar-
chitecture than ConvHVAE (which has 2 latent variables), it is likely that our improvement comes
from the RealNVP prior rather than the different architecture. We also conduct experiments on
Cifar10 (Krizhevsky, 2009). Although not as good as state-of-the-art deep VAEs, our ResnetVAE
using RealNVP prior with just one latent variable still shows great improvement over the Gaussian
prior, in both test bpd and sampling quality. See Appendix B.5 for more details.
Tables 1 and 2 show that using RealNVP prior with just one latent variable, it is possible to
achieve NLLs comparable to very deep state-of-the-art VAE (BIVA), ourperforming many pre-
vious works (including works on priors, and works of complicated hierarchical VAE equipped
with rich posteriors like VAE + IAF). This discovery shows that shallow VAEs with learned
prior and a small number of latent variables is a promising direction.
5.3	Qualitative results
Figure 1 samples images from ResnetVAE with/without RealNVP prior. Compared to standard
ResnetVAE, ResnetVAE with RealNVP prior produces fewer digits that are hard to interpret. The
5
Under review as a conference paper at ICLR 2020
Table 3: Average test NLL (lower is better) of different models, with Gaussian prior & Gaussian
posterior (“standard”), Gaussian prior & RealNVP posterior (“RNVP q(z|x)”), and RealNVP prior
& Gaussian posterior (“RNVP p(z)”). Flow depth K = 20.
Datasets	DenseVAE			ResnetVAE			PixelVAE		
	standard	RNVP q(z|x)	RNVP p(z)	standard	RNVP q(z|x)	RNVP p(z)	standard	RNVP q(z|x)	RNVP p(z)
StaticMNIST	88.84	86.07	84.87	82.95	80.97	79.99	79.47	79.09	78.92
MNIST	84.48	82.53	80.43	81.07	79.53	78.58	78.64	78.41	78.15
FashionMNIST	228.60	227.79	226.11	226.17	225.02	224.09	224.22	223.81	223.40
Omniglot	106.42	102.97	102.19	96.99	94.30	93.61	89.83	89.69	89.61
Table 4: Test NLL of ResnetVAE on MNIST, with
RealNVP posterior (“q(z|x)”), RealNVP prior
(“p(z)”), and RealNVP prior & posterior (“both”).
Flow depth K is 2K0 for the posterior or the prior
in “q(z|x)” and “p(z)”, while K0 for both the pos-
terior and the prior in “both”.
K0
ResnetVAE with		1	5	10	20
q(z|x), K	= 2K0	80.29	79.68	79.53	79.49
both, K	K0	79.85	79.01	78.71	78.56
p(z), K =	2K0	79.58	78.75	78.58	78.51
Table 5: Average number of active units
of ResnetVAE, with standard prior & poste-
rior (“standard”), RealNVP posterior (“RNVP
q(z|x)”), and RealNVP prior (“RNVP p(z)”).
ResnetVAE
Datasets	standard	RNVP q(z|x)	RNVP p(z)
StaticMNIST	30	40	40
MNIST	25.3	40	40
FashionMNIST	27	64	64
Omniglot	59.3	64	64
last column of each 6x6 grid shows the training set images, most similar to the second-to-last column
in pixel-wise L2 distance. There are differences between the last two columns, indicating our model
is not just memorizing the training data. More samples are in Appendix B.11.
5.4	Ablation study
RealNVP prior leads to substantially lower NLLs than standard VAE and RealNVP posterior
Table 3 shows the NLLs of DenseVAE, ResnetVAE and PixelVAE with flow depth K = 20. The
NLLs of ConvVAE (with similar trends as ResnetVAE) and the standard deviations of all models
are reported in Table B.12, while the NLLs of ResnetVAE with different K (range from 1 to 50) can
be found in Table B.15. We can see that RealNVP prior consistently outperforms standard VAE and
RealNVP posterior in test NLL, with as large improvement as about 2 nats (compared to standard
ResnetVAE) or 1 nat (compared to ResnetVAE with RealNVP posterior) on ResnetVAE, and even
larger improvement on DenseVAE. The improvement is not so significant on PixelVAE, likely due
to the less information encoded in the latent variable of PixelVAE (Gulrajani et al., 2017).
Using RealNVP prior only has better NLL than using both RealNVP prior and posterior, or
using RealNVP posterior only, under the same model complexity, as shown in Table 4.
Active units Table 5 counts the active units (Burda et al., 2016) of different ResnetVAEs, which
quantifies the number of latent dimensions used for encoding information from input data. We can
see that, both RealNVP prior and posterior can make all units of a ResnetVAE to be active (which is
in sharp contrast to standard VAE). However, RealNVP prior can in fact result in substantially better
reconstruction loss than RealNVP posterior (see Table B.18). This indicates that, the good regular-
ization effect, “a learned RealNVP prior can lead to more active units than a fixed prior” (Tomczak
& Welling, 2018; Bauer & Mnih, 2019), is not the main cause of the huge improvement in NLLs,
especially for the improvement of RealNVP prior over RealNVP posterior.
5.5	Reconstruction loss and posterior overlapping
Better reconstruction loss, but larger KL divergence In Table 6, ELBO and reconstruction
loss (“recons”, which is 1 in Eq. (5)) of ResnetVAE with RealNVP prior are substantially higher
6
Under review as a conference paper at ICLR 2020
Table 6: Average test ELBO (“elbo”), reconstruction loss (“recons”), Ep*(χ)DκL(qφ(z∣x)∣∣pλ(z))
(“kl”), and Ep*(χ)DκL(qφ(z∣x)∣∣pθ(z∣x)) (“kQx") of ResnetVAE with different priors.
standard	RealNVP p(z)
Datasets	elbo	recons	kl	klz|x	elbo	recons	kl	klz|x
StaticMNIST	-87.61	-60.09	27.52	4.67	-82.85	-54.32	28.54	2.87
MNIST	-84.62	-58.70	25.92	3.55	-80.34	-53.64	26.70	1.76
FashionMNIST	-228.91	-208.94	19.96	2.74	-225.97	-204.66	21.31	1.88
Omniglot	-104.87	-66.98	37.89	7.88	-99.60	-61.21	38.39	5.99
pairs of qφ(z∣x); and (right) normalized distances. See Appendix B.4 for formulation.
than standard ResnetVAE, just as the trend of test log-likelihood (LL) in Table 3. Metrics of other
models are in Tables B.16 to B.19. On the contrary, Ep?(x)DKL(qg(z|x)kp、(z)) (“kl”, which is
2 + 3 ) are larger. Since ELBO equals to 1 - (2 + 3 ), this suggests that in our experiments,
the improvement in ELBO (and also NLL) of ResnetVAE with RealNVP prior all comes from the
improved reconstruction loss. This serves as a foundation for the analysis in Section 4.
kl can happen to be smaller when recons gets larger (e.g., our DenseVAE in Table B.16, and VAE
with dense layers of (Bauer & Mnih, 2019, Table 2)). Nevertheless, all our experiments with Real-
NVP prior show improved reconstruction loss, which validates the analysis of Section 4.
Smaller standard deviation of Gaussian posterior with RealNVP prior In Fig. 3, we plot the
histograms of per-dimensional stds of qφ(z∣x), as well as the distances and normalized distances
(which is roughly distance/std) between each closest pair of qφ(z∣x) (see Appendix B.4 for for-
mulations). The stds of qφ(z∣x) with RealNVP prior are substantially smaller, and the normalized
distances are larger. Larger normalized distances indicate less density of qφ (z|x) to be overlapping.
This fact is a direct evidence of the analysis in Section 4.
The test NLL of the model and the learned Std of qφ (z | x) is insensitive to the minimum clipping
value under a certain threshold See Appendix B.6 for experiment results.
Appropriate overlapping among qφ(z∣x) with learned prior To demonstrate that the stds of
qφ(z|x) with RealNVP prior are reduced according to the dissimilarity between X rather than being
reduced equally (i.e., qφ(z|x) exhibits “appropriate overlapping”), we plot the interpolations of Z
between the centers of qφ(z∣x) of two training points, and log pλ(z) of these interpolations in Fig. 2,
We visualize pλ(z), because it is trained to match qφ(z), and can be computed much more reliable
than qφ(z); and because the density of qφ(z) between z corresponding to two x points can be an in-
dicator of how qΦ(z|x) overlap between them. The learned RealNVP pλ (Z) scores the interpolations
of z between the centers of qφ(z∣x) of two training points, giving low likelihoods to hard-to-interpret
interpolated samples between two dissimilar x (the first three rows), while giving high likelihoods
to good quality samples between two similar x (the last three rows). In contrast, the unit Gaussian
prior assigns high likelihoods to all interpolations, even to hard-to-interpret ones. This suggests that
the posterior overlapping is more appropriate with RealNVP prior than with unit Gaussian prior.
Learned prior influences the trade-off between reconstruction loss and KL divergence, as
suggested by Section 4. We plot the rate-distortion curve (RD curve) (Alemi et al., 2018) of β-
ResnetVAE trained with different β and prior flow depth K in Fig. 4. Rate is DκL(qφ(z∣x)kpθ (z)),
while distortion is negative reconstruction loss. Each connected curve with the same shape of points
in Fig. 4 correspond to the models with the same K, but different β. We can see that the curves of
K = 1 is closer to the boundary formed by the green line and the x & y axes than K = 0, while
K = 20 & 50 are even closer. According to Alemi et al. (2018), points on the RD curve being closer
7
Under review as a conference paper at ICLR 2020
Figure 4: Rate (DKL(qφ(z∣x)∣∣Pθ(Z)) and dis-
tortion (-reconstruction loss) of β-ResnetVAE
trained with different β and prior flow depth K .
Figure 5: Average normalized distance of β-
ResnetVAE trained with different β and prior
flow depth K .
to the boundary suggests that the corresponding models are closer to the theoretical optimal models
on a particular dataset, when traded between reconstruction loss and KL divergence. Given this, we
conclude that learned prior can lead to a “better” trade-off from the perspective of RD curve.
We also plotted the average normalized distance of β-ResnetVAE trained with different β and prior
flow depth K in Fig. 5. The average Std of qφ(z∣x) can be found at Fig. B.6. Learned prior can
encourage less posterior overlapping than unit Gaussian prior for various β, not only for β = 1.
6	Related work
Learned priors, as a natural choice for the conditional priors of intermediate variables, have long
been unintentionally used in hierarchical VAEs (Rezende et al., 2014; S0nderby et al., 2016; Kingma
et al., 2016; Maal0e et al., 2019). A few works were proposed to enrich the prior, e.g., Gaussian
mixture priors (Nalisnick et al., 2016; Dilokthanakul et al., 2016), Bayesian non-parametric pri-
ors (Nalisnick & Smyth, 2017; Goyal et al., 2017), and auto-regressive priors (Gulrajani et al., 2017;
Chen et al., 2017), without the awareness of its relationship with the aggregated posterior, until the
analysis made by Hoffman & Johnson (2016). Since then, attempts have been made in matching
the prior to aggregated posterior, by using RealNVP (Huang et al., 2017), variational mixture of
posteriors (Tomczak & Welling, 2018), learned accept/reject sampling (Bauer & Mnih, 2019), and
kernel density trick (Takahashi et al., 2019). However, none of these works recognized the improved
reconstruction loss induced by learned prior. Furthermore, they did not show that learned prior with
just one latent variable can achieve comparable results to those of many deep hierarchical VAEs.
The trade-off between reconstruction loss and KL divergence was discussed in the context of β-
VAE (Higgins et al., 2017; Alemi et al., 2018; Mathieu et al., 2019), however, they did not further
discuss the impact of a learned prior on this trade-off. Mathieu et al. (2019) also discussed the
posterior overlapping, but only within the β-VAE framework, thus was only able to control the
degree of overlapping globally, without considering the local dissimilarity between x.
7	Conclusion
In this paper, using learned RealNVP prior with just one latent variable in VAE, we managed to
achieve test NLLs comparable to very deep state-of-the-art hierarchical VAE, outperforming many
previous works of complex hierarchical VAEs equipped with rich priors/posteriors. We hypothesized
that, when coupled with Gaussian posteriors, the learned prior can encourage appropriate posterior
overlapping, which is likely to benefit the reconstruction loss and lower-bound, supported by our
experimental results. We showed that with learned RealNVP prior, β-VAE can have better rate-
distortion curve (Alemi et al., 2018) than with fixed Gaussian prior. We believe this paper is an
important step towards shallow VAEs with learned prior and a small number of latent variables,
which potentially can be more scalable to large datasets than those deep hierarchical VAEs.
8
Under review as a conference paper at ICLR 2020
References
Alexander Alemi, Ben Poole, Ian Fischer, Joshua Dillon, Rif A. Saurous, and Kevin Murphy. Fixing
a Broken ELBO. In International Conference on Machine Learning, pp.159-168, July 2018.
Martin Arjovsky, SoUmith Chintala, and Leon Bottou. Wasserstein gan. arXiv preprint
arXiv:1701.07875, 2017.
Matthias Bauer and Andriy Mnih. Resampled priors for variational autoencoders. In The 22nd
International Conference on Artificial Intelligence and Statistics, pp. 66-75, 2019.
Samuel Bowman, Luke Vilnis, Oriol Vinyals, Andrew M. Dai, Rafal Jozefowicz, and Samy Bengio.
Generating Sentences from a Continuous Space. In Proceedings of the Twentieth Conference on
Computational Natural Language Learning (CoNLL)., 2016.
Yuri Burda, Roger B. Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. In 4th
International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May
2-4, 2016, Conference Track Proceedings, 2016.
Xi Chen, Diederik P. Kingma, Tim Salimans, Yan Duan, Prafulla Dhariwal, John Schulman, Ilya
Sutskever, and Pieter Abbeel. Variational lossy autoencoder. In 5th International Conference
on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track
Proceedings, 2017.
Nat Dilokthanakul, Pedro AM Mediano, Marta Garnelo, Matthew CH Lee, Hugh Salimbeni, Kai
Arulkumaran, and Murray Shanahan. Deep unsupervised clustering with gaussian mixture varia-
tional autoencoders. arXiv preprint arXiv:1611.02648, 2016.
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real NVP. In 5th
International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26,
2017, Conference Track Proceedings, 2017.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Infor-
mation Processing Systems, pp. 2672-2680, 2014.
Prasoon Goyal, Zhiting Hu, Xiaodan Liang, Chenyu Wang, and Eric P. Xing. Nonparametric vari-
ational auto-encoders for hierarchical representation learning. In Proceedings of the IEEE Inter-
national Conference on Computer Vision, pp. 5094-5102, 2017.
Ishaan Gulrajani, Kundan Kumar, Faruk Ahmed, Adrien Ali Taiga, Francesco Visin, David Vazquez,
and Aaron C. Courville. Pixelvae: A latent variable model for natural images. In 5th Interna-
tional Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017,
Conference Track Proceedings, 2017.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. Beta-vae: Learning basic visual concepts with a
constrained variational framework. In International Conference on Learning Representations,
volume 3, 2017.
Jonathan Ho, Xi Chen, Aravind Srinivas, Yan Duan, and Pieter Abbeel. Flow++: Improving Flow-
Based Generative Models with Variational Dequantization and Architecture Design. September
2018.
Matthew D. Hoffman and Matthew J. Johnson. Elbo surgery: Yet another way to carve up the
variational evidence lower bound. In Workshop in Advances in Approximate Bayesian Inference,
NIPS, 2016.
Chin-Wei Huang, Ahmed Touati, Laurent Dinh, Michal Drozdzal, Mohammad Havaei, Laurent
Charlin, and Aaron Courville. Learnable Explicit Density for Continuous Latent Space and Vari-
ational Inference. arXiv:1710.02248 [cs, stat], October 2017.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In 3rd Inter-
national Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9,
2015, Conference Track Proceedings, 2015.
9
Under review as a conference paper at ICLR 2020
Diederik P Kingma and Max Welling. Auto-Encoding Variational Bayes. In Proceedings of the
International Conference on Learning Representations, 2014.
Diederik P. Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling.
Improved variational inference with inverse autoregressive flow. In Advances in Neural Informa-
tion Processing Systems, pp. 4743-4751, 2016.
Durk P Kingma and Prafulla Dhariwal. Glow: Generative Flow with Invertible 1x1 Convolutions.
In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.),
Advances in Neural Information Processing Systems 31, pp. 10215-10224. Curran Associates,
Inc., 2018.
Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.
B. M. Lake, R. Salakhutdinov, and J. B. Tenenbaum. Human-level concept learning through prob-
abilistic program induction. Science, 350(6266):1332-1338, December 2015. ISSN 0036-8075,
1095-9203. doi: 10.1126/science.aab3050.
Hugo Larochelle and Iain Murray. The neural autoregressive distribution estimator. In Proceedings
of the Fourteenth International Conference on Artificial Intelligence and Statistics, pp. 29-37,
2011.
Lars Maal0e, Marco Fraccaro, Valentin Lievin, and Ole Winther. BIVA: A Very Deep Hierarchy of
Latent Variables for Generative Modeling. arXiv:1902.02102 [cs, stat], February 2019.
Emile Mathieu, Tom Rainforth, N. Siddharth, and Yee Whye Teh. Disentangling disentanglement
in variational autoencoders. In Proceedings of the 36th International Conference on Machine
Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, pp. 4402-4412, 2019.
Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. Adversarial variational bayes: Unifying
variational autoencoders and generative adversarial networks. In Proceedings of the 34th Inter-
national Conference on Machine Learning-Volume 70, pp. 2391-2400. JMLR. org, 2017.
Eric Nalisnick, Lars Hertel, and Padhraic Smyth. Approximate inference for deep latent gaussian
mixtures. In NIPS Workshop on Bayesian Deep Learning, volume 2, 2016.
Eric T. Nalisnick and Padhraic Smyth. Stick-breaking variational autoencoders. In 5th Interna-
tional Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017,
Conference Track Proceedings, 2017.
Danilo Rezende and Shakir Mohamed. Variational Inference with Normalizing Flows. In Pro-
ceedings of the 32nd International Conference on Machine Learning (ICML-15), pp. 1530-1538,
2015.
Danilo Jimenez Rezende and Fabio Viola. Taming VAEs. arXiv:1810.00597 [cs, stat], October
2018.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic Backpropagation and
Approximate Inference in Deep Generative Models. In Proceedings of the 31st International
Conference on Machine Learning - Volume 32, ICML’14, pp. II-1278-II-1286, Beijing, China,
2014. JMLR.org.
Mihaela Rosca, Balaji Lakshminarayanan, and Shakir Mohamed. Distribution Matching in Varia-
tional Inference. arXiv preprint arXiv:1802.06847, 2018.
Ruslan Salakhutdinov and Iain Murray. On the quantitative analysis of deep belief networks. In
Proceedings of the 25th International Conference on Machine Learning, pp. 872-879. ACM,
2008.
Tim Salimans, Diederik Kingma, and Max Welling. Markov chain monte carlo and variational
inference: Bridging the gap. In Proceedings of the 32nd International Conference on Machine
Learning (ICML-15), pp. 1218-1226, 2015.
10
Under review as a conference paper at ICLR 2020
CasPer Kaae S0nderby, TaPani Raiko, Lars Maal0e, S0ren Kaae S0nderby, and Ole Winther. Ladder
variational autoencoders. In Advances in Neural Information Processing Systems, pp. 3738-3746,
2016.
Hiroshi Takahashi, Tomoharu Iwata, Yuki Yamanaka, Masanori Yamada, and Satoshi Yagi. Vari-
ational autoencoder with implicit optimal priors. In The Thirty-Third AAAI Conference on Ar-
tificial Intelligence, AAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019., pp.
5066-5073, 2019.
Jakub Tomczak and Max Welling. VAE with a VampPrior. In International Conference on Artificial
Intelligence and Statistics, pp. 1214-1223, 2018.
Dustin Tran, Rajesh Ranganath, and David M. Blei. Variational gaussian process. In 4th Interna-
tional Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4,
2016, Conference Track Proceedings, 2016.
Rianne van den Berg, Leonard Hasenclever, Jakub M. Tomczak, and Max Welling. Sylvester nor-
malizing flows for variational inference. In Proceedings of the Thirty-Fourth Conference on Un-
certainty in Artificial Intelligence, UAI 2018, Monterey, California, USA, August 6-10, 2018, pp.
393-402, 2018.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST: A Novel Image Dataset for Bench-
marking Machine Learning Algorithms. arXiv:1708.07747 [cs, stat], August 2017.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In Proceedings of the British
Machine Vision Conference 2016, BMVC 2016, York, UK, September 19-22, 2016, 2016.
11
Under review as a conference paper at ICLR 2020
A Proof for Proposition 1
We first re-state Proposition 1:
Proposition. Given a finite number of discrete training data, i.e., p? (x) = N PN=I δ(x 一 x(i)),
if pθ (x|z) = Bernoulli(μθ(Z)), where the Bernoulli mean μg (Z) is produced by the decoder and
0 < μk (Z) < 1 for each ofits k-th dimensional output, then the optimal decoder μg (Z) is:
μθ(Z) = Ewi(Z) χ(i),
i
where w<z) = Pjq：(；Xj)) and Xwi(Z) = 1
We shall need the Euler’s equation for the calculus of variations (see Gelfand & Silverman (2000,
page 15, 24, and 35)):
Theorem A.1. Let L[f] be a functional of the form:
L[f]=Zb
a
F (x, f, ∂xf) dx
defined on the set of functions f(x) which satisfies the boundary conditions y(a) = A, y(b) = B,
then a necessary condition for L[f] to have an extremum for a given function f (x) is that, f(x)
satisfy Euler’s equation (Gelfand & Silverman, 2000, page 15):
∂F d ∂F _
∂f dx ∂(∂χf)	0
(A.1)
where we use ∂xf to denote
df
dx'
For multi-variate function f(x1, x2) and the functional:
L[f] =	R F(x1,x2, f, ∂x1f, ∂x2f) dx1 dx2
where R is a closed area, the Euler’s equation becomes (Gelfand & Silverman, 2000, page 24):
∂F ∂ ∂F ∂ ∂F =o
∂f ∂xι ∂(∂χιf) ∂X2 ∂(∂χ2f)
(A.2)
For vector value function f(x) = [f 1 (x), . . . , fn (x)] and the functional:
L[f] = Z bF(x,f1,...,fn,∂xf1,...,∂xfn)dx
a
a necessary condition for f(x) to be an extremal ofL[f] is that, every fi (x) satisfies Euler’s equa-
tion (Gelfand & Silverman, 2000, page 35).
With some straightforward extensions, we can also get to the Euler’s equation for multi-variate
vector value functions.
The original Euler’s equation requires the integral to be definite, whereas a neural network is defined
on the whole Rn space. However, in practice, the input values (i.e., the training and test data) are
usually within limited range, and the norm of the neural network parameters are not too large. As a
result, there should exist a sufficiently large number R, such that all intermediate outputs are within
the closed area [-R, R]n. In this sense, the definite and indefinite integral would not have too much
difference. We shall always use the indefinite integral notations in the following proof.
Finally, we get to the proof of Proposition 1:
Proof. To apply calculus of variations, We need to substitute the parameterized, bounded μj (z) with
a non-parameterized, unbounded mapping. Since 0 < μk (z) < 1, and 从6 (z) is produced by neural
network, which ensures μj(z) is a continuous mapping, then ∀θ, there exists unbounded t(z), s.t.
μk (Z)
exp(tk (Z))
1 + exp(tk(z))
tk(z) = logμk(Z)-log。一 μk(Z))
12
Under review as a conference paper at ICLR 2020
and for all continuous mapping t(z), there also exists 从6(z), satisfying the above equations. In fact,
this substitution is also adopted in the actual implementation of our models.
The probability of pθ(x|z) is given by:
Pθ(XIZ) = BernOUlli3θ(Z)) = Y S(Z)Xx (1 - μk(Z))(I-Xk)
k
Then we have:
logpθ(x∣z) = X {xk log μk(z) + (1 - Xk) log (1 -
μk(Z)) }
X {xk tk(z) - Xk log [1 +exp(tk(z))] - (1 - Xk) log [1 + exp(tk(z))] }
k
X {xk tk(z) - log [1 +exp(tk(z))] O
k
The training objective L can be then formulated as a functional on t(z):
L[t] = Ep?(x) Eqφ (z|x)
logpθ (x|z) + log
Pθ(Z)
qφ(ZIX)
where F (z, t) is:
〃p?(χ) qφ(z|x) ∙
F(z, t) dz
logPθ(x|z) + log pθ(Z)、)dz dx
qφ(zIx)
F(z,t)
p? (x) qφ(zIx)
tk(z) - log [1 +exp(tk(z))] } + log
pθ(Z) ]dx
qφ(z∣x)J
According to Euler’s equation (Gelfand & Silverman, 2000, page 14 and 35), the necessary condition
for L[t] to have an extremum for a given t(z) is that, t(z) satisfies ∂F∕∂tk = 0, ∀k. Thus We have:
∂F
∂tk
p*(X) qφ(ZIX) Xk- 1+xρχp((z)Z))
dx = 0
Xqφ(ZIXCi)) [χki) - μk(z)]
i
0
Xqφ(zIx(i))X(ki)
i
(Xqφ(z|x(i))^ μk(z)
μk(z)
Piqφ(zIx(i))X(ki)
∑7∙qφ(zlXcj))
ThatiS to say, μθ(z) = PpφqZ(χ∣Xjx)(i) = Pi Wi(Z) x(i).
□
Rezende & Viola (2018) has proved that whenpθ(x∣z) = N(从6(z),σ2I), where σ is a global fixed
constant, the optimal decoder μj(z) = Pi Wi(Z) x(i), which is exactly the same as our conclu-
sion. Rosca et al. (2018) has proved that the gradient of BernOUlli(μj(z)) is the same as that of
N(μθ(z), σ2I) when σ = 1, but they did not calculate out the optimal decoder. We push forward
both these works.
13
Under review as a conference paper at ICLR 2020
B	Experimental details
B.1	Datasets
MNIST MNIST is a 28x28 grayscale image dataset of hand-written digits, with 60,000 data points
for training and 10,000 for testing. When validation is required for early-stopping, we randomly split
the training data into 50,000 for training and 10,000 for validation.
Since We use Bernoulli pθ(x|z) to model these images in VAE, We binarize these images by the
method in (Salakhutdinov & Murray, 2008): each pixel value is randomly set to 1 in proportion to
its pixel intensity. The training and validation images are re-binarized at each epoch. HoWever, the
test images are binarized beforehand for all experiments. We binarize each test image 10 times, and
use all these 10 binarized data points in evaluation. This method results in a 10 times larger test set,
but We believe this can help us to obtain a more objective evaluation result.
StaticMNIST StaticMNIST (Larochelle & Murray, 2011) is a pre-binarized MNIST image
dataset, With the original 60,000 training data already splitted into 50,000 for training and 10,000
for validation. We alWays use validation set for early-stopping on StaticMNIST. MeanWhile, since
StaticMNIST has already been binarized, the test set is used as-is Without 10x enlargement.
FashionMNIST FashionMNIST (Xiao et al., 2017) is a recently proposed image dataset of
grayscale fashion products, With the same specification as MNIST. We thus use the same training-
validation split and the same binarization method just as MNIST.
Omniglot Omniglot (Lake et al., 2015) is a 28x28 grayscale image dataset of hand-Written char-
acters. We use the preprocessed data from (Burda et al., 2016), With 24,345 data points for training
and 8,070 for testing. When validation is required, We randomly split the training data into 20,345
for training and 4,000 for validation. We use dynamic binarization on Omniglot just as MNIST.
B.2	Network architectures
Notations In order to describe the detailed architecture of our models, We Will introduce auxil-
iary functions to denote netWork components. A function hφ(x) should denote a sub-netWork in
qφ(z|x), with a subset of φ as its own learnable parameters. For example, if we write qφ(z∣x)=
qφ(z∣hφ(x)) = N(μφ(hφ(x)), σφ(hφ(x)) I), it means that the the posterior qφ(z∣x) is a Gaussian,
whose mean and standard deviation are derived by one shared sub-network hφ(x) and two separate
sub-networks μφ(∙) and σφ(∙), respectively.
The structure of a network is described by composition of elementary neural network layers.
Linear[k] indicates a linear dense layer with k outputs. Dense[k] indicates a non-linear dense layer.
a → b indicates a composition of a and b, e.g., Dense[m] → Dense[n] indicates two successive
dense layers, where the first layer has m outputs and the second layer has n outputs. These two
dense layers can also be abbreviated as Dense[m → n].
Conv[H × W × C] denotes a non-linear convolution layer, whose output shape is H × W × C, where
H is the height, W is the width and C is the channel size. As an abbreviation, Conv[H1 ×W1 ×C1 →
H X W2 X C2] denotes two successive non-linear convolution layers. ResnetH denotes non-linear
resnet layer(s) (Zagoruyko & Komodakis, 2016). All Conv and Resnet layers by default use 3x3
kernels, unless the kernel size is specified as subscript (e.g., Conv1×1 denotes a 1x1 convolution
layer). The strides of Conv and Resnet layers are automatically determined by the input and output
shapes, which is 2 in most cases. LinearConvH denotes linear convolution layer(s).
DeConvH and DeResnetH denote deconvolution and deconvolutional resnet layers, respectively.
PixelCNNH is a PixelCNN layer proposed by Salimans et al. (2017). It uses resnet layers, instead
of convolution layers. Details can be found in its original paper.
Flatten indicates to reshape the input 3-d tensor into a vector, while UnFlatten[H X W X C]
indicates to reshape the input vector into a 3-d tensor of shape H X W X C. Concat[a, b] indicates
to concat the output of a and b along the last axis.
14
Under review as a conference paper at ICLR 2020
CouplingLayer and ActNorm are components of RealNVP, proposed by Dinh et al. (2017);
Kingma & Dhariwal (2018). InvertibleDense is a component modified from invertible 1x1 con-
volution (Kingma & Dhariwal, 2018). We shall only introduce the details of CouplingLayer, since
it contains sub-networks, while the rest two are just simple components.
General configurations All non-linear layers use leaky relu (Maas et al., 2013) activation.
The observed variable x (i.e., the input image) is always a 3-d tensor, with shape H×W ×C, whether
or not the model is convolutional. The latent variable z is a vector, whose number of dimensions
is chosen to be 40 on MNIST and StaticMNIST, while 64 on FashionMNIST and Omniglot. This
is because we think the latter two datasets are conceptually more complicated than the former two,
thus requiring higher dimensional latent variables.
The Gaussian posterior qφ(z∣x) is derived as:
qφ(ZIX) = N (μφ(hΦ(χ)), σφ(hφ(χ)) D
μφ(hφ(x)) = hφ(x) → Linear[Dim(z)]
log σφ(hφ(X)) = hφ(X) → Linear[Dim(Z)]
Note we make the network to produce log σφ(hφ(x)) instead of directly producing σφ(hφ(x)).
hφ(x) is the hidden layers, varying among different models.
For binarized images, We use Bernoulli conditional distribution pθ (x∣z), derived as:
Pθ (x∣z) = BernouUi[μθ (hθ (z))]
For DenseVAE:
log —μθ('θ(Z))	= hθ(z) → Linear[784] → UnFlatten[28 X 28 X 1]
1	- μθ(hθ(Z))
And for others (i.e., ConvVAE, ResnetVAE and PixelVAE):
log -μθ(夕(Z))、、= hθ(z) → LinearConv1×1 [28 X 28 X 1]
1	— μθ (hθ(Z))
Note we make the network to produce log 1/*(Z)Z))), the logits of Bernoulli distribution, instead
of producing the Bernoulli mean μ°(hθ (z)) directly.
DenseVAE hφ(x) and hθ(z) of DenseVAE are composed of dense layers, formulated as:
hφ(x) = x → Flatten → Dense[500 → 500]
hθ (z) = z → Dense[500 → 500]
Conv/ResnetVAE hφ(x) and hθ(z) of ConvVAE are composed of (de)convolutional layers, while
those of ResnetVAE consist of (deconvolutional) resnet layers. We only describe the architecture
of ResnetVAE here. The structure of ConvVAE can be easily obtained by replacing all (deconvolu-
tional) resnet layers with (de)convolution layers:
hφ(x) = x → Resnet[28 X 28 X 32 → 28 X 28 X 32
→ 14X 14X 64→ 14X 14X64
→7X 7X64→7X 7X 16]
→ Flatten
hθ (z) = z → Dense[784] → UnFlatten[7 X 7 X 16]
→ DeResnet[7 X 7 X 64 → 14 X 14 X 64
→ 14 X 14 X 64→ 28 X 28 X 32
→ 28 X 28 X 32]
15
Under review as a conference paper at ICLR 2020
PixelVAE hφ (x) of PixelVAE is exactly the same as ResnetVAE, while hθ(z) is derived as:
∙-v
hθ (z) = Concat[x, hθ (z)]
→ PixelCNN[28 × 28 × 33 → 28 × 28 × 33
→ 28 × 28 × 33]
∙-v
hθ(Z) = Z → Dense[784] → UnFlatten[7 X 7 X 16]
→ DeResnet[7 × 7 × 64 → 14 × 14 × 64
→ 14 X 14 X 64→ 28 X 28 X 32
→ 28 X 28 X 32]
As Salimans et al. (2017), we use dropout in PixelCNN layers, with rate 0.5.
RealNVP The RealNVP consists of K blocks, while each block consists of an invertible dense, a
coupling layer, and an actnorm. The RealNVP mapping for prior, i.e. fλ(Z), can be formulated as:
fλ(z) = Z → fι(hι) →-------→ fκ(hκ)
fk (hk ) = hk → InvertibleDense → CouplingLayer → ActNorm
CouplingLayer(u) = Concat ul,ur	Sigmoid (sλ(hλ,k(ul))) + tλ(hλ,k(ul))
hλ,k(ul) = ul → Dense[256]
sλ(hλ,k(ul)) = hλ,k(ul) → Linear[Dim(ur)]
tλ(hλ,k(ul)) = hλ,k(ul) → Linear[Dim(ur)]
where ul = u0:bDim(u)/2c is the left half of u, and ur = ubDim(u)/2c:Dim(u) is the right half.
The RealNVP posterior, derived from the original Gaussian posterior qφ(w∣x), is denoted as
qφ,η (Z|x), and formulated as:
qφ,η(ZIX) = qφ(w|x) f (W)	(B.1)
∂w
Z = fη (w)
where the structure of the RealNVP mapping fη (w) for posterior is exactly the same as fλ (Z) for
prior. The ELBO for VAE with RealNVP posterior is then simply:
L(λ, θ, φ,η) = Ep?(x) Eqφ(w∣x) log Pθ (x∣fη (w))+lθg Pλ(fη (w))
一logqφ(w∣x) + log det (*^)	(B.2)
B.3 Additional details of training and evaluation
General methodology All the mathematical expressions of expectations w.r.t. some distributions
are computed by Monte Carlo integration. For example, Eqφ(z∣χ) [f (z, x)] is estimated by:
1L
Eqφ(z∣x) [f (z, x)] ≈ LE f(z⑺,X)
L i=1
where z(i) is one sample from qφ(zIx).
Training We use Adam (Kingma & Ba, 2015) to train our models for 2,400 epochs. The batch size
is 128 for DenseVAE, ConvVAE and ResnetVAE, and 64 for PixelVAE. On MNIST, FashionMNIST
and Omniglot, we set the learning rate to be 10-3 in the first 800 epochs, 10-4 in the next 800
epochs, and 10-5 in the last 800 epochs. On StaticMNIST, we set the learning rate to be 10-4 in the
first 1,600 epochs, and 10-5 in the last 800 epochs.
L2 regularization with factor 10-4 is applied on weights of all non-linear hidden layers, i.e., kernels
of non-linear dense layers and convolutional layers, in hφ(x), hθ(z), fλ(z) and fη(w).
16
Under review as a conference paper at ICLR 2020
The ELBO is estimated by 1 z sample for each x in training. We adopt warm-up (KL anneal-
ing) (Bowman et al., 2016). The ELBO using warm-up is formulated as:
L(X; λ,θ,φ) = Eqφ(z∣χ) [logpθ(XIZ) + β (logPλ(Z) - log qφ(ZIX)H
β is increased from 0.01 to 1 linearly in the first 100 epochs, and it remains 1 afterwards. The
warm-up ELBO for VAEs with RealNVP priors and posteriors can be obtained by replacing pλ(Z)
and qφ(ZIX) of the above equation by Eq. (6) and Eq. (B.1), respectively.
We adopt early-stopping using NLL on validation set, to prevent over-fitting on StaticMNIST and
PixelVAE. The validation NLL is estimated using 100 Z samples for each X, every 20 epochs.
Training strategies for pλ (Z) We consider three training strategies for optimizing Eq. (7):
•	Post-hoc training (Bauer & Mnih, 2019): qφ(ZIX) andpθ(XIZ) are firstly trained w.r.t. the
unit Gaussian prior, then qφ(ZIX) andpθ(XIZ) are fixed and pλ (Z) is in turn optimized. This
is the most intuitive training method according to Eq. (5), however, it does not work as well
as joint training in terms of test negative log-likelihood (NLL), which is observed both in
our experiments (Table B.6) and by Bauer & Mnih (2019).
•	Joint training (Tomczak & Welling, 2018; Bauer & Mnih, 2019): pλ (Z) is jointly trained
along with qφ(ZIX) andpθ(XIZ), by directly maximizing Eq. (7).
•	Iterative training: Proposed by us, we alternate between training pθ(XIZ) & qφ(ZIX) and
training pλ(Z), for multiple iterations. The first iteration to trainpθ(XIZ) & qφ(ZIX) should
use the unit Gaussian prior. Early-stopping should be performed during the whole process if
necessary. See Algorithm B.1 for detailed procedure of this strategy. We adopt this method
mainly for investigating why post-hoc training does not work as well as joint training.
To train pλ (Z) with post-hoc strategy, we start from a trained VAE, adding RealNVP prior onto it,
and optimizing the RealNVP fλ(Z) for 3,200 epochs, with learning rate set to 10-3 in the first 1,600
epochs, and 10-4 in the final 1,600 epochs.
Algorithm B.1 Pseudocode for iterative training.
Iteration 1a: Train qφ(ZIX) andpθ(XIZ), with pλ(Z) = N(0, I).
Iteration 1b: Train pλ(Z) for 2M epochs, with fixed qφ(ZIX) andpθ(XIZ).
for i = 2 . . . I do
Iteration ia: Train qφ(ZIX) andpθ(XIZ) for M epochs, with fixed pλ (Z).
Iteration ib: Train pλ(Z) for M epochs, with fixed qφ(ZIX) andpθ(XIZ).
end for
Algorithm B.1 is the pseudocode of iterative training strategy. For Iteration 1a, all hyper-parameters
are the same with training a standard VAE, where in particular, the number of training epochs is set
to 2,400. For Iteration 1b, the learning rate is 10-3 for the first M epochs, and is 10-4 for the next
M epochs. For all the next iterations, learning rate is always 10-4 . The number of iterations I is
chosen to be 16, and the number of epochs M is chosen to be 100, for MNIST, FashionMNIST
and Omniglot. For StaticMNIST, we find it overfits after only a few iterations, thus we choose I
to be 4, and M to be 400. With these hyper-parameters, qφ(ZIX), pθ(XIZ) and pλ(Z) are iteratively
trained for totally 3,200 epochs on all datasets (starting from Iteration 1b), after the pre-training step
(Iteration 1a).
Regularization term for qφ(Z) In order to compare some metrics (e.g., the active units) of VAE +
RealNVP prior with those of standard VAE, we introduce an additional regularization term for qφ(Z),
such that qφ(Z) of VAE using RealNVP prior would have roughly zero mean and unit variance,
just as a standard VAE with unit Gaussian prior. The regularization term for qφ(Z) (denoted as
Reg [qφ(Z)]) and the final training objective augmented with the regularization term (denoted as
L(X; λ, θ, φ)) are:
Dim(z)
Reg [qφ(z)] = Dim(Z) X h (Mean(Zk))2 + (Var(zk) - 1)2 ]
Le(λ, θ, φ) = Ep?(x) L(X; λ, θ, φ) + Reg [qφ(Z)]
17
Under review as a conference paper at ICLR 2020
where zk is the k-th dimension of z, Dim(z) is the number of dimensions, Mean(zk) =
Ep?(x)Eqφ(z∣x) [zk] and Var(Zk) = Ep*(x) Eqφ(z∣x) [(zk - Mean(Zk))2] are the mean and variance
of each dimension.
Table B.1: Avg. Mean(Zk) and Var(Zk) of regularized/un-regularized ResnetVAE with RealNVP
prior.
Datasets	regularized		un-regularized	
	Avg. Mean(Zk)	Avg. Var(Zk)	Avg. Mean(Zk)	Avg. Var(Zk)
StaticMNIST	-0.02 ± 0.03	0.93 ± 0.01	0.05 ± 0.37	1.50 ± 0.02
MNIST	0.00 ± 0.00	0.98 ± 0.01	-0.01 ± 0.02	0.78 ± 0.04
FashionMNIST	0.00 ± 0.00	0.98 ± 0.00	0.03 ± 0.03	0.83 ± 0.04
Omniglot	0.00 ± 0.02	0.95 ± 0.00	0.00 ± 0.01	0.24 ± 0.01
Table B.1 shows the average Mean(Zk) and Var(Zk) of ResnetVAE with RealNVP prior, computed
on test data. Average Mean(Zk) is defined as Dim(Z) PDm(Z) Mean(Zk), while average Var(Zk) is
defined as Dim(Z) PDm(Z) Var(Zk). The means and standard deviations of the above table are Com-
puted w.r.t. repeated experiments. Using the regularization term for qφ(z) makes the Mean(Zk) and
Var(Zk) of z samples close to N (0, I), which is in sharp contrast with the un-regularized case. For
fair comparison in Table 5 and Fig. 3, it is crucial to have Mean(Zk) and Var(Zk) close to N (0, I).
Test NLLs are not reported here, because we find no significant difference between regularized and
un-regularized models in terms of test NLLs.
Evaluation The negative log-likelihood (NLL), the reconstruction loss and the KL divergence
Ep?(x) DκL(qφ(z)kPλ(z)) are estimated with 1,000 Z samples from q©(z|x(i)) for each x(i) from
test data:
1N
NLL ≈ N ^XLogMeanExPj=ι [logpθ(x(i)∣z(i,j)) + logpλ(z(i,j)) — logq©(z(i,j)|x(i))]
1 N 1000
Reconstruction Loss ≈ 1000N XX XX [logpθ(χ(i)∣z(ij))]
1 N 1000
Ep?(x) DKL(qφ(z)kpλ(z)) ≈ 1000N XX X [logqφ(z(i,j)∣x(i)) — logPλ(z(i,j))]
where each z(i,j) is one sample from q©(z|x(i)).
LogMeanExPjL=1 f (x(i), z(i,j)) is defined as:
1L
LogMeanEXPL=If (x(i), z(i,j))] = fmax + log L X kXP f(x(i), z(i,j)) — /max)]
L j =1
where fmax = maXjf(x(i), z(i,j)).
We use 1,000 samples, because the two major previous works ofus (i.e., Tomczak & Welling (2018);
Bauer & Mnih (2019)) both used this number of samples. It is true that some previous works may
have used substantially larger number of samples (e.g., Chen et al. (2017) used 4,096 samples),
and according to (Grosse et al., 2015), the Monte Carlo estimator for NLL is a stochastic lower-
bound, such that larger number of samples typically would lead to lower (yet better) estimated
NLLs. However, in order to have fair comparison with (Tomczak & Welling, 2018; Bauer & Mnih,
2019), we decide to stick to 1,000 samples.
Active units active units (Burda et al., 2016) is defined as the number of latent dimensions whose
variance is larger than 0.01. The variance of the k-th dimension is formulated as:
Vark = Varp*(x)(Eqφ(z∣x) [Zk])
18
Under review as a conference paper at ICLR 2020
where zk is the k-th dimension of z. We compute Vark on the training data, while the inner expec-
tation Eqφ(z∣χ) [zk] is estimated by drawing 1,000 samples of Z for each x.
B.4	FORMULATION OF CLOSEST PAIRS OF qφ(z∣x) AND OTHERS
Closest pairs of qφ(z∣x) For each x(i) from training data, We find the training point x(j), whose
posterior qφ(z|x(j)) is the closest neighbor to qφ(z∣x(i)):
j = argminj=i kμφ(XCj)) - μφ(Xei))k
where μφ(x) is the mean of qφ(z∣x), and ∣∣ ∙ ∣∣ is the L2 norm. These two posteriors qφ(z∣x(i)) and
qφ(z|x(j)) are called a closest pair of qφ(z∣x).
Distance of a pair of qφ(z∣x) The distance dj of a closest pair qφ(z∣x(i)) and qΦ(z|x(j)) is:
dij = μφ(Xj)) - μφ(XCi))
dij = ∣dij ∣
Normalized distance of a pair of qφ(z∣x) For each closest pair qφ(z∣x(i)) and qφ(z∣x(j)), we
compute its normalized distance dij by:
f =	2dij
S	Std[i; j]+Std[j; i]
(B.3)
td[i; j] is formulated as:
std[i;j] = jvarqφ(z∣χ(i)) ((Z - μφ(x(i))) ∙ dj	(B.4)
where Varq6(z|x(a))(f (z)) is the variance of f (z) w.r.t. qφ(z∣x(i)). Weuse 1,000 samples to estimate
each Std[i; j]. Roughly speaking, the normalized distance dij can be viewed as “distance/std” along
the direction of dj, which indicates the scale of the “hole” between qφ(z∣x(i)) and qφ(z|x(j)).
B.5	Experiment res ults on Cifar10
We conduct experiments on Cifar10 (Krizhevsky, 2009). Although the results on Cifar10 are not as
good as the state-of-the-art deep hierarchical VAEs, using RealNVP prior also shows great improve-
ment on test NLLs and sampling quality over Gaussian prior, with just one latent variable.
Architectures of ResnetVAE for Cifar10 The latent variable z is a 3-d tensor, with shape 8 × 8 ×
32. The Gaussian posterior qφ(z∣x) is derived as:
qφ(ZIX) = N(μ°(hφ(X)), σφ(hφ(X)) D
μφ(hφ(x)) = hφ(x) → LinearConv1×1 [8 X 8 X 32]
log σφ(hφ(x)) = hφ(x) → LinearConv1×1 [8 × 8 × 32]
hφ(x) = x→ Resnet[32 X 32 X 96 → 32 X 32 X 96 → 32 X 32 X 96
→ 16 X 16 X 192 → 16 X 16 X 192→ 16 X 16 X 192
→8X 8X256→8X 8X256]
The generative distribution pθ(x∣z) is derived as:
Pθ(x∣z) = DiscretizedLogistic(μθ(hθ(z)), sθ(hθ(z)))
μθ(hθ(z)) = hθ(z) → LinearConv1×1 [32 X 32 X 3]
logsθ(hθ(z)) = hθ (z) → LinearConv1×1 [32 X 32 X 3]
hθ (z) = z → DeResnet[8 X 8 X 256 → 8 X 8 X 256
→ 16 X 16 X 192→ 16 X 16 X 192→ 16 X 16 X 192
→ 32 X 32 X 96 → 32 X 32 X 96]
where DiscretizedLogistic is the discretized logistic distribution proposed by Kingma et al. (2016).
Note we also clip log sθ(hθ(z)) by -11,just as the for the log-std of qφ(z∣x).
19
Under review as a conference paper at ICLR 2020
Table B.2: Test bpd on Cifar10.
Model	bpd
Models without PixelCNN decoder VAE + IAF (Kingma et al., 2016) BIVA (Maal0e et al., 2019) ResnetVAE ResnetVAE + RealNVP p(z), K = 50 Models with PixelCNN decoder	3.11 3.08 3.98 3.54
VLAE (Chen et al., 2017)	2.95
(a) Gaussian p(z)
(b) RealNVP p(z)
Figure B.1: Samples of: (a) ResnetVAE with Gaussian p(z); and (b) ResnetVAE with RealNVP
p(z). Both models use just one latent variable of shape 8 × 8 × 32.
RealNVP prior The RealNVP prior for Cifar10 consists of 50 blocks, while each block consists
of an invertible 1x1 conv (Kingma & Dhariwal, 2018), a coupling layer, and an actnorm, which can
be formulated as:
fλ(z) = Z → f1(h1) →--------→ fκ(hκ)
fk (hk ) = hk → InvertibleConv1×1 → CouplingLayer → ActNorm
CouplingLayer(u) = Concatul, ur Sigmoid (sλ(hλ,k(ul))) + tλ(hλ,k(ul))
hλ,k (ul) = ul → Resnet[8 × 8 × 64 → 8 × 8 × 64]
sλ(hλ,k(ul)) = hλ,k(ul) → LinearConv1×1[8 × 8 × 32]
tλ(hλ,k(ul)) = hλ,k(ul) → LinearConv1×1 [8 × 8 × 32]
where ul = u:,:,0:bDim(u)/2c is the left half channels of u, and ur = u:,:,bDim(u)/2c:Dim(u) is the
right half channels.
Experiment results Table B.2 shows the test bpd of our ResnetVAE on Cifar10. Although the
test bpd of our model is still far from those of deep hierarchical models, it is worth mention that
BIVA used 15 latent variables to reach the state-of-the-art. Our experiments show that, adding a
deep RealNVP prior can make the test bpd much better, even with just one latent variable. This
suggests that a complicated prior can be very effective for reaching good test bpd, and is potentially
helpful for reducing the depth of hierarchical VAEs.
20
Under review as a conference paper at ICLR 2020
Figure B.2: The average learned Std of Gaussian qφ(z∣x) and the test NLL of ResnetVAE with Real-
NVP and Gaussian P(Z) on MNIST. We also plot “min_std” as a reference, the minimum attainable
std clipped by the configured mininum logstd.
The most surprising fact is that, in our experiments, it is not possible to obtain any meaningful
samples using ResnetVAE with Gaussian p(z). in contrary, the samples obtained using ResnetVAE
with RealNVp p(z) is much more like natural images.
B.6	Impact of the minimum clipping value for the STD OF qφ(z∣χ)
We have mentioned in Section 5.1 that We clip the std of Gaussian posterior qφ(z|x) by a minimum
value of e-11 ≈ 1.67 × 10-5, so as to avoid unstable training. The clipping is actually done by
directly clipping the linear activation of log-std of qφ(z∣x) by -11, and then obtain std, instead of
clipping the std itself. Formally, the clipped log-std and the std are derived by:
logstd = maximum {σφ(fφ(z)), -11}
std = exp (logstd)
where fφ(z) is the hidden feature network for qφ(z∣x), and σφ(∙) is a linear fully-connected layer.
We use the clipped log-std directly instead of the std whenever possible in computing the log-
likelihood of Gaussian qφ(z∣x), which can be formulated as:
log qφ(z∣x) = -2 log(2∏) - logstd - 2exp(-2logstd) ∙ (x - μφ(fφ(z)))2
where μφ(z) is a linear fully-connected layer, producing the mean of qφ(z∣x). Std clipping is widely
used in many open-source implementations of VAEs. However, this trick is seldom discussed in
published papers, which might suggest more theoretical work should be done on this topic.
The clipping value -11 in our experiments is not chosen for better model performance. The term
exp (-2logstd) will have a maximum value of approximately 3.58 × 109 with this value, which we
think is safe enough for not having NaNs in training, considering that we mainly use float32 in our
neural network implementation.
In fact, we find that the test NLL of the model and the learned std of qφ(z∣x) is insensitive to the
clipping value under a certain threshold, which is demonstrated in Fig. B.2 and Table B.3. As long
as the minimum std is small enough, both the RealNVp prior and the Gaussian prior could induce
adaptive learned std of Gaussian posterior qφ(z∣x). One surprising fact is that, there is a turning
point where the clipping value is not small enough yet (i.e., min logstd being -3 for RealNVp p(z),
and -1 for Gaussian p(z)), the final learned std is smaller than the learned std with smaller clipping
values. The cause of this phenomenon is not analyzed in our paper, and we leave it as a future work.
21
Under review as a conference paper at ICLR 2020
Table B.3: The minimum Std ("min_std")，average learned Std of Gaussian qφ(z∣x) (“std")，the ratio
of minimum std with respect to the learned std ("min_std / std"), the test NLL and the active units
(“au”) of ResnetVAE with RealNVP p(z) and Gaussian p(z) on MNIST.
ResnetVAE with	min logstd	min_std	std	min_std / Std	NLL	au
RealNVP p(z)	1	2.7183	2.7183	1.0000	78.61	40
	0	1.0000	1.0000	1.0000	78.57	40
	-1	0.3679	0.3679	1.0000	78.58	40
	-2	0.1353	0.1353	1.0000	78.57	40
	-3	0.0498	0.0500	0.9967	78.59	37
	-5	0.0067	0.1796	0.0375	78.56	40
	-8	0.0003	0.1662	0.0020	78.56	40
	-11	0.0000	0.1750	0.0001	78.60	40
	-14	0.0000	0.1791	0.0000	78.56	40
	-∞	0.0000	0.1693	0.0000	78.58	40
Normal	1	2.7183	2.7183	1.0000	184.16	40
	0	1.0000	1.0000	1.0000	94.46	40
	-1	0.3679	0.5890	0.6246	84.03	36
	-2	0.1353	0.6839	0.1979	81.75	25
	-3	0.0498	0.6877	0.0724	81.07	25
	-5	0.0067	0.6882	0.0098	81.05	26
	-8	0.0003	0.6871	0.0005	81.04	25
	-11	0.0000	0.6860	0.0000	81.03	26
	-14	0.0000	0.6896	0.0000	81.06	25
	-∞	0.0000	0.6873	0.0000	81.08	26
B.7	Comparison of our work and the previous paper using RealNVP prior
The hyper-parameters of Huang et al. (2017) are quite different from ours: they only used 3 layers
of ResNet blocks in their qφ(z∣x) and 4 layers in their pθ(x|z). Also, they trained their ResConv
VAE on StaticMNIST only for 300 epochs, using an initial learning rate of 0.0005 without learning
rate annealing, and without validation set. In addition, we adopt invertible dense and actnorm in our
RealNVP flow, while they just used the original RealNVP flow. Most parts of our ResnetVAE with
RealNVP prior have larger complexity than the ResConv VAE of Huang et al. (2017), expect that
they used 2 hidden layers in each coupling layer, while we just used 1.
Table B.4: Test NLL of our ResnetVAE with low complexity and ResConv VAE (Huang et al., 2017)
on StaticMNIST. “pure” refers to the original RealNVP flow, without invertible dense and actnorm.
Model	p(z) flow	q(z|x) flow	Epochs	Initial lr	Early stopping	NLL
Our ResnetVAE	pure, K = 12	/	300	0.0005	no	81.05
Low Complexity	pure, K = 8	pure, K = 8	300	0.0005	no	80.88
	K=12	/	300	0.0005	no	80.67
	K=8	K=8	300	0.0005	no	80.66
ResConv VAE	pure, K = 12	/	300	0.0005	no	81.44
(Huang et al., 2017)	pure, K = 8	pure, K = 8	300	0.0005	no	80.81
By removing the invertible dense and actnorm components from the RealNVP prior, enforcing the
encoder and the decoder to have similar model complexity, using the same number of training epochs
and learning rate, and not using validation set to do early-stopping, we obtained similar results as
Huang et al. (2017) (the first two rows of “Ours”). Adding invertible dense and actnorm (the third
and fourth rows of “Ours”) will improve the test NLL, suggesting that the invertible dense and
actnorm can indeed bring benefit to the model performance.
According to the above results, it is reasonable to believe that, it is the deeper RealNVP prior, more
complicated model architecture, and longer training time, that give us better test NLL than Huang
22
Under review as a conference paper at ICLR 2020
et al. (2017). In conclusion, there may not be too many differences between our model and that
of Huang et al. (2017). However, we tune the hyper-parameters much better, and demonstrate that
ResnetVAE with RealNVP prior and just one latent variable can work as well as many deep hierar-
chical VAEs on MNIST-like datasets. The additional analysis should also be our novel contribution,
when compared to the work of Huang et al. (2017).
B.8	DISCUSSIONS ABOUT DKL(qφ(z)kpλ(z))
The KL divergence between the aggregated posterior and the prior, i.e., DKL(qφ(z)kpλ(z)), is first
analyzed by Hoffman & Johnson (2016) as one component of the ELBO decomposition (5). Since
qφ(Z) = J qφ(z|x)p*(x)dx and pλ(z) = JPθ(z|x)pθ(x)dx, Rosca et al. (2018) used this KL
divergence as a metric to quantify the approximation quality of both p (x) to p?(x), and qφ(z∣x) to
Pθ(z|x). As we are focusing on learning the prior, DKL(qφ(z)∣∣Pλ(z)) can in turn be a metric for
quantifying whether pλ (z) is close enough to the aggregated posterior qφ(z).
The evaluation of DKL(qφ(z)kpλ(z)), however, is not an easy task. One way to estimate the KL
divergence of two arbitrary distributions is the density ratio trick (Sugiyama et al., 2012; Mescheder
et al., 2017), where DKL(qφ(z)kpλ(z)) is estimated by a separately trained neural network classifier.
However, Rosca et al. (2018) revealed that such approach can under-estimate DKL(qφ(z)kpλ(z)),
and the training may even diverge when z has hundreds of or more dimensions.
Another approach is to directly estimate DKL(qφ(z)kpλ(z)) by Monte Carlo integration. The KL
divergence can be rewritten into the following form:
DKL(qφ(z)kpλ(z))
∕qφ(Z)IOg 需dz
? ( [ / I、？/ ʌ 1	∕qφ(z∣χ0)p*(χ0)dx"
J U qφ(z∣x)P (x)dxj log	_pλ(z----------dz
?/Rqφ(ZIXO)p*(x0)dx0
J p?(x) J qφ(z∣x) log	_Pλ(z)---------dz dx
Ep*(χ) Eqφ(z∣χ) [logEp*(χ0) [qφ(z∣x0)] - logPλ(z)]
(B.5)
Rosca et al. (2018) has already proposed a Monte Carlo based algorithm to estimate
DKL(qφ(z)kpλ(z)), in case that the training data is statically binarized. Since we mainly use dy-
namically binarized datasets, we slightly modified the algorithm according to Eq. (B.5), to allow
sampling multiple x from each image. Our algorithm is:
Surprisingly, we find that increasing nx will cause the estimated DKL(qφ(z)kpλ(z)) to decrease on
MNIST, see Table B.5. Given that our nz = 10 for all nx, the number of our sampled z (even when
nx = 1) is 10 × 60, 000 = 6 × 105, which should not be too small, since Rosca et al. (2018) only used
106 z in their experiments. When nx = 8, the number of z is 8 × 10 × 60, 000 = 4.8 × 106, which
is 4.8x larger than Rosca et al. (2018), not to mention the inner expectation Ep*χ) [qφ(z∣x0)] is
estimated with 8x larger number of x0. There should be in total 38.4x larger number of log qφ(z∣x0)
computed for estimating DKL(qφ(z)kpλ(z)) than Rosca et al. (2018), which has already costed
about 3 days on 4 GTX 1080 Ti graphical cards. We believe such a large number of Monte Carlo
samples should be sufficient for any algorithm with good behavior.
Table B.5: DKL(qφ(z)kpλ(z)) of a ResnetVAE with RealNVP prior trained on MNIST, estimated
by Algorithm B.2. nz = 10 for all nx. We only tried nx for up to 8, due to the growing computation
time of O(nX).
nx	1	2	3	5	8
DKL(qφ(z)kpλ(z))	15.279	14.623	14.254	13.796	13.392
According to the above observation, we suspect there must be some flaw in Algorithm B.2. Because
of this, we do not adopt Algorithm B.2 to estimate DKL(qφ(z)kpλ(z)).
Since no mature method has been published to estimate DKL(qφ(z)kpλ(z)), we decide not to use
DKL(qφ(z)kpλ(z)) to measure how our learned pλ(z) approximates the aggregated posterior qφ(z).
23
Under review as a conference paper at ICLR 2020
Algorithm B.2 Pseudocode for estimating DκL(qφ(z)kpλ(z)) (denoted as marginal_kl) on dynam-
ically binarized dataset, where μ is the pixel intensities of each original image.
x_samples =[]
for μ in training dataset do
for i = 1 . . . nx do
sample X from Bernoulli(μ)
append X to x_samples
end for
end for
marginal_kl = 0
for x in x_samples do
for i = 1 . . . nz do
sample Z from qφ(z∣x)
PoSterior_list =[]
for x0 in x_samples do
append log qφ(z∣x0) to posteriorJist
end for
log qφ(z) = LogMeanExP(POSterior」ist)
marginal_kl = marginal_kl + log qφ(z) - logpλ(z)
end for
end for
marginal_kl = marginal-kl/(len(x_samples) X n)
B.9	Iterative training can lead to improved reconstruction loss and
INCREASED ACTIVE UNITS
Table B.6 shows the NLLs of iterative training and post-hoc training with ResnetVAE (see also
Appendix B.3). Although still not comparable to joint training, both methods can bring large im-
provement in NLLs over standard VAE. Also, iterative training even further outperforms post-hoc
training by a large margin.
Table B.7 shows that, compared with post-hoc training, iterative training can lead to larger recon-
struction loss and increased number of active units. Section 5.5 suggests that the learned prior can
encourage better trade-off between reconstruction loss and the KL divergence, thus a well-trained
prior is essential for obtaining good qφ(z∣x) andpθ(x|z). However, the prior is in turn determined
by qφ(z∣x) and pθ(x|z), according to Eq. (5). Thus, it is important to alternate between training
Pθ(x|z) & qφ(z∣x) and training pλ(z), until they catch up wtih each other and reach an equilibrium.
Considering these reasons, it is not surprising why iterative training can result in larger (yet better)
reconstruction loss than post-hoc training. It is also clear why joint training works the best: because
Pθ(x|z) & qφ(z∣x), andpλ(z), are trained to catch up with each other at every step, thus the training
is more sufficient when using joint training than using the other two techniques.
24
Under review as a conference paper at ICLR 2020
Table B.6: Test NLL of ResnetVAE, with prior trained by: joint training, iterative training, post-hoc
training, and standard VAE (“none”) as reference. Flow depth K = 20.
ResnetVAE
Datasets	joint	iterative	Post-hoc	none
StaticMNIST	79.99	80.63	80.86	82.95
MNIST	78.58	79.61	79.90	81.07
FashionMNIST	224.09	224.88	225.22	226.17
Omniglot	93.61	94.43	94.87	96.99
Table B.7: Average reconstruction loss (“recons")，Ep*(χ)DκL(qφ(z∣x)∣∣pλ(z)) (“kl”)and active
units (“au”) of ResnetVAE with iteratively trained and post-hoc trained RealNVP priors.
iterative	post-hoc
Datasets	recons	kl	au	recons	kl	au
StaticMNIST	-58.0	26.4	38.7	-60.1	25.3	30
DynamicMNIST	-57.2	25.1	40	-58.7	24.7	25.3
FashionMNIST	-207.8	19.4	64	-208.9	19.0	27
Omniglot	-63.3	37.9	64	-67.0	35.8	59.3
B.10	Additional quantitative results
Additional quantitative results are shown in Table B.8 to B.20.
Table B.8: Test NLL on StaticMNIST."*" and ""has the same meaning as Table 1.
Model	NLL
Models without PixelCNN decoder
ConvHVAE + Lars Priort (Bauer & Mnih, 2019)
ConvHVAE + VamPPriort (Tomczak & Welling, 2018)
ResConv + RealNVP Prior (Huang et al., 2017)
VAE + IAF-Kingma et al., 2016)
BIVA -Maal0e etal.,2019)
ConvVAE + RNVP p(z), K = 50
ResnetVAE + RNVP p(z), K = 50
Models with PixelCNN decoder
VLAE -Chenetal.,2017)
PixelHVAE + VamPPriort (Tomczak & Welling, 2018)
PixelVAE + RNVP p(z), K = 50
81.70
81.09
81.44
79.88
78.59
80.09 ± 0.01
79.84 ± 0.04
79.03
79.78
79.01 ± 0.03
25
Under review as a conference paper at ICLR 2020
Table B.9: Test NLL on MNIST."t" and ""has the same meaning as Table 1.
Model	NLL
Models without PixelCNN decoder ConvHVAE + Lars Priort (Bauer & Mnih, 2019) ConvHVAE + VamPPriort (Tomczak & Welling, 2018) VAE + IAF-Kingma et al., 2016) BIVA -Maal0e etal.,2019) ConvVAE + RNVP p(z), K = 50 ResnetVAE + RNVP p(z), K = 50 Models with PixelCNN decoder	80.30 79.75 79.10 ± 0.07 78.41 78.61 ± 0.01 78.49 ± 0.01
VLAE -Chenetal.,2017) PixelVAEt (Gulrajani et al., 2017) PixelHVAE + VamPPriort (Tomczak & Welling, 2018) PixelVAE + RNVP p(z), K = 50	78.53 79.02 78.45 78.12 ± 0.04
Table B.10: Test NLL on Omniglot. “^” and "" has the same meaning as Table 1.
Model	NLL
Models without PixelCNN decoder	
ConvHVAE + Lars Priort (Bauer & Mnih, 2019)	97.08
ConvHVAE + VamPPriort (Tomczak & Welling, 2018)	97.56
BIVA -Maal0e etal.,2019)	91.34
ConvVAE + RNVP p(z), K = 50	93.62 ± 0.02
ResnetVAE + RNVP p(z), K = 50	93.52 ± 0.02
Models with PixelCNN decoder	
VLAE -Chenetal.,2017)	89.83
PixelHVAE + VamPPriort (Tomczak & Welling, 2018)	89.76
PixelVAE + RNVP p(z), K = 50	89.60 ± 0.01
Table B.11: Test NLL on FashionMNIST. “t” and ""has the same meaning as Table 1.
Model	NLL
Models without PixelCNN decoder ConvHVAE + Lars Priort (Bauer & Mnih, 2019)	225.92
ConvVAE + RNVP p(z), K = 50	224.64 ± 0.01
ResnetVAE + RNVP p(z), K = 50	224.07 ± 0.01
Models with PixelCNN decoder PixelVAE + RNVP p(z), K = 50	223.36 ± 0.06
B.11	Additional qualitative results
Additional qualitative results are shown in Fig. B.3 to B.5.
26
Under review as a conference paper at ICLR 2020
ɔfɪOo 夕 3 S \ 7 夕、
? X Oo ? 5 5 \ 7 夕<
2 谖 O6 7b301∙7
∕⅛⅛√2/QQ Sq7
12B£5afif<r2 7
g//W夕//夕3J
夕513，包 0 7 4/
Fo 71/ / q b 夕 ⅜∏ σΛ /6
Oq 夕 7f71‰w 2
— 33夕 3ωb3q,
，/47Z2S3夕。
"/"7zλ5 3 夕G
αG∕brrΛ'-z?*
T3G。O - Coy/Q
*g77<∕。J/6
3352 G 夕 u<5*7?
，Q9ops5qD7
ʧ/彳/b∕c⅛∕3z
IJ 7/ 5Z 76 4/
f ?/ I √∙ Hʃ /
(a) standard	(b) RealNVP q(z|x)	(c) RealNVP p(z)
Figure B.3: Samples from ResnetVAE trained on MNIST. The last column of each 10x10 grid shows
the training set images, most similar to the second-to-last column in pixel-wise L2 distance.
(c) RealNVP p(z)
(a) standard
(b) RealNVP q(z|x)
S
-t
q—
*一；
q,血一
t守
IeaJ anf ɪ
V
.
1
Figure B.4: Samples from ResnetVAE trained on FashionMNIST. The last column of each 10x10
grid shows the training set images, most similar to the second-to-last column in pixel-wise L2 dis-
tance.
& R nj-ητcQ;
UFgn更卬反P电卡
k +•■&m 8 C159IlJ卫
g√t⅛ZJ9 总Q出工?
% a>/ 3 q B M」2
3 084力旧立£
Iτ9 3m6 £>占 Jj⅛
k a 0cd7 6nt4c⅛∙⅛
« 刃 UHGCt一QB9b
a*o3f⅞6 G 0
:Xr工B卢由「
⅜- D 4 7 O-弓
a 3 Par爸小
W 十-½ T Q AG
?日 K QiS 口 J
QELMr用漆
X 的 9、FH- 4
国8工» <4 “ e

(a) standard	(b) RealNVP q(z|x)	(c) RealNVP p(z)
Figure B.5: Samples from ResnetVAE trained on Omniglot. The last column of each 10x10 grid
shows the training set images, most similar to the second-to-last column in pixel-wise L2 distance.
B.12	Additional results: ablation study and posterior overlapping
Additional results about ablation study and posterior overlapping are shown in Table B.12 to B.19.
27
Under review as a conference paper at ICLR 2020
Table B.12: Average test NLL (lower is better) of different models, with Gaussian prior & Gaussian
posterior (“standard”), Gaussian prior & RealNVP posterior (“RNVP q(z|x)”), and RealNVP prior
& Gaussian posterior (“RNVP p(z)”). Flow depth K = 20.
Models	Datasets			
	StaticMNIST	MNIST	FashionMNIST	Omniglot
standard	88.84 ± 0.05	84.48 ± 0.03	228.60 ± 0.03	106.42 ± 0.14
DenseVAE RNVP q(z|x)	86.07 ± 0.11	82.53 ± 0.00	227.79 ± 0.01	102.97 ± 0.06
RNVP p(z)	84.87 ± 0.05	80.43 ± 0.01	226.11 ± 0.02	102.19 ± 0.12
standard	83.63 ± 0.01	82.14 ± 0.01	227.51 ± 0.08	97.87 ± 0.02
ConvVAE RNVP q(z|x)	81.11 ± 0.03	80.09 ± 0.01	226.03 ± 0.00	94.90 ± 0.03
RNVP p(z)	80.06 ± 0.07	78.67 ± 0.01	224.65 ± 0.01	93.68 ± 0.01
standard	82.95 ± 0.09	81.07 ± 0.03	226.17 ± 0.05	96.99 ± 0.04
ResnetVAE RNVP q(z|x)	80.97 ± 0.05	79.53 ± 0.03	225.02 ± 0.01	94.30 ± 0.02
RNVP p(z)	79.99 ± 0.02	78.58 ± 0.01	224.09 ± 0.01	93.61 ± 0.04
standard	79.47 ± 0.02	78.64 ± 0.02	224.22 ± 0.06	89.83 ± 0.04
PixelVAE RNVP q(z|x)	79.09 ± 0.01	78.41 ± 0.01	223.81 ± 0.00	89.69 ± 0.01
RNVP p(z)	78.92 ± 0.02	78.15 ± 0.04	223.40 ± 0.07	89.61 ± 0.03
Table B.13: Test NLL of ResnetVAE on MNIST, with RealNVP posterior (“q(z|x)”), RealNVP
prior (“p(z)”), and RealNVP prior & posterior (“both”). Flow depth K is 2K0 for the posterior or
the prior in “q(z|x)” and “p(z)”, while K0 for both the posterior and the prior in “both”.
K0
ResnetVAE with		1	2	5	10	20
q(z|x), K	= 2K0	80.29 ± 0.03	80.00 ± 0.00	79.68 ± 0.01	79.53 ± 0.02	79.49 ± 0.02
both, K	K0	79.85 ± 0.01	79.42 ± 0.02	79.01 ± 0.02	78.71 ± 0.00	78.56 ± 0.01
p(z), K =	2K0	79.58 ± 0.02	79.21 ± 0.01	78.75 ± 0.01	78.58 ± 0.01	78.51 ± 0.02
Table B.14: Test reconstruction loss of ResnetVAE on MNIST, with RealNVP posterior (“q(z|x)”),
RealNVP prior (“p(z)”), and RealNVP prior & posterior (“both”). Flow depth K is 2K0 for the
posterior or the prior in “q(z|x)” and “p(z)”, while K0 for both the posterior and the prior in “both”.
K0
ResnetVAE with	1	2	5	10	20
q(z|x), K = 2K0	-57.20 ± 0.05	-56.82 ± 0.01	-56.37 ± 0.04	-56.15 ± 0.05	-55.98 ± 0.05
both, K = K0	-56.34 ± 0.01	-55.64 ± 0.02	-54.68 ± 0.04	-54.04 ± 0.04	-53.76 ± 0.02
p(z), K = 2K0	-55.75 ± 0.04	-54.90 ± 0.03	-54.00 ± 0.04	-53.64 ± 0.02	-53.49 ± 0.03
Table B.15: Test NLL of ResnetVAE, with RealNVP prior of different flow depth K. Even K = 1,
RealNVP prior improves NLLs by about 1 nat. There is no over-fitting for K up to 50. However, we
think over-fitting may happen with sufficiently powerful prior, 50-blocks of RealNVP may simply
not be deep enough.
Datasets
Flow depth	StaticMNIST	MNIST	FashionMNIST	Omniglot
0	82.95 ± 0.09	81.07 ± 0.03	226.17 ± 0.05	96.99 ± 0.04
1	81.76 ± 0.04	80.02 ± 0.02	225.27 ± 0.03	96.20 ± 0.06
2	81.30 ± 0.02	79.58 ± 0.02	224.78 ± 0.02	95.35 ± 0.06
5	80.64 ± 0.06	79.09 ± 0.02	224.37 ± 0.01	94.47 ± 0.01
10	80.26 ± 0.05	78.75 ± 0.01	224.18 ± 0.01	93.92 ± 0.02
20	79.99 ± 0.02	78.58 ± 0.01	224.09 ± 0.01	93.61 ± 0.04
30	79.90 ± 0.05	78.52 ± 0.01	224.07 ± 0.01	93.53 ± 0.02
50	79.84 ± 0.04	78.49 ± 0.01	224.07 ± 0.01	93.52 ± 0.02
28
Under review as a conference paper at ICLR 2020
Table B.16:	Average test ELBO (“elbo”), reconstruction loss (“recons”),
Ep*(x)DκL(qφ(z∣x)kPλ(z)) (“kr)and Ep*(χ)DκL(qφ(z∣x)∣∣Pθ(z∣x)) (“kQx”)of various
DenseVAE. Flow depth K = 20.
Datasets	standard			
	elbo	recons	kl	klz|x
StaticMNIST	-94.53	-65.54	29.00	5.69
MNIST	-88.19	-62.04	26.16	3.71
FashionMNIST	-230.81	-211.71	19.11	2.22
Omniglot	-113.59	-78.25	35.33	7.17
		RealNVP	q(z|x)	
Datasets	elbo	recons	kl	klz|x
StaticMNIST	-90.78	-62.55	28.23	4.71
MNIST	-85.15	-58.83	26.31	2.62
FashionMNIST	-229.53	-210.35	19.18	1.75
Omniglot	-108.54	-73.71	34.83	5.57
		RealNVP p(z)		
Datasets	elbo	recons	kl	klz|x
StaticMNIST	-88.87	-63.35	25.52	4.00
MNIST	-82.57	-55.99	26.58	2.14
FashionMNIST	-227.72	-208.13	19.59	1.61
Omniglot	-107.99	-73.10	34.89	5.80
Table B.17:	Average test ELBO (“elbo”), reconstruction loss (“recons”),
Ep*(x)DκL(qφ(z∣x)kPλ(z)) (“kl”) and Ep*(χ)DκL(qφ(z∣x)∣∣Pθ(z∣x)) (“kQx”)of various
ConvVAE. Flow depth K = 20.
Datasets	standard			
	elbo	recons	kl	klz|x
StaticMNIST	-88.15	-60.34	27.81	4.51
MNIST	-85.89	-58.97	26.92	3.75
FashionMNIST	-230.43	-210.24	20.18	2.92
Omniglot	-104.70	-66.00	38.70	6.83
		RealNVP	q(z|x)	
Datasets	elbo	recons	kl	klz|x
StaticMNIST	-83.92	-56.67	27.25	2.82
MNIST	-82.46	-56.08	26.38	2.37
FashionMNIST	-228.20	-207.33	20.88	2.18
Omniglot	-99.92	-62.48	37.44	5.02
		RealNVP p(z)		
Datasets	elbo	recons	kl	klz|x
StaticMNIST	-82.91	-54.01	28.90	2.86
MNIST	-80.42	-53.33	27.09	1.75
FashionMNIST	-226.65	-204.93	21.72	2.00
Omniglot	-98.59	-59.07	39.51	4.91
29
Under review as a conference paper at ICLR 2020
Table B.18:	Average test ELBO (“elbo”), reconstruction loss (“recons”),
Ep*(x)DκL(qφ(z∣x)kPλ(z)) (“kr)and Ep*(χ)DκL(qφ(z∣x)∣∣Pθ(z∣x)) (“kQx”)of various
ResnetVAE. Flow depth K = 20.
Datasets	standard			
	elbo	recons	kl	klz|x
StaticMNIST	-87.61	-60.09	27.52	4.67
MNIST	-84.62	-58.70	25.92	3.55
FashionMNIST	-228.91	-208.94	19.96	2.74
Omniglot	-104.87	-66.98	37.89	7.88
		RealNVP	q(z|x)	
Datasets	elbo	recons	kl	klz|x
StaticMNIST	-84.72	-58.10	26.63	3.75
MNIST	-81.95	-56.15	25.80	2.42
FashionMNIST	-227.16	-206.61	20.54	2.14
Omniglot	-100.30	-63.34	36.96	6.00
		RealNVP p(z)		
Datasets	elbo	recons	kl	klz|x
StaticMNIST	-82.85	-54.32	28.54	2.87
MNIST	-80.34	-53.64	26.70	1.76
FashionMNIST	-225.97	-204.66	21.31	1.88
Omniglot	-99.60	-61.21	38.39	5.99
Table B.19:	Average test ELBO (“elbo”), reconstruction loss (“recons”),
Ep*(x)DκL(qφ(z∣x)kPλ(z)) (“kl”) and Ep*(χ)DκL(qφ(z∣x)∣∣Pθ(z∣x)) (“kQx”)of various
PixelVAE. Flow depth K = 20.
Datasets	standard			
	elbo	recons	kl	klz|x
StaticMNIST	-81.06	-69.02	12.03	1.59
MNIST	-79.88	-68.73	11.15	1.24
FashionMNIST	-225.60	-214.15	11.45	1.38
Omniglot	-91.58	-82.80	8.78	1.75
		RealNVP	q(z|x)	
Datasets	elbo	recons	kl	klz|x
StaticMNIST	-80.65	-67.91	12.75	1.57
MNIST	-79.59	-68.17	11.42	1.18
FashionMNIST	-225.05	-213.41	11.64	1.24
Omniglot	-91.26	-83.17	8.10	1.57
		RealNVP p(z)		
Datasets	elbo	recons	kl	klz|x
StaticMNIST	-80.60	-62.22	18.38	1.68
MNIST	-79.28	-64.41	14.87	1.13
FashionMNIST	-224.65	-210.16	14.49	1.26
Omniglot	-91.78	-79.70	12.07	2.16
30
Under review as a conference paper at ICLR 2020
Table B.20: Test NLL of ResnetVAE, with prior trained by: joint training, iterative training, post-
hoc training, and standard VAE (“none”) as reference. Flow depth K = 20.
Datasets	ResnetVAE			
	joint	iterative	post-hoc	none
StaticMNIST	79.99 ± 0.02	80.63 ± 0.02	80.86 ± 0.04	82.95 ± 0.09
MNIST	78.58 ± 0.01	79.61 ± 0.01	79.90 ± 0.04	81.07 ± 0.03
FashionMNIST	224.09 ± 0.01	224.88 ± 0.02	225.22 ± 0.01	226.17 ± 0.05
Omniglot	93.61 ± 0.04	94.43 ± 0.11	94.87 ± 0.05	96.99 ± 0.04
Figure B.6: Avg. Std of qΦ(z|x) of β-ResnetVAE trained with different β and prior flow depth K.
31
Under review as a conference paper at ICLR 2020
B.13	Additional results: two level hierarchical VAE with flow prior
In addition to the comprehensive experiments based on VAE, we also conduct experiments using
two-level ResnetHVAE (Tomczak & Welling, 2018), just as the two major previous works (Tomczak
& Welling, 2018; Bauer & Mnih, 2019).
Architectures of ResnetHVAE for binarized datasets The observed variable x is a 3-d tensor,
just the same as VAEs. The latent variables z1 and z2 are vectors with the same number of dimen-
sions, which is 40 on MNIST and StaticMNIST, and 64 on FashionMNIST and Omniglot. We use
Dim(z) to denote this dimensionality.
The overall factorization for HVAE is:
pθ(x,z2,z1) = pθ (x|z1, z2) pθ (z1 |z2) pλ (z2)
qφ(z2, zi|x) = qφ(z1∣z2, x) qφ(z2∣x)
where pλ(z2) is either a flow prior or a unit Gaussian prior, depending on the context.
The shared feature network hφ(x) for qφ(z2∣x) and qΦ(zι |x) is:
hφ(x) = x → Resnet[28 × 28 × 32 → 28 × 28 × 32
→ 14 × 14 × 64→ 14 × 14 × 64
→7×7×64→7×7× 16]
→ Flatten
The Gaussian posterior qφ(z2 |x) is derived as:
qφ(Z2IX) = N Sih©(Xy), σφ(hφ(Xy) I)
μφ(hφ(x)) = hφ(x) → Linear[Dim(z)]
log σφ(hφ(X)) = hφ(X) → Linear[Dim(z)]
The Gaussian posterior qφ(z1 IX, z2) is derived as:
qφ(zι ∣x, Z2) = N(μ0(gφ(x, Z2)), σφ(gφ(x, Z2))I)
“φ(gφ(x, Z2)) = gφ(x, Z2) → Linear[Dim(z)]
log σφ (gφ (x, Z2)) = gφ(x, Z2) → Linear[Dim(Z)]
gφ(x, Z2) = Concat[tφ(x), uφ(Z2)] → Dense[300]
tφ (x) = hφ(x) → Dense[150 → 150]
uφ(Z2) = Z2 → Dense[150 → 150]
The Gaussian prior pθ(Z1IZ2) is derived as:
Pθ(Z1∣Z2) = N Mθ(tθ(Z2)), σ2(tθ(Z2)) I)
μθ(tθ(Z2)) = tθ(Z2) → Linear[Dim(Z)]
log σθ(tθ(Z2)) = tθ(Z2) → Linear[Dim(Z)]
tθ(Z2) = Z2 → Dense[150 → 150]
For binarized images, the Bernoulli conditional distribution pθ (xIZ1, Z2) is derived as:
Pθ (x∣Z1,Z2) = BernouUi[μθ (gθ (zi, Z2))]
log μθ(gθ(Z1,Z2))	= gθ(zi, Z2) → LinearConv1×1 [28 X 28 X 1]
1 — μθ (gθ (z1,z2))
gθ(Z1, Z2) = Concat[Z1,Z2] → hθ(Z)
hθ (Z) = Dense[784] → UnFlatten[7 X 7 X 16]
→ DeResnet[7 X 7 X 64 → 14 X 14 X 64
→ 14 X 14 X 64 → 28 X 28 X 32
→ 28 X 28 X 32]
Note the shared feature network hφ(x) and the decoder network hθ(Z) have exactly the same archi-
tecture as VAE (see Appendix B.2).
32
Under review as a conference paper at ICLR 2020
Table B.21: Test negative log-likelihood of ResnetHVAE, compared with ResnetVAE. Flow depth
K = 20 for pλ (z) in ResnetVAE. For ResnetHVAE, pλ (z2) is a flow prior with K = 20, and
pλ(z1 |z2) is a Gaussian prior.
Models	Datasets StaticMNIST	MNIST	FashionMNIST Omniglot
standard ResnetVAE	RNVP p(z)	82.95 ± 0.09 81.07 ± 0.03	226.17 ± 0.05	96.99 ± 0.04 79.99 ± 0.02 78.58 ± 0.01	224.09 ± 0.01	93.61 ± 0.04
standard ResnetHVAE RNVP p(z2 )	81.58 ± 0.00 80.09 ± 0.03	225.18 ± 0.02	95.80 ± 0.03 79.95 ± 0.01 78.56 ± 0.01	224.11 ± 0.01	93.47 ± 0.01
Table B.22: Average number of active units of ResnetHVAE of z2 and z1, with unit Gaussian p(z2)
andp(z1) (“standard”), and RealNVP p(z2) and Gaussian p(z1) (“RNVP p(z2)”).
Datasets
ResnetHVAE	StaticMNIST		MNIST		FashionMNIST		Omniglot	
	z1	z2	z1	Z2	z1	z2	z1	z2
standard	11.3	25	8.7	18.3	16.3	16.7	12.3	57.7
RealNVP p(z2 )	1.3	40	0	40	0.3	64	0	64
Experimental results Table B.21 shows the test negative log-likelihood (NLL) of ResnetHVAE,
with or without RealNVP p(z2), compared with ResnetVAE, with or without RealNVP p(z). We
can see that, ResnetHVAE without flow prior has significantly better test NLLs than ResnetVAE
without flow prior. However, ResnetHVAE with RealNVP p(z2) only has roughly the same test
NLLs as ResnetVAE with RealNVP p(z).
Further investigation on the active units of ResnetHVAE (Table B.22) reveals the cause of such ex-
perimental results: when coupled withRealNVPp(z2), the second latent variable z1 in ResnetHVAE
degenerates, such that no dimension is active in the case of MNIST, FashionMNIST and Omniglot,
while almost no dimension is active in the case of StaticMNIST. We also added RealNVP to z1, but
the results showed no difference.
However, we find such degeneration may not always happen. We conducted a pilot experiment of
ResnetHVAE with RealNVP p(z2) and Gaussian p(z1) on Cifar-10. The shape of z2 and z1 are
both 8 × 8 × 16. The number of active units of z2 is 1024 (i.e., all dimensions are active), while
the number of active units of z1 is 768 (specifically, these 768 dimensions consist 12 channels of
8 × 8 feature maps). The test bpd of this experiment is 3.96. This suggests that the zero active
units of z1 in Table B.22 is not caused by the RealNVP prior. The resnet network architecture we
used in our experiments (on the binarized datasets) may have limited the model capacity, such that
the ResnetVAE with RealNVP prior is already very close to the limitation of test NLLs in such
condition, where adding an auxiliary latent variable will not help.
We also report the reconstruction loss, the KL divergence DκL(qφ(z2,zi∣x)∣∣pθ(z2,zι)) and
DκL(qφ(z2, zι∣x)∣∣Pθ(z2, zι∣x)) of ResnetHVAE with or without RealNVP p(z2), which have the
same trend as ResnetVAE (Table 6).
33
Under review as a conference paper at ICLR 2020
Table B.23:	Average	test ELBO		(“elbo”),	reconstruction		loss	(“recons”),	
DκL(qφ(z2, Zl∣x)kPθ(Z2,Z1))		(“kl”),	and D	KL(qφ(z2, Zl∣x)kpθ(Z2, Zl∣x))			(“klz|x	”) of of
ResnetHVAEs with different prior.								
		standard				RealNVP p(z2 )		
Datasets	elbo	recons	kl	klz∣x	elbo	recons	kl	klz|x
StaticMNIST	-85.67	-58.88	26.78	4.08	-82.82	-54.11	28.71	2.87
MNIST	-82.98	-57.19	25.79	2.90	-80.30	-53.55	26.76	1.74
FashionMNIST	-227.56	-207.46	20.10	2.38	-225.99	-204.48	21.51	1.88
Omniglot	-103.05	-65.04	38.01	7.25	-99.37	-61.00	38.37	5.90
B.14 Additional results: learned prior on low posterior samples
Rosca et al. (2018) observed that some of the samples from unit Gaussian prior can have low qφ(z)
and low visual quality, and they name these samples the “low posterior samples”. Unlike this paper,
they tried various approaches to match qφ(z) to unit Gaussian pλ(z), including adversarial train-
ing methods, but still found low posterior samples scattering across the whole prior. We shall see
that learning the prior can avoid having high pλ (z) on low posterior samples, and thus we suggest
adopting learned prior as a cheap solution to this problem.
We shall first introduce the algorithm proposed by Rosca et al. (2018), which can be used to obtain
low posterior samples (z samples which have low likelihoods on qφ(z)) from a trained VAE. Their
algorithm first samples a large number of z from the prior pλ (z), then uses Monte Carlo estimator
to evaluate qφ (z) on these z samples, and finally chooses a certain number of z with the lowest
qφ (Z) likelihoods as the low posterior samples. They also sampled one X from pθ (x|z) for each low
posterior sample z, and plotted the sample means of these x and the histograms of ELBO on these
X. Although we have found their Monte Carlo estimator for DKL(qφ(z)kpλ(z)) vulnerable (see
Appendix B.8), their low posterior samples algorithm only ranks qφ(z) for each z, and the visual
results of their algorithm seem plausible. Thus we think this algorithm should be still convincing
enough, and we also use it to obtain low posterior samples.
To compare the learned prior with unit Gaussian prior on such low posterior samples, we first train
a ResnetVAE with unit Gaussian prior (denoted as standard ResnetVAE), and then add a post-hoc
trained RealNVP prior upon this original ResnetVAE (denoted as post-hoc trained ResnetVAE).
We then obtain 10,000 z samples from the standard ResnetVAE, and choose 100 z with the lowest
qφ(z) among these 10,000 samples, evaluated on standard ResnetVAE. Fixing these 100 z samples,
we plot the histograms of log pλ (z) w.r.t. standard ResnetVAE and post-hoc trained ResnetVAE.
We also obtain one X sample from pθ (x|z) for each z, and plot the histograms of ELBO for each X
(w.r.t. the two models) and their sample means. See Fig. B.7.
The X samples of post-hoc trained ResnetVAE (bottom right) are the same as those of standard
ResnetVAE (top right), since post-hoc trained ResnetVAE has exactly the same pθ (x|z) and qφ(z∣x)
as standard ResnetVAE. However, the post-hoc trained prior successfully assigns much lower
logpλ(z) (bottom left) than unit Gaussian prior (top left) on the low posterior samples, which sug-
gests that a post-hoc trained prior can avoid granting high likelihoods to these samples in the latent
space. Note post-hoc trained ResnetVAE also assigns slightly lower ELBO (bottom middle) than
standard ResnetVAE (top middle) to X samples corresponding to these low posterior samples.
To verify whether a learned prior can avoid obtaining low posterior samples in the first place, we
obtained low posterior samples from a ResnetVAE with jointly trained prior (denoted as jointly
trained ResnetVAE), see Fig. B.8. Compared with Fig. B.7, we can see that logpλ (z) of these low
posterior samples and ELBO of the corresponding X samples are indeed substantially higher than
those of standard ResnetVAE and post-hoc trained ResnetVAE. However, the visual quality is not
perfect, indicating there is still room for improvement.
34
Under review as a conference paper at ICLR 2020
train samples	low posterior samples
ω4>susoX PjPPUP∞
-90	-75	-60	-45
/o7f7Grv7G 6g
彳 Qmgo!5ro
yfs3Q50 a√o
qg U 7*6
。凸o〃G3q oɔE
，弓Qu(b0 W
OUla qQ Q6
∖S>G □‹s5夕 G
▼ J£)G 夕 £rl P 70
g Z 80。Q7G
ω4>susoX POU3J1 OOqJSOd
elbo
77, rv7oCOG
yocl N 3 45r0
3gs3Q50a√o
Jg U7*6
OaO〃G3G oɔ弓
OUY0c?Q
∖s>( □‹s5夕G
▼ J£)G 》 £2.P 70
。IoZn。。Q7G
X 〜Pθ(x|z)
Figure B.7:	(left) Histograms of logpλ(z) of the low posterior samples, (middle) histograms of
ELBO of x samples corresponding to each low posterior sample, and (right) the means of these x,
on standard ResnetVAE and post-hoc trained ResnetVAE.
ω4>JOUSOX pousJI-Juδf
train_samples	low_posterior_samples
60
-
log Pλ(z)
elbo
5α5G3v3>∕t5
JT 5 么弓 3 J-TVJ 5
Kg〃 WʒU 5 群VB
"%%50>n5。
J9尸&” Hguκ2
52 3 3qJ3Jbo%
啖43夕以 qa厂B A-
33 4 J3E6X0F
XrC / QN 7ce>83
IΓ><T3OIΛIX<⅜∕G A
X 〜pθ(x|z)
Figure B.8:	(left) Histograms of logpλ(z) of the low posterior samples, (middle) histograms of
ELBO of X samples corresponding to each low posterior sample, and (right) the means of these x,
on jointly trained ResnetVAE.
B.15 WALL-CLOCK TIMES
Figure B.9:	Average training time of various models on MNIST, flow depth K = 20. Black sticks
are standard deviation bars. For PixelVAE, training mostly terminates in half way due to early-
stopping.
35
Under review as a conference paper at ICLR 2020
Figure B.10:	Average training time and test negative log-likelihood (NLL) of ResnetVAE with Re-
alNVP prior of different flow depth. Vertical sticks are standard deviation bars.
We report the average training time of various models trained on MNIST, see Figs. B.9 and B.10.
Each experiment runs on one GTX 1080 Ti graphical card. In Fig. B.9, we can see that the compu-
tational cost of RealNVP prior is independent with the architecture of other parts of a model. For
complicated architectures like ResnetVAE and PixelVAE, the cost of adding a RealNVP prior is
fairly acceptable, since it can bring large improvement. In Fig. B.10, we can see that for K > 50,
there is likely to be little gain in test NLL, but the computation time will grow even larger. That’s
why we do not try larger K in our experiments.
References
Matthias Bauer and Andriy Mnih. Resampled priors for variational autoencoders. In The 22nd
International Conference on Artificial Intelligence and Statistics, pp. 66-75, 2019.
Samuel Bowman, Luke Vilnis, Oriol Vinyals, Andrew M. Dai, Rafal Jozefowicz, and Samy Bengio.
Generating Sentences from a Continuous Space. In Proceedings of the Twentieth Conference on
Computational Natural Language Learning (CoNLL)., 2016.
Yuri Burda, Roger B. Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. In 4th
International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May
2-4, 2016, Conference Track Proceedings, 2016.
Xi Chen, Diederik P. Kingma, Tim Salimans, Yan Duan, Prafulla Dhariwal, John Schulman, Ilya
Sutskever, and Pieter Abbeel. Variational lossy autoencoder. In 5th International Conference
on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track
Proceedings, 2017.
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real NVP. In 5th
International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26,
2017, Conference Track Proceedings, 2017.
Izrail Moiseevitch Gelfand and Richard A. Silverman. Calculus of Variations. Courier Corporation,
2000.
Roger B. Grosse, Zoubin Ghahramani, and Ryan P. Adams. Sandwiching the marginal likelihood
using bidirectional Monte Carlo. arXiv:1511.02543 [cs, stat], November 2015.
Ishaan Gulrajani, KUndan Kumar, Faruk Ahmed, Adrien Ali Taiga, Francesco Visin, David Vazquez,
and Aaron C. Courville. Pixelvae: A latent variable model for natural images. In 5th Interna-
tional Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017,
Conference Track Proceedings, 2017.
Matthew D. Hoffman and Matthew J. Johnson. Elbo surgery: Yet another way to carve up the
variational evidence lower bound. In Workshop in Advances in Approximate Bayesian Inference,
NIPS, 2016.
Chin-Wei Huang, Ahmed Touati, Laurent Dinh, Michal Drozdzal, Mohammad Havaei, Laurent
Charlin, and Aaron Courville. Learnable Explicit Density for Continuous Latent Space and Vari-
ational Inference. arXiv:1710.02248 [cs, stat], October 2017.
36
Under review as a conference paper at ICLR 2020
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In 3rd Inter-
national Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9,
2015, Conference Track Proceedings, 2015.
Diederik P. Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling.
Improved variational inference with inverse autoregressive flow. In Advances in Neural Informa-
tion Processing Systems, pp. 4743-4751, 2016.
Durk P Kingma and Prafulla Dhariwal. Glow: Generative Flow with Invertible 1x1 Convolutions.
In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.),
Advances in Neural Information Processing Systems 31, pp. 10215-10224. Curran Associates,
Inc., 2018.
Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.
B. M. Lake, R. Salakhutdinov, and J. B. Tenenbaum. Human-level concept learning through prob-
abilistic program induction. Science, 350(6266):1332-1338, December 2015. ISSN 0036-8075,
1095-9203. doi: 10.1126/science.aab3050.
Hugo Larochelle and Iain Murray. The neural autoregressive distribution estimator. In Proceedings
of the Fourteenth International Conference on Artificial Intelligence and Statistics, pp. 29-37,
2011.
Lars Maal0e, Marco Fraccaro, Valentin Lievin, and Ole Winther. BIVA: A Very Deep Hierarchy of
Latent Variables for Generative Modeling. arXiv:1902.02102 [cs, stat], February 2019.
Andrew L. Maas, Awni Y. Hannun, and Andrew Y. Ng. Rectifier nonlinearities improve neural
network acoustic models. In Proc. Icml, volume 30, pp. 3, 2013.
Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. Adversarial variational bayes: Unifying
variational autoencoders and generative adversarial networks. In Proceedings of the 34th Inter-
national Conference on Machine Learning-Volume 70, pp. 2391-2400. JMLR. org, 2017.
Danilo Jimenez Rezende and Fabio Viola. Taming VAEs. arXiv:1810.00597 [cs, stat], October
2018.
Mihaela Rosca, Balaji Lakshminarayanan, and Shakir Mohamed. Distribution Matching in Varia-
tional Inference. arXiv preprint arXiv:1802.06847, 2018.
Ruslan Salakhutdinov and Iain Murray. On the quantitative analysis of deep belief networks. In
Proceedings of the 25th International Conference on Machine Learning, pp. 872-879. ACM,
2008.
Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P. Kingma. Pixelcnn++: Improving the
pixelcnn with discretized logistic mixture likelihood and other modifications. In 5th Interna-
tional Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017,
Conference Track Proceedings, 2017.
Masashi Sugiyama, Taiji Suzuki, and Takafumi Kanamori. Density Ratio Estimation in Machine
Learning. Cambridge University Press, 2012.
Jakub Tomczak and Max Welling. VAE with a VampPrior. In International Conference on Artificial
Intelligence and Statistics, pp. 1214-1223, 2018.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST: A Novel Image Dataset for Bench-
marking Machine Learning Algorithms. arXiv:1708.07747 [cs, stat], August 2017.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In Proceedings of the British
Machine Vision Conference 2016, BMVC 2016, York, UK, September 19-22, 2016, 2016.
37