Under review as a conference paper at ICLR 2020
Efficient High-Dimensional Data Representa-
tion Learning via Semi-Stochastic Block Co-
ordinate Descent Methods
Anonymous authors
Paper under double-blind review
Ab stract
With the increase of data volume and data dimension, sparse representation learn-
ing attracts more and more attention. For high-dimensional data, randomized
block coordinate descent methods perform well because they do not need to cal-
culate the gradient along the whole dimension. Existing hard thresholding algo-
rithms evaluate gradients followed by a hard thresholding operation to update the
model parameter, which leads to slow convergence. To address this issue, we
propose a novel hard thresholding algorithm, called Semi-stochastic Block Coor-
dinate Descent Hard Thresholding Pursuit (SBCD-HTP). Moreover, we present
its sparse and asynchronous parallel variants. We theoretically analyze the con-
vergence properties of our algorithms, which show that they have a significantly
lower hard thresholding complexity than existing algorithms. Our empirical eval-
uations on real-world datasets and face recognition tasks demonstrate the superior
performance of our algorithms for sparsity-constrained optimization problems.
1 Introduction
In modern high-dimensional data analytics, where the variable dimension can be equal or even larg-
er than the number of samples, sparse representation learning has become a mainstream method to
explore the potential real model of the problem and provides statistically reliable results. It has been
applied to many diverse domains such as high-dimensional statistics (Buhlmann & Van De Geer,
2011), signal processing (Lai et al., 2013), and computer vision (Wright et al., 2008). Many sparse
learning algorithms Such as '1 -norm convex relaxation methods (Negahban et al., 2009; Van de Geer
et al., 2008) have been proposed in the past few decades. In order to get a smaller estimation er-
ror, `0 -norm constrained algorithms are more prominent than `1 -norm convex relaxation algorithms
(Zhang et al., 2010). In this paper, we mainly focus on the following sparsity-constrained optimiza-
tion problem:
1n
min F(W) = — Efi(W)	s.t., kw∣∣o ≤ s,	(1)
w ni=1
where F(W) is a finite-sum convex and smooth function, each function fi (W) is associated with
the i-th sample, kW k0 is the number of nonzero entries in variable W, and s represents the sparsity
level. The goal of sparse representation learning is to recover w* based on the given data. Such a
formulation encapsulates plentiful important problems, including sparse graphical model learning
(Zhou et al., 2019), sparse linear/logistic regression (Blumensath & Davies, 2009; Foucart, 2011;
Pati et al., 1993; Bahmani et al., 2013), and low-rank regression (Rohde et al., 2011).
However, Problem (1) is in general NP hard, which is caused by the non-convexity of the sparsity
constraint. It makes us to obtain an approximate solution to Problem (1). One of the most widely
used methods for solving this problem is the hard thresholding based gradient descent method.
In recent years, sparse representation learning is becoming more common for large-scale and high-
dimensional data. However, deterministic gradient descent hard thresholding algorithms such as fast
gradient descent hard thresholding (FG-HT) (Jain et al., 2014; Yuan et al., 2014) have to compute
the gradients of all n component functions, which leads to a huge computing overhead. To address
this issue, many stochastic hard thresholding methods have been proposed. For instance, Nguyen
et al. (2017) proposed a stochastic gradient descent hard thresholding (SG-HT) algorithm, whose
1
Under review as a conference paper at ICLR 2020
Table 1: Comparison of some properties of the hard thresholding algorithms for solving sparsity-constrained
problems. |B| is the mini-batch size of block coordinate descent methods, and k is the number of coordinate
blocks. Since the sparsity level S = Ω(κgs*) required in our algorithms (i.e., SBCD-HTP and its parallel
variant, ASBCD-HTP) is much smaller than those of other algorithms (e.g., Ω(K2s*)), the condition number
Kb in our algorithms is also smaller than Ke in other algorithms, where b= 2Ω(κgs*)+s* and Se= 2Ω(κes*)+s*.
I = SUPP(HT(VF(w*), 2s)) ∪ SUPP(W*) with S = Ω(κes*) is a support set.
Methods	Required value of S	Gradient Complexity	Hard Thresholdir Complexity	g	Statistical Error
FG-HT	Ω(κes*)	O(nKe log( ɪ))	O(Ke log©))	O(,Ω(Kes*) + s*kVF (w*)k∞)
SG-HT	ω(Kes*)	O(Ke log( ɪ))	O(Ke log( 1))	O( nl PLkVfi(w*)k2)
SVRG-HT	Ω(κes*)	O((n+Ke)log( ɪ))	O(Ke log( 1))	O(√ekVF (w*)k∞+kVι F (x*)k)
ASBCDHT	ω(Kes*)	O((n+ κeB )log( ɪ))	O(Ke log( 1))	O(√ekVF (w*)k∞)
SBCD-HTP	Ω(κbbs*)	O((n+ 平)log( ɪ))	O(log( 1))	O(√bkVF (w*)k∞)
ASBCD-HTP	Ω(Kgs*)	O((n + 贷)log( 1))	O(log( 1))	O(√bkVF (w*)k∞)
S2BCD-HTP	Ω(Kbbs*)	O((n + 赁)log( *))	O(log( 1))	O(√bkVF (w*)k∞)
gradient and hard thresholding complexities are both O(Kelog(ɪ)). However, due to the stochastic
sampling, SG-HT can only attain a sub-optimal estimation bound, as shown in Table 1, which is
inferior to those of deterministic gradient methods such as FG-HT. Another limitation of SG-HT is
that it requires that the restricted condition number κse should not be larger than 4/3, which makes
SG-HT hard for solving high-dimensional representation learning problems.
Recently, many stochastic variance reduced methods (e.g., SAG (Roux et al., 2012), SVRG (Johnson
& Zhang, 2013)) and their accelerated variants such as (Defazio, 2016; Allen-Zhu, 2018) have been
proposed to accelerate stochastic gradient methods for convex optimization. All these methods enjoy
low per-iteration complexities comparable with stochastic gradient descent (SGD), and they also can
attain improved convergence rates. By incorporating the variance reduction technique into sparsity-
constrained optimization domain, Li et al. (2016b) proposed a stochastic variance reduced gradient
hard thresholding (SVRG-HT) algorithm, which can converge more quickly and obtain a smaller es-
timation error than SG-HT. Moreover, SVRG-HT allows an arbitrarily large condition number in its
theoretical analysis similar to FG-HT (Yuan et al., 2014). The gradient oracle and hard thresholding
complexities of SVRG-HT are O((n+κe) log(ɪ)) and O(Kelog(ɪ)), respectively. Nevertheless, it
can not leverage the coordinate block to accelerate convergence. Chen & Gu (2016) proposed an
accelerated stochastic block coordinate gradient descent hard thresholding (ASBCDHT) algorith-
m. The gradient oracle complexity of ASBCDHT is O((n + KskBI) log(ɪ)), which is superior to
SVRG-HT. Moreover, ASBCDHT also has the hard thresholding complexity, O(Kelog(ɪ)). How-
ever, the hard thresholding complexity of ASBCDHT still scales linearly with Kse, which is usually
expensive for real-world sparse learning problems. For large-scale and high-dimensional data, Li
et al. (2016a) also proposed the asynchronous parallel variant of SVRG-HT (called ASVRG-HT) by
utilizing multicore architectures. Although it makes each processor to evaluate a stochastic gradient
update on a global parameter stored in a shared memory in an asynchronous and lock-free mode,
ASVRG-HT attains the similar gradient and hard thresholding complexities as its general version
(i.e., SVRG-HT). Therefore, all the algorithms mentioned above have a linearly Kse-dependent hard
thresholding complexity. This motivates us to address the following key issue:
Can we design such algorithms that have a Kse-independent hard thresholding complexity and
a lower gradient oracle complexity?
To answer the above problem, we propose an efficient Semi-stochastic Block Coordinate Descent
Hard Thresholding Pursuit (SBCD-HTP) algorithm and its sparse Asynchronous variant (ASBCD-
HTP). The oracle complexities and statistical estimation error of the proposed algorithms and other
hard thresholding methods are summarized in Table 1. We highlight several theoretical advantages
of the proposed algorithms over the state-of-the-art methods as follows:
• SBCD-HTP and ASBCD-HTP substantially improve the restricted condition on the sparsity level
s, i.e., S = Ω(κbbs*), while most related algorithms such as SVRG-HT require S = Ω(κes*).
2
Under review as a conference paper at ICLR 2020
•	The statistical estimation error of both our algorithms is better than those of FG-HT, SVRG-HT
and ASBCDHT, as our algorithms have a smaller sparsity level S = Ω(κgs*) and thus have a smaller
cardinality, i.e., b =2s + s*. This makes the restricted condition number Kb of SBCD-HTP smaller
than other algorithms. Moreover, the statistical error of our algorithms is also smaller than that of
SG-HT (i.e., n1 pn=1kVfi(w*)k) due to the large magnitude of kV∕i(w*)k (ZhoUetaL,2018b).
•	Both SBCD-HTP and ASBCD-HTP enjoy a κse-independent hard thresholding complexity, which
is significantly better than the state-of-the-art hard thresholding algorithms. That is, the hard thresh-
olding complexity of our algorithms is κse times lower than that of FG-HT, SG-HT, SVRG-HT and
ASBCDHT with e = 2Ω(κeS*) + s*. Since both our algorithms have a significantly lower hard
thresholding complexity, they are more suitable for handling large-scale sparse representation learn-
ing problems, especially for high-dimensional data.
•	For sparsity-constrained problems, the gradient oracle complexities of both SBCD-HTP and
ASBCD-HTP (or S2BCD-HTP) are much lower than those of the state-of-the-art hard threshold-
ing algorithms, and that of S2BCD-HTP is similar to ASBCD-HTP under conditions on the delay.
Actually, the gradient oracle complexity of S2BCD-HTP is better than SBCD-HTP when we deal
with sparse data sets, since S2BCD-HTP only evaluates the common nonzero point of the random
sample and the sampled block. Furthermore, their gradient oracle complexities are significantly
lower than those of ASBCDHT and SVRG-HT, and much lower than that of FG-HT, since they
require a smaller sparsity level S = Ω(κgs*) and also have a smaller value of Kb with b = 2s + s*
compared with the restricted condition number κse used in other hard thresholding algorithms (Zhou
et al., 2018b).
2	Related Work
In this section, we briefly discuss the relevant research to our work, which beyonds the sparsity-
constrained optimization domain. Traditional gradient descent is computational expensive at each
iteration, and stochastic gradient descent has a low per-iteration complexity while obtaining a large
variance in estimating. Thus, many stochastic variance reduced algorithms (Defazio et al., 2014;
Roux et al., 2012; Shalev-Shwartz & Zhang, 2013; Shang et al., 2018; Johnson & Zhang, 2013) and
their variants (Schmidt et al., 2017; Konecny et al., 2015; Xiao & Zhang, 2014; Hannah et al., 2018)
have been proposed. The stochastic variance reduced methods are very promising for many machine
learning problems including sparse learning problems.
In contrast to gradient descent methods, coordinate descent algorithms have received increasing at-
tention due to their successful applications in high dimensional problems (Breheny & Huang, 2011;
Friedman et al., 2007). Among them, randomized block coordinate descent (RBCD) updates a
block of coordinate with respect to the entire training instances. The per-iteration cost is significant-
ly lower than gradient descent, while it still has a relationship with n component functions. Some
stochastic block coordinate descent (SBCD) algorithms such as (Dang & Lan, 2015; Xu & Yin,
2015; Konecny et al., 2017) were proposed. Such algorithms compute the stochastic gradient with
respect to a random sample restricted to one randomized coordinate block. Therefore, these algo-
rithms randomly sample a block of features and data instances at each iteration. However, they can
only achieve a sublinear convergence rate (Zhang & Gu, 2016). Recently, some mini-batch random-
ized block coordinate descent algorithms such as (Zhao et al., 2014; Wang & Banerjee, 2014) were
proposed to accelerate the convergence of stochastic block coordinate gradient descent. Our work
studies over the above research by considering a sparsity-constrained optimization problem. The
proposed algorithms enjoy a lower computational complexity in both gradient evaluation and hard
thresholding computation while obtaining a linear convergence performance. Moreover, its sparse
and asynchronous variants are also proposed to deal with sparse high-dimensional data.
3	Preliminaries
Throughout this paper, we use w* to denote the optimal solution of Problem (1), and its optimal
sparsity level is s* with kw* k0 ≤ s*. kwk is the Euclidean norm for a vector w ∈ Rd and kwk∞ is
the largest absolute entry in w. The hard thresholding operation HT (w, s) preserves the s largest
entries ofw in magnitude for vector w and the rest entries are set to zero. We use supp(w) to denote
the support set of w, i.e., the indices of its nonzero entries. Given an index setI, we defineIC as the
complement set of I, and VI ∈ Rd, where [vɪ j = Vj if j ∈I and [vɪ j =0 if j /1. Given an integer
3
Under review as a conference paper at ICLR 2020
Algorithm 1 Semi-stochastic Block Coordinate Descent Hard Thresholding Pursuit (SBCD-HTP)
Input: Number of outer-loops R, number of inner-loops m, step size η, sparsity level s.
Initialize: we0 .
1: for r = 0, 1, . . . , R - 1 do
2：	w0 = W = wr, VF(泊)=1 Pi=1 vfi(te), G = supp(we);
3:	for t = 0, 1, . . . , m - 1 do
4:	Randomly sample a mini-batch B from [n] uniformly;
5:	Randomly sample a block jt from [k] uniformly, and S = G ∪ Gjt ;
6：	VSg(Wt) = |B| Pit∈B vSfit (Wt)- VSfit * * * * * * * * * (W) + VSF(W)；
7:	wt+1 = wt - ηvSg (wt);
8： end for
9：	Wer+1 = HT(Wm,s)；
10： end for
Output: WeR.
n ≥ 1, we define [n] = {1, ∙∙∙ , n}. For a set B, We denote its cardinality by |B|. Moreover, We use
the common notations of Ω(∙) and O(∙) to characterize the asymptotics of two real sequences.
Throughout the analysis, We make tWo important assumptions on the objective function, Which are
commonly used in the analysis of hard thresholding algorithms (Li et al., 2016b； Shen & Li, 2017；
Chen & Gu, 2016； Gao & Huang, 2018). In high-dimensional sparse learning, the per-iteration hard
thresholding operation can be time-consuming or even more expensive than gradient computation.
Thus, we also take the complexity of hard thresholding into our consideration.
Assumption 1 (Restricted Strong Convexity) A differentiable function F(∙) is restricted P--
strongly convex at sparsity level sb, if there exists a uniform constant ρsb- > 0 such that for any
W, W0 ∈ Rd with kW - W0 k0 ≤ sb, we have
F(W) - F(w0) - hVF(w0), w - W0i ≥ P-∣∣w - w0『.	(2)
Assumption 2 (Restricted Strong Smoothness) For any i ∈ [n], the differentiable function fi(∙)
is restricted Psb+-strongly smooth at sparsity level sb, if there exists a uniform constant Psb+ > 0 such
that for any W, W0 ∈ Rd with ∣W - W0 ∣0 ≤ sb, we have
fi(W) - fi(W、-hVfi(WD, W 一 W0i≤ ρb∣∣w - W0k2.	⑶
Moreover, the restricted condition number is defined as： κsb= Ps+b /Psb-. Since the complexity of one
hard thresholding operation is linear with d, which is similar to a computational cost of a stochastic
gradient, we define the hard thresholding complexity as a metric for hard thresholding algorithms.
Definition 1 (Hard Thresholding Complexity) In a hard thresholding operation, a vector W is fed
into HT(∙, S) and then the output HT(w, S) is obtained.
Both gradient oracle and hard thresholding complexities can more comprehensively reflect the over-
all computational cost of first-order hard thresholding algorithms. Therefore, their per-iteration cost
is dominated by both the gradient evaluation and hard thresholding operations.
We also define the optimization error and statistical error to clearly understand our theoretical results.
Definition 2 (Optimization Error and Statistical Error) The optimization error is the difference
which will decrease with the increasing of iterations. A statistical error is the (unknown) difference
between the retained value and the true value.
This means that the optimization error will decrease to zero when the number of iteration is quite
large. However, a statistical error will still work during the optimized process. Therefore, the larger
the statistical error, the more difference to the true value.
4
Under review as a conference paper at ICLR 2020
4	Semi-Stochastic Block Coordinate Descent Hard
Thresholding Pursuit
In this section, we propose a novel Semi-stochastic Block Coordinate Descent Hard Thresholding
Pursuit (SBCD-HTP) algorithm, and also theoretically analyze its convergence properties.
4.1	OUR SBCD-HTP ALGORITHM
The proposed SBCD-HTP algorithm is summarized in Algorithm 1. In each outer-loop, we select
a snapshot point W and compute its full gradient VF (滴).In each inner-loop, We uniformly and
randomly select a mini-batch samples B and a block of coordinates Gjt, where jt ∈ {1, 2, . . . , k},
{G1 . . . Gk } is a partition of all the d coordinates and is divided uniformly at random. We usually set
k = 10 in our experiments. Note that, different from ASBCDHT, in line 5 We set S to be a union set
of the support set of snapshot point we and Gjt. In this Way, the coordinate of the support set S Which
includes the possibly optimal coordinates can be updated, and thus this can reduce the error caused
by randomness. Moreover, SBCD-HTP enjoys the dual advantages of randomized block coordinate
descent and semi-stochastic gradient descent to optimize the objective as shoWn in line 6. Note
that We perform the hard thresholding operation in only the outer-loop, Which is different from all
existing hard thresholding algorithms. Performing hard thresholding operations With high frequency
or prematurely in inner-loop Will make the gradient information lost, Which is the main reason for
sloW convergence and loW accuracy of existing hard thresholding sparsity-constrained algorithms.
Our hard thresholding algorithm not only guarantees the sparsity of model parameters, but also uses
the full dimensional information Without the interference of hard thresholding operations to update
model parameter wt+1. Our theoretical analysis and experimental results shoW that SBCD-HTP has
a faster convergence rate and more accurate results.
We have to clarify the difference betWeen the proposed SBCD-HTP and ASBCDHT in Chen &
Gu (2016) for clear improvements. There are mainly tWo changes: a) We design a bigger support
set With the union of the sampled block Gjt With the one of the current support of the snapshot
point we, since the current support of snapshot point contain the most related optimized information,
While that of Chen & Gu (2016) only use the sampled block. b) We move the hard thresholding
operation outside of the inner-loop, since the truncated function (hard thresholding) may loss many
information and bringing higher complexity. Moreover, the asynchronous variant of SBCD-HTP is
also proposed in this paper to optimize the sparse problem 1 in the sparse data. This is the first to
design this type of algorithm in sparse learning field, and the theoretical proof is also provided.
4.2	Convergence Analysis
We first analyze the convergence behavior of SBCD-HTP. The main result is summarized in the
folloWing theorem, Whose proof is provided in Section C.
Theorem 1 Suppose F(w) is ρsb--strongly convex and each function fi(w) is ρsb+-strongly smooth
with parameter b =2s + s*. Let Kb = ρ- and the sparsity level S ≥ Ω(κgs*). In addition, assume
ρsb
that the learning rate η ≤ 481+, the number of inner-loops m ≥ 1800κg, and the number of blocks
ρsb
k = 10, then the convergence rate Y =  ---kɑ—+ + 8pb；(：/"；) ≤ 1. For the sparsity-constrained
2ηmρsb (1-ω)	-ω n-
problem (1), the output of Algorithm 1 satisfies
E[F (Wer) - F (w*)] ≤ (1)r E[F (we0) - F (W*)] + ɪ kVι F (w*)∣∣2,	(4)
2	1-ω
where I = SuPP(HT(VF(w*), 2s)) ∪ SuPP(w*) and ω = 8ρ+η(1 + 面—|B1)).
Theorem 1 shoWs that SBCD-HTP enjoys a linear convergence rate for solving sparsity-constrained
problem (1). In order to ensure that the linear converging term in Eq. (4) satisfies (2)rE[F(WO) 一
F(W*)] ≤ , We can obtain the folloWing corollary.
Corollary 1 Suppose the conditions in Theorem 1 hold. To achieve (2)rE[F(W0) — F(w*)] ≤ G
the gradient oracle complexity OfSBCD-HTP is O((n + KbB|) log(1)), and its hard thresholding
COmPlexity is O(log(ɪ)).
5
Under review as a conference paper at ICLR 2020
Remark 1 This corollary is inferred under the high-dimensional condition. Actually, the worst
condition of the gradient oracle complexity of SBCD-HTP is O((n + κg∣B∣k+s )log( ɪ)). Due to
the small sparsity level s required in our Theorem 1 and the overlaps between Gjt and G, we can
obtain the gradient oracle complexity O((n+ KkBI) log(ɪ)) for SBCD-HTP
Table 1 summarizes the properties of some hard thresholding methods. Compared with the gra-
dient oracle complexities of FG-HT (Yuan et al., 2014) and SVRG-HT (Li et al., 2016b) (i.e.,
O(nκe log( ɪ)) and O((n+κe) log( ɪ )),resPectively), SBCD-HTP has amuch lower oracle complex-
ity. For ASBCDHT (Chen & Gu, 2016), whose gradient oracle complexity is O((n+ KkBI) log(ɪ)),
the gradient oracle complexity of SBCD-HTP is O((n+ KbBI) log(ɪ)) and is much lower than AS-
BCDHT, as κsb of SBCD-HTP is usually comparable to or even smaller than those of others (note that
S in b= 2s+s* is required to be smaller than others as shown below). Moreover, SBCD-HTP allows
S = Ω(κgs*), which is considerably superior to the condition of S = Ω(κeS*) required in other hard
thresholding algorithms such as (Zhou et al., 2018b). In particular, with the same parameter settings
of |B| and k, the gradient oracle complexity of our algorithm outperforms ASBCDHT. SBCD-HTP
also attains a lower hard thresholding complexity than the state-of-the-art hard thresholding method-
s. That is, the hard thresholding complexity of SBCD-HTP is O(log(ɪ)), which is κe-independent
and is significantly lower than those of other methods. This is a significant improvement on hard
thresholding complexity. Theorem 1 immediately implies the following results.
Corollary 2 Suppose the conditions in Theorem 1 hold. The output of Algorithm 1 satisfies
Ekwr- w*k ≤ S2( 1 )r[F(W01 - F(w*)] + (二 + τ^η-) √1∣VF(w*)k∞.	(5)
ρs-b	ρsb-	1 - ω
The right-hand side of Eq. (5) consists of two terms. The first term is the optimization error, which
approaches zero with the increase of r. The second term corresponds to the statistical error, which
is proportional to √b∣∣VF(w*)k∞. One can also observe that the statistical error bound is better
than those of FG-HT and ASBCDHT, since b in the statistical error O(√bkVF(W)k∞) has a
smaller cardinality (i.e., 2Ω(κgs*) + s*), while existing algorithms require a larger sparsity level
s0 = Ω(κes*) and have a larger cardinality. Therefore, compared with other existing algorithms,
SBCD-HTP enjoys a smaller statistical error, especially for the problems with a large restricted
condition number. It is usually better than the error bound O(√e∣∣VF(W)k∞ + ∣∣V∣F(W)k) with
e=2Ω(κeS*)+s* in SVRG-HT. Moreover, the error bound of SBCD-HTP is much smaller than that
of SG-HT (Nguyen et al., 2017) (i.e., O(*Pn=IkVfi (w*)k)), since the magnitude of the individual
gradient norm ∣∣Vfn(w*)k can still be relatively large (Zhou et al., 2018b).
5	Asynchronous Variant of SBCD-HTP
In this section, we propose the serial sparse and asynchronous parallel variants of SBCD-HTP for
high-dimensional sparse data sets. Our Asynchronous Sparse Semi-stochastic Block Coordinate
Descent Hard Thresholding Pursuit (ASBCD-HTP) algorithm is summarized in Algorithm 2. The
main difference between ASBCD-HTP and SBCD-HTP is sparse approximate gradients as in (Ma-
nia et al., 2017; Zhou et al., 2018a). In order to perform fully sparse updates, we use a diagonal
matrix D to re-weigh the dense fully gradient VF (We). The entries ofD are the inverse probabilities
of the corresponding coordinates belonging to a uniformly sampled support Tit of sample it . Let
Pit be the projection matrix for the support Tit, we can get Dit since Dit = Pit D. Then we can
find that Eit[DitVF (We)]=VF (We). In this setting, we can compute the full gradient VF (We) also in
a parallel way. In order to reduce thread interference and fully utilize the sparsity of datasets, we
restrict the sparse gradient Vg([Wb]Ti ) to the intersection set of S and Tit and obtain VSg([Wb]Ti ).
Specifically, each thread of ASBCD-HTP computes sparse approximate gradient independently and
then atomically write this gradient to the shared variable Wer . Note that Sparse variant of SBCD-
HTP (S2BCD-HTP) can be viewed as the single-thread version of ASBCD-HTP, which has linear
convergence and the same oracle complexity as ASBCD-HTP. The detailed description and analysis
of S2BCD-HTP is listed in Section D. Next we give the main theoretical result of ASBCD-HTP.
6
Under review as a conference paper at ICLR 2020
Algorithm 2 Asynchronous Sparse SBCD-HTP (ASBCD-HTP)
Input: The number of outer-loops R, number of inner-loops m, step size η, sparsity level s.
Initialize: we0 .
1:	for r = 0, 1, . . . , R - 1 do
2:	w0 = W = Wr, G = supp(^e), and VF(W) = nPn=IWi(W); //computed in parallel
3:	t = 0; //inner loop counter
4:	compute the while loop in parallel
5:	while t < m do
6:	Randomly sample it from [n] uniformly, and randomly sample jt from [k] uniformly;
7:	Tit :=support of sample it, and t = t + 1; //atomic increase counter t
8:	S = G ∪ Gjt, [Wb]Ti := inconsistent read of shared variable [Wer]Ti ;
9:	VSg([Wb]Tit) = VS fit ([Wb]Tit) - VS fit ([We]Tit) + Dit VS F (We);
10:	[Wer]S T Tit = [Wer]STTit - η VS g ([Wb]Tit); //atomic write
11:	end while
12:	Wer+1 = HT (Wer , s);
13:	end for
Output: WeR.
Theorem 2 Suppose F(W) is ρsb--strongly convex and each function fi(W) is ρsb+-strongly smooth
ρ+
with parameter S = 2s + s*. Let Kb = ɪ and the Sparsity level S ≥ Ω(κgs*). We assume that
ρsb
the step size η = 601+, the number of inner-loops m ≥ 120κg, the number of blocks k = 10 and
T ≤ min{5√√∆, 2κb {√b} (i.e., the linear speedup condition). Thenfor the sparsity-constrained
problem (1), the gradient oracle and hard thresholding complexities of Algorithm 2 are
O((n + κb∕k)log(1∕e)) and O(log(1∕c)),	(6)
where τ denotes the maximum number of concurrent threads (Mania et al., 2017) and ∆ =
maxj=ι∙∙∙d Pj, which is a indicator to measure the sparsity of datasets (Leblond et al., 2017).
Theorem 2 implies that ASBCD-HTP (or S2BCD-HTP) can still obtain similar gradient oracle com-
plexity and hard thresholding complexity as SBCD-HTP while obtaining a linear speedup ratio.
Compared with ASVRG-HT (Li et al., 2016a), ASBCD-HTP has a much lower gradient complexity,
i.e., O((n+κkb)log ɪ) for ASBCD-HTP vs. O((n+κe) log ɪ) for ASVRG-HT. For hard thresholding
complexity, ASBCD-HTP is κse times lower than ASVRG-HT, which is a significant improvement
in the asynchronous setting. Since ASVRG-HT has to compute hard thresholding operations at each
inner-loop, the practical performance may be worse, especially for high-dimensional data. We also
show this improvement by our empirical evaluations.
6 Experiments
In this section, we evaluate the performance of SBCD-HTP, S2BCD-HTP and ASBCD-HTP for
solving sparse linear regression and sparse logistic regression on real-world datasets. All the dense
algorithms were implemented in MATLAB, while the sparse and asynchronous algorithms were
implemented in C++ and executed through MATLAB interface for a fair comparison. We also
test the performance for face recognition tasks. The detailed information is described in Section
F. Since there is no ground truth on real-world datasets, we run all these baselines sufficiently long
until kWr - Wr+1k∕kWr k ≤ 10-6, and then we use the minimum F(Wr) as the approximate optimal
value F * for sub-optimality estimation in our experimental results Zhou et al. (2018b).
6.1	Dense Algorithms
We ran the experiments of all the dense algorithms on a PC with an Intel i7-7700 CPU and 32GB
RAM. We compare SBCD-HTP with several state-of-the-art sparsity-constrained algorithms, in-
cluding FG-HT (Yuan et al., 2014), SG-HT (Nguyen et al., 2017), SVRG-HT (Li et al., 2016b),
ASBCDHT (Chen & Gu, 2016) and FNHTP (Chen & Gu, 2017). The detailed descriptions of the
7
Under review as a conference paper at ICLR 2020
ʤ e≥≡aqo 6o^∣
de。e≥≡aqo 6o^∣
------FG-HT
-6 -SG-HT
SVRG-HT
ASBCDHT
FNHTP
SBCD-HTP
de。e≥≡aqo 601
de。e≥≡aqo 601
0	10	20	30	40	50	60	0	50	100	1 50	200	250
Number of Effective Passes	CPU Time ⑸
(a) news20
0	10	20	30	40	50	60	0	200	400	600
Number of Effective Passes	CPU Time ⑸
(b) rcv1-train
Figure 1:	Comparison of all the hard thresholding algorithms for solving sparse logistic regression problems. In
each plot, the vertical axis shows the objective value minus the minimum, and the horizontal axis is the number
of effective passes over data (left) or running time (seconds, right).
ʤ e≥≡2qo 601
0	10	20	30	40	50	60
Number of Effective Passes
de°e≥≡2qo 6o^∣
------FG-HT
-SG SG-HT
-∙B- -SVRG-HT
A ASBCDHT
-O- ∙FNHTP
-V -SBCD-HTP
------FG-HT
SG-HT
SVRG-HT
ASBCDHT
FNHTP
SBCD-HTP
0	100	200	300	400
CPU Time (s)
de°e≥≡2qo 6o^∣
0	10	20	30	40	50	60	0
Number of Effective Passes
de°e≥≡2qo 6o^∣
200	400
CPU Time (s)
600
(a) news20
(b) rcv1-train
Figure 2:	Comparison of all the hard thresholding algorithms for solving sparse linear regression problems.
algorithms and datasets are provided in Section F. We set the sparsity level s = 200 for real-word
datasets. For the number of inner-loops, we set m = 2n for SBCD-HTP and ASBCDHT, m = n
for SVRG-HT as suggested in (Li et al., 2016b; Chen & Gu, 2016) and 1000 (sparse linear regres-
sion)/3000 (sparse logistic regression) for FNHTP. We also set the batch-size of FNHTP to s, and
that of ASBCDHT and SBCD-HTP to 5. For the block coordinate descent methods, we set the
number of coordinate blocks k to 10. All the algorithms are tuned to their best performance.
We report the performance of all the algorithms in terms of both effective passes and CPU time.
More experimental results are presented in Section F. Figures 1 and 2 show that for both sparse linear
regression and sparse logistic regression problems, our algorithm converges significantly faster than
the baseline algorithms in terms of both effective passes and CPU time. The main reason is that we
use all the information of gradients, while the related algorithms use less information caused by too
frequent and premature hard thresholding operation in each inner-loop.
6.2 Asynchronous Algorithms
We ran the experiments of all the asynchronous algorithms on a PC with an Intel Xeon(R) Gold
5120 CPU and 64GB RAM. We compare our sparse and asynchronous algorithms with several state-
of-the-art algorithms, including ASG-HT, ASVRG-HT and A2SBCD-HT (asynchronous variant of
ABSCDHT). For the algorithms, we set the number of threads to 20.
We only report the performance of all the algorithms in terms of CPU time since the performance
in terms of effective passes is similar to the dense case. More experimental results are reported in
Section F. Figure 3 shows that ASBCD-HTP and S2BCD-HTP significantly outperform than other
algorithms. Moreover, ASBCD-HTP has a faster convergence performance than S2BCD-HTP be-
cause of asynchronous parallel acceleration. There are three main reasons for the advantages of our
ASBCD-HTP: a) We get the set S from the support set of snapshot point we and Gjt . In this way,
the coordinate of the support set which appears as the possible optimal solution can be updated, and
thus reducing the error caused by randomness. b) Because we put the hard thresholding operation
in each outer-loop, the computation cost per thread of our algorithm at each inner-loop is less, while
each thread of the baseline algorithms has a hard thresholding operation, which is expensive espe-
cially for high-dimensional datasets. c) Our algorithm updates wer with all information of gradients,
while the baseline algorithms update wer with less information caused by too frequent and premature
hard thresholding operation in each inner-loop. We also evaluate the improvement of asynchronous
parallel by running the same passes with different numbers of threads. We calculate the speed-up
8
Under review as a conference paper at ICLR 2020
0	12	3	4
CPU Time (S)
ʤ e≥≡2qo 601
-I-A2SBCD-HT
--¢- -ASG-HT
--π- -ASVRG-HT
--O--S2BCD-HTP
--ASBCD-HTp
CPU Time (S)
ʤ e≥≡aqo 6o^∣
-I-A2SBCD-HT
-6 -ASG-HT
--π- -ASVRG-HT
--O--S2BCD-HTP
-T- -ASBCD-HTp
20	40	60
CPU Time (S)
(a) real-sim	(b) news20	(c) rcv1-train	(d) rcv1-test
Figure 3: Comparison of ASG-HT, ASVRG-HT, A2SBCD-HT, S2BCD-HTP and ASBCD-HTP for solving
sparse logistic regression problems. The four asynchronous parallel algorithms (i.e., ASG-HT, ASVRG-HT,
A2SBCD-HT and ASBCD-HTP) run on 20 threads.
ʤ e≥≡2qo 601
0	5	10	15	20	25	30
CPU Time ⑸
0-s≈dn,peeds
0	5	10	15	20	25
Number of Threads
-th- thread = 1
----H- - thread = 4
--O- - thread = 8
--T- - thread = 16
de°e≥≡2qo 6o^∣
0	0,5	1	1.5	2	2.5	3
CPU Time (S)
0≡≈dnieeds
0	5	10	15	20	25
Number of Threads
(a) rcv1-test	(b) real-sim
Figure 4: Speedup evaluation on rcv1-test and real-sim. Left: Evaluation of objective function gap in terms of
CPU time for ASBCD-HTP with different threads. Right: Speedup ratio with respect to the number of threads.
ratio based on the running time of a single thread. From Figure 4, we can see that our ASBCD-HTP
is accelerated by a nearly linear ratio.
7 Conclusion
In this paper, we proposed an efficient semi-stochastic block coordinate descent hard thresholding
pursuit (SBCD-HTP) method for sparse representation learning. We proved that SBCD-HTP attains
the gradient oracle complexity of O((n + KbB|) log(ɪ)), and the hard thresholding complexity of
O(log( 1)), respectively, which is an improvement over the existing algorithms. Moreover, we also
presented the sparse and asynchronous parallel variants of SBCD-HTP. As far as we know, our
algorithms are the first to use the full information of gradient to update. This is an improvement on
sparse representation learning because it can be easily extended to other sparse learning algorithms.
References
Z. Allen-Zhu. Katyusha: The first direct acceleration of stochastic gradient methods. JMLR, 18
(221):1-51,2018.
Sohail Bahmani, Bhiksha Raj, and Petros T. Boufounos. Greedy sparsity-constrained optimization.
JMLR, 14(1):807-841, 2013.
Thomas Blumensath and Mike E Davies. Iterative hard thresholding for compressed sensing. Appl.
Comput. Harmon. Anal., 27(3):265-274, 2009.
Patrick Breheny and Jian Huang. Coordinate descent algorithms for nonconvex penalized regression,
with applications to biological feature selection. The annals of applied statistics, 5(1):232, 2011.
Peter Buhlmann and Sara Van De Geer. Statistics for high-dimensional data: methods, theory and
applications. Springer Science & Business Media, 2011.
Jinghui Chen and Quanquan Gu. Accelerated stochastic block coordinate gradient descent for spar-
sity constrained nonconvex optimization. In UAI, pp. 132-141, 2016.
Jinghui Chen and Quanquan Gu. Fast Newton hard thresholding pursuit for sparsity constrained
nonconvex optimization. In SIGKDD, pp. 757-766, 2017.
9
Under review as a conference paper at ICLR 2020
Cong D Dang and Guanghui Lan. Stochastic block mirror descent methods for nonsmooth and
stochastic optimization. SIAM Journal on Optimization, 25(2):856-881, 2015.
A. Defazio. A simple practical accelerated method for finite sums. In NIPS, pp. 676-684, 2016.
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method
with support for non-strongly convex composite objectives. In Advances in neural information
processing systems, pp. 1646-1654, 2014.
Simon Foucart. Hard thresholding pursuit: An algorithm for compressive sensing. SIAM J. Numer.
Anal., 49(6):2543-2563, 2011.
Jerome Friedman, Trevor Hastie, Holger Hofling, Robert Tibshirani, et al. Pathwise coordinate
optimization. The annals of applied statistics, 1(2):302-332, 2007.
Hongchang Gao and Heng Huang. Stochastic second-order method for large-scale nonconvex sparse
learning models. In IJCAI, pp. 2128-2134, 2018.
Athinodoros S. Georghiades, Peter N. Belhumeur, and David J. Kriegman. From few to many:
Illumination cone models for face recognition under variable lighting and pose. IEEE Trans.
Pattern Anal. Mach. Intell., 23:643-660, 2001.
Robert Hannah, Fei Feng, and Wotao Yin. A2bcd: Asynchronous acceleration with optimal com-
plexity. 2018.
Prateek Jain, Ambuj Tewari, and Purushottam Kar. On iterative hard thresholding methods for high-
dimensional m-estimation. In NIPS, pp. 685-693, 2014.
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance
reduction. In NIPS, pp. 315-323, 2013.
Jakub Konecny, Jie Liu, Peter Richtarik, and Martin Takac. Mini-batch semi-stochastic gradient
descent in the proximal setting. IEEE Journal of Selected Topics in Signal Processing, 10(2):
242-255, 2015.
Jakub Konecny, Zheng Qu, and Peter Richtarik. Semi-stochastic coordinate descent. optimization
Methods and Software, 32(5):993-1005, 2017.
Ming-Jun Lai, Yangyang Xu, and Wotao Yin. Improved iteratively reweighted least squares for
unconstrained smoothed \ell_q minimization. SIAM Journal on Numerical Analysis, 51(2):927-
957, 2013.
Remi Leblond, Fabian Pedregosa, and Simon Lacoste-Julien. ASAGA: asynchronous parallel
SAGA. In AISTATS, pp. 46-54, 2017.
Xingguo Li, Raman Arora, Han Liu, Jarvis Haupt, and Tuo Zhao. Nonconvex sparse learning via
stochastic optimization with progressive variance reduction. arXiv preprint arXiv:1605.02711,
2016a.
Xingguo Li, Tuo Zhao, Raman Arora, Han Liu, and Jarvis Haupt. Stochastic variance reduced
optimization for nonconvex sparse learning. In ICML, pp. 917-925, 2016b.
Horia Mania, Xinghao Pan, Dimitris Papailiopoulos, Benjamin Recht, Kannan Ramchandran, and
Michael I Jordan. Perturbed iterate analysis for asynchronous stochastic optimization. SIAM J.
Optim., 27(4):2202-2229, 2017.
Sahand Negahban, Bin Yu, Martin J Wainwright, and Pradeep K Ravikumar. A unified framework
for high-dimensional analysis of m-estimators with decomposable regularizers. In NIPS, pp.
1348-1356, 2009.
Yurii Nesterov. Introductory Lectures on Convex Optimization - A Basic Course, volume 87 of
Applied Optimization. Springer, 2004.
Nam Nguyen, Deanna Needell, and Tina Woolf. Linear convergence of stochastic iterative greedy
algorithms with sparse constraints. IEEE T. Inform. Theory., 63(11):6869-6895, 2017.
10
Under review as a conference paper at ICLR 2020
Yagyensh Chandra Pati, Ramin Rezaiifar, and Perinkulam Sambamurthy Krishnaprasad. Orthogonal
matching pursuit: Recursive function approximation with applications to wavelet decomposition.
In Proc. Conf. Signals, Systems and Computers,pp. 40-44, 1993.
Angelika Rohde, Alexandre B Tsybakov, et al. Estimation of high-dimensional low-rank matrices.
Ann. Statist., 39(2):887-930, 2011.
Nicolas Le Roux, Mark W. Schmidt, and Francis R. Bach. A stochastic gradient method with an
exponential convergence rate for finite training sets. In NIPS, pp. 2672-2680, 2012.
Mark Schmidt, Nicolas Le Roux, and Francis Bach. Minimizing finite sums with the stochastic
average gradient. Mathematical Programming, 162(1-2):83-112, 2017.
Shai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for regularized
loss minimization. Journal of Machine Learning Research, 14(Feb):567-599, 2013.
Fanhua Shang, Kaiwen Zhou, James Cheng, Ivor W. Tsang, Lijun Zhang, and Dacheng Tao.
VR-SGD: A simple stochastic variance reduction method for machine learning. CoRR, ab-
s/1802.09932, 2018.
Jie Shen and Ping Li. A tight bound of hard thresholding. JMLR, 18(1):7650-7691, 2017.
Sara A Van de Geer et al. High-dimensional generalized linear models and the lasso. Ann. Statist.,
36(2):614-645, 2008.
Huahua Wang and Arindam Banerjee. Randomized block coordinate descent for online and stochas-
tic optimization. CoRR, abs/1407.0107, 2014.
John Wright, Allen Y Yang, Arvind Ganesh, S Shankar Sastry, and Yi Ma. Robust face recognition
via sparse representation. IEEE transactions on pattern analysis and machine intelligence, 31(2):
210-227, 2008.
Lin Xiao and Tong Zhang. A proximal stochastic gradient method with progressive variance reduc-
tion. SIAM Journal on Optimization, 24(4):2057-2075, 2014.
Yangyang Xu and Wotao Yin. Block stochastic gradient iteration for convex and nonconvex opti-
mization. SIAM Journal on Optimization, 25(3):1686-1716, 2015.
Xiaotong Yuan, Ping Li, and Tong Zhang. Gradient hard thresholding pursuit for sparsity-
constrained optimization. In ICML, pp. 127-135, 2014.
Aston Zhang and Quanquan Gu. Accelerated stochastic block coordinate descent with optimal
sampling. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining, pp. 2035-2044. ACM, 2016.
Cun-Hui Zhang et al. Nearly unbiased variable selection under minimax concave penalty. Ann.
Statist., 38(2):894-942, 2010.
Tuo Zhao, Mo Yu, Yiming Wang, Raman Arora, and Han Liu. Accelerated mini-batch randomized
block coordinate descent method. In NIPS, pp. 3329-3337, 2014.
Shuai Zheng and James T. Kwok. Fast-and-light stochastic ADMM. In Proceedings of the Twenty-
Fifth International Joint Conference on Artificial Intelligence, IJCAI 2016, New York, NY, USA,
9-15 July 2016, pp. 2407-2613, 2016.
Baojian Zhou, Feng Chen, and Yiming Ying. Stochastic iterative hard thresholding for graph-
structured sparsity optimization. In ICML, pp. 7563-7573, 2019.
Kaiwen Zhou, Fanhua Shang, and James Cheng. A simple stochastic variance reduced algorithm
with fast convergence rates. In ICML, pp. 5980-5989, 2018a.
Pan Zhou, Xiaotong Yuan, and Jiashi Feng. Efficient stochastic gradient hard thresholding. In
NeurIPS, pp. 1988-1997, 2018b.
11
Under review as a conference paper at ICLR 2020
Appendix
A Key Lemmas
Lemma 1 ((Jain et al.,2014)) For any index set I, any W ∈ RI and HT(∙, s) : Rd → Rd be
the hard thresholding operator, which keeps the largest s entries (in magnitude) and sets the other
entries equal to zero. Thenfor any w* ∈ RI such that ∣∣w*ko ≤ s*, we have
kHT(w,s)- wk2 ≤^4kw — w*k2.
|I| - s*
Lemma 2 ((Li et al., 2016b)) Let w* ∈ Rd be the optimal sparse vector such that kw* k0 ≤ s*
and HT(∙, s) : Rd → Rd be the hard thresholding operator, which keeps the largest S entries (in
magnitude) and sets the other entries equal to zero. Given s > s* for any vector w ∈ Rd, we have
kHT(w, S)- w*k2 ≤ (1 + -2s== )∣w - w*k2.
Lemma 3 ((Nesterov, 2004)) For any given vector w, w0 ∈ Rd, suppose F(∙) also satisfies RSC
and RSS conditions, then the following inequality holds
∣∣VF(W) - VF(w0)k2 ≤ 2ρ+[F(W) - F(WD + BF(W0),w0 — w〉].
Lemma 4 Suppose F(w) is ρsb--strongly convex and each function fi(w) is ρsb+-strongly smooth with
parameter sb = 2s + s*. I = supp(w*) ∪ supp(wer) ∪ supp(wer+1), |B| is the batch size. For any
wt ∈ Rd and the sample it ∈ B, denote νit = (VI fit (wt) -VI fit (w*)) -(VI F (wt) -VI F (w*)),
we have
∣⅛ XBVit
E
2
=5 EkVI fi(Wt)- vI fi(w*)+vIF (w*)- vi F (Wt)k2
(7)
Proof.	See Section B.1 for the proof of Lemma 4.

Lemma 5 Suppose F(w) is ρsb--strongly convex and each function fi (w) is ρsb+-strongly smooth
with parameter sb = 2s + s*. Let w* ∈ Rd be the optimal sparse vector with kw* k0 ≤ s*.
I = supp(w*) ∪ supp(wer) ∪ supp(wer+1), δ > 1 is a uniform constant factor, and |B| is the batch
size. Forany wt, W ∈ Rd and the sample it, denote Vsg(wt) = ∣BB∣ Pit∈B Vsfit (Wt)-VS fit(W) +
VS F (we), then we can bound EkVS gI (wt)k2 as follows:
EkVSgI(wt)k2 ≤ 1(16δρ+(1 + W TBI )E[F(wt) - F(w*)]+4∣∣VIF(w*)∣∣2
k s	|B|(n - 1)
+ 16δρ+ IBnn %E[F(W)- F(W*)]),
Proof. See Section B.2 for the detailed proof of Lemma 5.
Lemma 6 Suppose F(w) is ρsb--strongly convex and each function fi (w) is ρsb+-strongly smooth
with parameter sb = 2s + s*. Let w* ∈ Rd be the optimal sparse vector with kw* k0 ≤ s*. I =
SuPP(W*)∪supp(Wr )∪supp(泊r+1), δ > 1 is a uniform constantfactor, and Dm = maxj=ι,…，d p7.
For any wt, wW ∈ Rd and the sample it, denote Vs g(wt) = Vs fit (wt) - Vs fit (wW) + Dit Vs F (wW),
we can bound EkVs gI (wt)k2 as follows:
EkVSgI(wt)k2 ≤ 1(12δρ+[F(wt) - F(w*)] + (3 + 12(Dm - 1))∣∣VIF(w*)∣∣2
k
+ (24δρsb+ + 48δρsb+(Dm2 - 1))[F (wW) - F(w*)]),
where pj is the probability that dimension j belonging to the support set of randomly sampling
sample it.
12
Under review as a conference paper at ICLR 2020
Proof. See Section B.3 for the proof of Lemma 6.
This lemma is different from other variance reduced results for sparsity-constrained problem (Mania
et al., 2017; Johnson & Zhang, 2013; Chen & Gu, 2016; Shang et al., 2018). Since this variance
reduced approximate gradients introduce an approximate full gradient to make fully sparse update
in cardinality constraint problem.
B Proofs of Key Lemmas
In this section, we prove three key lemmas.
B.1 Proof of Lemma 4
Proof.	Let Vit = (Vιfit(Wt)- Vifit(w*)) - (VιF(Wt)- ViF(w*)). Wehave
E
2
1>
=∣B∣2 E	工	VitVit
it,i0t∈B
∣B∣2 E X ν> νit + |B| EkVik2
it6=i0t∈B
|B| - 1	>	1	2
|B|n(n - 1) X ViVi0+ |B| EkVik
MBI- I、X V>Vio - /B1- 1 EkVik2 + ɪEkVik2
|B|n(n - 1) i,i0 i |B|(n - 1)	|B|
高(TBI)EkVik2.
|B|(n - 1)
This proof is similar to that of (Zheng & Kwok, 2016).
B.2 Proof of Lemma 5
Proof. It is obvious that the stochastic variance reduced gradient satisfies
E[Vg(wt)] = E 焉 X (Vfit (Wt)-Vfit (W))+ μ = VF (wt).	(12)
|B| it∈B
Thus, Vg(Wt) is an unbiased estimator of the full gradient VF(Wt). Now, we bound EkVgi (Wt)k2.
For any i ∈ {1,2,…,n}, We define the following function
hi(w) = fi(w) - fi(w*) - hVfi(w*),w - w)	(13)
It is easy to find that Vhi(w*) = 0, which implies that hi (w*) = minw hi(w). For any vector w,
we have
0 = hi(w*) ≤ minhi(w — ηVιhi(w))
η
2+
≤ min hi(w) - ηhVhi(w), VIhi(w)i +-^b∣∣Vihi(w)k2
η2
η2ρ+	(14)
=minhi(w) - η∣∣Vzhi(w)∣2 + -ʌʃb∣∣Vihi(w)∣2
η2
=hi(W) - /+ IIvihi(W)k2,
2ρsb+
where the second inequality follows from the RSS condition, and the second equality holds due to
the fact that hVhi (W), Vi hi(W)i = kVi hi(W)k2, and the last equality holds due to the fact that
η =卡 minimizes the function. Then, we have
kVιfi(W) - Vifi(W*)k2 ≤ 2ρ+[fi(W) - f3 -NIfi(W*),W - w*〉].	(15)
13
Under review as a conference paper at ICLR 2020
Since the sampling i is chosen uniformly from {1,2,…,n}, we have
EkVIfi(w) -Vifi(w*)k2
=EkVIF(w) - ViF(w*)∣∣2
1 n
=—EkVIfi(w) -Vifi(w*)k2
n z—z
i=1
(16)
≤ 2ρ+ [F (W) -F (w*) - (Vi F (w*),W - w*〉]
≤ 2ρ+ [F(w) - F(w*) + KVIF(w*), w - w*)|]
≤ 4δρ+ [F(w) - F(w*)],
where the last inequality follows from the restricted strong convexity of F (w). And we use a simple
underlying consensus that there must exist a uniform factor δ > 1 which makes the following
inequality true:
δ(F(w) - F(w*)) ≥ KVIF(w*),w - w*)| + ρbkw - w*∣∣2.
This property is called the extended restricted strong convexity used in the following proof. There-
fore, we have
2
EkVgI (wt)k2 = E∣ ɪ X (VI 九(Wt)-VI fit W)) + VIF (W)
=E ](卷 X Vifit (wt) - ViF(wt) - VIf”(w*) + VIF(w*)
+Vi F (wt) — VIF (w*) + VIF (w*)
—
∣B∣ X Vi fit (W)- Vi F (W)- VI f“ (w*) + VIF (w*))]
2
≤ 4E
∣B∣ X vIfit (Wt)- vIF(Wt)- vIfit (w*) + vIF(w*)
1	1 it∈B
2
+ 4E ∣-B∣ X Vifit (We) - ViF(W)- VI九(w*) + VIF(w*)
lBl it∈B
+ 4E∣∣ViF(wt) - ViF(w*)∣∣2 + 4∣∣ViF(w*)∣∣2
=4 肃-l"EkVIfi(wt) - ViF(wt) - VIfi(w*) + VIF(w*)∣∣2
IBKn T)
+ 4 肃―lBLEkVIfi(W) - ViF(泊)一VIfi(w*) + VIF(w*)∣∣2
IBKn T)
+ 4E∣∣ViF(wt) - ViF(w*)∣∣2 + 4∣∣ViF(w*)∣∣2
≤ (4 + 4 谒：IBI) )E∣Vifi(wt) - Vif (w*)k2 +4∣ViF(w*)∣∣2
+ 4 lBn~ -Bi)EkVIfi(W) - Vifi(w*)k2
≤ 16δρ+(1 + Iyn - lBIJE[F (wt) - F (W*)]+ 4∣Vi F (w*)∣∣2
l Bl(n - I)
(17)
+i6δρ+ l∕n IBI) E[F (W)-F (w*)]
where the first inequality follows from ∣∣α + b + C + d∣∣2 ≤ 4∣∣α∣∣2 +4|网|2+4∣∣c∣∣2 +4||川|2, the third
equality follows from Lemma 4, the second inequality follows from Ekx - Ex∣∣2 ≤ Ekxk2 and(16),
14
Under review as a conference paper at ICLR 2020
and the last inequality follows from (16). Due to the fact that EkVSgɪ(Wt)II2 ≤ 1/kEkVgi(Wt)II2
where I = Supp(w*) ∪ Supp(Wr+1) ∪ Supp(WT) and S = Gjt ∪ Supp(泊r), We have
EkVSgɪ(wt)k2 ≤ 1(16δρ+(1 + n.-1B1 )E[F(Wt)- F(w*)]+4∣∣ViF(w*)∣2
k s	|B|(n - 1)
n- |B|	(18)
+ 16δρ+ |B|(n -1) E[F (W)- F (w*)D,
where k is the number of blocks. Note that we take expectation with respect to randomized block in
the above equation. This completes the proof.

B.3 Proof of Lemma 6
Proof.	By the definition Vg(Wt) = Vfit (Wt) -Vfit (We) + Dit VF (We), we have E[Dit VF (We)]
VF (We). Therefore, Vg(Wt) is an unbiased estimator of VF(Wt).
Based on (16), we have the following result:
EkVgI(Wt)k2 =EkVIfit(Wt)-VIfij(We)+DitVIF(We)k2
≤ 3E∣[Vifit (沥一VIfit (w*)] - VIF(W) + VIF(w*)
+ VIF(We) -DitVIF(We)k2
+ 3EkVIfit(Wt)- VIfit(W*)k2 + 3∣VIF(W*)k2
≤ 6E∣∣[VIfit(涝)-VIfit(w*)] - VIF(W) + VIF(w*)∣2	(19)
+6EkVIF(We) - DVI F (We)k2
+ 3EkVIfit(Wt)- VIfit(W*)k2 + 3∣VIF(W*)k2
≤ 12δρ+ [F(Wt)- F(w*)] + (3 + 12(Dm - 1))∣∣VIF(w*)∣∣2
+ (24δρ+ + 48δρ+(Dm - 1))[F(W) - F(w*)],
where the first inequality follows from ka+b+ck2 ≤ 3kak2 + 3kbk2 +3kck2, the second inequality
follows from ka + bk2 ≤ 2kak2 + 2kbk2, the third inequality follows from EkW - EWk2 ≤ EkWk2
With E[VIfit(泊)-VIfit(w*)] = VIF(泊)-VIF(w*), and EkVIfi(Wt) - VIfi(W*)∣∣2 ≤
4δρ+[F(Wt) - F(w*)] with δ > 1, which is also satisfied for 泊 with δ > 1. Since Dm =
maxv=ι,…,d pl, we bound the ∣∣VIF(泊)-DVIF(泊)k2 term as follows:
k(D-I)VIF(We)k2
d1
=χ(需-1)[VIF W)]V
≤ (Dm2 - 1)kVIF(We)k2	(20)
=(Dm - 1)kVI F (W) - VI F (w*) + VI F (w*)∣2
≤ 2(Dm - 1)∣VIF(W)- VIF(W*)k2 + 2(Dm - 1)∣VIF(w*)∣∣2
≤ 8δρ+(Dm - 1)[FW)- F(w*)] + 2(Dm - 1)∣VIF(w*)∣∣2.
Due to the fact that EkVS gI (Wt)II2 ≤ 1 EkVgI (Wt)∣2, we have
EkVSgI(Wt)k2 ≤ 1(12δρ+[F(Wt) - F(w*)] + (3 + 12(Dm - 1))∣∣VIF(w*)∣∣2
k	(21)
+ (24δρ+ +48δρ+(Dm - 1))[FW)- F(w*)]),
where k is the number of blocks. Note that we take expectation with respect to randomized block in
the above equation. This completes the proof.
C	Proofs of Theorem 1 and Corollaries 1 and 2
In this section, we give the detailed proofs of Theorem 1 and Corollaries 1 and 2, which can guar-
antee the convergence properties of our SBCD-HTP.
15
Under review as a conference paper at ICLR 2020
C.1 Proof OF Theorem 1
Proof. In this subsection, we provide the proof of Theorem 1. Let wt+1 = Wt — NSgɪ (Wt) and
Z = Z * UZr UZr+1, where I * = SUPP(W*), Ir = supp(泊r) and Ir+1 = SUPP(Wr+1). Conditioning
on wt, we have the following expectation
E∣∣wt+1 — w*∣∣2
=EllWt - NSgɪ(Wt) — w* ∣∣2
=EIlWt — W*∣2 + η2E∣∣Vs gɪ (Wt)∣2 — 2η(Wt — w*, EVS gɪ (WDi
≤ EkWt — W*∣2 + η2E∣Vsgɪ(Wt)∣∣2 + 2η(w* — Wt, VIF(WtI)
k
≤ EkWt — W*∣2 + η2E∣Vsgɪ(Wt)∣∣2 + 孚[F(w*) — F(Wt)]
k
≤ EkWt — w*∣2 + 1(16δρ+ η2(1 + W TT )E[F(wt) — F(w*)]	(22)
k S	∖B∖(n — 1)
+ 16δρ+η2 n, ~∖β∖. E[F(W) — F(w*)] — 2ηδ[F(wt) — F(w*)]) + 4η2∣VιF(w*)∣2
∖B∖(n — 1)	k
=EkWt — w*k2 + 1((16δρ±η2(1 + W -叫)—2ηδ)E[F(wt) — F(w*)]
k S	∖B∖(n — 1)
+ 16δρ+η2 ∣7n=-≡yE[F(W) — F(w*)]) + 卑∣∣ViF(w*)∣∣2
∖q∖ (n 1)	k
where the first inequality follows from the expectation with respect to randomized block, the second
inequality follows from our extended restricted strong convexity in the proof of Lemma 5, and
the third inequality holds by Lemma 5. Notice that 滴=w0 =滴r-1. Summing (22) over t =
0, ∙∙∙ ,m — 1 and taking expectation with respect to all randomness, we have
Ekwm — W*k2 ≤ Ekter-1 — W*k2 + 1((16δρ+η2m(1 + ,：—园、)—2mηδ)E[F(wm) — F(w*)]
k s	∖B∖(n — 1)
+ 16δρ±η2m.yn ~∖B∖. E[F(WrT) — F(w*)]) + 空∣∣ViF(w*)∣∣2
∖q∖ (n 1)	k
(23)
Notice that the convergence property used here is similar to the proofs of the convergence results for
many hard thresholding algorithms (Chen & Gu, 2016; 2017; Li et al., 2016b).
Moreover, we can obtain
Ekwm — w*k2 ≤ EkWrT — w*k2 + 1((16δρ+η2m(1 + , ：-	、) — 2mηδ)E[F(wm) — F(w*)]
k S	∖B∖(n — 1)
+ 16δρ±η2m.yn ~∖B∖. E[F(Wr-1) — F(w*)]) + "∣∣ViF(w*)k2
∖q∖ (n ——1)	k
=EkWr-1 — w*k2 + 1((16δρ+η2m(1 + 焉-叱、)—2mηδ)E[FWr) — F(w*)]
k S	∖B∖(n — 1)
—(16δρ+η2m(1 + 焉-叱、)—2mηδ)E[F(Wr) — F(wm)]
S	∖B∖(n - I)
+ 16δ⅛η2m 消一叫 E[F(Wr-1) — F(w*)]) + 卒 ∣ViF(w*)∣∣2
S	∖B∖(n — 1)	k
≤ EkWr-1 — w*k2 + 1((16δρ±η2m(1 + 焉-叱、)—2mηδ)E[FWr) — F(w*)]
k	∖B∖(n — 1)
一(16δρ+η2m(1 + ^n-BIy) — 2mηδ)2X^ Ss))EkWm — w*k2
∖ ∖ (n — 1)	2σ (α — S )
+ 16δ⅛η2m 消一叫 E[F (Wr-1) — F (w*)]) + 卒 ∣∣Vi F (w* )∣∣2
S	∖B∖(n — 1)	k
(24)
16
Under review as a conference paper at ICLR 2020
where the second inequality holds due to the fact that we have the following results: When F (wer) -
F (wm) > 0, by using Lemma 3 on wm and wer, we have
{VF(wm), wer - Wmi ≤ F(Wr) - F(Wm) - ɪ∣∣VF(游「) - VF(wm)∣∣2
2ρsb+	(25)
≤ (1 - σ)[F(Wer ) - F(Wm)],
where the last inequality holds due to the fact that we assume that there exists a constant factorσ > 0
making this inequality true. Then by using the RSS condition, we have
F(Wr) - F(Wm) ≤ hVF(Wm),Wr - wmi + p+ ∣Wr - wm∣2
2	+	(26)
≤ (1 - σ)[F(Wr) - F(Wm)] + ρb∣Wr - Wm∣2.
After simplification, We have F(Wr) - F(Wm) ≤ *k泊r 一 Wm∣∣2. Using Lemma 1, We have
∣Wr - Wmk2 = IIHk(Wm)- Wmk2 ≤	IIWm - W*∣∣2,	(27)
IIl-s*
and
F(Wr) - F(Wm) ≤ B( - Sl ∣Wm - W*k2∙	(28)
2σ(d — ST)
When F(泊r) - F(Wm) < 0, we canomit-(16δρ+η2m(1+ &B])) - 2mηδ)E[F(泊r) - F(Wm)]
directly since 16δρ+η2m(1+ ∣Bn-|B1))-2mηδ is less than zero. Because this is the simple situation,
We only analyze the complex one in our Whole proof. Therefore, We have the folloWing inequality:
(I + -1(i6δρ+η2m(1 + IBnI--BL) - 2mηδ)2f((d-^Ss)))EkWm-w*∣2
≤ EkwTT - W* ∣2 + 1((16δρ+η2m(1 + V- lBI、) - 2mηδ)E[FWr) - F(w*)]	(29)
k s	|B|(n - 1)
+ 16δρ+η2m 高二 lB∖E[F(WrT)- F(w*)]) + 4η2m∣ViF(w*)∣2
|B|(n - 1)	k
Since Wer = Hk(Wm), i.e., Wer is the best S-sparse approximation of Wm, then We have the folloWing
result due to Lemma 2,
∣Wt- W*k2 ≤ (1 + √=-s**)∣wm - W*k2.	(30)
Let α = 1 + √2=s=, and by combining (29) and (30), we have
(I + 1(16δρ+η2m(1 + / lB∖) - 2mηδ)pb7d_S))Ek∕-w*∣2
k s	lBl(n - 1)	2σ(d - S*)
≤ αE∣Wr-1 - w*∣2 + k((16δρ+ η2m(1 + ∣嬴 -B、) - 2mηδ)E[F(Wr) - F(w*)]
+ 16δρ+η2m ^-lBl E[F(WrT)- F(w*)]) + 4η2-mα∣ViF(w*)∣2
s lBl(n - 1)	k
≤ 2α E[F (WrT)- F (w*)] + α ((16δρ+η2m(1 + 二 一吗)-2mηδ)E[F (Wr) - F (w*)]
ρsb-	k s	lBl(n - 1)
+ 16δρ+η2m ^-lBl E[F(WrT)- F(w*)]) + ^P |尸/(w*)∣2
l l(n - 1)	k
(31)
where the last inequality follows from the RSC condition and the definition of I, i.e., I =
supp(HT (VF (W*), 2S)) ∪ supp(W*).
17
Under review as a conference paper at ICLR 2020
Through proper simplification, we have the following result:
α (2mηδ - 16δρ+η2m(1 十 个]叫、))E[F (Wr) - F (w*)]
k	s	|B|(n - 1)	(32)
≤ (2- + 16δρ+ η2mα-n-B-)E[F(WrT)- F(w*)] + ^PIlVIF(w*)∣∣2
ρsb-	s	|B|k(n - 1)	k I
where the first term of the LHS of (31) is omitted due to the fact that the coefficient of Ekwr - w * ∣2
is larger than zero, when 8ρ+mη2(1 + ∣Bn-|B1)) 一 mη + ；：+(—]?)> 0, which can be naturally
satisfied under our parameter setting with s ≥ κsbs*. Followingsfrom (32), we have
E[F(wr) — F(w*)] ≤ βE[F(wr-1) - F(w*)]+ 而加、∣∣VIF(w*)∣∣2	(33)
δ(1 - ω)
whereβ = ηmρ-lδ(i-3)+1¾⅜-⅞⅛-)) andω = 8ρ+ η(1+1⅛⅛). ByaPPIying (33)recursively,
then We have the following desired bound with β ≤ 21 < 1:
E[FWr)- F(w*)] ≤ (1)rE[F(泊0) - F(w*)] + ɪ∣VIF(w*)∣∣2.	(34)
2	1-ω
This comPletes the Proof.
C.2 Proof of Corollary 1
Proof. We then give the Proof for the statistical estimation analysis. Following from the RSC
condition of F(∙),we have
F(w*) ≤ F(wr) + hVF(w*),w* - wri - ρ-∣∣ter - w*k2.	(35)
Let φ = (1 )rE[F(泊0) 一 F(w*)] + 1ηωIlVIF(w*)∣2. Combining (34) and (35), we have
E[F(wr) - φ] ≤ F(w*) ≤ F(wr) + hVF(w*),w* - wr〉一 PbIlwr - w*∣2.	(36)
Using the duality of norms, we have
EhVF(w*),w* - wri ≤ IIVF(w*)k∞E∣w7 - w*∣1 ≤ √b∣VF(w*)k∞E∣w7 - w*∣.	(37)
Combining (36), (37) and IEwI2 ≤ EIwI2, we have
ρ-(E∣er - w*∣)2 ≤ √b∣VF(w*)∣∞E∣而r- w*∣ + φ.	(38)
Let a = EIwer - w* I. From the above inequality, we solve the following quadratic function with
resPect to a,
ρ-a2 - √b∣VF(w*)∣∞a - φ ≤ 0,	(39)
which yields the bound
EM- w*∣≤ S 2( 2)r[F (w?- F (w*)] + WIVIF (w*)∣ + 2√bkVF(W*)k∞
V_________Pb________ ω	Pb	(40)
≤ SWFwl≡≡ + (P- + 二)√b∣VF(w*)∣∞.
This comPletes the Proof.
18
Under review as a conference paper at ICLR 2020
C.3 BOUND β < 1
Now we show that we can guarantee that β ≤ 1 by providing the appropriate choices of constants
with η, k and m used in the theorem. More specifically, let η ≤ C3 ≤ —1+, and ∣B∣ = 1, then We
ρsb	48ρsb
have
8P+ η(n - IBI) ≤____________8C3(n - |B|)__________
|B|(1- ω乂n - I) - |B|(n - 1)(i-8C3(1 + ∣B-n-1)))
If η ≥ C+ with C2 ≤ C3, then We have
ρs+b	2	3
k
1
≤ 4.
(41)
--------------7 ≤
ηδmρsb- (1 - ω )
1
2δmC2 .
3kκs
(42)
If we guarantee ɪδm⅛~ ≤ 1, then we have
5kκsb
6kκsb
m ≥	= C4κb,
δC2
(43)
where δ > 1. If we choose C2 =强,C3 = 48, C4 = 1800, k = 10 and δ = 2, then we have β ≤ 1.
Note that the convergence rate β does not include α, therefore, the required value of k comes from
the restricted constraint of the coefficient of ∣∣ιer - w*k2, which implys that S = Ω(κgs*).
C.4 Proof of Corollary 2
Proof.
E[F(Wr) - F(w*)] ≤ e + TɪkVιF(w*)∣∣2.
1-ω
Let %ι, %2,… be a non-negative sequence of random variables which is defined as
%r =max {f(Wr) - F(w*) - ɪj∣∣VIF(w*)∣∣2, θ}.
For a fixed > 0, it follows from (34) and the Markov inequality,
P(%r ≥ ) ≤ —-- ≤ -ɪ-~~---- -------——
For a given ζ ∈ (0, 1), let the right side of (46) be not greater than ζ, which requires
r ≥ log2 F(Wer)-F(W*).
ζ
(44)
(45)
(46)
(47)
Therefore, the result in (44) holds with probability at least 1 - Z. Thus, we need O(log(ɪ)) outer
iterations to get Wer satisfying (44). Since within each outer-iteration, we need to calculate a full
gradient and m stochastic variance reduced gradients with mini-batch size IBI, the overall oracle
complexity is O((n + KskBI) log ɪ). Note that in high-dimensional region, the effect of support of
snapshot point can be ignored. On the other hand, we only perform a hard thresholding operation at
each outer iteration. Then we have
log2
log( E)
log(2)
O(log(1∙)).
(48)
Thus, the hard thresholding oracle complexity is O(log(ɪ)). This completes the proof.	□
D Proof of Theorem of Sparse variant
In this section, we theoretically analyze the convergence properties of our Serial Sparse SBCD-HTP
(S2BCD-HTP).
19
Under review as a conference paper at ICLR 2020
Algorithm 3 Serial Sparse SBCD-HTP (S2BCD-HTP)
Input: number of outer-loops R, number of inner-loops m, step size η, sparsity level s;
Initialize: we0 ;
1:	for r = 0, 1, . . . , R - 1 do
0r
2:	w0 = w = wr ;
3:	VF (2=1 Pn=I VfiW);
4:	G = supp(we);
5:	for t = 0, 1, . . . , m - 1 do
6:	Randomly sample it from{1, 2, . . . , n} uniformly;
7:	Randomly sample jt from{1, 2, . . . , k} uniformly;
8:	Tit :=support of sample it;
9:	S = Ge ∪ Gjt ;
10:	VSg([wt]Tit) =VSfit([wt]Tit)-VSfit([we]Tit)+DitVSF(we);
11:	wt+1 = wt - ηVSg([wt]Tit);
12:	end for
13:	wer+1 = HT (wm, s);
14:	end for
Output: weR .
D.1 Proof
Proof.	We start with the iterate difference between wt+1 and w*. By expanding iterate difference
and taking expectation with respect to all randomness, we get
Ekwt+1 - w* k2 = Ekwt - ηVS gI (wt) - w* k2
= kwt - w*k2 + η2kVS gI (wt)k2 + 2ηEhVS gI (wt), w* - wti (49)
≤ kwt - w*k2 + η2kVs gɪ (wt)k2 + 2η EIF (wt),w* - wti,
k
where the last inequality holds by the unbiasedness of the sparse gradient estimator VgI (w t) and
the expectation with respect to randomized block. By using the ρs-b -strongly convex condition, we
get the bound forhVF(wt), w* - wti as follows:
{VF(Wt),w* - wti ≤ F(w*) - F(Wt) - ρb∣∣w* - wtk2.	(50)
Due to the support set I, we need the extended RSC condition with δ > 1, which has been intro-
duced in the proof of Lemma 5. Based on this underlying consensus, we can obtain the bound for
hVFI (wt), w* - wti.
hVIF(wt),w* -wti ≤ δ[F (w*) - F (wt)].
Using Lemma 6, we have the following sparse variance gradient bound,
E[kVsgI(wt)∣2] ≤ 1(12δρ+[F(wt) - F(w*)] + (3 + 12(Dm - 1))∣VIF(w*)∣2
k
+ (24δρsb+ + 48δρsb+(Dm2 - 1))[F (we) - F(w*)]),
By combining the above inequalities, we have
E∣wt+1 - w*∣2 ≤ ∣wt - w*∣2 + η2∣VsgI(wt)∣2 -半[F(wt) - F(w*)]
k
≤ kwt - w*k2 + 12η2δp+ - 2ηδ [F(Wt)- F(w*)]
k
+ 孚(12p+ + 24PnDm - 1))[F(W)- F(w*)]
k
2
+ n(3+12(Dm - 1))kVIF(w*)k2,
km
(51)
(52)
(53)
20
Under review as a conference paper at ICLR 2020
where the second inequality holds by Lemma 6. Summing the above inequality over t = 0, ∙∙∙ ,m -
1 and taking expectation with respect to all randomness in this epoch, we have
Ekwm - w*k2 ≤ EkterT - w*k2 - 2ηδm(1 J 6ηρ+) [F(Wm) - F(w*)]
+ 2δη2m(12ρ+ + 24PnDm - 1))[F(W) - F(w*)]	(54)
k
+ T(3+ 12(Dm - 1))kVιF(w*)k2.
k
Using (28), we obtain
Ekwm - w*k2 ≤ EkwTT - w*k2 - 2ηδm(1 - 6ηρ+)[F(WT) - F(w*)]
k
+ 2ηδm(1 - 6ηρ+)TP^d-sɪv kwm - w*k2
2kσ(d —s*)
δ 2m	(55)
+ -ηk-(24ρ+ +48ρ+ (Dm - 1))[F(WrT)- F(w*)]
k
+ η2-(3+12(Dm - 1))kVιF(w*)k2.
km
Since wer = HT (wm, s), i.e., wer is the best s-sparse approximation of wm, and due to Lemma 2,
then we have following result,
2ηδαm (I - 6ηρ+)[F(WT) - F(w*)] + (1 - 2η--(1 - 6ηρ+)-ρb-(d-Siy)kwr - w*k2
k	2kσ(d - s )
≤ αEkWr-1 - w*k2 + -n2-α(24ρ+ + 48ρ+(Dm - 1))[F(wrT)- F(w*)]
k
+ η2-α(3 + 12(Dm - 1))kVιF(w*)k2	(56)
k
≤ 2α[FWrT)- F(w*)] + -η2-α(24ρ+ + 48ρ+ (Dm - 1))[F(WrT) - F(w*)]
ρsb	k
+ η2-α(3 + 12(Dm - 1))kVιF(w*)k2,
k
where α = 1+ Js-^, and We use P--StrongIy convex of F(w) to bound ∣∣ιer-1 - wl k2 in the last
inequality. Through proper simplification, we can obtain
2η0δm(1 - 6ηρ+)[F(Wr) - F(w*)]
≤ (2α + (24ρ+ + 48ρ+(Dm - I))-η2mα)[F(wr-1) - F(w*)]
Psb-	k
+ η2-α(3 + 12(Dm - 1))kVιF(w*)k2,
km
(57)
where the second term of LHS of (56) is omitted due to the fact that the coefficient of Ekwer - wik2
is larger than zero, when 6Psb+-η2 --η +
kσ(d-s*)
δρ+ (d-s)
> 0, which can be naturally satisfied under our
parameter setting with s ≥ κsbsi. Therefore, we can obtain the following result,
E[F(wer) - F (wi)] ≤
2kα + (24 + 48(Dm - 1))δmαρ+ η2
Ps	m	S
2ηα--(1 - 6ηPsb+)
. [F(Wr-1) - F(w*)]
(3 + 12(Dm - 1))-αη2
2ηα--(1 - 6ηPsb+)
k”F (w*)k2,
(58)
+
21
Under review as a conference paper at ICLR 2020
where I = SuPP(W*) ∪ SuPP(HT(VF(w*), 2s)). We define ξ := [3+121(Dm-1)]η ∣∣ViF(w*)k2 .
2δ(1-6ρsb η)
In order to obtain a linear convergence rate, we have to bound
尸,_管 + (24 + 48(Dm - 1y)δmαρ+η2	k	+(12+ 24(Dm - 1))ρ+η
2ηαδm(1 - 6ηρsb+)	(η - 6ρsb+η2)δmρsb-	1 - 6ρsb+η
(59)
If we choose η ≤d瘾 °+, then the second term of β will be less than 1, i.e., (12+2：(Dm-I))Pb η ≤
1. As for the first term, when m ≥ 500DmKs with η ≥	+, δ = 4 and k = 10, we have
2	Dm2 -1	50Dm2 ρs+b
(η-6p+ η2 )δmρ- ≤
4. Thus, We get the linear convergence for our serial sparse SBCD-HTP (S2BCD-
HTP).
1
3
E[F(Wr) - F(w*)] ≤ - ∙ [F(泊r-1) - F(w*)]+ ξ,	(60)
which means that the total gradient evaluation oracle complexity is O((n + KkL log ɪ), and the hard
thresholding oracle complexity is O(log( ɪ)). This completes the proof.	□
E Proof of Theorem 2
In this section, we analyze the convergence properties of the proposed Asynchronous Sparse SBCD-
HTP (ASBCD-HTP). We first have to specify the iterates labeling order used in our asynchronous
analysis.
We use “After Read” labeling order (Leblond et al., 2017) in our proof, which enjoys a simpler
analysis but requires the order of randomly sampling step to be an uniform distributed sample.
In order to give a clear proof, we adopt the “After Read” labeling order and make the following
assumptions:
“After Read” labeling order:
1.	Inconsistently read the iterate Wt;
2.	Increase iterates counter t + 1 and sample a random sample it+1;
3.	Compute the update -ηVg(W^t);
4.	Atomic write the update to shared variable coordinately.
Assumption 4 The labeling order increases after Step 8 in Algorithm 2 finished, and thus the future
perturbation is not considered in the effect of asynchrony in the current step.
Assumption 5 We assume that uniform distributed samples and the independence of the sample
it+1 with Wt.
Following (Leblond et al., 2017), we can explicit the effect of asynchrony as follows:
t-1
Wt — Wt = η X	GuVg(Wu),	(61)
u=(t-τ)+
where Wt is the t-th inner iteration solution. In the proof of Theorem 2, we use Wt to represent
the temporary solution of the t-th iteration for a clean representation. And Gu is a diagonal matrix
with entries in {0, +1}. This explicitly defines the coordinate perturbation from the past updates.
Here, τ denotes the maximum number of overlaps between concurrent threads. We also denote
∆ = maxj=ι,…,dPj, which provides a measure of sparsity following (Leblond et al., 2017).
E.1 Proof
Proof. We analyze our asynchronous algorithm based on the “perturbed iterate analysis” frame-
work (Mania et al., 2017) with “After Read” labeling order. By expanding the iterate difference and
taking expectation with respect to the sample it+1 , we get
22
Under review as a conference paper at ICLR 2020
Eit+1 kwt+1 - w*k2 = Eit+1 kwt - ηvSgI(Wt)- w"l2
=kwt - W*k2 + η2kVsgi(Wt)k2 + 2ηEit+ι(W* - Wt, Vsgi(Wty)
2η	(62)
≤ ∣∣Wt - W*k2 + η2kVsgi(Wt)II2 + ɪEit+1 (w* - Wt, VgI(Wt))
+ 2ηEi+ι (Wt- Wt, Vsgi(Wt)),
where Wt is the “perturbed" iterate with perturbation ψ and the last inequality follows from the
expectation with respect to randomized block. We need to bound the term, 2η(w* - Wt, Vgi(Wtyi.
Since it+ι is independent to Wt, We can write:
Eit+ι (w* - Wt, Vgi(Wt)) = (w* - Wt, Eit+ι Vgi(Wt)) = (w* - Wt, VIF(Wt)).	(63)
We can use P--strong convexity bound of F(W) with constant factor δ > 1 as well as a squared
triangle inequality to get:
Twt- W*, VF(Wt)) ≤ -δ(F(Wt)- F(w*)) - p2+ ∣Wt - W*k2,
(64)
-kWt- w*∣2 ≤ ∣Wt- wtk2 - 2 ∣wt - w*∣2, (ka + b∣2 ≤ 2∣a∣2 +2∣b∣2),	(65)
(w* - Wt, ViF(Wt)) ≤ -δ(F(Wt)- F(w*)) + pb∣Wt - wt∣2 -与∣wt - w*∣2.	(66)
Then we make an identical deformation, and we have
~k Eit+ι hw*- wt, Vgi (Wt)) ≤- ηρs- Eit+ι kwt - w*k2+η~r~ Eit+ι kwt - wtk2
k	2k	k	(67)
-2ηδ(F(Wt)- F(w*)).
k
Putting it all together, we get the initial recursive inequity, written here explicitly:


at+1 ≤ (1- ηρ~ )at+η2Eit+ι IlVS gi (Wt)Il2+ηρs- Eit+1 kwt - wtk2
2k	t+	k	t+
+ 2ηEit+ι hwt - wt, VSgi(Wt)) —%et,
t+	k
(68)
where at := E^ikwt - w*k2 and et := E^JF(Wt) - F(w*)].
From Lemma 1 in (Leblond et al., 2017), we can bound the asynchronous terms kWt-wtk2, (Wt-wt
and Vgi(Wt)) as follows.
t-1
EhWt- wt, Vsgi(Wt)) = η X EhGuVSgi(Wu), Vsgi(Wu))
u=(t-τ)+
t-1
≤ η X E IhVsgi(Wu), Vsgi(Wu))|
u=(t-τ)+
t-1	√∆
≤ η E	气-(EkVSgi(Wu)k2 + EkVsgi(Wt)k2)
u=(t-τ)+
≤ η√2-	X	EkVsgi(Wu)k2 + η√2-EkVsgi(Wt)k2.
u=(t-τ)+
(69)
23
Under review as a conference paper at ICLR 2020
t-1
EM-wtk2 ≤ X	KGuvSgɪ(Wu),GvVSgɪ(Wv))|
V,u=(t-τ) +
t-1	t-1
≤ η2	X	kVsgɪ(WU)k2 + η2	X	KGUVSgɪ(WU),GvVSgɪ(WV)〉|
u=(t-τ )+	U,v=(t-τ )+,U=V
t-1	t-1
≤ η2	X	EkySgɪ(Wu)II2 + η2√∆(τ - 1)+	X	EkVSgɪ(Wu)∣∣2
U=(t-τ )+	U = (t-τ ) +
t-1
=η2(1 + √∆(τ - 1)+) X	EkVSgɪ(Wu)k2
u=(t-τ)+
t-1
≤ η2(1 + √∆τ)	X	EkVSgɪ(Wu)k2,
u=(t-τ)+
(70)
where GU are d × d diagonal matrices whose entries in {0, +1}. Each update in Wt is already in
Wt- this is the case of 0. On the contrary, some updates might be late: this is the case of +1. Then
by combining (68), (69) and (70), we get
-	t-1
at+1 ≤ (1 -等)αt + η2IVsgɪ(Wt)k2 + η3P-(1 + √∆τ)	X	叫叶」[网gɪ初)『
u=(t-τ)+
+ η2√∆	X	Eit+ιkVsgɪ(Wu)k2 + η2√∆τ跖+JVsgɪ(Wt)∣∣2 - *^
u=(t-τ)+
≤ (1 - ^2kS-)at + η2CIEit+ι IIVSgI(Wt)Il2 + η2C2	X	Eit+ι IIVSgI(WU)Il2--η^et,
u=(t-τ) +
(71)
where C1 :=1 + √∆τ and C2 := √∆ + ηρ-C1. T represents the mawimum number of overlaps
between concurrent threads. And ∆ = maxj=1,…，dPj, which is a measure of sparsity with 1/n ≤
∆ ≤ 1.	,
Using Lemma 6 to bound the sparse variance term, the above inequality becomes
αt+1 ≤ (1 -琮)at + '号(12δρ+[F(Wt)- F(w*)] + (24 + 48(Dm - 1))δρ+[F(W) - F(w*)]
+ (3 + 12(Dm - 1))∣ViF(w*)k2) +
牛 X	Eit+JVgι (Wu) k2-
u=(t-τ)+
沙
≤ at + η2C2	X	Eit+JVgι(W“)k2 -
u=(t-τ)+
2ηδ(1- 6ρ+ηC1) ”
k	e
+ (24ρ+ +48ρ+(Dm - 1))二阴S+(3 + 12(Dm - 1))YCI∣ViF(w*)∣∣2,
kk
(72)
where e := E[F(泊)一F(w*)], et := E[F(Wt) — F(w*)]. Note that we have W = w0 = Wr-1.
Summing (72) over t = 0, ∙∙∙ , m - 1 and taking expectation with all randomness in this epoch, we
get
αm ≤ a0 - 2ηkm(1 - 6ρ+ ηC1)^m + 牛 X X E[∣∣Vgι(Wu)∣∣2]
t=1 u=(t-τ)+	(73)
+ (24ρ+ + 48ρ+(Dm - 1))δm∣^Sr-1 + (3 + 12(Dm - 1))m^∣ViF(w*)∣∣2,
kk
24
Under review as a conference paper at ICLR 2020
where am = EkWm — w*k2, and a0 = E∣∣w0 — w*k2. Then We focus on upper bounding the third
term on the RHS of (73),
m t-1	m-1	m
X X	E[kVgι(Wu)k2] ≤ T X E[kVgι(Wt)k2] ≤ TXE[kVgι(Wt)『]
t=1 u=(t-τ)+	t=1	t=1
≤ τ(12ρ+δm^m + δm(24ρ+ + 48ρ+(Dm — 1))er-1 + m(3 + 12(Dm — 1))∣∣ViF(w*)∣∣2).
(74)
Substituting the above inequality into (73), we obtain
am ≤ a0 — 2ηkδm(1 — 6ρ+ η(τC2 + Cι))em + δρ+mL(24 + 48(Dm — 1))(τC2 + Cι)er-1
+ T (3 + 12(Dm - 1))(Cι + τC2)kVι F (w*)k2.
k
(75)
Then we bring (28) into the above inequality, we have
am ≤ a0 - 2ηδm(1 — 6ρ+η(τC2 + C1))E[F(Wm)- F(W) + F(Wr) — F(w*)]
k
+ δρ+mη2 (24 + 48(Dm — 1))(τC2 + C1)er-1
km
+ T (3 + 12(Dm — 1))(Cι + τC2)kVι F (w*)k2
k
≤ a0----ηk- (I — 6ρ+η(τC2 + CI))2r + 2ηδm(1 — 6ρ+η(TC2 + C1))2kS7(d — s*) am
+ δpbmη2 (24 + 48(Dm — l))(τC2 + Cι)βr-1
km
+ T (3 + 12(Dm — 1))(Cι + τC2)kVι F (w*)k2,
k
(76)
where the second inequality holds since we use (28) and make a proper simplification. Using Lemma
2, we have
(I — 2ηδm(1 — 6ρ+η(τC2 + CIy) 2pσd — ：： )ar
≤ 2α≡r-1 - 2αηδm (1 - 6ρ+ η(τC2 + Cι))≡r
ρsb	k	(77)
+ αδρ+mη2 (24 + 48(Dm - 1))(τC2 + Cl )er-i
km
+ αη2m (3 + 12(Dm — 1))(Cι + τC2)kVι F (w* )k2,
k
where a = 1 + Js-^, ar = k泊r — w*∣∣2, and I = SuPP(W*) ∪ SuPP(HT(VF(w*), 2s)).
The first term of LHS of (77) is omitted due to the coefficient of ar is larger than zero, when
6ρ+m(τC2 + Ci)η2 — mη + ；；+(-[) > 0, which can be naturally satisfied under our parameter
settings with S ≥ κgs*. Therefore, we obtain the following recursive inequality,
ear ≤
2kα + αδρ+mη2(24 + 48(Dm — 1))(τC2 + Ci)
Ps______________________________________ ar_1
2αηδm(1 — 6ρ+η(τC2 + Ci))
+ αη2m(3 + 12(Dm - I))(TC2 + CI)
2αηδm(1 — 6ρ+η(τC2 + Ci))
IlVIF (w*)k2.
(78)
25
Under review as a conference paper at ICLR 2020
We assume that the following settings has satisfied with the above constraint and then by choosing
m = 120κb, η = ɪ +, k = 10, We get
60ρsb
20 + 75(δ+)2 (I + 2(Dm -I))(TC2 + CI)
口 — 4δ (1- 10 (TC2 + C1))---------L + ξ∙MF(W*)k2,	(79)
Where ξ
folloWing constraint,
η(3+12(Dm-l))(τC2 + C1)
2δ(1-6ρ+ η(τC2 + C1))
In order to ensure linear speed up, τ needs to satisfy the
θ°l+°+7⅛2(I+2(Dm- 1))(τC2+C1) < ]
二	4δ(1 - 110 (τC2 + Cι))	≤ .
(80)
We assume that 37(ρsb+)2 ≥ Dm2 and δ = 10, then by simply setting τ ≤ min{ 5√∆，2κb, √√≡ },the
above constraint is satisfied With θ ≤ 0.7538, Which implies that the oracle gradient evaluation and
hard thresholding oracle complexities are O((n + Kb) log ɪ) and O(log(ɪ)), respectively. We can
simply infer the statistical estimation result as before. Here, We Will omit this part. This completes
the proof.
E.2 Discussion about Sparse Variance Bound
In the dense update case (i.e., Dm = 1), Lemma 6 is very similar to Lemma 5 When |B| = 1. In the
sparse update case, Lemma 6 highly correlates with the sparsity of datasets (α Dm), which could
be much loose in some extreme cases. If Dm is very large (imaging a dataset With some dimensions
contain only one entry among the n samples, so Dm = n), the step size of sparse (asynchronous)
SBCD-HTP will be much smaller than that of SBCD-HTP. Our algorithms can still converge to the
optimal solution up to the statistical error. Actually, it is still an open problem whether we can have
a tighter variance bound in the sparse update setting that is uncorrelated with Dm .
F More Experimental Results
Table 2: Summary of large-scale and high-dimensional datasets
Datasets	# Data	# Feature	Density
rcv1-train	20,242	47,236	0.16%
rcv1-test	677,399	47,236	0.15%
real-sim	72,309	20,958	0.024%
news20	19,996	30,000	0.49%
E2006-TFIDF	16,087	80,000	1.54%
F.1 Real-World Datasets and Equipments
We summarize the detailed information of all the real-world datasets in Table 2, including rcv1-
train, rcv1-test, real-sim, news20 and E2006-TFIDF. All these datasets are provided in the LibSVM
website1. The E2006-TFIDF dataset includes 16,087 training data points and 150,360 features.
1 https://www.csie.ntu.edu.tw/~cjlin∕libsvm∕
26
Under review as a conference paper at ICLR 2020
Instead of using all features, we randomly select 80,000 features for training. As for the news20
dataset, we randomly choose 30,000 features for training.
F.2 Baseline Methods
We compare our serial dense SBCD-HTP method with the state-of-the-art methods:
•	Fast Gradient with Hard Thresholding (FG-HT) (Yuan et al., 2014): This method is
based on a standard gradient descent step and combined with hard thresholding to solve
sparsity-constrained problems.
•	Stochastic Gradient with Hard Thresholding (SG-HT) (Nguyen et al., 2017): This
method combines stochastic gradient descent with hard thresholding for solving large-scale
sparsity-constrained problems.
•	Stochastic Variance Reduced Gradient with Hard Thresholding (SVRG-HT) (Li et al.,
2016b): This method incorporates the variance reduction technique in (Johnson & Zhang,
2013) into the hard thresholding algorithm to improve the efficiency of stochastic optimiza-
tion.
•	Accelerated Stochastic Block Coordinate Gradient Descent with Hard Thresholding
(ASBCDHT) (Chen & Gu, 2016): Block coordinate descent (BCD) is used for solving
sparsity-constrained problems.
•	Fast Newton Hard Thresholding Pursuit (FNHTP) (Chen & Gu, 2017): This method
tries to iteratively approximate the inverse Hessian matrix and combines a Newton algo-
rithm with hard thresholding.
For the proposed Serial Sparse SBCD-HTP (S2BCD-HTP) and Asynchronous sparse parallel
SBCD-HTP (ASBCD-HTP) methods, we compare them with the following asynchronous parallel
sparsity-constrained methods:
•	Asynchronous Accelerated Stochastic Block Coordinate Descent Hard Thresholding
(A2SBCD-HT): This algorithm is the asynchronous variant of ASBCDHT by using our
parallel framework.
•	Asynchronous Stochastic Gradient with Hard Thresholding (ASG-HT): This method
is an extension of SG-HT from serial dense setting to asynchronous sparse setting.
•	Asynchronous Stochastic Variance Reduced Gradient Hard Thresholding (ASVRG-
HT) (Li et al., 2016a): This method incorporates multicore structure to compute stochastic
variance reduced gradient in a parallel way.
F.3 Face Recognition Datasets
Although there are many datasets available for face recognition, we choose a common dataset (i.e.
the Extended Yale B database (Georghiades et al., 2001)). The Extended Yale B database contains
2,414 frontal-face images of 38 people under different controlled lighting conditions (Georghiades
et al., 2001). For each individual, we randomly choose 26 images for training and 15 images for
testing.
F.4 Experimental Setup
For each sparsity-constrained algorithm, the sparse level makes great difference to the solution of
sparse representation, especially at different noise levels. Therefore, in order to approach the best
performance of all these algorithms, we change the sparsity parameter within a certain range for each
algorithm. Thus, we can make sure that all these algorithms achieve the best recognition rates in the
parameter setting. In all settings of the experiments, the images are down-sampled to 32×32 pixels.
27
Under review as a conference paper at ICLR 2020
1
0.8
φ
ra
fY
U 0.6
o
0.4
o
φ
or
0.2
0
200	400	600	800
CPU Time (S)
8 6 4 2
Oooo
eɑ UOu600e比
0
0	100	200	300	400	500	600
CPU Time (s)
(a) Yale B
Figure 5: Recognition rates of all the algorithms on the Extended Yale B With different levels of Gaussian noise:
犷=0.3 (left) and 犷=0.6 (right).
-2-3
de°e≥10fqo 6。，
mini-batch |B|=1
-令-■ mini-batch |B|=3
一, mini-batch |B|=5
-¼ta , mini-batch |B|=7
-2-3
da e>AO-qo 601
mini-batch |B|=1
-- mini-batch |B|=3
-也一■ mini-batch |B|=5
-■ mini-batch |B|=7
2I)
3I)
200
Number of Effective Passes
400	600	800	1000
CPU Time (s)
O
(a) rcv1-train
Figure 6:	Mini-batch size in the convergence rate of SBCD-HTP algorithm for sparse logistic regression on
rcv1-train data.
Based on the sparse representation-based classification algorithm (SRC) (Wright et al., 2008), a
series of processing operations are made to the above dataset. We first rescale the training matrix
into [0,1] for the convenience of adding noise, and then add Gaussian noise with zero mean and
standard deviation H. Finally, We normalize the columns of the training matrix to have unit '2-norm.
The parameter settings of comparison algorithms for face recognition task are the same as those
described in Section 6.1.
F.5 Results
We first measure the effect of mini-batch size |B| in the convergence of SBCD-HTP on rcv1-train,
as shoWn in Figure 6. It is obvious that the large mini-batch size can accelerate the convergence rate
and make SBCD-HTP to obtain a better solution. This result is exactly in line With our theoretical
analysis. HoWever, When the mini-batch size is relatively large, it Will deteriorate the performance
of running time. One can simply choose the mini-batch size to 3 or 5 as suggested in Figure 6.
28
Under review as a conference paper at ICLR 2020
F.6 Face recognition tasks
Then, we compare the performance of some hard thresholding algorithms, including SG-HT, SVRG-
HT and ASBCDHT, with our SBCD-HTP for solving face recognition tasks on the Extended Yale B
dataset. As shown in Figure 5, SBCD-HTP can achieve the best performance among these stochastic
hard thresholding methods in a short time. It is clear that SBCD-HTP can not only optimize the
objective function efficiently but also obtain a better representative solution on real applications,
which is consistent with our theoretical results as described in Section 4.2. This result also shows
that our SBCD-HTP has a better generalization accuracy than other hard thresholding algorithms.
-2
d -4
B
0
E -6
o
Φ C
H1 -8
O
6
O -10
-12
-14
0	10	20	30	40	50	60
Number of Effective Passes
(a) E2006-TDIFT
0	10	20	30	40	50	60
Number of Effective Passes
(b) real-sim
Figure 7:	Comparison of all the algorithms for solving sparse logistic regression problems. In each plot, the
vertical axis shows the objective value minus the minimum, and the horizontal axis is the number of effective
passes over data or running time (seconds).
F.7 Linear/logistic registration
We show more practical evaluation results for sparse linear regression and sparse logistic regression
problems. All hardware environment and parameter settings are the same as in Section 6. For
SBCD-HTP, from Figures 7 and 8, we can come to the same conclusion as we did before, that is in
both sparse linear regression and sparse logistic regression, our algorithm converges faster than the
baseline algorithms in terms of effective passes and running time. However, we find an interesting
29
Under review as a conference paper at ICLR 2020
result, which is that for the E2006-TFIDF dataset, all the algorithms converge with high precision
solutions, so that we have to use some methods to magnify their differences. Of course, our SBCD-
HTP has the best performance, but the gap is not that big. We think that this is due to ease with which
E2006-TFIDF dataset can be optimized, since this phenomenon does not occur on other datasets.
For S2BCD-HTP and ASBCD-HTP, from Figure 9, our S2BCD-HTP and ASBCD-HTP far exceed
the baseline algorithms in terms of running time. Therefore, we can confirmedly obtain the conclu-
sion that our Sparse and Asynchronous algorithms outperform the state-of-the-art hard thresholding
algorithms for sparse datasets. This is a significant improvement for hard thresholding algorithms to
solve sparse learning problems in the high-dimensional asynchronous setting.
O-2-4
ʤ e>-efqo 60，
0	10	20	30	40	50	60	0
Number of Effective Passes
O-2-4
da e>AO-qo 60，
-----FG-HT
一 SG -SG-HT
一 B -SVRG-HT
一 ★ -ASBCDHT
一 θ -FNHTP
-V -SBCD-HTP
500	1000	1500
CPU Time (S)
(a) E2006-TDIFT
⅛
O
φ
B-B-B
o
ɪ -3
0	10	20	30	40	50	60
Number of Effective Passes
-----FG-HT
-S -SG-HT
-B -SVRG-HT
一★ -ASBCDHT
一θ -FNHTP
-V -SBCD-HTP
0	200	400	600	800	1000 1200
CPU Time (s)

(b) real-sim
Figure 8:	Comparison of all the algorithms for solving sparse linear regression problems.
F.7.1	Upper bound
In order to conduct an experiment on the upper bound of Theorem 1. We first generate some n × d
synthetic matrices X , each row of which is drawn independently from a d-dimensional Gaussian
distribution with mean 0 and covariance matrix Σ ∈ Rd×d . The response vector is generated from
the model y = Xw* +e, where w* ∈ Rd is the s*-sparse coefficient vector, and We need to generate
the noise drawn from a multivariate normal distribution N(0, σ2I) with σ2 = 0.01. The nonzero
entries in w* are sampled independently from a uniform distribution over the interval [-1, 1]. For
the experiments, we construct the following synthetic data set: n = 5000, d = 10000, s* = 500,
and the diagonal entries of the covariance matrix Σ are set to 1, and the other entries are set to 0.1.
30
Under review as a conference paper at ICLR 2020
-1
-1—A2SBCD-HT
-S- ASG-HT
一«- - ASVRG-HT
—S- 一S2BCD-HTP
-V -ASBCD-HTP
-1—A2SBCD-HT
一e-ASG-HT
一≡- -ASVRG-HT
—÷ -S2BCD-HTP
一 A -ASBCD-HTp
-2
0
6
-4
0	1	2	3	4	5
CPU Time (S)
CPU Time (S)
0	50	100	150	200
(a) real-sim
(b) news20
-n
⅛-2
⅛-2
-1—A2SBCD-HT
一6一 ASG-HT
一 ≡- - ASVRG-HT
一今-S2BCD-HTP
一 A - ASBCD-HTp
-V
ι∖
1\
-3
a -3
2 一
0
10
20
30
40
50
0
0.5
1
1.5
2
2.5
3
CPU Time (s)
CPU Time (s)
~θ ——
-1-A2SBCD-HT
一①-ASG-HT
一仔-ASVRG-HT
一 e -S2BCD-HTp
A -ASBCD-HTp


(c) rcv1-train
(d) rcv1-test
Figure 9:	Comparison of ASG-HT, ASVRH-HT, A2SBCD-HT, S2BCD-HTP and ASBCD-HTP for solving
sparse linear regression problems. The four asynchronous parallel algorithms (i.e., ASG-HT, ASVRH-HT,
A2SBCD-HT and ASBCD-HTP) run on 20 threads.
We Set the step-size η = 48p7 and batch size |B| = 1 during our experiments. The upper bound is
then computed according to Theorem 1. Under this condition, we obtain the result, as is shown in
Figure 10. As we can see that the upper bound converges faster at first and then it becomes slow
since the statistical error becomes the main factor. The convergence performance of SBCD-HTP is
always bounded below the black line, which demonstrates the correctness of our theorem.
F.8 Linear SVM
In this subsection, we consider the sparsity-constrained L2-SVM problem (i.e.,
minw * Pn=I (max{0,1 - y⑴w>x⑴})2 + λ∣∣w∣∣2, subject to ∣∣wko ≤ s), where y(i) is
the known label and x(i) is the given sample. We fix the regularization parameter λ = 10-4. The
other parameters are also same to the above experiments. The objective value gap and running time
of the baselines are provided in Figure 11. We can see that we also make a similar observation as
from the previous results. SBCD-HT performs the best among these considered baselines.
31
Under review as a conference paper at ICLR 2020
-5
0
-2-3-4
ʤ e>-θefqo 60，
10	20	30	40	50	60
Number of Effective Passes
(a) n = 5000, d = 10000, S = 500, s* = 500
Figure 10:	The upper bound of Theorem 1 vs the convergence performance.
Cl
ro
O
OJ
,≥
O
ω
K
O
O
0
50
0	10	20	30	40	50	60
100	150	200
Number of Effective Passes
CPU Time (s)
(a) news20
-3
-----FG-HT
-0 - SG-HT
-B -SVRG-HT
-★ -ASBCDHT
-V-SBCD-HTP
0
50
100	150	200	250	300
CPU Time (S)
(b) rcv1-train
Figure 11:	Comparison of FG-HT, SG-HT, SVRH-HT, ASBCDHT and SBCD-HTP for solving sparse SVM
problems.
32