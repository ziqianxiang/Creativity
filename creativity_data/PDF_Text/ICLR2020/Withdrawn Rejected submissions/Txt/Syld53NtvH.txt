Under review as a conference paper at ICLR 2020
Expected Tight Bounds for Robust Deep
Neural Network Training
Anonymous authors
Paper under double-blind review
Ab stract
Training Deep Neural Networks (DNNs) that are robust to norm bounded adver-
sarial attacks remains an elusive problem. While verification based methods are
generally too expensive to robustly train large networks, it was demonstrated by
Gowal et al. (2019) that bounded input intervals can be inexpensively propagated
from layer to layer through deep networks. This interval bound propagation (IBP)
approach led to high robustness and was the first to be employed on large networks.
However, due to the very loose nature of the IBP bounds, particularly for large/deep
networks, the required training procedure is complex and involved. In this paper,
we closely examine the bounds of a block of layers composed of an affine layer,
followed by a ReLU, followed by another affine layer. To this end, we propose
expected bounds (true bounds in expectation), which are provably tighter than
IBP bounds in expectation. We then extend this result to deeper networks through
blockwise propagation and show that we can achieve orders of magnitudes tighter
bounds compared to IBP. Using these tight bounds, we demonstrate that a simple
standard training procedure can achieve impressive robustness-accuracy trade-off
across several architectures on both MNIST and CIFAR10.
1	Introduction
Deep neural networks (DNNs) have demonstrated impressive performance in many fields of research
with applications ranging from image classification (Krizhevsky et al., 2012; He et al., 2016) and
semantic segmentation (Long et al., 2015) to speech recognition (Hinton et al., 2012), just to name a
few. Despite this success, DNNs are still susceptible to small imperceptible perturbations, which can
lead to drastic performance degradation, especially in visual classification tasks. Such perturbations
are best known and commonly referred to as adversarial attacks. Early work showed that simple
algorithms (e.g. maximizing the classification loss with respect to the input using a single optimization
iteration (Goodfellow et al., 2014)) can easily construct such adversaries. Since then, a research
surge has emerged to develop simple routines to construct adversarial examples consistently. For
instance, Moosavi-Dezfooli et al. (2016) proposed a simple algorithm, called DeepFool, which finds
the smallest perturbation that fools a linearized version of the network. Interestingly, the work of
Moosavi-Dezfooli et al. (2017) demonstrated that such adversaries can be both network and input
agnostic, i.e. universal deterministic samples that fool a wide range of DNNs across a large number
of input samples. More recently, it was shown that such adversaries can also be as simple as Gaussian
noise (Bibi et al., 2018). Knowing that DNNs are easily susceptible to simple attacks can hinder the
public confidence in them, especially for real-world deployment, e.g. in self-driving cars and devices
for the visually impaired.
Such a performance nuisance has prompted several active research directions, in particular, work
towards network defense and verification. Network defense aims to train networks that are robust
against adversarial attacks through means of robust training or procedures at inference time that
dampen the effectiveness of the attack (Madry et al., 2018; Wong & Kolter, 2018; Raghunathan et al.,
2018; Alfadly et al., 2019). On the other hand, verification aims to certify/verify for a given DNN
that there exists no small perturbation of a given input that can change its output prediction (Katz
et al., 2017; Sankaranarayanan et al., 2016; Weng et al., 2018a). However, there are also works at
the intersection of both often referred to as robustness verification methods, which use verification
methods to train robust networks. Such algorithms often try to minimize the exact (or upper bound) of
the worst adversarial loss over all possible bounded energy (often measured in '∞ norm) perturbation
around a given input.
1
Under review as a conference paper at ICLR 2020
Although verification methods prove to be effective in training robust networks (Wong & Kolter,
2018), they are computationally expensive, thus limiting their applicability to only small, at best
medium, sized networks. However, Gowal et al. (2019) recently demonstrated that robustly training
large networks is possible by leveraging the cheap-to-compute but very loose interval-based verifier,
known as interval domain from Mirman et al. (2018). In particular, they propagate the e-'∞ norm
bounded input centered at x ∈ Rn, i.e. [x - 1n, x + 1n], through every layer in the network at a
time. This interval bound propagation (IBP) is inexpensive and simple; however, it results in very
loose output interval bounds, which in turn necessitates a complex and involved training procedure.
Closer to our work, there has been several prior arts that propose to perform verification differently.
In particular, Webb et al. (2018) presents a statistical approach to assessing robustness of neural
networks. As opposed to verification methods, which in many cases can be hard to scale, that provide
a binary measure of robustness per sample, they propose to frame verification as a probability of
violation instead. That is to say, they investigate the probability of failure over a violation rather than
confirming that this probability is exactly zero. Moreover, (Weng et al., 2018b) propose CLEVER
which estimates a lower bound to the minimum perturbation rather that finding the lower bounds
exactly. In both works, the verification is tackled by, and closely related to our direction of work,
probabilistically estimating the bounds.
In this paper, we are interested in improving the tightness of output interval bounds (referred to as
bounds from now on). We do so by closely examining the bounds for a block of layers composed
of an affine layer, followed by a ReLU nonlinearity, followed by another affine layer under e-'∞
bounded input. In fact, we propose new expected bounds for this block of layers, which we prove to
be not only supersets to the true bounds of this block in expectation but also very tight to the true
bounds. Lastly, we show how to extend such a result to deeper networks through blockwise bound
propagation leading to several orders of magnitude tighter bounds as compared to IBP.
Contributions. Our contributions are three-fold. (i) We propose new bounds for the block of layers
composed of an affine layer, followed by a ReLU, followed by another affine layer. We prove that
these bounds are in expectation, under a distribution of network parameters, supersets to the true
bounds of this block. Moreover, we prove that these bounds are much tighter, in expectation (will
be formalized later) than the IBP bounds (Gowal et al., 2019) generated by propagating the input
bounds through every layer in the block. Our bounds get even tighter as the number of hidden nodes
in the first affine layer increases. (ii) We show a practical and efficient approach to propagate our
bounds (for the block of layers) through blocks (not through individual layers) of a deep network,
thus resulting in magnitudes tighter output bounds compared to IBP. (iii) We conduct experiments on
synthetic networks and on real networks, to verify the theory, as well as the factors of improvement
over IBP. Due to the tightness of our proposed expected bounds, we show that with a simple standard
training procedure, large/deep networks can be robustly trained on both MNIST (LeCun, 1998)
and CIFAR10 (Krizhevsky & Hinton, 2009) achieving state-of-art robustness-accuracy trade-off
compared to IBP. In other words, we can consistently improve robustness by significant margins with
minimal effect on test accuracy as compared to IBP.
2	Related Work
Training accurate and robust DNNs remains an elusive problem, since several works have demon-
strated that small imperceptible perturbations (adversarial attacks) to the DNN input can drastically
affect their performance. Early works showed that with a very simple algorithm, as simple as max-
imizing the loss with respect to the input for a single iteration (Goodfellow et al., 2014), one can
easily construct such adversaries. This has strengthened the line of work towards network verification
for both evaluating network robustness and for robust network training. In general, verification
approaches can be coarsely categorized as exact or relaxed verifiers.
Exact Verification. Verifiers of this type try to find the exact largest adversarial loss over all possible
bounded energy (usually measured in '∞ norm) perturbations around a given input. They are often
tailored for piecewise linear networks, e.g. networks with ReLU and LeakyReLU nonlinearities.
They typically require mixed integer solvers (Cheng et al., 2017; Lomuscio & Maganti, 2017; Tjeng
& Tedrake, 2019) or Satisfiability Modulo Theory (SMT) solvers (Xiaowei Huang & Wu, 2017;
Ehlers, 2017). The main advantage of these approaches is that they can reason about exact adversarial
robustness; however, they generally are computationally intractable for verification purposes let alone
any sort of robust network training. The largest network used for verification with such verifiers
was with the work of Tjeng & Tedrake (2019), which employed a mixed integer solver applied to
2
Under review as a conference paper at ICLR 2020
networks of at most 3 hidden layers. The verification is fast for networks that are pretrained with a
relaxed verifier but gets much slower on normally trained similar sized networks.
Relaxed Verification. Verifiers of this type aim to find an upper bound on the worst adversarial loss
across a range of bounded inputs. For instance, a general framework called CROWN was proposed
by Huan Zhang & Daniel (2018) to certify robustness by bounding the activation with linear and
quadratic functions, thus, enabling the study of generic, not necessarily piecewise linear, activation
functions. By utilizing the structure in ReLU based networks, the work of Weng et al. (2018a)
proposed two fast algorithms based on linear approximation on the ReLU units. Moreover, Wang
et al. (2018c) proposed ReluVal for network verification based on symbolic interval bounds, while
Wang et al. (2018b) proposed Neurify with much tighter bounds. Several other works utilized the
dual view of the verification problem (Wong & Kolter, 2018; Wong et al., 2018). More recently,
Salman et al. (2019) unified a large number of recent works in a single convex relaxation framework
and revealed several relationships between them. In particular, it was shown that convex relaxation
methods that fit this framework suffer from an inherent barrier compared to exact verifiers.
For completeness, it is important to note that there are also hybrid methods that combine both exact
and relaxed verifiers and have shown to be effective (Rudy Bunel, 2018). Although relaxed verifiers
are much more computationally friendly than exact verifiers, they are still too expensive for robust
training of large/deep networks (with more than 5 hidden layers). However, very loose relaxed
verifiers can possibly still be exploited for this purpose. For instance, Wang et al. (2018a) leveraged
symbolic interval analysis to verifiably train large networks. More recently, the work of Gowal et al.
(2019) proposed to use an inexpensive but very loose interval bound propagation (IBP) certificate
to train (for the first time) large robust networks with state-of-the-art robustness performance. This
was at the expense of a complex and involved training routine resulting from the loose nature of the
bounds. To remedy these training difficulties, we instead propose expected bounds, not for each layer
individually, but for a block of layers jointly. Such bounds are slightly more expensive to compute
but are much tighter in expectation. We then propagate these bounds through every block in a deeper
network to attain much tighter bounds overall as compared to layerwise IBP. The tighter bounds
enable the use of simple standard training routines for robust training of large networks, resulting in
state-of-art robustness-accuracy trade-off.
3	Expected Tight Interval B ounds
We analyze the interval bounds of a DNN by proposing expected true and tight bounds for a two-layer
network (Affine-ReLU-Affine). Then, we propose a mechanism to extend them for deeper networks.
First, we detail the interval bounds of Gowal et al. (2019) to put our bounds in context.
3.1	Interval Bounds for a Single Affine Layer
For a single affine layer parameterized by A1 ∈ Rk×n and b1 ∈ Rk, it is easy to show that its output
lower and upper interval bounds for an e-'∞ norm bounded input X ∈ [x - e1n, X + e1n] are:
l1 = A1x + b1 - |A1|1n, u1 = A1x + b1 + |A1|1n.	(1)
Note that |.| is an elementwise absolute operator. In the presence of any non-decreasing elementwise
nonlinearity (e.g. ReLU), the bounds can then be propagated by applying the nonlinearity to {l1, u1}
directly. As such, the interval bounds can be propagated through the network one layer at a time, as
proposed by Gowal et al. (2019). While this interval bound propagation (IBP) mechanism is a very
simple and inexpensive approach to compute bounds, these bounds can be extremely loose for deep
networks, requiring a complex and involved robust network training procedure.
3.2	Proposed Interval Bounds for an Affine-ReLU-Affine Block
Here, We consider a block of layers of the form Affine-ReLU-Affine in the presence of '∞ PertUr-
bations at the input. The functional form of this network is: g(X) = a2>max (A1X + b1, 0) + b2,
where max(.) is an elementwise operator. The affine mappings can be of any size, and throughout
the paper, we take A1 ∈ Rk×n and without loss of generality the second affine map is a single vector
a2 ∈ Rk . Note that g also includes convolutional layers, since they are also affine mappings.
Layerwise Interval Bound Propagation (IBP) on g. Here, we apply the layerwise propagation
strategy of Gowal et al. (2019) detailed in Section 3.1 on function g(X) with X ∈ [x - e1n, X + e1n]
3
Under review as a conference paper at ICLR 2020
to obtain bounds [LIBP, UIBP]. We use these bounds for comparison in what follows.
LIBP = a> (max(uι,。，+max(h, 0k)) - 船 (max(uι, 0/ - max(lι, 0k)) + g
UIB P = a> ( max-.。一 十max(h, 0k)) + 区>| ( max (uι, 0k) - max (lι, 0k)) +
Note that max (l1, 0k) and max (u1, 0k) are the result of propagating [x - 1n, x + 1n] through
the first affine map (A1, b1) and then through the ReLU nonlinearity, as stated in (1).
Expected Tight Interval Bounds on g. Our goal is to propose new interval bounds for g, as a
block, which are tighter than the IBP bounds [LIBP , UIBP], since we believe that tighter bounds
for a two-layer block, when propagated/extended to deeper networks, can be tighter than applying
IBP layerwise. Denoting the true output interval bounds of g as [Ltrue, Utrue], the following inequality
holds LtrUe ≤ g(x) ≤ UtrUe ∀x ∈ [x - e1n, X + e1n]. Deriving these true (and tight) bounds for g
in closed form is either hard or results in bounds that are generally very difficult to compute, deeming
them impractical for applications such as robust network training. Instead, we propose new closed
form expressions for the interval bounds denoted as [LM, UM], which we prove to be true bounds
and tighter than [LIBP, UIBP] in expectation under a distribution of the network parameters A1
and a2 . As such, we make two main theoretical findings. (i) We prove that LM and UM are true
bounds in expectation, i.e.	EA1,a2	[LM] ≤	EA1,a2	[Ltrue]	and EA1,a2	[UM]	≥ EA1,a2	[Utrue]	hold
for a sufficiently large input dimension n. (ii) We prove that [LM , UM] can be arbitrarily tighter than
the loose bounds [LIBP, UIBP] in expectation, as the number of hidden nodes k increases.
Analysis. To derive LM and UM , we study the bounds of the following function first:
g(X) = a>M (AιX + bi) + b2 = a>MAiX + a>Mbi + b2.	(2)
Note that g is very similar to the Affine-ReLU-Affine map captured by g with the ReLU replaced by
a diagonal matrix M constructed as follows. If we denote ui = AiX + bi + e|Ai |1n as the upper
bound resulting from the propagation of the input bounds [X - e1n, X + e1n] through the first affine
map (Ai, bi), then we have M = diag (1 {ui ≥ 0k}) where 1 is an indicator function. In other
words, Mii = 1 when the ith element of ui is non-negative and zero otherwise. Note that for a given
ui, g(X) is an affine function with the following output interval bounds for X ∈ [x - e1n, X + e1n]:
LM, UM = a>MAiX + a>Mbi + b2 干 e|a>MAi |1n	(3)
To compare LM and UM to Ltrue and Utrue, respectively, and since having access to Ltrue and Utrue
is not feasible, we make the following mild key assumption.
Assumption 1. (Key Assumption). Consider an '∞ bounded uniform random variable X, X ∈
[X - e1n, X + e1n] where Ai and a2 have elements that are i.i.d. Gaussian of zero mean and σA1
and σa2 standard deviations, then there exists a sufficiently large m, such that:
EA1,a2 [Ltrue]
≥L
approx
EAι,a2,X [g(X)] - mVVAfA1,a2,x [g(X)],
EA1 ,a2 [Utrue]
≤U
approx
EAι,a2,X [g(X)] + mVVArA1,a2,x [g(X)].
Under Assumption 1 and to show that LM and UM are true bounds in expectation, it is sufficient to
show that Lapprox ≥ EA1 ,a2 [LM] and similarly for UM . However, since it is generally difficult to
compute the right hand sides of Assumption 1, they can be well approximated by Lyapunov Central
Limit Theorem. More formally, follows the two propositions.
Proposition 1. For a ∈ Rn 〜N(0, σɑI) and a uniform random vector X 〜U[x — e1n, X + e1n]
where both a and X are independent, we have that LyaPUnoV Central Limit Theorem holds such that
1n
--〉：(Xiai - E[αiXi]) →d N(0, 1),
sn i=i
where s1 2n = Var (X(Xiai - E[0iXi])
where →d indicates convergence in distribution.
Proposition 2. Fora random matrix Ai with i.i.d. Gaussian elements of zero mean and σA1 standard
deviation and a uniform random vector X 〜U [x — e1n, X + e1n] we have that Covariance(AX)=
(i e2σA In + σA ɪ trace (xx>))I .
4
Under review as a conference paper at ICLR 2020
(a)	(b)
Figure 1: True Bounds in Expectation. As shown in Figure 1a, our proposed interval bounds [LM, UM], as
predicted by Theorem 1, get closer to being a true super set to the true interval bounds [Ltrue, Utrue] estimated by
Monte-Carlo Sampling as the input dimension n increases regardless of the number of hidden nodes. Figure 1b
shows that a similar behaviour is present even under varying network depth.
Following Propositions 1 and 2, we have that for a sufficiently large input n:
L
approx,
approx
Ea2 ,y [a>max (y, 0) + b2]干 m
JVara2 ,y [a>max (y, 0) + b2],
(4)
U

where the output of the first affine layer A1X + b1 is approximated by Lyapunov Central Limit
Theorem as y 〜N(b1, (3e2σAIn + σAιtrace(xx>))I).
Theorem 1.	(True Bounds in Expectation) Let Assumption 1 hold. For large input dimension n,
EA1,a2 [LM] ≤ EA1,a2 [Ltrue] and EA1,a2 [Utrue] ≤ EA1,a2 [UM].	(5)
Theorem 1 states that the interval bounds for function g are simply looser bounds to the function
of interest g in expectation under a plausible distribution of A1 and a2 . Now, we investigate the
tightness of these bounds as compared to the IBP bounds [LIBP , UIBP].
Theorem 2.	(Tighter Bounds in Expectation) Consider an '∞ bounded uniform random variable
input X ∈ [x - e1n, X + e1n ] to a block of layers in the form OfAffine-ReLU-Affine (parameterized
by A1, b1, a2 and b2 for the first and second affine layers respectively) and a2 〜N (0, σa21). Under
the assumption that √12∏ χj1> AICj) + 2n 1> bi ≥ e (IAI Cj)k2 - √12∏ kAi(:, j)ki) NA，He
have: Ea2 [(UIBP - LIBP) - (UM - LM)] ≥ 0.
Theorem 2 states that under some assumptions on A1 and under a plausible distribution for a2 , our
proposed interval width can be much smaller than the IBP interval width, i.e. our proposed intervals
are much tighter than the IBP intervals in expectation. Next, we show that the inequality assumption
in Theorem 2 is very mild. In fact, a wide range of (A1, b1) satisfy it, and the following proposition
gives an example that does so in expectation.
Proposition 3. For a random matrix A1 ∈ Rk×n with i.i.d elements A1 (i,j)〜N(0,1)，then
EAI (k AI Cj) k2--√≡=kA1 Cj) Ik) = √2 J(2、) - k∖ [— ≈ √k 1 1 - ∖∕~~√k∖ .
2∏	√2π	Γ (2) Vn ∖ NK)
Proposition 3 implies that as the number of hidden nodes k increases, the expectation of the right
hand side of the inequality assumption in Theorem 2 grows more negative, while the left hand side of
the inequality is zero in expectation when b1 〜N(0k, I). In other words, for Gaussian zero-mean
weights (A1, b1) and with a large enough number of hidden nodes k, the assumption is satisfied. All
proofs and detailed analyses are provided in the appendix.
Comment on the Assumptions on A1 and a2. Generally speaking, our proposed bounds [LM, UM]
computed using Equation 3 can be very loose and much worse compared to IBP if the network weights
A1 , a2 do not follow the Gaussian assumption. We leave for the appendix an example to such failure.
However, while the Gaussian i.i.d. assumption can be strong for general networks trained on real data,
it is not far from being reasonable due to commonly accepted training procedures. This is since it is
common to regularize network weights while training deep neural networks with an `2 regularizer
encouraging the weights to follow a zero mean Gaussian distribution not to mention that networks in
many cases are initialized in such manner. We dwell on this further in the appendix where we show
the histogram of the weights of networks trained on real data (MNIST, CIFAR10, CIFAR100). with
and without `2 regularization See appendix for details.
5
Under review as a conference paper at ICLR 2020
(b)
(d)
(a)
(c)
Figure 2: Tighter than IBP with Varying Input Size and Hidden Nodes. We show a bound tightness
comparison between our proposed interval bounds and those of IBP by comparing the difference and ratio of
their interval lengths with varying k, n, and for a two-layer network. The proposed bounds are significantly
tighter than IBP, as predicted by Theorem 2.
(a)	(b)	(c)	(d)
Figure 3: Tighter than IBP in Deeper Networks. We show a bound tightness comparison between our
proposed interval bounds and those of IBP by varying the number of layers for several choices of . The
proposed bounds are significantly tighter than IBP.
3.3 Extending Our Expected Tight Bounds to Deeper Networks
To extend our proposed bounds to networks deeper than a two-layer block, we simply apply our
bound procedure described in Section 3.2, recursively for every block. In particular, consider an
L-layer neural network defined as f (x) = ALReLU(AL-IReLu(∙ ∙ ∙ A2ReLU(AIx))) and an e-
'∞ norm bounded input centered at x, i.e. X ∈ [x - e1n, X + e1n]. Without loss of generality,
we assume f is bias-free for ease of notation. Then, the output lower and upper bounds of f are
LM = GL-1x - |GL-1|1n and UM = GL-1x + |GL-1|1n, respectively. Here, GL-1 is a
linear map that can be obtained recursively as follows:
Gi = Ai+ιMiGy (with Go = Ai) and Mi = diag (1 {(G-ιX + c∣G-ι |1n) ≥ 0}) (6)
Note that Gi-1x + |Gi-1|1n is the output upper bound through a linear layer parameterized by
Gi-1 for input X as in (1). With this blockwise propagation, the output interval bounds of f are now
estimated by the output intervals of f (x) = GL-IX.
4	Experiments
True Bounds in Expectation. Here, we validate Theorem 1 with several controlled experiments.
For a network g(X) = A2 max (A1X + b1,0) + b2 that has true bounds [Ltrue, Utrue] for X ∈
[x - 1n, x + 1n], we empirically show that our proposed bounds [LM, UM], under the mild
assumptions of Theorem 1, indeed are true, i.e. they are a super set to [Ltrue, Utrue] in expectation.
We also verify this as a function of the network input dimension n (as predicted by Theorem 1).
We start by constructing a network g where the biases b1 ∈ Rk and b2 ∈ R are initialized following
the default Pytorch initialization (Paszke et al., 2017). As for the elements of the weight matrices
A1 ∈ Rk×n and A2 ∈ R1 ×k, they are sampled fromN(0,1 /√n) andN(0,1 /√k), respectively. We
estimate Ltrue and Utrue by taking the minimum and maximum of 106 + 2n Monte-Carlo evaluations
of g. For a given X 〜N(0n, I) and with e = 0.1, we uniformly sample 106 examples from the
interval [X - 1n, X + 1n]. We also sample all 2n corners of the hyper cube [X - 1n, X + 1n]n. To
show that the proposed interval [LM , UM] is a super set of [Ltrue, Utrue] (i.e. they are true bounds),
we evaluate the length of the intersection of the two intervals over the length of the true interval
defined as Γ= |[Lm , UM ] ∩ [Ltrue, UtrUe ]|/| [Ltrue, Utrue ]|. Note that Γ = 1 if and only if [Lm , UM ]
is a super set of [Ltrue, Utrue]. For a given n, we conduct this experiment 103 times with varying A1,
A2, b1, b2 and X and report the average Γ. Then, we run this for a varying number of input size
n and a varying number of hidden nodes k, as reported in Figure 1a. As predicted by Theorem 1,
6
Under review as a conference paper at ICLR 2020
Γmin
Table 1: True and Tight Bounds on Real Networks. Table shows that our bounds are a super set to true
bounds computed with an exact MIP solver. Moreover, they are much tighter than bounds estimated by IBP.
Γ
Small MNIST	0.01	1.0 ± 0	1.0	1.0
	0.02	1.0 ± 0	1.0	1.0
	0.03	0.97 ± 0.088	0.635	1.0
e	I^^WIBP - WM^^I^^WIBP/ Wm^^I
Small MNIST	0.01	644.322	17.391
	0.02	1381.980	15.270
	0.03	2255.397	14.555
Figure 4: Qualitative Results. We plot visualizations of the output polytope of a 20-100-100-100-100-2
network through Monte-Carlo evaluations of the network with a uniform random input with varying . We also
plot our proposed bounds [LM, UM] in red. Each row is for a given with different randomized weights for the
network. As for the IBP bounds [LIBP, UIBP], they were omitted as they were significantly larger. For example,
for the first figure with = 0.05, IBP bounds are [-43.7, 32.9] for the x-axis and [-47.8, 37.0] for the y-axis.
Figure 1a demonstrates that as n increases, the proposed interval will be more likely to be a super set
of the true interval, regardless of the number of hidden nodes k . Note that networks that are as wide
as k = 1000, require no more than n = 15 input dimensions for the proposed intervals to be a super
set of the true intervals. In practice, n is much larger than that, e.g. n ≈ 3 × 103 in CIFAR10.
In Figure 1b, we empirically show that the above behavior also holds for deeper networks. We
propagate the bounds blockwise as discussed in Section 3.3 and conduct similar experiments on
fully-connected networks. We construct networks with varying depth, where each layer has the same
number of nodes equal to the input dimension k = n. These results indeed suggest that the proposed
bounds are true bounds and are more likely so with larger input dimensions. Here, n = 20 performs
better than n = 10 across different network depths.
Tighter Bounds in Expectation. Here, we experimentally affirm that our bounds can be much tighter
than IBP bounds (Gowal et al., 2019). In particular, we validate Theorem 2 by comparing the interval
length of our proposed bounds, WM = UM - LM, with that of IBP, WIBP = UIBP - LIBP, on
networks with functional form g. We compute both the difference and ratio of widths for varying
values of k, n, and . Figure 2 reports the average width difference and ratio over 103 runs in a similar
setup to the previous section. Figures 2a and 2b show that the proposed bounds indeed get tighter
than IBP, as k increases across all values (as predicted by Theorem 2). Note that we show tightness
results for = {0.01, 0.1} in Figure 2b as the performance of = {0.5, 1.0} was very similar to
= 0.1. Similar improvement occurs with increasing n, as in Figures 2c and 2d.
We also compare the tightness of our bounds to those of IBP with increasing depth for both fully-
connected networks (refer to Figures 3a and 3b) and convolutional networks (refer to Figures 3c and
3d). For all fully-connected networks, we take n = k = 500. Our proposed bounds get consistently
tighter as the network depth increases over all choices of . In particular, the proposed bounds can be
more than 106 times tighter than IBP for a 10 layer DNN. A similar observation can also be made
for convolutional networks, where it is expensive to compute our bounds using the procedure in
Section 3.3, so instead, we obtain matrices Mi using the easy-to-compute IBP upper bounds. Despite
this relaxation, we still obtain very tight expected bounds. Note that this slightly modified approach
reduces exactly to our bounds for two-layer networks.
True and Tight Bounds on Real Networks. Moreover, we also train small network, the architecture
is similar to Gowal et al. (2019), on the MNIST dataset 〜99%. To show that our bounds are also
true on real networks, and since the input dimension is too large for Monte-Carlo sampling, we use
the MIP formulation by Tjeng & Tedrake (2019). We then report Γ over varying testing . Table 1
demonstrates that indeed even on real networks beyond two layers and without the Gaussian weight
7
Under review as a conference paper at ICLR 2020
MNIST - Medium	MNIST - Large
MNlST-Small
8.76.54.32JO
SS ① utnnqoα ooæ
Test Accuracy
Nominal
IBP [⅛ain = 0.1]
IBP [⅛ain = 0.2]
IBP [⅛ain = 0.3]
OUΓS [Strain =。1]
OUTS [Etrain =。2]
OUrS [εtrain = 0.3]
Ours [εtrain = 0.4]
Figure 5:	Better Test Accuracy and Robustness on MNIST. We compare the PGD robustness and test
accuracy of three models (small, medium, and large) robustly trained on the MNIST dataset using our bounds
and those robustly trained With IBP. We have trained both methods using four different train, but We eliminated all
models With test accuracy loWer than 97.5%. Our results demonstrate an impressive trade-off betWeen accuracy
and robustness and, in some cases (medium and large models), We excel in both.
5 0 5 0 5
4 4 3 3 2
SS①Ulsnqoa αod
Test Accuracy
Nominal
IBP [εtrain = 2/255]
IBP [ɛtrain = 8/255]
Ours [εtrain = 2/255]
Ours [εtraiπ = 8/255]
Ours [εtrain = 16/255]
Ours [ɛtrain = 0∙l]
Figure 6:	Better Test Accuracy and Robustness on CIFAR10. We compare the PGD robustness and test
accuracy of three models (small, medium, and large) robustly trained on the CIFAR10 dataset using our bounds
and those robustly trained with IBP. We eliminated all models with test accuracy lower than 40.0%. PGD
robustness is averaged over multiple test (refer to appendix).
distributed assumption, our bounds are still a super set to the true bounds computed with an MIP
formulation. Moreover, Table 1 also demonstrates that our bounds are much tighter than IBP bounds.
Note that the reported results are averaged over 100 random MNIST images.
Qualitative Results. Following previous work (Wong & Kolter, 2018; Gowal et al., 2019), we
visualize examples of the proposed bounds in Figure 4 and compare them to the true ones for several
choices of ∈ {0.05, 0.1, 0.25} and a random five-layer fully-connected network with architecture
n-100-100-100-100-2. We also show the results of the Monte-Carlo sampling for an input size
n = 20. More qualitative visualizations for different values of n are in the appendix.
Training Robust Networks. Here, we conduct experiments showing that our expected bounds can be
used to robustly train DNNs. We compare our method against models trained nominally (i.e. only the
nominal training loss is used), and those trained robustly with IBP (Gowal et al., 2019). Given the well-
known robustness-accuracy trade off, robust models are often less accurate. Therefore, we compare
all methods using an accuracy vs. robustness scatter plot. Following prior work, we use Projected
Gradient Descent (PGD) (Madry et al., 2018) to measure robustness. We use a loss function similar
to the one proposed in (GoWal et al., 2019). In particular, We use L = '(fθ(x), ytrue) + κ'(z, ytrue),
where `, fθ (x), ytrue, and κ are the cross-entropy loss, output logits, true class label, and regularization
hyperparameter respectively. z represents the “adversarial" logits that combine the loWer bound of
the true label and the upper bound of all other labels, as in (GoWal et al., 2019). Nominal training
occurs When κ = 0. Due to the tightness of our bounds, in contrast to IBP, We folloW a standard
training procedure that avoids the need to vary κ or train during training.
Specifically, We train three netWork models (small, medium, large) provided by GoWal et al. (2019) on
both MNIST and CIFAR10. See appendix for more details. FolloWing the same setup in (GoWal et al.,
2019), We train all models With train ∈ {0.1, 0.2, 0.3, 0.4} and train ∈ {2/255, 8/255, 16/255, 0.1}
on MNIST and CIFAR10, respectively. In all experiments, and for stronger baselines and fair
comparison betWeen IBP training and our bounds employed in training, We grid search over
{0.1, 0.001, 0.0001} learning rates and employ a temperature over the logits With a grid of {1, 1/5}
as in (Hinton et al., 2015) and report the best performing models for both. Then, We compute PGD
robustness for every train of every model for all test ∈ {0.1, 0.2, 0.3, 0.4} for MNIST and for all
test ∈ {2/255, 8/255, 16/255, 0.1} for CIFAR10. To compare training methods, We compute the
average PGD robustness over all test and the test accuracy, and report them in a 2D scatter plot. We
8
Under review as a conference paper at ICLR 2020
report the performance results on MNIST and CIFAR10 for the small, medium, and large architectures
in Figure 5. For all trained architectures, we only report the results for those that achieve at least a
test accuracy of 97.5% and 40% on MNIST and CIFAR10, respectively; otherwise, it is an indication
of failure in training. Interestingly, our training scheme can be used to train all architectures for all
train. This is unlike IBP, which for example was only able to successfully train the large architecture
with train = 0.1 on MNIST. Moreover, models trained with our bounds always achieve better PGD
robustness than the nominally trained networks on all architectures while preserving similar if not
higher accuracy (on large networks). Models trained with IBP achieve high robustness but their test
accuracy is drastically affected. Several other experiments are left for the appendix.
5 Conclusion
In this work, we proposed new interval bounds that are tight, relatively cheap to compute, and true
in expectation. We analytically showed that for a Affine-ReLU-Affine block with large input and
hidden layer sizes, our bounds are true in expectation and can be several orders of magnitude tighter
than the bounds obtained with IBP. We conduct extensive experiments verifying our theory, even for
deep networks. As a result, we are able to train large models, with simple standard training routines
while achieving excellent trade-off between accuracy and robustness.
References
Modar Alfadly, Adel Bibi, and Bernard Ghanem. Analytical moment regularizer for gaussian robust
networks. arXiv:1904.11005, 2019.
Adel Bibi, Modar Alfadly, and Bernard Ghanem. Analytic expressions for probabilistic moments
of pl-dnn with gaussian input. In IEEE Conference on Computer Vision and Pattern Recognition
(CVPR18), 2018.
Chih-Hong Cheng, Georg NUhrenberg, and Harald Ruess. Maximum resilience of artificial neural
networks. In International Symposium on Automated Technology for Verification and Analysis,
2017.
RUdiger Ehlers. Formal verification of piece-wise linear feed-forward neural networks. In Interna-
tional Symposium on Automated Technology for Verification and Analysis, 2017.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv:1412.6572, 2014.
Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan
Uesato, Timothy Mann, and Pushmeet Kohli. On the effectiveness of interval bound propagation
for training verifiably robust models. International Conference on Computer Vision (ICCV), 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In IEEE Conference on Computer Vision and Pattern Recognition, 2016.
Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly,
Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al. Deep neural networks
for acoustic modeling in speech recognition: The shared views of four research groups. IEEE
Signal Processing Magazine, 2012.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. NIPS
Deep Learning and Representation Learning Workshop, 2015.
Pin-Yu Chen Cho-Jui Hsieh Huan Zhang, Tsui-Wei Weng and Luca Daniel. Efficient neural net-
work robustness certification with general activation functions. Advances in Neural Information
Processing Systems (NeurIPs), 2018.
Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer. Reluplex: An efficient
smt solver for verifying deep neural networks. In International Conference on Computer Aided
Verification, 2017.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.
Technical report, 2009.
9
Under review as a conference paper at ICLR 2020
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep con-
volutional neural networks. In Advances in Neural Information Processing Systems (NeurIPs),
2012.
Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998.
Alessio Lomuscio and Lalit Maganti. An approach to reachability analysis for feed-forward relu
neural networks. arXiv:1706.07351, 2017.
Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic
segmentation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. International Conference on
Learning Representations (ICLR), 2018.
Matthew Mirman, Timon Gehr, and Martin Vechev. Differentiable abstract interpretation for provably
robust neural networks. In International Conference on Machine Learning (ICML), 2018.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and
accurate method to fool deep neural networks. In IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2016.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal
adversarial perturbations. In IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), 2017.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
pytorch. Advances in Neural Information Processing Systems workshop, 2017.
Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certified defenses against adversarial
examples. International Conference on Learning Representations (ICLR), 2018.
Philip H.S. Torr Pushmeet Kohli-M. Pawan Kumar Rudy Bunel, Ilker Turkaslan. A unified view of
piecewise linear neural network verification. Advances in Neural Information Processing Systems
(NeurIPs), 2018.
Hadi Salman, Greg Yang, Huan Zhang, Cho-Jui Hsieh, and Pengchuan Zhang. A convex relaxation
barrier to tight robust verification of neural networks. Advances in Neural Information Processing
Systems (NeurIPs), 2019.
Swami Sankaranarayanan, Azadeh Alavi, Carlos D Castillo, and Rama Chellappa. Triplet probabilistic
embedding for face verification and clustering. In IEEE International Conference on Biometrics
Theory, Applications and Systems (BTAS), 2016.
Kai Xiao Tjeng, Vincent and Russ Tedrake. Evaluating robustness of neural networks with mixed
integer programming. International Conference on Learning Representations, (ICLR), 2019.
Shiqi Wang, Yizheng Chen, Ahmed Abdou, and Suman Jana. Mixtrain: Scalable training of formally
robust neural networks. arXiv:1811.02625, 2018a.
Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, and Suman Jana. Efficient formal safety
analysis of neural networks. In Advances in Neural Information Processing Systems (NeurIPs),
2018b.
Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, and Suman Jana. Formal security analysis
of neural networks using symbolic intervals. In 27th {USENIX} Security Symposium ({USENIX}
Security 18), 2018c.
Stefan Webb, Tom Rainforth, Yee Whye Teh, and M Pawan Kumar. A statistical approach to assessing
neural network robustness. International Conference on Learning Representations (ICLR), 2018.
Tsui-Wei Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Duane Boning, Inderjit S
Dhillon, and Luca Daniel. Towards fast computation of certified robustness for relu networks.
International Conference on Machine Learning (ICML), 2018a.
10
Under review as a conference paper at ICLR 2020
Tsui-Wei Weng, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, Dong Su, Yupeng Gao, Cho-Jui Hsieh, and
Luca Daniel. Evaluating the robustness of neural networks: An extreme value theory approach.
International Conference on Learning Representations (ICLR), 2018b.
Eric Wong and J Zico Kolter. Provable defenses against adversarial examples via the convex outer
adversarial polytope. International Conference on Machine Learning (ICML), 2018.
Eric Wong, Frank Schmidt, Jan Hendrik Metzen, and J Zico Kolter. Scaling provable adversarial
defenses. In Advances in Neural Information Processing Systems (NeurIPs), 2018.
Sen Wang Xiaowei Huang, Marta Kwiatkowska and Min Wu. Safety verification of deep neural
networks. International Conference on Computer Aided Verification, 2017.
11
Under review as a conference paper at ICLR 2020
Proposition 1. For a ∈ Rn 〜N(0, aaI) and a uniform random vector x 〜U[x — e1n, X + e1n]
where both a and X are independent, we have that Lyapunov Central Limit Theorem holds such that
1 n	n n
一 ^^(Xiai — E[θjXi]) →d N(0,1), where Sn = Var ∣ ^^(Xiai — E[ɑ⅛])
Sn i=1	∖i=1
where →d indicates convergence in distribution.
Proof. The Lyapunov condition
1n
∃δ > 0, 2+6〉：E ∣^IXiai — E[αiXi] ∣ + ^l
Sn i=ι L	」
→ 0, as n → ∞
is sufficient for Lyanunov Central Limit Theorem to hold. Note that
2	n	n ∕2∖	2	2 n
Sn = X Var (XiM = X E [α2x2] = σ2 X	+ G =蝙 F + X X
i=1	i=1	i=1 '	)	∖	i=1
⑺
(8)
Since for δ = 2, we have that
E[∣αiXi∣2+δ] = Z Z	a2X21■-exp (—岂)daidXi
J - 8 JXi- e	2E v 2πσa	∖ 2σa J
3σ4 ∕*χi+e ,	3σ4.	L	Lr
= 2^-	Xi dXi = ɪθɪ [(Xi + E) — (Xi - E)].
x Xi — e
Thereafter, Lyanunov Central Limit Theorem with δ = 2 is satisfied since
1 ʌ rl	一	一，2+δi
lim ~Γ X E [ IXiai- ElaiXi] 1	]
n→∞ s4 /一d	L	-I
n i=1
1n	4
nl→8 亚 X E[ I Xiai । ]
n i=1
lim
n→∞
3 En=I(Xi + E)5 — (Xi-E)5
S 婴 + £3 x2)2
1.3n ((Xmax + E)5 — (Xmin — E)')
≤ lim -------------------------2----= 0
-n→∞	10E (等 + Ei=1 x2)2
Proposition 2. For a random matrix Ai with i.i.d. Gaussian elements ofzero mean and aʌɪ standard
deviation and a uniform random vector X ~ U [x — E1n, X + E1n] we have that Covariance (AX)=
e2σAιn
-3-
+ aʌɪ trace (XXT) I I.
Proof. The former follows from the fact that
Covariance (AiX + bi) = Covariance (AiX) = E [AiXXTA>] — E [AiX] (E [AiX])t = E [AiXXtA>]
Ea1 [AiE [XXT] A>] = Ea1 Ai (Diag
E2
l3Eai [AiA>] + E [AixxtA>]
E2
-3-Eai [AiA>] + σA 1 trace (XXT) I =(
:)+ XXT) AT
E σ:ιn + σA 1 trace (XXT)) I
The last equality follows since:
(E [AixxTAT])ij = E [aTXXTaj] = trace (xxtE [ajaτ])
0	if i = j
ɑʌɪtrace (XXT) if i = j
□
□
12
Under review as a conference paper at ICLR 2020
Theorem 1. (True Bounds in Expectation) Let Assumption 1 hold. We have that for large input
dimension n,
EA1,a2 [Lm] ≤ EA1,a2 [Ltrue] , EA1,a2 [Utrue] ≤ EA1,a2 [Um] ∙
Proof.
LaPPrOx ≈ Ea2,y [a>max (y, 0) + b2] - m JVara2,y [a>max (y, 0) + b2]
b2 - m JEa2 [Vary (a>max (y, 0)+ b2∣a2)] + V%2 (Ey [a-max (y, 0) + b2∣a2])
b2 - m {a2 [(a> Θ a>) (Ey [max2 (y, 0)] - (Ey [max (y, 0)])2J + Vara2 (Ey [a/max (y, 0) + b2∣a2D )
1
C- k	k	"I	k	∖	2
X吟(Ey	[max2	(y,	0)])i - Xσ22	(Ey[max (y, 0)])2	+ Xσ22	(Ey[max (y,	0)])2
,i=1	i=1	」	i=1	/
b2 -
=b2 -
Note that ① follows by total expectation and total variance on the two terms, respectively. Lastly,
② follows from the closed form expression derived in Bibi et al. (2018) where Φ and φ are
the normal cumulative and probability Gaussian density functions, respectively. Note that y 〜
N (bi, (' σf* 1 n + σA 1 trace (XXT)) I)and that σyy = (' σf1 n + σA 1 trace (xxτ)
LaPPrOx - EA1,a2 [LM]
≈ b2 - mσa2Ψ - Ea1 ,a2 [a>M (Aix + bi) + b - e∣a>MAι∣1]
=EA1,a2 [e∣a>MAι∣1] - m^Ψ
n
=eEA1	XEa2 [∣a>MAι(:, j)∣∣A1]	- mσa2Ψ
j=i
①
=e
2 EAI
n _____________________
X JVara2 (a>MAi(:, j)))	- mσa2Ψ
j=i
—mσa2 Ψ
n k
Xt X Ai(i,j)21 {uj ≥ 0} -mσa2Ψ
j=i ∖ i=i
n k
EA1 Xt XAi(i,j)2 ∣ ∣S∣ - mσa2Ψ
j=1 ∖ i∈S
Note that ① follows from the mean of a folded Gaussian. The last equality follows by taking the
total expectation where S is the set of indices where uɪ ≥ 0 for all i ∈ S. Since ui is random, then
13
Under review as a conference paper at ICLR 2020
|S | is also random. Therefore, one can reparametrize the sum and thus we have
eσa2 ∖∣~ E∣s∣
EA1	t	A1(i,j)
||S|
j=1
i∈S
≥
n
k
2

The approximation follows from Stirlings formula γ(X+1/2)/r(x/2)≈ x∕2fo for large X where the last
inequality follows since E [p|S|] ≤ k].
Lapprox - EA1,a2 [LM] ≥
2σA1 σa2 n
√π
1-------
①
k
一
- mσa2
J
{z
②
Note that ① is O(n) while ② is O(√n). Thus, for sufficiently large input dimension n we have that
Lapprox ≥ EA1,a2 [LM] and since by construction EA1,a2 [Ltrue] ≥ Lapprox which completes the proof.
Note that a symmetric argument can be applied to show that EA1,a2 [Um] ≥ EA1,a2 [Utrue].	□
14
Under review as a conference paper at ICLR 2020
Theorem 2. (Tighter Bounds in Expectation) Consider an '∞ bounded uniform random variable
input x, i.e. x ∈ [x — e1n, X + e1n], to a block oflayers in theform OfAffine-ReLU-Affine (parame-
terized by Ai, bi, a2 and b2 for the first and second affine layers respectively) and a2 〜N (0, σɑ21).
Under the assumption that √2∏ χj 1>Ai(:,j) + 21n 1>bi ≥ e (∣∣Aι(:, j)g — √∏ ∣∣A1(:, j )仙)∀j,
we have: Ea2 [(Uibp — Libp) — (Um — LM)] ≥ 0.
Proof. Note that
[(UIBP — LIBP)-(UM — LM)] = e∣a> IlAIIIn + 2 ∣a> IIUIl- 2 ∣a> llɪil
—2el∣a>diag (1 {ui ≥ 0}) Ai||1„
Consider the coordinate splitting functions S++(.), S+-(.), S--(.) and S-+(.) such thatfor X ∈ Rn
S++(x) = x Θ 1 {uj ≥ 0, ɪi ≥ 0} where 1 {uj ≥ 0, ɪj ≥ 0} is a vector of all zeros and 1 in the
locations where both uj, ɪi ≥ 0. However, since ui ≥ ɪi, then S-+ (.) = 0. Therefore it is clear
that for any vector x and an interval [ɪi, ui], we have that
X = S++ (x) + S+- (x) + S-- (x),	(9)
since the sets {i; UL ≥ 0, ɪi ≥ 0}, {i; uɪ ≥ 0, ɪi ≤ 0} and {i; uɪ ≤ 0, ɪi ≤ 0} are disjoints and
their union {i = 1, i = 2,..., i = k}. We will denote the difference in the interval lengths as
WIB P — Wm for ease of notation. Thus, we have the following:
WIBP — WM = Es++ (|a> l) l A1 l1n + eS+ ^- (|a> l) lA1l1n + ES	(|a>1)l A1 l1n + $S++ (|aT l) |u1l
+ 2S+- (∣a>I) Iuil + 2S-- (∣a>∣) ∣uι∣ — 2S++ (∣a>∣) ∣h∣ — 2S+- (∣a>∣) ∣h∣
—2S--(Ia>∣) lɪil — 2e (S++(a>) + S+-(a>) + S--(a>)) diag (1 {ui ≥ 0}) 1n
=2eS++ (∣a>∣) ∣Ai∣1n + S+- (∣a>∣) (AiX + bi) + eS+- (∣a>∣) Ai 1n
—2e (S++ (a>) + S+- (a>)) Ai 1八
= 2eS++ (∣a>∣) ∣Ai∣1n + S+- (∣a>∣) ui — 2e (S++ (a>) + S+- (a>)) Ai 1n .
'----------V--------' '------V-----'
①	②	、	{z	}
③
Note that we used the property of the coordinate splitting functions defined in Eq 9 along with the
definitions of The previous The penultimate equality follows since S++(.) and S+-(.) corresponds
to the indices that are selected by ɪi and uɪ. The penultimate equality follows since S++ and S+-
corresponds to the indices that are selected by diag (1 {ui ≥ 0}).
Now by taking the expectation over a2, we have for ①：
k
2eE [S++ (∣a>∣∣Ai∣)] 1n = 2e XE [∣a2∣] ∣Ai(i, :)|1 {u； ≥ 0, ɪi ≥ 0} 1n
i=i
2eσa2 rT- X ∣Ai(i, :)ll {i1 ≥ 0} 1n
V	π i=i
/n n k
7 2 £ ElAi (i,j)∣l {11 ≥ 0}
V	j=ii=i
The second equality follows from the mean of the folded Gaussian. and the fact that ui ≥ ɪi.
15
Under review as a conference paper at ICLR 2020
For ②,We have:
E [S+-(|a>1) ul] = σa2 W Xu；l {u1 ≥ 0, liι ≤ 0}
π i=1
Lastly, for ③,We have:
2E
(S++(a>) + S+-(a>)) Ai
Using Holder,s inequality, i.e.	E[|x|]	≤	，E[x2], per coordinate of the vector
[Pk=i Ai(i, ：)a2 (l{ui ≥ 0})]
and by binomial expansion, We have at the jth coordinate
E
2t
k
X Ai(i, j)a21 {ui ≥ 0}
i=1
2
1
2e (X (Ai(i,j ))2 E [(a2)2i 1 {u1 ≥ 0} +2 XX
Aι(i,j)Aι(z,j)E [a2a2] 1 {u； ≥ 0} 1 {呜 ≥ 0})
2eJX(Ai (i,j))2 E [(a2)2i l{u1 ≥。} = 2”2 t
k
X (A(i,j))2 l{Ui ≥ 0}
i=1
The second equality folloWs from by the independence of ai2 and that they have zero mean. Therefore
it follows from ③ that:
2E
nuk
(S++(A>) + S+-(A>)) A；	1n ≤ 2eσa2 Xt X (A；(i,j))2 1 {ui ≥ 0}
j=1	i=1
Lastly, putting things together, i.e. E [① +② -③] We have that
/ 2 n k	/ 2 k
E [WIBP - WM] ≥ 2σa2√2ΣΣ∣Aι(i,j)∣l {I； ≥ 0} + σa2√ — EUII {ui ≥ 0」；≤ 0}
π j=1 i=1	π i=1
nu k
- 2eσa2 Xt X Aι(i,j)2l {u； ≥ 0}.
j=；	i=；
(10)
Note that to shoW that the previous inequality is non-negative, it is sufficient to shoW that the previous
inequality is non-negative for the non-intersecting sets {i : li； ≥ 0} and {i : ui； ≥ 0, li； ≤ 0}. Thus
the right hand side can be Written as the sum of tWo sets.
For the set {i : li； ≥ 0}, the RHS of inequality 10 reduces to
2σa
X( JnkAi(:,j)ki-kAi(:,j)k
(11)
For the set {i : ui； ≥ 0, li； ≤ 0} and using the definition of u；, the RHS of inequality 10 reduces to
16
Under review as a conference paper at ICLR 2020
厂，k ( n	n
σa2√ 2 XlX Aι(i,j)Xj+ bi + E X ∣Aι(i,j)∣
i=1 j=1
j=1
σa2 ʌ/∏ X 卜 j1> AIC, j ) + n 1>b + EkAIC,j )k
n
- 2σa2 X kA1(:,j)k2
j=1
n
1	- 2σa2	kA1(:,j)k2
j
X 卜a2 ∖∣~∏ (xj1>AlC, j) + n 1>b + EkAIC, j)kl) - 2Eσa2 kA1(:, j)∣∣2
(12)
Note that given the assumption in the Theorem where √2∏Xj1>Aι(:, j) + * 1>b ≥ 0 ≥
E (kAi(:,j)k2 - √2∏kAi(:,j)ki) ∀j , then if both Eq 11 and Eq 12 are non-negative complet-
ing the proof.	□
Lemma 1. For X ∈ Rk 〜N (0, I),
where k ≥ 5 we have that E
[√2π kxk1 - 2kxk2]
≥ 0.
Proof. Note that by the mean of a folded Gaussian, we gave that E [∣∣x∣∣ι] = Pk E [|xi]] = ky ∏
Moreover, note that
E[kXk2]=E
E [√y]
γ( 2)
1	∕∞χkexp
0
dx
2 ⅛11k+1)
22-1Γ(2)
Note that y is Chi-Square random variable and that f√y(x) = 2xfy(x2)=
xk-1
2 2-1Γ(2)
where the third inequality follows by integrating by parts recursively. Lastly, the last approximation
follows by stirling’s approximation for large k.
Proposition 3. For a random matrix A1 ∈ Rk×n
with i.i.d elements such Aι(i,j)〜N (0,1) ,then
□
EAi (k ai(：, j)∣∣2—√2∏ k ai(：, j)∣∣ι)=√2 γ( 2)，-kr∏ ≈ √k 1- - r∏ √k).
Proof. The proof follows immediately from Lemma 1.
□
17
Under review as a conference paper at ICLR 2020
A More Qualitative Results of the New Bounds
We conduct several more experiments to showcase the tightness of our proposed bounds to the
true bounds and compared them against propagating the bounds layerwise [lDM, UDM] for random
n - 100 - 100 - 100 - 100 - 2 networks initialized with N(0,1∕√n) similar to Wong & Kolter
(2018). We show our bounds compared to the polytobe estimated from MonteCarlo sampling on
results for n ∈ {2, 10, 20} and ∈ {0.05, 0.1, 0.25}. The layer wise bound propagation is shown in
the tables as the bounds were too loose to be presented visually.
Figure 7: Each row represents 5 different randomly initialized networks for a given with n = 2.
Note that the proposed bounds are far from being true this is as predicted by Theorem 1 for small n.
, Figure Number	I	lD M	I	UDM	I	lD M	UDM
= 0.05, Figure Number = 1	-10.1261	19.0773	-18.1500	13.3573
= 0.05, Figure Number = 2	-12.2529	14.3428	-14.4295	12.3479
= 0.05, Figure Number = 3	-12.6594	14.1837	-12.5873	12.2612
= 0.05, Figure Number = 4	-17.7825	16.4048	-15.3843	15.1688
= 0.05, Figure Number = 5	-12.5260	11.1149	-8.9242	12.7539
= 0.1, Figure Number = 1	-27.4598	23.6603	-17.9481	23.4817
= 0.1, Figure Number = 2	-23.2877	34.0542	-28.1535	21.8703
= 0.1, Figure Number = 3	-35.2950	36.4901	-31.7465	36.0421
= 0.1, Figure Number = 4	-31.7154	29.3062	-30.3900	35.7105
= 0.1, Figure Number = 5	-25.0870	39.4373	-24.5087	32.5493
= 0.25, Figure Number = 1	-54.0557	56.2884	-52.5686	73.9621
= 0.25, Figure Number = 2	-59.2115	82.8742	-75.7999	65.6898
= 0.25, Figure Number = 3	-50.1142	56.2330	-72.4221	54.4631
= 0.25, Figure Number = 4	-52.6030	83.3950	-92.8100	69.1401
= 0.25, Figure Number = 5	-89.1335	43.4685	-74.4519	91.5137
Table 2: Shows the interval bounds obtained by propagating ∈ {0.05, 0.1, 0.25} with n = 2 and
denoted as lD1 M, u1DM for the first output function of the 2-dimensional output network (shown along
the x-axis in the previous figure) while lD2M and u2DM is for the other function (shown along the y-axis
in the previous figure).
18
Under review as a conference paper at ICLR 2020
Figure 8: Each row represents 5 different randomly initialized networks for a given with n = 10.
Note how the bounds are more likely now to enclose the true output region for all given compared
to previous case where n = 2.
, Figure Number	lD M	I UDM	I	lD M I	UDM
= 0.05, Figure Number = 1	-16.9716	24.9259	-21.2584	20.6358
= 0.05, Figure Number = 2	-42.6267	48.1786	-38.6958	37.7851
= 0.05, Figure Number = 3	-41.4147	36.4056	-42.0363	36.6605
= 0.05, Figure Number = 4	-32.1013	25.1485	-37.7864	33.3652
= 0.05, Figure Number = 5	-45.4368	32.9774	-44.8946	38.6805
= 0.1, Figure Number = 1	-48.1221	86.6800	-54.3059	71.2724
= 0.1, Figure Number = 2	-51.2668	46.1237	-38.8089	33.6512
= 0.1, Figure Number = 3	-51.3915	52.4437	-52.7149	49.1031
= 0.1, Figure Number = 4	-71.7738	54.4836	-91.0335	37.0950
= 0.1, Figure Number = 5	-48.1744	33.2927	-40.9540	47.2282
= 0.25, Figure Number = 1	-152.7639	192.4156	-188.4030	148.2482
= 0.25, Figure Number = 2	-196.8923	195.4355	-163.2691	177.3766
= 0.25, Figure Number = 3	-141.6800	207.5414	-207.9396	190.2823
= 0.25, Figure Number = 4	-200.7513	156.2560	-227.6427	182.0180
= 0.25, Figure Number = 5	-153.3898	164.8314	-147.8662	137.4380
Table 3: Shows the interval bounds obtained by propagating ∈ {0.05, 0.1, 0.25} with n = 10 and
denoted as lD1 M, u1DM for the first output function of the 2-dimensional output network (shown along
the x-axis in the previous figure) while lD2M and u2DM is for the other function (shown along the y-axis
in the previous figure).
19
Under review as a conference paper at ICLR 2020
Figure 9: Each row represents 5 different randomly initialized networks for a given with n = 20.
The bounds almost always enclose the polytope computed from Monte-Carlo compared to n = 2, 10.
, Figure Number	lD M	I UDM	lD M I	UDM
= 0.05, Figure Number = 1	-43.6689	32.8572	-47.7856	36.9842
= 0.05, Figure Number = 2	-53.2447	47.1651	-46.5306	53.1638
= 0.05, Figure Number = 3	-59.1694	42.6647	-43.4659	57.2781
= 0.05, Figure Number = 4	-39.8479	42.4197	-42.1962	39.7649
= 0.05, Figure Number = 5	-54.3150	42.8637	-44.5742	43.8117
= 0.1, Figure Number = 1	-83.5804	81.9034	-97.5203	98.8713
= 0.1, Figure Number = 2	-64.8464	76.8083	-84.9223	83.9505
= 0.1, Figure Number = 3	-70.5862	92.6652	-88.6098	71.8915
= 0.1, Figure Number = 4	-78.0557	151.4360	-106.073	123.3686
= 0.1, Figure Number = 5	-91.8368	97.3438	-103.2845	76.6581
= 0.25, Figure Number = 1	-188.7623	256.2275	-211.3972	255.5101
= 0.25, Figure Number = 2	-219.5642	274.5287	-217.7622	349.4256
= 0.25, Figure Number = 3	-214.7457	160.7498	-186.5554	184.1767
= 0.25, Figure Number = 4	-188.7623	256.2275	-211.3972	255.5101
= 0.25, Figure Number = 5	-276.9137	177.7929	-202.2031	245.8731
Table 4: Shows the interval bounds obtained by propagating ∈ {0.05, 0.1, 0.25} with n = 20 and
denoted as lD1 M, u1DM for the first output function of the 2-dimensional output network (shown along
the x-axis in the previous figure) while lD2M and u2DM is for the other function (shown along the y-axis
in the previous figure).
20
Under review as a conference paper at ICLR 2020
B Experimental Setup for Training DNNs
small	medium	large
CONV 16 × 4 × 4+2 CONV 32 × 4 × 4 + 1 FC 100	CONV 32 X 3 X 3+1 CONV 32 X 4 X 4 + 2 CONV 64 X 3 X 3+1 CONV 64 X 4 X 4 + 2 FC512 FC512	CONV 64 X 3 X 3+1 CONV 64 X 3 X 3+1 CONV 128 X 3 X 3 + 2 CONV 128 X 3 X 3 + 1 CONV 128 X 3 X 3 + 1 FC 200
Table 5: Architectures for the three models trained on MNIST and CIFAR10. "CONV p × w × h+ s",
correspond to p 2D convolutional filters with size (w × h) and strides of s. While "FC d" is a fully
connected layer with d outputs. Note that the last fully connected layer is omitted.
21
Under review as a conference paper at ICLR 2020
C PGD Robustness on Specific Input Bounds on MNIST
Nominal
050505050
.0,9,9,8,8.7.7,6.6
Lo.o.o.o.o.o.o.o.
SS①ESnqOH CIDd
Figure 10: Compares PGD (test = 0.1) and test accuracy of our models against IBP on MNIST.
in =。,1 ]
in = 0.2]
in = 0.3]
in = 0.1]
in = 0.2]
in = 0.3]
in = 0.4]
IBP
IBP
IBP
Ours
Ours
Ours
Ours
MNIST-SmaII [εtest = 0.2]
8 6 4 2 0
60.0.0.0.
SSeSnqOH CIDd
Test Accuracy
IBP
IBP
IBP
Ours
Ours
Ours
Ours
Nominal
[ɛtrain =	1 ]
[ɛtrain = 0.2 ]
[ɛtrain = 0.3 ]
[ɛtrain = 0. 1 ]
[ɛtrain = 0.2 ]
[Etrain = O.3 ]
[ɛtrain = 0.4 ]
Figure 11: Compares PGD (test = 0.2) and test accuracy of our models against IBP on MNIST.
SS①ESnqOH CIDd
876543210
0.660.0.0.0.0.6
SS ① u-KnqoH CIDd
Figure 12: Compares PGD (test = 0.3) and test accuracy of our models against IBP on MNIST.
Nominal		
IBP	[ɛtrain	= 0,1]
IBP	[ɛtrain	= 0.2]
IBP	[ɛtrain	= 0.3]
Ours	[ɛtrain	= 0,1]
Ours	[ɛtrain	= 0.2]
Ours	[ɛtrain	= 0,3]
Ours	[ɛtrain	= 0.4]
♦ Nominal
IBP
IBP
IBP
Figure 13: Compares PGD (test = 0.4) and test accuracy of our models against IBP on MNIST.
22
Under review as a conference paper at ICLR 2020
D PGD Robustness on Specific Input Bounds on CIFAR10
CIFAR10 - Small [⅛st = 2/255]
0.9
Figure 14: Compares PGD (test = 2/255) and test accuracy of our models against IBP on CIFAR10.
CIFAR10 - Medium [εtest = 2/255]
0.5	0.6
Test Accuracy
CIFAR10 - Large [εtest = 2/255]

SS ① utnnqoH ClDd
3 2 10 1
6 6 60.6
CIFAR10 - Small [εtest = 8/255]
CIFAR10 - Medium [εtest = 8/255]
0.4	0.5	0.6	0.7	0.8	0.4	0.5	0.6	0.7	0.8
Test Accuracy
0.4	0.5	0.6	0.7	0.8
CIFAR10 - Large [εtest = 8/255]
♦ Nominal
■ IBP [⅛ain = 2/255]
■ IBP [εtrain = 8/255]
• Ours [ɛtrain = 2/255]
• Ours [εtrain = 8/255]
• Ours [εtrain = 16/255]
OUΓS [ɛtrain = 0∙l]
Figure 15: Compares PGD (test = 8/255) and test accuracy of our models against IBP on CIFAR10.
0.5
CIFAR10 - Small [εtest = 16/255]
CIFAR10 - Large [εte≡t = 16/255]
4 3 2 1
60.60.
SSESnqOa αed
CIFAR10 - Medium [εtest = 16/255]
Nominal
IBP [⅛≡	n = 2/255]
IBP [⅛a	n = 8/255]
Ours [⅛a	n = 2/255]
Ours [⅛a	n = 8/255]
Ours [⅛a	n = 16/255]
Ours [εtra	n = 0.1]
Figure 16: Compares PGD (test
= 16/255) and test accuracy of our models against IBP on CIFAR10.
SS①UZnqOH CIDd
Figure 17: Compares PGD (test = 0.1) and test accuracy of our models against IBP on CIFAR10.
23
Under review as a conference paper at ICLR 2020
E	Failure Example for Our Bounds
In this section, we show a faliure example to when the network weights do not follow Gaussian
Assumption 1, our bounds can be far too loose even compared to IBP. Consider a two layer neural
network where A1 = 1000 In×n, b1 = -999 1n, a2 = -10 1n and b2 = 0 for the interval
[-1n, 1n]. Then, we have that
L U = bi 干 e∣Aι∣1n
=1n (干 1000 - 999)= -1999 1n, 1n.
Since u ≥ 0, then M = In×n . Thereafter our estimated bounds as given by Equation 3 are given as:
Lm, UM = a>bι 干 e∣a>A41n
=9990n 干 10000 e1> 1n = -10n, 19990n.
As for IBP bounds, they are given as follows:
> max(u, 0n) + max(l, 0)	> max(u, 0n) - max(l, 0)
LIBP, UIBP = a2 (---------2----------J 干 |a2 | 1--------2---------
10 >	10 >
=-ɪ 1n 1n 干 ɪ 1n 1n = -10n, 0∙
Under this construction of weights to the network, different of the i.i.d Gaussian assumption, itis clear
that our bounds can be orders of magnitude looser to IBP. In the following section, we demonstrate
that networks trained on real data do have weights that are not far from the Gaussian assumption by
empirically investigating the histogram of the network weights.
24
Under review as a conference paper at ICLR 2020
F Histogram of Weights of Trained Networks
To quantify how reasonable is the assumption of Gaussian i.i.d weights in trained networks, we train
medium CNN (discussed in previous section) and show the histogram of the weights of the first 6
layers. The network is trained with and without `2 regularization on three datasets namely MNIST,
CIFAR10 and CIFAR100.
F.1 Histogram of Weights on MNIST
Figure 18: Histogram of weights of medium CNN trained on MNIST without `2 regularization.
Figure 19: Histogram of weights of medium CNN trained on MNIST with `2 regularization.
25
Under review as a conference paper at ICLR 2020
F.2 Histogram of Weights on CIFAR 1 0
Figure 20: Histogram of weights of medium CNN trained on CIFAR10 without `2 regularization.
Figure 21: Histogram of weights of medium CNN trained on CIFAR10 with `2 regularization.
26
Under review as a conference paper at ICLR 2020
F.3 Histogram of Weights on CIFAR 1 00
Figure 22: Histogram of weights of medium CNN trained on CIFAR100 without `2 regularization.
Figure 23: Histogram of weights of medium CNN trained on CIFAR100 with `2 regularization.
27
Under review as a conference paper at ICLR 2020
G Rebuttal
G.1 Reviewer 2
We thank R2 for their very thorough review of our paper. Regarding the disclaimer, we want to
clarify to other reviewers too that we have indeed tried to address all R2’s previous concerns in this
submission. We have conducted some extra experiments from the previous submission; we have also
done some serious changes to the structure and presentation of the paper as previously suggested by
R2. In what follows we try addressing R2’s current concerns.
•	The logical argument explaining the "why" these are valid is harder to understand and could
be clarified. My understanding so far ...
The overall description of R2 to the logical argument is correct. The proof is based on showing that
our bounds are ’supersets’ to some approximate bounds Lapprox and Uapprox. The approximate bounds
are indeed related to the unknown true bounds through Assumption 1. Note that Assumption 1 states
that there exists some m where the Assumption inequality holds. As we pointed out to R1, the larger
the constant m, the larger n has to be for Theorem 1 to hold. Please refer to the last inequality in the
proof of Theorem 1.
•	The caption of Figure 1 is misleading. It says that it shows that the proposed ...
True. This was a mistake of phrasing on our part. We have addressed this in the revised version.
•	The reporting of Figure 5 and 6 is weird because according to the text, each datapoint seems
to be the average robustness of ...
We have addressed this in the revised version. The reported models had only their robustness averaged
over multiple test. Please note that the robustness results over each individual test were left for the
appendix. The best models from the grid search over the hyperparameters are reported for both our
bound training and IBP training for fair comparison.
•	I appreciate the effort of the author to include experiments involving a MIP solver returning
the true bounds. This is very helpful in building confidence that the bounds generated are
correct. How is the real network trained? Is this based on a robustly trained network or is it
just standard training? Is 99% the nominal accuracy?
Yes, 99% is the nominal accuracy of the trained models. The network was trained normally without
any special regularization.
•	In general, I think that the paper would be better if there was more discussion of the failure
modes of the method. There are easy to identify failure cases where the proposed bound is
incorrect or loose. My opinion is that the paper would be stronger if it acknowledged them ...
We agree with R2. We have addressed this in the revised version. In particular, we reflected this
in the paragraph just before section 3.3. We have added in the appendix the example of the failure
case pointed out by R2. Moreover, as suggested by R1, we have investigated, at least empirically,
the histogram of the weights of a normally trained network on 3 real dataset in appendix F. In what
follows, we re-state our response to R1 in regards to the histogram results of the appendix to R2
for completion. "For the MNIST experiments, even without `2 regularization, the weights seem to
follow a bell shaped. With `2 regularization, the histogram tend to be much pointy. Of course, the
regularization parameter used in training interpolates between the two histograms, the larger the
regularization the more ’pointy’ the histogram will tend to be. A similar observation can be noted
for CIFAR10 and CIFAR100. We want to emphasize that while this is not a proof that the Gaussian
assumption holds in practice, we believe that at least the distribution of the weights does not seem
to be far from Gaussian and thereafter sheds light on why perhaps such an assumption may not be
utterly unreasonable on real networks."
•	The assumption about gaussian weights in A1 and a2 seems strong but is at least partially
motivated at the end of 3.2 (although not all networks are ...
R2 is correct. The i.i.d assumption over all elements of the weights matrix A breaks in the presence
of topelitz/circulant structure convolutional layers have. This, unfortunately, is as far as the current
theory can go. We find though that this have little impact on practically upon training the networks.
28
Under review as a conference paper at ICLR 2020
•	I think that the paper would benefit from having some discussion of other methods that can
derive "bounds" which may not actually be bounds. The works by Stefan Webb (A Statistical
...
Indeed, the two referenced works are related to our direction since both aim at either probabilistically
carrying out the verification or by estimating the lower bound of the minimum perturbation. We
elaborated on this in the revised version and added a paragraph in the introduction.
•	While reviewing the paper, I also spotted some strong similarities between the methods pro-
posed (particularly subsection 3.3) and the Fastlin method. The computation mechanism of
Fastlin (propagate recursively from the end through the linear layers and through diagonal
matrices that replace the ReLU activation function) is exactly the same as the proposed one
(Fastlin goes through the ...
The description of R1 for the intuition of the construction of the diagonal matrix M is correct. Indeed,
there are similarities between both our work and Fastlin. However, we do not believe that the works
are limited. Both works still provide an excellent trade-off between efficiency and computational
complexity, even when over estimating the intermediate upper bounds for the intermediate M, the
approximation for the superset bounds are still effective.
•	Minor comments, typos and references. We have addressed the issues in the revised version. We
have also tried to address other issues and writing style highlighted by R2.
G.2 Reviewer 1
We thank R1 for the time spent in reviewing our paper. Follows our response to R1’s concerns.
•	Some assumptions seem pretty unrealistic to me. For instance, the fact that the neural net-
work should have Gaussian i.i.d weights: it is thus ...
As correctly pointed out by R1, Gowal et al do not make such an assumption. They provide loose ’true’
bounds for deep architectures as a function of network parameters by simply computing worst case
bound per layer. As suggested by R1, we have conducted an experiment where we trained Medium
CNN with identical structure to previous experiments on MNIST, CIFAR10 and on CIFAR100 with
and without `2 regularization. We then plot the histogram of the weights of the 6 layers in the network
(3 convolutional and 3 fully connected). We report the results in Section E of the appendix in the
revised version. For the MNIST experiments, even without `2 regularization, the weights seem to
follow a bell shaped. With `2 regularization, the histogram tend to be much pointy. Of course, the
regularization parameter used in training interpolates between the two histograms, the larger the
regularization the more ’pointy’ the histogram will tend to be. A similar observation can be noted
for CIFAR10 and CIFAR100. We want to emphasize that while this is not a proof that the Gaussian
assumption holds in practice, we believe that at least the distribution of the weights does not seem
to be far from Gaussian and thereafter sheds light on why perhaps such an assumption may not be
utterly unreasonable on real networks.
•	A major difference with (Gowal et al) is that: here, the bounds are in expectation whereas
the bound in (Gowal et al) are deterministic. In this paper, there are some approximation
assumptions ...
This is true. Unfortunately, as far as the analysis goes, we do not yet have concentration results. One
direction of interest is to study the asymptotic behaviour (in terms of n and k) of the variance of
LM and UM over the distribution of network parameters. So far we have some little progress in that
direction as the analysis becomes untraceable.
•	Most of the proofs are long and complicated, and several equations of 7-8 lines could be
summarized with up to 2-3 lines maximum. I had a hard time to understand the proof of
Theorem 1, which is simply some algebra. This could be improved.
The equations were, for most part purposely, left to span multiple lines as we wanted to elaborate on
each step of the analysis.
•	I’m curious of the imagenet performance: couldn’t this technique be easily applied to
AlexNet, which is nowadays simple to manipulate? Is there a technical issue to do so?
29
Under review as a conference paper at ICLR 2020
While we were interested for the most part in the analysis, we do believe that the paper can benefit
from the ImageNet experiments vastly. They were left out for time/computation capacity related
reasons. We will try to address this within our capacity.
•	Is the assumption 1 really an assumption? given a,a’ and b,b’ there always exists m such that
a>=b-m and a’<=b’+m, like m>=max(|a-b|,|a’-b’|). Am I wrong? I think I do not understand
this assumption...
Yes, Assumption 1 is trivial. The key element to note from Assumption 1 is that there is some m
(that could be very large) where this inequality holds. Now, the question is how does this constant m
affect the analysis? Larger m results into requiring a larger input dimension n for Theorem 1 to hold.
To see this, refer to the last equation of the proof of Theorem 1. Note that the second term of the
right hand side of the inequality is a function of m. This shows that the larger the constant m for the
Assumption 1 to hold, the larger the input dimension for the inequality to be positive and thereafter
for Theorem 1 to hold.
•	How simple is it to extend those theoretical results for NNs to the case of CNNs?
The results hold for generic linear operators A and a2. For convolutional layers, the kernel can indeed
be represented with a structured topelitz/circulant matrix fitting into our analysis. However, we find a
trick to avoid the inefficient construction of such massive matrices. Note that to compute our bounds,
one only needs to compute Eq (3) efficiently where A1 is convolutional kernel. For A1x, this can be
computed by simply performing convolution to the input center x without constructing A1 (which is
a forward pass through the layer). The tricky part comes upon computing the term |a2>MA1 |1n. As
one need to compute |a2>MA1 | first where M is a diagonal matrix without constructing A1. The way
we go about this is by simply taking a backward pass through the layer that computes a2>MA1 for
some input center x as a function of x. That is we perform a forward pass for some input center x and
then compute the gradients as a function of x resulting in having access to a2>MA1 and thereafter
computing |a2>MA1 |1n becomes efficient. Note that this has been employed in the experiments as
the networks used did have convolutional layers.
G.3 Reviewer 3
We thank R3 for their time reviewing the paper. Follows our response.
•	However, these results shown in Figs 6 and 7 appear to be somewhat inconsistent with those
reported in the paper of IBP [Gowal et al., 2018)].
We would like to raise to the attention of R3 that the metric used here is PGD robustness and not
PGD accuracy. While they are to some extent related, the PGD robustness is the percentage of the
testing samples where the network prediction was not altered under PGD attacks from the prediction
of the network on the original samples regardless if the prediction is correct. Since the pretrained
models of IBP were not made available, we trained models with IBP by running the code provided by
the authors on their github page (with the same exact setup). Moreover, for a fair comparison and
stronger baselines, all IBP trained models were trained over different learning schedules. The best
models with the highest accuracy-robustness pair trade-off for IBP are reported.
•	I am not sure if it really makes sense to discuss which method is better in such ranges of
failed defense.
We believe that so long the model accuracy on the noise free-samples is still preserved (captured on
the x-axis of Figures 5 and 6) with an increase in robustness, this is good evidence for improvement.
For instance, for CIFAR10 on the small architecture (Figure 6), our models have improved the average
robustness over all test (captured by y-axis) while preserving similar accuracy to the nominal model.
This is unlike IBP, where despite that the robustness improved 30% the accuracy dropped by 20%
compared to the nominal model.
•	Sources for differences to Gowal et. al.
Indeed, as correctly pointed by R3, the reported robustness is averaged over all tested . However,
note that we report the robustness results for individual test in the supplementary material (Figures
10-17). The range of test for MNIST is identical to that of Gowal et. al.; however, the range for test
is indeed larger for CIFAR10 experiments.
30
Under review as a conference paper at ICLR 2020
•	Comment on the metric PGD Robustness and not PGD accuracy.
The focus of our work is to show an efficient approach to estimate the output bounds of a network.
Since the bounds are only ’true’ in expectation they can not be used for certification in the absence
of concentration results to our bounds. To that extent, verified accuracy (accuracy by running exact
solvers) and its lower and upper bound estimates, certified accuracy and PGD accuracy respectively,
do not make much sense in our framework. Therefore, we limit our experiments to showcasing that
such simple estimates of the bounds can be utilized to efficiently regularize networks to improve their
robustness.
31