Under review as a conference paper at ICLR 2020
Lipschitz Lifelong Reinforcement Learning
Anonymous authors
Paper under double-blind review
Ab stract
We consider the problem of knowledge transfer when an agent is facing a series
of Reinforcement Learning (RL) tasks. We introduce a novel metric between
Markov Decision Processes and establish that close MDPs have close optimal value
functions, that is that optimal value functions are Lipschitz continuous with respect
to tasks. These theoretical results lead us to a value transfer method for Lifelong
RL, which we use to build a PAC-MDP algorithm with improved convergence rate.
We illustrate the benefits of the method in Lifelong RL experiments.
1	Introduction
Lifelong Reinforcement Learning (RL) is an online problem where an agent faces a series of RL
tasks, drawn sequentially. Transferring the knowledge of prior experience while solving new tasks is
a key question in that setting (Lazaric, 2012; Taylor and Stone, 2009). We elaborate on the intuitive
idea that similar tasks should allow a large amount of transfer. An agent able to compute online
a similarity measure between source tasks and the current target task should be able to perform
transfer. By measuring the amount of inter-task similarity, we design a novel method for value
transfer, practically deployable in the online Lifelong RL setting. Specifically, we introduce a metric
between MDPs and prove that the optimal Q-value function is Lipschitz continuous with respect to
MDPs. This property allows to compute a provable upper-bound on the optimal value function of
a target task, given the learned optimal value function of a source task. Knowing this upper bound
allows to accelerate the convergence of an RMax algorithm (Brafman and Tennenholtz, 2002). This
transfer method is non-negative (it cannot cause performance degradation) as the computed upper
bound does not underestimate the optimal Q-value function.
Our contributions are as follows. First, we study theoretically the Lipschitz continuity of the optimal
value function in the task space (Section 3). Then, we use this continuity property to propose a
value-transfer method based on a local distance between MDPs (Section 4). Full knowledge of both
MDPs is not required and the transfer is non-negative, which makes the method both practical and
safe. In Section 4.2, we build a PAC-MDP algorithm called Lipschitz RMax, applying this transfer
method online in the Lifelong RL setting. We provide sample and computational complexity bounds
and showcase the algorithm in Lifelong RL experiments (Section 5).
2	Background and related work
Reinforcement Learning (RL) (Sutton and Barto, 1998) is a framework for sequential decision making.
The problem is typically modeled as a Markov Decision Process (MDP) (Puterman, 2014) consisting
in a 4-tuple hS, A, R, Ti where S is a state space, A an action space, Rsa is the expected reward of
taking action a in state s and Tsas0 is the transition probability of reaching state s0 when taking action
a in state s. Without loss of generality, we assume Rsa ∈ [0, 1]. Given a discount factor γ ∈ [0, 1),
the expected cumulative return Pt γt Rsatt obtained along a trajectory starting with state s and action
a is noted Q(s, a) and called the Q-function. The optimal Q-function Q* is the highest attainable
expected return from s, a and V*(s) = maXa∈A Q*(s, a) is the optimal value function in s.
Lifelong RL (Silver et al., 2013; Brunskill and Li, 2014) is the problem of experiencing online a
series of MDPs drawn from an unknown distribution. Each time an MDP is sampled, a classical RL
problem takes place where the agent is able to interact with the environment to maximize its expected
return. In this setting, it is reasonable to think that knowledge gained on previous MDPs could be
re-used to improve the performance in new MDPs. In this paper, we provide a novel method for such
1
Under review as a conference paper at ICLR 2020
transfer by characterizing the way the optimal Q-function can evolve across tasks. We restrict the
scope of the study to the case where sampled MDPs share the same state-action space S × A. For
brevity, we will refer indifferently to MDPs, models or tasks, and write them M = hR, Ti.
Using a metric between MDPs has the appealing characteristic of quantifying the amount of similarity
between tasks, which intuitively should be linked to the amount of transfer achievable. Song et al.
(2016) define a metric based on the bi-simulation metric introduced by Ferns et al. (2004) and the
Wasserstein metric (Villani, 2008). Value transfer is performed between states with low bi-simulation
distances. However, this metric requires knowing both MDPs completely and is thus unusable in the
Lifelong RL setting where we expect to perform transfer before having learned the current MDP.
Further, the transfer technique they propose does allow negative transfer (see Appendix, Section A).
Carroll and Seppi (2005) also define a value-transfer method based on a measure of similarity
between tasks. However, this measure is not computable online and thus not applicable to the
Lifelong RL setting. Mahmud et al. (2013) and Brunskill and Li (2013) propose MDP clustering
methods respectively using a metric quantifying the regret of running the optimal policy of one MDP
in the other MDP and the L1 norm between the MDP models. An advantage of clustering is to
prune the set of possible source tasks. They use their approach for policy transfer, which differs
from the value-transfer method proposed in this paper. Ammar et al. (2014) learn the model of a
source MDP and view the prediction error on a target MDP as a dissimilarity measure in the task
space. Their method makes use of samples from both tasks and is not readily applicable to the online
setting considered in this paper. Lazaric et al. (2008) provide a practical method for sample transfer,
computing a similarity metric reflecting the probability of the models to be identical. Their approach
is applicable in a batch RL setting as opposed to our online setting. The approach developed by Sorg
and Singh (2009) is very similar to ours in the sense that they prove bounds on the optimal Q-function
for new tasks, assuming that both MDPs are known and that a soft homomorphism exists between the
state spaces. Brunskill and Li (2013) also provide a method that can be used for Q-function bounding
in multi-task RL. Abel et al. (2018) present the MaxQInit algorithm, providing transferred bounds on
the Q-function with high probability while preserving PAC-MDP guarantees. Given a set of solved
tasks, they derive the probability that the maximum over the Q-values of previous MDPs is an upper
bound on the current task’s optimal Q-function. This results in a method for non-negative transfer
with high probability once enough tasks have been sampled.
3	Lipschitz continuity of Q-functions
The intuition we build on is that similar MDPs should have similar optimal Q-functions. Formally,
this insight can be translated into a continuity property of the optimal Q-functions over the MDP
space M. The remainder of this section mathematically formalizes this intuition that will be used
in the next Section to derive a practical method for value transfer. To that end, we introduce a
local pseudo-metric characterizing the distance between the models of two MDPs at a particular
state-action pair. A reminder and a detailed discussion on the metrics (and related objects) used
herein can be found in the Appendix, Section B.
Definition 1. Given two tasks M = hR,Ti and M =(R, Ti, and afunction f : S → R+, we define
the pseudo-metric between models at (s, a) ∈ S × A w.r.t. f as:
DMM (s, a)，∣rs - Rai + X f (s0)∣τaso - Ta ∣.	⑴
s0∈S
This pseudo-metric is relative to a positive function f . We implicitly cast this definition in the context
of discrete state spaces. The extension to continuous spaces is straightforward but beyond the scope
of this paper. Let QM denote the optimal Q-function of MDP M ∈ M.
Proposition 1 (Local PseUdo-LiPschitz continuity). For two MDPs M, M ,for all (s, a) ∈ S ×A,
IQM(s,a)- QM(s,a)i ≤ Δmm(s,a),
with the MDPs local PseUdo-metric ∆MM(s, a)
(2)
and the local MDP
min{dM(S,a),dM(S, a)}
dissimilarity dMM : S × A → R is the unique solution to the following fixed-point equation for d:
d(s, a) = DMM(s, a) + Y ^X TsSO maxd(s0, a0).
,mm	S⅛S	a0 ∈A
(3)
2
Under review as a conference paper at ICLR 2020
All the proofs of the paper can be found in the Appendix. This result establishes that the distance
between the optimal Q-functions of two MDPs at (s, a) ∈ S × A is controlled by a local dissimilarity
between the MDPs. The latter follows a fixed-point equation (Equation 3), which can be solved by
Dynamic Programming (DP) (Bellman, 1957). Note that, although the local MDP dissimilarity dMM is
asymmetric, ∆MM (s, a) is a pseudo-metric, hence the name pseudo-Lipschitz continuity. Similar
results for the value function of a fixed policy and the optimal value function VMM Can easily be
derived, as well as a global pseudo-Lipschitz continuity property (Appendix, Sections D and E). Thus,
overall, the optimal Q-functions of two close MDPs (in the sense of Equation 1) are themselves close
to each other. A direct consequence in Lifelong RL is that previously solved tasks can help bound the
value function of a new task. Even a partially learned Q-function can be used for that purpose if error
bounds are known or if it provably overestimates the true Q*. The next section exploits this property
to build a PAC-MDP algorithm that performs provably non-negative transfer between successive
tasks and accelerates learning.
4	Transfer using the Lipschitz continuity
A purpose of value transfer, when interacting online with a new MDP, is to initialize the value
function and drive the exploration to accelerate learning. We aim to exploit value transfer in a method
guaranteeing three conditions: C1. the resulting algorithm is PAC-MDP (Strehl et al., 2009); C2. the
transfer accelerates learning; C3. the transfer is non-negative. From Proposition 1, one can naturally
define a local upper bound on the optimal Q-function of an MDP given the optimal Q-function of
another MDP.
Definition 2. Given two tasks M and M, for all (s, a) ∈ S ×A, the Lipschitz upper bound on QM
induced by QMM is defined as UMM (s, a) ≥ QM (s, a) with:
UM (S, a) , QMM (S, a) + δMM (S, a)∙	(4)
The optimism in the face of uncertainty principle leads to consider that the long-term expected return
from any state is the 匚q maximum return, unless proven otherwise. The RMax algorithm (Brafman
and Tennenholtz, 2002) in particular explores an MDP so as to shrink this upper bound. RMax is a
model-based, online RL algorithm with PAC-MDP guarantees (Strehl et al., 2009) which means that
convergence to near-optimal policy is guaranteed in a polynomial number of steps. It relies on an
optimistic model initialization that yields an optimistic upper bound U on the optimal Q-function,
then acts greedily w.r.t. U. By default, it takes the maximum value U(s, a) = ι-1γ but any tighter
upper bound is admissible. Thus, shrinking U with Equation 4 is expected to improve the learning
speed for new tasks in Lifelong RL.
In RMax, during the resolution of a task M, S × A is split into a subset of known state-action
pairs K and its complement Kc of unknown pairs. A state-action pair is known if the number of
collected reward and transition samples allows estimating an -accurate model in L1-norm with
probability higher than 1 - δ. We refer to and δ as the RMax precision parameters. This translates
into a threshold nknown on the number of visits n(S, a) to a pair S, a that are necessary to reach this
precision. Given the experience of a set of m MDPS M = {M 1,..., Mm}, We define the total bound
as the minimum over all the Lipschitz bounds induced by each previous MDP.
Proposition 2. Given a partially known task M = hR, Ti, the set of known state-action pairs K,
and the set OfLipschitz bounds on QM induced by previous tasks {Umɪ, ∙ ∙ ∙, UMm }, thefunCtion Q
defined below is an upper bound on Q*M for all S, a ∈ S × A.
Rsa +γ	Tsas0 maxQ(S0,a0) if (S, a) ∈ K,
Q(S, a),	s0∈S	a0	(5)
U(S, a)	otherwise,
with U(S, a)
min
a) .
Traditionally in RMax, Equation 5 is solved to a precision Q via Value Iteration. This yields a
function Q that is a valid heuristic (provable upper bound on Q*M) for the exploration of MDP M.
3
Under review as a conference paper at ICLR 2020
4.1	A TRACTABLE UPPER BOUND ON QM
The key issue addressed in this Section is how to actually compute U (s, a). Consider two tasks M
and M, on which vanilla RMax has been applied, yielding the respective sets of known state-action
. TT 1 ≠>	1	∙.ι ,ι 1	1	1 1 nʌr ∕r^ A∖ 1 nʌr ∕λ≠1 A∖ 1 ,i	1	1
pairs K and K, along with the learned models M = hT, Ri and M = hT, Ri, and the upper bounds


Q and Q respectively on QM and QM. Equation 5 allows the transfer of knowledge from M to M if
UM(s, a) can be computed. Unfortunately, the true optimal value functions, transition and reward
models, necessary to compute UM, are unknown. Thus, we propose to compute a looser upper bound
based on the learned models and value functions. First, we provide an upper bound DDMM on the
pseudo metric between models M and M.
Proposition 3. Given two tasks M and M, K and K the respective sets ofstate-aCtion pairs where
their models are known with accuracy in L1-norm with probability at least 1 - δ,
Pr (DMM(s,a) ≥ DMM(s, a)) ≥ 1 - δ
with the following definition of the upper bound on the pseudo-metric between models DMM:
	'DMM (s,a) + 2B	if (s,a) ∈ K ∩ K
DMM(S, a)，<	mM DYV (s,a) + B ^ max DNM(s, a) + B μ∈M YV _	/ max2 Dμv(s,a) lμ,μ∈M2 YV	if (s, a) ∈ K ∩ KC if (s,a) ∈ Kc ∩ K if (s,a) ∈ Kc ∩ KC
(6)
where B = E(1 + Y max§o V(S0)).
This upper bound D mm on the distance between MDPs can be calculated analytically (see Appendix,
Section H). The magnitude of the B term is controlled by E. In the case where no information is
available on the maximum value of V, B = ι-γ. E measures the accuracy with which the tasks are
known: the smaller e, the tighter the B bound. Note that V is used as an upper bound on the true
VM. In many cases, max§o VM (s0)《1-1^; e.g. for stochastic shortest path problems, which feature
rewards only upon reaching terminal states, max§o VM(s0) = 1 and thus B = (1+ Y)E is a tighter
bound for transfer. Using DMM and Equation 3, one can derive an upper bound dM on dM, detailed
in Proposition 4.
Proposition 4. Given two tasks M and M, K the set ofstate-action pairsforwhichhR, T〉is known
with accuracy E in Li -norm with probability at least 1 一 δ. If γ(1 + E) < 1, the solution dM of the
^
following fixed-point equation on d is an upper bound on dMM with probability at least 1 - δ:
{Y( P T：。o max d(s0,a0) + E max d(s0,a0)) if s, a ∈ K,
s0∈S ss0 a0∈A	s0,a0∈S×A	(7)
γ max d(s0,a0) otherwise.
s0 ,a0∈S×A
Similarly as in Proposition 3, the condition Y(1 + E) < 1 illustrates the fact that for a large return
horizon (large Y), a high accuracy (small E) is needed for the bound to be computable. Finally, a
tractable upper bound on QM given M with high probability is given by
ʌ 一
UM(s,a) = Q(s, a) +min
{dM (s,a),dM (s,a)卜
(8)
And the associated upper bound on U(s, a) (Equation 5) given previous tasks M = {Mi}m=i is
U(s, a) = min {11γ,UMI (s,a),…，UM m (s,a)}
(9)
This upper bound can be used to transfer knowledge from a partially solved task to a target task.
If U(s, a) ≤ ι1γ for some (s, a) pairs, then the convergence rate can be improved. As complete
knowledge of both tasks is not needed, it can be applied online in a Lifelong RL setting. In the next
section, we explicit an algorithm that leverages this value transfer method.
4
Under review as a conference paper at ICLR 2020
Algorithm 1: Lipschitz RMax algorithm
Initialize M = 0.
for each newly sampled MDP M do
Initialize Q(s, a) = ι-1γ, ∀s, a, and K = 0
Initialize T and R (RMax initialization)
Q J UPdateQ(M, T, R)
for t ∈ [1, max number of steps] do
s = current state, a = arg max Q(s, a0)
a0
Observe reward r and next state s0
n(s, a) J n(s, a) + 1
if n(s, a) < nknown then
L Store (s, a, r, s0)
if n(s, a) = nknown then
Update K, TsaSO and Ra
Q J UPdateQ(M, T, R)
Save MM =(T, R, K, Q) in M
Function UPdateQ(M, T, R):
for M ∈M do
Compute DDMM and DDMM (Eq. 6)
Compute dM and dlM (DP on Eq. 7)
ʌ
Compute UM (Eq. 8)
Compute U (Eq. 9)
Compute and return Q (DP on Eq. 5 using U)
4.2	Lipschitz RMax
In Lifelong RL, MDPs are encountered sequentially. Applying RMax to task M yields the set of
known state-action pairs K, the learned models T and R, and the upper bound Q on QM. Saving this
information when the task changes allows to compute the upper bound of Equation 9 for the new task,
and to use it to shrink the optimistic heuristic of RMax. This effectively transfers value functions
between tasks based on task similarity. As the new task is explored online, the task similarity is
progressively assessed with better confidence, refining the values of DDMM, dM and eventually U,
allowing for more efficient transfer where the task similarity is appraised. The resulting algorithm,
Lipschitz RMax (LRMax), is presented in Algorithm 1. To avoid ambiguities with M, We use M
to store learned features (T, R, K, Q) about previous MDPs. In a nutshell, the behavior of LRMax
on a given task M is precisely that of RMax, but with a tighter admissible heuristic U that becomes
better as the new task is explored (while this heuristic remains constant in vanilla RMax). LRMax
is PAC-MDP (Condition C1) as stated in Propositions 5 and 6 below. With S = |S | and A = |A|,
the sample complexity of vanilla RMax is (O(S2A/(e3(1 一 γ)3)), which is improved by LRMax in
ʌ
Proposition 5 and meets Condition C2. Finally U is a proved upper bound with high probability on
QM, which avoids negative transfer and meets Condition C3.
Proposition 5 (Sample complexity (Strehl et al., 2009)). With probability 1 一 δ, the greedy policy
w.r.t. Q computed by LRMax achieves an -optimal return in MDP M after
O SS∣{s,a ∈ S ×A | U(s,a) ≥ VM(s) — e}|、
[	e3(1-γ )3	)
samples (when logarithm!CfaCtors are ignored), with U defined in Equation 9 a non-static, decreasing
quantity, upper bounded by ι1γ.
Consequently from Proposition 5, the sample complexity of LRMax is no worse than that of RMax.
Proposition 6 (Computational complexity). The total computational complexity of Lipschitz RMax is
O (τ + S2A2(S +log(AM2N +1) log -ɪ)
(1 一 γ)	Q(1 一 γ)
with τ the number of interaction steps, Q the precision of value iteration and N the number of tasks.
5
Under review as a conference paper at ICLR 2020
4.3	Refining LRMax bounds with maximum model distance
LRMax relies on upper bounds on the local distances between tasks (Equation 7). The quality of
the Lipschitz bound on Q*M greatly depends on the quality of those estimates and can be improved
accordingly. We discuss two methods to provide finer estimates.
First, from the definition of DMVM(s, a), it is easy to show that model pseudo-distances are al-
ways upper bounded by 1+γ. However, in practice, the tasks experienced in Lifelong RL might
not cover the full span of possible MDPs and may be systematically closer to each other than
1+γ. For instance, the distance between two games in the Arcade Learning Environment (ALE)
(Bellemare et al., 2013), is smaller than the maximum distance between any two MDPs defined
on the common state-action space of the ALE (extended discussion in Appendix, Section L). Let
DmaX(s, a)，maxM,M∈M2 {DMM (s, a)} be the maximum model distance at a particular s, a pair.
Prior knowledge might indicate a smaller upper bound for DmaX(s, a) than 1+-γ. We will note such
an upper bound DmaX. Solving Equation 7 boils down to accumulating DMM(s, a) values in 才(s, a).
Reducing a DMM(s, a) estimate in a single (s, a) pair actually reduces d(s, a) in all (s, a) pairs.
Thus, replacing DMM(s, a) in Equation 7 by min{DmaX, DMM(s, a)}, provides a smaller upper
bound dM on dM, and thus a smaller U which allows transfer if it is lesser than -γ. Consequently,
such an upper bound DmaX can make a difference between successful and unsuccessful transfer,
even if its value is of little importance. Conversely, setting a value for DmaX quantifies the distance
between MDPs where transfer is efficient.
Furthermore, one can estimate online the value of Dmax (s, a), lifting the previous hypothesis of
available prior knowledge. One can build an empirical estimate of the maximum model distance at s, a:
DDmaχ(s, a)，maxM M∈m^2{DDMM(s, a)}, M being the set of explored tasks. The pitfall being
that, with few explored tasks, DDmaχ(s, a) could underestimate DmaX(s, a). Proposition 7 provides a
lower bound on the probability that DDmaχ(s, a) does not underestimate DmaX(s, a), depending on
the number of sampled tasks. In turn this indicates when DDmaχ(s, a) upper bounds DmaX(s, a) with
high probability, which can be combined with Algorithm 1 to improve the performance.
Proposition 7. Consider an algorithm producing -accurate in L1-norm model estimates with
probability at least 1 - δ for a subset of S × A after interacting with an MDP. For all s, a ∈ S × A,
after sampling m tasks with pmin = minM ∈M Pr(M), the following lower bound holds:
Pr (DDmax(s, a) ≥ DmaX(s, a)) ≥ 1 - 2(1 - Pmin)m + (1 - 2Ρmin)m .
The assumption of a lower bound pmin on the sampling probability of a task implies that M is finite
and is commonly seen as a non-adversarial task sampling strategy (Abel et al., 2018).
5	Experiments
The experiments reported here1 illustrate how 1) LRMax allows for early performance increase
in Lifelong RL by efficiently transferring knowledge between tasks; 2) the Lipschitz bound of
Equation 8 improves the sample complexity compared to RMax by providing a tighter upper bound
on Q*. Graphs are displayed with 95% confidence intervals. Information in line with the Machine
Learning Reproducibility Check-list (Pineau, 2019) is documented in the Appendix, Section Q.
We evaluate different variants of LRMax in a Lifelong RL experiment. The RMax algorithm will
be used as a no-transfer baseline. LRMax(x) denotes Algorithm 1 with prior DmaX = x. MaxQInit
denotes the MaxQInit algorithm from Abel et al. (2018), consisting in a state-of-the art PAC-MDP
algorithm achieving transfer with PAC guarantees. Both LRMax and MaxQInit algorithms achieve
value transfer by providing a tighter upper-bound on Q* than ι-1γ. Computing both upper-bounds
and taking the minimum results in combining the two approaches. We include such a combination in
1Link to open-source code omitted for anonymity.
6
Under review as a conference paper at ICLR 2020
UjnsrtPeaUnOOSla ①A--①rt①MEJ①AV
—∙— RMax	-*一 MaxQInit
—■— LRMax	-L— LRMaxQInit
.LRMax(0.2)	―•— LRMaxQInit(0.1)
—*— LRMax(0.1)
(a) Average discounted return vs. tasks
0	250 500 750 1000 1250 1500 1750 2000
Episode number
(b) Average discounted return vs. episodes
UJnsJ p-unoɔsɪp eAI⅛-H
RMax Task = 1	MaxQInit Task = 11
----LRMax(0.1) Task = 1	MaxQInit Task = 12
LRMax(0.1) Task = 2
1.0	0.8	0.6	0.4	0.2	0.0
Prior knowledge (known upper-bound on maxs,a = DMM(s, a))
(c) Discounted return for specific tasks	(d) Algorithmic properties vs. Dmax
Figure 1: Experimental results
our study with the LRMaxQInit algorithm. Similarly, LRMaxQInit(x) consists in the latter algorithm,
benefiting from prior knowledge Dmax = x.
The environment we used in all experiments is a variant of the “tight” environment used by Abel
et al. (2018). This is a 11 × 11 grid-world, the initial state is in the centre, actions are the cardinal
moves (Appendix, Section M). The reward is zero everywhere except for the three goal cells in the
upper-right corner. Each time a task is sampled, a new reward value is drawn from [0.8, 1] for each of
the three goal cells and a probability of slipping (performing a different action than the one selected)
is picked in [0, 0.1]. Hence, tasks have different reward and transition functions. We sample 15
tasks in sequence among a pool of 5 possible different sampled tasks. Each is run for 2000 episodes
of length 10. The operation is repeated 10 times to provide narrow confidence intervals. We used
nknown = 10, δ = 0.05 and = 0.01 (discussion in Appendix, Section P). We drew tasks from a
finite set of five MDPs. This allows the application of MaxQInit and the subsequent comparison
below. Note, however, that LRMax does not require the set of MDPs to be finite, which is a noticeable
advantage in applicability. Other lifelong RL experiments are reported in the Appendix, Section N.
The results are reported in Figure 1. Figure 1a displays the discounted return for each task, averaged
across episodes. Similarly, Figure 1b displays the discounted return for each episode, averaged
across tasks (same color code as Figure 1a). Figure 1c displays the discounted return for five specific
instances, detailed below. To avoid inter-task disparities, all the aforementioned discounted returns
are displayed relatively to an estimator of the optimal expected return for each task. For readability
purposes, Figures 1b and 1c display a moving average over 100 episodes. Figure 1d reports the
benefits of various values of Dmax on the algorithmic properties.
In Figure 1a, we first observe that LRMax benefits from the transfer method, as the average discounted
return increases as more tasks are experienced. Moreover, this advantage appears as early as the
second task. Conversely, the MaxQInit algorithm needs to wait for task 12 before benefiting from
7
Under review as a conference paper at ICLR 2020
transfer. As suggested in Section 4.3, various amounts of prior allow the LRMax transfer method to
be more or less efficient: a smaller known upper-bound DmaX on DMM causes a larger discounted
return gain. Combining both approaches in the LRMaxQInit algorithm outperforms all other methods.
Episode-wise, we observe in Figure 1b that the LRMax transfer method allows for faster convergence,
hence decreases the sample complexity. Interestingly, LRMax features three stages in the learning
process. 1) The first episodes are characterized by a direct exploitation of the transferred knowledge,
causing these episodes to yield high payoff. This is due to the combined facts that the Lipschitz
bound of Equation 8 is larger on promising regions of S × A seen on previous tasks and the fact
that LRMax acts greedily w.r.t. that bound. 2) This high performance regime is followed by the
exploration of unknown regions of S × A, in our case yielding low returns. Indeed, as promising
regions are explored first, the bound becomes tighter for the corresponding state-action pairs, enough
for the Lipschitz bound of unknown pairs to become larger, thus driving the exploration towards
low payoff regions. Such regions are quickly identified and never revisited thereafter. 3) Eventually,
LRMax stops exploring and converges to the optimal policy. Importantly, in all experiments, LRMax
never features negative transfer as supported by the provability of the Lipschitz upper-bound with
high probability. This is indeed demonstrated by the fact that it is at least as efficient in learning as
the no-transfer RMax baseline.
Figure 1c displays the collected returns of RMax, LRMax(0.1), and MaxQInit for specific tasks. We
observe that LRMax benefits from the transfer as early as task 2, where the aforementioned 3-stages
behavior is visible. Again, MaxQInit needs to wait for task 12 to leverage the transfer method.
However, the bound it provides are tight enough to allow for almost zero exploration of the task.
In Figure 1d, we display the following quantities for various values of DmaX : ρLip , is the ratio of the
time the Lipschitz bound was tighter than the RMax bound ι-1γ; PSpeed-up, is the relative gain of
time steps before convergence when comparing LRMax to RMax. This quantity is estimated based on
the last updates of the empirical model M; PReturn, is the relative total return gain on 2000 episodes
of LRMax w.r.t. RMax. First, we observe an increase of ρLip as DmaX becomes tighter. This means
that the Lipschitz bound of Equation 8 becomes effectively smaller than y-1γ. This phenomenon
leads to faster convergence, indicated by PSpeed-up . Eventually, this increased convergence rate
allows for a net total return gain, illustrated by the increase of PReturn .
Overall, in this analysis, we have showed that LRMax benefits from an enhanced sample complexity
thanks to the value transfer method. The knowledge of a prior DmaX further increases this benefit.
The method is comparable to the MaxQInit method and has some advantages such as the early fitness
for use and the applicability to infinite sets of tasks. Moreover, the transfer is non-negative while
preserving the PAC-MDP guarantees of the algorithm. Additionally to the analysis performed here,
we show in the Appendix, Section O that, when provided with any prior knowledge DmaX, LRMax
increasingly stops using this prior as the task is explored. This confirms the claim of section 4.3 that
providing DmaX enables transfer even if it’s value is of little importance.
6 Conclusion
We have studied theoretically the Lipschitz continuity property of the optimal Q-function in the MDP
space. This led to a local Lipschitz continuity result, establishing that the optimal Q-functions of two
close MDPs are themselves close to each other. This distance between Q-functions can be computed
by Dynamic Programming. We then proposed a value-transfer method using this continuity property
with the Lipschitz RMax algorithm, practically implementing this approach in the Lifelong RL setting.
The algorithm preserves PAC-MDP guarantees, accelerates the learning in subsequent tasks and
performs non-negative transfer. Potential improvements of the algorithm were discussed in the form
of prior knowledge introduction on the maximum distance between models and online estimation
with high probability of this distance. We showcased the algorithm in lifelong RL experiments and
demonstrated empirically its ability to accelerate learning. The results also confirm that no negative
transfer occurs, regardless of parameter settings. It should be noted that our approach can directly
extend other PAC-MDP algorithms (Szita and Szepesvari, 2010; Rao and Whiteson, 2012; Pazis et al.,
2016; Dann et al., 2017) to the Lifelong setting. In hindsight, we believe this contribution provides a
sound basis to non-negative value transfer via MDP similarity, a development that was lacking in
the literature. Key insights for the practitioner lie both in the theoretical analysis and in the practical
derivation of a transfer scheme that achieves non-negative transfer with PAC guarantees.
8
Under review as a conference paper at ICLR 2020
Acknowledgements
Omitted for anonymity.
References
David Abel, Yuu Jinnai, Sophie Yue Guo, George Konidaris, and Michael Littman. Policy and Value
Transfer in Lifelong Reinforcement Learning. In International Conference on Machine Learning,
pages 20-29, 2018.
Haitham Bou Ammar, Eric Eaton, Matthew E Taylor, Decebal Constantin Mocanu, Kurt Driessens,
Gerhard Weiss, and Karl Tuyls. An automated measure of MDP similarity for transfer in rein-
forcement learning. In Workshops at the Twenty-Eighth AAAI Conference on Artificial Intelligence,
2014.
M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An
evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253-279,
2013.
Richard Bellman. Dynamic programming. Princeton, USA: Princeton University Press, 1957.
Ronen I. Brafman and Moshe Tennenholtz. R-max-a general polynomial time algorithm for near-
optimal reinforcement learning. Journal of Machine Learning Research, 3(Oct):213-231, 2002.
Emma Brunskill and Lihong Li. Sample complexity of multi-task reinforcement learning. arXiv
preprint arXiv:1309.6821, 2013.
Emma Brunskill and Lihong Li. Pac-inspired option discovery in lifelong reinforcement learning. In
International Conference on Machine Learning, pages 316-324, 2014.
James L. Carroll and Kevin Seppi. Task similarity measures for transfer in reinforcement learning
task libraries. In Proceedings. 2005 IEEE International Joint Conference on Neural Networks,
2005., volume 2, pages 803-808. IEEE, 2005.
Christoph Dann, Tor Lattimore, and Emma Brunskill. Unifying PAC and regret: Uniform PAC bounds
for episodic reinforcement learning. In Advances in Neural Information Processing Systems, pages
5713-5723, 2017.
Norm Ferns, Prakash Panangaden, and Doina Precup. Metrics for finite Markov decision processes.
In Proceedings of the 20th conference on Uncertainty in artificial intelligence, pages 162-169.
AUAI Press, 2004.
Alessandro Lazaric. Transfer in reinforcement learning: a framework and a survey. In Reinforcement
Learning, pages 143-173. Springer, 2012.
Alessandro Lazaric, Marcello Restelli, and Andrea Bonarini. Transfer of samples in batch reinforce-
ment learning. In Proceedings of the 25th international conference on Machine learning, pages
544-551. ACM, 2008.
MM Mahmud, Majd Hawasly, Benjamin Rosman, and Subramanian Ramamoorthy. Clustering
Markov decision processes for continual transfer. arXiv preprint arXiv:1311.3959, 2013.
Jerzy Neyman. X—outline of a theory of statistical estimation based on the classical theory of
probability. Philosophical Transactions of the Royal Society of London. Series A, Mathematical
and Physical Sciences, 236(767):333-380, 1937.
Jason Pazis, Ronald E. Parr, and Jonathan P. How. Improving PAC exploration using the median of
means. In Advances in Neural Information Processing Systems, pages 3898-3906, 2016.
Joelle Pineau. Machine learning reproducibility checklist. https://www.cs.mcgill.ca/
〜jpineau/ReproducibilityChecklist .pdf, 2019. Version 1.2, Mar.27 2019.
Martin L. Puterman. Markov decision processes: discrete stochastic dynamic programming. John
Wiley & Sons, 2014.
9
Under review as a conference paper at ICLR 2020
Karun Rao and Shimon Whiteson. V-MAX: tempered optimism for better PAC reinforcement learning.
In Proceedings of the 11th International Conference on Autonomous Agents and Multiagent
Systems, pages 375-382, 2012.
Daniel L. Silver, Qiang Yang, and Lianghao Li. Lifelong machine learning systems: Beyond learning
algorithms. In 2013 AAAI spring symposium series, 2013.
Jinhua Song, Yang Gao, Hao Wang, and Bo An. Measuring the distance between finite Markov
decision processes. In Proceedings of the 2016 international conference on autonomous agents
& multiagent systems, pages 468-476. International Foundation for Autonomous Agents and
Multiagent Systems, 2016.
Jonathan Sorg and Satinder Singh. Transfer via soft homomorphisms. In Proceedings of The 8th
International Conference on Autonomous Agents and Multiagent Systems-Volume 2, pages 741-748.
International Foundation for Autonomous Agents and Multiagent Systems, 2009.
Alexander L. Strehl, Lihong Li, and Michael L. Littman. Reinforcement learning in finite MDPs:
PAC analysis. Journal of Machine Learning Research, 10(Nov):2413-2444, 2009.
Richard S. Sutton and Andrew G. Barto. Introduction to reinforcement learning, volume 135. MIT
press Cambridge, 1998.
Istvan Szita and Csaba Szepesvari. Model-based reinforcement learning with nearly tight exploration
complexity bounds. In Proceedings of the 27th International Conference on Machine Learning,
pages 1031-1038, 2010.
Matthew E. Taylor and Peter Stone. Transfer learning for reinforcement learning domains: A survey.
Journal of Machine Learning Research, 10(Jul):1633-1685, 2009.
Cedric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media,
2008.
10
Under review as a conference paper at ICLR 2020
Appendix
A A negative transfer example
In their paper, Song et al. (2016) propose two transfer methods based on the metric between MDPs
they introduce, stemming from the bi-simulation metric introduced by Ferns et al. (2004). The
intuition is that, for a new target task, the value function of the closest source task in terms of that
metric is used as an initialization. However, if no similar source task is available, using the closest
task’s value function as an initialization can lead to negative transfer. We here understand negative
transfer as the fact that it prevents a learning algorithm to converge to the optimal policy while
interacting with a new task. We make the hypothesis that the learning algorithm acts greedily w.r.t.
the current Q-value function. This is for example the behavior of the RMax algorithm (Brafman and
Tennenholtz, 2002). We now illustrate a negative transfer case with an example. Let us consider the
2-states MDP of Figure 2. We assume that the transitions are deterministic and the initial state is
Figure 2: 2-states MDP
always s0. In the first MDP M1 ∈ M, the reward is 0 everywhere except for Rsa0 = 1. In the second
MDP M2 ∈ M, the reward is 0 everywhere except for Rsa11 = 1. With a discount factor γ = 0.9,
the value functions and Q-functions of both MDPs are summarized in Table 3 Using the weighted
	VM 3)	QMI (∙, ao)	QMI (∙, aC	VM 2(∙)	QM 2 (∙,aO)	QM2 (∙, aQ
s0	10	10	8.1	4.74	4.26	4.74
s1	9	8.1	9	5.26	4.74	5.26
Figure 3: Value functions and Q-functions of MDPs M1 and M2
transfer technique from M1 to M2 proposed by Song et al. (2016) (Definition 4.1), the Q-function
described below is used as an initialization for the exploration of M2 .
transfer QM2	(s0, a0)	2.03
transfer QM2	(s0,a1)	2.25
transfer QM2	(s1,a0)	2.5
transfer QM2	(s1,a1)	2.03
First, QtMransfer does not respect the principle of “optimism under the face of uncertainty” that often
results in sound and efficient exploration (Strehl et al., 2009; Brafman and Tennenholtz, 2002; Sutton
and Barto, 1998). Further, a greedy policy w.r.t. QtMransfer would never discover the state-action pair
s1 , a1 in M2 which is the maximum-reward pair. Instead, the agent would go from s0 to s1 and
perform self-loops thereafter.
As a conclusion, this negative transfer example motivates the need for distance between MDPs not
only to account for the best-source task to use for transfer but also to discourage the transfer when
the distance is too high. The approach we develop in this paper used the distance to build optimistic
upper-bounds on the Q-function. Those upper-bounds are simply of no use when the distance is too
high which is equivalent as avoiding transfer.
11
Under review as a conference paper at ICLR 2020
B Discussion on metrics and related notions
A metric on a set X is a function m : X × X → R which has the following properties for any
x, y, z ∈ X:
1.	m(x, y) ≥ 0,
2.	m(x, y) = 0 ⇔ x = y,
3.	m(x, y) = m(y, x),
4.	m(x, z) ≤ m(x, y) + m(y, z).
With only m(x, x) = 0 instead of property 2, m would be a pseudo-metric. Without property 3, one
has a quasi-metric. Without property 3 and 4, and when X is a set of probability measures, one has a
divergence.
In Definition 1, DM f(s,a) is indeed a pseudo-metric over MDPs since the choice of f can lead to a
zero distance between different models.
The local MDP dissimilarity between MDPs dMM (s, a) of Proposition 1 does not respect properties 2
and 3, hence the name dissimilarity. The ∆MM(s, a)
, min{dM(s,a),dM(S, a)}
quantity, however,
regains property 3 and is hence a pseudo-metric. An important consequence is that Proposition 1
is “in the spirit” of a Lipschitz continuity theorem but cannot be called as such, hence the name
pseudo-Lipschitz continuity.
The same goes for the global dissimilarity dM = ι-γ maxs,a∈s×/ [DM,7v* (s, a)]. However,
using min {dM, dM∣ allows to regain property 3 and makes this quantity a pseudo-metric again
between MDPs.
C Proof of Proposition 1
Lemma 1. Given two MDPS M and M, this equation on d is a fixed-point equation admitting a
unique solution which we call dMM
d(S, a) = DMM,γV * (S, a) + γ	Tsas0 max d(S0, a0), ∀S, a ∈ S × A.
s0
Proof of Lemma 1. The proof follows closely that in Puterman (2014) that proves that the Bellman
operator over value functions is a contraction mapping. Let d1 and d2 be two functions from S × A
to R and let L be the functional operator that maps any function d : S × A → R to
Ld : S, a 7→ DM V * (S, a) + γ	Tss0 max d(S , a ).
,γ M	aa	a0
s0
Then Ld1(S, a) - Ld2(S, a)	= γ s0 Tsas0 [maxa0 d1(S0, a0) - maxa0 d2(S0, a0)]. But
maxa0 d1(S0, a0) - maxa0 d2(S0, a0) ≤ maxa0 [d1 (S0, a0) - d2(S0, a0)] ≤ kd1 - d2k∞. And so
kLd1 - Ld2k∞ ≤ γkd1 - d2 k∞. Since γ < 1, L is a contraction mapping in the metric space
(S ×A, k∙ k∞). This metric space being complete and non-empty, it follows from Banach fixed
point theorem that d = Ld admits a single solution.	□
Lemma 1 guarantees the existence of dM. Proposition 1 states that for any two MDPs M and M and
for all (s, a) ∈ S × A, |QM(s, a)—
QM(s,a)l ≤ min {dM(s,a),dM(s,a) }∙
Proof of Proposition 1. The proof is by induction. The Value Iteration sequence of iterates (QnM)n∈N
for task M is:
Q0M (S, a) = 0,∀S,a ∈ S × A
QnM+1(S, a) = Rsa +γ X Tsas0maxQnM(S0,a0),∀S,a ∈ S × A.
a0∈A
s0∈S
12
Under review as a conference paper at ICLR 2020
ITaSO maχ QM (s', a0) - TsSO maχ qM (s', a0)
a IA	a IA
It is obvious that QM(s,a) - QM(s, a) ≤ dM(s,a). Suppose that IQM(s,a) - QM(s,a)l ≤
dMM (s, a). Then:
∣QM+1(s,a) - QM+1(s,a)∣= Ra - Ra + Y X
s0 ∈S
≤ ∣Ra - Ra∣+γ x τso maA QM(s0,a0)-	maA QM(s0,a0)
s0∈S	a	a
≤ ∣Ra - Ra∣+γ χ mIIA QM 3, a0)阳,-Tss ,∣
s0∈Sa
+ γ X TsSO mαχ QM(SiaO)- mαχ qM(SIaO)
∣a0 IA	a0 IA	∣
s0 IS
≤∣Ra - Ra∣ + x YVM (s0)∣ 琛，-TsS 0∣
s0 IS
+ Y X TssO maA ∣QM(s0, a0) - QM(s0, a0)∣
s， IS	a I
≤ DM ,γv 春 Ga) + γ X TaSO maX dM (SIaO)
Ma
sO IS
Since QM and QM are respectively the limits of the (QM )正闪 and (QM)^闪 sequences, the result
that |QM(s, a) - QM(s, a) ∣ ≤ dM(s, a) follows from passage to the limit.
By symmetry, on also has |QM(s, a) - QM(s, a)∣ ≤ dM(s, a) and thus |QM(s, a) - QM(s, a)∣ ≤
minndM (S,a),dM (S,a%	□
D Similar results to Proposition 1
Similar results to Proposition 1 can be derived with a similar proof as in Section C. The first result is
for the value function and is stated below.
Proposition (Local bound on the distance between value functions). For any two MDPs M and M,
for all S ∈ S,
1VM (S)- VM (S)I ≤ maχ∆M (s,a)
aIA
where the local MDP pseudo-metric ∆MM (S, a) has the same definition as in Proposition 1.
Another result can be derived for any policy π that one wishes to evaluate in both MDPs. For the
sake of generality, we state the result for any stochastic policy mapping states to distributions over
actions. A deterministic policy is a stochastic policy choosing the selected action with probability 1
and the others with probability 0.
Proposition (Local bound on the distance between value and Q-value functions for any policy.). For
any two MDPs M and M, for a stochastic policy π, for all s, a ∈ S ×A,
IVM(s) - VM(s)I ≤ ∆MM(s)
where dπM,M (S) is defined with the following fixed-point equation:
dMM (s)= Ea 〜∏ DM ,γvM (S,a) + Y X TaSO dMM (so),
sOIS
and ∆MM(s) = min {dMM(s),dMM(s)}∙
13
Under review as a conference paper at ICLR 2020
E Global pseudo-Lipschitz continuity result
A consequence of Proposition 1 is a global pseudo-Lipschitz continuity property:
Proposition 8 (Global PseUdo-LiPschitz continuity). For two MDPs M, M ,for all (s, a) ∈ S ×A,
|QM(s,a) - QM(s, a)| ≤ min {δM, δMO , with δM ,占 s,maXA nDMM(S，a)O. (IO)
DesPite being interesting from a theoretical PersPective, we do not use this result for transfer because
it is imPractical to comPute. Indeed, estimating the maximum in Equation 10 might be as hard as
solving both MDPs (which, when it haPPens, is too late for transfer to be useful).
Proof. The Proof is by induction and reuses the notations introduced in the Proof of ProPosition 1. It
is immediate that
IQM(S,a) - QM(s, a) I ≤ dM, and
IQM(S, a) - QM(S, a)| ≤ dM.
Hence, the result holds for n = 0. Let us suPPose that
IQM(s,α) - QM(s,a)∣ ≤ dM,and
∣QM(SM- QM(s,a)∣ ≤ dM.
Then,
∣qm+1(s, a) - qm+1(s, a)∣ ≤ DM ,γvM (s, a)+Y X t；s，maA ∣qm E a0) - qm E a0)∣
s0∈S	a∈
≤ s,mSXAhDM,γVM (s,a)i + Y X TasO ± s,mSXAhDMCVM(S,a)i
s0∈S	/
≤ max ZIhDM 7v * (s, a)i (1 + YY-)
s,α∈S×A L M,γvM	'」∖	1 — Y)
≤ dM
≤ dM
□
F	Proof of Proposition 2
Proof. The result is clear for all S,a ∈ K since the Lipschitz bounds are ProvabIy greater than QM.
For S, a ∈ K, the result is by induction. Let us consider the Dynamic Programming (Bellman, 1957)
sequences converging to QM and U at rank n whose definitions follow:
(QM,0(S, a) = 0
IQM,n (S, a) = Ra + Y Ps0 Tss0 maxa0 QM,n-1 (S0, a0)
U0(S, a) = 0
Un(S, a) = Rsa + YPs0 Tsas0 maxa0 Un-1(S0, a0)
Obviously, QM,o(s, a) ≤ Uo(s, a). Suppose the property true at rank n and consider rank n +1:
QM,n+l(S,a) - Un+l(S,a) = Y X Tas0 (maX QM,n(S0,a0) - maX Un(S0,a0))
s0
≤ Y X Tas，max (QM,n(s0,aθ) - Un(Sla'))
a0
s0
≤0
Which concludes the proof by induction. The result holds by passage to the limit since the considered
Dynamic Programming sequences converge to the true functions.	□
14
Under review as a conference paper at ICLR 2020
G	Proof of Proposition 3
Consider two tasks M =(T, Ri and M =(T, R)，with K and K the respective sets of state-action
ʌ ʌ ʌ ʌ ʌ
♦	i	. i ∙ -∣	1	ι-∣ τι ʃ	/m τ~>∖ ι Tt r	Irτι τ∖∖	ι	♦ j	♦
pairs where their learned models M = (T, Ri and M = (T, Ri are known with accuracy in
L1 -norm with probability at least 1 - δ, i.e. we have that,
Pr (|R； — RRS∣ ≤ E) ≥ 1 — δ, ∀s, a ∈ K,	(11)
Pr (kTSo - Ta kι ≤ e) ≥ 1 — δ, ∀s,a ∈ K,	(12)
and the same goes for M and its learned model M. We state the result for each one of the three
cases 1) s, a ∈ K ∩ K, 2) s, a ∈ K ∩ KC and 3) s, a ∈ KC ∩ Kc, the case s, a ∈ KC ∩ K being the
symmetric of case 2).
1) If s, a ∈ K ∩ K, then properties 11 and 12 hold for both (R, T) with (R, Tiand (R, Ti with
ʌ ʌ
(R, Ti. We have by definition:
DMM Ga) = ∣Ra - Ra∣ + Y X VM ⑺寒，一TssO ∣∙	(13)
s0∈S
The first term of the RHS of Equation 13 respects the following sequence of inequalities with
probability at least 1 — δ :
. _	--. . _ ʌ _ . . ʌ _	△一. . - _ △一.
∣Ra -	Rai	≤ ∣Ra	- R^aι	+ ιR^a —	R^:ι	+ 阀—Rai
.ʌ _ _ _ .
≤ |Ra - Ra∣ + 2E.	(14)
The second term of the RHS of Equation 13 respects the following sequence of inequalities with
probability at least 1 - δ :
Y X VM (s0)∣TSso - Tsso i≤ Y X V(s0) (lTαso - Tsso i + ITasO- Tsso i + ITssO- Tss，i)
s0∈S	s0∈S
≤	Y max V(SO) X ITaSO- TsSO ∣ + Y X V(s0)|Tss，-蹩，∣ +
s
sO∈S	sO∈S
Y max V(SO)E ITsSO- TsSO ∣
s	sO ∈S
≤	Y X V(SO)ITSsO- To3OI + 2eymax V(s0)∙	(15)
sO
sO∈S
Summation of Equations 14 and 15 reveals DMM (s, a) =1R： - RaI + Y P§o∈s V(s0)ITSsO - TaSO I
on the RHS of the inequality. Remarking this, we can upper-bound the model pseudo-distance of
Equation 13 by the expected quantity with probability at least 1 - δ, proving the Proposition for case
1):
DMVM (s, a) ≤ DMM (s, a) + 2e(1+ Y max V(S0)) ∙
2)	If s, a ∈ K ∩ Kc, then properties 11 and 12 hold for (R, T)with (R, Ti only. Similarly to the
proof of case 1), we upper bound sequentially the two terms of the RHS of Equation 13. With
probability at least 1 - δ, we have the following:
一 ʌ ʌ 一
IRa - RSI ≤ IRa - r;i + IR^a - roi
ʌ 一
≤ E + max IRRa — RI.	(16)
R
Similarly, with probability at least 1 - δ, we have:
Y X VM (s0) ITSasO - TsSO I≤ Y X V(s0)(ITaSO- TsSO I + ITsSO- TsSO I)
sO∈S	sO∈S
≤ Y max V(so)e + Y max X V(s0) it*，- Ts，i.	(17)
S0	T
sO∈S
15
Under review as a conference paper at ICLR 2020
Combining inequalities 16 and 17, we get the following with probability at least 1 - δ, noticing
DYVM(s,a) on the LHS:
DMVM(s, a) ≤ max DMV(s, a) + E(1 + Y max V(s0)),
which is the expected result.
3)	If s, a ∈ KC ∩ Kc, then properties 11 and 12 do not hold. In such a case, the result
DMVMGa) ≤ max2 DμV(s,a)
M M	μ,μ∈M2 YV
is straightforward by remarking that VM (S) ≤ V(S) with probability at least 1 - δ.
H Analytical calculation of dDMM IN Proposition 3
Consider two tasks M =(T, Ri and M =(T, R)，with K and K the respective sets of state-action
pairs where their learned models M = (T , Ri and M = (T, Ri are known with accuracy E in
L1 -norm with probability at least 1 - δ. We note Vmax , a known upper-bound on the maximum
achievable value. In the worst case where one does not have any information on the value of Vmax ,
one can always set VmaX = ι-1γ. We recall the definition of the upper bound on the pseudo-metric
between models:
DMVM (s,a)+2B
max DM(s, a) + B
μμ∈M YV
^
mM DlYV (s,a) + B
max2 DμV(s,a)
μ,μμ∈M2 YV
if (s,a) ∈ K ∩ K,
if (s, a) ∈ K ∩ Kc,
if (s,a) ∈ KC ∩ K,
if (s,a) ∈ KC ∩ Kc.
(18)
with B = E(1 + γ maxso V(s0)) and DMM defined as in Equation 13. We detail the computation
of DMM(s, a) for each cases 1) s, a ∈ K ∩ K,2) s, a ∈ K ∩ KC (the s, a ∈ KC ∩ K is symmetric
to this one), and 3) s, a ∈ KC ∩ Kc. Recall that We consider a finite, countable, state-action space
S × A.
1) If s, a ∈ K ∩ K, We have
D MM(s,a) = DYVM (s,a) + 2B
=∣Ra - Ra ।+γ X V(s0) it*，- Ta ∣ + 2e (i+Y max V(s0)).
s0∈S	s
Since s, a is a known state-action pair, everything is known and computable in this last equation. Note
that maxso V(s0) can be tracked along the updates of V and thus its computation does not induce any
additional complexity.
2) If s, a ∈ K ∩ Kc, We have
^
01 ) + E(1 + γ max V(s0)),
ʌ [ ʌ ʌ ]
Ir[ax]|R； — r| = max (R；, 1 — R；)
-ts01 ) + E(1+ γ max V(s0)).
First, we have
16
Under review as a conference paper at ICLR 2020
Maximizing the maxt∈[o,i]∣s∣ term is maximizing a convex combination of V (whose values are all
positive) whose terms are not independent (since the ts0 terms should sum to one). This is easily cast
as a linear programming problem. A straightforward (simplex-like) resolution procedure consists in
progressively adding mass on the terms that will maximize the convex combination as follows:
•	ts0 = 0, ∀s0 ∈ S
•	l = Sort states by decreasing value of V
•	While Ps∈S t(s) < 1
-s0 = PoP first state in l
ʌ
-Assign t(s0) - argmaxt∈[o,i] |僚，一t| to s0 (note that ts> ∈ {0,1})
Tf ps∈S ts > 1, then ts0 一 1 - ps∈S∖s0 t(S)
This allows calculating the maximum over transition models.
There is however a simPler comPutation that almost always yields the same result (when it does not,
it Provides an uPPer bound) and does not require the burden of the Previous Procedure. Consider the
subset of states for which V(s0) = maxs V(s) (often these are states in Kc). Among those states, let
us suppose there exists s+ unreachable from s, a, according to T, that is T*+ = 0. If MM has not been
fully exPlored, as is often the case in RMax, there may be many such states. Then the distribution t
with all its mass on s+ is a maximizer of the maxt∈[o,i] ∣s∣ term. Conversely, if such a state does not
exist (that is, if for all such states T*+ > 0), then maxs V(s) is an upper bound on the maxt∈[0j]∣s∣
term. Therefore:
max T V(s0)∣TSs0 - tso | ≤ max πV(s), with equality in many cases.
t∈[0,1]lsl ∖±S	) — S
3) If s, a ∈ KC ∩ Kc, the resolution is trivial and we have
DMM(S,aɔ= max Dμv(S,aɔ
μ,μ∈M2 YV
=	max 一 (|R： - R；l + Y X V(S0)|TssO-TssOI
Ra,τas0,Ra,TasO ∖	s⅛
=	max ∣r —尸∣ + Y max 〉 V(s0)∣tsθ — QI
r,r∈[0,1]	t,t∈[o,i]lSl 7-S
Pt=1 s ∈S
P t=ι
=	1 + γ max V(s).
s
I Proof of Proposition 4
Lemma 2. Given two tasks M and M, K the set of state-action pairs for whichhR, T〉is known
ʌ
with accuracy in L1-norm with probability at least 1 - δ. If Y(1 + ) < 1, this equation on d is a
fixed-point equation admitting a unique solution which we call dM
{DMM(s,a)+ γ ( P Ta o max d(s0,a0) + C max 2(s0,a0)) if s, a ∈ K,
_	LS	a∈A	s0 ,α0∈S×A	7
D MM (s,a)+ γ ι∏aXA 才(s0, a0) otherwise.
Proof of Lemma 2. The proof is similar to the proof of Lemma 1. Let d1 and d2 be two functions
from S × A to R and let L be the functional operator that maps any function d : S × A → R to
{Dmm(s,a) + γ ( P Tao max d(s0,a0) + c max d(s0, a0) ∣ if s, a ∈ K,
sO∈S ss aO∈A	sO,aO∈S×A
Dmm(s,a) + γ / maXA d(s0,a0) otherwise.
17
Under review as a conference paper at ICLR 2020
If s, a ∈ K, we have
Ld1 (s, a) - Ld2(s, a) = γ X Tsas0 maax d1 (s0, a0) - maax d2(s0, a0) +
s0
γ maxd1(s0, a0) - maxd2(s0, a0)
s0,a0	s0,a0
≤ (γ + γ) maxd1(s0, a0) - maxd2(s0, a0)
s0,a0	s0,a0
≤ γ(1 + ) max (d1(s0, a0) - d2(s0, a0))
s0,a0
≤ γ(1 + )kd1 - d2k∞.
If s, a ∈/ K, we have
Ld1(s, a) - Ld2(s, a) = γ maxd1(s0, a0) - maxd2(s0, a0)
s0,a0	s0,a0
≤ γ max (d1(s0, a0) - d2(s0, a0))
s0,a0
= γ(1 + )kd1 - d2 k∞.
In both cases, kLd1 - Ld2k∞ ≤ γ(1 + )kd1 - d2 k∞. If γ(1 + ) < 1, L is a contraction mapping
in the metric space (S ×A, k∙ k∞). This metric space being complete and non-empty, it follows
from Banach fixed point theorem that d = Ld admits a single solution.	□
Proofof Proposition 4. The proof is done by induction, by calculating the values of dM and dM
following the value iteration algorithm. Those values can respectively be computed via the sequences
ʌ
of iterates (dn)n∈N and (dn)n∈N defined as follows:
d0(s, a) = 0, ∀s, a ∈ S × A
dn+1(s, a) = DYV(s, a) + Y ^X 7% max dn(s0, a0)
s0∈S
and,
d0(s, a) = 0, ∀s, a ∈ S × A,
DMM(s, a) + γ ( P Ta o max dn(s0,a0) + E max dn(s0,a0)[ if s, a ∈ K,
s0∈S ss a0 ∈A	s0 ,a0 ∈S ×A
DMM(s, a) + γ max dn(s0, a0) otherwise.
s0 ,a0 ∈S ×A
The proof at rank n = 0 is trivial. Let us assume the proposition dn ≤ dn, ∀s, a ∈ S ×A true at rank
n and consider rank n + 1. There are two cases, depending on the fact that s, a is in K or not.
If s, a ∈ K, we have
dn+1 (s, a) — dn+1(s, a) = DMM(s, a) — DMM(s, a)+
max dn(s0, a0) — Tao
a0∈A	ss
max dn(s0, a0) ∣ 十
a0∈A
一YE max dn(s0,a0).
s0,a0∈S×A
Using Proposition 3, we have that DMM(s, a) is an upper bound on DYVM(s, a) with probability at
least 1 — δ. Hence
Pr (DYVM(s, a) — DMM(s, a) ≤ 0)≥ 1 — δ.
18
Under review as a conference paper at ICLR 2020
This plus the fact that dn ≤ dn by induction hypothesis, We have that
dn(s0,a0)
dn+1(s,a) - dn+1(s,a) ≤ YEmax
s0∈S a ∈A
—
+
- γ max
s0,a0∈S×A
dn(s0 ,a0)
≤ γs0,maS××Adn(S',a') X (Tsas`
—
+
0
s0∈S
- γE max
s0,a0∈S×A
dn(s0 ,a0)
Since Pr (∣∣T - T∣∣ι ≤ E) ≥ 1 - δ, we have with probability at least 1 - δ,
dn+1(s,a) - dn+1(s,a) ≤ γ max
s0,a0∈S×A
dn(s0,a0)E — Ye max
s0,a0∈S×A
dn(s0,a0)
0,
Which concludes the proof in this case.
Conversely, if s, a ∈/ K, We have
dn+1(s, a) - dn+1(s, a) = DMVM(s, a) - DMM(s, a)+
Y	Ta 0 max dn(s0, a0) - Y max
s ∈S ss a0∈A	s0,a0∈S×A
dn(s0,a0).
Using the same reasoning than in case s, a ∈ K, we have with probability higher than 1 - δ
dn+1(s,a) - dn+1(s,a) ≤ YETsas0 maAx
s0∈S
dn(s0,a0) 一 γ max
s0,a0∈S×A
dn(s0,a0)
≤ γ max
s0,a0∈S×A
dn(s0, a0) 一 γ max
s0,a0∈S×A
dn(s0,a0)
≤ 0,
Which concludes the proof in the second case.
J Proof of Proposition 6
Proof. We follow the proof of the computational complexity of RMax proposed by Strehl et al.
(2009). The cost of Lipschitz RMax is constant on most time steps since the action is greedily chosen
w.r.t. the upper-bound on the optimal Q-value function which is a lookup table. When updating a
new state-action pair (labelling it as a known pair), the algorithm performs 2N DP computations to
update the Lipschitz bounds plus one DP computation to update the total-bound. The cost of one DP
computation is given by (Strehl et al., 2009):
O (SA(S + log(A))占 log 4)
The result comes out by remarking that at most SA state-action pairs are updated, each resulting in
(N + 1) DP computations.	□
K Proof of Proposition 7
Proof. Consider a fixed state-action pair s, a ∈ S ×A. For two sampled tasks M, M ∈ M2, we
assume our algorithm to provide an upper-bound on DMM (s, a) with probability at least 1 - δ. This
assumption is actually guaranteed by Proposition 3 while running Algorithm 1. With probability at
least 1 - δ,
DMM(s,a) ≥ DMM(s,a),∀M,M ∈ M2.
□
19
Under review as a conference paper at ICLR 2020
Hence, with probability at least 1 - δ,
max DDMM(s,a) ≥ max DYV(s,a)
M,M∈M^2	M,M∈MM2 Y M
i.e. D
max(s, a) ≥ D
max(s, a).
In turn, the event of underestimating DmaX(s, a) occurs only if the two tasks, that We note M；, MS ∈
M2, maximizing M,M → DMM(s, a), are not sampled, i.e. do not belong to M. M； and M； are
Y VZM
not necessarily unique, but they could be. Since we aim at deriving a lower bound on the probability
∙-v
of sampling M1； and M2； , we consider the worst case where they are unique. The probability P
of sampling one particular task, whose sampling probability is p, after i samples, is given by the
cumulative distribution function of the geometric distribution and is p(1 - p)i-1. Consequently, if
the sampling probability p of this task is lower bounded by pmin, the quantity pmin(1 - pmin)i-1
lower bounds P. Let us write X the random variable of the number of samples required for sampling
either M1； or M2； for the first time. By considering that the sampling probability of either sampling
M； or M2 is lower bounded by 2pmin, we follow the same reasoning as for P and obtain that:
Pr(X = i) ≥ 2pmin(1 - 2pmin)i-1
Let us write Y the random variable of the number of samples required for sampling the remaining task
for the first time. We have the following result using the geometric distribution for the conditional
Pr(Y = k|X = i):
Pr(Y = k) = X Pr(Y = k,X = i)
i=1
k-1
X Pr(Y = k|X = i)Pr(X = i)
i=1
≥ 2 kX-1(1 - pmin)k-i-1(1 - 2pmin)i-1p2min	(19)
i=1
Pr(Y = k) is the probability of first success at step k. For Dmaχ(s, a) to estimate DmaX(s, a) in m
steps, we require that this success occurs any time during the first m steps, so we have:
m
Pr(Dmaχ(s,a) ≥ DmaX(S,a)) = XPr(Y = k)
k=2
Using Equation 19, we can deduce our result when remarking that necessarily pmin ≤ 1/2:
Pr(Dmax(s, a) ≥ DmaX(S, a)) ≥ 2pmmin ££(1 - Pmin)k-i-1(1 — 2Pmin)i-1
k=2 i=1
≥ 2p2min mX-2 Xk (1 - pmin)k-i(1 - 2pmin)i
k=0 i=0
m-2	k
≥ 2p2min X(1-pmin)kX
k=0	i=0
1 - 2pmin
1 - pmin
)2 2 mX2,	Ik 1 A	(I- 2pmin)k+1
k= (	) P I	(1 - Pmin)k
m-2
≥ 2pmin £ ((1- Pmin)k+1 — (1 — 2pmin)k + 1)
k=0
≥ 2pmin(1 - pmin)
1 -(1 - Pmin)m-1
1 - (1 - Pmin)
- 2pmin(1 - 2pmin)
1 -(1 - 2pmin)m-1
1 - (1 - 2pmin)
≥ 1-2(1-Pmin)m + (1 - 2Pmin)m
20
Under review as a conference paper at ICLR 2020
□
L Discussion on an upper bound on distances between MDP models
Section 4.3 introduced the idea of exploiting prior knowledge on the maximum distance between two
MDP models. This idea begs for a more detailed discussion. Consider two MDPs M and M. By
definition of the local model pseudo metric DMVM in Equation 1, the maximum possible distance is
Y VM
given by
DMMl(	1 — 1+ Y
ιmax _D V * (s, a)——	.
M,M∈M2 YM	1 - Y
But this assumes any transition or reward model can define M. In other words, the maximization
is made on the whole set of possible MDPs. To illustrate why this is too naive, consider a game
within the Arcade Learning Environment (Bellemare et al., 2013). We, as humans, have a strong bias
concerning similarity between environments. If the game changes, we still assume groups of pixels
will move together on the screen as the result of game actions. For instance, we generally discard
possible new games M that “teleport” objects across the screen without physical considerations. We
also discard new games that allow transitions from a given screen to another screen full of static.
These examples illustrate why the knowledge of Dmax is very natural (and also why its precise value
may be irrelevant). The same observation can be made for the “tight” experiment of Section 5; the set
of possible MDPs is restricted by some implicit assumptions that constrain the maximum distance
between tasks. For instance, in these experiments, all transitions move to a neighboring state and
never “teleport” the agent to the other side of the gridworld. Without the knowledge of Dmax , LRMax
assumes such environments are possible and therefore transfer values very cautiously. Overall, the
experiments of Section 5 confirm this important insight: safe transfer occurs slowly if no a priori is
given on the maximum distance between MDPs. On the contrary, the knowledge of Dmax allows a
faster and more efficient transfer between environments.
M	The “tight” environment used in experiments of Section 5
The tight environment is a 11 × 11 grid-world illustrated in Figure 4. The initial state of the agent
is the central cell displayed with an “S”. The actions are moving 1 cell in one of the four cardinal
directions. The reward is 0 everywhere, except for executing an action in one of the three teal cells in
the upper-right corner. Each time a task is sampled, a slipping probability of executing another action
as the one selected is drawn in [0, 1] and the reward received in each one of the teal cells is picked in
[0.8, 1.0].
21
Under review as a conference paper at ICLR 2020
20 cells
Figure 5: The corridor grid-world environment.

UJnsrtPeaUnoOSIa ①As-①rt①MBJeAV
0 8 6 4 2 0
......
Iooooo
UInpuno□α ①A①①PI①AV
Task number
—∙— RMax	―*— MaxQInit
—■— LRMax	_*— LRMaxQInit
.LRMax(0.2)	—L— LRMaxQInit(0.2)
3	6	9	12	15	18
Episode number
(a) Average discounted return vs. tasks	(b) Average discounted return vs. episodes
Figure 6: Results of the corridor lifelong RL experiment with 95% confidence interval.
N	Additional lifelong RL experiments
We ran additional experiments on the corridor grid-world environment represented in Figure 5. The
initial state of the agent is the central cell labeled with the letter “S”. The actions are {left, right}
and the goal is to reach the cell labeled with the letter “G” on the extreme right. A reward R > 0 is
received when reaching the goal and 0 otherwise. At each new task, a new value of R is sampled in
[0.8, 1]. The transition function is fixed and deterministic.
The key insight in this experiment is not to lose time exploring the left part of the corridor. We
ran 20 episodes of 11 time steps for each one of the 20 sampled tasks. Results are displayed in
Figure 6a and 6b, respectively for the average relative discounted return over episodes and over
tasks. Similarly as in Section 5, we observe in Figure 6a that LRMax benefits from the transfer
method as early as the second task. The MaxQInit algorithm benefits from the transfer from task
number 12. Prior knowledge Dmax decreases the sample complexity of LRMax as reported earlier
and the combination of LRMax with MaxQInit outperforms all other methods by providing a tighter
upper-bound on the optimal Q-value function. This decrease of sample complexity is also observed
in the episode-wise display of Figure 6b where the convergence happens more quickly on average for
LRMax and even more for MaxQInit. This figure allows to see the three learning stages of LRMax
reported in Section 5.
We also ran lifelong RL experiments in the maze grid-world of Figure 7. The tasks consists in
reaching the goal cell labeled with a “G” while the initial state of the agent is the central cell, labeled
with an "S”. Two walls configurations are possible, yielding two different tasks with probability 2 of
being sampled in the lifelong RL setting. The first task corresponds to the case where orange walls
are actually walls and green cells are normal white cells where the agent can go. The second task
is the converse, where green walls are walls and orange cells are normal white cells. We run 100
episodes of length 15 time steps and sample a total of 30 different tasks. Results can be found in
Figure 8. In this experiment, we observe the increase of performance of LRMax as the value of Dmax
decreases. The three stages behavior of LRMax reported in Section 5 does not appear in this case. We
tested the performance of using the online estimation of the local model distances of Proposition 7 in
the algorithm referred by LRMax in Figure 8. Once enough tasks have been sampled, the estimate
on the model local distance is used with high confidence on its value and refines the upper-bound
computed analytically in Equation 6. Importantly, this instance of LRMax achieved the best result in
this particular environment, demonstrating the usefulness of this result. This method being similar
22
Under review as a conference paper at ICLR 2020
Figure 7: The maze grid-world environment. The walls correspond to the black cells and either the
green ones or the orange ones.
Episode Number
Figure 8: Averaged discounted return over tasks for the maze grid-world lifelong RL experiment.
to the MaxQInit estimation of maximum Q-values, we unsurprisingly observe that both algorithms
feature a similar performance in the maze environment.
O PRIOR Dmax USE EXPERIMENT
Each time an s, a pair is updated, We compute the local distance upper bound DD (Equation 6) for all
ʌ
(s, a) ∈ S × A. In this computation, one can leverage knowledge of Dmax to select min{D , Dmax}.
We shoW that LRMax relies less and less on Dmax as knoWledge on the current task increases. For
this experiment, We used the tWo grid-Worlds environments displayed in Figures 9 and 10.
The reWards collected With any actions performed in the teal cells of both tasks are defined as:
Ra = exp (- (Sx-3；/-gy)2 ) , Vs = (s,, Sy) ∈S,a ∈A,
Where (sx, sy) are the coordinates of the current state, (gx, gy)the coordinate of the goal cell
labelled With a G and σ is a span parameter equal to 1 in the first environment and 1.5 in the second
environment. The agent starts at the cell labelled With the S letter. Black cells represent unreachable
23
Under review as a conference paper at ICLR 2020
Figure 9: 4 times 4 heat-map grid-world.
Slipping probability is 10%.
world. Slipping probability is 5%.
Figure 11: Proportion of times where DmaX ≤ DMM, i.e. use of the prior, Vs computation of the
Lipschitz bound. Each curve is displayed with 95% confidence intervals.
cells (walls). We run LRMax twice on the two different maze grid-worlds and record for each model
update the proportion of times DmaX is smaller than D in Figure 11 via the % use of Dmaχ.
With maximum value DmaX = 19, D is systematically lesser than Dmax, resulting in 0% use.
Conversely, with minimum value DmaX = 0, the use expectedly increases to 100%. The in-between
value of DmaX = 10 displays a linear decay of the use. This suggests that, at each update, D ≤ DmaX
is only true for one more unique s, a pair, resulting in a constant decay of the use. With fewer prior
(DmaX = 15 or 17), updating one single s, a pair allows D to drop under DmaX for more than one
pair, resulting in less use of the prior knowledge. The conclusion of this experiment if that DmaX is
only useful at the beginning of the exploration, while LRMaX relies more on its own bound D when
partial knowledge of the task has been acquired.
P	DISCUSSION ON RMAX PRECISION PARAMETERS , δ, nknown
We used nknown = 10, δ = 0.05 and = 0.01. Theoretically, nknown should be a lot larger (≈ 105)
in order to reach an accuracy = 0.01 according to Strehl et al. (2009). However, it is common
practice to assume such small values of nknown are sufficient to reach an acceptable model accuracy .
Interestingly, empirical validation did not confirm this assumption for any RMax-based algorithm. We
keep these values nonetheless for the sake of comparability between algorithms and consistency with
the literature. Despite such absence of accuracy guarantees, RMax-based algorithms still perform
surprisingly well and are robust to model estimation uncertainties.
24
Under review as a conference paper at ICLR 2020
Q Informations about the Machine Learning reproducibility
CHECKLIST
For the experiments run in Section 5, the computing infrastructure used was a laptop using a single
64-bit CPU (model: Intel(R) Core(TM) i7-4810MQ CPU @ 2.80GHz). The collected samples sizes
and number of evaluation runs for each experiment is summarized in Table 1.
Task	Number of experiment repetitions	Number of sampled tasks	Number of episodes	Maximum length of episodes	Total number of collected transition samples (s, a, r, s0)
“Tight” task of Figures 1a 1b and 1c	10	15	2000	10	3,000,000
“Tight” task of Figure 1d	100	2	2000	10	4,000,000
Corridor task Section N	1	20	20	11	4400
Maze task Section N	1	30	100	15	45000
Heat-map Section O	100	2	100	30	600,000
Table 1: Summary of the number of experiment repetition, number of sampled tasks, number of
episodes, maximum length of episodes and upper bounds on the number of collected samples.
The displayed confidence intervals for any curve presented in the paper is the 95% confidence
interval (Neyman, 1937) on the displayed mean. No data were excluded neither pre-computed.
Hyper-parameters were determined to our appreciation, they may be sub-optimal but we found the
results convincing enough to display interesting behaviors.
25