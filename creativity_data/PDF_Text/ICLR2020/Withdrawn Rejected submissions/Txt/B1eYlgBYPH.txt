Under review as a conference paper at ICLR 2020
A Deep Recurrent Neural Network via Un-
FOLDING REWEIGHTED '1-'1 MINIMIZATION
Anonymous authors
Paper under double-blind review
Ab stract
Deep unfolding methods design deep neural networks as learned variations of
optimization methods. These networks have been shown to achieve faster conver-
gence and higher accuracy than the original optimization methods. In this line
of research, this paper develops a novel deep recurrent neural network (coined
reweighted-RNN) by unfolding a reweighted 'ι-'ι minimization algorithm and
applies it to the task of sequential signal reconstruction. To the best of our knowl-
edge, this is the first deep unfolding method that explores reweighted minimization.
Due to the underlying reweighted minimization model, our RNN has a different
soft-thresholding function (alias, different activation function) for each hidden unit
in each layer. Furthermore, it has higher network expressivity than existing deep
unfolding RNN models due to the over-parameterizing weights. Moreover, we
establish theoretical generalization error bounds for the proposed reweighted-RNN
model by means of Rademacher complexity. The bounds reveal that the param-
eterization of the proposed reweighted-RNN ensures good generalization. We
apply the proposed reweighted-RNN to the problem of video-frame reconstruction
from low-dimensional measurements, that is, sequential frame reconstruction. The
experimental results on the moving MNIST dataset demonstrate that the proposed
deep reweighted-RNN significantly outperforms existing RNN models.
1	Introduction
The problem of reconstructing sequential signals from low-dimensional measurements across time
is of great importance for a number of applications such as time-series data analysis, future-frame
prediction, and compressive video sensing. Specifically, we consider the problem of reconstructing
a sequence of signals st ∈ Rn0, t = 1, 2, . . . , T, from low-dimensional measurements xt = Ast,
where A ∈ Rn×n0 (n n0) is a sensing matrix. We assume that st has a sparse representation
ht ∈ Rh in a dictionary D ∈ Rn0 ×h, that is, st = Dht . At each time step t, the signal st can be
independently reconstructed using the measurements xt by solving (Donoho, 2006):
mhin n1 kxt - ADhtk2 + λ∣∣ht∣∣ι},	(1)
where ∣∣ ∙ kp is the 'p-norm and λ is a regularization parameter. The iterative shrinkage-thresholding
algorithm (ISTA) (Daubechies et al., 2004) solves (1) by iterating over h，) = φλ (htl-1) —
c
C DTAT(ADh(l-1) - xt)), where l is the iteration counter, φγ (u) = sign(u)[0, |u| - γ]+ is
the soft-thresholding operator, Y = C, and C is an upper bound on the Lipschitz constant of the
gradient of 2 ∣∣xt 一 ADhtk2.
Under the assumption that sequential signal instances are correlated, we consider the following
sequential signal reconstruction problem:
min n $ kxt - ADhtk 2+λιkhtkι+λ2R(ht, ht-ι)o,	(2)
where λ1, λ2 > 0 are regularization parameters and R(ht, ht-1) is an added regularization term
that expresses the similarity of the representations ht and ht-1 of two consecutive signals. Wisdom
et al. (2017) proposed an RNN design (coined Sista-RNN) by unfolding the sequential version of
1
Under review as a conference paper at ICLR 2020
ISTA. That study assumed that two consecutive signals are close in the '2-norm sense, formally,
R(ht, ht-ι) = 2∣∣Dht - FDht-ιk2, where F ∈ Rn0 ×n0 is a correlation matrix between st and
st-ι. More recently, the study by Le et al. (2019) designed the '1-'1-RNN, which stems from
unfolding an algorithm that solves the '1-'1 minimization problem (Mota et al., 2017; 2016). This
is a version of Problem (2) with R(ht, ht-1) = kht - Ght-1 k1, where G ∈ Rh×h is an affine
transformation that promotes the correlation between ht and ht-1. Both studies (Wisdom et al.,
2017; Le et al., 2019) have shown that carefully-designed deep RNN models outperform the generic
RNN model and ISTA (Daubechies et al., 2004) in the task of sequential frame reconstruction.
Deep neural networks (DNN) have achieved state-of-the-art performance in solving (1) for individual
signals, both in terms of accuracy and inference speed (Mousavi et al., 2015). However, these models
are often criticized for their lack of interpretability and theoretical guarantees (Lucas et al., 2018).
Motivated by this, several studies focus on designing DNNs that incorporate domain knowledge,
namely, signal priors. These include deep unfolding methods which design neural networks to learn
approximations of iterative optimization algorithms. Examples of this approach are LISTA (Gregor
& LeCun, 2010) and its variants, including ADMM-Net (Sun et al., 2016), AMP (Borgerding et al.,
2017), and an unfolded version of the iterative hard thresholding algorithm (Xin et al., 2016).
LISTA (Gregor & LeCun, 2010) unrolls the iterations of ISTA into a feed-forward neural network
with weights, where each layer implements an iteration: ht(l) = φγ(l) (W(l)ht(l-1) + U(l)xt), with
W(Il = I - CDtAtAD, U(I) = CDTAT, and γ(l) being learned from data. It has been shown
(Gregor & LeCun, 2010; Sprechmann et al., 2015) that a d-layer LISTA network with trainable
parameters Θ = {W(l), U(l), γ(l)}ld=1 achieves the same performance as the original ISTA but with
much fewer iterations (i.e., number of layers). Recent studies (Chen et al., 2018; Liu et al., 2019) have
found that exploiting dependencies between W(l) and U(l) leads to reducing the number of trainable
parameters while retaining the performance of LISTA. These works provided theoretical insights to
the convergence conditions of LISTA. However, the problem of designing deep unfolding methods
for dealing with sequential signals is significantly less explored. In this work, we will consider a
deep RNN for solving Problem (2) that outputs a sequence, Si,..., ST from an input measurement
sequence, x1 , . . . , xT , as following:
ht = φγ(Wht-1 + Uxt),
St = Dht.	(3)
It has been shown that reweighted algorithms—such as the reweighted `1 minimization method
by Candes et al. (2008) and the reweighted '1-'1 minimization by Luong et al. (2018)—outperform
their non-reweighted counterparts. Driven by this observation, this paper proposes a novel deep RNN
architecture by unfolding a reweighted-`1 -`1 minimization algorithm. Due to the reweighting, our
network has higher expressivity than existing RNN models leading to better data representations,
especially when depth increases. This is in line with recent studies (He et al., 2016; Cortes et al.;
Huang et al., 2017), which have shown that better performance can be achieved by highly over-
parameterized networks, i.e., networks with far more parameters than the number of training samples.
While the most recent studies (related over-parameterized DNNs) consider fully-connected networks
applied on classification problems (Neyshabur et al., 2019), our approach focuses on deep-unfolding
architectures and opts to understand how the networks learn a low-complexity representation for
sequential signal reconstruction, which is a regression problem across time. Furthermore, while there
have been efforts to build deep RNNs (Pascanu et al., 2014; Li et al., 2018; Luo et al., 2017; Wisdom
et al., 2017), examining the generalization property of such deep RNN models on unseen sequential
data still remains elusive. In this work, we derive the generalization error bound of the proposed
design and further compare it with existing RNN bounds (Zhang et al., 2018; Kusupati et al., 2018).
Contributions. The contributions of this work are as follows:
•	We propose a principled deep RNN model for sequential signal reconstruction by unfolding
a reweighted `1 -`1 minimization method. Our reweighted-RNN model employs different
soft-thresholding functions that are adaptively learned per hidden unit. Furthermore, the
proposed model is over-parameterized, has high expressivity and can be efficiently stacked.
•	We derive the generalization error bound of the proposed model (and deep RNNs) by
measuring Rademacher complexity and show that the over-parameterization of our RNN
ensures good generalization. To best of our knowledge, this is the first generalization error
2
Under review as a conference paper at ICLR 2020
bound for deep RNNs; moreover, our bound is tighter than existing bounds derived for
shallow RNNs (Zhang et al., 2018; Kusupati et al., 2018).
•	We provide experiments in the task of reconstructing video sequences from low-dimensional
measurements. We show significant gains when using our model compared to several
state-of-the-art RNNs (including unfolding architectures), especially when the depth of
RNNs increases.
2 A DEEP RNN VIA UNFOLDING REWEIGHTED-'i-'i MINIMIZATION
In this section, We describe a reweighted 'ι-'ι minimization problem for sequential signal reconstruc-
tion and propose an iterative algorithm based on the proximal method. We then design a deep RNN
architecture by unfolding this algorithm.
The proposed reweighted `1 -`1 minimization. We introduce the following problem:
min n2kxt — ADZhtk2 + λ"∣g ◦ ZhtkI + λ2∣∣g ◦ (Zht - Ght-I)kι},	(4)
where “◦” denotes element-wise multiplication, g ∈ Rh is a vector of positive weights, Z ∈ Rh×h
is a reweighting matrix, and G ∈ Rh×h is an affine transformation that promotes the correlation
between ht-1 and ht . Intuitively, Z is adopted to transform ht to Zht ∈ Rh, producing a reweighted
version of it. Thereafter, g aims to reweight each transformed component of Zht and Zht - Ght-1
in the 'ι-norm regularization terms. Because of applying reweighting (CandeS et al., 2008), the
solution of Problem (4) is a more accurate sparse representation compared to the solution of the `1 -`1
minimization problem in Le et al. (2019) (where Z = I and g = I). Furthermore, the use of the
reweighting matrix Z to transform ht to Zht differentiates Problem (4) from the reweighted `1 -`1
minimization problem in Luong et al. (2018) where Z = I.
The objective function in (4) consists of the differentiable fidelity term f (Zht) = 11 IlXt - ADZhtk2
and the non-smooth term g(Zht) = λ1 kg ◦ Zhtk1 + λ2 kg ◦ (Zht - Ght-1)k1. We use a proximal
gradient method (Beck & Teboulle, 2009) to solve (4): At iteration l, we first update h(tl-1)—after
being multiplied by Zl—with a gradient descent step on the fidelity term as u = Zlh(tl-1) -
CZiVf (htl-1)), where Vf (h(l-1)) = DTAT(ADh(IT) - χj Then, h(l) is updated as
htl) = φ》gι,λc2gι,Ght-ι 仇h( j) - CZlVf (htlT))),	(5)
where the proximal operator Φ λι	λ?	(U) is defined as
ɪ	ɪ	Ugl, C gl,Ght-l' /
φ λif,, λ2f,, ~(U)=argmin n 1 g(v)+1 ||v - uιι2o,	(6)
C gl, C gl,~	v∈Rh	IC	2	J
with ~ = Ght-1. Since the minimization problem is separable, we can minimize (6) independently
for each of the elements gl, ~, u of the corresponding gl, ~, U vectors. After solving (6), we obtain
Φλι λ2 石(U) [for solving (6), we refer to Proposition B.1 in Appendix B]. For ~ ≥ 0:
W gl, U gl,~' /	一 、 /,	ɪ	-	j	—
		u —	λg gl -	λ2 gl,	~ + λg gl + λ2 gl	<u<∞		
		~,			~ + λg gl - λ2 gl	≤u≤~+	λg gl + λ2 gl	
Φλ1 λ2	(u) = —ggι , —2gl,~≥0' ) C gl , C gl ,	=‹	u-	λg gl +	λ2 gl,	λC1 gl - λC2 gl < u	< ~ + λg gl	-λ2 gl	(7)
		0,			-λg gl - λ2 gl ≤	U ≤ F gl -	λ2 gl	
		U +	λg gl +	λ2 gl,	-∞ < u < 一λg	gl - λ2 gl,		
and for ~ < 0:		u —	λg gl -	λ2 gl,	λCg gl + λC2 gl < U	<∞		
		0,			-λ1 gl + λ2 gl ≤	U ≤ λC1 gl +	λ2 gl	
Φλ1 λ2	(u) = —ggι , —2gι,~<0' >	=‹	U+	λg gl -	λ2 gl,	~ - λg gl + λ2 gl	< u < - λCg	gl + λ2 gl	(8)
C ,C ,		~,			~ - λg gl - λ2 gl	≤u≤~-	λg gl + λ2 gl	
		u —	λg gl +	λ2 gl	-∞ < u < ~-	λg gl - λ2 gl		
3
Under review as a conference paper at ICLR 2020
1
2
3
4
5
6
7
8
9
10
Figure 1: The generic form of the proximal operators for Algorithm 1 - but also the activation function
in the proposed reweighted-RNN. Note that per unit per layer gι leads to a different activation function.
Algorithm 1: The proposed algorithm for sequential signal reconstruction.
Input: Measurements xι,..., XT, measurement matrix A, dictionary D, affine transform G,
initial h0 , ≡ ho, reweighting matrices Zι,..., Zd and vectors g「...，gd, c, λι, λ2.
Output: Sequence of sparse codes hi,..., hτ.
for t = 1,...,T do
h(0) = Gh(-)ι
for l = 1 to d do
U = [Zι - CZlDTATAD]h(l-1) + CZlDTATXt
htl) = φλc1 gι,λC2gι,Ght-)ι (U)
end
end
return h，),..., h(d)
Fig. 1 depicts the proximal operators for ~ ≥ 0 and ~ < 0. Observe that different values of gl lead to
different shapes of the proximal functions Φ ʌɪ ʌɪ ~ (U) for each element U of u.
Our iterative algorithm is given in Algorithmgl 1 .c We reconstruct a sequence hi,..., hτ from a
sequence of measurements X1 , . . . , XT . For each time step t, Step 6 applies a gradient descent update
for f (Zht-I) and Step 7 applies the proximal operator Φʌɪ ʌɪ 「卜⑷ element-wise to the result.
-C-gl, gl,Ght-I
Let us compare the proposed method against the algorithm in Le et al. (2019)—which resulted in the
'i-'i-RNN—that solves the '1-'1 minimization in Mota et al. (2016) (where Zl = I and gl = I). In
that algorithm, the update terms in Step 6, namely I - ɪDTATAD and CDtAT, and the proximal
operator in Step 7 are the same for all iterations of l. In contrast, Algorithm 1 uses a different Zl
matrix per iteration to reparameterize the update terms (Step 6) and, through updating gl, it applies a
different proximal operator to each element U (in Step 7) per iteration l.
The proposed reweighted-RNN. We now describe the proposed architecture for sequential signal
recovery, designed by unrolling the steps of Algorithm 1 across the iterations l = 1, . . . , d (yielding
the hidden layers) and time steps t = 1, . . . , T. Specifically, the l-th hidden layer is given by
% gl, W gι,Gh(-)ι (WIht-)1 + UiXt) , if l = 1,
A gl, W gi,Ght-)i (W 山,T +勒"), if l> 1,
(9)
and the reconstructed signal at time step t is given by St = Dhtd); where Ul, Wl, V are defined as
Ul = 1 ZlDTAT, ∀l,	(10)
c
W1 = Z1G - 1ZIDTATADG,	(11)
c
Wl = Zl - 1 ZlDTATAD, l > 1.	(12)
c
The activation function is the proximal operator Φλι	λ? ((U) with learnable parameters λι, λ2, c,
ɪ	ɪ	U gl, U gl,~' /	■L
gl (see Fig. 1 for the shapes of the activation functions).
4
Under review as a conference paper at ICLR 2020
(a) The proposed reweighted-RNN
Figure 2: The proposed (a) reweighted-RNN vs. (b) 'ι-'ι-RNN and (c) Stacked RNN with d layers.
Ulf ... w
(b) 'ι-'ι-RNN.
(c) Stacked RNN.
Fig. 2(a) depicts the architecture of the proposed reweighted-RNN. Input vectors st, t = 1,...,T are
compressed by a linear measurement layer A, resulting in compressive measurements Xt. The recon-
structed vectors St, t = 1,...,T, are obtained by multiplying linearly the hidden representation h(d)
with the dictionary D. We train our network in an end-to-end fashion. During training, we minimize
the loss function L(Θ)
sɪ E,JPt=ι kst- Stk2]
using stochastic gradient descent on mini-
batches, where the trainable parameters are Θ = {A, D, G, h0, Zι,..., Zd, g「...，gd,c, λ1,λ2}.
We now compare the proposed reweighted-RNN [Fig. 2(a)] against the recent '1-'1-RNN (Le et al.,
2019) [Fig. 2(b)]. The l-th hidden layer in '1-'1-RNN is given by
Φλι,λ2,Gh(d)ι (WIh(-)1 + UiXt), if l = 1,
Φ 今,λ2 ,Gh(d)ι (W2h(l-1) + Uixt), if l> 1.
(13)
The proposed model has the following advantages over '1-'1-RNN. Firstly, '1-'1-RNN uses the
proximal operator Φʌɪ “ ~(u) as activation function, whose learnable parameters λι, λ? are fixed
"C，,~
across the network. Conversely, the corresponding parameters λ1 g( and λ2g( [see (7), (8), and Fig.
1] in our proximal operator, Φʌɪ 入？	(u), are learned for each hidden layer due to the reweighting
gl, gl,八'
vector gι; hence, the proposed model has a different activation function for each unit per layer. The
second difference comes from the set of parameters {Wι, Uι} in (13) and (9). The '1-'1-RNN model
uses the same {W2, Ui} for the second and higher layers. In contrast, our reweighted-RNN has
different sets of {Wι, Ui} per hidden layer due to the reweighting matrix Zi. These two aspects
[which are schematically highlighted in blue fonts in Fig. 2(a)] can lead to an increase in the learning
capability of the proposed reweighted-RNN, especially when the depth of the model increases.
In comparison to a generic stacked RNN (Pascanu et al., 2014) [Fig. 2(c)], reweighted-RNN promotes
the inherent data structure, that is, each vector St has a sparse representation ht and consecutive
ht,s are correlated. This design characteristic of the reweighted-RNN leads to residual connections
which reduce the risk of vanishing gradients during training [the same idea has been shown in several
works (He et al., 2016; Huang et al., 2017) in deep neural network literature]. Furthermore, in (10)
and (12), we see a weight coupling of Wl and Ul (due to the shared components of A, D and Z).
This coupling satisfies the necessary condition of the convergence in Chen et al. (2018) (Theorem
1). Using Theorem 2 in Chen et al. (2018), it can be shown that reweighted-RNN, in theory, needs
a smaller number of iterations (i.e., d in Algorithm 1) to reach convergence, compared to ISTA
(Daubechies et al., 2004) and FISTA (Beck & Teboulle, 2009).
3	Generalization error bound
While increasing the network expressivity, the over-parameterization of reweighted-RNN raises the
question of whether our network ensures good generalization. In this section, we derive and analyze
the generalization properties of the proposed reweighted-RNN model in comparison to state-of-
the-art RNN architectures. We provide bounds on the Rademacher complexity (Shalev-Shwartz &
Ben-David, 2014) for functional classes of the considered deep RNNs, which are used to derive
generalization error bounds for evaluating their generalization properties (we refer to Appendix C.1
for definitions of the Rademacher complexity and the generalization error bound).
5
Under review as a conference paper at ICLR 2020
Preliminaries: We consider a deep RNN as a d-layer network fW(d),U ∈ Fd,T : Rh × Rn 7→ Rh
with weight parameters W = (W1,...,Wd) andU = (U1,..., Ud), where Wl ∈ Rh×h, Ul ∈
Rh×n. As in Bartlett et al. (2017); Golowich et al. (2018); Neyshabur et al. (2019; 2015), we
derive generalization error bounds by controlling norms of the weight matrices. Let kWl kp,q =
(Pj (kwι,j kp)q )1/q define the 'p,q -norm, p,q ≥ 1, of the weight-matrix Wι, where Wlj ∈ Rh is
the jth row of Wl . Since we focus on deep networks with soft-thresholding-based activation units—
designed by unfolding algorithms for 'ι-norm minimization——we derive the network complexities
under bounding per-unit `1 regularization, i.e., kwl,j k1. We also denote kWl k1,∞ = maxj kwl,j k1
as the maximum of the '1-norms of the matrix,s rows. We assume that the 'ι-norm of the weights of
each neuron is bounded as kWl k1,∞ = maxj kwl,j k1 ≤ αl; similarly, kUlk1,∞ = maxj kul,j k1 ≤
βl, where ul,j is the jth row of the matrix Ul. As shown in (9), we can write the reweighted-RNN
model recursively as ht(1) = fW(1),U(ht(d-)1,xt) = Φ(W1h(td-)1 +U1xt) and ht(l) = fW(l),U(ht(-d)1,xt) =
Φ(Wl f!(-U (h(d)ι, Xt) + Ulxt), where Φ(∙) is an activation function. For convenience, we denote
the input layer as fW(0),U = ht(-d)1; namely, at t = 1, we have h(0l) ≡ h0.
We denote the true and training loss by LD(f) and LS(f), respectively, where S is the training set
(of size m) drawn i.i.d. from the distribution D. The generalization error is LD(f) - LS(f), with f a
function from the functional class Fd,T. At time step t, we define Xt ∈ Rn×m as a matrix composed
of m columns from the input vectors {xt,i}im=1. We also define kXtk2,∞ = max
k∈{1,...,n}
as the maximum of the '2-norms of the rows of matrix Xt, and ∣∣ho∣∣∞ = maxj ∣ho,j |.
Pim=1 xt2,i,k
Generalization error bound. We first derive the generalization error bound for the proposed
reweighted-RNN (with T time steps) based on Rademacher complexity (see Theorem 26.5 in Shalev-
Shwartz & Ben-David (2014) and Theorem C.1 in the Appendix).
Theorem 3.1 (Generalization error bound). Let Fd,T : Rh × Rn 7→ Rh denote the functional class
of reweighted-RNNwith T time steps and d layers, where ∣∣Wlkι,∞ ≤ αl, ∣∣Ulkι,∞ ≤ βl, and
1 ≤ l ≤ d. Assume that the input data ∣∣Xt∣2,∞ ≤ √mBχ, the initial hidden state is ho, and the
loss function is 1-Lipschitz and bounded by η. Then, for f ∈ Fd,T and any δ > 0, with probability at
least 1 - δ over a training set S of size m drawn i.i.d. from the distribution D,
LD(f) - LS(f) ≤ 2Rs(Fd,T) + 4η J2lθ*∕δ),	(14)
where
Rs (Fd,τ)	2(4dT lθg21θg n +且 'W βιΛl)2( R )2BX + Λ2T∣hok∞, (15)
d
with Λl defined as follows: Λl =	αk with 0 ≤ l ≤ d - 1 and Λd = 1.
k=l+1
Proof. The proof is given in Appendix D.	□
The generalization error in (14) is bounded by the Rademacher complexity, which depends on the
training set S. If the Rademacher complexity is small, the network can be learned with a small
generalization error. The bound in (15) is in the order of the square root of the network depth d
multiplied by the number of time steps T . The bound depends on the logarithm of the number of
measurements n and the number of hidden units h. It is worth mentioning that the second square root
in (15) only depends on the norm constraints and the input training data, and it is independent of the
network depth d and the number of time steps T under the appropriate norm constraints.
To compare our model with '1-'1-RNN (Le et al., 2019) and Sista-RNN (Wisdom et al., 2017), we
derive bounds on their Rademacher complexities for a time step t. The definitions of a functional class
Fd,t for the tth time step of reweighted-RNN, '1-'1-RNN, and Sista-RNN are given in Appendix
C.2. Let Ht-1 ∈ Rh×m denote a matrix with columns the vectors of the previous hidden state
{ht-1,i}im=1,and∣Ht-1∣2,∞
Jk∈max,h}Pm=ιh2τ,i,k ≤ √mBht-ι.
6
Under review as a conference paper at ICLR 2020
Corollary 3.1.1. The empirical Rademacher complexity of Fd,t for reweighted-RNN is bounded as
m	/2(4dlog2+ logn + logh)	4 ∖2r2 , 42r2	n^,
RS(Fd,t) ≤y--------------m-------------t(工BQ Bx + ^2Bht-I,	(16)
d
with m the number of training samples and Λl given by Λd = 1, Λl =	αk with 0 ≤ l ≤ d - 1.
k=l+1
Proof. The proof is a special case of Theorem 3.1 for time step t.	□
Following the proof of Theorem 3.1, We can obtain the Rademacher complexities for 'ι-'ι-RNN and
Sista-RNN:
Corollary 3.1.2. The empirical Rademacher complexity of Fd,t for 'ι-'ι-RNN is bounded as:
Rs (Fd,t) ≤/2(4d l 2og2+mog n + l匹∙ jβ2( 0⅛ )2Bx + α2α2(dτ) Bht-I ∙⑺
Corollary 3.1.3. The empirical Rademacher complexity of Fd,t for Sista-RNN is bounded as:
RS(Fd,t)
2
≤ /2(4dlog2+mθgn + logh) Jβ2(0⅛)2Bx+ (αια2d-1) + 认t⅛)! Bh一
(18)
By contrasting (16) with (17) and (18), we see that the complexities of 'ι-'ι-RNN and Sista-RNN
have a polynomial dependence on α1, β1 and α2, β2 (the norms of first two layers), whereas the
complexity of reweighted-RNN has a polynomial dependence on α1, . . . , αd and β1, . . . , βd (the
norms of all layers). This over-parameterization offers a flexible way to control the generalization
error of reweighted-RNN. We derive empirical generalization errors in Fig. 6 in Appendix A
demonstrating that increasing the depth of reweighted-RNN still ensures the low generalization error.
4	Experimental results
We assess the proposed RNN model in the task of video-frame reconstruction from compressive
measurements. The performance is measured using the peak signal-to-noise ratio (PSNR) between
the reconstructed St and the original frame st. We use the moving MNIST dataset (Srivastava et al.,
2015), which contains 10K video sequences of equal length (20 frames per sequence). Similar to the
setup in Le et al. (2019), the dataset is split into training, validation, and test sets of 8K, 1K, and 1K
sequences, respectively. In order to reduce the training time and memory requirements, we downscale
the frames from 64 × 64 to 16 × 16 pixels using bilinear decimation. After vectorizing, we obtain
sequences of s1, . . . , sT ∈ R256. Per sequence, we obtain measurements x1, . . . , xT ∈ Rn using a
trainable linear sensing matrix A ∈ Rn×n0, with T = 20, n0 = 256 and n < n0.
We compare the reconstruction performance of the proposed reweighted-RNN model against deep-
unfolding RNN models, namely, 'ι-'ι-RNN (Le et al., 2019), Sista-RNN (Wisdom et al., 2017),
and stacked-RNN models, that is, sRNN (Elman, 1990), LSTM (Hochreiter & Schmidhuber, 1997),
GRU (Cho et al., 2014), FastRNN (Kusupati et al., 2018)1, IndRNN (Li et al., 2018) and Spec-
tralRNN (Zhang et al., 2018). For the vanilla RNN, LSTM and GRU, the native Pytorch cell
implementations were used. The unfolding-based methods were implemented in Pytorch2, with
Sista-RNN and 'ι-'ι-RNN tested by reproducing the experiments in Wisdom et al. (2017); Le et al.
(2019). For FastRNN, IndRNN, and SpectralRNN cells, we use the publically available Tensorflow
implementations. While Sista-RNN, 'ι-'ι-RNN and reweighted-RNN have their own layer-stacking
schemes derived from unfolding minimization algorithms, we use the stacking rule in Pascanu et al.
(2013) [see Fig 2(c)] to build deep networks for other RNN architectures.
1Kusupati et al. (2018) also proposed FastGRNN; we found that, in our application scenario, the non-gated
variant (the FastRNN) consistently outperformed FastGRNN. As such, we do not include results with the latter.
2Our implementations are available at https://1drv.ms/u/s!ApHn770BvhH2aWay9xEhAiXydfo?e=aCX1X0.
7
Under review as a conference paper at ICLR 2020
Our default settings are: a compressed sensing (CS) rate of n/n0 = 0.2, d = 3 hidden layers3 *
with h = 210 hidden units per layer. In each set of experiments, we vary each of these hyper-
parameters while keeping the other two fixed. For the unfolding methods, the overcomplete dictionary
D ∈ Rn0×h is initialized with the discrete cosine transform (DCT) with varying dictionary sizes
of h = {27, 28, 29, 210, 211, 212} (corresponding to a number of hidden neurons in the other
methods). For initializing λ1, λ2 [see (2), (4)], we perform a random search in the range of [10-5, 3.0]
in the validation set. To avoid the problem of exploding gradients, we clip the gradients during
backpropagation such that the '2-norms are less than or equal to 0.25. We do not apply weight decay
regularization as we found it often leads to worse performance, especially since gradient clipping is
already used for training stability. We train the networks for 200 epochs using the Adam optimizer
with an initial learning rate of 0.0003, and a batch size of 32. During training, if the validation loss
does not decrease for 5 epochs, we reduce the learning rate to 0.3 of its current value.
Table 1 summarizes the reconstruction results for different CS rates n/n0 . The reweighted-RNN
model systematically outperforms the other models, often by a large margin. Table 2 shows similar
improvements for various dimensions of hidden units. Table 3 shows that IndRNN delivers higher
reconstruction performance than our model when a small number of hidden layers (d = 1, 2) is used.
Moreover, when the depth increases, reweighted-RNN surpasses all other models. Our network also
has fewer trainable parameters compared to the popular variants of RNN. At the default settings,
reweighted-RNN, the stacked vanilla RNN, the stacked LSTM, and the stacked GRU have 4.47M,
5.58M, 21.48M, and 16.18M parameters, respectively.
Table 1: Average PSNR [dB] on the test set with different CS rates.
CS Rate	SRNN	LSTM	GRU	IndRNN	FastRNN	SpectralRNN	Sista-RNN	`1 -`1 -RNN	Ours
0.1	25.11	24.58	25.18	25.68	25.21	25.15	25.16	24.68	26.25
0.2	31.14	29.46	31.19	32.90	32.05	31.65	31.53	30.79	34.19
0.3	35.38	32.91	36.49	37.12	36.40	36.89	36.96	37.77	42.39
0.4	38.05	34.95	39.47	40.84	39.21	40.22	39.57	40.35	46.03
0.5	39.34	36.28	41.12	45.49	41.87	41.36	41.56	43.35	48.70
Table 2: Average PSNR [dB] on the test set with different network widths h (2=0.2, d = 3).
h	sRNN	LSTM	GRU	IndRNN	FastRNN	SpectralRNN	Sista-RNN	'1-'1-RNN	Ours
27	23.35	22.87	23.55	23.82	23.83	22.92	23.86	23.90	28.09
28	25.81	23.88	26.67	27.10	2671	24.46	29.64	29.55	31.46
29	28.72	26.83	30.29	32.03	29.92	30.23	31.30	30.61	33.61
210	31.14	29.46	31.19	32.90	32.05	31.65	31.53	30.79	34.19
211	29.91	29.30	31.15	33.10	30.80	31.68	31.82	30.45	34.80
212	29.71	29.08	30.93	32.47	24.26	29.26	31.63	30.09	34.98
Table 3: Average PSNR [dB] on the test set with different network depths d (nn = 0.2, h = 210).
d	sRNN	LSTM	GRU	IndRNN	FastRNN	SpectralRNN	Sista-RNN	`1 -`1 -RNN	Ours
1	27.52	27.76	27.61	30.12	29.32	29.62	28.41	28.49	29.19
2	29.21	29.46	29.68	32.73	30.84	31.37	30.67	30.19	32.12
3	31.14	22.29	31.19	32.90	32.05	31.65	31.53	30.79	34.19
4	31.64	16.50	29.26	20.65	31.07	31.10	32.56	31.80	35.99
5	16.50	26.66	16.50	25.17	20.10	30.52	33.07	32.50	36.91
6	22.28	16.50	16.50	20.90	19.37	29.56	31.99	32.00	38.90
5	Conclusions
We designed a novel deep RNN by unfolding an algorithm that solves a reweighted `1 -`1 minimization
problem. Our model has high network expressivity due to per-unit learnable activation functions and
over-parameterized weights. We also established the generalization error bound for the proposed
model via Rademacher complexity. We showed that reweighted-RNN has good generalization
properties and its error bound is tighter than existing RNNs in function of the number of time
steps. Experimentation on the task of sequential video-frame reconstruction shows that our model (i)
outperforms various state-of-the-art RNNs in terms of accuracy and (ii) is capable of stacking many
hidden layers resulting in a better learning capability than the existing unfolding methods.
3In our experiments, the 2-layer LSTM network outperforms the 3-layer one (see Table 3), the default setting
for LSTM is thus using 2 layers.
8
Under review as a conference paper at ICLR 2020
References
Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks.
In Proceedings of the 33rd International Conference on International Conference on Machine
Learning (ICML), 2016.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Advances in Neural Information Processing Systems 30, pp. 6240-6249. 2017.
A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse
problems. SIAM Journal on Imaging Sciences, 2(1):183-202, 2009.
M. Borgerding, P. Schniter, and S. Rangan. AMP-inspired deep networks for sparse linear inverse
problems. IEEE Transactions on Signal Processing, 65(16):4293-4308, 2017.
Emmanuel J. Candes, Michael B. Wakin, and StePhen P. Boyd. Enhancing sparsity by reweighted 'ι
minimization. Journal of Fourier Analysis and Applications, 14(5):877-905, 2008.
Xiaohan Chen, Jialin Liu, Zhangyang Wang, and Wotao Yin. Theoretical linear convergence
of unfolded ista and its practical weights and thresholds. In Advances in Neural Information
Processing Systems 31, 2018.
K. Cho, B. Van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio.
Learning phrase representations using rnn encoder-decoder for statistical machine translation.
arXiv preprint arXiv:1406.1078, 2014.
Corinna Cortes, Xavier Gonzalvo, Vitaly Kuznetsov, Mehryar Mohri, and Scott Yang. AdaNet:
Adaptive structural learning of artificial neural networks. In Proceedings of the 34th International
Conference on Machine Learning, Sydney, Australia, 06-11 Aug .
I. Daubechies, M. Defrise, and C. De Mol. An iterative thresholding algorithm for linear inverse
problems with a sparsity constraint. Communications on Pure and Applied Mathematics, 57(11):
1413-1457, 2004.
D. Donoho. Compressed sensing. IEEE Transactions on Information Theory, 52(4):1289-1306,
April 2006.
Jeffrey L. Elman. Finding structure in time. Cognitive Science, 14(2):179 - 211, 1990.
Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of
neural networks. In Proceedings of the 31st Conference On Learning Theory, volume 75, pp.
297-299, 06-09 Jul 2018.
Karol Gregor and Yann LeCun. Learning fast approximations of sparse coding. In Proceedings of the
27th International Conference on International Conference on Machine Learning, ICML’10, pp.
399-406, 2010.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In 2016 IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770-778, June 2016.
S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735-1780,
1997.
G.	Huang, Z. Liu, L. v. d. Maaten, and K. Q. Weinberger. Densely connected convolutional networks.
In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Aditya Kusupati, Manish Singh, Kush Bhatia, Ashish Kumar, Prateek Jain, and Manik Varma.
Fastgrnn: A fast, accurate, stable and tiny kilobyte sized gated recurrent neural network. In
Advances in Neural Information Processing Systems 31, 2018.
Hung Duy Le, Huynh Van Luong, and Nikos Deligiannis. Designing recurrent neural networks by
unfolding an l1-l1 minimization algorithm. In Proceedings of IEEE International Conference on
Image Processing, 2019.
9
Under review as a conference paper at ICLR 2020
Quoc V. Le, Navdeep Jaitly, and Geoffrey E. Hinton. A simple way to initialize recurrent networks of
rectified linear units. CoRR, abs/1504.00941, 2015.
S. Li, W. Li, C. Cook, C. Zhu, and Y. Gao. Independently recurrent neural network (indrnn):
Building a longer and deeper rnn. In 2018 IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp. 5457-5466, June 2018.
Jialin Liu, Xiaohan Chen, Zhangyang Wang, and Wotao Yin. ALISTA: Analytic weights are as good
as learned weights in LISTA. In International Conference on Learning Representations, 2019.
A. Lucas, M. Iliadis, R. Molina, and A. K. Katsaggelos. Using deep neural networks for inverse
problems in imaging: beyond analytical methods. IEEE Signal Processing Magazine, 35(1):20-36,
2018.
W. Luo, W. Liu, and S. Gao. A revisit of sparse coding based anomaly detection in stacked rnn
framework. In 2017 IEEE International Conference on Computer Vision (ICCV), pp. 341-349,
Oct 2017.
H.	V. Luong, N. Deligiannis, J. Seiler, S. Forchhammer, and A. Kaup. Compressive online robust
principal component analysis via n-`1 minimization. IEEE Transactions on Image Processing, 27
(9):4314-4329, Sept 2018.
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of Machine Learning,
Second edition. MIT Press, Cambridge, Massachusetts, USA, 2018. ISBN 9780262039406.
J.	F. C. Mota, N. Deligiannis, A. C. Sankaranarayanan, V. Cevher, and M. R. D. Rodrigues. Adaptive-
rate reconstruction of time-varying signals with application in compressive foreground extraction.
IEEE Transactions on Signal Processing, 64(14):3651-3666, July 2016.
J.	F. C. Mota, N. Deligiannis, and M. R. D. Rodrigues. Compressed sensing with prior information:
Strategies, geometry, and bounds. IEEE Transactions on Information Theory, 63(7):4472-4496,
July 2017.
Ali Mousavi, Ankit B Patel, and Richard G Baraniuk. A deep learning approach to structured signal
recovery. In 2015 53rd Annual Allerton Conference on Communication, Control, and Computing
(Allerton), pp. 1336-1343. IEEE, 2015.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural
networks. In Proceedings of The 28th Conference on Learning Theory, 2015.
Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan Srebro. The
role of over-parametrization in generalization of neural networks. In Int. Conf. on Learning
Representations, ICLR 2019, 2019.
R. Pascanu, C. Gulcehre, K. Cho, and Y. Bengio. How to construct deep recurrent neural networks.
arXiv preprint arXiv:1312.6026, 2013.
R. Pascanu, C. Gulcehre, K. Cho, and Y. Bengio. How to construct deep recurrent neural networks.
In International Conference on Learning Representations (ICLR), 2014.
Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From Theory to
Algorithms. Cambridge University Press, New York, NY, USA, 2014. ISBN 1107057132,
9781107057135.
P. Sprechmann, A. M. Bronstein, and G. Sapiro. Learning efficient sparse and low rank models. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 37(9):1821-1833, Sep. 2015.
N. Srivastava, E. Mansimov, and R. Salakhudinov. Unsupervised learning of video representations
using LSTMs. In Proceedings of the 32nd International Conference on Machine Learning (ICML),
2015.
J. Sun, H. Li, and Z. Xu. Deep ADMM-Net for compressive sensing MRI. In Advances in Neural
Information Processing Systems, pp. 10-18, 2016.
10
Under review as a conference paper at ICLR 2020
S. Wisdom, T. Powers, J. Pitton, and L. Atlas. Building recurrent networks by unfolding iterative
thresholding for sequential sparse recovery. In 2017 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), March 2017.
B. Xin, Y. Wang, W. Gao, D. Wipf, and B. Wang. Maximal sparsity with deep networks? In Advances
in Neural Information Processing Systems,pp. 4340-4348, 2016.
Jiong Zhang, Qi Lei, and Inderjit S. Dhillon. Stabilizing gradients for deep neural networks via
efficient SVD parameterization. In Proceedings of the 35th International Conference on Machine
Learning, ICML 2018, 2018.
11
Under review as a conference paper at ICLR 2020
A Supplementary experiments
In our experiments, we use the publically available Tensorflow implementations for FastRNN4,
IndRNN5, and SpectralRNN6 cells. While Sista-RNN, 'ι-'ι-RNN and reweighted-RNN have their
own layer-stacking schemes derived from unfolding minimization algorithms, we use the stacking
rule in Pascanu et al. (2013) [see Fig 2(c)] to build deep networks for other RNN models.
Figure 3 shows the learning curves of all methods under the default setting. It can be seen that
reweighted-RNN achieves the lowest mean square error on both the training and validation sets. It can
also be observed that the unfolding methods converge faster than the stacked RNNs, with the proposed
reweighted-RNN being the fastest. More experimental results for the proposed reweighted-RNN
are provided to illustrate the learning curves, which measure the average mean square error vs. the
training epochs between the original frames and their reconstructed counterparts, with different CS
rates [Fig. 4], different network depths d [Fig. 6], and different network widths h [Fig. 5].
50	100	150	200
Training epochs
(a) sRNN.
50	100	150	200
Training epochs
(b) LSTM.
50	100	150	200
Training epochs
(c) GRU.
50	100	150	200
Training epochs
(d) IndRNN.
50	100	150	200
Training epochs
(e) FastRNN.
50	100	150	200
Training epochs
(f) SpectralRNN.
50	100	150	200
Training epochs
(g) Sista-RNN.
50	100	150	200
Training epochs
(h) 'ι-'ι-RNN.
50	100	150	200
Training epochs
(i) reweighted-RNN.
Figure 3:	Average mean square error between the original and reconstructed frames vs. training
epoches on the training and the validation sets for the default setting (a CS rate is 0.2, d = 3, h = 210).
Since we use different frameworks to implement the RNNs used in our benchmarks, we do not
report and compare the computational time for training of the models. Specifically, we rely on the
Tensorflow implementations from the authors of Independent-RNN, Fast-RNN and Spectral RNN,
while the rest is written in Pytorch. Furthermore, even among Pytorch models, the vanilla RNN,
LSTM, and GRU cells are written in CuDNN (default Pytorch implementations), so that they are
significantly faster in training than the others. This does not mean that these networks have better
runtime complexities, but rather more efficient implementations. However, an important comparison
could be made between 'ι-'ι-RNN Le et al. (2019) (as the baseline method) and Reweighted-RNN
due to their similarities in implementations. At the default settings, it takes 3,521 seconds and 2,985
seconds to train Reweighted-RNN and 'ι-'ι-RNN Le et al. (2019), respectively.
4Code available at https://github.com/microsoft/EdgeML
5Code available at https://github.com/batzner/indrnn
6Code available at https://github.com/zhangjiong724/spectral-RNN
12
Under review as a conference paper at ICLR 2020
50	100	150
Training epochs
-----train loss
(a)	CS Rate is 0.1.
]
P≡ST-B3≡
100	150	200
validation loss
Training epochs
(b)	CS Rate is 0.2.
-----train loss
-----validation loss
100
hobs P≡S7~ su≡
50	100	150	200
Training epochs
train loss
validation loss
200
Training epochs
(C) CS Rate is 0.3.
train loss
validation loss
50	100	150	200
Training epochs
二
(d) CS Rate is 0.4.	(e) CS Rate is 0.5.
Figure 4:	Reweighted-RNN: Average mean square error between the original and reConstruCted frames
vs. training epoChes on the training and the validation sets with different CS rates (d = 3, h = 210).
50	100	150	200
Training epochs
(a)	h = 27.
50	100	150	200
Training epochs
(b)	h = 28.
50	100	150	200
Training epochs
(C) h = 29.
(d) h = 210.
]
-P≡S7~SU≡
-----train loss
-----validation loss
⅛
50	100	150	200
Training epochs
Training epochs
(f) h = 212.
(e) h = 211.
Figure 5:	Reweighted-RNN: Average mean square error between the original and reconstructed
frames vs. training epoches on the training and the validation sets with different network widths h (a
CS rates is 0.2, d = 3).
ɪ00
IBnbe
50	100	150	200
Training epochs
(a)	d = L
50	100	150	200
Training epochs
(b)	d = 2.
ωl
IBnbe
50	100	150
Training epochs
(c)	d = 3.
200
P≡ST-B3≡
Training epochs
50	100
Training epochs
(d)	d = 4.
(e)	d = 5.
Training epochs
(f) d = 6.
Figure 6:	Reweighted-RNN: Average mean square error between the original and reconstructed
frames vs. training epoches on the training and the validation sets with different network depths d (a
CS rate is 0.2, h = 210).
13
Under review as a conference paper at ICLR 2020
B The Proximal Operator for Problem (4)
Proposition B.1. The proximal operator Φ ʌi λ2 方(u) in (6)for the reweighted '1 -'1 minimization
ɪ	'	'	C g, C g, ~' '	"	3
problem (4), for which g(v) = λιg∣v∣ + λ2g∣v -
~∣ ,is given by
	u- F g - λ g, ~ + 卷 g +
	~,	~ + F g -
φ λ1 g,λ2 g,~≥0(U)= '	u - F g + 乌 g,普 g - λ2 g
CC	0,	―誓g -率
	、u + λC1 g + λC2g,	-∞ < U <
	'u - Fg -乌g,誓g + λ2g
	0,	―誓g+率
φ λg, λ2 g,~<0(U)= (	U + F g - λC2 g,	h -普 g +
	~,	~-普 g-
	U - λC1 g + λC2g,	-∞ < U <
< ∞
u ≤ ⅜l g +
< u < - λ1
≤ U ≤ ~ 一
λ1 fr λ2 fr
工 g----T g
λc2 g <U< ∞
λ2 g ≤ U ≤ ~ +
<u< ~ + λ1 g
g ≤ U ≤ λ1 g -
-λ1 g - λ2 g
c g c g
≤ g g
g 及 cλ2
λc1 g + λc2
-λ2 g
λ2 g
等g
g + λC2 g
普 g + λC2
g
Proof. We compute the proximal operator Φ&g ʌɪg ~(u) (19) for ~ ≥ 0, it is similar for ~
From (6), Φ ʌig ʌɪg ~(u) is expressed by:
φλιε λ2更 ~(U) = argmin 12(V) ：= λgg∣v∣ + λ2g∣v - ~∣ + 1 ∣v - u∣2∣.
C g, C g,"	v∈R I	CC	2	J
(19)
(20)
< 0.
(21)
We first consider the ∂夕(v)∕∂v in V ∈ {(-∞, 0), (0, ~), (而, ∞)}, in which ∂夕(V) exists. Taking the
derivative of 夕(V) in these intervals delivers
d：(V)	=	λgg ∙ sign(v)	+ λ2g ∙ sign(v -	~)	+	(v	-	u),	(22)
∂V C	C
where sign(.) is a sign function. When setting ∂夕(v)∕∂v = 0 to minimize 夕(v), we derive:
(U —
U-
U+
: : :
ggg
λ22cλ22cλ22c
-+ +
ggg
- e- e- C
~ < v < ∞
0 <v< ~	(23)
-∞ < v < 0
From (21) and (23), we have
: : :
ggg
MδCXC
-+ +
ggg
- c一 c一 C
+
U
rjʌl
=
U)
为
⅛
c
φ
~ + λC1 g + λc2 g <U< ∞
λc1 g - λC2 g <U< ~ + λc1 g - λc2 g	(24)
-∞ < U < -λc1 g - λc2g
In the remaining range value of u, namely, - λ1 g 一 λC2 g ≤ U ≤ λ1 g 一 λC2 g and ~ + λ1 g 一 λC2 g ≤
U ≤ ~ + λ1 g + λ2g, we prove that the minimum of 夕(V) (21) is obtained when V = 0 and V = ~,
respectively.
Let us rewrite 夕(v), which was defined in (21), as
中(V) = g g∣v∣+ λ2 g∣v -川 +1 Iv - uI2	(25)
By applying the inequality ∣α - b∣ ≥ ∣α∣ - ∣b∣, where a, b ∈ R, to (25), we obtain:
夕(V) ≥ λ1 g∣v∣ + λ2g∣v∣ - λ2g∣~∣ + 1 v2 - vu + 1U2
CCC 2	2
≥ ∣v∣ (-1gg + -C2g - ∣u∣) + 2v2 - -C2g∣~∣+ 2u2	(26)
For - λ1 g-λ2 g ≤ u ≤ λ1 g-λ2 g, from (26),夕(v) is minimal when V = 0, due to λ1 g+ λ2 g-∣u∣ ≥
0.
U
U
—
—
g
14
Under review as a conference paper at ICLR 2020
Similarly, for ~ + λC1 g - λC2g ≤ U ≤ ~ + 卷g + λC2g, i.e.,卷g - λC2g ≤ U - ~ ≤ λC1 g + λC2g, we have
夕(V) ≥g|v 一 ~| —1 g|~| +—2g|v — ~| + -(v — ~)2 — |v 一 ~||u — ~| + -(u — ~)2
c	cc	2	2
≥∣v —	~|	(—g +—2g	—	|u	— ~|)	+	-(v	— ~)2-1 g|~|	+	3(U —	~)2.	(27)
cc	2	c 2
From (27),夕(V) is minimal when V = ~, since λ1 g + λ2g — |u — ~| ≥ 0. Combining these results
with the result in (24), we conclude the proof.	□
C Generalization and deep unfolding RNNs
C.1 Generalization error bound definition
Notations. Let fW(d) : Rn 7→ Rh be the function computed by a d-layer network with weight
parameters W. The network fW(d) maps an input sample xi ∈ Rn (from an input space X) to an
output yi ∈ Rh (from an output space Y), i.e., yi = fW(d)(xi). Let S denote a training set of size m,
i.e., S = {(xi, yi)}m=ι and EMz)〜S[∙] denote an expectation over (xi, Ni) from S. The set S is
drawn i.i.d. from a distribution D, denoted as S 〜Dm, over a space Z = XXY. Let F be a (class)
set of functions. Let ` : F × Z 7→ R denote the loss function and ` ◦ F = {z 7→ `(f, z) : f ∈ F}.
We define the true loss and the empirical (training) loss by LD(f) and LS(f), respectively, as follows:
LD (f )= E(XiZ)S ['(f(xi), %)],	(28)
and
Ls (f )= E(χi,yAs ['(f(xi)，%)].	(29)
The generalization error, which is defined as a measure of how accurately a learned algorithm is able
to predict outcome values for unseen data, is calculated by LD (f) — LS(f).
Rademacher complexity. Let F be a hypothesis set of functions (neural networks). The empirical
Rademacher complexity of F (Shalev-Shwartz & Ben-David, 2014) for a training sample set S is
defined as follows:
1m
Rs(F) = - E1	SUp X qf g)，	(30)
m ∈{±1}m f∈F
i=1
where = (1, ..., m); here i are independent uniformly distributed random (Rademacher) variables
from {±1}, according to P[i = 1] = P[i = —1] = 1/2.
The generalization error bound (Shalev-Shwartz & Ben-David, 2014) is derived based on the
Rademacher complexity defined in the following theorem:
Theorem C.1. (Shalev-Shwartz & Ben-David, 2014, Theorem 26.5)
Assume that ∣'(f, z)| ≤ η for all f ∈ F and Z. Then, for any δ > 0, with probability at least 1 — δ,
Ld (f) — Ls(f) ≤ 2Rs(' ◦ F) +4外尸"⑷.
m
(31)
It can be noted that the bound in (31) via the Rademacher complexity depends on the training set S,
which makes it applicable to a number of learning problems, e.g., regression and classification, under
given a loss function `.
C.2 Notation for Deep Unfolded RNNs
In this subsection, we provide the required notation for the proposed reweighted-RNN model, `1-
'1-RNN, and Sista-RNN, which will be used in the derivation of their respective generalization
analysis.
The proposed reweighted-RNN. Let ht(l) be the hidden states in layer l evolving in time step t. We
write the reweighted-RNN model recursively as h(t1) = fW(1),U (h(td-)1, xt) = Φ(W1ht(d-)1+U1xt) and
ht(l)=fW(l),U(ht(-d)1,xt)=Φ WlfW(l-,U1)(ht(-d)1
, xt) + Ulxt, where Φ is an activation function. The
15
Under review as a conference paper at ICLR 2020
hidden state is updated as shown in (9). The real-valued family of functions, Fd,t : Rh × Rn 7→ R,
for the functions fW(d),U in layer d is defined by:
Fd,t = n(h(td-)1,xt) 7→ Φ(wdTfW(d-,U1)(ht(-d)1, xt) + udTxt) : kWdk1,∞ ≤ αd, kUdk1,∞ ≤ βdo,
(32)
where αl , βl are nonnegative hyper-parameters for layer l, where 1 < l ≤ d. In layer l = 1, the
real-valued family of functions, F1,t : Rh × Rn 7→ R, for the functions fW(1),U is defined by:
F1,t =	n(h(td-)1,xt)	7→ Φ(w1Tht(-d)1	+	u1Txt)	:	kW1k1,∞	≤	α1,	kUk1,∞ ≤ β1o,	(33)
where α1, β1 are nonnegative hyper-parameters. We denote the input layer as fW(0),U = ht(-d)1, in
particular, at t = 1, h(0l) ≡ h0 .
The 'ι-'ι-RNN model (Le et al., 2019). The hidden state h，) for 'ι-'ι-RNN is updated as shown
in (13). The real-valued family of functions, Fd,t : Rh × Rn 7→ R, for the function fW(d),U in layer d
is defined by:
Fd,t = n(h(td-)1,xt) 7→ Φ(w2TfW(d-,U1)(h(td-)1, xt) + u1Txt) : kW2k1,∞ ≤ α2, kU1k1,∞ ≤ β1o,
(34)
where α2, β1 are nonnegative hyper-parameters for layer l, where 1 < l ≤ d. In layer l = 1, the
real-valued family of functions, F1,t : Rh × Rn 7→ R, for the functions fW(1),U is defined by:
F1,t = (ht(d-)1,xt) 7→ Φ(w1Tht(-d)1 + u1Txt) : kW1k1,∞ ≤ α1, kUk1,∞ ≤ β1 ,	(35)
where α1, β1 are nonnegative hyper-parameters.
The Sista-RNN model (Wisdom et al., 2017). The hidden state ht(l) in Sista-RNN is updated by:
ʃ φ(Wιh(-)ι + Uιxt),	l = 1,	(36)
t [ φ(W2h(l-1) + Um + U2h(-)J,l> 1,
The real-valued family of functions, Fd,t : Rh × Rn 7→ R, for the functions fW(d),U in layer d is
defined by:
Fd,t = n(ht(-d)1,xt) 7→ φw2TfW(d-,U1)(ht(-d)1, xt) + u1Txt + u2Th(td-)1
: kW2k1,∞ ≤ α2, kUk1,∞ ≤β1, kUk2,∞ ≤ β2o,	(37)
where α2, β1, β2 are nonnegative hyper-parameters. In layer l = 1,
F1,t = n(h(td-)1,xt) 7→ φw1Th(td-)1 + u1Txt : kW1k1,∞ ≤ α1, kUk1,∞ ≤ β1o,	(38)
where α1, β1 are nonnegative hyper-parameters.
D Proof of Theorem 3.1
Proof. We consider the real-valued family of functions Fd,T : Rh × Rn 7→ R for the functions fW(d),U
to update h(Td) in layer d, time step T , defined as
Fd,T = n(h(Td-)1,xT) 7→ Φ(wdTfW(d-,U1)(h(Td-) 1,xT) + udTxT) : kWdk1,∞ ≤ αd, kUdk1,∞ ≤ βdo,
(39)
16
Under review as a conference paper at ICLR 2020
where wd, ud are the corresponding rows from Wd, Ud, respectively, and αl , βl, with 1 < l ≤ d,
are nonnegative hyper-parameters. For the first layer and the first time step, i.e., l = 1, t = 1, the
real-valued family of functions, F1,1 : Rh × Rn 7→ R, for the functions fW(1),U is defined by:
F1,1 = (h0,x1) 7→ Φ(w1Th0 + u1Tx1) : kW1k1,∞ ≤ α1, kUk1,∞ ≤β1 ,	(40)
where α1 , β1 are nonnegative hyper-parameters. We denote the input layer as fW(0),U = h0 at the first
time step. From the definition of Rademacher complexity in (30) and the family of functions in (39)
and (40), we obtain:
mRS (Fd,T)
(41a)
≤E
∈{±1}m
sup
W,U
kwd k1 ≤αd
kudk1≤βd
m
X iΦ wdTfW(d-,U1) (hT-1,i, xT,i) + udTxT,i
i=1
ɪ log exp ( E
λ	∈{±1}m
sup
W,U
kwd k1 ≤αd
kudk1≤βd
m
λXi wdTfW(d-,U1)(hT-1,i,xT,i) +udTxT,i
i=1
llθg e∈{El}m
sup exp λ
W,U
kwdk1≤αd
kudk1≤βd
Xi wdTfW(d-,U1)(hT-1,i,xT,i) + λ X iudTxT,
i=1
i=1
,i
(41b)
≤
≤
≤ 1lθg e∈{El}m
m
m
m
λ i wdTfW(d-U1)(hT-1,i,xT,i
i=1	,
kwdk1≤αd
sup exp λ	iudTxT,i	,
kudk1≤βd	i=1
(41c)
where λ > 0 is an arbitrary parameter, Eq. (41b) follows Lemma D.1 for 1-Lipschitz Φ a long with
Inequality (62), and (41c) holds by Inequality (59).
For layer 1 ≤ l ≤ d and time step t, let us denote:
sup exp
W,U
kwl k1≤αl
sup exp
kulk1≤βl
(42)
(43)
dd
where Λl is defined as follows: Λd = 1, Λl = Q αk with 1 ≤ l ≤ d - 1, and Λ0 = Q αk.
k=l+1	k=1
Following the Holder,s inequality in (58) in case of P = 1 and q = ∞ applied to WT and
fW( -,U) (ht-1,i, xt,i) in (42), respectively, we get:
∆(hdt)-1,xt
(44)
≤
sup
W,U
kWd-1 k1,∞≤αd-1
kUd-1 k1,∞≤βd-1
exp λαd
m
X iΦ Wd-1fW( -,U) (ht-1,i, xt,i) + Ud-1xt,i
i=1
∞
17
Under review as a conference paper at ICLR 2020
≤ sup exp λαd max
W ,u	∖	k∈{i,…，h}
∣∣wd-1,k∣∣1≤αd-1
∣∣ud-1,kk1≤βd-1
m
X qφ (WTTk fWU(htT，i，xt,i) + uT-1,kxt,i
i=1
≤ sup exp λαd
W ,U	∖
l∣Wd-1,k∣∣1≤αd-1
Il ud- 1 ,k ∣ 1 ≤8d-1
Similarly, from (43), we obtain:
m
X e® (WT-1,kfWU(htT，i, xt,i) + UT-1,kxt,i)
i=1
∆Xd) ≤	sup exp λy'eiUTxt,i ≤ exp λ
m ∣1≤βd	∖ M J ∖
m
=JxWL
≤ exp λ
m
(45)
eixτ,i,κ
i=1
(46)
where {τ, κ} = argmax I P Gxt,i,j ∣.
t∈{1,...,T },j∈{1,...,n} 1 i=1	1
From (41c), (44), and (46), we get:
mRs (Fd,T)
≤
1iθg e∈{E1}m
sup
W,U
∣∣Wd-1,k Il 1 ≤ad-1
∣∣ud-1,k Il 1 ≤βd-1
exp λαd
m
X eiφ
i=1
1,kfW,U)(hT-1,i, xT,i) + UT-1,kxT,i
m
+ λ
eixτ,i,κ
i=1
≤
Eg e∈{E1}m
sup
W,U
∣∣Wd-1,k Il 1 ≤ad-1
∣∣ud-1,k Il 1 ≤βd-1
(m
λαd X eiφ (WT-1,kfW-2 (hT-1,i, xT,i) + UT-1,kxT,i
i=1
m
+ exp I λad X eiφ(WT-1,kfWU2(hT-1,i, xT,i) + UT-1,kxT,i
m
eixτ,i,κ
i=1
m
+ exp
—
+ exp
—
i=1
m
λαd X eiφ (WT-1,kfW-2 (hT-1,i, xT,i) + UT-1,kxT,i
i=1
m
λαd X eiφ (WT-1,k fWU2 (hT-1,i, xT,i) + UT-1,kxT,i
i=1
eixτ,i,κ
i=1
m
ei xτ,i,κ
i=1
)-λβdX …)))
≤
1 log ( 4
λ ∖ €<
∈{E1}m Ci)H 渡T) e
m
eixτ,i,κ
i=1
(47a)
≤
1log I 4d-1 E
λ ∖ e∈{土1}m
1,XT & exp (λ(X Biλ]X eixτ,i,κ))
∖ l=2	i=1	) ∖)
(47b)
≤
1log ( 4d-1	E exp
λ ∖	e∈{土1}m
d
βl
l=2
m
eixτ,i,κ
i=1
sup exp λΛ1 fei(UTxT,i)
∣U1∣1≤β1	∖ i=1 '	j )∖)
sup exp λΛ1
∣∣w1∣∣1≤α1	∖
18
Under review as a conference paper at ICLR 2020
≤ 1log I 4d-1	E
λ	∈{±1}m
exp
βl	Eixτ,i,κ	sup exp	λΛ0
i=1	W,U
kwd k1 ≤αd
kudk1≤βd
m
XEihT-1,i∞
i=1	∞
• exp λβ1 Λ
m
EixT,i
i=1	∞
(47d)
≤ 1 log I 4d	E
λ	∈{±1}m
sup exp
W,U
kwd k1 ≤αd
kudk1≤βd
m
λΛ0 X iΦ wdTfW(d-,U1)
i=1
(hT-2,i, xT -1,i) + udTxT-1
(47e)
where (47a) holds by inequality (59), and (47b) follows by repeating the process from layer d - 1 to
layer 1 for time step T. Furthermore, (47c) is obtained as the beginning of the process for time step
T - 1 and (47d) follows inequality (58).
Proceeding by repeating the above procedure in (47e) from time step T - 1 to time step 1, we get:
mRS (Fd,T )
≤ 1 log (4dT	E
λ	∈{±1}m
exp
exp
(48)
Let Us denote μ = argmax I P Eiho,jI, from (48), We have:
j∈{1,...,h} i=1
mRS(Fd,T)
≤ 1 log (4dT	E
λ	∈{±1}m
"酸+ ∖og E
λ	2λ ∖ e∈{±1}m
exp λ X βl
l=1
—
QXτ,i,J exp 卜ΛT X qho,μ
i=1
exp λ X βl
l=1
—
Ei xτ,i,κ
i=1
)))
≤
• exp 0ΛT X qhoj )
2
≤
2d4 + 3log E
λ	2λ ∈{±1}m
exp (2λ(X βM)( λλT÷T ) X eiXτ,i,J]
≤
+ 21λ log e∈{Ei}m
exp ^2λΛT X 地0,“)]
(49a)
2dT log 2ɪ 1 1 S 党
——+ 2λ logΣ e∈{Ei}m
j=1
1h
+ 2λ logΣ e∈{Ei}m
j=1
eχp (2λ(X β λi)( λ⅛T÷I ) X eiχτ,i,J]
eχp 2λΛ0T X Eih0,j
(49b)
≤
2dTlog2	1 X Y
——十 2λ logΣ Πe∈{Ei}m
j=1 i=1
eχp (2λ(X 风4)( ^T-J )qχτ,i,J]
1 hm
+ 2λ log XY e∈{Ei}m
j=1 i=1
exp 2λΛ0TEih0,j
19
Under review as a conference paper at ICLR 2020
≤
T+ 2λ log X Y
j=1 i=1
2 exp (2λ(χβ
2	l=1
xτ,i,j
≤
≤
≤
+2exp (-2λ(X β*l)( λ0-γ )xτJ]
hm
+2λ log XY
2 exp(2X/Thi。,) + 2 exp ( - 2λΛTho,j)]
2dT log 2	1 n
+ 2λ log∑
j=1
1h
+ 2λlog X
j=1
λ
exp(2λ2(X m )2( Λ-∙ )2X χτ,i,j)]
exp 2λ2Λ20T Xm h20,j
(49c)
2dT log 2	log n /ʌ
+ 丁+MTel
l=1
2dT log 2 + log √n + log √h
+λ
mBX + 二g- + λ∕0T mkho ll∞
2λ
ΛT-1 )2mBX + Λ2Tmkhok∞), (49d)
where (49a) follows inequality (61), and (49b) holds by replacing with Pjn=1 and Pjh=1, respectively.
In addition, (49c) follows (60), and (49d) is obtained by the following definition: At time step t, we
define Xt ∈ Rn×m, a matrix composed of m columns from the m input vectors {Xt,i}im=1; we also
define ∣∣Xt∣2,∞ = ,yax)Pm=I x2i k ≤ √mBx, representing the maximum of the '2-norms
of the rows of matrix Xt, and lh0l∞ = max |h0 j|.
j
Choosing λ = u
t
2dT log 2+log √∕n+log √h
Pl βιΛι)2(Λ⅛)2mΒX+Λ2tmkh0k∞
, we achieve the upper bound:
RS (Fd,T) ≤ t
2(4dT log2 + logn + log h)
m
(X β1Λ1 )2( Λt-≡Γ )2BX + Λ0Tkhok∞)∙
l=1	Λ0 - 1
(50)
It can be noted that RS (Fd,T) in (50) is derived for the real-valued functions Fd,T. For the vector-
valued functions Fd,T : Rh × Rn 7→ Rh (in Theorem 3.1), we apply the contraction lemma
(Lemma D.1) to a Lipschitz loss to obtain the complexity of such vector-valued functions by means
of the complexity of the real-valued functions. Specifically, in Theorem 3.1, under the assumption of
the 1-Lipschitz loss function and from Theorem C.1, Lemma D.1, we complete the proof.
□
D. 1 Comparison with existing generalization bounds
Recent works have established generalization bounds for RNN models with a single recurrent layer
(d = 1) using Rademacher complexity [see FastRNN in Kusupati et al. (2018)] or PAC-Bayes theory
[see SpectralRNN in Zhang et al. (2018)]. We re-state these generalization bounds below and apply
Theorem 3.1 with d = 1 to compare with our bound for reweighted-RNN.
FastRNN (Kusupati et al., 2018). The hidden state ht of FastRNN is updated as follows:
~
h t = φ(Wht-ι + UXt)
1	1~	,11
ht = aht + bht-1,
(51)
20
Under review as a conference paper at ICLR 2020
where 0 ≤ a, b ≤ 1 are trainable parameters parameterized by the sigmoid function. Under the
assumption that a + b = 1, the Rademacher complexity RS (FT ) of the class FT of FastRNN
(Kusupati et al., 2018), with kWkF ≤ αF, kUkF ≤ βF, and kxtk2 ≤ B, is given by
RS (FT ) ≤
(1 + a(2αp - 1))T+1 - 1
a(2αF - 1)
(52)
Alternatively, under the additional assumption that a ≤ ?*；i)t, the bound in KUsUPati et al.
(2018) becomes:
1
RS (FT) ≤
2a	2a(2αF - 1)(T + 1) -
√mBβF (	(2αF - 1)一
(53)
SpectralRNN (Zhang et al., 2018). The hidden state ht and output yt ∈ Rny of SpectralRNN are
computed as:
ht =φ(Wht-1 + Uxt)
yt =Yht,
(54)
where Y ∈ Rny×h. The generalization error in Zhang et al. (2018) is derived for a classification
problem. For any δ > 0,γ > 0, with probability ≥ 1 - δ over a training set S of size m, the
generalization error (Zhang et al., 2018) of SpectralRNN is bounded by
O
B2TYξ2ln(ξ) (IWkF + kUkF + IMF) Y + ln 胃
m
(55)
where ζ = max{kWk22T -2, 1} max{kUk22, 1} max{kYk22, 1} and ξ = max{n, ny, h}.
Reweighted-RNN. Based on Theorem 3.1, under the assumption that the initial hidden state h0 = 0,
the Rademacher complexity of reweighted-RNN with d = 1 is bounded as
Rs(F1,τ) ≤ J”log2 + mgn + logh(√2β1 IT-IBx).	(56)
We observe that the bound of SpectralRNN in (55) depends on T2, whereas the bound of FastRNN
either grows exponentially with T (52) or is proportional to T (53). Our bound (56) depends on
√T, given that the second factor in (56) is only dependent on the norm constraints α1, β1 and the
input training data; meaning that it is tighter than those of SpectralRNN and FastRNN in terms of the
number of time steps.
D.2 Background on Rademacher complexity calculus
The contraction lemma in Shalev-Shwartz & Ben-David (2014) gives the Rademacher complexity of
the composition of a class of functions with ρ-Lipschitz functions.
Lemma D.1. (Shalev-Shwartz & Ben-David, 2014, Lemma 26.9—Contraction lemma)
Let F be a set of functions, F = {f : X 7→ R}, and Φ1, ..., Φm, ρ-Lipschitz functions, namely,
∣Φi(α) 一 Φi (β)∣ ≤ ρ∣α 一 β | for all 1, β ∈ R for some ρ > 0. For any sample set S of m points
x1, ..., xm ∈ X, let (Φ ◦ f)(xi) = Φ(f(xi)). Then,
m ∈{±1}m
m
SUp X q(φ ◦ f )M)
f∈F i=1
m
≤ — E SUp X Gf(Xi)
m ∈{±1}m f∈Fi=1
(57)
1
E
alternatively, RS(Φ ◦ F) ≤ ρRS (F), where Φ denotes Φ1(X1), ..., Φm(Xm) for S.
Proposition D.2. (MOhri et al., 2018, PropositionA.1一Holder's inequality)
Let p,q ≥ 1 be conjugate: 11 + q1 = 1. Then, for all x, y ∈ Rn,
kχ ∙ yk1 ≤ kχkpkykq,
(58)
with the equality when |yi| = |xi |1-1 for all i ∈ [1, n].
21
Under review as a conference paper at ICLR 2020
Supporting inequalities:
(i)	If A, B are sets of positive real numbers, then:
SuP(AB) = SuP(A) ∙ SuP(B).	(59)
(ii)	Given x ∈ R, we have:
eχp(X) +eχpJχ) ≤ eχp(χ2∕2).	(60)
(iii)	Let X and Y be random variables, the CaUchy-BUnyakovsky-SchWarz inequality gives:
(E[XY])2 ≤ E[X2] ∙ E[Y2].	(61)
(iv)	If ψ is a convex fUnction, the Jensen’s ineqUality gives:
ψ(E[X]) ≤ E[ψ(X)].	(62)
E Additional experiments
We test oUr model on three popUlar tasks for RNNs, namely the seqUential pixel MNIST classification,
the adding task, and the copy task (Le et al., 2015; Arjovsky et al., 2016; Zhang et al., 2018).
Sequential pixel MNIST and permuted pixel MNIST classification. This task aims to classify
MNIST images to a class label. MNIST images are formed by a 28×28 gray-scale image With a label
from 0 to 9. We Use the reWeighted-RNN along With a softmax for category classification. We set
d = 5 layers and h = 256 hidden Units for the reWeighted-RNN. We consider tWo scenarios: the first
one Where the pixels of each MNIST image are read in the order from left-to-right and bottom-to-top
and the second one Where the pixels of each MNIST image are randomly permUted. The classification
accUracy resUlts are shoWn in Fig. 7(a) (for pixel MNIST) and Fig. 7(b) (for permUted pixel MNIST).
(a) Pixel MNIST.
(b) PermUted pixel MNIST.
FigUre 7: ReWeighted-RNN on the (a) pixel-MNIST classification and (b) permUted pixel MNIST
classification: Average classification accUracy vs. training epoches on the validation set.
JaU°əjŋnbs ɑŋən
(a) Adding task.
(b) Copy task.
Figure 8: Reweighted-RNN on the (a) adding task with average mean square error and the (b) copy
task with average cross entropy vs. training epoches on the validation set.
Adding Task. The task inputs two sequences of length T . The first sequence consists of entries
that are uniformly sampled from [0, 1]. The second sequence comprises two entries of 1 and the
22
Under review as a conference paper at ICLR 2020
remaining entries of 0, in which the first entry of 1 is randomly located in the first half of the sequence
and the second entry of 1 is randomly located in the second half. The output is the sum of the two
entrie of the first sequence, where are located in the same posisions of the entries of 1 in the second
sequence. We also use the reweighted-RNN with d = 5 layers and h = 256 hidden units for the input
sequences of length T = 300. Fig. 8(a) shows the mean square error versus training epoches on the
validation set.
Copy task. We consider an input sequence X ∈ AT+20 (Zhang et al., 2018), where A = {a0,…，a9}.
χo, ∙∙∙ ,χ9 are uniformly sampled from {a0, ∙∙∙ , a7}, XT +10 = a9, and the remaining Xi are set
to a8. The purpose of this task is to copy χ0,…，X9 to the end of the output sequence y ∈ AT+20
given a time lag T, i.e., {yT +10, ∙∙∙ , yT +19} ≡ {x0,…,x9} and the remaining yi are equal to a&.
We set the reweighted-RNN d = 5 layers and h = 256 hidden units for the input sequences of a time
lag T = 100. Fig. 8(b) shows the cross entropy versus training epoches on the validation set.
23