Under review as a conference paper at ICLR 2020
Certifying Distributional Robustness using
Lipschitz Regularisation
Anonymous authors
Paper under double-blind review
Ab stract
Distributional robust risk (DRR) minimisation has arisen as a flexible and effective
framework for machine learning. Approximate solutions based on dualisation
have become particularly favorable in addressing the semi-infinite optimisation,
and they also provide a certificate of the robustness for the worst-case population
loss. However existing methods are restricted to either linear models or very small
perturbations, and cannot find the globally optimal solution for restricted nonlinear
models such as kernel machines. In this paper we resolve these limitations for a
general class of kernel space, and our approach is based on a new upper bound of
DRRs using an empirical risk regularised by the Lipschitz constant of the model,
e.g., deep neural networks and kernel methods. As an application, we showed that
it also provides a certificate for adversarial training, and global solutions can be
achieved on product kernel machines in polynomial time.
1 Introduction
Regularised risk minimisation has been the workhorse of learning nonlinear hypotheses such as
deep neural networks and kernel machines. Recently, distributional robust risk (DRR) minimization
has emerged as a promising instance with marked efficacy and flexibility. Instead of perturbing the
observed data points, DRRs consider perturbations to the empirical distribution, constituting an
ambiguity set P that lives in the space of data distributions. Let Ω be an outcome space with (true)
distribution μ, e.g., the joint space of input and output. Given a loss function ', a model f suffers
a loss value 'f (ω) over an outcome ω, and the risk of f under μ is risk`(f, μ) := Eμ['f]. In DRR
minimisation, a model f is sought that minimises the expectation of loss ` over an ambiguity set P,
i.e., that minimises supν∈P risk`(f, ν) (Delage & Ye, 2010; Goh & Sim, 2010; Wiesemann et al.,
2014). The ambiguity sets can be constructed by moment matching (Bhattacharyya et al., 2005;
Farnia & Tse, 2016), divergence balls (Ben-Tal et al., 2013; Duchi et al., 2016; Hu & Hong, 2016), or
Wasserstein distance balls (Kantorovitch, 1958). In this work we focus on the last due to its favorable
properties in statistics and computation, along with extensive applications in DRR (Esfahani & Kuhn,
2018; Gao & Kleywegt, 2016; Zhao & Guan, 2018).
Despite the generality of DRR, its computational efficiency remains a challenge, since the supremum
is over a (typically) uncountably infinite dimension space. Tractable equivalent convex programs can
be derived only for a limited range of loss functions along with linear hypothesis spaces (Blanchet
et al., 2016; El Ghaoui & Lebret, 1997; Shafieezadeh-Abadeh et al., 2015; 2017; Xu et al., 2009a;b).
Although Shafieezadeh-Abadeh et al. (2017) developed lifted variants for reproducing kernel Hilbert
spaces (RKHS) to accommodate nonlinear hypotheses, the perturbation was applied to Φ(ω), where
Φ is the implicit feature map. This still falls short of robustness with respect to distributions over Ω.
A more promising technique for optimizing DRRs over nonlinear hypothesis spaces—including deep
neural networks and kernel machines—is by dualising it to a form that is amenable to (approximate)
optimisation (Esfahani & Kuhn, 2018). The fundamental strong duality result was established
independently by Blanchet & Murthy (2019) and Gao & Kleywegt (2016), and has been applied
to various tasks such as specification of regularisation parameter (Blanchet et al., 2016), design of
transport cost (Blanchet et al., 2017), and selection of ambiguity region size for optimal confidence
interval (Duchi et al., 2016). In particular, Sinha et al. (2018) used it to construct an efficiently
computable certificate on the level of robustness for the worst-case population loss. However, these
methods are still subject to marked restrictions when applied to smooth nonlinear models. First,
Sinha et al. (2018) restricts the perturbation to be small, which despite the common interest of
1
Under review as a conference paper at ICLR 2020
imperceptible perturbations, leaves unaddressed the equally interesting regimes of medium to large
perturbations; see discussions in Openreview (2018). Moreover, although the global solvability of the
inner Lagrangian penalty problem (robust surrogate loss) can be ensured by small enough perturbation,
there is no practical procedure to compute the threshold. Finally, the overall optimisation of the
nonlinear model is still subject to nonconvexity, precluding tractable global solutions for restricted
but still general classes of nonlinear models such as kernel methods.
The first goal of this work, therefore, is to develop a novel certificate on distributional robustness that
dispenses with these restrictions (§3). Specifically, we will leverage the McShane-Whitney extension
theorem (McShane, 1934; Whitney, 1934) to upper bound DRRs by the empirical risk regularized
with the Lipschitz constant of the model f, while additionally accounting for the underlying transport
cost and the loss `. The result vastly generalises the vector norm regularisation in linear binary
classification (Shafieezadeh-Abadeh et al., 2017, Thm. 3.11) to nonlinear models and extended
real-valued cost functions that encode constraints, along with an arbitrary metric space of labels that
is general enough for multiclass problems. Appealing to any magnitude of perturbation, it also enjoys
improved computational efficiency compared with the robust surrogate loss in Sinha et al. (2018).
A particularly effective domain to apply this new certificate is adversarial learning (Szegedy et al.,
2014), where models are trained to be resilient to malicious distortions on the data. Although Lipschitz
regularisation has been a popular recipe for robustness (e.g., Anil et al., 2019; Cisse et al., 2017; Farnia
et al., 2019; Gouk et al., 2018; Huster et al., 2018; Scaman & Virmaux, 2018) and generalisation
accuracy (Miyato et al., 2018; Yoshida & Miyato, 2017), it remains a heuristic and therefore our
second major contribution is to reveal in §3.1 that adversarial risks (Goodfellow et al., 2015; Madry
et al., 2018; Shaham et al., 2018) can be bounded by a DRR. Such a rigorous justification has hitherto
been restricted to logistic loss (Suggala et al., 2019, Thm. 9), and a similar tightness result has been
established only for linear models (Shafieezadeh-Abadeh et al., 2017, Thm. 3.20). As a result, our
new certificate amounts to a new bound on the worst-case risk under attacks, complementing the
existing certificates (Raghunathan et al., 2018; Tsuzuku et al., 2018; Weng et al., 2018; Wong &
Kolter, 2018; Wong et al., 2018) with a more computationally efficient approach. It further achieved
state-of-the-art accuracy under a range of attacks on standard benchmark datasets (§5).
In practice, however, the evaluation of Lipschitz constant L is NP-hard for neural networks (Scaman
& Virmaux, 2018), compelling approximations of it, or explicit engineering of layers to respect
LiPschitz while analyzing the expressiveness in specific cases (e.g., '∞ norm in Anil et al. (2019)).
We, instead, pursue a new path and explore the following question: does there exist a hypothesis space
which: a) is expressive enough in modeling; b) allows the exact value of L to be computed efficiently;
c) enforcing the Lipschitz constant leads to a convex constraint that renders efficient optimisation.
Interestingly, kernel machines satisfy all these requirements for some kernels. For example, Gaussian
kernels are universal, whose RKHS can approximate any continuous function on a compact set in a
uniform sense (Micchelli et al., 2006). The RKHS of multi-layer inverse kernels compactly encom-
passes '1-regularized neural networks (Shalev-Shwartz et al., 2011), degrading the generalisation
performance by only a polynomial constant (Zhang et al., 2016; 2017). Similar results have been
conjectured for Gaussian kernels (Shalev-Shwartz et al., 2011). Our third contribution proves that
b) can be achieved for product kernels such as Gaussian kernels with high probability by using the
Nystrom approximation (Drineas & Mahoney, 2005; Williams & Seeger, 2000), and ε approximation
error of L requires only O(1∕ε2) samples (§4). Empirically this approximation is also effective for
non-product kernels like inverse kernels. Such a sampling based approach also leads to a single
convex constraint, making it scalable to 60k examples with even an interior-point solver (§5). The
convenience in evaluating L renders our certificate of DRR even more favorable than those based on
robust surrogate losses (Blanchet & Murthy, 2019; Gao & Kleywegt, 2016; Sinha et al., 2018).
2	Preliminaries
The vast majority of our technical results and proofs are deferred to Appendix A, for which theorem-
like statements are numbered to be consistent. The extended real line is R := [-∞, +∞], R≥o :=
[0, ∞], and [n] := {1, 2, . . . , n}. For topological spaces X, Y , the Borel subsets are B(X), and the
Borel probability measures are P(X). The universal sigma algebra is U(X) := Tμ∈p(χ) Bμ(X)
where Bμ(X) is the completion of the Borel sets with respect to μ ∈ P(X). Let Lo(X, Y) denote
the Borel measurable mappings X → Y, and Lι(X, μ) denote the Borel functions f ∈ L0(X, R)
with R|f | dμ < ∞ for μ ∈ P(X).
2
Under review as a conference paper at ICLR 2020
For two measures μ, ν ∈ P(Ω) the set of (μ, ν)-couplings is
Π(μ, ν) := {π ∈ P(Ω X Ω) | μ = R π( ∙ , dω), V = R π(dω, ∙ )}.
Let C ∈ Lo(Ω × Ω, R). The C-transportation cost of μ,ν ∈ P(Ω), and C-transportation cost ball of
radius r ≥ 0, centred at μ ∈ P(Ω) are respectively
costc(μ, v):= inf { J Cdπ : π ∈ Π(μ, V)} and Bc(μ, r) := {ν ∈ P(Ω) | Costc(μ, V) ≤ r}. (1)
A function f : Ω → R is C-Lipschitz if there exists L ≥ 0 such that
∀ω1,ω2 ∈ dom f: ∣f(ωι)- f (ω2)∣≤ LC(ω1,ω2).	(2)
The least C-Lipschitz constant of f (cf. Cranko et al., 2019) is the infimum over L ≥ 0 satisfying
(2), and is denoted by lipc (f), so that when (X, d) is a metric space lipd(f) agrees with the usual
Lipschitz notion. When C : X → R (e.g., when C is a norm), we take c(x, y) := C(X 一 y) for all
x, y ∈ X in (1) and (2).
3 Certificate for Distributional Robustness
While an elegant concept, the DRR suffers from a lack of tractability. That is, in order to effectively
minimise it, we first need to be able to compute or estimate it. When the loss function is convex with
respect to the input space this is straight-forward, however in general approximations are necessary.
Our first contribution is an upper bound for it.
For a function f : X → R there is another function co f : X → R, called the convex en-
velope of f. It is the greatest closed convex function that minorises f. The quantity ρ(f) :=
supx∈x(f (x) 一 Co f (x)) was first suggested by Aubin & Ekeland (1976) to quantify the lack of
convexity of a function, and has since shown to be of considerable interest for a variety of nonconvex
applications (Askari et al., 2019; Kerdreux et al., 2019; LemareChaI & Renaud, 2001;Udell & Boyd,
2016). When X = Rn there are well-known ways to compute both Co f and ρ(f), and a brief
discussion on these appears in the appendix (Remark 2 on p. 18). ρ(f) = 0 when f is closed convex.
Theorem 1. Assume X is a separable Frechet space and fix μ ∈ P(X). Assume C : X → R ≥o is
sublinear and continuous, and f ∈ L ι(X, μ) is upper semicontinuous. Thenfor all r ≥ 0,
DRR := sup	risk`(f, ν) ≤ r lipc('f) + risk`(f, μ).	(3)
ν∈Bc(μ,r)
The tightness of the bound can be quantified as follows. Let ∆(μ) := r lipc('f) + risk`(f, μ)一
suPν∈Bc(μ,r) risk'(f, V). If lipc(f) < ∞ then
△ (〃)≤ r(lipc('f) - hlipc(Co'f) - r/('f 一Co'f)dμ]J,	(4)
where [ ∙]+:=max{ ∙ , 0} and 1/0 := ∞, so that when 'f is closed convex there is equality in (3).
Clearly (4) is tight for convex 'f . Furthermore, Proposition 1 shows that (4) is also tight for a
large family of nonconvex functions and distributions — particularly the upper-semicontinuous loss
functions on a compact set X0 ⊆ X, with the collection of probability distributions supported on X0 .
Proposition 1. Assume X is a separable Frechet space with Xo ⊆ X. Assume C : X → R≥o is
sublinear and continuous, and'f ∈ ∩μ∈p(χ0)Lι(X,μ) is uppersemicontinuous, has lipc('f) < ∞,
and attains its maximum on X0. Then for all r ≥ 0 with 1/0 := ∞,
sup ∆(μ)
μ∈P(Xo)
TQipc('f) - hlipc(Co'f) 一 r ρ('f )i+).
Theorem 1 subsumes many existing results (viz. Gao & Kleywegt, 2016, Cor. 2 (iv), Cisse et al.,
2017, §3.2, Sinha et al., 2018, Shafieezadeh-Abadeh et al., 2017, Thm. 3.20) with a great deal more
generality, applying to a very broad family of models, loss functions, and outcome spaces. It is the
first time to our knowledge that the slackness in (3) has been characterised tightly.
The extension of Theorem 1 for robust classification in the absence of label noise is straight-forward.
Corollary 1. Assume X is a separable Frechet space and Y is a topological space. Fix μ ∈
P(X × Y). Assume C : (X × Y) × (X × Y) → R satisfies c(x, y, x0, y0) = CX (X — x0) whenever
3
Under review as a conference paper at ICLR 2020
y = y0 and c(x, y, x0, y0) = ∞ whenever y = y0, where CX : X → R is symmetric, sublinear, and
continuous, and f ∈ L ι(X X Y,μ) is upper semicontinuous. Thenfor all r ≥ 0 there is (3).
To see the tightness of the bound, if lipc (`f) < ∞ there is (4), where the closed convex hull is
interpreted as co('f )(x,y) := co('f ( ∙ ,y))(x). If additionally 'f( ∙ ,y) is closed convex for all
y ∈ Y , there is equality in (3).
3.1 Distributional robustness as adversarial robustness
We next show how Theorem 1 can be useful for adversarial learning. The following objective function
has been proposed to build a robust classifier. Let X and Y be topological spaces, fix μ ∈ P(X × Y)
and let d be a metric on X. The following objective has been proposed (viz Goodfellow et al., 2015;
Madry et al., 2018; Shaham et al., 2018) as a means of learning models that are robust to adversarial
perturbations
adversarial risk :
/sup 'f (X,y)μ(dx × dy)
X∈Bd(x,r)
∕sup 'f (ω)μ(dω),
ω ∈Bj(ω,r)
(5)
where in the equality We extend d to a metric on Ω := X × Y with
00	0	0
d((x,y), (x0, y0)) := d(x, x0) + ∞ Jy 6= y0K .
We refer to (5) as the adversarial risk.
Theorem 2. Assume (X,c) is a separable Banach space. Fix μ ∈ P(X) and let Rμ(r)
{g ∈ Lo(X, R≥o) | R g dμ ≤ r}. Thenfor f ∈ Lo(Ω, R), r > 0 there is
∕μ(dω) sup	'f (ω0) ≤ sup risk`(f, ν)
ω0∈Bc(ω,g(ω))	ν∈Bc(μ,r)
variable-radius risk := sup
g∈Rμ(r)
DRR. (6)
The equality holds in (6) if μ is non-atomically concentrated on a compact subset of X, on which f is
continuous with the subspace topology.
We refer to the left-hand side (LHS) of (6) as the variable-radius risk. The variable-radius risk has
appeared in various forms in similar results, usually formulated using empirical distributions, that
is, an average of Dirac masses, (viz. Gao & Kleywegt, 2016; Shafieezadeh-Abadeh et al., 2017).
Of course any finite set is compact, and so any empirical distribution satisfies the concentration
assumption. Likewise the subspace topology on a finite set is the discrete topology, which makes the
continuity assumption trivial.
Both the adversarial risk and the variable-radius risk imply an uncertainty set over a collection of
adversaries that may perturb the data. Figure 5 in the appendix (on p. 20) shows the practical
difference between the kinds of adversaries in these uncertainty sets. Immediately there is a corollary
similar to Corollary 1 for Theorem 2.
It is easy to see that the variable-radius risk upper bounds the adversarial risk (5) by observing that
the constant function gr ≡ r is included in the supremum over Rμ(r) in (6). As a result,
(a)	(b)
adversarial risk ≤ variable-radius risk ≤ DRR ≤ Lipschitz regularised risk (RHS of (3)), (7)
where (a) is by Theorem 2 and (b) is by Theorem 1.
In general, it is difficult to characterise the tightness of the upper bounds in Theorem 1 and 2. So
we resorted to an empirical demonstration that the sum of all the three gaps in (7) is relatively low.
We randomly generated 100 Gaussian kernel classifiers f = P1==1 Yik(xi, ∙), with Xi sampled from
the MNIST dataset and γi sampled uniformly from [-2, 2]. The bandwidth was set to the median of
pairwise distances. In Figure 1, the x-axis is the adversarial risk in (5) where the perturbation δ is
bounded in 'p ball and computed by PGD. The y-axis is the Lipschitz regularised empirical risk. The
scattered dots lie closely to the diagonal, demonstrating that the above bounds are tight in practice.
4	Provab le Lipschitz Regularisation for Kernel Methods
Theorems 1 and 2 open up a new path to optimising the adversarial risk (5) by Lipschitz regularisation
(RHS of (4)), where the upper bounding relationship is established through DRR. In general, however,
it is still hard to compute the Lipschitz constant for a nonlinear model. However, we will show that
4
Under review as a conference paper at ICLR 2020
O 5
(m) b 出 JoSmI
5	10
20
10
w)bHJOSHH
0
0	10	20
Adversarial risk in Eq (5)
(b) kδk∞ ≤ 0.3
与80
I 60
小
/40
«20
≡0
0	20	40	60	80
Amax (GrG)I∙" - HPX ⑺
(a) 5-layer inverse kernel
Adversarial risk in Eq (5)
(a) kδk2 ≤3
Figure 1: Empirical evaluation of the sum of
the gaps from Theorems 1 and 2. The Lipschitz
constants supχ∈χ ∣∣Vf (X)Ilq (left: P = 2, right:
p= ∞, 1/p+ 1/q= 1) were estimated by BFGS.
(b) Gaussian kernel (σ=3)
Figure 2: Comparison of λmax(G>G) and the
RHS of (8), as upper bounds for the Lipschitz con-
stant. Smaller values are tighter. 100 functions
sampled in the same way as in Figure 1.
0
0
for some types of kernels, this can be done efficiently on functions in its RKHS. Thanks to the known
connections between kernel method and deep learning, this technique will also potentially benefit
the latter. For example, `1 -regularised neural networks are compactly contained in the RKHS of
multi-layer inverse kernels k(x, y) = (2 - x>y)-1 with ∣x∣2 ≤ 1 and ∣y∣2 ≤ 1 (Zhang et al., 2016,
Lemma 1 and Theorem 1) and (Shalev-Shwartz et al., 2011; Zhang et al., 2017), and even possibly
Gaussian kernels k(x, y) = exp(- ∣x - y∣2 /(2σ2)) (Shalev-Shwartz et al., 2011, §5).
Let us consider a Mercer’s kernel k on a convex domain X ⊆ Rd, with the corresponding RKHS
denoted as H. The standard kernel method seeks a discriminant function f from H with the
conventional form of finite kernel expansion f (x) = : P；=i Yak(χa, ∙), such that the regularised
empirical risk can be minimised with the standard (hinge) loss and RKHS norm. We start with
real-valued f for univariate output such as binary classification, and later extend it to multiclass.
Our goal here is to additionally enforce, while retaining a convex optimisation in γ := {γa },
that the Lipschitz constant of f falls below a prescribed threshold L > 0, which is equivalent to
supx∈X ∣Vf (x)∣2 ≤ L thanks to the convexity of X. A quick but primitive solution is to piggyback
on the standard RKHS norm constraint ∣f ∣H ≤ C , in view that it already induces an upper bound on
∣Vf (x)∣2 as shown in Example 3.23 of Shafieezadeh-Abadeh et al. (2017),
SUp l∣Vf(x)∣2 ≤ kf∣HSUpZTg(z), where g(z) ≥ SUp	∣k(x, ∙) - k(x0, ∙)∣h .(8)
x∈X	z>0	x,x0∈X: ||x—x0 ∣∣2=z
For Gaussian kernels, g(z) = max{σ-1, 1}z. For exponential and inverse kernels, g(z) = z (Bietti
& Mairal, 2019). Bietti et al. (2019) justified that the RKHS norm of a neural network may serve as a
surrogate for Lipschitz regularisation. But the quality of such an approximation, i.e., the gap in (8),
can be loose as we will see later in Figure 2. Besides, C and L are independent parameters.
How can we tighten the approximation? A natural idea is to directly bound the gradient norm at n
random locations {ws}sn=1 sampled i.i.d. from X. These are obviously convex constraints on γ. But
how many samples are needed in order to ensure ∣Vf (x)∣2 ≤ L + ε for all x ∈ X? Unfortunately,
as shown in Appendix A.1, n may have to grow exponentially by 1∕εd for a d-dimensional space.
Therefore we seek a more efficient approach by first slightly relaxing ∣Vf (x)∣2. Letgj(x) := ∂jf(x)
be the partial derivative with respect to the j-th coordinate of x, and ∂i,j k(x, y) be the partial
derivative to xi and yj . i or j being 0 means no derivative. Assuming SUpx∈X k(x, x) = 1 and
gj ∈ H (true for various kernels considered by Assumptions 1 and 2 below), we get a new bound
SUp ∣Vf(x)∣22
x∈X
SUp χd 1 hgj, k(x, ∙)iH ≤ sup χd 1 hgj, ΨiH = λmax(G>G), (9)
正Xj = =	H PkHH = I j =1
where λmax evaluates the maximum eigenvalue, and G := (g1, . . . , gd). The “matrix” is only
a notation because each column is a function in H, and obviously the (i, j)-th entry of G>G is
hgi, gjiH. Interestingly, λmax(G>G) delivers significantly lower (i.e., tighter) value in approximating
the Lipschitz constant supχ∈χ ∣∣Vf (x)∣∣2, compared with IIfkHmaxz>o g(z) from (8). Figure 2
compared these two approximants, where λmax(G>G) was computed from (11) derived below, and
the landmarks {ws } consisted of all training examples; drawing more samples led to little difference.
Such a positive result motivated us to develop refined algorithms to address the only remaining
obstacle to leveraging λmax(G>G): no analytic form for computation. Interestingly, it is readily
approximable in both theory and practice. Indeed, the role of gj can be approximated by gj, where
gj ∈ Rn is the Nystrom approximation (Drineas & Mahoney, 2005; Williams & Seeger, 2000):
5
Under review as a conference paper at ICLR 2020
gj := K-1/2(gj(w1),...,gj(wn))>=(Z>Z)T/2Z>gj (noting gj(w1) = (gj,k(w1, .))^(10)
where K ：= [k(wi,wi j]i,i，, Z := (k(w1, ∙),k(w2, ∙),...,k(wn, ∙)),	GG := (gι,...,gd).
So to ensure λmaχ(G>G) ≤ L2 + ε, intuitively we can resort to enforcing λmaχ(G>G) ≤ L2, which
also retains the convexity in the constraint in γ . However, to guarantee ε error, the number of samples
(n) required is generally exponential (Barron, 1994). Fortunately, we will next show that n can be
reduced to polynomial for quite a general class of kernels that possess some decomposed structure.
4.1	A COORDINATE-WISE NYSTROM APPROXIMATION FOR PRODUCT KERNELS
A number of kernels factor multiplicatively over the coordinates, such as periodic kernels (MacKay,
1998), Gaussian kernels, and Laplacian kernels. We will consider k(x, y) = Qjd=1 k0 (xj, yj ) where
X = Xd and k0 is a base kernel on X0. Let the RKHS of k0 be H0, and let μ0 be a finite Borel
measure with supp[μ0] = X0. Periodic kernels have k0(xj ,yj) = exp (- sin( V (Xj 一 yj ))2∕(2σ2)).
The key benefit of this decomposition is that the derivative ∂0,1k(x, y) can be written as
∂0,1k0(x1, y1) Qjd=2 k0(xj, yj). Since k0(xj, yj) can be easily dealt with, approximation will be
needed only for ∂0,1k0(x1,y1). Applying this idea to g1 = : P；=1 γa∂0,1k(xa, ∙), we can derive
kg1kH = ι-2 Xab=JaY <∂0,1k0(xa, ∙),∂0,1k0(χi, ∙))h0 γd=2 k0(χj,Xb),	(11)
hg1, g2iH = l-2 Xla,b=1γaγb∂0,1k0(x1a,xb1)∂0,1k0(xb2,x2a) Yjd= k0 (Xja, Xjb).
So the off-diagonal entries ofG>G can be computed exactly. To approximate the diagonal, we sample
{w1,..., wn} from μ0, set Z1 = (k0(w1, •),..., ko(w7, ∙)), and apply Nystrom approximation:
(∂0,1 k0(xa, ∙),∂0,1k0(x1, ∙))h ≈ ∂0,1 k0(xa, ∙)>Z1 ∙ (Z>Z1)-1 ∙ Z>∂0,1k0(x1, ∙)	(12)
0
where Z>∂0,1 k0(xoα, ∙) = (∂0,1k0(xa, w1),..., ∂0,1 k0(xoα, w?))>,	(13)
and analogously for Z>∂0,1k0(χi, ∙). We will denote this approximation of G>G as Pg. Clearly,
λmaχ(PG) ≤ L2 is a convex constraint on γ, based on i.i.d. samples {wj : S ∈ [n],j ∈ [d]} from μ0.
It is now important to analyse how many samples wjs are needed, such that
λmaχ(pG) ≤ L2	=⇒	λmaχ(G>G) ≤ L2 + ε With high probability.
4.2	General sample complexity and assumptions on the product kernel
Fortunately, product kernels only require approximation bounds for each coordinate, making the
sample complexity immune to the exponential growth in the dimensionality d. Specifically, we first
consider base kernels k0 with a scalar input, i.e., X0 ⊆ R. Recall from Steinwart & Christmann
(2008, Chapter 4) that the integral operator for k0 and μ0 is defined by
Tko = I ◦ S : L2(X0,μ0) → L2(X0,μ0)
where S : L2(X0,μ0) →C(X0),
(Sf)(X)
/ k0(χ,y)f (y)dμ0(y),
f ∈ L2(X0,μ0),
and I: C(X0) → L2(X0; μ0) is the inclusion operator. By the spectral theorem, if Tk° is compact,
then there is an at most countable orthonormal set {ej-}j-∈j of L2(X0, μ0) and {λj- }j-∈j with λ1 ≥
λ2 ≥ ... > 0 such that Tk°f = pj∙∈j λj- hf, ej)工2(天0；*0)ej for all f ∈ L2(X0, μ0). It is easy to
see that 夕j := √zλjej∙ is an orthonormal basis of H0 (Steinwart & Christmann, 2008).
Our proof is built upon the following two assumptions on the base kernel. The first one asserts that
fixing x, the energy of k0(χ, ∙) and ∂0,1k0(x, ∙) “concentrates” on the leading eigenfunctions.
Assumption 1. Suppose k°(x,x) = 1 and ∂0,1k0(x, ∙) ∈ H0 for all X ∈ X0. For all ε > 0, there
exists Nε ∈ N Such that the tail energy of ∂0,1k0(x, ∙) beyond the N-th eigenpair is less than ε,
UnifOrmlyfOr all X ∈ X。. That is, denoting Φm :=(夕 1,...,夕m),
Nε := inf { ∣∣∂0,1k0(x, ∙) — ΦmΦm∂0,1k0(x, ∙)∣∣Ho < ε forall X ∈ X0 and
∣∣k0(x, ∙) — ΦmΦmk0(X, ∙)∣∣h < ε forall X ∈ X0} < ∞.
6
Under review as a conference paper at ICLR 2020
The second assumption asserts the smoothness and range of eigenfunctions in a uniform sense.
Assumption 2. Under Assumption 1, {ej (x) : j ∈ Nε} is uniformed bounded over x ∈ X0, and the
RKHS inner product of ∂0,1 k0(x, ∙) with {ej : j ∈ N } is also uniformly bounded over X ∈ Xo:
Mε := sup max I^∂0,1ko(x, ∙), ejYy I < ∞,	and	Qε := SuP max |ej(x)| < ∞.
x∈X0 j∈[Nε]	H0	x∈X0 j∈[Nε]
Theorem 3. Suppose ko, Xo, and μo satisfy Assumptions 1 and 2. Let {ws : S ∈ [n],j ∈ [d]} be
sampled i.i.d. from μo. Thenfor any f whose coordinate-wise Nystrom approximation (11) and (12)
satisfy λmaχ(PG) ≤ L2, the Lipschitz condition λmaχ(G>G) ≤ L2 + ε is met with probability 1 一 δ,
as long as n ≥ Θ (表 N2M2Q2 log dNNε), almost independent of d. Here Θ hides all poly-log terms.
Satisfaction of Assumptions. In Appendix A.4 and A.5, we will show that for periodic kernel and
Gaussian kernel, Assumptions 1 and 2 hold true with。⑴ values of %, M*, and Qε. It remains open
whether non-product kernels such as inverse kernel also enjoy this polynomial sample complexity.
Appendix A.6 suggests that the complexity is quasi-polynomial for inverse kernels.
5	Experimental Results
We studied the empirical robustness and accuracy of the proposed Lipschitz regularisation technique
for adversarial training of kernel methods, under both Gaussian kernel and inverse kernel. Comparison
will be made with state-of-the-art defense algorithms under effective attacks.
Datasets. We tested on three datasets: MNIST, Fashion-MNIST, and CIFAR10. The number of
training/validation/test examples for the three datasets are 54k/6k/10k, 54k/6k/10k, 45k/5k/10k,
respectively. Each image in MNIST and Fashion-MNIST is represented as a 784-dimensional feature
vector, with each feature/pixel normalised to [0, 1]. For CIFAR10, we trained it on a residual network
to obtain a 512-dimensional feature embedding, which were subsequently normalised to [0, 1]. They
were used as the input for training all the competing algorithms and were subject to attack.
Attacks. To evaluate the robustness of the trained model, we attacked them on test examples using
the random initialized Projected Gradient Descent method with 100 steps (PGD, Madry et al., 2018)
under two losses: cross-entropy and C&W loss (Carlini & Wagner, 2017). The perturbation δ was
constrained in an '2 or '∞ ball. To evaluate robustness, we scaled the perturbation bound δ from 0.1
to 0.6 for '∞ norm, and from 1 to 6 for '2 norm (when δ = 6, the average magnitude per coordinate
is 0.214).
Algorithms. We compared four training algorithms. The Parseval network orthonormalises the
weight matrices to enforce the Lipschitz constant (Cisse et al., 2017). We used three hidden layers
of 1024 units and ReLU activation (Par-ReLU). Also considered is the Parseval network with
MaxMin activations (Par-MaxMin), which enjoys much improved robustness (Anil et al., 2019).
Both algorithms can be customised for '2 or '∞ attacks, and were trained under the corresponding
norms. Using multi-class hinge loss, they constitute strong baselines for adversarial learning.
Both Gaussian and inverse kernel machines applied Lipschitz regularisation by randomly and greedily
selecting {ws}, and they will be referred to as Gauss-Lip and Inverse-Lip, respectively. In practice,
Gauss-LiP with the coordinate-wise Nystrom approximation (λmaχ(Pg) from Eq (12)) can approxi-
mate λmax(G>G) with a much smaller number of sample than if using the holistic approximation
as in (10). Furthermore, we found an even more efficient approach. Inside the iterative training
algorithm, we used L-BFGS to find the input that yields the steepest gradient under the current
solution, and then added it to the set {ws} (which was initialized with 15 random points). Although
L-BFGS is only a local solver, this greedy approach empirically reduces the number of samples by an
order of magnitude. See the empirical convergence results in Appendix A.9. Its theoretical analysis
is left for future investigation. We also applied this greedy approach to Inverse-LiP.
Extending binary kernel machines to multiclass. The standard kernel methods learn a discrim-
inant function fC := Pa Yak(χa, ∙) for each class C ∈ [10], based on which a large supply of
multiclass classification losses can be applied, e.g., CS (Crammer & Singer, 2001) which was used
in our experiment. Since the Lipschitz constant of the mapping from {fc} to a real-valued loss is
typically at most 1, it suffices to bound the Lipschitz constant of x 7→ (f1 (x), . . . , f1o(x))> by
7
Under review as a conference paper at ICLR 2020
8°0°4°0°
() Aoalnooe31
O Gauss-Lip (PGD-Cw)
P Par-MaxMin (PGD-Cw)
A InverSe-LiP (PGD-Cw)
T-Par-ReLU (PGD-Cw)
O Gauss-Lip (PGD-Cw)
* Par-MaXMin (PGD-Cw)
A InverSe-LiP (PGD-Cw)
T-Par-ReLU (PGD-Cw)
8°0°4°0°
(<⅛) Aoalnooe31
O Gauss-Lip (PGD-Cw)
Par-MaxMin (PGD-cw)
-A-InverSe-LiP (PGD-cw)
-M- Par-ReLU (PGD-Cw)
1	2	3	4	5	6
£2 ball radius δ
(c) CIFAR10
f，2 ball radius δ
£2 ball radius δ
⑶ MNIST
(b) Fashion-MNIST
(<⅛) Aoalnooes31
1	2	3	4	5
Figure 3:	Test accuracy under PGD attacks on the C&W approximation with '2 norm bound
Qooo
8 6 4 2
() Aoalnooe31
O Gauss-Lip (PGD-Cw)
* Par-MaXMin (PGD-Cw)
A Inverse-UP (PGD-Cw)
T-Par-ReLU (PGD-Cw)
(*) Aoalnooes31
O Gauss-Lip (PGD-Cw)
-Ht-Par-MaxMin (PGD-Cw)
A InverSe-LiP (PGD-Cw)
-M-Par-ReLU (PGD-Cw)
8°0°4°0°
() Aoalnooe31
-©—Gauss-Lip (PGD-Cw)
* Par-MaXMin (PGD-cw)
-A-InverSe-LiP (PGD-cw)
-W-Par-ReLU (PGD-Cw)
0.1	0.2	0.3	0.4	0.5	0.6
2g ball radius ʌ
0.1	0.2	0.3	0.4	0.5	0.6
43o ball radius S
0.1	0.2	0.3	0.4	0.5	0.6
Q> ball radius 6
(a) MNIST	(b) FaShion-MNIST	(C) CIFAR10
Figure 4:	Test accuracy under PGD attacks on the C&W approximation with '∞ norm bound
maxλmaχ(G(x)G(x)>) ≤ max∣3∣H=1 λmax(XC=1 GGG) ≤ L2,	(14)
where G(X) = [Vf1(x),…，Vf10(x)] = [G>k(x, ∙),...,G>k(x, ∙)] with Gc := (gC,...,gd).
The last term in (14) can be approximated using the same technique as in the binary case. Furthermore,
the principle can be extended to '∞ attacks, whose details are relegated to Appendix A.10.
Parameter selection. We used the same parameters as in Anil et al. (2019) for training Par-ReLU
and Par-MaxMin. To defend against `2 attacks, we set L = 100 for all algorithms. Gauss-
Lip achieved high accuracy and robustness on the validation set with bandwidth σ = 1.5 for
FashionMNIST and CIFaR-10, and σ = 2 for MNIST. To defend against '∞ attacks, we set
L = 1000 for all the four methods as in Anil et al. (2019). The best σ for Gauss-Lip is 1 for all
datasets. Inverse-Lip used 5 stacked layers.
Results. Figures 3 and 4 show how the test accuracy decays as an increasing amount of perturbation
(δ) in '2 and '∞ norm is added to the test images, respectively. Clearly GauSS-LiP achieves higher
accuracy and robustness than Par-ReLU and Par-MaxMin on the three datasets, under both `2
and '∞ bounded PGD attacks with C&W loss. In contrast, Inverse-Lip only performs similarly
to Par-ReLU. Interestingly, we noticed that `2 based Par-MaxMin are only slightly better than
Par-ReLU under '2 attacks, although the former does perform significantly better under '∞ attacks.
For the sake of space, the results for cross-entropy PGD attacks are deferred to Figures 8 and 9 in
Appendix A.11. Here cross-entropy PGD attackers find stronger attacks to Parseval networks but not
to our kernel models. Our GauSS-Lip again significantly outperforms Par-MaxMin on all the three
datasets and under both '2 and '∞ norms. The improved robustness of GauSS-Lip does not seem to
be attributed to the obfuscated gradient (Athalye et al., 2018), because as shown Figures 3, 4, 8, 9,
increased distortion bound does increase attack success, and unbounded attacks drive the success
rate to very low. In practice, we also observed that random sampling finds much weaker attacks, and
taking 10 steps of PGD is much stronger than just one step.
Visualization. The gradient with respect to inputs is plotted in Figure 10 (in the appendix on p. 31)
for '2 trained Par-MaxMin and GauSS-Lip. The i-th row and j-th column corresponds to the targeted
attack of turning the original class j into a new class i, hence the gradient is on the cross-entropy loss
with class i as the ground truth. These two figures also explained why GauSS-Lip is more robust than
Par-MaxMin: the attacker can easily reduce the targeted cross-entropy loss by following the gradient
as shown in Figure 10a, and hence successfully attack Par-MaxMin. In contrast, the gradient shown
in Figure 10b does not provide much information on how to flip the class.
Conclusion. In this paper, we derived a new certificate for distributional robust risk minimization by
using Lipschitz regularization. Application to adversarial learning based on kernel methods exhibited
superior robustness, with provably polynomial sample complexity for product kernels. We will apply
this function space to GANs to witness the difference between probability distributions, leading to a
more stable training scheme as the inner level optimization becomes convex.
8
Under review as a conference paper at ICLR 2020
References
Cem Anil, James Lucas, and Roger Grosse. Sorting out Lipschitz function approximation. In
International Conference on Machine Learning (ICML), 2019.
Armin Askari, Alexandre d’ Aspremont, and Laurent El Ghaoui. Naive feature selection: Sparsity in
naive bayes. 2019. URL http://arxiv.org/abs/1905.09884.
Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of
security: Circumventing defenses to adversarial examples. In International Conference on Machine
Learning (ICML), 2018.
Jean-Pierre Aubin and Ivar Ekeland. Estimates of the duality gap in nonconvex optimization.
MathematicsofOperations Research, 1(3):225-245, 1976. ISSN 0364765X, 15265471. doi:
10/d6r3xf.
A. R. Barron. Approximation and estimation bounds for artificial neural networks. Machine Learning,
14:115-133, 1994.
Aharon Ben-Tal, Dick den Hertog, Anja De Waegenaere, Bertrand Melenberg, and Gijs Rennen.
Robust solutions of optimization problems affected by uncertain probabilities. Management
Science, 59(2):341-357, 2013.
C. Bhattacharyya, K. S. Pannagadatta, and A. J. Smola. A second order cone programming formula-
tion for classifying missing data. In Advances in Neural Information Processing Systems (NIPS),
pp. 153-160, 2005.
Alberto Bietti and Julien Mairal. Group invariance, stability to deformations, and complexity of deep
convolutional representations. Journal of Machine Learning Research, 20(25):1-49, 2019.
Alberto Bietti, Gregoire Mialon, Dexiong Chen, and Julien Mairal. A kernel perspective for regular-
izing deep neural networks. In International Conference on Machine Learning (ICML), 2019.
Jose Blanchet and Karthyek Murthy. Quantifying distributional model risk via optimal transport.
Mathematics of Operations Research, 2019. doi: 10.1287/moor.2018.0936.
Jose Blanchet, Yang Kang, and Karthyek Murthy. Robust Wasserstein profile inference and applica-
tions to machine learning. arXiv:1610.05627, 2016.
Jose Blanchet, Yang Kang, Fan Zhang, and Karthyek Murthy. Data-driven optimal transport cost
selection for distributionally robust optimization. arXiv:1705.07152, 2017.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017
IEEE Symposium on Security and Privacy (SP), pp. 39-57. IEEE, 2017.
Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier. Parseval
networks: Improving robustness to adversarial examples. In International Conference on Machine
Learning (ICML), 2017.
Koby Crammer and Yoram Singer. On the algorithmic implementation of multiclass kernel-based
vector machines. Journal of machine learning research, 2(Dec):265-292, 2001.
Zac Cranko, Aditya Menon, Richard Nock, Cheng Soon Ong, Zhan Shi, and Christian Walder.
Monge blunts Bayes: Hardness results for adversarial training. In Kamalika Chaudhuri and Ruslan
Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning,
volume 97 of Proceedings of Machine Learning Research, pp. 1406-1415. PMLR, 2019. URL
http://proceedings.mlr.press/v97/cranko19a.html.
Erick Delage and Yinyu Ye. Distributionally robust optimization under moment uncertainty with
application to data-driven problems. Operations Research, 58(3):595-612, 2010.
P. Drineas and M. Mahoney. On the nystr om method for approximating a gram matrix for improved
kernel-based learning. JMLR, 6:2153-2175, 2005.
9
Under review as a conference paper at ICLR 2020
John Duchi, Peter Glynn, and Hongseok Namkoong. Statistics of robust optimization: A generalized
empirical likelihood approach. arXiv:1610.03425, 2016.
Gregory E Fasshauer. Positive definite kernels: Past, present and future. Dolomite Res. Notes Approx.,
4, 01 2011.
Laurent El Ghaoui and Herve Lebret. Robust solutions to least-squares problems with uncertain data.
SIAMJ. MatrixAnal. Appl.,18(4):1035-1064,1997.
Peyman Mohajerin Esfahani and Daniel Kuhn. Data-driven distributionally robust optimization
using the Wasserstein metric: Performance guarantees and tractable reformulations. Mathematical
Programming, 171(1-2):115-166, 2018.
Farzan Farnia and David Tse. A minimax approach to supervised learning. In Advances in Neural
Information Processing Systems (NIPS), 2016.
Farzan Farnia, Jesse Zhang, and David Tse. Generalizable adversarial training via spectral normaliza-
tion. In International Conference on Learning Representations (ICLR), 2019.
G.	Fasshauer and M. McCourt. Stable evaluation of Gaussian radial basis function interpolants. SIAM
Journal on Scientific Computing, 34(2):A737-A762, 2012.
Rui Gao and Anton J Kleywegt. Distributionally robust stochastic optimization with Wasserstein
distance. arXiv:1604.02199, 2016.
Emmanuel Giner. Necessary and sufficient conditions for the interchange between infimum and the
symbol of integration. Set-Valued and Variational Analysis, 17(4):321, 2009.
Joel Goh and Melvyn Sim. Distributionally robust optimization and its tractable approximations.
Operations Research, 58(4):902-917, 2010.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv:1412.6572, 2015.
Henry Gouk, Eibe Frank, Bernhard Pfahringer, and Michael Cree. Regularisation of neural networks
by enforcing Lipschitz continuity. arXiv:1804.04368, 2018.
J.-B. Hiriart-Urruty. A general formula on the conjugate of the difference of functions. Canadian
Mathematical Bulletin, 29(4):482-485, 1986. ISSN 0008-4395, 1496-4287. doi: 10/cc3fs2.
Jean-Baptiste Hiriart-Urruty. From convex optimization to nonconvex optimization. necessary and
sufficient conditions for global optimality. In Francis H. Clarke, V. F. Dem’yanov, and F. Giannessi
(eds.), Nonsmooth Optimization and Related Topics, pp. 219-239. Springer US, 1989. ISBN 978-
1-4757-6019-4. doi: 10.1007/978-1-4757-6019-4_13. URL https://doi.org/10.1007/
978-1-4757-6019-4_13.
Jean-Baptiste Hiriart-Urruty and Claude Lemarechal. Convex Analysis and Minimization Algorithms
II. Springer-Verlag, 2010. ISBN 978-3-642-08162-0. OCLC: 864385173.
Zhaolin Hu and Jeff Liu Hong. Kullback-Leibler divergence constrained distributionally robust
optimization, 2016. Available from Optimization Online.
Todd Huster, Cho-Yu Jason Chiang, and Ritu Chadha. Limitations of the Lipschitz constant as a
defense against adversarial examples. arXiv:1807.09705, 2018.
L. Kantorovitch. On the translocation of masses. Management Science, 5(1):1-4, 1958.
Thomas Kerdreux, Igor Colin, and Alexandre d’ Aspremont. An approximate Shapley-Folkman
theorem. 2019. URL http://arxiv.org/abs/1712.08559.
H.	Konig. Eigenvalue Distribution ofCompact OPerators. Birkhauser, Basel, 1986.
John Lafferty and Guy Lebanon. Diffusion kernels on statistical manifolds. Journal of Machine
Learning Research, 6:129-163, 01 2005.
10
Under review as a conference paper at ICLR 2020
C.	Lemarechal and A. Renaud. A geometric study of duality gaps, with applications. Mathematical
Programming, 90(3):399-427, 2001. ISSN 0025-5610. doi:10/d7sn64.
Shao-Bo Lin, Xin Guo, and Ding-Xuan Zhou. Distributed learning with regularized least squares.
Journal of Machine Learning Research, 18(92):1-31, 2017.
D.	J. C. MacKay. Introduction to Gaussian processes. In C. M. Bishop (ed.), Neural Networks and
Machine Learning, pp. 133-165. Springer, Berlin, 1998.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
Learning Representations (ICLR), 2018.
E.	J. McShane. Extension of range of functions. Bull. Amer. Math. Soc., 40(12):837-842, 1934.
Charles A. Micchelli, Yuesheng Xu, and Haizhang Zhang. Universal kernels. Journal of Machine
Learning Research, 7:2651-2667, 2006.
Ha Quang Minh. Some properties of Gaussian reproducing kernel Hilbert spaces and their implica-
tions for function approximation and learning theory. Constructive Approximation, 32(2):307-338,
2010.
Ha Quang Minh, Partha Niyogi, and Yuan Yao. Mercer’s theorem, feature maps, and smoothing.
In Gdbor Lugosi and Hans Ulrich Simon (eds.), Conference on Computational Learning Theory
(COLT), pp. 154-168, 2006.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for
generative adversarial networks. In International Conference on Learning Representations (ICLR),
2018.
Openreview. OpenReview comments on “Certifying some distributional robustness with princi-
pled adversarial training" by Aman Sinha, Hongseok Namkoong, and John Duchi. https:
//openreview.net/forum?id=Hk6kPgZA-, 2018.
Jean-Paul Penot. On the minimization of difference functions. Journal of Global Optimization, 12
(4):373-382, 1998. ISSN 09255001. doi: 10/c59j7n.
Aldo Pratelli. On the equality between Monge’s infimum and Kantorovich’s minimum in optimal
mass transportation. Annales de l’Institut Henri Poincare (B) Probability and Statistics, 43(1):
1-13, 2007.
Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certified defenses against adversarial
examples. In International Conference on Learning Representations (ICLR), 2018.
C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. MIT Press,
Cambridge, MA, 2006.
Kevin Scaman and Aladin Virmaux. Lipschitz regularity of deep neural networks: analysis and
efficient estimation. In Advances in Neural Information Processing Systems (NIPS), 2018.
Hans Schneider. An inequality for latent roots applied to determinants with dominant principal
diagonal. Journal of the London Mathematical Society, s1-28(1):8-20, 1953.
Soroosh Shafieezadeh-Abadeh, Peyman Mohajerin Mohajerin Esfahani, and Daniel Kuhn. Distribu-
tionally robust logistic regression. In Advances in Neural Information Processing Systems (NIPS),
2015.
Soroosh Shafieezadeh-Abadeh, Daniel Kuhn, and Peyman Mohajerin Esfahani. Regularization via
mass transportation. arXiv:1710.10016, 2017.
Uri Shaham, Yutaro Yamada, and Sahand Negahban. Understanding adversarial training: Increasing
local stability of supervised models through robust optimization. Neurocomputing, 307:195-204,
2018.
11
Under review as a conference paper at ICLR 2020
S. Shalev-Shwartz, O. Shamir, and K. Sridharan. Learning kernel-based halfspaces with the 0-1 loss.
SIAMJournal on Computing, 40(6):1623-1646, 2011.
Z.-C. Shi and B.-Y. Wang. Bounds for the determinant, characteristic roots and condition number of
certain types of matrices. Acta Math. Sinica, 15(3):326-341, 1965.
Aman Sinha, Hongseok Namkoong, and John Duchi. Certifying some distributional robustness with
principled adversarial training. In International Conference on Learning Representations (ICLR),
2018.
Ingo Steinwart and Andreas Christmann. Support vector machines. Springer Science & Business
Media, 2008.
Arun Sai Suggala, Adarsh Prasad, Vaishnavh Nagarajan, and Pradeep Ravikumar. Revisiting ad-
versarial risk. In International Conference on Artificial Intelligence and Statistics (AISTATS),
2019.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. In International Conference on Learning
Representations (ICLR), 2014.
John F. Toland. A duality principle for non-convex optimisation and the calculus of variations.
Archive for Rational Mechanics and Analysis, 71(1):41-61, 1979. ISSN 0003-9527, 1432-0673.
doi: 10/cprjrs.
Joel A. Tropp. An introduction to matrix concentration inequalities. Foundations and Trends in
Machine Learning, 8(1-2):1-230, 2015.
Yusuke Tsuzuku, Issei Sato, and Masashi Sugiyama. Lipschitz-margin training: Scalable certification
of perturbation invariance for deep neural networks. In Advances in Neural Information Processing
Systems (NIPS), 2018.
Madeleine Udell and Stephen Boyd. Bounding duality gap for separable problems with linear
constraints. Computational Optimization and Applications, 64(2):355-378, 2016. ISSN 1573-
2894. doi: 10/f8pmnq.
Cedric Villani. OPtimal transport: old and new, volume 338. Springer Science & Business Media,
2008.
Tsui-Wei Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Luca Daniel, Duane S.
Boning, and Inderjit S. Dhillon. Towards fast computation of certified robustness for relu networks.
In International Conference on Machine Learning (ICML), 2018.
Hassler Whitney. Analytic extensions of differentiable functions defined in closed sets. Transactions
of the American Mathematical Society, 36(1):63-89, 1934.
Wolfram Wiesemann, Daniel Kuhn, and Melvyn Sim. Distributionally robust convex optimization.
Operations Research, 62(6):1358-1376, 2014.
C. K. I. Williams and M. Seeger. Using the NystrOm method to speed UP kernel machines. In
Advances in Neural Information Processing Systems (NIPS), 2000.
R. C. Williamson, A. J. Smola, and B. SchOlkopf. Generalization bounds for regularization networks
and support vector machines via entropy numbers of compact operators. IEEE Transactions on
Information Theory, 47(6):2516-2532, 2001.
Eric Wong and J Zico Kolter. Provable defenses against adversarial examples via the convex outer
adversarial polytope. In International Conference on Machine Learning (ICML), 2018.
Eric Wong, Frank Schmidt, Jan Hendrik Metzen, and J. Zico Kolter. Scaling provable adversarial
defenses. In Advances in Neural Information Processing Systems (NIPS), 2018.
Huan Xu, Constantine Caramanis, and Shie Mannor. Robust regression and lasso. In Advances in
Neural Information Processing Systems (NIPS), 2009a.
12
Under review as a conference paper at ICLR 2020
Huan Xu, Constantine Caramanis, and Shie Mannor. Robustness and regularization of support vector
machines. Journal of Machine Learning Research ,10:1485-1510, 2009b.
Yuichi Yoshida and Takeru Miyato. Spectral norm regularization for improving the generalizability
of deep learning. arXiv:1705.10941, 2017.
Yuchen Zhang, Jason D Lee, and Michael I Jordan. '1-regularized neural networks are improperly
learnable in polynomial time. In International Conference on Machine Learning (ICML), 2016.
Yuchen Zhang, Percy Liang, and Martin Wainwright. Convexified convolutional neural networks. In
International Conference on Machine Learning (ICML), 2017.
Chaoyue Zhao and Yongpei Guan. Data-driven risk-averse stochastic optimization with Wasserstein
metric. Operations Research Letters, 46(2):262-267, 2018.
Ding-Xuan Zhou. The covering number in learning theory. Journal of Complexity, 18(3):739-767,
2002.
H Zhu, C. K.I Williams, R. J Rohwer, and M Morciniec. Gaussian regression and optimal finite
dimensional linear models. In C. M. Bishop (ed.), Neural Networks and Machine Learning.
Springer-Verlag, Berlin, 1998.
Constantin Zalinescu. Convex Analysis in General Vector Spaces. World Scientific, 2002. ISBN
978-981-238-067-8. OCLC: 845511462.
13
Under review as a conference paper at ICLR 2020
Appendix
The pseudo-code of training binary SVMs by enforcing Lipschitz constant is given in Algorithm 1.
Algorithm 1: Training binary SVMs by enforcing Lipschitz constant L
1	Initialise the constraint set S by some random samples from X.
2	for i = 1, 2, . . . do
3	Train SVM using one of the following constraints:
①	Brute-force: ∣∣Vf(w)k2 ≤ L2, ∀ W ∈ S
②	Nystrom holistic: λmaχ(G>G) ≤ L2 using S as the set {w1,..., wn} in Eq (10)
① Nystrom coordinate wise: λmaχ(PG) ≤ L2 using S as the set {w1,...,wn}
in Eq (12)
4	Let the trained SVM be f(i) .
5	Find a new w to add to S by one of the following methods:
①	Random: randomly sample W from X.
①	Greedy: find argmaXχ∈χ ∣∣Vf(i)(x)∣∣ (local optimisation) by L-BFGS with 10
random initialisations. Add the distinct results upon convergence to S.
6	Return if L(i) := maxx∈X ∣∣Vf (i) (x)∣∣ falls below L.
Finding the exact arg maxx∈X ∣Vf (i) (x)∣ is intractable, so we used a local maximum found by
L-BFGS with 10 random initialisations as the Lipschitz constant of the current solution f(i) (L(i) in
step 6). The solution found by L-BFGS is also used as the new greedy point added in step 5b.
Furthermore, the kernel expansion f (x) = : P；=i Yak(xa, ∙) can lead to high cost in optimisation
(our experiment used l = 54000), and therefore we used another Nystrom approximation for
the kernels. We randomly sampled 1000 landmark points, and based on them we computed the
Nystrom approximation for each k(χa, ∙), denoted as 0(χa) ∈ R1000. Then f (x) can be written as
;Pa=ι Ya0(χa)>0(χ). Defining W = : P；=i Ya0(χa), we can equivalently optimise over w, and
the RKHS norm bound on f can be equivalently imposed as the '2-norm bound on w.
To summarise, Nystrom approximation is used in two different places: one for approximating the
kernel function, and one for computing ∣gj ∣H either holistically or coordinate wise. For the former,
we randomly sampled 1000 landmark points; for the latter, we used greedy selection as option b in
step 5 of Algorithm 1.
Detailed algorithm for multiclass classification. It is easy to extend Algorithm 1 to multiclass.
For example, with MNIST dataset, we solve the following optimisation problem to defend `2 attacks:
min
γ1,...,γ10
n	nn
X '(F (x), y),	where F = X Y 1k(xi, ∙);...; X α10k(xi, ∙)
i=1	i=1	i=1
S.t. SUP λmaχ J^G>”>Gc	≈
∣MkH≤1 匕	J
SUP λmaχ(χ G >VV>G Λ ≤ L2 ,
kvk2≤1	c=1
where '(F(x), y) is the Crammer & Singer loss, and the constraint is derived from (14) by using its
Nystrom approximation Gc = [gC,..., gd], which depends on {γ1,..., γ10} linearly. Note that the
constraint itself is a supremum problem:
SUP	u>
kvk2≤1,kuk2≤1
Since there is only one constraint, interior point algorithm is efficient. It requires the gradient of
the constraint, which can be computed by Danskin’s theorem. In particular, we alternates between
14
Under review as a conference paper at ICLR 2020
updating V and u, until they converge to the optimal v* and u*. Finally, the derivative of the constraint
with respect to {γc} can be calculated from PC=ι(u>G>v*)2, as a function of {γc}.
To defend '∞ attacks, We need to enforce the '∞ norm of the Jacobian matrix:
sup g1 (x)
x∈X
∞ = xs∈uXp 1≤mca≤x10 kgc(x)k1
1≤mca≤x10xs∈uXpkgc(x)k1
≤ max sup	u>G> 夕,
1≤c≤10kwk2≤1,kuk∞≤1
where the last inequality is due to
sup kg(x)kι = sup sup u>g(x) ≤ sup	u>G>v.
x∈X	x∈X kuk∞≤1	kvk2≤1,kuk∞≤1
Therefore, the overall optimisation problem to defense '∞ attacks is
n
min	∖f'(F(x), y), where F
γ1,...,γ10
i=1
nn
EY 1k(xi,∙);...EYiOk(Xi, ∙)
i=1	i=1
s.t. sup u>G > v ≤ L,	∀c ∈{1,..., 10}	(15)
kvk2≤1,kuk∞≤1
For each c, we alternatively update v and u in (15), converging to the optimal v* and u*. Finally,
the derivative of SUPkvk2≤1,∣∣u∣∣ ≤ι u>G>V With respect to YC can be calculated from u>G>v*, as
a function of Yc.
A Proofs of results
The following appendix contains the complete set of proofs and auxiliary results.
Proofs for §3: certificate for distributional robustness
Duality results like Lemma 1 have been the basis of a number of recent theoretical efforts in the
theory of adversarial learning (Blanchet et al., 2016; Gao & Kleywegt, 2016; Shafieezadeh-Abadeh
et al., 2017; Sinha et al., 2018), the results of Blanchet & Murthy (2019) being the most general to
date.
Lemma 1 (Blanchet & Murthy (2019, Thm. 1)). Assume Ω is a Polish space and fix μ ∈ P(Ω).
Let C : Ω X Ω → R≥o be lower semicontinuous with c(ω, ω) = 0 for all ω ∈ Ω, and f ∈ L1(Ω, μ)
is upper semicontinuous. Then for all r ≥ 0 there is
sup
ν∈Bc(μ,r)
fdν
λ≥% (λr+∕fλc dμ).
(16)
The necessity for such duality results like Lemma 1 is because while the supremum on the left hand
side of (16) is over a (usually) infinite dimensional space, the right hand side only involves only a
finite dimensional optimisation. The generalised conjugate in (16) also hides an optimisation, but
when the outcome space Ω is finite dimensional, this too is a finite dimensional problem.
The following is sometimes stated a consequence of or in the proof of the McShane-Whitney
extension theorem, but it is immediate to observe.
Lemma 2 (McShane-Whitney). Let X be a set. Assume C : X × X → R≥o satisfies c(x, x) = 0
for all x ∈ X, and f : X → R. Then
∀x,y ∈ X : f (x) - f (y) ≤ λc(x, y) =⇒ ∀y ∈ X : f (y) = SUP (f (x) - λc(x, y)).
x∈X
Lemma 3. Assume X is a locally convex Hausdorff topological vector space. Let C : X → R be
lower semicontinuous, sublinear, and continuous at 0, let f : X → R be closed convex. Thenfor
λ > 0 there is
∀y ∈ X
sup (f (x) — λc(χ - y)) = f( (y)
x∈X	∞
f(X) ⊆ λC(0)
f(X) 6⊆ λC(0).
15
Under review as a conference paper at ICLR 2020
Proof. Because f is closed convex, it is equal to its biconjugate (Zalinescu, 2002, Thm. 2.3.3),
because C is sublinear and lower SemiContinUoUS λc(x) = supχ*∈田⑼ (x,x*i for all X ∈ X
(Zalinescu, 2002, Thm. 2.4.14 (iv)). It follows that
sup
x∈X
f(x) - λc(x - y)
sup sup inf	[x,x*i- f*(x*) -(g*
x∈Xx*∈ f(X) g*∈ λc(0)
SUp SUp inf	({x,x* - g*i + hy,g*i- f*(x*)
x∈Xx*∈ f(X) g*∈ λc(0)
Because C is continuous at 0, 9c(0) is weak*-compact and convex (Zalinescu, 2002, Thm. 2.4.9),
and so we can apply a minimax theorem (Zalinescu, 2002, Thm. 2.10.2) to produce
sup sup inf hx, x* -g*i + hy, g*i -f*(x*)
x∈Xx*∈ f(X) g* ∈ λc(0)
= sup inf sup hx, x* -g*i + hy, g*i - f*(x*)
x*∈ f(X) g*∈ λc(0) x∈X
sup inf	hy,x*i-f*(x*)
x*∈ f(X) g*∈ λc(0)	∞
g* = x*
g * 6= x*
sup	hy,x*i-f*(x*)
x*∈ f(X)	∞
x* ∈ λC(0)
x* ∈/ λC(0)
f(y)	f(X) ⊆ λC(0)
∞	f(X) 6⊆ λC(0),
as claimed.
Remark 1. The minimisation ofg -h, where g and h are convex functions, is called difference convex
(DC) programming (Hiriart-Urruty, 1989). The condition in Lemma 3 bears a striking resemblance
to the common necessary condition (e.g. Hiriart-Urruty, 1989; Penot, 1998) for such problems
x ∈ arg inf f(x0) =⇒	h(x) ⊆ g(x).
x0∈X
Likewise there are similar sufficient conditions. The proof of Lemma 4 is also quite similar to the
proofs of the Toland (1979) duality formula (viz. Hiriart-Urruty, 1986)
inf (g(x) - h(x)) = inf (h* (x* ) - g* (x* )),
x∈X	x* ∈X*
which suggests that the principles of Lemma 4 may be more general. A generalisation to a general
convex function C satisfying C(0) = 0, would remove the positive homogeneity requirement of
Lemma 4, and allow any translation invariant metric in place of C. The assumptions we have made
are compatible with metrics which arise from norms, that is, the translation invariant and positively
homogeneous metrics.
Lemma 4. Assume X is a topological vector space. Let C : X → IR≥o, and f : X → R. Thenfor
λ > 0 there is
∀χ,y ∈ X ： f(X)- f(y) ≤ λc(χ - y) 0 ∂f (X) ⊆ 9λc(0).
Proof. Suppose λ > 0 is such that f(x) - f(y) ≤ λC(x - y) for all x, y ∈ X. Let x* ∈ f(X).
Then there is X ∈ X with
∀y ∈ X : hy - X,X*i ≤ f(y) - f(X)	≤ λC(y - X)
=⇒ ∀y ∈ X : hy, X*i ≤ f(y +X) -	f(X) ≤ λC(y),
this shows X*	∈	λC(0). Next assume λ > 0 satisfies f(X)	⊆ λC(0). Then
∀X	∈	X, ∃X* ∈ f (X), ∀y ∈ X : f(X) - f(y) ≤	hX - y, X*i ≤ λC(X - y),
where the second inequality is because X* ∈ λC(0) for all X* ∈ f (X).
Theorem 1. Assume X is a separable Frechet space and fix μ ∈ P(X). Assume C : X → IR ≥o is
sublinear and continuous, and f ∈ L ι(X, μ) is upper semicontinuous. Thenfor all r ≥ 0,
DRR ：= SUP	risk`(f, ν) ≤ r lipc('f) + risk`(f, μ).
ν∈Bc(μ,r)
16
Under review as a conference paper at ICLR 2020
The tightness of the bound can be quantified as follows. Let ∆(μ) := r lipc('f) + risk`(f, μ)一
suPν∈Bc(μ,r) risk'(f, ν). If lip/f) < ∞ then
∆(μ) ≤ r(liPc(f) - [liPc(co'f) 一 I/('f 一 coQf )d“J
where [ ∙]十:= max{ ∙ , 0} and 1/0 := ∞, so that when Qf is closed convex there is equality in (3).
Proof. Because C is assumed sublinear, it is positively homogeneous and there is c(x, x) = c(x-x)
c(0) = 0 for all X ∈ X. Therefore we can apply Lemma 1 and Lemma 2 to obtain
sup
ν∈Bc(μ,r) J
Qf dν = inf rλ +
J	λ≥0
≤ inf
一λ≥liPc('f)
/ Qλc dμ
rλ + / QfC dμ
r IiPC(Qf) + / 'f d〃.
Observing that co Qf ≤ Qf, applying Lemma 3 and Lemma 4 we find for all X ∈ X
sup (Qf(X) —	QfC(x)	— rλ)	= sup	(Qf(X) — sup (Qf (y) — λc(x	— y))	— rλ)
f∈[0,∞) '	f f∈[0,∞)	'	y∈X	)
= sup inf (Qf(x) — Qf(y) + λc(x — y) — rλ)
f∈[0,∞) y∈x '	)
≤ sup inf (Qf(x) — coQf(y) + λc(x — y) — λr)
f∈[0,∞) y∈x '	)
= sup (Qf (x) — co Qf (x) — ∞ Jlip/EQf) > λj —
f∈[0,∞) '
=Qf (x) — co Qf (x) — r lip/E Qf).
Similarly, for all x ∈ X there is
sup (Qf(X) — QfC(x) — rλ) ≤ sup (Qf(X) — QfC(X)) + sup -rλ
λ∈[0,∞) '	f f∈[0,∞) '	f f∈[0,∞)
=sup (Qf(X) - QfC(X))
f∈[0,∞)、	)
= sup inf (Qf(x) — Qf(y) + λc(x — y)
f∈ [0,∞) y∈X '	'
≤ inf sup (Qf(x) — Qf(y) + λc(x — y)
y∈X f∈[0,∞)、	‘
∞ ∞ f∞ c(x — y) > 0
= inf
y∈x [0	c(x — y) = 0
=0.
(1)
(2)
Then, using (1) and (2) we find
(r IiPC(Qf)+/Qf d”)- f∈in,∞)
=r lipc(Qf) + sup /(Qf - QfC - λr) dμ
λ∈ [0,∞) J
≤ r lipc(Qf) + ( sup (Qf - QfC - λr) dμ
J λ∈[0,∞)
(1),(2)	( f	1
≤ r IiPC(Qf) + min<	(Qf - co Qf) dμ - r lipc(co Qf), 0 >.
The proof is complete.
17
Under review as a conference paper at ICLR 2020
Proposition 1. Assume X is a separable Frechet space with Xo ⊆ X. Assume C : X → R≥o is
sublinear and continuous, and'f ∈ ∩μ∈p(χ0)Lι(X,μ) is Uppersemicontinuous, has lip/'f) < ∞,
and attains its maximum on X0. Then for all r ≥ 0 with 1/0 := ∞,
sup ∆(μ)
μ∈P(Xo)
T(lip。('f) - hliPc(Co'f) - r ρ('f )i+).
Proof. Let x0 ∈ X0 be be the point at which 'f (x0) = supx∈X0 'f (x). Then
	∆(δχo) = r lip0('f) + /'fdδχ0-	sup	' 'f dδχo ν∈Bc(δx0,r) = r lip。 ('f) +	'f dδx0 -	'f dδx0 =r lipc('f).	⑶
Then there is
(3)
r lipc ('f) ≤
SUP ∆(μ) ≤) r(lipc('f) - max{lipc(Co'f) - 1 ρ('f), θ}) ≤ r lip/'f),
μ∈P(Xo)	∖	l	r	"
which completes the proof.
Remark 2. When f : Rn → R satisfies f ≡ ∞ and f is minorised by an affine function, there is (cf.
Hiriart-Urruty & Lemar6chal, 2010, Prop. 1.5.4)
∀x ∈ Rn : cof (x) = inf< E aif(xi) | E a = 1, x = E αiXi >,
I i∈[n+1]	i∈[n+1]	i∈[n+1]
where the infimum is over all sequences (αi)i∈[n+1] and (xi)i∈[n+1] ⊆ Rn satisfying the conditions
above. Consequentially there is the common expression
ρ(f) = sup f X	αixi	- X	αif(xi)	|	(αi, xi)i∈[n+1] ⊆ R≥0	× Rn,	X	αi	= 1 .
I	i∈[n+1]	i∈[n+1]	i∈[n+1]	
In Lemma 5, by the weak* topology on P(Ω) We mean the coarsest topolgoy on P(Ω) that makes
the bounded continuous functions on Ω its topological dual space. Likewise ** denotes convergence
in this topology.
Lemma 5. Assume (Ω, c) is a compact Polish space and μ ∈ P(Ω) is non-atomic. For any V? ∈
P(Ω) and r > 0 there is a SeqUenCe (fi)i∈N ⊆ Aμ(r) := {f ∈ Lo(Ω, Ω) | J Cd(id, f )#μ ≤ r}
with (fi)#M ** ν?.
Proof. Let P(μ, V) := {f ∈ Lo(X, X) | f#M = ν}. Since μ is non-atomic and C is continuous
Pratelli (2007, Thm. B) shows
∀ν ∈ P(Ω)
f ∈P⅛,ν)∕c d(id,f )#〃
costc(μ, ν).
Let r? := costc(μ, ν?), obviously r? ≤ r. Assume r? > 0, otherwise the lemma is trivial. Fix a
sequence (εk)k∈N ⊆ (0, r?) with εk → 0. For U ≥ 0 let V(u) := μ + U(V? - μ). Then
costc(μ, v(0)) = 0 and costc(μ, ν(1)) = r?,
and because cost。metrises the weak* topology on P(Ω) (Villani, 2008, Thm. 6.9), the mapping
u → costc(μ, v(u)) is continuous. Then by the intermediate value theorem for every k ∈ N there is
some Uk > 0 with costc(μ, V(Uk)) = r? - εk, forming a sequence (uk)k∈N ⊆ [0,1]. Then for every
k there is a sequence (fjk)j∈N ⊆ P(μ, V(Uk)) so that (fjk)#μ * *v(k) and
lim
j∈N
/ C d(id, fjk )#M
inf C cd(id,fk)#M
f ∈P (μ,ν(k))√	#
costc(μ, v(k))
r? - εk.
18
Under review as a conference paper at ICLR 2020
Therefore for every k ∈ N there exists jk ≥ 0 so that for every j ≥ jk
/cd(id,fjk)#〃 ≤ r?.	⑶
Let us pass directly to this subsequence of (fjk)j∈N for every k ∈ N so that (3) holds for all j, k ∈ N.
Next by construction we have ν(uk) → ν?. Therefore (fjk)j,k∈N has a subsequence in k so that
(fjk)#M ** V?. By ensuring (3) is satisfied, the sequences (fjk)j∈N ⊆ Aμ(r) for every k ∈ N. ■
Theorem 2. Assume (X,c) is a SeParable Banach space. Fix μ ∈ P(X) and let Rμ(r):=
{g ∈ Lo(X, R≥o) | R g dμ ≤ r}. Thenfor f ∈ L0(Ω, R), r > 0 there is
variable-radius risk := sup / μ(dω) sup 'f(ω0) ≤ sup risk`(f, ν) = DRR.
g∈Rμ(r) J	ω0∈Bc(ω,g(ω))	ν∈B0(μ,r)
The equality holds in (6) if μ is non-atomically concentrated on a compact subset of X, on which f is
continuous with the subspace topology.
Proof. Inequality (6). For g ∈ Rμ(r), let Γg : X ⇒ X denote the set-valued mapping with
Γg(x) := Bc(x, g(x)). Let L0(X, Γg) denote the set of Borel a : X → X so that a(x) ∈ Γg(x) for
μ-almost all X ∈ X. Let Aμ(r) := Ug∈R*(r) Lo(X, Γg). Clearly for every a ∈ Aμ(r) there is
r ≥ J c(x,a(x))dμ = J
Cd(id, a)#M,
which shows {a#M | a ∈ Aμ (r)} ⊆ Bc(μ, r). Then if there is equality in (4), we have
sup
g∈Rμ(r)
sup f(x)
x0 ∈Γg (x)
sup sup
g∈Rμ(r ) a∈L0 (X,rg )
/ fda#〃
(4)
sup / f da#M
a∈Aμ(r) J
≤
sup
ν∈Bc(μ,r)
f da# ν,
which proves the inequality (6).
Equality (4). To complete the proof we will now justify the exchange of integration and supremum in
(4). The set L0(X, Γg) is trivially decomposable (Giner, 2009, see the remark at the bottom ofp. 323,
Def. 2.1). By assumption f is Borel measurable. Since f is measurable, any decomposable subset
of L0(X, X) is f -decomposable (Giner, 2009, Prop. 5.3) and f -linked (Giner, 2009, Prop. 3.7 (i)).
Giner (2009, Thm. 6.1 (c)) therefore allows us to exchange integration and supremum in (4).
Equality in (6). Under the additional assumptions there exists ν? ∈ P(Ω) with (via Blanchet &
Murthy, 2019, Prop. 2)
fdν?
sup
ν∈Bc(μ,r)
fdν.
The compact subset where μ is concentrated and non-atomic is a Polish space with the Banach metric.
Therefore Using Lemma 5 there is a sequence (fi)i∈N ⊆ Aμ(r) so that
lim
i∈N
fidμ =
fdν?
sup
ν∈Bc(μ,r)
fdν,
proving equality in (6).

19
Under review as a conference paper at ICLR 2020
Figure 5: In the classical adversarial risk (5) the perturbation size at each point is at most r (blue),
however with the variable-radius risk the expected perturbation size is at most r.
Proofs for §4: provable lipschitz regularisation for kernel methods
Theorem 3. Suppose k0, X0, and ν0 satisfy Assumptions 1 and 2. Let {wjs : s ∈ [n], j ∈ [d]} be
sampled i.i.d. from νo. Thenfor any f whose coordinate-wise Nystrom approximation (11) and (12)
satisfy λmaχ (Pg) ≤ L2 ,the Lipschitz condition λmaχ(G>G) ≤ L2 + ε is met with probability 1 一 δ,
as long as n ≥ Θ (5 N2M∣Qi log dNε), almost independent of d. Here Θ hides all poly-log terms.
Proofs and more results for §4: Kernel Approximation
A.1 Random sampling requires exponential cost
The most natural idea of leveraging the samples is to add the constraints kg(ws)k ≤ L. For Gaussian
kernel, we may sample from N(0, σ2I) while for inverse kernel we may sample uniformly from B.
This leads to our training objective:
1l	λ
min γElOSSf(X),y ) + 2 kfkH St	kg(W)k ≤ L, ∀s ∈ H
f∈H l	2
i=1
Unfortunately, this method may require O(2) samples to guarantee PjkgjkH ≤ L2 + ε w.h.p.
ThiS iS illuStrated in Figure 6, where k iS the polynomial kernel with degree 2 whoSe domain X iS the
unit ball B, and f (x) = 2 (Qx)I2. We seek to test whether the gradient g(x) = (VTx)V has norm
bounded by 1 for all x ∈ B, and we are only allowed to test whether kg(ws)k ≤ 1 for samples ws
that are drawn uniformly at random from B. This is equivalent to testing kVk ≤ 1, and to achieve
it at least one Ws must be from the ε ball around Vl kvk or -v/ kvk, intersected with B. But the
probability of hitting such a region decays exponentially with the dimensionality d.
The key insight from the above counter-example is that in fact kV k can be easily computed by
Pd=I(VTWs )2, where {Ws}d=ι is the orthonormal basis computed from the Gram-Schmidt process
on d random samples {Ws}sd=1 (n = d). With probability 1, n samples drawn uniformly from B
must span Rd as long as n ≥ d, i.e., rank(W) = d where W = (W1, . . . , Wn). The Gram-Schmidt
process can be effectively represented using a pseudo-inverse matrix (allowing n > d) as
kVk2=(WTW)-1/2WTV	,
where (WT W)-1/2 is the square root of the pseudo-inverse of WTW . This is exactly the intuition
underlying the Nystrom approximation that we will leveraged.
A.2 Spectrum of Kernels
Let k be a continuous kernel on a compact metric space X, and μ be a finite Borel measure on X
with supp[μ] = X. We will re-describe the following spectral properties in a more general way than
20
Under review as a conference paper at ICLR 2020
Figure 6: Suppose we use a polynomial kernel with degree 2, and f (x) = 2(v>x)2 for X ∈ B.
Then g(x) = (v>x)v. If we want to test whether supχ∈B Ilg(X) |卜 ≤ 1 by evaluating Ilg(W) ∣∣2 on
W that is randomly sampled from B such as wι and w2, we must sample within the ε balls around
the intersection of B and the ray along V (both directions). See the blue shaded area. The problem,
however, becomes trivial if we use the orthonormal basis {W1,W2}.
in §4. Recall from Chapter 4 of Steinwart & Christmann (2008) that the integral operator for k and μ
is defined by
Tk = Ik ◦ Sk ： L2(X,μ) → L2(X,μ)
where Sk : L2(X; μ) →C (X),
(Skf)(x)
/ k(χ,y)f (y)dμ(y),
f ∈ L2(X, μ),
Ik : C(X) → L2(X; μ), inclusion operator.
By the spectral theorem, if Tk is compact, then there is an at most countable orthonormal set (ONS)
{ej}j∈j of L2 (X, μ) and {λj}j∈j with λι ≥ λ2 ≥ ... > 0 such that
Tf = ∑>j hf, aiL2(x；“)a,	f ∈ L2(X,μ).
j∈J
In particular, we have(以,Gj)l2(xR) = δj (i.e., equals 1 if i = j, and 0 otherwise), and Tei =入超.
Since Gj is an equivalent class instead of a single function, we assign a set of continuous functions
ej = λj-1Skeej ∈ C(X), which clearly satisfies
hei, ejiL2(Xw) = δij, Tej = λjej.
We will call λj and ej as eigenvalues and eigenfunctions respectively, and {ej }j∈J clearly forms an
ONS. By Mercer’s theorem,
k(x, y) =	λjej(x)ej(y),	(5)
j∈J
and all functions in H can be represented by Pj∙∈j aj∙ej∙ where {aj∙/∕λj} ∈ '2( J). The inner
product in H is equivalent to
see that
(Pj∈j ajej, Pj∈ j bjjejH = Pj∈ j ajbj/λj. Therefore it is easy to
ψj := λ∕λj^ej,	j ∈ J
is an orthonormal basis of H, with Moreover, for all f ∈ H with f = Pj∈j ajej, we have
hf, ej iH = aj /λj, hf,ψj iH = aj / pλj, and
f = Ehf, ψjiH ψj = E √λj hf, ejiH ψj = E λjhf, ejiH ej.
jj	j
Most kernels used in machine learning are infinite dimensional, i.e., J = N. For convenience, we
define φm :=(夕 1,..∙, ψm ) and /m = diag(λ1,. . . , λm ).
21
Under review as a conference paper at ICLR 2020
A.3 General sample complexity and assumptions on the product kernel
In this section, we first consider kernels k0 with scalar input, i.e., X0 ⊆ R. Assume there is a
measure μo on X°. This will serve as the basis for the more general product kernels in the form of
k(x, y) = Qjd=1 k0(xj, yj) defined over X0d.
With Assumptions 1 and 2, we now state the formal version of Theorem 3 by first providing the
sample complexity for approximating the partial derivatives. In the next subsection, we will examine
how three different kernels satisfy/unsatisfy the Assumptions 1 and 2, and what the value of Nε is.
For each case, We will specify μo on Xo, and the measure on Xd is trivially μ = μd.
Theorem 4. Suppose {ws}n=ι are drawn iid from μo on Xo, where μo is the uniform dis-
tribution on [-v/2, v/2] for periodic kernels or periodized Gaussian kernels. Let Z :=
(ko(w1, ∙), ko(w2, •),..., ko(wn, ∙)), and gι = 1 Pa=I Yaga: X0 → R, where ∣∣γ∣∣∞ ≤ ci and
d
ga(y) = ∂0,1k(xa,y) = ha(yi) Y ko(xj,yj) With ha(∙) := ∂o,iko(xa, ∙).
j=2
Given ε ∈ (0,1], let Φm =(夕i,...夕m) where m = N. Then with probability 1 一 δ, thefollowing
holds when the sample size n = max(Nε, 352NεQ2 log 2Nε):
kgikH ≤ 12Y>KiY + 3cι(l + 2pNεMε)ε,	⑹
d
where (Ki)a,b = (hia)>Z(Z>Z)-iZ>hbiYk0(xja,xjb).
j=2
Then we obtain the formal statement of sample complexity, as stated in the following corollary, by
combining all the coordinates from Theorem 4.
Corollary 1. Suppose all coordinates share the same set of samples {ws}sn=i. Applying the results
in (6) for coordinates from 1 to d and using the union bound, we have that with sample size
n = max(Nε, 352 NεQ2 log 2Nε), thefollowing holds with probability 1 — dδ,
λmaχ(G>G) ≤ λmaχ(pG) + 3ci (l + 2pN；Mc》.	(7)
Equivalently, if Nε, Mε and Qε are constants or poly-log terms of ε which we treat as constant, then
to ensure λmax (G> G) ≤ λmax(PG) + ε with probability 1 一 δ, the sample size needs to be
n = ε5 c2^1+2pNεMε)2NεQε log 2^ ∙
Remark 1. The first term on the right-hand side of (7) is explicitly upper bounded by L2 in our
i
training objective. In the case ofTheorem 1, the values of Qε, N, and Mε lead to a O(者)sample
complexity. If we further zoom into the dependence on the period V, then note that N is almost
a universal constant while Mε = A^in (N - 1). So overall, n depends on V by *. This is not
surprising because smaller period means higher frequency, hence more samples are needed.
Remark 2. Corollary 1 postulates that all coordinates share the same set of samples {ws}sn=i. When
coordinates differ in their domains, we can draw different sets of samples for them. The sample
complexity hence grows by d times as we only use a weak union bound. More refined analysis could
save us a factor of d as these sets of samples are independent of each other.
ProofofTheorem 4. Let ε0 := (1 + 2√mMε)ε. Since〈g?,gb)制=〈九?,^)^ QQd =2 ko(xj,xj)
and k0(xja, xjb) ≤ 1, it suffices to show that for all a, b ∈ [l],
Towards this end, it is sufficient to show that for any h(∙) = Hχ∂0,iko(x, ∙) + Hy∂0,iko(y, ∙) where
χ,y ∈ Xo and 步x| + Wy | ≤ 1, we have
∣h>Z(Z>Z)-iZ>h -khkH0∣ ≤ ε0.	(8)
22
Under review as a conference paper at ICLR 2020
This is because, if so, then
h1a,hb1H0 -(h1a)>Z(Z>Z)-1Z>hb1
=1 (Uha + h1UH0 -khakHo TMUH0) -1 [m + M)>z (Z >z)-1z >ha + 措)
-(h1a)>Z(Z>Z)-1Z>h1a - (hb1)>Z(Z>Z)-1Z>hb1
≤ 2(4ε0 + ε0 + ε0) = 3ε0.
The rest of the proof is devoted to (8). Since n ≥ m, the SVD of Λm112Φ>bZ can be written as UΣV >,
where UU> = U>U = V>V = Im (m-by-m identity matrix), and Σ = diag(σ1, . . . , σm). Define
α = n-1/2VU >/m1/2@mh.
Consider the optimization problem o(α) := 1 ∣∣Zα - h∣∣H. It is easy to see that its minimal
objective value is o* := 1 IlhIlHo _ 1 h>Z(Z>Z)-1Z>h. So
0 ≤ 2o* = ∣∣h∣Ho - h>Z(Z>Z)-1Z>h ≤ 2o(α).
Therefore to prove (8), it suffices to bound o(α) = ∣∣Za - h∣H. Since √nΦmΛ1SUV>α =
ΦmΦ>m h, we can decompose ∣Zα - h∣H by
|Zα - h∣Ho ≤ ∣∣(Z - ΦmΦmZ)α∣∣H0 + ||停情。*。-√nΦmΛ^2UV>)α∣L	(9)
+ UUΦmΦ>m h - hUUH0 .
The last term ∣ΦmΦ>m h - h∣H is clearly below ε because by Assumption 1 and m = Nε
∣∣ΦmΦ>mh-h∣∣H0
≤ 步x| ∣∣ΦmΦm∂0,1k0(x, ∙) - ∂0,1ko(x, ∙)∣∣H0 + M| ∣∣ΦmΦm∂0,1ko(y, ∙) - ∂0,1k0(y, ∙)∣∣H0
≤(∣几∣ + MI)ε ≤ ε.
We will next bound the first two terms on the right-hand side of (9).
(i)	By Assumption 1, ∣∣ko(ws, ∙) - ΦmΦmko(ws, ∙)∣∣H ≤ ε, hence ∣∣(Z - ΦmΦmZ)α∣∣H ≤
ε√n ∣∣α∣2. To bound ∣∣α∣∣2, note all singular values of VU> are 1, and so Assumption 2 implies that
for all i ∈ [m],
卜-1/2 Qj,hiH0∣ = ∣hej,hiH0∣ = |〈ej,^xd0,1k0(χ, ∙) + Hyd0,1k0(y, "hJ	(IO)
≤ sup |〈ej,∂0,1k(x, ∙))Hι∣ ≤ Mε.
As a result,
∣∣ (Z - φmφmZ)αj∣∣H0 ≤ εn1∕2 ∙n-1∕2 1/m^ZOmhU ≤ ε√mMε.
(ii)	We first consider the concentration of the matrix R := nΛm~12Φ>bZZ>ΦmΛm1∕2 ∈ Rm×m.
Clearly,
E [Rij] = E — X : ei (Ws)ej (Ws)I = I ei (X)ej (X) dμ(X) = δij.
{ws}	{ws} n s 1
By matrix Bernstein theorem (Tropp, 2015, Theorem 1.6.2), we have Pr ∣R - Im∣sp ≤ ε ≥ 1 - δ
when n ≥ O(.). This is because ∣∣(eι(x),..., em(x))∣2 ≤ mQ2, ∣∣E{ws}[RR>] ∣∣sp ≤ mQ2∕n, and
Pr ∣R - Im∣sp ≤ε
≥ 1 - 2m exp
(等-ε+3 ε)
23
Under review as a conference paper at ICLR 2020
where the last step is by the definition of n. Since R = 1UΣ2U>, this means with probability 1 - δ,
Il1 U∑2U> - ImIlsp ≤ ε. So for all i ∈ [m],
n σ2- 1 ≤ε
which implies
σ=+ + 1
-1
≤ ε.
(11)
σ=— - 1 < ε
Moreover, λ1 ≤ 1 since k0(x, x) = 1. It then follows that
(ΦmΦmmZ - √nΦmΛm2UV>)α
H0
φm λ^ uςv> √nVU>Λm1/2φmh - √nφmΛm/2uv> √nvu>Λm1/2φmh
H0
=Λm∕2u(√nΣ - Im)U>Λm1∕2Φmlh
≤pλ1 md*- RlM/hlL
(because Φ>mΦm = Im )
≤ε√mMε	(by (11), (10), and λι ≤ 1).
Combining (i) and (ii), we arrive at the desired bound in (6).
ProofofCoroUary 1. Since PG approximates G>G only on the diagonal, PG - G>G is a diagonal
matrix which we denote as diag(δι,..., δd). Let U ∈ Rd be the leading eigenvector of PG. Then
Amax(P5G) - λmax(G>G) ≤ u> Pgu - u>G>Gu = u> (Pg - G>G)u = X δjuj
j
(by (6))	≤ 3cι(l + 2√N≡Mε)ε.
The proof is completed by applying the union bound and rewriting the results.
A.4 Case 1: Checking Assumptions 1 and 2 on periodic kernels
Periodic kernels on X0 := R are translation invariant, and can be written as k0 (x, y) = κ(x - y)
where κ : R → R is a) periodic with period v; b) even, with κ(-t) = κ(t); and c) normalized with
κ(0) = 1. A general treatment was given by Williamson et al. (2001), and an example was given by
David MacKay in MacKay (1998):
We define μo to be a uniform distribution on [-2, 2], and let ω0 = 2π∕v.
Since κ is symmetric, we can simplify the Fourier transform of κ(t)δv (t), where δv (t) = 1 if
t ∈ [-v/2, v/2], and 0 otherwise:
1 v/2
F (ω) =  	κ(t)cos(ωt)dt.
2π	v/2
24
Under review as a conference paper at ICLR 2020
It is now easy to observe that thanks to periodicity and symmetry of κ, for all j ∈ Z,
1 Z	k0(x,y)cos(jω0y)dy = 1 Z	κ(x - y)cos(jω0y)dy
v -v/2	v -v/2
=-Z	κ(z) cos(jωo(x — Z)) dz (note cos(jω0(x — Z)) also has period V)
v x-v/2
=— /	κ(z)[cos(jω0x)cos(jωoz)+sin(jωo x)sin(jω0z))dz (by periodicity)
v	-v/2
=—cos(jωox) /	K(Z)CoS(jω0z)dz (by symmetry of K)
v	-v/2
=^-2πF(jωo) cos(jωox).
v
And similarly,
1	Z	k0(x,y)sin(jω0y)dy = ^nF (jω0)sin(jω0 x).
v -v/2	v
Therefore the eigenfunctions of the integral operator Tk are
eo(x) = 1,	ej(x) := √2cos(jωox),	e-j(x) := √2sin(jωox) (j ≥ 1)
and the eigenvalues are λj = A^nF(jω0) for all j ∈ Z with λ-j = λj. An important property our
proof will rely on is that
e0j (x) = -j ω0 e-j (x), for all j ∈ Z.
Applying Mercer’s theorem in (5) and noting K(0) = 1, we derive Pj∈Z λj = 1.
Checking the Assumptions 1 and 2. The following theorem summarizes the assumptions and
conclusions regarding the satisfaction of Assumptions 1 and 2. Again we focus on the case of
X ⊆ R.
Theorem 1. Suppose the periodic kernel with period v has eigenvalues λj that satisfies
λj(1 + j)2 max(1, j2)(1 + δ(j ≥ 1)) ≤ C6 ∙ c-j, for all j ≥ 0,	(13)
where c4 > 1 and c6 > 0 are universal constants. Then Assumption 1 holds with
Nε = 1 + 2 [nεC ,	where n ：= logc4 (2^ max(1,看))∙	(14)
In addition, Assumption 2 holds with Qε = √2 and Mε = 2λν2π [nε[ = ±2π(N — 1)∙
For example, if we set v = π and σ2 = 1/2 in the kernel in (12), elementary calculation shows that
the condition (13) is satisfied with c4 = 2 and c6 = 1.6.
Proof of Theorem 1. First we show that h(x) := ∂0,1k0(x0, x) is in H0 for all x0 ∈ X0. Since
k0(x0,x) = Pj∈Z λj ej (x0)ej (x), we derive
h(x) = J^λj-ej(xo)∂1ej(x) = £ λjej(x0)(-jω°e-j(x)) = ω° £ λjje-j(xo)ej(x). (15)
j∈Z	j∈Z	j∈Z
h(χ) is in H if the sequence λj-je-j∙(χo)∕√zλj is square summable. This can be easily seen by (13):
ω0-2khk2H0=Xλjj2e2-j(x0)=Xλjj2e2-j(x0)
j	j∈Z
=X λj j 2e-j (xo) = λo + 2 X j 2λj≤ 二号.
j∈Z	j≥1	c4—1
25
Under review as a conference paper at ICLR 2020
Finally to derive Nε, we reuse the orthonormal decomposition of h(x) in (15). For a given set ofj
values A where A ⊆ Z, we denote as Φa the “matrix" whose columns enumerate the 夕j over j ∈ A.
Let us choose
A:
λj max(1, j2)(1 + j2)(1 + δ(j ≥ 1)) ≥ min(1, w0-2
If j ∈ A, then -j ∈ A. Letting N0 = {0,1, 2,...}, we note Pj∈N 1+j2 ≤ 2.1. So
h -ΦAΦA>h2H0 = w02 X λjj2e2-j(x0)
j∈Z∖A
w02	λjj2(ej2(x) + e2-j(x))δ(j ≥ 1) +δ(j = 0)
j∈N0∖A
w02	λjj2(1 + δ(j ≥ 1))
j∈N0∖A
W0 X {λjj2(1+ j2)(1 + δ(j ≥ 1))1+j2
j∈N0∖A
21
ε1
≤ 21	1 + j2
j∈N0
21
—X -ɪ ≤ ε2.
2.1 乙 1+ j2 一
j∈N0
Similarly, we can bound ∣∣k0(x0, ∙) - ΦAΦ>k0(x0, ∙)^h0 by
∣∣k0(x0, ∙) - ΦAΦ>ko(xo, ∙)∣∣Ho
λjej2(x0)≤ λj max(1,j2)ej2(x0)
j∈Z∖A	j∈Z∖A
E λα max(1,j2)[(e2(x) + e- (x))δ(j ≥ 1) + δ(j = 0)]
j∈N0∖A
= X 1% max(1,j2)(1 + j2)(1 + δ(j ≥ I))1+j2
j∈N0∖A
≤ ɪε2 X -ɪ, ≤ ε2.
一2.1 乙 1+ j2 一
j∈N0
To upper bound the cardinality of A, we consider the conditions for j ∈/ A. Thanks to the conditions
in (13), we know that any j satisfying the following relationship cannot be in A:
-|j|	-2 ε2	-|j|	1	4π2	2
C6 "	< min(1,wo	)—	⇔	C4"1	< 2 1	eɛ	mini	1, —r	lε .
So A ⊆ {j : |j| ≤ nε}, which yields the conclusion (14). Finally Qε ≤ √2, and to bound Mε, we
simply reuse (15). For any j with |j| ≤ nε,
I. . I	.......... 2π ɛ.	.	√2π ,一 、	一
I hh, ejiH∣	≤ ω0	lje-j (XO)I	≤ ^v^ 2b	bnεc	= -V- (Nε -	I) .	■
A.5 Case 2: Checking Assumptions 1 and 2 on Gaussian kernels
Gaussian kernels k(x, y) = exp(- kx - yk2 /(2σ2 )) are obviously product kernels with
k0(x1,y1) = κ(x1 - y1) = exp(-(x1 - y1)2∕(2σ2)). It is also translation invariant. The spectrum
of Gaussian kernel k0 on R is known; see, e.g., Chapter 4.3.1 of Rasmussen & Williams (2006) and
Section 4 of Zhu et al. (1998). Let μ be a Gaussian distribution N(0,σ2). Setting ε2 = α2 = (2σ2)-1
26
Under review as a conference paper at ICLR 2020
in Eq 12 and 13 of E Fasshauer (2011), the eigenvalue and eigenfunctions are (for j ≥ 0):
λj = c-jτ∕2,	where co = 2(3 + √5)
/ 、	51/8	√ √5 — 1 x2 ʌ 1 丁r 4 4--------x∖
ej (x) = jexp (_—_ σ2) √j! Hj (√1∙25 σ),
where Hj is the Hermite polynomial of order j.
Although the eigenvalues decay exponentially fast, the eigenfunctions are not uniformly bounded
in the L∞ sense. Although the latter can be patched if we restrict x to a bounded set, the above
closed-form of eigen-pairs will no longer hold, and the analysis will become rather challenging.
To resolve this issue, we resort to the period-ization technique proposed by Williamson et al. (2001).
Consider κ(x) = exp(-x2∕(2σ2)) when X ∈ [-v∕2,v∕2], and then extend K to R as a periodic
function with period v. Again let μ be the uniform distribution on [-v∕2,v∕2]. As can be seen from
the discriminant function f = ; P；=i Yik(Xi, ∙), as along as our training and test data both lie in
[-v∕4, v∕4], the modification of κ outside [-v∕2, v∕2] does not effectively make any difference.
Although the term ∂0,1k0(X1a, w11) in (13) may possibly evaluate κ outside [-v∕2, v∕2], it is only
used for testing the gradient norm bound of κ.
With this periodized Gaussian kernel, it is easy to see that Qε = √2. If we standardize by σ = 1
and set v = 5π as an example, it is not hard to see that (13) holds with c4 = 1.25 and c6 = 50. The
expressions of Nε and Mε then follow from Theorem 1 directly.
A.6 Case 3: Checking Assumptions 1 and 2 on non-product kernels
The above analysis has been restricted to product kernels. But in practice, there are many useful
kernels that are not decomposable. A prominent example is the inverse kernel: k(X, y) = (2-X>y)-1.
In general, it is extremely challenging to analyze eigenfunctions, which are commonly not bounded
(Lafferty & Lebanon, 2005; Zhou, 2002), i.e., supi→∞ supx |ei(X)| = ∞. The opposite was
(incorrectly) claimed in Theorem 4 of Williamson et al. (2001) by citing an incorrect result in
Konig (1986, p. 145), which was later corrected by Zhou (2002) and Steve Smale. Indeed, uniform
boundedness is not known even for Gaussian kernels with uniform distribution on [0, 1]d Lin et al.
(2017), and (Minh et al., 2006, Theorem 5) showed the unboundedness for Gaussian kernels with
uniform distribution on the unit sphere when d ≥ 3.
Here we only present the limited results that we have obtained on the eigenvalues of the integral
operator of inverse kernels with a uniform distribution on the unit ball. The analysis of eigenfunctions
is left for future work. Specifically, in order to drive the eigenvalue λi below ε, i must be at least
ddlog2 εe + 1. This is a quasi-quadratic bound if we view d and 1∕ε as two large variables.
It is quite straightforward to give an explicit characterization of the functions in H. The Taylor
expansion of ZT at Z = 2 is 1 P∞=0(-2)iχi. Using the standard multi-index notation with
α = (αι,..., αd) ∈ (N ∪ {0})d, ∣α∣ = Pd=ι α%, and Xa = x；1 ... Xad, We derive
k(x, y)
1
2 — x> y
∞
X 2-k-1 X	Cαkxαyα
k=0	a:|a| = k
£2-3-1CFxaya,
a
where Ca = QTk-；. So we can read off the feature mapping for x as
i=1 αi!
<χx) = {wαxα : α},	where Wa = 2-2(Ial+1)CF,
and the functions in H are
H = {f = X Hawaxa ： Wk'? < ∞ }.
(16)
Note this is just an intuitive “derivation” while a rigorous proof for (16) can be constructed in analogy
to that of Theorem 1 in Minh (2010).
27
Under review as a conference paper at ICLR 2020
A.7 Background of eigenvalues of a kernel
We now use (16) to find the eigenvalues of inverse kernel.
Now specializing to our inverse kernel case, let us endow a uniform distribution over the unit ball
B: p(χ) = V-I where Vd = nd/2r(d + 1)-1 is the volume of B, with Γ being the Gamma
function. Then λ is an eigenvalue of the kernel if there exists f = Pα Hαwαxα such that
Ry∈B k(x, y)p(y)f (y) dy = λf (x). This translates to
Vd-1 y∈B Xα
wα2 xαyα	Heweye dy = λ	Hαwαxα,	∀ x ∈ B.
β
α
Since B is an open set, that means
Wa Eweqα+βHe = λ"α,	∀ α,
β
where
[2 Qd=I γG J)
qa = Vd-1	ya dy = < Vd∙(Ia1+a"(2|a1 + d
y∈Bβ	Io	'
if all αi are even
otherwise
In other words, λ is the eigenvalue of the infinite dimensional matrix Q = [wαwβqα+β]α,β,
A.8 B ounding the eigenvalues
To bound the eigenvalues of Q, we resort to the majorization results in matrix analysis. Since
k is a PSD kernel, all its eigenvalues are nonnegative, and suppose they are sorted decreasingly
as λ1 ≥ λ2 ≥ .... Let	the row corresponding to α have `2 norm rα, and let them be sorted as
r[1] ≥ r[2] ≥ .. Then	by Schneider (1953); Shi & Wang (1965), we have
n
n
i=1
λi ≤	r[i],	∀n≥1.
i=1
So our strategy is to bound rα first. To start with, we decompose qα+β into qα and qβ via Cauchy-
Schwartz:
qα2 +β = Vd-
y b ya+e dy) ≤ VJ Z ^y2a dy ./	y2β dy = q2aq2e.
To simplify notation, we consider without loss of generality that d is an even number, and denote
the integer b := d/2. Now Vd = πb∕b!. Noting that there are ( k + d 1 ) values of β such that
∣β∣ = k, we can proceed by (fix below by changing ( k + d ) into ( k + d 1 ), or no need
because the former upper bounds the latter)
rα2
wα2	we2 qα2 +e ≤ wα2 q2α	we2 q2e = wα2 q2α
ee
∞
X2-k-1	X	Cekq2e
k=0	e:|e|=k
≤
wα2 q2α
wα2 q2α
∞
X2
k=0
∞
X2
k=0
-k-1	k+dd	|me|a=xkCekq2e
-k-1	k+d
d
k!
max —ʒ-------
lβl=k Qd=I βi!
2 Qd=Ir(仇 + 1)
Vd ∙ (2k + d) ∙ Γ(k + d)
wα2q2αVd-1X∞ 2-k	k+dd
k=0
k!
--------------∙- ∙ max
(2k + d)Γ (k + d) ∣β∣=k
YY r(β + 1)
<
2	b!	X 2-k-1 (k + d)!
waq2a ∙ ∏bd! ∙ X2	QbT
(since Γ(βi + 1) < Γ(βi + 1)= βi!).
28
Under review as a conference paper at ICLR 2020
The summation over k can be bounded by
XX 2-1 (kk±d!	= 1 b!(2d +	(	d ))	≤ 2 (b!2d +	2b)	≤ b!2d,
k = 0 Ik + b)	2	∖	∖	2 2	2
where the first equality used the identity P∞=1 2-k ( d +k ) = 2d. Letting l := ∣ɑ∣, we can
continue by
2	,	2
ra < Waq2α
b!田2d = 2-i-1	l!	2 Qd=U(% + 1)	(b!)22d
πbd! '	Qd=I ai∖ Vd ∙ (2l + d) ∙ Γ(l + b) πbd!
≤
2-l+dπ-2b
l!(b!)3
d!(l + b - 1)!(2l + d)
(since Γ(α⅛ + 2) < Γ(0⅛ + 1)= 0⅛!)
≤ 2-l+b-1π-2b ( l + b ) -1	(since * ≤ 2-b).
This bound depends on a, not directly on a. Letting nι
l + d - 1
l
and Nl = P匕0 nι
dd LL)
it follows that
lnι =
ι=0
l=1
¾f = (d +D L
ι=1
=(d +1) XX (
ι=1
(l + d)!
(d +1)!(l - 1)!
l+d	L+d+ 1
d +1	=(d +1) d + 2
L
L
Now we can bound Xnl by
≤
Nl
∏ λi ≤
i=1
L
JLf (2-l+b-1π-2b (l + b
l=0
nι
⇒	log XNl ≤
N-IE nι(
l0
l=0
-(l - b + 1)log2 - 2blogπ - log ( l + b
≤
L
-N-1 ∙ log 2 ∙ E lnι	(since log 2 < 2 log π as the coefficients of b)
—
—
l=0
(d + L +1 )T log2 ∙ (d +1) (d + L +1
d⅛ L log2
⇒	Xnl ≤
-L log 2
2-L.

This means that the eigenvalue Xi ≤ ε provided that i ≥ NL where L =「log2 ɪ]. Since NL ≤ dL+1,
that means it suffices to choose i such that
i ≥ ddlθg2 1 e+1.
This is a quasi-polynomial bound. It seems tight because even in Gaussian RBF kernel, the eigenvalues
follow the order of Xa = O(c-∣ɑ∣) for some c > 1 (Fasshauer & McCourt, 2012, p.A742).
29
Under review as a conference paper at ICLR 2020
Figure 7: Comparison of efficiency in enforcing Lipschitz constant by various methods
Experiments
A.9 Efficiency of enforcing Lipschitz constant by different methods
The six different ways to train SVMs with Lipschitz regularisation are summarized in Algorithm 1.
Figure 7 plots how fast the regularisation on gradient norm becomes effective when more and more
points w are added to the constraint set. We call them “samples” although it is not so random in the
greedy method, modulo the random initialization of BFGS within the greedy method. The horizontal
axis is the loop index i in Algorithm 1, and the vertical axis is L(i) therein, which is the estimation of
the Lipschitz constant of the current solution f(i) . We used 400 random examples (200 images of
digit 1 and 200 images of digit 0) in the MNIST dataset and set L = 3 and RKHS norm kf kH ≤ ∞
for all algorithms. Inverse kernel is used, hence no results are shown for coordinate-wise Nystrom.
Clearly the Nystrom algorithm is more efficient than the Brute-force algorithm, and the greedy
method significantly reduces the number of samples for both algorithms. In fact, Nystrom with
greedy selection eventually fell below the prespecified L, because of the gap in (9).
A.10 Extension to '∞-norm attacks FOR OUR kernel based method
We now extend our kernel based approach to '∞ norm ball attacks. Since most multiclass losses are
1-Lipschitz continuous with respect to '∞ norm on (f 1(x),..., f 10(x)), we will seek
sup sup	Il [g1(x),..., g10(x)]>u^	≤ L, where gc(x) := Vfc(x).
x∈X u: kuk∞ ≤1	∞
The left-hand side (LHs) can be bounded by
LHs = sup max kgc(x)k1 ≤
x∈X 1≤c≤10
max sup
1≤c≤10 kψkH≤1
I"I∣1∙
Given the Nystrom approximation Gc of Gc, we can enforce the convex constraint of
max SUp ∣∣G>v∣∣ ≤ L.
1≤c≤10 kvk2≤1 I	I1
30
Under review as a conference paper at ICLR 2020
A.11 More results on Cross-Entropy attacks
8°6°4°2°
() Aoalnooe31
O Gauss-Lip (PGD-crossent)
P Par-MaxMin (PGD-crossent)
A Inverse-LiP (PGD-crossent)
X Par-ReLU (PGD-crossent)
O GaUss-LiP (PGD-Crossent)~I
* Par-MaXMin (PGD-crossent)
-A-Inverse-Lip (PGD-crossent)
X Par-ReLU(PGD-CrIossent) ∣
8°6°4°2°
(<⅛) Aoalnooe31
O Gauss-Lip (PGD-crossent)
* Par-MaXMin (PGD-crossent)
a Inverse-LiP (PGD-crossent)
X Par-ReLU (PGD-Crossent)
£2 ball radius δ
b ball radius δ
为 ball radius δ
⑶ MNIST
(b) FaShion-MNIST
(C) CIFAR10
(<⅛) Aoalnooes31
1	2	3
*
1	2	3	4	5	6
Figure 8: Test accuracy under PGD attacks on cross-entropy approximation with '2 norm bound
∏J 60
i40
W 20
O Gauss-Lip (PGD-Crossent)
* Par-MaXMin (PGD-Crossent)
A Inverse-LiP (PGD-crossent)
X Par-ReLU (PGD-Crossent)
0	0.1	0.2	0.3	0.4	0.5	0.6
200 ball radius δ
(a) MNIST
O Gauss-Lip (PGD-Crossent)
⅜ Par-MaxMin (PGD-Crossent)
A Inverse-LiP (PGD-Crossent)
P Par-ReLU (PGD-Crossent)
0	0.1	0.2	0.3	0.4	0.5	0.6
Q1 ball radius δ
(b) Fashion-MNIST
8°0°4°2°
() Aoalnooe31
O GaUss-LiP (PGD-crossent)
* Par-MaXMin (PGD-crossent)
A Inverse-LiP (PGD-crossent)
X Par-ReLU (PGD-crossent)
0	0.1	0.2	0.3	0.4	0.5	0.6
& ball radius δ
(c) CIFAR10
A.12 Visualization of gradient
Figure 10: Gradients of targeted cross-entropy loss with respect to input images. One image per class
(0-9) was sampled randomly from the test set, shown in the first row. A black pixel is encoded by 0,
and a white pixel by 1. The 10 rows below show the gradient for different class targets. For example,
row 7 column 0 shows the gradient of f7 evaluated at the image of digit 0 shown at the top row. Red
and blue stand for positive and negative pixel values, respectively.
Figure 9: Test accuracy under PGD attacks on cross-entropy approximation with '∞ norm bound
(*) Aoalnooes31
A gradient-based attacker tries to decrease the targeted loss by following the negative gradient in
Figure 10b, i.e., reduce the pixel value in red area and increase pixel value in blue area.
In order to verify that the robustness of Gauss-Lip is not due to obfuscated gradient, we visualised
“large perturbation” adversarial examples, with the `2 norm upper bounded by 6. Figure 11 shows
how the PGD attacker uses the gradients to perturb the images step by step. At the end of PGD, there
are 46 cases where the original image was successfully attacked, i.e., turned into the target class. This
is over 50% of the total of 90 cases, and the resulting images look realistic.
31
4 4 4 4 4 4 4 4 4 4
3333 333333
么么么 么么么么么么q
// ////////
。Ooooooooo
7 g q
7 8 4
7 S q
7 θ q
7 8 4
7 8 4
7 S夕
7 8 4
g q
7 q
7 β
夕与夕与4 4夕与与夕
0 β β β β 0 B 0 β B
77,>77,7 77
LLbLbLL L」」
SSSSSS SSSS
4 4 4 4 4 4 4 4 4 4
3332333333
2 2 ¾ 么么么AAgq
// ////////
。Ooooooooo
O
3332 333333
么27 2么么292q
4
4
4
4
4
4
4
4
4
4

O
ɜ
3
3
3 3 3 3 3 3
4
4
4
4
4
ʃ
4
4
4
4
Qu Ωu QP Q3 QP flu flu QP
77777777 77
44 4 44 一夕夕444
O
二夕夕 夕夕么2 7 Ooq
/ / Z7∕r4 7p7

4
4
4
4
4

iteration 1
iteration 2
iteration 3
iteration 4
iteration 5
0∕234Sb7g"0∕234S67g 与
。/夕34SL7gq■。/224Sb79q・。/234Sb7gq
	±_	ɪ	ɪ	ɪ	ɪ		ɪ	ɪ	里			±_	ɪ	鼻	2_	5			ɪ				Ml	ɪ	&	上	ɪ	生	Z	ɪ	里			±	旦	ɪ	ɪ	ɪ	包	ɪ	£				±	直	鼻	ɪ	ɪ	巨		⅜	4
O		ɪ	ɪ	ɪ	ɪ		ɪ	ɪ	ɪ		ɪ		二	3	彳		§	ɪ	ɪ	Q		0.			ɪ	ɪ	ɪ	V	ɪ	ɪ	里		由			刍	ɪ	ɪ	三	ɪ	ɪ	飞		色		汽	3	ɪ	ɪ			ɪ	
ɪ	T		ɪ	A	ɪ	¥	?	ɪ	至		工	T			乩		~		:尸			@	ɪ		¾	ɪ	ɪ	ɪ	Λ	ɪ	至		三	ɪ		-1	W	ɪ	ɪ		ɪ	ɪ		工	ɪ		ʌ	->	ɪ	ɪ	信、	ɪ	_
ɪ	ɪ	ɪ		ɪ	ɪ	⅛	ɪ	豆	q		ɪ	ɪ	ɪ		:j	g»	ɪ	ɪ	T			0.	ɪ	ɪ		ɪ	ɪ	互	T	ɪ	±		ɪ	ɪ	ɪ		* .	ɪ	b	P	ɪ	q		Q		ɪ		1	ɪ	ɪ		ɪ	_
0.	ɪ	£	亘		ɪ	±_	ɪ	A	ɪ			七	「	亘		写*			ɪ			0.	立	ɪ	豆		ɪ		7	ɪ			@	ɪ	六	■、		ɪ	ɪ	ɪ	ɪ	至		Q	y	2_	ZI		ɪ	，多		旻	_ --
0	三	ɪ	ɪ			ɪ	Z	ɪ	g		-3	了		且	ɪ		切	7	上	ɪ				ɪ	21	ɪ		互	ɪ	ɪ	ɪ				T	Zl	ɪ		&	ɪ	T	ɪ		鱼		«	C	Ψ		¥	ɪ		Z
至	ɪ		ɪ	ɪ	ɪ		ɪ	ɪ	ɪ		1	Z	U	3	L				ɪ	⅛		互	ɪ	旦	ɪ	A	ɪ		ɪ	ɪ	至		ɪ		ɪ		ɪ	ɪ		ɪ	ɪ	ɪ		Q		ɪ	3	T	ɪ			ɪ	
ɪ	T	¥	富	⅞	ɪ	⅛		ɪ	&?		Q			呷 *	ɪ		学		ɪ	曳		0.	7	■	ɪ	曳	直	ɪ		ɪ	曳		鱼	±_	置	=;	ɪ	邕			ɪ	曳		Q	7	.=	2.	号	ɪ	¥		T	_
鱼	ɪ	ɪ	ɪ	曼	ɪ		ɪ				上	■一	TT	ɪ	:	厂产	三	^F		ɪ		ɪ	ɪ	⅝	ɪ	上	ɪ	*:：	ɪ		里			T	Z	ɪ	'C	ɪ	ɪ	ɪ		ʃ		星	Z		三	ɪ	ɪ	ɪ	歹		
	ɪ												*		歹								ɪ	三		ɪ								ɪ	~				'*						I						ɪ	ɪ	_
0
0
SSSSSS SSSS
44IA3 夕 4ft-*⅜Jq
3332 9333¾3
么Aq 夕Q久¾7 g q
/ / Z ? /Γ4 7 2 /
。Ooooooooo
与夕夕夕夕 4夕44夕
00 -r¾- ζJ3
5^
4
4
4
4
i
x
B
333Ct g337¾3
CCtA Aawi70Oq
/ Wr ZyjS-U47g?

4夕夕々夕“夕6彳3
2Oga3956g ð
7?722?77 尸 >
LL62L√b boo 切-
SSs写55 Ssss
4 4 4X35Γ6p-⅜rq
3332q3372 3
223 %“GG7gq
/ Ur Z3，Γ47gy
。Ooooeoooo
2女q
2 3 3 <一

4 ΛM H λ ʒ


o
40dx35∙4∙7f q
3332ms378¾
Aqq 2qGG7gq
/0 23≠Γ47gr
β ⅛
o q
g4
a λ
3 q
冬“
s q
6 6
S q
g
g
iteration 6
iteration 7
iteration 8
iteration 9
iteration 10
ɪ	~r	ɪ	ɪ	~4~	ɪ	ɪ	ɪ	ɪ	ɪ
	ɪ	ɪ	ɪ	ɪ	ɪ	~6~	ɪ	ɪ	~9~
ɪ		Tl	~3~		~Γ	~6\	ɪ	I-	~9~
~oη	r		ɪ	丁	~Γ		丁	丁	~27
ɪ	ɪ	ɪ		ɪ	F	~6~	ɪ	ɪ	~9~
ɪ	ɪ	ɪ	~4~		ɪ	ɪ	ɪ	丁	~4~
ɪ	ɪ	ɪ	~τ~	I-		~6~	~7~	ɪ	~9~
ɪ	~6~	~6~	ɪ	~6~	^T^		ɪ	ɪ	~6~
ɪ	ɪ	~7~	ɪ	~7~	ɪ	ɪ		ɪ	~9~
ɪ	ɪ	ɪ	ɪ	ɪ	ɪ	ɪ	ɪ		ɪ
~oη	-8-	~9^1	~3~	~9^1	ɪ	~9^1	T	ʃ	
prediction of Gauss-Lip
Figure 11: Gradients and perturbed images at each iteration in a 10-step PGD attack using (targeted) cross-entropy approximation, with the `2
norm upper bounded by 6. Here the classifier is Gauss-Lip (σ = 2). The table in the bottom right presents the final predictions of our trained
Gauss-Lip on the perturbed images.
Under review as a conference paper at ICLR 2020
qo 守j3“；67£
O	ɪ	2	3	4	5	6	ɪ	8	9
	ɪ	~oη	^0^	~oη	^0^	~6∏	T	~oη	ɪ
ɪ		ɪ	ɪ	ɪ	ɪ	ɪ	ɪ	ɪ	ɪ
ɪ	ɪ		ɪ	ɪ	I-	ɪ	ɪ	ɪ	ɪ
ɪ	ɪ	ɪ		ɪ	ɪ	ɪ	ɪ	ɪ	ɪ
ɪ	ɪ	~4~	ɪ		ɪ	~4~	ɪ	~4~	ɪ
ɪ	ɪ	~τ~	ɪ	I-		ɪ	~7~		ɪ
~6~	~6~	~6~	ɪ	ɪ	^τ^		ɪ	~6~	ɪ
ɪ	ɪ	~7~	ɪ	ɪ	ɪ	ɪ		ɪ	ɪ
ɪ	ɪ	ɪ	ɪ	ɪ	ɪ	ɪ	ɪ		~8~
~oη	-9-	~9\	-9-	~9∏	ɪ	~9∏	-9-	~9~\	
g
9 5 6
//-■?
7
夕g

7
V
9
13:
4o4JLcβ5^4∙7⅛0q
30-32Jr 5Q78q
么。以5r/σ4700q
/023≠r47g 夕
。C0 书 Q 5 G g a C
Figure 12: Left: perturbed images at the end of 100-step PGD attack using (targeted) cross-entropy
approximation. Right: classification on the perturbed image given by the trained Gauss-Lip. They
are quite consistent with human’s perception on the left images.
Figure 13: Perturbed images at the end of 100-step PGD attack using (untargeted) C&W approxi-
mation. The 11 rows show the images after 0, 10, 20, ..., 100 steps of PGD.
To further look into the attack result, we increased PGD to 100 iterations. As shown in Figure 12,
now the number of misclassified cases (i.e., unsuccessful attacks that failed to turn an image into the
targeted class) drops from 46 to 22, out of 90 cases. The final images are quite realistic. We will
further study these remaining cases in the future.
In the above experiments for Figures 11, 12, and 14, PGD was run on the cross-entropy objective.
For example, the row corresponding to class 4 tries to promote the likelihood of the target class 4.
Naturally the diagonal is not meaningful, hence left empty.
We further ran PGD for 100 iterations on C&W approximation (an untargeted attack used in Figure
3), and the resulting images after every 10 iterations are shown in Figure 13. Here 9 out of 10 images
were eventually turned into a different but untargeted class, and the final images are very realistic.
Another random set of images. To test if the above result is due to the particularly hard images
selected, we randomly selected another set of images and its results for 100-step PGD on cross-
entropy objective and C&W objective are shown in Figures 14 and 15, respectively. Interestingly,
C&W attack succeeds on all these images, and cross-entropy attack was only unsuccessful in turning
0 into 1.
33
Under review as a conference paper at ICLR 2020
ɪ	~Γ	ɪ	ɪ	ɪ
	ɪ	ɪ	ɪ	ɪ
ɪ		ɪ	~Γ	ɪ
ɪ	ɪ		ɪ	ɪ
ɪ	ɪ	ɪ		ɪ
~4~	ɪ	ɪ	ɪ	
~5~	ɪ	ɪ	ɪ	ɪ
6	6	6	6	6
ɪ	ɪ		ɪ	~T
ɪ	ɪ	ɪ	ɪ	ɪ
ɪ	ɪ	ɪ	ɪ	ɪ
ɪ	ɪ	ɪ	ɪ	ɪ
ɪ	ɪ	ɪ	ɪ	ɪ
ɪ	ɪ	ɪ	~Γ	ɪ
^2^	^2^	ɪ	~	^2^
ɪ	ɪ	丁	ɪ	ɪ
ɪ	ɪ	ɪ	~4~	ɪ
	ɪ	ɪ	ɪ	ɪ
6		6	6	6
ɪ	~T		ɪ	~T~
ɪ	ɪ	ɪ		ɪ
ɪ	ɪ	ɪ	ɪ	
Figure 14: (Another random trial) Left: perturbed images at the end of 100-step PGD attack using
(targeted) cross-entropy approximation. Right: classification on the perturbed image given by the
trained Gauss-Lip. They are quite consistent with human’s perception on the left images.
(untargeted) C&W approximation. The 11 rows show the images after 0, 10, 20, ..., 100 steps of
PGD.
Please note that despite the commonality in using the cross-entropy objective, the setting of targeted
attack in Figures 11, 12, and 14 is not comparable to that in Figure 8a, where to enable a batch test
mode, an untargeted attacker was employed by increasing the cross-entropy loss of the correct class,
i.e., decreasing the likelihood of the correct class. This is a common practice.
34