Under review as a conference paper at ICLR 2020
CurricularFace: Adaptive Curriculum Learn-
ing Loss for Deep Face Recognition
Anonymous authors
Paper under double-blind review
Ab stract
As an emerging topic in face recognition, designing margin-based loss functions
can increase the feature margin between different classes for enhanced discrim-
inability. More recently, absorbing the idea of mining-based strategies is adopted
to emphasize the misclassified samples and achieve promising results. However,
during the entire training process, the prior methods either do not explicitly em-
phasize the sample based on its importance that renders the hard samples not fully
exploited; or explicitly emphasize the effects of semi-hard/hard samples even at
the early training stage that may lead to convergence issue. In this work, we pro-
pose a novel Adaptive Curriculum Learning loss (CurricularFace) that embeds the
idea of curriculum learning into the loss function to achieve a novel training strat-
egy for deep face recognition, which mainly addresses easy samples in the early
training stage and hard ones in the later stage. Specifically, our CurricularFace
adaptively adjusts the relative importance of easy and hard samples during dif-
ferent training stages. In each stage, different samples are assigned with different
importance according to their corresponding difficultness. Extensive experimental
results on popular benchmarks demonstrate the superiority of our CurricularFace
over the state-of-the-art competitors. Code will be available upon publication.
Introduction
The success of Convolutional Neural Networks (CNNs) on face recognition can be mainly credited
to : enormous training data, network architectures, and loss functions. Recently, designing appro-
priate loss functions that enhance discriminative power is pivotal for training deep face CNNs.
Current state-of-the-art face recognition methods mainly adopt softmax-based classification loss.
Since the learned features with the original softmax is not discriminative enough for the open-set face
recognition problem, several margin-based variants have been proposed to enhance features’ dis-
criminative power. For example, explicit margin, i.e., CosFace (Wang et al., 2018a), Sphereface (Li
et al., 2017), ArcFace (Deng et al., 2019), and implicit margin, i.e., Adacos (Zhang et al., 2019a),
supplement the original softmax function to enforce greater intra-class compactness and inter-class
discrepancy, which are shown to result in more discriminate features. However, these margin-based
loss functions do not explicitly emphasize each sample according to its importance.
As demonstrated in Chen et al. (2019), hard sample mining is also a critical step to further improve
the final accuracy. Recently, Triplet loss (Schroff et al., 2015) and SV-Arc-Softmax (Wang et al.,
2018b) integrate the motivations of both margin and mining into one framework for deep face recog-
nition. Triplet loss adopts a semi-hard mining strategy to obtain semi-hard triplets and enlarge the
margin between triplet samples. SV-Arc-Softmax (Wang et al., 2018b) clearly defines hard samples
as misclassified samples and emphasizes them by increasing the weights of their negative cosine
similarities with a preset constant. In a nutshell, mining-based loss functions explicitly emphasize
the effects of semi-hard or hard samples.
However, there are drawbacks in training strategies of both margin- and mining-based loss functions.
For margin-based methods, mining strategy is ignored and thus the difficultness of each sample is
not fully exploited, which may lead to convergence issues when using a large margin on small
backbones, e.g., MobileFaceNet (Chen et al., 2018). As shown in Fig. 1, the modulation coefficient
for the negative cosine similarities I(∙) is fixed as a constant 1 in ArcFace for all samples during the
entire training process. For mining-based methods, over-emphasizing hard samples in early training
1
Under review as a conference paper at ICLR 2020
Early Stage
Harder	d
----ArcFace
----SV-Arc-Softmax(t=1.2)
----Ours(t=1.0)
----Ours(t=0.0)
Figure 1: Different training strategies for modulating negative cosine similarities of hard samples (i.e., the
mis-classified sample) in ArcFace, SV-Arc-Softmax and our CurricularFace. Left: The modulation coefficients
I (t, cos θj ) for negative cosine similarities of hard samples in different methods, where t is an adaptively
estimated parameter and θj denotes the angle between the hard sample and the non-ground truth j-class center.
Right: The corresponding hard samples’ negative cosine similarities N(t, cos θj) = I(t, cos θj) cos θj + c
after modulation, where c indicates a constant. On one hand, during early training stage (e.g., t is close to 0),
hard sample’s negative cosine similarities is usually reduced and thus leads to smaller hard sample loss than the
original one. Therefore, easier samples are relatively emphasized; during later training stage (e.g., t is close to
1), the hard sample’s negative cosine similarities are enhanced and thus leads to larger hard sample loss. On the
other hand, in the same training stage, we modulate the hard samples’ negative cosine similarities with cos θj .
Specifically, the smaller the angle θj is, the larger the modulation coefficient should be.
stage may hinder the model to converge. As SV-Arc-Softmax claimed, the manually defined constant
t plays a key role in the model convergence property and a slight larger value (e.g., >1.4) may cause
the model difficult to converge. Thus t needs to be carefully tuned.
In this work, we propose a novel adaptive curriculum learning loss, termed CurricularFace, to
achieve a novel training strategy for deep face recognition. Motivated by the nature of human
learning that easy cases are learned first and then come the hard ones (Bengio et al., 2009), our Cur-
ricularFace incorporates the idea of Curriculum Learning (CL) into face recognition in an adaptive
manner, which differs from the traditional CL in two aspects. First, the curriculum construction is
adaptive. In traditional CL, the samples are ordered by the corresponding difficultness, which are
often defined by a prior and then fixed to establish the curriculum. In CurricularFace, the samples
are randomly selected in each mini-batch, while the curriculum is established adaptively via mining
the hard samples online, which shows the diversity in samples with different importance. Second,
the importance of hard samples are adaptive. On one hand, the relative importance between easy
and hard samples is dynamic and could be adjusted in different training stages. On the other hand,
the importance of each hard sample in current mini-batch depends on its own difficultness.
Specifically, the mis-classified samples in mini-batch are chosen as hard samples and weighted by
adjusting the modulation coefficients I(t, cosθj ) of cosine similarities between the sample and the
non-ground truth class center vectors, i.e., negative cosine similarity N(t, cosθj). To achieve the
goal of adaptive curricular learning in the entire training, we design a novel coefficient function
I(∙) that is determined by two factors: 1) the adaptively estimated parameter t that utilizes moving
average of positive cosine similarities between samples and the corresponding ground-truth class
center to unleash the burden of manually tuning; and 2) the angle θj that defines the difficultness of
hard samples to achieve adaptive assignment. To sum up, the contributions of this work are:
•	We propose an adaptive curriculum learning loss for face recognition, which automatically
emphasizes easy samples first and hard samples later. To the best of our knowledge, it is
the first work to introduce the idea of adaptive curriculum learning for face recognition.
•	We design a novel modulation coefficient function I(∙) to achieve adaptive curriculum
learning during training, which connects positive and negative cosine similarity simulta-
neously without the need of manually tuning any additional hyper-parameter.
•	We conduct extensive experiments on popular facial benchmarks, which demonstrate the
superiority of our CurricularFace over the state-of-the-art competitors.
2
Under review as a conference paper at ICLR 2020
Related Work
Margin-based loss function Loss design is pivotal for large-scale face recognition. Current state-
of-the-art deep face recognition methods mostly adopt softmax-based classification loss. Since the
learned features with the original softmax loss are not guaranteed to be discriminative enough for
open-set face recognition problem, margin-based losses (Liu et al., 2016; Li et al., 2017; Deng et al.,
2019) are proposed. Though the margin-based loss functions are verified to obtain good perfor-
mance, they do not take the difficultness of each sample into consideration, while our CurricularFace
emphasizes easy samples first and hard samples later, which is more reasonable and effectiveness.
Mining-based loss function Though some mining-based loss function such as Focal loss (Lin
et al., 2017), Online Hard Sample Mining (OHEM) (Shrivastava et al., 2016) are prevalent in the field
of object detection, they are rarely used in face recognition. OHEM focuses on the large-loss samples
in one mini-batch, in which the percentage of the hard samples is empirically determined and easy
samples are completely discarded. Focal loss is a soft mining variant that rectifies the loss function
to an elaborately designed form, where two hyper-parameters should be tuned with a lot of efforts
to decide the weights of each samples and hard samples are emphasized by reducing the weight
of easy samples. The recent work, SV-Arc-Softmax (Wang et al., 2018b) fuses the motivations of
both margin and mining into one framework for deep face recognition. They define hard samples as
misclassified samples and enlarge the weight of hard samples with a preset constant. Our method
differs from SV-Arc-Softmax in three aspects: 1) We do not always emphasize the hard samples,
especially in the early training stages. 2) We assign different weights for hard samples according to
their corresponding difficultness. 3) There’s no need in our method to manually tune the additional
hyper-parameter t, which is estimated adaptively.
Curriculum Learning Learning from easier samples first and harder samples later is a common
strategy in Curriculum Learning (CL) (Bengio et al., 2009), (Zhou & Bilmes, 2018). The key prob-
lem in CL is to define the difficultness of each sample. For example, Basu & Christensen (2013)
takes the negative distance to the boundary as the indicator for easiness in classification. However,
the ad-hoc curriculum design in CL turns out to be difficult to implement in different problems.
To alleviate this issue, Kumar et al. (2010) designs a new formulation, called Self-Paced Learn-
ing (SPL), where examples with lower losses are considered to be easier and emphasized during
training. The key differences between our CurricularFace with SPL are: 1) Our method focuses on
easier samples in the early training stage and emphasizes hard samples in the later training stage.
2) Our method proposes a novel modulation function N(∙) for negative cosine similarities, which
achieves not only adaptive assignment on modulation coefficients I(∙) for different samples in the
same training stage, but also adaptive curriculum learning strategy in different training stages.
The Proposed CurricularFace
Preliminary Knowledge on Loss Function
The original softmax loss is formulated as follows:
eWyi xi +byi
L = - log Pn IeWj χi+bj ,	⑴
where xi ∈ Rd denotes the deep feature of i-th sample which belongs to the yi class, Wj ∈ Rd
denotes the j-th column of the weight W ∈ Rd×n and bj is the bias term. The class number and the
embedding feature size are n and d, respectively. In practice, the bias is usually set to bj = 0 and
the individual weight is set to ||Wj || = 1 by l2 normalization. The deep feature is also normalized
and re-scaled to s. Thus, the original softmax can be modified as follows:
es(cos θyi)
L = — log —7—--ʌ------------------------.	(2)
Ses(Cos θyi) + Pn=ij=y es(cosθj)	J
Since the learned features with original softmax loss may not be discriminative enough for open-set
face recognition problem, several variants are proposed and can be formulated in a general form:
esT (cos θyi)
L = -G(P(Xi))log-------------------------诋-----(3)
esT (cos θyi ) + Pn=1,j=yi eSN(t，COS θ)
3
Under review as a conference paper at ICLR 2020
sT (cos θyi )
where P(Xi) = -------------------e--------i …t Cos § .)is the predicted ground truth probability and
esT (cos θyi)+ jn=1,j6=yi e	, j
G(p(xi)) is an indicator function. T (cos θyi) and N(t, cos θj) = I(t, cos θj) cos θj +c are the func-
tions to modulate the positive and negative cosine similarities, respectively, where c is a constant,
and I (t, cos θj ) denotes the modulation coefficients of negative cosine similarities. In margin-based
loss function, e.g, ArcFace, G(p(xi)) = 1, T (cos θyi) = cos(θyi + m), and N(t, cos θj) = cos θj.
It only modifies the positive cosine similarity of each sample to enhance the feature discrimination.
As shown in Fig. 1, the modulation coefficients of each sample, negative cosine similarity I(∙) is
fixed as 1. The recent work, SV-Arc-Softmax emphasizes hard samples by increasing I(t, cos θj)
for hard samples. That is, G(p(xi)) = 1 and N(t, cosθj ) is formulated as follows:
cos θj ,
N(t, cosθ ) =
j t cos θj + t - 1
T(cosθyi) - cos θj ≥ 0
T(cosθyi) - cos θj < 0.
(4)
If a sample is defined to be easy, its negative cosine similarity is kept the same as the original one,
cos θj; ifas a hard sample, its negative cosine similarity becomes t cos θj + t - 1. That is, as shown
in Fig. 1, I(∙) is a constant and determined by a preset hyper-parameter t. Meanwhile, since t is
always larger than 1, t cos θj + t - 1 > cos θj always holds true, which means the model always
focuses on hard samples, even in the early training stage. However, the parameter t is sensitive that
a large pre-defined value (e.g., > 1.4) may lead to convergence issue.
Adaptive Curricular Learning Loss
Next, we present the details of our proposed adaptive curriculum learning loss, which is the first
attempt to introduce adaptive curriculum learning into deep face recognition. The formulation of
our loss function is also contained in the general form, where G(p(xi)) = 1, positive and negative
cosine similarity functions are defined as follows:
T(cosθyi) = cos(θyi +m),	(5)
cos θj ,
N(t,cosθj)= cosθj(t+cosθj),
T(cosθyi) - cos θj ≥ 0
T(cosθyi) - cos θj < 0.
(6)
It should be noted that the positive cosine similarity can adopt any margin-based loss functions and
here we adopt ArcFace as the example. As shown in Fig. 1, the modulation coefficient of hard sam-
ple negative cosine similarity I(t, θj ) depends on both the value of t and θj . In the early training
stage, learning from easy samples is beneficial to model convergence. Thus, t should be close to zero
and I(∙) is smaller than 1. Therefore, the weights of hard samples are reduced and the easy samples
are emphasized relatively. As training goes on, the model gradually focuses on the hard samples,
i.e., the value of t shall increase and I(∙) is larger than 1. Then, the weights of hard samples are
enlarged, which are thus emphasized. Moreover, within the same training stage, I(∙) is monoton-
ically decreasing with θj so that harder sample can be assigned with larger coefficient according
to its difficultness. The value of the parameter t is automatically estimated in our CurricularFace,
otherwise it would require a lot of efforts for manually tuning.
Adaptive estimation of t It is critical to determine appropriate values of t in different training
stages. Ideally the value oft can indicate the model training process. We empirically find the average
of positive cosine similarities is a good indicator. However, mini-batch statistic-based methods
usually face an issue: when many extreme data are sampled in one mini-batch, the statistics can be
vastly noisy and the estimation will be unstable. Exponential Moving Average (EMA) is a common
solution to address this issue (Li et al., 2019). Specifically, let r(k) be the average of the positive
cosine similarities of the k-th batch and be formulated as r(k) = Pi cos θyi , we have:
t(k) = αr(k) + (1 - α)t(k-1),	(7)
where t0 = 0, α is the momentum parameter and set to 0.99. As shown in Fig. 2, the parameter t
increases with the model training, thus the gradient modulation coefficients’ range of hard sample,
M(∙) = 2 cos θj + t, also increases. Therefore, hard samples are emphasized gradually. With the
EMA, we avoid the hyper-parameter tuning and make the modulation coefficients of hard sample
4
Under review as a conference paper at ICLR 2020
Algorithm 1: CurricularFace
Input: The deep feature of i-th sample xi with its corresponding label yi , last fully-connected layer
parameters W , cosine similarity cos θj between two vectors, embedding network parameters Θ,
learning rate λ, number of iteration k, parameter t, and margin m
k 4- 0,14- 0, ^m 4- 0.5;
while not converged do
k 4 k+ 1;
if cos(θyi + m) > cos θj then
I N (t, cos θj) = cos θj;
else
I N(t, cos θj) = (t(k) + cos θj) cos θj ;
end
T (cos θyi) = cos(θyi + m);
Compute the loss L by Eq. 8;
Compute the back-propagation error of xi and Wj by Eq. 9;
Update the parameters W and Θ by: W(k+1) = W(k) 一 λ(k) ∂∂L, Θ(k+1) = Θ(k) 一 λ(k) ILdl⅝∙；
Update the parameter t by Eq. 7;
end
Output: W, Θ
negative cosine similarities I(∙) adaptive to the current training stage. To sum up, the loss function
of our CurricularFace is formulated as follows:
L = - log
es cos(θyi +m)
e$ cos(θyi +m) + Pn ］竽	e§N(t(k) ,cos θj )
(8)
where N (t(k), cos θj) is defined in Eq. 6. The entire training process is summarized in Algorithm 1.
Fig. 3 illustrates how the loss changes from Arc-
Face to our CurricularFace during training. Here are
some observations: 1) As we excepted, hard samples
are suppressed in early training stage but emphasized
later. 2) The ratio is monotonically increasing with
cosθj, since the larger cosθj is, the harder the sample
is. 3) The positive cosine similarity of a perceptual-
well image is often large. However, during the early
training stage, the negative cosine similarities of the
perceptual-well image may also be large so that it
could be classified as the hard one.
Optimization Next, we show our CurricularFace
can be easily optimized by the conventional stochas-
tic gradient descent. Assuming xi denotes the deep
feature of i-th sample which belongs to the yi class,
the input of the proposed function is the logit fj ,
where j denotes the j-th class.
Iterations
Figure 2: Illustrations on the adaptive pa-
rameter t (red line) and gradient modulation
coefficients M(∙) = 2 cos θj + t of hard Sam-
ples (green area). Since the number of mined
hard samples reduces with the model training,
the green area M(∙) is relatively smooth in early
stage and there are some burrs in later stage.
In the forwarding process, when j = yi, it is the same as the ArcFace, i.e., fj = sT (cos θyi ),
T (cos θyi ) = cos(θyi +m). When j 6= yi, it has two cases, ifxi is an easy sample, itis the the same
as the original softmax, i.e., fj = s cos θj. Otherwise, it will be modulated as fj = sN (t, cos θj),
where N(t, cos θj) = (t + cos θj) cos θj. In the backward propagation process, the gradient of xi
and Wj can also be divided into three cases and formulated as follows:
∂L
∂Xi
∂L (S sin(θyi +m)
dfyi I -Sin θy
f SWj ,
d∂L s (2 cos θj + t)Wj
j = yi,easy,∂W
j 6= yi , hard
{∂L ( sin(θyi +m)).
f(S	Sin θy%	)xi,
f sxi,
∂∂L s(2cos θj + t)Xi
j = yi
j 6= yi , easy
j 6= yi , hard
(9)
Based on the above formulations, we can find the gradient magnitude of the hard sample is deter-
mined by two parts, the negative cosine similarity N(∙) and the value of t.
5
Under review as a conference paper at ICLR 2020
Figure 3: Illustrations on (ratio between our
loss and ArcFace, maximum cosθj ) from
early (Top) to later (Bottom) training stages.
Table 1: Decision boundaries of popular loss functions.
Loss	Decision Boundary
Softmax	CoS θ,, . = CoS θj 	yi	j	
SphereFace	cos(mθy.) = cos θj
CosFaCe	cos θy . — m = cos θj
ArCFaCe		cos(θy. + m) = CoS θj	
SV-Arc-Softmax	cos(θy. + m) = cos θj (easy) cos(θy. + m) = t cos θj + t - 1 (hard)
CurricularFace (OUrS)	cos(θy. + m) = cos θj (easy) cos(θy. + m) = (t + cos θj ) cos θj (hard)
Table 2: Verification performance of different values of t.
Dataset (%)	t = 0	t = 0.3	t= 0.7	t=1	Adaptive t
LFW	99.32	-9937-	99.42	99.45	99.47
CFP-FP	95.90	96.47	96.66	93.94	96.96
Discussions with SOTA Loss Functions
Comparison with ArcFace and SV-Arc-Softmax We first discuss the difference between our
CurricularFace and the two competitors, ArcFace and SV-Arc-Softmax, from the perspective of the
decision boundary in Tab. 1. ArcFace introduces a margin function T (cos θyi) = cos(θyi + m)
from the perspective of positive cosine similarity. As shown in Fig. 4, its decision condition changes
from cos θyi = cos θj (i.e., blue line) to cos(θyi + m) = cos θj (i.e., red line) for each sample.
SV-Arc-Softmax introduces additional margin from the perspective of negative cosine similarity for
hard samples, and the decision boundary becomes cos(θyi + m) = t cos θj + t - 1 (i.e., green
line). Conversely, we adaptively adjust the weights of hard samples in different training stages. The
decision condition becomes cos(θyi +m) = (t+cos θj) cos θj (i.e., purple line). During the training
stage, the decision boundary for hard samples changes from one purple line (early stage) to another
(later stage), which emphasizes easy samples first and hard samples later.
Comparison with Focal loss Focal loss is a soft mining-based loss, which is formulated as:
G(p(x)) = α(1 - p(xi))β, where α and β are modulating factors that need to be tuned manu-
ally. The definition of hard samples in Focal loss is ambiguous, since it always focuses on relatively
hard samples by reducing the weight of easier samples during the entire training process. In con-
trast, the definition of hard samples in our CurricularFace is more clear, i.e., mis-classified samples.
Meanwhile, the weights of hard samples are adaptively determined in different training stages.
Experiments
Implementation Details
Datasets We separately employ CASIA-WebFace (Yi et al., 2014) and refined MS1MV2 (Deng
et al., 2019) as our training data for fair comparisons with other methods. We extensively test our
method on several popular benchmarks, including LFW (Huang et al., 2007), CFP-FP (Sengupta
et al., 2016), CPLFW (Zheng et al., 2018), AgeDB (Moschoglou et al., 2017), CALFW (Zheng et al.,
2017), IJB-B (Whitelam et al., 2017), IJB-C (Maze et al., 2018), and MegaFace (Kemelmacher-
Shlizerman et al., 2016).
Training Setting We follow Deng et al. (2019) to generate the normalised faces (112 × 112) with
five landmarks (Zhang et al., 2016). For the embedding network, we adopt ResNet50 and ResNet100
as in Deng et al. (2019). Our framework is implemented in Pytorch (Paszke et al., 2017). We train
models on 4 NVIDIA Tesla P40 (24GB) GPU with batch size 512. The models are trained with
SGD algorithm, with momentum 0.9 and weight decay 5e - 4. On CASIA-WebFace, the learning
rate starts from 0.1 and is divided by 10 at 28, 38, 46 epochs. The training process is finished at
50 epochs. On MS1MV2, we divide the learning rate at 10, 18, 22 epochs and finish at 24 epochs.
We follow the common setting as Deng et al. (2019) to set scale s = 64 and margin m = 0.5,
respectively. Last but not least, since we only modify the loss function but use the same backbone
as previous methods (e.g., ArcFace), NO additional time complexity is introduced for inference.
6
Under review as a conference paper at ICLR 2020
Figure 4: From left to right, decision boundaries of ArcFace,
SV-Arc-Softmax, and ours. Blue line, red line, green line and
purple line denote the decision boundary of Softmax, ArcFace,
SV-Arc-Softmax, and ours, respectively. m denotes the angular
margin added by ArcFace. d denotes the additional margin of SV-
Arc-Softmax and ours. In SV-Arc-Softmax, d = (t - 1) cos θj +
t - 1. In ours, d = (t + cos θj - 1) cos θj.
ferent strategies for setting t.
Table 3: Verification performance of dif-
Methods (%) LFW CFP-FP
Mode(Cos θyi)	99.42	96.49
Mean(Pxi) ”	99.42	95.39
Mean(Cos。期木)	99.47	96.96
Figure 5: Illustration on convergence is-
sue with small backbone.
Ablation study
Effects on Fixed vs. Adaptive Parameter t We first investigate the effect of adaptive estimation
of t. We choose four fixed values between 0 and 1 for comparison. Specifically, 0 means the
modulation coefficient I(∙) of each hard sample's negative cosine similarity is always reduced based
on its difficultness. In contrast, 1 means the hard samples are always emphasized. 0.3 and 0.7 are
between the two cases. Tab. 2 shows that it is more effective to learn from easier samples first and
hard samples later based on our adaptively estimated parameter t.
Effects on Different Statistics for Estimating t We now investigate the effects of several other
statistics, i.e., mode of positive cosine similarities in a mini-batch, or mean of the predicted ground
truth probability for estimating t in our loss. As Tab. 3 shows, on one hand, the mean of positive
cosine similarities is better than the mode. On the other hand, the positive cosine similarity is more
accurate than the predicted ground truth probability to indicate the training stages.
Robustness on Training Convergence As claimed in Li (2019), ArcFace exists divergence issue
when using small backbones like MobileFaceNet. As the result, softmax loss must be incorporated
for pre-training. To illustrate the robustness of our loss function on convergence issue with small
backbone, we use the MobileFaceNet as the network architecture and train it on CASIA-WebFace.
As shown in Fig. 5, when the margin m is set to 0.5, the model trained with our loss achieves 99.25
accuracy on LFW, while the model trained with ArcFace does not converge and the loss is NAN
at about 2, 400-th step. When the margin m is set to 0.45, both losses can converge, but our loss
achieves better performance (99.20% vs. 99.10%). Comparing the yellow and red curves, since the
losses of hard samples are reduced in early training stages, our loss converges much faster in the
beginning, leading to lower loss than ArcFace. Later on, the value of our loss is slightly larger than
ArcFace, because we emphasize the hard samples in later stages. The results prove that learning
from easy samples first and hard samples later is beneficial to model convergence.
Comparisons with SOTA Methods
Results on LFW, CFP-FP, CPLFW, AgeDB and CALFW Next, we train our CurricularFace on
dataset MS1MV2 with ResNet100, and compare with the SOTA competitors on various benchmarks,
including LFW for unconstrained face verification, CFP-FP and CPLFW for large pose variations,
AgeDB and CALFW for age variations. As reported in Tab. 4, our CurricularFace achieves com-
parable result (i.e., 99.80%) with the competitors on LFW where the performance is near saturated.
While for both CFP-FP and CPLFW, our method shows superiority over the baselines including gen-
eral methods, e.g., (Wen et al., 2016), (Cao et al., 2018b), and cross-pose methods, e.g., (Tran et al.,
2017), (Peng et al., 2017), (Cao et al., 2018a) and (Deng et al., 2018). As a recent face recognition
method, SV-Arc-Softmax achieves better performance than ArcFace, but still worse than Our Cur-
ricularFace. Finally, for AgeDB and CALFW, as Tab. 4 shows, our CurricularFace again achieves
the best performance than all of the other state-of-the-art methods.
7
Under review as a conference paper at ICLR 2020
Table 4: Verification comparison with SOTA methods on vari- Table 5: 1:1 verification TAR (@FAR =
ous Small-SCale benchmarks.______________________________________ 1e - 4) on IJB-B and IJB-C.
Methods (%)	LFW	CFP-FP	CPLFW	AgeDB	CALFW	Methods (%)	IJB-B	IJB-C
Center Loss (ECCV’16)	98.75	-	77.48	-	85.48	SENet50 (FG’18)	80.0	84.1
SphereFace (CVPR’17)	99.27	-	81.40	-	90.30	Multicolumn (BMVC’18)	83.1	86.2
DRGAN (CVPR’17)	-	93.41	-	-	-	DCN (ECCV’18)	84.9	88.5
Peng et al. (ICCV’17) VGGFace2 (FG’18)	- 99.43	93.76 -	- 84.00	- -	- 90.57	ArcFace-R100 (CVPR’19)	94.2	95.6
Dream (CVPR’18)	-	93.98	-	-	-	Adacos (CVPR’19)	-	92.4
Deng et al. (CVPR’18)	99.60	94.05	-	-	-	P2SGrad (CVPR’19)	-	92.3
ArcFace (CVPR’19)	99.77	98.27	92.08	98.15	95.45	PFE (ICCV’19)	-	93.3
SV-Arc-Softmax	99.78	98.28	92.83	97.95	96.10	SV-Arc-Softmax	93.6	95.2
CurricularFace (Ours)	99.80	98.37	93.13	98.32	96.20	CUrricUlarFace (OUrs)	94.8	96.1
Table 6: Verification comparison with SOTA methods on MegaFace Challenge 1 using FaceScrub as the
probe set. Left table: ‘Id’ refers to the rank-1 face identification accuracy with 1M distractors, and ‘Ver’ refers to the face verification TAR at 10-6 FAR. ‘R’ refers to data refinement on both probe set and 1M distractors. Right figure: Rank-1 identification results of recent SOTA methods on probe set refined from ArcFace.
CASIA (%)	Id	Ver	∣∣ MSIMV2 (%)	Id	Ver	.	COsFace (CVPR’18)- 97.91
COntraStiVe Loss (CVPR’14)	65.21	78.86	CosFace-MS1MV2-R100	80.56	96.56 Triplet (CVPR;15)	64.79	78.32	CosFace-MS1MV2-R100, R	97.91	97.91	^^^^^agl(	R
Center Loss (ECCV,16)	65.49	80.14	ArCFaCe-MS1MV2-R100	81.03	96.98	L	p2sGrad (CVPR’19) -9725
SphereFace (CVPR;17)	72.73	85.56	ArCFaCe-MS1MV2-R100, R	98.35	98.48
CosFace (CVRP,18)	77.11	89.88	PFE(ICCV’19)	78.95	92.51
AM-Softmax (SPL,18)	72.47	84.44	Adacos, R(CVPR,19,)	97.41	—
ArcFace-CASIA-R50 (CVPR;19)	77.50	92.34	P2SGrad, R(CVPR,19,)	97.25	-	β	1
ArcFace-CASIA-R50, R	91.75	93.69	SV-Arc-Softmax, R	97.14	97.57	匕	CurrkUlarFace(Ours) -98.71
OUrS-CASIA-R50	77.65	92.91	OUrS-MS1MV2-R100	81.26	97.26
OUrS-CASIA-R50, R	92.48	94.55	OUrS-MS1MV2-R100, R	98.71	98.64
		Ll		 96.5	97	97.5	98	98.5	99
Results on IJB-B and IJB-C The IJB-B dataSet containS 1, 845 SubjectS with 21.8K Still imageS
and 55K frameS from 7, 011 videoS. In the 1:1 verification, there are 10, 270 poSitive matcheS and
8M negative matcheS. The IJB-C dataSet iS a further extenSion of IJB-B, which containS about 3, 500
identitieS with a total of 31, 334 imageS and 117, 542 unconStrained video frameS. In the 1:1 verifi-
cation, there are 19, 557 poSitive matcheS and 15, 638, 932 negative matcheS. On IJB-B and IJB-C
dataSetS, we employ MS1MV2 and the ReSNet100 for a fair compariSon with recent methodS. We
follow the teSting protocol in ArcFace and take the average of the image featureS aS the correSpond-
ing template repreSentation without bellS and whiStleS. Tab. 5 exhibitS the performance of different
methodS, e.g., Multicolumn (Xie & ZiSSerman, 2018), DCN (Xie et al., 2018), AdacoS (Zhang et al.,
2019a), P2SGrad (Zhang et al., 2019b), PFE (Shi et al., 2019) and SV-Arc-Softmax (Wang et al.,
2018b) on IJB-B and IJB-C 1:1 verification, our method again achieveS the beSt performance.
Results on MegaFace Finally, we evaluate the performance on the MegaFace Challenge. The
gallery Set of MegaFace includeS 1M imageS of 690K SubjectS, and the probe Set includeS 100K
photoS of 530 unique individualS from FaceScrub. We report the two teSting reSultS under two
protocolS (large or Small training Set). Here, we uSe CASIA-WebFace and MS1MV2 under the
Small protocol and large protocol, reSpectively. In Tab. 6, our method achieveS the beSt Single-
model identification and verification performance under both protocolS, SurpaSSing the recent Strong
competitorS, e.g., CoSFace, ArcFace, AdacoS, P2SGrad and PFE. We alSo report the reSultS following
the ArcFace teSting protocol, which refineS both the probe Set and the gallery Set. AS Shown from
the figure in Tab. 6, our method Still clearly outperformS the competitorS and achieveS the beSt
performance on both verification and identification.
Conclusions
In thiS paper, we propoSe a novel Adaptive Curriculum Learning LoSS that embedS the idea of adap-
tive curriculum learning into deep face recognition. Our key idea iS to addreSS eaSy SampleS in the
early training Stage and hard oneS in the later Stage. Our method iS eaSy to implement and robuSt to
converge. ExtenSive experimentS on popular facial benchmarkS demonStrate the effectiveneSS of our
method compared to the State-of-the-art competitorS. Following the main idea of thiS work, future
research can be expanded in various aspects, including designing a better function N(∙) for negative
coSine Similarity that ShareS Similar adaptive characteriStic during training, and inveStigating the
effects of noise samples that could be optimized as hard samples.
8
Under review as a conference paper at ICLR 2020
References
Sumit Basu and Janara Christensen. Teaching classification boundaries to humans. In AAAI, 2013.
Yoshua Bengio, Jerome Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In
ICML, 2009.
Kaidi Cao, Yu Rong, Cheng Li, Xiaoou Tang, and Chen Change Loy. Pose-robust face recognition
via deep residual equivariant mapping. In CVPR, 2018a.
Qiong Cao, Li Shen, Weidi Xie, Omkar M. Parkhi, and Andrew Zisserman. Vggface2: A dataset
for recognising faces across pose and age. In FG, 2018b.
Beidi Chen, Weiyang Liu, Animesh Garg, Zhiding Yu, Anshumali Shrivastava, and Anima Anand-
kumar. Angular visual hardness. In ICML Workshop on Deep Phenomena, 2019.
Sheng Chen, Yang Liu, Xiang Gao, and Zhen Han. Mobilefacenets: Efficient cnns for accurate
real-time face verification on mobile devices. In CCBR, 2018.
Jiankang Deng, Shiyang Cheng, Niannan Xue, Yuxiang Zhou, and Stefanos Zafeiriou. Uv-gan:
Adversarial facial uv map completion for pose-invariant face recognition. In CVPR, 2018.
Jiankang Deng, Jia Guo, and Stefanos Zafeiriou. ArcFace: Additive angular margin loss for deep
face recognition. In CVPR, 2019.
Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller. Labeled faces in the wild:
A database for studying face recognition in unconstrained environments. Technical Report 07-49,
University of Massachusetts, Amherst, October 2007.
Ira Kemelmacher-Shlizerman, Steven M Seitz, Daniel Miller, and Evan Brossard. The megaface
benchmark: 1 million faces for recognition at scale. In CVPR, 2016.
M Pawan Kumar, Benjamin Packer, and Daphne Koller. Self-paced learning for latent variable
models. In NIPS, 2010.
Buyu Li, Yu Liu, and Xiaogang Wang. Gradient harmonized single-stage detector. In AAAI, 2019.
Weiyang Li, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, and Le Song. Sphereface: Deep
hypersphere embedding for face recognition. In CVPR, 2017.
Xianyang Li. Airface: Lightweight and efficient model for face recognition. arXiv:1907.12256,
2019.
Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollar. Focal loss for dense object
detection. In ICCV, pp. 2980-2988, 2017.
Weiyang Liu, Yandong Wen, Zhiding Yu, and Meng Yang. Large-margin softmax loss for convolu-
tional neural networks. In ICML, 2016.
Brianna Maze, Jocelyn Adams, James A Duncan, Nathan Kalka, Tim Miller, Charles Otto, Anil K
Jain, W Tyler Niggel, Janet Anderson, Jordan Cheney, et al. Iarpa janus benchmark-c: Face
dataset and protocol. In ICB, 2018.
Stylianos Moschoglou, Athanasios Papaioannou, Christos Sagonas, Jiankang Deng, Irene Kotsia,
and Stefanos Zafeiriou. Agedb: the first manually collected, in-the-wild age database. In CVPR
Workshops, 2017.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
PyTorch. In NIPS Autodiff Workshop, 2017.
Xi Peng, Xiang Yu, Kihyuk Sohn, Dimitris Metaxas, and Manmohan Chandraker. Reconstruction-
based disentanglement for poseinvariant face recognition. In ICCV, 2017.
Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A unified embedding for face
recognition and clustering. In CVPR, 2015.
9
Under review as a conference paper at ICLR 2020
S. Sengupta, J.-C. Chen, C. Castillo, V. M. Patel, R. Chellappa, and D.W. Jacobs. Frontal to profile
face verification in the wild. In WACV, 2016.
Yichun Shi, Anil K Jain, and Nathan D Kalka. Probabilistic face embeddings. In ICCV, 2019.
Abhinav Shrivastava, Abhinav Gupta, and Ross Girshick. Training region-based object detectors
with online hard example mining. In CVPR, 2016.
Luan Tran, Xi Yin, and Xiaoming Liu. Disentangled representation learning GAN for pose-invariant
face recognition. In CVPR, 2017.
Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Dihong Gong, Jingchao Zhou, Zhifeng Li, and Wei
Liu. Cosface: Large margin cosine loss for deep face recognition. In CVPR, 2018a.
Xiaobo Wang, Shuo Wang, Shifeng Zhang, Tianyu Fu, Hailin Shi, and Tao Mei. Support vector
guided softmax loss for face recognition. arXiv:1812.11317, 2018b.
Yandong Wen, Kaipeng Zhang, Zhifeng Li, and Yu Qiao. A discriminative feature learning approach
for deep face recognition. In ECCV, 2016.
Cameron Whitelam, Emma Taborsky, Austin Blanton, Brianna Maze, Jocelyn Adams, Tim Miller,
Nathan Kalka, Anil K Jain, James A Duncan, Kristen Allen, et al. Iarpa janus benchmark-b face
dataset. In CVPR Workshops, 2017.
Weidi Xie and Andrew Zisserman. Multicolumn networks for face recognition. In BMVC, 2018.
Weidi Xie, Li Shen, and Andrew Zisserman. Comparator networks. In ECCV, 2018.
Dong Yi, Zhen Lei, Shengcai Liao, and Stan Z. Li. Learning face representation from scratch.
arXiv:1411.7923, 2014.
Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, and Yu Qiao. Joint face detection and alignment using
multitask cascaded convolutional networks. IEEE Signal Processing Letters, 23(10):1499-1503,
2016.
Xiao Zhang, Rui Zhao, Yu Qiao, Xiaogang Wang, and Hongsheng Li. Adacos: Adaptively scaling
cosine logits for effectively learning deep face representations. In CVPR, 2019a.
Xiao Zhang, Rui Zhao, Junjie Yan, Mengya Gao, Yu Qiao, Xiaogang Wang, and Hongsheng Li.
P2sgrad: Refined gradients for optimizing deep face models. In CVPR, 2019b.
Tianyue Zheng, Weihong Deng, and Jiani Hu. Cross-age lfw: A database for studying cross-age
face recognition in unconstrained environments. arXiv:1708.08197, 2017.
Tianyue Zheng, Weihong Deng, and Jiani Hu. Cross-pose lfw: A database for studying cross-pose
face recognition in unconstrained environments. Technical Report 18-01, Beijing University of
Posts and Telecommunications, February 2018.
Tianyi Zhou and Jeff Bilmes. Minimax curriculum learning: Machine teaching with desirable diffi-
culties and scheduled diversity. In ICLR, 2018. URL https://openreview.net/forum?
id=BywyFQlAW.
10