Under review as a conference paper at ICLR 2020
Improved Training of Certifiably Robust Mod-
ELS
Anonymous authors
Paper under double-blind review
Ab stract
Convex relaxations are effective for training and certifying neural networks against
norm-bounded adversarial attacks, but they leave a large gap between certifiable and
empirical robustness. In principle, convex relaxation can provide tight bounds if
the solution to the relaxed problem is feasible for the original non-convex problem.
Therefore, we propose two regularizers that can be used to train neural networks
that yield tighter convex relaxation bounds for robustness. In all of our experiments,
the proposed regularizers result in higher certified accuracy than non-regularized
baselines.
1	Introduction
Neural networks have achieved excellent performances on many computer vision tasks, but they
could be vulnerable to small, adversarially chosen perturbations that are barely perceptible to humans
while having a catastrophic impact on the network’s performance (Szegedy et al., 2013; Goodfellow
et al., 2014). Making classifiers robust to these adversarial perturbations is of great interest, especially
when neural networks are applied to safety-critical applications. Several heuristic methods exist for
obtaining robust classifiers, however powerful adversarial examples can be found against most of
these defenses (Carlini & Wagner, 2017; Uesato et al., 2018).
Recent studies focus on verifying or enforcing the certified accuracy of deep classifiers, especially for
networks with ReLU activations. They provide guarantees of a network’s robustness to any perturba-
tion δ with norm bounded by kδkp ≤ (Wong & Kolter, 2017; Wong et al., 2018; Raghunathan et al.,
2018; Dvijotham et al., 2018b; Zhang et al., 2018; Salman et al., 2019). There are exact verifiers that
find the exact minimum adversarial distortions δ or the robust error (Ehlers, 2017; Katz et al., 2017;
Tjeng et al., 2017), but due to the non-convex nature of the problem, exact verification is NP-hard. To
make verification efficient and scalable, convex relaxations are adopted to expand the non-convex
feasible set given by non-linear activations into convex sets, resulting in a lower bound on the norm
of adversarial perturbations (Zhang et al., 2018; Weng et al., 2018), or an upper bound on the robust
error (Dvijotham et al., 2018b; Gehr et al., 2018; Singh et al., 2018). With LP relaxations (Wong
et al., 2018), such a verifier can be efficient enough to estimate the lower bound of the margin in each
iteration for training certifiably robust networks. However, due to the relaxation of the underlying
problem, the lower bound of the margin is potentially loose, and thus a barrier remains between the
optimal values from the original and relaxed problems (Salman et al., 2019).
In this paper, we focus on improving the certified robustness of neural networks trained with convex
relaxation bounds. To achieve this, we first give a more interpretable explanation for the bounds
achieved in (Weng et al., 2018; Wong et al., 2018). Namely, the constraints of the relaxed problem are
defined by a simple linear network with adversaries injecting bounded perturbations to both the input
of the network and the pre-activations of intermediate layers. The optimal solution of the relaxed
problem can be written as a forward pass of the clean image through the linear network, plus the
cumulative adversarial effects of all the perturbations added to the linear transforms, which makes it
easier to identify the optimality conditions and serves as a bridge between the relaxed problem and the
original non-convex problem. We further identify conditions for the bound to be tight, and we propose
two indicators for the gap between the original non-convex problem and the relaxed problem. Adding
the proposed indicators into the loss function results in classifiers with better certified accuracy.
1
Under review as a conference paper at ICLR 2020
2	Background and Related Work
Adversarial defenses roughly fall into two categories: heuristic defenses and verifiable defenses. The
heuristic defenses either try to identify adversarial examples and remove adversarial perturbations
from images, or make the network invariant to small perturbations through training (Papernot &
McDaniel, 2018; Shan et al., 2019; Samangouei et al., 2018; Hwang et al., 2019). In addition,
adversarial training uses adversarial examples as opposed to clean examples during training, so that
the network can learn how to classify adversarial examples directly (Madry et al., 2017; Shafahi et al.,
2019; Zhang et al., 2019a).
In response, a line of works have proposed to verify the robustness of neural nets. Exact methods
obtain the perturbation δ with minimum kδkp such that f (x) 6= f (x + δ), where f is a classifier
and x is the data point. Nevertheless, the problem itself is NP-hard and the methods can hardly
scale (Cheng et al., 2017; Lomuscio & Maganti, 2017; Dutta et al., 2018; Fischetti & Jo, 2017; Tjeng
et al., 2017; Scheibler et al., 2015; Katz et al., 2017; Carlini et al., 2017; Ehlers, 2017).
A body of work focuses on relaxing the non-linearities in the original problem into linear inequality
constraints (Singh et al., 2018; Gehr et al., 2018; Zhang et al., 2018; Mirman et al., 2018), sometimes
using the dual of the relaxed problem (Wong & Kolter, 2017; Wong et al., 2018; Dvijotham et al.,
2018b). Recently, Salman et al. (2019) unified the primal and dual views into a common convex
relaxation framework, and suggested there is an inherent gap between the actual and the lower bound
of robustness given by verifiers based on LP relaxations, which they called a convex relaxation
barrier.
Some defense approaches integrate the verification methods into the training of a network to minimize
robust loss directly. Hein & Andriushchenko (2017) uses a local lipschitz regularization to improve
certified robustness. In addition, a bound based on semi-definite programming (SDP) relaxation
was developed and minimized as the objective (Raghunathan et al., 2018). Wong & Kolter (2017)
presents an upper bound on the robust loss caused by norm-bounded perturbation via LP relaxation,
and minimizes this upper bound during training. Wong et al. (2018) further extend this method to
much more general network structures with skip connections and general non-linearities, and provide
a memory-friendly training strategy using random projections. Since LP relaxation is adopted, the
aforementioned convex relaxation barrier exists for their methods.
While another line of work (IBP) have shown that an intuitively looser interval bound can be used to
train much more robust networks than convex relaxation for large '∞ perturbations (GoWal et al.,
2018; Zhang et al., 2019b), it is still important to study convex relaxation bounds since it can provide
better certificates against a broader class of adversaries that IBP struggles to certify in some cases,
such as `2 adversaries for convolutional networks. We give a brief discussion for the reason in
Appendix E.
We seek to enforce the tightness of the convex relaxation certificate during training. It reduces the
optimality gap between the original and the relaxed problem by adding the proposed indicators
into the loss function as regularizers. Compared with previous approaches, we have the following
contributions: First, based upon the same relaxation in (Weng et al., 2018), we illustrate a more
intuitive view for the bounds on intermediate ReLU activations achieved by (Wong et al., 2018) ,
which can be viewed as a linear network facing adversaries adding bounded perturbations to both the
input and the intermediate layers. Second, starting from this view, we identify conditions where the
bound from the relaxed problem is tight for the original non-convex problem. Third, based on the
conditions, we propose regularizers that encourage the bound to be tight for the obtained network,
which improves the certificate on both MNIST and CIFAR-10.
3	Problem Formulation
In general, to train an adversarially robust network, we solve a constrained minimax problem where
the adversary tries to maximize the loss given the norm constraint, and the parameters of the network
are trained to minimize this maximal loss. Due to nonconvexity and the complexity of neural
networks, it is expensive to solve the inner max problem exactly. To obtain certified robustness, like
many related works (Wong et al., 2018; Gowal et al., 2018), we minimize an upper bound of the inner
max problem, which is a cross entropy loss on the negation of the lower bounds of margins over each
2
Under review as a conference paper at ICLR 2020
other class, as shown in Eq. 4. Without loss of generality, in this section we analyze the original and
relaxed problems for minimizing the margin between the ground truth class y and some other class t
under norm-bounded adversaries, which can be adapted directly to compute the loss in Eq. 4.
The original nonconvex constrained optimization problem for finding the norm-bounded adversary
that minimizes the margin can be formulated as
minimize ct>xL, subject to zi+1 = σ(xi), xi = fi(zi), for i = 1, ..., L,	(O)
zι∈Bp,e(x)
where Ct = ey - et, ey and et are one-hot vectors corresponding to the label y and some other class
t, σ(∙) is the ReLU activation, and f is one functional block of the neural network. This can be a
linear 山yer(fi(zi) = Wizi + bi), or even a residual block. We use hi(x) = fi(σ(fi-ι(…fι(x))))
to denote the ReLU network UP to the i-th layer, and PO to denote the optimal solution to O.
Figure 1: The feasible sets (blue regions/lines) given by the bounded ReLU constraints (Eq. O),
convex hull (Convij) and the relaxation (Fast-Lin) discussed in this paper (specific choice for Eq. C)
for j ∈ Ii. The red lines and dots are the intersections between the boundaries of the convex feasible
sets and the ReLU constraints.
3.1	Efficient Convex Relaxations
Grouping of ReLU Activations The nonconvexity of O stems from the nonconvex feasible set
given by the ReLU activations. Since the network is a continuous function, the pre-activations Xi
have lower and upper bounds xi and Xi when the input zι ∈ Bp,e(χ). If a certain pre-activation Xij
has Xij < 0 < Xij, its corresponding ReLU constraint zi+ι,j = σ(xj) gives rise to a non-convex
feasible set as shown in the left of Figure 1, making Eq. O a non-convex optimization problem. On
the other hand, if Xij ≤ 0 or Xij ≥ 0, the constraints degenerate into linear constraints zi+ι,j = 0
and zi+ιj = Xij respectively, which do not affect convexity. Based on Xi and Xi, we divide the
ReLU activations into three disjoint subsets
I-	=	{j|Xij ≤ 0},	l+	=	{j∖Xij ≥ 0},	Ii	=	{j∖Xij	< 0 <	Xij}.	(1)
if j ∈ Ii , we call the corresponding ReLU activation an unstable neuron.
Convex relaxation expands the non-convex feasible sets into convex ones and solves a convex
optimization problem C . The feasible set of O is a subset of the feasible set of C , so the optimal value
of C lower bounds the optimal value of Eq. O. Moreover, we want problem C to be solved efficiently,
better with a closed form solution, so that it can be integrated into the training process.
Computational Challenge for the “optimal” Relaxation As pointed out by (Salman et al., 2019),
the optimal layer-wise convex relaxation, i.e., the optimal convex relaxation for the nonlinear
constraint zi+1 = σ(Xi) of a single layer, can be obtained independently for each neuron. For
each j ∈ Ii in a ReLU network, the optimal layer-wise convex relaxation is the closed convex
hull Cnvij of Sij = {(Xij, zi+ι,j)∖j ∈ Ii,zi+ιj = max(0, Xij),Xj ≤ Xij ≤ Xij}, which isjust
Cnvij = {(Xij, zi+ι,j)∖ max(0, Xij) ≤ zi+ι,j ≤ χ.xijx (Xij - Xij)}, corresponding to the triangle
ij -ij
region in the middle of Figure 1. Despite being relatively tight, there is no closed-form solution to
this relaxed problem. LP solvers are typically adopted to solve a linear programming problem for
each neuron. Therefore, such a relaxation is hardly scalable to verify larger networks without any
additional trick (like xiao et al. (2018)). Weng et al. (2018) find it to be 34 to 1523 times slower than
Fast-Lin, and it has difficulty verifying MLPs with more than 3 layers on MNiST. in (Salman et al.,
2019), it takes 10,000 CPU cores to parallelize the LP solvers for bounding the activations of every
neuron in a two-hidden-layer MLP with 100 neurons per layer. Since solving LP problems for all
neurons are usually impractical, it is even more difficult to optimize the network to maximize the
3
Under review as a conference paper at ICLR 2020
lower bounds of margin found by solving this relaxation problem, as differentiating through the LP
optimization process is even more expensive.
Computationally Efficient Relaxations In the layer-wise convex relaxation, instead of using a
boundary nonlinear in xij, (Zhang et al., 2018) has shown that for any nonlinearity, when both the
lower and upper boundaries are linear in xij , there exist closed-form solutions to the relaxed problem,
which avoids using LP solvers and improves efficiency. Specifically, the following relaxation of O
has closed-form solutions:
minimize ct>xL ,
subject to ai ∙ Xi + bi ≤ zi+ι ≤ ai ∙ Xi + bi,Xi ≤ Xi ≤ Xi,	(C)
xi = Wizi +bi,fori = 1, ..., L, z1 ∈ Bp,(x),
where ∙ denotes element-wise product, and for simplicity, We have only considered networks with no
skip connections, and represent both Full Connected and Convolutional Layers as a linear transform
fi (zi ) = Wi zi + bi .
Before we can solve C to get the lower bound of margin, we need to know thee range E, Xi∖ for the
pre-activations Xi. As in (Wong & Kolter, 2017; Weng et al., 2018; Zhang et al., 2018), we can solve
the same optimization problem for each neuron Xij starting from layer 1 to L, by replacing ct with ej
or -ej for Xij or Xi7- respectively.1
The most efficient approach in this category is Fast-Lin (Weng et al., 2018), which sets aj = 4j,
as shown in the right of Figure 1. A tighter choice is CROWN (Zhang et al., 2018), which chooses
different aj and aj such that the convex feasible set is minimized. However, CROWN has much
higher complexity than Fast-Lin due to its varying slopes. We give detailed analysis of the closed-
form solutions of both bounds and their complexities in Appendix F. Recently, CROWN-IBP (Zhang
et al., 2019b) has been proposed to provide a better initialization to IBP, which uses IBP to estimate
range 区，Xi] for CROWN. In this case, both CROWN and Fast-Lin have the same complexity and
CRoWn is a better choice.
4	Tighter Bounds via Regularization
Despite being relatively efficient to compute, Fast-Lin and CROWN are not even the tightest layer-
wise convex relaxation. Using tighter bounds to train the networks could potentially lead to higher
certified robustness by preventing such bounds from over-regularizing the networks. Nevertheless,
there exist certain parameters and inputs such that the seemingly looser Fast-Lin is tight for O, i.e., the
optimal value of Fast-Lin is the same as O. The most trivial case is where no unstable neuron exists.
In practice, as shown in the illustrative example in Appendix A, Fast-Lin can be tight even when
unstable neurons exist. It is therefore interesting to check the conditions for Fast-Lin or CROWN to
be tight for O, and enforcing such conditions during training to improve the certified robustness.
4.1	Conditions for Tightness
Here we look into conditions that make the optimal value PC of the convex problem C to be equal
to PO. Let {zi, Xi}L=ι be some feasible solution of C, from which the objective value of C can be
determined as pC = ct>XL . Let {zi0 , X0i }iL=1 be some feasible solution of O computed by passing
z10 through the ReLU sub-networks hi(z10 ) defined in O, and denote the resulting feasible objective
value as P0O = ct> X0L .
Generally, for a given network with the set of weights {Wi, bi}iL=1, as long as the optimal solution
{z*,X*}L=ι of C is equal to a feasible solution {zi, x；}= of O, we will have PO = pC since any
feasible PO of O satisfies PO ≥ PO, and by the nature of relaxation PC ≤ PO.
Therefore, for a given network and input X, to check the tightness of the convex relaxation, we
can check whether its optimal solution {z*,x*}L=i is feasible for O. This can be achieved by
passing z； through the ReLU network, and either directly check the resultant objective value PO, or
compare {z； , Xi； }iL=1 with the resultant feasible solution {zi0, X0i}iL=1. Further, we can encourage such
1For Xij, take an extra negation on the solution.
4
Under review as a conference paper at ICLR 2020
conditions to happen during the training process to improve the tightness of the bound. Based on
such mechanisms, we propose two regularizers to enforce the tightness. Notice such regularizers are
different from the RS Loss (Xiao et al., 2018) introduced to reduce the number of unstable neurons,
since we have shown with Appendix A that C can be tight even when unstable neurons exist.
4.2	A Intuitive Indicator of Tightness： PO -PC
The observation above motivates us to consider the non-negative value
d(X, δo, W, b) = PO(X, δo) - PC
(2)
as an indicator of the difference between {z*, x*}L=i and {z0, χi}L=ι, where PO(x, δ∩) = c>hL(x +
δ0) is the margin over class t computed by passing the optimal perturbation δ0 for C through the
original network. δ0 can be computed efficiently from the optimality condition of Fast-Lin or
CRoWN, as demonstrated in Eq. 16. For example, when P = ∞, the optimal input perturbation
δ0 of C is δ0 = -ESign(c> WL：i), which corresponds to sending z1 = zɪ = X - ESign(c> WL：i)
through the ReLU network; when P = 2, δ力
z1 = Zf
X-E
C>Wl：1
kc> WL：lk2 .
-E 序号:∣∣2, which corresponds to sending
The larger d(X, δof , W, b) is, the more relaxed C is, and the higher PfO - PfC could be. Therefore, we can
regularize the network to minimize d(X, δof, W, b) during training and maximize the lower-bound of
the marginPfC, so that we can obtain a network where PfC is a better estimate of PfO and the robustness
is better represented by PfC . such an indicator avoids comparing the intermediate variables, which
gives more flexibility for adjustment. it bears some similarities to knowledge distillation (Hinton
et al., 2015), in that it encourages learning a network whose relaxed lower bound gives similar outputs
of the corresponding ReLU network. it is worth noting that minimizing d(X, δof, W, b) does not
necessarily lead to decreasing P0O(X, δof) or increasing PfC. in fact, both P0O(X, δof) and PfC can be
increased or decreased at the same time with their difference decreasing.
The tightest indicator should give the minimum gap PfO - PfC , where we need to find the optimal
perturbation for O. However, the minimum gap cannot be found in polynomial time, due to the
non-convex nature of O. (Weng et al., 2018) also proved that there is no polynomial time algorithm
to find the minimum 'ι-norm adversarial distortion with 0.99 ln n approximation ratio unless NP=P,
a problem equivalent to finding the minimum margin here.
4.3	A Better Indicator for Regularization: Difference in Solutions
Despite being intuitive and is able to achieve improvements, Eq. 2 which enforces similarity between
objective values does not work as good as enforcing similarity between the solutions {zif, Xif}iL=1
and {zi0, X0i}iL=1 in practice, an approach we will elaborate below. For both CRoWN and Fast-Lin,
unless d(X, δof, W, b) = 0, {zif, Xif}iL=1 may deviate a lot from {zi0, X0i}iL=1 and does not correspond
to any ReLU network, even if d(X, δof , W, b) may seem small. For example, it is possible that zifj < 0
for a given z1f , but a ReLU network will always have zi0j ≥ 0.
We find an alternative regularizer more effective at improving verifiable accuracy. The regularizer
encourages the feasible solution {zi0, X0i}iL=1 of O to exactly match the feasible optimal solution
{zif, Xif}iL=1 of C. since we are adopting the layer-wise convex relaxation, the optimal solutions of
the unstable neurons can be considered independently.
Here we derive a sufficient condition for tightness for Fast-Lin, which also serves as a sufficient
condition for CRoWN. For linear programming, the optimal solution occurs on the boundaries of the
feasible set. since Fast-Lin is a layer-wise convex relaxation, the solution to each of its neurons in zi
can be considered independently, and therefore for a specific layer i and j ∈ Ii , the pair of optimal
solutions (Xifj, zif+1,j) should occur on the boundary in the right of Figure 1. it follows that the only
3 optimal solutions (xj, zf+ι,j) of C that are also feasible for O are (xj, 0), (xij,Xj) and (0,0).
Notice they are also in the intersection between the boundary of CROWN and O.
in practice, out of efficiency concerns, both Fast-Lin and CRoWN identify the boundaries that the
optimal solution lies on and computes the optimal value by accumulating the contribution of each
layer in a backward pass, without explicitly computing {zif, Xif}iL=1 for each layer with a forward
5
Under review as a conference paper at ICLR 2020
pass (see Appendix F for more details). It is therefore beneficial to link the feasible solutions
of O to the parameters of the boundaries. Specifically, let δj ∈ {bj, bj} be the intercept of the
line that the optimal solution (Xj, z↑+ιj) lies on. We want to find a rule based on {δ*}L=ι to
determine whether the bound is tight from the values of {x0i}iL=1. For both Fast-Lin and CROWN,
x	χ x	Xij x√
bj = 0,bj = -Xij-ij . For Fast-Lin, when δj = bj∙, only (Xj,z；+Ij) = (Xij 0) or (x j,Xj)
are fesible for O; when δj = bj, only (χtj,z"j) = (0,0) is feasible for O. Meanwhile,
zi0+1,j = max(X0ij, 0) is deterministic if X0ij is given. Therefore, when the bound is tight for Fast-Lin,
if δj = bj∙, then Xj = 0. Otherwise, if δj = bj, and Xj = xj or Xj. For CROWN, this condition
is also feasible, though it could be either Xj ≤ 0 or Xj ≥ 0 when δj = 0, depending on the optimal
slope D(L).
Indeed, We achieve optimal tightness (PC = PO) for both Fast-Lin and CROWN if Xj satisfy these
conditions at all unstable neurons. Specifically,
Proposition 1. Assume {zi, Xi}L=ι is obtained by the ReLU network h^ with input z1, and {δ*}L=o1
is the optimal solution of Fast-Lin or CROWN. If zj = X + δ0, and Xj ∈ S(δj) for all i =
1, ..., L - 1, j ∈ Ii, then {zi0, X0i}iL=1 is an optimal solution of O, Fast-Lin and CROWN. Here
S (δj)
Xij Xij
Xij -Xij
We provide the proof of this simple proposition in the Appendix.
It remains to be discussed how to best enforce the similarity between the optimal solutions of O and
Fast-Lin or CROWN. Like before, we choose to enforce the similarity between {X0i}iL=1 and the closest
optimal solution of Fast-Lin, where {Xi}L=ι is constructed by setting x； = x； = Wi(x + δ∩) + bi
and pass x； through the ReLU network to obtain Xi = hi,(X + δ0). By Proposition 1, the distance
can be computed by considering the values of the intercepts {6；}仁1 as
r(x, δ5,W,b)
PL=⅛I
L-；
X
i=；
f
E |xj| + E min(|xj
j∈Ii	j∈Ii
∖δij =0	δij=0
∖
-xj|, |xj - xj |)
)
(3)
1
where the first term corresponds to δj = 0 and the condition Xj ∈ {0}, and the second term
corresponds to δj = - WijXij and the condition Xj ∈ {xj∙, Xj}. To minimize the second term, the
~ij	ij
original ReLU network only needs to be optimized towards the nearest feasible optimal solution. It
is easy to see from Proposition 1 that if r(x, δ0, W, b) = 0, then PO = pC where C could be both
Fast-Lin or CROWN.
Compared with d(x, δ^,W, b), r(x, δ^,W, b) puts more constraints on the parameters W, b , since it
requires all unstable neurons of the ReLU network to match the optimal solutions of Fast-Lin, instead
of only matching the objective values P0O and P；C . In this way, it provides stronger guidance towards
a network whose optimal solution for O and Fast-Lin or CROWN agree. However, again, this is
not equivalent to trying to kill all unstable neurons, since Fast-Lin can be tight even when unstable
neurons exist.
6
Under review as a conference paper at ICLR 2020
Algorithm 1 Computing the Fast-Lin Bounds and Regularizers for '∞ Norm
Data: Clean images{x(j)}jm=1, ReLU network with parameters W, b, and maximum perturbation .
Result: Margin Lower Boundp%, Regularizer d(x, δ0,W, b) and r(x, δ0,W, b).
Initialize Wi：i — Wi, gι(x) = WiX + bi, Xi = gι(x) - dWi：iki,row, Xi = gι(x) +
EkW1:1 Il i,row
for i=2,. . . ,L-1 do
Compute D— WithEq.13, Wi：i — WiDi-iWi-i, gi(x) = WiDi-igi-i(x) + b
Xi - gi(x) -EkWi：i ki,row - Pi-II Pj∈ιi0 jj min((Wi：i，+i)：,j, 0)
X	— gi(x) + e|Wi：i ki,row + Pi-II Pj∈ιi0 XIjj∙ min(-(Wi：i，+i)：,j, 0)
end
Compute Dl- withEq. 13, WL：i — WlDl-iWl-i, gL(X)= WLDL—igL—i(X)+ bL
ComputePC withEq. 17, δ^《——e|c>WL：iki,row
Xi	— hi(X + δ0) for i = 1 to L
d(X, δo, w, b) - c>xL — PC
Compute r(X, δ才，W, b) with Eq. 3
4.4 Certified Robust Training in Practice
In practice, for classification problems with more than two classes, we will compute the lower bound
of the margins w.r.t. multiple classes. Denote PC and PO as the concatenated vector of lower bounds
of the relaxed problem and original problem for multiple classes, and dt , rt as the regularizers for the
margins w.r.t. class t. Together with the regularizers, we optimize the following objective
minimize LCE(-PC, y) + λ V"dt(X, δo, W, b) + YTrt(x, *才，W, b),	(4)
W,b
where LCE(-PC, y) is the cross entropy loss with label y, as adopted by many related works (Wong
et al., 2018; Gowal et al., 2018), and we have implicitly abbreviated the inner maximization problem
w.r.t. {δi }L=0i into the optimal values PC and solution δ0. More details for computing the intermediate
and output bounds can be found in Algorithm 1, where we have used ∣∣∙∣∣ i,row to denote row-wise 'i
norm, and (∙)∙.,j for taking the j-th column.
One major challenge of the convex relaxation approach is the high memory consumption. To compute
the bounds Xi, Xi, we need to pass an identity matrix with the same number of diagonal entries as the
total dimensions of the input images, which can make the batch size thousands of times larger than
usual. To mitigate this, one can adopt the random projection from Wong et al. (2018), which projects
identity matrices into lower dimensions as Wi:iR to estimate the norm of Wi:i. Such projections add
noise/variance to Xi, Xi, and the regularizers are affected as well.
5	Experiments
Models, Datasets and Hyper-parameters: We evaluate the proposed regularizer on two datasets
(MNIST and CIFAR10) with two different E each. We consider only '∞ adversaries. We experiment
with a variety of different network structures, including a MLP (2x100) with two 100-neuron hidden
layers as (Salman et al., 2019), two Conv Nets (Small and Large) that are the same as (Wong
et al., 2018), a family of 10 small conv nets and a family of 8 larger conv nets, all the same as (Zhang
et al., 2019b), and also the same 5-layer convolutional network (XLarge) as in the latest version
of CROWN-IBP (Zhang et al., 2019b). For CROWN-IBP, we use the updated expensive training
schedule as (Zhang et al., 2019b), which uses 200 epochs with batch size 256 for MNIST and 3200
epochs with batch size 1024 for CIFAR10. We use up to 4 GTX 1080Ti or 2080Ti for all our
experiments.
Improved Training with Convex Relaxation Table 1 shows comparisons with various approaches.
All of our baseline implementations have improved compared with (Wong et al., 2018). When adding
the proposed regularizers, the certified robust accuracy is further improved in all cases for both
CP (Wong et al., 2018) and CROWN-IBP (Zhang et al., 2019b). We also provide results against
7
Under review as a conference paper at ICLR 2020
Dataset	Model	Base Method	e	λ	γ	Rob. Err	PGD Err	Std. Err
MNIST	2x100, Exact	CP	^^0∏	-0-	0	-14.85%	10.9%	3.65%
MNIST	2x100, Exact	CP	0.1	2e-3	1	13.32%	10.9%	4.73%
MNIST	Small, Exact	CP	^^0∏	-0-	0	4.47%	2.4%	1.19%
MNIST	Small, Exact	CP	0.1	5e-3	5e-1	3.65%	2.2%	1.09%
MNIST	-	Best of PV1	0.1	-	-	4.44%	2.87%	1.20%
MNIST	-	Best of RS2	0.1	-	-	4.40%	3.42%	1.05%
MNIST	Small	CP	^^0∏	-0-	0	4.47%	3.3%	1.19%
MNIST	Small	CP	0.1	0	5e-1	4.32%	3.4%	1.51%
MNIST	Large	DAI3	^^0∏	-	-	34%^	2.4%	1.0%
MNIST	2x100, Exact	CP	-0：3	-0-	0	-61.39%	49.4%	33.16%
MNIST	2x100, Exact	CP	0.3	5e-3	5e-3	56.05%	44.3%	26.10%
MNIST	Small, Exact	CP	-0：3	-0-	0	-31.25%	15.0%	7.88%
MNIST	Small, Exact	CP	0.3	5e-3	5e-1	29.65%	13.7%	7.28%
MNIST	Small	CP	-0：3	-0-	0	42.7%	26.0%	15.93%
MNIST	Small	CP	0.3	2e-3	2e-1	41.36%	24.0%	14.29%
MNIST	XLarge	IBP4	^^0I3	-	-	8.05%	6.12%	1.66%
MNIST	XLarge	CROWN-IBP	0.3*	-	-	7.01%	5.88%	1.88%
MNIST	XLarge	CROWN-IBP	0.3*	0	5e-1	6.64%	-	1.76%
CIFAR10	Small	CP	2/255	-0-	0	-53.19%	48.0%	38.19%
CIFAR10	Small	CP	2/255	5e-3	5e-1	51.52%	47.0%	37.30%
CIFAR10	Large	DAI3	2/255	-	-	61.4%	55.6%	55.0%
CIFAR10	XLarge	IBP4	2/255*	-	-	49.98%	45.09%	29.84%
CIFAR10	XLarge	CROWN-IBP	2/255*	-	-	46.03%	40.28%	28.48%
CIFAR10	Large	CP	2/255	0	0	45.78%	38.5%	29.42%
CIFAR10	Large	CP	2/255	5e-3	5e-1	45.19%	38.8%	29.76%
CIFAR10	Small	CP	8/255	-0-	0	-75.45%	68.3%	62.79%
CIFAR10	Small	CP	8/255	1e-3	1e-1	74.70%	67.9%	62.50%
CIFAR10	Large	CP	8/255	-0-	0	-74.04%	68.8%	59.73%
CIFAR10	Large	CP	8/255	5e-3	5e-1	73.74%	68.5%	59.82%
CIFAR10	XLarge	IBP4	8/255*	-	-	-67.96%	65.23%	50.51%
CIFAR10	XLarge	CROWN-IBP	8/255*	-	-	66.94%	65.42%	54.02%
CIFAR10	XLarge	CROWN-IBP	8/255*	0	5e-1	66.64%	-	53.78%
Table 1: Results on MNIST, and CIFAR10 with small networks, large networks, and different coefficients of
d(x,δ5, W,b), r(x,δo, W,b). All entries With positive λ or Y are using our regularizers. For all models not
marked as “Exact”, we have projected the input dimension of Wi:1 to 50, the same as Wong et al. (2018). For val-
ues with *, larger e is used for training. e = 0.3, 2/255, 8/255 correspond to using e = 0.4, 2.2/255, 8.8/255
for training respectively. For the methods: 1: (Dvijotham et al., 2018a); 2: (Xiao et al., 2018); 3: (Mirman et al.,
2018); 4 (Gowal et al., 2018).
a 100-step PGD adversary for our CP models. We will add PGD error rates for our CROWN-IBP
models in our future version. Since both PGD errors and standard errors are reduced in most cases,
the regularizer should have improved not only the certified upper bound, but also improved the actual
robust error.
The relative improvement on 2x100 with our regularizer (10.3%/8.7%) are comparable to the im-
provements (5.9%/10.0%) from (Salman et al., 2019), despite the fact that we start from a stronger
baseline. This indicates that the improvement brought by using our regularizer is comparable with
using the expensive and unstable optimal layer-wise convex relaxation for relaxation.
Our results with Small are better than the best results of (Dvijotham et al., 2018a; Xiao et al., 2018)
on MNIST with = 0.1, though not as good as the best of (Mirman et al., 2018), which uses a larger
model. When applying the same model on CIFAR10, we achieve better robust error than (Mirman
et al., 2018).
The relative improvements in certified robust error for = 0.1 and 0.3 are 18%/3.4% for the small
exact model on MNIST, compared with 0.03%/3.13% for the random projection counterparts. In the
exact models, we have better estimates of Xi,Xi. These consistent improvements validate that our
proposed regularizers improve the performance.
In comparison with IBP-based methods, our regularizers is able to further improve CP on CIFAR10
with = 2/255, and demonstrate the best result among all approaches compared in this setting. To our
knowledge, this is the best result of models of the same size. By using our regularizers on CROWN-
IBP to provide a better initialization for the later training stage of IBP, our method also achieves the
best certified accuracy on CIFAR10 under = 8/255. To provide more comprehensive evaluations,
Table 4 shows the mean and variance of the results with smaller models, demonstrating consistent
8
Under review as a conference paper at ICLR 2020
Dataset	e('∞)	Model Family	Method	Verifi best	ed Test Err median	or (%) worst	Stand best	ard Test E median	ror (%) worst
MNIST	0.3	Gowal etal. (2018)		-805^	-	-	~666~	-	-
		8 large models	CI Orig		-8:47-	8.57	T.48-	1.52	T.99-
			CI ReImp	"7.99-	-8:38-	8.97	T.40-	1.69	^Γ9-
			CI Reg	~T2~	8.44	8.88	^Γ5Γ~	1.72	2.21
Table 2: Our results on the MNIST dataset, with CROWN-IBP. CI Orig are results copied from the paper, CI
ReImp are results of our implementation of CROWN-IBP, and CI Reg is with regularizer r.
improvements of our model, while Table 2 gives the best, median and worst case results with the
large models on the MNIST dataset. Note all the networks are significantly smaller than (Gowal
et al., 2018), and our batch size and number of epochs (at most 256 and 140) are also much smaller
than (Gowal et al., 2018) (1600 and 3200). Still, we are able to achieve better results.
6	Conclusions
We propose two regularizers that lead to tighter LP relaxation bounds for certifiable robustness.
Extensive experiments validate that the regularizers improve robust accuracy over non-regularized
baselines. This work is a step towards closing the gap between certified and empirical robustness.
Future directions include methods to improve computational efficiency for LP relaxations (and
certified methods in general), and better ways to leverage random projections for acceleration.
References
Nicholas Carlini and David Wagner. Adversarial examples are not easily detected: Bypassing ten
detection methods. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and
Security,pp. 3-14. ACM, 2017.
Nicholas Carlini, Guy Katz, Clark Barrett, and David L Dill. Provably minimally-distorted adversarial
examples. arXiv preprint arXiv:1709.10207, 2017.
Chih-Hong Cheng, Georg Nuhrenberg, and Harald Ruess. Maximum resilience of artificial neural
networks. In International Symposium on Automated Technology for Verification and Analysis, pp.
251-268. Springer, 2017.
Jeremy M Cohen, Elan Rosenfeld, and J Zico Kolter. Certified adversarial robustness via randomized
smoothing. arXiv preprint arXiv:1902.02918, 2019.
Souradeep Dutta, Susmit Jha, Sriram Sankaranarayanan, and Ashish Tiwari. Output range analysis for
deep feedforward neural networks. In NASA Formal Methods Symposium, pp. 121-138. Springer,
2018.
Krishnamurthy Dvijotham, Sven Gowal, Robert Stanforth, Relja Arandjelovic, Brendan O’Donoghue,
Jonathan Uesato, and Pushmeet Kohli. Training verified learners with learned verifiers. arXiv
preprint arXiv:1805.10265, 2018a.
Krishnamurthy Dvijotham, Robert Stanforth, Sven Gowal, Timothy Mann, and Pushmeet Kohli. A
dual approach to scalable verification of deep networks. arXiv preprint arXiv:1803.06567, 104,
2018b.
Ruediger Ehlers. Formal verification of piece-wise linear feed-forward neural networks. In Interna-
tional Symposium on Automated Technology for Verification and Analysis, pp. 269-286. Springer,
2017.
Matteo Fischetti and Jason Jo. Deep neural networks as 0-1 mixed integer linear programs: A
feasibility study. arXiv preprint arXiv:1712.06174, 2017.
Timon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat Chaudhuri, and Martin
Vechev. Ai2: Safety and robustness certification of neural networks with abstract interpretation. In
2018 IEEE Symposium on Security and Privacy (SP), pp. 3-18. IEEE, 2018.
9
Under review as a conference paper at ICLR 2020
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014.
Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan
Uesato, Timothy Mann, and Pushmeet Kohli. On the effectiveness of interval bound propagation
for training verifiably robust models. arXiv preprint arXiv:1810.12715, 2018.
Matthias Hein and Maksym Andriushchenko. Formal guarantees on the robustness of a classifier
against adversarial manipulation. In Advances in Neural Information Processing Systems, pp.
2266-2276, 2017.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531, 2015.
Uiwon Hwang, Jaewoo Park, Hyemi Jang, Sungroh Yoon, and Nam Ik Cho. Puvae: A variational
autoencoder to purify adversarial examples. arXiv preprint arXiv:1903.00585, 2019.
Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer. Reluplex: An efficient
smt solver for verifying deep neural networks. In International Conference on Computer Aided
Verification, pp. 97-117. Springer, 2017.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Alessio Lomuscio and Lalit Maganti. An approach to reachability analysis for feed-forward relu
neural networks. arXiv preprint arXiv:1706.07351, 2017.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017.
Matthew Mirman, Timon Gehr, and Martin Vechev. Differentiable abstract interpretation for provably
robust neural networks. In International Conference on Machine Learning, pp. 3575-3583, 2018.
Nicolas Papernot and Patrick McDaniel. Deep k-nearest neighbors: Towards confident, interpretable
and robust deep learning. arXiv preprint arXiv:1803.04765, 2018.
Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certified defenses against adversarial
examples. arXiv preprint arXiv:1801.09344, 2018.
Hadi Salman, Greg Yang, Huan Zhang, Cho-Jui Hsieh, and Pengchuan Zhang. A convex relaxation
barrier to tight robust verification of neural networks. NeurIPS, 2019.
Pouya Samangouei, Maya Kabkab, and Rama Chellappa. Defense-gan: Protecting classifiers against
adversarial attacks using generative models. arXiv preprint arXiv:1805.06605, 2018.
Karsten Scheibler, Leonore Winterer, Ralf Wimmer, and Bernd Becker. Towards verification of
artificial neural networks. In MBMV, pp. 30-40, 2015.
Ali Shafahi, Mahyar Najibi, Amin Ghiasi, Zheng Xu, John Dickerson, Christoph Studer, Larry S
Davis, Gavin Taylor, and Tom Goldstein. Adversarial training for free! arXiv preprint
arXiv:1904.12843, 2019.
Shawn Shan, Emily Willson, Bolun Wang, Bo Li, Haitao Zheng, and Ben Y. Zhao. Gotta catch ’em
all: Using concealed trapdoors to detect adversarial attacks on neural networks, 2019.
GagandeeP Singh, Timon Gehr, Matthew Mirman, Markus PuscheL and Martin Vechev. Fast and
effective robustness certification. In Advances in Neural Information Processing Systems, pp.
10802-10813, 2018.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing ProPerties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Vincent Tjeng, Kai Xiao, and Russ Tedrake. Evaluating robustness of neural networks with mixed
integer Programming. arXiv preprint arXiv:1711.07356, 2017.
10
Under review as a conference paper at ICLR 2020
Jonathan Uesato, Brendan O’Donoghue, Aaron van den Oord, and Pushmeet Kohli. Adversarial risk
and the dangers of evaluating against weak attacks. arXiv preprint arXiv:1802.05666, 2018.
Tsui-Wei Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Duane Boning, Inderjit S
Dhillon, and Luca Daniel. Towards fast computation of certified robustness for relu networks.
ICML, 2018.
Eric Wong and J Zico Kolter. Provable defenses against adversarial examples via the convex outer
adversarial polytope. arXiv preprint arXiv:1711.00851, 2017.
Eric Wong, Frank Schmidt, Jan Hendrik Metzen, and J Zico Kolter. Scaling provable adversarial
defenses. In Advances in Neural Information Processing Systems, pp. 8400-8409, 2018.
Kai Y Xiao, Vincent Tjeng, Nur Muhammad Shafiullah, and Aleksander Madry. Training for faster
adversarial robustness verification via inducing relu stability. arXiv preprint arXiv:1809.03008,
2018.
Dinghuai Zhang, Tianyuan Zhang, Yiping Lu, Zhanxing Zhu, and Bin Dong. You only propagate
once: Painless adversarial training using maximal principle. arXiv preprint arXiv:1905.00877,
2019a.
Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel. Efficient neural network
robustness certification with general activation functions. In Advances in Neural Information
Processing Systems, pp. 4939-4948, 2018.
Huan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Duane Boning, and Cho-Jui Hsieh. Towards
stable and efficient training of verifiably robust neural networks. arXiv preprint arXiv:1906.06316,
2019b.
11
Under review as a conference paper at ICLR 2020
A A TOY Example FOR Tight Relaxation
Figure 2: Illustration of the data distribution and the decision boundary of the network. In this case,
b = 0.3, e = 0.2, xo = [0.1,0.42]T.
We give an illustrative example where the optimal solution to the relaxed problem is a feasible
solution to the original non-convex problem for certain input samples even when unstable neurons
exist. It is a binary classification problem for samples x0 = [x01, x02]T ∈ R2. We assume x0 is
uniformly distributed in S0 ∪ S1, where S0 = {x0|x02 ≤ |x01 | - b, kx0k∞ ≤ 1}, S1 = {x0|x02 ≥
|x01| + b, kx0k∞ ≤ 1}, and 0 < b < 1. The ground-truth label for x0 ∈ S0 and x0 ∈ S1 are 0, 1
respectively. The maximal-margin classifier for such data distribution is I{z12≥∣z12∣}, where zι is the
input to the classifier. The data distribution and the associated maximal-margin classifier is shown in
Figure 2.
This maximal-margin classifier can be represented by a ReLU network with single hidden layer as
I{h2(z1)≥0}, where h2(z1) = W2σ(W1z1), and
10
10
Wi =	01	0	,W2 = [-1 -1 1 -1]∙
0	-1
(5)
Claim 1 (Convex relaxation can be tight when unstable neurons exist). The solution to the relaxed
problem 15 is feasible for the original non-convex problem O of the aforementioned ReLU network
h2 (xo + δo) = W2σ(W1(x0 + δo)) for any xo ∈ Si under any perturbation δo ∈ {δ∣kδ∣∣∞ ≤ e, 0 <
e < b}.
Proof. Both the network and Si are symmetric in x0i, therefore it is sufficient to prove the result for
x0i ≥ 0.
(1) If x0i ≥ e, since x0i ∈ Si, x02 ≥ b + e. The 4 neurons in xi = Wi(x0 + δ0) are either
non-negative or non-positive for any δo ∈ {δ∣kδ∣∣∞ ≤ e, 0 < e < b} and the convex relaxation is
tight.
(2) If x0i < e, for any input sample x0 ∈ {x|x = [a, c]T, 0 < a < b < b+ a ≤ c} ⊆ Si, the optimal
perturbation δO ∈ {δ∣kδ∣∣∞ ≤ e, 0 <a<e<b} can be inferred from Figure 2 as [e, -e]T. The
corresponding ReLU activations and the optimal solution are
z2 = [a + e, 0, c - e, 0] , x2 = c - a - 2e.	(6)
Meanwhile, for the relaxed problem, the lower and upper bounds of the hidden neurons xi = Wi zi
are
Xi = [a — e, —a — e, c — e, —C — e]，, x 1 = [a + e, —a + e, c + e, —C + e]，.
12
Under review as a conference paper at ICLR 2020
Therefore, the first 2 hidden neurons are unstable neurons, and the convex relaxation we are using
will relax the ReLU operation z2 = σ(x1) into z2 = D1x1 + δ1, where D1 is a diagonal matrix, and
δι are slack variables bounded by 0 ≤ δι ≤ 册.The diagonal entries of Di and the upper bounds δι
are defined by Eq. 13 as
diag(Di)=[中,-a+^, 1, °] , M = [La + Fa + '), La + Fa + '), 0, J ,
2	2	2	2
i.e., δ13 and δ14 are always 0. The relaxed linear network, as defined by the constraints in Eq. C with
our specific relaxation, is now determined as x2 = h2(z1) = W2(D1W1(x + δ0) + δ1). It can be
written into the same form as Eq. 14 as
Ctx =	Cth2(z1)	=	ctW2D1W1(x +	δo)	+ ctW2 δi	=	Ct W2：ix + ctW2=iδo +	ctW2δ1,	(7)
where
Ct = 1, W2:1 = [-1,1].
Therefore, to minimize the term ctWz：*。，we should choose δ0 = [e, -e]T, which is equal to δO.
To minimize qW2δ1, We should let δj = [(-。+；*+0, (-a+2*+0, °, 0i, which gives rise to the
optimal solution of the relaxed problem as
z； = Di Wi (X + δ0) + δ; = [a + e, 0, c — e, 0]t, x； = C — a — 2e,
the same as the optimal solution of the original non-convex problem given in Eq. 6. This shows both
of the regularizers, in this case instantiated as
d(x,δ0, W1, W2) = ct(x2 - x2),r(x,δ0, W1, W2) = 2 (|x11 - x11 | + |x12 - x12|) ,
are able to reach 0 for certain networks and samples when non-stable neurons exist.	□
It might seem that adding d(x, δ02, W, b) = p0O(x, δ02) - p2C as a regularizer into the loss function will
undesirably minimize the margin p0O(x, δ02) for the ReLU network. Theoretically, however, it is not
the case, since min p0O - p2C is a different optimization problem from neither min p0O nor maxp2C. In
fact, the non-negative d(x, δ02, W, b) could be minimized to 0 with both p0O and p2C taking large values.
In the illustrative example, it is easy to see that for any x0 ∈ {x|x = [a, C]T, 0 < a < b < b+ a ≤ C},
d(x, δ2, W, b) = ct(x； - x；) = 0, but PO=PC = C - a - 2e > 0 when e < b.
Moreover, since we are maximizing p2C via the robust cross entropy loss2 while minimizing the
non-negative difference P0O - P；C , the overall objective tends to converge to a state where both P0O
and P；C are large.
B Proof of Proposition 1
Proposition 1. Assume {zi, χi}L=ι is obtained by the ReLU network h^ with input zi, and {δ*}L=-1
is the optimal solution of Fast-Lin or CROWN. If zi = X + δ0, and Xij ∈ S(δj) for all i =
1, ..., L - 1, j ∈ Ii, then {zi0, X0i}iL=i is an optimal solution of O, Fast-Lin and CROWN. Here
S (δ2j)
Xij Xij
Xij -Xij
Proof. We only need to prove {zi0, X0i}iL=i is an optimal solution of both Fast-Lin and CROWN.
After that, {zi0, X0i}iL=i is both a lower bound and feasible solution of O, and therefore is the optimal
solution of O.
Here we define x； = Wiz； + b for i = 1,..., L, z+ = DiL)x； + δ* for i = 1,..., L - 1, and
z； = X + δ0. By definition, {x；, z*}L=ι is an optimal solution of Fast-Lin or CROWN. Also, since
2The cross entropy loss on top of the lower bounds of margins to all non-ground-truth classes, see Eq. 7.
13
Under review as a conference paper at ICLR 2020
z1 = zι, We have x1 = Wiz； + bi = x；. Next, We will prove if the assumption holds, We will have
z2 = z； for both Fast-Lin and CROWN.
For j ∈ I+, by definition of Fast-Lin and CROWN, D( j) = 1 and x；j ≥ x j > 0, so x0j = Xj ≥ 0,
z2；,j = Dij xi；j = x0ij = max(x0ij, 0) = z20 j.
For j ∈ I-, again, by definition, D(j) = 0, and x；j ≤ Xj < 0, so x；j = Xj < 0, z；j =
Di(Lj )x；ij = 0 = max(x0ij , 0) = z20j .
For j ∈ Ii :
•	If δ j = 0 and x0ij = 0 as assumed in the conditions, since z； = z；, we know
Xi；j = Wij zi； + bij = Wij zi0 + bij = X0ij = 0,
where W；j is the j-th row of W；. No matter what value D；(Lj ) is, z2；,j = D；(Lj )X；；j = 0,
z20j = max(X0；j, 0) = 0, the equality still holds.
•	If 6；j = - χX1-Xj , for both Fast-Lin and CROWN, z；j =-比：1-Xj (Xj-Xj). Further, if
x0ij ∈ {x j, X j} as assumed: if x； = X；j = Xj, then x“ < 0, so z[j = max(X；j, 0)=
0, and z2j = Xj-X.. (Xj - x j) = 0 = z2j; if Xj = X1j = Xj, then X1j > 0, z2j = X1j
and z2j = Xj-Xj (Xj- Xj) = X1j = z2 j.
Now we have proved z20 = z2； for both Fast-Lin and CROWN if the assumption is satisfied. Starting
from this layer, using the same argument as above, we can prove z30 = z3；,...,zL0 -； = zL； -； for both
Fast-Lin and CROWN. As a result, X0L = X；L and ctT X0L = ctT X；L = p；C, where C can be both Fast-lin
and CROWN. Therefore, {z0, x；}乜；is an optimal solution of C.	□
C Experimental Details
Experiments for Table 1 The small convnet has two convolutional layers of 16, 32 output channels
each and two FC layers with 100 hidden neurons. The large net has four Conv layers with 32, 32,
64 and 64 output channels each, plus three FC layers of 512 neurons. For all experiments, we are
using Adam (Kingma & Ba, 2014) with a learning rate of 1e - 3. Like (Wong et al., 2018), we train
the models for 80 epochs, where in the first 20 epochs the learning rate is fixed but the increases
from 0.01/0.001 to its maximum value for MNIST/CIFAR10, and in the following epochs, we reduce
learning rate by half every 10 epochs. We do not use weight decay. We also adopt a warm-up schedule
in all experiments, where λ, γ increases form 0 to the preset values in the first 20 epochs, due to the
noisy estimation from the random projections. Our implementation is based on the code released
by (Wong et al., 2018), so when λ = γ = 0, we obtain the same results as as (Wong et al., 2018).
Experiments for Other Tables We use the same hyper-parameters as (Zhang et al., 2019b), except
for “l Jdecay_step" and “epochs”, which are set to 20 and 140 for lower variance.
D Ablation Studies of the Two Regularizers
In this section, we give the detailed results with either λ or γ set to 0, i.e., we use only one regularizer
in each experiment, in order to compare the effectiveness of the two regularizers. All the results are
with the small model on CIFAR10 with E = 2/255. The best results are achieved with r(x, 6；, W, b).
We reasoned in 4.2 that d(x, δ0, W, b) may not perform well when random projection is adopted.
As shown in the supplementary, the best robust error achieved under the same setting when fixing
Y = 0 is higher than when fixing λ = 0, which means r(x, δ0, W, b) is more resistant to the noise
introduced by random projection. Still, random projections offer a huge efficiency boost when they
are used. How to improve the bounds while maintaining efficiency is an important future work.
14
Under review as a conference paper at ICLR 2020
λ	Y	Robust Err(%)	Std. Err (%)	λ	Y	RobustErr(%)	Std. Err (%)
1e-5	O	529O^	37.61	ɪ	1e-3	5256^	37.45
5e-5	O	53.O9	37.77	O	5e-3	53.38	38.19
1e-4	O	52.48	37.37	O	1e-2	52.6O	37.73
5e-4	O	52.85	37.13	O	2.5e-2	52.7O	37.78
1e-3	O	52.61	37.96	O	5e-2	53.13	38.36
2.5e-3	O	53.1O	38.24	O	1e-1	52.72	38.22
5e-3	O	52.76	38.15	O	2.5e-1	52.9O	38.O4
1e-2	O	53.14	38.58	O	5e-1	52.39	37.48
5e-2	O	52.82	39.89	O	1	52.27	38.07
1e-1	O	53.94	41.59	O	2	53.1O	38.64
5e-1	O	56.39	48.O6				
1	O		59.23	52.51				
Table 3: Ablation results on CIFAR10 with the small model, where = 2/255.
E DIFFICULTIES IN ADAPTING IBP FOR `2 ADVERSARY
The Inverval Bound Propagation (IBP) method discussed here is defined in the same way as (Gowal
et al., 2018), where the bound of the margins are computed layer-wise from the input layer to the
final layer, and the bound of each neuron is considered independently for both bounding that neuron
and using its inverval to bound other neurons.
It is natural to apply IBP against '∞ adversaries, since each neurons are allowed to change indepen-
dently in its interval, which is similar to the '∞ ball. One way to generalize IBP to other 'p norms is
to modify the bound propagation in the first layer, such that any of its output neuron (i) is bounded by
an interval centered at xii = Wι,iX + bι,i with a radius of EpkWι,i∣∣p*, where xii the clean image
x's response, and Wι,i is the first layer,s linear transform corresponding to the neuron, and ∣∣∙kp* is
the dual norm of ∣∣∙∣p, with P + p* = 1. We refer to this approach IBP('p, ep). Here by the example
of `2 norm, we show such an adaptation may not be able to obtain a robust convolutional neural
network compared with established results, such as reaching 61% certified accuracy on CIFAR10
with E2 = 0.25 (Cohen et al., 2019).
Specifically, for adversaries within the '2-ball B2,e2 (χ),IBP('2,⑹ computes the upper and lower
bounds as
x2i = Wι,iX + E2∣W1,ik2 + bι,i, x2i = Wι,iX - E2∣W1,ik2 + bι,i.	(8)
By comparison, for some adversary within the '∞-ball B∞,e∞ (x), IBP('∞, e∞) computes the upper
and lower bounds as
x∞ = Wι,iX + E∞kW1,ik1 + bι,i, X∞ = Wι,iX - E∞kW1,ik1 + bι,i∙	(9)
Since the two approaches are identical in the following layers, to analyze the best-case results of
IBP('2, €2) based on established results of IBP('∞, e∞), it suffices to compare the results of IBP('∞,
e∞) with e∞ set to some value such that the range X∞ 一 χ∞ of Eq. 9 is majorized by the range
x2 一 χi of Eq. 8. In this way, we are assuming a weaker adversary for IBP('∞, e∞) than the original
IBP('2, €2), so its certified accuracy is an upper bound of IBP('2, €2). Therefore, it suffices to let
E∞∣∣W1,i∣∣1 ≤ €2kW1,ik2, ∀i = 1, 2, ∙∙∙, n1∙	(10)
For any Wι,i ∈ Rd, we have ∣∣W1,i∣∣1 ≤ √d∣Wι,i∣2. TomakeEq. IOholdforany Wι,i ∈ Rd, we
can set
1
e∞ = √T2∙
In general, d is equal to the data dimension, such as 3O72 for the CIFAR1O dataset. However, for
convolutional neural networks, the first layer is usually convolutional layers and W1,i is a 3O72-
dimensional sparse vector with at most k × k × 3 non-zero entries at fixed positions for convolution
kernels with size k and input images with 3 channels. In (Zhang et al., 2O19b; Gowal et al., 2O18;
Wong et al., 2O18), k = 3 for their major results. In this case,
1
e∞ ≥ 3√3 e2∙
(11)
15
Under review as a conference paper at ICLR 2020
Under such assumptions, for ⑦=0.25, the certified accuracy of IBP('2, 0.25) on CIFAR10 should
be upper bounded by IBP('∞, 0.04811), unless changing the first layer bounds into '∞ norm based
bounds significantly harms the performance.3 The best available results of certified accuracies are
33.06% for IBP('∞, 0.03137)and 23.20% for IBP('∞, 0.06275)(Zhang et al., 2019b). Comparing
with the established results from (Cohen et al., 2019) (61%), we can conclude the certified accuracy
of IBP('2, 0.25) is at least 27.93% to 37.80% lower than the best available results, since We are
assuming a weaker adversary.
IBP('2, €2) is also not as good as the results with convex relaxation from (Wong et al., 2018), where
the best single-model (with projection as approximation) certified accuracy with 2 = 36/255 is
51.09%. For IBP, this adversary is no weaker than e∞ = 6.9282/255. The best available results
for IBP('∞, 2/255) and IBP('∞, 8/255) are 50.02% (Gowal et al., 2018)and 33.06% (Zhang et al.,
2019b) respectively, which indicates the certified accuracy of IBP('2, 36/255) is at least 1.07% to
18.03% worse (much loser to 18.03%) than the approximated version of convex relaxation under the
same `2 adversary.
F S olutions to the Relaxed Problems
In this section, we give more details about the optimal solutions of Fast-Lin (Weng et al., 2018) and
CROWN (Zhang et al., 2018), to make this paper self-contained. Recall that for layer-wise convex
relaxations, each neuron in the activation layer are independent. {aij,bj,aij,bij} are chosen to
bound the activations assuming the lower bound Xij and upper bound Xij of the preactivation Xij is
known. For j ∈ I ajXij + bj ≤ max(0,xj) ≤ χj-jχ-(Xij- Xij) ≤ ajjXij + bij; for j ∈ I+,
aij = aij = aij = bij = 1; for j ∈ Ii , aij = aij = bij = bij = 0.
Optimal Solutions of Fast-Lin In Fast-Lin (Weng et al., 2018), for j ∈ Ii, aij = aj =彳 Xj ,
j	J	Xj Xij
bj = 0, Tbij = - Xxi-Xij , shown as the blue region in the right of Figure 1. To compute the lower
and upper bound Xij and Xij for Xij, we just need to replace the objective of Eq. C with c>Xi, where
Cij is a one-hot vector with the same number of entries as Xi and the j-th entry being 1 for Xij and -1
for Xij (an extra negation is applied to the minimum to get Xij).
Such constraints allow each intermediate ReLU activation to reach their upper or lower bounds
independently. As a result, each intermediate unstable neuron can be seen as an adversary adding a
perturbation δj in the range [0, - Xxi-； ] to a linear transform, represented as Zij = Xa-X Xij + δj.
Such a point of view gives rise to a more interpretable explanation for Fast-Lin. If we construct a
network from the relaxed constraints, then the problem becomes how to choose the perturbations for
both the input and intermediate unstable neurons to minimize ct>XL of a multi-layer linear network.
Such a linear network under the perturbations is defined as
zi+1 = DiXi + δi, Xi = Wizi + bi, for i = 1, ..., L, z1 = X + δ0,	(12)
where Di is a diagonal matrix and δi is a vector. The input perturbation satisfies kδ0 kp ≤ €. The j -th
diagonal entry Dij and the j -th entry δij for i > 0 is defined as
D
Dij
0,
1,_
Xij
Xj-Xij ,
if j ∈ I+, δij ∈ Sδj = ( h0, - xxj⅛i , if j ∈ Ii
ifj ∈ Ii	{0},	otherwise.
(13)
With such an observation, we can further unfold the objective in Eq. C 12 into a more interpretable
form as
L-1	L-1
c>XL = c> fL(DL-1fL-1(…Dιfι(X)))+ C> X WL Y DkWkδi,	(14)
i=0	k=i+1
where the first term of RHS is a forward pass of the clean image X through a linear network
interleaving between a linear layer X = Wi z + bi and a scaling layer z = DiX, and the second term
3Which is unlikely, since ∞ is now a weaker adversary than 2 and the difference in gradient expression
only appears in the first layer.
16
Under review as a conference paper at ICLR 2020
is the sum of the i-th perturbation passing through all the weight matrices Wi of the linear operation
layers and scaling layers Di after it.
Therefore, under such a relaxation, only the second term is affected by the variables {δi}iL=-01 for opti-
mizing Eq. C. Denote the linear network up to the i-th layer as gi (x), and Wi:i0 = Wi Qik-=1i0 Dk Wk .
We can transform Eq. C with a^ = a^ = ɪ^ij—, bj = 0, bj = - JijXij into the following
Xij -Xij	x	Xij -Xij
constrained optimization problem
L-1
minimize ct>gL (x) +C>W c>WL.i+ιδi, subject to ∣∣δokp ≤ e, δj ∈ Sδj for i = 1,…,L - 1. (15)
δ0,...,δL-1
i=0
Notice c> WL：i+i is just a row vector. For i > 0,j ∈L, to minimize c> WL"+ιδj, we let δj be its
minimum value 0 if (c> Wl”+i j ≥ 0, or its maximum value δj = --ijxij if (c>WL:i+i)j < 0,
Wij -ij
where (ct>WL:i+1)j is the j-th entry ofct>WL:i+1. For δ0, when the infinity norm is used, we set
δij = - if (ct>WL:i+1)j ≥ 0, and otherwise δij = . For other norms, it is also easy to see that
“min c>Wl：iSo = -*,m,ax -c>W,L,ιδo = -ekc>WL：ik*,	(16)
kδ0 kp≤	kδ0 kp ≤1
where ∣∙∣ * isthe dual norm of the P norm. In this way, the optimal value PC PC of the relaxed problem
(Eq. 15) can be found efficiently without any gradient step. The optimal value can be achieved by
just treating the input perturbations and intermediate relaxed ReLU activations as adversaries against
a linear network after them.The resulting expression for the lower-bound is
LT x.∙
PO ≥ PC = c>gL(x) - e|c>WL：ik* - X Xmin((c>WL”+ι)j, 0).	(17)
M Wi Xij- Xij
Though starting from different points of view, it can be easily proved that the objective derived from
a dual view in (Wong et al., 2018) is the same as Fast-Lin.
Optimal Solution of CROWN The only difference between CROWN and Fast-Lin is in the choice
of aij for j ∈ Ii. For ReLU activations, CROWN chooses aij- = 1 if Xij ≥ -Xij, or aij- = 0
otherwise. This makes the relaxation tighter than Fast-Lin, but also introduces extra complexity
due to the varying D%. In Fast-Lin, Di is a constant once the upper and lower bounds Xi and Xi are
given. For CROWN, since 0 < ai7- < 1, ai7- = aij-, Di now changes with the optimality condition
of δi , which depends on the layer l and the index k of the neuron/logit of interest. Specifically, for
'∞ adversaries, the optimality condition of δi is determined by CkWi：i, so now we have to apply
extra index to the slope as Di(l,k), as well as the equivalent linear operator as Wl(:l1,k). As a result, the
optimal solution is now
L-1	X
c>kgι(X) - ≡kcιkWι¾k)k* - X X ^jL~ min((cιk W排 j, 0)∙	(18)
i=1j∈Ii M- Xij
This drastically increase the number of computations, especially when computing the intermediate
bounds Xi and Xi, where we can no longerjust compute a single Wi：i to get the bound, but have to
compute number-of-neuron copies of it for the different values of Di(ι,k) in the intermediate layers.
Practical Implementations of the Bounds In practice, the final output bound (also the intermedi-
ate bounds) is computed in a backward pass, since we need to determine the value (c>W(Ii+ι)j to
choose the optimal δi*j , which is the multiplication of all linear operators after layer i. Computing
Ck W(Ii+1 in a backward pass avoids repeated computation. It proceeds as
C> W(llk) = cTkW^+ιDilk)Wi.	(19)
G Detailed results on the family of small models
17
Under review as a conference paper at ICLR 2020
Method I error ∣ model A ∣ model B ∣ model C ∣ model D ∣ model E ∣ model F ∣ model G ∣ model H ∣ model I ∣ model J
CoPied	std. (%)	5.97 ± .08	3.20 ± 0	6.78 ± .1	3.70 ± .1	3.85 ± .2	3.10 ± .1	4.20 ± .3	2.85 ± .05	3.67 ± .08	2.35 ± .09
	verified (%)	15.4 ± .08	10.6 ± .06	16.1 ± .3	11.3 ± .1	11.7 ± .2	9.96 ± .09	12.2 ± .6	9.90 ± .2	11.2 ± .09	9.21 ± .3
Baseline	-std.(%)-	5.65 ± .04	3.23 ± .3	4.70 ± .4	2.94 ± .05	6.39 ± .3	2.89 ± .05	4.11 ± .3	2.55 ±.1	3.30 ± .4	2.56 ± .1
	verified (%)	14.70 ± .07	10.65 ± .3	13.78 ± 1.	9.89 ± .3	15.26 ± .6	9.54 ± .1	12.06 ± .9	8.92 ± .2	10.81 ± .8	9.67 ± .4
With r	-std.(%)- verified (%)	5.80 ± .04 14.54 ± .07	3.16 ± .06 10.46 ± .08	5.16 ± .3 13.37 ± .3	3.06 ± .05 9.91 ± .3	6.15 ± .2 14.43 ± .3	2.95 ± .05 9.48 ± .2	3.83 ± .2 11.01 ± .5	2.57 ± .1 8.83 ± .3	3.29 ± .04 10.18 ± .5	2.73 ± .4 9.49 ± .4
Table 4: Mean and standard deviation of the family of 10small models on MNIST with = 0.3. Baseline
is CROWN-IBP with epoch=140 and lr_decay_SteP=20. Like in CROWN-IBR We run each model 5 times to
compute the mean and standard deviation. “Copied” are results from (Zhang et al., 2019b).
18