Under review as a conference paper at ICLR 2020
Frontal low-rank random tensors for high-
ORDER FEATURE REPRESENTATION
Anonymous authors
Paper under double-blind review
Ab stract
Representing high-order (second-order or higher) information in deep neural net-
works is essential in many tasks such as fine-grained visual understanding and
multi-modal information fusion. Bilinear models are often used to extract second-
order information. As a basis, extracting higher-order information requires extra
computation. In this paper, we propose an approach to representing high-order
information via a simple yet effective bilinear form. Specifically, our contribution
is two-fold: (1) From the multilinear perspective, we derive a bilinear form of
low complexity, assuming that the three-way tensor has low-rank frontal slices.
(2) Rather than learning the tensor entries from data, we sample the entries from
different underlying distributions, and prove that the underlying distribution in-
fluences the information order. We perform temporal action segmentation exper-
iments to evaluate our method. The results demonstrate that our bilinear form,
employed as intermediate layers in deep neural networks, is computationally effi-
cient; meanwhile it is effective as it achieves new state-of-the-art results on public
benchmarks.
1	Introduction
A standard deep convolutional neural network mainly comprises convolutional layers and pooling
layers. Despite effectiveness, such layers can only extract first-order information and hence are
not suitable for fine-grained recognition and information fusion. It is reported in the literature that
modeling second-order feature interactions (e.g. statistical covariance and feature co-occurrence) is
essential for a wide range of visual tasks like fine-grained recognition (Lin et al., 2018b; Kong &
Fowlkes, 2017), human action understanding (Girdhar & Ramanan, 2017; Cherian et al., 2017) and
visual question answering (Kim et al., 2018; Fukui et al., 2016). When considering higher-order
information, the extracted features are more discriminative and yield impressive performance (Cui
et al., 2017; Koniusz et al., 2017).
Although modeling high-order feature interaction has many successful applications, most existing
methods have a critical drawback: The computational cost increases exponentially with respect to
the information order and the input feature dimension. Let us take bilinear pooling for second-order
information extraction as an example. Given two feature vectors x ∈ RDX and y ∈ RDY , a generic
bilinear model (Ben-Younes et al., 2017, Eq. (2)) is given by:
z = T ×1 x ×2y,	(1)
where T ∈ RDX ×DY ×DZ is a three-way tensor, and the operations ×1 and ×2 are the mode-1 and
mode-2 multiplication, respectively. Despite being a powerful scheme, such a model tends to suffer
from the curse of dimensionality and intractable computation (Bellman, 2015). Specifically, the
number of free parameters in T grows cubically as the feature dimensionality, i.e. O(DX DY DZ),
hence limiting its use in large-scale scenarios.
To reduce the complexity, a common solution is to impose specific assumptions on the structure of
T in Eq. (1). In the work of Kim et al. (2016); Ben-Younes et al. (2017); Kong & Fowlkes (2017),
low-rank assumption on T is employed. In the work of Gao et al. (2016), count sketch is used
to extract approximated second-order information, which can be represented by a shorter feature
vector than the full second-order information. Although such conventional bilinear pooling methods
can extract second-order information effectively, extracting higher-order information requires extra
operations (Cui et al., 2017) and may further increase the computational cost.
1
Under review as a conference paper at ICLR 2020
In this paper, we aim at deriving a simple bilinear model, with which changing the order of informa-
tion does not involve extra computation. Therefore, our investigations have two aspects: (1) From
the multilinear perspective, we assume that each frontal slice of T — but not T itself — is a low-
rank matrix. Then we derive a new bilinear form that first reduces the feature dimension and then
performs vector outer product. Different from Kim et al. (2016) and Ben-Younes et al. (2017), that
first lift the feature dimension and then perform Hadamard product, our method has runtime com-
plexity O(D√d + d) and space complexity O(Dyfd) compared to O(Dd + d) and O(Dd) in their
works1. Thus, our bilinear operation is lightweight and can be employed in deep neural networks
as intermediate layers in an efficient manner. (2) Rather than learning the model parameters from
data, the entries of each frontal slice are determined by random projection matrices drawn from a
specific distribution. Our key insight is that, while the low-rank structure allows a reduction in the
number of parameters, the loss in representational power is compensated by the random projection
matrices. In particular, we show that when these random matrices are sampled with different un-
derlying distributions, the model approximates feature maps of different reproducing kernel Hilbert
spaces (RKHSs). For example, when they are sampled from the Rademacher distribution, the model
approximates the multiplication of linear kernels (cf. Theorem 1). When they are sampled from
the Gaussian distribution with orthogonality constraints, the model approximates multiplication of
Gaussian kernels (cf. Theorem 2). Hence, we can explicitly manipulate the model capacity without
sacrificing the computational efficiency.
To verify the effectiveness of our method, we perform experiments on fine-grained action parsing,
in which our bilinear model with random tensor entries is used as intermediate layers in temporal
convolutional deep neural networks. The experimental results show that our method yields superior
efficiency and performance to other related methods on different datasets.
Our contributions can be summarized as follows:
•	We derive a novel bilinear model with a random three-way tensor. Using low-rank decomposition
of each frontal slice, our method significantly reduces the number of parameters of the bilinear
model, and hence can serve as a computationally efficient feature fusion operation.
•	Based on different tensor entry distributions, we prove that the proposed random tensors can
estimate the feature maps to reproducing kernel Hilbert spaces (RKHSs) with different composi-
tional kernels. Therefore, the order of information is influenced by such underlying distributions.
•	We combine our method with state-of-the-art deep neural networks for action segmentation and
produce superior results.
2	Related Work
Representing high-order information in practice. Due to the computational cost, second-order
information is mostly considered in various practical tasks, such as fine-grained image understand-
ing (Carreira et al., 2012; Gao et al., 2016; Yu & Salzmann, 2018; Koniusz et al., 2017; Li et al.,
2017a; Lin et al., 2015; 2018b;a; Kong & Fowlkes, 2017; Li et al., 2018; Wei et al., 2018; Gou
et al., 2018), fine-grained action understanding (Feichtenhofer et al., 2016; Girdhar & Ramanan,
2017; Cherian et al., 2017; Zhang et al., 2019), visual question answering (Yu et al., 2018; Ben-
Younes et al., 2017; Kim et al., 2018; Fukui et al., 2016) and beyond. In many studies, second-order
information is extracted only once before the classification layer in a deep neural net (Yu & Salz-
mann, 2018; Lin et al., 2015; 2018b;a; Kong & Fowlkes, 2017; Wang et al., 2017; Diba et al., 2017;
Feichtenhofer et al., 2016). For example, Lin et al. (2018b) compute the vector outer product to
merge outputs from two individual CNN streams. Wei et al. (2018) transform CNN features to the
compact Grassmann manifold using singular-value decomposition. Such methods either lift the fea-
ture dimension from D to D2, or introduce expensive computation in both forward and backward
passes, and hence are not applicable as intermediate layers in deep neural networks. Consequently,
extracting higher-order (≥ 3) information is even more challenging, although it is reported that
higher-order information is more discriminative. Koniusz et al. (2017) compute higher-order feature
occurrence in the bag-of-words pipeline, rather than in deep neural networks.
1 D and d are input and output feature dimensions of the bilinear model, respectively; usually, D d.
2
Under review as a conference paper at ICLR 2020
Low-rank tensor structure. There exist many investigations on low-rank tensor decomposition
for bilinear models. For instance, Kim et al. (2016) assume that each frontal slice of the three-way
tensor can be decomposed into two low-rank matrices, and the fusion of the two input features can
then be achieved by matrix multiplication and Hadamard product. To improve the performance of
visual question answering, Yu et al. (2017) introduce more operations after the low-rank bilinear
pooling (Kim et al., 2016) such as dropout, power normalization, L2 normalization and so forth.
Ben-Younes et al. (2017) rely on Tucker decomposition of the tensor, producing three matrices and
a smaller core three-way tensor.
High-order information approximation. Representing high-order information explicitly can lead
to combinatorial explosions. To avoid the curse of dimensionality, feature approximation methods
based on reproducing kernel Hilbert space (RKHS) theories have been recently investigated. For
example, Kar & Karnick (2012) use binary random entries to approximate inner product kernels,
especially the p-th order polynomial kernels. Gao et al. (2016) and Pham & Pagh (2013) use tensor
sketch to approximate polynomial kernels, which has lower approximation error bound but higher
computational cost. Yu et al. (2016) use orthogonal random features to approximate feature maps of
Gaussian kernels. To boost the computational speed, a structured version with normalized Walsh-
Hadamard matrices is proposed. Such feature approximation methods also improve the efficiency
of higher-order information extraction. For example, based on count sketch (Gao et al., 2016), Cui
et al. (2017) extract 4th-order information from CNN features in an efficient manner.
Herein, we design a new bilinear form with lower complexity than Kim et al. (2016), Yu et al. (2017)
and Ben-Younes et al. (2017). Also, it provides a generic framework to approximate feature maps
of RKHSs with compositional kernels. Via sampling tensor entries from different distributions, our
method shows that the output feature vector lies within certain RKHS, and hence we can manipulate
the model capacity while retaining the same computational complexity.
3	Proposed Method
In this section we first introduce how to decrease the number of parameters in the bilinear model
Eq. (1) via low-rank assumption. Afterwards, we present our investigations on how to approximate
feature maps of kernels via random projection.
3.1	Tensor frontal low-rank approximation
Here we follow the tensor notations in Kolda & Bader (2009). Eq. (1) can be re-written in terms of
matrix-vector multiplication as
Z = T(3) vec(x 0 y),	(2)
where T(3)is the mode-3 matricization of T, vec(∙) denotes column-wise vectorization of a matrix,
and 0 denotes vector outer product. In other words, the bilinear operation in Eq. (1) is equivalent
to first computing the correlation matrix between the two features and then performing a linear
projection.
It follows from Eq. (2) that each entry of the output feature vector z is a weighted sum of all the
entries in the correlation matrix x 0 y, i.e.,
DX DY
zk = hvec(T [:, :, k]), vec(x 0y)i =	T[i,j,k]vf(i,j),	(3)
i=1 j=1
where v := vec(x 0 y). If the frontal matrix T[:, :, k] is a rank-one matrix, i.e., T[:, :, k] = e 0 f
for some e ∈ RDX and f ∈ RDY , then we can rewrite Eq. (3) as zk = hvec(e 0 f), vec(x 0 y)i =
he, xihf, yi.
Thus, we define the projection matrices E = [e1, e2, ..., eM]T and F = [f1, f2, ..., fN]T for two
sets of vectors {ei}iM=1 ⊂ X and {fj}jN=1 ⊂ Y with M ≤ DX and N ≤ DY . Then, the fusion map
φ : RDX × RDY → RMN can be defined as
z := φ(x, y) = vec ((E x) 0 (F y)) .	(4)
3
Under review as a conference paper at ICLR 2020
If We assume further that T[:,:, k] is a rank-R matrix, i.e., T[:,:, k] = PR=I er 0 fj, We obtain
Z := φ(x, y) = Vec (X(Erx) 0 (Fry)) ,	(5)
Where Er = [er1, er2, ..., erM]T and Fr = [f1r , f2r , ..., fNr ]T for r = 1, 2, ..., R. With such a loW-rank
assumption, We avoid computing the high-dimensional correlation matrix x0y, Which considerably
reduces the model parameters from DXDY DZ to R(M DX + NDY ) With a small value of R.
A similar loW-rank assumption is also used in Ben-Younes et al. (2017) and Kim et al. (2016), in
Which the tWo input feature vectors are first projected to a common vector space and then fused
via the Hadamard product. Assuming the input feature vectors are of the same dimension D and
the output feature vector is of dimension d, then such operation requires O(Dd + d) operations to
compute and requires O(Dd) memory to store. In contrast, our method requires O(D√d + d) for
computation and O(Dvd) for storage. Since in practice it normally requires more dimensions to
represent a higher-order feature, i.e. d D, our method has a consistently better runtime (see Tab.
1) and hence is more suitable to be employed in a sophisticated deep neural netWork.
3.2 Random projection
To compensate for the loss in model capacity caused by the loW-rank assumption, We observe that the
entries and associated distributions of E and F defined in Eq. (4) and Eq. (5) can indeed influence
the model capacity, Without adding or removing learnable parameters, netWork layers, etc.
Inspired by this observation, We propose to manipulate the model capacity by randomly sampling
the entries of E and F from specific distributions and then perform random projection. Unlike
an end-to-end learning via back-propagation, our approach provides an alternative Way of building
expressive representation that is explainable and is simple to use in practice.
Rademacher random projection. Motivated by Kar & Karnick (2012) and Gao et al. (2016), We
specify model parameters, i.e. the projection matrices Er and Fr in the bilinear model (4) or (5),
With random samples from the Rademacher distribution. BeloW We shoW that the bilinear model
given in Eq. (4) unbiasedly approximates, With high probability, a feature map to a reproducing
kernel Hilbert space (RKHS), in Which the associated kernel is the multiplication of tWo linear
kernels in X and Y, respectively.
Theorem 1.	Let Er ∈ RM ×DX and Fr ∈ RN ×DY for any r ∈ {1, 2, ..., R} be Rademacher
random matrices whose entries are determined by an independent Rademacher random variable
σ ∈ {-1, 1}. For any x1, x2 ∈ X and y1, y2 ∈ Y, let z1 = φ(x1, y1) and z2 = φ(x2, y2) be the
output features given by Eq. (4). Define a kernel function by k(z1, z2) = hz1, z2i, then we have
E[k(z1, z2)] = RM N hx1, x2ihy1, y2i.
Next, We characterize the error of such kernel approximations.
Corollary 1. Let	zι	and	Z2	be defined as in Theorem 1. Let	k(zι,	Z2)	= R(zi,	Z2i.	Then, the
following inequality holds:
2MN
P(Ik(Z1, z2) — E[k(Z1, z2)]| > e) ≤ 2eχp (— 2p8R8 ),
(6)
for some constants > 0, andp ≥ 1, R ≥ 1, which are independent of the feature dimensions.
Proofs of both results can be found in Appendix A and B. More details on the constants p and R can
be found in Kar & Karnick (2012). To remove the effect of the scaling factors, We reWrite Eq. (5) as
Z = φ(x, y)
R√MN ∙ Vec(X(Erx)0 (F ry)
(7)
Therefore, Eq. (7) With binary tensor entries is capable of capturing second-order interactions be-
tWeen tWo features. We refer this bilinear form as “RPBinary” in the experiment section.
4
Under review as a conference paper at ICLR 2020
Gaussian random projection. To increase the model capacity, a common approach is to apply
the nonlinear activation function 夕(∙)，which gives
Z := φ(x, y) = Vec (X 2(Erx) 0 2(Fry)) .	(8)
Inspired by Yu et al. (2016), we consider
Er = ɪIM×DxRrPr, Fr = ɪIN×DySTQr with r = 1, 2,...,R,	(9)
σr	ρr
where Rr and Sr are diagonal matrices with diagonal entries sampled i.i.d. from the chi-squared
distributions χ2(DX) and χ2(DY ) with DX and DY degrees-of-freedom, respectively, Pr and Qr
are uniformly distributed random orthogonal matrices2, and IM ×DX and IN ×DY are identity matri-
ces with the first M and N rows, respectively. Here, {σr }rR=1 and {ρr }rR=1 are tunable bandwidth
parameters.
When the nonlinear function in Eq. (8) is given by 夕(Ex) :=，1/M[sin(Ex), CoS(Ex)] and
夕(Fy) := 1`1/N[sin(Fy), CoS(Fy)], the resulting representation approximates feature maps to
the RKHSs corresponding to a composition of Gaussian kernels (see Appendix C for the proof).
Theorem 2.	Let Er ∈ RM ×DX and Fr ∈ RN ×DY for any r ∈ {1, 2, ..., R} be random matrices
whose entries are determined as in Eq. (9). For any x1, x2 ∈ X and y1, y2 ∈ Y, let z1 = φ(x1, y1)
and z2 = φ(x2, y2) be the output features in Eq. (8). Define a kernel function k(z1, z2) = hz1, z2i,
then we have
E[k(z1, z2)]=X exp (-K ) exp (-K )+ b,	(10)
Where b := PR=I PR=r+1 E[Q(Er x1), Ψ(Er' x2 )i]E[Q(F r yi), 中(F r0 y2)i].
A characterization of the variance of k(z1, z2) in Theorem 2 is given in Appendix D, which sug-
gests that higher output feature dimension can reduce the variance of k(z1, z2). Consequently, just
by using the periodic function 夕(∙) and the Gaussian random projection, We can obtain infinite-
order information without extra computational cost. We refer this form as “RPGaussianFull” in the
experiment section. In principal, one can extract different types of high-order information by us-
ing different nonlinear functions in Eq. (8). Further investigations on such problems are currently
beyond our scope.
In practice, when used as an intermediate layer in deep neural networks, Sin and CoS functions are
known to be difficult and unreliable to train using back-propagation (Parascandolo et al., 2016).
Therefore, we perform Taylor expansion of Sin and CoS, and only use the first term. Namely, we
approximate sin(Ex) ≈ Ex and CoS(Ex) ≈ 1. Then, we can discard the nonlinear function 夕(∙)
in Eq. (8), and also can apply scaling factors as in Eq. (7). We refer this approximated version as
“RPGaussian” in our experiments. In addition, we adopt a simpler version Er = ^DDXIM×DχPr
and Fr = √DYIN×DγQr. According to Yu et al. (2016), such a simplified version exhibits
similar empirical behavior to the original version, especially when the feature dimensionality is
high. Moreover, rather than regarding the Gaussian radii as hyper-parameters, we learn them via
back-propagation when employing the model (7) as an intermediate layer in a deep neural network.
Last but not least, it is instructive to note that, in addition to what we have proposed, the distributions
of E and F can be arbitrary. We can even model these distributions using deep generative models,
which is the subject of our future work.
4	Experiments
We conduct experiments for the task of fine-grained temporal action segmentation, which aims at
assigning each individual frame an action label. In such experiments, the two input features x and
2Specifically, Pr and Qr are uniformly distributed on the Stiefel manifold (Yu et al., 2016; Muirhead,
2009).
5
Under review as a conference paper at ICLR 2020
Figure 1: Performance of our RPBinary and RPGaUssian model, versus dimension / rank, on datasets
50Salads and GTEA. In each plot, x-axis is the multiplier on the number of matrix rows N, y-axis
is the respective performance measure, and colors denote different ranks.
GTEA
RPGaussian
y in Eq. (7) and Eq. (8) are identical, and we set the same number of rows (i.e. M = N) to
matrices Er and Fr for all r = 1, . . . , R. Consequently, there only remain two hyper-parameters
in the bilinear model, i.e. the rank R and the number of rows N of matrices. The objectives of
our experiments are two-fold: (i) To verify the effectiveness of our method, we adopt the temporal
convolutional net (TCN) (Lea et al., 2017) due to its simple structure and replace the max pooling in
TCN by bilinear pooling as in Zhang et al. (2019). (ii) To demonstrate how our method can be used
in practice, we propose a bilinear residual module to merge the first and second-order information,
and combine it with the multi-stage temporal convolutional net (MS-TCN) (Farha & Gall, 2019) to
yield state-of-the-art performance.
Datasets and evaluation metrics. We evaluate our method on the 50Salads dataset (Stein &
McKenna, 2013) and the GTEA dataset (Fathi et al., 2011; Li et al., 2015). For a fair comparison, in
our experiments with TCN, we use the identical frame-wise features and temporal resolutions as Lea
et al. (2017). Also, in our experiments with MS-TCN, we use the identical frame-wise features and
temporal resolutions as Farha & Gall (2019). To evaluate the performance, we use three standard
metrics, i.e. frame-wise accuracy, edit score and F1 score as in Lea et al. (2017) and Farha & Gall
(2019). For the F1 score, we consider the intersection-over-union (IoU) ratio of 0.1, 0.25 and 0.5,
and denote them as F1@0.1, F1@0.25 and F1@0.5, respectively. Without explicit mentioning, our
F1 score means F1@0.1. Detailed definitions of these metrics are explained in Lea et al. (2017) and
Farha & Gall (2019). Since each dataset has several splits, we report the results of cross-validation.
4.1	Action segmentation with TCN architecture
We use the default architecture of TCN (Lea et al., 2017), which comprises an encoder and a decoder
with symmetric modules. The convolutional layer in each individual encoder has 64 and 96 filters,
respectively. We train the model using the Adam optimizer (Kingma & Ba, 2014) with a fixed
learning rate of 10-4. Batch size is set to 8, and the training process terminates after 300 epochs.
Ablation study: Comparison between bilinear forms. In this experiment, we set the rank R = 1
and N = D/2 for our methods where D is the input feature dimension to our bilinear model. The
results are shown in the first part of Tab. 1. For the performance evaluation, we repeat the ex-
periment 3 times and report the result with the highest sum of the three metrics. To evaluate the
efficiency, we report the runtime per batch (batch size=8) which is the averaged result after training
6
Under review as a conference paper at ICLR 2020
Table 1: Comparison with different bilinear pooling methods in terms of accuracy/edit score/F1
score and runtime (millisecond). For each metric and each setting, the best result is in boldface.
“LearnableProjection” indicates E and F in Eq. (7) are learned via back-propagation. “RPBinary”
indicates the model Eq. (7) with Rademacher random projection. “RPGaussianFull” and “RPGaus-
sian” indicate the model Eq. (8) with and without 夕(∙)，respectively, in which Gaussian random
projection is employed. In the column of complexity, D and d denote the input and output feature
dimension of our bilinear model, respectively.
Method	Complexity	50Salads		GTEA	
		Runtime	Performance	Runtime	Performance
Ours (RPBinary)	O(D√d + d)	68.3	66.0/65.9/70.9	80.1	65.2/73.2/77.0
Ours (RPGaussian)	θ(D√d + d)	68.9	67.6/65.2/72.9	69.9	66.9/76.5/79.8
Ours (RPGaussianFull)	O(D√d + d)	73.7	64.1/63.4/69.6	70.0	64.5/73.0/78.7
Ours (LearnableProjection)	O(D√d + d)	78.6	66.4/65.0/70.5	81.6	64.8/74.0/77.5
Compact (Gao et al., 2016)	O(D + dlogd)	95.9	67.2/65.8/71.7	120.2	65.9/75.3/78.1
Hadamard (Kim et al., 2016)	O(Dd + d)	83.8	67.7/64.4/71.5	83.6	66.0/76.6/79.0
FBP (Li et al., 2017b)	O((2k + 1)Dd + d)	130.5	64.0/61.0/67.5	127.6	63.4/71.6/74.1
dilated residual module
bilinear residual module
>uoo LXL
Figure 2: A combination of MS-TCN Farha & Gall (2019) and our bilinear pooling method, in
which the blue layers are proposed by Us and the orange layers are proposed by MS-TCn.
6u=ood -eωu≡m
UO-≡z=EE0U
Mod pθzμE-n6φ-J
>uoo Pm
UO-≡z=EE0U
XEuJ əs-m,-əuuelp

with the first split for 300 epochs for each dataset. Overall, the results suggest that “RPGaussian”
and “RPBinary" have comparable performances, and outperform “LearnableProjection” and “RP-
GaUSSianFUll”. While “LearnableProjection” is more flexible, it might require a more sophisticated
training to achieve the same performance as our random projection methods. Also, We suspect that
the inferior performance of “RPGaussianFull” is due to the difficulty of training.
In addition, we compare our methods with three widely used bilinear pooling methods, i.e., Com-
pact (Gao et al., 2016), Hadamard (Kim et al., 2016) and FBP (Li et al., 2017b). We use the same
output feature dimension for fair comparison. The results are shown in the second part of Tab. 1.
They suggest that these three methods perform inferior or comparably in terms of the three metrics
of action parsing, and are clearly less efficient, than our methods.
Ablation study: Investigation on the hyper-parameters of our bilinear forms. Here we investi-
gate the influence of the rank and the output feature dimension of RPGaussian and RPBinary, with
the bilinear model in Eq. (7). Fig. 1 shows the dependence of the model performance on ranks
R ∈ {1,2,4, 8,16} and matrix rows N ∈ {1,2,4,8} X [√D] where [n] denotes the nearest integer
to n. In this case, the output feature dimension is then {1, 4, 16, 64} × D. In all plots, the per-
formance increases consistently with the matrix row N (hence the output feature dimension). This
result is in line with our theoretical analysis on the kernel approximation error bound (see Corollary
1 and Appendix D): Larger values of M and N can yield lower variance upper bounds, hence better
kernel approximation. One can also observe that the performance saturates when further increasing
the output feature dimension. This is due to the limited capacity of the corresponding RKHS. More-
over, one can observe that increasing the rank may not consistently yield better performance. An
optimal rank depends on the dataset and the applied bilinear model.
7
Under review as a conference paper at ICLR 2020
Table 2: Comparison with other models on temporal action segmentation task. The best results are
in boldface, the second-best results are underlined.
	Acc.	50 Salads			F1@0.5	GTEA				
		Edit	F1@0.1	F1@0.25		Acc.	Edit	F1@0.1	F1@0.25	F1@0.5
Ours (RPBinary)	79.9	70.7	~78.0 -	-752-	65.4	77.0	81.4	-865-	84.5	71.7
Ours (RPGaussian)	80.6	71.0	78.4	75.8	66.7	77.2	82.2	86.7	84.3	72.7
Ours (RPGaussianFull)	77.4	67.5	74.9	70.9	607	76.8	82.5	87.3	84.3	71.8
TCN (Lea et al., 2017)	64.7	59.8	68.0	-63.9-	52.6	64.0	-	72.2	69.3	56.0
TDRN (Lei & Todorovic, 2018)	68.1	66.0	72.9	68.5	57.2	70.1	74.1	79.2	74.4	62.7
MS-TCN (Farha & Gall, 2019)	80.7	67.9	76.3	74.0	64.5	76.3	79.0	85.8	83.4	69.8
MS-TCN + Compact (Gao et al., 2016)	81.1	70.7	77.6	-751-	67.1	75.8	80.9	86.0	83.7	70.2
MS-TCN + Hadamard (Kim et al., 2016)	78.3	68.3	75.3	72.4	62.5	77.0	81.5	86.0	83.5	70.7
MS-TCN + FBP (Li et al., 2017b)	78.5	70.0	76.5	73.6	64.6	75.4	79.3	84.0	81.8	69.9
4.2 Action segmentation with MS-TCN
In this section, we demonstrate how to effectively incorporate our method into the state-of-the-art
action parsing network, MS-TCN (Farha & Gall, 2019), to superior performance.
Implementation Details. Based on our previous experimental results, we set rank R = 4 and
N = D/2 for our bilinear model. Furthermore, we propose a bilinear residual module to merge
the first and the second-order information, as illustrated in Fig. 2. First, we use bilinear pooling
to extract the second-order information, and then use a regularized power normalization (Zhang
et al., 2019) to densify the feature and use channel-wise max normalization to re-scale the feature
value (Lea et al., 2017). Afterwards, we use a convolution layer to reduce the feature dimension to
the number of classes. Since the second-order information tends to partition an action into smaller
segments (Zhang et al., 2019, Fig. 1), we use a larger convolution receptive field 25 according to
Lea et al. (2017). To prevent overfitting we use a dropout layer, and then we compute the average
between the first-order information and the second-order information.
Our bilinear residual module is applied at the end of each single stage of MS-TCN. To conduce
fair comparison with the baseline MS-TCN model, we keep other model configurations and the loss
function (including the hyper-parameters) unchanged. Similarly to Farha & Gall (2019), we use the
Adam (Kingma & Ba, 2014) optimizer with a learning rate of 0.0005. The batch size is set to 1.
Result. We compare our method with several state-of-the-art methods on the action segmentation
task. As shown in Tab. 2, our models (especially RPBinary and RPGaussian) consistently outper-
form the state-of-the-arts (TCN, TDRN, MS-TCN), validating the effectiveness of the proposed
bilinear model. Note that, even with RPGaussianFull, which is hard to train by back-propagation,
our bilinear form achieves competitive performance on the GTEA dataset. For a fair and complete
comparison, we further integrate three widely used light-weight bilinear models into the MS-TCN
model, namely, Compact (Gao et al., 2016), Hadamard (Kim et al., 2016) and FBP (Li et al.,
2017b). Again, the proposed bilinear models show superior performance on most of the evaluation
metrics. Together with the results presented in Tab. 1, they clearly demonstrate the representational
power and the computational efficiency of the proposed bilinear models.
5	Conclusion
In this work, we propose a novel bilinear model for fusing high-dimensional features. To reduce the
number of model parameters, we utilize low-rank tensor decomposition. Instead of using element-
wise product as in other works, we use the outer product of the features to model the high-order
correlations among feature channels. To enrich the model representiveness while retaining the num-
ber of parameters, we use random projection to approximate feature maps to reproducing kernel
Hilbert spaces associated with kernel compositions. To validate the effectiveness of our method, we
perform extensive experiments on the action segmentation task, and have achieved state-of-the-art
performance on challenging benchmarks. Our bilinear pooling operation is lightweight, easy to use,
and can serve as a natural tool for fine-grained visual understanding and information fusion. In the
future we will investigate how to combine our work with the deep generative models. That is, in-
stead of sampling entries from a pre-defined distribution, we aim to learn to model these distributions
using deep generative models.
8
Under review as a conference paper at ICLR 2020
References
Richard E Bellman. Adaptive control processes: a guided tour, volume 2045. Princeton university
press, 2015.
Hedi Ben-Younes, Remi Cadene, MatthieU Cord, and Nicolas Thome. Mutan: Multimodal tucker
fusion for visual question answering. In Proceedings of the IEEE international conference on
computer vision, pp. 2612-2620, 2017.
Joao Carreira, Rui Caseiro, Jorge Batista, and Cristian Sminchisescu. Semantic segmentation with
second-order pooling. In European Conference on Computer Vision, pp. 430-443. Springer, 2012.
Anoop Cherian, Piotr Koniusz, and Stephen Gould. Higher-order pooling of cnn features via kernel
linearization for action recognition. In IEEE Winter Conference on Applications of Computer
Vision (WACV), pp. 130-138. IEEE, 2017.
Yin Cui, Feng Zhou, Jiang Wang, Xiao Liu, Yuanqing Lin, and Serge Belongie. Kernel pooling for
convolutional neural networks. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pp. 2921-2930, 2017.
Ali Diba, Vivek Sharma, and Luc Van Gool. Deep temporal linear encoding networks. In Pro-
ceedings of the IEEE conference on Computer Vision and Pattern Recognition, pp. 2329-2338,
2017.
Yazan Abu Farha and Juergen Gall. Ms-tcn: Multi-stage temporal convolutional network for action
segmentation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.
Alireza Fathi, Xiaofeng Ren, and James M Rehg. Learning to recognize objects in egocentric activ-
ities. In IEEE Conference On Computer Vision and Pattern Recognition (CVPR), pp. 3281-3288.
IEEE, 2011.
Christoph Feichtenhofer, Axel Pinz, and Andrew Zisserman. Convolutional two-stream network
fusion for video action recognition. In IEEE Conference on Computer Vision and Pattern Recog-
nition, pp. 1933-1941, 2016.
Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and Marcus Rohrbach.
Multimodal compact bilinear pooling for visual question answering and visual grounding.
arXiv:1606.01847, 2016.
Yang Gao, Oscar Beijbom, Ning Zhang, and Trevor Darrell. Compact bilinear pooling. In Proceed-
ings of the IEEE conference on computer vision and pattern recognition, pp. 317-326, 2016.
Rohit Girdhar and Deva Ramanan. Attentional pooling for action recognition. In Advances in Neural
Information Processing Systems, pp. 34-45, 2017.
Mengran Gou, Fei Xiong, Octavia Camps, and Mario Sznaier. Monet: Moments embedding net-
work. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.
3175-3183, 2018.
Purushottam Kar and Harish Karnick. Random feature maps for dot product kernels. In Artificial
Intelligence and Statistics, pp. 583-591, 2012.
Jin-Hwa Kim, Kyoung-Woon On, Woosang Lim, Jeonghee Kim, Jung-Woo Ha, and Byoung-Tak
Zhang. Hadamard product for low-rank bilinear pooling. arXiv preprint arXiv:1610.04325, 2016.
Jin-Hwa Kim, Jaehyun Jun, and Byoung-Tak Zhang. Bilinear Attention Networks. In Advances in
Neural Information Processing Systems 31, pp. 1571-1581, 2018.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Tamara G Kolda and Brett W Bader. Tensor decompositions and applications. SIAM review, 51(3):
455-500, 2009.
9
Under review as a conference paper at ICLR 2020
Shu Kong and Charless Fowlkes. Low-rank bilinear pooling for fine-grained classification. In IEEE
Conference on Computer Vision and Pattern Recognition (CVPR),pp. 7025-7034. IEEE, 2017.
Piotr Koniusz, Fei Yan, Philippe-Henri Gosselin, and Krystian Mikolajczyk. Higher-order occur-
rence pooling for bags-of-words: Visual concept detection. IEEE transactions on pattern analysis
and machine intelligence, 39(2):313-326, 2017.
Colin Lea, Michael D. Flynn, Rene Vidal, Austin Reiter, and Gregory D. Hager. Temporal convolu-
tional networks for action segmentation and detection. In IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pp. 1003-1012, July 2017.
Peng Lei and Sinisa Todorovic. Temporal deformable residual networks for action segmentation in
videos. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.
6742-6751, 2018.
Peihua Li, Jiangtao Xie, Qilong Wang, and Wangmeng Zuo. Is second-order information helpful
for large-scale visual recognition. In IEEE international conference on computer vision (ICCV).
IEEE, pp. 2070-2078, 2017a.
Peihua Li, Jiangtao Xie, Qilong Wang, and Zilin Gao. Towards faster training of global covariance
pooling networks by iterative matrix square root normalization. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pp. 947-955, 2018.
Yanghao Li, Naiyan Wang, Jiaying Liu, and Xiaodi Hou. Factorized bilinear models for image
recognition. In Proceedings of the IEEE International Conference on Computer Vision, pp. 2079-
2087, 2017b.
Yin Li, Zhefan Ye, and James M Rehg. Delving into egocentric actions. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pp. 287-295, 2015.
Tsung-Yu Lin, Aruni RoyChowdhury, and Subhransu Maji. Bilinear cnn models for fine-grained
visual recognition. In Proceedings of the IEEE international conference on computer vision, pp.
1449-1457, 2015.
Tsung-Yu Lin, Subhransu Maji, and Piotr Koniusz. Second-order democratic aggregation. In Pro-
ceedings of the European Conference on Computer Vision (ECCV), pp. 620-636, 2018a.
Tsung-Yu Lin, Aruni RoyChowdhury, and Subhransu Maji. Bilinear convolutional neural networks
for fine-grained visual recognition. IEEE transactions on pattern analysis and machine intelli-
gence, 40(6):1309-1322, 2018b.
Robb J Muirhead. Aspects of multivariate statistical theory, volume 197. John Wiley & Sons, 2009.
Giambattista Parascandolo, Heikki Huttunen, and Tuomas Virtanen. Taming the waves: sine as
activation function in deep neural networks. 2016.
Ninh Pham and Rasmus Pagh. Fast and scalable polynomial kernels via explicit feature maps. In
Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and
data mining, pp. 239-247. ACM, 2013.
Sebastian Stein and Stephen J McKenna. Combining embedded accelerometers with computer vi-
sion for recognizing food preparation activities. In Proceedings of the 2013 ACM international
joint conference on Pervasive and ubiquitous computing, pp. 729-738. ACM, 2013.
Qilong Wang, Peihua Li, and Lei Zhang. G2denet: Global gaussian distribution embedding network
and its application to visual recognition. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 2730-2739, 2017.
Xing Wei, Yue Zhang, Yihong Gong, Jiawei Zhang, and Nanning Zheng. Grassmann pooling as
compact homogeneous bilinear pooling for fine-grained visual classification. In Proceedings of
the European Conference on Computer Vision (ECCV), pp. 355-370, 2018.
Felix Xinnan X Yu, Ananda Theertha Suresh, Krzysztof M Choromanski, Daniel N Holtmann-Rice,
and Sanjiv Kumar. Orthogonal random features. In Advances in Neural Information Processing
Systems, pp. 1975-1983, 2016.
10
Under review as a conference paper at ICLR 2020
Kaicheng Yu and Mathieu Salzmann. Statistically-motivated second-order pooling. In Proceedings
ofthe European Conference on Computer Vision (ECCV),pp. 600-616, 2018.
Zhou Yu, Jun Yu, Jianping Fan, and Dacheng Tao. Multi-modal factorized bilinear pooling with
co-attention learning for visual question answering. IEEE International Conference on Computer
Vision (ICCV), pp. 1839-1848, 2017.
Zhou Yu, Jun Yu, Chenchao Xiang, Jianping Fan, and Dacheng Tao. Beyond bilinear: Generalized
multimodal factorized high-order pooling for visual question answering. IEEE Transactions on
Neural Networks and Learning Systems, 29(12):5947-5959, 2018.
Yan Zhang, Siyu Tang, Krikamol Muandet, Christian Jarvers, and Heiko Neumann. Local temporal
bilinear pooling for fine-grained action parsing. In Proceedings IEEE Conf. on Computer Vi-
sion and Pattern Recognition (CVPR), June 2019. URL https://arxiv.org/abs/1812.
01922.
Appendix
A Proof of Theorem 1
Proof. It follows from Eq. (5) and the property of an inner product of rank-one operators that
k(z1, z2) := hz1, z2i
R
vec(ErX1 乳 Fryι), X Vec(Erx2 乳 Fry2)
r=1
RMN
RR
X X(vec(ErXi 乳 Fryι), Vec(EJrX2 乳 Fr'n介软MN
r=1 r0=1
RR
XX Erx1,Er0x2	Fry1, Fr0y2
r=1 r0=1
RR(M	、/ N
XX Xher,Xiiher0,X2i	I Xhfr,yf g
r=1 r0=1 i=1	j=1
(11)
Then, it follows that
RR(M	、/ N	\
E[k(zi, z2)] = XX X E[heir, Xiiheir0, X2i]	X E[hfjr, yiihfjr0, y2i]
r=i r0=i i=i	j =i
R / M	、/ N	\
= X XE[heir,Xiiheir,X2i]	XE[hfjr,yiihfjr,y2i]
r=i i=i	j=i
RR / M	、/ N
+X X	X E[heir, Xiiheir0, X2i]	IX E[hfjr, yiihfjr0, y2i]
r=i r0 =r+i i=i	j =i
= RMNhXi, X2ihyi, y2i.
The last equation follows from Kar & Karnick (2012, Lemma 2) and the fact that eir and fjr are
zero-mean random variables for all i = 1,...,M, j = 1,...,N and r = 1,...,R.	□
B Proof of Corollary 1
Proof. Let Wij := hei, Xiihei, X2ihfj, yiihfj, y2i for i = 1, . . . , M andj = 1, . . . , N. For each
Wij , it follows from Kar & Karnick (2012, Lemma 4) that
-p2f(pR2)2 ≤ Wij ≤ P2f(pR2)2,
1
Under review as a conference paper at ICLR 2020
1	∙ .1	. 1	Γ∙	1 ∙ .	. 1	.	⅛,	1	1 ,Λ ~.	1 T	f /	∖	EI
where we assume without loss of generality that p ≥ 1 and R ≥ 1. In our case, f(x) = x. Then,
We have -p4R4 ≤ Wij ≤ p4R4. Let SMN ：= R PM=I PN=I Wij. Hence, We have E[Smn]=
MNhx1, x2ihy1, y2i. Then, it follows from Hoeffding’s inequality that, for all > 0,
P(|SMN - E[SMN]| ≥ ) ≤ 2 exp
2MN
This concludes the proof.
(12)
□
C Proof of Theorem 2
Proof. Let R = 1. Then, k(zι, z2) ：=〈zi, z2i =〈夕(Exι),夕(Ex2)ih夕(Fxι), W(FX2) where
夕(Ex) := √L- [sin(e>x),..., sin(eMx), cos(e>x),..., cos(eMx)],
中(Fy) ：= √N [sin(f>y),..., sin(fM y), cos(f>y),..., cos(fM y)].
With E = σIM×DχRP and F = PIN×DγSQ, we have
E[k(z1, z2)] = E[hW(Ex1), W(Ex2)i]E[hW(Fy1), W(Fy2)i]
exp
kx1 - x2k2 ʌ…n (	^1- y k2 ∖
-2σ- eexp〈	2ρ2-)
where the last equality follows from Yu et al. (2016, Theorem 1). For R > 1, we have
RR
k(z1, z2) ：= hz1, z2i = XX
hW(Erx1), W(Er0x2)ihW(Frx1), W(Fr0x2)i.
r=1 r0 =1
Hence, with Er = σrIM×DχRrPr and Fr = PrIN×DγSrQr,
RR
E[k(z1, z2)] = X X E[hW(Erx1), W(Er0x2)i]E[hW(Fry1), W(Fr0y2)i]
r=1 r0 =1
R
X E[hW(Erx1), W(Erx2)i]E[hW(Fry1), W(Fry2)i]
r=1
RR
+X X E[hW(Erx1), W(Er0x2)i]E[hW(Fry1), W(Fr0y2)i]
r=1 r0 =r+1
R
X exp
r=1
YI) eχp (-fefl)
RR
+X X E[hW(Erx1), W(Er0x2)i]E[hW(Fry1), W(Fr0y2)i]
r=1 r0 =r+1
R
X exp
r=1
YI) exp (-⅛1)
+b
where b ：= PrR=1 PrR0=r+1 E[hW(Erx1), W(Er0x2)i]E[hW(F ry1), W(F r0y2)i].
□
D Approximation error b ound for Gaussian random projection
We characterize the variance of k(z1, z2) used in Theorem 2. To simplify the presentation, we focus
on the case that R = 1.
2
Under review as a conference paper at ICLR 2020
Corollary 2. Let z1, z2, and k(z1, z2) be defined as in Theorem 2, as well as R = 1, a = kx1 -
X2∣∣2∕σ and b = ∣∣yι 一 y2∣∣2∕ρ. Then, there exist functions f and g such that
Var(k(zι, z2)) ≤ A ∙ B + A ∙ C + B ∙ D,	(13)
Proof. Let R=1. Then, we have
C
D
k(zι, z2)=〈夕(Exι),夕(Ex2)ih夕(Fxι),夕(Fx2)i .
×----V-----}×---V-----}
UV
Since U and V are independent, we have
Var(k(z1,z2)) = Var(U)Var(V) + Var(U)(E[V])2 +Var(V)(E[U])2,	(14)
where
(E[U])2 = (E 心(Exι),以Ex2)i])2
Var(U )=Var(hψ(Ex1),ψ(Ex2)i)
(E[V ])2 = (E 心(Fyι)”(Fy2)i])2
Var(V) = Var(h^(Fyι),以Fy2)i)
It follows from Yu et al. (2016, Theorem 1) that
(E[U])2 = exp 一
kχι - χ2k2))2
(E[V])2 =Gp (-%泮))2.
Let a = ∣x1 一 x2∣22∕σ and b = ∣y1 一 y2∣22∕ρ. Then, by Yu et al. (2016, Theorem 1), there exists
a function f and g such that
Var(U)
Var(V)
Substituting everything back into (14) yields the result.
□
3