Under review as a conference paper at ICLR 2020
NAMSG: An Efficient Method For Training
Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
We introduce NAMSG, an adaptive first-order algorithm for training neural net-
works. The method is efficient in computation and memory, and is straightforward
to implement. It computes the gradients at configurable remote observation points,
in order to expedite the convergence by adjusting the step size for directions with
different curvatures in the stochastic setting. It also scales the updating vector
elementwise by a nonincreasing preconditioner to take the advantages of AMS-
GRAD. We analyze the convergence properties for both convex and nonconvex
problems by modeling the training process as a dynamic system, and provide a
strategy to select the observation factor without grid search. A data-dependent
regret bound is proposed to guarantee the convergence in the convex setting. The
method can further achieve a O(log(T)) regret bound for strongly convex func-
tions. Experiments demonstrate that NAMSG works well in practical problems
and compares favorably to popular adaptive methods, such as ADAM, NADAM,
and AMSGRAD.
1	Introduction and related work
Training deep neural networks (Collobert et al., 2011; Hinton et al., 2012; Amodei et al., 2016; He
et al., 2016) with large datasets costs a huge amount of time and computational resources. Efficient
optimization methods are urgently required to accelerate the training process.
First-order optimization methods (Robbins & Monro, 1951; Polyak, 1964; Bottou, 2010; Sutskever
et al., 2013; Kingma & Ba, 2015; Bottou et al., 2018) are currently the most popular for training
neural networks. They are easy to implement since only first-order gradients are introduced as input.
Besides, they require low computation overheads except for computing gradients, which is of the
same computational complexity as just evaluating the function. Compared with second-order methods
(Nocedal, 1980; Martens, 2010; Byrd et al., 2016), they are more effective to handle gradient noise.
Sutskever et al. (2013) show that the momentum is crucial to improve the performance of SGD.
Momentum methods, such as HB Polyak (1964), can amplify steps in low-curvature eigen-directions
of the Hessian through accumulation, although careful tuning is required to ensure fine convergence
along the high-curvature directions. Sutskever et al. (2013) also rewrite the Nesterov’s Accelerated
Gradient (NAG) (Nesterov, 1983) in a momentum form, and show the performance improvement over
HB. The method computes the gradient at a observation point ahead of the current point along the last
updating direction. They illustrate that NAG suppresses the step along high curvature eigen-directions
in order to prevent oscillations. However, all these approaches are approximation of their original
forms derived for exact gradients, without fully study on gradient noise. Kidambi et al. (2018) show
the insufficiency of HB and NAG in stochastic optimization, especially for small minibatches. They
further present ASGD (Jain et al., 2018) and show significant improvements. However, the method
requires tuning of 3 parameters, leading to huge costs that impede its practical applications.
Among variants of SGD methods, adaptive methods that scale the gradient elementwise by some form
of averaging of the past gradients are particularly successful. ADAGRAD (Duchi et al., 2011) is the
first popular method in this line. It is well-suited for sparse gradients since it uses all the past gradients
to scale the update. Nevertheless, it suffers from rapid decay of step sizes, in cases of nonconvex
loss functions or dense gradients. Subsequent adaptive methods, such as RMSPROP (Tieleman &
Hinton., 2012), ADADELTA (Zeiler, 2012), ADAM (Kingma & Ba, 2015), and NADAM (Dozat,
2016), mitigate this problem by using the exponential moving averages of squared past gradients.
1
Under review as a conference paper at ICLR 2020
However, Reddi et al. (2018) show that ADAM does not converge to optimal solutions in some
convex problems, and the analysis extends to RMSPROP, ADADELTA, and NADAM. They propose
AMSGRAD, which fixes the problem and shows improvements in experiments.
In this paper, we propose NAMSG, that is an efficient first-order method for training neural networks.
The name is derived from combining a configurable NAG method (CNAG) and AMSGRAD. NAMSG
computes the stochastic gradients at configurable observation points ahead of the current parameters
along the last updating direction. Nevertheless, instead of approximating NAG for exact gradients,
it adjusts the learning rates for eigen-directions with different curvatures to expedite convergence
in the stochastic setting, by selecting the observation distance. It also scales the update vector
elementwisely using the nonincreasing preconditioner of AMSGRAD. We analyze the convergence
properties by modeling the training process as a dynamic system, reveal the benefits of remote
gradient observations and provide a strategy to select the observation factor without grid search.
A regret bound is introduced in the convex setting, and it is further improved for strongly convex
functions. Finally, we present experiments to demonstrate the efficiency of NAMSG in real problems.
2	The NAMSG scheme
Before further description, we introduce the notations following Reddi et al. (2018), with slight abuse
of notation. The letter t denotes iteration number, d denotes the dimension of vectors and matrices,
denotes a predefined positive small value, and S+d denotes the set of all positive definite d × d matrix.
For a vector a ∈ Rd and a matrices M ∈ Rd × Rd, we use a/M to denote M-1a, diag(a) to denote
a square diagonal matrix with the elements of a on the main diagonal, Mi to denote the ith row of M,
and √M to denote M 1/2. For any vectors a, b ∈ Rd, We use √a for elementwise square root, a2 for
elementwise square, a/b for elementwise division, and max(a, b) to denote elementwise maximum.
For any vector θi ∈ Rd, θi,j denotes its jth coordinate where j ∈ {1, 2, . . . , d}. We define F ⊂ Rd
as the feasible set of points. Assume that F has bounded diameter D∞, i.e. kx - yk ≤ D∞ for any
χ,y ∈ F, and ∣∣Vft(χ)k∞≤G∞, kVft(χ)k1 ≤G1 for all X ∈ F. The projection operation is defined
as ΠF,A(y) = arg minx∈F kA1/2(x - y)k for A ∈ S+d and y ∈ Rd.
In the context of machine learning, we consider the minimization problem of a stochastic function,
min F(x) =Eξ[f(x,ξ)],	(1)
x∈Rd
where x is a d dimensional vector consisting of the parameters of the model, and ξ is a random datum
consisting of an input-output pair. Since the distribution of ξ is generally unavailable, the optimizing
problem (1) is approximated by minimizing the empirical risk on the training set {ζ1, ζ2, ..., ζN}, as
min L(X) = W X f(x,Zi).	⑵
x∈Rd	N
i=1
In order to save computation and avoid overfitting, it is common to estimate the objective function
and its gradient with a minibatch of training data, as
ft(x) = b X f(x,Zi),	and Vft(X) = b X Vf (x,Zi),	(3)
i∈St	i∈St
where the minibatch St ⊂ {1, 2, ..., N}, and b = |St| is the size of St.
Firstly, we propose a configurable NAG method (CNAG). Since the updating directions are partially
maintained in momentum methods, gradients computed at observation points, which lie ahead of the
current point along the last updating direction, contain the predictive information of the forthcoming
update. The remote observation points are defined as Xt = Xt 一 ηtUt-1 where ut-1 is the updating
vector, and X 1 = X1.
By computing the gradient at a configurable observation point Xt, and substituting the gradient with
the observation gradient in the HB update, we obtain the original form of CNAG, as
mt = βtmt-1 + (1 ― βt)Vft(Xt)
Xt+1 = Xt 一 αtmt	(4)
X t+1 = Xt — (1 + ηt)ɑtmt,
2
Under review as a conference paper at ICLR 2020
where αt, βt, ηt are configurable coefficients, and m0 = 0. The observation distance ηt can be
configured to accommodate gradient noise, instead of ηt = βt in NAG (Sutskever et al., 2013).
Both Xt and Xt are required to update in (4). To make the method more efficient, We simplify the
update by approximation. Assume that the coefficients αt, β1t, and ηt, change very slowly between
adjacent iterations. Substituting Xt by Xt + ηt-ιαt-ιmt-ι, we obtain the concise form of CNAG, as
mt = βtmt-i + (1 — βt)Vft(xt)
Xt+1 = Xt — at ((1 — μt)mt + μtVft(xt)),
(5)
where the observation factor μt = ηt(1 — βt)∕βt, and we use X instead of X for simplicity.
In practical computation of CNAG, we further rearrange the update form as
mt = t~γ∖—βtm—∖8tmt-i + at(1 — βt)(1 — μt) Vft(Xt)
αt-i (1 — μt-i)	(6)
Xt+1 = Xt — m t — atμt Vft(Xt),
where only 3 scalar vector multiplications and 3 vector additions are required per iteration besides
the gradient computation. Hereinafter, we still use (5) for simplicity in expressions.
Then, we study the relation of CNAG and ASGD, that guides the selection of the momentum
coefficient. Jain et al. (2018) shows that ASGD improves on SGD in any information-theoretically
admissible regime. By taking a long step as well as short step and an appropriate average of both of
them, ASGD tries to make similar progress on different eigen-directions. It takes 3 hyper-parameters:
short step a, long step parameter κ, and statistical advantage parameter ξ. a is the same as the
step size in SGD. For convex functions, K is an estimation of the condition number. The statistical
advantage parameter ξ ≤ Kc captures trade off between statistical and computational condition
numbers, and ξ《√κ in high stochasticity regimes. These hyper-parameters vary in large ranges,
and are difficult to estimate. The huge costs in tuning limits the application of ASGD.
The appendix shows that CNAG is a more efficient equivalent form of ASGD. For CNAG with
constant hyper-parameters, the momentum coefficient βt = β =(K — 0.49ξ)∕(κ + 0.7ξ). Since the
condition number is generally large in real high dimension problems, and the statistical advantage
parameter ξ ≤ √κ, β is close to 1. To sum up, the equivalence of CNAG and ASGD shows that
in order to narrow the gap between the step sizes on eigen-directions with different curvatures, the
momentum coefficient β should be close to 1.
Finally, we form NAMSG by equipping CNAG with the nonincreasing preconditioner of AMSGRAD,
and project the parameter vector X into the feasible set F. Algorithm 1 shows the pseudo code of
NAMSG. Compared with AMSGRAD, NAMSG requires low computation overheads, as a scalar
vector multiplication and a vector addiction per iteration, which are much cheaper than the gradient
estimation. Almost no more memory is needed if the vector operations are run by pipelines. In
most cases, especially when weight decay is applied for regularization, which limits the norm of the
parameter vectors, the projection can also be omitted in implementation to save computation.
3 An analysis on the effect of remote gradient observations
In Algorithm 1, the observation factor μt is configurable to accelerate convergence. However, it is
costly to select it by grid search. In this section we analyze the convergence rate in a local stochastic
quadratic optimization setting by investigating the optimizing process as a dynamic system, and
reveal the effect of remote gradient observation for both convex and non-convex problems. Based
on the analysis, we provide the default values and a practical strategy to set the observation factor
without grid search.
The problem (1) can be approximated locally as a stochastic quadratic optimization problem, as
min Φ(x) = L(X — X*)T H(X — X*),
χ∈F	2
(7)
where F is a local set of feasible parameter points. In the problem, the gradient observation is noisy
as Vft(X) = vΦ(x) + gt, where gt is the gradient noise.
3
Under review as a conference paper at ICLR 2020
Algorithm 1 NAMSG Algorithm
Input: initial parameter vector xι, coefficients {αt}T=ι, {β1t}T=1, β2, {μt}T=ι, e, iteration number
T
Output: parameter vector xT
1:	Set mo = 0, vo = 0, and Vo = e.
2:	for t = 1 to T - 1 do
3:	gt = ▽%(Xt).
4:	mt = β1tmt-1 + (1 - β1t)gt.
5:	vt = β2vt-1 + (1 - β2 )gt2 .
6:	Vt = max(Vt-i,vt),Vt = diag(vt).
7:	m t = (1 - μt)mt + μtgt.
8:	Xt+1 = ∏f √-^-(Xt — atm^t/√Vt).
9:	end for
Consider the optimization process OfNAMSG, and ignore the projections for simplicity. Since Vt
varies slowly when t is large, We can ignore the change of Vt between recent iterations. The operation
of dividing the update by √Vt can be approximated by solving a preconditioned problem, as
minΦ(X) = 1(X - X*)TVJ1/4HV^-1∕4(X - X*),	(8)
x∈F	2	t
where X = Vt1/4x, X* =匕 1/4x*. Define the preconditioned Hessian H =匕-1/4HVtT/4, which is
supposed to have improved condition number compared with H in the convex setting.
Then, we model the optimization process as a dynamic system. Solving the quadratic problem (7) by
NAMSG is equal to solving the preconditioned problem (8) by CNAG, as
m t = βm t-ι + (1- β )Vft(Xt)
Xt+1 = Xt - α ((1 - μ)mt + Nft(Xty),
-1/4
where the preconditioned stochastic function ft(X)= ft(Vt / x), the initial momentum mo = 0,
the coefficients α = (1 - βιt)ɑt, β = βιt, and μ = μt are considered as constants.
We use ν to denote a unit eigenvector of the Hessian H, and the corresponding eigenvalue is λ. We
define the coefficients as St = hy, Xti, Vt = hν, m/. According to (9), the coefficients are updated as
ʃ Vt+1 = 8vt + (1 - β)λ(St + δt),
t St+1 = St — αβ(1 一 μ)Vt — α(1 — β(1 — μ))λ(St + δt)
where the gradient error coefficient δt =(匕-1/4gt, Vi∕λ.
(10)
Substituting Vt by Vt = αVt, and denote T = αλ, we rewrite the update (10) into a dynamic system as
Vt+1 = AVt + bδ A = β	(1 - β)τ
St+l] = A [St] + bδt, A = [-β(1 - μ) 1 - (1 - β(1 - μ))τ
where A is the gain matrix. The eigenvalues of A are
(1 - β)τ
-(1 - β(1 * * * - μ))τ],
(11)
ri = 1 (P - pρ2	- 4β(1 - μτ))	,	r2	= 2	(P +	√ρ2	- 4β(1 -	μτ))	,	(12)
where P = 1 + β - T(1 - β(1 - μ)). Denote the corresponding unit eigenvectors as wi and w2, that
are solved numerically since the expressions are too complicated.
Define the coefficients c1, c2, d1, d2 satisfying
c1 w1 + c2 w2 = b, d1 w1 + d2 w2
V1
^51
(13)
Vt+1
S5t+1
From (11), (12) and (13), we obtain
t
ridiwi + r2d2w2 + E δι (rt-lc1W1 + r2-lc2W2) .	(14)
l=1
4
Under review as a conference paper at ICLR 2020
Assume that δt = σδ, where δ obeys the standard normal distribution, and σ is the standard deviation
of δt. From (14), We obtain E(St+ι) = rt d1W1,2 + τitd2W2,2 and
Mm Var(St) = (|c1|2|w1,2|2/(I-Ir1|2) + |c2 |2 |w2,2 |2/(ITr2|2)
t→+∞
+2Re(C1 c2WT^W2,2∕(l - r1r2))) δ2, ifmax(∣rι∣, ∣r2∣) < L
(15)
According to the analysis in Section 2, We recommend the momentum factor β = 0.999. Figure
1 presents the gain factor gfac = max(∣eι ∣, ∣r2∣) and the stand deviation limit limt→+∞ Std(St) of
CNAG. It is shown that compared with HB (μ = 0), a proper observation factor μ improves the
convergence rate significantly, and also accelerates the divergence in nonconvex problems Where
τ = αλ < 0. When the step size α is constant, compared with large curvatures, a small curvature λ
converges much slower, forming the bottleneck of the whole training process. The problem can be
alleviated by using a large μ. However, the noise level also increases along with μ when α is constant,
that prohibits too large μ. Consequently, we recommend μ = 0.1 to achieve fast convergence speed,
and μ = 0.2 to improve generalization at the cost of more iterations, since higher noise level is
beneficial for expanding the range of exploration. For NAMSG, experiments shows that when β2 is
close to 1, its variation does not effect the results significantly. We recommend β2 = 0.99. Only the
step size α is left for grid search.
Figure 1 also shows that a large β and a proper μ ensures a large convergence domain, while
0 < τ < 2 is required for convergence in SGD (β = 0). Since the range of eigenvalue λ is problem-
dependent, a large maximum τ (denoted by τmax) allows large step sizes. As shown in Figure 1
(a), μ does not effect gfac significantly for a tiny range of T close to 0. Then, gfac decreases almost
linearly according to τ to the minimum. Consequently, training with a large step size α and small
μ is beneficial for both the convergence of tiny positive λ, and the divergence of tiny negative λ in
nonconvex settings. While selecting a smaller μ and scaling α proportional to argmi□τgfac, the λ to
minimize gfac is unchanged, and the convergence rate for 0 < λ < τmax /α is generally improved
according to Figure 1. However, the noise level also increases, that prohibits too large α. We propose
a hyper-parameter policy named observation boost (OBSB). The policy performs grid search for a
small portion of iterations using a small μ to select an optimal initial a. In training, when the loss
flattens, it doubles μ, and scales α proportional to argmi□τgfac. The recommend initial μ is 0.05.
Figure 1: The gain factor max(|r1 |, |r2 |) and stand deviation limit of CNAG when β = 0.999: (a)
and (b) for τ ∈ [-0.001, 0.01], (c) and (d) τ ∈ [-0.3, 5], (e) and (f) for τ ∈ [-1, 40].
5
Under review as a conference paper at ICLR 2020
4 Convergence Analysis
In this section, we provide a data dependent regret bound of NAMSG in the convex setting, and
further improve the bound for strongly convex functions.
Since the sequence of cost functions ft (x) are stochastic, we evaluate the convergence property
of our algorithm by regret, which is the sum of all the previous difference between the online
prediction ft(xt) and the best fixed point parameter ft(χ*) for all the previous steps, defined as
RT = PT=ι(ft(xt) — ft(x*)). When the regret of an algorithm satisfies RT = o(T), the algorithm
converges to the optimal parameters on average.
Assuming that αt ≥ αt+1 , NAMSG insures Γt
+1
一 PVt∕αt ∈ S++. The positive
definiteness of Γt results in a nonincreasing step size and avoids the non-convergence of ADAM.
Following Reddi et al. (2018), we derive the following key results for NAMSG.
Theorem 1. Let {χt}, {Vt} and {vt} be the sequences obtained from Algorithm 1, at = ɑ∕√t,
βι = β11 < 1, 0 ≤ β1t+1 ≤ βιt, Y = βι∕√β2 < 1,1 — βι ≤ μt = μ< 1,forall t ∈ {1, ∙∙∙,T},
and x ∈ F . We have the following bound on the regret
<	1	D D∞√T XX "/2 , (I - μ)D∞ XX XX βltvt,i
≤ (1-βι (1-μ))	⅛ T + —2 — t=1 =
l 3	3β2	l 2、aP1 + log(T) X II II
+ b(1 - βl)(1 - Y) + μ )	√1-β2	= kgLT,ik2
(16)
By compared with the regret bound of AMSGRAD (Reddi et al., 2018), we find that the regret bounds
of the two methods have the similar form. However, when β1 and Y are close to 1, which is the
typical situation, NAMSG has lower coefficients on all of the 3 terms.
From Theorem 1, we can immediately obtain the following corollary.
Corollary 1. Suppose β1t = β1∕t, then we have
RT ≤
D∞ √T (1 + 2(1- μ)βι)
2a(1 - β (1 - μ))
G "/2	(3β2 + 2(1 — βI)(I- Y)μ2) aP1 + IOg(T) G I， H
⅛ vT,i + 2(1-βι)(1-βι (1-μ))(1-γ)√1-β2 "ET,ik2∙
(17)
The bound in Corollary 1 is considerably better than O(√dT) regret of SGD when Pd=i V1,/2《√d
and Pd=I ||gi：T,ik《√dT (Duchi et al.,2011).
For strongly convex functions, NAMSG further achieves a O(log(T)) regret bound with O(1∕t) step
size (Bottou et al., 2018; Wang et al., 2019) under certain assumptions.
Theorem 2. Assume that ∀x1 , x2 ∈ F, ft (xι) ≥ ft (x2) + Vft (χ2)> (χι - χ2) + 2 ∣∣xι - x2k2,
where λ is a positive constant. Let {χt}, {Vt} and {vt} be the sequences obtained from Algorithm 1.
The initial step size a ≥ maxi∈{i,…，+} (tV1/2 - (t - 1)V1-1 ) / ((1 - βι(1 - μ)) λ), at = a∕t,
0 ≤ βt = β1∕t2 < 1, γ = β1∕√β2 < 1, 1 — βι ≤ μt = μ < 1, C → 0+, for all t ∈ {1, ∙∙∙ ,T},
and x ∈ F. We have the following bound on the regret
(aG1	f3	β2	IJ 2、1 (I-M)β1D∞ XX ."/2、1+log(T)
RT ≤ (√Γ-W (2(1-βl)(1-γ) + μ)+ -2a — ⅛ vT,i) 1-βl(1-μ).
(18)
When the gradients are sparse, satisfying Pd=I VT7《√d,the bound is better than O(√dlog(T)).
The proof of theorems are given in the appendix. It should be noted that although the proof requires a
decreasing schedule of at and β1t to ensure convergence, numerical experiments show that piecewise
constant at and constant β1t provide fast convergence speed in practice.
6
Under review as a conference paper at ICLR 2020
5	Expreriments
In this section, we present experiments to evaluate the performance of NAMSG and the OBSB policy
for NAMSG, compared with SGD with momentum (Polyak, 1964), CNAG, and popular adaptive
stochastic optimization methods, such as ADAM (Kingma & Ba, 2015), NADAM (Dozat, 2016), and
AMSGRAD 1 (Reddi et al., 2018). We study logistic regression and neural networks for multiclass
classification, representing convex and nonconvex settings, respectively. The experiments are carried
out with MXNET (Chen et al., 2015).
5.1	Experiments on MNIST
We compare the performance of SGD, ADAM, NADAM, CNAG, AMSGRAD, NAMSG and OBSB,
for training logistic regression and neural network on the MNIST dataset (LeCun et al., 1998). The
dataset consists of 60k training images and 10k testing images in 10 classes. The image size is
28 × 28.
Logistic regression:In the experiment, the minibatch size is 256. The hyper-parameters for all the
methods except NAMSG and OBSB are chosen by grid search (see appendix), and the best results in
training are reported. In NAMSG and OBSB, only the step size α is chosen by grid search, and the
other hyper-parameters are set according to the default values. We report the train and test results in
Figure 2, which are the average of 5 runs. It is observed that OBSB perform the best with respect
to train loss, and NAMSG also converges faster than other methods. The test accuracy is roughly
consistent with the train loss in the initial epochs, after which they fluctuate for overfitting. The
experiment shows that NAMSG and OBSB achieves fast convergence in the convex setting.
Neural networks: In the experiment, we train a simple convolutional neural network (CNN) for the
multiclass classification problem on MNIST. The architecture has two 5 × 5 convolutional layers,
with 20 and 50 outputs. Each convolutional layer is followed by Batch Normalization (BN) (Ioffe
& Szegedy, 2015) and a 2×2 max pooling. The network ends with a 500-way fully-connected
layer with BN and ReLU (Nair & Hinton, 2010), a 10-way fully-connected layer, and softmax. The
hyper-parameters are set in a way similar to the previous experiment. The results are also reported in
Figure 2, which are the average of 5 runs. We can see that NAMSG has the lowest train loss, which
translates to good generalization performance. OBSB also converges faster than other methods. The
experiment shows that NAMSG and OBSB are efficient in non-convex problems.
Figure 2: Performance of SGD, ADAM, NADAM, CNAG, AMSG, NAMSG, and OBSB on MNIST.
(a) and (b) shows the performance in logistic regression. (c) and (d) compares the results in CNN.
5.2	Experiments on CIFAR- 1 0
In the experiment, we train Resnet-20 (He et al., 2016) on the CIFAR-10 dataset (Krizhevsky, 2009),
that consists of 50k training images and 10k testing images in 10 classes. The image size is 32 × 32.
The architecture of the network is as follows: In training, the network inputs are 28 × 28 images
randomly cropped from the original images or their horizontal flips to save computation. The inputs
are subtracted by the global mean and divided by the standard deviation. The first layer is 3 × 3
convolutions. Then we use a stack of 18 layers with 3 × 3 convolutions on the feature maps of
1 referred to as AMSG in the figures.
7
Under review as a conference paper at ICLR 2020
sizes {28, 14, 7} respectively, with 6 layers for each feature map size. The numbers of filters are
{16, 32, 64} respectively. A shortcut connection is added to each pair of 3 × 3 filters. The subsampling
is performed by convolutions with a stride of 2. Batch normalization is adopted right after each
convolution and before the ReLU activation. The network ends with a global average pooling, a
10-way fully-connected layer, and softmax. In testing, the original 32 × 32 images are used as inputs.
We train Resnet-20 on CIFAR-10 using SGD, ADAM, NADAM, CNAG, AMSGRAD, NAMSG, and
OBSB. The training for each network runs for 75 epochs. The hyper-parameters are selected in a
way similar to the previous experiments, excepting that we divide the constant step size by 10 at the
12000th iteration (in the 62th epoch). A weight decay of 0.001 is used for regularization. Two group
of hyper-parameters are obtained for each method, one of which minimizes the train loss before the
dropping of step size, and the other maximizes the mean test accuracy of the last 5 epoches.
Figure 3 shows the average results of 5 runs. In experiments to achieve the fastest training speed
(Figure 3 (a),(b)), OBSB converges the fastest, and NAMSG is also faster than other methods.
Compares with ADAM, OBSB is more than 1 time faster, and NAMSG is roughly 1 time faster to
reach the train loss before the dropping of step size. OBSB has the best test accuracy, and NAMSG
is better than other methods. CNAG achieves significant acceleration upon SGD, and is also faster
than ADAM, NADAM, and AMSGRAD. In experiments to achieve the best generalization (Figure 3
(c),(d)), OBSB still converges the fastest, NAMSG and CNAG converge at almost the same speed,
which is faster than other methods. The mean best generalization accuracy of SGD, ADAM, NADAM,
CNAG, AMSGRAD, NAMSG, and OBSB are 0.9129, 0.9065, 0.9066, 0.9177, 0.9047, 0.9138, and
0.9132, respectively. CNAG achieves the highest test accuracy. OBSB, NAMSG, and SGD obtains
almost the same final test accuracy, which is much higher than ADAM, NADAM, and AMSGRAD. It
should be noted that CNAG achieves the best test accuracy at the cost of grid search for 3 parameters,
while NAMSG and OBSB only search for the step size.
Figure 3: Performance of SGD, ADAM, NADAM, CNAG, AMSG, NAMSG, and OBSB for Resnet-
20 on CIFAR-10. (a) and (b) compares the best results for training. (c) and (d) shows the best results
for generalization.
The experiments show that in the machine learning problems tested, NAMSG and OBSB converges
faster compared with other popular adaptive methods, such as ADAM, NADAM, and AMSGRAD.
The acceleration is achieved with low computational overheads and almost no more memory.
6	Conclusions and discussions
We present the NAMSG method, which computes the gradients at configurable remote observation
points, and scales the update vector elementwise by a nonincreasing preconditioner. It is efficient
in computation and memory, and is straightforward to implement. A data-dependent regret bound
is proposed to guarantee the convergence in the convex setting. The bound is further improved to
O(log(T)) for strongly convex functions. The analysis of the optimizing process provides a hyper-
parameter policy (OBSB) which leaves only the step size for grid search. Numerical experiments
demonstrate that NAMSG and OBSB converge faster than ADAM, NADAM, and AMSGRAD, for
the tested problems.
8
Under review as a conference paper at ICLR 2020
References
D. Amodei, S. Ananthanarayanan, and et al. R. Anubhai. Deep speech 2 : End-to-end speech
recognition in English and Mandarin. In Proceedings of the 33rd International Conference
on Machine Learning (ICML 2016), pp.173-182, New York, New York, USA, 2016. Morgan
Kaufmann.
L. Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of
COMPSTAT’2010: 19th International Conference on Computational Statistics, pp. 177-186,
Heidelberg, 2010. Physica-Verlag HD.
L. Bottou, F. Curtis, and J. Nocedal. Optimization methods for large-scale machine learning. SIAM
Review, 60(2):223-311, 2018. doi: 10.1137/16M1080173.
R. H Byrd, S. L Hansen, J. Nocedal, and Y. Singer. A stochastic quasi-Newton method for large-scale
optimization. SIAM Journal on Optimization, 26(2):1008-1031, 2016.
Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu,
Chiyuan Zhang, and Zheng Zhang. Mxnet: A flexible and efficient machine learning library for
heterogeneous distributed systems. CoRR, abs/1512.01274, 2015. URL http://arxiv.org/
abs/1512.01274.
R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. Natural language
processing (almost) from scratch. Journal of Machine Learning Research, 12:2493-2537, 2011.
Timothy Dozat. Incorporating Nesterov momentum into Adam. In International Conference on
Learning Representations, 2016.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(7):257-269, 2011.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In 2016 IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770-778, June 2016. doi:
10.1109/CVPR.2016.90.
G. Hinton, L. Deng, D. Yu, G. E. Dahl, A. R. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen,
T. N. Sainath, and B. Kingsbury. Deep neural networks for acoustic modeling in speech recognition:
The shared views of four research groups. IEEE Signal Processing Magazine, 29(6):82-97, 2012.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. CoRR, abs/1502.03167, 2015. URL http://arxiv.org/
abs/1502.03167.
Prateek Jain, Sham M. Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford. Accelerating
stochastic gradient descent for least squares regression. In Sebastien Bubeck, Vianney Perchet,
and Philippe Rigollet (eds.), Proceedings of the 31st Conference On Learning Theory, volume 75
of Proceedings of Machine Learning Research, pp. 545-604. PMLR, 06-09 Jul 2018. URL
http://proceedings.mlr.press/v75/jain18a.html.
Rahul Kidambi, Praneeth Netrapalli, Prateek Jain, and Sham M. Kakade. On the insufficiency of
existing momentum schemes for stochastic optimization. In International Conference on Learning
Representations, 2018. URL https://openreview.net/forum?id=rJTutzbA-.
D. Kingma and J. Ba. Adam: A method for stochastic optimization. In 2015 International Conference
on Learning Representations (ICLR 2015), pp. 1-11, San Diego, CA, 2015.
A. Krizhevsky. Gradient-based learning applied to document recognition. Tech Report, 86(11):
2278-2324, 2009.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
J. Martens. Deep learning via Hessian-free optimization. In International Conference on Machine
Learning (ICML 2010), pp. 735-742, 2010.
9
Under review as a conference paper at ICLR 2020
H. Brendan McMahan and Matthew J. Streeter. Adaptive bound optimization for online convex
optimization. CoRR, abs/1002.4908, 2010. URL http://arxiv.org/abs/1002.4908.
Vinod Nair and Geoffrey E. Hinton. Rectified linear units improve restricted boltzmann machines. In
International Conference on Machine Learning, 2010.
Y. Nesterov. A method of solving a convex programming problem with convergence rate o(1/k2).
Soviet Mathematics Doklady, 27(2):372-376,1983.
J. Nocedal. Updating quasi-Newton matrices with limited storage. Mathematics of Computation, 35
(151):773-782, 1980.
B. T. Polyak. Some methods of speeding up the convergence of iteration methods. Ussr Computational
Mathematics and Mathematical Physics, 4(5):791-803, 1964.
Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of Adam and beyond. In
International Conference on Learning Representations, 2018. URL https://openreview.
net/forum?id=ryQu7f-RZ.
Herbert Robbins and Sutton Monro. A stochastic approximation method. Annals of Mathematical
Statistics, 22(3):400-407, 1951.
I. Sutskever, J. Martens, G. Dahl, and G. Hinton. On the importance of initialization and momentum
in deep learning. In International conference on machine learning, pp. 1139-1147, 2013.
T. Tieleman and G. Hinton. Rmsprop: Divide the gradient by a running average of its recent
magnitude. COURSERA: Neural Networks for Machine Learning, 2012.
Guanghui Wang, Shiyin Lu, Weiwei Tu, and Lijun Zhang. Sadam: A variant of adam for strongly
convex functions. 2019.
Matthew D. Zeiler. ADADELTA: an adaptive learning rate method. CoRR, abs/1212.5701, 2012.
URL http://arxiv.org/abs/1212.5701.
A Appendix
A.1 Proof of Theorem 1
In this proof, we use yi to denote the ith coordinate of a vector y .
From Algorithm 1,
xt+ι =πf √v^^ (Xt - atVt 1/2 ((I — μt) mt + μtgt,
=arg min 11V⅛1/4 (X - (Xt - α 泊 T/2 ((1 - μt) mt + μtgt)))∣∣
Furthermore, nʃ √^ (x*) = x* for all x* ∈ F. Using Lemma A1 with Ui = xt+ι and U = x*,
we have
∣∣πVt1/4 (xt+1 - x*) (Xt - αt匕-1/2 ((1 - μt) mt + μtgt) - x*) ∣∣
∣∣^Vt1/4	(Xt	-	χ*)	∣∣	+ α2	∣∣Vt 1/4 ((I - μt) mt + μtgt)∣∣	- 2αt	h(1 - μt) mt	+ μtgt, Xt	- x*i
∣∣匕1/4	(Xt-	x*)∣∣	+ α2	∣∣匕-1/4((1 - μt) mt + μtgt)∣∣
-2αt h(1 - μt) β1tmt-1 + (μt + (1 - μt)(1 - Bu)) gt, Xt - X*i
≤M4 (Xt
-X*)∣∣2 + 2α2 ((1 - μt)2 ∣∣%-"mt∣∣2 + μ2 ∣∣%-1∕4gt∣∣2
-2αt h(1 - μt) β1tmt-1 + (1 - β1t + β1tμt) gt, Xt - X*i.
(A2)
10
Under review as a conference paper at ICLR 2020
Since 0 ≤ β1t < 1, from the assumptions,
0 < 1 — βι ≤ μt = μ < 1.
(A3)
Rearrange the inequity (A2), we obtain
hgt,Xt - x*i
1
-2αt (1 - Bit(I - μ))
(收 1/4 (Xt- x*)∣∣ - KnVt1/4 (xt+ι - x*)∣∣ )
1 - β1t (1 - μ)
hmt-1,xt - x*i
—
(I - μ) β1t
1 - β1t(1 - μ)
(A4)
1
-2αt (1 - β1t(1 - μ))
(∣∣%1/4 (Xt-X“| - ∣∣%1/4 (χt+1- x*)∣∣)
+ 1 - β1t (1 - μ)
((1-μH∣VΓ1∕4mt∣∣ + μ2∣∣Vt-1∕4gt
+ 2(11-J) M1∕4mt-1∣∣2 + 2(1-(β- μ)-1tμ)) at ∣ 恒"(Xt-X∙
For simplicity, denote
T
P1= X (∣恒/4 (Xt-X*)∣∣ -∣恒/4 (Xt+1-X*)∣∣ +(1-μ)β1t∣恒/4 (Xt-X*)∣∣
t=1
/ (2at (1 - β1t(1 - M)))
T
P2= X at ((1-μ)2∣∣ 匕-1∕4mt∣∣ + μ2 ∣∣%-1/4 gt ∣∣ + 2(1 - μ)β1t ∣∣ 匕-1/4mt-11∣ )
IQ- β1t(1 - M))
(A5)
Because of the convexity of the objective function, the regret satisfies
TT
RT = X (ft (Xt)- ft (x*)) ≤ X hgt, Xt- X*i ≤ P1 + P2.	(A6)
i=1	t=1
The first inequity follows from the convexity of function ft . The second inequality is due to (A4).
We now bound the term PtT=1at ∣∣%-1"gt∣∣ .Wehave
T	T-1	d	2
X at ∣∣L/4gt∣∣ = X at ∣∣-gt∣∣ + aτ XM
t=1	t=1	i=1 vT,i
T-1	d 2	T-1	d	2
≤ X at ∣∣%-1∕4gt ∣∣2 + aτ X A ≤ X at ∣∣%-1∕4gt∣∣2 + ʌ X ,	gT,i	.
^t⅛	∣∣ t∣∣ A	∣∣ t∣∣	√tA q(1-β2) jβτ-%
T-1	2	d	T	d
≤ X atM1∕4gt∣∣+ J X协≤√⅛ X (√t XT
d
≤√⅛ X kg1:T，ik2t
T1
X t ≤
t=1
a √1+log(T)
√1 - β2
d
X kg1:T，ik2 .
i=1
(A7)
11
Under review as a conference paper at ICLR 2020
In (A7), the second inequity is follows from the definition of vt , the fifth inequality is due to
Cauchy-Schwarz inequality. The final inequality is due to the following bound on harmonic sum:
P= 1/t ≤ 1 + iog(τ).
From (A7), and Lemma A2, which bound PT=I α/恒-1/4 mt∣∣ , We further bound the term P? as
P2 ≤
1 - β1(I - μ)
at ((I- 〃)2 ∣∣VΓ1∕4mt∣∣ + 1(1 - μ)βιt ∣忖τ∕4mt-ι
+ a √1 + log(T)
√1 - β2
― 1 - β1(1 - μ)
μ2 XX IE："
i=1
(β2 XX at (∣∣VtT4mt∣∣2 + 1M"mt-i
μ2 XX kg1"2)
i=1
+ a √1 + log(T)
√1 - β2
― 1 - β1(1 - μ)
at∣%T4mt∣l2 + 2 Σ at∣%T4mt∣∣2
(A8)
1
1
1
ap1 + IOg(T) 2 G II II
+	√1-β2	μ 斗1"2
1
’2 XX at ∣∣%T∕4mt∣∣2 + ap√τ-gT) μ2 XX kgl"∣2
≤ 1 - βι(1 - μ) l2β
≤ ( -3β2— + μ2
≤ <2(1-βι)(1-γ)+ μ
α √1 + log(T)
(I - βι(I - μ)) √1 - β2
d
X kg1:T,ik2 .
i=1
The third inequity is due to βιt ≥ β1t+1 and ^1i/at ≥ V1-1 -/αt-ι by definition.
12
Under review as a conference paper at ICLR 2020
We also have
P1
-2αι (1 - βι(1 - M))
T
∣∣V1∕4 (χι-χ*)∣∣ + X(
t=2
2αt(I - βιt(1 - μ))
1
1
—
T
2at-i (1 - βιt-i(1 - μ))愕4 (xt-x*)∣l )+ X
βιt(I - μ)
2αt (1 - βιt(1 - μ))
1
—
-Xf
O7 X vi/i2 mi)2+X (X ("W)2 v vα/2
ai i=i	t=2	i=i	at
2(1 -尸1(I- μ))
%%!!
at-i J J
Td
+Xt=1Xi=1
βιt(1 - μ) (xt,i - K)2 v1,/2
D∞2
—2 (I- β1(I- μ))
αt
dT
ɪ X v1∕i2+X
i=1	t=2
1/2
vt,i
αt
v* 1 * * */2 ∖∖ Td
≡	+ XX
t=1 i=1
αt
—
d∞ Tt
2 (1 - βι(1 - M)) α
d	2 T d	i/2
X "/2 +	(1 - μ)D∞	X X βitvt,i
L 么1…-〃))=i=i
(A9)
In (A9), the second inequity follows from the assumption β1t < β1t-1, the third and the last inequality
is due to V1/2/at ≥ V1-1 卜弋—、by definition and the assumption at = α/√t.
Combining (A6), (A8), and (A9), we obtain
RT ≤
(1 - βι(1 - M))
+ (2a⅛
X vT/2+上μD∞∙X XX
i=i	t=i i=i
2、aP1 + Iog(T) X ll ll
+ μ )	√1-β2 工 kgi：T，i k2
αi
(A10)
1
The proof is complete.
The Lemmas used in the proof are as follows:
Lemma A1. (McMahan & Streeter, 2010)
For any Q ∈ Sf and convex feasible set F ∈ Rd, suppose Ui = minχ∈F ∣∣Q1/2 (x - zi) and
U2 = minχ∈F ∣∣Q1/2 (x - z2)∣∣ then we have ∣∣Q1/2 (Ui - U2)∣∣ ≤ ∣∣Q1/2 (zi - z2)∣∣.
Lemma A2. (Reddi et al., 2018)
For the parameter settings and conditions assumed in Theorem 1, which is the same as Theorem 4 in
Reddi et al. (2018), we have
T
Xa IV-1/4mt
t=1
α √1 + log T
一(1 - βI)(I - Y)√1 - β2
d
X kg1:T,i k2 .
i=1
2
≤
13
Under review as a conference paper at ICLR 2020
The proofs of Lemma A1 and A2 are described in Reddi et al. (2018).
A.2 Proof of Theorem 2
Because of the objective function is strongly convex, from (A3) and (A4) the regret satisfies
2 kxt - x*k2)
T
≤X
t=1
(2071-⅛^
(忖/4 (Xt-X*" - 忖/4 (xt+ι -x*)|| - - 2 kxt-x*k2
+ (1 - β1√t1 - μ)) (β2 (IIKT/4mtl + 2 BVt—1/4mt-1B ) + μ2 gτ∕4gt
+______(I - μ)Bιt
2at(1 - βιt(1 - μ力
(A11)
We divide the righthand side of (A11) to three parts, as
Q1
Q2
-x*)|| ) - 2 kxt - χ*k2
+ μ2 *∣∣%τ∕4gt∣∣2
T
Q3=X 2αt U)忖/4 (xt-x* )∣∣.
(A12)
Firstly, we bound the term Q1.
Q1 =	2ɑ1 (1 - β1(1 -	μ))	∣∣V1/4 (XI-	x*)∣∣	-	2ατ	(1 -	βιτ(1 -	μ))	^/4	(XT +1 -	x*)H
(2ɑt (1 - β1t(1 - μD
号/4(Xt- X*)∣∣2
1
2at-1 (1 - β1t-1(I - μD
T
+X
t=2
t=1
≤
1
2αι (1 - βι(1 - μ^
∣∣V1/4(xi -x*" - 2 ∣∣xi -X*k2
—
T
+X
t=2
1
2(1 - βιt(1 - M))
- 2 kχt - χ*k2)
—
1
αt-1
1
2αι(1 - βι(1 - μ))
T
+X
t=2
(2 (1 - β1t(1 - μ))
CtIa/4 (Xt-X*)∣∣ - t-011时4(χt-χ*)∣∣) - 2 kχt -χ*k2)
≤0
(A13)
The first inequity follows from βt is nonincreasing, the second equity fol-
lows from αt = α∕t.	The last inequity is because of the assumption a ≥
maχi∈{i,…,d} (tv1,/2 Tt-I)V1-1,) / ((I- 仇(I- μ))λ),and E → 0+.
14
Under review as a conference paper at ICLR 2020
Then, we bound the term Q2 .
Q2 ≤
1 - β1(I - μ)
T
X1 (β2 (IIVtT4mt∣I +2IIVtT4mt-1∣I)+μ2 H"gt∣I
― 1 - βι(1 - μ)
mt∣∣2+TIVtTZII2)+' 4 β IIVt-FtII2
T
X t (3 β2 II VtTF	+ μ2IIVL4gt∣I)
― 1 - βι(1 - μ)
The first inequity is because of m0 = 0, and Vt is nondecreasing.
We further bound the two terms in the righthand side of (A14).
(A14)
「m』2
d t t-j	t t-j	u
≤X XY β1(t-k+1)	X Y β1(t-k+1)gj2,i ∕ut
i=1	j=1 k=1	j=1 k=1
d t	t t-j
≤X(Xβt--k+i)) (XYβi(t-k+i)gj
i=1	j=1	j=1 k=1
j=1
(1 - βι) √1 — β2
1
d t t-j
X(XY βi(t-k+i)gj,i
i=1	j=1 k=1
d t	______
XX βt-j g2,i∕√β⅛
i=1 j=1
dt
XX γt-jM∣
i=1 j=1
( (1 - βι) √1 - β(I - Y)
G1
(A15)
α
α
α
1
1
1
t
t
1
The first inequality follows from Cauchy-Schwarz inequality.
d
I…I ≤ X
i=1
gt2i
(A16)
≤ X	g2,i	≤	1 G
-白 √1-W |gt,i| - √τ-h	1.
Combining (A14), (A15), and (A16), we obtain
Q2 ≤
αG1
(I - βι(1 - μ)) √1 - β
αG1
X t (2(1-βι)(1-Y)
β12
+ μ2
≤ _________一 1__________ I 3______β2_______
一(I- β1(1 - μ)) √1 - β2 I2 (I- β1)(1 - Y)
+ μ2 I (Iog(T) + I).
(A17)
The first inequity follows from the assumptions at = α∕t.
15
Under review as a conference paper at ICLR 2020
Algorithm A1 ASGD Algorithm
Input: initial parameter vector xι, short step α, long step parameter K ≥ 1, statistical advantage
parameter ξ ≤ √7K, iteration number T
Output: parameter vector xT
1:	Set Xi = xι, β = 1 — 0.72ξ∕κ.
2:	for t = 1 to T - 1 do
3:	gt = Vft(xt).
4:	χt+ι = βxt + (1- β) (xt- K7 gt).
C	,	一、	1 — ⅛
5:	xt+i = 0.7+(l-β) (Xt- αgt) + 0.7+(1-β) xt+1.
6:	end for
Finally, we bound the term Q3 .
Q3 ≤
T
________1_______^X ((1 - μ)Bιt
2(1 - βi(1 - μ)) t=i (	αt
T
(1 - μ)Bι	^X
2α (1 - βi(1 - μ)) t=i
≤
(1 - μ)β1
2α (1 - βι(1 - μ))
≤
(1 - μ)β1D∞
2α (1 - βι(1 - μ))
Td
X 1 X vi,/2 (xti)
t=1	i=1
Td
X H vi,/2
t=1	i=1
(A18)
≤
(I - μ)βiD∞	X "/2
2α (1-βi(1-μ))(…(T)) 二"
Both the first equity and the first inequity follow from the assumptions at = a/t and βit = βι∕t2.
The last inequity is due to Vt,i is nondecreasing by definition.
Combining (A11), (A13), (A17), and (A18), we obtain
RT ≤
aGi
√1 - β2
3	β12	2
2(1-βι)(1-γ) + μ) +
(1 - μ)β1D∞
2a
1 + log(T)
1 - β1(1 - μ)
(A19)
A.3 Equivalence of CNAG and ASGD
The pseudo code of ASGD (Kidambi et al., 2018; Jain et al., 2018) is shown in Algorithm A1. ASGD
maintains two iterates: descent iterate xt and a running average xt. The running average is a weighted
average of the previous average and a long gradient step from the descent iterate, while the descent
iterate is updated as a convex combination of short gradient step from the descent iterate and the
running average.
We rewrite the update of Algorithm A1 as
xt+i
xt+1
∙∙
AK
xt
xt
+ bgt
I-	"	" -I
βK	1 - βK
A =	(1-β)β	(1-β)2 + 0.7	, b
-(1-β) + 0.7	(1-β)+0.7 _
β-1.... 一
0 7 Ka
0.72 + (1-β)2K ∙∙
0.7((1-β) + 0.7) 0
(A20)
—
Define the variable transform as
myt  T xt T  l
xt	xt ,	0
♦♦ ∙ι
kKlK
1
(A21)
16
Under review as a conference paper at ICLR 2020
where k are l are adjustable coefficients.
Combining (A20) and (A21), we obtain
mt+1
xt+1
T mt + Ti)gt,T = TAT-1.
(A22)
In order to minimize the number of vector computations, we solve the adjustable coefficients k and l
by assigning T1,2 = 0, T2,1 = 1. We choose the solution as
k = -1, l
∙∙. ∙∙
(1- β)β
(1 — β) + 0.7
(A23)
Combining (A22) and (A23), we obtain
mt+1
xt+1
t mt + ^rbgt,τ
xt
■■
0.7β
(1-β)+0.7
1
(A24)
0
1
The update (A24) is identical to the practical form of CNAG update (6) with constant hyper-
parameters. The momentum coefficient of CNAG is
0.7β
βt = β =(i - β) + 0 7 =(K ― 0.49ξ)/(K + 0.7ξ),
(A25)
where the second equity follows from the definition of β in Algorithm A1.
It should be noted that the computational overheads of ASGD besides the gradient computation is 6
scalar vector multiplications and 4 vector additions per iteration, while CNAG reduces the costs to 3
scalar vector multiplications and 3 vector additions.
A.4 More details on experiments
We use constant hyper-parameters in the experiments. For ADAM, NADAM, and AMSGRAD, the
hyper-parameters (α, β1, β2) are selected from {0.0005, 0.001, 0.002, 0.005, 0.01, 0.02} ×
{0, 0.9, 0.99, 0.999, 0.9999} × {0.99, 0.999} by grid search. For SGD, the hyper-
parameters (α, β) are selected from {0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0} ×
{0,0.9,0.99,0.999,0.9999} by grid search. For CNAG, the hyper-parameters (α,β,μ)
are selected from {0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0} × {0, 0.9, 0.99, 0.999, 0.9999}
×{0.001, 0.01, 0.05, 0.1, 0.2, 0.3, 0.5, 0.9} by grid search. For NAMSG and OBSB, the hyper-
parameters (α) is selected from {0.0005,0.001,0.002,0.005,0.01,0.02} by grid search, (β1,β2, μ)
are set according to the default values.
In OBSB, the grid search runs for 5 epochs in the experiments on MNIST, and 20 epochs on CIFAR10.
The average convergence rate is computed each 2 epoches on MNIST, and 10 epochs on CIFAR10.
α and μ are scaled when the converging rate is halved to achieve fast convergence, and at the 50th
epoch (when the loss flattens) to maximize generalization.
The experiments are carried out on a workstation with an Intel Xeon E5-2680 v3
CPU and a NVIDIA K40 GPU. The source code of NAMSG can be downloaded at
https://github.com/rationalspark/NAMSG/blob/master/Namsg.py, and the
hyper-parameters can be downloaded at https://github.com/rationalspark/NAMSG/
blob/master/hyperparamters.txt. The simulation environment is MXNET, which can be
downloaded at http://mxnet.incubator.apache.org. The MNIST dataset can be down-
loaded at http://yann.lecun.com/exdb/mnist; the CIFAR-10 dataset can be downloaded
at http://www.cs.toronto.edu/~kriz/cifar.html.
17