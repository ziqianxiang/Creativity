Under review as a conference paper at ICLR 2020
Gradient-free Neural Network Training by
Multi-convex Alternating Optimization
Anonymous authors
Paper under double-blind review
Ab stract
In recent years, stochastic gradient descent (SGD) and its variants have been the
dominant optimization methods for training deep neural networks. However, SGD
suffers from limitations such as the lack of theoretical guarantees, vanishing gra-
dients, excessive sensitivity to input, and difficulties solving highly non-smooth
constraints and functions. To overcome these drawbacks, alternating minimization-
based methods for deep neural network optimization have attracted fast-increasing
attention recently. As an emerging and open domain, however, several new chal-
lenges need to be addressed, including 1) Convergence depending on the choice
of hyperparameters, and 2) Lack of unified theoretical frameworks with general
conditions. We, therefore, propose a novel Deep Learning Alternating Mini-
mization (DLAM) algorithm to deal with these two challenges. Our innovative
inequality-constrained formulation infinitely approximates the original problem
with non-convex equality constraints, enabling our proof of global convergence of
the DLAM algorithm under mild, practical conditions, regardless of the choice of
hyperparameters and wide range of various activation functions. Experiments on
benchmark datasets demonstrate the effectiveness of DLAM.
1 Introduction
Stochastic gradient descent (SGD) and its variants have become popular optimization methods for
training deep neural networks. These methods split a dataset into multiple batches and then optimize
them sequentially by gradient descent in each epoch. SGD has two main advantages: not only is it
simple to implement, but it can also be applied in online settings where new coming training data are
used to train models. However, while many researchers have provided solid theoretical guarantees
on the convergence of SGD (Kingma & Ba (2014); Reddi et al. (2018); Sutskever et al. (2013)), the
assumptions of their proofs cannot be applied to problems involving deep neural networks, which are
highly nonsmooth and nonconvex. Aside from the lack of theoretical guarantees, several additional
drawbacks restrict the applications of SGD. It suffers from the gradient vanishing problem, meaning
that the error signal diminishes as the gradient is backpropagated, which prevents the neural networks
from utilizing further training (Taylor et al. (2016)), and the gradient of the activation function is
highly sensitive to the input (i.e. poor conditioning), so a small change in the input can lead to a
dramatic change in the gradient.
To tackle these intrinsic drawbacks of gradient descent optimization methods, alternating minimization
methods have started to attract attention as a potential way to solve deep learning problems. Here, the
loss function of a deep neural network is reformulated as a nested function associated with multiple
linear and nonlinear transformations across multi-layers. This nested structure is then decomposed
into a series of linear and nonlinear equality constraints by introducing auxiliary variables and penalty
hyperparameters. The linear and nonlinear equality constraints generate multiple subproblems,
which can be minimized alternately. Some recent alternating minimization methods have focused on
applying the Alternating Direction Method of Multipliers (ADMM) (Taylor et al. (2016); Wang et al.
(2019)) and Block Coordinate Descent (BCD) (Jinshan Zeng (2018)), with empirical evaluations
demonstrating good scalability in terms of the number of layers and high accuracy on the test sets,
especially for neural networks that are very deep, thanks to parallelism (Taylor et al. (2016); Wang
et al. (2019)). For more information, please refer to Section H in the supplementary materials. These
methods also avoid gradient vanishing problems and allow for non-differentiable activation functions
such as binarized neural networks (Courbariaux et al. (2015)), as well as allowing for complex non-
smooth regularization and the constraints that are increasingly important for deep neural architectures
that are required to satisfy practical requirements such as interpretability, energy-efficiency, and cost
awareness Carreira-Perpinan & Wang (2014).
1
Under review as a conference paper at ICLR 2020
Table 1: Notations Used in This Paper
Notations	Descriptions
L	Number of layers.
Wl	The weight vector in the l-th layer.
bl	The intercept in the l-th layer.
zl	The temporary variable of the linear mapping in the l-th layer.
hl (zl )	The nonlinear activation function in the l-th layer.
al	The output of the l-th layer.
x	The input matrix of the neural network.
y	The predefined label vector.
R(zl , y)	The risk function in the l-th layer.
Ωι (Wι)	The regularization term in the l-th layer.
εl	The tolerance of the nonlinear mapping in the l-th layer.
However, as an emerging domain, alternating minimization for deep model optimization suffers
from a number of unsolved challenges including: 1. Convergence properties are sensitive to
penalty parameters. One recent work by Wang et al. firstly proved the convergence guarantee
of ADMM in the fully-connected neural network problem (Wang et al. (2019)). However, such
convergence guarantee is dependent on the choice of penalty hyperparameters: the convergence can
not be guaranteed any more when penalty hyperparameters are small. 2. Lack of unified theoretical
frameworks with general conditions. The global convergence of ADMM on deep learning has
rarely been explored (Wang et al. (2019); Zeng et al. (2019)). And existing few works are tailored for
and limited to few specific loss functions and activation functions: Zeng et al. proved that the ADMM
is convergent for square loss function and twice differentiable activation functions (e.g. sigmoid)
(Zeng et al. (2019)); Wang et al. proved the convergence of ADMM for the Relu activation function
(Wang et al. (2019)). Therefore, there lacks a unified theoretical framework which covers wide range
of commonly used losses and activation functions.
In order to simultaneously address these technical problems, we propose a new formulation of the
deep neural network problem, along with a novel Deep Learning Alternating Minimization (DLAM)
algorithm. The proposed framework is highly generic and sufficiently flexible to be utilized in
common fully-connected deep neural network models, as well as being easily extendable to other
models such as convolutional neural networks (Krizhevsky et al. (2012)) and recurrent neural networks
(Mikolov et al. (2010)). Specifically, we, for the first time, transform the original deep neural network
optimization problem into an inequality-constrained problem that can be infinitely approximate to
the original one. Applying this innovation to an inequality-constraint based transformation ensures
the convexity of all subproblems, and hence easily ensures global minima, the inequality-constraint
prevents the output of a nonlinear function from changing much and reduces sensitivity to the
input. The operation of matrix inversion is avoided by the quadratic approximation technique and
a backtracking algorithm. Moreover, while existing methods require typically strict and complex
conditions, such as Kurdyka-ojasiewicz (KL) properties (Lau et al. (2018)) to prove convergence, our
proposed method requires simple and mild conditions to guarantee convergence and covers most of
the commonly-used loss functions and activation functions, and the choice of hyperparameters has no
effect on the convergence of our DLAM algorithm theoretically. Our contributions include:
•	We propose a novel formulation for deep neural network optimization. The deeply nested acti-
vation functions are disentangled into separate functions innovatively coordinated by inequality
constraints that are inherently convex.
•	We present a novel and efficient DLAM algorithm. A quadratic approximation technique and a
backtracking algorithm are utilized to avoid matrix inversion. Every subproblem has a closed-form
solution, further boosting efficiency.
•	We investigate several attractive convergence properties of the DLAM algorithm under mild
conditions. The model assumptions are very mild, ensuring that most deep learning problems will
satisfy our assumptions. The new DLAM algorithm is guaranteed to converge to a critical point.
•	We conduct experiments on benchmark datasets to validate our proposed DLAM algorithm.
Experiments on two benchmark datasets show that the new algorithm performs well compared
with SGD or its variants and ADMM.
The rest of paper is organized as follows. In Section 2, we present the problem formulation and
the new DLAM algorithm. In Section 3, we introduce the main convergence results for the DLAM
algorithm. Section 4 reports the results of the extensive experiments conducted to validate the
convergence and effectiveness of the new DLAM. Section 5 concludes by summarizing the research.
2	The DLAM algorithm
In this section, we present our novel DLAM algorithm. Section 2.1 provides the new algorithm’s
formulation and Section 2.2 shows how the DLAM algorithm solve all the subproblems.
2
Under review as a conference paper at ICLR 2020
2.1	Inequality Approximation for Deep Learning
The important notations used in this paper are listed in Table 1. A typical fully-connected deep
neural network consists of L layers, each of which are defined by a linear mapping and a nonlinear
activation function. A linear mapping is composed of a weight vector Wl ∈ Rnl ×nl-1 , where nl is
the number of neurons on the l-th layer and an intercept bl ∈ Rnl ; a nonlinear mapping is defined by
an activation function hι(∙). Given an input a— ∈ Rnl-I from the (l - 1)-th layer, the l-th layer
outputs al = hl(Wlal-1 + bl). By introducing an auxiliary variable zl as the temporary result of the
linear mapping, the deep neural network problem is formulated mathematically as follows:
Problem 1:	minal,wl,bl,zl R(zl； y) +	Ω1(W1)
s.t. Zl = Wlal-I +bι(l = 1,…,L), aι = h(zι)(l = 1,…，L-1)
where a0 = x ∈ Rd is the input of the deep neural network, d is the number of feature dimensions,
and y is a predefined label vector. R(zL; y) ≥ 0 is the risk function for the L-th layer, which is convex
and proper, and Ωl(Wl) ≥ 0 is a regularization term on the l-th layer, which is also convex and
proper. The equality constraint al = hl(zl) is the most challenging to handle here, because common
activation functions such as tanh and smooth sigmoid are nonlinear. This makes them nonconvex
constraints and hence it is difficult to obtain a global minimum when updating zl (Taylor et al. (2016)).
To deal with this challenge, we innovatively transform the original nonconvex constraints into convex
inequality constraints, which can be infinitely approximate to Problem 1. To do this, we introduce a
tolerance εl > 0 and reformulate Problem 1 to reach the following form:
minWl,bl,zl,al R(ZL； y)+Xl=1 ω1 (Wl ) + X： Rhl(Zl)-εl ≤ al ≤ hl(zl )+ εl)
s.t.zl = Wl al-1 + bl(l = L …,L)
I(hl(Zl) - εl ≤ al ≤ hl(Zl) + εl) is an indicator function such that the value is 0 if hl(Zl) - εl ≤
al ≤ hl(Zl) + εl and ∞ otherwise. For the linear constraint Zl = Wlal-1+bl, this can be transformed
into a penalty term in the objective function to minimize the difference between Zl and Wlal-1+bl.
The formulation is shown as follows:
Problem 2: minwl,bl,zl,al F (W, b,z, a) = R(ZL y) + £5Cl(Wl)
+X φ(al-1,Wl,bl,Zl) +X	I(hl(Zl) -εl ≤ al ≤ hl(Zl) +εl)
The penalty term is defined as φ(al-ι, Wl,bl,zl) = (ρ∕2)∣∣zl - Wlal-I - blk2, where P > 0 a penalty
parameter. W = {Wl}lL=1, b = {bl}lL=1, z = {Zl}lL=1,a = {al}lL=-11. The reason for introducing
εl is that it allows us to project the nonlinear constraint to a convex εl-ball, thus transforming the
nonconvex Problem 1 into the multi-convex Problem 2, which is much easier to solve. Here, a
multi-convex problem means this problem is convex with regard to one variable while fixing others.
For example, Problem 2 is convex with regard to z when W, b, and a are fixed. As ρ → ∞ and
εl → 0, Problem 2 approaches Problem 1.
2.2 Alternating Optimization
We present DLAM algorithm developed to solve Problem 2, shown in Algorithm 1. Lines 4, 5, 7, and
10 update Wl, bl,Zl and al, respectively, and the four relevant subproblems are discussed as below:
1. Update Wl
The variables Wl (l = 1, ∙∙∙ ,L) are updated as follows:
Wlk+1 J argminwl Φ(ak+1 ,Wl,bk,zlk) + Ωl(Wl)	(1)
Because Wl and al-ι are coupled in φ(∙), solving Wl requires an inversion operation of ak-+1,
which is computationally expensive. Motivated by deep learning Alternating Direction Method of
Multipliers (dlADMM) (Wang et al. (2019)), we define Plk+1(Wl； θlk+1) as a quadratic approximation
of φ at Wlk , which is mathematically reformulated as follows (Beck & Teboulle (2009)):
Plk+1(WMk+1)= φ(a-ι1,Wlk,zlk,bk)+ < VWlkφWl -Wlk > +kθlk+1 ◦ (Wl - Wlk)。2仙/2
where θlk+1 > 0 is a parameter vector, ◦ denotes Hadamard product (the elementwise product), and
aob denotes a to the Hadamard power of b and k∙k ι is the 'ι norm. < •, • > is a Frobenius inner
product.Vwkφ = P(Wlkak-11 + bf - zlk)(ak-ι1)τ(l = 1,…，L) is the gradient of φ with regard
to Wl at Wlk. Obviously, Plk+1(Wlk； θlk+1) = φ(alk-+11, Wlk, blk, Zlk). Rather than minimizing the
3
Under review as a conference paper at ICLR 2020
Algorithm 1 DLAM Algorithm for Solving Problem 2
Require: y, a0 = x.
Ensure： aι ,Wι,bι, zι(l = 1,… ,L).
1:	initialize ρ, k = 0.
2:	repeat
3:	for l = 1 to L do
4:	Update Wlk+1	using Algorithm 2.
5:	Update blk+1 in equation 3.
6:	if l = L then
7:	Update zlk+1 in equation 5.
8:	else
9:	Update zlk+1 in equation 4.
10:	Update alk+1 using Algorithm 3.
11:	end if
12:	end for
13： k -⅛— k + 1.
14:	until convergence.
15： Output al , Wl , bl , zl.
original subproblem in equation 1, we instead minimize the following：
Wιk+1 - argminwι P∕+1(Wι; θ∣k+1) + Ω1(W1)	⑵
For Ωι(Wι), common regularization terms like '1 or '2 regularizations lead to closed-form solutions.
As for the choice of θlk+1, the backtracking algorithm is shown in Algorithm 2. Specifically, for a
given θιk+1, we minimize equation 2 to obtain Wιk+1 until the condition in Line 3 is satisfied. The
time complexity of Algorithm 2 is O(d2 3), where d is the dimension of the neurons or features.
Algorithm 2 Backtracking Algorithm to update Wιk+1
Require: alk-+11, Wlk, blk, zlk, ρ, some constant γ > 1.
Ensure: θlk+1,Wlk+1.
1： initialize α.
2： update ζ in equation 2 where θlk+1 = α.
3： whileφ(alk-+11,ζ,blk,zlk) > Plk+1(ζ; α) do
4:	α — αγ.
5： update ζ in equation 2 where θlk+1 = α.
6:	end while
7:	Output θk+1 — a.
8:	Output wf+1 — ζ.
2. Update bι
The variables bι(l = 1,…，L) are updated as follows:
bk+1 - argminbι。(。”11,卬/+1,d,zk).
Similarly to updating Wι, we define Uιk+1(bι; Lb) as a quadratic approximation of φ at bιk, which is
formulated mathematically as follows (Beck & Teboulle (2009)):
Uk+1(bι ； Lb) = Φ(a"ι1,Wιk+1,bk,zk) + (ak Φ)T (bι-甘)+ 也/2)|面-bf∣∣2.
where Lb ≥ P is a parameter and Vbkφ = ρ(bf + Wk+1ak+1 - Zk). Here, Lb ≥ P is required for
the convergence analysis (Beck & Teboulle (2009)). Without loss of generality, we set Lb = ρ. We
can now solve the following subproblem:
bk+1 - argminbι Uk+1(bι; P)	(3)
The solution to equation 3 is: bf+1 J W — VbkΦ∕ρ.
3. Update zι
The variables zι(l = 1,…，L) are updated as follows:
zιk+1 J argminzlφ(aιk-+11,Wιk+1, bιk+1,zι)+I(hι(zι)-ει ≤ aιk ≤ hι(zι)+ει)(l <L)
zLk+1 J arg minzL φ(akL+-11,WLk+1,bkL+1, zL) + R(zL; y)
4
Under review as a conference paper at ICLR 2020
As when updating bl, we define Vlk+1 (zl; Lz) as a quadratic approximation of φ at zlk, which is
formulated mathematically as follows:
Vlk+1(ziLz )= φ(a-1,Wk+1,bk+1,zk) + (Vzk φ)T (ZLzk ) + (Lz∕2)kzι-zk k2
where Lz ≥ P is a parameter and Vzk φ = ρ(zk 一 Wk+1Ok+1 一 bk+1). Without loss of generality, we
set Lz = ρ. Obviously, Vlk+1 (zlk; ρ) = φ(alk-+11, Wlk+1, blk+1, zlk). Hence, we solve the problems:
zk+1 - argminzι Vlk+1(zι; P) +I(hι(zι)-ει ≤ ak ≤ ht(zι)+ε"(l<L)	(4)
zL+1 - argminzL Vk+1(zL; ρ) + R(zL； y)	(5)
As for zι(l = 1, ∙∙∙ ,l 一 1), the solution is
zιk+1 - min(max(Bk+1 ,zk -Vφzk∕ρ'),B2~+'').
where Bk+1 and Bk+1 represent the lower bound and the upper bound of the set {zι∣hι(zι) 一 ει ≤
aιk ≤ hι(zι)+ει}. equation 5 is easy to solve using the Fast Iterative Soft Thresholding Algorithm
(FISTA) (Beck & Teboulle (2009)).
4.	Update aι
The variables aι (l = 1,…，L 一 1) are updated as follows:
ak+1 - argminaι φ(aι, Wι+ι,bk+ι,zk+ι) + I(hι(zk+1) — ε ≤ aι ≤ hι(zιk+1) + ε)
As when solving Wιk+1, the quadratic approximation of φ at aιk is defined as
Qk+1(aιk+1) = φ(ak,Wι+ι,b+ι,zι+ι) + (Vakφ)T(i)+ 什户1 ◦ (aι - af )。2仙/2
and this allows us to solve the following problem instead:
ak+1 - argminaι Qk+1(aι;τk+1) + I(hι(zk+1) - ει ≤ aι ≤ hι(zk+1) + ει)	(6)
where τk+1 > 0 is a parameter vector. Vak Φ = P(Wι+ι)T (Wt+ιak + bk+ι — zιk+ι)(l = 1,…，L — 1)
is the gradient of φ with regard to aι at aιk. Obviously, Qιk+1(aιk; τιk+1) = φ(aιk, Wιk+1, bιk+1, zιk+1).
Because Qιk+1(aι; τιk+1) is a quadratic function with respect to aι, the solution can be obtained
by ak+1 J O 一 Vak Φ∕τk+1 given a suitable τk+1. Now the main focus is how to choose τk+1.
Similar to Algorithm 2, the backtracking algorithm for finding a suitable τιk+1 is shown in Algorithm
3. The time complexity of Algorithm 3 is O(d2), where d is the dimension of neurons or features.
Algorithm 3 Backtracking Algorithm to update aιk+1
Require: alk, Wlk+1 , zlk+1, zlk+1, blk+1, ρ, some constant η > 1.
Ensure: τlk+1 ,alk+1 .
1:	Pick up t such that β = ak — Va⅛ φ∕t and hi (zk+1) — ει ≤ β ≤ hi (zk+1) + ε%.
2:	while φ(β, Wlk+1, zlk+1, blk+1) > Qlk+1 (β; t) do
3:	t — tη.
4:	β J ak — Vφak /t.
5:	end while
6:	Output τik+1 J t.
7:	Output ai	J β.
3	Convergence Analysis
In this section, we present the main convergence analyses for the DLAM algorithm. Specifically,
Section 3.1 introduces the assumption necessary to guarantee convergence. The main convergence
properties of the new DLAM algorithm are presented in Section 3.2. Due to space limit, the discussion
on the convergence conditions of the DLAM algorithm is in Section G in the supplementary materials.
3.1	Assumption
Firstly, we make the following assumption:
Assumption 1 (Quasilinearity). Activationfunctions hι(zι)(l = 1,…,n) are quasilinearfunctions.
The definition of Quasilinearity is given in the supplementary material. Assumption 1 is a mild
condition to ensure that the nonlinear constraint aι = hι (zι ) in Problem 1 is projected in a convex
5
Under review as a conference paper at ICLR 2020
set, and it allows for nonsmooth functions. Fortunately, most of the widely used nonlinear activation
functions, including tanh (Zamanlooy & Mirhassani (2014)), smooth sigmoid (Glorot & Bengio
(2010)), and the rectified linear unit (Relu) (Maas et al. (2013)) that are quasilinear.
Notice that no assumption is needed to imposed on the risk function R(zL ; y) and the regularization
term Ω1(W1). Therefore, they can be either smooth or nonsmooth: R(zl; y) can be a common least
square loss or cross entropy loss; Ω1(W1) can be a either '1 or '2 regularization term. They fit neatly
into our framework and incorporate several important theoretical properties.
3.2	Key Convergence Properties
We introduce several important convergence properties possessed by the DLAM algorithm in this
section. If Assumption 1 holds, then Lemmas 1-3 stated below are satisfied. These three lemmas
are proven to be possessed by the DLAM algorithm, and are key for demonstrating the theoretical
merits of DLAM; the proofs of them are provided in the supplementary materials. Finally, the global
convergence and convergence rate of the DLAM are proved based on Lemmas 1-3 stated as follows:
Lemma 1 (Sufficient Descent). For any ρ > 0 and εl > 0, we have
F(Wk,bk,Zk,ak) - F(Wk+1,bk+1,Zk+1,ak+1) ≥ X∖ kθlk+1 ◦ (Wlk+1 - WlkT2kι∕2+
(ρ∕2)X=1 M+1-bkk2 + (ρ∕2)X=1 kzk+1-zlkk2 + XL-11 kτlk+1 ◦ (alk+1 - alk产k1∕2	⑺
Lemma 1 depicts the monotonic decrease of the objective value during iterations. The proof of
Lemma 1 requires Assumption 1 and is detailed in the supplementary materials.
Lemma 2 (Convergent Sequence). (a). (Wk,bk,Zk, ak) andF(Wk, bk,Zk,ak) are convergent. That
is, as k → ∞, (Wk, b k, Z k, a k) → (W *, b *, Z *, a *) and F (Wk, b k, z k, a k) → F (W *, b *,z *, a *).
(b). (Wk,bk,Zk, ak) is bounded. That is, there exist scalars MW,Mb,MZ and Ma such that kWkk ≤
MW, kbkk ≤ Mb, kZkk ≤ MZ and kakk ≤Ma.
Lemma 2 guarantees that the variable (Wk , bk , zk , ak ) is convergent and bounded. The proof of
Lemma 2 requires Lemma 1 and can be found in the supplementary materials.
Lemma 3 (Subgradient Bound). There exist C = max(ρMa, ρMa2 + kθ1k+1 k, ρMa2 +
kθ2k+1k,…，ρM2 + ∣∣θL+1k), some gf+1 ∈ ∂Wk+ιF andg2+1 = Vbk+ι F such that
∣g1k+1∣≤C(∣Wk+1-Wk∣+∣bk+1-bk∣+∣Zk+1-Zk∣), ∣g2k+1∣ =ρ∣Zk+1-Zk∣
Lemma 3 ensures that the subgradient of the objective function is bounded by variables. The proof of
Lemma 3 requires Lemma 2 and the proof process is elaborated in the supplementary materials. We
will now move on to present the global convergence of the DLAM algorithm using the following two
theorems. The first theorem presents the global convergence of the DLAM algorithm.
Theorem 1 (Convergence to the Critical Point). For (W, b) in Problem 2, starting from any (W0,b0)
,Algorithm 1 converges to a critical point (W *, b *). That is, 0 ∈ ∂ W * F and 0 ∈ ∂ b * F.
Proof. By Lemma 2, (Wk, bk) is convergent to (W*, b*). From Lemma 3, there exist gk+1 ∈
∂wk+ιF and gk+1 ∈ ∂bk+ιF such that kgk+1k → 0 and ∣∣gk+1k → 0 as k → ∞. We have
0 ∈ ∂w* F and 0 ∈ ∂b* F. In other words, (W*, b*) is a critical point of F.	□
Theorem 1 shoWs that our proposed DLAM algorithm converges to a critical point globally no matter
what ρ and εl are chosen. This ensures that our DLAM algorithm is parameter-restriction free, so the
choice of hyperparameters has no effect on its convergence.
The next theorem shows that the convergence rate of DLAM is o(1∕k), which is shown as follows:
Theorem 2 (Convergence Rate). For (Wk, bk,Zk, ak), define ck = min0≤i≤k(PlL=1 ∣θli+1 ◦ (Wli+1 -
Wli)。2k1∕2+(ρ∕2)P=1 kbi+1-bik2 + (ρ∕2)P=1 kzi+1-zik2 + PL-lγkTli+1 ◦ (ai+1 - ai)-2k1∕2),
which reflects the convergence rate of Algorithm 1, then the convergence rate ofck is o(1∕k).
Proof. The proof of this theorem is in the supplementary materials.	□
4	Experiments
The DLAM algorithm is evaluated by several benchmark datasets. Effectiveness, efficiency and
convergence properties of DLAM are compared with state-of-the-art methods. All experiments were
conducted on 64-bit Ubuntu16.04 LTS with Intel(R) Xeon processor and GTX1080Ti GPU.
6
Under review as a conference paper at ICLR 2020
4.1	Experiment Setup
4.1.1	Dataset
In this experiment, two benchmark datasets were used for comparison: MNIST (LeCun et al. (1998))
and Fashion MNIST (Xiao et al. (2017)). The MNIST dataset has ten classes of handwritten-digit
images, which was firstly introduced by Lecun et al. in 1998 (LeCun et al. (1998)). It contains 55,000
training samples and 10,000 test samples with 196 features each, which is provided by the Keras
library (Chollet (2017)). Unlike the MNIST dataset, the Fashion MNIST dataset has ten classes
of assortment images on the website of Zalando, which is Europes largest online fashion platform
(Xiao et al. (2017)). The Fashion-MNIST dataset consists of 60,000 training samples and 10,000 test
samples with 784 features each.
4.1.2	Experiment Settings
We set up two different architectures of multi-layer neural networks in the experiment. Two network
structures contained two hidden layers with 100 and 500 hidden units each, respectively. The rectified
linear unit (Relu) was used for the activation function for both network structures. The loss function
was set as the deterministic cross-entropy loss. ρ was set to 10-4. ε was initialized as 10 and
updated adaptively as follows: if R(zLk ; y) > 10εk, εk+1 = max(2εk, 1); if εk > 10R(zLk ; y),
εk+1 = min(εk /2, 0.01), which balances between the loss function R(zL; y) and ε. The number of
iteration was set to 150. In the experiment, one iteration means one epoch.
4.1.3	Comparison Methods
Since this paper focuses on fully-connected deep neural networks, SGD and its variants and ADMM
are state-of-the-art methods and hence were served as comparison methods. For SGD-based methods,
the full batch dataset is used for training models. All parameters were chosen by the accuracy of
the training dataset. The baselines are: 1) Stochastic Gradient Descent (SGD) (Bottou (2010)).
The SGD and its variants are the most popular deep learning optimizers, whose convergence has
been studied extensively in the literature. 2) Adaptive gradient algorithm (Adagrad) (Duchi et al.
(2011)). Adagrad is an improved version of SGD: rather than fixing the learning rate during iteration,
it adapts the learning rate to the hyperparameter. 3) Adaptive learning rate method (Adadelta)
(Zeiler (2012)). As an improved version of the Adagrad, the Adadelta is proposed to overcome the
sensitivity to hyperparameter selection. 4) Alternating Direction Method of Multipliers (ADMM)
(Taylor et al. (2016)). ADMM is a powerful convex optimization method because it can split an
objective function into a series of subproblems, which are coordinated to get global solutions. It is
scalable to large-scale datasets and supports parallel computations.
4.2	Experimental Results
In this section, experimental results of DLAM algorithm are analyzed against comparison methods.
4.2	. 1 Convergence
First, we show that our proposed DLAM algorithm converges for both the MNIST dataset
and the Fashion MNIST dataset. The convergence of DLAM algorithm is shown in Figure 1.
The X axis and Y axis denote the number of
iterations and the logarithm of objective value,
respectively. Overall, the objective value de-
creased monotonically during iteration what-
ever network structures and datasets we choose.
Specifically, the objective value dropped tremen-
dously at the early stage and then converged
smoothly towards the critical point of the prob-
lem. We also found that the objective value
for the Fashion MNIST dataset decreased more
quickly than that for the MNIST dataset.
Figure 1: Convergence curves of DLAM algorithm
on MNIST and Fashion MNIST datasets for two neural
network structures: DLAM algorithm converged.
4.2.2 Performance
Figure 2 and Figure 3 show the curves of the training accuracy and test accuracy of our
proposed DLAM algorithm and baselines, respectively. Overall, both the training accuracy
7
Under review as a conference paper at ICLR 2020
AUejnUSV £U"J1
0	20 40 60 80 100 120 140	0	20 40 60 80 100 120 140	- 0	20 40 60 80 100 120 140	0	20 ⅛ SO 80 100 120 140
Iteration	Iteration	Iteration	Iteration
(a) Size of 100×100 on MNIST (b) Size of 100×100 on Fashion MNIST (c) Size of 500×500 on MNIST (d) Size of 500×500 on Fashion MNIST
Figure 2:	Training aCCuraCy of all methods for the MNIST and Fashion MNIST datasets on two
neural network struCtures: DLAM algorithm performed Competitively.
0.8
0.7
δ,0.6
S
805
< 0.4
班3
0.2
0.1
I I
&S3UU< Jsa1-
O 20 40 €0 BO IOO 120 140
Iteration
(a)	Size of 100×100 on MNIST
0	20 40 60 80 100 120 140
Iteration
(C)SiZe of 500×500 on MNIST
&S3UU< Jsa1-
→- SGD
-Adadelta
——Adagrad
-ADMM
-→- DLAM
0 2C 40 €0 BO 100 120 140
Iteration
(b)	Size of 100×100 on Fashion MNIST
0	20 40 60 BO 100 120 140
Iteration
(d) Size of 500×500 on Fashion MNIST
Figure 3:	Test aCCuraCy of all methods for the MNIST and Fashion MNIST datasets on two neural
network struCtures: DLAM algorithm performed Competitively.
and the test aCCuraCy of our proposed DLAM
outperformed all baselines for the MNIST
dataset, while those of our proposed DLAM
algortihm performed Competitively for the Fash-
ion MNIST dataset. SpeCifiCally, the Curves
of our DLAM algorithm soared to 0.7 at the
early stage, and then raised steadily towards to
0.8 or more. The Curves of the SGD-related
methods, SGD, Adadelta, and Adagrad, moved
more slowly than our proposed DLAM algo-
rithm. The Curves of the ADMM also roCketed
to around 0.8, but deCreased slightly later on.
4.2.3 Efficiency
MNIST dataset: From 11,000 to 55,000 training samples
SiZe P	11000	22000	33000	44000	55000
0.0001	0.1692	0.3216	0.5010	0.7164	0.9413
0.001	0.2061	0.4328	0.6951	0.9792	1.2442
0.01	0.3334	0.6516	1.0277	1.3956	1.7783
0.1	0.4795	0.9428	1.4524	1.959	2.4410
1	0.7684	1.4810	2.2626	3.0299	3.7504
FashionMNIST dataset: From 12,000 to 60,000 training samples					
SiZe P	12,000	24,000	36,000	48,000	60,000
0.0001	0.2500	0.5081	0.8492	1.1911	1.5092
0.001	0.2980	0.5980	0.9595	1.3265	1.6744
0.01	0.4199	0.8028	1.2787	1.7535	2.2025
0.1	0.5758	1.0928	1.7230	2.3261	2.9234
1	0.8795	1.6464	2.5580	3.4492	4.2902
Table 2: The relation between running time per iteration
(in seCond) and size of training samples as well as value
of ρ: generally, the running time inCreased as the training
sample and the value of ρ beCame larger.
In this subseCtion, the relationship between run-
ning time per iteration of our proposed DLAM
algorithm and two potential faCtors, namely, the
value of ρ, the size of training sample was ex-
plored. The running time was CalCulated by the
average of 150 iterations. The Computational result for the MNIST dataset and Fashion MNIST
dataset on the 100 × 100 neural network is shown in Table 2. The number of training samples of the
MNIST dataset ranged from 11,000 to 55,000, with an inCrease of 11,000 eaCh time, whereas The
number of training samples of the Fashion MNIST dataset ranged from 12,000 to 60,000, with an
inCrease of 12,000 eaCh time. The value of ρ ranged from 0.0001 to 1, with multiplying by 10 eaCh
time. Generally, the running time inCreased as the training sample and the value of ρ beCame larger.
5 Conclusion
Even though stoChastiC gradient desCent (SGD) is a popular method to train deep neural networks,
alternating minimization methods have attraCted inCreasing attention from a great deal of researChers
reCently as they have several advantages inCluding solid theoretiCal guarantees and avoiding gradient
vanishing problems. In this paper, we propose a novel formulation of the original deep neural network
problem and a novel Deep Learning Alternating Minimization (DLAM) algorithm. SpeCifiCally, the
nonlinear Constraint is projeCted into a Convex set so that all subproblems are solvable. At the same
time, the quadratiC approximation teChnique and the baCktraCking algorithm are applied to boost up
sCalability. Furthermore, several mild assumptions are established to prove the global ConvergenCe of
our DLAM algorithm. Experiments on real-world datasets demonstrate the ConvergenCe, effeCtiveness,
and effiCienCy of our DLAM algorithm.
8
Under review as a conference paper at ICLR 2020
References
Armin Askari, Geoffrey Negiar, Rajiv Sambharya, and Laurent El Ghaoui. Lifted neural networks.
arXiv preprint arXiv:1805.01532, 2018.
Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse
problems. SIAM journal on imaging sciences, 2(1):183-202, 2009.
Leon Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of
COMPSTAT’2010, pp. 177-186. Springer, 2010.
Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.
Miguel Carreira-Perpinan and Weiran Wang. Distributed optimization of deeply nested systems. In
Artificial Intelligence and Statistics, pp. 10-19, 2014.
Francois Chollet. Deep learning with python. Manning Publications Co., 2017.
Anna Choromanska, Sadhana Kumaravel, Ronny Luss, Irina Rish, Brian Kingsbury, Ravi Tejwani,
and Djallel Bouneffouf. Beyond backprop: Alternating minimization with co-activation memory.
arXiv preprint arXiv:1806.09077, 2018.
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural
networks with binary weights during propagations. In Advances in neural information processing
systems, pp. 3123-3131, 2015.
Wei Deng, Ming-Jun Lai, Zhimin Peng, and Wotao Yin. Parallel multi-block admm with o (1/k)
convergence. Journal of Scientific Computing, 71(2):712-736, 2017.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.
Saeed Ghadimi and Guanghui Lan. Accelerated gradient methods for nonconvex nonlinear and
stochastic programming. Mathematical Programming, 156(1-2):59-99, 2016.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the thirteenth international conference on artificial intelligence and
statistics, pp. 249-256, 2010.
Tim Tsz-Kit Lau Shaobo Lin Yuan Yao Jinshan Zeng, Shikang Ouyang. Global convergence
in deep learning with variable splitting via the kurdyka-ojasiewicz property. arXiv preprint
arXiv:1803.00225, 2018.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolu-
tional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
Tim Tsz-Kit Lau, Jinshan Zeng, Baoyuan Wu, and Yuan Yao. A proximal block coordinate descent
algorithm for deep neural network training. 2018.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. Rectifier nonlinearities improve neural network
acoustic models. 30(1):3, 2013.
Tomas Mikolov, Martin Karafiat, Lukas Burget, Jan Cernocky, and SanJeev Khudanpur. Recurrent
neural network based language model. In Eleventh Annual Conference of the International Speech
Communication Association, 2010.
Boris T Polyak. Some methods of speeding up the convergence of iteration methods. USSR
Computational Mathematics and Mathematical Physics, 4(5):1-17, 1964.
9
Under review as a conference paper at ICLR 2020
Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In
International Conference on Learning Representations, 2018. URL https://openreview.
net/forum?id=ryQu7f-RZ.
Herbert Robbins and S Monro. aa stochastic approximation method, o annals math. Statistics, 22:
400-407, 1951.
R Tyrrell Rockafellar and Roger J-B Wets. Variational analysis, volume 317. Springer Science &
Business Media, 2009.
David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by
back-propagating errors. nature, 323(6088):533, 1986.
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization
and momentum in deep learning. In International conference on machine learning, pp. 1139-1147,
2013.
Gavin Taylor, Ryan Burmeister, Zheng Xu, Bharat Singh, Ankit Patel, and Tom Goldstein. Training
neural networks without gradients: A scalable admm approach. In International Conference on
Machine Learning, pp. 2722-2731, 2016.
T Tieleman and G Hinton. Divide the gradient by a running average of its recent magnitude. coursera:
Neural networks for machine learning. Technical report, Technical Report. Available online:
https://zh. coursera. org/learn/neuralnetworks/lecture/YQHki/rmsprop-divide-the-gradient-by-a-
running-average-of-its-recent-magnitude (accessed on 21 April 2017).
Junxiang Wang, Fuxun Yu, Xiang Chen, and Liang Zhao. Admm for efficient deep learning with
global convergence. In Proceedings of the 25th ACM SIGKDD International Conference on
Knowledge Discovery & Data Mining, KDD ’19, pp. 111-119, New York, NY, USA, 2019. ACM.
ISBN 978-1-4503-6201-6. doi: 10.1145/3292500.3330936. URL http://doi.acm.org/10.
1145/3292500.3330936.
Yu Wang, Wotao Yin, and Jinshan Zeng. Global convergence of admm in nonconvex nonsmooth
optimization. Journal of Scientific Computing, pp. 1-35, 2015.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking
machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.
Yangyang Xu and Wotao Yin. A block coordinate descent method for regularized multiconvex
optimization with applications to nonnegative tensor factorization and completion. SIAM Journal
on imaging sciences, 6(3):1758-1789, 2013.
Babak Zamanlooy and Mitra Mirhassani. Efficient vlsi implementation of neural networks with
hyperbolic tangent activation function. IEEE Transactions on Very Large Scale Integration (VLSI)
Systems, 22(1):39-48, 2014.
Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701,
2012.
Jinshan Zeng, Shao-Bo Lin, and Yuan Yao. A convergence analysis of nonlinearly constrained admm
in deep learning. arXiv preprint arXiv:1902.02060, 2019.
Guoqiang Zhang and W Bastiaan Kleijn. Training deep neural networks via optimization over graphs.
arXiv preprint arXiv:1702.03380, 2017.
Ziming Zhang and Matthew Brand. Convergent block coordinate descent for training tikhonov
regularized deep neural networks. In Advances in Neural Information Processing Systems, pp.
1721-1730, 2017.
Ziming Zhang, Yuting Chen, and Venkatesh Saligrama. Efficient training of very deep neural networks
for supervised hashing. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 1487-1495, 2016.
10
Under review as a conference paper at ICLR 2020
Appendix
A	Definitions
First, the definition of Frechet subdifferential is shown as follows (Rockafellar & Wets (2009)):
Definition 1 (Frechet Subdifferential). For each xι ∈ dom(uι), the Frechet subdifferential of uι at
x1, which is denoted as ∂u1(x1), is the set of vectors v, which satisfy
lim inf (u1 (x2) - u1(x1) - vT (x2 - x1))/kx2 - x1k ≥ 0.
x2 6=x1 x2 →x1
The vector V ∈ ∂uι(xι) is a Frechet subgradient.
Then the definition of the limiting subdifferential, which is based on Frechet subdifferential, is given
in the following (Rockafellar & Wets (2009)):
Definition 2 (Limiting Subdifferential). For each x ∈ dom(u2), the limiting subdifferential (or
subdifferential) of u2 at x is
∂u2(x) ={vι∣∃ Xk → x, s.t. U2(χk) → U2(x),
Vk ∈ ∂u2(χk ),Vk → V}
where xk is a sequence whose limit is x and the limit of u2(xk) is u2 (x), vk is a sequence, which
is a Frechet subgradient of u2 at Xk and whose limit is V. The vector V ∈ ∂u2(x) is a limiting
subgradient.
Specifically, when u2 is convex, its limiting subdifferential is reduced to regular subdifferential
(Rockafellar & Wets (2009)); The limiting subdifferential is used to prove the global convergence
of the DLAM in the following convergence analysis. Without loss of generality, ∂R and
∂Ωι (l = 1, ∙∙∙ ,n) are nonempty, and the limiting subdifferential of F defined in Problem 2 is (XU
& Yin (2013)):
∂F(W,b,z,a)= ∂wF X VbF X ∂zF X ∂aF
where × means the Cartesian product.
Next, recall the definition of quasilinearity (Boyd & Vandenberghe (2004)):
Definition 3. A function f(X) is quasiconvex iffor any sublevel set Sα(f) = {X|f (X) ≤ α} is a
convex set. Likewise, A function f(X) is quasiconcave if for any superlevel set Sα(f) = {X|f (X) ≥
α} is a convex set. A function f(X) is quasilinear if it is both quasiconvex and quasiconcave.
B	Preliminary Lemmas
In this section, we give preliminary lemmas which are useful for the proofs of three properties. The proofs
of Lemmas 4 and 5 both require Assumption 1. The proof of Lemma 6 requires Lemmas 4 and 5. To
simplify the notation, Wk≤+l 1 = {{Wik+1}li=1, {Wik}iL=l+1}, bk≤+l 1 = {{bik+1}li=1, {bik}iL=l+1}, zk≤+l 1 =
{{zik+1}li=1, {zik}iL=l+1} and ak≤+l 1 = {{aik+1}li=1, {aik}iL=-l+1 1}.
Lemma 4. equation 2 holds if and only if there exists S ∈ ∂Ωι(Wk+1), the subgradient of Ωι (Wk+1) such
that
▽wk Φ + θk+1 ◦ (Wk+1 - Wk) + S = 0
Likewise, equation 4 holds if and only if there exists r ∈ ∂I(hl (zlk+1) - εl ≤ alk ≤ hl (zlk+1) + εl) such that
▽	zk φ + ρ(zlk+1 - zlk) + r = 0
equation 5 holds if and only if there exists u ∈ ∂R(zLk+1 ; y) such that
▽	zkL φ + ρ(zLk+1 - zLk) + u = 0
equation 6 holds if and only if there exists v ∈ ∂I(hl (zlk+1) - εl ≤ alk+1 ≤ hl (zlk+1) + εl) such that
▽	alk φ + ρ(alk+1 - alk) + v = 0
Proof. These can be obtained by directly applying the optimality conditions of equation 2, equation 4, equation 5
and equation 6, respectively.	□
11
Under review as a conference paper at ICLR 2020
Lemma 5. For equation 4, equation 5 and equation 3, if Lb ≥ ρ and Lz ≥ ρ,then the following inequalities
hold:
Ulk+1(blk+1;Lb) ≥ φ(alk-+11,Wlk+1,blk+1,zlk)
Vlk+1(zlk+1; Lz) ≥φ(alk-+11,Wlk+1,blk+1,zlk+1)
(8)
(9)
Proof. Because φ(al-1, Wl, bl, zl) is differentiable continuous with respect to bl and zl with Lipschitz coeffi-
cient ρ (the definition of Lipschitz differentiablity can be found in (Beck & Teboulle (2009))), we directly apply
Lemma 2.1 in (Beck & Teboulle (2009)) to φ to obtain equation 8 and equation 9, respectively.	□
Lemma 6. It holds thatfor ∀k ∈ N and l = 1, 2,…，L,
F(Wk≤+l1-1,bk≤+l-11,zk≤+l-11,ak≤+l-11)-F(Wk≤+l1,bk≤+l-11,zk≤+l-11,ak≤+l-11)
≥ ∣∣θk+1 ◦(Wlk+1 - Wk)-2kι∕2.	(10)
F (Wk+1 bk+1 k+1 k+1 ) F (Wk+1 bk+1 k+1 k+1 )
F (W≤l , b≤l-1 , z≤l-1 , a≤l-1 ) - F (W≤l , b≤l , z≤l-1 , a≤l-1 )
≥ (ρ∕2)kbk+1-bkk2.	(11)
F (Wk+1 bk+1 k+1 k+1 ) F (Wk+1 bk+1 k+1 k+1 )
F (W≤l , b≤l , z≤l-1 , a≤l-1 ) - F (W≤l , b≤l , z≤l , a≤l-1 )
≥ (ρ∕2)∣zlk+1-zlk∣22.	(12)
F (Wk+1 bk+1 k+1 k+1 ) F (Wk+1 bk+1 k+1 k+1 )
F (W≤l ,b≤l , z≤l , a≤l-1) -F(W≤l , b≤l ,z≤l , a≤l )
≥ kτιk+1 ◦ (ak+1- ak)°2kι∕2.	(13)
Proof. Essentially, all inequalities can be obtained by applying optimality conditions of updating Wlk+1, blk+1,
zlk+1 and alk+1, respectively. We only prove equation 10 and equation 12 since equation 13 and equation 11
follow the same routine of equation 10 and equation 12, respectively.
Firstly, we focus on proving equation 10. The stopping criterion of Algorithm 2 shows that
φ(ak^,W∣k+1,bk,zk) ≤ Pιk+1(Wιk+1; θk+1).	(14)
Because Ωw% (Wl) is convex, according to the definition of subgradient, We have
Ωι(Wlk) ≥ Ωl(Wlk+1) + ST(Wk - Wlk+1)	(15)
Where s is defined in the premise of Lemma 4. Therefore, We have
F (Wk+1	bk+1	k+1	k+1	)	F (Wk+1	bk+1	k+1	k+1	)
F (W≤l-1, b≤l-1, z≤l-1, a≤l-1) -F(W≤l , b≤l-1, z≤l-1 , a≤l-1)
=φ(ak-ι1 ,Wlk,bk,zk)+ Ωl(Wlk) - φ(ak+ι1, Wlk+1,bk腐)
一 Ωl(Wlk+1) (Definition of F in Problem 2)
≥ Ωl (Wlk) - Ωl(Wlk+1) - (Vwk φ)T(Wlk+1 - Wk)
-∣θk+1 ◦ (Wk+1 - Wk)02k 1/2( equation 14)
≥ sT (Wlk -Wlk+1)- (VW k φ)T (Wlk+1 -Wlk)-
∣∣θk+1 ◦ (Wk+1 - Wk)。2|| 1/2( equation 15)
=(s+VφWk)(Wlk -Wk+1)-∣θk+1 ◦ (Wk+1-Wk )°2k1∕2
=∣∣θk+1 ◦ (Wlk+1-Wk)°2k 1/2 (Lemma 4).
Secondly, We focus on proving equation 12. For l < L, because I(hl(zl) - εl ≤ alk ≤ hl(zl) + εl) is
convex With regard to zl, according to the definition of subgradient, We have
I(hl(zlk)-εl ≤ alk ≤ hl(zlk)+εl)
≥I(hl(zlk+1)-εl ≤alk ≤ hl(zlk+1)+εl)+rT(zlk -zlk+1)	(16)
12
Under review as a conference paper at ICLR 2020
where r is defined in Lemma 4.
F(Wk≤+l1,bk≤+l1
k+1
≤l-1,
k+1 )	F (Wk+1 bk+1	k+1
a≤l-1) - F (W≤l , b≤l , z≤l
k+1
a≤l-1
z
)
= φ(alk-+11, Wlk+1, blk+1, zlk)+I(hl(zlk)-εl ≤ alk ≤hl(zlk)+εl)
-φ(alk-+11,Wlk+1,blk+1,zlk+1)-I(hl(zlk+1)-εl≤alk≤hl(zlk+1)+εl)
(Definition of F in Problem 2)
≥-(Vzk φ)T(zk+1-zk )-(ρ∕2)kzk+1-zk k2
+I(hl(zlk)-εl≤alk ≤hl(zlk)+εl)
-I(hl(zlk+1)-εl ≤alk ≤hl(zlk+1)+εl)( equation 9)
≥ -(Vzkφ)τ(zk+1 - zk) - (ρ∕2)kzk+1 — Zkk2
+ rT (zlk - zlk+1)( equation 16)
= -(Vzlk φ)T (zlk+1 - zlk) - (ρ∕2)kzlk+1 - zlkk22
+ (Vzlk φ + ρ(zlk+1 - zlk))T (zlk+1 - zlk) (Lemma 4)
= (ρ∕2)kzk+1 — zk k2.
For ZL, the same routine applies.	□
C Proof of Lemma 1
Proof. This can be obtained by adding equation 10, equation 11 and equation 12 from l = 1 to l = L and
equation 13 from l = 1 to l = L — 1.	□
D Proof of Lemma 2
Proof. (a). By Lemma 1, F (Wk , bk , zk , ak) is non-increasing. Also, F (Wk , bk , zk , ak) ≥ 0. According to the
monotonic sequence theorem, F (Wk , bk , zk , ak) is convergent.
Next, we take the limit of Inequality 7 to obtain
lim (F(Wk,bk,zk,ak) - F (Wk+1, bk+1, zk+1, ak+1))
k→∞
≥ k→→∞ XL=Ikθk+1。(Wlk+1 - Wk)°2kι∕2
+(ρ∕2)XlL=1 kblk+1-blkk22+(ρ∕2)XlL=1 kzlk+1-zlkk22
+ XL=； kτlk+1 ◦ (ak+1- ak)02kι∕2
≥0
BecauseF(Wk,bk,zk,ak) is convergent, limk→∞(F (Wk, bk, zk, ak) - F (Wk+1, bk+1, zk+1, ak+1)) = 0.
Therefore, we have
k→∞ XL=Ikθk+1。(Wk+1 — Wlk)°2kι∕2
+(ρ∕2)XlL=1 kblk+1-blkk22+(ρ∕2)XlL=1 kzlk+1-zlkk22
+ XL=11 kτk+1 ◦ (ak+1- ak)°2kι∕2 = 0
Since θlk+1, τlk+1 > 0, we obtain as k → ∞, kWlk+1 - Wlkk → 0, kblk+1 - blkk → 0, kzlk+1 - zlkk → 0,
and ∣∣ak+1 - ak∣∣ → 0. This shows that (Wk, bk, zk, ak) converges to a point (W*, b*, z*, a*). Be-
cause F is continuous, limk→∞(Wk, bk, zk, ak) = F (limk→∞ Wk, limk→∞ bk, limk→∞ zk, limk→∞ ak) =
F(W*, b*, Z, a*).
(b). Because (Wk , bk , zk , ak) is convergent and a convergent sequence is bounded, therefore it is also
bounded.	□
13
Under review as a conference paper at ICLR 2020
E	Proof of Lemma 3
Proof. As shown in (Wang et al. (2015); Xu & Yin (2013)),
∂wk+ι F = {∂wk+1 F} X {∂wk+1 F} ×∙∙∙× {∂wk+1 F}.
▽bk+i F = V, k + 1 F × V, k+1F ×∙∙∙×V,k+1 F.
b1	b2	bL
where X denotes Cartesian Product.
For Wk+1,
dwk+1 F
=∂Ω(Wk+1) + VWk+i φ(α-1,Wk+1,bk+1,zk+1)
(Definition of F in Problem 2)
=VWk+i Φ(ak+ι,Wk+1,b^+1,zk+1)
-VWkΦ(αk+ ,Wk ,bk ,zk) - θk+1 ◦ (Wk+1 - Wk)
+ ∂Ωl(Wlk+1) + VWkφ(αk+ι, Wk,bk,zk)
+θk+1。(Wk+ι - Wk)
=ρ(Wιk+1 - Wk)4+1(αK1)τ + ρ(bk+1 - bk)(ak+1)τ
-ρ(zk+1 - zk)(ak+)τ - θk+1。(Wk+1 - Wk)
+ ∂Ωι (Wιk+1) + VW% Φ(ak+1 Wk ,bk ,zk)
+θk+1。(Wlk+1 - Wlk)
on one hand, we have
kρ(Wk+1 - Wk)ak+1(ak+1)τ + ρ(bk+1 - bk)(αk⅛)τ
-ρ(zk+1 - zk )(ak+1)τ - θk+1。(Wk+1 - Wk )∣∣
≤ Pk(WIk+1 - Wιk)ak+11(ak+11)τ k + ρk(bk+1 - bk)(ak+11)τ∣∣
+ρ∣ (zk+1 -zk)(a-1)τk + kθk+1。(Wk+1 - Wιk) k(triangle inequality)
≤ PkWIk+1 - Wιkk∣ak+Ikkak+11k + ρ∣bk+1 - bkkkak+1k
+ ρkzk+1 - zkkkak+1k + kθk+1kkWk+1 - Wιkk
(Cauchy-Schwarz inequality)
≤ PMa ∣∣bk+1 - bk k + PMakzk+1 - Zkk
(PMai + kθk+1k)kWk+1 - Wιk k(Lemma 2)
on the other hand, the optimality condition of equation 2 yields
0 ∈ ∂Ω(Wk+1)+VWkφ(a-1, Wk, bk, zk)+θk+1。(Wk+1 - Wk)
Therefore, there exists gk+1 ∈ ∂w k+i F such that
Mf+1k ≤ PMakbk+1 - bk k + PMakZk+1 - Zk k
+ (ρMa2 + kθk+1k)kWk+1- Wιkk
This shows that there exists gk+1 = gk+1 X gk+1 × ∙ ∙ ∙ × gk +l1 ∈ ∂wk+ιF and C = max(ρMa, ρMf +
kθk+1k,ρMa2 + kθk+1k, ∙∙∙ ,ρMa2 + kθL+1k) such that '
kgk+1k ≤ C(kWk+1 - Wkk + kbk+1 - bkk
+ kzk+1-zk k)
14
Under review as a conference paper at ICLR 2020
Similarly, for blk+1,
RFbk+ι = Vbk+i Φ(ak+ι1,Wι¾1,bk+1,zk+1)
=Vbk+ιφ(a-1,Wι-1,bk+1,zk+1)
- Vblk φ(alk-+11, Wlk-+11, blk, zlk) - ρ(blk+1 - blk)
(Vblkφ(alk-+11,Wlk-+11,blk,zlk)+ρ(blk+1-blk)) =0by
the optimality condition of equation 3)
= ρ(zlk+1 - zlk).
Therefore, there exists g2k,+l 1 = Vbk+1 F such that
kg2k,+l1k = ρkzlk+1 - zlkk
This shows that there exists gk+1 = gk+1 × gk+1 ×∙∙∙× gk+1 = Vbk+i F SuCh that
kg2k+1k = ρkzk+1 -zkk.
□
F Proof of Theorem 2
Proof. To prove this theorem, we will first show that ck satisfies two Conditions: (1). ck ≥ ck+1. (2). Pk∞=0 ck
is bounded. We then ConClude the ConvergenCe rate of o(1/k) based on these two Conditions. SpeCifiCally, first,
we have
Ck=min0≤i≤k(Xl=1 kθ+1 ◦ (Wi+1 - Wi)°2kι∕2
+ (P∕2)X=1 kb+1-bi k2 + (p∕2)X=1 kz 产-zik2
+ XL- kτιi+1 ◦ (ai+1 - ai)02kι∕2)
≥min0≤i≤k+1(Xl=1 kθl+1 ◦ (Wιi+1 - Wiy2kι∕2
+(ρ∕2)XlL=1 kbli+1-blik22+(ρ∕2)XlL=1 kzli+1-zlik22
+ XL=11 kW。(ai+1 - ai)°2kι∕2)
= ck+1
Therefore ck satisfies the first Condition. SeCond,
∞
k=0 ck
=Xk=0min0≤i≤k(XL=1 kθ*。(Wι+1 - Wiy2 k1∕2
+(ρ∕2)XlL=1 kbli+1-blik22+(ρ∕2)XlL=1 kzli+1-zlik22
+ XL- kτιi+1 ◦ (ai+1 - ai)02kι∕2)
≤ X∞∞=0(XL=1 kθk+1 ◦ (Wk+1 - Wlk)°2kι∕2
+(ρ∕2)XlL=1 kblk+1-blkk22+(ρ∕2)XlL=1 kzlk+1-zlkk22
+ XL： kτlk+1 ◦ (ak+1 - ak严∣ι∕2)
≤ F(W0, b0, z0, a0) - F(W*, b*, z*, a*)(LemmaI)
So Pk∞=0 ck is bounded and ck satisfies the seCond Condition. Finally, it has been proved that the suffiCient
Conditions of ConvergenCe rate o(1∕k) are: (1) ck ≥ ck+1, and (2) Pk∞=0 ck is bounded, and (3) ck ≥ 0 (Lemma
1.2 in (Deng et al. (2017))). SinCe we have proved the first two Conditions and the third one ck ≥ 0 is obvious,
the ConvergenCe rate of o(1∕k) is proven.
□
15
Under review as a conference paper at ICLR 2020
G	Discussion
We discuss convergence conditions of our DLAM compared with SGD-type methods and the ADMM method.
The comparison demonstrates that our convergence conditions are more general than others.
G.1 DLAM VERSUS SGD
One influential work by Ghadimi et al. (Ghadimi & Lan (2016)) guaranteed that the SGD converges to a critical
point, which is similar to our convergence results. While the SGD requires the objective function to be Lipschitz
differentiable, bounded from below (Ghadimi & Lan (2016)), our DLAM allows for non-smooth functions such
as Relu. Therefore, our convergence conditions are milder than SGD.
G.2 DLAM VERSUS ADMM
For ADMM, Zeng et al. showed that the ADMM is convergent to a critical point with a sublinear convergence
rate O(1/k) (Zeng et al. (2019)), which is similar to our convergence results. However, the ADMM requires
activation functions hi (•) to be twice-differentiable bounded, whereas our DLAM allows hi (•) to be non-smooth.
This also shows that the assumptions of our DLAM are more general than those of the ADMM.
H Related Work
All of the existing works on optimization methods in deep neural network problems falls into two
major classes: stochastic gradient descent methods and alternating minimization methods. This
research related to both is discussed in this section.
Stochastic gradient descent methods: The renaissance of SGD can be traced back to 1951 when
Robbins and Monro published the seminal paper (Robbins & Monro (1951)). The famous back-
propagation algorithm was introduced by Rumelhart et al. (Rumelhart et al. (1986)). Many variants
of SGD methods have since been presented, including the use of Polyak momentum, which accel-
erates the convergence of iterative methods (Polyak (1964)), and research by Sutskever et al., who
highlighted the importance of Nesterov momentum and initialization (Sutskever et al. (2013)). Many
well-known SGD methods that incorporate with adaptive learning rates have been proposed by the
deep learning community, including AdaGrad (Duchi et al. (2011)), RMSProp (Tieleman & Hinton),
Adam (Kingma & Ba (2014)) and AMSGrad (Reddi et al. (2018)).
Alternating minimization methods for deep learning: Previous work on the application of alter-
nating minimization algorithms to deep learning problems can be categorized into two main types.
The first research strand proposes the use of alternating minimization algorithms for specific ap-
plications. For example, Taylor et al. and Wang et al. presented an Alternating Direction Method
of Multipliers (ADMM) algorithm to transform a fully-connected neural network problem into an
equality-constrained problem, where many subproblems split by ADMM can be solved in parallel
(Taylor et al. (2016); Wang et al. (2019)), while Zhang et al. handled very deep supervised hashing
(VDSH) problems by utilizing an ADMM algorithm to overcome issues related to vanishing gradients
and poor computational efficiency (Zhang et al. (2016)). Zhang and Bastiaan trained a deep neural
network by utilizing ADMM with a graph (Zhang & Kleijn (2017)) and Askari et al. introduced a
new framework for multilayer feedforward neural networks and solved the new framework using
block coordinate descent (BCD) methods (Askari et al. (2018)). Others have proposed novel alternat-
ing minimization methods and proved their convergence results. For instance, Carreira and Wang
suggested a method involving the use of auxiliary coordinates (MAC) to replace a nested neural
network with a constrained problem without nesting (Carreira-Perpinan & Wang (2014)). Zeng et al.
and Lau et al. both proposed BCD algorithms, proving its convergence via the Kurdyka-ojasiewicz
(KL) property (Jinshan Zeng (2018); Lau et al. (2018)), while Choromanska et al. proposed a
BCD algorithm for training deep feedforward neural networks based on the concept of co-activation
memory (Choromanska et al. (2018)) and a BCD algorithm with R-linear convergence was proposed
by Zhang and Brand to train Tikhonov regularized deep neural networks (Zhang & Brand (2017)).
However, most of these researchers focused on specific applications of neural networks rather than
their general formulations. Even though several do discuss general neural network problems and
provide theoretical guarantees, the assumptions involved are hard to satisfy in practice.
16