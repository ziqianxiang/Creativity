Under review as a conference paper at ICLR 2020
Gradient Descent can Learn Less Over-
parameterized Two-layer Neural Networks
on Classification Problems
Anonymous authors
Paper under double-blind review
Ab stract
Recently, several studies have proven the global convergence and generalization
abilities of the gradient descent method for two-layer ReLU networks. Most studies
especially focused on the regression problems with the squared loss function, ex-
cept for a few, and the importance of the positivity of the neural tangent kernel has
been pointed out. However, the performance of gradient descent on classification
problems using the logistic loss function has not been well studied, and further
investigation of this problem structure is possible. In this work, we demonstrate
that the separability assumption using a neural tangent model is more reasonable
than the positivity condition of the neural tangent kernel and provide a refined
convergence analysis of the gradient descent for two-layer networks with smooth
activations. A remarkable point of our result is that our convergence and general-
ization bounds have much better dependence on the network width in comparison
to related studies. Consequently, our theory provides a generalization guarantee
for less over-parameterized two-layer networks, while most studies require much
higher over-parameterization.
1 Introduction
In recent years, many studies have been devoted to explaining the great success of over-parameterized
neural networks, where the number of parameters is much larger than that needed to fit a given
training dataset. This study also treats over-parameterized two-layer neural networks using smooth
activation functions and analyzes the convergence and generalization abilities of the gradient descent
method for optimizing this type of network.
For over-parameterized two-layer neural networks, Du et al. (2019); Arora et al. (2019); Chizat &
Bach (2018a); Mei et al. (2018) showed the global convergence of the gradient descent. These studies
are mainly divided into two groups depending on the scaling factor of the output of the networks
to which the global convergence property has been demonstrated using different types of proofs.
For the scaling factor 1/m, (m: the number of hidden units), Chizat & Bach (2018a); Mei et al.
(2018) showed the convergence to the global minimum over probability measures when m → ∞
by utilizing the Wasserstein gradient flow perspective (Nitanda & Suzuki, 2017) on the gradient
descent. For the scaling factor 1∕mβ (β < 1), Du et al. (2019) essentially demonstrated that the
kernel smoothing of functional gradients by the neural tangent kernel (Jacot et al., 2018; Chizat &
Bach, 2018b) has comparable performance with the functional gradient as m → ∞ by making a
positivity assumption on the Gram-matrix of this kernel, resulting in the global convergence property.
In addition, Arora et al. (2019) provided a generalization bound via a fine-grained analysis of the
gradient descent. These studies provide the first steps to understand the role of over-parameterization
of neural networks and the gradient descent on regression problems using the squared loss function.
For the classification problems with logistic loss, a few studies (Allen-Zhu et al., 2018a; Cao &
Gu, 2019a;b) investigated the convergence and generalization abilities of gradient descent under a
separability assumption with a suitable model instead of the positivity of the neural tangent kernel. In
this study, we further develop this line of research on binary classification problems.
Our contributions. We provide fine-grained global convergence and generalization analyses of
the gradient descent for two-layer neural networks with smooth activations under a separability
1
Under review as a conference paper at ICLR 2020
Table 1: Summary of hyperparameter settings and assumptions to achieve an expected -classification
error by gradient descent for binary classifications. The “Separability” column denotes the types of
models where a separability assumption is made. m is the number of hidden units, n is the size of the
training data, and T is the number of iterations of gradient descent. The notations Ω and Θ hide the
logarithmic terms in the big-Ω and -Θ notations. Smooth activations include sigmoid, tanh, swish
activations, and several smooth approximations of ReLU. As for Allen-Zhu et al. (2018a); Cao & Gu
(2019a;b), we pick up results specialized to two-layer networks.
	Activation	Separability	Deep	m	n	T
Allen-Zhu et al. (2018a)	ReLU	Smooth Target	yes	ΩD	Ω(e-4)	Θ(L
Cao & Gu (2019a)	ReLU	ReLUNN	yes	ΩD	Ω(e-4)	W)
Cao & Gu (2019b)	ReLU	ReLU NN	yes	ΩD	Ω(e-2)	W)
This work	Smooth	Neural Tangent	no	Ω(e-1) ΘΘ(e-3/2)	Ω(e-4) Ω(e-2)	Θ(-2) 2)
assumption with a sufficient margin using a neural tangent model, which is a non-linear model with
feature extraction through a neural tangent. We demonstrate that a separability assumption is more
suitable than the positivity condition of the neural tangent kernel because (i) the positive neural
tangent kernel leads to weak separability and conversely, (ii) separability leads to the positivity of
the neural tangent kernel only on a cone spanned by labels, which is very restrictive compared to
the whole space. Therefore, the separability condition is rather weak in this sense but it is enough to
ensure global convergence for the classification problems. Thus, a significantly better convergence
and generalization analyses with respect to network width can be obtained because the positivity
of the neural tangent kernel is not required. Consequently, our theory provides a generalization
guarantee for less over-parameterized two-layer networks trained by gradient descent, while most
existing results relying on the positive neural tangent kernel essentially require quite high over-
parameterization. To the best of our knowledge, there are no successful studies for our problem
setting (i.e., less over-parameterized two-layer neural networks with smooth activation functions for
the classification problems with logistic loss) in the literature. Most studies have focused on highly
over-parameterized neural networks with ReLU activation, and less over-parameterized settings have
been considered difficult for showing the global convergence property of gradient descent even in the
few studies using a separability condition (Allen-Zhu et al., 2018a; Cao & Gu, 2019a;b). However,
we note that these studies provided global convergence and generalization analyses of the (stochastic)
gradient descent for challenging settings (i.e., deep ReLU networks) by making a similar but different
assumption than ours. Thus, our and these studies do not include each other because of the difference
of the network structure (i.e., network depth and activation type) and assumptions.
We here describe the main result informally. A neural tangent model is an infinite-dimensional
non-linear model using transformed features (∂θσ(θ(0)>χ))θ(o)〜*0, where σ is a smooth activation
and μo is a distribution used to initialize the parameters of the input layer in two-layer neural
networks. Theorem 1 states that gradient descent can find an -accurate solution in terms of the
expected classification error for a wide class of over-parameterized two-layer neural networks under
a separability assumption using a neural tangent model.
Theorem 1 (Informal). Suppose that a given data distribution is separable by a neural tangent model
with a sufficient margin under L∞-constraint. If for any > 0, the hyperparameters satisfy one of
the following
(i)	β ∈ [0, 1), m = Ω(eι-⅛), T = Θ(e-2), η = Θ(m2β-1), n = Ω(e-4),
(ii)	β = 0, m = <Θ(e-3/2), T = Θ(c-1), η = Θ(m-1), n = ΩΩ(e-2).
then with high probability over the random initialization and choice of samples of size n, the gradient
descent with a learning rate η achieves an expected -classification error within T -iterations.
Related work. A few recent studies (Allen-Zhu et al., 2018a; Cao & Gu, 2019a;b) are closely
related to our work because they also treated the logistic loss function. As stated above, problem
settings in our and these studies are somewhat different, but we compare our result with those
2
Under review as a conference paper at ICLR 2020
specialized to two-layer network to show a better property of our problem setting and analyses.
Separability assumptions were made on an infinite-width two-layer ReLU network in Cao & Gu
(2019a;b) and on a smooth target function in Allen-Zhu et al. (2018a). For generalization analyses, our
result exhibits much better dependency on the network width owing to a better problem setting with a
fine-grained analysis. Table 1 provides a comparison of the hyperparameter settings of networks and
gradient descent in related studies to achieve an expected -classification error. As evident in Table 1,
for more comprehensive sizes of two-layer networks with respect to the network width, our theory
ensures the same generalization ability as those of Allen-Zhu et al. (2018a); Cao & Gu (2019a;b). In
fact, the network width Ω(e-1) and Ω(e-3/2) are sufficient in our setting.
For the stochastic gradient descent for two-layer networks, Brutzkus et al. (2018); Li & Liang (2018)
provided generalization analyses. Brutzkus et al. (2018) assumed that datasets are linear separable
and this restrictive assumption was relaxed to mixtures of well separated data distributions in Li
& Liang (2018). However, the analysis in Li & Liang (2018) is also tailored to only highly over-
parameterized settings. Concretely, a very large width m = Ω(e-24) and the number of samples
(iterations) n = Θ(T)= O(E-12) are required to achieve an expected E-ClaSSifiCatiOn error in Li
& Liang (2018). In addition, it should be noted that global convergence analyses (Allen-Zhu et al.,
2018b; Zou et al., 2018) in terms of optimization without the specification of network size will yield
loose generalization bounds because the complexities of neural networks cannot be specified.
Apart from the abovementioned studies, there are many other studies (Brutzkus & Globerson, 2017;
Zhong et al., 2017; Tian, 2017; Soltanolkotabi, 2017; Du et al., 2019; Zhang et al., 2018; Arora
et al., 2019; Oymak & Soltanolkotabi, 2019; Zhang et al., 2019; Wu et al., 2019) that focus on
regression problems but our study focuses on classification problems and demonstrates a better
property of gradient descent for over-parameterized networks by utilizing the problem structure of a
binary classification. Especially, we show that a separability assumption is more preferable than the
positivity condition of the neural tangent kernel (Du et al., 2019; Arora et al., 2019; Zhang et al., 2019;
Wu et al., 2019) and show that the required network width can be significantly reduced. Concretely,
the required network widths are at least Ω(n6) (DU et al., 2019; WU et al., 2019), Ω(n7E-2) (Arora
et al., 2019), and Ω(n4) (Zhang et al., 2019). Thus, these widths are very large, as compared to our
results because sample complexities are generally slower than or equal to n = Ω(e-2). In addition,
proof techniques are also different for the squared loss and the logistic loss functions because the
latter function lacks the strong convexity. Thus, we cannot utilize the linear convergence property
for the logistic loss and parameters will diverge, which also causes the difficulty of showing better
generalization ability without a fine-grained analysis.
2	Preliminary
Here, we describe the problem setting for the binary logistic regression and discuss the functional
gradients to provide a clear theoretical view of the gradient methods for two-layer neural networks.
2.1	Problem Setting
Let X = Rd and Y be a feature space and the set of binary labels {-1, 1}, respectively. We denote by
ν a true probability measure on X × Y and by νn an empirical probability measure, deduced from ob-
servations (xi, yi)in=1 independently drawn from ν, i.e., dνn(X, Y ) = Pin=1 δ(xi,yi) (X, Y )dXdY/n,
where δ is the Dirac delta function. The marginal distributions of ν and νn on X are denoted by
νX and νnX, respectively. For ζ ∈ R and y ∈ Y, let l(ζ, y) be the logistic loss: log(1 + exp(-yζ)).
Then, the objective function to be minimized is formalized as follows:
df	1 n
L⑼=E(X,Y )〜Vn[lfΘ(X ),Y )] = ny^l(fθ(xi),yi),
i=1
where fΘ : X → R is a two-layer neural network equipped with parameters Θ = (θr)rm=1. When we
consider a function fΘ as a variable of the objective function, we denote L(fΘ) d=ef L(Θ).
3
Under review as a conference paper at ICLR 2020
The two-layer neural network treated in this study is formalized as follows. For parameters Θ =
(θr)rm=1 (θr ∈ Rd) and fixed constants (ar)rm=1 ∈ {-1, 1}m:
1m
fθ(x) = —β Earσ(θ>x),	(1)
r=1
where — is the number of hidden units, β is an order of the scaling factor, and σ : R → R is a
smooth activation function such as sigmoid, tanh, swish (Ramachandran et al.)), and other smooth
approximations of ReLU. In the training procedure, the parameters Θ = (θr)rm=1 of the input layer
are optimized. This setting is the same as those in Du et al. (2019); Arora et al. (2019); Zhang et al.
(2019); Wu et al. (2019), except for the types of activation functions, scaling factor, and loss function.
2.2 Functional Gradient
We denote by L2(νχ) the function space from X to R, equipped with the inner product(•, ∙)l2(VX):
hφ,ψiL2(νX) = EX 〜VX [φ(X )ψ(X )] ,	∀φ, ∀ψ ∈ l2(Vn ).
Following the tradition in the literature of boosting and kernel methods, we call L2 (νnX ) the function
space, although this space is actually an n-dimensional space because the cardinality of the support
of νnX is n. The key notion to explain the behavior of the gradient descent is the functional gradient
in this function space L2(νnX). We define the functional gradient at a predictor f : X → R as,
Vf L(f )(x) d=4∂ζ l(ζ,yi)lζ=f (Xi)	(x = Xi),、
0	(otherwise).
This is simply a Frechet differential (functional gradient) in L2(νX). That is, it follows that
L(f+φ)=L(f)+hVfL(f),φiL2(VnX)+o(kφkL2(VnX)), ∀f,∀φ∈L2(νnX).
Therefore, the functional gradient descent using VfL(f) directly optimizes L in a function space
L2 (νnX) and converges to a global minimum because the objective function L is convex with respect
to a function f. However, because VfL(f) contains no information regarding the unseen data, this
method is meaningless in terms of generalization. Thus, some smoothing techniques are required
to guarantee the generalization. The gradient descent method for two-layer neural networks can be
recognized as a type of kernel-smoothed functional gradient using the neural tangent kernel (Jacot
et al., 2018), and this perspective is significantly useful in showing the global convergence because it
characterizes the behavior of the vanilla gradient descent in a function space.
3 B rief Review of Functional Gradient Methods
Functional gradient methods have been mainly studied for gradient boosting (Mason et al., 1999;
Friedman, 2001) and kernel methods (Kivinen et al., 2004; Smale & Yao, 2006; Ying & Zhou, 2006;
Raskutti et al., 2014; Wei et al., 2017) in the machine learning community, but more recently, it has
been found to be useful in explaining the behavior of gradient descent for neural networks (Jacot
et al., 2018; Chizat & Bach, 2018b; Du et al., 2019; Allen-Zhu et al., 2018b; Arora et al., 2019). Our
analysis is also heavily based on the functional gradient perspective of gradient descent. Thus, we
briefly review the functional gradient methods.
In gradient boosting, VfL(f) is approximated by finding a similar function in weak learners G:
φf ∈ argmax hVfL(f), φiL2(VX)
φ∈G	2 n
(2)
and the gradient method in a function space is performed using a descent direction -φf. This
approximation is a type of smoothing of functional gradients. In kernel methods, this smoothing
procedure is realized by using the kernel smoothing technique:
1n
TkVf L(f) d=f EVX Vf L(f )(X)k(X, •)] = - E VfL(f )(xi)k(xi,∙),	⑶
n i=1
4
Under review as a conference paper at ICLR 2020
where k is a kernel function. It should be noted that this kernel smoothing (3) is a special case of
gradient boosting (2) because of the following equation:
∣∣7Tkv fLf)	∈ argmax Ef Lf ),φiL2(νX),
kTkVfLf )kHk	kφkHk ≤1	2( n )
where (Hk, h, iH ) is the reproducing kernel Hilbert space associated with a kernel k. When this
kernel smoothing well approximates a functional gradient VfL(f) and satisfies
hVfL(f),TkVfL(f))L2(νχ) ≥ ∃μkVfL(f)kL2(νχ),	(4)
the kernel-smoothed functional gradient descent f+ J f 一 ηTk Vf L(f) performs like the pure
functional gradient descent, leading to the global convergence property because it tends to a stationary
point in a function space, which is simply a global minimum.
Recently, several studies (Jacot et al., 2018; Chizat & Bach, 2018b; Du et al., 2019; Allen-Zhu et al.,
2018b; Arora et al., 2019) implicitly or explicitly pointed out that the gradient descent for neural
networks is essentially recognized as an approximation to the kernel-smoothed functional gradient
method using a neural tangent kernel (NTK) (Jacot et al., 2018):
kNTK(x, x0) d=f Eθ(0)〜μo [∂θσ(θ(O)>x)>∂θσ(θ(O)>x0)],	(5)
where μo is a distribution to initialize the parameters of the input layer in this setting. In most proofs
using NTK, the global convergence property has been demonstrated by showing the condition (4)
from the positivity of the Gram-matrix H∞ d=ef (kNTK(xi, xj ))in,j=1 and the similarity between the
gradient descent and the kernel-smoothed functional gradient with NTK when m → ∞. This is a
reason why very high over-parameterization is generally required in related studies.
In this study, we found that the positivity of the Gram-matrix of NTK is not required on binary
classification problems and a separability assumption, which is a weaker condition than the positivity,
is enough for global convergence. Consequently, we can give global convergence and generalization
guarantees to a gradient method for less over-parameterized two-layer neural networks.
4 Global Convergence Analysis of the Gradient Method
The following is an update rule of gradient descent with respect to the input parameters Θ = (θr)rm=1:
Θ(t+1) J Θ(t) 一 ηVΘL(Θ(t)),	(6)
where VΘL(Θ(t)) = (∂θr L(Θ(t)))rm=1 and η > 0 is a learning rate. We here make the assumption:
Assumption 1.
(A1) Assume that supp(ν X) ⊂ {x ∈ X | kxk2 ≤ 1}. Let σ be a C 2 -class function and there exist
K1 , K2 > 0 s.t. kσ0 k∞ ≤ K1 and kσ00 k∞ ≤ K2.
(A2) A distribution μo on Rd used for the initialization of θr has a Sub-Gaussian tail bound:
∃A, ∃b > 0 such that Ps(o)〜*。[∣∣θ(0)∣∣2 ≥ t] ≤ Aexp(-bt2).
(A3) Assume that the number of hidden units m ∈ Z+ is an even number. Constant parameters
(ar)rm=1 and parameters Θ(O) = (θr(O))rm=1are initialized symmetrically: a『=1 for r ∈ {1,...,学},
ar = —1 for r ∈ {节 + 1,..., m}, and 0、0) =。：+m for r ∈ {1,...,节}, where the initial
parameters (θrɔ))r=[ are independently drawnfrom a distribution μo.
(A4) Assume that there exist ρ > 0 anda measurable function v : Rd → {w ∈ Rd | kwk2 ≤ 1} such
that the following inequality holds: for ∀(x, y) ∈ supp(ν) ⊂ X × Y,
y ∂θσ(θ(O)>x), v(θ(O))
yEθ(0)〜μo[∂θσ(θ(0)>x)>v(θ(0))] ≥ ρ.
(7)
5
Under review as a conference paper at ICLR 2020
Remark. Clearly, many activation functions (sigmoid, tanh, and smooth approximations of ReLU
such as swish) satisfy the assumption (A1). Typical distributions, including the Gaussian distribution,
satisfy (A2). The purpose of the symmetrized initialization (A3) is to bound the initial value of the
loss function L(Θ(0)) uniformly over the number of hidden units m. This initialization leads to
fΘ(0) (x) = 0, resulting in L(Θ(0)) = log(2). Assumption (A4) implies the separability of a dataset
using the neural tangent model. We next discuss the validity of this assumption.
4.1	Separability Assumption (A4) by the Neural Tangent
The explicit feature representation: x → ∂θσ(θ(0)>x), of NTK (5) is called the neural tangent,
which is a non-linear feature extraction from X to an infinite-dimensional space. That is, assumption
(A4) ensures the separability of the transformed data (∂θσ(θr(0)>x), y) through the neural tangent for
(x, y) ∈ SuPP(V) with a margin P in an infinite-dimensional space by the weight: v(θ(0))dμo. We
remark that this assumption is somewhat weaker than the positivity assumption on the Gram-matrix
of NTK and is satisfied in many cases by the universal approximation ability of the neural tangent
models. In addition, we remark that the separability of the training dataset instead of suPP(ν) is
enough to guarantee global convergence only for empirical risk minimization.
Theoretical comparison of kernel assumptions. In previous studies (Du et al., 2019; Arora
et al., 2019; Zhang et al., 2019; Wu et al., 2019) the positivity of the Gram-matrix H∞ =
(kNTK(xi, xj))in,j=1 was required to ensure the condition (4). Here, we remark that the assumption
(A4) is weaker than this positivity condition in the following sense.
Proposition 1. (i) Assume H∞	λ0In and kσ0k∞ ≤ K1, then there exists a measurable map
v : Rd → {w ∈ Rd | kwk2 ≤ 1} such that ∀i ∈ {1, . . . , n},
yi Ddθ "θ(OrTx"(θ ⑼)工(“0产 nλK .
(ii)	Suppose assumption (A4) holds, then Pin,j=1ξiH∞ξj ≥ ρ2kξk22, (∀ξ ∈ {(αiyi)in=1 | αi ≥ 0}).
As seen in Proposition 1-(i), the positivity H∞ λOIn leads to weak separability with a margin of
O(λO /n) on the training dataset. Conversely, from Proposition 1-(ii), the separability with a margin
of ρ leads to the positivity ρ2 of H∞ only on a cone spanned by the labels: {(αiyi)in=1 | αi ≥ 0}.
Because this cone is very restrictive, this limited positivity is much weaker than the positivity
on the whole space. However, we found that this limited positivity is sufficient to ensure the
global convergence of the gradient descent for binary classification problems with logistic loss.
Indeed, from Proposition 3, the positivity of H ∞ is required only along the functional gradients:
Vf L(fθ)(xi) = ∂ζl(fθ(xi), yi), and these functional gradients are always contained in this limited
space, unlike the squared loss function. This is a reason why the positivity of NTK is not required
for the binary classification problem with logistic loss. Thus, a much better convergence and
generalization ability can be shown for logistic loss than the previous results that relied on the
positivity of H ∞ because the positivity of NTK on the whole space is redundant and a separability
condition provides a better positivity only on a required small space. Concretely, from Proposition
1-(i), we can immediately check a deteriorated convergence result depending on the positivity of H∞
by replacing P in Theorem 2 with O(λo∕n), producing m = O(nλ-1e-1) when β = 0.
Remark. For regression problem, Allen-Zhu et al. (2018b;c); Zou et al. (2018); Oymak &
Soltanolkotabi (2019); Zou & Gu (2019) make a different separation where examples are away
from each other: kxi - xj k2 ≥ P.Zou & Gu (2019) shows that this assumption is essentially same as
the positivity of NTK. Thus, it also completely differs from (A4) as shown in Proposition 1
Universal approximation property of neural tangent models. We consider the case where all
feature vectors have a common bias term: x = (xO, . . . , xd-1, s) ∈ X (s > 0 is a sufficiently small
constant for a bias term). In this case, we can easily confirm that neural tangent models include
typical two-layer infinite-width neural networks with activation σ0: E[w(θ(O))σ0(θ(O)> x)] by setting
v(θ(O)) = (0, . . . , 0, w(θ(O))), where w is a real-valued function. Thus, Assumption (A.4) with a
certainly positive constant P is satisfied as long as a data distribution is separable by an infinite-width
two-layer network with mild weights w(θ). Moreover, we note that these networks have the universal
6
Under review as a conference paper at ICLR 2020
approximation property (Hornik, 1991; Sonoda & Murata, 2017), so that there are a lot of examples
such that the assumption (A4) is satisfied.
4.2	Main Results
We here define the L1-norm of the functional gradient, which measures the convergence.
nn
I∣vfL(fθ)kLι(VX)==f -X |dzl(fθ(χi),yi)1 = 2- X |yi - 2PΘ(Y = 1 | Xi) + 1].
n i=1	2n i=1
Here, Z is the first variable of l andpθ(Y = 1 | X)= ι+eχp(-拈(方))is a conditional probability on
Y = 1, definedby fθ. Because ∣∣Vf L(fθ)∣Lι(νχ) is simply a gap between the labels and conditional
label probabilities of the model, this norm is a reasonable measure for the binary classification
problems. The following is the first main result to ensure global convergence.
Theorem 2	(Global Convergence). Suppose Assumption 1 holds. Set K = K14 + 2K12K2 + K14K22.
For ∀β	∈ [0,	1),	∀δ	∈	(0,1) and	∀m	∈	Z+	such that m ≥	1；KI	log 竿,consider gradient
descent (6) with a learning rate of 0 < η ≤ min {mβ, 4m2βK-∣ and the number of iterations
T ≤ |_32nK<2^og(2)_| ∙ Then,itfollows thatwithprobability at least 1 一 δ over the random initialization,
TX kvfL(fθ(t))kLι(νχ) ≤ 16pOT≡ (m2β-1 + K).	⑻
We here derive a corollary, which states that an arbitrary small empirical classification error can be
achievable by appropriately setting η, m, and T as follows. From Markov’s inequality,
P(X,Y)“n[Yfθ(t)(X) ≤ 0] ≤ 2∣VfL(fθ)∣Lι(νχ),
where we used the following relationship:
0.5 |yi - 2pθ(Y = 1∣Xi) + 1∣≥ (1+exp(γ))-1 ^⇒ yifθ(xi) ≤ Y
Thus, a convergence rate of IVf L(fΘ(t) )I2L (νX) leads to a rate of empirical classification error.
Corollary 1. Suppose the same assumptions as in Theorem 2 hold. If for ∀, δ > 0, the hyperparam-
eters satisfy
β ∈ [0, 1), m = Ω(ρτ-βeT-β), T = Ω(ρ-2e-2), η = Θ(m2β-1),
then with probability at least 1 一 δ, gradient descent (6) with a learning rate of η finds a parameter
Θ(t) satisfying P(χ,γ)〜Vn [Yfθ(t)(X) ≤ 0] ≤ E within T-iterations.
The Landau notations are applied with respect to , ρ → 0. Utilizing this theorem, we can show the
convergence of the loss function which leads to a better result at the price of slight increase of m.
Theorem 3.	Suppose the same assumptions in Theorem 2 hold. Then there exists a uniform constant
C > 0 such thatfor 0 < ∀α ≤ 4K∙, with probability at least 1 一 δ,
T X L(Θ(t)) ≤ c(T + Om +exp (-M)
Corollary 2. Suppose the same assumptions as in Theorem 3 hold. Then there exists a uniform
constant C > 0 and the following statement holds. Iffor any E > 0, the hyperparameters satisfy
β = 0, m = Θ(ρ-2e-3/2 log(1∕e)), T = Θ(ρ-2e-1 log2(1/e)), η = Θ(m-1),
then with probability 1 — δ, T PTQI L(Θ(t)) ≤ C (E + PIT log2 (ɪ)) for 0 < ∀T ≤ T.
This corollary is obtained immediately from Theorem 3 by setting α = Θ(ρe3/2). The convergence
of classification error also derived from the corollary because L(fθ) ≥ IVf L(fΘ(t) )IL1(VX) and it
also derives a sharper bound on the distance IΘ(T) 一 Θ(Q) I2.
7
Under review as a conference paper at ICLR 2020
Proposition 2. Suppose the same assumption and consider the same hyperparameter setting as in
Corollary 2. Then, there exists a uniform constant C such that with probability 1 - δ,
kΘ(T) - Θ(0)k2 ≤ C3/4 log2(ρ-2-1).
Moreover, by combining Theorem 2 and Corollary 2 with the well-known margin bound (Koltchinskii
& Panchenko, 2002; Mohri et al., 2012; Shalev-Shwartz & Ben-David, 2014) on the expected
classification error and by specifying the Rademacher complexity of the function class attained by
gradient descent, we can obtain the second main result for the generalization ability of gradient
descent.
Theorem 4 (Generalization Bound). Suppose Assumption 1 holds. Set K = K14 + 2K12K2 + K14K22.
Fix ∀γ > 0. Consider the gradient descent (6) with a general hyperparameter setting in Theorem 2
or a specific setting in Corollary 2, with δ ∈ (0, 1). For these cases, we set parameters Cη,m,T and
Dη,m,T as follows:
(Former case)	Cη,m,τ = PTTT/2 (mβ-1 η-1/2 + √K) ,	D%m,τ = PT
(Latter case)	Cη,m,T = + ρ-2T-1 log2 (1/) ,	Dη,m,T = 3/4 log2(ρ-2-1).
Then, there exists a uniform constant C > 0 and it follows that with probability at least 1 - 3δ over a
random initialization and random choice of dataset S,
min
t∈{0,...,T -1}
P(X,Y)”[Yfθ(t) (X) ≤ 0] ≤ C(1 + exp(γ))Cη,m,τ + 3
∕log(2∕δ)
V -2n-
+ CYTm1-β Dη,m,τ(1 + Ki + K2)
d‰
(1 +K1 + K2)(log(m∕δ) + Dη2,m,T) .
(9)
Moreover, when σ is convex and σ(0) = 0, we can avoid the dependence with respect to the dimension
d. With probability at least 1 - 3δ over a random initialization and random choice of dataset S
min
t∈{0,...,T -1}
P(X,Y)”[Yfθ(t)(X) ≤ 0] ≤ C(1 + exp(γ))Cη,m,τ + 3
Jhg⑵ δ)
V	2n
CKIm 2-β
Y √n
Dη,m,T +
八og(Am∕δ)
b b
(10)
+
This theorem provides an upper-bound on an expected classification error with high probability
for a network obtained by gradient descent within T -iterations. There is a trade-off between the
optimization and complexity terms in (9), (10) with respect to η, m, and T. However, there are several
choices of these hyperparameters to achieve a desired precision of the expected classification error.
Corollary 3. Suppose the same assumptions as in Theorem 4 hold. If for any > 0, the hyperparam-
eters satisfy one of the following
(i) β ∈ [0, 1), m = Ω(ρ1-βe1-β), T = Ω(ρ-2e-2), η = Θ(ρ~2e~2T-1m2β-i), n = Ω(ρ-2e-4),
(ii) β = 0, m = Θ(ρ-2e-3/2 log(1∕e)), T = Θ(ρ-2e-i log2(1∕e)), η = Θ(m-i), n = ΩΩ(e-2),
then with probability at least 1 - δ, the gradient descent (6) with a learning rate of η finds a parameter
Θ(t) satisfying P(χ,γ)〜ν[Yfθ(t)(X) ≤ 0] ≤ E within T-iterations.
This corollary can be immediately proven by substituting the concrete values of β, m, T, η, and n
into the right hand side of inequalities (9), (10) and by checking that this hyperparameter setting
satisfies the conditions required in Theorem 4.
From Corollary 3, for an arbitrary small E > 0, an expected E-classification error is achieved by the
gradient descent within O(1∕E2)-or O(1∕E)-iterations when the transformed data distribution by the
neural tangent: (∂θ(。£0)>∙))θ〜μ0 is separable in the infinite-dimensional space L2(μ0) under the
L∞-constraint with a sufficient margin ρ. In comparison to the result in Allen-Zhu et al. (2018a);
Cao & Gu (2019a;b), which also derived a generalization bound by making a similar separability
assumption using a ReLU network or a smooth target function instead of the tangent model, our
8
Under review as a conference paper at ICLR 2020
result has much better dependency on the network width and can explain the generalization ability
for a less over-parameterized two-layer network, as summarized in Table 1. However, we note that
their theories cover deeper networks and are not included in our theory because of the difference of
the problem settings (e.g., network depth and the type of activation functions). To reduce the network
width, the best choice of β ∈ [0, 1) is β = 0 for the first setting, leading to a small network width
m = Ω(e-1). We note that an arbitrary large width is also covered by this result.
4.3 Proof Idea
In this section, we provide a proof idea for Theorems 2, 3, and 4. We introduce two important
propositions that connect the gradient descent with the functional gradient descent to justify an
intuitive explanation in Section 3. Proposition 3-(i) states that the gradient descent method is certainly
similar to the kernel smoothed gradient methods by the neural tangent kernel when a parameter Θ is
sufficiently close to a stationary point and the learning rate η is sufficiently small, and Proposition
3-(ii) states that the loss landscape is almost convex with respect to the parameter when fΘ is
sufficiently close to a stationary point in the function space. Here, We define an approximated neural
tangent kernel: kΘ depending on the parameters Θ as follows:
kΘ(x, x0) d=ef ∂ΘfΘ(x)>∂ΘfΘ(x0).	(11)
This kernel is actually an approximation to the vanilla NTK as follows:
m2β-1 kΘ(0) (x, x0) → kNTK(x, x0) (m → ∞).
Proposition 3. Suppose assumption (A1) holds and β ∈ [0, 1).
(i)	We set Θ+ = Θ 一 NθL(Θ`) and K = K2 + 2K + K2K2. If η ≤ mβ, then
∣L(∕θ+ ) — (l(∕θ) — η YfL(fθ),TkθVfL(fθ)iL2(νχ))1 ≤ 2m2KrkVθL(Θ)k∣.
(ii)	Itfollows that for Θ = ® )m=1 and Θ* = (θr)m=1, (θr, θr ∈ Rd),
L(Θ) + VθL(⑼>(Θ* - ⑼ ≤ L(Θ*) + mkVfL(fθ)kLι(VX)kΘ* - Θ∣∣2.
The next proposition states that the kernel smoothed gradients have comparable optimization ability
to pure functional gradients in terms of minimizing the L1-norm around an initial parameter Θ(0).
We define the ∣∣ ∙ ∣∣2,1-norm in the parameter space Θ = (θr)m=ι as ∣∣Θ∣∣2,1 d= Pm=IIIθr∣∣2∙
Proposition 4. Suppose Assumption 1 holds. For ∀δ ∈ (0, 1) and ∀m ∈ Z+, such that
m ≥ 16KI log 2n, the following statement holds with probability at least 1 一 δ over the random
initialization of Θ(0) = 00))m=1. If ∣∣Θ — Θ(0)∣2,1 ≤ 4K, then
ρ2
hvf L(fθ),Tkθ Vf L(fθ)iL2(νχ) ≥ 16mβ- kVf L(fθ)kLι(νX).
This proposition is specialized to binary classification problems because the positivity of the Gram-
matrix is required for regression problems to make a similar statement as discussed earlier.
Combining these two propositions, we can connect the gradient descent with the functional gradient
descent and show the global convergence (Theorem 2) which with the almost convexity (Proposition
3-(ii)) is also used to derive the convergence rate for the loss function (Theorem 3). In addition,
applying a well-known result (Koltchinskii & Panchenko, 2002), we can derive a skeleton of a
generalization bound (Theorem 4) composed of the margin distribution and Rademacher complexity.
As for the margin distribution, the upper bound is obtained by Theorem 2 and 3. As for ways to
bound Rademacher complexity, please see the Appendix.
5 Conclusion
In this paper, we have provided refined global convergence and generalization analyses of the gradient
descent for two-layer neural networks with smooth activations on binary classification problems. The
9
Under review as a conference paper at ICLR 2020
key in our analysis is the separability assumption by a neural tangent model and we have explained
the reasonability of this assumption in comparison to the positivity of NTK. Consequently, theoretical
justification has been provided for less over-parameterized neural networks. However, our theory is
restricted to the deterministic gradient descent and two-layer networks; hence, its possible extensions
to stochastic gradient descent and deep neural networks are also interesting. Another possible future
study is to relax the positivity assumption on the Gram-matrix for regression problems by utilizing
our theory and conducting further investigations of the trajectory of gradient descent, such as the
shortest pass analysis (Oymak & Soltanolkotabi, 2018).
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized
neural networks, going beyond two layers. arXiv preprint arXiv:1811.04918, 2018a.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. arXiv preprint arXiv:1811.03962, 2018b.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. On the convergence rate of training recurrent neural
networks. arXiv preprint arXiv:1810.12065, 2018c.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. arXiv preprint
arXiv:1901.08584, 2019.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Advances in Neural Information Processing Systems 30, pp. 6240-6249, 2017.
Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian
inputs. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp.
605-614, 2017.
Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz. Sgd learns over-
parameterized networks that provably generalize on linearly separable data. International Confer-
ence on Learning Representations 6, 2018.
Yuan Cao and Quanquan Gu. A generalization theory of gradient descent for learning over-
parameterized deep relu networks. arXiv preprint arXiv:1902.01384, 2019a.
Yuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and deep
neural networks. arXiv preprint arXiv:1905.13210, 2019b.
Geoffrey Chinot, Guillaume Lecu6, and Matthieu Lerasle. Robust statistical learning with Iipschitz
and convex loss functions. Probability Theory and Related Fields, pp. 1-44, 2019.
Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-parameterized
models using optimal transport. In Advances in Neural Information Processing Systems 31, pp.
3040-3050, 2018a.
Lenaic Chizat and Francis Bach. A note on lazy training in supervised differentiable programming.
arXiv preprint arXiv:1812.07956, 2018b.
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. International Conference on Learning Representations 7,
2019.
Jerome H Friedman. Greedy function approximation: a gradient boosting machine. The Annals of
Statistics, pp. 1189-1232, 2001.
Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks, 4(2):
251-257, 1991.
Arthur Jacot, Franck Gabriel, and CIement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in Neural Information Processing Systems 31, pp.
8580-8589, 2018.
10
Under review as a conference paper at ICLR 2020
Jyrki Kivinen, Alexander J Smola, and Robert C Williamson. Online learning with kernels. IEEE
transactions on signal processing, 52(8):2165-2176, 2004.
Vladimir Koltchinskii and Dmitry Panchenko. Empirical margin distributions and bounding the
generalization error of combined classifiers. The Annals of Statistics, 30(1):1-50, 2002.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. In Advances in Neural Information Processing Systems 31, pp. 8157-
8166, 2018.
Llew Mason, Jonathan Baxter, Peter L Bartlett, and Marcus R Frean. Boosting algorithms as gradient
descent. In Advances in Neural Information Processing Systems 12, pp. 512-518, 1999.
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-
layer neural networks. Proceedings of the National Academy of Sciences, 115(33):E7665-E7671,
2018.
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning. MIT
press, 2012.
Atsushi Nitanda and Taiji Suzuki. Stochastic particle gradient descent for infinite ensembles. arXiv
preprint arXiv:1712.05438, 2017.
Samet Oymak and Mahdi Soltanolkotabi. Overparameterized nonlinear learning: Gradient descent
takes the shortest path? arXiv preprint arXiv:1812.10004, 2018.
Samet Oymak and Mahdi Soltanolkotabi. Towards moderate overparameterization: global con-
vergence guarantees for training shallow neural networks. arXiv preprint arXiv:1902.04674,
2019.
Prajit Ramachandran, Barret Zoph, and Quoc V. Le. Searching for activation functions. arXiv preprint
arXiv:1710.05941.
Garvesh Raskutti, Martin J Wainwright, and Bin Yu. Early stopping and non-parametric regression:
an optimal data-dependent stopping rule. Journal of Machine Learning Research, 15(1):335-366,
2014.
Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to
algorithms. Cambridge university press, 2014.
Steve Smale and Yuan Yao. Online learning algorithms. Foundations of computational mathematics,
6(2):145-170, 2006.
Mahdi Soltanolkotabi. Learning relus via gradient descent. In Advances in Neural Information
Processing Systems 30, pp. 2007-2017, 2017.
Sho Sonoda and Noboru Murata. Neural network with unbounded activation functions is universal
approximator. Applied and Computational Harmonic Analysis, 43(2):233-268, 2017.
Yuandong Tian. An analytical formula of population gradient for two-layered relu network and its
applications in convergence and critical point analysis. In Proceedings of the 34th International
Conference on Machine Learning-Volume 70, pp. 3404-3413, 2017.
Yuting Wei, Fanny Yang, and Martin J Wainwright. Early stopping for kernel boosting algorithms:
A general analysis with localized complexities. In Advances in Neural Information Processing
Systems 30, pp. 6065-6075, 2017.
Xiaoxia Wu, Simon S Du, and Rachel Ward. Global convergence of adaptive gradient methods for an
over-parameterized neural network. arXiv preprint arXiv:1902.07111, 2019.
Yiming Ying and D-X Zhou. Online regularized classification algorithms. IEEE Transactions on
Information Theory, 52(11):4775-4788, 2006.
Guodong Zhang, James Martens, and Roger Grosse. Fast convergence of natural gradient descent for
overparameterized neural networks. arXiv preprint arXiv:1905.10961, 2019.
11
Under review as a conference paper at ICLR 2020
Xiao Zhang, Yaodong Yu, Lingxiao Wang, and Quanquan Gu. Learning one-hidden-layer relu
networks via gradient descent. arXiv preprint arXiv:1806.07808, 2018.
Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery guarantees
for one-hidden-layer neural networks. In Proceedings of the 34th International Conference on
Machine Learning-Volume 70, pp. 4140-4149, 2017.
Difan Zou and Quanquan Gu. An improved analysis of training over-parameterized deep neural
networks. arXiv preprint arXiv:1906.04688, 2019.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes
over-parameterized deep relu networks. arXiv preprint arXiv:1811.08888, 2018.
Appendix
A Relationship between Kernel Assumptions
Proof of Proposition 1. We here prove the statement (i). Since H∞ is invertible, we set w
(H ∞)-1(yι, ∙∙∙ ,yn)τ and set
n
v(θ(0)) = X ∂θσ(θ(0)τxj)wj.
j=1
Then, we get
y Ddσ(θ(0)τxi),v(θ(0))E	= y%H∞W = 1.
∖	L L2(μ0)
We can bound the norm of kv(θ(0))k2 as follows:
n
kv(θ(0))k2 ≤ X k∂θσ(θ(O)Txj)k2∣Wj|
j=1
≤	k∂θ σ(θ(0)τxj)k2n kwk2
≤ √nκ1∣∣w∣∣2
nK1
≤ F.
Thus, by resetting v(θ(0)) J ”K()), We conclude the statement (i).
12
Under review as a conference paper at ICLR 2020
We next prove the statement (ii). For ξ = (αiyi)in=1 (αi > 0),
n
n
E ξiH∞ξj = E Eθ(0)〜μo [ξi∂θ(θ(0)>Xi)>∂θ(θ⑼>Xj)ξj]
i,j=1
i,j=1
Eθ(0)〜μo
n
X ξi∂θ(θ(0)>xi)
i=1
2
2
≥ Eθ(0)〜μo
ξi∂θ(θ(0)>xi)>v(θ(0))!
≥	Eθ(0)〜μo
XX ξi∂θ (θ ⑼ >Xi)>v(θ(O))#)
(XX αiEθ⑼〜μo [yi∂θ(θ⑼>Xi)>v(θ⑼)
2
≥ ρ2
≥ ρ2
Xi=n1αi!2
n
Xαi2
i=1
= ρ2kξk22,
where We used ∣∣v(θ(0))∣∣2 ≤ 1 for the first inequality, the convexity of ∣∣ ∙ ∣∣2 and Jensen's inequality
for the second inequality, Assumption (A4) for the third inequality, and ∣∣ ∙ ∣2 ≤ ∣∣∙∣∣ 1 for the last
inequality. Thus, we finish the proof of the statement (ii).
□
B Auxiliary Results
In this section, we introduce several existing results for proving our statements. We first describe the
Hoeffding’s inequality.
Lemma 1 (Hoeffding’s inequality). Let Z, Z1, . . . , Zm be i.i.d. random variables taking values in
[-a, a] for a > 0. Then, for any > 0, we get
P
m
—T Zr — E[Z]
m
r=1
≤ 2 exp
2m
>
We here define the covering number as follows.
Definition 1 (Covering Number). Let (V, ∣∣ ∙ ∣) a metric space. A subset U ⊂ V is called an
E-(proper) cover of V if for ∀v ∈ V, there exists v0 ∈ U such that ∣∣v 一 v0∣ < 匕 Then, E-covering
number N (V, e, ∣∙∣) of V is defined as the cardinally ofthe smallest E-COVer of V, that is,
N (V, E, ∣ ∙ ∣∣) d=f min{∣U∣ | U is an E-COVerOf V}.
The following lemma provide a bound on the Rademacher complexity by Dudley’s integral. For a
real-valued function class F over X and a subset X = (xi)in=1, F|X is defined as {(h(xi))in=1 ∈
Rn | h ∈ F} ⊂ Rn, and F∣χ can be equipped with ∙ ∣∣∞-norm over X.
Lemma 2 (Bartlett et al. (2017)). Let F be a class of real-valued functions taking values in [0, 1]
from X and assume 0 ∈ F. For examples ∀X = (xi)in=1 of size n, we get
<(F|X) ≤ αin>f0
(4α+√n Za
Plog(N(F|x,E, ∣ ∙ ∣∞))de).
Note that we reformulate the statement in Lemma 2 from ∣∣ ∙ ∣∣2-covering to ∣∣ ∙ ∣∣∞-covering.
13
Under review as a conference paper at ICLR 2020
C Proofs of Main Results
In this section, we give an outline of proofs of Theorem 2 and 4.
Global convergence. We first introduce two important propositions which connects gradient meth-
ods with functional gradient methods. The following proposition states that gradient descent methods
become similar to kernel smoothed gradient methods by the neural tangent kernel when a parameter
Θ is sufficiently close to a stationary point and a learning rate η is sufficiently small.
Proposition 5 (Restatement of Proposition 3). Suppose assumption (A1) holds and β ∈ [0, 1).
(i) We set Θ+ = Θ 一 NθL(Θ`) and K = K2 + %K + K2K2. If η ≤ mβ, then
∣L(fθ+ )一 (L(fθ)-η BfL(fθ),TkθVfLgL2(νχ))∣ ≤ 2m2KτkVθL(Θ)k2.
(H)Itfollowsthatfor Θ = @)m=1 and Θ* = (θjt)m=1, (θr, θjt ∈ Rd),
L(Θ) + VθL(⑼>(Θ* - Θ) ≤ L(Θ*) + WkVfL(fθ)kLι(VX)kΘ* - Θk2.
The next proposition states that kernel smoothed gradients have comparable optimization ability to
pure functional gradients in terms of the L1-norm around an initial parameter Θ(0). We introduce the
k ∙ ∣∣2,1 -norm in the parameter space Θ = @)m=1 as ∣∣Θ∣∣2,1 d= Pm=IIIθr∣∣2∙
Proposition 6 (Restatement of Proposition 4). Suppose Assumption 1 holds. For ∀δ ∈ (0, 1) and
∀m ∈ Z+, such that m ≥ 16pKL log 竿,the following statement holds with probability at least 1 一 δ
over the random initialization of Θ(0) =泌0)m=1. If ∣∣Θ — Θ(0)∣2,ι ≤ 4K, then
2
hvf L(fθ),Tkθ Vf L(fθ)iL2(νχ) ≥ 1^m⅛-T kVf L(fθ)kLι(νX).
This proposition is specialized to binary classification problems because the positivity of the Gram-
matrix is needed for regression problems in order to make a similar statement as discussed earlier.
We specify the possible number of iterations of gradient descent (6) such that Θ(t) can remain in the
neighborhood: {Θ | ∣Θ — Θ⑼∣2 ≤ 需} ⊂ {Θ | ∣Θ — Θ⑼k2,1 ≤ 器}.
Proposition 7. Suppose Assumption (A1) and (A3) hold. Consider gradient descent (6) with learning
rate 0 < η < K+K? and the number ofiterations T ∈ Z+. Then,
1 T-1	2
T NkVθL(Θ㈤)k2 ≤ ηTlog(2).
(12)
Especially, we get ∣∣Θ(T) 一 Θ(0) ∣∣2 ≤ ,2ηT log(2). As a result, gradient descent can be performed
for j32ηK⅛g(2)k -iterations within {Θ | ∣Θ — Θ⑼ ∣2 ≤ 需} ⊂ {Θ | ∣Θ — Θ⑼ k2,1 ≤ 器}.
This proposition provides a bound on the distance ∣Θ(T) — Θ(0) ∣2, but we note that this bound will
be further sharpened after showing the convergence of the loss function (see Proposition 2). From
Proposition 5, 6, and 7, we notice that the gradient descent for L(Θ) performs like a pure functional
gradient descent up to O (mmρ2-iterations, resulting in significant decrease of loss functions. We
next provide the proof of Theorem 2 based on this idea.
Proof of Theorem 2. From Proposition 7, the assumption in Proposition 6 regarding Θ is satisfied.
Thus, Proposition 5 and 6 state that for t ∈ {0, . . . , T — 1},
L(fθ(t+i))≤ L(fθ(t))—16m⅞-τ kVf L(fθ(t) )∣Lι(νχ)+2m2Kτ kVeL@t))k2.
14
Under review as a conference paper at ICLR 2020
Summing this inequality over t ∈ {0,...,T - 1} and multiplying by 4mp2-, We have
1 T-1	16 2β-1	8 K T-1
T X ∣∣vf L(fΘ(t) )kLι(νX) ≤	2T L(fΘ(0) ) + -Tψ X kV©L(W))k2.
t=0	ηρ	ρ t=0
Applying L(Θ(0)) = log(2) and inequality (12), We complete the proof.
□
Proofof Theorem 3. Weset τ* = (aa『v(θ(0)))	and Θ* = Θ(0) + T*. Clearly, we have
∣∣Θ* - Θ(0)k2 ≤ α√m.	(13)
As shown in Proposition 5, we get
∣fθ.(x) -Vθfθ(0)(χ)>(θ* -Θ(0))∣ ≤ mkτ*k2 ≤ α2K2m1-β.
In addition, as shown in Proposition 6, since m ≥ 16KKL log 2n, the inequality (21) holds with
probability at least 1 - δ. Hence, we have for ∀i ∈ {1, . . . , n},
yifθ*(χi) ≥ yiVθfθ(o)(χ)>(Θ* - Θ(0)) - α2K2m1-β
m
=yiβ X ∂θσ(θ(0)>Xi)>v(θr0)) - α2K2ml-β
r=1
≥ Qpmi -α2κ2miτ ≥ αρm1-β.
2	2	2 一 4
Thus, the loss at a reference point Θ* can be bounded as follows:
L(Θ*) ≤ n X log(1 + exp(-yifθ*(xi)) ≤ exp(-yifθ* (xi)) ≤ exp (-αpm4------) .	(14)
From Theorem 2, Proposition 5-(ii), Proposition 7 and inequalities (13), (14), it follows that
∃C1,∃C2 > 0,∀T ≤ T,
1 T-1
T X (L(θ(t)) + VθL(Θ(t))>(Θ* - Θ(t)))
T t=0
κ T-1
≤ L(Θ*) + -KT E kVfL(f©(t))kLι(νχ)kΘ* - Θ(t)k2
m T t=0
T-1
≤ LS + 总 X kVfL(fθ(t))kLι(νχ)	max	kθ* - θ(t)k2
m T t=0	t∈{0,...,T -1}
≤ LS + m2K2TUX kVfL(fθ(t))kLι(νχ) t∈{.maT-i} (kθ* - θ(0)k2 + kθ(0) - θ(t)k2)
≤L(Θ*)+	C2— (α2m + ηT)
ρ ηTm
≤ exP (-"β)+C∙ Y + 严
(15)
15
Under review as a conference paper at ICLR 2020
Finally, we bound the average of VθL(Θ(t))> (Θ(t)-Θ*). Because -2a>b = ∣∣ak2 + ∣∣b∣∣2 -∣∣a+b∣∣2
for real vectors a, b, we get by setting a = -ηVθL(Θ(t)) and b = Θ(t) 一 Θ*,
T-1	T-1
T X VθL(θ(t))>(θ(t) - Θ*) = 2r- X(η2∣VθL(θ⑴)k2 + ∣Θ㈤一 Θ*k2 - kθ(t+1) - Θ*k2)
T t=0	2ηT t=0
t=0
≤ log(2)
≤ T
NL(Wt))k2 +壶回0)- θ*k2
α2m
+----
+ 2ηT
Thus, we get that ∃C > 0,
T ∑ L(θ(t))≤ c (T+争+eχp -)+α2 n+P rm
□
We next prove Proposition 2 that gives a sharper bound on kΘ(T) - Θ(0) k.
Proof of Proposition 2. Let L ∈ Z+ be a positive integer such that for T = O(ρ-2-1 log2 (1/)),
2L ≤ T < 2L+1. Clearly, L ≤ log2 T. From Corollary 2, we get for l ∈ {1, . . . , L}
2l-1	2l-1
2- X L(Θ⑴)≤ I X L(Θ㈤)≤ 2C (e + 2-lρ-2 log2(1/e)).
t=2l-1	t=0
Therefore, there exist 2l-1 ≤ ∃sl < 2l for l ∈ {1, . . . , L} such that
L(Θ(sl)) ≤ 2C (E + 2-lρ-2 log2(1/e)).
From the similar argument to the proof of Proposition 7, we get for a < b ∈ Z+,
Pb=a ∣VθL(Θ㈤)∣2 ≤ √2(b - a +1)η-1L(Θ(t)).
Thus, it follows that since s1
T-1
1, ∣∣VθL(Θ⑼)∣∣2 ≤ √mKι by (19), and 2l+1 — 2l-1 + 1 ≤ 2l+1,
L-1 sl+1
T-1
E kVθL(Θ㈤)k2 ≤ ∣∣VθL(Θ⑼)k2 + £ E ∣∣VθL(Θ㈤)k2 + E ∣∣VθL(Θ㈤)k2
t=0
l=1 t=sl
t=sL
≤ √mKι + X q23CηT(2% + ρ-2 log2(1∕e))
≤ √mKι + log2(T),23CηT(Te + ρ-2 log2(1∕e)).
Hence, by setting specific values of η, T, and m in Corollary 2, we get that ∃C0 > 0,
∣Θ⑴-Θ ⑼ ∣2 ≤ ηX ∣VθL(Θ㈤)∣2 ≤ η√mK1 +log2(T ),23Cη(Te + P- log2(1∕e))
t=0
□
Generalization bound. A generalization bound can be derived by utilizing the standard analysis
of the Rademacher complexity (Koltchinskii & Panchenko, 2002). We here introduce a function class
to be measured by the Rademacher complexity. Let lγ (v) (γ > 0) be the ramp loss:
lγ (v) d=ef
1
1 — v∕γ
0
(v < 0),
(0 ≤ v ≤ γ),
(v > γ).
16
Under review as a conference paper at ICLR 2020
Then, a class of all possible ramp losses over X × Y attained by the gradient descent (6) up to
T-iterations is defined as follows: Ω*m,τ d=f {Θ | ∣∣Θ - Θ(0)∣∣2 ≤ D*τ,m},
FlmT d=ef %(yfθ(x)) ： X × Y → [0,1] | Θ ∈ Ω%m,τ}.
Here, D*τ,m is set to be ,2ηT log(2) when considering a general hyperparameter setting in
Theorem 2 and is set to be a sharper bound in Proposition 2: Θ(e3/4 log2 (ρ-2e-1)) when considering
a specific hyperparameter setting in that proposition.
For a given dataset S = (xi, yi)in=1, the Rademacher complexity is defined by <(Fηγ,m,T |S) d=ef
n-1E[suph∈Fγ Pin=1 ih(xi, yi)], where the expectation is taken over the Rademacher random
variables (i)in=,1 ,which are i.i.d. with probabilities P[i = 1] = P[i = -1] = 0.5. The following
well-known result (Koltchinskii & Panchenko, 2002; Mohri et al., 2012; Shalev-Shwartz & Ben-
David, 2014) provides a bound on the expected classification error based on the empirical margin
distribution and the Rademacher complexity. The empirical margin distribution for S is defined as
the ratio of examples satisfying yifΘ(xi) ≤ γ in S.
Lemma 3 (Koltchinskii & Panchenko (2002); Mohri et al. (2012); Shalev-Shwartz & Ben-David
(2014)). Let ∀n ∈ Z+, ∀γ > 0, ∀η> 0, ∀m ∈ Z+, ∀T ∈ Z+, and ∀δ ∈ (0, 1). Then, with
probability at least 1 一 δ over the random choice of S of size n, every Θ ∈ Ω*m,τ satisfies
P(X,Y)”[Yfθ(X) ≤ 0] ≤ P(X,Y)"n[Yfθ(X) ≤ Y] + 2<(F∣mτ|s)+3P(2n)-1 log(2∕δ).
(16)
To instantiate this bound, we have to provide upper bounds on the empirical margin distribution and
the Rademacher complexity. We first give a bound on the Rademacher complexity.
Proposition 8. Suppose Assumption (A1) and (A2) hold. Let ∀γ > 0, ∀η> 0, ∀m ∈ Z+, ∀T ∈ Z+,
∀δ ∈ (0, 1), and ∀S be examples of size n. Then, there exists a uniform constant C > 0 such that
with probability at least 1 - δ with respect to the initialization ofΘ(0),
<(FY,m,τ|S) ≤ CYTm 1-βDη,m,τ(1+K1+K2) J^ log (n(1 + KI + K2)(log(m/S) + Dnm,t)).
Moreover, when σ is convex and σ(0) = 0, we can avoid the dependence with respect to the dimension
d. With probability at least 1 - δ over a random initialization of Θ(0),
<(FY,m,τ∣s) ≤ 8⅛β(Dn，m，T + rIog(Amδ)).
Proof of Theorem 4. We prove this theorem by instantiating inequality (16). Let (Θ(t))tT=-01 be a
sequence obtained by the gradient descent (6). Because (Θ(t))T=o1 is contained in Ω*m,τ, as
indicated in Proposition 7, inequality (16) holds for this sequence. As for the Rademacher complexity
in (16), we can utilize Proposition 8. Thus, the resulting problem is to prove the convergence of the
empirical margin distribution: P(x,y)〜νn[Yfθ(t)(X) ≤ γ]. We here give its upper-bound below.
0.5 |yi - 2pθ(Y = 1|xi) + 1∣≥ (1+exp(γ))-1 ^⇒ yifθ(xi) ≤ Y
Therefore, from Markov’s inequality,
P(X,Y )“n[Yfθ(t)(X) ≤ γ] = P(χo,γ0)“n 2 |Y0 - 2pθ(y = 1|X0) + 1∣≥ 1 + eXp(Y)
≤ (1 + exp(γ))kVfL(fθ)kLι(νX).
Combining this inequality with Lemma 3, then for ∀t ∈ {0, . . . , T - 1},
P(X,Y)j[Yfθ(t)(X) ≤ 0] ≤ (1+exp(γ))kVfL(fθ(t))kLι(νχ) +2<(F∣,m,τ|s) + 3 JW(箸.
Noting that η, m, and T satisfy the conditions in Theorem 2, we can complete the proof by taking the
average over t ∈ {0,..., T - 1} and applying Proposition 8 and Theorem 2.	□
17
Under review as a conference paper at ICLR 2020
D Proofs for Global Convergence
D.1 Proof of Proposition 5
Proof of Proposition 5. We first show the smoothness of fΘ (x) with respect to Θ for ∀x ∈ X,
(IlxIl2 ≤ 1). Noting that Vθfθ(x) = diag (mɪeaσ00(θ>x)xx>)m=1, We get for T = (Tr)m=1 such
that Prm=1 kτrk22 = 1 (τr ∈ Rd),
m
T>V2ΘfΘ(x)T = XTr>∂θ2rfΘ(x)Tr
r=1
m
≤ mβ Xlσ00(θ>x)l lτ7x∣2
r=1
m
≤ mβ X kτrk2
r=1
K2
--T .
mβ
This means that for T = (Tr )rm=1, (Tr ∈ Rd),
m
∣fθ+τ(x) - (fθ(x) + Vθfθ(x)>τ )∣ ≤ K kτ k2 = K X kτrk2.	(17)
r=1
Let us define gx(T) as the second-order term of Taylor’s expansion of fΘ(x) With respect to Θ:
fΘ+τ (x) = fΘ(x) + VΘfΘ(x)>T + gx(T).
From the inequality (17), we have ∣gχ(τ )| ≤ Kmek 2. Then, by the smoothness of l(Z, y) with respect
to Z and ∣∂2l(Z,y)∣ ≤ 1/4, we get
llll(fΘ+τ (x), y) - (l(fΘ(x), y)+∂ζ l(fΘ(x), y)(VΘfΘ(x)>T + gx(T)))lll
12
≤ 4 lvθfθ(x)>τ + gχ(τ)l
≤ 2 (kvθfθ(x)k2+Km2? 2)kτ ∣∣2.
By the triangle inequality, we get
ll(fθ+τ(x),y) - (l(fθ(x),y) + dζ 1(fθ(x),y')vθfθ(X)TT) |
≤
包l(fθ(x),y)gχ(τ)| + 2 (∣∣Vθfθ(x)k2 + Kmτfi)
≤
≤
IT I22
1
2
1
2
(kvθfθ(x)k2 + m2 +
K2kτk2 ∖
m2β )
K2kτk2 ∖
m2β	)
IT I22
ITI22 ,
(18)
where for the second inequality, we used ∣∂ζl(Z, y)| ≤ 1 and for the last inequality, we used
m
IvΘfΘ(x)I22 =X
r=1
1	2 m 1	K2
卫σ (θr X)X ≤ X	2β ∣σ (θr X)12 ≤	2β-1.
mβ	m2β	m2β-1
2	r=1
We here set T = -ηvΘL(Θ). The right hand side of (18) is upper bounded by
1
2
η κ2κ2 ∖
m4β-1 J
ITI22 .
18
Under review as a conference paper at ICLR 2020
because
m
kVθL(Θ)k2 = X k∂θrL(⑼k2
r=1
m
X
r=1
1n
-E ∂ζ l(fθ(χi), yi)∂θr fθ(χi)
n i=1
2
2
mn	2
≤ X -X ∣∂ζl(fθ(χi),yi)∣k∂θrfθ(χi)k2
r=1 n i=1
≤ - - X |dZ l(fθ(χi),yi)1! X( max	kdθr fθ(xj )k2)
n i=1	r=1 j∈{1,...,n}
m-	2
=kVf L(fΘ)kLι(νX ) E ( ymax γ 嬴 lσ (θr Xj )lkxj k2 )
r=1 j∈{1,...,n} m
≤ kVfL(fΘ)k2L1(νnX)m1-2βK12
≤ m1-2βK12 .
(19)
Therefore, we get
∣l(fθ+τ(χ),y) - (l(fθ(χ),y) - η∂ζl(fθ(χ),y)Vθfθ(χ)>VθL(Θ)) |
¾βK2) η2kvθL(θ)k2
≤ 2m2β-ι (k2 + 2K2 + k2k2)η2kVθL(⑼k2,
(20)
where we used β ∈ [0, -) and η ≤ mβ for the last inequality.
Noting that from the definition of kernel smoothing of functional gradients (3), we see
Vθfθ(x)>VθL(Θ) = Vθfθ(χ)> (- X ∂ζl(fθ(χi),yi)Vθfθ(χi))
= TkΘ Vf L(fΘ)(x).
Therefore, by taking the expectation of (20) according to the empirical distribution νn, we get
L(fΘ+τ) - L(fΘ)-ηhVfL(fΘ),TkΘVfL(fΘ)iL2(νnX)
2
≤ 2m⅛-1 (k2 + 2K2 + k2k2) kVθL(θ)k2.
This completes the proof of the statement (i).
From the convexity of l(ζ, y) with respect to ζ, we have
i(fθ*(X),Y) = I fθ(X) + Vθfθ(X)>(Θ* -⑼ + gχ(Θ* -Θ),Y)
≥ l(fθ(X),Y) + ∂ζl(fθ(X),Y) (Vθfθ(X)>(Θ* -⑼ + gχ(Θ* - Θ))
≥ l(fθ(X),Y) + Vθl(fθ(X),Y)>(Θ* -⑼-∣∂ζl(fθ(X),Y)| κ2kθm- θk2,
K2k	K K	K2 kΘ* —Θk2
where We used ∣gχ(Θ* 一 Θ)∣ ≤ 2"m3~~. Thus, by taking the expectation With respect to
(X, Y)〜Vn, we get
L(Θ*) ≥ L(Θ) + VθL(⑼>(Θ* -⑼-KkVfL(fθ)kLι(VX)kΘ* - Θk2.
This finishes the proof of the statement (ii).
□
19
Under review as a conference paper at ICLR 2020
D.2 Proof of Proposition 6
Proof of Proposition 6. Set Zr,i = yi∂θσ(θr(0)xi)>v(θr(0)). We find clearly |Zr,i| ≤ K1 from
Assumption 1. By applying Hoeffding’s inequality to Zr,i for each i ∈ {1, . . . , n} and taking an
union bound, we have
PΘ(0)
2 m/2	ρ
m,axn} m∑Zr,i- Eθr0) [Zr,i] > 2
r=1
≤ 2n exp
ρ2m
16K2
In other words, since m ≥ 16^21 log 寿,We have with probability 1 - δ,
max
i∈{1,...,n}
m/2
2 X Zr,i
m
r=1
- Eθ(0) [Zr,i]
≤ P .
Therefore, using Assumption 1 (A4) and noting Θ(0) = (θr )rm=1 is symmetrically initialized, we get
with probability 1 - δ for ∀i ∈ {1, . . . , n},
m
m Xyi∂θσ(θ(%i)>v(θ*) ≥ 2.	(21)
r=1
In the following proof, we assume Θ(0) = (θr(0))rm=1 satisfies this inequality. We get from the
K2-Lipschitz continuity of σ0 that for Θ = @)m=1 satisfying ∣∣Θ - Θ⑼ ∣∣2,ι ≤ 4Kρ,
m	m
—X yiσ0(θ>Xi)x>v(θr0))----X yiσo(θr0)>Xi)x>v(θr0))
m	r	rm	r	r
r=1	r=1
m
≤ m X ∣yiX>Vr(θr0))(σ0(θ>Xi) - σ0(θ(0)>Xi))∣
r=1
1m
≤ — XK2l(θr - θr0))>Xil
mr
r=1
≤ K ∣Θ- Θ(0)k2,1
≤ P.
4
This means that there exists (vr)rm=1 such that ∣vr∣2 ≤ 1 (∀r ∈ {1, . . . ,m}) and for ∀Θ = (θr)rm=1
satisfying ∣∣Θ - Θ⑼∣∣2,1 ≤ 黑,
1m
m∑^yi∂θσ(θ>Xi)>Vr ≥ P,	∀i ∈ {1,...,n}.
Then, we get the following bound: for ∀(αi)in=1 (αi ∈ (0, 1)),
nm	n	m	n
mɪɪyiai∂θ σ(θ>Xi)>Vr = m	αi	yi∂θσ(θ>Xi)>Vr ≥ 4 Σ αi > 0.	(22)
i=1 r=1	i=1	r=1	i=1
20
Under review as a conference paper at ICLR 2020
Noting that Vf L(fΘ)(xi) = l+exp-yf Θ (Xi)), We get
1n
hVfL(fθ),TkθVfL(fθ)iL2(νχ) = n E kθ(xi,Xj)VfLg(XiNfL(fθ)(xj)
i,j=1
1
n2
Vf L(fΘ )(xi)VΘfΘ (xi)
i=1	2
mn
n12 X X
r=1 i=1
VfL(fΘ)(xi)∂θrfΘ(xi)
2
2
m
3 X
n2
r=1
1n
m NVf L(fθ)(xi)∂θσ(θ>Xi)
2
2
mn
≥ nη2 X mβ X Vf L(fθ)(xi)∂θσ(θ>Xi)>Vr
r=1	i=1
≥
nm	2
小XX Vf L(fΘ)(xi)∂θσ(θr>xi)>vr
≥
m1-2β P
16n2
—1—!2
1 + exp(yifΘ(xi))
where we used Ilvrk2 ≤ 1 for the first inequality, the convexity of ∣∣ ∙ k2 for the second inequality,
and (22) for the last inequality. We can find that this inequality finishes the proof because
7——>、= η |yi — 2Pθ(Y = 1 | xi) + 1∣.
1 + exp(fΘ(xi)yi)	2
□
D.3 Proof of Proposition 7
The proof of Proposition 7 is based on the traditional convergence analysis of gradient descent for
smooth objective functions in finite-dimensional space.
Proof of Proposition 7. We first specify the smoothness of the logistic loss function. We set φ(v) =
log(1 + exp(-v)) and l(y, fΘ(x)) = φ(yfΘ(x)). By the simple calculation, we get that for r, s ∈
{1,...,m},
∂2	a a
AA AA l(y, fθ(X)) = φ (yfθ(χ))-^∙βσ (θ>x)σ (θ>χ)χχ
∂θr∂θs	m2β	r s
+ 1[r = s] -yφ (yfθ(x')')ar σ00(θ>x)xx>.
mβ	r
Noting that ∣∣φ0k∞ ≤ 1 and kφ00k∞ ≤ 1, we can see that the maximum eigen-value of
(∂2l(y, fθ(χ))∕∂θr∂θs)ms=ι is upper bounded by
M d=f JT (K2 + K2).
21
Under review as a conference paper at ICLR 2020
Indeed, for v = (vr)rm=1 such that Prm=1kvrk22 ≤ 1, (vr ∈ Rd), we have
XX v> 弋fX Vs = φ00(f (X)) 住 arσ0(θ>x)v>+ 得Φ'(f (x)) XX aσ00(θ>x)(v>x)2
r ∂θr ∂θs	m2β	r r	mβ	r r
r,s=1	r=1	r=1
≤ 4K2β (X Mk2! + K XX Mk2
≤
≤
≤
ZKe (√m∏2)2 + K
4m2β	mβ
4m2β-1 + mβ
4m2β-i(K2 + K2).
Therefore, the loss function L(Θ) is M -Lipschitz smooth with respect to Θ, that is, for
L(Θ0) ≤ L(Θ) + (VL(Θ), Θ0 - S' + MkΘ0 - Θ∣∣2.
Plugging Θ = Θ(t) and Θ0 = Θ(t+1) = Θ 一 ηVθL(Θ(t)) into this inequality, We get
L(Θ(t+1)) ≤ L(Θ⑴)一 η(1 一 等)∣∣VL(θ(t))∣∣2
≤L(Θ(t)) - 2kVL(Θ⑴)∣∣2,
Where We used η ≤ 1/M for the last inequality. By summing this inequality over t ∈ {0, . . . , T 一 1}
and multiplying by nT, we get
T-1
T X kVL(Θ⑴)k2 ≤ ηTL(Θ⑼) = ηTlog(2),
where we used L(Θ(0)) = log(2). Therefore, we have that from equation (23),
T-1
kΘ(t) -Θ(0)k2 ≤η XVΘL(Θ(t))
t=0	2
(23)
T-1
≤ η√T∖ X∣∣VθL(Θ㈤)∣∣2
t=0
≤ √2ηT log(2).
The last statement of Proposition 7 immediately follows from this and the following inequality.
m
kΘ(t) - Θ(0)k2,1 = X kθr(t) - θr(0)k2
r=1
≤ √mt
m
X kθr(t) - θr(0)k22
r=1
√m∣∣θ(t) - θ(0)∣∣2.
□
E Proofs for Generalization Bounds
Proofof Proposition 8. In this proof, We denote F = FYm T and Ω = Ωη,m,τ for simplicity. We
define Fy d=f {h(∙, y) : X → [0,1] | h ∈ F}. Then, for a given dataset S = (xi, yi)bι, We notice
22
Under review as a conference paper at ICLR 2020
that <(F|S) ≤ <(F1|X) + <(F-1 |X), where X = (xi)in=1. Thus, it is enough to provide an upper
bound on <(F1 |X) because a bound on the other complexity can be also derived in the same way.
We first give a uniform high probability bound on the initialization kθr(0) k2 for ∀r ∈ {1, . . . , m}. We
get from (A2), for t > 0,
P max	kθr(0) k2 ≥ t
r∈{1,...,m}
m
≤ XP kθr(0)k2 ≥ t ≤ mAexp(-bt2).
r=1
Thus, by choosing t so that δ = mAexp(-bt2), we confirm that with probability at least 1 - δ,
max	kθr(0)k2 ≤ R d=ef
r∈{1,...,m}
f^mA.
We introduce several notations. Fix Ro > 0. We denote θ = (θ, θ0) ∈ R2d, (θ, θ0 ∈ Rd)and, define
for θ,
gθ(X)
def σ(θ>x) - σ(θ0>x)
=F-W2
When θ = θ0, We define g^(x) = 0. From the Lipschitz continuity of σ, the range of g^ is [-Kι, Ki].
Moreover, we define
Ω =f {θ ∈ R2d | ∣∣θ∣∣2, ∣∣θ0∣∣2 ≤ R + Dη,m,τ, ∣∣θ - θ0∣∣2 ≤ Dn,m,τ},
Ω+ d=f {θ ∈ Ω | Ro < kθ - θ0∣∣2 ≤ Dη,m,τ},
Ω- d= {θ ∈ Ω | ∣∣θ -θ0∣∣2 ≤ Ro},
G+ d=f	{gθ	：	X→	[-K1,K1]	| θ ∈	Ω+},
G- d=f	{g	：	X→	[-K1,K1]	| θ ∈	Ω-},
H d=f {fθ : X → R | Θ ∈ Ω}.
Clearly, We see
Ω = Ω- ∪ Ω+ and {沏 | θ ∈ Ω} = G- ∪ 0+.
From the Lipschitz continuity of IY, we find <(Fi ∣χ) ≤ γ-1<(H∣χ).
We now derive an upper bound on the Rademacher complexity. Set CM d= m2-βDnmT.
<(H∣x ) = 1E
n
n
sup	ifΘ(xi)
θ∈ωi=1	.
1
n
n
sup	i(fΘ(xi) - fΘ(0) (xi))
Θ∈Ω . ,l
i=1
nm
sup XiX
θ∈ω M r=i
∣∣θr - θr0)∣∣2	σ(θ>Xi) - σ(θr0)>Xi)
CMmβ	rr	I∣θr-θr0)∣∣2
CM E
n
(24)
where we used the fact that fΘ(0) (xi) is a constant in the expectation for the second equality.
Since, for Θ ∈ Ω,
XX kθr - θ*∣2
CM	cm mβ
r=1
m2-β∣∣Θ - Θ(0)∣∣2
CM
≤ 1,
23
Under review as a conference paper at ICLR 2020
equation (24) can be upper-bounded by the Rademacher complexity of the convex hull. Hence,
<(H|X) ≤
CM E
n
nm
CM E
n
CM E
n
CM E
n
sup
Θ∈Ω
Prm=1 br≤1,br∈[0,1]
i	brar
i=1	r=1
σ(θ>xi) - σ(θ∖01)TXi)
-kθr-θ* k2-
n m	σ(θr>xi) - σ(θr>xi)
SUP	TCi Tbr ——丽―丽-------------
(θr ,θr )m=ι∈Ωm i=1	r=1	kθr-θrk2
Prm=1 br≤1,br∈[0,1]
n
sup ɪ2 Ci
(θ,θ0)∈Ω i=ι
kθ - θ0k2
n
Sup): cigθ (Xi)
θ∈Ω i=ι
≤
≤ CM E
n
nn
Sup	Cig(Xi) + Sup	Cig(Xi)
g∈G- i=1	g∈G+ i=1
CM(<(G-|X)+<(G+|X)). (25)
We used that for Θ ∈ Ω, (θr,。\0)) ∈ Ω (∀r ∈ {1,..., m}) because ∣∣Θ -㊀⑼口？ ≤ D*m,τ.
Moreover, the term ar disappeared by the symmetry. We used the fact that the convex hull of a
hypothesis class does not increase the Rademacher complexity for the first equality.
We next derive an upper bound on the Rademacher complexity <(G+|X) through the covering number
N(G+ ∣χ, c, ∣∙ ∣∣∞) and Lemma 2. To this end, We investigate the sensitivity of ||沏∣∣∞ with respect
to θ as follows.
Let θι = (θι, θ1) ∈ Ω+ and θ = (θ2, θ2) ∈ Ω+ be parameters such that
kθι - θ212 = q∣θι-θ2k2 + kθ1-θ2k2 ≤ C.
This leads to
∣θι - Θ2k2, kθ1 - θ212 ≤ C and ||仇一θ1∣∣2 -|化-θ∣∣2∣ ≤ 2c.
We get from these inequalities that for ∣X∣2 ≤ 1,
∣θ2 - θ20 ∣2(σ(θ1>X) - σ(θ10>X)) - ∣θ1 - θ10 ∣2(σ(θ2>X) - σ(θ20>X))
M (X)-湿(X)I = -----------------------kθl-θ1∣2kθ2 -θ2∣2--------------------
∣(kθ2 -θ2∣2 -∣θl-θ1 k2)(σ(θ[x)-σ(θjx))∣
一	kθ1 - θ1k2 kθ2 - θ2 ∣∣2
∣θ1 - θ10 ∣2(σ(θ1>X) - σ(θ10>X) - σ(θ2>X) + σ(θ20>X))
+	kθι-θ1 k2kθ2 -θ212
4Dη,m,τ cKι
≤ -R-.
Thus, if ∣∣θι - Θ2k2 ≤ C for θ1,θ2 ∈ Ω+, then ∣∣gθ1 - gθ21∞ ≤ 4D”,m,TcKι∕R2. Since,
G+ ⊂ {gθ I ∣∣θ∣2 ≤ 2R + 2Dη,m,τ, θ ∈ R2d},
we get for the unit-ball B1 ⊂ R2d with respect to ∣∣ ∙ ∣2,
N(G+Ix, C, ∣H∣∞) ≤ CdN (Bi, k (Rn RA 炉~T，卜 ∣2 I，
K1 (RDη,m,T + Dη2,m,T)
where C1 > 0 is a uniform constant. Hence,
logN(G+∣X,C, k ∙ k∞) ≤ O (dlog (1 + KIRD”％+ Dη,m,T))).
24
Under review as a conference paper at ICLR 2020
Applying Lemma 2 with α = Kι∕√n, We obtain
ud I X	C(N	d 1	√ I √n(R^Dη∣m,τ + Dη2,m,T)
<(G+lX) = O IK1t nlog (1 +-----------R0——
We next evaluate <(G-∣χ) by using a linear approximation. Since ∣σ00(∙)∣ ≤ K2, We get
∣σ(θ >x) — σ(θ>x) — σ0(θ>x)(θ0 — θ)>x∣ ≤ K2∣∣θ0 — θ∣∣2∙
Therefore, we get for θ = (θ, θ0) ∈ Ω-,
(X)- ">χ)(θ00-θ)>x ≤ K2kθ -θ0k2 ≤ K2R0.
kθ - θ0k2
From this approximation, the Rademacher complexity can be bounded as follows.
<(G-∣x) ≤ K2R0 + 1E
n
n
SUp X
.θ∈Ω- i=l
σ0(θ>Xi)(θ0 - θ)>Xi
ei	kθ -θ0k2
≤ K2R0 +1E
n
sup	iσ0(θ>xi)w>xi
kθk2≤R+Dη,m,T, i=1
kwk2≤1
When ∙√∣∣θι - θk2 + ∣∣wι - w2∣∣2 ≤ C for ∣∣θik2 ≤ R + Dη,m,τ and ∣∣wik2 ≤ 1, We get for
kxk2 ≤ 1,
∣σ0(θ>x)w>x — σ0(θ>x)w>x| ≤ ∣(σ0(θ>x) — σ0(θ>x))w>x| + ∣σ0(θ>x)(wι — W2)>x|
≤(K1 + K2 )C.
We set
G-0 d=ef {x → σ0(θ>x)w>x | ∣θ∣2 ≤ R + Dη,m,T, ∣w∣2 ≤ 1}.
Therefore, by the same argument as the case of G+, the following bound holds.
N (G-∣x ,c, k∙k∞) ≤ CdN(Bi, (k ,「Ieτ, k∙k2),
(K1 + K2)(R + Dη
,m,T)
where C2 > 0 is a uniform constant. Hence,
logN(G-∣X,C, k∙ k∞) ≤ O (dlog (1 + (KI + K2)(R + Dη,m,τ))).
By Lemma 2 with α = 1/√n, we get
<(G-|X) ≤ O
K2 R0 +
no log (1 + √n(K1 + K2)(R + Dη,m,T))
(27)
Combining (25), (26), (27) with R0 =，d/n, and LiPschiz continuity of IY, we obtain
<(Fι∣x) ≤ O (mL-Yia (1 + Ki + K2)ʌ/d log (n(1 + K1 + K2)(R + Dη,m,τ))).
Now, let us turn to the second part of Proposition 8. Let us assume that the function σ is convex and
satisfies σ(0) = 0. The main argument uses the convexity of activation function in the same spirit as
(Chinot et al., 2019). As for the first part, with probability larger than 1 - δ over the initialization
max ∣θr(0)∣2 ≤ R d=ef
r∈{1,...,m}
f^mA.
25
Under review as a conference paper at ICLR 2020
We only focus on the control of <(H|X).
<(H∣x ) = 1E
n
n
sup	ifΘ(xi)
θ∈ω i=1	.
1E
n
n
sup i fΘ(xi) - fΘ(0) (xi)
θ∈ωi=1
1E
n
nm
sup X G X 三(σ(θTXi) -σ(θ∖0)Txi))
θ∈ω i=ι r=ιm
1E sup X Ei三(σ(θTxi) - σ(θr0)Txi))
n	θ∈ω(1⅛a mβ
+ 1E sup X Ei-arβ (σ(θr0)Txi) - σ(θTxi))
n	θ∈Q(i,r⅛c m
where A = {(i, r) ∈ {1,…，n}×{1,…，m} : σ(θTxi) — σ(θr0)Txi) ≥ 0}.
Let us control the first term (i.e for (i, r) ∈ A). For any i, r in A let ψi,r : R 7→ R defined for all
u ∈ R as:
ψi,r (u) =σ	(u + θr(0)T xi) - σ(θr(0)T xi)
The functions ψi,r are such that ψi,r (0) = 0. There are convex because σ is. In particular for any
α ≥ 1 andU ∈ R, ψi,r (αu) ≥ αψi,r(u). Wealsohave ψi,r ((θr —θ^0))τxi) = σ(θTxi)-σ(θ(0)τxi).
Since Θ ∈ Ω we have ∣∣Θ - Θ(0)∣∣2 ≤ D*m,τ and for any r ∈ {1,…，m}, |包 - θ∖0) ∣∣2 ≤ D*m,τ.
As a consequence, for any (i, r) ∈ A, there exists βi,r ∈ [0, 1] such that
Dη,m,τ
∣θr- θ*k2
ψi,r ((θr - θ'0))Txi) = βi,r Ψi,r
Dη,m,τ
∣θr- θr0)∣2
(θr -θr(0))Txi
Since for any i ∈ {1,…，n}, Pm=I kθCMmik2 βi,r ≤ 1, We get
LE sup
n θ∈Q(i,r)∈A
≤ 1 CM E
n Dη,m,T
sup
θ = (Sr)生1"瓦一栋O) l∣2≤Dη,m,T (i,j)∈A
Eiar
⅛> βi^ Ψi.
Dη,m,T (θr
kθr - θr0)∣2
—
≤ 1 CM E
n Dη,m,T
1 CM E
n Dη,m,T
nm
sup	EEi y2 abrψi,r Mxi)
Θ = (θr)m=i：|Sr ∣2≤Dη,m,T	M M
b=(br )rm=1,br ∈[0,1],Prm=1 br ≤1
nm
sup	X Ei X abr (σ((θr +。9)Txi) 一。⑼。)Txi))
Θ = (θr)m=i：|Sr k2≤Dη,m,T	i=ι	r=1
b=(br)rm=1,br∈[0,1],Prm=1br≤1 i=1
26
Under review as a conference paper at ICLR 2020
Therefore, with probability larger than 1 - δ ,
1
-E SUP )
n θ∈ω,.δ~<λ
(i,r)∈A
(0)T
r
xi)
nm
≤ 一 C M- E	SuP	XiXarbr (σ((θr + θr)TXi) - σ(京Txi))
n Dη,m,T	kθrk2≤Dη,m,τikdr∣∣2≤R	i=ι	r=1
b=(br)rm=1,br∈[0,1],Prm=1br≤1
nn
≤ — 口 M E	SuP	Xg(σ((θ + 6)xi) + sup Xeiσ(θτ4))
n Dη,m,T	[θ”∣θ∣∣2≤Dη,m,T ；IBrk2≤Ri=1	Bk≤Ri=1
≤ K1 CM E
n Dη,m,T
nn
SuP	i(θ + θ6)T xi + SuP	iθ6Txi
θ“∣θ∣∣2≤Dη,m,T ；k^r k2≤Ri=1	kd"∣2≤Ri=1
≤
Kl CM
√n Dη,m,T
(Dη,m,T
+ 2R)
Kim1/2-β (Dη,m,τ + 2R)
√n
τ	.	.1	1 .	1 -	/ ■	∖	∙ AC 1 . T	-m>	-m> ι r∙ ι r∙ 11	_ τπ>
Let us turn to the	second term.	For any (i,	r)	in Ac let ψi,r	: R 7→ R defined for all	u ∈ R as:
ψ6i,r(u) = σ(u + θrTxi) - σ(θrTxi)
We have ψi,r (00) - θr)τxi) = σ(θ(r0')Txi) — σ(θrτg). Using the same path as for (i,j) ∈ A, with
probability larger than 1 - δ, we obtain,
1E SuP X q-arβ (σ(θr0)τxi)-
n θ∈Q(i,r⅛c mβ
nm
≤ 一 C M- E	sup	XiXa，r br (σ((θr + Gr )Txi) — σ(θrxi))
n η,m,τ	kθrk2≤Dη,m,T ,∣Brk2≤R + Dη,m,T i=1 r=1
b=(br)rm=1,br∈[0,1],Prm=1 br≤1
≤ K1 1L(3D	0
-√n Dη,m,τ(	η,m,
+ 2R) .
□
27