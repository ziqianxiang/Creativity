Under review as a conference paper at ICLR 2020
Towards Stable and comprehensive Domain
Alignment: Max-Margin Domain-Adversarial
Training
Anonymous authors
Paper under double-blind review
Ab stract
Domain adaptation tackles the problem of transferring knowledge from a label-
rich source domain to an unlabeled or label-scarce target domain. Recently
domain-adversarial training (DAT) has shown promising capacity to learn a domain-
invariant feature space by reversing the gradient propagation of a domain classifier.
However, DAT is still vulnerable in several aspects including (1) training instability
due to the overwhelming discriminative ability of the domain classifier in adversar-
ial training, (2) restrictive feature-level alignment, and (3) lack of interpretability
or systematic explanation of the learned feature space. In this paper, we propose a
novel Max-margin Domain-Adversarial Training (MDAT) by designing an Adver-
sarial Reconstruction Network (ARN). The proposed MDAT stabilizes the gradient
reversing in ARN by replacing the domain classifier with a reconstruction network,
and in this manner ARN conducts both feature-level and pixel-level domain align-
ment without involving extra network structures. Furthermore, ARN demonstrates
strong robustness to a wide range of hyper-parameters settings, greatly alleviating
the task of model selection. Extensive empirical results validate that our approach
outperforms other state-of-the-art domain alignment methods. Additionally, the
reconstructed target samples are visualized to interpret the domain-invariant feature
space which conforms with our intuition.
1	Introduction
Deep neural networks have gained great success on a wide range of tasks such as visual recognition
and machine translation (LeCun et al., 2015). They usually require a large number of labeled data
that can be prohibitively expensive to collect, and even with sufficient supervision their performance
can still be poor when being generalized to a new environment. The problem of discrepancy between
the training and testing data distribution is commonly referred to as domain shift (Shimodaira,
2000). To alleviate the effect of such shift, domain adaptation sets out to obtain a model trained
in a label-rich source domain to generalize well in an unlabeled target domain. Domain adaptation
has benefited various applications in many practical scenarios, including but not limited to object
detection under challenging conditions (Chen et al., 2018), cost-effective learning using only synthetic
data to generalize to real-world imagery (Vazquez et al., 2013), etc.
Prevailing methods for unsupervised domain adaptation (UDA) are mostly based on domain alignment
which aims to learn domain-invariant features by reducing the distribution discrepancy between the
source and target domain using some pre-defined metrics such as maximum mean discrepancy (Tzeng
et al., 2014). Recently, Ganin & Lempitsky (2015) proposed to achieve domain alignment by domain-
adversarial training (DAT) that reverses the gradients of a domain classifier to maximize domain
confusion. Having yielded remarkable performance gain, DAT was employed in many subsequent
UDA methods (Long et al., 2018; Shu et al., 2018). Even so, there still exist three critical issues of
DAT that hinder its performance: (1) as the domain classifier has high-capacity to discriminate two
domains, the unbalanced adversarial training cannot continuously provide effective gradients, which
is usually overcome by manually adjusting the weights of adversarial training according to specific
tasks; (2) DAT-based methods cannot deal with pixel-level domain shift (Hoffman et al., 2018); (3)
the domain-invariant features learned by DAT are only based on intuition but difficult to interpret,
which impedes the investigation of the underlying mechanism of adversarial domain adaptation.
1
Under review as a conference paper at ICLR 2020
To overcome the aforementioned difficulties, we propose an innovative DAT approach, namely
Max-margin Domain-Adversarial Training (MDAT), to realize stable and comprehensive domain
alignment. To demonstrate its effectiveness, we develop an Adversarial Reconstruction Network
(ARN) that only utilizes MDAT for UDA. Specifically, ARN consists of a shared feature extractor,
a label predictor, and a reconstruction network (i.e. decoder) that serves as a domain classifier.
Supervised learning is conducted on source domain, and MDAT helps learn domain-invariant features.
In MDAT, the decoder only focuses on reconstructing samples on source domain and pushing the
target domain away from a margin, while the feature extractor aims to fool the decoder by learning to
reconstruct samples on target domain. In this way, three critical issues can be solved by MDAT: (1)
the max-margin loss reduces the discriminative capacity of domain classifier, leading to balanced and
thus stable adversarial training; (2) without involving new network structures, MDAT achieves both
pixel-level and feature-level domain alignment; (3) visualizing the reconstructed samples reveals
how the source and target domains are aligned. We evaluate ARN with MDAT on five visual and
non-visual UDA benchmarks. It achieves significant improvement to DAT on all tasks with pixel-level
or higher-level domain shift. We also observe that it is insensitive to the choices of hyperparameters
and as such is favorable for replication in practice. In principle, our approach is generic and can be
used to enhance any UDA methods that leverage domain alignment as an ingredient.
2	Related Work
Domain adaptation aims to transfer knowledge from one domain to another. Ben-David et al. (2010)
provide an upper bound of the test error on the target domain in terms of the source error and the
H4H-distance. As the source error is stationary for a fixed model, the goal of most UDA methods
is to minimize the H4H-distance by reducing some metrics such as Maximum Mean Discrepancy
(MMD) (Tzeng et al., 2014; Long et al., 2015) and CORAL (Sun & Saenko, 2016). Inspired by
Generative Adversarial Networks (GAN) (Goodfellow et al., 2014), Ganin & Lempitsky (2015)
proposed to learn domain-invariant features by adversarial training, which has inspired many UDA
methods thereafter. Adversarial Discriminative Domain Adaptation (ADDA) tried to fool the label
classifier by adversarial training but not in an end-to-end manner. CyCADA (Hoffman et al., 2018)
and PixelDA (Bousmalis et al., 2017) leveraged GAN to conduct both feature-level and pixel-level
domain adaptation, which yields significant improvement yet the network complexity is high.
Another line of approaches that are relevant to our method is the reconstruction network (i.e. the
decoder network). The success of image-to-image translation corroborates that it helps learn pixel-
level features in an unsupervised manner. In UDA, Ghifary et al. (2016) employed a decoder network
for pixel-level adaptation, and Domain Separate Network (DSN) (Bousmalis et al., 2016) further
leveraged multiple reconstruction networks to learn domain-specific features. These approaches
treat the decoder network as an independent component that is irrelevant to domain alignment
(Glorot et al., 2011). In this paper, our approach proposes to utilize the decoder network as domain
classifier in MDAT which enables both feature-level and pixel-level domain alignment in a stable and
straightforward fashion.
3	Problem Formulation
3.1	Problem Definition and Notations
In unsupervised domain adaptation, we assume that the model works with a labeled dataset XS
and an unlabeled dataset XT. Let XS = {(xis, yis)}i∈[Ns] denote the labeled dataset of Ns samples
from the source domain, and the certain label yis belongs to the label space Y that is a finite set
(Y = 1, 2, ..., K). The other dataset XT = {xit}i∈[Nt] has Nt samples from the target domain but
has no labels. We further assume that two domains have different distributions, i.e. Xs 〜DS and
Xt 〜DT. In other words, there exist some domain shift (Ben-David et al., 2010) between DS and
DT . The ultimate goal is to learn a model that can predict the label yit given the target input xit .
3.2	Imbalanced Minimax Game in Domain-Adversarial Training
To achieve domain alignment, Domain-Adversarial Training (DAT) is a minimax game between
a shared feature extractor F for two domains and a domain classifier D. The domain classifier is
2
Under review as a conference paper at ICLR 2020
I 拉"'} I
Source labeled data
Target unlabeled data
Figure 1: The proposed architecture is composed of a shared feature extractor Ge for two domains,
a label predictor Gy and a reconstruction network Gr. In addition to the basic supervised learning
in the source domain, our adversarial reconstruction training enables the extractor Ge to learn
domain-invariant features. Specifically, the network Gr aims to reconstruct the source samples Xs
and to impede the reconstruction of the target samples xt, while the extractor Ge tries to fool the
reconstruction network in order to reconstruct the target samples xt.
trained to determine whether the input sample belongs to the source or the target domain while the
feature extractor learns to deceive the domain classifier, which is formulated as:
min max LDAT(Ds,Dt) = Ex 〜ds [ln F(x)] + Ex 〜Dt[ln(1 - D(F (x)))].	(1)
FD
In DAT, we usually utilize CNN as the feature extractor and fully connected layers (FC) as the
domain classifier. DAT reduces the cross-domain discrepancy, achieving significant performance
improvement for UDA. Nevertheless, the training of DAT is rather unstable. Without sophisticated
tuning of the hyper-parameters, DAT cannot reach the convergence. Through empirical experiments,
we observe that such instability is due to the imbalanced minimax game. The binary domain classifier
D can easily achieve convergence with very high accuracy at an early training epoch, while it is much
harder for the feature extractor F to fool the domain classifier and to simultaneously perform well on
the source domain. In this sense, the domain classifier dominates DAT, and the only solution is to
palliate the training of D by tuning the hyper-parameters according to different tasks. In our method,
we restrict the capacity of the domain classifier so as to form a minimax game in a harmonious
manner. Inspired by the max-margin loss in Support Vector Machine (SVM) (Cristianini et al., 2000)
(i.e. hinge loss), if we push the source domain and the target domain away from a margin rather than
as far as possible, then the training task of F to fool D becomes easier. For a binary domain classifier,
we define the margin loss as
Lmargin (y) = [0,m - t ∙ y「,	(2)
where y is the predicted domain label, [∙]+ := max(0, ∙), m is a positive margin and t is the ground
truth label for two domains (t = -1 for the source domain and t = 1 for the target domain). Then
we introduce our MDAT scheme based on an innovative network architecture.
3.3	Max-margin Domain-Adversarial Training
Besides the training instability issue, DAT also suffers from restrictive feature-level alignment - lack
of pixel-level alignment. To realize stable and comprehensive domain alignment together, we first
propose an Adversarial Reconstruction Network (ARN) and then elaborate MDAT.
As depicted in Figure 1, our model consists of three parts including a shared feature extractor Ge
for both domains, a label predictor Gy and a reconstruction network Gr . Let the feature extractor
Ge(x; θe) be a function parameterized by θe which maps an input sample x to a deep embedding
z. Let the label predictor Gy(z; θy) be a task-specific function parameterized by θy which maps an
embedding Z to a task-specific prediction y. The reconstruction network Gr (z; θr) is a decoding
function parameterized by θr that maps an embedding Z to its corresponding reconstruction x.
The first learning objective for the feature extractor Ge and label predictor Gy is to perform well in the
source domain. For a supervised K-way classification problem, it is simply achieved by minimizing
3
Under review as a conference paper at ICLR 2020
the negative log-likelihood of the ground truth class for each sample:
Ns	Ns
Ltask = X Ly (Xs yS) = - X ys ∙ log Gy (Ge(XS; θe); θy ),	⑶
i=1	i=1
where yis is the one-hot encoding of the class label yis and the logarithm operation is conducted on
the softmax predictions of the model.
The second objective is to render the feature learning to be domain-invariant. This is motivated
by the covariate shift assumption (Shimodaira, 2000) that indicates if the feature distributions
S(Z) = {Ge(x; θe)∣x 〜DS} and T(Z) = {Ge(x; θe)|x 〜DT} are similar, the source label predictor
Gy can achieve a similar high accuracy in the target domain. To this end, we design a decoder network
Gr that serves as a domain classifier, and then MDAT could be applied for stable training. Different
from the normal binary domain classifier, MDAT lets the decoder network Gr only reconstruct the
features in the source domain and push the features in the target domain away from a margin m.
In this way, the decoder has the functionality of distinguishing the source domain from the target
domain. The objective of training Gr is formulated as
Ns+Nt
Ns
Nt
min
θr
margn
(Lr(xi))
=1
mθin	Lr(xis) +	[m - Lr(xtj)]+,
(4)
L
where m is a positive margin and Lr (∙) is the mean squared error (MSE) term for the reconstruction
loss that is defined as
Lr (x) = ||Gr(Ge(x； θe); θr ) - x||2,	(5)
where ∣∣∙∣∣2 denotes the squared L2-norm.
Oppositely, to form a minimax game, the feature extractor Ge learns to deceive Gr such that the
learned target features are indistinguishable to the source ones, which is formulated by:
Nt
mθin X Lr (xtj).
e j=1
Then the whole learning procedure of ARN with MDAT can be formulated by:
(6)
(7)
(8)
where Ly denotes the negative log-likelihood of the ground truth class for labeled sample (xis , yis )
and α controls the interaction of the loss terms. In the following section, we provide theoretical
justifications on how MDAT reduces the distribution discrepancy, and discuss why it is superior to
the classic DAT.
3.4	Theoretical Justifications
In this section, we provide the theoretical justifications on how the proposed method reduces the
distribution discrepancy for UDA. The rationale behind domain alignment is motivated from the
learning theory of non-conservative domain adaptation problem by Ben-David et al. (Ben-David
et al., 2010):
Theorem 3.1 Let H be the hypothesis space where h ∈ H. Let (DS, s) and (DT, t) be the two
domains and their corresponding generalization error functions. The expected error for the target
domain is upper bounded by
Et(A) ≤ Es(A) + 2dH4H(DS, DT) + λ, ∀h ∈ H,	⑼
where "h4h(Ds, DT) = 2suPhι,h2∈H | P"~Ds[hι(x) = h2(x)] - Prχ~Dτ[hι(x) = h2(x)]∣ and
λ = minh[Es(A) +Et(A)].
4
Under review as a conference paper at ICLR 2020
Theoretically, when we minimize the H4H-distance, the upper bound of the expected error for the
target domain is reduced accordingly. As derived in DAT (Ganin & Lempitsky, 2015), assuming a
family of domain classifiers Hd to be rich enough to contain the symmetric difference hypothesis
set of Hp, such that Hp4Hp = {h∣h = hi ㊉ h2, hi, h2 ∈ Hp} where ㊉ is XOR-function, the
empirical Hp4Hp-distance has an upper bound with regard to the optimal domain classifier h:
_	, ʌ	ʌ	,	,一-_,、	r	_	,	、	r	.
dHp4Hp(Ds, DT) ≤ 2 sup | Pr [h(z) = 0]+ Pr [h(z) = 1] - 1|,	(10)
h∈Hd Z 〜DS	Z 〜DT
where DS and DT denote the distributions of the source and target feature space ZS and ZT ,
respectively. Note that the MSE of Gr plus a ceiling function is a form of domain classifier h(z),
i.e. d[m - Lr (∙)]+ - 0.5] for m = 1. It maps source samples to 0 and target samples to 1 which is
exactly the upper bound in Eq.10. Therefore, our reconstruction network Gr maximizes the domain
discrepancy with a margin and the feature extractor learns to minimize it oppositely.
3.5	Discussions
Compared with the conventional DAT-based methods that are usually based on a binary logistic net-
work (Ganin & Lempitsky, 2015), the proposed ARN with MDAT is more attractive and incorporates
new merits conceptually and theoretically:
(1)	Stable training and insensitivity to hyper-parameters. Using the decoder as domain classifier
with a margin loss to restrain its overwhelming capacity in adversarial training, the minimax game
can continuously provide effective gradients for training the feature extractor. Moreover, through
the experiments in Section 4, we discover that our method shows strong robustness to the hyper-
parameters, i.e. α and m, greatly alleviating the parameters tuning for model selection.
(2)	Richer information for comprehensive domain alignment. Rather than DAT that uses a bit
of domain information, MDAT utilizes the reconstruction network as the domain classifier that
could capture more domain-specific and pixel-level features during the unsupervised reconstruction
(Bousmalis et al., 2016). Therefore, MDAT further helps address pixel-level domain shift apart from
the feature-level shift, leading to comprehensive domain alignment in a straightforward manner.
(3)	Feature visualization for method validation. Another key merit of MDAT is that MDAT allows
us to visualize the features directly by the reconstruction network. It is crucial to understand to what
extent the features are aligned since this helps to reveal the underlying mechanism of adversarial
domain adaptation. We will detail the interpretability of these adapted features in Section 4.3.
4	Experiment
In this section, we evaluate the proposed ARN with MDAT on a number of visual and non-visual UDA
tasks with varying degrees of domain shift. We conduct ablation study to corroborate the effectiveness
of MDAT and unsupervised reconstruction for UDA. Then the sensitivity of the hyperparameters is
investigated, and the adapted features are interpreted via the reconstruction network in ARN.
Setup. We evaluate our method on four classic visual UDA datasets and a WiFi-based Gesture
Recognition (WGR) dataset (Zou et al., 2019). The classic datasets have middle level of domain shift
including MNIST (LeCun et al., 1998), USPS (Hull, 1994), Street View House Numbers (SVHN)
(Netzer et al., 2011) and Synthetic Digits (SYN). For a fair comparison, we follow the same CNN
architecture as DANN (Ganin & Lempitsky, 2015) while using the inverse of Ge as Gr with pooling
operation replaced by upsampling. For the penalty term α, we choose 0.02 by searching over the
grid {10-2, 1}. We also obtain the optimal margin m = 5 by a search over {10-i, 10}. Then we use
the same hyperparameter settings for all tasks to show the robustness. For the optimization, we
simply use Adam Optimizer (lr = 2 × 10-4, βi = 0.5, β2 = 0.999) and train all experiments for 50
epochs with batch size 128. We implemented our model and conducted all the experiments using the
PyTorch framework. More implementation details are illustrated in the appendix.
Baselines. We evaluate the efficacy of our approach by comparing it with existing UDA methods that
perform three ways of domain alignment. Specifically, MMD regularization (Long et al., 2015) and
Correlation Alignment (Sun & Saenko, 2016) employ the statistical distribution matching. DRCN
(Ghifary et al., 2016) and DSN (Bousmalis et al., 2016) use the reconstruction error for UDA,
5
Under review as a conference paper at ICLR 2020
Source Target	MNIST USPS	USPS MNIST	SVHN MNIST	SYN SVHN
Source-Only model	78.2	63.4	54.9	86.7
Train on target	96.5	99.4	99.4	91.3
[S] MMD (Long et al., 2015)	81.1	-	71.1	88.0
[S] CORAL (Sun & Saenko, 2016)	80.7	-	63.1	85.2
[R] DRCN* (Ghifary et al., 2016)	91.8	73.7	82.0	87.5
[R] DSN (Bousmalis et al., 2016)	91.3	-	82.7	91.2
[A] DANN (Ganin et al., 2016)	85.1	73.0	74.7	90.3
[A] ADDA (Tzeng et al., 2017)	89.4	90.1	76.0	-
[A] CyCADA (Hoffman et al., 2018)	95.6	96.5	90.4	-
[A] CADA (Zou et al., 2019)	96.4	97.0	90.9	-
[A] MECA (Morerio et al., 2018)	-	-	95.2	90.3
ARN w.o. MDAT	93.1±0.3	76.5±1.2	67.4±0.9	86.8±0.5
ARN with MDAT (proposed)	98.6±0.3	98.4±0.1	97.4±0.3	92.0±0.2
Table 1: We compare with general, statistics-based (S), reconstruction-based (R) and adversarial-
based (A) state-of-the-art approaches. We repeated each experiment for 3 times and report the average
and standard deviation (std) of the test accuracy in the target domain.
while many prevailing UDA methods adopt domain-adversarial training including DANN (Ganin &
Lempitsky, 2015), ADDA (Tzeng et al., 2017), MECA (Morerio et al., 2018), CyCADA (Hoffman
et al., 2018) and CADA (Zou et al., 2019). For all transfer tasks, we follow the same protocol as
DANN (Ganin & Lempitsky, 2015) that uses official training data split in both domains for training
and evaluates the testing data split in the target domain.
4.1	Overall Results
Table 2: Comparisons on WGR.
Source	Room A
Target	Room B
Method	Accuracy (%)
Source-only	58.4±0.7
[S] MMD	61.2±0.5
[R] DRCN	69.3±0.3
[A] DANN	68.2±0.2
[A] ADDA	71.5±0.3
[A] CADA	88.8±0.1
ARN+MDAT	91.3±0.2
MNIST什USPS. Both datasets are composed of grey-scale handwritten images with diverse
stroke weights, leading to low-level domain shift. Since USPS has only 7291 training images,
USPS→MNIST is more difficult. As shown in Table 1, our method achieves state-of-the-art accu-
racy of 98.6% on MNIST→USPS and 98.4% on USPS→MNIST, which demonstrates that ARN
can tackle low-level domain shift by only using ART (rather than many adversarial UDA methods
that adopt other loss terms to adjust classifier boundaries or conduct style transfer).
SVHN→MNIST and SYN→SVHN. The SVHN dataset
contains RGB digit images that introduce significant vari-
ations such as scale, background, embossing, rotation,
slanting and even multiple digits. The SYN data consists
of 50k RGB images of varying color, background, blur and
orientation. These two tasks have tremendous pixel-level
domain shfit. The proposed method achieves a state-of-
the-art performance of 97.4% for SVHN→MNIST, far
ahead of other DAT-based methods, significantly improv-
ing the classic DANN by 22.7%. Similarly, ARN with
MDAT also achieves a noticeable improvement of 5.3%
compared with the source-only model, even outperforming
the supervised SVHN accuracy 91.3%.
WiFi Gesture Recognition with Distant Domains. To
evaluate the proposed method on a non-visual UDA task, we applied our method to the WiFi gesture
recognition dataset (Zou et al., 2019). The WiFi data of six gestures was collected in two rooms
regarded as two domains. The results in Table 2 demonstrate that our approach significantly improves
classification accuracy against Source-Only and DANN by 32.9% and 23.1%, respectively.
6
Under review as a conference paper at ICLR 2020
α	0.01	0.03	0.07	0.1	0.2	0.3	0.5	1.0
DANN	71.1	74.1	72.7	74.1	74.7	9.6	9.7	10.3
ARN (m = 1)	95.7	95.9	93.3	93.2	80.1	75.3	73.1	67.5
m	0.1	0.3	0.5	0.7	1.0	2.0	5.0	10.0
ARN (α = 2e-2)	64.5	75.2	90.0	92.6	96.0	97.4	97.7	96.7
Table 3: The accuracy (%) with different hyperparameters on SVHN→MNIST.
4.2	Ablation Study and Sensitivity Analysis
The contribution of MDAT and image reconstruction in ARN. We design an ablation study to
verify the contribution of MDAT and unsupervised reconstruction in ARN. To this end, we discard the
term Lr(xt) in Eq.4, and evaluate the method, denoted as ARN w.o. MDAT in Table 1. (1) Comparing
ARN w.o. MDAT with source-only model, we can infer the effect of unsupervised reconstruction
for UDA. It is observed that ARN w.o. MDAT improves tasks with low-level domain shift such
as MNIST-USPS, which conforms with our discussion that the unsupervised reconstruction is
instrumental in learning low-level features. (2) Comparing ARN w.o. MDAT with the original ARN,
we can infer the contribution of MDAT. Table 1 shows that the MDAT achieves an impressive margin-
of-improvement. For USPS→MNIST and SVHN→MNIST, the MDAT improves ARN w.o. MDAT
by around 30%. It demonstrates that MDAT which helps learn domain-invariant representations is
the main reason for the tremendous improvement.
Parameter sensitivity. We investigate the effect of α and m on SVHN→MNIST. The results in
Table 3 show that ARN achieves good performance as α ∈ [0.01, 0.1] and even with larger α ARN is
able to achieve convergence. In comparison, denoting α as the weight of adversarial loss, the DANN
cannot converge when α > 0.2. For the sensitivity of m, the accuracy of ARN exceeds 96.0% as
m ≥ 1. These analyses validate that the training of ARN is not sensitive to the parameters and even
in the worst cases ARN can achieve convergence.
Gradients and training procedure. We draw the training procedure with regard to loss and target
accuracy in Figure 2(b) and Figure 2(a), respectively. In Figure 2(b), ARN has smoother and more
effective gradients (Lr) for all α, while the loss of DAT domain classifier (Ld) gets extremely
small at the beginning. This observation conforms with our intuition, which demonstrates that by
restricting the capacity of domain classifier MDAT provides more effective gradients for training
feature extractor, leading to a more stable training procedure. This could be further validated in
Figure 2(b) where the ARN accuracy is more stable than that of DAT across training epochs.
B
1.2-
1.0-
0.8-
0.6-
0.4-
0.2-
0.0-
—— ARNCr (α = 0.05)
——ARN Cr (α = 0.1)
——ARN rr(α= ι.o)
DAT Cd (α= 0.02)
——DATΛd(α=0.1)
——ARNCe (α= 0.02)
——ARN re(α= 0.05)
——ARNCeg=O1)
—— ARN re(σ= 1.0)
——ARNCr (α= 0.02)
0	10	20	30	40	50
Epoch
100
80
60
40
20
0
—— ARN (α = 0.02)
——ARN (a = 0.05)
——ARN (a = 0.1)
——ARN (ɑ =1.0)
DAT (a =0.02)
DAT(ff=0.1)
——DAT(σ= 1.0)
0	10	20	30	40	50
Epoch
(a) Convergence
(b) Test Accuracy
Figure 2: The training procedure with regard to loss and test accuracy. (Le := Eq. 6; Lr := Eq. 4; Ld
is the domain loss of DAT (Ganin & Lempitsky, 2015); α is the penalty term of Le and Ld.)
7
Under review as a conference paper at ICLR 2020
Source Images ∣ Target Images ∣ R-Target Images
MNIST→USPS
USPS→MNIST
0 ( t 7 4 ð D 9
2.S^9 9Λ9 I
Δ5V I » 3 ð S
SVHN→MNIST
“ © ∙? 4 0 1 3 ∖
SYN→SVHN
04181328
6 I » 9 7□ I O
loot?，
0 6 1-13 Z ?
C	I	??	7	S	I	O
I	0	2 /■	O ∖	,	S7
71 I 0 M I y I
” s q，6 “ $
34727 1”
√7 o & ? o I 5∣∣ .!V∣.ηur(∣∣9nr∣
I	11
L ”…∖，U「/	…，T - I
E >≡H tt '∣∣□4
ɪ'*	SI Il 3
Γ1¾ΛH ©n”；口 二二《
7 乙 /。/ 7 i∖
q	7 +	T	7	6	3，
3	u 7	2	Q	1	N	1
7	乙	/	0	4	/	⅛∙	S
<7	7	¾	√	7	6	3	9
3	4	7	N	7	I	3	f
Table 4: Visualizing the source image, target images and reconstructed target images (R-Target
Images) for four digit adaptation tasks.
4.3 Visualization and Analysis
Interpreting MDAT features via reconstructed images. One of the key advantages of ARN is that
by visualizing the reconstructed target images we can infer how the features are domain-invariant.
We reconstruct the MDAT features of the test data and visualize them in Table 4. It is observed that
the target features are reconstructed to source-like images by the decoder Gr . As discussed before,
intuitively, MDAT forces the target features to mimic the source features, which conforms with our
visualization. Similar to image-to-image translation, this indicates that our method conducts implicit
feature-to-feature translation that transfers the target features to source-like features, and hence the
features become domain-invariant.
T-SNE embeddings. We analyze the performance of domain alignment for DANN (DAT) (Ganin
& LemPitsky, 2015) and ArN (MdAt) by plotting T-SNE embeddings of the features Z on the
task SVHN→MNIST. In Figure 3(a), the source-only model obtains diverse embeddings for each
category but the domains are not aligned. In Figure 3(b), the DANN aligns two domains but the
decision boundaries Of the classifier are vague. In Figure 3(c), the proposed ARN effectively aligns
two domains for all categories and the classifier boundaries are much clearer.
(a) Source-only Model
(b) DANN
(c) ARN
Figure 3: T-SNE visualization on SVHN→MNIST with their corresponding domain labels (red: tar-
get; blue: source) and category labels (10 classes) shown in the left and right subfigures, respectively.
5 Conclusion
We proposed a new domain alignment approach namely max-margin domain-adversarial training
(MDAT) and a MDAT-based network for unsupervised domain adaptation. The proposed method
offers effective and stable gradients for the feature learning via an adversarial game between the
feature extractor and the reconstruction network. The theoretical analysis provides justifications on
how it minimizes the distribution discrepancy. Extensive experiments demonstrate the effectiveness
of our method and we further interpret the features by visualization that conforms with our insight.
Potential evaluation on semi-supervised learning constitutes our future work.
8
Under review as a conference paper at ICLR 2020
References
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman
Vaughan. A theory of learning from different domains. Machine learning, 79(1-2):151-175, 2010.
Konstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip Krishnan, and Dumitru Erhan.
Domain separation networks. In Advances in Neural Information Processing Systems, pp. 343-351,
2016.
Konstantinos Bousmalis, Nathan Silberman, David Dohan, Dumitru Erhan, and Dilip Krishnan.
Unsupervised pixel-level domain adaptation with generative adversarial networks. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3722-3731, 2017.
Yuhua Chen, Wen Li, Christos Sakaridis, Dengxin Dai, and Luc Van Gool. Domain adaptive faster
r-cnn for object detection in the wild. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 3339-3348, 2018.
Nello Cristianini, John Shawe-Taylor, et al. An introduction to support vector machines and other
kernel-based learning methods. Cambridge university press, 2000.
Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In
Francis Bach and David Blei (eds.), Proceedings of the 32nd International Conference on Machine
Learning, volume 37 of Proceedings of Machine Learning Research, pp. 1180-1189, Lille, France,
07-09 Jul 2015. PMLR.
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, FrancOiS
Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks.
The Journal of Machine Learning Research, 17(1):2096-2030, 2016.
Muhammad Ghifary, W Bastiaan Kleijn, Mengjie Zhang, David Balduzzi, and Wen Li. Deep
reconstruction-classification networks for unsupervised domain adaptation. In European Confer-
ence on Computer Vision, pp. 597-613. Springer, 2016.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Domain adaptation for large-scale sentiment
classification: A deep learning approach. In Proceedings of the 28th International Conference on
Machine Learning (ICML-11), pp. 513-520, 2011.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural
Information Processing Systems, pp. 2672-2680, 2014.
Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei Efros, and
Trevor Darrell. CyCADA: Cycle-consistent adversarial domain adaptation. In Proceedings of the
35th International Conference on Machine Learning, pp. 1989-1998, 2018.
Jonathan J. Hull. A database for handwritten text recognition research. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 16(5):550-554, 1994.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436, 2015.
Mingsheng Long, Yue Cao, Jianmin Wang, and Michael I. Jordan. Learning transferable features with
deep adaptation networks. In Proceedings of the 32Nd International Conference on International
Conference on Machine Learning - Volume 37, ICML’15, pp. 97-105, 2015.
Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I Jordan. Conditional adversarial
domain adaptation. In Advances in Neural Information Processing Systems, 2018.
Pietro Morerio, Jacopo Cavazza, and Vittorio Murino. Minimal-entropy correlation alignment for
unsupervised deep domain adaptation. In International Conference on Learning Representations,
2018. URL https://openreview.net/forum?id=rJWechg0Z.
9
Under review as a conference paper at ICLR 2020
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning
and Unsupervised Feature Learning, volume 2011, pp. 5, 2011.
Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the log-
likelihood function. Journal of statistical planning and inference, 90(2):227-244, 2000.
Rui Shu, Hung H Bui, Hirokazu Narui, and Stefano Ermon. A dirt-t approach to unsupervised domain
adaptation. In Proc. 6th International Conference on Learning Representations, 2018.
Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In
European Conference on Computer Vision, pp. 443-450. Springer, 2016.
Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, and Trevor Darrell. Deep domain confusion:
Maximizing for domain invariance. CoRR, abs/1412.3474, 2014. URL http://arxiv.org/
abs/1412.3474.
Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain
adaptation. In Computer Vision and Pattern Recognition (CVPR), volume 1, pp. 4, 2017.
David Vazquez, Antonio M Lopez, Javier Marin, Daniel Ponsa, and David Geronimo. Virtual and real
world adaptation for pedestrian detection. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 36(4):797-809, 2013.
Han Zou, Yuxun Zhou, Jianfei Yang, Huihan Liu, Hari Prasanna Das, and Costas J Spanos. Consensus
adversarial domain adaptation. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 33, pp. 5997-6004, 2019.
10
Under review as a conference paper at ICLR 2020
Appendix
Implementation Details
Hyperparameter For all tasks, we simply use the same hyperparameters that are chosen from the
sensitivity analysis. We use α = 0.02 and m = 5.0, and we reckon that better results can be obtained
by tuning the hyperparameters for specific tasks.
Network Architecture For a fair comparison, we follow the network in DANN (Ganin & Lempitsky,
2015) for digit adaptation and simply build the reconstruction network by the inverse network of the
extractor. Here we draw the network architectures in Table 5. For WiFi gesture recognition, we adopt
the same architecture as CADA (Zou et al., 2019) that is a modified version of LeNet-5.
Layer Index ∣	Feature Extractor	∣	Decoder Network ∣	Label Predictor
0	I	32	X 32 X 3 Image	
1	I	5 X 5 conv. 64 ReLU ∣	2048 dense, ReLU ∣	10 dense, softmax
2 I	3 X 3 max-pool, stride 2 ∣	3072 dense, ReLU ∣	
3 I	5 x 5 conv. 64 ReLU ∣	5 x 5 conv. 128 ReLU ∣	
4 I	3 X 3 max-pool, stride 2 ∣	upsample 2	∣	
5	I	5 x 5 conv. 128 ReLU ∣	5 x 5 conv. 64 ReLU ∣	
6 I	3072 dense, dropout, ReLU ∣	upsample 2	∣	
7	I	2048 dense, dropout ReLU ∣	5 x 5 conv. 64 ReLU ∣	
Table 5: The network architecture used in the experiments.
Sensitivity
We have presented all the results of the sensitivity study in Section 4.2, and now we show their
detailed training procedures in Figure 4(a) and 4(b). It is observed that the accuracy increases when
α drops or the margin m increases. The reason is very simple: (1) when α is too large, it affects
the effect of supervised training on source domain; (2) when the margin m is small, the divergence
between source and target domain (i.e. H4H-distance) cannot be measured well.
Ooooo
0 8 6 4 2
(东)>U23U<36JeJ.
——ARN (α = 0.01)
——ARN (σ = 0.02)
ARN (α = 0.03)
ARN (α=0.05)
ARN (α=0.07)
ARN (α = 0.1)
ARN (α = 0.2)
——ARN (σ=1.0)
0-.
0
10	20	30	40	50
Epoch
(a)	Loss penalty α
l0,0Q 。 O
0 8 6 4 2
1
(东)>U23U<36JeJ.
10	20	30	40	50
Epoch
(b)	Margin m
0-.
0
Figure 4: The training procedure of ARN with different hyper-parameters.
11
Under review as a conference paper at ICLR 2020
Visualization
Here we provide more visualization of the reconstructed images of target samples. In Figure 5,
the target samples are shown in the left column while their corresponding reconstructed samples
are shown in the right. We can see that for low-level domain shift such as MNIST-USPS, the
reconstructed target samples are very source-like while preserving their original shapes and skeletons.
However, for larger domain shift in Figure 5(c) and 5(d), they are reconstructed to source-like same
digits but simultaneously some noises are removed. Specifically, in Figure 5(d), we can see that
one target sample (SVHN) may contain more than one digits that are noises for recognition. After
reconstruction, only the right digits are reconstructed. Some target samples may suffer from terrible
illumination conditions but their reconstructed digits are very clear, which is amazing.
0il8l328(^<7 7O7)a
4 I ^97□IOOIOIi3 30
I α I VO I 9» t I a 1 7 I I 7
©317^0? ɔθv7>β702
4IO766i?y7l
0(t746O9>* 30 7 I 0 6
ZS4 9 9Λ 2 I O( I OO ð 4
Δ5M I >30C8∙7367 S⅛3
0 6
I I
I 0
0 3
C I
0 I
乙S
?
2
1
O
?
6
q
J(】— O GOa 3
l 70J6093
O
O
/
4
ɛ
& q ?
I 0 1
I Z i
“ 7 J
/ (，
» 3 O
O i【
7 5 0
O /
Z ɔ
7 I
? 7
Z 5
1 \
O O
Q
t?l。U。。2
Z
q
7
?
O
?
0
2
O
?
工
O
7
ɔ
G
(a)	MNIST→USPS
7
q
3
夕
7
rl
'.
5
1/ 0
^7+T
U 7 2
4 6 ʒ
4 S √
Q A ]
+ CQ
q ? 4
q
夕
7
4
3
g
fo
y
/ Li ∖
6 5 5
I V f
⅛ © 0
O-7 O
H 73
OSM
y q a
Oo 3 ɔ 3 Λl V
fo4√-1孑 6 / 6
0 I
1 3
5 ∖
W q
2. f
3 1
4 8
Q O
7XI0<∕∣y∣ς⅞O67OI5
975<ff6^S4O790l5l
347a7 I 1 I I 7135 IA
*∕W4355G0VI^T7ifi
7 1 (∕∙4 307 0 ɔ，I 7 3d，7
14A7fM754>l34TSI4
ITC960SM99S( 9 « T 7
3 9 7? Y V T 7 S I 7 6 T T OS
?o7 /夕, q。
<rr l√□Q G 5
D
Q
q
51上9 7Q7<
(b)	USPS→MNIST
7
q
,
7
rl
∖
5
乙/
7 ⅛
U 7
q 6
Q 2
o q
√夕
7 7
ʒ 4
√ 3
T g
Q fo
4 ⅛
S5loo3q-么
γs∖ω"775q
/6 —5 0w∙OM
‹ 7
H I
□ ?
Q i
c∣勺
s夕
o
n
q
fc q
q o
4 3
rɔ
孑3
Q /
/ q
6 r
O 1
1 3
5 ∖
,q
2 f
3 I
4 g
q o
5
∖
B
7
，” —” ▼ ―- 3 ∙ y ■■一 ，二■ . 〜 ・=y，， »---*■
Bn01U国咄比6Il北网。；IHK)HIm
rιιιr Jrniiiii11：ur∣rs∣.irinnn.：
wm0^iq∣5 ■ 11 ιrθιuι 11刈二出》川1”
□κι>ι∣□∣3∣nmιι 21run∣R∣'∣. r∣r∣
tRCIl2ir∣βlD7∏Ghll3l∣tlΠI3ll 11141
1 ,)i[ IiiWiiI ir-iniidrir 1 3r inn ιιt8∣∣7
(c) SVHN→MNIST
(d) SYN→SVHN
Figure 5: Visualization of the target samples and their corresponding reconstructed target samples.
12