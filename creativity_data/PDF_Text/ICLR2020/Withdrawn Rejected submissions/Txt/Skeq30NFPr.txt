Under review as a conference paper at ICLR 2020
Stochastic Mirror Descent on Overparame-
terized Nonlinear Models
Anonymous authors
Paper under double-blind review
Ab stract
Most modern learning problems are highly overparameterized, meaning that the
model has many more parameters than the number of training data points, and as
a result, the training loss may have infinitely many global minima (in fact, a man-
ifold of parameter vectors that perfectly interpolates the training data). Therefore,
it is important to understand which interpolating solutions we converge to, how
they depend on the initialization point and the learning algorithm, and whether
they lead to different generalization performances. In this paper, we study these
questions for the family of stochastic mirror descent (SMD) algorithms, of which
the popular stochastic gradient descent (SGD) is a special case. Recently it has
been shown that, for overparameterized linear models, SMD converges to the
global minimum that is “closest” (in terms of the Bregman divergence of the mir-
ror used) to the initialization point, a phenomenon referred to as implicit regular-
ization. Our contributions in this paper are both theoretical and experimental. On
the theory side, we show that in the overparameterized nonlinear setting, if the
initialization is close enough to the manifold of global optima, SMD with suffi-
ciently small step size converges to a global minimum that is approximately the
closest global minimum in Bregman divergence, thus attaining approximate im-
plicit regularization. For highly overparametrized models, this closeness comes
for free: the manifold of global optima is so high dimensional that with high
probability an arbitrarily chosen initialization will be close to the manifold. On
the experimental side, our extensive experiments on the MNIST and CIFAR-10
datasets, using various initializations, various mirror descents, and various Breg-
man divergences, consistently confirms that this phenomenon indeed happens in
deep learning: SMD converges to the closest global optimum to the initializa-
tion point in the Bregman divergence of the mirror used. Our experiments further
indicate that there is a clear difference in the generalization performance of the
solutions obtained from different SMD algorithms. Experimenting on the CIFAR-
10 dataset with different regularizers, `1 to encourage sparsity, `2 (yielding SGD)
to encourage small Euclidean norm, and `10 to discourage large components in
the parameter vector, consistently and definitively shows that, for small initial-
ization vectors, '10-SMD has better generalization performance than SGD, which
in turn has better generalization performance than `1 -SMD. This surprising, and
perhaps counter-intuitive, result strongly suggests the importance ofa comprehen-
sive study of the role of regularization, and the choice of the best regularizer, to
improve the generalization performance of deep networks.
1	Introduction
Deep learning has demonstrably enjoyed a great deal of success in a wide variety of tasks (Amodei
et al., 2016; Graves et al., 2013; Krizhevsky et al., 2012; Mnih et al., 2015; Silver et al., 2016; Wu
et al., 2016; LeCun et al., 2015). Despite its tremendous success, the reasons behind the good perfor-
mance of these methods on unseen data is not fully understood (and, arguably, remains somewhat of
a mystery). While the special deep architecture of these models seems to be important to the success
of deep learning, the architecture is only part of the story, and it has been now widely recognized that
the optimization algorithms used to train these models, typically stochastic gradient descent (SGD)
and its variants, play a key role in learning parameters that generalize well.
1
Under review as a conference paper at ICLR 2020
Since these deep models are highly overparameterized, they have a lot of capacity, and can fit to
virtually any (even random) set of data points (Zhang et al., 2016). In other words, these highly
overparameterized models can “interpolate” the training data, so much so that this regime has been
called the “interpolating regime” (Ma et al., 2018b). In fact, on a given dataset, the loss function
typically has (infinitely) many global minima, which however can have drastically different general-
ization properties (many of them perform very poorly on the test set). Which minimum among all the
possible minima we converge to in practice is determined by the initialization and the optimization
algorithm that we use for training the model.
Since the loss functions of deep neural networks are non-convex—sometimes even non-smooth—in
theory, one may expect the optimization algorithms to get stuck in local minima or saddle points.
In practice, however, such simple stochastic descent algorithms almost always reach zero training
error, i.e., a global minimum of the training loss (Zhang et al., 2016; Lee et al., 2016). More
remarkably, even in the absence of any explicit regularization, dropout, or early stopping (Zhang
et al., 2016), the global minima obtained by these algorithms seem to generalize quite well (contrary
to some other global minima). It has been also observed that even among different optimization
algorithms, i.e., SGD and its variants, there is a discrepancy in the solutions achieved by different
algorithms and how they generalize (Wilson et al., 2017). Therefore, it is important to ask
Which global minima do these algorithms converge to? And what properties do they have?
In this paper, we answer this question for the SGD algorithm, and more generally, for the family of
stochastic mirror descent (SMD) algorithms, which includes SGD as a special case. For any choice
of potential function, there is a corresponding mirror descent algorithm. We show that, for overpa-
rameterized nonlinear models, if one initializes close enough to the manifold of parameter vectors
that interpolates the data, then the SMD algorithm for any particular potential converges to a global
minimum that is approximately the closest one to the initialization, in Bregman divergence corre-
sponding to the potential. In highly overparameterized models, this closeness of the initialization
comes for free, something that is occasionally referred to as “the blessing of dimensionality.” For
the special case of SGD, this means that it converges to a global minimum which is approximately
the closest one to the initialization in the usual Euclidean sense.
We perform extensive systematic experiments with various initial points and various mirror descent
algorithms for the MNIST and CIFAR-10 datasets using standard off-the-shelf deep neural network
architectures for these datasets with standard random initialization, and we measure all the resulting
pairwise Bregman divergences. We found that every single result is exactly consistent with the above
theory. Indeed, in all our experiments, the global minimum achieved by any particular mirror
descent algorithm is the closest, among all other global minima obtained by other mirrors and
other initializations, to its initialization in the corresponding Bregman divergence. In particular,
the global minimum obtained by SGD from any particular initialization is closest to the initialization
in Euclidean sense, both among the global minima obtained by different mirrors and among the
global minima obtained by different initializations.
This result, proven theoretically and backed up by extensive experiments, further implies that, even
in the absence of any explicit regularization, these algorithms perform an implicit regularization. In
particular, it implies that, when initialized around zero, SGD acts as an `2 regularizer. Similarly,
by choosing other mirrors, one obtains different forms of implicit regularization (such as '1 or '∞),
which may have different performances on test data. This raises the question
How well do different mirrors perform in practice?
Perhaps, one might expect an `1 regularizer to perform better, due to the fact that it promotes sparsity,
and “pruning” in neural networks is believed to be helpful for generalization. On the other hand, one
may expect SGD (`2 regularizer) to work best among different mirrors, because typical architectures
have been tuned for and tailored to SGD. We run experiments with four different mirror descents,
i.e., '1 (sparse), '2 (SGD), '3, and '10 (as a surrogate for '∞), on a standard off-the-shelf deep
neural network architecture for CIFAR-10, namely ResNet-18. Somewhat counter-intuitively, our
results for test errors of different mirrors consistently and definitively show that the '10 regularizer
performs better than the other mirrors including SGD, while '1 consistently performs worse.
This flies in the face of the conventional wisdom that sparser weights (which are obtained by an
'1 regularizer) generalize better, and suggests that '∞, which roughly speaking penalizes all the
weights uniformly, may be a better regularizer for deep neural networks.
2
Under review as a conference paper at ICLR 2020
1.1	Related Work
There have been many efforts in the past few years to study deep learning from an optimization
perspective, e.g., (Achille & Soatto, 2017; Chaudhari & Soatto, 2018; Shwartz-Ziv & Tishby, 2017;
Allen-Zhu et al., 2019; Oymak & Soltanolkotabi, 2019; Azizan & Hassibi, 2019; Ma et al., 2018b;
Du et al., 2018; Li & Liang, 2018; Cao & Gu, 2019). While it is not possible to review all the
contributions here, we comment on the ones that are most closely related to ours and highlight the
distinctions between our results and those.
Many recent papers have studied the convergence of the (S)GD algorithm in the so-called “over-
parameterized” setting (or “interpolating” regime), which is common in deep learning (Oymak &
Soltanolkotabi, 2019; Allen-Zhu et al., 2019; Soltanolkotabi et al., 2017; Ma et al., 2018b). All these
works, similar to ours, assume that the initialization is close to the solution space (of global minima),
which is a reasonable assumption in highly overparameterized models, as we argue in Section A.4
of the supplementary material. However, our results are more general because they extend to SMD.
Furthermore, even for the case of SGD, our results are stronger than those in the literature, in the
sense that not only do we show convergence to a global minimum, but we also show that the weight
vector we converge to, w∞, say, is close to the interpolating weight vector closest to the initialization,
w*, say. Denoting the initialization by wo, Oymak & Soltanolkotabi (2019) showed that for SGD,
k w∞ 一 wo k is bounded by a constant factor of k w* - wo k. Our Theorem 4 shows the much stronger
statement that ∣∣w∞ 一 wok = ∣∣w* 一 wok + o(∣∣w* — wok). We further show that w∞ and w* are
very close to one another, viz. kw∞ 一 w* k2 = o(kw* 一 wok)), something that could not be inferred
from the previous work.
There exist a number of results that characterize the implicit regularization properties of different
algorithms in different contexts (Neyshabur et al., 2017; Ma et al., 2018a; Gunasekar et al., 2017;
2018a; Soudry et al., 2017; Gunasekar et al., 2018b; Azizan & Hassibi, 2019; Mianjy et al., 2018).
The closest ones to our results, since they concern mirror descent, are the works of (Gunasekar et al.,
2018a; Azizan & Hassibi, 2019). The authors in (Gunasekar et al., 2018a) consider linear overpa-
rameterized models, and show that if SMD happens to converge to a global minimum, then that
global minimum will be the one that is closest in Bregman divergence to the initialization, a result
they obtain by examining the KKT conditions. However, they do not provide any conditions for con-
vergence and whether SMD converges with a fixed step size or not. Azizan & Hassibi (2019) also
study linear models, but derive conditions on the step size for which SMD converges to the afore-
mentioned global minimum. Our results extend the aforementioned to nonlinear overparametrized
models, and show that, for small enough fixed step size, and for initializations close enough to the
space of interpolating solutions, SMD converges to a global minimum, something which had not
been shown in any of the previous work. Assuming every data point is revisited often enough, the
convergence we establish is deterministic. Finally, we show that the solution we converge to exhibits
approximate implicit regularization, something that was not not known for nonlinear models.
The remander of the paper is organized as follows. In Section 2, we review the family of mirror
descent algorithms and briefly revisit the linear overparameterized case. Section 3 provides our
main theoretical results, which are (1) convergence of SMD to a global minimum, and (2) proximity
of the obtained global minimum to the closest one from the initialization in Bregman divergence.
Our proofs are remarkably simple and are based on a powerful fundamental identity that holds for
all SMD algorithms in a general setting. In Section 4, we provide our experimental results, which
consists of two parts, (1) testing the theoretical claims about the distances for different mirrors and
different initializations, and (2) assessing the generalization properties of different mirrors. The
proofs of the theoretical results and more details on the experiments are relegated to the supplemen-
tary material.
2	Background and Warm-Up
Let us denote the training dataset by {(xi, yi) : i = 1, . . . , n}, where xi ∈ Rd are the inputs, and
yi ∈ R are the labels. The model (which can be, e.g., linear, a deep neural network, etc.) is defined
by the general function f(xi, w) = fi(w) with some parameter vector w ∈ Rp. Since typical deep
models have a lot of capacity and are highly overparameterized, we are particularly interested in the
overparameterized (or so-called interpolating) regime, where p > n (often p n). In this case,
3
Under review as a conference paper at ICLR 2020
there are many parameter vectors w that are consistent with the training data points. We denote the
set of these parameter vectors by
W = {w ∈ Rp | f (xi, w) = yi,i = 1, . . . ,n}	(1)
This a high-dimensional set (e.g. a (p - n)-dimensional manifold) in Rp and depends only on the
training data {(xi, yi) : i = 1,...,n} and the model f (∙, ∙).
The total loss on the training set (empirical risk) can be expressed as L(w) = Pin=1 Li(w),
where Li(∙) = '(y%, f (xi, W)) is the loss on the individual data point i, and '(∙, ∙) is a differen-
tiable non-negative function, with the property that '(yi, f (xi, W)) = 0 iff yi = f (xi, w). Often
'(yi, f (Xi, W)) = '(yi - f (xi, w)), with '(∙) convex and having a global minimum at zero (such as
square loss, Huber loss, etc.). In this case, L(W) = PZi '(yi - f (xi, w)).
W is the set of global minima, and every parameter vector W in W renders the loss on each data
point zero, i.e., Li(W) = 0 ∀i. The loss function is often attempted to be minimized by stochastic
gradient descent (SGD):
Wi = Wi-1 - η^Li(wi-i), i ≥ 1	⑵
assuming the data is indexed randomly. We use one index i for both the loss and the parameter
vector at step i (for i > n, one can cycle through the data, or select them at random, etc.).
2.1	Stochastic Mirror Descent
Mirror descent, first introduced by Nemirovski & Yudin (1983), is one of the most widely used
families of algorithms for optimization (Beck & Teboulle, 2003; Cesa-Bianchi et al., 2012; Zhou
et al., 2017), which includes the popular gradient descent as a special case. Consider a strictly
convex differentiable function ψ(∙), called the potentialfunction. Then updates for stochastic mirror
descent (SMD) are defined as
Vψ(Wi) = Vψ(Wi-ι) - η^Li(Wi-ι).	(3)
Note that, due to the strict convexity of ψ(∙), the gradient Vψ(∙) defines an invertible map, so
the recursion in (3) yields a unique Wi at each iteration, and thus is a well-defined update, i.e.,
Wi = Vψ-1 (Vψ(Wi-1) - ηVLi(Wi-1)) . Compared to classical SGD, rather than update the
weight vector along the direction of the negative gradient, the update is done in the “mirrored”
domain determined by the invertible transformation Vψ(∙). Mirror descent was originally conceived
to exploit the geometrical structure of the problem by choosing an appropriate potential. Note that
SMD reduces to SGD when Ψ(w) = 2∣∣w∣∣2, since the gradient Vψ(∙) is simply the identity map.
Alternatively, the update rule (3) can be expressed as
Wi = arg min ηWTVLi(Wi-1) + Dψ(W, Wi-1),	(4)
w
where
Dψ(W, Wi-1) := ψ(W) - ψ(Wi-1) - Vψ(Wi-1)T (W - Wi-1)	(5)
is the Bregman divergence with respect to the potential function ψ(∙). Note that Dψ(∙, ∙) is non-
negative, convex in its first argument, and that, due to strict convexity, Dψ(W, W0) = 0 iffW = W0.
Different choices of the potential function ψ(∙) yield different optimization algorithms, which will
potentially have different implicit biases. A few examples follow.
Gradient Descent. For the potential function Ψ(w) = 2∣∣w∣∣2, the Bregman divergence is
Dψ(w, w0) = 2∣∣w - W0k2, and the update rule reduces to that of SGD.
Exponentiated Gradient Descent. For ψ(W) = j Wj logWj, the Bregman divergence becomes
the unnormalized relative entropy (Kullback-Leibler divergence) Dψ (w,w0) = Pj Wjlog W -
Pj Wj + Pj Wj0, which corresponds to the exponentiated gradient descent (aka the exponential
weights) algorithm (Kivinen & Warmuth, 1997).
p-norms Algorithm. For any q-norm squared potential function Ψ(w) = ɪ ∣∣wk2, with P + 1 = 1,
the algorithm will reduce to the so-called p-norms algorithm (Grove et al., 2001; Gentile, 2003).
Sparse Mirror Descent. For ψ(W) = ∣W∣11++, the algorithm reduces to sparse mirror descent,
which is used in compressed sensing (Azizan & Hassibi, 2019).
4
Under review as a conference paper at ICLR 2020
2.2	Overparameterized Linear Models
Overparameterized (or underdetermined) linear models have been recently studied in many pa-
pers due to their simplicity, and the fact that there are interesting insights than one can obtain
from them. In this case, the model is f (xi , w) = xiT w, the set of global minima is W =
{w | yi = xTw, i = 1,...,n}, and the loss is Li(W) = '(yi 一 xTw). The following result charac-
terizes the solution that SMD converges to (Azizan & Hassibi, 2019; Gunasekar et al., 2018a).
Proposition 1. Consider a linear overparameterized model. For sufficiently small step size, i.e.,
for any η > 0 for which ψ(∙) 一 ηLi(∙) is convex, and for any initialization wo, the SMD iterates
converge to
w∞ = arg min Dψ (w, w0).
w∈W
Note that the step size condition, i.e., the convexity of ψ(∙) 一 ηLi(∙), depends on both the loss and
the potential function. For the case of SGD, ψ(w) = 2 ∣∣w∣∣2, and '{yi 一 XTW) = 2 (yi — XTw)2,so
the condition reduces to the well-known η ≤ ∣χ1ρ. In this case, Dψ (w,wo) is simply 2 ∣∣w — wo k2.
Corollary 2. In particular, for the initialization w0 = arg minw∈Rp ψ(w), under the conditions of
Proposition 1, the SMD iterates converge to
w∞ = arg min ψ(w).	(6)
w∈W
This means that running SMD for a linear model with the aforementioned wo, without any explicit
regularization, results in a solution that has the smallest potential ψ(∙) among all solutions, i.e., SMD
implicitly regularizes the solution with ψ(∙). In particular, this means that SGD initialized around
zero acts an an '2-norm regularizer. In this paper, We show that these results continue to hold for
highly overparameterized nonlinear models in an approximate sense.
3	Theoretical Results
In this section, we provide our main theoretical results. In particular, we show that for highly over-
parameterized models (1) SMD converges to a global minimum, (2) the global minimum obtained
by SMD is approximately the closest one to the initialization in Bregman divergence corresponding
to the potential.
Figure 1: An illustration of the parameter space. W represents the set of global minima, wo is the
initialization, B is the local neighborhood, w* and the closest global minimum to w° (in Bregman
divergence), and w∞ is the minimum that SMD converges to.
3.1	Convergence of Stochastic Mirror Descent
Let us define
DLi (w,w0) := Li(W)- LiM)- VLi(WO)T(w — w0),	(7)
which is defined in a similar way to a Bregman divergence for the loss function. The difference
though is that, unlike the potential function of the Bregman divergence, the loss function Li(∙)=
'(yi — f (xi, ∙)) need not be convex (even when '(∙) is) due to the nonlinearity of f (∙, ∙).
5
Under review as a conference paper at ICLR 2020
It has been argued in several recent papers that in highly overparameterized neural networks, any ran-
dom initialization w0 is close to W, with high probability (Li & Liang, 2018; Du et al., 2018; Azizan
& Hassibi, 2019; Allen-Zhu et al., 2019; Cao & Gu, 2019) (see also the discussion in Section A.4
of the supplementary material). Therefore, it is reasonable to make the following assumption about
the initialization.
Assumption 1. Denote the initial point by w0. There exists w ∈ W and a region B = {w0 ∈
Rp | Dψ (w, w0) ≤ } containing w0, such that DLi (w, w0) ≥ 0, i = 1, . . . , n, for all w0 ∈ B.
It is important to understand What this assumption means. Since Li(∙) is not necessarily convex, it
is certainly not the case that DLi(w, w0) ≥ 0 for all w0. However, since W is a minimizer of Li(∙),
there Will be a neighborhood around it such that for all w0 in this neighborhood DLi (w, w0) ≥ 0 (see
Fig. 2 for an illustration). What we are requiring is that the initialization w0 be inside the intersection
of all such neighborhoods for i = 1, 2, . . . , n. In other words, we require a w0 close enough to W.
Figure 2: An illustration of DLi (w, w0) ≥ 0 in a local region in Assumption 1.
Our second assumption states that in this local region, the first and second derivatives of the model
are bounded.
Assumption 2. Consider the region B in Assumption 1. fi(∙) have bounded gradient and Hessian
on the convex hull of B, i.e., kVfi(w0 )k ≤ γ, and α ≤ λmin(Hfi(w')) ≤ λmaχ(Hfi(w')) ≤ β,i =
1, . . . , n, for all w 0 ∈ conv B.
This is again a mild assumption, which is assumed in other related work such as (Oymak &
Soltanolkotabi, 2019) as well. Note that we do not require α to be positive (just its boundedness).
The following theorem states that under Assumption 1, SMD converges to a global minimum.
Theorem 3.	Consider the set of interpolating parameters W = {w ∈ Rp | f(xi, w) = yi, i =
1, . . . , n}, and the SMD iterates given in (3), where every data point is revisited after some steps.
Under Assumption 1, for sufficiently small SteP size, i.e., for any η > 0 for which ψ(∙) 一 ηLi(∙) is
strictly convex on B for all i, the following holds.
1.	All the iterates {wi } remain in B.
2.	The iterates converge (to w∞).
3.	w∞ ∈ W.
Note that, while convergence (to some point) with decaying step size is almost trivial, this result
establishes convergence to the solution set with a fixed step size. Furthermore, the convergence is
deterministic, and is not in expectation or with high probability. For example, this result also applies
to the case where we cycle through the data deterministically.
We should also remark that the choice of distance in the definition of the “ball” B was important
to be the Bregman divergence with respect to ψ(∙) and in that particular order. In fact, one cannot
guarantee that the SMD iterates get closer to an interpolating w at every step in the usual Euclidean
sense. However, once can establish that it gets closer in Dψ(w, ∙). Finally, it is important to note
that we need the step size to be small enough to guarantee the strict convexity of ψ(∙) 一 ηLi (∙) in B,
not globally.
Denote the global minimum that is closest to the initialization in Bregman divergence by w*, i.e.,
w* = argmin Dψ (w,wQ).	(8)
w∈W
Recall that in the linear case, this was what SMD converges to. We show that in the nonlinear case,
under Assumptions 1 and 2, SMD converges to a point w∞ which is “very close” to w*.
6
Under review as a conference paper at ICLR 2020
Theorem 4.	Define w* = argmi□w∈w Dψ (w, wo). Under the conditions of Theorem 3, and As-
sumption 2, the following holds
1.	Dψ(w∞, w0) = Dψ(w*, w0) + o()
2.	Dψ(w*, w∞) = o()
In other words, if we start with an initialization that is O() away from W (in Bregman divergence),
we converge to a point w∞ ∈ W that is o() away from w* . The Bregman divergence of this point
is o() from the minimum value it can take.
Corollary 5. For the initialization w0 = arg minw∈Rp ψ(w), under the conditions of Theorem 4,
w* = arg minw∈W ψ(w) and the following holds.
1.	ψ(w∞) = ψ(w*) + o()
2.	Dψ(w*, w∞) = o()
3.2 Proof Technique: Fundamental Identity of SMD
The main tool used for the proofs is a fundamental identity that holds for SMD.
Lemma 6. For any model f (∙, ∙), any differentiable loss '(∙), any parameter W ∈ W, and any step
size η > 0, the following relation holds for the SMD iterates {wi }
Dψ(w, wi-1) = Dψ (w, wi) + Dψ-ηLi (wi, wi-1) + ηLi (wi) + ηDLi (w, wi-1),	(9)
for all i ≥ 1.
This identity allows one to prove the results in a remarkably simple and direct way. Due to space
limitations, the proofs are relegated to the supplementary material.
The ideas behind this identity are related to H∞ estimation theory (Hassibi et al., 1999; Simon,
2006), which was originally developed in the 1990’s in the context of robust control theory. In fact,
it has connections to the minimax optimality of SGD, which was shown by (Hassibi et al., 1994) for
linear models, and recently extended to nonlinear models and general mirrors by (Azizan & Hassibi,
2019).
4	Experimental Results
In this section, we provide our experimental results, which consist of two main parts. In the first part,
we evaluate the theoretical claims by running systematic experiments for different initializations
and different mirrors, and evaluating the distances between the global minima achieved and the
initializations, in different Bregman divergences. In the second part, we assess the generalization
error of different mirrors, which correspond to different regularizers, in order to understand which
regularizer performs better.
Figure 3: An illustration of the experiments in Table 1
7
Under review as a conference paper at ICLR 2020
	SMD 1-norm	SMD 2-norm (SGD)	SMD 3-norm	SMD 10-norm
1-norm BD	141	9.19 × 103	-^4.1 × 104^^	2.34 × 105
2-norm BD	3.15 × 103	562	1.24 X 103	6.89 × 103
3-norm BD	4.31 × 104	107	53.5	1.85 X 102
10-norm BD	6.83 × 1013	972	7.91 X 10-5	2.72 X 10-8
Table 1: Fixed Initialization. Distances from final points (global minima) obtained by different algo-
rithms (columns) from the same initialization (Fig. 3), measured in different Bregman divergences
(rows). First Row: The closest point to w0 in `1 Bregman divergence, among the four final points,
is exactly the one obtained by SMD with 1-norm potential. Second Row: The closest point to w0
in `2 Bregman divergence (Euclidean distance), among the four final points, is exactly the one ob-
tained by SGD. Third Row: The closest point to w0 in `3 Bregman divergence, among the four final
points, is exactly the one obtained by SMD with 3-norm potential. Fourth Row: The closest point
to w0 in `10 Bregman divergence, among the four final points, is exactly the one obtained by SMD
with 10-norm potential.
4.1	Do SMDs Converge to the Closest Point in Bregman Divergence?
While accessing all the points on W and finding the closest one is impossible, we design system-
atic experiments to test this claim. We run experiments on some standard deep learning problems,
namely, a standard CNN on MNIST (LeCun et al., 1998) and the ResNet-18 (He et al., 2016) on
CIFAR-10 (Krizhevsky & Hinton, 2009). We train the models from different initializations, and with
different mirror descents from each particular initialization, until we reach 100% training accuracy,
i.e., a point on W. We randomly initialize the parameters of the networks around zero. We choose 6
independent initializations for the CNN, and 8 for ResNet-18, and for each initialization, we run dif-
ferent SMD algorithms with the following four potential functions: (a) `1 norm, (b) `2 norm (which
is SGD), (C) '3 norm, (d) '10 norm (as a surrogate for '∞). See Supplementary Material B for more
details on the experiments.
	Final 1	Final 2	Final 3	Final 4	Final 5	Final 6	Final 7
Initial 1	6 X 102	2.9 X 103-	2.8 X 103	2.8 X 103	2.8 X 103	2.8 X 103	2.8 X 103
Initial 2	2.8 X 103	6.1 X 102	2.8 X 103	2.8 X 103	2.8 X 103	2.8 X 103	2.8 X 103
Initial 3	2.8 X 103	2.9 X 103	5.6 X 102	2.8 X 103	2.8 X 103	2.8 X 103	2.8 X 103
Initial 4	2.8 X 103	2.9 X 103	2.8 X 103	5.9 X 102	2.8 X 103	2.8 X 103	2.8 X 103
Initial 5	2.8 X 103	2.9 X 103	2.8 X 103	2.8 X 103	5.7 X 102	2.8 X 103	2.8 X 103
Initial 6	2.8 X 103	2.9 X 103	2.8 X 103	2.8 X 103	2.8 X 103	5.6 X 102	2.8 X 103
Initial 7	2.8 X 103	2.9 X 103	2.8 X 103	2.8 X 103	2.8 X 103	2.8 X 103	6 X 102
Initial 8	2.8 X 103	2.9 X 103	2.8 X 103	2.8 X 103	2.8 X 103	2.8 X 103	2.8 X 103
Final 8
2.8×
2.8×
2.8×
2.8×
2.8×
2.8×
2.8 ×
5.8 X
Table 2: Fixed Mirror: SGD. Pairwise distances between different initial points and the final points
obtained from them by SGD (Fig. 4). Row i: The closest final point to the initial point i, among all
the eight final points, is exactly the one obtained by the algorithm from initialization i.
3333333n2
Ooooooow
Figure 4: An illustration of the experiments in Table 2
We measure the distances between the initializations and the global minima obtained from different
mirrors and different initializations, in different Bregman divergences. Table 1, and Table 2 show
some examples among different mirrors and different initializations, respectively. Fig. 5 shows the
8
Under review as a conference paper at ICLR 2020
distances between a particular initial point and all the final points obtained from different initializa-
tions and different mirrors (the distances are often orders of magnitude different, so we show them
in logarithmic scale). The global minimum achieved by any mirror from any initialization is the
closest in the correct Bregman divergence, among all mirrors, among all initializations, and among
both. This trend is very consistent among all our experiments, which can be found in Supplementary
Material B.
əou,ssɑue9p=onLLJJo Efμe60-l
MNIST. SGD Starting from Initial 4
Figure 5: Distances between a particular initial point and all the final points obtained by both dif-
ferent initializations and different mirrors. The smallest distance, among all initializations and all
mirrors, corresponds exactly to the final point obtained from that initial point by SGD. This trend is
observed consistently for all other mirror descents and all initializations (see the results in Tables 8
and 9 in the appendix).
I 0Ξ≡ω⅛SE
I Ossmiwi-Se
I os≡ω√l-l
I OSSWH-SE
I OLaWωlZJeuLL
IoLaWSILlraulr
I eαssllItDI一eulz
I CaWIwllu,i<5ull
I CCIWslt∩eulz
I eαwlωllc0l-euLL
I CawSlZl一eulr
I eαssl∑leulz
I s≡ω⅛sκ
I ZalΛIωl9Jeulz
I ZaWSl寸l-euLL
I ZaWSIlCI'l-eulr
I ZaWSIZl-euLL
I zαwslLIraulr
-LCIiΛllωlItDI一eι⅛∣
— Lαwsllgl√uLL
I s≡∞lʧl-f
-LaWlωll2leuLL
-LCIWSlZl一eulr
I s≡ωlLSE
πrararararararaggnnnnggrararararararara ————————
IzizizizeeeeizizizizizizizizizizizizeeeiliEccccccc
CIFAR-10. SGD Starting from Initial 2

4.2	Distribution of the Weights of the Network
One may be curious to see how the final weights obtained by these different mirrors look like,
and whether, for example, mirror descent corresponding to the `1 -norm potential induces sparsity.
Fig. 6 shows the histogram of the absolute value of the weights for different SMDs, when initialized
by the same set of close to zero weights. The histograms of the final weights look substantially
different and, since they all started from the same initial weights, this difference is fully attributable
to the different mirrors used. The histogram of the 'ι-SMD has more weights at and close to zero,
which confirms that it induces sparsity. The histogram of the `2 -SMD (SGD) looks almost perfectly
Gaussian, whereas the `3 and `10 histograms are shifted to the right, so much so that almost all
weights in the `10 solution are non-zero. See Appendix B for more details.
4.3	Generalization Errors of Different Mirrors
We compare the performance of the SMD algorithms discussed before on the test set. For MNIST,
perhaps not surprisingly, all the four SMD algorithms achieve around 99% or higher accuracy.
For CIFAR-10, however, there is a significant difference between the test errors of different mir-
rors/regularizers on the same ResNet-18 architecture. Fig. 7 shows the test accuracies of different
algorithms with eight random initializations around zero, as discussed before. Counter-intuitively,
`10 performs consistently best, while `1 performs consistently worse. This result suggests the impor-
tance of a comprehensive study of the role of regularization, and the choice of the best regularizer,
to improve the generalization performance of deep neural networks.
9
Under review as a conference paper at ICLR 2020
300000
250000
200000
IOOOOO
50000
0.01	0.02	0.03	0.04
Absolute Value of Weights
0.05
Absolute Value of Weights
300000 I---------1---------1---------1----, I
I I IlO norm
250000 k
13-norm
180000
160000
140000
AUU3nblυlt
Figure 6: Histogram of the absolute value of the final weights in the network for different SMD
algorithm with different potentials. Note that each of the four histograms corresponds to an 11 × 106-
dimensional weight vector that perfectly interpolates the data. Even though the weights remain quite
small, the histograms are drastically different. 'ι-SMD induces sparsity on the weights, as expected.
SGD appears to be produce a Gaussian distribution on the weights. '3-SMD starts to reduce the
sparsity, and `10 shifts the distribution of the weights significantly, so much so that almost all the
weights are non-zero.
Figure 7: Generalization performance of different SMD algorithms on the CIFAR-10 dataset using
ResNet-18. `10 performs consistently better, while `1 performs consistently worse.
10
Under review as a conference paper at ICLR 2020
References
Alessandro Achille and Stefano Soatto. On the emergence of invariance and disentangling in deep
representations. arXiv preprint arXiv:1706.01350, 2017.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In Proceedings of the 36th International Conference on Machine Learning.
PMLR, 2019.
Dario Amodei, Sundaram Ananthanarayanan, Rishita Anubhai, Jingliang Bai, Eric Battenberg, Carl
Case, Jared Casper, Bryan Catanzaro, Qiang Cheng, Guoliang Chen, et al. Deep speech 2: End-
to-end speech recognition in english and mandarin. In International Conference on Machine
Learning,pp.173-182, 2016.
Navid Azizan and Babak Hassibi. Stochastic gradient/mirror descent: Minimax optimality and
implicit regularization. In International Conference on Learning Representations (ICLR), 2019.
Amir Beck and Marc Teboulle. Mirror descent and nonlinear projected subgradient methods for
convex optimization. Operations Research Letters, 31(3):167-175, 2003.
Yuan Cao and Quanquan Gu. A generalization theory of gradient descent for learning over-
parameterized deep relu networks. arXiv preprint arXiv:1902.01384, 2019.
Nicolo Cesa-Bianchi, Pierre Gaillard, Gabor Lugosi, and Gilles Stoltz. Mirror descent meets fixed
share (and feels no regret). In Advances in Neural Information Processing Systems, pp. 980-988,
2012.
Pratik Chaudhari and Stefano Soatto. Stochastic gradient descent performs variational inference,
converges to limit cycles for deep networks. In International Conference on Learning Represen-
tations, 2018.
Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. arXiv preprint arXiv:1811.03804, 2018.
Claudio Gentile. The robustness of the p-norm algorithms. Machine Learning, 53(3):265-299,
2003.
Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recur-
rent neural networks. In 2013 IEEE international conference on acoustics, speech and signal
processing, pp. 6645-6649. IEEE, 2013.
Adam J Grove, Nick Littlestone, and Dale Schuurmans. General convergence results for linear
discriminant updates. Machine Learning, 43(3):173-210, 2001.
Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro.
Implicit regularization in matrix factorization. In Advances in Neural Information Processing
Systems, pp. 6152-6160, 2017.
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias in
terms of optimization geometry. In International Conference on Machine Learning, pp. 1827-
1836, 2018a.
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Implicit bias of gradient descent
on linear convolutional networks. arXiv preprint arXiv:1806.00468, 2018b.
Babak Hassibi, Ali H. Sayed, and Thomas Kailath. Hoo optimality criteria for LMS and backprop-
agation. In Advances in Neural Information Processing Systems 6, pp. 351-358. 1994.
Babak Hassibi, Ali H Sayed, and Thomas Kailath. Indefinite-Quadratic Estimation and Control: A
Unified Approach to H2 and H-infinity Theories, volume 16. SIAM, 1999.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
11
Under review as a conference paper at ICLR 2020
Jyrki Kivinen and Manfred K Warmuth. Exponentiated gradient versus gradient descent for linear
predictors. Information and Computation, 132(1):1-63, 1997.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Tech-
nical report, Citeseer, 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in Neural Information Processing Systems, pp. 1097-1105,
2012.
Yann LeCun, Leon Bottou, YoshUa Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436, 2015.
Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht. Gradient descent only
converges to minimizers. In Conference on Learning Theory, pp. 1246-1257, 2016.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. In Advances in Neural Information Processing Systems, pp. 8157-
8166, 2018.
Cong Ma, Kaizheng Wang, Yuejie Chi, and Yuxin Chen. Implicit regularization in nonconvex sta-
tistical estimation: Gradient descent converges linearly for phase retrieval and matrix completion.
In International Conference on Machine Learning, pp. 3351-3360, 2018a.
Siyuan Ma, Raef Bassily, and Mikhail Belkin. The power of interpolation: Understanding the effec-
tiveness of SGD in modern over-parametrized learning. In Proceedings of the 35th International
Conference on Machine Learning, volume 80, pp. 3325-3334. PMLR, 2018b.
Poorya Mianjy, Raman Arora, and Rene Vidal. On the implicit bias of dropout. In International
Conference on Machine Learning, pp. 3537-3545, 2018.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. Nature, 518(7540):529, 2015.
Arkadii Nemirovski and David Borisovich Yudin. Problem complexity and method efficiency in
optimization. 1983.
Behnam Neyshabur, Ryota Tomioka, Ruslan Salakhutdinov, and Nathan Srebro. Geometry of opti-
mization and implicit regularization in deep learning. arXiv preprint arXiv:1705.03071, 2017.
Samet Oymak and Mahdi Soltanolkotabi. Overparameterized nonlinear learning: Gradient descent
takes the shortest path? In Proceedings of the 36th International Conference on Machine Learn-
ing. PMLR, 2019.
Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via informa-
tion. arXiv preprint arXiv:1703.00810, 2017.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. Nature, 529(7587):484-489, 2016.
Dan Simon. Optimal state estimation: Kalman, H infinity, and nonlinear approaches. John Wiley
& Sons, 2006.
Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the optimiza-
tion landscape of over-parameterized shallow neural networks. arXiv preprint arXiv:1707.04926,
2017.
Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The im-
plicit bias of gradient descent on separable data. arXiv preprint arXiv:1710.10345, 2017.
12
Under review as a conference paper at ICLR 2020
Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal
value of adaptive gradient methods in machine learning. In Advances in Neural Information
Processing Systems, pp. 4151-4161, 2017.
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine trans-
lation system: Bridging the gap between human and machine translation. arXiv preprint
arXiv:1609.08144, 2016.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
Zhengyuan Zhou, Panayotis Mertikopoulos, Nicholas Bambos, Stephen Boyd, and Peter W Glynn.
Stochastic mirror descent in variationally coherent optimization problems. In Advances in Neural
Information Processing Systems, pp. 7043-7052, 2017.
13
Under review as a conference paper at ICLR 2020
Supplementary Material
A	Proofs of the Theoretical Results
In this section, we prove the main theoretical results. The proofs are based on a fundamental identity
about the iterates of SMD, which holds for all mirrors and all overparametereized (even nonlinear)
models (Lemma 6). We first prove this identity, and then use it to prove the convergence and implicit
regularization results.
A. 1 Fundamental Identity of SMD
Let us prove the fundamental identity.
Lemma 6. For any model f (∙, ∙), any differentiable loss '(∙), any parameter W ∈ W, and any SteP
size η > 0, the following relation holds for the SMD iterates {wi }
Dψ(w, wi-1) = Dψ(w, wi) + Dψ-ηLi (wi, wi-1) + ηLi (wi) + ηDLi (w, wi-1),	(9)
for all i ≥ 1.
Proof of Lemma 6. Let us start by expanding the Bregman divergence Dψ(w, wi) based on its defi-
nition
Dψ(w,Wi) = ψ(w) - ψ(wi) - Vψ(wi)T(W - Wi).
By plugging the SMD update rule Vψ(wi) = Vψ(wi-ι) - ηVLi(wi-ι) into this, We can write it
as
Dψ(W, Wi) = ψ(W) - ψ(Wi) - Vψ(Wi-1)T(W - Wi) + ηVLi(Wi-1)T(W - Wi).	(10)
Using the definition of Bregman divergence for (W, Wi-1) and (Wi, Wi-1), i.e., Dψ(W, Wi-1) =
ψ(W) - ψ(Wi-1) - Vψ(Wi-1)T (W - Wi-1) and Dψ(Wi, Wi-1) = ψ(Wi) - ψ(Wi-1) -
Vψ(Wi-1)T (Wi - Wi-1), we can express this as
Dψ(W, Wi) = Dψ(W, Wi-1) + ψ(Wi-1) + Vψ(Wi-1)T(W - Wi-1) - ψ(Wi)
- Vψ(Wi-1)T (W - Wi) + ηVLi (Wi-1)T (W - Wi) (11)
= Dψ(W, Wi-1) + ψ(Wi-1) - ψ(Wi) + Vψ(Wi-1)T(Wi - Wi-1)
+ ηVLi(Wi-1)T(W -Wi) (12)
= Dψ(W, Wi-1) - Dψ(Wi, Wi-1) + ηVLi (Wi-1)T (W - Wi).	(13)
Expanding the last term using W - Wi = (W - Wi-1) - (Wi - Wi-1), and following the definition
of DLi (., .) from (7) for (W, Wi-1) and (Wi, Wi-1), we have
Dψ(W, Wi) = Dψ(W, Wi-1) - Dψ(Wi, Wi-1) + ηVLi (Wi-1)T (W - Wi-1)
- ηVLi(Wi-1)T(Wi - Wi-1) (14)
= Dψ(W, Wi-1) - Dψ(Wi, Wi-1) + η (Li (W) - Li(Wi-1) - DLi (W, Wi-1))
- η (Li(Wi) - Li(Wi-1) - DLi (Wi, Wi-1)) (15)
= Dψ(W, Wi-1) - Dψ(Wi,Wi-1) + η (Li(W) - DLi (W, Wi-1))
- η (Li(Wi) - DLi(Wi, Wi-1))	(16)
Note that for all W ∈ W, we have Li(W) = 0. Therefore, for all W ∈ W
Dψ(W, Wi) = Dψ(W, Wi-1)-Dψ(Wi, Wi-1)-ηDLi(W, Wi-1)-ηLi(Wi)+ηDLi(Wi,Wi-1). (17)
Combining the second and the last terms in the right-hand side leads to
Dψ(W, Wi) = Dψ(W, Wi-1) - Dψ-ηLi (Wi, Wi-1) - ηDLi (W, Wi-1) - ηLi(Wi),	(18)
for all w ∈ W, which concludes the proof.	□
14
Under review as a conference paper at ICLR 2020
A.2 Convergence of SMD to the Interpolating Set
Now that we have proved Lemma 6, we can use it to prove our main results, in a remarkably simple
fashion. Let us first prove the convergence of SMD to the set of solutions.
Assumption 1. Denote the initial point by w0. There exists w ∈ W and a region B = {w0 ∈
Rp | Dψ (w, w0) ≤ } containing w0, such that DLi (w, w0) ≥ 0, i = 1, . . . , n, for all w0 ∈ B.
Theorem 3.	Consider the set of interpolating parameters W = {w ∈ Rp | f (xi , w) = yi , i =
1, . . . , n}, and the SMD iterates given in (3), where every data point is revisited after some steps.
Under Assumption 1, for sufficiently small step size, i.e., for any η > 0 for which ψ(∙) 一 ηLi(∙) is
strictly convex for all i, the following holds.
1.	All the iterates {wi } remain in B.
2.	The iterates converge (to w∞).
3.	w∞ ∈ W.
Proof of Theorem 3. First we show that all the iterates wil remain in B. Recall the identity of SMD
from Lemma 6:
Dψ(w, wi-1) = Dψ(w, wi) + Dψ-ηLi (wi, wi-1) +ηLi(wi) + ηDLi (w, wi-1)	(9)
which holds for all w ∈ W. If wi-1 is in the region B, we know that the last term DLi (w, wi-1)
is non-negative. Furthermore, if the step size is small enough that ψ(∙) 一 ηLi(∙) is strictly convex,
the second term Dψ-ηLi (wi, wi-1) is a Bregman divergence and is non-negative. Since the loss is
non-negative, ηLi(wi) is always non-negative. As a result, we have
Dψ(w, wi-1) ≥ Dψ(w,wi),	(19)
This implies that Dψ (w, wi) ≤ , which means wi is in B too. Since w0 is in B, w1 will be in B,
and therefore, w2 will be in B, and similarly all the iterates will remain in B.
Next, we prove that the iterates converge and w∞ ∈ W . If we sum up identity (9) for all i =
1, . . . , T , the first terms on the right- and left-hand side cancel each other telescopically, and we
have
T
Dψ(w, w0) = Dψ(w, wT) +	[Dψ-ηLi (wi, wi-1) + ηLi(wi) + ηDLi (w, wi-1)] .	(20)
i=1
Since Dψ(w, wT ) ≥ 0, we have PiT=1 [Dψ-ηLi(wi, wi-1) + ηLi(wi) +ηDLi(w,wi-1)] ≤
Dψ(w, w0). Ifwe take T → ∞, the sum still has to remain bounded, i.e.,
∞
[Dψ-ηLi (wi, wi-1) +ηLi(wi) + ηDLi (w, wi-1)] ≤ Dψ(w,w0).	(21)
i=1
Since the step size is small enough that ψ(∙) 一 ηLi(∙) is strictly convex for all i, the first term
Dψ-ηLi (wi, wi-1) is non-negative. The second term ηLi (wi) is non-negative because of the non-
negativity of the loss. Finally, the last term DLi (w, wi-1) is non-negative because wi-1 ∈ B for
all i. Hence, all the three terms in the summand are non-negative, and because the sum is bounded,
they should go to zero as i → ∞. In particular,
Dψ-ηLi (wi, wi-1) → 0	(22)
implies wi → wi-1, i.e., convergence (wi → w∞) (Note that the functions ψ 一 ηLi do not go to
zero, as there is a fixed number, i.e., n, of them). Further,
ηLi(wi) → 0.	(23)
This implies that all the individual losses are going to zero, and since every data point is being
revisited after some steps, all the data points are being fit. Therefore, w∞ ∈ W.	□
15
Under review as a conference paper at ICLR 2020
A.3 Closeness of the Final Point to the Regularized Solution
In this section, We show that with the additional Assumption 2 (which is equivalent to fi(∙) having
bounded Hessian in B), not only do the iterates remain in B and converge to the set W, but also they
converge to a point which is very close to w* (the closest solution to the initial point, in Bregman
divergence). The proof is again based on our fundamental identity for SMD.
Assumption 2. Consider the region B in Assumption 1. fi(∙) have bounded gradient and Hessian
on the convex hull of B, i.e., kVfi(w0 )k ≤ γ, and α ≤ λmin (Hfi(W')) ≤ λmaχ(Hfi (w0)) ≤ β,i =
1, . . . , n, for all w 0 ∈ conv B.
Theorem 4.	Define w* = arg minw∈W Dψ (w, w0). Under the assumptions of Theorem 3, and
Assumption 2, the following holds.
1.	Dψ(w∞,w0) = Dψ(w*, w0) + o()
2.	Dψ(w*, w∞) = o()
Proof of Theorem 4. Recall the identity of SMD from Lemma 6:
Dψ(w, wi-1) = Dψ(w, wi) + Dψ-ηLi (wi, wi-1) + ηLi(wi) + ηDLi (w, wi-1)	(9)
which holds for all w ∈ W. Summing the identity for all i ≥ 1, we have
∞
Dψ(w, w0) = Dψ(w, w∞) +	[Dψ-ηLi (wi, wi-1) + ηLi (wi) + ηDLi (w, wi-1)] .	(24)
i=1
for all w ∈ W . Note that the only terms in the right-hand side which depend on w are the first one
Dψ(w, w∞) and the last one η Pi∞=1 DLi (w, wi-1). In what follows, We will argue that, within B,
the dependence on w in the last term is weak and therefore w∞ is close to w*.
To further spell out the dependence on w in the last term, let us expand DLi (w, wi-1)
DLi (w, wi-1) = 0 - Li(wi-1) - VLi(wi-1)T (w - wi-1)	(25)
=-Li(Wi-ι) + '0(yi - fi(wi-Iy)Vfi(wi-ι)T(W — Wi-ι)	(26)
for all w ∈ W, where the first equality comes from the definition of DLi(∙, ∙) and the fact that
Li(W) = 0 for w ∈ W. The second equality is from taking the derivative of Li(∙) = '(yi — fi(∙))
and evaluating it at Wi-1.
By Taylor expansion of fi (W) around Wi-1 and using Taylor’s theorem (Lagrange’s mean-value
form), we have
fi(w) = fi(wi-i) + Vfi(Wi-I)T (W — Wi-i) + 1(w — Wi-I)T Hfi (Wi)(W — Wi-1),	(27)
for some Wi in the convex hull of W and Wi-ι. Since fi(W) = yi for all W ∈ W, it follows that
Vfi(Wi-I)T(w — Wi-ι) = y — fi(Wi-ι) 一 1(w - Wi-I)THfi(Wi)(W 一 Wi-1),	(28)
for all W ∈ W . Plugging this into (26), we have
DLi(w, Wi-i) = -Li(Wi-ι)+0(yi — fi(Wi-I)) +i—fi(wi-i) — |(w—Wi-i)THfi(Wi)(W—Wi-1 j
(29)
for all W ∈ W . Finally, by plugging this back into the identity (24), we have
∞
Dψ(W, W0) = Dψ(W, W∞) + X Dψ-ηLi (Wi, Wi-1) +ηLi(Wi) — ηLi(Wi-1)
i=1
+ η'0(yi — fi(wi-Iy)Vi — fi(wi-i) — 2(w — Wi-1)T Hfi (w^i)(w — Wi-1))]∙ (30)
for all W ∈ W . Note that this can be expressed as
∞1
Dψ(w, wo) = Dψ(w,w∞) + C-E 2η'0(yi — fi(wi-i))(w — Wi-i)THfi(w^i)(w — Wi-i), (31)
i=1
16
Under review as a conference paper at ICLR 2020
for all w ∈ W, where C does not depend on w:
∞
C = E [Dψ-ηLi(wi, Wi-ι) + ηLi(wi) - ηLl(wi-ι) + η'0(yi - f (Wi-I))(yi - fi(wi-ι))].
i=1
From Theorem 3, we know that w∞ ∈ W. Therefore, by plugging it into equation (31), and using
the fact that Dψ(w∞, w∞ ) = 0, we have
∞1
Dψ(w∞,wo)= C -E χη'0(yi - fi(Wi-i))(w∞ - Wi-i)THfi(Wi)(w∞ - Wi-i),	(32)
i=1 2
where Wi0 is a point in the convex hull of W∞ and Wi-1 (and therefore also in conv B), for all i.
Similarly, by plugging w*, which is also in W, into (31), We have
∞1
Dψ(w*,W0)= Dψ(w*,w∞) + C - X 2η'0(yi - fi(wi-i))(w* - Wi-I)THfi (wi'0)(w* - Wi-i),
i=1
(33)
where Wi is a point in the convex hull of w* and Wi-ι (and therefore also in Conv B), for all i.
Subtracting the last two equations from each other yields
∞1
Dψ(w∞,wo) - Dψ(w*,wo) = -Dψ(w*,w∞) + y^2η'0(yi - fi(wi-ι))∙
i=1
(W* - Wi-1)T Hfi(Wi00)(W* - Wi-1) - (W∞ - Wi-1)T Hfi (Wi0)(W∞ - Wi-1) .	(34)
Note that since all Wi0 and Wi00 are in conv B, by Assumption 2, we have
αkW∞ - Wi-1k2 ≤ (W∞ - Wi-1)T Hfi (Wi0)(W∞ - Wi-1) ≤ βkW∞ - Wi-1k2,	(35)
and
αkW* - Wi-1k2 ≤ (W* - Wi-1)T Hfi (Wi00)(W* - Wi-1) ≤ βkW* - Wi-1k2.	(36)
Further, again since all the iterates {Wi} are in B, it follows that kW∞ - Wi-1 k2 = O() and
kW*-Wi-1 k2 = O(). As a result the difference of the two terms, i.e., (W* -Wi-1)T Hfi (Wi00)(W* -
Wi-1) - (W∞ - Wi-1)T Hfi (Wi0)(W∞ - Wi-1) , is also O(), and we have
∞
Dψ(w∞,wo) - Dψ(w*,wo) = -Dψ(w*,w∞) + Xη'0(yi - fi(Wi-I))O(e).	(37)
i=1
Now note that '(y - fi(Wi-ι)) = '0(fi(W) - fi(Wi-ι)) = '(Nfi(Wi)T(W - Wi-I)) for some
Wi ∈ Conv B. Since ∣∣w - Wi-Ik2 = O(E) for all i, and since '(∙) is differentiable and fi(∙) have
bounded derivatives, it follows that '0(yi - fi(Wi-ι)) = o(e). Furthermore, the sum is bounded.
This implies that Dψ(W∞, W0) - Dψ(W*, W0) = -Dψ(W*, W∞) + o(E), or equivalently
Dψ(W∞,W0) - Dψ(W*, W0) + Dψ(W*,W∞) = o(E).	(38)
The term in parentheses Dψ (W∞, W0) - Dψ (W*, W0) is non-negative by definition of W*. The
second term Dψ(W*, W∞) is non-negative by convexity ofψ. Since both terms are non-negative and
their sum is o(E), each one of them is at most o(E), i.e.
Dψ(W∞, W0) - Dψ(W*, W0) = o(E)
Dψ(W*, W∞) = o(E)
(39)
which concludes the proof.	□
Corollary 5. For the initialization W0 = arg minw∈Rp ψ(W), under the conditions of Theorem 4,
W* = arg minw∈W ψ(W) and the following holds.
1.	ψ(W∞) = ψ(W*) + o(E)
2.	Dψ(W*, W∞) = o(E)
17
Under review as a conference paper at ICLR 2020
Proof of Corollary 5. The proof is a straightforward application of Theorem 4. Note that we have
Dψ(w,wo) = ψ(w) - ψ(wo) - Vψ(wo)T(W - wo)	(40)
for all w. When wo = argmi□w∈Rp ψ(w), it follows that Vψ(wo) = 0, and
Dψ (w, w0) = ψ(w) - ψ(w0).	(41)
In particular, by plugging in w∞ and w*, We have Dψ(w∞,wo) = ψ(w∞) 一 ψ(wo) and
Dψ(w*, wo) = ψ(w*) 一 ψ(wo). Subtracting the two equations from each other yields
Dψ (w∞,wo) — Dψ (w*,wo) = ψ(w∞) — ψ(w*),	(42)
which along with the application of Theorem 4 concludes the proof.	□
A.4 Closeness to the Interpolating Set in Highly Overparameterized Models
As we mentioned earlier, it has been argued in a number of recent papers that for highly overparam-
eterized models, any random initial point is, whp, close to the solution set W (Azizan & Hassibi,
2019; Li & Liang, 2018; Du et al., 2018; Allen-Zhu et al., 2019; Cao & Gu, 2019). In the highly
overparameterized regime, p n, and so the dimension of the manifold W, which is p 一 n, is very
large. For simplicity, we outline an argument for the case of Euclidean distance, bearing in mind
that a similar argument can be used for general Bregman divergence. Note that the distance of an
arbitrarily chosen wo to W is given by
min kw 一 wo k2
w
s.t. y = f(x, w)
where y = vec(yi, i = 1, . . . , n) and f (x, w) = vec(f (xi, w), i = 1, . . . , n). This can be approxi-
mated by
min kw 一 wo k2
w
s.t. y ≈ f(x, wo) + Vf (x, wo)T(w 一 wo)
where Vf (x, wo)T = vec(Vf (xi, w)T, i = 1, . . . , n) is the n × p Jacobian matrix. The latter
optimization can be solved to yield
∣∣w* — wok2 ≈ (y — f(x,wo))T (Vf(x,wo)TVf(x,wo)) 1 (y — f(x,wo))	(43)
Note that Vf (x, wo)TVf(x, wo) is an n × n matrix consisting of the sum ofp outer products. When
the xi are sufficiently random, and p	n, it is not unreasonable to assume that whp
λmin (Vf (x, WO)TVf (x, w°)) = Ω(p),
from which we conclude
I∣W* — Wok2 ≈ ky — f(x,wo)k2 ∙ O(1) = O(n),	(44)
pp
since y — f (x, wo) is n-dimensional. The above implies that wo is close to w* and hence W.
18
Under review as a conference paper at ICLR 2020
B	More Details on the Experimental Results
In order to evaluate the claim, we run systematic experiments on some standard deep learning prob-
lems.
Datasets. We use the standard MNIST (LeCun et al., 1998) and CIFAR-10 (Krizhevsky & Hinton,
2009) datasets.
Architectures. For MNIST, we use a 4-layer convolutional neural network (CNN) with 2 convo-
lution layers and 2 fully connected layers. The convolutional layers and the fully connected layers
are picked wide enough to obtain 2 × 106 trainable parameters. Since MNIST dataset has 60,000
training samples, the number of parameters is significantly larger than the number of training data
points, and the problem is highly overparameterized. For the CIFAR-10 dataset, we use the standard
ResNet-18 (He et al., 2016) architecture without any modifications. CIFAR-10 has 50,000 training
samples and with the total number of 11 × 106 parameters in ResNet-18, the problem is again highly
overparameterized.
Loss Function. We use the cross-entropy loss as the loss function in our training. We train the
models from different initializations, and with different mirror descents from each particular initial-
ization, until we reach 100% training accuracy, i.e., until we hit W.
Initialization. We randomly initialize the parameters of the networks around zero (N (0, 0.0001)).
We choose 6 independent initializations for the CNN, and 8 for ResNet-18, and for each initializa-
tion, we run the following 4 different SMD algorithms.
Algorithms. We use the mirror descent algorithms defined by the norm potential ψ(w) = 1 ∣∣wkq
for the following four different norms: (a) `1 norm, i.e., q = 1 + , (b) `2 norm, i.e., q = 2 (which is
SGD), (C) '3 norm, i.e., q = 3, (d) '10 norm, i.e., q = 10 (as a surrogate for '∞ norm). The update
rule can be expressed as follows.
wi,j =卜Wi-i,j∣q-1sign(wi-i,j) — ηVLi(wi-i)j∣ q 1 ∙
sign (∣Wi-1,j∣q-1sign(wi-1,j) — ηVLi(wi-ιj,	(45)
where wi-1,j denotes the j-th element of the wi-1 vector.
We use a fixed step size η. The step size is chosen to obtain convergence to global minima.
B.1	MNIST Experiments
B.1.1	Closest Minimum for Different Mirror Descents with Fixed
Initialization
We provide the distances from final points (global minima) obtained by different algorithms from the
same initialization, measured in different Bregman divergences for MNIST classification task using
a standard CNN. Note that in all tables the smallest element in each row is on the diagonal, which
means the point achieved by each mirror has the smallest Bregman divergence to the initialization
corresponding to that mirror, among all mirrors. Tables 3, 4, 5, 6, 7, 8 depict these results for 6
different initializations. The rows are the distance metrics used as the Bregman Divergences with
specified potentials. The columns are the global minima obtained using specified SMD algorithms.
___________________________Table 3: MNIST InitiaI Point 1________________________
	SMD 1-norm	SMD 2-norm (SGD)	SMD 3-norm	SMD 10-norm
1-norm BD	2.767	937.8	1.05 X 104^^	1.882 × 105
2-norm BD	301.6	58.61	261.3	2.118 × 104
3-norm BD	1720	37.45	7.143	2518
10-norm BD	7.453 × 108	773.4	0.2939	0.003545
19
Under review as a conference paper at ICLR 2020
Table 4: MNIST Initial Point 2
	SMD 1-norm	SMD 2-norm (SGD)	SMD 3-norm	SMD 10-norm
1-norm BD	278	945	1.37 X 104^^	2.01 × 105
2-norm BD	292	59.3	374	2.29 × 104
3-norm BD	1.51 × 103	38.6	11.6	2.71 X 103
10-norm BD	1.06 × 108	831	0.86	0.00321
Table 5: MNIST Initial Point 3
	SMD 1-norm	SMD 2-norm (SGD)	SMD 3-norm	SMD 10-norm
1-norm BD	3.02	968	1.06 X 104^^	1.9 X 105
2-norm BD	291	60.9	272	2.12 X 104
3-norm BD	1.49 X 103	39.1	7.82	2.49 X 103
10-norm BD	1.1 X 108	900	0.411	0.00318
Table 6: MNIST Initial Point 4
	SMD 1-norm	SMD 2-norm (SGD)	SMD 3-norm	SMD 10-norm
1-norm BD	278	1.21 X 103	1.08 X 104^^	1.92 X 105
2-norm BD	291	77.3	271	2.15 X 104
3-norm BD	1.48 X 103	49.7	7.56	2.52 X 103
10-norm BD	9.9 X 107	1.72 X 103	0.352	0.00296
Table 7: MNIST Initial Point 5
	SMD 1-norm	SMD 2-norm (SGD)	SMD 3-norm	SMD 10-norm
1-norm BD	279	958	1.08 X 104^^	2 X 105
2-norm BD	292	60.4	271	2.28 X 104
3-norm BD	1.49 X 103	39	7.52	2.69 X 103
10-norm BD	9.09 X 107	846	0.342	0.00309
Table 8: MNIST Initial Point 6
	SMD 1-norm	SMD 2-norm (SGD)	SMD 3-norm	SMD 10-norm
1-norm BD	2.96	930	1.08 X 104^^	1.9 X 105
2-norm BD	308	59	271	2.12 X 104
3-norm BD	1.63 X 103	38.6	7.46	2.47 X 103
10-norm BD	1.65 X 108	864	0.334	0.00295
B.1.2	Closest Minimum for Different Initilizations with Fixed Mirror
We provide the pairwise distances between different initial points and the final points (global min-
ima) obtained by using fixed SMD algorithms in MNIST dataset using a standard CNN. Note that
the smallest element in each row is on the diagonal, which means the closest final point to each
initialization, among all the final points, is the one corresponding to that point. Tables 9, 10, 11
and 12 depict these results for 4 different SMD algorithms. The rows are the initial points and the
columns are the final points corresponding to each initialization.
B.1.3	Closest Minimum for Different Initilizations and Different Mirrors
Now we assess the pairwise distances between different initial points and final points (global min-
ima) obtained by all different initilizations and all different mirrors (Table 8). The smallest element
in each row is exactly the final point obtained by that mirror from that initialization, among all the
mirrors and all the initial points.
20
Under review as a conference paper at ICLR 2020
Table 9: MNIST 1-norm Bregman Divergence Between the Initial Points and the Final Points ob-
tained by SMD 1-norm
	Final 1	Final2	Final 3	Final 4	Final 5	Final 6
Initial Point 1	2.7671	20311	20266	20331	20340	20282
Initial Point 2	20332	2.7774	20281	20299	20312	20323
Initial Point 3	20319	20312	3.018	20344	20309	20322
Initial Point 4	20339	20279	20310	2.781	20321	20297
Initial Point 5	20347	20317	20273	20316	2.7902	20311
Initial Point 6	20344	20323	20340	20318	20321	2.964
Table 10: MNIST 2-norm Bregman Divergence Between the Initial Points and the Final Points
obtained by SMD 2-norm (SGD)
	Final 1	Final 2	Final 3	Final 4	Final 5	Final 6
Initial Point 1	58.608	670.75	667.03	684.18	671.36	667.84
Initial Point 2	669.84	59.315	669.16	682.04	669.45	669.98
Initial Point 3	666.35	670.22	60.858	683.44	667.57	669.99
Initial Point 4	669.71	668.86	671.19	77.275	670.33	669.7
Initial Point 5	671.1	669.12	668.45	683.61	60.39	666.04
Initial Point 6	669.46	670.92	671.59	684.32	667.37	59.043
Table 11: MNIST 3-norm Bregman Divergence Between the Initial Points and the Final Points
obtained by SMD 3-norm
	Final 1	Final 2	Final 3	Final 4	Final 5	Final 6
Initial Point 1	7.143	35.302	32.077	32.659	32.648	32.309
Initial Point 2	32.507	11.578	32.256	32.325	32.225	32.46
Initial Point 3	31.594	34.643	7.8239	32.521	31.58	32.519
Initial Point 4	32.303	34.811	32.937	7.5589	32.617	32.284
Initial Point 5	32.673	34.678	32.071	32.738	7.5188	31.558
Initial Point 6	32.116	34.731	32.376	32.431	31.699	7.4593
Table 12: MNIST 10-norm Bregman Divergence Between the Initial Points and the Final Points
obtained by SMD 10-norm__________________________________________________________
	Final 1	Final 2	Final3	Final 4	Final 5	Final 6
Initial Point 1	0.00354	037^^	0.403	0.286	0.421	0.408
Initial Point 2	0.33	0.00321	0.369	0.383	0.415	0.422
Initial Point 3	0.347	0.318	0.00318	0.401	0.312	0.406
Initial Point 4	0.282	0.38	0.458	0.00296	0.491	0.376
Initial Point 5	0.405	0.418	0.354	0.484	0.00309	0.48
Initial Point 6	0.403	0.353	0.422	0.331	0.503	0.00295
21

	Fl SMD 1	F2 SMD 1	F3 SMD 1 回 SMD 1∣F5 SMD 1∣ F6 SMD 1				Fl SMD 2	F2 SMD 2	F3 SMD 2	F4 SMD 2	F5 SMD 2	F6 SMD 2	Fl SMD 3	F2 SMD 3	F3 SMD 3	F4 SMD 3	F5 SMD 3	F6 SMD 3 .	Fl SMD 10	F2 SMD 10	F3 SMD 10 回 SMD 10∣F5 SMD 10			F6 SMD 10
Il 1-norm BD	2.767105	20310.58	20266.27	20330.6	20340.2	20281.51	937.7902	20501.09	20453.6	20615.37	20505.63	20451.42	10500.44	24298.6	22690.41	22883.13	22928.17	22930.01	188233.4	200749.8	189599.3	192017.6	200332.6	189842.1
12 1-norm BD	20332.47	2.777443	20280.59	20298.8	20312	20322.66	20477.15	944.8926	20467.58	20572.54	20486.79	20481.46	22902.71	13736.89	22683.03	22823.09	22927.75	22951.2	188019	200838.7	189406.7	191694.7	200319.4	189452.9
13 1-norm BD	20319.38	20312.19	3.018036	20343.8	20308.9	20322.02	20443.74	20487.21	967.6324	20612.98	20486.93	20485.8	22897.06	24300.62	10609.4	22876.31	22901.84	22949.55	187883.2	201071.8	189571	192131.8	199958.1	189571.5
14 1-norm BD	20338.77	20279.16	20309.78	2.78104	20321.1	20297.36	20476.14	20461.38	20499.16	1214.917	20499.88	20469.45	22910.51	24283.22	22733.45	10756.58	22928.43	22938.72	187740.6	200692.5	189522.4	192082.9	200434.4	189653.4
15 1-norm BD	20347.03	20317.23	20273.07	20316.4	2.79019	20310.78	20498.73	20496.97	20464.54	20600.07	957.8013	20484.67	22921.69	24335.41	22722.83	22877.07	10812.1	22955.94	188056.5	200743.9	189707.6	192056.4	200478.6	189883
16 1-norm BD	20343.59	20322.62	20339.82	20318.4	20320.9	2.964027	20493.68	20504.14	20535.06	20590.71	20491.61	930.2714	22926.8	24311.73	22713.74	22837.8	22900.31	10848.27	187959.4	200482.2	189602.3	192052.5	200309.6	189738.7
Il 2-norm BD	301.6218	928.1953	922.246	925.889	929.909	940.8018	58.60796	670.7482	667.0325	684.1751	671.3561	667.8379	261.2823	760.2361	701.1998	706.1766	704.5516	704.0641	21179.18	22902.48	21188.34	21536.64	22803.44	21162.22
12 2-norm BD	938.5225	291.6223	924.8324	925.561	926.34	944.1569	669.8414	59.31496	669.1617	682.0358	669.4517	669.9774	703.9956	373.9718	702.4789	703.8165	703.9578	705.4268	21164.67	22901.68	21187.33	21523.37	22797.37	21152.21
13 2-norm BD	936.4615	928.4752	290.902	926.259	924.438	943.7131	666.3494	670.2202	60.85767	683.4393	667.5668	669.9933	700.8777	758.6538	272.0649	705.6848	701.1583	705.155	21164.46	22904.93	21186.56	21536.03	22787.11	21151.72
14 2-norm BD	938.7566	926.655	926.2601	290.552	928.945	944.0035	669.7086	668.8569	671.186	77.27538	670.3311	669.7023	703.3976	757.9133	704.6345	270.9099	703.842	704.6346	21161.99	22898.71	21186.54	21541.32	22799.92	21152.3
15 2-norm BD	940.8469	928.4445	923.667	927.336	291.765	939.7045	671.1005	669.1169	668.446	683.6102	60.39	666.0443	705.3977	758.6884	702.3112	705.3715	270.8719	701.3619	21166.8	22898.1	21191.65	21533.87	22805.13	21162.55
16 2-norm BD	937.93	929.7885	929.404	927.348	925.181	307.5172	669.4556	670.9225	671.5908	684.3248	667.3748	59.04266	702.8038	759.7584	703.6673	705.2996	700.8271	271.1133	21166.69	22894.56	21188.54	21530.77	22796.98	21153.93
Il 3-norm BD	1719.866	1543.515	1516.246	―1512.4	1521.08	1656.464	37.45108	67.57934	66.73737	78.02365	67.95686	66.51245	7.14298	35.30229	32.07697	32.65884	32.64842	32.30852	2517.617	2706.617	2491.476	2519.086	2688.245	2470.969
12 3-norm BD	1751.333	1510.961	1516.163	1514.81	1518.79	1658.074	66.28766	38.64332	66.75334	78.01804	66.94847	67.09068	32.50659	11.57823	32.25632	32.32539	32.2253	32.45956	2516.606	2705.533	2491.199	2518.034	2687.31	2470.926
13 3-norm BD	1751.98	1544.446	1486.664	1513.27	1517.48	1658.303	65.47958	67.39749	39.096	78.03239	66.49712	67.24052	31.59447	34.64265	7.823877	32.52136	31.58038	32.51863	2517.107	2706.491	2489.415	2519.598	2687.107	2470.339
14 3-norm BD	1751.523	1543.899	1517.328	1483.49	1522.07	1659.334	66.36948	67.31509	67.59354	49.69977	67.96119	67.20248	32.30269	34.81075	32.93691	7.558935	32.61658	32.28448	2517.248	2706.852	2491.392	2518.947	2687.751	2470.657
15 3-norm BD	1753.311	1545.901	1516.143	1515.92	1488.06	1657.359	66.56918	67.42434	67.07494	78.55313	39.04714	66.25287	32.67308	34.67835	32.07084	32.73818	7.518829	31.55844	2517.357	2706.916	2491.048	2519.073	2687.064	2471.216
16 3-norm BD	1751.224	1544.936	1520.698	1514.66	1519.78	1626.957	66.33501	67.47943	67.81073	78.43179	67.07613	38.58941	32.11641	34.73071	32.37629	32.43067	31.69857	7.459286	2517.511	2706.82	2490.098	2518.297	2687.431	2469.509
Il 10-norm BI	7.45E+08	1.06E+08	-1.1E+O8	9.9E+07	9.1E+07	1.65 E+08	773.3514	831.1445	^^900.464	1718.299	846.4625	864.5718	0.293932	1.233024	0.782131	0.615488	0.748684	0.707943	0.003545	0.370181	0.403135	^^0.28582	0.421482	0.408148
12 10-norm BI	7.45E+08	1.06E+08	1.1E+O8	9.9E+07	9.1E+07	1.65E+08	773.7523	830.5577	900.2781	1718.625	846.2303	864.6849	0.61333	0.860265	0.732687	0.725046	0.727329	0.727967	0.330493	0.003207	0.368537	0.382603	0.415105	0.422372
13 10-norm BI	7.45E+08	1.06E+08	1.1E+O8	9.9E+07	9.1E+07	1.65E+08	773.8534	831.2133	900.141	1718.575	846.1995	864.7488	0.63865	1.196859	0.410611	0.735479	0.634941	0.718673	0.347069	0.317821	0.00318	0.400619	0.311682	0.405827
14 10-norm BI	7.45E+08	1.06E+08	1.1E+O8	9.9E+07	9.1E+07	1.65E+08	773.8442	831.1647	900.4524	1718.06	846.5443	864.7191	0.585811	1.241436	0.824513	0.351863	0.819103	0.694199	0.281852	0.379772	0.457535	0.002963	0.49113	0.376261
15 10-norm BI	7.45E+08	1.06E+08	1.1E+O8	9.9E+07	9.1E+07	1.65E+08	773.8727	831.1471	900.3779	1718.562	845.8668	864.717	0.691508	1.273044	0.735977	0.814864	0.342117	0.783273	0.40501	0.417533	0.353985	0.483609	0.003094	0.479598
16 10-norm B【	7.45E+08	1.06E+08	LlE+08	9.9E+07	9.1E+07	L65E+08	773.9967	831.1642	900.7065	1718.531	846.6509	864.2966	0703456	1.22497	0.82352	0.679747	0.840384	0.33448	0403348	0.352543	0.421798	0.330755	0.503257	0.002948
Figure 8: Different Bregman divergences between all the final points and all the initial points for different mirrors in MNIST dataset using a standard CNN. Note
that the smallest element in every single row is on the diagonal, which confirms the theoretical results.
UlIderreVieW as a CoIIferelICe PaPersICLR 202。
Under review as a conference paper at ICLR 2020
B.2	CIFAR- 1 0 Experiments
B.2.1	Closest Minimum for Different Mirror Descents with Fixed
Initialization
We provide the distances from final points (global minima) obtained by different algorithms from
the same initialization, measured in different Bregman divergences for CIFAR-10 classification task
using ResNet-18. Note that in all tables the smallest element in each row is on the diagonal, which
means the point achieved by each mirror has the smallest Bregman divergence to the initialization
corresponding to that mirror, among all mirrors. Tables 13, 14, 15, 16, 17, 18, 19, 20 depict these
results for 8 different initializations. The rows are the distance metrics used as the Bregman Diver-
gences with specified potentials. The columns are the global minima obtained using specified SMD
algorithms.
Table 13: CIFAR-10 Initial PointI
	SMD 1-norm	SMD 2-norm (SGD)	SMD 3-norm	SMD 10-norm
1-norm BD	189	9.58 × 103	4.19 × 104^^	2.34 X 105
2-norm BD	3.12 X 103	597	1.28 X 103	6.92 X 103
3-norm BD	4.31 × 104	119	55.8	1.87 X 102
10-norm BD	1.35 × 1014	869	6.34 X 10-5	2.64 X 10-8
Table 14: CIFAR-10 Initial Point 2
	SMD 1-norm	SMD 2-norm (SGD)	SMD 3-norm	SMD 10-norm
1-norm BD	275	9.86 X 103	4.09 X 104^^	2.38 X 105
2-norm BD	4.89 X 103	607	1.23 X 103	7.03 X 103
3-norm BD	9.21 X 104	104	53.5	1.88 X 102
10-norm BD	1.17 X 1015	225	0.000102	2.65 X 10-8
Table 15: CIFAR-10 Initial Point 3
	SMD 1-norm	SMD 2-norm (SGD)	SMD 3-norm	SMD 10-norm
1-norm BD	141	9.19 X 103	4.1 X 104^^	2.34 X 105
2-norm BD	3.15 X 103	562	1.24 X 103	6.89 X 103
3-norm BD	4.31 X 104	107	53.5	1.85 X 102
10-norm BD	6.83 X 1013	972	7.91 X 10-5	2.72 X 10-8
Table 16: CIFAR-10 Initial Point 4
	SMD 1-norm	SMD 2-norm (SGD)	SMD 3-norm	SMD 10-norm
1-norm BD	255	9.77 X 103	4.18 X 104~	2.36 X 105
2-norm BD	3.64 X 103	594	1.26 X 103	6.96 X 103
3-norm BD	5.5 X 104	116	54	1.87 X 102
10-norm BD	3.74 X 1014	640	5.33 X 10-5	2.67 X 10-8
Table 17: CIFAR-10 Initial Point 5
	SMD 1-norm	SMD 2-norm (SGD)	SMD 3-norm	SMD 10-norm
1-norm BD	∏3	9.48 X 103	4.15 X 104~	2.32 X 105
2-norm BD	2.95 X 103	572	1.27 X 103	6.85 X 103
3-norm BD	3.68 X 104	109	56.2	1.84 X 102
10-norm BD	2.97 X 1013	151	5.74 X 10-5	2.61 X 10-8
23
Under review as a conference paper at ICLR 2020
Table 18: CIFAR-10 Initial Point 6
	SMD 1-norm	SMD 2-norm (SGD)	SMD 3-norm	SMD 10-norm
1-norm BD	128	9.25 × 103	4.25 × 104	2.34 X 105
2-norm BD	2.71 X 103	558	1.29 X 103	6.89 X 103
3-norm BD	3.34 × 104	104	55.3	1.85 X 102
10-norm BD	2.61 × 1013	612	4.74 X 10-5	2.62 X 10-8
Table 19: CIFAR-10 Initial Point 7
	SMD 1-norm	SMD 2-norm (SGD)	SMD 3-norm	SMD 10-norm
1-norm BD	223	9.76 X 103	4.38 X 104^^	2.27 X 105
2-norm BD	2.41 X 103	599	1.37 X 103	6.65 X 103
3-norm BD	2.3 X 104	116	61	1.78 X 102
10-norm BD	4.22 X 1012	679	6.42 X 10-5	2.55 X 10-8
Table 20: CIFAR-10 Initial Point 8
	SMD 1-norm	SMD 2-norm (SGD)	SMD 3-norm	SMD 10-norm
1-norm BD	145	9.37 X 103	4.17 X 104~	2.36 X 105
2-norm BD	2.48 X 103	576	1.26 X 103	6.99 X 103
3-norm BD	2.85 X 104	108	54.5	1.89 X 102
10-norm BD	1.81 X 1013	1.22 X 103	5.2 X 10-5	2.64 X 10-8
B.2.2	Closest Minimum for Different Initilizations with Fixed Mirror
We provide the pairwise distances between different initial points and the final points (global min-
ima) obtained by using fixed SMD algorithms in CIFAR-10 dataset using ResNet-18. Note that the
smallest element in each row is on the diagonal, which means the closest final point to each ini-
tialization, among all the final points, is the one corresponding to that point. Tables 21, 22, 23, 24
depict these results for 4 different SMD algorithms. The rows are the initial points and the columns
are the final points corresponding to each initialization.
24
Under review as a conference paper at ICLR 2020
Table 21: CIFAR-10 1-norm Bregman Divergence Between the Initial Points and the Final Points
obtained by SMD 1-norm
	Final 1	Final 2	Final 3	Final4	Final 5	Final 6	Final 7	Final 8
Initial 1	1.9 × 102	8.1 × 104	8.1 × 104	8.4 × 104	8 × 104	8.2 × 104	7.8 × 104	7.8 × 104
Initial 2	8.1 × 104	2.7 × 102	8.1 × 104	8.3 × 104	8 × 104	8.2 × 104	7.8 × 104	7.9 × 104
Initial 3	8.1 × 104	8.1 × 104	1.4 × 102	8.4 × 104	8 × 104	8.1 × 104	7.8 × 104	7.8 × 104
Initial 4	8.1 × 104	8.1 × 104	8.1 × 104	2.5 X 102	8 × 104		8.2 × 104	7.8 × 104	7.9 × 104
Initial 5	8.1 × 104	8.1 × 104	8.1 × 104	8.3 × 104	1.1 × 102	8.1 × 104	7.8 × 104	7.8 × 104
Initial 6	8.1 × 104	8.1 × 104	8.1 × 104	8.4 × 104	8 × 104	1.3 × 102	7.8 × 104	7.8 × 104
Initial 7	8.1 × 104	8.1 × 104	8.1 × 104	8.4 × 104	8 × 104	8.1 × 104	2.2 × 102	7.8 × 104
Initial 8	8.1 × 104	8.1 × 104	8.1 × 104	8.4 × 104	7.9× 104	8.1 × 104	7.8 × 104	1.5 × 102
Table 22: CIFAR-10 2-norm Bregman Divergence Between the Initial Points and the Final Points
obtained by SMD 2-norm (SGD)
	Final 1	Final 2	Final 3	Final 4	Final 5	Final 6	Final 7	Final 8
Initial 1	6 × 102	2.9 × 103	2.8 × 103	2.8 × 103	2.8 × 103	2.8 × 103	2.8 × 103	2.8 × 103
Initial 2	2.8 × 103	6.1 × 102	2.8 × 103	2.8 × 103	2.8 × 103	2.8 × 103	2.8 × 103	2.8 × 103
Initial 3	2.8 × 103	2.9 × 103	5.6 × 102	2.8 × 103	2.8 × 103	2.8 × 103	2.8 × 103	2.8 × 103
Initial 4	2.8× 103	2.9 × 103	2.8 × 103	5.9 × 102	2.8 × 103	2.8 × 103	2.8 × 103	2.8 × 103
Initial 5	2.8× 103	2.9 × 103	2.8 × 103	2.8 × 103	5.7 × 102	2.8 × 103	2.8 × 103	2.8 × 103
Initial 6	2.8× 103	2.9 × 103	2.8 × 103	2.8 × 103	2.8 × 103	5.6 × 102	2.8 × 103	2.8 × 103
Initial 7	2.8× 103	2.9 × 103	2.8 × 103	2.8 × 103	2.8 × 103	2.8 × 103	6 × 102	2.8 × 103
Initial 8	2.8× 103	2.9 × 103	2.8 × 103	2.8 × 103	2.8 × 103	2.8 × 103	2.8 × 103	5.8 × 102
Table 23: CIFAR-10 3-norm Bregman Divergence Between the Initial Points and the Final Points
obtained by SMD 3-norm
	Final 1	Final 2	Final 3	Final 4	Final 5	Final 6	Final 7	Final 8
Initial 1 ∣	55.844	103.47	103.61	104.05	106.2	105.32	110.88	104.56
Initial 2	105.87	53.455	103.68	104.04	106.31	105.34	110.93	104.58
Initial 3	105.89	103.59	53.527	104.09	106.29	105.35	110.99	104.55
Initial 4	105.83	103.54	103.64	53.978	106.23	105.3	110.87	104.54
Initial 5	105.82	103.55	103.64	104	56.161	105.34	110.88	104.55
Initial 6	105.91	103.6	103.66	104.1	106.28	55.316	110.94	104.55
Initial 7	105.87	103.51	103.67	103.98	106.26	105.25	61.045	104.5
Initial 8	105.77	103.54	103.59	104.04	106.25	105.28	110.88	54.509
25
Under review as a conference paper at ICLR 2020
Table 24: CIFAR-10 10-norm Bregman Divergence Between the Initial Points and the Final Points obtained by SMD 10-norm FHan	Fi∏ai2	Fi∏ai3	Final 4	FHar5	FHar6	FHan	FHar8							
Initial 1	2.64 × 10-8	2.89 × 10-8	2.99 × 10-8	2.81 × 10-8	2.85 × 10-8	2.82 × 10-8	2.66 × 10-8	2.82 × 10-8
Initial 2	2.79 × 10-8	2.65 × 10-8	2.83 × 10-8	2.83 × 10-8	2.71 × 10-8	2.74 × 10-8	2.69 × 10-8	2.88 × 10-8
Initial 3	2.89 × 10-8	2.87 × 10-8	2.72 × 10-8	2.94 × 10-8	2.84 × 10-8	2.89 × 10-8	2.78 × 10-8	2.94 × 10-8
Initial 4	2.79 × 10-8	2.86 × 10-8	2.92 × 10-8	2.67 × 10-8	2.84 × 10-8	2.81 × 10-8	2.69 × 10-8	2.85 × 10-8
Initial 5	2.76 × 10-8	2.88 × 10-8	2.95 × 10-8	2.93 × 10-8	2.61 × 10-8	2.73 × 10-8	2.66 × 10-8	2.83 × 10-8
Initial 6	2.80 × 10-8	2.76 × 10-8	2.93 × 10-8	2.79 × 10-8	2.76 × 10-8	2.62 × 10-8	2.71 × 10-8	2.85 × 10-8
Initial 7	2.73 × 10-8	2.76 × 10-8	2.82 × 10-8	2.79 × 10-8	2.71 × 10-8	2.77 × 10-8	2.55 × 10-8	2.83 × 10-8
Initial 8	2.73 × 10-8	2.79 × 10-8	2.85 × 10-8	2.78 × 10-8	2.75 × 10-8	2.74 × 10-8	2.73 × 10-8	2.64 × 10-8
B.2.3	Closest Minimum for Different Initilizations and Different Mirrors
Now we assess the pairwise distances between different initial points and final points (global min-
ima) obtained by all different initilizations and all different mirrors (Table 8). The smallest element
in each row is exactly the final point obtained by that mirror from that initialization, among all the
mirrors and all the initial points.
26
UnderreVieW as a COnferenCe PaPer at ICLR 2020
lɑl
lol
lol
lol
lol
lol
lol
lol

)41
L46
›23
)21
L14
L73
>06
LZ2
!.73
-
-
),67

385
)99
H3
384
?95
?93
:12_
L.92
i.71
0.22
L47
)66
J51
}86
)17
525
590
»30
)39
L56
J90
?35



791

34
56
10
15
J44

2
42
70
44
57
89

80.1
9
.87
3
Joo
J6o
?62
S52

J40
162
505
?53

)22
597

366
侬

93099584
047051134)8.8
15108
0 16 2
67.3 4
55巧5455
6 7 5
8 3 2
49387954

9 117
Q3.3.o.
386
?94_
L.46

),07
599

-
),44
,27

505
741
≡

)56
67
62
24
05
62
)73
>93
547_
J29
506

⅛17.2


W.2
5.33
,11
-
L05

2
16.
33
5
.9
.87
),01
-
川
I■
.26

'.oil
i.99
,15
),03
1.92
L07
5.45
),83
,19

,18
∙92l
),85
-
.88.29

i.03


!.73一
19:
19:
5
5
2
8
6
3
437
2
6
3
.22
,919
,88
,919
,04
,84

319
,81
2.7
197.;
.48
197.;
.51
197.
.51
L17

19
197.
85
87
6
88888888
E-0E-0E-0E-0E-0E-0E-0E-0
49996033
6 78|7!一7877
ZZZZZZ ZZ
-0-0-0-0-0-0-0-0
Eeeeeeee
22222222
55555555
55555555
E-0E-0E-0E-0E-0E-0E-0E-0
4E4E4E.4E.4E4E4E4E
7
33
.61
5
ʒ
7.
33
.08
9
6
16.
33
2
8
4 4 4
6 17
18.86.14.
331233
4 7 2
.1.64
5.04
05.05.05.
13
3
55555555
E-0E-0E-0E-0E-0E-0E-0E-0
7E7E7E7E7E7E7E7E
55555555
.08
.23.07,05
L«
55555555
E-0E-0E-0E-0E-0E-0E-0E-0
3E3E3E3E3E3E3E3E
55555555
72449425
40-^34521
463665.7.4
77277777
22122222
3 3 3 3 3 3 3
■42
词
.02
9
8
10.
33
2
2
12.


19
.32
.75
,18
2 8 7 3
Z0.1.711.519.
2 2 L 2 8
8 8 7 8 2
2 2 5 2
.89.42,67
.09
.84
.25

L.96

.32.39,81
巫
i.67
E-nor-norr-norrE-nor
2222222 2-
1.97
orororororororor
nnnnnnnn
11111111
12345678
6 8 6 5
LZ3.9
7 9 3 6
.99
.96
.63
.91
131
,221,441,421
.99
.55,17
.63
,53
-711

3

12.
.21
97
7 2 6
4 9.1
7 7 8
4 2 4
7 2 9
12 5
.89.31.83
5 4 4
.6.6.1
9972,64
84.90.91.



36998474
17633669
66266665
3.3.53.3.3.3.3.
00300000
11511111
75945614
455553.55
3.43.3.3.o3.3.
03000100
15 111 1 1
55555555
E-0E-0E-0E-0E-0E-0E-0E-0
7.7.7.7.7.7.7.7.
,000,000,000,000,000,000,000,000
18
.8
■041
■041
■041
■041
■041
■041
■041
.04l
56931137
718334 7-9
34337.5572
77775317
5 5 5 5 1 0 5
Illl 1 1
89442567
40247024
.122.132.1.1
3.3.3.3.9.3.3.3.
11111111
13
0.3533330.
6αα0.660.6
16066661
Illlll
—1—1—l—l—l-l-I—
61815215
74390560
92Q9QQ9Q
7.4&7.&&7.&
50555555
11111111
1899233 7
3.218325 O-
19992.89.8.8_
1227222 2
7 7 1 7 7 7 7_
1 1 Ill 1 -
7 3 9 7 4-
62.85662.85685662,62,14.
528528285 5 5-
8282288 8
2 2 2 2 2
12
E+
2
4
12
E+
2
4
12
E+
2
4
12
E+
2
4
12
E+
2
4
12
E+
2
4
12
E+
2
4
12
E+
2
4
43
43
43
43
43
43
43
43
3.1)93
57239653
7.L5&7,5.7.&7.
5 灯 055555
13111111
34333333
4 4 4 4 4 4 4
22529114
4424L4.3.15
5055555”
22222229
13389180
Ilα9.9.156916
4316151543161543
/ 333/ 33/
4 - 4 - 4 4-4
-13
-13
-13
-13
-13
-13
-13
-13
1.2
1.2
1.2
1.2
1.2
1.2
1.2
1-2_
Figure 9: Different Bregman divergences between all the final points and all the initial points for different mirrors in CIFAR-IO dataset using ResNet-18. Note that
the smallest element in every single row is on the diagonal, which confirms the theoretical results.
Under review as a conference paper at ICLR 2020
Figure 10: An illustration of the experimental results. For each initialization w0 , we ran different
SMD algorithms until convergence to a point on the set W (zero training error). We then measured
all the pairwise distances from different w∞ to different w0 , in different Bregman divergences. The
closest point (among all initializations and all mirrors) to any particular initialization w0 in Bregman
divergence with potential ψ(∙) = ∣∣ ∙ kq is exactly the point obtained by running SMD with potential
k ∙ kq from wo.
B.3	Distribution of the Final Weights of the Network
One may be curious to see how the final weights obtained by these different mirrors look like,
and whether, for example, mirror descent corresponding to the 'ι-norm potential induces sparsity.
We examine the distribution of the weights in the network for these algorithms starting from the
same initialization. Fig. 11 shows the histogram of the initial weights, which follows a half-normal
distribution. Figs. 12 (a), (b), (c), (d) show the histogram of the weights for 'ι-SMD, '2-SMD
(SGD), '3-SMD, and '10-SMD, respectively. Note that each of the four histograms corresponds to
an 11 × 106 -dimensional weight vector that perfectly interpolates the data. Even though, perhaps
as expected, the weights remain quite small, the histograms are drastically different. The histogram
of the '1 -SMD has more weights at and close to zero, which again confirms that it induces sparsity.
However, as will be shown in the next section, this is not necessarily good for generalization (in fact,
it turns out that '10-SMD has a much better generalization). The histogram of the '2-SMD (SGD)
looks almost identical to the histogram of the initialization, whereas the '3 and '10 histograms are
shifted to the right, so much so that almost all weights in the '10 solution are non-zero and in the
range of 0.005 to 0.04. For comparison, all the distributions are shown together in Fig. 12(e).
Figure 11: Histogram of the absolute value of the initial weights in the network (half-normal distri-
bution)
28
Under review as a conference paper at ICLR 2020
ll-norm
160000
120000
140000
40000
20000
(a)
—,----------1-----. I	J 300000
I B 13-norm
250000
Absolute Value of Weights
(c)
0.05
0.02	0.03
Absolute Value of Weights
(b)
Ooo
Ooo
UoO
0 5 0
2 11
AUU(unbəlt
0.02
Absolute Value of Weights
0.03
(d)
300000
250000
ll-norm
l2-norm
Absolute Value of Weights
(e)
Figure 12: Histogram of the absolute value of the final weights in the network for different SMD
algorithms: (a) 'ι-SMD, (b) '2-SMD (SGD),(c) '3-SMD, (d) '10-SMD. Note that each ofthe four
histograms corresponds to an 11 × 106-dimensional weight vector that perfectly interpolates the
data. Even though the weights remain quite small, the histograms are drastically different. '1-SMD
induces sparsity on the weights, as expected. SGD does not seem to change the distribution of the
weights significantly. '3-SMD starts to reduce the sparsity, and '10 shifts the distribution of the
weights significantly, so much so that almost all the weights are non-zero.
29
Under review as a conference paper at ICLR 2020
B.4 Generalization Errors of Different Mirrors/Regularizers
In this section, we compare the performance of the SMD algorithms discussed before on the test set.
This is important for understanding the effect of different regularizers on the generalization of deep
networks.
For MNIST, perhaps not surprisingly, all the four SMD algorithms achieve around 99% or higher
accuracy. For CIFAR-10, however, there is a significant difference between the test errors of dif-
ferent mirrors/regularizers on the same architecture. Fig. 13 shows the test accuracies of different
algorithms with eight random initializations around zero, as discussed before. Counter-intuitively,
`10 performs consistently best, while `1 performs consistently worse. We should reiterate that the
loss function is exactly the same in all these experiments, and all of them have been trained to fit the
training set perfectly (zero training error). Therefore, the difference in generalization errors is purely
the effect of implicit regularization by different algorithms. This result suggests the importance of a
comprehensive study of the role of regularization, and the choice of the best regularizer, to improve
the generalization performance of deep neural networks.
(求)AuB-lmu< ISBj_
Figure 13: Generalization performance of different SMD algorithms on the CIFAR-10 dataset using
ResNet-18. `10 performs consistently better, while `1 performs consistently worse.
30