Under review as a conference paper at ICLR 2020
Finding Mixed Strategy Nash Equilibrium for
Continuous Games through Deep Learning
Anonymous authors
Paper under double-blind review
Ab stract
Nash equilibrium has long been a desired solution concept in multi-player games,
especially for those on continuous strategy spaces, which have attracted a rapidly
growing amount of interests due to advances in research applications such as the
generative adversarial networks. Despite the fact that several deep learning based
approaches are designed to obtain pure strategy Nash equilibrium, it is rather lux-
urious to assume the existence of such an equilibrium. In this paper, we present
a new method to approximate mixed strategy Nash equilibria in multi-player con-
tinuous games, which always exist and include the pure ones as a special case. We
remedy the pure strategy weakness by adopting the pushforward measure tech-
nique to represent a mixed strategy in continuous spaces. That allows us to gen-
eralize the Gradient-based Nikaido-Isoda (GNI) function to measure the distance
between the players’ joint strategy profile and a Nash equilibrium. Applying the
gradient descent algorithm, our approach is shown to converge to a stationary
Nash equilibrium under the convexity assumption on payoff functions, the same
popular setting as in previous studies. In numerical experiments, our method con-
sistently and significantly outperforms recent works on approximating Nash equi-
librium for quadratic games, general blotto games, and GAMUT games.
1	Introduction
Nash equilibrium (Nash, 1950) is one of the most important solution concepts in game scenario with
multiple rational participants. It plays an important role in theoretical analysis of games to guide
rational decision-making processes in multi-agent systems. With the recent success of machine
learning applications in games, it attracts even more research interests on applying machine learning
technique for unsolved game theory problems, for example, computation of Nash equilibrium for
multi-player games. In this paper, we focus on games with continuous action spaces, which include
the famous application for Generative Adversarial Networks (GANs) (Goodfellow et al., 2014),
as well as many important game types such as the colonel blotto game (Gross & Wagner, 1950),
Cournot competition (R, 1996). We develop a solution which significantly improves the status-quo.
There have been several successful approaches to compute Nash equilibrium for multi-player
(mostly 2-player) continuous game (Raghunathan et al., 2019; Balduzzi et al., 2018; Letcher et al.,
2018). These works seek Nash equilibria corresponding to pure strategies, in which each player
takes a specific action to achieve its best payoff given other players’ actions. A major concern for
such a solution concept is its possible non-existence. As a result, the convergences to a Nash equi-
librium for these approaches were proven under the assumption for the existence of a pure strategy
Nash equilibrium, which can hardly be checked in practice, and their applicability is limited to spe-
cific types of games. On the contrary, it is known that mixed strategy Nash equilibria always exist
under mild conditions. And note that any pure strategy Nash equilibrium is also a mixed strategy
Nash equilibrium, which means the latter one is a much more desired solution concept.
However, a key challenge that obstructs the study of computing a mixed strategy Nash equilibrium,
especially for a continuous game, lies on how to design an efficient method to represent the mixed
strategy. To be precise, a pure strategy can be represented by a single variable choosing from some
region. But as a distribution on each player’s action space, a mixed strategy with respect to the player
is defined in a (subspace of) real space R. More generally, exact representation for a mixed strategy
of a player usually requires many variables in a continuous space. In addition, the corresponding
probability distribution may not have a density function in closed-form.
1
Under review as a conference paper at ICLR 2020
To address this challenge, we introduce a pushforward measure technique. It is a common tool in
measure theory to transfer a measure to some specific measure space (Bogachev, 2007). Specific to
a continuous game, the probability distribution corresponding to a mixed strategy is obtained via a
mapping parameterized by neural nets from a multi-dimensional uniform distribution.
With this pushforward representation, we generalize the Gradient-based Nikaido-Isoda (GNI) func-
tion introduced by Raghunathan et al. (2019), to handle mixed strategy Nash equilibria. The original
GNI function can be viewed as a measure for the distance between any joint strategy profile and a
Nash equilibrium after applying the payoff functions of players. With proper generalization and
modification, we develop its mixed strategy version as a proper measure for a Nash equilibrium. We
prove that the distance becomes zero if and only if a stationary mixed Nash equilibrium is obtained.
Then we apply the gradient descent algorithm to the general GNI function, which converges to a
stationary mixed Nash equilibrium under the convexity assumptions on the payoff functions.
Finally, we compare our method with baseline algorithms in numerical experiments. Our approach
shows effective convergence property in all the randomly generated quadratic games, general blotto
games, GAMUT games and Delta-Dirac GAN (in appendix), which outperforms other baselines.
2	Background and Problem Description
The discrete action space Nash equilibrium computation has been most widely studied in the litera-
tures. Most well-known being the Lemke-HoWson algorithm (Lemke & Howson, 1964) for solving
the bimatrix game. The state-of-art theoretical work of Tsaknakis & Spirakis (2007) provided a so-
lution of 1/3 approximation in polynomial time . And empirical work (Fearnley et al., 2015) shows
it also performs well against practical game solving methods for the bimatrix game.
However, the continuous action space game computation is widely used in practice, while few meth-
ods are known for the general Nash equilibrium computation and all restricted to pure strategies. The
GNI function is considered by Raghunathan et al. (2019) as one possible technique for multi-player
continuous game, but they only consider pure strategies. With further assumption that the utility
functions of players are twice continuous differentiable, gradient-based algorithms are developed
(Balduzzi et al., 2018; Letcher et al., 2018). But they do not provide a way to parameterize mixed
strategy of continuous action space, and even we are able to do so through neural networks, the
Hessian of utility functions, necessary in these approaches, is extremely hard to compute or store.
Game-theoretical approach has had useful applications to machine learning such as the optimization
of GAN network training (Daskalakis et al., 2017; Gidel et al., 2018) and adjustment on the gradient
descent method (Balduzzi et al., 2018). However they are limited to 2-player zero-sum games.
We are the first work to study the mixed strategy continuous game Nash equilibrium computa-
tion. Our work is motivated by the utilization of the Nikaido-Isoda (NI) function for loss function
minimization (Uryas’ ev & Rubinstein, 1994; Raghunathan et al., 2019). We start to establish a
theoretical formulation of the extend mixed strategy continuous action space Nash equilibrium as a
result of the minimization on a functional variation-based Nikaido-Isoda function.
2.1	Continuous Game Nash Equilibrium
Find x* = (x；,x；,... ,xN)
s.t. x* = arg UKmmin * fi(X)	⑴
x∈Rnix-i=x-i
Here N denotes the number of players, and xi ∈ Rni the strategy of the i-th player where ni is the
dimension of its action space. Let n = PN=I n% and X = (x1, x2, ∙∙∙ ,xn) ∈ Rn denotes the joint
pure strategy among all players while xf = (xi, ∙∙∙ , Xi-ι, χi+ι,…,XN) ∈ Rn-ni thejoint pure
strategy among players except i. fi : Rn → R denotes the utility function (cost) of i-th player. A
solution x* to (1) is called a pure strategy Nash equilibrium.
2.2 Nikaido-Isoda (NI) Function
In the paper by NikaidO et al. (1955), Nikaido-Isoda (NI) function is introduced as:
NN
φ(X) =	X	(fi(χ)	- .pTOminf fi(X))	, Xφi(X)	⑵
x∈R	X	χ∈Rn：χ-i=χ-i ) Y
i=1	i=1
2
Under review as a conference paper at ICLR 2020
From the Equation (2), we know φ(x) > 0 for ∀x ∈ Rn, and φ(x) = 0 is the global minimum ofNI
function which can only be achieved at a Nash equilibrium (NE). Therefore, a common algorithm
of computing NE points is minimizing the NI function above. However, it is a huge difficulty to
handle the global infimum. On one hand, the global infimum can not be obtained in finite time.
On the other hand, the infimum can be unbounded below in some games, for example the two-
player bi-linear games, where f1 (x) = x1T Mx2 = -f2 (x). All of the facts above show us the
shortcomings of NI function, and in order to rectify them, Raghunathan et al. (2019) introduce the
following Gradient-based Nikaido-Isoda (GNI) function.
2.3 Gradient-based Nikaido-Isoda (GNI) Function
Ifwe calculate local infimum in the NI function φ(x) instead of global infimum, the time complexity
and unbounded infimum are no longer shortcomings. To be precise, given the local radius λ, local
infimum can be approximated by steepest descent direction, and we get the following GNI function:
N
V(x; λ) = E (fi(X)- fi(xi,…，Xi-1,Xi - λVifi(x),Xi+ι, ∙∙∙ ,XN))
i=1
By minimizing V(x, λ), a stationary Nash point x*, where Vxifi(x*) = 0 for ∀i, can be approx-
imated efficiently. Furthermore, if all the utility functions fi are convex, then the stationary Nash
points (SNP) obtained are actually Nash Equilibrium (NE).
3 (Mixed-GNI) Gradient-based Nikaido-Isoda Function of Mixed
Strategy on Continuous Games
In this section, we are going to introduce our novel Gradient-based Nikaido-Isoda function of mixed
strategy on continuous games (Mixed-GNI), which is used to get an approximated solution of the
following optimization problem.
Find π* = (∏^,∏2,…，∏N)
s.t. π* = arg min E	fi(x1,x2,…,xn)	(3)
n：n —i = π- i xj ~∏j , ∀j
Before we solve this optimization problem, there is another fundamental question, which is how we
should represent (or parametrize) a distribution πi . The simplest way to do so is to parametrize its
density function. However, not every distribution has its density function, such as Dirac distribution,
and it will be inconvenient for us to do sampling from only a density function. Therefore, we
introduce another way, adopting the pushforward measure to represent a distribution.
Given a distribution μo and a mapping g(∙), data X drawn from μo can be transported into a new
distribution μι (constituted by g(x)). Technically speaking, μι is called the pushforward measure
of μo by mapping g, denoted by μι = g#(M0).
Here, for ∀j ∈ [N], we prepare each distribution πj a corresponding pushforward function gj :
Rd → Rnj , and we have:
πj = gj# (U)
where U stands for the uniform distribution on [0, 1]d. Each time we want to sample from distribu-
tion πi, we only need to sample several ωi ∈ [0, 1]d from distribution U and calculate gi(ωi). Then,
these gi(ωi) form a sample set from distribution πi. And optimization problem (3) becomes:
Find g* = (g；,g；,…，gN)
s.t. g* = arg min E	fi(g1(ω1),g2(ω2),…，gNQn))	(4)
g：g-i=g-i ωj∙~U, ∀j
To solve the optimization problem above, we consider the following Gradient-based Nikaido-Isoda
function of Mixed strategy on Continuous games (Mixed-GNI), generalized from the GNI function
introduced above, and we call this function V the local regret:
N
V (g1,g2,…，gN; λ) = ɪ2 Fi(g1, g2,…，gN ) - Fi(g1,…，gi-1,gi - λδgi Fi,…，gN )
i=1
N
EVi(g1,g2,…，gN； λ)
i=1
(5)
3
Under review as a conference paper at ICLR 2020
Here, δgi Fi stands for the 1-st order variation of functional Fi on element function gi and
Fi(gl,g2,…，gN ) = E	[fi(gl(ω1), gt2(ω2),…，gN (ωN ))]
ωj ~U, ∀j
By minimizing the functional V(g1,g2,…，gN； λ), We can approximately get stationary Nash
points (SNP), and even get Nash equilibrium if all the utility functions fi are convex. We will
prove them in the next section.
In practice, we further parametrize these pushforward functions as: gi(∙) = gi(∙,θi), to efficiently
calculate derivatives instead of variations. For simplicity, We denote gi as gθi. In order to obtain a
better expressibility, we use neural networks as the architecture to parametrize these pushforward
functions. Then, Mixed-GNI function V can be transformed to:
N
V (gθι ,gθ2 , ∙∙∙ ,gθN ; λ) =	Fi(gθι ,gθ2 , ∙∙∙ ,gθN ) - Fi(gθι,…，gθi-1 ,gθi-λ∂θi Fi , ,…，gθN )
i=1
Finally, the Mixed-GNI function can be estimated by sampling the points from distribution U as
an estimator of these Fi and minimized by implying gradient descent on these function parameters
θi, i ∈ [N], the convergence of which is proved in the next section.
4 Theoretical Analysis of Mixed-GNI
4.1 The Sufficient and Necessary Condition of S tationary Nash Point
As a mixed strategy of an N-player continuous game, (∏1,∏2,…，∏n) = (g#U, g#U,…，g#U)
is a stationary Nash point (SNP) if and only if for ∀i ∈ [N], the 1-st order variation
δgi (Fi)[σ(x)] = 0	(6)
holds at each direction σ(x). Here:
Fi(g1,g2,…，gN) = E	[fi(g1(ω1 ),g2(ω2), ∙∙∙ ,gN (ωN))]
ωj 〜U, ∀j
is the expectation of the i-th player’s utility with the form of N -variable functional.
compute the variation above and deduce the sufficient and necessary condition of SNP.
δgi (Fi)[σ(x)] = lim — (Fi(g1, g2,…，gN) - Fi(g1, ∙∙∙ ,gi -9…,gN))
→0
= E	[σ(ωi)T ∙ Vifi(gι(ω1),g2(ω2),…，gN(ωN))]
ωj~U, ∀j
=E [σ(ωi)T ∙ E	[Vifi(gι(ωι), g2(ω2),…，gN(3n))]]
32〜U	ωj 〜U, ∀j=i
Now, we
(7)
E	[σ(ωi) ∙ G(ωi)] = (
ωi 〜U	√[0,1]d
σ(ωi) ∙ G(ωi)dωi
where:
G(ωi) =	E	Vifi(gl(31), g2(ω2),…，gN(3N))]
ωj 〜U, ∀j=i
For SNP, Equation (6) holds at each direction σ(x), i.e. G(3i) ≡ 0. Therefore, we have
Theorem 1.	π = (∏ι, ∏2,…,∏n ) = (g#U, g# U,…,g# U) is a stationary Nash point (SNP) for
an N -player continuous game if and only if:
E	JVifi(g1(31),g2(32), ∙∙∙ ,gN(3n))] ≡ 0,	∀3i ∈ Rd
ωj 〜U, ∀j=i
holds for all i ∈ [N].
From Equation (7), we also know that:
δgi(Fi)[σ(3i)] = hG(3i), σ(3i)i
In other words, the steepest direction is:
δgi (Fi) = G(ωi) = E	[vifi(gl(31),g2(32 ),…，gN (ωN ))]
ωj 〜U, ∀j=i
Then we show the relationship between stationary Nash point and Nash equilibrium.
Theorem 2.	Denote SSNP, SNE as the set of SNPs and NEs ofa particular N -player continuous
game. Obviously, SNE ⊆ SSNP. If all utility functions fi are convex, we have: SNE = SSNP
4
Under review as a conference paper at ICLR 2020
Proof. Suppose π = (∏1,∏2,…，∏n) = (g#U, g#U,…，g#U) is an SNP, We will prove it an NE
when all functions fi are convex. According to the convexity and the condition of SNPs, we know
that for ∀i ∈ [N] and any other pushforward function gi：
Fi(g1,g2,…，gN) - Fi (g1,…，gi,…，gN)
= E	[fi(gl(ω1), ∙∙∙ , gi(ωi), …，gN (ωN)) - fi(gl(ω1), g2(ω2), …，gN (ωN))]
ωj ~U, ∀j
> E	[(gi(ωi) - gi(ωi))τ ∙ Vifi(gι (ω1),g2(ω2), …，gN(ωN))]	2、
3j~u, ∀j	(8)
=E [(gi(ωi) - gi(ωi))τ ∙ E	[Vifi(gι(ωι), g2(ω2),…，gN("n))]]
32〜U	ωj 〜U, ∀j=i
=E [(gi(ωi) - gi(ωi))T ∙ δgi (Fi)] = 0
3i〜U
which leads to our conclusion, thatπ = (g# U, g^ U,…，g# U) is a global Nash equilibrium. □
Next, we show the relationship between the zeros of Mixed-GNI function V(gi, g2,…，gN) and
SNPs of the N -player continuous game.
Lemma 1. Assume f : Rd → R is a twice differentiable function, and its 1-st order gradient Vf is
Lf -Lipschitz continuous. Then for ∀x, y ∈ Rd, we have:
|f(y) - f(X) - hVf(X),y - xi| 6 1 Lf ky - χk2
Proof. According to the condition of f, there holds the following equations.
|f (y) - f(X) - hVf (X), y -Xi
1
hVf (X + τ(y - X)) - Vf (X), y - Xidτ
6 Z KVf(X + T (y - X))-Vf(x),y - x)| dτ
0
6 [ kVf(x + τ(y - X))-Vf(x)k ∙ ky - XkdT
0
11
6 JJ LfTky - Xk2dT = 2Lf ky -Xk2
(9)
□
With this lemma, we can show that each global minimum of V(g1,g2,…，gN) is also an SNP.
Theorem 3.	If each utility function fi is twice differentiable and its 1-st order gradient Vfi is
Lf -Lipschitz continuous. Then:
2 kδgi Fi (g1,g2, ... ,gN )k2 6 Vi(g1,g2, ... ,gN; λ) 6 -2- kδgi Fi(g1,g2,…，gN )k2
holds when 0 < λ 6 LJ. Here, k∙k2 is afunctional norm which means:
kfk2 = Z	kf(ωi)k22dωi= E kf(ωi)k22
J[0,1]d	ωi 〜U
Proof.
Vi(gi,g2, ∙∙∙ ,gN； λ)
=Fi(gl,g2, ∙∙∙ ,gN) - Fi(gi,…，gi - λδgiFi, ∙∙∙ ,gN)
= E	[fi(g1(ω1),g2(ω2),…，gN(ωN)) — fi(gi(ωι),…，gi(ωi) — λδgiFi(ωi),…，gN(ωN))]
ωj 〜U, ∀j
(10)
Then, according to Lemma 1:
¼(gi,g2, ••. ,gN ； λ)
6 E	λ(δgi Fi(ωi))τ Vifi(gl(ω1),g2(ω2), …，gN (ωN )) + ~f λ2kδgi Fi(ωi)k2
ωj 〜U, ∀j	2
=λ E kδgiFi(g1,g2,…，gN)(ωi)k2 + -f λ2 E kδgiFi(g1,g2,…，gN)(ωi)k2
3i 〜U	2	3i 〜U
3λ
6 ^2 kδgi Fi(g1,g2,…，gN )k
(11)
5
Under review as a conference paper at ICLR 2020
And the other side of this inequality is similar.
□
The theorem above tells Us that, V(gι, g2,…，gN; λ) is always non-negative as long as λ 6 吉.
And its global minima, or in the other words, its zeros, are surely SNPs, because for ∀i ∈ [N]:
Vi(g1,g2, ... ,gN ； λ) = 0 ⇔ δgi Fi(g1,g2,…，gN) = 0
Finally, we analyze the stability of SNPs. In the following theorem, we show that the 2-nd order
variation of fUnctional V is a positive semidefinite operator, which confirms the stability of SNPs.
Theorem 4. The 2-nd order variation δ2V (g*; λ) is a positive Semidefinite operator for ∀g* ∈
SSNP and 0 6 λ 6 古.
Proof. The 1-st and 2-nd order variation of Vi (g; λ) satisfy:
δVi(g; λ) = δFi(g) - δFi(g) + λ δ2Fi(g)DiδFi(g),
where g = (g1,g2,…，gN),g = (gι,…，gi-ι,gi — λδgiFi,…，gN) and
(12)
Di = Diag(0n
1 ×n1 ,
0ni-ι ×ni-ι ,Ini×ni , 0ni+ι ×ni+ι,…，QnN ×nN )
is a n × n matrix. Given g* ∈ SSNP, then δFi(g*) = 0.
δ2Vi(g*; λ) = λ δ2Fi(g*)[2Di - λDiδ2Fi(g*)Di]δ2Fi(g*)
λ δ2Fi(g*)[2Di - λLf Di2]δ2Fi(g*)
λ δ2Fi(g*)Diδ2Fi(g*)
= λ (δ2Fi(g*)Di)T (δ2Fi(g*)Di)
which is positive semidefinite. Therefore:
(13)
N
is also positive semidefinite.
δ2V(g*; λ) = ]Tδ2¼(g*; λ)
i=1
□
4.2 Convergence Analysis
In this section, we analyze the convergence analysis of gradient descent:
g(k+1) = g(k) — P ∙ δV(g(k); λ)
According to the definition of fUnctional V(g; λ), it can be rewritten as the following form:
V(g; λ) = E	[Gv(g1(ω1),g2(ω2),…，gN(ωN))]
ωj〜U, ∀j∈[N]
where GV = PN=I fi(y1,y2,…，yN) — fi(yι, ∙ ∙ ∙ ,yi-ι,yi — λ^ ifi(y1,y2, ∙∙∙ ,yN), ∙ ∙ ∙ ,yN).
Theorem 5. Suppose VGv (x) is LG-Lipschitz Continuous.Through gradient descent, the function
Sequence g(k) converges SubIinearIy to a stationary Nash point (SNP) g* if P < 亡,λ 6 古.
Proof. According to Lemma 1, we have:
V(g(k+1) ； λ) 6 V(g(k)； λ) - “i,号 ∈N] [P VGV ((gw g2(ω2), ^^
,gN(ωN)) ∙ δV(g(k); λ)]
+ E	LGP2kδV(g(R λ)k2
“j〜U, ∀j∈[N] 2
=V(g(k)； λ)-(ρ- LGPP) kδV(g(R λ)k2
=V(g(k);λ) - (2ρLG-(PLG)2) kδV(g(Rλ)k2
2LG
Let k = 0,1,…，K, and add them up, We have:
V (g(K+1); λ) 6 V (g(0); λ) -
2ρLg - (PLG)2
2Lg
K
X kδV (g(k); λ)k2
k=0
(14)
6
Under review as a conference paper at ICLR 2020
Since λ 6 吉,we know that V(g(K+1); λ) > 0 by Theorem 3, We have
X kδV(g(k)； λ)k2 6 (2ρL02‰2 W)； λ)
⇒ min kδV(g(k)；λ)∣∣2 6 (	2L：	) V；J
k∈[K]	2ρLG - (ρLG)2	K + 1
which completes our proof.
(15)
□
5 Experiments
To evaluate the practical performance of our approach, we apply it to three types of games, two-
player quadratic games, general blotto games, and GAMUT games, the most popular games for
evaluation of Nash equilibrium algorithms. In all the experiments, we set the local radius λ = 1e-3
and we use gradient descent as our optimization method with step size ρ = 1e-2 and momentum
κ = 0.9. The network architecture we use for the pushforward functions gθ is a 6-layer fully
connected neural network with the size of each layer as: 20, 40, 160, 160, 40, 20. The size of its
output layer is the dimension of each player’s action space. From forward to backward, the activation
function we use is: tanh, tanh, tanh, ReLU, tanh, tanh.
We mainly compare our approach with three recent studies, gradient descent for GNI function
(Raghunathan et al., 2019) (gradGNI in short), Symplectic Gradient Adjustment algorithm (Bal-
duzzi et al., 2018) (SGA in short), and Stable Opponent Shaping (Letcher et al., 2018) (SOS in
short) as they outperformed other existing algorithms applicable to continuous game settings. For
all these methods, we either follow the standard hyper-parameters mentioned in the original papers,
or the ones resulting in the best convergence.
5.1	Two-player Quadratic Game
The two-player quadratic game is defined by the the players’ payoff functions fi (i = 1, 2):
fi(x) = xT Qix + riT x,	(16)
where Qi ∈ R(n1 +n2)×(n1 +n2), ri ∈ Rn1 +n2, x = (x1, x2) and xi ∈ Rni. In our experiments,
we choose n1 = n2 ∈ {3, 5, 10}. For each pair of ni, we randomly generate 100 instances for the
matrix Qi and ri for i = 1, 2. Each item in each matrix Qi and each vector ri follows the uniform
distribution on [0, 1] independently.
We show the converging process of all algorithms for one game instance (n1 = n2 = 3) in Fig. 1(a)
as an example. As we can see, our approach effectively converges to a stationary Nash equilibrium
point. While gradGNI also converge in this instance, its result has larger local regret. In other words,
it obtain worse approximation to Nash equilibrium, which coincides with the essential difference
between pure strategy and mixed strategy. The Mixed-GNI approach searches for the equilibrium
in the mixed strategy space, which includes the pure strategy space that gradGNI searches in. On
the other hand, SGA and SOS diverge in this game instance. We further take the average of the final
local regrets after 2000 iterations for all the 100 instances, summarized in Tab. 1. All the algorithms
show consistency as the dimension of action space increases, and Mixed-GNI outperforms others
regardless of the randomness of game structures.
5.2	General Blotto Game
We next consider the general blotto game, which differs from previous games in the action space of
each player for which further constraints apply.
In a blotto game, player 1 and 2 (sometimes known as two colonels) have a budget of resource X1,
X2 respectively. W.l.o.g we set X1 ≤ X2 . There are m battlefields in total. In each battlefield j,
when two players allocate x1j , x2j resource on it, the payoff of player i is:
Uij = f (xij -x-ij), where f(χ) = tanh (χ),	(17)
where -i denotes the player other than player i. Each player’s payoff across all m battlefields is
the sum of the payoffs across the individual battlefields. For each player i, a feasible pure strategy
xi = (xi1, . . . , xim) ∈ R+m must also satisfies Pjm=1 xij ≤ Xi. Here we adopt the generalized
7
Under review as a conference paper at ICLR 2020
(a) ni = 3, 2-player quadratic (b) m = 3, 2-player blotto	(c) ni = 3, 4-player gamut
Figure 1: Local Regret of Various Games.
	MiXed-GNI (ours)	gradGNI	SGA	SOS
Quadratic (ni = 3)	(1.63 ± 1.20)e-3	(1.01 ± 0.03)e-1	2.59 ± 0.17	1.86 ± 0.36
Quadratic (ni = 5)	(2.84 ± 1.95)e-3	(2.95 ± 0.19)e-1	3.92 ± 0.22	2.89 ± 0.37
Quadratic (ni = 10)	(3.76 ± 3.02)e-3	(1.47 ± 0.08)e-1	2.54 ± 0.09	2.46 ± 0.12
Blotto (m = 3)	(6.32 ± 4.97)e-6	(2.62 ± 0.38)e-5	(5.26 ± 0.91)e-5	(7.31 ± 0.82)e-5
Blotto (m = 5)	(4.52 ± 3.09)e-6	(1.10 ± 0.06)e-5	(1.21 ± 0.18)e-5	(7.39 ± 2.33)e-6
Blotto (m = 10)	(3.62 ± 2.39)e-6	(7.60 ± 0.49)e-6	(5.94 ± 0.26)e-6	(5.02 ± 0.28)e-6
GAMUT (ni = 3)	(4.95 ± 0.42)e-3	(4.80 ± 0.81)e-1	(0.94 ± 0.13)e-1	(0.96 ± 0.17)e-1
GAMUT (ni = 5)	(8.90 ± 0.79)e-3	(1.52 ± 0.27)e-1	(2.59 ± 0.60)e-1	(2.67 ± 0.81)e-1
GAMUT (ni = 10)	(1.54 ± 0.86)e-2	(1.84 ± 0.48)e-1	(1.76 ± 0.32)e-1	(1.90 ± 0.47)e-1
Table 1: Comparison results.
blotto game proposed by Golman & Page (2009) with continuous payoff functions. The payoff
functions in vanilla blotto game is discontinuous, for which our method as well as baselines fails.
In our experiments, we set m ∈ {3, 5, 10}. For each m, we randomly generate 100 instance for the
budget Xi, following the uniform distribution on [0, 1] independently.
We show the converging process of all algorithms for one game instance (m = 3) in Fig. 1(b) as
an example. All the algorithms converges for this game, while gradGNI, SGA and SOS converge
faster and more smoothly comparing with our Mixed-GNI. However, similar to the quadratic game,
their final results have larger local regrets. This coincides with the fact that the mixed strategy is
a better solution concept than the pure strategy, especially in blotto games. We further take the
average of the final local regrets after 2000 iterations for all the 100 instances, summarized in Tab.
1. All the algorithms show consistency as the dimension of action space increases, and Mixed-GNI
outperforms others regardless of the randomness of game structures.
5.3	GAMUT GAMES
Finally, we apply our method on the game instance generated by the comprehensive GAMUT
suite of game generators designated for testing game-theoretic algorithms (Nudelman et al., 2004).
GAMUT includes a group of random distributions, based on each of which the payoff of each player
for each pure strategy profile can be drawn independently. To be precise, we extend the quadratic
game to a multi-player version, where ri = 0, and 100 game instances with 4 players are generated.
For each instance, one of the distributions from the GAMUT set is selected, and each item in each
matrix Qi is sampled according to it independently.
We show the converging process of all algorithms for one game instance in Fig. 1(c). Both Mixed-
GNI and SGA converge, but SGA has a much worse final result than our Mixed-GNI. And this time,
gradGNI diverges while SOS converges much slower. Furthermore, we take the average of the final
local regrets after 2000 iterations for all the 100 instances, shown in Table 1.
From these different games, we know that our Mixed-GNI converges and performs better than two
baselines in all of the three games, which shows the effectiveness and efficiency of our Mixed-GNI
model. As the first algorithm to compute the mixed strategy Nash equilibrium of games with con-
tinuous action space, we believe that the technique we introduced here will enable new optimization
researches of many exciting interaction domains of algorithmic game theory and deep learning.
8
Under review as a conference paper at ICLR 2020
References
David Balduzzi, Sebastien Racaniere, James Martens, Jakob Foerster, Karl Tuyls, and Thore Grae-
pel. The mechanics of n-player differentiable games. arXiv preprint arXiv:1802.05642, 2018.
Vladimir I Bogachev. Measure theory, volume 1. Springer Science & Business Media, 2007.
Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Training gans with
optimism. arXiv preprint arXiv:1711.00141, 2017.
John Fearnley, Tobenna Peter Igwe, and Rahul Savani. An empirical study of finding approximate
equilibria in bimatrix games. In International Symposium on Experimental Algorithms, pp. 339-
351. Springer, 2015.
GaUthier GideL Hugo Berard, Gaetan Vignoud, Pascal Vincent, and Simon Lacoste-Julien.
A variational inequality perspective on generative adversarial networks. arXiv preprint
arXiv:1802.10551, 2018.
Russell Golman and Scott E Page. General blotto: games of allocative strategic mismatch. Public
Choice, 138(3-4):279-299, 2009.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems, pp. 2672-2680, 2014.
Oliver Gross and Robert Wagner. A continuous colonel blotto game. Technical report, RAND
PROJECT AIR FORCE SANTA MONICA CA, 1950.
Carlton E Lemke and Joseph T Howson, Jr. Equilibrium points of bimatrix games. Journal of the
Society for industrial and Applied Mathematics, 12(2):413-423, 1964.
Alistair Letcher, Jakob Foerster, David Balduzzi, Tim Rocktaschel, and Shimon Whiteson. Stable
opponent shaping in differentiable games. arXiv preprint arXiv:1811.08469, 2018.
John F Nash. Equilibrium points in n-person games. Proceedings of the national academy of
sciences, 36(1):48-49, 1950.
Hukukane Nikaido, Kazuo Isoda, et al. Note on non-cooperative convex games. Pacific Journal of
Mathematics, 5(Suppl. 1):807-815, 1955.
Eugene Nudelman, Jennifer Wortman, Yoav Shoham, and Kevin Leyton-Brown. Run the gamut:
A comprehensive approach to evaluating game-theoretic algorithms. In Proceedings of the Third
International Joint Conference on Autonomous Agents and Multiagent Systems-Volume 2, pp.
880-887. IEEE Computer Society, 2004.
Varian Hal R. Intermediate microeconomics: a modern approach, 1996.
Arvind U Raghunathan, Anoop Cherian, and Devesh K Jha. Game theoretic optimization via
gradient-based nikaido-isoda function. arXiv preprint arXiv:1905.05927, 2019.
Haralampos Tsaknakis and Paul G Spirakis. An optimization approach for approximate nash equi-
libria. In International Workshop on Web and Internet Economics, pp. 42-56. Springer, 2007.
Stanislav Uryas’ ev and Reuven Y Rubinstein. On relaxation algorithms in computation of nonco-
operative equilibria. IEEE Transactions on Automatic Control, 39(6):1263-1267, 1994.
9
Under review as a conference paper at ICLR 2020
A Experiments on Delta-Dirac GANs
Gidel et al. (2018) introduce a one-dimensional GAN where the real data follows a Dirac-Delta
distribution. This experiment is also conducted in our baseline paper by Raghunathan et al. (2019).
As a two-player game, the payoff functions for the two players are listed as follows:
f1(x1,x2) = log(1 + θx1) + log(1 + exp(x1x2))
f2(x1, x2) = - log(1 + exp(x1x2))
(18)
where the θ is the location of the Dirac spike. In Figure 2, we show the converging process of
all algorithms for one game instance (i.e. for a single θ). As we can see, our Mixed-GNI and
original gradGNI algorithms converge while the other two diverge. Compared with gradGNI, our
approach is converging much faster. We further take the average of the final local regrets after 2000
iterations for all the 100 instances (i.e. 100 different θs) and show the results in Table 2. In this
experiment, our Mixed-GNI algorithm again outperforms the others, regardless of the randomness
of game structures.
——Mixed-GNI
---gradGNI
-SGA
——SOS
iteration rounds
Figure 2: Local Regret of Dirac-Delta GAN Game.
MiXed-GNI (ours)	gradGNI	I	SGA	I	SOS
DDaC-DeltaGAN ∣ (7.23 ± 3.70)e-5 ∣	(6.52 ± 3.98)e-3	I (1.39 ± 0.96)e2∏	(1.41 ± 0.87)e2-
Table 2: Comparison Results
10