Under review as a conference paper at ICLR 2020
On the Approximation Errors of Node Sam-
pling for Graph Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
The recent advancements in graph neural networks (GNNs) have led to state-of-the-
art performances in various applications, including chemo-informatics, question-
answering systems, and recommender systems. However, scaling up these methods
to huge graphs such as social network graphs and web graphs still remains a
challenge. In particular, the existing methods for accelerating GNNs are either not
theoretically guaranteed in terms of approximation error, or they require at least
a linear time computation cost. In this study, we reveal the query complexity of
the uniform node sampling scheme for GraphSAGE, the graph attention networks
(GAT), and the graph convolutional networks (GCN). The key advantage of our
analysis is that the complexity is completely independent of the numbers of the
nodes, edges, and neighbors of the input and depends only on the error tolerance and
confidence probability while providing theoretical guarantee for the approximation
error. To the best of our knowledge, this is the first work to give a theoretical
guarantee of approximation for GNNs within a constant time complexity. Through
experiments using synthetic and real-world datasets, we demonstrate the speed and
precision of the node sampling scheme and validate our theoretical results.
1	Introduction
Machine learning on graph structures has various applications such as chemo-informatics (Gilmer
et al., 2017), question answering systems (Schlichtkrull et al., 2018), and recommender systems
(Fan et al., 2019). Recently, a novel machine learning model for graph data called graph neural
networks (GNNs) (Gori et al., 2005; Scarselli et al., 2009; Kipf & Welling, 2017; Hamilton et al.,
2017) demonstrated state-of-the-art performances in various graph learning tasks. However, large
scale graphs such as social network graphs and web graphs contain billions of nodes, and even a
linear time computation cost per iteration is prohibited. Therefore, applying GNNs to huge graphs is
challenging. Although Ying et al. (2018) succeeded in applying GNNs to a web-scale network using
MapReduce, it still requires massive computational resources.
There are several node sampling techniques to reduce GNN computation. For example, an empirical
neighbor sampling scheme is used to speed up GraphSAGE (Hamilton et al., 2017). FastGCN
employs a random layer-wise node sampling (Chen et al., 2018b). Huang et al. (2018) further
improved FastGCN by using an adaptive sampling technique to reduce the variance of estimators.
Chen et al. (2018a) proposed a variant of neighbor sampling, which used historical activations to
reduce the estimator variance. Overall, the existing sampling techniques for GNNs work well in
practice. However, these techniques are either not theoretically guaranteed in terms of approximation
error, or thy require at least a linear time computation cost.
In this study, we consider the problem of approximating the embedding of one node using GNNs in
constant time with maximum precision1. We analyze the neighbor sampling technique (Hamilton
et al., 2017) to show that a constant number of samples are needed to guarantee the approximation
error. It should be noted that the neighbor sampling was introduced as a heuristic method originally,
and they did not provide any theoretical guarantees. Specifically, given an error tolerance ε and
1e.g., predicting whether a user of an SNS clicks an advertisement by GNNs in real-time (i.e., when the user
accesses). A user may have many neighbors, but GNNs must respond in limited time, which prohibits exact
computation. It motivates us to approximate the exact computation in limited time with theoretical guarantee.
1
Under review as a conference paper at ICLR 2020
Table 1: ✓ indicates neighbor sampling approximates the network in constant time. X indicates any
algorithm cannot approximate the network in constant time. ✓ in the Gradient column indicates the
error between the gradient of the approximated embedding and that of the exact embedding is also
theoretically bounded. ✓* needs an additional condition to approximate it in constant time.
Activation	GAT, GraPhSAGE-{GCN, mean}		GraphSAGE-pool	GCN	
	Embedding	Gradient		Embedding	Gradient
sigmoid / tanh	✓ Thm. 1	✓ Thm. 4	X Thm. 9	✓ Thm. 1	✓ Thm. 4
ReLU	✓ Thm. 1	X Thm. 8	X Thm. 9	✓* Thm. 1	X Thm. 8
ReLU + normalization	X Thm. 7	X Thm. 7	X Thm. 9	X Thm. 7	X Thm. 7
confidence probability 1 - δ, our analysis shows that the estimate Zv of the exact embedding Zv
d∂z	∂z
of a node V such that Pr[∣∣zv - zv ∣∣2 ≥ ε] ≤ δ and the estimate d∂Zv of the exact gradient ∂θ of
d∂ z	∂z
the embedding Zv with respect to the network parameters θ, such that Pr[∣ 需-∂∂θL IIf ≥ ε] ≤ δ
can be computed in a constant time. Especially, the uniform node sampling can approximate the
exact embedding and its gradients within O( £ (log ɪ + log δ )L-1 log 1) time, where L denotes the
number of layers. This complexity is completely independent of the number of nodes, edges, and
neighbors of the input, which enables us to deal with graphs irrespective of their size. Moreover, the
complexity is a polynomial with respect to j and log 1. We demonstrate that the time complexity is
optimal when L = 1 with respect to the error tolerance ε.
Through experiments, we show that the approximation error between the exact computation and its
approximation rapidly converges to zero. To the best of our knowledge, this is the first constant time
approximation algorithm for GNNs with a theoretical guarantee in terms of approximation error.
Contributions: The contributions of this paper are summarized as follows:
•	We analyze the neighbor sampling technique for GraphSAGE, GAT, and GCN to provide
theoretical justification. Especially, our analysis shows that the complexity is completely
independent of the number of nodes, edges, and neighbors of the input.
•	We show that some existing GNNs, including the original GraphSAGE (Hamilton et al.,
2017), cannot be approximated in constant time by any algorithm (see Table 1 for details).
•	We empirically validate our theorems using synthetic and real-world datasets.
2	Related Work
2.1	Graph Neural Networks
Graph neural networks (GNNs) were first introduced by Gori et al. (2005) and Scarselli et al. (2009).
They obtained node embedding by recursively applying the propagation function until convergence.
Kipf & Welling (2017) proposed graph convolutional networks (GCN), which significantly outper-
formed the existing methods, including non-neural network based approaches. Gilmer et al. (2017)
proposed the message passing neural networks (MPNNs), a general framework of GNNs using the
message passing mechanism. VeIiCkOviC et al. (2018) proposed the graph attention networks (GAT),
which incorporate the attention mechanism into GNNs. With the advent of GAT, various GNN models
with the attention mechanism have been recently proposed (Wang et al., 2019; Park et al., 2019).
GraphSAGE (Hamilton et al., 2017) is another GNN model, which employs neighbor sampling to
reduce the computational costs of training and inference. Owing to neighbor sampling, GraphSAGE
can deal with large graphs. However, neighbor sampling was introduced without any theoretical
guarantee, and the number of samples is chosen empirically. An alternative computationally efficient
GNN would be FastGCN (Chen et al., 2018b), which employs layer-wise random node sampling
to speed up training and inference. Huang et al. (2018) further improved FastGCN by using an
adaptive node sampling technique to reduce the variance of estimators. Thanks to the adaptive
2
Under review as a conference paper at ICLR 2020
sampling technique, it reduces the computational costs and outperforms neighbor sampling in terms
of classification accuracy and convergence speed. Chen et al. (2018a) proposed an alternative neighbor
sampling technique, which uses historical activations to reduce the estimator variance. Additionally,
it could achieve zero variance after a certain number of iterations. However, because it used the
same sampling technique of GraphSAGE to obtain the initial solution, the approximation error was
not theoretically bounded until the Ω(n)-th iteration. Overall, the existing sampling techniques
work well in practice. However, these techniques are either not theoretically guaranteed in terms of
approximation error, or they require at least a linear time computation cost to calculate the embedding
of a node and its gradient of GNN models. Moreover, it is not clear whether we can apply the
sampling techniques to state-of-the-art GNN models such as GAT.
2.2	Sublinear Time Algorithms
The sublinear time algorithms were originally proposed for property testing (Rubinfeld & Sudan,
1996). Sublinear property testing algorithms check whether the input has some property π or the
input is sufficiently far from the property π with high probability in sublinear time with respect to the
input size. Sublinear time approximation algorithms are another type of sublinear time algorithms.
More specifically, they calculate a value sufficiently close to the exact value with high probability in
sublinear time. Constant time algorithms are a subclass of sublinear time algorithms. They work not
only in sublinear time with respect to the input size but also in constant time. The proposed algorithm
is classified as a constant time approximation algorithm.
The examples of sublinear time approximation algorithms include minimum spanning tree in metric
space (Czumaj & Sohler, 2004) and minimum spanning tree with integer weights (Chazelle et al.,
2005). Parnas & Ron (2007) proposed a method to convert distributed local algorithms into constant
time approximation algorithms. In their study, they proposed a method to construct constant time
algorithms for the minimum vertex cover problem and dominating set problem. A classic example
of sublinear time algorithms related to machine learning includes clustering (Indyk, 1999; Mishra
et al., 2001). Examples of recent work in this stream include constant time approximation of the
minimum value of quadratic functions (Hayashi & Yoshida, 2016) and constant time approximation
of the residual error of the Tucker decomposition (Hayashi & Yoshida, 2017). They adopted simple
sampling strategies to obtain theoretical guarantee similar to our work.
In this paper, we provide theoretical guarantee for approximation of GNNs within a constant time for
the first time.
3	Background
3.1	Notations
Let G be the input graph, V = {1, 2, . . . , n} be the set of nodes, n = |V| be the number of nodes,
E be the set of edges, m = |E | be the number of edges, deg(v) be the degree of a node v, N (v)
be the set of neighbors of a node v, xv ∈ Rd0 be the feature vector associated to a node v ∈ V ,
X = (x1, x2 . . . , xn)> ∈ Rn×d0 be the stacked feature vectors, and > denotes the matrix transpose.
3.2 Node Embedding Model
Algorithm 1 Oz : Exact embedding
We consider the node embedding problem
using GNNs. Especially, we employ the
message passing neural networks (MPNNs)
framework (Gilmer et al., 2017). This
framework includes many GNN models,
such as GraphSAGE and GCN. Algorithm
1 shows the algorithm of MPNNs. We re-
fer to zv(L) as zv simply. The aim of this
study is to develop a constant time approxi-
mation algorithm for calculating the embed-
ding vector zv and gradients dzv with the
given model parameters θ and node v .
Require: Graph G = (V , E ); Features X ∈ Rn×d0 ;
Node index v ∈ V ; Model parameters θ.
Ensure: Exact embedding zv
1:
2:
3:
4:
5
6
7
8
z(0) — Xi (∀i ∈ V)
for l ∈ {1, . . . , L} do
for i ∈ V do
h? J Pu∈N (i) Mliu(Zi	), zu	, e eiυ, θ)
ZTl- Ul(z(lτ), hil), θ)
end for
end for
(L)
return zv
3
Under review as a conference paper at ICLR 2020
3.3	Computational Model Assumptions
We have to specify how to access the input to design constant time algorithms because the constant
time algorithms cannot read the entire input. We follow the standard convention of sublinear time
algorithms (Parnas & Ron, 2007; Nguyen & Onak, 2008). We model our algorithm as an oracle
machine that can generate queries regarding the input, and we measure the complexity by query
complexity. Algorithms can access the input only by querying the following oracles: (1) Odeg(v): the
degree of node v, (2) OG(v, i): the i-th neighbor of node v, and (3) Ofeature(v): the feature of node v.
We assume that our algorithm can query the oracles in constant time per query.
3.4	Problem Formulation
Given a node v, we calculate the following functions with the least number of oracle accesses:
(1)Oz(v): the embedding Zv and (2) Og(v): the gradients of parameters ∂v. However, the
exact computation of Oz and Og needs at least deg(v) queries to aggregate the features from the
neighbor nodes, which is not constant time with respect to the input size. Thus, it is computationally
expensive to execute the algorithm for a huge network. Therefore, we consider making the following
approximations:
•	Oz(v, ε, δ): an estimate Zv of Zv such that Pr[∣∣Zi - Zik2 ≥ ε] ≤ δ,
•	Og(v, ε, δ): an estimate 裔 of 裔 such that Pr[k d∂zv - d∂zv∣∣f ≥ ε] ≤ δ,
where ε > 0 is the error tolerance, 1 - δ is the confidence probability, and ∣∣ ∙ ∣∣2 and ∣∣ ∙ ∣∣f are the
Euclidean and Frobenius norm, respectively.
Under the fixed model structure (i.e., the number of layers L, the message passing functions, and the
update functions), we construct an algorithm that calculates Oz and Og in constant time irrespective
of the number of nodes, edges, and neighbors of the input.
However, it is impossible to construct a constant time algorithm without any assumption about the
inputs. Therefore, we make the following mild assumptions:
Assumption 1 ∃B ∈ R s.t. ∣xi ∣2 ≤ B, ∣eiu ∣2 ≤ B, and ∣θ∣2 ≤ B.
Assumption 2 deg(i)Mliu and Ul are uniformly continuous in any bounded domain.
Assumption 3 (Only for gradient computation) deg(i)DMliu and DUl are uniformly continuous in
any bounded domain, where D denotes the Jacobian operator.
Building a constant time algorithm is impossible without these assumptions as shown in Section
5. Furthermore, we derive the query complexity of our algorithm when the message functions and
update functions satisfy the following assumptions.
Assumption 4 ∃K ∈ R s.t. deg(i)Mliu and Ul are K-Lipschitz continuous in any bounded domain.
Assumption 5 (Only for gradient computation) ∃K0 ∈ R s.t. deg(i)DMliu and DUl are K0-
Lipschitz continuous in any bounded domain.
4	Proposed Method
4.1	Constant Time Embedding Approximation
Here, we build a constant time approximation algorithm based on neighbor sampling, which approxi-
mates the embedding Zv with an absolute error of at most ε and probability 1 - δ. We recursively
construct the algorithm layer by layer by sampling r(l) neighboring nodes in layer l. We refer to
the algorithm that calculates the estimate of embeddings in the l-th layer Z(I) as Ozl(Il = 1,...,L).
The pseudo code is presented in Algorithm 2. Here, O(l-1)(v J U) means calling the function
Ozl-I) with the same parameters as the current function except for v, which is replaced by u. In the
following, we demonstrate the theoretical properties of Algorithm 2.
4
Under review as a conference paper at ICLR 2020
Algorithm 2 Ozl): Estimate the embedding Zvl)
Require: Graph G = (V, E) (as oracle); Features X ∈ Rn×d0 (as oracle); Node index v ∈ V;
Model parameters θ; Error tolerance ε; Confidence probability 1 - δ .
Ensure: Approximation of the embedding zv* (l) 2
1:	S(I) J sample r(l) (ε, δ) neighbors of V with uniform random with replacement.
Pu∈S(l) Mlvu (Ofeature (v), Ofeature (u), eiu , θ)	(l = 1)
Pu∈S(i) Mlvu(O(D(V J V), O( "(V J U), eiu, θ)	(I > I)
3:	Zvl) J Ul(OzlT)(V J v), hvl), θ) if l> 1 otherwise Ul(OfeatUre(v), hvl), θ)
4:	return Zi
Theorem 1. For all ε > 0, 1 > δ > 0, there exists r(l)(ε, δ) (l = 1, . . . , L) such that for all inputs
satisfying Assumptions 1 and 2, the following property holds true:
...ʌ	..	r	_
Pr[∣Qz(V,ε,δ) - ζv∣∣2 ≥ ε] ≤ δ.
Theorem 1 shows that the approximation error of Algorithms 2 is bounded by ε with the probability
1 - δ. It is proved by Hoeffding’s inequality (Hoeffding, 1963). The complete proof is available in
the appendices. Because the number of sampled nodes depends only on ε and δ and independent of
the number of nodes, edges, and neighbors of the input, Algorithm 2 works in constant time. We
provide the complexity when the functions are Lipschitz continuous.
Theorem 2. Under Assumption 1and 4, r(L) = O(* log 1) and r(1),..., r(LT) = O(表(log ɪ +
log 1)) are sufficient, and the query complexity OfAlgorithms 2 is O( £ (log ε + log 1 )L-1 log 1),
We show that the query complexity of Algorithm 2 is optimal with respect to ε if the number of layers
is one. In other words, a one-layer model cannot be approximated in o(ε2) time by any algorithm.
Theorem 3. Under Assumptions 1 and4 and L = 1, the time complexity of Algorithm 2 in Theorem
2 is optimal with respect to the error tolerance ε.
The proof is based on Chazelle et al.’s lemma (Chazelle et al., 2005). The optimality when L ≥ 2 is
an open problem.
4.2 Constant Time Gradient Approximation
Next, we propose a constant time algorithm that approximates the gradient of embeddings with
respect to the model parameters with an absolute error of at most ε and probability 1 - δ. The basic
strategy is to execute Algorithm 2 and calculate the gradients of the embedding Zv. Let 需 be the
gradient of the embedding Zv with respect to the model parameter θ (i.e., (d∂v Iijk = dzvi).
Theorem 4. For all ε > 0, 1 > δ > 0, there exists r(l)(ε, δ) (l = 1, . . . , L) such that for all inputs
satisfying Assumptions 1, 2, and 3, the following property holds true:
-..
∂Z(L)	∂ Z(L)
Pr[k A - %kF ≥ ε]≤ δ,
[(L)
where da¾ is the gradient of Zv , which is obtained by Oz (v, ε, δ), with respect to θ.
Therefore, we can calculate an estimate of the gradient of the embedding with respect to parameters
with an absolute error of at most ε and probability 1 一 δ by running OzL) (v, ε, δ) and calculating the
gradient of the obtained estimate of the embedding. We provide the complexity when the functions
are Lipschitz continuous.
Theorem 5. Under Assumptions 1, 4, and 5, r(L) = O(* log 1) and r(I),...,r(LT)=
O( ⅛ (log ε + log1)) are sufficient, and the gradient of the embedding with respect to pa-
rameters can be approximated with an absolute error of at most ε and probability 1 - δ in
O(击(log ε + log 1 )L~1 log 1) time.
5
Under review as a conference paper at ICLR 2020
Note that, technically, MPNNs do not include GAT because GAT uses embeddings of other neighbor-
ing nodes to calculate the attention value. However, Theorem 1 and 4 can be naturally extended to
GAT, and we can approximate GAT in constant time by neighbor sampling. The details are described
in the appendices.
5 Inapproximability
In this section, we show that some existing GNNs cannot be approximated in constant time. These
theorems state that these models cannot be approximated in constant time not only by our algorithm
but also by any other algorithm. In other words, for any algorithm that works in constant time, there
exists an error tolerance ε, a confidence probability 1 - δ, and a counter example input such that the
approximation error for the input is more than ε with probability δ. It indicates that the application of
an approximation method to these models requires great supervision because the obtained embedding
may be significantly different from the exact embedding.
Theorem 6.	If kxi k2 or kθ kF is not bounded, even under Assumption 1 and 2, the embeddings of
GraphSAGE-GCN cannot be approximated with arbitrary precision and probability in constant time.
Theorem 7.	Even under Assumption 1, the embeddings and gradients of GraphSAGE-GCN with
ReLU activation and normalization cannot be approximated with arbitrary precision and probability
in constant time.
We confirm Theorem 7 through computational experiments in Section 6.
Theorem 8.	Even under Assumptions 1 and 2, the gradients of GraphSAGE-GCN with ReLU
activation cannot be approximated with arbitrary precision and probability in constant time.
However, it should be noted that the embeddings of GraphSAGE-GCN with ReLU activation (without
normalization layer) can be approximated in constant time using our algorithm by Theorem 1. The
following two theorems state that these models cannot be approximated in constant time even under
Assumptions 1, 2, and 3.
Theorem 9.	Even under assumptions 1, 2, and 3, the embeddings of GraphSAGE-pool cannot be
approximated with arbitrary precision and probability in constant time.
Theorem 10.	Even under Assumptions 1, 2, and 3, the embeddings of GCN cannot be approximated
with arbitrary precision and probability in constant time.
Constant Time Approximaiton for GCN: Due to the inapproximability theorems, we cannot
approximate GCN in constant time. However, we can approximate GCN in constant time if the input
graph satisfies the following property:
Assumption 6 There exists a constant C ∈ R such that for any input graph G = (V, E) and node
v, u ∈ V, the ratio of deg(v) to deg(u) is at most C (i.e., deg(v)/deg(u) ≤ C).
Assumption 6 prohibits input graphs that have a skewed degree distribution. GCN needs Assumption
6 because (1) the norm of the embedding is not bounded and (2) the influence of anomaly nodes with
low degrees is significant without Assumption 6. It should be noted that the GraphSAGE-pool cannot
be approximated in constant time even under Assumption 6.
6 Experiments
We will answer the following questions through experiments:
Q1: How fast is the constant time approximation algorithm (Algorithm 2)?
Q2: Does Algorithm 2 accurately approximate the embeddings of GraphSAGE-GCN without nor-
malization (Theorem 1), whereas it cannot approximate the original one (Theorem 7)?
Q3: Does Algorithm 2 accurately approximate the gradients of GraphSAGE-GCN with sigmoid
(Theorem 4), whereas it cannot approximate that with ReLU activation (Theorem 8)?
Q4: Is the theoretical rate of the approximation error of Algorithm 2 tight?
6
Under review as a conference paper at ICLR 2020
-0 5 4 3 2 1
Oooooo
s3s E) 3EF
102	103	104
# nodes
6 4 2 0
■ ■ ■ ■
Oooo
CO-USN--SrEOC qΛΛ」QUB z
0^
---- with normalization 5 samples
---- with normalization 30 samples
----with normalization 100 samples
without normalization 5 samples
without normalization 30 samples
---- without normalization 100 samples
200	400	600	80?
# nodes
0 8 6 4 2 0
■ ■■■■■
Iooooo
∩ηaH J。」QUaz-I
COSSN=SrEOC0qW ∙10JJB
5 0 5 0 5 0
2 0 7 5 2 0
Iloooo
■ ■■■■■
0.012
0.010 ≡
__E
0.008 .≡>
0.0060
E
0.004 ⅛
0.002 -j
0.000
# nodes
(a) Speed.
10°
(b) Inference.	(c) Gradient.
■J
1°-
Zl
ιo° ιo1 ιoj ιo3 ιo4
# samples
」OU3 -XOJdde z^∣ pez=euuoN
0	200	400	600	800	l∞0
# samples
Z-I Pa--UUoN
」OU3 -XO.Jdde
OO
0	200	400	600	800	l∞0
# samples
(d) Theoretical rate.	(e) BA model.	(f) ER model.
Figure 1: (a) The inference speed of each method. (b) The approximation error of the original
GraphSAGE-GCN (i.e., with ReLU activation and normalization) and GraphSAGE-GCN with ReLU
activation. (c) The approximation error of the gradient with ReLU and sigmoid activations. (d)
The approximation error of Algorithm 2 and its theoretical bound. (e) The approximation error of
GraphSAGE-GCN, GAT, and GCN with the Barabasi-Albert model. (f) The approximation error of
GraPhSAGE-GCN, GAT, GCN, and GraPhSAGE-Pool with the Erdos-Renyi model.
Q5: Does neighbor sampling fail to approximate GCN when the degree distribution is skewed
(Theorem 10)? Does it succeed when the node distribution is flat (AssumPtion 6)?
Q6: Does neighbor samPling efficiently work for real data?
It should be noted that we focus on showing the aPProximation error of neighbor samPling in this
study as Hamilton et al. (2017) have already rePorted the effect of neighbor samPling for downstream
machine learning tasks (e.g., classification). The exPerimental details are described in the aPPendices.
Experiments for Q1: We measure the sPeed of exact comPutation and the constant time aPProxima-
tion of the two-layer GraPhSAGE-GCN and two-layer GAT. We initialize Parameters using the i.i.d.
standard multivariate normal distribution. The inPut graPh is a clique Kn . We use ten-dimensional
vectors from the i.i.d. standard multivariate normal distribution as the node features. We take
r(1) = r(2) = 100 samPles. Figure 1 (a) shows the sPeed of these methods as the number of nodes
increases. This shows that the constant time aPProximation is several orders of magnitude faster than
the exact comPutation when the number of nodes is large.
Experiments for Q2: We use the original one-layer GraPhSAGE-GCN (with ReLU activation and
normalization) and one-layer GraPhSAGE-GCN with ReLU activation. The inPut graPh is a clique
Kn, whose features are x1 = (1, 0)> and xi = (0, 1/n)> (i 6= 1), and the weight matrix is an
identity matrix I2. We use r(1) = 5, 30, and 100 as the samPle size. If a model can be aPProximated
in constant time, the aPProximation error goes to zero as the samPle size increases even if the
graPh size reaches infinity. The aPProximation errors of both models are illustrated in Figure 1 (b).
The aPProximation error of the original GraPhSAGE-GCN converges to aPProximately 0.75 even
if the samPle size increases. In contrast, the aPProximation error without normalization becomes
increasingly bounded as the samPle size increases. This is consistent with Theorems 1 and 7.
Experiments for Q3: We study the aPProximation errors of the gradients. We use the one-layer
GraPhSAGE-GCN with ReLU activation and sigmoid activation. The inPut graPh is a clique Kn ,
whose features are x1 = (1, 2)> and xi = (1, 1)> (i 6= 1), and the weight matrix is ((-1, 1)). We
use r(1) = 5, 30, and 100 as the samPle size. The aPProximation error of both models are illustrated
in Figure 1 (c). The aPProximation error with ReLU activation converges to aPProximately 1.0 even
if the samPle size increases. In contrast, the aPProximation error with sigmoid activation becomes
increasingly bounded as the samPle size increases. This is consistent with Theorems 4 and 8.
7
Under review as a conference paper at ICLR 2020
Experiments for Q4: We use the one-layer GraphSAGE-GCN with sigmoid activation. The input
graph is a clique Kn, where the number of nodes is n = 40000. We set the dimensions of intermediate
embeddings as 2, and each feature value is set to 1 with probability 0.5 and -1 otherwise. This
satisfies Assumption 1 (i.e., ∣∣Xik2 ≤ √2). We compute the approximation errors of Algorithm 2
with different numbers of samples. Figure 1 (d) illustrates the 99-th percentile point of empirical
approximation errors and the theoretical bound by Theorem 2 (i.e., ε = O(r-1/2)). It shows that the
approximation error decreases along with the theoretical rate. It indicates the theoretical rate is tight.
Experiments for Q5: We analyze the instances when neighbor sampling succeeds and fails for a
variety of models. First, we use the Barabasi-Albert model (BA model) (Barabasi & Albert, 1999)
to generate input graphs. The degree distribution of the BA model follows a power-law. Therefore,
neighbor sampling will fail to approximate GCN (Theorem 10, Assumption 6). We use two-layer
GraphSAGE-GCN, GAT, and GCN with ReLU activation. We use ten-dimensional vectors from the
i.i.d. standard multivariate normal distribution as the node features. In this experiment, we use the
same number r of samples in the first and second layer (i.e., r = r(1) = r(2)) and we change r from 8
to 1000. We use graphs with n = r2 nodes. Note that if the constant time approximation is possible,
the error is bounded as the number of samples increases even if the number of nodes increases. Figure
1 (e) shows the approximation error. It shows that the error of GCN linearly increases even if the
number of samples increases. However, the errors of GraphSAGE-GCN and GAT gradually decrease
as the number of samples increases. It indicates that we cannot bound the approximation error of
GCN however large number of examples we use. This is consistent with Theorem 10. This result
indicates that approximating GCN requires great supervision when the input graph is a social network
because the degree distribution of a social network presents the power-law as the BA-model.
Next, We use the ErdOS-Renyi model (ER model) (Erdos & R6nyi, 1959). It generates graphs with
flat degree distribution. We use the two-layer GraphSAGE-GCN, GAT, GCN, and GraphSAGE-pool.
Figure 1 (f) shows the approximation error. It shows that the errors of GraphSAGE-GCN, GAT, and
GCN gradually decrease as the number of samples increases. This is consistent with Theorem 1 and
Assumption 6. In contrast, the approximation error of GraphSAGE-pool does not decrease even if the
input graphs are generated by the ER model. This is consistent with Theorem 9.
Figure 2: Real data.
Experiments for Q6: We use three real-world datasets: Cora,
PubMed, and Reddit. They contain 2708, 19717, and 232965	7 * * 10
nodes, respectively. We randomly choose 500 nodes for validation Q 0.8
and 1000 nodes for test and use the remaining nodes for training. 106_
We use the two-layer GraphSAGE-GCN with sigmoid activation g
in this experiment. The dimensions of the hidden layers are set ∣0^4
to 128, and we use an additional fully connected layer to predict N 0.2
the labels of the nodes from the embeddings. We train the models OQ
with Adam (Kingma & Ba, 2015) with a learning rate of 0.001.
We first train ten models with training nodes for each dataset. The
micro-F1 scores of Cora, PubMed, and Reddit are 0.877, 0.839,
and 0.901, respectively. It should be noted that we do not aim to obtain high classification accuracy
here but intend to sanity check the models. In this experiment, we use the same number r of samples
in the first and second layer (i.e., r = r(1) = r(2)) and we change r from 1 to 100. We also
calculate the gradient of parameter matrices W(1) and W(2) with respect to the embedding obtained
by Algorithm 2. We compare these values with exact embeddings and exact gradients. Figure 6
illustrates the 99-th percentile point of the empirical approximation errors. We normalize them to
ensure that the value is 1.0 at r = 1 to demonstrate the decreasing rate of each error. It shows that the
approximation errors of embeddings and gradients rapidly decrease for the real-world data.
7 Conclusion
We proposed a constant time approximation algorithm for the embedding and gradient computation of
GNNs, where the complexity is completely independent of the number of nodes, edges, and neighbors
of the input. We proved its theoretical guarantee in terms of the approximation error. This is the
first constant time approximation algorithm for GNNs in the literature. We further demonstrated that
some existing GNNs cannot be approximated in constant time by any algorithm. Lastly, we validate
the theory through experiments using synthetic and real-world datasets.
8
Under review as a conference paper at ICLR 2020
References
Albert-Laszlo Barabasi and Reka Albert. Emergence of scaling in random networks. Science, 286
(5439):509-512,1999.
Bernard Chazelle, Ronitt Rubinfeld, and Luca Trevisan. Approximating the minimum spanning tree
weight in sublinear time. SIAM J. Comput., 34(6):1370-1379, 2005.
Jianfei Chen, Jun Zhu, and Le Song. Stochastic training of graph convolutional networks with
variance reduction. In Proceedings of the 35th International Conference on Machine Learning,
ICML, pp. 941-949, 2018a.
Jie Chen, Tengfei Ma, and Cao Xiao. FastGCN: Fast learning with graph convolutional networks
via importance sampling. In Proceedings of the Sixth International Conference on Learning
Representations, ICLR, 2018b.
Artur Czumaj and Christian Sohler. Estimating the weight of metric minimum spanning trees in
sublinear-time. In Proceedings of the 36th Annual ACM Symposium on Theory of Computing,
STOC, pp. 175-183, 2004.
Paul Erdos and Alfred R6nyi. On random graphs I. Publicationes Mathematicae, 6:290-297, 1959.
Wenqi Fan, Yao Ma, Qing Li, Yuan He, Yihong Eric Zhao, Jiliang Tang, and Dawei Yin. Graph neural
networks for social recommendation. In Proceedings of the 2019 World Wide Web Conference,
WWW, pp. 417-426, 2019.
Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural
message passing for quantum chemistry. In Proceedings of the 34th International Conference on
Machine Learning, ICML, pp. 1263-1272, 2017.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and
Statistics, AISTATS, pp. 249-256, 2010.
Marco Gori, Gabriele Monfardini, and Franco Scarselli. A new model for learning in graph domains.
In Proceedings of the International Joint Conference on Neural Networks, IJCNN, volume 2, pp.
729-734, 2005.
William L. Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large
graphs. In Advances in Neural Information Processing Systems 30, NIPS, pp. 1025-1035, 2017.
Kohei Hayashi and Yuichi Yoshida. Minimizing quadratic functions in constant time. In Advances in
Neural Information Processing Systems 29, NIPS, pp. 2217-2225, 2016.
Kohei Hayashi and Yuichi Yoshida. Fitting low-rank tensors in constant time. In Advances in Neural
Information Processing Systems 30, NIPS, pp. 2470-2478, 2017.
Wassily Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the
American Statistical Association, 58(301):13-30, March 1963.
Wen-bing Huang, Tong Zhang, Yu Rong, and Junzhou Huang. Adaptive sampling towards fast graph
representation learning. In Advances in Neural Information Processing Systems 31, NeurIPS, 2018.
Piotr Indyk. A sublinear time approximation scheme for clustering in metric spaces. In Proceedings
of the 40th Annual Symposium on Foundations of Computer Science, FOCS, pp. 154-159, 1999.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings of
the Third International Conference on Learning Representations, ICLR, 2015.
Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks.
In Proceedings of the Fifth International Conference on Learning Representations, ICLR, 2017.
Nina Mishra, Daniel Oblinger, and Leonard Pitt. Sublinear time approximate clustering. In Proceed-
ings of the Twelfth Annual Symposium on Discrete Algorithms, SODA, pp. 439-447, 2001.
9
Under review as a conference paper at ICLR 2020
Huy N. Nguyen and Krzysztof Onak. Constant-time approximation algorithms via local improvements.
In Proceedings of the 49th Annual IEEE Symposium on Foundations of Computer Science, FOCS,
pp. 327-336, 2008.
Namyong Park, Andrey Kan, Xin Luna Dong, Tong Zhao, and Christos Faloutsos. Estimating node
importance in knowledge graphs using graph neural networks. In Proceedings of the 25th ACM
SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD, pp. 596-606,
2019.
Michal Parnas and Dana Ron. Approximating the minimum vertex cover in sublinear time and a
connection to distributed algorithms. Theor. Comput. Sci., 381(1-3):183-196, 2007.
Ronitt Rubinfeld and Madhu Sudan. Robust characterizations of polynomials with applications to
program testing. SIAM J. Comput., 25(2):252-271, 1996.
Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The
graph neural network model. IEEE Trans. Neural Networks, 20(1):61-80, 2009.
Michael Sejr Schlichtkrull, Thomas N. Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max
Welling. Modeling relational data with graph convolutional networks. In Proceedings of the 15th
European Semantic Web Conference, ESWC, pp. 593-607, 2018.
Petar VeliCkovic, GUillem CUcUrUlL Arantxa Casanova, Adriana Romero, Pietro Lid, and YoshUa
Bengio. Graph attention networks. In Proceedings of the Sixth International Conference on
Learning Representations, ICLR, 2018.
Hongwei Wang, Miao Zhao, Xing Xie, Wenjie Li, and Minyi GUo. Knowledge graph convolUtional
networks for recommender systems. In Proceedings of the 2019 World Wide Web Conference,
WWW, pp. 3307-3313, 2019.
Rex Ying, RUining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, and JUre Leskovec.
Graph convolUtional neUral networks for web-scale recommender systems. In Proceedings of the
24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD
2018, pp. 974-983, 2018.
10
Under review as a conference paper at ICLR 2020
A Models
We introduce GraphSAGE-GCN, GraphSAGE-mean, GraphSAGE-pool, the graph convolutional
networks (GCN), and the graph attention networks (GAT) for completeness of our paper.
GraphSAGE-GCN (Hamilton et al., 2017): The message function and the update function of this
model is
Mlvu (zv , zu , evu , θ)
Zu
deg(v)
Ul(zv,hv,θ)=σ(W(l)hv),
where W (l) is a parameter matrix and σ is an activation function such as sigmoid and ReLU.
GraPhSAGE-GCN includes the center node itself in the set of adjacent nodes (i.e., N(V) - N(V) ∪
{v}).
GraphSAGE-mean (Hamilton et al., 2017): The message function and the uPdate function of this
model is
Mlvu (zv , zu , evu , θ)
Zv
deg(v)
Ul(zv,hv,θ) = σ(W (l)[zv, hv]),
where [∙] denotes concatenation
GraphSAGE-pool (Hamilton et al., 2017): We do not formulate GraPhSAGE-Pool using the message
function and the uPdate function because it takes maximum instead of summation. The model of
GraPhSAGE-Pool is
Zv(l) = max({σ(W (l)Zu(l-1) + b) | u ∈ N(V)}).
GCN (KiPf & Welling, 2017): The message function and the uPdate function of this model is
Z(l)
Mlvu(Zv，zu，evu，θ) = 八 / 、 / 、，
deg(V)deg(u)
Ul(Zv, hv, θ) = σ(WShv).
GAT (VeIiCkOviC et al., 2018): The message function and the update function of this model is
(I)	exp(LEAKYRELU(a(I)>[W(I)Zv, W(I)ZuD)
αvu = Puo∈N(v) exp(LEAKYRELU(a(l)>[W(l)Zv, W(l)Zuo])),
Ml
vu(Zv, Zu, evu, θ) = αvuZu,
Ul(Zv, hv, θ) = σ(W(l)hv).
Note that, technically, MPNNs do not include GAT because GAT uses embeddings of other neigh-
boring nodes to calculate the attention value αvu . However, we can apply the same argument as
MPNNs to GAT, and neighbor sampling can approximate GAT in constant time as other MPNNs. To
be precise, the following proposition holds true.
Proposition 11. If Assumptions 1 holds true and σ is Lipschitz continuous, if we take r(L) =
O(ε2 log 1) and r(1),..., r(LT) = O(ε2 (log ɪ + log δ)) samples, and let
Z(0) = z(0)
vv
11
Under review as a conference paper at ICLR 2020
Yl) _	exp(LEAKYRELU(a(I)>[W(I)ZvlT), W(I)ZulT)D)
αvu = Puo∈s(i) exp(LEAKYRELU(a(I)T[W(I)ZvlT), W(I)ZulOT)D)
hVl)= X OvuZUτ,
u∈S(l)
Zvl) = Ul (Zvl-1), h Vl), θ).
Then, the following property holds true.
Pr[kZvL)- ZvL) k2 ≥ ε] <δ.
We prove Proposition 11 in Section C.
B Experimental Setup
Experiments for Q1: For each algorithm and for n = 27, 28, . . . , 215, we (1) run the algorithm with
an input graph, (2) we calculate the exact embedding of one node and its estimate using 100 samples
(i.e., r(1) = r(2) = 100), (3) calculate the approximation error. We run the process above 10 times,
and we report the mean value of time consumption.
Experiments for Q4: We initialize the weight matrix W(1) with normal distribution and then
normalize it so that the operator norm kW (1) kop of the matrix equal to 1. This satisfies Assumption
1 (i.e., kθ∣∣2 ≤ √2). For each r = 1,..., 10000, We (1) initialize the weight matrix, (2) choose 400
nodes, (3) calculate the exact embedding of each chosen node, (4) calculate the estimate for each
chosen node with r samples (i.e., r(1) = r), and (5) calculate the approximation error of each chosen
node.
Experiments for Q5: In the experiment for the AB model, we (1) iterate r from 8 to 1000, (2) set
n = r2, (3) generate 10 graphs with n nodes using the BA model, (4) choose the node that has the
maximum degree for each generated graph, (5) calculate the exact embeddings and its estimate for
each chosen node with r samples (i.e., r(1) = r(2) = r), and (6) calculate the approximation error.
The experimental process for the ER model is similar to that for the ER model, but we (1) use the ER
model instead ofBA model and (2) set n = floor(r1.5) instead of n = r2 to reduce the computational
cost.
Experiments for Q6: To calculate the approximation errors of embeddings, for each trained model,
for r = 1 . . . 100, we (1) calculate the exact embedding of each test node, (2) calculate an estimate
of embedding of each test node with r samples (i.e., r(1) = r(2) = r), and (3) calculate the
approximation error of each test node. To calculate the approximation errors of gradients, for each
dataset, we (1) initialize ten models with Xavier initializer (Glorot & Bengio, 2010), (2) choose
random 1000 nodes, and (3) for each model, for each chosen node, and for r = 1 . . . 100, calculate
the exact and approximation gradients of the classification loss with respect to the parameters, and
we calculate their approximation error.
C Proofs
We introduce the following multivariate version of the Hoeffding’s inequality (Hoeffding, 1963) to
prove the theoretical bound (Theorem 1 and 4).
Lemma 12 (multivariate Hoeffding’s inequality). Let x1, x2, . . . , xn be independent d-dimensional
random variables whose two-norms are bounded ∣∣xi∣∣2 ≤ B, and let X be the empirical mean of
these variables X= n1 5∑n=ι Xi∙ Then,for any ε > 0,
Pr[∣∣X — E[X]∣∣2 ≥ ε] ≤ 2d exp
nε2 )
2B2d)
holds true.
12
Under review as a conference paper at ICLR 2020
Lemma 12 states that the empirical mean of n = O(ε⅛ log ∣) samples independently sampled from
the same distribution is the approximation of the exact mean with an absolute error of at most ε and
probability 1 - δ.
Lemma 13 (Hoeffding’s inequality (Hoeffding, 1963)). Let X1 , X2, . . . , Xn be independent random
variables bounded by the intervals [—B, B] and let X be the empirical mean of these variables
X = n Pn=I Xi. Then,forany ε > 0,
Pr[∣X — E[X]∣≥ ε] ≤ 2exp (- 2B2
holds true.
Proof of Lemma 12. Apply Lemma 13 to each dimension k of Xi. Then,
pr[|Xk -E[X]k| ≥ √d] ≤ 2eχp (-2B2d
It should be noted that |Xik | < B because kXi k2 < B. Therefore,
pr[∃k ∈ {1, 2,...,d} IXk — E[X]k | ≥ √=] ≤ 2d eχp (— 2B?d).
If |Xk — E[X]k ∣ < √⅛ holds true for all dimension k, then
ud
kX — E[X]k2 = t X(Xk — E[X]k)2 <
k=1
ydι2=ε
Therefore,
Pr[∣∣X — E[X]∣∣2 ≥ ε] ≤ 2d exp
(-黑d).
□
First, we prove that the embedding in each layer is bounded.
Lemma 14. Under Assumptions 1 and 2, the normSOftheembeddingS Ilzvl)∣∣2, kZvl)Il2, ∣Ihvl)I∣2,
and ∣∣h vl)∣2 (l = 1,...,L) are bounded by a COnStant B ∈ R.
Proof of Lemma 14. We prove the theorem by performing mathematical induction. The norm of
the input to the first layer is bounded by Assumption 1. The message function deg(i)Mliu and the
update function Ul is continuous by Assumption 2. Since the image f(X) of a compact set X ∈ Rd
is compact if f is continuous, the images of deg(i)Mliu and Ul are bounded by induction.	□
Proof of Theorem 1. We prove the theorem by performing mathematical induction on the number of
layers L.
Base case: It is shown that the statement holds true for L = 1.
Because UL is uniform continuous,
∃ε0 >0,∀zv,hv,h0v,θ,Ihv —h0vI2 <ε0 ⇒ IUL(zv, hv, θ) — UL(zv, h0v, θ)I2 <ε.	(1)
Let xk be the k-th sample in S(L) and Xk = deg(v)MLvu(zv(0), zx(0k), evxk , θ). Then,
E[Xk] = X MLvu(zv(0), zu(0), evu, θ) = h(vL).	(2)
u∈N (v)
There exists a constant C ∈ R such that for any input satisfying Assumption 1,
IXk I2 < C	(3)
holds true because Izv(0) I2, Izx(0k) I2, Ievxk I2, and IθI2 are bounded by Assumption 1 and
deg(v)MLvu iscontinuous. Therefore, if we take r(L) = O(左 log ∣) samples, Pr[∣h VL) — hvL)∣∣2 ≥
ε0] ≤ δ by the Hoeffding,s inequality and equations (2) and (3). Therefore, Pr[kzvL) — ZvL) ∣∣2 ≥
ε] ≤ δ.
13
Under review as a conference paper at ICLR 2020
Inductive step: It is shown that the statement holds true for L = l + 1 if it holds true for L = l. The
induction hypothesis is ∀ε > 0, 1 > δ > 0, ∃r(1) (ε, δ), . . . , r(L-1) (ε, δ) such that ∀v ∈ V,
Pr[∣QZLT)(V,ε,δ) - Zv ∣∣2 ≥ ε] ≤ δ.
Because UL is uniform continuous,
∃ε0 > 0,∀zv, hv, zv0 , h0v, θ, ∣[zv, hv] - [zv0 , h0v]∣2 < ε0 ⇒ ∣UL(zv, hv, θ) - UL(zv0 , h0v, θ)∣2 < ε,
(4)
where [∙] denotes concatenation. By the induction hypothesis,
∃r0 ⑴,...,r0(LT) such thatPr[∣∣O(LT)(V) - ZvLT)I∣2 ≥ ε0∕√2] ≤ δ∕2.	(5)
holds true. Let
hVL) =驾 X MLvu(ZvL7, ZuLT), evu, θ).
r(L)
u∈S(L)
Let xk be the k-th sample in S(L) and Xk = deg(V)MLvu(Zv(L-1), Zx(Lk-1), evxk, θ). Then,
E[Xk] = X MLvu(Zv(L-1), Zx(Lk-1), evxk, θ) = h(vL).	(6)
u∈N (v)
There exists a constant C ∈ R such that for any input satisfying Assumption 1,
∣Xk ∣2 < C,	(7)
because ∣Zv(L-1) ∣2, ∣Zx(Lk-1) ∣2, ∣evxk ∣2, and ∣θ∣2 are bounded by Assumption 1 and Theorem 14,
and deg(v)MLvu is continuous. If we take r(L) = O(+ log 1), then
Pr[∣∣hVl)- hvI)Il2 ≥ ε0∕(2√2)] ≤ δ∕4,	(8)
by the Hoeffding’s inequality and equations (6) and (7). Because deg(V)MLvu is uniform continuous,
∃ε00 > 0 such that I[Zv(L-1),Zu(L-1)] - [Zv0(L-1),Zu0(L-1)]I2 ≤ ε00
⇒ deg(v)kMLvu(ZvLT), ZuLT), evu, θ) - MLvu(Zv(LT), Zu(LT), evu, θ)k2 ≤ ε0∕(2√2).
(9)
By the induction hypothesis,
∃r00⑴，...，r00(I) SUCh thatPr[∣∣O(LT)(V) - ZvLT)I∣2 ≥ ε00∕√2] ≤ δ∕(8r(L)).	(10)
Therefore, the probability that the errors of all oracle calls are bounded is
Pr[∃υ ∈S(L),∣[OZLT)(V), O(LT)(u)] -[ZvLT), ZuLT)]∣2 ≥ ε00] ≤ δ∕4.	(11)
By equations (9) and (11),
Pr[∃u ∈ S(L),
deg(ν)∣MLvu(ZvLT), ZuLT), evu, θ) - MLvu(ZvLT), ZuLT), evu, θ)∣2 ≥ ε0∕(2√2)] ≤ δ∕4.
Pr[∣hvL)- hVL)∣2 ≥ ε0∕(2√2)] ≤ δ∕4.	(12)
By the triangular inequality and equations (8) and (12),
Pr[∣hvL) - hvL)∣2 ≥ ε0∕√2] ≤ δ∕2.	(13)
Therefore, if we take r(1) = max(r0(1), r00(1)), . . . , r(L-1) = max(r0(L-1), r00(L-1)), by equations
(5) and (13),
Pr[∣[ZvLT), hvL)] - [OZLT)(V), hVL)]∣2 ≥ ε0] ≤ δ.	(14)
Therefore, by equations (9) and (14),
Pr[∣ZvL)-ZvL) k2 ≥ ε] ≤ δ.
□
14
Under review as a conference paper at ICLR 2020
Proof of Theorem 2. We prove this by performing mathematical induction on the number of layers.
Base case: It is shown that the statement holds true for L = 1.
If UL is K-LiPschitz continuous, ε0 = O(ε) in equation (1). Therefore, r(L) = O(* log 1).
Inductive step: It is shown that the statement holds true for L = l + 1 if it holds true for L = l.
If UL and MLvu are K-LiPschitz continuous, ε0 = O(ε) in equation (4) and ε00 = O(ε) in equation
(9). Therefore, r(L) = O(* log 1). Wecall O(LT)(V)SUchthatPr[∣∣OZLT)(V) - ZvLT)I∣2 ≥
ε0∕√2] ≤ δ∕2 in equation (5). Therefore, ro(1),...,r0(LT) = O(表(log ɪ + log 1) are sufficient
by the induction hypothesis. We call OzLT)(V) SUchthatPr[∣OZLT)(V) - ZvLT)∣∣2 ≥ ε00/√2] ≤
δ∕(8r(L)) in equation (10). Therefore, r00(1),...,r"(LT) = O(表(log 1 + log 1) are sufficient
by the induction hypothesis because log 〃$㈤)=O (log 11 + log 1). In total, the complexity is
O(击(log ε + log ε )l-1 log 1)	□
Lemma 15 (Chazelle et al. (2005)). Let Ds be Bernoulli( 1+sε). Let n-dimentional distribution
D be (1) pick s = 1 with probability 1/2 and s = -1 otherwise; (2) then draw n values from Ds.
Any probabilistic algorithm that can guess the value of s with a probability error below 1/4 requires
Ω( 12) bit lookup on average.
Proof of Theorem 3. We prove there is a counter example in the GraphSAGE-GCN models. Sup-
pose there is an algorithm that approximates the one-layer GraphSAGE-GCN within o(ε2) queries.
We prove that this algorithm can distinguish D in Lemma 15 within o(ε2) queries and derive a
contradiction.
Let σ be any non-constant K-Lipschitz activation function. There exists a, b ∈ R (a > b) such that
σ(a) = σ(b) because σ is not constant. Let S = "(?-；(" > 0. Let ε > 0 be any sufficiently
small positive value and t ∈ {0, 1}n be a random variable drawn from D. We prove that we can
determine s with high provability within o(ε2) queries using the algorithm. Let G be a clique Kn
and W(1) = 1. Let us calculate aε and bε using the following steps: (1) set aε = a and bε = b; (2)
if aε — bε < ε, return aε and bε; (3) m = aε+bε; (4) if ∣σ(aε) — σ(m)∣ > ∣σ(m) — σ(bε)∣, then
set b1 = m, otherwise a1 = m; and (5) go back to (2). Here, ε∕2 ≤ a1 — b1 < ε, a ≤ aε++bε ≤ b,
and ∣σ(a1) — σ(b1)∣ ≥ Slε hold true. Let Xv = aε+bε + (2tv — 1)a⅛bε for all V ∈ V. Then,
E[hv | S = 1] = a1 and E[hv | S = —1] = b1. Therefore, Pr[∣zv — σ(a1)∣ < Sε | S = 1] → 1 as
n → ∞ and Pr[∣zv — σ(b1)∣ < Sε | S = —1] → 1 as n → ∞ because σ is K-Lipschitz. We set the
error tolerance to ∣ε and n to a sufficiently large number. Then S = 1 if |zv — σ(a1)∣ < ∣ε and
S = —1 otherwise with high probability. However, the algorithm accesses t (i.e., accesses Ofeature)
o(ε2) times. This contradicts with Lemma 15.	□
Lemma 16. Under Assumptions 1, 2 and 3, the norms of the gradients of the mes-
sage functions and the update functions ∣∣DUι(zvl-1), hvl), Θ)∣f, kDUι(Zvl-1), hvl), Θ)∣f,
kdeg(V)DMlvu(ZvlT), Vul),已皿。并正，and ∣∣deg(V)DMlvu(ZvlT), Vul),已皿。州尸 areboundedby
a constant B0 ∈ R.
Proof of Lemma 16. The input of each function is bounded by Lemma 14. Because DUl and
deg(V)DMlvu is uniform continuous, these images are bounded.	□
Proof of Theorem 4. We prove the theorem by performing mathematical induction on the number of
layers L.
Base case: It is shown that the statement holds true for L = 1.
15
Under review as a conference paper at ICLR 2020
When the number of layers is one,
∂θ
∂UθL (zv0), hVL), θ)+/(zv0), hVL), θ)暮
∂UL(zV0), hVL), θ) + SL(zV0), hVL), θ) X dMvu(zV0), zU0), evu,①.
∂θ	∂θ
∂hv	u∈N (v)
∂θ
% (zV0),h vL), θ)+∂⅛(zV0), h vL), θ)端
篝(zV0), hVL), θ) + ∂UL((zV0), hVL), θ)degLv) X W(zV0), zU0), evu, θ).
∂θ	∂h	r	∂θ
∂hV	∈S(L)
Because DUL is uniform continuous,
∃ε0 > 0 s.t. for allinput,khVL) - h(勺以 < ε0 ⇒
(k^L(ZV0), hVL), θ) - ^L(ZV0), hVL), θ)kF < ε/2 ∧
∂θ	∂θ
k∂UL((zV0), hVL), θ) - ∂UL-((zV0), hVL), Θ)∣∣f <ε∕(4B0)).	(15)
∂hV	∂hV
If We take r(L) = O(+ log δ),
Pr[∣∣hVL)- h(L)Il2 ≥ ε0] ≤ δ∕2	(16)
holds true for any input by the argument of the proof of Theorem 1.
Let Xk be the k-th sample in S(L) and Xk = deg(v) dMθvu (zV0), zU0), £vu, θ). Then,
E[Xk]= X ¾u(zV0), zU0), e°u, θ)
∂θ
u∈N (V)
∂hVL)
∂θ .
(17)
There exists a constant C ∈ R such that for any input satisfying Assumption 1,
IXk I2 < C,
(18)
because IZV(0)I2, IZx(0k)I2, IeVxk I2, and IθI2 are bounded by Assumption 1 and deg(v)DMLVu is
continuous. Therefore, if we take r(L) = O( * log 1) samples,
Pr[I
∂ hVL)
∂θ
∂h(L)
-⅞r If ≥ ε∕(4B0)] ≤ δ∕2
(19)
holds true by the Hoeffding’s inequality and equations (17) and (18). If
∂h(L)	∂h(L)
k -hr -号kF <"
and
k∣⅛(zV0), hVL), θ)-艾(zV0), hVL), θ)kF < ε∕(4B0))
∂hV	∂hV
hold true, then
ll∂hVL)	∂Ul
k ~∂θ~ ∂hF
(ZVO), h VL), θ)-
% ∂U⅛ (ZVO), hVL), θ)kF
16
Under review as a conference paper at ICLR 2020
≤k%Z(zVo),hVL),θ) - %dhU⅛(zVo),hVL),θ)kF
+k 端 d⅜ (zVo), hVL), θ)-端 dh⅛) (zVo), hVL), θ)kF
=k"∂θ kFkJ^⅛r(ZVO),hVL),θ)- EL) (zVo),hVL),θ)kF
dhV	dhV
+ k 嚓-嚓 kF k dh⅞) (ZVO), hVL), θ)kF
≤ B0k
∂UL
∂h VL)
(ZVO),hVL),θ)- J：L) (ZVO),hVL),θ)kF + B0k'∂θ
dhV	dθ
∂ hVL)
∂θ kF
—
<B0 4BB0 + B0 4⅛ = ε
(L)	(L)
Therefore, Pr[k -----------∂θ~ IIF ≥ ε] ≤ δ by equations (15), (16), and (19).
Inductive step:
It is shown that the statement holds true for L = l + 1 if it holds true for L = l.
dzVL)
dθ
∂UL (ZVj), hVL), θ) +
∂θ
∂UL
dz(LT)
ZV
(zv(L-1),h(vL),θ)
∂ZV(L-1)
∂θ
+ Z(ZVL-O, h- θ) uXV) ɪ (ZVL-I), ZuL-I), eVu, θ)
∂zVL)
dθ
+ Z(ZVj), h- θ) uX V)
+ Z(ZVj), h- θ) uX V)
∂UL (ZVLT), h VL), θ) +
∂θ
∂UL
∂ ZvLT)
∂MLVu
∂zVLT)
∂MLVu
∂zULT)
(ZV(L-1), Zu(L-1), eVu, θ)
∂ZV(L-1)
∂θ
dZ(L-1)
(ZVLT), ZuLT), eVu, θ)	Uf)
dθ
d (L-1)
(ZVj), h VL), θ) d⅛-
dθ
+
∂UL
∂ h VL)
(ZVL-1), hVL), θ)悴 X 书(ZVL-1), ZuL-1), eVu, θ)
u∈S(L)
+
+
∂UL
∂ h VL)
∂UL
∂ h VL)
(Z(L-I) h(L) θ) deg(V) X dMLVu
(ZV	, hV	, θ)	r(L)	与)dz(L-1)
(z(L-1) h(L) θ) deg(V) X dMLVu
(ZV	, hV	,θ)	r(L)	乙	αNL-1)
r	u∈S(L) dzu
(ZVLT), ZuLT), eVu, θ)
∂zVLT)
(ZVLT), ZuLT), eVu, θ)
∂θ
∂ZuLT)
∂θ
Because DUL is uniform continuous,
∃ε0 > 0 s.t. forallinput,∣∣[ZVLT), hL)] — [W'LT), hVL)]∣∣2 <ε0 ⇒
(k^L(ZVLT), hVL), θ) - ^L(ZVLT), hVL), θ)kF < O(ε) ∧
∂θ	∂θ
k
∂UL
∂ hVL)
(zV(L-1), h(VL), θ) -
∂UL
∂ h VL)
(ZVLT), hVL), θ)kF <O(ε)∧
17
Under review as a conference paper at ICLR 2020
k
∂UL
∂ ZVLT)
(ZVLT), hVL), θ) - Ad(U-I) (ZVLT), hVL), θ)kF < O(ε)).
∂zV
(20)
Because deg(v)DMLvu is uniform continuous,
∃ε00 > 0 s.t. for all input,k[zVLT), ZuLT)] - [ZVL-1), ZuLT)]∣∣2 < ε00 ⇒
(deg(v)k dMLVu (ZVLT), ZuLT), eVu, θ) -
deg(v)k
deg(v)k
∂Mlvu
dhVL)
dMLVu
(zV(L-1), zu(L-1), eVu, θ) -
∂UL
~θθ~
∂UL
(ZVLT), ZuLT), eVu, θ)kF < O(ε) ∧
∂h VL)
(ZVLT), ZuLT), evu, θ)kF < O(ε) ∧
∂JLτ)
zV
(ZVLT), ZuLT), eVu, θ) - Ad(U-I) (ZVLT), ZuLT), eVu, θ)kF < O(ε)).
dZV
(21)
By the argument of the proof of Theorem 1, if We take sufficiently large number of samples,
Pr[∣∣zVLT)- ZVLT)k2 ≥ O(min(ε0,ε00))] ≤ O(εδ),
(22)
Pr[khVL)- hVL)k2 ≥ O(ε0)] ≤ O(δ).
(23)
By the induction hypothesis, there exists r(1), . . . , r(L-1) such that
Pr[k
Pr[k
∂Z(L-1)
Zu
∂θ
∂ ZVLT)
∂θ
∂ Z(LT)
Zu
∂θ
∂ ZVLT)
∂θ
kF ≥ O(ε)] ≤ O(εδ),
kF ≥ O(ε)] ≤ O(εδ),
—
—
If We take r(L) = O(* log 1),
Pr[k
∂MLVu
u∈N (V)
deg(v)
∂θ
(ZV(L-1), Zu(L-1), eVu, θ)
r(L)
X dMLVu (ZVLT), ZuLT), eVu, θ)k ≥ O(ε)] ≤ O(δ),
∂θ
u∈S(L)
(24)
—
Pr[k X H(ZVLT), Zuj), eVu, θ) W
u∈N(V) ∂ZV	∂θ
—
等 X A (ZVLT), ZuLT), eVu, θ)dz⅛-11 k≥ O(ε)]≤ O(δ),
u∈S(L) dZV
Pr[k X F(ZVLT), ZuLT), eVu, θ) J
u∈N (V) ∂Zu
—
deg^ X	dMLVu (Z(L-I) z(L-1) e	θ) dZuLT) k ≥ O(ε)] ≤ O(δ)
r(L)	乙 A (L-1) (ZV	, zu	, eVu, θ)	∂θ	k≥ O(ε)] ≤ O(δ),
r	u∈S(L) ∂Zu
(25)
(26)
holds true by the Hoeffding’s inequality. Therefore,
Pr[k X dMLVu (ZVLT), ZuLT), eVu, θ)
∂θ
u∈N (V)
18
Under review as a conference paper at ICLR 2020
-	deg# X dMLvu(ZvLT), ZuLT), evu, θ)k≥ O(ε)] ≤ O(δ),	(27)
r(L)	∂θ	v u
u∈S(L)
W X *(ZvLT)，Zuj)，du θ) W
u∈N (v) ∂Zv	∂θ
-	deg(V) X dMLvu (Z(L-I) Z(L-I) e	θ)dzv___k ≥ O(ε)] ≤ O(δ)	(28)
r(L)	内 NL-I)(Zv	, Zu	, evu, θ)	∂θ k ≥ O(ε)] ≤ O(δ),	(28)
r	u∈S(L) dZv
Pr[k X *(ZvLT, ZUj), evu, θ) J
u∈N(v) dZu	dθ
-	deg(V) X dMLvU (Z(L-I) Z(L-I) e	θ)d?u_____k ≥ O(ε)] ≤ O(δ)	(29)
r(L)	乙 内 NL-I)(Zv	, Zu	, evu, θ)	dθ k≥ O(ε)] ≤ O(δ),	(29)
u∈S(L) dZu
holds true by equations (20), (21), (22), (23), (24), (25), and (26).
Therefore, if We take r(1),... r(L) sufficiently large, Pr[k % L -嗨 LIIF ≥ ε] ≤ δ holds true by
equations (20), (21), (22), (23), (27), (28), and (29).
□
Proof of Theorem 5. We prove this by performing mathematical induction on the number of layers.
Base case: It is shoWn that the statement holds true for L = 1.
If DUL is K0-Lipschitz continuous, ε0 = O(ε) in equation (15). Therefore, r(L) = O(ε2 log δ) is
sufficient.
Inductive step: It is shoWn that the statement holds true for L = l + 1 if it holds true for L = l.
If DUL and deg(V)DULvu is K0-Lipschitz continuous, ε0 = O(ε) in equation (20) and ε00 = O(ε)
in equation (21). Therefore, r(L) = O(ε2 log ∣) and r(1),..., r(LT) = O(* (log j + log 1)) are
sufficient.	□
Proof of Theorem 6. We shoW that one-layer GraphSAGE-GCN Whose activation function is not
constant cannot be approximated in constant time if kxv k2 or kθk2 are not bounded. There exists
a ∈ R such that σ(a) 6= σ(0) because σ is not constant. We consider the folloWing tWo types of
inputs:
•	G is the clique Kn, W(1) =	1, and xi = 0 for all nodes i ∈ V.
•	G is the clique Kn, W (1) =	1, xi = 0(i 6= V) for some V ∈ V, and	xv = an.
Then,	for the former input, Zv(1) =	σ(0). For the latter type of inputs,	Zv(1) = σ(a).	Let	A
be an	arbitrary constant algorithm and C be the number of queries A makes When	We set ε	=
∣σ(a) - σ(0) |/3. When A calculates the embedding of U = V ∈ V, the states of all nodes but U are
symmetrical until A makes a query about that node. Therefore, if n is sufficiently large, A does not
make any query about V with high probability (i.e., at least (1 - n-ɪ)C). If A does not make any
query about V, the state of A is the same for both types of inputs. If the approximation error is less
than ε for the first type of inputs, the approximation error is larger than ε for the second type of inputs
by the triangle inequality and vice versa. Therefore, A fails to approximate the embeddings of either
type of inputs with the absolute error of at most ε. As for θ, we set W (ɪ) = an and xv = 1 for the
second type of inputs. Then, the same argument follows.	□
19
Under review as a conference paper at ICLR 2020
Proof of Theorem 7. We consider the one-layer GraphSAGE-GCN with ReLU and normalization
(i.e., σ(x) = RELU(x)/kRELU(x)k2). We use the following two types of inputs:
•	G is the clique Kn, W(1) is the identity matrix I2, xi = (0, 0)>(i 6= v) for some node
v ∈ V, and xv = (1, 0)>.
•	G is the clique Kn, W(1) is the identity matrix I2, xi = (0, 0)>(i 6= v) for some node
v ∈ V, and xv = (0, 1)>.
Then, for the former type of inputs, h = (1/n, 0)>, Zi = (1,0)>, and ddwi2- = 1 for all i ∈ V. For
the latter type of inputs, hi = (0,1∕n)>, Zi = (0,1)>, and IWzi= 0 for all i ∈ V. Let A be an
arbitrary constant algorithm and C be the number of queries A makes when we set ε = 1/3. When
A calculates the embedding or gradient of u 6= v ∈ V, the states of all nodes but u are symmetrical
until A makes a query about that node. Therefore, if n is sufficiently large, A does not make any
query about V with high probability (i.e., at least (1 - n-i )C). If A does not make any query about
v , the state of A is the same for both types of inputs. If the approximation error is less than ε for
the first type of inputs, the approximation error is larger than ε for the second type of inputs by the
triangle inequality and vice versa. Therefore, A fails to approximate the embeddings and gradients of
either type of inputs with the absolute error of at most ε.	□
Proof of Theorem 8. We consider the one-layer GraphSAGE-GCN with ReLU (i.e., σ(x) =
RELU(x)). We use the following two types of inputs:
•	G is the clique Kn, W (1) = (-1, 1), xi = (1, 1)> (i 6= v) for some node v ∈ V, and
xv = (1, 2)>.
•	G is the clique Kn, W (1) = (-1, 1), xi = (1, 1)> (i 6= v) for some node v ∈ V, and
xv = (1, 0)>.
Then, for the former type of inputs, MEAN({xu | U ∈ N(v)}) = (1,1 + 1 )>, hv = Zv = 1,
and ∂∂W = (1,1 + 1) for all i ∈ V. For the latter type of inputs, MEAN({xu | U ∈ N(v)})=
(1,1 - 1 )>, hv = -1, Zv = 0, and ∣W = (0,0) for all i ∈ V. Let A be an arbitrary constant
algorithm and C be the number of queries A makes when we set ε = 1/3. When A calculates the
gradient of U 6= v ∈ V, the states of all nodes but U are symmetrical until A makes a query about that
node. Therefore, if n is sufficiently large, A does not make any query about v with high probability
(i.e., at least (1 - n-1 )C). If A does not make any query about v, the state of A is the same for both
types of inputs. If the approximation error is less than ε for the first type of inputs, the approximation
error is larger than ε for the second type of inputs by the triangle inequality and vice versa. Therefore,
A fails to approximate the gradients of either type of inputs with the absolute error of at most ε. □
Proof of Theorem 9. We consider the one-layer GraphSAGE-pool whose activation function satisfies
σ(1) 6= σ(0) and the following two types of inputs:
•	G is the clique Kn, W(1) = 1, b = 0, and xi = 0 for all nodes v ∈ V.
•	G is the clique Kn, W(1) = 1, b = 0, xi = 0 (i 6= v) for some node v ∈ V, and xv = 1.
Then, for the former type of inputs, Zi = σ(0) for all i ∈ V. For the latter type of inputs, Zi = σ(1)
for all i ∈ V. Let A be an arbitrary constant algorithm and C be the number of queries A makes when
we set ε = ∣σ(1) - σ(0)∣∕3. When A calculates the embedding of u = V ∈ V, the states of all nodes
but U are symmetrical until A makes a query about that node. Therefore, if n is sufficiently large, A
does not make any query about V with high probability (i.e., at least (1 - n-1 )C). If A does not make
any query about V, the state of A is the same for both types of inputs. If the approximation error is
less than ε for the first type of inputs, the approximation error is larger than ε for the second type of
inputs by the triangle inequality and vice versa. Therefore, A fails to approximate the embeddings of
either type of inputs with the absolute error of at most ε.	□
20
Under review as a conference paper at ICLR 2020
Proof of Lemma 10. We consider the one-layer GCN whose activation function satisfies σ(1) 6= σ(0).
We use the following two types of inputs:
•	G is a star graph, where v ∈ V is the center of G, W (1) = 1, and all features are 0.
•	G is a star graph, where V ∈ V is the center of G, W(1) = 1, and the features of √2n leafs
are 1 and the features of other nodes are 0.
Then, for the former type of inputs, zv = σ(0). For the latter type of inputs, zv = σ(1). Let
A be an arbitrary constant algorithm and C be the number of queries A makes when we set ε =
∣σ(1) - σ(0)∣∕3. When A calculates the embedding of U ∈ V that Xu = 0, the states of all nodes but
U are symmetrical until A makes a query about that node. Therefore, if n is sufficiently large, A does
not make any query about V with high probability (i.e., at least (1 - n-2∣ )C). If A does not make
any query about v, the state of A is the same for both types of inputs. If the approximation error is
less than ε for the first type of inputs, the approximation error is larger than ε for the second type of
inputs by the triangle inequality and vice versa. Therefore, A fails to approximate the embeddings of
either type of inputs with the absolute error of at most ε.	□
We prove the following lemma to prove Proposition 11.
Lemma 17. If Assumptions 1 holds true and σ is Lipschitz continuous, kzv(l) k2 and kzv(l) k2 (l =
1, . . . , L) of the GAT model are bounded by a constant
Proof of Lemma 17. We prove this by performing mathematical induction on the number of layers.
The norm of the input of the first layer is bounded by Assumption 1. Ifkzul-1) ∣∣2 and kZul-1) ∣∣2 are
bounded for all U ∈ V, ∣hVl) ∣∣2 and ∣∣hVl) ∣∣2 are bounded because hVl) and hVl) are the weighted sum
of zul-1) and zul-1). Therefore, 11/^)112 and IIzul) ∣∣2 are bounded because Ul is continuous. □
Proof of Proposition 11. We prove the theorem by performing mathematical induction on the number
of layers L.
Base case: It is shown that the statement holds true for L = 1.
Because UL is Lipschitz continuous,
∀zV,hV,h0V,θ,∣hV -h0V∣2 < O(ε) ⇒ ∣UL(zV,hV,θ) -UL(zV,h0V,θ)∣2 <ε.	(30)
Let eu = exp(LEAKYRELU(a(l)> [W(0) zV(0), W(0)zu(0)U])). Then,
h VL)- hVL)
E avuzu0) - E ɑvυzu0)
u∈S(L)	u∈N (V)
r(L) Σ
u∈S(L)
ɪp^u——zu0)
r(L)2^u0∈S(L) eu0
1
deg(v)
Σ	ɪP
u∈N (V) deg(V)
eu
u0 ∈N (V) eu
zu(0).
0
Let xk be the k-th sample in S(L) and Xk =exk. Then,
E[Xk]
M uχ v) eu.
(31)
There exists a constant c > 0, C > 0 such that for any input satisfying Assumption 1,
c< |Xk| < C,	(32)
because ∣zV(0)∣2, ∣zx(0k)∣2, ∣W(0)∣F, and ∣a(0)∣2are bounded by Assumption 1. Therefore, if we
take r(L) = O(ε2 log δ) samples,
Pr[|r⅛r X eu- N(V) X eu| ≥ O(ε)] ≤ O(δ)
u∈S(L)	u∈N (V)
21
Under review as a conference paper at ICLR 2020
by the Hoeffding’s inequality and equations (31) and (32). Because f(x) = 1/x is Lipschitz
continuous in x > c > 0,
Pr[| ɪp^-------------ɪ p 1------------1 ≥ O(ε)] ≤ O(δ)	(33)
r(L)乙u∈S(L) eu	N(V)乙u∈N(V) eu
Let
Then,
Yk
_______eXk_______z(0)
1 P	Q Zxk .
N(V)乙u0∈N(v) eu0
EYk] = Nv) UXv) F
eu
u0∈N (V) eu
zu(0).
0
(34)
There exists a constant C0 ∈ R such that for any input satisfying Assumption 1,
kYkk2 <C0	(35)
holds true because IlzuO) k 2 are bounded, and c < |eu| < C. Therefore, if we take r(L) = O( ε2 log ∣)
samples,
Pr[k r⅛r X	P
u∈S(L) deg(v)
eu
u0∈N (v) eu0
zu(0)
—
dɪ X 1	Peu---------zu0)k2 ≥ O(ε)] ≤ O(δ)
deg(v) u∈N(v) deg(V) Eu0∈N(V) eu0
(36)
holds true by the Hoeffding’s inequality and equations (34) and (35). Therefore,
Pr[k r1L) X
u∈S(L)
eu
忐 Σu0∈S(L) eu
zu(0)
—
dɪ X 1	Peu---------zu0)k2 ≥ O(ε)] ≤ O(δ)
deg(v) u∈N(v) deg(v) ∑u0∈N(v) eu0
(37)
holds true by the triangle inequality and equations (33) and (36), and Pr[∣zVL) 一zv(L) I2 ≥ ε] ≤ δ
holds true by equations (30) and (37).
Inductive step: It is shown that the statement holds true for L = l + 1 if it holds true for L = l.
Because UL is Lipschitz continuous,
∀zv, hv, h0v, θ, khv - h0v k2 < O(ε) ⇒ IlUL (zv, hv, θ) - UL (zv, h, θ)k2 < ε (38)
holds true.
Pr[k r1L) X
u∈S(L)
eu
r(L)∑uo∈s (L) eu
zu(L-1)
0
—
dɪ X 1	Peu--------zuLτ)k2 ≥ O(ε)] ≤ O(δ)
deg(V) u∈N(v) deg(v) ∑u0∈N(v) eu0
(39)
holds true by the same argument as the base step. If we take r(I),..., r(L-I) = O( ε2 (log ɪ + log 1))
samples,
Pr[kzuLT)- ZuLT)I∣2 ≥ O(ε)] ≤ O(εδ)	(40)
holds true by the induction hypothesis. Therefore, Pr[∣zVL) - ZVL)Il2 ≥ ε] ≤ δ holds true by
equations (38), (39), and (40).
□
22
Under review as a conference paper at ICLR 2020
D	Computational Model Assumptions
In this study, we model our algorithm as an oracle machine that can make queries about the input
and measure the complexity by query complexity. Modeling our algorithm as an oracle machine and
measuring the complexity by query complexity are reasonable owing to the following reasons:
•	In a realistic setting, data is stored in a storage or cloud and we may not be able to load
all the information of a huge network on to the main memory. Sometimes the network is
constructed for access to the information on demand (e.g., in web graph mining, the edge
information is retrieved when queried). In such cases, reducing the number of queries is
crucial because accessing storage or cloud is very expensive.
•	Our algorithm executes a constant number of elementary operations of O (log n) bits (e.g.,
accessing the O(n)-th address, sampling one element from O(n) elements). Therefore,
if we assume that these operations can be done in constant time, the total computational
complexity of our algorithms will be constant. This assumption is natural because most
computers in the real-world can handle 64 bit integers at once and most of the network data
contain less than 264 ≈ 1019 nodes.
•	Even if the above assumption is not satisfied, our algorithm can be executed in O(log n) time
in terms of the strict meaning of computational complexity. This indicates that our algorithm
is still sub-linear and therefore it scales well. It should be noted that it is impossible to
access even a single node in o(log n) time in the strict meaning of computational complexity
because we cannot distinguish n nodes with o(log n) bits. Therefore, our algorithm has
optimal complexity with respect to the number of nodes n.
E	Graph Embedding
In this study, we model our algorithm as an oracle machine that can make queries about the input
and measure the complexity by query complexity. Modeling our algorithm as an oracle machine and
measuring the complexity by query complexity are reasonable owing to the following reasons:
•	In a realistic setting, data is stored in a storage or cloud, and we may not be able to load
all the information of a huge network on to the main memory. Sometimes the network is
constructed for access to the information on demand (e.g., in web graph mining, the edge
information is retrieved when queried). In such cases, reducing the number of queries is
crucial because accessing storage or cloud is very expensive.
•	Our algorithm executes a constant number of elementary operations of O (log n) bits (e.g.,
accessing the O(n)-th address, sampling one element from O(n) elements). Therefore,
if we assume that these operations can be done in constant time, the total computational
complexity of our algorithms will be constant. This assumption is natural because most
computers in the real-world can handle 64 bit integers at once, and most of the network data
contain less than 264 ≈ 1019 nodes.
•	Even if the above assumption is not satisfied, our algorithm can be executed in O(log n) time
in terms of the strict meaning of computational complexity. This indicates that our algorithm
is still sub-linear and therefore, it scales well. It should be noted that it is impossible to
access even a single node in o(log n) time in the strict meaning of computational complexity
because we cannot distinguish n nodes with o(log n) bits. Therefore, our algorithm has
optimal complexity with respect to the number of nodes n.
23
Under review as a conference paper at ICLR 2020
Table 2: Time complexity of embedding algorithms. ∆ denotes the maximum degree. It should be
noted that that in the dense graph, O(m) = O(n2) by definition.
Sparse	Dense
Proposed	O( ε1L (log 1 +log 1 )L-1 log 1) Exact	O(∆L)	O( ε1L (IOg ε + log 1)LT log δ) O(mL) = O(n2L)
F Time Complexity
We summarize the time complexity of approximation and exact algorithms in Table 2. Let BL = {v}
and Bl = Su∈B N (u) (l = 0, . . . , L - 1). In other words, BL-k is the k-hop neighbors of node
v . We need all features of B0 to calculate the exact embedding of node v. If the graph is sparse, the
size of BL-k grows exponentially with respect to k because deg(u) nodes are added to BL-k+1 for
each node u ∈ BL-k . Namely, it is bounded by ∆k . If the graph is dense, BL-k ≈ V (1 ≤ k ≤ L)
Therefore, complexity is linear with respect to the number of layers L and edges m. In contrast, the
approximation algorithm runs in constant time irrespective of the density of the input graph.
24