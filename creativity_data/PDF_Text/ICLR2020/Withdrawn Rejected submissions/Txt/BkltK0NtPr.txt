Under review as a conference paper at ICLR 2020
What Illness of Landscape Can Over-
parameterization Alone Cure?
Anonymous authors
Paper under double-blind review
Ab stract
Over-parameterized networks are widely believed to have nice landscape, but
what rigorous results can we prove? In this work, we prove that: (i) from under-
parameterized to over-parameterized networks, there is a phase transition from hav-
ing sub-optimal basins to no sub-optimal basins; (ii) over-parameterization alone
cannot eliminate bad non-strict local minima. Specifically, we prove that for any
continuous activation functions, the loss surface of a class of over-parameterized
networks has no sub-optimal basin, where “basin” is defined as the setwise strict
local minimum. Furthermore, for under-parameterized network, we construct loss
landscape with strict local minimum that is not global. We then show that it is
impossible to prove “all over-parameterized networks have no sub-optimal local
minima”, by giving counter-examples for 1-hidden-layer networks with a class of
neurons.
Viewing various bad patterns of landscape as illnesses (bad basins, flat regions,
etc.), our results indicate that over-parameterization is not a panacea for every
“illness” of the landscape, but it can cure one practically annoying illness (bad
basins).
1 Introduction
It is empirically observed that over-parameterized networks can achieve small training error Bengio
et al. (2006); Zhang et al. (2017); Livni et al. (2014); Neyshabur et al. (2017); Geiger et al. (2018).
These observations are often explained by the intuition that more parameters can “smooth the
landscape” Livni et al. (2014); Lopez-Paz & Sagun (2018). While the intuition looks reasonable, can
we provide some rigorous theory to formalize this intuition that “over-parameterized networks have
nice landscape”?
Before answering this question, we would like to draw an analogy to the deep learning landscape.
If we view the landscape as a human body, the bad properties (e.g., sub-optimal minima, saddle
points, flat regions) can be regarded as different types of illness. Then, the aforementioned question
become: Can over-parameterization cure all these diseases? If not, which specific type of illness can
over-parameterization cure?
In this work, we aim at clearly identifying the “pharmacodynamics" of over-parameterization alone.
To this end, we derive a serial of results on the loss landscape. Interestingly, our results are established
in a way similar to a scientific experiment.
First, we hope to find out a property that holds for over-parameterized networks but not under-
parameterized networks. Here the over-parameterized networks are like the treatment groups, while
the under-parameterized networks are like the control group. In this way, we can claim that over-
parameterization does really benefit the landscape.
Second, our theory should has minimal assumptions on other aspects except over-parameterization.
Potentially, there are many other factors that affects the landscape. For example, with linear neurons
Kawaguchi (2016) “no bad local inima” can be proved, implying that the choice of activation functions
can impact the landscape. Thus, we should treat factors like activation function, loss function and
training data as “control variables". We would like to minimize the effect of these factors, and see
how far we can go based on merely the condition of over-parameterization.
1
Under review as a conference paper at ICLR 2020
Figure 1: An example of a weakly global function (a) and a non-weakly-global function (b). Both
functions have bad non-strict local minima, consisting a plateau of (-3, -1). The plateau in the right
figure is the bottom of a basin, entailing a bad strict local minimum in the sense of sets.
To conclude, in this work we endeavor to explore the “true benefit of over-parameterization
alone”.
•	“True”: In terms of “true”, we specify that the advantage of over-parameterization is not
the commonly-believed “elimination of bad local minima”, but “elimination of bad basin”.
On the positive side, we prove that bad basin does not exist in over-parameterized neural
networks. On the negative side, we show that bad local minima do exist.
•	“Benefit”: In terms of “benefit”, we emphasize that “elimination of bad basin” is indeed
a property brought by over-parameterization. To demonstrate this, we give an example to
show that bad strict local minima exist in “exactly” under-parameterized neural networks,
and analyze how over-parameterization makes a difference in the landscape of loss surface.
•	“Alone”: In terms of “alone”, we want to remove other confounding factors that may help
the landscape. For instance, linear networks are known to have nice landscape, implying
that activation functions can have a large impact on the landscape. Thus we want to reduce
the impact of neuron activations by imposing mild condition on the activations.
More details are given in the next subsection.
1.1	Main Contributions
We first explain some geometrical concepts. A function is called “global function” if no sub-optimal
local minimum exists. A function is called “weakly global function” if it admits no setwise bad
strict local minima (defined in Josz et al. (2018)), as illustrated in Figure 1(a). Intuitively, weakly
global functions may have flat regions of local minima (Figure 1(a)), but do not have “truly bad” local
minima that are surrounded by barriers (Figure 1(b)).
A key geometrical property studied in our paper is called “Property PT”: at any point, after a small
perturbation there is a strictly decreasing path to the global minima. Property PT was proposed in the
early work Yu & Chen (1995), and was claimed to imply “no bad local minima”, but we provide a
counter-example to show that this is not true. We show that Property PT actually implies “no bad
setwise strict local minima” (i.e. weakly global function).
Our first result roughly states that for any continuous activation function, the loss function of over-
parameterized networks (with a wide last hidden layer) is a weakly global function. Therefore,
over-parameterized networks have nice landscape in the sense that truly bad local minima do not
exist. We also extend this result to the case with one wide layer (not necessarily the last hidden layer)
1, though with extra assumptions on the structure and some neurons (see Assumption 2).
1	This setting of network structure is more general than the first setting, and is quite close to practice: for
instance, ImageNet dataset in ILSVRC competition has around 1.2 million training samples, while VGG and
Inception network have 3 and 1.3 million neurons in the widest layer respectively.
2
Under review as a conference paper at ICLR 2020
Last but not least, we show by counter-example that, with other assumptions unchanged, under-
parameterized networks may have sub-optimal basins, i.e, not weakly global.
We provide a brief summary of our contributions as follows:
•	We give an example to show over-parameterized neural networks can have bad local minima
for a large class of neurons (Section 6.2), which reveals the limitation of over-paramterized
networks.
•	We give an example to show that Property PT does not imply “no bad local minima”
(Section 6.1). This shows that the proof of a classical work Yu & Chen (1995) on over-
paramterized networks is incorrect. This example improves our geometrical understanding
of the landscape.
•	When the last hidden layer has at least N neurons (N is the number of samples), for any
continuous activations, we prove that the loss function is weakly global (Theorem 1).
•	When there is one wide layer with at least N neurons, under extra assumptions the loss
function is a weakly global function (Theorem 2).
•	We show that under-parameterized neural networks do have bad strict local minima, for
any number of neurons and any generic data x. Our results clarify the role of over-
parameterization in eliminating bad strict local minima.
1.2	Paper Organization
This paper is organized as follows. We first provide a geometric explanation of our main idea in
Section 2 and review some related works in Section 3. In Section 4, we present the main results.
In Section 5, we highlight the main proof ideas. Section 6 provides various examples of bad local
minima. Conclusions are presented in Section 7.
2	Geometric Explanation
2.1	Landscape Analysis of the 1 -Dimensional Case
Simple examples can go far: the phenomenons revealed by simple examples can often offer great
insight about very general problems. One good example is minv,w∈R(vw - 1)1 2 3: it is a non-convex
function but it does not have bad local-min as the reparameterization does not destroy the nice
landscape of the outer function. This insight can be extended to even the general deep linear networks
Kawaguchi (2016). In this part, we discuss the 1-dim non-linear problem (1 - vσ(w))2. Interestingly,
the geometrical phenomenons for this general 1-dim case still occur in deep over-parameterized
networks (but not in under-parameterized networks). Therefore, to understand the major geometrical
properties discussed in this paper, one only needs to understand this 1-dimensional example.
Consider a 1-hidden-layer-1-neuron network with network output y = vσ(wx). Assume that only one
data sample pair (x, y) = (1, 1) is given and the quadratic loss is employed. Thus, the empirical loss
is given by E(w, V) = (y - y)2. Then, with the given sample pair, the empirical loss is represented
by (1 - vσ(w))2.
The landscape of the considered network is determined by the activation function. In particular, we
discuss the following three cases:
1. The activation function is linear, i.e., σ(z) = z. The empirical loss is thus (1 - uv)2, which
obviously does not contain bad local minima.
2. The activation function is nonlinear and strictly monotone. Bad local minima also do not
exist in this case.
3. The activation function is nonlinear and non-monotone. In this case, bad local minima may
exist. For example, the activation function is quadratic, i.e. , σ(z) = z2 . Points on line
segment l : {(w, v) | w = 0, v < 0} are all spurious local minima with a loss of 1. To see
this, note that vw2 is non-positive in the neighborhood of line segment l, so (1 - vw2 )2 is
always at least 1. One important feature in this example is that w = 0 is a local minimum of
3
Under review as a conference paper at ICLR 2020
σ(w). This means that after perturbing w, σ(w) can only move in one direction so that the
sign of vw2 can be controlled.
Case 1 is generalizable to deep networks: as mentioned earlier, the geometrical property of deep
linear networks is similar to 1-dim linear problem. Which case can reflect the geometrical property of
a deep non-linear over-parameterized networks? Currently, it seems that Case 2 cannot be extended:
it is still open whether increasing activation can eliminate bad local-min 2. This is why we do not
want to focus on increasing activations since there is a gap between the simplest case and the general
case.
Our paper intends to show that Case 3 captures the major geometrical properties of a general deep
non-linear over-parameterized networks: it has bad non-strict local-min, but it has no bad basin.
The intuition comes as the following: in the example constructed, we can find that these bad local
minima in fact lie in a plateau (so they are non-strict). Moreover, at the boundary of the plateau
there is a decreasing direction. This implies that these bad local minima do not lie in a basin, so
theoretically they are possible to escape with algorithms like noisy GD. In the rest of this paper,
we will show that this is the case for all deep over-parameterized neural networks, i.e., all deep
over-parameterized neural networks do not contain bad basins. This result characterizes what benefits
over-parameterization alone brings to general neural networks.
2.2	Strict Local Minimum Exists for Under-parameterized Neural Networks
One may wonder whether the previous 1-dim example actually reveals the insight of all networks, not
just over-parameterized networks. We show that the answer is no: for under-parameterized networks,
strict bad local-min exists for fairly general settings. Our examples also explains the geometrical
reasons how over-parameterization “cures” the illenss of bad basins.
Suppose there are two sample pairs (x1, y1) = (1, 2), (x2, y2) = (2, 0). Consider a 1-hidden-layer-
1-neuron neural network y = vσ(wx). Let (v,w) = (1,0) and σ(z) = z2(1 一 z2) + 1. Then
(yι, y2) = (1,1). We plot the figure of the output data and activation function σ(∙) separately.
We will now show that (v, w) = (1, 0) is a bad strict local minimum through these two figures.
In Figure 2(a), We mark out the true data (y1,y2) = (2,0) and the current output (yι, y2) =
(1,1). Note that σ(wxι) = σ(wx2) = σ(0) = 1, if we change only V and not w, (y1, y2)=
(v0σ(wxι), v0σ(wx2)) = (v0, v0). This means that if we only perturb v, the new output (y1 ,y2) will
move along line h = {(e1, e2) : e1 = e2} (illustrated as the red line in Figure 2(a)). On the other
hand, (y1,y2) = (1,1) is exactly the projection of (yι, y2) to line h. Consequently, perturbing V
alone will move the new output (y1, y2) away from the true data (y1,y2), thus strictly increasing the
empirical loss (distance).
What if we perturb w simultaneously? In Figure 2(b) we plot the value of σ(wx) before and after
perturbation. Before perturbation, σ(wx1) = σ(wx2) = 1. After perturbation, both σ(w0x1) and
σ(w0x2) rise above 1. However, since x2 = 2 > 1 = x1, σ (w0x2) is always larger than σ(w0x1) due
to the special landscape of σ(∙). This implies that we always have y2 = v0σ(w0x2) > v0σ(w0x1)=
yi. Therefore, the feasible region of (y1, y2) is above line h (illustrated as the cyan region in Figure
2(a)), and hence any perturbation of (v, W) will result in a larger distance between (y1, y2) and
(y1, y2), increasing the empirical loss.
From the analysis above, we can conclude that a bad strict local minimum occurs when
•	Due to under-parameterization, the perturbation direction provided by the last hidden-layer
lies in a hyperplane of the whole space.
•	Due to landscape of activation function, the perturbation direction provided by the previous
hidden layers lies in a halfspace.
The first condition demonstrates the key difference between under-parameterized and over-
parameterized neural networks. For over-parameterized networks, one of the following two cir-
cumstances happens: a) the perturbation direction provided by the last hidden-layer is the whole
2assuming generic data. For very special data, constructing bad local-min is possible but it is not that
interesting, since the proofs of positive results often assume generic data.
4
Under review as a conference paper at ICLR 2020
Figure 2: The figure of the output data (a) and activation function σ(∙) (b) in the proposed 1-
hidden-layer-1-neuron example. In (a), (y1,y2) = (2,0) is the true data, (y1 ,y2) = (1,1) is the
current output and the cyan area is the feasible region of (y1 ,y'2) if We perturb both V and w. In
(b), wx1 = wx2 = 0 is the current input of σ(∙), while w2x1 and w2x2 are possible inputs after
perturbation.
space, implying that a decreasing path exists if the current point is not a global minimum; (b) the
perturbation direction provided by the last hidden-layer still lies in a hyperplane of the whole space.
In this circumstance, since neurons are over-parameterized, the input of the last hidden layer is
linearly dependent. Hence, there must exist a perturbation of the last hidden layer that keeps the
objective function value unchanged, eliminating bad strict local minimum. Thus over-parameterized
networks can not have bad strict local minimum. In contrast, for under-parameterized networks, even
if the the input of the last hidden layer is linearly independent, the perturbation direction provided by
the last hidden-layer still lies in a hyperplane of the whole space. If the true data does not lie in this
hyperplane, bad strict local minimum may exist.
The second condition demonstrates that existence of bad local minimum is highly related to the
perturbation direction determined by the landscape of activation function. The landscape that leads to
a restricted perturbation direction, as analyzed in the proof, is in fact rather common and not specified
to particular activation functions. This finding implies that existence of bad strict local minimum is
commonly seen in under-parameterization networks. It is noteworthy that over-parameterization, on
the contrary, does not play a decisive role in perturbation direction. That’s why bad local minimum
still exists in over-parameterization networks. However, due to the first condition, bad strict local
minimum is eliminated by over-parameterization.
Finally, this example can be generalized to under-parameterized neural networks with any width
through the following proposition:
Proposition 1 For any N input data x1,…,XN ∈ R With Xn = Xn, ∀n = n2, there exists N
output data y1, ∙ ∙ ∙ , yN ∈ R and a 1 -hidden-layer neural network with N - 1 neurons such that the
empirical loss E(∙) has bad strict local minimum.
It is noteworthy that the requirement for σ(∙) is quite mild. We only need to characterize first-order
and second-order derivatives of σ(∙) at specific points rather than in the whole domain of R. This
implies that such σ(∙) we construct can possess good properties such as monotonicity. Furthermore,
we can relax the requirement for the second-order derivative to be PN=I X ∙ aiσ2(wjXi) > 0 for all
j = 1,…，N — 1. With this relaxed requirement, σ(∙) can even be convex.
3	Related Works and Discussions
We notice that our results may have a strong connection to the “lottery ticket hypothesis" Frankle &
Carbin (2019). It was found in Frankle & Carbin (2019) that a large (and generally over-parameterized)
feedforward network contains a subnetwork that, when properly initialized, can be trained to achieve
similar performance to the original network, i.e., the “wining ticket". However, if such subnetwork is
5
Under review as a conference paper at ICLR 2020
randomly initialized, it no longer matches the performance of the original network. This experiment
is nice as it rules out the effect of representation power, and indicates that the worse performance of
small networks is due to optimization, not due to representation power. Which part of the optimization
makes the difference? Early works Dauphin et al. (2014) conjectured that saddle points cause bad
performance, but there is no clear evidence that saddle points exist in small networks but not in
large networks. Our result proved rigorously that at least bad basin is a difference between small
and big networks. Thus based on our result, we conjectured that the lottery tickets are due to bad
basins. The verification of this conjecture is beyond the scope of this paper (since checking basins is
computatationally expensive), and we leave it to future work.
The loss surface of neural networks has been extensively studied in recent years Du & Lee (2018); Ge
et al. (2018); Andoni et al. (2014); Sedghi & Anandkumar (2014); Janzamin et al. (2015); Haeffele &
Vidal (2015); Gautier et al. (2016); Brutzkus & Globerson (2017); Soltanolkotabi (2017); Soudry &
Hoffer (2017); Goel & Klivans (2017); Boob & Lan (2017); Du et al. (2017); Zhong et al. (2017); Li
& Yuan (2017); Liang et al. (2018a); Mei et al. (2018); Sirignano & Spiliopoulos (2018a;b); Safran
& Shamir (2018); Wang et al. (2018); Chizat & Bach (2018); Li & Liang (2018). Most works are
only about one-hidden-layer networks (many assume second layer weights are fixed, and many have
strong assumptions on data distributions). There are some works on deep neural networks, but they
either only show a subset of local minima are global minima Haeffele et al. (2014); Haeffele & Vidal
(2015); Soudry & Carmon (2016); Nguyen & Hein (2017a;b); Shamir (2018), or require special
neurons such as linear neurons Baldi & Hornik (1989); Kawaguchi (2016); Freeman & Bruna (2016);
Hardt & Ma (2017); Yun et al. (2017).
Some very recent works Du et al. (2018a); Allen-Zhu et al. (2018); Zou et al. (2018) prove that GD
(gradient descent) converges to global minima at a linear rate for deep neural networks, which extend
earlier works on 1-hidden-layer networks Li & Liang (2018); Du et al. (2018b). Their conclusions are
stronger than our landscape analysis since they can prove linear convergence, but their assumption
is also stronger. In particular, these works require a large number of neurons, e.g. at least O(nk)
neurons in every layer for deep networks, where n is the number of samples and k is a certain integer
such as k = 24 in Allen-Zhu et al. (2018). In practical neural networks such as VGG and ResNet,
the number of neurons is in the order of O(n), and even O(n2) neurons is difficult to implement in
practice. It will be interesting to prove results that combine the strengths of those works and our
landscape analysis.
Moreover, two recent works Nguyen et al. (2018); Liang et al. (2018b) have addressed similar issues
to our work. Specifically, Nguyen et al. (2018) covers a rather broad range of network structures, but
only holds for a limited family of activation functions, which does not include activation functions
such as ReLU, leaky ReLU, quadratic and Swish activation Ramachandran et al. (2018). Liang et al.
(2018b) eliminates bad local minima by adding a special type of exponential neuron. In contrast, our
result holds for any continuous activation function, so is closer to practical settings in this sense. In
addition, these works do not show a phase transition from under-parameterized to over-parameterized
networks.
Finally, landscape analysis is just one part of the deep learning theory, which includes representation,
optimization and generalization. In terms of generalization, many recent works Neyshabur et al.
(2017); Bartlett et al. (2017); Poggio et al. (2018) try to understand why over-parameteriztion does
not cause overfitting. This is a very interesting line of research, but its underlying assumption that
over-parameterization can lead to small training error still requires rigorous justification. Our study is
orthogonal and complimentary to the research on generalization error.
4	Main Theorems
4.1	Network Model
We consider a fully-connected neural network fW : Rdx → Rdy that maps an input data x ∈ Rdx to
a predicted output y ∈ Rdy. Specifically, the network output is given by
y = fw(x) = Wh+ioh(Wh …σ2(W2σ1(W1x)))	⑴
where H is the number of hidden layers, σ : Rh → R is the neuron activation function (sometimes
simply called “activation” or “neuron”) of the h-th hidden layer, Wh ∈ Rdh ×dh-1 is the weight
6
Under review as a conference paper at ICLR 2020
matrix to the h-th hidden layer for h = 1,…，H, WH ∈ RdH+1 ×dH is the weight matrix to the
output layer. Note that we define d0 = dx and dH+1 = dy, and the scalar function σh is applied
entry-wise if its input is a vector or a matrix. Throughout the paper, we omit the “bias” term in the
expression of neural networks for simplicity of presentation.
Let W = (Wι,…，WH) denote the collection of all network weights. Given N pairs of training
data (x(n), y(n)),n = 1,…，N, the empirical loss of the considered network as
ʌ.
E(W ) = l(Y,Y)	⑵
where Y，[y ⑴，…，y(N)] ∈ Rdy × N, Y，[fw (X(I)),…，fw (X(N))] ∈ Rdy × N ,and l(∙, ∙) is the
loss function. Then, the training problem of the considered network is to find W that minimize the
empirical loss E(W).
4.2	Main Theorems
In this section, we present our main result on the absence of sub-optimal basin on the loss surface.
Specifically, we define “basin" as the setwise strict local minimum, a notion borrowed from Josz et al.
(2018).
Definition 1 (Setwise strict local minimum) We say that a compact subset X ∈ S is a strict local
minimum of f : S → R in the sense of sets if there exists ε > 0 such that for all X ∈ X and for all
y ∈ S \ X satisfying kX - yk2 ≤ ε, it holds that f(X) < f(y).
Definition 1 generalizes the notion of strict local minimum from the sense of points to the sense of
sets. Any strict local minimum must be setwise strict local minimum, but not vice versa. A pleateau
at the bottom of a basin, as shown in the right part of Figure 1, is also a setwise strict local minimum.
Definition 2 (Weakly global function) We say that f : S → R is a weakly global function if it is
continuous and all setwise strict local minima are setwise global minima.
Definition 2 introduces an important class of continuous functions, termed weakly global functions,
which admits no bad strict local minima in the sense of sets, and hence no sub-optimal basin.
Now we specify our assumptions for the first theorem. on the training dataset, the loss functions, the
over-parameterization, and the activation functions.
Assumption 1
A1 There exists a dimension k such that X(kn) 6= X(kn ), ∀n 6= n0;
A2 The loss function l(Y, Y) is convex respect to Y;
A3 dH ≥ N;
A4 The activation function σh is Continuousfor all h = 1,…，H.
Assumption A1 implies that the input data samples need to be distinguished with each other in
one dimension. This can be always achieved if we allow an arbitrarily small perturbation on data.
Assumption A2 is satisfied for almost all commonly-used loss functions, including quadratic, cross
entropy, etc. Assumption A3 is the over-parameterization assumption, which only requires the last
hidden layer to be wide. There is no assumption on the width of all other hidden layers. Assumption
A4 is a very mild assumption on the neuron activation that it should be continuous.
We are now ready to present the first main theorem.
Theorem 1 Suppose that a fully connected neural network satisfies Assumption 1. Then, the empiri-
cal loss E(W) is a weakly global function.
7
Under review as a conference paper at ICLR 2020
Theorem 1 states that the empirical loss function of an over-parameterized neural network is weakly
global as long as the activation function is continuous. Note that the notion of weakly global function
is distinct from that of “no bad local valleys" used in Nguyen et al. (2018); Venturi et al. (2018), but
they both guarantee non-existence of bad strict local minimum. Formally, we have the following
corollary.
Corollary 1 The loss surface of a fully connected neural network satisfying Assumption 1 has no
bad strict local minimum.
Theorem 1	requires that the last hidden layer is sufficiently “wide". Next, we show that if we add a
pyramid structure after the “wide” hidden layer, the resulting loss surface still has no sub-optimal
basin. We specify such pyramid structure by the the following assumption.
Assumption 2 There exists 1 ≤ h0 ≤ H such that
B1 dho ≥ N, dho ≥ dh0+1 ≥ ∙∙∙ ≥ dH+1 ；
B2 For all h0 + 1 ≤ h ≤ H, the activation function σh is non-increasing or non-decreasing
over R.
Then it can be shown that our main result still holds after adding the pyramid architecture:
Theorem 2	Suppose that a fully connected neural network satisfies Assumption A1, A2, A4, and 2.
Then, the empirical loss E(W) is a weakly global function.
Corollary 2 The loss surface of a fully connected neural network satisfying Assumption A1, A2, A4,
and 2 has no bad strict local minimum.
5	Proof Ideas
5.1	Property that Eliminates Bad Strict Local Minimum
Consider a 1-hidden-layer network with d input dimensions and m hidden-layer neurons. The output
of the network is given by t = v>σ(WX) ∈ R1×N, where X = [x1,x2,…，XN] ∈ Rd×N is the
input data matrix consisting of N data samples, W ∈ Rm×d is the weights to the hidden layer,
v ∈ Rm is the weights to the output layer. The loss function is quadratic, i.e., l(y, t) = ||t - y||22.
The network is assumed to be over-parameterized, i.e., m > N.
Now we denote Z = σ(WX) ∈ Rm×N as the output matrix of the hidden layer. The training of the
considered network can be formulated as
min t - v> Z 2	(3)
v,W
A simple but important observation Gori & Tesi (1992) is that if Z is of full column rank, i.e., rank-N,
problem equation 3 (with fixed W) is strongly convex with respect to v . Then, we can find a strictly
decreasing path to the global infimum (with respect to v) of equation 3. Moreover, for any t, there
exists v such that v> Z = t, so the infimum equals 0, which is the global infimum ofequation 3.
Thus, there is a strictly decreasing path to the global infimum of equation 3, and hence the network
parameter (W, v ) cannot be a bad local minimum if the corresponding Z is rank-N.
It is noteworthy that when Z is not full rank, the infimum (with respect to v) of equation 3 may not be
equal to the global infimum of equation 3, so the previous decreasing-path approach does not work.
(Obviously, matrix Z is not always rank-N. For example, if W is a zero matrix, the corresponding Z
is at most rank-1 regardless of the number of hidden neurons and activation function.) Regarding this,
we introduce a property that can eliminate all bad setwise strict local minima on the loss surface.
Property PT: After a generic perturbation of any initial weight, there exists a strictly decreasing path
to the global infimum.
8
Under review as a conference paper at ICLR 2020
We claim that if Property PT holds, the loss function with respect to (v, W) is a weakly global
function, which admits no setwise bad strict local minimum. Note that weakly global function also
implies the non-existence of (point-wise) bad strict local minimum.
Whether Property PT holds highly depends on the activation function. In this paper, we identify a
class of activation functions that guarantee Property PT. Specifically, we show that if the activation
function is analytic and has non-vanishing derivatives at zero up to N -th order, Property PT holds.
This enables us to prove that the loss surface is weakly global. However, to extend this result to all
continuous activation functions remains a challenging task.
5.2	No Bad Strict Local Minimum for Continuous Activations
So far, we have identified a class of analytic activation functions such that the loss functions have
Property PT. Although these functions do not cover many commonly used activations like quadratic,
sigmoid, ReLU, etc, they constitute a dense set in the space of continuous functions. That is, for any
continuous function f, there exists a sequence of analytic functions that uniformly converges to f,
and each of the analytic functions has non-vanishing derivatives at zero up to N-th order. Further, we
show that the uniform convergence of activation function implies the compact convergence of loss
function. As the property of weakly global is preserved under compact convergence, we prove that
the loss surface weakly global for any continuous activation.
To summarize, our proof can be sketched in the following three steps:
Step 1: If the activation function has non-vanishing derivatives at zero up to N-th order, then Property
PT holds, and the loss surface is a weakly global function.
Step 2: The set of activation functions in Step 1 are dense in the space of continuous functions.
Step 3: By the “closedness" of weakly global functions, the loss surface for any continuous activation
is a weakly global function.
We note that in Step 3, the “closedness" of weakly global functions means that if a sequence of
weakly global functions converges compactly to a function, then that function is also weakly global.
This is a slight extension of a proposition in Josz et al. (2018).
6	Examples of Bad Local Minima
In this section, we will give some counter-examples to illustrate the “tightness” of our conditions
(cannot be weakened too much) and conclusion (cannot be strengthened under the current setting).
6.1	Property PT Does Not Imply “No Bad Local Minima”
It is worth mentioning that our result only implies the non-existence of bad strict local minima,
instead of bad local minima. This is in constrast with what Yu et al. claimed in Yu & Chen (1995)
that if Property PT holds, an over-parameterized single-hidden-layer neural network has no bad local
minimum (i.e., Theorem 3 in Yu & Chen (1995)). While this claim seems correct (in fact, it is correct
in one dimensional case), we could not provide a rigorous proof to this claim. Later, we found a
counter-example to this claim, which implies that Property PT can only eliminate bad strict local
minima, but cannot eliminate bad local minima.
This counter-example is drawn in Fig. 3, which illustrates the loss surface of Example 1 with one
hidden neuron. The point (v, W) = (1, 1) is abad non-strict local minimum. However, after a generic
perturbation within a sufficiently small neighbourhood of (1, 1), there exists a strictly decreasing path
to the global minimum (-2, -1).
6.2	Bad (Non-strict) Local Minima Can Exist for Wide Networks
Although Property PT does not imply the non-existence of bad local minima, one may wonder
whether it can be proved by other proof techniques. In particular, is it possible to prove the non-
existence of bad local minima under the same setting of Yu et al. Yu & Chen (1995)? Below, we
9
Under review as a conference paper at ICLR 2020
4
3 3
2
0
3
2
2
0
0
-2
Figure 3: The loss surface of Example 1 with d = 1, (x, y) = (1, 1), and σ(z) = -(z - 1)2/8. The
point (v, W) = (1, 1) is a bad non-strict local minimum.
give a counter-example of Theorem 3 in Yu & Chen (1995) that for a class of neuron activations that
satisfy the condition of Yu et al. Yu & Chen (1995)3 , bad local minima exist.
Example 1 (Bad non-strict local minimum) Consider a 1-hidden-layer neural network with 1 data
sample, 1 input dimension, and d hidden-layer neurons. The data sample satisfies x, y 6= 0. The
activation function σ is analytic and has non-vanishing derivatives at zero up to N-th order. Thus,
Property PT holds. Moreover, suppose that there exists t 6= 0 and δ such that σ(t) = 0 and σ(t0) ≤ 0
for t - δ < t0 < t + δ.
Now, let v* = (sign (y), sign (y), ∙∙∙ , sign (y))> and W * = (t∕x,t∕x,…,t∕x)τ. Then,
v*τσ(W*x) = 0 and the corresponding loss l(v*,W*) = y2 > 0. In addition, since y ∙ (vτσ(Wx))
is always non-positive in the neighborhood of (v*, W*), l(v, W) is always at least y2 in the neigh-
borhood of (v*, W*), implying that (v*, W*) is a bad local minimum.
Example 1 shows that the loss surface can have bad non-strict local minimum even if the activation
function has Property PT and the neural network is arbitrarily wide. It also shows the necessity of
rigorous mathematical analysis: while it is widely believed that over-parameterized networks can
“smooth” the landscape, the exact notion of “smoother landscape” was not clearly stated.
7	Conclusions
In this paper, we studied the loss surface of over-parameterized fully connected deep neural networks.
We show that if the last hidden layer has no less neurons than the number of samples, for any
continuous activation functions, the loss function is a weakly global function, i.e., the loss landscape
has no setwise strict local minima (bad basins). We also show that for almost all analytic activation
functions, starting from any point, after a small perturbation there exists a strictly decreasing path to
the global infimum. We then extend our result to the deep neural network with one wide layer (not
necessarily the last hidden layer) under extra assumptions. On the other hand, for a single-hidden-
layer network with hidden neurons less than the data samples, we construct an example to show that
sub-optimal strict local minimum exists. A geometric explanation is further provided to demonstrate
how over-parameterization helps in eliminating bad basins.
In a word, our work shows that over-parameterization cannot bring perfect landscape to deep neural
networks. However, it does really benefit the landscape by eliminating setwise strict local minimum.
Our work clarify the role of over-parameterization alone in smoothing the loss landscape.
3 Although Yu et al. Yu & Chen (1995) assumes sigmoid activation, their proof only uses the property that
the activation has non-vanishing derivatives up to the N-th order, which holds for sigmoid. Such property also
holds for a broad range of activations considered in the counter-example Example 1, thus this counter-example
can be viewed as a counter-example to the setting of Yu & Chen (1995).
10
Under review as a conference paper at ICLR 2020
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. arXiv preprint arXiv:1811.03962, 2018.
A. Andoni, R. Panigrahy, G. Valiant, and L. Zhang. Learning polynomials with neural networks. In
ICML, 2014.
P. Baldi and K. Hornik. Neural networks and principal component analysis: Learning from examples
without local minima. Neural networks, 2(1):53-58, 1989.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Advances in Neural Information Processing Systems, pp. 6240-6249, 2017.
Yoshua Bengio, Nicolas L Roux, Pascal Vincent, Olivier Delalleau, and Patrice Marcotte. Convex
neural networks. In NIPS, pp. 123-130, 2006.
D. Boob and G. Lan. Theoretical properties of the global optimizer of two layer neural network.
arXiv preprint arXiv:1710.11241, 2017.
A. Brutzkus and A. Globerson. Globally optimal gradient descent for a convnet with gaussian inputs.
arXiv preprint arXiv:1702.07966, 2017.
Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-parameterized
models using optimal transport. arXiv preprint arXiv:1805.09545, 2018.
Y. N. Dauphin, R. Pascanu, C. Gulcehre, K. Cho, S. Ganguli, and Y. Bengio. Identifying and attacking
the saddle point problem in high-dimensional non-convex optimization. In NIPS, pp. 2933-2941,
2014.
S. S Du and J. D Lee. On the power of over-parametrization in neural networks with quadratic
activation. arXiv preprint arXiv:1803.01206, 2018.
S. S. Du, J. D. Lee, and Y. Tian. When is a convolutional filter easy to learn? arXiv preprint
arXiv:1709.06129, 2017.
Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. arXiv preprint arXiv:1811.03804, 2018a.
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. arXiv preprint arXiv:1810.02054, 2018b.
Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural
networks. ICLR, 2019.
C D. Freeman and J. Bruna. Topology and geometry of half-rectified network optimization. ICLR,
2016.
A. Gautier, Q. N. Nguyen, and M. Hein. Globally optimal training of generalized polynomial neural
networks with nonlinear spectral methods. In NIPS, pp. 1687-1695, 2016.
R.	Ge, J. D Lee, and T. Ma. Learning one-hidden-layer neural networks with landscape design. ICLR,
2018.
Mario Geiger, Stefano Spigler, StePhane d,Ascoli, Levent Sagun, Marco Baity-Jesi, Giulio Biroli,
and Matthieu Wyart. The jamming transition as a paradigm to understand the loss landscape of
deep neural networks. arXiv preprint arXiv:1809.09349, 2018.
S.	Goel and A. Klivans. Learning depth-three neural networks in polynomial time. arXiv preprint
arXiv:1709.06010, 2017.
Marco Gori and Alberto Tesi. On the problem of local minima in backpropagation. IEEE Transactions
on Pattern Analysis & Machine Intelligence, (1):76-86, 1992.
B. Haeffele, E. Young, and R. Vidal. Structured low-rank matrix factorization: Optimality, algorithm,
and applications to image processing. In ICML, 2014.
11
Under review as a conference paper at ICLR 2020
B. D Haeffele and R. Vidal. Global optimality in tensor factorization, deep learning, and beyond.
arXiv preprint arXiv:1506.07540, 2015.
M. Hardt and T. Ma. Identity matters in deep learning. ICLR, 2017.
M. Janzamin, H. Sedghi, and A. Anandkumar. Beating the perils of non-convexity: Guaranteed
training of neural networks using tensor methods. arXiv preprint arXiv:1506.08473, 2015.
C Josz, Y Ouyang, UC IEOR, RY Zhang, J Lavaei, and S Sojoudi. A theory on the absence of
spurious solutions for nonconvex and nonsmooth optimization. NIPS, 2018.
Wilfred Kaplan. Approximation by entire functions. Michigan Math. J.,3(1):43-52, 1955. doi:
10.1307/mmj/1031710533. URL https://doi.org/10.1307/mmj/1031710533.
K. Kawaguchi. Deep learning without poor local minima. In NIPS, pp. 586-594, 2016.
Y. Li and Y. Yuan. Convergence analysis of two-layer neural networks with relu activation. In NIPS,
pp. 597-607, 2017.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. In Advances in Neural Information Processing Systems, pp. 8157-8166,
2018.
S. Liang, R. Sun, Y. Li, and R. Srikant. Understanding the loss surface of neural networks for binary
classification. 2018a.
Shiyu Liang, Ruoyu Sun, Jason D Lee, and R Srikant. Adding one neuron can eliminate all bad local
minima. NIPS, 2018b.
Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir. On the computational efficiency of training neural
networks. In Advances in Neural Information Processing Systems, pp. 855-863, 2014.
David Lopez-Paz and Levent Sagun. Easing non-convex optimization with neural networks. 2018.
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of
two-layers neural networks. arXiv preprint arXiv:1804.06561, 2018.
Boris Mityagin. The zero set of a real analytic function. arXiv preprint arXiv:1512.07276, 2015.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring general-
ization in deep learning. In Advances in Neural Information Processing Systems, pp. 5947-5956,
2017.
Q. Nguyen and M. Hein. The loss surface and expressivity of deep convolutional neural networks.
arXiv preprint arXiv:1710.10928, 2017a.
Q. Nguyen and M. Hein. The loss surface of deep and wide neural networks. arXiv preprint
arXiv:1704.08045, 2017b.
Quynh Nguyen, Mahesh Chandra Mukkamala, and Matthias Hein. On the loss landscape of a class
of deep neural networks with no bad local valleys. arXiv preprint arXiv:1809.10749, 2018.
T Poggio, K Kawaguchi, Q Liao, B Miranda, L Rosasco, X Boix, J Hidary, and HN Mhaskar. Theory
of deep learning iii: the non-overfitting puzzle. Technical report, Technical report, CBMM memo
073, 2018.
Prajit Ramachandran, Barret Zoph, and Quoc V Le. Searching for activation functions. 2018.
Itay Safran and Ohad Shamir. Spurious local minima are common in two-layer relu neural networks.
ICML, 2018.
H. Sedghi and A. Anandkumar. Provable methods for training neural networks with sparse connectiv-
ity. arXiv preprint arXiv:1412.2693, 2014.
O. Shamir. Are resnets provably better than linear predictors? arXiv preprint arXiv:1804.06739,
2018.
12
Under review as a conference paper at ICLR 2020
Justin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of neural networks. arXiv
preprint arXiv:1805.01053, 2018a.
Justin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of neural networks: A central
limit theorem. arXiv preprint arXiv:1808.09372, 2018b.
M. Soltanolkotabi. Learning relus via gradient descent. In NIPS, pp. 2004-2014, 2017.
D. Soudry and Y. Carmon. No bad local minima: Data independent training error guarantees for
multilayer neural networks. arXiv preprint arXiv:1605.08361, 2016.
D. Soudry and E. Hoffer. Exponentially vanishing sub-optimal local minima in multilayer neural
networks. arXiv preprint arXiv:1702.05777, 2017.
Luca Venturi, Afonso Bandeira, and Joan Bruna. Spurious valleys in two-layer neural network
optimization landscapes. arXiv preprint arXiv:1802.06384, 2018.
Gang Wang, Georgios B Giannakis, and Jie Chen. Learning relu networks on linearly separable data:
Algorithm, optimality, and generalization. arXiv preprint arXiv:1808.04685, 2018.
Xiao-Hu Yu and Guo-An Chen. On the local minima free condition of backpropagation learning.
IEEE Transactions on Neural Networks, 6(5):1300-1303, 1995.
Chulhee Yun, Suvrit Sra, and Ali Jadbabaie. Global optimality conditions for deep neural networks.
arXiv preprint arXiv:1707.02444, 2017.
C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning requires
rethinking generalization. ICLR, 2017.
K. Zhong, Z. Song, P. Jain, P. L Bartlett, and I. S Dhillon. Recovery guarantees for one-hidden-layer
neural networks. ICLR, 2017.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes
over-parameterized deep ReLU networks. arXiv preprint arXiv:1811.08888, 2018.
A Preliminaries
In order to provide rigorous proof and analysis, we re-introduce the considered network model and
relevant notations in a more detailed ways.
Consider a fully connected neural network with H hidden layers. Assume that the h-th hidden layer
contains dh neurons for 1 ≤ h ≤ H, and the input and output layers contain d0 and dH+1 neurons,
respectively. Given an input sample x ∈ Rd0, the output of the i-th neuron in the h-th hidden layer,
denoted by th,i, is given by
Xd0
k=1
w1,i,k xk + b1,i
1 ≤ i ≤ d1
(4a)
(dh-1	∖
wh,i,kth-1,k (x) + bh,i ,	2 ≤ h ≤ H, 1 ≤ i ≤ dh,	(4b)
k=1
where wh,i,k is the weight from the k-th neuron of the (h - 1)-th layer to the i-th neuron of the
h-th layer, bh,i is the bias added to the i-th neuron of the h-th layer, and σh is the neuron activation
function for the h-th hidden layer. Then, the i-th output of the network, denoted by tH+1,i, is given
by
dH
tH+1,i(x) =	wH+1,i,ktH,i (x), 1 ≤ i ≤ dH+1	(5)
k=1
where wH+1,i,k is the weight to the output layer, defined similarly to that of the hidden layers.
13
Under review as a conference paper at ICLR 2020
Consider a training dataset consisting of N samples. Denote the n-th sample by (x(n) , y(n)),
n = 1,…，N, where x(n) ∈ Rd0 and y(n) ∈ RdH+1 are the input and output samples, respectively.
In what follows, we rewrite all the training samples in matrix forms, which allows us to represent
the input-output relation of the neural network in a more compact way. Specifically, let X ,
[x(1),x(2),…，X(N)] ∈ Rd0 ×N and Y，[y(1),y(2),…，y(N)] ∈ RdH+1×N as the input and output
data matrices, respectively. Then, we define Wh ∈ Rdh-1 ×dh as the weight matrix from the (h- 1)-th
layer to the h-th layer, bh ∈ Rdh as the bias vector of the h-th layer, and Th ∈ Rdh×N as the output
matrix of the h-th layer. The entries of each matrix are given by
(Wh)i,k = wh,i,k, (bh)i = bh,i, (Th)i,n = th,i(x(n)).	(6)
Based on the above definition, we can immediately rewrite the output of each layer as
T1 = σ1 [W1 b1] 1X>	,	(7a)
h = 2, 3,…，H	(7b)
TH+1 = WH+1TH.	(7c)
where the activation function σh(∙) is applied entry-wise to the input matrix and outputs a matrix
with the same size. That is, (σ(A))i,j = σ(Ai,j) for any input matrix A.
In the rest of this paper, we simplify the feed-forward operation equation 7 by ignoring all the bias
neurons, yielding
T1 = σ(W1X),	(8)
Ti = σ(WiTi-i),	i = 2, 3, ∙∙∙ ,H,	(9)
TH+1 = WH+1TH.	(10)
We note that this simplification does not affect our analysis, and therefore the main results also hold
for feed-forward deep neural networks with bias. Let W = (Wι, ∙∙∙ , Wh+i) denote all the weights
and define the empirical loss as
E(W) =l(Y,TH+1) =l(Y,WH+1TH)	(11)
where l is the loss function. Then, the training problem of the considered network is to find W to
minimize the empirical loss E(W).
B Proof of Proposition 1
Proof: Denote X = (xi,…，XN)>. Let the neural network be defined by y = v>σ(wx>) where
V = (vι, .…，VN-ι)> and W = (wι, ∙∙∙ , WN-ι)> are weights to the output layer and the hidden
layer, respectively. Let l(y, y) = ∣∣y - yk2, then E(∙) = E(v, W) = PN=、(yi - v>σ(wxi))2. In the
following, we generically choose W and v, and then in turn determine the activation function σ(∙) and
the output data y to obtain the empirical loss E(V, W) with bad strict local minimum.
Denote Zj = (σ(wjxi),…，σ(wjXN))>,j = 1,…，N- 1. Since zι,…，ZN-ι are N — 1 vectors
in RN, they must be in a N - 1-dimensional hyperplane. Assume that this hyperplane, namely H, is
represented as a© +-+ aNeN = 0, where ei,…，eN are the N orthogonal unit vectors. Denote
a = (ai,…，an)>, then we have
N
a>Zj = ɪ2aiσ(wjXi) = 0,∀j = 1, ∙∙∙ ,N — 1.
i=1
Without loss of generosity, assume that
•	σ(∙) is twice-differentiable;
•	WjXi 6= Wj0Xi0 if i 6= i0 or j 6= j0;
14
Under review as a conference paper at ICLR 2020
•	ai = 0 for all i = 1,…，N.
•	zι,…，ZN-ι are linearly independent.
Otherwise we can perform an arbitrarily small perturbation of w to make the assumptions hold.
We now Come to the key step: determine σ(∙) to guarantee that there exists a neighborhood B(w, δ)
such that
N
Xaiσ(wjXi) > 0,∀j = 1,…，N - 1.
i=1
for all w0 ∈ B(w, δ)∖{w}. Since σ(∙) is twice-differentiable, we can properly determine the first-
order and second-order derivative of σ(∙) at all points of Wj Xi for j = 1, ∙∙∙ ,N —1 and i = 1,…，N.
Specifically, let
N
EaiXiσ0(wjXi) = 0, ∀j = 1,…，N — 1,
i=1
ai ∙ σ00(wjXi) > 0, ∀j = 1,…，N — 1, i = 1,…，N.
Then, by Taylor expansion, we have
N
aiσ(wj0 Xi)
i=1
N
X
i=1
aiσ(wjXi) + aiσ0(wjXi) ∙ Xi(Wj — Wj) + aiσ0(wjXi) •
2 Xx
(Wj0 — Wj )2 + o(Wj0 — Wj )2
N	N	1N
aσ aiσ(wjXi) + (Wj — Wj) ∙	aiXiσ0(wjXi) + G(Wj — Wj)2 ∙ aiX2σ00(wjXi) + o(wj — Wj)2
i=1	i=1	i=1
1	2N2	2
=0 + 0 +——(Wj — Wj) ∙ ɪ2X2 ∙ aiσ (WjXi) + o(w[ — Wj) > 0.
2	i=1
(12)
After σ(∙) is determined, choose arbitrary v > 0 and obtain y = v>g(wx). Let y = y — a, then
e(v,w) = ky - yk2 = I∣ak2.
Now consider any (v0, w0) such that w0 ∈ B(w,δ). Denote y0 = (v0)>σ(w0x) and zj =
(σ(wjxι), ∙∙∙ , σ(wjXN))>,j = 1,…，N — 1. If w0 = w, then v0 = V and thus y0 = y due
to linear independency of zι,…，ZN-ι. Since Zj = Zj for all j, y0 = PN-LI vjZj is also in hy-
perplane H. On the other hand, note that a is the normal vector of H, we have a>y = a>y0 = 0.
Therefore, (y — y)>(y — y0) = —a>(y — y0) = 0. Thus
E(v0, W0) = ky - yk2 = ky - yk2 + ky0 — yk2 > E(v, w).
IfW0 6= W, since W0 ∈ B(W, δ), we have a>Zj0 = PiN=1 aiσ(Wj0 Xi) > 0 by 12. Since v > 0, we have
a>y0 = PN-I Vj ∙ a>Zj > 0. Therefore, (y — y)>(y — y0) = —a>(y — y0) = a>y0 — a>y > 0.
Thus
E(v0,w0) = ky - yk2 = k(y — y) + (y - y0 )k2
=ky — yk2 + ky0 — yk2 + (y — y)>(y — y0) > e(v,w).
Therefore we show that E(v0, w0) > E(v, w) for all (v0, w0) ∈ B((v, w), δ)∖{(v, w)}, which means
that (v, w) is a bad strict local min of E(∙).	□
C Relationship B etween Activation Function and Property PT
Given the strong connection between Property PT and the loss surface, a natural but crucial question
is that what activation functions can guarantee Property PT. Intuitively, it seems that Property PT
15
Under review as a conference paper at ICLR 2020
can be easily met as long as the activation function is non-linear. Unfortunately, this is not true. In
Section 5, we mention that it is necessary for the output matrix Z to be full rank so that Property
PT holds in (3). Below we provide a simple rank-1 example of rank-deficient output matrix with
activation function σ(z) = zp.
Example 2	(Rank-deficient output matrix) Consider a 1-hidden-layer neural network with 2 data
samples, 1 input dimension, 2 hidden-layer neurons, and monomial activation σ(z) = zp, p > 0. We
have W ∈ R2×1, X ∈ R1×2, and therefore WX is always of rank 1. Denote WX
a1	a2
ta1 ta2
where t ∈ R. Then, Z = σ(WX)
a1p	ap2
tp a1p tp ap2
is always rank-deficient. This implies that we
cannot find any W such that Z is of full column rank. Property PT does not hold.
In fact, for polynomial activation functions, rank-deficient output is common if the number of samples
is sufficiently greater than the input dimension. The following is an example of rank-deficient output
matrix with general polynomial activation functions.
Example 3	(Rank-deficient output matrix) Consider a 1-hidden-layer fully-connected neural net-
work with N data samples, d0 input dimension, d1 hidden-layer neurons, and a polynomial activation
σι(z) = ao + aιz + …+ a,nZp. Assume N》do, di = N. We have Wi ∈ RN×d0 ,X ∈ Rd0 ×N,
and therefore W1X is at most rank-d0. Denote ◦ as the Hadamard product, then
Ti = σ(W1X) = ao ∙ 1N×N + ai ∙ (WiX) + …+ an ∙ (WiX)◦.・・◦ (WiX).	(13)
By the property of Hadamard product, we have
rank (Ti) ≤ 1 + do + d0 + …dp.	(14)
This implies that Ti is rank-deficient if N do.
D	Proof of Theorem 1: Single-Hidden-Layer Case
In this section, we provide the proof of Theorem 1 for 1-hidden-layer case. Following the ideas in
Section 5, our proof strategy consists of three steps.
Step 1: Prove the result for a specific class of activation functions.
We first introduce a special class of activation functions, specified in the following assumption.
Note that to ease notations, we will omit the subscript of activation function σi when considering a
1-hidden-layer network.
Assumption 3 (Special Activation Functions) The activation function σ is analytic, and its first n
derivatives at 0, i.e., σ(0), σ0(O), .…,σ(n-i) (0), are all non-zero.
Assumption 3 covers many commonly used activation functions such as sigmoid and softplus, but
it does not cover ReLU since it requires smoothness (as mentioned before, ReLU is covered by
using the approximation trick). Activation functions satisfying Assumption 3 have the following nice
property:
Lemma 1 IfAssumption 3 holds, thenfor any N scalars zi,z2,… ,zn ∈ R such that Zn = zno for
all n 6= n0, the following matrix
/ σ(0)	σ(0)	… σ(0)	∖
ziσo(0)	Z2σ0(0)	…	ZN σ0(O)
A =	.	.	.	(15)
..	.
..	.
∖zN-iσ(N T)(0) ZN-iσ(N T)(0)…ZN-iσN-i(0√
is non-singular.
16
Under review as a conference paper at ICLR 2020
Next, we borrow an important result of Mityagin (2015) which states that the zero set of an analytic
function is either the whole domain or zero-measure. The result is formally stated as the following
lemma.
Lemma 2 For any m ∈ Z+, let f : Rm → R be a real analytic function on Rm. If f is not
identically zero, then its zero Set Ω = {z ∈ Rm | f (Z) = 0} has zero measure.
Based on Lemma 2, we have the following result.
Lemma 3 Suppose that σ is an analytic function satisfying Assumption 3. Given a, b ∈ RN, let
Ω = {z ∈ RN | σ(a>z) = σ(b>z)}. If a = b, then Ω is ofmeasure zero.
Based on the above lemmas, we show that the output matrix of the hidden layer has full column rank
for almost all W1, specified by the following proposition.
Proposition 2 (General two-layer Case) Consider a two-layer neural network with output T1 =
σ(W1X), where X ∈ Rd0×N and Wi ∈ Rdι×d0. Let Ωι = {Wι ∈ Rdι×d0 | rank(Tι) <
min{dι, N}}. IfAssumption A1, A2, and 3 hold, then Ωι is ofmeasure zero.
Here we only provide the proof of Proposition 2 for a simple 1-dimensional-input case. We note that
it can be readily extended to the general case, detailed in Appendix L.
Proposition 3 (1-dimensional-input Two-layer Square Case) Consider a two-layer neural net-
work with one input neuron and d1 neurons in the hidden layer. Given an activation function
σ and one-dimensional input data X ∈ RN, let Ω = {wi ∈ Rd | det (σ(wιx>)) = 0}. If di = N
and Assumption A1, A2, and 3 hold, then Ω is ofmeasure zero.
Proof: We prove this result by induction on N. The conclusion is obvious when N = 1.
We consider the case with N > 1. Since f(wi) , det(wix>) is an analytic function with respect to
wi, from Lemma 2 We know that Ω is either RN or a zero-measure set. We now prove that Ω cannot
be RN.
Assume on the contrary that Ω = RN, i.e., f (Wi) = 0, ∀wι ∈ RN. Denote wi,i as the i-th entry of
wi, 1 ≤ i ≤ di. For any k ≥ 0, the k-th order partial derivative of f(wi) with respect to wi,i is
given by
∕∂ ∂ ∂f (w)
Gk (Wi),	= det
(Xk σ(k)(wι,iXi)
σ(wι,2Xi)
xkσ(k)(wι,iX2)…xN σ(k) (wi,iXN )∖
σ(wi,2x2)	…	σ(wι,2XN)
(16)
σ(wi,NXi)
As f(wi) = 0, ∀wi ∈ RN, we have Gk(wi)
σ(wι,N x2)	…	σ(wι,N xN))
0, ∀wi ∈ RN and ∀k ≥ 0.
Denote the n-th row of Gk(Wi) by Un = [σ(wi,nXi),…，σ(wi,nXN)]>,n = 2,…，N. We
show there exist some wi,2,…，wi,N such that u2,…，UN are linearly independent. We denote
U n = [σ(wi,nXi),…，σ(wi,n xN-i)]>,n = 2,…，Ν, and G= [U 2,…，UN ] ∈ R(N-i)×(N-i).
According to the induction hypothesis, the set {(wi,2,…，wi,N) | det(G) = 0} is zero-measure in
RN-i, implying that there exist some wi,2, •…，wi,N such that U2, •…，UN are linearly independent.
This also implies that U2,…,UN are linearly independent.
Now we have found some wi,2,…,wi,N such that U2, ∙∙∙ , UN are linearly independent. Fix
wi,2, ,…，wi,N and let wi,i = 0. Denote the first row of Gk as ak. Since det(Gk) = 0 for any
k ≥ 0, ak must be a linear combination of U2, •…，UN for any k ≥ 0, so all ak's lie in an (Ν - l)-
dimension subspace of RN. However, according to Lemma 1, the N vectors a0,…，aN-i are
linearly independent, which is a contradiction.
Therefore we have proved that Ω cannot be RN, so it must be a zero-measure set.	□
Proposition 3 states that the output matrix of the hidden layer is generically full-column-rank. This
property implies that the loss surface is a weakly global function. We have the following theorem.
17
Under review as a conference paper at ICLR 2020
Theorem 3 Consider a fully connected neural network with 1 hidden layer, activation function σ
and empirical loss function E(W) = l(Y, W2T1). Suppose that Assumption 1 and Assumption 3
hold. Then E(W) is a weakly global function.
Step 2: Show that the activation function in Assumption 2 can approximate any continuous function.
In order to extend Theorem 3 to all continuous activation functions without dealing them directly,
we use a mathematical trick that approximates the continuous activation by a class of analytical
functions.
Lemma 4 For any continuous function f : R → R, there exists a sequence of functions (fk)k∈N, all
satisfying Assumption 3, such that fk converges to f uniformly.
Lemma 4 means that the analytic functions satisfying Assumption 3 constitute a dense set (in the
sense of uniformly convergence) of the space of continuous function. As we will show in the next
step, this property allows us to approximate a neural network with any continuous activation function
by a sequence of neural networks under Assumption 3.
Step 3: Show that the property of weakly global function is preserved under compact convergence.
Having built the relation between the neural network with analytic activation functions and the neural
network with continuous activation function, the last step is to show that the weakly global property
is preserved under this relation. The following result is a modification of a result in Josz et al. (2018).
Proposition 4 Consider a sequence of functions (fk)k∈N and a function f, all from S ⊂ Rm to R.
If,
fk → f compactly	(17)
and if fk are weakly global functions on S, then f is a weakly global function on S.
Proposition 4 is slightly different from its original version in Josz et al. (2018): here we assume that
fk are weakly global functions instead of global functions. Nevertheless, we can still prove that f is
weakly global by using similar techniques as in Josz et al. (2018).
Based on Proposition 4, to prove Theorem 1 it suffices to find a sequence of weakly global functions
that compactly converges to the loss surface. Below we present the formal proof of Theorem 1 for
1-hidden-layer case.
Proof: We denote the considered network by N. From Lemma 4, there exists a sequence of activation
functions (σk)k∈N that uniformly converges to σ. For each k ∈ N, we construct a neural network,
denoted by Nk, by replacing the activation function in N with σk. For allNk, we assume the training
dataset to be identical to that of N . We also denote the output matrix of the hidden layer by T1(k) and
the empirical loss by
Ek(W) = l Y, W2T1(k) .	(18)
From Theorem 3, Ek is a weakly global function with respect to W, ∀k ∈ N. In what follows, we
prove that the sequence of the empirical loss functions (Ek)k∈N compactly converges to E.
Consider an arbitrary compact subset S in the space of W. For any W ∈ S, denote £1k],n(W)=
(TIk))i,n and tι,i,n(W) = (TI)i,n for any k ∈ N, 1 ≤ i ≤ d1, and 1 ≤ n ≤ N. That is, we rewrite
the output of each neuron in the hidden layer as a function of W. Then, we have
C),n(W) = σk (X(W1)i,jXj,n)	(19)
t1,i,n(W)= σ (X(Wι)i,jXj,n) .	(20)
18
Under review as a conference paper at ICLR 2020
Since σk uniformly converges to σ,甘?八 also uniformly converges to t1,i,n on S for all 1 ≤ i ≤ d1,
1	≤ n ≤ N.	,,
Now we consider the empirical loss equation 18. As every component of T1(k) uniformly converges
(k)
to the corresponding component of T1, it can be shown that WH+1T1 also uniformly converges to
WH+1T1 on S. By the continuousness of the loss function l, we have that Ek uniformly converges to
E on S.
Finally, noting that S is an arbitrary compact subset in the space of W, the empirical loss Ek
compactly converges to E on the space of W. Since Ek(W) is a weakly global function for every
k ∈ N, by Proposition 4, E(W) is also a weakly global function. We complete the proof. □
E Proof of Theorem 1: Extension to Deep Networks
In this section, we extend the proof of Theorem 1 from 1-hidden-layer case to deep neural networks.
It turns out that the extension is simpler than expected, as we only need to follow the same idea as
Section D and generalize some of the propositions and lemmas.
In the first place, we show that if the activation function of each hidden layer satisfies Assumption 3,
the output matrix of the last hidden layer, i.e., TH, is of full column rank for almost all W. Formally,
we have the following extended version of Proposition 2.
Proposition 5 Given a fully connected neural network with H hidden layers, activation func-
tion σh for each hidden layer, and empirical loss function E(W) = l(Y, WH+1TH). Let
Ω = {(W1, ∙∙∙ , Wh ) | rank (TH) < min{dH ,N}}. Suppose that Assumption 1 hold and σh
satisfies Assumption 3for all 1 ≤ h ≤ H, then Ω is a zero-measure set.
Proposition 5 immediately gives rise to the following theorem, which generalizes Theorem 3 to deep
neural networks.
Theorem 4 Given a fully connected neural network with H hidden layers., activation function σh for
each hidden layer, and empirical loss function E(W) = l(Y, WH+1TH). Suppose that Assumption 1
hold and σh satisfies Assumption 3for all 1 ≤ h ≤ H, then E(W) is a weakly global function.
To completely prove Theorem 1, what remains is to find a sequence of weakly global functions that
compactly converges to the loss surface. We have the following lemmas.
Lemma 5 Consider two continuous functions f : S → Rn and g : Rn → R, where S ⊂ Rm is a
compact set. Suppose that there exists two sequences of functions (fk)k∈N and (gk)k∈N, such that fk
uniformly converges to f on S, and gk compactly converges to g on Rn. Then, gk ◦ fk converges to
g ◦ f uniformly on S.
Lemma 6 Given a fully connected neural network with H hidden layers, activation function σh for
each hidden layer, and empirical loss function E(W) = l(Y, WH+1TH). Suppose that Assumption 1
holds. Then, there exists a sequence of weakly global functions that compactly converges to E(W).
Combining Proposition 4 and Lemma 6, we complete the proof of Theorem 1.
F	Proof of Theorem 2
Similar to that of Theorem 1, the proof of Theorem 2 consists of three steps.
Step 1: Prove the result for networks with specific activation functions.
From Assumption 2, there exists a wide layer of the network, i.e., the h0-th hidden layer. We show
that if the activation functions satisfy some specific conditions, the resulting empirical loss function
is a weakly global function. We first specify a class of activation functions for the pyramid structure.
19
Under review as a conference paper at ICLR 2020
Assumption 4 (Special Activation for Pyramid Structure) The activation function σh is continuous,
strictly increasing or strictly decreasing, and its range is R, i.e., σ(R) = R.
Clearly, the activation functions satisfying Assumption 4 also satisfies Assumption B2. Further, we
have the following theorem.
Theorem 5 Given a fully connected neural network with H hidden layers., activation function σh for
each hidden layer, and empirical loss function E(W) = l(Y, WH+1TH). Suppose that Assumption 1
and 2 hold, and
1.	For any 1 ≤ h ≤ h0, σh satisfies Assumption 3;
2.	For any h0 < h ≤ H, σh satisfies Assumption 4.
Then, E(W) is a weakly global function.
Theorem 5 identifies a special class of deep over-parameterized networks with pyramid structure,
whose empirical loss function is weakly global.
Step 2: Show that for each hidden layer, the activation function in Theorem 5 can approximate the
activation function in Theorem 2.
We use the same approximation trick as in the proof of Theorem 1. For the activation functions
satisfying Assumption 3, we have Lemma 4. Regarding the activation functions satisfying Assumption
4, we have the following lemma.
Lemma 7 For any continuous and non-increasing (or non-decreasing) function f : R → R, there
exists a sequence of functions (fk)k∈N, all continuous and satisfying Assumption 4, such that fk
converges to f compactly.
Step 3: Under compact convergence, show that the neural network considered in Theorem 2 is weakly
global.
We denote the considered network by N. From Lemma 4, for any 1 ≤ h ≤ h0, there exists a sequence
of activation functions (σh,k)k∈N, each satisfying Assumption 3, that uniformly converges to σh.
From Lemma 7, for any h0 < h ≤ H, there exists a sequence of activation functions (σh,k)k∈N,
each satisfying Assumption 4, that compactly converges to σh . Since uniform convergence implies
compact convergence, for all 1 ≤ h ≤ H, σh,k compactly converges to σh.
In the following, we show that the empirical loss of N can be approximated by a sequence of weakly
global function, which is identical to the analysis in Appendix E
For each k ∈ N, we construct a neural network, denoted by Nk , by replacing the activation function
of the h-th hidden layer with σh,k, for all h = 1,2,…，H. For all Nk, We assume the training
(k)
dataset to be identical to that of N. We also denote the output matrix of the h-th hidden layer by Th
and the empirical loss by
Ek(W) = l Y, WH+1TH(k) .	(21)
From Theorem 3, Ek is a weakly global function with respect to W, ∀k ∈ N.
Consider an arbitrary compact subset S in the space of W. For any W ∈ S, define t(ki n(W)=
(Thk))i,n and th,i,n(W) = (Th)i,n for any k ∈ N, 1 ≤ h ≤ H, 1 ≤ i ≤ d八,and 1 ≤ n ≤ N. That is,
we rewrite the output of each neuron in the hidden layers as a function of W . We prove by induction
that every sequence 砰i n)k∈N converges to th,i,n uniformly on S.
20
Under review as a conference paper at ICLR 2020
For h = 1, we have
CL(W) =σk (X(Wι)i,jXjj	(22)
th,i,n(W) = σ (X(Wl)i,jXj,n^ .	(23)
Since σk uniformly converges to σ, t∖kj n also uniformly converges to iι,j,n on S for all 1 ≤ j ≤ di,
1 ≤ n ≤ N.	, ,
For h > 1, assume that ^h-I %八 uniformly converges to th-ι,i,n on S for all 1 ≤ i ≤ dh-ι,
1 ≤ n ≤ N . For the h-th layer, we have
(dh-1	∖
X (Wh)i,j (Th-Jjn)
(dh-1	∖
X (Wh)i,j%i,j,n(W)1	(24)
~	(ddh-	ʌ
th,i,n(W) = σh I ɪ2 (Wh) i,j (Th-I) j,n )
(ddh-	~	∖
=σh I ∑ (Wh)i,jth-1,j,n(W)1 .	(25)
By the induction hypothesis, it is easy to show that Pd=-I(Wh)i,jth-Ijn(W) uniformly con-
verges to Pd=-I(Wh)i,jth-ι,j,n(W) on S. Note that (σh,k)k∈N converges to σh uniformly, and
uniform convergence implies compact convergence. It directly follows from Lemma 5 that thk)n(W)
converges to th,i,n (W).
Therefore, We conclude that thk) n converges to ^h,in uniformly on S for every 1 ≤ h ≤ H,
1 ≤ i ≤ dh, and 1 ≤ n ≤ N.	,,
Now we consider the empirical loss
Ek(W) = l (Y, WH+1TH(k)	(26)
E(W) = l(Y,WH+1TH) .	(27)
(k)
As every component of TH converges uniformly to the corresponding component of TH on S, it can
(k)
be shown that WH+1TH converges uniformly to WH+1TH on S. By Lemma 5, where we set both
gk and g to the loss function l, we have that Ek uniformly converges to E on S. Noting that S is an
arbitrary compact subset in the space of W, the empirical loss Ek converges to E compactly on the
space of W. Since Ek (W) is a weakly global function for every k ∈ N, by Proposition 4, E(W) is
also a weakly global function. We complete the proof.
G Proof of Theorem 3
Note that the proof of Theorem 3 is essentially the same with that of Theorem 4 if we set H = 1. We
omit the proof of Theorem 3 here and refer the readers to Appendix H.
H	Proof of Theorem 4
The proof consists of three steps. First, we show that for any W, we can perturb it to a point W0
whose corresponding TH is full rank. Second, we prove that starting from the perturbed point W0,
21
Under review as a conference paper at ICLR 2020
there exists a strictly decreasing path to the global infimum. Finally, we combine the two previous
steps to show that the loss function is weakly global.
Proof: We first prove that from any initial weight Wo = (Wo, ∙∙∙ , WH +1), there exists a strictly de-
creasing path towards infW E(W) 4 after an arbitrarily small perturbation. According to Proposition
5, all W’s that entail a non-full-rank TH only constitute a zero-measure set. Therefore, for any initial
weight Wo and an arbitrarily small δ > 0, there exists WP = (Wf, Wp, ∙∙∙ , WH +1) ∈ B(Wo, δ)
such that the corresponding THp is full rank. Since dH ≥ N, we have rank(THp ) = N.
In what follows, we show that starting from WP , there exists a continuous path along which the
empirical loss E(W) strictly decreases to infW E(W). Denote Wi：H = (Wι,…,WH), i.e., the
weights in the first H layers. By the feed-forward operation equation 8, TH is a function of W1:H.
Thus, E(W) can be rewritten as l(Y^ Wh+iT⅛(Wi：H)). Since l(Y,Y) is convex to Y, for any
WH+1, WH0 +1, and λ ∈ [0, 1], we have
E(W) =l (Y, (λWH+i + (1 - λ)WH +1)TH(W1：H))
=l (Y, λWH+iTH(Wi：H) + (1 - λ)WH +iTh(Wi：H))
≤λl (Y, Wh+iTh(Wi：H)) + (1 - λ)l (Y, WH +iTH(Wi：H))	(28)
Thus, with the weights to the first H hidden layers fixed, E(W) is convex with respect to
WH+i . This implies that starting from WP , we can find a strictly decreasing path towards
inf WH+1 l(Y, WH +i TH (WiP：H)) by fixing Wi：H = WiP：H and moving along WH+i. Moreover,
since TH(WPH) ∈ RdH×N is full column rank, for any Y ∈ RdH+1×N, there exists Wh+i such
that Wh+iTh(Wf：H) = Y, yielding
inf 1(Y,Wh+iTh (Wf：H)) = inf l(Y,Y)=inf E(W).	(29)
Wh+1	-	Y	W
Therefore, the constructed path is strictly decreasing towards infW E(W).
Now we prove by contraposition that E(W) is a weakly global function. Assume in contrast that there
exists a bad strict local minimum of E(W) in the sense of sets, denoted by W. Note by Definition 2,
W is a compact set. Let Wδ = {W0 | infW∈W kW0 - Wk2 ≤ δ}, then there exists δ > 0 such that
for all W ∈ W and W0 ∈ Wδ \ W, E(W) < E(W0). Denote ∂Wδ as the boundary of Wδ. Note
that both Wδ and ∂ Wδ are closed, there exists W * ∈ ∂Wδ such that E (W *) = inf W o∈∂Wδ E(W0).
Moreover, E(W*) = SuPW∈w E(W) + ε for some ε > 0.
Consider an arbitrary point Wo ∈ W. Since E(W) is a continuous function, there exists δ >
δo > 0 such that for any W0 ∈ B(Wo,δ0), |E(W0) - E(Wo)∣ < ε∕2. According to the first
conclusion, we can find WP ∈ B(W o, δ0) such that there exists a strictly decreasing path from WP
to infW E(W) . Since W is a bad local minimum, infW∈Wδ E(W) > infW E(W). Therefore, the
above strictly decreasing path starting from WP must pass through the boundary ∂Wδ . However,
E(Wp) < E(Wo) + ε∕2 < SuPW∈w E(W) + ε = E(W*) = infwo∈∂wδ E(W0). This implies
that the considered path can never be strictly decreasing, leading to a contradiction. Therefore, we
conclude that there is no bad strict local minima in the sense of sets, and therefore E(W) is a weakly
global function.	□
I	Proof of Theorem 5
First, we show that for any initial weight Wo, we can perturb it to a point WP , and starting from the
perturbed point WP , there exists a strictly decreasing path towards the global infimum.
Denote Θ as the space of W and let Wi：h = (Wi, W2, ∙∙∙ , Wh) denotes the weights for the first h
hidden layers. We consider the following set:
Ω = {W∣rank(Th0) = N, rank(Wh) = dh,, ∀ho < h ≤ H +1}.	(30)
4To be more specific, if the global infimum is achievable (i.e., global minimum), then the decreasing path ends
exactly at some point with empirical loss infW E(W). Otherwise, the empirical loss along the path converges
to infW E(W)
22
Under review as a conference paper at ICLR 2020
Since σh satisfies Assumption 3 for any 1 ≤ h ≤ h0, according to Lemma 5, all W1:h0’s that entail a
non-full-rank Th0 only constitute a zero-measure set. That is, all W1:h0’s such that Th0 is rank(N)
constitute a dense set. Further, note that full-rank matrices are dense. For all h0 < h ≤ H + 1, since
dh ≤ dh-1 from Assumption B1, {Wh|rank(Wh) = dh} is dense in Rdh×dh-1 . From the above
analysis, We conclude that Ω is dense in Θ. Therefore, for any initial weight Wo and an arbitrarily
small δ > 0, there exists WP = (Wp, W{ ∙∙∙ , WH +1) ∈ B(Wo, δ) such that WP ∈ Ω.
In what follows, we show that if E(W p) > infW(E(W)), there exists a continuous path starting
from WP, and the empirical loss along which strictly decreases to infW E(W).
Case 1:	There exists Y0 ∈ RdH+1×N such that l(Y, Y0) = infY l(Y, Y).
Denote Thp as the output of the h-th hidden layer at weight Wp, and YP = WH+1TH as the network
output at weight Wp. Since the loss function l(Y, Y) is convex with respect to Y, it is also continuous
with respect to Y. Further, we have
ʌ . , ʌ .
l(Y,YP) = E(WP) > infE(W) ≥ infl(Y,Y),	(31)
w	Y
and hence YP is not a global minimum of l(Y, YP). Then, there exists a continuous path Y (λ) :
[0,1] → R such that Y(0) = Yp, l(Y, Y(1)) = infY l(Y, Y), and l(Y, Y(λ)) is strictly decreasing
with respect to λ, i.e.,
l(Y, Y (λ1))	> l(Y,	Y (λ2)),	∀λ1<	λ2,	λ1,λ2	∈	[0, 1].	(32)
By Assumption 4, for each h0 < h ≤ H, σh has a continuous inverse σh-1 : R → R. Also, since
WhP is of full row rank for each ho < h ≤ H, it has a right inverse (Wf)* such that Wp (Wf)* = I.
Further, as Tpo has full column rank, it has a left inverse (Tho) * such that (Tpo) * (Tho) = I.
We construct W(λ) : [0, 1] → Θ as follows
W (λ) = (WP ,W2P, ∙∙∙ ,Who ,Wh0+ι(λ),Who+2, ∙∙∙ ,WH +ι), λ ∈ [0,1]	(33)
where Wh (λ) is defined recursively as follows
Who+1 (λ) = [σ-o1+1 (Tho+ι(λ)) -σ-01+ι (Th°+J] (Th0)* + Wh^	(34a)
Th(λ) = (Wp+ι)* (σ-+ι (Th+1 (λ)) - σ-+ι (ThQ) + Th,
h = ho + 1,ho + 2, ∙∙∙ ,H — 1	(34b)
TH(λ) = (WH+ι)* (Y(λ) - YP) + TH.	(34c)
For the constructed W (λ), we verify the following three facts.
(1)	W(λ) is a continuous path.
In fact, since Y(λ) is continuous, σ-1 is a continuous function for all ho < h ≤ H,
therefore each Th(λ) is continuous with respect λ for all ho < h ≤ H. Thus, W(λ) is also
continuous with respect to λ.
(2)	W(0) = WP, and W(1) is a global minimum of E(W).
TL T	.1 . W / Z->∖	Wτ> 1 -	JCd	1
Note that Y(0) = YP. From equation 34, we have
Th(0)= (WH+ι)* 0 + TH = TH	(35a)
Th(0)= (Wh+ι)* 0 + Th = Tp, h = ho + 1,ho + 2,…，H (35b)
Who+ι(0) = 0 (Th。)* + Wh0+I = Wp.+I.	(35c)
Notice that W(λ) is identical to WP except the weights to the (ho + 1)-th layer. Thus,
W(0) = WP. Now consider the output of each hidden layer at weight W (λ), denoted by
23
Under review as a conference paper at ICLR 2020
Thλ. For W(λ) and Wp, the weights to the first h0 hidden layers are the same, and hence
Thλ = Thp . From equation 34, we have
玲+1= σh0+1 (Wh0+1(λ)"0) = σ%o+ι (Wh0+1(λ)Th0)	(36a)
=σh0 + 1 (σ-+1 (ThO + 1 (X))- σ-+1(Tp0+1) + WPo+ITPo )	(36b)
=σh0+1 (σ-0l+1 (ThO+ι(λ)) - σ-1+ι (σh0+1 (Wh0+1τh0)) + who+ιτho) (36c)
=σh0+1 (σ-1+ι (Tho+ι(λ)))	(36d)
= ThO+1(λ)	(36e)
For h0 + 1 < h ≤ H, if Thλ-1 = Th-1 (λ), we have
Th = σh(WhTh-) = σh (WhTh-I(λ))	(37a)
=σh (Wh) (Wh))t (σ-1 (Th(λ))-σ-1 (Th)) + WpTh-I)	(37b)
= σh(σh1 (Th(λ)) - σ-1 加(Wh)ThL 1)) + WhTh-1)	(37c)
=σh(σ-1 (Th(λ)))	(37d)
= Th (λ).	(37e)
And similarly, for the network output at W(λ), denoted by Yλ We have
Yλ = WH +1TH = WH +1TH (λ)	(38a)
=WH+1 (WH+1)t (Y(λ) - Yp) + WH+1TH	(38b)
=Y(λ)- YP + WH +1TH	(38c)
=Y(λ).	(38d)
Then, the empirical loss
E(W(1)) = l(Y,Y1) = inf l(Y,Y) ≤ inf E(W).	(39)
γ>	W
Therefore we must have E (W (1)) = infW E(W). That is, E(1) is a global minimum of
E(W).
(3)	E(W(λ)) is strictly decreasing with respect to λ.
From (2), we have
E(W(λ)) = l(Y, Yλ) = l(Y, Y(λ)).	(40)
Then for any λ1, λ2 ∈ [0, 1] and λ1 < λ2, we have
. . ʌ , ʌ , ..
E(W(λ1)) = l(Y, Y(λ1)) > l(Y, Y(λ2)) = E(W(λ2)).	(41)
E(W(λ)) is strictly decreasing with respect to λ.
We conclude that W(λ) starts at from Wh and is a strictly decreasing path towards the global
infimum.
Case 2:	There does not exist Y0 ∈ RdH+1×N such that l(Y, Y0) = infY l(Y, Y).
Similar to Case 1, there exists a continuous path Y(λ) : [0, 1) → R such that Y(0) = Yh,
limh→1 l(Y,Y(λ)) = infγ l(Y,Y), and l(Y,Y(λ)) is strictly decreasing with respect to λ. We
can then construct a continuous path W(λ) : [0, 1) → Θ, such that E(W(λ)) is strictly decreasing,
and limh→1 E (W (λ)). Since the construction and analysis are identical to that in Case 1, we omit
the details here.
Then we prove by contraposition that E(W) is a weakly global function. Assume in contrast that there
exists a bad strict local minimum of E(W) in the sense of sets, denoted by W. Note by Definition 2,
W is a compact set. Let Wδ = {W0 | infW∈W kW0 - Wk2 ≤ δ}, then there exists δ > 0 such that
for all W ∈ W and W0 ∈ Wδ \ W, E(W) < E(W0). Denote ∂Wδ as the boundary of Wδ. Note
24
Under review as a conference paper at ICLR 2020
that both Wδ and ∂ Wδ are closed, there exists W * ∈ ∂Wδ such that E(W *) = inf W o∈∂Wδ E(W 0).
Moreover, E(W*) = SuPW∈w E(W) + ε for some ε > 0.
Consider an arbitrary point Wo ∈ W. Since E(W) is a continuous function, there exists δ >
δo > 0 such that for any W0 ∈ B(Wo,δ0), |E(W0) - E(Wo)∣ < ε∕2. According to the first
conclusion, we can find Wp ∈ B(W o, δ0) such that there exists a strictly decreasing path from Wp
to infW E(W) . Since W is a bad local minimum, infW∈Wδ E(W) > infW E(W). Therefore, the
above strictly decreasing path starting from Wp must pass through the boundary ∂Wδ . However,
E(WP) < E(Wo) + ε∕2 < SuPW∈w E(W) + ε = E(W*) = infwo∈∂Wδ E(W0). This implies
that the considered path can never be strictly decreasing, leading to a contradiction. Therefore, we
conclude that there is no bad strict local minima in the sense of sets, and therefore E(W) is a weakly
global function.
J Proof of Lemma 1
Notice that A is a Vandermonde matrix multiplied by σn-1(0) to the n-th row, n = 1, 2, ∙∙∙ ,N.
Since σn-1(0) 6= 0 according to Assumption 3, A is a non-singular matrix.
K Proof of Lemma 3
Assume that Ω is not of zero measure. Since σ(a>z) - σ(b>z) is an analytic function of z, Ω must
be RN according to Lemma 2. That is, σ(a>z) = σ(b>z) for all z ∈ RN. We will show that this
leads to a contradiction.
If a = 0 or b = 0, assume a = 0 without loss of generality. Since b 6= 0, there exists some z0 such
that b> z0 = 1. Therefore, for any λ ∈ R, we have
σ(λ) = σ(b>(λz0)) = σ(a>(λz0)) = σ(0).	(42)
Thus, σ is a constant function and therefore σ0 ≡ 0, a contradiction to Assumption 3.
Ifa, b 6= 0, then the set of all z satisfying a>z = 0 or b>z = 0 is of measure zero. Since a 6= b,
the set of all z satisfying a>z = b>z is also of zero measure. Therefore, there exists some z0 such
that a>zo = 0, b>z = 0, and a>z0 = b>z0. Denote a0 = a>z0 and b0 = b>z0. Since Ω = RN,
we conclude that for any λ 6= 0, σ(λa0) = σ(a>z0) = σ(b>z0) = σ(λb0). Note that a0b0 6= 0,
a0 6= b0, and σ(λa0) = σ(λb0) for all λ 6= 0. Letting λ → 0, we have
0=lim σ(λa0)- σ(λb0)= σ0(0)
λ→0	λa0 - λb0
where the second equality holds since σ is analytic. This also contradicts Assumption 3.
We conclude that for any a = b, Ω cannot be RN, and therefore must be of zero measure.
(43)
L	Proof of Proposition 2
Let w> be the i-th row of Wι, i = 1, 2, ∙∙∙ ,dι. According to Assumption A1, We can assume
without loss of generality that the first row of X has distinct entries, i.e., χ11),χ12) …,x[N) are
distinct from each other.
Notice that T1 ∈ Rd1 ×N. If d1 < N, we select the first d1 columns of T1 and obtain a sub-matrix
TI ∈ Rd1 ×d1. Let Ω1 = {Wι ∈ Rd1 ×d0 | rank(T∖) < dι}. We can show that Ω1 is a zero-measure
set by applying a similar analysis to Ti as in the proof of Proposition 3. The only change to make
is that here we calculate the partial derivatives with respect to wi,i,i. Notice that for any Wi ∈ Ωι,
any di X di sub-matrix of Ti should be singular. Since Ωι is a subset of Ω1, it should also be zero
measure.
If di ≥ N, we select the first N rows of Ti and obtain a sub-matrix Ti ∈ RN×N. Similarly, let
Wi ∈ RN×d0 be the first N rows of Wi. Let Ωi = {Wi ∈ RN×d0 | rank(Ti) < N}. Following a
similar analysis as in the case di < N, Ωi is of measure zero in RN×d0. Note that for any Wi ∈ Ωi,
its submatrix consisting of the first N rows is in ΩJ Thus, Ωi is of measure zero in Rd1 ×d0.
25
Under review as a conference paper at ICLR 2020
M Proof of Lemma 4
The proof of Lemma 4 consists of two parts. In the first part we show that the function class specified
by Assumption 3 is dense in the space of analytic functions. In the second part, following the fact
that the space of analytic functions is a dense set in the space of continuous function, we prove that
the function class specified by Assumption 3 is also dense in the space of continuous functions.
To prove the first part, we consider an arbitrary analytic function g : R → R, and then construct a
sequence of functions (fk)k∈N, all satisfying Assumption 3, such that fk converges to g uniformly.
Let
fk(x) = g(x) +
——-——-(Sin X + cos x).
s(k + 1)
Clearly, fk is analytic for any k ∈ N and s 6= 0. Further, we have
fkn)(O) = g(n)(O) + s⅛)(-1)n.
(44)
(45)
We next show that there exists s 6= O such that all fk’s satisfy Assumption 3. Consider the following
two cases: (1) g(n) (O) = O for all O ≤ n ≤ N; and (2) g(n)(O) 6= O for some O ≤ n ≤ N.
Case 1:	For any s 6= O, since g(n) (O) = O, we have
fkn) (O) = s(k⅛(-1)n = 0	(46)
for all n = 0,1,…,N. Thus, all fk's satisfy Assumption 3.
Case 2:	Since g(n) (0) = 0 for at least one n ∈ {0,1,…,N}, We can define
δmin =minn|g(n)(O)| | O ≤n≤ N, g(n)(O) 6=Oo	(47)
i.e., the minimum non-zero absolute value of g(n)(0), n = 0,1, ∙∙∙ , N. Clearly, δmin > 0. Letting
S = 2∕δmin, we have
fkn)(0)= g(n) (0) + x7kmnπ (-1)n	(48)
2(k + 1)
For g(n)(0) = 0, we have
fkn)(0) = ⅛nn (-1)n = 0.	(49)
2(k + 1)
For g(n)(0) 6= 0, we have
Ifkn)(O)I= g(n)(0) + 2(k+1)(-i)n
…)卜|卧(-1)"
、，	δmin
一δmin - 2(k + 1)
=δmin(2k + 1)
2(k + 1)
>0
(50a)
(50b)
(50c)
(50d)
(50e)
where equation 50c holds by the definition of δmin in equation 47. Therefore, all fk’s satisfy
Assumption 3.
We now prove the uniform convergence of fk for any S 6= 0. Specifically, for any > 0, we have
|fk(x) - g(x)| =
≤
-------Isin x + Cos x|
S(k+ 1)
√
s(k + 1)
(51a)
(51b)
(51c)
<
26
Under review as a conference paper at ICLR 2020
for all k > √2∕(es) - 1 and X ∈ R. Therefore, fk converges uniformly to g.
We conclude that function class specified by Assumption 3 is dense in the space of analytic functions.
Now we come to the second part. By the Carleman Approximation Theorem Kaplan (1955), the
space of analytic functions is dense in the space of continuous functions. That is, for any continuous
function f : R → R, there exists a sequence of analytic functions (gk)k∈N such that gk converges
to f uniformly. Following the idea of Cantor’s diagonal argument, we can construct a sequence of
functions satisfying Assumption 3, which also converges to f.
Note that each gk is an analytic function. By the analysis in the first part, for each k ∈ N, we can
construct a sequence of functions (fj(k))j∈N, all satisfying Assumption 3, such that fj(k) converges to
gk uniformly. Further, we can require that for each k ∈ N,
Ify)(X)- gk(x)∣ ≤ k+1, ∀x ∈ R, j ∈ N.	(52)
In fact, if equation 52 is not satisfied, we can always delete a finite number of functions at the
beginning of the sequence, so as to produce a new sequence that meets equation 52. Now considered
the sequence (fk(k))k∈N. Since gk converges to f uniformly, for any > 0, there exists a K1 ∈ N
such that |gk (X) - f (X)| ≤ /2 for any k ≥ K1 and X ∈ R. Then, for any k > max{K1,2/ - 1},
we have
Ifkk)(X)- f (X)I ≤ Ifkk)(X)- gk(X)∣+ |gk(X) - f (X)I
≤ ɪ+ e/2	(53a)
k + 1
≤.	(53b)
(k)
Therefore, fk converges to f uniformly. Noting that f is an arbitrary continuous function from R
to R, we complete the proof.
N Proof of Proposition 4
Consider a sequence of weakly global functions fk that converges compactly towards f . Since
S ⊂ Rm and Rm is a compactly generated space, it follows that f is continuous. We proceed
to prove that f is a weakly global function by contradiction. Suppose SM ⊂ S is a strict local
minimum that is not global minimum. There exists > 0 such that the uniform neighborhood
V := {y ∈ S | ∃ X ∈ SM : kX - yk2 ≤ } satisfies f(X) < f(y) for all X ∈ SM and for all
y ∈ V \ SM. Since f is continuous on the compact set SM, it attains a minimal value on it, say
inf SM f := α + infS f where α > 0 since SM is not a global minimum. Consider a compact set
V ⊂ K ⊂ S such that infK f ≤ α∕2 + infS f. Since f is continuous on the compact set ∂V,
it attains a minimal value on it, say inf∂V f := β + infSM f where β > 0 by strict optimality.
Let Y := min{α∕2,β}. For a sufficiently large value of k, compact convergence implies that
Ifk(y) - f (y)| ≤ γ∕3 for all y ∈ K. Since the function fk is compact on V, it attains a minimum,
say z0 ∈ V. Consider the compact set defined by Z := {z ∈ V | f(z) = f (z0)}. Therefore, for any
z ∈ Z,
fk(z) ≤ γ∕3+inff	(54a)
≤ β∕3 + inff	(54b)
< 2β∕3 + inff	(54c)
≤ -γ∕3+β+inff	(54d)
≤ -γ∕3+inff ≤ inffk.	(54e)
∂V	∂V
Thus, z ∈ int(V). So Z ⊆ int(V). Since both Z and ∂V are compact, we have d(∂V, Z) > 0. We
now proceed to show by contradiction that Z is a strict local minimum of fk. Assume that for all
0 > 0, there exists y0 ∈ S \ Z satisfying d(y0, Z) ≤ 0 such that fk(z) ≥ fk(y0) for some z ∈ Z.
We can choose 0 < d(∂V, Z) to guarantee that y0 belongs to V since Z ⊆ int(V). The point y0 then
27
Under review as a conference paper at ICLR 2020
contradicts the strict minimality of Z on V . This means that Z ∈ V is a strict local minimum of fk.
Now, observe that for any z ∈ Z,
inf fk ≤ Y/3 + inf f	(55a)
≤ γ∕3 + α∕2 + inf f	(55b)
≤ 2α∕3 + inf f	(55c)
< 5α∕6 + inf f	(55d)
≤ α - γ∕3 + inf f	(55e)
= -γ∕3+inff X	(55f)
= -γ∕3 + iVnf f	(55g)
≤ inf fk ≤ fk(z).	(55h)
Thus, Z is not a global minimum of fk . This contradicts the fact that fk is a weakly global function.
O	Proof of Proposition 5
The proof for Proposition 5 is a natural extension of Proposition 2. Specifically, we show that
Assumption A1 can be preserved for the input of every hidden layer, which allows us to prove
Proposition 5 by induction.
Proof:(Proof of Proposition 5) Denote Wi：h = (Wι, W2, ∙∙∙ , Wh), i.e., the weights of the first h
hidden layers. Define
Ωh = {Wi：h	|	rank(Th) < min{dh, N}}	(56)
Ωh = {Wi：h	|	∀i = 1,…，dh, ∃ni = n0i,	s.t.	(Th)i,n	= (Th)i,n[}.	(57)
Ωh is the set of Wi:h SUch that the output matrix of the h-th hidden layer is not full rank, which
generalizes Ωι defined in Proposition 2. Ωh is the set of Wi：h such that there exist identical entries
In every row of Th That is, for any Wι-h ∈ Ωh the resulting Th, If regarded as an input data matrix,
violates Assumption A1.
In the following, we prove by induction that Φh，Ωh ∪ Ωh is of measure zero for all 1 ≤ h ≤ H.
We first consider the case with h = 1. By Proposition 2, Ωι is of measure zero . Further, note
that (T1)i,n = σ((w1)i>x(n) ), where (w1)i> and x(n) are the i-th row of W1 and the n-th col-
umn of X (i.e., the n-th training data), respectively. Noting that Assumption A1 guarantees that
χ(D,χ(2),…,X(N) are distinct from each other, from Lemma 3, Ω1 is of measure zero. As a result,
Φι = Ωι ∪ Ωι is also of measure zero.
Now assume that Φh-1 is of measure zero. Then Φh can be decomposed into
Φh = n Wi：h | Wi：(h-i)∈ Φh-ι, Wi：h ∈ Ωh ∪ Ωh}
∪ {Wi：h | Wi：(h-i)∈ Φh-1, Wi：h ∈ Ωh ∪ Ωh}	(58)
By the induction hypothesis, the first component of the set union in equation 58 has zero measure
in the space of Wi：h. Moreover, for Wi：(h—i)∈ Ωh-ι, the resulting Th-ι, if regarded as an input
data matrix, satisfies Assumption A1. Noting that σh satisfies Assumption 3, following a similar
procedure as in the case of h = 1, we obtain that the set of Wh satisfying (W±(h-i), Wh) ∈ Ωhr has
zero measure in Rdh×dh-1 . This implies that the second component of the set union in equation 58
also has zero measure in the space of W1:h. Therefore, Φh is of measure zero for all 1 ≤ h ≤ H.
Noting that Ω = Ωh, we complete the proof of Proposition 5	□
28
Under review as a conference paper at ICLR 2020
P Proof of Lemma 5
Let D ⊂ Rn be the range of f on S . Since S is compact and f is continuous, D is also compact.
Define
D0 = {z ∈ Rn | ∃z0 ∈ D, ||z - z0 ||2 ≤ 1} .	(59)
Then, D0 is also compact.
Since g is continuous, its restriction on D0 is uniformly continuous. That is, for any > 0, there exits
δ > 0 such that
∣g(zι) - g(z2)l≤ 2, ∀z1,z2 ∈ D0, ||zi- Z2∣∣2 ≤ δ.	(60)
Further, since fk converges to f uniformly on S, there exists K1 ∈ N such that
||fk(x) - f (x)|| ≤ min {1, δ} , ∀k ≥ K1, x ∈ S.	(61)
Note that by the definition of D0, equation 61 also implies fk(x) ∈ D0 for all k ≥ K1 and x ∈ S.
Also, as gk compactly converges to g, gk uniformly converges to g on D0. Then, there exists K2 ∈ N
such that |gk(z) - g(z)| ≤ /2 for all k ≥ K2 and z ∈ D0.
For any k ≥ max{K1, K2} and x ∈ S, we have
|gk (fk(x)) -g(f(x))| ≤ |gk (fk(x)) - g (fk(x))| + |g(fk(x)) -g(f(x))|	(62a)
≤ 2 + |g (fk(X))- g(f (X))I	(62b)
≤ ≤2 + 2	(62c)
=	(62d)
where equation 62b follows from the fact that fk(X), f(X) ∈ D0, and equation 62b is due to
equation 60 and equation 61.
Therefore, we conclude that gk ◦ fk converges to g ◦ f uniformly on S . We complete the proof.
Q	Proof of Lemma 6
We denote the considered network by N. From Lemma 4, for any 1 ≤ h ≤ H, there exists a sequence
of activation functions (σh,k)k∈N, each satisfying Assumption 3, that uniformly converges to σh. For
each k ∈ N, we construct a neural network, denoted by Nk , by replacing the activation function of
the h-th hidden layer with σh,k, for all h 1,2, ∙ ∙ ∙ , ^H. For all ^Nk, ^we assume the training dataset
(k)
to be identical to that of N. We also denote the output matrix of the h-th hidden layer by Th and
the empirical loss by
Ek(W) = l Y, WH+1TH(k) .	(63)
From Theorem 3, Ek is a weakly global function with respect to W, ∀k ∈ N.
Consider the sequence of the empirical loss functions (Ek)k∈N. In what follows, we prove that Ek
compactly converges to E .
Consider an arbitrary compact subset S in the space of W. For any W ∈ S, define t(ki n(W)=
(Thk))i,n and th,i,n(W) = (Th)i,n for any k ∈ N, 1 ≤ h ≤ H, 1 ≤ i ≤ d八, and 1 ≤ n ≤ N. That is,
we rewrite the output of each neuron in the hidden layers as a function of W . We prove by induction
that every sequence (thk) n)k∈N converges to th,i,n uniformly on S.
For h = 1, we have
墙,n(W) =σk (X(Wι)i,jXjj	(64)
th,i,n(W)= σ (X(Wι)i,jXj,n] .	(65)
29
Under review as a conference paper at ICLR 2020
Since σk uniformly converges to σ, t(k) 冗 also uniformly converges to t1 j n on S for all 1 ≤ j ≤ d1,
1 ≤ n ≤ N.	, ,
For h > 1, assume that IFhk)I in UniformIy converges to th,-ι,in on S for all 1 ≤ i ≤ dh-ι,
1 ≤ n ≤ N . For the h-th layer, we have
(dh-1	∖
X (Wh)i,j (Th-J jJ
(dh-1	∖
X (Wh)i,j th-i,j,n(W )i	(66)
(dh-1	∖
X (Wh)i,j (Th-I) j,n J
/占~i)
=σh I ∑ (Wh)i,jth-i,j,n(W)1 .	(67)
By the induction hypothesis, it is easy to show that Pjd=h-11 (Wh)i,jtF(hk-)1,j,n(W) uniformly con-
verges to Pjd=h-11 (Wh)i,j tFh-1,j,n (W) on S. Note that (σh,k)k∈N converges to σh uniformly, and
uniform convergence implies compact convergence. It directly follows from Lemma 5 that tF(hk,i),n(W)
converges to tFh,i,n(W).
Therefore, we conclude that tF(hk,i),n converges to tFh,i,n uniformly on S for every 1 ≤ h ≤ H,
1 ≤ i ≤ dh, and 1 ≤ n ≤ N.
Now we consider the empirical loss
Ek(W) = l Y, WH+1TH(k)	(68)
E(W) = l(Y,WH+1TH) .	(69)
(k)
As every component of TH converges uniformly to the corresponding component of TH, it can be
shown that WH+1TH(k) converges uniformly to WH+1TH on S. By Lemma 5, where we set both gk
and g to the loss function l, we have that Ek uniformly converges to E on S. Noting that S is an
arbitrary compact subset in the space of W, the empirical loss Ek converges to E compactly on the
space of W. Since Ek (W) is a weakly global function for every k ∈ N, by Proposition 4, E(W) is
also a weakly global function. We complete the proof.
R Proof of Lemma 7
Consider an arbitrary continuous and non-increasing (or non-decreasing) function f : R → R. In
this proof we assume that f is non-increasing. Note that the non-decreasing case can be proved by
following the same idea, and we omit the details therein.
We construct a sequence of function (fk)k∈N as
{—x — k — 1 + f (—k — 1) x < —k — 1
f(x)- χ+++1	-k — 1 ≤ X ≤ k +1
—x + k + 1 + f (k + 1) — k+ι x > k + 1
(70)
First, we show that fk is continuous. To this end, we only need to verify that fk is left-continuous at
x = —k — 1 and right continuous at x = k + 1. From equation 70, we have
lim = f (—k — 1) = fk (—k — 1)	(71a)
x→(-k-1)-
2
lim = f (k + 1) — j— = fk (k + 1)	(71b)
x→(k+1)+	k + 1
30
Under review as a conference paper at ICLR 2020
Thus, fk(x) is continuous for all k ∈ N.
Second, we show that fk is strictly decreasing and f(R) = R. On both (-∞, -k - 1) and
(k + 1, +∞), fk is linear with negative slope, and hence strictly decreasing. Noting that fk is
non-increasing, for any x1, x2 ∈ [-k - 1, k + 1] and x1 < x2, we have
fk(xι) = f(xι) - x1k+1+1 ≥ f(x2) - x1k+1+1 > f(x2) - Xjk+：； = fk(X2). (72)
Therefore fk is strictly decreasing on [-k - 1, k +1]. As fk is continuous, we conclude that
fk is strictly decreasing on R. Further, from equation 70 we have limx→-∞ fk (x) = +∞ and
limx→+∞ fk(x) = -∞. Again, from the fact that fk is continuous, we have fk(R) = R.
Finally, we show that fk converges to f compactly. Consider an arbitrary compact set S ⊂ R
and > 0. Since S is bounded, there exists K ∈ N such that S ⊂ [-K - 1, K +1]. For any
k > max{K, /2 - 1} and x ∈ S, we have x ∈ [-k - 1, k +1], and
Ifk(x) - f(x)l = f (x) - X+k+1 - f (x) = 1X +k +21l ≤ ɪ < e. (73)
(k +1)2	(k +1)2 k +1
Thus, fk converges to f uniformly on S . As S is an arbitrary compact set on R, fk converges to f
compactly on R.
We complete the proof.
31