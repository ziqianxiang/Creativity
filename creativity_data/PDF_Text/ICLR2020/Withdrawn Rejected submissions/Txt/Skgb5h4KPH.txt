Under review as a conference paper at ICLR 2020
Frequency Principle: Fourier Analysis Sheds
Light on Deep Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
We study the training process of Deep Neural Networks (DNNs) from the Fourier
analysis perspective. We demonstrate a very universal Frequency Principle (F-
Principle) — DNNs often fit target functions from low to high frequencies — on
high-dimensional benchmark datasets such as MNIST/CIFAR10 and deep neural
networks such as VGG16. This F-Principle of DNNs is opposite to the behavior
of most conventional iterative numerical schemes (e.g., Jacobi method), which
exhibit faster convergence for higher frequencies for various scientific computing
problems. With theories under an idealized setting, we illustrate that this F-Principle
results from the smoothness/regularity of the commonly used activation functions.
The F-Principle implies an implicit bias that DNNs tend to fit training data by
a low-frequency function. This understanding provides an explanation of good
generalization of DNNs on most real datasets and bad generalization of DNNs on
parity function or a randomized dataset.
1 Introduction
Understanding the training process of Deep Neural Networks (DNNs) is a fundamental problem
in the area of deep learning. We find a common behavior of the gradient-based training process of
DNNs, that is, a Frequency Principle (F-Principle):
DNNs often fit target functions from low to high frequencies during the training process.
In another word, at the early stage of training, the low-frequencies are fitted and as iteration steps
of training increase, the high-frequencies are fitted. For example, when a DNN is trained to fit
y = sin(x) + sin(2x), its output would be close to sin(x) at early stage and as training goes on,
its output would be close to sin(x) + sin(2x). F-Principle was observed empirically in synthetic
low-dimensional data with MSE loss during DNN training (Xu et al., 2018; Rahaman et al., 2018).
However, in deep learning, empirical phenomena could vary from one network structure to another,
from one dataset to another and could exhibit significant difference between synthetic data and high-
dimensional real data. Therefore, the universality of the F-Principle remains an important problem
for further study. Especially for high-dimensional real problems, because the computational cost of
high-dimensional Fourier transform is prohibitive in practice, it is of great challenge to demonstrate
the F-Principle. On the other hand, the mechanism underlying the F-Principle and its implication
to the application of DNNs, e.g., design of DNN-based PDE solver, as well as their generalization
ability are also important open problems to be addressed.
In this work, we design two methods, i.e., projection and filtering methods, to show that the F-
Principle exists in the training process of DNNs for high-dimensional benchmarks, i.e., MNIST
(LeCun, 1998), CIFAR10 (Krizhevsky et al., 2010). The settings we have considered are i) different
DNN architectures, e.g., fully-connected network, convolutional neural network (CNN), and VGG16
(Simonyan & Zisserman, 2014); ii) different activation functions, e.g., tanh and rectified linear unit
(ReLU); iii) different loss functions, e.g., cross entropy, mean squared error (MSE), and loss energy
functional in variational problems. These results demonstrate the universality of the F-Principle.
To facilitate the designs and applications of DNN-based schemes, we characterize a stark difference
between DNNs and conventional numerical schemes on various scientific computing problems, where
most of the conventional methods (e.g., Jacobi method) exhibit the opposite convergence behavior
1
Under review as a conference paper at ICLR 2020
— faster convergence for higher frequencies. This difference implies that DNN can be adopted to
accelerate the convergence of low frequencies for computational problems.
We also intuitively explain with theories under an idealized setting how the smoothness/regularity
of commonly used activation functions contributes to the F-Principle. Note that this mechanism
is rigorously demonstrated for DNNs of general settings in a subsequent work (Luo et al., 2019).
Finally, we discuss that the F-Principle provides an understanding of good generalization of DNNs
in many real datasets (Zhang et al., 2016) and poor generalization in learning the parity function
(Shalev-Shwartz et al., 2017; Nye & Saxe, 2018), that is, the F-Principle which implies that DNNs
prefer low frequencies, is consistent with the property of low frequencies dominance in many real
datasets, e.g., MNIST/CIFAR10, but is different from the parity function whose spectrum concentrates
on high frequencies. Compared with previous studies, our main contributions are as follows:
1.	By designing both the projection and filtering methods, we consistently demonstrate the F-Principle
for MNIST/CIFAR10 over various architectures such as VGG16 and various loss functions.
2.	For the application of solving differential equations, we show that (i) conventional numerical
schemes learn higher frequencies faster whereas DNNs learn lower frequencies faster by the F-
Principle, (ii) convergence of low frequencies can be greatly accelerated with DNN-based schemes.
3.	We present theories under an idealized setting to illustrate how smoothness/regularity of activation
function contributes to the F-Principle.
4.	We discuss in detail the implication of the F-Principle to the generalization of DNNs that DNNs
are implicitly biased towards a low frequency function and provide an explanation of good and poor
generalization of DNNs for low and high frequency dominant target functions, respectively.
2	Frequency Principle
The concept of “frequency” is central to the understanding of F-Principle. In this paper, the “frequency”
means response frequency NOT image (or input) frequency as explained in the following.
Image (or input) frequency (NOT used in the paper): Frequency of 2-d function I : R2 → R
representing the intensity of an image over pixels at different locations. This frequency corresponds
to the rate of change of intensity across neighbouring pixels. For example, an image of constant
intensity possesses only the zero frequency, i.e., the lowest frequency, while a sharp edge contributes
to high frequencies of the image.
Response frequency (used in the paper): Frequency of a general Input-Output mapping f. For exam-
ple, consider a simplified classification problem of partial MNIST data using only the data with label 0
and 1, f (χ1,χ2,…，X784): R784 → {0,1} mapping 784-d space of pixel values to 1-d space, where
Xj is the intensity of the j-th pixel. Denote the mapping,s Fourier transform as f(k1,k2,…，k784).
The frequency in the coordinate kj measures the rate of change of f (χ1,χ2, •…，X784) with respect
to xj, i.e., the intensity of the j-th pixel. If f possesses significant high frequencies for large kj, then a
small change of xj in the image might induce a large change of the output (e.g., adversarial example).
For a dataset with multiple classes, we can similarly define frequency for each output dimension. For
real data, the response frequency is rigorously defined via the standard nonuniform discrete Fourier
transform (NUDFT), see Appendix A.
Frequency Principle: DNNs often fit target functions from low to high (response) frequencies during
the training process. An illustration of F-Principle using a function of 1-d input is in Appendix B. The
F-Principle is rigorously defined through the frequency defined by the Fourier transform (Appendix
A, Bracewell & Bracewell (1986)) and the converging speed defined by the relative error. By using
high-dimensional real datasets, we then experimentally demonstrate F-Principle at the levels of both
individual frequencies (projection method) and coarse-grained frequencies (filtering method).
3	F-Principle in MNIST/CIFAR 1 0 through projection method
Real datasets are very different from synthetic data used in previous studies. In order to utilize the
F-Principle to understand and better use DNNs in real datasets, it is important to verify whether the
F-Principle also holds in high-dimensional real datasets.
2
Under review as a conference paper at ICLR 2020
In the following experiments, we examine the F-Principle in a training dataset of {(xi , yi)}in=-01
where n is the size of dataset. xi ∈ Rd is a vector representing the image and yi ∈ {0, 1}10 is the
output (a one-hot vector indicating the label for the dataset of image classification). d is the dimension
of the input (d = 784 for MNIST and d = 32 × 32 × 3 for CIFAR10). Since the high dimensional
discrete Fourier transform (DFT) requires prohibitively high computational cost, in this section, we
only consider one direction in the Fourier space through a projection method for each examination.
3.1	Examination method: Projection
For a dataset {(xi, yi)}in=-01 we consider one entry of 10-d output, denoted by yi ∈ R. The high dimen-
Sional discrete non-uniform Fourier transform of {(xi, y,)}n-L is ^k = 1 Pn-L Vi exp (-i2πk ∙ Xi).
The number of all possible k grows exponentially on dimension d. For illustration, in each examina-
tion, we consider a direction of k in the Fourier space, i.e., k = kpL , pL is a chosen and fixed unit
vector, hence |k| = k, Then We have Vk = ɪ Pn-L Vi exp (-i2π(pι ∙ Xj)k), which is essentially
the 1-d Fourier transform of {(xpι,i, Vi)}n-, where Xpι,i = pɪ ∙ Xi is the projection of Xi on
the direction pL (Bracewell & Bracewell, 1986). For each training dataset, pL is chosen as the
first principle component of the input space. To examine the convergence behavior of different
frequency components during the training, we compute the relative difference between the DNN
output and the target function for selected important frequencies k’s at each recording step, that is,
nL
△f(k) = |hk — Vk |/|Vk |, where Vk and hk are 1-d Fourier transforms of {yi}n==OL and the correspond-
ing DNN output{ hi }n-1, respectively, along pɪ. Note that each response frequency component, hk,
of DNN output evolves as the training goes.
3.2	MNIST/CIFAR 1 0
In the following, we show empirically that the F-Principle is exhibited in the selected direction
during the training process of DNNs when applied to MNIST/CIFAR10 with cross-entropy loss. The
network for MNIST is a fully-connected tanh DNN (784-400-200-10) and for CIFAR10 is two ReLU
convolutional layers followed by a fully-connected DNN (800-400-400-400-10). All experimental
details of this paper can be found in Appendix C. We consider one of the 10-d outputs in each case
using non-uniform Fourier transform. As shown in Fig. 1(a) and 1(c), low frequencies dominate in
both real datasets. During the training, the evolution of relative errors of certain selected frequencies
(marked by black squares in Fig. 1(a) and 1(c)) is shown in Fig. 1(b) and 1(d). One can easily
observe that DNNs capture low frequencies first and gradually capture higher frequencies. Clearly,
this behavior is consistent with the F-Principle. For other components of the output vector and other
directions of p, similar phenomena are also observed.
Figure 1: Projection method. (a, b) are for MNIST, (c, d) for CIFAR10. (a, c) Amplitude |Vk | vs.
frequency. Selected frequencies are marked by black squares. (b, d) △F (k) vs. training epochs for
the selected frequencies.
4	F-Principle in MNIST/CIFAR 1 0 through filtering method
The projection method in the previous section enables us to visualize the F-Principle in one direction
for each examination at the level of individual frequency components. However, demonstration by
this method alone is insufficient because it is impossible to verify the F-Principle at all potentially
informative directions for high-dimensional data. To compensate the projection method, in this
3
Under review as a conference paper at ICLR 2020
section, we consider a coarse-grained filtering method which is able to unravel whether, in the radially
averaged sense, low frequencies converge faster than high frequencies.
4.1	Examination method: filtering
The idea of the filtering method is as follows. We split the frequency domain into two parts, i.e., a
low-frequency part with |k| ≤ ko and a high-frequency part with |k| > ko, where | ∙ | is the length
of a vector. The DNN is trained as usual by the original dataset {(xi, yi)}in=-01, such as MNIST or
CIFAR10. The DNN output is denoted as h. During the training, we can examine the convergence of
relative errors of low- and high- frequency part, using the two measures below
一(Pkl∣k∣≤ko ∣y(k)- h (k)∣2 !2	.	— (Pk(I-1∣k∣≤ko)∣y(k) - h (k)l2!1
elow =〈 Pki∣k∣≤ko∣y(k)l2	) ,	ehigh=< Pk(1-I∣k∣≤ko)∣y(k)∣2	),
respectively, where ∙ indicates Fourier transform, lk≤k0 is an indicator function, i.e.,
1,	|k| ≤ k0,
lkl≤k0	(0,	|k| > ko.
If we consistently observe elow < ehigh for different k0 ’s during the training, then in a mean sense,
lower frequencies are first captured by the DNN, i.e., F-Principle.
However, because it is almost impossible to compute above quantities numerically due to high
computational cost of high-dimensional Fourier transform, we alternatively use the Fourier transform
of a Gaussian function Gδ (k), where δ is the variance of the Gaussian function G, to approximate
l∣k∣>ko. This is reasonable due to the following two reasons. First, the Fourier transform of a
Gaussian is still a Gaussian, i.e., Gδ(k) decays exponentially as |k| increases, therefore, it can
approximate l∣k∣≤k0 by Gδ(k) with a proper δ(ko) (referred to as δ for simplicity). Second, the
computation of elow and ehigh contains the multiplication of Fourier transforms in the frequency
domain, which is equivalent to the Fourier transform of a convolution in the spatial domain. We
can equivalently perform the examination in the spatial domain so as to avoid the almost impossible
high-dimensional Fourier transform. The low frequency part can be derived by
yiow,δ，(y * Gδ)i,	(1)
where * indicates convolution operator, and the high frequency part can be derived by
yihigh,δ ,yi -yilow,δ.	(2)
Then, we can examine
1	1
P Pi∣ylow,δ-hiow,δ I2 Y	P Pi∣y*h,δ-hhigh,δ I2 Y	(3)
elow	I PiIyiow,δI2	) ,	ehigh	I	PiIyhigh,δI2	广
where hlow,δ and hhigh,δ are obtained from the DNN output h, which evolves as a function of
training epoch, through the same decomposition. If eιow < ehigh for different δ's during the training,
F-Principle holds; otherwise, it is falsified. Next, we introduce the experimental procedure.
Step One: Training. Train the DNN by the original dataset {(xi, yi)}in=-01, such as MNIST or
CIFAR10. xi is an image vector, yi is a one-hot vector.
Step Two: Filtering. The low frequency part can be derived by
n-1
yiow,δ = c X j (Xi- Xj),
Ci j=0
(4)
where Ci = Pjn=-01 Gδ(Xi - Xj) is a normalization factor and
Gδ(Xi — Xj) = exp JXi - Xj I2∕(2δ)) .	(5)
The high frequency part can be derived by yihigh,δ , yi - yilow,δ . We also compute hliow,δ and
hihigh,δ for each DNN output hi.
Step Three: Examination. To quantify the convergence of hlow,δ and hhigh,δ, we compute the
relative error elow and ehigh at each training epoch through Eq. (3).
4
Under review as a conference paper at ICLR 2020
4.2 DNNs with various settings
With the filtering method, we show the F-Principle in the DNN training process of real datasets for
commonly used large networks. For MNIST, we use a fully-connected tanh-DNN (no softmax) with
MSE loss; for CIFAR10, we use cross-entropy loss and two structures, one is small ReLU-CNN
network, i.e., two convolutional layers, followed by a fully-connected multi-layer neural network with
a softmax; the other is VGG16 (Simonyan & Zisserman, 2014) equipped with a 1024 fully-connected
layer. These three structures are denoted as “DNN”, “CNN” and “VGG” in Fig. 2, respectively. All
are trained by SGD from scratch. More details are in Appendix C.
We scan a large range of δ for both datasets. As an example, results of each dataset for several δ's are
shown in Fig. 2, respectively. Red color indicates small relative error. In all cases, the relative error
of the low-frequency part, i.e., elow, decreases (turns red) much faster than that of the high-frequency
part, i.e., ehigh. Therefore, as analyzed above, the low-frequency part converges faster than the
high-frequency part. We also remark that, based on the above results on cross-entropy loss, the
F-Principle is not limited to MSE loss, which possesses a natural Fourier domain interpretation by
the Parseval’s theorem. Note that the above results holds for both SGD and GD.
epoch	epoch	epoch
(a) δ = 3, DNN (b) δ = 3, CNN (c) δ = 7, VGG
epoch	epoch	epoch
(d) δ = 7, DNN (e) δ = 7, CNN ⑴ δ = 10, VGG
Figure 2: F-Principle in real datasets. elow and ehigh indicated by color against training epoch.
5 F-Principle in s olving differential equation
Recently, DNN-based approaches have been actively explored for a variety of scientific computing
problems, e.g., solving high-dimensional partial differential equations (E et al., 2017; Khoo et al.,
2017; He et al., 2018; Fan et al., 2018) and molecular dynamics (MD) simulations (Han et al., 2017).
However, the behaviors of DNNs applied to these problems are not well-understood. To facilitate the
designs and applications of DNN-based schemes, it is important to characterize the difference between
DNNs and conventional numerical schemes on various scientific computing problems. In this section,
focusing on solving Poisson’s equation, which has broad applications in mechanical engineering and
theoretical physics (Evans, 2010), we highlight a stark difference between a DNN-based solver and
the Jacobi method during the training/iteration, which can be explained by the F-Principle.
Consider a 1-d Poisson’s equation:
—∆u(x) = g(x), X ∈ Ω，( — 1,1),	(6)
u(-1) = u(1) = 0.	(7)
We consider the example with g(x) = sin(x)+4 sin(4x)—8 sin(8x)+ 16 sin(24x) which has analytic
solution uref (x) = g0(x) + c1x + c0, where g0 = sin(x) + sin(4x)/4 — sin(8x)/8 + sin(24x)/36,
c1 = (g0(—1)—g0(1))/2andc0 = —(g0(—1)+g0(1))/2. 1001 training samples {xi}in=0 are evenly
spaced with grid size δx in [0, 1]. Here, we use the DNN output, h(x; θ), to fit uref (x) (Fig. 3(a)). A
5
Under review as a conference paper at ICLR 2020
(a)	(b)	(c)	(d)
Figure 3: Poisson's equation. (a) Uref (x). Inset: ∣Uref (k)| as a function of frequency. Frequencies
peaks are marked with black dots. (b,c) ∆F (k) computed on the inputs of training data at different
epochs for the selected frequencies for DNN (b) and Jacobi (c). (d) kh - uref k∞ at different running
time. Green stars indicate kh - uref k∞ using DNN alone. The dashed lines indicate kh - uref k∞ for
the Jacobi method with different colors indicating initialization by different timing of DNN training.
DNN-based scheme is proposed by considering the following empirical loss function (E & Yu, 2018),
Iemp = X (2 |Vx h(xi )|2 - g (Xi )h(xi)) δx + β (h(x0 )2 + h(xn )2) .	(8)
The second term in Iemp (h) is a penalty, with constant β, arising from the Dirichlet boundary
condition (7). After training, the DNN output well matches the analytical solution uref . Focusing
on the convergence of three peaks (inset of Fig. 3(a)) in the Fourier transform ofuref, as shown in
Fig. 3(b), low frequencies converge faster than high frequencies as predicted by the F-Principle. For
comparison, we also use the Jacobi method to solve problem (6). High frequencies converge faster in
the Jacobi method (Details can be found in Appendix D), as shown in Fig. 3(c).
As a demonstration, we further propose that DNN can be combined with conventional numerical
schemes to accelerate the convergence of low frequencies for computational problems. First, we solve
the Poisson’s equation in Eq. (6) by DNN with M optimization steps (or epochs), which needs to be
chosen carefully, to get a good initial guess in the sense that this solution has already learned the low
frequencies (large eigenvalues) part. Then, we use the Jacobi method with the new initial data for the
further iterations. We use ∣∣h - uref ∣∣∞，maxx∈Q |h(x) - uref (x)| to quantify the learning result.
As shown by green stars in Fig. 3(d), kh - uref k∞ fluctuates after some running time using DNN
only. Dashed lines indicate the evolution of the Jacobi method with initial data set to the DNN output
at the corresponding steps. If M is too small (stop too early) (left dashed line), which is equivalent
to only using Jacobi, it would take long time to converge to a small error, because low frequencies
converges slowly, yet. If M is too big (stop too late) (right dashed line), which is equivalent to using
DNN only, much time would be wasted for the slow convergence of high frequencies. A proper choice
of M is indicated by the initial point of orange dashed line, in which low frequencies are quickly
captured by the DNN, followed by fast convergence in high frequencies of the Jacobi method.
This example illustrates a cautionary tale that, although DNNs has clear advantage, using DNNs alone
may not be the best option because of its limitation of slow convergence at high frequencies. Taking
advantage of both DNNs and conventional methods to design faster schemes could be a promising
direction in scientific computing problems.
6	A preliminary theoretical understanding
A subsequent theoretical work (Luo et al., 2019) provides a rigorous mathematical study of the F-
Principle at different frequencies for general DNNs (e.g., multiple hidden layers, different activation
functions, high-dimensional inputs). The key insight is that the regularity of DNN converts into the
decay rate of a loss function in the frequency domain. For an intuitive understanding of this key
insight, we present theories under an idealized setting, which connect the smoothness/regularity of
the activation function with different gradient and convergence priorities in frequency domain.
The activation function we consider is σ(x) = tanh(x), which is smooth in spatial domain and its
derivative decays exponentially with respect to frequency in the Fourier domain. For a DNN of one
6
Under review as a conference paper at ICLR 2020
hidden layer with m nodes, 1-d input x and 1-d output: h(x) = Pjm=1 aj σ(wj x +bj), aj, wj, bj ∈
R. We also use the notation θ = {θj } with θj = aj, θ2j = Wj, and θ3j = bj, j = 1, ∙∙∙ ,m. The
loss at frequency k is L(k) = ɪ ∣h(k) - /(k) ∣ , ∙ is the Fourier transform, f is the target function.
The total loss function is defined as: L = R-+∞∞ L(k) dk. Note that according to Parseval’s theorem,
this loss function in the Fourier domain is equal to the commonly used MSE loss. We have the
following theorems (The proofs are at Appendix E.). Define W = (w1,w2, ∙∙∙ , Wm)T ∈ Rm.
Theorem 1. Considering a DNN of one hidden layer with activation function σ(x) = tanh(x), for
any frequencies k1 and k2 such that |f(k1)| > 0, |f (k2)| > 0, and |k2 | > |k1| > 0, there exist
positive constants c and C such that for sufficiently small δ, we have
〃({W ：|*|>|*| 加S l,j} ∩ B)
μ(Bδ)
≥ 1 — C exp(-c∕δ),
where Bδ ⊂ Rm is a ball with radius δ centered at the origin and μ(∙) is the Lebesgue measure.
Theorem 1 indicates that for any two non-converged frequencies, with small weights, the lower-
frequency gradient exponentially dominates over the higher-frequency ones. Due to Parseval’s
theorem, the MSE loss in the spatial domain is equivalent to the L2 loss in the Fourier domain. To
intuitively understand the higher decay rate of a lower-frequency loss function, we consider the
training in the Fourier domain with loss function of only two non-zero frequencies.
Theorem 2. Considering a DNN of one hidden layer with activation function σ(x) = tanh(x).
Suppose the target function has only two non-zero frequencies k1 and k2, that is, |f (k1)| > 0,
|f (k2)| > 0, |k2| > |k1 | > 0, and |f(k)| = 0 for k 6= k1, k2. Consider the loss function of
L = L(k1) + L(k2) with gradient descent training. Denote
∕∂L(kι)	∂L(kι) ∂L(k2)∖
S =∖ ~HΓ ≤ (O^r ≤ -HΓ∖^
that is, L(k1) decreases faster than L(k2). There exist positive constants c and C such that for
sufficiently small δ, we have
μ ({W : S holds} ∩ Bδ)
μ(Bδ)
≥ 1 — C exp(-c∕δ),
where Bδ ⊂ Rm is a ball with radius δ centered at the origin and μ(∙) is the Lebesgue measure.
7	Discussions
DNNs often generalize well for real problems (Zhang et al., 2016) but poorly for problems like fitting
a parity function (Shalev-Shwartz et al., 2017; Nye & Saxe, 2018) despite excellent training accuracy
for all problems. Understanding the differences between above two types of problems, i.e., good and
bad generalization performance of DNN, is critical. In the following, we show a qualitative difference
between these two types of problems through Fourier analysis and use the F-Principle to provide an
explanation different generalization performances of DNNs.
For MNIST/CIFAR10, we examine 0totai,k = nɪ^ Pn=OtalT yɪ eχp(-i2∏k ∙ χi), where
{(xi , yi )}in=to0tal-1 consists of both the training and test datasets with certain selected output com-
ponent, at different directions of k in the Fourier space. We find that 0totai,k concentrates on the
low frequencies along those examined directions. For illustration, ytotai,k,s along the first principle
component are shown by green lines in Fig. 4(a, b) for MNIST/CIFAR10, respectively. When only
the training dataset is used, 0train,k well overlaps with 0totai,k at the dominant low frequencies.
For the parity function f (x) = Q；=i Xj defined on Ω = {-1,1}d, its Fourier transform is f(k)=
2d Px∈Ω Qd=I Xje-i2πk∙χ =(-i)d Qd=Isin 2πkj. Clearly, for k ∈ [-41, * 11 ]d, the power of the
parity function concentrates at k ∈ {-ɪ, ɪ}d and vanishes as k → 0, as illustrated in Fig. 4(c) for
the direction of 1d. Given a randomly sampled training dataset S ⊂ Ω with S points, the nonuniform
7
Under review as a conference paper at ICLR 2020
(a) MNIST
(c) parity
Figure 4: Fourier analysis for different generalization ability. The plot is the amplitude of the Fourier
coefficient against frequency k . The red dots are for the training dataset, the green line is for the
whole dataset, and the blue dashed line is for an output of well-trained DNN on the input of the whole
dataset. For (c), d = 10. The training data is 200 randomly selected points.
Fourier transform on S is computed as fs(k) = S Pχ∈s Q；=i Xje-i2πk∙χ. As shown in Fig. 4(c),
f(k) and fs (k) significantly differ at low frequencies.
By experiments, the generalization ability of DNNs can be well reflected by the Fourier analysis. For
the MNIST/CIFAR10, we observed the Fourier transform of the output of a well-trained DNN on
{xi }in=to0tal-1 faithfully recovers the dominant low frequencies, as illustrated in Fig. 4(a) and 4(b),
respectively, indicating a good generalization performance as observed in experiments. However,
for the parity function, we observed that the Fourier transform of the output of a well-trained DNN
on {xi}i∈s significantly deviates from f(k) at almost all frequencies, as illustrated in Fig. 4(c),
indicating a bad generalization performance as observed in experiments.
The F-Principle implicates that among all the functions that can fit the training data, a DNN is
implicitly biased during the training towards a function with more power at low frequencies. If
the target function has significant high-frequency components, insufficient training samples will
lead to artificial low frequencies in training dataset (see red line in Fig. 4(c)), which is the well-
known aliasing effect. Based on the F-Principle, as demonstrated in Fig. 4(c), these artificial low
frequency components will be first captured to explain the training samples, whereas the high
frequency components will be compromised by DNN. For MNIST/CIFAR10, since the power of high
frequencies is much smaller than that of low frequencies, artificial low frequencies caused by aliasing
can be neglected. To conclude, the distribution of power in Fourier domain of above two types of
problems exhibits significant differences, which result in different generalization performances of
DNNs according to the F-Principle.
8	Related work
There are different approaches attempting to explain why DNNs often generalize well. For example,
generalization error is related to various complexity measures (Bartlett et al., 1999; Neyshabur et al.,
2017; E et al., 2018), local properties (sharpness/flatness) of loss functions at minima (Keskar et al.,
2016; Wu et al., 2017), stability of optimization algorithms (Hardt et al., 2015), and implicit bias
of the training process (Soudry et al., 2018; Arpit et al., 2017; Xu et al., 2018). On the other hand,
several works focus on the failure of DNNs (Shalev-Shwartz et al., 2017; Nye & Saxe, 2018), e.g.,
fitting the parity function, in which a well-trained DNN possesses no generalization ability. We
propose that the Fourier analysis can provide insights into both success and failure of DNNs.
F-Principle was first discovered in (Xu et al., 2018; Rahaman et al., 2018) simultaneously through
simple synthetic data and not very deep networks. In the revised version, Rahaman et al. (2018) exam-
ines the F-Principle in the MNIST dataset. However, they add noise to MNIST, which contaminates
the labels and damages the structure of real data. They only examine not very deep (6-layer) fully
connected ReLU network with MSE loss, while cross-entropy loss is widely used. This paper verified
that F-Principle holds in the training process of MNIST and CIFAR10, both CNN and fully connected
networks, very deep networks (VGG16) and various loss functions, e.g., MSE Loss, cross-entropy
loss and variational loss function. In the aspect of theoretical study, based on the key mechanism
found by the theoretical study in this paper, Luo et al. (2019) shows a rigorous proof of the F-Principle
for general DNNs. The theoretical study of the gradient of tanh(x) in the Fourier domain is adopted
8
Under review as a conference paper at ICLR 2020
by Rahaman et al. (2018), in which they generalize the analysis to ReLU and show similar results.
Thm 1 is also used to analyze a nonlinear collaborative scheme for deep network training (Zhen
et al., 2018). In the aspect of application, based on the study of the F-Principle in this paper, Cai
et al. (2019) and Cai & Xu (2019) design DNN-based algorithms to solve high-dimensional and
high-frequency problems.
References
Devansh Arpit, Stanislaw Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S
Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. A closer look at
memorization in deep networks. arXiv preprint arXiv:1706.05394, 2017. 8
Peter L Bartlett, Vitaly Maiorov, and Ron Meir. Almost linear vc dimension bounds for piecewise
polynomial networks. In Advances in Neural Information Processing Systems,pp. 190-196, 1999.
8
Ronald Newbold Bracewell and Ronald N Bracewell. The Fourier transform and its applications,
volume 31999. McGraw-Hill New York, 1986. 2, 3.1
Wei Cai and Zhi-Qin John Xu. Multi-scale deep neural networks for solving high dimensional pdes.
arXiv preprint arXiv:1910.11710, 2019. 8
Wei Cai, Xiaoguang Li, and Lizuo Liu. A phase shift deep neural network for high frequency wave
equations in inhomogeneous media. Arxiv preprint, arXiv:1909.11759, 2019. 8
Weinan E and Bing Yu. The deep ritz method: A deep learning-based numerical algorithm for solving
variational problems. Communications in Mathematics and Statistics, 6(1):1-12, 2018. 5
Weinan E, Jiequn Han, and Arnulf Jentzen. Deep learning-based numerical methods for high-
dimensional parabolic partial differential equations and backward stochastic differential equations.
Communications in Mathematics and Statistics, 5(4):349-380, 2017. 5
Weinan E, Chao Ma, and Lei Wu. A priori estimates of the generalization error for two-layer neural
networks. arXiv preprint arXiv:1810.06397, 2018. 8
Lawrence C Evans. Partial differential equations. 2010. 5
Yuwei Fan, Lin Lin, Lexing Ying, and Leonardo Zepeda-Nunez. A multiscale neural network based
on hierarchical matrices. arXiv preprint arXiv:1807.01883, 2018. 5
Jiequn Han, Linfeng Zhang, Roberto Car, et al. Deep potential: A general representation of a
many-body potential energy surface. arXiv preprint arXiv:1707.01478, 2017. 5
Moritz Hardt, Benjamin Recht, and Yoram Singer. Train faster, generalize better: Stability of
stochastic gradient descent. arXiv preprint arXiv:1509.01240, 2015. 8
Juncai He, Lin Li, Jinchao Xu, and Chunyue Zheng. Relu deep neural networks and linear finite
elements. arXiv preprint arXiv:1807.03973, 2018. 5
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter
Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv
preprint arXiv:1609.04836, 2016. 8
Yuehaw Khoo, Jianfeng Lu, and Lexing Ying. Solving parametric pde problems with artificial neural
networks. arXiv preprint arXiv:1707.03351, 2017. 5
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014. C, 6
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced
research). URL http://www. cs. toronto. edu/kriz/cifar. html, 2010. 1
Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998. 1
9
Under review as a conference paper at ICLR 2020
Tao Luo, Zheng Ma, Zhi-Qin John Xu, and Yaoyu Zhang. Theory of the frequency principle for
general deep neural networks. arXiv preprint arXiv:1906.09235, 2019. 1, 6, 8
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring general-
ization in deep learning. In Advances in Neural Information Processing Systems, pp. 5947-5956,
2017. 8
Maxwell Nye and Andrew Saxe. Are efficient deep representations learnable? 2018. 1, 7, 8
Nasim Rahaman, Devansh Arpit, Aristide Baratin, Felix Draxler, Min Lin, Fred A Hamprecht,
Yoshua Bengio, and Aaron Courville. On the spectral bias of deep neural networks. arXiv preprint
arXiv:1806.08734, 2018. 1, 8, B
Shai Shalev-Shwartz, Ohad Shamir, and Shaked Shammah. Failures of gradient-based deep learning.
arXiv preprint arXiv:1703.07950, 2017. 1, 7, 8
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014. 1, 4.2
Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit
bias of gradient descent on separable data. Journal of Machine Learning Research, 19(70), 2018. 8
Lei Wu, Zhanxing Zhu, and Weinan E. Towards understanding generalization of deep learning:
Perspective of loss landscapes. arXiv preprint arXiv:1706.10239, 2017. 8
Zhi-Qin J Xu, Yaoyu Zhang, and Yanyang Xiao. Training behavior of deep neural network in
frequency domain. arXiv preprint arXiv:1807.01251, 2018. 1, 8, B
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016. 1, 7
Hui-Ling Zhen, Xi Lin, Alan Z Tang, Zhenhua Li, Qingfu Zhang, and Sam Kwong. Nonlinear
collaborative scheme for deep neural networks. arXiv preprint arXiv:1811.01316, 2018. 8
A RESPONSE FREQUENCY OF TRAINING DATA {yi}in=-01 ON INPUTS {xi}in=-01
In all our experiments, we consistently consider the response frequency defined for the mapping
function g between inputs and outputs, say Rd → R and any k ∈ Rd via the standard nonuniform
discrete Fourier transform (NUDFT)
n-1
^k = - X g(xi)e-i2πk∙xi,
n
i=0
which is a natural estimator of frequency composition of g. (More details can be found in https:
//en.wikipedia.org/wiki/Non-uniform_discrete_Fourier_transform.) As
n → ∞, ^k → / g(x)e-i2πk∙χν(x) dx, where V(x) is the data distribution.
We restrict all the evaluation of Fourier transform in our experiments to NUDFT of {yi}in=-01 at
{xi }in=-01 for the following practical reasons.
(i)	The information of target function is only available at {xi}in=-01 for training.
(ii)	It allows us to perform the convergence analysis. As t → ∞, in general, h(xi, t) → yi for
any i (h(xi,t) is the DNN output), leading to hk → yk for any k. Therefore, we can analyze the
convergence at different k by evaluating Δf(k) = |hk - yk |/|yk| during the training. If we use a
different set of data points for frequency evaluation of DNN output, then ∆F (k) may not converge to
0 at the end of training.
(iii)	yk faithfully reflect the frequency structure of training data {xi, yi}n-. Intuitively, high
frequencies of yk correspond to sharp changes of output for some nearby points in the training data.
Then, by applying a Gaussian filter and evaluating still at {xi}in=-01, we obtain the low frequency part
of training data with these sharp changes (high frequencies) well suppressed.
10
Under review as a conference paper at ICLR 2020
o 2
(a) target function
10° ιo1 ιo2 ιo3 ιo4
epoch
(b) relative error
Figure 5: 1d input. (a) f (x). Inset : |f (k)|. (b) ∆F (k) of three important frequencies (indicated by
black dots in the inset of (a)) against different training epochs.
In practice, it is impossible to evaluate and compare the convergence of all k ∈ Rd even with a
proper cutoff frequency for a very large d of O(102) (MNIST) or O(103) (CIFAR10) due to curse of
dimensionality. Therefore, we propose the projection approach, i.e., fixing k at a specific direction
and the filtering approach as detailed in Section 3 and 4, respectively.
B ILLUSTRATION OF F-PRINCIPLE FOR 1-D SYNTHETIC DATA
To illustrate the phenomenon of F-Principle, we use 1-d synthetic data to show the evolution of
relative training error at different frequencies during the training of DNN. we train a DNN to fit a
1-d target function f(x) = sin(x) + sin(3x) + sin(5x) of three frequency components. On n = 201
evenly spaced training samples, i.e., {xi}in=-01 in [-3.14, 3.14], the discrete Fourier transform (DFT)
of f (x) or the DNN output (denoted by h(x)) is computed by fk = 1 Pn-OI f(xi)e-i2πik7n and
hk = 1 Pn-01 h(xi)e-i2πjk∕n, where k is the frequency. As shown in Fig. 5(a), the target function
has three important frequencies as we design (black dots at the inset in Fig. 5(a)). To examine the
convergence behavior of different frequency components during the training with MSE, we compute
the relative difference between the DNN output and the target function for the three important
frequencies k s at each recording step, that is, ∆F(k) = ∣hk - fk ∣∕∣fk |, where | ∙ | denotes the norm
of a complex number. As shown in Fig. 5(b), the DNN converges the first frequency peak very fast,
while converging the second frequency peak much slower, followed by the third frequency peak.
Next, we investigate the F-Principle on real datasets with more general loss functions other than MSE
which was the only loss studied in the previous works (Xu et al., 2018; Rahaman et al., 2018). All
experimental details can be found in Appendix. C.
C Experimental settings
In Fig. 5, the parameters of the DNN is initialized by a Gaussian distribution with mean 0 and
standard deviation 0.1. We use a tanh-DNN with widths 1-8000-1 with full batch training. The
learning rate is 0.0002. The DNN is trained by Adam optimizer (Kingma & Ba, 2014) with the MSE
loss function.
In Fig. 1, for MNIST dataset, the training process of a tanh-DNN with widths 784-400-200-10 is
shown in Fig. 1(a) and 1(b). For CIFAR10 dataset, results are shown in Fig. 1(c) and 1(d) of a
ReLU-CNN, which consists of one convolution layer of 3 × 3 × 64, a max pooling of 2 × 2, one
convolution layer of 3 × 3 × 128, a max pooling of 2 × 2, followed by a fully-connected DNN
with widths 800-400-400-400-10. For both cases, the output layer of the network is equipped with a
softmax. The network output is a 10-d vector. The DNNs are trained with cross entropy loss by Adam
optimizer (Kingma & Ba, 2014). (a, b) are for MNIST with a tanh-DNN. The learning rate is 0.001
with batch size 10000. After training, the training accuracy is 0.951 and test accuracy is 0.963. The
amplitude of the Fourier coefficient with respect to the fourth output component at each frequency is
shown in (a), in which the red dots are computed using the training data. Selected frequencies are
marked by black squares. (b) ∆F (k) at different training epochs for the selected frequencies. (c, d)
11
Under review as a conference paper at ICLR 2020
are for CIFAR10 dataset. We use a ReLU network of a CNN followed by a fully-connected DNN.
The learning rate is 0.003 with batch size 512. (c) and (d) are the results with respect to the ninth
output component. After training, the training accuracy is 0.98 and test accuracy is 0.72.
In Fig. 2, for MNIST, we use a fully-connected tanh-DNN with widths 784-400-200-10 and MSE
loss; for CIFAR10, we use cross-entropy loss and a ReLU-CNN, which consists of one convolution
layer of 3 × 3 × 32, a max pooling of 2 × 2, one convolution layer of 3 × 3 × 64, a max pooling of
2 × 2, followed by a fully-connected DNN with widths 400-10 and the output layer of the network is
equipped with a softmax. The learning rate for MNIST and CIFAR10 is 0.015 and 0.003, respectively.
The networks are trained by Adam optimizer (Kingma & Ba, 2014) with batch size 10000. For
VGG16, the learning rate is 10-5. The network is trained by Adam optimizer (Kingma & Ba, 2014)
with batch size 500.
In Fig. 3, the samples are evenly spaced in [0, 1] with sample size 1001. We use a DNN with widths
1-4000-500-400-1 and full batch training by Adam optimizer (Kingma & Ba, 2014). The learning
rate is 0.0005. β is 10. The parameters of the DNN are initialized following a Gaussian distribution
with mean 0 and standard deviation 0.02.
In Fig. 4, the settings of (a) and (b) are the same as the ones in Fig. 1. For (c), we use a tanh-DNN
with widths 10-500-100-1, learning rate 0.0005 under full batch-size training by Adam optimizer
(Kingma & Ba, 2014). The parameters of the DNN are initialized by a Gaussian distribution with
mean 0 and standard deviation 0.05.
D Central difference scheme and Jacobi method
Consider a one-dimensional (1-d) Poisson’s equation:
—∆u(x) = g(x), X ∈ Ω = ( — 1,1)
u(x) = 0, x = -1, 1.
(9)
[—1, 1] is uniformly discretized into n + 1 points with grid size h = 2/n. The Poisson’s equation in
Eq. (9) can be solved by the central difference scheme,
— ∆ui
ui+1 — 2ui + ui-1
(δX)2
g(xi),
i = 1,2,…，n,
resulting a linear system
where
/ 2	-1
—1	2
A =	0	—1
..
..
..
00
Au = g,
0	—1	2	(n-1)×(n-1)
u1
U2
U =	.
.
Un-2
un-1
g1
g2
(10)
(11)
(12)
(13)
gn-2
gn-1
0	0	…0	\
—1	0	∙ • •	0
2	—1	…0
A class of methods to solve this linear system is iterative schemes, for example, the Jacobi method.
Let A = D — L — U , where D is the diagonal of A, and L and U are the strictly lower and upper
triangular parts of —A, respectively. Then, we obtain
u=D-1(L+U)u+D-1g.
(14)
At step t ∈ N, the Jacobi iteration reads as
ut+1 = D-1(L + U)ut + D-1g.
(15)
12
Under review as a conference paper at ICLR 2020
We perform the standard error analysis of the above iteration process. Denote u* as the true value
obtained by directly performing inverse of A in Eq. (11). The error at step t +1 is et+1 = ut+1 - u*.
Then, et+1 = RJet, where RJ = D-1(L + U). The converging speed of et is determined by the
eigenvalues of RJ, that is,
λk
λk(RJ)
cos k∏,
n
k = 1, 2,…,n — 1,
and the corresponding eigenvector vk ’s entry is
ikπ
vk，i = Sin 丁
,i = 1, 2,…，n - 1.
So we can write
n-1
et = X αtkvk,
k=1
where αtk can be understood as the magnitude of et in the direction of vk. Then,
n-1	n-1
et+1 =	αtk RJ vk =	αtkλkvk.
k=1	k=1
αk = λkαk.
Therefore, the converging rate of et in the direction of vk is controlled by λk. Since
kπ	(n - k)π
nn
(16)
(17)
(18)
(19)
(20)
the frequencies k and (n - k) are closely related and converge with the same rate. Consider the
frequency k < n/2, λk is larger for lower frequency. Therefore, lower frequency converges slower in
the Jacobi method.
E Proof of theorems
The activation function we consider is σ(x) = tanh(x).
σ(x) = tanh(x)
ex — e-x
ex + e-x
x ∈ R.
For a DNN of one hidden layer with m nodes, 1-d input x and 1-d output:
m
h(x) =	aj σ(wj x + bj),	aj, wj, bj ∈ R,	(21)
j=1
where wj , aj , and bj are called parameters, in particular, wj and aj are called weights, and bj is also
known as a bias. In the sequel, we will also use the notation θ = {θlj } with θ1j = aj , θ2j = wj ,
and θj = bj, j = 1,…，m. Note that σ(k) = - 0由"鼠/2)where the Fourier transformation and its
inverse transformation are defined as follows:
ʌ
f(k)
Z	f(x)e-ikxdx,	f(x)
-∞
1	+∞
J∞ fWkx dk.
The Fourier transform of σ(wj X + bj) with wj,bj ∈ R, j = 1, ∙∙∙ ,m reads as
Thus
Z --T7、/7、
σ(Wj ∙ +bjXk)
2πi	Cbj k ∖	1
|wj| exp ( Wj) eχp(-2wj) - eχp(2wj).
m
h(k) = X 2∏fexp
j=1	j
1
eχp(-2wj)- eχp( 2⅛)
(22)
(23)
13
Under review as a conference paper at ICLR 2020
We define the amplitude deviation between DNN output and the target function f(x) at frequency k
as
,	. Λ ʌ ,	.	ʌ ,
D(k)，h(k) - f(k).
Write D(k) as D(k) = A(k)eiφ(k), where A(k) ∈ [0, +∞) and φ(k) ∈ R are the amplitude and
phase of D(k), respectively. The loss at frequency k is L(k) = 1 ∣D(k)∣2, where | ∙ | denotes the
norm of a complex number. The total loss function is defined as: L = R-+∞∞ L(k) dk. Note that
according to Parseval’s theorem, this loss function in the Fourier domain is equal to the commonly
used loss of mean squared error, that is, L = R-+∞∞ 2 (h(χ) - f (χ))2 dx. For readers, reference, we
list the partial derivatives of L(k) with respect to parameters
∂L(k)
daj
∂L(k)
∂wj
dLk) = 2jk cos (j - φ(k))E0,
(24)
(25)
(26)
where
E =	Sgn(Wj)A(k)
0 - eχp( 2∏j)- eχp(-2∏j),
El	eχp( 2∏j ) + eχp(-2∏j)
E1 =
eχp( 2∏j)- eχp(- 2∏j)
The descent increment at any direction, say, with respect to parameter θlj , is
∂L _ /+∞ ∂L(k)
∂θlj = J∞ ^∂θl^
The absolute contribution from frequency k to this total amount at θlj is
IdLk) I ≈ A(k)eχp(-lπk∕2wjl) Fij (θj ,k),
(27)
(28)
where θj ， {wj,bj , aj }, θij ∈ θj, Fij (θj, k) is a function with respect to θj and k, which can be
found in one of Eqs. (24, 25, 26).
When the component at frequency k where h(k) is not close enough to f (k), eχp(-∣πk∕2wj-1)
would dominate Gij (θj, k) for a small wj. Through the above framework of analysis, we have the
following theorem. Define
W = (W1,W2 , …,Wm)T ∈ Rm.	(29)
Theorem. Consider a one hidden layer DNN with activation function σ(x) = tanh x. For any
frequencies k1 and k2 such that |f (k1)| > 0, |f (k2)| > 0, and |k2| > |k1| > 0, there exist positive
constants c and C such that for sufficiently small δ, we have
μ (nW ：|* ∣>∣*∣ for all j ∩ Bδ)
μ(Bδ)
≥ 1 - C eχp(-c∕δ),	(30)
where Bδ ⊂ Rm is a ball with radius δ centered at the origin and μ(∙) is the Lebesgue measure.
O
O
TV T	1 .1	t	t	t	7 7 lz∙∕7 ∖l lz∙∕7 ∖l	I I	I T I	t
We remark that c and C depend on k1, k2, |f(k1)|, |f (k2)|, sup |ai|, sup |bi|, and m.
14
Under review as a conference paper at ICLR 2020
Proof. To prove the statement, it is sufficient to show that μ(Sj,δ)∕μ(Bδ) ≤ Cexp(-c∕δ) for each
l,j, where
S一 {w ∈ Bj的图竽
I	I dθij I I dθij
(31)
We prove this for S1j,δ, that is, θj = o7-. The proofs for θj = Wj and bj are similar. Without loss of
generality, we assume that k1,k2 > 0, bj > 0, and Wj = 0, j = 1, ∙∙∙ ,m. According to Eq. (24),
the inequality | dLk1)| ≤ | *尸 | is equivalent to
A(⅛2) I eχp( 2⅜)- eχp(-2⅜)
A(ki)1 exp(嘉)-eχp(-翁)
∙∣sin (bWj2 - °的))1 ≥1 sin (j - °(ki)) I (32)
Notethat |h(k)| ≤ C Pj=I j exp(- 2∏j∣) for k > 0. Thus
Therefore,
lim h(k) = 0
W →0
and
,. ʌ,.
Wm0 DM = -Kk)
(33)
.,. .",..
Wm0 A(k) =|f(k)|
and
lim φ(k) = π + arg(f(k)).
W →0
(34)
O
O
__	_	.-	. _	.	.	一一 •一、 -ι , ʌ , .	. .	_	-	. . O ,.	..	..	-
For W ∈ Bδ with sufficiently small δ, A(k1) > 2|f(ki)| > 0 and A(k2) < 2|/屁)|. Alsonotethat
| sin( jk2 - φ(k2)) | ≤ 1 and that for sufficiently small δ,
eχp(π⅛)- eχp(-π⅛)
eχp(貌)-eχp(-嘉)
≤ 2 eχp
-π(k2 - k1)
2|wj |
(35)
Thus, inequality (32) implies that
.O ,	,	_
8|/的)|	( _ Mk2 - kι)∖
f1feχp (- 2|Wj| J-
Noticing that ∏ |χ| ≤ | sin x| (|x| ≤ ∏) and Eq. (34), we have for W ∈ Sj, for some
(36)
q ∈ Z,
that is,
I 也-arg(f(kι)) - qπ1 ≤
I Wi	I
.O ,	,	_
8n|f(k2)|	(	π(k2 - kι)∖
|f(ki)| ωφ (	2δ 一)
/ b b	,*,、、、	bikι	/	/ 八	,%,、、、
-C1 eχp(-c2∕δ) + qπ + arg(f (k1)) ≤ ——≤ c1 eχp(-c2∕δ) + qπ + arg(f (k1)),
Wi
where c1 = 8πf(乎 and c2 = π(k2 - k1). Define I := I+ U I- where
f (kι)l
I+ := {Wj > 0 : W ∈ Sj,δ}, I- := {Wj < 0 : W ∈ Sj,δ}∙
For Wj > 0, we have for some q ∈ Z,
0 < ------------j---------，— ≤ Wj ≤ ----------------j----------，—
Cl eχp(-c2∕δ) + qπ + arg(f (k1))	-c1 eχp(-c2∕δ) + qπ + arg(f (k1))
(37)
(38)
(39)
(40)
Since W ∈ Bδ and c1 eχp(-c2∕δ) + arg(f(k1)) ≤ 2π, we have 2bjfeqπ ≤ Wj ≤ δ. Then Eq. (40)
only holds for some large q, more precisely, q ≥ q0 :=鬻-2. Thus we obtain the estimate for the
(one-dimensional) Lebesgue measure of I+
∞
μ(I+) ≤ X
q=qo
bj kι	bj kι
--------------------------------—--------------------------------
-Cl eχp(-c2∕δ) + qπ + arg(f (kι))	ci eχp(-c2∕δ) + qπ + arg(f (kι))
∞	1
≤ 2|bj |kiCi eχp(-c2∕δ) ∙，----------X------------------------
念。(qπ + arg(f (kI)))2 - (CI eχp(-c2∕δ))2
≤ C eχp(-C∕δ)∙
(41)
15
Under review as a conference paper at ICLR 2020
The similar estimate holds for μ(I-), and hence μ(I) ≤ Cexp(-c∕δ). For W ∈ Bδ, the (m - 1)
dimensional vector (wι,…，Wj-ι,Wj+ι,…，Wm)T is in a ball with radius δ in RmT. Therefore,
we final arrive at the desired estimate
μ(Sj,δ) ≤ μ(I)ωm-iδm-1
μ(Bδ) -	ωmδm
≤ Cexp(-c∕δ),
(42)
where ωm is the volume of a unit ball in Rm .
□
Theorem. Considering a DNN of one hidden layer with activation function σ(x) = tanh(x).
Suppose the target function has only two non-zero frequencies k1 and k2, that is, |f (k1)| > 0,
|f (k2)| > 0, and |k2| > |k1 | > 0, and |f(k)| = 0 for k 6= k1, k2. Consider the loss function of
L = L(k1) + L(k2) with gradient descent training. Denote
∕∂L(kι)	∂L(kι) ∂L(k2)∖
S =∖ ~HΓ ≤ (O^r ≤ -HΓ∖^
that is, L(k1) decreases faster than L(k2). There exist positive constants c and C such that for
sufficiently small δ, we have
μ ({W : S holds} ∩ Bδ)
μ(Bδ)
≥ 1 - C exp(-c∕δ),
where Bδ ⊂ Rm is a ball with radius δ centered at the origin and μ(∙) is the Lebesgue measure.
Proof. By gradient descent algorithm, we obtain
∂L(kι) _ X ∂L(kι) ∂θij
∂t	∂θj ∂θlj	∂t
l,j
∂L(kι) ∂(L(kι)+ L(k2))
Fj	Ej
-X(陪)2- X
l,j	lj	l,j
∂L(kι) ∂L(k2)
Fj^^∂θj~
∂L(k2)	(∂L(k2) )2	∂L(kι) ∂L(k2)
= - Z k^θlT; - ⅛ ^^θlT^^θlT
l,j	l,j
and
dL _ d (L(kl) + L(k2)) _ X ( dL(kl) dL(k2) )2 0
西= ∂t = ? (Fr + RTJ ≤
l,j
(43)
To obtain
0 < ∂L(kι)	∂L(k2) = _X "(∂L(kι))2_ (∂L(k2))2
况 况—I dθij ) I dθij )
it is sufficient to have
I ∂L(kι) I > I ∂L(k2) I
I ∂θιj i i ∂θj i
(44)
(45)
Eqs. (43, 44) also yield to
∂L(kι)
∂t
< 0.
Therefore, Eq. (45) is a sufficient condition for S. Based on the theorem 1, we have proved the
theorem 2.	□
16
Under review as a conference paper at ICLR 2020
而一-U 二"Es
(a) True image	(b) DNN output
(C) 1-d slice	(d) DFT fit	(e) convergence order
-"=-U- a6」"_
(f) DNN output	(g) 1-d slice	(h) convergence order
Figure 6: F-Principle in fitting a natural image. The training data are all pixels whose horizontal
indices are odd. We initialize DNN parameters by a Gaussian distribution with mean 0 and standard
deviation 0.08 (small initial) or 1 (large initial). (a) True image. (b-g) correspond to the case of the
small initial parameters. (f-h) correspond to the case of the large initial parameters. (b) DNN outputs
of all pixels at different training epochs. (c, g) DNN outputs (blue) and the true gray-scale (red) of test
pixels at the red dashed position in (a). (d) |h(k)| (green) at certain training epoch and |f (k)| (red) at
the red dashed position in (a), as a function of frequency index. Selected peaks are marked by black
dots. (e, h) ∆F (k) computed by the training data at different epochs for the selected frequencies in
(d). (f) DNN outputs of training pixels (left) and all pixels (right) after training. We use a tanh-DNN
with widths 2-400-200-100-1. We train the DNN with the full batch and learning rate 0.0002. The
DNN is trained by Adam optimizer (Kingma & Ba, 2014) with the MSE loss function.
F	MEMORIZING 2-D IMAGE
We train a DNN to fit a natural image (See Fig. 6(a)), a mapping from coordinate (x, y) to gray scale
strength, where the latter is subtracted by its mean and then normalized by the maximal absolute
value. First, we initialize DNN parameters by a Gaussian distribution with mean 0 and standard
deviation 0.08 (initialization with small parameters). From the snapshots during the training process,
we can see that the DNN captures the image from coarse-grained low frequencies to detailed high
frequencies (Fig. 6(b)). As an illustration of the F-Principle, we study the Fourier transform of the
image with respect to x for a fixed y (red dashed line in Fig. 6(a), denoted as the target function f (x)
in the spatial domain). The DNN can well capture this 1-d slice after training as shown in Fig. 6(c).
Fig. 6(d) displays the amplitudes |f(k)| of the first 40 frequency components. Due to the small initial
parameters, as an example in Fig. 6(d), when the DNN is fitting low-frequency components, high
frequencies stay relatively small. As the relative error shown in Fig. 6(e), the first five frequency
peaks converge from low to high in order.
Next, we initialize DNN parameters by a Gaussian distribution with mean 0 and standard deviation 1
(initialization with large parameters). After training, the DNN can well capture the training data, as
shown in the left in Fig. 6(f). However, the DNN output at the test pixels are very noisy, as shown
in the right in Fig. 6(f). For the pixels at the red dashed lines in Fig. 6(a), as shown in Fig. 6(g),
the DNN output fluctuates a lot. Compared with the case of small initial parameters, as shown in
Fig. 6(h), the convergence order of the first five frequency peaks do not have a clear order.
17