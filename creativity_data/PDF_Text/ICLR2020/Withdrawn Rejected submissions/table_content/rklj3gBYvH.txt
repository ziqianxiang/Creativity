Table 1: Test accuracies on the Mini-ImageNet benchmark. The first set of results use convolutionalnetworks, while the second use much deeper residual networks, predominantly in conjunction withpre-training.
Table 2: Ablation study and comparison to MAML using different training setupsNORML learns to dynamically control the learning rates of each learner node’s input and outputweights. This is shown to give a significant improvement when compared to using constant, learn-able, and per-layer learnable learning rates. Intuitively it makes sense, since NORML can effectivelycontrol the learning rates of each parameter in the learner network. Additionally NORML can con-trol the learning rates dynamically. Dynamic control seems crucial to NORML’s success, since aparameter that receives a large update at step t, might need to make a small update at step t+ 1. Theimportance of being able to dynamically adapt the learning rate using prior update information, isconfirmed in the experiments where the input hidden state and input cell state of the meta-learnerare set to zero. Although the meta-learner can still learn nodal learning rates, it is unable to useinformation of previous update steps and ends up performing the same as MAML with a constantlearning rate.
