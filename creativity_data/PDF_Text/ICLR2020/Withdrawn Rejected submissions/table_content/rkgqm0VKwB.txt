Table 1: Comparison to previously published F1 scores for joint named entity recognition (NER)and relation extraction (RE). Ours (gold): our model, when gold entity labels are used as input tothe RE module. Ours: full, end-to-end model (see section 2). Bold: best scores. Subscripts denotestandard deviation across three runs. ∆: difference to our overall score.
Table 2: Ablation experiment results on the CoNLL04 corpus. Scores are reported as a micro-averaged F1 score on the validation set, averaged across three runs of 5-fold cross-validation. (a)Without entity pre-training (section 2.1). (b) Without entity embeddings (eq. 3). (c) Using a singleFFNN in place of FFNNhead and FFNNtail (eq. 4 and 5) (d) Without FFNNhead and FFNNtail (e)Without the bilinear operation (eq. 7). Bold: best scores. Subscripts denote standard deviationacross three runs. ∆: difference to the full models score.
Table A.1: Detailed entity and relation counts for each corpus used in this study.
Table A.2: Hyperparameter values and model details used across all experiments.
Table A.3: Hyperparameter values specific to individual datasets. Similar to Devlin et al. (2018), aminimal grid search was performed over the values 16, 32 for batch size and 2e-5, 3e-5, and 5e-5for learning rate.
