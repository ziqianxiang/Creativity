Table 1: Comparison of Conv-Tasnet and Demucs to state-of-the-art models that operate on thewaveform (Wave-U-Net) and on spectrograms (Open-Unmix without extra data, MMDenseLSTMwith extra data) as well as the IRM oracle on the MusDB test set. The Extra? indicates the number ofextra training songs used. We report the median over all tracks of the median SDR over each track,as done in the SiSec MUs evaluation campaign (Stoter et al., 2018). The All column reports theaverage over all sources. Demucs metrics are averaged over 5 runs, the confidence interval is thestandard deviation over âˆš5. In bold are the values that are statistically state-of-the-art either with orwithout extra training data.
Table 2: Mean Opinion Scores (MOS) evaluating the quality and absence of artifacts of the separatedaudio. 38 people rated 20 samples each, randomly sample from one of the 3 models or the groundtruth. There is one sample per track in the MusDB test set and each is 8 seconds long. Ratings of 5means that the quality is perfect (no artifacts).
Table 3: Mean Opinion Scores (MOS) evaluating contamination by other sources. 38 people rated 20samples each, randomly sampled from one of the 3 models or the ground truth. There is one sampleper track in the MusDB test set and each is 8 seconds long. Ratings of 5 means no contamination byother sources.
Table 4: Ablation study for the novel elements in our architecture described in Section 4. We useonly the train set from MusDB and report best L1 loss over the valid set throughout training as wellthe SDR on the test set for the epoch that achieved this loss.
