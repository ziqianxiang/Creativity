Table 1: Statistics on datasets used for pre-training. Each cell shows #image-text pairs (#images).
Table 2: Statistics on the datasets of downstream tasks.
Table 3: Evaluation on pre-training tasks and datasets using VQA, Image-Text Retrieval on Flickr30K,NLVR2 , and RefCOCO+ as benchmarks. All results are obtained from UNITER-base. Averages of R@1,R@5 and R@10 on Flickr30K for Image Retrieval (IR) and Text Retrieval (TR) are reported. Dark and lightgrey colors highlight the top and second best results across all the tasks trained with In-domain data.
Table 4: Results on downstream V+L tasks from UNITER model, compared with task-specific state-of-the-art(SOTA) and concurrent pre-trained models. ZS: Zero-Shot, IR: Image Retrieval and TR: Text Retrieval.
Table 5: Experiments on two-stage pre-training for VCR. Results are from UNITER-base on VCR val split. Stage I and Stage II denote first-stage and second-stage pre-training.					Table 6: Experiments on three modified settings for NLVR2 . All models use pre-trained UNITER-base.	5	ConclusionIn this paper, we present UNITER, a large-scale pre-trained model providing UNiversal Image-TExt Representations for Vision-and-Language tasks. Three main pre-training tasks are proposedand evaluated through extensive ablation studies. Trained with both in-domain and out-of-domaindatasets, UNITER outperforms state-of-the-art models over multiple V+L tasks by a significantmargin. Future work includes studying early interaction between raw image pixels and sentencetokens, as well as developing more effective pre-training tasks.
Table 7: VCR results from VLBERT (Su et al., 2019), ViLBERT (Lu et al., 2019), and UNITER.
Table 8: NLVR results on test-U split from VisualBERT (Li et al., 2019b), LXMERT (Tan & Bansal, 2019),and UNITER.
Table 9: A direct comparison between ViLBERT (Lu et al., 2019), VLBERT (Su et al., 2019), and ourUNITER, all trained on Conceptual Captions (Sharma et al., 2018) only.
