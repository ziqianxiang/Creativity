Table 1: Comparisons among three attention operators in terms of time and space complexities. Attnand Attn1/N denote the regular attention operators using softmax and scaling by 1/N for similarityscores normalization, respectively.
Table 2: Comparisons between the regular attention operator, the regular attention operator with apooling operation (Wang et al., 2018), and our proposed SAO in terms of the number of parameters,number of MAdd, memory usage, and CPU inference time on simulated data of different sizes. Theinput sizes are given in the format of “spatial sizes2 × number of input channels”. “Attn” denotes theregular attention operator. “Attn+Pool” denotes the regular attention operator which employs 2 × 2pooling operations on K and V input matrices to reduce required computational resources.
Table 3: Comparisons between SANet and otherCNNs in terms of the top-1 accuracy, the number oftrainable parameters, and MAdd on the ImageNetvalidation set.__________________________Model	Top-I	Params	MAddGoogleNet	0.698	6.8m	1550mVGG16	0.715	128m	15300mSqueezeNet	0.575	1.3m	833mMobileNetV1	0.706	4.2m	569mChannelNet-v1	0.705	3.7m	407mShuffleNet 1.5x	0.715	3.4m	292mMobileNetV2	0.720	3.47m	300mSANet (ours)	0.730	3.44m	288mCNNs and report the top-1 accuracy, the number of parameters, and MAdd in Table 3. Mo-bileNetV2 (Sandler et al., 2018) is the previous state-of-the-art model in terms of the computationalcost and model performance. Compared to MobileNetV2, our SANet significantly outperformsMobileNetV2 by a margin of 1% with a smaller number of parameters. By using our SAO, SANetcan achieve new state-of-the-art performance with limited computational resources. Consideringthat we only make minor modifications from the architecture of MobileNetV2, the performanceboost is significant. Compared to the module using regular convolutional layers, our proposed
Table 4: Comparisons between SANet, the net-work using the same architecture as SANet withSqueeze-and-Excite block (denoted as SENet) orwith regular attention operators (denoted as At-tnNet), the AttnNet using regular attention opera-tors with a pooling operation (AttnNet+Pool), andthe AttnNet with regular attention operators usingscaling by 1/N (denoted as AttnNet1/N) in terms ofthe top-1 accuracy, the number of total parameters,and MAdd on the ImageNet validation set.
Table 5: Comparisons among MobileNetV2,SANet, and AttnNet in terms of top-1 accuracy,memory usage, and execution time. We report theactual memory usage using inputs of 128 × 2242 ×3. The time reported here is the model executiontime of 100 iterations.__________________Model	Top-1	Memory	TimeMobileNetV2	0.720	7.4GB	14.4sSANet	0.730	7.5GB	14.9sAttnNet	0.730	11.7GB	27.1scates that our SAO is more efficient and effective compared to the regular attention operator usingpooling operations. Note that AttnNet1/N and SANet achieve the same performance as AttnNet,which demonstrates the feasibility of replacing the expensive softmax(∙) function with scaling by1/N. Notably, all networks with attention operators significantly outperform SENet which squeezesthe spatial information. This demonstrates the effectiveness of attention operators that effectivelybuild long-range relationships and lead to better performances.
Table 6: Comparisons between MobileNetV2,MobileNetV2 with SAOs (MobileNetV2+SAO),SANet, SANet without SAO (SANet w/o SAO),and SANet without the global item in Eq. (5) interms of the top-1 accuracy, and the number oftotal parameters on the ImageNet validation set.
Table 7: Comparison results between U-Net, U-Net with regular attention operator (U-Net+Attn), andU-Net with SAO (U-Net+SAO) in terms of peak signal to noise ratio (PSNR), structural similarityindex (SSIM), and normalized root mean square error (NRMSE). On the metrics followed by an↑, the higher value indicates the better performance. Conversely, lower value indicates the betterperformance on the metrics followed by a LDataset	Metric	U-Net	U-Net+Attn	U-Net+SAO	PSNR↑	31.571	31.951	31.929Planaria	SSIM↑	0.7707	0.7919	0.7898	NRMSEJ	0.0268	0.0257	0.0257	PSNR↑	32.433	32.571	32.549Tribolium	SSIM↑	0.9171	0.9201	0.9196	NRMSEJ	0.0241	0.0236	0.0239	PSNR↑	21958	22.361	22.329Flywing	SSIM↑	0.5592	0.5903	0.5916	NRMSEJ	0.0798	0.0763	0.0767SANet w/o SAO without using SAOs is 0.9% lower than that of SANet. Notably, SANet consumesthe least computational costs while achieving the best performance. The results demonstrate thatour SAO is consistently efficient and effective when being applied in different network architectures.
Table 8: Comparisons between SANet and theSANet removing trainable parameters in SAOs(SANet w/o params) in terms of the top-1 accuracyand number of parameters.
Table 9: Details of SANet architecture. Each line represents a sequence of operators and argumentsin the format of “input size / operator name / expansion rate / output channels / number of operators /stride”. We use “Conv2D”, “AvgPool”, and “FC” to denote 2D convolutional layer, global averagepooling layer, and fully-connected layer, respectively. We use kernel size of 3 × 3 in depth-wiseconvolutional layers. The first depth-wise convolutional layer in a sequence of operators appliesstride of s, while the other layers use stride of 1. We use k to denote the class number in the task.
Table 10: Summary of datasets used in our image restoration experiments. Planaria and Triboliumare used for image denoising experiments and FlyWing is used in image projection experiments.
