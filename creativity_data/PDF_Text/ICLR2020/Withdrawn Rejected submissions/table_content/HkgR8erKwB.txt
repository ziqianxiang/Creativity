Table 1: Different σp values experiments. All models composed of five fully connected layers withReLU activation functions.
Table 2: Different depth level experiments. For fair comparison all models have roughly the samenumber of parameters (〜80K for MNIST,〜800K for Fashion-MNIST,〜1500 for regression). Allmodels composed of fully connected layers with ReLU activation function.
Table 3: Results for multi-class image classification benchmarks. We report both loss and accuracymetrics for the training set and test set, together with the generalization loss and accuracy.
Table 4: Results for the out-of-distribution experiments. We report both loss and accuracy measuresfor the training set and test set, together with entropy values. We explore training on MNIST (MN),Fashion-MNIST (FMN), and CIFAR-10. Testing was done on MNIST (MN), Fashion-MNIST(FMN), NotMNIST, and SVHN. Reported losses and accuracies are computed using the out-of-distribution dataset.
Table 5: Model architectures. A description of the model architectures for MNIST, Fashion-MNISTand CIFAR-10. The numbers inside the parenthesis indicate the layer output dimension. For allmodels we use a ReLU activation function after each layer containing trainable weights.
Table 6: Results for experiments on a binary classification variant of MNIST. We report both loss andaccuracy metrics for the training set and test set, together with the generalization loss and accuracy.
Table 7: Results for multi-class image classification benchmarks. We report both loss and accuracymetrics for the training set and test set, together with the generalization loss and accuracy. Addition-ally, we report the KL-divergence for the BNN models. We compare BNN to softmax models andMC-Dropout models, trained with Bernoulli dropout. Dropout rates and σp values are in parenthesisnext to the model name, e.g., ‘softmax (0.5)’ refers to a softmax model with Bernoulli dropout ofrate 0.5.
Table 8: Results for the out-of-distribution experiments. We report both loss and accuracy measuresfor the training set and test set, together with entropy values. We explore training on MNIST (MN),Fashion-MNIST (FMN), and CIFAR-10. Testing was done on MNIST (MN), Fashion-MNIST(FMN), NotMNIST, and SVHN. Reported losses and accuracies are computed using the out-of-distribution dataset.
