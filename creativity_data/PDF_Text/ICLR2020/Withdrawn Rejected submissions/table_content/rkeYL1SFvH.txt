Table 1: Architecture of the system used to train massively multilingual sentence embeddings. SeeArtetxe & Schwenk (2018a) for details.
Table 2: Illustration how sentences in the wrong language can hurt the alignment process with amargin criterion. See text for a detailed discussion.
Table 3: Comparison of NMT systems trained on the Europarl corpus and on bitexts automaticallymined in Wikipedia by our approach at a threshold of 1.04. We give the number of sentences (firstline) and the BLEU score (second line of each bloc) on newstest2014.
Table 4: WikiMatrix: number of extracted sentences for each language pair (in thousands), e.g. Es-Ja=219 corresponds to 219,260 sentences (rounded). Thecolumn tisize,f gives the number of lines in the monolingual texts after deduplication and LID.
Table 5: BLEU scores on the TED test set as proposed in (Qi et al., 2018). NMT systems weretrained on bitexts mined in Wikipedia only (with at least twenty-five thousand parallel sentences).
Table 6: WikiMatrix (part 2): number of extracted sentences (in thousands) for languages witha rather small Wikipedia. Alignments with other languages yield less than 5k sentences and areomitted for clairty.
Table 7: BLEU scores on the TED test set as proposed in (Qi et al., 2018). NMT systems weretrained on bitexts mined in Wikipedia only. No other resources were used.
