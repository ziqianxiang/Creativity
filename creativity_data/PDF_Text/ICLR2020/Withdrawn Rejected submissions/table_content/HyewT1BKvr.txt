Table 1: AudioMNIST mean test accuracyNetwork	# Trainable parameters	Avg. accuracyAudioNet	17M	94.9% ± 1.54%SpectroBank-AudioNet	3.5 M	96.8% ± 1.22%SpectroBank-custom (Gammatone)	300 k	98.0% ± 0.41%SpectroBank-custom (SincNet)	300 k	97.2% ± 1.0%SpectroBank-custom (Wavelet)	300 k	89.9% ± 1.18%3.3	Google Speech Command ResultsThe Google Speech Command dataset provides similar data to the AudioMNIST one, with a largernumber of classes (35) to distinguish. In the original setting, the goal was to classify 15 unwantedwords together as unknown. However, in the experiments we performed, we classify each wordindependently. This dataset does not have pre-defined folds, but train, test and validation data arespecified explicitly. We focus on the "Basic" network of the SampleCNN group described in Kim et al.
Table 2: UrbanSound8K mean test accuracyNetwork	# Trainable parameters	Avg. accuracyM3	222 k	58.94% ± 3.83%SpectroBank-M3	22.5 k	59.17% ± 5.33%M5	561 k	66.98% ± 6.37%SpectroBank-M5	513 k	67.45% ± 5.48%SpectroBank-SampleCNN	1M	69.16% ± 5.95%The approach taken by Abdoli et al. (2019) is to perform classification on overlapping splits of initialaudio data (usually having a length of 1 second). They however also compare to a network taking asingle block of data (having a length of ca. 3 seconds). While the code was supposed to be made4https://github.com/philipperemy/very-deep-convnets-raw-waveforms7Under review as a conference paper at ICLR 2020available after final publication, the repository5 was still empty. The model was then reimplementedand trained according to the description found in the paper, using all 4 seconds of input data insteadof trimming it to 3 seconds. Instead of the mean accuracy claimed (83% ± 1.3%, from Table 2 inoriginal paper), our tests only achieved 63.8% ± 5.68%, which is a significant difference. We havebeen unable so far to explain this discrepancy.
Table 3: Description of the filter bank types and the parameters used during training. In most of ourexperiments, γ is fixed to 4.
Table 4: SpectroBank custom architecture for AudioMNIST.
Table 5: SpectroBank-XS custom architecture for AudioMNIST.
Table 6: M3-SpectroBank custom architecture for UrbanSound8K.
Table 7: M5-SpectroBank custom architecture for UrbanSound8K.
Table 8: SampleCNN-SpectroBank basic block. Choice of k is detailed in Kim et al. (2019)Layer	Output sizeInput	N×dConvolution (k filters, size 4, stride 1)	N×kBatch Normalization	N×kMaxPooling (stride 2)	与× k14Under review as a conference paper at ICLR 2020Table 9: SampleCNN-SpectroBank architecture for Google Speech Commands (n = 35) or Urban-sound8K (n = 10). BB stands for ’Basic Block’, and GMP for ’Global Max Pooling’LayerOutput sizeInputSpectroBank (80 filters, size 160, stride 40)Batch NormalizationBB 0 (k = 80)BB 1 (k = 80)BB 2 (k = 160)BB 3 (k = 160)BB 4 (k = 160)
Table 9: SampleCNN-SpectroBank architecture for Google Speech Commands (n = 35) or Urban-sound8K (n = 10). BB stands for ’Basic Block’, and GMP for ’Global Max Pooling’LayerOutput sizeInputSpectroBank (80 filters, size 160, stride 40)Batch NormalizationBB 0 (k = 80)BB 1 (k = 80)BB 2 (k = 160)BB 3 (k = 160)BB 4 (k = 160)BB 5 (k = 160)BB 6 (k = 320)Concatenate (GMP(BB 4), GMP(BB 5), GMP(BB 6))DenseBatch NormalizationDropout (0.25)Dense n16000 × 1
Table 10: Class statistics over different datasets.
