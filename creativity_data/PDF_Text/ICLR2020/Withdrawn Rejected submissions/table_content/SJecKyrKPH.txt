Table 1: Error on various rotated data corpus (%)Corpus	Approach	Channel=4	Channel=7	Channel=8	Channel=13	Channel=24	TI-POOLING	2.47	-	1.88	-	1.61MNIST-rot-12k	MINTIN	1.76	-	1.59	-	1.57	ICNN	1.35	-	1.12	-	0.98	TI-POOLING	8.7	-	8.2	-	7.3CIFAR-10-rot-12k	MINTIN	7.4	-	6.8	-	6.4	ICNN	5.3	-	4.5	-	4.3	TI-POOLING	-	1.44	-	1.46	-Half-rotated MNIST	MINTIN	-	1.32	-	1.23	-	ICNN	-	1.18	-	1.12	-	TI-POOLING	-	8.5	-	7.9	-Half-rotated CIFAR-10	MINTIN	-	7.2	-	7.1	-	ICNN	-	5.9	-	5.3	-5	ConclusionWe introduced a novel input-conditioned convolution filter framework, which unlike traditionalCNNs, generates a different set of input-aware convolution filters conditioned on an input imageinstance. We also proposed a decoder network to mitigate the transformation present in the inputimages with the help of a set of pre-defined representative images of the input classes. We employed7
Table 2: Error on various scaled data corpus (%)Corpus	Approach	Channel=5	Channel=9	TI-POOLING	1.52	1.32Scaling MNIST	MINTIN	1.01	0.96	ICNN	0.69	0.68	TI-POOLING	7.1	6.8Scaling CIFAR-10	MINTIN	6.7	6.3	ICNN	5.6	4.8our proposed framework in combination with parallel Siamese networks and max-pooling techniqueto present transformation-invariant neural network. Our proposed filter generator is, in principle, ageneric representation of a standard Convolution layer, which gives our proposed transformation-invariant network greater modeling flexibility. We investigated our proposed approach on the varia-tions of 2 computer vision datasets, which covers both rotation and scaling variations, and achievedstate-of-the-art results. Our proposed framework can also be easily integrated with any CNN modelto enhance its ability to incorporate contextual information present in images.
