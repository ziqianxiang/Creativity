Table 1: Modes captured and percentage of high quality sample on 2D synthetic data. Our modelconsistently captures the highest number of modes and produces competing percentage of high qual-ity samples.
Table 2: Grid Search for Hyper-parametersIn order to estimate both the missing modes and the sample qualities, we first train a regular CNNclassifier on the MNIST digits, and then apply it to compute the MODE Score which is proposed inChe et al. (2016). MODE Score is a variant of Inception Score (IS)(Salimans et al., 2016) and itsdefinition goes as follow:exp(EχDκL(p(y∣x)kp(y)) - Dkl(p*(ykp(y))))	(11)where p(y∣x) is the Softmax output of a trained classifier of the labels, and p*(y) and p(y) are themarginal distributions of the label of the generated samples and training samples respectively. Wetrain each architecture for 50 epochs and after that draw 10K samples for evaluation. The resultingdistribution of MODE Score is shown in Figure 4. It can be seen that our model clearly improvesthe sample quality and diversity compared to GANs.
Table 3: Degree of mode collapse, measured by modes captured and sample quality (as measured byKL) on Stacked-MNIST. Our model captures the most modes and also achieves the highest quality.
Table 4: FID scores for different models, lower is better. Best FID is the best run in 5 trial.
