Table 1: Performance (mean ± std of 5 repetitions) on Multi-MNIST, uncorrelated tasks, higher isbetter. Our architecture achieves consistently better accuracies than the single task baseline whileusing much less parameters (the third column shows the ratio between the number of parameters anda standard LeNet architecture). Scaling the number of tasks in our architecture costs no additionalhardware.
Table 2: Ablations on Multi-MNIST.
Table 3: Performance on CLEVR, higher is better. Our approach yields better accuracies also oncorrelated set of tasks with no additional hardware as tasks are added. Better accuracies are demon-strated both compared to the single task and uniform scaling approaches while using less parameters.
Table 4: Performance on CUB200, higher is better. Our architecture is scalable with the numberof tasks and outperforms other methods. All models trained for 200 epochs with lr 1e-4 using aresnet-18 backbone.
Table 5: Number of parameters in the M-MNIST and CLEVR architectures(a) M-MNIST experiment(b) CLEVR experimentModule / Architecture	# params	Module / Architecture	# paramsRecognition backbone	21,250	Recognition backbone	7,448,256Each branch	3,000	Each branch	7,473,152TD with laterals	6,651	TD with laterals	8,306,688task embedding	320		task embedding	1568Single-Task architecture	24,250	Single-Task architecture	14,921,408Multi-branched architecture	21,250 + 3000 ∙ K	Multi-branched architecture	14, 921,408 + 7, 473, 152 ∙ KTD modulation architecture	21,250 + 9651 + 320 ∙ K	TD modulation architecture	14, 921, 408 + 8, 306, 688 + 1568 ∙ KA Implementation detailsFor the MultiMNIST experiments, we use an architecture based on LeNet (LeCun et al., 1998)).
