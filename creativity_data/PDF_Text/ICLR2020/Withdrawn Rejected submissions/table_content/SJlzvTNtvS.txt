Table 1: Comparison with state-of-the-art image classifiers on CIFAR-10Architecture	Test Error (%)	Params (M)	Search Cost (GPU days)	Search MethodDenseNet-BC (Huang et al., 2017)	3.46	25.6	-	manualNASNet-A + cutout (Zoph et al., 2018)	2.65	3.3	2000	RLBlockQNN (Zhong et al., 2018)	3.54	39.8	96	RLAmoebaNet-B + cutout (Real et al., 2019)	2.55 ± 0.05	2.8	3150	evolutionHierarchical evolution (Liu et al., 2017)	3.75 ± 0.12	15.7	300	evolutionPNAS (Liu et al.,2018a)	3.41 ± 0.09	3.2	225	SMBoENAS + cutout (Pham et al., 2018)	2.89	4.6	0.5	RLDARTS (second order) (Liu et al., 2018b)	2.76 ± 0.09	3.3	4	gradient-basedSNAS (moderate) (Xie et al., 2018)	2.85 ± 0.02	2.8	1.5	gradient-basedSNAS (aggressive) (Xie et al., 2018)	3.10 ± 0.04	2.3	1.5	gradient-basedGDAS (Dong & Yang, 2019)	2.82	2.5	0.17	gradient-basedMiLeNAS	2.51± 0.11 (best: 2.34)	3.87	0.3	gradient-basedMiLeNAS	2.80± 0.04 (best: 2.72)	2.87	0.3	gradient-basedMiLeNAS	2.50	2.86	0.3	gradient-basedMiLeNAS	2.76	2.09	0.3	gradient-basedIn the evaluation stage, 20 searched cells were stacked to form a larger network; it was subsequentlytrained from scratch for 600 epochs with a batch size of 96 and a learning rate set to 0.03. TheCIFAR-10 evaluation results are shown in Table 1 (all architectures were searched using λ = 1). The
Table 2: Comparison with state-of-the-art image classifiers on ImageNetArchitecture	Test Error (%)		Params (M)	+× (M)	Search Cost (GPU days)	Search Method	top-1	top-5				Inception-v1 (Szegedy et al., 2015)	30.2	10.1	6.6	1448	-	manualMobileNet (Howard et al., 2017)	29.4	10.5	4.2	569	-	manualShuffleNet (Zhang et al., 2018)	26.3	-	〜5	524	-	manualNASNet-A (Zoph et al., 2018)	26.0	8.4	5.3	564	2000	RLAmoebaNet-A (Real et al., 2019)	25.5	8.0	5.1	555	3150	evolutionAmoebaNet-C (Real et al., 2019)	24.3	7.6	6.4	570	3150	evolutionPNAS (Liu et al., 2018a)	25.8	8.1	5.1	588	〜225	SMBODARTS (Liu et al., 2018b)	26.7	8.7	4.7	574	1	gradient-basedSNAS (Xie et al., 2018)	27.3	9.2	4.2	522	1.5	gradient-basedGDAS (Dong & Yang, 2019)	27.5	9.1	4.4	497	0.17	gradient-basedGDAS (Dong & Yang, 2019)	26.0	8.5	5.3	581	0.21	gradient-basedMiLeNAS	25.4	7.9	4.9	570	0.3	gradient-basedMiLeNAS	24.7	7.6	5.3	584	0.3	gradient-basedTransferability is a crucial criterion used to evaluate the potential of the learned cells (Zoph et al.,2018). To show if the cells learned through our method on CIFAR-10 can be generalized to largerdatasets, we use the same cells as in CIFAR-10 for the classification task on ImageNet. More detailsof the transfer learning experiments can be found in Appendix B.1.1. Table 2 presents the results
