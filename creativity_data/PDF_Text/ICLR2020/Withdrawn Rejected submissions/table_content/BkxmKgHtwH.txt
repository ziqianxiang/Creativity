Table 1: Percentage of xadv with more errors than x by attack method and dataset.
Table 2: Likert scale response: Does changing from x to xadv preserve meaning?Average scores as shown in Figure 2 vary by dataset, but users’ answers generally generally averageout to around 3 (“Not Sure”). Clarifying the survey question makes it clear that many of the examplesare not semantics-preserving.
Table 3: Likert scale response: Does changing from x to xadv change meaning?Table 3 shows that averages are now closest to “2 - Agree” rather than “3 - Not sure”. Thus, invertingthe question resulted in generated examples being rated as less semantics-preserving. Future humanevaluation studies should use similar practices to minimize the effects of this bias.
Table 4: Confusion matrix for humans guessing if perturbed examples are computer-altered5	To Need Standardized Metrics S upported By Human EvaluationA method that detects or generates semantics-preserving adversarial examples relies on a distancemetric d to measure how well two sentences align in meaning. Two inputs (x1 , x2) are paraphrases3For more discussion, see Section A.27Under review as a conference paper at ICLR 2020QQgO8 6 4 2() XUeW_3pun Auejnuu4OO.1O 0.15	0.20	0.25	0.30	0.35ε = 1 - USE Cosine SimiIarityThreshoId454°5 0 5 0 5 03 3 2 2 1 1(⅜ruet=<PUn X32n84510	20	30	40	50£ = Num synonyms
