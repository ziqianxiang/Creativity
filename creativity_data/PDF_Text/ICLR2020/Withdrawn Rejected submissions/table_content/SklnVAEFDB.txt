Table 1: The difference of BERT-AL and BERT-SUM	BERTSUM	BERT-AL	Architeture	BERT + summarization layer	segment-wise BERT + multi-channel LSTM + summarization layerPosition embedding	same as BERTs position embedding	copy and concatenate BERTs position embeddingMax input length	BERTs input length	arbitrarily long4 ExperimentIn this section, we demonstrate BERT-ALs effectiveness on text summarization by conducting ex-periments on the CNN/Daily Mail dataset. We compare BERTSUM with our models on variousdifferent settings, since BERTSUM is the state-of-the-art on CNN/Daily Mail dataset.
Table 2: Statistics of CNN/Daily Mail dataset	CNN	Daily Mail	TotalTrain	90266	196961	287227Validate	1220	12148	13368Test	1093	10397	11490We also perform the same preprocessing for data as BERTSUM did, including keeping entities,splitting sentences by CoreNLP and following methods in See et al. (2017).
Table 3: Setting of experiment groups	IBERT	LBERT-AL	nsegmentGroup-1	^8	TΓ2	64Group-2	16	512	32Group-3	128	512	4Group-4	256	1024	4Assume that we have only a pretrained BERT model and the length of its position embedding islBERT . Our task is to produce summaries on documents with ldoc tokens. The BERT-AL cantake Ibert-Al length tokens as input QBERT-AL = nsegment * LBERT and Ibert-Al >= hoc).
Table 4: Experiment results	Model	ROUGE-1	ROUGE-2	ROUGE-L	BERTSUM -	42.94	20.14	39.38	Baseline-1	^647	1.24	5.68Group-1	Baseline-2	40.38	17.86	36.77	BERT-AL	41.26	18.63	37.69	Baseline-1	ɪ^ɪ	3.69	10.47Group-2	Baseline-2	40.44	17.93	36.83	BERT-AL	41.73	19.00	38.18	Baseline-1	^4Γ7Γ9	18.57	37.50Group-3	Baseline-2	41.44	18.77	37.83	BERT-AL	42.14	19.38	38.60	Baseline-1	"42：30	19.58	38.73Group-4	Baseline-2	42.27	19.57	38.72	BERT-AL	42.61	19.79	39.071)	For all of the four groups, BERT-AL outperforms baselines, consistently. It proves that BERT-ALis effective on long document summarization task, which comes from merging Transformer’s localfeature extraction ability and LSTM’s global time capturing ability.
