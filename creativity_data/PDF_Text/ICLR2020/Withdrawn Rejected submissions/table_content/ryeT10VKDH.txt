Table 1: Transition Model and environment properties for Source and Target task and % changecompared to using a warm-start or standalone RL method. Note that the target domain perturba-tions introduced are significant enough such that source policy alone without any adaptation in thetarget domain produced no meaningful results (Figure-1). This notion of adaptation in the face ofuncertainty is a key advancement over traditional policy transfer, meta-learning, or adversarial RLmethods aiming to improve performance by learning a policy over a set of lightly perturbed tasks.
Table 2: Policy Network details and Network learning parameter detailsFigure 4: Policy Transfer from Inverted Pendulum to Non-stationary Inverted pendulum: (a) Aver-age Rewards and (b) Training length, TA-TL(ATL ours), UMA-TL(Jumpstart-RL) and Stand-aloneRLD.1 Inverted Pendulum (IP) to time-varying IPWe demonstrate our approach for a continuous state domain, Inverted Pendulum (IP) swing-up andbalance Figure-4. The source task is the conventional IP domain. The target task differs from thesource task in the transition model. The target task is a non-stationary inverted pendulum, where thelength and mass of the pendulum are continuously time varying with function Li = L0 + 0.5cos( ∏0i)and Mi = M0 + 0.5cos(5∏i∙), where L0 = 1, M0 = 1 and i = 1 ...N. The state variablesdescribing the system are angle and angular velocity {θ, θ} ∈ [-π, π]. The RL objective is toswing-up and balance the pendulum upright such that θ = 0, θ = 0. The reward function is selectedas R(θ,θ) = -10∣θ∣2 - 5∣θ∣2, which yields maximum value at upright position and minimum at thedown-most position. The continuous action space is bounded by T ∈ [-1, 1]. Note that the domainis tricky, since full throttle action is assumed to not generate enough torque to be able to swing thependulum to the upright position, hence, the agent must learn to swing the pendulum back and forthand leverages angular momentum to go to the upright position.
