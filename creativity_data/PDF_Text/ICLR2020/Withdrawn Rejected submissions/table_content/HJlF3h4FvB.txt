Table 1: Results of Self-distillation.
Table 2: Architecture of the student network. After each convolution layer, there is a Rectified LinearUnit(ReLU) layer.
