Table 1: Sentiment analysis, LSTM on IMDB and SST datasets. Embedding compression is calculatedas the ratio between the number of parameters in the full embedding layer and TT-embedding layer.
Table 2: NMT, Transformer-big on WMT‘14 English-to-German dataset. Both case-sensitivetokenized BLEU and de-tokenized SacreBLEU (Post, 2018) on newstest2014 are reported.
Table 3: LM, Transformer-XL (Dai et al., 2019) on WikiText-103 dataset.
Table 4: CTR prediction. The hashed dataset is constructed as specified in Section 4.4 with hashingvalue 105 . Embedding layers with more than 2000 unique tokens were replaced by TT-embeddingswith shape factorizations consisting of 3 or 4 factors.
Table 5: NMT, Transformer-big on WMT‘14 English-to-German dataset. Both case-sensitivetokenized BLEU and de-tokenized SacreBLEU (Post, 2018) on newstest2014 are reported.
Table 6: Full list of hyperparameters used to train Transformer-big for NMT.
Table 7: Full list of hyperparameters used to train Transformer-XL for LM.
