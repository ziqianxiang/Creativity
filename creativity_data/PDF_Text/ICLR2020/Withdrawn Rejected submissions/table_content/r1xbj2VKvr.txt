Table 1: Dataset StatisticsDataset	Nodes	Edges	Classes	Features	Avg DegreePubmed	19717	-44324-	3	500	4.47Blogcatalog	5196	171743	6	8189	66.11Reddit	232,965	11,606,919	41	602	100.30PPI	56944	818716	1214	50	28.764.2	Experimental settingsWe compare CADE against the following approaches in a fully unsupervised and inductive setting:•	GraphSAGE: In our proposed model, CADE, the base encoder mainly originates fromGraphSAGE, a hierarchical neighbor sampling and aggregating encoder for inductive learn-ing. Three alternative aggregators are used in Graphsage and CADE: (1) Mean aggregator,which simply takes the elementwise mean of the vectors in hku-∈N1 (v) ; (2) LSTM aggrega-tor, which adapts LSTMs to encode a random permutation of a node’s neighbors’ hk-1 ;(3) Maxpool aggregator, which apply an elementwise maxpooling operation to aggregateinformation across the neighbor nodes.
Table 2: Prediction results for Pubmed/Blogcatalog w.r.t different unseen ratioMethods unseen-ratio	10%	Pubmed 30%	50%	Blogcatalog						10%	30%	50%RaWFeatS	79.22	77.66	77.74	90.00	89.05	87.08G2G	80.70	76.67	76.31	62.35	56.19	48.46GraPhSAGE	82.05	81.32	79.68	71.48	69.33	64.92CADE-MS	84.25	83.40	81.74	77.35	73.71	70.88CADE-MA	84.56	83.03	82.40	84.33	82.21	79.04Comparation on node classification performance on Pubmed and Blogcatalog dataset with respect tovarying ratios of unseen nodes, are reported in Table 2. CADE-MS and CADE-MA outperform otherapproaches on Pubmed. On Blogcatalog dataset, however, RawFeats performs best mainly becausethat, in Blogcatalog dataset, node features are not only directly extracted from a set of user-definedtags, but also are of very high dimensionality (up to 8,189). Hence extra neighborhood information isnot needed. As shown in Table 2, CADE-MA performs better than CADE-MS, and both outperformGraphSAGE and G2G. CADE-MA is capable of reducing high dimensionality while losing lessinformation than CADE-MS, CADE-MA is more likely to search for the best aggregator functionthat can focus on those important features of nodes. As a result, the 256-dimensional embeddinglearnt by CADE-MA shows the cloest node classification performance to the 8k-dimensional rawfeatures.
Table 3: Link prediction results for Pubmed/PPI w.r.t different percentage of hidden-edgesDataset	Methods	90%:10%		80%20%		60%:40%		40%:60%			AUC	AP	AUC	AP	AUC	AP	AUC	AP	RawFeats	57.61	54.72	58.51	56.19	54.47	52.82	52.41	50.77	G2G	64.13	68.60	63.52	65.15	60.03	66.16	58.97	61.17Pubmed	GraphSAGE	85.49	82.79	87.64	83.35	81.07	77.47	79.34	74.92	CADE-MS	89.95	88.79	90.36	86.67	87.14	83.77	84.76	79.53	CADE-MA	89.73	89.76	90.94	88.90	90.54	87.89	85.27	80.15	RawFeats	57.46	56.99	57.34	56.86	57.35	56.75	56.83	56.36PPI	G2G	60.62	58.98	60.99	59.38	61.05	59.54	60.93	59.49	GraphSAGE	82.74	81.20	82.21	80.66	82.11	80.51	82.07	80.70	CADE-MS	85.87	85.08	84.21	83.48	84.46	82.84	83.61	82.39	CADE-MA	86.33	85.32	85.85	84.63	84.15	82.15	81.98	79.54Comparation on performance with respect to varying percentage of hidden edges are reported inTable3. CADE shows best link prediction performance on both datasets.
Table 4: node classification performance (f1-micro score) with different way of applying global biasmethod	F1-microGraPhSAGE	50.22CADE-gl	39.24CADE-gb	54.13CADE	58.225 ConclusionIn this paper, we proposed CADE, an unsupervised and inductive network embedding approachwhich is capable of preserving local connectiveness distinctively , as well as learning and memoriz-ing global identities for seen nodes while generalizing to unseen nodes. We applied a bi-attentionarchiteture upon hierarchical aggregating layers to capture the most relevant representations duallyfor any positive pair. We also present an effective way of combining inductive and transductive ideasby allowing trainable global embedding bias to be retrieved in hidden layers within the hierarchi-cal aggregating framework. Experiments demonstrate the superiority of CADE over the state-of-artbaselines on unsupervised and inductive tasks. In the future, we would explore several possibili-ties, such as expanding dual encoding from pair-wise to n-wise, or using dual encoding frameworkin supervised embedding learning, or combing dual encoding with G2G by learning distributionrepresentations dually for positive pairs.
