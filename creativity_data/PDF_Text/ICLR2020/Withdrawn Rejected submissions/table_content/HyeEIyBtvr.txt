Table 1: BetaNet-A compared with the state-of-the-art methods under comparable flops.
Table 2: BetaNet-B compared with the state-of-the-art methods under comparable GPU latency.				Method	Top1 (%)	Top5 (%)	GPU Latency (ms)	search cost (GPU hours)MobileNet V2 (Sandler et al., 2018)	72.0	91.0	6.1	-ShUffleNetV2 (1.5) (Ma et al., 2018)	72.6	-	7.3	-PrOxyLeSSNAS-gpu (Cai et al., 2018)	75.1	92.5	5.1	200MNASNet (Tan et al., 2018)	74.0	91.8	6.1	40,000BetaNet-B	75.8	92.8	6.2	160Searched Archtectures As shown in Fig 3, BetaNet-A and BetaNet-B are searched with flops andGPU latency limitation respectively. BetaNet-A tends to select operators with lower flops at frontlayers where feature maps are large and operators with higher flops elsewhere to enhance the ability.
Table 3: Comparison with the other weight sharing methods on their strategies.
