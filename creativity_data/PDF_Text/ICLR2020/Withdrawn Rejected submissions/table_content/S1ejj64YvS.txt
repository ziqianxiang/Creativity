Table 1: One-stage VAE error rate in MNIST and SVHN.
Table 2: Error rate in Cifar10. f denotes the best semi-supervised generative approach result. ∣denotes the model ensemble two categories. * denotes the corresponding backbone is not exactlyWideResNet (Zagoruyko & Komodakis, 2016), but belongs to one kind of its variations with acomparable amount of parameters.
Table 3: Error in Cifar100. f and * have the same meaning as described in Table 2.
Table 4: Ablation study with SVHN, Cifar10, and Cifar100.
Table 5: Generative performance measured by ELBO with ELBO ≤ log p(X)Model	Cifar10	Cifar100-Pure VAE	-226.25(±14.25)~^^-1292.91(±1.10)OSPOT-VAE	-237.62(±6.27)	-1271.82(±24.15)4.3 generative performanceEpemp(X)ELBO measures the margin between the true data distribution and the distribution learnedby generation models (Doersch, 2016). By comparing the Epemp(X) value of pure unsupervised VAEand our semi-supervised VAE model under the same “WideResNet-28-2” backbone, we demonstratethat good generative models and semi-supervised results can be obtained at the same time in OSPOT-VAE. The results in Table 5 show that the data generative distribution learned by our OSPOT-VAEmodel is as good as the pure VAE model. Further generated results are available in Appendix A.7.
Table 6: Schedule ParametersMNIST			SVHN(one-stage)		SVHN		Cifar10		Cifar100		hmax	tmax	hmax	tmax	hmax	tmax	hmax	tmax	hmax	tmaxβz	30	50	1	175	1e-3	150	1e-3	150	1e-1	150βc	30	50	1	175	1	150	1e-3	150	1e-3	150Iz	17.5	50	50	175	1280	150	200	150	1280	150Ic	17	50	50	175	2.3	150	2.3	150	4.6	150wMz	\	\	\	\	1e-3	150	1e-3	150	1e-1	150wMc	\	\	\	\	1	400	1	280	1	280and the related KKT conditions are∂L(Π,t)	λ∏ι + (1 - λ)∏2-TTT--- = t----------z------- = 0∂π	πK	(A.5)t * (^X ∏i — 1)=0i=1Solve the equation (A.5), We can get the closed form of ∏ as∏ = λ∏ι + (1 — λ)∏2 □A.5 Schedule AnalysisThe Warm-up scheduler aims to sloWly increase the parameters until they reach their maximum. For
Table 7: Details of Training Process	MNIST	SVHN(one-stage)	SVHN	Cifar10	Cifar100Latent Dim(Z/c)	10/10	32/10	128/10	128/10	128/100Mutual Info(Z/c)	17.5/17.0	50/50	1280/2.3	200/2.3	1280/4.6loss of - logpθ(X|c, Z)	BCE	BCE	MSE	MSE	BCEα of pmixup (X)	\	\	2	2	2optimizer		Adam		SGD	learning rate	5e-4	1e-3		0.1	lr scheduler(decay ratio)	\	\	every 50 epoch after 200-th(0.5)	[500,600,650](0.2)	weight decay	0	0		5e-4	A.6 Details of Training ProcessHere we list some important items need to set in the training process for different process. Weclassify these items into 2 categories: (1) items related to the loss function and (2) items related tothe optimization strategy. The details are as follows and we list the exact value in Table 7:1.	Items related to the loss function•	Latent dimThe latent dim of discrete variable c is the same as the number of classifications, thatis, K . For continuous variable z, the latent dim is determined by experiments.
