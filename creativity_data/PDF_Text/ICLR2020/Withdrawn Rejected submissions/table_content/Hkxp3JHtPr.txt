Table 1: Results for image datasets. We report the average and standard deviation of AUROC over90 experiments, for various ratios of labeled anomalies in the data γl .
Table 2: Results of the experiment where we increase the ratio of labeled anomalies γl in the trainingset, on the CatsVsDogs dataset.
Table 3: Results on classic AD benchmark datasets in the setting with a ratio of labeled anomaliesof γl = 0.01 in the training set. We report the avg. AUROC with st.dev. computed over 10 seeds.
Table 4: Ablative analysis of the Dual Prior method. AUROC is reported over average of 10 seedsfor the satellite and cardio. For CIFAR-10, results are reported for the experiment with 1% ofanomalies, averaged over 90 experiments.
Table 5: Complete results of the experiment where we increase the ratio of labeled anomalies γl in the training set. We report the avg. AUROC with st.dev.
Table 6: Complete results on classic AD benchmark datasets in the setting with a ratio of labeled anomalies of γl = 0.01 in the training set. We report the avg.
Table 7: Complete ablative analysis of the Dual Prior method. AUROC is reported over average of10 seeds for the satellite and cardio. For CIFAR-10, results are reported for the experiment with 1%of anomalies, averaged over 90 experiments.
Table 8: Hyper-parameters for the Dual Prior VAEData	ND Update Interval Learning Rate βKL	βCUBOMNIST	2	0.0005	0.005	0.005Fashion-MNIST	2	0.001	0.005	0.005CIFAR-10	2	0.001	0.005	0.005CatsVsDogs	1	0.0005	0.005	0.005arrhythmia	1	0.0005	0.5	0.5cardio	1	0.001	0.05	0.05satellite	1	0.001	0.05	0.05satimage-2	1	0.001	0.05	0.05shuttle	1	0.001	0.05	0.05thyroid	1	0.0001	0.05	0.05Table 9: Hyper-parameters for the Max-Min Likelihood VAEFor training, We follow Kaae S0nderby et al. (2016) recommendations for training VAEs and use a20-epoch annealing for the KL-divergence component, that is, the KL-divergence coefficient, βKL,is linearly increased from zero to its final value. Moreover, we allow the VAE to first learn a goodrepresentation of the normal data in a warm-up period of50 epochs, and then we begin applying thenovelty detection updates. For optimization, we use Adam (Kingma & Ba, 2014) with a learningrate schedule to stabilize training when outlier samples are fed after the warm-up period.
Table 9: Hyper-parameters for the Max-Min Likelihood VAEFor training, We follow Kaae S0nderby et al. (2016) recommendations for training VAEs and use a20-epoch annealing for the KL-divergence component, that is, the KL-divergence coefficient, βKL,is linearly increased from zero to its final value. Moreover, we allow the VAE to first learn a goodrepresentation of the normal data in a warm-up period of50 epochs, and then we begin applying thenovelty detection updates. For optimization, we use Adam (Kingma & Ba, 2014) with a learningrate schedule to stabilize training when outlier samples are fed after the warm-up period.
