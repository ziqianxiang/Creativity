Table 1: White-Box attack: Classification accuracy (%) of models trained on MNIST dataset usingdifferent training methods. For all attacks =0.3 is used and for PGD attack step is set to 0.01. Forthe proposed method, mean and standard-deviation of accuracy over three runs are reported. ForAblation-2 and Ablation-3, we generate noisy image by adding perturbation with l2 norm constraintequals to 4.
Table 2: White-Box attack: Classification accuracy (%) of models trained on CIFAR-10 datasetusing different training methods. For all attacks =8/255 is used and for PGD attack step is setto 2/255. For the proposed method, mean and standard-deviation of accuracy over three runs arereported. For Ablation-2 and Ablation-3, we generate noisy image by adding perturbation with l2norm equals to 0.2.
Table 3: Black-Box attack: Classificationaccuracy (%) of models trained on MNISTdataset using the proposed and PGD adver-sarial training methods, against FGSM attackwith =0.3. A normally trained model (sourcemodel) is used for generating adversarial sam-ples, and these samples are fed to the tar-get model. M represents MNIST-Network andsubscript denotes the training method.
Table 4: Black-Box attack: Classification accu-racy (%) of models trained on CIFAR-10 datasetusing the proposed and PGD adversarial trainingmethods, against FGSM attack with =8/255. Anormally trained model (source model) is usedfor generating adversarial samples, and thesesamples are fed to the target model. M repre-sents WideResNet-28-10 and subscript denotesthe training method.
Table 5: Spatial transformation attack: Performance of models trained on MNIST dataset usingdifferent training methods, against spatial transformation attack and perturbation attack i.e., PGDattack (=0.3, step=0.01, steps=100). For spatial transformation attack, (tx, ty, θ) are constrainedto (±3px, ±3px, ±30°).
Table 6: Network used for MNIST dataset. Net-A and Net-B are used for generating black-boxattacks.
Table 7: Performance of models trained on MNIST dataset using different training methods, againstDeepFool and C&W attack. Fooling rate (FR) defines the percentage of test set images that aremisclassified by the model, post attack. Mean l2 is the average l2 norm of perturbations generatedfor test set images. For robust models, Mean l2 should be large i.e., perturbation with large l2 normis required to fool the classifier. Note that for models trained using PGD method and the proposedmethod, perturbations with large l2 norm is required to fool the classifier.
Table 8: Performance of models trained on CIFAR-10 dataset using different training methods,against DeepFool and C&W attack. Fooling rate (FR) defines the percentage of test set imagesthat are misclassified by the model, post attack. Mean l2 is the average l2 norm of perturbationsgenerated for test set images. For robust models, Mean l2 should be large i.e., perturbation withlarge l2 norm is required to fool the classifier. Note that for models trained using PGD method andthe proposed method, perturbations with large l2 norm is required to fool the classifier.
Table 9: White-Box attack: Classification accuracy (%) of models trained on fashion-MNIST datasetusing different training methods. For all attacks =0.1 is used and for PGD attack step is set to 0.01.
Table 10: Black-Box attack: Target models trained on Fashion MNIST dataset. The FGSM adver-sarial samples ( = 0.1) required for the attack are generated from undefended pre-trained Net Aand B (refer table 6). For the target model, subscript denotes the training method and M representsMNIST-Network table (refer table 6)Source Model	Target Model		MPGD	MP roposedNet-A	84.5	80.64Net-B	83.58	77.52Figure 5: Plot of accuracy on test set versus per-turbation strength () of PGD attack obtained formodels trained on Fashion-MNIST dataset us-ing different training methods. PGD attack withsteps=40 is used.
