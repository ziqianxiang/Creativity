Table 1: Task-specific model information of BERTBASE (parameter count 109M).
Table 2: Fine-tuned parameters are L1-close and angular-close to pre-trained ones. We comparemeasured distance metrics with expected distances between two independent random initializations,either uniformly or normally distributed from 一 册 to √H where H = 768 is the hidden dimension.
Table 3: L0 -close fine-tuning results: layers excluded from fine-tuning, corresponding number ofparameters remaining to fine-tune, and the fine-tuning performance on the MRPC task (F1 score).
Table 4: Comparison between supermask pruned weights and magnitude-based pruned weights.
Table 5: Low-sparsity supermask performance. We report the sparsity levels achieved when thesupermasks were initialized at 0% sparsity. For several tasks, fine-tuning is achieved with less than3% of pre-trained weights pruned. For the supermask evaluation results, we include the mean andstandard deviation of 10 Bernoulli samplings of a single run.
Table 6: MRPC low-sparsity supermask performance at learning rates from 2 × 10-5 and 2 × 10-1.
