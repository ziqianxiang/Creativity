Table 1: Accuracy results on CIFAR-10 when quantizing either weights or activations or both.
Table 2: Accuracy results on SVHN when quantizing weights, activations, or both. On VGG-7*,MCQ shows minimal accuracy loss when quantizing both weights and activations and close to noaccuracy loss when not quantizing the first layer. For models A, B, C, and D the accuracy lowers asthe model size decreases. Quantizing only the activations barely lowers the baseline accuracy.
Table 3: Accuracy results on ImageNet when quantizing weights, activations, or both. Whenquantizing weights only, accuracy drops less than 1% in all tested models. Quantizing only theactivations generally leads to a lower accuracy loss compared to quantizing weights. Quantizing bothweights and activations leads to an additional accuracy loss of 0.6% in the worst case, i.e. ResNet-50.
Table 4: Evaluation of MCQ on language modeling, speech recognition, and machine translation. Allquantized models reach close to full precision performance. Note that, as opposed to the imageclassification task, we did not study different sampling amounts nor the effect of quantization onspecific network layers. A more in-depth analysis could then help to achieve close to full-precisionaccuracy at a lower bit-width on these additional models.
