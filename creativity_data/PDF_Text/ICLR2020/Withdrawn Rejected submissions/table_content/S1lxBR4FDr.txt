Table 1: White-Box setting: Classification ac-curacy (%) of models trained on MNIST datasetusing different training methods. For all attacks=0.3 is used and for PGD attack step=0.01 isused. For both IFGSM and PGD attacks steps isset to 40.
Table 2: White-Box attack: Classification accu-racy (%) of models trained on Fashion-MNISTdataset using different training methods. Forall attacks =0.1 is used and for PGD attackstep=0.01 is used. For both IFGSM and PGDattacks steps is set to 40.
Table 3: White-Box attack: Classification accuracy (%) ofmodels trained on CIFAR-10 dataset using different train-ing methods. For all attacks =8/255 is used and for PGDattack step =2/255 is used. For both IFGSM and PGD at-tacks steps is set to 7.
Table 4: Comparison of training timeper epoch of models trained on MNISTand CIFAR-10 datasets respectively,obtained for different training meth-ods. For PAT, steps=40 is used forMNIST dataset and steps=7 is used forCIFAR-10 dataset. f For EAT, trainingtime of pre-trained source models arenot considered.
Table 5: Black-box setting: Performance of models trained on MNIST, Fashion MNIST and CIFAR-10 datasets using different training method, against adversarial attacks in black-box setting. Sourcemodels are used for generating adversarial samples, and the target models are tested on these gener-ated adversarial samples.
Table 6: Performance of models trained using different training methods against DeepFool andC&W attacks. These attack methods measure the robustness of the model based on the average l2norm of the generated perturbations, higher the better. Success defines the percentage of samples oftest set that has been misclassified. Note that, for models trained using PAT and SADS, perturbationswith relatively large l2 norm is required to fool the classifier.
Table 7: Setup used for Ensemble Adversarial Training. For MNIST and Fashion-MNIST networksrefer table 8.
Table 8: Architecture of networks used for Ensemble Adversarial Training on MNIST and Fashion-MNIST datasets.
