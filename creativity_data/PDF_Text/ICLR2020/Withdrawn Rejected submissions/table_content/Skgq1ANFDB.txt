Table 1: A summary of various primal and dual concepts used in the paper. f denotes the function ofthe decision boundary, i.e. z(yL) - zt(L) where y is the true label and t is the attack target. m and Mare lower and upper bounds on the smallest and largest eigenvalues of the Hessian of f , respectively.
Table 2: Comparison of methods for providing provable robustness certification. Note that Cohenet al. (2019) is a probabilistic certificate.
Table 3: Comparison between CROWN-general (Zhang et al., 2018a) and CRC. Note that bothCROWN and CRC are computed on CPU. However, running time numbers are not directly comparablebecause our CRC implementation uses a batch of images while the CROWN implementation uses asingle image at a time.
Table 4: Comparison between CRT, PGD (Madry et al., 2018) and TRADES (Zhang et al., 2019).
Table 5: Comparison between CRT (left table) and Convex Outer Adversarial Polytope (right table)(Wong et al., 2018) with attack radius ρ = 1.58.
Table 6: Results for CIFAR-10 dataset (only curvature regularization, no CRT training)Corollary 1. Both Curvature-based Certificate and Attack Optimizations are convex optimizationproblems within an l2 ball of radius ρ around the input sample where curvature values are bounded.
Table 7: Comparison between Certified Robust accuracy and CRC for 2 layer sigmoid and tanhnetworks using global and local curvature boundsFor deeper networks, however, computing local curvature bounds is more challenging. We leaveexploring this direction for the future work.
Table 8: Comparison between certified robust accuracy for different values of the regularizationparameter γ for a single hidden layer convolutional neural network with softplus activation function33Under review as a conference paper at ICLR 2020G.2 Results for Tanh networksNetwork	Training	Standard Accuracy	Empirical Robust Accuracy	Certified Robust Accuracy	Certificate (mean)						CROWN	CRC2×[1024], tanh	PGD	98.76%	95.79%	84.11%	0.30833	0.61340	TRADES	98.63%	96.20%	93.72%	0.40601	0.86287	CRT, 0.01	98.52%	95.90%	95.00%	0.37691	1.470163×[1024], tanh	PGD	98.78%	94.92%	0.00%	0.12706	0.03036	TRADES	98.16%	94.78%	0.00%	0.15875	0.02983	CRT, 0.01	98.15%	95.00%	94.16%	0.28004	1.149954×[1024], tanh	PGD	98.53%	94.53%	0.00%	0.07439	0.00140	TRADES	97.08%	92.85%	0.00%	0.11889	0.00068	CRT, 0.01	97.24%	93.05%	91.37%	0.33649	0.93890Table 9: Comparison between CRT, PGD (Madry et al., 2018) and TRADES (Zhang et al., 2019) forTanh networks. CRC outperforms CROWN significantly for 2 layer networks and when trained withour regularizer for deeper networks. CRT outperforms TRADES and PGD giving higher certifiedaccuracy.
Table 9: Comparison between CRT, PGD (Madry et al., 2018) and TRADES (Zhang et al., 2019) forTanh networks. CRC outperforms CROWN significantly for 2 layer networks and when trained withour regularizer for deeper networks. CRT outperforms TRADES and PGD giving higher certifiedaccuracy.
Table 10: Comparison between CRT and Randomized Smoothing(Cohen et al., 2019). s denotes thestandard deviation for smoothing. We use ρ = 0.5. For CRT, we use γ = 0.01Network		Randomized Smoothing			CRT		S = 0.25	s = 0.50	s = 1.0	-2 ×	[1024], sigmoid	93.75%	93.09%	88.91%	95.61%~27	[1024], tanh	94.61%	93.08%	82.26%	95.00%3 ×	[1024], sigmoid	94.00%	93.03%	86.58%	94.99%3 ×	[1024], tanh	93.69%	91.68%	80.55%	94.16%^Z×	[1024], sigmoid	93.68%	92.45%	84.99%	93.41%^Z×	[1024], tanh	93.57%	92.19%	83.90%	91.37%34Under review as a conference paper at ICLR 2020Table 11: Comparison between CRC and CROWN-general (CROWN-Ada for relu) for differenttargets. For CRT training, we use γ = 0.01. We compare CRC with CROWN-general for differenttargets for 150 correctly classified images. Runner-up means class with second highest logit isconsidered as adversarial class. Random means any random class other than the label is consideredadversarial. Least means class with smallest logit is adversarial. For 2-layer networks, CRCoutperforms CROWN-general significantly even without adversarial training. For deeper networks (3and 4 layers), CRC works better on networks that are trained with curvature regularization.
Table 11: Comparison between CRC and CROWN-general (CROWN-Ada for relu) for differenttargets. For CRT training, we use γ = 0.01. We compare CRC with CROWN-general for differenttargets for 150 correctly classified images. Runner-up means class with second highest logit isconsidered as adversarial class. Random means any random class other than the label is consideredadversarial. Least means class with smallest logit is adversarial. For 2-layer networks, CRCoutperforms CROWN-general significantly even without adversarial training. For deeper networks (3and 4 layers), CRC works better on networks that are trained with curvature regularization.
Table 12: In this table, we measure the effect of increasing γ, when the network is trained withCRT on standard, empirical, certified robust accuracy, Klb and Kub (defined in subsection G.5) fordifferent depths (2, 3, 4 layer) and activations (sigmoid, tanh). We find that for all networks γ = 0.01works best. We find that the lower bound, Klb increases (for γ = 0) for deeper networks suggestingthat deep networks have higher curvature. Furthermore, for a given γ (say 0.005), we find that thegap between Kub and Klb increases as we increase the depth suggesting that K is not a tight boundfor deeper networks.
Table 13: In this table, we measure the impact of increasing curvature regularization (γ) on accuracy,empirical robust accuracy, certified robust accuracy, CROWN-general and CRC when the networkis trained without any adversarial training. We find that adding a very small amount of curvatureregularization has a minimal impact on the accuracy but significantly increases CRC. Increase inCROWN certificate is not of similar magnitude. Somewhat surprisingly, we observe that even withoutany adversarial training, we can get nontrivial certified accuracies of 84.73%, 88.66%, 89.61% on2,3,4 layer sigmoid networks respectively.
Table 14: Comparison between mean values of zy - zt for l2 bounded PGD and Curvature-BasedAttack (CBA) optimization. See the experimental details in Section G.6.
Table 15: Table showing attack success rates for different values of γ. Attack success rate denotes thefraction of points (X(0)) satisfying IX(attack) - X(0) I2 = ρ implying primal = dual in Theorem 2.
