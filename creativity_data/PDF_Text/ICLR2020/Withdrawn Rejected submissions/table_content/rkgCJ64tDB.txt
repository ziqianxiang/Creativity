Table 1: Classification accuracy on the SMNIST and SFashion dataset with and without batch-normalization.
Table 2: Network architectures used for the experiments in Section 5.2. cLxLxM’xM: a regular convolutionallayer With M' input channels, M output channels, and LxL spatial kernels. Sc(Na)LXLXM'xM: the first-layerconvolution operation (5) in ScDCFNet, where Nα is the number of the uniform grid points to discretize thescale interval [-1.6, 0], and LxL is the spatial kernel size on the largest scale α = 0. sc(Nα)LXLXLα XM’XM:the l-th layer (l > 1) convolution operation (6) in ScDCFNet, Where the extra symbol Lα stands for the filtersize in α. apLXL(sapLXL): the regular (ST -equivariant) LxL average-pooling. fcM: a fully connected layerWith M output channels. Batch-normalization layers are added to each convolutional layer if adopted duringtraining.
Table 3: Architectures of the auto-encoders used for the experiment in Section 5.3. The encoded representationis the output of the second layer. ctLXLXM’XM: transposed-convolutional layers With M’ input channels, Moutput channels, and LxL spatial kernels. us2X2: 2x2 spatial upsampling. See the caption of Table 2 for thedefinitions of other symbols. Batch-normalization (not shoWn in the table) is used after each convolutional layer.
