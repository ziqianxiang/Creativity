Table 1: Performance of our system against various baselines, which are evaluated using Accu-racy@{1, 3}. For all interacting baselines, 5 clarification questions are used. Best performances arein bold. We report the averaged results as well as the standard deviations from 3 independent runsfor each model variant and baseline.
Table 2: Human evaluation results. Count is the total number of interaction examples. The systemis evaluated with Accuracy@1 and the rationality score ranging from -2 (strongly disagree) to 2(strongly agree).
Table A.1: Target-tag annotation statistics. We show five sets of tags to the annotators. The higherranked ones are more likely to be related to the given target. The row mean # tags is the meannumber of tags that are annotated to a target, N.A. is the percentage of the tasks are annotated as”none of the above”, and mean κ is the mean pairwise Cohen’s κ score.
Table A.2: Comparison of the suggestion modules trained with different training data. Each modelis evaluated on three different tasks. First, use initial queries to predict targets. Second, use allattributes tags to predict targets; third, use both initial queries and tags as text input to predict targets.
Table A.3: Human evaluation results on FAQ Suggestion and Bird Identification on our proposedmodel and several baslines. The three FAQ systems ask 2.8, 3 and 3 turns of questions, respeCtively.
Table A.4: Examples of user interaCtions for FAQ suggestion human evaluation.
