Table 1: List of closely related methods. Note: OE uses OOD examples during training, but unrelated to testWe benchmark our algorithm with the works listed in Table 1 using the following metrics:1.	TNR@95TPR is the probability that an OOD (negative) example is correctly identifiedwhen the true positive rate (TPR) is as high as 95%. TPR can be computed as T PR =TP/(TP + F N), where TP and FN denote True Positive and False Negative respectively.
Table 2: Comparison of OOD Detection Performance for all combinations of model architecture and trainingdataset are shown. The hyperparameters of ODIN and the hyperparameters and parameters of Mahalanobis aretuned using a random sample of the OOD dataset.
Table 3: Comparison of results with OE (Hendrycks et al., 2019a). Since OE uses a different model from ours,we also report the corresponding baseline accuracy. We extract the mean TNR @ TPR95 for our technique byconsidering both ResNet and DenseNet models. Some more results are available in Appendix E.1.
Table 4: Detailed Ablation Results demonstrating the detection rates under 12 different settings. The MEANand STD-DEV are computed by using all elements in the table excepting the CIFAR-10 vs CIFAR-100 andCIFAR-100 vs CIFAR-10 entries. Note: We will of course fix the formatting of this for future versions; this isjust an initial draft during discussion period.
Table 5: Table shows results from preliminary experiments on combining OE with our method. The experimentwas conducted with pretrained WideResNet open-sourced by Hendrycks et al. (2019a). MSP uses MaximumSoftmax Probability; "Ours" refers to the metric ∆ (Eq. 5); "Ours+MSP" is obtained by dividing ∆ with MSP.
Table 6: Comparison of Mean TNR@TPR95 values.
Table 7: We compare our method with DPN, VD and Semantic by reporting results where available.
Table 8: The method even works quite well with a fully-connected neural network trained on MNIST. The resultsare shown for 300-unit single layer MLP, 300-150 two-layer MLP and 300-150-50 MLP.
