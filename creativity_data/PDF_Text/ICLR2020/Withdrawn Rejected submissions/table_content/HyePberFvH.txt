Table 1: Post-training Q-FP validation accuracy on CIFAR-10 when using K for model selectionrather than single-precision validation accuracy	MobileNet-v2		EfficientNet-b0	Precision	MCDA	(Wang et al., 2018)	MCDA	(Wang et al., 2018)(Weight, Act.)	(K =1.42)	(K =1.56)	(K =1.46)	(K =1.89)(32,32)	896	90.3	93.7	94.6(8,8)	88.5	86.7	90.0	89.0(6,6)	79.0	76.6	64.6	56.0(5,5)	48.4	40.8	22.0	18.6We then test their percentage validation accuracy decrease from using post-training quantization(i.e. no finetuning) with varying Q-FP precisions. In Figure 3, We see that the models with higherK values typically experience a larger drop in Q-FP accuracy, indicating they are more sensitive tofloating point rounding error. Notably, the model with lowest K for 7-bit MobileNet-v2 experiencesa lower percentage validation accuracy drop than three of the 8-bit models. In this case, MCDAmodel selection enables the saving of a bit of precision while achieving smaller accuracy decreasethan some of the trained 8-FP models.
