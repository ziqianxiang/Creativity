Table 1: Comparison between our method and various baselines. All methods use ten images per category (100in total), except for the average real images baseline, which reuses the same images in different GD steps. ForMNIST, our method uses 2000 GD steps, and baselines use the best among #steps âˆˆ {1, 100, 500, 1000, 2000}.
Table 2: Adapting models among MNIST (M), USPS (U), and SVHN (S) using 100 distilled images. Ourmethod outperforms few-shot domain adaptation (Motiian et al., 2017) and other baselines in most settings. Dueto computation limitations, the 100 distilled images are split into 10 minibatches applied in 10 sequential GDsteps, and the entire set of 100 distilled images is iterated through 3 times (30 GD steps in total). For baselines,we train the model using the same number of images with {1, 3, 5} times and report the best result.
Table 3: Adapting an AlexNet pre-trained on ImageNet to PASCAL-VOC and CUB-200. We use one distilledimage per category, repeatedly applied via three GD steps. Our method significantly outperforms the baselines.
Table 4: Continual learning results. Distilled images are trained with random Xavier Initialization distribution.
