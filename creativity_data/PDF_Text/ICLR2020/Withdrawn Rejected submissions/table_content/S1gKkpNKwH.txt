Table 1: Statistics for training chromatic networks. For mean reward, we take the average over 301 workerrollout-rewards for each step, and output the highest average over all timesteps. For max reward, we report themaximum reward ever obtained during the training process by any worker. “L”, “H41” and “H41, H41” standfor: linear policy, policy with one hidden layer of size 41 and policy with two such hidden layers respectively.
Table 2: Comparison of the best policies from five distinct classes of RL networks: chromatic (ours), masked(networks from Subsection 4.1), Toeplitz networks from (Choromanski et al., 2018), circulant networks andunstructured trained with standard ES algorithm (Salimans et al., 2017). All results are for feedforward netswith one hidden layer of size h = 41.
