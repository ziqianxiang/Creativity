Table 1: Negative log-likelihood and mean squared errorTraining set	1-3 objects		2-4 objects		3-5 objects	Metrics	NLL	MSE	NLL	MSE	NLL	MSEROOTS	0.9199	15.16	0.9205	25.37	0.9213	29.82GQN	0.9196	15.28	0.9201	26.86	0.9204	32.66Object Detection Quality. In this section, we only provide precision and recall results as objectdetection evaluation of ROOTS as GQN cannot detect objects. To estimate the true positive pre-diction, we first filter out predictions that are too far away from any ground truth by applying a8Under review as a conference paper at ICLR 2020汽宇F区再证可不片■国摩豆C七受FFigure 6:	A novel scene consisted of 9 objects is composited by ROOTS trained on 1-3 objects dataset.
Table 2: Precision and RecallTraining set	1-3 objects			2-4 objects			3-5 objects		Threshold	0.1	0.15	0.35	0.1	0.15	0.35	0.1	0.15	0.35Precision	76.82	91.56	98.68	66.76	86.07	98.29	66.64	86.09	96.65Recall	75.56	90.15	97.40	65.53	84.57	95.93	66.91	86.47	97.10Count Acc.		93.18			88.16			82.21	6 ConclusionWe proposed ROOTS, a probabilistic generative model for unsupervised learning of 3D scene rep-resentation and rendering. ROOTS can learn object-oriented interpretable and hierarchical 3D scenerepresentation. In experiments, we showed the generation, decomposition, and detection ability ofROOTS. For compositionality and transferability, we also showed that, due to the factorization ofstructured representation, new scenes can be easily built up by reusing components from 3D scenes.
Table 3: Quantitative generalization results on generationTraining set	1-3 Objects			2-4 Objects			3-5 Objects		Testing set	1-3 Objects	2-4 Objects	3-5 Objects	1-3 Objects	2-4 Objects	3-5 Objects	1-3 Objects	2-4 Objects	3-5 ObjectsGQN	15:57	27:49	43:57	1644	2677	3762	15:02	2373	3338ROOTS	14.72	35.22	71:43	13.97	25:35	42:21	11:48	19:62	29:88Table 4: Quantitative generalization results on object detectionTraining set	1-3 Objects			2-4 Objects			3-5 Objects		Testing set	1-3 Objects	2-4 Objects	3-5 Objects	1-3 Objects	2-4 Objects	3-5 Objects	1-3 Objects	2-4 Objects	3-5 ObjectsPrecision	9143	83:25	7181	9036	8655	80:83	9324	907Γ0	85:91Recall	89:84	80:02	65:67	89:22	84:68	78:29	93:66	89:94	85:91Count Acc:	93:07	81:04	57:91	93∙17	87:42	79:42	92:57	88:14	82:17A.2 Composing with more objectsIn this section, we provide one more example of compositionality. We first train ROOTS on 1-3objects dataset. As in Figure 8, during testing, we collect object representations from 7 differentscenes and reuse them to composite a new scene with 9 objects. Additionally, the sampled centerposition of each object and query viewpoints (represented as camera position, pitch, and roll) fedinto ROOTS are with respect to canonical coordinates. Note that, to predict the scale of objects in2D projection correctly (e.g., for the same object, the larger the distance between it and viewpoint is,the smaller that object should be in the 2D projection), ROOTS learns to infer local depth (positiontranslation) for each object given specific query viewpoint. This can be observed on the yellow
Table 4: Quantitative generalization results on object detectionTraining set	1-3 Objects			2-4 Objects			3-5 Objects		Testing set	1-3 Objects	2-4 Objects	3-5 Objects	1-3 Objects	2-4 Objects	3-5 Objects	1-3 Objects	2-4 Objects	3-5 ObjectsPrecision	9143	83:25	7181	9036	8655	80:83	9324	907Γ0	85:91Recall	89:84	80:02	65:67	89:22	84:68	78:29	93:66	89:94	85:91Count Acc:	93:07	81:04	57:91	93∙17	87:42	79:42	92:57	88:14	82:17A.2 Composing with more objectsIn this section, we provide one more example of compositionality. We first train ROOTS on 1-3objects dataset. As in Figure 8, during testing, we collect object representations from 7 differentscenes and reuse them to composite a new scene with 9 objects. Additionally, the sampled centerposition of each object and query viewpoints (represented as camera position, pitch, and roll) fedinto ROOTS are with respect to canonical coordinates. Note that, to predict the scale of objects in2D projection correctly (e.g., for the same object, the larger the distance between it and viewpoint is,the smaller that object should be in the 2D projection), ROOTS learns to infer local depth (positiontranslation) for each object given specific query viewpoint. This can be observed on the yellowsphere, green cylinder and blue cube in the bottom row in Figure 8.
Table 5: Configuration of Sufficient Statistic NetworksLatent Variable	Channel Numbers	K	Draw Rollouts	ConcateZnos	-[128,128, 64, 32, 3]-	丁	2	last stepDWhat 	Zn ,		-[128, 128,64, 32, 4]	ɪ	4	concatenatesScαle 	sq,n		-[128, 128, 64, 32, 2]	ɪ	4	last steppres 	zn		-[256, 256,128, 64,1]-	丁	-	-GPreS 	Sq,n		[271,256,128, 64, 32,1]	ɪ	-	-Zbg	[128,4]	丁	2	—	last stepGeneration Process Superscript on ConvDRAW is used to indicate which variable the Con-vDRAW module is responsible for. The Renderer module is implemented in the same way asConvDRAW with one difference that we do not model any variable in Renderer, making it a de-terministic decoder. All the ConvDRAW modules have a hidden state size of 128. We use STdenote spatial transformation process and ST-1 denotes the reversed process. K denote the indexset of context C and θ and φ are neural network parameters.
