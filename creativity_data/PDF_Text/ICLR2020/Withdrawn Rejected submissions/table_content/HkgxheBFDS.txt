Table 1: Example adversarial questions ( original, (attack ), together with their annotation as eithera valid counterexample or other type. Top: Named entity perturbations. Bottom: PoS perturbations.
Table 2: Analysis of undersensitivity attack sam-ples for both PoS and named entity perturbations.
Table 3: Breakdown of undersensitivity error rate overall (lower is better), and standard performancemetrics (EM, F1 ; higher is better) on different subsets of SQuAD2.0 evaluation data, all in [%].
Table 4: Breakdown of undersensitivity error rate overall (lower is better), and standard performancemetrics (EM, F1 ; higher is better) on different subsets of NewsQA evaluation data, all in [%].
Table 5: Robust training leads to improved generalisation under train/test distribution mismatch(data bias, top). Bottom: control experiment without train/test mismatch.
Table 6: Breakdown of undersensitivity error rate on SQuAD2.0 with a held-out attack space (loweris better).
Table 7: Breakdown of undersensitivity error rate on NewsQA with a held-out attack space (loweris better).
Table 8: Drastic example for lack of specificity: unrelated questions can trigger the same prediction(here: Catholic orthodoxy), and even with higher probability.
Table 9: Robust training leads to improved generalisation under train/test distribution mismatch(data bias).
Table 10: Comparison between BERT Large and BERT Large + Robust Training on two setsof adversarial examples: ADDSENT and ADDONESENT from Jia & Liang (2017) - results frommodels different from BERT are from Huang et al. (2018), GQA from (Lewis & Fan, 2019). S:single model; E: ensemble model.
Table 11: Example adversarial questions ( original, Iattack ), together with their annotation as eithera valid counterexample or other type. Top: Named entity perturbations. Bottom: PoS perturbations.
