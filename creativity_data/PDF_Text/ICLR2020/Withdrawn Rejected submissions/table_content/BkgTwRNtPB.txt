Table 1: box size update through rotationRotation	0	1	2	3	4	5l0,w0,h0	l,w,h	l,h,w	w,l,h	w,h,l	h,l,w	h,w,lthat contains a Multi-Head Attention (MHA) layer and FeedForward (FF) layer. Each layer adds aresidual connection (He et al., 2016) and batch normalization (Ioffe & Szegedy, 2015). We feed theembedded vector to conditional query decoder introduced later. The decoder chooses the packingbox from unpacked boxes and the rotation, coordinates of the packing box. After finishing one steppacking, we update the input state of encoder according the decoding result of last packing step,more specifically, set the packed state sp,i to True and update the last packed box shape (li , wi , hi)according to the box rotation as shown in Table 1 and replace the masked position with the packedbox location (xi, yi, zi) as shown in Fig. 1.
Table 2: The gap ratio of conditional query learning (CQL) and baselinesTable 2 shows the bin gap ratios and its variances on 512 test instances after 100 epochs trainingfor learning algorithm. We use the default settings for genetic algorithm. The results show that ourCQL model achieves low bin gap ratio and variance in most cases. It is clear that the model withconditional query mechanism is better than no query model, which justifies that the CQL fills the gapbetween sub-actions and makes the learning algorithm has the ability to reason following sub-actionaccording the embedded of previous outputs. Meanwhile, The rollout baseline with REINFORCEproduces similar results as no query model but with higher variance. Because the rollout method9Under review as a conference paper at ICLR 2020sums up all reward signal in every packing steps as the action value, its learning process treat everypacking step equally and back propagate gradients for all steps no matter the step is good or bad.
