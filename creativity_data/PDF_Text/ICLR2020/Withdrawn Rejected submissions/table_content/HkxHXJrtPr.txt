Table 1: Test set results for Twitter POS tagging and NER of several SPEN configurations. Resultswith * correspond to the setting of Tu & Gimpel (2018).
Table 2: Test set results for Twitter POS tagging and NER. |T | is the number of trained parameters;|I | is the number of parameters needed during the inference procedure. Training speeds (exam-ples/second) are shown for joint parameterizations to compare them in terms of efficiency. Bestsetting (highest performance with fewest parameters and fastest training) is in boldface.
Table 3: Left: differences in accuracy/F1 between test-time inference networks AΨ and cost-augmented networks FΦ (on development sets). The “margin-rescaled” row uses a SPEN with thelocal CE term and without zero truncation, where AΨ is obtained by fine-tuning FΦ as done by Tu &Gimpel (2018). Right: most frequent output differences between AΨ and FΦ on the development set.
Table 4: NER test F1 scores with global energy terms. ∣: We took the best configuration fromNER/NER+ and evaluated it in the NER++ setting.
