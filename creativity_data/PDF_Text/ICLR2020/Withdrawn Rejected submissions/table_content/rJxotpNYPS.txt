Table 1: Comparison with other state-of-the-art domain generalization methods. Methods in the firsthalf of the table (until the vertical line) use only labeled data. The second half of the table showsresults of DIVA when trained semi-supervised (+ X times the amount of unlabeled data). We reportthe average and standard error of the classification accuracy.
Table 2: Comparison of DIVA trained supervised to DIVA trained semi-supervised with additionalunlabeled data from M30。 and M60。. We report the average and standard error of the classificationaccuracy on M75。 .
Table 3: Results of the supervised experiments for the first part of domains. We report the averageand standard error of ROC AUC.
Table 4:	Results of the supervised experiments for the second part of domains. As well as the averageacross all domains. We report the average and standard error of ROC AUC.
Table 5:	Results of the semi-supervised experiments for domain C116P77. Comparison of baselinemethod, DA and DIVA trained with labeled data from domain C59P20, unlabeled data from domainC59P20 and no data from domain C59P20. We report the average and standard error of ROC AUC.
Table 6: Architecture for pθ(x∣Zd, Zχ, Zy). The parameter for Linear is output features. The parame-ters for ConvTranspose2d are output channels and kernel size. The parameter for Upsample is theupsampling factor. The parameters for Conv2d are output channels and kernel size.
Table 7: Architecture for pθd (zd|d) and pθy (zy|y). Each network has two heads one for the meanand one for the scale. The parameter for Linear is output features.
Table 8: Architecture for qφd (zd|x), qφx (zx|x) and qφy (zy |x). Each network has two heads one forthe mean one and for the scale. The parameters for Conv2d are output channels and kernel size. Theparameters for MaxPool2d are kernel size and stride. The parameter for Linear is output features.
Table 9: Architecture for qωd (d|zd) and qωy (y|zy). The parameter for Linear is output features.
Table 10: Comparison of DIVA with a VAE with a single latent space, a standard Gaussian prior andtwo auxillary tasks on Rotated MNIST. We report the average and standard error of the classificationaccuracy.
Table 11: Results of ablation study.
Table 12: Architecture for pθ(x∣Zd, Zχ, Zy). The parameter for Linear is output features. Theparameters for ResidualConvTranspose2d are output channels and kernel size. The parameters forConv2d are output channels and kernel size.
Table 13: Architecture for pθd (zd|d) and pθy (Zy |y). Each network has two heads one for the meanand one for the scale. The parameter for Linear is output features.
Table 14:	Architecture for q®&(zd|x), qφx (zχ∣x) and qφy (zy |x). Each network has two heads onefor the mean one and for the scale. The parameters for Conv2d are output channels and kernel size.
Table 15:	Architecture for qωd (d∣Zd) and qωy (y∣Zy). The parameter for Linear is output features.
Table 16: Prediction of y using a 2 layer MLP trained using zd, zx and zy . We report the mean andstandard error of the classification accuracy on the hold out test domain.
