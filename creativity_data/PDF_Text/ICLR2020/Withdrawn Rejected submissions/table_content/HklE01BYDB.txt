Table 1: A comparison over 6 DMC tasks of SAC from pixels, PlaNet, SLAC, and an upper bound of SACfrom proprioceptive state, numbers are averaged over the last 5 episodes across 10 seeds. The large performancegap between SAC:pixel and SAC:state motivates us to address the representation learning bottleneck in model-free off-policy RL. Best performance bolded.
Table 2: We list specifications of observation space O (proprioceptive and image-based), actionspace A, and the reward type for each task.
Table 3: Action repeat parameter used per task, following PlaNet and SLAC.
Table 4: A complete overview of used hyper parameters.
