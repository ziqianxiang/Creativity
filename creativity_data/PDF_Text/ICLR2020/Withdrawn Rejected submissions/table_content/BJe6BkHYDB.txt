Table 1: Hardware-aware search space for each mobile hardware. For layer at 1-16, it contains 4operators for selection, for layer 17-20, each layer has 5 operators.
Table 2: Comparisons with various state-of-the-art efficient NAS on ImageNet. We measure all themodel latency on our hardware platforms. *: For fair comparison, We use the block search modelinstead of the block search+ channel search. *: In the form "x(y)”，where "x" means the trainingtime reduction and ”y” means the accuracy achieved.
Table 3: Comparisons with Singlepath-Oneshot by different search spaces. We list out the trainingiterations on ImageNet (batchsize=1,024) and OUI (batchsize=64) for search cost comparison.
Table 4: Supernet architecture. Column-”Block” denotes the block type. ”TBS” means layer typeneeds to be searched. The ”stride” column represents the stride of the first block in each repeatedgroup. In our paper, we search the operations for total 20 layers.
