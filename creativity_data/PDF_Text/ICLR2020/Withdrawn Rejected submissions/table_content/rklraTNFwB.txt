Table 1: Recent work on instruction-following in 3D environments (not exhaustive). By transfer-learning from simulation, our best-performing agents overcome unique combination of policy-optimization challenges (partial observability, memory, raw perception, object manipulation andrelations) and can also interpret natural human commands.
Table 2: Example training (left) and test instruc-tions in the lifting task.
Table 3: Example training (left) and test instructions in the putting task. Underlined words areexamples of synonyms, italics indicate entire phrases provided by human annotators.
Table 4: Accuracy of different models on training instructions, the synonym evaluation and thenatural referring expression evaluation. MP: mean pool, SA: self-attention layer, CMSA: cross-modal self-attention layer, TN: typo-noise. Multitask: single agent trained on both ’lifting’ and’putting’ tasks. Scores show mean across 1,000 episodes (with instructions randomly chosen by theenvironment generator).
Table 5: Accuracy of different models on the putting task when different parts of instructions of theform "Put a [D.O.] on the [I.O.]" are replaced with synonyms. Underlined words in modelnames indicate pre-trained weights from text-based training. MP: mean pool, SA: self-attentionlayer, CMSA: cross-modal self-attention layer, TN: typo-noise. Multitask: single agent trained onboth ’lifting’ and ’putting’ tasks. The evaluation task covers episodes in which [I.O.] is either‘bed’ or ‘tray’. Scores show mean across 1,000 random episodes.
Table 6: The 80 ShapeNet category names and their sizes, corresponding WordNet synsets and anexample model id for a category exemplar used i1n7the lifting task. In the extra materials we providea full list of ShapeNet model ids in each category.
Table 7: The two immovable and ten movable objects in the putting experiment. To begin eachepisode, both immovable objects and three randomly-selected movable objects are randomly posi-tioned in the room.
