Table 1: Comparison of test accuracy (%) and F1-score with state-of-the-art multiview classifiersI Caltech101-20 (20 classes) ∣ Reuters (6 classes) ∣ NUS-WIDE-OBJ (31 classes)		Accuracy	FI-SCore	Accuracy	F1-score	Accuracy	F1-scoreDCCA	62.37 ± 0.43	46.40 ± 0.42	47.62 ± 0.57	46.49 ± 0.44	17.21 ± 0.19	13.11 ± 0.28DGCCA	75.18 ± 0.23	55.51 ± 0.78	73.12 ± 0.28	70.29 ± 0.59	21.76 ± 0.22	15.08 ± 0.30DCCAE	58.43 ± 0.31	43.36 ± 0.52	50.28 ± 0.43	49.55 ± 0.38	20.07 ± 0.20	14.93 ± 0.13ProxNet	86.56 ± 0.32	69.87 ± 0.65 J	85.14 ± 0.15	83.32 ± 0.29	23.88 ± 0.25	16.43 ± 0.32(a) Representation from DGCCA(b) Representation before prox-map (c) Representation after prox-mapFigure 4: t-SNE embedding of the extracted features from test set of Reuters (best viewed in color)Visualization of representation. To further understand the superior performance of ProxNet andto examine the quality of the learned representation, we plotted in Figure 4 the t-SNE embedding(van der Maaten & Hinton, 2008) of the extracted features for Reuters before and after the proximalmapping, using test set images. This allowed us to examine the effect of proximal mapping.
Table 2: Spearman’s correlation for word similarity	WS-353		WS-SIM		WS-REL		SimLex999		EN	DE	EN	DE	EN	DE	EN	DEBaseline	73.35	52.68	77.84	63.34	67.66	44.24	37.15	29.09linearCCA	73.79	68.45	76.06	73.02	67.01	62.95	37.84	43.34DCCA	73.86	69.09	78.69	74.13	66.57	64.66	38.78	43.29DCCAE	72.39	69.67	75.74	74.65	65.96	64.20	36.72	41.81ProxNet	75.38	69.19	78.28	75.40	70.97	66.81	39.99	44.23CL-DEPEMB	-	-	-	-	-	-	35.60	30.60I #train		length	I LSTM	AdvLSTM	ProxLSTMJV	225	15	94.02 ±0.72	94.96 ±0.44	95.52 ±0.63HAR	6.1k	128	89.75 ±0.59	92.01 ±0.18	92.08 ±0.23AD	5.5k	39	96.32 ±0.55	97.45 ±0.38	97.99 ±0.29IMDB	25k	239	92.65 ±0.04	93.65 ±0.03	94.16 ±0.11Table 3: Test accuracy for sequence classificationFigure 5: t-SNE embedding of theHAR dataset (best viewed in color)Results. Clearly, ProxNet always achieves the highest or close to highest Spearman’s correlationon all test sets and for both English (EN) and German (DE). We also included a baseline whichonly uses the monolingual word vectors. CL-DEPEMB is from Vulic (2017), and the paper only
Table 3: Test accuracy for sequence classificationFigure 5: t-SNE embedding of theHAR dataset (best viewed in color)Results. Clearly, ProxNet always achieves the highest or close to highest Spearman’s correlationon all test sets and for both English (EN) and German (DE). We also included a baseline whichonly uses the monolingual word vectors. CL-DEPEMB is from Vulic (2017), and the paper onlyprovided the results for SimLex999 with no code made available. It can be observed from Table 2that multiview based methods achieved more significant improvement over the baseline on Germandata than on English data. This is not surprising, because the presence of multiple views offers anopportunity to transfer useful information from other views/languages. Since the performance onEnglish is generally better than that of German, more improvement is expected on German.
Table 4: Summary of datasets for adversarial LSTM trainingDataset	Training	Test	Median length	Attributes	ClassesJV	225	370	15	12	9HAR	6,127	2,974	128	9	6AD	5,500	2,200	39	13	10IMDB	25,000	25,000	239	-	2The Human Activity Recognition dataset (HAR Anguita et al., 2013) is used to classify a person’sactivity (sitting, walking, etc.) based on a trace of their movement using sensors. The Arabic Digitsdataset (AD, Hammami & Bedda, 2010) contains time series corresponding to spoken Arabic digitsby native speakers, and the task is to classify digits. IMDB (Maas et al., 2011) is a standard moviereview dataset for sentiment classification. Details of the datasets are summarized in Table 4. The - isbecause IMDB is a text dataset, for which a 256-dimensional word embedding is learned.
