Table 1: Results on sentences classification dataset. Accuracy (acc) is used as the evaluation criteri-on. * represents the results achieved by ourself.
Table 2: The quantitative relation between the separation rank in TSLM and the parameters (i.e., R and L). Increasing L and R approximate the separation rank of recurrent network exponentially					and linearly, respectively. Example: n = {1, 2, 3,4}.					4 (i.e., sentence length), R ∈ {14, 15, 16, 17} and L ∈		R	14		15	16	17L=	1	14	15	16	17L=	2	455	560	680	816L=	3	8008	12376	18564	27132L=	4	1.9*104	4.3*104	9.2*104	3.5*105A.5.1 Experimental SetupSec. 4 analyzed the relation between contextual dependencies and recurrent network architec-ture through L and R. Specifically, the lower bound of the recurrent language model model-ing the sentence contextual dependencies will be increased by increasing L (L ∈ {1, 2, 3}) andR (R ∈ {100, 125, 150, 175, 200, 225, 250, 300}) in long-range/short-range dependency tasks. Re-call that our hypotheses are: For short-range dependent tasks, increasing R is sufficient to reflectthe network modeling ability, while for long-range dependent tasks, increasing L can improve thenetwork modeling ability more effectually than increasing R. We test our hypotheses through thetrend of F1 score. F1 is a mainly and common indicator of evaluation in these task.
Table 3: The statistical results about the average of the context similarity. Layers and hidden unitsare represented by L and R respectively.
