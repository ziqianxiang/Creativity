Table 1: Results for class incremental continual learning approaches averaged over 5 runs, baselinesand the reference isolated learning scenario for the three datasets. αT and γT indicate the respectiveaccuracy and NLL reconstruction metrics at the end of the last task increment T = 5. KLT denotesthe corresponding KL divergence. Arrows indicate whether lower or larger values are better.
Table 2: Results for incremental cross-dataset continual learning approaches averaged over 5 runs,baselines and the reference isolated learning scenario for the three datasets. αT and γT indicate therespective accuracy and NLL reconstruction metrics at the end of the last increment T = 3. KLTdenotes the corresponding KL divergence. Arrows indicate whether lower or larger values are better.
Table 3: Test accuracies and outlier detection values of the joint OCDVAE and dual model ap-proaches when considering 95 % of known tasks’ validation data is inlying. Percentage of detectedoutliers is reported based on classifier predictive entropy, reconstruction loss and our posterior basedEVT approach, averaged over 100 Z 〜qθ(z|x) samples per data-point respectively.
Table 4: Losses for the used WRN architecture with 2-D latent space and different β valuesfor MNIST. As stated in the main body, losses are normalized per dimension for training. Un-normalized values in nats are reported in brackets for reference purposes.
Table 5: Losses for the used WRN architecture with 60-D latent space and different β valuesfor MNIST. As stated in the main body, losses are normalized per dimension for training. Un-normalized values in nats are reported in brackets for reference purposes.
Table 6: Results for class incremental continual learning approaches averaged over 5 runs, baselinesand the reference isolated learning scenario for MNIST at the end of every task increment. αt and γtindicate the respective accuracy and NLL reconstruction metrics at the end of every task incrementt. KLt denotes the corresponding KL divergence.
Table 7: Results for class incremental continual learning approaches averaged over 5 runs, baselinesand the reference isolated learning scenario for FashionMNIST at the end of every task increment.
Table 8: Results for class incremental continual learning approaches averaged over 5 runs, baselinesand the reference isolated learning scenario for AudioMNIST at the end of every task increment.
Table 9: 14-layer WRN encoder with a widen factor of 10. Convolutional layers (conv) areparametrized by a quadratic filter size followed by the amount of filters. p and s represent paddingand stride respectively. If no padding or stride is specified then p = 0 and s = 1. Skip connectionsare an additional operation at a layer, with the layer to be skipped specified in brackets. Every con-volutional layer is followed by batch-normalization and a ReLU.
Table 10: 15-layered WRN decoder with a widen factor of 10. P refers to the quadratic input’s spatialdimension. Convolutional (Conv) and transposed convolutional (conv_t) layers are parametrizedby a quadratic filter size followed by the amount of filters. p and s represent padding and striderespectively. If no padding or stride is specified then p = 0 and s = 1. Skip connections are anadditional operation at a layer, with the layer to be skipped specified in brackets. Every convolutionaland fully-connected (FC) layer are followed by batch-normalization and a ReLU. The model endson a linear transformation with a Sigmoid function.
