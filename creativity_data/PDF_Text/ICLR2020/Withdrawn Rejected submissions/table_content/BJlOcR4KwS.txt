Table 1: Comparisons with baseline and SENet on ResNet-18, -50, and -101 in terms of accuracy,GFLOPs, CPU and GPU inference time on ImageNet. The top-1,-5 accuracy of our CE-ResNet ishigher than SE-ResNet while the computational cost in terms of GFLOPs, GPU and CPU inferencetime remain nearly the same.
Table 2: Comparisons with baseline and SE on lightweight networks, MobileNet V2 and ShuffleNetV2, in terms of accuracy and GFLOPs on ImageNet. Our CENet improves the top-1 accuracy by alarge margin compared with SENet with nearly the same GFLOPs.
Table 3: CE improves top-1 and top-5 accuracy of various normalization methods on ImageNet withResNet50 as backbone.
Table 4: Results of batch covariance decorrelation, adaptive variance inverse and channel equilib-rium. We use ResNet-50 as the basic structure. The top-1 accuracy increase (1.7) of CE-ResNet ishigher than combined top-1 accuracy increase (1.1) of BD-ResNet and AII-ResNet, indicating theeffects of BD and AII branch is complementary.
Table 5: Detection and segmentation results in COCO using Mask-RCNN We use the pretrainedCE-ResNet50 model (78.3) and CE-ResNet101 (79.0) in ImageNet to train our model. CENet canconsistently improve both box AP and segmentation AP by a large margin.
Table 6: We add CE after the second (CE2-ResNet50) and third (CE3-ResNet50) batch normaliza-tion layer in each residual block. The channel of the third batch normalization is 4 times than that ofthe second one but the top-1 accuracy of CE3-ResNet50 outperforms CE2-ResNet50 by 0.4, whichindicates CE benefits from larger number of channels.
