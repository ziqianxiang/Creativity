Table 1: Our quantization strategy achieves better BLEU scores than all other quantization methodsfor the Transformer on the WMT14 EN-DE, WMT14 EN-FR and WMT17 EN-DE test set.
Table 2: Performance of our quantization method on the WMT14 EN-DE and WMT14 EN-FR testset for a fixed number of training steps.
Table 3: Effect of quantizing single activations of the Transformer on the translation quality. Resultsare on the WMT14 EN-FR test set.					Module	Quantized Activation	No Bucketing PPL BLEU		Bucketing PPL BLEU	Encoder	(Input Embedding + Positional Encoding)	3.20	38.61	3.20	39.08Decoder	(Input Embedding + Positional Encoding)	3.20	39.35	3.20	39.36Multi-Head	Input (Q,K,V)	3.21	39.06	3.21	39.29Attention	LayerNorm Output	3.21	39.09	3.20	38.78Scaled Dot-Product Attention	Softmax Numerator	3.20	39.32	3.21	39.01	Softmax Denominator	3.21	39.35	3.21	39.11	Softmax Output	3.22	39.41	3.22	38.87	Output	3.21	38.73	3.21	39.02	ReLU Output	3.21	39.43	3.22	38.93Feed-forward	Feed-forward Output	3.54	38.03	3.20	39.27	LayerNorm Output	3.21	38.67	3.21	39.04	Numerator	3.53	37.75	3.21	38.86LayerNorm	Denominator	1748	0	-	-	Quotient	3.22	38.97	3.21	39.02Table 4: Variations to our quantization scheme evaluated on the WMT14 EN-FR translation task.
Table 4: Variations to our quantization scheme evaluated on the WMT14 EN-FR translation task.
Table 5: Impact of delaying quantization on translation quality. Results are on the WMT14 EN-DEand WMT14 EN-FR test set.
Table 6: Evaluation of our quantization method on the WikiText-2 and WikiText-103 languagemodeling tasks.
Table 7: Evaluation of our quantization method on the WMT14 EN-CS, WMT14 RU-EN andWMT14 ES-EN translation datasets.
Table 8: Comparison of our adaptive pruning scheme versus fixed rate pruning methods for equalpruning proportions. Total compression is of quantization combined with pruning.
