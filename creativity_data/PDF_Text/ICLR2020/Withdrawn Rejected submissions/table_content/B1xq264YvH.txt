Table 1: Class-Conditional Generationon IMDb movie reviews. Classificationaccuracy is measured by a sentimentclassifier trained on the IMDb trainingset. Bold indicates statistically signifi-cant best results at p ≤ 0.05.
Table 2: Abstractive summarization on CNN/DM. f indi-cates pretraining of the encoder side. PointerGen+BU from(Gehrmann et al., 2018), ELMo+SHDEMB from (Edunovet al., 2019), BERT+Two-Stage from (Zhang et al., 2019),UniLM+ExtLoss from (Dong et al., 2019). Bold indicatesstatistically significant best results among general modelsand encoder-agnostic models at p ≤ 0.05.
Table 3: Story generation on the Writing-Prompts dataset. Rank acc. refers to the top-1 prompt ranking accuracy metric described inSection 4.3. (Experiments use the GPT2 BPEscheme, so PPL numbers are not directly com-parable to those reported in (Fan et al., 2018)).
Table 4: Image paragraph captioning on VisualGenome, as measured by CIDEr and BLEU-4(B4) scores. Bold indicates statistically signifi-cant best results at p ≤ 0.05.
Table 5: IMDb conditional movie re-view generation results, comparing thelarger 345M parameter GPT-2 model tothe 117M parameter GPT model.
Table 6: Human evaluation of story generation quality. Participants were asked specific binaryquestions concerning the four criteria, the numbers for the four left categories represent percent-ages of approval. On the right, the methods are rated on a 4-point scale based on the combinationof the four criteria. Uncertainties represent a 95% confidence interval, bold indicates statisticallysignificant maxima for each category of the models under consideration.
Table 7: Example generations from models trained on the movie review generation task. In allcases the indicated sentiment was positive. The number in the left column is the number of trainingexamples (22k is the full dataset).
