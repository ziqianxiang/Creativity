Table 1: Self-supervised action recognition accuracy. (Left) We show the effect of different pre-training strategies on our model. * are our reimplementations. (Right) Comparison to the state of theart on UCF101 and HMDB51. (We report the performances from the original papers.)Comparison of pre-training strategies. In Table 1(Left) we compare our way of pre-trainingthe S3D model (i.e., using CBT visual loss) to existing approaches. In particular, we consider the5Under review as a conference paper at ICLR 2020Shuffle&Learn (Misra et al., 2016) and 3DRotNet (Jing et al., 2018) proxy tasks. We reimplementthe two methods using S3D CNN, and pre-train them on the same Kinetics data. We also considerrandom initialization. We report classification results on UCF101 and HMDB51 using frozen featuresand fine-tuned features passed to a linear classifier. We see that our method outperforms existingtraining methods by a large margin.
Table 2: Action anticipation accuracy. (Left) Comparison to the state of the art on Breakfast, 50Saldads, ActivityNet. Self-super = Y means the model was pre-trained in a self-supervised way, andthen fine-tuned using a linear classifier. Self-super = N means the model is trained end-to-end onthe specific task. (Right) Comparison with the average pooling and LSTM baselines on 50SaladsBreakfast, 50Salads and ActivityNet. We vary the observation window lengths (sec.)Effect of video length. In Table 2 (Right) we show the impact of the length of the training videos onthe performance. We compare with two baselines, average pooling (AvgPool) and LSTM (Hochreiter& Schmidhuber, 1997). The AvgPool baseline simply computes the average of all input visualfeatures over time. The LSTM baseline takes the same sequence of S3D features but recurrentlyupdates its hidden states over time. The final hidden state is used for classification. We adjust thehidden unit size of LSTM to make its number of parameters comparable to CBT. We can see that CBTsignificantly outperforms the two baselines on all three datasets. Moreover, we can see that as theobserved video length increases, the performance of CBT monotonically increases, while LSTM andAvgPool either plateaus or decreases. These results indicate that CBT is better at modeling long-termtemporal context.
Table 3: Ablation study on the action anticipation task. We show accuracy on Breakfast, 50Saladsand ActivityNet. (Left) Impact of the percentage of HowTo100M videos used, and the cross-modalobjective during pre-training. 0% corresponds to no pretraining, ie. using random weights. (Middle,Right) Impact of the number of layers (L) and attention heads (A) for the visual transformers.
Table 4: (Left) Video captioning results on the YouCook2 dataset (Zhou et al., 2018b). We comparewith previous state-of-the-art methods by Zhou et al. (2018c) and Sun et al. (2019a), the captiondecoder of all methods share the same architecture, the main difference comes from the visual encoder.
