Table 1: Ablation study. Impact of various training losses on ProtoAttend with softmax attention forFashion-MNIST. 1 ≤ i ≤ Nt is the training iteration index and Nt is the total number of iterations.
Table 2: ProtoAttend achieves interpretability without significant degradation in performance. Accu-racy and median number of prototypes to add up to 50%, 90% and 95% of the decision, quantifiedwith prototype weights.
Table 3: Label noise ratio vs. accuracy for baseline encoder, dropout method (Arpit et al., 2017)(optimizing the keep probability) and ProtoAttend with sparsemax attention and sparsity regularizationfor CIFAR-10. ______________________________________________________________________Noise level	Test accuracy %			Baseline	Dropout	ProtoAttend08	57:02	56776	605006	7127	72ΓT5	74:670.4	―	77.47	—	78.99	—	80.04	—As prototypical learning with sparsemax attention aims to extract decision-making information froma small subset of training samples, it can be used to improve performance when the training dataset8Under review as a conference paper at ICLR 2020contains noisy labels (see Table 3). The optimal value11 of λsparse increases with higher noisy labelratios, underlining the increasing importance of sparse learning.
Table 4: Datasets and database size D.
Table 5: Human ratings (mean score and 95% confidence interval) on how much an extra image helpsguessing the class of the input.
