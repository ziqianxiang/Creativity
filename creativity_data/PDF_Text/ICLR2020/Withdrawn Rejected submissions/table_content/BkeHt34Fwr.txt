Table 1: Results (with BOW) -Average accuracy on unlabeled nodes for Cora and CiteSeernetworks, with bag of words as input. Those results obtained with 20 X number of classes labelednodes, when the initial state was 4 random nodes from each class (same as Cai et al. (2017))* score is estimated from figure	** scores for Smaller budget, since it was the max reportedalgorithm	Cora	CiteSeerRandom	^0803	^0697KiPf (random) (KiPf & Welling, 2016)	^0801	^0679Kipf (specific train) (Kipf & Welling, 2016)	0.815	0.703Chang const Params (Cai et al., 2017)	0.823	0.707Chang adaPtive Params (Cai et al., 2017)	0.825	0.721HNE (Chen et al., 2019)	0.59*	-TV/MSD** (Berberidis & Giannakis, 2018)	0.78*	0.7*ClassSeek** (McDowell, 2015)	0.805*	0.695*Î£-OPt** (Ma et al., 2013)	0.73*	0.71*ALFNET (Bilgic et al., 2010)		0.78*	0.7*Geo Dist	^0806	^0680Attractor Basin (Muchnik et al., 2007)	0.585	0.646K Truss (Malliaros et al., 2016)	0.709	0.693reP dist LOF	0.803	0.691rep dist MAH	0.803	0.691
Table 2: Results - accuracy without content - Average prediction results (accuracy) on all unlabelednodes, with neighbors labels features as input. The results reported for different fractions of labelednodes, when the initial state was 1 random nodes from each class. In contrast with Table 1, here noexternal information on the nodes (such as a BOW) was used.
Table 3: Dataset statisticsData Set	Nodes	Edges	ClassesCORA	2,708	5,429	7-CrTESEER-	3,312	4,732	6-EMAIL-EU-	1,005	25,571	-42-PUBMED	19,717	44,338	3SUBELJ CORA	23,166	91,500	-10-WIKISPEEDIA-	4,604	119,882	1514Published as a conference paper at ICLR 2020Figure 3: Accuracy as a function of sampling fraction for different datasets and different local ALmethods. Results lower than the random sampling line (Thick gray line) represent AL algorithmsthat do not contribute to the accuracy. The two subplots surrounded by a box are the results with theBOW. We have tested Micro and Macro F1 as well as the loss, with similar results (data not shown).
