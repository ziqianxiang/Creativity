Table 1: Language modeling results. NLL is estimated with importance sampling. PPL is based onthe estimated NLL. KL and MI are approximated by their Monte Carlo estimates. Couple- standsfor “with the coupled deterministic network”. The better results between a model and the versionwith the coupled deterministic network are shown in bold. *The exact NLL is reported. ^Using ormodifying open-source code which does not follow our setup and evaluation. ^Previously reported.
Table 2: Mutual information (MI) and reconstruction metrics (i.e., BLEU-1 and BLEU-2). MI isapproximated by its Monte Carlo estimate. Other notations follow Table 1.
Table 3: The effect of the coupling weight λc. *Reported in the Table 1 and 2.
Table 4: Gradient norms of the reconstruction loss, the coupled reconstruction loss, the regulariza-tion loss, and the decoding signal w.r.t. the encoded text on each test set.
Table 5: Diversity and the first three samples from each model on Yelp. Dist-1 and Dist-2 stand forthe ratios of distinct unigrams and bigrams over all generated ones. Redundancies are shown in red.
Table 6: Diversity metrics and the first three samples from each model on PTB. Redundancies(pieces of text that have appeared in the same text before) are shown in red.
Table 7: Texts generated from the interpolations of two latent codes.
