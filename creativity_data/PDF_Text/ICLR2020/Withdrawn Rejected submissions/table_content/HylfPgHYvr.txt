Table 1: Average Euclidean (L2) distance (in pixels) between predicted and ground truthpositions, for a prediction horizon of 5 frames / 10 frames. To compute the distance, the pixel-based x-y-d coordinates of objects are projected back in an untilted 200x200x200 referenceCartesian coordinate system. Methods denoted by * have access to the ground truth objectpositions at all time, even behind occluders. Their performance is hence optimistic andnot directly comparable to the other methods trained only from observed object masks anddepth fields. **(Battaglia et al., 2016) is trained with more supervision, since target valuesinclude ground truth velocities, not available to other methods.
Table 2: Average Euclidean (L2) distance (in pixels in a 200 by 200 image) between predictedand ground truth positions, for a prediction horizon of 5 frames / 10 frames.
Table 3: Percentage of predictions within a 20-pixel neighborhood around the target as afunction of rollout length measured by the number of frames. 20 pixels corresponds to thesize of the smallest objects in the dataset.
Table 4: Detailed results of relative classification error of our model compared to the (Riochetet al., 2018) baseline on the IntPhys object permanence benchmark, block O1 / Occluded /Dynamic 1. Lower is better.
