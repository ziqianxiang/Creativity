Table 1: The results of both the top-1 accuracy and the average bit length in multiple bit-width. Thefull experiments of ResNet-18 are listed in Appendices EModel Name	Bit-W (bt/bsl)	Bit-A	λ2	Percentage of SLW	Average Bit Length	Top-1 Accuracy	RemarksAlexNet	32	32	-	-	-	75.06%	FPAlexNet	32	32	-	-	-	74.78%	FP(ReLU1)AlexNet	2/-	4	-	0%	2	70.57%	TTQ*AlexNet	2/3	4	-	^^12.05%^^	2.362	73.54%	BQAlexNet	2/3	4	1.2	1.39%	2.042	73.39%	BQ+WRAlexNet	2/3	4	-	0.83%	2.025	74.72%	SQResNet-18	32	32	-	-	-	76.07%	FPResNet-18	32	32	-	-	-	74.54%	FP(ReLU1)ResNet-18	2/-	4	-	0%	2	72.79%	TTQ*ResNet-18	2/2	4	-	^^059%~~	2.0118	73.92%	BQResNet-18	2/2	4	1.0	0.014%	2.0003	73.92%	BQ+WRResNet-18	2/2	4	-	0.0015%	2.00003	74.22%	SQResNet-18	2/3	4	-	-^734%~~	2.2203	74.07%	BQResNet-18	2/3	4	1.0	0.434%	2.013	74.05%	BQ+WRResNet-18	2/3	4	-	0.321%	2.0096	74.67%	SQResNet-18	2/4	4	-	^^28.80%^^	3.1521	74.54%	BQResNet-18	2/4	4	1.0	4.995%	2.2	74.32%	BQ+WR
Table 2: The results of both the top-1 accuracy and the average bit length on ImageNet dataset.
Table 3: The results of the weighted ridge (WR) regularizer on full precision models.
Table 4: Classification accuracy of the state-of-the-art quantization methods trained on ImageNetwith Resnet-18 model. Bit-W and Bit-A refer to the weights and activations bit-width respectively.
Table 5: Comparison of non-fixing the range of the quantization function (TTQ*) and fixing themModel Name	Bit-W	Bit-A	Top-1 Accuracy	RemarksAlexNet	32	-^32^^	75.06%	FPAlexNet	2	4	70.57%	TTQ*AlexNet	2	4	72.04%	TTQ**ResNet-18	32	-^32^^	76.07%	FPResNet-18	2	4	72.79%	TTQ*ResNet-18	2	4	73.55%	TTQ**To observe the effect of the fixed range of the basic quantization (BQ) method, we perform a par-ticular experiment. The results of our implementation of TTQ (TTQ*) is performed with the quan-tization function of which the range is not fixed. On the other hand, a version of implementation(TTQ**) have two different conditions compared to TTQ*: i) fixing the range of the quantizationfunction at initialization; and ii) using one coefficient as {-Ws, 0, +Ws }. When applying them,the accuracy of AlexNet and ResNet-18 are rather improved by 1.47% and 0.76%. The results ofTTQ** shows that the fixed range also can be utilized for quantization. Additionally, TTQ set aconstant t to 0.05 for quantizing weights under t ∙ max(∣w∣) as zero values. In our method, however,if bsi is 2, 3, or 4 bits then the t is derived as 1,吉,or 表.All our t are higher than 0.05 of TTQ andit can cause worse results since L2 regularizer gives more penalty to the larger weights.
Table 6: The results of applying the scaling factor r, derived from the maximum absolute values oftwo inputs of add operations, to mediate two different intervals on ResNet-18 with full precision.
Table 7: The experimental results of HWR and CQ in multiple bit-width and various λ2 .
