Table 1: Accuracies and standard deviation of Meta-Learning of Linear ModelsDataset	LR w/ MAML	LR + LINNET w/ MAML	LR W/ KLIN	LR W/ KFCSynthetic	51.38% ± 3.91	95.77% ± 0.41	99.00% ± 0.08	98.98% ± 0.10Omniglot	46.50% ± 2.80	71.31% ± 4.52	63.88% ± 3.93	71.56% ± 4.40computational complexity and decomposition rank by tuning the additional hyper-parameter r. TheCholesky decomposition might be preferable to the Kronecker one in memory-constrained applica-tions, since r can be used to control the memory requirements of the meta-optimizer. Moreover,such a decomposition imposes symmetry and positiveness on A, which might be desirable whenapproximating the Hessian or its inverse.
Table 2: Post-adaptation accuracies by different meta-optimizers adapting a small CNN.
Table A1: Post-adaptation accuracies by different meta-optimizers adapting a small CNN.
Table A2: Accuracies and standard deviation of Meta-Learning of Non-Linear ModelsOmniglot	CIFAR-FSNum. Layers	CNN	CNN W/ KFC	CNN	CNN W/ KFC1	29.20% ± 1.13	66.45% ± 1.42	24.86% ± 0.47	42.02% ± 0.522	60.94% ± 1.54	91.13% ± 0.93	38.44% ± 0.87	52.94% ± 1.333	91.69% ± 0.70	96.57% ± 0.86	50.91% ± 0.80	53.07% ± 0.584	97.56% ± 0.45	98.49% ± 0.41	55.03% ± 1.23	55.97% ± 0.5919Under review as a conference paper at ICLR 20201.00.8&0.6roU-U⅛θ40.20.0Omniglot---CNN w/ MAML
Table A3: Computational metrics for each of the methods. The timing metrics measure the time tocompute one meta-gradient step on an entire meta-batch. Inference time is identical across methodsfor models of the same size.
