Table 1: Notation SummarySymbol	Meaning	Symbol	Meaningt	step index	St	current painting state of step t, canvass*	target painting state, reference image	^ S*	reproduction of S*ct	constraint of step t	ot	observation of step tat	action of step t, at = [αt, lt, wt, ct]	rt	reward of step tqt	accumulated reward of step t	γ	discount factor for computing the rewardπ	painting policy, predict a by o predict r by o	Vπ	value function of the painting policy,R(at, st)	render function, render action to st state and the target state	O(S*, St)	observation function, encode the currentRr	real renderer	Rn	neural rendererL(s, s*)	loss function, measuring distance betweenstate S and objective state s*constrained agent, we replace the unconstrained agent with user defined constraints or the action ofprevious step.
Table 2: l2 loss of paintbrush modelsPaintbrush l2 Loss Paintbrush l2 Losscharcoal	2.12	×	10-3	impressionism	2.43	×	10-3pencil	8.37	×	10-5	marker	5.01	×	10-4bulk	9.16	×	10-4	dry brush	3.06	×	10-3calligraphy	5.92	×	10-4	Watercolor	1.16	×	10-46.2	Unconstrained Painting AgentWe train the unconstrained painting agent using the fixed envionment model With various dataset.
Table 3: l2 training loss of training schemesTraining Scheme: Dataset l2 Loss Training Scheme: Constraint l2 LossMNIST	8.16 ×	10-3	width (r)	1.25 × 10-2KanjiVG	4.02 ×	10-2	start position	(x0, y0)	1.43 × 10-2Figure 5: We trained our natural media painting agents using MNIST, KanjiVG, CelebA, and Ima-geNet as reference images (left). We generate results (right) using Rr for the training and validatingpFroorctehses.roll-out process, we employ a coarse-to-fine strategy to increase the resolution of the resultshown as Figure 6. First, We roll out the agent with a low-resolution reference image s* and get thereproduction s*. Then we divide s* and s* into patches and feed the agent as initial observation.
