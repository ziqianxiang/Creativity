Table 1: Comparing test accuracy of classifiers trained on latent variables of each model.
Table 2: Toy VAE Model for visualization of experimentsFunction	LayerEncoder	Fully Connected Layer 784x512 + Activation Fully Connected Layer 512x512 + Activation Fully Connected Layer 512x512 + ActivationSampling	Linear layer with 2 unitsDecoder	Fully Connected Layer 2x512 + Activation Fully Connected Layer 512x512 + Activation Fully Connected Layer 512x512 + Activation Fully Connected Layer 512x784 + Sigmoid*Activation functions are all either Leaky ReLu (alpha=0.2), ReLu, or Tanh as part of the experimentation.	B Model architecture, objective functions and details oftraining.
Table 3: Objective functions5Objective type			Objective	ELBO	Le =	二 Eqφ(z∣x) [logPθ(XIz)]	-KLSφ(ZIX)kp(Z))β-VAE	Lβ	二 Eqφ(z∣x) [logPθ (XIz)]	-β * KL(qφ(ZIX)kp(Z))μ-VAE	Lμ 二	二 Eqφ(z∣x) [logPθ (XIz)]	-B h∣P3PD=1 μdi)I + Pi=IPD=1 [logσ2]di)i5Note that each Eqφ(z∣χ)[.] and KL[.] is computed on expectation, Epdata(X)[」，but it is not shown explicitlyin the formulas above to make them more readable. Also, regularization term in μ-VAE is shown explicitly toemphasize how sample means are computed over batches.
Table 4: Model used for comparing VAE, β-VAE and μ-VAEFunction	LayerEncoder	2D Conv 28x28x64 + Leaky Relu 2D Conv 14x14x64 + Leaky Relu 2D Conv 7x7x64 + Leaky Relu Linear layer with (7x7x64)x10Sampling	10 Latent layer unitsDecoder	Linear Layer 10x(7x7x64) + Leaky Relu 2D DeConv 7x7x64 + Leaky Relu 2D DeConv 14x14x32 + Leaky Relu 2D DeConv 28x28x1 + SigmoidClassifier	Dense 10x1024 + Leaky Relu Dense 1024x1024 + Leaky Relu Dense 1024x1024 + Leaky Relu Dense 1024x10 + Softmax*All Leaky ReLU layers use alpha=0.2.	12Under review as a conference paper at ICLR 2020C MNIST ResultsFigure 8: Training curves of the model trained on MNIST. Regularization loss of μ-VAE defined asPB=I μn∣ + PB=1 [logσ2]n, i.e. term that replaces KL.
