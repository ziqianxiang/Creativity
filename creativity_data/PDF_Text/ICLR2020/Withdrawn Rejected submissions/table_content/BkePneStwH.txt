Table 1: BLEU scores of the machine translation system used for generating synthetic training datafor multilingual tuning and knowledge distillation corpus for XD. The table is taken from (Conneauet al.,2018)l2	Ifres de el bg ru tr ar Vi th Zh hi SW Ui~~En-l2BLEU^^49.3 48.5 38.8 42.4 34.2 24.9 21.9 15.8 39.9 21.4 23.2 37.5 24.6 24.12.2	Multilingual tuningIn multi-language fine-tuning, similarly to current state-of-the-art approach, We use machine trans-lation system to translate data to other languages. Then We hoWever use all the obtained data atthe same time to tune a single XLMMLM or XLMMLM+TLM. Since the pseudo corpus is fullyaligned, model does not see a single neW example. HoWever, the netWork might improve by discov-ering task-oriented rules and regularities learnt betWeen languages.
Table 2: Results of multilingual tuning (MLT) for all languages, as well as subsets of languages:removing German/Swahili/Urdu, compared with individual tuning (IndT, results of tuning a separatemodel for each language by Lample & Conneau (2019)). Results for the two XLM varieties (MLMand TLM) are shown separately. Zero-shot scores (with no directly supervised tuning performed forthese languages) are shown in gray.
Table 3: Results of cross-lingual knowledge distillation (XD) for all languages simultaneously, com-pared to multilingual tuning (MLT) and individual tuning (IndT, results of tuning a separate modelfor each language by Lample & Conneau (2019)). Results for the two XLM varieties (MLM andTLM) are shown separately. Zero-shot scores (with no directly supervised tuning performed forthese languages) are shown in gray.
Table 4: Results of cross-lingual knowledge distillation (XD) for specific languages (l2) in combi-nation with English, in comparison with true-label tuning (MLT) on the same language pair (l2andEnglish). Results are shown only for English (source of labels), the language used for XD and theoverall average for the sake of brevity.
Table 5: Results of cascaded combination of our best insights: multilingual tuning on all languagesbut Urdu with cross-lingual knowledge distillation (XD) on specific language sets; bests result givesmultilingual tuning with XD for English, German, French, and Spanish on topMLTno-urMLTno-ur +XDfrMLTno-ur + XD swMLTno-ur +XDurMLTno-ur + XD w/0 urMLTno-ur + XD 4-langen fr es de el bg ru tr ar Vi th Zh hi SW ur85.2 81.3	82.5	80.5	80.8	81.2	79.3	76.7 77.8	78.5	77.5	80.2	73.9	73.0	70.184.0 81.3	81.5	79.9	79.7	81.0	78.4	75.6 77.3	77.8	76.8	79.5	72.9	72.2	69.684.7 80.8	82.2	80.0	79.8	80.8	78.3	76.0 77.8	78.2	77.5	79.5	73.2	72.6	70.282.4 78.8	79.6	78.3	78.0	78.6	76.6	72.8 75.1	75.7	74.5	77.6	70.0	70.8	67.484.2 80.4	81.0	79.6	79.0	78.9	77.3	74.1 76.6	77.7	76.3	78.6	72.5	71.2	68.885.3 81.7	82.7	81.4	80.4	81.1	79.5	76.4 78.5	78.7	78.1	80.1	73.6	73.9	69.7avg78.978.078.4
Table 6: Comparison betWeen related Work and our methods: multilingual tuning (MLT) and cross-lingual knoWledge distillation (XD).
