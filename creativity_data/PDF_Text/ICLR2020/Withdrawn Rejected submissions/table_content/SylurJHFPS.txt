Table 1: The discrepancy across two models and two datasets in pre-train. For both da and ds, loweris better.
Table 2: The compare between the absolute discrepancy in pre-training and G is updated by DDφ'sfeedback signal. #samples denotes the amount of the generated data is used for updating the G. Forexample, 2S means the generated instances is two time as the amount of test data. Random denotesthe existing way but the other row are the results according to HW. < 0.3 - 0.5 means the generatedinstances whose score are between 0.3 and 0.5 assigned by D, are selected out.
Table 3: The compare between the approximate discrepancy in pre-training and G is updated byDDφ's feedback signal. #SamPleS denotes the amount of the generated data is used for updating theG. For example, 2S means the generated instances is two time as the amount of test data. Randomdenotes the existing way but the other row are the results according to HW. < 0.3 - 0.5 means thegenerated instances whose score are between 0.3 and 0.5 assigned by D, are selected out.
Table 4:	Generated sentences Who are scored 0.9 or higher by DDφ at the end of pre-training. Obvi-ously, they are better the next sentences listed in table 5.
Table 5:	Generated sentences Who are scored 0.1 or lower by DDφ at the end of pre-training. Obvi-ously, they are better the next sentences listed in table.
