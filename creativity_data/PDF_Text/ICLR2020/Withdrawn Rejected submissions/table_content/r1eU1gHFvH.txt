Table 1: Training conditions for experiment 11. *O-S included tests for softmax, dropout and thethree training conditions.
Table 2: Various quantities associated with the distribution of local codes (LCs) in with dropout(given as a percentage) applied during trainingNo. of LCs	0%	20 %	50%	70%	90%MinimUm	8	4	7	15	0MaximUm	34	29	41	57	125Mean	18.44	16.13	20.65	34.54	73.80Standard deviation	4.55	4.19	4.96	8.05	24.53To test if the different datasets could have come from the same distribution we did Kolmogorov-Smirnov hypothesis tests with the hypothesis being that the data were from the same distribution.
Table 3: Kolmogorov-Smirnov hypothesis tests for the dropout experiments compared to a run withno droPoUt.
Table 4: Kolmogorov-Smirnov hypothesis tests for the dropout experiments compared to a run with10% dropout.
Table 5: Kolmogorov-Smirnov hypothesis tests for the dropout experiments compared to a run with20% dropout.
Table 6: Kolmogorov-Smirnov hypothesis tests for the dropout experiments compared to a run with30% dropout.
Table 7: Kolmogorov-Smirnov hypothesis tests for the dropout experiments compared to a run with40% dropout.
Table 8: Example hot-pixel invariant MNIST input data images (after normalisation from the meanimage). ‘Invariant’ images have a hot-pixel short-cut code (of white pixels) that is the same for allexamples of that class, ‘Gaussian’ invariant images have a greyscale hot-pixel code that is drawnfrom a Gaussian distribution and different for each image.
Table 9: Average numbers of LCs in 4-layer MLPs trained on MNIST (standard) and modifiedMNIST with added hot pixel invarients.
Table 10: Training times for MLP networks with different MNIST training sets. Note that thepresence of a invariant reduces training time because the NN finds the shortcut (all NN are trainedto >99% acc), and introducing a GAussian distribution over that invariant (i.e. making it morevariable) generally increases the training time.
Table 11: Fits for the trend lines drawn in figures 4 and 12.
