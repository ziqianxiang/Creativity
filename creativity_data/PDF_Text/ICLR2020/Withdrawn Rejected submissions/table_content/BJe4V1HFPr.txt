Table 1: Common part of network architectureLayer	-	SC	SC	SC	SC	SC	SC	FChannel	3	32	64	128	256	512	1024	2048Size	256	128	64	32	16	8	4	-4 ExperimentsFor our main experiments presented here, we use anime illustrations obtained from Danbooru1 .
Table 2: Weighting and training hyperparametersWeight	ValueλC1	0.2λE-KL	10-4λS-KL	2 × 10-5λD	1λC2	1λcont	0.05Stage	Learning rate S	Others		Algorithm	Batch	Time1	0.005	5 × 10-5	Adam	8	400kC2 pre-train	-	10-4	Adam	16	200k2	0.01	2 × 10-5	RMSprop	8	400kFigure 2: Images generated by fixing the style in each group of two rows and varying thecontent. Two different styles are shown. Leftmost column taken from training set, courtesyof respective artists. Top group: Sayori. Bottom group: Swordsouls.
Table 3: Weighting and training hyperparameters for NISTParameter	W vs. D + R	D vs. W + RλC1	0.1	0.1λE-KL	10-4	10-4λS-KL	10-4	10-4λD	1	1λC2	0.2	1λcont	0.5	0.1Stage 1 time	300k	300kC2 pre-train time	800k	100kStage 2 time	320k	320kidentity when only the writer label is known but not the digit label, or vice versa, when onlythe digit label is known but not the writer label.
Table 4: Mean Euclidean distance of a sample to the center of its classEncoder	By writer	By digit	Whole datasetEW	1.2487	0.2788	1.2505ED	0.7929	1.2558	1.2597EV	1.2185	0.4672	1.2475(a) First 2 dimensions			Encoder	By writer	By digit	Whole datasetEW	2.6757	2.0670	2.6957ED	2.4020	2.6699	2.7409EV	2.6377	1.7629	2.7363(b) First 8 dimensionsThe encoder aims to encode the unlabelled feature in the dataset while avoiding encodingthe labelled feature. Intuitively, this means that each unlabelled class should be distributeddifferently in the encoder’s code space while each labelled class should be distributed similarly.
Table 5: Average probability given to the correct classEncoder	By writer	By digit	Encoder	By writer	By digitEW	0.000293	0.9001	EW	0.000363	0.9327ED	0.001441	0.1038	ED	0.002845	0.1015EV	0.000337	0.6179	EV	0.000843	0.9380(a) First 2 dimensions	(b) First 8 dimensionsTable 6: Average rank of the correct classEncoder	By writer	By digit	Encoder	By writer	By digitEW	1608	1.12	EW	1330	1.12ED	582	5.20	ED	422	3.98EV	1409	1.49	EV	838	1.08(a) First 2 dimensions	(b) First 8 dimensionsWe can see that EW causes the samples to cluster very tightly by digit while ED has theopposite effect, which is consistent with the visualization in figures 6a and 9. Conversely, EDcauses the sample to cluster somewhat more tightly by writer, while EW has the oppositeeffect.
Table 6: Average rank of the correct classEncoder	By writer	By digit	Encoder	By writer	By digitEW	1608	1.12	EW	1330	1.12ED	582	5.20	ED	422	3.98EV	1409	1.49	EV	838	1.08(a) First 2 dimensions	(b) First 8 dimensionsWe can see that EW causes the samples to cluster very tightly by digit while ED has theopposite effect, which is consistent with the visualization in figures 6a and 9. Conversely, EDcauses the sample to cluster somewhat more tightly by writer, while EW has the oppositeeffect.
Table 7: Encoder By writer By digit FW	0.000454	0.94 ED	0.005331	0.13 EV	0.000846	0.70 (a) First 2 dimensions	Top-1 accuracy Encoder By writer By digit -EW	0.001400	0.94 ED	0.015424	0.23 EV	0.004946	0.95 (b) First 8 dimensions17Under review as a conference paper at ICLR 2020C.1 Using Reconstruction Result as Stage 1 Classifier InputBy design, assuming sufficiently powerful networks, the stage 1 method in (Chou et al., 2018)should reconstruct x perfectly with G(E (x), S (a)) while at the same time not include anystyle information in E(x). But in reality, this method worked poorly in our experiments.
Table 8: Top-1 classification accuracy of two classifiers on samples generated from twogenerators	Adversarial C2	Non-adversarial C2G trained with adversarial C2	14.37%	86.65%G trained with non-adversarial C2	1.85%	88.59%Figure 13: Images generated from fixed style and different contents, when explicit conditionon content is removed.
