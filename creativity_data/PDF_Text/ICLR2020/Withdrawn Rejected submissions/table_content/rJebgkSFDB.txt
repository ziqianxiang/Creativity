Table 1: Classification accuracies (%) on miniImageNet and CIFAR-FS.
Table 2: Classification accuracies (%) on Omniglot.
Table 3: Classification accuracies (%) on miniImageNet with 5 way using deep architecturesMethod	1-shot	5-shotMETA-SGD (Li et al., 2017)	54.24±0.03	70.86±0.04(Gidaris & Komodakis, 2018)	56.20±0.86	73.00±0.64(Bauer et al., 2017)	56.30±0.40	73.90±0.30(Munkhdalai et al., 2017)	57.10±0.70	70.04±0.63(Qiao et al., 2018)	59.60±0.41	73.54±0.19LEO (Rusu et al., 2019)	61.76±0.08	77.59±0.12SNAIL (Mishra et al., 2018)	55.71±0.99	68.88±0.92TADAM (Oreshkin et al., 2018)	58.50±0.30	76.70±0.30METAVRF (OurS)	63.40±0.06	77.85±0.28We evaluate our method by using pre-trained embeddings and compare with latest methods withdeep embedding architectures. We use the pre-trained embeddings from a 28-layer Wide ResidualNetwork (WRN-28-10) (Zagoruyko & Komodakis, 2016), in a similar fashion to (Rusu et al., 2019;Bauer et al., 2017; Qiao et al., 2018). Specifically, the WRN-28-10 was trained to classify imageswhich are all from the meta-training set into the corresponding categories. We select activations inthe 21th layer, with average pooling over spatial dimensions, as feature embeddings. The dimensionof pre-trained embeddings is 640. As shown in Table. 3, our MetaVRF achieves the state-of-the-artpreformance on both cases of 1-shot and 5-shot.
Table 4:	The fully connected network ψ(∙) used for regression.
Table 5:	The CNN architecture ψ(∙) for Omniglot.
Table 6:	The CNN architecture ψ(∙) for CIFAR-FSOutput size	Layers32 × 32 × 3	Input images16×16×64^^conv2d (3×3, Stride=1, SAME, RELU), dropout 0.5,pool (2×2, Stride=2, SAME)8×8×64	conv2d (3×3, stride=1, SAME, RELU),	dropout 0.5, pool (2×2, stride=2, SAME)4×4×64	conv2d (3×3, stride=1, SAME, RELU),	dropout 0.5, pool (2×2, stride=2, SAME)2×2×64	conv2d (3×3, stride=1, SAME, RELU),	dropout 0.5, pool (2×2, stride=2, SAME)256	flattenTable 7:	The CNN architecture ψ(∙) for miniImageNetOutput size Layers84×84×3 Input images42×42×64^^conv2d (3×3, Stride=1, SAME, RELU), dropout 0.5,pool (2×2, Stride=2, SAME)21×21×64	conv2d (3×3,	stride=1,	SAME,	RELU), dropout 0.5, pool (2×2, stride=2, SAME)10×10×64	conv2d (3×3,	stride=1,	SAME,	RELU), dropout 0.5, pool (2×2, stride=2, SAME)5×5×64	conv2d (3×3,	stride=1,	SAME,	RELU), dropout 0.5, pool (2×2, stride=2, SAME)2×2×64	conv2d (3×3,	stride=1,	SAME,	RELU), dropout 0.5, pool (2×2, stride=2, SAME)256	flattenD.2 Inference networksThe architecture of the inference network for the regression task is in Table 8. For few-shot classifica-tion tasks, all models share the same architecture, as in Table 9.
Table 7:	The CNN architecture ψ(∙) for miniImageNetOutput size Layers84×84×3 Input images42×42×64^^conv2d (3×3, Stride=1, SAME, RELU), dropout 0.5,pool (2×2, Stride=2, SAME)21×21×64	conv2d (3×3,	stride=1,	SAME,	RELU), dropout 0.5, pool (2×2, stride=2, SAME)10×10×64	conv2d (3×3,	stride=1,	SAME,	RELU), dropout 0.5, pool (2×2, stride=2, SAME)5×5×64	conv2d (3×3,	stride=1,	SAME,	RELU), dropout 0.5, pool (2×2, stride=2, SAME)2×2×64	conv2d (3×3,	stride=1,	SAME,	RELU), dropout 0.5, pool (2×2, stride=2, SAME)256	flattenD.2 Inference networksThe architecture of the inference network for the regression task is in Table 8. For few-shot classifica-tion tasks, all models share the same architecture, as in Table 9.
Table 8: The inference network φ(∙) used for regression.
Table 9: The inference network φ(∙) used for Omniglot, miniImageNet, CIFAR-FSOutput size	Layersk × 256	Input feature256	instance pooling256	fully connected, ELU256	fully connected, ELU256	fully connected, ELU256	LSTM cell, tanh to μw, log σW	Table 10: The prior network used for regression.
Table 11: The prior network used for Omniglot, miniImageNet, CIFAR-FSOutput size	Layers512 256 256 256 256	The concatenation of query feature and aggregated support features instance pooling fully connected, ELU fully connected, ELU fully connected to μw, log σWTable 12: Iteration and batch SiZe for all datasets.
Table 12: Iteration and batch SiZe for all datasets.
