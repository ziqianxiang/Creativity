Table 1: Accuracy on BERT embedded text classification datasets and UCI datasets with a smallnumber of labeled examples. The kNN baseline, logistic regression, and the 3-Layer NN + Dropoutwere trained on the labeled data only. Numbers reported for each method are the best of 3 runs(ranked by performance on the validation set). nl and nu are the number of labeled and unlabeleddata points.
Table 2: Accuracy of the FlowGMM, VAE model (M1+M2 VAE, Kingma et al., 2014), DIGLM(Nalisnick et al., 2019) in supervised and semi-supervised settings on MNIST, SVHN, and CIFAR-10. FlowGMM Sup (All labels) as well as DIGLM Sup (All labels) were trained on full traindatasets with all labels to demonstrate general capacity of these models. FlowGMM Sup (nl la-bels) was trained on nl labeled examples (and no unlabeled data). For reference, at the bottom welist the performance of the Î -Model (Laine & Aila, 2016) and BadGAN (Dai et al., 2017) as rep-resentative consistency-based and GAN-based state-of-the-art methods. Both of these methods usenon-invertible architectures with substantially higher base performance and, thus, are not directlycomparable.
Table 3:	Semi-supervised classification accuracy for FlowGMM-cons and VAE M1 + M2 model(Kingma et al., 2014) on MNIST for different number of labeled data points nl.
Table 4:	Negative log-likelihood and Expected Calibration Error for supervised FlowGMM trainedon MNIST (1k train, 1k validation, 10k test) and CIFAR-10 (50k train, 1k validation, 9k test).
