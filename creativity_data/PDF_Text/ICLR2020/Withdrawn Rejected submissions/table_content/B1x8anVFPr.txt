Table 1: Post-LN Transformer v.s. Pre-LN TransformerPost-LN Transformer	Pre-LN TransformerXpostj = MUItiHeadAmXpost, [xp1st, ∙∙	"PO"]) xpre,1 =LayerNOTm(Xpre)post,2	post	post,1 Xl,i	= Xl,i + Xl,i	Xpre,2 = MUItiHeadAtt(Xpre,1,唱型，…/片])Xlp,oist,3 = LayerNorm(Xlp,oist,2)	pre,3	pre	pre,2 Xl,i = Xl,i + Xl,iXlp,oist,4 = ReLU(Xlp,oist,3W 1,l + b1,l)W2,l + b2,l	Xlp,rie,4 = LayerNorm(Xlp,rie,3)	Xpost,5 = Xpost,3 + Xpost,4	Xlprie,5 = ReLU(Xlprie,4W 1,l + b1,l )W 2,l + b2,lXllp,+ois1t,i = Laly,ierNorm(lX,ilp,oist,5)	pre	pre,5	pre,3 Xl+i,i = Xl,i + Xl,i 	Final LayerNorm: XFnal i J LayerNorm(XLreI i)architecture for the Transformer and BERT (Vaswani et al., 2017; Devlin et al., 2018) follows “self-attention (FFN) sub-layer → residual connection → layer normalization”, which we call the Trans-former with Post-Layer normalization (Post-LN Transformer), as illustrated in Figure 1.
