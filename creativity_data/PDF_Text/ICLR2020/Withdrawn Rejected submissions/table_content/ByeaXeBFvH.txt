Table 1: Average absolute difference of model uncertainty and std. of several runs (N = 3) betweendistillation models and ensemble of models. Knowledge distillation (Hinton et al., 2015) was notreported, as it does not offer uncertainty quantification. Lower is better.
Table 2: Average test performance and std. over several runs (n = 3) for different baselines andHydra for MNIST. For all models, classification accuracy (ACC), negative log-likelihood (NLL),Brier score (BS) and model uncertainty (MU) are reported. For #Params and FLOPs, we also reportin brackets the percentage with respect to the ensemble. Bold numbers represent best performancewith respect to the specific evaluation metric (columns) across distillation models (rows) except forMU, where we report the model uncertainty closest to the ensemble one.
Table 3: Average test performance and std. over several runs (n = 3) for different baselines andHydra for CIFAR-10. For all models, classification accuracy (ACC), negative log-likelihood (NLL),Brier score (BS) and Model Uuncertainty (MU) are reported. Bold numbers represent best per-formance with respect to the specific evaluation metric (columns) across distillation models (rows)except for MU, where we report the model uncertainty closest to the ensemble one.
Table 4: UCI regression benchmark (Dua & Taniskidou, 2017). Average test negative log-likelihood(NLL) and std. over several runs (n = 3) of the 9 different datasets considered. As Prior Net-works (Malinin et al., 2019) cannot be applied to regression tasks, we denote this with ”N/A”.
Table 5: Model capacity and efficiency: We report number of model parameters (#Params) andnumber of floating point operations (FLOPs) for the ensemble and all distillation models. We alsoreport in brackets the percentage with respect to the ensemble.
