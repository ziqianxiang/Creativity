Table 1: Simulation scenario and Vehicle parametersThe agent is to drive into the roundabout from the 1st entrance and drive out in the 4th exit as fast aspossible. If there is no collision with other vehicles, it succeeds. The ego vehicle follows the policiescomputed the speed using the reinforcement learning algorithm, whereas other participants use thedefault car following model (Kraus) integrated in SUMO. Basically, the agent is to learn a speedprofile under the default lateral control strategy of SUMO.
Table 2: SSAC HyperparametersFor SSAC, there are two networks: the policy network and the Q network. For the policy network,we use a fully-connected MLP with two hidden layers of 256 units, outputting the mean and standarddeviations of a Gaussian distribution. For the Q network and the Lyapunov network, we use afully-connected MLP with two hidden layers of 256 units, outputting the Q value and the Lyapunovvalue. All the hidden layers use Relu activation function and we adopt the same invertible squashingfunction technique as Soft Actor-critic (SAC) Haarnoja et al. (2018) to the output layer of the policynetwork.
Table 3: CRM HyperparametersFor CRM, there are two networks: the decoder and the encoder. For the encoder, we use a fully-connected MLP with two hidden layers of 256 and 128 units repectively, outputting the mean andstandard deviations of a Gaussian distribution. For the decoder, we use a fully-connected MLP withtwo hidden layers of 128 and 256 units respectively, outputting the reconstruction of input. All thehidden layers use Sigmoid activation function.
