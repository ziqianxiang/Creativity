Table 1: CIFAR10: Comparison of EM, BBSL and RLLS (dirichlet shift). Value before the semicolon isthe average MSE in the estimated shift weights (as defined in Sec. 3.3). Value after the semicolon is the averagerank of a method relative to the others in the group that use the same calibration. α represents the dirichlet shiftparameter (larger α corresponds to less extreme shift), n represents the sample size for both the validation setand the label-shifted test set. A bold value in a group is not significantly different from the best-performingmethod in the group, as measured by a paired Wilcoxon test atp < 0.01. See Table E.2 for an equivalent tablebut with statistical comparisons done across all calibration methods. EM tends to outperform BBSL and RLLSwhen calibration techniques involving class-specific bias parameters are used.
Table 2: MNIST: Comparison of EM, BBSL and RLLS (“tweak-one” shift). Value before the semicolon isthe average MSE in the estimated shift weights. Value after semicolon is the average rank of a method relative toothers in the group that use the same calibration. A bold value in a group is not significantly different from thebest-performing method in the group, as measured by a paired Wilcoxon test atp < 0.01. See Table F.2 for anequivalent table but with statistical comparisons done across all calibration methods. EM tends to outperformBBSL and RLLS when calibration techniques involving class-specific bias parameters are used.
Table 3: CIFAR100: Comparison of EM, BBSL and RLLS (dirichlet shift). Value before the semicolon isthe avg. MSE in the estimated shift weights. Value after the semicolon is the avg. rank of a method relativeto the others in the group that use the same calibration. A bold value in a group is not significantly differentfrom the best-performing method in the group, as measured by a paired Wilcoxon test at p < 0.01. See TableG.1 for an equivalent table but with statistical comparisons done across all calibration methods. EM tends tooutperform BBSL and RLLS when calibration techniques involving class-specific bias parameters are used.
Table 4: Kaggle Diabetic Retinopathy: Comparison of EM, BBSL and RLLS. ρ represents proportion ofhealthy examples in shifted domain; source domain has ρ = 0.73. Value before semicolon is the averageMSE in the estimated shift weights. Value after the semicolon is the average rank of a method relative toothers in the group that use the same calibration. A bold value in a group is not significantly different from thebest-performing method in the group (paired Wilcoxon test atp < 0.01). See Table H.1 for an equivalent tablebut with statistical comparisons done across all calibration methods. EM tends to outperform BBSL and RLLSwhen calibration techniques involving class-specific bias parameters are used.
Table 5: CIFAR10: Comparison of calibration methods when using EM adaptation to dirichlet shift,with ∆%accuracy as the metric. Unlike BBSL and RLLS, the EM algorithm does not rely on retraining toproduce domain adapted probabilities. Value before the semicolon is the average change in %accuracy relativeto a baseline of no adaptation. Value after the semicolon is the average rank compared to other methods in thesame column. Bold values in a column are not significantly different from the best performing method in thecolumn, as measured by a paired Wilcoxon test at p ≤ 0.01. Calibration techniques involving class-specific biasparameters (namely BCTS and VS) tend to achieve the best performance.
Table 6: CIFAR100: Comparison of calibration methods when using EM adaptation to dirichlet shift,with ∆%accuracy as the metric. Unlike BBSL and RLLS, the EM algorithm does not rely on retraining toproduce domain adapted probabilities. Value before the semicolon is the average change in %accuracy relativeto a baseline of no adaptation. Value after the semicolon is the average rank compared to other methods in thesame column. Bold values in a column are not significantly different from the best performing method in thecolumn, as measured by a paired Wilcoxon test at p ≤ 0.01. Calibration techniques involving class-specific biasparameters (namely BCTS and VS) tend to achieve the best performance.
Table 7: Kaggle Diabetic Retinopathy: Comparison of calibration methods when using EM adaptationto domain shift, with ∆%accuracy as the metric. ρ represents proportion of healthy examples in shifteddomain; source distribution has ρ = 0.73. Unlike BBSL and RLLS, the EM algorithm does not rely on retrainingto produce domain adapted probabilities. Value before the semicolon is the average change in %accuracy relativeto a baseline of no adaptation. Value after the semicolon is the average rank compared to other methods in thesame column. Bold values in a column are not significantly different from the best performing method in thecolumn, as measured by a paired Wilcoxon test at p ≤ 0.01. Calibration techniques involving class-specific biasparameters (namely BCTS and VS) tend to achieve the best performance.
Table C.1: The strategy for computing EM source priors heavily affects domain adaptation ifprobabilities retain systematic bias. Value before the semicolon is the average improvement in%accuracy (across 100 trials) caused by applying domain adaptation to the predictions on a diabeticretinopathy prediction task. Value after the semicolon is the average rank of a particular methodrelative to the other method in the pair. Domain shift is induced by varying the proportion of “healthy”examples ρ; in the source distribution, ρ = 0.73. We see that calibration methods that lack class-specific bias parameters (i.e. no calibration, TS and NBVS) can hurt domain adaptation if sourcepriors are initialized by averaging true labels rather than the predicted probabilities. A bold value in apair is significantly better than the non-bold value according to a paired Wilcoxon test at p ≤ 0.01.
Table C.2: Similar to Table C.1, but using Jensen-Shannon Divergence as a metric to assess thequality of domain adaptation. See Sec. B for a description of hoW the Jensen-Shannon Divergencemetric is calculated.
Table D.1: CIFAR10: NLL and ECE for different calibration methods. Metrics were computedon a test set that had the same distribution as the validation set. Value before the semicolon is theaverage of the metric over all the runs. Value after the semicolon is the average rank of the methodrelative to other methods in the column. n indicates the number of examples used for calibratin. Boldvalues in a column are not significantly different from the best performing method in the column, asmeasured by a paired Wilcoxon test atp ≤ 0.01. See Sec. 4.1 for details on the experimental setup.
Table D.2: CIFAR100: NLL and ECE for different calibration methods. Analogous to TableD.1.
Table D.3: Kaggle Diabetic Retinopathy Detection: NLL and ECE for different calibrationmethods. Analogous to Table D.1.
Table E.1: CIFAR10: Comparison of calibration methods when using EM adaptation to“tweak-one” shift, with ∆%accuracy as the metric. Analogous to Table 5.
Table E.2: CIFAR10: Comparison of all calibration and domain adaptation methods, usingMSE (Sec. 3.3) as the metric (dirichlet shift). Value before the semicolon is the average of themetric over all trials. Value after the semicolon is the average rank of the domain adaptation +calibration method combination relative to the other method combinations in the column. Boldvalues in a column are not significantly different from the best-performing method in the column asmeasured by a paired Wilcoxon test at p < 0.01. EM with BCTS or VS tends to achieve the bestperformance. See Sec. 4.1 for details on the experimental setup.
Table E.3: CIFAR10: Comparison of all calibration and domain adaptation methods, usingMSE (Sec. 3.3) as the metric (“tweak-one” shift). Value before the semicolon is the average ofthe metric over all trials. Value after the semicolon is the average rank of the domain adaptation+ calibration method combination relative to the other method combinations in the column. Boldvalues in a column are not significantly different from the best-performing method in the column asmeasured by a paired Wilcoxon test at p < 0.01. EM with BCTS or VS tends to achieve the bestperformance. See Sec. 4.1 for details on the experimental setup.
Table E.4: CIFAR10: Comparison of all calibration and domain adaptation methods, using JSDivergence (Sec. B) as the metric (dirichlet shift). Value before the semicolon is the average ofthe metric over all trials. Value after the semicolon is the average rank of the domain adaptation+ calibration method combination relative to the other method combinations in the column. Boldvalues in a column are not significantly different from the best-performing method in the column asmeasured by a paired Wilcoxon test at p < 0.01. EM with BCTS or VS tends to achieve the bestperformance. See Sec. 4.1 for details on the experimental setup.
Table E.5: CIFAR10: Comparison of all calibration and domain adaptation methods, using JSDivergence (Sec. B) as the metric (“tweak-one” shift). Value before the semicolon is the averageof the metric over all trials. Value after the semicolon is the average rank of the domain adaptation+ calibration method combination relative to the other method combinations in the column. Boldvalues in a column are not significantly different from the best-performing method in the column asmeasured by a paired Wilcoxon test at p < 0.01. EM with BCTS or VS tends to achieve the bestperformance. See Sec. 4.1 for details on the experimental setup.
Table E.6: CIFAR10: Comparison of EM, BBSL and RLLS (dirichlet shift) using JS Diver-gence as the metric. Analogous to Table 1, but with JS Divergence as the metric rather thanMSE.
Table E.7: CIFAR10: Comparison of EM, BBSL and RLLS (“tweak-one” shift) using JS Diver-gence as the metric. Analogous to Table 1, but with tweak-one shift instead of dirichlet shift andJS Divergence as the metric rather than MSE.
Table E.8: CIFAR10: Comparison of EM, BBSL and RLLS (“tweak-one” shift) using MSE asthe metric. Analogous to Table 1, but with tweak-one shift instead of dirichlet shift.
Table F.1: MNIST: Comparison of all calibration and domain adaptation methods, using MSE(Sec. 3.3) as the metric (dirichlet shift). Value before the semicolon is the average of the metricover all trials. Value after the semicolon is the average rank of the domain adaptation + calibrationmethod combination relative to the other method combinations in the column. Bold values in acolumn are not significantly different from the best-performing method in the column as measured bya paired Wilcoxon test at p < 0.01. EM with BCTS or VS tends to achieve the best performance,particularly for larger amounts of shift (corresponding to smaller α). See Sec. 4.1 for details on theexperimental setup.
Table F.2: MNIST: Comparison of all calibration and domain adaptation methods, using MSE(Sec. 3.3) as the metric (“tweak-one” shift). Value before the semicolon is the average of themetric over all trials. Value after the semicolon is the average rank of the domain adaptation +calibration method combination relative to the other method combinations in the column. Boldvalues in a column are not significantly different from the best-performing method in the column asmeasured by a paired Wilcoxon test at p < 0.01. EM with BCTS or VS tends to achieve the bestperformance. See Sec. 4.1 for details on the experimental setup.
Table F.3: MNIST: Comparison of EM, BBSL and RLLS (dirichlet shift). Analogous to Table2, but with dirichlet shift rather than tweak-one shift.
Table G.1: CIFAR100: Comparison of all calibration and domain adaptation methods, usingMSE (Sec. 3.3) as the metric (dirichlet shift). Value before the semicolon is the average of themetric over all trials. Value after the semicolon is the average rank of the domain adaptation +calibration method combination relative to the other method combinations in the column. Boldvalues in a column are not significantly different from the best-performing method in the column asmeasured by a paired Wilcoxon test atp < 0.01. EM with VS tends to achieve the best performance,particularly for larger amounts of shift (corresponding to smaller α). See Sec. 4.1 for details on theexperimental setup.
Table G.2: CIFAR100: Comparison of all calibration and domain adaptation methods, usingJS Divergence (Sec. B) as the metric (dirichlet shift). Value before the semicolon is the averageof the metric over all trials. Value after the semicolon is the average rank of the domain adaptation+ calibration method combination relative to the other method combinations in the column. Boldvalues in a column are not significantly different from the best-performing method in the column asmeasured by a paired Wilcoxon test atp < 0.01. Although TS attains good performance according tothe JS Divergence metric, this is not the case with the MSE metric (Table G.1). See Sec. 4.1 fordetails on the experimental setup.
Table G.3: CIFAR100: Comparison of EM, BBSL and RLLS (dirichlet shift). Analogous toTable 3, but with JS Divergence (Sec. B) as the metric rather than Mean Squared Error.
Table H.1: Kaggle Diabetic Retinopathy Detection: Comparison of all calibration and domainadaptation methods, using MSE (Sec. 3.3) as the metric. ρ represents the porportion of healthyexamples in the sfhited domain; the source distribution has ρ = 0.73. Value before the semicolon isthe average of the metric over all trials. Value after the semicolon is the average rank of the domainadaptation + calibration method combination relative to the other method combinations in the column.
Table H.2: Kaggle Diabetic Retinopathy Detection: comparison of all calibration and domainadaptation methods, using MSE (Sec. 3.3) as the metric. Like Table H.1, but with JS Divergence(Sec. B) as the metric rather than MSE.
Table H.3: Kaggle Diabetic Retinopathy Detection: Comparison of EM, BBSL and RLLS.
Table I.1: CIFAR10: NLL vs ECE, ∆%Accuracy, dirichlet shift. Entry in “calibration method”column indicateS how the calibration method for any given run waS Selected: either according towhether it produced the beSt NLL or whether it produced the beSt ECE, where NLL and ECE werecalculated on the unShifted teSt Set. Value before the Semicolon iS the average change in %accuracyrelative to unadapted predictionS. Value after the Semicolon iS the average rank of the given metricrelative to the other metric in the pair. A bold value iS Significantly better than the non-bold value inthe pair uSing a paired Wilcoxon teSt atp ≤ 0.01. See Sec. 4.1 for detailS on the experimental Setup.
Table I.2: CIFAR10: NLL vs. ECE, metric: ∆%accuracy, “tweak-one” shift. AnalogouS toTable I.1. The “tweak-one” shift strategy is explained in Sec. 4.1.
Table I.3: CIFAR10: NLL vs. ECE, metric: JS Divergence, dirichlet shift. Analogous to TableI.1, but using JS Divergence (Sec. B) as the metric rather than change in %accuracy.
Table I.4: CIFAR10: NLL vs. ECE, metric: JS Divergence, “tweak-one” shift. Analogous toTable I.1.
Table I.5: CIFAR10: NLL vs. ECE, metric: MSE, dirichlet shift. Analogous to Table I.1, butusing MSE (Sec. 3.3) as the metric rather than change in %accuracy.
Table I.6: CIFAR10: NLL vs ECE, metric: MSE, “tweak-one” shift. Analogous to Table I.1.
Table I.7: CIFAR100: NLL vs ECE, metric: ∆%Accuracy, dirichlet shift. Analogous to TableI.1Shift Estimator	Calibration Method	α = 0.1			α = 1.0			α = 10.0				n=7000	n=8500	n=10000	n=7000	n=8500	n=10000	n=7000	n=8500	n=10000EM	Best NLL	0.113;0.31	0.112; 0.27	0.112; 0.29	0.111;0.17	0.108; 0.09	0.107; 0.1	0.108; 0.03	0.106; 0.03	0.104; 0.0EM	Best ECE	0.119; 0.69	0.119; 0.73	0.119; 0.71	0.118; 0.83	0.117; 0.91	0.116; 0.9	0.116; 0.97	0.114; 0.97	0.112; 1.0BBSL-soft	Best NLL	0.226;0.41	0.22; 0.35	0.215; 0.39	0.188;0.3	0.182; 0.31	0.177; 0.24	0.177;0.2	0.171; 0.2	0.165; 0.29BBSL-soft	Best ECE	0.226; 0.59	0.22; 0.65	0.216; 0.61	0.189; 0.7	0.183; 0.69	0.178; 0.76	0.178; 0.8	0.172; 0.8	0.166; 0.71RLLS-soft	Best NLL=	0.186;0.5	0.179; 0.42	0.176; 0.48	0.155; 0.49	0.149; 0.48	0.144; 0.51	0.148; 0.34	0.142; 0.46	0.137; 0.49RLLS-soft	Best ECE	0.186; 0.5	0.179; 0.58	0.176; 0.52	0.155; 0.51	0.149; 0.52	0.144; 0.49	0.149; 0.66	0.142; 0.54	0.137; 0.51Table I.8: CIFAR100: NLL vs ECE, metric: JS Divergence, dirichlet shift. Analogous to TableI.1Shift I Calibration ∣	α = 0.1	∣	α = 1.0	∣	α = 10.0Estimator ∣ Method ∣	n=7000	n=8500	n=10000~∣	n=7000	n=8500	n=10000~∣	n=7000	n=8500	n=10000--EM	I	BestNLL~I	0.1994;0.37	0.2011; 0.36^^0.20436;0.35	I	0.13788;0.22^^0.1307; 0.23^^0.12736;0.26	I	0.10309;0.2	0.09864; 0.2^^0.09667;0.17EM	I	BestECE	∣	0.28904;0.63	0.27676;0.64	0.26944;0.65	∣	0.15848;0.78	0.14828;0.77	0.14304;0.74	∣	0.11248;0.8	0.10512;0.8	0.10192;0.83BBSL-soft I	BestNLL~I 0.94791;0.36^^0.66421;0.36^^0.57766;0.37 ∣ 0.23665;0.24^^0.18917; 0.23^^0.16374; 0.2 I 0.15332; 0.24^^0.11667; 0.23^^0.09866; 0.1BBSL-soft	I	BestECE	∣	1.01696;0.64	0.69643; 0.64	0.60503;0.63	∣	0.24203; 0.76	0.19391; 0.77	0.16837; 0.8	∣	0.1567;0.76	0.11969;0.77	0.10204;0.9RLLS-soft_I	BestNLL~I 0.64403;0.5~0.52134; 0.54^^0.47947;0.54 I 0.1941;0.48~0.15799; 0.55~0.1352;0.43 I 0.11958;0.39~0.0966; 0.45~0.08386; 0.27RLLS-soft	I	BestECE	∣	0.65047; 0.5	0.52242; 0.46	0.48347; 0.46	∣	0.19225; 0.52	0.15747; 0.45	0.13543;0.57	∣	0.12059; 0.61	0.09732;0.55	0.08476; 0.73
Table I.8: CIFAR100: NLL vs ECE, metric: JS Divergence, dirichlet shift. Analogous to TableI.1Shift I Calibration ∣	α = 0.1	∣	α = 1.0	∣	α = 10.0Estimator ∣ Method ∣	n=7000	n=8500	n=10000~∣	n=7000	n=8500	n=10000~∣	n=7000	n=8500	n=10000--EM	I	BestNLL~I	0.1994;0.37	0.2011; 0.36^^0.20436;0.35	I	0.13788;0.22^^0.1307; 0.23^^0.12736;0.26	I	0.10309;0.2	0.09864; 0.2^^0.09667;0.17EM	I	BestECE	∣	0.28904;0.63	0.27676;0.64	0.26944;0.65	∣	0.15848;0.78	0.14828;0.77	0.14304;0.74	∣	0.11248;0.8	0.10512;0.8	0.10192;0.83BBSL-soft I	BestNLL~I 0.94791;0.36^^0.66421;0.36^^0.57766;0.37 ∣ 0.23665;0.24^^0.18917; 0.23^^0.16374; 0.2 I 0.15332; 0.24^^0.11667; 0.23^^0.09866; 0.1BBSL-soft	I	BestECE	∣	1.01696;0.64	0.69643; 0.64	0.60503;0.63	∣	0.24203; 0.76	0.19391; 0.77	0.16837; 0.8	∣	0.1567;0.76	0.11969;0.77	0.10204;0.9RLLS-soft_I	BestNLL~I 0.64403;0.5~0.52134; 0.54^^0.47947;0.54 I 0.1941;0.48~0.15799; 0.55~0.1352;0.43 I 0.11958;0.39~0.0966; 0.45~0.08386; 0.27RLLS-soft	I	BestECE	∣	0.65047; 0.5	0.52242; 0.46	0.48347; 0.46	∣	0.19225; 0.52	0.15747; 0.45	0.13543;0.57	∣	0.12059; 0.61	0.09732;0.55	0.08476; 0.73Table I.9: CIFAR100: NLL vs ECE, metric: MSE, dirichlet shift. Analogous to Table I.121Under review as a conference paper at ICLR 2020Shift Estimator	Calibration Method	P = 0.5			ρ = 0.9				n=500	n=1000	n=1500	n=500	n=1000	n=1500EM	Best NLL	3.79; 0.21	4.315; 0.26 二	4.543; 0.19	3.548; 0.02	3.57; 0.0	3.746; 0.02 2.405; 0.98EM	Best ECE	3.49; 0.79	4.099; 0.74	4.179; 0.81	2.074; 0.98	3.57; 1.0	Table I.10:	KaggleDR: NLL vs ECE, metric: ∆%Accuracy. Shift strategy modifies the proportionof healthy examples. Analogous to Table I.1Shift Estimator	Calibration Method	ρ = 0.5			ρ = 0.9		
Table I.9: CIFAR100: NLL vs ECE, metric: MSE, dirichlet shift. Analogous to Table I.121Under review as a conference paper at ICLR 2020Shift Estimator	Calibration Method	P = 0.5			ρ = 0.9				n=500	n=1000	n=1500	n=500	n=1000	n=1500EM	Best NLL	3.79; 0.21	4.315; 0.26 二	4.543; 0.19	3.548; 0.02	3.57; 0.0	3.746; 0.02 2.405; 0.98EM	Best ECE	3.49; 0.79	4.099; 0.74	4.179; 0.81	2.074; 0.98	3.57; 1.0	Table I.10:	KaggleDR: NLL vs ECE, metric: ∆%Accuracy. Shift strategy modifies the proportionof healthy examples. Analogous to Table I.1Shift Estimator	Calibration Method	ρ = 0.5			ρ = 0.9				n=500	n=1000	n=1500	n=500	n=1000	n=1500EM	Best NLL=	0.11;0.42	0.093;0.31 二	0.079; 0.33	0.078; 0.08	0.062; 0.0	0.059; 0.07EM	Best ECE	0.104; 0.58	0.092; 0.69	0.079; 0.67	0.11;0.92	0.062; 1.0	0.102; 0.93BBSL-soft	Best NLL=	0.166; 0.37	0.12;0.32 =	0.096; 0.31	0.107; 0.24	0.079; 0.0	0.077; 0.32BBSL-soft	Best ECE	0.158;0.63	0.123;0.68	0.101; 0.69	0.125;0.76	0.079; 1.0	0.086; 0.68RLLS-soft	Best NLL=	0.128;0.44 二	0.112;0.38 二	0.093; 0.31	0.089; 0.38	0.079; 0.0	0.077; 0.38RLLS-soft	Best ECE	0.123; 0.56	0.109; 0.62	0.098; 0.69	0.092; 0.62	0.079; 1.0	0.085; 0.62Table I.11:	KaggleDR: NLL vs ECE, metric: JS Divergence. Shift strategy modifies the proportionof healthy examples. Analogous to Table I.1Shift Estimator	Calibration Method	ρ = 0.5			ρ = 0.9		
