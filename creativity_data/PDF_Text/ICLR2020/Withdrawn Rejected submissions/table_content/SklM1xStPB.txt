Table 1: Evaluation of models on the code repair task. Given an input code snippet, each model needsto predict a corrected version of that code snippet. “Structural Match” indicates that the generatedoutput is identical to the target output up to renaming the identifiers (i.e., variables, functions).
Table 2: Span-based Correction (Bryant et al., 2017): Evaluation on Grammar Error Correction(GEC) Task. Note that our models use no pretraining, spell checking or other external data, whichare commonly used in GEC tasks.
Table 3: Evaluation of models on the edit representation tasks of Yin et al. (2019).
Table 4: C# Fixer Accuracy (%) in the One-Shot Generation Task	Copy+Seq2Seq		SeqCopySpan		@ 1	@ 5	@1	@ 5CA2007	16.8	24.4	36.9	46.5IDE0004	14.8	20.8	23.5	33.6RCS1015	24.0	25.3	23.9	26.8RCS1021	1.8	4.4	7.8	16.8RCS1032	1.8	2.7	2.5	3.7RCS1058	20.6	20.9	19.9	22.7RCS1077	3.2	3.9	4.5	5.8RCS1089	59.8	59.8	59.8	59.9RCS1097	1.6	3.7	14.9	27.7RCS1118	45.1	69.6	46.0	55.6RCS1123	15.8	19.5	27.7	22.7RCS1146	12.2	16.5	19.7	31.5RCS1197	1.1	1.8	1.7	2.3RCS1202	6.5	8.4	11.6	23.3RCS1206	34.9	35.0	36.2	37.5RCS1207	2.1	4.2	5.0	8.2Table 5: Indicative evaluation of models on CNN/DM summarization.
Table 5: Indicative evaluation of models on CNN/DM summarization.
