Table 1: The unified span-relation model can work on multiple NLP tasks, in contrast to previousworks usually designed for a subset of tasks.
Table 2b: Relation-oriented tasks. Directed arcs indicatethe relations between spans.
Table 2a: Span-oriented tasks. Spans are annotatedby underlines and their labels.
Table 3: Statistics of the GLAD benchmark consisting of 10 tasks from 8 datasets. ? Following Heet al. (2018), we use a subset of OntoNotes 5.0 dataset based on CoNLL 2012 splits (Pradhan et al.,2012). ◦ Previous works use gold standard spans in these evaluations. * We use standard bracketscoring program Evalb (Collins, 1997) in constituency parsing.
Table 4: Comparison between the SpanRel model and the task-specific state-of-the-art models.3Following Luan et al. (2019), we perform NER and RE jointly on WLP dataset. We use gold entitiesin SemEval-2010 Task 8, gold aspect terms in SemEval-2014 Task 4, and gold opinion expressionsin MPQA 3.0 to be consistent with existing works.
Table 5: Comparison between STL and MTL+fine-tuning of the SpanRel model across all tasks.
Table 6: Performance of pairwise multi-task learning with GloVe andBERTbase. blue↑ indicates results better than STL, red^ indicates worse, andblack means almost the same (i.e., a difference within 0.5). We show the per-formance after fine-tuning. Dataset of source tasks POS, Consti., Dep. is PTBand dataset of NER is CoNLL-2003.
Table 7: Single-task learning performance of the SpanRel model with different token representa-tions. BERTlarge requires a large amount of memory so we cannot feed the entire document to themodel in coreference resolution.
Table 8: Task-specific hyperparameters. Span-oriented tasks do not need pruning ratio.
