Table 1: Improvement of knowledge distillation (KD) with the proposed structure on CIFAR 100Netl	Net2	independent		KD	ours	PendResNet 32	VGG 13	74.08	67.74	68.83	71.12	0.8ResNet 110	ResNet 20	71.69	69.86	70.12	73.36	0.6WRN 28-10	ResNet 32	78.98	69.86	69.85	74.87	0.5MobileNet	ResNet 32	74.08	69.89	69.88	71.77	0.7ResNet 110	ResNet 32	71.69	69.86	70.12	73.36	0.7MobileNet	VGG 13	74.08	67.74	68.83	71.12	0.7Table 2: Improvement of attention transfer (AT) with the proposed method on CIFAR 100Netl	Net2	independent		AT	ours	PendResNet 110	ResNet 20	71.69	68.32	68.34	68.67	0.6WRN 28-10	ResNet 32	78.98	69.86	69.87	70.64	0.7ResNet 110	ResNet 32	71.69	69.86	70.22	71.23	0.7WRN 40-4	ResNet 32	75.67	69.86	70.03	70.59	0.7WRN 28-10	WRN 40-4	78.98	75.67	75.36	76.09	0.7Table 3: Improvement of mutual learning (ML) with the proposed structure on CIFAR 100Netl	Net2	independent		ML		ours		PendResNet 32	ResNet 32	69.86	69.86	71.14	71.21	73.68	73.58	0.9MobileNet	ResNet 32	74.08	69.86	75.62	71.1	76.2	72.76	0.8WRN 28-10	ResNet 32	78.98	69.86	78.53	72.18	80.65	73.08	0.5
Table 2: Improvement of attention transfer (AT) with the proposed method on CIFAR 100Netl	Net2	independent		AT	ours	PendResNet 110	ResNet 20	71.69	68.32	68.34	68.67	0.6WRN 28-10	ResNet 32	78.98	69.86	69.87	70.64	0.7ResNet 110	ResNet 32	71.69	69.86	70.22	71.23	0.7WRN 40-4	ResNet 32	75.67	69.86	70.03	70.59	0.7WRN 28-10	WRN 40-4	78.98	75.67	75.36	76.09	0.7Table 3: Improvement of mutual learning (ML) with the proposed structure on CIFAR 100Netl	Net2	independent		ML		ours		PendResNet 32	ResNet 32	69.86	69.86	71.14	71.21	73.68	73.58	0.9MobileNet	ResNet 32	74.08	69.86	75.62	71.1	76.2	72.76	0.8WRN 28-10	ResNet 32	78.98	69.86	78.53	72.18	80.65	73.08	0.5MobileNet	MobileNet	74.08	74.08	75	75.16	75.5	76.1	0.9WRN 28-10	MobileNet	78.98	74.08	78.83	76.41	81.03	76.82	0.5WRN 28-10	WRN 28-10	78.98	78.98	78.83	78.95	81	80.66	0.5Table 4: Improvement of knowledge distillation (KD) with the proposed structure on tiny imagenetNetl	Net2	independent		KD	ours	PendResNet 32	VGG 13	49.01	44.61	55.76	57.56	0.9ResNet 32	ResNet 20	49.01	46.85	49.57	50.6	0.9MobileNet	ResNet 20	55.38	46.85	51.8	52.15	0.7
Table 3: Improvement of mutual learning (ML) with the proposed structure on CIFAR 100Netl	Net2	independent		ML		ours		PendResNet 32	ResNet 32	69.86	69.86	71.14	71.21	73.68	73.58	0.9MobileNet	ResNet 32	74.08	69.86	75.62	71.1	76.2	72.76	0.8WRN 28-10	ResNet 32	78.98	69.86	78.53	72.18	80.65	73.08	0.5MobileNet	MobileNet	74.08	74.08	75	75.16	75.5	76.1	0.9WRN 28-10	MobileNet	78.98	74.08	78.83	76.41	81.03	76.82	0.5WRN 28-10	WRN 28-10	78.98	78.98	78.83	78.95	81	80.66	0.5Table 4: Improvement of knowledge distillation (KD) with the proposed structure on tiny imagenetNetl	Net2	independent		KD	ours	PendResNet 32	VGG 13	49.01	44.61	55.76	57.56	0.9ResNet 32	ResNet 20	49.01	46.85	49.57	50.6	0.9MobileNet	ResNet 20	55.38	46.85	51.8	52.15	0.7MobileNet	ResNet 32	55.38	49.01	54.48	54.85	0.8MobileNet	ResNet 110	55.38	52.32	58.15	58.2	0.9WRN 28-10	ResNet 32	58.91	49.01	55.7	55.34	0.6Experiment	imagenet (Russakovsky et al. 2015). CIFAR-100 datasetDataset and Simulation SettingWe evaluate the proposed method with two datasets -CIFAR-100 (Krizhevsky, Hinton, and others 2009) and tiny
Table 4: Improvement of knowledge distillation (KD) with the proposed structure on tiny imagenetNetl	Net2	independent		KD	ours	PendResNet 32	VGG 13	49.01	44.61	55.76	57.56	0.9ResNet 32	ResNet 20	49.01	46.85	49.57	50.6	0.9MobileNet	ResNet 20	55.38	46.85	51.8	52.15	0.7MobileNet	ResNet 32	55.38	49.01	54.48	54.85	0.8MobileNet	ResNet 110	55.38	52.32	58.15	58.2	0.9WRN 28-10	ResNet 32	58.91	49.01	55.7	55.34	0.6Experiment	imagenet (Russakovsky et al. 2015). CIFAR-100 datasetDataset and Simulation SettingWe evaluate the proposed method with two datasets -CIFAR-100 (Krizhevsky, Hinton, and others 2009) and tinyTable 5: Improvement of attention transfer (AT) with the proposed method on tiny imagenetNetl	Net2	independent		AT	ours	PendResNet 110	ResNet 20	52.32	46.85	51.49	51.9	0.6WRN 28-10	ResNet 32	58.91	49.01	53.56	54.15	0.7ResNet 110	ResNet 32	52.32	49.01	54.52	54.91	0.8WRN 40-4	ResNet 32	55.19	49.01	54.33	54	0.7WRN 28-10	WRN 40-4	58.91	55.19	60.98	61.36	0.8Table 6: Comparison
Table 5: Improvement of attention transfer (AT) with the proposed method on tiny imagenetNetl	Net2	independent		AT	ours	PendResNet 110	ResNet 20	52.32	46.85	51.49	51.9	0.6WRN 28-10	ResNet 32	58.91	49.01	53.56	54.15	0.7ResNet 110	ResNet 32	52.32	49.01	54.52	54.91	0.8WRN 40-4	ResNet 32	55.19	49.01	54.33	54	0.7WRN 28-10	WRN 40-4	58.91	55.19	60.98	61.36	0.8Table 6: ComparisonNetl	Net2	independent		partial	full	Pend	WRN 28-10	ResNet 32	78.98	69.86	74.81	74.87	0.5	0.5mob	ResNet 32	74.08	69.86	70.3	71.77	0.9	0.7consists of 32 × 32 RGB color images drawn from 100classes, which are split into 50, 000 train and 10, 000 testimages. Tiny imagenet dataset is a down-sampled version ofImageNet dataset. It consists of 64 × 64 RGB color imagesdrawn from 200 classes, which are split into 100, 000 trainand 10, 000 test images.
Table 6: ComparisonNetl	Net2	independent		partial	full	Pend	WRN 28-10	ResNet 32	78.98	69.86	74.81	74.87	0.5	0.5mob	ResNet 32	74.08	69.86	70.3	71.77	0.9	0.7consists of 32 × 32 RGB color images drawn from 100classes, which are split into 50, 000 train and 10, 000 testimages. Tiny imagenet dataset is a down-sampled version ofImageNet dataset. It consists of 64 × 64 RGB color imagesdrawn from 200 classes, which are split into 100, 000 trainand 10, 000 test images.
