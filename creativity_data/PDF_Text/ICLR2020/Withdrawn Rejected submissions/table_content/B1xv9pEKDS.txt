Table 1: Results of LightPAFF on BERT tasks. “BERT” represents the BERT-base model releasedby Devlin et al. (2018) and is taken as the teacher model, “Patient KD” distills BERT-base model intoTransformer model during fine-tuning, “Distilled BiLSTM“ distills BERT-large model into LSTMmodel during fine-tuning, “Transformer small” and “BERT small” share the same model structureand number of parameters, with the only difference that “Transformer small” has no BERT pre-training, “LightPAFF” is our method applied on “BERT small”.
Table 2: Inference speedup of LightPAFF. Evaluation is conducted on NVIDIA Tesla 16GB V100GPU or Intel(R) Xeon(R) Platinum 8168 CPU, with batch size of 1 sentence and max sequencelength of 128 tokens.
Table 3: Results of LightPAFF on GPT-2 tasks in terms of perplexity (ppl). “w/ FT” and “w/oFT” mean the pre-trained model is evaluated on downstream tasks with and without fine-tuning.
Table 4: Inference speedup of LightPAFF on GPT-2. Evaluation is conducted with batch size of 1sentence and max sequence length of 512 tokens.The GPU/CPU configurations follow that in lastSection.
Table 5: Results of lightPAFF on sequence to sequence based language generation tasks in termsof BLEU score. “MASS” represents the MASS model released by Song et al. (2019) and is takenas the teacher model, “Transformer small” and “MASS small” share the same model structure andnumber of parameters, with the only difference that “Transformer small” has no MASS pre-training,“LightPAFF” is our method applied on “MASS small”.
Table 6: Inference speedup LightPAFF on MASS. Evaluation is conducted by generating one sen-tence at a time autoregressively. The GPU/CPU configurations follow that in last Section.
Table 7: Ablation study on knowledge distillation in pre-training and fine-tuning stages, where “PT”means pre-training, “FT” means fine-tuning and “KD” means knowledge distillation. The results ofBERT/GPT-2/MASS are the accuracy on SST-2 task, the perplexity on WikiText-2, and the BLEUscore on WMT16 En-De (100K) respectively.
Table 8: The comparison of accuracy, and average number of input tokens in the pre-training taskbetween BERT, GPT-2 and MASS.
Table 9: Statistics of the three datasets of language modeling tasks in GPT-2.
Table 10: The accuracy of LightPAFF with different number of parameters on PolyDis task. We alsolist the results of the original BERT model with 110M parameters.
Table 11: The accuracy comparison of LightPAFF without and with unlabeled data on the BERTfine-tuning tasks.
