Table 1: Test samples and their perturbed versions. See more examples in Appendix A.
Table 2: Pilot Study I. Note that against the certified defenses of Raghunathan et al. (2018) and Kolter& Wong (2017), Song et al. (2018) achieved (manual) success rates of 86.6% and 88.6%.
Table 3: Pilot Study I. The adversarial images are generated against the adversarially trained Resnets.					Questionnaire	OUR Method	SONG et al. (2018)	Zhao et al. (2018b)		PGDQuestion Q1: Yes	100 %	85.9 %		97.8 %	76.7 %Question Q2: Yes	100 %	79.3 %		89.7 %	66.8 %Question Q3: NO	100 %	71.8 %		94.6 %	42.7 %Pilot Study II - SNLI. Using the transpose CNN as decoder pφ, we generate adversarial hypothesesfor the SNLI sentence pairs with the premises kept unchanged. Then, we select manually 20 pairs ofclean sentences (premise, hypothesis), and adversarial hypotheses. We also pick 20 pairs of sentencesand adversarial hypotheses generated this time using Zhao et al. (2018b)’s method against theirtreeLSTM classifier. We choose this classifier as its accuracy (89.04%) is close to ours (89.42%).
Table 4: Pilot Studies. * Some adversarial images and original ones were found blurry to evaluate.
Table 5:	CelebA samples, their clean reconstructions, and adversarial examples (in red boxes).
Table 6:	SVHN. Images in red boxes are all adversarial.
Table 7: MNIST. Images in red boxes are all adversarial.
Table 8: Examples of adversarially generated hypotheses with the true premises kept unchanged.
Table 9: Some generated examples deemed adversarial by our method that are not.
Table 10: Model Configurations + SNLI Classifier + Hyper-parameters.
