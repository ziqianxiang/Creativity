Table 1: Performance comparison at one million samples. Last column shows percentage improve-ment of SOP+ERE over SAC.
Table 2: SOP HyperparametersParameter	ValueShared optimizer learning rate discount (γ) target smoothing coefficient (ρ) target update interval replay buffer size number of hidden layers for all networks number of hidden units per layer mini-batch size nonlinearity	Adam (Kingma & Ba, 2014) 3∙10-4 0.99 0.005 1 106 2 256 256 ReLU	SAC adaptive entropy target	dim(A) (e.g., 6 for HalfCheetah-v2)SOP gaussian noise std σ = σι = σ2	0.29	TD3 gaussian noise std for data collection σ guassian noise Std for target policy smoothing σ	0.1 * action limit 0.2	TD3+ gaussian noise std for data collection σ guassian noise Std for target policy smoothing σ	0.15 0.2	ERE ERE initial no	0.995	PER PER β1 (α in PER paper) PER β2 (β in PER paper)	0.4 0.4	EXP Exponential λ	5e - 0613Under review as a conference paper at ICLR 2020C ERE PseudocodeAlgorithm 2 SOP with Emphasizing Recent Experience1:	Input: initial policy parameters θ, Q-function parameters φ1, φ2, empty replay buffer D of sizeN, initial η0, recent and max performance improvement Irecent = Imax = 0.
