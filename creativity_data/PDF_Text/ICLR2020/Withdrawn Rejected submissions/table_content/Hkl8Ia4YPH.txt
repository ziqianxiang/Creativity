Table 1: Description of the datasets used for experimentsDataset	# Articles	Expansion Ratio	Summary Length	Article LengthCNN/DM	312,085	13.0	55.6	789.9BigPatent	1,341,362	36.4	116.5	3572.83.2	Evaluation metricsIn our experiments, we employ the following automated metrics for evaluating the performance ofcompared models.
Table 2: Performance on automated metrics of different models. For perplexity, the lower, the better.
Table 3: Perplexity of ground-truth articles. PPL(S→A) denotes the perplexity by the conventionalseq2seq model with input summary. PPL(K→A) denotes the perplexity by the hierarchical modelwith generated sketch.
Table 4: Human evaluation results. Pairing accuracy measures the relevance of generated articles.
Table 5: Sample-level Pearson correlation score of different automated metrics with human evaluation.
Table 6: Ablation study of training strategies and the influence of the length of sketch. PPL(K*→A)is the perplexity of ground-truth articles generated based on extracted “oracle” sketches. 0.5× and2× denote the length of the sketch for training, compared with that of the geometric mean of thesummary length and the article length, which is used in our model.
