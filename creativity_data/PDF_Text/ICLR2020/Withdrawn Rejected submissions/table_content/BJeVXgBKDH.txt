Table 1: Automatic evaluation results on the SQuAD dataset. For each metric, best result is boldedModel	BLEU-1	BLEU-2	BLEU-3	BLEU-4	ROUGE-LSeq2Seq + att + AE	35.90	23.52	15.15	10.10	-3105-TransSeq2Seq + AE	24.90	15.32	9.46	6.13	30.76HierSeq2Seq + AE	38.08	25.33	16.48	11.13	-32.82-HierTransSeq2Seq + AE	31.49	20.05	12.60	8.68	31.88Table 2: Automatic evaluation results on the MS MARCO dataset. For each metric, best result isboldedModel	Syntax		Semantics		Relevance		Score	Kappa	Score	Kappa	Score	KappaSeq2Seq + att + AE	86	0.57	79.33	0.61	70.66	0.56TransSeq2Seq + AE	86	0.66	84	0.62	50	0.60HierSeq2Seq + AE	80	0.49	73.33	0.54	81.33	0.64HierTransSeq2Seq + AE	90	0.62	85.33	0.65	68	0.56Table 3: Human evaluation results (column “Score”) as well as inter-rater agreement (column“Kappa”) for each model on the SQuAD test set. The scores are between 0-100, 0 being the worstand 100 being the best. Best results for each metric (column) are bolded. The three evaluationcriteria are: (1) syntactically correct (Syntax), (2) semantically correct (Semantics), and (3) relevantto the text (Relevance).
Table 2: Automatic evaluation results on the MS MARCO dataset. For each metric, best result isboldedModel	Syntax		Semantics		Relevance		Score	Kappa	Score	Kappa	Score	KappaSeq2Seq + att + AE	86	0.57	79.33	0.61	70.66	0.56TransSeq2Seq + AE	86	0.66	84	0.62	50	0.60HierSeq2Seq + AE	80	0.49	73.33	0.54	81.33	0.64HierTransSeq2Seq + AE	90	0.62	85.33	0.65	68	0.56Table 3: Human evaluation results (column “Score”) as well as inter-rater agreement (column“Kappa”) for each model on the SQuAD test set. The scores are between 0-100, 0 being the worstand 100 being the best. Best results for each metric (column) are bolded. The three evaluationcriteria are: (1) syntactically correct (Syntax), (2) semantically correct (Semantics), and (3) relevantto the text (Relevance).
Table 3: Human evaluation results (column “Score”) as well as inter-rater agreement (column“Kappa”) for each model on the SQuAD test set. The scores are between 0-100, 0 being the worstand 100 being the best. Best results for each metric (column) are bolded. The three evaluationcriteria are: (1) syntactically correct (Syntax), (2) semantically correct (Semantics), and (3) relevantto the text (Relevance).
Table 2:7Under review as a conference paper at ICLR 2020Model	Syntax		Semantics		Relevance		Score	Kappa	Score	Kappa	Score	KappaSeq2Seq + att + AE	83.33	0.68	69.33	0.65	38.66	0.52TranSSeq2Seq + AE	80.66	0.73	73.33	0.55	35.33	0.47HierSeq2Seq + AE	85.33	0.73	70.66	0.68	51.33	0.60HierTranSSeq2Seq + AE	86	0.87	73.33	0.65	32.66	0.60Table 4: Human evaluation results (column “Score”) as well as inter-rater agreement (column“Kappa”) for each model on the MS MARCO test set. The scores are between 0-100, 0 beingthe worst and 100 being the best. Best results for each metric (column) are bolded. The three eval-uation criteria are: (1) syntactically correct (Syntax), (2) semantically correct (Semantics), and (3)relevant to the text (Relevance).
Table 4: Human evaluation results (column “Score”) as well as inter-rater agreement (column“Kappa”) for each model on the MS MARCO test set. The scores are between 0-100, 0 beingthe worst and 100 being the best. Best results for each metric (column) are bolded. The three eval-uation criteria are: (1) syntactically correct (Syntax), (2) semantically correct (Semantics), and (3)relevant to the text (Relevance).
