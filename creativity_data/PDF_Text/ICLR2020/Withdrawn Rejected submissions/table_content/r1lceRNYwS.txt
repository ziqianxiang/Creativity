Table 1: Top-1 transfer learning accuracy (%) on the Kinetics and ImageNet validation sets. “Randommeans a randomly initialized ResNet-18 without any training.
Table 2: Top-1 finetuning results on UCF101 and HMDB51 datasets using models pretrainedon Kinetics. We report results under two data augmentation pipelines, where “Multi-scale Aug.”represents the pipeline used in Hara et al. (2018) and “Color-noise Aug.” is ours used during Kineticstraining. We also provide performance of training from scratch (“Scratch”) and from supervisedlytrained models on Kinetics (“Supervised”). For 3DRotNet, we compare to its model trained withRGB inputs, where 64-RGB means 64 RGB frames. 1: AleXNet results are all pretrained on UCF101.
Table 3: Top-1 accuracy (in %) of transfer learning to Kinetics and ImageNet from VIE-Singlemodels trained using different amount of videos or with videos cut into different number of bins.
Table 4: Top-1 transfer learning accuracy (%) on Kinetics for Input-Single models.
Table 5: Top-1 transfer learning accuracy (%) on ImageNet for supervised Kinetics modelsModels	Conv3	Conv4	Conv5Supervised-Single	22.32	37.82	38.26Supervised-TRN	22.82	41.13	39.15Supervised-3DResNet	28.09	34.40	30.56Supervised-Slow	21.86	40.77	32.87Supervised-SlowFast	20.25	37.41	30.75our random crop chooses from more spatial location and resolution choices, although we are lack ofan aspect ratio change. In testing for both pipelines, each video is split into consecutive 16-frameclips and the outputs of all clips are averaged to get the final prediction. As for other parameters, theinitial learning rate is 0.01 and the weight decay is 1e-4 for the training from scratch. For finetuning,the initial learning rate is 0.0005 and the weight decay is 1e-5. The learning rate is dropped by 10after validation performance saturates. We report the results on the first split for both UCF101 andHMDB51, which should be close to the 3-split average result.
