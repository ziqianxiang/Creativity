Table 1: Hyperparameters used for DDQN and AG-DDQN resultsHyperparameter	ValueBatch size	32Replay buffer size	1,000,000Discount rate	0.99Steps per learning update	4Learning iterations per round	1Learning rate	0.0005 —Optimizer	AdamWeight Initialiser	HeMin Epsilon	0.1Epsilon decay steps	1,000,000Fixed network update frequency	10000 —Loss	HuberClip rewards	Clipto [-1,+1]Initial random steps	25,000 —The architecture and hyperparameters used for the AG-DDQN results that are relevant to DDQN arethe same as for DDQN and then the rest of the hyperparameters are given by Table 2.
Table 2: Hyperparameters used for AG-DDQN resultsHyperparameter	Value	DescriptionEvaluation episodes	5	The number of no explo- ration episodes we use to in- fer the action grammarReplay buffer type	Action balanced	The type of replay buffer We useSteps before inferring grammar	75,001	The number of steps we run before inferring the action grammarAbandon ship	1.0	The threshold for the aban- don ship algorithmMacro-action exploration bonus	4.0	How much more likely we are to pick a macro-action than a primitive action when acting randomly12Under review as a conference paper at ICLR 2019C AG-SAC HyperparametersThe hyperparameters used for the discrete SAC results are given by Table 3. The network architec-ture for both the actor and the critic was the same as in the original Deepmind Atari paper (Mnihet al., 2015).
Table 3: Hyperparameters used for SAC and AG-SAC resultsHyperparameter	ValueBatch size	64Replay buffer size	1,000,000Discount rate	0.99Steps per learning update	4Learning iterations per round	1Learning rate	0.0003Optimizer	AdamWeight Initialiser	HeFixed network update frequency	8000Loss	Mean squared errorClip rewards	-Clipto [-1,+1]Initial random steps	20,000The architecture and hyperparameters used for the AG-SAC results that are relevant to SAC are thesame as for SAC and then the rest of the hyperparameters are given by Table 4.
Table 4: Hyperparameters used for AG-SAC resultsHyperparameter	Value	DescriptionEvaluation episodes	5	The number of no explo- ration episodes we use to in- fer the action grammarReplay buffer type	Action balanced	The type of replay buffer We useSteps before inferring grammar	30,001	The number of steps we run before inferring the action grammarAbandon ship	2.0	The threshold for the aban- don ship algorithmMacro-action exploration bonus	4.0	How much more likely we are to pick a macro-action than a primitive action when acting randomlyPost inference random steps	5,000	The number of random steps we run immediately after updating the action set with new macro-actions13Under review as a conference paper at ICLR 2019D SAC, AG-SAC and Rainb ow Atari ResultsBelow we provide all SAC, AG-SAC and Rainbow results after running 100,000 iterations for 5random seeds. We see that AG-SAC improves over SAC in 19 out of 20 games (median improvementof 96%, maximum improvement of 3756%) and that AG-SAC improves over Rainbow in 17 out of20 games (median improvement 62%, maximum improvement 13140%).
Table 5: SAC, AG-SAC and Rainbow results on 20 Atari games. The mean result of 5 random seeds is shownwith the standard deviation in brackets. As a benchmark we also provide a column indicating the score an agentwould get if it acted purely randomly.
Table 6: The lengths of moves attempted and executed in the best performing AG-SAC seed for each game.
