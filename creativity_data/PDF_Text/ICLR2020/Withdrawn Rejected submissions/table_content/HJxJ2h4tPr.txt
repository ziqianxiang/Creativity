Table 1: Public and final leaderboard scores for the ESAâ€™s Kelvin competition. Lower score meansa better reconstruction.
Table 2: ResidualBlock(h) architecturelayer0	Conv2d(in=h, out=h, k3, s1,p1)layer1	PReLUlayer2	Conv2d(in=h, out=h, k3, s1,p1)layer3		PReLU	Table 3: HRNet architectureStep	Layers	Number of paramsencode	Conv2d(in=2, out=64, k3, s1,p1)	1216	PReLU	1	ResidUalBlock(64)	73858	ResidUalBlock(64)	73858	Conv2d(in=64, out=64, k3, s1,p1)	36928fuse	ResidUalBlock(128)	295170	Conv2d(in=128, out=64, k3, s1, p1)	73792	PReLU	1decode	ConVTransPose2d(in=64, out=64, k3, s1)	36928	PreLU	1	Conv2d(in=64, out=1, k1, s1)	65residual (optional)	UPsamPle(scale_factor=3.0, mode=bicubic)	0		591818 (total)
Table 3: HRNet architectureStep	Layers	Number of paramsencode	Conv2d(in=2, out=64, k3, s1,p1)	1216	PReLU	1	ResidUalBlock(64)	73858	ResidUalBlock(64)	73858	Conv2d(in=64, out=64, k3, s1,p1)	36928fuse	ResidUalBlock(128)	295170	Conv2d(in=128, out=64, k3, s1, p1)	73792	PReLU	1decode	ConVTransPose2d(in=64, out=64, k3, s1)	36928	PreLU	1	Conv2d(in=64, out=1, k1, s1)	65residual (optional)	UPsamPle(scale_factor=3.0, mode=bicubic)	0		591818 (total)Thanks to weight sharing, HighRes-net super-resolves scenes with 32 views in 5 recursive steps,while requiring less than 600K parameters. ShiftNet has more than 34M parameters (34187648)but is dropped during test time. We report GPU memory requirements in table 4 for reproducibilitypurposes.
Table 4: GPU memory requirements to train HighRes-net + ShiftNet on patches of size 64x64 withbatches of size 32, and a variable number of low-resolution frames.
Table 5: Registration matters: Train and validation scores for HighResNet trained with and withoutShiftNet-Lanczos. Lower is better.
Table 6: Train and validation scores for HighRes-net + ShiftNet-Lanczos trained and tested withdifferent references as input. Lower is better.
Table 7: Validation scores vs. nviews for HighRes-net + ShiftNet. Lower is better.
