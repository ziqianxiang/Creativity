Table 1:   Results of factorization methods for the LSTM encoder-decoder model.  “Pruned” refersto a model where the smallest x% of parameters have been pruned.  “In-training” refers to the in-training factorization method described in Section 2.2 performed on the embedding/projection layer.
Table 2:    Results of in-training factorization for the transformer architecture.  “Pruned” refers toa model where the smallest x% of parameters have been pruned.  “In-training (embed)” refers toin-training factorization performed on the embedding and projection layers.   “In-training (+feed-forward)” refers to in-training factorization performed on the embedding, projection, and feed for-ward layers.  “In-training (+attention)” refers to in-training factorization performed on the embed-ding, projection, feed forward, and attention layers.
Table 3:    Factorized and non-factorized transformer performance by language. The non-factorizedmodel uses an embedding dimension of 512, and the factorized model uses an inner size of 256.
Table 4:    Transformer model parameters for each language.
Table 5:  Comparison of in-training and post-training factorization for the transformer model on theGerman test set.  “In-training (+attention)” refers to in-training factorization applied to the embed-ding, projection, feed-forward, and attention layers. “Post-training” refers to post-training factoriza-tion applied to all weights. “Post-training then pruned” refers to pruning occurring after post-trainingfactorization.
