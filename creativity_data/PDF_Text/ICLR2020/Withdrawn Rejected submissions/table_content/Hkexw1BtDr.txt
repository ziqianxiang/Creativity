Table 1: Performance evaluation on ER and BA datasets. The bold numbers indicate the bestperformance within the same category of algorithms. The relative differences shown in brackets aremeasured with respect to S2V-DQN.
Table 2: Performance evaluation on SATLIB, PPI, REDDIT, and as-Caida datasets. The bold numbersindicate the best performance within the same category of algorithms. The relative differences shownin brackets are measured with respect to S2V-DQN, except for the case of as-Caida dataset whereS2V-DQN underperforms significantly.2	Traditional	DRL-basedType	|V |	Value	CPLEX_RedUMIS	S2V-DQN	AUtODP-10	AUtODP-100	AUtODP-1000SATLIB	(1209, 1347)	Obj. Time	426.8	426.9	413.8	423.8 (+2.4%)	424.8 (+2.7%)	425.4 (+2.8%) 9.490	30.110	2.260	0.311 (-86.2%)	1.830 (-19.0%)	16.409(+626.1%)PPI	(591, 3480)	TOibmj.e	1147.5	1147.5	893.0	1144.5(+28.2%)	1146.5 (+28.4%)	1147.0(+28.4%) 24.685	30.23	6.285	0.786 (-87.5%)	1.770 (-71.8%)	11.469 (+82.3%)REDDIT	22 3648	Obj. (MULTI-5K)	(22, 3648)	Time	370.6	370.6	370.1	370.6 (+0.1%)	370.6 (+0.1%)	370.6	(+0.1%) 0.008	0.159	0.076	0.071 (-6.6%)	0.551 (+625.0%)	5.500	(+7136.8%)REDDIT	(2 3782)	Obj. (MULTI-12K) (2, 3782)	Time	303.5	303.5	302.8	257.4	(-15.0%)	292.6	(-3.4%)	303.5	(+0.2%) 0.007	0.188	1.883	0.003	(-99.8%)	0.025	(-98.7%)	0.451	(-76.1%)REDDIT	(6 3782)	Obj. (BINARY)	(6, 3782)	Time	329.3	3293	328.6	329.3 (+0.2%)~329.3	(+0.2%)	329.3 (+0.2%) 0.007	0.306	0.055	0.020 (-63.6%)	0.173	(+214.6%)	2.627 (+4676.4%)as-Caida	(8020, 26 475) Obj. Time	20 049.2	20 049.2	324.0	20 049.2	20 049.2	20 049.2 0.477	1.719	601.351	0.812	6.106	62.286et al. (2017), the edge ratio of ER graphs and average degree of BA graphs are set to 0.15 and 8,respectively. Datasets are specified by their type of model and an interval for choosing the numberof vertices uniformly at random, e.g., ER-(50, 100) denotes the set of ER graphs generated with thenumber of vertices larger than 50 and smaller than 100. Next, we consider experiments on morechallenging datasets with larger sizes, namely the SATLIB, PPI, REDDIT, and as-Caida datasetsconstructed from SATLIB benchmark (Hoos & Stutzle, 2000), protein-protein interactions (Hamiltonet al., 2017), social networks (Yanardag & Vishwanathan, 2015) and road networks (Leskovec &Sosic, 2016), respectively. See the appendix for more details on the datasets. The time limit of the
Table 3: Performance evaluation for large-scale graphs. Out of budget (OB) is marked for runsviolating the time and the memory budget of 10 000 seconds and 32 GB RAM, respectively. Thebold numbers indicate the best performance within the same category of algorithms. The relativedifferences shown in brackets are measured with respect to S2V-DQN.
Table 4: Performance evaluation on the MWIS problem for the ER datasets. The bold numbersindicate the algorithm with the best performance. The relative differences shown in brackets aremeasured with respect to CPLEX.
Table 5: Performance evaluation on the PCMIS problem for the ER datasets. The bold numbersindicate the algorithm with the best performance. The relative differences shown in brackets aremeasured with respect to CPLEX.
Table 6: Additional evaluation of the supervised learning (SL) based framework on ER datasets. Thebold numbers indicate the best performance within the same category of algorithms. The relativedifferences shown in brackets are measured with respect to the best performing SL-based model.
Table 7: Generalization performance of AutoDP-1000 across synthetic graphs with varying typesand sizes. Rows and columns correspond to datasets used for training and evaluating the model,respectively.
Table 8: Choice of hyperparameters for the experiments on performance evaluation. The REDDITcolumn indicates hyperparameters used for the REDDIT (BINARY, MULTI-5K, MULTI-12K)datasets.
Table 9: Performance comparison of CPLEX with different optimality gap used for the stoppingcriterion.
Table 10: Number of nodes, edges and graphs for each dataset used in the Table 2. Number of graphsis expressed as a tuple of the numbers of training, validation and test graphs, respectively.
Table 11: Number of nodes and edges for each dataset used in the Table 3.
