Table 1: Comparison with the prior character-level language models on enwik8 and text8. We reportbit-per-character (bpc) for test sets as well as the number of parameters.
Table 2: Comparison of lightweight Transformers that use under 8M parameters. L, Hmodel andDmodel indicate the number of layers and heads, the hidden dimension over the model. The FLOPsindicates the number of calculations to generate 512 length of a character sequence. We usedDgroup = Dmodel/G and H = Hmodel/G for Group-Transformers to set the same number of thetotal heads in the attention module.
Table 3: Ablation study on the proposed modules, group attention and group feed-forward layer.
Table 4:	Examples of word completions. The seed text is prepared from text8 test dataset.
Table 5:	Examples of sentence completions. The seed text is prepared from enwik8 test dataset.
Table 6: Comparison with the prior word-level language models on wikitext-103. We report perplex-ity (ppl) for test sets as well as the number of parameters.
Table 7: Performance comparison between the numbers of groups under the similar number ofparameters. We denote “L” and “D” as the number of layers and the hidden dimension, respectively.
Table 8: Ablation study in modeling query, key, and value with our group operations.
