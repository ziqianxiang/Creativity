Table 1: Improvements in density estimation (in bits/dim) using proposed method for RealNVPModel	CelebA	CIFAR-10	ImageNet 32x32	ImageNet 64x64RealNVP (Dinh et al., 2016)	3.02	3.49	4.28	3.98RealNVP flow model with Likelihood Contribution based Multiscale Architecture (ours)	2.71	3.43	4.21	3.927Under review as a conference paper at ICLR 2020Table 2: Density estimation results (in bits/dim) for RealNVP, Glow and i-ResNet with LCMA trainedon CIFAR-10. * Model for i-ResNet has not fully convergedType of multi-scale architecture	RealNVP	Glow	i-ResNetConventional Multi-scale Architecture	3.49	3.35	3.45Likelihood Contribution based Multi-scale Architecture	3.43	3.31	3.40*5.2	Qualitative ComparisonAn ideal dimension factorization method should capture the local variance over series of flow layers,which helps in qualitative sampling. For LCMA implementation, we introduced local max and minpooling operations on log-det heuristic to decide which dimensions to be gaussianized early (Section3). Figure 2(a) shows samples from original datasets, Figure 2(b) shows the samples from trainedRealNVP flow model with conventional multi-scale architecture and Figure 2(c) shows the samplesfrom RealNVP with LCMA trained on various datasets. The finer facial details such as hair styles,eye-lining and facial folds in Celeba samples generated from RealNVP with LCMA were perceptuallybetter than the baseline. The global feature representation observed is similar to that in RealNVP,
Table 2: Density estimation results (in bits/dim) for RealNVP, Glow and i-ResNet with LCMA trainedon CIFAR-10. * Model for i-ResNet has not fully convergedType of multi-scale architecture	RealNVP	Glow	i-ResNetConventional Multi-scale Architecture	3.49	3.35	3.45Likelihood Contribution based Multi-scale Architecture	3.43	3.31	3.40*5.2	Qualitative ComparisonAn ideal dimension factorization method should capture the local variance over series of flow layers,which helps in qualitative sampling. For LCMA implementation, we introduced local max and minpooling operations on log-det heuristic to decide which dimensions to be gaussianized early (Section3). Figure 2(a) shows samples from original datasets, Figure 2(b) shows the samples from trainedRealNVP flow model with conventional multi-scale architecture and Figure 2(c) shows the samplesfrom RealNVP with LCMA trained on various datasets. The finer facial details such as hair styles,eye-lining and facial folds in Celeba samples generated from RealNVP with LCMA were perceptuallybetter than the baseline. The global feature representation observed is similar to that in RealNVP,as the flow architecture was kept the same. The background for natural images such as Imagenet32 × 32 and 64 × 64 was constructed at par with the original flow model. As has been observedin different flow models such as RealNVP and Glow, the latent space holds knowledge about thefeature representation in the data. We performed linear interpolations in latent space to ensure itsefficient construction and generated images, as shown in Figure 3. The interpolations observed weresmooth, with intermediate samples perceptibly resembling synthetic faces, signifying the efficient
Table 3: Ablation study results for multi-scale architectures with various factorization methodsEvaluationsFixed RandomPermutationMultiscale architecturewith early gaussianization ofhigh log-det dimensionsRealNVP(Dinh et al., 2016)Multiscale architecturewith early gaussianization oflow log-det dimensionsQuantitativeEvaluation (Bits/dim)3.053.103.022.71QualitativeEvaluation
