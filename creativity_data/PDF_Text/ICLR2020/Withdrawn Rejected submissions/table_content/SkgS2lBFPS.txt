Table 1: Examples from our Hard STS dataset and our negation split. The sentence pair in the firstrow has dissimilar structure and vocabulary yet a high gold score. The second sentence pair hassimilar structure and vocabulary and a low gold score. The last sentence pair contains negation,where there is a not in Sentence 1 that causes otherwise similar sentences to have low semanticsimilarity.
Table 2: Results of our models and models from prior work. The first six rows are pretrainedmodels from the literature, the next two rows are strong baselines trained on the same data as ourmodels, and the last seven rows include model ablations and BGT, our final model. We show results,measured in Pearson’s r × 100, for each year of the STS tasks 2012-2016 and our two Hard STSdatasets.
Table 3: Performance, measured in Pearson’s r × 100, for different data splits of the STS data. Thefirst row shows performance across all unique examples, the next row shows the negation split, andthe last four rows show difficult examples filtered symmetric word error rate (SWER). The last tworows show restively easy examples according to SWER.__________________________Model	es-es	ar-ar	en-es	en-ar	en-trBILINGUALTRANS	-83.4-	-726-	-64.1-	-37.6-	-59.1-BGT w/o LangVars	81.7	72.8	72.6	73.4	74.8BGT w/o Prior	84.5	73.2	68.0	66.5	70.9BGT		85.7	74.9	75.6	73.5	74.9Table 4: Performance measured in Pearson’s r × 100, on the SemEval 2017 STS task on the es-es,ar-ar, en-es, en-ar, and en-tr datasets.
Table 4: Performance measured in Pearson’s r × 100, on the SemEval 2017 STS task on the es-es,ar-ar, en-es, en-ar, and en-tr datasets.
Table 5: STS performance on the 2012-2016 datasets and our STS Hard datasets for a randomlyinitialized Transformer, the trained English language-specific encoder from BGT, and the trainedsemantic encoder from BGT. Performance is measured in Pearson’s r × 100.
Table 6: Average STS performance for the 2012-2016 datasets, measured in Pearson’s r × 100, fol-lowed by probing results on predicting number of subjects, number of objects, constituent tree depth,top constituent, word content, length, number of punctuation marks, the first punctuation mark, andwhether the articles in the sentence are the correct gender. All probing results are measured inaccuracy ×100.
Table 7: Style transfer generations from our learned BGT model. Source refers to the sentence fedinto the semantic encoder, Style refers to the sentence fed into the English language-specific encoder,and Output refers to the text generated by our model.
Table 8: Results for different ways of incorporating the sentence embedding in the decoder for aBiLSTM on the Semantic Textual Similarity (STS) datasets, along with the time taken to train themodel for 1 epoch. Performance is measured in Pearson’s r × 100.
Table 9: Results on the Semantic Textual Similarity (STS) datasets for different configurations ofEnglishTrans, along with the time taken to train the model for 1 epoch. (XL/YL) means X layerswere used in the encoder and Y layers in the decoder. Performance is measured in Pearson’s r × 100.
Table 10: Results on the Quora Question Pairs (QQP) datasets for prior work, our baselines and ourBGT model using two classification strategies. Performance is measured in accuracy ×100.
