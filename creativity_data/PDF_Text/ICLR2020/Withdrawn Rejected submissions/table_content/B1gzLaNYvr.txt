Table 1: Dataset DetailsDataset	Train Validation Test Sequence Length Input Channels Output ClassesSynthetic Anomaly Detection	45000	5000	10000Electric Devices	6244	2682	7711Character Trajectories	1383	606	869FordA	2520	1081	1320Forest Cover	107110	45906	65580ECG Thorax	1244	556	1965WESAD	5929	846	1697UWave Gesture	624	272	358250502065005075070033311018272022423Therefore, the final term imposing sparsity on the classifier can be written as:Y(kx - D(E(x; WE); WD)k2) + β(kD(E(x; WE); WD)kι) ⇒C × kD(E(x; WE); WD) Θ β*(x)kι + k (x - D(E(χ; WE); WD)) © Y*(x)k2In contrast to the instance-based value of β, we used the average saliency value in our experiments.
Table 2: HyperparametersHyperparameter	ValueInitial learning rate	0.0001Learning rate reduction factor	0.9Learning rate reduction tolerance	4Activation regularization (L1) - β	0.0001Reconstruction weight - γ	4.0Max epochs	50Batch size	256Early stopping patience	10D Loss LandscapeWe analyze the loss landscape in order to asses the impact of stacking the auto-encoder on top ofthe original network on the overall optimization problem. We follow the scheme suggested by Li etal. (2018) Li et al. (2017) where we first perform filter normalization using the norm of the filters.
Table 3: Results for the different datasets in terms of accuracy for both the classifier as well asTSInsight.
