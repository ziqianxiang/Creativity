Table 1: NomenclatureSymbol	DimenSiOn(S)	Definitionn, N	N	Polynomial term order, total approximation order.
Table 3: IS/FID scores on CIFAR10 (Krizhevsky et al., 2014) with linear blocks.
Table 4: The algorithm on the left describes the SNGAN generator. The algorithm on the rightpreserves the resnet blocks of the SNGAN generator, but converts it into a polynomial (namedSNGAN-poly). The different lines are emphasized with blue color.
Table 5: Global transformation validation on SNGAN. The first two results assess the addition of anon-linear activation function after the global transformation. The last two rows compare the additionof a global transformation on the original generator.
Table 6: Ablation experiment on splitting the noise z into non-overlapping chunks for the injection.
Table 7: Ablation experiment on normalizing the Arisz vector before the Hadamard product.
Table 8: Ablation experiment on adding a skip connection to each Hadamard product.
Table 9: Ablation experiment (conditional GAN setting) on normalizing the Aris z vector before theHadamard product.
Table 10: Ablation experiment (conditional GAN setting) on adding a skip connection to eachHadamard product.
Table 11: IS/FID scores on CIFAR10 (Krizhevsky et al., 2014) utilizing DCGAN (Radford et al.,2015) and SNGAN (Miyato et al., 2018) architectures for unsupervised image generation. Eachnetwork is run for 10 times and the mean and standard deviation are reported. In both cases, insertingblock-wise noise injections to the generator (i.e., converting to our proposed PolyGAN) results in animproved score. Higher IS / lower FID score indicate better performance.
Table 12:	Quantitative results on conditional image generation. We implement both SNGAN trainedon CIFAR10 and SAGAN trained on Imagenet (for 400, 000 iterations). Each network is run for 10times and the mean and variance are reported.
Table 13:	Number of parameters for the generators of each approach and on various databases. Ascan be seen, our method only marginally increases the parameters while substantially improving theperformance. On the other hand, “Concat” significantly increases the parameters without analogousincrease in the performance.
