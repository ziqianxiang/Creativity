Table 1: BLEU (cased) scores on WMT’14 testsets for En→De, En→Ru, and En→Fr translationtasks. All the models were trained with gradient aggregation to replicate a 8-GPU setup on a singlephysical GPU.
Table 2: BLEU (cased) scores on WMT’14 testsets for English-German (En-De) and English-Russian (En-Ru) language pairs (in both directions). All models were trained with 1 GPU (nogradient aggregation). The decrease in scores in this table compared to the ones in Table 1 is due tothe number of GPUs used (1 vs. 8).
Table 3: Perplexity scores on one-billion-word language modeling benchmark. All models are ofbase-size and were trained for 100K updates with gradient aggregation to produce a virtual 4-GPUsetup on a single GPU.
