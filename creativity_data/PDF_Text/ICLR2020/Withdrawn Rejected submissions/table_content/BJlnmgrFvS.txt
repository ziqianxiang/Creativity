Table 1: Performance comparison at one million samples (mean and std over batches and randomseeds). Last column shows percentage improvement of BAIL over BCQ.
Table 2:	Upper Envelope HyperparametersParameter	Valueoptimizer	Adam (Kingma & Ba, 2014)learning rate	3 ∙ 10-3discount (γ)	0.99regularization constant λ 2 ∙ 10-2K	10,000number of hidden units 128 × 128Table 3:	BAIL HyperparametersParameter	Valuedata in batch	106optimizer	Adam (Kingma & Ba, 2014)learning rate	10-3regularization constant λ	0mini-batch size	100BAIL-border p%	25%BAIL-TD x	0.96number of hidden units	400 × 300B.2	Implementation of competing algorithmsFor the behavioral DDPG algorithm, we used the implementation of Fujimoto et al. (2018b). For
Table 3:	BAIL HyperparametersParameter	Valuedata in batch	106optimizer	Adam (Kingma & Ba, 2014)learning rate	10-3regularization constant λ	0mini-batch size	100BAIL-border p%	25%BAIL-TD x	0.96number of hidden units	400 × 300B.2	Implementation of competing algorithmsFor the behavioral DDPG algorithm, we used the implementation of Fujimoto et al. (2018b). Forthe behavioral SAC algorithm, we implemented it in Pytorch, mainly following the pseudocodeprovided by (Achiam), and used hyperparameters in Haarnoja et al. (2018b). For the BCQ algorithm,we used the authors’ implementation (Fujimoto et al., 2018a). For behavioral cloning and its variantsin the ablation study section, the network structure, learning rate, mini-batch size, and so on areidentical to those in Table 3 for BAIL.
