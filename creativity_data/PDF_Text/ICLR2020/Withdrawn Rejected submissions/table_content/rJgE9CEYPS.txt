Table 1: Video face verification performance on YouTube Face dataset, compared with state-of-theart methods and baseline methods.
Table 2: Comparsion with different participants and aggreation stragey on the IQIYI-VID-FACE challenge. By combing with PolyNet, DDL achieves state-of-the-arts peformance.
Table 3: Peformance comparsions on IJB-A verificatiton benchmark. The True Accept Rates (TAR)vs. False Postive Rate (FAR) are reported.
Table 4: Peformance comparsions on IJB-C verificatiton benchmark. The True Accept Rates (TAR)vs. False Postive Rate (FAR) are reported.
Table 5: Video action recognition results on ActivityNet-1.2 dataset. We compare randomly, uni-formly and filter by DDL to select 9 clips to aggregate. Accuracy is reported on the valiation set.
Table 6: Video action recognition results on Kinetics-700 dataset. We compare randomly, uniformlyand filter by DDL to select 5 clips to aggregate. Accuracy is reported on the validation set and is theaverage of top1 and top 5 accuracy. Dense sample strategy sample 10 clips along the temporal axisand random crop 3 clips on the spatial axis.
Table 7: Results for cross dataset and cross model experiment. All results are reported with base ac-tion recognition R(2+1)D-ResNet-50 on ActivityNet-1.2 dataset. The training procedures for DDLvary from dataset and base model.
Table 8: The architecture of our light-weight DDNet. Typical DDNet is a channel reduction Resnet-18 network and only introduces 81.9 Mflops computation, which is super efficient.
Table 9: Peformance comparsions for different architecture on IJB-C verificatiton benchmark. TheTrue Accept Rates (TAR) vs. False Postive Rate (FAR) are reported. CD represents channel reduc-tion as shown in Table 8.
Table 10: Combine DDL with more baselines including SphereFace (SF) and CosFace (CF). Allbaseline models are trained with same backbone and datasets. Average pooling is used as the defaultaggregation strategy.
Table 11: Peformance comparsions for discussion of class data imbalance when training DDL. Headand tail data setting are of near same number samples, but varies from class numbers.
Table 12: Peformance for training both base model and DDL on IMDB dataset on IJB-C benchmark.
Table 13: Peformance comparsions for cascade training on IJB-C dataset. By refining the classcentroid and cascade training, DDL achieves performance gain.
