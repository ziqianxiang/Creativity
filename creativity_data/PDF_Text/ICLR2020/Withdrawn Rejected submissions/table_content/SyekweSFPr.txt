Table 1: Adversarially trained transfer learning, R2-D2 (Bertinetto et al., 2018), ADML (Yin et al.,2018), and our adversarially queried (AQ) R2-D2 model on 5-shot Mini-ImageNet. Natural accuracyis denoted Anat , and robust accuracy, Aadv , is computed with a 20-step PGD attack as in Madryet al. (2017) with = 8/255. A description of our training regime can be found in Appendix A.5.
Table 2: 5-shot MiniImageNet (MI) and CIFAR-FS (FS) results comparing naturally trained meta-learners. Anat and Aadv are natural and robust test accuracy, respectively, where robust accuracy iscomputed with respect to a 20-step PGD attack.
Table 3: MAML models on Mini-ImageNet and Omniglot. Anat and Aadv are natural and robusttest accuracy, respectively, where robust accuracy is computed with respect to a 20-step PGD attack.
Table 4: Comparison of adversarially queried (AQ) meta-learners on 1-shot and 5-shot CIFAR-FS.
Table 5: Comparison of adversarially queried (AQ) meta-learners on 1-shot and 5-shot Mini-ImageNet. Anat and Aadv are natural and robust test accuracy, respectively, where robust accuracyis computed with respect to a 20-step PGD attack. Top 1-shot and 5-shot robust accuracy is bolded.
Table 6: Adversarially trained transfer learning and adversarially queried meta-learning on 5-shotMini-ImageNet. Anat and Aadv are natural and robust test accuracy, respectively, where robustaccuracy is computed with respect to a 20-step PGD attack. Top natural and robust accuracy foreach architecture is bolded.
Table 7: Performance on 5-shot Omniglot. Robust accuracy, Aadv, is computed with respect toa 20-step PGD attack. Anat(AT) and Aadv(AT) are natural and robust test accuracy with 7-PGDtraining during fine-tuning. Top robust accuracy with and without adversarial fine-tuning is bolded.
Table 8: Performance on 5-shot Mini-ImageNet. Robust accuracy, Aadv , is computed with respectto a 20-step PGD attack. Anat(AT) and Aadv(AT) are natural and robust test accuracy with 7-PGDtraining during fine-tuning. Top robust accuracy with and without adversarial fine-tuning is bolded.
Table 9: 5-shot Mini-ImagNet (MI) and CIFAR-FS (FS) results comparing meta-TRADES to ad-versarial querying. Anat and Aadv are natural and robust test accuracy, respectively, where robustaccuracy is computed with respect to a 20-step PGD attack.
Table 10: Adversarially queried MAML compared with a MAML variant where only the last layeris re-trained during fine-tuning on 5-shot Mini-ImageNet. Anat and Aadv are natural and robusttest accuracy, respectively, where robust accuracy is computed with respect to a 20-step PGD at-tack. Anat(AT) and Aadv(AT) are natural and robust test accuracy, respectively with 7-PGD trainingduring fine-tuning. Layers are fine-tuned for 10 steps with a learning rate of 0.01.
Table 11: Natural test accuracy of naturally trained R2-D2, MetaOptNet, and the MetaOptNet headwith R2-D2 backbone on the Mini-ImageNet (MI) and CIFAR-FS (FS) data sets.
Table 12: Robust test accuracy of adversarially queried R2-D2, MetaOptNet, and the MetaOptNetand heads with R2-D2 backbone on Mini-ImageNet (MI) and CIFAR-FS (FS) data sets. Robustaccuracy is computed with respect to a 20-step PGD attack.
Table 13: 5-shot MiniImageNet results for our highest performing R2-D2 with feature denoisingblocks. Anat and Aadv are natural and robust test accuracy, respectively, where robust accuracy iscomputed with respect to a 20-step PGD attack. Top robust accuracy is bolded.
Table 14: 5-shot MiniImageNet results against DeepFool (DF) (2 iteration) '∞ attack, MI-FGSM(MI) ( = 8/255) attack, and PGD attack with 20 random restarts (20-PGD). We compare R2-D2trained with adversarial-querying (AQ) to the adversarially trained transfer learning R2-D2 as insection 4.1.
Table 15: 5-shot CIFAR-FS results comparing the superresolution defense (SR defense) and De-fenseGAN. Anat and Aadv are natural and robust test accuracy, respectively, where robust accuracyis computed with respect to a 20-step PGD attack. Both methods perform worse than their adversar-ially queried counterpart. Top robust accuracy is bolded.
Table 16: 1-shot MiniImageNet (MI) and CIFAR-FS (FS) results comparing naturally trained meta-learners. Anat and Aadv are natural and robust test accuracy, respectively, where robust accuracy iscomputed with respect to a 20-step PGD attack.
Table 17: Adversarially trained transfer learning and adversarially queried meta-learning on 5-shotCIFAR-FS. Anat and Aadv are natural and robust test accuracy, respectively, where robust accu-racy is computed with respect to a 20-step PGD attack. Top natural and robust accuracy for eacharchitecture is bolded.
Table 18: Adversarially trained transfer learning and adversarially queried meta-learning on 1-shotMini-ImageNet. Anat and Aadv are natural and robust test accuracy, respectively, where robustaccuracy is computed with respect to a 20-step PGD attack. Top natural and robust accuracy foreach architecture is bolded.
Table 19: Adversarially trained transfer learning and adversarially queried meta-learning on 1-shotCIFAR-FS. Anat and Aadv are natural and robust test accuracy, respectively, where robust accu-racy is computed with respect to a 20-step PGD attack. Top natural and robust accuracy for eacharchitecture is bolded.
Table 20: Performance on 1-shot Omniglot. Robust accuracy, Aadv , is computed with respect toa 20-step PGD attack. Anat(AT) and Aadv(AT) are natural and robust test accuracy with 7-PGDtraining during fine-tuning.
Table 21: Performance on 1-shot Mini-ImageNet. Robust accuracy, Aadv, is computed with respectto a 20-step PGD attack. Anat(AT) and Aadv(AT) are natural and robust test accuracy with 7-PGDtraining during fine-tuning.
Table 22: 1-shot Mini-ImagNet (MI) and CIFAR-FS (FS) results comparing meta-TRADES to ad-versarial querying. Anat and Aadv are natural and robust test accuracy, respectively, where robustaccuracy is computed with respect to a 20-step PGD attack.
Table 23: 5-Shot CIFAR-FS results against DeePFool(DF) (2 iteration) '∞ attack, MI-FGSM (MI)( = 8/255) attack, and PGD attack with 20 random restarts (20-PGD). We compare R2-D2 trainedwith adversarial-querying (AQ) to the transfer learning R2-D2 as in section 4.1.
Table 24: 5-shot CIFAR-FS results against black-box transfer attacks crafted on an adversari-ally trained (transfer learning) ResNet-12 model using 7-PGD. We then test R2-D2 trained withadversarial-querying (AQ) and the transfer learning R2-D2 model on these crafted Perturbations.
