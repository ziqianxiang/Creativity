Table 1: Four evaluation settings considered in this work, described in §4.4.
Table 2: “In domain” generalization accuracy of EBMs (each row) on various text corpora. A columncorresponds to the corpus used to get positives and to fit the train and test language models, which are TransfBig(§4.2) with different initial seeds. The last row is the accuracy when using as energy the log-probability of thetraining language model over the whole sequence.
Table 3: Cross-architecture generalization accuracy using the Wikitext dataset for both training and testing(Ctrain = Ctest). Each row is a model architecture used for generating the training negatives (Atrain), and eachcolumn is a model architecture for generating the testing negatives (Atest). The energy function is UniTransf.
Table 4: Cross-corpora generalization accuracy using TransfBig generator and UniTransf energy function(except for the last row which used a bidirectional transformer). Each row specifies the corpora used at trainingtime, Ctrain. Each column shows the corpus used at test time, Ctest.
Table 5: Generalization of BiTransf and UniTransf energy function to state of the art generators. The energyfunctions (discriminators) are trained on the concatenation of all three corpora (same as in table 4 ALL rows).
Table 6: Generalization of the energy function to unconditional generation from various GPT2 models (modelsize in parantheses, followed by sampling method used). Each row contains the accuracy on the correspondingtest set. TF-IDF results are taken from Radford & Wu (2019).
Table 7: Effect of different strategies to minenegatives using TransfBig generator and BiL-STMSmall energy function on Book Corpus.
Table 8: Number of BPE tokens in millions for each dataset.
Table 9: Number of parameters (in millions) for the generator language models. The computational cost isdirectly related to the number of parameters in other layers than the input embedding layer (second row).
Table 10: Number of parameters in millions for the scoring functions. The computational cost is directly relatedto the number of parameters in other layers than the input embedding layer (second row).
Table 11: Hyper-parameter values used in our scoring functions.
