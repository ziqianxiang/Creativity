Table 1: A summary of KD methods for BERT. Abbreviations: INIT(initializing student BERT withsome layers of pre-trained teacher BERT), DA(conducting data augmentation for task-specific train-ing data). Embd, Attn, Hidn, and Pred represent the knowledge from embedding layers, attentionmatrices, hidden states, and final prediction layers, respectively.
Table 2: Results are evaluated on the test set of GLUE official benchmark. All models are learnedin a single-task manner. “-” means the result is not reported.
Table 3: The model sizes and inference time for baselines and TinyBERT. The number of layersdoes not include the embedding and prediction layers.
Table 4: Results (dev) of wider or deeper TinyBERT variants and baselines.
Table 5: Ablation studies of different proce-dures (i.e., TD, GD, and DA) of the two-stagelearning framework. The variants are validatedon the dev set.
Table 6: Ablation studies of different distilla-tion objectives in the TinyBERT learning. Thevariants are validated on the dev set.
Table 7: Results (dev) of different mapping strategies.
Table 8: Results (dev) of baselines and TinyBERT on question answering tasks.
Table 9: Results of different methods at pre-training state. TD and GD refers to Task-specificDistillation (without data augmentation) and General Distillation. The results are evaluated on de-velopment set.
Table 10: Comparisons between TinyBERT and BERT-PKD, and the results are evaluated on thetest set of official GLUE tasks.
Table 11: Comparisons between TinyBERT with DistilBERT, and the results are evaluated on the de-v set of GLUE tasks. Mcc refers to Matthews correlation and Pear/Spea refer to Pearson/Spearman.
