Table 1: Performance test (50 trials of training for each cases)Reward	Environment	Baseline	Robots (R) / Tasks(T)			2R/20T 3R/20T 3R/30T 5R/30T 5R/40T 8R/40T 8R/50TLinear	Deterministic	Optimal Ekisi et al. (%SGA)	98.31%	97.50%	97.80%	95.35%	96.99%	96.11%	96.85% ±4.23	±4.71	±5.14	±5.28	±5.42	±4.56	±3.40 99.45%	100%	82.65%	86.35%	92.25%	91.85%	80.60% (137.3)	(120.6)	(129.7)	(110.4)	(123.0)	(119.9)	(119.8)	Stochastic	-Optimal- (%SGA)	NTA (130.9) (115.7) (122.8) (115.6) (122.3) (113.3) (115.9)Nonlinear	Deterministic	-Optimal- (%SGA)	N/A (111.5) (118.1) (118.0) (110.9) (118.7) (111.2) (112.6)	Stochastic	-Optimal- (%SGA)	N/A (110.8) (117.4) (119.7) (111.9) (120.0) (110.4) (112.4)Table 2: Scalability test (mean of 20 trials of training, linear & deterministic env.)Linear & Deterministic	Testing size (Robot (R) / Task (T))							2R/20T	3R/20T	3R/30T	5R/30T	5R/40T	8R/40T	8R/50TPerformance with full training	98.31%	97.50%	^^97.80%	95.35%	96.99%	96.11%	96.85%# Training for 93% optimality	19261.2	61034.0	^^99032.7	48675.3	48217.5	45360.0	47244.2small loss in performance. For upper-direction transfer tests (trained with smaller size problem andtested with larger size problem), the performance loss was up 4 percent.
Table 2: Scalability test (mean of 20 trials of training, linear & deterministic env.)Linear & Deterministic	Testing size (Robot (R) / Task (T))							2R/20T	3R/20T	3R/30T	5R/30T	5R/40T	8R/40T	8R/50TPerformance with full training	98.31%	97.50%	^^97.80%	95.35%	96.99%	96.11%	96.85%# Training for 93% optimality	19261.2	61034.0	^^99032.7	48675.3	48217.5	45360.0	47244.2small loss in performance. For upper-direction transfer tests (trained with smaller size problem andtested with larger size problem), the performance loss was up 4 percent.
Table 3: Transferability test (50 trials of training for each cases, linear & deterministic env.)	TeSting Size(Robot(R)ZTaSk(T))						Training size (Robot(R)/Task(T))	2R/20T	3R/20T	3R/30T	5R/30T	5R/40T	8R/40T	8R/50T2R/20T	98.31%	93.61%	97.31%	92.16%	92.83%	90.94%	93.44%3R/20T	95.98%	97.50%	96.11%	93.64%	91.75%	91.60%	92.77%3R/30T	94.16%	96.17%	97.80%	94.79%	93.19%	93.14%	93.28%5R/30T	97.83%	94.89%	96.43%	95.35%	93.28%	92.63%	92.40%5R/40T	97.39%	94.69%	95.22%	93.15%	96.99%	94.96%	93.65%8R/40T	95.44%	94.43%	93.48%	93.93%	96.41%	96.11%	95.24%8R/50T	95.69%	96.68%	97.35%	94.02%	94.50%	94.86%	96.85%9Under review as a conference paper at ICLR 2020(Al-ωlu⅛o %) SuelUJOJJedFigure 2: Tested with 1) single layer, 2) heuristic PGM 3) Max-operationTable 4: IPMS test results for makespan minimization (our algorithm / best Google OR tool result)Makespan minimization for Deterministic environment		# Machines					3	5	7	10	50	106.7%	117.0%	119.8%	116.7%# Tasks	75	105.2%	109.6%	113.9%	111.3%	100	100.7%	111.0%	109.1%	109.0%
Table 4: IPMS test results for makespan minimization (our algorithm / best Google OR tool result)Makespan minimization for Deterministic environment		# Machines					3	5	7	10	50	106.7%	117.0%	119.8%	116.7%# Tasks	75	105.2%	109.6%	113.9%	111.3%	100	100.7%	111.0%	109.1%	109.0%Google (2012). This library provides metaheuristics such as Greedy Descent, Guided Local Search,Simulated Annealing, Tabu Search. We compare our algorithm’s result with the heuristic with thebest result for each experiment. We consider cases with 3, 5, 7, 10 machines and 50, 75, 100 jobs.
