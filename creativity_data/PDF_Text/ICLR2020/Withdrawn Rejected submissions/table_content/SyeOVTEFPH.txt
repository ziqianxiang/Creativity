Table 1: Robustness experiments on CIFAR-10. PGD attacks are generated with = 8. PGD10 andPGD100 attacks are generated with 5 random restarts, while PGD1000 attacks are generated with 2random restartsMethod	Natural acc. (in %)	Whitebox acc. (in %)			Transfer (in %) acc. (PGDi00θ)	Corruption acc. (in %)		PGD10 I	PGDi0θ I PGDιo00			Resnet-18						Clean	94.21	0.02	0.00	0.00	3.03	72.71Adversarial	83.20	43.79	42.30	42.36	59.80	73.73IAAT	87.26	43.08	41.16	41.16	59.87	78.82WideResnet 32-10						Clean	95.50	0.05	0.00	0.00	5.02	78.35Adversarial	86.85	46.86	44.82	44.84	62.77	77.99IAAT	91.34	48.53	46.50	46.54	58.20	83.13Table 2: Robustness experiments on CIFAR-100. PGD attacks are generated with = 8. PGD10and PGD100 attacks are generated with 5 random restarts. PGD1000 attacks are generated with 2random restartsMethod	Natural acc. (in %)	Whitebox acc. (in %)			Transfer acc. (in %) PGDi000		PGD10 I PGDιo0		I PGDi000	Resnet-18					Clean	74.88	0.02	0.00	0.01	1.81
Table 2: Robustness experiments on CIFAR-100. PGD attacks are generated with = 8. PGD10and PGD100 attacks are generated with 5 random restarts. PGD1000 attacks are generated with 2random restartsMethod	Natural acc. (in %)	Whitebox acc. (in %)			Transfer acc. (in %) PGDi000		PGD10 I PGDιo0		I PGDi000	Resnet-18					Clean	74.88	0.02	0.00	0.01	1.81Adversarial	55.11	20.69	19.68	19.91	35.57IAAT	63.90	18.50	17.10	17.11	35.74	WdeResnet 32-10						Clean	79.91	0.01	0.00	0.00	1.20Adversarial	59.58	26.24	25.47	25.49	38.10IAAT	68.80	26.17	24.22	24.36	35.18Robustness to other attacks: While our instance adaptive algorithm is trained on PGD attacks,we are interested to see if the trained model improves robustness on other adversarial attacks.
Table 3: Robustness results on other attacks for models trained using PGD-10 for WRN-32-10 modelon CIFAR-10 dataset. Accuracies are reported in %Algorithm	Natural acc.	PGD-1000	DeePFool	MIFGSM	CW40Adversarial training	8685	-44.84-	-65.28-	-54.66-	55.62	IAAT	91.34	46.54	66.58	53.99	56.80Table 4: Robustness experiments on Imagenet. All adversarial attacks are generated with PGD-1000.
Table 4: Robustness experiments on Imagenet. All adversarial attacks are generated with PGD-1000.
Table 5: Ablation: Effect of warmup on CIFAR-10Method	Natural acc. (%)	Whitebox acc. (in %)			Transfer acc.(%) PGD1000	Corruption acc. (in %)		PGD10	PGD100	PGD1000		Resnet-18						IAAT (no warm)	89.62	40.55	38.15	38.08	58.89	81.10IAAr (Warm)	87.26	43.08	41.16	41.16	59.87	78.82	WideResnet32-10							IAAT (no warm)	92.62	45.12	41.08	41.11	53.08	84.92IAAT (Warm)	90.67	48.53	46.50	46.54	58.20	83.13Table 6: Ablation: Effect of warmup on CIFAR-100Method	Natural acc. (in %)	Whitebox acc. (in %)			Transfer acc.(%) PGD1000		PGD10	PGD100 I PGD1000		Resnet-18					Adaptive (no warm)	68.34	14.76	13.29	13.30	32.39Adaptive (warm)	63.90	18.50	17.10	17.11	35.74	WideResnet32-10						Adaptive (no warm)	75.48	18.14	13.78	13.71	24.00Adaptive (warm)	68.80	26.17	24.22	24.36	35.18Table 7: Ablation: Comparison of IAAT with exact line search. Accuracies are reported in % forResnet-18 model trained on CIFAR-10 dataset.
Table 6: Ablation: Effect of warmup on CIFAR-100Method	Natural acc. (in %)	Whitebox acc. (in %)			Transfer acc.(%) PGD1000		PGD10	PGD100 I PGD1000		Resnet-18					Adaptive (no warm)	68.34	14.76	13.29	13.30	32.39Adaptive (warm)	63.90	18.50	17.10	17.11	35.74	WideResnet32-10						Adaptive (no warm)	75.48	18.14	13.78	13.71	24.00Adaptive (warm)	68.80	26.17	24.22	24.36	35.18Table 7: Ablation: Comparison of IAAT with exact line search. Accuracies are reported in % forResnet-18 model trained on CIFAR-10 dataset.
Table 7: Ablation: Comparison of IAAT with exact line search. Accuracies are reported in % forResnet-18 model trained on CIFAR-10 dataset.
Table 8: Comparison with MixupMethod	Natural acc. (in %)	Whitebox acc. (in %)			Transfer attack (in %) PGD1000		PGD10	PGD100	PGD1000	Resnet-18					Mixup	89.47	42.60	38.42	38.49	59.48IAAT	87.26	43.08	41.16	41.16		59.87		WideResnet32-10						Mixup	92.57	45.01	36.6	36.44	63.57IAAT	90.67	48.53	46.50	46.54		58.20	A	AppendixA.1 Comparison with MixupA recent paper that addresses the problem of improving natural accuracy in adversarial training ismixup adversarial training (Lamb et al., 2019), where adversarially trained models are optimizedusing mixup loss instead of the standard cross-entropy loss. In this paper, natural accuracy wasshown to improve with no drop in adversarial robustness. However, the robustness experimentswere not evaluated on strong attacks (experiments were reported only on PGD-20). We compare ourimplementation of mixup adversarial training with IAAT on stronger attacks in Table. 8. We observethat while natural accuracy improves for mixup, drop in adversarial accuracy is much higher thanIAAT.
Table 9: Sensitivity of IAAT performance to hyperparameters β and γ. Models are trained onCIFAR-10 dataset using Wideresnet-32-10.
Table 10: Hyper-parameters for experiments on CIFAR-10 and CIFAR-100Hyperparameters	Resnet-18	WideReSnet-32-10Optimizer	SGD =	SGDStart learning rate	0.1	0.1Weight decay	0.0002	0.0005Number of epochs trained	200	110Learning rate annealing	Step decay	Step decayLearning rate decay steps	[80,140,170]	[70, 90,100]Learning rate decay factor	0.1	0.2Batch size	128	128Warmup period	5 epochs	10 epochsE used in warmup (Ew)	8	8Discretization γ	1.9	1.9Exponential averaging factor β	0.1		0.1	Attack parameters during training		Attack steps	10	10Attack E (for adv. training only)	8	8Attack learning rate	2/255	2/255Table 11: Hyper-parameters for experiments on ImagenetHyperparameters	Imagenet
Table 11: Hyper-parameters for experiments on ImagenetHyperparameters	ImagenetOptimizer Start learning rate Weight decay Number of epochs trained Learning rate annealing Learning rate decay steps Learning rate decay factor Batch size	SGD 0.1 X (effective batch size / 256) 0.0001 110 Step decay with LR warmup [35, 70, 95] 0.1 	32 Per GPU	Warmup period E used in warmup (Ew) Discretization γ Exponential averaging factor β	30 epochs 16 4 	0.1	Attack parameters during trainingAttack steps	30Attack E (for adv. training only)	16Attack learning rate		1/255	E.2 ImagenetFor Imagenet implementation, we mimic the setting used in Xie et al. (2019). During training,adversaries are generated with PGD-30 attacks. This is computationally expensive as every trainingupdate is followed by 30 backprop iterations to generate the adversarial attack. To make trainingfeasible, we perform distributed training using synchronized SGD updates on 64 / 128 GPUs. Wefollow the training recipe introduced in Goyal et al. (2017) for large batch training. Also, duringtraining, adversarial attacks are generated with FP-16 precision. However, in test phase, we useFP-32.
Table 12: Training time for Imagenet experimentsModel	NUmberofGPUs used	Training timeResnet-50	64	92 hrs =Resnet-101	128	78 hrsResnet-152		128		94 hrsResnet-50 model was trained on 64 Nvidia V100 GPUs, while Resnet-101 and Resnet-152 modelswere trained on 128 GPUs. Time taken for instance adaptive adversarial training for all models isreported in Table. 12.
