Table 1: Results on the PubMed dataset. TLM uses a GPT-like transformer language models (Rad-ford  et  al.,  2019)  conditioned  on  introduction  (I)  and  with  extracted  sentences  (E).  The  highestROUGE scores for abstractive methods are bold-faced.  Hybrid refers to models that use two-stepextractive-abstractive summarization.
Table 2:  Model ablation study on the PubMed dataset.  With Encoder Mem, the baseline HRED isaugmented with a neural encoder memory bank as described in Section 3.1. We then regularize theencoder memory, Regularize Mem, using the equation (12).  We also experiment with the DecoderMem that performs read/write mechanism described in Section 3.2 without memory transfer. Finally,with Mem Transfer, we obtain the whole Mem2Mem setup.
Table 3:  Rouge scores of unsupervised extracted input sentences with respect to the ground truthsummaries.  For Mem2Mem Encoder Memories, we used the sentences that had encoder memoryattention A above 80%.  Gold Ext is the gold extracted ROUGE scores of all sentences selected bya greedy selection from the input article that have the highest per-sentence ROUGE scores, as inSubramanian et al. (2019).
