Table 1: The average test accuracy (%, higher is better) and backward transfer (BWT, lower is better)after learning all tasks on each benchmark, respectively. The results are averaged over 10 trials.
Table 2: Distribution of classes in each imbalanced dataset for the respective tasks Tn=1:10.
Table 3: Network architecture used for Vision Datasets Mixture benchmark in Section 4.4. Forconvolutional layers, the output size denotes channel size of output. The negative threshold for allof the LeakyReLU nonlinearities were set to 0.2.
Table 4: The average test accuracy (%, higher is better) for different initial Î· values after learning alltasks on the Permuted MNIST, Imbalanced Permuted MNIST and Split MNIST continual learningbenchmarks, respectively. The results are averaged over 5 trials.
