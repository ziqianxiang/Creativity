Table 1: Robustness of the Gambler’s loss to noisy labels. We see that simply using gambler’s loss on labelnoise problems improve. For the two categories using early stopping, the number in parenthesis refers to the avg.
Table 2: On MNIST, using our method always achieves SOTA performance. On CIFAR-10 (CF10), our methodis especially strong when the noise level is extreme (r Z 0.7,0.8,0.85), agreeing With our finding in the previoussection. (*NA means that the ES stopping point is not reached by the algorithm.)Dataset	FC	CT	CT+	CTES	CT+ES	GblerLAESMN r = 0.2	98.8 ± 0.1	97.2 ± 0.0	97.8 ± 0.1	98.6 ± 0.2	97.6 ± 0.1	99.0 ± 0.1MN r = 0.5	79.6 ± 2.0	93.3 ± 0.1	95.4 ± 0.2	97.6 ± 0.1	96.3 ± 0.2	98.4 ± 0.2MN r = 0.65	29.1 ± 0.1	92.5 ± 0.1	92.5 ± 0.1	97.3 ± 0.2	95.6 ± 0.2	97.6 ± 0.3MN r = 0.8	19.8 ± 0.1	78.1 ± 0.5	67.5 ± 0.1	82.1 ± 1.7	71.4 ± 0.3	95.0 ± 0.5MN r = 0.85	11.8 ± 0.0	60.5 ± 0.2	10.1 ± 0.0	66.5 ± 0.5	NA	88.0 ± 1.0	nll loss	CT	-CTES	GblrsAES	GblrsLAES	CF10 r =0.2	58.1 ± 0.1	71.0 ± 0.6	69.5 ± 0.4	67.5 ± 1.0	68.2 ± 0.0	CF10 r =0.5	35.4 ± 0.2	65.2 ± 0.2	64.5 ± 0.8	59.5 ± 0.4	60.1 ± 0.2	CF10 r =0.6	28.3 ± 0.1	61.1 ± 0.2	62.0 ± 0.3	51.3 ± 0.5	57.1 ± 0.5	CF10r=0.7	20.9 ± 0.4	48.9 ± 0.2	49.1 ± 1.0	48.2 ± 0.5	50.6 ± 0.5	CF10r=0.8	15.1 ± 0.5	25.0 ± 0.1	25.8 ± 1.0	37.0 ± 0.7	40.7 ± 2.0	CF10 r = 0.85	13.0 ± 0.5	17.3 ± 0.1	NA	18.9 ± 1.0	27.0 ± 2.1	combine our method with CT but was not very successful, but CT throw away a large portion of thetraining dataset, and changes the underlying distribution, making the AES criterion not applicable.
Table 3:	architecture of the neural network used in MNISTCCNN on MNIST28 × 28 Gray Image20 × 5 conv, 1 ReLU2 × 2 max-pool, stride 250 × 5 conv, 20 ReLU2 × 2 max-pool, stride 2dense 800 → 500dense 500 → 11Table 4:	architecture of the neural network used in cifar10CNN on cifar1032 × 32 RGB Image5 × 5 conv,128 LReLU5 × 5 conv,128 LReLU2 × 2 max-pool, stride 2dense 128 - 128dense 128 → 11I effect of tuning o and critical behaviorIn this section, we give more experiments on the critical behavior discussed in 2.3. First see Figure 8.
Table 4:	architecture of the neural network used in cifar10CNN on cifar1032 × 32 RGB Image5 × 5 conv,128 LReLU5 × 5 conv,128 LReLU2 × 2 max-pool, stride 2dense 128 - 128dense 128 → 11I effect of tuning o and critical behaviorIn this section, we give more experiments on the critical behavior discussed in 2.3. First see Figure 8.
Table 5: Robustness of the Gambler’s loss to asymetric noise. We set o = 2.0 for this experiment.
Table 6: Robustness of the Gambler’s loss to noisy labels, in comparison with the upperbound.
Table 7: Additional Resutls on IMDB dataset.
