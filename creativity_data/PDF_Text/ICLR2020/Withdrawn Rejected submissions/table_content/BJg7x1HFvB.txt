Table 1: Student models with various numbers of transformer layers (L) and hidden embedding sizes (H).
Table 2: Training Strategies that build compact models by applying language model (LM) pre-training before knowledge distillation (KD). The first two rows apply distillation on task data. Thethird row applies distillation with an LM objective on general-domain data.
Table 3: Model Quality. All students are 6/768 BERT models, trained by 12/768 BERT teachers.
Table 4: Datasets used for analysis. A star indicates thathard labels are discarded. NLI* refers to the collection ofMNLI (390k), SNLI (570k) and QQP (400k). The reviewsdatasets are part of Amazon Product Data. Unless otherwisestated, DLM consists of BookCorpus & English Wikipedia.
