Table 1: Dataset task and learning setting	Cora		Citeseer	Pubmed	PPI	CEP	HIVTask	Node Classification Graph Prediction	✓	✓	✓	✓	✓	✓Setting	Transductive Learning Inductive Learning	✓	✓	✓	✓	✓	✓and general:LReLU(aT [Whi||Whj]) (concat), (Whi)T Whj (dot product), (Whi)T BWhj (general)GATs uses the concat attention. Zhang et al. (2018) and Ryu et al. (2018) separately explores dotproduct and general attention in GNNs. Also, LReLU(∙) means the leaky ReLU activation.
Table 2: Discrepancy between static attention and learned attention by the first head in the first layer	Cora	Citeseer	Pubmed	PPI	CEP	HIVuniform vs learned	0.0083	0.0020	0.0059	0.5442	0.1754	0.2376GCN vs learned	0.1118	0.0796	0.1999	0.5791	0.1759	0.2258Is the statistics of the learned attention related to the intrinsic properties of the graph? Q5: How totransfer attention for further usage?To answer Q1, we propose multiple metrics for characterizing attention distributions. For Q2, weexamine the metrics at different layers and compare the change of them over layers. To answer Q3,we run experiments to see how varying the dataset, attention variant and the learning setting impactsthe learned attention. Previous works (KiPf & Welling, 2017; Hamilton et al., 2017; Velickovic et al.,2018) only perform transductive learning on the citation networks and inductive learning on PPI.
Table 3: Graph Classification AccuracyConcat General Dot productAll Layers 94.1 ± 0.5%	95.6 ± 0.7%First Layer 81.3 ± 1.1%	88.4 ± 0.8%Second Layer 83.5 ± 0.6%	89.7 ± 0.6%95.5 ± 0.4%91.3 ± 0.6%85.3 ± 0.5%Figure 4: t-SNE visualization of ‘concat’ attention based features. From left to right, the features arefrom all layers, the first, and the second layer, respectively. See Appendix B.5 for more results.
Table 4: Attention-based SParsification against random SParsification on PubmedSparsification	% edges left	Test scoreNone	100%	78.20 ± 0.70%local top-k (k=1)	57.37 ± 0.78%	77.57 ± 0.12%local top-k (k=2)	71.44 ± 0.74%	78.33 ± 0.29%uniform neighbor	59.13%	74.40 ± 0.36%uniform neighbor	72.72%	75.47 ± 1.03%Table 5: local top-1 sparsificatiton with light GATs of different sizes followed by GraphSAGEPrediction on PPI. For reference, GraPhSAGE can reach a test score of 0.9802 on unsParsified graPhsand a test score of 0.5431 on graphs sparsified by baseline methods with about 30% edges left.
Table 5: local top-1 sparsificatiton with light GATs of different sizes followed by GraphSAGEPrediction on PPI. For reference, GraPhSAGE can reach a test score of 0.9802 on unsParsified graPhsand a test score of 0.5431 on graphs sparsified by baseline methods with about 30% edges left.
Table 6: Statistics and properties for single graph datasetsProperties	Datasets			Cora	Citeseer	Pubmed# node feats	1433	3703	500# classes/labels	7	6	3# graphs	1	1	1# nodes	2708	3327	19717# isolates	0	48	0# edges	5278	4552	44324# mean degree	1.95	1.37	2.25# train nodes	140 (5.2%)	120 (3.6%)	60 (0.3%)# train edges	5278	4552	44324# val nodes	300(11.1%)	500 (15.0%)	500 (2.5%)# val edges	5278	4552	44324# test nodes	1000 (36.9%)	1000 (30.1%)	1000 (5.1%)# test edges	5278	4552	44324Table 7: Statistics and properties for multi-graph datasetsProperties	Datasets			PPI	CEP	HIV# node feats	50	58	75
Table 7: Statistics and properties for multi-graph datasetsProperties	Datasets			PPI	CEP	HIV# node feats	50	58	75# classes/label size	121 (binary multi-label)	1 (regression)	1 (binary)# graphs	24	29978	41913# nodes	56944	829135	1069968# isolates	256	0	2351# edges	793632	1000952	1151942# average degree	13.9	1.2	1.1# train graphs	20	17986 (60.0%)	33530 (80.0%)# train nodes	44906 (78.9%)	497311 (60.0%)	846823 (79.1 %)# train edges	613184 (77.3%)	600365 (60.0%)	906666 (78.7 %)# val graphs	2	5995 (20.0%)	4191 (10.0%)# val nodes	6514 (11.4%)	165916 (20.0%)	117216 (11.0%)# val edges	99460 (12.5%)	200269 (20.0%)	128751 (11.2%)# test graphs	2	5997 (20.0%)	4192 (10.0%)# test nodes	5524 (9.7%)	165908 (20.0%)	105929 (9.9%)# test edges	80988 (10.2%)	200318 (20.0%)	116525 (10.1%)12
Table 8: GAT performance with three attention typesDatasets	Reference	Concat	General	Dot productCora	83.0 ± 0.7%	83.0 ± 0.7%	84.2 ± 0.5%	84.0 ± 0.5%Citeseer	72.5 ± 0.7%	72.5 ± 0.7%	71.5 ± 0.9%	71.4 ± 0.8%Pubmed	79.0 ± 0.3%	79.0 ± 0.3%	78.6 ± 0.0%	78.2 ± 0.7%PPI	0.973 ± 0.00	0.973 ± 0.00	0.982 ± 0.00	0.975 ± 0.00PPI trans 5%		0.476 ± 0.03	0.565 ± 0.01	0.524 ± 0.01PPI trans 79%		0.950 ± 0.00	0.936 ± 0.01	0.947 ± 0.00Cora inductive		87.6 ± 1.7%	88.1 ± 1.7%	88.4 ± 1.3%Citeseer inductive		84.2 ± 0.9%	84.1 ± 1.5%	84.8 ± 1.3%Pubmed inductive		85.3 ± 1.1%	86.5 ± 1.2%	86.0 ± 0.8%CEP	0.66 ± 0.12 (Ryu et al., 2018)2	0.43 ± 0.02	0.39 ± 0.02	0.44 ± 0.02HIV	0.776 (Li etal., 2017)	0.746 ± 0.02	0.760 ± 0.01	0.758 ± 0.02B.4	Test performance across attention variantsWe evaluate test performance using different metrics for different datasets - accuracy for Cora,Citeseer, Pubmed, micro-averaged F1 score for PPI, mean absolute error for CEP and roc auc scorefor HIV. See table 8 for a summary of the prediction performance, where different attention variantsmostly have similar performance. The reference numbers are from VeIiCkOviC et al. (2018) unlessstated otherwise. For Cora, Citeseer, Pubmed and PPI, we include the original results of GATs forreference. For the rest datasets, we include the best performance of previous work for reference
