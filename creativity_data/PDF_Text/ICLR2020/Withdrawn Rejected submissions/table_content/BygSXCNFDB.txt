Table 1: Example of the observations provided by the CookingWorld environment.
Table 2: Statistics of the two families of text-based games used in the experiments. The averageis among the different games in CookingWorld and among different instance of the same game forCoinCollector.	__________________________________________	CoinConector	CookingWorldVocabulary Size (| V |)	1,250	20,000Action Space	10	20,0005#Level	1 (Hard)	222Max Room	90	12Avg. #Tok. Description	64 ± 9	97 ± 49Avg. #Adm. Actions	10	14 ± 132.2 Phase 2: GeneralizationPhase 2 of Go-Explore uses the trajectories found in phase 1 and trains a policy based on thosetrajectories. The goal of this phase in the original Go-Explore algorithm is to turn the fragile policyof playing a trajectory of actions in sequence into a more robust, state-conditioned policy that canthus deal with environmental stochasticity. In our variant of the algorithm the purpose of the secondphase is generalization: although in our environment there is no stochasticity, our goal is to learna general policy that can be applied across different games and generalize to unseen games. In theoriginal Go-Explore implementation, the authors used the backward Proximal Policy Optimizationalgorithm (PPO) (Salimans & Chen, 2018; Schulman et al., 2017) to train this policy. In this workwe opt for a simple but effective Seq2Seq imitation learning approach that does not use the reward
Table 3: CookingWorld results on the three evaluated settings single, joint and zero-shot.
