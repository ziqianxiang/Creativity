Table 1: The data statistics for two different datasets, which both derived from the exiting data-to-text datasets.
Table 2: Model performance under automatic evaluation. Results are averaged over 3 runs Â± onestandard deviation. Except for the content score 2, the higher the score is, the better performance is.
Table 3: Human Evaluation Results. Top: Humans are asked to score the model outputs in termsof content fidelity, style preservation, and fluecny, respectively, from 1 (strongly bad) to 5 (stronglygood). As expected, the rule-based method reaches the maximum possible scores in terms of stylepreservation and fluency, but a much lower score in terms of content fidelity. Our LSTM-basedmodel is more balanced across all aspects, and performs significantly better in accurately describingdesired content. Bottom: Humans are asked to rank a pair of generated sentences in which one isfrom our model and the other from the comparison method. Our model wins on more than 50%instances compared to each of other models on two datasets.
Table 4: Example outputs. Text of erroneous content is highlighted in red, where [...] indicatesdesired content is missing. Text portions in the reference sentences and the generated sentences byour model that fulfill the stylistic characteristics are highlighted in blue. Please see the text for moredetails.
