Table 1: Results show the performance gap between seen (‘Val Seen’) and unseen (‘Val Unseen’) en- vironments in several VLN tasks. Room-to-Room and Room-for-Room are evaluated with ‘Success Rate’, CVDN is evaluated with ‘Goal Progress’, Touchdown is evaluated with ‘Task Completion’.				Task	Method	Result				Val Seen	Val Unseen	AbsGap ∣∆∣	R2R (Anderson et al., 2018b)	38.6	21.8	16.8	RPA (Wang et al., 2018b)	42.9	24.6	18.3	S-Follower (Fried et al., 2018)	66.4	35.5	30.9	RCM (Wang et al., 2019b)	66.7	42.8	23.9Room-to-Room	SMNA (Ma et al., 2019a)	67	45	22(Anderson et al., 2018b)	Regretful (Ma et al., 2019b)	69	50	19	EnvDrop (Tan et al., 2019)	62.1	52.2	9.9	ALTR (Huang et al., 2019)	55.8	46.1	9.7	RN+Obj (Hu et al., 2019)	59.2	39.5	19.7	CG (Anderson et al., 2019)	31	31	0	Our baseline	56.1	47.5	8.6	Our learned-semantic	53.1	53.3	0.2Room-for-Room	Speaker-Follower	51.9	23.8	28.1	RCM	55.5	28.6	26.9(Jain et al., 2019)	Our baseline	54.6	30.7	23.9	Our learned-semantic	36.2	36.1	0.1Γ,∖7ΓΛM	NDH	5.92	2.10	3.82
Table 2: Results on our re-splitting data showing the path-level and environment-level localities.
Table 3: Results showing that our semantic feature representations eliminate the performance gap inall three datasets.
