Table 1: Comparison between GR and other learning supervisions. 0-0.5 and 0-1 indicate the em-phasis focus is adjustable and ranges from 0 to 0.5 and0 to 1, respectively. Note that GR manipulatesthe gradients and is independent of specific losses, e.g., CCE, MAE and GCE.
Table 2: Classification accuracies (%) of CCE, and GR on clean CIFAR-10 and CIFAR-100. λ = 0means the emphasis focus is 0 where we fix β = 2. β = 0 means all examples are treated equally.
Table 3: Results of CCE, GR on CIFAR-10 with noisy labels. For every model, we show its best testaccuracy during training and the final test accuracy when training terminates, which are indicated by‘Best’ and ‘Final’, respectively. We also present the results on corrupted training sets and originalintact one. The overlap rate between corrupted and intact sets is (1 - r). Therefore, we can regardthe intact training set as a validation set. When λ is larger, β should be larger as shown in Figure 1c.
Table 4: The results of GR and other noise-robust approaches on CIFAR-10 using GoogLeNet V1.
Table 5: The accuracies (%) of GR and recent approaches on CIFAR-100. The results of fixedparameters (β = 8, λ = 0.5) are shown in the second last column. With a little effort for optimisingβ and λ, the results and corresponding parameters are presented in the last column. The trend isconsistent with Table 3: When r raises, we can increase β, λ for better robustness. The increasingscale is much smaller. This is because CIFAR-100 has 100 classes so that its distribution of pi(input-to-label relevance score) is different from CIFAR-10 after softmax normalisation.
Table 6: The classification accuracy (%) on Clothing1M with ResNet-50. CCE and GCE werereported in (Patrini et al., 2017) and (Wang et al., 2019c), respectively. CCE* and GCE* are ourreproduced results using the Caffe framework (Jia et al., 2014).
Table 7: The video retrieval results on MARS. For fair comparison, all other methods useGoogLeNet V2 except DRSA and CAE using more complex ResNet-50.
Table 8: Results of GR and other standard regularisers on CIFAR-100. We set r = 40%, i.e., thelabel noise is severe but not belongs to the majority. We train ResNet-44. We report the average testaccuracy and standard deviation (%) over 5 trials. Baseline means CCE without regularisation.
Table 9: The test accuracy (%) of GR and other standard regularisers on Vehicles-10. We trainResNet-44. Baseline means CCE without regularisation. We test two cases: with symmetric labelnoise r = 40% and without symmetric label noise r = 0.
Table 10: The test accuracy (%) of GR and other standard regularisers trained under asymmetriclabel noise. We train ResNet-44. Baseline means CCE without regularisation. We simply fix β =8, λ = 0.5 when GR is used. Better results can be expected if β, λ are optimised for each case.
Table 11: How much fitting of the clean training subset and how much fitting of the noisy trainingsubset? Is it plausible to correct the labels of training data?Our results demonstrate the effectiveness of label correction using DNNs trained by GR.
Table 12: Exploration of GR with different emphasis focuses (centres) and spreads on CIFAR-100when r = 20%, 40%, 60%, respectively. This table presents detailed information of optimising λ, βmentioned in Table 5 in the paper. Specifically, for each λ, we try 5 β values from {2, 4, 6, 8, 10} andselect the best one as the final result of the λ. We report the mean test accuracy over 5 repetitions.
