Table 1: Average MSE on Bindingk model	5	10	20ADKL-GP	0.1017 ± 0.0013	0.0895 ± 0.0015	0.0860 ± 0.0016ADKL-KRR	0.1000 ± 0.0012	0.0893 ± 0.0015	0.0862 ± 0.0009BMAML	0.1059 ± 0.0021	0.1020 ± 0.0029	0.4616 ± 0.4210CNP	0.1063 ± 0.0023	0.1239 ± 0.0219	0.1382 ± 0.0049ECFP4+KRR	0.1166 ± 0.0020	0.1003 ± 0.0009	0.0956 ± 0.0009ECFP4+RF	0.1129 ± 0.0002	0.1016 ± 0.0008	0.0970 ± 0.0003LearnedBasis	0.1274 ± 0.0037	0.1308 ± 0.0032	0.1329 ± 0.0043R2D2	0.1104 ± 0.0023	0.0962 ± 0.0021	0.0921 ± 0.0010Table 2: Average MSE on Antibacterial290 are the Random Forest algorithm with ECFP4 (Extended Connectivity FingerPrints of diameter 4) as291 molecular input representation, and ECFP4 with KRR and tanimoto similarity as a kernel function292 (Olier et al., 2018). During meta-test, each task is partitioned into query and support sets, then the293 support set is used to generate a model which is evaluated on the query set to compute the MSE. This294 process is repeated 30 times per task and the average MSE over the repetitions per task and over all295 tasks is reported in Tables 1 to 3.
Table 2: Average MSE on Antibacterial290 are the Random Forest algorithm with ECFP4 (Extended Connectivity FingerPrints of diameter 4) as291 molecular input representation, and ECFP4 with KRR and tanimoto similarity as a kernel function292 (Olier et al., 2018). During meta-test, each task is partitioned into query and support sets, then the293 support set is used to generate a model which is evaluated on the query set to compute the MSE. This294 process is repeated 30 times per task and the average MSE over the repetitions per task and over all295 tasks is reported in Tables 1 to 3.
Table 3:	Average MSE		on	Table 4: Wilcoxon			Table 5: Wilcoxon	Sinusoidals				p-values	一 BindingDB		p-values -	BindingDB319320321322323324325326327328329330331332333334335336
Table 6: Effect of using task regularization (parameter γtask) on the MSE performancealgorithm	K	γpseudo	γtask Configuration	0.00	0.01	0.10ADKL-KRR	20	0.01	a	0.0585	0.0327	0.0289	10	0.00	b	0.4051	0.2944	0.3671		0.10	c	0.4363	0.2964	0.2882ADKL-GP	5	0.10	d	2.4920	2.2511	2.2994ADKL-KRR	20	0.00	e	0.0574	0.0305	0.0302ADKL-GP	5	0.01	f	2.5611	2.1511	2.2112		0.01	g	3.2933	2.7663	3.0971	10	0.01	h	0.7675	0.7105	0.4352	20	0.00	i	0.1201	0.0873	0.0646ADKL-KRR	20	0.10	j	0.0575	0.0447	0.0273ConfigurationsFigure 5: Relative improvement of the MSE depending on the γtask parameterFor a more in-depth analysis, we show below the similar tables and figures for different values of K (5, 10 and 20). These results confirm that regularizing the task encoder is helpful for any value ofK, even though the impact seems to become much more important as K increases (observe that themaximum improvement in each figure increases with K).
Table 7: Effect of the pseudo-examples regularization (parameter γpseudo)on the MSE performancealgorithm	K	γtask	γpseudo Conf.	0.00	0.01	0.10ADKL-GP	10	0.10	a	0.6079	0.4352	0.5244	20	0.01	b	0.0873	0.0761	0.0882ADKL-KRR	20	0.00	c	0.0526	0.0375	0.0380ADKL-GP	5	0.10	d	2.2801	2.2112	2.2994ADKL-KRR	20	0.01	e	0.0535	0.0325	0.0325ADKL-GP	5	0.01	f	2.9466	2.7663	2.7121	20	0.10	g	0.1147	0.1144	0.0870		0.00	h	0.1201	0.0958	0.0940	5	0.01	i	3.1136	2.1511	2.2511		0.00	j	2.8528	2.5611	2.4920Figure 6: Relative improvement of the MSE depending on the γtask parameterOnce again, for a more in-depth analysis, we show below the same format of tables and figures fordifferent values of K , confirming again that regularizing using the pseudo-representation can be veryhelpful for any value of K . It is worth noticing here that the improvement gain is more consistent for14Under review as a conference paper at ICLR 2020494
