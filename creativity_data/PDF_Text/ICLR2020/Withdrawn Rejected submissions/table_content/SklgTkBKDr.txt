Table 1: Experiment results for visual and textual dataModel	MNIST-R		MNIST-S		Sum-64		Sum-768		MAE	MSE	MAE	MSE	MAE	MSE	MAE	MSESB-s	5.19	40.60	7.60	88.71	1.78	5.03	0.65	1.02SB-l	5.07	39.50	6.99	75.08	0.75	1.23	-	-DS-s	5.20	40.73	7.62	89.26	1.78	5.04	0.49	0.55DS-l	4.19	25.77	7.69	88.72	1.53	4.24	-	-RNN	3.88	22.09	7.36	82.50	0.88	1.42	0.18	0.34RCN	3.43	17.91	7.00	75.01	0.91	1.49	0.12	0.16DCR	3.23	15.57	6.91	74.60	0.90	1.02	0.55	1.00the utility of the summary, because repeating an SCU means that the same information is containedmultiple times in the summary, which is not desired. Synergy effects are not modeled by SCUs.
Table 3: Extrapolation resultsTable 2: Results for varying set lengthsModel	MNIST-R		MNIST-S		Model	MNIST-R		MNIST-S		MAE	MSE	MAE	MSE		MAE	MSE	MAE	MSE^SB-s-	4.93	38.56	7.77	91.13	SB-s	4.92	57.17	25.57	1292.23SB-l	4.87	37.37	7.12	74.70	SB-l	7.25	80.18	32.95	1350.70DS-s	4.93	38.08	7.76	91.07	DS-s	6.55	66.38	16.22	406.44DS-l	4.06	24.00	7.83	86.73	DS-l	6.58	53.21	37.51	1702.10RNN	3.85	21.35	7.46	80.90	RNN	9.37	128.19	35.84	1687.71RCN	3.24	17.34	7.16	76.87	RCN	4.23	29.75	21.74	866.62DCR	3.68	20.20	7.02	76.28	DCR	4.25	37.79	16.69	450.814.3	Extrapolation to longer/shorter sequencesDescription. In the third experiment, we are interested in the extrapolation abilities of the differentarchitectures, i.e. how well they are able to extrapolate beyond the observed training instances. Thisprovides a good insight in whether the models were able to identify and learn the underlying natureof the problem or if they fit to tightly to the observed distribution. To this end, we train the models onsets with length 10. Furthermore, the validation set, according to which the best model is selected,also contains only sets with length 10. The test set, however, only contains sets of size 5 and 20.
Table 2: Results for varying set lengthsModel	MNIST-R		MNIST-S		Model	MNIST-R		MNIST-S		MAE	MSE	MAE	MSE		MAE	MSE	MAE	MSE^SB-s-	4.93	38.56	7.77	91.13	SB-s	4.92	57.17	25.57	1292.23SB-l	4.87	37.37	7.12	74.70	SB-l	7.25	80.18	32.95	1350.70DS-s	4.93	38.08	7.76	91.07	DS-s	6.55	66.38	16.22	406.44DS-l	4.06	24.00	7.83	86.73	DS-l	6.58	53.21	37.51	1702.10RNN	3.85	21.35	7.46	80.90	RNN	9.37	128.19	35.84	1687.71RCN	3.24	17.34	7.16	76.87	RCN	4.23	29.75	21.74	866.62DCR	3.68	20.20	7.02	76.28	DCR	4.25	37.79	16.69	450.814.3	Extrapolation to longer/shorter sequencesDescription. In the third experiment, we are interested in the extrapolation abilities of the differentarchitectures, i.e. how well they are able to extrapolate beyond the observed training instances. Thisprovides a good insight in whether the models were able to identify and learn the underlying natureof the problem or if they fit to tightly to the observed distribution. To this end, we train the models onsets with length 10. Furthermore, the validation set, according to which the best model is selected,also contains only sets with length 10. The test set, however, only contains sets of size 5 and 20.
