Table 1: Classification accuracy (%) of single inference using data augmentation (DA), multi-tasklearning (MT), and ours self-supervised data augmentation (SDA) with rotation. The best accuracyis indicated as bold, and the relative gain over the baseline is shown in brackets.
Table 2: Classification accuracy (%) of the ten-crop, indenpendent ensemble, and our aggregationusing rotation (SDA+AG). The best accuracy is indicated as bold, and the relative gain over thebaseline is shown in brackets.
Table 3: Classification accuracy (%) using self-supervised data augmentation with rotation and colorpermutation. SDA+SD and SDA+AG indicate the single inference trained by LSDA+SD, and theaggregated inference trained by LSDA, respectively. The relative gain is shown in brackets.
Table 4: Classification accuracy (%) depending on the set of transformations. The best accuracy isindicated as bold.
Table 5: Average classification accuracy (%) with 95% confidence intervals of 1000 5-way few-shottasks on mini-ImageNet, CIFAR-FS, and FC100. f and åŽ» indicates 4-layer convolutional and 28-layer residual networks (Zagoruyko & Komodakis, 2016), respectively. Others use 12-layer residualnetworks as Lee et al. (2019). The best accuracy is indicated as bold.
Table 6: Classification accuracy (%) on imbalance datasets of CIFAR10/100. Imbalance Ratio isthe ratio between the numbers of samples of most and least frequent classes. The best accuracy isindicated as bold, and the relative gain is shown in brackets.
