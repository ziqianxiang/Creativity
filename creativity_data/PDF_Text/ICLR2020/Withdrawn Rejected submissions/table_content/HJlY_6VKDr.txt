Table 1: Attacker’s success rate of black-box attacks for state-of-the-art defenses(b) To the best of our knowledge, there are no papers on defenses against adversarial machine learn-ing based on our concept of buffer zones. This new concept offers a conceptually simple and efficientdefense strategy with rigorous security argument.
Table 2: Mixed black-box attack on CIFAR-10	Vanilla	2-Networks	4-Networks	8-NetworksClean Accuracy	0.8835	0.7599	0.6832	0.6091	:FGSM Targeted-	0.831	-0.951	-0.985	-0.995iFGSM Targeted	0.766	-0978	-0.998	1FGSM Untargeted	0.297	-0797	-0.931	-0976iFGSM Untargeted	0.282	0.814	0.963	0.992	-Table 3: δ values for mixed black-box attacks on FashionMNIST, CIFAR-10, and CIFAR-100	Defense targeted	δt	Defense untargeted	δuFashionMNIST	4-Networks	0.144	8-Networks	0.226-CIFAR-10-	2-Networks	0.161	4-Networks	0.247CIFAR-100~~	Vanilla	0.0423	2-Networks	0.216are uniformly selected from the interval [-0.5, +0.5]. Images x are 32 × 32 × 3 where 3 stands forthe red, blue, green values (each in [-0.5, +0.5]) of a pixel. The same non-zero entry of b is addedto each of the red, blue, green values of the corresponding pixel in x. (We can think of an imagetransformation cj (x) as an extra randomly fixed layer added to the layers which form thej-th CNN.)We tested three BUZz designs: One with 8 networks each using a different image resizing operationfrom 32 to 32, 40, 48, 64, 72, 80, 96, 104. The second with 4 networks being the subset of the 8networks that use image resizing operations from 32 to 32, 48, 72, 96. The third with 2 networksbeing a subset of the 8 networks that use image resizing operations from 32 to 32 and 104.
Table 3: δ values for mixed black-box attacks on FashionMNIST, CIFAR-10, and CIFAR-100	Defense targeted	δt	Defense untargeted	δuFashionMNIST	4-Networks	0.144	8-Networks	0.226-CIFAR-10-	2-Networks	0.161	4-Networks	0.247CIFAR-100~~	Vanilla	0.0423	2-Networks	0.216are uniformly selected from the interval [-0.5, +0.5]. Images x are 32 × 32 × 3 where 3 stands forthe red, blue, green values (each in [-0.5, +0.5]) of a pixel. The same non-zero entry of b is addedto each of the red, blue, green values of the corresponding pixel in x. (We can think of an imagetransformation cj (x) as an extra randomly fixed layer added to the layers which form thej-th CNN.)We tested three BUZz designs: One with 8 networks each using a different image resizing operationfrom 32 to 32, 40, 48, 64, 72, 80, 96, 104. The second with 4 networks being the subset of the 8networks that use image resizing operations from 32 to 32, 48, 72, 96. The third with 2 networksbeing a subset of the 8 networks that use image resizing operations from 32 to 32 and 104.
Table 4: Mixed black-box attacks on defenses Randomized 1-Net (Xie et al. (2018)), Adv Retrained1-Net (Tramer et al. (2017)), Mul-Def 2,4,8-Net (Srisakaokul et al. (2018)), Mixed Arch 2-Net (Liuet al. (2017)) and BUZz 2, 4, 8-Net.
Table 5: Attacker’s success rate of black-box attacks for state-of-the-art defensesrepresents black-box access to the target model f which only returns the final class label F (f (x))for a query x (and not the score vector f (x)). Initially, the attacker has (part of) the training data setX, i.e., he knows D = {(x, F (f (x))) : x ∈ X0} for some X0 ⊆ X. Notice that for a single iterationN = 1, Algorithm 1 therefore reduces to an algorithm which does not need any oracle access toO; this reduced algorithm is the one used in the pure black-box attack (Carlini & Wagner (2017b);Athalye et al. (2018a); Liu et al. (2017)). In this paper we assume the strongest black-box adversaryin Algorithm 1 with access to the entire training data set X0 = X (notice that this excludes test datafor evaluating the attack success rate).
Table 6: Training parameters used in the experimentsTraining Parameter	ValueOptimization Method	ADAMLearning Rate	0.0001Batch Size	64Epochs	100Data Augmentation	NoneTable 7: Papernot Black-Box Attack Parameters	|Xo|	N	λ	Testing set-CIFAR-10-	50000	ɪ	=OT	10000-CIFAR-100-	50000	ɪ	ɪr	-10000-FaShiOn MNIST^	60000	ɪ	ɪr	10000Table 8: Architectures of synthetic neural networks g from (Carlini & Wagner (2017a))Layer TyPe	Fashion MNIST and CIFAR-10	CIFAR-100Convolution + ReLU	3 X 3 X 64	3 X 3 X 64=Convolution + ReLU	3 × 3 × 64	3 x 3 x 64Max Pooling	2X2	2 x 2Convolution + ReLU	3 X 3 X 128	3 x 3 x 128Convolution + ReLU	3 X 3 X 128	3 X 3 X 128Max Pooling	2X2	2 x 2
Table 7: Papernot Black-Box Attack Parameters	|Xo|	N	λ	Testing set-CIFAR-10-	50000	ɪ	=OT	10000-CIFAR-100-	50000	ɪ	ɪr	-10000-FaShiOn MNIST^	60000	ɪ	ɪr	10000Table 8: Architectures of synthetic neural networks g from (Carlini & Wagner (2017a))Layer TyPe	Fashion MNIST and CIFAR-10	CIFAR-100Convolution + ReLU	3 X 3 X 64	3 X 3 X 64=Convolution + ReLU	3 × 3 × 64	3 x 3 x 64Max Pooling	2X2	2 x 2Convolution + ReLU	3 X 3 X 128	3 x 3 x 128Convolution + ReLU	3 X 3 X 128	3 X 3 X 128Max Pooling	2X2	2 x 2Fully Connected + ReLU	256	256Fully Connected + ReLU	256	256Softmax	10	100White-box attack on the synthetic network. The targeted iterative Fast Gradient Sign Method(FGSM) of (Goodfellow et al. (2014)) is given in Algorithm 2 (for our implementation we usethe cleverhans library, see https://github.com/tensorflow/cleverhans). The non-iterative variant has outer loop size H = 1. For a untargeted attack no adversarial label l0 is given as
Table 8: Architectures of synthetic neural networks g from (Carlini & Wagner (2017a))Layer TyPe	Fashion MNIST and CIFAR-10	CIFAR-100Convolution + ReLU	3 X 3 X 64	3 X 3 X 64=Convolution + ReLU	3 × 3 × 64	3 x 3 x 64Max Pooling	2X2	2 x 2Convolution + ReLU	3 X 3 X 128	3 x 3 x 128Convolution + ReLU	3 X 3 X 128	3 X 3 X 128Max Pooling	2X2	2 x 2Fully Connected + ReLU	256	256Fully Connected + ReLU	256	256Softmax	10	100White-box attack on the synthetic network. The targeted iterative Fast Gradient Sign Method(FGSM) of (Goodfellow et al. (2014)) is given in Algorithm 2 (for our implementation we usethe cleverhans library, see https://github.com/tensorflow/cleverhans). The non-iterative variant has outer loop size H = 1. For a untargeted attack no adversarial label l0 is given asinput and the loss function L is only a function ofx, l, and θg (the loss function L should be properlydefined to make the attack targeted attack or untargeted). The algorithm walks in H iterations along18Under review as a conference paper at ICLR 2020the gradient towards the boundary of x’s region (where a region with label l0 should start). The
Table 9: Parameters for iFGSM and FGSM in the mixed black-box attack on BUZz for CIFAR-10,CIFAR-100 and Fashion MNIST	euntargeted	HUntargeted	etargeted	Htargeted-CIFAR-10-	10/256	10	二	1/20	10-CIFAR-100-	-10/256-	10	-1720-	10FaShiOn MNSr	0.1	10	-	0.3	10We enumerate in Table 9 the parameters e and H used in our experiments - We have taken thesefrom literature (Meng & Chen (2017)). Notice that literature often reports e/H as the step size inFGSM while we list values for e which corresponds to the size of the final perturbation that leadsto the adversarial example (and as such we can interpret e as the threshold for noise being visuallyperceptible by the human eye).
Table 10: Mixed black-box attack on CIFAR-10- the best case	Vanilla	2-Networks	4-Networks	8-NetworksClean Accuracy	0.8835	0.76	:	0.69	:	0.62 ZFGSM Targeted	0.831	0.96	099	0.99iFGSM Targeted	0.766	0.98	099	1FGSM Untargeted	0.297	085	094	0.97iFGSM Untargeted	0.282	0.87	∙	0.97	∙	0.99 一Fashion MNIST PaPernot Attack10.90.80.70.60.50.40.30.20.10—III
