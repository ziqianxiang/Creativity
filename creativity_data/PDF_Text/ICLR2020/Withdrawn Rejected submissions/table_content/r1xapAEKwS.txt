Table 1: Recognition error rate (%) and number of nonzero weights		Error rate (%)					Number of nonzero weights				SDGM			Baselines		SDGM			Baselines	Dataset	w/ sparse	w/o sparse	LR	SVM	RVM	w/ sparse	w/o sparse	LR	SVM	RVMRipley	9.1	9.9	11.4	10.6-	9.3	6	1255	2	38	4Banana	10.6	10.8	47.0	10.9	10.8	11.1	2005	2	135.2	11.4Waveform	10.1	9.5	13.5	10.3	10.9	11.0	2005	20.73	146.4	14.6Titanic	22.7	23.3	22.7	22.1	23.0	74.5	755	2.98	93.7	65.3Breast Cancer	29.4	35.1	27.5	26.9	29.9	15.73	1005	8.88	116.7	6.3Normalized mean	1.00	1.05	1.79	1.02	1.03	1.00	129.35	0.60	8.11	0.86In Figure 3(b), the number of components after training corresponds to the number of initial com-ponents until the number of initial components is eight. When the number of initial componentsexceeds ten, the number of components after training tends to be reduced. In particular, eight com-ponents are reduced when the number of initial components is 20. The results above indicate theSDGM can reduce unnecessary components.
Table 2: Recognition error rates (%) on image classificationMNIST	(D	= 2)	MNIST	(D =	10)	Fashion-MNIST	CIFAR-10Softmax	3.19	1.01	8.78	11.07SDGM	2.43	0.72	8.30	10.05trained in an end-to-end manner. In particular, the SDGM plays the same role as the softmax func-tion since the SDGM calculates the posterior probability of each class given an input vector. We canshow that a fully connected layer with the softmax is equivalent to the discriminative model basedon a single Gaussian distribution for each class by applying a simple transformation (see AppendixA), whereas the SDGM is based on the Gaussian mixture model.
