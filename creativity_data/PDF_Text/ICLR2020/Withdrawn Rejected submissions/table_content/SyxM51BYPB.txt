Table 1: An overview of first-order optimization methods using the generic framework	SGD	SGDM	AdaGrad	RMSProP	Adammt	gt	Pit=1 βt-igi	gt	gt	(1 -β1)Pit=1β1t-igivt	I	I	Pit=1 gi2	(1-β) Pt=ι βt-⅞T	(1 - β2) Pit=1 β2t-igi2lr	ηt	ηt	η √v	η √v	η √v	paper, more details can be found in McMahan (2010b). The last term in the above equation isstabilizing regularization to ensure low regret. We mention that the Qτ can be either regarded asscale of regularization or generalized learning rate which plays crucial role in our paper. As wecan see, FTPRL appears quite different from MDA stated in Equation. 5, however, in McMahan(2010a;b) they show that in the case of selecting quadratic stabilizing regularization, the FTPRL andgeneralized MDA only has differences in parameter centering. In fact, MDA is illustrated in theEquation. 4 regularizing the parameter to be close to the origin, on contrast, FTPRL is regularizingthe parameter at current feasible point. No surprising, McMahan (2010a) propose the equivalenceproof of FTPRL and a variation algorithm of the MDA as follow.
Table 2: An overview of first-order optimization methods using Mirror Descent expression	SGD	SGDM		AdaGrad	Adam	A	gi：t ∙ X	(g1:t	i=t,j=t -X βj+1-i) ∙ X i=1	gt∙ X	(g2:t	i=t,j =t -X βj+1-i)∙X i=2C	2 ||x - xτ||2		2 ||x - xτ ||2	1 t 2 Eσ"lx - χτ ||2 T = 1	1 2	t X στ ||x - χτ ||2 τ=2ρ	1		1 1-β	1		1* Term A, C, ρ are defined in Equation 11. See below.
Table 3: hyperparameter tuning, learn-ing rate halve at iteration 75, 125, 175.
