Table 1: Percentage (%) of environments where the final performance “improves" when usingregularization, according to our definition in Section 4.2.
Table 2: The average rank in the mean return for different regularization methods. L2 regularizationtops the ranking for most algorithms and environment difficulties.
Table 3: The standard deviation of the ranks for different regularization methods. L2, L1 and weight Clipping mostly have slightly smaller standard deviations. 5 Robustness with Hyperparameter Changes					In the previous section, the experiments are conducted mostly with the default hyperparameters inthe codebase we adopt, which are not necessarily optimized. For example, PPO Humanoid baselineperforms poorly using default hyperparameters, not converging to a reasonable solution. Meanwhile,it is known that RL algorithms are very sensitive to hyperparameter changes (Henderson et al.,2018). Thus, our findings can be vulnerable to such variations. To further confirm our findings,we evaluate the regularizations under a variety of hyperparameter settings. For each algorithm, wesample five hyperparameter settings for the baseline and apply regularization on each of them. Dueto the heavy computation budget, we only evaluate on five MuJoCo environments: Hopper, Walker,Ant, Humanoid, and HumanoidStandup. Under our sampled hyperparameters, poor baselines aremostly significantly improved. For further details on sampling and training curves, please refer toAppendix E and K.
Table 4: The average rank in the mean return for different regularization methods, under five randomlysampled training hyperparameters for each algorithm.
Table 5: The standard deviation of the ranks for different regularization methods, under five randomlysampled training hyperparameters.
Table 6: Percentage (%) of environments where the final performance ”improves” when applyingregularization on policy / value / policy and value networks.
Table 7: Hyperparameter setting for A2C MuJoCo and RoboSchool tasks15Under review as a conference paper at ICLR 2020Hyperparameter	ValueHidden layer size	32 × 2Number of hidden layers	2Sharing policy and value weights	FalseRollout timesteps per actor	1024Number of actors	1Value network step size	1e — 3, constantMax KL divergence	0.01Discount factor (γ)	0.99GAE parameter (λ)	0.98Conjugate gradient damping	0.1Conjugate gradient iterations	10Value network optimization epochs	10Value network update minibatch size	64Probability ratio clipping range	0.2Hyperparameter	ValueHidden layer size	64 × 2
Table 8: Hyperparameter setting for TRPO Mujoco and RoboSchool tasks. The original OpenAIimplementation does not support multiple actors sampling trajectories at the same time, so wemodified the code to support this feature and accelerate training.
Table 9: Hyperparameter setting for PPO MuJoCo and RoboSchool tasksHyperparameterHidden layer sizeNumber of hidden layersSamples per batchReplay buffer sizeLearning rateDiscount factor (γ)Target smoothing coefficient (τ)Target update intervalReward ScalingValue256 × 222561063e - 4 constant0.990.0051
Table 10: Hyperparameter setting for SACE	Hyperparameter Sampling DetailsIn Section 5, we present results based on five hyperparameter settings. To obtain such hyperparametervariations, we consider varying the learning rates and the hyperparameters that each algorithm is verysensitive to. For A2C, TRPO, and PPO, we consider a range of rollout timesteps between consecutivepolicy updates by varying the number of actors or the number of trajectory sampling timesteps foreach actor. For SAC, we consider a range of reward scale and a range of target smoothing coefficient.
Table 11: Sampled hyperparameter settings for Section 5F Hyperparameter Experiment Improvement PercentageWe provide the percentage of improvement result in Table 12 as a complement with Table 4.
Table 12: Percentage (%) of environments where the final performance ”improves” when usingregularization, under five randomly sampled training hyperparameters for each algorithm.
Table 13: Percentage (%) of environments that, when using a regularization, ”improves”. For eachalgorithm, one single strength for each regularization is applied to all environments.
Table 14: The average rank in the mean return for different regularization methods. For eachalgorithm, one single strength for each regularization is applied to all environments.
Table 15: The standard deviation of rank in the mean return for different regularization methods,when one single strength for each regularization is applied to all environments.
Table 16: The fixed single regularization strengths that are used in each algorithm to obtain results inTable 13 and Table 14.
