Table 1: The averaged accuracy of tasks after training of all the 5 tasks are done. WR, ER, and GRmean weight regularization, experience replay, and generative replay, respectively.
Table 2: Ablation experiment for effectiveness of domain translation on Split CIFAR10 setting.
Table 3: Comparison of domain transla-tion tricks with DiVA. Results are aver-aged after each setting is all done.
Table 4: Implementation details of the DiVA for Permuted MINST and Split MNIST settingsNetWork	Operation	Kernel or Input dims	Stride/Padding or Output dims	BatchNorm	ActivationEncoder	Convolution Convolution Convolution Convolution Fully-connected Fully-connected Fully-connected	-3 × 3 × 1 × 32 3 × 3 × 32 × 64 3 × 3 × 64 × 128 3 × 3 × 128× 256 2 × 2 ×256 256 256	2/1 2/1 2/1 2/1 256 128 128	√ √ √ √ √ - -	ReLU ReLU ReLU ReLU ReLU ReLU SoftplusDecoder	Fully-connected Convolution Convolution Convolution Convolution	128 3 × 3 × 128 × 128 3 × 3 × 128 × 64 3 × 3 × 64 × 32 3 × 3 × 32 × 1	2 × 2 × 128 2/1 2/1 2/1 2/1	√ √ √ √ -	ReLU ReLU ReLU ReLU SigmoidPrior Net	Fully-connected Fully-connected	10 10	128 128	- -	ReLU SoftplusClassifier	Fully-connected	128	10	-	SoftmaxHyper- parameters	Optimizer Batch size Softplus λ for DiVA Gaussian noise Epochs per task	Adam(lr=0.001, betas=(0.9, 0.999), eps=1e-08) 64 beta=1, threshold=20 Permuted: 5e+3, Split: 2e+2 mu=0, std=0.3 Permuted: {100, 100, 200, 200, 200} Split: {100,100, 200, 200, 200}			11Under review as a conference paper at ICLR 2020error or binary cross entropy. Also, when we conducted some experiments with this adding noisetechnique for the EWC and the VCL, there was no noticeable increase in final accuracy for 5 tasksincremental learning. The detailed structures of the DiVA for Permuted and Split MNIST settingsare described in Table 4, and learning curves for Permuted and Split MNIST tasks are illustrated inFigure 3. Also, for Split CIFAR10 setting, we implemented our encoder network for DiVA basedon https://github.com/xternalz/WideResNet-pytorch. Depth, widen_factor, and dropRate are 28, 4,and 0.5, respectively. Also, we set the latent dimension for mu and variance of DiVA as 1024 andstacked one fully connected layer for both prior and classification network. We did not apply randomnoise to the Split CIFAR10 setting. The decoder network is doubled input and output channels thandescribed in Table 4. For experimental results, we used 60,000 images for training and 10,000 imagesfor testing in MNIST variant settings. Also, we used 50,000 images for training and 10,000 images
