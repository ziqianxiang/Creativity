Table 1: Per-core supergraph size, when training our low-bandwidth model on TPUv2.
Table 2: Global steps (gradient updates) per second for different model, hardware, and block size combina-tions.
Table 3: Training speed (graphs/s) on slices of the training data with large bandwidth graphs discarded.
Table 4: Number of training graphs that fit into a supergraph on average. See Section C.1. The number ofnodes in a supergraph is constant for the GPU model thanks to the sparse representation.
Table 5: Training steps (and training time) until 78% validation accuracy on bandwidth-filtered data.
Table 6: Validation accuracy on varying slices of the training data.
