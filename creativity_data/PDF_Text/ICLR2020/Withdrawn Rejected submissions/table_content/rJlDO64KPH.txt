Table 1: Vary mixing ratio and beam size. An LPM weight α = 0.2 is used.
Table 2: Vary proposal model update strategies and online model initialization.
Table 3: Ablation study for length filtering.
Table 4: Length filtering using different ref-erence lengths.
Table 5: Comparison of LPM and alternative hypothesis weighting methods.				Table 6: Using py of different de- velopment token perplexity.	PV Ix y|x	dev-clean / dev-other WER (%)			py PPL	dev-{clean / other}	k=2	k=4	k=8	34.24	9.00 / 26.47Proposed	9.34 / 27.45	9.00 / 26.47	9.15 / 26.52	64.22	10.08 / 26.92LenNorm	10.52 / 29.10	10.32 / 28.69	10.20 / 28.39	97.87	10.90 / 27.97Shuffled	10.76 / 29.32	10.94 / 29.17	10.56 / 29.20	142.12	11.53 / 28.74Uniform	10.96 / 29.46	10.74 / 29.09	10.84 / 29.21	180.71	13.18 / 30.744.5	Final Results and Comparison with LiteratureThe best performing model is trained for 3.2M steps, with a learning rate annealed by a factor of twoevery 1.28M steps when using 360 hours of unpaired speech, and every 0.64M steps when using 860hours of unpaired speech. Reference lengths for filtering are obtained from ASR+LM beam searchdecoding. We compare LPM to fully supervised models and a pseudo labeling (PL) method inTable 7. Compared to LPM, PL demands extensive hyper-parameter tuning for the ASR+LM beamsearch as well as heuristic data filtering methods. to obtain high quality pseudo transcripts. We7Under review as a conference paper at ICLR 2020chose PL as a baseline because we follow the same experimental setup, and, more importantly, itachieves the state-of-the-art results on LibriSpeech when using train-clean-100 as paired data andtrain-other-360 as unpaired speech. A more complete table with results from the literature is in theAppendix.
Table 7: Results of baselines and the proposed methods.
Table 8: Comparison of the label quality on the unlabeled speech data between the two methodswhen trained on 100hr Dl and 860hr Dus .
Table 9: Comparison of beam search hypotheses from the supervised model (100hr Dl) and themodel trained with LPM (100hr Dl + 360hr Dus). Hypotheses are ranked by ASR score qy|x.
Table 10: LM token perplexity of ground truth texts and hypotheses obtained with greedy decoding.
Table 11: A more comprehensive comparison with semi-supervised ASR studies using LibriSpeech,including the performances of the baseline/topline supervised model used in each study, since theydiffer significantly across different papers. Dl and Du denote the amount of paired and unpaireddata used in each experiment, and (S)/(T)/(S+T) denote the type of the unpaired data, correspondingto SPeech/text/both, respectively. Experiments with the asterisk sign (*) contain results that are notreported in the original paper, but are obtained from the authors of the paper.
Table 12: Results of varying LPM weight α.
Table 13: Results of baselines and the proposed methods.
