Table 1: RMSE of different models on regression tasksmethods	wine-quality	power-plant	kin8nm	boston-housing	concrete-strengthFF	0.60 ± 0.04*z	4.05 ±0.17*	0.100 ± 0.002^^	2.82 ± 0.76*	5.10 ±0.49*FM	0.73 ± 0.09	4.43 ± 0.15	0.155 ± 0.004	4.80 ± 1.14	8.52 ±0.59PK	4.39 ± 5.50	4.05 ± 0.15*	0.100 ± 0.005	41.9 ± 77.2	7.95 ±2.42PNN	5.49 ± 16.5	5.83 ± 1.39	0.102 ± 0.007	4.59 ± 2.74	5.58 ±0.48LPNN	0.82± 0.18 一	4.13 ± 0.16	0.099 ± 0.0Q6*~	4.05 ± 2.13	5.20 ±0.62Table 2: Error rates of different models on classification tasksmethods	mnist	fashion-mnist	skin	sensIT	letter	covtype-b	covtypeFF	0.0185	0.108	0.0313	0.176	0.096	0.113	0.146FM	0.0573	0.167	0.0439	0.260	0.546	0.208	0.575PK	0.0506	0.168	0.0039	0.225	0.248	0.191	0.494PNN	0.0503	0.127	0.0018	0.199	0.104	0.097	0.103LPNN	0.0171	0.117	0.0017	0.175	0.0729	0.117	0.1404	Experiment4.1	Experiment setupIn this section, we evaluate the LPNN on several learning tasks. The LPNN is comparedagainst feedforwrad network and three polynomial learning models. All models are summarizedbelow.
Table 2: Error rates of different models on classification tasksmethods	mnist	fashion-mnist	skin	sensIT	letter	covtype-b	covtypeFF	0.0185	0.108	0.0313	0.176	0.096	0.113	0.146FM	0.0573	0.167	0.0439	0.260	0.546	0.208	0.575PK	0.0506	0.168	0.0039	0.225	0.248	0.191	0.494PNN	0.0503	0.127	0.0018	0.199	0.104	0.097	0.103LPNN	0.0171	0.117	0.0017	0.175	0.0729	0.117	0.1404	Experiment4.1	Experiment setupIn this section, we evaluate the LPNN on several learning tasks. The LPNN is comparedagainst feedforwrad network and three polynomial learning models. All models are summarizedbelow.
Table 3: Effect of batch normalization and dropoutL	BN and dropout	only dropout	only BN	neither1	-7.77 ± 0.53^^	7.78 ± 0.54	7.82 ± 0.55	7.76 ± 0.532	6.05 ± 0.50	5.98 ± 0.57	6.26 ± 0.87	6.30 ± 0.963	5.20 ± 0.62	5.12 ± 0.58	5.82 ± 1.13	6.89 ± 2.065	4.72 ± 0.66	4.92 ± 0.78	5.46 ± 1.83	7.86 ± 3.1010	5.11 ± 2.18	4.71 ± 0.90	4.97 ± 0.97	7.49 ± 2.77and responses of the activation functions at three different layers. We plot the response hiagainst the corresponding ui for each hidden unit i to generate a subplot. We randomly select400 instances and plot each (hi, ui) pair. We plot four hidden units at each of all three hiddenlayers and generate plots in Figure 1.
Table 4: Effect of batch normalization and dropout on the mnist datasetL	BN and dropout	only dropout	only BN	neither1	0.0191	0.0208	0.0242	0.01712	0.0191	0.0188	0.0202	0.01923	0.0170	0.0187	0.0229	0.02075	0.0207	0.7657	0.0271	0.894710	0.0230	0.0298	0.0207	0.0241performances on wine-quality and boston-housing because the model does not fit the two tasks-its performances are bad on most splits.
