Table 1: Classification accuracy performance in % on digits for the two training criteria on thetarget domain test set. Standard deviation is in %.
Table 2: CE on ads for ADV modelsDataset	ads-kaggle	ads-realNaive	0.403 =	XSource-Missing	0.545±0.019	0.663±0.011Source-Partial	0.406±0.00046	0.622±0.0048AdaPtation-Missing	0.397±0.0057	0.660±0.025Adaptation-Partial	0.403±0.0030	0.634±0.0082Adaptation-Imputation	0.389±0.014	0.583±0.013improves over the same model without adaptation. The only exception is the --Partial settingin ads-real. A third observation is that the missing component indeed contains relevant infor-mation: CE performance on source data (not reported in Table 2) shows that Source-Missingwhich exploits the x-2 component is consistently higher than Source-Partial which doesnot exploit this component, leading to relative gains of the former over the latter of 5.6% onads-kaggle and 8.2% on ads-real. Adaptation-Imputation is able to generate and toexploit this information.
Table 3: Accuracy on digits and CE on ads-kaggle for ADV Adaptation-ImputationDataset	digits				ads-kaggleAdaptation direction	MNIST → USPS	USPS → MNIST	SVHN → MNIST	MNIST → MNIST-M	Known → NewL2 + L3	64.2±1.8	51.3±2.5	44.5±1.4	24.1±2.6	0.410±0.0020="	Li + L2 + L3		75.2±1.5	81.5±0.8	54.0±1.4	58.5±1.6	0.401±0.0014LMSE	71.9±3.7	81.4±1.2	52.5±3.7	56.5±2.8	0.400±0.001T=LADV	28.6±3.2	39.4±5.2	28.8±3.8	30.0±3.7	0.469±0.13LADV + LMSE	75.2±1.5	81.5±0.8	54.0±1.4	58.5±1.6	0.401±0.00140.1 × LADV + LMSE	73.4±2.7	81.3±0.8	53.0±2.0	56.2±2.6	0.401±0.0O2F=LADV + 0.001 × LMSE	37.3±2.5	31.2±3.8	45.0±2.6	50.0±3.4	0.440±0.11LADV + 0.005 × LMSE	47.8±3.7	49.6±5.8	46.0±2.6	50.6±2.2	0.388±0.015LADV + 0.01 × LMSE	53.6±2.4	57.0±3.6	43.4±1.1	51.0±2.5	0.397±0.0046LADV + 0.1 × LMSE	68.2±4.2	50.3±6.8	54.0±2.1	51.5±3.6	0.402±0.0046LADV + LMSE	75.2±1.5	81.5±0.8	54.0±1.4	58∙5±1.6	0.401±0.00146 ConclusionWe have proposed a new model to solve unsupervised adaptation problems in the presence of non-stochastic noise in the target domain, by using distant supervision from a complete source domain9Under review as a conference paper at ICLR 2020through domain adaptation and imputing missing values on the target domain in a latent space. This
Table 5: Statistics on ads datasetsDataSet	ads-kaggle				ads-real			Domain	Source		Target		Source		Target	I	SPlit	Train	Test-	Train	Test-	Train	-Test-	Train	TeStPositive	246 872	61 841	92 333	22 943	X	二	X	X	XNegative	699 621	174 783	854 160	213 681	X	X	X	XTotal	946 493	236 624	946 493	236 624	24 465 756	3 760 233	819 073	147 358P (Y)	0.2608	0.2613	0.0976	0.0970	X	X	X	XEvaluation On both datasets the train and test sets are fixed. We run each model five times with adifferent seed for initialization (using Xavier Uniform initialization) and report mean and standarddeviations in Table 2.
Table 6: Mean and standard deviation for each feature on ads-kaggleDataset	ads-kaggle	Domain	Source ∣	Targetfeature 1	0.80±2.21-	4.4 ×10-4±0.041feature 2	9.16±13.04	9.01±13.42feature 3	4.40±6.32	3.44±6.19feature 4	2.58±3.27	0.94±2.31feature 5	61.09±37.67	0.0±0.0feature 6	11.26±12.24	0.090±1.69feature 7	4.10±6.23	0.0034±0.13feature 8	5.12±4.50	1.91±4.26feature 9	14.32±11.57	3.273±5.36feature 10	0.046±0.22	1.35 ×10-5±0.0037feature 11	1.08±2.11	4.25 ×10-4±0.029feature 12	0.083±0.78	6.68 ×10-5±0.018feature 13	2.74±3.59	1.21±3.36F Embedding visualization on D I GI T SIn this section we visualize the embeddings learned by the various models on digits by projectingthe embeddings in a 2D space using t-SNE (the original embedding size being 2048). Figure 7 rep-resents the embeddings learned for ADV models on MNIST → MNIST-M. Figures 8 and 9 represent
