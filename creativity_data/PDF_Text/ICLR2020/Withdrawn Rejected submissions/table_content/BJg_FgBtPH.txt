Table 1: An example of loW BLEU score and loW semantic similarity betWeen model response andreference response While the generated response appears reasonable Within the dialogue.
Table 2: Case study. Both our coherence metric and the human evaluation agreed that the generatedresponse is not coherent with the given query, while RUBER indicated this reply is coherent.
Table 3: Correlation between RUBER+BERT and context coherence metric c(r|q) with humanratings (without and with fine-tuning of GPT-2).
Table 4: Correlation between fluency metric f (r) and human ratings without and with fine-tuningof GPT-2. Pairwise mean and max correlations of human ratings.
Table 5: Comparison between the Baseline Dataset and our generated datasets (WS Dataset and CTGDataset) using Spearman Correlations, Pearson Correlations and variance with collected humanratings.
Table 6: Comparison between the Baseline Dataset and our generated datasets (WS Dataset andCTG Dataset) using Inter-Rater Spearman and Inter-Rater Pearson correlations.
