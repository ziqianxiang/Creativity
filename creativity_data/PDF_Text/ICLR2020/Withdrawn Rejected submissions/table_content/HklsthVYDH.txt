Table 1: Attacker network architecture: k, c, s, p denotethe kernel size, output channels, stride and padding pa-rameters of convolutional layers, respectively.
Table 2: Results of different defense methods under the white-box setting.
Table 3: Running timefor one epoch over CIFAR-10 (1st Row) and CIFAR-100 (2nd Row). (Unit: s)Plain Net	PGM Net	Naive L2L	Grad L2L	2-Step L2L106.5 ± 1.5	1310.8 ± 14.2	293.7 ± 3.1	617.5 ± 6.1	805.1 ± 8.1106.9 ± 1.4	1354.8 ± 14.1	310.0 ± 2.9	623.1 ± 6.3	824.7 ± 8.4Experiment Results. Table 2 shows theresults of all methods over CIFAR-10 andCIFAR-100 under the white-box setting.
Table 4: Results of the black-box setting over CIFAR-10. We evaluate L2L methods with slim attackernetworks. _________________________________________________________________Surrogate	Plain Net		-^FGSM Net		PGM Net		FGSM	PGM10	FGSM	PGM10	FGSM	PGM10Plain Net	40.03	5.60	74.42	75.25	67.37	65.92FGSM Net	79.20	85.02	89.90	80.40	64.28	63.89PGM Net	83.80	84.73	84.33	85.29	67.05	65.54Naive L2L	45.52	25.95	83.99	77.94	68.14	67.13Grad L2L	86.10	86.87	87.93	88.01	71.15	69.952-Step L2L 85.83		87.10	86.51	87.60	70.58	69.38Table 5: Experiments under the black-box setting over CIFAR-100. Note that here we only evaluateL2L methods using the slim attacker network.
Table 5: Experiments under the black-box setting over CIFAR-100. Note that here we only evaluateL2L methods using the slim attacker network.
Table 6: Experiments under the black-box setting on SVHN. Note that here we only evaluate L2L methods using the wide attacker network.						Surrogate	Plain Net		FGSM Net		PGM Net		FGSM	PGM10	FGSM	PGM10	FGSM	PGM10Plain Net	21.72	6.94	41.81	33.13	56.77	49.41FGSM Net	57.36	51.54	56.25	38.11	55.99	48.96PGM Net	81.04	81.52	78.66	80.42	54.85	49.21Naive L2L	73.02	42.14	78.11	59.79	85.31	61.08Grad L2L	71.74	74.31	77.19	80.70	71.99	58.712-Step L2L	65.78	74.07	76.13	82.80	61.69	54.1312Under review as a conference paper at ICLR 2020B Slim NetworkTable 7 presents another architecture that we used in the L2L. In this network, the second convolu-tional layer uses downsampling, while the second last deconvolutional layer uses upsampling. Dueto the downsampling, this network is computationally cheap and thus it is computationally fast. Forexample the running time of per epoch for L2L with slim attacker is 480; whereas L2L with theoriginal architecture is 620. However, it loses some information of input and is less stable thanthe original architecture (Table 1). Inspired by residual learning in He et al. (2016), we addressthe above issues by using a skip layer connection to ease the training of this network. Specifically,the last layer takes the concatenation of A(x, y, θ) and the output of the second last layer as input.
Table 7: Attacker Network Architecture.
Table 8: Results ofL2L with the other attacker network under white-box setting over CIFAR.
