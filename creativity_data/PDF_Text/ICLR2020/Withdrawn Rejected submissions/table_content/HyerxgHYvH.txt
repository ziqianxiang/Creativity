Table 1: Comparison of proposed model with existing deep networks showing classification accu-racy in percentage.
Table 2: Comparison with Neural Arithmetic and Logic Unit (NALU) (Trask et al., 2018). Thiscomparison is only values between 0-30 as this is the range in which NALU is trained and operates.
Table 3: Results showing generalization of our proposed approach on higher digit integers	Stanford Dataset			Inhouse Dataset				+	-	×	+	-	×	÷2 digit	100	100	100	100	100	100	1003 digit	100	100	100	100	100	100	1004 digit	100	100	-	100	100	100	1005 digit	100	100	-	100	100	100	1006 digit	100	100	-	100	100	100	1007 digit	100	100	-	100	100	100	1007	ConclusionIn this paper we show that many complex tasks can be divided into smaller sub-tasks, furthermoremany complex task share similar sub-tasks. Thus instead of training a complex end-to-end neuralnetwork, many small networks can be trained independently each accomplishing one specific op-eration. More difficult or complex task can then be solved using a combination of these smallernetwork. In this work we first identify several fundamental operations that are commonly used tosolve arithmetic operations (such as 1 digit multiplication, addition, subtraction, place value shifteretc). These fundamental operations are then learned using simple feed forward neural networks. Wethen reuse these smaller networks to develop larger and a more complex network to solve variousproblems like n-digit multiplication and n-digit division.
