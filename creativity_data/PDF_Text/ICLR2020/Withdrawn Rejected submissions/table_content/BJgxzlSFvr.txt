Table 1: Different regularization rates applied to MNIST.					Table 2: Errors of “mean” and “max” pooling as for MNIST.									error		MAP	NDCG3	NDCG5Dropout’s p for Conv.	1	1	1	1	1	AttRN-HL-mean	^^0.44%~~	0.65%	0.52%Dropout’s p for FC	~Γ~	-^1 ^^	0.9	1	0.9	sd.	(0.01%)	(0.02%)	(0.02%)L2 ’s λ for All	~0~	1e-5	1e-5	1e-4	1e-4	AttRN-HL-max	^^044%~~	0.66%	0.53%						sd.	(0.01%)	(0.02%)	(0.02%)The pretrained models are standard digit classification. The softmax layers of the five models asembeddings are plugged into our full model (where the CNNs are trained together with the rest). Weinitialize the similarity matrix W with the identity matrix and the remaining parameters with zeros.
Table 3: Errors of different image retrieval methods for MNIST.
Table 4: Errors of “mean” and “max” pooling as for MNIST.
Table 5: L2 norms of each layer in the convolutional neural net before and after training AttRN as forMNIST.
Table 6: Errors of different text retrieval methods for 20 NeWsgroUPs with regard to different topics.
Table 7: Errors of different text retrieval methods for 20 Newsgroups with regard to differentsuperclasses.
Table 8: Different regularization rates applied to CIFAR-10.
Table 9: Errors of different image retrieval methods for CIFAR-10.
Table 10: Errors of “mean” and “max” pooling as for CIFAR-10.
