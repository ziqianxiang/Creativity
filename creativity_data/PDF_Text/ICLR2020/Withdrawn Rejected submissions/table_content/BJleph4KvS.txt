Table 1: Summary statistics of the data sets used in our experimentsData Set	MUTAG	PROTEINS	NCI1	NCI109	MUTAGENmax #nodes	28	620	111	111	417min #nodes	10	4	3	4	4avg #nodes	17.93	39.06	29.87	29.68	30.32avg #edges	19.79	72.82	32.30	32.13	30.77#graphs	188	1,113	4,110	4,127	4,337#classes	2	2	2	2	2Baselines and running environment We compare HaarPool with SortPool (Zhang et al., 2018a),DiffPool (Ying et al., 2018), gPool (Gao & Ji, 2019), SAGPool (Lee et al., 2019), EigenPool (Maet al., 2019a), CSM (Kriege & Mutzel, 2012) and GIN (Xu et al., 2019) on the above data sets. Theexperiments use PyTorch Geometric1 (Fey & Lenssen, 2019) and were run in Google Cloud using 4Nvidia Telsa T4 with 2560 CUDA cores, compute 7.5, 16GB GDDR6 VRAM.
Table 2: Performance comparison for graph classification tasks (test accuracy in percent, showingthe standard deviation over 10 repetitions of the experiment).
Table 4: Hyperparameter settingData Set	MUTAG	PROTEINS	NCI1	NCI109	MUTAGENbatch size	60	50	100	100	100max #epochs	30	20	150	150	50early stopping	15	20	50	50	50learning rate	0.01	0.001	0.001	0.01	0.01weight decay	0.0005	0.0005	0.0005	0.0001	0.000520.85 and 32.74 respectively. In the experiments, the network uses GIN convolution (Xu et al., 2019)as graph convolution and with HaarPooling or SAGPooling (Lee et al., 2019). With SAGPooling,the network architecture uses two combined layers of GIN convolution and SAGPooling followed bycombined layers of GIN convolution and global max pooling, denoted by GIN-SP-GIN-SP-GIN-MP,where SP means the SAGPooling and MP means global max pooling. With HaarPooling, we testwith two architectures: GIN-HP-GIN-HP-GIN-MP and GIN-HP-GIN-GIN-MP, where HP meansHaarPooling. The data for training, validation and test are 35000, 5000 and 10000 respectively. Thehidden nodes in convoluational layers is 64, batch size is 60 and learning rate is 0.001.
Table 5: Training, validation and test accuracies on TrianglesArchitecture	Accuracy (%)			Training	Validation	TestGIN-SP-GIN-SP-GIN-MP	45.6	45.3	44.0GIN-HP-GIN-HP-GIN-MP	47.5	46.3	46.1GIN-HP-GIN-GIN-MP	47.3	45.8	45.5G	Property comparison of pooling methodsHere we provide a comparison of the properties of HaarPooling with existing pooling methods. Theproperties in comparison includes time complexity and space complexity, and whether involving theclustering, hierarchical pooling (which is then not a global pooling), spectral-based, node feature orgraph structure and sparse representation. We compare HaarPooling (denoted by HaarPool in the ta-ble) to other methods (SortPool, DiffPool, gPool, SAGPool and EigenPool). The SortPool (i.e. Sort-Pooling) is a global pooling which uses node signature (i.e. Weisfeiler-Lehman color of vertex) sortsall vertices by the values of the channels of the input data. Thus, the time complexity (worst case)of SortPool is O(|V |2) and space complexity is O(|V |). Other pooling methods are all hierarchicalpooling. DiffPool and gPool both use the node feature and have time complexity O(|V |2) The Diff-Pool learns the assignment matrices in end-to-end manner and has space complexity O(k|V |2) forpooling ratio k. The gPool projects all nodes to a learnable vector to generate scores for nodes, andthen sorts the nodes by the projection scores; the space complexity is O(|V | + |E|). SAGPool usesthe graph convolution to calculate the attention scores of nodes and then selects top ranked nodes for
Table 6: Property comparison for pooling methods.
