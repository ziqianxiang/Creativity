Table 1: Details of the pretrained sentence embedders we test in this paper. For methods whichproduce word embeddings, “composition” denotes how a single embedding was obtained from thesentence’s word embeddings. ELMo embeddings are averaged across the three bi-LSTM layers;BERT and GPT embeddings come from the final hidden layer. All of the models besides tf-idf andthe fine-tuned version of BERT are common pretrained versions; further details are in Appendix A.
Table 2: Popular and outlier near neighbors for the given query (top). The first sentence is inthe 5-nearest neighborhood for all embedders; the remaining sentences are highly-ranked by thegiven embedder and outside the 50-nearest neighborhood for all other embedders. See Table 3(Appendix B) for additional examples.
Table 3: Additional outlier near neighbors for the given query (top; same as Table 2). The firstsentence is in the 5-nearest neighborhood for all embedders; the remaining sentences are highly-ranked by the given embedder and outside the 50-nearest neighborhood for all other embedders.
Table 4: Results for the query-paraphrase experiment (§8), sorted by decreasing MRR. # top and# top-5 are the number of queries for which the paraphrase was the nearest neighbor and in the5-nearest neighborhood (max. 75), respectively.
