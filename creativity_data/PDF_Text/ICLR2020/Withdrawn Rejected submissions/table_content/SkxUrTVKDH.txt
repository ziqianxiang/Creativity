Table 1: Top-1/Top-5 accuracy(%) on ImageNet-2012 and test accuracy on MNIST/Cifar-10. ?:results from the official pytorch website. We use the official pytorch codes to run the competitors. Allmodels are trained by 100 epochs. In this table, we run the experiment by ourselves except for SGDMom-Wd on ImageNet which is reported in https://pytorch.org/docs/stable/torchvision/models.html.
Table 2: This table shows results for different κ, the results are all the best test accuracy. Here wetest two widely-used models: VGG16 and ResNet56 on Cifar10. For results in this table, we keepν = 100. Full means that we use the trained model weights directly, Sparse means the model weightsare combined with mask generated by Γ support. Sparse result has no finetuning process, the result iscomparable to its Full counterpart. For this experiment, we propose that κ = 1 is a good choice. Forall the model, we train for 160 epochs with initial learning rate (lr) of 0. 1 and decrease by 0.1 atepoch 80 and 120.
Table 3: Sparsity rate and validation accuracy for different κ at different epochs. Here we pick thetest accuracy for specific epoch. In this experiment, we keep ν = 100. We pick epoch 20, 40, 80 and160 to show the growth of sparsity and sparse model accuracy. Here Sparsity is defined in Sec. 4,and Acc means the test accuracy for sparse model. A sparse model is a model at designated epoch tcombined with the mask as the support of Γt .
Table 4: Results for different ν, the results are all the best test accuracy. Here we test two widely-usedmodel : VGG16 and ResNet56 on Cifar10. For results in this table, we keep κ = 1. Full meansthat we use the trained model weights directly, Sparse means the model weights are combined withmask generated by Γ support. Sparse result has no finetuning process, the result is comparable to itsFull counterpart. For all the model, we train for 160 epochs with initial learning rate (lr) of 0.1 anddecrease by 0.1 at epoch 80 and 120.
Table 5: Sparsity rate and validation accuracy for different ν at different epochs. Here we pick thetest accuracy for specific epoch. In this experiment, we keep κ = 1. We pick epoch 20, 40, 80 and160 to show the growth of sparsity and sparse model accuracy. Here Sparsity is defined in Sec. 4 asthe percentage of nonzero parameters, and Acc means the test accuracy for sparse model. A sparsemodel is a model at designated epoch t combined with mask as the support of Γt .
Table 6: Computational and Memory Costs. ( And GPU memory means the whole memory modelweights and the activation cache. )Layer	FC1	FC2	FC3Sparsity	0049	0.087	0.398Number of Weights	235200	30000	1000Table 7: This table shows the sparsity for every layer of Lenet-3. Here sparsity is defined in Sec. 4,number of weights denotes the total number of parameters in the designated layer. It is interestingthat the Γ tends to put lower sparsity on layer with more parameters.
Table 7: This table shows the sparsity for every layer of Lenet-3. Here sparsity is defined in Sec. 4,number of weights denotes the total number of parameters in the designated layer. It is interestingthat the Γ tends to put lower sparsity on layer with more parameters.
Table 8: This table shows the sparsity for every layer of Conv-2. Here sparsity is defined in Sec. 4,number of weights denotes the total number of parameters in the designated layer. The sparsity ismore significant in fully connected (FC) layers than convolutional layers.
Table 9: This table shows the sparsity for every layer of Conv-4. Here sparsity is defined in Sec.
Table 10: Hyperparameter setting for the experiments in Figure 5.
