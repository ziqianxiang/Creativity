Table 1: Transformations and labels on asample input image created by two of theself-supervised tasks used in our work.
Table 2: Test accuracy (%) on standard domain adaptation benchmarks. Our results are organizedaccording to the self-supervised task(s) used: R for rotation, L for location, and F for flip. Weachieve state-of-the-art accuracy on four out of the seven benchmarks.
Table 3: Test accuracy (%) on GTA5 â†’ Cityscapes. Our method significantly improves over sourceonly, and also over CyCADA when combined. This indicates that additional self-supervision usingour training algorithm further aligns the domains.
Table 4: Qualitative comparison of segmentation results using our method, the baseline, and theground truth label.
