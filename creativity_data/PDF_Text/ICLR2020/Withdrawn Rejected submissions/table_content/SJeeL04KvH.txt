Table 1: MNIST accuracy figures	Data Distribution	FA	FA+WD	FA+RM+AH	FA+RM	FA+AHC=1.0	iid	97.9 ± 0.1 二	97.9 ± 0.00F	97.9 ± 0.06	97.8 ± 0.06	98.0 ± 0.04	non-iid	94.7 ± 0.07	94.8 ± 0.2	94.7 ± 0.08	94.5 ± 0.3	95.5 ± 0.04C=0.5	iid	97.4 ± 0.06	97.3 ± 0.03	98.1 ± 0.06	97.4 ± 0.05	98.2 ± 0.01	non-iid	92.1 ± 0.8	92.0 ± 0.4	92.5 ± 0.1	91.6 ± 0.3	93.2 ± 0.8CIFAR10 : We use a network with two convolutional layers (with 32 and 64 feature maps and 5x5kernels) followed by two fully connected layers (with 1024 and 10 neurons). 2x2 max pooling wasused after each convolutional layer. As shown in table 2, the use of representation matching yieldsa significant improvement in accuracy for the non-iid case while slightly improving performancein the iid case. The benefit of adaptive hyper-parameters over a fixed hyper-parameter scheduleare more equivocal. The evolution of adaptive hyper-parameters is shown in the second column ofFig. 3. Unlike the MNIST case, the REINFORCE algorithm chooses to push down the learning ratefor the iid case as well which actually leads to better performance in the iid case compared to thefixed schedule (FA vs. FA+AH and FA+RM+AH vs. FA+RM in table 2). In the non-iid case, thefixed schedule performs better, though.
Table 2: CIFAR10 accuracy figures	Data Distribution	FA	FA+WD	FA+RM+AH	FA+RM	FA+AHC=1.0	iid	81.9 ± 0.3	81.4 ± 0.2	84.3 ± 0.1	83.5 ± 0.1	83.4 ± 1.2	non-iid	44.2 ± 0.9	44.3 ± 0.8	52.4 ± 2.2	52.9 ± 0.05	44.8 ± 5.8C=0.5	iid	76.8 ± 0.3	76.6 ± 0.2	79.2 ± 1.9	76.0 ± 0.2	82.0 ± 1.04	non-iid	33.4 ± 1.0	34.4 ± 0.9	39.8 ± 3.8	43.3 ± 0.8	19.8 ± 7.0Figure 3: Evolution of the mean of the hyper-parameter distribution P(H∣ψ) for the iid and non-iidcases. Results taken from the FA+RM+AH algorithm when C = 1.0. The evolution of the meansare shown separately for the learning rate (first row) and for the number of SGD steps per round(second row), with one column each for the MNIST, CIFAR10, and keyword spotting (KWS) tasks.
Table 3: Keyword spotting task accuracy figures	Data Distribution	FA	FA+WD	FA+RM+AH	FA+RM	FA+AHC=1.0	iid	93.0 ± 0.2	93.5 ± 0.2	94.4 ± 0.2	92.9 ± 0.3	94.0 ± 0.4	non-iid	28.4 ± 7.2	29.9 ± 7.3	81.1 ± 0.5	79.2 ± 0.6	9.8 ± 0.2C=0.5	iid	91.4 ± 0.3	93.0 ± 0.4	94.3 ± 0.1	91.0 ± 0.2	93.2 ± 0.4	non-iid	11.45 ± 1.7	11.8 ± 2.7	74.9 ± 2.5	60.7 ± 3.3	10.0 ± 0.3Communication and computational overhead : Our representation matching scheme does notintroduce any communication overhead between the clients and the central server. The adaptivehyper-parameters scheme introduces a negligible communication overhead for sending two scalarhyper-parameters to the clients each round. As for computational overhead, we quantify the wall-clock run-time1 for training a client for 30 SGD iterations or mini-batches (mini-batch size of 64).
Table 4: Wall-clock run-time in seconds. Mean and std. from 200 trials.
