Table 1: Top 1 accuracy of Dropout and corresponding RotationOut on CIFAR100(a)ResNet110: Standard Dropout	(b)ResNet110: tan θ 〜N (0,σ2)keep rate	Avg top-1(%)	Best top-1(%)	σ	Avg top-1(%)	Best top-1(%)0	71.93 ± 0.16	72.12 ± 0.18	0	71.93 ± 0.16	72.12 ± 0.180.9	73.13 ± 0.30	73.32 ± 0.28	0.333	74.23 ± 0.25	74.41 ± 0.160.8	73.33 ± 0.27	73.59 ± 0.28	0.500	74.14 ± 0.11	74.35 ± 0.130.7	72.42 ± 0.23	72.71 ± 0.14	0.655	73.13 ± 0.14	73.45 ± 0.110.6	71.79 ± 0.20	72.10 ± 0.21	0.816	71.83 ± 0.35	72.17 ± 0.33(c) WideResNet28: Standard Dropout			(d)WideResNet28: tanθ 〜N(0,σ2)		keep rate	Avg top-1(%)	Best top-1(%)	σ	Avg top-1(%)	Best top-1(%)0	78.05 ± 0.23	78.20 ± 0.21	0	78.05 ± 0.23	78.20 ± 0.210.9	78.61 ± 0.09	78.78 ± 0.10	0.333	78.94 ± 0.22	79.09 ± 0.220.8	78.77 ± 0.20	78.91 ± 0.19	0.500	79.47 ± 0.14	79.60 ± 0.120.7	78.75 ± 0.15	78.87 ± 0.13	0.655	79.69 ± 0.11	78.80 ± 0.150.6	78.55 ± 0.07	78.75 ± 0.18	0.816	79.76 ± 0.32	79.93 ± 0.33Table 1 shows the results on CIFAR100 dataset with two architectures. Table 1a and 1b are theresults for ResNet110. Table 1c and 1d are the results for WideResNet28-10. Results in the samerow compare the regularization abilities of Dropout and the equivalent keep rate RotationOut. Wecan find dropping too many neurons is less effective and may hurt training. Since WideResNet28-10has much more parameters, the best performance is from a heavier regularization.
Table 2: Comparison with state of the art: Top 1 accuacy of ResNet50 on ImageNet ValidationModel	top-1(%)	top-5(%)ResNet-50 (He etal., 2016)		76.51 ± 0.07	93.20 ± 0.05ResNet-50 + dropout(kp=0.7)(Srivastava et al., 2014)	76.80 ± 0.04	93.41 ± 0.04ResNet-50 + DroPPath(kp=0.9)(Larsson et al., 2016)	77.10 ± 0.08	93.50 ± 0.05ResNet-50 + SPatialDroPout(kp=0.9)(Tompson et al., 2015)	77.41 ± 0.04	93.74 ± 0.02ResNet-50 + Cutout (DeVries & Taylor, 2017)	76.52 ± 0.07	93.21 ± 0.04ResNet-50 + DroPBloCk(kp=0.9)(Ghiasi et al., 2018)	78.13 ± 0.05~	94.02 ± 0.02ResNet-50 + RotationOut	77.87 ± 0.34	93.94 ± 0.17ResNet-50 + RotationOut (Block)	78.58 ± 0.48	94.27 ± 0.24COCO Object Detection. Our proposed method can also be used in other vision tasks, for ex-ample Object Detection on MS COCO (Lin et al., 2014). In this task, we use RetinaNet (Lin et al.,2017) as the detection method and apply RotationOut to the ResNet backbone. We use the same hy-perparameters as in ImageNet classification. We follow the implementation details in (Ghiasi et al.,2018): resize images between scales [512, 768] and then crop the image to max dimension 640. Themodel are initialized with ImageNet pretraining and trained for 35 epochs with learning decay at 20and 28 epochs. We set α = 0.25 and γ = 1.5 for focal loss, a weight decay of 0.0001, a momentumof 0.9 and a batch size of 64. The model is trained on COCO train2017 and evaluated on COCOval2017. We compare our result with DropBlock (Ghiasi et al., 2018) as table 3 shows.
Table 3: Object detection in COCO using RetinaNet and ResNet-50 FPN backboneModel	Initialization	AP	AP50	AP75RetinaNet	ImageNet	36.5	55.0	39.1RetinaNet, no DropBlock	Random	36.8	54.6	39.4RetinaNet, Dropout, keep_prob = 0.9	Random	37.9	56.1	40.6RetinaNet, keep_Prob = 0.9, block_size = 5	Random	38.4	56.4	41.2RetinaNet, RotationOut	ImageNet	38.2	56.2	41.0RetinaNet, RotationOut (Block)	ImageNet	38.7	56.6	41.4Due to limited computing resources, we finetune the model from PyTorch library’s pretraining Im-ageNet classification models while DropBlock method trained the model from scratch. We thinkit is fair to compare DropBlock method since the initialization does not help increase the resultsas showed in the first two rows. Our RotationOut can still have additional 0.3 AP based on theDropBlock result.
Table 4: Auto2Text experiment on the WSJMothod	DistanceNo regularization	9.1Standard Dropout(kp=0.9)	8.6Weight Drop(kp=0.8)	7.8Variational Weight Drop(kp=0.8)	7.5Locked Drop(kp=0.7)	7.3Locked Drop(kp=0.8)+Variational Weight Drop(kp=0.8)	6.7RotationOut	68RotationOut +Variational Weight Drop(kp=0.9)	6.4A.6 Rethinking Small Batchsize BatchNormalizationBN also introduces noise to the neurons by using the batch mean and variance. The noise to differentneurons/channels are independent, so the effect of BN’s noise is similar to Dropout. It is widelybelieved that the noise causes BN performance to decrease with small batch size (Wu & He, 2018;Luo et al., 2018). However, Dropout usually decrease the performance when the keep rate is verylow. We study the effect of BN’s noise and argue that BN is not a linear operation. The nonlinearityincreases when the batch size decreases, which is also one reason for the small batch size BN’sperformance drop.
