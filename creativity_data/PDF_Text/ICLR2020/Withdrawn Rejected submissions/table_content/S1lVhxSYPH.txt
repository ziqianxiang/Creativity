Table 1: Test accuracy along with the number of multiplications, additions, operations and modelsize for MobileNets-V1 and strassenified MobileNets-V1 (ST-MobileNets) with the width multiplier0.5 on ImageNet dataset. r is the hidden layer width of a strassenified convolution layer, cout is thenumber of output channels of the corresponding convolution layer. A multiply-accumulate operationis abbreviated as MAC.
Table 2: Top-1 accuracy along with the computational costs, model size, and energy per inferencefor baseline MobileNets-V1, ST-MobileNets, and Hybrid MobileNets on ImageNet dataset. α is thefraction of channels generated by the full-precision weight filters at each layer , cout is the number ofremaining channels generated by the ternary strassen filters at the corresponding convolutional layer,r is the hidden layer width of the strassenified convolutions. The last column shows the throughputof proposed models on an area-equivalent hardware accelerator comprising both MAC and adderunits when compared to the throughput of baseline MobileNets with 16-bit floating-point weightson a MAC-only accelerator.
Table 3: Hyperparameters for training Hybrid MobileNetsTraining phase	HyperparametersTrain using full-precision strassen matrices	Batch size per GPU: 128 Number of GPUS used: 4 Optimizer: Nesterov accelerated gradient (NAG) (Momentum: 0.9, Weight decay: 0.0001) Number of epochs: 200 Weight initialization: Xavier Initial, final learning rate: 0.2, 0.0 Learning rate schedule: cosine decay Number of warmup epochs: 5 Starting warmup learning rate: 0.0 Size of the input image: 224 X 224 X 3Activate quantization for strassen matrices	Batch size per GPU: 128 Number of GPUs used: 4 Optimizer: Nesterov accelerated gradient (NAG) (Momentum: 0.9, Weight decay: 0.0001) Number of epochs: 75 Initial, final learning rate: 0.02, 0.0 Learning rate schedule: cosine decayFreeze strassen matrices to ternary values	Batch size per GPU: 128 Number of GPUs used: 4 Optimizer: Nesterov accelerated gradient (NAG) (Momentum: 0.9, Weight decay: 0.0001) Number of epochs: 25 Initial, final learning rate: 0.002, 0.0 Learning rate schedule: cosine decayfor the convolution operation becomes difficult. The Inception module addresses this by allowingthe GoogLeNet architecture to use convolutional filters of different sizes - a small sized (1 X 1) filterconvolution, a medium sized (3 × 3) filter convolution, and a large sized (5 × 5) filter convolutionat each layer and let the network decide for itself the appropriate convolutional filter to capture thenecessary features. The 1 × 1 or 3 × 3 convolutions cover a small receptive field of the input andcan capture fine grain details and features in the input volume, whereas the 5 × 5 filters are able tocover a large receptive field, and thus can capture spread out features of higher abstraction.
Table 4: Top-1 and top-5 accuracy (%) of Mobilenet (full resolution and multiplier of 0.5) on Ima-genet for different number of bits per weight and activation.
