Table 1: Comparison with hand-designed networks and state-of-the-art NAS methods onCIFAR-10 (Lower test error is better). The results are grouped as follows: manually designednetworks, published NAS algorithms, and our experimental results. The average test error of ourexperiment used five random seeds. Table entries with ”-” indicates that either the field is notapplicable or unknown. The methods listed in this table are trained with auxiliary towers and cutoutaugmentation. Running time cost is measured on NVIDIA TITAN X GPU. The reported time ofCoNAS includes both training one-shot model and gathering measurements for the sparse recovery.
Table 2: Image Classification Test Error of CoNAS on Multiple Datasets. We compare theperformance of CoNAS on different datasets with existing NAS results. The experiment details forCoNAS is described in Appendix A.5.
Table 3: Comparison of state-of-the-art NAS methods and hand-designed networks on PTB(Lower perplexity is better). The results are grouped in following orders: manually designednetworks, published NAS algorithms, and our experimental results. The average test error of ourexperiment used five random seeds. Table entries with ”-” indicates either the field is not applicableor unknown.
Table 4: Lasso Parameter Stability Experiment.
Table 5: Randomly Wired Model Performance on CIFAR-10. (Lower test error is better) Trainedwith auxiliary towers and cutout augmentation for 600 epochs (equivalent training setup to CIFAR-10from DARTs.
