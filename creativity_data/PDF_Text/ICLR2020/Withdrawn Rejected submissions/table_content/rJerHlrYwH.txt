Table 1:  Linear classifier hψ trained with 100% of labels.   Comparison to linear separabilityof other self-supervised methods.  In all cases a feature extractor is optimized in an unsupervisedmanner, and a linear classifier is trained on top using all labels in the ImageNet dataset.
Table 2:  ResNet classifier hψ trained with 1% or 10% of labels.  Comparison to other methodsfor semi-supervised learning using 1% or 10% of labeled data.  Representation learning methodsuse a classifier to discriminate an unsupervised representation, and optimize it solely for the super-vised objective on labeled data. Label-propagation methods on the other hand further constrain theclassifier with smoothness and entropy criteria on unlabeled data, making the additional assumptionthat  all training images fit into a single (unknown) testing category.    denotes methods implementedin this work, fixed and fine-tuned denote whether the feature extractor is allowed to accommodatethe supervised objective.
Table 3: Faster-RCNN hψ trained with 100% of PASCAL labels. Comparison of PASCAL 2007image detection accuracy to other transfer methods. The supervised baseline learns from the entirelabeled ImageNet dataset and fine-tunes for PASCAL detection. The second class of methods learnsfrom the same unlabeled images before transferring. All of these methods pre-train on the ImageNetdataset, except for DeeperCluster which learns from the larger, but uncurated, YFCC100M dataset(Thomee et al., 2015). All results are reported in terms of mean average precision (mAP).    denotesmethods implemented in this work.
Table 4: Data efficient classification results with Top-1 accuracy.
Table 5: Data efficient classification results with Top-5 accuracy (data in Fig. 1).
