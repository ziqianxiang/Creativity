Table 1:	Forms of hierarchical attention. End2End=No training labels, ×words=for every word inthe target, ×summaries=for every summary in the target, γ=sentence(post)-level, β=high-levelModelNallapatietal. (2016), Cheng & Lapata(2016), Tanetal. (2017)Hsu et al. (2018)OursEnd2End	Decoding γ computations		βyes	seq	×words	nono	seq	1	noyes	hier	×summaries	yesTraining ObjectiveWe train our hierarchical encoder-decoder network similarly to an attentive seq2seq model Bah-danau et al. (2014), but with an additional weighted sum of sigmoid cross-entropy loss on stoppingdistribution; see Eq. 1. Given a summary, Yi = hwi0, . . . , wiqi, our word-to-word decoder generatesa target Y = hyio,..., y%q), with words from a same vocabulary U. We train our model end-to-endby minimizing the objective given in Eq. 8.
Table 2:	The left rows contain interleaving of 3 articles with 2 to 5 sentences and the right rowscontain their interleaved titles. Associated sentences and titles are depicted by similar symbols.
Table 3: Rouge F1-Scores for seq2seq modelson the Pubmed Easy Corpus.
Table 4: Recalls on a sampled Pubmed Hard-Corpus. Int=Interleaved, Dis=Disentangled.
Table 5: Rouge Recall-Scores of models on the Medium and Hard Corpus.
Table 6: Rouge Recall-Scores of models on thePubmed Hard Corpus.
Table 7: Rouge Recall-Scores of models on theStack Exchange Medium and Hard Corpus.
Table 8: Rouge Recall-Scores of ablated models on the Hard Pubmed Corpus.
Table 9: Interleaved sentences of 3 articles, and corresponding ground-truth and hier2hier generatedsummaries. The top 2 sentences that were attended (γ) for the generation are on the left. Addition-ally, top words (β) attended for the generation are colored accordingly.
Table 10: The left rows contain interleaving of 3 articles with 2 to 5 sentences and the right rowscontain their interleaved titles. Associated sentences and titles are depicted by similar symbols.
Table 11:	Interleaved sentences of 3 articles, and corresponding ground-truth and hier2hier gen-erated summaries. The top 2 sentences that were attended (γ) for the generation are on the left.
Table 12:	The left rows contain interleaving of 4 articles with 2 to 5 sentences and the right rowscontain their interleaved titles. Associated sentences and titles are depicted by similar symbols.
Table 13: Interleaved sentences of 4 articles, and corresponding ground-truth and hier2hier gen-erated summaries. The top 2 sentences that were attended (γ) for the generation are on the left.
