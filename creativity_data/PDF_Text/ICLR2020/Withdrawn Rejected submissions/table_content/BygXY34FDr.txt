Table 1: Zero-shot learning Results: Success rate of the agent reaching all goals within 500 steps withoutretraining. The agent navigated to each goal location starting from 10 random locations within the simulator. Adetailed description of each model can be found in Section 7.
Table 2: Explored Hyperparameters12Under review as a conference paper at ICLR 2020gPolicy LossReturn LossPolicy LossReturn LossFigure 5: Architecture used for the SFDP-A3C modelThe input to the model is the current state of the agent st and the goal location g as images. These go through ashared siamese layer to generate embeddings for the goal and the state. The policy is conditioned on the USFvector (dotted line indicates gradients do not flow from policy to the USFA head). The USFA ψ is trained withthe temporal difference error using φ to give the expected future state occupancies. The discounted episodereturn is used to train both ω and USFA vector.
