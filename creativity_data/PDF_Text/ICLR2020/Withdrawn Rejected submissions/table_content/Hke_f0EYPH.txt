Table 1: ui and li are layer-wise bounds found using interval bounds propagation, and uic and lic arelayer-wise bounds using convex outer bounds. Θ denotes element-wise multiplication. || ∙ ∣∣op,∞denote the '∞ induced operator norm. g is a classification-calibrated loss function. dB and d° arequantities relating to lower bounds on the minimum adversarial distortion. Expectations are takenover training data. The training cost is compared to the standard training.
Table 2: CNN-Cert certified accuracies and training times. Std in (∙). * denotes a different settingof regularization parameter λ, decreased from 2 to 1. MMR is trained for 4 epochs due to its longtraining time. Our methods or variants are in blue.
Table 3: Table of Notation13Under review as a conference paper at ICLR 2020C Double Margin Visual IllustrationMargin s:Tracks linear sensitivityMargin v:Tracks non-linear sensitivityLesslinearsensitivityLessnonlinearsensitivityFigure 2: A visual illustration of Double Margin’s sensitivity quantification. s is propagated as s =2 |W| (σ(Z + S) — σ(Z — s)). V is propagated as V = 2 |W| (σ(Z + V) — σ(Z — v)) + 1 ∣W∣∣σ(z +S + v) + σ (Z — s — v) — 2σ (z)|.
Table 4: CNN-Cert certified accuracies for Effectiveness on other normsMethod	Training Time (sec)		cert = 0	0.01	0.03	0.05	0.07	0.10	0.20	0.30	0.40Small CNN MNIST, 4 layer											Double Margin	=0.2	521.1	95.5%	95.5%	95.2%	94.2%	93.5%	91.7%	81.3%	0.0%	0.0%Double Margin Ensemble		1411.2	95.5%	95.5%	95.5%	95.5%	94.5%	93.5%	85.5%	0.0%	0.0%Method		Training Time (sec)	cert = 0	0.5/255	1/255	2/255	3/255	5/255	7/255	8/255	9/255Small CNN CIFAR, 4 layer											Double Margin	= 8/255	2517.3	42.8%	41.8%	39.8%	36.8%	36.3%	26.8%	18.8%	15.8%	13.8%Double Margin Ensemble		6566.2	46.0%	44.0%	43.5%	40.0%	38.0%	27.0%	22.0%	16.0%	11.0%Table 5: CNN-Cert accuracies for Model ensemble. The ensemble uses three models with differentrandom initialization and random training batches. For fair comparison with standard Double Margin,three models with the same random initializations and training batches are trained independently.
Table 5: CNN-Cert accuracies for Model ensemble. The ensemble uses three models with differentrandom initialization and random training batches. For fair comparison with standard Double Margin,three models with the same random initializations and training batches are trained independently.
Table 6: CNN-Cert certified accuracies on GTSRB networks.
Table 7: CNN-Cert certified accuracies for various `p versions of Double Margin computed fordifferent `p perturbation sizes.
Table 8: CNN-Cert certified accuracies on all MNIST networks. Adv. Training (Madry et al., 2018),Bounded (Kolter & Wong, 2018), IBP (Gowal et al., 2018), ReLU Stability (Xiao et al., 2019). Forthe medium CNN architecture, accuracies are found over 20 points.
Table 9: CNN-Cert certified accuracies on all CIFAR networks. Adv. Training (Madry et al., 2018),Bounded (Kolter & Wong, 2018), IBP (Gowal et al., 2018), ReLU Stability (Xiao et al., 2019). Forthe mediun CNN architecture, accuracies are found over 20 points.
Table 10: CNN-Cert certified accuracies computed on the entire test set.
Table 11: Certified accuracies for large and medium CNN models computed using IBP (Gowal et al.,2018).
Table 12: Equivalent certification methods to different variants of CNN-Cert and IBPTables 13, 14, 15, 16 show certified accuracies using the different certification methods. We findthat CNN-Cert-ReLU and CNN-Cert-Ada generally yield the highest accuracies for most networks.
Table 13: CNN-Cert-Ada certified accuracies for Comparing different certification methods.
Table 14: CNN-Cert-ReLU certified accuracies for Comparing different certification methods.
Table 15: CNN-Cert-Zero certified accuracies for Comparing different certification methods.
Table 16: IBP certified accuracies for Comparing different certification methods.
