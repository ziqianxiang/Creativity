Table 1: BLEU scores for IWSLT14 German-English translation.什)tuned With checkpoint av-eraging and length penalty. ⑴ from Edunov et al.
Table 2: BLEU scores for IWSLT15 English-Vietnamese translation. (f) from Luong et al.
Table 3: BLEU scores for WMT14 English-Germantranslation. ⑴ tuned with checkpoint averaging. ⑴trained on WMT16, a slightly different version of train-ing data. () from Vaswani et al. (2017). (?) from Ottet al. (2018). (•) from Wu et al. (2019).
Table 4: ROUGE F1 scores for Gigaword abstrac- Table 5: ROUGE F1 scores on the official test set.
Table 7: Qualitative examples from IWSLT German-English translation. Numbers inside the parenthesis aresentence-level BLEU scores. Red word is where the baseline Transformer makes a mistake without consideringthe possible future phrase and fails to recover. On the other hand, our model makes the right decision at theblue word, hence generates more coherent sentence. Please refer to Section 4.6 for detailed explanation.
Table 8: Qualitative examples from IWSLT German-English translation. Numbers inside the paren-thesis are sentence-level BLEU scores. Red word is where the baseline Transformer makes a mistakewithout considering the possible future phrase and fails to recover. On the other hand, our modelmakes the right decision at the blue word, hence generates more coherent sentence. Please refer toSection 4.5 in the main paper for detailed explanation.
Table 9: Qualitative examples from the Gigaword summarization dataset. Baseline model suffersfrom early mistakes. Our model generates more coherent summaries.
