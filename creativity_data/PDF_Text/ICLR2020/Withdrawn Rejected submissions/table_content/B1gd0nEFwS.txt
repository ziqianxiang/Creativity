Table 1: Average per-class accuracy (Tavg) for universal-DA tasks on Office-Home dataset (with ∣C∣∕∣Cs ∪ Ct| = 0.15). Scores for the prior works are directly taken from UAN (You et al., 2019).													Method	Office-Home													Ar→Cl Ar→Pr Ar→Rw Cl→Ar Cl→Pr Cl→Rw Pr→Ar Pr→Cl Pr→Rw Rw→Ar Rw→Cl Rw→Pr												AvgResNet (He et al., 2016)	59.37	76.58	87.48	69.86	71.11	81.66	73.72	56.30	86.07	78.68	59.22	78.59	73.22IWAN (Zhang et al., 2018b)	52.55	81.40	86.51	70.58	70.99	85.29	74.88	57.33	85.07	77.48	59.65	78.91	73.39PADA (Zhang et al., 2018b)	39.58	69.37	76.26	62.57	67.39	77.47	48.39	35.79	79.60	75.94	44.50	78.10	62.91ATI (Busto et al., 2017)	52.90	80.37	85.91	71.08	72.41	84.39	74.28	57.84	85.61	76.06	60.17	78.42	73.29OSBP (Saito et al., 2018b)	47.75	60.90	76.78	59.23	61.58	74.33	61.67	44.50	79.31	70.59	54.95	75.18	63.90UAN (You et al., 2019)	63.00	82.83	87.85	76.88	78.70	85.36	78.22	58.59	86.80	83.37	63.17	79.43	77.02Source-free adaptation													Ours USFDA-a	63.35	83.30	89.35	70.96	72.34	86.09	78.53	60.15	87.35	81.56	63.17	88.23	77.03Ours USFDA-b	62.46	82.71	88.26	71.10	70.88	85.75	78.21	59.18	86.05	82.17	63.22	87.68	76.47unique negative classes in Cn . Note that, we impose no restriction on how the hypothetical classesare created (e.g. one can composite non-animal with animal). A random mask is defined whichsplits the images into two complementary regions using a quadratic spline passing through a centralimage region (see Appendix Algo. 2). Then, the negative image is created by merging alternate maskregions as shown in Fig. 3A. For the I→C task of ImageNet-Caltech, the source domain (ImageNet),consisting of 1000 classes, results in a large number of possible negative classes (i.e. |Cn| = |Cs|C2).
Table 2: Tavg on Office-31 (with |C|/|Cs ∪ Ct| = 0.32), VisDA (with |C|/|Cs ∪ Ct| = 0.50), andImageNet-Caltech (with |C|/|Cs ∪ Ct| = 0.07). Here, SF denotes support for source-free adaptation.
Table 3: Evaluation of the proposed method on A→D task of Office-31 (Saenko et al., 2010)dataset, pretraining the ResNet-50 backbone (M) on Places instead of Imagenet. Note that, we set|C |/|Cs ∪ Ct| = 0.32, similiar to the setting used in Table 2 of the main paper. Additionally, the lasttwo columns of the table show a comparison between our method and UAN (You et al., 2019) withregard to the number of trainable parameters and total training time for adaptationMethod	ResNet-50 finetuning	Avg. Per-ClaSS accuracy, Tavg	Number of Trainable ParameterS	Training time for AdaPtationUAN*	✓	60.98	26.7 Million	280SUAN*	X	52.48	5.6 Million	125SUSFDA-a	X	62.74	—	3.5 Million	44sAFigure 7: Comparison of target-unknown accuracy Tunk across varied label-set relationships for thetask A→D in Office-31 dataset. A) Visual representation of label-set relationships and Tunk at thecorresponding instances for B) UAN* (You et al., 2019) and C) ours source-free model. Effectively,the direction along x-axis (blue horizontal arrow) characterizes increasing Open-set complexity. Thedirection along y-axis (red vertical arrow) shows increasing complexity of Partial DA scenario. Andthe pink diagonal arrow denotes the effect of decreasing shared label space.
Table 4: Accuracy (%) on unsupervised closed-set DA (all use ResNet50). Ours is w/o hyperparmetertuning. Refer Section C.5.
Table 5: NetwOrk arChiteCture fOr Procurement stage. Hyperparameter α						0.2Component	Trainable?	Operation	Notation	Features	Batch Norm?	Non-LinearityResnet-50	X		M	2048		(UptO AvgPOOl layer)						Feature Extractor	✓		Fs	256				Input		2048	X			Fully COnneCted		1024	X	ELU		Fully COnneCted		1024	✓	ELU		Fully COnneCted		256	X	ELU		Fully COnneCted		256	✓	ELUFeature Decoder	✓		5	2048				Input		256	X			Fully COnneCted		1024	X	ELU		Fully COnneCted		1024	✓	ELU		Fully COnneCted		2048	X	ELU		Fully COnneCted		2048	X	-Classifier	✓		D	|Cs | + |Cn |				Input		256	X			Fully COnneCted	Dsrc	|Cs|	X			Input		256	X	
Table 6: Feature Extractor Architecture used for training UAN (You et al., 2019) under the "noResNet-50 finetuning" case (Refer Table 3 and Section C.1)Operation	Features	Non-LinearityInput	-2048-	Fully COnneCted	512	ReLUFully COnneCted	256	ReLUFully COnneCted	512	ReLUFully COnneCted	2048	ReLUTable 7: Network architecture for Deployment stage. Hyperparameter β = 0.1Component	Trainable?	Operation	Notation	Features	Batch Norm?	Non-LinearityResnet-50	X		M	2048		(UptO AvgPOOl layer)						Feature Extractor	✓		Ft	256				Input		2048	X			Fully COnneCted		1024	X	ELU		Fully COnneCted		1024	✓	ELU		Fully COnneCted		256	X	ELU		Fully COnneCted		256	✓	ELUClassifier	X		D	|Cs | + |Cn |				Input		256	X	
Table 7: Network architecture for Deployment stage. Hyperparameter β = 0.1Component	Trainable?	Operation	Notation	Features	Batch Norm?	Non-LinearityResnet-50	X		M	2048		(UptO AvgPOOl layer)						Feature Extractor	✓		Ft	256				Input		2048	X			Fully COnneCted		1024	X	ELU		Fully COnneCted		1024	✓	ELU		Fully COnneCted		256	X	ELU		Fully COnneCted		256	✓	ELUClassifier	X		D	|Cs | + |Cn |				Input		256	X			Fully COnneCted	Dsrc	|Cs|	X			Input		256	X			Fully COnneCted	Dneg	|Cn |	X	Table 8: Specifications of the machine used for both Procurement and Deployment stagesCPU	GPU	RAM VRAM CUDAInteIi7-7700K NVIDIAGeFOrCeGTX 1080Ti 32 GB^^11 GB V8.0.6119Under review as a conference paper at ICLR 2020
Table 8: Specifications of the machine used for both Procurement and Deployment stagesCPU	GPU	RAM VRAM CUDAInteIi7-7700K NVIDIAGeFOrCeGTX 1080Ti 32 GB^^11 GB V8.0.6119Under review as a conference paper at ICLR 2020D.2 References to codeProposed Method. Our complete documented code (including data loaders, training pipeline etc.)used for running the experiments is available for reproducibility (refer to the private commentcontaining the link). Details of dataset splits can be found in Section B.1. For evaluating UAN (Youet al., 2019), we execute the official implementation provided by the authors on github1.
