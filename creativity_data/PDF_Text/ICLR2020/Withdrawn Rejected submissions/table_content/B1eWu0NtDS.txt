Table 1: Rankings of filters for the Shapley value (SH) and the importance switches (IS) methodson a four-layer network, 10-20-100-25. For each layer the top five neurons are shown, the numbersin bold indicate the common top neurons across both the methods.
Table 2: The structured pruning of LeNet-5 and VGG-16The final experiment demonstrates how our method compares to magnitude pruning commonlydone in compression literature. In figure 2, our method (blue trace) outperforms magnitude pruningmethods (L1 and L2 norm over weights). No retraining is used in this case to show how the proposedmethod retains the relevant neurons that affect the predictive accuracy.
