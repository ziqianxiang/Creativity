Table 1: Classification Accuracy with different augment strategyDatasets	w/o augment	image flip	image crop	with augmentReduced Cifar-10	74.7	^753	75.6	81.3Reduced Imagenet	78.7	78.9	78.8	88.1In our experiments, we augment the training set with 1000 noised images every 100 training epoches,and repeat the procedure 5 times. By comparison, the unaugmented GAN is run over the same initialtraining data, with the number of epochs the same as the product of 100 and augmentation times.
Table 2: IS/FID scores of on Places with different Data Augmentation strategies				Subsets	Noise	Image Flip	Image Crop	Our StrategyOcean	4.31/102.7	4.33/107.8	3.99/114.2	5.75/79.3Orchard	4.39/105.3	4.28/106.4	3.97/113.2	5.83/82.4Pier	4.34/107.1	4.30/109.8	4.00/109.3	5.79/90.2Table 2 lists the results of data augmentation based on different subsets of Places dataset. We haveconducted the set of experiments with varying numbers of parallel folders and have discovered thatthe value K=5 has helped achieve the best image quality. Notice that the images generated with ourmethod are produced with the state-of-the-art WGAN-GP as each parallel generative model. Here inaddition to considerations in K-fold Cross ValidationBishop (2006), our choice of K-values are mo-tivated by distributed MD-GAN as described in Hardy et al. (2018), where K-values ranging from3 to 10 are shown to achieve good performances. Table 2 list IS and FID of results experimented ondifferent sub datasets with conventional data augmentation. Clearly, adaptive sample augmentationproduces better images as measured quantitatively and some other simple augmentation strategies.
