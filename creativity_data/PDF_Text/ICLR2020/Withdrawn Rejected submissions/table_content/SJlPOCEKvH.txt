Table 1: Pre-training development losses and GLUE task development accuracies for various levelsof pruning. Each development accuracy is accompanied on its right by the achieved training loss,evaluated on the entire training set. Averages are summarized in Figure 1. Pre-training losses areomitted for models pruned after downstream fine-tuning because it is not clear how to measure theirperformance on the pre-training task in a fair way.
Table 2: We compute the magnitude sorting order of each weight before and after downstream fine-tuning. If a weight’s original position is 59 / 100 before fine-tuning and 63 / 100 after fine-tuning,then that weight moved 4% in the sorting order. We then list the average movement of weights ineach model, along with the standard deviation. Sorting order changes mostly locally across tasks: aweight moves, on average, 0-4% away from its starting position. As expected, larger datasets andlarger learning rates have more movement (per epoch). We also see that higher magnitude weightsare more stable than lower weights, see Figure 7.
Table 3: The values of BERT’s weights are normally distributed in each weight matrix. The meansand variances are listed for each.
