Table 1: Model accuracy of MI base task teststraining runs	OrtA	RRA	RRA	StdA	OrtAA	rrAa	RRAA	StdAA1	0.976	0.682	0.767	0.973	0.973	0.855	0.997	0.9562	0.944	0.75	0.779	0.986	0.954	0.909	0.99	0.9733	0.975	0.731	0.855	0.964	0.979	0.875	0.99	0.9864	0.993	0.923	0.855	0.981	0.994	0.984	0.995	0.9655	0.965	0.405	0.852	0.94	0.967	0.965	0.999	0.986Avg.	0.971	0.707	0.822	0.967	0.973	0.938	0.994	0.973Std. Dev.	0.018	0.187	0.045	0.018	0.015	0.056	0.004	0.013Table 2: Model accuracy of MI transfer task teststraining runs	OrtAB	rrAb	RRAB	StdAB	StdB1	0.978	0.914	0.996	0.136	0.9662	0.948	0.956	-1-	0.136	0.9583	0.976	0.961	0.993	0.136	0.9844	0.994	0.97	0.996	0.136	0.9745	0.968	0.959	0.998	0.136	0.989Avg.	0.973	0.956	0.997	0.136	0.974Std. Dev.	0.017	0.022	0.003	0	0.013Table 3: Model accuracy of MNIST base task teststraining runs	OrtA	StdA	RRA	OrtAA	StdAA	rrAa
Table 2: Model accuracy of MI transfer task teststraining runs	OrtAB	rrAb	RRAB	StdAB	StdB1	0.978	0.914	0.996	0.136	0.9662	0.948	0.956	-1-	0.136	0.9583	0.976	0.961	0.993	0.136	0.9844	0.994	0.97	0.996	0.136	0.9745	0.968	0.959	0.998	0.136	0.989Avg.	0.973	0.956	0.997	0.136	0.974Std. Dev.	0.017	0.022	0.003	0	0.013Table 3: Model accuracy of MNIST base task teststraining runs	OrtA	StdA	RRA	OrtAA	StdAA	rrAa1	0.9792	0.9827	0.981	0.9819	0.9854	0.98562	0.8322	0.9815	0.9824	0.982	0.9837	0.98513	0.9511	0.9815	0.9828	0.9792	0.983	0.98624	0.8841	0.9817	0.9816	0.9814	0.9842	0.98675	0.8266	0.9829	0.9816	0.9816	0.9846	0.9861-Avg.-	0.895	0.982	0.982	0.981	0.984	0.986Std. Dev.	0.0689	0.0007	0.0007	0.0011	0.0009	0.0006Table 4: Model accuracy of MNIST transfer task teststraining times	OrtAB	StdAB	RRAB	StdB
Table 3: Model accuracy of MNIST base task teststraining runs	OrtA	StdA	RRA	OrtAA	StdAA	rrAa1	0.9792	0.9827	0.981	0.9819	0.9854	0.98562	0.8322	0.9815	0.9824	0.982	0.9837	0.98513	0.9511	0.9815	0.9828	0.9792	0.983	0.98624	0.8841	0.9817	0.9816	0.9814	0.9842	0.98675	0.8266	0.9829	0.9816	0.9816	0.9846	0.9861-Avg.-	0.895	0.982	0.982	0.981	0.984	0.986Std. Dev.	0.0689	0.0007	0.0007	0.0011	0.0009	0.0006Table 4: Model accuracy of MNIST transfer task teststraining times	OrtAB	StdAB	RRAB	StdB1	0.949	0.956	0.9612	0.87142	0.8629	0.9458	0.954	0.96573	0.9458	0.9543	0.9648	0.96594	0.9495	0.9559	0.9604	0.87895	0.8564	0.9466	0.9649	0.9638Avg.	0.913	0.952	0.961	0.929Std. Dev.	0.0485	0.0051	0.0044	0.0494Table 5: Model accuracy of Cifar teststraining runs	StdA	RRA3	StdAA	RRAA	StdAB	RRAB	StdB
Table 4: Model accuracy of MNIST transfer task teststraining times	OrtAB	StdAB	RRAB	StdB1	0.949	0.956	0.9612	0.87142	0.8629	0.9458	0.954	0.96573	0.9458	0.9543	0.9648	0.96594	0.9495	0.9559	0.9604	0.87895	0.8564	0.9466	0.9649	0.9638Avg.	0.913	0.952	0.961	0.929Std. Dev.	0.0485	0.0051	0.0044	0.0494Table 5: Model accuracy of Cifar teststraining runs	StdA	RRA3	StdAA	RRAA	StdAB	RRAB	StdB1	0.8263	0.5784	0.8351	0.869	0.2063	0.2602	0.25812	0.7868	0.5553	0.7936	0.8538	0.1838	0.2687	0.26593	0.7729	0.7686	0.7718	0.8449	0.1795	0.2803	0.23864	0.865	0.7382	0.8472	0.8692	0.1738	0.2969	0.24185	0.781	0.6775	0.7787	0.8418	0.1704	0.2733	0.2621Avg.	0.806	0.664	0.805	0.856	0.183	0.276	0.253Std. Dev.	0.0387	0.0945	0.0340	0.0130	0.0141	0.0138	0.0123Table 6: Model L2 loss of smoke tests. Results for B2 : ×107.
Table 5: Model accuracy of Cifar teststraining runs	StdA	RRA3	StdAA	RRAA	StdAB	RRAB	StdB1	0.8263	0.5784	0.8351	0.869	0.2063	0.2602	0.25812	0.7868	0.5553	0.7936	0.8538	0.1838	0.2687	0.26593	0.7729	0.7686	0.7718	0.8449	0.1795	0.2803	0.23864	0.865	0.7382	0.8472	0.8692	0.1738	0.2969	0.24185	0.781	0.6775	0.7787	0.8418	0.1704	0.2733	0.2621Avg.	0.806	0.664	0.805	0.856	0.183	0.276	0.253Std. Dev.	0.0387	0.0945	0.0340	0.0130	0.0141	0.0138	0.0123Table 6: Model L2 loss of smoke tests. Results for B2 : ×107.
Table 6: Model L2 loss of smoke tests. Results for B2 : ×107.
Table 7: Forward and reverse pass of the neural network in MI testsForward pass (294 weights):I(12) → tanh(F C (10) +b1) → tanh(F C (7) +b2) → tanh(F C (5) +b3) → tanh(F C (4) +b4) →tanh(FC(3) + b5) → tanh(FC(2) + bg) → O(2).
Table 8: Hyper parameters of MI testsBatch size	512 I Learningrate ∣ 0.0004 ∣ λι^6 ∣1E — 2Training Epochs	20000 for RRAAAAB ；nd StdAAAab； 40000 for StdB1Table 9: Forward and reverse pass network of MNIST Classification testsForward pass (38645 weights):I(28, 28, 1) → relu(C(3, 64, 1) +b1) → MP → relu(C(3, 64, 1) +b2) → MP → relu(C(3, 1, 1) +b3) = Ir→ FC(10) → O(10)Reverse pass:Ir - b3 → relu(D(3, 64,1)) → UP - b? → relu(D(3, 64,1)) → UP - bi → relu(D(3,1,1)) → I0 (28, 28,1)Table 10: Hyper parameters of MNIST Classification testsBatch size	64	I λι 〜3 I	1E — 5Learning rate	0.001 for RRA,StdA and OrtA； 0.0001 for RRAAAB, StdAAAb/b and OrtAAABTraining Epochs	100 for RRA, StdA and OrtA； 400 for RR3AA, StdAA and OrtAA； 700 for RRAb, StdAB and OrtAB	Table 11: Forward and reverse pass of the neural network in Cifar testsForward pass:I(32, 32, 3) →relu(BN(C(3,64,1) + bi)) → relu(BN(C(3, 64, 1) + b2)) → MP→ relu(BN (C(3,	128,	1) + b3))	→ relu(BN (C(3, 128, 1)	+	b4)) → MP → relu(BN (C(3, 256, 1) + b5))→ relu(BN(C(3,	256,	1) +b6))	→ relu(BN (C(3, 256, 1)	+b7)) → MP→ relu(BN (C(3, 512, 1) + b8))→ relu(BN (C(3,	512,	1) + b9))	→ relu(BN (C(3, 512, 1)	+	bi0)) → MP → relu(BN (C(3, 512, 1) + bii))→ relu(BN(C(3,	512,	1) + bi2)) → relu(BN(C(3, 512, 1) + bi3)) = Ir → relu(BN (FC(4096) +bi4))
Table 9: Forward and reverse pass network of MNIST Classification testsForward pass (38645 weights):I(28, 28, 1) → relu(C(3, 64, 1) +b1) → MP → relu(C(3, 64, 1) +b2) → MP → relu(C(3, 1, 1) +b3) = Ir→ FC(10) → O(10)Reverse pass:Ir - b3 → relu(D(3, 64,1)) → UP - b? → relu(D(3, 64,1)) → UP - bi → relu(D(3,1,1)) → I0 (28, 28,1)Table 10: Hyper parameters of MNIST Classification testsBatch size	64	I λι 〜3 I	1E — 5Learning rate	0.001 for RRA,StdA and OrtA； 0.0001 for RRAAAB, StdAAAb/b and OrtAAABTraining Epochs	100 for RRA, StdA and OrtA； 400 for RR3AA, StdAA and OrtAA； 700 for RRAb, StdAB and OrtAB	Table 11: Forward and reverse pass of the neural network in Cifar testsForward pass:I(32, 32, 3) →relu(BN(C(3,64,1) + bi)) → relu(BN(C(3, 64, 1) + b2)) → MP→ relu(BN (C(3,	128,	1) + b3))	→ relu(BN (C(3, 128, 1)	+	b4)) → MP → relu(BN (C(3, 256, 1) + b5))→ relu(BN(C(3,	256,	1) +b6))	→ relu(BN (C(3, 256, 1)	+b7)) → MP→ relu(BN (C(3, 512, 1) + b8))→ relu(BN (C(3,	512,	1) + b9))	→ relu(BN (C(3, 512, 1)	+	bi0)) → MP → relu(BN (C(3, 512, 1) + bii))→ relu(BN(C(3,	512,	1) + bi2)) → relu(BN(C(3, 512, 1) + bi3)) = Ir → relu(BN (FC(4096) +bi4))→ relu(BN(FC(4096) + bi5)) → relu(BN(FC(10) + bi6))→ O(10).
Table 10: Hyper parameters of MNIST Classification testsBatch size	64	I λι 〜3 I	1E — 5Learning rate	0.001 for RRA,StdA and OrtA； 0.0001 for RRAAAB, StdAAAb/b and OrtAAABTraining Epochs	100 for RRA, StdA and OrtA； 400 for RR3AA, StdAA and OrtAA； 700 for RRAb, StdAB and OrtAB	Table 11: Forward and reverse pass of the neural network in Cifar testsForward pass:I(32, 32, 3) →relu(BN(C(3,64,1) + bi)) → relu(BN(C(3, 64, 1) + b2)) → MP→ relu(BN (C(3,	128,	1) + b3))	→ relu(BN (C(3, 128, 1)	+	b4)) → MP → relu(BN (C(3, 256, 1) + b5))→ relu(BN(C(3,	256,	1) +b6))	→ relu(BN (C(3, 256, 1)	+b7)) → MP→ relu(BN (C(3, 512, 1) + b8))→ relu(BN (C(3,	512,	1) + b9))	→ relu(BN (C(3, 512, 1)	+	bi0)) → MP → relu(BN (C(3, 512, 1) + bii))→ relu(BN(C(3,	512,	1) + bi2)) → relu(BN(C(3, 512, 1) + bi3)) = Ir → relu(BN (FC(4096) +bi4))→ relu(BN(FC(4096) + bi5)) → relu(BN(FC(10) + bi6))→ O(10).
Table 11: Forward and reverse pass of the neural network in Cifar testsForward pass:I(32, 32, 3) →relu(BN(C(3,64,1) + bi)) → relu(BN(C(3, 64, 1) + b2)) → MP→ relu(BN (C(3,	128,	1) + b3))	→ relu(BN (C(3, 128, 1)	+	b4)) → MP → relu(BN (C(3, 256, 1) + b5))→ relu(BN(C(3,	256,	1) +b6))	→ relu(BN (C(3, 256, 1)	+b7)) → MP→ relu(BN (C(3, 512, 1) + b8))→ relu(BN (C(3,	512,	1) + b9))	→ relu(BN (C(3, 512, 1)	+	bi0)) → MP → relu(BN (C(3, 512, 1) + bii))→ relu(BN(C(3,	512,	1) + bi2)) → relu(BN(C(3, 512, 1) + bi3)) = Ir → relu(BN (FC(4096) +bi4))→ relu(BN(FC(4096) + bi5)) → relu(BN(FC(10) + bi6))→ O(10).
Table 12: Hyper parameters of Cifar testsBatch size	200 I λι〜i3 I 1E -7Learning rate	0.1 (0 to 80 epochs); 0.01 (81 to epochs)； 0.001 (after 120 epochs)Training Epochs	180for RRAAA and StdAAa； 50 for RRAB and StdAB/B18Under review as a conference paper at ICLR 2020Table 13: Forward and reverse pass of network in smoke testsGenerator forward pass:I (16, 16, 1) → relu(C (5, 64, 1) + b1) → UP → relu(C (5, 128, 1) + b2) → UP → relu(C (5, 128, 1) + b3)→ relu(C(5, 64,1) + b4) → relu(C(5, 32,1) + b5) → relu(C(5,1,1) + b6)→ O(64, 64,1) = Ir.
Table 13: Forward and reverse pass of network in smoke testsGenerator forward pass:I (16, 16, 1) → relu(C (5, 64, 1) + b1) → UP → relu(C (5, 128, 1) + b2) → UP → relu(C (5, 128, 1) + b3)→ relu(C(5, 64,1) + b4) → relu(C(5, 32,1) + b5) → relu(C(5,1,1) + b6)→ O(64, 64,1) = Ir.
Table 14: Hyper parameters of smoke testsBatch size	64 I Learning rate ∣ 0.0002	I λi~6 I 0.1Training Epochs	40000 for RRA and StdA; 1000 for RRAb1 , RRAb2 , StdAB]	and StdAB2Table 15: Hyper parameters of VGG19 training	λi~i6		1E - 10Learning rate	0.01(0to7 epochs); 0.001(7to10 epochs); 0.0001(10 to 15 epochs); 0.00001 (after 15 epochs)Training Epochs	36 epochs for RRA6 and StdA; 22 epochs for RRAA and StdAATable 16: Forward and reverse pass of VGG19 networkForward pass:I(224, 224, 3) → relu(C(3, 64, 1) +b1) → relu(C(3, 64, 1) +b2) → MP → relu(C(3, 128, 1) +b3)→ relu(C(3, 128, 1) + b4) → MP → relu(C (3, 256, 1) + b5) → r elu(C (3, 256, 1) + b6)→ relu(C(3, 256, 1) + b7) → relu(C (3, 256, 1) + b8) → MP → r elu(C (3, 512, 1) + b9)→ relu(C(3, 512, 1) + b10) → relu(C(3, 512, 1) + b11) → relu(C(3, 512, 1) + b12) → MP→ relu(C(3, 512, 1) + b13) → relu(C(3, 512, 1) + b14) → relu(C(3, 512, 1) + b15)→ relu(C(3, 512, 1) + b16) = Ir → MP → relu(F C(4096) + b17) → relu(F C (4096) + b18)→ relu(FC(1000) + b19) → θ(1000).
Table 15: Hyper parameters of VGG19 training	λi~i6		1E - 10Learning rate	0.01(0to7 epochs); 0.001(7to10 epochs); 0.0001(10 to 15 epochs); 0.00001 (after 15 epochs)Training Epochs	36 epochs for RRA6 and StdA; 22 epochs for RRAA and StdAATable 16: Forward and reverse pass of VGG19 networkForward pass:I(224, 224, 3) → relu(C(3, 64, 1) +b1) → relu(C(3, 64, 1) +b2) → MP → relu(C(3, 128, 1) +b3)→ relu(C(3, 128, 1) + b4) → MP → relu(C (3, 256, 1) + b5) → r elu(C (3, 256, 1) + b6)→ relu(C(3, 256, 1) + b7) → relu(C (3, 256, 1) + b8) → MP → r elu(C (3, 512, 1) + b9)→ relu(C(3, 512, 1) + b10) → relu(C(3, 512, 1) + b11) → relu(C(3, 512, 1) + b12) → MP→ relu(C(3, 512, 1) + b13) → relu(C(3, 512, 1) + b14) → relu(C(3, 512, 1) + b15)→ relu(C(3, 512, 1) + b16) = Ir → MP → relu(F C(4096) + b17) → relu(F C (4096) + b18)→ relu(FC(1000) + b19) → θ(1000).
Table 16: Forward and reverse pass of VGG19 networkForward pass:I(224, 224, 3) → relu(C(3, 64, 1) +b1) → relu(C(3, 64, 1) +b2) → MP → relu(C(3, 128, 1) +b3)→ relu(C(3, 128, 1) + b4) → MP → relu(C (3, 256, 1) + b5) → r elu(C (3, 256, 1) + b6)→ relu(C(3, 256, 1) + b7) → relu(C (3, 256, 1) + b8) → MP → r elu(C (3, 512, 1) + b9)→ relu(C(3, 512, 1) + b10) → relu(C(3, 512, 1) + b11) → relu(C(3, 512, 1) + b12) → MP→ relu(C(3, 512, 1) + b13) → relu(C(3, 512, 1) + b14) → relu(C(3, 512, 1) + b15)→ relu(C(3, 512, 1) + b16) = Ir → MP → relu(F C(4096) + b17) → relu(F C (4096) + b18)→ relu(FC(1000) + b19) → θ(1000).
