Table 1: Correlation between the reward differences and semantic differences by replacing a given word with asimilar word (RP_S) and a distinct word (RP_D), respectively.
Table 2: Sentence-level correlation with human evaluation. All p-value (not shown) are less than 0.001.
Table 3: Evaluation scores on generated captions. The best score is in bold font and the second best score isunderlined. SPICE is the handcrafted evaluation metric. CHAIRs and CHAIRi represent the object hallucinationratio at sentence level and instance level, respectively. HE indicates human evaluation. VC indicates vocabularycoverage and NS is the ratio of novel sentences.
Table 4: Percentage of different grammar errors found in the generated captions. Re represents Redundancy,AE is Agreement Error, AM denotes Article Misuse and IS is Incomplete Sentence.
Table 5: Ablation methods of rAIRL. “term1” is the constant term in Eq. (9) and “term2” is the conditionalterm in Eq. (16). GE denotes grammar error rate.
Table 6: Comparison with existing methods on the handcrafted evaluation metrics.
Table 7: Results of rewriting caption from the located position by rAIRL on MS COCO standard split. Besideeach score we report its improvement relative to rewriting from a random position.
Table 8: Full results of the sentence-level correlation. All p-value (not shown) are less than 0.001.
Table 9: Results of using different model architectures in our method.
Table 10: Results of the conventional handcrafted metrics on MS COCO test split.
Table 11: Results on COCO test server. Methods marked With * adopt RL of CIDEr optimization.
Table 12: Formulas of different loss functions.
