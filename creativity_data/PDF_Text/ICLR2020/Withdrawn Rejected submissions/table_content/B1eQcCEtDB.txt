Table 1: Perplexity degradations for generations from popular language models. State-of-the-artperformance is usually reported via perplexity with respect to the test corpus (one-step predictionloss), but there is a striking blowup in the perplexity (i.e. exponential of the entropy) of these modelsâ€™long-term generations. Test ppl. is the exponential of the cross-entropy of the model with respectto the test corpus.
Table 2: Sample generations from a calibrated,state-of-the-art Transformer model trained on theGBW corpus, seeded with prefixes of sentences (in italics) from the holdout validation set.
Table 3: More generations from a state-of-the-art Transformer model trained on GBW, seeded withprefixes of sentences from the holdout validation set.
