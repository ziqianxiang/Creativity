Table 1: Architecture for EMNISTLayer	Size	ActivationInput	32 × 32 × 1	-Convolution	28 × 28 × 6	tanhAvg Pooling	14 × 14 × 6	tanhConvolution	10 × 10 × 16	tanhAvg Pooling	5 × 5 × 16	tanhConvolution	1 × 1 × 120	tanhFully Connected	84	tanhFully Connected	26		RBFTable 2: Architecture for CIFAR-10Layer	Size	ActivationInput	32 × 32 × 1	-Convolution	28 × 28 × 6	reluMax Pooling	14 × 14 × 6	-Convolution	10 × 10 × 16	reluMax Pooling	5 × 5 × 16	-Convolution	1 × 1 × 120	reluFully Connected	84	reluFully Connected	10		Softmax
Table 2: Architecture for CIFAR-10Layer	Size	ActivationInput	32 × 32 × 1	-Convolution	28 × 28 × 6	reluMax Pooling	14 × 14 × 6	-Convolution	10 × 10 × 16	reluMax Pooling	5 × 5 × 16	-Convolution	1 × 1 × 120	reluFully Connected	84	reluFully Connected	10		Softmaxpixels and 3 channels. The validation splits for both datasets were generated by random sampling ofa training dataset taking 10% out. All models were trained with a mini-batch size of 128.
Table 3: ResNet classification error for best independent and parallel-tempered (PT) solutions, from five runswith randomized 90/10 train-validation splits.
Table 4: Comparison study of dependance of average acceptance rate across all the replicas on differencevalues of constant C .
