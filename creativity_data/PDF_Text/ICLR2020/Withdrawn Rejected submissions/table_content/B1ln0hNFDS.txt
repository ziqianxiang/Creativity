Table 1: Comparison between our research and other studies. Note that this table can only summa-rize mainstreams in different research directions considering the huge research diversity. Please seeSection 2 for detailed discussions of related work.
Table 2: Location instability of feature maps between per- formers and explainers that were trained using the Pascal- Part dataset (Chen et al., 2014). A low location instability indicates a high filter interpretability. Please see Appendix F for comparisons with more baselines.			pretable track. When we used an explainer to diagnose feature map- s of a VGG network, about 86%- 96% activation scores came from explainable features.			We set η = 1.0 × 106 for the AlexNet, VGG-M, and VGG-S, because these three CNNs havesimilar numbers of layers. We set η = 1.5 × 105 for the VGG-16 and ResNets, since they have moreconv-layers. For each type of CNN performers, the parameter setting was uniformly applied to thelearning of different performers for various categories. We set 入⑷=5 X 104吗a)[k max(x1, 0) ∣∣],where the expectation was averaged over features of all images.
Table 4: Location instability of feature maps in per-formers and explainers that were trained using theCUB200-2011 dataset (Wah et al., 2011). A low lo-cation instability indicates a high filter interpretabili-ty. Please see Appendix F for comparisons with morebaselines.
Table 5: Multi-category classificationerrors using features of performers andexplainers based on the Pascal-Partdataset (Chen et al., 2014). Please seeAppendix J for more results of performersand explainers.
Table 6: Classification errors of original ResNets and interpretable ResNetsTherefore, our learning explainers to diagnose a pre-trained CNN without hurting the discriminationpower of the CNN has distinctive contributions beyond learning models with interpretable features.
Table 7: p values of explainers.
Table 8: Location instability of feature maps in performers and explainers. Performers are learnedbased on the Pascal-Part dataset (Chen et al., 2014)AlexNet (relu4 layer)	0.1542AlexNet (the top conv-layer)	0.1502Explainer	0.0906VGG-M (relu4 Iayer)	0.1484VGG-M (the top conv-layer)	0.1476Explainer	0.0815VGG-S (relu4 layer)	0.1518VGG-S (the top conv-layer)	0.1481Explainer	0.0704VGG-16 (relu5-2 Iayer)	0.1444VGG-16 (the top conv-layer)	0.1373Explainer	0.0490Table 9: Location instability of feature maps in performers and explainers. Performers are learnedbased on the CUB200-2011 dataset (Wah et al., 2011)G	About evaluation metricThe location instability is designed to evaluate the fitness between an intermediate-layer filter f andthe representation of a specific object part, and it has been widely used in (Zhang et al., 2018c;a).
Table 9: Location instability of feature maps in performers and explainers. Performers are learnedbased on the CUB200-2011 dataset (Wah et al., 2011)G	About evaluation metricThe location instability is designed to evaluate the fitness between an intermediate-layer filter f andthe representation of a specific object part, and it has been widely used in (Zhang et al., 2018c;a).
Table 10: Classification errors based on feature maps of performers and explainers.
