Table 1: Comparison with prior work on NIST Chinese-English translation task. The evaluationmetric is tokenized case-insensitive BLEU. The first four rows are numbers reported in the papers ofprior work. The first two baselines are the results that we got by running the Transformer (Vaswaniet al., 2017) and the Document Transformer (Zhang et al., 2018) on the NIST dataset. The sent-reranker is a variation of our model in which sentences in documents are assumed to be independent.
Table 2: BLEU scores on NIST dev set MT06 from rerankers which are incorporated with variouslanguage models. In the language model column X: Y means the language model X is trained ondataset Y. A bigger language model improves the doc-reranker but does not help the sent-reranker.
Table 3: Perplexity per word (whitespace delimited token) of language models on NIST dev set.
Table 4: Effect of different components in doc-reranker.
Table 5: SacreBLEU of different models on WMT19 validation and test sets and perplexity per wordof the language models on the English side of WMT19 validation set.
Table 6: Example outputs from the Document Transformer (proposal model) and our doc-reranker.
Table 7: Example outputs from the sent-reranker and the doc-reranker.
