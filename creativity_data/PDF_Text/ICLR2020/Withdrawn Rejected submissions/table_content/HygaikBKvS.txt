Table 1: Comparison of state-of-the-art agents on 57 Atari games trained up until 200M environmentsteps (per game) and DMLab-30 trained until 10B steps (multi-task; all games combined). The firsttwo rows are quoted from Xu et al. (2018) and Hessel et al. (2019), the third is our implementation ofa pixel control agent from Hessel et al. (2019) and the last two rows are our proposed LASER (LArgeScale Experience Replay) agent. All agents use hyper-parameter sweeps expect for the marked.
Table 2: Per level performance of various agents at 50M and 200M environment steps (see Figure 1).
