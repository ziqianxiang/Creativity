Table 1: Results for 7 training schemes and 5 evaluation schemes for ReLU networks. The best and second-bestresults among robust training methods (plain and at excluded) are highlighted by dark gray and light gray.
Table 2: Results of FC1 models with non-ReLU activation functions on MNIST.
Table 3: Definition of t1 and t2 for different activation functions.
Table 4: Values of α, and γ for different experiments.
Table 5: Full results of 9 training schemes and 7 evaluation schemes for ReLU networks.
Table 6: Results when using IBP-inspired model linearization. The color green, red and gray indicate better,worse and same results compared to those in Table 1	TLin	Tpec	TPEC TLin	TLin	Tpec	TPEC TLin	TLin	Tpec	TPEC TLin	MNIST-FC1,ReLU,l∞			MNIST	-CNN, ReLU, 1∞		CIFAR10 - CNN, R		^eLU, l∞plain		9.85	0.8207		10.56	0.8804		9.33	0.9331at		10.77	0.8972		11.39	0.9489		9.12	0.9128KW		8.48	0.7066		11.61	0.9674		8.43	0.8432MMR	12	8.04	0.6703	12	10.68	0.8897	10	8.05	0.8053MMR+at		7.68	0.6402		11.22	0.9351		8.45	0.8450PER		9.34	0.7780		11.17	0.9305		8.61	0.8606PER+at		9.38	07816		11.74	09784		8.68	08681	MNIST - FC1, ReLU, 12~~			MNIST - CNN, ReLU, 12~~			CIFAR10 - CNN, ReLU,TT-		plain		9.68	0.6914		13.64	0.9742		11.73	0.9775at		10.44	0.7457		13.76	0.9829		11.67	0.9725KW		7.72	0.5514		12.63	0.9021		10.23	0.8525MMR	14	5.86	0.4186	14	8.52	0.6086	12	9.05	0.7542MMR+at		5.91	0.4221		12.13	0.8664		10.33	0.8608PER		11.47	0.8194		13.75	0.9819		9.13	0.7609PER+at		11.34	0.8100		13.72	09796		10.71	08926Table 7: Number of steps of bound calculation for the optimal in Fast-Lin (TLin) and PEC (TPEC) for ReLU
Table 7: Number of steps of bound calculation for the optimal in Fast-Lin (TLin) and PEC (TPEC) for ReLUnetworks. Note that TLin is a constant for different models given the original interval [e,司.
Table 8: Number of steps of bound calculation for the optimal in CROWN (TCRO) and PEC (TPEC) fornon-ReLU networks. Note that TCRO is a constant for different models given the original interval 怛,可.
