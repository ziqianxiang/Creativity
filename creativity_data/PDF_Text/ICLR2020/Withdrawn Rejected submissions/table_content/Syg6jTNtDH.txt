Table 1: Vocabularies of different methods.
Table 2: Results on word similarity tasks trainedon Wiki-1B. For reference, we also show the re-sults of the official skip-gram and GloVe trainedon larger corpora.
Table 3: Magnitude and numeration evaluation results for our methods and baselines. Accuraciesof OVA, SC and BC are expressed as percentages. Lower AVGR indicates better performance.
Table 4: The results of the numeral prediction tasks.
Table 5: The results of sequence labeling. We report the accuracy, precision, recall, F1 score for theoriginal, augmented, and harder test sets with different training data sizes. Accuracy is in the tokenlevel and the other metrics are in the entity level. We report the standard deviations in Appendix H.
Table 6: Hyper-parameter values for GMM and SOM based methods for each experiment.
Table 7: Values of general hyper-parameters for each experiment.
Table 8: Examples of prototypes and their nearest non-numerical words.
Table 9: Training speed for each methods.
Table 10: Probing test results. MLP1 and MLP2 denote MLP with one and two hidden layersrespectively.
Table 11: Results of the magnitude evaluation using MLP2 trained on the Subtraction probing task.
Table 12: Statistics of low-resource customer-service dataset.
Table 13: The results of sequence labeling. We report the accuracy, precision, recall, F1 score forthe original, augmented, and harder test sets with different training data sizes. Accuracy is in thetoken level and the other metrics are in the entity level.
