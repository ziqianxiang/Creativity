Table 1: Results of boosting filters (GT-filters) of a particular one layer network on MNIST, Cifar-10, and Cifar-100. [-] indicates the corresponding layer boosted. ( Here 8@3*3 means that this layerhas 8 filters with size of 3 *3)adding filters/layers, we decrease the learning rate by 1/10, continue training 70 epochs; and thenfurther decrease the learning rate by 1/10 again, and go on training 30 epochs.
Table 2: Results of boosting filters (GT-filters) of VGG16 networks. [-] indicated the convolutionfilters of corresponding layer, the last [-] indicates the number of linear units.
Table 3: Varying Epoch J in boosting VGG family network on CIFAR10 dataset by GT-filters Alg.
Table 4: Varying threshold τ in boosting VGG family network on CIFAR10 dataset by GT-filtersAlg (J = 40). [160, 160] indicates a block of two conv layer; each has 160 filters. Each networkhas an input conv layer.
Table 5: Under the same budget of computational cost (total FLOPSs), our growing filter strategy forboosting network achieves better accuracy and smaller complexity than standard training networks,comparable accuracy to those with much more cost.
Table 6: Comparison of Autogrow and our algorithm in found nets and performance, where oursachieves comparable accuracies with much smaller model complexity.
Table 7: Comparison between the standard ResNets and our boosting algorithm at different thresh-olds () with J = 40 and τ = 0.4 on CIFAR10 dataset. The boosted networks have stable perfor-mance with varying thresholds and improve the standard ResNet34 with less number of layers.
