Table 1: Image classification on CIFAR10 with ResNet. The # abbreviates ‘number of’, while theparameters are measured in millions. The term ‘block’ abbreviates a ‘residual block’. Note thateach baseline, e.g. ResNet18, has the same performance with the respective Newton-ResNet, butsignificantly more parameters.
Table 2: In this experiment all activation functions are removed. The performance of ResNet18 dropsdramatically by 58% when we remove the activation functions. On the contrary, the performance ofNewton-ResNet is decreased by less than 7% (in comparison to Table 3).
Table 3: The differences of the proposed method with the original residual block are highlighted inblue. The Xqroj,lin_Proj are 1 X 1 convolutions added for normalization purposes in the proposedresidual block.
Table 4: CIFAR100 classification with ResNet. The accuracy of the compared methods is similar, butNewton-ResNet has 30% less parameters.
Table 5: Experiment on CIFAR100 by removing all activation functions.
Table 6: Image classification (ImageNet) with ResNet. The column of “Speed” refers to the inferencespeed (images/s) of each method.
Table 7: Image classification (ImageNet) without “relu”. Newton-ResNet outperforms the corre-sponding ResNet by a substantial margin.
Table 8: Quantitative results on image generation with resnet-based generator and discriminator.
Table 9: Speech classification with ResNet. The accuracy of the compared methods is similar, butNewton-ResNet has 38% less parameters.
