Table 1: Comparison of methods developed for training for consistency with specifications.
Table 2: Comparison of GRU training methods on the MMNIST task. We evaluate against thetermination specification on different metrics, and also report nominal accuracy. ’-'indicates a trivialverified accuracy of 0% obtained with bound propagation. The entries with Verified TerminationAccuracies corresponding to 0.0 are those where we were able to generate adversarial examples(counter-examples) to the specification for every point in the test-set. We found that adversarialtraining is difficult because of the presence of the sigmoid & tanh activation functions commonly usedin GRUs. To have a meaningful baseline, we performed adversarial training on an RNN (feedforwardcells with ReLU activation). For = 0.1, attacking the loss from Wang et al. (2019) to produce longersequences performs better, while for the other values adversarial training with the STL quantitativeloss performs better. Adversarial training performs well but is difficult to verify. At larger , verifiedtraining results in both better guarantees (specification conformance), and better nominal accuracies.
Table 3: We train the RNN with ReLU activations from (Wang et al.,2019) to be verifiable with = 0.3, and compare its verifiability withMILP based verification reported in Wang et al. (2019) at differentperturbation radii. The nominal accuracy for the model trained to beverifiable is 93.9% and model trained in a standard manner is 96.4%.
Table 4: Mean/Variance performance (across 5 agents of each type) across different metrics. For eachagent, reward is computed as mean across 100 episodes. is distance from the center of the grid cells,and for each E we report the fraction of the cells for which we are able to certify that 夕recharge holds.
Table 5: Language model perplexity, number offailures during an exhaustive enumerative searchover the 25M perturbations, and computationalcost of verification (number of forward passes).
Table 6: Comparison of methods developed for enforcing/checking consistency with specifications.
Table 7: Samples of generated text for nominal and verifiably training GRUs on the Shakespearecorpus under greedy decoding. <eol> refers to the end of line.
