Table 1: Overview of training benchmarks.
Table 2: Comparison of final model quality. Shorthand: AS=AdaScale, LSW=Linear scaling rule withwarm-up, gray=model quality significantly worse than for S = 1 (5 trials, 0.95 significance), N/A=trainingdiverges, ElastiC↑∕]=elastic scaling With increasing/decreasing scale (See Figure 4). Linear scaling leads topoor model quality as the scale increases, while AdaScale preserves model performance for nearly all cases.
Table 3: Learning rate schedules for training benchmarks.
Table 4: AdaScale final metrics with varying moving average parameter.
Table 5: Comparison of AS and LSW+. Shorthand: AS=AdaScale, LSW+=Stretched linear scaling rulewith warm-up, takes the same number of steps as AS gray=model quality significantly worse than for S = 1 (5trials, 0.95 significance), N/A=training divergesTask	S	Total batch size	Validation metric		Training loss		Total iterations				AS	LSW+	AS	LSW+	AS	LSW+cifar10	1	128	94.1	94.1	0.157	0.157	39.1k	39.1k	8	1.02k	94.1	94.0	0.153	0.145	5.85k	5.85k	16	2.05k	94.1	94.1	0.150	0.136	3.36k	3.36k	32	4.10k	94.1	94.0	0.145	0.128	2.08k	2.08k	64	8.19k	93.9	93.0	0.140	0.128	1.41k	1.41kimagenet	1	256	76.4	76.4	1.30	1.30	451k	451k	16	4.10k	76.5	76.5	1.26	1.27	33.2k	33.2k	32	8.19k	76.6	76.4	1.23	1.24	18.7k	18.7k	64	16.4k	76.5	76.5	1.19	1.20	11.2k	11.2k	128	32.8k	76.5	75.5	1.14	1.20	7.29k	7.29kspeech	1	32	79.6	79.6	2.03	2.03	84.8k	84.8k	4	128	81.0	81.0	5.21	4.22	22.5k	22.5k	8	256	80.7	80.7	6.74	6.61	12.1k	12.1k	16	512	80.6	N/A	7.33	N/A	6.95k	6.95k	32	1.02k	80.3	N/A	8.43	N/A	4.29k	4.29k
