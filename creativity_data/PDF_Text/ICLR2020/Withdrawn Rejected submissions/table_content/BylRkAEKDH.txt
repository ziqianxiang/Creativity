Table 1: TabNet achieves high performance with small number of network parameters. Mean and std.
Table 2: Performance for forest cover type dataset. The performance of the comparison models* arefrom (Mitchell et al., 2018). AutoInt models pairwise feature interactions with an attention-base deepneural network (Song et al., 2018). AutoML Tables (denoted as ??) is an automated machine learningdevelopment tool based on ensemble of models including linear feed-forward deep neural network,gradient boosted decision tree, AdaNet (Cortes et al., 2016) and ensembles (AutoML, 2019). ForAutoML Tables (??), the amount of node hours reflects the measure of the count of searched modelsfor the ensemble and their complexity.6 A single TabNet model, without fine-grained hyperparametersearch, can outperform the accuracy of ensemble models with very thorough hyperparameter search.
Table 3: Performance for poker hand induction dataset. The input-output relationship is deterministicand hand-crafted rules implemented with several lines of code can get 100% accuracy. Yet, neuralnetworks and decision tree models severely suffer from the imbalanced data and cannot learn therequired sorting and ranking operations with the raw input features. The results for comparisonmodels* are from (Yang et al., 2018).
Table 4: Performance for Sarcos robotics arm inverse dynamics dataset. Three TabNet models ofdifferent sizes are considered (denoted with -S, -M and -L). The performance of the comparisonmodels * are from (Tanno et al., 2018)._____________________________________Model	TestMSE	Number OfparametersRandom forest*	2.39*	16.7KStochastic decision tree*	2TTT*	28KMulti layer perceptron*	2.T3*	0.T4M-Adaptive neural tree ensemble*-	T.23*	0.60MGradient boosted tree*	T44*	0.99MTabNet-S	125	63KTabNet-M	028	059MTabNet-L	0.14 ä¸€	1.75MTable 5: Performance on Higgs boson dataset. Two TabNet models of different sizes are considered(denoted with -S and -M). The performance of the comparison models* are from (Mocanu et al.,2018). Sparse evolutionary training applies non-structured sparsity integrated into training, yieldinglow number of parameters. With its compact representation, TabNet, (without any further pruning orextra non-structured sparsity), yields almost similar performance with sparse evolutionary training forthe same number of parameters. Gradient boosted tree models are implemented using (Tensorflow,2019), see Appendix for details.
Table 5: Performance on Higgs boson dataset. Two TabNet models of different sizes are considered(denoted with -S and -M). The performance of the comparison models* are from (Mocanu et al.,2018). Sparse evolutionary training applies non-structured sparsity integrated into training, yieldinglow number of parameters. With its compact representation, TabNet, (without any further pruning orextra non-structured sparsity), yields almost similar performance with sparse evolutionary training forthe same number of parameters. Gradient boosted tree models are implemented using (Tensorflow,2019), see Appendix for details.
Table 6: Performance for Rossmann store sales dataset (Kaggle, 2019a). We use the exactly samepreprocessing and data split with (Catboost, 2019) - data from 2014 is used for training and validation,whereas 2015 is used for testing. The performance of the comparison models* are from (Catboost,2019).	___________________________________________________________Model	TeStMSEXGBoost*	490.83*LightGBM*	504.76*CatBoost*	489.75*TabNet	485.12B.2	KDD datasetsTable 7: Performance on three KDD datasets on Customer Relationship Management: Appetency,Churn and Upselling. We apply the similar preprocessing and data partitioning as (Prokhorenkovaet al., 2018). The PerformanCe of the COmPariSOn models* are from (PrOkhOrenkOva et al., 2018).
Table 7: Performance on three KDD datasets on Customer Relationship Management: Appetency,Churn and Upselling. We apply the similar preprocessing and data partitioning as (Prokhorenkovaet al., 2018). The PerformanCe of the COmPariSOn models* are from (PrOkhOrenkOva et al., 2018).
Table 8: Performance for KDD Census Income (Dua & Graff, 2017). The task is income predictionfrom demographic and employment related variables. The performance of the comparison models*are from (Oza, 2005).________________________________________________Model	Test accuracy (%)XGBoost	9576CatBoost	95:72MUlti-Iayer perceptron*	9519Boosting, Multi-layer perceptron*-	94:86-Bagging, Multi-layer perceptron*-	95:33TabNet	95.49models yield similar results). For these cases, TabNet shows very similar (or slightly worse) perfor-mance than XGBoost and CatBoost, that are known to be very robust as they contain high amount ofensembles.
Table 9: Performance for loan delinquency prediction on a proprietary dataset, constructed from(Mac, 2019). The task is to classify loan delinquency status (among four categories), from manyinput features including personal information and financial status. The training dataset consists of93k samples. The dataset is highly imbalanced as the delinquency situation is observed rarely.
Table 10: Ablation studies for the TabNet model for the forest cover type dataset.
Table 11: Results for semi-supervised learning for Adult Census Income, along with the supervisedlearning benchmarks. 50 samples with labels are randomly chosen from the training dataset. Wereoptimize the learning hyperparameters on a separate validation set for a fair comparison.
