Table 1: Timesteps rolled out by planner (planning levels) as a fraction of MPC-8 for model-basedplanning algorithms. Shown are the average over all environments and the range (min-max) acrossthe environments for 5 seeds. For more detailed graphs, see Fig. A.1 in Appendix A.
Table 2: Average lifetime rewards. S, NS, and CW, denote the standard, novel states, and changingworlds settings; (D) and (S) denote dense/sparse reward mazes. Shown is the average for 5 seedswith two standard deviations. Best results are bolded. See Appendix A for full learning curves.
Table C.1: Effect of varying threshold hyperparametersStandard Deviation σthres :	4	8 (Default) 14Average reward	0.47 ± 0.20 0.42 ± 0.05	0.44 ± 0.16Bellman Error thres :	10	25 (Default) 40Average reward	0.47 ± 0.17 0.47 ± 0.09	0.43 ± 0.24Figure C.	1: Learning curves for hyperparameter sweep. Left: standard deviation σthres. Right:Bellman error thres. Legend shows value of relevant hyperparameter. 13 seeds were run in total.
