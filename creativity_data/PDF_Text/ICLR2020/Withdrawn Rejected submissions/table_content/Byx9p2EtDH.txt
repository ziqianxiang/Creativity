Table 1: MULTIPOLAR vs. Baselines. Bootstrap mean and 95% confidence bounds of averageepisodic rewards over various training samples across six environments.
Table 2: Results for MULTIPOLAR and its degraded versions in Hopper.
Table 3: Results for MULTIPOLAR with different source policy sampling schemes in Hopper.
Table 4: Results for MULTIPOLAR with different number of source policies in Hopper.
Table 5: Hyperparameters for Acrobot, CartPole, Hopper, Ant and InvertedPendulumSwingup.
Table 6: Hyperparameters for LunarLander.
Table 7: Sampling range for Ant kinematic and dynamic parameters.		Table 8: Sampling range for CartPole kine- matic and dynamic parameters.	Kinematics		Kinematics	Links	Length Range	Links	Length Range (m)Legs	[0.4,1.4] X default length	Pole	[0.1,3]	一	Dynamics		DynamicsDamping	[0.1, 5]	Force	[6,13]Friction	[0.4, 2.5]	Gravity	[-14, -6]Armature	[0.25, 3]	Poll mass	[0.1, 3]Links mass	[0.7, 1.1] × default mass	Cart mass	[0.3, 4]	Table 9: Sampling range for Hopper kine-matic and dynamic parameters.
Table 9: Sampling range for Hopper kine-matic and dynamic parameters.
Table 10: Sampling range for InvertedPendu-lumSwingup kinematic and dynamic param-eters.
Table 11: Sampling range for Acrobot kinematic and dynamic parameters.		Table 12: Sampling range for Lu- narLander kinematic and dynamic parameters.		Kinematics	Kinematics	Links	Length Range (m)	Side engine height	[10, 20]Link 1&2	[0.3, 1.3]	—	Dynamics	Dynamics	Scale	[25, 50]Links mass	[0.5, 1.5]	Initial Random	[500, 1500]Links center mass	[0.05, 0.95] × link length	Main engine power	[10, 40]Links inertia moments	[0.25, 1.5]	Side engine power	[0.5, 2]			ΓQ 1 Ql		Side engine away	[8, 18]13Under review as a conference paper at ICLR 2020A.3 Evaluation MethodIn this section, we explain how we calculated the mean of average episodic rewards in Tables 1, 2, 3,and 4, over a specific number of training samples (the numbers at the header of the tables e.g., 25K,50K, 75K, and 100K for the CartPole) which we denote by T in what follows. For each experimentin an environment instance, we computed the average episodic reward by taking the average of therewards over all the episodes the agent played from the beginning of the training until collecting Tnumber of training samples. Then we collected the computed average episodic rewards of all theexperiments, i.e., all the combinations of three random seeds, three random sets of source policies
Table 13: Results for MULTIPOLAR with low-performing source policies vs. with randomly ini-tialized source policies in Hopper.
