Table 1: Translation results on the WMT 2016 Romanian→English task on the test set(newstest2016). (I = target hypothesis length, T = number of iterations for the hypothesisgeneration/refinements)Bleu [%]Category	Decoding Complexity	Method	Length				Predicted	Gold		Fertility-based (GU et al., 2018)	29.1	-		CTC (Libovicky & Helcl, 2018)	24.7	-Non-autoregressive	O(1)	Imitation learning (Wei et al., 2019)	28.9	-		Reinforcement learning (Shao et al., 2019)	27.9	-		Generative flow (Ma et al., 2019)	30.2	-Non-autoregressive	O(T)	Extra refinement model (Lee et al., 2018)	30.2	31.3+ Refinements		MTM (This Work)	30.5	32.0Autoregressive	O(I)	Enc-Dec Transformer	32.9	-4	ExperimentsWe implemented the MTM in the RETURNN framework (Doetsch et al., 2017) and evaluate theperformance on the WMT 2016 Romanian→English translation task1. All data used in the experi-ments are preprocessed using the Moses (Koehn et al., 2007) tokenizer and frequent-casing (Vilaret al., 2010). We learn a joint source/target byte pair encoding (BPE) (Sennrich et al., 2016) with20k merge operations on the bilingual training data. Unless mentioned otherwise, we report results
Table 2: Comparison of target hypothesis length modeling for the MTM on the development set(newsdev2016).
Table 3: Comparison for a selected set of hyperparameters: number of layers N, hidden size dmodel,number of attention heads h, attention key size dk, attention value size dv , dropout probability PdropI N (⇔ NenC + Ndec)	dmodel	h	dk	dv	Pdrop	BLEU [%]	Ter [%]MTM I	12	512	16	32	32	0.1	31.9	55.1		4	128	128		31.6	55.5		8	64	64		31.9	54.9		32	16	16		31.3	55.86						30.6	56.418						32.0	54.924						32.4	54.6	256		16	16		29.4	57.8	1024		64	64		32.2	54.8					0.0	29.2	58.1					0.2	31	56.1Enc-Dec ∣	6+6	512	8	64	64	0.2	34.7	52.33+3						35.0	51.89+9						35.7	51.412+12						35.6	51.6	256		32	32		33.6	53.1	1024		128	128		35.5	51.5
Table 4: Comparison of different decoding strategies in greedy decoding for the MTM on the devel-opment set (newsdev2016) of the Romanian→English task.
