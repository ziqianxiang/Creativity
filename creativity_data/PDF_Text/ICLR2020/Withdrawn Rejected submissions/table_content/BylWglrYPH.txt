Table 1: Accuracies in identifying sequence structureSoftmaxMax-PoolingHidden UnitsConvolutionInput ArrayRecurrent Net Multi-layer Perceptron Convolution50%	50%	100%The results on the test set in Table 1 show that neither the multi-layer perceptron nor the recurrentnetwork learn a rule that generalises effectively to unseen syllables. However, the weight sharingin the convolutional net requires that the same function is applied to each syllable, giving perfectgeneralisation. The filter, being applied at every position, cannot discriminate between syllables,and instead can only respond to the information about temporal structure in the channels. So, forexample, in the case of the sequence wo fe wo, the input channels at the wo position take the values101, representing the fact that the same token occurs in the first and third temporal slots.
Table 2: Accuracies in translating instructions into actions.
Table 3: Performance of LSTM Architectures on the palindromic language.
