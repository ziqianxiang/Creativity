Table 1: Performance of different models on QED and DRD2 optimization tasks. Italicized mod-els with + are augmented with iterative target augmentation. We emphasize that iterative targetaugmentation remains critical to performance in the semi-supervised and transductive settings; dataaugmentation without an external filter instead decreases performance.
Table 2: Ablation analysis of filtering at training and test time. “Train” indicates a model whosetraining process uses data augmentation according to our framework. “Test” indicates a model thatuses the external filter at prediction time to discard candidate outputs which fail to pass the filter. Theevaluation for VSeq2Seq(no-filter) is conducted after 10 augmentation epochs, as the best validationset performance only decreases over the course of training.
Table 3: Model performance on Karel program syn-thesis task. MLE+ is our augmented version of theMLE model (Bunel et al., 2018).
Table 4: Number of source-target pairs in training, validation, and test sets for each task.
Table 5: Performance of our model VSeq2Seq+ with different values of K. All other experimentsuse K = 4.
Table 6: Performance of our proposed augmentation scheme, VSeq2Seq+, compared to an alterna-tive version (VSeq2Seq+, keep-targets) which keeps all generated targets and continually grows thetraining dataset.
Table 7: Ablation analysis of filtering at training and test time. “Train” indicates a model whosetraining process uses data augmentation according to our framework. “Test” indicates a model thatuses the external filter at prediction time to discard candidate outputs which fail to pass the filter.
