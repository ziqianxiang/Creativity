Table 1: Derived coefficients used to initialize the weights of polynomial activations by approximat-ing ReLU, Swish and TanHActivation	ApproximationReLU Swish TanH ReLU Swish TanH ReLU Swish TanH ReLU	f2(x) = 0.47 + 0.50 * X + 0.09 * x2 f2(x) = 0.24 + 0.50 * x + 0.10 * x2 f2(x) = 2.5e-12 + 0.29 * X — 4.0e-12 * x2 f3(x) = 0.47 + 0.50 * x + 0.09 * x2 — 1.7e-10 * x3 f3(x) = 0.24 + 0.50 * x + 0.10 * x2 — 1.2e-10 * x3 f3(x) = 1.0e-9 + 0.51 * x — 2.2e-10 * x2 — 0.01 * x3 f4(x) = 0.29 + 0.50 * x + 0.16 * x2 + 1.8e-10 * x3 — 3.3e-3 * x4 f4(x) = 0.07 + 0.50 * x + 0.17 * x2 + 1.8e-10 * x3 — 3.2e-3 * x4 f4(x) = —1.1e-8 + 0.51 * x + 4.5e-9 * x2 — 0.01 * x3 — 1.5e-10 * x4 f5(x) = 0.29 + 0.50 * x + 0.16 * x2 — 1.6e-8 * x3 — 3.3e-3 * x4 +7.5e-10 * x5Swish	f5(x) = 0.07 + 0.50 * x + 0.17 * x2 — 1.5e-8 * x3 — 3.2e-3 * x4 +6.9e-10 * x5TanH	f5(x) = 3.0e-8 + 0.67 * x — 2.4e-8 * x2 — 0.04 * x3 + 1.7e-9 * x4 + 1.1e-3 * x5ReLU	f6(x) = 0.21 + 0.50 * x + 0.23 * x2 + 7.6e-8 * x3 — 1.1e-2 * x4 —3.5e-9 * x5 + 2.3e-4 * x6Swish	f6(x) = 0.02 + 0.50 * x + 0.21 * x2 — 1.0e-7 * x3 — 8.1e-3 * x4 +3.7e-9 * x5 + 1.4e-4 * x6TanH	f6(x) = —3.7e-8 + 0.67 * X + 2.6e-8 * x2 — 0.04 * x3 — 4.9e-9 * x4 + 1.1e-3 * x5 + 3.4e-10 * x6ReLU	f7(x) = 0.21 + 0.50 * X + 0.23 * x2 + 1.4e-6 * x3 — 1.1e-2 * x4 —1.2e-7 * x5 + 2.3e-4 * x6 + 2.7e-9 * x7Swish	f7(x) = 0.02 + 0.50 * X + 0.21 * x2 + 4.8e-8 * x3 — 8.1e-3 * x4 —6.5e-9 * x5 + 1.4e-4 * x6 + 1.9e-10 * x7TanH	f7(x) = —2.2e-6 + 0.79 * X — 3.4e-7 * x2 — 0.09 * x3 — 1.2e-10 * x4 +4.8e-3 * x5 + 8.5e-10 * x6 — 9.1e-5 * x7ReLU	f8(x) = 0.17 + 0.50 * x + 0.29 * x2 — 1.4e-7 * x3 — 2.6e-2 * x4 +9.7e-9 * x5 + 1.2e-3 * x6 — 2.0e-10 * x7 — 2.1e-5 * x8Swish	f8(x) = 6.5e-3 + 0.50 * x + 0.23 * x2 + 2.2e-8 * x3 — 0.01 * x4 —1.7e-9 * X5 + 4.7e-4 * X6 + 4.0e-11 * x7 — 7.0e-6 * X8TanH	f8(x) = 1.3e-6 + 0.79 * x — 8.0e-7 * X2 — 0.09 * X3 + 1.3e-7 * X4 +4.8e-3 * X5 — 8.2e-9 * x6 — 9.1e-5 * X7 + 1.6e-10 * X8ReLU	f9(x) = 0.17 + 0.50 * x + 0.29 * X2 + 9.0e-8 * x3 — 2.6e-2 * x4 —1.2e-8 * X5 + 1.2e-3 * X6 + 6.5e-10 * x7 — 2.1e-5 * x8 — 1.1e-11 * X9Swish	f9(x) = 6.5e-3 + 0.50 * x + 0.23 * x2 — 1.3e-8 * x3 — 0.01 * x4 + 1.8e-9 * X5 + 4.7e-4 * x6 — 9.5e-11 * x7 — 7.0e-6 * X8 + 1.7e-12 * X9TanH	f9(x) = —5.7e-8 + 0.87 * X + 1.0e-7 * X2 — 0.13 * x3 — 2.3e-8 * X4 +1.2e-2 * X5 + 1.6e-9 * x6 — 5.0e-4 * x7 — 3.3e-11 * X8 + 7.8e-6 * X95.1	MNISTThe MNIST dataset consists greysale handwritten digits (0-9) of size 28 X 28 pixel, with 60,000
Table 2: Comparison of activation functions on MNISTMethod	Network-1	NetWork-2	NetWork-3	NetWork-4ReLU	0.77	0.66	0.51	0.50LReLU	0.74	0.57	0.54	0.59PReLU	0.71	0.55	0.45	0.51ELU	0.86	0.54	0.58	0.68GELU	0.79	0.63	0.55	0.67SELU	0.76	0.69	0.63	0.68Swish-1	0.75	0.63	0.66	0.52Ours (n=2)	0.64	0.48	0.51	0.55Ours (n=3)	0.61	0.50	0.54	0.46Ours (n=4)	0.69	0.62	0.69	0.66Ours (n=5)	0.71	0.58	0.59	0.49Ours (n=6)	0.67	0.65	0.58	0.50Ours (n=7)	0.83	0.59	0.62	0.71Ours (n=8)	0.81	0.69	0.68	0.54Ours (n=9)	0.75	0.71	1.03	0.58Table 3: Comparison of various activation functions performance on CIFAR using ResNet-164(R164), Wide ResNet 28-10 (WRN), and DenseNet 100-12 (Dense)Method	R164	CIFAR10 WRN	Dense	CIFAR100		
Table 3: Comparison of various activation functions performance on CIFAR using ResNet-164(R164), Wide ResNet 28-10 (WRN), and DenseNet 100-12 (Dense)Method	R164	CIFAR10 WRN	Dense	CIFAR100						R164	WRN	DenseSoftplus	94.6*	94.9*	94.7*	76.0*	78.4*	83.7*ReLU	93.8*	95.3*	94.8*	74.2*	77.8*	83.7*LReLU	94.2*	95.6*	94.7*	74.2*	78.0*	83.3*PReLU	94.1*	95.1*	94.5*	74.5*	77.3*	81.5*ELU	94.1*	94.1*	94.4*	75.0*	76.0*	80.6*GELU	94.3*	95.5*	94.8*	74.7*	78.0*	83.8*SELU	93.0*	93.2*	93.9*	73.2*	74.3*	80.8*Swish-1	94.7*	95.5*	94.8*	75.1*	78.5*	83.8*Swish	94.5*	95.5*	94.8*	75.1*	78.0*	83.9*Ours (n = 2)	94.0	94.7	93.7	75.1	78.6	79.5Ours (n = 3)	93.6	95.6	94.1	74.6	77.8	78.9* Reported in Ramachandran et al. (2019)pool as the gradients are usually high during backpropogation. We train using SGD with an initiallearning rate of 0.1, a momentum of 0.9 and a weight decay of 0.0005.
