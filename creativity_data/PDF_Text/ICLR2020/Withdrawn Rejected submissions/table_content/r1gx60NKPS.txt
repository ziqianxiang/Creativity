Table 1: BLEU*/ROUGE* and RoBERTa-STS scores on sentence pair examples from STS-BSentence pair	BLEU*	ROUGE*	RoBERTa-STS	LabelThe last time the survey was conducted, in 1995, thoSe numberS matched. In 1995, the last survey, those numbers were equal.	0.99	1.42	4.65/5	5:00/5A band is performing on a stage. A band is playing onstage.	1.14	2.29-	3:85/5	5.00/5Two white dogs are swimming in the water. The birds are swimming in the water.	3.00	-3:23-	1:19/5	0.80/5A man plays the piano. A man is playing a piano.	0.92	-2T7-	5:00/5	5.00/5Pardon the brevity of this answer, but I would say ”named” is preferred within the context of your example. Named is preferred in your example, since you are formally giving a name to your method.	0.25	0.72	3.73/5	4:40/5-δ 1	2	3	4	5RoBERTa Scores, Scaled to 5(a)	(b)Figure 1: Comparison of RoBERTa-STS scores(a) and BLEU* scores(b) with labels from the STS-Bdev set.
Table 2: Correlation with human judgement of similarity on STS-B Benchmark development set	ROUGE	BLEU	RoBERTa-STSPearSon correlation with human judgement	0.55	0.50	0.92	—4Under review as a conference paper at ICLR 2020	ROUGE	BLEU	RoBERTa-STSSpearman,s RC	-0.255-	0.216	^0744Kendall's T	0.215	0.186	0.69	—Table 3: Results of logical entailment experiments4	Assessing Evaluation Metrics4.1	Metric ScorecardTo overcome the previously highlighted challenges and provide a framework in which metrics com-paring reference summaries/translation can be assessed and improved, we establish first-principlescriteria on what a good evaluator should do:•	The first one is that it should be highly correlated with human judgement of semanticsimilarity.
Table 3: Results of logical entailment experiments4	Assessing Evaluation Metrics4.1	Metric ScorecardTo overcome the previously highlighted challenges and provide a framework in which metrics com-paring reference summaries/translation can be assessed and improved, we establish first-principlescriteria on what a good evaluator should do:•	The first one is that it should be highly correlated with human judgement of semanticsimilarity.
Table 4: Results of grammatical error experiments	ROUGE-1	ROUGE-2	ROUGE-L	BLEU	RoBERTa-STSPearson Corr.	0.498	0.491	0.526	0.253	0.63	—Table 5: Results of WMT experiments5	Machine Translation ExperimentsIn the previous section, we outlined a series of criteria to assess evaluation metrics and shown how,for each dimension, RoBERTa-STS significantly outperformed ROUGE and BLEU. In the followingsection, we report results showing how BLEU, ROUGE and RoBERTa-STS correlate with humanjudgement of quality in the case of machine translation.
Table 5: Results of WMT experiments5	Machine Translation ExperimentsIn the previous section, we outlined a series of criteria to assess evaluation metrics and shown how,for each dimension, RoBERTa-STS significantly outperformed ROUGE and BLEU. In the followingsection, we report results showing how BLEU, ROUGE and RoBERTa-STS correlate with humanjudgement of quality in the case of machine translation.
Table 6: BLEU*/ROUGE* and RoBERTa-STS scores on sentence pair examples from STS-BId	Sentence pair	BLEU*	ROUGE*	RoBERTa-STS	Label1	The company claims it,s the largest single Apple VAR Xserve sale to date. The company claimed it is the largest sale of Xserves by an Apple retailer.	0.44	1.62	4.36/5	5.00/57Under review as a conference paper at ICLR 20202	A woman puts flour on a piece of meat. A woman is putting flour onto some meat.	0.63	1.78	5.07/5	5.00/53	He later learned that the incident was caused by the Concorde’s sonic boom. He later found out the alarming incident had been caused by Concorde,s powerful sonic boom.	0.74	2.58	4.96/5	5:00/54	It indeed appears the Andromeda galaxy (M31) and The Milky Way (MW) are en route to a colli- sion. In a few billion years, the Milky Way and An- dromeda will collide.	0.20	1.09	3.37/5	4:40/55	You definitely do NOT want to be supporting your weight with your arms on the bike for normal rid- ing. No, don’t support your weight on your arms Your hands simply aren’t really made for supporting all that weight.	0.28	1.13	2.73/5	4:20/56~6~	7 detained for ,house sister, scandal China detains 7 for ”house sister” scandal	-0:33-	1.52	4:25/5	4.20/57j~	A man plays the violin. A man is playing violin.	-∏4-	2.41	5:12/5	5.00/58	It is simply the number ofballs bowled divided by the number of wickets taken. Bowling strike rate is defined for a bowler as the average number of balls bowled per wicket taken.	0.80	1.84	3.83/5	4:40/59	Police helicopter crashes into pub in Glasgow - several casualties Helicopter crashes into roof of Glasgow club	0.47	1.36	3.58/5	4.00/5~T0~	Oil falls in Asian trade Oil prices down in Asian trade	-162-	3.14	4:89/5	5.00/5Tr	A skateboarder jumps off the stairs. A dog jumps off the stairs.	-3:21-	3.77	1:09/5	0.80/512	Wigan 3-2 Wolves: Match report, pictures & video highlights Arsenal 0-0 Chelsea: Match report, pictures & video highlights	3.39	3.26	0:58/5	1.20/5In table 6 we see examples of many different error cases and ,in most sentences, we also have morethan one cause for the drastic difference between BLEU/ROUGE and the label. For instance, in rows1 and 6 we see that the cause for the error is the reordering of sub-sentences, spelling/punctuationand newly introduced words that don’t change the meaning but merely extend it. While BLEU and
Table 7: BLEU*/ROUGE* and RoBERTa-STS scores on sentence pair examples from STS-BId	Sentence pair	BLEU*	ROUGE*	RoBERTa-STS	Label1	It would be unusual for a snake to attack a Station- ary person. I’m no herpetologist, but in my experience, snakes are in the ”you don’t bug me, I won’t bug you” Category.	0.34	0.00	1.4/5	4.20/52	New UN peacekeeping chief named for Central African Republic UN takes over peacekeeping in Central African Republic	1.16	2.39	3.69/5	2.00/53	From Broadway comedies like "The Seven Year Itch” (1952), ”Will Success Spoil Rock Hunter?” Playwright George Axelrod, who anticipated the sexual revolution with The Seven Year Itch and Will Success Spoil Rock Hunter?	2.03	1.31	3.16/5	2.00/54	a group of navy seals are singing A group of military personnel are playing in a brass quintet.	0.40	1.45	0.75/5	2.40/5In the above examples, we will find two points that will helps us better understand the RoBERTa-STS as a neural evaluator. Firstly, we see that the neural network sometimes lacks a sense of contextthat is not given in the sentence explicitly. While these language models are trained on a large corpusand capture a sense of the words and language, we still see that their performance is not perfect. Wesee these examples in row 4, where the model cannot relate a navy seal as a military personnel. Oras in row 1, where the model cannot model an idiom.
Table 8: Average error of BLEU* and RoBERTa-STS in the their low scoring sets. With rowscorresponding to which models failure cases and the columns to which model is used to score	BLEU*	RoBERTa-STSBLEU*	2.93/5	047/5RoBERTa-STF	1.68/5	0.89/5 一9Under review as a conference paper at ICLR 2020We see in table 8 that BLEU* has a remarkable error in both its failure cases and also the failurecases of RoBERTa-STS while RoBERTa-STS outperforms BLEU* in each category.
