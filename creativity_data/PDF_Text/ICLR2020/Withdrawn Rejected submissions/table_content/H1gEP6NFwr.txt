Table 1: Experimental settings shown in the original papers of popular optimizers. The largedifferences in test problems and tuning methods make them difficult to compare. γ denotes learningrate, μ denotes momentum, λ is the weight decay coefficient.
Table 2: Optimizers evaluated. For each hyperparameter, we calibrated a ‘sampling distribution’ togive good results across tasks (Section 3.2). U [a, b] is the continuous uniform distribution on [a, b].
Table 3: Models and datasets used. We use the DeepOBS benchmark set (Schneider et al., 2019). Details are provided in Appendix A.		Table 4: Optimizers and tunable parameters. Y is learning rate, μ is momentum, λ is the weight decay coefficient.	Architecture	Datasets	Optimizer	Tunable parametersConvolutional net Variational autoencoder Wide residual network Character RNN Quadratic function LSTM	FMNIST, CIFAR10/100 FMNIST, MNIST SVHN Tolstoi’s War and Peace Artificial datatset IMDb	SGD SGDM SGDMC SGDMCWC SGDMW	Y (μ=0, λ=0) γ,μ (λ=0) γ (μ=0.9, λ=0) γ (μ=0.9, λ=10-5) Y, μ, λ		Adagrad AdamLR Adam	γ Y (β1=0.9, β2=0.999, =10-8) Y,β1,β2,e				4	Experiments and resultsTo assess the tunability of optimizers’ hyperparameters for the training of deep neural networks, webenchmark using the open-source suite DeepOB S (Schneider et al., 2019). The architectures anddatasets we experiment are given in Table 3. We refer the reader to Schneider et al. (2019) for specificdetails of the architectures. To obtain a better balance between vision and NLP applications, weadded an LSTM network with the task of sentiment classification in the IMDB dataset (Maas et al.,2011), details for which are provided in Appendix A.
Table 5: Performance of various experiments.											Optimizer	CPE	CPU	CPL	Optimizer	CPE	CPU	CPL	Optimizer	CPE	CPU	CPLAdagrad	91.3	91.4	91.6	Adagrad	76.4	77.1	77.9	Adagrad	30.4	31.8	33.1Adam	91.3	91.5	91.8	Adam	77.2	78.4	79.5	Adam	39.4	42.2	45.1AdamLR	91.3	91.6	91.9	AdamLR	78.8	79.4	80.0	AdamLR	42.2	43.0	43.8SGD	90.4	90.8	91.2	SGD	77.0	77.8	78.6	SGD	31.8	34.2	36.6SGDM	90.5	90.9	91.3	SGDM	77.8	78.6	79.5	SGDM	40.6	43.3	46.0SGDMC	90.7	90.9	91.1	SGDMC	78.6	79.4	80.1	SGDMC	42.1	43.3	44.5SGDMCWC	90.7	90.9	91.1	SGDMCWC	81.1	81.6	82.0	SGDMCWC	39.2	40.3	41.5SGDMW	90.4	90.8	91.3	SGDMW	79.7	80.4	81.2	SGDMW	33.5	37.2	41.05.a: FMNIST 2C2D.		Higher is better		5.b: CIFAR 10. Higher is better				5.c: CIFAR 100. Higher the better			Optimizer	CPE	CPU	CPL	Optimizer	CPE	CPU	CPL	Optimizer	CPE	CPU	CPLAdagrad	843	84.8	85.3	Adagrad	948	94.9	95.0	Adagrad	556	56.2	56.7Adam	83.6	84.5	85.5	Adam	94.5	94.8	95.2	Adam	54.4	55.7	57.0AdamLR	85.8	86.0	86.3	AdamLR	95.1	95.3	95.4	AdamLR	56.9	57.2	57.5SGD	68.1	69.3	70.5	SGD	94.6	94.9	95.2	SGD	40.3	42.5	44.6SGDM	74.3	75.9	77.5	SGDM	94.8	95.2	95.6	SGDM	51.4	54.0	56.5SGDMC	79.3	80.1	81.0	SGDMC	94.9	95.1	95.3	SGDMC	55.6	57.0	58.3SGDMCWC	78.8	79.4	80.0	SGDMCWC	95.2	95.4	95.5	SGDMCWC	54.2	55.6	57.0SGDMW	75.7	77.1	78.6	SGDMW	95.0	95.2	95.3	SGDMW	45.1	48.2	51.2
Table 6: Architecture of the LSTM network used for IMDb experimentsLayer name ∣	DescriptionEmbEmbedding LayerVocabulary of 10000Embedding dimension: 32LSTMLSTM_1	Input size: 32Hidden dimension: 128FCLayer	Linear(128 → 2)Classifier ∣	Softmax(2)B α- TUNAB ILITYWe provide additional methods to analyze tunability here. Let p(t) denote the best performanceobserved after using budget t of hyperparameter optimization algorithm. We call an optimizerα-tunable (α ∈ [0,1]) at t if p(t) ≥ α ∙ P(T). Thus α-tunability is the ratio of number of timesthe neural network needs to be retrained with optimizer’s hyperparameters being provided by anautomatic method, to the total budget T (maximum number of configurations tested).
Table 7: Sharpness for various optimizers examined.
Table 8: ωepoch -tunability performance of optimizers on the classification tasks for CPE, CPU, andCPL weighting schemes. We additionally provide the average number of epochs required by eachoptimizer for a single configuration.
