Table  1:  Table  summarizes  the  top-1  accuracy  and  the  number  of  parameters  of  different  K  onCIFAR100 for VGG-16 and ResNet-50 with GAP+FC and 1x1Conv. K is defined as the number ofmodified layers counting from the last layer.  The first column for each modification method showsthe most compressed model within 1% accuracy difference to the corresponding baseline model andthe second column presents the best performed model for each modification method.  We can seethat 1x1Conv gives even a slightly higher test accuracy while having fewer parameters.
Table 2:  Left:  Top-1 accuracy of VGG-16 with random shuffle enabled at either training and testtime for the last 3 layers. Shuffled model is robust to the standard test scheme while the test accuracyof a standard VGG-16 drops to the random guess level if evaluated with shuffling. Right: Effect ofdata augmentation on classification results for ResNet-50 on CIFAR100.  The data augmentationhere is the random flipping and the random cropping. We present here the best performed model foreach method. We can see that modified models reach higher test accuracy when data augmentationis  not  applied.   ResNet-50  with  1x1Conv  trained  without  data  augmentation  shows  a  significantperformance improvement over the baseline from 65.64% to 73.65% on CIFAR100.
Table 3:  Image classification results on Small-ImageNet for VGG16 and ResNet50 with GAP+FCand 1x1Conv. K is defined as the number of modified layers counting from the last layer.
Table  4:  ImageNet  classification results  for  ResNet-152,  ResNet-50,  VGG-16  MobilenetV2  andSqueezeNet with 1x1Conv.  The best performed models are selected for 1x1Conv.  We observe thatour modification reduces the number of parameter without loss of the test accuracy.
