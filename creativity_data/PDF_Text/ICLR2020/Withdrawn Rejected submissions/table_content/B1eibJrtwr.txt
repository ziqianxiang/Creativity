Table 1: Automatic evaluation results on MultiWOZ. We use Pointer-Generator as the base modeland gradually add different semantic scaffolds.
Table 2: An example dialog and Pointer-Generator, SPNet and ground truth summaries. We un-derline semantic slots in the conversation. Red denotes incorrect slot values and green denotes thecorrect ones.
Table 3: The upper is the scoring part and the lower is the the ranking part. SPNet outperformsPointer-Generator in all three human evaluation metrics and the differences are significant, with theconfidence over 99.5% in student t test. In the ranking part, the percentage of each choice is shownin decimal. Win, lose and tie refer to the state of the former summary in ranking.
Table 4:	Supplement to the case in Table 2. The summary generated by Transformer is shown insupplement summary. Red denotes incorrect slot values and green denotes the correct ones. HumanEvaluation part provides the evaluatorâ€™s choice and feedback in ranking summary pairs. Content inthe brackets is not shown to the evaluators.
Table 5:	An example dialog and Pointer-Generator, SPNet and ground truth summaries. The dialogspans over three domains: restaurant, hotel and taxi. We underline semantic slots in the conversation.
Table 6:	An example dialog and Pointer-Generator, SPNet and ground truth summaries. The dialogspans over one domain: restaurant. We underline semantic slots in the conversation. Red denotesincorrect slot values and green denotes the correct ones. Blue denotes the content not covered byground truth in SPNet,s summary.
