Table 1: Word translation retrieval P@1 for various language pairs of MUSE evaluation dic-tionary (Conneau et al., 2017). NN: nearest neighbours. CSLS: Cross-Domain Similarity LocalScaling. (‘en’ is English, ‘fr’ is French, ‘de’ is German, ‘ru’ is Russian, ‘it’ is Italian) (‘uni.’ and‘bi.’ denote unigrams and bigrams respectively) (→ denotes translation from the first language tothe second and J the other way around.)4.2	Monolingual Word Representation QualityWe assess the monolingual quality improvement of our proposed cross-lingual training by evaluatingperformance on monolingual word similarity tasks. To disentangle the specific contribution of thecross-lingual loss, we train the monolingual counterpart of Bi-Sent2vec, Sent2vec on the samecorpora as our method.
Table 2: Monolingual word similarity task performance of our methods when trained on en-itParaCrawl data. We report Pearson correlation scores.
Table 3: Cross-lingual Sentence retrieval. We report P@1 scores for 2000 source queries searchingover 200 000 target sentences. Reduction in error is calculated with respect to Bi-Sent2vec uni.
Table 4: Cross-lingual Word and Sentence retrieval for dis-similar language pairs (P@1scores). ‘en’ is English, ‘fi’ is Finnish, ‘hu’ is Hungarianuse a zero-shot setting, i.e., we train a classifier on embeddings in the source language, and reportthe accuracy of the same classifier applied to the target language. As the classifier, we use a simplefeed-forward neural network with two hidden layers of size 10 and 8 respectively, optimized usingthe Adam optimizer. Each document is represented using the sum of its sentence embeddings.
Table 5: MLDoc Benchmark results (Schwenk & Li, 2018). A document classifier was trained onone language and tested on another without additional training/fine-tuning. We report % accuracy.
Table 6: Dataset Sizes. ‘En’,‘De’,‘Fi’,‘Fr’,‘Hu’,‘It’,‘Es’ and ‘Ru’ stand for English, German,Finnish, French, Hungarian, Italian, Spanish and Russian respectively.
Table 7: Hyperparameters for the trained models13Under review as a conference paper at ICLR 2020A.3 Additional monolingual Quality TablesMethod\Dataset	SimLex-999 en	WS en	-353 esMUSE	0.38	0.74	0.61RCSLS	0.38	0.74	0.62FastText- Common Crawl	0.49	0.75	0.54BIVEC	0.40	0.72	0.57TransGram	0.42	0.74	0.59Sent 2vec uni.	0.49	0.58	0.51Bi-Sent2vec uni.	0.57	0.78	0.60Bi-Sent2vec uni. + bi.	0.60	0.82	0.66Table 8: Monolingual word similarity task performance of our methods when trained on en-esParaCrawl data. We report Pearson correlation scores.
Table 8: Monolingual word similarity task performance of our methods when trained on en-esParaCrawl data. We report Pearson correlation scores.
Table 9: Monolingual word similarity task performance of our methods when trained on en-frParaCrawl data. We report Pearson correlation scores.
Table 10: Monolingual word similarity task performance of our methods when trained on en-de ParaCrawl data. We report Pearson correlation scores.
