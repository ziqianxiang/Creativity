Table 1: Estimated O(N2) and O(N) activationmemory for BERT and BlockBert.
Table 2: Pre-training Performance Analysis.
Table 3: Dev set results on SQuAD 1.1/2.0. The result of XLNet(-Base) is from (Yang et al., 2019).
Table 4: MrQA test results (Tasks are sorted decreasingly by average paragraph length).
Table 5: BERT notations.	Table 6: Pre-training hyper-		parameters.	A.2 Profiler ImplementationAmong the three types of training memory, model memory and optimizer memory is relativelyeasy to profile (can be computed by enumerate each tenor and summing up tensor.numel()* tensor.element_size()). To calculate activation memory, Sohoni et al. (2019) traversePyTorch’s autograd graph and sum up necessary storage space. They find that the summation ofmodel memory, optimizer memory and activation memory matches PyTorch memory profiling tooltorch.cuda.max_memory_allocated. Based on their observation, We usetorch.cuda.max_memory_allocated — model memory — optimizer memory	(4)as an estimate to activation memory. When profiling BERT, We first pre-train it for 1000 steps, andthen compute its model and optimizer memory. Finally, We esitmate its activation memory accordingto Equation 4.
Table 7: Test time statistics (sec) for different input size (Batch size × Sequence length). OOMindicates out-of-memory.
