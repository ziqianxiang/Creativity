Table 1: Performance of the proposed method compared to eight other continual learning methods onMNIST. SGD with dropout, EWC, online EWC and SI suffer from catastrophic forgetting on SetA after seeing Set B, but were able to adapt well to the new task (Set B). DGR, DGR + distillationand RtF forget more on Set A compared to our method. LwF retains knowledge learned on Set A;however, it fails to adapt to Set B. We also note that all baseline methods assume knowledge of taskboundaries, whereas the proposed method does not.
Table 2: Performance of the proposed method compared to eight other continual learning methods onCIFAR-10. SGD with dropout, EWC, online EWC and SI suffer from catastrophic forgetting on SetA after seeing Set B, but were able to adapt well to the new task (Set B). DGR, DGR + distillationand RtF also forget on Set A, this could relate to the fact that CIFAR-10 data is more complex and itis difficult for these methods to train a generative model for replaying. LwF retains some knowledgelearned on Set A; however, it fails to adapt to Set B compared to our method.
Table 3: Performance of the proposed method compared to eight other continual learning methodson 5-task CIFAR. SGD with dropout, EWC, online EWC and SI suffer from catastrophic forgettingon previous tasks after seeing newer tasks, but were able to adapt well to the last task. DGR, DGR+ distillation and RtF forget more on the previous tasks compared to our method. LwF retainsknowledge learned on first set; however, it fails to adapt to newer tasks.
Table 4: Performance comparison between our method and the vanilla baseline on ImageNet. Perfor-mance evaluated as top-1 percent correct on test data for the original task. Baseline represents thenetwork trained with cross-entropy loss. “Original” denotes the network after being trained on theoriginal task (classes in Set A) and “After Seeing Set B” denotes the network after it is fine-tunedon the new task (classes in Set B). See corresponding sections for more details on the dataset andnetwork used for each row.
Table 5: Test accuracy under the original hyperparameter setting and the change in test accuracyunder different hyperparameter settings relative to the original setting. Increasing the number ofpositive examples (from 5 to 30) slightly improves performance. Decreasing the number of negativeexamples (from 40 to 10) slightly reduces test accuracy. Increasing the embedding dimension (from100 to 200) results in similar performance as the original setting, whereas decreasing the embeddingdimension (from 100 to 50) moderately lowers the performance. Overall, the method is fairly robustto hyperparameter changes.
Table 6: Test accuracy with all components, and changes in test accuracy relative to the originalproposed method after various components are removed. Not normalizing the embedding outputdramatically lowers test accuracy, and especially limits the ability to adapt to the new task (Set B).
