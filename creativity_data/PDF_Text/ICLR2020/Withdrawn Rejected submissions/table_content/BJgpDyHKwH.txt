Table 1: Testing error (%) of differentCoMHE variants on CIFAR-100.
Table 2: Error (%) on CIFAR-100 under different dimension of projection.
Table 3: Error (%) on CIFAR-100 under different numbers of projections.
Table 4: Error (%) on CIFAR-100 with different network width.
Table 5: Error on CIFAR-100 with different network depth.
Table 7: Error (%) on CIFAR-10/100 using ResNets.
Table 8: Top-1 center crop error (%) on ImageNet-2012.
Table 9: Testing accuracy (%) on ModelNet-40.
Table 10: Our plain CNN architectures with different convolutional layers. Conv1.x, Conv2.x and Conv3.xdenote convolution units that may contain multiple convolution layers. E.g., [3×3, 64]×3 denotes 3 cascadedconvolution layers with 64 filters of size 3×3.
Table 11: Our ResNet architectures with different convolutional layers. Conv0.x, Conv1.x, Conv2.x, Conv3.xand Conv4.x denote convolution units that may contain multiple convolutional layers, and residual units areshown in double-column brackets. Conv1.x, Conv2.x and Conv3.x usually operate on different size featuremaps. These networks are essentially the same as [1], but some may have a different number of filters ineach layer. The downsampling is performed by convolutions with a stride of 2. E.g., [3×3, 64]×4 denotes 4cascaded convolution layers with 64 filters of size 3×3, and S2 denotes stride 2.
Table 12: Training Runtime (s / 100 iterations) comparison on CIFAR-100.
