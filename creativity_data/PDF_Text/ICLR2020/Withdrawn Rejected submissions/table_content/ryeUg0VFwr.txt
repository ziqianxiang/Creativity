Table 1: Median normalized best average eval-uation scores (averaged over 5 runs) across 60stochastic Atari 2600 games, measured as percent-ages and number of games where an offline agentachieves better scores than a fully trained onlineDQN (Nature) agent.
Table 2: The hyperparameters used by the offline and online RL agents in our experiments.
Table 3: Median normalized scores (Section A.3) across stochastic version of 60 Atari 2600 games, measured aspercentages and number of games where an agent achieves better scores than a fully trained online DQN (Nature)agent. All the offline agents below are trained using the DQN replay dataset. The entries of the table without anysuffix report training results with the five times as many gradient steps as online DQN while the entires withsuffix (1x) indicates the same number of gradient steps as the online DQN agent. All the offline agents exceptDQN use the same multi-head architecture as QR-DQN.
