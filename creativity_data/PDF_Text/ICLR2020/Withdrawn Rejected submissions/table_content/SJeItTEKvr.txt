Table 1: Statistics of Datasets for the simple example ofBiRname	domain	instances	nominal	numeric	labelsscene	image	-^2407^^	0	294	6corel5k	images	5000	499	0	374delicious	text (web)	16105	500	0	983EUR-Lex(dc)	text	19348	0	5000	412EUR-Lex(ed)	text	19348	0	5000	3993For the simple implement example, we closely follow the experimental setup in Liu et al. (2018).
Table 2: The Results of Micro-F1 on the Feature Data Sets (Mean ± Standard Deviation)Dataset	PLST	MMOC	kNN	ML-kNN	LMMO-kNN	BiRscene	0.5924±0.03=	0.7297±0.03	0.7221±0.02=	0.7387±0.03	0.7300±0.03	0.807789±0.025169corel5K	0.0801±0.01	-	0.1006±0.01	0.0278±0.01	0.1834±0.02	0.343582±0.015272delicious	0.1423±0.01	-	0.2338±0.01	0.1738±0.01	0.3046±0.01	0.249541±0.012315EUR-Lex (dc)	0.2304±0.03	-	0.6988 ±0.01	0.6705±0.01	0.7388±0.01	0.684401±0.021701EUR-Lex (edT	0.1059±0.0Γ-	-	0.4664±0.0Γ-	0.3864±0.01	0.4706±0.02	0.53867±0.031739Table 3: The Results of Example-F1 on the Feature Data Sets (Mean ± Standard Deviation)Dataset	PLST	MMOC	kNN	ML-kNN	LMMO-kNN	BiRscene	0.4588±0.Of=	0.7025±0.03	0.6815±0.02z	0.6874±0.03	0.7101±0.03	0.804263±0.025516corel5K	0.0587±0.01	-	0.0773±0.01	0.0178±0.01	0.1517±0.01	0.195901±0.013047delicious	0.1131±0.00	-	0.2094±0.01	0.1518±0.00	0.2665±0.00	0.253124±0.002821dEUR-Lex (dc)	0.1530±0.02	-	0.6569±0.01	0.6038±0.01	0.7230±0.01	0.678230±0.010376EUR-Lex (ed)"	0.0725±0.0Γ-	-	0.4207±0.0Γ~	0.3385±0.00	0.4630±0.00	0.395483±0.012076Results of the simple example of BiR are summarized in Table 2 and 3. For the feature input dataset,we report the Micro-F1 and example-based-F1 with the 10-fold validation on each dataset. Thereason with using these measures was analyzed in Mao et al. (2012). Result for all other baselinemethod are taken from the paper Liu et al. (2018).
Table 3: The Results of Example-F1 on the Feature Data Sets (Mean ± Standard Deviation)Dataset	PLST	MMOC	kNN	ML-kNN	LMMO-kNN	BiRscene	0.4588±0.Of=	0.7025±0.03	0.6815±0.02z	0.6874±0.03	0.7101±0.03	0.804263±0.025516corel5K	0.0587±0.01	-	0.0773±0.01	0.0178±0.01	0.1517±0.01	0.195901±0.013047delicious	0.1131±0.00	-	0.2094±0.01	0.1518±0.00	0.2665±0.00	0.253124±0.002821dEUR-Lex (dc)	0.1530±0.02	-	0.6569±0.01	0.6038±0.01	0.7230±0.01	0.678230±0.010376EUR-Lex (ed)"	0.0725±0.0Γ-	-	0.4207±0.0Γ~	0.3385±0.00	0.4630±0.00	0.395483±0.012076Results of the simple example of BiR are summarized in Table 2 and 3. For the feature input dataset,we report the Micro-F1 and example-based-F1 with the 10-fold validation on each dataset. Thereason with using these measures was analyzed in Mao et al. (2012). Result for all other baselinemethod are taken from the paper Liu et al. (2018).
Table 4: The Predicive accuracy On MUlti-Label image data set scene 2000 with 5 labelslabels∖methods	MIML	AttCNN	BiRDNNdesert	一	0.869±0.0If=	0.902142±0.013688	0.912812±0.018705mountains	0.791±0.024	0.843571±0.024917	0.877187±0.018436sea	0.729±0.026	0.713571±0.038502	0.792812±0.020572sunset	0.864±0.033	0.943571±0.010271	0.951562±0.019469trees	0.801±0.015	0.787142±0.018058	0.879687±0.026612overall accuracy	0.811±0.022	0.827514±0.011797	0.882812±0.010703average precision	0.783±0.020-	0.850935±0.024998	0.862063±0.034688We compare deep natural network implementation out model (BiRDNN) with AttCNN Hand et al.
