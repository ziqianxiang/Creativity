Table 1: Classification error using retrospection on F-MNIST, SVHN and CIFAR-10 datasetResNet - 44, 56, 110 (He et al., 2016). Models in each experiment are trained for 200 epochs, usingthe training configuration (mini-batch, lr policy) detailed in (He et al., 2016). Here, we observe that4Under review as a conference paper at ICLR 2020using the retrospection loss in later stages of training results in best improvement in performance.
Table 3: Performance on using retrospection on task of dyadic emo-tion recognition on DialogueRNNenty five epochs produces best performance. For experiments on both IEMOCAP and AVEC, theretrospective update frequency is one epoch. The loss scaling margin, κ, is set to 4 at initializationand is updated by 2% at each retrospective update. Experiments are conducted using the official6Under review as a conference paper at ICLR 2020code repository (Co, 2019). Results in Table 3 show that using the retrospection loss when trainingDialogueRNN improves performance on both IECOMAP and AVEC datasets.
Table 5: Classification performance using retrospection loss on graph neural networksB Robustness of ResultsTo ensure consistency of comparison, we reported performance in the main paper by initializing bothretrospective and original experiments with the same weights. Now, for comprehensive analysis, wereport experimental values for Image Classification, Speech Recognition and Text Classificationtasks averaged over 10 runs. (Note that for few-shot learning, we already included this informationin the main paper.) Table 6, Table 7 and Table 8 present the corresponding mean and standarddeviation of the results for image, text and speech classification respectively. We also note that allthe results in the submitted paper are in the same range as the mean ± std in the results below,although these were separately performed - showing the consistency.
Table 6: Mean and Standard Deviation of error rates for Image ClassificationMethod	IECOMAP		AVEC		Fl-Score	Accuracy	MSE	R (Pear Score)Retrospective	64.40 ± 0.4	64.97 ± 0.5	0.1772 ± 0.0006	0.332 ± 0.008Original	62.60 ±0.9	62.70 ±0.7	0.1798 ±0.0005	0.317 ±0.007Table 7: Mean and Standard Deviation of results for Text ClassificationModel	Validation Set		Testing Set		Original	Retrospective	Original	RetrospectiveLeNet	9.77 ±0.05	9.60 ± 0.03	10.26 ±0.05	9.86 ± 0.04VGG-11	5.15 ±0.08 一	4.37 ± 0.04~	5.03 ±0.06 一	4.16 ± 0.05-Table 8: Mean and Standard Deviation of error rates for Speech RecognitionC Ablation Study: MomentumWe also conducted an additional study to analyse the impact of choice of momentum values inthe optimizer when using the retrospection loss in training. As in other ablation studies, we trainLeNet for image classification on F-MNIST using SGD and experiment with different values of12Under review as a conference paper at ICLR 2020the momentum parameter (0.5, 0.7, 0.9). The other parameter configurations remain the same asinitially presented in Section 4.1 (lr=0.1, batch_size=32). As highlighted by results in Table 9, theretrospection loss is independent of momentum value since retrospective training results in better
Table 7: Mean and Standard Deviation of results for Text ClassificationModel	Validation Set		Testing Set		Original	Retrospective	Original	RetrospectiveLeNet	9.77 ±0.05	9.60 ± 0.03	10.26 ±0.05	9.86 ± 0.04VGG-11	5.15 ±0.08 一	4.37 ± 0.04~	5.03 ±0.06 一	4.16 ± 0.05-Table 8: Mean and Standard Deviation of error rates for Speech RecognitionC Ablation Study: MomentumWe also conducted an additional study to analyse the impact of choice of momentum values inthe optimizer when using the retrospection loss in training. As in other ablation studies, we trainLeNet for image classification on F-MNIST using SGD and experiment with different values of12Under review as a conference paper at ICLR 2020the momentum parameter (0.5, 0.7, 0.9). The other parameter configurations remain the same asinitially presented in Section 4.1 (lr=0.1, batch_size=32). As highlighted by results in Table 9, theretrospection loss is independent of momentum value since retrospective training results in betterperformance than original training (w/o retrospection) for all the different momentum values.
Table 8: Mean and Standard Deviation of error rates for Speech RecognitionC Ablation Study: MomentumWe also conducted an additional study to analyse the impact of choice of momentum values inthe optimizer when using the retrospection loss in training. As in other ablation studies, we trainLeNet for image classification on F-MNIST using SGD and experiment with different values of12Under review as a conference paper at ICLR 2020the momentum parameter (0.5, 0.7, 0.9). The other parameter configurations remain the same asinitially presented in Section 4.1 (lr=0.1, batch_size=32). As highlighted by results in Table 9, theretrospection loss is independent of momentum value since retrospective training results in betterperformance than original training (w/o retrospection) for all the different momentum values.
Table 9: Impact of choice of momentum value on retrospection lossD Ablation Study: Warm-up PeriodAs in other ablation studies, we train LeNet on the task of FMNIST (60,000 images) image clas-sification for 70,000 iterations with batch_size = 32 using SGD (mom=0.9). The error rates withdifferent warm-ups are presented in Table 10. We observed that on simpler datasets (like FMNIST)since networks start at a reasonable accuracy, retrospection is effective even when we introduce itwith a very low warm-up period (Iw = 0).
Table 10: For simple datasets like FMNIST, the retrospection loss is effective even when we intro-duce it with a low warm-up period. All different warm-up configurations improve performance overbaseline.
Table 11: All the different warm-up configurations improve over baseline trained without retrospec-tion loss. A higher warm-up period results in better performance when more complex datasets aretrained using large networks.
Table 12: Error rates on using retrospection loss with different norms for Image Classification onF-MNIST dataset.
Table 13: Error rates on using retrospection loss with different norms for Image Classification onSVHN dataset.
