Table 1: Comparison of MaskConvNet-F or MaskConvNet-P with state-of-the-art pruning approachesNetwork Slimming Liu et al. (2017), L1-Pruning Li et al. (2016) and BAR Lemaire et al. (2019), onCIFAR-10 with various ConvNet architectures.
Table 2: Comparison of MaskConvNet-FP with both parameter counts and FLOP as target metricswith Li et al. (2016) on ImageNet ILSVRC 2012 with ResNet-34 as baseline architecture.
Table 3: Hyper-parameters for Single-metric experimentsModel	VGG-19 Target P	Target F	WideResnet-28-10 Target P	Target Fbase λm	1	5	base 3, 5 λv	56	1,2*	5		Table 4: Hyper-parameters for Multi-metric experiments	MaskConvNet-P (λbmase, λbvase, warmup)	MaskConvNet-F (λ警se, λbase, WarmuP)	MaskConvNet-PF (λbma1se,λbva1se, λbma2se,λbma2se, WarmuP)VGG-16	3,4, 20	5, 6, 20	3, 4, 5, 6, 20VGG-19	3,4,20	3,4,20	3, 4, 3, 4, 20Resnet-56	2, 3, 50	1,2,50	2,3, 1,2,50Resnet-164	1,5, 10	1,5,10	1,5, 1,5,10Resnet-34	—	一	5, 20, 5, 20, 3A.2 Mask HistogramAs shown in Figure 5, the mask values accumulate mostly at 0, then at 1. This observation is differentcompared to findings by Gordon et al. (2018), where the batch-norm scale parameter histogramresembles Gaussian distribution.
Table 4: Hyper-parameters for Multi-metric experiments	MaskConvNet-P (λbmase, λbvase, warmup)	MaskConvNet-F (λ警se, λbase, WarmuP)	MaskConvNet-PF (λbma1se,λbva1se, λbma2se,λbma2se, WarmuP)VGG-16	3,4, 20	5, 6, 20	3, 4, 5, 6, 20VGG-19	3,4,20	3,4,20	3, 4, 3, 4, 20Resnet-56	2, 3, 50	1,2,50	2,3, 1,2,50Resnet-164	1,5, 10	1,5,10	1,5, 1,5,10Resnet-34	—	一	5, 20, 5, 20, 3A.2 Mask HistogramAs shown in Figure 5, the mask values accumulate mostly at 0, then at 1. This observation is differentcompared to findings by Gordon et al. (2018), where the batch-norm scale parameter histogramresembles Gaussian distribution.
Table 5: Sensitivity analysis for base regularization multipliers λbmase and λbvase with VGG-19 trainedon CIFAR-10 with target FLOP sparsity 50%base λm	base λv	Accuracy (%)	FLOPs sparsity (%)	Parameter sparsity (%)1	3	92.43	49.10	91.123	9	92.10	53.48	92.937	4	91.96	50.13	91.282	2	92.02	46.93	89.225	9	91.75	50.03	91.364	6	92.01	52.30	92.501	10	92.33	44.72	87.565	7	92.50	52.66	92.779	10	92.15	49.63	90.9010	2	92.43	51.73	92.15	mean ± std	92.17 ± 0.25	50.07 ± 2.70	91.18 ± 1.68Table 6: Latency Measurements on CIFAR-10Model	Target FLOP sparsity (%)	Actual FLOP sparsity (%)	Latency (ms)VGG-19	0 (Baseline)	0.00	22.45 ± 1.17	50	51.91 ± 2.46	12.94 ± 0.42	60	62.35 ± 2.92	11.42 ± 0.28	70	69.52 ± 0.01	10.81 ± 0.06
Table 6: Latency Measurements on CIFAR-10Model	Target FLOP sparsity (%)	Actual FLOP sparsity (%)	Latency (ms)VGG-19	0 (Baseline)	0.00	22.45 ± 1.17	50	51.91 ± 2.46	12.94 ± 0.42	60	62.35 ± 2.92	11.42 ± 0.28	70	69.52 ± 0.01	10.81 ± 0.06WideResnet-28-10	0 (Baseline)	0.00	64.15 ± 0.21	50	54.59 ± 1.5	25.43 ± 1.09	60	58.54 ± 2.5	22.79 ± 0.94	70	72.58 ± 4.2	19.92 ± 0.93A.5 Effect of Mask Decay and WarmupFigure 6 shows different mask decays give different rate of unsparsification. Also with warmuptraining (late sparsification), accuracy is improved and the pruned structure becomes differentcompared to the one without warmup - High contrast is visible between sparsity level between lastlayers and early layers. Last layers are sparsified more.
