Figure 1: Overview of pruning BERT using Reweighted Proximal Pruning algorithm and then fine-tuning ona wide range of downstream transfer learning tasks. Through RPP, We find the identical universal sparsity Sw.
Figure 2: Evaluate the performance of pruned BERTBASE using NIP and RPP, respectively (MLM and NSPaccuracy on pre-training data and F1 score of fine-tuning on SQuAD 1.1 are reported).
Figure 3: Visualization of sparse pattern S in pruned BERTBASE model w. We sample 6 matrices (3 querymatrices at the top row and 3 key matrices at the bottom row) from layer 2, layer 3 and layer 11 in the sparestpruned BERTBASE .
Figure 4: t-SNE visualization of word embeddings in the original BERT model and the pruned BERT modelusing RPP. From left to right: t-SNE of original BERT embedding, together with an enlarging region aroundword “intelligent”; t-SNE of embedding in pruned BERT, together with an enlarging region. These visualiza-tions are obtained by running t-SNE for 1000 steps with perplexity=100.
Figure A1: Evaluate the performance of pruned BERTBASE using NIP and RPP, respectively (MLM and NSPaccuracy on pre-training data and F1 score of fine-tuning on QQP are reported).
Figure A2: Evaluate the performance of pruned BERTBASE using NIP and RPP, respectively (MLM and NSPaccuracy on pre-training data and F1 score of fine-tuning on MRPC are reported).
Figure A3: Evaluate the performance of pruned BERTBASE using NIP and RPP, respectively (MLM and NSPaccuracy on pre-training data and accuracy of fine-tuning on MNLI are reported).
Figure A4: Evaluate the performance of pruned BERTBASE using NIP and RPP, respectively (MLM and NSPaccuracy on pre-training data and accuracy of fine-tuning on MNLIM are reported).
Figure A5: Evaluate the performance of pruned BERTBASE using NIP and RPP, respectively (MLM and NSPaccuracy on pre-training data and accuracy of fine-tuning on QNLI are reported).
Figure A6: Evaluate the performance of pruned BERTBASE using NIP and RPP, respectively (MLM and NSPaccuracy on pre-training data and accuracy of fine-tuning on SST-2 are reported).
Figure A7: Evaluate the performance of pruned BERTBASE using NIP and RPP, respectively (MLM and NSPaccuracy on pre-training data and accuracy of fine-tuning on CoLA are reported).
Figure A8: Training loss curve of applying iterative pruning and RPP on BERTAs we mentioned in our main paper, we investigate a series of pruning techniques to prune BERT,include the iterative pruning method (Han et al., 2015) and the one-shot pruning (Liu et al., 2019b).
