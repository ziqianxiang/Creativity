Figure 1: Complex simulated character trained using advantage-weighted regression. Left: Hu-manoid performing a spinkick. Right: Dog performing a canter.
Figure 2: Snapshots of AWR policies trained on OpenAI Gym tasks. Our simple algorithm learnseffective policies for a diverse set of discrete and continuous control tasks.
Figure 3: Learning curves of the various algorithms when applied to OpenAI Gym tasks. Resultsare averaged across 5 random seeds. AWR is generally competitive with the best current methods.
Figure 4: Left: Learning curves comparing AWR with various components removed. Each com-ponent appears to contribute to improvements in performance, with the best performance achievedwhen all components are combined. Right: Learning curves comparing AWR with different ca-pacity replay buffers. AWR remains stable with large replay buffers containing primarily off-policydata from previous iterations of the algorithm.
Figure 5: Snapshots of 34 DoF humanoid and 64 DoF dog trained with AWR to imitate referencemotion recorded from real world subjects. AWR is able to learn sophisticated skills with characterswith large numbers of degrees of freedom.
Figure 6: Learning curves on motion imitationtasks. On these challenging tasks, AWR generallylearns faster than PPO and RWR.
Figure 8: Learning curves of the various algorithms when applied to OpenAI Gym tasks. Resultsare averaged over 5 random seeds. AWR is generally competitive with the best current methods.
Figure 9: Learning curves on motion imitation tasks. On these challenging tasks, AWR generallylearns faster than PPO and RWR.
Figure 10: Learning curves comparing AWR policies trained with and without weight clipping.
