Figure 1: Manifestation of error-potentials in time-domain: Grand average potentials (error-minus-correct conditions) are shown for Maze, Catch and Wobble game environments. Thick black linedenotes the average over all the subjects. The game environments are explained in section2	Definitions and PreliminariesConsider a Markov Decision Process (MDP) problem M, as a tuple < X , A, P, P0, R, γ >, withstate-space X, action-space A, transition kernel P, initial state distribution P0, accompanied withreward function R, and discounting factor 0 ≤ γ ≤ 1. Here the random variable Z(s, a) denotesthe accumulated discounted future rewards starting from state s and action a.
Figure 2: Integrating DRL with Implicit Human Feedbacklearn a reward function to augment the DRL algorithms and accelerate the training of RL agent.
Figure 3: Experimental frameworkO IOO 200	300	400Episode(a) Learning Curve300-α,pos-d 山α,*j,u-dE0u⅛IU---1---1-1---1----1--1---No ErrP	Ol	02	03	04	05Subject(b) Complete EpisodeFigure 4: RL with full access to ErrP feedback.
Figure 4: RL with full access to ErrP feedback.
Figure 5: Detection performance and generalizability of ErrP: (a) 10-fold CV performance of eachgame i.e., no generalization, (b) generalizability from Catch to Maze over subjects compared with10-fold CV, (c) generalizability over all combinations of three games compared with 10-fold CV.
Figure 6:	Evaluation of First Framework: Training with Implicit Human Feedback in the loop.
Figure 7:	Evaluation of Second Framework: Learning from Imperfect Demonstrations Labeled byErrP. Figures (a) and (b) are for the demonstration with 10 trajectories, and figures (c) and (d) arefor 20 trajectories.
Figure 8: First Framework Evaluation on Box World Game. The number in brackets denotes theinquiry interval NE .
