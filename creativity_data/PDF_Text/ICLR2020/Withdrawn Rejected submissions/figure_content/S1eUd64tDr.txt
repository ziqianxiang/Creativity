Figure 1: Our Inter-Layer Weight Prediction (ILWP) scheme. (a) ILWP with the full search strategydescribed in Section 3.1; (b) ILWP with the local search strategy described in Section 3.2; (c) ILWPtrained with inter-layer loss and without searching described in Section 3.3. The ref index is thereferenced kernel index for ILWP, i.e. the index of the most similar weight kernel on target layer.
Figure 2: Heatmaps that show the percentage in the number of best predictions with respect tominimizing L1 distance between the source layer (x-axis) and the target layer (y-axis) in the neuralnetworks trained on CIFAR10 (top) and CIFAR100 (bottom). From left to right are MobileNet,MobileNetV2, ShuffleNet, and ShuffleNetV2where P[∙] is a probability, L(∙, ∙) is a distance between two kernels, and (u, V) are the indices of thelayer and kernel, where 1 ≤ u < i - 1 (See Figure 1).
Figure 3: Heatmaps that show the percentage in the number of best predictions with respect tominimizing L1 distance between the source layer (x-axis) and the target layer (y-axis) in the neuralnetworks trained on CIFAR10 (top) and CIFAR100 (bottom) with inter-layer loss. From left to rightare MobileNet, MobileNetV2, ShuffleNet, and ShuffleNetV2usually remains more high-valued non-zero weights than the traditional quantization methods. Sincehigh weight values importantly contribute to the prediction performance (Han et al., 2015b), ourmethod can retain both high accuracy and weight compression performance.
Figure 4: Comparison of parameter size in kilobytes (KB) and top-1 test accuracy in MobileNettrained on CIFAR-10 dataset for different λ in Eq. (3) after n-bit linear quantization. 32-bit meansusing full precision floating point.
Figure 5: Comparison of Parameter size in kilobytes (KB) for different quantization bits in{2, 3, 4, ..., 8} on CIFAR-10 (top) and CIFAR-100 (bottom) datasets. From left to right are Mo-bileNet, MobileNetV2, ShuffleNet, and ShuffleNetV2. (a) ILWP with the full search strategy(ILWP-FSS); (b) ILWP with the local search strategy (ILWP-LSS); (c) ILWP trained with inter-layer loss and without searching (ILWP-ILL) described in Section 3.3. In all the figures, each dotrepresents the average test accuracy over 5 training times in each quantization bit in {2, 3, 4, ..., 8}.
Figure 6: Performance results of our proposed methods on CIFAR-10 (top) and CIFAR-100 (bottom)datasets for the parameter size in kilobytes (KB) and test accuracy. From left to right are MobileNet,MobileNetV2, ShuffleNet, and ShuffleNetV2. x-axis: the total size of the quantized and Huffmancoded depth-wise convolutional kernels in storage; y-axis: Top-1 test accuracy. (a) ILWP-FSS; (b)ILWP-LSS; (c) ILWP-ILL. In all the figures, each dot represents the average test accuracy over 5training times in each quantization bit in {2, 3, 4, ..., 8}.
Figure 7: Comparison of the distributions for the weights and residuals in all the depth-wise convo-lution kernels in MobileNet trained on CIFAR-100. (a) Distribution of the weight kernels in baselinemodel; (b) Distribution of residuals in ILWP-FSS; (c) Distribution of the residuals in ILWP-ILL.
