Figure 1: A schematic illustration of the different architectures employed in our experiments. Quasi-Multitask Learning (Q-MTL) averages the predictions of multiple classification units similar to en-sembling without the computational bottleneck of adjusting the parameters of multiple LSTM cells.
Figure 2: The accuracy of the different model types over the training epochs on the dev set.
Figure 3:  Scatter plot comparing the accuracy of the individual classifiers from Q-MTL (k = 10)and their corresponding STL counterpart. Each model that is above the diagonal line performs betterafter training in the Q-MTL setting.
Figure 4: The average Frobenius norms of the learned parameter matrices V and W for the differentapproaches and treebanks.
Figure  5:  Model  performances  obtained  by  the  different  approaches  when  a  varying  amount  ofnoisy training samples are introduced during training.  The rightmost plot (titled Average) containsthe averaged accuracies over the 10 treebanks.
Figure 6: Training times of the different approaches for the different languages.
Figure 7: PTA and Q-MTL compared in an analogue manner. Model selection (MS) is compared tomodel averaging (MA) and MLP (20 MLP) compared to linear classifier (0 MLP). From PTA (MS@ 0 MLP) to Q-MTL (MA @ 20 MLP) we can see all combinations of these parameters.
