Figure 2: Example of an undesirable behavior of a Gaussian search distribution. The dashed linesrepresent density level lines of the search distribution. Because the latter cannot have a curvedprofile, it is forced to drastically reduce its entropy until it reaches the straight part of the valley.
Figure 5:	ECDFs curves comparing GNN-xNES and xNES on the Rotated Rosenbrock and BentCigar functions, in dimensions d=2,5,10.
Figure 6:	Scaling comparison of GNN-xNES and xNES on the Rastrigin, Griewank-Rosenbrockand Schwefel functions, d=2,5,10.
Figure 7:	Direct Policy Search experiments5.3 Reinforcement Learning experimentsThe goal of this section is to present additional comparison between xNES and GNN-xNES onRL-based objective functions - less synthetic than the previously considered BBOB functions. ESalgorithms have recently been used for direct policy search in Reinforcement Learning (RL) andshown to reach performances comparable with state-of-the-art MDP-based techniques (Liu et al.,2019; Salimans et al., 2017). Direct Policy Search ignores the MDP structure of the RL environmentand rather considers it as a black-box. The search for the optimal policy is performed directly inparameter space to maximize the average reward per trajectory:f (X)= ET〜Px X rj	(17)j∈τwhere px is the distribution of trajectories induced by the policy (the state-conditional distributionover actions) parametrized by x, and r the rewards generated by the environment. The objective(17) can readily be approximated from samples by simply rolling out M trajectories, and optimizedusing ES. In our experiments2, we set M = 10 and optimize deterministic linear policies (as inRajeswaran et al. (2017)).
Figure 8: Two-dimensional visualizations. The black dotted lines represent the isolines of the levelcurves of a NICE search distribution trained with GNN-xNES.
Figure 9: ECDFs curve for the Attractive Sector function, d=2,5,10(c) Attractive Sector, d=10(a) Rosenbrock, d=2(b) Rosenbrock, d=5	(c) Rosenbrock, d=10Figure 10: ECDFs curve for the Rosenbrock function, d=2,5,10latent space non-stationary) can lead to conflicting updates and might hinder the benefits broughtby the GNN’s additional flexibility. Designing a GNN update strategy complying with the use ofevolution paths could therefore be a way of further improving GNN-CMA-ES, and is left for futurework.
Figure 10: ECDFs curve for the Rosenbrock function, d=2,5,10latent space non-stationary) can lead to conflicting updates and might hinder the benefits broughtby the GNN’s additional flexibility. Designing a GNN update strategy complying with the use ofevolution paths could therefore be a way of further improving GNN-CMA-ES, and is left for futurework.
Figure 11: Scaling comparison of GNN-xNES and xNES on the Gallagher’s Gaussian 101 Peaksand Gallagher’s Gaussian 21 Peaks functions, d=2,5,10.
Figure 12: ECDFs curves for the Rosenbrock function, (d=2,5,10) comparing the CMA-ES andGNN-CMA-ES(a) Rosenbrock, d=2(b) Rotated Rosenbrock, d=2(c) Bent Cigar, d=2Figure 13:	ECDFs curves for xNES, GNN-xNES and GNN-xNES-no-history, for which the historysize T = 1. Using past populations to estimate expectations improves the optimization.
Figure 13:	ECDFs curves for xNES, GNN-xNES and GNN-xNES-no-history, for which the historysize T = 1. Using past populations to estimate expectations improves the optimization.
Figure 14:	ECDFs curves for xNES, GNN-xNES and GNN-xNES-nmp, which is not mode pre-serving. Ensuring that the training of the GNN doesn’t impact the mode of the search distributionimproves the optimization.
