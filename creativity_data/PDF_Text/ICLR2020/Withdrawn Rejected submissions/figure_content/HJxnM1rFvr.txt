Figure 1: General architecture for all models: HUBERT models have a TPR layer; BERT and BERT-LSTM donâ€™t. BERT and TPR layers can be shared between tasks but the classifier is task-specific.
Figure 2: TPR layer architecture for HUBERT (LSTM). R and S are global Role and Symbolembedding matrices which are learned and re-used at each time-step.
Figure 3: TPR layer architecture for HUBERT (Transformer). R and S are global Role and Symbolembeddings which are learned and shared for all token positions.
Figure 4: POS tags vs Role frequencies when selecting the two roles with highest value in theattention distribution. Subsequently the tags referring to similar grammar roles are merged into onecategory to generate better visualization.
