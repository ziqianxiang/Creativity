Figure 1: Attacked images (label “espresso maker”) against adversarially trained models with largeε. Each of the adversarial images above are optimized to maximize the classification loss.
Figure 2: Scaled pixel-level differences between original and attacked images for each attack (label“espresso maker”). The L1, L2, and L∞ norms of the difference are shown after the attack name.
Figure 3: Snow before and after optimization.
Figure 4: Accuracies ofL2 and Elastic attacks at different distortion sizes against a ResNet-50 modeladversarially trained against L2 at ε = 9600 on ImageNet-100. At small distortion sizes, the modelappears to defend well against Elastic, but large distortion sizes reveal a lack of transfer.
Figure 5:	Varying distortion size against adversarially trained models reveals full attack strength.
Figure 6:	UAR scores demonstrate the need to evaluate against diverse attacks.
Figure 7:	UAR scores for jointly adv. trained defenses (rows) against distortion types (columns).
Figure 8:	Left: train and validation curves for joint training against L∞, ε = 8 and Elastic, ε = 4,Right: train and val curves for standard adversarial training for L∞, ε = 8. The joint validationaccuracy of L∞ decreases as training progresses, indicating overfitting.
Figure 9: Differences of the attacked images and original image for different attacks (label “espressomaker”). The L1, L2, and L∞ norms of the difference are shown in parentheses. As shown, ournovel attacks display qualitatively different behavior and do not fall under the Lp threat model.
Figure 10: A comparison of L1-JPEG and L2-JPEG attacks.
Figure 11: Accuracy of adversarial attack (column) against adversarially trained model (row) on ImageNet-IOO.
Figure 12:	UAR scores (multiplied by 100) for adv. trained defenses (rows) against distortion types(columns) for ImageNet-100.
Figure 13:	Adversarial accuracies of attacks on adversarially trained models for different distortionsizes on ImageNet-100. For a given attack ε, the best ε0 to train against satisfies ε0 > ε because therandom scaling of ε0 during adversarial training ensures that a typical distortion during adversarialtraining has size smaller than ε0 .
Figure 14: Accuracy of adversarial attack (column) against adVersarially trained model (row) on CIFAR-10.
Figure 15:	UAR scores on CIFAR-10. Displayed UAR scores are multiplied by 100 for clarity.
Figure 16:	Replica of the first three block rows of Figure 11 with different random seeds. Deviationsin results are minor.
Figure 17: Replica of Figure 11 with 50 steps instead of 200 at evaluation time. Deviations in results are minor.
Figure 18: Evaluation accuracies of jointly trained models. Attack and training ε values are equal.
Figure 19: All attacks (columns) vs. jointly adversarially trained defense (rows).
Figure 20: Train and validation curves for joint training against L∞, ε = 4 and elastic, ε = 8using ResNet-101. As shown, the validation accuracies decrease as training progresses, indicatingoverfitting.
Figure 21: Accuracies of defenses (rows) on ImageNet-C-100 corruptions (columns).
Figure 22: Adversarial attacks at low distortion sizes ε against an undefended model. The pair of(ε, accuracy) is shown after the attack name. Visual differences between the original and attackedimages are imperceptible for L∞, L2, and JPEG, minor for L1, Elastic, and Gabor, and weather-related for Fog and Snow.
