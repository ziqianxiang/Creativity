Figure 1: Cumulative control performance comparison. The Y-axis indicates the total cost duringone episode and the X-axis indicates the total time steps in thousand. The shadowed region showsthe 1-SD confidence interval over 10 random seeds. Across all trials of training, LAC converges tostabilizing solution with comparable or superior performance compared with SAC and SPPO. Theexperiment on Complicated-Repressilator is deferred to Appendix F.
Figure 2: Value of Lagrange multiplier λ during the training of LAC policies. The Y-axis indicatesthe value of λ and the X-axis indicates the total time steps in thousand. The shadowed region showsthe 1-SD confidence interval over 10 random seeds. The value of λ gradually drops and becomeszero at convergence, which implies the satisfaction of stability condition.
Figure 3: State trajectories over time under Policies trained by LAC and SAC and tested in thepresence of parametric uncertainties and process noise, for CartPole and Repressilator. Solid lineindicates the average trajectory and shadowed region for the 1-SD confidence interval. In (a) and (b),the pole length is varied during the inference. In (c) and (d), three parameters are selected to reflectthe uncertainties in gene expression. The X-axis indicates the time and Y-axis shows the angle ofpole in (a,b) and concentration of protein to be controlled in (c,d), respectively. Dashed line indicatesthe reference signal. The line in orange indicates the dynamic in original environment. For eachcurve, only the noted parameter is different with the original setting. We also show the curves inseparate zoom-in view in Appendix I.1.
Figure 4: Performance of policies trained by LAC, SAC and SPPO, along with controllers designedby LQR in the presence of persistent disturbances with different magnitudes. X-axis indicates themagnitude of the applied disturbance. For CartPole (a) the Y-axis indicates the probability of fallingover and in other three examples (b)-(d) it indicates the total cost. Both policies are evaluated for 100trials in each setting.
Figure 5: State trajectories under policies trained by LAC and SAC when tracking different referencesignals. Solid line indicates the average trajectory and shadowed region for the 1-SD confidenceinterval. The X-axis indicates the time and Y-axis shows the concentration of protein to be controlled.
Figure 6: Influence of different Lyapunov function candidates and network structures. In (a) and (c),the Y-axis indicates total cost of policies during training by LAC with Lyapunov function candidatesof different length of horizon N and structures, and the X-axis indicates the total time steps inthousand. (b) and (d) shows the death-rate of policies in the presence of instant impulsive force Franging from 80 to 150 Newton.
Figure 7: Snapshot of environments using OpenAI Gym.
Figure 8: A snapshot of natural oscillatory behaviour of a repressilator system consisting of 3 genes.
Figure 9: A snapshot of natural behaviour of a repressilator system consisting of 4 genes. The X-axisdenotes time and Y-axis denotes value/concentration of each state.
Figure 10: Cumulative control performance comparison. The Y-axis indicates the total cost duringone episode and the X-axis indicates the total time steps in thousand. The shadowed region shows the1-SD confidence interval over 10 random seeds. Across all trials of training, LAC converges to stablesolution with comparable or superior performance compared with SAC.
Figure 11: State trajectories over time under policies trained by LAC and SAC in the Repressilatorand Complicated Repressilator. In each experiment, the policies are tested over 20 random initialstates and all the resulting trajectories are displayed above. The X-axis indicates the time and Y-axisshows the concentration of Protein 1.
Figure 12: State trajeCtories over time under poliCies trained by LAC and SAC in the two Markovianjump systems. In eaCh experiment, the poliCies are tested over 20 random initial states and all theresulting trajeCtories are displayed above. The X-axis indiCates the time and Y-axis shows the valueof states.
Figure 13:	State trajectories over time under policies trained by SPPO and tested in the presenceof parametric uncertainties and process noise, for CartPole and Repressilator. The setting of theuncertainty is the same as in Section 5.4.1.
Figure 14:	State trajectories under policies trained by SPPO when tracking different reference signals.
Figure 15: Zoom-in view of Figure 3 (a)Figure 16: Zoom-in view of Figure 3 (b)25Under review as a conference paper at ICLR 2020Figure 17: Zoom-in view of Figure 3 (c)26Under review as a conference paper at ICLR 2020noise IeVel=IFigure 18: Zoom-in view of Figure 3 (d)27Under review as a conference paper at ICLR 2020I.2 Zoom-in View of Figure 5Figure 19: Zoom-in view of Figure 5 (a)28Under review as a conference paper at ICLR 2020200	400	600	BOO	1000	1200Figure 20: Zoom-in view of Figure 5 (b)29Under review as a conference paper at ICLR 2020J Hyperparameters
Figure 16: Zoom-in view of Figure 3 (b)25Under review as a conference paper at ICLR 2020Figure 17: Zoom-in view of Figure 3 (c)26Under review as a conference paper at ICLR 2020noise IeVel=IFigure 18: Zoom-in view of Figure 3 (d)27Under review as a conference paper at ICLR 2020I.2 Zoom-in View of Figure 5Figure 19: Zoom-in view of Figure 5 (a)28Under review as a conference paper at ICLR 2020200	400	600	BOO	1000	1200Figure 20: Zoom-in view of Figure 5 (b)29Under review as a conference paper at ICLR 2020J HyperparametersTable 1: Hyperparameters of LAC
Figure 17: Zoom-in view of Figure 3 (c)26Under review as a conference paper at ICLR 2020noise IeVel=IFigure 18: Zoom-in view of Figure 3 (d)27Under review as a conference paper at ICLR 2020I.2 Zoom-in View of Figure 5Figure 19: Zoom-in view of Figure 5 (a)28Under review as a conference paper at ICLR 2020200	400	600	BOO	1000	1200Figure 20: Zoom-in view of Figure 5 (b)29Under review as a conference paper at ICLR 2020J HyperparametersTable 1: Hyperparameters of LACHyperparameters	Repressilator	CartPole	FetchReach	HalfCheetahTime horizon N	5	5	5	∞Minibatch size	256	256	256	256
Figure 18: Zoom-in view of Figure 3 (d)27Under review as a conference paper at ICLR 2020I.2 Zoom-in View of Figure 5Figure 19: Zoom-in view of Figure 5 (a)28Under review as a conference paper at ICLR 2020200	400	600	BOO	1000	1200Figure 20: Zoom-in view of Figure 5 (b)29Under review as a conference paper at ICLR 2020J HyperparametersTable 1: Hyperparameters of LACHyperparameters	Repressilator	CartPole	FetchReach	HalfCheetahTime horizon N	5	5	5	∞Minibatch size	256	256	256	256Actor learning rate	1e-4	1e-4	1e-4	1e-4Critic learning rate	3e-4	3e-4	3e-4	3e-4Lyapunov learning rate	3e-4	3e-4	3e-4	3e-4Target entropy	-3	-1	-5	-6
Figure 19: Zoom-in view of Figure 5 (a)28Under review as a conference paper at ICLR 2020200	400	600	BOO	1000	1200Figure 20: Zoom-in view of Figure 5 (b)29Under review as a conference paper at ICLR 2020J HyperparametersTable 1: Hyperparameters of LACHyperparameters	Repressilator	CartPole	FetchReach	HalfCheetahTime horizon N	5	5	5	∞Minibatch size	256	256	256	256Actor learning rate	1e-4	1e-4	1e-4	1e-4Critic learning rate	3e-4	3e-4	3e-4	3e-4Lyapunov learning rate	3e-4	3e-4	3e-4	3e-4Target entropy	-3	-1	-5	-6Soft replacement(τ)	0.005	0.005	0.005	0.005Discount(γ)	0.75	1.0	1.0	0.995α3	1.0	1.0	1.0	1.0Lyapunov critic network structure	(256,256,16)	(64,64,16)	(64,64,16)	(256,256,16)
Figure 20: Zoom-in view of Figure 5 (b)29Under review as a conference paper at ICLR 2020J HyperparametersTable 1: Hyperparameters of LACHyperparameters	Repressilator	CartPole	FetchReach	HalfCheetahTime horizon N	5	5	5	∞Minibatch size	256	256	256	256Actor learning rate	1e-4	1e-4	1e-4	1e-4Critic learning rate	3e-4	3e-4	3e-4	3e-4Lyapunov learning rate	3e-4	3e-4	3e-4	3e-4Target entropy	-3	-1	-5	-6Soft replacement(τ)	0.005	0.005	0.005	0.005Discount(γ)	0.75	1.0	1.0	0.995α3	1.0	1.0	1.0	1.0Lyapunov critic network structure	(256,256,16)	(64,64,16)	(64,64,16)	(256,256,16)For LAC, there are two networks: the policy network and the Lyapunov critic network. For thepolicy network, we use a fully-connected MLP with two hidden layers of 256 units, outputting themean and standard deviations of a Gaussian distribution. As mentioned in section 4, it should benoted that the output of the Lyapunov critic network is a square term, which is always non-negative.
