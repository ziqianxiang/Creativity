Figure 1: Examples of knockoffs generated from (top to bottom) MNIST, COIL20, and Yale datasets withseveral data models: (a) Original data, 16 sample images. (b) Multivariate Gaussian model knockoffs (Barber& Candis, 2019), cf. Section 2. (C) Deep knockoffs (Romano et al., 2019). (d) Variational autoencoder (VAE)model knockoffs, Section 3. (e) Gaussian process latent variable model (GPLVM) knockoffs, Section 3.
Figure 2: Performance of a deep learning softmax classifier for multiple datasets as a function of the percentageof features selected by multiple baseline and knockoff-based algorithms. The algorithms tested for comparisonare GauSSian knockoffs (Candis et al., 2018), deep knockoffs (Romano et al., 2019), Fisher score (Duda et al.,2001), ReliefF (Robnik-Sikonja & Kononenko, 2003), F-Score (Wright, 1965) andRFS (Nie et al., 2010), Fordatasets featuring rich training data, knockoff-based algorithms outperform baselines, with Gaussian knockoffsoutperformed by their counterparts.
