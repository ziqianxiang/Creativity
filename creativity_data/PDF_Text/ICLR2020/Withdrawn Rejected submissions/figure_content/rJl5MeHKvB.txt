Figure 1: a) Our full task architecture. The green encoder layer is shared across all tasks. The targetdecoder is a fully connected layer mapping to the correct label size. The auxiliary tasks connect tohidden states. Note the sequence-level tasks only connect to the final hidden state. b) Each auxiliarytask decoder DX is composed of a recurrent layer RX and an output layer OX . DAE) Autoencoder.
Figure 2: Performance across three target tasks by number of auxiliary tasks used (averaged over allpossible orderings). In general, We observe that the greater the number of auxiliary tasks, the greaterthe performance for all three tasks. The marginal improvement from including additional auxiliarytasks appears to taper off as the number of tasks increases.
Figure 3:	The average marginal contribution of each auxiliary task across target tasks. The PL-AEtask consistently offers the greatest marginal improvement, and the forecasting task consistentlyprovides the least.
Figure 4:	A) The effect training data size has on auxiliary tasks. As the amount of data increases,we tend to see an increase in the improvement afforded by the auxiliary tasks. B) The relationshipbetween auxiliary task performance (in MAPE, lower is better) and target task performance (inAUC-ROC, higher is better) on the T1D data. Calculated over all training data sizes, there is asomewhat weak relationship (Pearson R = -0.53, line not shown). However, when we condition onthe amount of training data, we find that different relationships emerge (Pearson R = 0.34 whentraining size = 1,000 vs. -0.72 when training size = 1,500).
Figure 5: Average AUC-ROC for every combination of Auxiliary task for all three analyses. Here,columns marked ,NOT, included all but the indicated auxiliary task.
