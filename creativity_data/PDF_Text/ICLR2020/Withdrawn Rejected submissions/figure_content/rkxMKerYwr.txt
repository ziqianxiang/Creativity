Figure 1: Demonstration of a DNN forward, distributions propagation and the intuition of the labeldistribution. (a) Given an input x, We train hidden layers and classifiers to output label distributions bl.
Figure 2: The teacher-student paradigm for analyzing layer behaviors of DNNs. We analyze one-layerbehavior in (a), infinite-layer behavior in (b) and finite-layer behavior in (c).
Figure 3: Wasserstein distance across different layers for different networks.
Figure 4: Distribution propagation across different layers for different networks.
Figure 5: Demonstration of properties on easy, hard and confused samples.
Figure 6: Wasserstein distance between the distribution in an epoch and the target distribution acrossdifferent training epochs for different networks. We choose the 1,4, 9-th layer for ResNet and the1,6,13-layer for VGG.
Figure 7: Definition of intermediate layer of ResNet-18.
Figure 8: Definition of intermediate layer of VGG-16.
Figure 9: Wasserstein distance across different layers of ResNet-18 (left) and VGG-16 (right) ondifferent test set. Specifically, we shuffle the VOC2007 dataset (including training set and test set)and divide it into training set and test set following the ratio of original dataset. We shuffle twiceand define them as “trial_0" and “trial_1”，respectively. Besides, the “orig.” means that we use theoriginal dataset. From Figure 9, different experiments consistently have the same decreasing tendencythrough the depth of a neural network.
Figure 10: Wasserstein distance across different layers of ResNet-18 (left) and VGG-16 (right) ontest set. The tendency of distribution propagation on test set is the same as the training set(Figure 3),suggesting that the distribution propagation is irrelevant to the property of the dataset.
Figure 11: Chebyshev distance across different layers of ResNet-18 (left) and VGG-16 (right) ontraining set. Although the tendency of Chebyshev distance is analogous to Wasserstein distance,Chebyshev distance ignores any metric structure (Frogner et al., 2015). Therefore, we use Wassersteindistance to quantify the discrepancy of label distributions.
Figure 12: Jensen-Shannon divergence across different layers of ResNet-18 (left) and VGG-16 (right)On training set. The conclusion of JenSen-Shannon divergence is the same as the Chebyshev distance.
Figure 13: Wasserstein distance across different layers of ResNet-50 on training set (left) and test set(Iight). We get the same COnClUSiOn on ResNet-50 that Wasserstein distance between the distributionof any layer and the target distribution tends to decreases along the depth of a DNN.
Figure 14: Chebyshev distance (left) and Jensen-Shannon divergence (right) across different layersof ResNet-50 on training set.
Figure 15:	Wasserstein distance between distributions of adjacent training epoch.
Figure 16:	Distribution propagation across different training epochs ofResNet-18 and VGG-16.
Figure 17: More results of easy, hard and confused samples.
