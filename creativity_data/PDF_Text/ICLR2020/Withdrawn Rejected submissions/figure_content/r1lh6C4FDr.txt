Figure 1: Comparison between the average learning curves (with error bars) of LSTM models withand without regularized flexible activation functions on Multi stock indices return data in forecastingmulti-variate return. (a) Two-layer LSTM model with layer sizes: [5, 16, 8]; (b) Three-layer LSTMmodel with layer sizes: [5, 8, 4, 4]; (c) One-layer LSTM model with layer sizes: [5, 16]; (d) Two-layer LSTM model with layer sizes: [5, 16, 16].
Figure 2: Comparison between the average learning curves (with error bars) of auto-encoder modelswith and without regularized flexible activation functions on MNIST dataset.
Figure 3: Comparison between the average learning curves (with error bars) of CNN models (LeNet-5) with and without regularized flexible activation functions on CIFAR-10 dataset.
Figure 4:	The shapes of combined activation function proposed in Section 2.2 with different set ofparameters.
Figure 5:	Comparison between the learning curve of LSTM model with and without flexible acti-vation functions on Multi stock indices return data in forecasting uni-variate return. (a) Two-layerLSTM model with layer sizes: [5, 16, 8]; (b) Three-layer LSTM model with layer sizes: [5, 8, 4, 4];(c) One-layer LSTM model with layer sizes: [5, 16]; (d) Two-layer LSTM model with layer sizes:[5, 16, 16].
Figure 6:	Comparison between the average learning curves (with error bars) of LSTM models withand without regularized flexible activation functions on Multi stock indices return data in forecastingmulti-variate return.
Figure 7: Comparison between the average learning curves (with error bars) of LSTM models withand without regularized flexible activation functions on Multi stock indices return data in forecastingmulti-variate return. (a) layer size [4, 16], window size 10; (b) layer size [4, 16], window size 30(a) Time Step [4, 16L window size = 100.0001760.0001740.0001720.00017025(b) Time Step [4, 16], window sizeFigure 8: Comparison between the average learning curves (with error bars) of auto-encoder modelswith and without regularized flexible activation functions on MNIST dataset (5-Layer).
Figure 8: Comparison between the average learning curves (with error bars) of auto-encoder modelswith and without regularized flexible activation functions on MNIST dataset (5-Layer).
