Figure 1: Rotational blur dataset generation. (a) input panorama image, (b) panorama projection on aunit sphere , (c) intermediate frames between the initial and final images (d) blurred image obtainedby averaging the captured frames.
Figure 2: Overview of our network.  (a) The middle frame is predicted using an encoder-decoderstructure.  The non-middle frames are reconstructed by transforming the multi-layer features ofthe middle frame.  (b) Feature transformer network (FTN) transforms features locally via localwarping (LW) and globally via spatial transformer network (STN). Image transformer network (ITN)transforms predicted middle frame via STN. Finally, the predicted frames are passed through arefining network.
Figure 3: Rotation blurred images generated from panorama scenes.  The top row is ground truthframes and the bottom row is restored frames from the blurs.
Figure 4: Heavily blurred (dynamic) inputs from the high speed videos and the restored video frames.
Figure 5: Partially blurred dynamic motion example. The man is blurred with dynamic motion whilethe background is close to static.
Figure 6:  STN transformation visualization.   The ground truth middle frame is transformed tonon-middle frame using pure STN transformation.
Figure 7: Sequential error propagation. Unlike Jinet al. (2018), our model can successfully recovernon-middle frames even when the middle-frameprediction fails.
Figure 8: Dynamic blurred images from the high speed videos.
Figure 9: Restored video frames for motion-blurred examples from Jianping Shi (2014). Click on theimages in Adobe Reader to play the videos.
Figure 10: Rotation blurred images from the SUN360 panorama dataset.
