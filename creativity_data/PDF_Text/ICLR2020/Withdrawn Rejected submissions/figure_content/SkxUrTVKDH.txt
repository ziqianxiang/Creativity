Figure 1: Visualization of solution path and filter patterns in the third convolutional layer (i.e., conv.c5) ofLetNet-5, trained on MNIST. The left figure shows the magnitude changes for each filter of the models trainedby SPlitLBI and SGD, where x-axis and y-axis indicate the training epochs, and filter magnitudes ('2-norm),respectively. The SplitLBI path of filters selected in the support of Γ are drawn in blue color, while the red colorcurves represent the filters that are not important and outside the support of Γ. We visualize the correspondinglearned filters by Erhan et al. (2009) at 20 (blue), 40 (green), and 80 (black) epochs, which are shown in the rightfigure with the corresponding color bounding boxes, i.e., blue, green, and black, respectively. It shows that ourSplitLBI enjoys a sparse selection of filters without sacrificing accuracy (see Table 1).
Figure 2: Visualization of the first convolutional layer filters of ResNet-18 trained on ImageNet-2012. Given the input image and initial weights visualized in the middle, filter response gradientsat 20 (purple), 40 (green), and 60 (black) epochs are visualized by Springenberg et al. (2014). The”SLBI-10” (”SLBI-1”) in the right figure refers to SplitLBI with κ = 10 and κ = 1, respectively.
Figure 3: Loss and training accuracy by different κ and ν validates the global convergence of SplitLBI.
Figure 4: Sparsity and validation accuracy by different κ and ν show that moderate sparse modelsmay achieve comparable test accuracies to dense models without fine-tuning. Sparsity is obtained asthe percentage of nonzeros in Γt and sparse model at epoch t is obtained by projection of Wt onto thesupport set of Γt, i.e. pruning the weights corresponding to zeros in Γt. The best accuracies achievedare recorded in Tab. 2 and 4 of Appendix for different κ and ν, respectively. X-axis and Y-axisindicate the training epochs, and sparsity/accuracy. The results are repeated for 5 times. Shaded areaindicates the variance; and in each round, we keep the exactly same initialization for each model. Ineach round, we use the same initialization for every hyperparameter. For all the model, we train for160 epochs with initial learning rate (lr) of 0.1 and decrease by 0.1 at epoch 80 and 120.
Figure 5: SplitLBI with early stopping finds sparse subnets whose test accuracies (stars) after retrainare comparable or even better than the baselines (Network Slimming (reproduced by the releasedcodes from Liu et al. (2019) ) , Soft-Filter Pruning(Tab. 10), Scratch-B(Tab. 10), Scratch-E(Tab. 10),and “Rethinking-Lottery” (Tab. 9a)) as reported in Liu et al. (2019). Sparse filters of VGG-16 andResNet-56 are show in (a) and (b), while sparse weights of VGG-16 and ResNet-50 are shown in (c)and (d).
Figure 6: Validation curves of dense models Wt for different κ and ν . For SLBI we find that themodel accuracy is robust to the hyperparameters both in terms of convergence rate and generalizationability. Here validation accuracy means the accuracy on test set of Cifar10. The first one is the resultfor VGG16 ablation study on κ, the second one is the result for ResNet56 ablation study on κ, thethird one is the result for VGG16 ablation study on ν and the forth one is the result for ResNet56ablation study on ν.
Figure 7: Fine-tuning of sparse subnets learned by SplitLBI may achieve comparable or betterperformance than dense models. F-epochk indicates the fine-tuned model comes from the Epochk. SplitLBI (Lottery) and SGD (Lottery) use the same sparsity rate for each layer and the sameinitialization for retrain.
Figure 8: Sparsity changing during training process of SplitLBI (Lottery) for VGG and ResNets(corresponding to Fig. 5). We calculate the sparsity in every epoch and repeat five times. The blackcurve represents the mean of the sparsity and shaded area shows the standard deviation of sparsity.
