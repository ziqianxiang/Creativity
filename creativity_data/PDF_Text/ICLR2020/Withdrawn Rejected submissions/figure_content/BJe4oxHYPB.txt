Figure 1: Illustration of our proposed re-parameterization m = σ(βc), where σ(z) = ɪ+I-Z is thesigmoid function and β acts as a temperature. As β increases, σ(βz) approaches b(z), which can canbe used to frame a 'o-regularized problem (Equation 4). Note that the gradients of σ(βc) vanish as βincreases, suggesting that β should be annealed slowly during training.
Figure 2: Learning a binary mask with weights frozen at initialization with Stochastic Sparsification(SS, Algorithm 2 with one iteration) and Continuous Sparsification (CS), on a 6-layer CNN on CIFAR-10. Left: Training curves with hyperparameters for which masks learned by SS and CS were bothapproximately 50% sparse. CS learns the mask significantly faster while attaining similar early-stopperformance. Right: Sparsity and test accuracy of masks learned with different settings for SSand CS: our method learns sparser masks while maintaining test performance, while SS is unable tosuccessfully learn masks with over 50% sparsity.
Figure 3: Test accuracy of tickets found by different methods on CIFAR-10. Error bars depictvariance across 3 runs. Left: Performance of tickets found on a 6-layer CNN, when trained fromscratch. Right: Performance of tickets found on a ResNet 20, when rewinded to the second trainingepoch. In both experiments, tickets found by CS outperform ones found by IMP. In most cases,CS successfully finds winning tickets in 2 iterations (purple curves).
Figure 4: Performance of different methods whenperforming one-shot pruning on VGG. CS main-tains over 90% test accuracy after removing 99.7%of the weights, while other methods fail to success-fully remove more than 98% of the parameters.
Figure 5: Impact on relative test accuracy and sparsity of tickets found in a ResNet 20 trained onCIFAR-10, for different values of λ and fixed settings for βT and s0 .
Figure 6: Impact on relative test accuracy and sparsity of tickets found in a ResNet 20 trained onCIFAR-10, for different values of βT and fixed settings for λ and s0 .
Figure 7: Impact on relative test accuracy and sparsity of tickets found in a ResNet 20 trained onCIFAR-10, for different values of s0 and fixed settings for βT and λ.
Figure 7: unlike the exploration on λ and βT , we can see that s0 has a strong and consistent effecton the sparsity of the found tickets. For this reason, we suggest proper tuning of s0 when the goalis to achieve a specific sparsity value. Since the percentage of remaining weights is monotonicallyincreasing with s0 , we can perform binary search over values for s0 to achieve any desired sparsitylevel. In terms of performance, lower values for s0 naturally lead to performance degradation, sincesparsity quickly increases as s0 becomes more negative.
Figure 8: Performance of tickets found by Iterative Magnitude Pruning in a ResNet 20 trained onCIFAR, for different pruning rates.
