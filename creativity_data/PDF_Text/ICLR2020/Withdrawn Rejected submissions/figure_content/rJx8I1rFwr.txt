Figure 1: Illustration of the two important properties of our precise collaborative hallucinator, which facilitatedata hallucination to improve the performance of classification tasks. (a) Precision: a classifier trained onhallucinated examples should match the performance of a classifier trained on real examples, demonstrated bythe closeness of their decision boundaries. Real examples and their classifier are shown as dark colored shapesand solid lines, respectively; hallucinated examples and their classifier are shown as light colored shapes anddashed lines, respectively. (b) Collaboration: all the components need to be trained jointly. In addition to theclassification objective imposed on the learner, a collaborative objective is introduced on the hallucinator asdirect and early supervision. We integrate these properties into the meta-learning with hallucination frameworkfor few-shot learning.
Figure 2: Meta-Iearning with our precise collaborative hallucinator. In each episode, given an initial (sampled)training set Strain, We sample its subset Strain. With real seed examples sampled from Strain and noise vectorz, We obtain a set of hallucinated examples SGain through the generator G. Strain and SGain are combinedto create an augmented training set Strain. Conditioning on Sarain (SGain, or Strain), a learner classificationnetwork hcls (hG, or hreal) learns a new embedding space and outputs class probabilities P (pG, or Preal) fora set of real test examples Stest. The classification objective Llearner is a combination of the hard precision'celarner (classification loss calculated based on P and ground-truth labels y) and the SOf precision-inducing loss'pearner (calculated based on Preal and pG). The collaborative objective Lhal shares the same formulation ofLlearner ("., a combination of 'hal and 'pae), but is directly enforced before the embedding layers as earlysupervision for the hallucinator. The hallucinator and the learner are trained end-to-end based on the combinationof Llearner and Lhal. Dotted red arrows indicate the flow of gradients during back-propagation.
Figure 3: t-SNE visualizations of hallucinated examples for investigating the impact of collaborative objective(CO) Without precision. Seeds are shoWn as stars, real examples as crosses, hallucinations as triangles. PECANwithout CO: (a) in the pre-trained ResNet-10 feature space X, (b) in the neW embedding space Î¦ learned by PN;PECAN with CO: (C) in the Pre-Irained ReSNet-10 feature space X. Best viewed in color with zoom.
Figure 4: Visual comparisons of top-1 classification results on two representative novel classes between ourPECAN and the state-of-the-art meta-learned hallucinator (Wang et al., 2018). Top row: bullmastiff; bottomrow: American chameleon. Left 3 columns: test images that are correctly classified by both approaches; middle3 columns: target test images that are misclassified by Wang et al. (2018) as other classes (the names of thepredicted classes by Wang et al. (2018) are overlaid on the images), but correctly classified by PECAN; right3 columns: test images from other classes that are misclassified by Wang et al. (2018) as the target class, butcorrectly classified by PECAN. Our approach is able to model a large range of visual variations and diversity,e.g., bullmastiffs in different poses, and chameleons in different viewpoints and background, whereas Wang et al.
