Figure 1: In (a), the population error of the gradient flow solution vs. α in the sparse regression problemdescribed in Section 4. In (b), the excess `1 norm (blue) and excess `2 norm (red) of the gradient flow solution,i.e. kβα (∞)k1 - IleL 1 k1 and IIea (∞)k2 - kβL2 k2. In (c), the largest α SuCh that βα (∞) achieves populationerror at most 0.025 is shown. The dashed line indicates the number of samples needed by βL 1.
Figure 2: (a) qD (z) for several values of D. (b) The ratio[1, 0, 0, . . . , 0] is the first standard basis vector and 1d =QD (ei)QD(id∕kid k2)as a function of α, where e1[1, 1, . . . , 1] is the all ones vector in Rd. Thiscaptures the transition between approximating the `2 norm (where the ratio is 1) and the `1 norm (where theratio is l/ʌ/d). (C) sparse regression simulation as in Figure 1, using different order models. The y-axis is aD(the scale of β at initialization) needed to recover the planted predictor to accuracy 0.025. The dashed lineindicates the number of samples needed in order for βL 1 to approximate the plant.
Figure 3: Regimes in Matrix Completion We generated a 10 × 10 rank-one matrix completion problemwith ground truth M* = u*(v*)> by generating u*, v* ∈ R10 with i.i.d. N(0,1) entries and observingN = 60 random entries Ω. We fit the observed entries by minimizing the squared loss on a matrix factorizationmodel F (U, V ) = UV > with U, V ∈ Rd×2k. For different scalings α, we examine the matrix M(∞)reached by gradient flow on U, V (solved using python ODE solvers) and plot (i) the reconstruction error onunobserved entries Eij∈ω (Mj - M(∞)ij)2, and (ii) the amount by which the unobserved entries changedduring optimization Eij∈ω (M(∞)ij - M(0)ij)2. In (a) We used k = 2d and initialized to Uo = V0 = αI. In(b) for varying k, We initialized to U0 = αU0 and V0 = αV0 with U0, V0 ∈ RdXk with i.i.d. N(0,1) entries.
Figure 4: Synthetic Data: we generated a small regression training set in R2 by sampling 10 points uniformlyfrom the unit circle, and labelling them with a 1 hidden layer teacher network with 3 hidden units. We trainedoverparametrized depth-D, ReLU networks with 30 units per layer with squared loss using full GD and a smallstepsize 0.01. The weights of the network are set using the Uniform He initialization, and then multiplied byα. The model is trained until ≈ 0 training loss. Shown in (a) and (b) are the test error and grad distance vs. thedepth-adjusted scale of the initialization, αD , when a small constant stepsize is used. for (c) and (d), we fix αnear the transition into the kernel regime, and show the test error and grad distance vs. the stepsize. MNIST:we trained a depth-2 with 5000 hidden units with cross-entropy loss using SGD until it reached 100% trainingaccuracy. The stepsizes were optimally tuned for each α individually. In (e), the dashed line shows the testerror of the resulting network vs. α. We repeated the experiment, but froze the bottom layer and only trained theoutput layer until convergence. The solid line shows the test error of this predictor vs α. CIFAR10: we traineda VGG11-like deep convolutional network with cross-entropy loss using SGD and a small stepsize 10-4 for2000 epochs; all models reached 100% training accuracy. In (f), the dashed line shows the final test error vs.
Figure 5: The plots demonstrate the regimes where gradient flow implicitly minimizes nuclear normand `2 norm, respectively.
Figure 6: Training curves for the CIFAR10 experimentsMNIST Since our theoretical results hold for the squared loss and gradient flow dynamics, herewe empirically assess whether different regimes can be observed when training neural networksfollowing standard practices.
