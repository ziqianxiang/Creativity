Figure 1: Illustration of some automodulator capabilities. The model can directly encode real (un-seen) input images (left). Inputs can be mixed by modulating one with another or with a randomlydrawn sample, at desired scales (center). Finally, taking random modulations for certain scalesproduces novel samples conditioned on the input image (right).
Figure 2:	The model grows step-wise during training; the resolution doubles on every step. Inputx is encoded into a latent enocoding z (a dimensionality of 512 used throughout this paper). Thedecoder acts by modulating an empty canvas ξ(0) by the latent encoding and produces the output X.
Figure 3:	(a) The autoencoder-like usage of the model. (b) Modulations in the decoder can comefrom different latent vectors. This can be leveraged in feature/style mixing, conditional sampling,and during the model training (first pass). (c) The second pass during training.
Figure 4: (a) Style-mixing example using the same source images as Karras et al. (2019) (underliningthat our model can directly work with real input images). (b-c) Random samples at 256 × 256.
Figure 5: The effect of training with face identity invariance enforcement under azimuth rotation.
Figure 6: Random samples from the automodulator trained on FFHQ at a resolution 256×256.
Figure 7: Random samples for an automodulator trained on CELEBA-HQ at resolution 256×256.
Figure 8: Additional samples from an automodulator trained on LSUN Bedrooms and Cars a reso-lution of at 256×256.
Figure 9: Uncurated examples of reconstruction quality in 256×256 resolution with images fromthe FFHQ test set (top row).
Figure 10: Uncurated examples of reconstruction quality in 256×256 resolution with images fromthe CelebA-HQ test set (top row).
Figure 11: Conditional sampling of 128×128 random face images based on ‘coarse’ (latent reso-Iutions 4×4 - 8×8) and ‘intermediate' (16×16 - 32×32) latent features of the fixed input. Theinput image controls the coarse features (such as head shape, pose, gender) on the top and more finefeatures (expressions, accessories, eyebrows) on the bottom.
Figure 12: Interpolation between random test set CELEBA-HQ images in 128×128 (in the corners)which the model has not seen during training. The model captures most of the salient features in thereconstructions and produces smooth interpolations at all points in the traversed space.
Figure 13:	Style mixing of FFHQ face images. The source images are unseen real test images, notself-generated images.
Figure 14:	Style mixing of LSUN Cars. The source images are unseen real test images, not self-generated images.
