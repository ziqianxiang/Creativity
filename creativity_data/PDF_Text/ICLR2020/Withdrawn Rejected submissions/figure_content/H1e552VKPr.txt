Figure 1: Example to moti-vate subgraph attentionA survey on network representation learning and graph neural net-works can be found in Wu et al. (2019). Here we briefly discusssome more prominent approaches for node classification and graph classification. Kipf & Welling(2017) propose a version of graph convolution network (GCN) which learns a weighted mean ofneighbor node attributes to find the embedding of a node by minimizing the cross entropy loss fornode classification. Different extensions of GCN are available in literature for inductive learning(Hamilton et al., 2017) and link prediction (Zhang & Chen, 2018). VelickoVic et al. (2018) Pro-pose GAT which uses attention mechanism to learn the importance of a node to determine the labelof another node in the neighborhood of it in the graph convolution framework. Recently, a GCNbased unsupervised approach (DGI) is proposed (Velickovic et al., 2019) by maximizing mutualinformation between patch representations and corresponding high-level summaries of a graph.
Figure 2: Architecture of SubGattPool Network for graph classificationgraph. Let Us denote these level graphs (i.e., graphs at different levels) by G1,…，GR. There is aGNN layer between the level graph Gr (i.e., the graph at level r) and the level graph Gr+1. ThisGNN layer comprises ofan embedding layer which generates the embedding of the nodes ofGr anda pooling layer which maps the nodes of Gr to the nodes of Gr+1 . We refer the GNN layer betweenthe level graph Gr and Gr+1 by rth layer of GNN, Vr = 1, 2,…，R - 1. The last level graphGR contains only one node, whose featUre sUmmarizes the entire inpUt graph. Pleas note, nUmberof nodes N1 in the first level graph depends on the inpUt graph, bUt we keep the nUmber of nodesNr in the consequent level graphs Gr (∀r = 2,…，R) fixed for all the input graphs (in a graphclassification dataset), which help Us to design a shared attention mechanism, as discUssed later.
Figure 3: t-SNE visualization of the graphs from MUTAG (different colors show different labels ofthe graphs) by the representations generated by: (a) DIFFPOOL; (b) SubGattPool, but the SubGattembedding and pooling layers being replaced by GCN; (c) SubGattPool without intra and interlayer attention; (d) the complete SubGattPool network. Compared to (a), there are improvement ofperformances for both the SubGatt layer and intra/inter-level attention individually. Finally differentclasses are separated most by SubGattPool which again shows the merit of the proposed algorithm.
Figure 4: Sensitivity analysis of SubGatt network for node classification on Cora with respect todifferent hyper-parametersA.2 Node Classification Results on the Publicly Available Splits of theDatasetsSome existing node classification and semi-supervised node representation techniques (Kipf &Welling, 20l7; VelickoVic et al., 2018) use publicly available splits (into training, validation andtest sets) of some popularly used datasets such as Cora, Citeseer and Pubmed to show the meritof the proposed algorithms. These splits were introduced in (Yang et al., 2016). However, thesesplits were created randomly, as mentioned in the cited paper: “We randomly sample 20 instancesfor each class as labeled data, 1000 instances as test data”. Kipf & Welling (2017) introduces “anadditional validation set of 500 labeled examples for hyperparameter optimization”. We use exactlythe same splits of the nodes as in (Kipf & Welling, 2017) and report the results in Table 4. It turnsout that SubGatt is outperformed by GAT on this publicly available splits of the datasets, whereasthe performance is competitive to GCN. This is slightly contradicting to what we observe in Table2, where the performance of SubGatt is mostly superior to GCN and GAT on the random splits ofthe dataset into training, validation and test sets. One possible reason can be the larger training size(80% training, in which 10% was used only for validation) of the node classification experiments11Under review as a conference paper at ICLR 2020Dataset	#Train/ValidationZTest	GCN	GAT	SubGatt
