Figure 1: (a) Fully connected autoencoder trained on 10 images from CIFAR10. When fed Gaussiannoise or test images from CIFAR10 the model outputs an image that is visually indistinguishablefrom a training image. (b) A fully connected network trained to encode a sequence of 389 frames ofsize 128 × 128 from the Disney film “Steamboat Willie.” Inputting Gaussian noise and iterating thenetwork yields the entire video.
Figure 2: Fully connected autoencoder trained on 10 images from CIFAR10.
Figure 3: Impact of optimizer and nonlinearity on number of training examples stored as attractors.
Figure 4: Impact of initialization on number of training examples stored as attractors. In all ex-periments, we used a fully connected network with 11 hidden layers and 256 hidden units per layertrained using the Adam optimizer (lr=10-4). (*) NA indicates that the training error did not decreasebelow 10-8 in 1,000,000 epochs.
Figure 5: (a) U-Net convolutional autoencoder trained on 10 images from CIFAR10. When fedGaussian noise or test images from CIFAR10 the model outputs images that are visually indistin-guishable from training images. All training images are attractors (EV is eigenvalue). (b) Identifyingattractors from iterating the trained autoencoder starting in 1000 test examples from CIFAR10 and1000 examples of Gaussian noise. No attractors are found outside the training set.
Figure 6: Impact of width and depthon number of MNIST examples (out of100) stored as attractors. The networksused have SELU activations, default ini-tialization and are trained using Adamuntil MSE ≤ 10-8.
Figure 7: Top eigenvalues of Jacobian for all 100 train-ing examples.
Figure 8: 2 MNIST digit training sequences of length 10 stored as limit cycles of an encoder. It-eration from random noise converges to each of the sequences. Our network has 256 hidden unitsper layer, 31 hidden layers, SELU activations, default initialization, and is trained using the Adamoptimizer until MSE ≤ 10-8.
Figure 9: Storing 100 MNIST examples through sequences. The networks used have SELU activa-tions, default initialization and are trained using Adam until MSE ≤ 10-8.
Figure 10: Convolutional autoencoder architecture used in our experiments.
Figure 11: Identifying attractors from random inputs across optimizers for the SELU nonlinearity.
Figure 12: Identifying attractors from random inputs across nonlinearities.(a) Leaky RELU networktrained using Adam; 59 observed attractors outside trainset. (b) SELU network trained using Adam;17 observed attractors outside trainset. (C) Network with x+ sm；0x activations; 9 observed attractorsoutside trainset. (d) Network with cos x - x activations; 0 observed attractors outside trainset.
Figure 13: Identifying attractors from random inputs across initializations. (a) SELU networktrained with Adam with initialization U [-0.01, 0.01]; 5 observed attractors outside the trainingset. (b) SELU network trained with Adam with initialization U [-0.02, 0.02] ; 8 observed attractorsoutside the training set. (c) SELU network trained with Adam with initialization U [-0.05, 0.05], 12observed attractors outside the training set. (d) SELU network trained with Adam with initializationU [-0.1, 0.1]; almost no inputs converged to training examples within 1000 iterations.
Figure 14: Iterating 10,000 test examples and 10,000 examples of random noise does not lead to anyobservable attractors outside the training set.
