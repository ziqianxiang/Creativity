Figure 1: Uncertainty in physics.
Figure 2: Hierarchical Relation Network (HRN) architecture. Force, collision and past effects onparticles are computed and then propagated through each object hierarchy. The propagated effectsare used to predict the next particle positions. Gray blocks represent graph convolutional effectpropagation modules.
Figure 3: Shape loss. The HRN shape loss only constraintsparticle distances within object particle groups (left). Our newfully-connected shape loss constraints distances between allparticles pairs within each objects (right).
Figure 4: Recurrent training.
Figure 5: Dynamics prediction comparisons. Our method is compared to the HRN baseline andground truth. a) A soft cube bounces of the ground. b) Two rigid cubes collide. Our methodpreserves the geometry of objects better over long time horizons.
Figure 6: Example sampled multiple trajectories. We use dropout on the force and collision mod-ule to sample multiple trajectories given the same initial input. Dark colors depict the ground truthtrajectory. Light colors depict imagined sampled trajectories. The performance of our model inforward simulation is maintained at a high level despite the introduced graph-convolutional dropout.
Figure 7: Average episode length in the”cube moving task”. We compare a de-terministic environment against action spacenoise and 2 randomly seeded stochastic envi-ronments. The agent learns faster in stochas-tic environments through better initial explo-ration and converges to a shorter policy.
Figure 8:	Average episode length in the ”ball-hits-tower task”. We compare a determinis-tic environment against action space noise, astochastic environment where the dropout rate isfixed and 3 randomly seeded stochastic environ-ments where the dropout rate is annealed. Theagent finds shorter policies earlier in the train-ing indicating more efficient exploration in thestochastic environments.
Figure 9:	Policy comparisons. a) Cube moving task. Top: Policy learned in a deterministicenvironment (longer, 4 time steps). Bottom: Policy learned in a stochastic environment (shorter,2 time steps). The first two frames are model inputs. The red cubes indicate the target position towhich the green cubes have to be moved. b) Ball hitting tower task. Agents in deterministic andstochastic environments converge to similar 4 step policies. The figure depicts one 4 step policyexample.
Figure 10: Effect of the dropout rate on the distribution width of the sampled trajectories. 0.5 - top,0.3 - middle, 0.1 - bottom. Dropout applied in the force module.
Figure 11: Effect of the dropout rate on the distribution width of the sampled trajectories. 0.3 - top,0.1 - bottom. Dropout applied in the force and collision modules.
