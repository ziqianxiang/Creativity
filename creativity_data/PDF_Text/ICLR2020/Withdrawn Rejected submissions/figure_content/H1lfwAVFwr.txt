Figure 1: Normal distribution representations of CLAC (blue) and SAC (orange) model policies with standarddeviations of 0.8 and 0.9.  Light blue displays the value of the hidden state feature defining the state transitionprobability. Beta distribution used to gereate state features in red.
Figure 2:  Results from tests in the generalizability of learned policies in SAC and CLAC models, varied bymutual information coefficient and entropy coefficient.  Again the number of states N=10, and the number ofagents trained is 5.  During training, hidden state features are resampled after each trial from the same Betadistribution.
Figure 3: For all tests 5 separate agents are trained. Humanoid and Ant are trained for 0.5M steps, Walker andCheetah for 0.4 M, and the pendulums for 200K. CLAC mutual information coefficients are chosen to be theoptimal values sampled from the range [0,0.3] in 0.025 increments.
Figure 4: Results from tests in the generalizability of learned policies in SAC and CLAC models with 5 agentseach.  Static parameters represent the same environment features used to test agents.  Testing parameters rep-resent sampling from a disjoint set around the training parameters.   Extreme testing parameters represent adisjoint set further from the training than the random parameters. Error bars represent 95% confidence interval.
Figure 5: Top: Diagram of the Continuous N-Chain Learning environment. Middle: Example of a set of hiddenstate values.  Right:  Graph of the probability of moving to the next state given the absolute distance from thehidden state value and action preformed. Bottom: Function describing this state transition probability.
Figure 6:  Images of the six roboschool environments used to compare performance CLAC against existingDeep RL methods.  Top-left:  humanoid.  Top-middle Walker 2D. Top-right Inverted Pendulum.  Bottom-left:Half-Cheetah. Bottom-middle: Ant Walker. Bottom-right: Inverted Double Pendulum.
