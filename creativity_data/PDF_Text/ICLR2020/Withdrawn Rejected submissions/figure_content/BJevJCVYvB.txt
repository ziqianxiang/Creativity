Figure 1: Illustration of the Polyak step-sizein 1D. In this case, and further assumingthat f? = 0, the algorithm coincides with theNewton-Raphson method for finding roots ofa function.
Figure 2: A simple example where thePolyak step-size oscillates due to non-convexity. On this problem, ALI-G con-verges whenever its maximal learning-rateη is lower than 10.
Figure 3: Final objective function when training a Differentiable Neural Computer for 10k steps(lower is better). The intensity of each cell is log-proportional to the value of the objective function(darker is better). ALI-G obtains good performance for a very large range of η (10-1 ≤ η ≤ 106).
Figure 4: Objective function over the epochs on CIFAR-10, CIFAR-100 and SVHN (smoothed with amoving average over 5 epochs). On SVHN, ALI-G obtains similar performance to its competitorsand converges faster. On CIFAR-10 and CIFAR-100, which are more difficult tasks, ALI-G yields anobjective function that is an order of magnitude better than the baselines.
Figure 5: Illustration of the function f, which satisfies the RSI. When starting at w = -3/5, gradientdescent with the Polyak step-size oscillates between w = -3/5 and w = 3/5.
