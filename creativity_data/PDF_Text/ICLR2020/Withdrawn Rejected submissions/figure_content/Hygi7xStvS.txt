Figure 1: Impact of the model size on compression rate. We report the compression rate in bpc fortransformer networks of various sizes. In particular, we vary the number of layers (6, 12, 18, 24)as well as the dimension of each layer (256, 512, 1024). Left: we observe that on the full enwik8dataset, larger models obtain better compression rate. Middle: using models deeper than 12 layersdoes not seem to improve the compression. Right: Instantaneous compression rate (averaged overthe last 1 million characters) for 12 layers models. Larger models obtain better performance, evenat the beginning of compression.
Figure 2: Instantaneous compression rate (averaged over thelast 1 million characters) for models with different contextsizes.
Figure 3: Instantaneous compression rate (averaged over the last 1 million characters) for differentfixed learning rates (left) and linear decay schedule (right). We observe that higher learning ratesget better compression rate at the beginning of the compression phase, but lower learning rates tendto obtain better results at the end. A linear decay schedule allows to match the best performing fixedlearning rate at most steps of the compression.
Figure 4: Impact of different optimization parameters. Left: we report the bpc over the full enwik8data for different learning rate with linear decay and number of warmup steps. Top right: small Î²1values for ADAM variants lead to better compression rate on the full enwik8. Bottom right: as forsupervised learning, gradient clipping is helpful in the compression setup.
Figure 5: Instantaneous compression rate (averagedover the last 1 million characters) for models with andwithout n-grams. We observe that adding n-gram in-formation lead to a better compression rate for thewhole compression phase.
Figure 6: Compression ratio dependingon kmax and the number of vectors usedto represent the n-grams5.7	IMPACT OF PREPROCESSING AND n-GRAMSWe only apply a minimal preprocessing step to the data, consisting in replacing uppercase charactersby lowercase ones, with an additional escape code. This simple step, also used by other compressionalgorithms such as CMIX, leads to an improvement of 0.01 BPC. We also considered adding n-graminformation to the input of the transformer model. At each time step t, the input corresponds tothe current character, as well as the n-grams ending at position t. To avoid storing a dictionaryof n-grams in the archive, we instead hash the n-grams into one a fixed number of bins, each binbeing associated to a learned embedding vector. We report the effect of adding n-grams in Table 6and Figure 5. We show that adding this information lead to an improvement of 0.025 BPC, andthis improvement can still be observed at the end of the compression phase. By combining theoptimal parameters derived from the different experiments reported in this section, we can compressenwik8 using 1.33 bits per character. While this is far from the performance of CMIX, it improvesprevious results obtained with transformer networks (Bellard, 2019).
