Figure 1: Training results for CIFAR-10. Architectures are (a) AlexNet, (b) VGG19 (BN), (c)PreReSNet-56, (d) ResNet-110, (e) DenseNet-100 (BC) using SoftAdam, AdamW and SGD. Eachline indicates the average of 3 training runs with random initializations.
Figure 2: Language model validation perplexity during training for SoftAdam, Adam/AdamW andSGD.
