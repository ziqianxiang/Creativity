Figure 1: Example images from Dtrain and Dvalid from both classes. In both distributions, crosssize can vary between samples. In Dtrain, two crosses (denoting class 0) are always accompaniedby a box xd in the bottom right-hand corner, while a single cross (denoting class 1) is always ac-companied by a distractor in the bottom left-hand corner. In Dvalid, the relationship between classesand crosses remains the same, but the logic governing the location of the distractor is reversed. Thedistractor is indicated with a red arrow.
Figure 2: Schematic of the model used in all experiments (alongside an 18-layer ResNet). Theactdiff penalty was only applied to the encoder path of the model. The reconstruction path (postclassification) was optionally used when a reconstruction was requested of the model. Skip connec-tions were optionally employed in the style of UNet. Both of these optional paths are denoted usingalternating dashed lines. All losses are denoted using standard dashed lines.
Figure 3: Saliency maps showing the different major behaviours on Dvalid observed across the mod-els tested on the input image (top left; distractor indicated with a red arrow) with the dialated mask(bottom left). These images correspond to the rows of Table 1. CNN classification demonstratesoverfitting, where the saliency is concentrated on the distractor (center top left). CNN ClassifyMasked demonstrates that the model has not learned to ignore the distractor because it never sawone during training (center bottom left). UNet actdiff (center top right) and ResNet18 actdiff (centerbottom right) demonstrate that the model has successfully learned to ignore the distractor. Note thatthe network learned to reconstruct the distractor in the location observed during Dtrain (top right,red arrow). The gradmask model fails to ignore the distractor and does not pay equal attention to thetwo features of interest (bottom right).
Figure 4: The average saliency map from 100 randomly-selected test-set images in the Cardiac (top),Liver (middle), and Pancreas (bottom) dataset.The pathology column shows the mean non-blurredsegmentation for each dataset. All three datasets show clear and consistent changes in the locationof the strongest saliencies when using both Actdiff and Gradmask when compared with a normalclassifier.
Figure 5: Results on the chest X-ray task. (a) Saliency maps of the different models with differentmethods to prevent incorrect feature attribution. The task is to predict Emphysema (a lung con-dition). Two different images from the test set are shown with the masks that were used duringtraining. The top image is a negative example and the bottom positive. (b) Test Results (Best ValidEpoch) using a ResNet on the Chest X-ray task. SPC=site-pathology correlation.
Figure 6: Results of the maximum masks experiments. (a) Best Test AUC for the best Valid AUCfor each of the maximum masks conditions. (b) Best valid Epoch for each of the maximum masksconditions.
Figure 7: Line plots showing the Valid AUC for each of the 500 epochs during training for all modelson the synthetic dataset. We can see that training models with masked data has no substantial benefiton the validation set, models simply trained to classify (or, in addition, reconstruct a masked versionof the input) overfit early in training, gradmask models fails to train, and actdiff surpasses theperformance in all cases where it is effective (i.e., not the CNN model).
Figure 8: Mean resized X-Ray from the NIH dataset (left) and PadChest dataset (right). There areclear differences in the site distributions that are obvious around the edges of the image.
Figure 9: Saliency maps showing where the model attributes areas of the visual input space tothe prediction made by the network for the Liver detection (top), Caridac Left Atrium detection(middle), and Pancreas (bottom) datasets. The top 10% of gradients are shown in each image forvisualization. The top left image shows the raw input, and to its right is the anatomy segmentationbefore and after blurring. From left to right along the bottom, the ResNet model outputs are shownin the second row and the UNet results are shown in the third row, for the baseline classificationmodel, ActDiff, Gradmask, and ActDiff & Gradmask (“ActGrad”). The rightmost column showsoutputs specific to the UNet reconstructions: the top image shows the standard reconstruction, rightmiddle image shows the output of the Reconstruct Masked task, and bottom image shows the featureattribution of the Reconstruct Masked model.
