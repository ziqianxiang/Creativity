Figure 1: The intuition behind floating neurons. The connections between floating neurons aredefined based on their relative similarities. Thus, neurons needing an information and providers ofit tend to approach each other.
Figure 2: A schematic representation of a floating neuron and its connections. Each neuron has aninput embedding, an output embedding, and a transformation function.
Figure 3: The architecture of Farfalle Neural Networks.
Figure 4: The l2-norm of each input neuron’s Figure 5: The input embeddings are projectedembedding is calculated and plotted at its corre- to 2D space using UMAP. Only neurons in thesponding cell.	inner 20 × 20 box are included. Neurons corre-sponding to adjacent cells are connected with aline.
Figure 6: The effect of different parameters on the performance of FNNs. The default values fornumber of iterations, embedding size, number of neurons, and normalization function are 3, 256,1024, and l2 respectively. The default values are used if not specified.
Figure 7: Unrolling Farfalle Neural Networks. FNNs have a recurrent block of floating neurons.
Figure 8: The recurrent structure allows the network to balance the number of neurons employedin different levels of abstraction. This illustration employs unrolled structure to show how neuronscan combine information from different levels of abstraction. Red lines specify active connections(connections that significantly influence output) of each iteration.
