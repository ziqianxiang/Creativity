Figure 1: For the normalized classifier, we use the technique called weight imprinting proposedby Qi et al. (2018). Each column Wi ∈ Rd of the classifier weight is normalized so that Wi has anorm of 1 (i.e., ∣Wi∣∣ = 1), and classification is performed by taking the inner products betweenWi and normalized feature vector Z ∈ Rd. Note that variable d is the dimension of the featurespace. Before the network is fine-tuned, the initial weight for a novel class can be obtained byincluding feature vector Z of the novel class in the classifier weight. When multiple novel examplesper class are available (i.e., K-shot learning with K > 1), the initial weight for the novel class canbe obtained by normalizing class mean 1 /K P j= Zj again. Because the output range of WZ is[-1, 1], ensuring that the probability of the correct label approximates 1 using softmax activation isdifficult. This problem can be avoided by applying scale factor s ∈ R to the output, as discussed byQi et al. (2018).
Figure 2: Classification accuracy for validation classes with different learning rates. We usedthe ReSNet-18, normalized classifier, and Adam optimizer. We chose the 5-shot cross-domain taskfor visualization because the transition of validation accuracy is clearer than in other tasks. Weset the learning rates as 0.01, 0.001, and 0.0001, and conducted four trials with randomly selectedvalidation classes and support sets. Note that classification accuracy can be significantly changed bythe randomly selected classes and samples. Therefore, We focused on the transition of the validationaccuracy rather than the validation accuracy itself.
Figure 3: Classification accuracy for novel classes with different optimizers in the high-resolutionsingle-domain task. We used the ResNet-18 as a feature extractor. This shows that the adaptivegradient optimizer (blue bars) achieved higher accuracy than other methods (red bars), particularlywith the normalized classifier.
Figure 4: Relationship between test accuracy and updated part of the network in the 5-shot cross-domain task. We used the Adam optimizer for fine-tuning. The result for “VGG-16 NormalizedBN & FC” was left empty because the network did not have a BN layer. In addition, we did notconduct experiments for the VGG-16 with the simple classifier because the loss did not decrease inthe pretraining phase.
