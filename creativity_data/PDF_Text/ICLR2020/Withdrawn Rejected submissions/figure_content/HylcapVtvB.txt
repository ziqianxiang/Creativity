Figure 1: MNIST experiments. All plots show test-set-accuracy vs. the number of extra pointslabeled. Left: Comparison of our DIVERSEPUBLIC technique with the active learning baselinesdescribed above. All models are fine-tuned starting from the same baseline (‘checkpoint 1’: testaccuracy 97.5% and εdp = 3.2). Active learning improves the performance of the DP-modelto as much as 98.8% in the best case with no increase in privacy cost. Right: Comparison of ourNEARPRIVATE technique with the DIVERSEPUBLIC technique. Since we spent εdpPCA + εsupport = 1.0on the NEARPRIVATE technique, we fine-tune the NEARPRIVATE model from ‘checkpoint 0’ withprivacy cost εdp = 2.2. Thus, both lines have the same privacy cost (εlimit = 3.2), regardless of thenumber of extra points used.
Figure 2: SVHN experiments. Left: Comparison of our DIVERSEPUBLIC technique with the activelearning baselines. All models are fine-tuned starting from the same baseline (‘checkpoint 1’: test ac-curacy 75.0% and εdp = 6.0). Active learning improves the performance of the DP-model to as muchas 85% in the best case with no increase in privacy cost. Recall also that the 75.0% number is itself abaseline established in this paper. Right: Comparison of NEARPRIVATE with DIVERSEPUBLIC .
Figure 3: SVHN experiments with dataset pollution. In this experiment, we train the DP baselinewith 30,000 of the SVHN training images. The extra public dataset is a combination of 40,000SVHN training images and 10,000 CIFAR-10 training images. Left: DIVERSEPUBLIC comparedagainst other active learning baseline techniques as in Figure 2a. In this case, the active learningtechniques do not outperform random selection by very much. Uncertainty by itself is not a sufficientpredictor of whether extra data will be helpful here, since the baseline model is also uncertain aboutthe CIFAR-10 images. Right: NEARPRIVATE compared against DIVERSEPUBLIC , but startedfrom different checkpoints (‘checkpoint 0’ with εdp = 5.0, ‘checkpoint 1’ with εdp = 6.0) to keepthe privacy cost constant as in Figure 2b. In this case, NearPrivate substantially outperformsDiversePublic by selecting less of the CIFAR-10 images.
Figure 4: DIVERSEPUBLIC analysis. Left: We apply DIVERSEPUBLIC to the same DP checkpoint(dashed horizontal line), varying the number of clusters (horizontal axis) and the number of chosenpoints from each cluster (lines with error bars). Right: For Ncluster = 100, Neach = 20, we visualizethe most central example from each cluster. Since NearPrivate does not have explicit clustering,we use DiversePublic for this visualization. Green borders mean that the initial checkpoint (dashedhorizontal line) predicted correctly; while red bordered examples (originally predicted incorrectly)have dots on the left showing predictions and dots on the right showing true labels.
Figure 5: NEARPRIVATE analysis. We apply NEARPRIVATE to DP checkpoints (horizontal axis) ofdifferent privacy cost εdp. The black line with triangle represents those starting checkpoints. Middle:The initial training privacy cost εdp of DP checkpoints at different epochs of a single training run.
