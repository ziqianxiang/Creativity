Figure 1: IP obtained using our proposed estimator for a small DNN averaged over 5 training runs.
Figure 2: IP of a CNN consisting of three convolutional layers with 4, 8 and 12 filters and one fullyconnected layer with 256 neurons and a ReLU activation function in in each hidden layer. MI wascomputed using the training data of the MNIST dataset and averaged over 5 runs.
Figure 3: IP of the VGG16 on the CIFAR-10 dataset. MI was estimated using the training data andaveraged over 2 runs. Color saturation increases as training progresses. Both the fitting phase andthe compression phase is clearly visible for several layers.
Figure 4: IP of the VGG16 on the CIFAR-10 dataset. MI was estimated using the test data andaveraged over 2 runs. Color saturation increases as training progresses. The fitting phase is clearlyvisible while the compression phase can only be seen in the output layer.
Figure 5: Leftmost figure displays a 2-dimensional illustration of the data described in Appendix Aand rightmost figure shows how the entropy decreases as the variance of the distribution decreases.
Figure 6: Leftmost figure displays a 2-dimensional illustration of the data described in Appendix Aand rightmost figure shows how mutual information decreases as less of the distributions overlap.
Figure 7: Different approaches for calculating kernels based on tensor data. First row shows themultivariate approach of Yu et al. (2019), second row depicts the tensor kernel approach used in thispaper, and third row displays the kernel obtained using matricization-based tensor kernels Signorettoet al. (2011) that preserve structure between channels. Bright colors indicate high values while darkvalues indicate low values in all the kernel matrices.
Figure 8: IP of a MLP consisting of four fully connected layers with 1024, 20, 20, and 20 neuronsand a tanh activation function in in each hidden layer. MI was estimated using the training data ofthe MNIST dataset and averaged over 5 runs.
Figure 9: IP of a CNN consisting of three convolutional layers with 4, 8 and 12 filters and one fullyconnected layer with 256 neurons and a tanh activation function in in each hidden layer. MI wasestimated using the training data of the MNIST dataset and averaged over 5 runs.
Figure 10:	Evolution of kernel width as a function of iteration for the three networks that we consid-ered in this work. The plots demonstrate how the optimal kernel width quickly stabilizes and stayrelatively stable throughout the training.
Figure 11: Mean difference in MI of subsequent layers ` and ` + 1. Positive numbers indicatecompliance with the DPI. MI was estimated on the MNIST training set for the MLP and on theCIFAR-10 training set for the VGG16.
