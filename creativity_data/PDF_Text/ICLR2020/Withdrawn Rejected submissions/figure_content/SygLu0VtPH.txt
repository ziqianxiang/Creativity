Figure 1: Agent Model. The agent model consists of three modules. A visual component (theencoder of a variational autoencoder) produces a latent code zt at each time step t, which is concate-nated with the hidden state ht of the LSTM-based memory component that takes zt and previouslyperformed action at-1 as input. The combined vector (zt, ht) is input into the controller componentto determine the next action of the agent. In this paper, the agent model is trained end-to-end with amultiobjective genetic algorithm.
Figure 2: In the CarRacing-v0 task the agent has to learn to drive across many procedu-rally generated tracks as fast as possible from 64 Ã—64 RGB color images. In the VizDoom:Take Cover domain the agent has to learn to avoid fireballs and to stay alive as long as possible.
Figure 3: VizDoom Evolutionary Training. Shown is (a) mean performance over generations to-gether with one standard error. For a representative run of DIP (b), we plot the euclidean distancesof the weights of the intermediate solutions (i.e. individuals with the highest task reward discoveredso far) compared to the final solution in addition to their age and the average population age.
Figure 4: Still frames of a learned policy. The evolved champion learned to primarily pay attentionto the walls and fireballs, while ignoring the floor and ceiling. Interestingly the agent also seems topay attention to the health and ammo indicator.
Figure 5: t-SNE mapping of the latent+hidden vector (a), latent vector alone (b), and hidden vectoralone (c). While the compressed latent vector is not enough to infer the correct action (b), the hiddenLSTM vector alone contains enough information for the agent to decide on the correct action (c).
Figure 6: Average activation levels of LSTM in two different situations. For visualization purposesonly, game images are colored more or less blue depending on the LSTM activation values. Theevolved forward model seems to have learned to predict if a fireball would hit the agent at thecurrent position. In (a) the agent can take advantage of that information to avoid the fireball whilethe agent does not have enough time to get out of the way in situation (b) and gets hit. Shown on topare the actions the agent takes in each frame.
Figure 7: Development of the evolved representation over generations. Shown are t-SNE mappingsof the 288-dimensional vectors (32-dimensional latent vectors + 256-dimensional hidden state vec-tor) together with saliency maps of specific game situations. Early on in evolution the agent startspaying attention to the fireballs (generation 24) but only moves to the right (blue) or stands still(black). Starting around generation 34 the agent starts to move to the left and right, with the saliencymaps becoming more pronounced. From generation 56 on the compressed learned representation(latent vector+hidden state vector) allows the agent to infer the correct action almost all the time.
Figure 8: Average reward across ages and number of individuals per age.
