Figure 1:	Learning curves for locomotion tasks, averaged across three runs of each algorithm withrandom Initialization for RL, warm initialization using source policy for ATL and Jumpstart(Warm-Start) methods and Source policy performance in Target Task without any adaptation.
Figure 2:	Trajectory KL divergence Total Intrinsic Return -	eζt averaged across three runs.
Figure 3: Target Trajectory under policy πθ and local trajectory deviation produced by source opti-mal policy π* and source transition PSD Additional Results and Details of Experimental DomainTo establish a standard baseline, we also include the results for classic cart-pole and Inverted pen-dulum balancing tasks, based on the formulation Barto et al. (1983). We also demonstrate the cross-domain transfer capabilities using a variant of the proposed algorithm Joshi & Chowdhary (2018)and the results for Cart-Pole to Inverted Pendulum and Inverted Pendulum to Bicycle transfers areprovided in the following sections.
Figure 4: Policy Transfer from Inverted Pendulum to Non-stationary Inverted pendulum: (a) Aver-age Rewards and (b) Training length, TA-TL(ATL ours), UMA-TL(Jumpstart-RL) and Stand-aloneRLD.1 Inverted Pendulum (IP) to time-varying IPWe demonstrate our approach for a continuous state domain, Inverted Pendulum (IP) swing-up andbalance Figure-4. The source task is the conventional IP domain. The target task differs from thesource task in the transition model. The target task is a non-stationary inverted pendulum, where thelength and mass of the pendulum are continuously time varying with function Li = L0 + 0.5cos( ∏0i)and Mi = M0 + 0.5cos(5∏i∙), where L0 = 1, M0 = 1 and i = 1 ...N. The state variablesdescribing the system are angle and angular velocity {θ, θ} ∈ [-π, π]. The RL objective is toswing-up and balance the pendulum upright such that θ = 0, θ = 0. The reward function is selectedas R(θ,θ) = -10∣θ∣2 - 5∣θ∣2, which yields maximum value at upright position and minimum at thedown-most position. The continuous action space is bounded by T ∈ [-1, 1]. Note that the domainis tricky, since full throttle action is assumed to not generate enough torque to be able to swing thependulum to the upright position, hence, the agent must learn to swing the pendulum back and forthand leverages angular momentum to go to the upright position.
Figure 5: Negative Transfer Inverted Pendulum: (a) Average Reward and (b) Training length(a)	(b)Figure 6: Adap-to-Learn Policy Transfer (a) Inverted-Pendulum (b) Cart-Pole5a and 5b demonstrate convergence to average maximum reward with lesser training samples forproposed ATL method compared to Initialized-RL(UMA-TL) and standalone-RL methods. It is tobe observed that UMA-TL method suffers from a negative transfer phenomenon. The agent underUMA-TL converges to a much lower average reward by getting stuck in local minima and neverachieves the upright balance of the pendulum.
Figure 6: Adap-to-Learn Policy Transfer (a) Inverted-Pendulum (b) Cart-Pole5a and 5b demonstrate convergence to average maximum reward with lesser training samples forproposed ATL method compared to Initialized-RL(UMA-TL) and standalone-RL methods. It is tobe observed that UMA-TL method suffers from a negative transfer phenomenon. The agent underUMA-TL converges to a much lower average reward by getting stuck in local minima and neverachieves the upright balance of the pendulum.
Figure 7: (a) Cart-Pole and Bicycle Domain (b)Average Rewards for TA-TL (ours), UMA-TL (jump-start) and RL(a)(b)Figure 8: Policy Transfer from Cart-Pole to Bike Balancing: (a) Total simulation time (in seconds)the agent was able to balance the bike in training (b) Total time required to solve the task for TA-TL(ours), UMA-TL (jumpstart) and RLThe states of the bicycle task are angle and angular velocity of the handlebar and the bike fromvertical (θ, θ, ω, ω) respectively. For the given state the agent is in, it chooses a continuous action ofapplying torque to the handlebar, T ∈ [-2N m, 2Nm] trying to keep the bike upright. The detailsof bicycle dynamics are beyond the scope of this paper; interested readers are referred to Randlov &AlStrom (1998); AStrom et al. (2005), and references therein.
Figure 8: Policy Transfer from Cart-Pole to Bike Balancing: (a) Total simulation time (in seconds)the agent was able to balance the bike in training (b) Total time required to solve the task for TA-TL(ours), UMA-TL (jumpstart) and RLThe states of the bicycle task are angle and angular velocity of the handlebar and the bike fromvertical (θ, θ, ω, ω) respectively. For the given state the agent is in, it chooses a continuous action ofapplying torque to the handlebar, T ∈ [-2N m, 2Nm] trying to keep the bike upright. The detailsof bicycle dynamics are beyond the scope of this paper; interested readers are referred to Randlov &AlStrom (1998); AStrom et al. (2005), and references therein.
Figure 9: Policy Transfer from Mountain Car to Inverted Pendulum: (a) Average Rewards and (b)Training lengthWe also demonstrate the cross-domain transfer between mountain car to an inverted pendulum. Thesource and target task are characterized by different state and action space. The source task MCis a benchmark RL problem of driving an underpowered car up a hill. The dynamics of MC aredescribed by two continuous state variables (x,X) where X ∈ [-1.2, 0.6] and X ∈ [-0.07, 0.07]and one continuous action F ∈ [-1, 1]. The reward function is proportional to the negative ofthe squared distance of the car from goal position. The target task is conventional IP with state(θ, θ) ∈ (-π, π) and action T ∈ [-1, 1]. We present the performance of transfer methods basedon sample efficiency in learning the target task and speed of convergence to the maximum averagereward. Similar to the bicycle domain transfer Figure 9a and 9b shows the quality of transfer forATL through faster convergence to average maximum reward with lesser training samples comparedto UMA-TL and RL methods.
