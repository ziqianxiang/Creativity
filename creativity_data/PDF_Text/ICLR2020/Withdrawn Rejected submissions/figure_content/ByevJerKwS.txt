Figure 1: Reconstruction of the individual onthe left by attacking three face recognition mod-els (logistic regression, one-hidden-layer and two-hidden-layer neural network) using the existingattack algorithm in (Fredrikson et al., 2015)datasets, although may not contain the target individuals, still provide rich knowledge about how aface image might be structured; extraction and proper formulation of such prior knowledge will helpregularize the originally ill-posed inversion problem. We also move beyond specific attack algorithmsand explore the fundamental reasons for a modelâ€™s susceptibility to inversion attacks. We show thatthe vulnerability is unavoidable for highly predictive models, since these models are able to establisha strong correlation between features and labels, which coincides exactly with what an adversaryexploits to mount MI attacks.
Figure 2: Overview of the proposed GMI attack method.
Figure 3: Qualitative comparison of the proposed GMI attack with the existing MI attack (EMI), thepure image inpainting method (PII). The ground truth target image is shown in 1st col.
Figure 4: (a)-(c): The performance of the GMI attack against models with different predictive powersby varying training size, dropout, and batch normalization, respectively. (d) Attack accuracy ofthe GMI attack against models with different DP budgets. Attack accuracy of PII is plotted as abaseline.
Figure 5: Visualization of the recovered input im-ages by the GMI and the EMI attack.
