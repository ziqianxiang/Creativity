Figure 1: The Axial Transformer model for 2-dimensional tensors. Before sampling a channel weencode all previous channels and frames with 8 blocks of unmasked row and unmasked column at-tention (left). Then, for each row, we apply 4 blocks of unmasked row and masked column attentionto integrate the previously sampled rows for the active channels into our encoded representation(middle). Finally, we shift the encoded representation up to make sure the conditioning informationsatisfies causality, and we run the inner decoder consisting of 4 blocks of masked row attention tosample a new row in the image (right).
Figure 2: Types of axial attention layers that are the building blocks of the Axial Transformer. Theblue locations correspond to the receptive field of the output red location.
Figure 3: Arrangement of inputs to the encoding network of the Axial Transformer. Previouslyavailable or generated channels of an image or video are sequentially stacked in the input. A variablenumber of padding planes are used as placeholders for future generated channels. A final integerplane signals to the Axial Transformer the channel that is being generated at that step.
Figure 4: 64 × 64 ImageNet samples at temperature 1.08Under review as a conference paper at ICLR 2020Figure 5: 32 × 32 ImageNet samples at temperature 0.99Figure 6: 15 × 64 × 64 BAIR Robot Pushing samples at temperature 1.09Under review as a conference paper at ICLR 2020ReferencesJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprintarXiv:1607.06450, 2016.
Figure 5: 32 × 32 ImageNet samples at temperature 0.99Figure 6: 15 × 64 × 64 BAIR Robot Pushing samples at temperature 1.09Under review as a conference paper at ICLR 2020ReferencesJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprintarXiv:1607.06450, 2016.
Figure 6: 15 × 64 × 64 BAIR Robot Pushing samples at temperature 1.09Under review as a conference paper at ICLR 2020ReferencesJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprintarXiv:1607.06450, 2016.
