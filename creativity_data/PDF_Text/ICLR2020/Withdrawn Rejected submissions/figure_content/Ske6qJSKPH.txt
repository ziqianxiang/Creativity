Figure 1: Loss surface and trajectories for 500 steps of gradient descent with HD and RTHO forBeale function (left) and (smoothed and simplified) Bukin N. 6 (right). Center: best objective valuereached within 500 iterations for various values of β that do not lead to divergence.
Figure 2: Left: schedules found by LRS-OPT (after 5000 iterations of SGD) on 4 different random seeds.
Figure 3: Results of VGG-11 on CIFAR-10, and SGDM as the inner optimizer concerning: (Left) accuracy,(Center) loss of the objective function on the validation Set and (Right) generated learning rate schedule for eachmethod.
Figure 4: Results of ResNet-18 on CIFAR-100, and Adam as the inner optimizer concerning: (Left) accuracy,(Center) loss of the objective function on the validation set and (Right) generated learning rate schedule for eachmethod.
Figure 5:	Comparison between optimized and online schedules for the remaining three seeds. Foreach method, we report the schedule generated with the hyper-learning rate (or step-size for adaptingit) that achieves the best final validation accuracy.
Figure 6:	Results of ResNet-18 on CIFAR-100, and SGDM as the inner optimizer concerning: (Left) accuracy,(Center) loss of the objective function on the validation set and (Right) generated learning rate schedule for eachmethod. In cyan We report the results for MARTHE with no = 0.
Figure 7:	Results of VGG-11 on CIFAR-10, and SGDM as the inner optimizer concerning: (Left) accuracy,(Center) loss of the objective function on the validation set and (Right) generated learning rate schedule for eachmethod.
Figure 8:	Results of ResNet-18 on CIFAR-100, and Adam as the inner optimizer concerning: (Left) accuracy,(Center) loss of the objective function on the validation set and (Right) generated learning rate schedule for eachmethod.
Figure 9: Sensitivity analysis of inadaptive MARTHE with respect to η0 and μ fixing the value of βto 10-7 (Left) and 10-8 (Right). We used VGG-11 on CIFAR-10 with SGDM as optimizer. Darkercolors mean lower final accuracy.
