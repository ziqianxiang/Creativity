Figure 1:	FleXOR components added to the quantized DNNs to compress quantized weights throughencryption. Encrypted weight bits are decrypted by XOR gates to produce quantized weight bits.
Figure 2:	Encrypted weight bits are sliced and reconstructed by a XOR-gate network which can beshared (in time or space). Then quantized bits after XOR gates are finally reshaped.
Figure 3: Test accuracy and training loss (average of 6 runs) with LeNet-5 on MNIST when M㊉is randomly filled with {0, 1}. Nout is 10 or 20 to generate, effectively, 0.4, 0.6, or 0.8 bit/weightquantization.
Figure 4:	Using the same weight storage footprint, FleXOR enables various internal quantizationschemes. (Left): 1-bit internal quantization. (Right): 3-bit internal quantization with 3 differentM㊉ configurations.
Figure 5:	Test accuracy comparison on ResNet-32 (for CIFAR-10) using various XOR trainingmethods. Nout = 10, Nin = 8, q = 1 (thus, 0.8bit/weight), and Stanh = 10.
Figure 6:	Test accuracy and distribution of encrypted weights (at the end of training) of ResNet-32on CIFAR-10 using various Stanh and the same Nout , Nin , and q as Figure 5.
Figure 7: Test accuracy of ResNet-32 on CIFAR10 using learning rate warmup and various q, Nin ,and Nout . The results on the right side are obtained by 5 runs.
Figure 8: The left graph shows hyperbolic tangent (y = tanh(x ∙ Stanh)) graphs with various scalingfactors (Stanh), . The right graph shows their derivatives. These graphs support the arguments of‘Optimize Stanh ’ in Section 4.
Figure 9: Test accuracy and training loss of LeNet-5 on MNIST when number of ‘1’s in each row ofM㊉ is fixed to be 2 (Ntap = 2). Nout is 10 or 20 to generate, effectively, 0.4, 0.6, or 0.8 bit/weightquantization. With low Ntap of M㊉，MNIST training presents less variations on training loss andtest accuracy that in Figure 3.
Figure 10:	Distribution of encrypted weight values for FC1 layer of LeNet-5 at different trainingsteps using Stanh = 100 and Nout = 10. (Left): M㊉ is randomly filled (Ntap ≈ Nin/2). (Right):Ntap = 2 for every row of M ㊉.
Figure 11:	Distributions of encrypted weights (at the end of training) in various layers of ResNet-32 on CIFAR-10 using various Stanh and the same Nout, Nin, and q as Figure 5. The ResNet-32network mainly consists of three layers according to the feature map sizes: Layer1, Layer2 andLayer3.
Figure 12: Comparison of various hyper-parameter choices for CIFAR-10 or ImageNet.
Figure 13: Test accuracy of ResNet-32 on CIFAR-10 using learning rate warmup (for 100 epochs)and Nout = 2015Under review as a conference paper at ICLR 2020		Bits/Weight	ReSNet-20		ReSNet-32		Comp. RatioFP		32	91.87%	-	92.33%	-	10XNin=10,Nout=10		10	90.21%	-1.66%	91.40%	-0.93%	-29.95 ×Nin	=9, Nout=10	0.9	90.03%	-1.84%	91.28%	-1.05%	31.82×Nin	=8, Nout=10	0.8	89.59%	-2.28%	90.86%	-1.47%	35.32×Nin	=7, Nout =10	0.7	89.24%	-2.63%	90.54%	-1.79%	39.68 ×Nin	=6, Nout =10	0.6	89.21%	-2.66%	90.41%	-1.92%	45.27 ×Nin	=5, Nout =10	0.5	88.54%	-3.33%	89.82%	-2.51%	52.70 ×Table 4: Weight compression comparison of ResNet-20 and ResNet-32 on CIFAR-10 when Nout =10. Parameters and recipes not described in the table are the same as in Table 1. We also presentcompression ratio for fractional quantized ResNet-32 when one scaling factor (α) is assigned to eachoutput channel.
Figure 14: Test accuracy (Top-1) of ResNet-18 on ImageNet using FleXOR.
