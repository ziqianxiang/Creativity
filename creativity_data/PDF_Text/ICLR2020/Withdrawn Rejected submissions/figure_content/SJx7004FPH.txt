Figure 1: An illustration of the L°-vicinity and the sparse L°-vicinity of a pre-trained parameterin a three-dimensional parameter space. The L0-vicinity is continuous and contains parameters thatare L0-close, whereas the sparse L0-vicinity is a discrete subset of L0-close parameters that are alsoL0-small.
Figure 2: L1 - and angular distances in parameter subspaces between pre-trained and fine-tunedweights. Shown are metrics across the 12 encoder stack layers for the self-attention projectionmatrices (WQ, WK, WV and WD) and feed-forward matrices (WI and WO). The results presentedhere are for MNLI fine-tuning, but similar patterns are observed across all GLUE tasks.
Figure 3:	Performance of supermask fine-tuned models across GLUE tasks. We show the mean ofperformance metrics across 10 independent Bernoulli samplings. Note the baseline performance foreach task marked by the leftmost end of each curve.
Figure 4:	Supermask sparsity levels across layers. Shown is the low-sparsity MNLI supermask witha global sparsity level of 12.9%, but similar patterns are observed across all GLUE tasks.
Figure 5: Pruned weight distributions, compared between supermask and magnitude-based pruning.
Figure 6: Low-sparsity supermask performance, i.e. task performance of super-masks initialized at0% sparsity, compared against baseline.
Figure 7: Fractions of overlap of zero elements in supermasks across GLUE tasks, compared torandomly generated masks. Each value in the grid shows the fraction of pruned elements in onetask (horizontal axis) that are also pruned in the other (vertical axis). Here, we show low-sparsitysupermasks (initialized at 0% sparsity) and compare the masks in the value layer of the first encoder,which is one of the most sparse layers in the entire model.
Figure 8:	Iterative pruning during fine-tuning. We plot the evaluation performance at sparsity levelsfrom 10% to 90% across GLUE tasks. Note the baseline performance for each task marked by theleftmost end of each curve (0% sparsity).
Figure 9:	The supermasks are initialized to a soft magnitude-based pruning mask and the sparsitylevel shifts during supermask training. This figure shows the initial sparsity level plotted against thefinal sparsity level. We note that, at lower initial sparsity levels, the supermask is pushed to a greatersparsity level, whereas at higher sparsity levels, the supermask is pushed to a lower sparsity level.
Figure 10: Scaling of parameter distance with the number of fine-tuning iterations. We find thatangular distance correlates with the amount of fine-tuning (Table 1). Each data point corresponds toa different GLUE task.
