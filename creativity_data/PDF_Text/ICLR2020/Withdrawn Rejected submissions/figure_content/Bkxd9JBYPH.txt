Figure 1: Main idea. (a) Covariance matrix Σ for DNNs is intractable to infer, store and sample (anexample taken from our MNIST experiments). (b) Our main insight is that the spectrum (eigenvalues)of information matrix (inverse of covariance) tend to be sparse. (c) Exploiting this insight a LaplaceApproximation scheme is devised which applies a spectral sparsification (LRA) while keepingthe diagonals exact. With this formulation, the complexity becomes tractable for sampling whileproducing more accurate estimates. Here, the diagonal elements (nodes in graphical interpretation)corresponds to information content in a parameter whereas the corrections (links) are the off-diagonals.
Figure 2: Sparse Information Matrix. We perform a low rank approximation on Kronecker factoredeigendecomposition that preserves Kronecker structure in eigenvectors for two reasons: (a) reducingdirectly (UA Θ UG)i：L is memory-wise infeasible, and (b) sampling scheme then only involves matrixmultiplications of smaller matrices UA1:a and UG1:g . Notations on indicing rules are also depicted.
Figure 3: Illustration of algorithm 1. A low rank approximation on Kronecker factored eigende-composition that preserves Kronecker structure in eigenvectors constitutes steps 1 to 5.
Figure 4: Uncertainty on toy regression. The black dots and the black lines are data points (x, y).
Figure 5: Effects of Low Rank Approximation in Frobenius norm of error. This measure isnormalized from 0 to 1. Lower the better. Laplace based methods such as EFB, Diag, KFAC andDEF are compared in terms of diagonal, off-diagonal and overall resulting approximation error toexact block diagonal hessian. Eigenvalue histogram is also plotted.
Figure 6: Normalized Entropy histogram (left:DEF Laplace and right: deterministic) on MNISTvs notMNIST experiments. Our method clearlyseparates the out-of-distribution and wrongly clas-sified samples (out-dist) to the correctly classifiedsamples from in-domain distribution (in-dist).
Figure 7: Toy regression uncertainty. User-set Laplace means user-set τI for covariance estimation.
Figure 8: Toy regression uncertainty and covariance visualization (only the first layer is shownhere). OKF Laplace means using the left hand side of equation 3 without further approximation (onlypossible with this small model and data-set).
Figure 9: Visualization of the approximate Hes-sian with different data points.
Figure 10: Grid search results. For Diag (left two figures) and KFAC Laplace (right two) anextensive grid search has been conducted to ensure fair comparison. Here, we report the results withpseudo observation term of 50000 on MNIST. This ensures that a main difference to DEF Laplace isthe expression for model uncertainty as the inference and network architectures are kept the same.
