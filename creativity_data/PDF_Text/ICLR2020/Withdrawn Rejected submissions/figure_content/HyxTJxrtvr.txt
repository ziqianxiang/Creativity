Figure 1: An illustration of our video instance segmentation model. Clockwise from top left: inputimage, predicted multi-object instance segmentation, visualisation of the high-dimensional embed-ding and predicted monocular depth.
Figure 2: Our video instance segmentation and depth model architecture. The embedding, zt , istrained to explicitly encode appearance, motion and geometry cues in order to predict an instanceembedding and monocular depth prediction.
Figure 3: Partial occlusion. The segmented brown car is correctly segmented even when beingpartially occluded by the segmented red car, as the embedding contains past temporal context and isaware of the motion of brown car.
Figure 4: Continuous tracking. The segmented pink and purple cars are accurately tracked evenwith missing detections.
Figure 5: Total occlusion. The segmented green car correctly tracked, even though it was com-pletely occluded by another car.
Figure 6: Video instance segmentation of parked cars.
Figure 7: Video instance segmentation of other traffic.
Figure 8: Failure case: the vehicle is segmented into two separate instances.
Figure 9: Failure case: two far-away cars are segmented as one instance.
Figure 10: Without depth, the car circled in red is wrongly tracked in frame 5 and 9, while our modelcorrectly tracks it as the network has learned a consistent embedding based not only on appearance,but also on 3D geometry. Also, the RGB projection of the embedding from our model is considerablybetter and much more structured.
Figure 11: Without depth, the circled car merges into the red-segmented car, while our model doesnot as there is a significant difference in depth between the two cars.
Figure 12: The model without depth is not able to handle complete occlusion, while ours can.
