Figure 1: The proposing MLA-GAN model is illustrated along with multi-manifold learning GANs(abbr. mmGANs) and unsupervised manifold alignment methods (abbr. UMA). In MLA-GAN, thelatent features of cats and dogs manifolds are aligned. This is achieved by restricting the inversesof the generative maps (f(1))-1, (f(2))-1 to be represented by a common encoder hE (see Eq. 1).
Figure 2: The generators and the encoder of a linear layer are illustrated. Generation: A latent pointz is transformed by the same weight U, then different biases a(1) and a(2) are added to generate x(1)and x(2) for different manifolds. The biases are regularized to have the same tangential component(to the column space of W or U>) ak to comply with Eq. 1. Encoding: The points x(1) and x(2)lying on different manifolds are mapped to the same z if they have the same tangential componentsxk . Note this becomes false if the biases are not regularized.
Figure 3: Images generated from the trained MLA-GANs. Each i-th row presents samples fromthe i-th manifold (only 8 out of 20 are shown for 3D-Chair) and each column presents the samplesgenerated from the same latent code z, which is randomly sampled from p(z). See also Appx. J.
Figure 4: Disentangled features. Images are arranged the same as Fig. 3, except the columns showlinear changes in the latent space along the first eigenvector of the disentanglement score analysis(see Sec. 4.2). Slant, width (MNIST), height and brightness (3D-Chair) components are shown.
Figure 5: Style transfer results3. In each source image set, the fourth and the fifth images are derivedfrom the third image by adding frames, rectangles or changing colors. The derived source imagesare clearly off the trained distribution, but their style-transfer results are good and consistent withthe original style-transfer results (highlighted in red boxes).
