Figure 1: An overview of the MAHRL approach. From left to right: the NC state includes the relative goalposition and distance, an egocentric velocity field capturing the relative velocity of obstacles and other agents,and the physical character link positions and linear velocities; for each agent this state is input to a multi-layerCNN, including two dense hidden layers, and outputs actions-the value function uses a similar network. Thesehigh-level actions are in the form of two-step plans dictating future foot placements; the LC consumes thesefootstep plans as g, which produces the angle-axis targets for joint PD controllers.
Figure 2: Comparative study of the learning curves of MAHRL, MADDPG, and PPO in the Mall scenario.
Figure 3: Comparative study of the learning curves of MAHRL, MADDPG, and PPO in the Pursuit scenario.
Figure 4: Comparative study of the learning curves of MAHRL based on PPO (blue) and MAHRL using TD3(red) in the Soccer scenario. The yellow and green agents are the same team, while the blue and red on the otherteam.
Figure 6: Comparative analysis of collisions counts across all baselines, MADDPG, MAHRL with and withoutheterogeneous agents, and PPO in the pursuit/tag scenario. MAHRL outperforms both MADDPG and PPO. Inthis game, the collisions are indicators of poor policies leading to negative collisions during pursuit and evasion.
Figure 8: Rasterized images from the mall environment with humanoid agents navigating and avoiding eachother while seeking goals. A video for this example can be found here.
Figure 9: (a) Agents reaching series of targets in arbitrary environments (images in raster order). (b) Egressscenarios with a group of (left) 5 and (right) 21 agents. The density of the second group results in a physics-based bumping, pushing, and trampling.
Figure 10: The performance of the method from two quantitative perspectives, (a) the computational perfor-mance with respect to agent count and (b) the generalization performance with respect to average reward value.
Figure 7: Rasterized overlays from the pur-suit environment, where the pursuer agents(red) learn to work together to corner andtackle the navigating agent (blue). A videofor this example can be found here.
Figure 11: Hyperparameter exploration learning curves for the LC training.
Figure 12: The agent moves towards and reaches goals repeatedly (top-left to bottom-right). The learned modelproduces smooth goal seeking trajectories show in in blue.
Figure 13: Three examples demonstrating reciprocal collision avoidance. The size and position of the egocen-tric state sampling field relative of the agent is shown in (a).
Figure 14: Reciprocal collision avoidance with obstacles. Each agentâ€™s initial position is the target location ofthe other agent.
Figure 15: An agent reaching goals in arbitrary complex environments. The red circle indicates the final goal.
