Figure 1: Comparison of λ-Jeffreys for λ = 0.5 (red), Reverse KL (green), Forward KL (blue) andJSD (orange) divergences on the task of approximating a mixture of 4 Gaussians (black dashed line)with an equiprobable mixture of two Gaussians with learnable location and scale. Plots a)-c) showpairwise comparisons of optimal log-densities, the plot d) compares optimal densities themselves.
Figure 2: Comparison between λ-IJAE and λ-IJAE-L1, λ-IJAE-L2 models. We see that λ-IJAEresults form pareto frontier with respect to IS and LPIPS for different choice of λ.
Figure 3: Evaluation of λ-IJAE on CIFAR and TinyImageNet compared to baselines. Two metrics,LPIPS and IS metrics are considered to access reconstruction and sampling quality. Consideringboth metrics λ-IJAE achieves a better trade-off between reconstruction and sampling quality withindatasets.
Figure 4: Samples from models trained on CIFAR10 dataset. Images for baselines were obtainedrunning publicly available code.
Figure 5: Reconstructions on the CIFAR10 dataset for IJAE model and closest baselines. Reconstruc-tions for baselines were obtained running publicly available code.
Figure 6: Samples from models trained on TinyImagenet dataset. Images for baselines were obtainedrunning publicly available code.
Figure 7: Reconstructions on the TinyImagenet dataset for IJAE model and closest baselines. Recon-structions for baselines were obtained running publicly available code.
