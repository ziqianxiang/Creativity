Figure 1: Comparison between conventional quantization and hybrid weight representation (HWR).
Figure 2: Overall processes of the hybrid weight representation (HWR). The weights are initializedfrom a full precision model trained with ReLU1 activation. First of the centralized quantization(CQ), the weights are quantized with the weight ridge (WR) as a regularizer. By WR, the quantizedweights are also centralized under threshold and categorized into ternary weights (TW) and sparse-large weights (SLW) by threshold. Second, selective quantization (SQ) is applied to fine-tune theweights of the previous step. Finally, SLW is encoded by one usable state of TW as a prefix.
Figure 3:	Specifications of weighted ridge (WR). (a) plots both a normal L2 and a part ofL1 (pL1).
Figure 4:	In the selective quantization (SQ), two different quantization functions are applied foreach category (TW and SLW).
Figure 5:	Encoding trees of hybrid weight representation (HWR). In HWR, we represent TW withvalues while SLW with indices of values. The number of bits and quantization steps of TW (bt , st)are fixed to 2, 3. In contrast, them of SLW (bsl, ssl) can be changed to (2, 4), (3, 8), or (4, 16).
Figure 6: The trade-off between accuracy and model size in various λ2(2,1, 2, 1, 1,得).
Figure 7: The weight distributions of each method.
Figure 8: Layer-wise comparison of the percentage of SLW and the maximum absolute value.
Figure 9: Before add operations in ResNet, the outputs of last convolution layers from each blockare scaled by a scale factor r, derived from the maximum absolute values of convolution layers.
Figure 10: Computing architecture for HWR in inference time. According to the TW of each weight(W1), the operation for input (X1) is determined. There are two operations such as sign assignment(sign asgmt) and integer multiplication (MUTT) for each type, TW and SLW.
Figure 11: Computing architecture for HWR in inference time. There are two weight matrices suchas a dense matrix WTW and a sparse matrix WSLW. The original weight matrix can be restored bysum of WTW and WSLW . In inference time, sign assignment operation is carried out with WTWwhile sparse convolution is carried out with WSLW. The output of a layer is sum of outputs fromtwo operations.
Figure 12: Another version of WR, using the part of exponential lasso (pEL1) instead of pL1. (a)denotes both L2 and pEL1. The version of WR is addition of them as in (b). (c) is the derivative ofthe version of WR.
