Figure 1: Depiction of TabNet,s sparse feature selection for Adult Census Income prediction (DUa &Graff, 2017). TabNet employs multiple decision blocks that focus on processing a subset of inputfeatures for overall decision making. Two decision blocks shown as examples process the featuresthat are professional occupation related and investment related in order to predict the income level.
Figure 2: Illustration of decision tree-like classification using conventional neural network blocksand the corresponding two-dimensional manifold (xi and x2 are the input dimensions, and a and dare constants). By employing multiplicative sparse masks to inputs, the relevant features are selected.
Figure 3: (a) TabNet architecture, composed of feature transformer, attentive transformer and featuremasking at each decision step. Split block divides the processed representation into two, to beused at the attentive transformer of the subsequent step and to be used towards construction ofthe overall output. At each decision step, its feature selection mask can provide insights about itsfunctionality, and the masks can be aggregated (using the Agg. block) ultimately to obtain globalfeature important attribution behavior. (b) A feature transformer block example - 4-layer network isshown, where 2 of them are shared across all decision steps and 2 of them are decision step-dependent.
Figure 4: Feature importance masks M[i] (that indicate which features are selected at ith step) andthe aggregate feature importance mask Magg showing the global instance-wise feature selection forSyn2 and Syn3 datasets from (Chen et al., 2018). Brighter colors show a higher value. E.g. for Syn2dataset, only four features (X3-X6) are used.
Figure 5: Feature importance masks M[i] (that indicate which features are selected at ith step) andthe aggregate feature importance mask Magg showing the global instance-wise feature selection forSyn4 and Syn6 datasets from (Chen et al., 2018). Brighter colors show a higher value. E.g. for Syn4dataset, the chosen features depend on the value of X11.
Figure 6: (a) Comparison to previous work for the ratio of the feature importance of “Odor” featureto all the features of the top feature for the mushroom edibility prediction (DUa & Graff, 2017)(task: classify whether a mushroom is edible or poisonous). With “Odor” feature only, > 98.5% testaccuracy can be obtained, so a high feature importance is expected to be assigned to it, as observedwith TabNet. (b) Comparison to previous work for importance ranking of features in the Adult CensusIncome dataset (Dua & Graff, 2017) (task: distinguish whether a person’s income is above $50,000).
Figure 7: Simplified diagram for TabNet feedforward pass for an input with 3 features, assumingNsteps = 2. At the first step, the model selects only the first feature, and applies feature processingon it. At the second step, the model selects the last feature, and applies the feature processing on it.
Figure 8: Decoder architecture to transform the encoded representation into reconstructed tabulardata features. Each decision step is composed of a feature transformer block (see Fig. 3), and afully-connected layer.
