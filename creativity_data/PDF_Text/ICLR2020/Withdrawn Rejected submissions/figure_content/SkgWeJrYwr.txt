Figure 1: (a): Redundant but not irrelevant; (b): Irrelevant but not redundantFilter methods are better at identifying redundant features while wrapper methods are better at iden-tifying irrelevant features, and this highlights the power of embedded methods as they utilize aspectsof both in feature selection as mentioned in Bolon-Canedo et al. (2013). Since most wrapper meth-ods do not take advantage of filter method based identification of redundant features, there is a needto incorporate a filter based technique to identify redundant features into wrapper methods, whichwe address using autoencoders.
Figure 2: Autoencoder trained to eliminate 1 feature from a set of k featuresThis hidden layer can either be dense, LSTM, or of other types depending on the data we are dealingwith. To evaluate a feature, we set its corresponding values in the training set to 0 and pass theset into the autoencoder. We then take the Mean Squared Error (MSE) between the output and theoriginal input before the values corresponding to the evaluated feature were set to 0, and performthis for each of the k features separately. The feature with the lowest MSE is the least salient featurebecause the other features in the latent space consisting of k - 1 neurons were able to compensate- with least reconstruction error - for the loss of this feature. We refer to this MSE as a featureâ€™sRedundancy Score.
Figure 3: Accuracy vs Feature Count plots for the final models trained with the selected features forthe (a) MNIST, (b) Reuters, (c) Wisconsin Breast Cancer, and (d) RadioML2016.10b datasets.
Figure 4: (a): Toy example from TensorFlow Playground; (b) Feature 1 and (c) Feature 2.
Figure 5: Input layer weight magnitudes after training for (a) 10, (b) 100, and (c) 1000 epochs.
