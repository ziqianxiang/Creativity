Figure 1: a) Augmenting the loss function with AR constraints allows an agent to reach a targetpolicy by following different paths. Attractive and Repulsive policies represent any other agent’spolicy. b) General flow of the proposed ARAC strategy.
Figure 2: Agent trained to imitate a target while avoiding a repulsive policy using a proactive strategy.
Figure 3: Average return and one standard deviation on 5 random seeds across 7 MuJoCo tasks forARAC against single SAC agents (with and without NFs). Curves are smoothed usingSavitzky-Golay filtering with window size of 7.
Figure 4: Average return and one standard deviation on 5 random seeds across 8 MuJoCo tasks.
Figure 5: Mapping in two-dimension space (t-SNE) of agents’ actions for two arbitrary states. Eachcolor represents a different agent.
Figure 6: Comparison of ARAC agents using (1) AR with radial flows, (2) AR with only the base(Gaussian) policy and (3) no AR with radial flows.
Figure 7: Average return and one standard deviation on 5 random seeds across 7 MuJoCo tasks forARAC against baselines. Curves are smoothed using Savitzky-Golay filtering with window size of 7.
Figure 8: Single state didactic illustration of attraction-repulsion operators. Comparing behavior ofNF policy against Gaussian policy with learned variance under a repulsive constraint.
