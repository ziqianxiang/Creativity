Figure 1: Distribution of the latents of a flow prior with β = 0 after training on MNIST withdim(Z) = 2. (a) Distribution of fθ (Eφ (x)) for each x in the test set. Datapoints are coloredaccording to class. (b) Distribution of Eφ (x) for each x in the test set. (c) Same as (b) but withoutclass coloring. (d) Distribution of f—1 (z0) where z0 〜N(0, I). It can be seen that while theautoencoder learns a complex latent distribution with classes well separated, the normalizing flow isable to learn a close match.
Figure 2: Experimental results using different values of β for different models. X-axis represents β,not to scale. Y-axis represents FID score.
Figure 3: Interpolation between samples from the CIFAR-10 test set for a flow prior with fixed γand β = 0. Leftmost and rightmost columns contain real images from the test set before encoding,middle columns contain interpolations between them.
Figure 4: PCA of Eφ(x) for each x in the CelebA training set. The black line shows the pathtaken when interpolating between two points inZT , while the red line shows the path taken wheninterpolating between the same two points in Z0 . The black line takes the most direct route, whilethe red line follows a path of higher average density.
Figure 5: Adding glasses to a face. Top row of each set: interpolation in Z0 . Bottom row of eachset: interpolation in ZT. Leftmost column: the original image before encoding. The top row showsa more abrupt change towards the end, while the bottom row is closer to a constant rate of change.
Figure 6: Interpolation between two random samples from the training set. Top row of each set:interpolation in Z0. Bottom row of each set: interpolation in ZT. Leftmost and rightmost columns:the original images before encoding. The top row shows more realistic images as a result of theinterpolation path having higher average density.
Figure 7: Conditional samples using Variational U-net. Top row: original. Bottom row: proposed.
