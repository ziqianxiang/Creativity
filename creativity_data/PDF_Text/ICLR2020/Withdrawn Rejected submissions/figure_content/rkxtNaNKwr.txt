Figure 1: Team represented asmulti-headed policy net πPolicy Gradient: The procedure describedso far resembles a standard EA except thateach agent k stores each of its experiencesin its associated replay buffer (Rk ) insteadof just discarding it. However, unlike EA,which only learns based on the low-fidelityglobal reward, MERL also learns from theexperiences within episodes of a rollout us-ing policy gradients. To enable this kindof "local learning", MERL initializes onemulti-headed policy network πpg and onecritic Q. A noisy version of πpg is thenused to conduct its own set of rollouts inthe environment, storing each agent k’s ex-periences in its corresponding buffer (Rk)similar to the evolutionary rollouts.
Figure 2: High level schematic of MERL highlightingthe integration of local and global reward functionsAgent-Specific Reward Optimization:Crucially, each agent’s replay buffer is kept separate from that of every other agent to ensure diversityamongst the agents. The shared critic samples a random mini-batch uniformly from each replay bufferand uses it to update its parameters using gradient descent. Each agent πpkg then draws a mini-batchof experiences from its corresponding buffer (Rk ) and uses it to sample a policy gradient from theshared critic. Unlike the teams in the evolutionary population which directly seek to optimize theteam reward, πpg seeks to maximize the agent-specific local reward while exploiting the experiencescollected via evolution.
Figure 3: Illustration of environments tested (Lowe et al., 2017; Rahmattalabi et al., 2016)We adopt environments from (Lowe et al., 2017) and (Rahmattalabi et al., 2016) to perform ourexperiments. Each environment consists of multiple agents and landmarks in a two-dimensionalworld. Agents take continuous control actions to move about the world. Figure 3 illustrates the fourenvironments which are described in more detail below.
Figure 6: Performance on the Rover Domain.
Figure 7: MATD3’s performance for dif-ferent scalarization coefficientsTeam Behaviors: Figure 8 illustrates the trajectories gen-erated for the Rover Domain with a coupling of n = 3. The trajectories for partially and fully trainedMERL are shown in Figure 8 (a) and (b), respectively. During training, when MERL has not discov-ered team success (no POIs are successfully observed), MERL simply optimizes the agent-specificreward for each agent. This allows it to reach trajectories such as the ones shown in 8(a) where eachagent learns to go towards a POI.
Figure 9: Selection rate for migrat-ing policies6 ConclusionIn this paper, we introduced MERL, a split-level algorithm that leverages both agent-specific andteam objectives by combining gradient-based and gradient-free optimization. MERL achieves thisby using a fast policy-gradient optimizer to exploit dense agent-specific rewards while concurrentlyleveraging neuroevolution to tackle the team-objective.
Figure 10: Evolutionary Algorithm Population size sweep on the rover domain with a coupling of 3.
Figure 11: Evolutionary Strategies population size sweep on the rover domain with a coupling of 3Figure 11 compares ES with varying population sizes in the rover domain with a coupling of 3. Sigmafor all ES runs are set at 0.1. Among the ES runs, a population size of 100 yields the best resultsconverging to 0.1 in 100-millions frames. MERL (red) on the other hand is ran for 2-million framesand converges to 0.48.
Figure 12: Evolutionary Strategies Noise magnitude (sigma) sweep on the rover domain with acoupling of 3Figure 12 compares ES with varying variance of noises (sigma) that control the magnitude of eachperturbation. The experiments are conducted in the rover domain with a coupling of 3 with apopulation size of 100. Among the ES runs, a sigma of 0.1 yields the best results converging to 0.1in 100-millions frames. MERL (red) on the other hand is ran for 2-million frames and converges to0.48.
Figure 13: Predator-prey with varying numbers of prey. Prey are 30% faster than the predatorsFigure 13 shows the results of running MATD3 with varying number of prey in the predator-preydomain. The experiments are ongoing.
