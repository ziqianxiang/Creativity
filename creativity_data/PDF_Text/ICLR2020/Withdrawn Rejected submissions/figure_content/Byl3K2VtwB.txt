Figure 1: Learning embeddings by detecting communities.
Figure 2: Effect of batch size5.2	Community detection evaluationSetup. Next, we aim to show the applicability of normcut loss to a graph-like setting, such aslearning word embeddings. Here, the nodes are words and the connection weight between twowords is measured by the following “kernel” function:k(wi, wj)log(#(wi, wj))ιog( log(#Wi)log(#Wj),0)where #wi is the number of times word wi appears in the corpus, while #(wi , wj ) is the numberof times the words appear together. The adjacency matrix obtained using the above function isthe PPMI matrix, a well-established concept in NLP (Levy & Goldberg, 2014). Following Yin& Shen (2018), we construct a word corpus of 10000 words that appear >100 times in the Text8corpus (Mahoney, 2011). Words are said to appear together if they are within a window of five.
Figure 3: Embedding size vs. #communitiesFigure 4: Hierarchy of words/topics5.3	Effects of minibatch trainingSetup. We evaluate the effects of minibatch training on the classification accuracy on three citationnetworks by varying the batch size from 20 to 500.
Figure 4: Hierarchy of words/topics5.3	Effects of minibatch trainingSetup. We evaluate the effects of minibatch training on the classification accuracy on three citationnetworks by varying the batch size from 20 to 500.
Figure 5: Relationship between embedding size and #communitiesA	Additional experimental resultsA. 1 Relationship between embedding size and number of communitiesTaking up the discussion in Section 5.4, Figure 5 provides additional results with different valuesof p for the Stochastic Block Model. We observe that the ROC scores increase with p. This isexpected as larger p values correlate with a clearer community structure, which yields better linkprediction. On the other hand, with smaller p values, we observe the dimensionality trade-off better.
