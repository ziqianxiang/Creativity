Figure 1: Overall structure of our model. TED first pretrains on news articles and then finetuneswith theme modeling and denoising. (from left to right).
Figure 2: Theme modeling is essentially a semantic classifier. The input sentence pair is first pro-cessed by adding a “class” token in the beginning and a “separation” token in between. Then thesentence pair is fed into the transformer encoder, and then a linear classifier.
Figure 3: Proportion of novel grams in summaries on the CNN/DM test set. We compare threesystems, TED, PGNet and reference summaries. Numbers of PGNet are computed from its publiclyreleased output.
Figure 4: An example of a generated summary by TED. The reference summary and parts of theinput article are also included.
