Figure 1: Shape of S-APL activation during training a simple network of MLPs on MNIST dataset.
Figure 3: Training loss trajectory for different S-APL initializations compared to fixed ReLU andleaky ReLU.
Figure 4: tSNE visualization of the pre-softmax layerâ€™s outputs. Left: a ReLU activated Lenet5architecture trained on CIFAR-10. Right: same network trained with S-APL activation functions.
