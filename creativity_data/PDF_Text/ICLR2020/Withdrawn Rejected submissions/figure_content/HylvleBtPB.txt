Figure 1: Learning curve of distance D. Training phase 1 (left): fix both model, learn T only.
Figure 2: An example of predicted key-points, with (below) and without (above) confident labelsReorder In absolute order prediction, we use an N-way classifier to predict the absolute positionof a token. In relative order prediction, a binary classifier is used to predict the relative order of apair of tokens. During inference, beam search is used to find the best order assignment of tokens, sothat the total cost is minimized. We simply use the probability p of the classifier output as the costof a particular assignment.
Figure 3: A example of order prediction. From left to right: (1) ground-truth absolute order. (2)predicted absolute order. (3) ground-truth relative order. (4) predicted relative order.
Figure 4:	An example illustrating CLCR in inference. For a pair of language (in this example, En-glish and Chinese), one can decide to perform transformation and reordering on one side, and per-form extraction on both sides. After training of the transformation (T), extraction (E) and reordering(R) model, for input text in one of the two languages, a sequence of operation is performed: Embed-T-E-R or Embed-E depending on the language. The generated sequence-of-vector representation(in the box with dashed line) is the same for text in both languages, assuming perfect T, E, and Rmodels.
Figure 5:	An example of zero-shot cross-lingual transfer with CLCR11Under review as a conference paper at ICLR 2020C Model details and more analysisIn our experiments, we use the ninth layer output of the 12-layer BERT-base model as mono-lingualcontextual representations. Based on the observation in (Tenney et al., 2019a), middle-high layers oftransformer language models typically exhibit good semantic properties. The English and ChineseBERT-base models we used are identical except the language of the corpus used to trained themodel. We also experimented with using different pre-trained language models, for example ERNIE(Sun et al., 2019) for Chinese (BERT-base is still used for English), but the cross-lingual transferperformance is inferior. This shows that for best aligning of representations in CLCR, the twomono-lingual representation models should share the same structure and objective in pre-training.
