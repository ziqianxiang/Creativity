Figure 1: Given five tasks to solve, there are many ways that they can be split into task groups for multi-task learning. How do we find the best one? We propose a computational framework that, for instance,suggests the following grouping to achieve the lowest total loss, using a computational budget of 2.5 units:train network A to solve Semantic Segmentation, Depth Estimation, and Surface Normal Prediction; trainnetwork B to solve Keypoint Detection, Edge Detection, and Surface Normal Prediction; train network C witha less computationally expensive encoder to solve Surface Normal Prediction alone; including Surface Normalsas an output in the first two networks were found advantageous for improving the other outputs, while the bestNormals were predicted by the third network. This task grouping outperforms all other feasible ones, includinglearning all five tasks in one large network or using five dedicated smaller networks.
Figure 2: The task groups picked by each of ourtechniques for integer budgets between 1 and 5.
Figure 3: The performance/inference time trade-off for var-ious methods. Data presented tabularly in Table 7.
Figure 5: Task affinities for multi-task learning vs.
Figure 6: Our experiments re-run on all 4-task subsets, then averaged.
