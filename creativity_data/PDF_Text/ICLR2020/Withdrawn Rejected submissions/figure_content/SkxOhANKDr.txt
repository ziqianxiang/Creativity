Figure 1: Overview of our generative cleaning network method for deep neural network defense.
Figure 2: Visualization of the generative cleaning process. The adversarial perturbation was pro-duced using PGD attack with maximum perturbation = 16/255.
Figure 3: Left: Classification accuracy of our method defense on BPDA attack with different quan-tization parameter. Right: results against white-box BPDA attack with 10 to 100 attack iterations.
Figure 4: Left: Classification accuracy of our method defense on PGD attack with 10 to 100 attackiterations. Right: results against white-box PGD attack with 0.01 to 1.0 adversarial epsilon.
