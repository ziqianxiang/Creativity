Figure 1: Expected returns achieved by linear policy with 2 parameters on Sparse MountainCardomain (background). Gradient is 0 in the dark blue area. Trajectories show the evolution of policyparameters over 1000 iterations of TRPO, with 5 random seeds. Same colors indicate the samerandom seeds. (a) Random-walk type behaviour observed when parameters are initialized usingGlorot initialization (Glorot & Bengio, 2010). (b) Convergence observed when parameters areinitialized in a region with gradients (1, 40).
Figure 2: Example of R3L exploration onsparse MountainCar. Green segments aresampled transitions, executed in simulation.
Figure 3: Results for classic control tasks, comparing our proposed method (R3L-TRPO/DDPG),vanilla TRPO/DDPG, and VIME-TRPO. Trendlines are the medians and shaded areas are theinterquartile range, taken over 10 randomly chosen seeds. Also shown is the average undiscountedreturn of successful trajectories generated with R3L exploration. The dashed offset at the start ofR3L-TRPO/DDPG reflects the number of timesteps spent on R3L exploration.
