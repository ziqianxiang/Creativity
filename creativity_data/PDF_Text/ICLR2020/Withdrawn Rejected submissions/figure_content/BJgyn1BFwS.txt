Figure 1: Convergence of the global robustness estimation wrt error tolerance e (decreasing valuesalong the x-axis). For each e We compute 50 empirical estimates (blue dots). As the error tolerancedecreases the estimated quantities converge to a tight cluster. The red line interpolates the numberof sampled images needed for the computation of each blue dot.
Figure 2: Analysis of empirical accuracy/robustness trade-off computed on 4000 deterministic FCNsand CNNs (each blue dot in the center plots represents the accuracy/robustness result for each net-work). Each set of plots has been labeled with the functions used to compute robustness. First row:analyses on the MNIST dataset. Second row: analyses on the Fashion-MNIST dataset. Third row:analyses on the CIFAR-10 dataset. We observe a negative trend between accuracy and robustness.
Figure 3: Quantitative analysis of global adversarial robustness on models trained with iterativemagnitude pruning. (a): robustness results on MNIST; (b): robustness results on Fashion-MNIST;(c): quantitative robustness on MNIST (top) and Fashion-MNIST (bottom). We observe that prun-ing leads to comparable trade-off wrt when pruning is not applied. The boxplots show a lack ofcorrelation between the proportion of weights pruned and global robustness.
Figure 4: Global robustness analysis of Bayesian NNs. (a): results on MNIST; (b): results onFashion-MNIST. Each plot is labeled With the notion of robustness used. In the rightmost plot, SGDwas measured with attacks an order of magnitude weaker as the models were more vulnerable.
