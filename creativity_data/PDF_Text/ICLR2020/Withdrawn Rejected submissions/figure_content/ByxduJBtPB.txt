Figure 1: We consider perfectly fitting training points using cubic splines while minimizingsmoothness of the function. (Left) depicts the true function f? and the mass Pxy on each point(x,y) size of the circles) (Middle) With a small number of standard training samples (circles) fromthe distribution depicted in (a), augmentating the dataset with local perturbations (crosses) causesthe augmented estimator to have larger error than the standard estimator due to being maximallysmooth while also fitting the augmented local perturbations. (Right) Our proposed X-regularizationregularizes the model predictions towards the predictions of a standard model (trained withoutperturbations) on unlabeled data, allowing for fitting both the local and global structure.
Figure 2: Illustration of the 3-D example described in Sec. 4.1.1. In (a), (b) We depict the errorsθaug 一 θ (green solid) and θstd 一 θ (blue solid) projected onto Null(∑std) that is spanned byeigenvectors eι and e2 of Σ with λ?》λι. Red lines depict the projection of parameter errors one2 which determines the bias. In (a) θ?》θ? and bias increases upon augmentation. In (b) θ?》θ?and bias decreases upon augmentation. (c), (d)The space of safe augmentation directions (orange)that don’t increase bias for a given θ? are cone-shaped, where the cone width depends on the alignmentofθ? with eigenvectors of Σ and the skew in eigenvalues of Σ.
Figure 3: Top 4 eigenvectors of Σ in the splines problem, representing wave functions in the inputspace. The eigenvalues corresponding to “global” eigenvectors which vary less over the domain arelarger, making errors in global dimensions costly in terms of test error.
Figure 4: Effect of data augmentation on test error as we vary the number of training samples. We plotthe difference in errors of the augmented estimator and standard estimator. In both the spline staircasesimulations and data augmentation with adversarial '∞ perturbations via adversarial training (AT)on Cifar- 1 0, we find that as we increase the number of samples, the increase in test error decreases.
Figure 5: Visualization of the effect of single augmentation points in the noiseless spline problemgiven an initial dataset XStd = {Φ(t): t ∈ {0,1,2,3,4}}. The standard estimator defined by XStd is linear.
Figure 6: Nullspace projections onto global direction q3 and local direction q2s in Null(Σ) via Πlg,representing global and local eigenvectors respectively. The local perturbation ΠlgΦ(1.5) has bothlocal and global components, creating a high-error component in the global direction.
Figure 7: (a) Difference in robust test error between our X-regularized adversarial training modeland the vanilla adversarial training (AT) model for Cifar- 1 0. X-regularization (instantiated as RST)keeps the robust accuracy within 2% of the AT model for small subsamples and even improves overthe AT model for larger subsamples of CIFAR-10. (b) Relative difference in standard error betweenaugmented estimators (our X-regularized model and the AT model) and the standard estimator onCifar- 1 0. We achieve up to 20% better standard error than the standard model for small subsamples.
