Figure 1: The overall architecture is based on a distributed deep RL agent (black). In addition,modulations z are sampled once per episode and modulate the behaviour policy. A bandit (purple)adapts the distribution over z based on a fitness ft (z) that approximates learning progress (cyan).
Figure 2: LavaWorld experiments, with 31 distinct modulations z. Left: If the candidate behavioursare stationary (not affected by learning), the bandit nearly recovers the performance of the best fixedchoice. The factored variant (blue) is more effective than a flat discretization (green). Using the trueLP (z) instead of a less informative proxy (red) has a distinct effect: in the stationary sparse-rewardcase this makes the bandit equivalent to uniform sampling. Right: in a non-stationary setting wherethe modulated Q-values are learned over time, the dynamics are more complex, and even the banditwith ideal fitness again comes close to the best fixed choice, while uniform or proxy-based variantsstruggle more (see Appendix B).
Figure 3: The best fixed choices of a modulation z vary per game, and this result holds for multipleclasses of modulations. Left: Repeat probabilities ρ. Right: Optimism ω.
Figure 4: No fixed arm always wins. Shown are normalized relative ranks (see text) of differentfixed arms (and curated bandit), where ranking is done separately per modulation class (subplot).
Figure 5: Tuning exploration by early performance is non-trivial. The bar-plot shows the perfor-mance drop when choosing a fixed z based on initial performance (first 10% of the run) as comparedto the best performance in hindsight (scores are normalized between best and worst final outcome,see Appendix D). This works well for some games but not others, and better for some modulationclasses than others (colour-coded as in Figure 6), but overall its not a reliable method.
Figure 6: Tuning exploration by choosing a set of modulations is non-trivial. This figure contrastsfour settings: the effect of naive (‘extended’) versus curated z-sets, crossed with uniformly samplingor using the bandit, and for each of the four settings results are given for multiple classes of modu-lations (colour-coded). The length of the bars are normalized relative ranks (higher is better). Notehow uniform does well when the set is curated, but performance drops on extended z-sets (except forω-modulations, which are never catastrophic); however, the bandit recovers a similar performanceto the curated set by suppressing harmful modulations (the third group is closer to the first than tothe fourth). The ‘combo’ results are for the combinatorial , ρ, ω space, where the effect is evenmore pronounced.
Figure 7: A few games cherry-picked to show that the combinatorial , ρ, ω bandit (thick red) cansometimes outperform all the fixed settings of z. The thick black line shows the fixed referencesetting = 0.01.
Figure 8: Example visitation densities in LavaWorld of different modulated policies that all use thesame set of Q-values.
Figure 9:	Fixed arms, bandit and uniform over the curated sets for epsilon.
Figure 10:	Fixed arms, bandit and uniform over the curated sets for temperatures.
Figure 11:	Fixed arms, bandit and uniform over the curated sets for action repeats.
Figure 12:	Fixed arms, bandit and uniform over the curated sets for optimism.
Figure 13: Comparing the combinatorial , ρ, ω-bandit to the per-modulation-class bandits.
Figure 14: The learning curves for the uniform and the (, ρ, ω)-bandit on the curated set.
Figure 15: Bandits on extended sets versus curated sets. For single-class modulation, the gaps aresmall overall (as shown in Figure 6). One negative result is that on the fully combinatorial spaceT, , b, ρ, ω, the bandit performance drops substantially (compare red to magenta). We assume thisis due to the dominant effect of strong action biases b that effectively impoverish the behaviour formany sampled z .
Figure 16: Comparison of several bandits (and uniform) on the combinatorial space spanned bythe extended set of 3 modulations , ρ, ω. The performance of uniform (gray), UCB (green) andThompson Sampling (blue) varies considerably from one game to another, while our proposed adap-tive mechanism (‘bandit’, red) seems to deal quite effectively with this variability and achieves verygood performance across almost all games. Note that the bandit and uniform results here correspondto the extended ‘combo’ results in Figure 4.
Figure 17: Choices of by the bandit across time (log-scale) on the extended set of . Note thatin general the values of start high and gradually diminish, but the emerging schedules are quitedifferent across games.
Figure 18: Choices of action repeat over time for different games on the extended set of ρ.
Figure 19: Probabilities of selected action bias scales b (first four subplots) and other modulationdimensions across training, on the game of Seaquest.
