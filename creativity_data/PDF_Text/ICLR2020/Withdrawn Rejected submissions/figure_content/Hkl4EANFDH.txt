Figure 1: On the importance of trajectories: an example with 2-dimensional logistic regres-sion. Having learned task T1 , the model is trained on T2 with two different objectives: minimizingthe loss on T2 (Finetuning) and a regularized objective (EWC; Kirkpatrick et al. (2017)). We add asmall amount of Gaussian noise to gradients in order to simulate the stochasticity of the trajectory.
Figure 2: Evolution of task performance over the course of continual learning on one orderingof Omniglot. For visibility we only show accuracies for every fifth task. The rectangular shadedregions delineate the period during which each task is being trained upon; with the exception of ER,this is the only period the model has access to the data for this task.
Figure 3: Low-resource adaptation results. The source (resp. target) task performance is representedon the vertical (resp. horizontal) axis. Pareto optimal configurations for each method are highlightedand the frontier is represented with dashed lines. The solid gray lines indicate the score of theoriginal model trained on the source task.
