Figure 1: Graphical model of CPVAE (left) vs. standard VAE (right)2	Method2.1	Cross-population Variational AutoencoderWe propose a generative model of a data instance xki belonging to population k using two latentvariables zki and tki . The latent variables are mapped to the observed space using non-linear map-pings, fθs (zki) and fθk (tki), each parameterized by a neural network. While we share the param-eters θs among all populations, θk are only shared among instances belonging to the population k .
Figure 2: Grassy-MNIST reconstructions with two populations: digits 0-4 and digits 5-9. Firstrow: original images. Second row: full reconstructions. Third row: private space (digit-only)reconstructions. Fourth row: shared space (background only) reconstructions. Note that this modelhas never seen the original digits or the original backgrounds.
Figure 3: Visualization of subgroups within MNIST digit population after t-SNE projection of latentspace for our method (left) vs a standard VAE (right). Each color represents one of the five digitswithin the population.
Figure 4: CPVAE reconstructions of CelebA dataset images. Top row: original images. Second row:reconstructions with male and shared space decoders. Third row: reconstructions with non-maleand shared space decoders. Bottom row: shared decoder only.
Figure 5: Image reconstructions for CPVAE vs VAE in continual learning setting. Each columnshows the reconstructions for the labeled task as well as previous tasks, demonstrating CPVAE’sability to retain previously learned information by utilizing both population-specific and shared la-tent vectors.
Figure 6: Generative samples from CPVAE trained on Grassy MNIST. Each row is from a populationcorresponding to a different digit.
