Figure 1: Illustration of general triplet comparisons.
Figure 2: (a) The architecture of Ordinal Embedding Neural Network (OENN). As example a triplet(22, 10, 37) and its answer Rt are fed to the architecture. (b) The EmbNet neural network, which isused as a building blocks of ordinal embedding architecture.
Figure 3: (a) Three datasets used for the reconstruction experiment. (b) Embedding output of OENNwith |T | = 80000 input triplet answers. The training triplet error (TEtrain) is written on top of eachplot (c) Training and test error of the OENN algorithm with respect to varying number |T of inputtriplet answers.
Figure 4: (a) Training triplet error with varying number of items for three different ordinal embed-ding methods. (b) Test triplet error for varying number n of items. Note that we scale the numberof input triplets as |T| = nd log n. (c) Training time (embedding time) of the three methods inseconds. LOE and TSTE did not scale to data sets larger than 10000 points. Note that the right partcorresponds to the extended range of items. In addition, the x-axis on the right part is exponential,thus the slope of the curve is not comparable to the left part of the plot.
Figure 5: (a) The heat-map of pairwise distances between the embedding outputs. The cencept namefor each chunk of 2500 items is written on both axes. (b) An example of 60 images depicted on theiractual embedded location in two dimensions. Four images from each category are chosen at random.
Figure 6:	(a) Triplet error with varying number of items and hidden layer size. The x and y axescorrespond to the number of items (n) and the hidden layer size (w) respectively. Note that x-axisgrows exponentially. (b) Triplet error with varying dimension and hidden layer size. The x and yaxes correspond to the number of dimensions (d) and the hidden layer size (w) respectively.
Figure 7:	The horizontal axis of the heatmap represents the number of points. Note that it increasesexponentially. The vertical axis represents the size of the chosen input encoding. This axis increaseslinearly. The color of the heat map represents the triplet error obtained in the training procedure.
Figure 8:	The reconstruction of three toy datasets with various number of input triplet answers.
Figure 9: An example of a pathological dataset for ordinal embeddingA.6 Extra datasets, pathological examplesIn our three examples for the reconstruction experiment the datasets exhibit some nice properties:large number points, clustered data distribution. The natural question would be: “Does the proposedmethod still work in absence of the nice properties? Do we still get a good reconstruction of theinitial dataset?”Here, we present two examples in which the above properties do not hold. We chose a sine waveas the data distribution. The two datasets have 25 and 100 points respectively (see Figure 9, leftcolumn). We perform the ordinal embedding in two dimensions with nd log(n) triplet answers,which are produced based on Euclidean distances in two dimensions. Figure 9 (right column) depictsthe embedding result. The training triplet error for both experiments is still less than 2%, whichshows that the solution is a valid solution.
