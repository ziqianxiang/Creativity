Figure 1: Illustration of adaptive planning horizonin FourRoom with an imperfect model. The modelis perfect in three rooms while totally wrong inthe left bottom room. Non-adaptive MVE divergesdue to the large model errors (right). In contrast,AdaMVE is able to adapt the planning horizon atdifferent state (see (a), darker color means longerplanning horizon), outperforming both the model-based and model-free baselines.
Figure 2: FourRoom Envand 0 in the other positions. A state is represented by the (x, y) coordinate. We evaluate the adaptiveplanning horizon using the following models:â€¢	Oracle model. The transition function behaves exactly the same as the true environment.
Figure 3: Experiment results on FourRoom. (a)-(f) Visualization of learned planning horizon for allstates. For each state, the average horizon H(S) weighted by (8) is presented. We use Hmax = 5 thusH(S) <= Hmax/2 = 2.5 from definition. Our method can successfully adapt the planning horizonwhen the model is imperfect. (g)-(i) Policy learning performance with different models. The shadedarea shows the standard error. Results clearly show that AdaMVE significantly outperforms MVEwhen the model is imperfect (no wall model and 3room model).
Figure 4: Results of continuous control. (a)-(c) PointMass Navigation example environments. (d)-(e)PointMass Navigation policy learning performance using pretrained model. (f)-(g) PointMass Navi-gation policy learning performance using online learned model. (h)-(i) Policy learning performanceon HalfCheetah and Swimmer. AdaMVE outperforms both DDPG and MVE with pretrained model.
Figure 5: We compare AdaMVE with MVE_h1 (fixed planning horizon 1) and MVE_h3 (fixedplanning horizon 3). AdaMVE outperforms both methods on both domains in terms of sampleefficiency and final performance.
Figure 6: Evaluation of model error transformation.
Figure 7: Supporting Results. When HmaX = 5, the maximum of weighted average horizon H is 2.5according to the definition.
