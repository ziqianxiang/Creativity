Figure 1: Guess Number: entropy of the messages m. Shaded regions mark standard deviation.
Figure 2: Image Classification: entropy of the messages m in function of log number of target classes,Nl . Shaded regions mark standard deviation.
Figure 3: Learning in presence of random labels. GS (SM) indicates models trained with Gumbel-Softmax (Softmax) channel. Linear are models with the channel removed.
Figure 4: Robustness to adversarial examples: higher accuracy given fixed implies more robustness.
Figure 5: Guess Number: Receiver’s dependence on messages, measured as performance drop undermessage intervention.
Figure 6: Guess Number: Entropy of the messages m, depending on vocabulary size, training method,and relaxation temperature τ (when trained with Gumbel-Softmax) or Sender’s entropy regularizationcoefficient λs. Shaded regions mark standard deviation.
Figure 7: Image Classification: entropy of the messages H(m) across vocabulary sizes. Successfulruns are pooled together. Shaded regions mark standard deviation.
Figure 8: Image Classification: entropy of the messages H(m) across Receiver model sizes. Suc-cessful runs are pooled together. Shaded regions mark standard deviation.
Figure 9: Guess Number: Entropy of the emergent protocol when communication is performed withvariable-length messages. Shaded regions mark standard deviation.
Figure 10: Evolution of H(m) over training epochs. Gumbel Softmax-based optimization, GuessNumber. For each game configuration, specified by the number of bits Receiver lacks, we sampleone successful (black line) and one failed (red line) training trajectory. The blue line marks Hmin ,minimal entropy for a successful solution.
Figure 11: Evolution of H (m) over training epochs. Stochastic Computation Graph-based optimiza-tion, Guess Number. For each game configuration, specified by the number of bits Receiver lacks, wesample one successful (black line) and one failed (red line) training trajectory. The blue line marksHmin , minimal entropy for a successful solution.
