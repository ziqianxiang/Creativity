Figure 1: Performance of the standard Transformer (dp = d/h) compared with the fixed head size(dp = 32) models on a language modeling task (LM1B) on the test set. We vary the embeddingsize of the standard Transformer from 256 to 512. We train the fixed head size models with a fixedembedding size of 256 and a head size of 32, and vary the number of heads from 4 to 70, whilematching the number of parameters. The plots clearly indicate that fixing the head size allows us totrain Transformers with a smaller embedding size (plot (b)), and with a better scaling of performance(plot (a)). Note that for perplexity lower values are better.
Figure 2: Comparison of a 24 layer standard Transformer model BERTLARGE with the fixed head sizemodel on the SQuAD and MNLI dev sets. We vary the embedding size of the BERT models from 512to 1024. We train the fixed head size models with a fixed embedding size of 512 and a head size of128, with a varying number of heads from 8 to 32, while matching the number of parameters. Fixingthe head size allows us to train models with an embedding size of 512 with a better performance.
Figure 3: Ablation studies on LM1B: (a) We fix the embedding size of all the models to 256 andvary the capacity of the standard Transformers by increasing the size of the feedforward layers. Forthe modified models we fix the head size to 32, so 8 head modified model is the same as the 8head standard Transformer. We notice that again in Transformers increasing the number of headsbeyond 16 hurts the performance, whereas with a fixed head size increasing the number of headsmonotonically improves the performance. (b) We show the effect of head size on the performancewith different number of heads. Both plots clearly show the advantage in having an additional way totune the capacity of Transformers with a fixed embedding size.
Figure 4: Performance of the standard Transformer training compared with the fixed head size(dp) models for a language modeling task (LM1B) on the test set. Unlike Fig.1, we vary both theembedding size and the number of heads of the standard Transformer to keep its head size fixed to 32.
