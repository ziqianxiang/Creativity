Figure 1: Overview of ROOTS. (A) The context observations are first fed into an encoder and obtain theat for eachz4z1scene-volume feature-map. (B) The scene-object latent-map has a cell of zn = znpres , zpnos , znwhspatial volume cell of the target environment. (C) After obtaining znpres , zpnos, we obtain znwhatby findingnwhat is notobject patches from context and then clustering them for each object. The resulting representation za 3D-representation. (D) Decoding process p(xq |s)p(s|z, vq). The representation is rearranged according toquery viewpoint vq by using f3D→2D (zpnos , vq) and then projected to a target 2D image. In (D), left-bottomis an example of 3D full view. A projection camera is shown on the left corner. On the right, we perform 2projection steps: (1) converting the coordinate w.r.t. the projection camera (bottom) and (2) projecting onto 2Dcanvas (up).
Figure 2:	Examples of generations from two different scenes together with ground truth placed in the first rowfor each scene. ROOTS shows better generation on objects boundary and clearer occlusion, compared withGQN, especially when there are more object in the scene.
Figure 3:	A: Generated examples for scene decomposition from three different scenes visualized as 2D imagescaptured by a query camera (viewpoint). For example, in the first group of visualization in A, ROOTS cansegment a scene into the foreground and background first and decompose foreground into each individualobject further, which are a green sphere, a blue cylinder, a partially observed cube and a yellow cube. We alsoshow the foreground occlusion mask, which is obtained via each object’s mask and the distance between theobject and query camera. B: Recovered generations of two objects from global 3D object-wise representationunder different query viewpoints. The recovered 2D projection have different pose when seen from differentviewpoints.
Figure 4: Visualization of changing zpnos,x and zpnos,y of the red cylinder in a scene through generations from4 different cameras. We also simulated the 3D scene in the center. Cameras are shown as an example. We putwalls for readers to better understand the relative height of cameras. There is no wall in the real dataset.
Figure 5:	TOP: Original generations from three different scenes, icons on the right side highlight objects ineach scene. Each column shows a 2D image captured by one camera. Bottom: Manipulated generations fromthe same three scenes under the same set of cameras. We switch the green cylinder from scene 1 with the purplesphere from scene 2 and we copy green sphere from scene 2 and put it into scene 3.
Figure 6:	A novel scene consisted of 9 objects is composited by ROOTS trained on 1-3 objects dataset.
Figure 7:	Left: Context images provided to ROOTS, we select images that do not have the green sphereprojected. Middle: Target images, from which we can see that there is one green sphere existed. Right:Generations from ROOTS given context images from the left.
Figure 8: TOP: Target images taken by several query viewpoints from 9 different scenes with icons on theright hand-side highlighting the objects in the scene. Bottom: New scene created with objects collected fromthe above 9 scenes.
Figure 9: Scene Representation NetworkObject Representation Network: We use object Representation Network to implement order in-variant encoder at object level, frepr-obj(∙). AS visualized in Figure 10, We design a branch to passlow level feature to provide richer conv-features. The dimension d in the second input equals to thesummation of the dimension of {zpres, sscale, and spos }. The corresponding values are listed inTable 5. The order invanriant is implemented in the same way as in Scene Representation Network.
Figure 10: Object Representation Networkwhere hi is the output of the cell and ci is the recurrent state of the ConvLSTM, xi is the input. Bothhi and ci are initialized with zeros at the i = 0 step.
