Figure 1: A circular trajectory, passed through a ReLU network with σw = 2. The plots show thepre-activation trajectory at different layers projected down onto 2 dimensions.
Figure 2:	Expected length of a line connecting two MNIST data points as it passes through a sparse-Gaussian deep network, plotted at each layer d.
Figure 3:	Expected growth factor, that is, the expected ratio of the length of any very small linesegment in layer d + 1 to its length in layer d. Figure 3a shows the dependence on the variance ofthe weights’ distribution, and Figure 3b shows the dependence on sparsity.
Figure 4:	The dependence on α and k of expected value of a subvector dzJi. In Figure 4a, dz is arealisation of the uniform distribution over the unit sphere. In Figure 4b, dz has the first entry equalto 1, and the rest zeros.
Figure 5:	Expected growth factor for trajectories joining randomly chosen (normalised) points inR500 . Figure 5a and Figure 5c show the dependence on the standard deviation of the weights’distribution for a straight and curved trajectory respectively, and Figure 5b and Figure 5d show thedependence on sparsity with a straight and curved trajectory respectively. In this experiment wehave chosen as the curved trajectory a straight line which has been modified to be a semi-circulararc in 100 randomly chosen hyperplanes.
Figure 6:	Expected growth factor for trajectories passed through trained feedforward ReLU net-works trained on MNIST.
