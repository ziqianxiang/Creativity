Figure 1: Triplet loss for embedding DNA sequence inputs. Given anchor point A, a positivepoint P that is close to A in terms of edit distance, and a negative point N that is not closeto A. We hope our embedding function f satisfies f (A) is closer to f (P ) than f(N) in `p.
Figure 2: Our embedding model leveraging two-layer GRU.
Figure 3: Our three phase training method: Phase 1 as warm start; Phase 2 learns similarity;Phase 3 for hard case. Embed refers to embedding model depicted in Figure 2.
Figure 4: Gen50ks: top K recalls for our algorithm (Neural) vs. CGK, CGK’ and identity,as a function of the number of candidates.
Figure 5: Uniref: top K recalls for our algorithm (Neural) vs. CGK, CGK’ and identity, asa function of the number of candidates.
Figure 6: Trec: top K recalls for our algorithm (Neural) vs. CGK, CGK’ and identity, as afunction of the number of candidates.
Figure 7: Two possible cases of the sub-index stringA : MajorA : Minorφ : Aligned：:Not Aligned(3) Conditioned on t ≤ T,∆j0 - ∆j1,0,w.p. 2/3w.p. 1/3Proof. Since j0 is the first aligned index after j , we know that for any j < jo < j0 , jo is notaligned, therefore one of fjx and fjy is major and the other one is minor. By Algorithm 2,for the string with the minor copy, with probability 1, the next copy on jo + 1 is a majorcopy; and for the string with the major copy, the next copy on jo + 1 is a major copy orminor copy with equal probability. However, if both copies on jo + 1 is major, we knowjo + 1 is aligned, and thus j0 = jo + 1. In other words, this case only happens for the indexjo = j0 - 1, and for other indices, the two strings are not aligned, and have major copies inturn. That already proves (1).
Figure 8: t-SNE visualization of neural embedding and CGK embedding. The purple pointwith orange arrow represents the query string, and the red points represent top-10 editdistance nearest neighbors of the query strings, and the blue points represent the others.
