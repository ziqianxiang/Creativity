Figure 1:	Left and center: Learning curves for BADGE versus k-DPP sampling with gradient embeddingsfor different scenarios. Right: A run time comparison (seconds) for BADGE versus k-DPP sampling corre-sponding to the middle scenario. The performance of the two sampling approaches nearly perfectly overlaps.
Figure 2:	A comparison of batch selection algorithms in gradient space. Left and center: Plots showing thelog determinant of the Gram matrix of the gradient embeddings within batches as learning progresses. Right:The average embedding magnitude (a measurement of predictive uncertainty) in the selected batch. Thek-centers sampler finds points that are not as diverse or high-magnitude as other samplers. Notice also thatk-MEANS++ tends to actually select samples that are both more diverse and higher-magnitude than a k-DPP,a potential pathology of the k-DPP’s degree of stochastisity. Standard errors are shown by shaded regions.
Figure 3: Active learning test accuracy versus the number of total labeled samples for a range of conditions.
Figure 4: A pairwise penalty matrix over all experiments.
Figure 5: Cumulative distribution functionof the normalized errors of all algorithms.
Figure 6: Full learning curves for OpenML #6 with MLP.
Figure 7:	Full learning curves for OpenML #155 with MLP.
Figure 8:	Full learning curves for OpenML #156 with MLP.
Figure 9:	Full learning curves for OpenML #184 with MLP.
Figure 10:	Full learning curves for SVHN with MLP, ResNet and VGG.
Figure 11:	Full learning curves for MNIST with MLP.
Figure 12: Full learning curves for CIFAR10 with MLP, ResNet and VGG.
Figure 13:、 W t，”，042000	4000 βooo aooo#LabeIS queriedI(XXX) 12000OPenML#6, MLP, Batoh size: IoOoOAOalrOOV20004000 KOO aooo 10000#LabeIS queried---- Coreset -------- BADGE ---------- Entropy -------- Marg -------- RandZoomed-in learning curves for OpenML #6 with MLP.
Figure 14:	Zoomed-in learning curves for OpenML #155 with MLP.
Figure 15:	Zoomed-in learning curves for OpenML #156 with MLP.
Figure 16:	Zoomed-in learning curves for OpenML #184 with MLP.
Figure 17: Zoomed-in learning curves for SVHN with MLP, ResNet and VGG.
Figure 18: Zoomed-in learning curves for MNIST with MLP.
Figure 19: Zoomed-in learning curves for CIFAR10 with MLP, ResNet and VGG.
Figure 20: Pairwise penalty matrices of the algorithms, grouped by different batch sizes. The parenthesizednumber in the title is the total number of (D, B, A) combinations aggregated, which is also an upper bound onall its entries. Element (i, j) corresponds roughly to the number of times algorithm i outperforms algorithm j.
Figure 21:	Pairwise penalty matrices of the algorithms, grouped by different neural network models. Theparenthesized number in the title is the total number of (D, B, A) combinations aggregated, which is alsoan upper bound on all its entries. Element (i, j) corresponds roughly to the number of times algorithm ioutperforms algorithm j . Column-wise averages at the bottom show aggregate performance (lower is better).
Figure 22:	CDFs of normalized errors of the algorithms, group by different batch sizes. Higher CDF indicatesbetter performance. From left to right: batch size = 100, 1000, 10000.
Figure 23:	CDFs of normalized errors of the algorithms, group by different neural network models. HigherCDF indicates better performance. From left to right: MLP, ResNet and VGG.
Figure 24:	A comparison of batch selection algorithms in gradient space. Plots a and b show the logdeterminants of the Gram matrices of gradient embeddings within batches as learning progresses. Plots cand d show the average embedding magnitude (a measurement of predictive uncertainty) in the selectedbatch. The k-centers sampler finds points that are not as diverse or high-magnitude as other samplers. Noticealso that k-MEANS++ tends to actually select samples that are both more diverse and higher-magnitude thana k-DPP, a potential pathology of the k-DPP’s degree of stochastisity. Among all algorithms, CONF hasthe largest average norm of gradient embeddings within a batch; however, in OpenML #6, and the first fewinterations of SVHN, some batches have a log Gram determinant of -∞ (shown as gaps in the curve), whichshows that Conf sometimes selects batches that are inferior in diversity.
Figure 25:	Learning curves and running times for OpenML #6 with MLP.
Figure 26:	Learning curves and running times for OpenML #155 with MLP.
Figure 27:	Learning curves and running times for OpenML #156 with MLP.
Figure 28:	Learning curves and running times for OpenML #184 with MLP.
Figure 29:	Learning curves and running times for SVHN with MLP and ResNet.
Figure 30:	Learning curves and running times for MNIST with MLP.
Figure 31:	Learning curves and running times for CIFAR10 with MLP and ResNet.
