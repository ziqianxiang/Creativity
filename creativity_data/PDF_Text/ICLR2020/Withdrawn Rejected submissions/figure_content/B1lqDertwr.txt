Figure 1: Reward vs. timesteps, for four algorithms (columns) and four environments (rows).
Figure 2: Final reward vs. single hyperparameter change. "Rollout Timesteps" refers to the numberof state-action samples used for training between policy updates.
Figure 3: Final reward vs. changes in the width and depth of network.
Figure 4: Reward vs. timesteps, for four algorithms (columns) and three environments (rows).
Figure 5: The distribution of reward over 100 trajectories for PPO Humanoid after running 2e7timesteps. The bins indicate reward range and the y-axis indicates the frequency of trajectory rewardsin a range.
Figure 6: The distribution of reward over 100 trajectories for TRPO Ant after running 5e6 timesteps.
Figure 7: The performance comparison between regularization and baseline after training for variousamounts of timesteps. Models under conventional regularization achieve the same performance withless timesteps than the baseline.
Figure 8: The effect of combining L2 regularization with entropy regularization. For PPO Humanoid-Standup, we use the third randomly sampled hyperparameter setting. For A2C HumanoidStandupand TRPO Ant, we use the baseline as in Section 4.
Figure 9: Comparison between L? regularization and weight decay. For PPO Humanoid andHumanoidStandup, we use the third randomly sampled hyperparameter setting.
Figure 10: Training curves of A2C regularizations under five randomly sampled hyperparameters.
Figure 11: Training curves of TRPO regularizations under five randomly sampled hyperparameters.
Figure 12: Training curves of PPO regularizations under five randomly sampled hyperparameters.
Figure 13: Training curves of SAC regularizations under five randomly sampled hyperparameters.
Figure 14: The interaction between policy and value network regularization for A2C. The optimalpolicy regularization and value regularization strengths are listed in the legends. Results of regu-larizing both policy and value networks are obtained by combining the optimal policy and valueregularization strengths.
Figure 15: The interaction between policy and value network regularization for TRPO.
Figure 16: The interaction between policy and value network regularization for PPO.
Figure 17: The interaction between policy and value network regularization for SAC.
