Figure 1: Distributions of users over time. Left: A distribution which remains constant over time,following the i.i.d assumption. Right: Self-induced distributional shift (SIDS) results in a change inthe distribution of users in our content recommendation environment (see Section 4.2 for details).
Figure 3: Q-learning sometimes fails the unit test; empirical p(cooperate) stays around 80-90%in 3 of 5 experiments (bottom row). Each column represents an independent experiment. Q-valuesfor the cooperate and defect actions stay tightly coupled in the failure cases (col. 1,2,5), whilein the cases passing the unit test (col. 3,4) the Q-value of cooperate is driven down over time.
Figure 2: Average level of non-myopic cooperate behavior observed in the unit test for HIDS,with and without two different meta-learning algorithms (PBT and REINFORCE). Lower is better,since the goal is for non-myopic incentives to remain hidden. Despite making the inner loop fullymyopic (γ = 0), both outer-loop optimizers reveal HIDS, however, leading agents to choose thecooperate action (top rows of (a) and (b)). Environment-swapping significantly mitigates HIDS(bottom rows of (a) and (b)).
Figure 4: Content recommendation experiments. Left: using Population Based Training (PBT)increases accuracy of predictions faster, leads to a faster and larger drift in users’ interests, P (y|x),(Center); as well as the distribution of users, P (x), (Right). Shading shows std error over 20 runs.
Figure 5: Amount of self-induced covariate shift (left) and self-induced concept shift (right) as afunction of performance (accuracy) averaged over all trials, learners, and time-steps. Only relativelystrong learners (those which achieve accuracy > 60%) exhibit HIDS.
Figure 6: Average level of non-myopic (i.e. cooperate) behavior learned by agents in the unit testfor HIDS. Despite making the inner loop fully myopic (γ = 0), population-based training (PBT) cancause HIDS, leading agents to choose the cooperate action (top row). Environment-swappingsuccessfully prevents this (bottom row). Columns (from left to right) show results for populationsof 10, 100, and 1000 learners. In the legend, “interval” refers to the interval (T) of PBT (see Sec.
Figure 7: The same experiments as Figures 3, 8, run for 50,000 time-steps instead of 3000, toillustrate the persistence of non-myopic behavior.
Figure 8: More independent experiments with Q-learning, exactly following Figure 3. Q-learningfails the unit test in a total of 10/30 experiments (including those from Figure 3).
Figure 9: More independent experiments with Q-learning, exactly following Figure 3, except alsousing context swapping. This leads to a 100% success rate on the unit test.
Figure 10: context swapping doesn’t have the desired effect in the content recommendation environ-ment.
Figure 11: Content recommendation results for different values of α1, α2.
