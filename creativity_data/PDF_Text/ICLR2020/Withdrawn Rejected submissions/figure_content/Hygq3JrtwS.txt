Figure 1: Sensitivity Safter versus test loss L for popular CNN architectures. The parameter initialization isXavier (Glorot & Bengio, 2010) with uniform distribution unless stated as standard normal distribution. Thenetworks are trained and fully converged on a subset of the CIFAR10 training dataset and are evaluated on theentire CIFAR10 testing dataset. Each point indicates a network with different numbers of channels and hiddenunits, and is averaged over multiple runs. For more details on configurations refer to Appendix 8.1. The Pearsoncorrelation coefficient ρ between data points is indicated in the figure.
Figure 2: Test loss versus sensitivity for networks trained on a subset of the CIFAR-10 training dataset where thenetwork parameters are initially drawn from a standard normal distribution. Each point indicates a network withdifferent numbers of channels and hidden units and is the average over multiple runs. The interval indicatesthe minimum and maximum values of sensitivity over multiple runs. (a) Fully connected neural networks.
Figure 3: Test loss versus sensitivity for networks trained on a subset samples of the CIFAR-10 dataset wherenetworks are initialized with different methods. On the right, we have a zoom in plot of the left figure.
Figure 4: Test loss of trained modelsversus sensitivity of untrained modelsfor networks whose parameters are ini-tially drawn from the standard normaldistribution. Note that the regulariza-tion techniques BN, dropout and max-pooling are removed from Alexnet,VGG11, and VGG13 configurations.
Figure 5: Test loss versus sensitivity for networks trained on 1000 samples of the CIFAR-10 training datasetpresenting the effect of initialization, dropout and batch normalization. Each point is the average over multipleruns and indicates a different architecture. (a) The networks are 5 layer FC, 2-4 layer CNN where the parametersare initially drawn from either Xavier uniform distribution (XU) or standard normal distribution (SN). (b) Thenetworks are 3, 5, 7 layer FC and 1-4 layer CNN. The top most right pink point is the same network architectureas the top most right teal point when dropout is added to the configuration. Hence, for all network architectureswe observe a shift of the numerical points towards bottom left of the figure when dropout is applied. (c) Thenetworks are 3, 5, 7 layer FC and 1-4 layer CNN. In (b) and (c) the networks parameters are initially drawn fromthe standard normal distribution.
Figure 6: var (Equation (12)) versus S (Equation (2)) for networks trained on 1000 samples of the CIFAR-10training dataset. The non-linearity is ReLU, and χ is neglected in the computation of Equation (13) in the figures.
Figure 7:	Test loss versus sensitivity for networks trained on 6000 samples of the MNIST training dataset. Eachpoint indicates a network with a different width and the sensitivity and test loss are averaged over multiple runs.
Figure 8:	L verSuS S for networkS trained on 1000 SampleS of the CIFAR-100 training dataSet. Each pointindicateS a network with a different width and the SenSitivity and teSt loSS are averaged over multiple runS.
Figure 9:	Variance, bias and sensitivity versus test loss for a regression task using the MSE loss. The fullyconnected neural networks are trained and evaluated on the Boston house price dataset. Each point indicates anaverage over multiple runs of a network with different widths H.
Figure 10: Sensitivity versus test loss for networks at different stages of training and trained on different numbersof training samples. Each point indicates an average over multiple runs of a network with a different width anddepth. (b) is the zoom in of (a) on the bottom left, and we add the results of the same networks trained withdifferent numbers of samples. In (b) The network parameters are drawn from a normal distribution by using theHe technique. (c) and (d) are the zoom in of (a) on the top left, and we add the results for the same networkstrained with different number of training samples in (c) and at different stages of training in (d). In (c) and (d)the network parameters are drawn from the standard normal distribution.
