Figure 1: Comparison between a GNN-graph and an equivalent HAG. (a) Input graph; (b) 1-layerGNN computation graph (GNN-graph); (c) HAG that avoids redundant computation. The GNN-(k)graph computes new activations hv by aggregating the previous-layer activations of vâ€™s neighbors.
Figure 2: End-to-end performance comparison be-tween GNN-graphs and HAGs. We measure theper-epoch training time and inference latency on a2-layer GCN model with 16 hidden dimensions ineach layer. The performance numbers are normal-ized by the GNN-graph numbers.
Figure 3: Comparing the number of aggregations and amount of data transfers between GPU threadsto perform aggregations (lower is better). The y-axes are normalized by GNN-graphs, and the lastcolumn in each figure is the geometry mean over all datasets.
Figure 4: End-to-end GCN training timeon the COLLAB dataset using HAGswith different capacities. We train GCNfor a maximum of 350 epochs by follow-ing prior work (Kipf and Welling, 2016).
Figure 5: End-to-end performance comparison between GNN-graphs and HAGs. We measure theper-epoch training time and inference latency on a 2-layer GIN and a 2-layer SGC model. Bothmodels have 16 hidden dimensions in each layer. The performance numbers are normalized by theGNN-graph numbers (higher is better).
Figure 6: Time-to-accuracy comparison between HAG and GNN-graph for training a 2-layer GCNmodel on the Reddit dataset.
