Figure 1:   The proposed Mem2Mem architecture for abstractive summarization.  Sentence-level representa-tions from the document encoder hidden states are concatenated and reduced to a fix sized memory M0. Mi  isread using a RAM like mechanism. The memory readout vector is used to condition word-level attention. Thedecoder hidden states modulated by word-level attention is then used to update the memory state Mi+1 via agated write operation (z‚Å±).
Figure 2:  An example of encoder memory attention A.  Rows denote different memory slots andcolumns indicate input sentence representations with indices.  Note that the regularization loss re-moves the redundancy on different memory slots and helps each slot to focus on a single sentence.
