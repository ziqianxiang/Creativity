Figure 1: Reference game. Alice wants the blue triangle, and asks Bob to get it for her. He selectsthe green square, and Alice tells him “No” and shows him the item she wanted.
Figure 2: Models trained with self-play per-form as well in novel roles as do vanilla mod-els trained with direct supervision in the tar-get role.
Figure 3: Emerged lexicons. This figure show features along the y-axis and symbols along thex-axis. Each index counts the co-occurence of the symbol and the feature. For example, the trans-former self-play model appears to map a symbol to each distinct value of the first attribute, ignoringother attributes and word order. See more lexicons over other random seeds in B.3.
Figure 4: General architecture. This architecture underlies each model We use; only the imple-mentation of the Decoder and Encoder modules vary between models. In the baseline models noparameters shared within an agent. In shared embedding models, the embeddings (purple) are sharedacross roles. In symmetric models, the encoder and decoder (pink) are shared across both roles. Theblue modules are non-parametric.
Figure 5: Recurrent Default.
Figure 6: Recurrent self-play.
Figure 8: Transformer self-play.
