Figure 1: Overview of the proposed model, VideoEpitoma, with two stages. The first stage is theTimestep Selector, left. Based on a lightweight CNN, LightNet, it learns to select the most relevanttimesteps for classifying the video. This selection is conditioned on both the features of timestepand its context. The second stage is the video classifier, right. It depends on heavyweight CNN,HeavyNet, to effectively represent only timesteps selected in the previous stage. Then it temporallymodels these selected timesteps to arrive at the video-level feature, which is then classified.
Figure 2: Bottom, the Timestep Selec-tor learns concept kernels to representthe dominant visual concepts across thevideos. Top, the gating module learns toselect only a few timesteps according totheir importance to the current video.
Figure 3: For selecting timesteps during training, the gating module uses gated-sigmoid as theactivation for the gating value α. It has some desirable properties. i. Unlike ReLU, having upperbound does not allow a timestep feature to dominate others. ii. Unlike sigmoid, being clippedallows the network to discard insignificant timesteps, i.e. those with gating values α < 0.5. In testtime, we replace the gated-sigmoid with step-function for binary gating of timesteps.
Figure 4: Our stand-alone Timestep Selector helps improving the performance and reduces thecomputation of off-the-shelf CNN classifiers - be it 2D∕3d heavyweight CNNs or even lightweigh3D CNNs. More over, if the selector is trained end-to-end with the CNN classifier, the compUtationis reduced even further.
Figure 5: The ratio of selected timesteps for the action categories of Breakfast. When VideoEpitomais trained end-to-end, the Timestep Selector learns a better selection to the benefit of the classifer.
Figure 6: In the Timestep Selector, the gat-ing meachnism is conditioned on both thetimestep-level feature and the video-level con-text, which results is a better conditional gat-ing. If the gating is only frame-conditioned,the ratios of the selected timesteps for actioncategories have small variance. Which meansthe gating is less dependent on the context,i.e. the action category. On contrary, the no-tice a big variance for the frame and context-conditioned. The gating becomes more de-pendent on the action category when selectingthe timesteps.
Figure 7: VideoEpitoma, with end-to-end selector, reduces the computation ofCNNs by selecting less timesteps.
Figure 8: VideoEpitoma improvesthe performance of the off-the-shelfResNet2D for recognizing the actions ofCharades at different time scales.
