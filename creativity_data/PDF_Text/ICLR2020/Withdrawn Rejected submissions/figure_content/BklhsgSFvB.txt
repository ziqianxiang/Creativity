Figure 1: The overall architecture of our multi-task learning framework (L2T-MITTEN). For eachinput data, we will transfer its task-specific representations among different tasks, and assemble thetransferred representations via a task-specific Interaction Unit to get the final representation for eachtask.
Figure 2: Position-wise mutual attention mechanism.
Figure 3: Visualization of the learned multi-level task dependency structure for DBLP dataset(a) General tasks (conferences) dependencystructure	(b) Nodes (authors) cluster(c) Data-specific tasks (conferences) dependency structure for each cluster (Left: cluster A, Right: cluster F)First, in Figure 3a, where we directly visualize the learned general task dependency matrix, we cansee our approach indeed captures the task dependency structure in general, i.e. conferences fromthe same domain are more likely to be in the same sub-tree. Moreover, in Figure 3b we plot theauthors (nodes) according to the learned data-specific task dependency matrix and we can see thatthere are some clusters formed by authors. Further, we visualize the mean value of the data-specifictask dependency for each cluster, as shown in Figure 3c. We can see that different cluster doeshave different task dependency. This is desirable since when predicting if an author has publishedpapers in some conferences, authors from different domains should have different transfer weightamong conferences (tasks). As a summary, it is demonstrated that our approach can capture the taskdependency at multiple levels according to specific data.
Figure 4: Graph convolutional networks architecture. Note that in node-level task, the Set2Set layer(global pooling) is eliminated.
Figure 5: Text classification network architecture.
