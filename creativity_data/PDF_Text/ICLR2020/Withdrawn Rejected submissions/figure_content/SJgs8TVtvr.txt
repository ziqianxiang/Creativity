Figure 1: Overview of the proposed model MoE-Sim-VAE. Data (in panel A) gets encoded via aencoder network (B) into a latent representation (C) which is trained to be a mixture of standardGaussians. Via a clustering network (G), which is trained to reconstruct a user-defined similaritymatrix (F), the encoded samples get assigned to the data mode-specific decoder subnetworks (whichWe call experts) in the MoE Decoder (D). The experts reconstruct the original input data and can beused for data generation when sampling from the variational distribution (E).
Figure 2: Generation of MNIST digit images. Data points from the latent representation weresampled from the variational distribution (A) which is learned to be a mixture of standard Gaussiansand then clustered and gated (B) to the data-mOde-SPecific experts of the MoE Decoder (C).(D) Allsamples from the variational distribution were correctly classified and therefore also correctly gated.
Figure 3: Comparison of MoE-Sim-VAE to the most popular competitor methods on defining celltypes in peripheral blood mononuclear cell data via CyTOF measurements. On the x-axis differ-ent inhibitor treatments are listed whereas the y-axis reports the respective F-measure, defined inEquation 12, as performance measure of the methods. Each violin plot represents a run on a dif-ferent inhibitor with multiple wells, whereas the line connects the means of the performance on thespecific inhibitor.
Figure A1: Testing MoE-Sim-VAE on data sampled from a Gaussian mixture model with randomsampled parameters. Figure A1a: Testing with exact numbers of experts. When comparing toclassification results with GMMs one can see that our model achievs better results until around30 mixture components and is still compatitive until 40 mixture components. With more then 40mixture components ourr MoE-Sim-VAE is not able anymore to compete with a GMM. Figure A1b:Testing for specific number of synthetic mixture components and iterating number of experts. Untila number of GMM components of 23 MoE-Sim-VAE is very precise in learning the real number ofclusters even when allowing the model to have 40 experts.
Figure A2: More detailed overview of results and generated samples of MNIST images. The ploton the left side shows the latent representation where the red crosses are the cluster centers. Thosecan be used as a mean to sample from a standard Gaussian for data generation via the MoE Decoder.
Figure A3: Comparison of two sample MMD test (Sutherland et al., 2019) on the distributionsfrom the different mixture components in the latent representation. The heatmaps on the left sideshow the estimation of the MMD which can be seen as the distance between pairs of distributions.
Figure A4: Ablation study on the similarity matrix S. Both figures show the MMD statistic andUMAP (McInnes et al., 2018) projection of reconstructed MNIST digits computed on the latentrepresentation. Figure A4a shows the results on MoE-Sim-VAE trained with the similarity matrix.
Figure A5: Comparison of data generation process between Moe-Sim-VAE and VaDE (Jiang et al.,2017). Figure A5a shows the accuracy of how certain a specific digit can be generated from therespective cluster in the latent representation whereas Figure A5b compares the number of runsuntil a sample from the latent representation satisfied the posterior criterion from VaDE. It needsto be mentioned that MoE-Sim-VAE does not require any thresholding such that we ran the datageneration process multiple times with the same settings to compare with VaDE. In total 10000samples are generated for each digit.
Figure A6: Confusion map for data generation using MoE-Sim-VAE. Besides the systematic error ofconfusing digit 5 and 8, which can also depend on the clustering network, the digit generation of ourmodel performs very precise with a high accuracy of generating the digit asked for. In comparisonto VaDE (Jiang et al., 2017) our model does not need any threshold on samples from the latentrepresentation which reduces the computational costs by far.
Figure A7: Confusion maps for data generation using VaDE.
Figure A7: Confusion maps for data generation using VaDE.
Figure A7: Confusion maps for data generation using VaDE.
Figure A7: Confusion maps for data generation using VaDE.
Figure A8: The boxplots show similar as in Weber & Robinson (2016) the reproducibility of MoE-Sim-VAE on the four datasets when running MoE-Sim-VAE 30 times. The variance on defining thecorrect subpopulations of MoE-Sim-VAE is quite small and therefore also an improvment to manymethods compared in Weber & Robinson (2016).
