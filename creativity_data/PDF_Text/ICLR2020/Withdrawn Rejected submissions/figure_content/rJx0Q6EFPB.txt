Figure 1: An overview of Transformer distillation: (a) the framework of Transformer distillation, (b)the details of Transformer-layer distillation consisting of Attnloss(attention based distillation) andHidnloss(hidden states based distillation).
Figure 2: The illustration of TinyBERT learningwhere the matrices H S ∈ Rl ×d and H T ∈ Rl×d refer to the hidden states of student and teachernetworks respectively, which are calculated by Equation 4. The scalar values d and d0 denote thehidden sizes of teacher and student models, and d0 is often smaller than d to obtain a smaller studentnetwork. The matrix Wh ∈ Rd ×d is a learnable linear transformation, which transforms the hiddenstates of student network into the same space as the teacher network,s states.
