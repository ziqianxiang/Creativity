Figure 1: Motivating results. For cifar10, AdaScale preserves model quality for many scales S. Whenplotted in terms of scale-invariant iterations, training curves align closely. With AdaScale, “warm-up” behavioremerges from adapting a simple learning rate schedule (exponential decay) to scale S (learning rate plot croppedto show behavior). Meanwhile, linear scaling (with warm-up heuristic) degrades model quality as S increases.
Figure 2: Gain ratios. Plots compare moving averagert estimates to values computed offline (using 1000batches). The values align closely. Abrupt changesalign with learning rate step changes.
Figure 3: AdaScale training curves. For many scales and benchmarks, AdaScale trains quality models.
Figure 4: Elastic AdaScaling. For imagenet, AdaScale is approximately scale invariant, even if S changesabruptly (at τt = 133k, 225k). Unlike AdaScale, LSW degrades model quality in this setting (see Table 2).
Figure 5: Scale invariance for many learning rate schedules. Heat maps cover the space of exponentialdecay lr schedules for cifar10. At scale 16, validation accuracies for AdaScale align closely with results forsingle-batch training, with the space of 94+% schedules growing moderately with AdaScale. With LSW, noschedule achieves 94% accuracy. On the right, direct lr search at scale 16 produces inferior results to AdaScale(here the total iterations, 3.28k, is the average total iterations among 94+% AdaScale trials). Thus, AdaScaleinduces a superior family of schedules for scaled training. The white ' × , indicates the lr used for Figure 1.
Figure 6:	AdaScale training curves with varying moving average parameter.
Figure 7:	Gain ratios for transformer. Plots compare moving average rt estimates to values computedoffline (using 1000 batches).
Figure 8:	AdaScale training curves for cifar10. AdaScale trains quality models at various scales.
Figure 9:	Learning rate adaptation for elastic AdaScaling. Gain ratio and learning rate curves for elas-tic scaling scenarios align with the corresponding curves for constant scaling scenarios, despite abrupt scalechanges. (at Tt = 133k, 225k, dotted lines)D.4 Elastic scalingLearning rate and gain ratio curves for the two dynamic scaling scenarios we consider (discussed in§4) align surprisingly well with the corresponding curves for the scenarios where the scale is keptconstant throughout the training. This is shown in Figure 9. The abrupt change in scale causes thegain ratio to change quickly which in turn leads to an almost immediate change in learning rate.
