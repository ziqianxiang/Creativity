Figure 1: The generative process of our model. Latent variables model-ing the linguistic variation in French and English, Zfr and Zen as wellas a latent variable modeling the common semantics, Zsem , are drawnfrom a multivariate Gaussian prior. The observed text in each languageis then conditioned on its language-specific variable and Zsem .
Figure 2: The computation graph for the variational lower bound used during training. The En-glish and French text are fed into their respective inference networks and the semantic inferencenetwork to ultimately produce the language variables Zfr and Zen and semantic variable Zsem. Eachlanguage-specific variable is then concatenated to Zsem and used by the decoder to reconstruct theinput sentence pair.
Figure 3: The relationship between average performance for each year of the STS tasks 2012-2016(Pearson’s r × 100) and batch size (maximum number of words per batch).
