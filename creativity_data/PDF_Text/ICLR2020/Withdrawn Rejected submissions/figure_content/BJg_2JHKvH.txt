Figure 1: Illustration of semi-supervised learning with FlowGMM on a binary classification prob-lem. Colors represent the two classes or the corresponding Gaussian mixture components. Labeleddata is shown with triangles, colored by the corresponding class label, and blue dots represent un-labeled data. (a): Data distribution and the classifier decision boundary. (b): The learned mappingof the data to the latent space. (c): Samples from the Gaussian mixture in the latent space. (d):Samples from the model in the data space.
Figure 2: Illustration of FlowGMM performance on synthetic datasets. Labeled data is shown withcolored triangles, and unlabeled data is shown with blue circles. Colors represent different classes.
Figure 3: Visualizations of the latent space representations learned by supervised FlowGMM onMNIST. (a): Latent space interpolations between test images from the same class and (b): fromdifferent classes. Observe that interpolations between objects from different classes pass throughlow-density regions. (c): Histogram of distances from unlabeled data to the decision boundaryfor FlowGMM-cons trained on 1k labeled and 59k unlabeled data and FlowGMM Sup trained on1k labeled data only. FlowGMM-cons is able to push the decision boundary away from the datadistribution using unlabeled data. (d): Feature visualization for CIFAR10: four test reconstructionsare shown as an intermediate feature is perturbed. The value of the perturbation Î± is shown in red vsthe distribution of the channel activations. Observe that the channel visualized activates on zeroedout pixels to the left of the image mimicking the random translations applied to the training data.
Figure 4: Illustration of FlowGMM on synthetic datasets: two circles (top row), eight Gaussians(middle row) and pinwheel (bottom row). (a): Data distribution and classification decision bound-aries. Unlabeled data is shown with blue circles and labeled data is shown with colored triangles,where color represents the class. Background color visualizes the classification decision boundariesof FlowGMM. (b): Mapping of the data to the latent space. (c): Gaussian mixture in the latentspace. (d): Samples from the learned generative model corresponding to different classes, as shownby their color.
Figure 5:	Left: Log likelihoods on in- and out-of-domain data for our model trained on MNIST.
Figure 6:	Visualizations of the latent space representations learned by supervised FlowGMM onMNIST. (a): Images corresponding to means of the Gaussians corresponding to different classes.
