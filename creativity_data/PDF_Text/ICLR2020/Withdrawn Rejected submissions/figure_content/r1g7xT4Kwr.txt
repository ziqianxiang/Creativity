Figure 1: Our PLEX framework adopts a hierarchical policy approach, where a Planner proposessub-goals for an Executor to act upon within an environment. The Planner receives an egocentric,top-down view with the target location and an embedding summary provided by the Executor. TheExecutor receives visual sensory data (i.e. colour and depth) as its input and a sub-goal providedby the Planner. Our method reduces the need for long-term planning and addresses the knownsample inefficiency problem accompanying memory models within deep reinforcement learningapproaches.
Figure 2: A system overview of PLEX which consists of two main components: a Planner andExecutor agent. The Executor’s task is to perform a series of actions such that it will traverse theenvironment towards a target location given by a point-goal vector (a sub-goal that is provided bythe Planner). The Planner’s task is to generate sub-goals that help navigate the entire system toan end goal (which only the Planner can see). An environment’s state is comprised of an RGB-Dobservation, an egocentric map and a point-goal vector measurement pointing towards a target loca-tion as defined by Anderson et al. (2018). The Executor is provided with the RGB-D observation,a sub-goal generated by the Planner and returns an Executor Latent Information (ELI) vector thatsummarises its rollout. The Planner sees an egocentric map with a roughly 4m view horizon and theELI vector. The Planner and Executor agents are trained simultaneously and using the sub-goal bythe Planner and ELI vector by the Executor, we create a two-way communication channel, enablingour framework to regulate the flow of information between our two agents.
Figure 3: (a) The geodesic distance travelled by a random exploration policy with increasing numberof random actions. (b) The geodesic distance normalised by the number of random steps taken. Bothfigures are averaged from 500 scenes.
Figure 4: (a) The observed reward obtained by each model over 30M environment steps. (b) Theobserved success rate of each model. In this case, a success is defined as the agent issuing a stopaction when the geodesic shortest path distance between the agent and its goal is less than 0.2m.
Figure 5: (a) The impact towards obtained rewards with and without the ELI vector being providedto the Planner on the LSTM variant of our PLEX framework. (b) The performance of our methodcompared with PPO LSTM (Savva et al., 2019) on close and far subsets of the test set. In this case,close goals were selected when mean geodesic shortest path distance was less than 5.76m and wereclassified as far goals otherwise.
