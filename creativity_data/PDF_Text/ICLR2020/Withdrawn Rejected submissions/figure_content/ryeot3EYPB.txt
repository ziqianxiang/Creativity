Figure 1: The process diagram of the UDA with self-training methodUnsupervised Data Augmentation The second approach is utilizing the state-of-the-art semi-supervised learning technique, Unsupervised Data Augmentation (UDA) algorithm (Xie et al., 2019),to leverage the unlabeled data. The objective function of UDA can be written as,mil E	[-log(pθ (y|x))] + λ E [Dkl(pθ (y∣X)∣∣Pθ (y|x))]	(1)θ (x,y)∈Lsrc	x∈Utgtwhere X is an augmented sample generated by a predefined augmentation function X = q(χ). Theaugmentation function can be a paraphrase generation model, or a noising heuristic. Here, we use amachine translation system as the praraphrase generation model. The UDA loss enforces the classifierto produce label consistent predictions for pairs of original and augmented samples. With UDAmethod, the model is better adapted to the target domain by learning a label consistent model overthe target domain.
Figure 2: Plot of the accuracy vs. the number of training samples in log scale when runningUDA(XLM) method. On the left is the cross-lingual setting. On the right is the monolingual setting.
