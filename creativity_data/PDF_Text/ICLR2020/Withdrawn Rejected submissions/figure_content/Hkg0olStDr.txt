Figure 1: Mean accuracy of collabora-tor Selection for three orderingS for Ro-tated MNIST (RM), DigitS (DI), Mic2Mic(M2M), Office (OC) and DomainNet(DN).
Figure 2: Comparison of target domain accuracy and training convergence across non-distributedand distributed training methods. For Lazy-synchronized approach, we use sync-up step p = 4. ForMic2Mic, the PSGD baseline did not improve the accuracy, as such we omit it from the figure.
Figure 3: Distributed adversarial training architecture OfMDDA∖In non-sync steps:Accumulate local gradientsIn sync-up steps:A Aggregate accumulated gradients,八n Update iA.4 Details of Domain Adaptation AlgorithmsWe now discuss the adversarial training formulation of the various uDA algorithms with which weevaluated the efficacy of our proposed MDDA framework. As shown in Table 4.2, we evaluate ourmethod with four domain adaptation techniques: ADDA (Tzeng et al. (2017)), Gradient Reversal(Ganin et al. (2016)) and Wasserstein DA (Shen et al. (2018)), and CADA (Zou et al. (2019)) Beloware the adversarial training formulations of these techniques as proposed in their original papers.
