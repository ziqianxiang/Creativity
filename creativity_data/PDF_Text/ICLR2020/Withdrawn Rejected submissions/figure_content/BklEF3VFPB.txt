Figure 1: The proposed architecture is composed of a shared feature extractor Ge for two domains,a label predictor Gy and a reconstruction network Gr. In addition to the basic supervised learningin the source domain, our adversarial reconstruction training enables the extractor Ge to learndomain-invariant features. Specifically, the network Gr aims to reconstruct the source samples Xsand to impede the reconstruction of the target samples xt, while the extractor Ge tries to fool thereconstruction network in order to reconstruct the target samples xt.
Figure 2: The training procedure with regard to loss and test accuracy. (Le := Eq. 6; Lr := Eq. 4; Ldis the domain loss of DAT (Ganin & Lempitsky, 2015); α is the penalty term of Le and Ld.)7Under review as a conference paper at ICLR 2020Source Images ∣ Target Images ∣ R-Target ImagesMNIST→USPSUSPS→MNIST0 ( t 7 4 ð D 92.S^9 9Λ9 IΔ5V I » 3 ð SSVHN→MNIST“ © ∙? 4 0 1 3 ∖SYN→SVHN041813286 I » 9 7□ I Oloot?，0 6 1-13 Z ?C	I	??	7	S	I	OI	0	2 /■	O ∖	,	S771 I 0 M I y I
Figure 3: T-SNE visualization on SVHN→MNIST with their corresponding domain labels (red: tar-get; blue: source) and category labels (10 classes) shown in the left and right subfigures, respectively.
Figure 4: The training procedure of ARN with different hyper-parameters.
Figure 5: Visualization of the target samples and their corresponding reconstructed target samples.
