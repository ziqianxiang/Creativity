Figure 1: Three attribution methods, SP (Zeiler & Fergus, 2014), LIME (Ribeiro et al., 2016), andMP (Fong & Vedaldi, 2017), often produce unrealistic, out-of-distribution perturbation samples.
Figure 2: The results of filling the object mask in the real image (a) via four different filling methods.
Figure 3: Error plots for SSIM (a), Pearson correlation of HOG features (b), and Spearman rankcorrelation (c) scores obtained from 1000 random ImageNet-S images (higher is better). LIME-Gis consistently more robust than LIME on both low and high resolutions i.e. S ∈ {50, 150} (greenand blue bars). The same trends were also observed on the Places365-S dataset (Fig. S4). The exactnumbers are reported in Table S3.
Figure 4: We ran SP and SP-G using a 53 × 53 patch on a nail class image. Here are the perturbationsamples derived for both methods when the patch is slided horizontally across one row at 5 locations{9, 24, 36, 44, 53} (a); and their respective target-class probability scores (b). SP-G samples aremore realistic than those by SP and yielded a heatmap that localizes the object more accurately (a).
Figure 5: In Sec. 4.3, we compared the robustness of LIME vs. LIME-G heatmaps when runningusing 5 different random seeds. This is an example where LIME-G heatmaps are more consistentthan LIME’s (d vs. b). While LIME grayish samples (a) are given near-zero probabilities, LIME-Gsamples (here, inpainted using the same masks as those in the top row) are often given high proba-bilities except when the kuvasz dog’s eye is removed (c). LIME-G consistently assign attributionsto the dog’s eye (d) while LIME heatmaps appear random (b). The top-1 predicted labels for 4 outof 5 LIME samples (a) here are paper towel.
Figure S1: MP almost perturbs the entire image to generate its respective heatmap. Top row: AnImageNet-S image (a) and its respective MP (b) and MP-G (d) heatmaps. The pixels perturbed byMP (c) covers almost 99% of the pixels. In contrast, MP-G often perturbs 〜40% of the image (e).
Figure S2: Error plots for the Deletion and Insertion metrics (Petsiuk et al., 2018) for all 6 attri-bution methods across two different datasets: ImageNet-S (a-c) and P山ces365-S (b-d). We usedrandom noise heatmaps (i.e. having no information about the input image or the classifier) as thebaseline method (cyan bars). We did not find any significant differences between G-methods andtheir respective counterparts across both two metrics and both datasets. Note that our Deletion andInsertion scores for the SP and LIME algorithms match closely to those in Petsiuk et al. (2018). SeeTable S2 for the exact numbers.
Figure S3: Common images across all three metrics where LIME-G is consistently more robustthan LIME (top) and vice versa (bottom). Interestingly, we found the intersection of the three setscontains images of mostly scenes, close-up or tiny objects (top). In contrast, the common set ofimages where LIME is more robust than LIME-G contains mostly birds and medium-sized objects(bottom).
Figure S4: Bar plots comparing the robustness (higher is better) of G-methods and their counter-parts when changing hyperparameters (described in Sec. 4.3) under three different similarity met-rics: SSIM (a), Pearson correlation of HOG features (b), and Spearman rank correlation (c). Eachbar shows the mean and standard deviation similarity score across 1000 pairs of heatmaps, eachproduced for one random Places365-S image. SP-G is more robust than SP under SSIM (a) andPearson correlation (b). In contrast, MP-G is consistently more sensitive than MP under all metrics(red bars are shorter than light-red bars). The exact numbers are reported in Table S4.
Figure S5: We filtered out images where YOLO-v3 detected more than one objects (a). The re-maining images are used in ImageNet-S (b). We believe our observations on the ImageNet-S datasetcarry over to the full ImageNet.
Figure S7: Inpainting using the preservation objective generates unrealistic samples (Sec.4.1). Werandomly chose 50 validation-set images (a) from 52 ImageNet bird classes and compute their seg-mentation masks via a pre-trained DeepLab model (Chen et al., 2017) (b). We found that usingthe DeepFill-v1 inpainter to inpaint the foreground region (i.e. our “deletion” task) yields realisticsamples where the object is removed (d). In contrast, using the inpainter to fill in the backgroundregion (i.e. “preservation” task) yields unrealistic images whose backgrounds contain features (e.g.
Figure S8: Top-10 cases where the LIME-G outperformed (left) and underperformed (right) LIMEon the object localization task (IoU scores). From left to right, on each row: we show a real imagewith its ground-truth bounding box, LIME heatmap & its derived bounding box, LIME-G heatmap& its derived bounding box. See https://drive.google.com/drive/u/2/folders/10JeP9dpuoa0M16xe2FloBEWajQ7PNKSX for more examples of the LIME and LIME-G IoUresults.
Figure S9: Top-10 cases where the MP-G outperformed (left) and underperformed (right) MP onthe object localization task (IoU scores). From left to right, on each row: we show a real im-age with its ground-truth bounding box, MP heatmap & its derived bounding box, MP-G heatmap& its derived bounding box. See https://drive.google.com/drive/u/2/folders/1nlr5v2RbSiKigp8PRb_aQbauXBmZpHyU for more examples of the MP and MP-G IoU re-sults.
