Figure 1:	The relative performance of optimizers is consistent with the inclusion relationships, re-gardless of whether we compare final validation error (top) or test error (bottom). For all workloads,we tuned the hyperparameters of each optimizer separately, and selected the trial that achieved thelowest final validation error.
Figure 2:	The relative training speed of optimizers is consistent with the inclusion relationships.
Figure 3: Tuning more hyperparameters removes the differences in test error between optimizersobserved by Wilson et al. (2017). Tuning a subset of optimizer hyperparameters and the initiallearning rate is sufficient to equalize performance between all optimizers (left). More extensivehyperparameter tuning in our setup, including the learning rate schedule, improves results for alloptimizers and still does not produce any differences between optimizer performances (right).
Figure 4: Tuning more hyperparameters changes optimizer rankings from Schneider et al. (2019)to rankings that are consistent with the inclusion relationships. The leftmost columns for eachworkload reproduce the rankings from Schneider et al. (2019), while the remaining columns tuneover increasingly general search spaces. All columns use our random search tuning protocol.
Figure 5: Example plot of final validation error projected onto the axes of the hyperparameter space.
Figure 6: Validation performance of the best trial mostly converges with as few as 24 hyperparametertuning trials for Transformer on LM1B. Shaded regions indicate 5th and 95th percentiles estimatedwith bootstrap sampling (see Appendix C). The search spaces can be found in Appendix D.4.
Figure 7:	Validation performance of the best trial mostly converges with as few as 24 hyperparametertuning trials for ResNet-50 in ImageNet. Shaded regions indicate 5th and 95th percentile estimatedwith bootstrap sampling (see Appendix C). The search spaces can be found in D.3.
Figure 8:	Test performance of the best trial mostly converges with as few as 23 hyperparametertuning trials for a 2-layer LSTM on War and Peace. Shaded regions indicate 5th and 95th percentileestimated with bootstrap sampling (see Appendix C). The search spaces can be found in D.8.2.
Figure 9: The relative performance of optimizers is consistent with the inclusion relationships whenwe select for lowest training loss. Note that SGD, Adam, and NAdam for ResNet-50 on ImageNetused label smoothing in their final search spaces (see Section D.3), which makes their loss valuesincommensurate with the other optimizers. This is because their final search spaces were optimizedto minimize validation errorâ€”if we had optimized their search spaces to minimize training errorinstead, we would not have used label smoothing, and we expect their training loss values would beconsistent with the inclusion relationships.
