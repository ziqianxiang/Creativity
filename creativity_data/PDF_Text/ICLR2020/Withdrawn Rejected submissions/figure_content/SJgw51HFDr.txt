Figure 1: meProp versus SWAT (a) Shows the forward and backward pass of MeProp and SWATfor a fully connected layer. (b) Computational flow of meProp for any layer l (c) Computationalflow of SWAT for any layer l.
Figure 2: Convergence Analysis: (a) Sensitivity Analysis of ResNet18 for the Forward Pass on theCIFAR100 dataset. (b) Sensitivity Analysis of ResNet18 for the Backward Pass on the CIFAR100dataset. (c) Shows the training curve of ResNet18 on ImageNet for meProp and SAW algorithm.
Figure 3: Different ways of performing top-k operation. ‘N’ denotes the #samples in the mini-batch or filters in the layer, ‘C’ denotes the #channels in the layer. ‘H’ and ‘W’ denote the heightand width of the filter/activation map in the layer. Color represent the selected activations/weightsby the Top-K operation.
Figure 4:	Comprehensive analysis of sparsity Vs accuracy trade-off: (a) Accuracy of SWAT andSAW algorithms on CIFAR10 dataset. (b) Accuracy of SWAT and SAW algorithms on CIFAR100dataset. The dashed line represents the baseline accuracy for the corresponding model. Datapointsfor SAW algorithm are represented as dots whereas for SWAT algorithm they are represented asstars.
Figure 5:	Trend of SWAT algorithm on ImageNet dataset: (a) Validation curve of SWAT al-gorithm (b) Validation Accuracy of SWAT, SAW and DSG algorithms at different sparsity con-straints. Dotted line represents the baseline back-propagation algorithm. ‘RN18’ represent ResNet-18, ‘DN121’ represent DenseNet-BC-121 and ‘DSG’ denote the results reported by the DynamicSparse Graph(Liu et al., 2019) algorithm.
Figure 6:	(a) Influence of Depth and Width: Accuracy of SAW and SWAT algorithms on CI-FAR100 dataset for ResNet-50,101 and WRN-28-10 (b) Threshold value (K-th largest values) inTop-K operation of different layers during trainingEfficient Top-K Implementation: Top-K3 operation on 1 dimensional array of size n can be naivelyimplemented using sorting. The computational complexity of a naive Top-K operation is O(n log n).
Figure 8: Reduction in memory accesses during the backward pass. (a) Reduction in parameteraccess (b) Reduction in activation access per sample (Dataset: ImageNet)Memory Overhead Reduction: During training, most of the weights and activations are stored inDRAM and accessing DRAM consumes three orders of magnitude more energy consumption thancomputation (Horowitz). So reducing the memory access during training will directly reduce theenergy consumption. SWAT algorithm uses sparse input activation (al-1) and weight (wl-1) in thebackward, so input activation and weight can be compressed and stored in the memory in sparse for-mat thereby reducing the DRAM access in the backward pass. Figure 8a shows the reduction of 2x,3.3x, and 5x at sparsity of 50%, 70% and 80% in the parameters access in the backward pass. Theother significant memory overhead is saving the activation, and this overhead is dependent not onlyon the model size but also on the batch-size used during training. Figure 8b shows the activationmemory overhead for different architectures for a mini-batch size of 1. The graph only shows theactivation of batch-normalization and convolutional layer since memory overhead for the activationsof the linear layer is negligible compared to them. Note that SWAT algorithm sparsifies the compu-tation of the linear, and convolutional layers, so full input activation of the batch-normalization layerare saved for the backward pass. Thus, SWAT algorithm achieves activation compression of around1.3x, 1.5x, and 1.7x at sparsity of around 50%, 70%, and 80%. The activation of batch-normalizationlayer limits the overall activation compression.
Figure 9: Vector sparsification in high-dimension approximately preserves the direction:(a)Shows the sparsification angle distribution at different Top-K percentage for a 1000 dimensionalvector in 10, 000 trials. Random represents the angle distribution between 2 random vectors. (b)and (c) Shows the percentage of Top-K components needed for sparsification angle to be within δ in1000 trials and the variance in those trials. (d) Shows the relation between Top-K sparsification andthe sparsification angle in 1000 trials. (e) Shows how the sparsification angle (at 70% sparsification)varies during training for ResNet-18 architecture on CIFAR100 dataset.
Figure 10: Sparsity Variation using Periodic Top-K Implementation. Network: ResNet-18, Dataset:CIFAR100, Top-K period: 100 iterations, Target Sparsity: 90%17Under review as a conference paper at ICLR 2020Since the periodic Top-K used the same threshold during the entire period, therefore, it is crucial toconfirm that periodic Top-K implementation does not adversely affect the sparsity during training.
