Figure 1: We propose a method for unsupervised domain adaptation that uses self-supervision to align thelearned representations of two domains in a shared feature space. Here we visualize how these representationsmight be aligned in the feature space. a) Without our method, the source domain is far away from the targetdomain, and a source classifier cannot generalize to the target. b) Training a shared representation to supportone self-supervised task on both domains can align the source and target along one direction. c) Using multipleself-supervised tasks can further align the domains along multiple directions. Now the source and target areclose in this shared feature space, and the source classifier can hope to generalize to the target.
Figure 2: Our method jointly trains a supervised head on labeled source data and self-supervisedheads on unbaled data from both domains. The heads use high-level features from a shared encoder,which learns to align the feature distributions.
Figure 3: Results on MNIST→MNIST-M (left) and CIFAR-10→STL-10 (right). Test error con-verges smoothly on the source and target domains for the main task as well as the self-supervisedtask. This kind of smooth convergence is often seen in supervised learning, but rarely in adversariallearning. The centroid distance (linear MMD) between the feature distributions of the two domainsconverges alongside, even though it is never explicitly optimized for.
