Figure 1: Visualization of PCGrad’s effect on a 2D multi-task optimization problem. (a) A multi-task objectivelandscape. (b) & (c) Contour plots of the individual task objectives that comprise the multi-task objective. (d)Trajectory of gradient updates on the multi-task objective using the Adam optimizer. The gradient vectors of thetwo tasks at the end of the trajectory are indicated by blue and red arrows, where the relative lengths are on a logscale.(e) Trajectory of gradient updates on the multi-task objective using Adam with PCGrad. For (d) and (e),the optimization trajectory goes from black to yellow.
Figure 2:  Visual depiction of conflicting gradients and PCGrad.  In (a),  we see that tasks A and B haveconflicting gradient directions, which can lead to destructive interference and unstable learning.  In (b), weillustrate the PCGrad algorithm in cases where gradients are conflicting. PCGrad projects the gradient of task Aonto the normal vector of task B’s gradient. In (c), we show that tasks with non-conflicting gradients are notaltered under PCGrad, thereby keeping tasks with constructive interference.
Figure 3: We show visualization of 50 tasks used in MT50 from Meta-World (Yu et al., 2019), which we use forour multi-task RL experiments. MT10 is a subset of the total 50 tasks, which includes reach, push, pick & place,open drawer, close drawer, open door, press button top, open window, close window, and insert peg inside.
Figure 4:  An analysis of the gradients during the first 1000 updates of training, on a toy 10-task sinusoidregression problem. Left: The cosine similarity between the gradients of 2 of the 10 tasks (selected arbitrarily,and fixed throughout this plot).  We observe a substantial amount of thrashing with standard Adam training,while Adam with PCGrad reduces the thrashing and leads to more closely aligned updates. Right: Adam withPCGrad improves performance compared to standard Adam.
Figure 5:  Learning curve on MT10, MT50 and goal-conditioned pushing.  PCGrad outperforms the othermethods in the three settings in terms of both success rates / average distance to the goal and data efficiency.
Figure 6: Ablation study on only using the magnitude and the direction of the gradients modified by PCGrad(left) and comparison between PCGrad and GradNorm (Chen et al., 2018) (right). PCGrad outperforms bothablations and GradNorm with a large margin, indicating the importance of modifying both the gradient directionsand magnitudes in multi-task learning.
Figure 7: Ablation study on using a fixed task order during PCGrad. PCGrad with a random task order doessignificantly better PCGrad with a fixed task order in MT50 benchmark.
