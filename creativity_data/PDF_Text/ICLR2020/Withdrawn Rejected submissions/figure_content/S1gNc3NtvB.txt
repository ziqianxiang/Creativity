Figure 1: The proposed architecture with its modules inspired by computer architectures. In this work themodules are based on neural networks. Information flow is divided into data and control streams. The modulesinside the highlighted area are learning the algorithmic solution in a reinforcement learning setting, whereasthe others (data modules) are learned independently in a supervised setting or can use hardcoded information.
Figure 2: Examples of search trees that the architecture implicitly learned to generate to solve a given symbolicplanning task, where si corresponds to task configurations and ak to ALU operations that transform the taskconfiguration. (a) corresponds to a task from curriculum level 3 with a maximum number of computationssteps of 15 including backtracking. (b) shows the tree for a task that required 330.631 computation steps(corresponds to level 82.656) that was solved by an algorithmic solution that triggered learning only until 15steps, the complexity shown in (a).
Figure 3: (a-c) The gray dashed line marks the maximum fitness and the colored lines show the fitness. Thelight colors indicate that the maximum fitness is achieved and no learning is triggered. The colored dashed linesindicate when a curriculum level was solved successfully. When no learning is triggered after a new level isunlocked, the model generalized to more complex tasks. The top numbers indicate the number of computationalsteps the model needs to perform correctly to solve samples of the associated curriculum level. (c) Comparisonwith the original DNC and a stack-augmented neural network on the Learning to Search task over 10 runs.
Figure 4: Transferring the learned algorithmic solution (left) to a new data representation (R2) and (middle &right) to two new task domains (R3). In all setups, all 200.000 samples over all curriculum levels are solvedcorrectly without triggering learning, indicated by the constant maximum fitness, showing the straightforwardtransfer of the learned solution â€” the abstract features R2 and R3 of the learned solution.
Figure 5: The behavior of the learned model on a task from Level 3 (see Sec. 3.2 for details) and the corre-sponding search tree that is constructing implicitly. In the search phase, the model fully explores one nodeby successively applying all operations, before reading the next node, until the goal is found. Then behaviorchanges in the backtrack phase, where the solution of the planning task is emitted as the states from start togoal in reverse order along with nop operations. The algorithmic behavior can also be seen in the repetitivepatterns of the attention vector, showing the five attention mechanisms for reading (temporal and usage linkagein both directions, and content lookup), that represents how strong each mechanism for reading is used in eachcomputation step.
Figure 7: Evaluation of an additional novelty signal and automatic restarts.
Figure 8: Evaluation of the introduced constrained write head.
Figure 9: Evaluation of the introduced usage-linkage attention and the hard attention memory access.
Figure 10: Evaluation of the usage-linkage attention on the Learning to Plan setup.
Figure 11: Evaluation of the bad memories mechanism.
