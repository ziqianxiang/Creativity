Figure 1: The framework of our end-to-end JAME model. The model is fitted with the adjacencymatrix A as input and outputs adjacency matrix A, attribute matrix X and binary label matrix Y,respectively. Shared Embedding layer aggregates information from structure, attribute and labelswhile Loss Weighting layer learns optimal weights for each embedding task.
Figure 2: Network visualization of JAME embeddings observing full labeled networks.
Figure 3: Link prediction performance of different methods for different embedding sizes5.7	Attribute InferenceAttribute inference aims at predicting the value of attributes associated to the nodes in the network.
Figure 4: Attribute inference performance of different methods for different embedding sizes5.8	Loss Weighting for Multi-Task LearningMulti-task learning concerns the problem of optimizing a joint model with respect to multiple ob-jectives. Training the model with different loss weights can effect the performance in multiple tasks.
Figure 5: Running time comparison of baselines with respect to the number of nodes on Facebook.
