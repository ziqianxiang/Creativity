Figure 1: Illustration of a one-hidden layer neu-ral net with d inputs, k hidden units and Koutputs along with a one-hot encoded label.
Figure 2: Depiction of the training and generalization dynamics of gradient methodsbased on the information and nuisance spaces associated with the neural net Jacobian.
Figure 3: The singular values of the normalizedJacobian spectrum KJCJ- J(Wo) of a neuralnetwork with K = 3. Here, the data is gener-ated according to the Def. 2.4 with K classesand σ = 0.1. The cluster centers are picked sothat the distance between any two is at least0.5. We consider two cases: n = 30C (solidline) and n = 60C (dashed line). These plotsdemonstrate that the top KC singular valuesgrow proportional to √n.
Figure 5: Evolution of the residual (rτ = f(Wτ ) - y) along the information/nuisancespaces of the final Jacobian on (a) the training data and (b) the test data and c)misclassification error on training and test. This experiment uses uncorrupted labels.
Figure 6: Evolution of the residual (rτ = f(Wτ ) - y) along the information/nuisancespaces of the Jacobian at 50 epochs on (a) training data and (b) test data and (c)misclassification error on training and test data. 50% of the labels have been corrupted.
Figure 7: Fraction of the energy of the labelvector that lies on the nuisance space of theinitial Jacobian and final Jacobian as wellas the test error as a function of the amountof label corruption.
Figure 8: Plots of the (a) total test error and (b) the test error components for themodel in Section 5.1. The test error decreases rapidly over the information subspacebut slowly increases over the nuisance subspace.
Figure 9: Scaled spectral densities of the full train Jacobian at different stages oftraining	InI (y)∣∣'2 怙必	InN (y)l'2 怙必	JI Mh 怙必	InI （，0儿2 什。必	InN （，0儿2 什。必	JI M2 什。必Ttrain *- Jinit	0.38081	0.92465	0.027224	0.37114	0.92858	0.027293Ttrain *- J fin?l	0.9869	0.16131	0.00070893	0.98669	0.1626	0.00070354τtest Jinit	0.38184	0.92423	0060229	0.37227	0.92812	0.06037τtest J final	0.80926	0.58746	0.0013734	0.80912	0.58764	0.0013716Table 3: Depiction of the alignment of the initial label/residual with the informa-tion/nuisance space using uncorrupted CIFAR-10 data.
Figure 10:	Experiments on the original, uncorrupted CIFAR-10 dataset. *: We discussthese plots in Section 4.
Figure 11:	Experiments with 25% label corruption onCIFAR-10 data	InI (y"'2	InN (y)l'2	JI Mg2	InI (r0)∣∣g2	InN (r0)∣∣'2	JI M2	怙必	怙必	怙必	什。必	什0必	什。必Ttrain Jinit.	0.31756	0.94824	0.0056031	0.325	0.94571	0.005592Ttrain- J final	0.50238	0.86465	000047518	0.50718	0.86184	0.00047967Table 6: Depiction of the alignment of the initial label/residual with the informa-tion/nuisance space using 75% label corruption on CIFAR-10 data.
Figure 12:	Experiments with 50% label corruption on CIFAR-10 data. *: We discussthese plots in Section 4.
Figure 13: Experiments with 75% label corruption on1001234---------------10 10 10ygrene laudiseR0	200	400	600	800Epochs100123---10 10 10ygrene laudiseR0	200	400	600	800Epochs(a)	Residual projection on initial train Jacobian(b)	Residual projection on final train Jacobian(c) Training and test error of experimentFigure 14: Experiments with 100% label corruption on CIFAR-10 data57
Figure 14: Experiments with 100% label corruption on CIFAR-10 data57Under review as a conference paper at ICLR 2020	InI⑹必	InN (y)l'2	JI Mg2	InI (，0儿2	InN (r0)∣∣'2	JI M2	怙必	怙必	怙必	什。必	什0必	什。必Ttrain- Jinit	0.31581	0.94882	0.0094479	0.31854	0.94791	0.0094092Ttrain- J final	0.47747	0.87865	000045241	0.47921	0.8777	0.00045344Table 7: Depiction of the alignment of the initial label/residual with the informa-tion/nuisance space using 100% label corruption on CIFAR-10 data.
Figure 15: Test error vs. final projection of labels on nuisance subspace for CIFAR-10experiments. See discussion of this plot in Section 4C.3 Experiments on subsampled 3-class CIFAR- 1 0We created a new dataset of 3 classes by sub-sampling the original CIFAR-10 dataset. To do this, wediscarded all training and test data except those belonging to classes 0 (airplane), 1 (automobile) and2 (bird) and sampled 3333 examples of each classes for a total of 9999 training images. We trainedthe neural network model described in Section 4 with no output scaling using SGD and Adam. Weapplied standard data augmentation (random crop and flip) to increase generalization. Informationsubspace is spanned by the top 50 singular vectors.
Figure 16: Experiments on 3-class uncorrupted CIFAR-10 dataset	InI (y 儿 2 回`2	InN (y 儿 2 回`2	JI Mlg2 怙心	InI （，0 儿 2 什。北2	InN (r0 儿2 什。L2	JI恤 什。必Ttrain Jinit		0.7239	0.68991	0.0054426	0.88552	0.4646	0.0040958Ttrain J 200epochs	0.97266	0.23224	0.0026234	0.96849	0.24905	0.0030069τtrain J fin?l		0.98743	0.15804	0.0031639	0.97606	0.21749	0.0034312Ttest Jinit		0.73366	0.67951	0010328	0.88827	045932	0.0077364τtest J final		0.8974	0.44123	00027082	0.89772	044057	0.0029383Table 8: Depiction of the alignment of the initial label/residual with the informa-tion/nuisance space using 3-class uncorrupted CIFAR-10 data.)59Under review as a conference paper at ICLR 202010-5012340 - - - -1 10 10 10ygrene laudiseR(a) Residual projection on initial train Jacobian0	100	200	300	400Epochs10-4
Figure 17: Experiments on 3-class uncorrupted CIFAR-10 data, trained with Adam	InI (y"'2 怙必	InN (y"g2 怙必	JI Mh 忖必	InI （，0儿2 什。必	InN (r0)∣∣'2 什。必	JI川2 什。1'2Ttrain Jinit.	0.7025	0.71169	0.0053554	0.8135	0.5815	0.0044353Ttrain- Jfinal	0.9969	0078332	00030954	0.9907	0.1361	0.0030632Table 9: Depiction of the alignment of the initial label/residual with the informa-tion/nuisance space on 3-class uncorrupted CIFAR-10 data, trained with Adam.
Figure 18: Experiments with 25% label corruption on 3-class CIFAR-10 data01230 - - -1 10 10ygrene laudiseR10-40	200	400	600	800Epochs(b) Residual projection on final train Jacobian	InI (y)∣∣'2 怙必	InN (y"g2 怙必	JI引1	InI (r0)∣∣3 什。I'2	InN (r0)∣∣'2 什。I'2	JI Tlg2 什。必τtrain- Jinit	0.63965	0.76866	0.0047649	0.76602	0.64282	0.0043665丁 train- Jfinal	0.90862	0.41763	00023974	0.9254	0.379	0.0021731Table 10: Depiction of the alignment of the initial label/residual with the informa-tion/nuisance space using 25% label corruption on 3-class CIFAR-10 data.
Figure 19:	Experiments with 50% label corruption on 3-class CIFAR-10 data61Under review as a conference paper at ICLR 2020	InI (y)∣∣'2 怙必	InN (y"'2 怙必	JI Mh 忖必	InI (r0)∣∣'2 什。必	InN (r0)∣∣'2 什。必	JI川2 什。1'2Ttrain Jinit	0.58664	0.80985	0.0017197	0.64281	0.76602	0.0019814丁 train- Jdip	0.61125	0.79144	0.0026809	0.65671	0.75415	0.0031267T train Jfinal	0.75143	0.65981	0.0018702	0.76311	0.64627	0.0012039τtest J i"it.	0.7236	0.69021	0.012765	-07594-	0.65062	-0.01235(test J dip .	0.81473	0.57984	0009931	-0.8322-	0.55447	0.010515τtest Jfinal	0.75362	0.65731	00033476	0.77225	0.63532	0.0021479Table 11: Depiction of the alignment of the initial label/residual with the informa-tion/nuisance space using 50% label corruption on 3-class CIFAR-10 data.
Figure 20:	Experiments with 75% label corruption on 3-class CIFAR-10 data	InI (y)l'2	InN (y 儿 2	JI Mg2	InI (r0)∣∣g2	InN (r0)∣∣'2	JI	忖必	忖必	怙必	什。I'2	什0必	什。1'2Ttrain Jinit	0.58032	0.81439	0.0014306	0.68898	0.72478	0.0015956Ttrain- Jfinal	0.64704	0.76246	0.0015808	0.78391	0.62088	0.00091715Table 12: Depiction of the alignment of the initial label/residual with the informa-tion/nuisance space using 75% label corruption on 3-class CIFAR-10 data.
Figure 21: Experiments with 100% label corruption on 3-class CIFAR-10 data	InI (y)l'2	InN (y 儿 2	JI y∣h	InI (r0)∣∣'2	InN (r0)∣∣'2	JI，1'2	忖必	忖必	怙必	什。I'2	什。必	什。1'2T train- Jinit	0.614	0.78931	0.0024274	0.78608	0.61813	0.0022444Ttrain- Jfinal	0.60094	0.7993	0001845	0.7657	0.6432	0.00098981Table 13: Depiction of the alignment of the initial label/residual with the informa-tion/nuisance space using 100% label corruption on 3-class CIFAR-10 data.
Figure 22:	Experiments on uncorrupted MNIST data64Under review as a conference paper at ICLR 2020ygrene laudiseR1001-00	100	200	300	400Epochs100ygrene laudiseR12-0 -00	100	200	300	400Epochs(b) Residual projection on final train Jacobian(a) Residual projection on initial train JacobianrorrE86420........................
Figure 23:	Experiments with 25% label corruption on MNIST data	InI (y"'2	InN (y"g2	JI Mh	InI (r0)∣∣'2	InN (r0)∣∣'2	JI恤	怙心	怙心	忖必	什。I'2	什。I'2	lr0l'2τtrain- Jinit	0.38906	0.92121	0.02197	0.38586	0.92256	0.022028Ttrain- J final	0.94093	0.33861	00010507	-0.9391	0.34365	0.0010331Table 15: Depiction of the alignment of the initial label/residual with the informa-tion/nuisance space using 25% label corruption on MNIST data.
Figure 24: Experiments with 50% label corruption on MNIST data0	100	200	300	400Epochs(f) Training and test error of experiment	InI⑹必	InN (y)l'2	JI Mg2	InI (，0儿2	InN (r0)∣∣'2	JI M2	怙必	怙必	怙必	什。必	什0必	什。必Ttrain Jinit	0.31995	0.94744	0.0061168	0.32072	0.94718	0.0061618Ttrain- J final	0.56007	0.82844	000063427	0.55869	0.82938	0.00062509Table 17: Depiction of the alignment of the initial label/residual with the informa-tion/nuisance space using 75% label corruption on MNIST data.
Figure 25: Experiments with 75% label corruption on MNIST data0	100	200	300	400Epochs(a) Residual projection on initial train Jacobian01230 - - -1 10 10 10ygrene laudiseR0	100	200	300	400Epochs(b) Residual projection on final train JacobianFigure 26: Experiments with 100% label corruption on MNIST data67Under review as a conference paper at ICLR 2020	InI⑹必	InN (y)l'2	JI Mg2	InI (，0儿2	InN (r0)∣∣'2	JI M2	怙必	怙必	怙必	什。必	什0必	什。必Ttrain- Jinit	0.31793	0.94811	0.0050114	0.34633	0.93811	0.0050949Ttrain- J final	0.57769	0.81626	000060636	0.59174	0.80613	0.00062812Table 18: Depiction of the alignment of the initial label/residual with the informa-tion/nuisance space using 100% label corruption on MNIST data.
Figure 26: Experiments with 100% label corruption on MNIST data67Under review as a conference paper at ICLR 2020	InI⑹必	InN (y)l'2	JI Mg2	InI (，0儿2	InN (r0)∣∣'2	JI M2	怙必	怙必	怙必	什。必	什0必	什。必Ttrain- Jinit	0.31793	0.94811	0.0050114	0.34633	0.93811	0.0050949Ttrain- J final	0.57769	0.81626	000060636	0.59174	0.80613	0.00062812Table 18: Depiction of the alignment of the initial label/residual with the informa-tion/nuisance space using 100% label corruption on MNIST data.
Figure 27: Test error vs. final projection of labels on nuisance subspace for MNISTexperimentsC.5 Singular value decomposition of the JacobianThe matrix representation of the Jacobian arising in the numerical experiments is in general verylarge and pose a computational challenge. In particular, the Jacobian matrix for a ResNet20 model is500k × 270k (or 540 GB memory with float32 entries, more than 1 TB with float64) on the CIFAR-10training data. Storing these matrices in memory for singular value decomposition is not feasible andtherefore other methods are necessary to approximate the singular values and singular vectors.
