Figure 1: The concept of online Adversarial Feature map Distillation (AFD) Each point representsa feature map for the corresponding input denoted by different colors. The thin line arrow indicatesthe evolvement of feature map data points as iteration goes on and the broader arrow indicates theway each method compares the feature maps from different networks. (a) In direct feature mapalignment, networks are trained such that the distance between each pair of points with the samecolor is minimized. (b) In AFD, the discriminators contain information on feature map distributionsand thus the networks are trained such that the distributions match. (best viewed in color)However, aforementioned online distillation methods make use of only the logit information. Whilethe logit contains the probabilistic information over classes, the feature map, the output of convo-lution layer, has more meaningful and abundant feature information on image intensity and spatialcorrelation. In offline distillation which utilizes a pre-trained model as a teacher network, manymethods such as FitNet (Romero et al., 2014), attention transfer (AT) (Zagoruyko & Komodakis,2016a) and factor transfer (FT) (Kim et al., 2018) make use of this intermediate feature repre-sentation as a target to learn for the student network, but in online distillation, to the best of ourknowledge, no feature map-based knowledge distillation method has been proposed.
Figure 2: Overall schematic of online adversarial feature map distillation (AFD). At feature map-level, each network is trained to deceive the corresponding discriminator so that it can mimic theother network’s feature map distribution. While at logit-level, KL loss to learn the peer network’slogit is applied as well as the conventional CE loss.
Figure 3: Schematic of cyclic-learning framework for training 3 networks simultaneously.
