Figure 1: Number of linear regions in log scale v.s.
Figure 2: Visualization of NoisyMNIST in (a) and convergence plots for baseline and attentionmodels on NoisyMNIST, including (b) plot of test loss and (c) plot of test accuracy over iterations.
Figure 3: Top 100 eigenvalues of the Hessian matrix forbaseline and attention models.
Figure 4: Convergence plots for classification tasks: Image size: 48 × 48, Digit patch size s一泼.εJ ‰⅛U3W‹1526Under review as a conference paper at ICLR 2020Learning rate: 0.005Figure 5: Convergence plots for classification tasks: Image size: 48 × 48, Digit patch size sLearning rate: 0.001827Under review as a conference paper at ICLR 2020《泼,εJ ‰⅛U3W≤Learning rate: 0.001Figure 6: Convergence plots for classification tasks: Image size: 48 × 48, Digit patch size s528Under review as a conference paper at ICLR 2020SSS
Figure 5: Convergence plots for classification tasks: Image size: 48 × 48, Digit patch size sLearning rate: 0.001827Under review as a conference paper at ICLR 2020《泼,εJ ‰⅛U3W≤Learning rate: 0.001Figure 6: Convergence plots for classification tasks: Image size: 48 × 48, Digit patch size s528Under review as a conference paper at ICLR 2020SSSFigure 7: Convergence plots for regression task varying the number of samples Ns . "Baseline"indicates 2- layer NN not using attention, "attention" denotes attention models, and "attentionregualrized" denotes attention models trained with L1 regularization29Under review as a conference paper at ICLR 2020Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
Figure 6: Convergence plots for classification tasks: Image size: 48 × 48, Digit patch size s528Under review as a conference paper at ICLR 2020SSSFigure 7: Convergence plots for regression task varying the number of samples Ns . "Baseline"indicates 2- layer NN not using attention, "attention" denotes attention models, and "attentionregualrized" denotes attention models trained with L1 regularization29Under review as a conference paper at ICLR 2020Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds globalminima of deep neural networks. In Proceedings of the 36th International Conference on MachineLearning, ICML ’19, 2019.
Figure 7: Convergence plots for regression task varying the number of samples Ns . "Baseline"indicates 2- layer NN not using attention, "attention" denotes attention models, and "attentionregualrized" denotes attention models trained with L1 regularization29Under review as a conference paper at ICLR 2020Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds globalminima of deep neural networks. In Proceedings of the 36th International Conference on MachineLearning, ICML ’19, 2019.
