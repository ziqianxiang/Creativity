Figure 1: Column-1: Average l2 distance between the logits of clean and its corresponding noisy im-ages (dper) versus iterations. Column-2: Average l2 distance between the logits of images belongingto the same class (dintra) in a training mini-batch versus iterations. Column-3: Average l2 distancebetween the logits for images belonging to different classes (dinter) in a training mini-batch versusiterations. Rows represent the training method. Row-1: Normal training. Row-2: PGD adversarialtraining.
Figure 2: Plot of accuracy on test set versus perturbation strength () of PGD attack obtained formodels trained on MNIST and CIFAR-10 datasets using different training methods. PGD attackwith steps=40 is used for MNIST, and for CIFAR-10 steps=7 is used. Note: Legends are the samefor both the plots.
Figure 3: Plot of average loss on test set versus perturbation strength () of FGSM attack, obtainedfor models on MNIST and CIFAR-10 datasets respectively using the proposed training method.
Figure 4: Effect of hyper-parameter(Î») of the proposed training method. For PGD attack, we set=0.3, step=0.01 and steps=100. Note that x-axis is in log scale.
Figure 5: Plot of accuracy on test set versus per-turbation strength () of PGD attack obtained formodels trained on Fashion-MNIST dataset us-ing different training methods. PGD attack withsteps=40 is used.
Figure 6: Plot of average loss on test set versusperturbation strength () of FGSM attack, ob-tained for the model trained using the proposedtraining method on Fashion-MNIST dataset.
