Figure 1: 100-dimensional word2vec embeddings projected into a 2-dimensional space demonstrateimpressive semantic structure. Common household items are clustered in specific relationships tothe rooms in which they tend to be located. The angle of correspondence is at least as significant asproximity. Of particular interest is the word ‘mirror’, which is aligned both with the items found ina bedroom and with the items found in a bathroom.
Figure 2: Cosine distances between sentences embedded using various neural models. Each sen-tence pair comprises two rows, with distances shown after the second component sentence. Mostembedding models place the syntactically similar but semantically distinct sentences ‘I am a cat’and ‘I am not a cat’ startlingly close to one another. Only the recently-developed embedding modelsInferSent and Transformer-XL (bolded) embed these sentences to positions that are farther apartthan the semantically similar sentences ‘I am a cat’ and ‘I am a domesticated cat’.
Figure 4: Indexing labels chosen by six neural embedding models when applying the methodologydescribed in Section 4.1. Correct answers are shown in bold-face type. Interestingly, the best per-forming models tend to select a label in the right general category even when they do not select theoptimal answer.
Figure 3: Object labels Ai and descriptionsBi that define the canonical vector V used inthe experiments for Figure 4.
Figure 6:	Relation extraction. Given a starting node label, vector offsets were used to identifyrelations that fall loosely under the ’is-a’ umbrella, such as ’occupation’, ’instance-of’ and subclass-of’. Relations that could be verified in WikiData are bolded. Italicized orange text indicates relationsthat are not reflected in WikiData, but that perhaps should be.
Figure 5: Example texts Ai and Bi used todefine the vector V between a descriptionand the corresponding object(Ai , Bi), where each Ai is a node label and eachBi is the label of an object that fulfills the desiredrelation, extrapolations to new input nodes can be calculated in the following manner:1.	Create a set of canonical vectors Vi = Bi - Ai2.	For each previously unseen node label x, convert x into a vector representation v3.	Find the indexing point p = v + Vnearest , where Vnearest is the canonical vector whoseinitial point Ai has the closest cosign distance to v4.	Search the set of possible output nodes to find l* = argminunL distcos(li,p)Note that, when indexing as per Section 4.2, we calculated a single canonical vector V based onall of the exemplar (Ai, Bi) pairs. When finding relations, however, we seek to match the givenexemplar relations as closely as possible. Thus we use a suite of vectors Vi , selecting the one whosesource word Ai is most similar to the input node vector v. This relational vector Vi is then used toquery the knowledge base.
Figure 7:	Relation extraction using the InferSent embedding model, showing the first four query re-sults. Bolded text indicates relations that exist in WikiData. Orange italicized text indicates relationsthat are not present in WikiData, but that are arguably correct. Green italicized text indicates pluralversions of valid relations found in WikiData.
Figure 8:	Left: Example directives and action labels used in the UAV control task. Right: Illus-tration of the UAV control task. Sentence represenations are used to map user directives to actionlabels, which in turn can be mapped to direct motor controls.
Figure 9:	Classification accuracy on a UAV control task in which human utterances are mapped tothe most similar action label based on cosign distances within the embedding space.
Figure 10: Classification accuracy on three tasks related to semantic geometry. The highest value ineach column is bolded.
