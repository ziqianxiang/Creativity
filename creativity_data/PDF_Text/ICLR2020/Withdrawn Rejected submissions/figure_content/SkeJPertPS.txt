Figure 1: Split examples in decision tree according to the split functions: (a) the conventionalmethod chooses the split that maximizes the information gain. (b) In contrast, the proposed methodadditionally enforces the size of child nodes to be equal, resulting in a random balanced tree. Notethat CoBRF has far mode nodes which improves the generalization ability for domain adaptation.
Figure 2: Hyperplanes by the proposed methods. (a,b) The hyperplanes estimated by binary pseudolabels followed by translation for even split. Dotted line is the hyperplane estimated using linearSVM. The data are evenly split by the hyperplane shift (solid lines). Among these hyperplanes, theone with maximum information gain is chosen: yellow hyperplane in (a). (c) In CoBRF, both thesource information gain and target entropy is considered. The yellow is better in source informationgain, the target data split is biased, while the blue splits the source and target evenly well.
Figure 3: Visualization of trees learned by the proposed methods. The white and gray circles at theleaf nodes represent the source and target data fallen into the node, respectively. As a tree withoutdomain alignment only considers the labeled source data, the target data distributions in leaf nodesare not even, whereas that with domain alignment generates more uniform splits. Refer to Sec. 3.2and Fig. 2c.
