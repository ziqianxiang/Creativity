Figure 1: CIFAR 10 + Res-18. We compare Optimistic-AMS Grad with AMSGrad in terms oftraining (cross-entropy) loss, training accuracy, testing loss, and testing accuracy. All measures areplotted against the numbers of epochs. (One epoch means all training data points are used once). Wecan see that Optimistic-AMSGrad noticeably improves AMSGrad in all four measures.
Figure 2: CIFAR 100 + Res-18. We compare Optimistic-AMS Grad with AM S Grad in terms oftraining (cross-entropy) loss, training accuracy, testing loss, and testing accuracy.
Figure 3: MNIST-back-image + CNN. We compare OPTIMISTIC-AMSGRAD with AMSGRAD interms of training (cross-entropy) loss, training accuracy, testing loss, and testing accuracy.
Figure 4: CIFAR 10 + Res-18. We compare three methods: Optimistic-AMSGrad, AMSGrad,and OPTIMISTIC-ADAM+^t, in terms of training (cross-entropy) loss, training accuracy, testingloss, and testing accuracy. We observe that Optimistic-AMSGrad consistently improves the twobaselines.
Figure 5: CIFAR 100 + Res-18. We compare three methods: Optimistic-AMSGrad, AMSGrad,and OPTIMISTIC-ADAM+Vt, in terms of training (cross-entropy) loss, training accuracy, testing loss,and testing accuracy.
Figure 6: MNIST-back-image noisy dataset + a four-layer convolutional neural network.
Figure 7: CIFAR 10 + Res-18. We compare OPTIMISTIC-AMSGRAD (for = 3,5, 10)withAMS-Grad in terms of training (cross-entropy) loss, training accuracy, testing loss, and testing accuracy.
Figure 8: CIFAR 100 + Res-18. We compare OPTIMISTIC-AMSGRAD (for = 3, 5, 10) withAMS Grad in terms of training (cross-entropy) loss, training accuracy, testing loss, and testingaccuracy. Again, the choice of r does not affect the results too much.
Figure 9: MNIST-back-image noisy dataset + a four-layer convolutional neural network.
