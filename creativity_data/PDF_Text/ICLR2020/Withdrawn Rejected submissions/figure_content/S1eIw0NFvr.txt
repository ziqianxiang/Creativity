Figure 1: Distributions of top-1 and top-5 model accuracy for populations of independently trainedsparse and non-sparse models on ImageNet and CIFAR-10. The distributions for CIFAR-10 top-5accuracy (not shown) are tightly clustered and overlapping in [0.9965, 1.0]. The distributions arefairly tight with one outlier for the ImageNet baseline model.
Figure 2: Visualization of pruning identified exemplars (PIE30) for the CIFAR-10 dataset. This subsetof impacted images is identified by considering a set of 30 non-sparse wide ResNet models and 30models trained to 30% sparsity.
Figure 3: Normalized recall difference (bars) and absolute recall difference (points) per class for 70%sparsity (left) and 90% sparsity (right). The class labels are sampled for readability; there are 317significant classes for 70% sparsity and 582 significant classes for 90%. Note the difference in scaleon the y-axis.
Figure 4: Excluding pruning identified exemplars (PIE) improves test-set top-1 accuracy for both Ima-geNet and CIFAR-10. This holds for PIE images identified at all levels of sparsity considered. Sparsemodel find generalizing to PIE more challenging, and inference on PIE images alone substantiallydegrades generalization performance. Left: Average top-1 test-set accuracy across 30 non-sparseResNet-50 models when inference is restricted to PIE images (blue), non-PIE images (dark purple)and a random sample of the test set (black line) which is a constant independent of PIE sparsity level.
Figure 5: Limited human study of the relative distribution of PIE and non-PIE properties. Challeng-ing exemplars: images positively codified as showing common image corruptions such as blur oroverlaid text, or images where the object is in the form of an abstract representation or where theexemplar requires fine grained classification. Poorly specified task: images where multiple classesare visible in the same image, or images with incorrect or insufficient ground truth.
Figure 6: Visualization of PIE90: images where modal label differs between 30 non-sparse ResNet-50 models and 30 models trained to 90% sparsity on ImageNet. A qualitative inspection of PIEssuggests that there are certain shared properties between the images most impacted by pruning; theseimages tend to be of lower image quality and frequently exhibit image corruptions (overlaid text,contrast, motion blur, zoom), mislabelled or unintuitive label, depict class in abstract form, requirefine-grained classification or depict atypical class examples.
Figure 7: Sparse models are less robust to natural adversarial examples. At high levels of sparsity,models are also more brittle to common image corruptions. We measure the relative Top-1 and Top-5test set ResNet-50 accuracy normalized by average sparse model performance on an uncorruptedImageNet test set. Left: Mean test-set accuracy on ImageNet-A (across 30 models). Right: Test-setperformance on a subset of ImageNet-C corruptions. An extended list of all corruptions considered isincluded in the appendix.
Figure 8: Expanded version of the 70% sparsity chart from Figure 3. Normalized recall difference(bars) and absolute recall difference (points) per class. Every third class label is shown.
Figure 9: Images surfaced by PIE evidence common corruptions such as motion blur, defocus orpost-processing with overlaid text. Many PIE images depict objects in an abstract form, such as apainting, drawing or rendering using a different material. PIEs displayed were identified by comparingthe modal label of a set of 90% sparse and non-sparse ResNet-50 models.
Figure 10: Excluding pruning identified exemplars (PIE) improves test-set top-1 accuracy for bothImageNet and CIFAR-10. The sensitivity to PIE images is amplified at higher levels of sparsity.
