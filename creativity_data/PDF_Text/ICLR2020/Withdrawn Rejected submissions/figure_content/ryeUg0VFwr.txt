Figure 1: Median normalized online evaluation scores averaged over 5 runs (shown as traces) across stochasticversion of 60 Atari 2600 games of (a) offline RL agents trained using the DQN replay dataset containing 200million frames. Offline REM outperforms offline QR-DQN and surpasses gains from online C51. (b) online RLagents trained for 200 million frames (standard protocol). Online REM with 4 Q-networks performs comparablyto online QR-DQN. Each iteration corresponds to 1 million frames.
Figure 2: Normalized performance improvement (in %) over online DQN (Nature), per game, of (a) offlineDQN (Nature) and (b) offline QR-DQN trained using the DQN replay dataset for same number of gradientupdates as online DQN. The normalized online score for each game is 0.0 and 1.0 for the worse and betterperforming agent among fully trained online DQN and random agents respectively.
Figure 3: Neural network architectures for DQN, QR-DQN and the proposed expected RL variants, i.e.,Ensemble-DQN and REM, with the same multi-head architecture as QR-DQN. The individual Q-heads share allof the neural network layers except the final fully connected layer.
Figure 4: Offline REM vs. Baselines. Normalized scoresaveraged over 5 runs (shown as traces) across 60 stochasticAtari 2600 games of offline agents trained using the DQNreplay dataset trained for 5X gradient steps.
Figure 5: Effect of Dataset Size. Normalized scores (averaged over 5 runs) of QR-DQN and multi-head REMtrained offline on stochastic version of 5 Atari 2600 games for 5X gradient steps using only a fraction of theentire DQN replay dataset (200M frames) obtained via randomly subsampling trajectories.
Figure 6: Effect of Dataset Quality. Normalized scores (averaged over 3 runs) of QR-DQN and multi-headREM trained offline on stochastic version of 5 Atari 2600 games for 5X gradient steps using logged data fromonline DQN trained only for 20M frames (20 iterations). The horizontal line shows the performance of bestpolicy found during DQN training for 20M frames which is significantly worse than fully-trained DQN.
