Figure 1: A. Configuration of two parameter vectors Ql(k) and Q(mk) (black) at a minimum θ and apotential path (green) towards a permutation point θl(⇔k)m (?). The path is parametrized by the distanced. Along the path the distance d (blue dashed lines) decreases continuously starting at dl(,km) (θ). Notethat the path can lead to a permutation point far away from the initial configuration, outside the linearsubspace spanned by the parameter vectors Ql(k) and Q(mk). B. We exclude hypothetical paths wherethe distance along the path increases.
Figure 2: Paths to the same or to different permutation points. Top row. Configuration of the 5weight vectors Wi(,1:)/bi(1) of the first layer at the global minimum (blue) and at a permutation point(red) reached after merging the parameter vectors of two neurons. Note that the global minimum (i.e.,the starting configuration) is the same for (A), (B) and (C) but the loss at the permutation point canbe the same - (A) and (B) - or different - (A) and (C) - depending on the pair of neurons chosen formerging. Numbers indicate neurons. Bottom row. Quadratic loss L as a function of the distance dbetween the neurons to be merged. The distance was decreased in 200 logarithmically spaced stepsfrom d = d(1m(θ*) to 1/104 of the initial value. For each d, full batch gradient descent on the loss Lwas performed until convergence. Training data was generated by sampling 103 two-dimensionalinput points xμ from a standard normal distribution and computing labels yμ = f (xμ; θ*) using ateacher network (shown in blue, equivalent to the configuration at the global minima). The teacherhad a single layer of five hidden neurons with rectified-linear activation function g and one linearoutput layer.
Figure 3: A low-loss permutation path in the loss landscape of a multi-layer network using astudent-teacher setup trained on MNIST. We merged the parameter vectors of two neurons withhigh cosine-similarity in the second hidden layer of a three-layer student network. The correspondingteacher network with H = 10, 15, 20 or 25 was trained on MNIST. For each hidden layer size wetrained 6 teacher networks with different random seeds and display one curve per hidden layer sizeand seed. A. In most cases, the mean squared loss L between teacher and student output increasesmonotonically along our constructed paths from a global minimum until the permutation point. Inthese cases the latter corresponds to the loss barrier along the path. Note that the barrier height (lossat saddle) decreases with H . B. The MNIST classification accuracy on the training set decreases onlymarginally when moving to a permutation point.
Figure 4: A. The loss landscape (schematic) as a function of two parameters: the difference Wm(k,)i -Wl(,ki ) between the weights from neuron i to neurons m and l in layer k and the weight Wn(k,m+1)from neuron m to neuron n in the next layer (see B for network graph). The red curve indicates thepath from one of the global minima (red triangle) to a half-way configuration where the differencebetween the input weight vectors of neurons m and l in layer k vanishes (Wm(k,)i = Wl(,ki), redsquare). Along the axis Wm(k,)i - Wl(,ki) = 0, we can change the output weight Wn(k,m+1) at constantloss (dashed horizontal line), as long as the sum Wn(k,m+1) + Wn(k,l+1) = c remains constant. The pointWn(k,m+1) = Wn(k,l+1)where the two output weights are identical (and assuming the same matchingcondition for the other output weights) defines the permutation point θl(⇔k)m (red ?) where we canswap the indices of neurons m and l in layer k at equal loss and continuously in parameter space. Ifwe then shift (dashed green line) all output weights of neuron m in layer k to zero (Wn(k,m+1) = 0 forall n, green filled circle), we are free to change the weight Wm(k,)i at constant loss (green arrows) so asto perform further permutations of neurons in layer k.
Figure 5: Loss L (vertical axis) on the permutation path as a function of the distance d between thetwo parameter vectors to be permuted (schematic). A. In the teacher network the two parametervectors have a distance d°. Along the path, the distance is reduced to zero. At the permutation point(*), the loss reaches a maximum which corresponds to a saddle point of the total loss function. B.
Figure 6: Visualizing how permutation points arise in between global minima for a (hypothetical)network function with scalar inputs and one hidden layer with three neurons. Here we do not considerbiases for simplicity. Blue dots: 3! many equivalent global minima. A red ?: One permutation point(of first order at layer k = 1) represented by its weights in the first layer, i.e. (2.5, 2.5, 0.4). All red ?:One permutation set where the weight value 2.5 is duplicated. All green ?: The other permutationset where the weight value 0.4 is duplicated. Note that overall, we have 6 permutation points thatgive rise to the same network function as the weight configuration with two hidden neurons with(2.5,0.4). Indeed, T(K = 1,nι = 3) = (3-1) 11 = 1 confirms Why the number permutation pointscorresponding to this particular weight configuration with two hidden neurons is equal to the numberof global minima. Note that the Weight values are assigned randomly for visualization purposes.
Figure 7: A. Zooming in permutations points of one of the permutation sets (red ? in Fig. 6). B.
