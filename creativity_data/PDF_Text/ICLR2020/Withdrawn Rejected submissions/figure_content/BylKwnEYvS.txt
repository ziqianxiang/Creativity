Figure 1: A non-convex loss surface is illustrated in a). In general, the loss will be non-convex onstraight paths connecting random points x。, Xb and the global minimizer x*. We consider a modelof NMF with a planted solution, and as shown in b) the loss is typically convex on straight pathsbetween points xa and a planted solution x* . Additionally, as illustrated in c), the loss is typicallyconvex on straight paths between points xa and xb .
Figure 2: The function (|x|p +|y|pq1{p is an example of a star-convex function for 0 < P < 1. Itis non-convex in general, but con-vex towards (0, 0q.
Figure 3: We here illustrate the loss surface of NMF on straight paths connecting two random pointsfor 8 real-world datasets. We overlap 5 independent lines for each dataset. Note that the curves arealways convex, suggesting that the loss surface is "typically" convex.
Figure 4: The loss surface of NMF along the straight line from a random point w0 to the local optimaw* found via gradient descent from an independent starting point. We overlap 5 independent lines,zoom in for detail. For all datasets, this loss surface is convex, which is in line with what one wouldexpect from Theorem 1.
Figure 5: Illustration of how the relative deviation σ{μ of the curvature equation 6 depend on thedataset size. For all datasets, the relative deviations decrease with more samples, suggesting that the(positive) curvature become increasingly concentreated around its mean for larger matrices.
Figure 6: a) and b) show the cosine similarity between the negative gradient —V'(U, V) and thestraight line pU0, V0q → (U*, V*), during training, for the ORL and movielens dataset. Noticethat the cosine similarity is large and non-negative throughout training, showing that the gradientslargely follow a straight line. c) and d) illustrate how the curvature equation 6 varies during trainingfor the ORL and movielens dataset. Note that it is always positive, and thus that the function satisfiesstar-convexity. Shaded regions represents 95 % confidence computed over 5 iterations.
Figure 7: Histogram of singularvalues σ% for found U* and a ran-dom matrix. Note the similarity ofthe spectrum. Best viewed in color.
Figure 8: The loss landscape of a 110-layer Resnet architecture at epoch 200 along two randomdirections, visualized as in Li et al. (2018). The network in the right image is four times as wide,and its loss landscape is increasingly convex. In Table 2 we generalize this idea, showing that thelength scale of local convexity increases with network width.
Figure 9: The curvature equation 6 during training, as in Figure 6, for various datasets. Note that itis always positive, and thus that the function satisfies star-convexity. Note the small shaded regions,which represents the 95 % confidence computed over 5 repetitions.
Figure 10: The cosine similarity between the negative gradient —V'(U, V) and the straight linepU0,V0q → (U*, V*), during training as in Figure 6. Note the small shaded regions, whichrepresents the 95 % confidence computed over 5 repetitions.
Figure 11: Illustration of how the relative deviation σ{μ of the curvature equation 6 depend onthe dataset size as in Figure 6. Unfortunately, computing these figures for the larger Netflix andGoodbooks datasets is computationally infeasible for us.
Figure 12: Histogram of singular value spectra for U* found via gradient descent and a randommatrix of the same shape with entries drawn iid from a half-normal distribution. For the datasetswith larger ranks, the spectra typically agree significantly.
Figure 13: Histogram of singular value spectra for V* found via gradient descent and a randommatrix of the same shape with entries drawn iid from a half-normal distribution. For the datasetswith larger ranks, the spectra typically agree significantly.
