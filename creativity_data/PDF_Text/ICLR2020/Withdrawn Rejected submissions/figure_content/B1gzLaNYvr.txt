Figure 1: Plot at the top shows the original input while plot at the bottom indicates the reconstructedinput from the raw auto-encoder or the fine-tuned auto-encoder.
Figure 2: Plot at the top shows the original input while plot at the bottom indicates the reconstructedinput either from TSInsight or the auto-encoder with just sparsity inducing norm.
Figure 3: Output from different attribution methods as well as the input after suppressing all thepoints except the top 5% highlighted by the corresponding attribution method on an anomalousexample from the synthetic anomaly detection dataset.
Figure 4: Suppression results against a large number of baseline methods averaged over 5 randomruns. Enlarged version of the plots is available in Appendix H.
Figure 5: Spectrum analysis of the auto-encoderâ€™s average Jacobian computed over the entire testset of the forest cover dataset. Large singular values corresponds to directions which the networkretained. The sharp decrease in the spectrum for TSInsight suggests that the network was successfulin inducing a contraction of the input space.
Figure 6: Accuracy of the adversarial examples curated using FGSM attack with varying max .
Figure 7: System PipelineMarina M-C Vidovic, Nico Gornitz, KlaUs-Robert Muller, and Marius Kloft. Feature importancemeasure for non-linear learning algorithms. arXiv preprint arXiv:1611.07567, 2016.
Figure 8: Loss landscape where the bottom surface indicates the manifold for the classifier while thesurface on the top indicates the manifold for the auto-encoder attached to the classifier.
Figure 9: Auto-encoder training with different base models (CNN and LSTM). TSInsight was able todiscover salient regions of the input regardless of the employed classifier.
Figure 10: Enlarged suppression plots (a). Copy of Fig. 4.
Figure 11: Enlarged suppression plots (b). Copy of Fig. 4.
Figure 12: Enlarged suppression plots (c). Copy of Fig. 4.
Figure 13: Enlarged suppression plots (d). Copy of Fig. 4.
