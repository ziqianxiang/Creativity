Figure 1: (Top) Structure and possible interpretation of a training entry.
Figure 2: Model architecture3.2	Learning StrategyOne of the major goals of this work is to introduce a representation learning mechanism thatcombines several observations and is focused on discovering regularities underlying them (implicitinformation) rather than on compressing input data into more interpretable features of explicitly3Under review as a conference paper at ICLR 2020presented information. We suggest that one of the possible ways to do that is to learn representationsnot independently but in the process of observation facilitated inference. Hence, our dataset onlycontains the results for the inference problems but does not label the underlying regularities. In fact,in some cases, it is possible to assign several labels to a single regularity (for details see Section 4)which makes labeling counterproductive. Such an approach has its own limitations, yet it is based oncertain aspects of representation learning in biological systems (see Section 5.2 for details).
Figure 3: Learned representations of mathematical problems, associative recall, and contextual2-armed banditcalculated the distances between these points to illustrate the relative position of the representations.
Figure 4: Distance between manifolds of learned representationsTo prove that our model learns representations empirically shaped by the objective of the inferenceas opposed to an a priori meaning of underlying regularities, we coupled observations of binaryoperations with unrelated inference problems (e.g. Ox3=x1>x2 with Px3=x1 div x2 for all Ox3=x1>x2and Px3=x1 div x2). Performance of the model did not change significantly. Therefore, we concludedthat, unless observation contains necessary information (e.g. like in the case of associative recall or5Under review as a conference paper at ICLR 20202-armed bandit tasks), a correct inference can be supported by any regularity that empirically appearsto be useful.
Figure 5: Representation learned by β-VAE (with β ∈ [1, 16, 64])(b) β = 165	Related Work5.1	Representation LearningOne of the major goals of representation learning has been an ability to encode raw data in the waysuch an encoding could be useful in subsequent learning of a downstream task (Kingma & Ba, 2014;Rasmus et al., 2015; Hsu et al., 2018) or have useful qualities, such as interpretability, smoothness ofthe manifold, explanatory power, sparsity, disentanglement of the underlying factors of variation, etc.
