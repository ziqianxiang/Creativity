Figure 1: Projecting normalised gradient vectors from lordinary and l(un)targeted onto one another.
Figure 2: Randomly-selected images generatedby a GAN finetuned to attack Wong & Kolter’s(2018) classifier, which is robust to perturbations.
Figure 3: Selected successful targeted unre-stricted adversarial examples on ImageNet, gener-ated by a BigGAN (Brock et al., 2019) finetunedto attack ResNet-152 (He et al., 2016).
Figure 4:	The success rates of the adversarialattacks by finetuned GANs (the computed labelmatches the target label and the true label remainsthe same).
Figure 5:	How often adversarial images are notidentified as being generated. If the generatedimages were completely realistic, the expectedresult would be 90.
Figure 6: Plots showing attack efficacy in the presence of adversarial training.
Figure 7: Successful targeted unrestricted adver-sarial examples for target class ‘tabby cat’.
Figure 9: Successful targeted unrestricted adver-sarial examples for target class ‘orange’.
Figure 8: Successful targeted unrestricted adver-sarial examples for target class ‘slug’.
Figure 10: Successful targeted unrestricted ad-versarial examples for target class ‘church’.
Figure 11: Samples for fixed inputs to the BigGAN implementation we use, taken 15 gradient stepsafter the checkpoint we begin adversarial finetuning from.
Figure 12: Examples generated by one adversarially-finetuned GAN to perform an untargeted attackon Wong & Kolter’s (2018) classifier, which is provably robust to perturbation attacks.
Figure 13:	A sequence of images tracking the output of the generator network for one fixed randomsample in latent space as adversarial finetuning takes place. Five samples are given for each intendedtrue label. The finetuning is an untargeted attack against Wong & Kolter’s (2018) provably-robustnetwork.
Figure 14:	The success rates of the adversarialattacks by finetuned GANs. More precisely, ofgenerated images for which the computed labeloutput by the classifier matches the target label,the percentage which are truly adversarial (in thesense that the true label of the image matchesthe intended true label passed to the generatornetwork) is reported.
Figure 15:	The success rates of the adversarialattacks by a pretrained but not finetuned GAN.
Figure 16:	Measures of how realistic the adver-sarial images generated by finetuned GANs are.
Figure 17:	Measures of how realistic the adversar-ial images generated by a pretrained but not fine-tuned GAN are. More precisely, the proportionof generated inputs for which the classified labelmatches the target label which were not identifiedas being generated when placed amongst nine im-ages from the training dataset. If the generatedimages were completely realistic, the expectedresult would be 90.
Figure 18:	Results against Wong & Kolter (2018)generated by adversarially finetuned GANs.
Figure 19:	Results against Wong & Kolter (2018)generated by a pretrained but not finetuned GAN.
Figure 20:	Results against an ordinary neural net-work generated by adversarially finetuned GANs.
Figure 21:	Results against an ordinary neural net-work generated by a pretrained but not finetunedGAN.
Figure 22:	Adversarial success rates for a non-finetuned generator, as described above.
Figure 23:	Realism rates for adversarial imagesproduced by a non-finetuned generator, as de-scribed above. If the generated images were com-pletely realistic, the expected result would be 90.
Figure 24:	On MNIST we pretrained the GAN for 705,000 iterations, and then finetuned for another45,000. Here, we show results using the same setup as in Figure 13. We show the results after thenumber of iterations equivalent to pretraining, to pretraining and finetuning, and at convergence. Thefinal results both took longer and are visually less convincing than comparable results in Figure 13.
Figure 25:	Adversarial finetuning with naive generator loss lordinary +1Untargeted. As expected, our customloss function as described in section 3.3 significantly improves convergence to generator weightswhich continue to generate images realistic enough to maintain their true classes.
Figure 26: The effect of attack rate μ on image quality and proportion misclassified, using otherwise thesame setup as in Figure 13. An attack rate of 1 is equivalent to not having the attack rate. As expected,lower attack rates mean higher visual quality, but a less successful attack.
