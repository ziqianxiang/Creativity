Figure 1: On the left: matrix encoding linear Toeplitz policy at time t for the RL task with 6-dimensional state vector and 4-dimensional action vector. On the right: that policy in the vector-ized form. As we see, a policy defined by a matrix with 24 entries is effectively encoded by a9-dimensional vector.
Figure 2: On the left: partitioning of edges into distinct weight classes obtained for the linear policyfor HalfCheetah environment from OpenAI Gym. On the right: the same, but for a policy withone hidden layer encoded by two matrices. State and action dimensionalities are: s = 17 and a = 6respectively and hidden layer for the architecture from (b) is of size 41. Thus the size of the matricesare: 17 × 6 for the linear policy from (a) and: 17 × 41, 41 × 6 for the nonlinear one from (b).
Figure 3: The results from training both a mask m and weights θ of a neural network with twohidden layers, 41 units each. ‘Usage’ stands for number of edges used after filtering defined by themask. At the beginning, the mask is initialized such that |m| is equal to 50% of the total number ofparameters in the network.
Figure 4: Random partitioning experiments versus ENAS for Walker2d. Curves of different colors correspondto different workers. The maximal obtained rewards for random partitionings/distributions are smaller than forchromatic networks by about 1000. (a): Fixed random population of 301 partitioning for joint training. (b):Replacing the ENAS population sampler with random agent. (c): Training with ENAS.
Figure 5: Training curves for four OpenAI Gym environments: HalfCheetah, Striker, Hopper and Ant.
Figure 6: Minitaur Ablation Studies12Under review as a conference paper at ICLR 2020(a)(b)Figure 7: HalfCheetah Ablation Studies13Under review as a conference paper at ICLR 2020(b)Figure 8: Other environments (Swimmer, Reacher, Hopper, Walker, Pusher, Striker, Thrower, Ant)14Under review as a conference paper at ICLR 2020A.2 Maximum Reward CurvesIn order to view the maximum rewards achieved during the training process, for eachworker at every NAS iteration, we record the maximum reward within the interval[NAS.iteration ∙ T, (NASJteration +1) ∙ T), where T Stadns for the current number of conductedtimestpes.
Figure 7: HalfCheetah Ablation Studies13Under review as a conference paper at ICLR 2020(b)Figure 8: Other environments (Swimmer, Reacher, Hopper, Walker, Pusher, Striker, Thrower, Ant)14Under review as a conference paper at ICLR 2020A.2 Maximum Reward CurvesIn order to view the maximum rewards achieved during the training process, for eachworker at every NAS iteration, we record the maximum reward within the interval[NAS.iteration ∙ T, (NASJteration +1) ∙ T), where T Stadns for the current number of conductedtimestpes.
Figure 8: Other environments (Swimmer, Reacher, Hopper, Walker, Pusher, Striker, Thrower, Ant)14Under review as a conference paper at ICLR 2020A.2 Maximum Reward CurvesIn order to view the maximum rewards achieved during the training process, for eachworker at every NAS iteration, we record the maximum reward within the interval[NAS.iteration ∙ T, (NASJteration +1) ∙ T), where T Stadns for the current number of conductedtimestpes.
Figure 9: Maximum Reward Curves - Minitaur Ablation Studies15Under review as a conference paper at ICLR 2020(a)-4000 -ISO。。IjalfCheetah,, 2 Hidden Layers Size 41 Policy, 17 partitions, 5Q0 Timestepg(b)Figure 10: Maximum Reward Curves - HalfCheetah Ablation Studies16Under review as a conference paper at ICLR 2020-2000--1000-(b)Figure 11: Maximum Reward Curves - other environments (Swimmer, Reacher, Hopper, Walker,Pusher, Striker, Thrower, Ant)-1500-17Under review as a conference paper at ICLR 2020B	Exact Setup and HyperparametersB.1	Controller Setup
Figure 10: Maximum Reward Curves - HalfCheetah Ablation Studies16Under review as a conference paper at ICLR 2020-2000--1000-(b)Figure 11: Maximum Reward Curves - other environments (Swimmer, Reacher, Hopper, Walker,Pusher, Striker, Thrower, Ant)-1500-17Under review as a conference paper at ICLR 2020B	Exact Setup and HyperparametersB.1	Controller SetupWe set LSTM hidden layer size to be 64, with 1 hidden layer. The learning rate was 0.001, and theentropy penalty strength was 0.3. We used a moving average weight of 0.99 for the critic, and useda temperature of 1.0 for softmax, with the training algorithm as REINFORCE.
Figure 11: Maximum Reward Curves - other environments (Swimmer, Reacher, Hopper, Walker,Pusher, Striker, Thrower, Ant)-1500-17Under review as a conference paper at ICLR 2020B	Exact Setup and HyperparametersB.1	Controller SetupWe set LSTM hidden layer size to be 64, with 1 hidden layer. The learning rate was 0.001, and theentropy penalty strength was 0.3. We used a moving average weight of 0.99 for the critic, and useda temperature of 1.0 for softmax, with the training algorithm as REINFORCE.
Figure 12: The blue bars count the number of produced partitionings with the entropy within givenrange. For the comparison the entropy of the random uniform partitioning is presented as a red line.
Figure 13: Displacement Ranks of Weight Matrices induced by Partitions at the end of training. Weround an entry of the matrix to 0 if its absolute value is less than 0.1.
Figure 14: Variation of Information (VI), RandIndex, and Distance. X-axis corresponds to NASiteration number during training.
Figure 15: (a): Random partitioning used to train Walker2d. (b): Transfer of the partitioning fromHalfCheetah to Walker2d. Transfered partitionings do not underperform.
