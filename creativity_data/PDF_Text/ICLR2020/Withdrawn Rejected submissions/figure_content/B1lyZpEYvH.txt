Figure 1: The proposed MUlti-AsPect Masker (MAM) model architecture for A aspects.
Figure 2: Justifications obtained for a hotel review, with an attention model and Multi-AspectMasker, where the colors represent the aspects: Service, Cleanliness, Value, Location, and Room.
Figure 3:	Our model MAM highlights all the words corresponding to aspects. SAM only highlightsthe most crucial information, but some words are missing out, and one is ambiguous. MAA andMASA fail to identify most of the words related to the aspect Appearance, and only a few wordshave high confidence, resulting in noisy labeling. Additionally, MAA considers words belonging tothe aspect Taste whereas the Filtered Beer dataset does not include it in the aspect set.
Figure 4:	MAM finds the exact parts corresponding to the aspect Appearance and Palate whilecovering most of the aspect Smell. SAM identifies key-information without any ambiguity, but lackscoverage. MAA highlights confidently nearly all the words while having some noise for the aspectAppearance. MASA selects confidently only most predictive words.
Figure 5:	MAM can identify accurately what parts of the review describe each aspect. Due to thehigh imbalance and correlation, MAA provides very noisy labels, while MASA highlights only afew important words. We can see that SAM is confused and performs a poor selection.
Figure 6:	On a short review, MAM achieves near-perfect annotations, while SAM highlights onlytwo words where one is ambiguous with respect to four aspects. MAA mixes between the aspectAppearance and Smell. MASA identifies some words but lacks coverage.
Figure 7:	MAM emphasizes consecutive words, identifies important spans while having a smallamount of noise. SAM focuses on certain specific words and spans, but labels are ambiguous. TheMAA model highlights many words, ignores a few important key-phrases, and labels are noisy whenthe confidence is not high. MASA provides noisier tags than MAA.
Figure 8:	Our MAM model finds most of the important span of words with a small amount of noise.
Figure 9: Baseline model Emb + EnCCNN + Clf.
Figure 10:	BaSeline model Emb + EnCCNN + AShared + Clf.
Figure 11:	Baseline model Emb + EnCLSTM + AShared + Clf.
Figure 12: Baseline model Emb + EnCLSTM+ AAspeCt-Wise + Clf. Attention is either additive or sparse.
