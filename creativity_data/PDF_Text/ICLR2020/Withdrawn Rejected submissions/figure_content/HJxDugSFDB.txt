Figure 2: Diagram of our full model. Solid arrowsshow the generative model, dashed arrows showthe inference model. Rewards are not shown forclarity.
Figure 3: Example image observations for our continuous control benchmark tasks: DeepMind Control’scheetah run, walker walk, ball-in-cup catch, and finger spin, and OpenAI Gym’s half cheetah, walker, hopper,and ant (left to right). Images are rendered at a resolution of 64 × 64 pixels.
Figure 4: Experiments on the DeepMind Control Suite from images (unless otherwise labeled as "state"). SLAC(ours) converges to similar or better final performance than the other methods, while almost always achievingreward as high as the upper bound SAC baseline that learns from true state. Note that for these experiments,1000 environments steps corresponds to 1 episode.
Figure 5: Experiments on the OpenAI Gym benchmark tasks from images. SLAC (ours) converges to higherperformance than both PlaNet and SAC on all four of these tasks. The number of environments steps in eachepisode is variable, depending on the termination.
Figure 6: Comparison of differ-ent design choices for the latentvariable model.
Figure 7: Example image sequence seen for the cheetah task (first row), corresponding posterior sample(reconstruction) from our model (second row), and generated prediction from the generative model (last tworows). The second to last row is conditioned on the first frame (i.e., the posterior model is used for the firsttime step while the prior model is used for all subsequent steps), whereas the last row is not conditioned on anyground truth images. Note that all of these sampled sequences are conditioned on the same action sequence, andthat our model produces highly realistic samples, even when predicting via the generative model.
Figure 8: Diagram of our full model, reproducedfrom the main paper. Solid arrows show the gen-erative model, dashed arrows show the inferencemodel. Rewards are not shown for clarity.
Figure 9: Example image sequences, along with generated image samples, for three of the DM Control tasksthat we used in our experiments. See Figure 7 for more details and for image samples from the cheetah task.
Figure 10: Example image sequences, along with generated image samples, for the four OpenAI Gym tasksthat we used in our experiments.
