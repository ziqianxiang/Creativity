Figure 1: Left: Architecture of a ReLU network N(x) : R2 → R with two hidden layers, eachof width 5. Center: Graph of the piecewise linear function N(x) as a function of the two inputvariables. Right: Boundaries between linear regions of the network N , essentially a “flattened”version of the center image. Boundaries corresponding to neurons from the first layer are shownin blue, from the second layer in red. Green shading indicates the gradient ∂N/∂x1 along inputdimension 1; note that the gradient is constant within each region since N(x) is piecewise linear.
Figure 2: Schematic of our algorithms for identifying the first layer ofN and the additional layers.
Figure 3: Results of our first-layer algorithm, applied to networks with two or more hidden layersas the width of the first layer varies. All other layers have fixed width 10. Untrained networks haveinput and output dimension 10, those trained on the memorization task have input dimension 10 andoutput dimension 2, and those trained on MNIST have input dimension 784 and output dimension10. Left: The number of queries issued by our algorithm per parameter identified in the network N;the algorithm is terminated once the desired number of neurons have been identified. Right: Lognormalized error log(∣∣W1 - W1 ∣∣2∕∣∣W11|2) for W1 the approximated weights. Weight vectorswere scaled to unit norm to account for isomorphism (see §3.2). Curves are averaged over 5 runs inthe case of MNIST and 40 runs otherwise, with standard deviations shown.
Figure 4: Results of our algorithm for additional layers, applied to networks with two layers, thefirst layer of width 10, as the width of the second layer varies. Left: Number of estimated layer2 neurons. Right: Log normalized error between estimated and corresponding true neurons (as inFigure 3 above) for approximated weights W1 and biases b1. Curves are averaged over 4 runs, withstandard deviations between runs shown as shaded regions.
