Figure 1:	Classification accuracy does not deteriorate with model size factor. Top-1 accuraciesachieved across model sizes and datasets. (a) ResNet18s, Inception-v3s, and Inception-v3s with asingle layer varied trained in ImageNet, (b) Regularized AlexNets and ResNet56s with and withouttraining on random labels in CIFAR-10, (c) Glorot, He, and LeCun-initialized AlexNets in CIFAR-10.
Figure 2:	Some, but not all networks become more robust when model size is increased. (a)ResNet18 trained in ImageNet, (b) Glorot-initialized AlexNet in CIFAR-10, (c) He-initializedAlexNet in CIFAR-10. Each point shows the proportion of labels that did not change when thepercentage of units on the x-axis were randomly ablated. (a) and (b) show an increase in robustnesswith model width while (b) and (c) illustrate that initialization affects robustness trends.
Figure 3: Robustness and redundancy emerge in ResNet18s, robustness alone in Inception-v3s(ImageNet): (a) Areas under ablation curves, (b) principle component compressibility that retains95% of activational variance, (c) the average number of similar neurons per neuron. Each point is anaverage across layers.
Figure 4: Robustness and/or redundancy emerge in AlexNet and ResNet18 models, even whentrained on randomly labeled data (CIFAR-10): (a) Areas under ablation curves, (b) principlecomponent compressibility that retains 95% of activational variance, (c) the average number ofsimilar neurons per neuron. Each point is an average across layers.
Figure 5: Initialization influences robustness and redundancy; high variance initialization leadsto redundancy without robustness in AlexNets (CIFAR-10) (a) Areas under ablation curves, (b)principle component compressibility that retains 95% of activational variance, (c) the average numberof similar neurons per neuron. Each point is an average across layers.
Figure A1: Parameters: (a) Multilayer perceptrons, (b) AlexNets and ResNet56s, (c) ResNet18s,Inception-v3s, and Inception-v3s with a single layer varied. The log number of trainable parametersat each model size.
Figure B2: Absolute value correlation coefficients between neurons in AlexNet models: Thecolumns correspond to unregularized, regularized (data augmentation, dropout, and weight decay),and random-label-fitting AlexNets in CIFAR-10. The rows correspond to hidden layers of thenetworks. The first two of which are convolutional and the last two fully connected. Blue bars givemeans. Each plot shows the distribution for the 1/4x, 1x, and 4x model sizes.
Figure C3: Robustness and redundancy are invariant in ResNet18s under different amounts oftraining past convergence (ImageNet). Trends in (a) accuracy, (b) robustness, (c) compressibility,and (d) similarity across training epochs after convergence in ResNet18.
Figure C4: AlexNet robustness and redundancy trends with uniform initializations resemblethose with normal initializations (CIFAR-10). Trends in (a) accuracy, (b) robustness, (c) compress-ibility, and (d) similarity with He, LeCun, and Glorot uniform initializations across size factors forAlexNets.
Figure C5: The emergence of redundancy is sensitive to initialization variance in MLPs trainedon 10,000 dimensional data. Trends in (a) accuracy, (b) robustness, (c) compressibility, and (d)similarity with multiple initialization variances across size factors for MLPs trained on 10,000dimensional synthetic uncorrelated data. The legend gives σ for the normal weight initialization.
Figure C6: The emergence of redundancy is not sensitive to initialization variance in MLPstrained on 10 dimensional data. Trends in (a) accuracy, (b) robustness, (c) compressibility, and(d) similarity with multiple initialization variances across size factors for MLPs trained on 10,000dimensional synthetic uncorrelated data. The legend gives σ for the normal weight initialization.
Figure C7: ResNet56 robustness and redundancy layerwise (CIFAR-10): Trends in (a) robustness,(b) compressibility, and (c) similarity among layers/blocks within ResNet56 in CIFAR-10. Eachlayer/block has a unique curve, with the initial convolutional and final block layers exhibiting themost robustness and redundancy.
Figure C8: ResNet18 robustness and redundancy layerwise (ImageNet): Trends in (a) robustness,(b) compressibility, and (c) similarity among layers/blocks within ResNet18 in ImageNet. The finalblock layer is the most robust while the initial convolutional layer is the most compressible andsimilar. This might be a sign of the phenomenon we describe as “weight balancing” or the formationof silent or constant units in the block layers, particularly the final one.
Figure C9: Inception-v3 robustness and redundancy layerwise (ImageNet): Trends in (a) robust-ness, (b) compressibility, and (c) similarity among the final 35 × 35, 17 × 17, and 8 × 8 blocks withinInception-v3 in ImageNet. Layers develop robustness and compressibility in order of their depth inthe network, while the final 17 × 17 layer is the most similar.
