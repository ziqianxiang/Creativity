Figure 1: Visual captioning models are often not visually-grounded. As human, we perform localiza-tion to check whether the generated caption is visually-grounded. If the localized image region isincorrect, we update the model. However, without the ground-truth grounding annotation, how doesthe model know the localized region is incorrect? To overcome this issue, we propose to performlocalization and reconstruction to regularize the captioning model to be visually-grounded withoutrelying on the grounding annotations.
Figure 2: Proposed cyclical training regimen: decoding -→ localization -→ reconstruction. Thedecoder attends to the image regions and sequentially generate each of the output words. Thelocalizer then uses the generated words as input to locate the image regions. Finally, the shareddecoder during reconstruction stage uses the localized image regions to regenerate a sentence thatmatches with the ground-truth sentence.
Figure 3: Proposed model architecture (left) and how the model operates during decoding, localization,and reconstruction stages (right). During the decoding stage, the soft-attention module uses thehidden state of the Attention LSTM to compute attention weights on image regions. During thelocalization and reconstruction stage, the soft-attention module instead uses the generated word fromdecoding stage to compute attention weights on image regions.
Figure 4: Average F1all -score per class as afunction of class frequency.
Figure 5:	Generated captions and corresponding visual grounding regions with comparison betweenbaseline (left) and proposed approach (right). Our proposed method is able to generate moredescriptive sentences while selecting the correct regions for generating the corresponding words.
Figure 6:	Illustration of Grounding metrics.
Figure 7:	Demonstration of our human evaluation study on grounding. Each human subject isrequired to rate which method (A or B) has a better grounding on each highlighted word.
Figure 8:	Correct (top) examples and examples with errors (bottom) from the proposed method.
Figure 9: A group of men in white uniforms are standing in a field with a crowd watching. We can seethat our proposed method attends to the sensible image regions for generating visually-groundablewords, e.g., man, uniforms, field, and crowd. Interestingly, when generating standing, the model paysits attention on the image region with a foot on the ground.
Figure 10: A young girl wearing a winter hat and a purple coat is smiling at the camera. Theproposed method is able to select the corresponding image regions to generate girl, hat, and coatcorrectly. We have also observed that the model tends to localize the person’s face when generatingcamera.
Figure 11: A white horse with a rider in a blue helmet and white shirt jumping over a hurdle.
Figure 12: A man in a red shirt is standing on a wooden platform. Our method correctly attends onthe correct regions for generating man, shirt, and platform.
Figure 13: A man in a yellow jacket and blue helmet riding a bike. The proposed method correctlygenerates a descriptive sentence while precisely attending to the image regions for each visually-groundable words: man, jacket, helmet, and bike.
Figure 14: A man in an orange shirt and a hat is standing next to a blue wall. While our method isable to ground the generated sentence on the objects like: man, shirt, hat, and wall , it completelyignores the person standing next to the man in the orange cloth.
Figure 15: A girl in a white shirt and black pants is jumping on a red couch. Our method is able toground the generated descriptive sentence with the correct grounding on: girl, shirt, pants, and couch.
Figure 16: A man in a blue robe walks down a cobblestone street. Our method grounds the visually-relevant words like: man, robe, and street. We can also see that it is able to locate the foot on groundfor walks.
