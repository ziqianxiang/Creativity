Figure 1: The PARCUS architecture is applied to the i-th example of a dataset, i.e., “Alice is marriedto Bob”, with “married” being highlighted. After an initial phase where rationales ri are extractedand words are mapped to embeddings xi , we extract features by computing the similarity betweenthe token’s embedding and the parameters of our model. Then, we combine these features with alinear layer which outputs per-token predictions. At training time only, predictions are multipliedby a boosting factor f (rij). Results are then summed to yield the sentence prediction yi.
Figure 2: Top-10 most relevant tokens for positive prediction, averaged on unseen data.
Figure 3: Plot of the gating function g(v) = av-1 for different values ofa. In practice, the argumentv will be the cosine similarity between 2 vectors, hence g : [-1, 1] → [0, 1](a) Cosine similarityFigure 4: From left to right: (a) Cosine similarity with respect to the [1,1] vector v and (b) its gatedactivation for a = 100. The gate function acts as a filter on cosine similarity, thus promoting thosevectors “close enough” to v while penalizing the others.
Figure 4: From left to right: (a) Cosine similarity with respect to the [1,1] vector v and (b) its gatedactivation for a = 100. The gate function acts as a filter on cosine similarity, thus promoting thosevectors “close enough” to v while penalizing the others.
Figure 5: From left to right: Cosine similarity with respect to the [1,1] vector and its gated activation.
Figure 6: Robustness to different degrees of rationale noise on the Spouse dataset. From bottom totop, the curves refer to different training sizes: 10, 30, 60, 150, 300. We fix f(T) = erWe investigated how injecting random rationales (i.e., ones) in prior information affects the finalperformances. Clearly, having a 100% noise corresponds to not having rationales at all, thereforewe run a simple experiment (with fixed hyper-parameters) 10 times on Spouse, with the goal tostudy robustness to noise. Figure 6 shows the result, where different curves stand for differenttraining sizes; as one would expect, for few data points (blue and red curves) noise has biggerinfluence whereas for 150 (orange curve) and 300 (green curve) data points the effect is negligible.
Figure 7: Visualization of the most important tokens for positive prediction. Sentences were ex-tracted from the Spouse dataset. The entities we are interested in are always called “alex” and“chris”.
