Figure 1: Flowcharts for AlphaGo Master, AlphaGo Zero and AlphaZero; each of them is a simpli-fication of its predecessor; that is, AlphaGo Zero removes the need of human data in training, whilein AlphaZero gating is further discarded (gating refers to playing tournament using existing neuralmodels in PV-MCTS thus a model parameter will passed down to selfplay workers only when itswin-percentage exceeds a certain threshold, e.g., 55%). Notice that AlphaGo Zero and AlphaZeroused asynchronous training, i.e., selfplay game generation and neural network training were con-ducted simultaneously given a sufficient number of games have been stored in the buffer. Detaileddifferences on search and learning, such as hyperparameters, are omitted in these diagrams.
Figure 2: AlphaZero-2HNN versus AlphaZero-3HNN learning progresses measured in trainingtime. MCTS-3HNN performs 800 iteration MCTS faster than MCTS-2HNN because it uses thedefault expand threshold of 10, while MCTS-2HNN uses 0. Consequently, 3HNN leads to fasterAlphaZero learning than that of 2HNN. For test set T∈, because final move is random, we only testthe value error on these examples (rightmost).
Figure 3: AlphaZero-2HNN versus AlphaZero-3HNN. The only difference is that MCTS-3HNNused nmcts = 800 with default expand threshold of 10, while MCTS-2HNN is with nmcts = 160with expand threshold of 0. AlphaZero-3HNN and AlphaZero-2HNN respectively took 90 and 97hours to complete 80 iterations.
Figure 4: AlphaZero-2HNN versus AlphaZero-3HNN. R2 in 3HNN is removed.
Figure 5: Match results of 3HNN vs 2HNN. MCTS-3HNN-1s-e10 means MCTS-3HNN using 1sper move with default expand threshold of 10. MCTS-2HNN-1s-e0 means using 2HNN with 1sper move with expand threshold of 0. MCTS-2HNN/3HNN-160 means using 2HNN/3HNN with160 simulations and expand threshold of 0. Taking the last 20 iteration results, we report the meanwin-rates for MCTS-3HNN-1s-e10, MCTS-3HNN-160 (left figure), and MCTS-3HNN-160 (right)are respectively 52.73 ± 1.04, 51.25 ± 1.05 and 56.54 ± 0.83, with 95% confidence. For MCTS, asin training, we use cpuct = 1.5 but set η = 0.
Figure 6: The left subfigure is obtained by inferring the 3HNN model from iteration 80 as in Fig-ure 4. The right subfigure is the ground truth of each cell (black means first player win, assumingblack plays first). For each cell, the upper number is the prior probability from policy head, thelower number is the action estimate from action-value head. Note that the action-value is with re-spect to the player to play after taking that action. The policy head provided non-zero probabilitiesat only three strongest central openings, while the action-value head provides a set of estimates thatare mostly consistent with the ground truth of each opening move.
