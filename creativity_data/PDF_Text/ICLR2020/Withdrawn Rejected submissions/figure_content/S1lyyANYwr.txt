Figure 1: (a) Example of a gridworld environment. (b,c) Performance over the training for Unconstrained (red),Lyapunov-based (green), and our method (blue) all trained with n-step SARSA on 2D GridWorld task over20 random seeds. The x-axis is the number of episodes in thousands. The dotted black line in (c) denotes theconstraint threshold, d0 . The bold line represents mean, and the shaded region denotes 80% confidence-intervals.
Figure 2:	A2C Performance over the training for Unconstrained (red), Lyapunov-based (green), and our method(blue) all trained with A2C on MuJoCo tasks over 10 random seeds. The x-axis is the number of episodes inthousands. The dotted black line denotes d0 . The bold line represents the mean, and the shaded region denotesthe 80% confidence-intervals.
Figure 3:	PPO Performance over the training for Unconstrained (red), Lyapunov-based (green), and our method(blue) all trained with PPO on MuJoCo tasks over 10 random seeds. The x-axis is the number of episodes inthousands, and y-axis denotes the undiscounted accumulated returns. The dotted black line denotes d0 . The boldline represents the mean, and the shaded region denotes the 80% confidence-intervals.
Figure 4: MuJoCo Safety EnvironmentsI Details of the MuJoCo ExperimentsI.1	Environments Descriptionâ€¢	Point-Gather: The environment (Fig.4c) is taken from Achiam et al. (2017), where thepoint mass agent gets a reward of +10.0 for collecting a green apple, and a cost of 1 forcollecting a red bomb. Two apples and eight bombs are spawned randomly at the startof each episode. The constraints are defined over the nmber of bombs collected over theepisode. Episode horizon is 15 and threshold d0 = 4.
