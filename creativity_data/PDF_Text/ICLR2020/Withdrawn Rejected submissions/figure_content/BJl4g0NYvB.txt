Figure 1: Overview of Causal Induction and Inference Procedure. During training each episodesamples one of K training environments and uses the interaction policy πI to probe the environmentand collect a trajectory of visual observations. Using supervised learning we train the causal inductionmodel F, which takes as input the trajectory of observational data and constructs C, the estimate ofCtrain, WhiCh captures the underlying causal structure. Then, the predicted structure C is providedas input to the policy πG conditioned on goal g, which learns to use the causal model to efficientlycomplete a specified goal in the training environments. At test time, F and πG are fixed and the agentis evaluated on new environments with unseen causal structures.
Figure 2: Iterative Causal Induction Network. Our iterative network architecture for inducing thecausal structure from a visual trajectory of observational data with horizon H. First each frame isencoded into a latent state embedding s. Then the difference between state embeddings across timesteps (state residual) is computed, and concatenated with the corresponding action. This is fed intothe Edge Decoder module, which predicts an edge update, as well as an attention vector which isused to weight how the edge update is applied to nodes. This results in an estimate of the causalgraph Ci at the ith iteration. On the last step one more edge update based on the current graph isapplied, and a final predicted graph C is outputted.
Figure 3: Types of Causal Structures (Left) We explore four types of causal structures, ONE-TO-One, One-to-Many, Many-to-One, and Masterswitch. These cover a standard causalmapping, common cause causal patterns, common effect causal patterns, and causal chains. Goal-Conditioned Policy (Right). The policy takes as input the current image, goal image, and predictedcausal graph C. The current image and goal image are concatenated channel wise and encoded. Thisencoding is used to predict an attention vector over the “effects” in C which extract the relevantedges, which is then concatenated with the image encoding to predict the action.
Figure 4: F1 Score on Unseen Causal Structures. The F1 Scores of edges on the predicted causalstructure compared to the ground-truth on unseen causal structures. We compare across variablenumbers of seen structures {10, 50, 100, 500} and problem size {5, 7}. Our iterative approach withattention outperforms the comparisons across almost all settings.
Figure 5: Policy Success Rates (Unseen Causal Structures). The final success rates of the goal-conditioned policy for each method on unseen causal structures for either 10, 50, 100, or 500 seencausal structures for 5 or 7 switches. Our iterative approach achieves the best generalization onunseen tasks across almost all settings.
Figure 6: Sample of Causal Induction. Here we show an example of our Iterative Causal InductionModel for 5 switches, in the “One-to-Many” case. Given the trajectory of actions and images ofthe scene, the model needs to reason about which lights were turned on, and how what update thisimplies in the graph. In this example, the first observed action turns on one of the switches, and themodel makes the corresponding update to the graph. The next switch does not change the lightingso the model outputs no update to the graph. The next action sees one light go on, and updates thecorresponding switch. The next action turns on two lights, and the graph is updated to reflect this.
