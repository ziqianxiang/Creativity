Figure 1: A unifying formulation of different learning algorithms. Each algorithm is a specialinstance of the generalized ERPO formalism (Eq.1), by using different rewards and taking differentvalues of the weight hyperparameters α, β.
Figure 2: Exploration space exposed to model learning in different algorithms. (a): The effectiveexploration space of MLE is exactly the set of training examples. (b): Data Noising and RAML usediffused rewards and allow larger exploration space surrounding the training examples. (c): Standardpolicy optimization such as SPG (section 3.1) basically allows the whole exploration space.
Figure 3: Performance of learned policies. The x-axis is the number of expert demonstrations fortraining. The y-axis is the average returns. “BC” is Behavior Cloning. “Random” is a baselinetaking a random action each time. Results are averaged over 50 runs.
Figure 4: Learning curves of GAIL experiments. The x-axis is the number of training iterations.
