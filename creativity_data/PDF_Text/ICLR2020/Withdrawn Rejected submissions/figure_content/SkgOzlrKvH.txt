Figure 1: Illustrative example in 2D. The 1Drepresentation space is illustrated as a dottedline, and arrows indicate the embedding from2D to 1D. (a) Optimal embedding when G isthe class of linear functions. (b) optimal em-bedding with a complex nonlinear functionclass: zero source error and divergence loss,but the embedding destroys label consistencyand leads to maximal target risk.
Figure 2:	Empirical ver-ification on Amazon reviewsdataset. (a) Vary the numberof layers in the encoder whilefixing the predictor. (b) Fixthe total number of layers andoptimize the domain-invariantloss in different layers.
Figure 3: Amazon reviews dataset. First row: Fixed predictor class, varying number of layers in theencoder. Second row: Fixed total number of layers and optimizing domain-invariant loss in a singleintermediate layer or MDM.
Figure 4: Digit classification. (a) Fixed predictor class, varying number of layers in the encoder.
Figure 5: DANN with FC layers.
Figure 6: Office-31 Dataset. First row: Fixed predictor class, varying encoder depth. Second row:Fixed total number of layers, optimizing domain-invariant loss in a single layer or MDM.
Figure 7: Predictor complexity trade-off on MNISTâ†’MNIST-M. (a) Fix the encoder class andvary the number of layers in the predictor. (b) Fix the encoder class and vary the hidden width ofthe predictor.
