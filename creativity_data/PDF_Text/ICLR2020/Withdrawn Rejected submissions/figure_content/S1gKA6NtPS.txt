Figure 1: A. Sampling an expression from the RNN. Nodes are selected one at a time in autoregressivefashion along the pre-order traversal of the corresponding expression tree. For each token, the RNN outputsa categorical distribution over tokens, a token is sampled, and the parent and sibling of the next token areused as the next input to the RNN. In this example, the sampled expression is sin(cx)/ log(y), where thevalue of the constant c is optimized with respect to an input dataset. Numbers indicate the order in whichtokens were sampled. Colors correspond to the arity of the token. White circles represent empty tokens. B.
Figure 2: Average recovery (top) and NRMSE (bottom) for various ablations across the 12 Nguyen bench-marks. Dotted lines correspond to DSR (no ablations) and GP baselines. Error bars represented standarderror (n = 10). Additional descriptions for each ablation experiment are provided in Appendix A.
Figure 3: Average recovery (left) and NRMSE (right) for various noise levels across the 12 Nguyen bench-marks. Solid lines represent 20 data points per benchmark (default); dashed lines represent 200 data pointsper benchmark (10-fold increase). Error bars represent standard error (n = 10).
Figure 4: Reward training curves for DSR and GP for each of the 16 benchmark expressions. Each curveshows the best reward (1/(1 + NRMSE)) found so far as a function of training step, averaged across alln = 100 independent training runs. A value of 1.0 denotes that all training runs have recovered the correctexpression by that training step. Error bands represent standard deviation (n = 100).
Figure 5: Recovery rate training curves for DSR and GP for each of the 16 benchmark expressions. Eachcurve shows the fraction of independent training runs that correctly recovered the benchmark expressionas a function of training step. A value of 100% denotes that all training runs have recovered the correctexpression by that training step.
