Figure 1: Architecture of the layer-wisegenerator of the dynamic model. Inde-pendent noise samples {z1,…，zN} aredrawn from a standard Gaussian with di-agonal covariance, and input to layer-wise generators {Gι, ∙∙∙ , GN}. Eachgenerator Gj outputs parameters θj forthe corresponding j -th layer of the neu-ral network representing the dynamicmodel.
Figure 2: NChain task.
Figure 3: Results on the 40-link chain environment. Each line is the mean of three runs, with theshaded regions corresponding to ±1 standard deviation. Both our method and MAX actively reduceuncertainty in the chain, and therefore are able to quickly explore to the end of the chain. -greedyDDQN fails to explore more than 40% of the chain.
Figure 4: (a) The Acrobat system. (b) Performance of each method on the Acrobot environment(average of five seeds), with error bars representing ± one standard deviation. The length of eachhorizontal bar indicates the number of environment steps each agent/method takes to swing theacrobot to fully horizontal on both (left and right) directions.
Figure 5: (a) The Ant Maze environment. (b) Performance of each method with mean and ±1standard deviation (shaded region) over five seeds. x-axis is the number of steps the ant has moved,y-axis is the percentage of the U-shaped maze that has been explored. Our method (red) is able tonavigate to the end of the maze much faster than any other method. Figures (c-f) show the behaviorof the agent at different stages of training. Points are color-coded with blue points occurring at thebeginning of the episode, and red points at the end.
Figure 6: (a) The Robotic Hand task in motion. (b) Performance of each method with mean and ±1standard deviation (shaded region) over five seeds. x-axis is the number of manipulation steps, y-axis is the number of rotation states of the block that has been explored. Our method (red) exploresclearly faster than all other methods.
Figure 7: Results on the 40-link chain environment. Each line is the mean of three runs, with theshaded regions corresponding to ±1 standard deviation. Both our method and MAX actively reduceuncertainty in the chain, and therefore are able to quickly explore to the end of the chain. -greedyDDQN fails to explore more than 40% of the chain. Both ICM and Disagreement perform better butexplore less efficiently.
Figure 8: Performance of each exploration method on the Acrobot environment.
Figure 9: Performance of each exploration method on the Ant Maze environment.
Figure 10: Performance of each exploration method on the Block Manipulation environment.
