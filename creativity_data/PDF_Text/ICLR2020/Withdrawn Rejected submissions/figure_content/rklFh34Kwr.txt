Figure 1: Residual blocks in respectively the ResNet and ResNet++ architectures.
Figure 2: Calibration plot for Cifar10 Figure 3: Calibration plot for ImageNet(Ioffe & Szegedy, 2015). These methods implicitly add noise into the model parameters(Kingma et al., 2015; Teye et al., 2018) and significantly boost training performance andgeneralization for image classifiers. These methods can be interpreted as a coarse approx-imation of Bayesian Inference (Kingma et al., 2015; Teye et al., 2018). But a stochasticgradient sampler like ATMC already adds the necessary amount of noise and combinedwith BatchNorm or Dropout it leads to underfitting. We thus define a BatchNorm freeversion of ResNet called ResNet++ that includes SELUs (Klambauer et al., 2017), Fixupinitialization (Zhang et al., 2019a) and weight normalization (Salimans & Kingma, 2016)(see Fig. 1). We use ATMC to fill the significant gap in performance due to the absence ofBatchNorm in ResNet++.
