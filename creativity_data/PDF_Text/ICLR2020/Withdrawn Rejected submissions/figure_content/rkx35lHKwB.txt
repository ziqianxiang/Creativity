Figure 1: Generalizing the knowledge of solving a task to a new set of actions. (a) CREATE is asequential environment where the task is to help the green ball reach the goal (blue) by selecting toolsand deciding where to place them. (b) In Shape Stacking the goal is to stack a tall tower by selectingthe right shapes and their placements. Scenario A depicts the training scenario when the agent learnsto utilize a given set of actions to solve the task. Scenario B presents an unseen set of actions to theagent which is expected to generalize to solve the task zero-shot.
Figure 2: Framework for generalization to unseen actions. (1) Action datasets for all training actionsare used to train a Hierarchical VAE (HVAE) model. (2) The action encoder embeds each dataset todefine the approximate posterior qa(c|D) over action latents c. (3) The instance encoder qs(z|x, c)encodes each data sample x, while conditioned on the action latent c, into a distribution over instancelatents z. (4) The decoder p(x|z, c) reconstructs the action sample x based on the action embeddingc and sample latent z . (5) The policy Ï€ takes current state st and the inferred action embeddings cifor each of the given actions and produces a categorical distribution to represent the policy. Similarflow occurs at inference, when new actions and their datasets are given.
Figure 3: Quantitative results: displayed are 3 of the CREATE tasks, the Block Stacking task, theRecommender task and the Grid World task. The performance displayed is measured on generalizationto the test set of actions across 3,200 episodes. All results are averaged across 6 seeds. The legenddescribes ablations of our method (shades of red), embedding baselines (shades of blue), policyarchitecture baselines (shades of green), and alternate modalities in learning embeddings (yellow).
Figure 4: Varying difficulty of test action space: (i) Each test action is at least a specific angle apartfrom all actions seen during training (ii) Each test action is at least a specific distance in embeddingspace apart from all actions seen during training (iii) Test set contains seen/unseen ratioto generalize in the right most column. In both cases the policy chooses the right types of actions andbarely misses the objective.
Figure 5: Qualitative analysis: shown are two success cases and one failure case for CREATE andObject Stacking. In CREATE the trace of the ball trajectory is outlined. All of the tools or objects inthese results the policy is generalizing to select and was not trained over these actions.
Figure 6:	T-SNE Visualization of learned embedding space for CREATE environment. Tools inCREATE are labeled by their properties which define them to be floor, trampolines, high-frictional,etc. The action embeddings clearly group similar actions together.
Figure 7:	T-SNE Visualization of learned embedding space for Grid World environment.
Figure 8:	T-SNE Visualization of learned embedding space for Shape-stacking environment.
Figure 9: Success rate curves comparing the learning of the primary method Ours versus ablations.
Figure 10: Fine-tuning the policy to adapt to a new action space by re-initializing the final layer.
Figure 11: Grid world environment. The agent is the red triangle and the goal is the green cell.
Figure 12: CREATE environment. In CREATE, the green ball is falling into the scene, which mustpush the red target ball into the blue goal location. The top and bottom rows show actual evaluationresults when our model is tested on CREATE UP and CREATE Down, respectively.
