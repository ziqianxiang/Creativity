Figure 1: Global-local model architecture. An image pair and an estimated flow field are first fed through theglobal module that estimates the “global parameters” vector g, representing the camera motion. From theseglobal parameters, the local module generates three convolutional filter banks and applies them to the opticalflow field. The output of the local module is then processed by a convolution to generate the final depth estimate.
Figure 2: Qualitative comparison of the depth maps generated with the baselines and our approach. Overall,Struc2Depth’s predictions are generally poor in homogeneous and repetitive regions, while DispNet tends toover-smooth depth maps. In contrast, our method can predict fine details of the scene geometry.
Figure 3: For large networks, the losson training points (solid lines) is signif-icantly higher than the validation loss(dashed lines). In contrast, our global-local architecture learns generaliziblerepresentations.
Figure 4: Depth estimation errors with increasing number of training pixels per image and dense supervision(D). When supervision gets sparser, our method’s performance degrades more gracefully than the baseline.
Figure 5: Relation between per pixel flow and depth error for our method and triangulation, either with perfectpose (GT Pose) or with the pose provided by our fine-tuned global network (see Sec. 4.4). Our approach learnsto filter out errors in correspondences by exploiting its receptive field larger than one and regularities of thoseerrors in the data.
Figure 6: Qualitative results on the simulated Scenes11 dataset. Given the availability of noise-free depth mapsfor training and high quality optical flow, our approach can learn extremely sharp depth maps, comparable to theones learned by an encoder-decoder with dense supervision.
Figure 8: Qualitative results on RGB-D. Also this dataset represents a challenge for all methods, given the largebaseline between views, noisy correspondences and noisy training depth maps. Nonetheless, our approach isstill able to estimate sharp depth maps, sometimes capturing fine details which even an encoder-decoder trainedwith dense supervision fails to catch (bottom-row).
Figure 9: Local network filters generated by several image pairs. Generally, filters are different for each imagepair. However, when the relative transformation between the two views is similar, filters also tend to be similar(first and second row). In contrast, when the relative transformation is completely different, filters tend to have adissimilar pattern (first and third row).
