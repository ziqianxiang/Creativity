Figure 1: Left mosaic: the left halves of the MNIST digits in a random sample from the test set areinferred from the right half, with cutoff k0 = 200. Three images are shown for each digit. Withineach triplet, the middle image represents the mean over pixel intensity of the inferred conditiondistribution, while left and right images corresponds to a plus and minus one standard deviationfrom the mean in the direction of largest covariance (in the space of half-images). A particularlyinteresting example is highlighted. Top-right: loss per epoch for k0 = 200. Bottom-right: thesingular values for different values of the cutoff k0, after 150 epochs in each case.
Figure 2: Loss and inaccuracy (error rate) on test sets for two classification tasks. The models weretrained either using the cross-entropy (CE) or our approach (DCCI), with or without regularizationlayers. On the MNIST dataset, we used an “all CNN” network, and for the CIFAR10 dataset we useda short VGG variation with 10 convolutions and 3 fully connected layers. In the regularized form,post-activation Batchnorm layers were placed after each convolutional layers on the VGG network.
Figure 3: Top-left: First 20 relevant variables (on X) determined by DCCA for a system where Xconsists of two coordinates uniformly sampled over a circle and a surrounding ring, and Y consistsof the same points but shifted by a small normally distributed vector. The variables are arrangedfrom left-to-right and top-to-bottom in order of decreasing relevance. Top-right: the same variablesmultiplied by the marginal pX . Bottom row: introducing a gap in the ring allows for a monotonousfunction of the angle to serve as second most relevant variable (instead of the sine/cosine couple).
Figure 4: Left: Best mean squared error for images reconstructed from the k most relevant variables,as a function of k (the latent dimension). This logarithmic plot shows that improvements stop oncethe dimension reaches 19 (where the two lines cross). Right: images produced by the generator fromlatent variables sampled according to the best Gaussian fit in latent space, for feature subspaces ofdimensions 2, 8 and 19.
