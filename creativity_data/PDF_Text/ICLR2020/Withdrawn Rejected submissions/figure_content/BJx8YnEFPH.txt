Figure 1: Block diagram of the DVRL framework for training. A batch of training samples is used asthe input to the data value estimator (with shared parameters across the batch) and the output corre-sponds to selection probabilities: Wi = hφ(xi, y%) of a multinomial distribution. The sampler, basedon this multinomial distribution, returns the selection vector S = (si,…，SBs) where Si ∈ {0,1} andP(Si = 1)= Wi. The target task predictor model is trained only using the samples with selectionvector Si = 1, using conventional gradient-descent optimization. The selection probabilities Wi rankthe samples according to their importance - these are used as data values. The loss of the predic-tor model is evaluated on a small validation set, which is compared to the moving average of theprevious losses (δ) to determine the reward. Finally, the reinforcement signal guided by this rewardupdates the data value estimator. Block diagrams for inference are shown in Appendix A.
Figure 2: Performance after removing the most (marked with ×) and least (marked with ) impor-tant samples according to the estimated data values in a conventional supervised learning setting.
Figure 3: Prediction performance after removing the most (marked with ×) and least (marked with) important samples according to the estimated data values with 20% noisy label ratio. Additionalresults on Blog, HAM 10000, and CIFAR-10 datasets can be found in Appendix C.3. The predictionperformance is lower than state of the art due to a smaller training set size and the introduced noise.
Figure 4: Discovering corrupted samples in three datasets with 20% noisy label ratio. ‘Optimal’saturates at 20%, perfectly assigning the lowest data value scores to the samples with noisy labels.
Figure 5: Number of validation samples needed for DVRL. Discovering corrupted samples in threedatasets (Adult, Blog and Fashion MNIST) with various number of validation samples. X-axisrepresents the fraction of inspected data and y-axis is the fraction of discovered corrupted samples.
Figure 6: Block diagram of the proposed DVRL framework at inference time. (a) Data valuation,(b) Prediction. For data valuation, the input is a set of samples and the outputs are the correspondingdata values. For prediction, the input is a sample and the output is the corresponding prediction.
Figure 7: Prediction performance after removing the most and least important samples, accordingto the estimated data values. We assume a label noise with 20% ratio on (a) Blog, (b) HAM 10000,(c) CIFAR-10 datasets.
Figure 8: Discovering corrupted samples in three datasets ((a) Adult, (b) Fashion-MNIST, (c) Flowerdatasets) in the presence of 20% noisy labels. ‘Optimal’ saturates at the 20 % of the fraction,perfectly assigning the lowest data value scores to the samples with noisy labels. ‘Random’ does notintroduce any knowledge on distinguishing clean vs. noisy labels, and thus the fraction of discoveredcorrupt samples is proportional to the amount of inspection.
Figure 9: Learning curves of DVRL for 6 datasets with 20% noisy labels. x-axis: the numberof iterations for data value estimator training, y-axis: validation performance (log loss). (Orange:validation log loss without DVRL, Blue: validation log loss with DVRL)E Confidence intervals of DVRL performance on corruptedSAMPLE DISCOVERY EXPERIMENTSAdult and Blog DatasetFigure 10: Corrupted sample discovery performance with 95% confidence intervals (computed by10 independent runs) according to the estimated data values by DVRL. We assume a label noise with20% ratio on (a) Adult and Blog, (b) Fashion-MNIST and Flower (c) HAM 10000 and CIFAR-10datasets.
Figure 10: Corrupted sample discovery performance with 95% confidence intervals (computed by10 independent runs) according to the estimated data values by DVRL. We assume a label noise with20% ratio on (a) Adult and Blog, (b) Fashion-MNIST and Flower (c) HAM 10000 and CIFAR-10datasets.
Figure 11: t-SNE analyses on the final layer representations of each store type in Rossmann dataset.
Figure 12: Histograms of the training samples from the target store type in Train on All setting basedon the sorted data values estimated by DVRL. (x-axis: the sorted data values (in percentiles), y-axis:counts of training samples from the target store type (in ratio).
