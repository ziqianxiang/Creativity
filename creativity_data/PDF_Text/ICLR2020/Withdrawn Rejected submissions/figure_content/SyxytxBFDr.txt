Figure 1: Training performance our’s and OpenAI Baseline’s implementations of Proximal PolicyOptimization (PPO) for 1 million timesteps with Swimmer, Hopper, and Humanoid. The top isplotted against simulator time, while the bottom is bottom is against wall clock time.
Figure 2: Two instantiations of the procedurally generated variable terrain environments using ATLASand Cassie (top) as well as shelf and tabletop versions of the Reconfiguration Planning environmentutilizing HERB as the robot.
Figure 3: Sampling throughput for a batch of 1024 samples collected in parallel for four environmentsof various complexity as well as throughput scaling with increasing cores. Top left: samplingthroughput relative to native C. Bottom left: sampling throughput on a log scale. Right: scaling withincreasing cores.
