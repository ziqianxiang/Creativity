Figure 1: Distribution of nonzero accumulator values Gt[i] = PT=1 gτ[i]2 in the embedding layerof the Transformer from Section 4.1, after t = 116200 training iterations. Left: Histogram ofaccumulator values, with counts on a linear scale. Right: Same plot as left, except with logarithmicvertical scale, showing prevalence of extremely small values.
Figure 2: The effective learning rate multiplier of Adam as function of steps for the commonly founddefault hyperparameters β1 = 0.9, β2 = 0.999 in deep learning libraries.
Figure 3: Training loss (i.e. log-perplexity) curves for several optimization methods on thetransformer-big model on the WMT14 en→fr dataset.
Figure 4: Convergence of AdaGrad with momentum in the translation setting of Section 4.1, varyingonly the initial accumulator value . Note that using the default value = 10-1 in the TensorFlowAdaGrad implementation might cause dramatic misconceptions about poor convergence.
Figure 5: Training curves for top-1 training and test accuracy of a ResNet-50 on ImageNet, trainedwith several optimization methods. Without the cyan curve, the plots suggest poor generalization ofadaptive methods, despite their head start in optimization. However, the gap is closed upon a carefultuning of AdaGrad with momentum.
Figure 6: Attempted replication of the results of Wilson et al. (2017). Top: Image classificationwith a CNN. Results match perfectly, with non-adaptive methods generalizing the best. Middle:Character-level language modeling with a 2-layer LSTM. The original reported hyperparametersare the best and all optimizers converge to reasonable solutions, but contradictory conclusions aboutgeneralization arise. Bottom: 3-layer LSTM for generative parsing. Training does not converge withall reported learning rates; conclusions about generalization are unclear.
