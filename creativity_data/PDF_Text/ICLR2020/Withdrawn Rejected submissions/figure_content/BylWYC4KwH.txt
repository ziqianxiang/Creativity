Figure 1: The overview of our concept discovering algorithm. Given a deep classification model, wefirst provide semantically meaningful clusters by segmentation followed by k-means clustering as inGhorbani et al. (2019). Then, we discover complete and interpretable concepts under the constraintthat each concept is salient to one (or a few) unique cluster, while projecting features onto the span ofconcept vectors does not deteriorate the classification performance. After the concepts of interest areretrieved, we can calculate the importance of each concept and the classes where each concept is themost important by ConceptSHAP.
Figure 2: Two random training images and the respecting ground truth concepts that are positivealong with a table that matches ground truth concepts to shape. Each object shape in the imagecorresponds to a ground truth concept (with random color and location), and the ground truth labeldepends solely on ground truth concept 1 to 5. Only the training image and ground truth label areprovided during training (in the unsupervised case), and the goal of the discovering concept algorithmis to correctly retrieve ground truth concepts ξ1 to ξ5 .
Figure 3: Visualization Result for the nearest neighbors of each discovered concepts in ours-supervised and TCAV along with ground truth concept 1 to 5 that is constructed to be the minimumset of ground truth variable. We note that only the shape is revelent of the concept, as the colorand location can be random. We show that each of our discovered concepts in ours-supervisedcorresponds to one of ground truth concept 1 to 5 (with a random order). While TCAV also showsmeaningful discovered concepts, they fail to retrieve all ground truth concepts that are used by themodel. Higher resolution examples will be shown in the appendix due to space constraint.
Figure 4: The Nearest Neighbors, ConceptSHAP, and related class for each concept obtained in AwA.
Figure 5: Additional Nearest Neighbors for each concept obtained in IMDB.
Figure 7: The alignment score and completeness score for our method With different hyper-parameterfor the toy dataset.
Figure 8: The alignment score and completeness score for all methods with different number ofdiscovered concepts are chosen for the toy dataset.
Figure 9: Completeness score for different number of concepts in AwA.
Figure 10: Completeness score for different standard deviation of added noise in AwA.
Figure 11: Nearest Neighbors for each concept obtained in AwA without the completeness metric.
