Figure 1: Results for non-contextual static dataset optimization on MNIST: (a) and (b): Stroke width optimization,and (c): Maximization of disconnected black pixel blobs. From left to right: MINs, MINs w/o Inference(Section 3.2), which sample x from the inverse map conditioned on the highest seen value of y, MINs w/oReweighting (Section 3.3), and direct optimization of a forward model, which starts with a random datasetimage and optimizes it for the highest score based on the forward model. Observe that MINs can producethickest characters which resemble valid digits. Optimizing the forward function often turns non-digit pixelson, thus going off the valid manifold. Both the reweighting and inference procedure are important for goodresults. Quantitative results are provided in Appendix D.3. Different rows (for F) are obtained by optimizingfrom different initial points. Scores are listed beneath each figure. The larger score the better, provided thesolution x is the image of a valid digit.
Figure 2: MIN optimization to obtain the youngest faces when trained on faces older than 15 (left) and olderthan 25 (right). Generated faces (bottom) are obtained via inference in the inverse map at different points duringmodel training. Real faces of varying ages (including ages lower than those used to train the model) are shownin the top rows. We overlay the actual age (negative of the score function) for each face on the real images,and the age obtained from subjective user rankings on the generated faces. The score function being optimized(maximized) in this case is the negative age of the face.
Figure 3: Optimized x producedfrom contextual training on Celeb-A. Context = (brown hair, blackhair, bangs, moustache and f (x) =`1 (wavy hair, eyeglasses, smiling,no beard). We show the producedx? for two contexts. The model op-timizes score for both observed con-texts such as brown or black hair andextrapolates to unobserved contextssuch as brown and black hair.
Figure 4: Contextual MBO on MNIST. In (a) and (b), topone-half and top one-fourth of the image respectivelyand in (c) the one-hot encoded label are provided ascontexts. The goal is to produce the maximum strokewidth character that is valid given the context. In (a) and(b), we show triplets of the groundtruth digit (green),the context passed as input (yellow) and the producedimages x from the MIN model (purple).
Figure 5: Additional results for non-contextual image optimization. This task is performed on the CelebA dataset.
Figure 6: Optimal x solutions produced by a cGAN for the youngest face optimization task on the IMDB-facesdataset. We note that a cGAN learned to ignore the score value and produced images as an unconditional model,without any noticeable correlation with the score value. The samples produced mostly correspond to the mostfrequently occurring images in the dataset.
Figure 7: Images returned by the MIN optimization for optimization over images. We note that MINs performsuccessful optimization over the an objective defined by the sum of desired attributes. Moreover, for unseencontexts, such as both brown and black hair, the optimized solutions look aligning with the context reasonably,and optimize for the score as well.
Figure 8: Results for non-contextual static dataset optimization on MNIST annotated with quantitative scorevalues achieved mentioned below each figure.
