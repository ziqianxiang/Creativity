Figure 1: Prior active learning methods in MNIST degrades with oracle noise. Noise channel isassumed to be a 10-symmetric channel, where ε is the probability of label error.
Figure 2: Demonstration of appending the denoising layer to the model M for getting M* in (a),and K-SC channel with probability of error ε in (b).
Figure 3: Active learning results for various algorithms under different levels of noise strength inthe oracle decision (noise free, ε = 0.1 and 0.3) for MNIST, CIFAR10 and SVHN Image datasets.
Figure 4: Uncertainty σ across active learning ex-periment for K-SC (ε = 0.3).
Figure 5: Annotation confusion matrix of 10 classes of ESC50A ESC50 Crowd Labeling ExperimentWe selected 10 categories of ESC50 and use Amazon Mechanical Turk for annotation. In eachannotation task, the crowd worker is asked to listen to the sound track and pick the class that thesound belongs to, with confidence level. The crowd worker can also pick “Unsure” if he/she doesnot think the sound track clearly belongs to one of the 10 categories. For quality control, we embedsound tracks that clearly belong to one class (these are called gold standards) into the set of tasks anannotator will do. If the annotator labels the gold standard sound tracks wrong, then labels from thisannotator will be discarded.
Figure 6:	Active learning results for various algorithms under oracle noise strength ε = 0.2, 0.4 forMNIST Image dataset.
Figure 7:	Uncertainty σ across active learning experiment for K-SC (ε = 0.2, 0.4) on MNISTdataset.
Figure 8: Active learning results for various algorithms under oracle noise strength ε = 0.2, 0.4 forCIFAR10 Image dataset.
Figure 9: Active learning results for various algorithms under oracle noise strength ε = 0.2, 0.4 forSVHN Image dataset.
Figure 10: Uncertainty σ across active learning experiment for K-SC (ε = 0.2, 0.4) on SVHNdataset.
Figure 11:strength εActive learning results for various algorithms in noise free setting and under oracle noise= 0.1, 0.3 for CIFAR100 Image dataset.
