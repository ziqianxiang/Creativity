Figure 1: Invariant learned forbAbI task 16, basic induction,Where X:bernhard denotes a vari-able With default symbol bernhard.
Figure 2: Graphical overview of soft unification within a memory network. Each sentence is pro-cessed by two bi-directional RNNs for memory and unification. At each iteration the context atten-tion selects which sentences to unify and the invariant produces the same answer as the example.
Figure 3:	Test accuracy over iterations for Unification MLP and Unification CNN models with 1invariant versus no unification. We observe that with soft unification the models achieve higheraccuracy with fewer iterations than their plain counterparts on both per task training sizes.
Figure 4:	Invariants learned across the four datasets using the three architectures. For iterativereasoning datasets, bAbI and logical reasoning, they are taken from strongly supervised UMN.
Figure 5:	Variable bindings produced from equation 3. Darker cells indicate higher attention values.
Figure 6:	Results of Unification MLP and CNN on increasing number of invariants. There is noimpact on performance when more invariants per task are given. Upon closer inspection, we noticedthe models ignore the extra invariants and only use 1. We speculate the regularisation ψ encouragesthe models to use a single 1 invariant.
Figure 7:	bAbI task 6, yes or no questions. The invariant does not variablise the answer.
Figure 8:	Invariants learned on tasks 1, 2 and 11 with arity 1 and 2 from the logical reasoning dataset.
Figure 9:	Invariants learned that do not match the data generating distribution from UMLP andUCNN using ≤ 1000 examples to train. In these instances the unification still bind to the the correctsymbols in order to predict the desired answer; quantitatively we get the same results. Variabledefault symbols are omitted for clarity.
Figure 10:	Further attention maps for equation 3, darker cells indicate higher attention values.
