Figure 1: The Noise-Prior GAN (NPGAN) architecture. Unlike previous work, the NP networklearns a prior over the generators conditioned on the noise distribution z . This allows it to bothcontrol the sampling frequency of the generators and shape the input appropriate to each one, in anend-to-end differentiable framework.
Figure 2: A single generator fails to capture theunderlying structure without generating pointsoff the support. Only our NPGAN learns tosample the generators in proportion to the data(the generator is indicated by the point’s color).
Figure 3: Only our NPGAN is able to learnto create a discontinuity in the support of onegenerator and capture three manifolds with justtwo generators (the generator is indicated by thepoint’s color).
Figure 4: We investigate the NP network by using two-dimensional uniform noise and plotting thelearned prior over the generators. The three unequally sampled Gaussians in the data can have theirdensity matched with three generators (c-d) or by creating a discontinuity with two generators (a-b).
Figure 6: The parabola dataset is naturallysolved with two generators, but the NPGAN isrobust to the number of generators that are cho-sen. The other models can only work if the pre-cise optimal number of generators is known apriori.
Figure 7: Randomly selected images from each model trained on the CelebA+Photo dataset.
Figure 5: A single generator is unable to modelthis non-disconnected data. The disconnectedassumption of the other models forces each gen-erator to produce points that are separable fromother generators’.
