Figure 1: (a) Standard convolution only composes local information. (b) Global feature interactionmethods modify input feature maps by incorporating global information. (c) Our CGC, in a funda-mentally different manner, modulates convolution kernels under the guidance of global context. ~denotes convolution.
Figure 2: Our proposed CGC consists of three components, namely the Context Encoding Module,the Channel Interacting Module, and the Gate Decoding Module. The Context Encoding Moduleencodes global context information into a latent representation C ; the Channel Interacting Moduletransforms C to O with output dimension o; the Gate Decoding Module produces G(1) and G(2)from C and O to construct the gate G. ~, denote convolution and element-wise multiplicationoperations, respectively.ãŠ‰ is shown in Equation 1.
Figure 3: The training curves of ResNet-50 and ResNet-50 + CGC (ours) on ImageNet.
Figure 4: Visualization of the feature maps produced by ResNet-50 and CGC-ResNet-50 from 8ImageNet validation set images. (Best viewed on a monitor when zoomed in)A.3 VisualizationTo understand how CGC helps the model capture more informative features under the guidanceof context information, we visualize the feature maps of ResNet-50 and our CGC-ResNet-50 byGrad-CAM++ (Chattopadhay et al., 2018). As Figure A.3 shows, overall, the feature maps (Afterthe CGC) produced by our CGC-ResNet-50 cover more informative regions, e.g., more instances ormore parts of the ground-truth object, than vanilla ResNet-50.
Figure 5: Visualization of the difference matrix between inter-class distances and intra-class dis-tances of the last gate in the network on ImageNet validation set. (Best viewed on a monitor whenzoomed in)the same class, adjusting the kernels adaptively is beneficial for correct classification, which is con-sistent with the neuroscience research that motivates our CGC.
