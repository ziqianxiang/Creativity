Figure 1: A comparison of FOP to common optimizers on toy problems. The red dot indicatesthe initial position on the loss surface. The purpose of these visualizations is not to establish the su-periority of one optimizer over another, but rather to gain an intuition for their qualitative behavior.
Figure 2: Results on CIFAR-10 (top) and ImageNet (bottom), averaged over 3 runs. All modelsare trained with momentum as the base optimizer. We can see that FOP converges more quickly thanstandard and baseline methods, with slightly superior generalization performance. Learning a spatialcurvature matrix adds negligible computational cost to the training process.
Figure 3:	Adding FOP improves the hyperparameter robustness of standard optimizers. (a)The final test accuracy of a 9-layer CNN trained on CIFAR-10 for different settings of SGD withmomentum (top) and SGD with momentum and FOP (bottom), averaged over three runs. Settings inwhich adding FOP improves performance by at least one standard deviation are highlighted in blue.
Figure 4:	Bipedal Walker on varying terrains. We plot the the median reward (of the best seenpolicy so far) per PPO step across 10 runs for (a) and 5 runs for (b), respectively. The shadingdenotes the 95% bootstrapped confidence intervals. Momentum with FOP outperforms momentumalone significantly, both in speed of learning and final score, in addition to beating Adam.
Figure 5: Understanding the learned preconditioning matrices (MM>) for CIFAR-10. (a)The evolution of an example preconditioning matrix throughout the training process from the ninthlayer in a ResNet-18 model trained on ImageNet. Each layer learned a similar whitening structure.
