Figure 2: Illustrations of regular attention operator (a), KAOKV (b) and KAOQKV (c) on 2-D data. Inthe regular attention operator (a), the input tensor is unfolded into a mode-3 matrix and fed into theattention operator. The output of the attention operator is folded back to a tensor as the final output.
Figure 3: Architectures of the BaseModule (a), BaseSkipModule (b), AttnModule (c), and AttnSkip-Module (d) as described in Section 3.2. The skip connections indicated by single dashed paths arenot used when s > 1 or c â‰  d. Those indicated by double dashed paths are not used when s > 1.
