Figure 1: In this example batch RL problem, the robot’s goal is to travel the minimum distancearound the black walls to get to the red flag. A trained behavior policy generated the batch data;the probability of each of the states appearing in the batch, pB (s), is in yellow (white locations arenot contained in the batch). If the offline RL policy estimates the value of going up or left from thestart position is high, it will have no way to refine this estimate using the batch data, or learn a goodpolicy in this region of state space. The KL-constraint ensures that the RL policy will stay withinthe support of the batch data. However, the behavior policy is suboptimal, so using behavior cloningto directly imitate the batch data will result in suboptimal return. Instead, the KL-constrained modelcan learn to find the optimal policy, which is within the support of the batch.
Figure 2: Comparison of batch RL algorithms for different offline learning conditions. WOP con-sistently exceeds the performance of Batch Q-learning, Behavioral Cloning (BC), DBCQ, and theBehavior policy used to generate the batch data. Error bars show 95% CI of the mean over 50 trials.
Figure 3: KL-divergence of the policyfrom the prior is lower with KL-controlthroughout training. Bands show σ.
Figure 4: Z-scored reward. Red metricswere used in training rewards, green are post-hoc. Traditional RL methods like BatchQ exploit simple action-based rewards, likeasking questions. In contrast, KL-controlmethods shift their distribution towards po-lite, supportive, and cheerful conversation,allowing them to elicit higher human reward(blue).
Figure 5: (a) 64-most frequent emojis as predicted by (Felbo et al., 2017) used for calculating emo-tion embeddings. (b) Assigned weights used in producing the sentiment reward from the predictedemoji values.
Figure 6: Comparison of batch RL algorithms for different offline learning conditions in Acrobot-v1. WOP exceeds the performance of Batch Q-learning, Behavioral Cloning (BC), DBCQ, and theBehavior policy used to generate the batch data. Error bars show 95% CI of the mean over 10 trials.
Figure 7: Normalized reward scores obtained by models trained with respect to different rewards.
Figure 8: Interactive evaluation ratings page available at https://neural.chat.
Figure 9: Interactive evaluation chat interface.
