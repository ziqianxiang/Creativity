Figure 1: Diagram of the computations performed by the EfficientLab neural network. Nodes rep-resent functions and edges represent output tensors. Output spatial resolutions are written next tothe output edge. The high level architecture shows the EfficientNet feature extractor on the left withmobile inverted bottleneck convolutional blocks (see Tan & Le (2019); Sandler et al. (2018) formore details). On the right is the residual skip decoder module that we utilize in the upsamplingbranch of EfficientLab.
Figure 2: Each point represents the mean IoU for the validation tasks with a sampled learning rate Î±.
Figure 3: Mean IoU results as a function of the training set size of our EfficientLab model adaptedto tasks of the FP-k dataset. Error bars represent 95% confidence intervals. The meta-learned ini-tialization utilizes the top performing learning algorithm in Table 2. The meta-learned initializationoutperformed EfficientLab initialized with an ImageNet-trained encoder and a randomly initializeddecoder for all numbers of labeled training examples that we evaluated. For additional experimentaldetails see C in the appendix.
Figure 4: Randomly sampled example 5-shot predictions on the test images from testtasks. Positive class prediction is overlaid in red. From left to right, top to bottom, theclasses are apple_icon, australian_terrier, church, motorbike, flying_frog,flying_snakes, hover_board, porcupineB FP-k DatasetTable 3 contains the five tasks in PASCAL-5i that have direct analogs in FSS-1000. Each rowcontains the name of a task in FSS-1000 and PASCAL-5i, respectively. We combine all examplesfor synonymous tasks. During evaluation, we simply randomly sample 20 test examples, and samplea training set of k examples over the range: [1, 5, 10, 50, 100, 200, 400]. For more details on ourtraining and evaluation procedures see C.
