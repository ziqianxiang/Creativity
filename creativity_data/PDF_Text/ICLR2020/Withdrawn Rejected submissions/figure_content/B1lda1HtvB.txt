Figure 1: Probability of successfully recovering the support of a sparse signal β * ∈ RD vs. thenumber of observations N . Comparison between the proposed method, HC and LASSO under alinear additive noise model y = Xβ* + w using (a) D = 64 and (b) D = 128. Central lines are themeans while shaded are represent the standard deviation.
Figure 2: (a) Classification accuracy (mean and standard deviation) vs. the number of irrelevant noisydimension (D) for the XOR problem. (b) The Informative Features Weight Ratio (IFWR), centrallines are the means while shaded are represent the standard deviation. IFWR is the sum of weightsWd over the informative features divided by the sum over all weights. (c) Box plots for the medianrank of the two informative features. Black line and dashed red represent the median and mean ofeach method. Optimal median rank in this experiment is 1.5.
Figure 3: Classification of T-cells sub-populations. Accuracy vs. number of features selected by eachmethod. Comparison of the proposed method (STG and HC) to Random Forests (RF) and LASSO.
Figure 4: The plot of ∂μEz∣∣Z||o∣μ=o = √2∏σ2 e-812 for σ = [0.001, 2].
Figure 5: Comparison between STG and Hard-Concrete on the Two-Moon dataset (a), XOR (b) andMADELON (c). The shaded area represents the standard deviation, calculated by running the sameexperiment 10 times with different random initializations.
Figure 6: Support recovery in the linear regression (see Section 6.1), the dimension D = 64. (a)Phase transition comparison between the STG, HC, LASSO and the deterministic non convex (DNC).
Figure 7: Demonstrating a cross validation procedure on the RAI dataset (see section I for details).
Figure 8: (a) Classification accuracy on the MADELON data sets. We evaluate performance using5-fold Cross validation for different number of seleCted features. In this dataset, only the first 20Coordinates are informative. In that regime the proposed method outperforms RF and LASSO. (b)An empiriCal evaluation of the effeCt the regularization parameter λ. The IFWR and the number ofseleCted features are presented on both sides of the y-axis of this plot. For both plots, the mean ispresented as a solid/dashed line, while the standard deviation is marked as a shaded Color around themean. (C) Box plots for the median rank of the 5 original informative features. BlaCk line and dashedred represent the median and mean of eaCh method. Optimal median rank in this experiment is 3.
Figure 9: (a) Realizations from the "Two moons" shaped binary classification class. X1 and X2 arethe relevant features, Xi, i = 3, ..., D are noisy features drawn from a Gaussian with zero mean andvariance of 1. (b) Classification accuracy (mean and standard deviation based on 20 runs) vs. thenumber of irrelevant noisy dimension.
Figure 10: (a) Nine samples from MNIST (white) overlaid with the subset of 13 features (black)selected by STG. Based on only these features, the binary classification accuracy reaches 92.2%.
Figure 11: Classification of the binary noisy digits from the Gisette dataset. The total numberof feature is 5000 of which 2500 are irrelevant probes. Here we compare the performance of theproposed approach to RF and Tree based classifier. The lines represent a least squares polynomial fitplot of the accuracy vs. number of selected features.
Figure 12: Classification of the version of the RCV1 textual dataset. The total number of feature is47, 236. Here we compare the performance of the proposed approach to RF and LASSO. The linesrepresent a least squares polynomial fit plot of the accuracy vs. number of selected features.
