Figure 1: Dynamics variations in two dimensions and with different €, for Hopper (top row), Walker (middlerow), and HalfCheetah (bottom row). Fourth column is PPO trained with dynamics sampled uar from theWasserstein ball. Darker is better, with the numbers in the graded scale being the range of scores that the taskscan generate.
Figure 2: Results evaluating performance when considering high-dimensional variations on the Hopper (toprow) and HalfCheetah (bottom row) environment. All figures show the empirical distribution of returns on1,000 testing systems. The first column demonstrates the robustness of PPO when trained on the referencedynamics and trained with high dimensional variations. The second column reports empirical test returns ofWR2L’s policy trained on only two parameter changes (e.g., friction and density) of the environment but testedon systems with all high-dimensional dynamical parameters modified. The third column trains and tests WR2Laltering all dimensional parameters of the simulator. The fourth column reports results when PPO is trained ondynamics chosen uar and tested with high dimensional variation.
Figure 3: Robustness tests for various tasks; dynamics varied along one dimension.
Figure 4: Robustness results on the inverted pendulum demonstrating that our method outperformsstate-of-the-art in terms of average test returns and that DDPG lacks in robustness performance.
