Figure 1:	We propose the GDA setting, where We consider SFD and SLD simultaneously. To tackle thisproblem, we propose to use self-training to estimate and align the target label distribution, and use a prototype-based method for conditional alignment. In this way, we can better align the feature distribution of eachcategory. Due to label shift, previous methods that learn marginal domain-invariant features will incorrectlyalign samples from different categories, leading to negative transfer.
Figure 2:	Overview of the proposed COAL model. Our model is trained iteratively between twosteps. In step A, we forward the target samples through our model to generate the pseudo labels andmask. In step B, we train our models by self-training with the pseudo labels and mask to align thelabel distributions, and prototype-based ConditionaI alignment with the minimax entropy.
Figure 3: (a): Image examples from Digits, Office-Home (VenkatesWara et al., 2017), and Domain-Net Peng et al. (2019a). (b): illustrations of Reversely-unbalanced Source (RS) and UnbalancedTarget (UT) distribution in MNIST→USPS task. (c): Natural SLD of DomainNet.
Figure 4: Feature visualization: t-SNE plot for source features, DAN features, DANN features, andCOAL features on GDA task Real → Clipart. Figure (a)-(d): features from each class. Differentcolors denote different categories. Figure (e)-(h): features from each domain. Blue and red pointsrepresents features from the source domain and target domain, respectively.
Figure 5: Performance on USPS→MNISTtask with different degrees of SLD. 0% and100% denote the BS-BT and RS-UT settingsrespectively. Others are the linear interpo-lations of BS-BT and RS-UT. The resultsdemonstrate that our model are more robustto different degrees of SLD.
