Figure 1: Toy examples using the max path ensemble. (a) When calculating return of the secondtrajectory, the max chooses G(1), so that the bad action aι at state s3 is blacklisted. This leads to anoptimistic view when evaluating the long-term returns of actions. The dashed circle means that itis ignored in the computation of return. (b) and (c) demonstrate the over-estimation problem of themax statistics when uncertainty exists in transitions or rewards.
Figure 2: The max and the min statistics on three different types of sparse-reward environments.
Figure 3: Left: screenshot of the flat and incline environment for biped walking. Right: theperformance of PPO and PPO with min and max statistics. We ran 5 seeds for each setting.
Figure 4: Experiments on MuJoCo environments. Three order statistics max, min and max-abswere compared with the vanilla PPO algorithm. Each setting was run with 5 seeds.
Figure 5: Experiments on the Atari environments. A2C’ was the original A2C algorithm withoutmini-batch, and A2C was using the same mini-batch configuration as other three settings: max, minand max-abs. Each setting was run for 3 seeds. (This figure is best viewed in color.)4.3	Continuous Control and the Overreaction InclinationWe tested various advantage estimations on 5 continuous control benchmarks based on the MuJoCophysics simulator. They are not sparse-reward environments, and most of them are also not sensitiveto individual actions. For example, the design of the HalfCheetah agent makes it seldom fall downor be stuck. We used PPO in this set of experiments. The implementation was based on thebaselines (Dhariwal et al., 2017) code repository, and the default hyper-parameters were adopted. Wetested three order statistics including the max, the min and the max-abs statistics. In all settings, thepath ensemble had index set {1, 16, 64, 2048}, and the regulatory ratio was set to ρ = 0.4. Resultsare shown in Figure 4. We observed that the max-abs statistics was consistently better than thebaseline PPO algorithm, while the performances of the max and the min statistics depended uponthe specific environment. So we conclude that the overreaction inclination by the max-abs statisticsis generally effective for a wide range of environments.
Figure 6: The order statistics over the path ensemble in MuJoCo environments. x-axis labels thedifferent order statistics. The four series in the sub-figures were varying the regulatory ratio ρ, andthe flat line indicated performance of the baseline PPO algorithm. Each data point was averaged over5 runs.
Figure 7: The -greedy exploration. Red: MaxAbs, Purple: PPO, Rest: -greedy explorations,where values of 0.05, 0.1, 0.2, 0.3, 0.4. Each experiment has 5 runs.
Figure 8: Conservative policy via clipping. Red: the Min path-ensemble with the default clippingparameter 0.2, Purple: PPO with the default clipping parameter 0.2, Rest: PPO with smaller clippingconstants 0.1 and 0.05. Each experiment has 5 runs.
Figure 9: Ensemble λs. Green: MaxAbs, Blue: PPO, Orange: MaxAbs of λs of {0.1, 0.3, 0.6, 0.95}.
