Figure 2: UnderwaterGAN architecture. UWGAN takes color image and its depth map as input, then itsynthesizes underwater realistic images based on underwater optical imaging model by learning parametersthrough generative adversarial training.
Figure 3: ProPosed U-net Architecture for underwater image restoration and enhancement.
Figure 4: Typical images of datasets. (a)-(b) are color images and depth maps of NYU-Depth datasets, (c)are sample images of RealA dataset, (d) are sample images of RealB dataset, (e) are sample images of RealCdataset.
Figure 5: Qualitative comparisons for samples from the real-world underwater image dataset RealC. (a)-(j)represent the samples selected from RealC.
Figure 6: Qualitative comparisons for samples from real-world underwater image dataset RealA and RealB.
Figure 7: Underwater target detection results before and after enhancement. (A) Real-world underwaterimages and (B) output of our model for the real-world image. Red boxes represent scallops, blue boxesrepresent sea cucumbers, and green boxes represent sea urchins.
Figure 8: The visual quality of the sample image in RealC dataset with different loss functions. From (a) to(i) are respectively enhanced results of the loss function L1, L2, SSlM, MSSSIM, GDL, L1 + L2, L1 +SSIM, L1 + MSSIM, and L1 + GDL.
