Figure 1: Node embeddings of Zachary’s karate club network trained on a node classification task(red vs. blue). Node coordinates in (b)-(e) are the embedding coordinates. Notice that GCN doesnot produce linearly separable embeddings ((b) vs. (c)), while GCN-LPA performs much better evenin the presence of noisy edges ((d) vs. (e)). Additional visualizations are included in Appendix E.
Figure 4: Training time Perepoch on random graphs.
Figure 2:	Sensitivity to # LPAiterations on Citeseer dataset.
Figure 3:	Sensitivity to λ onCiteseer dataset.
Figure 5: An illustrating example of label propagation in LPA. Suppose labels are propagated forthree iterations, and no self-loop exists. Blue nodes are labeled while white nodes are unlabeled. (a)va’s label propagates to v1 (yellow arrows). Note that the propagation of va’s label to v3 is cut offsince v3 is labeled thus absorbing va ’s label. (b) va ’s label that propagated to v1 further propagatesto v2 and vb (yellow arrows). Meanwhile, va ’s label is reset to its initial value then propagatesfrom va again (green arrows). (c) The propagating process in iteration 3. Purple arrows denote thepropagation of va’s label starting from va for the third time. (d) All possible paths of length no morethan three from va to vb containing unlabeled nodes only. Note that there is no path of length onefrom va to vb .
Figure 6: Visualization of GCN and GCN-LPA with 1 〜4 layers on karate ClUb network.
