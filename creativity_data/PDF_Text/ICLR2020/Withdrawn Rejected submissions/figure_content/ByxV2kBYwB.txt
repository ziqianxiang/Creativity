Figure 1: Knowledge distillation of generative models for few-shot learning. We aim to recognize two novelclasses from 2 examples per class (Figure 1a). The desired classifier is the one that would be learned fromabundant real examples (Figure 1b). To this end, we distill the knowledge of the desired large-sample (dashed)classifier into a generative model, and thus enable it to produce additional examples from the few real examplesin a way that minimizes the discrepancy between the (solid) classifier trained on synthesized examples togetherwith the few real examples and the large-sample (dashed) classifier (Figure 1c). Real examples are shown assquares, synthetic examples as triangles, and classifier decision boundaries as solid or dashed lines.
Figure 2: Framework overview of meta-learning a generative model through knowledge distillation. Duringeach iteration of meta-training, a small support set Strain is augmented with a set StGrain of examples synthe-sized by a generator G. Examples from the resulting set Staruagin are used to build a student classifier model S.
Figure 3: Visualization with t-SNE of the evolution of the decision boundary for two novel classes, when meta-training the generator through progressive distillation by weakening the student. Real examples (small dots)are progressively removed, and synthesized examples (triangles) are generated in a way that helps maintain thestudent decision boundary (red dashed line) as close as possible to the desired decision boundary that would beformulated by a large set of real examples (black solid line).
Figure 4: Visualization of synthesized examples for four novel classes. The single black framed image comefrom the original dataset and is used as a seed for synthesising new examples. Color framed images correspondto the nearest neighbor real images of the synthesized examples in the feature space. Best viewed in color.
