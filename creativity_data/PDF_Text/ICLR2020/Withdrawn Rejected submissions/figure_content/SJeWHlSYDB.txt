Figure 1: Relative error |Aiejst - Aitjrue|/|Aitjrue| versus observation noise (a) and number of trainingpoints (b). (a) For X=20 observations and Z=10 latent variables, we generate N =20000 datapointsfrom the model x=Az, for independent zero mean unit variance Laplace components on z. Theelements of A used to generate the data are uniform random ±1. We use Sy =1, Sz=1000 samplesand 2000 EM iterations to estimate A. The error is averaged over all i, j and 10 experiments. Wealso plot standard errors around the mean relative error. In blue we show the error in learning theunderlying parameter using the standard EM algorithm. As expected, as γ→0, the error blows up asthe EM algorithm ‘freezes’. In orange we plot the error for EM using spread noise; no slowing downappears as the observation noise γ decreases. In (b), apart from very small N, the error for the spreadEM algorithm is lower than for the standard EM algorithm. Here Z=5, X =10, Sy =1, Sz =1000,γ = 0.2, with 500 EM updates used. Results are averaged over 50 runs of randomly drawn A.
Figure 2: Samples from a generative model (deterministic output) trained using δ-VAE with (a) fixedLaplace covariance, (b) fixed Gaussian covariance and (c) learned Gaussian covariance. We first trainwith one epoch a standard VAE as initialization to all models, and keep latent code Z 〜N (Z ∣ 0,Iz)fixed when sampling from these models, so we can more easily compare the sample quality. Figure(d) visualizes the absolute mean of the leading 20 eigenvectors of the learned covariance.
Figure 3: Samples from a generative model with deterministic output trained using δ-VAE with (a)fixed and (b) learned spread with injective function. We use a similar sampling strategy as in theMNIST experiment to facilitate sample comparison between the different models - see Section(I).
Figure 4: Samples from a generative model (deterministic output) trained using δ-VAE with (a)Laplace noise with fixed covariance, (a) Gaussian noise with fixed covariance and (c) Gaussian noisewith learned covariance.
Figure 5: Samples from a generative model with deterministic output trained using δ-VAE with (a)fixed and (b) learned spread with injective mean transform.
