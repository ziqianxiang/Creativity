Figure 1: Transformer variants, showing just a single layer block (there are L layers total). Left:Canonical Transformer(-XL) block with multi-head attention and position-wise MLP submodulesand the standard layer normalization (Ba et al., 2016) placement with respect to the residual con-nection (He et al., 2016a). Center: TrXL-I moves the layer normalization to the input stream of thesubmodules. Coupled with the residual connections, there is a gradient path that flows from outputto input without any transformations. Right: The GTrXL block, which additionally adds a gatinglayer in place of the residual connection of the TrXL-I.
Figure 2: Average return on DMLab-30, re-scaled such that a human has mean 100 score on eachlevel and a random policy has 0. Left: Results averaged over the full DMLab-30 suite. Right:DMLab-30 partitioned into a “Memory” and “Reactive” split (described in Appendix D). TheGTrXL has a substantial gain over LSTM in memory-based environments, while even slightly sur-passing performance on the reactive set. We plot 6-8 hyperparameter settings per architecture (seeAppendix B). MERLIN scores obtained from personal communication with the authors.
Figure 3: Numpad results demonstrating that the GTrXL has much better memory scaling propertiesthan LSTM. Left: As the Numpad environment’s memory requirement increases (because of largerpad size), the GTrXL suffers much less than LSTM. However, because of the combinatorial natureof Numpad, the GTrXL eventually also starts dropping in performance at 4x4. We plot mean andstandard error of the last 200 episodes after training each model for 0.15B, 1.0B and 2.0B environ-ment steps for Numpad size 2, 3 and 4, respectively. Center, Right: Learning curves for the GTrXLon 2 × 2 and 4 × 4 Numpad. Even when the LSTM is trained for twice as long, the GTrXL still hasa substantial improvement over it. We plot 5 hyperparameter settings per model for learning curves.
Figure 4: Learning curves for the gating mechanisms, along with MERLIN score at 100 billionframes as a reference point. We can see that the GRU performs as well as any other gating mech-anism on the reactive set of tasks. On the memory environments, the GRU gating has a significantgain in learning speed and attains the highest final performance at the fastest rate. We plot bothmean (bold) and the individual 6-8 hyperparameter samples per model (light).
Figure 5: Sensitivity analysis of GTrXL variants versus TrXL and LSTM baselines. We sample 25different hyperparameter sets and seeds and plot the ranked average return at 3 points during training(0.5B, 1.0B and 2.0B environment steps). Higher and flatter lines indicate more robust architectures.
Figure 6: Learning curves comparing a thinner GTrXL (GRU) with half the embedding dimensionof the other presented gated variants and TrXL baselines. The Thin GTrXL (GRU) has fewer pa-rameters than any other model presented but still matches the performance of the best performingcounterpart, the GTrXL (Output), which has over 10 million more parameters. We plot both mean(bold) and 6-8 hyperparameter settings (light) per model.
Figure 7: Ablation of the gated identity initialization on Memory Maze by comparing 10 runs of amodel run with the bias initialization and 10 runs of a model without. Every run has independentlysampled hyperparameters from a distribution. We plot the ranked mean return of the 10 runs of eachmodel at 1, 2, and 4 billion environment steps. Each mean return is the average of the past 200episodes at the point of the model snapshot. We plot human performance as a dotted line.
Figure 8: Left: The Numpad environment, showing the controllable “sphere” robot and a full 3x3pad. Pads are activated when the robot collides with their center. The robot can move on the planeas well as jump to avoid pressing numbers. Right: Top down view of “Memory Maze”: (1) Centralchamber, (2) blocks among which the apple is placed, (3) landmarks the agent can use to locate theapple, (4) one of the possible location of the apple.
Figure 9: Learning curves for the DMLab Arbitrary Visuomotor Mapping task using a reducedaction set.
Figure 10: The 25 hyperparameter settings sampled for the sensitivity ablation (Sec. 4.3.2). X-axisis in log scale and values are sampled from the corresponding ranges given in Table 8.
Figure 11: Median human-normalized returns as training progresses for both GTrXL and LSTMmodels. We run 8 hyperparameter settings per model.
