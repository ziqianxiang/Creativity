Figure 1: Image-based continuous control tasks from the DeepMind control suite (Tassa et al., 2018)used in our experiments. Each task offers an unique set of challenges, including complex dynamics,sparse rewards, hard exploration, and more. Refer to Appendix A for more information.
Figure 2: Separate β-VAE and policy training with no shared gradients SAC+VAE:pixel (iter, N), withSAC:state shown as an upper bound. N refers to frequency in environment steps at which the β-VAE updatesafter initial pretraining. More frequent updates are beneficial for learning better representations, but cannotfully address the gap in performance. Full results in Appendix C.
Figure 3: An unsuccessful attempt to propagate gradients from the actor-critic down to the encoder of theβ-VAE to enable end-to-end off-policy training. The learning process of SAC+VAE:pixel exhibits instabilitytogether with the subpar performance comparing to the baseline SAC+VAE:pixel (iter, 1), which does not sharegradients with the actor-critic. Full results in Appendix D.
Figure 4: Our algorithm (SAC+AE) auguments SAC (Haarnoja et al., 2018) with a regularized autoen-coder (Ghosh et al., 2019) to achieve stable end-to-end training from images in the off-policy regime. Thestability comes from switching to a deterministic encoder that is carefully updated with gradients from thereconstruction J (AE) (Equation (3)) and soft Q-learning J(Q) (Equation (1)) objectives.
Figure 5: The main result of our work. Our method demonstrates significantly improved performance overthe baseline SAC:pixel. Moreover, it matches the state-of-the-art performance of model-based algorithms, suchas PlaNet (Hafner et al., 2018) and SLAC (Lee et al., 2019), as well as a model-free algorithm D4PG (Barth-Maron et al., 2018), that also learns from raw images. Our algorithm exhibits stable learning across ten randomseeds and is extremely easy to implement.
Figure 6: Linear Projections of latent rePresentation sPaces learned by our method (SAC+AE:Pixel) and thebaseline (SAC:Pixel) onto ProPriocePtive states. We comPare ground truth value of each ProPriocePtive co-ordinate against their reconstructions for Cheetah_run, and conclude that our method successfully encodesProPriocePtive state information. For visual clarity we only Plot 2 Position (out of 8) and 2 velocity (out of 9)coordinates. Full results in APPendix F.
Figure 7: Encoder pretrained with our method (SAC+AE:PiXel) on Walker-Walk is able to generalize tounseen Walker-Stand and Walker_run tasks. All three tasks share similar image observations, but havequite 祖fferent reward structure. SAC with a pretrained on Walker-Walk encoder achieves impressive finalperformance, while the baseline struggles to solve the tasks.
Figure 8:	We use six domains spanning the total of twelve challenging continuous controltasks: finger _{spin,turn_easy,turn_hard}, cartpole _{balance,swingup},cheetah_run,	walker _{stand,walk,run},	reacher _{easy,hard}, andball_in_cup_catch.
Figure 9:	Separate β-VAE and policy training with no shared gradients SAC+VAE:pixel (iter, N),with SAC:state shown as an upper bound. N refers to frequency in environment steps at whichthe β-VAE updates after initial pretraining. More frequent updates are beneficial for learning betterrepresentations, but cannot fully address the gap in performance.
Figure 10: An unsuccessful attempt to propagate gradients from the actor-critic down to the encoderof the β-VAE to enable end-to-end off-policy training. The learning process of SAC+VAE:pixelexhibits instability together with the subpar performance comparing to the baseline SAC+VAE:pixel(iter, 1), which does not share gradients with the actor-critic.
Figure 11: Different autoencoder architectures, where we vary the number of conv layers and thenumber of output channels in each layer in both the encoder and decoder. For example, 4 × 32specifies an architecture with 4 conv layers, each outputting 32 channels. We observe that thedifference in capacity has only limited effect on final performance.
Figure 12: Linear projections of latent representation spaces learned by our method (SAC+AE:pixel)and the baseline (SAC:pixel) onto proprioceptive states. We compare ground truth value ofeach proprioceptive coordinate against their reconstructions for Cheetah_run, and conclude thatour method successfully encodes proprioceptive state information. The proprioceptive state ofcheetah_run has 8 position and 9 velocity coordinates.
Figure 13: An auxiliary signal is provided by reconstructing a low-dimensional state from the cor-responding image observation. Perhaps surprisingly, such synthetic supervision doesn’t guaranteesufficient signal to fit the high-capacity encoder, which we infer from the suboptimal performanceof SAC:PiXel (state supervision) compared to SAC:PiXel in ball_in_cup_catch.
Figure 14:	Training curves for the policy used to collect the buffer (SAC+AE:pixel (collector)),and the two policies learned on that buffer using proprioceptive (SAC:state (fixed buffer)) and pixelobservations (SAC+AE:pixel (fixed buffer)). We see that our method actually outperforms proprio-ceptive observations in this setting.
Figure 15:	We study the importance of the action repeat hyper parameter on final performance.
Figure 16:	We benchmark SAC, TD3, DDPG, and D4PG when learning from proprioceptive stateson multiple tasks of various difficulty from DMC. We run 10 random seeds for each algorithm andevaluate on 10 trajectories (except for D4PG, as its implementation is not publicly available). Wethen report mean and standard deviation. For D4PG we take the performance after 108 environmentsteps reported in Tassa et al. (2018). We observe that SAC demonstrates superior performance andsample efficiency over the other methods on all of the tasks.
