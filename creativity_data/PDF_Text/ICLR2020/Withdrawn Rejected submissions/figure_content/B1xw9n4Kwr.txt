Figure 1: Illustration of a multilayer perceptron•	The post-subnetwork at ws is defined as the network formed by the subgraph rooted at the rightvertex of ws. The pre-subnetwork at ws is defined as the union of all output paths ending in ws,denoted by pre(ws).
Figure 2: Path Decomposition Examples.
Figure 3: Training Curve Characterization for Two-layer Linear Neural Network.
Figure 4: Training Curve Characterization for Two-layer Non-Linear Neural Network.
Figure 5: Depth induced momentum.
Figure 6: Convergence Rate for a 3-layer RELU-activated Network.
Figure 7: Mean and Std across the training process. (X-axis is the number of steps)2000	3000Mean of Weights, Sigmoid・ width 2000 layer 0 . width 2000 layer 1-0.0001-0.0002-0.0003Figure 8: KS test p-values across the training process.
Figure 8: KS test p-values across the training process.
Figure 9: Turning point test p-valueslayer 1rwidth:2000, sigmoid10000	20000	30000	40000	50000StepsE Justify iid assumption for weightsThe Turning Point Test is commonly used to check iid-ness. Figure 9 shows the p-value through thetraining process. As shown, p-values do not exhibit any major regime shifts and the null hypothesis(the weights are iid samples) stays far from rejectable with any commonly used significance levels(0.1, 0.05 . . . ). iid-ness can also be seen as a consequence of closeness to initialization in response 1.
Figure 10: Momentum’s dependence on output scale.
