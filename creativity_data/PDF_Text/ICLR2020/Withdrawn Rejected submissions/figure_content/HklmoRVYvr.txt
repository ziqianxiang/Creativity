Figure 1: Graphical illustrations of the standard RNN, Higher-Order RNN (Soltani & Jiang, 2016),Single and Double LH-STM. Xk and Xk, respectively indicate the input and predicted frame at timek. Hk-3, Hk-2, Hk-1 are past hidden states. A blue line indicates a recurrent connection; greencolor indicates History Soft-Selection unit, and a circle donates a memory unit.
Figure 2: A graphical illustration of the History Soft-Selection Unit (HistSSel). It computes therelationship between the last Hk-1 and the earlier hidden state Hk-m:k-2.
Figure 3: The use of input history with LH-STMSingle (left) and Double (right)History	LH-STM	PSNR	SSIM	VIFInput	Single	25.78	0.779	0.302	Double	26.45	0.788	0.308State	Single	27.83	0.816	0.354	Double	28.64	0.826	0.367Table 1: Comparison of using input vs state his-tory with LH-STM Single and Double on the KTHhuman actions dataset (resolution 128 × 128).
Figure 4: Per-frame comparisonson the KTH human actions dataset(all actions, 64 × 64, 80 frameprediction). (∙): averaged score0,9∞0	204δ 60^Time stepsβ W fl ⅛9 9 9 8 8 8 8α α α α α α αSdldlTime stepsFigure 5: Per-frame comparisonson the KTH human actions dataset Figure 6: Per-frame comparisons(‘boxing’, ‘waving’, and ‘clapping’on BAIR robot pushing datasetactions, 64 × 64, 80 frame	(25 frame prediction)prediction). (∙): averaged score	(∙): averaged score0 9 8 7 6 53 2 2 2 Z 2
Figure 5: Per-frame comparisonson the KTH human actions dataset Figure 6: Per-frame comparisons(‘boxing’, ‘waving’, and ‘clapping’on BAIR robot pushing datasetactions, 64 × 64, 80 frame	(25 frame prediction)prediction). (∙): averaged score	(∙): averaged score0 9 8 7 6 53 2 2 2 Z 2哙Sd104.4	BAIR Robot Pushing DatasetOur experimental setup follows that of Lee et al. (2018). All models are conditioned on five framesto predict the next ten frames 5. The models are tested to predict 25 frames. We evaluated ourLH-STM models (Single and Double) and compared them with the base model (ContextVP), the3SAVP-VAE, and SAVP-Deterministic models (Lee et al., 2018) in Table 3. Our Double LH-STMachieved better PSNR and SSIM scores than the SAVP-Deterministic and SAVP-VAE models. It canbe seen from the per-frame prediction results (Figure 6), that the performance LH-STM is higher overtime, especially on the PSNR and SSIM metrics. On the LPIPS metric, the performance of DoubleLH-STM is comparable to SAVP-Deterministic and better than SAVP-VAE. Qualitative results canbe found in Appendix E (Figure 17 and Figure 18). This dataset contains random robot motionwhich makes long-term prediction extremely hard with deterministic models. However, the overall
Figure 7: Qualitative comparison of Double LH-STM, Single LH-STM, Double LH-STM with inputhistory (See Section 4.2 for the details), the base model (ContextVP (Byeon et al., 2018)), and thestate-of-the-art model (SAVP-VAE and SAVP-Deterministic (Lee et al., 2018)). Double LH-STM inoverall produces the sharper images and greater realism compared to other models. Compare to theDeterministic model (Lee et al., 2018), our model can capture more precise motion.
Figure 8:	ContextVP-4 network archtectureWeighted Blending: This layer learns the relative importance of each direction during training.
Figure 9:	A graphical illustration of skip connections for the direction t-. Blue dotted lines indicateconcatenation-based skip connection in the LH-STM block. A green solid line indicates a gated skipconnection across the 1-3 and 2-4 layers.
Figure 10:	Per-frame comparisons of KTH human actions dataset (80 frame prediction, resolution 64×64) withmotion-based test set separation. We split the video samples into five bins by motion magnitude computed asaveraged L2 norm between target frames following Villegas et al. (2017a). The first row is with smallest motionand the bottom row With largest motion. (∙) indicates averaged score.
Figure 11:	Per-frame comparisons of BAIR robot pushing dataset with motion-based test set separation. Wesplit the video samples into five bins by motion magnitude computed as averaged L2 norm between target framesfollowing Villegas et al. (2017a). The first row is with smallest motion and the bottom row with largest motion.
Figure 12:	80 frame prediction on KTH human actions dataset separated by actions: hand waving, handclapping, and boxing. (∙) indicates averaged score.
Figure 13:	80 frame prediction on KTH human actions dataset separated by actions: jogging, running, andwalking. (∙) indicates averaged score.
Figure 14:	Qualitative comparison of Double LH-STM, Single LH-STM, Double LH-STM with inputhistory (See Section 4.2 for the details), the base model (ContextVP (Byeon et al., 2018)), and thestate-of-the-art models (SAVP-Deterministic and SAVP-VAE Lee et al. (2018)) on the KTH humanactions dataset. Top row shows the ground truth, and the rest is the prediction results. The model istrained for 10 frames and predicted for 40 frames given 10 frames.
Figure 15: Qualitative comparison of Double LH-STM, Single LH-STM, Double LH-STM with inputhistory (See Section 4.2 for the details), the base model (ContextVP (Byeon et al., 2018)), and thestate-of-the-art models (SAVP-Deterministic and SAVP-VAE Lee et al. (2018)) on the KTH humanactions dataset. Top row shows the ground truth, and the rest is the prediction results. The model istrained for 10 frames and predicted for 40 frames given 10 frames.
Figure 16: An example video with one of the largest motion on the KTH human actions dataset. Themotion magnitude is computed as averaged L2 norm between target frames following Villegas et al.
Figure 17: Qualitative comparison of Double LH-STM and the state-of-the-art models (SAVP-Deterministic and SAVP-VAE Lee et al. (2018)) on the BAIR robot pushing dataset. Top row showsthe ground truth, and the rest is the prediction results. The model is trained for 10 frames andpredicted for 25 frames given 5 frames.
Figure 18: Qualitative comparison of Double LH-STM and the state-of-the-art models (SAVP-Deterministic and SAVP-VAE Lee et al. (2018)) on the BAIR robot pushing dataset. Top row showsthe ground truth, and the rest is the prediction results. The model is trained for 10 frames andpredicted for 25 frames given 5 frames.
Figure 19: An example video with one of the largest motion on the BAIR robot pushing dataset. Themotion magnitude is computed as averaged L2 norm between target frames following Villegas et al.
