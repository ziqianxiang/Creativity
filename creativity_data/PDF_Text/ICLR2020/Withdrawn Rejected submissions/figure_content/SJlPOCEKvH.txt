Figure 1: (Blue) The best GLUE dev accuracy and training losses for models pruned during pre-training, averaged over 5 tasks. Also shown are models with information deletion during pre-training(orange), models pruned after downstream fine-tuning (green), and models pruned randomly duringpre-training instead of by lowest magnitude (red). 30-40% of weights can be pruned using mag-nitude weight pruning without decreasing dowsntream accuracy. Notice that information deletionfits the training data better than un-pruned models at all sparsity levels but does not fully recoverevaluation accuracy. Also, models pruned after downstream fine-tuning have the same or worsedevelopment accuracy, despite achieving lower training losses. Note: none of the pruned modelsare overfitting because un-pruned models have the lowest training loss and the highest developmentaccuracy. While the results for individual tasks are in Table 1, each task does not vary much fromthe average trend, with an exception discussed in Section 4.3.
Figure 2: (Left) Pre-training loss predicts information deletion GLUE accuracy linearly as sparsityincreases. We believe the slope of each line tells us how much a bit of BERT is worth to each task.
Figure 3: (Left) The measured difference in pruning masks between models pruned during pre-training and models pruned during downstream fine-tuning. As predicted, the differences are lessthan 6%, since fine-tuning only changes the magnitude sorting order of weights locally, not globally.
Figure 4: (Left) The average, min, and max percentage of individual attention heads pruned at eachsparsity level. We see at 60% sparsity, each attention head individually is pruned strictly between55% and 65%. (Right) We compute the magnitude sorting order of each weight before and afterdownstream fine-tuning. If a weight’s original position is 59 / 100 before fine-tuning and 63 / 100after fine-tuning, then that weight moved 4% in the sorting order. After even an epoch of downstreamfine-tuning, weights quickly stabilize in a new sorting order which is not far from the original sortingorder. Variances level out similarly.
Figure 5: The sum of weights pruned at each sparsity level for one shot pruning of BERT. Given themotivation for our saliency criterion, it seems strange that such a large magnitude of weights can bepruned without decreasing accuracy.
Figure 7: We show how weight sort order movements are distributed during fine-tuning, given aweight’s starting magnitude. We see that higher magnitude weights are more stable than lowermagnitude weights and do not move as much in the sort order. This plot is nearly identical for everymodel and learning rate, so we only show it once.
Figure 8: A heatmap of the weight magnitudes of the 12 horizontally stacked self-attention keyprojection matrices for layer 1. A banding pattern can be seen: the highest values of the matrix tendto cluster in certain attention heads. This pattern appears in most of the self-attention parametermatrices, but it does not cause pruning to prune one head more than another. However, it may proveto be a useful heuristic for attention head pruning, which would not require making many passesover the training data.
Figure 9: A heatmap of the weight magnitudes of BERT’s subword embeddings. Interestingly,pruning BERT embeddings are more interpretable; we can see shorter subwords (top rows) havesmaller magnitude values and thus will be pruned earlier than other subword embeddings.
