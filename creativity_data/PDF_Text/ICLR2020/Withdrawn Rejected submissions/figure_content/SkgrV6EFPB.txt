Figure 1: Approach Overview: (a) First, model a CNN as a network of channels. (b) Next, We Pro-pose NN-Mass and NN-Density, where NN-Mass is a theoretically-grounded metric that indicatesgeneralization capability. (c) Exploit NN-Mass to directly design significantly compressed models.
Figure 2: (a) CNN consisting of three cells of dc layers each. (b) An input channel i contributes toa given output channel j with some fixed probability αij . (c) Each cell contains dc layers with wcchannels per layer. Output channels at each layer get contributions from output channels of last layerand additional long-range links from previous layers (via concatenation). Not all links are shown.
Figure 3: CIFAR-10 Width Multiplier wm = 2: (a) Shallower models with higher density can reachcomparable accuracy to deeper models with lower density. This does not help since models withdifferent depths achieve comparable accuracies at different densities. (b) Models with very different#parameters (box W) achieve similar test accuracies. (c) Models with similar accuracy often havesimilar NN-Mass: Models in W get clustered into Z. Results are reported as mean of three runs.
Figure 4:	Similar observations hold for low- (wm = 1) and high-width (wm = 3) models: (a,b) Many models with very different #parameters (boxes U and V) get clustered into buckets Wand Z (see also other buckets). (c, d) For high-width, we observe significantly tighter clusteringcompared to the low-width case. Results are reported as mean of three runs.
Figure 5:	Impact of varying width: (a) Width multiplier, wm = 1, (b) wm = 2, and (c) wm = 3.
Figure 6: Similar results are obtained for the CIFAR-100 dataset (wm = 2). (a) Models in boxW have significantly different #parameters but achieve similar accuracy. (b) These models getclustered into buckets Y and Z. (c) The R2 value for fitting a linear regression model is 0.84 whichshows that NN-Mass is a good predictor of test accuracy. Results are reported as mean of three runs.
Figure 7: Not all input channels contribute equally to all output channels. (a) Training accuracyfor three cases: (i) Set αj's to constant probabilities by initializing the input-to-output contributionweights to zero at every layer and then taking the softmax. This results in constant probabilities ascontributions from input to output channel at each layer (N is the number of input channels). (ii) Atraditional CNN case where all channel-wise convolutions are directly added (i.e., contributions fromall inputs to all outputs (αij ’s) are all ones). (iii) Channel-wise contributions are fixed to randomprobabilities, i.e., αij ’s are random. (b) Test accuracy for the above three cases demonstrates thatthe unequal contributions from input-to-output channels achieves significantly higher accuracy.
Figure 8: An example CNN to calculate NN-Density and NN-Mass. Not all links are shown inthe main figure for simplicity. The inset shows the contribution from all long-range and short-rangelinks: The feature maps for randomly selected channels are concatenated at the current layer (similarto Densenets [Huang et al. (2017)]). At each layer in a given cell, the maximum number of channelsthat can contribute long-range links is given by tc .
Figure 9: (a) Small-World Networks in traditional network science are modeled as a superposition ofa lattice network (G) and a random network R [Watts & Strogatz (1998a); Newman & Watts (1999);Monasson (1999)]. (b) A CNN with both short-range and long-range links can be similarly modeledas a random network superimposed on a lattice network. Not all links are shown for simplicity.
Figure 10: NN-Mass as an indicator of generalization performance compared to parameter counting.
Figure 11: Linear modeled trained in Fig. 5(b) is used to predict the test accuracy of completelynew architectures. The resulting R2 = 0.79 is still high and is comparable to the trainingR2 = 0.84. The linear model was trained on the test accuracies and NN-Mass of models with{31, 40, 49, 64} layers, and densities varying as {0.10, 0.15, 0.20, 0.25, 0.30}. To create the test-ing set, we trained completely new models with {28, 43, 52, 58} layers, and densities varying as{0.125, 0.175, 0.225, 0.275, 0.325}.
