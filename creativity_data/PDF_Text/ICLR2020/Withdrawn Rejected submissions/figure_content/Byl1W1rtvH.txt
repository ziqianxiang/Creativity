Figure 1: (a) The generative model of a three-hidden-layer rGBN-RNN, where the bottom part is the deeprecurrent topic model (rGBN), document contexts of consecutive sentences are used as observed data, and upperis the language model. (b) Overview of the language model component, where input xj,t denotes the tth word injth sentence of a document, xj,t = yj,t-1, hlj,t is the hidden state of the stacked RNN at time step t, and Î¸jl isthe topic weight vector of sentence j at layer l. (c) The overall architecture of the proposed model, including thedecoder (rGBN and language model) and encoder (variational recurrent inference), where the red arrows denotethe inference of latent topic weight vectors, black ones the data generation.
Figure 2: BLEU scores of different methods for BNC. x-axis denotes test-BLEU, y-axis self-BLEU, and abetter BLEU score would fall within the lower right corner.
Figure 3: Visualization of the L2-norm of the hidden states of the language model of rGBN-RNN, shown in thetop-row, and that of GBN-RNN, shown in the bottom row.
Figure 4:	Topics and their temporal trajectories inferred by a three-hidden-layer rGBN-RNN from theAPNEWS dataset, and the generated sentences under topic guidance (best viewed in color). Top words of eachtopic at layer 3,2,1 are shown in orange, yellow and blue boxes respectively, and each sentence is shown in adotted line box labeled with the corresponding topic index. Sentences generated with a combination of topics indifferent layers are at the bottom of the figure.
Figure 5:	Examples of generated sentences and paragraph conditioned on a document from APNEWS (greendenotes novel words, blue the key words in document and generated sentences.)in each sentence of the input paragraph. It is clear that both the proposed GBN-RNN and rGBN-RNNcan successfully capture the key textual information of the input paragraph, and generate diverserealistic sentences. Interestingly, the rGBN-RNN can generate semantically coherent paragraphs,incorporating contextual information both within and beyond the sentences. Note that with thetopics that extract the document-level word cooccurrence patterns, our proposed models can generatesemantically-meaningful words, which may not exist in the original document.
Figure 6:	Topics and their temporal trajectories inferred by a three-hidden-layer rGBN-RNN from the IMDBdataset, and the generated sentences under topic guidance (best viewed in color). Top words of each topicare shown in orange, yellow and blue box, and each sentence is shown in a dotted line box labeled with thecorresponding topic index. Sentences generated with a combination of topics in different layers are at the bottomof the figure.
Figure 7:	Topics and their temporal trajectories inferred by a three-hidden-layer rGBN-RNN from the BNCdataset, and the generated sentences under topic guidance (best viewed in color). Top words of each topicare shown in orange, yellow and blue box, and each sentence is shown in a dotted line box labeled with thecorresponding topic index. Sentences generated with a combination of topics in different layers are at the bottomof the figure.
Figure 8:	BLEU scores of different methods for IMDB. x-axis denotes test-BLEU, and y-axis self-BLEU. Leftpanel is BLEU-3 and right is BLEU-4, and a better BLEU score would fall within the lower right corner, whereblack point represents mean value and circles with different colors denote the elliptical surface of probability ofBLEU in a two-dimensional space.
Figure 9:	BLEU scores of different methods for APNEWS. x-axis denotes test-BLEU, and y-axis self-BLEU.
