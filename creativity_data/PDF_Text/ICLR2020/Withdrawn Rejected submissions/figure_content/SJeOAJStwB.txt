Figure 1:	Magnitude of neurons in the last fully-connected layer of the NetA-Baseline model, atthe first round of Algorithm 1 on CIFAR-10. Brighter ones illustrates having greater magnitude.
Figure 2:	Comparison of test accuracy and parameter divergence with respect to the degree of datanon-IIDness in the training of NetA-Baseline on CIFAR-10. Note that EMD values of Non-IID(5)and Non-IID(2) from the population distribution are 1.0 and 1.6, respectively.
Figure 3:	Inordinate magnitude of parameter divergence in the CIFAR-10 training with NetA-Deepest (left), weight decay factor of 0.0005 (middle), and Dropout (right), respectively, underthe non-IID data setting. Dotted and solid lines indicate the results under IID and Non-IID(2) set-ting, respectively. WD: weight decay; DO: dropout.
Figure 4:	Steep fall phenomenon in the CIFAR-10 training with Adam-WB (left), NetC-Baseline(middle), and Batch Normalization (right), respectively, under the non-IID data setting. Dotted andsolid lines indicate the results under IID and Non-IID(2) setting, respectively. BN: Batch Normal-ization; BRN: Batch Renormalization.
Figure 5:	Loss surface of the global model parameters under Non-IID(2) setting. The red dashedboxes indicate the same level of loss value.
Figure 6: Excessively high training loss in the CIFAR-10 training with NetB-Baseline (left), theweight decay factor of 0.0005 (middle), and data augmentation (right), respectively, under the non-IID data setting. Dotted and solid lines indicate the results under IID and Non-IID(2) setting, respec-tively. WD: weight decay; DA: data augmentation. Note that training loss values of each learnerwere calculated on their local data before each synchronization.
Figure 7: The network configurations.
Figure 8:	Comparison of test accuracy and parameter divergence under Non-IID(2) setting withrespect to FedProx factor μ in the training of NetA-BaSeIine on CIFAR-10 (Optimizer: NMom-A).
Figure 9:	Behavior comparison with respect to the degree of data non-IIDness in the training ofNetA-Baseline on CIFAR-10 (Optimizer: NMom-A).
Figure 10:	Behavior comparison between NMom-WB and Nmom-A in the training ofNetA-Baseline on CIFAR-10. Dotted and solid lines indicate the results under IID and Non-IID(2)setting, respectively.
Figure 11:	Behavior comparison between Adam-WB and Adam-A in the training of NetA-Baselineon CIFAR-10. Dotted and solid lines indicate the results under IID and Non-IID(2) setting, respec-tively.
Figure 12:	Behavior comparison of the NetA models with respect to network depth on CIFAR-10(Optimizer: NMom-A). Dotted and solid lines indicate the results under IID and Non-IID(2) setting,respectively.
Figure 13:	Behavior comparison of the NetA models with respect to network width on CIFAR-10(Optimizer: pure SGD). Dotted and solid lines indicate the results under IID and Non-IID(2) setting,respectively.
Figure 14:	Behavior comparison of the NetB models with respect to network width on CIFAR-10(Optimizer: pure SGD). Dotted and solid lines indicate the results under IID and Non-IID(2) setting,respectively.
Figure 15:	Behavior comparison of the NetC models with respect to network width on CIFAR-10(Optimizer: pure SGD). Dotted and solid lines indicate the results under IID and Non-IID(2) setting,respectively.
Figure 16: Behavior comparison of the NetA models with respect to network width on CIFAR-10(Optimizer: NMom-A). Dotted and solid lines indicate the results under IID and Non-IID(2) setting,respectively.
Figure 17:	Behavior comparison of the NetB models with respect to network width on CIFAR-10(Optimizer: NMom-A). Dotted and solid lines indicate the results under IID and Non-IID(2) setting,respectively.
Figure 18:	Behavior comparison of the NetC models with respect to network width on CIFAR-10(Optimizer: NMom-A). Dotted and solid lines indicate the results under IID and Non-IID(2) setting,respectively.
Figure 19:	Behavior comparison of the NetA models with respect to network width on CIFAR-10(Optimizer: Adam-A). Dotted and solid lines indicate the results under IID and Non-IID(2) setting,respectively.
Figure 20:	Behavior comparison of the NetB models with respect to network width on CIFAR-10(Optimizer: Adam-A). Dotted and solid lines indicate the results under IID and Non-IID(2) setting,respectively.
Figure 21: Behavior comparison of the NetC models with respect to network width on CIFAR-10(Optimizer: Adam-A). Dotted and solid lines indicate the results under IID and Non-IID(2) setting,respectively.
Figure 22:	Behavior comparison with respect to weight decay levels in the training of NetA-Baseline on CIFAR-10 (Optimizer: pure SGD). Dotted and solid lines indicate the results underIID and Non-IID(2) setting, respectively.
Figure 23:	Behavior comparison with respect to weight decay levels in the training of NetA-Baseline on CIFAR-10 (Optimizer: NMom-A). Dotted and solid lines indicate the results underIID and Non-IID(2) setting, respectively.
Figure 24:	Behavior comparison with respect to weight decay levels in the training of NetA-Baseline on CIFAR-10 (Optimizer: Adam-A). Dotted and solid lines indicate the results underIID and Non-IID(2) setting, respectively.
Figure 25:	Behavior comparison under Non-IID(2) setting with respect to FedProx factor μ in thetraining of NetA-Baseline on CIFAR-10 (Optimizer: NMom-A).
Figure 26:	Behavior comparison with/without Batch Normalization/Renormalization in the trainingof NetA-Baseline on CIFAR-10 (Optimizer: pure SGD). Dotted and solid lines indicate the resultsunder IID and Non-IID(2) setting, respectively.
Figure 27:	Behavior comparison with/without Batch Normalization/Renormalization in the trainingof NetA-Baseline on CIFAR-10 (Optimizer: NMom-A). Dotted and solid lines indicate the resultsunder IID and Non-IID(2) setting, respectively.
Figure 28: Behavior comparison with/without Batch Normalization/Renormalization in the trainingof NetA-Baseline on CIFAR-10 (Optimizer: Adam-A). Dotted and solid lines indicate the resultsunder IID and Non-IID(2) setting, respectively.
Figure 29: Behavior comparison with/without data augmentation in the training of NetA-Baselineon CIFAR-10 (Optimizer: pure SGD). Dotted and solid lines indicate the results under IID andNon-IID(2) setting, respectively.
Figure 30: Behavior comparison with/without data augmentation in the training of NetA-Baselineon CIFAR-10 (Optimizer: NMom-A). Dotted and solid lines indicate the results under IID and Non-IID(2) setting, respectively.
Figure 31: Behavior comparison with/without data augmentation in the training of NetA-Baselineon CIFAR-10 (Optimizer: Adam-A). Dotted and solid lines indicate the results under IID and Non-IID(2) setting, respectively.
Figure 32:	Behavior comparison with/without Dropout in the training of NetA-Baseline on CIFAR-10 (Optimizer: pure SGD). Dotted and solid lines indicate the results under IID and Non-IID(2)setting, respectively.
Figure 33:	Behavior comparison with/without Dropout in the training of NetA-Baseline on CIFAR-10 (Optimizer: NMom-A). Dotted and solid lines indicate the results under IID and Non-IID(2)setting, respectively.
Figure 34: Behavior comparison with/without Dropout in the training of NetA-Baseline on CIFAR-10 (Optimizer: Adam-A). Dotted and solid lines indicate the results under IID and Non-IID(2)setting, respectively.
Figure 35: Loss surface of the global model parameters with Adam-WB and with Adam-A underNon-IID(2) setting.
Figure 36: Loss surface of the global model parameters with/without Batch Normalization underNon-IID(2) setting.
