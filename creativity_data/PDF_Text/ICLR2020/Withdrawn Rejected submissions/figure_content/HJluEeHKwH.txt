Figure 1: We trained an energy-based model with unrolled gradient descent and DCEM for a 1Dregression task with the target function shown in black. Each method unrolls through 10 optimizersteps. The contour surfaces show the (normalized/log-scaled) energy surfaces, highlighting thatunrolled gradient descent models can overfit to the number of gradient steps. The lighter colorsshow areas of lower energy.
Figure 2: Visualization of the samples that CEM and DCEM generate to solve the cartpole taskstarting from the same initial system state. The plots starting at the top-left show that CEM initiallystarts with no temporal knowledge over the control space whereas embedded DCEM’s latent spacegenerates a more feasible distribution over control sequences to consider in each iteration. Embed-ded DCEM uses an order of magnitude less samples and is able to generate a better solution to thecontrol problem. The contours on the bottom show the controller’s cost surface C(z) from eq. (8)for the initial state — the lighter colors show regions with lower costs.
Figure 4: We evaluated our final models by running 100 episodes each on the cheetah and walkertasks. CEM over the full action space uses 10,000 trajectories for control at each time step whileembedded DCEM samples only 1000 trajectories. DCEM almost recovers the performance of CEMover the full action space and PPO fine-tuning of the model-based components helps bridge the gap.
Figure 3: Our RSSM with action se-quence embeddingsVideos of our trained models are available at:https://sites.google.com/view/diff-cross-entropy-methodDiscussion and limitations. DCEM in the control setting has many potential future directionsto explore and help bring efficiency and policy-based fine-tuning to model-based reinforcementlearning. Much more analysis and experimentation is necessary to achieve this as we faced manyissues getting the model-based cheetah and walker tasks to work that did not arise in the ground-truthcartpole task. We discuss this more in app. E. We also did not focus on the sample complexity of ouralgorithms getting these proof-of-concept experiments working. We also note that other reasonablebaselines on this task could involve distilling the controller into a model-free policy and then doingsearch on top of that policy, as done in POPLIN (Wang & Ba, 2019).
Figure 5: Left: Convergence of DCEM and unrolled GD on the regression task. Right:Ablation showing what happens when DCEM and unrolled GD are trained for 10 inner steps andthen a different number of steps is used at test-time. We trained three seeds for each model and theshaded regions show the 95% confidence interval.
Figure 6: Visualization of the predictions made by ablating the number of inner loop iterations forunrolled GD and DCEM. The ground-truth regression target is shown in black.
Figure 7:	Impact of the activation function on the initial decoder valuesCMore details: Decoder initializations and activationFUNCTIONSWe have found the decoder to be influenced by the activation function that’s used with it and havefound the ELU (Clevert et al., 2015) to perform the best. fig. 7 conveys some intuition behind thischoice. We randomly initialize a neural network u = fθ(z) with no biases, where θ = {Wi}i forevery layer weight Wi, and then scale the weights with αθ. We then sample Z 〜N(0,I), passthem through fαθ, and plot the outputs. The ReLU (Nair & Hinton, 2010) induces an extremelybiased distribution which is seen more prevalently as α grows that is not as present when using theELU or hyperbolic tangent since they are almost linear around zero. Despite the reasonable lookinginitializations for the hyperbolic tangent, we found that it does not perform as well in practice inour experiments. We found that the initial scale α of the decoder’s parameters is also important forlearning because of the network is not initially producing samples that cover the full output space asshown with α = 1, it seems hard for it to learn how to expand to cover the full output space.
Figure 8:	Training and validation loss convergence for the cartpole task. The dashed horizontalline shows the loss induced by an expert controller. Larger latent spaces seem harder to learn andas DCEM becomes less differentiable, the embedding is more difficult to learn. The shaded regionsshow the 95% confidence interval around three trials.
Figure 9: Improvement factor on the ground-truth cartpole task from embedding the action spacewith DCEM compared to running CEM on the full action space, showing that DCEM is able torecover the full performance. We use the DCEM model that achieves the best validation loss. Theerror lines show the 95% confidence interval around three trials.
Figure 10: Learned DCEM reward surfaces for the cartpole task. Each row shows a different initialstate of the system. We can see that as the temperature decreases, the latent representation can stillcapture near-optimal values, but they are in much narrower regions of the latent space than whenτ = 1.
Figure 11: Phase 1: The two base proprioceptiVe PlaNet training runs that use CEM oVer the fullaction space. The eValuation loss uses 10 episodes and we show a rolling aVerage of the trainingloss.
Figure 12: Phase 2: The training runs of learning an embedded DCEM controller with onlineupdates. The eValuation loss uses 10 episodes and we show a rolling aVerage of the training loss.
Figure 13: Phase 3: The training run of PPO-fine-tuning into the model-based components — weonly use the PPO updates to tune these components and do optimize for the likelihood in this phase.
Figure 14: Visualization of the DCEM iterates on the cheetah to solve a single control problemstarting from a random initial system state The rows show iterates 1, 5, 7, 10 from the top to bottom.
Figure 15: Visualization of the DCEM iterates on the walker to solve a single control problemstarting from a random initial system state. The rows show iterates 1, 5, 7, 10 from the top tobottom.
