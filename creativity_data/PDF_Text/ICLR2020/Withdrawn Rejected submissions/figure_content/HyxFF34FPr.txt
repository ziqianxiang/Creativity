Figure 1: The anchor-based object detection frameworks need to (a) design anchors according tothe ground-truth box distributions; (b) match anchors with ground-truth boxes to generate trainingtarget (anchor classification and refinement); and (c) utilize the target generated by (b) for training.
Figure 2: FoveaBox object detector. For each output spacial position that potentially presents anobject, FoveaBox directly predicts the confidences for all target categories and the bounding box.
Figure 3: Anchor-based object detection v.s. FoveaBox object detection. left: The anchor-basedmethod uniformly places A (A = 3 in this example) anchors on each output spacial position, and uti-lizes IoU to define the positive/negative anchors; right: FoveaBox directly define positive/negativesamples for each output spacial position by ground-truth boxes, and predicts the box boundariesfrom the corresponding position.
Figure 4: On each FPN feature level, FoveaBox attaches two subnetworks, one for classifying thecorresponding cells and one for predict the (x1, y1, x2, y2) of ground-truth object box. Right isthe score output map with their corresponding predicted boxes before feeding into non-maximumsuppression (NMS). The score probability in each position is denoted by the color density. Moreexamples are shown in Fig.5.
Figure 5:	FoveaBox results on the COCO minival set. These results are based on ResNet-101,achieving a single model box AP of 38.6. For each pair, left is the detection results with boundingbox, category, and confidence. Right is the score output map with their corresponding boundingboxes before feeding into non-maximum suppression (NMS). The score probability in each positionis denoted by the color density.
Figure 6:	AP difference of FoveaNet and RetinaNet on COCO dataset. Both models use ResNet-FPN-50 as backbone and 800 input scales.
Figure 7: Feature alignment process.
