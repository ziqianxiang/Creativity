Figure 1: Average performance (over 5 seeds) of Algorithm 3 (—), and Algorithm 4 (—), under theNR-MDP setting with δ = 0.1. The evaluation is performed without adversarial perturbations, on arange of mass values not encountered during training.
Figure 2: Average performance (over 5 seeds) of Algorithm 3 (—), and Algorithm 4 (—), under theNR-MDP setting with δ = 0.1. The evaluation is performed on a range of noise probability andmass values not encountered during training.
Figure 3: Average performance (over 5 seeds) of Algorithm 3 (—), and Algorithm 4 (—), under theNR-MDP setting with δ = 0. The evaluation is performed without adversarial perturbations, on arange of mass values not encountered during training.
Figure 4: Average performance (over 5 seeds) of Algorithm 3 (—), and Algorithm 4 (—), under theNR-MDP setting with δ = 0. The evaluation is performed on a range of noise probability and massvalues not encountered during training.
Figure 5: The instantaneous reward obtained by executing πθhide , and πθhide in different settings.
Figure 6: Average performance (of the best performing seed) of Algorithm 3 (—), and Algorithm 4(—), under the NR-MDP setting with δ = 0.1. The evaluation is performed without adversarialperturbations, on a range of mass values not encountered during training.
Figure 7: Average performance (of the best performing seed) of Algorithm 3 (—), and Algorithm 4(—), under the NR-MDP setting with δ = 0. The evaluation is performed without adversarialperturbations, on a range of mass values not encountered during training.
