Figure 1: Loss residual and error rate of IG, SVRG, SAGA for Logistic Regression on Covtype dataset with 581,012 data points. We compare performance of CRAIG (10% selected subset) vs. entiredata set. We achieve the average speedup of 7x for achieving similar loss residual and error rateacross the three optimization methods.
Figure 2: (top) Speedup of CRAIG applied to to get similar loss residual as IG after 50 epoch, and(bottom) distance to the optimal solution vs various subset sizes for (left) Covtype, (middle) SensIT,and (right) Ijcnn1. Smaller subsets provides larger speedups, but may converge farther away from theoptimal solution.
Figure 3: Loss residual of CRAIG for Logistic Regression on (left) Covtype, (middle) SensIT, and(right) Ijcnn, where we process the points of the subset according to the ordering provided by CRAIGvs three random permutations of the same subset. Notice convergence is significantly faster when weprocess the points in CRAIG order.
Figure 4: Training loss and test accuracy of CRAIG applied to (a) SGD on MNIST with a 1-layerneural network, and (b) SGD, Adam, Adagrad, NAG, on CIFAR-10 with ResNet-56. CRAIGprovides 2x to 3x speedup and a better generalization performance.
Figure 5: Loss residual vs. time for IG on 10 subsets of size 10%, 20%, 30%, ..., 100% selected byCRAIG, random subsets, and random subsets weighted by |V |/|S |. Stepsizes are tuned for everysubset separately, by preferring smaller training loss from a large number of parameter combinationsfor two types of learning scheduling: exponential decay η(t) = noabt/nc with parameters no and a toadjust and t-inverse η(t) = η0(1 + bbt/nc)-1 with η0 and b to adjust.
Figure 6: Training loss and test accuracy for SGD applied to full MNIST vs. subsets of size 60%selected by CRAIG and random subsets of size 60%. Both the random subsets and the subsets foundby CRAIG change at the beginning of every epoch.
