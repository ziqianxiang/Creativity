Figure 1: (a). Illustration of paired, aligned vs unpaired, unaligned demonstrations in the alignment taskset Dx,y (b). Alignment: we learn state, action maps f,g between the self (x) and expert (y) domain fromunpaired, unaligned demonstrations by minimizing a distribution matching loss and an imitation loss. (c)Adaptation: adapt the expert domain policy πy,τ or demonstrations to obtain a self domain policy ∏x,τa simple training algorithm to learn MDP reductions. In section 6, we experimentally evaluate GAMA andfind that meaningful state correspondences between various domains are learned from unpaired, unaligneddemonstrations. We then compare the CDIL performance of GAMA against several baselines in bothembodiment and viewpoint mismatch scenarios and show the effectiveness of our approach.
Figure 2: Example MDP reduction fromMx toMy. φ, ψ are state and action mapsgraph with colored edges in which the nodes correspond to states visited by an optimal policy, andthe colored edges correspond to actions taken. An MDP reduction from Mx to My homomorphs theexecution graph of an optimal policy in Mx to a execution graph of an optimal policy in My . Figure2 shows an example of a valid reduction from Mx to My: states 1, 2 in Sx are mapped (merged) tostate a in Sy and the blue, green actions in Ax are mapped to the brown action in Ay . Intuitively, ifMx ≥φ,ψ My, then (φ, ψ) compresses Mx by merging all optimal state, action pairs that have identicaldynamics properties.
Figure 3: Illustration ofMDP alignment problemfor different viewpoints of an agent performing a task are MDP permutations since there is a one-to-onecorrespondence between state, actions at same timestep in the execution trace of an optimal policy.
Figure 4: Visualization of the learned state maps for pen-pen (ToP Left), pen-cart (ToP Right),Snake4-Snake3 (Bottom Left), reach2-reach3 (Bottom Right). GAMA is able to recover MDP reduc-tions (Top Left/Right) and finds interpretable correspondences between domains that are not perfectlyalignable, yet intuitively share structure (Bottom Left/Right). Baselines fail in most casesTable 1: Quantitative evaluation of learned state maps. GAMA reliably finds MDP permutations whilebaselines incur 10× larger deviation loss from the ground truth permutation map. Error bars/regions showthe standard deviation over 5 runs.
Figure 5: CDIL performance. Alignment complexity (Left), Adaptation complexity (Middle), andtransferability (Right) for W2C/R2W on the top/bottom rows, respectively. GAMA outperforms baselinesin all metrics. Notably, adaptation complexity of GAMA is close to that of the self-demo baseline. Errorbars/regions show the standard deviation over 5 runs.
