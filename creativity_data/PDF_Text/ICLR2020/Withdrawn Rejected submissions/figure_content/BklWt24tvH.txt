Figure 1: Topology of different communication structures and LSC falls into the hierarchical one.
Figure 2: Procedure for dynamically establishing structured communication network. Left: Eachagent determines its communication importance weight based on partial local observation. For in-stance, the agent “G” finds the target (red square), then it will be possible to get a higher weight “4”and become the central. Right: The importance weight generation step and network constructionstep will be repeated iteratively. After communication and action procedures, agents will generatetheir new communication importance weights, and determine to keep or change their roles respec-tively. Further, the structured communication network will be re-established.
Figure 3: GNN-based communication extraction procedure. Each node denotes an agent. The edgeembedding can be considered as the communication message. The network learning procedure prop-erly fits the communication procedure, and effectively learn valuable messages involving the globalnetwork structure and agents relationship. Left: Low-level normal agents transfer their local valu-able embeddings to the associated central agents. Middle: High-level central agents communicatewith each other to gain a sense of global perception. Right: All central agents broadcast embeddinginformation to their normal agents to form global cooperation.
Figure 4: Algorithm framework of LSC with Structured Communication Network Module andCommunication-based Policy Module, where si, oi, ai and wi denote state (global perception),observation, action and importance weight of agent i. The former module uses partial observationto establish the communication structure. The latter employs GNN-based communication and Q-Network to extract communication content and produces collaboration policies respectively basedon established communication structure.
Figure 5: Reward of algorithms and message pattern visualization in the MAgent environment.
Figure 6: Behavior illustration. The first row shows two typical behavior by LSC. In the second row,the top and bottom plot denote the early state and the near to final battle state, respectively.
Figure 7: Reward curves on StarCraft25	Conclusion and Future WorkIn this paper, a novel learning structured communication (LSC) algorithm is proposed for multi-agent reinforcement learning. The hierarchical structure is self-learned by cluster based routingprotocol. The communication message representation is naturally embedded and extracted via agraph neural network. Experiments in large-scale games (MAgent and StarCraft2) demonstrated thatour LSC can outperform existing learning-to-communicate algorithms with better communicationefficiency, cooperation capability, and scalability. In the future, we will improve LSC by consideringsome practical constraints, such as communication bandwidth and delay.
Figure 8: Battle scenarios of MAgent and StarCraft2.
Figure 9: Discussions on weight generator and group radius.
