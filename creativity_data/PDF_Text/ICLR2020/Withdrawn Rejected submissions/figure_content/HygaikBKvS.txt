Figure 1: Sharing experience between agents leads to more efficient hyper-parameter sweeps on 57Atari games. Prior art results are presented as horizontal lines (with scores cited from Gruslys et al.
Figure 2: Left: Learning entirely off-policy from experience replay fails, while combining on-policydata with experience replay leads to improved data efficiency: We present sweeps on DMLab-30 withexperience replays of 10M capacity. A ratio of 87.5% implies that there are 7 replayed transitions inthe batch for each online transition. Furthermore we consider an agent identical to “LASER 87.5%replay” which however draws all samples from replay. Its batch thus does not contain any online dataand we observe a significant performance decrease (see Proposition 2 and 3). The shading representsthe point-wise best and worst replica among 3 repetitions. The solid line is the mean. Right: Theeffect of capacity in experience replay with 87.5% replay data per batch on sweeps on DMLab-30.
Figure 3: Left: Naively sharing experience between distinct agents in a hyper-parameter sweep fails(green) and is worse than the no-replay baseline (blue). The proposed trust region estimator mitigatesthe issue (red). Right: Combining population based training with trust region estimation improvesperformance further. All replay experiments use a capacity of 10 million observations and 87.5%replay data per batch.
Figure 4: Left: Increasing the V-trace clipping constant P does not enable shared experience replay.
