Figure 1: FGSM (left) and PGD (right) attacks on the CIFAR-10 image classification dataset forvarious values of with and without adversarial training. Notice that our method alone surpassesthe strong adversarial training defence. When combined together the robustness is increased evenfurther. Results are averaged over 10 runs.
Figure 2: Contour plot of the loss surface of the real model evaluated on the l∞ neighbourhood ofa CIFAR-10 image for θ = {1, 0.95, 0.9, 0.8} (from left to right). The direction of the gradient wascomputed for θ = 1 and a random, orthogonal direction. These were kept fixed for the subsequentplots θ = 0.95, 0.9 and 0.8. The red circle denotes the = 32 neighbourhood.
Figure 3: Contour plot of the loss surface of the real model evaluated on the l∞ neighbourhood ofa CIFAR-10 image for θ = {1, 0.95, 0.9, 0.8} (from left to right). For each plot (i.e. each value ofθ), we recomputed the direction of the gradient. The red circle denotes the = 32 neighbourhood.
Figure 4: Contour plot of the loss surface of our model, adversarially trained model for variousvalues of θ evaluated on the l∞ neighbourhood of an unseen CIFAR-10 image. The same directionwas used for all three plots. The red circle denotes the = 32 neighbourhood.
