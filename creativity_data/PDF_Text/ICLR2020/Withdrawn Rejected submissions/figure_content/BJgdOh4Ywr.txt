Figure 1: A pair of sequences from the expert (blue) and agent (red). The spatial distance is shown via dotedblack lines and the temporal distance for d(oe, o7a) is visulized in green. On the right the flow of control for thelearning system is shown.
Figure 2: Siamese network structure. The convolutional portion of the network includes 2 convolution layersof 8 filters with size 6 × 6 and stride 2 × 2, 16 filters of size 4 × 4 and stride 2 × 2. The features are thenflattened and followed by two dense layers of 256 and 64 units. The majority of the network uses ReLUactivations except for the last layer that uses a sigmoid activation. Dropout is used between convolutionallayers. The RNN-based model uses a LSTM layer with 128 hidden units, followed by a dense layer of 64 units.
Figure 3: Rasterized frames of the agent’s motion after training on humanoid3d walking (row 1,2),running (row 3-5), zombie (row 6,7) and jumping(row 8-10). The multi-coloured agent is a renderingof the imitation video. A video of these results is available here: https://youtu.be/s1KiIrV1YY4πE . Overlaps in specific areas of the space for similar classes across learned π and expert πE dataindicate a well-formed distance metric that does not sperate expert and agent examples. There isalso a separation between motion classes in the data, and the cyclic nature of the walking cycle isvisible.
Figure 4: Baseline comparisons between our sequence-based method, GAIL and TCN (4a) on thehumanoid2d environment. Two additional baseline comparison between VIRL and TCN in 4b. In4cthe benefit of combing spatial and temporal distances is shown. In these plots, the large solid linesare the average performance of a collection of policy training simulations. The dotted lines of thesame colour are the specific performance values for each policy training run.
Figure 5: Ablation analysis of VIRL. We find that training RL policies is sensitive to the size anddistribution of rewards. A few modifications assist in the siamese network’s ability to computeuseful distances. Including VAE and AE losses to assist in representation learning. The addition ofmulti-task training data is also important for learning better policies.
Figure 6:	Training losses for the siamese distance metric. Higher is better as it indicates the distancebetween sequences from the same class are closer.
Figure 7:	Ablation analysis of VIRL. We find that training RL policies is sensitive to the size anddistribution of rewards. The siamese network benefits from several training adjustments that makeit more suitable for RL.
Figure 8: Additional dblation analysis of VIRL on humanoid3d.
Figure 9: RL algorithm comparison on humanoid2d environment.
Figure 10: Still frame shots from a policy trained in the humanoid2d environment.
