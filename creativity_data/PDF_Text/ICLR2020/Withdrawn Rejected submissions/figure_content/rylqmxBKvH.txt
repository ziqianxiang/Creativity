Figure 1: Schema of our model. Generator G takes a sequence y and outputs an inpainted sequenceX; measurement process F takes the inpainted sequence then outputs fake observations y.
Figure 2: Samples from test sets. SST data (a) are masked with Cloud, natural video datasets (b,c,d)are masked with Remove-Pixel and Raindrops. Sequences are accelerated 3 times to make movementsmore visible. Each sample from top row to bottom: observed y, and recovered x.
Figure 3: Supervised variants, used as baseline in Section 3.315Under review as a conference paper at ICLR 2020Figure 5: Samples from our model for SST. From top to bottom: Cloud at LWP threshold 55, 60, 65,70, 75, and 80 g/m2.
Figure 5: Samples from our model for SST. From top to bottom: Cloud at LWP threshold 55, 60, 65,70, 75, and 80 g/m2.
Figure 6: Samples from our model for FaceForensics++. From top to bottom: Raindrops, Remove-Pixel, and Moving-Vertical-Bar.
Figure 7: Samples from our model for KTH. From top to bottom: Raindrops, Remove-Pixel, andMoving-Vertical-Bar.
Figure 8: Samples from our model for BAIR. From top to bottom: Raindrops, Remove-Pixel, andMoving-Vertical-Bar.
Figure 9: Samples from DINEOF (Alvera-Azcdrate, 2011) for SST, FaceForensics++, KTH andBAIR.
Figure 10: Samples from Newson et al. (2014) for FaceForensics++, KTH, and BAIR. Only withMoving-Vertical-Bar. Samples for other measurements cannot be calculated in reasonable time.
Figure 11: Samples from supervised variants.
Figure 12: Samples for ablation study.
Figure 13: Comparison of samples from our model and two state-of-the-art unsupervised imageinpainting methods. Note that the temporal coherence is broken for Ulyanov et al. (2017) due to theinstant appearance and disappearance of facial parts.
