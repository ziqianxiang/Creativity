Figure 1: ICNN: Model ArchitectureInput-conditioned filter generation: In theory, this generator module can be implemented as oneof any deep differentiable functions. However, due to the remarkable success of convolutional neuralnetworks in computer vision related tasks Deng et al. (2009); Simonyan & Zisserman (2014), wedecided to make use of convolution based operations to implement this module. Given an inputimage X, it is first passed to a Convolution layer followed by a Max-pool layer to decrease thespatial size of the generated feature map. This feature map is then passed to a Deconvolutionallayer, which performs fractionally-strided convolutions [17], to generate a distinctive set of filters.
Figure 2: (a) Proposed convolution module (b) Detailed architecture of input-conditioned filter gen-erationselected a single image corresponding to every digit. This method can be easily extended to includemultiple representative images of a single class in the training process. Further, to validate theeffectiveness of the decoder network, we selected a random set of images from the test set andobserved that the constructed images have less transformation when compared with the input image.
Figure 3: (a) Input test image (b) Constructed image using the Decoder network4	Experiments and DiscussionWe analyzed the performance of our proposed network and presented the results of three computervision datasets. MNIST dataset LeCun & Cortes (2010) variations designed to test rotation andscaling variations have been used for generating experimental results and are compared with state-of-the-art results presented by Zhang et al. (2018). We also make use of artificially rotated and scaledCIFAR-10 dataset for generating experimental results. To validate the ability to handle differentvariations, we kept our network topology same for different datasets. We have the same number ofconvolutions and dense layers as compared to Laptev et al. (2016); Zhang et al. (2018) in order tovalidate the performance of our proposed convolution framework. For all the experiments, 10% of5Under review as a conference paper at ICLR 2020the training data is separated and used for validation. Also, all the reported results were aggregatedafter multiple runs.
Figure 4: t-SNE visualization of filter weightsTable 1: Error on various rotated data corpus (%)Corpus	Approach	Channel=4	Channel=7	Channel=8	Channel=13	Channel=24	TI-POOLING	2.47	-	1.88	-	1.61MNIST-rot-12k	MINTIN	1.76	-	1.59	-	1.57	ICNN	1.35	-	1.12	-	0.98	TI-POOLING	8.7	-	8.2	-	7.3CIFAR-10-rot-12k	MINTIN	7.4	-	6.8	-	6.4	ICNN	5.3	-	4.5	-	4.3	TI-POOLING	-	1.44	-	1.46	-Half-rotated MNIST	MINTIN	-	1.32	-	1.23	-	ICNN	-	1.18	-	1.12	-	TI-POOLING	-	8.5	-	7.9	-Half-rotated CIFAR-10	MINTIN	-	7.2	-	7.1	-	ICNN	-	5.9	-	5.3	-5	ConclusionWe introduced a novel input-conditioned convolution filter framework, which unlike traditionalCNNs, generates a different set of input-aware convolution filters conditioned on an input imageinstance. We also proposed a decoder network to mitigate the transformation present in the inputimages with the help of a set of pre-defined representative images of the input classes. We employed
