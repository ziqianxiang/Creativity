Figure 1: Given an image from domain A (zebras), we extract its deep features using a networkpre-trained for classification, specifically VGG-19 pre-trained on ImageNet, and translate them intodeep features of domain B (giraffes). We first translate high level semantics encoded in conv_5_1of the zebra to those of a giraffe, represented by the inner images. Then, we use a cascade of deep-to-shallow adversarially trained translators, one for each deep feature layer, to translate shallower layers,i.e. conv_4_1 (middle sized zebra and giraffe) and then conv_3_1 (outer images) . The presentedimages were obtained by training feature inversion networks, from deep features to image space.
Figure 2: Translation architecture. We translate between domains A and B starting from the deepestfeature maps A5 and B5, which encode the highest level semantic content of the images. Translationproceeds from deeper to shallower feature maps until reaching the image itself. The feature maps areextracted by feed forwarding every image through the pre-trained VGG-19 network and sampling fiveof its layers. Every layer,s translation is learned individually, conditioned on the translation result ofthe next deeper layer (except the deepest layer, whose translation is unconditional).
Figure 3: Translation of the top left image at test time. The input image (left) is fed forward throughVGG-19 layers, as indicated by the right purple arrows. Then, starting from the deepest layer a5 wetranslate each layer. The final result is obtained from the shallowest layer using feature inversion.
Figure 4: Translation of layer i is conditioned on the previously translated layer i + 1. The twotranslators GiA and GiB are trained simultaneously (see left figure), where i + 1, . . . , 5 translators arefixed. On the right we show the schematic architecture of GiB which has two inputs: ai ∈ Ai andbi+1. ai is fed-forward through several layers to yield AdaIN parameters which control the generationof bi. Since b has twice the spatial size of bi+ι, We add an upsampling layer marked by ↑.
Figure 5:	Examples of challenging translation results, featuring significant shape deformations.
Figure 7: Translation of different VGG layers,separately. Low level semantics translation failsto deform the geometry of the object.
Figure 6:	Translation of the 5th (deepest) layerwith different loss combinations. Using all threecomponents yields the best result.
Figure 8: Comparison to other image-to-image translation methods. The unpaired translations, fromleft to right, are zebra ÷÷ giraffe, elephant ÷÷ zebra and dog ÷÷ cat, where every translation has fourexamples, two in each direction. While previous translation methods struggle to deform the geometryof the source images, our method is able to preform drastic geometric deformation, while preservingthe poses of the subjects and the overall composition of the image.
Figure 9: Translation with different pre-trained networks. All the networks were pre-trained onImageNet. VGG fine-tuned was further trained to classify between zebra and giraffe. Evidently,using fine-tuned VGG-19 does not assist the translation process. In addition, translation of AlexNetfeatures doesn't preserve the shape of the input image.
Figure 10: Comparison of the deepest latent spaces (5th layer), projected using t-SNE. The latentspace of the source domain is in blue, and the target domain is in red. The distribution of thetranslation results (in cyan) is most similar to that of the target domain when using our method.
Figure 11: Qualitative comparisons. MSCOCO zebra to giraffe.
Figure 13: Qualitative comparisons. MSCOCO giraffe to zebra.
Figure 14: Qualitative comparisons. MSCOCO giraffe to zebra.
Figure 15: Qualitative comparisons. zebra to elephant.
Figure 16: Qualitative comparisons. zebra to elephant.
Figure 17: Qualitative comparisons. MSCOCO elephant to zebra.
Figure 18: Qualitative comparisons. MSCOCO elephant to zebra.
Figure 19: Qualitative comparisons. Kaggle cat to dog.
Figure 20: Qualitative comparisons. Kaggle cat to dog.
Figure 21: Qualitative comparisons. Kaggle dog to cat.
Figure 22: Qualitative comparisons. Kaggle dog to cat.
Figure 23: Translation results from cats to dogs (faces).
Figure 24: Translation results from dogs to cats (faces).
Figure 25: Coarse to fine translation of zebra to giraffe. Two different examples are shown in eachrow. The original image (left) is translated by the deepest translator (second left) and then in coarseto fine manner, shallower layers are translated (second right and most right).
Figure 26: Coarse to fine translation of giraffe to zebra. Two different examples are shown in eachrow. The original image (left) is translated by the deepest translator (second left) and then in coarseto fine manner, shallower layers are translated (second right and most right).
Figure 27: Nearest neighbor comparison to our result for zebra to giraffe translation. The NNs werefound by exhaustive search on all the giraffe dataset using perceptual metric (LPIPS). The closestgiraffe to the source zebra vary in scale, position and content31Under review as a conference paper at ICLR 2020Figure 28: Nearest neighbor comparison to our result for giraffe to zebra translation. The NNs werefound by exhaustive search on all the giraffe dataset using perceptual metric (LPIPS). The closestzebra to the source giraffe vary in scale, position and content.
Figure 28: Nearest neighbor comparison to our result for giraffe to zebra translation. The NNs werefound by exhaustive search on all the giraffe dataset using perceptual metric (LPIPS). The closestzebra to the source giraffe vary in scale, position and content.
