Figure 1: Flow diagrams of policy learning of (a) an individual policy while interacting with anenvironment, (b) individual policy learning inside internal simulation, (c) collective policy learninginvolving multiple agents’ internal models. Note that for (b) and (c), there is no interaction with theenvironment. For collective world model, we omit the decoding part of V for clear presentation.
Figure 2: (a): Relationship between the temperature parameter τ of MD-RNN in internal simulationand the score received from the real environment. τ governs the stochasticitiy of the virtual environ-ment generation, and there is an optimal value, 1.7 that gives the best score. (b): As τ increases, thesimulated environment becomes more unpredictable and we accordingly need more training epochs.
Figure 3: (a): Original observation (top) with decodings of the biased representations of differentinternal models (occlusion and magnification). (b) - (c): Virtual scores When the policy is trainedinside its own internal simulation (left bars), when using the policy that is trained in other’s internalsimulation (center bars), and When the collective policy is used (right bars). If other’s policy is usedin the virtual environment generated by oneself, the virtual score decreases catastrophically, Whereasthe collective policy gives reasonable scores inside all simulations. (d): "Joint" is the collectivepolicy Without T and A components; When the collective policy is trained Without co-observation, itperforms Worse than individual policies.
Figure 4: (a): Biased representation (occlusion) in VIZDOOM environment. (b) - (e): 4 differentagents’ internal models with the bias setting given in (a). Although trained under exactly the samesetting, internal models (b) and (d) learns reasonable policies inside simulation, while (c) and (d)became delusional. For all four internal models, their virtual scores are similar (blue bar), but theactual score significantly varies between normal and delusional agents. Also, once an agent’s modelis trained, delusional agents do not become normal during the policy training phase and vice versa.
