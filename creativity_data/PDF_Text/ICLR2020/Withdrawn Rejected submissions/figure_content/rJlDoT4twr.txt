Figure 1: (a) Joint continual learning model consisting of a shared probabilistic encoder with vari-ational approximation qθ(Z|x), probabilistic decoder pφ(x, Z) and probabilistic classifier pξ (y, Z).
Figure 2: Trained FashionMNIST OCDVAE evaluated on unknown datasets. All metrics are av-eraged over 100 approximate posterior samples per data point. (Left) Classifier entropy values areinsufficient to separate most of unknown from the known task’s test data. (Center) Reconstructionloss allows for a partial distinction. (Right) Our posterior-based open set recognition considers thelarge majority of unknown data as statistical outliers across a wide range of rejection priors Ωh9Under review as a conference paper at ICLR 2020Table 3: Test accuracies and outlier detection values of the joint OCDVAE and dual model ap-proaches when considering 95 % of known tasks’ validation data is inlying. Percentage of detectedoutliers is reported based on classifier predictive entropy, reconstruction loss and our posterior basedEVT approach, averaged over 100 Z 〜qθ(z|x) samples per data-point respectively.
Figure 3:	2-D latent space visualization for continually learned incremental upper-bound MNIST atthe end of every task increment for CDVAE (a-e) and at the end of training for all task incrementsfor PixCDVAE (f).
Figure 4:	2-D MNIST latent space visualization with different β values for the used WRN architec-ture.
Figure 5:	Generated images for continually learned incremental MNIST at the end of task incrementsfor CDVAE (a-d), OCDVAE (e-h), PixCDVAE (i-l) and PixOCDVAE (m-p).
Figure 6:	Generated images for continually learned incremental FashionMNIST at the end of taskincrements for CDVAE (a-d), OCDVAE (e-h), PixCDVAE (i-l) and PixOCDVAE (m-p).
Figure 7:	Generated images for continually learned incremental AudioMNIST at the end of taskincrements for CDVAE (a-d), OCDVAE (e-h), PixCDVAE (i-l) and PixOCDVAE (m-p).
Figure 8: Illustration of generated images X 〜 pφ,t(x∣z) with Z 〜 p(z) and their correspondingclass c obtained from the classifier Pξ,t(y∣z), together with their open set outlier probability ωct,for an OCDVAE model trained on incremental MNIST, after the last task increment t = T . Fromtop to bottom the identified classes are: 0, 3, 4, 5, 6 and 9. It is observable how the statistical outlierprobability is proportional to the degree of interpolation between classes, blur and thus ambiguity.
Figure 9: AudioMNIST confusion matrices for incrementally learned classes of the OCDVAEmodel. When adding classes two and three the model experiences difficulty in classification, how-ever is able to overcome this challenge by exhibiting backward transfer when later learning classesfour and five. It is also observable how forgetting of the initial classes is limited.
Figure 10:	Trained FashionMNIST OCDVAE evaluated on unseen datasets. All metrics are reportedas the mean over 100 approximate posterior samples per data point. (a) The classifier entropy valuesby itself are insufficient to separate most of unknown from the known task’s test data. (b) Recon-struction loss allows for a partial distinction. (c) Our posterior-based open set recognition considersthe large majority of unknown data as statistical outliers across a wide range of rejection priors Ωt.
Figure 11:	Trained MNIST OCDVAE evaluated on unseen datasets. All metrics are reported as themean over 100 approximate posterior samples per data point. (a) The classifier entropy values byitself are insufficient to separate most of unknown from the known task’s test data. (b) Reconstruc-tion loss allows for distinction if the cut-off is chosen correctly. (c) Our posterior-based open setrecognition considers the large majority of unknown data as statistical outliers across a wide rangeof rejection priors Ωt. (d-f) Corresponding outlier detection using the two separate models of thedual-model approach.
Figure 12:	Trained AudioMNIST OCDVAE evaluated on unseen datasets. All metrics are reportedas the mean over 100 approximate posterior samples per data point. (a) The classifier entropy valuesby itself are insufficient to separate most of unknown from the known task’s test data. (b) Recon-struction loss allows for a partial distinction. (c) Our posterior-based open set recognition considersthe large majority of unknown data as statistical outliers across a wide range of rejection priors Ωt.
