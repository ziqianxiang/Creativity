Figure 1: Existing explanation methods focus on image classification problems (left), whereas Weexplore explanations for image similarity models (right). We pair a saliency map, which identifiesimportant image regions, but often provides little useful information, with an attribute (e.g., golden),which is more human-interpretable and, thus, a better explanation than saliency alone.
Figure 2: Approach Overview. During training, each saliency map produced by the generator isencouraged to match at least one ground truth attribute’s activation maps. Then, at test time, werank attribute explanations by how well the saliency and attribute activation maps match, along withthe likelihood of the attribute and its explanation suitability prior. Note that we assume the imagesimilarity model has been pretrained and is kept fixed in all our experimentseither in the form of gradients or activations of specific layers (e.g. (Cao et al., 2015; Chang et al.,2019; Nguyen et al., 2016; Selvaraju et al., 2017; Simonyan et al., 2014; Yosinski et al., 2015;Zhang et al., 2016; Zhou et al., 2016)). Most of them produce a saliency map by using some versionof backpropagation from class probability to an input image. In contrast, “black box” approachesrequire no knowledge of the internals (e.g. weights, gradients) of the models. These methods obtainsaliency maps by perturbing the input in a predefined way and measuring the effect of it on themodel output, such as class score. We adapt and compare three “black box” and one “white box”methods for our saliency map generator in Figure 2. “Black box” approaches include a SlidingWindow (Zeiler & Fergus, 2014), which masks image regions sequentially, and Randomized InputSampling for Explanations (RISE) (Petsiuk et al., 2018), which masks random sets of regions. Bothmeasure the effect removing these regions have on the class score. LIME (Ribeiro et al., 2016)first obtains a super-pixel representation of an image. Super-pixel regions are randomly deleted,and their importance is estimated using Lasso. “White box” Mask (Fong & Vedaldi, 2017) learnsa saliency map directly by using different perturbation operators and propagating the error to a lowresolution mask. Although there is some limited work which adapts some saliency methods to the
Figure 3: Qualitative results of our attribute explanations for pairs of examples on the PolyvoreOutfits and the AwA datasets. The attribute predicted as explanation for each reference-query matchis shown below the saliency map. The most likely attribute for the query image as predicted by ourattribute classifier is shown directly underneath it.
Figure 4: Example of the effect replacing the attribute Used as an explanation of the model's behaviorhas on image similarity score (higher score means items are more compatible).
Figure 5: Qualitative examples comparing the saliency map generator candidates on the PolyvoreReference	Query	Sliding	.	.. .	____.	.	>	.....y	LIME	Mask	RISEImage Image Window■ 3p Vm I _D 上 st. ■ zj&L 这—	——v	.
Figure 6: Qualitative examples comparing the saliency map generator candidates on the Animalswith Attributes dataset.	13Under review as a conference paper at ICLR 2020Table 3: Runtime comparison of the compared saliency generation methods and how using a fixedreference image, or manipulating both the query and reference images affects performance.
Figure 7: Additional qualitative examples of our SANE explanations on the Polyvore Outfits dataset.
Figure 8: Additional qualitative examples of our SANE explanations on the AwA dataset.
Figure 9: Examples of the attribute deletion process used to evaluate how good an attribute is asan explanation. We measure the similarity of the input image and some reference image as well asbetween the returned image and the reference image. If a large change in similarity is measured thenthe attribute is considered a “good” explanation. If similarity stays about the same, the attribute isconsidered a “poor” explanation, e.g., trying to remove “active” from the pandas on the right.
Figure 10: The likelihood each attribute in the AwA dataset was identified as the best attribute foran image pair on held-out data.
Figure 11: Six clusters defining the attributes for two approaches to attribute discovery. .
