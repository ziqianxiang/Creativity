Figure 1: System Architecture of CaffeSCONE and Gotentheir training data. E then trains the DNN with the attested algorithm. Once training is done, Cisends queries to E, which then computes the prediction according to the trained model parameters.
Figure 2: Protocol for Outsourcing Linear Operation 0Reducing Run-time of Share Reconstruction Unlike the original standalone protocol whereeach party only needs to learn a share hcii of c but not c = a 0 b itself, it is necessary for ourenclaves to know c because they need to perform the succeeding non-linear operations of non-linearlayers. (In some existing protocols, c is actually recovered “implicitly” via cryptographic means,say, within a garbled circuit.) A naive way is to let Si encrypt their respective shares to the otherenclave E1-i. Again, we use the common seed to form a secure channel which lets Si one-time-padits own share ci into a ciphertext C1-i for E1-i via the key Ki→1-i derived from the seed. In total,we reduce pre/post-processing time by roughly 87.5% and halve the communication cost.
Figure 3: Training Throughput of CaffeSCONEXOeJny WWFO IOOOOO 200000 300000 400000 500000 600000Running time (s)Figure 4: Accuracy Convergence in VGG11Table 1: Time Distribution on Linear/Non-linear Layers	Linear Layers		Non-linear Layers		Total	Time (ms)	Proportion	Time (ms)	Proportion	Time (ms)CaffeSCONE (BS=128)	9243^	57.7%	6774~	42.3%	16017Goten (BS=512)	4306^	58.1%	3106~	41.9%	741^Speedup	8.59×	-	8.72×	-	8.64×Throughput of CaffeSCONE First, we show the training throughput of CaffeSCONE in Fig. 3,by which we emphasize that using more cores on CPU cannot improve the performance of such apure-SGX approach. Moreover, we benchmark the throughput with batch sizes of 128 (a commonsetting in plaintext setting) and 512 (the setting we adopted for Goten). We confirmed that the formerone has better performance for VGG11 in CaffeScone, and thus we adopt it in the later experiments.
Figure 4: Accuracy Convergence in VGG11Table 1: Time Distribution on Linear/Non-linear Layers	Linear Layers		Non-linear Layers		Total	Time (ms)	Proportion	Time (ms)	Proportion	Time (ms)CaffeSCONE (BS=128)	9243^	57.7%	6774~	42.3%	16017Goten (BS=512)	4306^	58.1%	3106~	41.9%	741^Speedup	8.59×	-	8.72×	-	8.64×Throughput of CaffeSCONE First, we show the training throughput of CaffeSCONE in Fig. 3,by which we emphasize that using more cores on CPU cannot improve the performance of such apure-SGX approach. Moreover, we benchmark the throughput with batch sizes of 128 (a commonsetting in plaintext setting) and 512 (the setting we adopted for Goten). We confirmed that the formerone has better performance for VGG11 in CaffeScone, and thus we adopt it in the later experiments.
Figure 5: Speedup vs. Arith. Intensity of GPU-powered Conv. of Shape (B, Cout, Cin, Ihw)Micro-benchmarks: Speedup of Our GPU Outsourcing Protocol As our main contribution isthe performance speedup on linear layers, we further isolate the performance gain of them. Fig. 5shows the speedup and arithmetic intensity, which is explained in Appendix D, of each convolutionlayer presented in VGG with CIFAR-10. The shapes correspond to the batch size, the number ofinput channels, the number of output channels, the height and width of input images. The filter sizeof all layers is 3 × 3. The results illustrate that Goten are most beneficial to neural networks withhigh-arithmetic-intensity linear layers.
Figure 6: The Architecture of VGG11random matrix/tensor so it can be replaced trivially. Now, S0 has the view of S0 in the originalprotocol plus two random matrices/tensors.
