Figure 1: We take a graph perspective on past experience (middle) and extract a subgraph (right)such that its structure allows to compute exact Q-values using Q-iteration for the resulting finiteMDP. Its Q-values represent lower bounds to the Q-values in the actual continuous MDP.
Figure 2: Educational example with four transitions and three states (state 0 is terminal). We char-acterize transitions based on the graph structure: (in-)directly connected to a terminal state (blue,orange); loose ends (green) and disconnected but infinite paths (red). The right plot illustrates thestandard deviation over predicted Q-values for each type of transition.
Figure 4: The 7-state star problem (Baird, 1999). Using vanilla TD-learning, state values andweights spiral out to infinity (orange dots). Applying our graph-based lower bounds however makesTD-learning converge to the correct solution (blue solid line).
Figure 3: Simulated Peg-In-Hole task.
Figure 5: Learning curves for vanilla DDPG and Qgraph-bounded Q-learning(’QG’) on the left; anumber of baselines in the center; and performance under limited graph capacity on the right.
Figure 6: Standard deviation of pre-dicted Q-values.
Figure 7: Vanilla DDPG performance for all tested learning rates.
