Figure 2: l-th feature ex-traction block.
Figure 1: s-th step of the generation process.
Figure 3: Example of tensor representation. n, a, and b denote the numbers of nodes, node labels,and edge labels, respectively. Xπ ∈ {0,1}n×a, whose i-th row is a one-hot vector correspondingto the label of v∏(i), stores information about nodes. An ∈ {0,1}n×n×b, whose (i,j)-th elementis a one-hot vector corresponding to the label of e∏(i),∏(j), stores information about edges. If noedge exists between v∏(%) and v∏j), we replace it with a zero vector. Note that (G,∏) and (Xπ ,Aπ)correspond uniquely.
Figure 4: s-th step of the generation process (S = 6,t = 3). In its s-th step, given a subgraph G<s,our model estimates the new node Vs and new edges {eι,s,..., es-ι,s} to add next. Specifically,it takes a tensor pair (X<s,A<s,<s) as input and predicts the label of the new node Xs and thelabels of the new edges At,s (t = 1,…,S - 1). In detail, first, the feature extractor calculatess - 1 node feature vectors HV = (h；,…,hV-ι)T and a graph feature vector hG from a tensor pair(X<s,A<s,<s). Next, the node estimator predicts the label of the new node Xs from the graphfeature vector. When the EOS is output, the generation is terminated. Finally, the edge estimatorpredicts the labels of the new edges At,s (t = 1,..., S - 1) from the node feature vectors, thegraph feature vector, the embedded vector of the predicted node label, and the embedded vectors ofpreviously predicted edge labels in this step.
Figure 5: Operation of graph attention in the l-th feature extraction block, where query, key, andvalue vectors are all node feature vectors (i.e., Q = K = V = HV = (hV(l-1),..., h；--I))T) (selfattention). For simplicity, We focused on one query/output vector and one subspace.
Figure 6:	Operation of graph attention in the estimation of the new edge et,s (s = 6,t = 3). Thequery vector is Concat(h：, h；), and the value and key vectors are {Concat(hT, h；, hT,s)∣τ =1,…,t - 1}, where h； is the feature vector of the node Vt, h； is the embedded vector ofthe predicted label of the new node Vs, Xs, and hers. is the embedded vector of the predictedlabel of the new edge e「,s, Aτ,s. In other words, Q = Concat(h；, h；)T, K = V =(Concat(h；1, hs；, he1,s), ..., Concat(ht；-1, hs；, hte-1,s))T (source target attention).
Figure 7:	BFS tree example of G<s (left), possible connections to the new node Vs (middle), andimpossible connections to the new node vs (right).
Figure 8: Distributions of attention weights in edge estimation on the grid dataset. The blue rep-resents the distribution of nodes predicted to have no connection to the new node, and the orangerepresents that of nodes predicted to have connection. More than 90 % of attention weights areless than 0.05 for the former distribution, whereas approximately 70 % of attention weights are lessthan 0.05 and more than 15 % are larger than 0.95 for the latter. Note that the scale of the y-axis isdifferent between the top and halves.
Figure 9: Illustration of attention weight zeroing. In the estimation of et,s, we ignore nodes that arepredicted to have no connection with Vs (v2 or e2,s in this case) and set their attention weights tozero deterministically.
Figure 10: Generated graphs (grid).
Figure 11: Generated graphs (lobster).
Figure 12: Generated graphs (community).
Figure 13: Generated graphs (B-A).
Figure 14: Generated molecular graPhs.
