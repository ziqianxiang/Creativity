Figure 1: We modify the components of the Bellman update to cover different types of irrationali-ties: changing the max into a softmax to capture noise, changing the transition function to captureOPtimiSm/pessimism or the illusion of control, changing the reward values to capture the nonlin-ear perception of gains and losses (prospect theory), changing the average reward over time into amaximum (extremal), and changing the discounting to capture more myopic decision-making.
Figure 2: The log loss (lower = better) of the posterior as a function of the parameter we vary foreach irrationality type. These six irrationalities all have parameter settings that outperform rationalexperts. For the models that interpolate to rational expert, we denote the value that is closest torational using a dashed vertical line.
Figure 3: A best case analysis for each irrationality type: the log loss/L2 distance from mean(lower=better) for experts, as a function of the length of trajectory observed. Each irrationalityuses the parameter value that is most informative. As discussed in section 3.2, different irrational-ity types have different slopes and converge to different values. In addition, the best performingirrationality type according to log loss is not the best performing type according to L2 loss.
Figure 4: (a) Optimism bias produces different actions for θ* = (4,1) vs. θ* = (1,4) in the statesshown: the rational policy is to go away from the hole regardless of θ, but an optimistic expert takesthe chance and goes for the larger reward - UP in the first case, down in the second. (b) Pessimismbias produces different actions for θ* = (1,1) vs. θ* = (4,4): when the reward is sufficiently large,the expert becomes convinced that no action it takes will lead to the reward, leading it to performrandom actions.
Figure 5: (a) Boltzmann-rationality produces different policies for θ* = (1,1) vs. θ* = (4,4):when ∣∣θ∣∣ is larger, the policy becomes closer to that of the rational expert. (b) A Myopic expertproduces different policies for θ* = (4,1) vs. θ* = (4,0): while the rational expert always detoursaround the hole and attempts to reach the larger reward, myopia causes the myopic expert to go forthe smaller source of reward when it is non-zero.
Figure 6: The informativeness of policies correlates with the informativeness of trajectories of length30, as discussed in section 3.2and noise via Boltzmann rationality were the most informative irrationalities in our environments,far surpassing the performance of the rational expert for their ideal settings. Our contribution overallwas to identify a systematic set of irrationalities by looking at deviations in the terms of the Bellmanupdate, and show that being irrational is not automatically harmful to inference by quantifying andcomparing the inference performance for these different types.
Figure 7: Log loss for the posterior on θ, given trajectories from the Prospect Theory expert and theIllusion of Control expert.
Figure 8: The L2 distance (lower = better) of posterior mean of θ to the true θ*,s as a function of theparameter we vary for each irrationality type. These six irrationalities all have parameter settingsthat outperform rational experts. For the models that interpolate to rational expert, we denote thevalue that is closest to rational using a dashed vertical line.
Figure 9: The L2 distance (lower=better) of the posterior mean θ to th true θ*, given trajectoriesfrom the Prospect Theory expert and the Illusion of Control expert.
Figure 10: A comparison of reward inference using a correct model of the irrationality type, ver-sus always using a Boltzman model. (Lower log loss = better.) The inference impairment fromusing the misspecified irrationality model (Boltzmann) greatly outweighs the variation in inferenceperformance caused by the various irrationality types themselves. Hence, compared to using a mis-specified model of irrationality, expert irrationality is not in itself a major impairment to rewardinference, and sometimes expert irrationality can even helps when a model of the irrationality isknown.
Figure 11: An example of why assuming Boltzmann is bad for a myopic agent - the Boltzmannrational agent would take this trajectory only if the reward at the bottom was not much less than thereward at the top. The myopic agent with n ≤ 4, however, only ”sees” the reward at the bottom.
