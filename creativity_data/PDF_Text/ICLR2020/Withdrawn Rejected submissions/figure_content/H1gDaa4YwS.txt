Figure 1:right side illustrates how parameters are reused for a convolutional layer. Conv and deconv denote convolutionand deconvolutional operations, and fm and BNm denote activation function and batch normalization of layerm, respectively. Shared kernel and bias are represented by km and bm, and we assume that the deconvolutioninternally transposes the kernel tensor.
Figure 2: MI planes for the initial models RRA, KRA,StdA and Orta. End points of RRA are located in themiddle of the graphs. But for StdA and OrtA, I(L; Y)values of every layer are very high, which means morespecific features about the given output data set arelearned by them.
Figure 3: MI plane and accuracy comparisons for continued training (RRAA,RRAA, StdAA and OrtAA). The RRAA model slightly outperforms the otherthree models, indicating the positive effect of the full racecar training.
Figure 5: Comparisons betweenregenerated inputs. F.l.t.r.: OrtA,StdA, RR3A, and the reference.
Figure 6: Accuracy comparisons forbase task A and transfer learning taskB. RR3AA and RR3AB achieve the bestperformance for the base task andtransfer learning task, respectively.
Figure 7: Accuracy compar-isons of task A and task B. RRAAand RR1A3B got bestperformancefor tast A and B.
Figure 8: Regenerated low resolution results and high resolutionoutputs comparison between StdA, RRA and reference. Only RRAsuccessfully recovers the input.
Figure 9: Accuracy comparisons of trans-fer learning tasks. RRAA and RRAB got bestperformance for task B1 and B2 .
Figure 10: Left: stylization test from a natural image to the starry night painting style. Right: stylization testfrom horse to zebra.
Figure 11: First primary stylization test,RRA6 did not change specific features in theresults.
Figure 12: Stylization comparison fromlow resolution to high resolution. RRA6's re-sults are closer to the high resolution.
Figure 13: RRA6 generates a zebra pat-tern in the horse,s shadow, but yields gen-erally improved results compared to the reg-ular transfer in StdA.
Figure 14: MI plane of StdA as an example.
Figure 15:	Left: data from MNIST; Right: data from n-MNIST with motion blur.
Figure 16:	Left: training processes of RRA (blue, accuracy: 0.9810, cost: 5.675 seconds/epoch), StdA (orange,accuracy: 0.9827, cost: 3.522 seconds/epoch) and OrtA (red, accuracy: 0.9792, cost: 4.969 seconds/epoch).
Figure 17:	Left: training processes of RR1A3 (blue, accuracy: 0.5784, cost: 64 seconds/epoch) and StdA(orange, accuracy: 0.8272, cost: 63 seconds/epoch). Middle: training processes of RR1A3A (green) and StdAA(blue). Right: training processes of RR1A3B (blue), StdAB (pink) and StdB (green).
Figure 18: Training processes OfRRABI (orange), StdABI (blue) and StdBI (red). The green inset shows thefinal loss values.
Figure 19: Example outputs comparisons between RRAb「StdAB「StdBI and reference. We can see thatrrAbi works better than StdABi, while StdBi failed for this task producing a mostly black image.
Figure 20: Training processes OfRRAB2 (orange), StdAB2 (blue) and StdB2 (red). The green inset shows thefinal loss values.
Figure 21: Example outputs and mean absolute error comparisons between RRAB2, StdAB2, StdB2 andreference. We can see that error of RRAB2 is lower than StdAB2 and StdB2.
Figure 22: Top5 accuracy of RR1A6 (red, accuracy: 0.6673, cost: 0.636 second/batch) and StdA (blue, accuracy:0.7324, cost: 0.308 second/batch).
Figure 23: Top5 accuracy of RR1A6A (grey) and StdAA (blue).
