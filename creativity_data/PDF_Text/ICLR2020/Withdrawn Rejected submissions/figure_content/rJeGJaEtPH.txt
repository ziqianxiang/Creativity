Figure 1: The MIST architecture - A network Hn estimates locations and scales of patches encodedin a heatmap h. Patches are then extracted via a sampler S , and then fed to a task-specific networkTτ . In this example, the specific task is to re-synthesize the image as a super-position of (unknownand locally supported) basis functions. As top-k operation is non-differentiable, we back-propagateby lifting through G ; see Section 3 for details.
Figure 2: MNIST character synthesis examples for (top) the “easy” single instance setup and (bottom)the hard multi-instance setup. We compare the output of MISTs to grid, channel-wise, sequentialEslami et al. (Eslami et al., 2015) and Zhang et al. (Zhang et al., 2018c).
Figure 3: Two auto-encoding examples learnt from MNIST-hard. In the top row, for each example wevisualize input, patch detections, and synthesis. In the bottom row we visualize each of the extractedpatch, and how it is modified by the learnt auto-encoder.
Figure 4: Inverse rendering of Gabor noise; we annotate correct / erroneous localizations.
Figure 5: Two qualitative examples for detection and classification on our Multi-MNIST dataset.
Figure 6: Qualitative SVHN results.
Figure 7: Evolution of the loss and heatmap during training. (a) the task (classification) loss Ltaskand (b) the heatmap related loss kG ({xk}) - Hη (I)k22 for each iteration. In (c), we show how theheatmap evolves for an example image from the SVHN dataset during training (from top to bottom)-brighter the intensity, higher the scores; see Section D.
Figure 8: Examples with uneven distributions of digits.
