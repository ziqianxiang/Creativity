Figure 1: Fairness type 1.
Figure 2:	Fairness type 2.
Figure 4: Fair networks for fairness type 2Figure 3:	Networks for fairness type 1Algorithm 1 Type 1 Fair learning algorithmRequire: α, the learning rate, λ the fairness constraint, m the batch size, nb_iter, the number ofiterations,nw the number of iterations for the Wasserstein estimators.
Figure 3:	Networks for fairness type 1Algorithm 1 Type 1 Fair learning algorithmRequire: α, the learning rate, λ the fairness constraint, m the batch size, nb_iter, the number ofiterations,nw the number of iterations for the Wasserstein estimators.
Figure 5: Accuracy / fairness tradeoffs between our Wasserstein approach and more traditional GANapproaches similar to Beutel et al. (2017); Madras et al. (2018) for demographic parity.
Figure 6: Fair auto encoder.a) T-shirt, b) fair representation of T-shirt, c) shirt, d)fair representationof shirtLearning based on fair representations: To illustrate Proposition 1 and the fact that our fairnessconstrinat can be applied to other type of problem than classification, we consider classification ofT-shirts versus shirts (Y = {T shirt, shirt}) in the fashion-MNIST dataset Xiao et al. (2017) (12000training examples, 2000 test examples). These two classes are known to be the most challengingto distinguish in this dataset (accuracy around 0.9). We bias the dataset by adding a color (S ={turquoise, yellow}) correlated to the class variable Y : P(S = yellow|Y = T - shirt) = 0.9,P (S = T urquoise|Y = shirt) = 0.9. We apply the following experimental process: train on thebiased dataset, and compare validation performances both on the biased test set and the same biasedtest set, with switched colors: P(S = yellow|Y = T - shirt) = 0.1 and P(S = Turquoise|Y =shirt) = 0.1.
