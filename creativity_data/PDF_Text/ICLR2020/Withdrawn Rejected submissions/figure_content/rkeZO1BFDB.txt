Figure 1: Overview of Meta Learner that learns to benefit from two learning schemes.
Figure 2: XL-R2R dataset analysis. (a) is instruction length distribution. (b) is sub-instructionnumber per instruction distribution. (c) and (d) are top 7 part-of-speech tag distribution of Chineseand English instructions.
Figure 3: Architecture of the proposed cross-lingual VLN framework.
Figure 4: Learning curves of SPL on validation seen and unseen sets. AN is to train the modelwith Chinese annotations only. EN+AN and EN+AN+MT are to train the model with 100% Englishhuman data, or with 100% English human data plus Chinese MT data, with a certain amount ofChinese annotations to the training set. Our method is to enable knowledge transfer via adversariallearning, building on top of meta+txt2img.
Figure 5: Cross-lingual VLN modelsModel —	#paras	Validation Seen					Validation Unseen						PL	NEJ	SR ↑	SPL ↑	CLS ↑	PL	NEJ	SR ↑	SPL ↑	CLS ↑Base-mono		12.69	5.52	45.3	37.3	53.2	10.89	7.71	26.2	21.5	39.7Base-Bi	19.6M	12.74	5.67	43.6	35.9	51.8	10.684	7.408	27.7	22.5	41.0Shared Enc	17.8M	13.12	5.64	44.5	36.4	52.0	11.050	7.395	27.7	22.5	41.0Shared Dec	13.5M	12.13	5.06	51.0	43.7	56.9	11.069	7.048	31.0	25.5	42.8Shared Enc-Dec	11.7M	12.62	4.97	52.0	43.7	56.7	11.187	7.080	31.1	24.9	42.1Table 2: Performance comparison for cross-lingual VLN models. All models are trained and testedwith English and Chinese annotation data. Results are averaged over 3 runs.
Figure 6: Case Study. We choose a succeeded instruction from the validation set for illustration.
Figure 7: Statistics of human annotated and machine translated data. (a) is instruction length dis-tribution. (b) is sub-instruction number per instruction distribution. (c) is top 7 part-of-speech tagdistribution of annotated and machine translated instructions.
