Figure 1: A 16-4 Wide-ResNet, trained with ghost batch normalization on CIFAR-10 for 200 epochs.
Figure 2: The performance of a 16-4 Wide-ResNet on CIFAR-10 using SGD with Momentum anda batch size of 64. We train both with batch normalization, and also without batch normalizationusing “zeroInit”. We identify both the optimal effective learning rate which maximizes the testaccuracy and the optimal effective learning rate which minimizes the training loss, and we presentthe mean performance of the best 12 out of 15 runs. a) Initially the test accuracy rises as the epochbudget increases, however when training without batch normalization it begins to fall beyond 400training epochs. b) The training loss falls monotonically as the epoch budget rises. c) With batchnormalization, the learning rate which minimizes the training loss falls rapidly as the epoch budgetrises, while the learning rate which maximizes the test accuracy only varies by a factor of 2 when theepoch budget rises over two orders of magnitude. d) Similarly, without batch normalization usingzeroInit, the learning rate which minimizes the training loss falls as the epoch budget rises while thelearning rate which maximizes the test accuracy is constant for all epoch budgets considered.
Figure 3: Our standardized learning rate schedule, both with and without learning rate warmup.
Figure 4: A 16-4 Wide-ResNet, trained with batch normalization on CIFAR-10 for 200 epochs. Forcompleteness, we provide the full results of a learning rate sweep at two batch sizes, 64 and 1024.
Figure 5: A 16-4 Wide-ResNet, trained without batch normalization using zeroInit on CIFAR-10 for 200 epochs.
Figure 6: A fully connected autoencoder, trained on MNIST for 200 epochs. We report the perfor-mance of SGD and SGD w/ Momentum. We perform a grid search to identify the optimal learningrate which maximizes the mean-squared error (MSE) on the test set, and report the mean performanceof the best 5 of 7 runs. a) The test MSE of SGD w/ Momentum is initially independent of batch size,but it begins to rise when the batch size exceeds 128. The test MSE of vanilla SGD starts rising forbatch sizes exceeding 16. b) We see similar phenomena on the training set MSE. c) The optimaleffective learning rate is proportional to batch size when the batch size is small for both vanilla SGDand SGD w/ Momentum, while it becomes independent of batch size for larger batch sizes. Theoptimal effective learning rate in the curvature dominated regime is larger for SGD w/ Momentum.
Figure 7: A word-level LSTM language model trained on PTB for 40 epochs. We report theperformance of SGD and SGD w/ Momentum. We perform a grid search to identify the optimallearning rate which maximizes the test set perplexity, and report the mean performance of the best 5of 7 runs. a) The test set perplexity of SGD w/ Momentum is independent of batch size when thebatch size is small, but begins to rise when the batch size exceeds 128. The test set perplexity ofvanilla SGD starts rising for batch sizes exceeding 64. b) We see similar phenomena on the trainingset perplexity. c) The optimal effective learning rate is proportional to square root of the batch sizewhen the batch size is small, while it levels off for larger batch sizes. The gradients of consecutiveminibatches in a language model are not independent, violating the assumptions behind linear scaling.
Figure 8: The performance of a word-level LSTM language model trained on the Penn TreeBankdataset using SGD with Momentum and a batch size of 64 at a range of epoch budgets. Weidentify both the optimal effective learning rate which minimizes the test set perplexity and theoptimal effective learning rate which minimizes the training set perplexity, and we present the meanperformance of the best 5 out of 7 runs. a) Initially the test set perplexity falls as the epoch budgetincreases, however it begins to rise beyond 56 training epochs. b) The training set perplexity fallsmonotonically as the epoch budget rises. c) The learning rate that minimizes the training set perplexityfalls as the epoch budget rises, while the learning rate that minimizes the test set perplexity onlyvaries by a factor of 2 when the epoch budget rises over two orders of magnitude.
Figure 9: The performance of a fully connected autoencoder on MNIST using SGD with Momentumand a batch size of 32 with varying training epoch budget. We identify both the optimal effectivelearning rate which minimizes the test set MSE and the optimal effective learning rate whichminimizes the training set MSE, and we present the mean performance of the best 5 out of 7 runs. a)Initially the test set MSE falls as the epoch budget increases, and it only starts going up very slightlyfor large epoch budgets. b) The train set MSE falls monotonically as the the epoch budget rises. c)The learning rate that minimizes the test set MSE decreases, while the learning rate that minimizesthe train set MSE remains constant as the epoch budget rises. This is contrary to what we observe infigures 2 and 8. The reason for this is apparent from figure d), where we plot the test set MSE duringtraining for all 7 runs for an epoch budget of 800 for learning rate = 0.004 and = 0.002. Wenotice that for a larger learning rate, the model overfits on the training set faster, causing the test setMSE to rise by the time of the first learning rate drop at 400 epochs. This suggests that early stoppinghas more influence on the final test performance in this architecture than stochastic gradient noise.
Figure 10: Performance of a 16-4 Wide-ResNet with batch normalization trained on CIFAR-10using SGD with Momentum at a batch size of 64. We tune the initial and the final learning ratesindependently, as described in section G. We plot both the optimal initial and final learning rates formaximizing the test set accuracy, as well as the optimal initial and final learning rates for minimizingthe training set loss, and we present the mean performance of the best 5 out of 7 runs. a) The testaccuracy initially increases with increasing compute budget before saturating for epochs budgetsgreater than 800. b) Meanwhile the training loss falls monotonically as the epoch budget rises. c)The optimal initial learning rate which maximizes the test accuracy is constant for epoch budgetsgreater than 400, while the optimal final learning rate decays rapidly as the epoch budget increases.
