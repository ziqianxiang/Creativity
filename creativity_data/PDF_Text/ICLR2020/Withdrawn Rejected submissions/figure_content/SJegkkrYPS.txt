Figure 1: (a) Our general sparsity mechanism in which we update the sparsity mask after eachbatch until we reach a desired level of sparsity. (b) Schedule comparison between this work,Mostafa & Wang (2019), and Lym et al. (2019). Our work has the shortest pruning era and moregradually reaches our final sparsity.
Figure 2: This figure illustrates sparsity across different weight dimensions and different granulari-ties. (a) Window sparsity,, which prunes the 5 smallest weights ina3 Ã— 3 window. (b) CKpruningwhere whole RXS convolutional kernels are pruned. (c) Sparsity in fully-connected weights. (d)Block sparsity in a fully-connected layer.
Figure 3: FC pruning taking advantage of the sparsity in the previous convolution layer.
Figure 4: top left: Convergence plots of all pruning methods with first epoch of pruning 30, topright: Sparsity plot of all methods, bottom left: and CK starting pruning at different epochs, bottomright: Resnet v1 and v1.5 at 60% sparsity for 90 epochs.
Figure 5: Resnet50 v1.5 layer sparsity for CK, start 30, targetting 70% sparsity.
Figure 6: Convergence plots of Resnet50 on Tiny-Imagenet with Window (left) and CK Pruning(right). At all sparsity levels we are the near or above the baseline. Though the differences betweenmodels is small, 60% seems to performs the best and is a peak with respect to 40% and 80%.
Figure 7: Convergence and sparsity plots of Resnet50v1.5 on Imagenet with CK Pruning, firstepoch of sparsity = 30. As the amount of sparsity increases, the accuracy of the model seems todecrease semi-linearly. We can achieve up to 70% while still being above 74% sparsity.
Figure 8: Convergence and sparsity plots of Resnet50v1.5 on Imagenet with Intra-Epoch Pruning,first epoch of sparsity = 30. The left plot shows training over the full 100 epochs instead of zoomingin on the tail end of training. This allows us to observe the importance of the learning rate dropsat epoch 30 and 60. The drop at epoch 90 does have a small increase, as well.
Figure 9:	Convergence and sparsity plots of Resnet50v1.5 on Imagenet with Window Pruning, firstepoch of sparsity = 30. With window, likw with CK, the impact of the learning rate drops at epochs30 and 60 is big. Note that the sparsity of the window is not as smooth as CK, showing that it is lessuniformly sparse from epoch to epoch.
Figure 10:	Convergence plots of Resnet50v1.5 on Imagenet at 60%, top-1 and top-5 accuracy. Forboth top-1 an top-5, window pruning starting at 0 does not perform as well as the other methods.
Figure 11:	Convergence plots with CK (top left), intra (top right), window (bottom left), and sparsityplot with window (bottom right). Resnet50v1.5 on Imagenet with first epoch of sparsity = 30.
