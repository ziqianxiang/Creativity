Figure 1: A: SPECTRA. Illustration ofan entity-centric transition model. B: Naive Perception mod-ule with a CNN-based encoder and a slot-wise decoder. Hyperparameters description in AppendixA.
Figure 2: left: Full and sparse settings are trained on environment containing one box and evaluatedout-of-distribution on two boxes. We plotted the validation losses of both settings during training.
Figure 3: Comparison of slot-wise masked decodings when the perception module is trained sep-arately or jointly with the sparse transitions. We show the reconstruction associated with the slotsthat contain information about the agent. When the perception module is trained jointly, slots in thelearned latent set are biased to be entity-centric (here agent-centric).
Figure 4: Loss vs training updates, with training is done in pixel space, transitions are sampledrandomly and results are averaged over 3 runs. left: Validation perception loss Lpercep of joint andseparate training right: Validation transition loss Ltrans of joint and separate training. Separatetraining is better in terms of perception loss but joint training gives a better transition model. Weposit that this is because the slots are biased to be entity-centric and transformations involving onlyrelevant entities are easier to learn.
Figure 5: Reconstructions of a slot for different levels of its activation gate (Eq. 1). Transformationslearned when the perception module is trained jointly vs separately from the sparse transitions.
Figure 6: Comparison is done against randomly sampled transitions. left: Number of entitieschanged in the 1-step buffer during training. As expected, the number of transitions with 2 spa-tial locations changed in the grid increases whereas the ones with no location changed decreases.
Figure 7: Transition model with and without selection phase.
Figure 8: Additional visualisations of masked decodings from joint and separate training settings.
Figure 9: Additional visualisations of masked decodings from joint and separate training settings.
Figure 10:Total transition—SPECTRA→Separate training—SPECTRA→Joint trainingAdditional visualisations of masked decodings from joint and separate training settings.
Figure 11: Additional visualisations of the transformations learned when the perception module istrained jointly and separately from the transitions. Joint training yield more visually interpretableand localized transformations of the slots.
