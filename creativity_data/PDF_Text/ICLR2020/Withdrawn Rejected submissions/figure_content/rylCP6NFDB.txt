Figure 1: Demonstration of experimental environments•	How is the effectiveness of HTRPO?•	How does each component of HTRPO contribute to its effectiveness?•	How is the performance of policy gradient methods trained with hindsight data in continu-ous environments?•	How sensitive is HTRPO to network architecture and some key parameters?5.1	Experimental SettingsWe implement HTRPO on a variety of reinforcement learning environments, including Bit Flipping,Grid World and Fetch. Among them, Bit Flipping, Grid World, Fetch Reach and Fetch Push areimplemented as descrete-action environments while we also conduct continuous version of experi-ments in Fetch Reach, Fetch Push and Fetch Slide. A glimpse of these environments is demonstratedin Figure 1 while the detailed introductions are included in Appendix F.1. The reward mechanismsare intentionally modified to sparse reward regulations. Besides, for continuous version of Fetchexperiments, we apply an additional policy entropy bonus to encourage more exploration.
Figure 2:	Evaluation curves for discrete environments. The full lines represent the average evalua-tion over 10 trails and the shaded regions represent the corresponding standard deviation.
Figure 3:	Evaluation curves for continuous environments. The full lines represent the average eval-uation over 10 trails and the shaded regions represent the corresponding standard deviation.
Figure 4:	Evaluation curves with other network structures. Horizontally, 3 figures in each lineillustrate the perfomances in one environment with different network architectures. Vertically, eachcolumn illustrate the performances of one kind of network architecture in different environments.
Figure 5:	Evaluation curves for different number of alternative goals: 8-Bit Flipping, 16-Bit Flip-ping, Empty Maze, Four Rooms, Discrete Fetch Reach, Discrete Fetch Push, Continuous FetchReach, Continuous Fetch Push and Contiuous Fetch Slide. The full lines represent the averageevaluation over 10 trails and the shaded regions represent the corresponding standard deviation.
Figure 6:	Training curves for all environments: 8-Bit Flipping, 16-Bit Flipping, Empty Maze, FourRooms, Discrete Fetch Reach, Discrete Fetch Push, Continuous Fetch Reach, Continuous FetchPush and Contiuous Fetch Slide. The full lines represent the average evaluation over 10 trails andthe shaded regions represent the corresponding standard deviation.
Figure 7:	Success Rate for all environments:8-Bit Flipping, 16-Bit Flipping, Empty Maze, FourRooms, Discrete Fetch Reach, Discrete Fetch Push, Continuous Fetch Reach, Continuous FetchPush and Contiuous Fetch Slide. The full lines represent the average evaluation over 10 trails andthe shaded regions represent the corresponding standard deviation.
Figure 8:	Success Rate for HTRPO, TRPO with Sparse Reward and TRPO with Dense Reward forContinuous Fetch Reach, Continuous Fetch Push and Contiuous Fetch Slide. The full lines representthe average evaluation over 10 trails and the shaded regions represent the corresponding standarddeviation.
Figure 9:	Estimation of KL Divergence Expectation for different variants of HTRPO in ContinuousFetch Reach, Continuous Fetch Push and Contiuous Fetch Slide. The full lines represent the averageevaluation over 10 trails and the shaded regions represent the corresponding standard deviation.
