Figure 1: Physical finger platform in action with different policies.
Figure 2: Gaussian (left) and uniform(right) shaking examples.
Figure 3: The effect of voxels on supressing spatial noise of the physical system. The trajectoriesare produced by linear open-loop controllers as those in section 4.1 for the purpose of illustratingthe effect of voxelization.
Figure 4: PD controller with various noise intensities on Kp parameter.
Figure 5: The time evolution of the GP approximated gt for PD feedback controller at some exem-plary time instancesFigure 6: The time evolution of the GP approximated gt for nonlinear sinusoidal open-loop con-troller at some exemplary time instances.
Figure 6: The time evolution of the GP approximated gt for nonlinear sinusoidal open-loop con-troller at some exemplary time instances.
Figure 7: Pysical gradients computed for various time steps along a source trajetcory using twoperturbed trajectories of linear open-loop controller.
Figure 8: Zero-shot planning with constraint satisfaction. The orange trajectory is the source pro-duced by the nominal controller. The green and blue are two sampled trajectoreis that are producedby perturbing kp to kp by eq. (21)4.4	Zero-shot planning taskOur previous experiments in sections 4.1,4.2 and 4.3 showed that learning the physical derivativemap is feasible for various types of controllers. In this section, we demonstrate an example of aconstrain satisfaction task by means of the physical derivative map. In this experiment, the su-perscript (s) corresponds to the nominal trajectory which is called source. Assuem the system iscontrolled by a PD controller to reach a target state x*, i.e., the control torques are designed asU = kPS)(X - x*) + kds)X. The controller does a decent job to reach the target state given rea-sonable values for kp and kd . However, such controller does not give us a clear way to shape thetrajectory that starts from x° and ends at x*. Assume it is desired that the nominal controlledtrajectory T(S) passes through an intermediate state x* at time t on its way towards the targetstate x* (we can equally assume that the system must avoid some regions of the state space be-cause of safety reasons). The solution with physical derivatives is as follows . Assume kd(S) isfixed and only kp(S) is changeable. If the physical derivatives map is available, we have access togt(kp - kPS)) = (x* - X(S))/(kp - kPs)). By simple algebraic rearrangement, We have(21)The new parameter of the policy is supposed to push the source trajectory T(S) towards a targettrajectory T* that passes through the desired state xt* at time t. The result of this experiment on our
Figure 9: Components of the physical finger platformB Additional Plots illustrating Real World Challenges(section 3)(a) t = 200(b) t = 400Figure 10: Same controller applied for multiple runs. The trajectories are produced by the linearopen-loop controller similar to those used in section 4.1. See the plot for a different set of nominalparameters of the controller in fig. 13 in the Appendix (Zooming is recommended)(e) t = 1000(a) t = 20030 O(e) t = 1000(b) t = 400	(c) t = 600	(d) t = 800Figure 11:	The same as fig. 10 but for a different setting.
Figure 10: Same controller applied for multiple runs. The trajectories are produced by the linearopen-loop controller similar to those used in section 4.1. See the plot for a different set of nominalparameters of the controller in fig. 13 in the Appendix (Zooming is recommended)(e) t = 1000(a) t = 20030 O(e) t = 1000(b) t = 400	(c) t = 600	(d) t = 800Figure 11:	The same as fig. 10 but for a different setting.
Figure 11:	The same as fig. 10 but for a different setting.
Figure 12:	Noisy linear open-loop controller(d) t = 800(e) t = 1000(a) t = 200	(b) t = 400	(c) t = 600Figure 13: The same as fig. 12 but for a different nominal parameters of the policy.
Figure 13: The same as fig. 12 but for a different nominal parameters of the policy.
Figure 14: The effect of temporal noise in delaying one trajectory versus the other one and itscorrection. The trajectories are produced by the linear open-loop controller similar to those usedin section 4.1(Zooming is recommended)C Applications of physical derivativesIf we know how the states of a trajectory change as a result of a change in the policy parameters,the policy can be easily updated to push the trajectory towards a desired one. For example, assumeWe are interested in going from the current trajectory T(θ) to the target trajectory T*. The distancebetween these trajectories can get minimized by perturbing the policy parameters in the direction-∂∣∣T(θ) - T*k∕∂θ. This direction is already available since we have estimated ∂T(θ)∕∂θ asa physical derivative. As an exemplary case, We shoW this application of our method in practicein section 4. Other applications of physical derivatives are in robust control and safety. In both cases,the physical derivative allows us to predict the behaviour of the system if the policy changes in aneighbourhood around a nominal policy. Then, it is possible to make sure that some performance orsafety criteria will not be violated for the local perturbation in the policy. As a concrete example, foran autonomous driving system, there can be a calibration phase during which physical derivativesof the car is estimated by perturbing the controller parameters around different nominal policieswhich are likely to occur in real roads. The calibration must be done in a safe condition and beforedeploying the system. When deployed, the estimated physical derivatives can be used to predict theeffect of a change of the policy on the behaviour of the system and neutralize the change if it wouldmove the car towards unsafe regions of its state space. The command that changes the policy can be
Figure 15: The maximum potential error in the estimated gradients when the space is voxelized. Ascan be seen, the error is vanishing when the corresponding voxels to x(i) and x(j) are far from eachother.
Figure 16: The time evolution of the learned GP models from directional derivatives for ∂x3 /∂kpby Gaussian sampling20Under review as a conference paper at ICLR 2020Figure 17: The time evolution of the learned GP models from directional derivatives for ∂x3 /∂kpby uniform samplingz/vw「.「・・・・・・ ，」・••・・・T=I350T = 2500T = 2650T = 2800T = 2350T=Io50T= 1700T = 50T =250T=I50⅛ΛzM ASIIVWVI ：/vwT . . ∙ , ∙ ∙ “ ∙ ∙ ∙ ∙	” ∙ , ∙ , ∙ ∙ ” ∙ ∙ ∙ ∙ J ..」・・・・・ ∙ ∙ T" [ ∙ , ∙ ∙ . . ∙..
Figure 17: The time evolution of the learned GP models from directional derivatives for ∂x3 /∂kpby uniform samplingz/vw「.「・・・・・・ ，」・••・・・T=I350T = 2500T = 2650T = 2800T = 2350T=Io50T= 1700T = 50T =250T=I50⅛ΛzM ASIIVWVI ：/vwT . . ∙ , ∙ ∙ “ ∙ ∙ ∙ ∙	” ∙ , ∙ , ∙ ∙ ” ∙ ∙ ∙ ∙ J ..」・・・・・ ∙ ∙ T" [ ∙ , ∙ ∙ . . ∙..
Figure 18: The time evolution of the learned GP models from directional derivatives for ∂x2 /∂kpby uniform sampling21Under review as a conference paper at ICLR 2020Figure 19: The time evolution of the learned GP models from directional derivatives for ∂x1 /∂kpby Gaussian samplingT = O	T=Io	T =30	T =50T = 70	T=IIo	T=I60	T= 210T = 270	T = 390	T = 500	T = 830T=IoOO	T =1130	T= 1300	T= 1460Figure 20: The time evoltuion of the learned GP models from directional derivatives for ∂x2 /∂kpby Gaussian sampling22Under review as a conference paper at ICLR 2020Figure 21: The time evolution of the learned GP models from directional derivatives for ∂x3 /∂kpby Gaussian samplingFigure 22: The time evolution of the learned GP models from directional derivatives for ∂x1 /∂kpby uniform sampling23Under review as a conference paper at ICLR 2020
Figure 19: The time evolution of the learned GP models from directional derivatives for ∂x1 /∂kpby Gaussian samplingT = O	T=Io	T =30	T =50T = 70	T=IIo	T=I60	T= 210T = 270	T = 390	T = 500	T = 830T=IoOO	T =1130	T= 1300	T= 1460Figure 20: The time evoltuion of the learned GP models from directional derivatives for ∂x2 /∂kpby Gaussian sampling22Under review as a conference paper at ICLR 2020Figure 21: The time evolution of the learned GP models from directional derivatives for ∂x3 /∂kpby Gaussian samplingFigure 22: The time evolution of the learned GP models from directional derivatives for ∂x1 /∂kpby uniform sampling23Under review as a conference paper at ICLR 2020Figure 23: The time evolution of the learned GP models from directional derivatives for ∂x2 /∂kpby uniform samplingFigure 24: The time evolution of the learned GP models from directional derivatives for ∂x3 /∂kpby uniform sampling
Figure 20: The time evoltuion of the learned GP models from directional derivatives for ∂x2 /∂kpby Gaussian sampling22Under review as a conference paper at ICLR 2020Figure 21: The time evolution of the learned GP models from directional derivatives for ∂x3 /∂kpby Gaussian samplingFigure 22: The time evolution of the learned GP models from directional derivatives for ∂x1 /∂kpby uniform sampling23Under review as a conference paper at ICLR 2020Figure 23: The time evolution of the learned GP models from directional derivatives for ∂x2 /∂kpby uniform samplingFigure 24: The time evolution of the learned GP models from directional derivatives for ∂x3 /∂kpby uniform sampling24Under review as a conference paper at ICLR 2020Figure 25: The time evoltuion of the histogram of cos(α) where α is the angle between the true andpredicted directional derivative. The perturbations in the training phase are generated by Gaussiansampling.
Figure 21: The time evolution of the learned GP models from directional derivatives for ∂x3 /∂kpby Gaussian samplingFigure 22: The time evolution of the learned GP models from directional derivatives for ∂x1 /∂kpby uniform sampling23Under review as a conference paper at ICLR 2020Figure 23: The time evolution of the learned GP models from directional derivatives for ∂x2 /∂kpby uniform samplingFigure 24: The time evolution of the learned GP models from directional derivatives for ∂x3 /∂kpby uniform sampling24Under review as a conference paper at ICLR 2020Figure 25: The time evoltuion of the histogram of cos(α) where α is the angle between the true andpredicted directional derivative. The perturbations in the training phase are generated by Gaussiansampling.
Figure 22: The time evolution of the learned GP models from directional derivatives for ∂x1 /∂kpby uniform sampling23Under review as a conference paper at ICLR 2020Figure 23: The time evolution of the learned GP models from directional derivatives for ∂x2 /∂kpby uniform samplingFigure 24: The time evolution of the learned GP models from directional derivatives for ∂x3 /∂kpby uniform sampling24Under review as a conference paper at ICLR 2020Figure 25: The time evoltuion of the histogram of cos(α) where α is the angle between the true andpredicted directional derivative. The perturbations in the training phase are generated by Gaussiansampling.
Figure 23: The time evolution of the learned GP models from directional derivatives for ∂x2 /∂kpby uniform samplingFigure 24: The time evolution of the learned GP models from directional derivatives for ∂x3 /∂kpby uniform sampling24Under review as a conference paper at ICLR 2020Figure 25: The time evoltuion of the histogram of cos(α) where α is the angle between the true andpredicted directional derivative. The perturbations in the training phase are generated by Gaussiansampling.
Figure 24: The time evolution of the learned GP models from directional derivatives for ∂x3 /∂kpby uniform sampling24Under review as a conference paper at ICLR 2020Figure 25: The time evoltuion of the histogram of cos(α) where α is the angle between the true andpredicted directional derivative. The perturbations in the training phase are generated by Gaussiansampling.
Figure 25: The time evoltuion of the histogram of cos(α) where α is the angle between the true andpredicted directional derivative. The perturbations in the training phase are generated by Gaussiansampling.
Figure 26: The time evolution of the histogram of cos(α) where α is the angle between the true andpredicted directional derivative. The perturbations in the training phase are generated by uniformsampling.
Figure 27: The time evolution of the histogram of cos(α) where α is the angle between the true andpredicted directional derivative. The perturbations in the training phase are generated by Gaussiansampling.
Figure 28: The time evolution of the histogram of cos(α) where α is the angle between the true andpredicted directional derivative. The perturbations in the training phase are generated by uniformsampling.
Figure 29: Examples of trajectories produced by the perturbed controller and the computed deriva-tives along the trajectory. The arrows are plotted as they originate from the perturbed trajectoriesonly for easier distinction. Each arrow corresponds to the change of the states at a certain time stepon the source trajectory as a result of perturbing the policy. Each figure corresponds to a pair of nom-inal values of {w, b} for the linear open-loop controller of section 4.1 and the perturbed trajectoriesare produced by Gaussian sampling.
