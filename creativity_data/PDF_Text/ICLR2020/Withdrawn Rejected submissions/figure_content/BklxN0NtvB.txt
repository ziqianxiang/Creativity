Figure 1: Deploying a neural network layer, l, on an analog in-memory crossbar involves first flat-tening the filters for a given layer into weight matrix Wl , which is then programmed into an arrayof NVM devices which provide differential conductances Gl for analog multiplication. A randomGaussian ∆Wl is used to model the inherent imprecision in analog computation.
Figure 2: (a) Different loss terms on the test dataset and model test accuracy as a function of noisestandard deviation, the losses are normalized to the pretrained model loss lpretrained, calculatedusing clean weights. Accuracy is calculated by performing the inference 100 times on the test set,error bars show the standard deviation.(b) Estimate of normalized mutual information between theinput and output of the baseline LeNet and its variants as a function of noise standard deviation. Arandom subset of 200 training images are used for this estimate, with each inference repeated 100times on a random realization of the network to estimate H(Y |X). Mutual information decays withrising noise, deeper and narrower networks are more susceptible to this decay.
Figure 3: (a) Test accuracy as a function of noise level, here We have ηtr =力位，error bars show thestandard deviation of different training and inference runs. Our method with distillation achievesthe best robustness. (b) Comparison of model performance at noise levels different from the traininglevel.
Figure 4: (a) Test accuracy as a function of noise level for 4-bit ResNet-18, here We have ηtr = ηinf,error bars shoW the standard deviation of different training and inference runs. Retraining Withdistillation and noise injection achieves the best results With quantization. (b) Test accuracy ofdifferent models during retraining With noise level η = 0.057.
Figure 5: Schematic of our retraining method combining distillation and noise injection.
Figure 6: Results on LeNet and its variants show that our method of combining distillation andnoise injection improves noise robustness for different model architectures on MNIST. The benefitof our method is the most significant when the network struggles to learn with vanilla noise injectionretraining method. This threshold noise level depends on the network architecture, as we haveremarked for mutual information decay.
