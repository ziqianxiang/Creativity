Figure 1: Illustration of an attention-based encoder-decoder model2.2	Training approachesAs shown in equations 2 and 3, minimizing the KL-divergence between the true distribution andthe model distribution can be approximated by minimizing the NLL. This motivates the approach totrain the model in teacher forcing mode, where p(yt|y1:t-1, x1:L; θ) is computed with the correctoutput history y1:t-1, as shown in equations 5 and 6. In this case, the loss can be written as:LyT)⑻=- Pn=I logp(yinττ|xinL； θ) = - Pn=I PT=I logp(y(n) 似3,xinL； θ)	(13)This approach yields the correct model (zero KL-divergence) if the following assumptions hold:1) the model is powerful enough ; 2) the model is optimized correctly; 3) there is enough trainingdata to approximate the expectation shown in equation 2. In practice, these assumptions are oftennot true, hence the model is prone to make mistakes. To illustrate the problem, suppose there is areference output y*.τ for the test input x；：L. Due to data sparsity in high-dimensional space, x；：Lis likely to be unseen during training. If the probability p(y；|y；：t-i, x；：l； θ) is wrongly estimatedto be small at time step t, the probability of the reference output sequence p(y；：T |x1：l； θ) Will alsobe small, i.e. it will be unlikely for the model to generate y；：T.
Figure 2: Illustration of attention forcingthe output and align it with the input. As the reference alignment is known, the decoder can focuson inferring the output, and the attention mechanism can focus on generating the correct alignment.
Figure 3: Illustration of a speech synthesis systemduration model predicts the duration of each linguistic feature; the feature model maps the linguisticfeatures to y1:T . This paper focuses on the state-of-the-art approach, where θ, as well as φ, is aneural network. φ can be considered a neural vocoder, which is not limited by the assumptionsmade by the conventional vocoders (Lorenzo-Trueba et al., 2018; Kalchbrenner et al., 2018). θ isan attention-based seq2seq model, as described in section 2.1. Compared with the conventionalapproach, the attention-based model has several advantages, such as performance gain and less needfor data labeling (Wang et al., 2017). Note that as shown in figure 3, θ learns not only to map acharacter sequence to a feature sequence, but also to align them. In contrast, φ does not align itsinput and output (Shen et al., 2018; Oord et al., 2016).
Figure 4: Result of the listening test comparing teacher forcing and attention forcing5	Experiments5.1	Speech SynthesisThe TTS experiments are conducted on LJ dataset (Ito, 2017), which contains 13,100 utterancesfrom a single speaker. The utterances vary in length from 1 to 10 seconds, totaling approximately24 hours. A transcription is provided for each waveform, and the corresponding vocoder featuresare extracted with PML vocoder (Degottex et al., 2016). The training-validation-test split is 13000-50-50. The waveform-level model is the Hierarchical Recurrent Neural Network (HRNN) neuralvocoder (Mehri et al., 2016; Dou et al., 2018). The model structure is exactly the same as describedin Dou et al. (2018), and the model configuration is adjusted for efficiency. The frame-level model isvery similar to Tacotron (Wang et al., 2017). The model structure and configuration are the same asdescribed in Wang et al. (2017), except that: 1) the decoder target is vocoder features; 2) the attentionmechanism is the hybrid (content-based + location-based) attention (Chorowski et al., 2015); 3) eachdecoding step predicts 5 vocoder feature frames. The neural vocoder is always trained with teacherforcing. The frame-level model is trained with either teacher forcing or attention forcing. Details ofthe setup (data, models and training) are presented in appendix A.2.1.
