Figure 1: Multigrid memory architecture. Top Left: A multigrid convolutional layer (Ke et al.,2017) transforms input pyramid X, containing activation tensors {x0, x1, x2}, into output pyramid Yvia learned filter sets that act across the concatenated representations of neighboring spatial scales.
Figure 2: Memory interfaces. Left: Multiple readers (red, orange) and a single writer (blue)simultaneously manipulate a multigrid memory. Readers are multigrid CNNs; each convolutionallayer views the hidden state of the corresponding grid in memory by concatenating it as an additionalinput. Right: Distinct encoder (green) and decoder (purple) networks, each structured as a deepmultigrid memory mesh, cooperate to perform a sequence-to-sequence task. We initialize the memorypyramid (LSTM internals) of each decoder layer by copying it from the corresponding encoder layer.
Figure 3: Mapping, localization, and exploration. An agent is comprised of a deep multigridmemory, and two deep multigrid CNNs (query and policy subnetworks), which have memory readaccess. Navigating a maze, the agent makes a local observation at each time step, and chooses a nextaction, receiving reward for exploring unseen areas. The query subnet, given a local patch, mustreport all previously observed maze locations matching that patch. Subnet colors reflect Figure 2.
Figure 4:	Generalization of localiza-tion. Fixing parameters after training thequery subnet on random motion (left),its localization loss remains low whiletraining the exploration policy (middle),whose reward improves (right).
Figure 5:	MNIST recall & classification. Givena random sequence of images followed by a repeat(green), output the class of the next image (red).
Figure 6:	Learning curves.
Figure 7: Information routing. Shown are example paths of information flow in a multigridarchitecture. Progressing from one layer to the next, information flows between grids at the samelevel (via convolution, green), as well as to adjacent grids at higher resolution (via up-sampling andconvolution, red) or lower resolution (via down-sampling and convolution, not shown). Informationfrom location (1,1) (blue) of the source grid at [layer 1, level 1] can be propagated to all the locationsmarked as blue in subsequent layers and levels, following the indicated paths. With two morelayers (not shown), it could reach any location in any level. Similarly rapid routing occurs alongdown-sampling links (in combination with up-sampling); information on fine-scale levels can quicklyflow to coarser levels and then to any location. In a real multigrid network, the portion of this routingcapacity actually used is determined by the learned network parameters (convolutional filters).
Figure 8: Multigrid memory writer-reader(s) architecture for spatial navigation. At each timestep, the agent moves to a new location and observes the surrounding 3 × 3 patch. The writer receivesthis 3 × 3 observation along with the agent’s relative location (with respect to the starting point),updating the memory with this information. Two readers receive randomly chosen 3 × 3 and 9 × 9queries, view the current map memory built by the writer, and infer the possible locations of thosequeries.
Figure 9: Multigrid memory encoder-decoder architecture for MNIST sorting. After processingthe input sequence, the encoder (top) transfers memory into the decoder, which predicts the sequenceof classes of the input digits in sorted order.
Figure 10: Visualization of DNC mem-ory in mapping task. Due to its definedaddressing mechanism, the DNC always al-locates a new continuous memory slot ateach time-step. It does not appear to main-tain an interpretable structure of the map.
