Figure 1: Illustrative figure: An agent is learning priors from exploration data from World 1 Stage 1in Nintendo Super Mario Bros game. In this paper, the agent focuses on learning two types of priors:learning an action-state preference score for local regions and a dynamics model. The action-statescores on the middle left learns that approaching the “Koopa” from the left is undesirable while fromthe top is desirable. On the middle right, a dynamics model can be learned to predict a future statebased on the current state and action. The agent can apply the priors to a new task World 2 Stage 1 toachieve reasonable policy with zero shot.
Figure 2: An overview of SAP framework. (a) and (b) in the figure correspond to the Scoring,Aggregating part. (c) and (d) together describes the dynamics learning and planning. See Section 3.2for details.
Figure 3: (a) The hidden reward gridworld example. The agent needs to learn the points of eachobject and collect as many points as possible in a new world configuration. See Section 5.1 for moredetails. (b) & (c) The total returns of different methods on the Super Mario Bros. All the experimentsare run with 50000 game steps on multiple episodes. Error bars are shown as 95% confidence intervalexcept for the given interaction data. See Section 5.2.1 for details. Best view in color.
Figure 4: Ablative results. (a),(c) are SAP on W1S1 and W2S1 respectively with the groundtruthdynamics model. (b),(d) are similar to (a),(c) but without the true “done” signal. Error bar is 95%confidence interval. We see that with a perfect dynamics model the performance is boosted for bothmodel-based methods. However, even with a disabled “done” signal, SAP still works well while NHPperforms significantly worse than before.
Figure 5: Visualized greedy actions fromthe learned scores in a new environment(W5S1).
Figure 6: Four variants of the 3D robot reacher environments. See Section 5.2.1 for details.
Figure 7: A visualization of the sub-regions in the Super Mario Bros game. In this game, there are intotal 8 sub-regions.
Figure 8: More visualizations on the greedy action map on W1S1, W2S1(new task) and W5S1(newtask). Note the actions can be different from the policy from MPC.
Figure 9: Visualization of the learned score on pre-extracted objects. The grayscale area is thevisualized score and the pink area is a separator. Best viewed in color. In (a), we synthetically put themario agent in 8 relative positions to “koopa” conditioned on the action “move right”. The score issignificantly lower when the agent’s position is to the left of “koopa” compared to other position. In(b), it is the same setup as in (a) but conditioned on the action “jump”. We find that across (a) and (b)the left position score of (b) is smaller than that of (a) which is consistent with human priors. In (c)and (d), we substitute the object from “koopa” to the ground. We find that on both (c) and (d) thescore are similar for the top position which means there is not much difference between differentactions. Note this figure is only for visualizations and we even put the agent in positions that cannotbe achieved in the actual game.
