Figure 1: Clustering states in embedding space. We start with the original states (left most), obtainand cluster the embeddings (middle two), and finally label original states by clusters (right most).
Figure 2: Illustration of policy trainedthrough clustering. The first-step pol-icy guide the agent towards the correctcluster (white arrow), and the second-step policy guides the agent towards thegoal (grey arrow).
Figure 3: Illustration of policy trainedthrough optimizing on negative distancein embedding space. Note that movingstraight in embedding space (right) maycorrespond to moving around a wall inthe maze (left).
Figure 4: Visualization of environments used in this paper. The environments are GridWorld,HalfCheetah, Pendulum, Reacher, and AntMaze respectively.
Figure 5: CPC representation of GridWorld environment:U-Maze (left), Four-Rooms (middle), and Block-Maze(right). By clustering embeddings (bottom, all embeddingsare visualized by T-SNE), we are able to recover clusterscorresponding to natural structures of the mazes (top).
Figure 6: Plot of true distance be-tween states in maze vs distance in rep-resentation space. The x-axis is the truedistance obtained by A*, while the y-axis is the distance in the representationspace. A line of best fit is provided forillustration.
Figure 7: Learning curves of GridWorld environments, averaged over 3 runs. The reward curve withclusters is shifted down by 1 to remove the cluster reward.
Figure 8: Illustrations of embeddings of Pendulum andlearning curves for different reward schemes. Embeddings(bottom-left) cluster primarily by angle of the arm (top-left).
Figure 9: Illustrations of embeddings of AntMaze andlearning curves for different reward schemes. Embeddings(bottom-left) cluster primarily by position of the agent (top-left). Orange curve achieves similar performance to pinkcurve with smaller variance.
Figure 10: Illustration of learning curves for dif-ferent setups. Learning on the original sparse re-ward problem with embeddings as features (yel-low) did not lead to significant learning improve-ment, while using embeddings to provide rewards(orange) achieved the best learning.
Figure 11: Illustrations of two background textures and clustering of their corresponding embed-dings (clustered by embeddings, illustrated by mapping to position of arm). The cloth texture (left)was used during training, while the wood texture (right) was only used during validation. Bothtextures led to embeddings properly clustered by the angle of the arm.
Figure 12: Illustrations of embeddings of HalfCheetah andlearning curves for different reward schemes. Embeddings(bottom-left, visualized by T-SNE) cluster by x position ofthe agent (top-left, with x-aixs being x position and y-axisbeing y-position of the agent).
Figure 13: Illustrations of embeddings of Reacher andlearning curves for different reward schemes. Embeddings(bottom-left) cluster primarily by angles of sections of thearm (top-left). Orange curve achieves almost-perfect per-formance at 750k steps.
