Figure 1: Panel A-C show the structure of LBF, Ada-BF and disjoint Ada-BF respectively.
Figure 2: Histogram of the classifier’s score distributions of keys (Malicious) and non-keys (Benign)for Task 1. We can see that nj (number of keys in region j) is monotonic when score > 0.3. Thepartition was only done to ensure PjI ≥ C5.2	Task 2: Virus ScanBloom filter is widely used to match the file’s signature with the virus signature database. Ourdataset includes the information of 41323 benign files and 96724 viral files. The virus files arecollected from VirusShare database (Vir). The dataset provides the MD5 signature of the files,legitimate status and other 53 variables characterizing the file, like “Size of Code”, “Major LinkVersion” and “Major Image Version”. We trained a machine learning model with these variablesto differentiate the benign files from the viral documents. We randomly selected 20% samplesas the training set to build a binary classification model using Random Forest model 2. We used“sklearn.ensemble.RandomForestClassifier” to tune the model, and the Random Forest classifier costsabout 136Kb. The classification model achieves 0.98 prediction accuracy on the testing set. Thepredicted the class probability (with the function “predict_prob” in “sklearn” library) is used as thescore s(x). Other implementation details are similar to that in Task 1.
Figure 3: Histogram of the classifier score distributions for the Virus Scan Dataset. The partition wasonly done to ensure pjι ≥ c.
Figure 4: FPR with memory budget for all the five baselines (the bit budget of BF = bitmap size +learner size). (a) FPRs comparison of Malicious URL detection experiment; (b) FPRs comparison ofVirus scan experiment.
Figure 5:	FPR comparison of tuning c while fixing the number of groups K and tuning both (K, c)Appendix B	More comparis ons between the LBF and sandwichedLBFWe0:① ≥4s0d ①-BL(a)	(b)Figure 6:	FPR comparison between LBF and sandwiched LBF under different bitmap sizes. (a)malicious URL experiment; (b) malware detection experimentAppendix C	Comparing the Bloom filter to HierarchicalHashingThe machine learning model used in the learned Bloom filters is critical because it has discriminationpower between the keys and non-keys and is more efficient in identifying keys in some cases. Toshow its unique role, we replaced the machine learning model with another Bloom filter such thatit becomes a hierarchical Bloom filter (learner is replaced by an initial filter). To implement thehierarchical Bloom filter, we spare 50% of the bit budget to the initial filter and use the other bits tobuild the backup filter.
Figure 6:	FPR comparison between LBF and sandwiched LBF under different bitmap sizes. (a)malicious URL experiment; (b) malware detection experimentAppendix C	Comparing the Bloom filter to HierarchicalHashingThe machine learning model used in the learned Bloom filters is critical because it has discriminationpower between the keys and non-keys and is more efficient in identifying keys in some cases. Toshow its unique role, we replaced the machine learning model with another Bloom filter such thatit becomes a hierarchical Bloom filter (learner is replaced by an initial filter). To implement thehierarchical Bloom filter, we spare 50% of the bit budget to the initial filter and use the other bits tobuild the backup filter.
Figure 7: FPR comparison between LBF and sandwiched LBF under different bitmap sizes. (a)malicious URL experiment; (b) malware detection experimentAppendix D Proof of the StatementsProof of Lemma 1: Let	Zj(x)	=	Pm=I I(S(X)	∈	[τj-ι,τj)∣x	∈	S),	then	Zj(x)〜Bernoulli(pj ), and mj = Pim=1 Zj (xi ) counts the number of non-keys falling in group j andp^j = 誓.To upper bound the probability of the overall estimation error of pj first, We need toevaluate its expectation, E (PK=IIPj - pj |).
