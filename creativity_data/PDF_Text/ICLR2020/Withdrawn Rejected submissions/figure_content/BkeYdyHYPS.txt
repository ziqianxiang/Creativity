Figure 1: Evo-NAS agent sampling a sequence of architectural choices. Each of the colored blocksrepresents an architectural choice being set to a specific value. Each value of the generated sequenceis sampled from the policy learned by the agent’s neural network with probability P, or reused fromthe parent sequence with probability 1 - P.
Figure 2: Reward moving average (left) and best reward (right) on “Learn to count” task averagedover 20 replicas. Shaded area represents 70% confidence interval.
Figure 3: Reward moving average (left) and best reward (right) on NASBench task averaged over 64replicas. Shaded area represents 70% confidence interval.
Figure 4: ReWard moving average (left) and best reWard (right) on the ImageNet proxy task.
Figure 5: Overview of the how the Evolutionary agent creates a sequence of values for eacharchitectural choice specifying a generated model, given a parent trial. Each of the colored blocksrepresents an architectural choice set to a specific value. Each action is re-sampled randomly withprobability p, or reused from the parent with probability 1 - p.
Figure 6: Overview of how the Neural agent samples a trial. Each of the colored blocks represents anarchitectural choice set to a specific value. Each action is sampled from a distribution defined by anRNN.
Figure 7: Correlation of validation accuracy with test accuracy (Left) and validation ROC-AUC withtest ROC-AUC (Right). The correlation is higher for ROC-AUC. For plotting the correlations, Weused the ConsumerComplaints dataset.
Figure 8: Number of trials performed for the experiments from Figure 9. The empty circles representthe number of trials performed in each of the 10 experiment replicas. The filled circles represent themeans of the empty circles. We superpose ±1 standard deviation bars.
Figure 9: Results of the experiments on 7 text classification tasks. Each experiment was run 10times. For each run, we have selected the model that obtained the best ROC-AUC on the validationset (the best reward). These best models were then evaluated by computing the ROC-AUC on theholdout testset. The empty circles in the plot represent the test ROC-AUC achieved by each of the 10best models. The filled circles represent the means of the empty circles. We superpose ±1 standarddeviation bars.
Figure 10: Reward moving average for the compared agents. The average is computed over a windowof 50 consecutive trials. We ran 3 replicas for each experiment. The shaded area represents minimumand maximum value of the rolling average across the runs.
Figure 11: Quality metrics of the different agents during the first 3k trials of architecture search onthe image classification proxy task. The reward is computed on the validation set, while the test setis used only for the final evaluation of the selected network. We report the moving average of thereward over 50 trials (Left) and the best reward attained so far (Right).
Figure 12: Neural networks achieving the best reward for image classification generated by: (a)Neural agent, (b) Evolutionary agent, (c) Evo-NAS agent. For a detailed description of the FactorizedHierarchical Search Space and its modules refer to (Tan et al., 2018).
