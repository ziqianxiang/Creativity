Figure 1: The left figure shows that the performance drops largely with the increase of sentencelength on the De-En dataset. The right figure shows the attention map from the 3-th encoder layer.
Figure 2: Multi-scale attention hybrids point-wise transformation, convolution, and self-attention tolearn multi-scale sequence representations in parallel. We project convolution and self-attention intothe same space to learn contextual representations.
Figure 3: BLEU scores of models on differentgroups with different source sentence lengths.
Figure 4: Dynamically selected kernels ateach layer: The blue bars represent the ra-tio between the percentage of the convolutionwith smaller kernel sizes and the percentageof the convolution with large kernel sizes.
