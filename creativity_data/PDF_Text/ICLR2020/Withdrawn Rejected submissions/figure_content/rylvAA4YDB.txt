Figure 1: IsoNN Framework Architecture. (The left subplot provides the outline of the proposedframework, including the graph isomorphic feature extraction component and the classificationcomponent respectively. Meanwhile, the right subplot illustrates the detailed architecture of theproposed framework, where the graph isomorphic features are extracted with the graph isomorphiclayer, min-pooling layer and Softmax layer, and the graphs are further classified with three fully-connected layers.)routes among communities). Meanwhile, many concrete real-world application problems, e.g., braingraph based patient disease diagnosis, molecule function classification and community vibrancyprediction can also be formulated as the graph classification problems.
Figure 2: An Illustration of Deep Architecture of IsoNN.
Figure 3: EffeCtiveness of Different kAE do not Contain any struCtural information. For HIV-DTI, AE gets 0 in F1. This is beCause thedataset Contains too many zeros, whiCh makes the AE learns trivial features. Also, for PTC, its F1is higher than other models, but the aCCuraCy only get 50.0, whiCh indiCates AE aCtually have abad performanCe sinCe it Cannot disCriminate the Classes of the instanCes (i.e., prediCting all positiveClasses). CNN performs better than AE but still worse than IsoNN. The reason Can be that it learnssome struCtural information but fails to learn representative struCtural patterns. SDBN is designedfor brain images, so it may not work for MUTAG and PTC. One possible reason for WL got badresults is the isomorphism test is done on the whole graph, whiCh may lead to erratiC results. GCNperforms better than GIN but worse than IsoNN, showing that GCN Can learn some sturCtual infor-mation without node labels, but GIN Cannot work with the adjaCenCy matrix as input. Is oNN-fastaChieves the best sCores on MUTAG and seCond-best on HIV-fMRI, yet worse than several othermethods on other datasets. This Can be the approximation on P may impair the performanCe. Com-paring IsoNN with AE, IsoNN aChieves better results. This means the struCtural information ismore important than only ConneCtivity information for the ClassifiCation problem. If Compared withCNN, the results also show the Contribution of breaking the node-order in learning the subgraphtemplates. Similar to SDBN, IsoNN also finds the features from subgraphs, but IsoNN gets betterperformanCe with more ConCise arChiteCture. Contrasting with GCN and GIN, IsoNN Can maintainthe expliCt subgraph struCtures in graph representations, while the GCN and GIN simply use theaggragation of the neighboring node features, losing the graph-level substruCture infomation.
Figure 4: EffeCtiveness of Different CC(b) Different c7 6 5 4 3 2 1a-E-WE-H2	3	4	5	6k(C)ISONN & ISONN-fastFigure 5: Time Complexity Study5.4 Time Complexity S tudyTo study the effiCienCy of IsoNN and IsoNN-fast, we ColleCt the aCtual running time on trainingmodel, whiCh is shown in Figure 5. In both Figures 5(a) and 5(b) 2, the x-axis denotes its value fork or c and the y-axis denotes the time Cost with different parameters. From Figure 5(a), four linesshow the same pattern. When the k inCreases, the time Cost grows exponentially. This pattern Canbe direCtly explained by the size of the permutation matrix set. When we inCrease the kernel sizeby one, the number of Corresponding permutation matriCes grows exponentially. While Changing c,shown in Figure 5(b), it is easy to observe that those Curves are basiCally linear with different slopes.
Figure 5: Time Complexity Study5.4 Time Complexity S tudyTo study the effiCienCy of IsoNN and IsoNN-fast, we ColleCt the aCtual running time on trainingmodel, whiCh is shown in Figure 5. In both Figures 5(a) and 5(b) 2, the x-axis denotes its value fork or c and the y-axis denotes the time Cost with different parameters. From Figure 5(a), four linesshow the same pattern. When the k inCreases, the time Cost grows exponentially. This pattern Canbe direCtly explained by the size of the permutation matrix set. When we inCrease the kernel sizeby one, the number of Corresponding permutation matriCes grows exponentially. While Changing c,shown in Figure 5(b), it is easy to observe that those Curves are basiCally linear with different slopes.
Figure 6: Deep IsoNN Framework Architecture with Two Graph Isomorphic Layers.
Figure 7: Convergence Analysisscore between one subgraph to one kernel template. Thus, we can also regard each element in Q1as a kernel template. Since we have c channel in the first component, the second component will beused on every channel of Q1. If the channel number of the second component is m, then the firstdimension of the learned feature tensor Q2 of the second component is C * m. For a deeper modelwith 3 or more graph isomorphic feature extraction components, our will do similar operations tothe second isomorphic components. The first dimension of the final tensor Q will be the product ofchannels in all former graph isomorphic layers.
