Figure 1: CWAE-IRL Network architecture3.1	EncoderThe encoder is a neural network which inputs the current state, current action and next state and gen-erates a probability distribution q(rt|st+1, st, at), assumed to be isotropic gaussian. Since a near-optimal policy is inputted into the IRL framework, minibatches of randomly sampled (st+1 , st, at)are introduced into the network. Two hidden layers with dropout are used to encode the input datainto a latent space, giving two outputs corresponding to the mean and log-variance of the distribu-tion. This step is similar to the backward inference problem in IRL methodology where the rewardfunction is constructed from the sampled trajectories for a near-optimal agent.
Figure 2: Reward comparison of IRL algorithms for an objectworld of grid size 10 with randomplacement of objects. (a) Groundtruth reward (b) Bayesian IRL reward (c) Maximum entropy reward(d) Deep maximum entropy reward (e) VAE reward (f) Training loss (blue), validation loss (red) andlikelihood loss (green) over epochsBIRL completely fail to learn the rewards. Deep Maximum Entropy tends to give negative rewardsto state spaces which are not well traversed in the example trajectories. However, CWAE-IRL gener-alizes over the spaces even though they have not been visited frequently. Also, due to the constraintof being close to the prior gaussian belief, the scale of rewards are best captured by the proposedalgorithm as compared to the other algorithms which tend to overscale the rewards.
Figure 3: Reward error for predicted minus the actual error recovered at each time step (smoothed)with 1 Ïƒ confidence interval for the pendulum environment5 ConclusionsI have presented an algorithm for inverse reinforcement learning which learns the latent rewardfunction using a conditional variational auto-encoder with Wasserstein loss function. It learns thereward function as a continuous, Gaussian distribution while trying to reconstruct the next stategiven the current state and action. The proposed model makes the inference process scalable whilemaking it easier for inference of reward functions. Inferring a continuous parametric distribution ofthe reward functions can prove useful for classifying behaviors of decision making agents in diverseapplications such as autonomous driving.
