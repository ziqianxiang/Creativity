Figure 1: Nash scheme Figure 2: Dominant scheme Figure 3: Iterated Dominance schemePJeMaJ uevwTraining iteration ×1OOFigure 4: Individual agent effort (Iterated Dominance) Figure 5: Rewards (Iterated Dominance)values of . It shows that agents indeed learn to all exert effort, at a much lower cost to the principalthan in the dominant strategy scheme (roughly 20% of the cost under the dominant strategy scheme).
Figure 4: Individual agent effort (Iterated Dominance) Figure 5: Rewards (Iterated Dominance)values of . It shows that agents indeed learn to all exert effort, at a much lower cost to the principalthan in the dominant strategy scheme (roughly 20% of the cost under the dominant strategy scheme).
Figure 6: Nash scheme Figure 7: Dominant schemeFigure 8: Iterated Dominance schemeconjecture that MARL converges to iterated dominance solutions under less restrictive conditions thanthose we used to prove our theoretical results (i.e. for less demanding algorithms, or for REINFORCEor other known simple RL algorithms in less restricted classes of games).
Figure 8: Iterated Dominance schemeconjecture that MARL converges to iterated dominance solutions under less restrictive conditions thanthose we used to prove our theoretical results (i.e. for less demanding algorithms, or for REINFORCEor other known simple RL algorithms in less restricted classes of games).
Figure 9: Nash scheme Figure 10: Dominant scheme Figure 11: Iterated Dominance scheme6.3	MARL in Markov Games (Environments with Multiple TimestepsOur analysis in the main paper has focused on normal-form games. However, many environmentsrelate to temporally extended interaction between agents. A key model for such repeated multi-agentinteraction across multiple timesteps is that of Markov games (Shapley, 1953; Littman, 1994). InMarkov games, in each state agents take actions (possibly based only on partial observations of thetrue world state), and each agent obtaining an individual reward. One prominent method for applyingmulti-agent learning in such settings is that of independent MARL, where agents each learn a behaviorpolicy through their individual experiences interacting with one another in the environment.
Figure 12: Nash schemeFigure 13: Dominant scheme Figure 14: Iterated Dominance schemeOpt-in Proportion	Opt-in proportion	Opt-in proportionTo demonstrate the how our work applies to a Markov game setting, we consider an environmentsimilar to the principal-agent joint project setting discussed in the main text, but with multipletime-steps.
Figure 15: Nash schemeFigure 16: Dominant scheme Figure 17: Iterated Dominance schemeHigh effort proportion High effort proportionTraining iteration ×100High effort proportionPJeMaJ uevsO IOO 200	300	400	500	600Training iteration ×100Figure 18: Individual agent effort (Iterated Dominance) Figure 19: Rewards (Iterated Dominance)7 Convergence Rates for IW-MCPITheorem 3.4 provides an asymptotic result showing that in the limit of infinite data IW-MCPI almostsurely converges to a iterated elimination solution. The speed of convergence is game-dependent.
Figure 18: Individual agent effort (Iterated Dominance) Figure 19: Rewards (Iterated Dominance)7 Convergence Rates for IW-MCPITheorem 3.4 provides an asymptotic result showing that in the limit of infinite data IW-MCPI almostsurely converges to a iterated elimination solution. The speed of convergence is game-dependent.
