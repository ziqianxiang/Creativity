Figure 1: The scheme of our model UFLST, which consists of two alternate processes: clusteringand episodic training. At each round, unlabeled data points are clustered based on extracted features,and pseudo labels are assigned according to cluster identities. After clustering, a set of episodic tasksare constructed by sampling from the pseudo-labeled data, and the few-shot learner is trained, whichfurther optimizes feature representations. The two processes are repeated.
Figure 2: Comparing the performances of KRJD and the Euclidean metric. Top 10 neighboursof a chosen query character from Omniglot are shown. Black box: the query character. Greenbox: the positive characters in the neighbourhood of the query character. Red box: the negativecharacters in the neighbourhood of query character. (A) The ranking result using Euclidean metric.
Figure 3: Behaviors of progressive clustering. (A) Visualizing clustering results over training roundsby T-SNE. 10 characters from the Futurama alphabets in the Omniglot dataset were selected. (B)NMI vs. training round. (C) Classification accuracy vs. training round.
Figure 4: Top 10 neighbours of a chosen query character from Omniglot dataset. Black box: thequery character. Green box: the positive characters in the neighbourhood of the query character.
Figure 5: Top 10 neighbours of a chosen query image from the Market1501 dataset. Black box: thequery image. Green box: the positive images in the neighbourhood of the query image. Red box: thenegative images in the neighbourhood of query image. The number under each image represents itstrue class. Upper panels in (A), (B) and (C): the ranking results using the Euclidean metric. Lowerpanels in (A), (B) and (C): the ranking results using KRJD.
Figure 6: Clusters generated by DBSCAN over training rounds. The results of the first 5 rounds areshown. For the convenience of visualization, we only display 50 clusters and the number of datapoints in each of them.
