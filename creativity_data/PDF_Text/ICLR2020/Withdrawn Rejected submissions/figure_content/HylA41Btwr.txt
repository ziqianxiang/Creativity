Figure 1: Graphical illustration of the landscape of GAN outer optimization problem minY φ(Y, X).
Figure 2: (a) True(blue) and generated(red) Samples (b) loss for the generator (c) loss for the dis-criminator from 5-Gaussian synthetic data. (d) True(blue) and generated(red) Samples (e) loss forthe generator (f) loss for the discriminator from 25-Gaussian synthetic data.
Figure 3: Population version and empirical version. Population version: the probability densitiesare changing. Empirical version: the samples are moving.
Figure 4: Illustration of the learning process of the single mode. The generated samples are moving,which correspond to the adjustment of the probability densities. They represent two views on thelearning process.
Figure 5: Illustration of the process of learning a multi-mode distribution. Will decompose thisprocess into two parts in the next figure.
Figure 6: Illustration of learning a multi-mode distribution. We decompose the process as the macro-learning and the micro-learning. The macro-learning refers to the moving of the whole mode towardsthe underlying mode. The micro-learning refers to the adjustment of the distribution within eachmode. If macro-learning fails, then an entire mode is missed in the generated distributions, whichcorresponds to mode collapse. Note that even for the single-mode, the learning process can bedecomposed into the macro learning and the micro learning.
Figure 7: Learning a two-mode distribution. During training, we learned a generator that maps thelatent samples zi’s to xi’s, thus fit the data. If we sample a new zi, then it will be mapped to a newpoint in the underlying data distribution.
Figure 8: Graphical illustration of LaSalle’s invariance principle. Figure from Cohen & Rouhling(2017). Starting from K, the dynamics will finally converge to the set M .
Figure 9: The first figure is a connected component of a graph. It contains 10 vertices and 10 directededges. It can be decomposed into a cycle and two subtrees. The cycle consists of vertices 1, 2, 3, 4.
Figure 10: Illustration of the proof of Claim G.1. For the figure on the left, we pick an arbitrary treewith the head being vertex 9, which corresponds to y6 = y7. We change y7 to y7 = χ7 to obtain thefigure on the right. Since one more cycle is created, the function value increases by -1 log2.
Figure 11: Generated (a)MNIST (b) CIFAR-10 (c) CelebA (d) LSUN samples by CPGAN.
