Figure 1: Comparison of two methods for back-propagation on NODE. As in figure (a), the ODE solver isdiscretized at points {t0, t1 , ..., tN} during forward pass. Black dashed curve shows hidden state solved inforward-time, denoted as z(t). Figure (b) shows the adjoint method, red solid line shows the hidden statesolved in reverse-time, denoted as h(t). Ideally z(t) = h(t) and dashed curve overlaps with solid curve;however, the reverse-time solution could be numerically unstable, and causes z(t) 6= h(t), thus causes errorin gradient. Figure (c) shows the direct back-propagation through ODE solver. In direct back-propagation,we save evaluation time points {t0, t1, ...tN} during forward pass; during backward pass, we re-build thecomputation graph by directly evaluating at the same time points. In this way, z(ti) = h(ti). Since the hiddenstate can be accurately reconstructed, the gradient can be accurately evaluated.
Figure 2: Diecrete-time and continuous-time models on a graph. Nodes are represented with circles, and eachnode is represented with a unique color. Edges are represented with solid lines. For discrete-time models in (a),the hidden states of nodes are updated with discrete steps. For continuous-time models in (b), hidden states ofeach node evolves continuously with time. The dynamics of nodes are represented with dashed lines, with thesame color as corresponding nodes.
Figure 1: Structure of bijective blocks. F and G can be any differentiable neural network whose output hasthe same shape as its input. Blue dot (Orange diamond) represents the forward (inverse) of a bijective function,corresponding to ψ (ψ-1) in Eq. 8 of the main paper. Left (right) figure represents the forward (inverse) as inEq. 8.
