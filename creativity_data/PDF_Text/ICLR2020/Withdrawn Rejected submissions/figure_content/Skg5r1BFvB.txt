Figure 1: ∣∣Θ - Θ* “ and Mean Control Error.
Figure 2: Example trajectories produced by the LQR controllers. We test the LQR policy to followthree types of paths: parabola, circle, and lemniscate. We first train a decoder, then test it on systemswith m = 0.1, k = 0.7 (left column), and m = 1.0, k = 0.7 (right column). Dashed line withcircles:target trajectories. ?: optimal policy. ◦: decoder trained by 1 iteration on randomly drawncontexts. 4: decoder trained by 3 iterations on randomly drawn contexts. ×: decoder trained by 10iterations on randomly drawn contexts.
