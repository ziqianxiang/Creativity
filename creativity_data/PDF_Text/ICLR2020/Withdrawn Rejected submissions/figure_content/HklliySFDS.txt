Figure 1: Training of GIM-LSTM on 3 subtasks.
Figure 2: Test of GIM-LSTM. Input x is presented to allautoencoders. In this case, AE2 obtains the minimum re-constructione error. Hence, the second module (dashedline) is chosen to produce the output. The input is propa-gated up until the second module.
Figure 4: SSMNIST Tasks description4.2	SSMNIST TASKSequential Stroke MNIST (SSMNIST) (de Jong, 2016) is avariation of the popular MNIST dataset of handwritten digits.
Figure 3: A-LSTM learning curveon Sequence Classification. Finalconcatenated length of 12.
