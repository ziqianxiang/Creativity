Figure 1: Sparse Momentum is applied at the end of each epoch: (1) take the magnitude of theexponentially smoothed gradient (momentum) of each layer and normalize to 1; (2) for each layer,remove p = 20% of the weights with the smallest magnitude; (3) across layers, redistribute theremoved weights by adding weights to each layer proportionate to the momentum of each layer;within a layer, add weights starting from those with the largest momentum magnitude. Decay p.
Figure 2: Comparisons against compression methods on MNIST with 95% confidence intervals.
Figure 3: Test set accuracy with 95% confidence intervals on MNIST and CIFAR at varying sparsitylevels for LeNet 300-100 and WRN 28-2.
Figure 4: Parameter sensitivity analysis for prune rate and momentum with 95% confidence intervals.
Figure 5: Dense vs sparse histograms of class-specialization for convolutional channels on CIFAR-10.
