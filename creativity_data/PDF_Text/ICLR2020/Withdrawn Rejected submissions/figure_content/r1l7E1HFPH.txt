Figure 1: Training performance of κ-PI-DQN (Top) and κ-VI-DQN (Bottom) on Breakout, for thehyper-parameter CFA = 0.05 (right) and for the ‘naive’ baseline N(κ) = T (left).
Figure 2: Training performance of κ-PI-TRPO (Top) and κ-VI-TRPO (Bottom) on Walker, for thehyper-parameter CFA = 0.2 (right) and for the ‘naive’ baseline N(κ) = T (left).
Figure 3: Lowering the discount factoroptimal policy of Mδγ(λV) is the κ-greedy policy w.r.t. V . GAE, instead of solving the κ-greedypolicy while keeping V fixed, changes the policy and updates V by the return concurrently. Thus,this approach is conceptually similar to κ-PI-TRPO with N (κ) = T . There, the value and policyare concurrently updated as well, without clear separation between the update of the policy and thevalue.
Figure 4: Performance of κ-PI-DQN and κ-VI-DQN on Breakout for different values of CFA .
Figure 5: Training performance of κ-PI-DQN (Top) and κ-VI-DQN (Bottom) on SpaceInvaders, forthe hyper-parameter CFA = 0.05 (right) and for the ‘naive’ baseline N(κ) = T (left).
Figure 6: Training performance of κ-PI-DQN (Top) and κ-VI-DQN (Bottom) on Seaquest, for thehyper-parameter CFA = 0.05 (right) and for the ‘naive’ baseline N(κ) = T (left).
Figure 7: Training performance of κ-PI-DQN (Top) and κ-VI-DQN (Bottom) on Enduro, for thehyper-parameter CFA = 0.05 (right) and for the ‘naive’ baseline N(κ) = T (left).
Figure 8: Training performance of κ-PI-DQN (Top) and κ-VI-DQN (Bottom) on BeamRider, forthe hyper-parameter CFA = 0.05 (right) and for the ‘naive’ baseline N(κ) = T (left).
Figure 9: Training performance of κ-PI-DQN (Top) and κ-VI-DQN (Bottom) on Qbert, for thehyper-parameter CFA = 0.05 (right) and for the ‘naive’ baseline N(κ) = T (left).
Figure 10: Performance of κ-PI-TRPO and κ-VI-TRPO on Walker2d-v2 for different values ofCFA.
Figure 11: Performance of GAE and ‘Naive’ baseline (Top row) and κ-PI-TRPO (Bottom left) andκ-VI-TRPO (Bottom right) on Ant-v2.
Figure 12: Performance of GAE and ‘Naive’ baseline (Top row) and κ-PI-TRPO (Bottom left) andκ-VI-TRPO (Bottom right) on HalfCheetah-v2.
Figure 13: Performance of GAE and ‘Naive’ baseline (Top row) and κ-PI-TRPO (Bottom left) andκ-VI-TRPO (Bottom right) on HumanoidStandup-v2.
Figure 14: Performance of GAE and ‘Naive’ baseline (Top row) and κ-PI-TRPO (Bottom left) andκ-VI-TRPO (Bottom right) on Swimmer-v2.
Figure 15: Performance of GAE and ‘Naive’ baseline (Top row) and κ-PI-TRPO (Bottom left) andκ-VI-TRPO (Bottom right) on Hopper-v2.
Figure 16: Training performance of κ-PI-TRPO (Left) and the ‘naive’ baseline N (κ) = T (Right)on the CartPole environment.
Figure 17: Training performance when lowering the discount factor (Left) and of κ-PI-TRPO (Left)when discount factor is set to 0.36 (Right) on the CartPole environment.
Figure 18: Training performance when contribution of κ is fixed for a) the shaped reward (Left) andb) the discount factor (Right) on the CartPole environment.
Figure 19: Training performance of κ-PI-TRPO (Top Left: training curves, Top Right: cumulativereturn) and the ‘naive’ baseline N(κ) = T (Bottom) on the Mountain Car environment.
Figure 20: Training performance when lowering the discount factor (Left) and of κ-PI-TRPO (Left)when discount factor is set to 0.995 (Right) on the MountainCar environment.
Figure 21: Training performance when contribution of κ is fixed for a) the shaped reward (Left) andb) the discount factor (Right) on the MountainCar environment.
Figure 22: Training performance of κ-PI-TRPO (Top Left: training curves, Top Right: cumulativereturn) and the ‘naive’ baseline N (κ) = T (Bottom) on the Pendulum environment.
Figure 23: Training performance when lowering the discount factor (Left) and of κ-PI-TRPO (Left)when discount factor is set to 0.96 (Right) on the Pendulum environment.
Figure 24: Training performance when contribution of κ is fixed for a) the shaped reward (Left) andb) the discount factor (Right) on the Pendulum environment.
Figure 25: Cumulative training performance of κ-PI-TRPO on HalfCheetah (Left, corresponds toFigure 12) and Ant (Right, corresponds to Figure 11) environments.
