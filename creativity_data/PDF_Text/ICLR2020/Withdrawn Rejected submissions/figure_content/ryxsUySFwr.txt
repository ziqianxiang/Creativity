Figure 1: A neural network trained to predict house prices in Kentucky from curb-side images.
Figure 2: A neural network is trainedto predict Kentucky housing prices. Thedistribution of predictions on the valida-tion data (black line) and the actual dis-tribution of housing prices (dotted line).
Figure 3: Left: The relative size of “discarded” feature information h⊥ (x) decreases as the depthof neural networks increases. Center: The intrinsic dimensionality of the feature vectors h(x), asmeasured by spectrum of its principal components, decreases with weight decay. Right: Likelihoodof in-distribution and OOD features as measured by a Gaussian decomposed into the likelihood ofeach principal component. The Gaussian was fit on (in-distribution) training set features. OODfeatures have low likelihood on the small principal components, suggesting that they do not lie inthe same low-dimensional distribution.
Figure 4: Smoothed XOR regression task. Left: Target XOR function over in-dist. and OOD data.
Figure 5: Housing price predictions - (Left) Prediction vs. actual price for in-distribution and OODhouses. (Middle-left) The OOD score p(h(x)) from the GMM correlates with the predictive errorand can identify the most erroneous housing predictions. Age estimations - (Middle-right) Predic-tion vs. actual age for adults (in-distribution) and children (OOD). (Right) Similarly to housing, theGMM’s OOD score identifies high-error predictions.
