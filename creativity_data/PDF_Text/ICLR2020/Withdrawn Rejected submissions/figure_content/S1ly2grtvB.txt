Figure 1: An overview of UMN framework: X and Y are the observed inputs and potentially corruptlabels. θ and θ0 are semi-supervised generative models. η and η0 are noise functions that perturb theinput X . The red and green colored arrows represent the flow of unlabeled and labeled data throughthe models. θ is updated via a stochastic gradient descent approach so as to minimize the classificationand VAE losses, and θ0 is the Exponential Moving Average (EMA) of θ. The sample-wise uncertainty() between the predictions of θ and θ0 are used to re-weigh the gradients that are back propagated toθ via the classification loss. The general architecture for θ is adapted from (Kingma et al., 2014). VAEindicates Variational AutoEncoder and C-VAE represents Conditional-VAE. Note that in θ0 , whenmaking predictions (on labeled data) only the weights from encoder of the VAE and the classifier areused. For simplicity, the KL loss for the predictions on unlabeled data with respect to an uniformprior is not shown. For simplicity, we omit the optional consistency loss term between the classifiersof θ and θ0 for unlabeled data in the figure.
Figure 2: The graphical model generating the observed labeland the labeled loss term can be formulated asEqφ (za |x) f ()qφ (y |za).	(7)where f (e) = log[ (CTa1-0]is a function with constant e. From Eq. 7, We can see that it assignsthe same weight for all samples belonging to the same category, i.e., all labeled terms contributeequally to the gradient back propagated regardless of their label correctness asEqφ(za∣χ)[f(e)Vφqφ(y∣Za)].	(8)Although this idea is simple and easy to follow, the assumption may not be the case in the realscenario. In general, the value of e which is the class conditional corruption rate, remains unknownbeforehand. Another issue is that it’s not accurate to apply the same categorical corruption ratio toall data samples of a given class. Our approach solves these problems by estimating sample-wiseuncertainty during the training stage.
Figure 3: Illustration of error rates’ comparisons between UMN and other benchmarks on SVHNduring training stage under different corruption ratios (20%, 30% and 40%). UMN is less prone tooverfitting on mislabeled data at higher corruption ratio.
Figure 4: Illustration of latent feature distribution and progression of . (a) 2D embedding of thelearned latent feature za (output of encoder in the VAE which serves as the input to the classifierbranch) obtained using t-SNE (Maaten & Hinton, 2008) for the MNIST test data. Left and Rightplots show distributions obtained without and with the use of uncertainty estimate , respectively.
Figure 5: Plots showing the distribution of MNIST training samples in the learned latent space fordifferent approaches. We have used t-SNE to visualize the distribution in 2-dimensions. The differentcolors indicate the 10 different classes of MNIST. "Circles" are used to represent true labels and"Cross" to represent mislabeled samples. We can see that similarly colored circles (same class) arebetter grouped by our proposed UMN approach when compared to supervised and semi-supervisedapproaches. For the experiment, we use the MNIST dataset with 100 labeled samples with 50% labelcorruption. The network architecture used for UMN is described in section 4.2.1. Using the samearchitecture, for the supervised experiment we train only the classifier branch of the model, and forthe semi-supervised experiment we set the uncertainty estimate to be = 0.
Figure 6: The network structure for Variational Autoencoder for UMN used for SVHN and CIFAR10.
