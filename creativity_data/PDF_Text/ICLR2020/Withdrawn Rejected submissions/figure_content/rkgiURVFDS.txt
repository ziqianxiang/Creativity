Figure 1: MNIST 1/7 (n = 13007, left) and full MNIST (n = 60000, right) test set certified accuracyto adversarial training label flips as q is varied. The hyperparameter q controls a robust/non-robustaccuracy trade-off. The solid lines represent certified accuracy, except for the undefended classifierwhich represents our attack. The dashed lines of the same color are the overall non-robust accuracyof each classifier. The black dotted line is the performance of a constant classifier, assuming equalrepresentation of the classes.
Figure 2: Dogfish (n = 1800) test set certified accuracy to adversarial label flips as q is varied. Theclassifiers’ certified accuracy curves cross each other further along the x-axis.
Figure 3: IMDB Review Sentiment (n = 25000) test set certified accuracy. The non-robust accuracyslightly decreases as q increases; for q = 0.01 the non-robust accuracy is 79.108%, while for q =0.25 it is 78.732%.
Figure 4: MNIST 1/7 test set certified accuracy with and without `2 regularization in the computationof α. Note that the unregularized solution achieves almost 100% non-robust accuracy, but certifiessignificantly lower robustness. This implies that the “training” process is not robust enough to labelnoise, hence the lower margin by the ensemble. In comparison, the regularized solution achievessignificantly higher margins, at a slight cost in overall accuracy.
Figure 5: Left: Required margin p to certify a given number of label flips using the generic KLbound (10) versus the tight discrete bound (4). Right: The same comparison, but inverted, showingthe certifiable robustness for a given margin. The tight bound certifies robustness to approximatelytwice as many label flips.
