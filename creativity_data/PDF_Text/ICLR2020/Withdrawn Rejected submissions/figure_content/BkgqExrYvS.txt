Figure 1: PopSGD convergence (test loss at the step versus parallel time) for various node counts n on a reallinear regression (left) and logistic regression (right) datasets. The baseline is sequential SGD, whichis identical to PopSGD with node count 1.
Figure 2: PopSGD test accuracy using 32 nodes on Piz Daint, measured at a fixed arbitrary node. The X axismeasures SGD steps per model, whereas the Y axis measures Top-1 accuracy. The dotted red lineis the accuracy of the Torchvision baseline. PopSGD surpasses the test accuracy of the baseline by0.34%, although it processes each sample 4× less times, and each model sees 8× less gradient updates.
Figure 3: PopSGD convergence (training loss at the step versus parallel time) on the synthetic regression taskversus the number of nodes n (left), and versus sequential SGD with different batch sizes (right).
Figure 4: PopSGD train and test accuracy using 32 nodes on Piz Daint, measured at a fixed arbitrary node, fortraining ResNet50 on ImageNet. The round multipler value is mult = 4. The X axis measures SGDsteps per model, whereas the Y axis measures Top-1 accuracy. The dotted red line is the accuracy ofthe Torchvision baseline (Marcel and Rodriguez, 2010). PopSGD is below the test accuracy of thebaseline by < 0.5%.
