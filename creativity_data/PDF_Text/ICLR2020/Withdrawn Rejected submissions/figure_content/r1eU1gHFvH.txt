Figure 1: Schematic for building a random code with known properties. Black circles representones, white circles represent zeros. Class prototypes (P1, P2, and P3) were made with length LP; thenumber of prototypes are np , which is three in this example, their weight is four, with a sparsenessnumber, Sp of 1. Random vectors, Rχ, were made, as shown, these have length LR and there arenR of them; they have weight, WR of two and sparseness number, Sr, of 4. To assemble a newcodeword, a prototype is chosen, in this example, P2, and ‘perturbation’ errors are applied, in thisexample, the perturbation rate is 1, soa single one is turned to a zero in the modified prototype (P2).
Figure 2:	Input data that is few in number and sparse exhibits more local codes. As the prototypevector in all these cases has a sparseness of 1/10, the weight of the random wR and prototype, wP,parts of the codeword and the codewords are 500bits long, note that purple: Sx = 0.2, wR=50,wP = 50; green: Sx = 0.3, wR=100, wP = 50; black dashed: Sx = 0.4, wR=150, wP = 50 Graydashed and dot-dashed lines are drawn at nH LN =500 and 1000 respectfully.
Figure 3:	Experiment 10. The number of local codes increases with training time. The point atwhich the rate of LC code addition decreases correlated with both when the NN achieves ≈99%classification accuracy and when it achieves classification accuracy of more than 0. Key: red: uhln=250; blue: Uhln = 500; black: uhln = 1000; purple: uhln = 2000.
Figure 4:	The number of local codes increases with improvement on generalisation accuracy. Fig.4ais over 250 repeats trained for the same amount of time, Fig.4b are repeated measurements on thesame NNs as they are trained.
Figure 5:	Network architecture parameters can drastically effect the emergence of local codes. 5a:Switching from HLNs with a sigmoidal activation function to a rectified linear (ReLU) units; 5b:Switching from a distributed to a 1-hot output encoding.
Figure 6: ‘Pseudo-deep, NNs that exhibit LCs lose them if modern deep-learning training techniquesare applied.
Figure 7: How noise affects local codes. Fig.7a: perturbing the prototype part of the codeworddecreases the drive to learn local codes. Perturbation, P, is measured as the number of bits inprototype block code that are randomly flipped. Fig.7b: increasing dropout increases the numberof local codes. As the dropout percentage increases, generally, the mean and range of local codesfound increases, suggesting that localized encoding by the network offers some advantage againstnoise.
Figure 8: Examples of interpretable local codes found in a distributed network. Left: a selectivelyon unit with a selectivity of 〜+0.12; Right: selectively off unit with a selectivity of 〜一0.2. Redcircles belong to a single category, blue stars are all the members of all other categories, the x-axisis the activation of a hidden layer neuron (HLN) and points are jittered randomly around 1 on they-axis for ease of viewing. There is a clear separation between activations for the class depicted inred (A) and all other activations (not-A), thus examination of the activations of these units wouldreveal the presence or absence of the red class.
Figure 9: Experiment 4. The effect of changing the input vector length.
Figure 10: Extra data for the dropout tests.
Figure 11: Including activation noise affects the number of local codes, σ is the standard deviationof the Gaussian;y distributed noise. For NN with 1000 HLNs adding noise generally increases thenumber of local codes, for NN with 500 HLNs there is generally no effect or a decrease with highsigma. Lines are drawn to mark the standard error around the mean of LCs at σ = 0, i.e. no addednoise.
Figure 12: Local codes are generally positively correlated with accuracy. Key: red: hhln = 2000;nHLN = 1000; nHLN = 500; nHLN = 250.
Figure 13: Shuffling input data to create ‘memorising networks, reduces the number ofLCs but doesnot completely inhibit them. red: unshuffled data (control); blue shuffled input data.
