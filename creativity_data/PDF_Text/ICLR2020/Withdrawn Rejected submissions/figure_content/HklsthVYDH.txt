Figure 1: Illustration for the hardness of Prob-lem (1). A wrong update direction leads to alimiting cycle and algorithms fail to converge.
Figure 2: An illustration of L2L. We learn a neu-ral network to model the algorithmfor generatingadversarial attack.
Figure 4: Robust accuracy against perturbation mag-nitudes and number of iterations of PGM over CIFAR-100. Top two figures show the absolute accuracy overadversarial samples; the bottom two show the differ-ence accuracy (PGM Net as baseline). For larger per-turbation magnitude see Appendix Ethat L2L attackers are able to generate strong attacks. As can be seen, PGM-100 is strongerthan Grad L2L attacker (47.72% vs. 49.68%), but similar to the 2-Step L2L attacker (52.07% vs.
Figure 5: Illustrative examples of FGSM (Top), PGM-20 (Mid), and 2-Step L2L (Bottom) perturba-tions for a cat under PGM Net and 2-Step L2L with perturbation magnitude = 0.031.
Figure 6: Reward V5. iteration of the trained policy using original GAIL and L2L GAIL.
Figure 7: An illustration example for the architecture of ResBlocks.
Figure 8: An example of the limiting circle: arrows denote the update directionsE Robustness Evaluation ChecklistRecently, there are many works on robustness defense that have been proven ineffective (Athalyeet al., 2018; Carlini et al., 2019). Our work follows the most reliable and widely used robust model14Under review as a conference paper at ICLR 2020approach â€” adversarial training, which finds a set parameters to make the model robust. We donot make any modification to final classifier model. Unlike previous works (e.g. Defense-GAN,Samangouei et al. (2018)), our model does not take the attacker as a part of the final model and doesnot use shattered/obfuscated/masked gradient as a defense mechanism. We also demonstrate thatthe evaluation of the robustness of our proposed L2L method is trustworthy by verifying all itemslisted in (Carlini et al., 2019).
Figure 9: Robust accuracy against perturbation magnitudes of PGM over CIFAR-100.
