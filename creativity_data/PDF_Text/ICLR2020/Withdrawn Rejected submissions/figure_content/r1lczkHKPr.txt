Figure 1: (a) Structure of Composite Q-learning. Target networks are omitted for visibility. Qidenotes the Truncated and Shifted Q-functions at step i and Q the Composite Q-function. In-coming edges yield the targets for the corresponding heads. Edges denoted by γ are discounted.
Figure 2: (a) In this MDP of horizon K, the agent ought to arrive at terminal state sK-1 usingactions {a, b, c}. The initial state is s0 and the optimal policy is given in red. (b) Mean results andtwo standard deviations over 10 runs on the MDP with a horizon ofK = 20. The left plot depicts thevalue of s0 and action a as estimated by the different approaches over time. Dashed lines indicateconvergence to the optimal policy. The predicted Truncated Q-values for state s0 and action a withhorizons 1 to 4, denoted by Tr1 , . . . , Tr4 , and predicted Shifted Q-values for state s0 and action a,denoted by Sh1, . . . , Sh4, are to the right. Dotted lines indicate the true optimal respective Q-values.
Figure 3: Results of four individual runs on the MDP with a horizon of K = 20 for Composite andShifted Q-learning, with different learning rates for the Shifted Q-function (denoted by the numbersin parentheses). The learning rates for the full Q-function and for the Truncated Q-functions are setto 10-3 in all experiments.
Figure 4:	Results of four individual runs on the MDP with a horizon of K = 20 for CompositeQ-learning, with different learning rates for the Truncated Q-function (denoted by the numbers inparentheses). The learning rates for the full Q-function and for the Shifted Q-functions are set to10-3 in all experiments.
Figure 5:	Visualization of Walker2d-v2 (left), Ant-v2 (middle) and Hopper-v2 (right).
Figure 6: Results (top) and TD-errors (bottom) for Walker2d-v2 (left), Ant-v2 (middle) andHopper-v2 (right). The plots show median and interquartile ranges over 11 training runs, each rep-resenting mean evaluation performance over 100 initial states. The lower plots show TD-errors overtime for the different horizons of the Truncated Q-function (denoted by Trh for horizon h) as wellas the TD-errors for the complete Q-estimate. Please note that TD-error here means the deviationfrom the associated target.
Figure 7: Results (top) and normalized area under the learning curve (bottom) for Composite TD3in the Walker2d-v2 environment with different truncation horizons n (left) and different regulariza-tion weights β (right). The plots show median and interquartile ranges over 11 training runs, eachrepresenting mean evaluation performance over 100 initial states.
Figure 8: Results for TD3 and TD3(∆) with the same number of parameters for the critic as Com-posite TD3, i.e. four layers with 500 neurons. The plots show median and interquartile ranges over11 training runs, each representing mean evaluation performance over 100 initial states. We denoteapproaches using a critic with four layers by 4L.
Figure 9: (a) Results for a shallow architecture of the Composite Q-network for the Walker2d-v2environment. The plots show median and interquartile ranges over 11 training runs, each represent-ing mean evaluation performance over 100 initial states. (b) Architecture of the shallow CompositeQ-network.
Figure 10: Results for Composite TD3 and TD3 as in Figure 6.
Figure 11: Results for Composite TD3 and MVE-TD3 as in Figure 6.
Figure 12: Results for Composite TD3 and TD3(∆) as in Figure 6.
