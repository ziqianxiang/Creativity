Figure 1: Different Clients(target) and datasets in the dataserver (source). Images are randomly chosen from S*Target datasetStanford DogsCityscapesKITTISource Dataset: Downsampled ImageNetSource Dataset: COCOSource Dataset: COCOIt is evident that even downloading and storing all these datasets locally may not be affordable foreveryone, let alone pre-training a model on this massive amount of data. Furthermore, for commer-cial applications data licensing may be a financial issue to consider. Recent works (He et al., 2018;Ngiam et al., 2018) have also shown that there is not a “the more the better” relationship between theamount of pre-training data and the downstream task performance. Instead, they demonstrated thatselecting an appropriate subset of the pre-training data was important to achieve good performanceon the target dataset.
Figure 3: Relationship between domainclassifier and proxy task performance onsubsets S.
Figure 4: Transfer learning on object detection and instance segmentation. We report results on Cityscapes (toprow) and KITTI (bottom row) when sampling {20%, 40%, 50%} of MS-COCO images (server).
