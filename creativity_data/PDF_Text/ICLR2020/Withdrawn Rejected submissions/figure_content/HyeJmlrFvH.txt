Figure 3: Training loss on CIFAR10 (left) and ImageNet (middle) for ResNet models. QSGD,QSGDinf, and NUQSGD are trained by simulating the quantization and dequantizing of the gradientsfrom 8-GPUs. On CIFAR10, SGD refers to the single-GPU training versus on Imagenet it refers to2-GPU setup in the original ResNet paper. SGD is shown to highlight the significance of the gapbetween QSGD and QSGDinf. SuperSGD refers to simulating full-precision distributed trainingwithout quantization. SuperSGD is impractical in scenarios with limited bandwidth. (Right) Estimatednormalized variance on CIFAR10 on the trajectory of single-GPU SGD. Variance is measured forfixed model snapshots during training. Notice that the variance for NUQSGD and QSGDinf is lowerthan SGD for almost all the training and it decreases after the learning rate drops.
Figure 4: Scalability behavior for NUQSGD versus the full-precision baseline when training ResNet34and ResNet50 on ImageNet. The ResNet34 graph examines strong scaling (left), splitting a globalbatch of size 256 onto the available GPUs, whereas the ResNet50 graph examines strong scaling(middle) keeping a fixed per-GPU batch size of 16. Each time bar is split into computation (bottom),encoding cost (middle), and transmission cost (top). Notice the significant negative scalability of theSGD baseline in both scenarios. By contrast, the 4-bit communication-compressed implementationachieves positive scaling, while the 8-bit variant stops scaling between 4 and 8 nodes due to thehigher communication and encoding costs. End-to-end training time for ResNet50/ImageNet forNUQSGD and EF-SignSGD versus the SuperSGD baseline (right).
Figure 5: Estimated variance (left) and normalized variance (right) on CIFAR10 on the trajectory ofsingle-GPU SGD. Variance is measured for fixed model snapshots during training. Notice that thevariance for NUQSGD and QSGDinf is lower than SGD for almost all the training and it decreasesafter the learning rate drops. All methods except SGD simulate training using 8 GPUs. SuperSGDapplies no quantization to the gradients and represents the lowest variance we could hope to achieve.
Figure 6: Accuracy on the hold-out set on CIFAR10 (left) and on ImageNet (right) for trainingResNet models from random initialization until convergence. For CIFAR10, the hold-out set is thetest set and for ImageNet, the hold-out set is the validation set.
Figure 7: Estimated normalized variance on CIFAR10 (left) and ImageNet (right). For differentmethods, the variance is measured on their own trajectories. Note that the normalized variance ofNUQSGD and QSGDinf is lower than SGD for almost the entire training. It decreases on CIFAR10after the learning rate drops and does not grow as much as SGD on ImageNet. Since the variancedepends on the optimization trajectory, these curves are not directly comparable. Rather the generaltrend should be studied.
Figure 8: Scalability behavior for NUQSGD versus the full-precision baseline when trainingResNet152 on ImageNet.
Figure 9: End-to-end training time for ResNet50/ImageNet for NUQSGD and EF-SignSGD versusthe SGD baseline.
Figure 10: Optimal value of problem Q1 versus p ∈ [0, 1] for exponentially spaced collection oflevels of the form (0,ps,…，p2,p, 1).
