Figure 1: The surfaces of five binary classification losses derived by perturbing the predictions Yi ,Yj of twoinstances inside a random mini-batch Y ∈ R10,Y ∈ {0,1}10, with true targets Yi = 0, Yj = 1. AUC, AP andF1 are converted to losses via 1 — x, while the positive class for MCR, EER and F1 is estimated as Y ≥ 0.
Figure 2: A single-parameter prediction model y(x, α) = αx - 1 classifies a single-feature binary dataset(leftmost plot), i.e. α ∈ R,x ∈ RN ×1, where initially α = 0.3. In the three rightmost plots, the x-axis showsthe variation in true (cyan) and surrogate (magenta) losses for the whole space of a. The plots indicate thatEquation 5 forces the surrogate to approximate the true loss at the regions around the current α (dashed verticalline), while the parameter α is updated towards the minimum of the surrogate by Equation 4.
Figure 3: Illustrating the convergence for different loss functions on the IJCNN dataset, using two types ofʌsurrogates, SL-S: randomly from scratch, SL-R: refined from the universal surrogate. ' and L represent thesurrogate optimization performance on the training set, while 'Test the true loss on the tesing set.
