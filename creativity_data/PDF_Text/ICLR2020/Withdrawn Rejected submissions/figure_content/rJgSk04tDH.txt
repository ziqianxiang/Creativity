Figure 1: We consider four difficult tasks, where the agent (magenta) is a simulated quadrupedalrobot. In AntMaze, the agent must navigate to the end of a U-shaped corridor (target given by greenarrow); in AntPush, the agent must navigate to the target by first pushing a block obstacle to theright; in AntBlock and AntBlockMaze, the agent must push a small red block to the target location;see Nachum et al. (2018b) for more details. Task success rates are plotted for three HRL algorithms—HIRO (Nachum et al., 2018a), HIRO with goal relabelling (inspired by Levy et al. (2017)), andOptions (Frans et al., 2018) — and shallow (non-hierarchical) agents with and without the use ofmulti-step rewards (n-step returns) over 10M training steps, averaged over 5 seeds. In this work,we isolate and evaluate the key properties of hierarchy which yield the stark difference in empiricalperformance between HRL and non-HRL methods.
Figure 2: We present the results for different HRL methods while changing the temporal abstractionused for training (ctrain , top) or the temporal abstraction used for experience collection (cexpl, bot-tom). Average success rates and standard errors are calculated for 5 randomly seeded runs, trainedfor 10M steps with early stopping. Recall that our HRL baselines use ctrain = cexpl = 10. Whenvarying ctrain, we find that the choice of horizon matters only so far as ctrain > 1. For cexpl, whilethere exists correlation between performance and temporal abstraction, using no temporal abstrac-tion (cexpl = 1) can still make non-negligible progress compared to the shallow policies in Figure 1.
Figure 3: We evaluate and compare the performance of training a non-hierarchical shadow agenttrained on experience collected by a hierarchical agent, thus disentangling the potential benefits ofHRL for exploration from the potential benefits of HRL for training. In all environments exceptAntMaze, the shadow agent can achieve performance competitive with HRL, given an appropriatemulti-step reward horizon (crew = 3 performs best). Overall, this suggests that the effect of hier-archy on ease of training (as opposed to exploration) is modest, and can mostly be replicated by anon-hierarchical agent given good experience and the use of multi-step rewards.
Figure 4: We compare the performance of HRL to Explore & Exploit (E&E) and Switching En-Semble (SE) - two non-hierarChical exploration methods that make use of HRL-inspired temporallyextended modulation of behaviors (length of modulation given by cswitch). We find that the non-hierarchical methods are able to match the performance of HRL on these tasks (with the only excep-tions being Explore & Exploit on AntBlockMaze and Switching Ensemble on AntPush), suggestingthat exploration is the key to success on these tasks. These results also make clear the importance oftemporally extended exploration; using cswitch > 1 is almost always better than cswitch = 1.
Figure 5: A summary of our conclusions on the benefits of hierarchy.
Figure 6: A diagram showing the structural form of an HRL agent. Every C steps, a higher-levelpolicy ∏hi chooses a high-level action gt. In the options framework, this high-level action is anidentifier, choosing which of m options to activate. In goal-conditioned HRL, the high-level actionis a goal-state. In either case, a lower-level policy ∏io is used to produce a sequence of atomic actionsat. After c steps, control is returned to the higher-level policy.
Figure 7: We evaluate the importance of modularity - in this case, using separate networks for sep-arate policies. We find that using separate networks is consistently better, suggesting that modularityis important for both HRL and HRL-like methods.
Figure 8: We expand on the results from Figure 3, evaluating and comparing the performance oftraining a non-hierarchical shadow agent trained on experience collected by a hierarchical agent.
Figure 9: Results of running a two-sided t-test on our experimental results. The sign designates thedirection of the t-test result; i.e., a negative sign for A vs. B indicates that the mean of the results forA is less than that of B.
