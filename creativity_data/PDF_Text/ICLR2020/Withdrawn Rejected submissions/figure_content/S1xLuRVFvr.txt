Figure 1: An overview of activation decomposi-tion. The overall activation map on each imagehighlights the regions contributing the most to thesimilarity. The partial activation map highlightsthe regions in one image that have large activa-tion responses on a specific position in the otherimage, e.g. mouth or eye.
Figure 2: Activation decomposition on a simple architecture for deep metric learning.
Figure 3: Maximum mask for GMP. Zis the constant normalization term forGAP.
Figure 4: Qualitative results on CUB.
Figure 5: Qualitative results for model diagnosis.
Figure 6: Example of cross-view pattern discovery, e.g. image orientation estimation. (Best viewed in color)When the metric learning is applied to cross-view applications, e.g. image retrieval (Zhai et al.
Figure 7: Estimation error distri-bution of overall and partial map.
Figure 8: Top retrieved images by interactive retrieval on face and re-ID datasets. The red box onthe left column indicates the query part.
Figure 10: Examples of cross-view pattern discovery, i.e., image orientation estimation. (Best viewed in color)13Under review as a conference paper at ICLR 2020A.6.3 Interactive Retrieval on Person Re-IDA.7 Cross-view Pattern Discovery DetailsA.7.1 Explanation of Figure 690°360°0°180°270°Partial∖.	90.2°0°Ground TruthRotation AngleAligned Image Pair180°302.1°122.1°
Figure 13: Example of cross-view pattern discovery, e.g. image orientation estimation. (Best viewed in color)Here we explain Fig. 6 again (Fig. 13) with more details. We first reiterate the application andexperiment setting. The dataset (CVUSA) we used in the experiment is for image-based cross-view geo-localization (Zhai et al. (2017); Hu et al. (2018)) and contains image pairs from street andaerial views. Each matched image pair corresponds to the same GPS location. The objective ofgeo-localization is to find the best matching GPS-tagged aerial-view image in a reference datasetfor a query street-view image. In the CVUSA dataset (Zhai et al. (2017)), the image pairs arealigned in terms of orientation. For example, as shown in Fig. 13, the left two images (street-view panorama and aerial-view images) are the original image pair and the yellow line denotes the0。which corresponds to the south direction (180。corresponds to the North direction). We use[0,360] ◦ to denote different angles as marked on these two images.
