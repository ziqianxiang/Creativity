Figure 1: Illustration of our proposed SSE-PT modelTransformer Encoder On top of the embedding layer, we have B blocks of self-attention layersand fully connected layers, where each layer extracts features for each time step based on theprevious layerâ€™s outputs. Since this part is identical to the Transformer encoder used in the originalpapers (Vaswani et al., 2017; Kang and McAuley, 2018), we will skip the details.
Figure 2: Illustration of how SASRec (Left) and SSE-PT (Right) differs on utilizing the EngagementHistory of A Random User in Movielens1M Dataset.
