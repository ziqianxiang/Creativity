Figure 1: Knowledge Distillation on BERT with smaller student vocabulary. (Left) A pre-trainedteacher BERT model with default BERT parameters (e.g., 30K vocab, 768 hidden state dimension).
