Figure 1: Validation accuracy during the training process on VGG-16 (a, b) and ResNet-18 (c, d)for winning tickets, boosting tickets, and randomly initialized weights. In both models, the boostingtickets show faster convergence rate and equally good performance as the winning tickets.
Figure 2: Validation accuracy when the total number of epochs are 20, 40, 60, 80, 100 for boththe boosting tickets (straight lines) and winning tickets (dash lines) on VGG-16. Plot (a) and (b)contains the validation accuracy for all the training epochs in different scales. Plot (c,d,e,f) comparethe validation accuracy between models trained for fewer epochs and the one for 100 epochs.
Figure 3: The final test accuracy achieved whentotal number of epochs vary from 20 to 100 onfour different tickets. Each line denotes one win-ning ticket found by learning rate 0.005, 0.01,0.05, and 0.1 for VGG-16 (a) and ResNet-18 (b).
Figure 4: Under various pruning ratios, the changesof validation accuracy after the first and fifth train-ing epoch, trained from the original initializedweights of boosting tickets and randomly reinitial-ized ones for VGG-16 (a) and ResNet-18 (b).
Figure 5: Plot (a) and (b) correspond to boosting tickets for various of model widths. Plot (c) and(d) correspond to boosting tickets for various of model depths. While a wider model always boostsfaster, deep models have similar boosting effect when the depth is large enough.
Figure 6: The clean accuracy (a) and robust accuracy(b) of pruned models on the validation set. The mod-els are pruned based on different training methods(natural training, FGSM-based adversarial training,and PGD-based adversarial training). For each ob-tained boosting ticket, it is retrained with PGD-basedadversarial training with 100 training epochs.
Figure 7: We show clean (a,c) and robust accuracy (b,d) for both winning tickets and randomlyinitialized weights on LeNet (a,b) and Vgg-16 (c,d) on MNIST with adversarial training.
Figure 8: Validation robust accuracy of pruned models with PGD-based adversarial training onVGG-16 where the total number of epochs are 20, 40, 60, 80, 100 respectively. Plot (a) and (b)show all the results while plot (c), (d), (e), (f) compare each model with the baseline model. Thebaseline model is obtained by 100-epoch PGD-based adversarial training on the original full model.
Figure 9: We compare tickets obtained via iterative pruning and one shot pruning on VGG-16 (left)and ResNet-18 (right). We plot the validation accuracy of models from both approaches and thecorresponding randomly initialized models.
