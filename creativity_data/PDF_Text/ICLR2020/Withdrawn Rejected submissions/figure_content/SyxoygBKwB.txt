Figure 1: Influence of parameters and α on the maximal L2 norm of ∆: maxi ∣δi ∣2 . On the left,attacks are computed on HERO. On the right, they are computed on Space Invaders.
Figure 2: Influence of parameters and α on the average behavior of the agent playing Space Invadersunder CopyCAT attack . On the left, the attack success rate. On the right, the average cumulativereward.
Figure 3: Influence of parameters and α on the average behavior of the agent playing HERO underCopyCAT attack . On the left, the attack success rate. On the right, the average cumulative reward.
Figure 5: CopyCAT against FGSM for policy targeted attacks in Air Raid. On the left, the attacksuccess rate. On the right, the average cumulative reward under attack.
Figure 4: CopyCAT against FGSM for policy targeted attacks in HERO. On the left, the attacksuccess rate. On the right, the average cumulative reward under attack.
Figure 6: DQN playing Space Invaders. On the left, the unattacked current observation. In the middle,a mask of CopyCAT is applied to lure the agent into taking the ”no-op” action. Parameters used are:e = 3 ∙ 10-2, α = 1.3 ∙ 10-5. The L? norm of the corresponding mask (shown on the right) is 0.78.
Figure 7: Untrained DQN attacked by CopyCAT to follow Rainbow’s policy.
Figure 8: CopyCAT attacking DQN on Air Raid, in the black-box setting to make it match Rainbow’spolicy.
Figure 9: The rescaled attack for the target class “tiger_shark”.
Figure 10: Top: images from the test set, unseen during the training of the attack. Down: attackedimages: (image + attack). Below each image, the label predicted by VGG16.
Figure 11: DQN attacked by CopyCAT to follow Rainbow’s policy. Comparison to FGSM anditerative-FGSM.
