Figure 1: Layer-wise performance of distilled BERT-12 embeddings for all possible choices of f, gwith N = 100000.
Figure 2: Layer-wise bias of distilled BERT-12 embeddings for f= mean, g = mean, N = 100000Left: Gender, Center: Race, Right: Religion	B, P	GE, P	GC, P	Gender M, P	GE	GC	M	Race M	GE	Religion GC	MWord2Vec	0.0503	0.1758	0.075	0.2403	0.1569	0.0677	0.2163	0.0672	0.0907	0.053	0.14GloVe	0.0801	0.3534	0.0736	0.1964	0.357	0.0734	0.1557	0.1171	0.2699	0.0702	0.0756BERT-12	0.0736	0.3725	0.0307	0.3186	0.2868	0.0254	0.3163	0.2575	1.2349	0.0604	0.2955BERT-24	0.0515	0.6418	0.0462	0.234	0.4674	0.0379	0.2284	0.1956	0.6476	0.0379	0.2316GPT2-12	0.4933	25.8743	0.0182	0.6464	2.0771	0.0062	0.7426	0.6532	4.5282	0.0153	0.776GPT2-24	0.6871	40.1423	0.0141	0.8514	2.3244	0.0026	0.9019	0.8564	8.9528	0.0075	0.9081RoBERTa-12	0.0412	0.2923	0.0081	0.8546	0.2077	0.0057	0.8551	0.8244	0.4356	0.0111	0.844RoBERTa-24	0.0459	0.3771	0.0089	0.7879	0.2611	0.0064	0.783	0.7479	0.5905	0.0144	0.7636XLNet-12	0.0838	1.0954	0.0608	0.3374	0.6661	0.042	0.34	0.2792	0.8537	0.0523	0.318XLNet-24	0.0647	0.7644	0.0407	0.381	0.459	0.0268	0.373	0.328	0.8009	0.0505	0.368DistilBERT-6	0.0504	0.5435	0.0375	0.3182	0.3343	0.0271	0.3185	0.2786	0.8128	0.0437	0.3106Table 3: Social bias within static embeddings from different pretrained models with respect to a setof professions Nprof. Parameters are set as f = mean, g = mean, N = 100000 and the layer ofthe pretrained model used in distillation is ［鲁C. Lowest bias in a particular column is denoted inbold.
Figure 3: Layerwise Performance of BERT-24 static embeddings for all Possible choices of f, g19Under review as a conference paper at ICLR 2020C GPT-2Figure 4: Layerwise performance of GPT2-12 static embeddings for all possible choices of f, gFigure 5: Layerwise performance of GPT-24 static embeddings for all possible choices of f, g20Under review as a conference paper at ICLR 2020Model	N	RG65	WS353	SimLex999	SimVerb3500Word2Vec	-	0.6787	0.6838	0.4420	0.3636GloVe	-	0.6873	0.6073	0.3705	0.2271GPT2-12	10000	0.2843 (0)	0.4205 (1)	0.2613 (2)	0.1472 (6)GPT2-12	50000	0.5000 (2)	0.5815 (1)	0.4378 (2)	0.2607 (2)GPT2-12	100000	0.5156 (1)	0.6396 (0)	0.4547 (2)	0.3128 (6)GPT2-24	10000	0.3149 (0)	0.5209 (0)	0.2940 (0)	0.1697 (0)GPT2-24	50000	0.5362 (2)	0.6486 (0)	0.4350 (0)	0.2721 (0)GPT2-24	100000	0.5328 (1)	0.6830 (0)	0.4505 (3)	0.3056 (0)Table 4: Performance of Static Embeddings on Word Similarity and Word Relatedness Tasks. f andg are set to mean for all GPT2-models and (#) indicates the layer the embeddings are distilled from.
Figure 4: Layerwise performance of GPT2-12 static embeddings for all possible choices of f, gFigure 5: Layerwise performance of GPT-24 static embeddings for all possible choices of f, g20Under review as a conference paper at ICLR 2020Model	N	RG65	WS353	SimLex999	SimVerb3500Word2Vec	-	0.6787	0.6838	0.4420	0.3636GloVe	-	0.6873	0.6073	0.3705	0.2271GPT2-12	10000	0.2843 (0)	0.4205 (1)	0.2613 (2)	0.1472 (6)GPT2-12	50000	0.5000 (2)	0.5815 (1)	0.4378 (2)	0.2607 (2)GPT2-12	100000	0.5156 (1)	0.6396 (0)	0.4547 (2)	0.3128 (6)GPT2-24	10000	0.3149 (0)	0.5209 (0)	0.2940 (0)	0.1697 (0)GPT2-24	50000	0.5362 (2)	0.6486 (0)	0.4350 (0)	0.2721 (0)GPT2-24	100000	0.5328 (1)	0.6830 (0)	0.4505 (3)	0.3056 (0)Table 4: Performance of Static Embeddings on Word Similarity and Word Relatedness Tasks. f andg are set to mean for all GPT2-models and (#) indicates the layer the embeddings are distilled from.
Figure 5: Layerwise performance of GPT-24 static embeddings for all possible choices of f, g20Under review as a conference paper at ICLR 2020Model	N	RG65	WS353	SimLex999	SimVerb3500Word2Vec	-	0.6787	0.6838	0.4420	0.3636GloVe	-	0.6873	0.6073	0.3705	0.2271GPT2-12	10000	0.2843 (0)	0.4205 (1)	0.2613 (2)	0.1472 (6)GPT2-12	50000	0.5000 (2)	0.5815 (1)	0.4378 (2)	0.2607 (2)GPT2-12	100000	0.5156 (1)	0.6396 (0)	0.4547 (2)	0.3128 (6)GPT2-24	10000	0.3149 (0)	0.5209 (0)	0.2940 (0)	0.1697 (0)GPT2-24	50000	0.5362 (2)	0.6486 (0)	0.4350 (0)	0.2721 (0)GPT2-24	100000	0.5328 (1)	0.6830 (0)	0.4505 (3)	0.3056 (0)Table 4: Performance of Static Embeddings on Word Similarity and Word Relatedness Tasks. f andg are set to mean for all GPT2-models and (#) indicates the layer the embeddings are distilled from.
Figure 6: Layerwise performance of RoBERTa-12 static embeddings for all possible choices of f, g21Under review as a conference paper at ICLR 2020Figure 7: Layerwise performance of RoBERTa-24 static embeddings for all possible choices of f, gModel	N	RG65	WS353	SimLex999	SimVerb3500Word2Vec	-	0.6787	0.6838	0.4420	0.3636GloVe	-	0.6873	0.6073	0.3705	0.2271RoBERTa-12	10000	0.5719 (0)	0.6618 (0)	0.4794 (0)	0.3968 (0)RoBERTa-12	50000	0.6754 (0)	0.6867 (0)	0.501 (0)	0.4123 (0)RoBERTa-12	100000	0.6597 (0)	0.6915 (0)	0.5098 (0)	0.4206 (0)RoBERTa-12	500000	0.6675 (0)	0.6979 (0)	0.5268 (5)	0.4311 (0)RoBERTa-12	1000000	0.6761 (0)	0.7018 (0)	0.5374 (5)	0.4442 (4)RoBERTa-24	10000	0.5469 (1)	0.6144 (0)	0.4499 (0)	0.3403 (0)RoBERTa-24	50000	0.6837 (1)	0.6412 (0)	0.4855 (0)	0.371 (0)RoBERTa-24	100000	0.7087 (7)	0.6563 (6)	0.4959 (0)	0.3802 (0)RoBERTa-24	500000	0.7557 (8)	0.663 (6)	0.5184 (18)	0.412 (6)RoBERTa-24	1000000	0.739 (8)	0.6673 (6)	0.5318 (18)	0.4303 (9)Table 5: Performance of Static Embeddings on Word Similarity and Word Relatedness Tasks. f andg are set to mean for all RoBERTa-models and (#) indicates the layer the embeddings are distilledfrom. Bold indicates best performing embeddings for a given dataset.
Figure 7: Layerwise performance of RoBERTa-24 static embeddings for all possible choices of f, gModel	N	RG65	WS353	SimLex999	SimVerb3500Word2Vec	-	0.6787	0.6838	0.4420	0.3636GloVe	-	0.6873	0.6073	0.3705	0.2271RoBERTa-12	10000	0.5719 (0)	0.6618 (0)	0.4794 (0)	0.3968 (0)RoBERTa-12	50000	0.6754 (0)	0.6867 (0)	0.501 (0)	0.4123 (0)RoBERTa-12	100000	0.6597 (0)	0.6915 (0)	0.5098 (0)	0.4206 (0)RoBERTa-12	500000	0.6675 (0)	0.6979 (0)	0.5268 (5)	0.4311 (0)RoBERTa-12	1000000	0.6761 (0)	0.7018 (0)	0.5374 (5)	0.4442 (4)RoBERTa-24	10000	0.5469 (1)	0.6144 (0)	0.4499 (0)	0.3403 (0)RoBERTa-24	50000	0.6837 (1)	0.6412 (0)	0.4855 (0)	0.371 (0)RoBERTa-24	100000	0.7087 (7)	0.6563 (6)	0.4959 (0)	0.3802 (0)RoBERTa-24	500000	0.7557 (8)	0.663 (6)	0.5184 (18)	0.412 (6)RoBERTa-24	1000000	0.739 (8)	0.6673 (6)	0.5318 (18)	0.4303 (9)Table 5: Performance of Static Embeddings on Word Similarity and Word Relatedness Tasks. f andg are set to mean for all RoBERTa-models and (#) indicates the layer the embeddings are distilledfrom. Bold indicates best performing embeddings for a given dataset.
Figure 8: Layerwise performance of XLNet-12 static embeddings for all possible choices of f, gFigure 9: Layerwise performance of XLNet-24 static embeddings for all possible choices of f, g23Under review as a conference paper at ICLR 2020Model	N	RG65	WS353	SimLex999	SimVerb3500Word2Vec	-	0.6787	0.6838	0.4420	0.3636GloVe	-	0.6873	0.6073	0.3705	0.2271XLNet-12	10000	0.604 (0)	0.6482 (0)	0.483 (0)	0.3916 (0)XLNet-12	50000	0.6056 (1)	0.6571 (0)	0.5157 (1)	0.3973 (1)XLNet-12	100000	0.6239 (1)	0.6629 (0)	0.5185 (1)	0.4044 (3)XLNet-12	500000	0.6391 (3)	0.6937 (3)	0.5392 (3)	0.4747 (4)XLNet-12	1000000	0.6728 (3)	0.7018 (3)	0.5447 (4)	0.4918 (4)XLNet-24	10000	0.6525 (0)	0.6935 (0)	0.5054 (0)	0.4332 (1)XLNet-24	50000	0.6556 (0)	0.6926 (0)	0.5377 (5)	0.4492 (3)XLNet-24	100000	0.6522 (3)	0.7021 (3)	0.5503 (6)	0.4545 (3)XLNet-24	500000	0.66 (0)	0.7378 (6)	0.581 (8)	0.5095 (6)XLNet-24	1000000	0.7119 (6)	0.7446 (7)	0.5868 (9)	0.525 (6)Table 6: Performance of Static Embeddings on Word Similarity and Word Relatedness Tasks. fand g are set to mean for all XLNet-models and (#) indicates the layer the embeddings are distilledfrom. Bold indicates best performing embeddings for a given dataset.
Figure 9: Layerwise performance of XLNet-24 static embeddings for all possible choices of f, g23Under review as a conference paper at ICLR 2020Model	N	RG65	WS353	SimLex999	SimVerb3500Word2Vec	-	0.6787	0.6838	0.4420	0.3636GloVe	-	0.6873	0.6073	0.3705	0.2271XLNet-12	10000	0.604 (0)	0.6482 (0)	0.483 (0)	0.3916 (0)XLNet-12	50000	0.6056 (1)	0.6571 (0)	0.5157 (1)	0.3973 (1)XLNet-12	100000	0.6239 (1)	0.6629 (0)	0.5185 (1)	0.4044 (3)XLNet-12	500000	0.6391 (3)	0.6937 (3)	0.5392 (3)	0.4747 (4)XLNet-12	1000000	0.6728 (3)	0.7018 (3)	0.5447 (4)	0.4918 (4)XLNet-24	10000	0.6525 (0)	0.6935 (0)	0.5054 (0)	0.4332 (1)XLNet-24	50000	0.6556 (0)	0.6926 (0)	0.5377 (5)	0.4492 (3)XLNet-24	100000	0.6522 (3)	0.7021 (3)	0.5503 (6)	0.4545 (3)XLNet-24	500000	0.66 (0)	0.7378 (6)	0.581 (8)	0.5095 (6)XLNet-24	1000000	0.7119 (6)	0.7446 (7)	0.5868 (9)	0.525 (6)Table 6: Performance of Static Embeddings on Word Similarity and Word Relatedness Tasks. fand g are set to mean for all XLNet-models and (#) indicates the layer the embeddings are distilledfrom. Bold indicates best performing embeddings for a given dataset.
Figure 10: Layerwise performance of DistilBERT-6 static embeddings for all possible choices off, g24Under review as a conference paper at ICLR 2020Model	N	RG65	WS353	SimLex999	SimVerb3500Word2Vec	-	0.6787	0.6838	0.4420	0.3636GloVe	-	0.6873	0.6073	0.3705	0.2271DistilBERT-6	10000	0.57 (0)	0.6828 (1)	0.4705 (0)	0.2971 (0)DistilBERT-6	50000	0.7257 (1)	0.6928 (1)	0.5043 (0)	0.3121 (0)DistilBERT-6	100000	0.7245 (1)	0.7164 (1)	0.5077 (0)	0.3207 (1)DistilBERT-6	500000	0.7363 (1)	0.7239 (1)	0.5093 (0)	0.3444 (2)DistilBERT-6	1000000	0.7443 (1)	0.7256 (1)	0.5095 (0)	0.3536 (3)Table 7: Performance of Static Embeddings on Word Similarity and Word Relatedness Tasks. f andg are set to mean for all DistilBERT-models and (#) indicates the layer the embeddings are distilledfrom. Bold indicates best performing embeddings for a given dataset.
Figure 11: Layerwise bias of BERT-24 static embeddings for f = mean, g = mean, N = 100000Left: Gender, Center: Race, Right: Religion25Under review as a conference paper at ICLR 2020Figure 12: Layerwise bias of GPT2-12 static embeddings for f = mean, g = mean, N = 100000Left: Gender, Center: Race, Right: ReligionFigure 13: Layerwise bias of GPT2-24 static embeddings for f = mean, g = mean, N =100000Left: Gender, Center: Race, Right: Religion26Under review as a conference paper at ICLR 2020Figure 14: Layerwise bias of RoBERTa-12 static embeddings for f = mean, g = mean, N =100000Left: Gender, Center: Race, Right: ReligionFigure 15: Layerwise bias of RoBERTa-24 static embeddings for f = mean, g = mean, N =100000Left: Gender, Center: Race, Right: Religion27Under review as a conference paper at ICLR 2020Figure 16: Layerwise bias of XLNet-12 static embeddings for f = mean, g = mean, N = 100000
Figure 12: Layerwise bias of GPT2-12 static embeddings for f = mean, g = mean, N = 100000Left: Gender, Center: Race, Right: ReligionFigure 13: Layerwise bias of GPT2-24 static embeddings for f = mean, g = mean, N =100000Left: Gender, Center: Race, Right: Religion26Under review as a conference paper at ICLR 2020Figure 14: Layerwise bias of RoBERTa-12 static embeddings for f = mean, g = mean, N =100000Left: Gender, Center: Race, Right: ReligionFigure 15: Layerwise bias of RoBERTa-24 static embeddings for f = mean, g = mean, N =100000Left: Gender, Center: Race, Right: Religion27Under review as a conference paper at ICLR 2020Figure 16: Layerwise bias of XLNet-12 static embeddings for f = mean, g = mean, N = 100000Left: Gender, Center: Race, Right: ReligionFigure 17: Layerwise bias of XLNet-24 static embeddings for f = mean, g = mean, N = 100000Left: Gender, Center: Race, Right: Religion28
Figure 13: Layerwise bias of GPT2-24 static embeddings for f = mean, g = mean, N =100000Left: Gender, Center: Race, Right: Religion26Under review as a conference paper at ICLR 2020Figure 14: Layerwise bias of RoBERTa-12 static embeddings for f = mean, g = mean, N =100000Left: Gender, Center: Race, Right: ReligionFigure 15: Layerwise bias of RoBERTa-24 static embeddings for f = mean, g = mean, N =100000Left: Gender, Center: Race, Right: Religion27Under review as a conference paper at ICLR 2020Figure 16: Layerwise bias of XLNet-12 static embeddings for f = mean, g = mean, N = 100000Left: Gender, Center: Race, Right: ReligionFigure 17: Layerwise bias of XLNet-24 static embeddings for f = mean, g = mean, N = 100000Left: Gender, Center: Race, Right: Religion28Under review as a conference paper at ICLR 2020Figure 18: Layerwise bias of DistilBERT-6 static embeddings for f = mean, g = mean, N =
Figure 14: Layerwise bias of RoBERTa-12 static embeddings for f = mean, g = mean, N =100000Left: Gender, Center: Race, Right: ReligionFigure 15: Layerwise bias of RoBERTa-24 static embeddings for f = mean, g = mean, N =100000Left: Gender, Center: Race, Right: Religion27Under review as a conference paper at ICLR 2020Figure 16: Layerwise bias of XLNet-12 static embeddings for f = mean, g = mean, N = 100000Left: Gender, Center: Race, Right: ReligionFigure 17: Layerwise bias of XLNet-24 static embeddings for f = mean, g = mean, N = 100000Left: Gender, Center: Race, Right: Religion28Under review as a conference paper at ICLR 2020Figure 18: Layerwise bias of DistilBERT-6 static embeddings for f = mean, g = mean, N =100000Left: Gender, Center: Race, Right: ReligionG.2 Adjective Results	B, P	GE, P	GC, P	Gender M, P	GE	GC	M	Race M	GE	Religion GC	MWord2Vec	0.0482	0.1656	0.0435	0.1347	0.1247	0.0343	0.1178	0.0661	0.13	0.0434	0.1264
Figure 15: Layerwise bias of RoBERTa-24 static embeddings for f = mean, g = mean, N =100000Left: Gender, Center: Race, Right: Religion27Under review as a conference paper at ICLR 2020Figure 16: Layerwise bias of XLNet-12 static embeddings for f = mean, g = mean, N = 100000Left: Gender, Center: Race, Right: ReligionFigure 17: Layerwise bias of XLNet-24 static embeddings for f = mean, g = mean, N = 100000Left: Gender, Center: Race, Right: Religion28Under review as a conference paper at ICLR 2020Figure 18: Layerwise bias of DistilBERT-6 static embeddings for f = mean, g = mean, N =100000Left: Gender, Center: Race, Right: ReligionG.2 Adjective Results	B, P	GE, P	GC, P	Gender M, P	GE	GC	M	Race M	GE	Religion GC	MWord2Vec	0.0482	0.1656	0.0435	0.1347	0.1247	0.0343	0.1178	0.0661	0.13	0.0434	0.1264GloVe	0.095	0.2206	0.0403	0.1289	0.2017	0.0355	0.1108	0.0714	0.2341	0.0606	0.0675BERT-12	0.0506	0.2637	0.0213	0.2684	0.1879	0.0175	0.2569	0.2358	0.8858	0.0365	0.2677BERT-24	0.0389	0.4405	0.0277	0.199	0.2978	0.0248	0.189	0.1768	0.5505	0.0316	0.212
Figure 16: Layerwise bias of XLNet-12 static embeddings for f = mean, g = mean, N = 100000Left: Gender, Center: Race, Right: ReligionFigure 17: Layerwise bias of XLNet-24 static embeddings for f = mean, g = mean, N = 100000Left: Gender, Center: Race, Right: Religion28Under review as a conference paper at ICLR 2020Figure 18: Layerwise bias of DistilBERT-6 static embeddings for f = mean, g = mean, N =100000Left: Gender, Center: Race, Right: ReligionG.2 Adjective Results	B, P	GE, P	GC, P	Gender M, P	GE	GC	M	Race M	GE	Religion GC	MWord2Vec	0.0482	0.1656	0.0435	0.1347	0.1247	0.0343	0.1178	0.0661	0.13	0.0434	0.1264GloVe	0.095	0.2206	0.0403	0.1289	0.2017	0.0355	0.1108	0.0714	0.2341	0.0606	0.0675BERT-12	0.0506	0.2637	0.0213	0.2684	0.1879	0.0175	0.2569	0.2358	0.8858	0.0365	0.2677BERT-24	0.0389	0.4405	0.0277	0.199	0.2978	0.0248	0.189	0.1768	0.5505	0.0316	0.212GPT2-12	0.4631	26.0809	0.0176	0.6126	2.1238	0.0068	0.7101	0.621	4.4775	0.0152	0.7525GPT2-24	0.6707	40.4664	0.0141	0.8367	2.1771	0.0023	0.89	0.843	8.3889	0.0064	0.9006RoBERTa-12	0.0381	0.1754	0.005	0.8472	0.1649	0.0046	0.8444	0.8153	0.2608	0.0069	0.8387RoBERTa-24	0.0248	0.2626	0.0064	0.7647	0.1821	0.0048	0.7562	0.73	0.4492	0.0117	0.7472XLNet-12	0.0399	0.6265	0.0312	0.2214	0.3354	0.0237	0.2196	0.1911	0.4716	0.0321	0.2549
Figure 17: Layerwise bias of XLNet-24 static embeddings for f = mean, g = mean, N = 100000Left: Gender, Center: Race, Right: Religion28Under review as a conference paper at ICLR 2020Figure 18: Layerwise bias of DistilBERT-6 static embeddings for f = mean, g = mean, N =100000Left: Gender, Center: Race, Right: ReligionG.2 Adjective Results	B, P	GE, P	GC, P	Gender M, P	GE	GC	M	Race M	GE	Religion GC	MWord2Vec	0.0482	0.1656	0.0435	0.1347	0.1247	0.0343	0.1178	0.0661	0.13	0.0434	0.1264GloVe	0.095	0.2206	0.0403	0.1289	0.2017	0.0355	0.1108	0.0714	0.2341	0.0606	0.0675BERT-12	0.0506	0.2637	0.0213	0.2684	0.1879	0.0175	0.2569	0.2358	0.8858	0.0365	0.2677BERT-24	0.0389	0.4405	0.0277	0.199	0.2978	0.0248	0.189	0.1768	0.5505	0.0316	0.212GPT2-12	0.4631	26.0809	0.0176	0.6126	2.1238	0.0068	0.7101	0.621	4.4775	0.0152	0.7525GPT2-24	0.6707	40.4664	0.0141	0.8367	2.1771	0.0023	0.89	0.843	8.3889	0.0064	0.9006RoBERTa-12	0.0381	0.1754	0.005	0.8472	0.1649	0.0046	0.8444	0.8153	0.2608	0.0069	0.8387RoBERTa-24	0.0248	0.2626	0.0064	0.7647	0.1821	0.0048	0.7562	0.73	0.4492	0.0117	0.7472XLNet-12	0.0399	0.6265	0.0312	0.2214	0.3354	0.0237	0.2196	0.1911	0.4716	0.0321	0.2549XLNet-24	0.0468	0.5423	0.025	0.3307	0.2697	0.0153	0.3144	0.2871	0.4318	0.0282	0.3235DistilBERT-6	0.0353	0.4274	0.0247	0.2825	0.2461	0.0185	0.2824	0.2603	0.6842	0.035	0.2994
Figure 18: Layerwise bias of DistilBERT-6 static embeddings for f = mean, g = mean, N =100000Left: Gender, Center: Race, Right: ReligionG.2 Adjective Results	B, P	GE, P	GC, P	Gender M, P	GE	GC	M	Race M	GE	Religion GC	MWord2Vec	0.0482	0.1656	0.0435	0.1347	0.1247	0.0343	0.1178	0.0661	0.13	0.0434	0.1264GloVe	0.095	0.2206	0.0403	0.1289	0.2017	0.0355	0.1108	0.0714	0.2341	0.0606	0.0675BERT-12	0.0506	0.2637	0.0213	0.2684	0.1879	0.0175	0.2569	0.2358	0.8858	0.0365	0.2677BERT-24	0.0389	0.4405	0.0277	0.199	0.2978	0.0248	0.189	0.1768	0.5505	0.0316	0.212GPT2-12	0.4631	26.0809	0.0176	0.6126	2.1238	0.0068	0.7101	0.621	4.4775	0.0152	0.7525GPT2-24	0.6707	40.4664	0.0141	0.8367	2.1771	0.0023	0.89	0.843	8.3889	0.0064	0.9006RoBERTa-12	0.0381	0.1754	0.005	0.8472	0.1649	0.0046	0.8444	0.8153	0.2608	0.0069	0.8387RoBERTa-24	0.0248	0.2626	0.0064	0.7647	0.1821	0.0048	0.7562	0.73	0.4492	0.0117	0.7472XLNet-12	0.0399	0.6265	0.0312	0.2214	0.3354	0.0237	0.2196	0.1911	0.4716	0.0321	0.2549XLNet-24	0.0468	0.5423	0.025	0.3307	0.2697	0.0153	0.3144	0.2871	0.4318	0.0282	0.3235DistilBERT-6	0.0353	0.4274	0.0247	0.2825	0.2461	0.0185	0.2824	0.2603	0.6842	0.035	0.2994Table 8: Social bias within static embeddings from different pretrained models with respect to a setof adjectives, Nadj . Parameters are set as f = mean, g = mean, N = 100000 and the layer of thepretrained model used in distillation is [ X J.
