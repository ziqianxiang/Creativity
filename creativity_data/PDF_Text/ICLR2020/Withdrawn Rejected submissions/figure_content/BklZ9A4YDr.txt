Figure 1: Our “hybrid gradient” method. We parametrize the design decisions as a real vector β andoptimize the function of performance L with respect to β . From β to the generated training imagesand ground truth, we compute the approximate gradient by averaging finite difference approxima-tions. From training samples X to L, we compute analytical gradients through backpropagationwith unrolled training steps.
Figure 2: The details of using “hybrid gradient” to incrementally update β and train the network.
Figure 3: Sampled shapes from our probabilistic context-free grammar, with parameters optimizedusing hybrid gradient.
Figure 4: Mean angle error on the test images vs. computation time, compared to two black-boxoptimization baselines.
Figure 5: The test set of the MIT-Berkeley Intrinsic Images dataset.
Figure 6: The original scenes in the SUNCG dataset, and our scenes with camera and objects per-turbed using our PCFG.
Figure 7: Training images generated using PCFG with 3DMM face model, and 6 example imagesfrom the test set.
