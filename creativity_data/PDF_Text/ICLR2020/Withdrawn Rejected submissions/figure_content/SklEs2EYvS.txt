Figure 1: The main framework of the proposed Structural Multi-Agent Learning. Given the statesof the multi-agent system at frame t, all agents communicate with each other through the edgesrather than upload messages to the environment or global manager to achieve effective and efficientinteraction. Then each agent makes a decision according to the current state and received messages.
Figure 2: Illustrations of the experimental environment and continuous tasks we consider. Agentnumbers of the three environments above are respectively 4, 4 and 10.
Figure 3: Total agent rewards on WaterworldFigure 4: Comparison of normalized agent scorebaselines. We observed that the agents trained by our method tend to keep close with other agentsnear them more frequently. In the Multi-Walker domain, the walkers learn to carry the packageforward without letting it fall by the messages from the other three walkers. In the Multi-ant case,the legs learn to avoid collision with each other. The histogram in Figure 4 is the normalized agentreward(divided by the return of SMAL) in all three domains, which shows that our method can begeneralized to these systems. Our method surpasses MADDPG in all three environments, indicatingthat the graph network is a better supervisor than the centralized critic. Also, out method outperformsIC3Net, which focuses on efficient communication and doesn’t include a reward sharing mechanism.
Figure 4: Comparison of normalized agent scorebaselines. We observed that the agents trained by our method tend to keep close with other agentsnear them more frequently. In the Multi-Walker domain, the walkers learn to carry the packageforward without letting it fall by the messages from the other three walkers. In the Multi-ant case,the legs learn to avoid collision with each other. The histogram in Figure 4 is the normalized agentreward(divided by the return of SMAL) in all three domains, which shows that our method can begeneralized to these systems. Our method surpasses MADDPG in all three environments, indicatingthat the graph network is a better supervisor than the centralized critic. Also, out method outperformsIC3Net, which focuses on efficient communication and doesn’t include a reward sharing mechanism.
Figure 5: Trade-off between communication costs and performance, the number of edges decreaseswhen λ increases. We choose ten values of λ for every environment.
Figure 6: Graph inferenceTable 2: Ablation study of graphMethod	Total ReWard(λ = 0)		Environment	Waterworld	Multi-walker	Multi-antGraph	812.45	70:68	126.11Mean	636.92	5934	115.70Quantitive improvements contributed to the graphare shown in table 2. In the second experiment, allthe weights in the graph are set as one. Agents di-rectly receive the messages from other agents andtake their mean vector as the input message. Withthe graph to model the proportion of messages andrewards, the agents choose their actions more in-telligently.
