Figure 1: Genetic Loss-function Optimization (GLO) overview. A genetic algorithm constructscandidate loss functions as trees. The best loss functions from this set then has its coefficientsoptimized using CMA-ES. GLO loss functions are able to train models more quickly and moreaccurately.
Figure 3: Training curves for different loss functions on MNIST. Baikal and BaikalCMA result infaster and smoother training compared to the cross-entropy loss.
Figure 2: Mean testing accuracyon MNIST, n = 10. Both Baikaland BaikalCMA provide statisti-cally significant improvements totesting accuracy over the cross-entropy loss.
Figure 4: Sensitivity to different dataset sizes for different loss functions on MNIST. For each size,n = 5. Baikal and BaikalCMA increasingly outperform the cross-entropy loss on small datasets,providing evidence of reduced overfitting.
Figure 5: Testing accuracy across varying training steps on CIFAR-10. The Baikal loss, which hasbeen transferred from MNIST, outperforms the cross-entropy loss on all training durations.
Figure 7: Output probabilities of networks trained with cross-entropy loss and BaikalCMA. WithBaikalCMA, the peaks are shifted away from extreme values and more spread out, indicating im-plicit regularization. The BaikalCMA histogram matches that from a network trained with a confi-dence regularizer (Pereyra et al., 2017).
Figure 6: Binary classification loss functions atx0 = 1. Correct predictions lie on the right sideof the graph, and incorrect ones on the left. Thelog loss decreases monotonically, while Baikaland BaikalCMA present counterintuitive, sharpincreases in loss as predictions, approach the truelabel. This phenomenon provides regularizationby preventing the model from being too confidentin its predictions.
Figure 8: Loss function surface plots for binary classification. y is the prediction for one class, andx is the true label for that class. Baikal is channel-shaped, while the log loss has a saddle-like shape.
