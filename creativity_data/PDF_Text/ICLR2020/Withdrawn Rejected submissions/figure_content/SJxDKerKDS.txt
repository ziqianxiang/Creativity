Figure 1: The high-level steps involved in the AG-RL algorithm. In the first Gather Experience step the baseagent interacts as normal with the environment for N1 episodes. Then We feed the experiences of the agent to agrammar calculator which outputs macro-actions that get appended to the action set. The agent starts interactingwith the environment again with this updated action set. This process repeats as many times as required, as setthrough hyperparameters.
Figure 2: Example of a solution to the Towers of Hanoi game. Letters a to f represent the 6 different possiblemoves. After 7 moves the agent has solved the game and receives +100 reward.
Figure 3: Comparing AG-DDQN to DDQN for 8 Atari games. Graphs show the average evaluation score over5 random seeds where an evaluation score is calculated every 25,000 steps and averaged over the previous 3scores. For the evaluation methodology we used the same no-ops condition as in van Hasselt et al. (2015). Theshaded area shows ±1 standard deviation.
Figure 4: Comparing AG-SAC to SAC for 20 Atari games. Graphs show the average evaluation score over 5random seeds where an evaluation score is calculated at the end of 100,000 steps of training. For the evaluationmethodology we used the same no-ops condition as in van Hasselt et al. (2015).
Figure 5: Ablation study comparing DDQN to different versions of AG-DDQN for the game Qbert. The darkline is the average of 5 random seeds, the shaded area shows ±1 standard deviation across the seeds.
Figure 6:	The Hindsight Action Replay (HAR) process with Action Set: {a, b, c:{abab}} meaning that thereare 2 primitive actions a and b, and one macro-action c which represents the sequence of primitive actions abab.
Figure 7:	Action Grammar Reinforcement Learning for Towers of Hanoi (5 Disk Environment). Red verticallines correspond to grammar updates.
