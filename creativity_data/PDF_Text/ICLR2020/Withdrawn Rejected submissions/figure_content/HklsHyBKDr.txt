Figure 1: Schematic of Gaussian-noise-augmented stochastic RNN.
Figure 2: (a) Example trajectories of Brownian harmonic oscillator over time, with each color rep-resenting a different trajectory. (b, c, d) Estimates of the past information contained in the stochasticvariable (x-axis) vs. future information (y-axis). The feasible region is shaded. The models to theupper-left perform better future prediction, while compressing the past more, therefore they are ofhigher efficiency. Points correspond to estimates with a learned critic; bars represent the gap betweenthe lower and upper bounds using the known conditional distribution instead of a learned critic; col-ors represent the noise level, log(noise), added to the hidden state. The stochastically trained RNNis marked as ◦, and deterministically trained RNN with post-hoc noise injection is marked as O.
Figure 3: (Left) Comparison among critic based estimators, InfoNCE, JS and NWJ. (Middle) Com-parison between estimations from Barber-Agakov and InfoNCE lower bound on future informationI (Xfuture; Z). (Right) Illustration of the convergence for minibatch upper and lower bounds withtwo noise levels, 0.02 (Blue) and 0.05 (Red). Dashed line is the ln(number of samples), which isthe limit for minibatch lower bounds.
Figure 4: (Left) Estimates of past and future information over training iterations on the trainingand testing BHO data. We can see that our MI estimates quickly overfit, Which We remedy here byearly stopping. (Right) Impact of training dataset sizes on InfoNCE estimator on more complicateddataset, Aaron’s Sheep, as introduced in Section 3.3. The original dataset has size 7200 for training,and 800 for evaluation. We augment the dataset by random scaling the input values per sequence.
Figure 5: Evaluate mutual information estimators given optimal encoder. For past informationestimation, I(Hidden; Past), InfoNCE lower bound (colored points) and MB-Lower and MB-Upper(colored bars) are used; for future information, I(Hidden; Future), only InfoNCE is applied, dueto lack of tractable conditional distribution p(Y |T). Heat-color represents the level of trade-offparameter β .
Figure 6: (Left) Evaluation on Aaron Sheep dataset by comparing training explicitly with noise (◦)and post-hoc noise injection after training (O). The color bar shows the noise level in log10 scale.
Figure 7: Conditional generation of QuickDraw sketches by SketchRNN models trained with dif-ferent noise levels.
Figure 8: Left Column. Top: Estimation of past and future information for RNN trained with 100samples, with color indicating the noise levels in log10 scale. Bottom: Validation loss for differentnoise levels. Right Column. Conditional generated samples from models with different levels ofpast information. The generation is conditioned on a 25-step stroke, which is taken from a held-outsample. The samples from the model with 4.0 nats past information is of better sample quality thanmodels with higher past informations. For more samples, see Figure 12.
Figure 9: Conditionally generated samples from models with different levels of past information.
Figure 10: The impact of training objectives on BHO dataset for SimpleRNN (Left) and LSTM(Right). Models trained with maximum likelihood estimations are marked with ◦, and modelstrained with contrastive loss are marked with O. The color bar shows the noise level in log10 scale.
Figure 11: The impact of dropout on predictive information capacity for SimpleRNN (Left) andLSTM (Right). Grey ◦ marks the result of stochastically trained RNN as described in Figure 2.
Figure 12: Conditionally generated samples from models with different levels of past information.
