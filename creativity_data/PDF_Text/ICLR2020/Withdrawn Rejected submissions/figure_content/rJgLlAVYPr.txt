Figure 1: Architecture of white box network for two layers (L = 2) with each layer containing twounitary operations {f1, f2} and two binary operations {g1, g2} as its function blocks.
Figure 2: Architecture of end-to-end PathNet comprising a two-layer (L = 2) network with sixmodules (M = 6) in each layer. The number of maximum distinct modules per layer that arepermitted in the pathway is three (N = 3), and the final layer comprises a fully connected linearlayer. The activated path and modules are colored in red.
Figure 3: Three ladder logic diagrams (LLDs) for the experiment.
Figure 4: Sample structures for three LLDs suggested by WBN. (a) LLD1 sample #1 constructedby WBN (b) LLD1 sample #2 constructed by WBN (c) LLD2 sample #1 constructed by WBNblocks used in LLDs are nondifferentiable Boolean discrete functions. To solve this problem, wecontinuously extended the function blocks such that gradient descent could be applied. The fourfunction blocks used in this experiment are as follows.
Figure 5: Binary MNIST classification results. (a) transfer result for 1vs3 → 7vs8. Total speed-upradio is 1.18. (b) transfer result for 4vs7. → 2vs5. Total speed-up radio is 1.134.2	Binary MNIST classificationWe performed experiments to verify the end-to-end PathNet structure. We used the selection matrixinstead of the genetic algorithm, which is different from the original PathNet. Experiments wereconducted to determine whether the selection matrix could select the appropriate path. We per-formed the experiments using the transfer learning paradigm suggested in the original PathNet andobserved a positive transfer by using our network.
Figure 6: CIFAR classification results. (a) Transfer results for class l. On average, test accuracyincreased from 51.28% to 52.42% after transfer. (b) Transfer results for class 2. On average, testaccuracy increased from 58.50% to 59.73% after transfer.
Figure 7: Additional sample structures for three LLDs suggested by WBN. (a), (b) are from LLD1,(c) is from LLD2 and (d), (e) are from LLD3.
Figure 8:	Sample target functions for three LLDs suggested by WBN. (a) sample equation for LLD1target function. (b) sample equation for LLD2 target function. (c) sample equation for LLD3 targetfunction11Under review as a conference paper at ICLR 2020Without transf⅛r(7vs8)After traπsfer(lvs3->7vs8)O	200	400	600	800	IOOOIteration numberFigure 9:	Learning curve for binary MNSIT classification (1vs3 → 7vs8). We conduct experiments10 times to get the learning curves.
Figure 9:	Learning curve for binary MNSIT classification (1vs3 → 7vs8). We conduct experiments10 times to get the learning curves.
Figure 10:	Learning curve for CIFAR classification. (a) learning curve for class 2. (b) learning curvefor class 1. We conduct experiments 10 times to get the learning curves.
