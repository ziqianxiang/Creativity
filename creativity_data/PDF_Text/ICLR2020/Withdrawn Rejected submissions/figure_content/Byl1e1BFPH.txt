Figure 1: The pipeline of DualSMC. The planner and filter are linked via belief representatives.
Figure 2: The left column shows the actual trajectories of the robot, and other columns are its planningtrajectories at different times. The robot can better localize itself stepping across dashed blue lines.
Figure 3: Averaged filtering error as a function of the number of robot steps for floor positioning.
Figure 4: The DualSMC planner trained under POMDPs generates different polices according to theuncertainty of belief state. For (b), we assign true state values to the top-M particles before planning.
Figure 5: DualSMC learns to go up to the wall of decals first before turning back to the goal. Notethat the robot figures out its location at t = 30 and where to start turning back at t = 60.
Figure 6: The modified Reacher environment and the goal estimation of DualSMC v.s. time.
Figure 7: Smoothed training curves on modifiedReacher. See experiment details in Appendix D.3.
Figure 8: Ot is the observed optimality variable that depends on the latent state st and action at .
