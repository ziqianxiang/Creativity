Figure 1: Encoder-agnostic variants considered. All methods utilize a task-specific source encoder,but vary in which parts of the decoder are pretrained and which are randomly initialized. Repr-Transformer trains a new full transformer decoder, Context-Attn trains a new context attention layer,Pseudo-Self attention only modifies part of the self attention layer. Residual connections and layernormalization have been omitted for clarity. Green indicates that parameters are initialized withpretrained weights, gray indicates random initialization. Red vectors indicate the target activationsat each layer. Blue vectors indicate the source features at the output of the encoder. xN indicatesthat the section within the dotted lines is stacked N times.
Figure 2: Effect of introducing randomly initialized parameters.
Figure 3: Data efficiency analysis withIMDb. PPL shown in blue (left), classi-fication accuracy shown in orange (right).
