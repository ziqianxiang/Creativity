Figure 1: Illustration of the proposed SimpleOptimizer architecture.
Figure 2: Comparison to baseline models. Results show that baseline models still have convergenceissue over 10k steps. In contrast, SimpleOptimizer resolves the issue without complicated design.
Figure 3: Ablation analysis of proposed SimpleOptimizer. (a) Adding back the bias term, we observethat meta-optimizer cannot converge. (b) With weighted difference of loss as the training objective,meta-optimizer further improves. Adding aggregate information sharing also helps in convergence.
Figure 4: Generalization to different models and datasets. (a) Results of generalization to LeNet onMNIST dataset. (b) Results of generalization to LeNet on CIFAR-10 dataset.
Figure 5: Generalization to deep models. (a) Experimental results of generalization to Deeper MLPon MNIST dataset. (b) Experimental results of generalization to GoogleNet V1 on CIFAR-10 dataset.
