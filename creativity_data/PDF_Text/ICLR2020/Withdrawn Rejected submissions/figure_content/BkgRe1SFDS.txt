Figure 1: Top Left: overall pipeline of our 2-phase framework. Top Right (world graph discovery): a subgraphexemplifies traversal between waypoint states (in blue), see Section 3 for more details. Bottom (HierarhicalRL): an example rollout from our proposed HRL policy with Wide-then-Narrow Manager instructions and worldgraph traversals, solving a challenging Door-Key task, see Section 4 for more details.
Figure 2: Our recurrent latent model with differentiable binary latent units to identify waypoint states. A priornetwork (left) learns the state-conditioned prior in Beta distribution, pψ(zt∣st)=Beta(at, βt). An inferenceencoder learns an approximate posterior in HardKuma distribution inferred from the state-action sequence input,qφ(zt∣a, Z)=HardKuma(&t, 1). A generation networkpθ reconstructs a from {st∣zt=1}.
Figure 3: Left: a standard Feudal Network. Right: using Wide-then-Narrow goals. The Manager first outputs awaypoint state as the wide goal gw , then attends to a closer-up area around gw to narrow down the final goal gn.
Figure 4: Validation performance during training (mean and standard-deviation of reward, 3 seeds) forMultiGoal. Left: Comparing Vp and Vrand, with or without traversal, all models use WN and πg initialization.
Figure 5: Visualization of the 2D grid environments in our experiments, along with the learnedwaypoints in blue.
Figure 6: Visualization of tasks in our experiments.
