Figure 1: The target phrase real property is represented by four clustering centers. The previouswork discovers the four modes by finding clustering centers which well compress the embedding ofobserved co-occurring words. Instead, our compositional model learns to predict the embeddingsof cluster centers from the sequence of words in the target phrase so as to reconstruct the (unseen)co-occurring distribution well.
Figure 2: Our model for sentence representation. We represent each sentence as multiple code-book embeddings (i.e., clustering centers) predicted by our sequence to embeddings model. Ourloss encourages the model to generate codebook embeddings whose linear combination can wellreconstruct the embeddings of co-occurring words (e.g., Music), while not able to reconstruct thenegatively sampled words (i.e., the co-occurring words from other sentences) to avoid predictingcommon topics which co-occur with every sentence (e.g., is in this example).
Figure 3: Comparing the F1 of ROUGE-1 score on unsupervised methods that do not access thesentence order informationD More ExamplesWe visualize predicted embeddings from 10 randomly selected sentences in our validation set. Theformat of the file is similar to Table 1. The first line of an example is always the prepossessed inputsentence, where <unk> means an out-of-vocabulary placeholder. The embedding in each row isvisualized by the nearest five neighbors in a GloVe embedding space and their cosine similarities tothe vector.
