Figure 1: (a) Red solid arrows illustrate the transitions affecting the gradient update. We illustratethe termination condition mechanism with an example. For t = (1, 2, 3,4), Vex is large enoughso βθ(St) = 0. The transitions sampled by policy ∏σ affect the gradient update. The others aredropped. Note that an option σ can be one-step long, i.e. βθ(St) = 1, βθ(St+1) = 0, βθ(St+2) = 1.
Figure 2: Comparison of SAUNA with PPO on 6 MuJoCo environments (106 timesteps, 6 differentseeds). Red is our method PPO+Vex . Line: average performance. Shaded area: standard deviation.
Figure 3: Comparison of SAUNA with PPO on 3 MuJoCo environments (106 timesteps, 6 differentsees). Red is our method PPO+V ex, Orange is PPO+V ex without the filtering out of noisy samples.
Figure 4: Gradients L1-norm from the (a) first layer and (b) last layer of the shared parametersnetwork for PPO and when SAUNA i applid to PPO. Task: HalfCheetah-v2.
Figure 5: (a) Example of PPO getting trapped in a local minimum (top row) while PPO+Vex reachesa better optimum (bottom row). (b) V ex score for PPO (orange) and SAUNA (green).
Figure 6: Network-agnostic variance explained head.
Figure 7: Comparison of SAUNA with A2C on 6 MuJoCo environments (106 timesteps, 6 differentseeds). Red is our method A2C+V ex . Line: average performance. Shaded area: standard deviation.
Figure 8: Comparison of SAUNA with PPO on the more challenging Roboschool environment (108timesteps, 6 different seeds). Red is our method PPO+V ex . Line: average performance. Shadedarea: standard deviation.
Figure 9: Comparison of our method with PPO on 6 MuJoCo environments (106 timesteps, 6 dif-ferent seeds). Red is our method PPO+Vex , Orange is PPO+Vex without the filtering out of noisysamples. Line: average performance. Shaded area: standard deviation.
