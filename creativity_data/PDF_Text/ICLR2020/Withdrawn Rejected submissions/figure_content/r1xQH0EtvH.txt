Figure 1: A minefield of bad minima: we train a neural net classifier and plot the iterates of SGD after eachtenth epoch (red dots). We also plot locations of nearby “bad” minima with poor generalization (blue dots). Wevisualize these using t-SNE embedding. All blue dots achieve near perfect train accuracy, but with test accuracybelow 53% (random chance is 50%). The final iterate of SGD (yellow star) also achieves perfect train accuracy,but with 98.5% test accuracy. Miraculously, SGD always finds its way through a landscape full of bad minima,and lands at a minimizer with excellent generalization.
Figure 2: (left) CIFAR10 trained with ResNet-18 and alinear model having comparable number of parameters.
Figure 3: Top: Decision boundaries of two networks with different parameters. Network (a) generalizeswell. Network (b) generalizes poorly (perfect train accuracy, bad test accuracy). The flatness and large volumeof (a) make it likely to be found by SGD, while the sharpness and tiny volume of (b) make this minimizerunlikely. Red and blue dots correspond to the training data. See https://www.youtube.com/watch?v=4VUJyQknf4s&t= for an animation of these boundaries when perturbed. Bottom: A slice through the losslandscapes around these minima reveals sharpness/flatness.
Figure 4: A slice through the loss landscape of two minima for the SVHN loss function using ResNet-18.
Figure 5: Relationship between generalization, sharpness, and volume. Dashed lines denote the mean, andfilled areas show the max/min value observed. Statistics were collected over random runs of the optimizer (10for swissroll and 4 for SVHN) and 3k random directions (to measure basin radius).
Figure 6: Swissroll decision boundary for various levels of generalization gap (indicated above plots).
Figure 7: SVHN loss along random directions, and the“basin” lying beneath the cutoff loss value.
Figure 8: A neural network fails to solve a classificationproblem when the ideal solution is “sharp.”this problem by pinching the margin between the inner red and blue rings. In this case, a networktrained with random initialization is shown in Fig. 8b. Now, SGD finds networks that cherry-pick redpoints, and arc away from the more numerous blue points to maintain a large margin. In contrast, asimple circular decision boundary as in Fig. 8a would pass extremely close to all points on the innerrings, making such a small margin solution less stable under perturbations and unlikely to be foundby SGD.
