Figure 1:	Wildly unsupervised domain adaptation (WUDA).
Figure 2:	WUDA ruins representative UDA methods. Representative UDA methods includes deep adaptationnetworks (DAN, a IPM based method (Long et al., 2015)), domain-adversarial neural network (DANN, aadversarial training based method (Ganin et al., 2016)), asymmetric tri-training domain adaptation (ATDA, apseudo-label based method (Saito et al., 2017)) and transferable curriculum learning (TCL, a robust UDAmethod (Shu et al., 2019)). B-Net is our proposed WUDA method. We report target-domain accuracy of allmethods when the noise rate of source domain changes (a) from 5% to 70% (symmetry-flip noise) and (b)from 5% to 45% (pair-flip noise). Clearly, when the noise rate of source domain increases, target-domainaccuracy of representative UDA methods drops quickly while that of B-Net keeps stable consistently.
Figure 3: Butterfly Framework. Two networks (F1 and F2) in Branch-I are jointly trained on noisy sourcedata and pseudo-labeled target data (mixture domain). Two networks in Branch-II (Ft1 and Ft2) are trainedon pseudo-labeled target data. By using dual-checking principle, Butterfly checks high-correctness data outfrom both mixture and pseudo-labeled target data. After cross-propagating checked data, Butterfly can obtainhigh-quality domain-invariant representations (DIR) and target-specific representations (TSR) simultaneouslyin an iterative manner. Note that the interaction between DIR and TSR happens via the shared CNN. Besides,in the first training epoch, since we do not have any pseudo-labeled target data, we need to use noisy sourcedata as the pseudo-labeled target data, which follows Saito et al. (2017).
Figure 4: Target-domain accuracy vs. number of epochs on four SYND→MNIST WUDA tasks.
Figure 5: Target-domain accuracy vs. number of epochs on four MNIST→SYND WUDA tasks.
Figure 6: Visualization of MNIST and SYND.
Figure 7: Visualization of Bing, Caltech256, ImageNet and SUN (taking “horse” as the common class).
Figure 8: The architecture of B-Net for digit WUDA tasks SYND - MNIST. We added BN layer in the lastconvolution layer in CNN and FC layers in F1 and F2. We also used dropout in the last convolution layer inCNN and FC layers in F1, F2, Ft1 and Ft2 (dropout probability is set to 0.5).
Figure 9: The architecture of B-Net for human-sentiment WUDA tasks. We added BN layer in the first FClayers in F1 and F2. We also used dropout in the first FC layers in F1, F2, Ft1 and Ft2 (dropout probability isset to 0.5).
Figure 10: The architecture of B-Net for real-world WUDA tasks. We added BN layer in the first FC layers inF1, F2, Ft1 and Ft2. We also used dropout in the first FC layers in F1, F2, Ft1 and Ft2 (dropout probabilityis set to 0.5).
