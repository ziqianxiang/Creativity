Figure 1: A. State-action representation of a simple state-space with recurrency and terminal states.
Figure 2: Decision tree with added decision complexity. Panels as in Fig. 3. A higher local policydivergence at state 2 is observed (as compared to Fig. S1).
Figure 3: Path programming the optimal policy in the Tower of Hanoi game. A. Tower of Hanoistate-space graph. B-C. Normalized policy divergence PD and its time derivative for each state.
Figure 4: Path programming the optimal policy in a grid world with a wormhole. Panels as inFig. 3. Dotted lines with short dashes correspond to bottleneck states marked + in panel A. Dottedlines with long dashes correspond to wormhole states marked W in panel A. The darkness of the statecoloring reflects state occupation density under the optimal policy.
Figure S1:	Breadth two, depth three decision tree. Panels as in Fig. 3. A. The state-space graphof a breadth two, depth three decision tree. States on the optimal path are highlighted in blue.
Figure S2:	Tower of Hanoi with the option to remain at a state. Panels as in Fig. 3. We presentan extended set of results. Panels C, D, G, and H have already been displayed in Fig. 3 while panelsA, B, E, and F show their unnormalized counterparts.
Figure S3:	Tower of Hanoi with forced resets on arrival at the goal. Instead of having the optionto remain at a goal state, we consider an alternative scenario in which the agent is automaticallytransported back to the initial state on after arriving at the goal. The path gradient dynamics arebroadly similar and retain their hierarchical characteristics however the prominence of the goal stateis diminished both in terms of policy divergence (since the agent no longer has any choice at the goalstate) and counter difference (since they goal state can no longer be repeatedly exploited for reward).
Figure S4:	Path programming the optimal policy in a grid world without a wormhole. Panelsas in Fig. 3. Dotted lines with short dashes correspond to bottleneck states marked + in panel A.
