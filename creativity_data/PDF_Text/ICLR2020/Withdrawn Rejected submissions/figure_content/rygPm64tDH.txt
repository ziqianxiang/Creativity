Figure 1: Left: Expected gradients attributions (from 100 samples) on CIFAR10 for both the baselinemodel and the model trained with an attribution prior, for five randomly selected images classifiedcorrectly by both the baseline and the regularized model. Training with an attribution prior generatesvisually smoother attribution maps in all cases. Notably, these smoothed attributions also appear morelocalized towards the object of interest. Right: Training with an attribution prior induces robustnessto Gaussian noise, achieving more than double the accuracy of the baseline at high noise levels. Thisrobustness isn’t achievable by choosing gradients as the attribution function.
Figure 2: Left: A neural network trained with our graph attribution prior (bold) attains the best testperformance, while a neural network trained with the same graph penalty on the gradients (italics,adapted from (Ross et al., 2017b)) does not perform significantly better than a standard neural network.
Figure 3: Left: A sparse attribution prior enables more accurate test predictions (top) and sparsermodels (bottom) across 100 small subsampled datasets (100 training and 100 validation sampleseach). Top right: Across the full range of tuned parameters, the sparse attribution prior achievesgreater sparsity and a smooth sparsity-performance tradeoff. Bottom right: A sparse attribution priorconcentrates a larger fraction of global feature importance in the top few features.
Figure 4: Feature attribution values attained using expected gradients converge as the number ofbackground samples drawn is increased.
Figure 5: A comparison of attribution methods on ImageNet. Integrated gradients fails to highlightblack pixels as important when black is used as a baseline input.
Figure 6: Robustness to noise on CIFAR-10 with a stricter λ threshold. Here, there is little differencein test accuracy on the original test set between the baseline and the image attribution prior model(0.930 ± 0.002 for the baseline vs. 0.925 ± 0.002 for the pixel attribution prior). Both the imageattribution prior model and the gradient-based model afford small improvements in robustnesscompared to the baseline. As in the main text, results here are the mean and standard deviation across5 random initializations.
Figure 7: Plotting the trade-off between accuracy and minmizing total variation of expected gradients(left) or gradients (right). For both methods, there is a clear elbow point after which test accuracydegrades to no better than random. The total variation of attributions is judged based on the attributionbeing penalized: expected gradients for the left plot, gradients for the right plot.
Figure 8: Left: Expected gradients attributions (with 100 samples) on MNIST for both the baselinemodel and the model trained with an attribution prior, for five randomly selected images classifiedcorrectly by both the baseline and the regularized model. Red pixels indicate pixels positivelyinfluencing the prediction, while blue pixels negatively influence the prediction. Training with anattribution prior generates visually smoother attribution maps and tend to better highlight relevantparts of the image. Right: Training with an attribution prior induces robustness to noise, more so thanan equivalent model trained by minimizing the total variation of gradients or an equivalent baseline.
Figure 9:	Attribution maps generated by Expected Gradients on the VGG16 architecture before andafter fine-tuning using an attribution prior.
Figure 10:	Attribution maps generated by Integrated Gradients on the VGG16 architecture beforeand after fine-tuning using an attribution prior.
Figure 11:	Attribution maps generated by raw gradients on the VGG16 architecture before and afterfine-tuning using an attribution prior.
Figure 12: Fine tuning without graph prior penalty leads to no significant improvement in modelperformance.
Figure 13: Top pathways for neural networks with and without attribution priorsFine-tuning optimizes for smoothness over the graphFigure 14: Fine tuning optimizes for the metric we care about: smoothness over the graph26Under review as a conference paper at ICLR 2020validation set. For each of the 100 experimental replicates, 100 data points were sampled uniformlyat random from the training and validation sets to yield a 100/100/3000 split.
Figure 14: Fine tuning optimizes for the metric we care about: smoothness over the graph26Under review as a conference paper at ICLR 2020validation set. For each of the 100 experimental replicates, 100 data points were sampled uniformlyat random from the training and validation sets to yield a 100/100/3000 split.
Figure 15: Validation performance and gini coefficient as a function of regularization strength for allmodels, averaged over 100 subsampled datasets. Blank areas indicate where some of the 100 modelsdiverged for a given hyperparameter setting as described in subsection I.4.
Figure 16: Additional results from the maintext experiments, with the addition of first-layer sparsegroup lasso. Top: The sparse attribution prior provides the best performance, and the Gini penaltyon gradients provides the next best. First-layer SGL does not improve over unregularized models.
Figure 17:	Summary of feature attributions for top 20 features from each model (best model fromeach class chosen as described in the main text), for a single randomly chosen replicate of the 100small-data subsamples.
Figure 18:	Sparsity vs performance plot for additional models on full NHANES dataset.
