Figure 1: Depiction of ourfitness measure in blue duringtraining with yaxis on the leftof the figures versus the FIDmeasure in red with yaxis onthe right of the figures (left)DCGAN and (right) WGAN.
Figure 2: Depiction of the closest reconstruction to the target image (left) from WGAN trained as (fromleft to right) (i) standard, all dataset (ii) with K-means subsampling K = 10 (iii) with K-means subsamplingK = 100 (iv) with 300 dimensional latent space dimensionA current limitation of metrics for GANs is their dependence on another pretrained deep neuralnetwork model. This poses limits as to what dataset GANs can be evaluated on, as well as anyinternal bias induced by the different models used. On the other hand our measure does not relyon any other trained model and thus can be applied as is across different datasets. We now brieflydescribe those metric and then empirically observe how our metric compares to FID.
Figure 5: WGAN experimenton CIFAR10 (left) and LSUN(right) for the same networkwith and without residual con-nectionsHistogram of the components of zHistogram of the components of z*Figure 6: The initialization ofZ and the final z*. For illus-trative purposed, the wide dis-tributions on the left are shownwith smaller standard devia-tions than in practice.
Figure 6: The initialization ofZ and the final z*. For illus-trative purposed, the wide dis-tributions on the left are shownwith smaller standard devia-tions than in practice.
Figure 7: WGANexperiment onCIFAR10 (left)and LSUN (right)for the samenetwork with andwithout residualconnections.
Figure 8: (left): Histograms of m(xi, G) for different subsampling methods. We notice that the K100 methodis the best among the three in terms of goodness of fit. We perform random subsampling (R100), kmeansclustering (K100), and a mixture of the two (K10R10). In the K10R10 case, we first use kmeans clusteringwith 10 clusters and then subsample randomly by 10 in each of those clusters. (right) Histograms of m(xi , G)for WGAN and our mixture of WGANsthe new measure of F (G) after training. We obtain the key following conclusions. As we sub-sample our data and our parameters to data ratio increases, our goodness of fit measure decreases.
