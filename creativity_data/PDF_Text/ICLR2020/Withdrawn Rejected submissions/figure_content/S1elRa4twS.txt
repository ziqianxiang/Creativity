Figure 1: The above consist of two separate figures. (left) The training pipeline of tiMe in thesimplest setting. (right) Architecture for the second phase when BCQ is used in the first phase.
Figure 2: All three figures are from the 3 meta-train MDPs scenario. (left) Performance of BatchSAC in one meta-test MDP and the learnt value estimates of initial state-action pairs. The esti-mates do not diverge but are significantly higher than the actual returns, demonstrating the MDPmis-identification phenomena. In contrast, tiMe’s performance is close to optimal. (middle) Thebehavior of the Batch SAC agent. The dark circle, red circle and dark crosses indicates the agent’sstarting locations, final location and the 3 meta-train goal locations. The agent fails to find goodactions during meta-test and navigates to the location closest to the 3 meta-train goals. (right) Thebehavior of an agent trained with tiMe. The crosses and circles indicate each meta-test MDP goallocation and the agent’s corresponding final location after evaluation in the corresponding meta-testMDP. The agent trained with tiMe finds near-optimal actions in all 3 meta-test MDPs.
Figure 3: The leftmost column illustrates hopper and halfcheetah. The remaining columns indi-cate the performance of SAC trained from scratch versus tiMe for different unseen MDPs duringzero-shot meta-test. Because of these two properties, we highlight the difficult nature of obtain-ing high performance in this setting. The second to forth columns correspond to small, medium,and large target velocities respectively. The first and second row indicates performance on hopperand halfcheetah respectively. The x-axis indicates SAC’s number of environment interactions. They-axis indicates the average episode return. The final performance of SAC is close-to-optimal inall plots in the sense that running SAC for many more timesteps will not increase its performancesignificantly.
