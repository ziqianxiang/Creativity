Figure 1: Convolutional recurrent network with multiple cell types. The Standard model forCIFAR10 starts with two layers of regular convolutional processing, followed by 2 layers of re-current processing. Each recurrent layer consists of three types of cells. The excitatory principalneurons (PN) output non-negative connection weights and target the next area. Output-gating (OG)and input-gating (IG) neurons are inhibitory and only target neurons within the same area.
Figure 2: Model excitatory neurons have higher orientation selectivity and natural image se-lectivity than inhibitory neurons. (a,b) The average gOSI (a), orientation skewness (b), and imageskewness (c) for each type of neurons in the Standard networks. Error bar is the 95% confidenceinterval computed with bootstrapping across 5 networks.
Figure 3: Model excitatory neurons are more sparsely connected. (a-d) The distribution ofconnection weights for all recurrent connections (a), and for the PN-to-PN (b), OG-to-PN (c), andIG-PN connections (d) in area 3 of one standard network. Blue line: the threshold used to computethe proportion of strong connections, namely those that exceed the threshold. (e) The connectiondensity (proportion of strong connections) for three types of excitatory and inhibitory connectionsacross area 3 and 4.
Figure 4: The E-I differences emerge across network variations and datasets. The E-I dif-ferences are summarized using the difference in image skewness between principal neurons (PN)and interneurons (Int), and the difference in connection density between Int-to-PN and PN-to-PNconnections. Light circles: individual networks. Dark circles: average. Experimental estimates arederived from Znamenskiy et al. (2018), Song et al. (2005), and Packer & Yuste (2011) (AppendixE).
Figure 5: The E-I differences emerge early in training. The orientation skewness (a), imageskewness (b), and connection density (c) for areas 3 (top) and 4 (bottom) throughout training.
Figure 6: Selectivity and connectivity in networks with various numbers of excitatory andinhibitory channels. (a) The image skewness of principal neurons (PN, left) and interneurons (Int,right) for networks of various numbers of excitatory and inhibitory channels. (b) The connectiondensity of PN-PN connections and Int-PN connections for various networks. Results from areas 3and 4, and from two types of interneurons are combined. All networks shown have accuracy above60%.
Figure 7: Removing asymmetry between excitatory and inhibitory neurons. Difference in con-nection density against difference in image selectivity for areas 3 (a) and 4 (b) for the originalnetworks (blue), their InhPN variants (green), and NoConstraint variants (orange). Light symbols:individual networks; dark symbols: average. Circle: Standard model and its variants; triangle:StandardEI and its variants.
Figure 8: Model excitatory neurons have higher orientation selectivity than inhibitory neu-rons. (a) A total of 432 gratings, using 18 orientations (10° apart), 6 spatial frequencies (1, 2, 3,4, 6, 8) and 4 phases (90° apart). (b) The orientation tuning curves of example PN, OG, and IGneurons from area 3 of one Standard network. The number in each panel indicates the neuron’sglobal Orientation Selectivity Index (gOSI).
Figure 9: The E-I differences emerge across a range of network variations. (a-c) The E-Idifferences are summarized using the difference in image skewness between principal neurons (PN)and interneurons (Int), and the difference in connection density between interneuron-to-PN and PN-to-PN connections, in the same way as Figure 4. (a) Models with different readout and gatingmechanisms. The Standard model is denoted here as the LastTMulSub model. (b) Models intransition from the Standard model to the StandardEI model. The latter is denoted here as theNoOG-ECurrBN_SSubSFG-RecurrI model. (c) Models with variations of batch normalization ordropout. All model variants develop the E-I differences. Results are combined from areas 3 and 4.
Figure 10: The classification accuracy during training. The learning rate is decayed at epoch100, 150, and 200.
Figure 11: Performance of networks with various numbers of excitatory (principal neuron,PN) and inhibitory (interneuron, Int) channels.
Figure 12: Performance of the InhPN and NoConstraint networks compared with the standardmodels.
Figure 13: Detailed comparison between the Standard model and InhPN network. ComparinggOSI (a), orientation skewness (b), image skewness (c), and connection density (d) for areas 3 (left)and 4 (right) between the Standard model and the InhPN network.
Figure 14: Detailed comparison between Standard model and NoConstraint network. Compar-ing gOSI (a), orientation skewness (b), image skewness (c), and connection density (d) for areas 3(left) and 4 (right) between Standard model and the NoConstraint network. In the NoConstraintmodel, the OG-PN connectivity is close to zero, suggesting that OGs are only weakly participatingin the computation.
Figure 15: Detailed analysis of the StandardEI model.
Figure 16: Analysis of networks trained on shuffled labels. L2 is used to emphasize that L2weight regularization is intact in networks trained on shuffled labels, unlike Zhang et al. (2016). TheShufled12/ELShuffled-L2 networks achieve on average 43.9%∕24.6% accuracy on the randomly-labeled training set.
Figure 17: Analyzing the impact of having no inhibitory neurons. (a,b) Accuracy in networkswithout inhibitory neurons. (a) Standard and StandardEI networks, (b) Similar architectures, butwithout batch normalization (NoNorm). (c) Performance of networks with varying numbers of ex-citatory and inhibitory channels. Maximum (left) and average (right) performance across 5 randomseeds. (d) InhPN and NoConstraint variants of networks without batch normalization reproducedfindings from Figure 7.
