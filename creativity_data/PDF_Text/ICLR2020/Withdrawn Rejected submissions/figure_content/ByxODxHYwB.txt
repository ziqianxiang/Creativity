Figure 1: (Left) DocNADE (LVT+MST): Multi-source transfer learning in TM by introducing pre-trained word embeddings from a WordPool at each autoregressive step i. Double circle â†’ multino-mial (softmax) unit. (Right) Multi-source transfer learning in TM by introducing pretrained (latent)topic embeddings from a TopicPool, illustrating topic alignments between source and target cor-pora in GVT+MST configuration. Each outgoing row from Zk signify a topic embedding of thecorresponding source corpus, DCk . Here, TM refers to a DocNADE topic model.
Figure 2: (a, b, c, d) Retrieval performance (precision) on four datasets. (e) Precision at recallfraction 0.02, each for a fraction (20%, 40%, 60%, 80%, 100%) of the training set of TMNtitle. (f)Zero-shot and data-augmentation (DA) experiments for topic coherence on TMNtitle and Ohsumed.
