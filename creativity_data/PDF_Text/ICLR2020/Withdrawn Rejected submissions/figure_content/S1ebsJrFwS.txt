Figure 1: A schematic illustration of our explainable recommendation system.
Figure 2: An illustration of themethod. At the root node, all ex-amples are embedded into y,the function f then returns thedecision rule to split the sam-ples between the root,s right andleft children. The process is re-cursively repeated until reachingleaf nodes, where, using the cur-rent set embedding, g returns theleaf value v.
Figure 3:	Synthetic problem. (a,b) Performance as a function of the labeled set size (|L0 |). The depthof all models, except for the dynamic meta tree, is 2 in panel (a) and 3 in panel (b). (c) The averagenumber of inner nodes, as well as the 25th and 75th percentiles, used by the dynamic trees.
Figure 4:	Samples of global trees and meta-trees that are either dynamic or of a fixed architecture.
Figure 5: KNN Trees performance as a function of k on all three datasets.
Figure 6: Model sensitivity to dh (top row) and tree depth (bottom) for the three datasets.
Figure 7: Measuring the robustness of trees generated by the meta-tree model (“Meta”) and thelocal-trees (“Local”) baseline. Comparing the trees generated using the original user’s training setand after removing a random subset from it. (a) Exact match of the features used in each tree node(order must be identical). (b) Exact match of the set of features used. (c) The Jaccard index of the setof features used.
Figure 8: Analysis of the model behavior on the MovieLens-100k dataset. (a) Percentage of treesusing only the item average rating as a function of the user’s training set size. (b) Performancecomparison of the meta-tree algorithm (“Meta”) and SVD++ for different per-user training set size.
Figure 9: Comparison of the meta-tree model (“Meta”) and the SVD++ algorithm performanceson the MovieLens-100k dataset with respect to the users’ ratings correlation with the average itemrating.
