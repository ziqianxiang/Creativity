Figure 1: Sample images highlighting the properties and applications of “robust representations”studied in this work. All of these manipulations use only gradient descent on simple, unregularized,direct functions of the representations of adversarially robust neural networks Goodfellow et al.
Figure 2: A limitation of standard neural network representations: it is straightforward to constructpairs of images (x1, χ2) that appear completely different yet map to similar representations.
Figure 3: Visualization of inputs that are mapped to similar representations by models trained onthe Restricted ImageNet dataset. Target (χ2) & Source (xi): random examples image from thetest set; Robust and Standard (x；): result of minimizing the objective (4) to match (in '2-distance)the representation of the target image starting from the corresponding source image for (top): arobust (adversarially trained) and (bottom): a standard model respectively. For the robust model, weobserve that the resulting images are perceptually similar to the target image in terms of high-levelfeatures (even though they do not match it exactly), while for the standard model they often lookmore similar to the source image which is the seed for the optimization process. Additional resultsin Appendix B.1, and similar results for ImageNet are in Appendix B.1.4.
Figure 4: Optimizing objective (4) with PGD and an '2 -norm constraint around the source image.Onthe x-axis is the radius of the constraint set, and on the y-axis is the distance in representation spacebetween the minimizer of objective (4) within the constraint set and the target image, normalized bythe norm of the representation of the target image: i.e., a point (xi, y" on the graph corresponds toy = min∣∣δk2≤χi ∣∣R(x + δ) - R(Xtarg)∣∣2∕∣IR(Xtarg)∣∣2∙ Notably, We are unable to closely matchthe representation of the target image for the robust network until the norm constraint grows verylarge, and in particular much larger than the norm of the perturbation that the model is trained tobe robust against (ε in objective (3)). Shown are 95% confidence intervals over random choice ofsource and target images.
Figure 5: A visualization of the final solutions to the optimizing objective (4) with PGD whenconstraining the solution to lie in an '2 ball around the source image for an adversarially robustneural network. We note that even the radius of the constraint set is small and we cannot match therepresentation very well, salient features of the target image still arise.
Figure 6: Robust representations yield semantically meaningful embeddings. Target: random im-ages from the test set (col. 1-5) and from outside of the training distribution (6-10); Result: imagesobtained from optimizing inputs (using Gaussian noise as the source image) to minimize '2-distanceto the representations of the corresponding image in the top row. (More examples appear in Ap-pendix B.1.)tually plausible manner without any of the “ghosting” artifacts present in input-space interpolation.
Figure 7: Correspondence between image-level patterns and activations learned by standard androbust models on the Restricted ImageNet dataset. Starting from randomly chosen seed inputs(noise/images), We use PGD to find inputs that (locally) maximally activate a given componentof the representation vector (cf. Appendix A.6.1 for details). In the left column we have the seedinputs x0 (selected randomly), and in subsequent columns we visualize the result of the optimiza-tion (5), i.e., x0, for different activations, with each row starting from the same (far left) input x0 for(top): a robust (adversarially trained) and (bottom): a standard model. Additional visualizations inAppendix B.3, and similar results for ImageNet in B.3.2.
Figure 8: Maximizing inputs x0 (found by solving (5) with x° being a gray image) and most or leastactivating images (from the test set) for two random activations of a robust model trained on theRestricted ImageNet dataset. For each activation, we plot the three images from the validation setthat had the highest or lowest activation value sorted by the magnitude of the selected activation.
Figure 9: Figure reproduced from (Olah et al.,2017)—a visualization of a few components ofthe representation layer of GoogLeNet. Whileregularization (as well as Fourier parameteriza-tion and colorspace decorrelation) yields visu-ally appealing results, the visualization does notreveal consistent semantic concepts.
Figure 10: A visualization of the first four com-ponents of the representation layer of VGG16when regularization via random jittering and ro-tation is applied. Figure produced using the Lu-cid a visualization library.
Figure 12: Image interpolation using robust representations compared to their image-space coun-terparts. The former appear perceptually plausible while the latter exhibit ghosting artifacts. Forpairs of images from the Restricted ImageNet test set, we solve (6) for λ varying between zeroand one, i.e., we match linear interpolates in representation space. Additional interpolations appearin Appendix B.2.1 Figure 17. We demonstrate the ineffectiveness of interpolation with standardrepresentations in Appendix B.2.2 Figure 18.
Figure 13: Robust representations yield semantically meaningful inverses: Original: randomly cho-sen test set images from the Restricted ImageNet dataset; Inverse: images obtained by inverting therepresentation of the corresponding image in the top row by solving the optimization problem (1)starting from: (a) different test images and (b) Gaussian noise.
Figure 14: Robust representations yield semantically meaningful inverses: (Original): randomlychosen out-of-distribution inputs; (Inverse): images obtained by inverting the representation of thecorresponding image in the top row by solving the optimization problem (1) starting from Gaussiannoise.
Figure 15: Standard representations do not yield semantically meaningful inverses: (Original): ran-domly chosen test set images from the Restricted ImageNet dataset; (Inverse): images obtained byinverting the representation of the corresponding image in the top row by solving the optimizationproblem (1) starting from Gaussian noise.
Figure 16: Visualization of inputs that are mapped to similar representations by models trained onthe ImageNet dataset. Target (χ2) & Source (xi): random examples image from the test set; Robustand Standard (xi): result of minimizing the objective (4) to match (in '2-distance) the representationof the target image starting from the corresponding source image for (top): a robust (adversariallytrained) and (bottom): a standard model respectively. For the robust model, We observe that theresulting images are perceptually similar to the target image in terms of high-level features, whilefor the standard model they often look more similar to the source image which is the seed for theoptimization process.
Figure 17: Additional image interpolation using robust representations. To find the interpolationin input space, we construct images that map to linear interpolations of the endpoints in robustrepresentation space. Concretely, for randomly selected pairs from the Restricted ImageNet test set,we use (1) to find images that match to the linear interpolates in representation space (6).
Figure 18: Image interpolation using standard representations. To find the interpolation in inputspace, we construct images that map to linear interpolations of the endpoints in standard represen-tation space. Concretely, for randomly selected pairs from the Restricted ImageNet test set, weuse (1) to find images that match to the linear interpolates in representation space (6). Image spaceinterpolations from the standard model appear to be significantly less meaningful than their robustcounterparts. They are visibly similar to linear interpolation directly in the input space, which is infact used to seed the optimization process.
Figure 19: Correspondence between image-level features and representations learned by a ro-bust model on the Restricted ImageNet dataset. Starting from randomly chosen seed inputs(noise/images), we use a constrained optimization process to identify input features that maximallyactivate a given component of the representation vector (cf. Appendix A.6.1 for details). Specif-ically, (left column): inputs to the optimization process, and (subsequent columns): features thatactivate randomly chosen representation components, along with the predicted class of the feature.
Figure 20: Correspondence between image-level features and representations learned by a ro-bust model on the Restricted ImageNet dataset. Starting from randomly chosen seed inputs(noise/images), we use a constrained optimization process to identify input features that maximallyactivate a given component of the representation vector (cf. Appendix A.6.1 for details). Specif-ically, (left column): inputs to the optimization process, and (subsequent columns): features thatactivate select representation components, along with the predicted class of the feature.
Figure 21: Correspondence between image-level patterns and activations learned by standard androbust models on the Restricted ImageNet dataset. Starting from randomly chosen seed inputs(noise/images), we use PGD to find inputs that (locally) maximally activate a given component ofthe representation vector (cf. Appendix A.6.1 for details). In the left column we have the original in-puts (selected randomly), and in subsequent columns we visualize the result of the optimization (5)for different activations, with each row starting from the same (far left) input for (top): a robust(adversarially trained) ResNet-50 model, (middle): a standard ResNet-50 model and (bottom): astandard VGG16 model.
Figure 22: Correspondence between image-level patterns and activations learned by standard androbust models on the complete ImageNet dataset. Starting from randomly chosen seed inputs(noise/images), we use PGD to find inputs that (locally) maximally activate a given component ofthe representation vector (cf. Appendix A.6.1 for details). In the left column we have the original in-puts (selected randomly), and in subsequent columns we visualize the result of the optimization (5)for different activations, with each row starting from the same (far left) input for (top): a robust(adversarially trained) ResNet-50 model, (middle): a standard ResNet-50 model and (bottom): astandard VGG16 model.
Figure 23: Visualization of the results adding various neurons, labelled on the left, to randomlychosen test images. The rows alternate between the original test images, and those same imageswith an additional feature arising from maximizing the corresponding neuron.
