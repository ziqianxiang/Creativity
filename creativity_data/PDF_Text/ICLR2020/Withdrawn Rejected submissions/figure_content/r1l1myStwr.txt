Figure 1: The performance of MOCA on the sinusoid regression problem. Right: The belief over run lengthversus time. The intensity of each point in the plot corresponds to the belief in run length at the associated time.
Figure 2: Performance of MOCA versus baselines in sinusoid regression (left; lower is better), RainbowMNIST (center; higher is better), and miniImageNet (right; higher is better), versus hazard rate. Note thatfor both problems, MOCA always outperforms the baselines and the performance degrades only slightly fromthe performance of the oracle. In contrast, sliding window methods result in severely degraded performance.
Figure 3: Performance change from augmenting a model trained with MOCA with task supervision at test time(violet) and from using changepoint estimation at test time for a model trained with task-supervision (teal), forsinusoid (left), Rainbow MNIST (center), and miniImageNet (right).
Figure 4: Performance versus the training horizon (T)for the sinusoid with hazard 0.01. The lowest hazardWaS used to increase the effects of the short traininghorizon. A minor decrease in performance is visiblefor very small training horizons (around 20), but flat-tens off around 100 and above. It is expected that thesediminishing marginal returns will occur for all systemsand hazard rates.
Figure 5: Test negative log likelihood ofMOCA on the sinusoid problem With par-tial task segmentation. The partial segmenta-tion during training results in negligible per-formance increase, while partial supervisionat test time uniformly improves performance.
Figure 6: Time per iteration versus iteration number at testtime. Note that the right hand side of the curve shows theexpected linear complexity expected of MOCA. Note thatfor these experiments, no hypothesis pruning was performed,and thus at test time performance could be constant time asopposed to linear. This figure shows 95% confidence inter-vals for 10 trials, but the repeatability of the computation timeis consistent enough that they are not visible.
