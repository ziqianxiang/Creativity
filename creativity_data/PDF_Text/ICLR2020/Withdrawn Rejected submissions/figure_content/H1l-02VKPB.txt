Figure 1: An illustration of the proposed topology-aware pooling layer that selects k = 3 nodes. Thisgraph contains four nodes, each of which has 2 features. Given the input graph, we firstly use anattention operator to compute similarity scores between every pair of connected nodes. Here, we useself-attention without linear transformation for notation simplicity. In graph (b), We label each edgeby the similarity score between its two ends. Then we compute the ranking score of each node bytaking the average of the similarity scores between it and its neighboring nodes. In graph (c), welabel each node by its ranking score and bigger node indicates a higher ranking score. By selectingthe nodes with the k = 3 largest ranking scores, the selected graph is shown in graph (d).
Figure 2: An illustration of the topology-aware pooling network.㊉ denotes the concatenationoperation of feature vectors. Each node in the input graph contains three features. We use a GCNlayer to transform the feature vectors into low-dimensional representations. We stack two blocks, eachof which consists of a GCN layer and a TAP layer. A global reduction operation such as max-poolingis applied to the outputs of the first GCN layer and TAP layers. The resulting feature vectors areconcatenated and fed into the final multi-layer perceptron for prediction.
Figure 3: Illustrations of coarsened graphs generated by TAP and TAP w/o GCT. Here, GCT denotesthe graph connection term. The input graph (a) contains 12 nodes. The pooling layers select 6 nodesto form new graphs. The nodes that are not selected are colored black. The new graph in (b) generatedby TAP w/o GCT is sparsely connected. The new graph generated by TAP is illustrated in (c), whichis shown to be much better connected.
Figure 4: Comparison results of TAPNets usingdifferent λ values in TAP layers.
