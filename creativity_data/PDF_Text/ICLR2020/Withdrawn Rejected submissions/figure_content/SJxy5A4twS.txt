Figure 1:	Superbloom model architectureimprove efficiency, we hash each element as follows. Given m hash functions hj, j ∈ {1, . . . , m},each element s is represented by the hashes (h1 (s), . . . , hm(s)), which we refer to as a Bloomdigest, given its similarity to the Bloom filter data structure. The set of values the hashes can takeis much smaller than the original spaces SI , SO, which allows us to reduce the vocabulary size andthus the size of embedding matrices.
Figure 2:	Illustration of approximate and exact inference, with a number of hashes m = 2, a four-to-one hashing scheme, and a beam width B = 2.
Figure 3: Rec@1 with respect to label frequency, starting from the most frequent labels.
Figure 4:	Examples of Superbloom model predictions. For each example, we output the top 10predictions of the model (computed using Algorithm 1 with a beam width B = 10). The entitynames shown here are obtained by removing the prefix “https://en.wikipedia.org/wiki/” from theentity URL.
Figure 5:	Natural language fill-in-the-blank examples. BERT is the base BERT model in Devlinet al. (2018); baseline-h256 and hash20-h1024 are the Superbloom models with 1M vocabulary,with model parameters listed in Table 4.
