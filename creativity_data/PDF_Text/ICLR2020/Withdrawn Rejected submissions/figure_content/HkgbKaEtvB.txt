Figure 1: Application of a neural network in the context of remote sensing. Here, hundreds of inputfeature maps might be available (multi-spectral image data collected at different times). Transferringdata from the server to the client running the model can be extremely time-consuming. Our frameworkuses various types of selection masks that can be adapted to the specific transfer capabilities betweenthe server and the client (e.g., if channel- or pixel-wise data transfers are possible). Also, a differentloss Qi can be assigned to each individual mask to penalize selections made by it. The masks as wellas the given network are optimized simultaneously in an end-to-end fashion to achieve a good modelperformance and to select only small amounts of the input data. During the inference phase, only theselected parts have to be transferred. Similar bandwidth-restricted scenarios can be found in otherdata-intensive disciplines as well such as astrophysics or in the context of sensor data analytics.
Figure 2: Different selection masks that can be used to select parts of the input data. For each of themasks, an individual loss Qi can be defined to penalize selections made by that mask. While the finalmasks are discrete, differentiable surrogates are used during training.
Figure 3: Implementation of masks•	channel(any): To select an arbitrary number of k input channels, a joint mask mD ∈{0, 1}1×1×k×2 is used, which contains, for each of the k channels, two weights. For instance,a mask mD with m[D1,1,1,:] = (1, 0) and m[D1,1,2,:] = (0, 1) corresponds to selecting the firstbut not the second channel. Before applying the mask to an image x ∈ Rw×h×k, the firsttwo axes are broadcasted, which yields a mask mD ∈ {0, 1}w×h×k×2.
Figure 4: block(any)3Under review as a conference paper at ICLR 2020Algorithm 1: LearnSelectionMasks(f, T)1234567891011121314Input : model f and training set Tm J InitAllMasks()	// initialize all selection masks
Figure 5: extend / mergeavailable, whose transfer costs vary (e. g., compressed images or thumbnails of different sizes). Often,pre-trained networks with a fixed input structure are given. The selection of different versions forsuch networks can be handled via simple operators, see Figure 5: The extend operator can be usedto extend a given input feature map (e. g., by generating ten compressed versions of different quality),whereas the merge operator can combine feature maps in a user-defined way (e. g., by summing upthe input channels). For instance, an extend operation followed by a channel(xOr) selectionand a merge operation can be used to gradually select a certain version of each input channel withoutsignificantly changing the input for a given network in each step, thus allowing to learn masks forpre-trained networks without having to retrain the network weights from scratch, see Section 4.
Figure 6: channel(any) mask realization results on remote, supernovæ, and cifar10. Theblack line is the average value of the runs and individual runs are displayed in different colors.
Figure 7: Selected channels for remotenels. We used remote, supernovæ, and cifar10 as datasets, for which different outcomeswere expected. For each of the c channels, we assigned the same mask loss Qi = 1/c. The overallmask loss Q was the sum over all channels, which corresponds to the ratio of the data that need to betransferred. The outcome is shown in Figure 6. As expected, channel-wise selection worked best onremote due to many channels carrying similar information. Only if less than 20% of the channelswere selected, the accuracy started to drop. In Figure 7, the selection process is sketched, whereeach row represents a different epoch (from top to bottom: 0, 50, 100, 150, 200) and where eachcolumns corresponds to one of the channels. For supernovæ, the removal of a single channel didnot significantly affect the classification accuracy. For some runs, all channels were removed at once,which indicates that the steps made for λ were too large (thus, a smaller λfac should be considered).
Figure 9: pixel(any) mask realization results on remote, supernovæ, and cifar10.
Figure 10: JPEG on cifar10were initialized in such a way that only the version with the highest quality was selected initially.
Figure 11: Reduced images7Under review as a conference paper at ICLR 20200.90-0.85 -0.80-ycarucc1.00-S 0.98-10.96-W 0.94 -.0 .8 .6 .41000Q ssol GEPJ(a) cifar10S 0.6-m 0.4-‘二工⅛ 0.2-
Figure 13: Results for the combination of selection masks on cifar10, svhn, and mnist, whereJPEG qualities for each channel were used and, at the same time, pixels could be selected.
Figure 12: Combination of multiple selection masks.
Figure 14: Influence of λ5 ConclusionsThe transfer of data between servers and clients can become a major bottleneck during the inferencephase of a neural network. We propose a framework that allows to automatically select those partsof the data needed by the network to perform well, while, at the same time, minimizing the amountof selected data. Our approach resorts to various types of selection masks that are jointly optimizedtogether with the corresponding network during the training phase. Our experiments show that it isoften possible to achieve a good accuracy with significantly less input data needed to be transferred.
Figure 15: Influence of the ratio betweenexploration and fixation during training.
