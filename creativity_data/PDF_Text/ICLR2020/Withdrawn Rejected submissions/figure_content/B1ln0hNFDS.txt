Figure 1: Explainer network. We use an explainer network to disentangle object-part features thatused by a pre-trained performer network. The explainer network disentangles object-part features(B) from the performer to mimic signal processing in the performer. The explainer network can alsoinvert the disentangled object-part features to reconstruct features of the performer without muchloss of information. We compare traditional features (A) in the performer and the disentangledfeatures (B) in the explainer on the right. The gray and green lines indicate the information-passroute during the inference process and that during the diagnosis process, respectively.
Figure 2: The explainer network (left). Detailed structures within the interpretable1 track, the ordi-nary track, and the decoder are shown on the right. People can change the number of conv-layersand FC layers within the encoder and the decoder for their own applications.
Figure 3: Visualization of interpretable filters in the explainer and ordinary filters in the performer.
Figure 4:	Comparisons of feature maps of different performers corresponding to different valuesof p. 95.8% of VGG-16 features were disentangled as object parts, while only 58.1% of AlexNetfeatures were disentangled as object parts. Note that each visualized filter of the performer does notstrictly represent a single object part, which is different from interpretable filters in the explainer.
Figure 5:	Grad-CAM attention maps and quantitative analysis. We used (Selvaraju et al., 2017)to compute grad-CAM attention maps of interpretable feature maps in the explainer and ordinaryfeature maps in the performer. Interpretable filters focused on a few distinct object parts, whileordinary filters separated its attention to both textures and parts. We can assign each interpretablefilter with a semantic part. E.g. the network learned 58, 167, and 243 filters in the conv-interp-2 layerto represent the head, neck, and torso of the bird, respectively. We used the linear model in (Zhanget al., 2018b) to estimate contributions of different filters to the classification score. We summedup contributions of a part’s filters as the part’s quantitative contribution. Please see Appendix D formore results.
