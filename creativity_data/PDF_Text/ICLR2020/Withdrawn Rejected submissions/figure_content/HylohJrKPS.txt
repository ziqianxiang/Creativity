Figure 1: Categorization of Deep Novel View Synthesis. Our problem belongs to the novel viewsynthesis on a single image domain, and our pipeline is unsupervised.
Figure 2: Optical Zoom vs 3D zoomtrained disparity estimation network with transfer learning. Our 3D-Zoom Net is based on a fullyconvolutional network architecture that learns the under-laying 3D structure of the scene without theneed of intermediate disparity as it is trained based on a novel back re-projection reconstruction costthat enforces both 3D geometry and natural appearance. Additionally, we include an adversarialnetwork that acts as a no-reference measure that penalizes unnaturally rendered areas. Our proposedmodel, Deep 3D-Zoom Net, can perform inference of naturally looking 3D-zoomed images veryfast. We show the efficacy of our proposed model in generating 3D-Zoomed images at various zoomfactors on the KITTI (Geiger et al., 2012; Menze & Geiger, 2015) and Cityscapes (Cordts et al.,2016) datasets.
Figure 3: Deep 3D-Zoom Net for inference. It consists of synthesis network, the refinement blockand the blending operation.
Figure 4: Fully convolutional synthesis network. Our synthesis network follows the UNET archi-tecture with residual blocks.
Figure 5: Training strategy for Deep 3D-Zoom Net. The re-projection reconstruction loss is com-puted between the zoomed-out re-projection and the input image. An adversarial loss is computedover the network output.
Figure 6: Results on KITTI2012 / NIQE for sampled images (top, bottom) and subjective compar-ison with the results showed in (Liu et al., 2018) for visual quality only. In terms of natural imagegeneration, our Deep 3D-Zoom Net outperforms geometric-aware networks with no visible artifactsfor the equivalent zoom factors (1.6 top, 2.4 bottom). Note ground truth is just for reference, andwas not used to train our model.
Figure 7: Model performance on Cityscapes dataset. Images generated with different zoom factorsshowing our network performs well even on unseen scenes. Forward warping, guided by disparityestimation from (Gonzalez Bello & Kim, 2019), produces blurred, occluded, and deformed results.
Figure 8: Results from ablation studies / NIQE score. A progressive improvement in terms ofstructure and sharpness can be appreciated from our model trained without perceptual loss to ourmodel trained with perceptual loss and refinement block.
Figure 9: (a) Our fully convolutional patch discriminator network. (b) Adversarial learning ablationstudy. From top to bottom, input images, Deep 3D-Zoom Net with GAN, and Deep 3D-Zoom Netw/o GAN. The adversarial loss helps by reducing ghosting artifacts as can be appreciated in thepower generator (right) and car boot (left).
Figure 10: Additional results on ablation study for our Deep 3D-Zoom Net with the KITTI2012 dataset. Different zoom factors were used to subjectively matchthe next, or ’t+1’, frame in the dataset sequence for reference only. It is noted that the better results are obtained by incorporating the perceptual loss, the refinementblock and the adversarial loss into our Deep 3D-Zoom Net.
Figure 11: Selection volume activation maps. For the given input image at zoom f actor = 2.0 the 32 selection volume channels are activated from the far-distantto the near-distant objects in the scene.
Figure 12: Subjective comparison on zoomed versions of the input images by three different methodsfor zoom factor 2.5. It is noted that the forward warping generates low-quality 3D-zoomed versionsof the input image, while our Deep 3D-Zoom Net generates cleaner and sharper results with theinput image’s structures well preserved.
