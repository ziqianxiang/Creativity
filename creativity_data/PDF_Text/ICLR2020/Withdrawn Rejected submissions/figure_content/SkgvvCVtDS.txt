Figure 1: Steps of the Learning algorithmwhere the objective C ∈ Rn, right-hand side b ∈ Rm, constraint matrix A ∈ Rm×n, and the number ofvariables is greater than the number of constraints, i.e., n > m. The goal of an LP is to find an optimaldecision variable X that minimizes the c>x value over the feasibility region P = {x ∈ Rj | Ax = b}.
Figure 2: Performance comparison of the supervised learning with Q*-values (blue line) against arandom pivoting rule (purple line), pure Dantzig’s rule (orange line), pure steepest edge rule (greenline), and the best possible strategy (red line) for weighted and unweighted iterations on training (firstrow) and test sets (second row) for 850 epochs.
Figure 3: Performance comparison of the deep reinforcement learning algorithm (blue line) against arandom pivoting rule (purple line), pure Dantzig’s rule (orange line), pure steepest edge rule(greenline), and the best possible strategy (red line) for weighted and unweighted iterations on training (firstrow) and test (second row) sets for 500 epochs, given with both the reinforcement learning loss andtrue loss against Q*-values.
