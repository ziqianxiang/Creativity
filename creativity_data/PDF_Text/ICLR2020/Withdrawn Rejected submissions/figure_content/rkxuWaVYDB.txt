Figure 1: Average performance loss vs. attack amplitude ε: (top-left) discrete MountainCar, (top-right) Cartpole, (bottom-left) continuous MountainCar, (bottom-right) LunarLander. The attackpolicy φ has been trained only for ε = 0.05 In red is shown the FGM attack, in blue the attackproposed in this paper.
Figure 2:	Game of Pong. (Left part) Images and corresponding agent's action distributions beforeand after the attack. (Right part) Average agent,s reward at different phases of the attack training.
Figure 3:	Un-discounted value of the perturbed policy of the main agent. Left: our proposed attack,middle: attack in Huang et al. (2017), right: attack in Pattanaik et al. (2018).
Figure 4: Performance loss distribution under the optimal and gradient-based attacks with amplitudeε = 0.05 for the discrete MountainCar environment. The attacks are against policies trained usingDQN (top) and DRQN (bottom).
Figure 5: Mountaincar environment - distribution of the average discounted reward performancedecrease for ε = 0.05, with respect to the main agent’s policy the attacker has trained for.
Figure 6: Cartpole environment - distribution of the average discounted reward performance decreasefor ε = 0.05. On the left we show the distribution with respect to the main agent’s policy the attackerwas trained on, and on the right an average across all the main agent’s policies.
Figure 7: Continuous MountainCar environment - distribution of the average discounted rewardperformance decrease for ε = 0.05. On the left we show the distribution with respect to the mainagent’s policy the attacker was trained on, and on the right an average across all the main agent’spolicies.
Figure 8: LunarLander environment - distribution of the average discounted reward performancedecrease for ε = 0.05. On the left we show the distribution with respect to the main agent’s policythe attacker was trained on, and on the right an average across all the main agent’s policies.
Figure 9: Average performance loss vs. attack amplitude ε in the various environments, when theadversary attacks the main agent’s policy it was trained on. (top-left) discrete MountainCar, (top-right)Cartpole, (bottom-left) continuous MountainCar, (bottom-right) LunarLander. Attacks are againstusing policies obtained using DQN, DRQN, or DDPG.
Figure 10: Mountaincar environment. On the left is shown the average episode loss when exploringusing a gradient-based exploration methods vs. uniform noise exploration. On the right are shownstatistics regarding episodes reward for ε = 0.05, and norm `2, after training.
Figure 11: Diagram of the adversarial agent perturbing observations of the environment. On therightis shown the environment model for the malicious agent. The equivalent MDP is denoted by M.
