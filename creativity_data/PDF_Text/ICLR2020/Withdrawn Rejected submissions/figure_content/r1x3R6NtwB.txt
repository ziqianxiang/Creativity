Figure 1: Illustration of the proposed method. (a) The proposed DroPGrad method imposes anoise term n to augment the gradient in the inner-loop optimization during the meta-training stage.
Figure 2: Comparison between the proposed Binary and Gaussian DropGrad methods. Wecompare the 1-shot (left) and 5-shot (right) performance OfMAML (Finn et al., 2017) trained withtwo different forms of DropGrad under various dropout rates on mini-ImageNet. The proposedDropGrad method is particularly effective with the dropout rate in [0.1,0.2]. Moreover, the GaussianDropGrad method consistently obtains better results compared to the Binary DropGrad scheme.
Figure 3:	Validation loss over training epochs. We show the validation curves of the MAML (left)and MetaSGD (right) frameworks trained on the 5-shot mini-ImageNet dataset. The curves validatethat the proposed DropGrad method alleviates the overfitting problem.
Figure 4:	Few-shot reinforcement learning results. Two settings, HalfCheetah-Dir (left) and Ant-Dir (right), are considered in our experiments using the MAML-TPRO framework. We show thereward curves after the model is updated with the few trajectories and both rewards converge favor-ably against the original training.
Figure 5: Qualitative results of object online tracking on the OTB2015 dataset. Top and bottomroWs shoW the sample results of MetaCREST and MetaSDNet, respectively. Red boxes are theground truth, yelloW boxes represent the original results, and green boxes stand for the results Wherethe DropGrad method is applied.
