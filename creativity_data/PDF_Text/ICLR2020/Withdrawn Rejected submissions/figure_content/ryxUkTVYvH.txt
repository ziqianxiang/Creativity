Figure 1: Face completion results of the proposed method on CelebA-HQ (Karras et al., 2017) at 1024 × 1024resolution. The leftmost column are masked images while the rightmost are synthesized images. The learnedFOAM filters are shown with higher intensities meaning more attention. At lower resolutions, the model focusesmore on learning coarse structures (i.e. the lower-frequency signals). As the resolution increases, the modelpays more attention to finer details (i.e. the higher-frequency information). Therefore, the FOAM partiallyand implicitly performs as a “band-pass filter” guiding the generation process. For instance, the model paysmore attention to regions with richer details, such as hair and eyes, especially at high resolutions. The learnedFOAM is also relatively stable when the target regions are similar, see the last two rows. Best viewed in colorand magnification.
Figure 2: Overview of the proposed completionmodel. See text for details.
Figure 3: Illustration of computing facial landmarks for structure-awarecompletion. See text for detail.
Figure 4: The ablation study shows the impact of es-sential components of our method. A model that istrained with adversarial Ladv and regular reconstructionloss L1 generates only blurry images. After adopt-ing the progressive training method and a set of de-signed loss functions, the synthesized image quality isimproved. By incorporating FOAM, the model focuseson learning only finer details while growing, resultingin sharper images with fewer distortions. Best viewedin magnification.
Figure 5: Illustration of the FOAM using an example of growing a 32 × 32 network to 64 × 64. The proposedFOAM consists of a read and a write operation to realize attentive “band-pass filters”. See text for detail.
Figure 6: Examples of high-resolution face completion results by our method at 1024 X 1024 resolution.
Figure 7: Examples of controlling attributes. All images are at512 X 512 resolution. The leftmost column are masked images,and the rest are generated faces. More in Fig. 14 and Fig. 15.
Figure 8: Examples of joint attribute and expression control. All images are at 512 × 512 resolution. Thoughthe source and synthesized faces have different identities, their expressions are very similar.
Figure 9: Comparisons on the naturalness: ours andCTX (Yu et al., 2018). Left: There was a significantlyhigher percentage of images completed by our modelthat looked more realistic than those completed by CTX.
Figure 11: High-resolution face completion results with center masks. All images are at 1024 × 1024 resolu-tion. For each group, from left to right: masked, synthesized, and real images.
Figure 12: More examples of High-resolution face completion results with center masks. All images are at1024 × 1024 resolution. For each group, from left to right: masked, synthesized, and real images.
Figure 13: More examples of High-resolution face completion results with hand-drawn masks. All images areat 1024 × 1024 resolution. For each group, from left to right: masked, synthesized, and real images.
Figure 14: Face completion results with attribute controller. Attribute “Male vs Female” is used to control theappearance. Landmarks from source actors are used to control the facial expressions of synthesized images.
Figure 15: Snapshots of face completion results with relative and soft attribute controller. The Demo videowill be available as supplementary material.
Figure 16: Face completion results with attribute controller. Attribute “Male vs Female” is used to control theappearance. Landmarks from source actors are used to control the facial expressions of synthesized images.
Figure 17: Examples of images used in the user study. The preferred images are marked with red boxes.
Figure 18: The progressive training process of our approach. The training of the completion network(or the “generator” G) and the discriminator D starts at low resolution (4 × 4). Higher layers areadded to both G and D progressively to increase the resolution of the synthesized images. TheIr X r I cubes in the figure represent convolutional layers that handle resolution r. For the conditionalversion, attribute labels Aobs are concatenated to the latent vectors. The discriminator D splits intotwo branches in the final layers: Dcls that classifies if an input image is real, and Dattr that predictsattribute vectors. Note that XG and XD are both a set of inputs as defined in the paper. We useimages in this Figure as a simplified illustration.
Figure 19: Illustrations of a single layer of our architecture. There are skip connections betweenmirrored encoder and decoder layers. Left: the structure of the completion network; the skip con-nection is a copy-and-concatenate operation. This structure helps preserve the identity informationbetween the synthesized images and real faces, resulting in little deformation. Right: the structureof the conditional completion network; residual connections are added to the encoder, and the skipconnections are residual blocks instead of direct concatenation. The attributes of the synthesizedcontents can be manipulated more easily with this structure. Each blue rectangle represents a set ofConvolutional, Instance Normalization and Leaky Rectified Linear Unit (LeakyReLU) (Maas et al.,2013) layers.
