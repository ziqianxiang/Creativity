Figure 1: Multi-task learning for image segmentation with lazy labels. The figure uses ScanningElectron Microscopy (SEM) images of food microstructures as an example and demonstrates asegmentation problem of three classes, namely air bubbles (green), ice crystals (red) and backgroundrespectively. Most of the training data are weak annotations containing (i) partial marks of ice crystalsand/or air bubbles instances and (ii) fine separation marks of boundaries shared by different instances.
Figure 2: Architecture of the multi-task U-net. The left part of the network is a contracting pathsimilar to the standard U-net. For multi-task learning, We construct several expansive paths withspecific multi-task blocks. At each resolution, task 1 (Detection in yellow) and task 3 (Segmentationin red) run through a common sub-block, but the red path learns an additional residual to betterlocalize object boundaries. Long skip connections with the layers from contracting path are builtfor yellow/red paths via concatenation. Task 2 (Separation, in green) mainly follows a separatedexpansive path, with its own up-sampled blocks. A link with the last layer of task 3 is added via askip connection in order to integrate accurate boundaries in the separation task.
Figure 3:	Example of annotated images. Some of the annotations are missing because not all imagesare labelled for task 1 and task 2. The marks in red are for air bubbles and the ones in green are forice crystal instances. The blue curves on the third row are labels for interfaces of touching objects.
Figure 4:	The error bars for the PL andmulti-task U-net, each computed from 8different experimentsWe train a single task U-net (i.e., without the multi-task block) on the weakly labelled set (task 1),with the 15 annotated images. The single task U-net on weak annotations gives an overall dice scoreat 0.72, the lowest one among the three other methods tested. One reason for the low accuracy of thesingle task U-net on weak (inaccurate) annotations is that in the training labels, the object boundariesare mostly ignored. Hence the U-net is not trained to recover them, leaving large parts of the objectnot recognized. Second, we consider strong annotations as training data, without the data of the othertasks, i.e. only 2 images with accurate segmentation masks are used. The score of the U-net trainedon SL is only 0.82, which is significantly lower than the 0.94 obtained by the multi-task U-net.
Figure 5:	Segmentation and separation results (best viewed in color). Left: the computed contoursare shown in red for air bubbles and green for ice crystals. While multi-task U-net and PL supervisednetwork both have good performance, PL misclassifies the background near object boundaries. Right:Examples of separation by the multi-task U-net and the ground truth.
Figure 6: The image (left), the predicted probability map (middle) from the detection task, and theground truth segmentation mask (right). The red and green on the middle and right images stand forair bubble and ice crystals respectively.)4.2 H&E-stained image dateset for gland segmentationWe also apply the approach to the segmentation of tissue in histology images. In this experiment,we use the GlaS challenge dataset (Sirinukunwattana et al., 2017) that consists of 165 H&E stainedimages. The dataset is split into three parts, with 85 images for training, and 60 for offsite test and 20images for onsite test (we will call the latter two sets Test part A and Test part B respectively in thefollowing).
Figure 7: Segmentation results on the gland dataset (best view in color). The ground truth and theresults. For (c) and (d), Red contour denotes the results from 9.4% strong labels; Green contourdenotes results from 4.7% strong labelsof the strong labels are used for training the multi-task U-net as illustrated in Algorithm 1 (in theappendix).
Figure 8: Overall dice scores for the air bubbles and ice crystals plotted versus the test image number.
Figure 9: Twelve patches from the twelve test images respectively.
Figure 10: Comparison of multi-task U-net with the GAC and Grabcut methods using the weak labelsof the validation data. Green curves: contour of the ice crystals. Red curves: air bubbles.
Figure 11: Gland segmentation dataset with SL in the first row (highlighted in red), WL in thesecond row for detection (in orange) and weak labels in the third row for separation (in dark green).
