Figure 1: Learning the (h, y) pair transitions the neighborhood of h (represented as an ellipse) toa new state in which a memory h0 is drawn as the distribution of positive memories. Small circlesrepresent the uncertainty of using a particular memory to model h0. In the new memory configuration,we age positive keys (now faded in grey) making them more likely of being overwritten by othertraining examples.
Figure 2: A memory network consists of a neural encoder and an external memory that augments itscapacity by preserving longer distinct versions of h during training. Some memory entries (in gray)are positive candidates to correctly answer to the embedding h.
Figure 3: Architecture of the neural dialogue model that incorporates a KB. Note the ith decodingstep of the word y^i given the attention over the external memory which encodes KB triplets in itskeys and that uses memory dropout for model regularization.
Figure 4: Average Pearson correlation values between pairs of keys at different training steps. Thehigher, the more correlation, and the maximum value is 1.0. As training progresses, redundant keysmay be allocated to the external memory.
Figure 5: Entity F1 scores considering the use of memory dropout in the MANN model.
Figure 6: Entity F1 score of the testing dataset considering different memory sizes.
