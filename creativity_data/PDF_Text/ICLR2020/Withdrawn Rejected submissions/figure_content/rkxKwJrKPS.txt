Figure 1: Method diagram for QXplore. We define two Q-functions which sample trajectories fromtheir environment and store experiences in separate replay buffers. Q is a standard state-action value-function, whereas Qx’s reward function is the unsigned temporal difference error of the current Q ondata sampled from both replay buffers. A policy defined by Qx samples experiences that maximizethe TD-error of Q, while a policy defined by Q samples experiences that maximize discounted rewardfrom the environment.
Figure 2: A neural network trained to predict a constant value does not interpolate or extrapolatewell outside its training range, which can be exploited for exploration. Predictions of 3-layer MLPsof 256 hidden units per layer trained to imitate f (x) = 0 on R → R with training data sampleduniformly from the range [-0.75, -0.25] ∪ [0.25, 0.75]. Each line is the final response curve of anindependently trained network once its training error has converged (MSE < 1e-7).
Figure 3: Performance of QXplore compared with RND and -greedy sampling. QXplore outperformsRND and -greedy on continuous control tasks. QXplore performs better due to efficient explorationsampling by Qx and the separation of the exploration and exploitation objectives. Q indicates theperformance of our exploitation Q-function, while Qx indicates the performance of our explorationQ-function, whose objective does not directly maximize reward but which may lead to high rewardregardless.
Figure 4: Example trajectories showing Qx’s behavior late in training that is distinctive of TD-errormaximization. The corresponding Q network reliably achieves reward at this point. In ”fake-out”,Qx approaches the reward threshold and suddenly stops itself. In ”cross and re-cross”, Qx crossesthe reward threshold going forward and then goes backwards through the threshold.
Figure 5: Plot showing the performance of two ablations, 1-Step Reward Prediction (QXplore-1-step)and Single-Policy QXplore (QXplore-value), compared to the original QXplore method. In the 1-Stepablation, Qx is trained to predict a combination of extrinsic reward and reward prediction error, andfails to make progress. In the Single-Policy ablation, the policy converges faster, but to a worse policythan vanilla QXplore due to the need to balance TD-error and extrinsic reward maximization.
Figure 6: Plots showing the performance of QXplore where the objective of Qx is replaced by theRND exploration objective, as well as the mean position of the cheetah during an episode throughouttraining. While Qx does sample reward, it does so too infrequently to guide Q to learn the task.
Figure 7: The performance of QXplore’s Q function with Qx maximizing signed versus unsignedTD-error on two different reward variants of SparseHalfCheetah. While Q is able to learnthe task for all variants, performance is reduced with the signed objective. Performance on theSparseHalfCheetah variant with -1 to 0 reward function is shown shifted to match the axes ofthe 0 to 1 variant for comparison.
Figure 8: Learning rate sweeps for Q and Qx(b) FetchPush-v1D.1 RND Parameter SweepsAs we have adapted RND to operate with vector observations and continuous actions, we performedseveral hyperparameter sweeps to ensure a fair comparison. We report in Figure 10 the results ofvarying both predictor network learning rate “lr” and extrinsic reward weight “rw” independentlyon the SparseHalfCheetah task. The baseline values for these parameters used elsewhere are0.001 and 2 respectively. We observe that RND is fairly sensitive to reward weight, but a value of 1or two performs well, while a learning rate of 0.001 appears to learn faster early in training withoutloss of final performance.
Figure 9: Sample ratio sweeps for Q and Qxbaseline-qrat_50_qxrat_50----qrat θ qxrat lθθ—qrat_7 5_qxrat_2 5—qrat_2 5_qxrat_7 5-30 T0	2000	4000	6000	8000 IOOOOepisode(b) FetchPush-v1PJeMaJ-200--300--100-RND Parameter Sweepso-rwl-rwl-rwθl-rw05
Figure 10: Parameter sweeps for RND. A reward weight of either 1 or 2 works best, with a learningrate of 0.0001 a close second.
Figure 11: QXplore performance on SparseHalfCheetah with the 0 to 1 reward function.
Figure 12:	Several alternate initialization schemes for Q and Qx . While Q is adversely impacted, Qxis relatively robust even to very poor initializations such as “Normal” and “Uniform.”robustness to the lnoιsy tv' problem - Q500	・ YIQ×P∣oreQXpIore w/ Noisy ObsPJeMaJ-400robustness to the 'noisy tvl problem - Qx0-500-QXPlore 500000 I 1000000	1500000	2000000	2500000-QXpIore w/ Noisy Obs	updates1000000	1500000	2000000	2500000updates(a) Noisy observation effects on Q	(b) Noisy observation effects on Qxaverage position of cheetah3 2PJeMaJ1000000	1500000	2000000	2500000
Figure 13:	QXplore trained on a ‘noisy tv’ variant of SparseHalfCheetah where one element ofthe observation vector is normally distributed random value whose variance increases if the cheetahmoves in the negative direction. The performance of QXplore is not impacted in any way by thisnoise, and it trains as normal.
