Figure 1: Domain specific detection and segmentation. Our self-supervised method detects theskier well, while YOLO trained on a general dataset does not generalize to this challenging domain.
Figure 2: Method overview. Encoder-decoder network (S) with an attention mechanism defined byproposal-based detection (D) and spatial transformers (T, T -1). Via the use ofan inpainting network(I), our approach makes it possible to train this network in an entirely self-supervised manner onunknown scenes with a moving background and captured with a hand-held camera.
Figure 3: Ablation study on H36M. (a) Uniform sampling does not converge. (b) Joint training ofO and G (c) only G (d) direct regression of a single bounding box using O and G (e) Ours.
Figure 4: Multi-person detection and segmentation results, generated by sampling our modelmultiple times. As the model is trained on single persons this only works for non-intersecting cases.
Figure 5: Qualitative results on Ski-PTZ-camera and Handheld190k. Example results on thetest images. (a) The detection results show the predicted bounding box with red dashed lines, therelative confidence of the grid cells with blue dots and the bounding box center offset with greenlines. (b) Soft segmentation mask predictions. Note that in the second row, the moving clouds are notsegmented but the shadow of the person can be included.
Figure 6: Reconstructed scene and mask generated by Bielski & Favaro (2019) and our methodon training examples. On the Ski-PTZ-Dataset the poles and snow patches are segmented asforeground. On the Handheld190k dataset, the masks contain the foreground subject together withthe ground they are standing on.
Figure 7: Off-the-shelf inpainting results, on skiing. (a) Input image with the hidden middle part,followed by inpainting with (b) Pathak et al. (2016), (c) Yu et al. (2018) trained on ImageNet. (d) andYu et al. (2018) trained on Places2.
Figure 8: The capture setup of our in-house handheld posing dataset. The subject is recordedby three persons with handheld GoPro action cameras.
Figure 9: Detection and segmentation results on the test subjects of Ski-PTZ-camera and Hand-held190k dataset. (a) Detection result. The blue dots coincide with the grid cell centers and their sizeindicates the confidence of the bounding box proposals. The selected bounding box is illustrated witha red dashed line and the center of the grid cell yielding this proposal is connected to the center of thered box through the green line. (b) The inpainting result where the region inside the detected boundingbox is reconstructed by our inpainting network (only needed for training). (c) The synthesized imagewith the predicted foreground and background regions combined. (d) Segmentation mask result. (e)Groundtruth segmentation mask.
Figure 10: Detection and segmentation results on H36M. Results match in quality with those fromRhodin et al. (2019), with a slight bleeding due to not having a perfect background prediction oracle.
Figure 11: Optical flow failure cases. Top: Input images with no or too complex motion information.
Figure 12: Examples of heatmaps and detection results from Zhou et al. (2016) on H36M. Theheatmap generated by the ImageNet pre-trained model has multiple high activations corresponding toa given ImageNet category. Since the candidate bounding boxes are selected such as to overlay theconnected components of the heatmap, they can cover a large area. Therefore using these candidatesto efficiently localize the salient object can often fail.
Figure 13: Qualitative results comparing the bounding box detections obtained through our im-portance sampling strategy against the Gumbel-Softmax categorical reparameterization withtemperature hyperparameter set to 0.1. Top: Ours. Bottom: Categorical reparameterization. Theresults are taken from the same iteration for both models.
