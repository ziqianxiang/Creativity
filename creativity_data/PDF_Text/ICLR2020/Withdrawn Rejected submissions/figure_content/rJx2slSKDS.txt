Figure 1: Generated faces by StyleGAN with different priors. For (a) and (b), the first row shows thegenerated faces with the normal distribution and the second row displays the faces with the uniformdistribution. The priors are both normal when training.
Figure 2: Volume of spheres in different dimensional spaces. The volume of the sphere in thehigh-dimensional space is highly concentrated near the surface. The interior is nearly empty.
Figure 3: The illustration of average distance between two points randomly sampled on unit spheresof various dimensions. (a) The average distance (red curve) and standard deviation (gray background).
Figure 4: Reconstructed faces by VAE and SAE. SAE only uses the spherical constraint in equation(10) instead of the variational inference in VAE.
Figure 5: Generated faces with inputs of different priors. With the pre-trained decoders of VAE andSAE, the faces are generated with the random vectors sampled from the four probability priors.
Figure 6: Reconstructed faces by VAE and SAE. Our SAE algorithm only uses the spherical constraintin equation (10) instead of priors.
Figure 7: Generated faces with inputs of different priors. With the pre-trained decoders of VAE andSAE, the faces are generated with random vectors sampled from the four probability priors.
Figure 8: Generated faces with inputs of different priors. With the pre-trained decoders, the faces aregenerated with random vectors sampled from the four probability priors.
Figure 9: Reconstructed faces by VAE and SAE. Our SAE algorithm only uses the spherical constraintin (10) instead of priors.
Figure 10: Generated faces with inputs of different priors. With the pre-trained decoders, the facesare generated with random vectors sampled from the four probability priors.
Figure 11: Reconstructed letters by VAEs with different priors on latent spaces. Our SAE algorithmonly uses the spherical constraint in equation (10) instead of priors. For all experiments on MNIST,we take dz = 10.
Figure 12: Generated letters with inputs of different priors. With the pre-trained decoders, the lettersare generated with random vectors sampled from the four probability priors.
Figure 13: Visualization of inferred codes N on CelebA with t-SNE. We randomly sample 5,000faces from CelebA for illustration. The distribution of the latent codes from the variational inference(VAE) shows a standard normal one. However, the distribution of the latent codes from the sphericalinference (SAE) is prone to be globally uniform while maintaining the variation of density.
Figure 14: Visualization of inferred codes Z on MNIST with t-SNE. We randomly sample 500 lettersfrom each class in MNIST to form the whole set for illustration. The latent codes derived from SAEis much better than that from VAE and S-VAE. The margins between different classes are wider,meaning that the latent codes from the spherical inference conveys more discriminative informationin the way of unsupervised learning. This experiment also indicates that SAE captures the intrinsicstructure of multi-class data better than VAE and S-VAE.
