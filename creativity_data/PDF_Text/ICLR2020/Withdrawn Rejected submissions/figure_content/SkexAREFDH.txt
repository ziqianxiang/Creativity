Figure 1: Illustration of the proposed DNN compression framework. DNN weight W is sparse and Vis quantized. V is a “soft duplicate” of W and they are converged to be equal.
Figure 2: Visualization of the compressed results of different layers on LeNet-5 and AlexNet. Thenumber of nonzero weights is shown in log10 scale.
