Figure 1:	The 1 D-toy environmentDespite its simplicity, DDPG can fail on 1 D-TOY. We first show that DDPG fails to reach 100%success. We then show that if learning a policy does not succeed soon enough, the learning processcan get stuck. Besides, we show that the initial actor can be significantly modified in the initialstages before finding the first reward. We explain how the combination of these phenomena canresult into a deadlock situation. We generalize this explanation to any deterministic and sparse rewardenvironment by revealing and formally studying a undesirable cyclic process which arises in suchcases. Finally, we explore the consequences of getting into this cyclic process.
Figure 2:	Success rate of variants of ddpg on 1D-toy over learning steps, averaged over 10k seeds.
Figure 3:	(a) Number of rewards found in mini-batches during training. After a rollout of n steps, theactor and critic are both trained on n minibatches of size 100. The red dotted line indicates an averageof 6.03 rewarded transitions present in these n minibatches. (b) In red, normalized probability offinding the earliest reward at this step. In blue, for each earliest reward bin, fraction of these episodesthat fail to converge to a good actor after 100k steps. Note that when the reward is found after one ortwo episodes, the convergence to a successful actor is certain.
Figure 4:	Drift of max |Q| and max ∣π∣ in the DRIFT environment, for 10 different seeds. In theabsence of reward, the critic oscillates briefly before stabilizing. However, the actor very quicklyreaches a saturated state, at either ∀s, π(s) = 0.1 or -0.1.
Figure 5:	Visualization of the critic in a failing run, in which the actor is stuck to ∀s, π(s) = 0.1.
Figure 6: Deadlock observed in 1 D -toy, represented as the cycle of red arrows.
Figure 7: A cyclic view of the undesirable convergence process in continuous action actor-criticalgorithms, in the deterministic and sparse reward case.
Figure 8: (a) Example of a monotonous function approximator. (b) Simply changing the vertical scaleof the graphs presented in Figure 5b reveals that the function approximator is not perfectly flat, andhas many unwanted local extrema. Specifically, continuously moving from π(0) = 0.1 to π(0) < 0requires crossing a significant valley in Q(0, a), while π(0) = 0.1 is a strong local maximum.
Figure 9: (a) Applying ddpg-argmax to 1 D-toy. (b) Applying sac to 1 D-toy. In both cases,the success rate reaches 100% quickly. (c) Applying ddpg and ddpg-argmax to a sparse-rewardvariant of the Reacher-v2 environment. (d) Applying ddpg and ddpg-argmax to a sparse-rewardvariant of the HalfCheetah-v2 environment. Details on the changes made to Reacher-v2 andHalfCheetah-v2 are available in Appendix F.2.
Figure 10: Example of anon-continuous function f with values in {(2)n | n ∈ N}, approximatingthe identity function. However, this function is not differentiable because the difference quotient doesnot converge but instead oscillates between two values.
Figure 11: Performance of ddpg and ddpg-argmax on a sparse variant of HalfCheetah-v2. Toensure exploration of the state space is not a problem, the policy is replaced with a good pre-trainedpolicy in one episode over 20.
