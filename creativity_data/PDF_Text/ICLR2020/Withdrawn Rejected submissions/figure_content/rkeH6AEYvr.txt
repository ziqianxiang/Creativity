Figure 1: (a), (b) The receptive field problem of fully convolutional processing. A simple CNNconsisted of 2 convolutional layers (colored green), followed by a global pooling layer (colored red),processes an image in two different resolutions. The shaded regions indicate the receptive fields ofneurons from different layers. As the resolution of the input increases, the final latent representationbecomes a bag of increasingly more local features, lacking coarse information. (c) A sketch ofour proposed architecture. The arrows on the left side of the image demonstrate how we focus onimage sub-regions in our top-down traversal, while the arrows on the right show how we combine theextracted features in a bottom-up fashion.
Figure 2: Three unrolled processing levels of our architecture. The network starts by processingthe coarsest scale (Feature Extraction Module), and uses the extracted features to decide where tofocus (Location Module). Features extracted from subsequent detail levels are combined together(Aggregation Module), and then are used to enrich the features from the coarser scales (MergingModule) before the final classification (Classification Module).
Figure 3: Example images from our MNIST-based datasets, along with the attended locations ofM328 models. We blur the area outside the attended locations, because it is processed only in lowerresolution during the first processing level. This way we aim to get a better understanding of whatour models “see”.
Figure 4: Experimental results on plain, textured and noisy MNIST. The differences in accuracybetween many models were very small, and as a result, in the provided graphs we report the averageof 20 different evaluations on the validation set, where each time we randomly change the positionsof the digits inside the images. For textured and noisy MNIST, we randomly change the backgroundand the noise pattern of each image as well.
Figure 5: Two missclasication examples of model M328,2.2, that demonstrate the interpretability ofour models’ decisions because of the employed attention mechanism.
Figure 6: Experimental results on ImageNet. In the y-axis we provide the top-1 accuracy on thevalidation set, while in the x-axis we provide the required number of FLOPs (×106) per image.
Figure 7: Examples of attended locations from a M1128 model. Image parts are blurred according tothe resolution they are processed.
Figure 8: The parsing tree that is implicitly created by our model according to the example of Figure1 (c). Nodes correspond to the attended image locations, and edges relate each location with itsconstituent parts.
