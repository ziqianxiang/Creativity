Figure 1: Comparison between theory and experiments for prior distributions of outputs for a singleinput. The agreement between our theoretical predictions (smooth thick lines) and experimental data(rugged thin lines) is superb, correctly capturing the initial deviations from Gaussian processes atn = ∞ (black), all the way down to n 〜10 for linear activation and to n 〜30 for ReLU activation.
Figure S1: Comparison between theory and experiments for prior distributions of outputs for asingle input. Our theoretical predictions (smooth thick lines) and experimental data (rugged thinlines) agree, correctly capturing the initial deviations from the Gaussian processes (black, n = ∞),at least down to n = n? with n?〜10 for linear cases, n?〜30 for ReLU cases and depth L = 2quadratic case, and n?〜100 for depth L = 3 quadratic case. This also illustrates that nonlinearactivations quickly amplify non-Gaussianity.
Figure S2: Test accuracy for NE = 10000 MNIST test data as a function of the inverse width= 1/nL-1 of the hidden layer with quadratic activation. For each number NR of subsampledtraining data, the result is averaged over 10 distinct choices of such subsamplings. For small numbersof training data, finite widths result in regularization effects, improving the test accuracy.
