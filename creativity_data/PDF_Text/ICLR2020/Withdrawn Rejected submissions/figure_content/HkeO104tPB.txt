Figure 1: An illustration of the rope push-ing task. The Sawyer robot is given an im-age of the current configuration of the ropeand an image of the goal configuration (illus-trated as the translucent rope) and the task isto push the rope to the goal configuration.
Figure 3: The final distance to goal of different methods in different environments throughout the training. Theobservations are from the RGB-D images rendered in simulation.
Figure 4: An example observation image (top left) andgoal image (top right); final distance to goal (bottom).
Figure 5: The success (top) and the final distance to goal (bottom) of different ablations of our method.
Figure 6: The false negative rate and the reward accuracy calculated from a batch sampled from the replaybuffer of different ablations of our method.
Figure 7: The success (upper row) and the final distance to goal (lower row) of different methods in differentenvironments throughout the training. The success is defined as the mean probabily of getting the R+ rewardover all time steps. The final distance to goal is defined as the L2 distance to the goal in the state space, in thelast time step of the episode. The input to the policy is the low dimension state representation.
Figure 8: Learning with indicator rewards on the rope push environment using different filtering threshold. Therange given in Section 5.3 is [1, 3]. Here we show 6 values of q0 evenly spaced within this range.
