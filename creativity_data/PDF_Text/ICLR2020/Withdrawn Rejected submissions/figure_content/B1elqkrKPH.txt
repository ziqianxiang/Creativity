Figure 1: The top layer representations of the six images on the right learned by a prototypical CNNare equally (dis)similar to the reference image (left), even though the images at the bottom row aretransformations of it.
Figure 2: All-CNN: distribution of the invariance score at each layer of the baseline model and themodel trained data augmentation invariance (higher is better).
Figure 3: WRN: distribution of invariance score at each layer of the baseline model and the modeltrained data augmentation invariance (higher is better).
Figure 4: DenseNet: distribution of invariance score at each layer of the baseline model and themodel trained data augmentation invariance (higher is better).
Figure 5: Dynamics of the data augmentation invariance loss Li(nl)v during training (lower is better).
