Figure 1: In (a), we have the Gcdf loss for different values of sigma. In (b), the Cross-entropy lossfor different temperatures and in (c), the Hinge loss for different margins.
Figure 2: Tracking the the cross-entropy loss with different temperatures while training with thestandard cross-entropy loss (T = 1) on CIFAR10. A relatively large learning rate of 0.1 is usedin (a) while a much smaller learning rate of 0.001 is used in (b). Even though both learning ratessucceed at minimizing to almost zero the loss function they are trained on, the smaller learning ratedoes not implicitly minimize as well the cross-entropy loss for larger temperatures.
Figure 3: Tracking the Hinge loss with different margin parameters while training with the standardcross-entropy loss on CIFAR10. A relatively large learning rate of 0.1 is used in (a) while a muchsmaller learning rate of 0.001 is used in (b). Even though both learning rates succeed at minimizingto almost zero the loss function they are trained on, the smaller learning rate does not implicitlyminimize as well the Hinge loss for larger margins.
Figure 4: Tracking the Gcdf loss with different standard deviation parameters while training withthe standard cross-entropy loss on CIFAR10. A relatively large learning rate of 0.1 is used in (a)while a much smaller learning rate of 0.001 is used in (b). Even though both learning rates succeedat minimizing to almost zero the loss function they are trained on, the smaller learning rate does notimplicitly minimize as well the Gcdf loss for larger standard deviations.
Figure 5: Tracking the Gcdf loss with different standard deviation parameters while training with thestandard cross-entropy loss on CIFAR10. A relatively small batch size of 256 is used in (a) while amuch larger batch size of 16384 is used in (b). Even though both batch sizes succeed at minimizingto almost zero the loss function they are trained on, the larger batch size does not implicitly minimizeas well the Gcdf loss for larger values of σ .
Figure 6: For each value of the training loss (cross-entropy) achieved during training on CIFAR10,we plot the Gcdf loss at that time on the y axis. In (a), we use a smaller value of σ = 0.5, in (b) anintermediate value of σ = 1 and in (c) a larger value of σ = 8. The train error is plotted against thetrain loss in (d). Four training runs corresponding to four different learning rates are drawn in eachcase.
Figure 7: At a given fixed cross-entropy training loss (here approximately 0.6 in all cases), the Gcdfloss for varying σ,s in (a), the cross-entropy loss with varying temperatures in (b) and the Hinge losswith varying Y's in (c) are plotted. Larger learning rates obtain better values of the alternative lossesfor larger σ's, temperatures and Y's while smaller learning rates are generally better for smallervalues of these parameters.
Figure 8: Tracking the Gcdf loss with different standard deviation parameters while training withthe standard cross-entropy loss on MNIST. A relatively large learning rate of 0.1 is used in (a) whilea much smaller learning rate of 0.001 is used in (b).
Figure 9: Tracking the Gcdf loss with different standard deviation parameters while training withthe standard cross-entropy loss on MNIST. A relatively small batch size of 256 is used in (a) whilea much larger batch size of 16384 is used in (b).
Figure 10: Tracking the Hinge loss with different margin parameters while training with the standardcross-entropy loss on MNIST. A relatively large learning rate of 0.1 is used in (a) while a muchsmaller learning rate of 0.001 is used in (b).
Figure 11: Tracking the the cross-entropy loss with different temperatures while training with thestandard cross-entropy loss (T = 1) on MNIST. A relatively large learning rate of 0.1 is used in (a)while a much smaller learning rate of 0.001 is used in (b).
Figure 12: At a given fixed cross-entropy training loss (here approximately 0.08 in all cases), theGCdf loss for varying σ,s in (a), the cross-entropy loss with varying temperatures in (b) and theHinge loss with varying Y's in (c) are plotted. Larger learning rates obtain better values of thealternative losses for larger σ's, temperatures and Y's while smaller learning rates are generallybetter for smaller values of these parameters. The training dataset is MNIST.
Figure 13: Tracking the Hinge loss with different margin parameters while training with the standardcross-entropy loss on CIFAR10. A relatively batch size of 256 is used in (a) while a much largerbatch size of 16384 is used in (b). Even though both batch sizes succeed at minimizing to almostzero the loss function they are trained on, the larger batch size does not implicitly minimize as wellthe Hinge loss for larger margins.
Figure 14: Tracking the cross-entropy loss with different temperature parameters while trainingwith the standard cross-entropy loss (T = 1) on CIFAR10. A relatively batch size of 256 is usedin (a) while a much larger batch size of 16384 is used in (b). Even though both batch sizes succeedat minimizing to almost zero the loss function they are trained on, the larger batch size does notimplicitly minimize as well the cross-entropy loss for larger temperatures.
Figure 15: For each value of the training loss (cross-entropy) achieved during training on CIFAR10,we plot the Gcdf loss at that time on the y axis. In (a), we use a smaller value of σ = 0.5, in (b) anintermediate value of σ = 1 and in (c) a larger value of σ = 8. Three training runs corresponding tothree different batch sizes are drawn in each case. Larger batch sizes are slightly better at minimizingthe Gcdf loss with small σ during training while smaller batch sizes are better at minimizing theGcdf loss with larger σ. There exists an intermediate value (here σ = 1) where all the batch sizesconsidered in our experiments are essentially equivalent at implicitly minimizing the Gcdf loss.
Figure 16: At a given fixed cross-entropy training loss (here approximately 0.6 in all cases), the Gcdfloss for varying σ’s in (a), the cross-entropy loss with varying temperatures in (b) and the Hinge losswith varying Y 's in (c) are plotted. The small batch size obtain better values of the alternative lossesfor larger σ's, temperatures and Y's while the large batch size is better for smaller values of theseparameters. The training dataset is CIFAR10.
