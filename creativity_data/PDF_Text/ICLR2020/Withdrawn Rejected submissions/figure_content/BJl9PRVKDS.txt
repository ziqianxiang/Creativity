Figure 1: Left: Training loss Vs number of pieces (α number of parameters) for various algorithmsfitting a CPWL function to a quadratic. Middle: Breakpoint distribution for a He initialization acrossa 3 layer network. Right: Delta-slope distribution for a He initialization across a 3 layer network.
Figure 3: Roughness (summed by layer) during training for a 5 layer ReLU network with 8 units perhidden layer, learning the quadratic function x2/2 (left) and the periodic function sin(x) (right)spective on how depth and parameterization affect the training of neural networks. One of the mostuseful and perplexing properties of deep neural networks has been that, in contrast to other highcapacity function approximators, overparameterizing a neural network does not tend to lead to ex-cessive overfitting (Savarese et al., 2019). Where does this generalization power come from? Muchrecent work (Neyshabur et al., 2018; 2015) has argued that it comes from an implicit regularizationinherent in the optimization algorithm itself (i.e. SGD). In contrast, for the case of shallow anddeep univariate fully connected ReLU nets, we provide causal evidence that it is due to the specific,very flat CPWL initialization induced by common initialization methods. In order to test this inboth shallow and deep ReLU networks, we compare training with the standard flat initialization to a‘spiky’ initialization.
Figure 4: ’Spiky’ (orange) and standard initialization (blue), compared before (left) and after (right)training. Note both cases had similar, very low training set error.
Figure 5: Roughness vs. Width (left) and the variance of the initialization (right) for both datagap cases shown in Figure 8. Each data point is the result of averaging over 4 trials trained toconvergence.
Figure 6: ’Spiky’ (orange) and standard initialization (blue), compared before training (left) andpost-training (right) using a deep networkFigure 7: Growth in the (minimum) amount of local minima, as a function of the number of break-points and data points. Right plot is identical, but with log scalingA.1 Uniform InitializationTrained on a shallow, 21 unit FC ReLU network. Trained on function over the interval [-2,2].
Figure 7: Growth in the (minimum) amount of local minima, as a function of the number of break-points and data points. Right plot is identical, but with log scalingA.1 Uniform InitializationTrained on a shallow, 21 unit FC ReLU network. Trained on function over the interval [-2,2].
Figure 8: Training data sampled from two ground truth functions, one smoothly (left) and the othersharply (right) discontinuous, each with a data gap at [-0.5, 0.5].
Figure 9: Shallow (left) and deep (right) plots of initial and final breakpoint distributions, alongwith the underlying true curvature of the functions (top to bottom) sin(x), x3, sin(X), and a cubicspline of a few arbitrary data pointsThis gives us Equation (2), as desired. Let the subscripts p, q denote the parameters sorted by βpvalue. In this setting, let β0 , -∞, and βH+1 , ∞. Then,16Under review as a conference paper at ICLR 2020/HXp=0HXp=0HXp=0HXp=0
