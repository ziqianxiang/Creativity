Figure 1: Comparison of approaches with ResNet50 on ImageNet dataset under different quantiza-tion levels. Left: Quantization on both weight and activation. Right: Weight-only quantization. Thebit-width represents equivalent computation cost for mixed-precision methods (AutoQB and HAQ).
Figure 2: Effect of weight clamping. (a) The ratio of variances with respect to the number ofneurons. Note that the plot is only a sampling result and different samples can give different results,but the order of magnitude remains meaningful. (b) Learning curves with different settings.
Figure 3: Effect of quantization error in calculting gradient for PACT. Left: 2bit MobileNet-V2 onImageNet. Right: 4bit MobileNet-V2 on ImageNet.
Figure 4: Plot of the sigmoid function (a) and its derivative (b).
Figure 5: Empirical verification of Eq. (18) and Eq. (28). (a) κ0 calculated with Eq. (30a). (b) κ1given by Eq. (30b) for different layers.
Figure 6: Impact of weight quantization on the variance of effective weight under different channelnumbers.
Figure 7: Comparison of constant rescaling and rescaling with standard deviation.
Figure 8: Learning curves for PreResNet-50 with 2bit weight-only quantization on ImageNet. Forred lines, rescaling is only applied to the last fully-connected layer, while for blue lines, all layersare rescaled.
