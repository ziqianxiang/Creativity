Figure 1: We perform a double finetune from BERT to an intermediate task to our medical question-similarity task for four different intermediate tasks: quora question-question pairs (top left), medicalquestion-answer pairs (top right), medical answer-answer pairs (bottom left), and medical question-category pairs (bottom right)For each intermediate task, we train the network for 5 epochs (Liu et al., 2019) with 364 thousandtraining examples to ensure that differences in performance are not due to different dataset sizes.
Figure 2: The intermediate task of training on question-answer pairs (BERT+QA) reliably outper-forms other intermediate tasks: Quora question pairs (BERT+QQP), medical answer completion(BERT+AA), and medical question categorization (BERT+QC). Differences are exacerbated withfewer training examples. Error bars represent one standard deviation across different data splits.
