Figure 1: Examples of object recognition datasets. Caltech-101 (Fei-Fei et al., 2004) is more similarto ImageNet with 101 classes of common objects. CUB-200 (Welinder et al., 2010) and Stanford Cars(Krause et al., 2013) are essentially different with various kinds of birds and vehicles, respectively.
Figure 2: The deviation of the weight parameters from the pretrained ones in the transfer process todifferent target datasets. For all datasets, √1n IlW - Wo∣∣f increases monotonously. More knowledgecan be preserved on target datasets more similar to ImageNet, yielding smaller √n ∣∣W - Wo∣∣f.
Figure 3: Generalization w.r.t. landscape at convergence on Stanford Cars dataset with ResNet-50.
Figure 4: The t-SNE (van der Maaten & Hinton, 2008) visualization of the weight matrices of thepretrained and randomly initialized networks on different datasets. Surprisingly, weight matrices onthe same dataset may be distant at convergence when using different initializations. On the contrary,even for discrepant datasets, the weight matrices stay close to the initialization when using the samepretrained parameters.
Figure 5: Optimization landscape at initialization. (a) A figure of convergence on fine-grainedclassification task Food-101. The convergence of fine-tuning from pretrained networks is significantlyfaster than training from scratch. (b) (c) Visualizations of loss landscapes of the 25th layer inResNet-50 centered at initialization on Stanford Cars dataset. ImageNet pretrained landscape is muchsmoother, indicating better Lipschitzness and predictability of the loss function. Randomly initializedlandscape is more bumpy, making optimization unstable and inefficient.
Figure 6: Variation of the loss in ResNet-50 with ImageNet pretrained weight and random initial-ization. We compare the variation of loss function in the direction of gradient during the trainingprocess on CUB-200 dataset. The variation of pretrained networks is substantially smaller than therandomly initialized one, implying a more desirable loss landscape and more stable optimization.
Figure 7: The stabilization of gradient by pretrained weights on CUB-200. (a) (b) Distribution of themagnitude of gradient in the 25th layer of ResNet-50. (c) Magnitude of the projection of weight onthe singular vectors of gradient. (d) Scaling of the gradient in different layers in back-propagation.
Figure 8: Performance by varying either the inputs orlabels. (a) Accuracy and deviation of transferringfrom different datasets to MNIST, where ψι and ψ2 denote the convolutional kernels in the first andthe second layers, and W denotes the weight of the fully-connected layer. (b) Accuracy on Webcamwith Caltech and Caltech-color pretraining. (c) (d) Test accuracy and convergence of transferring toMIT-indoors and CUB-200 from models pretrained on large-scale ImageNet and Places, respectively.
Figure 9: Transfer performance w.r.t. the pretraining epochs from Food-101 to CUB-200. (a)Accuracy of pretraining task and (b) Accuracy on target dataset by fine-tuning from models pretrainedfor different numbers of epochs. Although accuracy of pretraining increases stably, transferabilityincreases only during the early epochs and decreases afterwards.
Figure 10: Comparison of landscapes centered at the minima on Food-101 datasets with ResNet-50.
Figure 11: Variation of the loss in ResNet-50 with ImageNet pretrained weight and random initial-ization. We compare the variation of loss function in the direction of gradient during the trainingprocess on Stanford Cars dataset. The variation of pretrained networks is substantially smaller thanthe randomly initialized one, implying a more desirable loss landscape and more stable optimization.
Figure 12: Landscapes centered at the initialization point of each layer in ResNet-50 using ImageNetpretrained weight. The smoothness of landscapes in each layer are nearly identical, indicating aproper scaling of gradient.
Figure 13: Landscapes centered at the initialization point of each layer in ResNet-50 initializedrandomly. At the higher layers, the landscapes tend to be smooth. However, as the gradient ispropagated to lower layers, the landscapes are becoming full of ridges and trenches in spite of thepresence of Batch-Norm and skip connections.
