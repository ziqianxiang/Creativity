Figure 1: Standard CNN architectures (a) contain several convolutional layers, all of which are individuallyadapted using backpropagation. By contrast, We propose the use of a single learned convolutional layer (b) thatis applied repeatedly to simulate a CNN pipeline.
Figure 2: Accuracy versus compression trade-off curves for our basic shared architecture in Fig. 1, for differentwidths n of the shared convolutional layer, compared to the baseline VGGNet (Simonyan & Zisserman, 2015),for CIFAR-10 (a) and CIFAR-100 (b). The compression factor is plotted on a logarithmic scale.
Figure 3: Accuracy versus compression trade-off curves of the ‘S’ and ‘SL’ variants of VGGNet for differentwidths n of the shared convolutional layer, relative to the baseline VGGNet (Simonyan & Zisserman, 2015),for CIFAR-10 (a), CIFAR-100 (b), and Tiny ImageNet (c). The compression factor C is plotted on a log scale.
Figure 4: A visual depiction of the linear layers used to blend the input channels in our approach. We show thelayers for the two variants in the order (left to right) in which they appear in the networks. For each layer, theinput channels are ordered along the x-axis, and the output channels along the y-axis. For each output channel(row), we highlight the lowest 32 weights (in terms of absolute value) in blue, and the highest 32 in red.
Figure 5: Analysing the effects of pruning on one of our largest models, SL-ResNet-50 (n = 512), trainedon Tiny ImageNet. We iteratively zero out an increasing fraction of the linear layer parameters, starting fromthose having the smallest absolute value. The accuracy of the network stays constant even when 60% of theparameters are pruned, at which point the compression rate (in comparison to the non-shared baseline withequivalent performance) has increased from 1.4 to 2.6.
Figure 6: The building blocks used in our ResNet-like architectures. (a) and (b) show the non-sharedand shared basic blocks. Our shared variant of the residual basic block reuses the same convolutionallayer within and across the different blocks. (c) and (d) show the bottleneck blocks. In this case,since the three convolutions have different sizes, We cannot share a single set of parameters acrossthe whole network; instead, We consider the block as a single entity and reuse it across the network.
Figure 7: A Visual depiction of the linear layers used to blend the input channels in the ‘SL’ Variantsof VGGNet trained on CIFAR-10, CIFAR-100 and Tiny ImageNet. The linear layers are presentedin the order (left to right) in which they appear in the networks. For each layer, the input channels areordered along the x-axis, and the output channels along the y-axis. For each output channel (row),we highlight the lowest 32 weights (in terms of absolute Value) in blue, and the highest 32 in red.
Figure 8: A visual depiction of the linear layers used to blend the input channels in four ‘SL’ variantsof ResNet, in the order (left to right) in which they appear in the networks. For each layer, the inputchannels are ordered along the x-axis, and the output channels along the y-axis. For each outputchannel (row), we highlight the lowest 32 weights (in terms of absolute value) in blue, and thehighest 32 in red.
