Figure 1: Synthesizing the motion of sitting. Left: Input image and 3D chair detection. Middle: Physics Sim-ulated environment for learning human-chair interactions. Right: Two examPles of the synthesized motions.
Figure 2: Left: Overview of the hierarchical system. Right: Illustration of the subtasks.
Figure 3: State representation of the humanoid andchair. The red and green dots on the humanoid de-note the root and non-root joints. The red dots onthe ground and chair denote the walk target and thecenter of the seat surface.
Figure 4: Curriculum learning for the meta con-troller. The humanoid spawn location is initially setto less challenging states (Zone 1), and later movedto more challenging states (Zone 2 and 3).
Figure 5: Execution of subtasks. From top to bottom:forward walking, target directed walking, left turn, rightturn, and sit.
Figure 6: Qualitative results of our approach and the baselines. Row 1 and 2 show failure cases from thekinematics and physics baselines, respectively. The former violates physics rules (e.g. sitting in air), and bothdo not generalize to new human-chair configurations. Row 3 to 4 show two successful cases and row 5 showsone failure cases from our approach.
Figure 7: Transition matrices of starting from differ-ent sides of the chair.
Figure 8: Qualitative results on the Hard setting. The humanoid can sit down successfully when starting fromthe back side of the chair.
Figure 9: Synthesizing sitting motions from a single image. The first column shows the 3D reconstructionoutput from Huang et al. (2018).
