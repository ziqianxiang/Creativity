Figure 1: Three candidate architectures considered for domain-invariant representation learning. In(a), a set of common network parameters are trained to model both source and target domains. In(b), the domain characteristics are explicitly modeled by two sets of convolutional filters in eachconvolutional layer. Our approach is illustrated in (c) where domain-adaptive bases are learned to“absorb” domain shifts, while the decomposition coefficients are shared across domains to promotecommon semantics. The feature space of the three candidate architectures, MNIST → SVHN, arevisualized using t-SNE Maaten & Hinton (2008) in (d), (e), (f), respectively. The obtained cross-domain invariance can be clearly observed in (f).
Figure 2: Illustrations of the proposed domain-adaptive filter decomposition.
