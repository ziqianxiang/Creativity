Figure 1: Given a strongly aliased low-resolution input rendering with one sample per pixel, re-current non-adversarial training ((Chaitanya et al., 2017) with modifications for fair comparisons)produces blurry results, and existing adversarial methods ((Chu et al., 2018), re-trained) introducestrong flickering artifacts. Trained on the same data, due to the proposed DRR connections ournetwork infers more consistent spatio-temporal features (see the supplemental footage for a clearassessment of the temporal differences).
Figure 2: Our DRR model applied to 2 different test scenes. The comparisons show input LR colorimages and the high-resolution outputs inferred by our method. The inputs exhibit strong spatialaliasing, and a similar amount of temporal aliasing, as is visible in the videos of the supplementalmaterial. Despite these challenges, our model infers a stable and detailed output sequence.
Figure 3: We propose to connect the center latent-space of ResBlock n at time t - 1 to the sameResBlock n at time t, yielding our DRR connec-tion. A variant of our approach is DRR-in, in addi-tion to a recurrent connections similar to previouswork (RDA).
Figure 4: A visual comparison of the different types of recurrent connections with otherwise identi-cal models (all three using our discriminator supervision, generator and warping). Our DRRs yieldan improved image quality and temporal stability, as shown in the supplemental material.
Figure 5: Ablation study for the discriminator network: The top row shows different discriminatorsetups: without adaptive balancing, with conditional LR color input, the standard size and our largervariant. The bottom row shows the influence of different auxiliary losses.
Figure 6: Models trained with natural video all enhance aliasing as the larger changes betweenadjacent pixels are detected as edges. Also, re-training a model such as TecoGAN with our data setyields undesirable results.
Figure 7: Top row: Examples of input (LR) and target (HR) data used for training. Natural VSRuses a smooth input, illustrated with down-sampling on the right. The bottom row shows additionaldata captured from the renderer: diffuse color, depth, roughness, surface normals and motion.
Figure 8: Our modified generator: in addition to the LR color and frame-recurrent input (gener-ator output of the previous frame encoded in 3 * 16 channel) We also add the LR depth. Afteran initial convolution most work is done in the 10 sequential ResBIocks (same as TecoGAN∙, 9omitted for visibility). The latent image is then scaled to output resolution by 2 resize convolutionsand fine-tuned by another 3 convolutions after the bilinear interpolated LR color is appended. Allconvolutions have 3 × 3 kernel size and ReLU or no activation.
Figure 9: The extended discriminator: 3-frame sequences of HR color are provided as input fromthe left. The down-scaling is done by strided convolutions with a stride of2. All convolutions have4 × 4 kernel size and lReLU activation with leak 0.2. The dense layer for the final score has σactivation.
Figure 10: Several example sequences from our test data set (each top row shows the input, thebottom row the result inferred by our model).
Figure 11: Additional example sequences from our test data set (each top row shows the input, thebottom row the result inferred by our model).
