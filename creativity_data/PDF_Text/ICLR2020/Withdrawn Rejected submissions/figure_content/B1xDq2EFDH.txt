Figure 1: Overview of the proposed graph for training Gaussian robust networks. The yellow blockcorresponds to an arbitrary network Φ(., θ) viewed as the composition of two subnetworks separated by aReLU. The stream on the bottom computes the output mean μ4 of the network Φ(:, θ) assuming that (i) thenoise input distribution is independent GaUSSian with variances σ2, and (ii) Ω(. : θ2) is approximated bya linear function. This evaluation for the output mean is efficient as it only requires an extra forward pass(bottom stream), as opposed to other methods that employ computationally and memory intensive networklinearizations or data augmentation.
Figure 2:	General trade-off between accuracy and robustness on LeNet. We see, in all plots, that theaccuracy tends to be negatively correlated with robustness over varying noise levels and amount of augmen-tation. Baseline refers to training with neither data augmentation nor our regularizer. However, it is hard tocompare the performance of our method against data augmentation from these plots as we can only comparethe robustness of models with similar noise-free testing accuracy.
Figure 3:	Fair robustness comparison of LeNet with data augmentation and our regularizer. We onlyreport results for models with a test accuracy that is at least as good as the accuracy of the baseline with atolerance: 0%, 0.39%, and 0.75% for MNIST, CIFAR10, CIFAR100, respectively. Only the models with thehighest robustness are presented. Training with our regularizer can attain similar/better robustness than 21-foldnoisy data augmentation on MNIST and CIFAR100, while maintaining a high noise-free test accuracy.
Figure 4: This Figure shows an example of the noise level over varying level of input σ on the digit8. In particular, one can observe that with σ large than 0.7 the among of noise is severe even for thehuman level. Training on such extreme noise levels will deem data augmentation to be difficult.
Figure 5: The robustness is a function of the ratio of the orange area to the blue area in the whitecircle.
Figure 6: The robustness is thus measured as the area under the curve of testing accuracy versusinput noise level (standard deviation).
Figure 7: Fair robustness comparison of AlexNet with data augmentation and our regularizer. Thereported models trained with our regularizer on CIFAR10 and CIFAR100 on all training σx are within 1.68%and 4.83% of the baseline accuracy, respectively. The models trained with the proposed regularizer achievebetter robustness than 11-fold and 6-fold noisy data augmentation on CIFAR10 and CIFAR100, respectively.
