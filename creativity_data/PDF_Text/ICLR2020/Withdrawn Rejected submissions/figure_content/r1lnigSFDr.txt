Figure 1: LSTM with refine gate The refine gate rt modifies another gate, such as the forget gate ftfor recurrent models. It interpolates between upperbound Uft and lowerbound Lft functions of theforget gate. The resulting effective forget gate gt is then used in place of ft in the state update (5).
Figure 2: Refine gate in action: (a) [Solid] A function α(ft) satisfying natural properties is chosen todefine a band within which the forget gate is refined. (b) The forget gate ft(x) is conventionally definedwith the sigmoid function (black). The refine gate interpolates around the original gate ft to yield aneffective gate gt within the upper and lower curves, gt ∈ ft±α(ft). (c) Contours of the effective gate gtas a function of the forget and refine gates ft, rt. High effective activations can be achieved with moremodest ft,r values. (d) The gradient Vgt as a function of effective gate activation gt. [Black, blue]:Lower and upper bounds on the ratio of the gradient when using a refine gate vs. without.
Figure 3: (Left) Copy task length 500 (Right) Adding task length 2000. Every method besides theLSTM solves the Adding task. The only methods capable of solving copy are OR-,UR-,O-, andC-LSTM models, with all other models aside from U-LSTM stuck at baseline. Refine gates are fastest.
Figure 4: Histograms of forget gate ft activations (averaged over time and batch) before (Top) and after(Bottom) training on Copy (y-axis independently scaled). C-LSTM initializes with extremal activationswhich barely change during training. Standard LSTM initialization cannot learn large enough ft andmakes no progress on the task. U-LSTM makes progress by encouraging a range of forget gate values,but this distribution does not change significantly during training due to saturation. UR-LSTM startswith the same distribution, but is able to learn extremal gate values. Complementary to here whenlearning large activations is necessary, Appendix E.1 shows a reverse task where the UR-LSTM is ableto un-learn from a saturated regime.
Figure 5: Learning curves with deviations on pixel image classification, at the best stable learning rate.
Figure 7: We evaluated the image matching tasks from Hung et al. (2018), which test memorization andcredit assignment, using an A3C agent (Mnih et al., 2016) with an LSTM policy core. We observe thatgeneral trends from the synthetic tasks (Section (3.1)) transfer to this reinforcement learning setting.
Figure 8: The addition of distractor rewards changes the task and relative performance of differentgating mechanisms. For both LSTM and RMA recurrent cores, the UR- gates still perform best.
Figure 9: Distribution of forget gate activations after extremal initialization, and training on the Addingtask. The UR-LSTM is able to learn much faster in this saturated gate regime while the LSTM does notsolve the task. The smallest forget unit for the UR-LSTM after training has characteristic timescaleover an order of magnitude smaller than that of the LSTM.
Figure 10: The full Active Match task with large distractor rewards, using agents with LSTM or DNCrecurrent cores.
Figure 11: Program Execution evaluation accuracies.
