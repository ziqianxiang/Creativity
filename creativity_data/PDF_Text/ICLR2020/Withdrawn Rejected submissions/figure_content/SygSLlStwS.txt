Figure 2: Performance on standard meta-RL benchmarks for left: Half Cheetah velocity right: Humanoiddirection. The return is evaluated on (in-distribution) meta-test tasks at different steps during the meta-trainingprocess, following the protocol from Rakelly et al. (2019). Note that both our method and PEARL aresubstantially more efficient than the on-policy MAML algorithm, and our method performs comparably to theoff-policy PEARL algorithm.
Figure 3: Target velocity (reward function)extrapolation for HalfCheetah environment.
Figure 4: Dynamics extrapolation on HalfCheetah with negated joints. Left: Return after adaptation for eachmethod. Right: Ablation analysis on the effect of relabeling. The results show that MIER with cross taskrelabeling substantially outperforms prior methods when adapting to this task with out-of-distribution dynamics.
