Figure 1: Temporal localization un-der label misalignment. Models aretrained with noisy labels that differfrom the actual ground-truth, whilethe final inference objective is the pre-cise localization of events.
Figure 2: Inherent drawbacks of the classical trick of smoothing the labels only. Issue 2: ambiguouspredictions of event locations require the use of additional heuristics. Issue 3: close events cannotbe easily disentangled. Issue 4: the need to estimate the left-tail of label distributions compels themodel to detect events before their actual occurrence, which might lead to overfitting.
Figure 3: Modelling novelty. By smoothing both the labels and predictions, the model directlyinfers point predictions rather than distributions. Among other things, this modification allows forend-to-end learning of localization and alleviates the need for peak-picking. (left) Classical approachof smoothing the labels only. (right) Our novel end-to-end approach and its S oftLoc loss function.
Figure 4: The different issues arising from only smoothing the labels are solved by our approach.
Figure 5: F1 piano onset detection performance ofour approach (softness SM= 100ms) and the bench-mark models as a function of label noise levels.
Figure 6: Drum detection performance with respect to model softness and label noise. F1-scores areGaussian Nadaraya-Watson estimates based on 210 runs (white dots) sampled uniformly at random.
Figure 7: F1 piano onset detection perfor-mance of the SoftLoc model (SM = 100ms) asa function of label misalignment.
Figure 8: Out-of-sample predictions of our SoftLoc model trained on data subject to variouslevels of noise, ranging from (a) the noise-free case σ = 0ms to (d) the extremely noisyσ = 200ms. (Schubert-Piano Sonata inA minor, D 784, Opus 143, 3. Mov)13Under review as a conference paper at ICLR 2020A.2 Further IllustrationsTimeliness of SoftLoc predictions Figure 8 illustrates how consistently precise and well-centered(i.e., neither too late nor early) the predictions are regardless of the noise setting. Indeed, there isalmost no difference in prediction centering when comparing the results for σ = 0ms or σ = 200ms.
Figure 9: In-sample performance of the noisy training labels themselves (as predictions)When compared to the clean ground-truth. (Liszt - Hungarian Rhapsody No. 10)14Under review as a conference paper at ICLR 2020B Video Action S egmentationB.1	Impact of the Softness ParameterAs depicted in Figure 10, training with the S oftLoc loss function instead of the standard cross-entropyyields improved performance (up to 25%) in all noise settings almost regardless of the softnessSM . The only exception occurs when selecting a softness level that is too wide while training withnoise-free (δ = 0) labels. As also observed in Section 5.1.2, the model achieves optimal performancewhen the softness level SM is slightly larger than noise level δ. However, although the efficiency ofthe approach is bound to decrease when the disparity between selected softness and noise level isbecoming too large, a performance close to the optimal one can be achieve with a wide range ofsoftnesses SM.
Figure 10: Video Action Segmentation. Relative performance of the ED-TCN model trained withLSoftLoc — relative to CE — with respect to the softness level SM for various noise levels δ.
