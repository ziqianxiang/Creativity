Figure 1: An example of a Hebbian update for theclass, c = 6 ∈ y1:B . Here, we are given the hid-den activations of the final hidden layer, h. Multi-ple hidden activations corresponding to class c = 6(represented by the pink boxes) are averaged intoone vector denoted by h ∈ R1×m. This Heb-bian update visualization reflects Lines 4-6 in Al-gorithm 1 and is repeated for each unique class inthe target vector y1:B .
Figure 2: (a) The average test accuracy on a sequence of 10 Permuted MNIST tasks Tn=i：io and (b)a sequence of 5 binary classification tasks from the original MNIST dataset Tn=15 The average testaccuracy over all learned tasks is provided in the legend. The addition of DHP in all cases improvesthe model,s ability to reduce forgetting. The error bars correspond to the SEM across 10 trials.
Figure 3: (left) Hebbian learning rate and decay value η, (middle) Frobenius Norm of the Hebbianmemory traces kHebbkF, (right) Frobenius Norm of the plasticity coefficients kαkF while trainingeach task T1:10.
Figure 4: A sensitivity analysis on the Hebb decay term η in Eq. 3. We show the average test accu-racy for different initial values of η after learning all tasks on the (left) Permuted MNIST, (center)Imbalanced Permuted MNIST and (right) Split MNIST problems. The shaded regions correspondto the standard error of mean (SEM) across 5 trials.
Figure 5: The average test accuracy on a sequence of on a sequence of 10 imbalanced PermutedMNIST tasks Tn=i：io. The average test accuracy over all learned tasks is provided in the legend.
Figure 6: The average test accuracy on a sequence of 5 diffferent vision datasets Tn=i：5. The averagetest accuracy over all learned tasks is provided in the legend. The error bars correspond to the SEMacross 10 trials.
Figure 7: The difference between (a) task-specific consolidation methods (e.g., EWC, SI and MAS)and (b) Differentiable Hebbian Consolidation in a simple two tasks continual learning scenario. Thebaseline standard weight connections are represented by light gray lines and the dual-weight plasticconnections are represented by red lines. After learning Task 1, the parts of the network which aredeemed important for Task 1 are indicated in orange. Task 2 has a different set of input patterns andutilizes the parts shown in green. The Differentiable Hebbian Consolidation framework consolidatesrepresentations learned by the network and the hidden activations are accumulated directly into thesoftmax output layer parameters when a class has been seen by the network. This allows the networkto retain these learned deep representations for a longer timescale without having to sacrifice as muchcapacity by dynamically learning to adjust the degree of plasticity in the weights.
Figure 8: This replicates the Split CIFAR-10/100 experiment of Zenke et al. (2017b). First, thenetwork was trained on the full CIFAR-10 dataset (Task Tn=1) and sequentially on 5 additionaltasks each corresponding to 10 consecutive classes from the CIFAR-100 dataset (Tasks Tn=2:6).
