Figure 1: Navigation environments. The red sphere indicates the goal an agent needs to reach, withthe starting point at the opposite end of the maze. The agent is trained on environment a). To testgeneralization, we use the environments with b) reversed starting and goal positions, c) mirroredmaze with reversed starting and goal positions and d) randomly generated mazes.
Figure 2: Our 3-layer HRL architecture. The planning layer ∏ receives a birds eye view of theenvironment and the agent's position Sxy and sets an intermediate target position g2. The interfacelayer ∏2 splits this subgoal into reachable targets gι. A goal-conditioned control policy ∏o learnsthe required motor skills to reach the target gι given the agent,s joint information Sjoints.
Figure 3: Planner layer π2(sxy, G, I). Given the top-view environment image I and goal G on themap, the maximum value propagation network (MVProP) calculates a value map V. By using theagent's current position Sxy, We estimate an attention mask M restricting the global value map V toa local and reachable subgoal map V. The policy π2 selects the coordinates with maximum valueand assigns the lower policy π1 with a sugboal that is relative to the agent,s current position.
Figure 4: A visual comparison of(left) our dynamic attention windowwith a (right) fixed neighborhood.
Figure 5: Success rates in forward maze wrt thenumber of environment steps. All algorithmseventually learn the task. HIRO converges thefastest because it benefits from dense rewards.
Figure 6: Environments used for testing in Section 5.1. The red sphere indicates the goal of the task.
