Figure 1: Graphical depiction of various models: vanilla BNNs, BNNs with hierarchical GP-MetaPriors, and BNNs with hierarchical GP-MetaPriors and auxiliary variables.
Figure 2: Marginal and conditional covariance structuresover weights in a 1x50x1 neural network. Sampling from theposterior of the hierarchical model reveals that even a diago-nal GP approximation can capture off-diagonal correlationsinduced through unit correlations. Also note the off-diagonalbands in the marginal plots above, which indicate the corre-lation structures induced by the latent variables of the hiddenunits connecting the layers.
Figure 3: Illustration of the performance of Bayesian neural networks with a standard Normal priorover the weights and a mean-field Gaussian variational approximation [MFVI], a meta-NN prior,a meta-GP prior and a meta-GP prior with auxiliary inputs. Top: 1-D regression on 100 noisyobservations. The plots show the predictive mean and uncertainty as well as the training points.
Figure 4: The CDFs of predictive entropies on in-distribution and out-of-distribution test sets forvarious methods [Left] and the predictive class probability by these methods for representativesamples from an in-distribution test set [Top right] and an out-of-distribution test test [Bottom right].
Figure 5: Illustration of the effect of local variables and different kernels for the local variables.
Figure 6: Active learning with BNNs using mean-field Gaussian variational inference [MFVI] and ameta-GP hierarchical prior [MetaGP] on several UCI regression datasets. Each trace shows the rootmean squared error (RMSE) on the test set as more data points are selected and moved from the poolset to the training set, averaged over 40 runs. The objective function for selecting points from thepool set is the predictive variance. Best viewed in colour.
Figure 7: An evaluation of the covariance matrix approximations in a toy regression example. Top:objective function during training vs epoch/time. Bottom: Predictions after training using one of theapproximations discussed in the text. Global: there is one GP for all weights in the network. Layer:there are multiple GPs, one for each weight layer in the network. Best viewed in colour.
Figure 8: An evaluation of the covariance matrix approximations in a toy classification example. Top:objective function during training vs epoch/time. Bottom: Predictions after training using one of theapproximations discussed in the text. Global: there is one GP for all weights in the network. Layer:there are multiple GPs, one for each weight layer in the network. Best viewed in colour.
Figure 9: Performance of mean-field variational inference, NN-Metarep with variational inference and GP-Metarep with variational inference on a toy regressionproblem with various number of training points. Best viewed in colour.
Figure 10: We first train a model with an input-dependent kernel on a sinusoid data set (top) and thenvary the period hyperparameter of the input-dependent kernel whilst keeping other hyperparametersand variational parametes fixed (middle and bottom). Best viewed in colour.
Figure 11: Training on multiple related tasks and adaptation to novel tasks at test time. In this case, the latent variables (as well as weight code hypeÎ¹parameters)are shared across tasks while each individual has its own input-dependent kernel. At test time, only the private parameters for the new task are re-initialised andoptimised. Best viewed in colour.
Figure 12: Full results of the MNIST out-of-distribution uncertainty experiment. Best viewed incolour.
Figure 13: For clarify, we show a subset of the results of the MNIST out-of-distribution uncertaintyexperiment in fig. 12, for GP-metarep and deep kernel learning. Best viewed in colour.
Figure 14: Predictive distribution for representative MNIST test examples by various methods. Bestviewed in colour.
Figure 15: Predictive distribution for representative KMNIST test examples by various methods. Bestviewed in colour.
Figure 16: Performance of GP-metarep on a toy regression problem, with various numbers of hiddenunits and different activation functions. Best viewed in colour.
Figure 17: Active learning with BNNs using maximum a posteriori estimation [BNN-MAP], mean-field Gaussian variational inference [BNN-MFVI] and a meta-GP hierarchical prior [BNN-MetaGP]on a toy multi-class classification task. For each plot, the filled circle markers are the current trainingpoints, with different colours illustrating different classes. The shaded crosses are the examplesin the pool set, one of which we wish to pick and evaluate to be included in the training set. Theunfilled circle markers are the examples from the pool set selected at a step. The objective functionfor selecting points from the pool set is the entropy of the predictive probability. Best viewed incolour.
Figure 18: Left: Training data and predictions using a Bayesian neural network with a mean-field variational Gaussian approximation. The network has one hiddenlayer of 50 rectified linear units, hence there are 50 input weights and 50 output weights. Right: the marginal variational approximations over the weights aftertraining. Most hidden units are pruned out after training, as observed in Trippe & Turner (2018). In this case, there are 12 active hidden units. We include thehistogram of the weights in each layer in fig. 3.
Figure 19: Middle: Training data and predictions using a Bayesian neural network with an input-dependent meta GP prior. Similar to fig. 18, the network has onehidden layer of 50 units. The distributions of the weights in the network, given an input, are included. Note that, for each weight, we first sample the latent variable %,and then propagate this sample through the meta GP mapping to obtain a distribution of each weight. In each subplot, we plot 10 distributions corresponding to10 different Z samples. On the left figures, as part of the input to the GP is far away from the training inputs, the weights become uncertain which leads to highuncertainty in the prediction. On the contrary, around the training points, the GP outputs are more certain and more diverse. Note that, the marginal of each weight isan infinite mixture of Gaussian densities. Best viewed in colour.
