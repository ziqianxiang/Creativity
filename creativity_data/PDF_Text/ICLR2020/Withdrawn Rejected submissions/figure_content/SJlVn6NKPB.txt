Figure 1: Learned representations of out-of-sample image scenes, visualized with PCA followed byt-SNE and colored by OpenStreetMap category. Without any labels, Contrastive Sensor Fusion haslearned a representation that groups remote sensing images into semantically meaningful categories.
Figure 2: Coterminous remote sensing imagery from three different sensors: Airbus SPOT, NAIP(visualized here in near-infrared, red, and green), and Airbus Pleiades (See Appendix A.3 for details).
Figure 3: Contrastive Sensor Fusion architecture during training. Weights are shared across encodercopies. The contrastive loss trains the encoder to represent the same (different) location the same(different) way regardless of sensor/channel combination. The process to create views is explained inmore detail in Figure A.6, and the computation of the loss is detailed in Appendix A.2.
Figure 4: We compare the clustering of features based on OSM class using a nearest neighbormetric. The plots show the fraction of same-class neighbors for each point (k = 10) as input channelsare added (left), and the fraction of same-class neighbors as a function of k (right). One, two, andthree-channel experiments always use a single sensor, taking the red band only, the red and greenbands, and the RGB bands respectively. Our features outperform ImageNetâ€™s in this unsupervisedclustering metric and improve when multiple sensors are fused.
Figure 5: For each of the first three principal components of the 12-channel CSF representationspace, we show 10 images from each single sensor (with inputs for the other two sensor zeroed) thatmaximally activate these directions. These principal components of representation space representcontain concepts (fields, bridges, and bare ground / concrete) stable across sensor combinations.
