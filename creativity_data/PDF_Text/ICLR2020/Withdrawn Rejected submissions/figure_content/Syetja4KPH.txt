Figure 1: The Augmented Chain MDP5.1	The Augmented Chain EnvironmentSetup: The chain environment includes a chain of states S = 1, ..., n. In each step, the agentcan transition left or right. This standard-settings is augmented with additional k actions whichtransitions the agent to the same state (self-loop). We name this variation ”The Augmented ChainEnvironment”. All states have zero rewards except for the far-right n-state which gives a rewardof 1. Each episode is of length H = n - 1, and the agent will begin each episode at state 1.
Figure 2: Experimental results in the ”Augmented Chain Environment”. (a) Comparison of -greedyexploration with RLSVI exploration. (b) Effect of modeling the noise parameter as a random vari-able in comparison to different choices of a constant value. (c) Effect of the likelihood matchingmechanism. (d) Effect of the buffer size.
Figure 3: Learning curves of DQN(blue), DRLSVI(red), Rainbow(green) for the first 10M timesteps, for 5 different Atari games5.1.4	Buffer SizeThe previous experiment may suggest that catastrophic forgetting in DRLSVI can be avoided bysimply increasing the buffer size. In the following experiment, We examine the simple Chain envi-ronment (no self-loop actions; k = 0), with the following modification: we replaced the meaningof the actions in half of the states, i.e., to move right in the odd states, the agent needs to take theopposite action from the even states. We compare our algorithm with variants that do not match thelikelihood with different buffer sizes. Figure 2 (c) shows the performance of each of the algorithmsin this setup. We can see that our algorithm (blue) doesn’t suffer from degradation of performance.
