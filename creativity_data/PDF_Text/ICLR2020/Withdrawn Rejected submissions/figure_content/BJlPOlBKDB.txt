Figure 1: Image (left) (resp. Fourier (right)) space MSE against image (resp. Fourier) space empirical posteriorvariance constructed with 2 and 50 posterior samples respectively. The coordinate of each point is given by theimage (resp. Fourier) space MSE averaged over a reconstructed image and the pixel-wise image (resp. Fourier)space posterior variance averaged over this reconstructed image. The black lines represent the location obtainedby averaging over a sampling rate over the whole testing set, with steps of 2.5% sampling rate. The light redand blue lines are the example of a trajectory for a given image of the testing set when increasing the samplingrate.
Figure 2: Comparison of reconstruction quality and variance estimation quality for CLUDAS (U-masks),as well as the CLOMDAS (MSE-masks), for different sampling rates. The zoomed-in data are taken at thelocation highlighted by the yellow square on the upper-right image. The MSE-/U-masks show the evolution ofthe masks with increasing sampling rate (x-axis). The data are averaged on 2 testing samples6.3	Comparison with other methodsWe compare out method to the following•	Learning based compressed sensing (LBC)(Gozcu et al., 2018; Sanchez et al., 2019):This method incrementally builds up ω by computing an expected improvement of a loss `at each step. This expected improvement is simply obtained by searching which line willadd the largest improvement at the next step, and once it has been found, it is permanentlyadded to the mask. Then, the algorithm proceeds until the cardinality constraint ∣ω∣ = n ismet. When trained with MSE, we will refer to the method as LBC-M, and when trained tominimize variance, we will refer to it as LBC-V•	Vellagoundar & Machireddy (2015): This method proposed the simple heuristic ap-proach of (i) constructing a PDF from a training data and (ii) sampling at random fromthe obtained PDF. Our implementation used the spectrum of the whole averaged trainingset for the PDF.
Figure 3: Comparison of fixed masksobtained by the learning-based methodof GozCu et al. (2018). The horizontalaxis shows the mask growing as ele-ments are iteratively added to it.
Figure 4: Gaussian mask design used during training. We start with a plain Gaussian, rescale it to ensureselection in the tails,always sample the 4 highest energy lines in the center of the Fourier space, and finallyrenormalize and follow the modified Gaussian to sample the remaining lines.
Figure 5: The adaptive sampling method (left) uses the loss heuristic ` (MSE, uncertainty) on currently ob-served information prior to the observation and then choosing based on this. In contrast to this, the greedymethod wants to use the largest expected improvement in ` and so has to evaluate it in each possible future,effectively deciding a posteriori based on which yielded the actual greatest improvement.
Figure 6: MSE (left) and image posterior variance (right) averaged over the whole testing set for differentadaptive methods, average respectively on 2 and 10 samples from the posterior distribution.
Figure 7: Performance of baselines for all sampling rates considered for MSE, SSIM and Posterior variancefor (a) 2 averaged samples, (b) 10 averaged samples. The results are averaged across the whole testing set andseveral runs of each method was done to get error bars.
