Figure 1: Comparison performance in DOOM between our algorithm and the other two mainstreamworks. (a) Due to the reward signal from killing only, it is difficult for the agent to find otherobjectives, even if they are useful for the task. (b) Trying to design the reward function using theprior knowledge of experts. This usually leads to the agent tending to get easy rewards rather thansatisfying our preference. (c) Instead of applying excessive prior knowledge, a learnable weightvector is introduced to weigh the reward function of each objective and let the agent understand thepurpose of the task.
Figure 2: A sketch of Efficient Delivery. The purpose of the agent is to control the UAV to deliveryas many packages as possible. When the current delivery location is reached, the next deliverylocation appears. In order to accomplish the delivery task more efficiently, it is necessary for theagent to weigh the importance of delivery, charging and acceleration at every moment.
Figure 3: Performance comparison on Efficient Delivery games. (a) We compare our algorithm us-ing all three objectives (delivery, charging and acceleration) with that using two objectives (deliveryand charging), as well as DQN, which uses the reward function from delivery only. (b) We comparethe performance of our algorithm under different discount factor Î³.
