Figure 1: Top Left: Calibration measured through three metrics on MNIST with randomly assignedlabels, with the fraction of randomly assigned labels on the x axis (all others labels are correct).
Figure 2: Left: Per-class Adaptive Calibration Error (ACE) for a trained 110-layer ResNet on theCIFAR-100 test set. We observe that calibration error is non-uniform across classes, which is difficultto express when measuring error only on the maximum probabilities. Right: Reliability Diagramsare an interpretable visualization of calibration error. This is an example of the reliability of a slightlyoverconfident CNN trained on Fashion-MNIST.
Figure 3: Left: Model sharpness, where the vast majority of confidence scores are very near 1 or verynear 0, motivates ACE. This class-conditional histogram of confidence scores on 12,000 samples fromFashion-MNIST demonstrates that sharpness. Right: Binning scheme chosen by ACE measuredat each step of training on Fashion-MNIST. Each bin measuring the calibraion error of datapointswithin a particular range has a lower bound to the bin and an upper bound. As the model’s predictionsbecome sharper, the chosen bins adapt to focus on regions with a high concentration the predictions.
Figure 4: Top Left: Lower bounds of calibrations ranges over the course of training for adaptivecalibration error on Fashion-MNIST, focusing almost entirely on small ranges and motivatingthresholding. Top Right: On the MNIST training set with thresholding, so few values are small thatthe bottom of the lowest range often spikes to .99 and higher due to every datapoint being fit. BottomLeft: ACE on Fashion-MNIST validation with 100 calibration ranges. Bottom Right: Thresholdedadaptive calibration with 50 calibration ranges over the course of training on Fashion-MNIST’svalidation set.
Figure 5: This python code (with numpy imported as np, and with an expected calibration errorfunction loaded in the environment) implements a pathology in ECE which will return 0 calibrationerror by including overconfident and underconfident values in the same bin. The softmax here has450 predictions at .52 (ECE only looks at the maximum value) and evaluates all of them to beincorrect, despite being predicted with high confidence. The softmax also has 550 predictions at .58,all evaluated to be correct, though they’re predicted with low confidence (relative to 1 for correct and0 for incorrect). Despite this extreme miscalibration, the expected calibration error is exremely low( 0.003).
