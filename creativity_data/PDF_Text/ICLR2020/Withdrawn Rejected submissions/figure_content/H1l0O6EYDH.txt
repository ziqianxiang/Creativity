Figure 1: Left: architecture of our PC layer based on fast DHWT algorithm in Algorithm 1, Right:comparison of the number of multiplications between our new PC layers and the conventional PClayer.
Figure 2:  Structure of block units.  (a):  the basic block of ShuffleNet-V2; (b):  block using randomconstant pointwise convolution (RCPC) layers; (c):  block using conventional transform pointwiseconvolution (CTPC) layers with ReLU applied after each of CTPC layer; (d):  our proposed blockusing CTPC layers without ReLU. Block (b) randomly initialized the weights of PC layer with thedistribution of    (   1/   N/2, 1/   N/2), where N is the number of input channel and fixed theseweights during training.
Figure 3: Performance curve of hierarchically applying our optimal block on CIFAR100, Top: in theviewpoint of the number of learnable weight parameters, Bottom:  in the viewpoint of the numberof FLOPs.  The performance of baseline models was evaluated by ShuffleNet-V2 architecture withwidth hyper-parameter 0.5x, 1x, 1.1x, 1.5x.  Our models were all experimented with 1.1x setting,and each dot in the figures represents mean accuracy of 3 network instances. Note that the blue linedenotes the indicator of the efficiency of weight parameters or FLOPs in terms of accuracy.  Theupper left part from the blue line is the superior region while lower right part from blue line is theinferior region compared to the baseline models.
Figure 4:  Histograms of hierarchy level (low-level, middle-level, high-level) activations after theproposed PC layer based on conventional transforms, Left: DWHT, Right: DCT. Both DWHT andDCT models are based on ShuffleNet V2 1.1x model where we replaced all of stride 1 blocks with(d)-DWHT w/o ReLU and (d)-DCT w/o ReLU blocks, respectively in Figure 2.
Figure 5: Histogram of 3     3 depthwise convolution weights in the third block, out of last 3 blocks.
Figure 6:  Ablation study of negative slope term g in activation function F ,  which is defined asF (x) = max(0, x) + g   min(0, x). The performance of models were evaluated based on DCT orDWHT-13-H ShuffleNet-V2 1.1x where we applied F as an activation function after every DCT orDWHT based PC layer and Batch Normalization layer.
Figure 7: Ablation study of weight decay values (5e-4, 2e-3, 1e-2, 1e-1).  We applied these weightdecay values only on 3    3 depthwise convolution weights of last 3 blocks in DCT-based model andDWHT-based model, while all the other learnable weights were regularized with weight decay of5e-4.
Figure 8:  Performance curve of hierarchically applying our optimal block (See Table 2 for detailsettings) on CIFAR100, Top:  in the viewpoint of the number of learnable weight parameters, Bot-tom: in the viewpoint of the number of FLOPs. The performance of baseline models was evaluatedby MobileNet-V1 architecture with width hyper-parameter 0.2x, 0.35x, 0.5x, 0.75x, 1x, 1.25x. Ourproposed models were all experimented with 1x setting, and each dot in the figures represents meanaccuracy of 3 network instances.  Our models are experimented with 10-H, 6-H, 3-H models (firstcolumn) ,  7-M, 3-M-Rear,  3-M-Front models (second column) and 10-L, 6-L, 3-L models (finalcolumn), listed in ascending order of the number of learnable weight parameters and FLOPs.
Figure 9: Histograms of 3     3 depthwise convolution weights, Top: histogram of first block out oflast 3 blocks, Bottom:  histogram of second block out of last 3 blocks.  DWHT-3-H and DCT-3-Hmodels are based on ShuffleNet-V2 1.1x model with (d)-DWHT w/o ReLU and (d)-DCT w/o ReLUblock in Figure 2, respectively. Baseline model is ShuffleNet-V2 1.1x model.
Figure 10: Performance curve of hierarchically applying our proposed DCT/DWHT based PC layersand RCPC layer on CIFAR100 dataset. Experimental settings are the same as described in Figure 3.
