Figure 1: The architecture of NP2MT model. The example shows how the target phrase y3:4 istranslated conditioning directly on the source phrase x2:3, using the phrase-level attention. Notethat for brevity, in the phrase-level encoder, we only show one possible segmentation of the sourcesentence. We use "•一” to indicate all the possible segments Xij in Eq. 1.
Figure 2: The decoding process of NP2MT model with dictionary extension. The example showshow a German sentence is translated into the English sentence. Different colors are used to representthe aligned phrases. The system found the translations for the underlined OOV words in German.
Figure 3: Comparison on BLEU score improvement and lookup phrase ratio under different sourcelanguage OOV rates both under in-domain dictionaries. NP2MT and Transformer start from BLEUscore 30.99 and 31.27 on DE-EN, 29.93 and 29.72 on EN-VI, respectively.
