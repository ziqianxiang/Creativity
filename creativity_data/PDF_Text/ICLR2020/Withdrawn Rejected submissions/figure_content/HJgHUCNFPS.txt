Figure 1:  Input and output flow of our proposed framework.  (a) Video frames (both certain anduncertain environment) from resource constrained devices. (b) Annotate frames by detecting objectsof  interest,  apply  keyframes  selection  mechanism,  generate  summary,  encode  and  transmit  it  tomaster device. (c) Decode generated summary, perform features extraction, and forward it to activityprediction model at master device to get the output class with probability score.
Figure 2:  The conceptual diagram of our proposed framework, where slave devices with cameracapture multi-view video data,  apply shot segmentation,  extract keyframes,  encode and transmitthem to master device for computing inter-view correlations and activity recognition.
Figure 3: Sample results from road dataset. (a) Normal input images, (b) fog applied with differentparameters, (c) object detection in foggy images.  The detection results are encouraging for objectdetection in foggy environment.
Figure 4: Sample results generated on UCF-50 video captured from two different views. Keyframesfrom both input videos are generated and then VGG-19 features are extracted.  These features arecompared and the selected single feature vector is encoded into 11000.  Final output of activity isgenerated by passing this feature vector to trained model that gives probability score along with thepredicted class.
Figure 5:  Transmission time and storage size comparison of transmitting and storing overall video(Office-0 video) versus keyframes and encoded keyframes. A huge difference of saving transmissiontime and storage capacity can be observed among all these possible settings.
