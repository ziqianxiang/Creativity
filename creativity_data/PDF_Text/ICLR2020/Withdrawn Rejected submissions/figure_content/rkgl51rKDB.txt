Figure 1: (a) Two level hierarchy ofMGHRL. High level policy μh takes in state and outputs goalsat intervals. LoW level policy μl takes in state and desired goals to generate primitive actions. (b)High level meta-training framework of MGHRL. The context encoder network uses high-levelcontext data Chi to infer the posterior over the latent context variable z, and is optimized With gradi-ents from the critic as well as from an information bottleneck on z. The actor network μφ(g∣s,z)and critic netWork Qθ(s, g, z) both treat z as part of the state.
Figure 2: The four evaluated robotics environmentsFetch-Reach Fetch has to move the gripper to the desired goal position. This task is very easy tolearn and is therefore a suitable benchmark to ensure that a new idea works at all.
Figure 3: Average success rates for MGHRL, HAC, PEARL agents in each task, each algorithm wastrained for 1e6 steps. The error bar shows 1 standard deviation.
