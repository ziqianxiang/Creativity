Figure 1: The motivation of learning class-specific filters. In a normal CNN, each filter correspondsto multiple classes, since it extracts a mixture of features from different classes Zhang et al. (2018b),which is a symptom of filter ambiguity. In contrast, We enforce each filter to correspond to one(or few) classes, namely to be class-specific, which brings better interpretability and class-relatedfeature representation. Such representation not only facilitates understanding the inner logic ofCNNs, but also improves different application tasks related to the mechanism of filters, which isverified by our exhaustive experiments.
Figure 2: The intuition of disentangling filters to class-specific. In a normal CNN, each filterextracts a mixture of features from different classes (Zhang et al., 2018b), which is a symptomof filter ambiguity. In contrast, when filters are ideally class-specific, each filter extracts featuresmainly from only one class. Given the labeled class of an input, the filters irrelevant to the classhave weak activation, and hence classification performance changes little when they are shut down.
Figure 3: The framework of Label-Sensitive Gate (LSG) training. During LSG training, we al-ternately train a CNN through both the LSG path and the standard (STD) path. In the STD path,network parameters are optimized to minimize the cross-entropy. In the LSG path, feature mapsafter the last convolutional layer pass through the learnable gate which is a row vector in the LSGmatrix indexed by the label of the input. Network parameters and LSG matrix are optimized tominimize the cross-entropy in conjunction with a sparsity regularization for the LSG matrix. Whentesting, we just run the STD path.
Figure 4: Visualization of the gate matrices from LSG training w/o the sparsity regularization on thegate matrix. The x-axis is the filter id from 1 to 64, the y-axis is the class id in CIFAR10 from 0 to9, and the color represents how much a filter is related to a class.
Figure 5: Classification confusion matrix for STD / LSG models When masking filters highly relatedto the first class (a1, b1), and the first and sixth classes (a2, b2).
Figure 6: The similarity between feature vectors from a class (y-axis) and a row in the LSG matrix(x-axis), averaged over all TP/FN samples. We reorder classes in CIFAR10 for better visualization.
Figure 7: The correlation (inner product) matrix of filters in AlexNet and ResNet20 trained withSTD/LSG. (e) shows the ratio of each matrix, elements larger than a varying threshold.
Figure 8: Visualizing the localization in STD CNN and LSG CNN with CAM (Zhou et al., 2016).
Figure 9: Manual fixed gate matrix. Each class monopolizes 6 filters and 4 extra filters are sharedby all classes.
Figure 10: Cluster center experiment. The x-axis is the channel id, and the y-axis is class id. Eachrow is a the mean of a cluster in the feature vectors, space. and the color represents the value of anelement in the mean.
