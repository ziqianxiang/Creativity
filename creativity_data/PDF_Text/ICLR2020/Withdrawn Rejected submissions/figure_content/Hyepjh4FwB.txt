Figure 1: ProtoAttend bases the decision on a few prototypes from the database. This enablesinterpretability of the prediction (by visualizing the highest weight prototypes) and confidenceestimation for the decision (by measuring agreement across prototype labels).
Figure 2: ProtoAttend method for training and testing. Shared encoder between input samples andthe candidate samples generates input representations, that are mapped to key, query and valueembeddings (with a single nonlinear layer). The alignment between keys and queries determinesthe weights of the prototypes, and the linear combination of the values determines the final decision.
Figure 3: Impact of confidence on ProtoAttend accuracy. Reliability diagram for Fashion-MNIST, asin (Papernot & McDaniel, 2018). Bars (left axis) indicate the mean accuracy of predictions binned byconfidence; the red line (right axis) shows the number of samples across bins.
Figure 4:	Example inputs and ProtoAttend prototypes for (a) MNIST (with sparsemax), Fashion-MNIST dataset (with sparsemax and sparsity regularization) and (b) Fruits (with sparsemax andsparsity regularization). For MNIST & FaShion-MNIST, prototypes typically consist of discriminativefeatures such as the straight line shape for the digit 1, and the long heels and strips for the sandal. ForFruits, prototypes often correspond to the same fruit captured from a very similar angle.
Figure 5:	Example inputs and ProtoAttend prototypes for DBPedia (with SParsemax). While classify-ing the inputs as athlete, prototypes have very similar sentence structure, words and concepts.
Figure 6:	Example inputs and ProtoAttend prototypes for Adult Census Income (with SParSemaXand sparsity regularization). For the first example, all prototypes have similar age, two share similareducation level and one has the same occupation. For the second example, three prototypes have thesame occupation, all work more than 40 hours/week, and three have postgraduate education.
Figure 7: Samples found by ProtoAttend vs. representer point selection (Yeh et al., 2018) andinfluence function (Koh & Liang, 2017) for the two examples from (Yeh et al., 2018) on Animalswith Attributes dataset. See Supplementary Material for more examples.
Figure 8: Confidence-controlled prediction. (a) Accuracy vs. ratio of samples for MNIST. Wecompare dkNN (Papernot & McDaniel, 2018) and prototypical learning (with softmax attentionand λconf =0.1) using the same network architecture from (Papernot & McDaniel, 2018) withoutaugmentation. (b) Accuracy vs. ratio of samples for CIFAR-10. We compare prototypical learning(with softmax attention and λconf =0.1) with trust score (Jiang et al., 2018) and deep ensemble(Lakshminarayanan et al., 2017) methods for the same baseline encoder network architecture.
Figure 9: Out-of-distribution detection. (a) Ratio of samples above the confidence level for pro-totypical learning with softmax attention, trained with Fashion-MNIST, and tested on the showndatasets. E.g. if we assess the ratio of samples above confidence 0.9, it is far more likely that thosesamples come from the same distribution with the training dataset. (b) ROC curve for in-distributionvs. out-of-distribution detection, using CIFAR-10 as in-distribution and SVHN as out-of-distribution,computed using the method from (Hendrycks & Gimpel, 2016) and compared to the proposed base-line in (Hendrycks & Gimpel, 2016). Softmax attention and confidence regularization (λconf = 0.1)are used.
Figure 10:	Example inputs and corresponding prototypes for CIFAR-10.
Figure 11:	Example inputs and corresponding prototypes for DBPedia (with SParSemax).
Figure 12: Example inputs and corresponding prototypes for ISIC Melanoma (with sparsemaxattention).
Figure 13: Relevant samples found by ProtoAttend with sparsemax attention vs. representer pointselection (Yeh et al., 2018) for the examples from Supplementary Material of (Yeh et al., 2018).
Figure 14: Accuracy vs. ratio of samples for (a) MNIST and (b) Fashion MNIST, for confidencelevels between 0 and 0.999.
Figure 15: Accuracy vs. ratio of samples for (a) DBpedia and (b) Adult Census Income, for confidencelevels between 0 and 0.999.
Figure 16: Area-under-curve (AUC) vs. ratio of samples for ISIC Melanoma with softmax attention,for confidence values ranging between 0 and 0.99.
Figure 17: Number of training iterations vs. median number prototypes to explain 95% of the decision(in logarithmic scale), for Fashion-MNIST with softmax attention.
