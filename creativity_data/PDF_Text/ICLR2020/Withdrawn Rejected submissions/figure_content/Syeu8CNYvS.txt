Figure 1: ADKL-KRR. The blue and orange colors show the procedure for a task during internal trainand test, respectively. During training, ADKL first computes a task embedding zt = ψη(Dttrn) that isused with a pseudo-representations U by the network cρ to produce a the task-specific kernel function.
Figure 2: Statistics on bio-assay modelling tasks. Left: Number of samples per task. Middle: Noise-to-signal ratio. Right: Within-taskversus overall molecular diversity.
Figure 3: Average MSE performance on the meta-test during active learning. The width of the shadedregions denotes the uncertainty over five runs for the sinusoidal collection. No uncertainty is shownfor the real-world tasks as they were too time consuming.
Figure 4: Relative decrease/increase in the meta-test MSE compared to different baselines. In (a) and(c) the baselines are γtask = 0 and γpseudo = 0, respectively. In (b) and (d) the baselines are |U|= 07 ConclusionWe investigate bio-assays modelling using FSR methods. Our proposed method, ADKL, storesmeta-knowledge in kernel functions and adapts to new tasks using KRR or GP. Our experimentsprovide evidence that the additional adaptation capacity at test-time provided by ADKL increasesgeneralization significantly. Also, in a Bayesian setup, ADKL provides better predictive uncertainty,increasing their utility in bioassay modelling. However, there is still room to improve ADKL andmost meta-learning methods to be better than traditional chemoinformatic methods. We hope, bymaking our bio-assay task collections publicly available, that the community will leverage them topropose new competitive FSR methods.
Figure 5: Relative improvement of the MSE depending on the γtask parameterFor a more in-depth analysis, we show below the similar tables and figures for different values of K (5, 10 and 20). These results confirm that regularizing the task encoder is helpful for any value ofK, even though the impact seems to become much more important as K increases (observe that themaximum improvement in each figure increases with K).
Figure 6: Relative improvement of the MSE depending on the γtask parameterOnce again, for a more in-depth analysis, we show below the same format of tables and figures fordifferent values of K , confirming again that regularizing using the pseudo-representation can be veryhelpful for any value of K . It is worth noticing here that the improvement gain is more consistent for14Under review as a conference paper at ICLR 2020494495496497498499500501502503504505506507
Figure 7: Average relative improvement of the MSEand joint impact of γtask and γpseudo .
Figure 8: Meta-test time predictions on the Sinusoids collection16Under review as a conference paper at ICLR 2020515 C S upplementary results for the real-world datasets0.35 -0.30 -0.25 -MH 0.20 -qW 0.15-0.10 -0.05 -0.00 -antibacterial, |K| = 50.200 -0.175 -0.150 -■ 0.125-≡
Figure 9: Distribution of the mean squared error (MSE) across the tasksFigure 9 shows the distribution over the random support/query sets generated at test time. Note thatthe results presented in the main paper estimate the influence of the initialisation by using multipleseeds and computing the standard deviation on the average MSE (averaged over the support/querysplits).
