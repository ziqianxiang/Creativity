Figure 1: The overview of our aggregation algorithm for attack-resistant federated learning.
Figure 2: Parameter confidence assignment basedon the residual which is the distance from a point tothe regression line. In the green area, the parameterconfidence is 1; in the orange area, the confidencedecays from 1 to Î´ ; in other areas, the confidenceis set to 0.
Figure 3: Results of label-flipping attacks on theMNIST dataset. The number of participants is100, within which 0 to 10 of them are attackers.
Figure 4: Result of backdoor attack success rateon CIFAR-10 under different aggregation algo-rithms. Ours outperforms other baselines.
Figure 5: Results of removing users with digit 0.
Figure 6: A backdoored image. Thereis a white color pattern in the left uppercorner.
Figure 7: Results of backdoor attack success rate on theMNIST dataset with ResNet-18. Our algorithm is stillstable while others fail for the complicated model.
Figure 8: Accuracy of the global model and the attack success rate under backdoor attack. The attacksuccess rate of baseline methods start to increase when the model converges.
