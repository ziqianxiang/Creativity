Figure 1: In both plots, the solid line with markers plots the loss vs. gradient error variance (LHS of(4)) across 50 steps of SGD for several different setups. The left dashed line represents the RHS of(4) and the right dashed line is the RHS with C instead of c.
Figure 2: Adaptation of various training schemes over four different training environments (a) to(d). In each training environment, the top plot shows the exponential moving averages (0.999) of theper-sample online accuracy of the five training schemes, while the bottom plot shows the maximumnumber of updates applied to any given convolution or fully-connected kernel memory cell. For thedistribution shifts in (b), the enabled augmentations at each contiguous 10k samples is shown (CD =class distribution, ST = spatial transforms, BG = background gradients, WN = white noise).
Figure 3: Signal flow graph for a forward and backward quantized convolutional or dense layer.
Figure 4: Maximum magnitude of weight gradients versus training step for standard SGD on a CNNtrained on MNIST.
Figure 5:	Samples of different types of distribution shift augmentations.
Figure 6:	The left two heat maps are used to select the base / standard SGD learning rate. The righttwo heat maps are used to select the SKS learning rate using the optimal SGD learning rate for biastraining from the previous sweeps. For the SKS sweeps, the learning rate is scaled proportional tothe square-root of the batch size B . This results in an approximately constant optimal learning rateacross batch size, especially for the max-norm case. Accuracy is reported averaged over the last 500samples from a 10k portion of the online training set, trained from scratch.
Figure 7:	Accuracy across a variety of SKS ranks and weight bitwidths, showing the expected trendsof increasing accuracy with rank and bitwidth. Accuracy is calculated by averaging the accuracyon the last 500 samples from a 2k portion of the training data. For bitwidths of 1 and 2, mid-risequantization is used (e.g., 1 bit quantizes values to -0.5 and 0.5 instead of -1 and 0).
