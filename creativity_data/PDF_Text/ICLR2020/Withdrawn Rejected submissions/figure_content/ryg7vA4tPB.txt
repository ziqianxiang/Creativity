Figure 1: Dynamic sparse training aims to change connectivity during training to help out optimza-tion.
Figure 2: (left) Performance of various dynamic sparse training methods on ImageNet-2012 classi-fication task. We use 80% sparse ResNet-50 architecture with uniform sparsity distribution. Pointsat each curve correspond to the individual training runs with training multipliers from 1 to 5 (exceptpruning which is scaled between 0.5 and 2). We repeat training 3 times at every multiplier and reportthe mean accuracies. The number of FLOPs required to train a standard dense Resnet-50 along withits performance is indicated with a dashed red line. (right) Performance of RigL at different sparsitylevels with extended training. Results are averaged over 3 runs.
Figure 3: (left) RigL significantly improves the performance of Sparse MobileNets on ImageNet-2012 dataset and exceeds the pruning results reported by Zhu & Gupta (2018). Performance ofthe dense MobileNets are indicated with red lines. (right) Performance of sparse MobileNet-v1architectures presented with their inference FLOPs. Networks with ERK distribution get betterperformance with the same number of parameters but take more FLOPs to run. Training widersparse models with RigL (Big-Sparse) yields a significant performance improvement over the densemodel.
Figure 4: (left) Final validation loss of various sparse training methods on character level languagemodelling task. Cross entropy loss is converted to bits (from nats). Performance and the training costof a dense model is indicated with dashed red lines. (right) Test accuracies of sparse WideResNet-22-2’s on CIFAR-10 task.
Figure 5: (left) Performance of RigL at different sparsities using different sparsity masks (right)Ablation study on cosine schedule. Other methods are in the appendix.
Figure 6: (left) Training loss evaluated at various points on interpolation curves between a magnitudepruning model (0.0) and a model trained with static sparsity (1.0). (right) Training loss of RigL andStatic methods starting from the static sparse solution, and their final accuracies.
Figure 7: (left) Effect of sparsity distribution choice on sparse training methods at different sparsitylevels. We average over 3 runs and report the standard deviations for each. (right) Effect of momen-tum value on the performance of SNFS algorithm. Setting the momentum coefficient of the SNFSalgorithm to 0 seems to perform best, suggesting the accumulated values are not important.
Figure 8:	Cosine update schedule hyper-parameter sweep done using dynamic sparse training meth-ods SET (left) and SNFS (right).
Figure 9:	Using other update schedules with RigL: (left) Constant (middle) Exponential (k=3) and(right) Linear16Under review as a conference paper at ICLR 2020Sparsity Sweep o∩ CIFAR-10d00∙300.500.450.400.35SSOCCrŋ0.250.200.150.10
Figure 10:	Final training loss of sparse models (left) and performance of RigL at different maskupdate intervals (right).
Figure 11: Sparsities of individual layers of the ResNet-50.
