Figure 1: The summary of our DP-auto-GAN algorithmic framework. Pre- and post-processing (inblack) are assumed to be public knowledge. Encoder and generator (in green) are trained withoutnoise injection, whereas decoder and discriminator (in yellow) are trained with noise. The four redarrows indicate how data are forwarded for each training: autoencoder training, generator training,and discriminator training. After training, the generator and decoder (but not encoder) are releasedto the public to generate synthetic data.
Figure 2: Dimension-wise probability scatterplots for different values of . For each point in the plotrepresents one of the 1071 features in MIMIC-III dataset. The x and y coordinates of each point arethe proportion of 1 in real and synthetic datasets of a feature, respectively.
Figure 3: Dimension-wise prediction scatterplots for different values of . Each point representsone of 1071 features in MIMIC-III dataset. For each point, the x and y coordinates represent theAUROC score of a logistic regression classifier trained on real and synthetic datasets, respectively.
Figure 4: Dimension-wise prediction scatterplot for different values of . Each point represents oneof 14 features in the ADULT dataset. Blue points and single green points correspond to categoricalfeatures, and are plotted according to F1 score. Red points correspond to real-valued features, andwe plot R2 score. For each point, x and y coordinate represents relevant score evaluated on real andsynthetic datasets, respectively.
Figure 5: Scatterplot of projection of dataset on first two principle component of the real datasetof plots clearly shows the similarities of trends between the plots for real dataset and for differentvalues of , as low as = 1. Finally we also evaluate Wasserstein distributional distance betweensynthetic and real data, shown in Table 3.
Figure 6: Full plots of dimension-wise prediction for mimic dataset(c) = 1.27655	(d) = 0.94145Below we provide 1-way histogram for ADULT dataset. As one can see, DP-auto-GAN identifiesthe marginal distribution of capital gain and capital loss quite well and it does reasonably well onhours-per-week feature.
Figure 7: 1-way histogram for different values of . Three rows correspond to capital gain, capitalloss and weekly work-hours(0): Linear(in-feature=15, out-feature=60, bias=True)(1): LeakyReLU(negative-slope=0.2)(2): Linear(in-feature=60, out-feature=106, bias=True)(3): Sigmoid()))Generator((block-0): Sequential((0): Linear(in-feature=64, out-feature=64, bias=False)(1): BatchNorm1d()(2): LeakyReLU(negative-slope=0.2))(block-1): Sequential((0): Linear(in-feature=64, out-feature=64, bias=False)(1): BatchNorm1d()(2): LeakyReLU(negative-slope=0.2))(block-2): Sequential(
