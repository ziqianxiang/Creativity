Figure 1: Impact of stability onmemory-efficient training. We mea-sure the angle between the true gra-dient (using stored activations) andthe memory-saving gradient (using re-computing activations). For all epochsafter the dotted vertical line, the affinemodel had numerically infinite or nangradients, while the additive model givesaccurate memory-efficient gradients.
Figure 2: Measuring stability of additive and affine INN classifiers on CIFAR-10. Right: wetrack the reconstruction errors for a fixed minibatch over the course of training. In each iteration(vertical slice) we plot the errors of each minibatch element. Orange points in the affine plot denoteinf and nan values. c denotes the condition number of the Jacobian, σmax and σmin denote themax and min singular values. At the top, we show the original images x and in each snapshot, thereconstructions X = FT(F(x)) and reconstruction errors R = |x - X|.
Figure 3: Crafting non-invertible inputs for a CIFAR-10 Glow model. For three different images,we show: 1) the original datapoint X from which we start running PGD; 2) the crafted input X0 thatresults from PGD; and 3) the reconstruction X0 = F-1 (F(x0)) of the crafted input. In all cases, thereconstructions of adversarial inputs are heavily corrupted, indicating that the attacks were successful,and exposing non-invertibility in this Glow model.
Figure 4: Reconstructions of images through multiple forward/inverse passes. First row in the‘Recons’ subfigures are real-data, and each row after is one forward/inverse pass.
Figure 5: Instability in affine and additive models. On the right, we track the reconstruction errorsfor a fixed minibatch over the course of training: in each iteration (vertical slice) we plot the errorsof each minibatch element. Thus, we can observe the distribution of errors. Orange points in theaffine plots denote inf and nan values. c denotes the condition number of the Jacobian, σmaxand σmin denote the maximum and minimum singular values, respectively. At the top, we showthe original images X and in each snapshot, the reconstructions X = FT(F(x)) and reconstructionerrors R = |x - X|. Note that the X- and y-axes differ for different settings, as the models becomeunstable at different points during training.
Figure 6: Test classification accuracyon CIFAR-10.
Figure 7:	Crafting non-invertible inputs for a CIFAR-10 Residual Flow model.
Figure 8:	Glow becomes non-invertible for inputs outside the training distribution. We use aSOTA Glow model trained on images normalized to [-0.5, 0.5], and use PGD to find an adversarialinput constrained to the larger range [-0.7, 0.7]. This attack succeeds in finding examples that inducedramatic reconstruction error.
Figure 9: Toy example to motivate the decorrelation task. Left: standard normal data, 2. left: scaledby D and rotated data by R, 2. right: scaling and rotation backwards, low correlation but highercondition number, right: rotation backwards, low correlation and low condition number.
Figure 10:	Comparing stability of additive flows of different depths. These models all haveActNorm both between and inside the additive blocks.
Figure 11:	Additional settings for additive coupling, where: 1) ActNorm was applied only betweenblocks; and 2) ActNorm was applied only inside blocks. Both models become unstable and exhibitsevere reconstruction artifacts.
Figure 12:	Additional setting for affine coupling, where ActNorm was applied only between blocks.
Figure 13:	Decorrelation loss plots. These plots track the norm of the off-diagonal entries in thecorrelation matrix while training with the decorrelation objective.
Figure 14: Additive: ActNorm Between Blocks40k	80k	120kIterationOk2 3 4 5 6 7 8-------Ooooooo>s EnE⊂一 W40k	80k	120kIteration(b)	Max singular values(c)	Min singular valuesFigure 15:	Additive: ActNorm Both Inside and Between Blocks7 5 3 1 9 7 5101010101 1 1UJnN Uo-I-Puoo5 3 1 9 7 5 3尸 AAoooOWwwiiii>∞ EnEXBW
Figure 15:	Additive: ActNorm Both Inside and Between Blocks7 5 3 1 9 7 5101010101 1 1UJnN Uo-I-Puoo5 3 1 9 7 5 3尸 AAoooOWwwiiii>∞ EnEXBW2 3 4 5 6-----OooooIllll>∞ EnEWWOk 40k	80k	120kIteration(a)	Condition numberOk 40k	80k	120kIteration(b)	Max singular valuesOk 40k	80k	120k
Figure 16:	Additive: ActNorm Inside Blocks22Under review as a conference paper at ICLR 2020Ok 60k	120k	180kIteration(a) Condition numberOk 2k 4k 6k 8k IOkIteration(a)	Condition numberO IOOO 2000Iteration>∞ UJnuJXBW(a) Condition numberOk 2k 4k 6kIteration(a) Condition number8 7 6 5 4 3 2 1Oooooooo11111111>∞ UJnuJ×ΠJW
Figure 17: Additive: No ActNorm>∞ EnuJ-XBWOk 2k 4k 6k 8k IOkIteration(b) Max singular valuesFigure 18: Affine: No ActNormo IOOO 2000Iteration(b) Max singular valuesFigure 19: Affine: ActNorm Both Inside and Between Blocks∩ls1>ω UJnEXBWOk Ik 2k 3k 4k 5k 6kIteration(b)	Max singular values2 3 4 5 6 7 8-------Ooooooo>s EnE'≡一 W
Figure 18: Affine: No ActNormo IOOO 2000Iteration(b) Max singular valuesFigure 19: Affine: ActNorm Both Inside and Between Blocks∩ls1>ω UJnEXBWOk Ik 2k 3k 4k 5k 6kIteration(b)	Max singular values2 3 4 5 6 7 8-------Ooooooo>s EnE'≡一 W60k	120k	180kIteration(c)	Min singular values>∞ EnE-UMOk 2k 4k 6k 8k IOk
Figure 19: Affine: ActNorm Both Inside and Between Blocks∩ls1>ω UJnEXBWOk Ik 2k 3k 4k 5k 6kIteration(b)	Max singular values2 3 4 5 6 7 8-------Ooooooo>s EnE'≡一 W60k	120k	180kIteration(c)	Min singular values>∞ EnE-UMOk 2k 4k 6k 8k IOkIteration(c) Min singular valuesO456789012
Figure 20:	Affine: ActNorm Between Blocks23Under review as a conference paper at ICLR 2020I	Refitting Prior in Flow- GANIn general, if the model is not optimized with forward KL, DKL(Pdata∣∣Pθ), as in the case Whenoptimizing with maximum likelihood, we cannot be sure F(x), X 〜Pdata is best fitted with astandard Normal. Hence, a reasonable strategy, without changing anything in the learned network, isto refit the prior parameters. Here we simply optimize for maximum likelihood (as typically done forflow models) while only fitting a diagonal variance in prior. This can be interpreted as increasing theentropy of our model. In Kingma & Dhariwal (2018), they observed the opposite phenomenon that amodel trained with maximum likelihood generates better samples after decreasing the entropy in theprior. See Figure 21 for samples after refitting the prior.
Figure 21:	The row number corresponds to the number of epochs after refitting the prior variance.
