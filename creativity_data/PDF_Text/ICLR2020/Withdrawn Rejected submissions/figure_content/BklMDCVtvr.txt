Figure 1: The role learning module. The role attention vector at is encouraged to be one-hot throughregularization; if at were one-hot, the produced role embedding rt would correspond directly to oneof the roles defined in the role matrix R. The LSTM can be unidirectional or bidirectional.
Figure 2: Left: Example of successive constituent surgeries. The roles assigned to the input symbolsare indicated in the first line (e.g., run was assigned role 11). Altered output symbols are in blue.
Figure 3: The Tensor Product Encoder architecture. The yellow circle is an embedding layer forthe fillers, and the blue circle is an embedding layer for the roles. These two vector embeddings arecombined by an outer product to produce the green matrix representing the TPR of the constituent.
