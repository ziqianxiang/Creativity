Figure 1: Scheme ofrepresentationOur model consists of the following two components. See Figure 1 for an illustration, where theimage is illustrated by the big rectangle. A pixel is illustrated by a dot. The local image content isillustrated by a small square around it. The displacement of the pixel is illustrated by a short arrow,which is within the small square. The vector representation of the local image content is representedby a long vector, which rotates as the image undergoes deformation due to the pixel displacements.
Figure 2:	Learned results on V1Deform. (a) Learned units. Each block shows two learned units within thesame sub-vector. (b) Fitted Gabor patterns. (c) Distributions of spatial-frequency bandwidth (in octaves) andspatial phase φ.
Figure 3:	Learned results on band-pass image pairs from V1Deform. (a) Learned units. Each block shoWs tWolearned units Within the same sub-vector. (b) Distribution of the Gabor envelope shapes in the Width and lengthplane. (c) Difference of frequency f, orientation θ and phase φ of paired units Within each sub-vector.
Figure 4: Examples of inference of displacement field on V1Deform, V1FlyingObjects and MPI-Sintel. Foreach block, from left to right are It, It+1, groUnd trUth displacement field and inferred displacement field bypre-trained FloWNet 2.0 model and our learned model respectively. * indicates that the results are refined bythe refinement CNN. The displacement fields are color coded. See sUpplementary for the color code (LiU et al.,2010).
Figure 5: Examples of inferred displacement fields by unsupervised learning. The top row shows the observedimage sequences, while the bottom row shows the inferred color coded displacement field (Liu et al., 2010).
Figure 6:	Color map for the color coded displacement fields. The displacement of every pixel in thismap is the vector from the center of the square to this pixel. The center pixel does not move. Therange of color is taken according to the maximum length of flows in each displacement field.
Figure 7:	Animation of the learned filtersD Learned filtersFigure 8 shows the learned filters under different settings, including learned on V1FlyingObjects,learned with parametric M , learned with local mixing model (eqn 5) and learned unsupervisedly.
Figure 8:	Filters learned from under different settings: (a) filters learned on V1FlyingObjects withnon-parametric M ; (b) filters learned with parametric M ; (c) filters learned with non-parametricM and local mixing motion model (model used in section 5.3); (d) filters learned on MUG Facialexpression dataset with unsupervised learning (model used in section 5.4).
Figure 10: Examples of frame interpolation, learned With non-parametric M . For each block, the firstframe and last frame are given, While the frames betWeen them are interpolated frames.
Figure 11: (a) Filters learned with higher dimension of sub-vectors. The total number of units inthe whole vector is fixed to 96. Each block shows the learned units within the same sub-vectors. (b)Filters learned without sub-vector assumption.
Figure 12: More examples of inferred displacement fields by unsupervised learning. The top row shows theobserved image sequences, while the bottom row shows the inferred color coded displacement field.
