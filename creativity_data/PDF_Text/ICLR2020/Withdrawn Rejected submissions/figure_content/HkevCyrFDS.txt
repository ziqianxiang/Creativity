Figure 1: Overview of our proposed UVIT model: given an input video sequence, we first decom-pose it to the content by Content Encoder and the style by Style Encoder. Then the content is pro-cessed by special RNN units- TrajGRUs (Shi et al., 2017) to get the content used for translation andinterpolation recurrently. Finally, the translation content and the interpolation content are decodedto the translated video and the interpolated video together with the style latent variable. We depicthere the video translation loss (orange), the cycle consistency loss (violet), the video interpolationloss (green) and the style encoder loss (blue).
Figure 2: Video interpolation (left) and video translation (right): two processes share modules or-ganically. The input latent content is processed by the Merge Module to merge information fromTrajGRUs in both the forward and the backward direction. The translated content (lttrans) is obtainedby updating interpolated content (ltinterp) with the content (lt) from the current frame (at).
Figure 3: Label-to-image: multi-subset outcomes. Top left: input semantic labels; Top right: trans-lated day video; Middle left: translated sunset video; Middle right: translated rain video; Bottomleft: translated snow video; Bottom right: translated night video.
Figure 4: Label-to-image: translation with multimodality. Top left: label inputs; Top right: trans-lated video 1; Bottom left: ground truth; Bottom right: translated video 2. By incorporating differentstyle codes, UVIT could generate different translation results.
Figure 5: Label-to-image: qualitative comparison. First row: label inputs; Second row: improvedRecycleGAN outputs; Third row: UVIT outputs. Fourth row: ground truth.
Figure 6: Viper sunset to viper day. Top: input sunset video; Bottom: translated day video.
Figure 7: Style inconsistency of RecycleGAN. Top: label inputs; Bottom: RecycleGAN outputs. Itis evident that the frames produced by RecycleGAN fail to be style consistent. The first frame is inthe rain scenario, the following frames gradually turns to the sunset scene images.
Figure 9: Label-to-image: qualitative comparison. First row: label inputs; Second row: improvedRecycleGAN outputs; Third row: UVIT outputs. Fourth row: ground truth. Incomplete translationof the large road in improved RecycleGAN outputs becomes better for the UVIT model.
Figure 10: Label-to-image: qualitative comparison. First row: label inputs; Second row: improvedRecycleGAN outputs; Third row: UVIT outputs. Fourth row: ground truth. The boundary betweendifferent cars is clearer in the UVIT model output.
Figure 11: Label-to-image: translation with multimodality. The translated day sequences havedifferent car style versions. Top left: label inputs; Top right: translated video 1; Bottom left: groundtruth; Bottom right: translated video 2.
Figure 12: Cityscapes to Viper translation. Top left: input Cityscapes video; Top right: translatedViper video in the night scenario; Bottom left: translated Viper video in the snow scenario; Bottomright: translated Viper video in the sunset scenario.
