Figure 1: Probability P(f) that a function obtains upon random choice of parameters versus LempelZiv complexity KLZ (f) for (a) an n = 7 perceptron with b = 0 and weights sampled from aGaussian distributions, (b) an n = 7 perceptron with b = 0 and weights sampled from a uniformdistribution centred at 0 and (c) a 1-hidden layer neural network (with 64 neurons in the hiddenlayer). Weights w and the threshold bias terms are sampled from N (0, 1). For all cases 108 sampleswere taken and frequencies less than 2 were eliminated to reduce finite sampling effects. We presentthe graphs with the same scale for ease of comparison.
Figure 2: Probability P (f) vs rank for functions for a perceptron with n = 7, σb = 0, and weightssampled from independent Gaussian distributions. In Figures 2b and 2c the functions are rankedwithin their respective Ft. The seven highest probability functions in Figure 2c are f = 0101 . . .
Figure 3: Effect of adding a bias term sampled from N (0, σb) to a perceptron with weights sampledfrom N (0, 1). (a) Increasing σb increases the bias against entropy, and with a particular strong biastowards t = 0 and t = 2n. (b) P (t = 0) increases with σb and asymptotes to 1/2 in the limitσb → ∞.
Figure 4: P(T = t) becomes on average more biased towards low entropy for increasingnumber of layers or increasing σb. Here we use n = 7 input layers, with input {0, 1}7 (centereddata) or {-1, 1}7 (uncentered data) The hidden layers are of width 2n-1 = 64 to guarantee fullexpressivity.。仅=1.0 in all cases. The insets show how P(t = 0) asymptotes to 1 with increasinglayers or σb .
Figure 5: Probability vs rank for functions (ranked by probability) from samples of size 108, withinput size n = 7, and every weight and bias term sampled from N(0, 1) unless otherwise specified,over initialisations of: (a) a perceptron with b = 0; (b) a perceptron; (c) a one-hidden layer neuralnetwork (with 64 neurons in the hidden layer); (d) a perceptron with b = 0 and weights sampled fromidentical centered uniform distributions (note how similar (a) is to (d)!). We cut off frequencies lessthan 2 to eliminate finite size effects. In (a) and (b) lines were fitted using least-squares regression;for (c) the line corresponding to the ansatz in Equation (10) is plotted instead.
Figure 6: (a) Probability of functions versus their rank (ranked by probability) for a perceptronwith n = 5, and weights sampled i.i.d. from a Gaussian and no threshold bias term, acting oneither centered {-1, 1}5 or uncentered {0, 1}5 data. (b) Probability of functions versus their LZcomplexity for a perceptron with n = 6, and weights sampled i.i.d. from a Gaussian and no thresholdbias term, acting on the centered Boolean hypercube {-1, 1}5.
Figure 7: P(f) vs KLZ(f) at a selection of values of T (f), for a perceptron with input dimension7, weights sampled from N (0, 1) and no threshold bias terms. We observe a large range in KLZ(ft)for ft ∈ Ft which increases as t approaches 64, which is to be expected - for example the functionf = 0101 . . . is very simple and has maximum entropy, and we expect there to exist higher com-Plexity functions at higher entropy. Consistent with the bound in (Vane-Perez et al., 2018), simplerfunctions tend to have higher probabilities than more complex ones. The data-points at especiallyhigh probabilities in Figure 7d correspond to the function f = 0101 . . . and equivalent strings afterpermuting dimensions.
Figure 8: P(f) vs KLZ(f) at a selection of values of T (f), for a perceptron with input dimension7, weights sampled from a centered uniform distribution and no threshold bias terms. We compareto Figure 7, and observe that uniform sampling reduces slightly reduces the simplicity bias withinthe sets Ft (see Appendix E.2).
Figure 9: We assume that the signs to the right of those shown are all negative. We can list thevarious non-equivalent classes of functions and their conditions in a pictorial form. A condition atany node in the graph is also a condition for any daughter node - by way of example, we would readthe conditions oft = 4 for the sign arrangement + + -- (see Figure 9a) as ((a4 > a1 +a2) ∩ (a3 <a1 + a2)).
Figure 10: Transformed 2D Gaussian for proof in Lemma H.3.
Figure 11: Probability versus rank (ranked by probability) of different Boolean functions{-1, 1}7 → {-1, 1} produced by a neural network with 1 hidden layer with tanh activation,and weights distributed by a Gaussian with different variance hyperparameters chosen to lie in thechaotic (σb = 0.0) and ordered (σb = 10.0) regimes.
Figure 12: Probability of different values of T for a CNN with 4 layers, no pooling, and ReLUactivations. The parameters were sampled 105 times and the network was evaluated on a fixedrandom sample of 100 images from MNIST or CIFAR10. The parameters were sampled i.i.d. froma Gaussian, with paramaters σw = 1.0, and σb = 0.0, 1.0. The input images from CIFAR10 whereeither left uncentered (with values in range [0, 1]) (uncentered), or where centered by substractingthe mean value of every pixel (uncentered).
Figure 13: Probability of different values of T for the perceptron with n = 7 input neurons andσb = 0.0. The parameters were sampled 105 times and the perceptron was evaluated on a randomsubsample of size 64 of either {0, 1}n ((a)) or {-1, 1}n ((b)). The weights were sampled i.i.d. froma Gaussian, with paramaters σw = 1.0.
Figure 14: σb = 1.0Figure 15: Probability of different values of T for the perceptron evaluated on {-1, 1}n inputs andvarying σb = 0.0. The parameters were sampled 107 times. The weights were sampled i.i.d. from aGaussian, with paramaters σw = 1.0.
Figure 15: Probability of different values of T for the perceptron evaluated on {-1, 1}n inputs andvarying σb = 0.0. The parameters were sampled 107 times. The weights were sampled i.i.d. from aGaussian, with paramaters σw = 1.0.
Figure 16: Probabilities of finding functions by SGD versus by randomly sampling parameters(ABI), conditioned on 100% training accuracy, for a training set of size 32 for learning a Booleanfunction on {0, 1}7 with a fully connected neural network with layer widths (7,40,40,1). Boththe parameter sampling and SGD initialization where i.i.d. Gaussians with weight variances of1/(layer width), and bias variances of 1. SGD used a batch size of 8, and cross-entropy loss, andwas stopped as soon as 100% accuracy was reached. Histogram shows number of functions on eachbin, and is normalized row-wise, so that a value of 1.0 corresponds to the maximum number offunctions in the row, and 0.0 corresponds to 0.0 functions. These are mapped to colors from yellow(1.0) to purple (0.0) as shown in the colorbar.
Figure 17: Test accuracy, sensitivity (at 99% specificity), and hTi versus the shift hyperparameter(shift of the bias term in the last layer), for different architectures and datasets. The networkswere trained with SGD (batch size 32, cross entropy loss), until reaching 100% training accuracy.
