Figure 1: Schematic of the lift-the-flap task and human behavioral experiment. (a) The taskrequires subjects to capitalize on the scene context in a natural image to infer What is behind theblack box (the hidden target). The original image (bottom left) reveals the target object ("trafficlight”); this image was not shown in the actual experiments. (b) A blurred image with the hiddentarget was presented to the subject. To identify contextual areas of importance surrounding thehidden target, subjects used the computer mouse to click on image a pre-specified number of times(red dots). Upon clicking a certain location, a circle of fixed radius was revealed at high resolution.
Figure 2: Architecture overview of the ClickNet model. The diagram depicts the iterative modularsteps carried out by ClickNet for contextual reasoning over multiple clicks in the lift-the-flap task(Figure 1b). ClickNet consists of 3 main modules: feature extraction, attention, and recurrentmemory. For illustrative purposes, only the first and second clicks in a trial are shown here. ClickNetperforms feature extraction using the VGG16 network pre-trained on ImageNet and produces featuremaps a0 . Conditioned on the hidden state h0 and feature maps a0, ClickNet produces an attentionmap α0, which is used to select the next click location (red dots) and to modulate the feature maps forcontextual reasoning (Figure S1). The recurrent network in the LSTM module (Figure S2) integratesover time the attentionally modulated feature maps bz0, and outputs a predicted class label after eachclick (here “bird” and “trafficlight” before the 1st and 2nd clicks). After the first click, the inputimage gets updated with parts at clicked locations revealed in high resolution. These three modularsteps repeat until the specified number of clicks have been made. See supplementary figures S1and S2 for implementation details on the attention and LSTM modules, respectively.
Figure 3: Contextual reasoning accuracy of humans and models. Performance for humans(horizontal lines) and models (bars) for (a) 1 click (gray), and (b) 8 clicks (black) (Fig S4 showsresults for 2 and 4 clicks, and additional comparison models). Section 3.3 defines the evaluationmetric and Section 4.5 describe each model. Error bars denote SEM across images.
Figure 4: Example visualization for humans and ClickNet. Two example trials (first four columnsis example 1, last four columns is example 2), either With 1 click (rows 1) or 8 clicks (rows 2), forone human (columns 1, 2, 5, 6) or ClickNet (column 3, 4, 7, 8) (Fig S3 shows results for 2 and 4clicks). Red dots denote clicked locations. Top-left corner shows output labels after the requirednumber of clicks. Column 2 and 6 show the mouse click maps aggregated over subjects. Brighterregions denote more mouse clicks (see scale bar on right). Column 4 and 8 show the attentional mappredicted by ClickNet. Brighter regions denote higher attentional values.
Figure 5: Improvement in contextual reasoning accuracy with context-object ratio andpatterns of mistakes. (a) Partial view for illustrative purposes of confusion matrix showing themistakes made by ClickNet among 10 of the 80 object categories in MSCOCO(Fig S5 showsthe complete confusion matrix with all 80 categories). The element in row i, column j denotesthe probability that ClickNet predicted label j while the ground truth label was i (see scale bar atright). The sum of probabilities in a row in the full confusion matrix (but not here) equals 1. (b)Human (circle) and model (triangle) accuracy for 1 click (gray) and 8 clicks (black) as a function ofcontext-object ratio, shown in logarithmic scale (Sec. 3.3). Right: 3 example images with differentcontext-object ratios. Only the images on the first column were shown (the second column is shownhere for illustrative purposes only). Error bars indicate SEM across images.
Figure 6: Model click locations were similar to human sampling. (a-b) Consistency ofclick patterns between human subjects (human-human), consistency between humans and model(human-model), consistency between humans and random (human-random) and consistencybetween models and random (model-random) for 1 click (a) and 8 clicks (b) measured by thedistribution of normalized Euclidean distances with respect to the diagonal of the image betweenany pairs of clicks by humans and ClickNet or random clicks. In each trial, we permute the sequenceof mouse clicks between pairs of human and model clicks such that their sum of Euclidean distanceis minimized across all clicks. The white circles denote the median of the distribution and the lightgrey bar denote the 1st and 3rd quartiles. (b) Euclidean Distance between click locations and centerof the hidden target bounding box normalized by the diagonal of the hidden target bounding box.
Figure S1:	Schematic illustration of the attention module implementation. Expanding on theoverall ClickNet architecture shown in Fig 2, here We zoom into the attention module. The attentionmodule takes as inputs the features at each location a〃 and the output of the LSTM module htand selects the next click location mt and a map that modulates the features at each location (seeSection 4 for a description of all the variables).
Figure S2:	Schematic illustration of the LSTM module implementation. Expanding on theoverall ClickNet archietcture shown in Fig 2, here we zoom into the LSTM module. The LSTMmodule takes as input context gist vector bzt and integrates the information with the previous stateto inform the attention module in the next time step via ht and to predict a class label (see Section 4for a description of all the variables).
Figure S3:	Example Visualziation for humans and ClickNet. This figure shows click locationsand attention maps using the same format as Fig 4, here adding results for 2 clicks and 4 clicks.
Figure S4:	Contextual reasoning accuracy of humans and models. Expanding on the results inFig. 3a-b, here we add the results for 2 clicks and 4 clicks, as well as additional comparative models(Section 4.5, Section A and Section B Section D) describe each model).
Figure S5:	Confusion matrix for ClickNet with all click conditions. The format is the same asin Figure 5a, except showing all 80 categories here. The element in row i, column j denotes theprobability that ClickNet predicted label j while the ground truth label was i (see scale bar on right).
Figure S6:	Image-by-image consistency in the spatiotemporal pattern of click sequences acrossdifferent number of clicks. CLick sequence score is originally defined in evaluating eye fixationsequence consistency Madsen et al. (2012); Zhang et al. (2018). Here, we use the same metricsto compare the click sequences between-subjects and between ClickNet and subjects for 2 (lightgray), 4 (dark gray), and 8 (black) clicks. The larger the click sequence score, the more similar thesequences are. The dashed line indicates chance performance, obtained by randomly permuting theclicks among images. Results shown here are averaged over subject pairs.
Figure S7:	Example images from congruent and incongruent context. (a) congruent example: afork (from another image) pasted in a dining scene; (b) incongruent example: a knife (from anotherimage) pasted in a ski field. Red bounding boxes indicate target object locations.
