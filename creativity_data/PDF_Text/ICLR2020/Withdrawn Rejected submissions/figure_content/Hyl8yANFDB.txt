Figure 1: Supervised learning on Atari: Gain as a function of distance in the replay buffer from theupdate sample. We use dotted lines for the point at 0 distance, to emphasize that the correspondingstate was used for the update. (a-b) The curve around 0 indicates the temporal structure captured bythe TD and regression objectives.
Figure 2: Policy evaluation on Atari: Gain as a function of distance in the replay buffer of the updatesample. (a) We use dotted lines for the point at 0 distance to emphasize that the corresponding statewas used for the update. (a-b) Compared to regression in Fig. 1a, almost no temporal structure iscaptured, which can be seen by how narrow the curve is around distance 0.
Figure 3: Policy evaluation on Atari: evolution of the distribution of ∆, the difference since lastvisit, during training. In (a) the DNN is forced to memorize, as such the density of ∆ is concentratedaround 0 (thin red/white band). In (b-c), Q-Learning and Sarsa, the density is much less peaked at 0(larger yellow/green bands) as the DNN learns about states without visiting them. In (d) the DNNlearns quickly presumably without memorizing (the distribution of ∆ is more spread out and not asconcentrated around 0, seen by the larger yellow/green band), as it is trained on Monte-Carlo returns,and quickly converges as can be seen by the high density of positive ∆s early. In (e,f) we see theeffect of using λ returns (see appendix A.6 for all values of λ).
Figure 4: Q-Learning on Atari: Gain as a function of distance in the replay buffer of the updatesample. (a-b) Compared to policy evaluation, gain appears to be better, but not as large as forregression. (b) For Adam and RMSProp, we include the corresponding curves for policy evaluationin lighter shades.
Figure 5: TD gain for policy evaluation withTD(λ) & Adam. Note the larger gain as λ goes to1, as well as the asymmetry around 0.
Figure 6: Episodic rewards over Q-Learning onAtari. We train an agent while witholding statesfrom the training set with probability p.
Figure 7: TD gain as a function of other metrics. Shaded regions are the standard error to the mean.
Figure 8: Policy evaluation with LQL , TD gain as a function of cosine distance in hidden space(second to last layer).
Figure 9: Spread of singular values after 100k iterations. Despite having seen roughly the sameamount of data, the control experiments generally have seen fewer unique states, which may explainthe observed difference. Shaded regions show bootstrapped 95% confidence intervals.
Figure 10:	Spread of singular values after 500k iterations. Shaded regions show bootstrapped 95%confidence intervals.
Figure 11:	Evolution of TD gain, YTnDear during training. Control experiment with Adam, MsPacman,averaged over 10 runs. Note that index 0 is excluded as its magnitude would be too large and dim allother values.
Figure 12: Measuring the cosine similarity in a sequence. The y axis shows the cosine similaritybetween VθLT D(λ)(St) and VθLTD(λ)(St+k) for the nearest k ∈ [-30, 30] neighbours in the replaybuffer. The larger λ is, the more similar gradient updates are for two states when those states areclose in time. Shaded regions show bootstrapped 95% confidence intervals.
Figure 13: Measuring the distribution of cosine similarity. The y axis shows the density of a givencosine similarity (X axis) between VθLτD(λ)(Si) and VθLτD(λ)(Sj) for 20k (i,j) pairs in thereplay buffer. The distributions for positive cosine similarities are similar for all values of λ. On theother hand, on the left side of the plot we can see that when λ gets closer to 0, there are more andmore gradients “pointing the wrong way”, i.e. with a negative cosine similarity. The labels show theexpected cosine similarity over the replay buffer (E = ...), which show that as λ increases, gradientsare more similar. Interestingly there are almost no pairs of states with a cosine similarity of 0.
Figure 15: The lifetime agent reward (i.e. the AUC of a curve as in Figure 6) as a function of thelifetime average near TD gain YTnDear . The line is a linear regression of the points, with a correlationcoefficient of r = 0.4337. We vary the capacity of the agent by changing the number of hidden unitsof every layer, which is reflected by the size of the circles.
