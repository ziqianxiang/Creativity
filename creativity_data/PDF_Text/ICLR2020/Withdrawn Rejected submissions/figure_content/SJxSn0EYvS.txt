Figure 1: The first raw are the confidence map predictions of the RNN, where each person i is a sumof the J confidence maps predicted at iteration i. Sk = i indicates Hk is matched with Hi. S = {s}ris the assignment of the GT heatmaps, which appear in some arbitrary order. In this example, Z = 8and Z = 6, hence s7,8 = -1.
Figure 2: An example output of a single LSTM iteration, in which J heatmaps are predicted for asingle person. We overlay them as is on the image, and sum them up for the last image to showcorrespondence for all the joints of the person.
Figure 3: Overview of the approach. AE network predicts joint heatmaps collectively. It followsthis stage by a heuristic-based association using the embeddings. Instead, our approach estimatesan individual human pose directly in every iteration t. The input to the first ConvLSTM is a con-catenation of the current hidden state from the previous iteration, the intermediate features extractedfrom the AE network, and the LSTM prediction of the individual heatmaps using the feedback loop.
Figure 4: Showing the effect of the feedback loop. As expected, it helped in suppressing wrongpeaks in its confidence map. This can be observed in top left photo, where the person in the back isnot distinctly detected from the person in the front.
Figure 5: Qualitative results from COCO in the first row, and OCHuman in the second row[addexamples from OCHuman. maybe also show results of the other approachespose skeleton features obtained by a BU approach should be used, instead of the reverse directionemployed in TD approaches, where bounding-box proposals of two heavily overlapped people willlikely result in eliminating one of them when applying NMS.
Figure 6: Exam-ining the effect ofthe stop thresh-old value duringtraining.
Figure 7: Qualitative comparison between Associative Embedding and our learning-based approach.
Figure 8: Qualitative results on OCHuman including some failure cases such as in the last row wherethe poses are mixed or slightly overestimated such as in the third row.
