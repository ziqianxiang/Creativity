Figure 1: Overview of the proposed method. The input is an initial image and an action in the formof language instruction. The output is a generated image that depicts the effect of the performedaction. The model needs to, first of all, correctly identify the object that should be manipulated and,thereupon, place the object in the correct location by understanding relational concepts.
Figure 2: Examples of input (left) and output (right) image for each of the four actions remove,replace, add, and move.
Figure 3: Detailed overview of the proposed method. A CNN-based encoder is used to represent theinput image X and the last output from a LSTM is used as representation of the input sentence s. Arelational network combines these two representations into a shared representation,夕(X, s). Thegenerated output image is obtained with an image decoder.
Figure 4: Integration of the model with a GAN. The discriminator is conditioned on the sharedrepresentation of the input image and sentence.
Figure 5: Results from two models. Five input images (columns) for each of the four actions(a-d). The first row in each sub-figure is the input image data and the fourth row is the targetoutput image. The second and third row is the generated output from the CAE+LSTM+RN andCAE+LSTM+RN+GAN model, respectively. The action sentence can be inferred by comparing thedifference between the first and fourth column.
Figure 6: Real-world images with four action sentences. The action sentences where: ”move the redsmall cube left of red big sphere”, ”remove the blue big pyramid”, ”replace the green small cube witha blue big pyramid”, and ”add a green small cube on top of red big sphere”.
Figure 7: Pre-processing of real-world images. (left) Original real-world image with 6 objects.
Figure 8: Results on real-world images with pretrained CAE-LSTM-RN-GAN model on simulatedimages. First row show the pre-processed input image, second row show the generate output image,and third row show the target image.
