Figure 1: Multiple levels of semantics extracted from two synthesized scenes.
Figure 2: Method for identifying the emergent variation factors in generative representation. By deploying abroad set of off-the-shelf  image classifiers as scoring functions, F ( ), we are able to assign a synthesized imagewith semantic scores corresponding to each candidate variation factor.  For a particular concept, we learn adecision boundary in the latent space by considering it as a binary classification task. Then we move the sampledlatent code towards the boundary to see how the semantic varies in the synthesis, and use a re-scoring techniqueto quantitatively verify the emergence of the target concept.
Figure 3: (a) Four levels of visual abstractions emerge at different layers of StyleGAN. Vertical axis shows thenormalized perturbation score ∆si. (b) User study on how different layers correspond to variation factors fromdifferent abstraction levels.  (c) Layer-wise manipulation result.  The first column is the original synthesizedimages, and the other columns are the manipulated images at layers from four different stages respectively. Blueboxes highlight the results from varying the latent code at the most proper layers for the target concept.
Figure 4: (a) Independent attribute manipulation results on high layers. The middle row are the source images.
Figure 5: Objects are transformed by GAN to represent different scene categories. On the top shows that theobject segmentation mask varies when manipulating a living room to bedroom, and further to dining room. Onthe bottom visualizes the object mapping that appears during category transition, where pixels are counted onlyfrom object level instead of instance level. GAN is able to learn shared objects as well as the transformation ofobjects with similar appearance when trained to synthesize scene images from more than one category.
Figure 6: Comparison of the top scene attributes identified in the generative representations learned by StyleGANmodels for synthesizing different scenes. Vertical axis shows the perturbation score ∆si.
Figure 7: Manipulation results on StyleGAN models trained for synthesizing different scenes. For each triple,on top shows the target attribute, the first image is the source image, the other two images are generated byincreasing the manipulation magnitude. Please see the demo video for continuous manipulation via this link.
Figure 8: Effects on scene attributes (already sorted) when varying a particular variation factor (in red color).
Figure 9: (a) Some variation factors identified from PGGAN (bedroom). (b) Layer-wise analysis on BigGANfrom attribute level.
Figure 10: The definition of layout for indoor scenes. Green lines represent for the outline prediction from thelayout estimator.  The dashed line indicates the horizontal center, and the red point is the center point of theintersection line of two walls. The relative position between the vertical line and the center point is used to splitthe dataset. For example, image on the left is treated as positive sample, while the one on the right is treated asnegative sample.
Figure 11: Samples for training decision boundary with respect to layout, scene category, and various sceneattributes.
Figure 12: Ablation study on the proposed re-scoring technique with StyleGAN model for bedroom synthesis.
Figure 13: Layout and category manipulation results.
Figure 14: Manipulating the attributes of indoor scenes at different scores (low to high).
Figure 15: Manipulating the attributes of outdoor scenes at different scores (low to high).
Figure 16: Independent and joint manipulation results.
Figure 17: Generator architectures of StyleGAN (Karras et al. (2018b)) and BigGAN (Brock et al. (2019)), bothof which introduces layer-wise stochasticity (i.e., latent codes are fed into all convolutional layers instead ofonly the first layer). Note that the diagrams are borrowed from the original papers.
Figure 18: Comparison results between manipulating latent codes at only upper (attribute-relevant) layers andmanipulating latent codes at all layers with respect to indoor lighting on StyleGAN.
Figure 19: Manipulation at the bottom layers in 4 different directions, including layout, category, indoor lighting,and color scheme on StyleGAN.
