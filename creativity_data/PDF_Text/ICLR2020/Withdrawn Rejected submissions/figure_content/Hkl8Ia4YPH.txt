Figure 1: Illustration of the proposed hierarchical generation model for SUmmary-to-article generation:(a) The conventional seq2seq model generates an article directly based on the summary; (b) Ourproposed hierarchical summary-to-article generation approach first generates a sketch based on thesummary and then completes the article based on the generated sketch.
Figure 2: Overview of the proposed hierarchical SUmmary-to-article generation model.
Figure 3: Illustration of the proposed training strategy: (a) MARL: An end-to-end joint trainingframework for hierarchical summary-to-article generation, G1 and G2 are updated during MARLwhile G3 is fixed.; (b) denoising seq2seq pre-training for the sketch-to-article model G2 to improvethe robustness of G2 against the noise in the generated sketch.
Figure 4:	Sample generated by our model on the CNN / Daily Mail dataset.
Figure 5:	Sample generated by baseline model on the CNN / Daily Mail dataset. We highlightrepetitive parts of generated article in red.
Figure 6:	Sample generated by our model on the CNN / Daily Mail dataset.
Figure 7:	Sample generated by baseline model on the CNN / Daily Mail dataset. We highlightrepetitive parts of generated article in red.
Figure 8:	Sample generated by our model on the B igPatent dataset.
Figure 9:	Sample generated by baseline model on the B igPatent dataset. We highlight repetitiveparts of generated article in red.
Figure 10:	Sample generated by our model on the BigPatent dataset.
Figure 11:	Sample generated by baseline model on the B igPatent dataset. We highlight repetitiveparts of generated article in red.
