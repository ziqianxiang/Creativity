Figure 1: (a) Generative model (blue lines represent generative model parameters). (b) Inferencemodel used in StackedWAE. (c) Standard WAE; the generative model has only one latent withprior p(z1) = p(z1 |z2)p(z2|z3)p(z3)dz1dz2dz3. (d) Inference model with skips connections andparameter sharing with the generative model, as in S0nderby et al. (2016).
Figure 2: 5-layer StackedWAE. (a) Training curves; each term in Eq. (10) is shown throughouttraining. (b) Model samples. (c) Z5 latent-space interpolations.
Figure 3:	5-layer STAcKEDWAE. (a) Visualisations of latent spaces Zi. Each colour correspondsto a digit label. dZ5 = 2 can be directly plotted; for higher dimensions We use UMAP (McInnes &Healy, 2018) to plot a two dimensional representation. (b) Reconstructions for different encodinglayers. The bottom row is data; the ith row from the bottom is generated using the latent codes ziwhich are from the ith encoding layer.
Figure 4:	1-layer implicit-prior WAE. (a) Reconstructions (within pairs of rows, data is above withthe corresponding reconstructions below). (b) Model samples. (c) Z5 latent-space interpolations.
Figure 5: 6-layer StackedWAE. (a) Model samples. (b) Points interpolations; the first andlast columns are actual data points. (C) Visualisations of latent spaces Zi, as in Figure 3a.(d)Reconstructions for different encoding layers, as in Figure 3b.
Figure 6: (a) Residual network with 3 hidden convolutions. (b) Details of the architecture use inSection 3.2.
