Figure 1: Neural Network Architectures. The trained layer is in bold face. The activation functionafter the trained parameters is x2(blue neurons). The activation function before the trained parame-ters is xp (purple neurons).
Figure 2: Training loss for random sample experiment(a) Two-layer network with perturbation on input (b) Three-layer network on top 100 PCA direc-tionsFigure 3: MNIST with original labelMNIST with random label We further test our results on MNIST with random labels to verifythat our result does not use any potential structure in the MNIST datasets. The setting is exactly thesame as before. As shown in Figure 4, the training loss can also converge.
Figure 3: MNIST with original labelMNIST with random label We further test our results on MNIST with random labels to verifythat our result does not use any potential structure in the MNIST datasets. The setting is exactly thesame as before. As shown in Figure 4, the training loss can also converge.
Figure 4: MNIST with random label6	ConclusionIn this paper, we showed that even a mildly overparametrized neural network can be trained tomemorize the training set efficiently. The number of neurons and parameters in our results are tight(up to constant factors) and matches the bounds in Yun et al. (2018). There are several immediateopen problems, including generalizing our result to more standard activation functions and providinggeneralization guarantees. More importantly, we believe that the mildly overparametrized regime ismore realistic and interesting compared to the highly overparametrized regime. We hope this workwould be a first step towards understanding the mildly overparametrized regime for deep learning.
Figure 5: Synthetic ExampleTraining lossFigure 6: Two-layer network on original MNISTmuch harder to correctly optimize for those directions. In Figure 7, after adding the perturbationthe smallest singular value of the matrix X becomes better, and as we can see the loss decreasesgeometrically to a very small value (< 1e - 5).
Figure 6: Two-layer network on original MNISTmuch harder to correctly optimize for those directions. In Figure 7, after adding the perturbationthe smallest singular value of the matrix X becomes better, and as we can see the loss decreasesgeometrically to a very small value (< 1e - 5).
Figure 7: Two-layer network on MNIST, with noise std 0.01(S5cÎ·)601Figure 8: Three-layer network with top 100 PCA directions of MNIST, 0.05 noise per directionMNIST with random label When we try to fit random labels, the original MNIST input does notwork well. We believe this is again because there are many small singular values for the matrix X inTheorem 4, so the data does not have enough effective dimensions fit random labels. The reason thatit was still able to fit the original labels to some extent (as in Figure 6) is likely because the originallabel is correlated with some features of the input, so the original label is less likely to fall into thesubspace with smaller singular values. Similar phenomenon was found in Arora et al. (2019b).
Figure 8: Three-layer network with top 100 PCA directions of MNIST, 0.05 noise per directionMNIST with random label When we try to fit random labels, the original MNIST input does notwork well. We believe this is again because there are many small singular values for the matrix X inTheorem 4, so the data does not have enough effective dimensions fit random labels. The reason thatit was still able to fit the original labels to some extent (as in Figure 6) is likely because the originallabel is correlated with some features of the input, so the original label is less likely to fall into thesubspace with smaller singular values. Similar phenomenon was found in Arora et al. (2019b).
Figure 9: Two-layer network on MNIST, with noise std 0.01, random labelsFigure 10: Three-layer network with top 100 PCA directions of MNIST, 0.05 noise per direction,random labelsB Detailed Description of Perturbed Gradient DescentIn this section we give the pseudo-code of the Perturbed Gradient Descent algorithm as in Jin et al.
Figure 10: Three-layer network with top 100 PCA directions of MNIST, 0.05 noise per direction,random labelsB Detailed Description of Perturbed Gradient DescentIn this section we give the pseudo-code of the Perturbed Gradient Descent algorithm as in Jin et al.
