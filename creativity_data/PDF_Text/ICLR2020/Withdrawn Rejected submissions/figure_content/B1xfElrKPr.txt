Figure 1: A simplified illustration of our TP-Attention mechanism for one head at position t inlayer l. The main difference from standard Attention is the additional role representation that iselement-wise multiplied with the filler/value representation.
Figure 2: The accuracies of our implementation of the Transformer (700k steps) and the TP-Transformer (700k and 1.7M steps) for every module of the Mathematics Dataset.
Figure 3: Samples of correctly processed problems from the arithmetiC-Jnixed module. ‘#‘ and‘%’ are the start- and end-of-sentence symbols. The colored squares indicate the k-means clusterof the role-vector assigned by one head in the final layer in that position. Blue and gold rectanglesrespectively highlight numerator and denominator roles. They were discovered manually. Note howtheir placement is correctly swapped in rows 2, 3, and 4, where a number in the denominator of adenominator is treated as if in a numerator. Role-cluster 9 corresponds to the role ones-digit-of-a-numerator-factor, and 6 to ones-digit-of-a-denominator-factor; other such roles are also evident.
Figure 4: TP-Transformer attention maps for three examples as described in section 5.2.
Figure 5: The best-so-far smoothed negative log-likelihood of word-pieces on held-out data through-out training.
