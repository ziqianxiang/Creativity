Figure 1:	An illustrative example showing the advantage of our Self-Repulsive Langevin dynamics.
Figure 2:	Roadmap of the theoretical analysis. Theorem 4.3 shows the mean-field limit when M → ∞.
Figure 3: Sample quality of SRLD and Langevin dynamics for sampling the correlated 2D distribu-tion. The auto-correlation is the averaged auto-correlation of the two dimensions.
Figure 4: Sampling trajectory of the correlated 2D distribution.
Figure 5:	Sample quality and autocorrelation of the mixture distribution. The auto-correlation is theaveraged auto-correlation of the two dimensions.
Figure 6:	Sampling trajectory of the mixture of Gaussian.
Figure 7:	Sample quality and autocorrelation of the higher dimensional Gaussian distribution. Theauto-correlation is the averaged auto-correlation of all dimensions.
Figure 8: Sample quality and autocorrelation of the higher dimensional mixture distribution. Theauto-correlation is the averaged auto-correlation of all dimensions.
Figure 9:	Comparison between SRLD and Langevin dynamics on test RMSE. The results are com-puted based on 20 repeats. The error bar is calculated based on RMSE of SRLD - RMSE of Langevindynamics in 20 repeats to rule out the variance of different data splittingA.5 Contextual BanditContextual bandit is a class of online learning problems that can be viewed as a simple reinforcementlearning problem without transition. For a completely understanding of contextual bandit problems,we refer the readers to the Chapter 4 of (Bubeck et al., 2012). Here we include the main idea forcompleteness. In contextual bandit problems, the agent needs to find out the best action given someobserved context (a.k.a the optimal policy in reinforcement learning). Formally, we define S asthe context set and K as the number of action. Then we can concretely describe the contextualbandit problems as follows: for each time-step t = 1,2, ∙∙∙ ,N, where N is some pre-defined timehorizon (and can be given to the agent), the environment provides a context st ∈ S to the agent,then the agent should choose one action at ∈ {1,2,…，K} based on context st. The environmentwill return a (stochastic) reward r(st, at) to the agent based on the context st and the action at thatsimilar to the reinforcement learning setting. And notice that, the agent can adjust the strategy ateach time-step, so that this kinds of problems are called “online” learning problem.
Figure 10:	Comparison between SRLD and Langevin dynamics on test log-likelihood. The resultsare computed based on 20 repeats. The error bar is calculated based on log-likelihood of SRLD -log-likelihood of Langevin dynamics in 20 repeats to rule out the variance of data splitting.
Figure 11:	Visualization of the wheel bandit (δ = 0.95), taken from (Riquelme et al., 2018).
