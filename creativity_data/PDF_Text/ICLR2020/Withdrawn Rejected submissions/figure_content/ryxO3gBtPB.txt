Figure 1: We distill the knowledge of tens of thousands of images into a few synthetic training images calleddistilled images. On MNIST, 100 distilled images can train a standard LENET with a random initialization to89% test accuracy, compared to 99% when fully trained. On CIFAR10, 100 distilled images can train a networkwith a random initialization to 41% test accuracy, compared to 80% when fully trained. In Section 3.6, we showthat these distilled images can efficiently store knowledge of previous tasks for continual learning.
Figure 2: Distilled images trained for random initialization a batch of 100 distilled images (ten per class). Only30 of 100 distilled images are shown here. Please see the appendix for the full result.
Figure 3: Distillation performance with varying numbers of GD steps and a fixed number of distilled images.
Figure 4: Dataset distillation for random initializations on MNIST. This batch of 100 distilled images arerepeatedly applied in 2000 GD steps.. These images train average test accuracy to 88.51% ± 1.11%.
Figure 5: Dataset distillation for random initializations on CIFAR10. This batch of 100 distilled images arerepeatedly applied in 50 GD steps. These images train average test accuracy to 41.23% ± 0.88%.
Figure 6: Dataset distillation for adapting random pretrained models from USPS to MNIST. 100 distilled imagesare split into 10 GD steps, shown as 10 rows here. Top row is the earliest GD step, and bottom row is the last.
Figure 7: Dataset distillation for adapting random pretrained models from MNIST to USPS. 100 distilled imagesare split into 10 GD steps, shown as 10 rows here. Top row is the earliest GD step, and bottom row is the last.
Figure 8: Dataset distillation for adapting random pretrained models from SVHN to MNIST. 100 distilledimages are split into 10 GD steps, shown as 10 rows here. Top row is the earliest GD step, and bottom row isthe last. The 10 steps are iterated over three times to finish adaptation, leading to a total of 30 GD steps. Theseimages train average test accuracy on 200 held out models from 51.64% ± 2.77% to 85.21% ± 4.73%.
