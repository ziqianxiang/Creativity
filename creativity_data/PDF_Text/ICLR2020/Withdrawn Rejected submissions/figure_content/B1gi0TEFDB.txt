Figure 1: Convergence comparison between original distributed SGD (Dense-SGD), Topk spar-sification (TopK-SGD) and Randk sparsification (RandK-SGD) at 16 distributed workers on theCIFAR10 (Krizhevsky et al., 2010) and ImageNet (Deng et al., 2009) data sets. k = 0.001d forTopK-SGD and RandK-SGD.
Figure 2: The hiStogramS of ut1 of TopK-SGD. For each model, the gradient hiStogramS are plottedevery 200 iterationS from iteration 200 to 1600 (other iterationS have Similar ShapeS).
Figure 3: The shaPe of π(2i) with different i with d =(c) Illustrated areas100, 000 and σ = 1.
Figure 4: The GPU comPutation time (lower isbetter) ofToPk, DGCk and Gaussiank. We use thePyTorch tensor API, “tensor.toPk()”, for the ToPkoPerator.
Figure 5:	The comparison of bounds with a range of k .
Figure 6:	The convergence performance (top-1 validation accuracy) of distributed SGD withGaussianK-SGD using k = 0.001d compared to TopK-SGD and Dense-SGD on 16 workers.
Figure 7:	The cumulative distribution of Ut during the TopK-SGD training process.
Figure 8:	The histograms of ut1 during the Dense-SGD training process.
Figure 9:	The histograms of ut1 during the GaussianK-SGD training process.
Figure 10:	Number of communicated gradients vs. accuracy. k = 0.001dOur proposed Gaussiank operator could under- or over- sparsify the gradients, which makes the num-ber of selected gradients is larger or smaller than k. To demonstrate the sensitivity of GaussianK-SGD to the configured k, we first evaluate the accumulated number of communicated gradientsover the training process, which is shown in Fig. 10. It is seen that at the first several epochs,13Under review as a conference paper at ICLR 2020our GaussianK-SGD under-sparsifies the gradients (requires higher communication overheads), andafter that, GaussianK-SGD over-sparsifies the gradients (requires lower communication overheads)with little loss of accuracy.
Figure 11:	Sensitivity of GaussianK-SGD using kto Dense-SGD on 16 workers.
