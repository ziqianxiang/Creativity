Figure 1: 3D-SIC takes an RGB-D scan as input and predicts its semantic instance completion:from the scanâ€™s color images and geometry (encoded as a TSDF), objects in the observed scene aredetected (as 3D bounding boxes and class labels) and for each object, the complete geometry of thatobject is predicted as per-instance masks (in both seen and unseen regions).
Figure 2: Our 3D-SIC network architecture takes an RGB-D scan as input. Color images are pro-cessed with 2D convolutions to spatially compress the information before back-projecting into 3D,to be merged with the 3D geometry features of the scan (following Dai & NieBner (2018); HoU et al.
Figure 3: Qualitative results on real-world ScanNet (Dai et al., 2017a) scenes, with close-ups shownon the right. Note that different colors denote distinct object instances in the visualization. Our ap-proach effectively predicts complete individual object geometry, including missing structural com-ponents (e.g., missing chair legs), across varying degrees of partialness in input scan observations.
