Figure 1: Training (a) a base network /e(∙; θ, ∙), (b) modulators /e(∙; ∙, α/, (C) a selection networkfs (∙; φ).(xm: an example, ym: a label, ym: a prediction, Zm: an embedded vector of a support (n=s)or query (n=q) set in a domain m.)2	Methods2.1	Problem statementWe follow the common setting of few-shot classification in the meta-learning community (Vinyalset al., 2016). For a N -way, K-shot classification task, an episode which consists of a support setS = {(xis,yis)}iN=K1 and a query set Q = {(xiq,yiq)}iT=1 is sampled from a given dataset, wherexis, xiq, yis and yiq represent examples and their correct labels respectively and T is the number ofquery examples. Once a model has been trained with respect to a number of random episodes atmeta-training time, it is expected to predict a correct label for an unlabeled query given only a fewlabeled examples (i.e., support set) even if all these came from classes which have never appearedduring meta-training.
Figure 2: Architecture of one embedding model /e (∙; θ,ɑ∕.
Figure 3: Inference (a) with one model chosen by the selection network fs(∙; φ) (b) by averagingoutput probabilities from all constituent models in the pool.
Figure 4: Test accuracy on various seen domains. (Accuracy values are shown in Table A5.)(b) 5-way 5-shot(a) 5-way 1-shotFigure 5: Test accuracy on various unseen domains. (Accuracy values are shown in Table A6.)implication is that the best single model might be better than the averaging approach if a model fromthe same domain exists. It is also worth noting that DoS-Ch is quite competitive despite much lessnumber of parameters than DoS.
Figure 5: Test accuracy on various unseen domains. (Accuracy values are shown in Table A6.)implication is that the best single model might be better than the averaging approach if a model fromthe same domain exists. It is also worth noting that DoS-Ch is quite competitive despite much lessnumber of parameters than DoS.
Figure A1: Contributions of individual models in model averaging methods.
