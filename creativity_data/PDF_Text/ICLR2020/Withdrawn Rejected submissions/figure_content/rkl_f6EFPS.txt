Figure 1: Discrete (standard) neural network approximating a continuous networkAssumption 3. As the width n increases, networks NNn have a continuous limit (39) NNn → NNc,where NNc is a continuous neural network (19), and n = min{nl}. That network NNc has globallybounded operator derivatives Dk for orders k = 1, 2. We define D12 = max{D1 , D2}.6 7See Figure 1 for a visualization of A3 and Table 1 for the description of A3. The assumption meansthat with the increase of n, the network uses the same internal structure which just becomes morefine-grained. The continuous limit holds in the case of explicit duplication, convolutional networksand corresponding explicit regularization. The supplementary material contains a more completeexplanation.
Figure 2: The effect of the layer width nl, l = 1, horizontal axis on the variance of the fault toleranceerror Var∆, vertical axis7 Experimental EvaluationIn this section, we test the theory developed in previous sections in proof-of-concept experi-ments. We first show that we can correctly estimate the first moments of the fault toleranceusing T1 for small (10-50 neurons) and larger networks (VGG). We test the predictions ofour theory such as decay of Var∆, the effect of our regularizer and the guarantee from Al-gorithm 1. See the supplementary material for the technical details where we validate the as-sumption of derivative decay (A3) explicitly. Our code is provided at the anonymized repositorygithub.com/iclr-2020-fault-tolerance/code.
Figure 1: Discrete (standard) neural network approximating a continuous networkAssumption 3. As the width n increases, networks NNn have a continuous limit (39) NNn → NNc,where NNc is a continuous neural network (19), and n = min{nl}. That network NNc has globallybounded operator derivatives Dk for orders k = 1, 2. We define D12 = max{D1 , D2}.6 7See Figure 1 for a visualization of A3 and Table 1 for the description of A3. The assumption meansthat with the increase of n, the network uses the same internal structure which just becomes morefine-grained. The continuous limit holds in the case of explicit duplication, convolutional networksand corresponding explicit regularization. The supplementary material contains a more completeexplanation.
Figure 2: Non-regularized MNIST (first two rows), regularized MNIST (second two rows). Odd rows show thecharts of the decay of first derivatives (D) and second derivatives (average over all i, j Hessian’s components |Hij |,and only the diagonal elements |Hii |). Next, validation accuracy is shown as well is the product of the infinitynorms of the weights ∣∣Wl∣∣∞ •…∙∣∣Wi ∣∣∞. Even rows show first-layer weights as imagesW prod1250We assume kxk∞ ≤ 1 (otherwise we rescale W1 ). We group the terms into distinct cases i = j and i 6= j :E∆ = -p∆0(0) +	E(ξi)2∆00(t(ξ)) +	Eξi∆00(t(ξ))ξji=jV------------E1i6=jJ X---------------E2}370371372373374
Figure 3: Non-regularized Fashion MNIST (first two rows), regularized Fashion MNIST (second two rows). De-scription is the same as in Fig. 2H diag500	1000	1500nval_lossW prodSSOzle>Figure 4: Non-regularized Boston Housing datasetconsider the expression for the variance and explicitly compute a correlation between ξi∆0i(t(ξ)) and ξj∆0j (t(ξ)).
Figure 4: Non-regularized Boston Housing datasetconsider the expression for the variance and explicitly compute a correlation between ξi∆0i(t(ξ)) and ξj∆0j (t(ξ)).
Figure 5: Boston-trained networks. Rank loss (worst value 0.5) of ordering networks using bounds (left 4) andrelative error in predicting the value of ∆ (right 4). Mean of the error E∆ (top) and standard deviation (bottom).
Figure 6: Random networks. Rank loss (worst value 0.5) of ordering networks using bounds (left 4) and relativeerror in predicting the value of ∆ (right 4). Mean of the error E∆ (top) and standard deviation (bottom). Variableinput on a fixed network (1st and 3rd columns) and variable network with fixed input (2nd and 4th columns).
Figure 7:	The distribution ∆ of the error in the output for the network obtained using Algorithm 1 from the mainpaper20g -eulπdE山0.30.20.10.0-1 01234567Algo iteration and stage (color)0.0004----Experiment---Theorytŋ0.0002 >,0.00008Figure 8:	Evolution of the experimental error probability and the variance of the error over algorithm’s iterations.
Figure 8:	Evolution of the experimental error probability and the variance of the error over algorithm’s iterations.
Figure 9: Comparison of networks trained with different dropout. Shown: accuracy and Var∆ plots for differenttrain dropout probability. Green curve shows accuracy of correct network, red shows accuracy for the crashingnetwork. Dashed line show train dataset and solid represent test dataset. Variance of ∆ estimated by b4 shown inorange, by b3 in blue. Error bars are standard deviations in 10 repetitions of the experiment.
Figure 10: Horizontal axis represents increasing regularization parameter. Left vertical axis represents accuracyand has test crashing network accuracy (red) and correct network accuracy (green). Right axis shows variance ofthe error Var∆ estimated by bound b3 used as a regularizer. Error bars are standard deviations in 5 repetitions ofthe experiment.
Figure 11: Distribution of relative error in the predicted ∆ in percent for 25 subset pairs. Distribution for the meanis shown in blue, for the variance in orange.
Figure 12: Decay with training of the loss (blue line, left vertical axis) and the error ExdyEξ△ (red, right axis)together with bounds b3 and b4 predictions (red and brown curves) and experimental values (dots). ”Th8” meansAC2Since Zi = P Wikxk, ;Izi = Wij. Therefore We plug that in:kjAnd multiply with xj :∂xj Xj=X ∂i Wij XjNow we compute IW- using that in vector Z only Zi depends on Wij and also that Z = Wx:∂y ∂η ∂zi ∂η∂Wij = ∂Zi ∂Wij = ∂ZiXjThen we multiply it by Wij and sum over i:X ∂⅜ Wij = X ∂η XjWijii545 And now we note that the expressions for 孕-Xj and P Jy^r Wij are exactly the same.
