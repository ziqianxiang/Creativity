Figure 1: Left: An illustration of the ROS-HPL framework. The robot puts α of its effort in maxi-mizing the intrinsic rewards and achieving the sub-goal, and 1 - α of its effort in maximizing theextrinsic rewards and achieving the goal. Right: Trajectory generated from our method (α = 0.8)for searching the target object music player under our SETTING-STSE.
Figure 2: Network architecture of our hierarchical reinforcement learning model.
Figure 3: Trajectories generated from Option-Critic and Hrl for searching the target objectmusic player from the same starting position under the SETTING-STSE.
Figure 4: The average discounted cumulative extrinsic rewards (left) and average SPL (right) of ourmethod with different α values under SETTING-STSE.
Figure 5: Network architecture of our hierarchical reinforcement learning model.
Figure 6: Trajectories generated by our method for the robot to search (a) music (b) music (c)television and (d) sofa.
