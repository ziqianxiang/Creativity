Figure 1: We study three properties of realistic environments that can have a large effect on the difficultyof reinforcement learning: (left) non-episodic learning, where the agent is not reset automatically to the sameinitial state distribution, but must handle whatever situation it puts itself in; (middle) environment shaping as analternative to reward shaping, where the agent has a single, sparse reward, but the environment is varied so asto provide a curriculum (e.g., due to its natural dynamics, or by a cooperative teacher agent); (right) dynamicenvironments, where the environment changes due to the actions of other agents and natural phenomena, evenif the agent does not take a coordinated course of action - SUCh dynamic phenomena can, as We will show,alleviate some of the difficulties in non-episodic learning.
Figure 2: Tasks in our partially observed stochastic environment for crafting an axe (left) and hunting a deer(middle). The agent (purple triangle) receives a local observation (shaded gray square) and must interact withobjects in the correct sequences to complete the task. In the food collection task in Unity (right), the agent’sgoal is to collect green food and avoid red poison.
Figure 3: Proportion of validation tasks solved in each setting. Agents learning in static non-episodic en-vironments struggle to learn useful behaviors, while agents learning in dynamic non-episodic environmentsare substantially more successful. Episodic learning is easier than non-episodic learning on the first task, butnon-episodic learning in dynamic environments is almost as effective as episodic learning on the hunting task.
Figure 4: Proportion of validation tasks solved as the dynamic effect probability (i.e., resource probability anddeer movement probability) is varied in the non-episodic setting. For all tasks, the standard static environmentdoes not allow for effective learning, but a number of dynamic environment variants allow the agent to learnthe task successfully. Evaluation is still carried out in a static environment.
Figure 5: Proportion of validation tasks solved for environment shaping with sparse reward, subgoal reward,and different forms of reward shaping. We see that environment shaping can obtain better final performancethan reward shaping.
Figure 6: Proportion of validation tasks solved in each setting. Agents learning in static non-episodic en-vironments struggle to learn useful behaviors, while agents learning in dynamic non-episodic environmentsare substantially more successful. Episodic learning is easier than non-episodic learning on the first task, butnon-episodic learning in dynamic environments is almost as effective as episodic learning on the hunting task.
Figure 7: Performance of human guided environment shaping. We ask a human user to interactively shape theenvironment and observe the human can effectively guide the shaping compared to a predefined environmentshaping schedule.
Figure 8: Performance of environment shaping and reward shaping on the axe-making task in an environmentwith wall obstacles. The distance-based reward suffers while environment shaping, despite operating off of asparse reward, obtains peak validation performance. We find that this advantage in robustness of environmentshaping is present in both the episodic and non-episodic settings, but is enhanced in the former.
Figure 9: State visitation counts for the non-episodic setting visualized at 4 stages during training under thedifferent methods of shaping. Yellow corresponds to high visitation, dark purple corresponds to low visitation.
Figure 10: State visitation counts for the episodic setting visualized at 4 stages during training under thedifferent methods of shaping. Yellow corresponds to high visitation, dark purple corresponds to low visitation.
Figure 11: Sample trajectories on validation environments demonstrating learned behavior trained under en-vironment shaping with sparse reward (left) as well as under shaping with the one-time reward (right), both inthe non-episodic setting. The shaded region represents the agent’s ego-centric partial view of the environment.
Figure 12: Sample trajectories on validation environments demonstrating learned behavior trained under en-vironment shaping with sparse reward (top two), distance-based reward shaping (third), and one-time rewardshaping (bottom), all in the non-episodic setting. While the environment shaped agents accomplish the desiredtask within 15 timesteps, biased task specification in the last two result in interpretable but suboptimal behav-ior. The distance-based reward shaped agent goes to the correct resources in order, but without interacting witheither. The one-time reward shaped agent picks up the axe, but fails to do anything afterwards. Both rewardshaped agents here fail to solve the task within the allotted 100 timesteps.
