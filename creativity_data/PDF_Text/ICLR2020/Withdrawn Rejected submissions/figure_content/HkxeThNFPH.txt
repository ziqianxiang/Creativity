Figure 1: DDPG (red), DDPG-Lagrangian (cyan), SDDPG (blue), SDDPG a-projection (green) on HalfCheetah-Safe and Point-Gather. SDDPG and SDDPG a-projection perform stable and safe learning, although the dynamicsand cost functions are unknown, control actions are continuous, and deep function approximations are used. Unitof x-axis is in thousands of episodes. Shaded areas represent the 1-SD confidence intervals (over 10 randomseeds). The dashed purple line in the two right figures represents the constraint limit.
Figure 2: PPO (red), PPO-Lagrangian (cyan), SPPO (blue), SPPO a-projection (green) on HalfCheetah-Safeand Point-Gather. SPPO a-projection perform stable and safe learning, when the dynamics and cost functionsare unknown, control actions are continuous, and deep function approximation is used.
Figure 4: DDPG (red), DDPG-Lagrangian (cyan),SDDPG (blue), DDPG a-projection (green) on RobotNavigation. Ours (SDDPG, SDDPG a-projection)balance between reward and constraint learning. Unitof x-axis is in thousands of steps. The shaded areasrepresent the 1-SD confidence intervals (over 50 runs).
Figure 3: Robot navigation task details.
Figure 6: Generalization over success rate (d) andconstraint satisfaction (e) on a different environment.
Figure 5: Navigation routes of two learned policiesin the simulator (a) and (b). On-robot experiment (c).
Figure 7: The Robot Locomotion Control TasksPoint-GatherIn these experiments, there are three different agents: (1) a point-mass (X ⊆ R9, A ⊆ R2); anant quadruped robot (X ⊆ R32 , A ⊆ R8); and (3) a half-cheetah (X ⊆ R18, A ⊆ R6). Forall experiments, we use two neural networks with two hidden layers of size (100, 50) and ReLUactivation to model the mean and log-variance of the Gaussian actor policy, and two neural networkswith two hidden layers of size (200, 50) and tanh activation to model the critic and constraint critic.
Figure 8:	DDPG (red), DDPG-Lagrangian (cyan), SDDPG (blue), SDDPG a-projection (green) on Ant-Gatherand Point-Circle. Ours SDDPG and SDDPG a-projection algorithms perform stable and safe learning, althoughthe dynamics and cost functions are unknown, control actions are continuous, and deep function approximationis used. Unit of x-axis is in thousands of episodes. Shaded areas represent the 1-SD confidence intervals (over10 random seeds). The dashed purple line in the two right figures represents the constraint limit.
Figure 9:	PPO (red), PPO-Lagrangian (cyan), SPPO (blue), SPPO a-projection (green) on Ant-Gather andPoint-Circle. SPPO a-projection performs stable and safe learning, when the dynamics and cost functions areunknown, control actions are continuous, and deep function approximation is used.
Figure 10: (a) Training and (b) evaluation environments, generated from real office building plans. Theevaluation environment is an order of magnitude bigger.
Figure 11: Navigation routes of two policies on a similar setup (a) and (b). Log of on-robot experiments (c).
