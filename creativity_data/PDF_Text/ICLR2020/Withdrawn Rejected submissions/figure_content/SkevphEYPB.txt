Figure 1: (a) The structure of a standard layer with BatchNorm. (b) The structure of a layer with POP-Norm.
Figure 2:	Train Loss for POP-Norm embedded VGG16 with different activation functions on CIFAR100: (a)Sigmoid, (b) Tanh and (c) ReLU.
Figure 3:	Train Loss for different normalization approaches embedded VGG16 with different activation func-tions on CIFAR100: (a) Sigmoid, (b) Tanh and (c) ReLU.
