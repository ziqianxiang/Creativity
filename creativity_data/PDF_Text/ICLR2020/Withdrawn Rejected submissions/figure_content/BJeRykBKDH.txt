Figure 1: The illustration of a single step of message passing (computing the inner message, (dx)mit),and co-attention (computing the outer message (dx)ntu) on two nodes (i and u) of molecule x.
Figure 2: A high-level overview of our paired training architecture. The next-level features of atomi of molecule x, (dx)hit, are derived by combining its input features, (dx) hit-1, its inner message,(dx)mit, computed using message passing, and its outer message, (dx)nit, computed using co-attentionover the second molecule, dy .
Figure 3: An overview of the binary (left) and multi-label (right) drug-drug interaction (DDI) task. Inboth cases, two drugs (represented as their molecular structures) are provided to the model in order topredict existence or absence of adverse interactions. For binary classification, the DDI predictor isalso given a particular side effect as input, and is required to specifically predict existence or absenceof it. For multi-label classification, the DDI predictor simultaneously predicts existence or absence ofall side-effects under consideration.
Figure 4: Histograms of entropies of co-attentional coefficients, for the three layers of a trainedK = 3 co-attentive model (left) and the uniform distribution (right).
Figure 5: t-SNE projections of 200 learnt drug-drug representations, across 9 side-effects. Eachside effect is colour-coded with a shade of red for positive drug-drug pairs and a shade of green fornegative pairs.
Figure 6: t-SNE projection of 2000 learnt drug-drug representations, sampled to cover 10 side effects.
