Figure 1: Schematic view of Adaptive Online Planning (AOP).
Figure 2: Reward curves for changing worlds lifelong learning tasks. Rewards are for a singletimestep, not over an episode. Note that some worlds may be more difficult than others, and yield anaturally lower reward. The results are averaged over 5 seeds; the shaded area depicts one standarddeviation above and below the mean. Curves are smoothed and the rewards are clipped to -3 forvisual clarity. See Fig. A.2 in Appendix A for corresponding plots for all environments.
Figure 3: Policy episode performance for changing worlds Hopper. Left: performance of policy only(no planner) throughout training. Right: performance on initial starting state and target velocity afteradditional TD3 training (blue: AOP-TD3, red: TD3, gray: from scratch). 5 seeds are shown.
Figure 4: Graphs for Maze (D). From left to right: (1-NS): average Bellman error of the the firsttime a goal is presented (blue) vs the last time it is presented (orange). (2-NS) average number ofplanning steps. Events denote the 4 times the set of goals switch during the agentâ€™s lifetime. (3-CW): average Bellman error by the time since the last world change. (4-CW): average number ofplanning steps. Both quickly decrease as the agent becomes more familiar with the world.
Figure 5: Graphs for AOP in changing worlds Hopper. Red lines denote world changes. Policy usesis the percent of the time that the policy was used instead of the initial plan (see line 3 of Alg. 3).
Figure 6: Example trajectory traces of Cartesian hopper positions for different algorithms. Anunobserved change in target speed is encountered at the timestep marked by red outline.
