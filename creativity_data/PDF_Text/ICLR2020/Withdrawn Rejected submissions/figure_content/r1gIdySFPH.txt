Figure 1: Left: Robot learning to open a doorwith Skew-Fit, without any task reward. Right:Samples from a goal distribution when using (a)Skew-Fit and (b) unweighted (ie. uniform) sam-pling. When used as goals, the diverse samplesfrom Skew-Fit encourage the robot to practiceopening the door more frequently.
Figure 2: Our method, Skew-Fit, samples goals for goal-conditioned RL in order to induce a uniform statevisitation distribution. We start by sampling from our replay buffer, and weighting the states such that rare statesare given more weight. We then train a generative model pφt+1 with the weighted samples. By sampling newstates with goals proposed from this new generative model, we obtain a higher entropy distribution of states inour replay buffer at the next iteration.
Figure 3: (Left) The set of final states visited by our agent and MLE over the course of training. In contrast toMLE, our method quickly approaches a uniform distribution over the set of valid states. (Right) The entropy ofthe sample data distribution, which quickly reaches its maximum for Skew-Fit. The entropy was calculated viadiscretization onto a 60 by 60 grid.
Figure 4: We evaluate on these continuous control environments. From left to right: Visual Pusher, a simulatedpushing task; Visual Door, a door opening task; Visual Pickup, a picking task; and Real World Visual Door,a real world door opening task. All tasks are solved from images and without any task-specific reward. SeeAppendix D for details.
Figure 5:(Left) Learning curves for simulated continuous control experiments. Lower is better. For eachenvironment and method, we show the mean and standard deviation of 6 seeds and smooth temporally across25 epochs within each seed. Skew-Fit consistently outperforms RIG and various baselines. See the text fordescription of each method. (Right) The first column displays example test goal images for each environment.
Figure 6: Cumulative total pickups during exploration for each method. The prior methods fail to pay attentionto the object and only pick it up at the same rate as the initial policy. In contrast, after seeing the object picked upa few times, Skew-Fit practices picking up the object more often by sampling the appriopriate exploration goals.
Figure 7: Learning curve for Real WorldVisual Door environment. We visually labela success if the policy opens the door to thetarget angle by the last state of the trajec-tory. Skew-Fit results in considerable sampleefficiency gains over prior work on this real-world task.
Figure 8: (Top) Coverage over time on the classic 4-room domain, shown on the right. (Bottom) Coverage overtime on a more challenging maze domain, shown on the right. In both cases, we see that not using Skew-Fit(α = 0) results in significantly slower learning that primarily stays near the start (yellow star).
Figure 9: (a) Comparison of Skew-Fit vs MLE goal sampling on final distance to goal on RL version of thepointmass environment. Skew-Fit consistently learns to solve the task, while MLE often fails. (b) Heatmaps offinal distance to each possible goal location for Skew-Fit and MLE. Skew-Fit learns a good policy over the entirestate space, but MLE performs poorly for states far away from the starting position (the bottom left corner).
Figure 10: (Left) Ant navigation environment. (Right) Evaluation on reaching joint and XY position. Policies aretrained from state. Reward is L2-norm between the current and target joint angle and XY position concatenatedtogether. We use Skew-Fit to sample goals for relabeling and exploration, and compare to other goal samplingmethods. See main paper for description of baselines.
Figure 11: We compare using SAC (Haarnoja et al., 2018) and TD3 (Fujimoto et al., 2018) as the underlyingRL algorithm on Visual Door, Visual Pusher and Visual Pickup. We see that Skew-Fit works consistently wellwith both SAC and TD3, demonstrating that Skew-Fit may be used with various RL algorithms.
Figure 12: We sweep different values of α on Visual Door, Visual Pusher and Visual Pickup. Skew-Fit helpsthe final performance on the Visual Door task, and outperforms No Skew-Fit (alpha=0) as seen in the zoomedin version of the plot. In the more challenging Visual Pusher task, we see that Skew-Fit consistently helps andhalves the final distance. Similarly, in we observe that Skew-Fit consistently outperforms No Skew-fit on VisualPickup. Note that alpha=-1 is not always the optimal setting for each environment, but performs strongly in eachcase in terms of final performance.
Figure 13: Gradient variance averaged across parameters in last epoch of training VAEs. Values of α less than-1 are numerically unstable for importance sampling (IS), but not for Skew-Fit.
Figure 14: Proposed goals from the VAE for RIG and with Skew-Fit on the Visual Pickup, Visual Pusher, andVisual Door environments. Standard RIG produces goals where the door is closed and the object and puck is inthe same position, while RIG + Skew-Fit proposes goals with varied puck positions, occasional object goals inthe air, and both open and closed door angles.
Figure 15: Proposed goals from the VAE for RIG (left) and with RIG + Skew-Fit (right) on the Real WorldVisual Door environment. Standard RIG produces goals where the door is closed while RIG + Skew-Fit proposesgoals with both open and closed door angles.
Figure 16: Example reached goals by Skew-Fit and RIG. The first column of each environment section specifiesthe target goal while the second and third columns show reached goals by Skew-Fit and RIG. Both methodslearn how to reach goals close to the initial position, but only Skew-Fit learns to reach the more difficult goals.
