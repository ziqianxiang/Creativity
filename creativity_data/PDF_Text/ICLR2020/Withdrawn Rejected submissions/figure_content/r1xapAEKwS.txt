Figure 1: Snapshots of the training process of SDGM. The black and green circles are trainingsamples from classes 1 and 2, respectively. The dashed black line is the decision boundary betweenclasses 1 and 2 and thus satisfies P(c = 1|x) = (c = 2|x) = 0.5. The dashed blue and red lines arethe boundaries between the posterior probabilities of components where P(c, m|x) = 0.5.
Figure 2:	Changes in learned class boundaries according to number of initial components.
Figure 3:	Evaluation results using synthetic data. (a) recognition error rate, (b) number of compo-nents after training, (c) number of nonzero weights after training, and (d) weight reduction ratio.
Figure 4: Visualization of CNN features on MNIST after end-to-end learning. In this visualization,five convolutional layers with four max pooling layers between them and a fully connected layerwith a two-dimensional output are used. (a) results when a fully connected layer with the softmaxfunction is used as the last layer. (b) when SDGM is used as the last layer instead. The colors red,blue, yellow, pink, green, tomato, saddlebrown, lightgreen, cyan, and black represent classes from 0to 9, respectively. Note that the ranges of the axis are different between (a) and (b).
Figure 5: Relationship of our study with other studies.
