Figure 1: Values of sorted local minima (top) and train and test AUC (bottom) for 4D XOR. Left: n = 1000, p = 4.
Figure 2: Values of sorted local minima(top) and train and test AUC (bottom) for 5D XOR. Left: n = 3000, p = 5.
Figure 3: Test AUC of best energy minimum out of 10 random initializations vs. data dimension p for a NN with 500hidden nodes. Left: k = 3. Middle: k = 4. Right: k = 5.
Figure 4: Training and testing AUC vs number of hidden nodes. Left: k = 3, p = {30, 35}, middle: k = 4, p ={20, 25}, right k = 5,p = {10, 15}.
Figure 5: Connection pruning framework for a fully connected NN with one or several hidden layer(s) and one outputlayer. The input layer is not a real layer with parameters, it consists of the data features which feed into the NNs fortraining or testing purpose.
Figure 6: Average test AUC vs number ofhidden nodes. The NN-CPA and NN-CPNAstarting with H hidden nodes as the NN,but are reduced to 64 connections at theend of training. Shown are the means, andpruning, the original fully connected neural network will generate a sparse neural network with only a few connectionsin each layer. We will keep training for a few more epochs so that the resulted sparse NN can eventually fall into alocal optimum.
Figure 7: Testing AUC vs number of trials. Left: k = 3, p = 60; Middle: k = 4, 40; Right: k = 5, p = 20.
Figure 8: Loss and AUC evolution for training a pruned sub-network using CPNA for k = 4, p = 40 XOR data.
