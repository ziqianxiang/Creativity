Figure 1: Hyperbolic versus exponential discount-ing. Humans and animals often exhibit hyperbolicdiscounts (blue curve) which have shallower dis-count declines for large horizons. In contrast, RLagents often optimize exponential discounts (or-ange curve) which drop at a constant rate regard-less of how distant the return.
Figure 2: The Pathworld. Each state (white circle) indicates the accompanying reward r and thedistance from the starting state d. From the start state, the agent makes a single action: which whichpath to follow to the end. Longer paths have a larger rewards at the end, but the agent incurs a higherrisk on a longer path.
Figure 3: In each episode of Pathworld an unobservedhazard λ 〜p(λ) is drawn and the agent is subject toa total risk of the reward not being realized of (1 -e-λ)d(a) where d(a) is the path length. When the agent’shazard prior matches the true hazard distribution, thevalue estimate agrees well with the theoretical value.
Figure 4: Multi-horizon model predicts Q-values for nγ separate discount functions thereby modelingdifferent effective horizons. Each Q-value is a lightweight computation, an affine transformationoff a shared representation. By modeling over multiple time-horizons, we now have the option toconstruct policies that act according to a particular value or a weighted combination.
Figure 5:	We compare the Hyper-Rainbow (in blue) agent versus the Multi-Rainbow (orange) agenton a random subset of 19 games from ALE (3 seeds each). For each game, the percentage performanceimprovement for each algorithm against Rainbow is recorded. There is no significant differencewhether the agent acts according to hyperbolically-discounted (Hyper-Rainbow) or exponentially-discounted (Multi-Rainbow) Q-values suggesting the performance improvement in ALE emergesfrom the multi-horizon auxiliary task.
Figure 6:	Performance improvement over Rainbow using the multi-horizon auxiliary task in AtariLearning Environment (3 seeds each).
Figure 7: Measuring the Rainbow improvements on top of the Multi-C51 baseline on a subset of 10games in the Arcade Learning Environment (3 seeds each). On this subset, we find that the multi-horizon auxiliary task interfaces well with n-step methods (top right) but poorly with a prioritizedreplay buffer (bottom left).
Figure 8: We reproduce two figures from Sozou (1998). There is a correspondence betweenhazard rate priors and the resulting discount function. In RL, we typically discount future rewardsexponentially which is consistent with a Dirac delta prior (black line) on the hazard rate indicating nouncertainty of hazard rate. However, this is a special case and priors with uncertainty over the hazardrate imply new discount functions. All priors have the same mean hazard rate E[p(λ)] = 1.
Figure 9: Case when the hazard coefficient k does notmatch that environment hazard. Here the true haz-ard coefficient is k = 0.05, but we compute valuesfor hyperbolic agents with mismatched priors in rangek = [0.025, 0.05, 0.1, 0.2]. Predictably, the mismatchedpriors result in a higher prediction error of value butperforms more reliably than exponential discounting, re-sulting in a cumulative lower error. Numerical results inTable 2.
Figure 10: If the true hazard rate is now drawn accordingto a uniform distribution (with the same mean as before)the original hyperbolic discount matches the functionalform better than exponential discounting. Numericalresults in Table 3.
Figure 11: Summary of our approach to approximating hyperbolic (and other non-exponential)Q-values via a weighted sum of exponentially-discounted Q-vaulues.
Figure 12: By instead evaluating our integral up to γmax ratherthan to 1, we induce an approximation error which increaseswith t. Numerical results in Table 5.
Figure 13: Comparison of hyperbolic coefficient integral estimation between the two approaches.
Figure 14: The (preliminary) performance improvement over Rainbow using the multi-horizonauxiliary task in Atari Learning Environment when we instead prioritize according to the TD-errorscomputed from the largest γ (3 seeds each).
