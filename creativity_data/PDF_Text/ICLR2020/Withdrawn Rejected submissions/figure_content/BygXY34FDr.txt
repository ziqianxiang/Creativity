Figure 1: Proposed Network Architecture “VUSFA”: The model’s input is the current state of the agent st andthe goal location g as images. These go through a shared simaese encoder E (z|st). The reparametrized output zis used to train the ω vector. The policy is conditioned on the USF vector (dotted line indicates gradients do notflow from policy to the USFA head). The USFA ψ is trained with the temporal difference error using φ to givethe expected future state occupancies. The discounted episode return is used to train both ω and USFA vectors.
Figure 2: Agent’s transfer learning ability: No. of train-ing time-steps plotted against the average length of anepisode. Shorter episode lengths indicate the agent haslearnt to navigate to goals in a lower no. of steps (shadedarea is the standard deviation over 100 time-steps).
Figure 3: The twenty goal locations used for trainingGoal Location	ACtiOn -Forward	Action -Forward	Action -ForwardFigure 4: Example of a trajectory of an agent inside the AI2THOR simulatorB.1	Hyperparameter TuningFor the proposed method, we varied the weight of the USF TD error loss, KL loss and the informationconstraint Ic .
Figure 4: Example of a trajectory of an agent inside the AI2THOR simulatorB.1	Hyperparameter TuningFor the proposed method, we varied the weight of the USF TD error loss, KL loss and the informationconstraint Ic .
Figure 5: Architecture used for the SFDP-A3C modelThe input to the model is the current state of the agent st and the goal location g as images. These go through ashared siamese layer to generate embeddings for the goal and the state. The policy is conditioned on the USFvector (dotted line indicates gradients do not flow from policy to the USFA head). The USFA ψ is trained withthe temporal difference error using φ to give the expected future state occupancies. The discounted episodereturn is used to train both ω and USFA vector.
Figure 6: Proposed architecture by Ma et al. (2018b) to train an actor-critic agent with USF (image from Maet al. (2018b))E Mathematical framework for training the VarationalInformation B ottleneck with Dual Gradient DescentThe theory of Information Bottleneck (IB) by Tishby & Zaslavsky (2015) explains a method ofdesigning a neural network considering the trade-off between accuracy and complexity. The theoryof the IB can be described with respect to supervised learning as follows:Assume there are set of classes Y, a multi-dimensional feature vector X. The network’s goal isto compress X to a "less complex" representation Z that shares enough information to predict Ycorrectly. The concept of IB describes the fluctuation between compression versus preserving relevantinformation in Z. Mathematically, the idea of generating an optimal Z can be presented as describedin Equation 15.
