Figure 1: Diverse colorizations, which our networkcreated for the same grayscale image. One of themshows ground truth colors, but which? Solution atthe bottom of next page.
Figure 2: One conditional affine cou-pling block (CC).
Figure 3: Haar wavelet downsampling reduces spatialdimensions & separates lower frequency content (a)from higher frequencies (h,v,d).
Figure 4: cINN model for conditionalMNIST generation.
Figure 6: MNIST samples from our cINN conditionedon digit labels. All ten digits within one row (0, . . . , 9)were generated using the same latent code z, but chang-ing condition c. We see that each z encodes a singlestyle consistently across digits, while varying z betweenrows leads to strong differences in writing style.
Figure 7: To perform style transfer, wedetermine the latent code z = f (x; c, θ)of a test image (left), then use the inversenetwork g = f -1 with different condi-tions C to generate the other digits in thesame style, X = g(z; C, θ).
Figure 8: cINN model for diverse colorization. The conditioning network h consists of a truncatedVGG (Simonyan & Zisserman, 2014) pretrained to colorize ImageNet, with separate convolutionalheads h1, h2, h3, . . . tailoring the extracted features to each individual conditional coupling block(CC). After each group of coupling blocks, we apply Haar wavelet downsampling (Fig. 3) to reducethe spatial dimensions and, where indicated by arrows, split off parts of the latent code z early.
Figure 9: Effects of linearly scaling the latent Code Z while keeping the condition fixed. Vector z*is “typical” in the sense that kz* k2 = E kzk2 , and results in natural colors. As we move closerto the center of the latent space (kzk < kz* k), regions with ambiguous colors become desaturated,while less ambiguous regions (e.g. sky, vegetation) revert to their prototypical colors. In the oppositedirection (kzk > kz* k), colors are enhanced to the point of oversaturation.
Figure 10: For color transfer, we first compute the latent vectors z for different color images (L, a, b)(top row). We then send the same z vectors through the inverse network with a new grayscalecondition L* (far left) to produce transferred colorizations a*, b* (bottom row). Differences betweenreference and output color (e.g. pink rose) can arise from mismatches between the reference colorsa, b and the intensity prescribed by the new condition L* .
Figure 11: Diverse colorizations producedby our cINN.
Figure 12: Failure cases of our method. Top: Sam-pling outliers. Bottom: cINN did not recognizean object’s semantic class or connectivity.
Figure 13: Alternative methods have lower diver-sity or quality, and suffer from inconsistencieswithin objects, or color blurriness and bleeding(compare Fig. 11, bottom).
Figure 14: In an ablation study, we train a cINN using the grayscale image directly as conditionalinput, without a conditioning network h. The resulting colorizations largely ignore semantic contentwhich leads to exaggerated diversity. More ablations are found in the appendix.
Figure 15: Training curves for each task, ablating the different improvements. "div." denotes that thetraining diverges, and the lowest loss so far is given.
Figure 16: Qualitative comparison between smaller cINN and colorGAN on LSUN bedrooms.
