Figure 1: An illustration of cross-lingual knowledge distillation (XD). Before the first step the samepolyglot model is used as teacher and student. At the first step the student model adapts its represen-tation of other languages to English via translation examples (red arrows). Then the representationsfor other languages become closer to English in the latent logits space (orange dots). However,since the same model operates on English as well, representations for English also change and cannot serve as optimal targets anymore (purple dot). That is why we employ teacher network to pro-duce gold target for the next step (black dot). At the last step we continue alignment to the originalEnglish target provided by the teacher model. We repeat XD until convergence.
