Figure 1: (a), (b) Proposed generative and inference models. Diamonds and circles represent,respectively, deterministic and stochastic states. (c) Corresponding architecture with two parts:inference on conditioning frames on the left, generation for extrapolation on the right. hθ and gθ aredeep CNNs, and other named networks are Multilayer Perceptrons (MLPs).
Figure 2: Mean PSNR and SSIM scores with respect to t for all tested models on the SM-MNISTdataset, with their 95%-confidence intervals. Vertical bars mark the length of train sequences.
Figure 3: Conditioning frames and corresponding ground truth and best samples with respect toPSNR from SVG and our method for an example of the SM-MNIST dataset.
Figure 4: PSNR, SSIM and LPIPS scores with respect to t for all tested models on the KTH dataset.
Figure 5: Conditioning frames and corresponding ground truth, best samples from SVG, SAVP andour method, and worst and random samples from our method, for an example of the KTH dataset.
Figure 7: Generation examples at doubled frame rate, using a halved ∆t compared to training.
Figure 6: PSNR, SSIM and LPIPS scores with respect to t for all tested models on the BAIR dataset.
Figure 8: Video (bottom right) generated from the dynamic latent state y inferred with a video (top)and the content variable W computed with the conditioning frames of another video (bottom left).
Figure 9: From left to right, xs, bs (reconstruction of Xs by the VAE of our model), results of theinterpolation in the latent space between Xs and xt, bt and xt. Each trajectory is materialized inshades of grey in the frames.
Figure 10: PNSR, SSIM and LPIPS scores with respect to t on the KTH dataset for SVG and ourmodel with two choices of encoder and decoder architecture for each: DCGAN and VGG.
Figure 11: Conditioning frames and corresponding ground truth and best samples with respect toPSNR from SVG and our method, and worst and random samples from our method, for an exampleof the SM-MNIST dataset.
Figure 12: Additional samples for the SM-MNIST dataset (cf. Figure 11).
Figure 13: Additional samples for the SM-MNIST dataset (cf. Figure 11). SVG fails to maintain theshape of a digit, while ours is temporally coherent.
Figure 14: Additional samples for the SM-MNIST dataset (cf. Figure 11). This example was chosenin the worst 1% test examples of our model with respect to PSNR. Despite this adversarial criterion,our model maintains temporal consistency as digits are not deformed in the course of the video.
Figure 15: Conditioning frames and corresponding ground truth, best samples from SVG, SAVP andour method, and worst and random samples from our method, for an example of the KTH dataset.
Figure 16: Additonal samples for the KTH dataset (cf. Figure 15). In this example, the shadow ofthe subject is visible in the last conditioning frames, foreshadowing its appearance. This is a failurecase for SVG and SAVP which only produce an indistinct shadow, whereas SAVP and our modelmake the subject appear. Yet, SAVP produces the wrong action and an inconsistent subject in its bestsample, while ours is correct.
Figure 17: Additonal samples for the KTH dataset (cf. Figure 15). This example is a failure case foreach method: SV2P produce blurry frames, SVG and SAVP are not consistent (change of action orsubject appearance in the video), and our model produces a ghost image at the end of the predictionon the worst sample only.
Figure 18: Additonal samples for the KTH dataset (cf. Figure 15). Our model is the only one tomake a subject appear in the ground truth.
Figure 19: Additonal samples for the KTH dataset (cf. Figure 15). The subject in this example isboxing, which is the most challenging action in the dataset as all methods are far from the groundtruth.
Figure 20: Conditioning frames and corresponding ground truth, best samples from SVG, SAVP andour method, and worst and random samples from our method, for an example of the BAIR dataset.
Figure 21: Additonal samples for the KTH dataset (cf. Figure 20).
Figure 22: Additonal samples for the KTH dataset (cf. Figure 20).
Figure 23: Generation examples at doubled frame rate, using a halved ∆t compared to training.
Figure 24: Video (bottom right) generated from the combination of dynamic variables (y, z) inferredwith a video (top) and the content variable (w) computed with the conditioning frames of anothervideo (bottom left).
Figure 25: Additional example of content swap (cf. Figure 24).
Figure 26: Additional example of content swap (cf. Figure 24). In this example, the extracted contentis the video background, which is successfully transferred to the target video.
Figure 27: Additional example of content swap (cf. Figure 24). In this example, the extracted contentis the video background and the subject appearance, which are successfully transferred to the targetvideo.
Figure 28: Additional example of content swap (cf. Figure 24). This example shows a failure case ofcontent swapping.
Figure 29: From left to right, xs , xbs (reconstruction of xs by the VAE of our model), results of theinterpolation in the latent space between xs and xt , xbt and xt . Each trajectory is materialized inshades of grey in the frames.
Figure 30: Additional example of interpolation in the latent space between two trajectories (cf.
