Published as a conference paper at ICLR 2020
Posterior sampling for multi-agent reinforce-
ment learning: solving extensive games with
IMPERFECT INFORMATION
Yichi Zhou, Jialian Li, Jun Zhu *
Dept. of Comp. Sci. & Tech., BNRist Center, Institute for AI, Tsinghua University; RealAI
vofhqn@gmail.com,lijialian7@163.com,dcszj@mail.tsinghua.edu.cn
Ab stract
Posterior sampling for reinforcement learning (PSRL) is a useful framework for
making decisions in an unknown environment. PSRL maintains a posterior distri-
bution of the environment and then makes planning on an environment sampled
from the posterior distribution. Though PSRL works well on single-agent rein-
forcement learning problems, how to apply PSRL to multi-agent reinforcement
learning problems is largely unexplored. In this work, we extend PSRL to two-
player zero-sum extensive-games with imperfect information (TEGI), which is a
class of multi-agent systems. Technically, we combine PSRL with counterfac-
tual regret minimization (CFR), which is a leading algorithm for TEGI with a
known environment. Our main contribution is a novel design of interaction strate-
gies. With our interaction strategies, our algorithm provably converges to the Nash
Equilibrium at a rate of O (ʌ/log T/T). Empirical results show that our algorithm
works well.
1	Introduction
Reinforcement Learning (RL) (Sutton & Barto, 2018) provides a framework for decision-making
problems in an unknown environment, such as robotics control. In an RL problem, agents improve
their strategies by gaining information from iterative interactions with the environment. One typical
target in designing RL algorithms is to reduce the number of interactions needed to find good strate-
gies. Thus, how to reduce the number of samples by designing efficient interaction strategies is one
of the key challenges in RL.
Posterior sampling for RL (PSRL) (Strens, 2000) provides a useful framework for deciding how
to interact with the environment. PSRL originates from the famous bandit algorithm Thompson
Sampling (Russo et al., 2018), which uses samples from posterior distributions of bandit parameters
to calculate current policy. PSRL also maintains a posterior distribution for the underlying envi-
ronment and uses an environment which is sampled from this posterior to compute its interaction
strategies. The interaction strategies are then used to interact with the environment to collect data.
The design of the interaction strategies depends on the specific property of the task. For example, in
a single-agent RL (SARL) problem, PSRL takes the strategy with the maximum expected reward on
the sampled environment as the interaction strategy (Osband et al., 2013). Theoretical and empirical
results (Osband & Van Roy, 2016) both demonstrate that PSRL is one of the near-optimal methods
for SARL. Moreover, although PSRL is a Bayesian-style algorithm, empirical evaluation (Chapelle
& Li, 2011) and theoretical analysis on the multi-armed bandit problems (Agrawal & Goyal, 2017)
suggest that it also enjoys good performance for a problem with fixed parameters.
However, applying PSRL to multi-agent RL (MARL) tasks requires additional design on the interac-
tion strategies. This is because the goal of MARL is quite different from that of SARL. In an MARL
problem, each agent still aims to maximize its own reward, but the reward of an agent’s strategy re-
lies not only on the environment, but also on the strategies of other agents. Therefore, in MARL, the
goal of learning is generally referred to finding a Nash Equilibrium (NE) where no agent is willing
* J.Z is the corresponding author.
1
Published as a conference paper at ICLR 2020
to deviate its strategy individually. So we should design the interaction strategies with which the
agents can find or approximate the NE efficiently.
More specifically, we consider the RL problems in imperfect information extensive games (Osborne
& Rubinstein, 1994). Extensive games provide a unified model for sequential decision-making
problems in which agents take actions in turn. Imperfect information here means that agents can
keep their own private information, such as the private cards in poker games. Games with imperfect
information are also fundamental to many practical applications such as economics and security.
In particular, we concentrate on two-player zero-sum imperfect information games (TEGI) where
there are two players gaining opposite rewards and a chance player to model the transitions of the
environment. When the environment (i.e. the transition functions of the chance player and the
reward functions) is known, counterfactual regret minimization (CFR) (Zinkevich et al., 2008) is
the leading algorithm in approximating the NE in a TEGI. However, in the RL setting where the
environment is unknown, CFR is not applicable.
In this work, we present a posterior sampling algorithm for TEGIs with the technique of CFR. That
is, we apply CFR to the environment sampled from the posterior distribution. Our main contribution
is a novel design of interaction strategies for the RL problem of TEGIs. With the proposed strategies,
We show that our algorithm can Provably converge to an approximate NE at a rate of O (dlog TlT).
Empirical results show that our algorithm works well.
2	Preliminary
In this section, we formally define the task of two-player zero-sum imperfect information games
(TEGI) under Reinforcement Learning; and then briefly review two closely related techniques,
namely counterfactual regret minimization (CFR) (Zinkevich et al., 2008) and posterior sampling
for reinforcement learning (PSRL) (Osband et al., 2013), which inspire our solution.
2.1	Problem formulation of TEGI
We start from the definition of general N -player extensive games (See (Osborne & Rubinstein, 1994,
pg. 200) for a formal definition.), which include TEGI as a nontivial special case when N = 2.
Definition 1 (Extensive game). An extensive game has the following components:
•	A finite set of players that includes N agents and a chance player C representing the "Nature" of
the game.
•	A finite set H of sequences that satisfies: 1) The empty sequence is a member of H; 2) If a
sequence {a 1, ∙∙∙ , ak } belongs to H, then for all 1 ≤ l < k, {a 1, ∙∙∙ ,aι} is a member of H.
Here, each member of H is a history and each component of a history is an action taken by a
player. The set of available actions after a history is denoted by α(h) = {a : (h, a) ∈ H} and
the set of terminal histories is denoted by Z.
•	A function P such that P(h) is the player who takes the action after history h. Hi represents the
set of all h that P (h) = i.
•	Afunction c^ that is the strategy of the chance player, i.e., c^ (h, a) is the probability of action a
occurs after h ifP (h) = C.
•	For each player i (besides the chance player), a partition Ii of H: Ii is an information partition
of player i; a set I ∈ Ii is a subset of H such that if h1 , h2 ∈ I, then player i cannot distinguish
them.
•	A reward function r* where r* (h, i) is the distribution of the reward of player i at h ∈ Z. We
assume the rewards are bounded in [-1, 1].
For convenience, let A = maxh Ia(h) ∣ be the maximal size of actions for one history. A strategy σl
for player i is a mapping from Hi to the distribution over valid actions. We use σi(h, a) to represent
the probability of taking action a at h ∈ Hi and σi(h) to be the vector of σi(h, a), a ∈ a(h). And a
strategy profile σ consists of the strategies of all players in [ N ] := {1, ∙∙∙ , N}, i.e., σ = {στ}i∈ [ N ].
2
Published as a conference paper at ICLR 2020
We use σ-i to refer to the strategies of all players except i. Since player i cannot distinguish
h1, h2 ∈ I ∈ Ii, σi(h1) and σi(h2) must be the same and we denote σi (I) = σi(h1). For the
clarity of notation, We abbreviate (c*,r*) as d^. Let ui(h∣σ, d^) denote the expected reward of
player i ∈ [N] at history h under strategy σ. For convenience, let ui(σ∣d^) = ui(hr ∣σ, d*) where
hr is the root of H and ui (h∖r^) is the expected reward of player i for h ∈ Z.
∏σ(h∖d^) is the probability of reaching h with σ and c*. It is easy to see that we can de-
compose ∏σ(h∖d^) into the product of the contribution of each player. That is, ∏σ(h∖d^) =
Qi∈[N]l1{c} ∏σ(h∖d^). We use D(h) to refer to the depth of h in the game tree and D%(h) to refer to
the number of h’s ancestors whose player is i. Obviously, we have D(h) = 1 + Pi∈{N}∪{C} Di(h).
And let D = maxh D(h) and Di = maxh Di(h).
With the above notations, then in a TEGI there are two-players besides the chance player (i.e., N =
2), player 1 and player 2, and u 1(h∖r^) + U2(h∖r^) = 0 for all histories h ∈ Z.
Nash Equilibrium and exploitability: In a multi-agent system, a solution is often referred to a
Nash Equilibrium (NE) (Osborne & Rubinstein, 1994). In a TEGI, σ = (σ1, σ2) is a NE if and
only if ui(σ∣d^) = maxσ*,a ui(σ^,z, σ-l∖df'). Our target is to approximate NE. More specifically, in
TEGIs, the approximation error ofσ = (σ1, σ2) is usually measured by its exploitability:
expl(σ∣d^) = maxu 1(σ^,1, σ2∣d*) + maxu2(σ1, σ^,2∣d*).	(1)
σ*,1	σ*,2
If the environment of a TEGL i.e. d^, is known for the players, we can directly use counterfactural
regret minimization (CFR) (Zinkevich et al., 2008) to minimize the exploitability of this TEGI, as
briefly reviewed in Sec. 2.2.
In this paper, we concentrate on the more challenging yet relatively under-explored setting of TEGI,
where the environment d^ is unknown and moreover d* is subject to some intrinsic uncertainty.
For example, poker games with unknown deck fall into this setting. In practice, this setting is not
uncommon in industrial organisation with unknown entry, exit and firm-specific sources (Ericson
& Pakes, 1995). In this setting, players have to interact with the unknown environment to gain
sufficient knowledge of the game for making optimal decisions, thereby becoming a reinforcement
learning (RL) task. In particular, the task of finding approximate NEs for TEGIs with unknown d^
is a Multi-Agent Reinforcement Learning (MARL) task (BUSOniU et al., 2010).
Moreover, to consider the intrinsic uncertainty of the unknown environment, we adopt a Bayesian
formulation for our TEGI task, which can flexibly incorporate the prior information and enjoys
various potential benefits (Russo & Van Roy, 2014). Formally, we consider the setting where the
chance player and the reward functions follow a prior distribution P°. That is, the underlying d^ =
(c*,r*) is sampled from Po(c, r). Here C and r are not necessarily independent. After playing t
games, players collect some samples from d* = (c*,r*) and they can get the posterior distribution,
denoted as Pt. For example, in the case where r* (h, i) is a Bernoulli distribution and its prior is a
Beta distribution, the posterior distribution Pt (r) is also a Beta distribution. Similarly if the prior
for c* is a Dirichlet distribution, then Pt(c) is a Dirichlet distribution since c* (h) is a multinomial
distribution for h ∈ HC .
To solve the above TEGI problems, we present a method that draws inspirations from the solutions
for two simplified settings, as briefly reviewed below.
2.2	Counterfactual regret minimization (CFR)
As stated above, when the parameters of the game, i.e. d* = (c*, r*), are known1, counterfactual
regret minimization (CFR) (Zinkevich et al., 2008) provides an effective solution to TEGIs with
state-of-the-art performance. Formally, CFR is a self-play algorithm that generates a sequence of
strategy profiles, {σt}tT=1, by minimizing the following regrets:
TT
R*T,i = maixX ui(σi, σt-i∖d*) -Xui(σt∖d*).
σ t=1	t=1
1In this case, the distribution of d* degenerates to a single fixed value; therefore no uncertainty on d*.
3
Published as a conference paper at ICLR 2020
Algorithm 1 CFR-PSRL
while t <T do
∙-v
Sample dt and dt from the posterior Pt .
for all i ∈ {1, 2} do
Select σt by exploiting CFR to minimize the regret: maxσi Pt≤r ui(σl,σ-q,∣dt) 一
ft≤τ ui (σt∣dt).	一
end for
Calculate interaction strategies σι ,t and d? ,t with Eq.(6).
Use σι,t and σ2,t to interact with the environment to gather data and then compute Pt +1.
end while
Output: σ = 1 PT=I σt .
For convenience, We write σT = T1 PT=1 σt if σT(I) = Zgtn)：)(I) ∙One important observation
(Zinkevich et al., 2008) is that in a TEGI the exploitability is: t
expl(σt ∣d^) = T (RT1 + RT2) .	(2)
Therefore, CFR makes σT converge to the NE by minimizing RTI and RT2.
2.3	Posterior sampling for reinforcement learning (PSRL)
Posterior sampling for reinforcement learning (PSRL) (Osband et al., 2013) provides an effective
method for solving RL problems when the environment has uncertainty. Formally, PSRL applies to
the Bayesian RL setting with a given prior distribution over the transition and reward function; and
the agents can access this prior distribution and then update the posterior distribution using the ob-
servations collected from interactions with the environment. PSRL provides a framework on how to
select the strategy to interact with the environment under the Bayesian setting. The process of PSRL
can be decomposed into two steps: (1) sampling one environment from the posterior and computing
strategies for agents according to the sampled environment; (2) using the computed strategies to
interact with the underlying environment and updating the posterior distribution with the collected
data. The two steps are iterated. The strategies that are used to play games are called interaction
strategies.
For different kinds of RL problems, the interaction strategies for PSRL are different. For example,
PSRL chooses the strategy with the maximum expected value as the interaction strategy in the single-
agent RL problems (SARL). However, this cannot be trivially extended to MARL since the learning
goal turns to be the NE. Hence, in order to apply PSRL to our problem of TZEISs, we need to design
of proper interaction strategies.
3	Method
In this section, we formally present our method, which conjoins the merits of PSRL and CFR and
can efficiently compute the approximate NE for TEGI tasks. The key is our design of a proper
interaction strategy, which can coordinate with CFR to interact with the environment.
Before diving into details, we given an overview of our algorithm. We call two games interacted with
the environment as one episode. In episode t, we sample a dt from posterior distribution Pt and then
apply CFR to dt to get a policy tuple (σt ,σ2). Then we sample another dt to calculate the interaction
strategies. Then we use the interaction strategies to interact with the environment to collect data and
update the posterior. Our algorithm can converge to the NE at a rate of O(，log(T”T). The time
complexity of computing the interaction strategies is linear to |H|.
Here we introduce our method in detail. The algorithm is presented in Alg.1. The detailed format
of the interaction strategy will be given soon in Eq. (6).
4
Published as a conference paper at ICLR 2020
To compute the approximate NE, we adopt a CFR algorithm to minimize the following regret for
episode T :
Rr = max	ui(σl, σ-z∣dt) —	ui(σt∣dt),
σi
σ t≤r	t≤r
(3)
where dt is sampled from the posterior distribution at episode t (i.e., Pt). Then We take σ =
Tr Pt≤r σt as the output strategy for our algorithm. Obviously, simply minimizing RT will not
make the exploitability expl(σ ∣d^) small, as d^ can be very different with dt, so we need the interac-
tion strategy to be efficient enough to make sure the difference between dt and d^ is relatively small.
The following equation establishes a relation between the exploitability expl(σ∣d^) and regret RT:
expl(σ∣d*) = T I RRr + RRr + X X(Ui(σTi,σ-i∣d*) — ui(σT,σ-i∣dt))) .	(4)
i∈{1,2} t≤T
Here by fixing that the player —i plays the strategy σ- at episode t,2 we use σTz =
arg maxσ Pt≤τ ui (σi,σ-,i∣d^) to denote player is optimal strategy in the underlying game d*,
and σT = arg maxσ Pt≤τ ui (σi, σ-,i∣dt) to denote player is optimal strategy when the game at
episode t is dt . For convenience, let Gri = T Pt≤r(ui(σr,, σ-z∣d^) — ui(σT, σ-z∣dt)) denote
the gap between exploitability and the regret from CFR. Intuitively, σt is generated by CFR with
a biased knowledge on the environment. The bias can be described by the term Gri . As we can
minimize RT by CFR, We only need to minimize GT in order to minimize expl(σ∣d^). Thus, the
target of the interaction strategies is to fix the bias, i.e., minimize Gri .
The remaining challenge is to design interaction strategies to minimize GTi efficiently. In episode t,
∙-v
we first draw dt = (ct ,rt)〜Pt. Then for i ∈ {1, 2}, we compute the strategy that maximizes the
cumulative reward gaps between games sampled from the posterior:
t
/	.	.	∙∙w	∖
σcti = arg maix	ui(σi, σt-0 i|dct) — ui(σi, σt-0 i|dt0 ) .	(5)
σi t0=1
Interaction strategy: We adopt the following interaction strategies for episode T:
σι,t = (σT, σT)	and	σ2,t = (σT ,σT)
(6)
The computation of σcti can be implemented in time O(|H|) . To make the whole procedure clear,
∙-v
we use a simple toy game to show the game tree at episode t in Fig. 1. We present dt, dt and σt.
The interaction strategy is then calculated from these quantities. It needs to be emphasized that the
strategies σt1 and σt2 are generated by CFR in episode t — 1 and they are used as the opponents’
strategies in the interaction strategies.
With the interaction strategies (σcT1, σT2 ) and (σT1, σcT2 ), we can prove the following bound on the
exploitability expl(σ).
Theorem 1. Let ξ = PD=1 Jmaxσ PI∈i D(I)= j πlσi (I). Use Ed* to represent the expectation
over all the prior distribution Po(d^). Ifthe true game is SamPledfrom a prior Po over the chance
player nodes and terminal nodes, then for σ T computed by Alg. 1, we have
T(RT + RRT)= o(T ((ξ1 + ξ2)√AT)),	(7)
GT = O G P∣ZTnZTT+ + q IH C IDc at ln( ∣H C ∣T)) ),	(8)
Ed* expl(σT∣d^) = O (T ((ξ1 + ξ2)√AT + PZTin(ZT) + qIHCIDCAT ln(∣HC∣T))).
_______________________ (9)
2Here for two-player games, -i denotes the other player not i; for multi-player games, -i generally denotes
all players except i.
5
Published as a conference paper at ICLR 2020
Figure 1: A toy game. Here P (h1) = C, h2, h3 are the nodes for player 1 and h4, h5 are the
-⅛∙
nodes for player 2. At episode t, dt and dt are sampled from the posterior distribution, as shown
as ct, Ct,u 1(∙∣dt) and U 1(∙∣dt). Then σ1 and σ2 can be calculated by CFR. Finally We use these
parameters on the graph to calculate the interaction strategies with Eq. (5).
Here ξ is a game-dependent parameter, related to the structure of the game. Its definition comes
from Corrollary 2 in Burch (2018). Under some mild assumptions, we have that -∖∕∖Ii| ≤ ξ ≤ ∖Ii∖.
The present theorem is significant at least in the folloWing aspects.
Firstly, the per episode running time is linear to the size of game tree and the bound is sublinear to
T. Thus, we can expect our algorithm to reach a certain approximate error in a finite time.
Secondly, our theorem holds for any prior distribution over d*. In practical TEGIs, it is possible that
the priors for h1 and h2, h1, h2 ∈ HC, are independent. Our theorem and algorithms can also be
applied to such situations.
Lastly, our interaction strategies σ1 T and ^2T only contribute to the bound for GT, which can be
treated as the error for interaction strategy’s exploring the environment. If we apply PSRL to a
single-agent tree game, the Bayesian regret might be considered as some error caused by interacting
with the environment. Using the analysis in (Osband et al., 2013), we can get that PSRL enjoys an
averaged Bayesian regret bound of order O (P∖Z∖∖n(∖Z∖T)∕T + p ∖H C ∖DC A ln( ∖H C ∖T)/T) for a
general prior. Therefore, our bound for GTi has a comparable order to the bound for the average
Bayesian regret in single-agent tree games.
3.1	Proof sketch of Theorem 1
Before giving the detailed proof, we introduce some additional notations. For episode t, we generate
two trajectories by interacting with the environment. More specifically, we use Ti,t (i ∈ {1, 2}) to
denote the trajectory generated by ^i,t in environment d*. We use E小 to denote the expectation
over all trajectories for episode t. Then we use TiC,t = {h1C,t, h2C,t, ..., hCmi,t,t} to denote the trajectory
for the chance player in episode t, and here mi,t denotes the length of TiC,t . Furthermore, we denote
the terminal node for Ti,t as zi,t. Besides, we denote the collection ofT1,1, T2,1..., T1,t-1, T2,t-1 and
the related rewards as Ht, which represents all the observations before episode t. For each history
h, we further use nt (h) to denote the count that h has been visited in Ht. Further, we use ETi,t to
denoting the expectation over all possible trajectories Ti,t .
Below we give the key part for the proof. Obviously, we need to bound the regret of CFR, i.e., RRT,
and GT. We can directly apply the technique in Theorem 1 of (Burch, 2018, pg. 34)3 to bound Rr
3Theorem 1 in (Burch, 2018) was used to analyze regret with the chance player is fixed in different time
steps. But it can also be used to bound Eq. (7)
6
Published as a conference paper at ICLR 2020
with Eq. (7). Next we show the key part for bounding GTi . Using the definition of σT0i , we have:
GT ≤ T Xm (σTl,σ- ∣d* )-u (σT,,σ-∖dt)))
t≤T
≤τ1max£(Ui(σi,σ-i∣d*) - ui(σi,σ-i∣dt))).
T σi
t≤T
And then, in Lemma 1 and 2, We decompose the above bound into the weighted sum of |c* (h)-
ct(h)| and ∣r^(h) - rt(h)|. And soon later we will show how each term decreases as the number of
episodes increases.
Lemma 1. At episode T, with σ defined in Eq. (6), we can upper bound the expectation of GT:
1T
EHT {ed [gT∣Ht] } ≤T EEHt {etdt,d. Ui((σit,σ-i∖dt) - ui(σi,σ-i∣d^-)] ∣Ht}
t=1
1T
+ T EEHt {eldt,d^ [ui(σi,σ-i∖d*) - ui(σit,σ-∖dt)] ∣Ht] . (10)
t=1
Lemma 1 decomposes the expectation of GTi into two terms, representing the reward difference
between d* and dt and the difference between d* and dt. Below we give an intuitive sketch for
- *aV	∙
bounding the first term ui(σit, σ-z∖dt) - Ui(σt, σ-z∖d^).
Lemma 2. At episode T, the expectation of ui(σit, σ-z∖d,t) - ui(σit, σ- ∖d^) can be bounded by the
∙-v
summation ofthe difference between d* and dt:
EHt {edt,d^ hut(σi,σ-i∖d.t) - ut(σi,σ-t∖d^)] ∣Ht}
≤EHt ∖ Eddt,d>-EF X X Ct(hj,t,a) - C-(hj,t,a)∖ ∣hJ
(	j = 1 a∈α (h)
+ E Ht {E dtd E Ti,t [Ui (zt,t∖r t) -Ui (zt,t∖r*)] IHt O .	(II)
According to the definition of the expectation E& d* ETi,t, We can see that Eq. (11) is a weighted
sum of ∖ct(h) - c*(h)∖ and ∖ut(h∖rt) - ui(h∖r-)∖. Recall that ui(h∖r) refers to the expectation of
r(h) for player i. Intuitively, we can use concentration bound on ∖ct(h) - c-(h)∖, so for h with a
large weight, we should visit it for more times. Notice that the weight in Eq. (11) is essentially the
probability of reaching h under our interaction strategy σ and the real environment c*. Hence if we
use σ to interact with the environment, we can expect our algorithm can visit h with large weight
for sufficient times.
∙-v
To simplify the derivation, we tentatively assume that dt and d* are identically distributed for nodes
hjC,i and zi,t conditioning on Ht. That is, for any node h, with Pr referring to the probability of
some event, we here assume that
∙-v
Pr (d*∖Ht,h ) = Pr (dt∖Ht,h).
In fact this assumption fails when h is reached, because the probability to reach h is influenced by d*
and d. We will remove this assumption and provide a rigorous proof in Appendix A. For (hj,t, a) and
zt,t, we can insert the empirical mean estimations ct(hj,t, a) and Ut(zt,t) and use the frequentists'
concentration bound (Hoeffding, 1994; Weissman et al., 2003). Then for any δ ∈ (0, 1), we have
the following inequalities:
EHt ∖ EdtdETi,t	E	∖C(hj,t,a) - c*(小)∖
(	a∈α(hjC,t )
EHt {EdtdE7i,t [ui(h∖rt) - ui(h∖r*)] ∣Ht} ≤ EHt
Ht ≤ EHt
2ln(2 A∕δ)
max(nt( hjj, 1)
2log(2 /)
max( nt (z^), 1)
+ 4∖Z∖δ.
∣∣Ht
+ 2∖HC∖δ,
2
2

7
Published as a conference paper at ICLR 2020
nt (h)	___ _________
Then for a history h ∈ Z U HC, We have P ∖∕ip≤ ≤ Jnt〈h). Using the Jensen's inequality to
n=i
the summation over Z and HC , We can get the folloWing bound:
T
'E E H {e dit,d^ Ui^( (σ i,σ-i∣dt) - ui (σ it,σ-i∣d*)] ∣Ht} = O ( VWinWy + HH C ∣DC AT ln( ∣H C ∣T)
We can apply the same method to P EHt nEdt,d- [ui(σi,σ-iιd*) - ui(σi,σ-∖dt)] ∖H-t o and get
Eq. (8). Combining the results of Eq. (7) and (8), We can finish the proof of the theorem 1.
4	Related Work
Other methods for TEGIs under unknown environment: There also exist some Works on TEGIs
under an unknoWn environment. Fictitious play (FP) (BroWn, 1951) is another popular algorithm for
approximating NE. In FP, the agent takes the best response to the average strategy of its opponent.
Heinrich et al. (2015) extend FP to TEGIs. Though it may be easier to combine FP With other
machine learning techniques than CFR, When the chance player is knoWn, the convergence rate of
FP is usually Worse than CFR variants. Monte Carlo CFR With outcome sampling (MCCFR-OS)
(Lanctot et al., 2009) can also be applied to TEGIs to approximate NE in a model-free style. It uses
Monte Carlo estimates of the environment to conduct CFR and can converge to the NE. Since it is
updated Without a model of the environment, it is much less efficient than model-based methods.
There is also Work that applies SARL methods to TEGIs. For example, Srinivasan et al. (2018)
adapts actor-critic to games in a model-free style.
MDP: SARL problems are often formalized as the Markov Decision Process (MDP). In the sim-
plest MDP With no transitions, i.e. the Multi-armed bandit problems, the problem-dependent regret
upper bound of PSRL (also named Thompson Sampling in bandit problems) has been carefully an-
alyzed (AgraWal & Goyal, 2017). The problem-dependent bounds for general MDP is still an open
problem. Besides PSRL, there is another kind of provable algorithms for MDP (Jaksch et al., 2010;
Azar et al., 2013) folloWing the Optimism in the Face of Uncertainty principle. They estimate the
uncertainty of the underlying MDP and then use the currently optimal policy to interact With the
environment.
Stochastic Games: the stochastic game (Littman, 1994) is also one kind of multi-agent systems. In
a stochastic game, players take actions at each state and then the environment transits to a neW state
and returns immediate reWards. Nash Q-learning (Hu & Wellman, 2003) converges to approximate
NE by extending Q-learning to games, but it lacks finite-time analysis. Some other work (SzePeSvdri
& Littman, 1996; Perolat et al., 2015; Wei et al., 2017) concentrates on tWo-player zero-sum stochas-
tic games in RL setting. This kind of games don’t involve imPerfect information, and this makes
them different from TEGI.
5	Experiments
To emPirically evaluate our algorithm, we test it on imPerfect-information Poker games. In this
section, we first introduce our baseline methods and then Present the details of the games. Finally,
we show the results.
We choose three kinds of methods as our baselines. The first one is Fictitious Play (FP) and the
second is Monte Carlo CFR with outcome samPling (MCCFR). Both are algorithms for MARL.
Thus we can comPare the Performance of our algorithm and these existing methods. We choose two
variants of our algorithm as the other kind of baselines, which is used to comPare different choices
of interaction strategies. Details of baselines are given below:
• Fictitious self-Play (FSP): FSP is another PoPular algorithm to solve games in the RL setting. In
FP, when d^ is known, each player chooses the best response of its opponent's average strategy.
When d* is not known, we need other RL algorithms to learn the best response. We combine FSP
with two kinds of RL algorithms: 1) FSP with a fitted-Q iteration algorithm (FSP-fitted-Q): we
8
Published as a conference paper at ICLR 2020
Cards: 3 betm: 4
(OI 6O_) A±=qfodx 山
(a) Leduc-4
Figure 2: Results for different algorithms on variants of Leduc-4 and Leduc-5. Here “default” refers
to our algorithm CFR-PSRL.
(OT 6O_) A七=qe-o-dx 山
Cards: 3 betm: 5
(b) Leduc-5
follow (Heinrich et al., 2015) to use a Fitted-Q iteration to learn the best response. We use the
same hyperparameters as reported in Heinrich et al. (2015); 2) FSP with PSRL (FSP-PSRL): we
use a combination of FP and PSRL to give a new baseline: In episode t, we compute player i’s
best response under d 〜 P t, that is, arg max。石 P=,0 ui (σ∖σ-i∣dt).
•	Monte Carlo CFR with outcome sampling (MCCFR-OS): MCCFR-OS uses samples of the game
tree to conduct CFR. Estimated counterfactual values are used to update the policies and the
average polices can converge to NE. This method can be applied to TEGIs under the RL setting.
We use -greedy with = 0.1 as the exploration strategy for MCCFR-OS in our experiments.
•	Variants of Alg. 1: Though we have proved the convergence of Alg. 1 with interaction strategy
(σi, σ-), the proposed method does not necessarily work well in practice. In our experiments,
we evaluate four interaction strategies: 1) Random: the players take actions randomly; 2) Naive:
the players use the output of the CFR procedure, i.e., σt, to interact with the environment; 3) the
best response to σ- (Bestresp): (σ0i, σ-) where σ: is the best response to σ- under dt i.e.,
σ0i = arg max〃i ui (σi, σ-z∣dt); 4) Default: the interaction strategies in Eq. (6).
We test these algorithms on variants of Leduc Hold’em poker (Southey et al., 2012) which is widely
used in imperfect-information game solving. We generate games by keeping the tree structure of the
Leduc Hold’em poker and replacing c and r by randomly generated functions. More specifically,
when generating the tree structure, to control the sizes of the generated game tree, we restrict each
player not to bid more than 4 or 5 times the big blind. The numbers of histories in the generated
games are 9435 and 34776 respectively. The reward function ri (h) is a binary distribution. With a
probability p the value of r1(h) is -1 and with probability 1 -p, the value is 1. The prior P0(r1(h))
is a uniform [0, 1] distribution over parameter p. Obviously, r2 (h) = -r1 (h). Let ed denote the
vector in Rd with every element is 1. c(h) is sampled from Dirichlet(e|A(h)|).
We generate 20 variants for Leduc(4) and Leduc(5) respectively. And on each generated game, each
algorithm updates its strategies for 10000 times, and after each update, it interacts with the environ-
ment for 2 rounds of games. The result is shown in Fig. 2. As Fig. 2 shows, the exploitability
of naive CFR fails to decrease after 10000 rounds on both Leduc(4) and Leduc(5). This might be
caused by the lack of efficient exploration of the environment. MCCFR-OS and FSP-Fitted-Q have
poor performances comparing to other algorithms. This may be caused by the data-inefficiency of
model-free methods and the inefficient exploration strategies. Random interaction and FP can grad-
ually decrease the exploitability, but our algorithm decrease at a higher speed. Thus the empirical
result shows that our algorithm outperforms baselines on the two games.
6 Conclusions and discussions
In this work, we consider the problem of posterior sampling for TEGIs, which is a class of multi-
agent reinforcement learning problems. By a novel design of interaction staregies, we conjoin the
9
Published as a conference paper at ICLR 2020
merits of PSRL and CFR and present a provably convergent algorithm for TEGIs. Our algorithm
empirically works well.
In the future, there are various directions to improve the result. For example, our bound is a Bayesian
bound describing the expected performance. Considering one sample from the prior, Frequentists’
methods such as UCBVI (Azar et al., 2013) also give a high probability regret bound for SARL
of a similar order to PSRL. Further, comparing with the worst-case bound, the problem-dependent
performance is much more important. Though it is possible that our method has a better performance
on a specific TEGI than the bound in Theorem 1, our algorithm is very possibly not the best in the
sense of problem-dependent performance.
Another direction is that our method heavily relies on the structure of TEGIs and the solution concept
of Nash Equilibrium. Thus, further work is needed to extend posterior sampling to more complicated
multi-agent systems, such as stochastic games (Littman, 1994) and extensive games with more than
two players.
Moreover, the generalization for PSRL is another important but challenging future work direction. It
is worth of a systematic investigation to bridge the gap between the provable tabular RL algorithms
and PSRL methods with generalization. Bootstrapping might be one possible direction. Osband
et al. (2016) applies the principle of PSRL to DQN by using bootstrapping. Another possible direc-
tion is to adapt more practical Bayesian inference algorithms to RL tasks.
Acknowlegement
This work was supported by the National Key Research and Development Program of China (No.
2017YFA0700904), NSFC Projects (Nos. 61620106010, U19B2034, U1811461), Beijing NSF
Project (No. L172037), Beijing Academy of Artificial Intelligence (BAAI), Tsinghua-Huawei Joint
Research Program, a grant from Tsinghua Institute for Guo Qiang, Tiangong Institute for Intelli-
gent Computing, the JP Morgan Faculty Research Program and the NVIDIA NVAIL Program with
GPU/DGX Acceleration.
References
Shipra Agrawal and Navin Goyal. Near-optimal regret bounds for thompson sampling. Journal of
the ACM (JACM), 64(5):30, 2017.
Mohammad Gheshlaghi Azar, Remi Munos, and Hilbert J Kappen. Minimax pac bounds on the
sample complexity of reinforcement learning with a generative model. Machine learning, 91(3):
325-349, 2013.
George W Brown. Iterative solution of games by fictitious play, 1951. Activity Analysis of Produc-
tion and Allocation (TC Koopmans, Ed.), pp. 374-376, 1951.
Neil Burch. Time and space: Why imperfect information games are hard. 2018.
LUcian Buyoniu, Robert Babuska, and Bart De Schutter. Multi-agent reinforcement learning: An
overview. In Innovations in multi-agent systems and applications-1, pp. 183-221. Springer, 2010.
Olivier Chapelle and Lihong Li. An empirical evaluation of thompson sampling. In Advances in
neural information processing systems, pp. 2249-2257, 2011.
Richard Ericson and Ariel Pakes. Markov-perfect industry dynamics: A framework for empirical
work. The Review of economic studies, 62(1):53-82, 1995.
Johannes Heinrich, Marc Lanctot, and David Silver. Fictitious self-play in extensive-form games.
In International Conference on Machine Learning, pp. 805-813, 2015.
Wassily Hoeffding. Probability inequalities for sums of bounded random variables. In The Collected
Works of Wassily Hoeffding, pp. 409-426. Springer, 1994.
Junling Hu and Michael P Wellman. Nash q-learning for general-sum stochastic games. Journal of
machine learning research, 4(Nov):1039-1069, 2003.
10
Published as a conference paper at ICLR 2020
Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement
learning. Journal ofMachine Learning Research, 11(APr):1563-1600, 2010.
Marc Lanctot, Kevin Waugh, Martin Zinkevich, and Michael Bowling. Monte carlo sampling for
regret minimization in extensive games. In Advances in neural information processing systems,
PP. 1078-1086, 2009.
Michael L Littman. Markov games as a framework for multi-agent reinforcement learning. In
Machine learning proceedings 1994, PP. 157-163. Elsevier, 1994.
Ian Osband and Benjamin Van Roy. Why is Posterior samPling better than oPtimism for reinforce-
ment learning? arXiv preprint arXiv:1607.00215, 2016.
Ian Osband, Daniel Russo, and Benjamin Van Roy. (more) efficient reinforcement learning via
Posterior samPling. In Advances in Neural Information Processing Systems, PP. 3003-3011, 2013.
Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. DeeP exPloration via
bootstraPPed dqn. In Advances in neural information processing systems, PP. 4026-4034, 2016.
Martin J Osborne and Ariel Rubinstein. A course in game theory. MIT Press, 1994.
Julien Perolat, Bruno Scherrer, Bilal Piot, and Olivier Pietquin. APProximate dynamic Programming
for two-Player zero-sum markov games. In International Conference on Machine Learning (ICML
2015), 2015.
Daniel Russo and Benjamin Van Roy. Learning to oPtimize via Posterior samPling. Mathematics of
Operations Research, 39(4):1221-1243, 2014.
Daniel J Russo, Benjamin Van Roy, Abbas Kazerouni, Ian Osband, Zheng Wen, et al. A tutorial on
thomPson samPling. Foundations and TrendsR in Machine Learning, 11(1):1-96, 2018.
Finnegan Southey, Michael P Bowling, Bryce Larson, Carmelo Piccione, Neil Burch, Darse Billings,
and Chris Rayner. Bayes’ bluff: OPPonent modelling in Poker. arXiv preprint arXiv:1207.1411,
2012.
Sriram Srinivasan, Marc Lanctot, Vinicius Zambaldi, Julien Perolat, Karl Tuyls, Remi Munos, and
Michael Bowling. Actor-critic Policy oPtimization in Partially observable multiagent environ-
ments. In Advances in Neural Information Processing Systems, PP. 3422-3435, 2018.
Malcolm Strens. A bayesian framework for reinforcement learning. In ICML, volume 2000, PP.
943-950, 2000.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT Press, 2018.
Csaba SzePeSvdri and Michael L Littman. Generalized markov decision processes: Dynamic-
Programming and reinforcement-learning algorithms. In Proceedings of International Conference
of Machine Learning, volume 96, 1996.
Chen-Yu Wei, Yi-Te Hong, and Chi-Jen Lu. Online reinforcement learning in stochastic games. In
Advances in Neural Information Processing Systems, PP. 4987-4997, 2017.
Tsachy Weissman, Erik Ordentlich, Gadiel Seroussi, Sergio Verdu, and Marcelo J Weinberger. In-
equalities for the l1 deviation of the emPirical distribution. Hewlett-Packard Labs, Tech. Rep,
2003.
Martin Zinkevich, Michael Johanson, Michael Bowling, and Carmelo Piccione. Regret minimization
in games with incomPlete information. In Advances in neural information processing systems, PP.
1729-1736, 2008.
11
Published as a conference paper at ICLR 2020
A Proof for theorem 1
Let σ = TT pt≤τ σt denote the average strategy tuple UP to episode T. We can decompose the
exploitability at episode 1 into the CFR regret and an extra exploration term:
expl (σ ∣d^)= 1( RRt + RT + X X(U (σT∖σ∏d*『U (σT ,σ-∖d))),
i∈{1,2} t≤T
where RT，σT, and σ0T are formally defined as:
RiT = max	U (σ∖σ∏dt) —	Ui (σt∖dt),
σi
t≤T	t≤T
σTi = arg max ^X U (σ', σ-z∖d^),
σi	t≤T
T
σT0i = arg max	Ui(σi, σt-i∖dt).
σi
t=1
Here 弱 is the regret from CFR. And σT and σT represent the optimal strategies under d^ and dt
respectively, fixing that the its opponent plays the strategy σt-i at episode t.
Recall that dt is sampled from the posterior distribution Pt . The proof for this decomposition is
given below:
Proof. Inserting a term ui (σT, σ-z∣ dt) for episode t, We have:
X ui (σTi,σ-i∣d* ) = X( U (σTi,σ-i∣d*) - ui (σ'Ti ,σ-i∖dt)+ U (σ'Ti ,σ-i∣dt))
t≤T	t≤T
=X(U(σTi,σ-i∖d*) - U(σT,σ-i∖dt)) + X U(σt∖dt) + R
i,t	t≤T
Moreover, with U1(σt∖dt) + U2(σt∖dt) = 0 and the definition of expl, we finish the proof.
□
By directly applying the result in (Burch, 2018), we can upper bound the CFR regret with
RT ≤ T (ξi√AT),
where ξi = PjD=1 maxσi PI∈Ii,D(I)=j πσi i (I) represents the complexity of the game tree.
For convenience, let GT = T Pt≤τ(U(σT,r,σ-z∖d^) — U(σT,σ-∖dt)) denote the gap between
exploitability and the regret from CFR. Since we have upper bounded RTi , we only need to bound
GT and then we can upper bound the exploitability of the average strategy σ.
Obviously, GT represents the difference between d* and dt. Thus we need to design a suitable
interaction strategy to make sure that GTi is small. Using the definition of σT0i , we have
GT ≤ T X( Ui (σTl,σ- ∖d* )-U (σT,,σ-∖dt)))
t≤T
≤T1max X(U(σi,σ-i∖d*) — Ui(σi,σ-i∖dt))).
T σi
t≤T
12
Published as a conference paper at ICLR 2020
∙-v
We select the interaction strategy as follows. First, We draw dt 〜Pt. For i ∈ {1,2}, We compute
σ i = arg max ɪ2 Ui (σi, σ-i∖dtj) — Ui (σi, σ-i∖djfj)
σ	t0‹t
(12)
And then We use (σt ,σ^) and (σ2 ,σt1') to interact with the environment. Following lemma provides
an upper bound on Gt. Then we get below lemma:
Lemma 3. With σ i defined in Eq. (12) ,the expectation of GT Can be bounded by:
1T
E Hτ {E d^,dτ ∖τ1 I Hτ] } ≤ T EE Ht {E dlt,dt ∖u( (σ i,σ-i∖dt) - Ui (σ i,σ-i∖d*)] ∣ Ht}
t=1
1T
+ T EEHt {Edt,d^,dt [Ui(σi,σ-i∖d^-) - Ui(σi,σ-i∖dt)]∣ Ht}.
t=1
(13)
This lemma decompose GT into two terms, which can be bounded with careful analysis of the
posterior distribution. We first give the proof for this lemma.
Proof. We have
E Hτ
E d*,dτ
ma x (X(Ui(σi,σ-i∖d*)-
Edτ ,dτ
Edτ ,dτ
=E HT
-T -1
∑( ui (σ τ,
t=1
∖t=1
ma x (X(Ui(σi,σ-i∖dτ)-
. σ ∖t=1
T
∑(Ui(σT,σ-l∖<dτ) - Ui
t=1
ui (σ iσ-Γ∖d)) )))]∣Ht}
• u (σ i,σ-i∖dt )))]∣Ht}
(σT,σ-∖dt))J ∣ HtI
σ-∖dτ)- UX/,σ-∖dt)) IHt
(
(
(

}
+ EHτ {
EdT ,dτ \u((σT,σ-∖dτ) - Ui(σT,σ-∖dτ)] I Ht]
③
≤ EHτ
喟X(X(ui(σi,σ-i∖dτ) - Ui(σi,σ-i∖dt)))[ ∣ Ht|
+ EHτ {Edτ,dτ ∖ii(σT,σ-∖dτ) - Ui(σT,σTi∖dτ)] ∣ Ht]
1 ,dτ -1
-T -1	-I
Σ∙	∙	∙	∙	∙	∙	I
(u,(σT-1 ,σ-l∖dτ-1) - ui(σT-1 ,σ-l∖dt)) ∣ Ht-
_t =1	_|
1
+ EHτ {Edτ,dτ ∖i((σT,σ-∖d^τ) - Ui(σT,σ-i∖dτ)] ∣ Ht}
≤ XEHt {Eldt,dt [Ui(σi,σ-i∖d.t) - Ui(σσt,σ-∖dt)] ∣ Ht}
t=1
T
=XEHt {Eidt,d^ [i^((σi,σ-i∖dt) - Ui(σiσ-i∖d*)] ∣ Ht}
t=1
T
+ EEHt {Edt,d*,dt [Ui(σiσγτidιt-) - Ui(σi,σ-i∖dt)] ∣ Ht}.
t=1
Here equality = holds since d* and dτ are identical distributed conditioning on Ht. Equation =
∙-v
holds due to the definition of σT. Then we get ③ with the maximum function. Equality = holds
13
Published as a conference paper at ICLR 2020
with the definition of σT-1. Inequality ③ holds by using the induction to episode t. Finally We get
equality ◎ by inserting ui(σi, σ-z∣d^).
Therefore, we finish the proof.	□
Then we only to give upper bounds for the above two terms. Also mentioned in sec.3.1, we intro-
duce some additional notations. For episode t, we generate two trajectories by interacting with the
environment. More specifically, we use Ti,t (i ∈ {1, 2}) to denote the trajectory generated by σi,t
with d*. We use ETit to denote the expectation over all trajectories for episode t. Then we denote
TiC,t = {h1C,t, h2C,t, ..., hCmi,t,t} the trajectory for the chance player in episode t, and here mi,t denotes
the length of TiC,t. Furthermore, we denote the terminal node for episode t as zi,t. Besides, we de-
note the collection ofT1,1, T2,1..., T1,t-1, T2,t-1 as Ht, which represents all the observations before
episode t. For each history h, we further use nt(h) to denote the count that h has been visited in Ht.
Further, we use ETi,t to denoting the expectation over all possible trajectories Ti,t.
T
Then we concentrate on the first term P EHt
t=1
{edt,d. Uii(σi,σ-i∣dt) - ui(σi,σ-i∣d*)] ∖Ht} and
the second term has similar proof. Since the strategy tuple is the same for the two utilities, we can
decompose their difference with below lemma.
ii
Lemma 4. At episode t, the expectation of ui(σi. σt ∣dt) — ui(σ'o- ∣d ) can be upper bounded:
EHt {eαt,d Ui(((σi,σ-i∣dt) — ui(σi,σ-i∣d*)] ∣Ht}
(	mi,t
=EHt ∖ EdtdETi,t EE 伍t(hj,t,a) — c*(hj,t,a))ui(%炳,。;&) ∣Ht
I	j =1 a∈α (h)
+ EHt nEdt,d-ETi,t [ui(zi,t∖rt) - ui(zi,t[r*)] ∣Ht0 ∙
Proof. From the root node to hC t, players take actions according to (σi, σt-). Thus we should have
EHt {edt,d^ ui((σi,σ-i∖dt) — ui(σi,σ-i∖d*)] ∣Ht}
=EHt {edtdETi,t hui(hlt∖σi,σ-i,dt) — ui(hlt∖σit,σ-i,d*)] ∣Ht}
=EHt ∖ Eldt,d^ETi,t	X (Ct(hC,t,a)ui(hC,t∖σi,σ-i,dt) - c*(h^^,a)ui(hC,t∖σit,σ-,d*)) ∣Ht
I	La∈α (h k)	」
=EHt ∖ Edlt,dtETi,t	X (Ct(hlt,α) — ct(hlt,α))ui(hlt∖∂it,σ-i,dt) ∣Ht
I	aaa(h (h £t)	」
+ EHt Jedt,dETi,t	X	c*(hC,t,a)(ui(hC,t∖Ci,σ-,dt) — ui(hC,t∖Ci,σ-
I	La∈α (h C,t)
=EHt j Eιdt,dtETi,t	X (Ct(hC,t,a) — cf(hlt,a))ui(hC,t∖Cit,σ1,dt) ∣Ht
I	La∈α (h C,t)	」
+ EHt {edtdETi,t hui(hC,t∖Ci,σ-i,dt) — ui(hC,t∖CE,d*)] ∣Ht}
mi,t
E E (C t (hj,t,a) — c* (hj,t,a)) Ui (hj,t∖σ i,σ-i,dt) ∣ Ht
j=1 a∈α(hjC,t)
+ EHt {Edt ,d*ETi,t [ui(zi,t ∖rt) — ui(zi,t ∖r*)] ∣Ht 0 ∙
14
Published as a conference paper at ICLR 2020
Here equation ① holds by inserting a term C(hC,t, a)ui(hCt∖σit, σ-, dt); equation ② holds since
that hCt is reached following the underlying transition c* (hC,t ； and equation ③ holds by using
induction.
Therefore We finish the proof.	□
We upper bound the term ui(zi,t ∖rt) 一 U(z"r*) first. We can refer to the technique of previous
work in PSRL. Recall that in episode t, players reaches terminal node zi,t with a visited count
nt(zi,t). We denote that U[(zi,t) as the empirical mean of ui(zi,t∖r^). SimPly we insert U[(zi,t) to
get
ui(Zi,t∖rt) — ui(zi,t∖r") ≤ ∖ui(zi,t[rt) — Ut(zi,t)∖ + ∖Ut(Zi,t) - ui(zi,t∖r")∖.
First we consider the second one ∖uIt(zi,t) 一 ui(zi,t∖r^)∖ and we have similar bound for the first
term. Conditioning on r^ (Zi t), we can apply the Chernoff-Hoeffding bound (Hoeffding, 1994). For
6 ∈ (0, 1)	，	__________________
Pr	t(zi,t-U(zi^-)∖ ≥ ∕2mxn‰卜(zit))≤ ^,	(14)
where Pr denote the probability.
Then we use the above inequality to get below lemma:
Lemma 5. At episode t, the expectation of ∖ut.(zi,t) — Ui(zi,t∖r^) ∖ can be bounded by
E Ht nE cit,d- E Ti,t [ ∖u t (%t)—ui (zi,t∖r*) ∖ ] ∖Ht o
≤EHt(Edt,d* EK "SmaX(ngU 1) #Ht) + 2U
Proof. Notice that Eq. 14 holds conditioning on r^ (zi,t) and the expectation is taken over the prior
Po. Then we need to carefully apply the Eq. 14. For the convenience of notation, we use ∏t(h∖d^)
to represent ∏~w 0―(h∖d^). We further use I(∙) to indicate the identical function. Then we expand
the expectation into integration
E Ht nE dt,* E Tit [ ∖uι t (zi,t)—ui (zi,t∖r*) ∖ ] \Ht o
=X ∖ ∖Ut(z) — ui(z∖r^)∖∏t(z∖d*)Pr(d*,dt∖Ht)Pr(Ht)d(d*d Ht)
z∈Z
≤ XZ J。吁/?、∏t(z∖dt)Pr(d*,dtH)Pr(Ht)d(d*,dt, Ht)
z∈Z	2max(nt(z), 1)
+ X / 2I (∖t^t(z) — Ui(z∖r*) ∖ ≥ 2LIn(TI) ) Pr(d*∖Ht)Pr(Ht)d(d*, Ht) (15)
z∈Z	2 max(nt (z ), 1)
③ E Ht (E d.，"sma≡i⅛ #H)______________________
+ X / 2I (∖t^t(z) — Ui(z∖r*)∖ ≥ 2Liln(2/：1 1∖ ) Pr(Ht ∖d*)Po(d*)d(d*, Ht)
z∈Z	2 max(nt(z), 1)
管E ʃE Γ I	2log(2/6)-^IHI
≤EH jEdt,dt- [∖∕max(nt(zi,t), 1) JHt j +20
Here we extend the expectation into integration in equation 1 . Inequality 2 holds by separating
the integral space. Equation 3 uses the Bayes rule and thus we can apply Eq. (14) to get equation
4 .
Therefore we finish the proof.
□
15
Published as a conference paper at ICLR 2020
For another term ∣ui(zi,t[rt) 一 Ut(zi,t)|, We can still apply the technique in Lemma 5 to get below
lemma:
Lemma 6. At episode t, the expectation of ∣ui(zi,t∣rt) — Ut((zi,t)| can be bounded by
EHt {edt,dETUUi(Zi,t∣rt) — Ut(Zi,t)I] ∣WtO ≤ EHt ∣Edt,d.
- / 2log(2/) ∙
m max(nt(Zi,t), 1)
I Wt I +2 |Z|S.
Proof. We can directly prove that
E Ht nE dt,d*E Ti,t [ |ui (zi,t[rt)—U t (zi,t) |] mo
=X IIU(s∣rt) — Ut(S)I∏t(s∣d^)Pr(d,dt[Ht)Pr(Wt)d(d,dt, Wt)
s∈Z
≤ X I ʌ∕9 ln(2/δ). ∏ ∏t(s∣dt)Pr(d*,dt∖Ht)Pr(Ht)d(d*,dt, Ht)
s∈Z	2max(nt(s), 1)
+ X 2 2I I ∣ui(s∣rt) — Ui(s)∣ ≥ J —W2/)、、] Pr(d∣Ht)Pr(Ht)d(d*, Ht)
+ S∈ZJ V (It) t( )l≥y 2max(nt(S), 1))	(I t ( t)( , t
=E (E -	" s 2log(2/δ)	# ∣h )
Ht [ dtd* m max(nt(zi,t), 1) I tʃ
+ X 2 2I ( ∣ui(s∣rt) — uUt(S)I ≥ JC 吁/)、,、I Pr(Ht∣d*)P0(d*)d(d*, Ht)
s∈Z	t t	2max(nt(S), 1)	t 0	t
≤EHlE" "SmaX(ng：：,»#|Ht)+2/
(16)
∙-v
The above proof is almost the same as that in Lemma 5 expect inequality (16). Since d* and dt are
identically distributed conditioning on Ht, then we apply below equality to Eq. (16):
I (Ui(sIrt) -Ut(s)I ≥ ∕2mil‰) PMdtHt)PMHt
=I (sIr*)一U t (S )I ≥ √snm^)PMd*H)PrH) )
Then we finish the proof.
□
Hence we combine the results in Lemma 5 and 6 and get the conclusion that for any δ ∈ (0, 1),
EHt {Edt,dETi,t [ui(Zi,tIrt) — ui(Zi,tIr*)] ∣Ht} ≤ Eh, {e展,迹
2 /	2log(2/)
m max(nt (Zi,t), 1)
Ht + 4IZIδ.
Using a pigeon-hole principle and choosing δ = 1/(IZ IT), we have below lemma:
Lemma 7. At episode T, the expectation Ofthe summation of ui (zi,t[r t) - ui (zi,t∖r*) over the prior
distribution P0 has an order of:
-t	-
Epo EU (Zi,t Ir t) — Ui (Zi,t Ir*) = O (p∖zi∏niziΓ)).
t=1
16
Published as a conference paper at ICLR 2020
Then We consider chance player node hj,t. We also denote c(hj,t, a) as the empirical mean of chance
player’s probability to choose a at hjC,t. Notice that the utility is bounded in [-1, 1]. We have
X (C (hj,t,a )-c*( hj,t,a ))ui (hj,t∣σ i,σ-i,dt)
a∈α(hjC,t)
≤ 2	X	C t (hj,t,a) — c* (hj,t,a) I
a∈α(hjC,t)
≤2	X	Ct(hj,t,a)- c(hC,t,a)1+ 2 X c(hj,t,a)- c*(hj,t,a)l∙
a∈α(hjC,t)	a∈α(hjC,t)
Then conditioning on C (hjt a), We use the concentration bound for Lι norm (i.e. the deviation
inequality (Weissman et al., 2003) to get that for δ ∈ (0, 1)
P Ia)^^(S -(小)l ≥ SmaXwAj∕δl 1)卜(S)) <
Similar to the analysis in r, We give beloW lemma:
Lemma 8. At episode t, the expectation of ∣C( hj,t, a) — c^ (hj,t, a) ∣ can be bounded by
X	EHt nEdt,dETMljC(hj,t,a)—c*(hj,t,a)i] ∣Hto
a∈α(hjC,t )
≤ E HjE dt,d
- I^^2ln(2A∕δ)^^
m max(nt (hj,t), 1)
Proof. We use similar techniques to get
E Ht ' E dt,d ETM
I H^ + IH c Iδ.
X	∣C( hj,t,a )-c* (hj,t,a) I ∣Ht
a∈α(hjC,t)
C	∣C(h, a) - cr-(h, a)∣∏t(h∣d^)Pr(d*,dt∖Ht)Pr(Ht)d(d*,dt, Ht)
h∈HC a∈α(h)
≤
h∈H
2ln(2 Afl、∏t (sd*) Pr(d*,dt∖Ht) Pr (Ht) d (d*,dt, Ht)
max(nt (h), 1)
+ X I ( X IcC(h, a) - c*(h, a)I
h∈HC	a∈α(h)
≥ jm≡Aδ⅛( pm" ) Pr Ht)dd Ht)
(17)
=EHt IEdt,d-
"I 2ln(2A∕δ)
V max(nt(h), 1)
H
Ht
+ X f I ( X |C(h,a) - c* (h,a) I
h∈HC	a∈α(h)
≥	δi∖ p"Hm)P0(d)dd Ht)
≤EHt ∣E^lt,d^
-2 2log(2A/δ) ~
V max(nt(z%,t), 1)
∣Ht∣ + IHC Iδ.
The proof process above is similar to that of Lemma 5.
□
Once again, We get beloW lemma:
17
Published as a conference paper at ICLR 2020
Lemma 9. At episode t, the expectation of ∣c(hj,t, a) — c(hj,t, a) ∣ can be bounded by
X EHt {Edt,d*ETi,t [∣c(hj,t,a)- c(hjt,a)∣] mo
α∈α(hCt)
钟 Ht{Edt,d* "MU I %t)+∣HC0
Proof. We use similar techniques to get
EHt ∖ Edt,dEK	X	∖^^hhj,t,a) — c(hj,t,a)∣ ∣Ht
I	Lα∈α( hC,t)	」
X [ X 向 h,
h∈Hc	a∈α (h)
a) — C(h, a)忻t(h∣d^)Pr(d∖ dt∣Ht)Pr(Ht)d(d*, dt, Ht)
≤ X 1 J 2M(2;F πt(s∣d^)Pr(d^,dt∣Ht)Pr(Ht)d(d,dt, Ht)
h∈HcJ VmaXSt(h),I)
+ Xc∕I ( X J(h,a) — c(h,a)≥ :：:；；；RI)) PMdHt)PT(Ht)d(d, Ht)
(18)
=E HlE dt,d^ √m≡ [∣Ht}
+ X /1 (X ∣ C(h,a) — c^-(h,a)∣ ≥ {/；；；[1I)) Pr(Ht∣ d*)P0(d*)d(d*, Ht)
≤ E 乂 {e	卜 m2Xon U') j∣ Ht)+∣ H c∣ δ
The proof process here is similar to that of Lemma 6.	口
Next, We use a pigeon-hole principle and choosing δ = 1 /(∣ HC ∣ T), We have below lemma:
Lemma 10. At episode T, the expectation of the summation of ∣ ct (hC,t, a) — c* (hj,t, a) ∣ over the
prior distribution Po has an order of:
T mi,t
Epo ∑∑ ∑ ∣Ct(hj,a) —c*(hj,t, a)11 = O(ʌ/ ∣ Hc∣ DcATln( ∣Hc∣ T)).
t=1 j=1 a∈a (h)
Therefore we use the conclusion in Lemma7 and 10 to get
t	Z___________________
EEHt {edt,d^ ∖^((σit,σ-i∣dt) — ui(σitσ-^ii∣d*)] ∣Ht} = O(√∣Z∣Tln( ∣Z∣T)+y∕∣Hc∣ DcATln( ∣HcT)).
t=1
The similar proof can be applied to the second term to get the same upper bound by simply replacing
∙-v
dt with dt:
t	Z___________________
EEHt {Edt,d*,dt [u((σ(σ-i∣d*) — u((σ(,σ-i∣dt)] ∣ Ht} = O(√∣Z∣Tln( ∣Z∣T)+y∕∣HcD)cATln( ∣HcT^)).
t=1
Sum the analysis together, we get to the conclusion that
EHT {E…悔∣Ht]} = O(严半叵+ LA联正)
18