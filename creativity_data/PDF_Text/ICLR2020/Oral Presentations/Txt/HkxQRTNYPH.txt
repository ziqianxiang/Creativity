Published as a conference paper at ICLR 2020
Mirror-Generative Neural Machine Transla-
TION
Zaixiang Zheng1, Hao Zhou2, Shujian Huang1, Lei Li2, Xin-Yu Dai1, Jiajun Chen1
1National Key Laboratory for Novel Software Technology, Nanjing University
zhengzx@smail.nju.edu.cn,{huangsj,daixinyu,chenjj}@nju.edu.cn
2ByteDance AI Lab
{zhouhao.nlp,lileilab}@bytedance.com
Ab stract
Training neural machine translation models (NMT) requires a large amount of par-
allel corpus, which is scarce for many language pairs. However, raw non-parallel
corpora are often easy to obtain. Existing approaches have not exploited the full
potential of non-parallel bilingual data either in training or decoding. In this paper,
we propose the mirror-generative NMT (MGnmt), a single unified architecture
that simultaneously integrates the source to target translation model, the target to
source translation model, and two language models. Both translation models and
language models share the same latent semantic space, therefore both translation
directions can learn from non-parallel data more effectively. Besides, the transla-
tion models and language models can collaborate together during decoding. Our
experiments show that the proposed MGnmt consistently outperforms existing
approaches in a variety of language pairs and scenarios, including resource-rich
and low-resource situations.
1	Introduction
Neural machine translation (NMT) systems (Sutskever et al., 2014; Bahdanau et al., 2015; Gehring
et al., 2017; Vaswani et al., 2017) have given quite promising translation results when abundant
parallel bilingual data are available for training. But obtaining such large amounts of parallel data
is non-trivial in most machine translation scenarios. For example, there are many low-resource
language pairs (e.g., English-to-Tamil), which lack adequate parallel data for training. Moreover, it
is often difficult to adapt NMT models to other domains if there is only limited test domain parallel
data (e.g., medical domain), due to the large domain discrepancy between the test domain and the
parallel data for training (usually news-wires). For these cases where the parallel bilingual data
are not adequate, making the most use of non-parallel bilingual data (always quite cheap to get) is
crucial to achieving satisfactory translation performance.
We argue that current NMT approaches of exploiting non-parallel data are not necessarily the best,
in both training and decoding phases. For training, back-translation (Sennrich et al., 2016b) is the
most widely used approach for exploiting monolingual data. However, back-translation individually
updates the two directions of machine translation models, which is not the most effective. Specif-
ically, given monolingual data x (of source language) and y (of target language)1, back-translation
utilizes y by applying tgt2src translation model (TMy→χ) to get predicted translations x. Then the
pseudo translation pairshX, yiare used to update the src2tgt translation model (TMx→y). X can be
used in the same way to update TMy→x. Note that here TMy→x and TMx→y are independent and up-
dated individually. Namely, each updating of TMy→x will not directly benefit TMx→y. Some related
work like joint back-translation Zhang et al. (2018) and dual learning He et al. (2016a) introduce
iterative training to make TMy→x and TMx→y benefit from each other implicitly and iteratively. But
translation models in these approaches are still independent. Ideally, gains from non-parallel data
can be enlarged if we have relevant TMy→x and TMx→y. In that case, after every updating of TMy→x,
we may directly get better TMx→y and vice versa, which exploits non-parallel data more effectively.
1Please refer to Section 2 for the notation in details.
1
Published as a conference paper at ICLR 2020
MGNMT
Figure 1: The
graphical model of
MGnmt.
Figure 2: Illustration of the mirror property of MGnmt.
For decoding, some related works (GUlCehre et al., 2015) propose to interpolate external language
models LMy (trained separately on target monolingual data) to translation model TMx→y, which in-
cludes knowledge from target monolingual data for better translation. This is particularly useful for
domain adaptation because We may obtain better translation output quite fitting test domain (e.g.,
social networks), through a better LMy. However, directly interpolating an independent language
model in decoding maybe not the best. First, the language model used here is external, still inde-
pendently learned to the translation model, thus the two models may not cooperate well by a simple
interpolation mechanism (even conflict). Additionally, the language model is only included in de-
coding, which is not considered in training. This leads to the inconsistency of training and decoding,
which may harm the performance.
In this paper, we propose the mirror-generative NMT (MGnmt) to address the aforementioned
problems for effectively exploiting non-parallel data in NMT. MGnmt is proposed to jointly train
translation models (i.e., TMx→y and TMy→x) and language models (i.e., LMx and LMy) in a unified
framework, which is non-trivial. Inspired by generative NMT (Shah & Barber, 2018), we propose to
introduce a latent semantic variable z shared between x and y . Our method exploits the symmetry,
or mirror property, in decomposing the conditional joint probability p(x, y|z), namely:
logp(x,y|z) = log p(x|z) + logp(y|x,z) = logp(y|z) + logp(x|y,z)
1 [log p(y |x, Z) + log p(y |z) + log p(χ∣y, Z) + log p(χ∣z)]
2 I	二{Z	—}	I —y—	—}	I 二	一}	I	—{Z	—}
src2tgt TMx→y	target LMy	tgt2src TMy→x	source LMx
(1)
The graphical model of MGnmt is illustrated in Figure 1. MGnmt aligns the bidirectional trans-
lation models as well as language models in two languages through a shared latent semantic space
(Figure 2), so that all of them are relevant and become conditional independent given Z . In such
case, MGnmt enables following advantages:
(i)	For training, thanks to Z as a bridge, TMy→x and TMx→y are not independent, thus ev-
ery updating of one direction will directly benefit the other direction. This improves the
efficiency of using non-parallel data. (Section 3.1)
(ii)	For decoding, MGnmt could naturally take advantages of its internal target-side language
model, which is jointly learned with the translation model. Both of them contribute to the
better generation process together. (Section 3.2)
Note that MGnmt is orthogonal to dual learning (He et al., 2016a) and joint back-translation (Zhang
et al., 2018). Translation models in MGnmt are dependent, and the two translation models could
directly promote each other. Differently, dual learning and joint back-translation works in an im-
plicit way, and these two approaches can also be used to further improve MGnmt. The language
models used in dual learning faces the same problem as Gulcehre et al. (2015). Given Gnmt, the
proposed MGnmt is also non-trivial. Gnmt only has a source-side language model, thus it cannot
enhance decoding like MGnmt. Also, in Shah & Barber (2018), they require Gnmt to share all the
parameters and vocabularies between translation models so as to utilize monolingual data, which is
not best suited for distant language pairs. We will give more comparison in the related work.
Experiments show that MGnmt achieves competitive performance on parallel bilingual data, while
it does advance training on non-parallel data. MGnmt outperforms several strong baselines in
different scenarios and language pairs, including resource-rich scenarios, as well as resource-poor
circumstances on low-resource language translation and cross-domain translation. Moreover, we
show that translation quality indeed becomes better when the jointly learned translation model and
language model of MGnmt work together. We also demonstrate that MGnmt is architecture-free
which can be applied to any neural sequence model such as Transformer and RNN. These pieces of
evidence verify that MGnmt meets our expectation of fully utilizing non-parallel data.
2
Published as a conference paper at ICLR 2020
Figure 3: Illustration of the architecture of MGnmt.
2	Background and Related Work
Notation Given a pair of sentences from source and target languages, e.g., (x, y)，We denote X as
a sentence of the “source” language, and y as a sentence of the “target” language. Additionally, We
use the terms “source-side” and “target-side” of a translation direction to denote the input and the
output sides of it, e.g., the source-side of the ”tgt2src“ translation is the target language.
Neural machine translation Conventional neural machine translation (NMT) models often adopt
an encoder-decoder framework (Bahdanau et al., 2015) with discriminative learning. Here NMT
models aim to approximate the conditional distribution logp(y|x; θxy) over a target sentence y =
hy1,... ,yLyi given a source sentence X = (x1,... ,xLxi. Here we refer to such regular NMT
models as discriminative NMT models. Training criterion for a discriminative NMT model is to
maximize the conditional log-likelihood logp(y|x; θxy) on abundant parallel bilingual data DXy =
{x(n), y(n) |n = 1...N} of i.i.d observations.
As pointed out by Zhang et al. (2016) and Su et al. (2018), the shared semantics Z between X and
y are learned in an implicit way in discriminative NMT, which is insufficient to model the semantic
equivalence in translation. Recently, Shah & Barber (2018) propose a generative NMT (GNMT) by
modeling the joint distribution p(X, y) instead of p(y|X) with a latent variable z:
log p(χ, y |z; θ = {θx ,θxy}) = log p(χ∣z; θx) + log p(y |x, Z; %)
where GNMT models log p(X|z; θx) as a source variational language model. Eikema & Aziz (2019)
also propose a similar approach. In addition, Chan et al. (2019) propose a generative insertion-based
modeling for sequence, which also models the joint distribution.
Exploiting non-parallel data for NMT Both discriminative and generative NMT could not di-
rectly learn from non-parallel bilingual data. To remedy this, back-translation and its variants (Sen-
nrich et al., 2016b; Zhang et al., 2018) exploit non-parallel bilingual data by generating synthetic
parallel data. Dual learning (He et al., 2016a; Xia et al., 2017) learns from non-parallel data in a
round-trip game via reinforcement learning, with the help of pretrained language models. Although
these methods have shown their effectiveness, the independence between translation models, and
between translation and language models (dual learning) may lead to inefficiency to utilize non-
parallel data for both training and decoding as MGnmt does. In the meantime, iterative learning
schemes like them could also complement MGnmt.
Some other related studies exploit non-parallel bilingual data by sharing all parameters and vocab-
ularies between source and target languages, by which two translation directions can be updated by
either monolingual data (Dong et al., 2015; Johnson et al., 2017; Firat et al., 2016; Artetxe et al.,
2018; Lample et al., 2018a;b), and Gnmt as well in an auto-encoder fashion. However, they may
still fail to apply to distant language pairs (Zhang & Komachi, 2019) such as English-to-Chinese
due to the potential issues of non-overlapping alphabets, which is also verified in our experiments.
Additionally, as aforementioned, integrating language model is another direction to exploit monolin-
gual data (Gulcehre et al., 2015; Stahlberg et al., 2018; Chu & Wang, 2018) for NMT. However, this
kind of methods often resorts to external trained language models, which is agnostic to translation
task. Besides, although Gnmt contains a source-side language model, it cannot help decoding. In
contrast, MGnmt jointly learns translation and language modeling probabilistically and can natu-
rally rely on both together for a better generation.
3
Published as a conference paper at ICLR 2020
Algorithm 1 Training MGNMT from Non-Parallel Data
Input: (pretrained) MGNMT M(θ) , source monolingual dataset Dx, target monolingual dataset Dy
1:	while not converge do
2:	Draw source and target sentences from non-parallel data: X(S)〜Dχ, y(t)〜Dy
3:	Use M to translate x(s) to construct a pseudo-parallel sentence pair hx(s) , yp(ss)eui
4:	Compute L(x(s); θχ, θyχ, φ) with hx(s) , yp(ss)eui by Equation (5)
5:	Use M to translate y(t) to construct a pseudo-parallel sentence pair hx(pts)eu, y(t) i
6:	Compute L(y(t) ; θy, θχy, φ) with hx(pts)eu, y(t)i by Equation (4)
7:	Compute the deviation Vθ by Equation (6)
8:	Update parameters θ → θ 十 ηVθ
9:	end while
3 Mirror-Generative Neural Machine Translation
We propose the mirror-generative NMT (MGnmt), a novel deep generative model which simulta-
neously models a pair of src2tgt and tgt2src (variational) translation models, as well as a pair of
source and target (variational) language models, in a highly integrated way with the mirror property.
As a result, MGnmt can learn from non-parallel bilingual data, and naturally interpolate its learned
language model with the translation model in the decoding process.
The overall architecture of MGnmt is illustrated graphically in Figure 3. MGnmt models the
joint distribution over the bilingual sentences pair by eχploiting the mirror property of the joint
probability: logp(x, y|z) = 1 [logp(y∣x, z) + logp(y∣z) + logp(x∣y, z) + logp(x∣z)], where the
latent variable Z (We use a standard Gaussian prior Z 〜N(0, I)) stands for the shared semantics
between x and y, serving as a bridge between all the integrated translation and language models.
3.1	Training
3.1.1	Learning from Parallel Data
We first introduce how to train MGnmt on a regular parallel bilingual data. Given a parallel bilin-
gual sentence pair hx, yi, we use stochastic gradient variational Bayes (SGVB) (Kingma & Welling,
2014) to perform approχimate maχimum likelihood estimation of log p(x, y). We parameterize the
approximate posterior q(z|x, y; φ) = N(μφ(χ, y), Σφ(χ, y)). Then from Equation (1), we can have
the Evidence Lower BOund (ELBO) L(x, y; θ; φ) of the log-likelihood of the joint probability as:
logp(χ, y) ≥ L(χ, y; θ, φ) = Eq(z∣x,yM[2{logp(y∣χ, z; θxy) + logp(y|z; θy)
+ logp(x|y, z; θyx) + log p(x|z; θx)}]	(2)
-DKL[q(Z]x,y; φ)“p(Z)]
where θ = {θx , θyx , θy , θxy } is the set of the parameters of translation and language models. The
first term is the (expected) log-likelihood of the sentence pair. The expectation is obtained by Monte
Carlo sampling. The second term is the KL-divergence between z’s approximate posterior and prior
distributions. By relying on a reparameterization trick (Kingma & Welling, 2014), we can now
jointly train all the components using gradient-based algorithms.
3.1.2 Learning from Non-Parallel Data
Since MGNMT has intrinsically a pair of mirror translation models, we design an iterative training
approach to exploit non-parallel data, in which both directions of MGnmt could benefit from the
monolingual data mutually and boost each other. The proposed training process on non-parallel
bilingual data is illustrated in Algorithm 1.
Formally, given non-parallel bilingual sentences, i.e., x(s) from source monolingual dataset Dx =
{x(s) |s = 1...S} and y(t) from target monolingual dataset Dy = {y(t) |t = 1...T}, we aim to
maximize the lower-bounds of the likelihood of their marginal distributions mutually:
log P(X(S)) + log p(y(t)) ≥ L(X(S); θx, θyx, φ) + L(y(t); θy, θxy, φ)	(3)
4
Published as a conference paper at ICLR 2020
where L(x(s); θx, θyx, φ) and L(y(t); θy, θxy, φ) are the lower-bounds of the source and target
marginal log-likelihoods, respectively.
Let us take L(y(t) ; θy, θxy, φ) for example. Inspired by Zhang et al. (2018), we sample x with
p(x|y(t)) in source language as y(t)’s translation (i.e., back-translation) and obtain a pseudo-parallel
sentence pair hx, y(t)i. Accordingly, we give the form of L(y(t) ; θy, θxy, φ) in Equation (4). Like-
wise, Equation (5) is for L(y(t); θy, θxy, φ). (See Appendix for the their derivation).
L(y(t); θy,θχy, φ) = Ep(χ∣y(t)) [Eq(z|x,y(t)M[2{logp(y(t)|z； θy) + logP(y(t)|x, z; θχy)}]
-DκL[q(z∣χ,y(t); Φ)∣∣p(z)]]	(4)
L(X(S)； θχ,θyχ, O)= Ep(y∣χ(s)) [Eq(z∣χ(s) Mφ) [1 {logP(X(S) |z； θx) + logP(X(S) |y,z； θyx)}]
-DκL[q(z∣X(S),y; Φ)∣∣p(z)]]	⑸
The parameters included in Equation (3) can be updated via gradient-based algorithm, where the
deviations are computed as Equation (6) in a mirror and integrated behavior:
Vθ = V{θχ,θyjL(X(S); ∙) + V%θχy}L(y㈤；∙)+ Vφ[L(X(S); ∙) + L(y(t); ∙)]	(6)
The overall training process of exploiting non-parallel data does to some extent share a similar idea
with joint back-translation (Zhang et al., 2018). But they only utilize one side of non-parallel data to
update one direction of translation models for each iteration. Thanks to z from the shared approxi-
mate posterior q(z|X, y; φ) as a bridge, both directions of MGNMT could benefit from either of the
monolingual data. Besides, MGnmt’s “back-translated” pseudo translations have been improved
by advanced decoding process (see Equation (7)), which leads to a better learning effect.
3.2 Decoding
Thanks to simultaneously modeling of translation models and language models, MGnmt is now
able to generate translation by the collaboration of translation and language models together. This
endows MGnmt’s translation in target-side language with more domain-related fluency and quality.
Due to the mirror nature of MGnmt, the decoding process is also of symmetry: given a source
sentence X (or target sentence y), we want to find a translation by y = argmaxy P(y|X) =
argmaxy P(X, y) (X = argmaxx P(X|y) = argmaxx P(X, y)), which is approximated by a mir-
ror variant of the idea of EM decoding algorithm in Gnmt (Shah & Barber, 2018). Our decoding
process is illustrated in Algorithm 2.
Let’s take the srg2tgt translation as example. Given a source sentence X, 1) we first sam-
ples an initial z from the standard Gaussian prior and then obtain an initial draft translation as
y = argmaxy p(y∣X, z); 2) this translation is iteratively refined by re-sampling Z this time from the
approximate posterior q(z∣X, y; φ), and re-decoding with beam search by maximizing the ELBO:
y —argmaxy L(x, y; θ, φ)
=argmaxy Eq(z∣x,y*) [logp(y|x, Z) + logp(y∣z) + logp(x∣z) + logp(X∣y, z)]	(7)
=argmaxy Eq(z∣x,%φ) [ £[[ogp(yi∣y<i, x, z) + logp(y，|y<i, z)], + 1ogp(x∣z) + logp(X∣y, z)]
i |	{z	} |	{z	}
Decoding Score	Reconstructive Reranking Score
The decoding scores at each step are now given by TMx→y and LMy, which is helpful to find a sen-
tence y not only being the translation of X but also being more possible in the target language2. The
reconstructive reranking scores are given by LMx and TMy→x, which are employed after translation
candidates are generated. MGnmt can leverage this kind of scores to sort the translation candi-
dates and determine the most faithful translation to the source sentence. It is to essentially share the
2Empirically, we find that using logp(yi|y<i, x, z) + β logp(yi|y<i, z) with a coefficient β ≈ 0.3 leads to
more robust results, which shares the similar observations with Gulcehre et al. (2015).
5
Published as a conference paper at ICLR 2020
Algorithm 2 MGNMT Decoding with EM Algorithm
Input: MGNMT M(θ), input sentence x, input language l
Output: x’s translation y
procedure: DECODING(x, l)
1:	if l is the “target” language then
2:	SWaP the parameters of M(θ) regarding language: {θχ, θyχ} o {θy, θχy}
3:	end if
4:	y = RUN(x)
5:	return translation y
procedure: RUN(x)
1:	Sample Z from standard Gaussian: Z 〜N(0, I)
2:	Generate initial draft translation: y = argmaXy log p(y∣x, z)
3:	while not converage do
4:	Sample Z = {z(k) }S=ι from variational distribution: z(k)〜 q(z∣x, y)
5:	Generate translation candidates	{y}	via beam search by maximizing
K1 Pz(k)[Pi logp(yi∣y<i,x,z(k)) + logp(y ∣y<i,z(k))]	. “decoding scores” in Equation (7)
6:	Determine the best intermediate translation y via ranking {y} by maximizing Kr Pz(k) [log p(x|z(k))+
log p(x|y, z(k))]	. “reconstructive reranking scores” in Equation (7)
7:	end while
8:	return translation y = y
Table 1: Statistics of datasets for each translation tasks.
Dataset	Wmt14 ENCDE	Nist ENCZH	Wmt16 EncRo	Iwslt16 EncDe
Parallel	4.50m	1.34m	0.62m	0.20m (TED)
Non-parallel	5.00m	1.00m	1.00m	0.20m (News)
Dev/Test	newstest2013∕14	^^MT06∕MT03	newstest2015∕16	tst13/14&newstest2014
same idea as Ng et al. (2019) and Chen et al. (2019), Which propose a neural noisy channel rerank-
ing to incorporate reconstructive score to rerank the translation candidates. Some studies like Tu
et al. (2017), Cheng et al. (2016) also eχploit this bilingual semantic equivalence as reconstruction
regularization for training.
4 Experiment
Dataset To evaluate our model in resource-poor scenarios, We conducted eχperiments on WMT16
EngliSh-to/from-Romanian (Wmt16 EN什RO)translation task as low-resource translation and
IWSLT16 EngliSh-to/from-German (Iwslt16 EN什DE) parallel data of TED talk as cross-domain
translation. As for resource-rich scenarios, we conducted eχperiments on WMT14 English-to/from-
German (Wmt14 EN什De), NIST EngliSh-to/from-Chinese (NIST EN什ZH) translation tasks. For
all the languages, we use the non-parallel data from News Crawl, except for NIST EN什Zh, where
the Chinese monolingual data were eχtracted from LDC corpus. Table 1 lists the statistics. In par-
ticular, for cross-domain translation, all models are trained using parallel data from Ted domain,
and non-parallel data from News domain if applicable. We then evaluate these models on both Ted
and News testsets, respectively.
Experimental settings We implemented our models on the top of Transformer (Vaswani et al.,
2017) and Rnmt (Bahdanau et al., 2015) and Gnmt (Shah & Barber, 2018) as well on Pytorch3. In
this section, we only compare experimental results on Transformer implementation.4
For all languages pairs, sentence were encoded using byte pair encoding (Sennrich et al., 2016a,
BPE) with 32k merge operations, jointly learned from the concatenation of the parallel training
dataset only (except for Nist Zh-En whose BPEs were learned separately). We used the Adam op-
timizer (Kingma & Ba, 2014) with the same learning rate schedule strategy as Vaswani et al. (2017)
with 4k warmup steps. Each mini-batch consists of about 4,096 source and target tokens respec-
3The original Gnmt is based on RNN, and we adapted Gnmt to Transformer.
4See Appendix for results on Rnmt, which is consistent to Transformer.
6
Published as a conference paper at ICLR 2020
Table 2: Statistics of the training datasets for each translation tasks. These values of DKL[q(z)||p(z)]
are to some extent large, which means that MGnmt does rely on the latent variable.
Dataset	Wmt14 ENCDE	Nist ENCZH	Wmt16 ENCRO	Iwslt16 ENCDE
KL-annealing steps	35k	13.5k	8k	4k
DKL[q(Z)Up(Z)]	6.78	8.26	6.36	7.81
Table 3: BLEU scores on low-resource translation (WMT16 EN什Ro), and cross-domain translation
(Iwslt EN什De). Note that for cross-domain translation, all models are trained with Ted domain
as parallel data, and News domain as monolingual data if applicable, whereas these models are
evaluated on the testsets of the both domains, respectively.
Model	Low-Resource		CRoss-DoMAIN (para. TED & mono. NEWs)			
	Wmt16 En-Ro	EncRo Ro-En	TED		NEWs	
			EN-DE	DE- EN	EN-DE	DE-EN
Transformer (Vaswani et al., 2017)	―321 ~~	-^332-	27.5	32.8	17.1	19.9
GNMT (shah & Barber, 2018)	32.4	33.6	28.0	33.2	17.4	20.1
GNMT-M-ssL + non-parallel (shah & Barber, 2018)	―341 ~~	-^353-	28.4	33.7	22.0	24.9
Transformer+BT + non-parallel (sennrich et al., 2016b)	33.9	35.0	27.8	33.3	20.9	24.3
Transformer+JBT + non-parallel (Zhang et al., 2018)	34.5	35.7	28.4	33.8	21.9	25.1
Transformer+Dual + non-parallel (He et al., 2016a)	34.6	35.7	28.5	34.0	21.8	25.3
MGNMT	-327^^	^^33.9-	28.2	33.6	17.6	20.2
MGNMT + non-parallel	34.9	36.1	28.5	34.2	22.8	26.1
tively. We trained our models on a single GTX 1080ti GPU. To avoid that the approximate posterior
“collapses” to the prior that learns to ignore the latent representation while DKL (q(z)||p(z)) trends
closely to zero (Bowman et al., 2016; Shah & Barber, 2018), we applied KL-annealing and word
dropout (Bowman et al., 2016) to counter this effect. For all experiments, word dropout rates were
set to a constant of 0.3. Honestly, annealing KL weight is somewhat tricky. Table 2 lists our best
setting of KL-annealing for each task on the development sets. The translation evaluation metric is
BLEU (Papineni et al., 2002). More details are included in Appendix.
4.1	Results and Discussion
As shown in Table 3 and Table 4, MGnmt outperforms our competitive Transformer base-
line (Vaswani et al., 2017), Transformer-based Gnmt (Shah & Barber, 2018) and related work
in both resource-poor scenarios and resource-rich scenarios.
MGnmt makes better use of non-parallel data. As shown in Table 3, MGNMT outperforms
our competitive Transformer baseline (Vaswani et al., 2017), Transformer-based Gnmt (Shah &
Barber, 2018) and related work in both resource-poor scenarios.
1.	On low-resource language pairs. The proposed MGNMT obtains a bit improvement over Trans-
former and Gnmt on the scarce bilingual data. Large margins of improvement are obtained by
exploiting non-parallel data.
2.	On cross-domain translation. To evaluate the capability of our model in the cross-domain set-
ting, we first trained our model on Ted data from Iwslt benchmark to simulate general-domain
training, and then exposed the model to News non-parallel bilingual data from News Crawl to ac-
cessing target domain knowledge. As shown in Table 3, being invisible to target domain training
data leads to poor performance in target domain testset (News) of both Transformer and MGnmt.
In this case, non-parallel data of News domain contributes significantly, leading to 5.7〜6.4 BLEU
gains. We also conduct a case study on the cross-domain translation in Appendix.
3.	On Resource-rich scenarios. We also conduct regular translation experiments on two resource-
rich language pairs, i.e., EN什DE and NIST EN什Zh. As shown in Table 4, MGNMT can obtain
comparable results compared to discriminative baseline Rnmt and generative baseline Gnmt on
pure parallel setting. Our model can also achieve better performance by the aid of non-parallel
bilingual data than the compared previous approaches, consistent with the experimental results in
resource-poor scenarios.
4.	Comparison to other semi-supervised work. We compare our approach with well-established
approaches which are also designed for leveraging non-parallel data, including back-translation
(sennrich et al., 2016b, Transformer+BT), joint back-translation training (Zhang et al., 2018, Trans-
7
Published as a conference paper at ICLR 2020
Table 4: BLEU scores on resource-rich language pairs.
Model	Wmt14		Nist	
	EN-DE	De-En	EN-ZH	ZH-EN
Transformer (Vaswani et al., 2017)	27.2	30.8	39.02	45.72
GNMT (Shah & Barber, 2018)	27.5	31.1	40.10	46.69
Gnmt-M-Ssl + non-parallel (Shah & Barber, 2018)	29.7	33.5	41.73	47.70
Transformer+BT + non-parallel (Sennrich et al., 2016b)	29.6	33.2	41.98	48.35
Transformer+JBT + non-parallel (Zhang et al., 2018)	30.0	33.6	42.43	48.75
TranSformer+Dual + non-parallel (He et al., 2016b)	29.6	33.2	42.13	48.60
MGnmt	27.7	31.4	40.42	46.98
MGnmt + non-parallel	30.3	33.8	42.56	49.05
Figure 4: BLEU vs. scales of non-
parallel data on IWSLT EnoDe tasks.
Figure 5: BLEU increments vs. adding one side mono-
lingual (w/o interactive training) or non-parallel bilin-
gual data for MGNMT on IWSLT ENoDE tasks.
former+JBT), multi-lingual and semi-supervised variant of Gnmt (Shah & Barber, 2018, Gnmt-
M-Ssl), and dual learning (He et al., 2016a, Transformer+Dual). As shown in Table 3, while
introducing non-parallel data to either low-resource language or cross-domain translation, all listed
semi-supervised approaches gain substantial improvements. Among them, our MGnmt achieves
the best BLEU score. Meanwhile, in resource-rich language pairs, the results are consistent. We
suggest that because the jointly trained language model and translation model could work coordi-
nately for decoding, MGnmt surpasses joint back-translation and dual learning. Interestingly, we
can see that the GNMT-M-SLL performs poorly on NIST ENoZH, which means parameters-sharing
is not quite suitable for distant language pair. These results indicate its promising strength of boost-
ing low-resource translation and exploiting domain-related knowledge from non-parallel data for
cross-domain scenarios.
Mgnmt is better at incorporating language
model in decoding In addition, we find from Ta-
ble 5 that simple interpolation of NMT and exter-
nal Lm (separately trained on target-side mono-
lingual data) (Gulcehre et al., 2015, Transformer-
Lm-Fusion) only produces mild effects. This can
be attributed to the unrelated probabilistic model-
ing, which means that a more naturally integrated
solution like MGnmt is necessary.
Table 5: Incorporating Lm for decoding
(Iwslt task).
Model	EN-DE	DE-EN
MGnmt: dec. w/o Lm	21.2	24.6
MGnmt: dec. w/ Lm	22.8	26.1
Transformer	17.1	19.9
TranSformer+LM-FUSION	18.4	21.1
Comparison with noisy channel model
reranking (Ng et al., 2019) We com-
pare MGnmt with the noisy channel
model reranking (Ng et al., 2019, NCMR).
NCMR uses logp(y|x) + λ1 log p(x|y) +
λ2 log p(y) to rerank the translation candi-
Table 6: Comparison with NCMR (Iwslt task).
Model	EN-DE	DE-EN
MGnmt + non-parallel	22.8	26.1
Transformer+BT w/ NCMR (w/o)	21.8 (20.9)	25.1 (24.3)
Gnmt-M-Ssl w/ NCMR (w/o)	22.4 (22.0)	25.6 (24.9)
dates obtained from beam search, where λ1 = 1 and λ2 = 0.3, which are similar to our decoding
setting. As shown in Table 6, NCMR is indeed effective and easy-to-use. But MGnmt still works
better. Specifically, the advantage of the unified probabilistic modeling in MGnmt not only im-
proves the effectiveness and efficiency of exploiting non-parallel data for training, but also enables
the use of the highly-coupled language models and bidirectional translation models at testing time.
8
Published as a conference paper at ICLR 2020
Effects of non-parallel data. We conduct experiments regarding the scales of non-parallel data on
IWSLT EnoDe to investigate the relationship between benefits and data scales. As shown in Fig-
ure 4, as the amount of non-parallel data increases, all models become strong gradually. MGnmt
outperforms Transformer+JBT consistently in all data scales. Nevertheless, the growth rate de-
creases probably due to noise of the non-parallel data. We also investigate if one side of non-parallel
data could benefit both translation directions of MGnmt. As shown in Figure 5, we surprisingly
find that only using one side monolingual data, for example, English, could also improve English-
to-German translation a little bit, which meets our expectation.
Effects of latent variable z. Empirically, Figure 6 shows gains
become little when KL term gets close to 0 (z becomes uninfor-
mative), while too large KL affects negatively; meanwhile, Table
2 shows that the values of DKL[q(z)||p(z)] are relatively reason-
able; besides, decoding from a zero z leads to large drops. These
suggest that MGnmt learns a meaningful bilingual latent variable,
and heavily relies on it to model the translation task. Moreover,
MGnmt adds further improvements to decoding by involving lan-
guage models that condition on the meaningful semantic z . (Table
5). These pieces of evidence show the necessity of z.
Figure 6: ∆BLEU wrt DKL .
Speed comparison MGNMT introduces
extra costs for training and decoding com-
pared to Transformer baseline. When be-
ing trained on parallel data, MGnmt only
slightly increases the training cost. However,
the training cost regarding non-parallel train-
ing is larger than vanilla Transformer because
of the on-fly sampling of pseudo-translation
pairs, which is also the cost of joint back-
translation and dual learning. As shown in
Table 7, we can see that on-fly sampling im-
plies time-consumption, MGnmt takes more
Table 7: Training (hours until early stop) and decod-
ing cost comparison on IWSLT task. All the exper-
iments are conducted on a single 1080ti GPU.
Model	Training (hrs)	Decoding
Transformer	^17	1.0×
Transformer+BT	〜25	1.0×
Gnmt-M-Ssl	〜30	2.1×
Transformer+JBT	〜34	1.0×
Transformer+Dual	〜52	1.0×
MGnmt	^22	2.7×
MGNMT + non-parallel	〜45	2.7×
training time than joint back-translation but less than dual learning. One possible way to improve
the efficiency may be to sample and save these pseudo-translation pairs in advance to the next epoch
of training.
As for inference time, Transformer+{BT/JBT/Dual} are roughly the same as vanilla Transformer
because essentially they do not modify decoding phase. Apart from this, we find that the decod-
ing converges at 2〜3 iterations for MGnmt, which leads to 〜2.7 X time cost as the Transformer
baseline. To alleviate the sacrifice of speed will be one of our future directions.
Robustness of noisy source sentence We conduct ex-
periments on noisy source sentence to investigate the ro-
bustness of our models compared with Gnmt. The exper-
imental setting is similar to Shah & Barber (2018), i.e.,
each word of the source sentence has a 30% chance of
being missing. We conduct experiments on WMT En-
De. As shown in Table 8, MGnmt is more robust than
Gnmt with noisy source input. This may be attributed to
Table 8: Comparison on robustness of
noisy source sentence.
Model	GNMT	MGnmt
En-De	273	27.7
De-En	31.1	31.4
En-De (noisy)	19.4	20.3
De-En (noisy)	23.0	24.1
the unified probabilistic modeling of TMs and LMs in MGnmt, where the backward translation
and language models are naturally and directly leveraged to better ”denoise” the noisy source input.
Nevertheless, the missing content in the noisy source input is still very hard to recover, leading to
a large drop to all methods. Dealing with noisy input is interesting and we will leave it for future
study.
5	Conclusion
In this paper, we propose the mirror-generative NMT model (MGnmt) to make better use of non-
parallel data. MGnmt jointly learns bidirectional translation models as well as source and target
9
Published as a conference paper at ICLR 2020
language models in a latent space of the shared bilingual semantics. In such a case, both translation
directions of MGnmt could simultaneously benefit from non-parallel data. Besides, MGnmt can
naturally take advantage of its learned target-side language model for decoding, which leads to better
generation quality. Experiments show that the proposed MGnmt consistently outperforms other
approaches in all investigated scenarios, and verify its advantages in both training and decoding. We
will investigate whether MGnmt can be used in completely unsupervised setting in future work.
6	Acknowledgements
We would like to thank the anonymous reviewers for their insightful comments. Shujian Huang is
the corresponding author. This work is supported by National Science Foundation of China (No.
U1836221, 61672277), National Key R&D Program of China (No. 2019QY1806).
References
Mikel Artetxe, Gorka Labaka, Eneko Agirre, and Kyunghyun Cho. Unsupervised neural machine
translation. In ICLR, 2018.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. In ICLR, 2015.
Samuel R. Bowman, Luke Vilnis, Oriol Vinyals, Andrew M. Dai, Rafal JOzefowicz, and Samy
Bengio. Generating sentences from a continuous space. In CoNLL, 2016.
William Chan, Nikita Kitaev, Kelvin Guu, Mitchell Stern, and Jakob Uszkoreit. Kermit: Generative
insertion-based modeling for sequences. arXiv preprint arXiv:1906.01604, 2019.
Peng-Jen Chen, Jiajun Shen, Matt Le, Vishrav Chaudhary, Ahmed El-Kishky, Guillaume Wenzek,
Myle Ott, and MarcAurelio Ranzato. Facebook ais wat19 myanmar-english translation task sub-
mission. WAT 2019, 2019.
Yong Cheng, Wei Xu, Zhongjun He, Wei He, Hua Wu, Maosong Sun, and Yang Liu. Semi-
supervised learning for neural machine translation. In ACL, 2016.
Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and YoshUa Bengio. Learning phrase representations using rnn encoder-decoder
for statistical machine translation. In EMNLP, 2014.
Chenhui Chu and Ying Wang. A survey of domain adaptation for neural machine translation. In
COLING, 2018.
Daxiang Dong, Hua Wu, Wei He, Dianhai Yu, and Haifeng Wang. Multi-task learning for multiple
language translation. In ACL, 2015.
Bryan Eikema and Wilker Aziz. Auto-encoding variational neural machine translation. In
RepL4NLP, 2019.
Orhan Firat, Baskaran Sankaran, Yaser Al-Onaizan, Fatos T. Yarman-Vural, and Kyunghyun Cho.
Zero-resource translation with multi-lingual neural machine translation. In EMNLP, 2016.
Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann Dauphin. Convolutional
sequence to sequence learning. In ICML, 2017.
Caglar Gulcehre, Orhan Firat, Kelvin Xu, Kyunghyun Cho, Loic Barrault, Huei-Chi Lin, Fethi
Bougares, Holger Schwenk, and Yoshua Bengio. On using monolingual corpora in neural ma-
chine translation. arXiv preprint arXiv:1503.03535, 2015.
Di He, Yingce Xia, Tao Qin, Liwei Wang, Nenghai Yu, Tie-Yan Liu, and Wei-Ying Ma. Dual
learning for machine translation. In Advances in Neural Information Processing Systems, pp.
820-828, 2016a.
10
Published as a conference paper at ICLR 2020
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In CVPR, pp. 770-778, 2016b.
Melvin Johnson, Mike Schuster, Quoc V Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil
ThoraL Fernanda Viegas, Martin Wattenberg, Greg Corrado, et al. Googles multilingual neural
machine translation system: Enabling zero-shot translation. Transactions of ACL, 5, 2017.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. ICLR, 2014.
Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2014.
Guillaume Lample, Alexis Conneau, Ludovic Denoyer, and Marc’Aurelio Ranzato. Unsupervised
machine translation using monolingual corpora only. In ICLR, 2018a.
Guillaume Lample, Myle Ott, Alexis Conneau, Ludovic Denoyer, and MarcAurelio Ranzato.
Phrase-based & neural unsupervised machine translation. In EMNLP, 2018b.
Tomas Mikolov, Martin Karafiat, LUkas Burget, Jan Honza Cernocky, and Sanjeev KhUdanpur.
Recurrent neural network based language model. In INTERSPEECH, 2010.
Nathan Ng, Kyra Yee, Alexei Baevski, Myle Ott, Michael Auli, and Sergey Edunov. Facebook fairs
wmt19 news translation task submission. In WMT, 2019.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic
evaluation of machine translation. In ACL, 2002.
Mike Schuster and Kuldip K Paliwal. Bidirectional recurrent neural networks. IEEE Transactions
on SignalProcessing, 45(11):2673-2681,1997.
Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with
subword units. In ACL, 2016a.
Rico Sennrich, Barry Haddow, and Alexandra Birch. Improving Neural Machine Translation Models
with Monolingual Data. In ACL, 2016b.
Harshil Shah and David Barber. Generative neural machine translation. Neurips, 2018.
Felix Stahlberg, James Cross, and Veselin Stoyanov. Simple fusion: Return of the language model.
In WMT, 2018.
Jinsong Su, Shan Wu, Deyi Xiong, Yaojie Lu, Xianpei Han, and Biao Zhang. Variational recurrent
neural machine translation. In AAAI, 2018.
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.
In NIPS, 2014.
Zhaopeng Tu, Yang Liu, Lifeng Shang, Xiaohua Liu, and Hang Li. Neural machine translation with
reconstruction. In AAAI, 2017.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is All you Need. In NIPS, 2017.
Yingce Xia, Tao Qin, Wei Chen, Jiang Bian, Nenghai Yu, and Tie-Yan Liu. Dual supervised learning.
In ICML, 2017.
Biao Zhang, Deyi Xiong, Jinsong Su, Hong Duan, and Min Zhang. Variational neural machine
translation. In EMNLP, 2016.
Longtu Zhang and Mamoru Komachi. Chinese-japanese unsupervised neural machine translation
using sub-character level information. CoRR, abs/1903.00149, 2019.
Zhirui Zhang, Shujie Liu, Mu Li, Ming Zhou, and Enhong Chen. Joint training for neural machine
translation models with monolingual data. In AAAI, 2018.
11
Published as a conference paper at ICLR 2020
A Learning from Non-parallel Data: Derivation
We first take the target marginal probability log p(y(t)) for example to show its deviation. Inspired
by Zhang et al. (2018), we introduce y(t) ’s translation x in source language as intermediate hidden
variable, and decompose log p(y(t)) as:
p(x,y(t))
Q(X)
log p(y(t)) = log	p(x, y(t)) = log	Q(x)
≥ X Q(x)log P(X,,"、)(Jenson inequality)
x	Q(x)
(8)
Q(x)logp(x,y(t)) - Q(x) log Q(x)
x
In order to make the equal sign to be valid in Equation (8), Q(x) must be the true tgt2src translation
probability p* (x∣y(t)) , which can be approximated by MGNMT through p* (x∣y(t)) = p(x∣y(t))
p(χ,y⑴)
p* (y ⑴)
1 Ez[p(x,y(t)[z)[ ViaMonte Carlo sampling5 * *. Analogously, the intermediate hidden
variable x is the translation of y(t) given by MGNMT itself (described in Section 3.2), which pro-
duces a pair of pseudo parallel sentences hx, y(t)i. This is similar to the back-translation (Sennrich
et al., 2016b), which requires an externally separate tgt2src NMT model to provide the synthetic
data other than the unified model itself as in MGnmt.
Remember that we have derived the low-bound of log p(x, y) in Equation (2). As a result, we now
get the lower bound of logp(y(t)) as L(y(t); θy, θxy, φ) by
logp(y(t)) ≥ L(y(t); θy, θxy, φ) = Ep(χ∣y(t)) [Eq(z∣χ,y(t)M∣2{logp(x∣z) + p(x∣y㈤,z)
+logp(y(t)|z) + log p(y(t)|x, z)}]	(9)
-DκL[q(z∣χ,y(t); Φ)∣∣p(z)] - logp*(χ∣y(t))]
Since p(x|y(t),z), p(x∣z) and p*(x|y(t)) are irrelevant to parameters {θy, θxy}, L(y(t); θy,θxy, φ)
could be simplified on a optimization purpose, namely:
L(y(t)； θy,θxy, φ) = Ep(x∣y(t))[Eq(z∣x,y(t)M [g{logp(y(t)|z； θy) + logp(y(t) |x,z； θxy)}]
-DκL[q(z∣χ,y(t); Φ)∣∣p(z)]]	(10)
The lower-bound L(y(t); θy, θxy, φ) of logp(y(t)) serves as a training objective to optimize
{θy, θxy, φ}. Likewise, the lower bound of the likelihood on the target marginal probability
log p(x(s)) could be derived as:
L(X(S)； θx,θyx, φ) = Ep(y∣χ(s)) [Eq(z∣χ(s) Mφ)[1 {logP(X(S) |z； θx) + logP(X(S) |y,z； θyx)}]
-DκL[q(z∣X(S),y; Φ)∣∣p(z)]]	(11)
B Implementation Details
We follow the Gnmt (Shah & Barber, 2018) to implement our MGnmt. For machine translation,
we have a source sentence X = hX1, . . . , XLxi and a target sentence y = hy1, . . . , yLyi. As afore-
mentioned, MGNMT consists of a variational src2tgt and a tgt2src translation models (TMx→y (θxy),
5In order to make the equal sign to be valid in Equation 8, Q(x) must satisfy the following condition
P(QyX；)= c, where C is a constant and does not depend on x. Given Px Q(X) = 1, Q(X) can be calculated
as:
Q(X) = Pixy")
c
p(χ,y⑴)
Px p(x,y(t))
where p* (X∣y(t)) denotes the true tgt2src translation probability, while the target marginal probability p* (y)
c =今 due to the assumption that the target sentences in Dy are i.i.d.
12
Published as a conference paper at ICLR 2020
TMx→y (θyx)), as well as a source and a target variational language model(LMx(θx),LMy(θy)). These
four components are conditioned on a shared inference model q(z|x, y; φ) as approximate posterior.
The overall architecture is shown in Figure 3.
Now, we first introduce the implementations based on Rnmt, which is similar to (Shah & Barber,
2018). Then we introduce the Transformer-based variant.
B.1 Rnmt-based MGnmt
Language Model Let’s take the target language model LMy(θy) as an example. LMy (θy) mod-
els the computation of p(y|z; θy), which is implemented by a GRU-based (Cho et al., 2014)
RNNLM (Mikolov et al., 2010) with the latent variable z as additional input. The probabilities
p(y|z), for i = 1, ..., Lx are factorized by:
Ly
p(y|z) =	p(yj|y<t,z) = softmax(E(yj)>Wyhjy)	(12)
j
where Wy is a learnable linear transformation matrix, E(yj) is the embedding of yj, and the hidden
state hjy is computed as:
hjy = GRU(hty-1, [z; E(yt-1)])	(13)
where [∙; ∙] is a concatenation operation.
Likewise, the source language model LMx(θx) models p(x|z; θx) in a mirror way.
Translation Model Let’s take the src2tgt translation model TMx→y (θxy) as an example.
TMx→y (θxy) models the computation of p(y|x, z; θxy), which is implemented by the variational vari-
ant of the widely-used Rnmt (Bahdanau et al., 2015). Rnmt uses an encoder-decoder framework.
The conditional probabilities p(y|x, z), for i = 1, ..., Lx are factorized by:
Ly
p(y|x, z) =	p(yj |y<t, x, z) = softmax(E(yj)>Uysjy)	(14)
j
where Uy is a learnable linear transformation matrix, and the decoder hidden state sjy is computed
as:
S = GRU(Sy-1, [z; E(yt-1)])	(15)
Lx
Cy = E ajiVχ, αji = Softmax(a(gy, Vx))	(16)
i
Sj = GRu(q ,cy)	,	(17)
where vix is the i-th encoder hidden state, Cjy is the attentive context vector, which is a weighted
average of the source hidden states by attentive weight αji given by the attention model a. The
encoder hidden state vix is modeled by a bidirectional GRU (Schuster & Paliwal, 1997; Cho et al.,
2014):
vx = BiC-RU(vx±ι,E (χi±ι))	(18)
Likewise, the tgt2src translation model TMx→y(θyx)) models p(x|z; θx) in a mirror way.
Inference Model The inference model q(z |x, y; φ) serves as an approximate posterior, which is a
diagonal Gaussian:
q(Z|x, y;φ) = N(μφ(x, y), £0(x, y))	(19)
13
Published as a conference paper at ICLR 2020
We first map the sentences x, and y to a sentence representation vector using a bidirectional GRU,
followed by an average pooling, respectively:
一 1 三 «---------→ 一
/=	Eg--RU(rχ±ι,E(χi±ι))	(20)
Lx
i
汽=L Xg---U(ry±1,E(yt±ι))	(21)
yj
where rx and ry is the fixed-length sentence vector which is the average of the hidden states of the
bidirectional GRU of x and y, respectively. We then parameterize the inference model by:
q(z∣x,y; φ) = N(W*[rx; ry], diag(exp(W，[rx; ry])))	(22)
B.2	Transformer-based MGnmt
Theoretically, MGnmt is independent from neural architectures we choose. As for Transformer-
based MGnmt, we substitute the translation models from Rnmt to Transformer (Vaswani et al.,
2017), which is also extended to condition on latent semantic. The language models and inference
model remain the same.
B.3	Hyperparameters
Rnmt-based MGnmt adopts 512-dimensional GRUs, 512-dimensional word embeddings, and a
100-dimensional latent variable z . As for Transformer-based MGNMT, we use the same configu-
rations as transformer-base in Vaswani et al. (2017). The embeddings of the same language
are shared in the MGnmt in our implementations. For KL-annealing (Bowman et al., 2016), we
multiply the KL divergence term by a constant weight, which we linearly anneal from 0 to 1 over
the initial steps of training. The KL-annealing steps are sensitive to languages and the amount of
dataset. We include the KL-annealing steps of best results for each language in the paper.
B.4	Implementation of Other Baselines
Back-translation (Sennrich et al., 2016b, BT), joint back-translation (Zhang et al., 2018, JBT), and
dual learning (He et al., 2016a, Dual) are effective training strategies which do not depend on specific
architecture. Suppose that we have monolingual data Dx and Dy, and bilingual parallel Dxy. Note
that the forward and backward TMs here are all Transformer or Rnmt.
•	BT: To train TMx→y, we first petrain a backward translation model TMy→x. And then we
use TMy→x to translate Dx into a pseudo source corpus Dx0 by beam search (b = 2), and
Dx0 and Dy form the pseudo parallel corpus, namely Dx0y. We finally use the collection
of Dx0y and Dxy to train TMx→y. The BT training for TMy→x is similar by alternating the
language.
•	JBT: JBT is an extension of BT in an alternative and iterative manner. 1) We first pretrain
TMx→y and TMy→x on Dxy, respectively. 2) We use TMy→x,TMx→y to generate pseudo
parallel corpora Dx，y/Dxy，, respectively. 3) We then re-train TMx→y,TMy→x on the collec-
tion of Dxy and Dx0 y/Dxy0 for 1 epoch, respectively. So now we have a pair of better TMx→y
and TMy→x. 4) We finally repeat 2) and 3) with the better TMs until training converges.
• Dual: 1) We first pretrain TMx→y and TMy→x on Dxy, respectively, and LMx and LMy
on Dx and Dy respectively. Note that in the following training process, the LMs are
fixed. 2) To train TMs from monolingual corpora, the rest of the training process fol-
lows He et al. (2016a) to iteratively and alternatively optimize the language model re-
ward and reconstruction reward. Our implementation is heavily inspired by https:
//github.com/yistLin/pytorch-dual-learning.
14
Published as a conference paper at ICLR 2020
C Experiments on Rnmt
C.1 Identical Set of Experiments as Transformer
We show experiments on Rnmt in Table 9 and 10, which shows the consistent trending as
Transformer-based experiments. These results suggest that MGnmt is architecture-free, which can
theoretically and practically be adapted to arbitrary sequence-to-sequence architecture.
C.2 Comparison with Gnmt in Its Original Setting.
The lack of official Gnmt codes and their manually created datasets makes it impossible for us to
directly compare MGnmt with Gnmt in their original setting. This is why we initially resorted
to standard benchmark datasets. Nevertheless, we try to conduct such comparisons (Table 11).
We followed Shah & Barber (2018) to conduct English-French experiments. The parallel data are
provided by Multi UN corpus. Similar to Shah & Barber (2018), we created a small, medium and
large amount of parallel data, corresponding to 40K, 400K and 4M sentence pairs, respectively. We
created validation set of 5K and test set of 10K sentence pairs. For non-parallel data, we used the
News Crawl articles from 2009 to 2012. Note that in Shah & Barber (2018), there is a monolingual
corpora consisting 20.9M monolingual sentences used for English, which is too large and time-
consuming. Here we used 4.5M monolingual sentences for English and French, respectively. As
shown in Table 11, MGnmt still outperforms Gnmt.
Table 9: BLEU scores on low-resource translation (WMT16 EN什Ro), and cross-domain translation
(IWSLT EN什DE).
Model	Low-Resource		Cross-Domain (para. Ted & mono. News)			
	Wmt16EnoRo		Ted		News	
	En-Ro	Ro-En	En-De	De-En	En-De	De-En
Rnmt	-29.3^^	^^29.9	-23.1	28.8	13.7	16.6
Gnmt	30.0	30.7	23.4	29.4	13.8	16.9
GNMT-M-SSL + non-parallel	-3T6^^	-^325-	-23.6	29.6	17.5	22.0
RNMT-BT + non-parallel	31.0	31.7	23.7	29.9	16.9	21.5
RNMT-JB T + non-parallel	31.7	32.3	24.0	30.1	17.6	22.1
RNMT-DUAL + non-parallel	31.9	32.5	23.4	29.6	17.3	21.9
RNMT-LM-FUSION + non-parallel	29.5	30.3	-	-	14.1	17.0
MGnmt	-30.4^^	-^3T2-	-23.7	29.8	13.8	17.0
MGNMT + non-parallel	32.5	32.9	24.2	30.4	18.7	23.3
D Case Study
As shown in Table 12, we can see that without being trained on in-domain (News) non-parallel
bilingual data, the baseline Rnmt shows obvious style mismatches phenomenon. Although all the
enhanced methods alleviate this domain inconsistency problem to some extent, MGnmt produces
the best in-domain-related translation.
Table 10: BLEU scores on resource-rich language pairs. We report results of Newstest2014 testset
for WMT14, and MT03 testset for NIST.
Model	Wmt14		Nist	
	En-De	De-En	EN-ZH	ZH-EN
Rnmt (Bahdanau et al., 2015)	21.9	26.0	31.77	38.10
Gnmt (Shah & Barber, 2018)	22.3	26.5	32.19	38.45
Gnmt-M-Ssl + non-parallel (Shah & Barber, 2018)	24.8	28.4	32.06	38.56
RNMT-B T + non-parallel (sennrich et al., 2016b)	23.6	27.9	32.98	39.21
Rnmt-JBT + non-parallel (Zhang et al., 2018)	25.2	28.8	33.60	39.72
MGnmt	22.7	27.9	32.61	38.88
MGNMT + non-parallel	25.7	29.4	34.07	40.25
15
Published as a conference paper at ICLR 2020
Table 11: BLEU scores of RNMT-based experiments on English-French using similar settings as
Shah & Barber (2018). Numbers in parentheses are quoted from GNMT paper. Note that because
we used 4.5M English monolingual sentences instead of the original 20.9M (too time-consuming),
the reproduced results of “Gnmt-M-Ssl” are a bit lower.
Model	40K		400K		4M		avg.	∆
	En-Fr	Fr-En	EN-FR	Fr-En	En-Fr	Fr-En		
Rnmt	11.86	12.30	27.81	28.13	37.20	38.00	25.88	0
Gnmt	12.32(12.47)	13.65(13.84)	28.69(28.98)	29.51(29.41)	37.64(37.97)	38.49(38.44)	26.72	0.84
GNMT-M-SSL + non-parallel	18.60 (20.88)	18.92 (20.99)	35.90(37.37)	36.72(39.66)	38.75(39.41)	39.30(40.69)	31.37	5.49
MGnmt	12.52	14.02	29.38	30.10	38.21	38.89	27.19	1.31
MGNMT + non-parallel	19.25	19.33	36.73	37.57	39.45	39.98	32.05	6.17
Table 12: An example from Iwslt De-En cross-domain translation. In this case, all the models
were first trained on parallel bilingual data from Ted talks (Iwslt20 1 6), and exposed to non-
parallel bilingual data of News domain (News Crawl).
Source	“Die BenZinSteuer ist einfach nicht ZukunftSfahig"，So Lee Munnich, ein ExPerte fur Verkehrsgesetzgebung an der Universitat Von Minnesota.
Reference	“The gas tax is just not sustainable"，said Lee Munnich, a transportation policy expert at the University of Minnesota.
Rnmt	“The gasoline tax is simply not sustainable," so Lee Munnich, an expert on the University of Minnesota.
RNMT-BT	“The gas tax is simply not sustainable," So Lee Munnich, an expert on traffic legislation at the University of Minnesota .
Rnmt-JB T	“The gas tax is just not sustainable," say Lee Munnich, an expert on traffic legislation at the University of Minnesota .
MGnmt	“The gas tax is just not sustainable," said Lee Munnich, an traffic legislation expert at the University of Minnesota.
16