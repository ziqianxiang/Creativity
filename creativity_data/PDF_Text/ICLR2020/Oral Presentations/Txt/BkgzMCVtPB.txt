Published as a conference paper at ICLR 2020
Optimal Strategies
Against Generative Attacks
Roy Mor
Tel Aviv University
Tel Aviv, Israel
Erez Peterfreund
The Hebrew University of Jerusalem
Jerusalem, Israel
Matan Gavish
The Hebrew University of Jerusalem
Jerusalem, Israel
Amir Globerson
Tel Aviv University
Tel Aviv, Israel
Ab stract
Generative neural models have improved dramatically recently. With this progress
comes the risk that such models will be used to attack systems that rely on sensor
data for authentication and anomaly detection. Many such learning systems are
installed worldwide, protecting critical infrastructure or private data against mal-
function and cyber attacks. We formulate the scenario of such an authentication
system facing generative impersonation attacks, characterize it from a theoretical
perspective and explore its practical implications. In particular, we ask fundamen-
tal theoretical questions in learning, statistics and information theory: How hard
is it to detect a “fake reality”? How much data does the attacker need to collect
before it can reliably generate nominally-looking artificial data? Are there optimal
strategies for the attacker or the authenticator? We cast the problem as a maximin
game, characterize the optimal strategy for both attacker and authenticator in the
general case, and provide the optimal strategies in closed form for the case of
Gaussian source distributions. Our analysis reveals the structure of the optimal
attack and the relative importance of data collection for both authenticator and at-
tacker. Based on these insights we design practical learning approaches and show
that they result in models that are more robust to various attacks on real-world
data.
1	Introduction
Generative models have attracted considerable attention since the introduction of Generative Adver-
sarial Networks (Goodfellow et al., 2014a). Empirically, GANs have been shown to generate novel
data instances that resemble those in the true distribution of the data. The success of GANs also
comes with the risk that generative models will be used for attacking sensor-based security systems.
One example is identity authentication systems, where an individual is identified via her images, and
GANs might be able to generate such images to gain access (Thies et al., 2016). Another is anomaly
detection systems protecting critical infrastructure. As demonstrated by recent cyber-attacks (no-
tably the Stuxnet attack) sensors of these systems can be hijacked, so that GANs can be used to
generate “normal” looking activity while the actual system is being tampered with. The latter is, in
fact, a new form of a man-in-the-middle attack.
Our goal here is to construct a theoretical framework for studying the security risk arising from
generative models and explore its practical implications. We begin with a simple key insight. If the
attacker (i.e., the generative model) has unlimited observations of the source it is trying to imitate, it
will be able to fool any authenticator. On the other hand, if the attacker has access to fewer sensor
observations than the number of fake observations it needs to generate, it seems intuitively clear that
it cannot always succeed (as we indeed prove in Sec. 4). Therefore, the optimal defense and attack
strategies depend crucially on the amount of information available to the attacker and authenticator.
Motivated by the above insight, we cast the authentication setting as a two-player maximin game
(authenticator vs. attacker) where all observations are finite. Specifically, there are three key obser-
1
Published as a conference paper at ICLR 2020
vation sets to consider: those available to the attacker, those that the attacker needs to generate, and
those available to the authenticator when designing the system. Our goal is to understand how these
three information sources determine the optimal strategies for both players. Under the realistic as-
sumption that cyber attackers are sophisticated enough to play optimal or close to optimal strategies,
a characterization of the maximin authentication strategy can be of significant value.
We prove several theoretical results characterizing the optimal strategy for both players. These
results highlight the role played by the available observations as well as the functional form for an
optimal attacker and authenticator. We refer to the setting above as “GAN in The Middle” (GIM)
due to its similarity to “man in the middle” attacks. After describing our theoretical results, we
show how to learn both authenticator and attacker policies in practice, where both are based on
neural architectures. Our GIM method can be applied to multiple practical problems. The first is
building an authentication system that is robust to impersonation attacks. The second is building a
data generating mechanism that can generate novel data instances. Finally, we evaluate the method
empirically, showing that it outperforms existing methods in terms of resilience to generative attacks,
and that it can be used effectively for data-augmentation in the few-shot learning setting.
2	Problem statement
We begin by motivating our problem and formulating it as a two-player zero-sum game. As a sim-
ple illustrative example, consider a face authentication security system whose goal is to maximize
authentication accuracy. The system is initialized by registering k images of an individual θ (the
“source”), whose identity is to be authenticated. At test-time, each entity claiming to be θ is re-
quired to present to the system n of its images, and the authentication system decides whether the
entity is θ or an impersonator. We let m denote the maximum number of “leaked” images any at-
tacker obtained. We observe that if an attacker obtained men images of θ, it can present n of those
images. Thus the observations generated by the attacker are indistinguishable from ones generated
by θ, leading to failure of the authentication system (see Sec. 4.2 below). Hence, the number of
images ofθ that the attacker obtains and the size of the authentication sample are of key importance.
We now turn to formally stating the problem.
Notation. The set of possible observations is denoted by X . Let H denote the known set of
possible sources θ, where each source θ P H is defined by a probability density fθ, and an ob-
servation of a source θ P H is an X -valued random variable with density fθ. We assume that
subsequent observations of the source are IID, so that n sequential observations have density
fθnq(xι,..., Xn) ：“ ΠNι fθ (xi). We allow θ to be sampled from a known distribution Q on
H and denote the corresponding H-valued random variable by Θ. In what follows we will denote
the number of observations leaked to the attacker by m, the number of “registration” observations
available to the authenticator by k, and the number of observations required at authentication by n
(these may be generated by either the attacker or the true source).
The Authentication Game. The game begins with a random source Θ being drawn from H accord-
ing to Q. The authenticator first receives information about the drawn source, and then chooses a
decision rule for deciding whether a given test sequence x P Xn is an authentic sequence of obser-
pnq
vations sampled from fθ or a fake sequence generated by the attacker. Formally, the authenticator
learns about the source by seeing k IID “registration” observations A “ A1, . . . , Ak „ fθpkq. The
set of all possible decision rules is then D : Xk X Xn → {0,1} (where a decision of 1 corresponds
to the true source and 0 to an attacker). After the authenticator fixes its strategy, the attacker can
seek the best attack strategy. We assume that the attacker has access to m “leaked” IID observations
Y “ Y1 , . . . , Ym „ fθpmq as information about the source θ. Then it generates an attack sequence
X P Xn and presents it to the authenticator, which uses its decision rule to decide whether X is an
authentic sequence of observations sampled from fθpnq or a fake sequence generated by the attacker.
Formally, the strategy set of the attacker is all functions G : Xm → ∆(Xn), where ∆(Xn) is the set
of probability distributions over Xn, and gX|Y is the associated conditional probability density. We
note that the set H, the parametric family fθ, and the prior probability Q are known to both players.
Also, note that the leaked sample Y revealed to the attacker is not available to the authenticator, and
the “registration” sample A is not available to the attacker.
The goal of the authenticator is to maximize its expected accuracy, and the goal of the attacker is
to minimize it (or equivalently maximize its success probability). We define the utility (payoff) of
2
Published as a conference paper at ICLR 2020
the game as the expected prediction accuracy of the authenticator. To define expected accuracy we
consider the case of equal priors for attack and real samples.1 Formally, for a pair of strategies
pD, Gq and a specific source θ, the expected accuracy of the authenticator is then:
V (θ, D, G) “ 2 EA„f Pkq EY „f pmq ”EX„f Pnq [D(A,X)] + Ex„g(y)□ ´ D(A,X)]]	(2.1)
Since this utility only depends on G in the second term, minimizing it is equivalent to G maximizing
its success probability. To obtain the overall utility for the authenticator, we take the expected value
w.r.t Θ and define V(D, G) “ Eθ〜QV(Θ, D, G). Finally, We arrive at the following maximin game:
Vgame “ max min V (D, G)	(2.2)
game	DPD GPG
where D, G are the sets of all possible authenticator and attacker strategies, respectively. In Sec. 4
we show that this game has a Nash equilibrium, we characterize the optimal strategies and game
value in general, and find them in closed form for the case of Multivariate Gaussian sources.
3	Related work
Adversarial hypothesis testing (AHT): Hypothesis testing (HT) is a rich field in statistics that
studies how one can detect whether a sample was generated by one of two sets of distributions. A
variant of HT that is related to our work but distinct from it is AHT (e.g., see Brandao et al., 2014;
Barni & Tondi, 2013b;a; 2014; Bao et al., 2011; Zhou et al., 2019; Bruckner & Scheffer, 2011;
Bruckner et al., 2012). These works describe an HT setting where the sample is generated by one of
two hypotheses classes, but is then modified by an adversary in some restricted way. E.g., in Barni
& Tondi (2013b;a; 2014) the adversary can change the sample of one class up to a fixed distance
(e.g., Hamming). Given the quality of current generative models and the rapid pace of progress,
when considering an impersonation attack, it seems that the only relevant restriction one can assume
on an attacker is on the information it has. This is not captured by prior work since it assumes that
the adversary has a restricted strategy set. In contrast, our work considers a novel problem setting
where both players are not limited in their strategy set in any way. This leads to a novel analysis that
focuses on the dependence on the finite information available to each player (m, n, k).
Adversarial Examples: It has been observed (Goodfellow et al., 2014b) that deep learning models
can be very sensitive to small changes in their input. Such “misleading” inputs are known as ad-
versarial examples and much recent work has analyzed the phenomenon (Ilyas et al., 2019; Shamir
et al., 2019; Zhang et al., 2019), addressed the problem of robustness to these (Moosavi-Dezfooli
et al., 2016; Papernot et al., 2017; Yuan et al., 2017), and studied when robusntess can be certified
(Raghunathan et al., 2018; Wong et al., 2018). The setting of robust classification in the presence of
adversarial examples can also be thought of as a specific case of AHT (see above), where a classi-
fier is required to predict the class of an observation that could have been perturbed by a restricted
adversary (Wong et al., 2018; 2019) or generated by an adversary limited to generating examples
that will be classified correctly by humans (Song et al., 2018). In contrast, in our setting the attacker
is not limited in any way, nor does it have another utility in addition to impersonating the source.
Furthermore, in adversarial examples, there is no notion of limited information for the adversary,
whereas our work focuses on the dependence of the game on the information available to the players
(sample sizes n, m, k).
GAN: The GAN model is a game between a generator and a discriminator. While our concept of
generative attacks is inspired by GAN, it is very different from it: a successful GAN generator is
not necessarily a successful attacker in our setting, and vice-versa (e.g., given sufficiently expressive
generators and discriminators, GANs can “memorize” the training data, and thus the discriminator
will perform at chance level.2 Such a discriminator will not be useful as a defense against generative
attacks). Unlike GANs, in our setting, sample sizes are of key importance. Thus, our attacker will
not memorize the data it sees, as this will be detected when generating n > m examples.
Conditional GANs: In conditional GANs (Mirza & Osindero, 2014) the generator uses side infor-
mation for generating new samples. The attacker in our approach (analogous to GAN generator) has
input, but this input is not available to the authenticator (analogous to GAN discriminator). Thus,
1We give equal prior probability to attack and real samples for simplicity and clarity, and since this is
common in GAN formulations. All results can be trivially generalized to any prior probability for attack.
2If the generator is not expressive enough, the learned GAN may have small support (Arora et al., 2017).
3
Published as a conference paper at ICLR 2020
the objective of the learning process is fundamentally different.
Few-shot learning and generation: Our work relates to few-shot learning (Snell et al., 2017;
Vinyals et al., 2016; Finn et al., 2017; Lake et al., 2011; Koch et al., 2015) and few-shot genera-
tive models (Rezende et al., 2016; Zakharov et al., 2019; Lake et al., 2015; Edwards & Storkey,
2017; Hewitt et al., 2018) in the sense that both authenticator and attacker need to learn from a
limited set of observations. However, in our setting the authenticator is required to predict whether
a sample came from the true source or an attacker impersonating the source while taking into con-
sideration the amount of information both players have. These are notions that are not part of the
general few-shot learning setup. Also, in prior work on few-shot generation, the generator is either
measured through human evaluation (Lake et al., 2015) or trained to maximize the likelihood of its
generated sample (Rezende et al., 2016; Edwards & Storkey, 2017; Hewitt et al., 2018). In contrast,
in our setting the attacker’s objective is to maximize the probability that its generated sample will be
labeled as real by an authenticator. To this end, we show that the attacker must consider the sample
sizes m, n, k, which the generative models in prior work do not account for. Furthermore, we show
in Sec. F.4, and in Figures 1c,6, that the maximum likelihood (ML) solution is indeed sub-optimal
in our setting.
Image to image translation: Several GAN models have been introduced for mapping between two
domains (Zhu et al., 2017; Huang et al., 2018; Isola et al., 2017; Wang et al., 2017; Park et al., 2019).
This relates to our work since the attacker also needs to learn to map the leaked sample to an attack
sample. However, in our setting the mapping is not to a different domain but rather to other images
from the same distribution, which results in a different objective.
Data Augmentation: Generative models have also been used for augmenting data in supervised
learning, and in particular few-shot learning (Koch et al., 2015; Snell et al., 2017; Vinyals et al.,
2016; Lake et al., 2011). One such approach is Data Augmentation GAN (DAGAN) (Antoniou
et al., 2018), which takes as input an image and generates a new image. It relates to our framework
in the limited case of m “ 1, n “ 2, k “ 1, in the sense that the generator’s objective is to map one
image to two. However, in DAGAN the only goal of the discriminator is to improve the generator,
and the generator is limited to the functional form of adding a new image to the existing one, which
is a sub-optimal attack strategy, as can be seen from the Gaussian case of our problem.
4	Theoretical results
In this section, we study the game defined in Eq. 2.2. First, in Sec. 4.1 we show the existence of a
Nash equilibrium and characterize the optimal strategies for both players. Specifically, we show that
the optimal attacker strategy minimizes a certain divergence between the source and the attacker’s
conditional distribution ofX given A. Next, Sec. 4.2 shows that when there are more leaked samples
m than generated samples n, the authenticator will fail. Finally, in Sec. 4.3 we provide a closed-form
solution for both attacker and authenticator for the case of multivariate Gaussian distributions and
analyze the effect of the dimension of the observations and the sample sizes m, n, and k. Proofs are
provided in the appendix.
4.1	Characterizing the optimal strategies
We begin by showing that the game defined in Eq. 2.2 admits a Nash equilibrium. Namely, Theorem
4.1	below shows that there exists a pair of strategies (D*, G*) that satisfy:
max min V(D, G) “ min max V(D, G) = V(D*, G*)
Theorem 4.1. Consider the attacker G* defined by:
(4.1)
gX∣Y P argminEa~∕a
gX|Y
|fX|A(x|A) ´ gX|A(x|A)| dx
xPXn
(4.2)
Where, fA paq
∖θpH Q(θ)fθkq(a)dθ is the marginal density of A, Qθ∣a
is the poste-
rior over H given A, and fγ∣A(y∣a) “ Mh fθmq(y)Qθ∣A(θ∣a)dθ. Also, fχ∣A(x∣a) “
∖θpH fθnq(x)Qθ∣A(θ∣a)dθ and gχ∣A(x∣a) “ Jχm gχ∣γ(x∣y)fγ∣A(y∣a)dy are the conditional
densities of X given A, generated by the source and the attacker respectively. Consider the au-
thenticator defined by D*(a,x) “ IlfX ∣A(x∣a) > gX∣A (x|a)], where I is the indicator function.
Then (D*, G*) is a solution ofEq. 2.2 that satisfies Eq. 4.1.
4
Published as a conference paper at ICLR 2020
The proof (see Sec. D) follows by first showing that since Dpa, xq P t0, 1u, it holds that for any G, the
optimal authenticator strategy is a MAP test between the two hypotheses (true source or attacker).
We then show that given D* ,the game objective for G becomes Eq. 4.2. Namely, the optimal attacker
minimizes the '1 distance over the space Xk X Xn between the true source's conditional distribution
of X given A, and its own. Therefore, since the proposed G* minimizes Eq. 4.2 by definition, it
holds that ming V(D*, G) = V(D*, G*) “ maxD V(D, G*) and it follows that D*, G* satisfy
Eq. 4.1.
4.2	Replay Attacks： Authentication failure for n ≤ m
When n ≤ m, the attacker generates a number of observations that is at most the number of ob-
servations it has seen. intuitively, an optimal attack in this case, is to simply “replay” a subset of
size n from the m observations. This is known as a replay-attack (Syverson, 1994). This subset
constitutes an iiD sample of length n of the observed source, and is, therefore, a legitimate “fresh”
sample. in this case, it seems like the attack cannot be detected by the authenticator. indeed it
is easy to show using Theorem 4.1 that this attack is optimal and therefore for n ≤ m We have:
maxDPD minGPG V(D, G) “ 0.5 (see Sec. E)
4.3	The Gaussian case
We now turn to the case of multivariate Gaussian distributions where we can find the exact form of
the attacker and authenticator, providing insight into the general problem. Specifically, we consider
the setting where the source distributions are d-dimensional multivariate Gaussians with an unknown
mean and known covariance, and the prior Q over H is the improper uniform prior.3 We assume
n > m to keep the problem non-trivial. Let the observations be d-dimensional Gaussian vectors with
a known covariance matrix Σ P RdXd and an unknown mean vector θ P Rd. The set of possible
sources H becomes Rd, the Gaussian mean vectors. For any sample of n examples Z P Rn**,
we let zi denote the i’th example, and N “ n EL% denote the sample mean. Finally, for any
V P Rd, B P Rdf we define }v}B “ VTBv. The following theorem gives a closed-form solution
for both attacker and authenticator for the game defined in Eq. 2.2.
Theorem 4.2. Define δ “ m{n ≤ 1 and let P “ m{k. Consider the attacker G* defined by the
following generative process: Given a leaked sample Y P Rm ** G * generates a sample X P RnXd
as follows: it first samples n vectors W1, . . . , Wn „iid N(0, Σ) and then sets: Xi “ Wi ´ WN ` YN .
Define the authenticator D* by:
D*(a,x) = I }x—训∑T Vd(1'P))'1 'ρδ)log ^ρ-'4)	(4.3)
Σ	n(1 ´ δ)	ρ ` δ
Then (D*, G*) is a solution of Eq. 2.2 that satisfies Eq. 4.1.
The proof (see Sec. F) starts by showing that Va > 0, given D(a, x) = I[}x — a}∑τ V α], the
optimal strategy for G is to set xN = yN with probability 1 (as done in G*). To prove this, we first use
the Prekopa-Leindler inequality (Prekopa, 1973) to show that in this case G's maximization objective
is log-concave. We then show that any G that satisfies xN = yN with probability 1 is a local extremum,
and since the objective is log-concave it follows that it is the global maximum. We continue by
showing that given G*, the proposed D* is optimal. To do so, we first find the distribution of G*'s
attack sample using the Woodbury matrix identity, and then show that D* is indeed the optimal
decision rule. Finally, using the max-min inequality, this implies that (D*, G*) satisfy Eq. 4.1.
There are several interesting observations about the above optimal strategies. Perhaps the most
intuitive strategy for the attacker would have been to sample n elements from a Gaussian with mean
YN and the known covariance Σ. in expectation, this sample would have the correct mean. However,
this turns out to be sub-optimal, as can be seen in Figures 1c and 6 (we refer to this as an ML attack.
See Sec. F.4 in the appendix for the derivation and visualizations). instead, the optimal attacker
begins by drawing an iiD sample W from a Gaussian distribution with mean 0, and then “forces”
the sample mean to be exactly YN by shifting the sample points by YN — WN . This optimal attacker
3Similar results can be derived for the conjugate prior case, namely, proper Gaussian priors.
5
Published as a conference paper at ICLR 2020
strategy can be viewed as matching the sufficient statistics of the leaked sample Y in the generated
sample X. The optimal authenticator is a MAP test for the optimal attacker, as in Theorem 4.1.
As a corollary to Theorem 4.2 We obtain the value of the game (i.e.,the accuracy of D*).
Corollary 4.3. Define δ and ρ as in Theorem 4.2. Then the game value for the Gaussian case is:
1	+	1	„Y	^d	dp1 +	P	log 山)´ Y ^d	dpδ + P log 山 Y	(4 4)
2	+ 2ΓP d q	[Y]2,	2P1 ´	δ) g δ + P) Y12,	2P1 ´ δ) g δ + P)	(.)
Where γ is the lower incomplete Gamma function, and Γ is the Gamma function.
The proof (see Sec. F) follows by showing that the test statistic used by D* is Gamma distributed.
Fig. 1 demonstrates several interesting aspects of the above results. First, Fig. 1a shoWs that the
authenticator accuracy improves as n (the size of the test sample) grows. Furthermore, accuracy
also improves as the dimension d grows, meaning that for a specified authentication accuracy, the
required ratio n{m becomes smaller with the dimension d. This is a very encouraging result since
although this dimensional dependency is proved only for Gaussian sources, it suggests that for real-
world high-dimensional sources (e.g., faces, video, voice, etc.) the authenticator can achieve high
accuracy even when requiring a small (and practical) authentication sample.
Intuitively it may seem like authentication is impossible when the authenticator has less data than the
attacker (i.e., m > k). Surprisingly, this is not the case. As can be seen in Fig. 1b, even when m > k,
the authenticator can achieve non trivial accuracy.4 An intuitive explanation of this phenomenon is
that the test statistic used by the authenticator is >X — A>, which, due to the variance in the attacker
estimation, has higher variance when X is generated by an attacker than when X is generated by
the true source. This, in turn, allows the authenticator to discriminate between the hypotheses.
A closed-form solution for the general case remains an open problem. We believe that solving
for Gaussians is an important step forward, since it exposes interesting structural properties of the
solution, which we use in practice. Furthermore, if G has an encoder-decoder structure, it is not
unreasonable that the source distribution in latent space can be approximately Gaussian (as in VAE).
5 GAN in the Middle Networks
So far we explored the general formalism of authentication games. Here we consider specific archi-
tectures for D and G. As in GAN based models (Mirza & Osindero, 2014; Mescheder et al., 2018;
Karras et al., 2018a;b), we use neural nets to model these, while using insights from our theoretical
analysis. Below we provide implementation details for the GIM model (see Sec. H and code for
more details). In our analysis, we considered the non-differentiable zero-one loss since it is the real
accuracy measure. In practice, we will use cross-entropy as used in most GAN approaches.
Authenticator Architecture: The authenticator is implemented as a neural network DPa, xq that
maps from a source information sample a P Xk and a test sample x P Xn to a probability that
the test sample came from the true source. Our framework does not restrict the authenticator to any
specific function type, but in practice one must implement it using some model. We recall that our
theoretical results do suggest a certain functional form. The Gaussian results in Sec. 4.3 show that
the optimal authenticator is a test on the sufficient statistic of the source parameters. Motivated by
this result, and in the spirit of Siamese networks (Koch et al., 2015; Chopra et al., 2005), we con-
sider the following form for the authenticator. We define a function TD that maps a sample to a fixed
sized vector, analogous to the sufficient statistic in the theorem. We apply TD to both a and x. Then,
these two outputs are used as input to a comparison function σ which outputs a scalar reflecting their
similarity. Thus the authenticator can be expressed as: DPa, xq “ σPTD Paq, TDPxqq.
Attacker Architecture: The attacker is implemented as a stochastic neural network GPyq that maps
a leaked sample y P Xm to an attack sample x P Xn. Our theoretical results suggest a certain
functional form for this network. The Gaussian analysis in Sec. 4.3 shows that the optimal attacker
generates a sample whose sufficient statistic matches that of the leaked sample. Motivated by this
result, we consider the following functional form for the attacker. First, it applies a function TG that
maps the leaked sample Y to a fixed sized vector TGPY q, analogous to the sufficient statistic in the
4See Sec. C in the appendix, for additional figures showing that as the dimension grows, the expected
accuracy goes to 1.
6
Published as a conference paper at ICLR 2020
theorem. It then draws n random latent vectors W1 , . . . , Wn and matches their mean to the leaked
sufficient statistic to obtain the latent vectors Wi1. Namely it sets Wi1 “ Wi — W + TG PY) as done
in Theorem 4.2. Finally, it uses a decoder function φ that maps^each latent vector W[ to the domain
X. Thus, the attacker can be expressed as: G(Y)i “ φ(Wi — W + TG(Y)) @i P [n].
Optimization Details: Each iteration begins when a source θ is chosen randomly from the set of
sources in the training dataset (e.g., a person to be authenticated). Samples A, Y, Xθ are drawn from
the set of examples available for θ , where Xθ represents a test sample from θ . Then, given a leaked
sample Y, the generator G generates a fake sample XG , passes it to D and suffers the appropriate
loss. Finally, D receives as input the source information sample A, outputs a prediction for each of
the test samples Xθ , XG , and suffers the appropriate loss. Optimization is done via gradient ascent
on authenticator parameters and descent on attacker parameters, as is typical for GAN problems.
6 Experiments
We next evaluate our method empirically. In all experiments, we use the model described in Sec. 5.
We optimize the model with adversarial training, using the loss suggested by Mescheder et al. (2018)
and Adam (Kingma & Ba, 2015). Our implementation is available at https://github.com/
roymor1/OptimalStrategiesAgainstGenerativeAttacks. Also, see Sec. H for fur-
ther implementation details.
Gaussian sources: For the case of Gaussian sources, we arrived at a complete characterization
of the solution in Sec. 4.3. Thus, we can learn the models using our GIM algorithm and check
whether it finds the correct solution. This is important, as the GIM objective is clearly non-convex
and GANs are generally hard to train in practice and lack convergence guarantees (Mescheder et al.,
2018). We ran all experiments using a multivariate Gaussian with Σ “ Id , and in each game the
source mean was drawn from the prior distribution Q “ N(0, 10Id). This approximates the im-
proper uniform prior since the prior has much larger variance than the sources. Fig. 1a shows the
empirical game value compared with the theoretical one as a function of the test sample size n, for
fixed m “ 1, k “ 10, and different values of d. It can be seen that there is an excellent fit between
theory and experiment.
(a)	(b)	(c)
Figure 1: Game value (expected authentication accuracy) for the Gaussian case. (a) A comparison between
empirical and theoretical game value for different d values (m “ 1, k “ 10). Solid lines describe the theoretical
game values whereas the * markers describe the empirical accuracy when learning with the GIM model. (b)
Theoretical game value as a function of δ, ρ (see Corollary 4.3) for d “ 100. (c) Empirical accuracy of
an optimal authenticator against two attacks: the theoretically optimal attack G* from Theorem 4.2 and a
maximum likelihood (ML) attack (See Sec. F.4) for the Gaussian case. It can be seen that the ML attack is
inferior in that it results in better accuracy for the authenticator, as predicted by our theoretical results.
Authentication on Faces and Characters: We next evaluate GIM in an authentication setting on
two datasets: the VoxCeleb2 faces dataset (Nagrani et al., 2017; Chung & Zisserman, 2018), and
the Omniglot handwritten character dataset (Lake et al., 2015). Additional information about the
datasets, splits, and modeling details is provided in Sections G and H. Our goal is to check whether
the GIM authenticator is more robust to generative attacks than a state of the art authentication
system. To evaluate this, we consider several attackers: 1) A “random source” attacker (RS): a
naive attacker that ignores the leaked sample Y. It simply draws a random source from the dataset,
and samples n real images of that source. From the authenticator’s perspective, it’s equivalent to a
sample version of the verification task (Koch et al., 2015; Schroff et al., 2015; Deng et al., 2018), in
7
Published as a conference paper at ICLR 2020
Table 1: Accuracy of GIM and baselines against attacks. Avg acc denotes average over all attacks.
Authenticator	Dataset	m	n	k	RS	Replay	GIM	Avg acc
GIM	VoxCeleb2	1	5	5	0.897	0.837	0.822	0.852
ArcFace	VoxCeleb2	1	5	5	0.998	0.598	0.526	0.707
GIM	Omniglot	1	5	5	0.912	0.942	0.868	0.907
Siamese	Omniglot	1	5	5	0.994	0.509	0.785	0.763
which an agent is presented with a pair of real images and needs to decide whether they are from the
same source or not. 2) Replay attacker (Replay): an attacker which, upon seeing a leaked sample
Y , draws n random images of the leaked sample (with replacement). 3) A GIM attack, which is the
“worst case” attacker G, learned by our GIM model.
For VoxCeleb we compare the GIM authenticator to the ArcFace method (Deng et al., 2018), which
is currently state of the art in face verification. As a baseline for Omniglot, we use the Siamese
network suggested by Koch et al. (2015), which achieves state of the art in the verification task
on Omniglot. Results are shown in Table 1. It can be seen that on average across attacks, GIM
outperforms the baselines. The only attack for which GIM is inferior is RS. This is not surprising as
this is the objective that both baselines are trained for.
Qualitative evaluation of attacker: In Fig. 2 we provide images generated by the GIM attacker
for the test set of both Omniglot and Voxceleb. The images demonstrate qualitatively the strategy
learned by the attacker. In the Voxceleb2 dataset, face images are drawn from a video of the person
talking. Note that as in real samples from the data, the attack sample varies in pose and expression
and not in the background or clothing.
(a)
ψ
14
l÷
⅛
ψ
(b)
lψ
-i史Jn
T 丁 M
ŋ
广


m
ɔ
ɔ
□
ɔ
ɔ
ɔ


ɔ

Figure 2: Images generated by the GIM attacker based on one leaked image. In each row, the leftmost image
is the real leaked image, and the rest of the images are an attack sample generated by the GIM attacker. (a)
Voxceleb2 dataset. (b) Omniglot dataset.
Data augmentation: Finally, we use GIM for data augmentation in one-shot classification on Om-
niglot. This is done by using the GIM attacker to generate more data for a given class. We first
train GIM on the training set with parameters m “ 1, n “ 5, k “ 5. Then, during both training
and testing of one-shot classification, given an example, we use the attacker to augment the single
example available for each of the classes, by adding to it the n “ 5 examples our attacker generated
from it. We use Prototypical Nets (Snell et al., 2017) as the baseline model. We find that without
using our augmentation method, Prototypical Nets achieve 95.9% accuracy on the test split, and
with our method, they achieve 96.5%, which is similar to the improvement achieved by Antoniou
et al. (2018) with Matching networks (Vinyals et al., 2016) as the few-shot classification algorithm.
7 Conclusions
We defined the notion of authentication in the face of generative attacks, in which a generative model
attempts to produce a fake reality based on observations of reality. These attacks raise numerous in-
teresting theoretical questions and are very important and timely from a practical standpoint. We
proposed to study generative attacks as a two-person zero-sum game between attacker and authen-
ticator. In our most general setup both attacker and authenticator have access to a finite set of
8
Published as a conference paper at ICLR 2020
observations of the source. We show that this game has a Nash equilibrium, and we characterize
the optimal strategies. In the Gaussian version of the game, a closed form of the optimal strate-
gies is available. A nice outcome of the analysis is that the game value depends on m, n, k only
through their ratios δ “ m{n (i.e., the ”expansion ratio” between attack and leaked sample sizes)
and ρ “ m{k (i.e., the “information ratio” between the number of source observations available to
attacker and authenticator). As we show in Fig. 1b, there is a large range of values for which high
accuracy authentication is possible, and as d grows we observe that the high authentication accuracy
region in the pδ, ρq plane grows sharply. We introduce the GIM model, which is a practical approach
to learning both authenticator and attacker, and whose structure is inspired by our analysis. GIM
achieves accuracy that is very close to the theoretical rates in the Gaussian case, and is also more
robust to attacks when compared to state of the art authenticators on real data. Many theoretical
and practical questions remain. For example, finding closed form optimal strategies for other distri-
butions, and going beyond IID generation. The non IID setting is of particular importance for the
problem of fake video (Thies et al., 2016) and audio (Arik et al., 2018) generation, which we intend
to study in the future.
Acknowledgements
This work has been supported by the Blavatnik Interdisciplinary Research Center (ICRC), the Feder-
mann Research Center (Hebrew University) and Israeli Science Foundation research grants 1523/16
and 1186/18.
References
Antreas Antoniou, Amos J. Storkey, and Harrison Edwards. Augmenting image classifiers using
data augmentation generative adversarial networks. In Artificial Neural Networks and Machine
Learning,pp. 594-603, 2018.
Sercan Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloning with a
few samples. In Advances in Neural Information Processing Systems, pp. 10040-10050, 2018.
Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium
in generative adversarial nets (GANs). In Proceedings of the 34th International Conference on
Machine Learning-Volume 70, pp. 224-232. JMLR. org, 2017.
Ning Bao, O. Patrick Kreidl, and John Musacchio. A network security classification game. In
Game Theory for Networks - 2nd International ICST Conference, GAMENETS 2011, Shang-
hai, China, April 16-18, 2011, Revised Selected Papers, pp. 265-280, 2011. doi: 10.1007/
978-3-642-30373-9∖.19. URL https://doi.org/10.1007/978-3-642-30373-9_
19.
Mauro Barni and Benedetta Tondi. Multiple-observation hypothesis testing under adversarial con-
ditions. In 2013 IEEE International Workshop on Information Forensics and Security, WIFS
2013, Guangzhou, China, November 18-21, 2013, pp. 91-96, 2013a. doi: 10.1109/WIFS.2013.
6707800. URL https://doi.org/10.1109/WIFS.2013.6707800.
Mauro Barni and Benedetta Tondi. The source identification game: An information-theoretic per-
spective. IEEE Trans. Information Forensics and Security, 8(3):450-463, 2013b. doi: 10.1109/
TIFS.2012.2237397. URL https://doi.org/10.1109/TIFS.2012.2237397.
Mauro Barni and Benedetta Tondi. Binary hypothesis testing game with training data. IEEE Trans.
Information Theory, 60(8):4848-4866, 2014. doi: 10.1109/TIT.2014.2325571. URL https:
//doi.org/10.1109/TIT.2014.2325571.
Fernando G. S. L. Brandao, Aram Wettroth Harrow, James R. Lee, and Yuval Peres. Adversarial
hypothesis testing and a quantum stein’s lemma for restricted measurements. In Innovations in
Theoretical Computer Science, ITCS’14, Princeton, NJ, USA, January 12-14, 2014, pp. 183-194,
2014. doi: 10.1145/2554797.2554816. URL https://doi.org/10.1145/2554797.
2554816.
9
Published as a conference paper at ICLR 2020
Michael Bruckner and Tobias Scheffer. Stackelberg games for adversarial prediction problems. In
Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining, San Diego, CA, USA, August 21-24, 2011, pp. 547-555, 2011. doi: 10.1145/
2020408.2020495. URL https://doi.org/10.1145/2020408.2020495.
Michael Bruckner, Christian Kanzow, and Tobias Scheffer. Static prediction games for adversarial
learning problems. J. Mach. Learn. Res., 13:2617-2654, 2012. URL http://dl.acm.org/
citation.cfm?id=2503326.
Sumit Chopra, Raia Hadsell, and Yann LeCun. Learning a similarity metric discriminatively, with
application to face verification. In IEEE Computer Society Conference on Computer Vision and
Pattern Recognition, pp. 539-546, 2005.
Arsha Nagrani Chung, Joon Son and Andrew Zisserman. Voxceleb2: Deep speaker recognition. In
Interspeech, 2018.
Jiankang Deng, Jia Guo, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep
face recognition. arXiv preprint arXiv:1801.07698, 2018.
Harrison Edwards and Amos Storkey. Towards a neural statistician. In International Conference on
Learning Representations, 2017.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. CoRR, abs/1703.03400, 2017. URL http://arxiv.org/abs/1703.
03400.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems, pp. 2672-2680, 2014a.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In International Conference on Learning Representations, 2014b.
Luke B Hewitt, Maxwell I Nye, Andreea Gane, Tommi Jaakkola, and Joshua B Tenenbaum. The
variational homoencoder: Learning to learn high capacity generative models from few examples.
In Proceedings of the Thirty-Fourth Conference on Uncertainty in Artificial Intelligence, pp. 988-
997, 2018.
Xun Huang and Serge J. Belongie. Arbitrary style transfer in real-time with adaptive instance nor-
malization. In IEEE International Conference on Computer Vision, pp. 1510-1519, 2017.
Xun Huang, Ming-Yu Liu, Serge Belongie, and Jan Kautz. Multimodal unsupervised image-to-
image translation. In Proceedings of the European Conference on Computer Vision, pp. 172-189,
2018.
Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander
Madry. Adversarial examples are not bugs, they are features. ArXiv, abs/1905.02175, 2019.
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. Image-to-image translation with con-
ditional adversarial networks. In The IEEE Conference on Computer Vision and Pattern Recog-
nition, pp. 5967-5976, 2017.
Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and
super-resolution. In European Conference on Computer Vision, 2016.
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for im-
proved quality, stability, and variation. In International Conference on Learning Representations,
2018a.
Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative
adversarial networks. arXiv preprint arXiv:1812.04948, 2018b.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations, 2015.
10
Published as a conference paper at ICLR 2020
Gregory Koch, Richard Zemel, and Ruslan Salakhutdinov. Siamese neural networks for one-shot
image recognition. In ICML deep learning workshop, volume 2, 2015.
Brenden M. Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua B. Tenenbaum. One shot learning
of simple visual concepts. In Proceedings of the 33th Annual Meeting of the Cognitive Science
Society, 2011.
Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning
through probabilistic program induction. Science, 350(6266):1332-1338, 2015.
Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for GANs do
actually converge? In Proceedings of the 35th International Conference on Machine Learning,
pp. 3478-3487, 2018.
Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint
arXiv:1411.1784, 2014.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and
accurate method to fool deep neural networks. In Proceedings of the IEEE conference on com-
puter vision and pattern recognition, pp. 2574-2582, 2016.
A. Nagrani, J. S. Chung, and A. Zisserman. Voxceleb: a large-scale speaker identification dataset.
In Interspeech, 2017.
Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram
Swami. Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM
on Asia conference on computer and communications security, pp. 506-519, 2017.
Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic image synthesis with
spatially-adaptive normalization. CoRR, abs/1903.07291, 2019. URL http://arxiv.org/
abs/1903.07291.
Andras PrekoPa. On logarithmic concave measures and functions. Acta Scientiarium Mathemati-
carum, 34:335-343, 1973.
Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certified defenses against adversarial exam-
ples. In International Conference on Learning Representations, 2018.
Danilo Jimenez Rezende, Shakir Mohamed, Ivo Danihelka, Karol Gregor, and Daan Wierstra. One-
shot generalization in deep generative models. In Proceedings of the 33nd International Confer-
ence on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, pp. 1521-
1529, 2016. URL http://proceedings.mlr.press/v48/rezende16.html.
Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A unified embedding for face
recognition and clustering. In The IEEE Conference on Computer Vision and Pattern Recognition,
pp. 815-823, 2015.
Adi Shamir, Itay Safran, Eyal Ronen, and Orr Dunkelman. A simple explanation for the existence of
adversarial examples with small hamming distance. CoRR, abs/1901.10861, 2019. URL http:
//arxiv.org/abs/1901.10861.
Jake Snell, Kevin Swersky, and Richard S. Zemel. Prototypical networks for few-shot learning. In
Advances in Neural Information Processing Systems, pp. 4080-4090, 2017.
Yang Song, Rui Shu, Nate Kushman, and Stefano Ermon. Constructing unrestricted adversarial
examples with generative models. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31,
pp. 8312-8323. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/
8052-constructing-unrestricted-adversarial-examples-with-generative-models.
pdf.
Paul Syverson. A taxonomy of replay attacks [cryptographic protocols]. pp. 187 - 191, 07 1994.
ISBN 0-8186-6230-1. doi: 10.1109/CSFW.1994.315935.
11
Published as a conference paper at ICLR 2020
Justus Thies, Michael Zollhofer, Marc Stamminger, Christian Theobalt, and Matthias Nieβner.
Face2face: Real-time face capture and reenactment of rgb videos. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pp. 2387-2395, 2016.
Oriol Vinyals, Charles Blundell, Tim Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. Matching
networks for one shot learning. In Advances in Neural Information Processing Systems, pp. 3630-
3638, 2016.
Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro.
High-resolution image synthesis and semantic manipulation with conditional gans. CoRR,
abs/1711.11585, 2017. URL http://arxiv.org/abs/1711.11585.
Eric Wong, Frank Schmidt, Jan Hendrik Metzen, and J. Zico Kolter. Scaling provable adversarial
defenses. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, pp. 8400-8409. Curran Associates,
Inc., 2018.
Eric Wong, Frank R. Schmidt, and J. Zico Kolter. Wasserstein adversarial examples via projected
sinkhorn iterations. In Proceedings of the 36th International Conference on Machine Learning,
ICML 2019, 9-15 June 2019, Long Beach, California, USA, pp. 6808-6817, 2019. URL http:
//proceedings.mlr.press/v97/wong19a.html.
Xiaoyong Yuan, Pan He, Qile Zhu, Rajendra Rana Bhat, and Xiaolin Li. Adversarial examples:
Attacks and defenses for deep learning. CoRR, abs/1712.07107, 2017. URL http://arxiv.
org/abs/1712.07107.
Egor Zakharov, Aliaksandra Shysheya, Egor Burkov, and Victor S. Lempitsky. Few-shot adversarial
learning of realistic neural talking head models. CoRR, abs/1905.08233, 2019. URL http:
//arxiv.org/abs/1905.08233.
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P. Xing, Laurent El Ghaoui, and Michael I. Jor-
dan. Theoretically principled trade-off between robustness and accuracy. In Proceedings of the
36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach,
California, USA, pp. 7472-7482, 2019. URL http://proceedings.mlr.press/v97/
zhang19p.html.
Yan Zhou, Murat Kantarcioglu, and Bowei Xi. A survey of game theoretic approach for adversarial
machine learning. Wiley Interdiscip. Rev. Data Min. Knowl. Discov., 9(3), 2019. doi: 10.1002/
widm.1259. URL https://doi.org/10.1002/widm.1259.
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation
using cycle-consistent adversarial networks. In Proceedings of the IEEE international conference
on computer vision, 2017.
12
Published as a conference paper at ICLR 2020
A Introduction to the appendix
Here we provide additional information and proofs for the main paper. In Sec. B we provide a
visualization of the game setting. In Sec. C we plot the game value in the Gaussian case for different
parameter values. In Sec. D we provide a proof for Theorem 4.1, in Sec. E we formally state the
theorem discussed in Sec. 4.2 and prove it, and in Sec. F we prove Theorem 4.2 and also derive
the game value for the sub-optimal “ML attacker”. Finally, in Sections G, H we provide additional
details about the experiments and the implementation of GIM.
B Problem setup visualization
Our problem setup is illustrated in Fig. 3 for a face authentication scenario. A source θ (in this
case an individual) generates IID observations (images). k images are used by the authenticator to
study the source which it aims to authenticate. m images are “leaked” and obtained by an attacker
who wishes to impersonate the source and pass the authentication. At test time the authenticator
is presented with n images that were generated either by the true source θ, or by the attacker, and
decides whether the entity that generated the images was the source θ or an attacker.
Figure 3: An illustration of the game described in the main text. An authenticator receives a sample
of n images and needs to decide whether these were generated by a known source, or by an adversary
that had access to leaked images. In order to decide, the authenticator is supplied with a sample of
k images from the real source.
C Game value visualizations
In corollary 4.3 we provide the game value (the expected authenticator accuracy) for the case of
multivariate Gaussian sources. In this section, we present additional visualizations of the game
value as a function of the different parameters of the game. Fig. 4 visualizes the game value as a
function of δ and ρ, for different values of d (the observation dimension). δ “ mm is the “expansion
ratio” between the source information available to the attacker through the leaked sample and the
size of the attacker's attack sample. P “ 器 is the “information ratio” between the number of source
observations available to attacker and authenticator. One can clearly see that as d, the observation
dimension, grows large, so does the accuracy of the authenticator. Even for δ values higher than 0.5,
and ρ values which intuitively would give the attacker an advantage (e.g.,m “ 10).
13
Published as a conference paper at ICLR 2020
Fig. 5 visualizes the game value as a function of δ and , for different values of d (the observation
dimension). Where e “ k is the “information expansion” of the attacker With respect to the authen-
ticator source information. Again, one can clearly see that as d, the observation dimension, grows
large, so does the accuracy of the authenticator.
Figure 5: Game value as a function of δ, e for different dimensions d.
D Additional theorems and proofs for Sec. 4.1
We begin With some additional notation. Let
fA paq “	Qpθqfθpkq paqdθ
θPH
14
Published as a conference paper at ICLR 2020
denote the marginal density of A. Let Qθ∣a denote the posterior probability over H given A. That
is:
QΘ∣Apθlaq =
Q(θ)fθkq(a)	= QPθfkPa)
∖vph Q(V VqfVkPa)dv—	fAPa)
Also, let fY |A, fX|A, gX|A, denote the conditional densities defined by:
fY |APy|aq “
f	fθmq(y)Qθ∣A(θ∣a)dθ
θPH
fX|A(x|aq “ f	fθn (xqQΘ|A(θ|aqdθ
θPH
gX|A(x|aq “	gX|Y (x|yqfY |A(y|aqdy
yPXm
Lemma D.1. Let G be an attacker defined by the conditional probability distribution gX|Y. Then
@a P Xk, x P Xn a best response strategy for D is:
D(a,x) “ I[fχ∣A(x∣a) > gχ∣A(x∣a)S	(D.1)
Proof. Given an attacker strategy gX |Y , the objective for D is given by:
argmax Eθ〜QV(Θ, D, Gq
D
“ argmaxEθ~qEa〜fPkqlEX〜fPnq [D(A,X)S + EY„fPmqEχ~gχ∣γ(∙∣y)[l´ D(A,X)S]
“ argmax EA„fA ”EX „fx|AP-|A) rD(A,X qs + EY ~fγ ∣a(∙∣A)ex ~gχγ (∙∣Y )r1 ´ d(a,x qsl
“ argmax	fA(aq	fX|A(x|aqD(a, xq ` gX |A (x|aqr1 ´ D(a, xqs dxda
D	aPXk	xPXn
Note that D can be optimized independently for each pair a, x P Xk X Xn. Hence, @a, X P Xk X Xn
the objective is:
argmax	fX|A(x|aqD(a, xq ` gX|A(x|aqr1 ´ D(a, xqs
Dpa,x)Pt0,1u
And thus, the optimal decision rule for D is:
Dpa, X) = I [fX|A(x|a) > gx|A(x|aq]
As required.	□
Lemma D.2. @a, X P Xk X Xn let the Strategyfor D be defined by:
D(a, X) = I fX|A(X|a) > gX|A(X|a)
Then G * is a best response Strategyfor G, iff it minimizes the '1 distance between the distributions
fx∣A, gχ∣A over the space Xk X Xn. Namely:
gX* |Y
P argmin EA„fA
gX|Y
L
xPXn
IfXIA(XIA) — gXIA(XIA)I dx
(D.2)
15
Published as a conference paper at ICLR 2020
Proof. Let D be defined as in Eq. D.1, the objective for G is:
argmin 1 Eθ~qV(Θ, D, G)
G2
“ argminEθ~qEa~/PkqlEX„fPnq [D(A,X)S + EY„fPmqEχ~gχ∣γ(∙∣y)口 ´ D(A,X川
“ argminEa~∕a ∣Eχ~fx∣A(∙∣A)H(A,X)S + Ex~ox∣a(∙∣a)R ´ D(A,X川
gX|Y
“ argminʃ	dafA(a) ʃ	dx [fχ∣A(x∣a)D(a,x)+ gχ∣A(x∣a)[1 — D(a,x)]‰
“ argmin	dafA(a)	dx max fX|A(x|a),gX|A(x|a)
gX|Y	aPXk	xPXn
(D.3)
=argmin1 f	dafA(a) f dχ “fX|A(x|a) + gχ|A(x|a) + ∣fX|A(x|a) ´ gX|A(x|a)"
gX|Y 2 aPXk	xPXn
“argmin 1 + 1J 卜 dafA(a) ʃ	dx ∣fx|a(x|。)— gχ∣a(x∣α)∣
“argmin	dafA(a)	dx ∣fχ∣A(x∣a) ´ gχ∣A(x∣a)∣
gX|Y	aPXk	xPXn
As required. Where in Eq. D.3 We used the definition of D.	□
Theorem 4.1. Consider the attacker defined by:
gX|Y P argminEa~∕a
gX|Y
J
xPXn
∣fX∣A(xlA) ´ gX∣a(x1a)∣ dχ
(D.4)
and let G * be the corresponding map from Xm to the set of probability distributions over Xn.
Consider the authenticator defined by:
D* (a, x) = IlfX| A(x|a) > gX|A(x|a)]	(D.5)
where I is the indicator function. Then (D*, G*) is a solution of Eq. 2.2 that satisfies Eq. 4.1.
Proof. FromLemmasD.1, D.2 we have that maxD V(D, G*) = V(D*, G*) = ming V(D*, G),
from which it follows that Eq. 4.1 is satisfied and thus (D*, G*) is a solution ofEq. 2.2.	□
E	Theorem and proof for Sec. 4.2
Theorem E.1. For all n ≤ m it holds that:
max min V(D, G) = 0.5
DPD GPG
Proof. Consider the attacker Greplay defined by the following generative process: Given a leaked
sample Y P	Rmxd,	GrePIay generates a sample X P	RnXd SUch that	Xi	=匕	@i	P	[n].	(this is
possible since we assumed n ≤ m). Namely, we have:
n
gX|Y(x|y) =	δ(xi — yi)
i“1
16
Published as a conference paper at ICLR 2020
Where δ is the Dirac delta. Thus @a, X P Xk X Xn:
gX|A(x|aq “	dygX|Y (x|yqfY |A(y|aq
yPXm
“J 久 dygχ∣γ(x∣y)j *dθfθmq(y)Qθ∣A(θ∣a)
“ f	dθ f	dy ∏ δ(xi ´ yi)fθmq(y)Qθ∣A制。)
θPH	yPXm	i“1
“ f	dθQθ∣A(θla) f	dy立 δ(xi ´ yi)fθnq(y) f	dy1fθmfq(y1)
θPH	yPXn	i“1	y1PX m´n
“ f	dθQθ∣A(θla) f	dy ∏ δ(xi ´ yi)fθnq(y)
θPH	yPXn	i“1
“ f	dθQθ∣A(θ∣a)fθnq(χ)
θPH
“fX|A(x|aq
Define:
Do(a,x) = 0 @a,x P Xk X Xn
Then according to Theorem 4.1 pD0, Greplayq is a solution of Eq. 2.2 that satisfies Eq. 4.1, and there-
fore:
max min V pD, Gq “V pD0, Greplayq
DPD GPG
“ 1 Eθ~qEa5Pkq EY„pm) ∣Eχ-fPnqrD(A,X)S ' Ex~g(y)[1 ´ D(A,
“ 2 EΘ~QEA〜f Pkq EY „f pm) ”EX„f PnqrOs ' EX~G(Y )r1s]
1
“—
2
As required.
□
F Additional theorems and proofs for Sec. 4.3
F.1 Notation and definitions
In this section, we consider the case where the sources are d-dimensional Gaussian vectors with a
known covariance matrix Σ “ CCT P RdXd and unknown mean vector θ P Rd. That is, the set
of possible sources is H “ Rd and given θ P H the associated probability density over the domain
X “ RdiS fθ (χ) “ ?加"detp∑q∣ eχp (´2 (x ´ θqTςT(X ´ θ))
A sample of n examples X P Xn is considered a matrix X P Rnxd, where the first index represents
the sample and the second represents the observation space, Rd. I.e., Xij P R is the j’th element of
the i,th example in the sample X P Rn**.
We continue with a few more notations that simplify the proofs. Given a matrix X P Rnxd, we let
Xc “ “X1T, . . . , XTn ‰T P Rnd be the concatenation vector representing X. Given a vector θ P Rd,
we let θc,n = [θτ, ..., θT‰T P Rnd be the concatenation of n copies of θ. Given a matrix
X P Rnxd,welet X ≡ nn Xn“1 Xi P Rd denote its mean along the sample dimension. For any matrix
B P Rd**, we denote:
»B	0fi
diag(B,k) “	.. P Rkdxkd
0B
17
Published as a conference paper at ICLR 2020
and
»B…B
repPB,k) “	.	...	. P RkdXkd
B … B
Finally, we define strategies for both attacker and authenticator, which we prove in what follows to
be the optimal strategies for the game.
Definition F.1. Let G* denote an attacker defined by the following generative process: Given a
leaked sample Y P Rm** G * generates an attack sample X P RnXd as follows. Itfirst samples n
vectors W1, . . . , Wn „iid Np0, Σq and then sets:
Xi “ Wi — W ' Y	(F.1)
Also, let gX* |Y denote its associated conditional probability.
Definition F.2. For any ɑ P r`, let Da denote an authenticator defined as:
Da(a, X)= I ∣}x —训∑-1 V a]
Where I is the indicator function
F.2 Technical lemmas
Lemma F.3. Let Xι,...,Xk „ N(μ, Σ) Where μ P Rd, Σ P RdXd. Then
1k	1
X = k∑Xj 〜Npμ,k ς)
j“1
Proof. We begin by observing that Xc 〜N (μc,k, diag(Σ, k)). Let B = 1 [Id … IdsP RdXkd
and observe that X = BXc. Therefore, since this is an afine transformation of a Gaussian vector
we have:
X 〜N(Bμc,k, Bdiag(∑, k)BT) = N(μ, 1∑)
k
As required.	□
Lemma F.4. Let X P Rd be a Gaussian vector s.t X 〜N(μ, Σ), and let Xc,n P Rnd be the
concatenation of n copies of X. Then:
Xc,n 〜N(μc,n, rep(∑, n))
Proof. Let
»Id fi
B =	.	P RndXd
.
Id
and observe that Xc,n = BX. Therefore, since this is an affine transformation ofa Gaussian vector
we have:
Xc,n 〜N(Bμ, B∑Bτ) = N(μc,n, rep(∑, n))
As required.	□
Lemma F.5. Let θ P Rd, Σ P RdXd represent the mean and covariance of a Gaussian distribution.
Let X P RnXd be a random SamPIe generated by the attacker defined in Def. F1. Then:
Xc 〜N(θc,n, diag(∑, n) ' YeP((nm)∑, n)) ” N(θc,n, Ψ)
mn
18
Published as a conference paper at ICLR 2020
Proof. Observe that Wc 〜N(0,diag(Σ,n)). Using Lemma F.3 We get Y 〜N(θ, mlΣ) and
observe that Wc,n = rep( 1 Id, n)Wc. Using Lemma F.4 we get Yc,n 〜N(θc,n,*rep(Σ, n)). We
define the folloWing block matrices
Z “ IWnl
B = [Ind - 1 rep(Id, n), Ind]
and observe that:
Z 〜N( θnd
θc,n
diag(Σ, nq
0
Note that Xc “ Wc — W2,n ' 匕,n “ BZ and therefore we get:
X 〜 N(B 0nd B diag(Σ, nq
c	θc,n ,	0
“N
θc,n,diag(Σ,nq ` rep
((n-m )∑,n))
mn
As required.
□
Lemma F.6. Let Σ “ CCT P RdXd represent the covariance of a Gaussian distribution, and
consider the following covariance matrix:
Ψ “ diag(Σ, n) ' rep((n——m)Σ, n)
mn
Then its inverse is:
n—m
Ψ “ diag(Σ ,n)----K-rep(Σ , n)
n2
and the determinant is:
det(Ψ) “
´ - )d det(∑)
m
Proof. We begin by defining the following block matrices:
Σ
U “	： P RndXd
.
Σ
Id
V =(n-m)	.	P RndXd
mn ：
Id
(F：2)
(F：3)
19
Published as a conference paper at ICLR 2020
and note that rep(( n´1 )∑,n) “ UV T .Then:
Ψ-1 =PdiagpΣ,nq ' repPPn——m)Σ,n))-1
mn
“ pdiagpΣ,n ' UVT)T
“qdiagpΣ, n)´1 ´ diagpΣ, n)´1UpId ' VTdiagpΣ, n)´1U)—1VTdiagpΣ, n)´1
p==qdiαgpΣ~1, n) ´ diagpΣ~1, n)UpId ' VTdiagpΣ~1, n)u)´1VTdiagpΣ~1, n)
Id
“diagpΣ~1, nq ´	.
Id
pId `
n´m
nm
rId
…Ids
Id
.qτn´m [∑-1
. nm
Id
Σ-1
Id
“diagpΣ-1,n) ´ . midn´m [∑-1 … Σ-1‰
. n nm
Id
»Id fi
“diagpΣ-1,nq ´ 九 n21	.	[∑-1 ∙∙∙ ∑-1‰
Id
=diagpΣ-1, nq ´
n≡mreρp∑-1nq
n2
As required. Where in piq we used the Woodbury matrix identity, and in piiq we used the inverse of
a diagonal block matrix.
Next, we turn to find the determinant of Ψ:
detpψq =detpdiag(Σ,n) ' reppp———^)Σ,n))
mn
=detpdiagpΣ, nq ` UVTq
pi=iiqdetpdiagpΣ, nqqdetpId ` V T diagpΣ, nq-1Uq
p=ivqdetpΣqn detpId ` VTdiagpΣ-1,nqUq
Σ
nm
“detp∑q detpId '-[Id … Ids diagpΣ 1,n).)
nm	.
Σ
“det(Σ)ndet(Id ' n´mId)
m
“detpΣ)n ´-『
m
Where in piii) we used the matrix determinant lemma, and in piv) we used the determinant of a
diagonal block matrix.	□
Lemma F.7. Let
hpx, μ) = exp{ — ^ɪ- px ´ μ)TΣ-1px ´ μ)}I [xTΣ-1x V α‰ @px, μ) P Rd X Rd
and define the function:
ψpμ) “ ʃ	dxhpx,μ)
Then ψpμ) is log-concave over the space Rd.
Proof. We begin by noting that the function (X — μ)TΣ-1px — μ) is convex w.r.t both X and μ, hence
its negative is concave, and thus, by definition the function:
expt—12p px — μ)T ∑-1px — μ)}	(f.4)
2σ-
20
Published as a conference paper at ICLR 2020
is log-concave w.r.t x, μ. We now show that h(x, μ) is log-concave w.r.t both X and μ. First, w.r.t μ.
Let β P [0,1], μι, μ2 P Rd, and observe that:
h(x,βμι ' (1 — β)μ2)
“expt — ——2(x	— βμι	—	(1	—	β)μ2)τ∑	1 (x	— βμι ´	(1 —	β)μ2)}I[xT∑	1X V	a]
2σ2
piq	β	T 1
》eχpt—2σ2^(x—μ1q ς (x — μ1qu expt一
I[xT Σ-1X V a]
p“q expt — -β2(x — μ1qτ∑T(x — μ1qu expt —
2σ2
(I[xT∑τX V a]qβ(I[xT∑-1X V a]qp1—eq
“h(x, μ1qβ h(x, M2qp1-eq
三科(x — μ2)Tς I(X — μ2qu
(12^^ — μ2 )t ∑T(x — μ2)U
Therefore h(x, μ) is log-concave w.r.t μ. Where in (i) We used the log-concavity of the function in
Eq. F.4, and in Pii) we used the fact that I[xτ∑Tχ V α] p {0,1}.
Now, w.r.t x. Let β P r0, 1s, x1, x2 P Rd. Observe that for any convex function q we have:
I [q(xi) V α]β I [q(x2)V a]，1—β，“iqI [q(xι) V α] I [q(x2)V α]
“I rqpx1) V α ^ qpx2 ) V αs
≤ Ireq(X1) ' (1 — β)q(x2) V a]
piiiq
≤ I [q(βx1 ' (1 — β)x2) V a]
Therefore I[xTΣ-1x V a] is log-concave. Where in (i) we used the fact that I[q(x) V a] p {0,1},
in (ii) we used the fact that q(x1) V a ^ q(x2) V a 今 βq(x1) ' (1 — β)q(x2) V a, and in (iii)
we used the convexity of q. Hence, observing h(x, μ) we have:
h(βx1 ' (1 — β)x2,μ)
“ expt—--2(βx1 ' (1 — β)x2 — μ)τ∑ 1(βx1 ' (1 一 β)x2 — μ)}
2σ2
I [(βx1 ' (1 — β)x2)τ∑T(βx1 ' (1 — β)x2) V a‰
(》)expt — -β2(x1 — μ)T∑T(x1 — μ)U exp{—(1 丁q (x2 — μ)T∑T(x2 一 μ)}
2σ2	2σ 2
I [(βx1 ' (1 — β)x2)τ∑T(βx1 ' (1 — β)x2) V a‰
pvq	β	T 1	(1 — β)	T 1
》eχp{—2σ^(x — μ1q ς (x — μ1qu eχp{—2^2-(x — μ2q ς (x — μ2qu
(I [xTΣi1x1 V a‰qβ(I [xTΣi1x2 V a‰qp1-βq
“h(x1, μqβh(x2, μqp1一βq
Therefore h(x, μq is log-concave w.r.t x. Where in (ivq we used the log-concavity of the function in
Eq. F.4, and in (vq we used the log-concavity of I[xT Σ-1x V a].
Finally, by using Prekopa-Leindler inequality (Prekopa, 1973) we have that ψ(μq is log-concave, as
required.	□
F.3 Proof of Theorem 4.2
Lemma F.8. Consider the attacker G*, defined in Def F1. The best response strategy for the
authenticator against this attacker is:
D*(a,x) “ I ”}x —训∑τ V a*]
Where:
*	d(m ` kq(n ` kq	n(m ` kq
a k2(n — mq	og m(n ' kq
21
Published as a conference paper at ICLR 2020
Proof. The best response authenticator satisfies:
D* P argmax V(D, G)
DPD
“ argmaX 2eθ~qEa~∕PkqEY„Pmq [Eχ„Pnq [D(A, X)S + EX~gχγp∙∣γ)[1 ´ D(A,X)SS
“ argmaxEθ~qEA~/Pkq [Eχ„fPnq [D(A, X)S + Eχ~gχ∣θ(.∣θ)□ ´ D(A, X)SS
F.5
“argmax Eθ~qEa~n (Θck,diag(∑,kqq
DPD	,
“ex„N(Θc,n,diag(∑,nqq rDpA, X)S + ex„n(θc,n,ψq r1 ´ D(A, Xqs‰
F“ argmaxEθ〜Q J	da J dx exp{ —1 (a — Θc,k)Tdiag(Σ, k)T(a — Θc,k)}[
exp{ — 2(x — Θc,n)Tdiag(Σ,n)T(x — Θc,n)}D(a, x) +
{(nqd exPt— 1 (X — θc,n)TψT(x — θc,nqur1 — D(a, X)Ss
D can be chosen independently for each pair (a, X)P Xk X Xn. Therefore, for any (a, X)P Xk X Xn
the decision rule for D(a, x) “ 1 is:
J	dθQ(θ) expt—1 r(x — θc,n)Tdiag(Σ,n)T(x — θcη) +
θPRd	2
(a — θc,k )Tdiag(∑,k)T(a — θc,k)]}>
f	dθQ(θ) expt —1 r(x — θc,n)TψT(x — θc,n) + (a — θc,k)Tdiag(∑, k)T(a — θc,k)]}
θPRd	2
Observing the LHS integral and using the improper uniform prior assumption we have:
f	dθQ(θ) expt —1 r(x — θc,n)Tdiag(∑τ,n)(x — θc,n) +
θPRd	2
(a — θc,k )Tdiag(∑τ,k)(a — θc,k )SU
nk
“	dθ expt——r£ xtg´1Xi + £ aTΣ-1aj — 2θTΣ-1(nX + ka) + (n + k)θT∑-1θ]}
θPRd	2 i“1	j“1
f 1 r登 TL_i 3 TL_i	/	7「nX + kɑ、T「_i znX + kaλ-l.
“ expt—2 r∑ Xi ς Xi+ ∑ aj ς aj —(n + k)( n + k ) ς ( n + k )su
i“i	j“i
n m n n ' k「/A	nX + ka TΓ v´l/n	nX + kaλ-∣1
dθ expt — —ʒ-r(θ----~ς~) ς (θ------7T)SU
θPRd	2	n + k	n + k
f 1 r 登 TL _1	3 TL _1	/	7「nX + kɑ、T「_1 znX + kaλ-l.
“ expt—2r∑ Xi ς Xi+ ∑ aj ς aj —(n + k)( n + k q ς ( n + k qsu
i“i	j “i
C n2∏kqd S
Observing the RHS and using the improper uniform prior assumption we have:
ʃ
θPRd
“ J
θPRd
F“.6 J
θPRd
dθQ(θ) expt—2 r(x — θc,n)TψT(x — θc,n) + (a — θc,k)Tdiag(∑τ,k)(a — θc,k)SU
dθ expt—2 r(x — θc,n)T ψτ (x — θc,n) + (a — θc,k )Tdiag(∑τ,k)(a — θck )SU
dθ expt—2 [(x — θc,n)T (diag(∑τ,n)
n—m i
丁 rep0	,n))(x —θc,nq +

(a — θc,k)Tdiag(∑τ,k)(a — θc,k )SU
22
Published as a conference paper at ICLR 2020
n	n n	k
“I	dθ exp{—— [£ (Xi — ff)τΣ 1(X2 — θ) ' £ (aj — ff)τΣ 1(αj — θ)—
JθpRd	2 i=1	j=1
nn
—££ (Xi - θ)t ∑ ―气厂助}
i=1j=1
nk
“	dθ exp{——[£ (Xi— θ)T∑ 1(x2 — θ)' £ (aj— θ)T∑ 1(aj— θ)—
J%Rd	2 i=1	j=1
(n — m)(X — θ)τ Σ ´1(X — θ)]}
1n
“	dθ exp{—9r∑
JθcRd	2 i=1
k
XT Σ-1Xi + XaT Σ-1θj — (n — m) XT Σ-1X+
j=ι
(m ' k)θτΣ—1θ — 2θτΣ—1(mX ' ka)]}
1n	k
“ exp{ —- [ɪ^ xtΣ-1Xi ' XaT∑—1aj — (n — m)XτΣ-1X—
i=1	j=1
/	,、/mX ' ka、TL 1 zmX ' ka、、、
(m ' k)(—ZTHT∑´1 (--7τ)]}
m ' k	m ' k
C	, m' kmX' ka、TL ι, mX' ka、_.、
dθ exp{—-' [(θ — --+1-)tΣT(θ —	)]}
JθpRd	2	ill ' i	ill ' i
2∏	"	1 Λ
(E)d detNq exp{—2r ∑
2 = 1
k
xtΣ´1Xi ' XaT∑—1aj — (n — m)XTΣ-1X—
j=1
/	1、∕ιmX ' ka、Tl_1 zmX + ka、、、
(m ' k)(-------)t ∑t(---------)]}
m ' k	m ' k
Therefore the decision rule is
n11 i-nX ' ka、丁「_1 znX ' ka
V(-)d exp{2r(n ' k)(Kr)Tς 1^rr
Y Ilb	2	n ' rυ	n ' rυ
)]}J( -2∏r qd >
Y n + k
，1「/	、 TC 1	/	,「mX ' ka、Tc 1 zmX ' ka、、、 /, 2π 、力
exp{2[(n — m)x ς´1x'(m'k)(FV)Tς´1(bΓ)]}V(E)d
"()d exp{ 1 [(n ' k)(R)T∑T(R)]}>
m(n ' k)	2	n ' k	n ' k
1	T 1	mXa ' kaa T 1 mXa ' kaa
exp{χ[(n — m)XTΣTX'(m ' k)(——)tΣT( ——)]}
2	m' k	m' k
n(m ' k)	T 1	mXa ' kaa T 1 mXa ' kaa
令dlog—7-----> (n — m)XT Σ 1X'(m ' k)(-)t Σ 1(--)—
m(n' k)	m' k	m' k
nXa ' kaa	nXa ' kaa
(n ' k)(--7γ)t∑´ 1(-—r)
n'k	n'k
If 「、/ , 7λl	n(m' k)
ed(m ' k)(n ' k) log ---r >
m(n ' k)
k2(n — m)XTΣ-1X — 2k2(n — m)XTΣ—1a ' k2(n — m)aTΣ´1a
台d(m + k)(n ' k) log n(m + k) >(x — a)T∑―1(X — a)
k2(n — m)	m(n ' k)
As required.
□
Lemma F.9. Consider the authenticator Da, as defined in Def. F.2. Then any attacker G, repre-
sented by a conditional probability gχ∖γ, that satisfies the condition X = y for any leaked sample
y P RmXd and attacker generated SamPle x P {Rnxd : gχ∣γ(x∖y) > 0}, satisfies:
G P argmin V (Da, G1) ∀α P R+
G1pG
23
Published as a conference paper at ICLR 2020
Proof. The best response attacker satisfies:
gX∣γ P2：器皿2旧©7旧4„/PkqEY„Pmq [Eχ~fPnq [Da(A,X)] + Eχ~gχ∣γ(∙∣y)□ ´ Da(A,X)]]
“ argmin Eθ~qEA~/ Pkq EY „f Pmq Eχ~gxlY (∙∣y)口 — Dα(A, X)]
“ argmax Eθ~qEA~/ Pkq EY „f Pmq EX ~gχ∣γ (∙∣y )[Dα(A,X)]
“ argmaxE®„qEa„n(θ,∑qEγ„n(θ,∑)Ex~gχ∣γ(，|Yq III>X ´ A>∑τ
<α
Lem“aF.3 argmaxEΘ~QEA~N(θ, 1 ∑)ey„n(θ,∑)Ex~gχ∣γ(∙∣y) II ∣>x ´ A
“ argmax	dy	dxgχ∣γ(x|y)	daI }x ´ a}∑τ
gχ∣γ JyPRmxd	JxPRnXd	JaPRd	L
2
∑τ
<α
J/	dθQ(θ)
θPRd
< ɑ
k	1m
exp{-2(a ´ θq ς (a ´ θqu eχpt—2 X (yj ´ θq ς (yj ´ θqu
22
j “1
Note that gχ∣γ(x|y) can be chosen independently for each y P Rmxd. Thus, We can optimize it
independently for each y P RmXd and we have:
gX∣Y(∙∣y) P argmax	dxgχ∣γ(x|y)	daI }x — a}∑τ < α	dθQ(θ)
gχ∣γ(∙∣y) JxPRnXd	JaPRd	L	θ JθPRd
k	1m
exp{—2(a — θq ς (a — θqu exp{―2 X (yj — θq ς (yj — θqu
22
j“1
Note that for any PDF f over RnXd and a function 夕：RnXd → R , it holds that
,xpRn^d dxf (x)夕(x) ≤ Supx夕(x). Therefore, there exists a deterministic distribution gχ∣γ(x|y) “
δ(x ´ x1) that achieves the maximum. Thus, it’s sufficient to find a vector xG that achieves the
maximum:
XG P argmax	dx1δ(x1 — x) daI >x1 — α>∑τ < α	dθQ(θ)
x	Jx1 PRnXd	JapRd	L	」JθpRd
k	1m
exp{—5(4 ´ θq ς (a ´ θqu exp{—5 X Pyj ´ θq ς Pyj ´ θqu
22
j“1
“ argmax daI }x — 4}∑τ < α	dθQ(θ)
x	aaPRd	θPRd
k	1m
exp{—2(a — θq ς (a — θqu exp{-2 X (yj — θq ς (yj — θqu
22
j“1
“q argmax	daI \}x — a}∑τ < α dθ
x	aaPRd	θPRd
1m
exp{—2rk(a — θq ς Pa — θq + X Pyj — θq ς Pyj — θqsu
2	j“1
“ argmax f	daexp{ —；[kaT∑-1a]}I [}X — a}∑τ < α]「 dθ
x	aaPRd	2	θPRd
expt —ɪ r(m + k)θτ ΣΙ1θ — 2θτ Σ-1(my + ka)]}
1T1	1	T1
“ argmax	da expt——[kaτΣ 1a-f Pmy + ka)TΣ 1(my + ka)]}
x	aaPRd	2	m + k
2	(m + k)	mya + kaa T 1	mya + kaa
I }X — a}∑τ < α	∕θexp{ —	pθ —⅛vqTς (”	)u
θPRd	2	m +	m +
24
Published as a conference paper at ICLR 2020
11
“ argmax	daexp{ —χ[k4τΣ 1a-Nmy ' ka)'∑ Pmy ' kaq]}
X	JaPRd	2	m' k
I }χx — α}∑τ V α]
1 mk T 1	2mk T 1 mk T 1
“ argmax dαexp{——[---------aT Σ 1a-------yτ Σ 1a '----yτ Σ 1yS}
x	JaPRd	2 m ' k	m ' k	m ' k
I ∣}x — α}∑τ V α]
mk	T 1	2
“ argmax dαexp{-----------ττ[(a — y)T∑ 1(a — y)S}I }x — α}∑τ V a
χ	JaPRd	2(m + kq	L	」
Where in (*) We used the fact that Q(θ) is the improper uniform prior. Note that the expression
depends only on the mean xa. Therefore, it’s sufficient to find a mean vector xa that maximizes the
expression. We substitute the integration variable to 夕 “ a — X and obtain:
mk
XG P argmax	dφexpt——~^XrQ + X — y) ∑ 1(,ψ + X — y)S}
由	Jt0PRd：0T∑-10Vα}	2(m 十 k)
” argmaxψ(y — X) ” argmaxψ(μ)
X	X
Where ψ is defined as in Lemma F.7 (with σ “ mm'k-), from which it follows that ψ(μ) is log-
concave, and therefore has at most one local extremum Which can only be a maximum. Therefore,
it is sufficient to show that μ “ 0 (i.e., X “ y) is a local extremum by equating the gradient at the
point to 0.
B-ψ(μ) = B- /	dψexp{—Ix mk 卜、[Q — μ)τ£—1(。— μ)s}
Bμ	Bμ JtWPRd:"∑-%vα}	2(m 十 k)
=—2( mk 卜)[	即exp{— 2( mk 卜 1[(。— μ)τ”—1(。— 〃)s}
2(m 十 k) JtwPRd:WT∑-1wvα}	2(m 十 k)
才(。—μ)T ” - 1(。— μ)
Bμ
“一(mkkι [	d。expt— 2( mk k)限。—μ)T”-1(。— “)]}”-1(μ —。)
Pm ' k) JtwPRd：wT∑-1wvα}	2(m 十 k)
Therefore:
-B ψ(μ)lμ = 0 =( mk At)f	d。exp{— X mk k)r。T ” -1。]}” —1。
Bμ	(m 十 k) JtwPRd：wT∑Tw<α}	2(m 十 k)
Note that since the domain of integration is symmetric about the origin with respect to negation
and the integrand is odd with respect to the integration variable, the integral is equal to zero. I.e.,
Βμμψ(μ)∣μ=o = 0. Therefore, X = y (μ = 0) achieves the global maximum, and any attacker that
satisfies the condition: X = y for any leaked sample y P RmXd and attacker generated sample
X P tRnxd : gχ∣γ(X∣y) > 0} satisfies:
G P argmin V (Dα, G 1) Va P R+
G1PG
As required.	□
Corollary F.10. Consider an authenticator Da, as defined in Def F2. Then the attacker G * ,defined
in Def. F.1, is a best response. i.e.:
G * P argmin V (Dα, G 1) @a P R+
G1PG
Proof. Directly from Lemma F.9
□
Theorem F.11. The game value is:
maxminV(D,G) = min max V(D, G) = V(D*, G*) =
1	1	d	dn(m 十 k) n(m 十 k) d dm(n 十 k)	n(m	十 k)
2 + 2Γ(d) Y	2,	2k(n — m)	og m(n 十 k)	Y 2, 2k(n — m)	og	m(n	十 k)
Where γ is the lower incomplete gamma function.
25
Published as a conference paper at ICLR 2020
Proof. From the max-min inequality we have:
max min V(D, G) ≤ min max V(D, G)
On the other hand, using Lemma F.8 and Corollary F.10 we have:
max min V(D, G) > min V(D*, G) F“0 V(D*, G*) F“ max V(D, G*) > min max V(D, G)
DG	G	D	GD
Therefore:
maxminV(D,G) “ min max V(D, G) “ V (D*, G*)
The game value is given by:
V(D*, G*) =Eθ~qV(Θ,D*, G*)
“1 Eθ~qEa~/PkqEY„fPmqlEX„fPnq [D*(A,X)]+ Ex„gXYp.∣γ)[1 ´ D*(A,X)]]
“2 Eθ„QEA„f(kq EY -fΘmq
∣Eχ~fΘnq l1r>x ´ a>∑t < α*sl + Eχ~gXlYp∙∣γq I1 ´ Ir>x ´ a>∑t < α*sll
=2 ` 2 Ee-QEAfkq Ex~fθnq ∣Ir>χ ´ A>∑τ < α*sι ´
2Ee~QEA~fgkqEx~gXlθ(∙∣θ) ∣Ir>x ´ A>∑τ < α*sl
Observing the first term we have:
1 峪7%„废 Exfnq ∣Ir>χ ´ A3 < α*s]
=1 Eθ~qEa~/PkqEχ~fpnq [I[(X ´ A)T∑FX ´ A) < α*]‰
=1 Eθ~qEα~/PkqEχ~fpnq [I[(X ´ A)T(CCT)T(X ´ A) < α*]‰
=1 Eθ~qEα~/PkqEχ~fgm [I[(X ´ A)TCTCT(X ´ A) < α*]‰
=1 Eθ~qEα~/PkqEχ~fP“q [I[(CT(X ´ A))T(CT(X ´ A)) < α*]‰
” 1 Eθ~qEα~/PkqEχ~fP“q [I[ZTZ < α*]‰
= (*)
Observe that
Z = CT(X ´ A) = CTr(X ´ Θ) ´ (A ´ Θ)S
=CTrId,—Id] [X´θ = L,—CTs [X´θ
Note that:
X — Θ
A — Θ
„ N(02d,
Γ1Σ
n
0d^d
0d^d
1Σ
k
)
Therefore:
Z „ "C「—c TsE Ml—CTT ])
“N(0d, (1 + 1 )Cτ∑LT)
nk
=N(0d, n+kC´1 CCTC´T)
nk
n+ k
“N (0d, 一厂 Id)
nk
26
Published as a conference paper at ICLR 2020
We denote Z “ ʌ/n'k Z 〜 N (0d, Id), and thus Z\,...,Z& are independent standard normal
random variables and ZTZ 〜χ2(d). Therefore, ZTZ “ n'kZTZ 〜Γ(k = d, θ = 2n'k) and
we have:
(*)=2Eθ~qEa5Pkq Ex„fPnq “I[ZTZ < α*]‰
“2EΘ〜QEZTZ~r(k = 2,θ = 2n±kq “IrZTZ < α*]‰
Pi) 1 ɪɪ-,	1	fd nka*
“ 2 Eθ~Q Γ(12)γ p 2,2(n'k)q
11	d	nkα*
=2∏)Yp2, 2(n + k) q
Where in (i) We used the CDF of the Gamma distribution in which γ is the lower incomplete gamma
function.
Similarly, observing the second term we have:
2 Eθ~QEA~fΘkq Eχ~gχiθ(.∣θ) ”Ir>X ´ A>∑τ < α*s]
≡ 1 Eθ„QEA„fΘkqEx~gXlθ(.∣θ) “IrVTV < α*s‰
= (**)
Where:
V = CT(X ´ A) “ CTr(X ´ Θ) ´ (A ´ Θ)s
“ c TrId ,—Id] „X ´ Θl = rc ´1, ´e Ts
X — Θ
A — Θ
Using the definition of G* (Definition F.1) and Lemma F.3 We have:
X — Θ
A — Θ
ɪ Σ
〜 N (02d ,
0d^d
0d^d
1Σ
k
m
)
Therefore:
V 〜N(0d, rc´1, —CTs
ɪ Σ
m
0d^d
0d^d! Γ c´T
1∑1LC ´T
m ` k
q“ NQdFrldd
And similarly to the first term, We get:
VTV 〜r(k “ d,θ “2修)
And thus:
11 d	mkα*
(**)= 2Γ(2) Yp 2, 2(m + k))
Therefore, the game value is given by:
V(d* g*)= 1 + lɪrγ(d nkα* ) — γ(d mkα* )s
V(D , G ) 2 十 2Γ(d )rY (2, 2(n + k)) Yp 2, 2(m + k))S
As required.
1	1 1 d dn(m ` k) n(m`k) d dm(n ` k) n(m`k)
2 + 2 Γ(d) Y 2, 2k(n — m) og m(n + k)	Y 2, 2k(n — m) og m(n + k)
□
Finally, We prove Theorem 4.2 and Corollary 4.3.
27
Published as a conference paper at ICLR 2020
Theorem 4.2. Define δ = m{n ≤ 1 and let P “ m{k. Consider the attacker G* defined by the
following generative process: Given a leaked sample Y P RmXd, G* generates a sample X P RnXd
iid
as follows: it first samples n vectors W1, . . . , Wn „ N p0, Σq and then sets: Xi “ Wi ´ W ` Y.
Define the authenticator D* by:
C*r	、	2 Il- -∣∣2	/ d p1 + Pq(I + ρδ-1) 1 ρ P + 1∖
D pa,xq “ I }x - a}∑τ V------------npι ´ δq-------log (ρ'7J	(F.5)
Then pD*, G*q is a solution of Eq. 2.2 that satisfies Eq. 4.1.
Proof. Directly from Lemma F.8, Corollary F.10, and Theorem F.11 by assigning δ “ m, ρ “
m	n □
k.	LJ
Corollary 4.3. Define δ and P as in Theorem 4.2. Then the game value for the Gaussian case is:
1	1 d dp1 ` Pq	1 ` P	d dpδ ` Pq	1 ` P
2'2Γ(1 卜(2，Jlog δΓ7)-γ(2，J log 4力 (F.6)
Where γ is the lower incomplete Gamma function, and Γ is the Gamma function.
Proof. Directly from Theorem F.11 by assigning δ “ m ,ρ “ m.	□
F.4 Game value for a maximum likelihood attacker
In this section, we consider the most intuitive attacker strategy, which one could naively see as
optimal. However, we show that this intuitive “optimal attacker” is sub-optimal as can be seen in
Fig. 1c in the main paper. We consider an attacker that draws the attack sample from a Gaussian
distribution with the maximum likelihood estimate of the mean and the known covariance. We
denote this attacker by the name ML attacker. We find the best response authenticator to this attacker
and the associated game value. Fig. 6 visualizes the difference in theoretical game value between the
ML attacker (see Definition F.12) and the optimal attacker (see Definition F.1) for different values
of d (the dimension of observations), and demonstrates that the ML attacker is indeed sub-optimal.
(a)	(b)	(c)
Figure 6: The difference in game value (expected authentication accuracy of the optimal authenticator) be-
tween the ML attacker and the optimal attacker for different values of the observations’ dimension d, as a
function of the parameters P “ m, δ “ m. Namely: maxD{V(D, Gml)} — maxD{V(D, G*)}. (a) Differ-
ence in game value for d “ 10. (b) Difference in game value for d “ 100. (c) Difference in game value for
d“ 1000.
Definition F.12. Let GML denote an attacker defined by the following generative process: Given a
leaked sample Y P RmXd, GML generates an attack sample X „ N(Y, Σ)
Lemma F.13. Let θ P Rd, Σ P RdXd represent the mean and covariance ofa Gaussian distribution.
Let X P RnXd be a random sample generated by the attacker defined in Def. F.12. Then:
Xc „ N(θc,n, diagP∑, n) ' rep(( j)∑, n)) ” N(θc,n, Ψml)
28
Published as a conference paper at ICLR 2020
Proof. Let Wι,..., Wn 吧 N(0, Σ), observe that Xi = Y ' Wi @i P [n], and thus:
Xc = Yc,n ' Wc
Where Wc 〜 N(0 ∙ 1dn diag(Σ,n). Using Lemma F.3 we have Y 〜 N(θ,.Σ) and using Lemma
F.4 We have Yζ,n 〜N(θc,n rep(春∑, n)).
Let Z = IW]	and B =	[Ind^nd	,	Ind^ndS,	then Xc	=	Wc	' Yζ,n	= BZ. Note that:
0nd
θc,n
Z〜N (
diag(Σ, nq	0
0	rep( mm Σ,n)
and therefore we have:
Xc 〜N(B I0力“di*0 rep(⅛,nq] BT)
“N(θc,n, diag(∑, n) ' rep(m∑, n))
As required.	□
Lemma F.14. Let Σ = CCT P RdXd represent the Covariance of a Gaussian distribution, and
Considerthefollowing covariance matrix: Ψml = diag(Σ,n) ' rep(+Σ,n). Then:
ψMl = diag(£T,n) — n ' mrep(£T,nq
and the determinant is:
det(Ψ) = det(Σ)n( n'm )d
m
Proof. To find the inverse of ΨML we first define:
»夕
U=
p Rnd^ d
nd^d
(F.7)
Σ
Therefore we have:
ψMl =(diag(2 n) + repW2 n))T
= (diag(Σ,n) ' UVT)T
“qdiag(Σ, n)´1 ´ diag(Σ, n)´1U(Id ' VTdiag(Σ, n)-1U)-1VTdiag(Σ, n)´1
p=qdiag(∑-1, n) ´ diag(Σ~1, n)U(Id ' VTdiag(Σ~1, n)U)-1VTdiag(Σ~1, n)
Id
“diag(Σ-1, n) ´
“diag(Σ-1, n) ´
“diag(Σ-1, n) ´
“diag(Σ-1, n) ´
Id
Id
Id
Id
Id
(Id ' — rId	…Ids
m
Id
Id
(Id + "τ ɪ [∑τ …∑τ‰
mm
((nɪm)Id)Tɪ [∑τ	…∑τ‰
mm
Id
ςT
n`m
ςT
Id
• ∙ ∙
1
.Id ET
• ∙ ∙
29
Published as a conference paper at ICLR 2020
“diag(∑-1, n) ´
------rep(∑-1, n)
n`m
As required. Where in pi) we used the Woodbury matrix identity, and in pii) we used the inverse of
a diagonal block matrix.
Next, we turn to find the determinant of ΨML :
det(Ψ) “ det(diag(Σ,n) ' repPP~)Σ, n))
m
“ detpdiagpΣ, n) ` UV T)
piiiq det(diag(Σ, n)) det(Id ' VTdiag(Σ, n)´1U)
p“q det(Σ)n det(Id + VTdiagp∑τ, n)U)
J∑-
“ det(Σ)n det(Id + — rid … Ids diag(∑-1,n).)
m.
Σ
n+m
“ det(Σ)n det((	)Id)
m
“ det(Σ)n( -±m )d
m
Where in (iii) we used the matrix determinant lemma, and in (iv) we used the determinant of a
diagonal block matrix.	□
Lemma F.15. Consider the attacker GML, defined in F.12. The best response strategy for the au-
thenticator against this attacker is:
Dml(a, X)= IUx — a}∑τ V °ml]
Where:
d(n + k)(nm + nk +mk)	nm + nk +mk
αML = ----------k2-2-----------lOg(	m(n + k) q
Proof. The best response authenticator satisfies:
D* P argmax V(D, GMLq
DPD
“argmaX1 Eθ~qEa~/PkqEY„PmqlEX„Pnq [D(A, X)s + EXyYp.∣γ)[1 — D(A,
“argmaX Eθ~qEa~/PkqIEX„Pnq [D(A, X)s + EX/^⑼口 ´ D(A, X)s]
Lemma F.13
argmax Eθ~qEa~n(©。% ,diag(∑,k))
DPD	,
[EX~N(®c,n,diag(£,n))rD(A,X)s + EX„N(θc,n,ΨML)[1 ´ D(A,X)S]
“argmax Eθ~q ʃ	da ʃ	dxexpt—2(a — Θc,k)Tdiag(Σ, k)Τ(a — Θ")}
r	∣det(diag(Σ, —))|
expt—2(x —㊀叫)Tdiag(Σ,-)T(x —㊀Μ)}D(a, x) +
{ ∣det(ψ⅛l)| exPt — 2 (X — θc,n)TψM1L(x — θc,n)}r1 — D(a, x)ss
LemmaE14argmax旧㊀〜Q f da f	dxexpt—1 (a — Θc,k)Tdiag(Σ,k)T(a — Θc,k)}[
DPD	aPRkd	xPRnd	2
(-(n mm)d expt — 2(x — Θc,n)Tdiag(Σ, —)Τ(x — Θc,n)}D(a, x) +
30
Published as a conference paper at ICLR 2020
exp{ — 2(X ´ Θc,nqTΨm1lPx ´ Θc,n)}[1 ´ D(a,x)ss
D can be chosen independently for each pair (a, X)P Xk X Xn. Therefore, for any (a, X)P Xk X Xn
the decision rule for D(a, Xq “ 1 is:
ʌ/(n + m)d f	dθQ(θ) expt-1 (x - θc,n)Tdiag(Σ,n)T(x - θc,n)}
m	θPRd	2
expt-2(a - θc,k)Tdiag(Σ, k)L1(a - θc,k)} >
11
dθQ(θ) expt--(x - θc,n)TΨm1l(x - θc,n)U expt--(a - θc,k)Tdiag(Σ, k) 1(a - θc,k)}
θPRd	2	2
Observing the LHS integral and using the improper uniform prior assumption, we obtain:
f	dθQ(θ) expt-1 r(x - θc,n)Tdiag(Σ, n)L1(x - θc,n) +
θPRd	2
(a - θc,k)Tdiag(Σ,k)T(a - θ°,k)S}
1n	k
“	dθ expt -- rX xT∑l1Xi + X aʃ∑-1aj — 2θT∑-1(nX + ka) + (n + k)θT∑l1θ]}
θPRd	2 i“1	j“1
1n
“ exPt-2 rX
k
xt∑l1Xi + X aj∑Taj∙ ´ (n + k)(
d	dθ expt - n'*
θPRd	2
1n
“ exPt- 2 rX
j“1
r(θ´ ≡kα)T ∑”
n`
k
nX ' ka τ ι nX + ka
I~q q ς P	I~q )SU
n ` k	n ` k
nX ' ka
n + k )SU
xt∑l1Xi + X ajTΣ´1ɑj — (n + k)(
j“1
nX ' ka τ i nX + ka
I~q q ς P	I~q )SU
n ` k	n ` k
7(1^^Ide^
Observing the RHS and using the improper uniform prior assumption, we obtain:
f	dθQ(θ) expt-1 r(x ´ θc,n)TΨm1l(x ´ θc,n) + (a ´ θc,k)τdiag(∑τ, k)(a ´ θck)]}
θPRd	2
“ ʃ	dθ expt- 2 r(X - θc,nqT ψMl(x - θc,nq + (a - θc,k 尸成明①一1, k)(a - θc,k)s}
F“4 f	dθexpt-1 r(x — θc,n)T(diag(∑τ,n)--1— rep(∑τ,n))(x — θc,n) +
θPRd	2	n + m
(a — θc,k)Tdiag(∑τ,k)(a —	)]}
k
LI(Xi ´ θ)+ X Paj ´ θqTΣ´1 Paj ´ θ)´
1n
“J	壮 dθ exPt-2 r∑(xi - θ)T∑
2
----(x — θ)T ∑l1(x — θ)su
n+m
j“1
nk
“qexp{ — 2 rX XT∑τ±i + X aT∑l1
2 i“1	j“1
n2	T 1 nm + nk + mk
aj------X Σ X-------------V
n+m
n`m
，T ∑Tv]}
ʃ
θPRd
1n
“ exp{- 2 r∑
2 i“1
1	nm + nk + mk
dθ expt- ʒ--1-----
2	n+m
k
T1	T1
XT Σ 1Xi + Maj Σ 1aj- ´
j“1
2
n2 T ´1
----X Σ
n`m
nm + nk + mk T夕´i
n+m
X ´
J( 21n [ 1))d det(Σ)
nm` nk `mk
31
Published as a conference paper at ICLR 2020
Where inPiq We denotedV “ ηmm'ηp*mq". Therefore, the decision rule is
J”	`nX ' ka ττ ´_↑ Jnx ' kɑʌ-,ʌ /	1	、
expt 2 rpn'kqp ^rqT ς 1P -nTk- NP mpnτ⅛)q
》
1	n2	T ´1
expt — I-x Σ x '
2 n'm
nm ' nk ' mk T
-------v
n'm
∑-1vSU^P
1
nm ' nk ' mk
qd
nm ' nk ' mk
mPn ' kq
qd

》
1	n2	T 1
exp{-[---x Σ 1x '
2 n'm
nm 'nk + mk VT ∑-iv ´ pn ` kqp
n'm
nx ' ka τ´-innX ' kaλ-∣1
I~q q ς P	I~q qsu
n'k	n'k
nm ' nk ' mk
Od logp--Γ-T^M-q >
mpn ' kq
n2 T 1 nm ' nk ' mk
----XT Σ 1x +----------V
n'm
nm ' nk ' mk
θd logp	mPn ' kq
n'm
q>
OPX ´ a)τΣ-1px ´ a) v
TL 1	/ n∕+ ' ka∖T□ 1 zηx ' ka、
τ ∑-1v ´ Pn ' kqp-^qT ∑-1p---^)
n'k	n'k
k2n2
Pn ' k qPnm ' nk ' mkq
dPn ' kqPnm ' nk ' mkq
k2n2
nm ' nk ' mk
logp mPn' kq	q
As required.
Theorem F.16. Fix the attacker to be GML as defined in F.12, then the game value is:
Lemma F.15
max V pD, GMLq “ V pDML, GMLq “
1	1 1 d dpnm ' nk ' mkq nm ' nk ' mk
2'2rpdq rγp2,	2nk	Og -mpn ' kq-q´
d dmpn ' kq	nm ' nk ' mk
YP 2,	2nk	log	mPn' k)	qs =
1'14 [γ( d, d P1' P' δq log 1'∣'-^ q ´ Y p d, d Pρ' δq log 1'∣'-^ qs
2	2 Γp d q	22	ρ' δ	22	P' δ
Where P “ 贽,δ “ mm, and Y is the lower incomplete gamma function.
□
Proof. The game value is given by:
V pDML, GMLq
“eΘ〜QV(θ, DML, GMLq
“2EΘ~QEA〜fPk)EY„fPmqlEX„fPn) rDMLpA,xqs ' EX„gM|Y (∙∣Yqr1 ´ DML(A,χqsl
“ 2旧㊀〜QEA〜fθkq Eγ〜/gm)
忸χ~fΘnq ”Ir>X ´ a>∑t v aMLsl ' Eχ„gMY(∙∣γ)”1 ´ Ir>x ´ a>∑t v aMLsll
11	2
“ 2'2Eθ~QEA„jf^kq EX~fΘnq IIr>X ´ A>∑τ
2 Eθ~QEA~fΘkq EX ~οM∣Θ(∙∣θ) ”Ir>X ´ A>∑τ
vα
v αMLs

Observing the first term, we can see that by replacing α* with qml in the analog part of the proof
for Theorem F.11 We get:
2 EΘ~QEA-f Pkq EX〜f Pnq ”I r>X ´ A>∑τ V aML sl
11 d nkaML
2∏yYP 2, 2pn ' kqq
32
Published as a conference paper at ICLR 2020
Again, similarly to the analog part of the proof for Theorem F.11, observing the second term we
have:
1 EΘ„QEA„f(kq EX ~gMlΘ(∙∣Θ) ”Ir>X ´ A>Σ- V αML si
≡2EΘ~QEA~fPkqEX~gMlΘ(∙∣θ) “IrVTV V aMLs‰
=(*)
Where:
V = CT(X ´ A) = CT r(X ´ θ)´(A ´ Θqs
=CTrId,—Ids [X´θ = rC´1, ´eTs [X´θ
Using the definition of GML (Definition F.12) We have X 〜 N(θ, InnmmΣ), using Lemma F.3 We
have A 〜N(θ, ɪΣ), and thus:
X — Θ
A — Θ
//θ] l^nim ∑
〜N(H,∣nm
0
k ς
)
Therefore:
V 〜N(0,rC T-C Ts [臂 ς	ι^l [ Cc´rl )= N (Qd,gn^m Id)
0 k ∑J |_-C _|	nmk
-»-v T 1	. τV
We denote V
nmk
nm'nk'mk
V and thus Vb ...,Vd are independent standard normal random
=
variables and VTV 〜χ2(d). Therefore
V T V 〜r(k=d,θ=2 Tr)
And We have:
nmkαM L
(*q = 2卷γ(d, 2(nm + nk + mk)
)
Hence, the game value is given by:
V(D* G*) = 1 + 1 1 rγ(d nkαML
V(D , G)	2 + 2Γ(d)rY( 2, 2(n + k)
nmkαM L
)—γ(d,―,------------
2 2(nm ` nk ` mk)
)s =
1	1 1 d d(nm + nk + mk)
2	+ 2r(d) rγP2,	2nk
d dm(n + k)	nm + nk + mk
Y 2，	2nk	og m(n + k)'
nm + nk + mk
log	m(n + k))—
)s
As required.
□
G	Experiments - datasets
BeloW We provide details on the datasets used for the authentication experiments on faces and char-
acters. The VoxCeleb2 (Nagrani et al., 2017; Chung & Zisserman, 2018) dataset contains cropped
face videos of 6112 identities. We used the original split of 5994 identities for training and 118 for
test. For each identity, we saved every fifth frame, resized each frame to 64 X 64, and augmented it
using horizontal flip. The Omniglot dataset (Lake et al., 2015) contains handWritten character im-
ages from 50 alphabets. There are 1623 different characters, and 20 examples for each character. We
use the splits and augmentations suggested by Vinyals et al. (2016) and used by Snell et al. (2017).
33
Published as a conference paper at ICLR 2020
H Experiments - implementation details
In this section, we describe our implementation of the GIM model for the different experiments,
in detail. Recall from Sec. 5, that in general, the authenticator is a neural network Dpa, xq that
can be expressed as Dpa, xq “ σpTD paq, TD pxqq, and the generator is a neural network Gpyq that
can be expressed as Gpyqi “ 夕(Wi — W ' TGPy)) @i P [n]. In What follows We describe our
implementation of these models for each of the experiments.
H.1 Gaussian sources
Authenticator Architecture: For the statistic function TD, We use a concatenation of the mean and
standard deviation of the sample. For the comparison function σ, We use the element-Wise absolute
difference betWeen the statistics TDpa), TDpx), folloWed by a linear layer.
Attacker Architecture: For the statistic function TG, we use the sample mean, i.e., TG (y) “ y. The
noise vectors Wi are generated as folloWs: First, n Gaussian noise vectors Z1, . . . , Zn „iid N p0, Id)
are drawn, then each vector Zi is passed through a linear layer to obtain Wi. Finally, the decoder 夕
is the identity function.
Optimization details: The model is trained in an authentication setup as in our theoreti-
cal setup, using alternating gradient descent as is common in GAN optimization (Mescheder
et al., 2018). Each iteration begins when a source θ P Rd is drawn from the prior distribu-
tion Q = N(0,10Id). Samples A P Rkxd,Y P Rmxd,Xθ P RnXd are drawn IID from
fθ “ N(θ, Id), where Xθ represents a real sample from θ. The attacker, given the leaked sam-
ple Y, generates a fake sample XG “ G (Y) P RnXd, passes it to D, and suffers the loss
— log(sigmoid(D(A, XG))). The authenticator, D, receives as input the source information sam-
ple A, outputs a prediction for each of the test samples Xθ, XG, and suffers the binary cross-entropy
loss —0.5 (log (sigmoid(D(A, Xθ))) ` log (sigmoid(1 — D(A, XG)))). Each experiment is trained
for 200K iterations with a batch size of 4000 using the Adam optimizer (Kingma & Ba, 2015) with
learning rate 10 ´4.
H.2 Experiments on Voxceleb2 and Omniglot
To describe the models we begin with some notation. We let c denote the number of image channels,
h denote the image size (We only consider square images of size C X h X h), and l denote the latent
dimension of the model.
Authenticator Architecture: As mentioned above, the authenticator is a neural network model
that can be expressed as:
D(a, x) “ σ(TD (a), TD(x))
The statistic function TD maps a sample of images to a statistic vector s P R6l in the following
way: Each image in the sample is mapped using encoders EDrc, Eenv : [´l, IscXhXh → Rl to two
latent vectors vsrc, venv P Rl, respectively. vsrc is designed to represent the source θ, and venv is
designed to represent the environment (e.g., pose, lighting, expression). To represent the source of
the sample, the sample mean of vsrc is taken. To represent the sample distribution, venv is passed
through a non-linear statistic module ζ which is meant to capture more complex statistical functions
of the sample.5 Finally, TD (x) is obtained by concatenating VSrc and Z(Venv). E.g., for X we have:
TD (x) “ concat
(-之 EDc(Xi), Z (EDnv
n i“1
The comparison function σ : R6l → R receives two latent vectors sa “ TD (a), sx “ TD (X)
representing the statistics of the samples a and X respectively. The vectors are concatenated and
then passed through a Multi-Layered Perceptron which outputs a scalar reflecting their similarity.
Namely:
σ(sa, sx) “ MLP(concat(sa,sx))
The full architecture of the authenticator is depicted in Fig. 7.
5 ζ is implemented as the concatenation of the standard deviation and mean of the sample after passing each
example through a Multi-Layered Perceptron.
34
PUbHShed as a COnferenCe PaPer at ICLR 2020
FiguresAn OVervieW Ofrhe im-emenrars'n Ofrhe GIM aurhenricasr archirecsre for rhe experimen∞On rhe
VOXCeIeb2 and Omniglor daras
AttaCker ArChiteCtllre: OUr implementation Ofthe attacker is5'spired by the architecture SUg—
gested by ZakharoV et ah (2019-WhiChreHeS on an implicit assumption that an image COUld be
modeled as a mapping OftWO Iatent VeCtorSimage SPaCThe first VeCtOrrePreSentS the SOUrCe φ
and is the Same for any image Ofthe SeCOlld VeCtOrrePreSeIltS the environment (g PosHght
expression) and is different for each image Ofthe source，
TheattaCker model consists Oftheflowing components AIl image encoder EhC -
「——Lxhxh i 冠-hat maps an image to a Iatent VeCtOr representing the SOUrCean image
encoder E二——Llrxhxh i冠-hat maps an image to a latent VeCtOrrePreSentg the environ—
merity a MUitlayered PerCePtrOIl MLP2→冠 that maps GaUSSiaIl noise to the environment
latent Spacan environment decoder 弋i a → Wxhxh rmaps a Iatent VeCtOrtO an environ—
merit image WhiCh COUId represent aspects Ofthe environment SUCh as faciaIlandmarkSand fħlaπy"
a generator e ..里Gxhxh × 「——Lxhxh -hat maps an environment image COnCatenated to
the real image to a IleW imagThe generator is based Onthe image to image model USed by za—
kharov et ah (2019) and JOhlISOIl et ah (2016and USeS the SOUrCe Iatent VeCtor asput for AdaPtiVe
instance IIOnnaliZatn (HUang 浮 BelOngie" 2017
The attacker generates a fake sample X m「——1 lx0xhxh based on a Ieaked sample Ym
「——L Ix0xhxh -U -he following waEaCh image En the Ieaked sample is mapped USg E
and EIatent VeCtOrS UTC-6冠，A latent environment VeCtoL如is COlIStrUCted for
each fake image Xi in the following WaF FirSL S GaUSSian noise VeCtOrS Zl 二,二 Nn,后 Λ∕^(oj II}
6In ZakharoVaL (2019)" OUr so—called environmenr image isdeed a facia IlandmarkSimageWhiChiS
USed aspurrhe modeLln OUr WOrk- we allow rhe modelIeam WhiCh environmenf image is USefUL
35
Published as a conference paper at ICLR 2020
are drawn, then each vector Zi is passed through MLPG to obtain Wi, and finally, vienv is obtained
by matching the mean of the new latent environment vectors to the sample mean Uenv. Namely:
Venv “ Wi ´ W ` Uenv Vi P [n]
Each fake image Xi is then generated deterministically as follows: vienv is used as input to the
decoder ψenv which outputs an environment image. This image is concatenated along the channel
dimension to a random image from the leaked sample Y , and then passed as input to the generator φ,
which also receives Usrc as input to its Adaptive instance norm layers. The output of the generator
is the fake image Xi for all i P rns. The full architecture of the attacker is depicted in Fig. 8.
Figure 8: An overview of the implementation of the GIM attacker architecture for the experiments on the
Voxceleb2 and Omniglot datasets.
Optimization details: The model is trained in an authentication setup as in our theoreti-
cal setup, using alternating gradient descent with the regularization parameter as suggested by
Mescheder et al. (2018). Each iteration begins when a source θ P Rd is drawn uniformly
from the dataset. Samples A P [—1, ι]g^^h,γ P [_1, ism^^^h,Xθ P [´],("会陵方
are sampled uniformly from the images available to the source θ. The attacker, given the
leaked sample Y, generates a fake sample XG “ G pYq, passes it to D, and suffers the loss
— logpsigmoidpDpA, XG qqq. The authenticator, D, receives as input the source information sam-
ple A, outputs a prediction for each of the test samples Xθ, XG, and suffers the binary cross-entropy
loss —0.5 plog psigmoidpDpA, Xθ qqq ` log psigmoidp1 — DpA, XG qqqq.
The experiments on Omniglot were trained for 520k iterations with batch size 128 using the Adam
optimizer (Kingma & Ba, 2015) with learning rate lθ´6 for D, lθ´5 for G, and lθ´7 for MLPG (as
done by Karras et al. (2018b)). The regularization parameter was set to 0.
The experiments on Voxceleb2 were trained for 250k iterations with batch size 64 using the Adam
optimizer (Kingma & Ba, 2015) with learning rate lθ´4 for both D and G and lθ´6 for MLPG.
The regularization parameter was set to 10 (as done by Karras et al. (2018b)) since we noticed that
it stabilized and sped up the training, and in contrast to Omniglot and the Gaussian experiments did
not seem to hurt the results.
36