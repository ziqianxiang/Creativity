Published as a conference paper at ICLR 2020
Rotation-invariant clustering of neuronal
RESPONSES IN PRIMARY VISUAL CORTEX
Ivan Ustyuzhaninov,1-3 Santiago A. Cadena,1-3 Emmanouil Froudarakis,4,5 Paul G. Fahey,4,5
Edgar Y. Walker,4,5 Erick Cobos,4,5 Jacob Reimer,4,5 Fabian H. Sinz,4,5
Andreas S. Tolias,1,4-6 Matthias Bethge,1-3,5# Alexander S. Ecker1-3,5##,*
1	Centre for IntegratiVe Neuroscience, University of Tubingen, Germany
2	Bernstein Center for Computational Neuroscience, University of Tubingen, Germany
3	Institute for Theoretical Physics, University of Tubingen, Germany
4	Department of Neuroscience, Baylor College of Medicine, Houston, TX, USA
5	Center for Neuroscience and Artificial Intelligence, BCM, Houston, TX, USA
6	Department of Electrical and Computer Engineering, Rice University, Houston, TX, USA
↑ Authors contributed equally
*	Present address: Department of Computer Science, University of Gottingen, Germany
*	ecker@cs.uni-goettingen.de
Abstract
Similar to a convolutional neural network (CNN), the mammalian retina encodes
visual information into several dozen nonlinear feature maps, each formed by one
ganglion cell type that tiles the visual space in an approximately shift-equivariant
manner. Whether such organization into distinct cell types is maintained at the
level of cortical image processing is an open question. Predictive models building
upon convolutional features have been shown to provide state-of-the-art perfor-
mance, and have recently been extended to include rotation equivariance in order
to account for the orientation selectivity of V1 neurons. However, generally no di-
rect correspondence between CNN feature maps and groups of individual neurons
emerges in these models, thus rendering it an open question whether V1 neurons
form distinct functional clusters. Here we build upon the rotation-equivariant rep-
resentation of a CNN-based V1 model and propose a methodology for clustering
the representations of neurons in this model to find functional cell types indepen-
dent of preferred orientations of the neurons. We apply this method to a dataset
of 6000 neurons and visualize the preferred stimuli of the resulting clusters. Our
results highlight the range of non-linear computations in mouse V1.
Align readouts
1 Fit rotation-equivariant CNN
Feature weights → clusters
Cluster aligned readouts
Figure 1: An overview of our approach. 1 Fit rotation-equivariant CNN to predict neural responses
and use readout vectors ri as proxies for neural computations. 2 Align readouts to account for
different preferred orientations. 3 Cluster the aligned readouts.
1
Published as a conference paper at ICLR 2020
1	Introduction
A compact description of the nonlinear computations in primary visual cortex (V1) is still elusive.
Like in the retina (Baden et al., 2016; Sanes & Masland, 2015), such understanding could come
from a functional classification of neurons. However, it is currently unknown if excitatory neurons
in V1 are organized into functionally distinct cell types.
It has recently been proposed that predictive models of neural responses based on convolutional neu-
ral networks could help answer this question (Klindt et al., 2017; Ecker et al., 2019). These models
are based on a simple principle (Fig. 1-1 ): learn a core (e.g. a convolutional network) that is shared
among all neurons and provides nonlinear features Φ(x), which are turned into predictions of neural
responses by a linear readout for each neuron (AntoKk et al., 2016). Models based on this basic ar-
chitecture exploit aspects of our current understanding ofV1 processing. First, convolutional weight
sharing allows us to characterize neurons performing the same computation but with differently lo-
cated receptive fields by the same feature map (Klindt et al., 2017; McIntosh et al., 2016; Kindel
et al., 2019; Cadena et al., 2019). Second, V1 neurons can extract local oriented features such as
edges at different orientations, and most low-level image features can appear at arbitrary orienta-
tions. Therefore, Ecker et al. (2019) proposed a rotation-equivariant convolutional neural network
model of V1 that extends the convolutional weight sharing to the orientations domain.
The basic idea of previous work (Klindt et al., 2017; Ecker et al., 2019) is that each convolutional
feature map could correspond to one cell type. While this idea is conceptually appealing, it hinges
on the assumption that V1 neurons are described well by individual units in the shared feature space.
However, existing models do not tend to converge to such solutions. Instead, V1 neurons are better
described by linearly combining units from the same spatial location in multiple different feature
maps (Ecker et al., 2019). Whether or not there are distinct functional cell types in V1 is therefore
still an open question.
Here, we address this question by introducing a clustering method on rotation-equivariant spaces.
We treat the feature weights (Fig. 1-1 ) that map convolutional features to neural responses as
an approximate low-dimensional vector representation of this neuron’s input-output function. We
then split neurons into functional types using a two-stage procedure: first, because these feature
weights have a rotation-equivariant structure, we find an alignment that rotates them into a canonical
orientation (Fig. 1-2 ); in a second step, we cluster them using standard approaches such as k-means
or Gaussian mixture models (Fig. 1-3 ). We apply our method to the published model and data of
Ecker et al. (2019) that contains recordings of around 6000 neurons in mouse V1 under stimulation
with natural images. Our results suggest that V1 neurons might indeed be organized into functional
clusters. The dataset is best described by a GMM with around 100 clusters, which are to some extent
redundant but can be grouped into a smaller number of 10-20 groups. We analyse the resulting
clusters via their maximally exciting inputs (MEIs) (Walker et al., 2018) to show that many of these
functional clusters do indeed correspond to distinct computations.
2	Related work
Unsupervised functional clustering via system identification As outlined in the introduction,
our work builds directly upon the methods developed by Klindt et al. (2017) and Ecker et al. (2019).
While these works view the feature weights as indicators assigning each neuron to its ‘cell type’
(feature map), we here take a different view on the same model: rather than focusing on the convo-
lutional features and viewing them as cell types, we treat the feature weights as a low-dimensional
representation of the input-output function of each neuron and perform clustering in this space. This
view on the problem has the advantage that there is no one-to-one correspondence between the
number of feature maps and the number of cell types and we disentangle model fitting from its inter-
pretation. On the other hand, our approach comes with an addition complexity: because the feature
weights obey rotational equivariance and we would like our clustering to be invariant to rotations,
we require a clustering algorithm that is invariant with respect to a class of (linear) transformations.
Invariant clustering A number of authors have developed clustering methods that are invariant
to linear (Tarpey, 2007), affine (Brubaker & Vempala, 2008) or image transformations by rotations,
2
Published as a conference paper at ICLR 2020
scalings and translations (Frey & Jojic, 2002). Ju et al. (2019) cluster natural images by using a
CNN to represent the space of invariant transformations rather than specifying it explicitly.
Alignments Instead of using custom clustering algorithms that are invariant under certain trans-
formations, we take a simpler approach: we first transform our features such that they are maximally
aligned using the class of transformations the clustering should be invariant to. This approach has
been used in other contexts before, usually by minimizing the distances between the transformed
observations. Examples include alignment of shapes in Rd using rigid motions (Gower, 1975; Dry-
den & Mardia, 1998), alignment of temporal signals by finding monotonic input warps (Zhou & De
la Torre, 2012), or alignment of manifolds with the distance between the observations being defined
according to the metric on the manifold (Wang & Mahadevan, 2008; Cui et al., 2014). There is
also work on alignment objectives beyond minimizing distances between transformed observations,
examples of which include objectives based on generative models of observations (Kurtek et al.,
2011; Duncker & Sahani, 2018) or probabilistic ones which are particularly suited for alignment
with multiple groups of underlying observations (Kazlauskaite et al., 2019).
3	Rotation-equivariant clustering
Our goal is to cluster neurons in the dataset into groups performing similar computations. To do so,
we use their low-dimensional representations obtained from the published rotation-equivariant CNN
of Ecker et al. (2019), which predicts neural activity as a function of an external image stimulus. We
briefly review this model before describing our approach to rotation-invariant clustering.
CNN model architecture The model consists of two parts (Fig. 1-1 ):
1.	A convolutional core that is shared by all neurons and computes feature representations
Φ(x) ∈ RW×H×K, where x is the input image, W × H is the spatial dimensionality and
K is the number of feature maps.
2.	A separate linear readout Wn = Sn 0 rn, ∈ RW×H×K for each neuron n = 1,...,N,
factorized into a spatial mask sn and a vector of feature weights rn .
The predicted activity of a neuron n for image x is
an(x) = f (Wn ∙ Φ(x)) = f (rn ∙ Sn ∙ Φ(x)) ∈ R	(1)
where f (∙) is a non-linear activation function. Such a CNN therefore provides K-dimensional fea-
ture weights rn, characterising linear combinations of spatially weighted image features Sn ∙ Φ(x)
that are predictive of neural activity. We treat these feature weights as finite-dimensional proxies of
actual computations implemented by the neurons. Because masks Sn (another component of read-
outs Wn defined above) are irrelevant for our analysis, we will often refer to rn simply as readout
vectors. We will be referring to the matrix having ri as its rows as the readout matrix R ∈ RN ×K .
Rotation-equivariant core Feature representations Φ(x) are rotation-equivariant, meaning that
weight sharing is not only applied across space but also across rotations: for each convolutional
filter there exist O rotated copies, each rotated by 2∏∕O. Feature vectors φj(x) at position (i,j)
therefore consist of F different features, each computed in O linearly-spaced orientations (such
that F × O = K). We can think of φij (x) as being reshaped into an array of size F × O. Having
computed φj (x), We can compute φj (x0) with x0 being a stimulus X rotated around (i,j) by 2∏∕O
by cyclically shifting the last axis of φij (x) by one step (this mechanism is illustrated in Fig. 2).
Rotation-equivalent computations The linear readout adheres to the same rotation-equivariant
structure as the core. As our goal is to cluster the neurons by the patterns of features they pool
while being invariant to orientation, we need to account for the set of weight transformations that
correspond to a rotation of the stimulus when clustering neurons. We illustrate this issue with a
small toy example consisting of six neurons that fall into two cell types (Fig. 2B). Within each cell
type (columns in Fig. 2B), the individual neurons differ only in their orientation. More formally,
we define the computations performed by two neurons ni and nj to be rotation-equivalent if there
exists a rotation ψij such that for any input stimulus x we have ani (x) = anj (ψij(x)). We will
refer to such neurons as rotation-equivalent as well.
3
Published as a conference paper at ICLR 2020
A
,O。
Sn ∙ φ (密)=/5 O^ 0.6 ; Sn ∙ φ (学)=
% * ZV
。。③	CoQ
2.6	1	0.4] 右 /Λ⅛fe∖ 「0.4	2.6	1
0.9	0.5 2 _|	; Sn ∙ φ(∕J= |_2	0.9	0.5
% W \"	勿运M'
Type 1: 1×
+ 2×
Type 2: 1×
+ 2×
X 1 0.4 2.6
=	0.5 2 0.9
0
an
2π∕3
an
π∕3
an
1 0.4 2.6
0.5 2
1 0.4
.5 2
0.9
2.6
0.9
12
1.5
2.4
001
002
3.5
feature vectors (reshaped)
0 ( AX 1 0.4 2.6	1 0 0	3
an \	) — k([θ.5 2 0.9 θ 0 2	0_|)	―3
a∏∕3 ( ʌ = XC 1 0.4 2.61
an	乙 l[0.5 2 0.9
2.2
2∏∕3
an
X01.5 02.4
20..691 θ2000101=	3.6
、	{	'
feature vectors (reshaped)
B
0
0
0
0
0
1
2
0
Figure 2: Toy example illustrating the computations in a rotation-equivariant CNN with two features
(red and blue; cartoon feature representations are shown on top of corresponding values of Φ(x))
in three orientations (0, π∕3, 2π∕3). A: Output of the rotation-equivariant CNN for an input image
rotated by ∏∕3 (base orientation) can be computed by a cyclic shift. B: Example of two distinct
types of neurons (columns) in three orientations (rows). Computations for both types consist of
linear combinations of the two features computed by the CNN with the same weights, but in different
relative orientations. Readouts of neurons of the same type in different orientations are cyclic shifts
of each other, since they produce the same outputs on correspondingly rotated inputs.
Readouts of rotation-equivalent neurons Directly clustering the readout matrix R does not re-
spect the rotation equivalence, because readout vectors of neurons implementing rotation-equivalent
computations are not identical (Fig. 2). To address that, we first modify R to obtain a matrix R
with the rows corresponding to rotation-equivalent neurons aligned to a canonical form, and then
cluster R to obtain functional cell types. Rotating an input x by a multiple of 2π∕O corresponds to
cyclically shifting Φ(x), so the readout vectors of two rotation-equivalent neurons whose orienta-
tion difference ψij is a multiple of 2π∕O are also cyclic shifts of each other (Fig. 2). For arbitrary
rotations that are not necessarily a multiple of 2π∕O, we assume the readout rnj of neuron nj to
be a linear interpolation of cyclic shifts of rni corresponding to the two nearest rotations which are
multiples of 2π∕O. Formally, we define a cyclic rotation matrix by an angle α ∈ [0, 2π) as follows
(matrix has shape O × O ; column indices are shown above the matrix):
1	i	i+1	i+2	i+3	O					
0	.. 0	..	.0 .0	1-γ 0	γ 1-γ	0	.. γ	..	.0 .0	i=		αO __27_	mod O,	(2)
.				.		,				
. .				. .			αO			
0	..	.	1-γ	γ	0	0	..	.0	γ=	-		- 2π	i.	
Given a readout vector rn ∈ RK, we can think of it as a matrix rn ∈ RO×F with columns corre-
sponding to readout coefficients for different orientations of a single feature. The cyclic rotation of
a readout rn by an angle α ∈ [0, 2π) can be expressed as a matrix multiplication rn (α) = Sαrn .
Note, by writing rn(α) as a function of α we refer to cyclically rotated (transformed) readouts,
while rn are the fixed ones coming from a pre-trained CNN and rn (0) = rn.
For two rotation-equivalent neurons ni and nj , the readout vector rnj can be computed as a cyclic
rotation of rni by α, which is the rotation angle of ψij . If α is a multiple of 2π∕O; otherwise it is
only an approximation which becomes increasingly accurate as O increases.
Aligning the readouts Assuming V1 neurons form discrete functional cell types, all neurons in
the dataset (and hence the readouts characterising them) can be partitioned into non-overlapping
classes w.r.t. the rotation equivalence relation we introduced above. Choosing one representative of
each class and replacing the rows of R with their class representatives, We can obtain R from R.
Next, we discuss an algorithm for finding such representatives of each class.
4
Published as a conference paper at ICLR 2020
We claim that by minimising the sum of pairwise distances between the cyclically rotated readouts
NN
{α*} = arg min
||ri (Oi) - Tj (αj 川,
{αi} i=1 j=i+1
(3)
we can transform each readout into a representative of a corresponding equivalence class (same for
all readouts of a class), i.e. r,(a；) = rj (0j) if neurons i and j are rotation-equivalent. This is
indeed the case because the readouts of the same equivalence class lie on the orbit obtained by cycli-
Cally rotating any representative of that class. Such angles {a；} that neurons of the same class end
up on the same point on the orbit (i.e. aligned to the same class representative) clearly minimise
Eq. (3), and since different orbits do not intersect (they are different classes of equivalence), read-
outs of different equivalence classes cannot end up at the same point. Note that the resulting class
representatives are not arbitrary, but those with the smallest sum of distances between each other in
the Euclidean space. This mechanism is illustrated in Fig. 1-2 .
Clustering aligned readouts Having obtained R With rows ri (a；), We can cluster the rows of
this matrix using any standard clustering method (e.g. K-Means, GMM, etc.) to obtain groups of
neurons (cell types) performing similar computations independent of their preferred orientations.
Continuous relaxation of cyclic rotations The rotation-
invariant clustering described above is based on solving the opti-
misation problem in Eq. (3). To do so, we would typically use a
gradient-based optimisation, which is prone to local minima be-
cause of the way we define cyclic rotations in Eq. (2). According
to that definition, a rotated readout is a linear combination of two
nearest base rotations, or rather a linear combination of all such
rotations with only two coefficients being non-zero. That means
that gradients of all but two coefficients w.r.t. the angle a are
zero, and the optimisation would converge to a local minimum
corresponding to the best linear combination of the two base ro-
tations initialised with non-zero coefficients (Fig. 3).
To address this issue, we propose to approximate Eq. (2), such
that rni (a) is a linear combination of all base rotations with non-
zero coefficients, with the coefficients for the two nearest base
rotations being the largest. Specifically we compute the coeffi-
cients by sampling the von Mises density at fixed orientations to
ensure cyclic boundary conditions and define T% (α), a continu-
ous relaxation of Tni (α), as r^ (ɑ) = Sarni where
Figure 3: Distance between two vec-
tors (top left corner) with first one
fixed and second cyclically shifted
by an angle on the x-axis. Contin-
uous relaxation (shades of blue) of
linearly interpolated (black) cyclic
shifts smooths gradients and helps
overcome local minima.
γ1	γ2
YO	Yi
.
.
.
Y2	Y3
YO
γO-i	.1	exp(Tcos(a - (i - 1) ∙ 2π∕0))	/八
. with Yi = 丁上-----------------------------.	(4)
.
./	P exp(T cos(a — (i - 1) ∙ 2π∕0))
Y1	i=1
The parameter T ≥ 0 controls the sparseness of coefficients {Yi}. For small T, many of the co-
efficients are significantly greater than zero, allowing the optimiser to propagate the gradients and
reduce the effect of initialisation. As T increases, Tn (α) becomes more similar to rn (α), and the
rotations by multiples of 2π∕O are recovered. In the limit, Tni (2∏k∕0) → rn (2πk∕0) as T → ∞
(Fig. 3). Instead of fixing T, we learn it by optimising the regularised alignment objective with
additional reconstruction loss preventing trivial solutions (e.g. all coordinates of Ti(α,) being the
same for T = 0):
NN	N
{ai； } = argminX X ||Ti(Oi)- Tj(aj)|| + βX ||Ti(O) -Ti||.	⑸
{αi},T i=1 j=i+1	i=1
5
Published as a conference paper at ICLR 2020
A
β=2
Figure 4: Synthetic data set: generation, alignment and dependence on noise. A: Panel R shows
the unaligned synthetic data set as well as the corresponding shifted GP samples for each of the two
groups of neurons (see details in the main text). Colored boxes in R correspond to the colors of
corresponding GP samples. Panels R(α*) and R(0) show aligned readouts and readouts rotated
by 0 using Eq. (4) respectively. R(0) should be similar to R for an adequate choice of β (and
consequently optimised value of T). B: Effect of observation noise. Means of pairwise distances
for each of the two groups shown in matrix R for two levels of Gaussian noise added to the dataset.
Black dashed line: expected pairwise distance due to noise only (i.e. for perfectly aligned data with
added noise). Raw and aligned matrices for each of the two groups are shown below the bar plots.
β=0.01 B
~ , , , ~ ,,
R(α*) R(0) 曲 ιθ1 NoiseSD = 0.01 30 NoiseSD = 1.07
4 Experiments
Synthetic dataset We generate a small toy dataset consisting of 16 hypothetical neurons (readouts)
of two cell types to illustrate the proposed alignment method. Each readout consists of just one
feature in eight base orientations (linearly spaced between 0 and 7∏∕4) and is generated by one of
the two underlying types of readouts cyclically shifted by a random angle φ ∈ [0,2∏). To generate
such a dataset, we draw two independent noiseless functions from a Gaussian process (GP) with
a periodic kernel (with period 2∏), then for each readout we randomly choose one of the two GP
samples, shift it by an angle φ and evaluate the shifted function at the base orientations to obtain an
8-dimensional vector modelling the observed readout values. This process is illustrated in Fig. 4A.
Neural data We use the same dataset as in Ecker et al. (2019), consisting of simultaneous record-
ings of responses of 6005 excitatory neurons in mouse primary visual cortex (layers 2/3 and 4).
Model details We analyse a rotation-equivariant CNN consisting of a three-layer core with 16
features in 8 orientations in each layer (kernel sizes 13, 5, 5) and 128-dimensional readouts (F = 16,
O = 8). We use the pre-trained model provided by Ecker et al. (2019). We align the readout matrix
R by minimising Eq. (5) w.r.t. the rotation angles αi and temperature T. We fit models for 20
log-spaced values of β in [0.001, 10], and choose for analysis the one with the smallest alignment
loss (Eq. (3)) among the models with optimised temperature T > 5. We use Adam (Kingma & Ba,
2015) with early stopping and initial learning rate of 0.01 decreased three times.
Clustering aligned readouts We use the Gaussian mixture model implemented in scikit-learn
(Pedregosa et al., 2011) for clustering the aligned readouts R. We use spherical covariances to
reduce the number of optimised parameters. To obtain a quantitative estimate of the number of
clusters in R, we randomly split the dataset of 6005 neurons into training (4000 neurons) and test
(2005 neurons) sets, fit GMMs with different numbers of clusters on the training set, and then
evaluate the likelihood of the fitted model on the test set.
6
Published as a conference paper at ICLR 2020
5 Results
5.1	Synthetic data set alignment
We start by demonstrating on a synthetic dataset (Sec. 4) that optimising Eq. (5) can successfully
align the readouts (Fig. 4A), assuming β has been chosen appropriately. Note that readouts have
been shifted by arbitrary angles (not multiples of n/4 as demonstrated for readouts in colored boxes
in Fig. 4A), and they are aligned precisely via interpolation Eq. (4). We can also see the effect
of the parameter β, controlling the relative weight of the reconstruction term (i.e. similarity of
readouts rotated by 0 degrees to the observations). Small values of β incur a small cost for poor
reconstructions resulting in small optimised values of T and over-smoothed aligned readouts.
We next ask whether the alignment procedure still works in the presence of observational noise
(Fig. 4B). For small to moderate noise levels (Fig. 4B, left), alignment reduces the pairwise distances
to the level expected from the observation noise (shown at the top), confirming the visual impression
(shown at the bottom) that alignment works well. For high noise levels (Fig. 4B, right), alignment
breaks down as expected, and we observe overfitting to the noise patterns (shown by the pairwise
distances after alignment dropping below the level expected from observation noise alone).
5.2	Mouse V1 dataset
Clustering We evaluate the GMM used to cluster R for different
numbers of clusters. The test likelihood starts to plateau at around 100
clusters (Fig. 5), so we use 100 clusters in the following.
Visualization of clusters To visualize the clustering result, we com-
pute a two-dimensional t-SNE embedding (van der Maaten & Hinton,
2008) of the matrix of aligned readouts R, which is coloured accord-
ing to the GMM clustering of R with 100 clusters (Fig. 6). Note that
we use the embedding only for visualization, but cluster 128D aligned
readouts in R directly. In addition to the embeddings, we also visualize
the computations performed by some of the clusters by showing maxi-
mally exciting inputs (MEIs). We compute MEIs via activity maximi-
sation (Erhan et al., 2009; Walker et al., 2018) and show the stimuli that
maximally drive the 16 best-predicted neurons of each cluster. We ob-
Figure 5: Test set like-
lihood of GMMs applied
to R as a function of the
number of clusters.
serve that MEIs corresponding to neurons of the same cluster are generally consistent up to rotation
and receptive field location, suggesting that the proposed clustering method captures the similarities
in the neural computations while ignoring the nuisances as desired.
Network learned redundant features We noticed a number of clusters with similar MEIs (e.g.
Block 9 and Block 13 in Fig. 6). There could be two reasons for this observation: (a) the neural com-
putations corresponding to these clusters could be different in some other aspect, which we cannot
tell by inspecting MEIs as they represent only the maximum of a complex function, or (b) the fea-
tures learned by the CNN could be redundant, i.e. the hidden layers could learn to approximate the
same function in multiple different ways. To answer this question, we compute a cluster confusion
matrix (Fig. 7, left), which quantifies how similar the response predictions of different clusters are
across images. The element (p, q) corresponds to the correlation coefficient between the predicted
responses on the entire training set of hypothetical neurons with cluster means for clusters p and q
used as readouts, accounting for potential differences in canonical orientation across clusters. By
greedily re-arranging clusters in the matrix into blocks based on their correlations, we show that
the 100 clusters in the model can be grouped into a much smaller number of functionally distinct
clusters. Using a correlation threshold of 0.5 in this re-arrangement procedure, we obtain an exam-
ple arrangement into 17 blocks (Fig. 7). Thus, the network has learned an internal representation
that allows constructing very similar functions in multiple ways, suggesting that further pruning the
learned network before clustering could lead to a more compact feature space for V1.
Finally, to quantify how consistent the resulting 17 groups of clusters are, we compute an MEI
confusion matrix (Fig. 7, right panel). Its (i, j) element is the predicted activity of neuron j for the
MEI of neuron i, after accounting for orientation and receptive field location (i.e. aj (yi), where
7
Published as a conference paper at ICLR 2020
Block 13
Block 12
Block 9
Block 5
Block 1
Block8
Block 6
“ “ F。
。•柄	6	3
n-nτo-s
Ka-
n B3 口 q
* , QD
HH
Block 7
Block 11
Block 3
y「7 K3 鼻∙
I^l ■■ IB IBl
■■
-¾ α ʧ
Block 17
Block 2
mBB-m
Block16___I 翻」窿］-梗
M U Kl Dl
L□ ES ■1 K□
而■ ■ ■
Block 14
1_3 « r∙<j Kfl

K≡ f** 京 3
UΛ kd Kd Kd
13 KSI-B
UDJ^B
L∙	/ Z-M-X j CRTL	1 1 1 ∙	c∙ J 1	1 •	1	1 J ɪɪ*	1	1	1 ∙	J J 1 zɔʌ JT-K K 1 j
Figure 6: 2D t-SNE embedding of the aligned readouts R, colored according to the GMM clustering
with 100 components. Black stars show the locations of cluster centers. For some of the clusters,
the MEIs of 16 best predicted neurons of that cluster are shown. The titles in the MEI subfigures
show which matrix block in Fig. 7 (left) that cluster belongs to.
yi is the MEI of neuron i moved and rotated such that it optimally drives neuron j). We show this
matrix using the same grouping as for the cluster confusion matrix above and restrict it to the 542
(out of 6005) best predicted neurons (with test set correlation ≥ 0.7). Note that some of the blocks
from the cluster confusion matrix do not appear here, indicating that those clusters include poorly
predicted neurons (e.g. block 17). The MEI confusion matrix exhibits a block-diagonal structure,
with most MEIs driving neurons within the same blocks most strongly, albeit with different degrees
of within-block similarity.
8
Published as a conference paper at ICLR 2020
Figure 7: Left: Cluster confusion matrix (100 × 100) for 100 clusters shown in Fig. 6. Rows
and columns have been arranged into 17 groups (blocks). Right: MEI confusion matrix for well-
predicted neurons (test correlation ≥ 0.7) arranged into the same 17 blocks as on the left.
6 Conclusions and future work
We have presented an approach to clustering neurons into putative functional cell types invariant
to location and orientation of their receptive field. We find around 10-20 functional clusters, the
boundaries of some of which are not very clear-cut. To systematically classify the V1 functional cell
types, these proposals need to be subsequently examined based on a variety of biological criteria
reflecting the different properties of the neurons and the prior knowledge about the experiment.
9
Published as a conference paper at ICLR 2020
References
Jgn AntoKk, Sonja B. Hofer, James A. Bednar, and Thomas D. Mrsic-flogel. Model Constrained
by Visual Hierarchy Improves Prediction of Neural Responses to Natural Scenes. PLoS Comput
Biol, 2016.
Tom Baden, PhiliPP Berens, Katrin Franke, Miroslav Romdn Ros6n, Matthias Bethge, and Thomas
Euler. The functional diversity of retinal ganglion cells in the mouse. Nature, 529, 2016.
SPencer Ch. Brubaker and Santosh VemPala. IsotroPic Pca and affine-invariant clustering. In Pro-
ceedings of the 2008 49th Annual IEEE Symposium on Foundations of Computer Science, FOCS
2008, 2008.
Santiago A. Cadena, George H. Denfield, Edgar Y. Walker, Leon A. Gatys, Andreas S. Tolias,
Matthias Bethge, and Alexander S. Ecker. DeeP convolutional models imProve Predictions of
macaque v1 resPonses to natural images. PLoS computational biology, 15(4), 2019.
Zhen Cui, Hong Chang, Shiguang Shan, and Xilin Chen. Generalized unsuPervised manifold align-
ment. In Advances in Neural Information Processing Systems (NIPS). 2014.
Ian L. Dryden and Kanti V. Mardia. Statistical Shape Analysis. Wiley, Chichester, 1998.
Lea Duncker and Maneesh Sahani. TemPoral alignment and latent gaussian Process factor inference
in PoPulation sPike trains. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi,
and R. Garnett (eds.), Advances in Neural Information Processing Systems (NIPS). 2018.
Alexander S. Ecker, Fabian H. Sinz, Emmanouil Froudarakis, Paul G. Fahey, Santiago A. Cadena,
Edgar Y. Walker, Erick Cobos, Jacob Reimer, Andreas S. Tolias, and Matthias Bethge. A rotation-
equivariant convolutional neural network model of Primary visual cortex. In International Con-
ference on Learning Representations, 2019.
Dumitru Erhan, Yoshua Bengio, Aaron Courville, and Pascal Vincent. Visualizing higher-layer
features of a deeP network. Technical rePort, University of Montreal, 2009. Also Presented at the
ICML 2009 Workshop on Learning Feature Hierarchies, Montreal, Canada.
Brendan J Frey and Nebojsa Jojic. Fast, large-scale transformation-invariant clustering. In Advances
in Neural Information Processing Systems (NIPS). MIT Press, 2002.
James C. Gower. Generalized Procrustes analysis. Psychometrika, 40(1), 1975.
XU Ju, Joao F. Henriques, and Andrea Vedaldi. Invariant information clustering for unsupervised
image classification and segmentation. In The IEEE International Conference on Computer Vision
(ICCV), 2019.
Ieva Kazlauskaite, Carl Henrik Ek, and Neill Campbell. Gaussian process latent variable alignment
learning. In AISTATS, volume 89. PMLR, 2019.
William F. Kindel, Elijah D. Christensen, and Joel Zylberberg. Using deep learning to probe the
neural code for images in primary visual cortex. Journal of Vision, 19(4), 2019.
Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In 3rd Interna-
tional Conference on Learning Representations (ICLR), 2015.
David Klindt, Alexander S. Ecker, Thomas Euler, and Matthias Bethge. Neural system identifica-
tion for large populations separating “what” and “where”. In Advances in Neural Information
Processing Systems (NIPS), 2017.
Sebastian A. Kurtek, Anuj Srivastava, and Wei Wu. Signal estimation under random time-warpings
and nonlinear signal alignment. In J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett, F. Pereira, and
K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems (NIPS). 2011.
Lane McIntosh, Niru Maheswaranathan, Aran Nayebi, Surya Ganguli, and Stephen Baccus. Deep
learning models of the retinal response to natural scenes. In Advances in Neural Information
Processing Systems (NIPS). 2016.
10
Published as a conference paper at ICLR 2020
Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas,
Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and Edouard DUch-
esnay. Scikit-learn: Machine learning in python. Journal of Machine Learning Research, 2011.
Joshua R. Sanes and Richard H. Masland. The types of retinal ganglion cells: current status and
implications for neuronal classification. Annual review of neuroscience, 38, 2015.
Thaddeus Tarpey. Linear Transformations and the k-Means Clustering Algorithm: Applications to
Clustering Curves. The American Statistician, 61, 2007.
Laurens van der Maaten and Geoffrey E. Hinton. Visualizing high-dimensional data using t-sne.
Journal of Machine Learning Research, 9, 2008.
Edgar Y. Walker, Fabian H. Sinz, Emmanouil Froudarakis, Paul G. Fahey, Taliah Muhammad,
Alexander S. Ecker, Erick Cobos, Jacob Reimer, Xaq Pitkow, and Andreas S. Tolias. Inception in
visual cortex: in vivo-silico loops reveal most exciting images. bioRxiv, 2018.
Chang Wang and Sridhar Mahadevan. Manifold alignment using procrustes analysis. In Proceedings
of the 25th International Conference on Machine Learning (ICML), 2008.
Feng Zhou and Fernando De la Torre. Generalized time warping for multi-modal alignment of
human motion. In 2012 IEEE Conference on Computer Vision and Pattern Recognition, 2012.
11
Published as a conference paper at ICLR 2020
A Random permutations of features
Figure A1: 2D t-SNE embedding of the aligned readouts RR with feature weights randomly permuted
for each of the neurons. The colors correspond to the GMM clustering with 100 components. Black
stars show the locations of cluster centers. For some of the clusters, the MEIs of 16 best predicted
neurons of that cluster are shown.
12
Published as a conference paper at ICLR 2020
Figure A2: 2D t-SNE embedding of the aligned readouts RR with feature weights randomly permuted
across the neurons. The colors correspond to the GMM clustering with 100 components. Black stars
show the locations of cluster centers. For some of the clusters, the MEIs of 16 best predicted neurons
of that cluster are shown.
Control 1
Aligned readouts	(random permutations for each neuron)
Control 2
(random permutations across neurons)
Figure A3: t-SNE embeddings for the aligned readouts (Fig. 6), and the controls with randomly
permuted features for each neuron (Fig. A1) and across the neurons (Fig. A2).
13
Published as a conference paper at ICLR 2020
B Synthetic dataset: dependence on noise
Noise SD = 0.00
Noise SD = 0.10
Noise SD = 0.20
Raw data	Aligned
t-SNE (aligned)
t-SNE (aligned)
Noise SD = 0.75	Noise SD = 1.00
Noise SD = 2.00	Noise SD = 5.00
Figure B1: Alignment of a synthetic dataset of 100 observations generated using the procedure
described in Sec. 4 for different amount of i.i.d. Gaussian noise added to the observations. The
panels for each noise level show the 16 (out of 100) examples of the raw and aligned data as well
the t-SNE embeddings of raw and aligned data coloured according to the GMM clustering with two
components.
14
Published as a conference paper at ICLR 2020
C Merges and splits of cluster confusion matrix blocks
Figure C1: Sequential merges of the three pairs of blocks with the highest correlations in the cluster
confusion matrix (Fig. 7, left). The merged blocks and the correlation values are shown in the titles
of panels.

BlockI 3
-- ' ” KI
• ÷ * '
-	4
Figure C2: Sequential splits of the three pairs of blocks in the cluster confusion matrix (Fig. 7, left).
The merged blocks, the correlation values, and the examples of MEIs of one of the GMM clusters
in each of the splitted blocks are shown for each splitting step.
15