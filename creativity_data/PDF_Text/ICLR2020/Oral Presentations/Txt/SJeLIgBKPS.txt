Published as a conference paper at ICLR 2020
Gradient Descent Maximizes the Margin of
Homogeneous Neural Networks
Kaifeng Lyu & Jian Li
Institute for Interdisciplinary Information Sciences
Tsinghua University
Beijing, China
vfleaking@gmail.com,lijian83@mail.tsinghua.edu.cn
Ab stract
In this paper, we study the implicit regularization of the gradient descent algorithm
in homogeneous neural networks, including fully-connected and convolutional
neural networks with ReLU or LeakyReLU activations. In particular, we study
the gradient descent or gradient flow (i.e., gradient descent with infinitesimal step
size) optimizing the logistic loss or cross-entropy loss of any homogeneous model
(possibly non-smooth), and show that if the training loss decreases below a certain
threshold, then we can define a smoothed version of the normalized margin which
increases over time. We also formulate a natural constrained optimization problem
related to margin maximization, and prove that both the normalized margin and its
smoothed version converge to the objective value at a KKT point of the optimiza-
tion problem. Our results generalize the previous results for logistic regression
with one-layer or multi-layer linear networks, and provide more quantitative con-
vergence results with weaker assumptions than previous results for homogeneous
smooth neural networks. We conduct several experiments to justify our theoretical
finding on MNIST and CIFAR-10 datasets. Finally, as margin is closely related
to robustness, we discuss potential benefits of training longer for improving the
robustness of the model.
1 Introduction
A major open question in deep learning is why gradient descent or its variants, are biased towards
solutions with good generalization performance on the test set. To achieve a better understanding,
previous works have studied the implicit bias of gradient descent in different settings. One simple
but insightful setting is linear logistic regression on linearly separable data. In this setting, the model
is parameterized by a weight vector w, and the class prediction for any data point x is determined
by the sign of w>x. Therefore, only the direction w/kwk2 is important for making prediction.
Soudry et al. (2018a;b); Ji & Telgarsky (2018; 2019c); Nacson et al. (2019c) investigated this prob-
lem and proved that the direction of w converges to the direction that maximizes the L2-margin
while the norm of w diverges to +∞, if we train w with (stochastic) gradient descent on logistic
loss. Interestingly, this convergent direction is the same as that of any regularization path: any se-
quence of weight vectors {wt} such that every wt is a global minimum of the L2-regularized loss
L(W) + λ ∣∣wk2 with λt → 0 (Rossetetal., 2004). Indeed, the trajectory of gradient descent is also
pointwise close to a regularization path (Suggala et al., 2018).
The aforementioned linear logistic regression can be viewed as a single-layer neural network. A nat-
ural and important question is to what extent gradient descent has similiar implicit bias for modern
deep neural networks. For theoretical analysis, a natural candidate is to consider homogeneous neu-
ral networks. Here a neural network Φ is said to be (positively) homogeneous if there is a number
L > 0 (called the order) such that the network output Φ(θ; x), where θ stands for the parameter and
x stands for the input, satisfies the following:
∀c > 0 : Φ(cθ; x) = cLΦ(θ; x) for all θ and x.	(1)
It is important to note that many neural networks are homogeneous (Neyshabur et al., 2015a; Du
et al., 2018). For example, deep fully-connected neural networks or deep CNNs with ReLU or
1
Published as a conference paper at ICLR 2020
LeakyReLU activations can be made homogeneous if we remove all the bias terms, and the order L
is exactly equal to the number of layers.
In (Wei et al., 2019), it is shown that the regularization path does converge to the max-margin di-
rection for homogeneous neural networks with cross-entropy or logistic loss. This result suggests
that gradient descent or gradient flow may also converges to the max-margin direction by assuming
homogeneity, and this is indeed true for some sub-classes of homogeneous neural networks. For gra-
dient flow, this convergent direction is proven for linear fully-connected networks (Ji & Telgarsky,
2019a). For gradient descent on linear fully-connected and convolutional networks, (Gunasekar
et al., 2018b) formulate a constrained optimization problem related to margin maximization and
prove that gradient descent converges to the direction of a KKT point or even the max-margin di-
rection, under various assumptions including the convergence of loss and gradient directions. In
an independent work, (Nacson et al., 2019a) generalize the result in (Gunasekar et al., 2018b) to
smooth homogeneous models (we will discuss this work in more details in Section 2).
1.1	Main Results
In this paper, we identify a minimal set of assumptions for proving our theoretical results for ho-
mogeneous neural networks on classification tasks. Besides homogeneity, we make two additional
assumptions:
1.	Exponential-type Loss Function. We require the loss function to have certain exponential
tail (see Appendix A for the details). This assumption is not restrictive as it includes the
most popular classfication losses: exponential loss, logistic loss and cross-entropy loss.
2.	Separability. The neural network can separate the training data during training (i.e., the
neural network can achieve 100% training accuracy)1.
While the first assumption is natural, the second requires some explanation. In fact, we assume
that at some time t0, the training loss is smaller than a threshold, and the threshold here is chosen
to be so small that the training accuracy is guaranteed to be 100% (e.g., for the logistic loss and
cross-entropy loss, the threshold can be set to ln 2). Empirically, state-of-the-art CNNs for image
classification can even fit randomly labeled data easily (Zhang et al., 2017). Recent theoretical work
on over-parameterized neural networks (Allen-Zhu et al., 2019; Zou et al., 2018) show that gradient
descent can fit the training data if the width is large enough. Furthermore, in order to study the
margin, ensuring the training data can be separated is inevitable; otherwise, there is no positive
margin between the data and decision boundary.
Our Contribution. Similar to linear models, for homogeneous models, only the direction of
parameter θ is important for making predictions, and one can see that the margin γ(θ) scales linearly
with kθk2L , when fixing the direction of θ. To compare margins among θ in different directions, it
makes sense to study the normalized margin, γ(θ) := γ(θ)∕∣∣θ∣∣L∙
In this paper, we focus on the training dynamics of the network after t0 (recall that t0 is a time
that the training loss is less than the threshold). Our theoretical results can answer the following
questions regarding the normalized margin.
First, how does the normalized margin change during training? The answer may seem complicated
since one can easily Come UP with examples in which Y increases or decreases in a short time
interval. However, we can show that the overall trend of the normalized margin is to increase in the
following sense: there exists a smoothed version of the normalized margin, denoted as Y such that
(1) |Y - γ∣ → 0 as t → ∞; and (2) Y is non-decreasing for t > t0.
Second, how large is the normalized margin at convergence? To answer this question, we formulate
a natural constrained optimization problem which aims to directly maximize the margin. We show
that every limit point of {θ(t)∕ kθ(t)k2 : t > 0} is along the direction of a KKT point of the max-
margin problem. This indicates that gradient descent/gradient flow performs margin maximization
implicitly in deep homogeneous networks. This result can be seen as a significant generalization of
previous works (Soudry et al., 2018a;b; Ji & Telgarsky, 2019a; Gunasekar et al., 2018b) from linear
classifiers to homogeneous classifiers.
1 Note that this does NOT mean the training loss is 0.
2
Published as a conference paper at ICLR 2020
-Ir=O.01, w/ bias - Ir=O.01, w/o bias
U-UeUJ P"Z-"E,JOU
1.50 ×10-3
1.0。X10-3.
0.50 ×10-3
0.00 ×10-3
-0.50 ×10-3
---- loss-based Ir, w/ bias — loss-based Ir, w/o bias
1.50 X I。-? ∙
1.00×10-3∙
0.50 X 10-3 ∙
0.00×10-3∙
-0.50× 10~3∙
2500 5000 7500 10000	0	2500 5000 7500 10000
#epochs	#epochs
C-EnE P"Z=eE」。U
IO0
gio-200
o>10-,0°
glθ-≡o°
IOT8.
ibɪ ιo2 ιo3 id4	ιo° ioɪ ιo2 ιo3 ιo4	∣
#epochs	#epochs
(a)	(b)
Figure 1: (a) Training CNNs with and without bias on MNIST, using SGD with learning rate 0.01.
The training loss (left) decreases over time, and the normalized margin (right) keeps increasing after
the model is fitted, but the growth rate is slow (≈ 1.8 × 10-4 after 10000 epochs). (b) Training
CNNs with and without bias on MNIST, using SGD with the loss-based learning rate scheduler. The
training loss (left) decreases exponentially over time (< 10-800 after 9000 epochs), and the normal-
ized margin (right) increases rapidly after the model is fitted (≈ 1.2 × 10-3 after 10000 epochs, 10×
larger than that of SGD with learning rate 0.01). Experimental details are in Appendix K.
As by-products of the above results, we derive tight asymptotic convergence/growth rates of the loss
and weights. It is shown in (Soudry et al., 2018a;b; Ji & Telgarsky, 2018; 2019c) that the loss de-
creases at the rate of O(1/t), the weight norm grows as O(log t) for linear logistic regression. In this
work, we generalize the result by showing that the loss decreases at the rate of O(1/(t(log t)2-2/L))
and the weight norm grows as O((logt)1/L) for homogeneous neural networks with exponential
loss, logistic loss, or cross-entropy loss.
Experiments.2 The main practical implication of our theoretical result is that training longer can
enlarge the normalized margin. To justify this claim empiricaly, we train CNNs on MNIST and
CIFAR-10 with SGD (see Section K.1). Results on MNIST are presented in Figure 1. For constant
step size, we can see that the normalized margin keeps increasing, but the growth rate is rather slow
(because the gradient gets smaller and smaller). Inspired by our convergence results for gradient
descent, we use a learning rate scheduling method which enlarges the learning rate according to the
current training loss, then the training loss decreases exponentially faster and the normalized margin
increases significantly faster as well.
For feedforward neural networks with ReLU activation, the normalized margin on a training sample
is closely related to the L2-robustness (the L2-distance from the training sample to the decision
boundary). Indeed, the former divided by a Lipschitz constant is a lower bound for the latter. For
example, the normalized margin is a lower bound for the L2-robustness on fully-connected networks
with ReLU activation (see, e.g., Theorem 4 in (Sokolic et al., 2017)). This fact suggests that training
longer may have potential benefits on improving the robustness of the model. In our experiments, we
observe noticeable improvements of L2-robustness on both training and test sets (see Section K.2).
2	Related Work
Implicit Bias in Training Linear Classifiers. For linear logistic regression on linearly separable
data, Soudry et al. (2018a;b) showed that full-batch gradient descent converges in the direction of the
max L2-margin solution of the corresponding hard-margin Support Vector Machine (SVM). Subse-
quent works extended this result in several ways: Nacson et al. (2019c) extended the results to the
case of stochastic gradient descent; Gunasekar et al. (2018a) considered other optimization methods;
Nacson et al. (2019b) considered other loss functions including those with poly-exponential tails;
Ji & Telgarsky (2018; 2019c) characterized the convergence of weight direction without assuming
separability; Ji & Telgarsky (2019b) proved a tighter convergence rate for the weight direction.
Those results on linear logistic regression have been generalized to deep linear networks. Ji & Tel-
garsky (2019a) showed that the product of weights in a deep linear network with strictly decreasing
loss converges in the direction of the max L2 -margin solution. Gunasekar et al. (2018b) showed
2Code available: https://github.com/vfleaking/max-margin
3
Published as a conference paper at ICLR 2020
more general results for gradient descent on linear fully-connected and convolutional networks with
exponential loss, under various assumptions on the convergence of the loss and gradient direction.
Margin maximization phenomenon is also studied for boosting methods (Schapire et al., 1998;
Schapire & Freund, 2012; Shalev-Shwartz & Singer, 2010; Telgarsky, 2013) and Normalized Per-
ceptron (Ramdas & Pena, 2016).
Implicit Bias in Training Nonlinear Classifiers. Soudry et al. (2018a) analyzed the case where
there is only one trainable layer of a ReLU network. Xu et al. (2018) characterized the implicit bias
for the model consisting of one single ReLU unit. Our work is closely related to a recent independent
work by (Nacson et al., 2019a) which we discuss in details below.
Comparison with (Nacson et al., 2019a). Very recently, (Nacson et al., 2019a) analyzed gradient
descent for smooth homogeneous models and proved the convergence of parameter direction to a
KKT point of the aforementioned max-margin problem. Compared with their work, our work adopt
much weaker assumptions: (1) They assume the training loss converges to 0, but in our work we
only require that the training loss is lower than a small threshold value at some time t0 (and we
prove the exact convergence rate of the loss after t0); (2) They assume the convergence of parameter
direction3, while We prove that KKT conditions hold for all limit points of {θ(t)∕∣∣θ(t)∣∣2 : t> 0},
without requiring any convergence assumption; (3) They assume the convergence of the direction
of losses (the direction of the vector whose entries are loss values on every data point) and Linear
Independence Constraint Qualification (LICQ) for the max-margin problem, while we do not need
such assumptions. Besides the above differences in assumptions, we also prove the monotonicity of
the normalized margin and provide tight convergence rate for training loss. We believe both results
are interesting in their own right.
Another technical difference is that their work analyzes discrete gradient descent on smooth homo-
geneous models (which fails to capture ReLU networks). In our work, we analyze both gradient
descent on smooth homogeneous models and also gradient flow on homogeneous models which
could be non-smooth.
Other Works on Implicit Bias. Banburski et al. (2019) also studied the dynamics of gradient flow
and among other things, provided mathematical insights to the implicit bias towards max margin
solution for homogeneous networks. We note that their analysis of gradient flow decomposes the
dynamics to the tangent component and radial component, which is similar to our proof of Theo-
rem 4.1 in spirit. Wilson et al. (2017); Ali et al. (2019); Gunasekar et al. (2018a) showed that for the
linear least-square problem gradient-based methods converge to the unique global minimum that is
closest to the initialization in L2 distance. Du et al. (2019); Jacot et al. (2018); Lee et al. (2019);
Arora et al. (2019b) showed that over-parameterized neural networks of sufficient width (or infinite
width) behave as linear models with Neural Tangent Kernel (NTK) with proper initialization and
gradient descent converges linearly to a global minimum near the initial point. Other related works
include (Ma et al., 2019; Gidel et al., 2019; Arora et al., 2019a; Suggala et al., 2018; Blanc et al.,
2019; Neyshabur et al., 2015b;a).
3	Preliminaries
Basic Notations. For any N ∈ N, let [N] = {1, . . . , N}. kvk2 denotes the L2-norm of a vector
v. The default base of log is e. For a function f : Rd → R, Vf (x) stands for the gradient at X if it
exists. A function f : X → Rd is Ck-smooth if f is k times continuously differentiable. A function
f : X → R is locally Lipschitz if for every x ∈ X there exists a neighborhood U of x such that the
restriction of f on U is Lipschitz continuous.
Non-smooth Analysis. For a locally Lipschitz function f : X → R, the Clarke’s subdifferential
(Clarke, 1975; Clarke et al., 2008; Davis et al., 2020) at x ∈ X is the convex set
∂o f (x) := Conv ∖ lim Vf (Xk) : Xk → x,f is differentiable at Xk
k→∞
3 Assuming the convergence of the parameter direction may seem quite reasonable, however, the problem
here can be quite subtle in theory. In Appendix J, we present a smooth homogeneuous function f, based on
the Mexican hat function (Absil et al. (2005)), such that even the direction of the parameter does not converge
along gradient flow (it moves around a cirle when t increases).
4
Published as a conference paper at ICLR 2020
For brevity, we say that a function z : I → Rd on the interval I is an arc if z is absolutely continuous
for any compact sub-interval of I. For an arc z, z0(t) (or 等(t)) stands for the derivative at t if it
exists. Following the terminology in (Davis et al., 2020), we say that a locally Lipschitz function
f : Rd → R admits a chain rule if for any arc Z : [0, +∞) → Rd, ∀h ∈ ∂of (z(t)) : (f ◦ z)0(t)=
hh, z0 (t)i holds for a.e. t > 0 (see also Appendix I).
Binary Classification. Let Φ be a neural network, assumed to be parameterized by θ. The output of
Φ on an input x ∈ Rdx is a real number Φ(θ; x), and the sign of Φ(θ; x) stands for the classification
result. A dataset is denoted by D = {(xn, yn) : n ∈ [N]}, where xn ∈ Rdx stands for a data input
and yn ∈ {±1} stands for the corresponding label. For a loss function ` : R → R, we define the
training loss of Φ on the dataset D to be L(θ) := PnN=I '(ynΦ(θ; Xn)).
Gradient Descent. We consider the process of training this neural network Φ with either gradient
descent or gradient flow. For gradient descent, we assume the training loss L(θ) is C2-smooth and
describe the gradient descnet process as θ(t + 1) = θ(t) - η(t)VL(θ(t)), where η(t) is the learning
rate at time t and VL(θ(t)) is the gradient of L at θ(t).
Gradient Flow. For gradient flow, we do not assume the differentibility but only some regu-
larity assumptions including locally Lipschitz. Gradient flow can be seen as gradient descent with
infinitesimal step size. In this model, θ changes continuously with time, and the trajectory of param-
eter θ during training is an arc θ : [0, +∞) → Rd , t 7→ θ(t) that satisfies the differential inclusion
dθd(t) ∈ -∂oL(θ(t)) for a.e. t ≥ 0. The Clarke,s subdifferential ∂oL is a natural generalization of
the usual differential to non-differentiable functions. If L(θ) is actually a C1-smooth function, the
above differential inclusion reduces to dθd(t) = -VL(θ(t)) for all t ≥ 0, which corresponds to the
gradient flow with differential in the usual sense.
4	Gradient Descent / Gradient Flow on Homogeneous Model
In this section, we first state our results for gradient flow and gradient descent on homogeneous
models with exponential loss '(q) := e-q for simplicity of presentation. Due to space limit, We
defer the more general results which hold for a large family of loss functions (including logistic loss
and cross-entropy loss) to Appendix A, F and G.
4.1	Assumptions
Gradient Flow. For gradient flow, we assume the following:
(A1). (Regularity). For any fixed x, Φ( ∙; x) is locally Lipschitz and admits a chain rule;
(A2). (Homogeneity). There exists L > 0 such that ∀α > 0 : Φ(αθ; x) = αLΦ(θ; x);
(A3). (Exponential Loss). '(q) = e-q;
(A4). (Separability). There exists a time t0 such that L(θ(t0)) < 1.
(A1) is a technical assumption about the regularity of the network output. As shown in (Davis et al.,
2020), the output of almost every neural network admits a chain rule (as long as the neural network
is composed by definable pieces in an o-minimal structure, e.g., ReLU, sigmoid, LeakyReLU).
(A2) assumes the homogeneity, the main property we assume in this work. (A3), (A4) correspond to
the two conditions introduced in Section 1. The exponential loss in (A3) is main focus of this section.
(A4) is a separability assumption: the condition L(θ(t0)) < 1 ensures that '(ynΦ(θ(to); Xn)) < 1
for all n ∈ [N], and thus yn Φ(θ(t0); xn ) > 0, meaning that Φ classifies every xn correctly.
Gradient Descent. For gradient descent, we assume (A2), (A3), (A4) similarly as for gradient
flow, and the following two assumptions (S1) and (S5).
(S1)	. (Smoothness). For any fixed x, Φ( ∙; x) is C2-smooth on Rd \ {0}.
(S5)	. (Learning rate condition, Informal). η(t) = η0 for a sufficiently small constant η0. In fact,
η(t) is even allowed to be as large as O(L(t)-1 polylog ^(^)). See Appendix E.1 for the
details.
5
Published as a conference paper at ICLR 2020
(S5)	is natural since deep neural networks are usually trained with constant learning rates. (S1)
ensures the smoothness ofΦ, which is often assumed in the optimization literature in order to analyze
gradient descent. While (S1) does not hold for neural networks with ReLU, it does hold for neural
networks with smooth homogeneous activation such as the quadratic activation φ(x) := x2 (Li et al.,
2018b; Du & Lee, 2018) or powers of ReLU φ(x) := ReLU(x)α for α > 2 (Zhong et al., 2017;
Klusowski & Barron, 2018; Li et al., 2019).
4.2	Main Theorem: Monotonicity of Normalized Margins
The margin for a single data point (xn, yn) is defined to be qn (θ) := ynΦ(θ; xn), and the margin for
the entire dataset is defined to be qmin(θ) := minn∈[N] qn(θ). By homogenity, the margin qmin (θ)
scales linearly with kθk2L for any fixed direction since qmin (cθ) = cLqmin(θ). So we consider the
normalized margin defined as below:
γ(θ) = qmin( kθ∣2) = qmθkr.	⑵
We say f is an E-additive approximation for the normalized margin if Y - E ≤ f ≤ γ, and C-
multiplicative approximation if cγ ≤ f ≤ γ.
Gradient Flow. Our first result is on the overall trend of the normalized margin γY(θ(t)). For both
gradient flow and gradient descent, we identify a smoothed version of the normalized margin, and
show that it is non-decreasing during training. More specifically, we have the following theorem for
gradient flow.
Theorem 4.1 (Corollary of Theorem A.7). Under assumptions (A1) - (A4), there exists an
O(kθk-L)-additive approximation function γ(θ) for the normalized margin such that thefollowing
statements are true for gradient flow:
1.	For a.e. t > to,今γ(θ(t)) ≥ 0;
2.	For a.e. t > to, either 6γ(θ(t)) > 0 or S 口热点=0;
3.	L(θ(t)) → 0 and ∣∣θ(t)∣∣2 → ∞ as t → +∞; therefore, ∣γ(θ(t)) — 7(θ(t))∣ → 0.
More concretely, the function γ(θ) in Theorem 4.1 is defined as
log L⅛ = -l°g (PN=I e-qn⑻)
kθkL =	kθkL
(3)
Note that the only difference between γ(θ) and 7(θ) is that the margin qmin(θ) in γ(θ) is re-
placed by log L(θ) = -LSE(-qι(θ),..., —qN(θ)), where LSE(aι,..., 0n) = log(exp(a1) +
•… + exp(aN)) is the LogSumExp function. This is indeed a very natural idea, and previous
works on linear models (e.g., (Telgarsky, 2013; Nacson et al., 2019b)) also approximate qmin with
LogSumExp in the analysis of margin. It is easy to see why 7(θ) is an O(kθ∣∣-L)-additive ap-
proximation for γY(θ): eamax ≤ PnN=1 ean ≤ N eamax holds for amax = max{a1, . . . , aN}, so
amax ≤ LSE(aι,..., aN) ≤ amax + log N; combining this with the definition of 7(θ) gives
γ(θ)-kθk-L log N ≤ γ(θ) ≤ γ(θ).
Gradient Descent. For gradient descent, Theorem 4.1 holds similarly with a slightly different
function ^(θ) that approximates γ(θ) multiplicatively rather than additively.
Theorem 4.2 (Corollary of Theorem E.2). Under assumptions (S1), (A2) - (A4), (S5), there exists
an (1 — O(1∕(log L1)))-multiplicative approximation function γ(θ) for the normalized margin such
that the following statements are true for gradient descent:
1. For all t > to, ^(θ(t + 1)) ≥ γ(θ(t));
2.
For all t > to, either ^(θ(t + 1)) > ^(θ(t)) or
θ(t+1)
kθ(t+i)k2
θ(t).
W≡'
3.	L(θ(t)) → 0 and ∣∣θ(t)∣∣2 → ∞ as t → +∞; therefore, ∣γ(θ(t)) — ^(θ(t))∣ → 0.
Due to the discreteness of gradient descent, the explicit formula for γ(θ) is somewhat technical, and
we refer the readers to Appendix E for full details.
6
Published as a conference paper at ICLR 2020
Convergence Rates. It is shown in Theorem 4.1, 4.2 that L(θ(t)) → 0 and kθ(t)k2 → ∞. In fact,
with a more refined analysis, we can prove tight loss convergence and weight growth rates using the
monotonicity of normalized margins.
Theorem 4.3 (Corollary of Theorem A.10 and E.5). For gradient flow under assumptions (A1) -
(A4) or gradient descent under assumptions (S1), (A2) - (A4), (S5), we have the following tight
bounds for training loss and weight norm:
L(θ(t)) = θ (T(logj2-2∕L) and	kθ(t)k2 = θ ((logT)1/L),
where T = t for gradient flow and T = Ptτ-=1t η(τ) for gradient descent.
4.3	Main Theorem: Convergence to KKT Points
For gradient flow, Y is upper-bounded by Y ≤ Y ≤ sup{qn(θ) : ∣∣θ∣∣2 = 1}. Combining this with
Theorem 4.1 and the monotone convergence theorem, it is not hard to see that limt→+∞ γ(θ(t))
and limt→+∞ γ(θ(t)) exist and equal to the same value. Using a similar argument, We can draw the
same conclusion for gradient descent.
To understand the implicit regularization effect, a natural question arises: what optimality property
does the limit of normalized margin have? To this end, we identify a natural constrained optimization
problem related to margin maximization, and prove that θ(t) directionally converges to its KKT
points, as shown below. We note that we can extend this result to the finite time case, and show
that gradient flow or gradient descent passes through an approximate KKT point after a certain
amount of time. See Theorem A.9 in Appendix A and Theorem E.4 in Appendix E for the details.
We will briefly review the definition of KKT points and approximate KKT points for a constraint
optimization problem in Appendix C.1.
Theorem 4.4 (Corollary of Theorem A.8 and E.3). For gradient flow under assumptions (A1) - (A4)
or gradient descent under assumptions (S1), (A2) - (A4), (S5), any limit point θ of { 口册卜:t ≥ 0}
is along the direction of a KKT point of the following constrained optimization problem (P):
min 2∣∣θ∣∣2	St qn(θ) ≥ 1	Nn ∈ [N]
That is, for any limit point θY, there exists a scaling factor α > 0 such that αθY satisfies Karush-Kuhn-
Tucker (KKT) conditions of (P).
Minimizing (P) over its feasible region is equivalent to maximizing the normalized margin over all
possible directions. The proof is as follows. Note that we only need to consider all feasible points
θ with qmin (θ) > 0. For a fixed θ, αθ is a feasible point of (P) iff α ≥ qmin (θ)-1∕L. Thus, the
minimum objective value over all feasible points of (P) in the direction of θ is 2 ∣∣θ∕qmin(θ)1/L k 2 =
2Y(θ)-2∕L. Taking minimum over all possible directions, we can conclude that if the maximum
normalized margin is γ*, then the minimum objective of (P) is 2Y-2/L.
It can be proved that (P) satisfies the Mangasarian-Fromovitz Constraint Qualification (MFCQ) (See
Lemma C.7). Thus, KKT conditions are first-order necessary conditions for global optimality. For
linear models, KKT conditions are also sufficient for ensuring global optimality; however, for deep
homogeneous networks, qn(θ) can be highly non-convex. Indeed, as gradient descent is a first-order
optimization method, if we do not make further assumptions on qn (θ), then it is easy to construct
examples that gradient descent does not lead to a normalized margin that is globally optimal. Thus,
proving the convergence to KKT points is perhaps the best we can hope for in our setting, and it is
an interesting future work to prove stronger convergence results with further natural assumptions.
Moreover, we can prove the following corollary, which characterizes the optimality of the normal-
ized margin using SVM with Neural Tangent Kernel (NTK, introduced in (Jacot et al., 2018)) defined
at limit points. The proof is deferred to Appendix C.6.
Corollary 4.5 (Corollary of Theorem 4.4). Assume (S1). Then for gradient flow under assump-
tions (A2) - (A4) or gradient descent under assumptions (A2) - (A4), (S5), any limit point θY of
{θ(t)∕kθ(t)k2 : t ≥ 0} is along the max-margin direction for the hard-margin SVM with kernel
7
Published as a conference paper at ICLR 2020
Kg(x, x0) = (VΦχ(θ), VΦχo (θ)), where Φχ(θ) := Φ(θ; x). That is, for some α > 0, αθ is the
optimal solution for the following constrained optimization problem:
min 1 kθk2	s.t. yn <θ, VΦχn ⑹)≥ 1	∀n ∈ [N]
Ifwe assume (A1) instead of (S1) for gradient flow, then there exists a mapping h(x) ∈ ∂ 0Φχ(θ)
such that the same conclusion holdsfor Kg(x, x0) = hh(x), h(x0)i.
4.4	Other Main Results
The above results can be extended to other settings as shown below.
Other Binary Classification Loss. The results on exponential loss can be generalized to a much
broader class of binary classification loss. The class includes the logistic loss which is one of the
most popular loss functions, '(q) = log(1 + e-q). The function class also includes other losses with
exponential tail, e.g., '(q) = e-q , '(q) = log(1 + e-q ). For all those loss functions, We can use
its inverse function `-1 to define the smoothed normalized margin as follows
γ(θ)
'-1(L(θ))
kθkL
Then all our results for gradient flow continue to hold (Appendix A). Using a similar modification,
we can also extend it to gradient descent (Appendix F).
Cross-entropy Loss. In multi-class classification, we can define qn to be the difference between
the classification score for the true label and the maximum score for the other labels, then the margin
qmin := mi□n∈ [N] qn and the normalized margin 7(θ) := qmθn(θ) can be similarly defined as before.
In Appendix G, we define the smoothed normalized margin for cross-entropy loss to be the same as
that for logistic loss (See Remark A.4). Then we show that Theorem 4.1 and Theorem 4.4 still hold
(but with a slightly different definition of (P)) for gradient flow, and we also extend the results to
gradient descent.
Multi-homogeneous Models. Some neural networks indeed possess a stronger property than
homogeneity, which we call multi-homogeneity. For example, the output of a CNN (without bias
terms) is 1-homogeneous with respect to the weights of each layer. In general, we say that a neu-
ral network Φ(θ; x) with θ = (w1, . . . , wm) is (k1, . . . , km)-homogeneous if for any x and any
cι,...,cm > 0, we have Φ(c1w1,..., CmWm； x) = Qm=I Cki ∙ Φ(wι,..., Wm； x). In the previous
example, an L-layer CNN with layer weights θ = (w1, . . . , wL) is (1, . . . , 1)-homogeneous.
One can easily see that that (k1, . . . , km)-homogeneity implies L-homogeneity, where L =
Pim=1 ki, so our previous analysis for homogeneous models still applies to multi-homogeneous mod-
els. But it would be better to define the normalized margin for multi-homogeneous model as
勺(WI W ):= q , ( w1	Wm ) = qmin 	⑷
γ(w1,…，wm): qmin( kwik2 ,…, kWmk2 ) Q乙 kWikki .	(4)
In this case, the smoothed approximation of Y for general binary classification loss (under some
conditions) can be similarly defined for gradient flow:
1	`	'-1(L)
mw"∙, Wm):= QmiW>,	⑸
It can be shown that Y is also non-decreasing during training when the loss is small enough (Ap-
pendix H). In the case of cross-entropy loss, we can still define 7 by (5) while '(∙) is set to the
logistic loss in the formula.
5	Proof S ketch: Gradient Flow on Homogeneous Model with
Exponential Loss
In this section, we present a proof sketch in the case of gradient flow on homogeneous model with
exponential loss to illustrate our proof ideas. Due to space limit, the proof for the main theorems on
gradient flow and gradient descent in Section 4 are deferred to Appendix A and E respectively.
8
Published as a conference paper at ICLR 2020
For convenience, we introduce a few more notations for a L-homogeneous neural network Φ(θ; x).
Let Sd-1 = {θ ∈ Rd : kθk2 = 1} be the set of L2-normalized parameters. Define ρ := kθk2
and θ := kθθk^ ∈ Sd-1 to be the length and direction of θ. For both gradient descent and
gradient flow, θ is a function of time t. For convenience, we also view the functions of θ, in-
cluding L(θ), qn(θ), qmin(θ), as functions of t. So we can write L(t) := L(θ(t)), qn(t) :=
qn (θ(t)), qmin (t) := qmin (θ(t)).
Lemma 5.1 below is the key lemma in our proof. It decomposes the growth of the smoothed normal-
ized margin into the ratio of two quantities related to the radial and tangential velocity components
of θ respectively. We will give a proof sketch for this later in this section. We believe that this
lemma is of independent interest.
Lemma 5.1 (Corollary of Lemma B.1). For a.e. t > t0,
d、 .c ,	d、	~、Jd,	∖-1dθ
五 log ρ > 0 and 五 log 7 ≥ L 节 log P	H	∙
dt	dt	dt	dt
Using Lemma 5.1, the first two claims in Theorem 4.1 can be directly proved. For the third claim,
we make use of the monotonicity of the margin to lower bound the gradient, and then show L → 0
and ρ → +∞. Recall that 7 is an O(P-L)-additive approximation for 7. So this proves the third
claim. We defer the detailed proof to Appendix B.
To show Theorem 4.4, we first change the time measure to log P, i.e., now we see t as a function of
log ρ. So the second inequality in Lemma 5.1 can be rewritten as 九：P ≥ Lk d黑。∣∣2. Integrating
on both sides and noting that γ7 is upper-bounded, we know that there must be many instant log P
such that ∣ d dθg ρ ∣∣2 is small. By analyzing the landscape of training loss, We show that these points
are “approximate” KKT points. Then we show that every convergent sub-sequence of {θ(t) : t ≥ 0}
can be modified to be a sequence of “approximate” KKT points which converges to the same limit.
Then we conclude the proof by applying a theorem from (Dutta et al., 2013) to show that the limit of
this convergent sequence of “approximate” KKT points is a KKT point. We defer the detailed proof
to Appendix C.
Now we give a proof sketch for Lemma 5.1, in which we derive the formula of γ7 step by step. In
the proof, we obtain several clean close form formulas for several relevant quantities, by using the
chain rule and Euler’s theorem for homogeneous functions extensively.
Proof Sketch of Lemma 5.1. For ease of presentation, we ignore the regularity issues of taking
derivatives in this proof sketch. We start from the equation 篝 =一(d◦ L(θ(t)),嗥)=-∣∣器卜
which follows from the chain rule (see also Lemma I.3). Then we note that 需 can be decomposed
into two parts: the radial component V := θθ> 第 and the tangent component U := (I — θθ>)需.
The radial component is easier to analyze. Bythechainrule, ∣v∣2 = θ> 第 =P (θ,得)=P ∙ ɪ dp2.
For 2 dpp2, we have an exact formula:
N
LXe-qnqn.	(6)
n=1
The last equality is due to (∂°qn, θ) = Lqn by homogeneity of qn. This is sometimes called Euler's
theorem for homogeneous functions (see Theorem B.2). For differentiable qn, it can be easily proved
by taking the derivative over c on both sides of qn (cθ) = cLqn(θ) and letting c = 1.
With (6), we can lower bound ɪ PP by
1 dρ2
2 ~di
N
LXe-qnqn
n=1
N1
≥ LEe qnqmin ≥ L ∙Llog L,
n=1
(7)
where the last inequality uses the fact that e-qmin ≤ L. (7) also implies that 2 pP2 > 0 for t > t0
since L(to) < 1 and L is non-increasing. As Pd log P =击 ppp2, this also proves the first inequality
of Lemma 5.1.
9
Published as a conference paper at ICLR 2020
Now, We have ∣∣v∣2 =今(ɪ%) =ɪ*∙ d logP on the one hand; on the other hand, by the
chain rule we have 噜=a(p需一笔θ)=表(P需一(θ> 得)θ) = U. SoWehave
2
dL
—
dt
dθ
dt
∣∣vk2+∣∣uk2=2 -dp- ∙ d log P+ρ2
2 dt dt
dθ
dt
2
2
2
Dividing 2 dρ2 on the leftmost and rightmost sides, we have
dL 11 dp2 ʌ 1 d	d d ∖-1dθ
一版12-dΓ)	= dtlogp + (原logp) 而 J
By 一 ddL ≥ 0 and (7), the LHS is no greater than 一 ddL ∙ (L ∙ L log +) 1 = L log log ɪ. Thus we
have dt log log L 一 LK log P ≥ L (第 log p) 1 ∣∣ 制∣ , where the LHS is exactly 捋 log γ.	□
6 Discussion and Future Directions
In this paper, we analyze the dynamics of gradient flow/descent of homogeneous neural networks
under a minimal set of assumptions. The main technical contribution of our work is to prove rigor-
ously that for gradient flow/descent, the normalized margin is increasing and converges to a KKT
point of a natural max-margin problem. Our results leads to some natural further questions:
•	Can we generalize our results for gradient descent on smooth neural networks to non-
smooth ones? In the smooth case, we can lower bound the decrement of training loss by
the gradient norm squared, multiplied by a factor related to learning rate. However, in the
non-smooth case, no such inequality is known in the optimization literature.
•	Can we make more structural assumptions on the neural network to prove stronger results?
In this work, we use a minimal set of assumptions to show that the convergent direction of
parameters is a KKT point. A potential research direction is to identify more key properties
of modern neural networks and show that the normalized margin at convergence is locally
or globally optimal (in terms of optimizing (P)).
•	Can we extend our results to neural networks with bias terms? In our experiments, the
normalized margin of the CNN with bias also increases during training despite that its
output is non-homogeneous. It is very interesting (and technically challenging) to provide
a rigorous proof for this fact.
Acknowledgments
The research is supported in part by the National Natural Science Foundation of China Grant
61822203, 61772297, 61632016, 61761146003and the Zhongguancun Haihua Institute for Fron-
tier Information Technology and Turing AI Institute of Nanjing. We thank Liwei Wang for helpful
suggestions on the connection between margin and robustness. We thank Sanjeev Arora, Tianle Cai,
Simon Du, Jason D. Lee, Zhiyuan Li, Tengyu Ma, Ruosong Wang for helpful discussions.
References
Pierre-Antoine Absil, Robert Mahony, and Benjamin Andrews. Convergence of the iterates of de-
scent methods for analytic cost functions. SIAM Journal on Optimization, 16(2):531-547, 2005.
Alnur Ali, J. Zico Kolter, and Ryan J. Tibshirani. A continuous-time view of early stopping for
least squares regression. In Kamalika Chaudhuri and Masashi Sugiyama (eds.), Proceedings
of Machine Learning Research, volume 89 of Proceedings of Machine Learning Research, pp.
1370-1378. PMLR, 16-18 Apr 2019.
10
Published as a conference paper at ICLR 2020
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the
36th International Conference on Machine Learning, volume 97 of Proceedings of Machine
Learning Research,pp. 242-252, Long Beach, California, USA, 09-15 JUn 2019. PMLR.
Cem Anil, James Lucas, and Roger Grosse. Sorting out Lipschitz function approximation. In Ka-
malika ChaUdhUri and RUslan SalakhUtdinov (eds.), Proceedings of the 36th International Con-
ference on Machine Learning, volUme 97 of Proceedings of Machine Learning Research, pp.
291-301, Long Beach, California, USA, 09-15 JUn 2019. PMLR.
Sanjeev Arora, Nadav Cohen, Wei HU, and YUping LUo. Implicit regUlarization in deep matrix fac-
torization. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett
(eds.), Advances in Neural Information Processing Systems 32, pp. 7411-7422. CUrran Asso-
ciates, Inc., 2019a.
Sanjeev Arora, Simon S DU, Wei HU, ZhiyUan Li, RUss R SalakhUtdinov, and RUosong Wang. On
exact compUtation with an infinitely wide neUral net. In H. Wallach, H. Larochelle, A. Beygelz-
imer, F. d'Alche-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing
Systems 32, pp. 8139-8148. CUrran Associates, Inc., 2019b.
Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of secu-
rity: Circumventing defenses to adversarial examples. In Jennifer Dy and Andreas Krause (eds.),
Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceed-
ings of Machine Learning Research, pp. 274-283, Stockholmsmssan, Stockholm Sweden, 10-15
Jul 2018. PMLR.
Andrzej Banburski, Qianli Liao, Brando Miranda, Tomaso Poggio, Lorenzo Rosasco, and Jack
Hidary. Theory III: Dynamics and generalization in deep networks. CBMM Memo No: 090,
version 20, 2019.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan,
and R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp. 6240-6249.
Curran Associates, Inc., 2017.
Battista Biggio, Igino Corona, Davide Maiorca, BIaine Nelson, Nedim Srndic, Pavel Laskov, Gior-
gio Giacinto, and Fabio Roli. Evasion attacks against machine learning at test time. In Hendrik
Blockeel, Kristian Kersting, Siegfried Nijssen, and Filip ZeIezny (eds.), Machine Learning and
Knowledge Discovery in Databases, pp. 387-402, Berlin, Heidelberg, 2013. Springer Berlin Hei-
delberg.
Guy Blanc, Neha Gupta, Gregory Valiant, and Paul Valiant. Implicit regularization for deep neural
networks driven by an ornstein-uhlenbeck like process. arXiv preprint arXiv:1904.09080, 2019.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017
IEEE Symposium on Security and Privacy (SP), pp. 39-57, May 2017. doi: 10.1109/SP.2017.49.
Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier. Parse-
val networks: Improving robustness to adversarial examples. In Doina Precup and Yee Whye
Teh (eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70
of Proceedings of Machine Learning Research, pp. 854-863, International Convention Centre,
Sydney, Australia, 06-11 Aug 2017. PMLR.
Francis H. Clarke, Yuri S. Ledyaev, Ronald J. Stern, and Peter R. Wolenski. Nonsmooth analysis
and control theory, volume 178. Springer Science & Business Media, 2008.
Frank H. Clarke. Generalized gradients and applications. Transactions of the American Mathemat-
ical Society, 205:247-262, 1975.
Frank H Clarke. Optimization and Nonsmooth Analysis. Society for Industrial and Applied Mathe-
matics, 1990. doi: 10.1137/1.9781611971309.
Michel Coste. An Introduction to O-minimal Geometry. 2002.
11
Published as a conference paper at ICLR 2020
Haskell B Curry. The method of steepest descent for non-linear minimization problems. Quarterly
ofApplied Mathematics, 2(3):258-261, 1944.
Damek Davis, Dmitriy Drusvyatskiy, Sham Kakade, and Jason D. Lee. Stochastic subgradient
method converges on tame functions. Foundations of Computational Mathematics, 20(1):119-
154, Feb 2020.
Dmitriy Drusvyatskiy, Alexander D Ioffe, and Adrian S Lewis. Curves of descent. SIAM Journal
on Control and Optimization, 53(1):114-138, 2015.
Simon Du and Jason Lee. On the power of over-parametrization in neural networks with quadratic
activation. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International
Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp.
1329-1338, Stockholmsmassan, Stockholm Sweden, 10-15 Jul 2018. PMLR.
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Pro-
ceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings
of Machine Learning Research, pp. 1675-1685, Long Beach, California, USA, 09-15 Jun 2019.
PMLR.
Simon S. Du, Wei Hu, and Jason D. Lee. Algorithmic regularization in learning deep homogeneous
models: Layers are automatically balanced. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31,
pp. 382-393. Curran Associates, Inc., 2018.
Joydeep Dutta, Kalyanmoy Deb, Rupesh Tulshyan, and Ramnik Arora. Approximate KKT points
and a proximity measure for termination. Journal of Global Optimization, 56(4):1463-1499,
2013.
Gauthier Gidel, Francis Bach, and Simon Lacoste-Julien. Implicit regularization of discrete gradient
dynamics in linear neural networks. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-
Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp.
3196-3206. Curran Associates, Inc., 2019.
Giorgio Giorgi, Angelo Guerraggio, and Jorg Thierfelder. Chapter IV - Nonsmooth Optimization
Problems. In Mathematics of Optimization, pp. 359 - 457. Elsevier Science, Amsterdam, 2004.
ISBN 978-0-444-50550-7.
Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of
neural networks. In SebaStien Bubeck, Vianney Perchet, and Philippe Rigollet (eds.), Proceedings
of the 31st Conference On Learning Theory, volume 75 of Proceedings of Machine Learning
Research, pp. 297-299. PMLR, 06-09 Jul 2018.
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias
in terms of optimization geometry. In Jennifer Dy and Andreas Krause (eds.), Proceedings of
the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine
Learning Research, pp. 1832-1841, Stockholmsmssan, Stockholm Sweden, 10-15 Jul 2018a.
PMLR.
Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. Implicit bias of gradient descent
on linear convolutional networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-
Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31, pp. 9482-
9491. Curran Associates, Inc., 2018b.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on ImageNet classification. In The IEEE International Conference on
Computer Vision (ICCV), December 2015.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-
Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31, pp. 8571-
8580. Curran Associates, Inc., 2018.
12
Published as a conference paper at ICLR 2020
Ziwei Ji and Matus Telgarsky. Risk and parameter convergence of logistic regression. arXiv preprint
arXiv:1803.07300, 2018.
Ziwei Ji and Matus Telgarsky. Gradient descent aligns the layers of deep linear networks. In
International Conference on Learning Representations, 2019a.
Ziwei Ji and Matus Telgarsky. A refined primal-dual analysis of the implicit bias. arXiv preprint
arXiv:1906.04540, 2019b.
Ziwei Ji and Matus Telgarsky. The implicit bias of gradient descent on nonseparable data. In
Alina Beygelzimer and Daniel Hsu (eds.), Proceedings of the Thirty-Second Conference on Learn-
ing Theory, volume 99 of Proceedings of Machine Learning Research, pp. 1772-1798, Phoenix,
USA, 25-28 JUn 2019c. PMLR.
Jason M Klusowski and Andrew R Barron. Approximation by combinations of relu and squared
relu ridge functions with `1 and `0 controls. IEEE Transactions on Information Theory, 64(12):
7649-7656, 2018.
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox,
and R. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp. 8570-8581.
Curran Associates, Inc., 2019.
Bo Li, Shanshan Tang, and Haijun Yu. Better approximations of high dimensional smooth functions
by deep neural networks with rectified power units. arXiv preprint arXiv:1903.05858, 2019.
Xingguo Li, Junwei Lu, Zhaoran Wang, Jarvis Haupt, and Tuo Zhao. On tighter generalization
bound for deep neural networks: CNNs, ResNets, and beyond. arXiv preprint arXiv:1806.05159,
2018a.
Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized
matrix sensing and neural networks with quadratic activations. In Sebastien Bubeck, Vianney
Perchet, and Philippe Rigollet (eds.), Proceedings of the 31st Conference On Learning Theory,
volume 75 of Proceedings of Machine Learning Research, pp. 2-47. PMLR, 06-09 Jul 2018b.
Cong Ma, Kaizheng Wang, Yuejie Chi, and Yuxin Chen. Implicit regularization in nonconvex statis-
tical estimation: Gradient descent converges linearly for phase retrieval, matrix completion, and
blind deconvolution. Foundations of Computational Mathematics, Aug 2019.
Mor Shpigel Nacson, Suriya Gunasekar, Jason Lee, Nathan Srebro, and Daniel Soudry. Lexico-
graphic and depth-sensitive margins in homogeneous and non-homogeneous deep models. In
Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International
Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research,
pp. 4683-4692, Long Beach, California, USA, 09-15 Jun 2019a. PMLR.
Mor Shpigel Nacson, Jason Lee, Suriya Gunasekar, Pedro Henrique Pamplona Savarese, Nathan
Srebro, and Daniel Soudry. Convergence of gradient descent on separable data. In Kamalika
Chaudhuri and Masashi Sugiyama (eds.), Proceedings of Machine Learning Research, volume 89
of Proceedings of Machine Learning Research, pp. 3420-3428. PMLR, 16-18 Apr 2019b.
Mor Shpigel Nacson, Nathan Srebro, and Daniel Soudry. Stochastic gradient descent on separa-
ble data: Exact convergence with a fixed learning rate. In Kamalika Chaudhuri and Masashi
Sugiyama (eds.), Proceedings of Machine Learning Research, volume 89 of Proceedings ofMa-
chine Learning Research, pp. 3051-3059. PMLR, 16-18 Apr 2019c.
Behnam Neyshabur, Ruslan R Salakhutdinov, and Nati Srebro. Path-SGD: Path-normalized opti-
mization in deep neural networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and
R. Garnett (eds.), Advances in Neural Information Processing Systems 28, pp. 2422-2430. Curran
Associates, Inc., 2015a.
13
Published as a conference paper at ICLR 2020
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On
the role of implicit regularization in deep learning. In 3rd International Conference on Learning
Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Workshop Track Proceedings,
2015b.
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A PAC-bayesian approach to
spectrally-normalized margin bounds for neural networks. In International Conference on Learn-
ing Representations, 2018.
J Jr Palis and Welington De Melo. Geometric theory of dynamical systems: an introduction. Springer
Science & Business Media, 2012.
Aaditya Ramdas and Javier Pena. Towards a deeper geometric, analytic and algorithmic understand-
ing of margins. Optimization Methods and Software, 31(2):377-391, 2016.
Saharon Rosset, Ji Zhu, and Trevor J. Hastie. Margin maximizing loss functions. In S. Thrun,
L. K. Saul, and B. Scholkopf (eds.), Advances in Neural Information Processing Systems 16, pp.
1237-1244. MIT Press, 2004.
Robert E. Schapire and Yoav Freund. Boosting: Foundations and Algorithms. The MIT Press, 2012.
ISBN 0262017180, 9780262017183.
Robert E. Schapire, Yoav Freund, Peter Bartlett, and Wee Sun Lee. Boosting the margin: A new
explanation for the effectiveness of voting methods. Ann. Statist., 26(5):1651-1686, 10 1998. doi:
10.1214/aos/1024691352.
Shai Shalev-Shwartz and Yoram Singer. On the equivalence of weak learnability and linear sepa-
rability: New relaxations and efficient boosting algorithms. Machine learning, 80(2-3):141-163,
2010.
Jure Sokolic, Raja Giryes, Guillermo Sapiro, and Miguel R. D. Rodrigues. Robust large margin
deep neural networks. IEEE Trans. Signal Processing, 65(16):4265-4280, 2017. doi: 10.1109/
TSP.2017.2708039.
Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The im-
plicit bias of gradient descent on separable data. Journal of Machine Learning Research, 19(70):
1-57, 2018a.
Daniel Soudry, Elad Hoffer, and Nathan Srebro. The implicit bias of gradient descent on separable
data. In International Conference on Learning Representations, 2018b.
Arun Suggala, Adarsh Prasad, and Pradeep K Ravikumar. Connecting optimization and regulariza-
tion paths. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett
(eds.), Advances in Neural Information Processing Systems 31, pp. 10608-10619. Curran Asso-
ciates, Inc., 2018.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfel-
low, and Rob Fergus. Intriguing properties of neural networks. In International Conference on
Learning Representations, 2013.
Matus Telgarsky. Margins, shrinkage, and boosting. In Sanjoy Dasgupta and David McAllester
(eds.), Proceedings of the 30th International Conference on Machine Learning, volume 28 of
Proceedings of Machine Learning Research, pp. 307-315, Atlanta, Georgia, USA, 17-19 Jun
2013. PMLR.
Lou van den Dries and Chris Miller. Geometric categories and o-minimal structures. Duke Mathe-
matical Journal, 84(2):497-540, 1996.
Colin Wei and Tengyu Ma. Improved sample complexities for deep neural networks and robust
classification via an all-layer margin. In International Conference on Learning Representations,
2020.
14
Published as a conference paper at ICLR 2020
Colin Wei, Jason D Lee, Qiang Liu, and Tengyu Ma. Regularization matters: Generalization and
optimization of neural nets v.s. their induced kernel. In H. Wallach, H. Larochelle, A. Beygelz-
imer, F. d'Alche-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing
Systems 32, pp. 9709-972LCUrran Associates, Inc., 2019.
Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal
valUe of adaptive gradient methods in machine learning. In I. GUyon, U. V. LUxbUrg, S. Bengio,
H. Wallach, R. FergUs, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information
Processing Systems 30, pp. 4148-4158. CUrran Associates, Inc., 2017.
TengyU XU, Yi ZhoU, Kaiyi Ji, and Yingbin Liang. When will gradient methods converge to max-
margin classifier Under relU models? arXiv preprint arXiv:1806.04339, 2018.
ChiyUan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning reqUires rethinking generalization. In International Conference on Learning Rep-
resentations, 2017.
Hongyi Zhang, Yann N. DaUphin, and TengyU Ma. FixUp initialization: ResidUal learning withoUt
normalization. In International Conference on Learning Representations, 2019.
Kai Zhong, Zhao Song, Prateek Jain, Peter L. Bartlett, and Inderjit S. Dhillon. Recovery gUarantees
for one-hidden-layer neUral networks. In Doina PrecUp and Yee Whye Teh (eds.), Proceedings of
the 34th International Conference on Machine Learning, volUme 70 of Proceedings of Machine
Learning Research, pp. 4140-4149, International Convention Centre, Sydney, AUstralia, 06-11
AUg 2017. PMLR.
Difan ZoU, YUan Cao, DongrUo ZhoU, and QUanqUan GU. Stochastic gradient descent optimizes
over-parameterized deep relU networks. arXiv preprint arXiv:1811.08888, 2018.
GUUs ZoUtendijk. Mathematical programming methods. 1976.
15
Published as a conference paper at ICLR 2020
A Results for General Loss
In this section, we state our results for a broad class of binary classification loss. A major con-
sequence of this generalization is that the logistic loss, one of the most popular loss functions,
'(q) = log(1 + e-q) is included. The function class also includes other losses with exponential tail,
e.g., '(q) = e-q3,'(q)=log(1 + e-q3).
A.1 Assumptions
We first focus on gradient flow. We assume (A1), (A2) as we do for exponential loss. For (A3), (A4),
we replace them with two weaker assumptions (B3), (B4). All the assumptions are listed below:
(A1). (Regularity). For any fixed x, Φ( ∙; x) is locally LiPSChitz and admits a chain rule;
(A2). (Homogeneity). There exists L > 0 such that ∀α > 0 : Φ(αθ; x) = αLΦ(θ; x);
(B3)	. The loss function '(q) can be expressed as '(q) = e-f (q) such that
(B3.1).	f : R → R is C1-smooth.
(B3.2).	f0(q) > 0forallq ∈ R.
(B3.3).	There exists bf ≥ 0 such that f0(q)q is non-decreasing for q ∈ (bf, +∞), and
f0(q)q → +∞ as q → +∞.
(B3.4).	Let g : [f (bf), +∞) → [bf, +∞) be the inverse function of f on the domain
[bf, +∞). There exists bg ≥ max{2f (bf), f (2bf)}, K ≥ 1 such that g0(x) ≤
Kg0 (θx) and f0(y) ≤ Kf0(θy) for all x ∈ (bg, +∞), y ∈ (g(bg), +∞) and
θ ∈ [1/2, 1).
(B4)	. (Separability). There exists a time t0 such that L(t0) < e-f(bf) = `(bf).
(A1) and (A2) remain unchanged. (B3) is satisfied by exponential loss '(q) = e-q (with f (q) = q)
and logistic loss '(q) = log(1 + e-q) (with f(q) = - log log(1 + e-q)). (B4) are essentially the
same as (A4) but (B4) uses a threshold value that depends on the loss function. Assuming (B3), it
is easy to see that (B4) ensures the separability of data since '(qn) < e-f(bf) implies qn > bf ≥ 0.
For logistic loss, we can set bf = 0 (see Remark A.2). So the corresponding threshold value in (B4)
is '(0) = log2.
Now we discuss each of the assumptions in (B3). (B3.1) is a natural assumption on smoothness.
(B3.2) requires '(∙) to be monotone decreasing, which is also natural since '(∙) is used for binary
classification. The rest of two assumptions in (B3) characterize the properties of '0(q) when q is
large enough. (B3.3) is an assumption that appears naturally from the proof. For exponential loss,
f0(q)q = q is always non-decreasing, so we can set bf = 0. In (B3.4), the inverse function g is
defined. It is guaranteed by (B3.1) and (B3.2) that g always exists and g is also C1-smooth. Though
(B3.4) looks very complicated, it essentially says that f 0 (Θ(q)) = Θ(f0(q)), g0(Θ(q)) = Θ(g0(q))
as q → ∞. (B3.4) is indeed a technical assumption that enables us to asymptotically compare
the loss or the length of gradient at different data points. It is possible to base our results on weaker
assumptions than (B3.4), but we use (B3.4) for simplicity since it has already been satisfied by many
loss functions such as the aforementioned examples.
We summarize the corresponding f, g and bf for exponential loss and logistic loss below:
Remark A.1. Exponential loss '(q) = e-q satisfies (B3) with
f(q) = q	f0(q)	= 1	g(q)	= q	g0(q)	= 1	bf	= 0	`(bf)	=	1.
Remark A.2. Logistic loss '(q) = log(1 + e-q) satisfies (B3) with
e-q
f (q) = - loglog(1 + e-q ) = θ⑷	f 0(q) = (1 + e-q )log(1 + e-q) = θ⑴
q	e-q
g(q) = - log(ee	-1) = θ(q)	g0(q) = —q~~7 = θ⑴
ee - 1
bf = 0	`(bf ) = log 2.
The proof for Remark A.1 is trivial. For Remark A.2, we give a proof below.
16
Published as a conference paper at ICLR 2020
Proof for Remark A.2. By simple calculations, the formulas for f (q), f0(q), g(q), g0 (q) are correct.
(B3.1) is trivial. f0(q) = .+e-q)；og(i+e-q) > 0, So (B3.2) is satisfied. For (B3.3), note that
f 0(q)q =(i+eq)iog(i+e-q). The denominator is a decreasing function since
((1 + eq)log(1 + e-q)) = eq log(1 + e-q) - 1 <eq ∙ e-q - 1 = 0.
Thus, f 0(q)q is a strictly increasing function on R. As bf is required to be non-negative, we set
bf = 0. For proving that f(q)q → +∞ and (B4), we only need to notice that f(q)〜ɪe-ɪ = 1
and g0(χ) = 1∕f0(g(χ))〜1.	E □
d
dq
A.2 Smoothed Normalized Margin
For a loss function '(∙) satisfying (B3), it is easy to see from (B3.2) that its inverse function '-1(∙)
must exist. For this kind of loss functions, we define the smoothed normalized margin as follows:
Definition A.3. For a loss function '(∙) satisfying (B3), the smoothed normalized margin γ(θ) of θ
is defined as
W= * = g(lθg 妥)=g(ig (PN=I e-f(qn ⑻)))
γ(θ) :	LL	L	，
ρL	ρL	ρL
where '-1(∙) is the inverse function of '(∙) and P := ∣∣θ∣∣2.
Remark A.4. For logistic loss '(q) = log(1 + e-q), γ(θ) = PT log eχp(L)-ι;for exponential loss
'(q) = e-q, γ(θ) = P-L log L, which is the same as (3).
Now We give some insights on how well γ(θ) approximates γ(θ) using a similar argument as in
Section 4.2. Using the LogSumExp function, the smoothed normalized margin γ(θ) can also be
written as
1Q∖	g(-LSE(-f (qI),..., -f (qN)))
γ(θ) =-------------PL--------------.
LSE is a (log N)-additive approximation for max. So We can roughly approximate 7(θ) by
7(θ) ≈
g(-max{-f (qι),..., -f(qN)})
PL
g(f(qmin))	-/zjʌ
PL	fl"
Note that (B3.3) is crucial to make the above approximation reasonable. Similar to exponential loss,
we can show the following lemma asserting that 7 is a good approximation of γ.
Lemma A.5. Assuming (B3)4, we have the following properties about the margin:
(a)	f (qmin) — log N ≤ log L ≤ f (qmin).
(b)	If log L > f (bf) ,then there exists ξ ∈ (f (qmin) 一 log N, f (qmin)) ∩ (bf, +∞) such that
g	g0 (ξ) log N
Y-	PL
≤ 7 ≤ γ.
(c)	ForaSeuqnceofParameters {θm ∈ Rd : m ∈ N}, if L(θm) → 0, then ∣γ(θm) 一 7(θm)∣ → 0.
Proof. (a) can be easily deduced from e-f (qmin) ≤ L ≤ Ne-f(qmin). Combining (a) and the
monotonicity of g(∙), we further have g(s) ≤ g(log L) ≤ qmin for S := max{f (bf), f (qmin) 一
log N}. By the mean value Theorem, there exists ξ ∈ (s, f(qmin)) such that g(s) = g(f(qmin)) -
g0(ξ)(f (qmin) -	s)	≥	qmin -	g0(ξ) log N. Dividing	PL	on each side of	qmin -	g0(ξ)	log N ≤
g(log L) ≤ qmin proves (b).
Now we prove (c). Without loss of generality, we assume log £(； ) > f (bf) for all θm,. It follows
from (b) that for every θm there exists ξm ∈ (f(qmin(θm))-logN, f(qmin(θm)))∩(bf, +∞) such
that
γ(θm ) - (g0(ξm)log N )∕P(θm)L ≤ 7(θm) ≤ 租 θm)∙	(8)
4Indeed, (B3.4) is not needed for showing Lemma A.5 and Theorem A.7.
17
Published as a conference paper at ICLR 2020
Note that	ξm	≥	f (qmin(θm)) - log N ≥	log L(θ1m)	- log N →	+∞.	So	jg(ξξm))=
f 0(g(ξm))g(ξm) → +∞ by (B3.3). Also note that there exists a constant Bo such that 7(θm) ≤ Bo
for all m since Y is continuous on the unit sphere Sd-1. So We have
g0	(Sm) _ g0(ξm) g(Sm)	≤	g0(Sm) qmin(Om)	_ g0(ξm) —(0 )≤ g (ξm) B → 0
P(OL = ^ξm) P(θm)L	≤	TO ρ(θm)L	= TO	∙ Y( m)- TO	∙	,
where the first inequality follows since ξm, ≤ f (qmin(θm)). Together with (8), we have ∣Y(θm)-
γ(θm)1 → 0.	口
Remark A.6. For exponential loss, we have already Shown in Section 4.2 that γ(θ) is an O(ρ-L)-
additive approximation for 7(θ). For logistic loss, it follows easily from g0(q) = Θ(1) and (b) of
Lemma A.5 that γ(θ) is an O(ρ-L)-additive approximation for 7(θ) if L is sufficiently small.
A.3 Theorems
Now we state our main theorems. For the monotonicity of the normalized margin, we have the
following theorem. The proof is provided in Appendix B.
Theorem A.7. Under assumptions (A1), (A2), (B3)4, (B4), the following statements are true for
gradient flow:
1.	For a.e. t > to, S7(θ(t)) ≥ 0;
2.	For a.e. t > to, either 第γ(θ(t)) > 0 or 舟 口热点=0，，
3.	L(θ(t)) → 0 and ∣∣θ(t)∣∣2 → ∞ as t → +∞; therefore, ∣γ(θ(t)) — 7(θ(t))∣ → 0.
For the normalized margin at convergence, we have two theorems, one for infinite-time limiting
case, and the other being a finite-time quantitative result. Their proofs can be found in Appendix C.
As in the exponential loss case, we define the constrained optimization problem (P) as follows:
mm 2 ∣∣θk2
s.t. qn(θ) ≥ 1	∀n ∈ [N]
First, we show the directional convergence of θ(t) to a KKT point of (P).
Theorem A.8. Consider gradient flow under assumptions (A1), (A2), (B3), (B4). For every limit
point θ of {θ(t) : t ≥ 0}, θ∕qmin(θ)1∕L is a KKTpoint of(P).
Second, we show that after finite time, gradient flow can pass through an approximate KKT point.
Theorem A.9. Consider gradient flow under assumptions (A1), (A2), (B3), (B4). For any , δ > 0,
there exists r := Θ(log δ-1) and ∆ := Θ(e-2) such that θ∕qmin(θ)1∕L is an (e, δ)-KKTpoint at
some time t* satisfying log ∣∣θ(t*)∣∣2 ∈ (r,r + ∆).
For the definitions for KKT points and approximate KKT points, we refer the readers to Ap-
pendix C.1 for more details.
With a refined analysis, we can also provide tight rates for loss convergence and weight growth. The
proof is given in Appendix D.
Theorem A.10. Under assumptions (A1), (A2), (B3), (B4), we have the following tight rates for loss
convergence and weight growth:
L(θ(t))
(g(log t)2/L A
It(Iogt)2 )
and ∣∣θ(t)∣2 = Θ(g(log t)1/L).
Applying Theorem A.10 to exponential loss and logistic loss, in which g(x) = Θ(x), we have the
following corollary:
Corollary A.11. If '(∙) is the exponential or logistic loss, then,
L(θ(t)) = θ (“3”；2-2/L)	and kθ(t)k2 = θ((l°gt)1/L).
t(log t)
18
Published as a conference paper at ICLR 2020
B Margin Monotonicity for General Loss
In this section, we consider gradient flow and prove Theorem A.7. We assume (A1), (A2), (B3),
(B4) as mentioned in Appendix A.
We follow the notations in Section 5 to define P := ∣∣θk2 and θ := ^^^ ∈ Sd-1, and sometimes
we view the functions of θ as functions of t.
B.1	Proof for Proposition 1 and 2
To prove the first two propositions, we generalize our key lemma (Lemma 5.1) to general loss.
Lemma B.1. For γ defined in Definition A.3, the following holdsfor all t > to,
d ,	. c , d , ~、J d , ∖T dθ	小、
万 log P > 0 and —log 7 ≥ L —log P)	— .	(9)
dt	dt	dt	dt
Before proving Lemma B.1, we review two important properties of homogeneous functions. Note
that these two properties are usually shown for smooth functions. By considering Clarke’s subdif-
ferential, we can generalize it to locally Lipschitz functions that admit chain rules:
Theorem B.2. Let F : Rd → R be a locally Lipschitz function that admits a chain rule. If F is
k-homogeneous, then
(a)	For all x ∈ Rd and α > 0,
∂cιF(ax) = αk-1∂ ◦F (x)
That is, ∂jF(ax) = {αk-1h : h ∈ ∂OF(x)}.
(b)	(Euler’s Theorem for Homogeneous Functions). For all x ∈ Rd,
(x,∂°F(x))= k ∙ F(X)
That is, hx, h) = k ∙ F(X) forall h ∈ ∂°F(x).
Proof. Let D be the set of points x such that F is differentiable at x. According to the definition of
Clarke’s subdifferential, for proving (a), it is sufficient to show that
{ lim VF(axk) : Xk → x, αxk ∈ D} = {αk-1 lim VF(Xk) : Xk → x, Xk ∈ D}	(10)
k→∞	k→∞
Fix Xk ∈ D. Let U be a neighborhood of Xk. By definition of homogeneity, for any h ∈ Rd and
any y ∈ U \ {Xk},
F(αy) - F(aXk) - (αy - αXk, αk-1h) = k-1 F(y) — F(Xk) 一〈y — Xk, h)
kαy - aXk ∣∣2	ky — Xkk2	.
Taking limits y → Xk on both sides, we know that the LHS converges to 0 iff the RHS converges to
0. Then by definition of differetiability and gradient, F is differentiable at αXk iff it is differentiable
at Xk, and VF(αXk) = αk-1h iff VF(Xk) = h. This proves (10).
To prove (b), we fix X ∈ Rd. Let z : [0, +∞) → Rd, α 7→ αX be an arc. By definition of
homogeneity, (F ◦ z)(α) = αkF(X) for α > 0. Taking derivative with respect to α on both sides
(for differentiable points), we have
∀h ∈ ∂OF(αX) :(x, h) = kαk-1F(x)	(11)
holds for a.e. α > 0. Pick an arbitrary α > 0 making (11) hold. Then by (a), (11) is equivalent to
∀h ∈ ∂oF(x) :(x, αk-1h) = kαk-1F(x), which proves (b).	□
Applying Theorem B.2 to homogeneous neural networks, we have the following corollary:
Corollary B.3. Under the assumptions (A1) and (A2), for any θ ∈ Rd and X ∈ Rdx,
(θ, ∂oΦχ(θ)) = L ∙Φχ(θ),
where Φx(θ) = Φ(θ; X) is the network output for a fixed input X.
19
Published as a conference paper at ICLR 2020
Corollary B.3 can be used to derive an exact formula for the weight growth during training.
Theorem B.4. For a.e. t ≥ 0,
1dρ2 = L XX e-f (qn)f 0 (qn)qn.
n=1
Proof. The proof idea is to use Corollary B.3 and chain rules (See Appendix I for chain rules in
Clarke,s sense). Applying the chain rule on t → P = ∣∣θk2 yields 1 dpt- = - <θ, h〉for all
h ∈ ∂OL and a.e. t > 0. Then applying the chain rule on θ → L, we have
N
-∂oL⊆ X e-f(qn)f0(qn)∂oqn
n=1
{XX e-f，"")/0 (qn)hn ： hn ∈ ∂° q^j .
ByCorollaryB.3, hθ, hn〉= Lqn, and thus ɪ 零=L PN=I e-f (qn)f 0(qn)qn.
□
For convenience, we define ν(t) := PnN=1 e-f(qn)f0(qn)qn for allt ≥ 0. Then Theorem B.4 can
be rephrased as 1 务=Lν(t) for a.e. t ≥ 0.
Lemma B.5. For allt >t0,
ν(t) ≥
g(log L1) L
g0(log L1)
Proof. By Lemma A.5, qnr ≥ g(log LL) for all n ∈ [N]. Then by Assumption (B3),
f0(qn)qn ≥ f 0(g(Iog 7)) ∙ g(log 7) = g(log j) ∙
L	L g0(log LL)
Combining this with the definitions of ν(t) and L gives
N
n=1
≥ XX e-f(qn) g(l0g 1)
一士	g0(log L)
g(log1) L
g0(iog L1)
□
Prooffor Lemma B.1. Note that 技 log P =泰埠=LV(t) by Theorem B.4. Then it simply
follows from Lemma B.5 that 6 log ρ > 0 for a.e. t > t°. For the second inequality, We first prove
that log7 = log ('-1 (L)IPL = log (g(log L1 )/pL) exists for all t ≥ t0. L(t) is non-increasing
with t. So L(t) < e-f(bf) for all t ≥ t°. This implies that (1) log L1 is always in the domain of g;
(2) p > 0 (otherwise L ≥ Ne-f(0) > e-f(bf), contradicting (B4)). Therefore, Y := g(log L1 )/pL
exists and is always positive for all t ≥ t0, which proves the existence of log γ7.
By the chain rule and Lemma B.5, we have
dtlog 7 = ddt (log (g(log L))
rl g g0(log L) 1 d dL∖ 72 V (t)
LlogP) = gιoiir∙LΛ-M-L ∙下
≥ ɪ J-dLʌ - l2 ∙也
ν(t) ∖ dt J	p2
1	dL	L2 ν (t)2
一ν(t) Idt	p2 ).
On the one hand,-等=∣∣ 需 ∣∣j for a.e. t > 0 by Lemma I.3; on the other hand, Lν(t) = (θ,需)
by Theorem B.4. Combining these together yields
色 log 7 ≥ ɪfl dθ 2 - ∕θ ")∖ =L
dt gY ≥ V (t) ∖∖∖ dt 2 ∖θ, dt / J V (t)
2
2
(i - θθ>)dθ
dt
20
Published as a conference paper at ICLR 2020
By the chain rule, dθ = P (I - θθ>)第 for a.e. t > 0. So We have
dθ
d ρ ρ2
dtlog Y ≥ 西 dt
2	-1
2=L(dtlog P)
dθ
dt
2
2
□
B.2 Proof for Proposition 3
To prove the third proposition, We prove the folloWing lemma to shoW that L → 0 by giving an
upper bound for L. Since L can never be 0 for bounded ρ, L → 0 directly implies ρ → +∞. For
showing ∣7 - γ∣ → 0, we only need to apply (c) in Lemma A.5, which shows this when L → 0.
Lemma B.6. For all t > t0,
G(1/L(t))≥ 它饱产飞-t0)	for G(X)=Z…⅛g°g⅛7Ldu.
Therefore, L(t) → 0 and ρ(t) → +∞ as t → ∞.
Proof for Lemma B.6. By Lemma I.3 and Theorem B.4,
—
dL
dt
dθ
dt
2 ≥鸣A L2罢
Using Lemma B.5 to lower bound V and replacing P with (g(log L)/7) 1/L by the definition of Y
we have
dL ≥L2 ( ∖ L)
(7(t) )2/L
lg(log L )J
≥ LNO『2L ∙
g(log 1 )：-2/L L
where the last inequality uses the monotonicity of γ7. So the following holds for a.e. t ≥ t0,
g0(log L1 )2
g(log L )2-22L
d 1
dt L
≥ LWO)2L.
Integrating on both sides from t0 to t, we can conclude that
G(1∕L) ≥ L2γ(t0)2/L(t - to).
Note that 1/L is non-decreasing. If 1/L does not grow to +∞, then neither does G(1/L). But the
RHS grows to +∞, which leads to a contradiction. So L → 0.
To make L → 0, qmin must converge to +∞. So P → +∞.
□
C Convergence to the Max-margin Solution
In this section, we analyze the convergent direction of θ and prove Theorem A.8 and A.9, assuming
(A1), (A2), (B3), (B4) as mentioned in Section A.
We follow the notations in Section 5 to define P := ∣∣θk) and θ :二 k^ ∈ Sd-1, and sometimes
we view the functions of θ as functions of t.
C.1 Preliminaries for KKT Conditions
We first review the definition of Karush-Kuhn-Tucker (KKT) conditions for non-smooth optimiza-
tion problems following from (Dutta et al., 2013).
Consider the following optimization problem (P) for x ∈ Rd :
min f(x)
s.t. gn(x) ≤ 0	∀n ∈ [N]
where f, g1 , . . . , gn : Rd → R are locally Lipschitz functions. We say that x ∈ Rd is a feasible
point of (P) if x satisfies gn(x) ≤ 0 for all n ∈ [N].
21
Published as a conference paper at ICLR 2020
Definition C.1 (KKT Point). A feasible point x of (P) is a KKT point if x satisfies KKT conditions:
there exists λ1, . . . , λN ≥ 0 such that
1.	0 ∈ ∂f(x) + Pn∈[N] λn∂°gn(x);
2.	∀n ∈ [N] : λngn (x) = 0.
It is important to note that a global minimum of (P) may not be a KKT point, but under some
regularity assumptions, the KKT conditions become a necessary condition for global optimality. The
regularity condition we shall use in this paper is the non-smooth version of Mangasarian-Fromovitz
Constraint Qualification (MFCQ) (see, e.g., the constraint qualification (C.Q.5) in (Giorgi et al.,
2004)):
Definition C.2 (MFCQ). For a feasible point x of (P), (P) is said to satisfy MFCQ at x if there
exists v ∈ Rd such that for all n ∈ [N] with gn(x) = 0,
∀h ∈ ∂°gn(x) : hh, Vi > 0.
Following from (Dutta et al., 2013), we define an approximate version of KKT point, as shown
below. Note that this definition is essentially the modified -KKT point defined in their paper, but
these two definitions differ in the following two ways: (1) First, in their paper, the subdifferential is
allowed to be evaluated in a neighborhood of x, so our definition is slightly stronger; (2) Second,
their paper fixes δ = 2, but in our definition we make them independent.
Definition C.3 (Approximate KKT Point). For , δ > 0, a feasible point x of (P) is an (, δ)-KKT
point if there exists λn ≥ 0, k ∈ ∂o f (x), hn ∈ ∂°gn(x) for all n ∈ [N] such that
1.	k + Pn∈[N] λnhn (x)2 ≤ ;
2.	∀n ∈ [N] : λngn(x) ≥ -δ.
As shown in (Dutta et al., 2013), (, δ)-KKT point is an approximate version of KKT point in the
sense that a series of (, δ)-KKT points can converge to a KKT point. We restate their theorem in
our setting:
Theorem C.4 (Corollary of Theorem 3.6 in (Dutta et al., 2013)). Let {xk ∈ Rd : k ∈ N} be a
sequence of feasible points of (P), {k > 0 : k ∈ N} and {δk > 0 : k ∈ N} be two sequences. xk is
an (k, δk)-KKT point for every k, and k → 0, δk → 0. If xk → x as k → +∞ and MFCQ holds
at x, then x is a KKT point of (P).
C.2 KKT Conditions for (P)
Recall that for a homogeneous neural network, the optimization problem (P) is defined as follows:
min 2 kθk2
s.t. qn (θ) ≥ 1	∀n ∈ [N]
Using the terminologies and notations in Appendix C.1, the objective and constraints are f(x) =
1 ∣∣xk2 and gn(x) = 1 - qn(x). The KKT points and approximate KKT points for (P) are defined
as follows:
Definition C.5 (KKT Point of (P)). A feasible point θ of (P) is a KKT point if there exist
λ1 , . . . , λN ≥ 0 such that
1.	θ - PN=I λnhn = 0 for some hi,..., h∙N satisfying hn ∈ ∂oqn(θ);
2.	∀n ∈ [N] : λn(qn(θ) - 1) = 0.
Definition C.6 (Approximate KKT Point of (P)). A feasible point θ of (P) is an (, δ)-KKT point
of (P) if there exists λ1, . . . , λN ≥ 0 such that
LM — PN=I λnhn∣∣2 ≤ E for some hi,..., hN satisfying h ∈ ∂°qn(θ);
22
Published as a conference paper at ICLR 2020
2.	∀n∈ [N] : λn(qn(θ)-1) ≤δ.
By the homogeneity of qn, it is easy to see that (P) satisfies MFCQ, and thus KKT conditions are
first-order necessary condition for global optimality.
Lemma C.7. (P) satisfies MFCQ at every feasible point θ.
Proof. Take v := θ. For all n ∈ [N] satisfying qn = 1, by homogeneity of qn,
hv , hi = Lqn (θ) = L > 0
holds for any h ∈ -∂oqn(θ) = ∂◦(l - qn(θ)).	□
C.3 Key Lemmas
k d1 k (θ,第)to be the cosine of the angle between θ and 需.
Define β (t) :
Here β(t) is only
defined for a.e. t > 0. Since qn is locally Lipschitz, it can be shown that qn is (globally) Lipschitz
on the compact set Sd-1, which is the unit sphere in Rd . Define
Bo =SuPl qL : θ ∈ Rd ∖{0}j
=Sup {qn : θ ∈ Sd-1} < ∞.
B1 :=sup ∣kL-k∣ : θ ∈ Rd \ {0}, h ∈ ∂oqn,n ∈ [N]}
= sup {∣∣h∣∣2 : θ ∈ Sd-1, h ∈ ∂◦qn,n ∈ [N]} < ∞.
For showing Theorem A.8 and Theorem A.9, we first prove Lemma C.8. In light of this lemma, if
we aim to show that θ is along the direction of an approximate KKT point, we only need to show
β → 1 (which makes → 0) and L → 0 (which makes δ → 0).
Lemma C.8. Let C1, C2 be two constants defined as
C _	√2	c _ 2eNK2 ( Bi Yog2 K
CI = Y(t0FL,	C := LW/L kY(t0)J	,
where K, bg are constants specified in (B3.4). If log L ≥ bg at time t > to, then θ := θ∕qmin(θ)UL
is an (e, δ) -KKT point of (P), where E := Ci √1 — β and δ := CC/(log L).
Proof Let h(t):=第(t) for a.e. t > 0. By the chain rule, there exist hi,..., h∙N such that
hn ∈ ∂Oqn and h = Pn∈[N] e-f(qn)f0(qn)hn. Let 鼠：=hn/qi-n/L ∈ ∂Mn(θ) (Recall that
θ := θ∕qmin(e)1/L). Construct λn := q1m-n22Lρ ∙ e-f (qn)f 0(qn)/ ∣∣h∣∣C. Now We only need to show
2
N
θ - X λnhn
n=1
2
≤ γ2∕L (I — β)
N
X λn(qn(θ) - 1) ≤
n=1
2eNK2 ( B1 Yog2 K)	1
WL Y~J	f (YPL)
(12)
(13)
2
Then θ can be shown to be an (e, δ)-KKT point by the monotonicity γ(t) ≥ 7(t0) for t > to.
Proof of (12). From our construction, Pn=I λnhn = q--?LPh/ IlhIlC. So
N	2	2
θ - X λnhn	=	qm2ι/LP2	θ -	∣7h∣j-	=	qm2/LPC(2 -	2β)	≤	ξC7L (I- β^
n=1	2	IhI2 2	γ
where the last equality is by Lemma A.5.
23
Published as a conference paper at ICLR 2020
Proof for (13). According to our construction,
N	-2/L N
X λn(qn(θ) - 1) =	X e-f(qn)f0(qn)(qn - qmin).
n=1	khk2 n=1
Notethatkhk2 ≥(h, ^ = Lν∕ρ. By Lemma B.5 and LemmaD.1, we have
V ≥ g(log L) L ≥ ɪlogɪ ∙ L ≥ ɪf (7PL)e-f (qmin),
g g0(log L) 2 2K BL	- 2Kj v 11 )	,
where the last inequality uses f (YPL) = log L1 and L ≥ e-f Smin). Combining these gives
N	2K -2/L 2 N
X λn(qn (θ) - 1) ≤ Tqmin LP X ef (qmin)-f (qn)f 0(qn)(qn - qmin).
n=ι	LfWPL) n=ι
If qn > qmin, then by the mean value theorem there exists ξn ∈ (qmin, qn) such that f(qn) -
f (qmin ) = f0(ξn)(qn-qmin ). ByASSUmPtion(B3.4), WeknoWthat f (qn) ≤ K dlοg2(qn√ξn)ef0(ξn).
Note that dl0g2(qn∕ξn)e ≤ log2(2B1pL/qmin) ≤ log2(2B1∕7). Then we have
N	-2/L 2
X λn(qn(θ) - 1) ≤，min LP K log2(2BJY)	X	e-f 0(ξn)(qn-qmin) f 0(ξn)(qn - qmin)
Lf(	Lf (YPL)	J
n=1	n: qn 6=qmin
≤
2K丁2/lp2
Lf (YPL)
Kl°g2(2Bι/^) ∙ Ne
where the second inequality uses qm-2/LP2 ≤ Y-2/L by Lemma A.5 and the fact that the function
X → e-xx on (0, +∞) attains the maximum value e at X = 1.	口
By Theorem A.7, we have already known that L → 0. So it remains to bound β(t). For this, we
first Prove the following lemma to bound the integral of β(t).
Lemma C.9. For all t2 > t1 ≥ t0,
Z (尸(丁)-2 - 1) ∙ ； logP(T) ∙ dτ ≤ y log Y7M.
t1	dτ	L	γY(t1)
Proof. ByLemma B.4,六 log P = 2p2 % = LV fora.e. t > 0. ByLemmaB.1, for a.e. t ∈ (t1,t2),
ddt log Y ≥ l ∙
P dθ	d
LVdt ∙ dt logP
2
By the chain rule, dθ = P (I - θθ>)第.Sowe have
P dθ
LV dt
2 = U d02 -D 喘,昨
2	D dθ, θE
(14)
(15)
where the last equality follows from the definition of β. Combining 14 and 15, we have
dt logY ≥L(r-- 1)∙ dt log p∙
Integrating on both sides from t1 to t2 Proves the lemma.
□
A direct corollary of Lemma C.9 is the uPPer bound for the minimum β2 - 1 within a time interval:
Corollary C.10. For all t2 > tι ≥ to ,then there exists t* ∈ (t1,t2) such that
β(t )-2 _ 1 v 1 log Y(t2 ) - log Y(t1)
β(t*)	- 1 ≤ L ^ logP(t2) - logP(t1).
24
Published as a conference paper at ICLR 2020
Proof. Denote the RHS as C. Assume to the contrary that β(τ) -2 - 1 >
C for a.e. τ ∈ (t1 , t2). By
Lemma B.1, log ρ(τ) > 0 for a.e. τ ∈ (t1, t2). Then by Lemma C.9, we have
7 log Y-∣2τ	> Z	C ∙ ʃ logP(T)	∙ dτ = C ∙ (IogP(t2)	- logρ(tι))	= lo log	Yττ∖,
L Y(tι) Jt1 dτ	L	Y(ti)
which leads to a contradiction.	□
In the rest of this section, we present both asymptotic and non-asymptotic analyses for the directional
convergence by using Corollary C.10 to bound β(t).
C.4 Asymptotic Analysis
We first prove an auxiliary lemma which gives an upper bound for the change of θ.
Lemma C.11. For a.e. t > t0,
dθ	B1 d ,
而 ≤ TY ∙ dt log ρ.
2
Proof. Observethatll 剽2 = 1 IU - θθ>) 制2 ≤ P U 制2∙Itis Sufficienttobound Il dt∣∣2∙ By
the chain rule, there exists hp,..., h∙N : [0, +∞) → Rd satisfying that for a.e. t > 0, hn(t) ∈ ∂◦qn
and d = Pn∈[N] e-f (qn)f0(qn)hn(t). By definition of Bp, khnb ≤ BiPL-1 for a.e. t > 0. So
we have
dθ
dt
2
≤	e-f(qn)f0(qn) khnk2
n∈[N]
≤ X e-f(qn)f0(qn)qn •工∙ BIPL-1.
n∈[N]	qn
Note that every summand is positive. By Lemma A.5, qn is lower-bounded by qn ≥ qmin ≥
g(log LP), so We can replace qn with g(log LP) in the above inequality. Combining with the fact that
Pn∈[N] e-f(qn)f0(qn)qn is just ν, we have
dθ	≤ ν , ∙ Bp PL-1 = BV
dt 2	g(log LL)	YP
(16)
□
SOWehave ll制l2 ≤ P lldt ll2 ≤ 务 LV =务 d logP.
To prove Theorem A.8, we consider each limit point θ∕qmin(θ)i", and construct a series of ap-
proximate KKT points converging to it. Then θ∕qmin (θ)1∕L can be shown to be a KKT point by
Theorem C.4. The following lemma ensures that such construction exists.
Lemma C.12. For every limit point θ of {θ(t) : t ≥ 0∣, there exists a Sequence of {tm : m ∈ N}
O

such that tm ↑ +∞, θ(tm) → θ, and β(tm) → 1.
Proof. Let {m > 0 : m ∈ N} be an arbitrary sequence with m → 0. Now we construct {tm} by
induction. Suppose tp < t2 < •…< tm-i have already been constructed. Since θ is a limit point
and γY(t) ↑ γY∞ (recall that γY∞ := limt→+∞ γY(t)), there exists sm > tm-1 such that
i∣θ(Sm)-θll2 ≤ em and LIog γγm) ≤ em.
Let s0m > sm be a time such that log P(s0m) = log P(sm) + m. According to Theorem A.7,
lg P → +∞, so s0m must exist. We construct tm ∈ (sm, s0m) to be a time that β(tm)-2 - 1 ≤ 2m,
where the existence can be shown by Corollary C.10.
25
Published as a conference paper at ICLR 2020
Now We show that this construction meets our requirement. It follows from β(tm)-2 - 1 ≤ Em that
β(tm) ≥ 1/∙∖∕1 + Em → 1. ByLemmaC.11, we also know that
[M(tm) - θ( ≤ |M(tm) - θ(Sm)IL + (θ(Sm) - θ( ≤ Lγ(t ) . Em + Em → 0.
This completes the proof.
□
Proof of Theorem A.8. Let θ := θ∕qmin(θ)1∕L for short. Let {tm, : m ∈ N} be the sequence con-
structed as in Lemma C.12. For each tm, define E(tm) and δ(tm) as in Lemma C.8. Then we know
that θ(tm)∕qmin(tm)“L is an (e(tm), δ(tm))-KKT point and θ(tm)∕qmin(tm)14 → θ,E(tm) →
0, δ(tm) → 0. By Lemma C.7, (P) satisfies MFCQ. Applying Theorem C.4 proves the theorem. □
C.5 Non-asymptotic Analysis
ProofofTheorem A.9. Let Co := L log γγ(∞∞). Without loss of generality, we assume E < 空Ci
andδ < C2∕f(bf).
Let ti be the time such that logρ(tι) = LL log 鸣焉)=Θ(log δ-i) and t? be the time such that
logρ(t2) - logρ(tι) = 2CoC2E-2 = Θ(e-2). By Corollary C.10, there exists t* ∈ (t1,t2) such
that β(t*)-2 - 1 ≤ 2e2C-2.
Now we argue that θ(t*) is an (e, δ)-KKT point. By Lemma C.8, we only need to show
Cip1 - β(t*) ≤ E and C2∕f (Y(t*)ρ(t*)L) ≤ δ. For the first inequality, by assumption E < ɪ6Ci,
we know that β(t*)-2 - 1 < 3, which implies ∣β(t*)∣ < 22. Then we have 1 - β(t*) ≤
2e2C-2 ∙ 2+β(t)≤ E2C-2. Therefore, C2，1 - β(t*) ≤ E holds. For the second inequality,
Y(t*)ρ(t*)L ≥ Yg ∙ gC2δ)1 ≥ g(C2δ-1). Therefore, C2∕f6(t*)ρ(t*)L) ≤ δ holds.	□
C.6 Proof for Corollary 4.5
By the homogeneity of qn , we can characterize KKT points using kernel SVM.
Lemma C.13. If θ* is KKT point of (P), then there exists hn, ∈ ∂°Φχn (θ*) for n ∈ [N ] such that
L θ* is an optimal solution for the following constrained optimization problem (Q):
mm 2 kθk2
s.t. yn hθ, hni ≥ 1	∀n ∈ [N]
Proof. It is easy to see that (Q) is a convex optimization problem. For θ = L θ*, from Theorem B.2,
we can see that yn, (θ, hni = 2qn (θ*) ≥ 2 > 1, which implies Slater,s condition. Thus, we only
need to show that L θ* satisfies KKT conditions for (Q).
By the KKT conditions for (P), we can construct hn ∈ ∂°qn(θ*) for n ∈ [N] such that θ* -
PN=I λnynhn = 0 for some λ2,..., λw ≥ 0 and λn(qn(θ*) - 1) = 0. Thus, = θ* satisfies
1.	Lθ* - PN=1 Lλnynhn = 0;
2.	L λn (yn (Lθ* , hn) - 1) = L λn(9n (θ) - 1) ≥ 0.
So L θ* satisfies KKT conditions for (Q).	□
Now we prove Corollary 4.5 in Section 4.3.
Proof. By Theorem A.8, every limit point θ is along the direction of a KKT point of (P). Combining
this with Lemma C.13, we know that every limit point θ is also along the max-margin direction
of (Q).
26
Published as a conference paper at ICLR 2020
For smooth models, hn, in (Q) is exactly the gradient VΦχn (θ). So, (Q) is the optimization problem
for SVM with kernel Kq(x, x0) = (VΦχ(8), VΦχ0(θ). For non-smooth models, we can construct
an arbitrary function h(x) ∈ ∂0Φχ(θ) that ensures h(xn,) = hn. Then, (Q) is the optimization
problem for SVM with kernel Kq(x, x0) = (h(x), h(x0)).	□
D Tight B ounds for Loss Convergence and Weight Growth
In this section, we give proof for Theorem A.10, which gives tight bounds for loss convergence and
weight growth under Assumption (A1), (A2), (B3), (B4).
D.1 Consequences of (B3.4)
Before proving Theorem A.10, we show some consequences of (B3.4).
Lemma D.1. For f (∙) and g(∙), we have
1.	Forall X ∈ [bg, +∞), g0(X)∈ [2Kχ, 2Kx];
2.	Forall y ∈ [g(bg), +∞), f(y)∈ [2Ky, 2Ky].
Thus, g(x) = Θ(xg0(x)), f(y) = Θ(yf 0(y)).
Proof. To prove Item 1, it is sufficient to show that
xx
g(x) = bf+	g0(u)du ≥	g 0 (u)du
f(bf )	x/2
≥ (x/2) ∙ g0(x) = ɪ ∙xg0(x).
—' x) K	2K	八)
x = f(bf) + Z	g0(u)f0(g(u))du ≥ f (g(X)) Z	g0(u)du
f(bf)	K	f(g(x)/2)
f0(g(X))	1	g(X)
= S(X)/2) ∙	= 2K ∙ g7χ.
To prove Item 2, We only need to notice that Item 1 implies yf 0(y) = g0(fy))))∈ [康f (y), 2Kf (y)]
for all y ∈ [g(bg),十∞).	□
Recall that (B3.4) directly implies that f 0 (Θ(X)) = Θ(f0(X)) and g0(Θ(X)) = Θ(g0(X)). Combin-
ing this with Lemma D.1, we have the following corollary:
Corollary D.2. f (Θ(X)) = Θ(f (X)) and g(Θ(X)) = Θ(g(X)).
Also, note thatLemmaD.1 essentially shows that (log f (χ))0 = Θ(1∕x) and (logg(x))0 = Θ(1∕x).
So log f(X) = Θ(log X) and log g(X) = Θ(log X), which means that f and g grow at most polyno-
mially.
Corollary D.3. f(X) = XΘ(1) and g(X) = XΘ(1).
D.2 Proof for Theorem A.10
We follow the notations in Section 5 to define P := ∣∣θk2 and θ := k^ ∈ Sd-1, and sometimes
we view the functions of θ as functions of t. And we use the notations B0 , B1 from Appendix C.3.
The key idea to prove Theorem A.10 is to utilize Lemma B.6, in which L(t) is bounded from above
by G-ι1Ω(t)). So upper bounding L(t) reduces to lower bounding G-1. In the following lemma, We
obtain tight asymptotic bounds for G(∙) and G-1(∙):
27
Published as a conference paper at ICLR 2020
Lemma D.4. Forfunction G(∙) defined in Lemma B.6 and its inverse function G-1(∙), we have the
following bounds:
G(x)
g(log x)2/L
~ ( (log x)2 x.
and
G-1(y) = Θ
(log y)2	A
g(log y)2/Ly.'
Proof. We first prove the bounds for G(x), and then prove the bounds for G-1(y).
Bounding for G(x). Let CG = Rexp(bg) 息%U)2/l du. For X ≥ exp(bg),
G-C q Γ	g0(logu)2 d
G(X) = CG + I	2	γ2 21L du
exp(bg) g(log u)2-2/L
=C	[x	( g0(lθg u)lθg u A2 g(lθg U)2L d
=G + 7exp(bs)1	g(lθg U)	)	(log u)2	U
By Lemma D.1,
G(X) ≤ CG + 4K2g(log x)22l [x	7f∖du = O (g(：：g X)：L xA .
exp(bg) (log U)2	(log X)2
On the other hand, for X ≥ exp(2bg), we have
If	1 1	1	fx	g(log u)22L	,	g((log x)∕2)2∕L	rx	1	gg(log x)2/L、
G(x) ≥	du ≥	---—du = Ω	X .
—4K2 Jxc (logu)2	—	4K2	Jxc (logu)2	V (logx)2	J
Bounding for G-1(y). Let X = G-1(y) for y ≥ 0. G(X) always has a finite value whenever
X is finite. So X → +∞ when y→ +∞. According to the first part of the proof, we know
that y = Θ (或溜?)； x). Taking logarithm on both sides and using Corollary D.3, We have
log y= Θ(log X). By Corollary D.2, g(log y) = g(Θ(log X)) = Θ(g(logX)). Therefore,
(IOg y)2 y = Θ ( (IOg X)2 yA = Θ(x).
g(log y)2/L	∖g(log x)2∕Ln)
This implies that X = Θ (g(0gy12∕L y).
□
For other bounds, we derive them as follows. We first show that g(log L) = Θ(ρL). With this
equivalence, We derive an upper bound for the gradient at each time t in terms of L, and take
an integration to bound L(t) from below. Now we have both lower and upper bounds for L(t).
Plugging these two bounds to g(log L) = Θ(ρL) gives the lower and upper bounds for ρ(t).
Proof for Theorem A.10. We first prove the upper bound for L. Then we derive lower and upper
bounds for ρ in terms of L, and use these bounds to give a lower bound for L. Finally, we plug in
the tight bounds for L to obtain the lower and upper bounds for ρ in terms of t.
Upper Bounding L. By Lemma B.6, we have + ≥ GT (Ω(t)). Using Lemma D.4, we have
L = Ω (g(lOg)2∕L t), which completes the proof.
Bounding Pin Terms of L. γ(t) ≥ 7(t0), so PL ≤ Y(^g(log L) = O(g(log L)). On the
other hand, g(log L) ≤ qmin ≤ Bopl. So pl = Ω(g(log L)). Therefore, we have the following
relationship between pL and g(log ɪ):
PL = Θ(g(log L1)).	(17)
28
Published as a conference paper at ICLR 2020
Lower Bounding L. Let hi,..., h∙N be a set of vectors such that hn ∈ dqn and
By (17) and Corollary D.2, f0(qn) = f0(O(ρL)) = f0(O(g(log LL))) = O(f0(g(log -L)))=
O(1∕g0(log 1)). Again by (17), we have Ilhnk2 ≤ BiPL-1 = O(g(log LL)1-1/L). Combining
these two bounds together, it follows from Corollary D.2 that
f0(qn)" OeL )=O (g⅛).
Thus,
—
2
2
dL
dt
dθ
dt
≤
e-f(qn)∙ m[NX]{f0(qn) khnk2} ≤L2 O
(log L1 )2
g(iog L)2/L j.
2
2
2
By definition of G(∙), this implies that there exists a constant C such that £G(L) ≤ C for any L that
is small enough. We can complete our proof by applying Lemma D.4.
Bounding P in Terms of t. By (17) and the tight bound for L(t), PL = Θ(g(log L))=
Θ(g(Θ(log t))). Using Corollary D.2, we can conclude that PL = Θ(g(log t)).	□
E	Gradient Descent: Smooth Homogeneous Models with
Exponential Loss
In this section, we discretize our proof to prove similar results for gradient descent on smooth ho-
mogeneous models. As usual, the update rule of gradient descent is defined as
θ(t +1) = θ(t) 一 η(t)VL(t)	(18)
Here η(t) is the learning rate, and VL(t) := VL(θ(t)) is the gradient of L at θ(t).
We first focus on the exponential loss. At the end of this section (Appendix F), we discuss how to
extend the proof to general loss functions with a similar assumption as (B3).
The main difficulty for discretizing our previous analysis comes from the fact that the original ver-
sion of the smoothed normalized margin 7(θ) := P-L log LL becomes less smooth when P → +∞.
Thus, if We take a Taylor expansion for γ(θ(t + 1)) from the point θ(t), although one can show
that the first-order term is positive as in the gradient flow analysis, the second-order term is unlikely
to be bounded during gradient descent with a constant step size. To get a smoothed version of the
normalized margin that is monotone increasing, we need to define another one that is even smoother
than Y
Technically, recall that 篝=-∣∣VL∣∣2 does not hold exactly for gradient descent. However, if the
smoothness can be bounded by s(t), then it is well-known that
L(t + 1)-L(t) ≤-η(t)(i - s(t)η(t))kVLk2.
By analyzing the landscape of L, one can easily find that the smoothness is bounded locally by
O(L ∙ Polylog(LL)). Thus, if We set η(t) to a constant or set it appropriately according to the loss,
then this discretization error becomes negligible. Using this insight, we define the new smoothed
normalized margin Y in a way that it increases slightly slower than Y during training to cancel the
effect of discretization error.
29
Published as a conference paper at ICLR 2020
E.1 Assumptions
As stated in Section 4.1, we assume (A2), (A3), (A4) similarly as for gradient flow, and two addi-
tional assumptions (S1) and (S5).
(S1). (Smoothness). For any fixed x, Φ( ∙; x) is C2-smooth on Rd \ {0}.
(A2). (Homogeneity). There exists L > 0 such that ∀α > 0 : Φ(αθ; x) = αLΦ(θ; x);
(A3). (Exponential Loss). '(q) = e-q;
(A4). (Separability). There exists a time t0 such that L(θ(t0)) < 1.
(S5). (Learing rate condition). Pt≥0 η(t) = +∞ and η(t) ≤ H(L(θ(t))).
Here H(L) is a function of the current training loss. The explicit formula of H(L) is given below:
H(x) :
μ(x)
Cη κ(x)
θ Qlog IL，
where Cn is a constant, and κ(x), μ(x) are two non-decreasing functions. For constant learning rate
η(t) = η0, (S5) is satisfied when η0 if sufficiently small.
Roughly speaking, Cηκ(x) is an upper bound for the smoothness ofL in a neighborhood of θ when
x = L(θ). And we set the learning rate η(t) to be the inverse of the smoothness multiplied by a
factor μ(x) = o(1). In our analysis, μ(x) can be any non-decreasing function that maps (0, L(to)]
1/2
to (0,1/2] and makes the integral f0 / μ(x)dx exist. But for simplicity, we define μ(x) as
μ(x):
log L⅛)
2log 1
The value of Cη will be specified later. The definition of κ(x) depends on L. For 0 < L ≤ 1, we
define κ(x) as
κ(x) := x
2-2/L
For L > 1, we define κ(x) as
(’(log 1X ∈ (0,e2/L-2]
κmax	x ∈ (e2/L-2, 1)
where KmaX ：= e(2-2/L)(In(2-2/L)T). The specific meaning of Cn,κ(x) and μ(x) will become
clear in our analysis.
E.2 Smoothed Normalized Margin
Now we define the smoothed normalized margins. As usual, we define γ(θ)
time, we also define
. At the same
γ(θ):
eφ(L)
PL
Here φ : (0, L(t0)] → (0, +∞) is constructed as follows. Construct the first-order derivative of
φ(x) as
φ(x) = -SUp 11 + 2(1 + λ(w1∕L)”(w) : W ∈ [x, L(to)]1,
I W log W1	J
where λ(x) := (log 1 )-1. And then we set φ(χ) to be
φ(x) = log log J + / (φ0(w)
+
It can be verified that φ(x) is well-defined and φ0(x) is indeed the first-order derivative of φ(x).
Moreover, we have the following relationship among γ, Y and γ.
30
Published as a conference paper at ICLR 2020
Lemma E.1. γ(θ) is well-defined for L(θ) ≤ L(to) and has thefollowing properties:
(a)	If L(θ) ≤ L(to) ,then Y(θ) < Y(θ) ≤ Y(θ) ∙
(b)	Let {θm ∈ Rd : m ∈ N} be a sequence of parameters. IfL(θm) → 0, then
^(θm)
Y(θm)
→ 1.
Proof. First We verify that Y is well-defined. To see this, We only need to verify that
I(x) := / φφ0(w) +----------ι ] dw
√0	∖	W log w /
exists for all x ∈ (0, L(t0)], then it is trivial to see that φ0(w) is indeed the derivative of φ(w) by
I0(x) = φ0(x) + ±.
Note that I(x) exists for all x ∈ (0, L(t0)] as long as I(x) exists for a small enough x > 0. By
definition, it is easy to verify that r(w) := 1+2(1+λ(W)/L)μ(W)
'	'	、'	W log W
Thus, for a small enough w > 0, We have
is decreasing When w is small enough.
-φ0(w) = r(w)
1 + 2(1 + L(log ɪ)-1)黑号
______________g W
1	Jog L⅛)(1 + L (log 1 )T)
W log W	W(log w1 )2
So We have the folloWing for small enough x:
I(X)= ZX-log 卷[LI(Iog )-1)dw = Jo	w(log W1 )2	一	log at。) (log 1+2L(log 1 )2) I0 =-logL⅛! (⅛^ + 2L(⅛).	(19)
This proves the existence of I(x).
Now we prove (a). By Lemma A.5, γ(θ) ≤ γ(θ), so we only need to prove that ^(θ) < 7(θ).
To see this, note that for all W ≤ L(t0), r(w) >	1 ι . So we have -φ0(w) >	1 ι , and this
、一、， W log w	' 、	' W log W
implies I(x) < 0 for all x ≤ L(t0). Then it holds for all θ with L(θ) ≤ L(t0) that
Y(θ)	eΦ(L(θ))	eloglog L1θ) +I(L⑻)
==
γ(θ)	log L(θ)	eloglog S
eI(L(θ)) < 0
(20)
To prove (b), we combine (19) and (20), then for small enough L(θm), we have
γ(θ)	γ(θ) γ(θ)
γ(θ)
So 瑞=1 - O
log L⅛!
qmin (θm)
□
31
Published as a conference paper at ICLR 2020
Now we specify the value of Cη . By (S1) and (S2), we can define B0 , B1 , B2 as follows:
Bo ：= sup {qn ： θ ∈ Sd-1,n ∈ [N]},
Bi := sup {kVqnk2 : θ ∈ Sd-1,n ∈ [N]},
B2 ：= sup {∣∣V2qn∣∣2 : θ ∈S d-1,n ∈ [N ]}.
Then we set Cn = ɪ (BI + p(to)-LB2)min {γ(to)-2+2”, B-2+2/L}.
E.3 Theorems
Now we state our main theorems for the monotonicity of the normalized margin and the convergence
to KKT points. We will prove Theorem E.2 in Appendix E.4, and prove Theorem E.3 and E.4 in
Appendix E.5.
Theorem E.2. Under assumptions (S1), (A2) - (A4), (S5), the following are true for gradient de-
scent:
1.	For all t ≥ to, γ(t + 1) ≥ ^(t);
2.	For all t ≥ to, either ^(t + 1) > ^(t) or θ(t + 1) = θ(t);
3.	L(t) → 0 and ρ(t) → ∞ as t → +∞; therefore, ∣7(t) 一 7(t)∣ → 0.
Theorem E.3. Consider gradient flow under assumptions (S1), (A2) - (A4), (S5). For every limit
point θ of {θ(t) : t ≥ θ}, θ∕qmin(θ)1∕L is a KKTpoint of(P).
Theorem E.4. Consider gradient descent under assumptions (S1), (A2) - (A4), (S5). For any , δ >
0, there exists r := Θ(log δ-i) and ∆ := Θ(e-2) such that θ∕qmin(θ)1∕L is an (e, δ)-KKTpoint at
some time t* satisfying log ρ(t*) ∈ (r,r + ∆).
With a refined analysis, we can also derive tight rates for loss convergence and weight growth. We
defer the proof to Appendix E.6.
Theorem E.5. Under assumptions (S1), (A2) - (A4), (S5), we have the following tight rates for
training loss and weight norm:
L(t) = θ (T(lθg T)2-2∕L )	and P(t) = θ((Iog T尸/L)，
where T = Ptτ-=1t0 η(τ).
E.4 Proof for Theorem E.2
We define ν(t) := PnN=1 e-qn(t)qn(t) as we do for gradient flow. Then we can get a closed form
for hθ(t), -VL(t)i easily from Corollary B.3. Also, We can get a lower bound for ν(t) using
Lemma B.5 for exponential loss directly.
CorollaryE.6. hθ(t), -VL(t)i = LV(t). If L(t) < 1, then V(t) ≥ 入视)))∙
As we are analyzing gradient descent, the norm of VL and V2L appear naturally in discretization
error terms. To bound them, we have the following lemma when 7(θ) and ρ(θ) have lower bounds:
LemmaE.7. For any θ, if 7(θ) ≥ ^(to), ρ(θ) ≥ ρ(to), then
kVL(θ)k22 ≤ 2Cηκ(L(θ))L(θ) and	V2L(θ)2 ≤ 2Cηκ(L(θ)).
32
Published as a conference paper at ICLR 2020
Proof. By the chain rule and the definitions of B1, B2, we have
N
∣∣VL∣∣2 = - X e-qn Vqn	≤LρL-1 Bi
n=1	2
N
V2L2 = X e-qn(VqnVqn> - V2qn)
n=1	2
N
≤ X e-qn (B2ρ2L-2 + B2pl-2) ≤ Lp2L-2 (B2 + P-LB2).
n=1
Note that ^(to)pL ≤ γpL ≤ log L ≤ BopL. So γ(to)-i log L ≤ PL ≤ B-I log 1. Combining all
these formulas together gives
2-2/L
kVLk2 ≤ Bi min {γ(to)-2+" B-2+2/L} L2 ∙ (log L∙)	≤ 2gK(L)L
2-2/L
∣∣V2L∣I2 ≤ (B2 + P-L B2) min {夕(力。厂2+2〃，B-2+2/L} L ∙ (⅛ L)	≤ 2Cη κ(L),
which completes the proof.	□
For proving the first two propositions in Theorem E.2, we only need to prove Lemma E.8. (P1) gives
a lower bound for γ. (P2) gives both lower and upper bounds for the weight growth using ν(t). (P3)
gives a lower bound for the decrement of training loss. Finally, (P4) shows the monotonicity of ^,
and it is trivial to deduce the first two propositions in Theorem E.2 from (P4).
Lemma E.8. For all t = t0, t0 + 1, . . . , we interpolate between θ(t) and θ(t + 1) by defining
θ(t + α) = θ(t) - αη(t)VL(t) for α ∈ (0, 1). Then for all integer t ≥ t0, ν(t) > 0, and the
following holds for all α ∈ [0, 1]:
(P1). γ(t + α) >γ(to).
(P2). 2Lɑη(t)ν(t) ≤ p(t + α)2 — p(t)2 ≤ 2Lαη(t)ν(t)(1 + '('("*'"))).
(P3). L(t + α) — L(t) ≤ —αη(t)(1 — μ(L(t))) ∣∣VL(t)∣∣2.
(P4). log Y(t + α) — log^(t) ≥ 掰% ∣∣(l — θ(t)θ(t)>) VL(t)[ ∙ log 爷毕.
To prove Lemma E.8, we only need to prove the following lemma and then use an induction:
Lemma E.9. Fix an integer T ≥ t0. Suppose that (P1), (P2), (P3), (P4) hold for any t + α ≤ T.
Then if (P1) holds for (t, α) ∈ {T} × [0, A) for some A ∈ (0, 1], then all of (P1), (P2), (P3), (P4)
hold for (t, α) ∈ {T } × [0, A].
Proof for Lemma E.8. We prove this lemma by induction. For t = t0, α = 0, ν(t) > 0 by (S4) and
Corollary E.6. (P2), (P3), (P4) hold trivially since log ^(t + α) = log ^(t), L(t + α) = L(t) and
log ^(t + α) = log^(t). By LemmaE.1, (P1) also holds trivially.
Now we fix an integer T ≥ t0 and assume that (P1), (P2), (P3), (P4) hold for any t + α ≤ T (where
t ≥ t0 is an integer and α ∈ [0, 1]). By (P3), L(t) ≤ L(t0) < 1, so ν(t) > 0. We only need to show
that (P1), (P2), (P3), (P4) hold for t = T andα ∈ [0, 1].
Let A := inf {α ∈ [0, 1] : α = 1 or (P1) does not hold for (T, α)}. If A = 0, then (P1) holds for
(T, A) since (P1) holds for (T — 1, 1); if A > 0, we can also know that (P1) holds for (T, A) by
Lemma E.9. Suppose that A < 1. Then by the continuity of 7(T + α) (with respect to α), we know
that there exists A0 > A such that Y(T + a) > γ(to) for all α ∈ [A, A0], which contradicts to the
definition of A. Therefore, A = 1. Using Lemma E.9 again, we can conclude that (P1), (P2), (P3),
(P4) hold for t = T and α ∈ [0,1].	□
33
Published as a conference paper at ICLR 2020
Now we turn to prove Lemma E.9.
Proof for Lemma E.9. Applying (P3) on (t, α) ∈ {t0, . . . , T - 1} × 1, we have L(t) ≤ L(t0) < 1.
Then by Corollary E.6, we have ν(t) > 0. Applying (P2) on (t, α) ∈ {t0 , . . . , T - 1} × 1, we can
get ρ(t) ≥ ρ(t0).
Fix t = T. By (P2) with α ∈ [0, A) and the continuity of Y We have 7(t + A) ≥ ^(to). Thus,
Y(t + α) ≥ ^(to) for all α ∈ [0, A].
Prooffor(P2). By definition, ρ(t+α)2 = ∣∣θ(t) - αη(t)VL(t)k2 = ρ(t)2 +α2η(t)2 kVL(t)k2 -
2αη(t) hθ(t), VL(t)i. By Corollary E.6, we have
ρ(t + α)2 - ρ(t)2 = α2η(t)2 ∣VL(t)∣22 + 2Lαη(t)ν (t).
So ρ(t + α)2 - ρ(t)2 ≥ 2Lαη(t)ν(t). For the other direction, we have the following using Corol-
lary E.6 and Lemma E.7,
ρ(t + α)2 - ρ(t)2 = 2Lαη(t)ν (t)
≤ 2Lαη(t)ν (t)
αη(t) ∣∣VL(t)k2∖
+	2Lν(t)	)
1	C-1κ(L(t))-1μ(L(t)) ∙ 2gκ(L(t))L(t)
+	2LL(t)λ(L(t))-1
2Lαη(t)ν(t) (1 + λ(L(t))μ(L(t))∕L).
Proof for (P3). (P3) holds trivially for α = 0 or VL(t) = 0. So now we assume that α 6= 0 and
VL(t) 6= 0. By the update rule (18) and Taylor expansion, there exists ξ ∈ (0, α) such that
L(t + α) = L(t) + (θ(t + α) — θ(t))> VL(t) + 2(θ(t + α) — θ(t))>V2L(t + ξ)(θ(t + α) — θ(t))
≤ L(t) - αη(t) ∣VL(t)∣2 + 2α2η(t)2 ∣∣V2L(t + ξ)∣∣2 ∣VL(t)∣2
=L(t) - αη(t) (1 - 2αη(t) ∣∣V2L(t + ξ)∣∣2) ∣VL(t)k2 .
By LemmaE.7, ∣∣V2L(t + ξ)∣∣2 ≤ 2Cη ∙ κ(L(t + ξ)), so Wehave
L(t+α) ≤ L(t) - αη(t) (1 - αCηη(t)κ(L(t + ξ))) ∣VL(t)∣22 .	(21)
Now we only need to show that L(t + α) < L(t) for all α ∈ (0, A]. Assuming this, we can have
κ(L(t + ξ)) ≤ κ(L(t)) by the monotonicity of κ, and thus
L(t + α) ≤ L(t) - αη(t) (1 - αCη η (t)κ(L(t))) ∣VL(t)∣22
≤L(t)- αη(t)(1 - μ(L(t))) ∣∣VL(t)∣∣2 .
Now we show that L(t + α) < L(t) for all α ∈ (0, A]. Assume to the contrary that α0 :=
inf {α0 ∈ (0, A] : L(t + α0) ≥ L(t)} exists. If α0 = 0, let p : [0, 1] → R, α 7→ L(t + α), then
p0(0) = limaψo '"+0)-'") ≥ 0, but it contradicts to p0(0) = -η(t) ∣∣VL(t)∣2 < 0. If α0 > 0,
then by the monotonicity of κ we have κ(L(t + ξ)) < κ(L(t)) for all ξ ∈ (0, α0), and thus
L(t + α) ≤ L(t) - αη(t) (1 - μ(L(t))) ∣VL(t)∣2 < L(t),
which leads to a contradiction.
Prooffor (P4). Wedefine v(t) := θ(t)θ(t)>(-VL(t)) andu(t) :=(I - θ(t)θ(t)>) (-VL(t))
similarly as in the analysis for gradient flow. For v(t), we have
kv(t)k2 = Pty hθ(t), -L(t)i = ρt) ∙ Lν(t)∙
34
Published as a conference paper at ICLR 2020
Decompose ∣∣VL(t)k2 = ∣∣v(t)∣∣2 + ∣∣u(t)k2. Thenby (P3), we have
L(t + α) - L⑴ ≤ -αη⑴(I- μ(L(U)) (p(t)2 ∙ L2ν⑴2 + kuk2).
M11ltinlχ7inσ 1+λ(L(D)μ(L(t))/L Cn hntb cir!pc ,szp hae
WiuiLipiymg	(1 μ(L(t)))ν(t)	on both Sides, We have
1 + λ(L(t))μ(L(t))∕L	(	λ(L(t))μ(L(t))、/L2ν(t)	|回2)
(1 - μ(L(t)))ν(t) (L(t + a) -L(t)) ≤ -αη(t) C +-------L---------八E + 西)
By Coroiiary E.6, we can bound ν(t) by ν(t) ≥ L(t)∕λ(L(t)). By (P2), we have the inequaiity
αη(t)ν(t) (1 + λ(L(t)Lμ(LIt))) ≥ ρ(t+α22-ρ(t)2. So Wefurther have
1 + λ(L⑴(L⑴)/L	"∕* 1 、	/	1 ( (2 , 、2	(十、2、(L I	P⑴2 IlTfIl2、
(1 - μ(L(t)))L(t)∕λ(L(t)) (Lmtn ≤ -而(ρ(t+a) -P(t) )(2 + 2LV(ψ kuk2)
From the definition φ, it is easy to see that -φ0(L(t)) ≥ 门1+)，£(；)”，<(?)/■%、、. Let ψ(χ) = 一 log x,
(1-μ(Llt)))L(t"λ(Llt))
then ψ0(x) = 一 1. Combining these together gives
φ (L(X(L( + α) - L(t)) + ψ0(P⑴2)(P(t + α)2 一 ρ(t)2) (5 + 9P^ 2,γ2 kuk2) ≥ 0
2	2Lν (t)
Then by convexity of φ and ψ, We have
(φ(L(t + α)) — φ(L(t))) + (log ρ(t + α)2 - log 涛)(2 + 2LV⅛ kuk2) ≥ 0
And by definition of γ, this can be re-written as
log γ(t + α) 一 log^(t) = (φ(L(t + α)) 一 φ(L(t))) + L (log 附 + ① 一 log 焉)
≥ 一 (log ρ(t + α)2 - log P⅛) 2L‰ kuk2
P(t)2	2	P(t+α)
=E kuk2 'log	.
Prooffor(PI). By (P4), log ^(t + α) ≥ log γ(t) ≥ log ^(t0). Note that φ(x) ≥ log log ɪ. So we
have
7(t + α) > γ(t + α) ≥ γ(to),
which completes the proof.	□
For showing the third proposition in Theorem E.2, we use (P1) to give a iower bound for ∣VL(t)∣2,
and use (P3) to show the speed of ioss decreasing. Then it can be seen that L(t) → 0 and P(t) →
+∞. By Lemma E.1,we then have ∣7 一 γ∣ → 0.
LemmaE.10. Let Eo := L(t0)2(log LI^J)2-2/L. Thenfor all t > to,
1	t-1	L(t0 )	1
E(L(U) ≥ 5L%(tO)2/L E η(T)	for E(X)=	. ʃ 2,-1、2 2/L 万 i du.
2	&	Jx	min{u2(log 1 )2-2/L,Eo}
Therefore, L(t) → 0 and P(t) → +∞ as t → ∞.
Proof. For any integer t ≥ to, μ(L(t)) ≤ 1 and ∣∣VL(t)∣2 ≥ ∣∣v(t)∣2. Combining these with (P3),
we have
L(t + 1) - L(t) ≤ 一1 η(t) kv(t)k2 = 一1 η(t)LW.
2	2	P(t)2
35
Published as a conference paper at ICLR 2020
By (P1), ρ(t)-2 ≤ ^(to)24 (log L(t)) / . By Corollary E.6, V(t)2 ≥ L(t)2 (log Lt)) . Thus
we have
L(t +I)- L⑴ ≤ -2η(t) ∙ L2Y(tO)2/LL⑴2 (log L(t))	.
It is easy to see that	^产〜工 is Unimodal in (0,1), so E0(χ) is non-decreasing and E(X) is
convex. So we have u
E(L(t + 1)) - E(L(t)) ≥ E0(L(t)) (L(t + 1) - L(t))
1	L(t)2 (log 击)2-2/L
≥ η(t)∙ ∙ L2Y(t0)2/L ∙ . JzV八2∖-----1 12 2/L P I
2	mm{L(t)2(log L(t))2-2/L,Eo}
≥ 1 η(t) ∙ L2Y(t0)2/L,
which proves E(L(t)) ≥ 2L2^(to)2∕L Ptr=t° η(τ). Note that L is non-decreasing. If L does not
decreases to 0, then neither does E(L). But the RHS grows to +∞, which leads to a contradiction.
So L → 0. To make L → 0, qmin must converge to +∞. So P → +∞.	□
E.5 Proof for Theorem E.3 and E.4
The proofs for Theorem E.3 and E.4 are similar as those for Theorem A.8 and A.9 in Appendix C.
Define β (t) :
gjt)k2〈°，-▽£(t))as we do in Appendix C. It is easy to see that Lemma C.8
still holds if We replace 7(t0) with ^(to). So We only need to show L → 0 and β → 1 for proving
convergence to KKT points. L → 0 can be followed from Theorem E.2. Similar as the proof for
Lemma C.9, it follows from Lemma E.8 and (15) that for all t2 > t1 ≥ t0,
t2 -1
X Cβ(t)-2 -1) ∙ log
t=t1
ρ≡J2 ≤ 1log YM
ρ(t)	≤ L g Y(tι).
(22)
Now we prove Theorem E.4.
Proof for Theorem E.4. We make the following changes in the proof for Theorem A.9. First, we
replace γ(to) with ^(to), since Y(t) (t ≥ to) is lower bounded by γ(to) rather than γ(to). Second,
when choosing t1 andt2, we make logρ(t1) and log ρ(t2) equal to the chosen values approximately
with an additive error o(1), rather than make them equal exactly. This is possible because it can be
shown from (P2) in Lemma E.8 that the following holds:
ρ(t+ 1)2 - ρ(t)2 = O(η(t)ν(t))
=O(K(L(t))-1 ∙ L(t)p(t)L) = o (log ρ(t))2-27L) = o(ρ(t)-L+2).
Dividing ρ(t)2 on the leftmost and rightmost sides, we have
ρ(t + 1)∕ρ(t) = 1 + o(ρ(t)-L) = 1 + o(1),
which implies that log ρ(t + 1) - log ρ(t) = O(1). Therefore, for any R, we can always find the
minimum time t such that log Po ≥ R, and it holds for sure that log ρ(t)-R → 0 as R → +∞. □
For proving Theorem E.3, we also need the following lemma as a variant of Lemma C.11.
Lemma E.11. For all t ≥ t0,
O ,	、 O ,
θ(t + 1) - θ(t
ρ(t + 1)
P(t)
+ 1 log
ρ(t + 1)
P(t)
36
Published as a conference paper at ICLR 2020
Proof. According to the update rule, we have
同 + I)- ML =小 θ(t + I)-中θ(t)2
≤ ρ⅛ (kθ(t+1)-θ(t)k2+1(中-1) θT)
=P(η+⅛ kvL(t)k2+ (1- P(ρ+⅛).
By (16), ∣∣VL(t)k2 ≤ Y(1V(t). So We can bound the first term as
η⑴	∣∣vL(t)k ≤ _BL η⑴V⑴	≤ B1_ p(t +1)2 — P⑴2
ρ(t +1) k ( )k2 - Y(t) ∙ ρ(t +1)ρ(t) - 7(t) ∙ 2Lρ(t + 1)ρ(t)
B1	ρ(t + 1)2 - ρ(t)2 ρ(t + 1)
-- T ∖ T ∙	∖	Γ^7	∙	∖
2Lγ(t)	ρ(t + 1)2	ρ(t)
Bl	ρ(t + 1)	ρ(t + 1)
≤ 斤∙ ~P^T g ~7(tr
where the last inequality uses the inequality a-b ≤ log(a/b). Using this inequality again, We can
bound the second term by
1 - Pt、≤ log P(t + 1).
ρ(t +1) —	ρ(t)
Combining these together gives ∣∣θ(t + 1) - θ(t) ( ≤ (LBT) ∙ Pt+1 + 1)log PP+-.
□
Now we are ready to prove Theorem E.3.
Proof for Theorem E.3. As discussed above, we only need to show a variant of Lemma C.12 for
gradient descent: for every limit point θ of {θ(t) : t ≥ θ} ,there exists a sequence of {tm : m ∈ N}
O

such that tm ↑ +∞, θ(tm) → θ, and β(tm) → 1.
We only need to change the choices of sm, s0m, tm in the proof for Lemma C.12. We choose sm >
tm-1 to be a time such that
and
11 Λ-lim∞ 小⑴
Llog I Fmr
3
m.
≤
Then we let s0m > sm be the minimum time such that logρ(s0m) ≥ logρ(sm) + m. According to
Theorem E.2, sm and s0m must exist. Finally, we construct tm ∈ {sm, . . . ,s0m - 1} to be a time that
β(tm)-2 - 1 ≤ 2m, where the existence can be shown by (22).
To see that this construction meets our requirement, note that β(tm)-2 - 1 ≤ 2m → 0 and
M(tm) - θ( ≤ M(tm) - θ(sm)∣L + M(Sm)- ^^||2 ≤ ^L^(t ) e^m + 1^ ^ Cm + Cm → 0,
where the last inequality is by Lemma E.11.	□
E.6 Proof for Theorem E.5
Proof. By a similar analysis as Lemma D.4, we have
E(X) - ZxLO0 θ (u2(lθg1 )2-2∕L ) du - C) θ ( (⅛τ¼L) du
θ (x(lθgX)2-2∕L
37
Published as a conference paper at ICLR 2020
We can also bound the inverse function E-1(y) by Θ (#^y)2-2∕L ). With these, We can use a
similar analysis as Theorem A.10 to prove Theorem E.5.
First, using a similar proof as for (17), we have PL = Θ(log ±). So we only need to show L(t)=
Θ(T(logT)2-2∕L ). With a similar analysis as for (P3) in Lemma E.9, we have the following bound
for L(τ + 1) - L(τ):
L(T + 1) - L(T) ≥ -η(τ)(1 + μ(L(τ))) kVL(τ)k2 .
Using the fact that μ ≤ 1/2, we have L(τ + 1) 一 L(τ) ≥ 一2η(τ) ∣∣VL(τ)k2. By Lemma E.7,
kVL(T)k2 ≤ 2Cη κ(L(T))L(T). Using a similar proof as for Lemma E.10, we can show that
E(L(t)) ≤ O(T). Combining this with Lemma E.10, we have E(L(t)) = Θ(T). Therefore,
L(t) = Θ( T(log T)2-2∕L ).	口
F Gradient Descent: General Loss Functions
It is worth to note that the above analysis can be extended to other loss functions. For this, we need
to replace (B3) with a strong assumption (S3), which takes into account the second-order derivatives
off.
(S3)	. The loss function '(q) can be expressed as '(q) = e-f ⑷ such that
(S3.1).	f : R → R is C2-smooth.
(S3.2).	f0(q) > 0forallq ∈ R.
(S3.3).	There exists bf ≥ 0 such that f 0(q)q is non-decreasing for q ∈ (bf, +∞), and
f0(q)q → +∞ as q → +∞.
(S3.4).	Let g : [f (bf), +∞) → [bf, +∞) be the inverse function of f on the domain
[bf, +∞). There exists p ≥ 0 such that for all x > f(bf),y > bf,
∣g00(χ)∣‹p and	f00(y)∣‹p
I赤)∣≤ X and	If^∣≤ y.
It can be verified that (S3) is satisfied by exponential loss and logistic loss. Now we explain each of
the assumptions in (S3). (S3.2) and (S3.3) are essentially the same as (B3.2) and (B3.3). (S3.1) and
(B3.1) are the same except that (S3.1) assumes f is C2-smooth rather than C1-smooth.
(S3.4) can also be written in the following form:
d
；log g0(x) ≤ P ∙ ； log x and
dx
dylog f0(小 P
:log y.
dy
(23)
That is, logg0(x) and logf0 (y) grow no faster than O(log x) and O(log y), respectively. In fact,
(B3.4) can be deduced from (S3.4). Recall that (B3.4) ensures that Θ(g0(x)) = g0(Θ(x)) and
Θ(f0(y)) =f0(Θ(y)). Thus, (S3.4) also gives us the interchangeability betweenf 0, g0 and Θ.
Lemma F.1. (S3.4) implies (B3.4) with bg = max{2f (bf), f (2bf)} and K = 2p.
Proof. Fix x ∈ (bg, +∞), y ∈ (g(bg), +∞) and θ ∈ [1/2, 1). Integrating (23) on both sides of the
inequalities from θx to x and θy to y, we have
|logg0(x) - logg0(θx)∣ ≤ p ∙ log1 and |log f0(y) - log f 0(θy)∣ ≤ P ∙ log1.
Therefore, we have g0(χ) ≤ θ-pg0(θχ) ≤ Kg0(θχ) and f 0(y) ≤ θ-pf 0(θy) ≤ Kf 0(θχ).	口
To extend our results to general loss functions, we need to redefine κ(x), λ(x), φ(x), Cη, H(x) in
order. For κ(x) and λ(x), we redefine them as follows:
κ(x) =sup[ W(IOg 1)； ： W ∈ (0,x]l
[g0(IOg w)2	J
λ(x) :
g0(log 1)
g(log X)
38
Published as a conference paper at ICLR 2020
ByLemma D.1, w(gogiθg[j = O (w(log WW)4-2/L/g(log 1 )2) → 0. So κ(x) is well-defined.
Using λ(x), we can define φ(χ) and ^(θ) as follows.
φ(x) := log g(log
φ0(x) := - sup
x
(1 + 2(1 + λ(w)∕L)μ(w)) : W ∈ [x, L(to)]
+
dw
eφ(L(θ))
γ(θ) =----L一.
ρL
Using a similar argument as in Lemma E.1, we can show that γ(θ) is well-defined and γ(θ) <
Y(θ) ：= g(log L1 )∕ρL ≤ γ(θ). When L(θ) → 0, we also have ^(θ)∕γ(θ) → 1.
For Cη , we define it to be the following.
Cn ：= LBi ((皆 Y Bi + r^p+1-(PB1 + B2))(华了 min {^(t0)-2+2',B-2+”}.
2	∖∖^(t0)√	log L(tθ)	γ ∖^(t0)√	1	J
Finally, the definitions for μ(x) := (log LI^)∕(2log ɪ) and H(x) := μ(x)∕(gκ(x)) in (S5)
remain unchanged except that κ(x) and Cη now use the new definitions.
Similar as gradient flow, we define ν(t) := PnN=1e-f(qn(t))f0(qn(t))qn(t). The key idea behind
the above definitions is that we can prove similar bounds for ν(t), ∣VL∣2, ∣V2L∣2 as Corollary E.6
and Lemma E.7.
Lemma F.2. (θ(t), -VL(t)i = Lν(t). If L(t) < e-f (bf) ,then V (t) ≥ 入沈)))and λ(L(t)) has the
lower bound λ(L(t)) ≤ 2p+i (log Lt)).
Proof. It can be easily proved by combining Theorem B.4, Lemma B.5 and Lemma D.1 together.
□
LemmaF.3. For any θ, if L(θ) ≤ L(to), 7(θ) ≥ ^(to) ,then
∣∣VL(θ)k2 ≤ 2gκ(L(θ))L(θ) and ∣∣V2L(θ)∣∣2 ≤ 2gκ(L(θ)).
Proof. Note that a direct corollary of (23) is that f0(x1) ≤ f0(x2)(x1/x2)p for x1 ≥ x2 > bf . So
we have
f0(qn) ≤ f0 (g(lθg L)
p
qn
g(log LL )J	g0(log LL)
p
≤
R
g0 (log 1),
where R := (Bo∕^(to))p. Applying (S3.4), We can also deduce that
If00(qn)∣ ≤ 上∙ f0(qn) ≤
qn
pR
g(log LL)g0(log 1).
Now we bound ∣VL∣2 and ∣V2L∣2. By the chain rule, we have
kVLk2
LρL-1
n=1
N
g0(log L1)
n=1
N
≤ X e-f(qn)
n=1
R2
g0(log LL)2 +
pR
g(log LL)g0(log LL)
+ g0(log 1)
B2ρL-2
—
2
1
2
R
≤ gLC⅛ ((R+pλ(L)) B2 +;B) R
39
Published as a conference paper at ICLR 2020
For P We have γ(to)ρL ≤ log L1 ≤ BoρL, so We can bound ρ2L-2 by ρ2L-2 ≤ M(log L1 )2L-2,
where M := min {夕(t0)-2+2〃，6-2+2”}. ByLemmaF.2, we have λ(L) ≤ 2p+1^grι ≤
I 2p+1 , and also g (loLIL) = λ(L) ∙ 7 ≤ ∣2p+1B1 . Combining all these formulas together gives
lθg Lw)	P	( - log L(t0)	J
kVL∣∣2 ≤ BlR2Mg0QLI*)2 (log 1)	2 ≤ 2CηK(L)L
MLU2 ≤
R+
B12 +
2p+1B1B2
log L⅛7
L
RM ———1-
g0(log L1 )2
2-2/L
≤ 2Cηκ(L),
which completes the proof.
□
With Corollary E.6 and Lemma E.7, we can prove Lemma E.8 with exactly the same argument.
Then Theorem E.2, E.3, E.4 can also be proved similarly. For Theorem E.5, we can follow the
argument for gradient flow to show that it holds with slightly different tight bounds:
Theorem F.4. Under assumptions (S1), (A2), (S3), (A4), (S5), we have the following tight rates for
training loss and weight norm:
L(t)=θ (gaooTT；) and p(t)=θ(g(logT)i/l),
where T = Ptτ-=1t0 η(τ).
G Extension: Multi-class Clas sification
In this section, we generalize our results to multi-class classification with cross-entropy loss. This
part of analysis is inspired by Theorem 1 in (Zhang et al., 2019), which gives a lower bound for the
gradient in terms of the loss L.
Since now a neural network has multiple outputs, we need to redefine our notations. Let C be
the number of classes. The output of a neural network Φ is a vector Φ(θ; x) ∈ RC. We use
Φj(θ; x) ∈ R to denote the j-th output of Φ on the input x ∈ Rdx. A dataset is denoted by
D = {xn, yn}nN=1 = {(xn, yn) : n ∈ [N]}, where xn ∈ Rdx is a data input and yn ∈ [C] is the
corresponding label. The loss function of Φ on the dataset D is defined as
N
L(θ) := X-log
n=1
eφyn (θ;Xn )
eφj (θ;Xn )
The margin for a single data point (xn, yn) is defined to be qn(θ) := Φyn (θ; xn) -
maxj 6=yn {Φj (θ; xn)}, and the margin for the entire dataset is defined to be qmin(θ) =
mi□n∈[N] qn(θ). We define the normalized margin to be γ(θ) := qmin(θ) = qmin(θ)∕ρL, where
ρ := ∣∣θk2 and θ:= θ∕ρ ∈ Sd-1 as usual.
Let '(q) := log(1+e-q) be the logistic loss. Recall that '(q) satisfies (B3). Let f(q) = 一 log '(q)=
- log log(1 + e-q). Let g be the inverse function of f. So g(q) = - log(ee-q -1).
The cross-entropy loss can be rewritten in other ways. Let snj := Φyn (θ; xn) 一 Φj (θ; xn). Let
7n := -LSE({-Snj : j = yn}) = - log (Pj=yn e-snj ). Then
N	/	∖ N	N	N
L(θ) :=	X log	(1+	X e-snj )	= X log(1 +	e-0n )	= X	'(7n)	= X e-f*
n=1	j 6=yn	n=1	n=1	n=1
Gradient Flow. For gradient flow, we assume the following:
(M1). (Regularity). For any fixed X and j ∈ [C], Φj (∙; x) is locally Lipschitz and admits a chain
rule;
40
Published as a conference paper at ICLR 2020
(M2). (Homogeneity). There exists L > 0 such that ∀j ∈ [C], ∀α > 0 : Φj (αθ; x)
αLΦj (θ; x);
(M3). (Cross-entropy Loss). L(θ) is defined as the cross-entropy loss on the training set;
(M4). (Separability). There exists a time t0 such that L(t0) < log 2.
If L < log 2, then	j6=y e-snj < 1 for all n ∈ [N], and thus snj > 0 for all n ∈ [N], j ∈ [C]. So
(M4) ensures the separability of training data.
Definition G.1. For cross-entropy loss, the smoothed normalized margin 7(θ) of θ is defined as
- log(eL - 1)
γ(θ):
'-1(L)
PL
ρL
where '-1(∙) is the inverse function of the logistic loss '(∙).
Theorem 4.1 and 4.4 still hold. Here we redefine the optimization problem (P) to be
mm 2 kθk2
s.t. snj(θ) ≥ 1	∀n∈ [N],∀j ∈ [C]\{yn}
Most of our proofs are very similar as before. Here we only show the proof for the generalized
version of Lemma 5.1.
Lemma G.2. Lemma 5.1 is also true for the smoothed normalized margin 7 defined in Defini-
tion G.1.
Proof. Define ν(t) by the following formula:
“S pP≡jsj
Using a similar argument as in Theorem B.4, it can be proved that 2 dpt2 = LV(t) for a.e. t > 0.
It can be shown that Lemma B.5, which asserts that ν(t) ≥ g(og LI) L, still holds for this new
g0(log L)
definition of V(t). By definition, Snj ≥ qn for j = yn. Also note that e-qn ≥ e-qn. So Snj ≥ qn ≥
q7n. Then we have
N P	e — snj	N -q	N
V(t) ≥ X Ij	-S	∙ 7n = X	∙ 7n = X e-f (qn)f 0(7n)7n.
1 +	j6= e-snj	1 + e-qn
n=1	j6=yn	n=1	n=1
Note that L = PnN=1 e-f(qqn). Then using Lemma B.5 for logistic loss can conclude that V(t) ≥
g(log L) L
g0(log L) L.
The rest of the proof for this lemma is exactly the same as that for Lemma 5.1.	□
Gradient Descent. For gradient descent, we only need to replace (M1) with (S1) and make the
assumption (S5) on the learning rate.
(S1). (Smoothness). For any fixed X and j ∈ [C], Φj(∙; x) is C2-smooth;
(S5). (Learing rate condition). Pt≥0 η(t) = +∞ and η(t) ≤ H(L(θ(t))).
Here H(x) := μ(χ)/(Cκ(x)) is defined to be the same as in Appendix F (when '(∙) is set to
logistic loss) except that we use another Cη which will be specified later.
We only need to show that Lemma F.2 and Lemma F.3 continue to hold. Using the same argument
as we do for gradient flow, we can show thatγ(t) and λ(x) do satisfy the propositions in Lemma F.2.
41
Published as a conference paper at ICLR 2020
For Lemma F.3, we first note the original definition of Cη involves B0, B1, B2, which are undefined
in the multi-class setting. So now we redefine them as
Bo ：= sup {snj : θ ∈ Sd-1,n ∈ [N],j ∈ [C]},
Bi := SUp {kVsnj∣∣2 : θ ∈S d-1,n ∈ [N ],j ∈ [C]},
B2 ：= sup {∣∣V2Snj∣∣2 : θ ∈S d-1,n ∈ [N ],j ∈ [C]}.
By the property of LSE, We can use Bo, Bi, B2 to bound qn, Vqr, V2qr.
qn ≤ qn ≤ BOP ,
kVqnk2
∣∣v2 词 2
C
X
j=i
e-snj
K~^∑77 Vsnj
k=i e-snk
C
≤X
j=i
e-snj
PC=I e-snk
kVsrjk2 ≤ BiρL-i
C
X
j=i
e-snj
∖PC=1 e-snk
(V2srj - Vsrj Vsr>j ) +
e-2snj
PkC=i e-snk
2 VsrjVsrj
/
2
2
≤ B2ρL-2 + 2Bi2ρ2L-2 .
Using these bounds, We can prove With the constant Cη defined as folloWs:
Cη :
1
2
Bo
γ(to)
+
Bi2 + (2 log 2)
Bo ∖
γ(to)J
p
M,
where M := min {γ(to)-2+2∕L, B-2+2/L}.
H Extension: Multi-homogeneous Models
In this section, we extend our results to multi-homogeneous models. For this, the main difference
from the proof for homogeneous models is that now we have to separate the norm of each homo-
geneous parts of the parameter, rather than consider them as a whole. So only a small part to proof
needs to be changed. We focus on gradient flow, but it is worth to note that following the same
argument, it is not hard to extend the results to gradient descent.
Let Φ(wι,..., Wm； x) be (kι,...,km)-homogeneous. Let Pi = Ilwik2 and Wi = 口蓝.1.The
smoothed normalized margin defined in (5) can be rewritten as follows:
Definition H.1. For a multi-homogeneous model with loss function '(∙) satisfying (B3), the
smoothed normalized margin γ(θ) of θ is defined as
g(log L1) = '-1(L)
Qi=i Pk = Qm=I Pk.
We only prove the generalized version of Lemma 5.1 here. The other proofs are almost the same.
Lemma H.2. For all t > to,
d
dtlog Pi > 0
d	md	-
for all i ∈ [m] and — log T ≥ 2_^ki — log Pi
dt	dt
i=i
dwi 2
dt ∣
(24)
Proof. Note that K log Pi = 2p2 dpL = kiV(t) by Theorem B.4. It simply follows from Lemma B.5
that ddt log P > 0 for a.e. t > to. And it is easy to see that log Y = log (g(log LL)∕pl) exists for all
42
Published as a conference paper at ICLR 2020
t ≥ t0. By the chain rule and Lemma B.5, we have
ddt logA = ddt (log (g(log I))- XX ki logP)
g0(lοg L1) 1 d dC∖ S k2ν(t)
g(log L1) ∙LΛ-- i=1 ^7T
≥ ɪ , (_ dL) _XX 包的
—V(t)	V dt)	± PP
≥ ɪ •(_ dL _XX k2ν (t)2'
一V(t) ∖ dt ⅛ Pi ,
On the one hand,-箸=Pm=Ill dWi∣∣2 for a.e. t > 0 by Lemma I.3; on the other hand, kiν(t)
(wi, dwti) by Theorem B.4. Combining these together yields
d	1m
dt logY ≥ 西 E
i=1
2
2
By the chain rule,瞥=ρ^(I - WiWi>)赞 for a.e. t > 0. So We have
V log γ ≥
dt
2
2
□
For cross-entropy loss, We can combine the proofs in Appendix G to shoW that Lemma H.2 holds if
We use the folloWing definition of the smoothed normalized margin:
Definition H.3. For a multi-homogeneous model With cross-entropy, the smoothed normalized mar-
gin γA(θ) of θ is defined as
〜='-1(L) = -log(eL - 1)
Y = Q 乙 PF =
where '-1(∙) is the inverse function of the logistic loss '(∙).
The only place We need to change in the proof for Lemma H.2 is that instead of using Lemma B.5,
we need to prove V(t) ≥ ：0黑工)L in a similar way as in Lemma G.2. The other parts of the proof
are exactly the same as before.
I Chain Rules for Non-differentiable Functions
In this section, we provide some background on the chain rule for non-differentiable functions. The
ordinary chain rule for differentiable functions is a very useful formula for computing derivatives
in calculus. However, for non-differentiable functions, it is difficult to find a natural definition of
subdifferential so that the chain rule equation holds exactly. To solve this issue, Clarke proposed
Clarke’s subdifferential (Clarke, 1975; 1990; Clarke et al., 2008) for locally Lipschitz functions, for
which the chain rule holds as an inclusion rather than an equation:
Theorem I.1 (Theorem 2.3.9 and 2.3.10 of (Clarke, 1990)). Let z1, . . . , zn : Rd → R and f :
Rn → R be locally Lipschitz functions. Let (f ◦ z)(x) = f (z1 (x), . . . , zn(x)) be the composition
of f and z. Then,
)
∂°(f ◦ z)(x) ⊆ Conv
aihi : α ∈ ∂ ◦f (zι(x),...,zn(x)), h ∈ ∂°Zi(x)
For analyzing gradient flow, the chain rule is crucial. For a differentiable loss function L(θ), we
can see from the chain rule that the function value keeps decreasing along the gradient flow dθtt)
-VL(θ(t)):
丁 = <VL(θ(t)), T
dt	dt
dθ(t)
dt
(25)
—
43
Published as a conference paper at ICLR 2020
But for locally Lipschitz functions which could be non-differentiable, (25) may not hold in general
since Theorem I.1 only holds for an inclusion.
Following (Davis et al., 2020; Drusvyatskiy et al., 2015), we consider the functions that admit a
chain rule for any arc.
Definition I.2 (Chain Rule). A locally Lipschitz function f : Rd → R admits a chain rule if for any
arc z : [0, +∞) → Rd,
∀h ∈ ∂◦f (z(t)) : (f ◦ z)0(t) = hh, z0(t)i	(26)
holds for a.e. t > 0.
It is shown in (Davis et al., 2020; Drusvyatskiy et al., 2015) that a generalized version of (25) holds
for such functions:
Lemma I.3 (Lemma 5.2 (Davis et al., 2020)). Let L : Rd → R be a locally Lipschitz function that
admits a chain rule. Let θ : [0, +∞) → Rd be the gradient flow on L:
d"(" ∈ —∂°L(θ(t))	fora.e. t ≥ 0,
dt
Then
dL(θ≡ = - I=-min{khk2 ： h ∈ "(θ(t))}
holds for a.e. t > 0.
We can see that C1-smooth functions admit chain rules. As shown in (Davis et al., 2020), if a
locally Lipschitz function is subdifferentiablly regular or Whitney C1-stratifiable, then it admits a
chain rule. The latter one includes a large family of functions, e.g., semi-algebraic functions, semi-
analytic functions, and definable functions in an o-minimal structure (Coste, 2002; van den Dries &
Miller, 1996).
It is worth noting that the class of functions that admits chain rules is closed under composition.
This is indeed a simple corollary of Theorem I.1.
Theorem I.4. Let z1 , . . . , zn : Rd → R and f : Rn → R be locally Lipschitz functions and assume
all of them admit chain rules. Let (f ◦ z)(x) = f(z1 (x), . . . , zn(x)) be the composition off and
z. Then f ◦ z also admits a chain rule.
Proof. We can see that f ◦ z is locally Lipschitz. Let x : [0, +∞) → Rd, t 7→ x(t) be an arc.
First, we show that z ◦ x : [0, +∞) → Rd, t 7→ z(x(t)) is also an arc. For any closed sub-interval
I, z(x(I)) must be contained in a compact set U. Then it can be shown that the locally Lipschitz
continuous function z is (globally) Lipschitz continuous on U. By the fact that the composition
of a Lipschitz continuous and an absolutely continuous function is absolutely continuous, z ◦ x is
absolutely continuous on I, and thus it is an arc.
Since f and z admit chain rules on arcs z ◦ x and x respectively, the following holds for a.e. t > 0,
∀α ∈ ∂°f (z(x(t))): (f ◦ (Z ◦ x))0(t) = hα, (z ◦ x)0(t)i,
∀hi ∈ ∂°zi(x(t)): (Zi ◦ x)0(t) = hhi, x0(t)i .
Combining these we obtain that for a.e. t > 0,
n
(f ◦ Z ◦ x)0(t) = X αi hhi, x0(t)i ,
i=1
for all α ∈ ∂of(z(x(t))) and for all h ∈ ∂oZi(x(t)). The RHS can be rewritten as
hpn=ι αihi, x0(t)i. By Theorem I.1, every k ∈ ∂◦(f ◦ z)(x(t)) can be written as a convex Combi-
nation of a finite set of points in the form of Pin=1 αihi. So (f ◦ Z ◦ x)0(t) = hk, x0(t)i holds for
a.e. t > 0.	□
44
Published as a conference paper at ICLR 2020
J Mexican Hat
In this section, we give an example to illustrate that gradient flow does not necessarily converge in
direction, even for C ∞ -smooth homogeneous models.
It is known that gradient flow (or gradient descent) may not converge to any point even when opti-
mizing an C∞ function (Curry, 1944; Zoutendijk, 1976; Palis & De Melo, 2012; Absil et al., 2005).
One famous counterexample is the “Mexican Hat” function described in (Absil et al., 2005):
f (u, V) := f (r cos φ, r Sin φ):
(e0
1-r2(1 — C(r) sin (夕一ι⅛
r<1
r≥1
where C(r)=4。+;;二尸 ∈ [0,1]. It Can be shown that f is C∞-smooth on R2 but not analytic.
See Figure 2 for a plot for f(u, v).
Figure 2: A plot for the Mexican Hat function f(u, v).
However, the Maxican Hat function is not homogeneous, and Absil et al. (2005) did not consider
the directional convergence, either. To make it homogeneous, we introduce an extra variable z , and
normalize the parameter before evaluate f . In particular, we fix L > 0 and define
h(θ) = h(x,y,z)= ρL(1 — f (u,v)) where U = x∕ρ, V = y∕ρ, P =，x2 + y2 + z2.
We can show the following theorem.
Theorem J.1. Consider gradient flow on L(θ) = PnN=1 e-qn(θ), where qn(θ) = h(θ) for all
n ∈ [N]. Suppose the polar representation of (u,v) is (r cos φ, r sin φ). If 0 < r < 1 and
φ = 1-lr2 holds at time t = 0, then “噩上 does not converge to any point, and the limit points of
{ kθθf : t > 0} form a circle {(x, y,z) ∈ S2 : x2 + y2 = 1,z = 0}.
Proof. Define ψ = φ — i-^. Our proof consists of two parts, following from the idea in (Absil
et al., 2005). First, we show that 爷=0 as long as ψ = 0. Then we can infer that ψ = 0 for all
t ≥ 0. Next, we show that r → 1 as t → +∞. Using ψ = 0, we know that the polar angle φ → +∞
as t → +∞. Therefore, (u, V) circles around {(u, V) : u2 + V2 = 1}, and thus it does not converge.
Proof for 爷=0. For convenience, we use W to denote z∕ρ. By simple calculation, we have the
following formulas for partial derivatives:
∂x = PLT(Lu(1 —f) — (1 — u2) ∂f + UV∂V)
∂ = PLT (Lv(1 - f)+ uv∂U — (1 — v2)∂V)
∂h = PLT (Lw(I -f) + Uw ∂U+ Vw ∂V)
(ff =	U	∙	∂f	-
∂u	r	∂r
∂f =	V	.	∂f	+
∂v	r	∂r	+
V ∂f
r2 ∙ ∂φ
u ∂f
r2	∂中
45
Published as a conference paper at ICLR 2020
For gradient flow, we have
dθ
dt
du
Ne-hVh
dt
P ((1-u2
dx
dt
dy dz
—Uv凉一Uw加
Ne-hρL-2 -(1 - U2
+ uv
dv
dt
P ((i2
dx	dz
dt	dt
Ne-hρL-2 -(1 - v2
+ uv
By writing down the movement of (u, v) in the polar coordinate system, we have
dr
u du
v dv
dt
r dt + r dt
N e-h ρL-2
U(1 - r2
+ v(1 - r2
—
r
-Ne-hρL-2(1 - r2
d夕
dt
v du
u dv
r2 dt + r2	dt
N e-h ρL-2
r2
∂f
v^——U
∂U
=-Nf r1 ∙ ∂f
For ψ = 0, the partial derivatives of f with respect to r and 夕 can be evaluated as follows:
∂f
∂r
—(e-1⅛ ) — e
dr
1 C0∕ ∖	,	____∖	, ∂ψ
1-r2 C0(r)sιn ψ — e 1-r2 C(r) cos ψ ∙——
∂r
dr
e-1⅛ ) + e-1⅛ C (r) -d
1 - r2
2r
(1 - r2)2
1
(1 - C(r))e- 1-r2
d
—
—

1
∂f
∂夕
__」一、	, ∂ψ
—e 1-r2 C(r) cos ψ ∙ -^―
_	1
—e 1-r2 C(r).
Soif ψ = 0, then d
= 0 by the direct calculation below:
dψ d夕 d
—=———
dt
dt
dr 1 - r2
Ne-hρL-2
dr
2r
dt
Ne-hρL-2e
1 ∂f +
(1 - r2)2
4r2
十 (1 - r2)4
C(r) -
4r2
(1 - r2)4
0.
1
Proof for r → 1. Let (U, v) be a convergent point of {(u, v) : t ≥ 0}. Define r = √u2 + v2. It
is easy to see that r ≤ 1 from the normalization of θ in the definition. According to Theorem 4.4,
We know that (u,v) is a stationary point of 租u(t),v(t)) = 1 — f (u(t),v(t)). If f = 0, then
f(U,v) > f (u(0), v(0)), which contradicts to the monotonicity of 7(t) = γ(t) = 1 — f (u(t),v(t)).
If r < 1, then ψ = 0 (defined as ψ for (U, v)). So ∂f =—.工产(1 一 C(r))e- 1-r2 = 0, which
again leads to a contradiction. Therefore, f= 1, and thus r → 1.	□
K Experiments
To validate our theoretical results, we conduct several experiments. We mainly focus on MNIST
dataset. We trained two models with Tensorflow. The first one (called the CNN with bias) is a stan-
dard 4-layer CNN with exactly the same architecture as that used in MNIST Adversarial Examples
46
Published as a conference paper at ICLR 2020
Challenge5. The layers of this model can be described as conv-32 with filter size 5 × 5, max-pool,
conv-64 with filter size 3 × 3, max-pool, fc-1024, fc-10 in order. Notice that this model has
bias terms in each layer, and thus does not satisfy homogeneity. To make its outputs homogeneous
to its parameters, we also trained this model after removing all the bias terms except those in the
first layer (the modified model is called the CNN without bias). Note that keeping the bias terms in
the first layer prevents the model to be homogeneous in the input data while retains the homogeneity
in parameters. We initialize all layer weights by He normal initializer (He et al., 2015) and all bias
terms by zero. In training the models, we use SGD with batch size 100 without momentum. We
normalize all the images to [0, 1]32×32 by dividing 255 for each pixel.
K. 1 Evaluation for Normalized Margin
In the first part of our experiments, we evaluate the normalized margin every few epochs to see how
it changes over time. From now on, we view the bias term in the first layer as a part of the weight
in the first layer for convenience. Observe that the CNN without bias is multi-homogeneous in layer
weights (see (4) in Section 4.4). So for the CNN without bias, we define the normalized margin
Y as the margin divided by the product of the L2-norm of all layer weights. Here We compute the
L2-norm ofa layer weight parameter after flattening it into a one-dimensional vector. For the CNN
with bias, we still compute the smoothed normalized margin in this way. When computing the L2-
norm of every layer weight, we simply ignore the bias terms if they are not in the first layer. For
completeness, we include the plots for the normalized margin using the original definition (2) in
Figure 3 and 4.
----Ir=O.01, w/ bias	Ir=O.01, w/o bias
#epochs
ptuz=roE.lou
0.40 × Io-5
0.20 × Io-5
0.00 × 10~5
-0.20 × IOT
-0.40 × IOT
l∂0 IO1 IO2 io3
#epochs
IO4
Figure 3: Training CNNs with and without bias on MNIST, using SGD with learning rate 0.01. The
training accuracy (left) increases to 100% after about 100 epochs, and the normalized margin with
the original definition (right) keeps increasing after the model is fitted.
>UE3uuπ 6u≡st
Figure 4: Training CNNs with and without bias on MNIST, using SGD with the loss-based learning
rate scheduler. The training accuracy (left) increases to 100% after about 20 epochs, and the nor-
malized margin with the original definition (middle) increases rapidly after the model is fitted. The
right figure shows the change of the relative learning rate α(t) (see (27) for its definition) during
training.
5https://github.com/MadryLab/mnist_challenge
47
Published as a conference paper at ICLR 2020
SGD with Constant Learning Rate. We first train the CNNs using SGD with constant learning
rate 0.01. After about 100 epochs, both CNNs have fitted the training set. After that, we can see that
the normalized margins of both CNNs increase. However, the growth rate of the normalized margin
is rather slow. The results are shown in Figure 1 in Section 1. We also tried other learning rates
other than 0.01, and similar phenomena can be observed.
SGD with Loss-based Learning Rate. Indeed, we can speed up the training by using a proper
scheduling of learning rates for SGD. We propose a heuristic learning rate scheduling method, called
the loss-based learning rate scheduling. The basic idea is to find the maximum possible learning
rate at each epoch based on the current training loss (in a similar way as the line search method).
See Appendix L.1 for the details. As shown in Figure 1, SGD with loss-based learning rate schedul-
ing decreases the training loss exponentially faster than SGD with constant learning rate. Also, a
rapid growth of normalized margin is observed for both CNNs. Note that with this scheduling the
training loss can be as small as 10-800, which may lead to numerical issues. To address such is-
sues, we applied some re-parameterization tricks and numerical tricks in our implementation. See
Appendix L.2 for the details.
Experiments on CIFAR-10. To verify whether the normalized margin is increasing in practice, we
also conduct experiments on CIFAR-10. We use a modified version of VGGNet-16. The layers of
this model can be described as conv-64 ×2, max-pool, conv-128 ×2, max-pool, conv-256
×3, max-pool, conv-512 ×3, max-pool, conv-512 ×3, max-pool, fc-10 in order, where
each conv has filter size 3 × 3. We train two networks: one is exactly the same as the VGGNet
we described, and the other one is the VGGNet without any bias terms except those in the first layer
(similar as in the experiments on MNIST). The experiment results are shown in Figure 5 and 6. We
can see that the normalize margin is increasing over time.
---Ir=O.1, w/ bias Ir=O. 1, w/o bias
IO0 IO1 IO2 IO3 IO4
#epochs
Figure 5: Training VGGNet with and without bias on CIFAR-10, using SGD with learning rate 0.1.
SSo- 6lπlπroh
loss-based Ir, w/o bias
Figure 6: Training VGGNet with and without bias on CIFAR-10, using SGD with the loss-based
learning rate scheduler.
Test Accuracy. Previous works on margin-based generalization bounds (Neyshabur et al., 2018;
Bartlett et al., 2017; Golowich et al., 2018; Li et al., 2018a; Wei et al., 2019; Banburski et al.,
48
Published as a conference paper at ICLR 2020
2019) usually suggest that a larger margin implies a better generalization bound. To see whether the
generalization error also gets smaller in practice, we plot train and test accuracy for both MNIST and
CIFAR-10. As shown in Figure 7, the test accuracy changes only slightly after training with loss-
based learning rate scheduling for 10000 epochs, although the normalized margin does increase a
lot. We leave it as a future work to study this interesting gap between margin-based generalization
bound and generalization error. Concurrent to this work, Wei & Ma (2020) proposed a generalization
bound based on a new notion of margin called all-layer margin, and showed via experiments that
enlarging all-layer margin can indeed improve generalization. It would be an interesting research
direction to study how different definitions of margin may lead to different generalization abilities.
Figure 7: (Left). Training and test accuracy during training CNNs without bias on MNIST, using
SGD with the loss-based learning rate scheduler. Every number is averaged over 10 runs. (Right).
Training and test accuracy during training VGGNet without bias on CIFAR-10, using SGD with the
loss-based learning rate scheduler. Every number is averaged over 3 runs.
K.2 Evaluation for Robustness
Recently, robustness of deep learning has received considerable attention (Szegedy et al., 2013; Big-
gio et al., 2013; Athalye et al., 2018), since most state-of-the-arts deep neural networks are found
to be very vulnerable against small but adversarial perturbations of the input points. In our experi-
ments, we found that enlarging the normalized margin can improve the robustness. In particular, by
simply training the neural network for a longer time with our loss-based learning rate, we observe
noticeable improvements of L2-robustness on both the training set and test set.
We first elaborate the relationship between the normalized margin and the robustness from a theo-
retical perspective. For a data point z = (x, y), we can define the robustness (with respect to some
norm ∣∣ ∙ ∣∣) of a neural network Φ for Z to be
Rθ (z) := inf {kx - x0 k : (x0, y) is misclassified} .
x0∈X
where X is the data domain (which is [0, 1]32×32 for MNIST). It is well-known that the normalized
margin is a lower bound of L2-robustness for fully-connected networks (See, e.g., Theorem 4 in
(Sokolic et al., 2017)). Indeed, a general relationship between those two quantities can be easily
shown. Note that a data point z is correctly classified iff the margin for z, denoted as qθ(z), is
larger than 0. For homogeneous models, the margin qe(Z) and the normalized margin q^(z) for
X have the same sign. If qf^(∙) : Rdx → R is β-Lipschitz (with respect to some norm ∣∣ ∙ ∣∣), then
it is easy to see that R(z) ≥ q^(z)∕β. This suggests that improving the normalize margin on
the training set can improve the robustness on the training set. Therefore, our theoretical analysis
suggests that training longer can improve the robustness of the model on the training set.
This observation does match with our experiment results. In the experiments, we measure the L2 -
robustness of the CNN without bias for the first time its loss decreases below 10-10, 10-15, 10-20,
10-120 (labelled as model-1 to model-4 respectively). We also measure the L2-robustness for
the final model after training for 10000 epochs (labelled as model-5), whose training loss is about
10-882 . The normalized margin of each model is monotone increasing with respect to the number
of epochs, as shown in Table 1.
49
Published as a conference paper at ICLR 2020
model-1	model-2 — model-3 ------------ model-4	model-5
Figure 8: L2-robustness of the models of CNNs without bias trained for different number of epochs
(see Table 1 for the statistics of each model). Figures on the first row show the robust accuracy on the
training set, and figures on the second row show that on the test set. On every row, the left figure and
the right figure plot the same curves but they are in different scales. From model-1 to model-4,
noticeable robust accuracy improvements can be observed. The improvement of model-5 upon
model-4 is marginal or nonexistent for some , but the improvement upon model-1 is always
significant.
Table 1: Statistics of the CNN without bias after training for different number of epochs.
model name ∣ number of epochs		train loss	normalized margin	train acc	test acc
model-1	38	^^10-10.04-	5.65 × 10-5	100%	99.3%
model-2	75	10-15.12	9.50 × 10-5	100%	99.3%
model-3	107	10-20.07	1.30 × 10-4	100%	99.3%
model-4	935	10-120.01	4.61 × 10-4	100%	99.2%
model-5	10000	10-881.51	1.18 × 10-3	100%	99.1%
We use the standard method for evaluating L2-robustness in (Carlini & Wagner, 2017) and the source
code from the authors with default hyperparameters6. We plot the robust accuracy (the percentage
of data with robustness > ) for the training set in the figures on the first row of Figure 8. It can
be seen from the figures that for small (e.g.,	< 0.3), the relative order of robust accuracy is
just the order of model-1 to model-5. For relatively large (e.g., > 0.3), the improvement of
model-5 upon model-2 to model-4 becomes marginal or nonexistent in certain intervals of ,
but model-1 to model-4 still have an increasing order of robust accuracy and the improvement
of model-5 upon model-1 is always significant. This shows that training longer can help to
improve the L2-robust accuracy on the training set.
We also evaluate the robustness on the test set, in which a misclassified test sample is considered
to have robustness 0, and plot the robust accuracy in the figures on the second row of Figure 8. It
can be seen from the figures that for small (e.g.,	< 0.2), the curves of the robust accuracy of
model-1 to model-5 are almost indistinguishable. However, for relatively large (e.g., > 0.2),
again, model-1 to model-4 have an increasing order of robust accuracy and the improvement of
6https://github.com/carlini/nn_robust_attacks
50
Published as a conference paper at ICLR 2020
model-5 upon model-1 is always significant. This shows that training longer can also help to
improve the L2-robust accuracy on the test set.
We tried various different settings of hyperparameters for the evaluation method (including different
learning rates, different binary search steps, etc.) and we observed that the shapes and relative
positions of the curves in Figure 8 are stable across different hyperparameter settings.
It is worth to note that the normalized margin and robustness do not grow in the same speed in our
experiments, although the theory suggests Rθ(Z) ≥ q^(z)∕β. This may be because the LiPschitz
constant β (if defined locally) is also changing during training. Combining training longer with
existing techniques for constraining Lipschitz number (Anil et al., 2019; Cisse et al., 2017) could
potentially alleviate this issue, and we leave it as a future work.
L	Additional Experimental Details
In this section, we provide additional details of our experiments.
L.1 Loss-based Learning Rate Scheduling
The intuition of the loss-based learning rate scheduling is as follows. If the training loss is α-
smooth, then optimization theory suggests that We should set the learning rate to roughly 1∕α. For
a homogeneous model with cross-entropy loss, if the training accuracy is 100% at θ, then a simple
calculation can show that the smoothness (the L2-norm of the Hessian matrix) at θ is O(C ∙ Poly(P)),
where L is the average training loss and Poly(P) is some polynomial. Motivated by this fact, we
parameterize the learning rate η(t) at epoch t as
α(t)
η(t) := L(t-Iy,	(27)
where L(t - 1) is the average training loss at epoch t - 1, and α(t) is a relative learning rate to
be tuned (Similar parameterization has been considiered in (Nacson et al., 2019b) for linear model).
The loss-based learning rate scheduling is indeed a variant of line search. In particular, we initialize
α(0) by some value, and do the following at each epoch t:
Step 1. Initially a(t) J α(t - 1); Let L(t - 1) be the training loss at the end of the last epoch;
Step 2. Run SGD through the whole training set with learning rate η(t) := α(t)/L(t - 1);
Step 3. Evaluate the training loss L(t) on the whole training set;
Step 4. If L(t) < L(t - 1), α(t) J α(t) ∙ r〃 and end this epoch; otherwise, α(t) J α(t)∕rd and
go to Step 2.
In all our experiments, we set α(0) := 0.1, ru := 21/5 ≈ 1.149, rd := 21/10 ≈ 1.072. This specific
choice of those hyperparameters is not important; other choices can only affact the computational
efficiency, but not the overall tendency of normalized margin.
L.2 Addressing Numerical Issues
Since we are dealing with extremely small loss (as small as 10-800), the current Tensorflow imple-
mentation would run into numerical issues. To address the issues, we work as follows. Let LB (θ)
be the (average) training loss within a batch B ⊆ [N]. We use the notations C, snj, qn, qn from
Appendix G. We only need to show how to perform forward and backward passes for LB (θ).
Forward Pass. Suppose we have a good estimate F for log LB (θ) in the sense that
RB (θ) := LB (θ)e-F = B X log I 1+ X e-snj ⑻)e-F	(28)
n∈B	j6=yn
is in the range of float64. RB(θ) can be thought of a relative training loss with respect to F.
Instead of evaluating the training loss LB (θ) directly, we turn to evaluate this relative training loss
in a numerically stable way:
51
Published as a conference paper at ICLR 2020
Step 1. Perform forward pass to compute the values of snj with float32, and convert them into
float64;
Step 2. Let Q := 30. If qn(θ) > Q for all n ∈ B, then we compute
1 ____	,_ , _,	~,
RB (θ) = B X e-(qn⑻+F),
n∈B
where ⅞n(θ) ：= -LSE({-Snj : j = yn}) = - log (Pj=yn e-snj) is evaluated in a
numerically stable way; otherwise, we compute
Rb (θ) = B X log1p( X e-n叫 e-F,
n∈B	j 6=yn
where log1p(x) is a numerical stable implementation of log(1 + x).
This algorithm can be explained as follows. Step 1 is numerically stable because we observe from
the experiments that the layer weights and layer outputs grow slowly. Now we consider Step 2. If
qn(θ) ≤ Q for some n ∈ [B], then LB(θ) = Ω(e-Q) is in the range of float64, so We can
compute RB (θ) by (28) directly except that we need to use a numerical stable implementation of
log(1 + x). For qn(θ) > Q, arithmetic underflow can occur. By Taylor expansion of log(1 + x), we
know that when X is small enough log(1 + x) ≈ X in the sense that the relative error llogg(++X-x| =
O(x). Thus, we can do the following approximation
log 1 + X e-snj ⑻ e-F ≈ X e-snj ⑻∙ e-F	(29)
j 6=yn	j6=yn
for qn (θ) > Q, and only introduce a relative error of O(Ce-Q) (recall that C is the number of
classes). Using a numerical stable implementation of LSE, we can compute Gn easily. Then the
ee
RHS of (29) can be rewritten as e (qn(θ)+F). Note that computing e (qn(θ)+F) does not have
underflow or overflow problems if F is a good approximation for log LB (θ).
Backward Pass. To perform backward pass, we build a computation graph in Tensorflow for the
above forward pass for the relative training loss and use the automatic differentiation. We parame-
terize the learning rate as η = η ∙ eF. Then it is easy to see that taking a step of gradient descent for
LB (θ) with learning rate η is equivalent to taking a step for RB (θ) with ^ Thus, as long as η can
fit into float64, we can perform gradient descent on RB (θ) to ensure numerical stability.
The Choice of Fe. The only question remains is how to choose Fe. In our experiments, we set
F(t) := log L(t - 1) to be the training loss at the end of the last epoch, since the training loss
cannot change a lot within one single epoch. For this, we need to maintain log L(t) during training.
This can be done as follows: after evaluating the relative training loss R(t) on the whole training
set, we can obtain log L(t) by adding F(t) and log R(t) together.
It is worth noting that with this choice of F, η(t) = α(t) in the loss-based learning rate scheduling.
As shown in the right figure of Figure 4, α(t) is always between 10-9 and 100, which ensures the
numerical stability of backward pass.
52