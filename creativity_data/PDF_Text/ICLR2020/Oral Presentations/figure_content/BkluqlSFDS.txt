Figure 1: Comparison among various federated learning methods with limited number of communications onLeNet trained on MNIST; VGG-9 trained on CIFAR-10 dataset; LSTM trained on Shakespeare dataset over:(a) homogeneous data partition (b) heterogeneous data partition.
Figure 2: Convergence rates of various methods in two federated learning scenarios: training VGG-9 onCIFAR-10 with J = 16 clients and training LSTM on Shakespeare dataset with J = 66 clients.
Figure 3: The effect of number of localtraining epochs on various methods.
Figure 4: Performance on skewedCIFAR-10 dataset.
Figure 5: Representations generated by the first convolution layers of locally trained models, FedMA globalmodel and the FedAvg global model.
Figure 6: Data effiCienCy under the in-Creasing number of Clients.
