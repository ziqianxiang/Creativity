Figure 1: Overview of architecturesto a shared queue or replay buffer. Asynchronously, the learner reads batches of trajectories fromthe queue/replay buffer and optimizes the model.
Figure 2: Variants of “near on-policy” when evaluating a policy π while asynchronously optimizingmodel parameters θ.
Figure 3: Detailed Learner architecture in SEED (with an optional replay buffer).
Figure 4: Comparison of IMPALA and SEED under the exact same conditions (175 actors, samehyperparameters, etc.) The plots show hyperparameter combinations sorted by the final performanceacross different hyperparameter combinations.
Figure 5: Training on 4 DeepMind Lab tasks. Each curve is the best of the 24 runs based on finalreturn following the evaluation procedure in Espeholt et al. (2018). Sample complexity is maintainedup to 8 TPU v3 cores, which leads to 11x faster training than the IMPALA baseline. Top Row: X-axis is per frame (number of frames = 4x number of steps). Bottom Row: X-axis is hours.
Figure 6: Median human-normalized score on Atari-57 for SEED and related agents. SEED wasrun with 1 seed for each game. All agents use up to 30 random no-ops for evaluation. Left: X-axisis hours Right: X-axis is environment frames (a frame is 1/4th of an environment step due to actionrepeat). SEED reaches state of the art performance 3.1x faster (wall-time) than R2D2.
Figure 7: Learning curves on 57 Atari 2600 games for SEED (8 TPUv3 cores, 610 actors, evaluatedwith 1 seed). Each point of each curve averages returns over 200 episodes. No curve smoothing wasperformed. Curves end at approximately 43 hours of training, corresponding to 40e9 environmentframes.
