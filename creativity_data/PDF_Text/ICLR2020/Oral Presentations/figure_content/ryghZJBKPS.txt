Figure 1: Left and center: Learning curves for k-MEANS++ and k-DPP sampling with gradient embeddingsfor different scenarios. The performance of the two sampling approaches nearly perfectly overlaps. Right:A run time comparison (seconds) corresponding to the middle scenario. Each line is the average over fiveindependent experiments. Standard errors are shown by shaded regions.
Figure 2: A comparison of batch selection algorithms using our gradient embedding. Left and center: Plotsshowing the log determinant of the Gram matrix of the selected batch of gradient embeddings as learningprogresses. Right: The average embedding magnitude (a measurement of predictive uncertainty) in theselected batch. The FF-k-CENTER sampler finds points that are not as diverse or high-magnitude as othersamplers. Notice also that k-MEANS++ tends to actually select samples that are both more diverse andhigher-magnitude than a k-DPP, a potential pathology of the k-DPP’s degree of stochastisity. Standard errorsare shown by shaded regions.
Figure 3: Active learning test accuracy versus the number of total labeled samples for a range of conditions.
Figure 4: A pairwise penalty matrix over all experiments.
Figure 5: The cumulative distribution function ofnormalized errors for all acquisition functions.
Figure 6: Full learning curves for OpenML #6 with MLP.
Figure 7: Full learning curves for OpenML #155 with MLP.
Figure 8: Full learning curves for OpenML #156 with MLP.
Figure 9: Full learning curves for OpenML #184 with MLP.
Figure 10: Full learning curves for SVHN with MLP, ResNet and VGG.
Figure 11: Full learning curves for MNIST with MLP.
Figure 12: Full learning curves for CIFAR10 with MLP, ResNet and VGG.
Figure 13:	Zoomed-in learning curves for OpenML #6 with MLP.
Figure 14:	Zoomed-in learning curves for OpenML #155 with MLP.
Figure 15:	Zoomed-in learning curves for OpenML #156 with MLP.
Figure 16:	Zoomed-in learning curves for OpenML #184 with MLP.
Figure 17: Zoomed-in learning curves for SVHN with MLP, ResNet and VGG.
Figure 18:	Zoomed-in learning curves for MNIST with MLP.
Figure 19:	Zoomed-in learning curves for CIFAR10 with MLP, ResNet and VGG.
Figure 20:	Pairwise penalty matrices of the algorithms, grouped by different batch sizes. The parenthesizednumber in the title is the total number of (D, B, A) combinations aggregated, which is also an upper boundon all its entries. Element (i, j) corresponds roughly to the number of times algorithm i beats algorithm j.
Figure 21:	Pairwise penalty matrices of the algorithms, grouped by different neural network models. Theparenthesized number in the title is the total number of (D, B, A) combinations aggregated, which is also anupper bound on all its entries. Element (i, j) corresponds roughly to the number of times algorithm i beatsalgorithm j . Column-wise averages at the bottom show aggregate performance (lower is better). From left toright: MLP, ResNet and VGG.
Figure 22: CDFs of normalized errors of the algorithms, group by different batch sizes. Higher CDF indicatesbetter performance. From left to right: batch size = 100, 1000, 10000.
Figure 23:	CDFs of normalized errors of the algorithms, group by different neural network models. HigherCDF indicates better performance. From left to right: MLP, ResNet and VGG.
Figure 24:	A comparison of batch selection algorithms in gradient space. Plots a and b show the logdeterminants of the Gram matrices of gradient embeddings within batches as learning progresses. Plots cand d show the average embedding magnitude (a measurement of predictive uncertainty) in the selectedbatch. The k-centers sampler finds points that are not as diverse or high-magnitude as other samplers. Noticealso that k-MEANS++ tends to actually select samples that are both more diverse and higher-magnitude thana k-DPP, a potential pathology of the k-DPP’s degree of stochastisity. Among all algorithms, CONF hasthe largest average norm of gradient embeddings within a batch; however, in OpenML #6, and the first fewinterations of SVHN, some batches have a log Gram determinant of -∞ (shown as gaps in the curve), whichshows that Conf sometimes selects batches that are inferior in diversity.
Figure 25: Learning curves and running times for OpenML #6 with MLP.
Figure 26:	Learning curves and running times for OpenML #155 with MLP.
Figure 27:	Learning curves and running times for OpenML #156 with MLP.
Figure 28: Learning curves and running times for OpenML #184 with MLP.
Figure 29: Learning curves and running times for SVHN with MLP and ResNet.
Figure 30: Learning curves and running times for MNIST with MLP.
Figure 31: Learning curves and running times for CIFAR10 with MLP and ResNet.
