Figure 1: Visualization of our method,JEM, which defines a joint EBM fromclassifier architectures.
Figure 3:	Class-conditional samples.
Figure 4: CIFAR100 calbration results. ECE = Expected Cali-bration Error (Guo et al., 2017), see Appendix E.1.
Figure 5: Adversarial Robustness Re-sults with PGD attacks. JEM addsconsiderable robustness.
Figure 6: Distal Adversarials. Con-fidently classified images generated fromnoise, such that: p(y = “car”|x) > .9.
Figure 7: Class-conditional Samples. Left to right: CIFAR10, SVHN.
Figure 8: CIFAR100 Class-conditional Samples.
Figure 9: Each row corresponds to 1 class, subfigures corresponds to different values of log p(x).
Figure 10: Histograms (oriented horizontally for easier visual alignment) of log p(x) arranged byclass.
Figure 11: left: samples with highest log p(x), right: left: samples with lowest log p(x)Figure 12: left: samples with highest p(y|x), right: left: samples with lowest p(y|x)17Published as a conference paper at ICLR 2020E CalibrationE.1 Expected Calibration ErrorExpected Calibration Error (ECE) is a metric to measure the calibration of a classifier. It works byfirst computing the confidence, maxy p(y|xi), for each xi in some dataset. We then group the itemsinto equally spaced buckets {Bm}mM=1 based on the classifier’s output confidence. For example, ifM = 20, then B0 would represent all examples for which the classifier’s confidence was between0.0 and 0.05.
Figure 12: left: samples with highest p(y|x), right: left: samples with lowest p(y|x)17Published as a conference paper at ICLR 2020E CalibrationE.1 Expected Calibration ErrorExpected Calibration Error (ECE) is a metric to measure the calibration of a classifier. It works byfirst computing the confidence, maxy p(y|xi), for each xi in some dataset. We then group the itemsinto equally spaced buckets {Bm}mM=1 based on the classifier’s output confidence. For example, ifM = 20, then B0 would represent all examples for which the classifier’s confidence was between0.0 and 0.05.
Figure 13: CIFAR10 Calibration results(d) CIFAR100 JEM (4k labels)18Published as a conference paper at ICLR 2020F Ouf-Of-Distribution DetectionF.1 Experimental detailsTo obtain OOD results for unconditional Glow, we used the pre-trained model and implementationof https://github.com/y0ast/Glow-PyTorch. We trained a Class-Conditional modelas well using this codebase which was used to generate the class-conditional OOD results.
Figure 14: Gradient-free adversarial attacks.
Figure 15: PGD attacks comparing JEM to the IG EBM of Du & Mordatch (2019).
Figure 16: Comparing the effect of the number of samples in the EOT attack. We find negligibledifference between 5 and 10 for JEM-1 (red and green curves).
Figure 17: PGD transfer attack L∞. We attack JEM-0 and evaluate success of the same adversarialexamples under JEM-1 and JEM-10. Whenever an adversarial example is refined back to its correctclass, we set the distance to infinity. Note that the adversarial examples do not transfer well fromJEM-0 to JEM-1/-10.
