Table 1: Statistics of datasets for each translation tasks.
Table 2: Statistics of the training datasets for each translation tasks. These values of DKL[q(z)||p(z)]are to some extent large, which means that MGnmt does rely on the latent variable.
Table 3: BLEU scores on low-resource translation (WMT16 EN什Ro), and cross-domain translation(Iwslt EN什De). Note that for cross-domain translation, all models are trained with Ted domainas parallel data, and News domain as monolingual data if applicable, whereas these models areevaluated on the testsets of the both domains, respectively.
Table 4: BLEU scores on resource-rich language pairs.
Table 5: Incorporating Lm for decoding(Iwslt task).
Table 6: Comparison with NCMR (Iwslt task).
Table 7: Training (hours until early stop) and decod-ing cost comparison on IWSLT task. All the exper-iments are conducted on a single 1080ti GPU.
Table 8: Comparison on robustness ofnoisy source sentence.
Table 9: BLEU scores on low-resource translation (WMT16 EN什Ro), and cross-domain translation(IWSLT EN什DE).
Table 10: BLEU scores on resource-rich language pairs. We report results of Newstest2014 testsetfor WMT14, and MT03 testset for NIST.
Table 11: BLEU scores of RNMT-based experiments on English-French using similar settings asShah & Barber (2018). Numbers in parentheses are quoted from GNMT paper. Note that becausewe used 4.5M English monolingual sentences instead of the original 20.9M (too time-consuming),the reproduced results of “Gnmt-M-Ssl” are a bit lower.
Table 12: An example from Iwslt De-En cross-domain translation. In this case, all the modelswere first trained on parallel bilingual data from Ted talks (Iwslt20 1 6), and exposed to non-parallel bilingual data of News domain (News Crawl).
