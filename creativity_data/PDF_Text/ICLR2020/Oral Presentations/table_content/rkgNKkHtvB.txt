Table 1: Memory and time complexity of attention variants. We write l for length, b for batch size,nh for the number of heads, nc for the number of LSH chunks, nr for the number of hash repetitions.
Table 2: Accuracies on the duplication task of a 1-layer Transformer model with full attention andwith locality-sensitive hashing attention using different number of parallel hashes.
Table 3: Memory and time complexity of Transformer variants. We write dmodel and dff for modeldepth and assume dff â‰¥ dmodel ; b stands for batch size, l for length, nl for the number of layers.
Table 4: BLEU scores on newstest2014 for WMT English-German (En-De). We additionally reportdetokenized BLEU scores as computed by sacreBLEU (Post, 2018).
