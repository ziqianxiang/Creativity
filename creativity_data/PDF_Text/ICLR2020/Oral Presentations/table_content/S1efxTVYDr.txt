Table 1: Comparison with baseline and existing systems on supervised translation tasks. Here,“++/+” after the BLEU score indicates that the proposed method was significantly better than thecorresponding baseline Transformer (base or big) at significance levels p <0.01/0.05. “STD” repre-sents synthetic training data from (Sennrich et al., 2016b).
Table 2: BLEU score comparisons between MASS and previous methods of unsupervised NMT.
Table 3: Performance on the text summarization taskOur results for text summarization are listed in Table 3. We compared our +D2GPo with our baselineMASS, which is the current state-of-the-art model; +D2GPo consistently outperformed the baselineon all evaluation metrics. The models with a semi-supervised setting yielded a large-margin im-provement relative to the model without any pre-training, which demonstrates that the supervisedpre-training is effective in the text summarization task.
Table 4: Perplexity on WritingPrompts.
Table 5: Image caption performance on the MSCOCO Karpathy test split.
Table 6: Ablation study on our proposed D2GPo with different evaluation functions on the super-vised NMT WMT14 EN-DE task, with the Transformer-base model.
Table 7: Comparison of our baseline and our D2GPo method under different training data scales interms of BLEU on the WMT16 EN-RO test set.
Table 8: Captions generated for the left image by the various models described in the paper. Themodels trained with SCST return a more accurate and more detailed summary of the image. Themodels trained with D2GPo return a more grammatically complete sentence.
Table 9: The statics of low frequency words in the reference and generations.
Table 10: Example stories generated by the baselines and our full models.
