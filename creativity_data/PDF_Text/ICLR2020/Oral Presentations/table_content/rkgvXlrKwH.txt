Table 1: Performance of SEED, IMPALA and R2D2.
Table 2: Google Research Football “Hard” using two kinds ofreward functions. For each reward function, 40 hyperparame-ter sets ran with 3 seeds each which were averaged after 500Mframes of training. The table shows the median and maximumof the 40 averaged values. This is a similar setup to Kurachet al. (2019) although we ran 40 hyperparameter sets vs. 100but did not rerun our best models using 5 seeds.
Table 3: Cost of cloud resources as ofSep. 2019.
Table 4: Training cost on DeepMind Lab for 1 billion frames.
Table 5: Training cost on Google Research Football for 1 billion frames.
Table 6: Hyperparameter ranges used in the stability experiments.
Table 7: Hyperparameter ranges used for experiments with scoring and checkpoint rewards.
Table 8: SEED agent hyperparameters for Atari-57.
Table 9: Atari-57 environment processing parameters.
Table 10: Final performance of SEED 8 TPU v3 cores, 610 actors (1 seed) compared to R2D2(averaged over 3 seeds) and Human, using up to 30 random no-op steps at the beginning of eachepisode. SEED was evaluated by averaging returns over 200 episodes for each game after trainingon 40e9 environment frames.
Table 11: Cost of performing 1 billion frames for both IMPALA and SEED split by component.
Table 12: Cost of performing 1 billion frames for both IMPALA and SEED when running on asingle Nvidia P100 GPU on DeepMind Lab.
Table 13: End-to-end inference latency of IMPALA and SEED for different environments and mod-els.
