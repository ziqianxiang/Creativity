title,year,conference
 Natural gradient works efficiently in learning,1998, Neural Computation
 Coupling adaptive batch sizes with learning rates,2017, InProceedings of the 33rd Conference on Uncertainty in Artificial Intelligence
 Improving the convergence of back-propagation learning with second ordermethods,1989, In Proceedings of the 1988 Connectionist Models Summer School
 SGD-QN: careful quasi-Newton stochastic gradient de-scent,2009, J
 MXNet: A flexible and efficient machine learning library for heterogeneous distributedsystems,2015, In 31st Conference on Neural Information Processing Systems
 A modular approach to block-diagonal Hessian approxi-mations for second-order optimization methods,2019, CoRR
 Fast approximatenatural gradient descent in a Kronecker-factored eigenbasis,2018, 2018
 Efficient per-example gradient computations,2015, CoRR
 Deep residual learning for image recognition,2016, In2016 IEEE Conference on Computer Vision and Pattern Recognition
 Long short-term memory,1997, Neural Computation
 Donâ€™t unroll adjoint: Differentiating SSA-form programs,2018, CoRR
 Batch normalization: Accelerating deep network training by reducing in-ternal covariate shift,2015, In Proceedings of the 32nd International Conference on Machine Learning
 Not all samples are created equal: Deep learning with importancesampling,2018, In Proceedings of the 35th International Conference on Machine Learning
 Limitations of the empirical Fisher approximatiom,2019, InAdvances in Neural Information Processing Systems 32
 Topmoumoute online natural gradient algo-rithm,2007, In Advances in Neural Information Processing Systems 20
 New perspectives on the natural gradient method,2014, CoRR
 Optimizing neural networks with Kronecker-factored approximate cur-vature,2015, In Proceedings of the 32nd International Conference on Machine Learning
 Kronecker-factored curvature approximations for recurrentneural networks,2018, In 6th International Conference on Learning Representations
 Second-order stagewise backpropagation for Hessian-matrix analyses andinvestigation of negative curvature,2008, Neural Networks
 DeepOBS: A deep learning optimizer benchmark suite,2019, In7th International Conference on Learning Representations
 Striving for simplicity:The all convolutional net,2015, In 3rd International Conference on Learning Representations
 Diagonal of the Hessian: Most networks used in deep learning useReLU activation functions,2014, ReLU functions have no curvature as they are piecewise linear
