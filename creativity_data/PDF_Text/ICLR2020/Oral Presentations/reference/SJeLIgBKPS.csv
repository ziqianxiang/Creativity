title,year,conference
 A convergence theory for deep learning via over-parameterization,2019, In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds
 Implicit regUlarization in deep matrix fac-torization,2019, In H
 Onexact compUtation with an infinitely wide neUral net,2019, In H
 Obfuscated gradients give a false sense of secu-rity: Circumventing defenses to adversarial examples,2018, In Jennifer Dy and Andreas Krause (eds
 Theory III: Dynamics and generalization in deep networks,2019, CBMM Memo No: 090
 Evasion attacks against machine learning at test time,2013, In HendrikBlockeel
 Implicit regularization for deep neuralnetworks driven by an ornstein-uhlenbeck like process,2019, arXiv preprint arXiv:1904
 Towards evaluating the robustness of neural networks,2017, In 2017IEEE Symposium on Security and Privacy (SP)
 Parse-val networks: Improving robustness to adversarial examples,2017, In Doina Precup and Yee WhyeTeh (eds
 An Introduction to O-minimal Geometry,2002, 2002
 The method of steepest descent for non-linear minimization problems,1944, QuarterlyofApplied Mathematics
 Stochastic subgradientmethod converges on tame functions,2020, Foundations of Computational Mathematics
 Gradient descent finds globalminima of deep neural networks,2019, In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds
 Algorithmic regularization in learning deep homogeneousmodels: Layers are automatically balanced,2018, In S
 Chapter IV - Nonsmooth OptimizationProblems,2004, In Mathematics of Optimization
 Size-independent sample complexity ofneural networks,2018, In SebaStien Bubeck
 Characterizing implicit biasin terms of optimization geometry,2018, In Jennifer Dy and Andreas Krause (eds
 Implicit bias of gradient descenton linear convolutional networks,2018, In S
 Delving deep into rectifiers: Surpassinghuman-level performance on ImageNet classification,2015, In The IEEE International Conference onComputer Vision (ICCV)
 Neural tangent kernel: Convergence and gen-eralization in neural networks,2018, In S
 Risk and parameter convergence of logistic regression,2018, arXiv preprintarXiv:1803
 Gradient descent aligns the layers of deep linear networks,2019, InInternational Conference on Learning Representations
 A refined primal-dual analysis of the implicit bias,2019, arXiv preprintarXiv:1906
 The implicit bias of gradient descent on nonseparable data,2019, InAlina Beygelzimer and Daniel Hsu (eds
 Approximation by combinations of relu and squaredrelu ridge functions with `1 and `0 controls,2018, IEEE Transactions on Information Theory
 Algorithmic regularization in over-parameterizedmatrix sensing and neural networks with quadratic activations,2018, In Sebastien Bubeck
 Lexico-graphic and depth-sensitive margins in homogeneous and non-homogeneous deep models,2019, InKamalika Chaudhuri and Ruslan Salakhutdinov (eds
 Convergence of gradient descent on separable data,2019, In KamalikaChaudhuri and Masashi Sugiyama (eds
 Stochastic gradient descent on separa-ble data: Exact convergence with a fixed learning rate,2019, In Kamalika Chaudhuri and MasashiSugiyama (eds
 Path-SGD: Path-normalized opti-mization in deep neural networks,2015, In C
 In search of the real inductive bias: Onthe role of implicit regularization in deep learning,2015, In 3rd International Conference on LearningRepresentations
 Boosting: Foundations and Algorithms,2012, The MIT Press
 Robust large margindeep neural networks,2017, IEEE Trans
 The implicit bias of gradient descent on separabledata,2018, In International Conference on Learning Representations
 Connecting optimization and regulariza-tion paths,2018, In S
 Intriguing properties of neural networks,2013, In International Conference onLearning Representations
 Improved sample complexities for deep neural networks and robustclassification via an all-layer margin,2020, In International Conference on Learning Representations
 Regularization matters: Generalization andoptimization of neural nets v,2019,s
 The marginalvalUe of adaptive gradient methods in machine learning,2017, In I
 FixUp initialization: ResidUal learning withoUtnormalization,2019, In International Conference on Learning Representations
 Recovery gUaranteesfor one-hidden-layer neUral networks,2017, In Doina PrecUp and Yee Whye Teh (eds
 Mathematical programming methods,1976, 1976
