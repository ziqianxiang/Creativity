title,year,conference
 The case for full-matrix adaptive regularization,2018, arXiv preprint arXiv:1806
 On the convergence of a class of adam-type algorithms fornon-convex optimization,2018, arXiv preprint arXiv:1808
 Transformer-XL: At-tentive language models beyond a fixed-length context,2019, CoRR
 SAGA: A fast incremental gradient method with supportfor non-strongly convex composite objectives,2014, In NIPS
 Spider: Near-optimal non-convex optimization via stochasticpath integrated differential estimator,2018, arXiv preprint arXiv:1807
 Accelerated gradient methods for nonconvex nonlinear and stochasticprogramming,2016, Mathematical Programming
 Linear convergence of variance-reduced stochastic gradient without strong con-vexity,2014, arXiv preprint arXiv:1406
 Deep learning,2016, MIT press
 Beyond convexity: Stochastic quasi-convex optimiza-tion,2015, In Advances in Neural Information Processing Systems
 Deep residual learning for image recognition,2016, In Proceedingsof the IEEE conference on computer vision and pattern recognition
 Long short-term memory,1997, Neural computation
 Accelerated gradient descent escapes saddle points faster thangradient descent,2018, In Conference On Learning Theory
 Accelerating stochastic gradient descent using predictive variance reduc-tion,2013, In Advances in Neural Information Processing Systems
 ADAM: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Semi-stochastic gradient descent methods,2013, arXiv:1312
 Learning multiple layers of features from tiny images,2009, Technicalreport
 Regularizing and optimizing LSTM language models,2018, InInternational Conference on Learning Representations
 Recurrent neural network basedlanguage model,2010, In INTERSPEECH 2010
 Understanding the exploding gradient problem,2012, CoRR
 On the difficulty of training recurrent neural networks,2013, InInternational conference on machine learning
 Deepcontextualized word representations,2018, arXiv preprint arXiv:1802
 Introduction to optimization,1987, optimization software
 On the convergence of ADAM and beyond,2019, arXiv preprintarXiv:1904
 Accelerated proximal stochastic dual coordinate ascent for regu-Iarized loss minimization,2014, In International Conference on Machine Learning
 Escaping saddle points with adaptive gradientmethods,2019, arXiv preprint arXiv:1901
 LSTM neural networks for language modeling,2012, InINTERSPEECH 2012
 Regularization of neural networks us-ing DropConnect,2013, In S
 The marginal value of adaptivegradient methods in machine learning,2017, In Advances in Neural Information Processing Systems
 Recent trends in deep learning based naturallanguage processing,2017, CoRR
 On the convergence of adaptive gradient methodsfor nonconvex optimization,2018, arXiv preprint arXiv:1808
 Stochastic nested variance reduction for nonconvex optimization,2018, InProceedings of the 32nd International Conference on Neural Information Processing Systems
 Adashift: Decorrelation and convergenceof adaptive learning rate methods,2018, arXiv preprint arXiv:1810
 A sufficient condition for convergences of ADAMand RMSProp,2018, arXiv preprint arXiv:1811
 Many efforts have been made to accelerate gradient-based methods,2018, Oneelegant approach is variance reduction (e
