title,year,conference
 Natural gradient works efficiently in learning,1998, Neural computation
 Learning to learn by gradient descent bygradient descent,2016, In Advances in Neural Information Processing Systems
 On the optimization of deep networks: Implicitacceleration by overparameterization,2018, In International Conference on Machine Learning
 Mirror descent and nonlinear projected subgradient methods forconvex optimization,2003, Operations Research Letters
 Learning a synaptic learning rule,1991, Universitede MOntreal
 Learningfeed-forward one-shot learners,2016, In Advances in Neural Information Processing Systems
 Learning to learn without gradient descent by gradientdescent,2017, In International Conference on Machine Learning
 Approximation by superpositions of a sigmoidal function,1989, MCSS
 Imagenet: A large-scale hierarchical image database,2009, In International Conference on Computer Vision and PatternRecognition
 Natural neuralnetworks,2015, In Advances in Neural Information Processing Systems
 Model-Agnostic Meta-Learning for Fast Adapta-tion of Deep Networks,2017, In International Conference on Machine Learning
 Breaking the activation functionbottleneck through adaptive parameterization,2018, In Advances in Neural Information ProcessingSystems
 Transferringknowledge across learning processes,2019, In International Conference on Learning Representations
 Dynamic few-shot visual learning without forgetting,2018, InInternational Conference on Computer Vision and Pattern Recognition
 Deep residual learning for imagerecognition,2016, In International Conference on Computer Vision and Pattern Recognition
 Mask r-cnn,2017, In InternationalConference on Computer Vision
 Long short-term memory,1997, Neural computation
 Batch normalization: Accelerating deeP network training byreducing internal covariate shift,2015, In International Conference on Machine Learning
 Meta-learning rePresentations for continual learning,2019, arXivpreprint arXiv:1905
 Adam: A Method for Stochastic OPtimization,2015, In InternationalConference on Learning Representations
 Uncertainty in multitask transfer learning,2018, In Advances in Neural InformationProcessing Systems
 One shot learning ofsimPle visual concePts,2011, In Proceedings of the Annual Meeting of the Cognitive Science Society
 Meta-learning withdifferentiable convex oPtimization,2019, In CVPR
 Meta-Learning with AdaPtive Layerwise Metric and SubsPace,2018, InInternational Conference on Machine Learning
 Learning to oPtimize,2016, In International Conference on Machine Learning
 Meta-sgd: Learning to learn quickly forfew-shot learning,2017, arXiv preprint arXiv:1707
 Learning without forgetting,2016, In European Conference on ComputerVision
 Taming MAML: Efficient unbiased meta-reinforcement learning,2019, In International Conference on Machine Learning
 Deep learning via hessian-free optimization,2010, In International Conference on MachineLearning
 Optimizing neural networks with kronecker-factored approximatecurvature,2015, In International Conference on Machine Learning
 Guided meta-policy search,2019, arXiv preprint arXiv:1904
 Meta-learningupdate rules for unsupervised representation learning,2019, In International Conference on LearningRepresentations
 A Simple Neural AttentiveMeta-Learner,2018, In International Conference on Learning Representations
 Fast-slow recurrent neural networks,2017, In Advancesin Neural Information Processing Systems
 Learningrapid-temporal adaptations,2018, In International Conference on Machine Learning
 Numerical optimization,2006, Springer
 Tadam: Task dependent adaptive metricfor improved few-shot learning,2018, In Advances in Neural Information Processing Systems
 Revisiting natural gradient for deep networks,2014, In InternationalConference on Learning Representations
 Film: Visualreasoning with a general conditioning layer,2018, In Association for the Advancement of ArtificialIntelligence
 Optimization as a model for few-shot learning,2016, In InternationalConference on Learning Representations
 Learning multiple visual domainswith residual adapters,2017, In Advances in Neural Information Processing Systems
 Meta-learning with latent embedding optimization,2019, In InternationalConference on Learning Representations
 Exact solutions to the nonlineardynamics of learning in deep linear neural networks,2013, arXiv preprint arXiv:1312
 Evolutionary principles in Self-referential learning,1987, PhD thesis
 Language modeling with recurrent highway hypernetworks,2017, In Advances in NeuralInformation Processing Systems
 On the importance of ini-tialization and momentum in deep learning,2013, In International Conference on Machine Learning
 Learning to learn: Introduction and overview,1998, In In Learning ToLearn
 Deep meta-learning: Learning to learn in the conceptspace,2018, arXiv preprint arXiv:1802
 FastContext Adaptation via Meta-Learning,2019, International Conference on Machine Learning
