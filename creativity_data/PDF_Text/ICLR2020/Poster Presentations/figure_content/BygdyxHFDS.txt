Figure 1: Our RL agent is augmented with a cu-riosity module, obtained by meta-learning over acomplex space of programs, which computes apseudo-reward rbat every time step.
Figure 2: Example diagrams of published algorithms covered by our language (larger figures in theappendix). The green box represents the output of the intrinsic curiosity function, the pink box isthe loss to be minimized. Pink arcs represent paths and networks along which gradients flow backfrom the minimizer to update parameters.
Figure 3: Fast Action-Space Transition(FA S T):top-performing intrinsic curiosity algorithm dis-covered in our phase 1 search.
Figure 4: Correlation between program performance in gridworld and in harder environments (lunarlander on the left, acrobot on the right), using the top 2,000 programs in gridworld. Performance isevaluated using mean reward across all learning episodes, averaged over trials (two trials for acrobot/ lunar lander and five for gridworld). The high number of algorithms performing around -300 in themiddle of the right plot is an artifact of averaging the performance of two seeds and the mean per-formance in Acrobot having two peaks. Almost all intrinsic curiosity programs that had statisticallysignificant performance for grid world also do well on the other two environments. In green, theperformance of three published works; in increasing gridworld performance: disagreement (Pathaket al., 2019), inverse features (Pathak et al., 2017) and random distillation (Burda et al., 2018).
Figure 5: Curiosity by predictive error on inverse features by Pathak et al. (2017). In pink, paths andnetworks where gradients flow back from the minimizer.
Figure 6: Curiosity by ensemble predictive variance Pathak et al. (2019). In pink, paths and net-works where gradients flow back from the minimizer.
Figure 7: Predicting algorithm performance from the structure of the program alone. Comparisonbetween predicted and actual performance on a test set; showing a correlation of 0.54. In black, theidentity line.
Figure 8: Predicting algorithm performance allows us to find the best programs faster. We investigatethe number of the top 1% of programs found vs. the number of programs evaluated, and observethat the optimized search (in blue) finds 88% of the best programs after only evaluating 50% of theprograms (highlighted in green). The naive search order would have only found 50% of the bestprograms at that point.
Figure 9: In black, mean performance across 5 trials for all 26,000 programs evaluated (out of theirfinished trials). In green mean plus one standard deviation for the mean estimate and in red oneminus one standard deviation for the mean estimate. On the right, you can see program means formroughly a gaussian distribution of very big noise (thus probably not significant) with a very small(between 0.5% and 1% of programs) long tail of programs with statistically significantly goodperformance (their red dots are much higher than almost all green dots), composed of algorithmsleading to good exploration.
Figure 10: Cycle-Consistency Intrinsic Motivation algorithm, found by our search (3 of the top 16programs on grid world are variants of this program). The purple Predict Target From Query boxesfeed the query to a neural network, return the prediction as output and add the prediction loss to theoptimization, back-propagating to the network and the query, but not the target. Notice that Î¸1 is notgetting trained because no loss back-propagates there; thus producing a random feature embeddingsf (t) from s(t). The algorithm combines several concepts seen in the literature, such as an untrainednetwork like RND Burda et al. (2018) and predicting another state in feature space like Pathak et al.
Figure 11: Top variant in preliminary search on grid world; variant on random network distillationusing an ensemble of trained networks instead of a single one.
