Figure 1: Performance comparison for CNN on CIFAR-10.
Figure 2: Performance comparison for DenseNet-201 on CIFAR-100.
Figure 3:	Hyperparameters and sparsity for DenseNet-201 on CIFAR-100.
Figure 4:	Performance comparison for BNN on MNISTlearned weights are binarized to generate a full BNN whose test accuracy is in Figure 4(b). On theone hand, we see from Figure 4(a)-(b) that the achieved training loss and test accuracy by BNN isworse than the standard full-precision DNN (possibly with soft-thresholding). This is expected asBNN imposes regularization and constraints on the optimization problem and reduces the searchspace. However, the difference in test accuracy is quite small. On the other hand, we see fromFigure 4(c) that the regularization in the proposed formulation (3) is very effective in promotingbinary weights: 15% of weights are in the range (-1,-0.5) and 15% of weights are in the range(0.5,1), and all the other weights are either -1 or 1. As all weights are exactly or close to 1 or -1, wecould just binarize the weights to exactly 1 or -1 only once by hard thresholding, after the trainingis completed, and thus the incurred performance loss is small (98% versus 95% for test accuracy).
