Figure 1: (a) Examples of randomized inputs (color values in each channel are normalized forvisualization) generated by re-initializing the parameters of a random layer. Examples of seen andunseen environments on (b) CoinRun, (c) DeepMind Lab, and (d) Surreal robotics control.
Figure 2: Samples of dogs vs. cats dataset. Thetraining set consists of bright dogs and dark cats,whereas the test set consists of dark dogs andbright cats.
Figure 3: (a) We collect multiple episodes from various environments by human demonstrators andvisualize the hidden representation of trained agents optimized by (b) PPO and (C) PPO + oursconstructed by t-SNE, where the colors of points indicate the environments of the correspondingobservations. (d) Average success rates for varying number of MC samples.
Figure 4: Visualization of activation maps via Grad-CAM in seen and unseen environments in thesmall-scale CoinRun. Images are aligned with similar states from various episodes for comparison.
Figure 5: The performances of trained agents in unseen environments under (a) large-scale CoinRun,(b) DeepMind Lab and (c) Surreal robotics control. The solid/dashed lines and shaded regionsrepresent the mean and standard deviation, respectively.
Figure 6:	Learning curves on (a) small-scale, (b) large-scale CoinRun and (c) DeepMind Lab. Thesolid line and shaded regions represent the mean and standard deviation, respectively, across threeruns.
Figure 7:	The performance in unseen environments in small-scale CoinRun. The solid/dashed lineand shaded regions represent the mean and standard deviation, respectively, across three runs.
Figure 8:	The performance of random networks in various locations in the network architecture on(a) seen and (b) unseen environments in large-scale CoinRun. We show the mean performancesaveraged over three different runs, and shaded regions represent the standard deviation.
Figure 9:	Network architectures with random networks in various locations. Only convolutionallayers and the last fully connected layer are displayed for conciseness.
Figure 10: Examples of seen and unseen environments in small-scale CoinRun.
Figure 11: Examples of seen and unseen environments in large-scale CoinRun.
Figure 12: The top-down view of the trained map layouts.
Figure 13: (a) An illustration of network architectures for the Surreal robotics control experiment,and learning curves with (b) regularization and (c) data augmentation techniques. The solid line andshaded regions represent the mean and standard deviation, respectively, across three runs.
Figure 14: Examples of seen and unseen environments in the Surreal robot manipulation.
Figure 15:	Performances of trained agents in seen and unseen environments under (a/b) CartPole and(c/d) Hopper. The solid/dashed lines and shaded regions represent the mean and standard deviation,respectively.
Figure 16:	(a) Modified CoinRun with good and bad coins. The performances on (b) seen and (c)unseen environments. The solid line and shaded regions represent the mean and standard deviation,respectively, across three runs. (d) Average success rates on large-scale CoinRun for varying thefraction of clean samples during training. Noe that Î± = 1 corresponds to vanilla PPO agents.
Figure 17:	Visualization of the hidden representation of trained agents optimized by (a) PPO, (b)PPO + L2, (c) PPO + BN, (d) PPO + DO, (e) PPO + CO, (f) PPO + GR, (g) PPO + IV (h) PPO + CJ,and (i) PPO + ours using t-sNE. The point colors indicate the environments of the correspondingobservations.
