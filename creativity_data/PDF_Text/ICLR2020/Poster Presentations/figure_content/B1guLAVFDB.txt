Figure 1: Partial span recovery of small networks with layer sizes specified in the legend. Note that 784->80->[6,3]indicates a 4 layer neural network with hidden layer sizes 784, 80, 6, and 3, in that order. Full span recovery is notalways possible and recovery deteriorates as width decreases and depth increases.
Figure 2: Full span recovery of realistic networks with moderate widths and reasonable architectures. Full recoveryoccurs with only 100 samples for a rank 80 weight matrix in all settings.
Figure 3: Fooling ReLU networks into misclassifying noise as digits by introducing Gaussian noise into the null spaceafter span recovery. The prediction of the network is presented above the images, along with its softmax probability.
