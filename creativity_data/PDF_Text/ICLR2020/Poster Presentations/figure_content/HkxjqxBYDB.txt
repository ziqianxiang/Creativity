Figure 1: Comparison of selected poli-cies based on episodic memory and as-sociative memory. An agent starts fromtwo places A and B, to collect two ex-periences.
Figure 2: Comparison of episodic memory and as-sociative memory.
Figure 3: Overall framework of ERLAM.
Figure 4: Maps of Monster Kong. Compared to MonsterKong1, MonsterKong2 has a different goaland MonsterKong3 has a totally different MDP.
Figure 5: Learning curves of ERLAM, EMDQN, and DQN on Monster Kong. The top row com-pares the average scores per episode between all models. The bottom row shows state-action valueestimates by associative memory, episodic memory, and Q networks when running ERLAM. Theblack dash line represents the actual discounted state-action values of the best learned policy.
Figure 6: Visualization of trajecto-ries. The blue line and red line vi-sualize two policies using episodicmemory, while the yellow dashline represents the combinatorialtrajectory by value propagation inassociative memory.
Figure 7: Comparison between ERLAM (i.e., DQN with associative memory) and EMDQN (i.e.,DQN with episodic memory) measured in improvements of scores over DQN as shown in Eq. 6.
Figure 8: Examples of learning curves on 10 million frames compared with EMDQN and DQN.
Figure 9: Learning curves onand vanilla DQN.
