Figure 1: (a) Actor-learner architecture with a target network, which is used to generate agentexperience in the environment and is updated every Ttarget learning steps from the online network. (b)Schematic of the agents, with the policy (θ) and value (φ) networks sharing most of their parametersthrough a shared input encoder and LSTM [or Transformer-XL (TrXL) for single Atari levels]. Theagent also receives the action and reward from the previous step as an input to the LSTM. For DMLaban additional LSTM is used to process simple language instructions.
Figure 2: (a) Multi-task DMLab-30. IMPALA results show 3 runs of 8 agents each; within a runhyperparameters were evolved via PBT. For V-MPO each line represents a set of hyperparametersthat are fixed throughout training. The final result of R2D2+ trained for 10B environment stepson individual levels (Kapturowski et al., 2019) is also shown for comparison (orange line). (b)Multi-task Atari-57. In the IMPALA experiment, hyperparameters were evolved with PBT. ForV-MPO each of the 24 lines represents a set of hyperparameters that were fixed throughout training,and all runs achieved a higher score than the best IMPALA run. Data for IMPALA (“Pixel-PopArt-IMPALA” for DMLab-30 and “PopArt-IMPALA” for Atari-57) was obtained from the authors ofHessel et al. (2018). Each agent step corresponds to 4 environment frames due to the action repeat.
Figure 3: V-MPO trained on single example levels from DMLab-30, compared to IMPALA and morerecent results from R2D2+, the larger, DMLab-specific version of R2D2 (Kapturowski et al., 2019).
Figure 4: Example levels from Atari. In Breakout, V-MPO achieves the maximum score of 864in every episode. No reward clipping was applied, and the maximum length of an episode was 30minutes (108,000 frames). Supplementary video for Ms. Pacman: https://bit.ly/2lWQBy5Environment (4×agent) steps (B)Environment steps (B)Environment steps (B)Environment steps (B)Environment steps (B)(a)(b)	(c)	(d)Figure 5: (a) Humanoid “run” from full state (Tassa et al., 2018) and (b) humanoid “gaps” frompixel observations (Merel et al., 2019). Purple curves are the same runs but without parametricKL constraints. Det. eval.: deterministic evaluation. Supplementary video for humanoid gaps:https://bit.ly/2L9KZdS. (c)-(d) Example OpenAI Gym tasks. See also Fig. 11 in theAppendix for Gym Humanoid-V1.
Figure 5: (a) Humanoid “run” from full state (Tassa et al., 2018) and (b) humanoid “gaps” frompixel observations (Merel et al., 2019). Purple curves are the same runs but without parametricKL constraints. Det. eval.: deterministic evaluation. Supplementary video for humanoid gaps:https://bit.ly/2L9KZdS. (c)-(d) Example OpenAI Gym tasks. See also Fig. 11 in theAppendix for Gym Humanoid-V1.
Figure 6: Multi-task Atari-57 with population-based training (PBT) (Jaderberg et al., 2017a). Allsettings of the PBT experiment were the same as without except the learning rates were also sampledlog-uniformly from [8 × 10-5 , 3 × 10-4) and η from [0.05, 0.5). Along with α sampled log-uniformly from [0.001, 0.01) as in the original experiment, hyperparameters were evolved via copyand mutation operators roughly once every 4 × 108 environment frames.
Figure 7: KL constraints during optimization for the Seaquest example in Fig. 4c. Values aresubsampled but not smoothed to show the variability.
Figure 8: Same as Fig. 4c (Atari Seaquest), but trained with uniform weights on the top 50% ofadvantages.
Figure 9: Same as Fig. 2a (multi-task DMLab-30), but trained without top-k, i.e., all advantages areused in the E-step. Note the small dip in the middle is due to a pause in the experiment and resettingof the human-normalized scores.
Figure 10: Example frame from the humanoid gaps task, with the agent’s 64×64 first-person viewon the right. The proprioceptive information provided to the agent in addition to the primary pixelobservation consisted of joint angles and velocities, root-to-end-effector vectors, root-frame velocity,rotational velocity, root-frame acceleration, and the 3D orientation relative to the z-axis.
Figure 11: 17-dimensional Humanoid-V1 task in OpenAI Gym.
