Figure 1: Left: Number of models with training accuracy> 99% for each hyperparameter type. Mid.: Distri-bution of training cross-entropy (that of training error in Fig. 4). Right: Distribution of generalization gap.
Figure 2: Left: Graph at initialization of IC algorithm. Middle: The ideal graph where the measure μcan directly explain observed generalization. Right: Graph for correlation where μ cannot explain observedgeneralization.
Figure 3: Joint Probability table for a single SabTogether forms a 2 by 2 table that defines the joint distribution of the Bernoulli random variablesPr(g(O) > g(b) | Sab) and Pr(μ(a) > μ(b) | Sab). For notation convenience, we use Pr(μ, g ∣ Sab),Pr(g ∣ Sab) and Pr(μ ∣ Sab) to denote the joint and marginal. If there are N = 3 choices for eachhyperparameter in S then there will be N|s| such tables for each hyperparameter combination.
Figure 4: Distribution of training error on the trained models.
