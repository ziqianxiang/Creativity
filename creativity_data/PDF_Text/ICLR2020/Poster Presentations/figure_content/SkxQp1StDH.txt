Figure 1: Visualization of synthetic example network together with our embedding. The graph isembedded into the 2-variate normal distributions, which are represented by a σ ellipse (boundaryof 1 standard deviation around mean μ). The σ ellipses of the green nodes are contained in one ofthe greys nodes, which represents that the distance (measured with divergence in embedded space)between green and grey is small, but in the opposite direction very large.
Figure 2: Visualization of approximated values w.r.t. inverse true distance (d-,V )u,v with boxplots,representing the means, first and third quartiles as well as 1.5 interquartile range. Violin plots indicatethe kernel densities. Columns respectively correspond to dataset political blogs (first column),Cora (second column), and arXiv hep-th (last column). Rows from top to bottom represent APPHOPE, DeepWalk, GraPh2Gauss, and our KL method. Ideally, as the ground-truth value increases,approximated values are expected to follow this trend as well. The baseline methods show only aweak separation between the closest distances and all other. For our embedding method (last row)a monotonic increasing relation is visible, which was reflected in the highest mutual information,Pearson and Spearman,s correlation coefficient in Table 2.
Figure 3: Visualization of the hyperboloid geometry of directed graphs for one-dimensional Gaussiandistributions. The coordinate frame is shown With the green (σ) and orange lines (μ). The nodes fromthe synthetic directed network from Figure 1 are shown as points with different colors of groups,depending on the position in the network. The embeddings for nodes (μ, σ) in the statistical manifoldare found by minimizing the objective function (2) and then making the mappings to the hyperboloidmodel.
Figure 4: Relative effect of correction ▽ L in comparison to VL. Starting from the same representa-tion, we optimized with gradient descent optimization with VL and VL with learning rate 1 * 10-6.
