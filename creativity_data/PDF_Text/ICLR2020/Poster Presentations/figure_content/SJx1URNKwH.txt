Figure 1: Video retargeting on a budget. Our goal is to retarget a source video to a target, quickly (running afew, T , iterations for adaptation) and efficiently (given a few, K, frames from the target domain). We achievethat by (meta)learning a model θ that is able to quickly and efficiently adapt to a given target video.
Figure 2: Meta-learning for video retargeting. Our goal is to learn a generic retargeting model θ (parametersof a Pix2PixHD (Wang et al., 2018b) in our case), such that it can be efficiently personalized for a specific persongiven only a few samples/frames of their appearance. We achieve it using meta-learning, where our model isoptimized for personalization to a new person, given only K samples of their appearance, and being trained forT iterations.
Figure 3: Training data. Frames from the additional training data we collected. We download 10 videos fromYouTube, distinct from the ones used for personalization and evaluation.
Figure 5: Personalization using K frames. We find that while all initializations get better With increasing K,and using MetaPix consistently outperforms simple pretraining. Moreover we note that even using a modeltrained with MetaPix for K = 5 works well at any K value used at test time, showing the generalizability ofMetaPix. It is worth noting that the biggest gap is seen at lower values of K, showing our method is most usefulin cases where one has little data for personalization.
Figure 6: Personalization after T iterations. We compare performance on increasing the computational budgetfor personalization. As expected all initializations improve with T, though MetaPix consistently outperformsrandom or pretraining. Again we see strong generalizability, as a MetaPix model trained for T = 20 performswell at other T values used at test time.
Figure 7: Visualizing finetuning between MetaPix and Pretrained. We compare MetaPix’s initialization forthe K = 5, T = 200 task to our pretrained model initialization. We visualize models obtained during iterationsof finetuning, at 0, 10, 20, 40, 80 and 200 iterations for 5 random test pose-image pairs. The images generatedby MetaPix’s initialization are temporally coherent, whereas the pretrained weights produce various trainingimages depending on the pose. As observed in the intermediate iterations, the initialization translates its temporalcoherence properties across finetuning iterations as well. This further reinforces our belief that MetaPix learns aninitialization that is able to quickly adapt to the actor and background appearance from the few samples providedat test time.
Figure 8: Qualitative Variation in K. We compare the MetaPix-trained models (MT) with their pretrainedcounterparts (PT) for K = [3, 5, 10, 20]. We fix the base architecture to Pix2PixHD and T = 20. With higherK, both methods generate good images, but with lower K, MetaPix generates backgrounds and clothing thatbetter match the ground-truth. Our results illustrate that meta-learning excels in the few-shot regime.
