Figure 1: Illustration of mixout(u). Suppose that u and w are a target model parameter and acurrent model parameter, respectively. (a): We first memorize the parameters of the vanilla networkat u. (b): In the dropout network, we randomly choose an input neuron to be dropped (a dottedneuron) with a probability of p. That is, all outgoing parameters from the dropped neuron areeliminated (dotted connections). (c): In the mixout(u) network, the eliminated parameters in (b)are replaced by the corresponding parameters in (a). In other words, the mixout(u) network at wis the mixture of the vanilla network at u and the dropout network at w with a probability of p.
Figure 2: We present kwft - wpre k2, validation accuracy on MNIST (target task), and validationaccuracy on EMNIST Digits (source task), as the function of the probability p where wft and wpreare the model parameter after finetuning and the pretrained model parameter, respectively. We reportmean (curve) ± std. (shaded area) across 10 random restarts. (a): mixout(wpre, p) L2-penalizesthe deviation from wpre, and this penalty becomes strong as p increases. However, with dropout(p),wft becomes away from wpre asp increases. (b): After finetuning on MNIST, both mixout(wpre, p)and dropout(p) result in high validation accuracy on MNIST for p ∈ {0.1, 0.2, 0.3}. (c): Valida-tion accuracy of dropout(p) on EMNIST Digits drops more than that of mixout(wpre, p) for allp. mixout(wpre, p) minimizes the deviation from wpre and memorizes the source task better thandropout(p) for all p.
Figure 3: Distribution of dev scores on each task from 20 random restarts when finetuningBERTLARGE with Devlin et al. (2018)’s: both dropout(0.1) and wdecay(0, 0.01), Wiese et al.
Figure 4: Distribution of RTE dev scores (Accuracy) from 20 random restarts when finetun-ing BERTLARGE with dropout(p) (orange) or mixout(wpre, p) (blue). Error intervals showmean±std. We do not use wdecay(0) nor wdecay(wpre). In the case of mixout(wpre, p), thenumber of usable models after finetuning with mixout(wpre, {0.7, 0.8, 0.9}) is significantly morethan the number of usable models after finetuning with dropout(p) for all p.
Figure 5: Behavior of mixout(u, p) for a strongly convex loss function. We plot the line obtainedby least squares regression with mixout(u, {0.0, 0.3, 0.6, 0.9}) (each green line) on a syntheticdataset (blue dots) generated by the true line (each blue dotted line). As p increases, the regressionline (each green line) converges to the target line generated by the target model parameter u (eachorange dotted line) rather than the true line (each blue dotted line).
Figure 6: Distribution of dev scores on each task from 20 random restarts when finetuningBERTLARGE with dropout({0.0, 0.1, …,0.5}). Error intervals show mean±std. When We usedropout(0.1), we have the highest average dev scores on MRPC and STS-B and the second-highestaverage dev scores on RTE and CoLA. These results show that dropout(0.1) is almost optimal forall tasks in terms of mean dev score.
