Figure 1: DivideMix trains two networks (A and B) simultaneously. At each epoch, a network modelsits per-sample loss distribution with a GMM to divide the dataset into a labeled set (mostly clean) and anunlabeled set (mostly noisy), which is then used as training data for the other network (i.e. co-divide). At eachmini-batch, a network performs semi-supervised training using an improved MixMatch method. We performlabel co-refinement on the labeled samples and label co-guessing on the unlabeled samples.
Figure 2: Training on CIFAR-10 with 40% asymmetric noise, warm up for 10 epochs. (a) Standard trainingwith cross-entropy loss causes the model to overfit and produce over-confident predictions, making ` difficult tobe modeled by the GMM. (b) Adding a confidence penalty (negative entropy) during warm up leads to moreevenly-distributed `. (c) Training with DivideMix can effectively reduce the loss for clean samples whilekeeping the loss larger for most noisy samples.
Figure 3: Area Under a Curve for clean/noisy image classification on CIFAR-10 training samples. Our methodcan effectively filter out the noisy samples and leverage them as unlabeled data.
Figure 4: ClothingIM images identified as noisy samples by our method. Ground-truth labels are shown abovein red and the co-guessed labels are shown below in blue.
Figure 5: T-SNE of training images after training the model using DivideMix for 200 epochs on CIFAR-10with 80% label noise. Different colors indicate (a) noisy training labels or (b) true labels. DivideMix is able tolearn the true class distribution of the training data despite the label noise.
