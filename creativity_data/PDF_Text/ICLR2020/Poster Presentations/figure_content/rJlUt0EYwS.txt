Figure 2: Overview of the NExT Framework. Natural language explanations are firstly parsed intological forms. Then We partition the raw corpus S into labeled dataset Sa and unlabeled datasetSu = S- {χa}N=aι. We use matching modules to provide supervision on Su. Finally, supervisionfrom both Sa and Su is fed into a classifier.
Figure 3: Neural Execution Tree (NExT) softly executes the logical form on the sentence.
Figure 4: NExT’s performance w.r.t.
Figure 5: Performance of NExT v.s. traditionalsupervised method. Blue line denotes NExT anddashed line denotes annotating numbers, normalline means performance. Red line denotes tradi-tional supervised method, and dashed line meansperformance, normal line means annotating num-bers.
Figure 6: Performance with different number of explanations. We choose supervised semi-supervisedbaselines for comparison. We did experiments on TACRED and Restaurant. Gray dashed lines mean theperformance with the corresponding labeled data.
Figure 7: Adjusting NExTFramework (Fig. 2) for NLPro-LOG.
Figure 8: Heatmap for keyword chief executive of and sentence OBJ-PERSON, executive director ofthe SUBJ-ORGANIZATION at Saint Anselm College in Manchester. Results show that our string matching module cansuccessfully grasp relevant words.
