Figure 1: Hierarchical visual foresight: Our method takes as input the current image, goal image, an actionconditioned video prediction model fθ, and a generative model gφ (z). Then, it samples sets of possible statesfrom gφ (z) as sub-goals. It then plans between each sub-goal, and iteratively optimizes the sub-goals to min-imize the worst case planning cost between any segment. The final set of sub-goals that minimize planningcost are selected, and finally the agent completes the task by performing visual model predictive control withthe sub-goals in sequence. In this example the task is to push a block off the table, and close the door. Givenonly the final goal image, HVF produces sub-goals for (1) pushing the block and reaching to the handle and (2)closing the door.
Figure 2: Quantitative Results for Maze Naviga-tion. Success rate for navigation tasks using no sub-goals, HVF subgoals, and ground truth hand specifiedsubgoals. We observe that HVF significantly improvesperformance on the “Medium” and “Hard” difficultycompared to not using subgoals. Computed over 100randomized trials.
Figure 5: Qualitative Results for Desk Manipulation. Example generated subgoals from HVF for the deskmanipulation tasks with one or two subgoals. We observe interesting behavior: in the Door Closing + BlockPushing task with one subgoal, the subgoal is to slide the door then push the block, while in the Door Closing+ 2 Block Pushing the first subgoal is to push the blocks, then grasp the door handle, then slide the door.
Figure 4: Quantitative Results for Desk Manipu-lation: Using HVF drastically improves performance.
Figure 6: BAIR Dataset Qualitative Results. Thesubgoals generated by HVF on the BAIR dataset, whichwe find correspond to meaningful states between thestart and goal. For example, when moving objects wesee subgoals corresponding to reaching/grasping.
Figure 7: Architecture of stochastic variational video prediction model (SV2P) (Babaeizadeh et al., 2017)used for dynamics model fθ (taken from (Babaeizadeh et al., 2017) with permission). The architecture has twomain sub-networks, one convolutional network which approximates the distribution of latent values give all theframes, and a recurrent convolutional network which predicts the pixels of the next frame, given the previousframe, sampled latent, and action (if available). The code is open sourced in Tensor2Tensor (Vaswani et al.,2018) library.
Figure 8:	BAIR Dataset Additional Results.
Figure 9:	Qualitative examples of TAP. Notice it is prone to predicting very close to the current or goal statefor subgoals.
