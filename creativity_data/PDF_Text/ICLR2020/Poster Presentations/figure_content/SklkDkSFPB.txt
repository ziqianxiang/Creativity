Figure 1: The three-step BlockSwap pipeline. Beginning with a large network, we sample a listof candidate architectures by replacing its blocks with cheap alternatives. In Step 2, we rank eachcandidate by its Fisher potential after a single minibatch of training data. In Step 3, we select thehighest ranked architecture and train it with attention transfer from the original network.
Figure 2: For parameter budgets of (a) 200K, (b) 400K and (c) 800K we train a single blocktypenetwork and 100 random mixed blocktype networks. This lets us examine the distribution of mixedblocktype architectures to verify the existence of performant ones. The dotted red line on each plotrepresents models composed of a single block type, and the shaded blue density represents randomlyconstructed, mixed-blocktype networks. For example, (a) shows that a WRN-40-2 with every blocksubstituted for BG(2,4) trains to 5.0% error, while the mean error of randomly constructed networksat the same parameter budget lies at around 5.15% error. This implies that random architecture searchwill not work here, and that a better strategy is needed.
Figure 3: CIFAR-10 top-1 test error of students versus parameters. BlockSwap models (blue) givelower error for each parameter budget when compared to depth/width reduced or pruned models.
Figure 4: Each bar shows a WideResNet-40-2 with one block substituted for G(4), where (a) showsfinal CIFAR-10 error and (b) shows resulting number of parameters, with blocks coloured by networkstage. This shows us the sensitivity of each block; for example, block 12has relatively few parameters,but substituting it for a cheaper block is very detrimental to final error.
Figure 5: An example of (a) a bad architecture and (b) a good architecture for a given parameterbudget (200K), with blocks coloured by network stage. Capacity score is given by the accuracy thatthe selected blocktype achieves after being trained on its own. We can see that the good model (b)follows the pattern observed in Figure 4, while the bad model (a) does not.
Figure 6: ResNet-34 block substitutions chosen by BlockSwap at 3M (top) and 8M (bottom) parame-ters on ImageNet.
