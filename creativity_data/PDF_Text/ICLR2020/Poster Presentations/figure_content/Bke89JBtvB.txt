Figure 1: Batch-shaping loss. (a) Illustration of the computation of the batch-shaping loss. (b) Theoutput distribution for four gates and their shapes over different epochs. We can see that initially gatesare firing in a conditional pattern, but after removing the shaping loss and introducing the L0-lossthey may become fully active, stay conditional or turn off completely.
Figure 2: Illustration of our channel gated ResNet block and the gating module.
Figure 3: Comparison of the results of our algorithm and competing methods on (a) CIFAR-10 and(b) ImageNet datasets. (c) shows the effect of the batch-shaping loss on our ResNet20 and ResNet32gated models trained on CIFAR-10. It also presents the effect of increasing the network’s width.
Figure 4: Images with highest MAC usage (top row) and lowest MAC usage (bottom row) from theCityscapes datasets. The network uses more features for difficult examples and fewer features forsimple examplesperformance of our gated network further reaches an IoU of 0.747 and pixel accuracy of 0.948 using95% of the PSPNet MAC count (λ = 0.05). Figure 4 shows example images from the cityscapesdataset consuming the highest and lowest MAC counts.
Figure 5: The distribution of different gate activation patterns in our ResNet34-L0 and ResNet34-BASmodels trained on ImageNet while inducing 60% sparsity. Gates are categorized as always on/off, ifthey are on/off for more than 99% of the inputs.
Figure 6: The histogram shows how often individual filters are executed in each layer of a ResNetblock (column). For illustration purposes, filters are sorted by execution frequency over the entirevalidation set. This histogram is computed for our ResNet34-BAS model.
Figure 7: Illustration of image categories that activate individual gates in different layers of aResNet34-BAS. For visualization, we only considered gates that are barely on as they are morespecialized and activate for a more specific subset of categories. (a-c) show all possible categories thatactivate individual gates in the 5th, 10th, and 12th gated ResNet blocks of this model, respectively.
Figure 8: Comparison of the effect of the batch-shaping loss and L0 loss on our ResNet18 andResNet34 gated models trained on ImageNetFor batch-computing, a custom convolutional kernel could use the calculated mask on the fly. Wesimulated this for the GPU in the same table. Custom hardware could naturally give us benefits fromall MACs saved, including energy savings. All the reported inference times were measured using amachine equipped with an Intel Xeon E5-1620 v4 CPU and an Nvidia GTX 1080 Ti GPU.
