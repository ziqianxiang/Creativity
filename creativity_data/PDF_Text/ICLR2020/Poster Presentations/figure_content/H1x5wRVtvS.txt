Figure 1: Illustration of (a) VHE, (b) StaCkGAN++, (c) raster-scan-GAN, (d) Vanilla-GAN, and (e) simple-raster-sCan-GAN. VHE-raster-scan-GAN consists of (a) and (c). χψd is down-sampled from X With scalingfactor d. VHE-StackGAN++, consisting of (a) and (b), VHE-Vanilla-GAN, consisting of (a) and (d), andVHE-SimPle-raster-scan-GAN, consisting of (a) and (e), are all used for ablation studies.
Figure 2: Comparison on image generation given texts from CUB, Flower, and COCO. Shown in the top roware the textual descriptions and their associated real images; see Appendix C.2 for higher-resolution images.
Figure 3: Example results of VHE-raster-scan-GAN on three different tasks: (a) image generation given fivetextual attributes; (b) image generation given a long class-specific document (showing three representativesentences for brevity) from CUB; and (c) latent space interpolation for joint image-text generation on CUB (leftcolumn) and Flower (right column), where the texts in the first and last row are given.
Figure 4: Visualization of example semantic and visual concepts captured by a three-stochastic-hidden-layerVHE-raster-scan-GAN from (a) Flower, (b) Bird, and (c) COCO. In each subplot, given the real text tn shown atthe bottom, we draw {θn }3=1 via Gibbs sampling; we show the three most active topics in Φ(l) (ranked by theweights of θɑ)) at layer l = 3, 2,1, where each topic is visualized by its top three words; and we feed {θf) }3=1into raster-scan-GAN to generate three random images (one per layer, coarse to fine from layers 3 to 1).
Figure 5: An example topic hierarchy learned on COCO and its visual representation. We sample θ/"〜q(θn1:3) | Φ, Xn) for all n; for topic node k of layer l, We show both its top words and the top two images rankedby their activations θnk.
Figure 6: Example results of using VHE-raster-scan-GAN for (a) image-to-teXtUal-tags generation, where thegenerated tags highlighted in red are included in the original ones; (b) image-text-pair generations (columnsfrom left to right are based on Flower, CUB, and COCO, respectively).
Figure 7: Generated random images by VHE-raster-scan-GAN conditioning on five binary attributes.
Figure 8: Image generation conditioning on long encyclopedia documents using VHE-raster-scan-GAN trained on (a) CUB-E and (b) Flower. Shown in the top part of each subplot are representativesentences taken from the long document that describes an unseen class; for the three rows of imagesshown in the bottom part, the first row includes three real images from the corresponding unseenclass, and the other two rows include a total of six randomly generated images conditioning on thelong encyclopedia document of the corresponding unseen class.
Figure 9: Example results of facial image generation conditioning on five textual attributes, byVHEStackGAN++ and VHE-raster-scan-GAN trained on the CelebA dataset. Both models aretrained with 20 epochs, with the output resolution set as 128 X 128. Note our current networkarchitecture, designed mainly for natural images, has not yet been fine-tuned for facial images.
Figure 10: The images above the blue line are the larger-size replots of CUB Bird images in Figure 2,while the images below the blue line are results for ablation study.
Figure 11: The images above the blue line are the larger-size replots of Flower images in Figure 2,while the images below the blue line are results for ablation study.
Figure 12: The images above the blue line are the larger-size replots of COCO images in Figure 2,while the images below the blue line are results for ablation study.
Figure 13: Example text-to-image generation results on COCO.
Figure 14: Example of latent space interpolation on CUB.
Figure 15: Example of latent space interpolation on CUB.
Figure 16: Example of latent space interpolation on CUB.
Figure 17: Example of latent space interpolation on Flower.
Figure 18: Example of latent space interpolation on Flower.
Figure 19: Top-5 retrieved images given a text query. Rows 1 to 3 are for Flower, CUB, and COCO, respectively.
Figure 20: Example results of image regeneration using VHE-StackGAN++ and VHE-raster-scan-GAN. Anoriginal image is fed into the VHE image encoder, whose latent representation is then fed into the GAN imagegenerator to generate a corresponding random image. The models in columns 1-4 are trained on Flower, columns5-8 on CUB, and columns 9-12 on COCO.
Figure 21:	An example topic hierarchy taken from the directed acyclic graph learned by a three-layer VHE-raster-scan-GAN of size 256-128-64 on Flower.
Figure 22:	Analogous plot to Fig. 21 on CUB.
Figure 23:	Analogous plot to Fig. 21 on COCO.
Figure 24: The architecture of VHE in VHE-StackGAN++ and VHE-raster-scan-GAN.
Figure 25: The structure of StaCk-GAN++ in VHE-StaCkGAN++, where JCU is a type of discrimina-tor proposed in Zhang et al. (2017b).
Figure 26: The structure of raster-sCan-GAN in VHE-raster-scan-GAN, where JCU is a type of discriminatorproposed in Zhang et al. (2017b).
Figure 27: The generated random images of Obj-GAN given text lack diversity.
