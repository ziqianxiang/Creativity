Figure 1: Network with manifold constrained weights are relatively insensitive to the choice ofinitial weight scaling q*: We compare training loss and test accuracy for Euclidean, Stiefel andOblique networks with two different values of q*. The manifold constrained networks minimize thetraining loss at approximately the same rate, being faster than both Euclidean networks. Despitethis, there is little difference between the test accuracy of the Stiefel and Oblique networks and theEuclidean networks initialized with q* = 1/64. Notably, the latter attains a marginally higher test setaccuracy towards the end of training.
Figure 2: Left: at initialization the maximum curvature of the loss landscape (measured by the λmaxof the Fisher correlates highly (ρ = 0.65) with the maximum squared singular value of the JacobianJhg. The choice of choice of the preactivation variance, q* affects not only the conditioning of thegradients but also the gradient Lipschitz constant. Right: we verify this separately for mean-squarederror, for which a strictly monotonic relationship between q* and λmax and Jxh0g is predicted, giventhat the Hessian of the output layer is the identity. This is corroborated by a correlation coefficient ofρ = 0.81.
Figure 3: For manifold constrained networks, gradient smoothness is not predictive of optimizationrate. Euclidean networks with a low initial λmax (G) rapidly become less smooth, whereas Euclideannetworks with a larger λmax(G) remain relatively smoother. Notably, the Euclidean network withq* = 1/64 has almost an order of magnitude smaller λmax(G) than the Stiefel and Oblique networks,but reduces training loss at a slower rate.
Figure 4: Distribution of λmax(Hg) as a function of q*: In general, increasing the variance of thedistribution of hg does not result in a monotonic increase in the spectral radius of the Hessian of theGLM layer. We plot the distribution of the maximum eigenvalues as a function of the variance of thesoftmax layer obtained from factorizing 10,000 random matrices.
Figure 5: Training performance for networks with q* = 2 and q* = 8. The behavior of the trainingloss as well as the validation accuracy is consistent with the observations that for a large rangeof parameters q* the manifold constrained networks are insensitive to initialization and gradientsmoothness.
Figure 6: Linear neural networks with orthogonal initializations increase rapidly in their maxi-mum eigenvalue of the empirical Fisher information, similar to their non-linear, nearly isometriccounterparts.
