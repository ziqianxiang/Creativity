Figure 1: Posterior distribution of language IDs for the outputs from different models. Each transla-tion is represented as a point inside the simplex ∆2 = {(pde,pes,pfr)|pk ∈ (0, 1),pde+pes+pfr = 1}where pk is the estimated probability of being translated into language k ∈ (de, es, fr). We distin-guish the language that has the largest probability with different colors.
Figure 2: A sampled pair together with its real target from the distilled data of the base-AT model.
Figure 3: Complexity C(d) (↑ more complex), faithfulness F(d) (] more faithful), training BLEU,and reordering score (↑ more monotonic alignment) of different distilled sets of WMT14-ENDE.
Figure 4: The performance of NAT models of varying capacity trained on both the real and thedistilled data from tiny, small, base and big AT models on WMT14-ENDE newstest 2014 test sets.
Figure 5: Reborn experiments: (from left to right) performance of the base AT model, performanceof the vanilla NAT model, C(d) and F(d) of distilled data sets. R-i denotes the i-th reborn iteration.
Figure 6: MoE experiments: (from left to right) performance of the base AT model, performance ofthe vanilla NAT model, C(d) and F (d) of distilled data sets w.r.t the number of experts.
Figure 7: Density of conditional entropy C(d) of each sentence pairs in different distilled data setsand the real data.
Figure 8: The performance of variant measure (BLEU ↑, METEOR ↑, RIBES ↑, ChrF ↑, TER LBEER ↑) for the vanilla NAT model trained on the distilled data from tiny, small, base and big ATmodels on WMT14-ENDE newstest 2014 test sets.
