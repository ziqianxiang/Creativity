Figure 1: The orientation tilt-illusion (O,Toole & Wenderoth, 1977) is a contextual illusionwhere a central grating,s perceived orientation is influenced by a surround grating,s orienta-tion. (a) When a central grating has a similar orientation as its surround, it is judged as tilting awayfrom the surround (repulsion). When the two gratings have dissimilar orientations, the central gratingis judged as tilting towards the surround (attraction). (b) We extend the recurrent circuit proposedby Mely et al. (2018) to explain this and other contextual illusions into a hierarchical model thatlearns horizontal (within a layer) and top-down (between layer) interactions between units. The(S)	(F )、circuit simulates dynamical suppressive (Hxyk) and facilitative (Hxyk) interactions between units ina layer ', which receives feedforward drive from a center pathway encoding feature k (e.g., edgesoriented at 0° or 22.5°) at position (x, y) in an image. Blocks depict different layers, and arrowedconnections denote top-down feedback. (C) A deep network schematic of the circuit diagram in(b), which forms the basis of the Y-Net introduced here. Horizontal and top-down connections areimplemented with feedback gated recurrent units (fGRUs). Image encodings pass through theseblocks on every timestep, from bottom-up (left path) to top-down (right path), and predictions areread out from the fGRU closest to image resolution on the final timestep. This motif can be stackedto create a hierarchical model.
Figure 2: Object contour detection in BSDS500 images. (a) The γ-Net is on par with humans andthe state-of-the-art for contour detection (BDCN; He et al. 2019) when trained on the entire trainingdataset with augmentations. In this regime, it also outperforms the published F1 ODS of all otherapproaches to BSDS500 (LPCB: Deng et al. 2018, RCF: Liu et al. 2019, CED: Wang et al. 2019, DB:Kokkinos 2015, HED: Xie & Tu 2017, and OEF: Hallman & Fowlkes 2015). Theγ-Netoutperformsthe BDCN when trained on 5%, 10%, or 100% of the dataset. Performance is reported as F1 ODS(Arbelaez et al., 2011). (b) BDCN and Y-Net predictions after training on the different proportions ofBSDS500 images. (c) The evolution of γ-Net predictions across timesteps of processing. Predictionsfrom a γ-Net trained on 100% of BSDS are depicted: its initially coarse detections are refined overprocessing timesteps to select figural object contours.
Figure 3: Membrane prediction in serial electron microscopy (SEM) images of neural tissue.
Figure 4: Optimizing for contour detection produces an orientation-tilt illusion in the γ-Net.
Figure 5: Contour detection performance of the Y-Netdepends on an onentatιon-tιlt illusion.
Figure S1: We trained the reference 3D U-Net from (Lee et al., 2017) on the SNEMI3D dataset tovalidate the implementation. Segmentations here are derived by watershedding and agglomerationwith GALA (Nunez-Iglesias et al., 2013), resulting in “superhuman” ARAND (evaluated accordingto the SNEMI3D standard; lower is better) of 0.04, which is below the reported human-performancethreshold of 0.06 and on par with the published result (see Table 1 in Lee et al. 2017, mean affinityagglomeration).
Figure S2: Examples of tilt-illusion stimuli. (a) For training images, we sample over a range of sizeand wavelength to generate single oriented grating patches. (b) Test images are obtained by samplinga full range of surround orientation, while fixing all other parameters such as size and frequency ofgratings as well as the orientation of the center gratings (at 45 degrees).
Figure S3: Searching over learning rates did not rescue BSDS performance on small BSDS datasetshad no affect on performance. over learning rates did not rescue from overfitting. The left panel depictstraining and validation losses for BSDS on different sized subsets of BSDS500. (b) Performance aftertraining with three different learning rates on the 5% split. There is little difference in best validationperformance between the three learning rates. (c) The full training and validation loss curves for theBDCN trained on 5% of BSDS. The model overfits immediately. The model also overfit on the otherdataset sizes, but because there was more data, this happened later in training.
Figure S4: Performance OflesiOned Y-Nets. We evaluate the F1 ODS of these models on BSDS500test images (left) and test for the presence of an orientation-tilt illusion (right). Top row: A com-parison of γ-Nets with “untied weights” (i.e., unrolled to 8-timesteps with unique weights on ev-ery timestep), only top-down connections, only horizontal connections (using the standard 3×3horizontal kernels), and only horizontal connections but with larger kernels (15× 15). The full Y-Net outperformed each of these models on all subsets of BSDS500 data. Both horizontal-only modelscaptured the repulsive regime of the orientation-tilt illusion when center and surround gratings weresimilar, but only the version with larger kernels also showed an attractive regime, when the center andsurround orientations were dissimilar. Bottom row: A comparison of Y-Nets with no recurrence (i.e.,one timestep of processing), no constraint for non-negativity (see fGRU formulation in main text), noparameters for additive feedback (μ and κ), and no parameters for multiplicative feedback (γ and ω).
Figure S5: Y-Nets trained for contour detection learn to approach a steady-state solution. Theprocessing timecourse of Y-Netpredictions on representative images from the BSDS500 test set. TheL2 norm of per-pixel differences between predictions on consecutive timesteps (i.e., timestep 2 -timestep 1) approaches 0, indicating that the model converges towards steady state by the end of itsprocessing timecourse.
Figure S6: Performance of Y-Nets during experiments to correct an orientation-tilt illusion. Theillusion-corrected model was trained to have veridical representations of the central grating intilt-illusion stimuli. To control for potential detrimental effects of the training procedure per se,a control model (“domain-transfer control”) was trained to decode orientations of single-gratingstimuli. (a) Training causes contour-detection performance of both models to drop. However, theillusion-corrected model performance drops significantly more than the biased model (see main textfor hypothesis testing). The losses for both models converge towards 0 across training, indicatingthat both learned to decode central-orientations of their stimuli. (b) Contour detection examples forbiased and bias-corrected models across steps of this training procedure.
Figure S7: Differences in contour predictions for the illusion-corrected and domain-transfer controlY-Netson BSDS500.
