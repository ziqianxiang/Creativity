Figure 1: Illustration of the computation of the recall metric RBERT. Given the reference X andcandidate X, We compute BERT embeddings and pairwise cosine similarity. We highlight the greedymatching in red, and include the optional idf importance weighting.
Figure 2: BERTSCORE visualization. The cosine similarity of each word matching in PBERT arecolor-coded.
Figure 3: Pearson correlation of FBERT computed with different models, across different layers, withsegment-level human judgments on the WMT16 to-English machine translation task. The WMT17English-Chinese data is used for the BERT Chinese model. Layer 0 corresponds to using BPEembeddings. Consistently, correlation drops significantly in the final layers.
