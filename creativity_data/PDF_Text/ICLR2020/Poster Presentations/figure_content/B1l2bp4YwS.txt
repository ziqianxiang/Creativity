Figure 1:	Accuracy as a function of GNNmp capacity dw and n. (Best seen in color.)d ∈ (5, 10, 20, 15). To reduce the dependence on the initial conditions and training length, for eachhyperparameter combination, I trained 4 networks independently (using Adam and learning ratedecay) for 4000 epochs. The GNNmp chosen was that proposed by Xu et al. (2018), with the additionof residual connections—this network outperformed all others that I experimented with.
Figure 2:	(a) GNNs are significantly more powerful when given discriminative node attributes. (b) Testaccuracy indicated by color as a function of normalized depth and width. Points in highlighted areascorrespond to networks with super-critical capacity, whereas the diagonal line separates networksthat more deep than wide. (For improved visibility, points are slightly perturbed. Best seen in color.)striking to observe that even the most powerful networks considered could not achieve a test accuracyabove 95% for n > 16; for n = 40 their best accuracy was below 80%.
Figure 3: Examples of graphs giving rise to lower bounds.
Figure 4: Toy example of message exchange from the perspective of the red node. The arrows show where eachreceived message comes from and the message content is shown in light gray boxes. Red’s knowledge of thegraph topology is depicted at the bottom.
