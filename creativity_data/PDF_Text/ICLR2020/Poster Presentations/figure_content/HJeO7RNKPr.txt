Figure 1: DeePV2D predicts depth from video. It is the composition of classical geometric algo-rithms, made differentiable, and combined into an end-to-end trainable network architecture. Videoto depth is broken down into the subproblems of motion estimation and depth estimation, which aresolved by the Motion Module and Depth Module respectively.
Figure 2: The Depth Module performs stereo matching over multiple frames to estimate depth.
Figure 3: The Motion Module updates the input Pose estimates by solving a least squares optimiza-tion problem. The motion module predicts the residual flow between pairs of frames, and uses theresidual terms to define the optimization objective. Pose increments Î¾ are found by performing asingle differentiable Gauss-Newton optimization step.
Figure 4: Visualization of predicted depth maps on NYU, ScanNet, and SUN3D. On ScanNet andSUN3D (marked with *) we show the results of the model trained only on NYU data.
Figure 5: Impact of the number of iterations (left) and frames (right) on sc-inv validation accuracy.
Figure 7: Additional results on the NYU depth dataset Silberman et al. (2012) using 7-frame videoclips. We show results compared with Laina et al. (2016) and Ummenhofer et al. (2017).
Figure 8:	Motion Module Architecture: The Encoder(left) extracts a dense 1/4 resolution featuremap for each of the input images. The Residual Flow Network (right) takes in a pair of feature mapsand estimates the residual flow and corresponding weights. This residual flow is estimated with anencoder-decoder network, with skip connections formed by concatenating feature maps. Numbersin parenthesis correspond to the number of output channels for each layer.
Figure 9:	Depth Module Architecture: The 2D encoder (top) is applied to each image in the videosequence. The 2D Encoder consists of a series of residual convolutions and 2 Hourglass Networks.
