Figure 1: We formalize the problem of budgeted training, in which one maximizes performancesubject to a fixed training budget. We find that a simple and effective solution is to adjust the learningrate schedule accordingly and anneal it to 0 at the end of the training budget. This significantlyoutperforms off-the-shelf schedules, particularly for small budgets. This plot shows several trainingschemes (solid curves) for ResNet-18 on ImageNet. The vertical axis in the right plot is normalizedby the validation accuracy achieved by the full budget training. The dotted green curve indicates anefficient way of trading off computation with performance.
Figure 2: We normalize various learning rate schedules by training progress (left). Our solution tobudgeted training is simple and universal — we decrease the learning rate linearly across the entiregiven budget (right).
Figure 3: Budgeted convergence: full gradient norm ||g；|| vanishes over time (color curves) aslearning rate αt (black curves) decays. The first row shows that the dynamics of full gradient normcorrelate with the corresponding learning rate schedule while the second row shows that such phe-nomena generalize across budgets for budget-aware schedules. Such generalization is most obviousin plot (h), which overlays the full gradient norm across different budgets. If a schedule does notdecay to 0, the gradient norm does not vanish. For example, ifwe train a budget-unaware exponen-tial schedule for 50 epochs (c), the full gradient norm at that time is around 1.5, suggesting this is aschedule with insufficient final decay of learning rate.
Figure 4: Comparing AMSGrad (Reddi et al., 2018)with linear schedule. (a) while AMSGrad makes fastinitial descent of the training loss, it is surpassed ateach given budget by the linear schedule. (b) bud-geted convergence is not observed for AMSGrad —the full gradient norm ∣∣gt∣∣ does not vanish (colorcurves). Comparing to a momentum SGD, AMS-Grad recommends magnitudes larger learning rate βt(black curve).
Figure 5: Comparing SGDR (Loshchilov & Hut-ter, 2017) with linear schedules. (a) SGDR makesslightly faster initial descent of the training loss, butis surpassed at each given budget by the linear sched-ule. (b) for SGDR, the correlation between full gradi-ent norm ∣∣gt∣∣ and learning rate at is also observed.
Figure A: Comparison between budget-aware linear schedule and adaptive learning rate methods onCIFAR-10. We see while adaptive learning rate methods appear to descent faster than full budgetlinear schedule, at each given budget, they are surpassed by the corresponding linear schedule.
Figure B: One issue with off-the-shelf SGDR (T0 = 10, Tmult = 2) is that it is not budget-aware andmight end at a poor solution. We convert it to a budget aware schedule by setting it to restart n timesat even intervals across the budget and n = 2 is shown here (SGDR-r2).
Figure C: Training loss and validation accuracy for training ResNet-18 on CIFAR-10 using stepdecay and linear schedule. No generalization gap is observed when we only modify learning rateschedule. This figure provides details for the discussion of “don’t waste budget”.
Figure D: The corresponding weight norm plots for Fig 3 and Fig 5. We find that the weight normexhibits a similar trend as the gradient norm.
