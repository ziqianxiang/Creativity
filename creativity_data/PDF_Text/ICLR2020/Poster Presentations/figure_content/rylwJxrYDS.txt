Figure 1: (a) The vq-wav2vec encoder maps raw audio (X) to a dense representation (Z) whichis quantized (q) to Z and aggregated into context representations (C); training requires future timestep prediction. (b) Acoustic models are trained by quantizing the raw audio with vq-wav2vec,then applying BERT to the discretized sequence and feeding the resulting representations into theacoustic model to output transcriptions.
Figure 2: (a) The Gumbel-Softmax quantization computes logits representing the codebook vectors(e). In the forward pass the argmax codeword (e2) is chosen and for backward (not shown) theexact probabilities are used. (b) K-means vector quantization computes the distance to all codewordvector and chooses the closest (argmin).
Figure 3: Comparison of PER on the TIMIT dev set for various audio codecs and vq-wav2veck-means trained on Librispeech 100h.
