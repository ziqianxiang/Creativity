Figure 1: In transformation-based defenses, the image is transformed stochastically where eachsample tx is drawn from the distribution T (x) and then fed to the CNN (blue box). In our defensemethod, for each input image x, we build the marginal distribution of softmax probabilities fromthe transformed samples tXI), tX2), ∙ ∙ ∙. The distributions are fed to a separate distribution ClaSSi-fier which performs the final classification. Note that our distribution classifier is trained only ondistributions obtained from clean images while tested on both clean and adversarial images.
Figure 2: MNIST dataset: (a) Effect of number of pixel deflections (d) on the voting predictions ofclean and adversarial images for MNIST digits 6 and 8. The numbers represent the percentage ofimages predicted to the various classes, eg. for class label 8, 78% of the transformed clean imagesare predicted correctly. For class label 8, with the image transformation, there is recovery on theadversarial images (24% recovery), but on the clean images, there is a deterioration in performanceas some images are misclassified. However, it is interesting that the misclassifications have the samevoting classes as the transformed adversarial images. Similar overlap in voting classes is observedfor digit 6. (b) Effect of increasing number of pixel deflections on the distances of distributionsobtained from clean and adversarial images. The standard error bars, taken over 3 random seeds forthe transformation, are smaller than the plot points. Best viewed in color.
Figure 3: An example image and its adversarial counterpart (ground truth: class 6) undergoingincreasing number of pixel deflections (d). The clean image gets misclassified by voting as thedistribution mass shifts to the incorrect class. Note that for the purpose of visualization, insteadof the per-class marginal distributions of the softmax, here we show the joint distribution over thesoftmax values of classes 5 and 6. The softmax values of the other 8 classes are not shown.
Figure 4: Distributions of the softmax obtained from 8 selected clean images of MNIST digit 6(A -H) and their adversarial counterparts (A-H) at d=300. Here We show the joint distribution ofthe softmax at class 5 and 6. The clean and adversarial images misclassified by voting show similarλ∙jdistribution shapes, as indicated by the groupings and arrows, eg. between E and A, E, F, betweenF and B and so on.
Figure 5: MNIST, CIFAR10 and CIFAR100 results: For each attack and transformation-based de-fense, we compare the clean and adversarial (adv.) test accuracies with baseline majority voting andthe three distribution classifier methods. The distribution classifiers generally show improvementsover voting. Best viewed in color.
Figure 6: MNIST dataset: (a) Example of two clean images with class label 5, where the distributionshapes look similar to C and G in Figure 4 but the distribution classifier can still discriminate be-tween the distributions from class labels 5 and6 and classify them correctly. (b) On the clean images,both voting and DRN accuracies improve with more number of transformed samples. As number ofsamples increases, voting saturates while DRN continues to improve. (c) On the adversarial images,the accuracies stay more of less the same.
Figure 7: Black-box boundary attack on MNIST and CIFAR10, with the average L2 of the attackperturbation shown, with the bars denoting the standard error of the means. Note that the adversarialimages for vote, DRN and DRN LAT are of lower quality.
Figure 8:	Effect of increasing number of pixel deflections on the distances of distributions obtainedfrom clean and adversarial images of the MNIST dataset. Best viewed in color.
Figure 9:	Adversarial examples where pixel deflection with voting recovers from adversarial attack,where ground truth label is 6 but CNN predicts as 0.
Figure 10: Adaptation of DRN for the distribution classifier task.
