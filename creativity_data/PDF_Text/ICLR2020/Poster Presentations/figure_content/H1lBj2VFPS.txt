Figure 1: An overview of LLSQ: using pre-trained weights for fast convergence; Retraining of thenetwork with quantized weights and activations; BN fusion for efficient inference; Quantization ofbias and scaling factors; Deployment of the quantized model to our accelerator. As shown in thisfigure, weights, activations, bias, and scaling factors are quantized to low-bit integers. And thebandwidth of accumulator can be set to lower (e.g., 16-bit in our experiments).
Figure 2: L2 distance of quantization. Thedata is from weights of the first FC layer inAlexNet. As shown in the figure, the optimalÎ±* changes with the updating of weights.
Figure 3: Quantization results on different networks.
Figure 4: Distribution of the bias/scaling factor. The data is from VGG-Small with w4a4 quantiza-tion.
Figure 5: Test curves for AlexNet, ResNet18, and ResNet34 on ImageNet.
