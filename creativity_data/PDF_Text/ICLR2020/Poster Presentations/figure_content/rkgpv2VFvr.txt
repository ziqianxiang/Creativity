Figure 1: (a) The architecture of the neural network we propose to Ieam T tasks simultaneously.
Figure 2: Discounted cumulative reward averaged over 100 experiments of DQN and MDQN foreach task and for transfer learning in the Acrobot problem. An epoch consists of 1, 000 steps, afterwhich the greedy policy is evaluated for 2, 000 steps. The 95% confidence intervals are shown.
Figure 3: Discounted cumulative reward averaged over 40 experiments of DDPG and MDDPG foreach task and for transfer learning in the Inverted-Double-Pendulum and Hopper problems. Anepoch consists of 10, 000 steps, after which the greedy policy is evaluated for 5, 000 steps. The 95%confidence intervals are shown.
