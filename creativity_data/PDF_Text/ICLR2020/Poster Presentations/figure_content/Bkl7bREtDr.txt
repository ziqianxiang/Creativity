Figure 1: Our AMRL-Max model compared to a standard LSTM memory module (Hochreiter &Schmidhuber, 1997) trained on a noise-free memory task (T-L, left) and the same task with observa-tional noise (T-LN, right). In both cases, the agent must recall a signal from memory after navigatingthrough a corridor. LSTM completely fails with the introduction of noise, while AMRL-Max learnsrapidly. (68% confidence interval over 5 runs, as for all plots.)We make the following three contributions. First, in Section 3, we introduce our approach, AMRL.
Figure 2: Model Architectures. AMRL (d) extends LSTMs (a) with SET based aggregators (c).
Figure 3: Overview of tasks used in our experiments: (a) Length-10 variant of TMaze (the full taskhas length 100); (b) a 3-room variant of MC-LS (Full Minecraft tasks have 10 or 16 rooms.); (c)MC-LSO; (d) original (MC-LS(O)) and (e) noisy (MC-LSN) Minecraft observation; (e) sampleoptimal trajectory (left-right, top-down) through a 1-room variant of MC-LSO.
Figure 4: TMaze Results (5 seeds): AMRL-Max and AMRL-Avg achieve superior performanceunder observation noise, exploration, and interference short-term tasks. Best viewed in color.
Figure 5: Minecraft results (5 seeds): AMRL-Avg and AMRL-Max outperform alternatives in termsof learning speed and final performance.
Figure 6: Gradient signal over 0-100 steps. AMRL models and SUM maintain the strongest gradient.
Figure 7: MAX models and DNC have greatest SNR. LSTM and LSTM.STACK perform worstwith exponential decay. SUM and AVG have only linear decay, confirming our analytic finding.
Figure 8: Overall Performance and Performance in relation to SNR and Gradient. Increasing eitherSNR or Gradient strength tends to increase performance. See text for details on the SUM model.
Figure 9: Results: T-LO (left) and T-LSO (right) experiments. These are included in overall perfor-mance but not discussed in the main body. Our results confirm that AMRL models maintain orderdependent memories while the SET ablation does not. Note: “ST” here is short for “AMRL”.
Figure 10: Top down map of Chicken environment (left) and a view from the agent near the start ofthe episode when the signal is green (center). Results are shown on the right.
Figure 11: (Left:) Decay of gradient in strong signal setting. (Right:) Decay of gradient when theinput is sampled from U{1, -1}. Results are surprisingly similar to non-noisy setting.
Figure 12: (Left:) SNR of various models in the weak signal setting. (Right:) SNR of models instrong signal setting defined in A.3. LSTM and LSTM_STACK fail when the signal is removed,since they are very dependent on being temporally close to the signal, instead of simply dependingon the number of prior signals. Again, MAX and DNC perform the best.
