Figure 1: Splitting an input feature I into Ihb (blue), the most-significant Bhb bits, and Ilb (orange),the remaining Blb bits. The total bitwidth is B .
Figure 2: The PG building block in CNN models - Input features are split into Ihb and Lb. Inthe prediction phase, Ihb first convolves with the full precision filters W to obtain Ohb. In theupdate phase, if the partial sum Ohb of a feature exceeds the learnable threshold âˆ†, we will updatethat feature to high-precision by adding Olb to Ohb. Otherwise, we skip the update phase, and theoutput feature therefore remains computed at ultra low-precision. The prediction and update phasesshare the same weights. denotes the Hadamard product.
Figure 3: Effect of clipping - A toy example illustrating how a clip threshold helps separating Pre-diction values apart. The first row is quantization and prediction without a clip threshold, while thesecond row has a clip threshold. (a) Distribution of floating-point input features I. (b) Distributionof I after quantizing I to 4 bits. (c) Distribution of Ihb which takes the higher 2 bits of I.
Figure 4: Precision gating (PG) results on CNNs and LSTM - compare PG against uniformquantization (UQ) and PACT.
Figure 5: Visualization of gating ratio - Top: feature maps from the final precision gating block inResNet-20 on CIFAR-10. Bottom: ratio of computing using a high-precision (brighter pixel meanshigher ratio). PG effectively identifies the location of the object of interest and increases bitwidthwhen computing in this region.
