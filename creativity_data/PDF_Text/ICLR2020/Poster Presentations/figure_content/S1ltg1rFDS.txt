Figure 1: (a) ModelWin MDP from Thomas & Brunskill (2016). (b) The RMSE of different methodsas we change the length of horizon w.r.t the target policy reward. IPS depends on the horizon lengthwhile our method is independent of the horizon length.
Figure 2: The RMSE of different methods w.r.t the target policy reward as we change the number oftrajectories. Our black-box approach outperforms other methods on three problems.
Figure 3: The median and error bars at 25th and 75th percentiles of different methods w.r.t the targetpolicy reward as we change the number of trajectories. The trend of results is similar to Figure 2.
Figure 4: Bias and standard deviation of different methods for the ModelWin MDP (Fig. 1). Ourmethod has a smaller bias but larger variance compared to other algorithms.
Figure 5: The RMSE of different methods w.r.t the target policy reward as we change the behaviorpolicy. Our method outperform other approaches on different behavior policies.
