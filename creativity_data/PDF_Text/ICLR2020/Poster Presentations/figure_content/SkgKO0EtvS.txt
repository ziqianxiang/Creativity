Figure 1: A visualisation of the relation between local computations of graph algorithms (left) andthe neural graph algorithm executor (right). In graph algorithms, node values y~i(t) (e.g. reachability,shortest-path distance) are updated at every step of execution. Analogously, the node values arepredicted by the neural executor from the hidden representation ~hi(t) computed via message passing.
Figure 2: Illustrating the alignment of one step of the Bellman-Ford algorithm (left) with one step ofa message passing neural network (right), and the supervision signal used for the algorithm learner.
Figure 3: The per-step algorithm execution performances in terms of reachability accuracy (left),distance mean-squared error (middle) and predecessor accuracy (right), tested on 100-node graphsafter training on 20-node graphs. Please mind the scale of the MSE plot.
Figure 4: Identified reachability paths for a noisy Caveman graph and tree graph, using GNNExplainer.
