Figure 1: Reliability diagrams of uncalibrated models. Probabilities are generated by a logisticsigmoid layer. The larger the deviation from the diagonal, the more uncalibrated is the model. Wepresent four different common loss functions used to train knowledge graph embedding models. (a)Uncalibrated TransE on WN11. (b) Uncalibrated ComplEx on FB13. Best viewed in colors.
Figure 2: Calibration plots for the best calibrated model-loss combinations. Isotonic regressiondelivers the best results, getting very close to the perfectly calibrated line, both when used with theground truth method or our proposed synthetic method. Best viewed in colors.
Figure 3: Synthetic calibration on FB15K-237 and WN18RR, with varying positive base rates. Thebaseline stands for using the positive base rate as the probability prediction. Results are evaluatedunder the closed-world assumption, using the same positive base rate used to calibrate the models.
Figure 4: Histograms show the total count instances for each bin used by calibration plots presentedin Figure 2. Best viewed in colors.
Figure 5: Impact of Î· (eta) and k (embedding size) on the Brier score. We used TransE and theSelf-Adversarial loss for all datasets. Best viewed in colors.
