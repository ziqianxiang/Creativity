Figure 1: Generated left and right images by our proposed Deep 3D Pan for an input center image.
Figure 2: Synthesis techniques based on adaptive convolutions. The background is the input image.
Figure 3: Our proposed global dilation (d) filter with a general cross shape.
Figure 4: Our proposed “t-shaped” kernels are overlaid on top of a center input image. The distancebetween samples (dilation) is adaptive according to the amount and direction of 3D panning to beapplied to the input image and the local 3D geometry of the scene.
Figure 5: Disparity (Dp) and occlusion (Op) maps generated from the proposed "t-shaped” kernel.
Figure 6: Our t-net architecture. The t-net estimates the kernel values and the dilation weights usedfor the local adaptive t convolutions with global and local adaptive dilation.
Figure 7: (a) Shifted-LR versions of the center-view contain different information as they are sam-pled from different groups of pixels via bilinear interpolation depending on the stride (controlled bythe maximum disparity). (b) Our light sr-block. All convs have 3x3 kernels otherwise specified.
Figure 8: Comparison against the state-of-the-art methods for stereoscopic view synthesis.
Figure 9: Effect of feeding different values of Pa while keeping the input image unchanged. Differ-ent values of Pa generate different occlusions and different holes in disparities (see red and yellowboxes), which indicate the occluded regions in the target panned image.
Figure 10: Disparity refinement blockFigure 11: Our novel special post-processing step (spp)(a) Primitive disparity map Dp (b) Refinement block (c) Naive post processing (pp) (d) Our post processing (spp)Figure 12: Primitive disparity and different refinement options.
Figure 11: Our novel special post-processing step (spp)(a) Primitive disparity map Dp (b) Refinement block (c) Naive post processing (pp) (d) Our post processing (spp)Figure 12: Primitive disparity and different refinement options.
Figure 12: Primitive disparity and different refinement options.
Figure 13: Results on the CityScapes dataset. Our method trained on KITTI-only (K), generalizesvery well on the unseen images with an improvement over 3dB against the Deep3D baseline.
Figure 14: Results on the VICLAB_STEREO (VL) dataset. The monster-net trained on theK+CS+VL datasets achieves better structures in homogeneous areas (highlighted in red).
Figure 15: Our models generating 3D panned views at 30% beyond the baseline for the leftwardand rightward camera panning. The magnification of the figures helps to better compare betweenthe fixed local dilation monster-net and adaptive local dilation monster-net.
Figure 16: Ablation studiestradeoff” (Blau & Michaeli, 2018) which suggests that better synthetic looking images not alwaysyield higher PSNR/SSIM.
Figure 17: Qualitative comparison between our method and the SOTA for the unsupervised monoc-ular depth estimation task on the KITTI2015 dataset.
