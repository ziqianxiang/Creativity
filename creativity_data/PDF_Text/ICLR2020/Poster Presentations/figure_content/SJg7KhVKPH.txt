Figure 1: Training regimes for decoder networks able to emit outputs at any layer. Aligned trainingoptimizes all output classifiers Cn simultaneously assuming all previous hidden states for the currentlayer are available. Mixed training samples M paths of random exits at which the model is assumedto have exited; missing previous hidden states are copied from below.
Figure 2: Variants of the adaptive depth prediction classifiers. Sequence-specific depth uses a multi-nomial classifier to choose an exit for the entire output sequence based on the encoder output s (2a).
Figure 3:	Trade-off between speed (average exit or AE) and accuracy (BLEU) for depth-adaptivemethods on the IWSLT14 De-En test set.
Figure 4:	Effect of the hyper-parameters σ and λ on the average exit (AE) measured on the valid setof IWSLT’14 De-En.
Figure 5:	Speed and accuracy on the WMT’14 English-French benchmark (c.f. Figure 3).
Figure 6: Examples from the WMT'14 En-Fr test set (newstest14) with Tok-LL geometric-like depthestimation. Token exits are in blue and confidence scores are in gray. The ‘@@' are due to BPE orsubword tokenization. For each example the source (Src) and the reference (Ref) are provided inthe caption.
Figure 7: Example from the IWSLT’14 De-En teSt Set with Tok-LL geometric-like depth eStimation.
Figure 8: WMT'14 En-Fr test set: exit distributions in the beginning (relative-position: rpos<0.1)and near the end (rpos>0.9) of the hypotheses of three models.
Figure 9: Joint histogram of the exits and the confidence scores for 3 Tok-LL geometric-like modelson newstest14.
Figure 10: Illustration of gradient scaling.
