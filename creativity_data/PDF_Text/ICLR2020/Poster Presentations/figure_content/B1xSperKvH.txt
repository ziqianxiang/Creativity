Figure 1: Surrogate gradient of the spiking neuronactivation function (Eq. 11). α = 0.3, β = 0.01.
Figure 2: Residual architecture forSNNIn this section, we describe the changes made to the VGG(Simonyan & Zisserman, 2014) and residual architecture (Heet al., 2016) for hybrid learning and discuss the process ofthreshold computation for both the architectures.
Figure 3: Average number of spikes for each layer in a VGG16 architecture for purely convertedSNN and SNN trained with hybrid technique. The converted SNN and SNN trained with hybridtechnique achieve an accuracy of 89.20% and 91.87%, respectively, for the randomly selected 1500samples from the test set. Both the networks were inferred for 100 time steps and ‘v’ represents thethreshold voltage for each layer obtained during the conversion process (Algorithm 1).
Figure 4: Linear and Exponential approximationof the gradient of the spiking neuron (stepfunction).
Figure 5: Training and Inference time and memory for ANN, SNN trained with backpropagationfrom scratch, and SNN trained with hybrid technique. All values are normalized based on ANNvalues. The y-axis is in log scale. The performance was evaluated on one Nvidia GeForce RTX2080 Ti TU102 GPU with 11 GB of memory. All the networks were trained for VGG16 architecture,CIFAR10 dataset, 100 time steps, and mini-batch size of 32. ANN and SNN require 250 epochs oftraining from scratch, hybrid conversion-and-STDB based training requires 250 epochs of ANNtraining followed by 20 epochs of spike-based backpropagation.
