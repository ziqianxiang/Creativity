Figure 1: Performance of hybrid models constructed by first taking BERTBASE, a 12 layer Trans-former model, and replacing the self-attention layers with depth-wise separable convolution layers,in a varying number of the Transformer blocks closer to the input. Surprisingly, replacing 1 or 2self-attention layers with convolutions improves the performance, while replacing more hurts theperformance. This suggests both that Transformers have functionality beyond just computing con-textual mappings, and having simpler layers to realize contextual mapping can aid Transformers.
