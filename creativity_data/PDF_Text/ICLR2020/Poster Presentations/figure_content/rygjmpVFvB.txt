Figure 1: Complement points (in Green) betweentwo circles (in Orange).
Figure 2: Boundary points (in Green) betweenfour circles (in Orange).
Figure 3: Illustration of the generation of the un-seen data in the boundary around the training data.
Figure 4: Illustration ofthe difference-set seek-ing in MNIST.
Figure 5: DSGANlearns the difference be-tween two sets.
Figure 6: Illustration for designing Pdd.
Figure 7: Influence of α on the synthetic dataset. We observe that the samples of pg (green points)move farther away from Pd as α increases; however, they are still bounded by the support of pχ.
Figure 8: Comparison of the reconstructed results of the VAE and our method. The seen class, whichis at the bottom of the images, is a car. Other rows are images from the unseen classes. Our methodexhibits a relatively larger gap, in terms of the reconstruction error between the seen data and unseendata, than the VAE.
Figure 9: Illustration of the differences between traditional GAN and DSGAN.
Figure 10: Sampled generated images of GAN and DSGAN on CelebA.
