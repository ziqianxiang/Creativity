Figure 1: Left: After learning Task 2, the learner has already forgetten how to solve Task 1. Thisis “catastrophic forgetting”. Middle: The basic idea of the data-based LLL approach. A generatoris learned to generate examples it has seen before. Using the generator, the learner also learnsfrom examples from the previous task to prevent it from forgetting. Right: A language model thatsimultaneously takes on the roles of learner and generator.
Figure 2: Upper: LM learns to answer question given context. Lower: LM learns to generatetraining samples given generation token GEN.
Figure 3: Training progress of five tasks. Thegraph records the performance of the modelat each epoch of each task.
Figure 4: Performance after each epoch underfive different sampling ratios, with or without taskspecific-specific tokens.
Figure 5: Overview of the forgetting progress for different methods and permuted orders. The blueline indicate the scores of the first task after training each task. The orange line corresponds to thatof the second task.
Figure 6: Training progress of the five tasks with reverse order, i.e. small to large. The graphrecords the performance of the model at each epoch of each tasks. The order of tasks in the progressfollows: WOZ, QA-SRL, SST, WikiSQL, and then SQuAD. From the figure, we can clearly see thatFine-tune and MAS completely forget WOZ and WikiSQL.
