Figure 1: For a quadratic function f(x, y) =-3x2+4xy-y2, our algorithm moves closerto the ridge every iteration and it moves alongthe ridge once it hits the ridge. Without theFR correction term, gradient dynamics candrift away from the ridge.
Figure 3: Trajectory of FR and other algorithms in low dimensional toy problems. Left: for g1 , (0, 0) is localminimax. Middle: for g2, (0, 0) is NOT local minimax. Right: for g3, (0, 0) is a local minimax. The contoursare for the function value. The red triangle marks the initial position.
Figure 4:	Comparison between FR and other algorithms on GANs with saturating loss. First Row: Generatordistribution. Only consensus optimization (CO) and FR capture all three modes. Second Row: Discriminatorprediction. The discriminator trained by FR converges to a flat line, indicating being fooled by the generator.
Figure 5:	Comparison between FR and GDA on 2D mixture of Gaussians. Left: GDA; Right: FR.
Figure 6: Path-norm and path-angle of FRalong the linear path.
Figure 7: Gradient norms of GDA and FR.
Figure 8: Comparison between FR and GDA on MNIST dataset. Left: GDA; Right: FR.
Figure 9: Ablation study on the effect of preconditioning. Vanilla FR also converges at the end of trainingthough it takes much longer. The KDE plots use Gaussian kernel with bandwidth 0.1.
Figure 10: Left: distance to the origin for GDA, GDA with 0.2 momentum, FR, FR with 0.5 momentum andFR with 0.8 momentum; Right: trajectory of each algorithm; we plot the values of x1 and y1 and the contourfor the function value on the plane (x1 , 0, y1 , 0).
Figure 11: Empirical investigation on the effect of positive momentum. With larger momentum coefficient (e.g.,Î³ = 0.9), the convergence ofFR gets further improved. The KDE plots use Gaussian kernel with bandwidth 0.1.
Figure 12: Top-20 eigenvalues.
