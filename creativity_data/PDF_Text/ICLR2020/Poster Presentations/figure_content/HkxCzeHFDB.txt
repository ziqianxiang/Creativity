Figure 1: Depiction of the proposed approach. See also the provided pseudocode. When learning task 1, first,parameters of the network Î¸ and output layer w are fitted (Panel A). Afterwards, the learned GP is sparsified andinducing points u1 , .. are found (Panel B). When moving to the next task the same steps are repeated. The onlydifference is that now the previously found summaries (in this case points u1, .., u8) are used to regularise thefunction (via KL-divergence term), such that the first task is not forgotten.
Figure 2: Detecting task boundaries using the predictive uncertainty of a Gaussian Process. As GP predictionsrevert to the prior (shaded blue) when queried far from observed data (shown as black dots), We can test for adistribution shift by comparing the GP posterior over functions (in green) to the prior. Small distance betweenpredictive distributions at test points (red dots) suggest a task switch.
Figure 3: Comparing optimisation criteria for varyingnumber of inducing points.
Figure 4: Inducing point optimisation for the first task on the Permuted-MNIST benchmark. The number ofinducing points was limited to 10. Left: A example optimisation shown in the feature space of a trained network.
Figure 5: Visualising KL terms and test statistics on multipleOmniglot tasks.
Figure 6: Inducing points for Greek alphabet of the Omniglot benchmark. The number of inducing points waslimited to 3 per character.
Figure 7: Ablation study comparing aggregation methods and whether tests should be conducted in log-space.
