Figure 1: A comparison of different exploration methods on Montezuma’ s Revenge.
Figure 2: Evaluation of different bonus-based exploration methods on several Atari games, curvesare averaged over 5 runs, shaded area denotes variance.
Figure 3: Improvements (in percentage of AUC) of Rainbow with various exploration methods overRainbow with -greedy exploration in 60 Atari games. The game MONTEZUMA’ S REVENGE isrepresented in purple.
Figure 4: Normalized score of bonus-basedmethods compared to -greedyoutperforms them by a significant margin on Gravitar. These games were initially classified to behard exploration problems because a DQN agent using -greedy exploration was unable to achieve ahigh scoring policy; it is no longer the case with stronger learning agents available today.
Figure 5: Improvements (in percentage of AUC) of Rainbow with CTS and RND over Rainbowwith -greedy exploration in 60 Atari games when hyperparameters have been tuned on SEAQUEST,Qbert, Pong, Breakout and Asterix. The game Montezuma’ s Revenge is represented inpurple. Games in the training set have stars on top of their bar.
Figure 6: Evaluation of bonus-based methods when training is extended to one billion framesa high-scoring policy, irrespective of the sample cost (Espeholt et al., 2018; Burda et al., 2019;Kapturowski et al., 2019). In this section we present results that contradict this hypothesis. Wereuse agents trained in Section 4.2.2 and lengthen the amount training data they process to 1 billionenvironment frames. We use Atari 2600 games from the set hard exploration with sparse rewardsand the Atari training set. See Figure 6 for training curves. All exploration strategies see their scoregracefully scale with additional data on easier exploration problems. In hard exploration games,none of the exploration strategies seem to benefit from receiving more data. Score on most gamesseem to plateau and may even decrease. This is particularly apparent in Montezuma’ s Revengewhere only RND actually benefits from longer training. After a while, agents are unable to makefurther progress and their bonus may collapse. When that happens, bonus-based methods cannoteven rely on -greedy exploration to explore and may therefore see their performance decrease. Thisbehavior may be attributed to our evaluation setting, tuning hyperparameters to perform best after onebillion training frames will likely improve results. It is however unsatisfactory to see that explorationmethods do not scale effortlessly as they receive more data. In practice, recent exploration bonuseshave required a particular attention to handle large amount of training data (e.g. Burda et al., 2019).
Figure 7: Training curves on Montezuma’ s Revengeε-greedy on Montezuma's Revenge13Published as a conference paper at ICLR 2020GravitarPrivate EyeAverage Score	Average Score	Average Score400003000020000100000So ans1000500FreewayPitfall!353025
Figure 8: Evaluation of different bonus-based exploration methods on the set of hard explorationgames with sparse rewards. Curves are average over 5 runs and shaded area denotes variance.
Figure 9: Evaluation of different bonus-based exploration methods on the Atari training set. Curvesare average over 5 runs and shaded area denotes variance.
Figure 10: Evaluation of different bonus-based exploration methods on the set of hard explorationgames with sparse rewards. Exploration methods are trained for one billion frames. Curves areaverage over 5 runs and shaded area represents variance.
Figure 11:	Evaluation of different bonus-based exploration methods on the Atari training set. Explo-ration methods are trained for one billion frames. Curves are average over 5 runs and shaded arearepresents variance.
Figure 12:	Training curves of Rainbow with -greedy, CTS, PixelCNN, NoisyNets, ICM and RND.
Figure 13: Improvements (in percentage of AUC) of Rainbow with various exploration methodsover Rainbow with -greedy exploration in 60 Atari games when hyperparameters are tuned onMontezuma’ s Revenge. The game Montezuma’ s Revenge is represented in purple.
Figure 14: Improvements (in percentage of AUC) of Rainbow with CTS and RND over Rainbowwith -greedy exploration in 60 Atari games when hyperparameters have been tuned on SEAQUEST,Qbert, Pong, Breakout and Asterix. The game Montezuma’ s Revenge is represented inpurple. Games in the training set have stars on top of their bar.
