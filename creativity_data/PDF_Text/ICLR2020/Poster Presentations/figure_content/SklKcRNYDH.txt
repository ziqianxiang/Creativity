Figure 1: Illustration of tensor index I: the vector x is reshaped into tensor x of d dimensions, andthe bijection I maps the ith coordinate of x, x[i], to x[I (i)].
Figure 2: Memory-performance tradeoff forGBW language modeling with a Transformernetwork. Cyan points: performance after dou-bling model size using optimizer RAM savings.
Figure 3: Comparison of quantities in the nu-merical regret bounds. Since the vertical scaleis logarithmic, the multiplicative regret boundgap compared to AdaGrad is half the height dif-ference between blue and red bars.
Figure 4: Training curves and final loss comparison for a convex problem with synthetic data. Left:Loss curve for each optimizer. Right: Final loss vs. optimizer parameter count.
Figure 5: Memory-performance comparison for CIFAR-10 classification with an 18-layer ResNet:final test error vs. optimizer parameter count. Note that the horizontal scale is logarithmic.
