Figure 1: The off-policy learning as supervised learning framework for general policy gradientmethods.
Figure 2: The training curves of the proposed RPG and state-of-the-art. All results are averaged overrandom seeds from 1 to 5. The x-axis represents the number of steps interacting with the environment(we update the model every four steps) and the y-axis represents the averaged training episodic return.
Figure 3: The trade-off between sample efficiency and optimality on DoubleDunk,BreakOut,BANKHEIST. As the trajectory reward threshold (c) increase, more samples are needed for thelearning to converge, while it leads to better final performance. We denote the value of c by thenumbers at the end of legends.
Figure 4: The binary tree structure MDP with two initial states (Si = {sι, s；}), similar as discussedin Sun et al. (2017). Each path from the root to the leaf node denotes one possible trajectory in theMDP.
Figure 5: The directed graph that describes the conditional independence of pairwise relationshipof actions, where Qi denotes the return of taking action aι at state s, following policy ∏ in M,i.e., QM(s, ai). I1,2 is a random variable that denotes the pairwise relationship of Qi and Q2, i.e.,I1,2 = 1, i.i.f.Qi ≥ Q2, o.w.I1,2 = 0.
