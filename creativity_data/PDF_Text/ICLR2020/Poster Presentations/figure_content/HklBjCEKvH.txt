Figure 1: An illustration of kNN-LM. A datastore is constructed with an entry for each training settoken, and an encoding of its leftward context. For inference, a test context is encoded, and the kmost similar training contexts are retrieved from the datastore, along with the corresponding targets.
Figure 2: Varying the size of the datastore. (a) Increasing the datastore size monotonically improvesperformance, and has not saturated even at about 3B tokens. A kNN-LM trained on 100M tokenswith a datastore of 1.6B tokens already outperforms the LM trained on all 3B tokens. (b) The optimalvalue of λ increases with the size of the datastore.
Figure 3: Transformer LM layer.
Figure 4: Effect of the number of nearest neigh-bors returned per word on Wikitext- 1 03 (val-idation set). Returning more entries from thedatastore monotonically improves performance.
Figure 5: Effect of interpolation parameter λon in-domain (left y-axis) and out-of-domain(right y-axis) validation set performances. Moreweight on pkNN improves domain adaptation.
Figure 6:	Example where the kNN model has much higher confidence in the correct target than theLM. Although there are other training set examples with similar local n-gram matches, the nearestneighbour search is highly confident of specific and very relevant context.
Figure 8: Training curves for the TransformerLM with and without dropout. Turning offdropout allows the training loss to go to 0, in-dicating that the model has sufficient capacity tomemorize the training data.
Figure 7:	Interpolating the Transformer LM withn-gram LMs on WIKITEXT- 1 03 (validation set).
