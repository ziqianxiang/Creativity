Figure 1: Semi-supervised 3D object de-tection. Pre-training with view-contrastiveprediction improves results, especially whenthere are few 3D bounding box annotations.
Figure 2: Contrastive predictive neural 3D mapping. Left: Learning visual feature represen-tations by moving in static scenes. The neural 3D mapper learns to lift 2.5D video streams toegomotion-stabilized 3D feature maps of the scene by optimizing for view-contrastive prediction.
Figure 3: 3D feature flow and object proposals, in dynamic scenes. Given the input frames on theleft, our model estimates dense egomotion-stabilized 3D flow fields, and converts these into objectproposals. We visualize colorized pointclouds and flow fields in a top-down (bird’s eye) view.
Figure 4: Unsupervised 3D moving object de-tection with a stationary camera.
Figure 5: Unsupervised 3D moving object de-tection with a moving camerahere we evaluate its accuracy directly. Weuse two-frame video sequences of dynamicscenes from our CARLA validation set.
Figure 6: Dataset comparison for view prediction. From left to right: Shepard-Metzler (Eslamiet al., 2018), Rooms-Ring-Camera (Eslami et al., 2018), ShapeNet arrangements (Tung et al., 2019),cars (Tatarchenko et al., 2016) and CARLA (used in this work) (Dosovitskiy et al., 2017). CARLAscenes are more realistic, and are not object-centric.
Figure 7: View prediction in CARLA. For each input, we show the target view and view predictionattempts, by a GRNN-style architecture (Tung et al., 2019), a VAE variant of it, and GQN (Eslamiet al., 2018). The predictions are blurry and only coarsely correct, due to the complexity of the RGB.
Figure 8: 2D image patch retrievals acquired with the contrastive model (left) vs the regressionmodel (right). Each row corresponds to a query. For each model, the query is shown on the farleft, and the 10 nearest neighbors are shown in ascending order of distance. Correct retrievals arehighlighted with a green border. The neighbors of the contrastive model often have clear semanticrelationships (e.g., curbs, windows); the neighbors of the RGB model do not.
Figure 9: Left: 3D object proposals and their center-surround scores (normalized to the range[0,1]). For each proposal, the inset displays the corresponding connected component of the 3D flowfield, viewed from a top-down perspective. In each row, an asterisk marks the box with the highestcenter-surround score. Right: Observed and estimated heightmaps of the given frames, computedfrom 3D occupancy grids. Note that the observed (ground truth) heightmaps have view-dependent“shadows” due to occlusions, while the estimated heightmaps are dense and viewpoint-invariant.
