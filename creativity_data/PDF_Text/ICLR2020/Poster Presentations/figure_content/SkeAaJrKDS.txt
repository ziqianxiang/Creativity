Figure 1: Illustration of SAVE. When acting,the agent uses a Q-function, Qθ, as a prior forthe Q-values estimated during MCTS. OverK steps of search, Q0 ≡ Qθ is built upto QK, Which is returned as QMCTS (Equa-tions 1 and 4). From QMCTS, an action a isselected via epsilon-greedy and the resultingexperience (s, a, r, s0 , QMCTS) is added to areplay buffer. When learning, the agent usesreal experience to update Qθ via Q-learning(LQ) as Well as an amortization loss (LA)Which regresses Qθ toWards the Q-values es-timated during search (Equation 6).
Figure 2: Results on Tightrope. (a-c) Tabular results comparing SAVE, PUCT, UCT, and Q-learning(with MCTS at test time) for varying percentages of terminal actions on the x-axes and for differentsearch budgets. The y-axes show reward for either the sparse or dense reward setting of Tightrope.
Figure 3: Results on Construction. (a-c) Each subplot shows results for SAVE, SAVE withoutamortization loss, Q-learning with MCTS at test time, and pure search (UCT). The x-axis showsthe effect of increasing the number of MCTS simulations at test time. During training, SAVE withand without amortization loss used a search budget of 10 simulations. UCT used a search budgetof 1000 simulations. Points show medians across 10 seeds, and error bars indicate min and maxseeds. (d) Ablation experiments on the Covering task. We compare SAVE to variants that do nothave an amortization loss, which use an L2 amortization loss, which do not use the Q-Learning loss,and which use PUCT rather than UCT. Results are shown at the hardest level of difficulty for theCovering task with a test budget of 10. The colored bars show median reward across 10 seeds, anderror bars show min and max seed.
Figure 4: (a-b) Results on the Marble Run environment for model-free Q-Learning as well as SAVEas a function of curriculum difficulty level, for two different settings of the cost of “sticky” blocks.
Figure 5: Results on Atari.
