Figure 1: Scaling of CHOCO-SGD with sign compression to large number of devices on Cifar10dataset. Left: best testing accuracy of the algorithms reached after 300 epochs. Right: best testingaccuracy reached after communicating 1000 MB.
Figure 2:	Image classification: ResNet-20 on CIFAR-10 on social network topology.
Figure 3:	Language modeling: LSTM on WikiText-2 on social network topology.
Figure 4: Parameter deviations for Resnet20 trained on Cifar10 (using CHOCO-SGD) on social networktopology (32 workers). (Left) performance of the averaged model compared to the average of performances oflocal models. (Right) parameters divergence: averaged L2 consensus distance between local models xi and theaveraged model X = 1 Pi=I xi,i.e., 1 Pi=IkXi - X∣∣2∙diverging from the averaged model, while decreasing only when the stepsize decreases. The samebehavior was also reported in Assran et al. (2019).
Figure 5: Large-scale training: Resnet-50 on ImageNet-1k in the datacenter setting. Thetopology has 8 nodes (each accesses 4 GPUs). We use sign as the compression scheme, for CHOCO-SGD and Centralized SGD. For centralized SGD baseline without compression, we use all-reduce toaggregate the gradients; we use all-gather for centralized SGD with sign gradients quantization. Thebenefits of Choco-SGD can be further pronounced when scaling to more nodes.
Figure 6:	Scaling of CHOCO-SGD with sign compression to large number of devices on Cifar10dataset. Convergence curves for 64 nodes. Vertical lines corresponds to the epoch/bits budget used inFig. 1.
Figure 7:	Training ResNet-20 on CIFAR-10 with decentralized algorithm on a real world social networktopology. The topology has 32 nodes and we assume each node can only access a disjoint subset of the wholedataset. The local mini-batch size is 32.
Figure 8:	Training LSTM on WikiText2 with decentralized algorithm on a real world social networktopology. The topology has 32 nodes and we assume each node can only access a disjoint subset of the wholedataset. The local mini-batch size is 32.
Figure 9: Large-scale training: ResNet-50 on ImageNet in the datacenter.
