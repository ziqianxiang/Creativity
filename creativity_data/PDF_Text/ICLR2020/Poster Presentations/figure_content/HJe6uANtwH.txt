Figure 1: Illustration of a Capsule network with a backbone block, 3 convolutional capsule layers, 2 fully-connected capsule layers, and a classifier. The first convolutional capsule layer is called the primary capsulelayer. The last fully-connected capsule layer is called the class capsule layer.
Figure 2:	Illustration of the Inverted Dot-Product Attention Routing With the Pose admitting matrix structure.
Figure 3:	Illustration of the proposed concurrent routing from iteration 2 to t with the example in Figure 1.
Figure 4: Convergence analysis for CapsNets on CIFAR-10 with simple backbone model. Top: convergenceplots for different routing mechanisms. Bottom left: classification results with respect to different routingiterations. Inverted Dot-Product Attention-A denotes our routing approach without Layer Normalization. In-verted Dot-Product Attention-B denotes our routing approach with sequential routing. Inverted Dot-ProductAttention-C denotes our routing approach with activations in capsules. * indicates a uniform prediction. Bot-tom right: memory usage and inference time for the proposed Inverted Dot-Product Attention Routing. Forfairness, the numbers are benchmarked using the same 8-GPU machine with batch size 128. Note that for fu-ture comparisons, we refer the readers to an alternative implementation for our model: https://github.
Figure 5: Table and convergence plot for baseline CNN and CapsNets with different pose structures.
