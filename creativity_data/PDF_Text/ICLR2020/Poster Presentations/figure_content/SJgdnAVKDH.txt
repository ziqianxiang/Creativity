Figure 1: BLEU on WMT100K dataset from the supervised base-line and different self-training variants. We plot the results over 3iterations. “ST” denotes self-training while “NST” denotes noisyself training.
Figure 2: Two examples of error heat map on the toy sum dataset that shows the effect of smoothness.
Figure 3: Analysis of noisy self-training on WMT English-German dataset, demonstrating the effectof parallel data size, monolingual data size, and noise level.
Figure 4: Validation loss v.s. number of update steps, for the baseline model on WMT100K dataset.
Figure 5: Error heat maps on the toy sum dataset over the first two iterations. Deeper color representlarger errors.
