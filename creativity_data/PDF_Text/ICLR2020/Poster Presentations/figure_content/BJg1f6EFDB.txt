Figure 1: (a) Each point represents the Pearson correlation coefficient of effective attention and rawattention as a function of token length. (b) Raw attention vs. (c) effective attention, where each pointrepresents the average (effective) attention of a given head to a token type.
Figure 2: (a) Identifiability of contextual word embeddings at different layers. Here, g is trained andtested on the same layer. (b) gclions,l trained on layer l and tested on all layers.
Figure 3: (a) Contribution of the input token to the embedding at the same position. The orange linerepresents the median value and outliers are not shown. (b) Percentage of tokens P that are not themain contributors to their corresponding contextual embedding at each layer.
Figure 4: (a) Relative contribution per layer of neighbours at different positions. (b) Total contribu-tion per neighbour for the first, middle and last layers.
Figure 5: Effective attention (a) vs. raw attention (b). (a) Each point represents the average effec-tive attention from a token type to a token type. Solid lines are the average effective attention ofcorresponding points in each layer. (b) is the corresponding figures using raw attention weights.
Figure 7: Train and test token identifiability rates for the linear perceptron and MLP.
Figure 8: Linear Perceptron trained to minimize L2 distance generalizing to all layers.
Figure 9: MLP trained to minimize L2 distance generalizing to all layers.
Figure 10: MLP trained to minimize cosine distance generalizing to all layers.
Figure 11:	Token identifiability across single layers. These results are for non fine-tuned BERT onMRPC.
Figure 12:	Token identifiability across single layers, comparing non fine-tuned (dashed) BERTagainst BERT fine-tuned on MRPC (solid).
Figure 13:	Token identifiability across single layers, comparing non fine-tuned BERT (dashed)against BERT fine-tuned on CoLA (solid).
Figure 14: Recovering neighbouring input tokens using。燃 l.
Figure 15: Recovering neighbouring input tokens using gm∖.
Figure 16: Recovering neighbouring input tokens using grmplFigure 17: Recovering neighbouring input tokens using。储.
Figure 17: Recovering neighbouring input tokens using。储.
Figure 18:	Normalized total contribution to the [CLS] token (a) centered around [CLS] at position0 (b) centered around [SEP].
Figure 19:	[CLS]: Aggregates context from all tokens but more strongly from those around the first[SEP] token. We hypothesize that this is due to the Next Sentence Prediction pre-training.
Figure 20: he: Aggregates most context from the main verb of the sentence, ”said”.
Figure 21: said: Aggregates context mainly from its neighborhood, the main verb of the subordinatesentence and the border between the two input sentences.
Figure 22: fit: In the first layers it aggregates most context from its neighborhood and towards thelast layers it gets the context from its direct object (strategy) and from the token with the samemeaning in the second sentence.
Figure 23:	long: It is part of a composed adjective (long-term) and aggregates most of its contextfrom the other part of the adjective (term) as well as from the same tokens in the second sentence.
Figure 24:	strategy: Aggregates context from the word growth, which is the first one of the nounphrase ”growth strategy”.
Figure 25:	[SEP]: This token that has no semantic meaning aggregates context mostly from [CLS]and its own neighborhood.
Figure 26:	Layer 1: Most token types are equally mixed and have already less than 35% mediancontribution from their corresponding input. The only exception are the [CLS] tokens, which remainwith over 40% median original contribution.
Figure 27:	Layer 2: Similar to the previous layer with less contribution over all and [SEP] behavingsimilarly to [CLS].
Figure 28: Layer 3: Similar to layer 2 with decreasing contribution overall.
Figure 29: Layer 4: The original input contribution to [CLS] and [SEP] falls significantly. The trendthat the word types will follow until the last layer is already clear: Most nouns (NNP, NNS, NN),verbs (VBN, VB, VBD, VBP), adjectives (JJ, JJS) and adverbs (RBR, RBS) keep more contributionfrom their corresponding input embeddings than words with “less” semantic meaning like Wh-pronouns and determiners (WP, WDT), prepositons (IN), coordinating conjunctions (CC), symbols(SYM), possessives (PRP$, POS) or determiners (DT).
Figure 30: Layer 5: The trend started in the previous layer continues, with a reduction of internalvariability within those word types with less original contribution.
Figure 31: Layer 6: Similar behavior as in the previous layer with minor evolution.
Figure 32: Layer 7: Minor changes with respect to Layer 6.
Figure 33: Layer 8: At this point there is clearly a different behavior between the tokens with mostcontribution which present more intra-class variability, and those with less contribution, which aremore uniform.
Figure 34: Layer 9: SEP changes increasing the contribution, while the rest stays similar.
Figure 35: Layer 10: The contribution evolves with the same pattern as in previous layers.
Figure 36: Layer 11:The contribution evolves with the same pattern as in previous layers.
Figure 37: Layer 12: Finally, nouns, verbs, adjectives, adverbs, receive more contribution from theircorresponding input than determiners, prepositions, pronouns, ”to” words and symbols.
Figure 38:	Identifiability of contextual word embeddings at different layers on CoLA.
Figure 39:	Identifiability of contextual word embeddings at different layers on a the first 500 sen-tences of MNLI-matched (19,839 tokens).
Figure 40:	(a) Contribution of the input token to the embedding at the same position. (b) Percentageof tokens P that are not the main contributors to their corresponding contextual embedding at eachlayer.
Figure 41:	(a) Relative contribution per layer of neighbours at different positions. (b) Total contri-bution per neighbour for the first, middle and last layers.
Figure 42: (a) Contribution of the input token to the embedding at the same position. (b) Percentageof tokens P that are not the main contributors to their corresponding contextual embedding at eachlayer.
Figure 43: (a) Relative contribution per layer of neighbours at different positions. (b) Total contri-bution per neighbour for the first, middle and last layers.
