Figure 1: SNOW vs. transfer/HfelOng learning to build task1-3 from task0. The dotted box indicatesa model which stays frozen (or no parameter update), skipping back-propagation during training.
Figure 2: In SNOW architecture, the source model produces intermediate features and the deltamodels selectively subscribe to them via channel pooling in their training and serving processes. Thelocal features from a delta model and the subscribed features are combined through concatenation.
Figure 3: Forward and backward propagation paths for channel pooling.
Figure 4: Training performance comparison on five different datasets on a single GPU.
Figure 6: Validation accuracy curves during training for each dataset13Published as a conference paper at ICLR 2020A.3 Learning without ForgettingIn our main experiment, we do not include the results from SNOW with Learning without Forgetting(LF) (Li & Hoiem, 2018) due to the catastrophic forgetting issue as shown in Table 9. Simply, theaccuracies from LF for the experimented datasets are quite off from other competing algorithms,making it hard to put them on the same charts. We explored a few sequences to get the best outcomefor LF and came up with the following in Table 9, which led to the accuracy changes for the datasetsbelow. We used a batch size of 128, 0.005 learning rate, and Î»o = 1 as suggested in (Li & Hoiem,2018). As you can see in the result for the training order A, once Action dataset is used, the accuracyagainst Car dataset drops significantly, as both are very heterogeneous (in terms of target domainand dataset size). Yet, adding CUB has somewhat limited impacts on Action. Also, we observe thatthe accuracy of LF depends on a training order. The accuracy from the training order A and trainingorder B is significantly different before adding Food data. Overall, the combination of datasets in ourexperiment seems to be a very challenging scenario for LF in terms of catastrophic forgetting.
