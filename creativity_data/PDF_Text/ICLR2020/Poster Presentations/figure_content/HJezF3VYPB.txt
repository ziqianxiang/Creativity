Figure 1: (a) We propose an approach for the UFDA setting, where data are not shareable between differentdomains. In our approach, models are trained separately on each source domain and their gradients areaggregated with dynamic attention mechanism to update the target model. (b) Our FADA model learns to extractdomain-invariant features using adversarial domain alignment (red lines) and a feature disentangler (blue lines).
Figure 2: We demonstrate the effectiveness of FADA on four datasets: (1) “Digit-Five”, which includes MNIST(mt), MNIST-M (mm), SVHN (sv), Synthetic (syn), and USPS (up). (2) Office-Caltech10 dataset, which containsAmazon (A), Caltech (C), DSLR (D), and Webcam (W). (3) DomainNet dataset, which includes: clipart (clp),infograph (inf ), painting (pnt), quickdraw (qdr), real (rel), and sktech (skt). (4) Amazon Review dataset, whichcontains review for Books (B), DVDs (D), Electronics (E), and Kitchen & housewares (K).
Figure 3: Feature visualization: t-SNE plot of source-only features, f-DANN (Ganin & Lempitsky, 2015)features, f-DAN (Long et al., 2015) features and FADA features in sv,mm,mt,sy→up setting. We use differentmarkers and colors to denote different domains. The data points from target domain have been denoted by redfor better visual effect. (Best viewed in color.)5.1	Experiments on Digit RecognitionDigit-Five This dataset is a collection of five benchmarks for digit recognition, namely MNIST (Le-Cun et al., 1998), Synthetic Digits (Ganin & Lempitsky, 2015), MNIST-M (Ganin & Lempitsky,2015), SVHN, and USPS. In our experiments, we take turns setting one domain as the target domainand the rest as the distributed source domains, leading to five transfer tasks. The detailed architectureof our model can be found in Table 7 (see supplementary material).
Figure 4: (a)A-Distance of ResNet, f -DANN, and FADA features on two different tasks. (b) training errorsand dynamic weight on A,C,W→D task. (c)-(d) confusion matrices of f-DAN, and FADA on A,C,D→W task.
