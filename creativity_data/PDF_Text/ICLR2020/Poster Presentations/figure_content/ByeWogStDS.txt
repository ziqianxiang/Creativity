Figure 1: Temporal hierarchy studied in this paper. A latentcode zt is sampled from the manager policy πθh (zt|st) everyp time-steps, using the current observation skp . The actionsat are sampled from the sub-policy πθl (at|st, zkp) condi-tioned on the same latent code from t = kp to (k + 1)p - 1In this work, we propose a method to learn a hierarchical policy and efficiently adapt all the levels inthe hierarchy to perform a new task. We study hierarchical policies composed of a higher level, ormanager πθh (zt|st), and a lower level, or sub-policy πθl (at0 |zt, st0). The higher level does not takeactions in the environment directly, but rather outputs a command, or latent variable zt ∈ Z, thatconditions the behavior of the lower level. We focus on the common case where Z = Zn makingthe manager choose among n sub-policies, or skills, to execute. The manager typically operates at alower frequency than the sub-policies, only observing the environment every p time-steps. When themanager receives a new observation, it decides which low level policy to commit to for p environmentsteps by the means of a latent code z. Figure 1 depicts this framework where the high level frequencyp is a random variable, which is one of the contribution of this paper as described in Section 4.4. Notethat the class of hierarchical policies we work with is more restrictive than others like the optionsframework, where the time-commitment is also decided by the policy. Nevertheless, we show that thisloss in policy expressivity acts as a regularizer and does not prevent our algorithm from surpassingother state-of-the art methods.
Figure 2: Environments used to evaluate the performance of our method. Every episode has a differentconfiguration: wall heights for (a)-(b), ball positions for (c)-(d)(a) Block Hopper (b) Block Half Cheetah (c) Snake Gather	(d) Ant GatherFigure 3: Analysis of different time-commitment strategies on learning from scratch.
Figure 3: Analysis of different time-commitment strategies on learning from scratch.
Figure 4: Using a skill-conditioned baseline, as defined in Section 4.2, generally improves perfor-mance of HiPPO when learning from scratch.
Figure 5: Comparison of HiPPO and HierVPG to prior hierarchical methods on learning from scratch.
Figure 6: Benefit of adapting some given skills when the preferences of the environment are differentfrom those of the environment where the skills were originally trained. Adapting skills with HiPPOhas better learning performance than leaving the skills fixed or learning from scratch.
Figure 7: HIRO performance on Ant Gather with and without access to the ground truth (x, y), whichit needs to communicate useful goals.
Figure 8: Sensitivity of HiPPO to variation in the time-commitment.
Figure 9: Sensitivity of HiPPO to variation in the number of skills.
