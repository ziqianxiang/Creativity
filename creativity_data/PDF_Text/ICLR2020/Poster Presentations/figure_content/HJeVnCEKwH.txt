Figure 1: Visualizations of Example 1. Left: projection of the game vector field on the plane θ2 = 1. Right:Generator loss. The descent direction is (1,中)(in grey). As the generator follows this descent direction, thediscriminator changes the value of 中,making the saddle rotate, as indicated by the circular black arrow.
Figure 2: Above: game vector field (in grey) for different archetypal behaviors. The equilibriumof the game is at (0, 0). Black arrows correspond to the directions of the vector field at differentlinear interpolations between two points: • and ?. Below: path-angle c(α) for different archetypalbehaviors (right y-axis, in blue). The left y-axis in orange correspond to the norm of the gradients.
Figure 3: Path-angle for NSGAN (top row) and WGAN-GP (bottom row) trained on the differentdatasets, see Appendix C.3 for details on how the path-angle is computed. For MoG the ending pointis a generator which has learned the distribution. For MNIST and CIFAR10 we indicate the Inceptionscore (IS) at the ending point of the interpolation. Notice the “bump” in path-angle (close to α = 1.0),characteristic of games rotational dynamics, and absent in the minimization problem (d). Details onerror bars in §C.3.
Figure 4: Eigenvalues of the Jacobian of the game for NSGAN (top row) and WGAN-GP (bottomrow) trained on the different datasets. Large imaginary eigenvalues are characteristic of rotationalbehavior. Notice that NSGAN and WGAN-GP objectives lead to very different landscapes (see howthe eigenvalues of WGAN-GP are shifted to the right of the imaginary axis). This could explain thedifference in performance between NSGAN and WGAN-GP.
Figure 5: NSGAN. Top k-Eigenvalues of the Hessian of each player (in terms of magnitude) indescending order. Top Eigenvalues indicate that the Generator does not reach a local minimum but asaddle point (for CIFAR10 actually both the generator and discriminator are at saddle points). Thusthe training algorithms converge to LSSPs which are not Nash equilibria.
Figure 6: WGAN-GP. Top k-Eigenvalues of the Hessian of each player (in terms of magnitude) indescending order. Top Eigenvalues indicate that the Generator does not reach a local minimum but asaddle point. Thus the training algorithms converge to LSSPs which are not Nash equilibria.
Figure 7: The norm of gradient during training for the standard GAN objective. We observe thatwhile extra-gradient reaches low norm which indicates that it has converged, the gradient descent onthe contrary doesn’t seem to converge.
Figure 8: Path-angle and Eigenvalues computed on MNIST with Adam.
Figure 9: Path-angle and Eigenvalues for NSGAN on CIFAR10 computed on CIFAR10 with Adam. We can seethat the model has eigenvalues with negative real part, this means that we’ve actually reached an unstable point.
