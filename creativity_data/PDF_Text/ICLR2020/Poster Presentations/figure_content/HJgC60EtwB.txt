Figure 1: Three domains showing RE-MPO (blue), SRE-MPO (green) and E-MPO (red). Theaddition six domains can be found in the appendix. In addition, the results for R-MPO, SR-MPO andMPO can be found in Appendix H.4 with similar results.
Figure 2: (1) The Shadow hand domain (left) and results for RE-MPO and E-MPO (middle left).
Figure 3: (1) Domain Randomization (DR): Domain randomization performance for the Cartpolebalance (left) and Pendulum swingup (middle left) tasks. 3 (2) Stochastic Value Gradients (SVG): Tworight images show the performance of Robust Entropy-regularized SVG (RE-SVG) and SRE-SVGcompared to E-SVG for Pendulum and Cartpole respectively.
Figure 4: Modifying the uncertainty set: Pendulum Swingup when modifying the third perturbationof the uncertainty set to values of 1.2 (left), 1.3 (middle) and 2.0 (right) meters respectively.
Figure 5: Training uncertainty sets of transition models on different batch sizes of offline data. Theperformance of Data Driven R-MPO (DDR-MPO) can be seen in the figures above for CartpoleSwingup (left) and Pendulum Swingup (right) respectively.
Figure 6: All nine domains showing RE-MPO (blue), SRE-MPO (green) and E-MPO (red).
Figure 7: All nine domains showing RE-MPO (blue), SRE-MPO (green) and E-MPO (red) as afunction of evaluation steps during training.
Figure 8: All nine domains showing R-MPO (blue), SR-MPO (green) and MPO (red).
Figure 9: All nine domains showing R-MPO (blue), SR-MPO (green) and MPO (red) as a functionof evaluation steps during training.
Figure 10: Increasing the range of the training uncertainty set for Cartpole balance (top row) andPendulum swingup (bottom row).
Figure 11: Comparing entropy-regularized objective to the non-entropy regularized objective (leftfigure). The entropy-regularized version does no worse than the non entropy-regularized setup and insome cases, for example Cheetah, performs considerably better than the expected return objective(right figure).
Figure 12: Additional Training Samples: Two plots show 3 times more additional training samplesfor non-robust E-MPO (dark grey) in the Cartpole Balance and Pendulum Swingup tasks respectively.
Figure 13:	Domain Randomization (DR): Domain randomization performance for the Cartpolebalance (left) and Pendulum swingup (middle) tasks. As we increase the number of perturbations forDR to 100 (right figure), we see that performance improves but still does not outperform RE-MPO,which still only uses 3 perturbations.
Figure 14:	Modifying the uncertainty set: The top row indicates the change in performance forCartpole balance as the third perturbation of the uncertainty set is modified to 1.5, 2.5 and 3.5 metersrespectively. The bottom row shows the performance for Pendulum Swingup for final perturbationchanges of 1.2, 1.3 and 2.0 meters respectively.
Figure 15: Changing the nominal model: The top two figures indicate setting the nominal modelas the median and largest perturbation of the uncertainty set for Cartpole Balance respectively. Theright two figures are the same setting but for the Pendulum swingup domain. Legend: E-MPO (red),RE-MPO (blue), SRE-MPO (green).
