Figure 1: The overview of our proposed method. We formulate it as a Min-Max game. Thedata of each batch is augmented by multiple pre-processing components with sampled policies{τι,τ2, ∙∙∙ , TM}, respectively. Then, a target network is trained to minimize the loss of a largebatch, which is formed by multiple augmented instances of the input batch. We extract the traininglosses ofa target network corresponding to different augmentation policies as the reward signal. Fi-nally, the augmentation policy network is trained with the guideline of the processed reward signal,and aims to maximize the training loss of the target network through generating adversarial policies.
Figure 2: An example of dynamic augmentation policies learned with ResNet-50 on ImageNet.
Figure 3: The basic architecture of the controller for generating a sub-policy, which consists of twooperations with corresponding parameters, the type and magnitude of each operation. When a policycontains Q sub-policies, the basic architecture will be repeated Q times. Following the setting ofAutoAugment (Cubuk et al., 2019), the number of sub-policies Q is set to 5 in this paper.
Figure 4: The Comparison of normalized perfor-mance between AUtoAUgment and our method.
Figure 6: Probability distribution of the parameters in the learned augmentation policies on CIFAR-10 over time. The number in (b) represents the magnitude of one operation. Larger number standsfor more dramatic image transformations. The probability distribution of each parameter is the meanof each five epochs.
