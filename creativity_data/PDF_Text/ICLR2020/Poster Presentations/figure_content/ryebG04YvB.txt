Figure 2: Wide Resnet 32-10 and the blocks used for freezing/retrainingLβ0≈2 kcolb£Now that we have identified feature extractors as a source of robustness, it is natural to investigatewhether robustness is preserved when transfer learning using robust feature extractors. We will2We adv. train the WRN 32-10 on CIFAR-100 using a 7-step '∞ PGD attack With SteP-Size=2 and e = 8.
Figure 1: Robustness is preserved when we retrain only the deepest block(s) of robust CIFAR-10 and CIFAR-100 models using natural examples. The vertical axis is the accuracy on PGD-20generated adversarial examples (i.e. robustness) after re-training deep layers. The robustness of theadversarially trained models if all layers are frozen are shown with dashed lines.
Figure 3: When the number of training data per-class is very limited (right bars), adversarially robusttransfer learning [Transferred] is better in all metrics. However, as the number of training dataincreases (left bars), fine-tuning with adversarial examples of the target domain [Fine-tuned withAT] results in more robustness. Adversarially robust transfer learning always results in models thatwork better on natural examples and is 3× faster than fine-tuning with adversarial examples of thetarget domain. Using a pre-trained robust ImageNet improves both robustness and generalization.
Figure 4: Training an MLP for CIFAR-100 on top ofthe robust feature extractors from ImageNet. The x-axis corresponds to the number of hidden layers (0 is alinear classifier and corresponds to experiments in sec-tion 4.1.1). Robustness stems from robust feature extrac-tors. Adding more layers on top of this extractor doesnot hurt robustness. Interestingly, simply adding morelayers does not improve the validation accuracy and justresults in more overfitting (i.e. training accuracy becomes100%). We can slightly improve generalization usingbatch norm (BN) and dropout (DO).
Figure 5: Gradients of the loss w.r.t to input images for theCIFAR-100 transfer learning experiments of sections 4.1.1 &4.2.1. The top row contains sample CIFAR-100 images. Otherrows contain image gradients of the model loss. The secondrow is for a model transferred from a naturally trained ImageNetsource. Rows 3-5 are for models transferred from a robust Im-ageNet source. These rows correspond to an MLP with 0 (row3), 1 (row 4), and 2 (row 5) hidden layers on top of the robustfeature extractor. The gradients in the last three rows all showinterpretable generative behavior.
Figure 6: Our LwF loss has a term that enforces the similarity of feature representations (i.e. penul-timate layer activations) between the source model and the fine-tuned model.
Figure 7: Figures 7a and 7b both show that the values of ∣∣z(χ, θr), z(xa, θr)k2 are high most of thetime, consequently, LwF is better done without data augmentation.
Figure 8: When the number of training data per-class is very limited (right bars), adversarially robusttransfer learning [Transferred] is better overall. However, as the number of training data increases(left bars), fine-tuning with adversarial examples of the target domain [Fine-tuned with AT] resultsin an overall better performing model. Adversarially robust transfer learning is 3× faster than fine-tuning with adversarial examples of the target domain.
