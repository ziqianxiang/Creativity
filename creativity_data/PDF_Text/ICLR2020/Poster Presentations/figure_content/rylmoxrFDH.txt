Figure 1: Dynamics of the variance and correlation maps, with simulations of a network of widthN = 1000, 50 realisations, for various hyperparameter settings: σm2 ∈ {0.2, 0.5, 0.99} (blue, greenand red respectively). (a) variance evolution, (b) correlation evolution. (c) correlation mapping (cinto cout), with σb2 = 0.0014	Numerical and experimental results4.1	SimulationsWe first verify that the theory accurately predicts the average behaviour of randomly initialisednetworks. We present simulations for the deterministic surrogate in Figure 1. We see that theaverage behaviour of random networks are well predicted by the mean field theory. Estimates ofthe variance and correlation are plotted, with dotted lines corresponding to empirical means andthe shaded area corresponding to one standard deviation. Theoretical predictions are given by solidlines, with strong agreement for even finite networks. Similar plots can be produced for the LRTsurrogate. In Appendix D we plot the depth scales as functions of σm and σb .
Figure 2: Top: Training performance of the deterministic surrogate (left) and the LRT surrogate forstochastic binary weights and continuous neurons (right). The vertical axis represents network depthagainst the variance of the means σm2 . Both surrogates were trained with σb2 = 0. Thus, as σm2 → 1we approach criticality in both cases. Overlaid are curves proportional to the correlation depth scaleξc . Bottom: Training performance of the deterministic surrogate and its binary counterparts aftertraining on the MNIST dataset for 5 epochs. Left: performance of the continuous surrogate. Centre:the performance of the stochastic binary network, averaged over 5 Monte Carlo samples. Right:100 Monte Carlo samples. The deterministic binary evaluation is similar to a single Monte Carlosample, resembling the central figure.
Figure 3: Plots of the valid critical initialisations for the deterministic surrogate model, for stochasticbinary weights and stochastic or deterministic binary neurons. Presented are the critical initialisa-tions in the (0也,σ2), for both the a) stochastic neuron case with φ(z) = erf( 4 z), b) the determinis-tic sign neuron case with φ(z) = erf(2 ∙), and (C) the logistic based stochastic neuron, with tanh()approximation. We see all lines are above σ2 = 1 for all but small σb2 << 1.
