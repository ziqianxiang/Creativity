Figure 1: We present a dynamical distance learning (DDL) method that can learn a 9-DoF real-worlddexterous manipulation task directly from raw image observations. DDL does not assume access tothe true reward function and solves the 180 degree valve-rotation task in 8 hours by relying only on10 human-provided preference labels.
Figure 2: We evaluate our method both in simulation and on a real-world robot. We show that ourmethod can learn to turn a valve with a real-world 9-DoF hand (a), and run ablations in the simulatedversion of the same task (b). We also demonstrate that our method can learn pole balancing (c) andlocomotion (d, e, f) skills in simulation.
Figure 3: (Left) learning curves for the valve ro-tation task learned from state. (Right) Same taskfrom vision. The curves correspond to the finaldistance (measured in radians) of the valve fromthe target angle during a rollout. Our method(DDLfP, orange) solves the task in 8 hours. Itsperformance is comparable to that of SAC withtrue rewards, and VICE with example outcomeimages. DDLfP only requires 10 preferencequeries, and learns without true rewards or out-come images. We compare our method in the sim-ulated version of this task in Figure 5.
Figure 4: Learning curves for MuJoCo tasks with DDLfP. The y-axis presents the true return of thetask. We compare DDLfP to SAC trained directly from the true reward function, which providesan oracle upper bound baseline, and the prior method proposed by Christiano et al. (2017). Theprior method uses an on-policy RL algorithm which typically requires more samples than off-policyalgorithms, and thus we also plot its final performance after 20M training steps with red star. At thetime of the submission, the Ant-v3 run is still in progress and the complete learning curve will beincluded in the final.
Figure 5: We compare DDL against alternativemethods for learning distances on the simulatedvalve turning task, when learning from the under-lying low-dimensional state (left) and from im-ages (right). Dynamical distances used greed-ily (orange) or learned with TD (green) generallyperform poorly. HER (red) can learn from low-dimensional states, but fails to learn from images.
Figure 6: (Top) Learning curves for DDLUS. The y-axis plots the environment return (not accessibleduring the training) for InvertedDoublePendulum-v3, and the L2-distance travelled from the originfor Hopper-v3, HalfCheetah-v3, and Ant-v3. (Bottom) Frequency histograms of skills learned withDDLUS (blue) and DIAYN (orange) (Eysenbach et al., 2018) across different training runs, evalu-ated according to the travelled L2-distance from the origin.
Figure 7: Evaluation of the learned distance in a 2D point environment. The state is the Xy-coordinates of the point, and action corresponds to 2D velocities. The black bars denote walls,blue star is a goal state, and the heat map denotes the estimated distance to the goal. (a) Our methodlearns an accurate estimate of the shape of the distance function. (b) Ground-truth distance.
Figure 8: Human preference queries for the vision-based DClaw experiment presented in Sec-tion 6.1. Each image row presents the set of images shown to the human operator on a single queryround. On each row, the first 10 images correspond to the last states of the most recent rollouts andthe right-most image corresponds to the last goal. For each query, the human operator picks a newgoal by inputting its index (between 0-10) into a text-based interface. The goals selected by humanare highlighted with white borders.
