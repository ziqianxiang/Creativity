Figure 2: Visualization of state alignmentdistribution qφ(z|x). β-VAE is a variant VAE that introduces an adjustable hyperparameter β to theoriginal objective:L(θ,φ; x,z,β) = Eqφ(z|x) [logPθ(x|z)] - βDκL (qφ(z∣x)∣∣p(z))	(1)Larger β will penalize the total correlation (Chen et al., 2018) to encourage more disentangled latentrepresentations, while smaller β often results in sharper and more precise reconstructions.
Figure 1: Using VAE as a state predic-tive model will be more self-correctablebecause of the stochastic sampling mech-anism. But this won’t happen when we useVAE to predict actions.
Figure 3: comparison with Bc, GAiL and AiRL when dynamics are different from experts.
Figure 4: Imitation Learning of Actors With Heterogeneous Action Dynamics.
Figure 5: (a), (b) show the effects of Wasserstein distance and KL regularization on HalfCheetah-v2and Humanoid-v2 given 20 demonstration trajectories. And (c) presents the result on Antmaze.
Figure 6: Two-ring MDP with deterministic transitionFigure 6 shows the states and transition of an MDP. The demonstration policy jumps back and forthbetween si and s2 periodically. Because our algorithm has the action prior (local alignment), it isclear that we can solve this problem. The dynamics of many periodic games, such as Walker andHalfCheetah in MuJoco, are extension of this two-ring graph.
