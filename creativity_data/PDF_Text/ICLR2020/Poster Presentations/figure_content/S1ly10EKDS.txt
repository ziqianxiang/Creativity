Figure 1: Error decay of VRTD With i.i.d. sample6 ConclusionIn this paper, We provided the convergence analysis for VRTD With both i.i.d. and Markovian samples.
Figure 2:	Error decay of VRTD with Markovian sampleAcknowledgmentsThe work was supported in part by US National Science Foundation under the grants CCF-1801855,ECCS-1818904, and CCF-1909291. The authors would like to thank Bowen Weng at the Ohio StateUniversity for the helpful discussions on the experiments. The authors would also like to thank a fewanonymous reviewers for their suggestions on the analysis of the overall computational complexityas well as additional experiments, which significantly help to improve the quality of the paper.
Figure 3:	Error decay of VRTD in Frozen Lake problemA.2 Mountain CarMountain Car is a game in OpenAI Gym, which is driven by an MDP with an infinite statespace and a finite action space. At each time step, an agent randomly chooses an action∈ {push left, push right, no push}. In this problem, the ground truth value of θ* is not known.
Figure 4: NEU decay of VRTD in Mountain Car problemA.3 Comparison between VRTD and TD with a Changing StepsizeIn this subsection, we provide an additional experiment to compare the performance of VRTD given inAlgorithm 2 (under constant stepsize) with the TD algorithm (under a changing stepsize as suggestedby the reviewer). We adopt the same setting of Frozen Lake as in Appendix A.1. Let VRTD takea batch size M = 5000 and stepsize α = 0.1. For a fair comparison, we start TD with the sameconstant stepsize α = 0.1 and then reduce the stepsize by half whenever the error stops decrease. Thecomparison is reported in Figure 5, where both curves are averaged over 1000 independent runs. Thetwo algorithms are compared in terms of the squared error versus the total number of pseudo-gradientcomputations (equivalently, the total number of samples being used). It can be seen that VRTDreaches the required accuracy much faster than TD.
Figure 5: Comparison of error decay of VRTD and TD with changing stepsize in Frozen LakeproblemB A Counter ExampleIn this section, we use a counter-example to show that one major technical step for characterizing theconvergence bound in Korda and La (2015) does not hold. Consider Step 4 in the proof of Theorem 3in Korda and La (2015). For the following defined (θ)e(θ) = (θ - θ*)>[E(v>v|Fn) - Eψ,θn(v>v)](θ - θ*),(7)14Published as a conference paper at ICLR 2020where Ψ denotes the stationary distribution of the corresponding Markov chain, Korda and La (2015)claimed that the following inequality holdsk(θ)k2 ≤ 2H kE(v|Fn) - EΨ,θn (v)k2.	(8)This is not correct. Consider the following counter-example. Let the batch size M = 3 and thedimension of the feature vector be one, i.e., Φ ∈ RlSl×1. Hence, all variables in eq. (8) and eq. (7) arescalars. Since the steps for proving eq. (8) in Korda and La (2015) do not have specific requirementsfor the transition kernel, eq. (8) should hold for any distribution of v. Thus, suppose v follows theuniform distribution over [-3, 3]. Further assume that in the n-th epoch, the samples of v are givenby {1,2, -3}. Recall that E(∙∣Fn) is the average over the batch samples in the n-th epoch. We have:Eψ,θn (V) = 0,	Eψ,θn (v2) = 3,	E(v∣Fn) = 0,	E(v2∣Fn) = ɪ
