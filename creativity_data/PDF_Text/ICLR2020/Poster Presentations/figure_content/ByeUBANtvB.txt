Figure 1: Learning feedback weights through perturbations. (A) Backpropagation sends error infor-mation from an output loss function, L, through each layer from top to bottom via the same matricesWi used in the feedforward network. (B) Node perturbation introduces noise in each layer, ξi, thatperturbs that layer’s output and resulting loss function. The perturbed loss function, L, is correlatedwith the noise to give an estimate of the error current. This estimate is used to update feedbackmatrices Bi to better approximate the error signal.
Figure 2: Node perturbation in small 4-layer network (784-50-20-10 neurons), for varying noiselevels c, compared to feedback alignment and backpropagation. (A) Relative error between feedfor-ward and feedback matrix. (B) Angle between true gradient and synthetic gradient estimate for eachlayer. (C) Percentage of signs in Wi and Bi that are in agreement. (D) Test error for node perturba-tion, backpropagation and feedback alignment. Curves show mean plus/minus standard error over 5runs.
Figure 3: Results with five-layer MNIST autoencoder network. (A) Mean loss plus/minus standarderror over 10 runs. Dashed lines represent training loss, solid lines represent test loss. (B) Latentspace activations, colored by input label for each method. (C) Sample outputs for each method.
Figure 4: Convergence of node perturbation method in a two hidden layer neural network (784-50-20-10) with MSE loss, for varying noise levels c. Node perturbation is used to estimate feedbackmatrices that provide gradient estimates for fixed W. (A) Relative error (kWi - BikF/kWikF)for each layer. (B) Angle between true gradient and synthetic gradient estimate at each layer. (C)Percentage of signs in Wi and Bi that are in agreement. (D) Relative error when number of neuronsis varied (784-N-50-10). (E) Angle between true gradient and synthetic gradient estimate at eachlayer.
