Figure 1: Illustration of three different bounding processes: Fully-Forward (a), Fully-Backward (b),and Backward&Forward (c). We show an example ofa 2-layer Transformer, where operations can bedivided into two kinds of blocks, “Feed-forward” and “Self-attention”. “Self-attention” contains op-erations in the self-attention mechanism starting from queries, keys, and values, and “Feed-forward”contains all the other operations including linear transformations and unary nonlinear functions. Ar-rows with solid lines indicate the propagation of linear bounds in a forward manner. Each backwardarrow Ak → Bk with a dashed line for blocks Ak , Bk indicates that there is a backward boundpropagation to block Bk when computing bounds for block Ak . Blocks with blue rectangles haveforward processes inside the blocks, while those with green rounded rectangles have backward pro-cesses inside.
