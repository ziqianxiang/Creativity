Figure 1: Overview of the MAD competition procedure. (a): A large unlabeled image set of Webscale. (b): The subset of natural images selected from (a) on which two classifiers (VGG16BNand ResNet34 in this case) make different predictions. Note that collecting the class label for eachimage in this subset may still be prohibitive because of its gigantic size. (c): Representative ex-amples sampled from top-k images on which VGG16BN’s and ResNet34’s predictions differ themost, quantified by Eq. (3). Although the two classifiers have nearly identical accuracies on theImageNet validation set, the proposed MAD competition successfully distinguishes them by findingtheir respective counterexamples. This sheds light on potential ways to improve the two classifiersor combine them into a better one. The model predictions are shown along with the images, wheregreen underlined and red italic texts indicate correct and incorrect predictions, respectively.
Figure 2: Comparison of weighted and un-weighted distances. In the sub-tree of Word-Net, we highlight the shortest paths from“fountain” to “church” and from “drake” to“American coot” in green and red, respec-tively. The semantic distance between thetwo aquatic birds is much shorter than thatbetween the two constructions of completelydifferent functionalities (verified by our in-ternal subjective testing). The proposedweighted distance is well aligned with hu-man cognition by assigning a much smallerdistance to the red path (0.0037) comparedwith the green one (0.0859).
Figure 4: Visual comparison of images selected by MAD and in the ImageNet validation set. (a):“manhole cover” images selected by MAD along with the predictions by the associated classifiers.
Figure 3: PairWiSe accuracy matrix A withbrighter colors indicating higher accuracies (bluenumberS). WSL-ReSNeXt101-32×48 iS abbrevi-ated to WSL for neat preSentation.
Figure 5: The SRCC values between top-30 andother top-k rankings, k = {1, 2, . . . , 29}.
Figure 6: Visual comparison of “broccoli” and “soccer ball” images (a) selected by MAD and (b) inthe ImageNet validation set.
Figure 7: Examples of network bias. WSL-ResNeXt101-32×48 (WSL) tends to focus on foregroundobjects, while EfficientNet-B7 attends more to background objects.
Figure 8: Examples of relation inference. (a): Snorkel is correlated to underwater environment. (b):Basketball is correlated to basketball court. (c): Toaster is correlated to toasted bread. (d): Diskbrake is correlated to freewheel and spokes. Similar with how humans recognize objects, it wouldbe reasonable for DNN-based classifiers to make predictions by inferring useful information fromobject relationships, only when their prediction confidence is low. However, this is not the case inour experiments, which show that classifiers may make high-confidence predictions by leveragingobject relations without really “seeing” the predicted object.
Figure 9: Examples of network bias to low-level visual features, such as color, shape and texture,while overlooking conflicting semantic cues. An ideal classifier is expected to utilize both low-level(appearance) and high-level (semantic) features when making predictions. ResNeXt101-32×4 isabbreviated to ResNeXt101 for neat presentation.
