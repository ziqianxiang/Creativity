Figure 1: SpiderBoost with 10% sparsity (k = 0.1d) compared to SpiderBoost without sparsity. Leftfigure compares the two algorithms using Resnet-18 on Cifar-10. Right figure compares the twoalgorithms using a conVolutional neural network trained on MNIST. The x-axis measures gradientqueries oVer N, where N is the size of the respectiVe datasets. Plots are in log-scale.
Figure 2: (a): SGD learning rate is 0.2 and batch size is 20. (b): SGD batch size is 103 and learningrate schedule is 0.1 for epochs 0 - 10, 0.01 for epochs 10 - 20, and 0.001 for epochs 20 - 40. Thex-axis measures gradient queries over N , where N is the size of the respective datasets. Plots are inlog-scale.
Figure 3: SpiderBoost with various values of sparsity, where (sparsity=k/d) corresponds to Spider-Boost with sparsity k/d. Both figures use MNIST. The x-axis measures gradient queries over N ,where N is the size of the respective datasets. Plots are in log-scale.
