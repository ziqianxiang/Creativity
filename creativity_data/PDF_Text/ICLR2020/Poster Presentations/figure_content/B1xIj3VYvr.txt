Figure 1: Weakly supervised clustering framework. Our framework (green dashed line) consists ofthe U CC model (magenta dashed line) and the unsupervised instance clustering branch.
Figure 2: Weakly supervised clustering framework. (a) UCC model: θfeature extracts J features,shown in colored nodes. KDE module obtains feature distribution for each feature. Then, θdrn pre-diets the ucc label 诅∙ Concurrently, decoder module θde∞der in autoencoder branch reconstructs theinput images from the extracted features. (b) Unsupervised clustering: Trained feature extractor,fture, is USed to extract the features of all instances in X and unsupervised clustering is performedon extracted features. Note that li is clustering label of xi ∈ X.
Figure 3:	Clustering accuracy vs ucc accuracy plots of UCC and UCCα=1 models together withk-means and spectral clustering accuracy baselines on MNIST, CIFAR10 and CIFAR100 datasets.
Figure 4:	Example images from hold-out test dataset with corresponding uccground truthmasks and predicted masks by U CCsegment, Unet and K-means clustering models.
Figure 5: KDE module - the Gaussian kernel (κ(v - fσj,i)) for each extracted feature for a sample isillustrated with colored curves and previously accumulated kernels are shown in gray. Estimated fea-ture distributions, which are obtained by employing Equation 2, are sampled at some pre-determinedintervals and passed to θdrn.
Figure 6: Confusion matrices of our UCC and UCC2+ models for ucc prediction on MNIST.
Figure 7: Confusion matrices of ourPredicted label(b) UCC2+U C C and U C C 2+ models for ucc prediction onCIFAR10.
Figure 8:	Confusion matrices of our UCC and UCC2+ models for ucc prediction on CIFAR100.
Figure 9:	Inter-class JS divergence matrix calculated over the distributions of features extracted byour F ullySupervised and UCC models on MNIST test dataset.
Figure 10: Distributions of extracted features by our FullySupervised model on MNIST test dataset. Each column corresponds to a feature learned by model andeach row corresponds to an underlying class in the test dataset.
Figure 11: Distributions of extracted features by our UCC model on MNIST test dataset. Each column corresponds to a feature learned by model and each rowcorresponds to an underlying class in the test dataset.
Figure 12: Confusion matrix of our U C Csegment model for ucc predictions on our segmentationdataset.
Figure 13: Training and validation loss curves during training of our Unet model. We have used thebest model weights, which were saved at iteration 58000, during training. Models starts to overfitafter iteration 60000 and early stopping terminates the training.
