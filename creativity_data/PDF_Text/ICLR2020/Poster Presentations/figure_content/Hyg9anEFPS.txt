Figure 1: Overview of our image-guided rendering approach: based on the nearest neighbor views,we predict the corresponding view-dependent effects using our EffectsNet architecture. The view-dependent effects are subtracted from the original images to get the diffuse images that can be re-projected into the target image space. In the target image space we estimate the new view-dependenteffect and add them to the warped images. An encoder-decoder network is used to blend the warpedimages to obtain the final output image. During training, we enforce that the output image matchesthe corresponding ground truth image.
Figure 2: EffectsNet is trained in a self-supervised fashion. In a Siamese scheme, two randomimages from the training set are chosen and fed into the network to predict the view-dependenteffects based on the current view and the respective depth map. After re-projecting the source imageto the target image space we compute the diffuse color via subtraction. We optimize the network byminimizing the difference between the two diffuse images in the valid region.
Figure 3: Ablation study w.r.t. the EffectsNet. Without EffectsNet the specular highlights are not assmooth as the ground truth. Besides, the EffectsNet leads to a visually consistent temporal animationof the view-dependent effects. The close-ups show the color difference w.r.t. ground truth.
Figure 4: Prediction and removal of view-dependent effects of a highly specular real object.
Figure 5: Comparison to Pix2Pix on real data. It can be seen that Pix2Pix can be used to synthesizenovel views. The close-up shows the artifacts that occur with Pix2Pix and are resolved by ourapproach leading to higher fidelity results.
Figure 6: Comparison to the IBR method InsideOut of Hedman et al. (2016) and the learned IBRblending method DeepBlending of Hedman et al. (2018). To better show the difference in shading,we computed the quotient of the resulting image and the ground truth. A perfect reconstructionwould result in a quotient of 1. As can be seen our approach leads to a more uniform error, whilethe methods of Hedman et al. show shading errors due to the view-dependent effects.
Figure 7: Renderings of our ground truth synthetic data. Based on the Mitsuba Renderer (Jakob,2010), we generate images of various objects that significantly differ in terms of material propertiesand shape.
Figure 8: Based on a set of multi-view images, we reconstruct a coarse 3D model. The cameraposes estimated during reconstruction and the 3D model are then used to render synthetic depthmaps for the input views.
Figure 9: Comparison of the estimated diffuse images based on EffectsNet and the ground truthrenderings. The input data has been synthesized by a standard Phong renderer written in DirectX.
Figure 10: Comparison of our neural object rendering approach to IBR baselines. The naive IBRmethod uses the same four selected images as our approach as input and computes a pixel-wiseaverage color. The method of Debevec et al. (1998) uses all reference views (n = 20) and a pertriangle view selection. The training set contained 1000 images.
Figure 11: In comparison to Pix2Pix with position maps as input, we can see that our techniqueis able to generate images with correct detail as well as without blur artifacts. Both methods aretrained on a dataset of 920 images.
Figure 12: In this graph we compare the influence of the training corpus size on the MSE for ourapproach and Pix2Pix trained on position maps. The full dataset contains 920 images. We graduallyhalf the size of the training set. As can be seen, the performance of our approaches degrades moregracefully than Pix2Pix.
Figure 13: Image synthesis on real data in comparison to classical computer graphics renderingapproaches. From left to right: Poisson reconstructed mesh with per-vertex colors, texture-basedrendering, our results and the ground truth. Every texel of the texture is a cosine-weighted sum ofthe data of four views where the normal points towards the camera the most.
Figure 14: Image synthesis on real data: we show a comparison to the IBR technique of Debevecet al. (1998). From left to right: reconstructed geometry of the object, result of IBR, our result, andthe ground truth.
