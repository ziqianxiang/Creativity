Figure 1: Learning rate schedules and CIFAR10 test accuracies for workers participating in SWAP. The large-batch phase with synchronized models is followed by the small-batch phase with diverging independent models.
Figure 2: CIFAR10 train and test error restricted to a 2D plane spanned by the output of phase 1 (‘LB’), oneof the outputs of phase 2 (‘SGD’) and the averaged model (‘SWAP’).
Figure 3: CIFAR10 train and test error restricted to a 2D plane spanned by the output of three workers afterphase 2 (‘SGD1’, ‘SGD2’, ‘SGD3’) and location of the average model (‘SWAP’). The minimum test errorachievable for models restricted to this region of the plane (marked as BEST).
Figure 4: Cosine similarity between direction of gradient descent and ∆θ5	ExperimentsIn this section we evaluate the performance of SWAP for image classification tasks on the CIFAR10,CIFAR100, and ImageNet datasets.
Figure 5: Learning rate and mini-batch schedules used for ImageNet. The original schedule for 8 GPUS wastaken from an existing DAWNBench submission. For a larger batch experiment, we double the batch size,double the number of GPUs and double the learning rate of the original schedule. For SWAP, we switch fromthe modified schedule to the original schedule as we move from phase 1 to phase 2.
Figure 6: Illustration of SWA with different batch sizes5.3 EMPIRICAL COMPARISON OF SWA AND SWAPWe now compare SWAP with SWA: the sequential weight averaging algorithm from Izmailov et al.
