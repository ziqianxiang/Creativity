Figure 1: The three distillation settings We consider: (a) compressing a model, (b) transferring knowledge fromone modality (e.g., RGB) to another (e.g., depth), (c) distilling an ensemble of nets into a single network. Theconstrastive objective encourages the teacher and student to map the same input to close representations (insome metric space), and different inputs to distant representations, as indicated in the shaded circle.
Figure 2: The correlations between class logits of a teacher network are ignored by regular cross-entropy.
Figure 4: Distillation from an ensemble of teachers. We vary the number of ensembled teachers and compareKD with our CRD by using (a) WRN-16-2 and (b) ResNet20. Our CRD consistently achieves lower error rate.
Figure 5: Effects of varying the number of negatives, shown in (a), or the temperature, shown in (b).
Figure 6: The correlations between class logits output by the teacher network show the â€œdark knowledge"Hinton et al. (2015) that must be transferred to a student networks. A student network that captures thesecorrelations tends to perform better at the task. We visualize here the difference between correlation matrices ofthe student and teacher at the logits, for different student networks on a Cifar100 knowledge distillation task:(a) A student trained without distillation; (b) A student distilled by attention transfer Zagoruyko & Komodakis(2016a) (c) A student distilled by KL divergence Hinton et al. (2015); (d) A student distilled by our contrastiveobjective. Our objective greatly improves the structured knowledge (correlations) in the output units.
