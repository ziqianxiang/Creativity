Figure 1: The agent (in orange) is tasked with reaching its goal, the checkered flag (middle frame). It maytake the shorter path (right frame), which entails breaking the vases in its way, or it may prefer the safer path(left frame) which is longer but keeps the vases intact. The former path is irreversible, and the initial state isunreachable from the final state (red arrow). On the contrary, the latter path is completely reversible, and theinitial state remains reachable from the final state. Now, an arrow of time (pink) measures the disorder, whichmight help a safe agent decide which path to take.
Figure 2: The potential difference (i.e. change in h-potential) between consecutive states along a trajec-tory on the Vaseworld (2D world) environment. Thedashed vertical lines denote when a vase is broken.
Figure 3: The h-potential along a trajectory froma random policy, annotated with the correspondingstate images on the Sokoban (2D world) environment.
Figure 4: The h-potential (for Mountain Car) atzero-velocity plotted against position. Also plot-ted (orange) is the height profile of the mountain.
Figure 5: The h-potential as a function of state (position and velocity) for (continuous) Mountain-Car with andwithout friction. The overlay shows random trajectories (emanating from the dots). Gist: with friction, we findthat the state with largest h is one where the car is stationary at the bottom of the valley. Without friction, thereis no dissipation and the car oscillates up and down the valley. Consequently, we observe that the h-potential isconstant (up-to edge effects) and thereby uninformative.
Figure 6: The true arrow of time (the Free-Energy functional, in blue) plotted against thelearned arrow of time (the H -functional, i.e. thenegative spatial expectation of the h-potential;plotted in orange) after linear scaling and shifting.
Figure 7: The two-state Markov chain considered in Examples 1 and 3.
Figure 8: The four-state Markov chain considered in Examples 2 and 4.
Figure 9: The potential difference η plotted along trajectories, where the state-space is augmented with tem-porally uncorrelated (TV-) and correlated (causal) noise. The dashed vertical lines indicate time-steps where avase is broken. Gist: while our method is fairly robust to TV-noise, it might get distracted by causal noise.
Figure 10: The h-potential along a trajectory sam-pled from a random policy. Gist: The h-potentialincreases step-wise along the trajectory every time anagent (irreversibly) breaks a vase. It remains constantas the agent (reversibly) moves about.
Figure 11: Histogram (over trajectories) of valuestaken by h at time-steps t = 0, t = 32 and t = T =128.
Figure 12: Probability of reaching the goal and the expected number of vases broken, obtained over 100 eval-uation episodes (per step). Gist: while the safety Lagrangian results in fewer vases broken, the probability ofreaching the goal state is compromised. This trade-off between safety and efficiency is expected (cf. Moldovan& Abbeel (2012)).
Figure 13: The intrinsic reward (Eqn 28) plotted against an engineered reward, which in this case is the amountof moisture gained by the tomato plant the agent just watered. Gist: the h-Potential captures useful informationabout the environment, which can then be utilized to define intrinsic rewards.
Figure 14: h-Potential averaged over 8000 trajectories, plotted against timestep t; shaded band shows thestandard deviation. Gist: as required by its objective (Eqn 1), the h-Potential must increase in expectationalong trajectories.
Figure 15: Random samples from 200 transitions that cause the largest increase in the h-potential (out of asample size of 8000 transitions). The orange, white, blue and green sprites correspond to a wall, the agent, abox and a goal marker respectively. Gist: pushing boxes against the wall increases the h-potential.
Figure 16:	The initial state of the conveyor belt environment with an agent (orange robot), a vase (blue) and aconveyor belt (red arrows). The conveyor belt carries the vase rightwards, until it falls off it and breaks.
Figure 17:	Illustration of the policies we use to test the safety reward assigned by the h-potential. Gist: thegood agent removes the vase from the vase and stays put. The passive policy stays put and does nothing as thevase falls off the belt. The malicious policy removes the vase from the belt (possibly to collect a reward) onlyto put it back on it again. The inept policy removes the vase from the belt, but pushes it to a corner (the agentlacks the ability to pull it back).
Figure 18: The normalized returns awarded to the various policies discussed in text and illustrated in Fig 17,averaged over 5 training runs (shaded bands are the standard deviations). Gist: the h-potential penalizes allirreversible behaviour, including the vase being pushed in to a corner by the inept policy (in addition to it fallingoff the belt due to the passive and malicious policies).
Figure 19: Gist: the learned h-Potential takes large values around (θ, θ) = 0, since that is Where mosttrajectories terminate due to the effect of damping.
Figure 20:	Random trajectories (white curves emanating from the dots) overlayed on the h-potential as afunction of state (position and velocity). Gist: the h-potential trained on random trajectories fails to whollycharacterize the dynamics of the considered environment. This is due to a lack of adequate exploration by therandom policy.
Figure 21:	The h-potential as a function of state (position and velocity). The overlaid curves (in white,emanating from the dots) show samples from the trajectory buffer used to train the respective h-potential. Gist:we find in Fig 21a that the exploration bias causes the h-potential to over-specialize to one section of thestate-space whilst ignoring the other. This can be contrasted with Fig 21b, where the trajectories are gatheredby initializing a random policy at states reached by exploratory policies trained to maximize random rewardfunctions.
Figure 22: Learned h-Potential as a function of position x. Observe the qualitative similarity to the potentialΨ defined in Eqn 32.
