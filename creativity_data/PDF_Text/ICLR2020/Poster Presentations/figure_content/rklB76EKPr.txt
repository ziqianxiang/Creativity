Figure 1: Illustration of the Huberised and partially Huberised logistic loss (left panel). Recall that the logisticloss comprises the log-loss (or cross-entropy) composed with a sigmoid link. The Huberised lossarises from loss-based gradient clipping, and linearises the entire logistic loss beyond a threshold.
Figure 2: Results on synthetic data. In (b), the solid (dashed) curve denotes empirical risk without (with) outliers.
Figure 3: Risk minimiser for the Huberised logistic loss.
Figure 4: Risk minimiser for the partially Huberised logistic loss.
Figure 5: Illustration of Huberised logistic loss. The effect of Huberisation is to linearise the loss beyond acertain point, or equally, to cap the derivative when its magnitude exceeds a certain threshold.
Figure 6: Illustration of link function for the Huberised logistic loss. The effect of Huberisation is to make thelink function saturate slower. Note that when T ≤ 1, the link function is not invertible everywhere.
Figure 7: Illustration of partial Huberisation, in terms of the underlying proper loss and its composite form.
Figure 8: Illustration of generalised cross-entropy loss for various choices of α ∈ [0, 1).
Figure 9: Illustration of link function for partially Huberised logistic loss. The effect of partial Huberisation is tomake the link function saturate slower. When τ < 2, the link function is not invertible everywhere.
Figure 10: Distribution proposed in Long & Servedio (2010), which defeats any member of a broad family ofconvex losses. The data comprises six points, with the blue points labelled positive, and the redpoints labelled negative. The two “fat” points have twice as much probability mass as their “thin”counterparts. While the dataset is trivially linearly separable, minimising a broad range of convexlosses with a linear model under any non-zero amount of symmetric label noise results in a predictorthat is tantamount to random guessing.
