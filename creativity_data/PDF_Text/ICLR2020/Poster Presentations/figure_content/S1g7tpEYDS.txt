Figure 1: Qualitative evaluation of sample quality for VAEs, WAEs, 2sVAEs, and RAEs on CelebA.
Figure 2: Generating structured objects by GVAE, CVAE and GRAE. (Upper left) Percentage ofvalid samples and their average mean score (see text, Section 6.2). The three best expressions(lower left) and molecules (upper right) and their scores are reported for all models.
Figure 3:	(Left) Test reconstruction quality for a VAE trained on MNIST with different numbersof samples in the latent space as in Eq. 7 measured by FID (lower is better). Larger numbers ofMonte-Carlo samples clearly improve training, however, the increased accuracy comes with largerrequirements for memory and computation. In practice, the most common choice is therefore k = 1.
Figure 4:	PRD curves of all RAE methods (left), reflects a similar story as FID scores do. RAE-SN seems to perform the best in both precision and recall metric. PRD curves of all traditionalVAE variants (middle). Similar to the conclusion predicted by FID scores there are no clear winner.
Figure 5: PRD curves of all methods on image data experiments on MNIST. For each plot, we showthe PRD curve when applying the fixed or the fitted one by ex-post density estimation (XPDE).
Figure 6: PRD curves of all methods on image data experiments on CIFAR10. For each plot, weshow the PRD curve when applying the fixed or the fitted one by ex-post density estimation (XPDE).
Figure 7:	PRD curves of all methods on image data experiments on CELEBA. For each plot, weshow the PRD curve when applying the fixed or the fitted one by ex-post density estimation (XPDE).
Figure 8:	Qualitative evaluation for sample quality for VAEs, WAEs and RAEs on MNIST. Left:reconstructed samples (top row is ground truth). Middle: randomly generated samples. Right:spherical interpolations between two images (first and last column).
Figure 9:	Qualitative evaluation for sample quality for VAEs, WAEs and RAEs on CIFAR-10. Left:reconstructed samples (top row is ground truth). Middle: randomly generated samples. Right:spherical interpolations between two images (first and last column).
Figure 10:	Nearest neighbors to generated samples (leftmost image, red box) from training set.
Figure 11: Different density estimations of the 2-dimensional latent space of a VAE learned onMNIST. The blue points are 2000 test set samples while the orange ones are drawn from the es-timator indicated in each column: isotropic Gaussian (left), multivariate Gaussian with mean andcovariance estimated on the training set (center) and a 10-component GMM (right). This clearlyshows the aggregated posterior mismatch w.r.t. to the isotropic Gaussian prior imposed by VAEsand how ex-post density estimation can help fix the estimate.
Figure 12: Different density estimations of the 16-dimensional latent spaces learned by all modelson MNIST (see Table 1) here projected in 2d via T-SNE. The blue points are 2000 test set sampleswhile the orange ones are drawn from the estimator indicated in each column: isotropic Gaussian(left), multivariate Gaussian with mean and covariance estimated on the training set (center) and a10-component GMM (right). Ex-post density estimation greatly improves sampling the latent space.
