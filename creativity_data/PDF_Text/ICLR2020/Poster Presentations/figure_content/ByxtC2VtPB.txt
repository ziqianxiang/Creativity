Figure 1: Intuitive mechanisms in the input space of different input-processing based defenses. x is the crafted adversarial example, x0 is theoriginal clean example, which is virtual and unknown for the classifiers. δ is the adversarial perturbation.
Figure 2: The results are averaged on 100 randomly test clean samples of CIFAR-10. The adversarial attack is untargeted PGD-10. Note thatthe ∆Gy calculated here is the minus value of it in Eq. (12) and Eq. (15).
Figure 3: Results on CIFAR-10. (a) AUC scores on 1,000 randomly selected test clean samples and 1,000 adversarial counterparts craftedon these clean samples. (b) The adversarial accuracy w.r.t clean accuracy on 1,000 randomly selected test samples. The adversarial attack isuntargeted PGD-10, with = 8/255 and step size 2/255. Each point for a certain method corresponds to a set of hyperparameters.
Figure 4: Classification accuracy under the adaptive PGD attacks on CIFAR-10. The number of adaptive samples refers to the execution timesof sampling xs in each iteration step of adaptive PGD. The dash lines are the accuracy of trained models without MI-OL under PGD attacks.
Figure 5: Adversarial examples crafted by adaptive attacks with = 16/255 on CIFAR-10, against the defense of Interpolated AT + MI-OL.
