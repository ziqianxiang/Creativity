Figure 1: Replaced token detection pre-training consistently outperforms masked language modelpre-training given the same compute budget. The left figure is a zoomed-in view of the dashed box.
Figure 2: An overview of replaced token detection. The generator can be any model that producesan output distribution over tokens, but we usually use a small masked language model that is trainedjointly with the discriminator. Although the models are structured like in a GAN, we train thegenerator with maximum likelihood rather than adversarially due to the difficulty of applying GANsto text. After pre-training, we throw out the generator and only fine-tune the discriminator (theELECTRA model) on downstream tasks.
Figure 3: Left: GLUE scores for different generator/diScriminator sizes (number of hidden units).
Figure 4: Left and Center: Comparison of BERT and ELECTRA for different model sizes. Right:A small ELECTRA model converges to higher downstream accuracy than BERT, showing the im-provement comes from more than just faster training.
