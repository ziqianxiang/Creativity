Figure 2: Neural network for disentanglement of consistent features (K = 3).
Figure 3: Unreliable components and blind spots of a weak DNN (AlexNet) w.r.t. a strong DNN(ResNet-34). Please see Section 4.1 for definitions of “unreliable components” and “blind spots.”(left) When we used features of the weak DNN to reconstruct features of the strong DNN, wevisualize raw features of the strong DNN, feature components that can be reconstructed, and blindspots (x∆) disentangled from features of the strong DNN. The weak DNN mainly encoded the headappearance and ignored others. We can find that some patterns in the torso are blind spots of theweak DNN. (right) When we used features of the strong DNN to reconstruct features of the weakDNN, we visualize raw features of the weak DNN, feature components that can be reconstructed,and unreliable features (x∆) disentangled from features of the weak DNN. Based on visualizationresults, unreliable features of the weak DNN usually repesent noisy activations.
Figure 4: Effects of network compression and knowledge distillation. (left) We visualized the dis-carded feature components when 93.3% parameters of the DNN were eliminated. We found thatthe discarded components looked like trivial features and noises. In the curve figure, a low valueof V ar(x∆) indicates a lower information discarding during the compression, thereby exhibiting ahigher accuracy. (right) We visualized and quantified knowledge blind spots of born-again networksin different generations. Nets in new generations usually had fewer blind spots than old nets.
Figure 5: Consistent and inconsistent feature components disentangled between DNN B learned forbinary classification and DNN A learned for fine-grained classification.
