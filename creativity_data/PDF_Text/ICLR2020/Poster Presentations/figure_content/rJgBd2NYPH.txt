Figure 1: Architecture overview of the proposed deep graph matching networks that consist of theproposed channel-independent embedding and Hungarian attention layer over the loss function.
Figure 2: Illustration of the proposed CIE layer for embedding based deep graph matching. Theoperation “Linear” refers to the linear mapping, e.g. Hw) → Wy)Hw) in Eq (9).
Figure 3: A working example illustrating of our proposed Hungarian attention pipeline starting fromsimilarity matrix. Sinkhorn algorithm solves similarity matrix into a doubly-stochastic matrix in adifferentiable way. A discrete permutation matrix is further obtained via Hungarian algorithm. Ourproposed Hungarian attention, taking the ground truth matching matrix into account, focuses onthe “important” digits either labeled true or being mis-classified. The output matrix is obtained byattention pooling from doubly-stochastic matrix, where we compute a loss on it.
Figure 4: Performance study on Pascal VOC. Note in (a) the loss is calculated on all matching digitsfor both CIE1-P and CIE1-H. Note around 10th epoch, the accuracy of CIE1-P almost reaches thehighest, but the loss keeps descending until 30th epoch. This indicates that in most of the latterepochs, P-loss performs “meaningless” back-propagation to drag the output to binary. H-loss, byaccommodating smoothness, can emphasize most contributing digits and achieves higher accuracy.
Figure 5: Visualization of a matching result: 10 key points in each image with 7 and 8 correctmatchings dispalyed, respectively. Different colors across images indicate node correspondence.
Figure 6: Results on synthetic test where two different loss functions are compared in ablative study.
Figure 7: Image examples from Pascal VOC and Willow.
