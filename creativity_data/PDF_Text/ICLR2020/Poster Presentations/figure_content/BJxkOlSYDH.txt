Figure 1: Overview of our pruning method. We use a small batch of data points to quantify the relativeimportance Sj of each filter W' in layer ' by considering the importance of the corresponding feature mapa' = φ(z') in computing the output z'+1 of layer ' + 1, where φ(∙) is the non-linear activation function. Wethen prune filters by sampling each filter j with probability proportional to Sj and removing the filters that werenot sampled. We invoke the filter pruning procedure each layer to obtain the pruned network (the prune step);we then retrain the pruned network (retrain step), and repeat the prune-retrain cycle iteratively.
Figure 2: Early layers of VGG are relatively harder to approximate due to their large spatial dimensions as shownin (a). Our error bounds naturally bridge layer compressibility and importance and enable us to automaticallyallocate relatively more samples to early layers and less to latter layers as shown in (b). The final layer - due toits immediate influence on the output - is also automatically assigned a large portion of the sampling budget.
Figure 3: The accuracy of the generated pruned models for the evaluated pruning schemes for various targetprune ratios. Note that the x axis is the percentage of parameters retained, i.e., (1 - pruneratio). ThiNet wasomitted from the plots for better readability. Our results show that our approach generates pruned networks withminimal loss in accuracy even for high prune ratios. Shaded regions correspond to values within one standarddeviation of the mean.
Figure 4: The performance of the comparedalgorithms on pruning a lightweight networkfor a real-time regression task (Amini et al.,2018).
Figure 5: The performance of our approach on a LeNet300-100 architecture trained on MNIST with noderandomization (denoted by "rand"), with partial derandomization (denoted by "partial"), and with completederandomization (denoted by "derand"). The plot in (a) and (b) show the resulting test accuracy for variouspercentage of retained parameters 1 - (pruneratio) before and after retraining, respectively. The additionalerror of the derandomized algorithm can be neglected in practical settings, especially after retraining.
Figure 6:	The results of our evaluations of the algorithms in the prune-only scenario, where the network isiteratively pruned down to a specified target prune ratio and the fine-tuning step is omitted. Note that the x axisis the percentage of parameters retained, i.e., (1 - pruneratio).
Figure 7:	The performance of our approach on a regression task used to infer the steering angle for an autonomousdriving task (Amini et al., 2018). (a) An exemplary image taken from the data set. (b) The performance of ourpruning procedure before retraining evaluated on the test loss and compared to competing filter pruning methods.
