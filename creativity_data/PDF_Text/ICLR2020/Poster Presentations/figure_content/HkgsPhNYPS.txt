Figure 1: Comparing the performance of SELF with previous works for learning under different(symmetric) label noise ratios on the (a) CIFAR-10 & (b) CIFAR-100 datasets. SELF retains higherrobust classification accuracy at all noise levels.
Figure 2: Overview of the self-ensemble label filtering (SELF) framework. The model starts initeration 0 with training from the noisy label set. During training, the model maintains a self-ensemble, a running average of itself (Tarvainen & Valpola, 2017) to provide a stable learning signal.
Figure 3: Maintaining the (a) model and (b) predictions ensembles is very effective against noisymodel updates. These ensembles are self-forming during the training process as a moving-averageof (a) model snapshots or (b) class predictions from previous training steps.
Figure 4: Ablation study on the importance of the components in our framework SELF, evaluatedon (a) Cifar-10 and (b) Cifar-100 with uniform noise. Please refer Tab. 5 for details of components.
Figure 5: Simple training losses to counter label noise. (a) shows the prediction of a sample givena model. The red bar indicates the noisy label, blue the correct one. Arrows depict the magnitudeof the gradients (b) Typical losses reweighting schemes are not wrong but suffer from the gradientvanishing problem. Non-linear losses such as Negative-log-likelihood are not designed for gradientascent.
Figure 6: The entropy loss for semi-supervised learning. (a) Extreme predictions such as [0, 1]are encouraged by minimizing the entropy on each prediction. (b) Additionally, maximizing theentropy of the mean prediction on the entire dataset or a large batch forces the model to balance itspredictions over multiple samples.
Figure 7: Sample training curves of our approach SELF on CIFAR-100 with (a) 60% and (b) 80%noise, using noisy validation data. Note that with our approach, the training loss remains close to 0.
