Figure 1: Adaptation to the transfer distribution (average log-likelihood of the model during fine-tuning adaptation to transfer examples, vertical axis), as more transfer examples are seen by thelearner (horizontal axis). The curves are the median over 20,000 runs, with their 25-75th quantilesintervals. The dotted line is the asymptotic log-likelihood (here, that of the ground truth p). Thered region corresponds to the range where the effect is the most significant (10-30 samples from thetransfer distribution).
Figure 2: Evolution of the belief that A → B is the correct causal model, as the number of episodesincreases, starting with an equal belief for both hypotheses. (Left) multinomial logistic CPDs, (right)MLP parametrization.
Figure 3: The complete experimental setup. The ground-truth variables (A, B) are assumed tooriginate from the true underlying causal model, but the observations available to the learner aresamples from (X, Y ). The observed variables (X, Y ) are derived from (A, B) via the action of adecoder D. The encoder E must be learned to undo this action of the decoder, and thereby recoverthe true causal variables up to symmetries. The components of the data generation on the left arehidden to the model.
Figure 4: Evolution of structural parameters θE and γ, as number of episodes increases. Angle of therotation for the decoder is set to Θd = -n/4, so there are two valid solutions for the angle Θe of theencoder: either Θe = n/4, or Θe = -n/4; the model converges to the former solution.
