Figure 1: iRNN depicted by unfolding into K recursions for one transition from g0 = hm-1 tohm = gκ. Here,2(x, g, h) = φ(U(g + h) + Wx + b) - α(g + h). See Sec. A.2 for implementationand pseudo-code. This resembles Graves (2016), who propose to vary K with m as a way to attendto important input transitions. However, the transition functions used are gated units, unlike ourconventional ungated functions. As such, while this is not their concern, equilibrium may not evenexist and identity gradients are not guaranteed in their setup.
Figure 2: Phase-space trajectory with tanh activa-tion of RNN, FastRNN, iRNN. X-axis denotes 1stdimension, and Y-axis 2nd dimension of 2D hiddenstate subject to random walk input with variance10 for 1000 time-steps. Parameters U, W, b are ran-domly initialized. RNN states are scaled to fit plotsince FastRNN is not required to be in the cube.
Figure 3: Exploratory experiments for the Add task (a) Convergence with varying K; (b) Ratio ∣∣ ∂T k/k ∂hhTl killustrates VaniShing/Exploding gradient (∣ ∂^hTl k and loss gradients are omitted but displayed in A.7.8. ForiRNN (a) and (b) together show strong correlation of gradient with accuracy in contrast to other methods.
Figure 4: Following Arjovsky et al. (2016) we display average Cross Entropy for the Copy Task (SequenceLength (with baseline memoryless strategy)): (a) 200 (0.09) (b) 500 (0.039). Mean Squared Error for the AddTask, baseline performance is 0.167 (Sequence Length) : (c) 200 (d) 750. For both tasks, iRNN runs K = 5.
Figure 5: Mean Squared Error shown for the Add Task (Sequence Length) : (c) 100 (d) 400(b)A.7 Additional ExperimentsA.7.1 Copying and Addition TasksFigure 5 shows the results for remaining experiments for the addition task for length 100, 400.
Figure 6: Linear convergence in iRNN .
Figure 7: Histogram of the eigenvalues of VφU — I for iRNN on HAR-2 dataset.
Figure 8: Comparison between RNN and iRNN on the magnitudes of gradients.
Figure 9: Exploratory experiments for the Add task : (a) Gradient norms w.r.t. loss ∣∣ 条 k, (b)Gradient norms ∣∣ *τ ∣∣. This together with Figure 3 shows that the gradients are identity everywherefor K = 10	-21Published as a conference paper at ICLR 2020Theorem 3 (Local Convergence with Linear Rate). Assume that the function F in Eq. 12 and the(i)parameter ηk in Eq. 13 satisfy[ηki)]2kVF(gi)F(gi)k2 + 2ηk"F(gi)>VF(gi)F(gi) < 0,∀i,∀k.	(18)Then there exists > 0 such that if kg0 - heq k ≤ where heq denotes the fixed point, the sequence{gi} generated by the Euler method converges to the equilibrium solution in Meq(hk-1, xk) locallywith linear rate.
