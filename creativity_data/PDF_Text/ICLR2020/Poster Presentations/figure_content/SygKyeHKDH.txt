Figure 1: The R2D3 distributed system diagram. The learner samplesbatches that are a mixture of demonstrations and the experiences the agentgenerates by interacting with the environment over the course of training.
Figure 2: Hard-Eight task suite. In each task an agent (H) must interact with objects in its environment in orderto gain access to a large apple (H) that provides reward. The 3D environment is also procedurally generated sothat every episode the state of the world including object shapes, colors, and positions is different. From thepoint of view of the agent the environment is partially observed. Because it may take hundreds of low-levelactions to collect an apple the reward is sparse which makes exploration difficult.
Figure 3: High-level steps necessary to solve the Baseball task. Each step in this sequence must be completed inorder, and must be implemented by the agent as a sequence of low level actions (no option structure is availableto the agent). The necessity of completing such a long sequence of high level steps makes it unlikely that thetask will ever be solved by random exploration. Note that each step involves interaction with physical objects,shown in bold.
Figure 4: (a) Recurrent head used by R2D3 agents. (b) Feedforward head used by the DQfD agent. Headsin both a) and b) are used to compute the Q values. (c) Architecture used to compute the input featurerepresentations. Frames of size 96x72 are fed into a ResNet, the output is then augmented by concatenating theprevious action at-1, previous reward rt-1, and other proprioceptive features ft, such as accelerations, whetherthe avatar hand is holding an object, and the hand’s relative distance to the avatar.
Figure 5: Reward vs actor steps curves for R2D3 and baselines on the Hard-Eight task suite. The curves arecomputed as the mean performance for the same agent across 5 different seeds per task. Error regions show the95% confidence interval for the mean reward across seeds. Several curves overlap exactly at zero reward for thefull range of the plots. R2D3 can perform human-level or better on Baseball, Drawbridge, Navigate Cubes andWall Sensor. R2D2 could not get any positive rewards on any of the tasks. DQfD and BC agents occasionally seerewards on Drawbridge and Navigate Cubes tasks, but this happens rarely enough that the effect is not visible inthe plots. Indicators (H) mark analysis points in Section 6.3.
Figure 7: Guided exploration behavior in the Push Blocks task. (a) Spatial pattern of exploration behaviorat 〜5B actor steps (reward-driven learning kicks off for R2D3 only after 〜20B steps). Overlay of agent'strajectories over 200 episodes. Blocks and sensors are not shown for clarity. R2D2 appears to follow a randomwalk. R2D3 concentrates on a particular spatial region. (b) Interactions between the agent and blocks during thefirst 12B steps. Each line shows a different random seed. R2D2 rarely pushes the blocks. (c) Example trajectoryof R2D3 after training, the agent pushes the blue block onto the blue sensor, then collects the apple (green star).
Figure 8: Guided exploration behavior in the Baseball task. (a) Sub-behaviors expressed by five R2D2 and fiveR2D3 agents after 0.5B steps of training (left) and 4B steps of training (right). Each point is estimated from 200episodes. At 0.5B steps, none of the agents received any reward over the 200 evaluation episodes, while at 4Bsteps, three of the R2D3 agents received reward on almost every episode. Even when the R2D3 agents are notreceiving reward, they are expressing some of the necessary behaviors provided through human demonstrations.
Figure 9: We show the rewards of the R2D3 agent on different tasks for each seed separately.
Figure 10: R2D3 learning curves with varying demo ratios for all tasks.
Figure 11: Further detail of guided exploration behavior in the Push Blocks task (as in Figure 7). (a) Proportionof episodes in which the agent pushes a crate into the recess during the initial 12B steps of training. (b)Proportion of episodes in which the crate pushed into the recess actually matches the sensor color. Data areonly shown when crates are pushed into the recess on at least 5 out of 200 episodes. Dashed line shows theprobability expected if a random crate was pushed into the recess. Thus, while (c) shows that by 12B steps theR2D3 agent may have reasonable success in pushing crates into the recess, it has not yet mastered the logic thatthe crate color must much the sensor color.
Figure 12: Further detail of guided exploration behavior in the Push Blocks task (as in Figure 7). (a) Spatialpattern of exploration behavior for the R2D2 agent over the course of 〜12B steps of training. Each row shows adifferent random seed; the number of training steps increases from the leftmost column to the rightmost column.
