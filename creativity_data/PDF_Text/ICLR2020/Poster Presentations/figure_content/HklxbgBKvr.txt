Figure 1: Comparison of methods on optimizing the energy of protein contact ISing models. Left: the cu-mulative maximum reward depending on the number of rounds for one selected protein target (1A3N). Right:the mean cumulative maximum relative to Random for alternative protein targets. Since f (x) can be well-approximated by a model trained on few examples, model-based training (DyNA PPO)results in a clear im-provement over model-free training (PPO).
Figure 2: Analysis of the performance of DyNA PPO on the Ising model. Left: Performance of DyNAPPO depending on the number of inner policy optimization rounds using the surrogate model. Using 0 roundscorresponds to PPO training. Since the surrogate model is sufficiently accurate, it is useful to perform manyinner loop optimization rounds before querying f(x) again. Right: the R2 of the surrogate model. Since it isalways above the threshold for model-based training (0.5; dashed line), it is always used for training.
Figure 3: Comparison of methods on optimization transcription factor binding sites. Left: maximumcumulative reward f (x) as a function of samples. Right: fraction of local optima found. DyNA PPO optimizesf (x) faster and finds more optima than PPO and baseline methods. Results are shown for one representativetranscription factor target (SIX6 REF R1).
Figure 4: Analysis of the progression of model-based training on the transcription factor task. Left: themean R2 model score averaged across replicas as a function of the number of training samples. The horizontaldashed line indicates the minimum threshold (0.5) for model-based training. Right: the fraction of replicatesthat performed model-based training based on this threshold. Shows that models tend to be inaccurate in earlyrounds and are therefore not used for model-based training. This explains the relatively small improvement ofDyNA PPO over PPO in Figure 3.
Figure 5: Comparison of the proposed exploration bonus vs. entropy regularization on the transcriptionfactor task. Left: performance with exploration bonus as a function of the density penalty λ (Section 2.4).
Figure 6: Comparison of methods on the AMP design task. Left: Model-based training using DyNA PPOand model-free PPO clearly outperform the other methods in terms of the maximum cumulative reward. Right:The mean pairwise hamming distance between sequences proposed at each round, which is lower for DyNAPPO and PPO but does not converge to zero due to the density-based exploration bonus (Figure 11).
Figure 7:	Comparison of PPO against alternative RL methods. Shown are the cumulative maximum rewardand mean pairwise hamming distance for the transcription factoring binding, protein Ising, and AMP problem(Section 4).
Figure 8:	Comparison of additional baselines. We consider the performance of optimizers based on MCMC(Section A.2). SuCh methods are known to be effective optimizers when evaluating the black-box functionis inexpensive, and thus many iterations of sampling can be performed. The focus of our experiments is onresource-constrained black-box optimization. We find that their low sample efficiency makes them undesirablefor biological sequence design. We also consider DbAS with a LSTM generative model instead of a VAEwith multi-layer perceptron decoder to disentangle the choice of generative model in DbAS from the overalloptimization strategy. DbAs VAE outperforms DbAs RNN on all problems except for TF bind.
Figure 9:	Comparison of alternative approaches for promoting diversity. Left column: The proposeddensity based exploration bonus as described in Section 2.4, which adds a penalty to the reward of a sequencex that is proportional to the distance-weighted number of past sequence that are less than a specified distanceaway from x (here edit distance one). Middle column: An alternative approach where the exploration bonusof a sequence is proportional to the distance to the nearest neighboring past sequence. Right column: standardentropy regularization. Shown are the cumulative maximum reward and alternative metrics for quantifyingdiversity depending on the penalty strength (λ in Section 2.4) of each exploration approach. Without explorationbonus (penalty = 0.0; red line), PPO does not find the optimum (cumulative maximum is below 1.0) andthe hamming distance and uniqueness of sequences within a batch converge to zero. PPO finds the optimalsolutions and continues to generate diverse sequences by increasing the strength of any of the three explorationapproaches. The density based exploration bonus is most effective in recovering all optima (second row, leftplot) and enables a more fine-grained control of diversity compared to the distance based approach. Results areshown for target CRX-REF-RI of the transcription factor binding problem.
Figure 10: tSNE embedding of proposed sequences with and without exploration bonus colored by theoptimization round when they were proposed. Without exploration bonus (penalty = 0.0), sequences clusterinto few groups of low diversity at different rounds. With the exploration bonus, the diversity of proposedsequences remain high also in later rounds. Results are shown for target CRX_REF_R1 of the transcriptionfactor binding problem.
Figure 11: Analysis of density-based exploration bonus on the AMP problem. The top row shows thesensitivity to the distance radius and the bottom row to the regularization strength λ (Section 2.4). Diversitycorrelates positively with the distance radius and regularization strength. = 2 and λ = 0.1 provides thebest trade-off between optimization performances (cumulative maximum reward) and diversity (mean pairwisehamming distance and uniqueness). Penalizing only exact duplicates ( = 0) is less effective in maintaining ahigh hamming distance than taking neighboring sequences into account ( > 0).
Figure 12:	Analysis of model accuracy during model based training on the AMP problem. Shown arethe mean absolute error (top row) and model uncertainty (bottom row; standard deviation) depending on thenumber of inner model-based optimization rounds m (x-axis; see Algorithm 1) and outer optimization roundsn (colors). Columns correspond to different thresholds for stopping model optimization (see Section 2.3).
Figure 13:	Optimization performance on the AMP problem depending on the uncertainty threshold forstopping model-based optimization and the maximum number model optimization rounds M. Withoutthreshold (Inf; red line), DyNA PPO converges to a sub-optimal solution, in particular when the maximumnumber of model-based optimization rounds M is high. A threshold of 0.5 prevents a performance decreasedue to inaccuracy of the model (see Figure 12).
Figure 14: Sensitivity of DyNA PPO depending on the choice of the minimum cross-validation score τ formodel-based optimization. Shown are the results for the transcription factor binding-, protein contact Ising-,and AMP problem. DyNA PPO reduces to PPO if τ is above the maximum cross-validation score of modelsthat are considered during model selection, e.g. if τ = 1.0. If τ is too low, also inaccurate models are selected,which reduces the overall accuracy of the ensemble model and optimization performance. A cross-validationscore between 0.4 and 0.5 is best for all problems.
Figure 15: Comparison of cross-validation splitting strategies. Shown are the mean optimization trajecto-ries over 12 TfBind8 targets and 10 Protein Ising models when splitting the input data into 5 folds with shufflingthe input data (Shuffle), without shuffling the input data (NoShuffle), and splitting the input data by optimiza-tion rounds (BatchShuffle). Splitting the input data approximately by rounds (NoShuffle) or exactly by rounds(BatchShuffle) results in a performance increase on Protein Ising problems compared with splitting the datarandomly (Shuffle). NoShuffle performs as well as BatchShuffle on TfBind8, and better than BatchShuffle andShuffle on all PdbIsing targets.
