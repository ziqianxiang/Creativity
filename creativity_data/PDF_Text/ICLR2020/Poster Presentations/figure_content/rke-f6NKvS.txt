Figure 1: A toy environment where the agent aims to walk from a starting state (the yellow entry) to a goal state(the green entry). The reward is sparse: R(s, a) = -1 unless s is at the goal (which is also the terminal state.)The colors of the entries show the learned value functions. Entries in black edges are states in demonstrations.
Figure 2: Illustration of the correction effect. Aconservatively-extrapolated value function V , asshown in the figure, has lower values further awayfrom U , and therefore the gradients of V point to-wards U . With such a value function, suppose weare at state s which is ε-close to U . The locally-correctable assumption of the dynamics assumesthe existence of acx that will drive us to state scxthat is closer to U than s. Since scx has a relativelyhigher value compared to other possible futurestates that are further away from U (e.g., s0 shownin the figure), scx will be preferred by the optimiza-tion (4.3). In other words, if an action a leads tostate s with large distance to U, the action won’tbe picked by (4.3) because it cannot beat acx .
Figure 3: The learning curves of VINS+RL (Algo-rithm 3) vs the prior state-of-the-art Nair et al.’18 onPick-And-Place and Push. Shaded areas indicates onestandard error estimated from 10 random seeds.12Figure 3 shows that VINS initialized RL algorithm outperforms prior state-of-the-art in sampleefficiency. We believe the main reason is that due to the initialization of value and model, we payless samples for warming up the value function. We note that our initial success rate in RL is slightlylower than the final result of VINS in Table 1. This is because in RL we implemented a slightly worsevariant of the policy induced by VINS: in the policy of Algorithm 2, we use option 2 to search theaction uniformly. This suffices because the additional interactions quickly allows us to learn a goodmodel and the BC constraint is no longer needed.
