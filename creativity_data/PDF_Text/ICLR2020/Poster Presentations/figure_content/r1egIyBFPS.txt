Figure 1: The HISS architecture, illustrated on a three-node binary subtree, where i is the parent ofj, k, and P is the parent of i.
Figure 2: Comparison with human-independent methods in terms of hit rate (left), expression lengthreduction (middle), tree size reduction (right), on Traverse Equivalence dataset.
Figure 3: Comparison with human-dependent methods in terms of expression length reduction (left),and tree size reduction (right), on the Halide dataset.
Figure 4: Attention weights on the input sequences for each token decoded. The x-axis shows theoutput sequence, and the y-axis shows the input sequence. The input sequence is encoded fromleaves to root and the output sequence is decoded from root to leaves. Tokens are re-arranged in thenatural order for better visualization.
Figure 5: Performance of different variants of Hiss on the Halide dataset.
Figure 6: Evaluation of similarity of the embeddings of equivalent expressions. The upper plots((a-1) and (a-2)) are evaluated on the original Hiss model. The lower plots ((b-1) and (b-2)) areevaluated on the No-Embed-Loss model. Left plots ((a-1) and (b-1)) are scatter plots of embeddingsprojected onto two-dimensional space using t-SNE. Points corresponding to equivalent expressionsare shown in the same color. Right plots ((a-2) and (b-2)) are box plots of intra- (blue) and inter-subset (orange) distances of the embeddings trained with `2 loss. The bars in the box represent 25%,50% and 75% quartile values. The line intervals denote the 1.5 interquartile range (IQR) beyond thequartile values. The dots represent the extreme values.
Figure 7: Expected reward of Hiss with the embedding similarity loss (Hiss) and without theembedding similarity loss (No-Embed-Loss)16Published as a conference paper at ICLR 2020Table 6:	Simplification traces of Hiss with and without the subtree selector on Eq. (11). Thesubexpressions selected by the subtree selector are boxed, unless the entire expression is chosen.
