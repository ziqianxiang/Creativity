Figure 1: Adjusting the output of the synthesized styl-ized images in real-time without retraining. Each col-umn shows a different stylized image for the same con-tent and style image. Note how each row still resemblesthe same content and style while different in details.
Figure 2: Architecture of the proposed model. The loss adjustment parameters αc and αs is passedto the network Λ which will predict activation normalizers γα and βα that normalize activationof main stylizing network T . The stylized image is passed to a trained image classifier where itsintermediate representation is used to calculate the style loss Ls and content loss Lc . Then the lossfrom each layer is multiplied by the corresponding input adjustment parameter. Models Λ and T aretrained jointly by minimizing this weighted sum. At generation time, values for αc and αs can beadjusted manually or sampled randomly.
Figure 3: Effect of adjusting the input parameters αs on stylization. Each row shows the stylizedoutput when a single αls increased gradually from zero to one while other αs are fixed to zero.
Figure 4: Effect of adjusting the style weight instyle transfer network from (Johnson et al., 2016).
Figure 5: Effect of randomizing α and additive Gaussian noise on stylization. Top: randomizing αresults to different stylizations while the style features appear in the same spatial position (e.g., lookat the swirl effect on the left eye). Middle: the effect of adding random noise to the content imagein moving these features with fixed α. Bottom: combination of this two randomization techniquescan generate highly versatile outputs. Notice how each image in this row differs in both style andthe spatial position of style elements.
Figure 6: Qualitative comparison between the base model from (Johnson et al., 2016) with ourproposed method. For the base model, each column has been retrained with all the weights set tozero except for the mentioned layers which has been set to 1e-3. For our model, the respectiveparameters αsl has been adjusted. Note how close the stylizations are and how the combination oflayers stays the same in both models.
Figure 8: Diversity comparison of our methodand baselines. First row shows results for a base-line that adds random noises to the style param-eters at run-time. While we get diverse styliza-tions, the results are not similar to the input styleimage. Second row contains results for a base-line that adds random noises to the style param-eters at both training time and run-time. Modelis robust to the noise and it does not generate di-verse results. Third row shows stylization resultsof StyleNet (Ulyanov et al., 2017). Our methodgenerates diverse stylizations while StyleNet re-sults mostly differ in minor details. More exam-ples can be seen at Figure 14.
Figure 7: Effect of adjusting the input parame-ters αs on style loss from different layers acrosssingle style image of Figure 3 (top) or 25 differentstyle images (bottom). In each curve, one of theinput parameters αls has been increased from zeroto one while others are fixed at to zero (left) andto one (right). Then the style loss has been cal-culated across 100 different content images. Ascan be seen, increasing αls decreases the loss ofthe corresponding layer. Note that the losses isnormalized in each layer for better visualization.
Figure 9: More results for adjusting the input parameters in real-time and after training. In eachblock the style/content pair is fixed while the parameters corresponding to conv3 and conv4 in-creases vertically and horizontally from zero to one. Notice how the details are different from onelayer to another and how the combination of layers may result to more favored stylizations. For aninteractive presentation please visit https://goo.gl/PVWQ9K.
Figure 10: More results of stochastic stylization from the same pair of content/style. Each blockrepresents randomized stylized outputs given the fix style/content image demonstrated at the top.
Figure 11: More examples for effect of adjusting the input parameters αs in real-time. Each rowshows the stylized output when a single αls increased gradually from zero to one while other αsare fixed to zero. Notice how the details of each stylization is different specially at the last columnwhere the weight is maximum. Also how deeper layers use bigger features of style image to stylizethe content.
Figure 12: More examples for effect of adjusting the style weight in style transfer networkfrom (Johnson et al., 2016). Each column demonstrates the result of a separate training. As canbe seen, the ”optimal” weight is different from one style image to another and there can be morethan one ”good” stylization depending on ones personal choice.
Figure 13: Results of combining losses from different layers at generation time by adjusting theircorresponding parameters. The first column is the style image which is fixed for each row. Thecontent image is the same for all of the outputs. The corresponding parameter for each one of thelosses is zero except for the one(s) mentioned in the title of each column. Notice how each layerenforces a different type of stylization and how the combinations vary as well. Also note how asingle combination of layers cannot be the ”optimal” stylization for any style image and one mayprefer the results from another column.
Figure 14: Diversity comparison of our method with StyleNet (Ulyanov et al., 2017). Our methodgenerates diverse stylizations while StyleNet results mostly differ in minor details.
