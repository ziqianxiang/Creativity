Figure 1: SCALOR inference procedure: (A) Proposal, (B) Accept-Reject, (C) Propagation, (D) BackgroundModule and Rendering Process. (A) The proposal module takes the input image and propagation mask andcombines them to make the proposal representation. (B) From the proposal representation, the proposal maskis generated, and then compared to the propagation mask to make an accept-reject decision. Only the acceptedproposals are considered as discovered objects. (C) The tracker RNNs decide what and where to propagate afterlooking at the input image. The gray boxes represent what is not propagated. (D) Given inferred foregroundobjects and the input image, the background module infers the background representation. The renderingprocess combines the foreground and background representations according to the foreground mask assignmentprinciple saying that two objects cannot coexist in the same position, we shall also see later fur-ther reasons as to why this design is effective. The final discovery model can then be written as:P(ZD IZP) = P(ZDIZP) QHW=Ip(ZDhwIZP, zD,τ), where the accePtance model is:accePt	accePtp(ZDhw∣ZP, zd ,τ) = f(z：chew>p, ZD ,τ )p(Zwrhw )zt,h,wp(Zwhatw A…(6)Background Transition. Unlike SQAIR, SCALOR is endowed with a background model. Thebackground image is encoded into a D-dimension continuous vector Ztbg from the background tran-sition P(Ztbg IZb<gt, Zftg). The background RNN encodes the temPoral transition of background images.
Figure 2: Quantitative result showing superior performance of SCALOR (SC) compared to SQAIR (SQ). (a)Tracking Accuracy (b) Object Count and Reconstruction Errorcomponent. Furthermore the obtained representation does not contain explicit interpretable featureslike position and scale, etc. SPACE (Lin et al., 2020) combines both of the above approaches byusing object detection for foreground and mixture decomposition for background. It improves uponSPAIR by parallelizing the latent inference process.
Figure 3: Qualitative results of SCALOR for Moving dSprites and Moving MNIST (HD) tasks: a) Inferredbounding boxes superimposed on the original image sequence. White circles indicate discovery at that timestep,b) Reconstructed sequence, c) Per-object reconstruction(a)(b)Figure 4: Qualitative samples of tracking on Moving dSprites task with dynamic background: a) Originalimage sequence with inferred bounding boxes, b) Reconstructed sequenceexample, in the MD setting, there are always between 18 to 25 objects in the overall environment,while only about 20 of them are visible on average in each frame.
Figure 4: Qualitative samples of tracking on Moving dSprites task with dynamic background: a) Originalimage sequence with inferred bounding boxes, b) Reconstructed sequenceexample, in the MD setting, there are always between 18 to 25 objects in the overall environment,while only about 20 of them are visible on average in each frame.
Figure 8: Attention-Rejection Ablation study and computational efficiency comparison. (a) Propagation rate,(b) Number of discovered objects, (c) Inference time and, (d) Training convergence timeand choose a convolutional network as the image encoder and decoder. The NLL value for our modelis 28.30, and for the VAE and VRNN baseline it is 27.59 and 27.79 respectively. While learning ahighly structured representation, SCALOR can still obtain a comparable generation quality.
Figure 9: Frequent Dense Discovery experiment: a) First row: inferred bounding boxes superimposed on theoriginal sequence. b) Second row: discovery bounding boxes. c) Third row: discovery reconstruction. d) Lastrow: propagation reconstructionThis experiment evaluates the ability to discover many newly introduced objects across time-steps.
Figure 10: Sample from Very High Density setting: a) First row: inferred bounding boxes, b) Second row:overall reconstruction, c) Third row: discovery bounding boxes, d) Fourth row: discovery reconstruction, d)Fifth row: propagation bounding boxes, e) Sixth row: propagation reconstruction16Published as a conference paper at ICLR 2020Figure 10 shows samples from the Very High Density experiment. This experiment places 90-110objects in the overall environment, around 90 of which are visible at every time-step on average.
Figure 12: Generalization with respect to longer sequences. a) First row: bounding boxes for the first 10time-steps. b) Second row: bounding boxes for the last 10 time-stepsFigure 13: Generalization Experiment: a) First row: generalization to unseen shapes. b) Second row: general-ization to a larger number of objectsWe conduct three sets of experiments on generalization. In the first experiment, we investigategeneralization to longer sequences. In this setting, the model is trained on trajectories of length 10while being tested on trajectories of length 20. In the second experiment, we evaluate generalizationin more crowded scenes. In this setting, the model is trained on 15-25 objects and tested on 50-60objects. The third experiment tests the generalization of the model to unseen objects. The modelis trained only on moving MNIST images containing digits 0 to 5 while being tested on imagescontaining digits 6 to 9. Figures 12 and 13 show samples from these experiments. “SCALOR - LG”in Table 1 also provides tracking results for the “length generalization” setting.
Figure 13: Generalization Experiment: a) First row: generalization to unseen shapes. b) Second row: general-ization to a larger number of objectsWe conduct three sets of experiments on generalization. In the first experiment, we investigategeneralization to longer sequences. In this setting, the model is trained on trajectories of length 10while being tested on trajectories of length 20. In the second experiment, we evaluate generalizationin more crowded scenes. In this setting, the model is trained on 15-25 objects and tested on 50-60objects. The third experiment tests the generalization of the model to unseen objects. The modelis trained only on moving MNIST images containing digits 0 to 5 while being tested on imagescontaining digits 6 to 9. Figures 12 and 13 show samples from these experiments. “SCALOR - LG”in Table 1 also provides tracking results for the “length generalization” setting.
Figure 15: Tracking sample 119Published as a conference paper at ICLR 2020Figure 17: Tracking sample 320Published as a conference paper at ICLR 202021Published as a conference paper at ICLR 2020E Model Architecture DetailsIn this section, we provide additional details of the architecture and hyperparameters used for pedes-trian detection. When a new frame is provided, the network uses a fully convolutional encoder toobtain a H ×W feature map. The feature map is fed into a convolutional LSTM to model the sequen-tial information along the sequence. The convolutional LSTM is shared across discovery moduleand propagation module. The discovery module and propagation module also share the zwhat en-coder and decoder. The encoder has a convolutional network followed by one fully connected layer,while the glimpse decoder uses a fully convolutional network with sub-pixel layer (Shi et al., 2016)for upsampling. The background module shares a similar structure with the zwhat encoder and de-coder. It takes a 4-dimensional input, i.e., RGB and foreground mask, and outputs a 3-dimensionalimage. We use GRUs in propagation trackers and in prior transition networks.
Figure 17: Tracking sample 320Published as a conference paper at ICLR 202021Published as a conference paper at ICLR 2020E Model Architecture DetailsIn this section, we provide additional details of the architecture and hyperparameters used for pedes-trian detection. When a new frame is provided, the network uses a fully convolutional encoder toobtain a H ×W feature map. The feature map is fed into a convolutional LSTM to model the sequen-tial information along the sequence. The convolutional LSTM is shared across discovery moduleand propagation module. The discovery module and propagation module also share the zwhat en-coder and decoder. The encoder has a convolutional network followed by one fully connected layer,while the glimpse decoder uses a fully convolutional network with sub-pixel layer (Shi et al., 2016)for upsampling. The background module shares a similar structure with the zwhat encoder and de-coder. It takes a 4-dimensional input, i.e., RGB and foreground mask, and outputs a 3-dimensionalimage. We use GRUs in propagation trackers and in prior transition networks.
