Figure 1: Example of a pointcloud generated using our proposed semantically-guided architec-ture, colored by RGB values from the input image and corresponding predicted semantic labels.
Figure 2: Diagram of our proposed architecture for self-supervised monocular depth estimationwith semantically-guided feature learning. The semantic network is fixed and initialized from pre-trained weights, while the depth network is trained end-to-end in a self-supervised way, includingpixel-adaptive convolutions (Guidance) on its decoder to learn semantic-dependent geometric fea-tures.
Figure 3: Qualitative results of our proposed two-stage training to address the infinite depthproblem. Top images were obtained evaluating the first-stage depth network, and bottom imageswere obtained using the second-stage depth network, trained with a filtered dataset.
Figure 4: Class-specific depth evaluation for our proposed architecture (blue), relative to our base-line (red). The rightmost column indicates class-average depth metrics, obtained by averaging allindividual classes. The introduction of semantically-guided features, in conjunction with our pro-posed two-stage training methodology to address the infinite depth problem, consistently improveddepth results for all considered classes (lower is better).
Figure 5: Qualitative results of our proposed architecture. The left, middle, and right columnsshow respectively input images, baseline predicted depth maps (Guizilini et al., 2019), and the depthsmaps obtained using our proposed architecture. Our semantic-aware depth network predicts sharperboundaries and fine-grained details on distant objects. The dotted lines indicate class-average errors,obtained by averaging all the class-specific depth errors.
Figure 6: Examples of erroneous semantic predictions that still led to accurate depth predictionsusing our proposed semantically-guided depth framework.
