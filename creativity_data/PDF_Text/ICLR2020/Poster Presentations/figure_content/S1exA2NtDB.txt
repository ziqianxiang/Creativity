Figure 1: (a) ES-MAML and PG-MAML exploration behavior. (b) Different exploration methodswhen K is limited (K = 5 plotted with lighter colors) or large penalties are added on wrong goals.
Figure 2: ES-MAML exploration on six circletask (K = 20).
Figure 3: The Forward-Backward and Goal-Velocity MAML problems. We compare the perfor-mance for Linear (L) policies and policies with one hidden layer (H) for different K .
Figure 4: Stability comparisons of ES and PG on the Biased-Sensor CartPole and Swimmer,Walker2d environments. (L), (H), and (HH) denote linear, one- and two-hidden layer policies.
Figure 5: Low K comparisons between ES-MAML and PG-MAML.
Figure A1: Comparisons between the FO-Hessian and FO-NoHessian variants of Algorithm 5.
Figure A2: Comparisons between FO-NoHeSSian and Algorithm 3, by rolloutsFprwarqIBackvyardAQtForw^rdBac∣<ward^alfChleetah lP-IeMBM ⅛><-400-0.20.40.6RolloutsFO-H,K=20■ F0-H,K=5ZO-H,K=20, ZO-H, K=5-600 ∏0.00.8	1.01∈7—FO-HfK=40400 ^	FO-H,K=20ZO-H,K=40
Figure A3: RBO-MAML and ASEBO-MAML compared to ES-MAML.
Figure A4: Comparing the exploration behavior of PG-MAML and ES-MAML on the Navigation-2D task. We use K = 20 queries for each algorithm.
Figure A5: Comparisons between ES-MAML and PG-MAML using the queries K from (Finn et al.,2017).
Figure A6: The MSE of the adapted policy, for varying number of gradient steps and query numberK. Runs are averaged across 3 seeds.
