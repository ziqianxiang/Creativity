Figure 1: Illustration of the generalization gap. Large-batch SGD (A2, blue) matches the training curves ofsmall-batch SGD (A1, green), i.e. has no optimization difficulty (left & middle). However, it does not reach thesame test accuracy (right) while the proposed post-local SGD (A5, red) does. Post-local SGD (A5) is defined bystarting local SGD from the model obtained by large-batch SGD (A2) at epoch 150. Mini-batch SGD with largermini-batch size (A3, yellow) even suffers from optimization issues. Experiments are for ResNet-20 on CIFAR-10(Bloc = 128), with fined-tuned learning rate for mini-batch SGD with the warmup scheme in Goyal et al. (2017).
Figure 2: Training ReSNet-20 on CIFAR-10 under different K and H, with fixed Bloc = 128. All results areaveraged over three runs and all settings access to the same total number of training samples. We fine-tune thelearning rate of mini-batch SGD for each setting.
Figure 3: Top-1 test accuracy of training ResNet-20 on CIFAR-10. Box-plot figures are derived from 3 runs.
Figure 4: Understanding the generalization ability of post-local SGD for large-batch training (ResNet-20 onCIFAR-10 with BK = BlocK = 2048). We use fixed B = Bg = 128 with K = 16 GPUs. The detailedexperimental setup as well as more visualization of results are available in Appendix C.4.
Figure 5: The data transmission cost (in seconds) of an all-reduce operation for 100 MB, over the differentnumber of cores, using PyTorch’s built-in MPI all-reduce operation. Each evaluation is the average result of100 data transmissions on a Kubernetes cluster. The network bandwidth is 10 Gbps, and we use 48 cores perphysical machine.
Figure 6: Numerical illustration of local SGD on a convex problem.
Figure 7: Training CIFAR-10 With ReSNet-20 via local SGD (2 义 I-GPU). The local batch size Bloc is fixedto 128, and the number of local steps H is varied from 1 to 16. All the experiments are under the same trainingconfigurations.
Figure 8: The performance of local SGD trained on ImageNet-1k with ReSNet-50.We evaluate the modelperformance on test dataset after each complete accessing of the whole training samples. We apply the large-batch learning schemes (Goyal et al., 2017) to the ImageNet for these two methods. For local SGD, the numberof local steps is set to H = 8.
Figure 9: The relationship between steps to top-1 test accuracy and batch size, of training ResNet-20 onCIFAR-10. The “step” is equivalent to the number of applying gradients. The global mini-batch size is increasedby adding more workers K With fixed Bloc = 128. Results are averaged over three runs, each With fine-tunedlearning rate.
Figure 10:	Investigate how local step warm-up strategy impacts the performance of training CIFAR-10 withReSNet-20 via local SGD (8 义 2-GPU). The local batch size Bloc is fixed to 128. The warmup strategies are“linear” and “constant”, and the warm-up period used here is equivalent to the number of local steps H.
Figure 11:	Investigate how different warm-up period of the local step warm-up impacts the performance oftraining CIFAR-10 with ReSNet-20 via local SGD (8 义 2-GPU). The local batch size BloC is fixed to 128, andthe strategies to warm-up the number of local steps H are “linear”, “exponential” and “constant”.
Figure 12: The effectiveness and necessary of turning on the post-local SGD after the first learning rate decay.
Figure 13: Sharpness visualization of the minima for ResNet-20 trained on CIFAR-10. The training is on topof K = 16 GPUs and the local batch size is fixed to Bioc = 128. The dashed lines are standard mini-batch SGDand the solid lines are post-local SGD with H = 16. The sharpness visualization of minima is performed viafilter normalization (Li et al., 2018). The model is perturbed as W λ λd by a shared random direction d, andis evaluated by the whole dataset (training or test respectively). The top-1 test accuracy of mini-batch SGD is92.25, while that of post-local SGD is 92.61. The sharpness of these two minima is consistent over 10 differentrandom directions.
Figure 14: The spectrum of the Hessian for ReSNet-20 trained on CIFAR-10. The training is on top of K = 16GPUs with KBloc = 2048. The spectrum is computed using power iteration (Martens & Sutskever, 2012; Yaoet al., 2018) with the relative error of 1e-4. The top-1 test accuracy of mini-batch SGD is 92.57, while that ofpost-local SGD ranges from 92.33 to 93.07. Current large-batch SGD tends to stop at points with considerably“larger” Hessian spectrum, while large-batch trained with post-local SGD generalizes to solution with lowcurvature and with better generalization.
Figure 15: 1-d linear interpolation between models WPoSt-iocmsgd and Wmini-batchSGD, i.e., W = λwmini-batch sgd +(1 — λ)WPoSt-localSGD, for different minima of ReSNet-20 trained on CIFAR-10. The training is on top ofK = 16 GPUs and the local batch size is fixed to BloC = 128. The solid lines correspond to evaluate W onthe Whole training dataset While the dashed lines are on the test dataset. The Post-local SGD in Figure 15(a)and Figure 15(d) is trained from the checkPoint of Wmini-batch SGD before Performing the first learning rate decay,While that of Figure 15(c) and Figure 15(d) is trained from scratch. The toP-1 test accuracy of mini-batch SGDis 92.25, While that of Post-local SGD in Figure 15(a), Figure 15(b), Figure 15(c) and Figure 15(d), are 92.61,92.35, 93.13 and 93.04 resPectively.
Figure 16: The performance of post-local SGD training for ImageNet-1k. We evaluate the model performanceon test dataset after each complete accessing of the whole training samples. Note that due to the resourcelimitation of the main experimental platform in the paper, these experiments are on top of a 8 × 4-GPU (V100)Kubernetes cluster with 10 Gbps network bandwidth.
Figure 17: Illustration of a hierarchical network architecture of a cluster in the data center. While GPUs withineach node are linked with fast connections (e.g. NVLink), connections between the servers within and betweendifferent racks have much lower bandwidth and latency (via top-of-the-rack switches and cluster switches). Thehierarchy can be extended several layers further and further. Finally, edge switches face the external network ateven lower bandwidth.
Figure 18: An illustration of hierarchical local SGD, for Bioc = 2, using H = 3 inner local steps and Hb = 2outer ‘block’ steps. Local parameter updates are depicted in red, whereas block and global synchronization isdepicted in purple and black respectively.
Figure 19: The performance of hierarchical local SGD trained on CIFAR-10 with ReSNet-20 (2 义 2-GPU).
