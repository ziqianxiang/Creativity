Figure 1: We find existing de-fenses (orange line) ineffectiveagainst recent attacks. Our de-fense (red line) in contrast signif-icantly mitigates the attacks.
Figure 2: We perturb posteriorpredictions y = y + δ, with anobjective of poisoning the adver-sary's gradient signal.
Figure 3: Attackers vs. Our Defense. Curves are obtained by varying degree of perturbation (Eq. 7) in ourdefense. ↑ denotes higher numbers are better and，lower numbers are better. Non-replicability objective ispresented on the x-axis and utility on the y -axis.
Figure 4: Knockoff attack vs. Ours + Baseline Defenses (best seen magnified). Non-replicability is presentedon the x-axis. On y-axis, We present two utility measures: (a) top: Utility = Li distance (b) bottom: Utility =Defender,s accuracy. Region above the diagonal indicates instances where defender outperforms the attacker.
Figure 7: Test loss. Visual-ized during training. Coloursand lines correspond to val-ues in Fig. 6.
Figure 5: Attacker argmax.
Figure 6: Histogram of Angular Devia-tions. Presented for MAD attack on CI-FAR10 with various choices of .
Figure 8: MAD Ablation experiments. Utility = (left)L1 distance (right) defender test accuracy.
Figure 9: Subverting the Defense.
Figure A1: Overview of Attack, Defense, and Evaluation Metrics. We consider an attacker A Who exploitsblack-box access to defended model FV to train a stolen model FA. In this paper, We take the role of thedefender who intends to minimize replicability (i.e., ACC(FA, DteSt)), while maintaining utility of the predic-tions. We consider two notions of utility: (1) minimizing perturbations in predictions, measured here using Lidistance; and (2) maintaining accuracy of the defended model on test set ACC(FV, DteSt). Note that for a fairhead-to-head comparison, we use the same held-out test set DteSt to evaluate accuracies of both the defendedmodel FV and stolen model FA. Similar to all prior work, we assume Dtrain, DteSt are drawn i.i.d from the same(victim) distribution DV. Notation used in the above figure is further elaborated in Table A1.
Figure A2: Influenceof attacker architec-ture choices on a fixedsurrogate.
Figure A3: Influence of Initialization of a VGG16 Surrogate Model.
Figure A4: Evaluation of all attacks on undefended victim models.
Figure A5: Stolen model trained using knockoff strategy on complete posterior information (y) and only thetop-1 label of the posteriors (arg maxk yk).
Figure A6: Budget vs. Test Accuracy. Supplements Fig. 3c in the main paper.
Figure A7: Attacker argmax. Supplements Fig. 4 in the main paper.
Figure A8: Histogram of Angular Deviations (Black-box setting). Supplements Fig. 6 in the main paper.
Figure A9: MAD ablation experiments. Supplements Fig. 8 in the main paper.
