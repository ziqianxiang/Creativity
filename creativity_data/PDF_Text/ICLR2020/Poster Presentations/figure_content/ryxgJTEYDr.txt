Figure 1: Illustration of our model (Left): An intrinsic competition mechanism, based on the amountof information each primitive requests, is used to select a primitive to be active for a given input.
Figure 2: Snapshots of motions learned by the policy. Top: Reference motion clip. Middle:Simulated character imitating the reference motion. Bottom: Probability of selecting each primitive.
Figure 3: Convergence of four primitives on Four Room Maze: Left: We trained four primitiveson the Four Room Maze task, where the goal was sampled from one of the two fixed goals. We seethat the proposed algorithm is able to learn four primitives. Right: We transfer the learned primitivesto the scenario where the goal is sampled from one of the four possible goals. The checkpointedmodel is ran on 100 different episodes (after a fixed number of steps/updates) and the normalizedfrequency of activation of the different primitives is plotted.
Figure 4: Embeddings visualizing the states (S) andgoals (G) which each primitive is active in, and theactions (A) proposed by the primitives for the motionimitation tasks. A total of four primitives are trained.
Figure 5: Multitask training. Each panel corresponds to a different training setup, where differenttasks are denoted A, B, C, ..., and a rectangle with n circles corresponds to an agent composed ofn primitives trained on the respective tasks. Top row: activation of primitives for agents trained onsingle tasks. Bottom row: Retrain: Two primitives are trained on task A and transferred to taskB. The results (success rates) indicate that the multi-primitive model is substantially more sampleefficient than the baseline (transfer A2C). Copy and Combine: More primitives are added to themodel over time in a plug-and-play fashion (two primitives are trained on task A; the model isextended with a copy of the two primitives; the resulting four-primitive model is trained on taskB.) This is more sample efficient than other strong baselines, such as (Frans et al., 2017; Baconet al., 2017). Zero-Shot Generalization: A set of primitives is trained on task C, and zero-shotgeneralization to task A and B is evaluated. The primitives learn a form of spatial decompositionwhich allows them to be active in both target tasks, A and B. The checkpointed model is ran on 100different episodes, and the normalized frequency of activation of the different primitives is plotted.
Figure 6: Continual Learning Scenario: The plot on the left shows that the primitives remainactivated. The solid green line shows the boundary between the tasks. The plot on the right showsthe number of samples required by our model and the transfer baseline model across different tasks.
Figure 7: Left: Multitask setup, where we show that we are able to train eight primitives whentraining on a mixture of four tasks in the Minigrid environment. Here, the x-axis denotes the numberof frames (timesteps). Right: Success rates of the different methods on the Ant Maze tasks. Successrate is measured as the number of times the ant is able to reach the goal (based on 500 sampledtrajectories).
Figure 8: Performance on the 2D bandits task. Left: The comparison of our model (blue curve -decentralized policy) with the baseline (red curve - flat policy) in terms of success rate shows theeffectiveness of our proposed approach. Right: Relative frequency of activation of the primitives(normalized to sum up to 1). Both primitives are utilized throughout the training.
Figure 11: RGB view of the UnlockPickup environment.
Figure 12: View of the four-room environmentthe state (or the cell) the agent is in right now. ie the environment returns a vectors of zeros with onlyone entry being 1 and the index of this entry gives the current position of the agent. The environmentdoes not return any information about the goal state.
Figure 13: View of the Ant Maze environment with 3 goalsG.1 Model Architecture for Ant Maze EnvironmentG.1.1 Training SetupWe describe the design of the learning agent in terms of an encoder-decoder architecture.
