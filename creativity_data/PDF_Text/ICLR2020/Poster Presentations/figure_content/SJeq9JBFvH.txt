Figure 1: (a) System-level overview of the proposed framework, in which a probabilistic generativesampling model (DPS) and a subsequent task model are jointly trained to fulfill a desired systemtask. (b,c) Two illustrative task-based sampling paradigms: image classification from a partial set ofpixels (b), and image reconstruction from partial Fourier measurements (c), respectively.
Figure 2: MNIST classification for (a) image-domain, and (b) FoUrier-domain subsampling. (top)Several example images in the respective sampling domains, (2nd and 3rd row) learned task-adaptive(DPS-topK and DPS-top1, respectively) subsampling patterns, with their relative sample incidenceacross a 1000 such realizations (inset), and (bottom) hold-out classification results of the pro-posed DPS methods, compared to the LOUPE baseline, and two non-learned baseline samplingapproaches.
Figure 3: A realization of the learneddistribution (inset) for reconstruction.
Figure 4: Image reconstruction performance from partial k-space (Fourier) measurements on a CUs-tom toy dataset consisting of lines and circles with random locations and sizes. Illustrative examplesof the k-space and target images are given in (a). The sampling patterns, reconstructed images andPSNR value (across the entire test set) for the different sampling strategies are displayed in: (b)uniform, (c) random, (d) low pass, (e) LOUPE, (f) DPS-top1, and (g) DPS-topK, respectively. In allcases, only 3.1% of the Fourier coefficients have been selected.
Figure 5: Image reconstruction performance from partial k-space (Fourier) measurements on theCIFAR10 database. Illustrative examples of the k-space and target images are given in (a). Thesampling patterns, images and statistical quality metrics for uniform, random, low pass, LOUPE,DPS-top1, and DPS-topK are given in (b-g), respectively. In all cases, 12.5% of the Fourier coeffi-cients have been selected.
Figure 6: Training graphs for the â€˜lines and circles' reconstruction problem from section 4.2. (a)The graph shows that even though neural network loss functions are typically non-convex, the op-timization trajectory of the loss value still followed a smooth path. The train and validation lossexactly behave the same way, suggesting no overfitting. (b) The corresponding PSNR value duringtraining.
