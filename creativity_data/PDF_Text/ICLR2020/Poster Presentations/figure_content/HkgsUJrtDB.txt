Figure 1: Trade-off between the accuracy of classifier and fairness on the adult dataset under theequality of opportunity notion. (a, b) By increasing λ from 0 to 1000, EO violation (the blue curveon the left axis) approaches to 0. The fairer solution comes at the price of a slight increase of thetraining/test error (Red curve, right axis). (C) Comparison of the existing approaches With Renyiclassifier, under the equality of opportunity notion. Renyi classifier demonstrates a better accuracyfor a given level of fairness measured by EO violation.
Figure 2: Trade-off between accuracy and fairness for logistic regression classifier regularized withRenyi, HSIC, and Pearson measures, on German Credit, Adult, and Bank datasets. (Top) The dropin the accuracy of the model regularized by Renyi, is less than the same model regularized byHSIC, and Pearson correlation. Moreover, as can be observed for both Bank and Adult datasets,Pearson and HSIC regularizers usually cannot increase p% beyond a certain limit, due to the factthat removing all linear correlations does not guarantee independence between the predictor andthe sensitive attribute. (Down) When the sensitive attribute is not binary (or we have more thanone sensitive attribute), obtaining a fair model for HSIC and Pearson regularizers is even harder.
Figure 3: Performance and fairness of K-means algorithm in terms of Renyi regularizer hyper-parameter λ. By increasing λ, the standard deviation of the w vector components (each componentrepresents the relative proportion of the privileged group in the corresponding cluster) is reducedaccordingly. Both plots demonstrate that the standard deviation of w is reduced fast with respect toλ, and the increase in loss is small when λ ≤ 0.005. However, to reach a completely fair clustering,a λ ≥ 1 must be chosen that can increase the loss (the right axis, red curve) drastically.
Figure 4: Applying K-means algorithm without fairness on the synthetic dataset.
Figure 5: Applying fair K-means algorithm with different values of λ on the synthetic dataset.
Figure 6: The relationship between Renyi correlation, Pearson correlation, and normalized mutualinformation. Direct optimization of normalized mutual information is intractable due to its non-convexity. However, as We can observe on the right-hand-side, by minimizing Renyi correlation to0, the normalized mutual information is converging to 0 accordingly.
