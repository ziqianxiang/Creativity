Figure 1: SVP applied to active learning (left) and core-set selection (right). In active learning,we followed the same iterative procedure of training and selecting points to label as traditionalapproaches but replaced the target model with a cheaper-to-compute proxy model. For core-setselection, we learned a feature representation over the data using a proxy model and used it to selectpoints to train a larger, more accurate model. In both cases, we found the proxy and target modelhave high rank-order correlation, leading to similar selections and downstream results.
Figure 2: SVP performance on core-set selection. Average (± 1 std.) top-1 error of ResNet164 over5 runs of core-set selection with different selection methods, proxies, and subset sizes on CIFAR10.
Figure 3: Comparing selection across model sizes and methods on CIFAR10. Average Spear-man’s correlation between different runs of ResNet (R) models and at varying depths. We computedrankings based on forgetting events (left), entropy (middle), and greedy k-centers (right). We saw asimilarly high correlation across model architectures (off-diagonal) as between different runs of thesame architecture (on-diagonal), suggesting that small models are good proxies for data selection.
Figure 4:	Top-1 test error on CIFAR10 for varying model sizes (left) and over the course of training asingle model (right), demonstrating a large amount of time is spent on small changes in accuracy.
Figure 5:	Top-1 test error on CIFAR100 for varying model sizes (left) and over the course of traininga single model (right), demonstrating a large amount of time is spent on small changes in accuracy.
Figure 6:	Quality of proxies compared to target models. Average (± 1 std.) top-1 error from 3runs of active learning with varying proxies, selection methods, and budgets on five classificationdatasets. Dotted lines show the top-1 error of the proxy models, while solid lines show the top-1 errorof the target models. CIFAR10 and CIFAR100 experiments used varying depths of pre-activationResNet (R) models as proxies and ResNet164 (R164) as the target model (e.g., R20-R164 is ResNet20selecting for ResNet164). ImageNet used ResNet18 (R18) as the proxy and ResNet50 (R50) as thetarget. Amazon Review Polarity and Amazon Review Full used VDCNN9 (V9) and fastText (FT) asproxies and VDCNN29 (V29) as the target. Across datasets, proxies, methods, and budgets, smallerproxies had higher top-1 error than the target model, but selecting points that were nearly as good asthe points selected by the target that did not harm the final target model’s predictive performance.
Figure 7: SVP performance on core-set selection. Average (± 1 std.) top-1 error of ResNet164 over5 runs of core-set selection with different selection methods, proxies, and subset sizes on CIFAR100.
Figure 8: Comparing selection across model architectures on ImageNet. Spearman’s correlationbetween max entropy rankings from PyTorch (Paszke et al., 2017) pretrained models on ImageNet.
Figure 9: Comparing selection across model sizes and methods on CIFAR100. Average Spear-man’s correlation between different runs of ResNet (R) models and a varying depths. We computedrankings based on forgetting events (left), entropy (middle), and greedy k-centers (right). We sawa similarly high correlation across model architectures (off-diagonal) as between runs of the samearchitecture (on-diagonal), suggesting that small models are good proxies for data selection.
Figure 10: Spearman’s rank-order correlation between different runs of ResNet (R) with pre-activationand a varying number of layers on CIFAR10 (left) and CIFAR100 (right). For each combination,we compute the average from 20 pairs of runs. For each run, we compute rankings based on theorder examples are added in facility location using the same initial subset of 1,000 randomly selectedexamples. The results are consistent with Figure 3c and Figure 9c, demonstrating that most of thevariation is due to stochasticity in training rather than the initial subset.
Figure 11: Average (± 1 std.) Spearman’s rank-order correlation with ResNet164 during 5 trainingruns of varying ResNet architectures on CIFAR10 (left) and CIFAR100 (right), where rankings werebased on forgetting events.
Figure 12: Average (± 1 std.) Spearman’s rank-order correlation with ResNet164 during 5 trainingruns of varying ResNet architectures on CIFAR10 (left) and CIFAR100 (right), where rankings werebased on entropy.
Figure 13: Average (± 1 std.) Spearman’s rank-order correlation between epochs during 5 trainingruns of varying ResNet architectures on CIFAR10 (left) and CIFAR100 (right), where rankings werebased on forgetting events.
Figure 14: Average (± 1 std.) Spearman’s rank-order correlation between epochs during 5 trainingruns of varying ResNet architectures on CIFAR10 (left) and CIFAR100 (right), where rankings werebased on entropy.
Figure 15: Pearson correlation coefficient between different runs of ResNet (R) with pre-activationand a varying number of layers on CIFAR10 (top) and CIFAR100 (bottom). For each combination,we compute the average from 20 pairs of runs. For each run, we compute rankings based on thenumber of forgetting events (left), and entropy of the final model (right). Generally, we see a similarlyhigh correlation across model architectures (off-diagonal) as between runs of the same architecture(on-diagonal), providing further evidence that small models are good proxies for data selection.
Figure 16: 2D t-SNE plots from the final hidden layer of a fully trained ResNet164 model onCIFAR10 and a 30% subset selected (black). The top row uses another run of ResNet164 to selectthe subset and the bottom row uses ResNet20. Rankings are computed using forgetting events (left),entropy (middle), and greedy k-centers (right).
Figure 17: 2D t-SNE plots from the final hidden layer of a fully trained ResNet164 model onCIFAR10 and a 30% subset selected (black) with ResNet20 trained after a varying number of epochs.
