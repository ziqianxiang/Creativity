Figure 1: Illustration of the I-ProjeCtion vs. the M-ProjeCtion for modelling behavior. (a): A robotreaches a target point while avoiding an obstacle. There are two different types of solutions, aboveand below the obstaCle. A single Gaussian is fitted to the exPert data in joint sPaCe. (b): The M-projection fails to reach the target and collides with the obstacle. (c): The I-projection ignores thesecond mode and reaches the target while avoiding the obstacle.
Figure 2: An illustrative example of the benefits of EIM versus an adversarial formulation. (a): Thetrue log density ratio of the model and the target distribution (both are Gaussians). The location ofthe optimum is unbounded. (b): In the adversarial formulation, the generator minimizes the expectedlog density ratio. If we neglect that the adversarial discriminator changes with every update step ofthe generator, the generator updates yield an unbounded solution. Hence, too aggressive updates ofthe generator yield unstable behavior. (c): The upper bound of EIM introduces an additional KL-term as objective. Optimizing this objective directly yields the optimal solution without the need torecompute the density ratio estimate.
Figure 3: Average I-projection achieved for EIM, the f -GAN, and the modified EIM versions. Thetask is to fit a model to samples from a randomly generated GMM of different dimensions. Boththe model and the target GMM have the same number of components. EIM clearly outperforms thegenerative adversarial approaches, especially for larger dimensions. The ablation study shows thatthe separated, closed-form updates clearly yield better results. Neglecting the KL has a big influencefor lower dimensions, but is out-weighted by the error of the discriminator at higher dimensions.
Figure 4: Average distance to line and samples for robot line reaching. While EIM for small numbersof components ignores modes, not considering the whole line, it learns models that achieve theunderlying task, i.e., reach the line. Providing additional information to the density ratio estimatorfurther decreases the average distance to the line. EM, on the other hand, averages over the modes,and thus, fails to reach the line even for large numbers of components.
Figure 5: Results on the traffic prediction tasks. Naturally, EM achieves the highest training log-likelihood. Yet, for large numbers of components, a severe amount of overfitting is observed. EIM,on the other hand, has no problems working with high numbers of components and achieves ahigher test log-likelihood, despite optimizing a different objective. We also provided a ’road mask’as additional features for the discriminator. We used this road mask to evaluate how realistic thegenerated samples are. While EIM without features produced more realistic samples on the Lanker-shim dataset, we needed the feature input to outperform EM on this evaluation on the SDD dataset.
Figure 6: Results on the obstacle avoidance task. (a): Even for a small number of components, EIMhas a rather high probability of success, i.e., placing a trajectory that does not hit any obstacle. Evenwith a sufficient number of components, i.e., eight, EM fails to achieve good results. (b) and (c):Samples of a mixture with 4 components, learned by EIM and EM respectively. EM clearly averagesover multiple modes in the data distribution.
Figure 7: Samples from the Dataset, EIM, EIM with features and EM, plotted over the referenceimage from the Stanford Drone Dataset and the generated mask. In the mask green corresponds tovalid regions and red to invalid regions. EIM with the additional feature input generates samplesthat stay within the ’road mask’ and are therefore considered to be more realistic.
