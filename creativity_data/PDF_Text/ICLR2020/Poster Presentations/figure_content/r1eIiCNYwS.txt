Figure 1: The eXtra Hop attention in Transformer-XH (a) and its application to multi-hop QA (b).
Figure 2: Distributions of learned attention weights of three hops on three groups: From All (Node)→ (to) All, All → (to) Ans (ground truth answer node), and Supp (nodes with the supporting facts)→ (to) Ans. X-axes are attention values scaled by number of nodes.
