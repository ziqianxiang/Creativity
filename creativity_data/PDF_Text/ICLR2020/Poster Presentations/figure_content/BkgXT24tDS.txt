Figure 1: Density of weights in ResNet-18high computational overhead. Powers-of-Two quantization levels (Miyashita et al., 2016; Zhouet al., 2017) are then proposed because of its cheap multiplication implemented by shift operationson hardware, and super high resolution around the mean. However, the vanilla powers-of-two quan-tization method only increases the resolution near the mean and ignores other regions at all whenthe bit-width is increased. Consequently, it assigns inordinate quantization levels for a tiny rangearound the mean. To this end, we propose additive Powers-of-Two (APoT) quantization to resolvethese two contradictions, our contribution can be listed as follows:1.	We introduce the APoT quantization scheme for the weights and activations of DNNs.
Figure 2: Quantization of unsigned data to 3-bit or 4-bit (Î± = 1.0) using three different quantization levels.
Figure 3: Hardware accelerator with different quantization schemes. When k increase, weights usually has lessPoT terms, thus accelerates the computation.
Figure 4: The evolution of clipping ratio of the first three layers in ResNet-20. (a) demonstrates clipping ratiois too sensitive to threshold to hurt its optimization without weights normalization. (b) shows that weightsdistribution after normalization is relatively more stable during training.
Figure 5: When weights are normalized the distribution of weights are more stable. The dashed line shows themean value of weights.
Figure 6: A summary of projection error and clipping error in different layers.
