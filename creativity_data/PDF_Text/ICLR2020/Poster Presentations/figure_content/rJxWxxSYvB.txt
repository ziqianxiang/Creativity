Figure 1: Illustration of weight symmetry in a neural network with feedforward and feedback connections.
Figure 2: A. Layers of the convolutional network trained on CIFAR-10 and the corresponding network of LIFneurons that undergoes RDD feedback training. Fully-connected feedforward weights (blue) and feedbackweights (red) are shared between the two networks. Every training epoch consists of an RDD feedback trainingphase where feedback weights in the LIF net are updated (bold red arrows) and transferred to the convolutionalnet, and a regular training phase where feedforward weights in the convolutional net are updated (bold bluearrows) and transferred back to the LIF net. B. RDD feedback training protocol. Every 30 s, a different layerin the LIF network receives driving input and updates its feedback weights (red) using the RDD algorithm.
Figure 3: A. Evolution of sign alignment (the percent of feedforward and feedback weights that have the samesign) for each fully-connected layer in the network when trained on CIFAR-10 using RDD feedback training(blue), using the algorithm proposed by Akrout et al. (2019) (purple), and using feedback alignment (red). B.
Figure 4: Evolution of Rself for each fully-connected layer in the network when trained on CIFAR-10 usingRDD feedback training (solid lines) and using feedback alignment (dashed lines). RDD feedback trainingdramatically decreases this loss compared to feedback alignment, especially in earlier layers.
Figure 5: Comparison of Fashion-MNIST, SVHN, CIFAR-10 and VOC train error (top row) and test error(bottom row). RDD feedback training substantially improves test error performance over feedback alignmentin both learning tasks.
Figure S1: Comparison of average spike rates in the fully-connected layers of the LIF network vs. activities ofthe same layers in the convolutional network, when both sets of layers were fed the same input. Spike rates inthe LIF network are largely correlated with activities of units in the convolutional network.
