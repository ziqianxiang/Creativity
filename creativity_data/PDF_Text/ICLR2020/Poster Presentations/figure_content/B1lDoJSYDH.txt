Figure 1: We use spherical filter shapes for our continuous convolutions but use regular grids tostore the filter values. The left part of the figure shows a spherical region with radius R and a pointwith relative coordinates r with respect to the center. We transform r via a mapping Λ to normalizedcoordinates in a regular grid. The thin dotted lines illustrate the distortion of the mapping. To lookup the final filter value we use trilinear interpolation in the regular grid.
Figure 2: Schematic of our network with a depth of four. In the first depth level we compute convo-lutions at each dynamic particle location with the set of static particles that defines the environmentas well as the dynamic particle set. We also directly process the features of each particle via afully-connected stream. In the following levels, we compute convolutions only on the dynamicparticles. At each level we use addition to aggregate the features computed by convolutions andfully-connected layers. Between the second and third level we also include a residual connection.
Figure 3: Comparison to ground-truth physics simulation. Two fluid bodies collide. Top: simulationby our trained network. Bottom: simulation of the same scenario by DFSPH (Bender & Koschier,2015), a high-fidelity solver that was executed with small timesteps (down to 0.001s). Despite usinga much larger timestep (0.02s), our convolutional network produces results of comparable visualfidelity. Note that our particles are falling slightly more slowly due to differences in the integrationof positions and the much larger timestep. See the supplementary video.
Figure 4: Qualitative comparison with DPI-Nets on a test sequence from our dataset. Two fluidbodies collide inside a container. DPI-Nets works well on data with little variance but has problemswith more complex scenes and high particle velocities. The DPI-Nets simulation becomes unstableimmediately after the fluid hits the box. The fluid behavior predicted by our network matches theground truth more closely and remains stable for the whole sequence. The two networks have beentrained on the same data. Test sequences are distinct from training sequences. See the supplementaryvideo.
Figure 5: Generalization to environments with drastically different geometry than seen during train-ing. Top: we use an emitter to fill up a virtual river with fluid particles, demonstrating generalizationwith respect to scene geometry and the number of particles. Bottom: a waterfall scene showing thefluid particles and the particle representation of the environment. See the supplementary video.
Figure 6: Average distance from the ground-truth particles to the predicted particles on a complexscene. Top: error over time for our trained network and DFSPH. We use a timestep of ∆t = 5 ms forDFSPH and ∆t = 20 ms for our method, which also corresponds to the frame sampling rate. Theground truth was generated with DFSPH and a timestep of 1ms. Bottom: simulation produced byour network. Large errors are concentrated in the beginning of the sequence when the fluid initiallycollides with the environment and the fluid behavior is most chaotic. During this phase the error ishigher for our method than DFSPH. After 200 frames the error levels become similar.
Figure 7: We can control the viscosity of the simulated fluid at test time by changing the inputparameter ν in the input feature vector.
Figure 8: We sample from 10 different box-like containers during data generation. For the simplifiedversion of our dataset used in the quantitative comparison with the baselines we only use the firstcontainer (leftmost container in the first row).
Figure 9: We randomly place fluid bodies of different initial shapes in the scene during data gener-ation. We sample from 5 different shapes and vary the size, the orientation and the initial particlevelocity. All particles from the same fluid body start with the same initial velocity. The image showsthe particles generated from each shape for a specific size and orientation.
