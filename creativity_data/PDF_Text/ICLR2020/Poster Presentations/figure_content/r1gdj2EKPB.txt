Figure 1: Description of crucial challenges for continual learning with Omniglot dataset experiment. Catas-trophic forgetting: Model should not forget what it has learned about previous tasks. Scalability: Theincrease in network capacity with respect to the number of tasks should be minimized. Order sensitivity: Themodel should have similar final performance regardless of the task order. Our model with Additive ParameterDecomposition effectively solves these three problems.
Figure 2: An illustration of Additive Parameter Decomposition (APD) for continual learning. APD effectivelyprevents catastrophic forgetting and suppresses order-sensitivity by decomposing the model parameters intoshared σ and sparse task-adaptive τt , which will let later tasks to only update shared knowledge. Mt is thetask-adaptive mask on σ to access only the relevant knowledge. Sparsity on τt and hierarchical knowledgeconsolidation which hierarchically rearranges the shared parameters greatly enhances scalability.
Figure 3:	Accuracy over efficiency of expansion-based continual learning methods and our methods. We reportperformance over capacity and performance over training time on both datasets.
Figure 4:	Performance disparity of continual learning baselines and our models on CIFAR-100 Split. Plotsshow per-task accuracy for 3 task sequences of different order. Performance disparity of all methods for 5 tasksequences of different order are given in Figure A.8 in the Appendix.
Figure 5: (a)-(c) Catastrophic Forgetting on CIFAR-100 Superclass: Performance of our models on the 1st,6th,and 11th task during Continual learning. (d)-(e) Task Forgetting on CIFAR-100 Split: Per-task PerformanCeof APD(1) (T1:5) when 1st task is dropped during Continual learning.
Figure 6:	Left: Performance comparison with several benchmarks on Omniglot-rotation (standard deviationinto parenthesis). Right: The number of the parameters which is obtained during course of training onOmniglot-rotation.
Figure 7:	Visualizations of the model paramters during continual learning. The colored markersdenote the parameters for each task i, and the empty markers with black outlines denote the task-shared parameters. Dashed arrows indicate the drift in the parameter space as the model trains on asequence of tasks.
