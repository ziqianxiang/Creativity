Figure 1: Monotonic Attention (Left) versus Monotonic Multihead Attention (Right).
Figure 2: Latency-quality tradeoffs for MILk (Arivazhagan et al., 2019) and MMA on IWSLT15 En-Vi andWMT15 De-En. Black dashed line indicates the unidirectional offline transformer model with greedy search.
Figure 3:	Effect of Î»var on the average attention span. The variance loss works as intended by reducing thespan with higher weights.
Figure 4:	Effect of the number of decoder attention heads and the number of decoder attention layers on qualityand latency, reported on the WMT13 validation set.
Figure 5: Running examples on IWSLT15 English-Vietnamese dataset4.5 Rank of the headsIn Figure 6, we calculate the average and standard deviation of rank of each head when generatingevery target token. For MMA-IL, we find that heads in lower layers tend to have higher rank andare thus slower. However, in MMA-H, the difference of the average rank are smaller. Furthermore,the standard deviation is very large which means that the order of the heads in MMA-H changesfrequently over the inference process.
Figure 6: The average rank of attention heads during inference on IWSLT15 En-Vi. Error bars indicate thestandard deviation. L indicates the layer number and H indicates the head number.
Figure 7: Effect average loss, weighted average loss and variance loss on MMA-H on WMT15 DeEn develop-ment set.
