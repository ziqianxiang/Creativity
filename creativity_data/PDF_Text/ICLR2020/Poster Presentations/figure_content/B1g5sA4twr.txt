Figure 1: Left: Train and test error as a function of model size, for ResNet18s of varying widthon CIFAR-10 with 15% label noise. Right: Test error, shown for varying train epochs. All modelstrained using Adam for 4K epochs. The largest model (width 64) corresponds to standard ResNet18.
Figure 2: Left: Test error as a function of model size and train epochs. The horizontal line corre-SPonds to model-wise double descent-varying model size while training for as long as possible. Thevertical line corresponds to epoch-wise double descent, with test error undergoing double-descentas train time increases. Right Train error of the corresponding models. All models are Resnet18strained on CIFAR-10 with 15% label noise, data-augmentation, and Adam for up to 4K epochs.
Figure 3: Test loss (per-token perplexity) as afunction of Transformer model size (embed-ding dimension dmodel) on language trans-lation (IWSLT‘14 German-to-English). Thecurve for 18k samples is generally lower thanthe one for 4k samples, but also shifted tothe right, since fitting 18k samples requiresa larger model. Thus, for some models, theperformance for 18k samples is worse thanfor 4k samples.
Figure 4: Model-wise double descent for ResNet18s. Trained on CIFAR-100 and CIFAR-10, withvarying label noise. Optimized using Adam with LR 0.0001 for 4K epochs, and data-augmentation.
Figure 5: Effect of Data Augmentation. 5-layer CNNs on CIFAR10, with and without data-augmentation. Data-augmentation shifts the interpolation threshold to the right, shifting the testerror peak accordingly. Optimized using SGD for 500K steps. See Figure 27 for larger models.
Figure 7: Noiseless settings. 5-layerCNNs on CIFAR-100 with no label noise;note the peak in test error. Trained withSGD and no data augmentation. See Fig-ure 20 for the early-stopping behavior ofthese models.
Figure 6: SGD vs. Adam. 5-Layer CNNson CIFAR-10 with no label noise, and nodata augmentation. Optimized using SGDfor 500K gradient steps, and Adam for 4Kepochs.
Figure 8: Transformers on language trans-lation tasks: Multi-head-attention encoder-decoder Transformer model trained for80k gradient steps with labeled smoothedcross-entropy loss on IWSLT‘14 German-to-English (160K sentences) and WMT‘14English-to-French (subsampled to 200K sen-tences) dataset. Test loss is measured as per-token perplexity.
Figure 9: Left: Training dynamics for models in three regimes. Models are ResNet18s on CIFAR10with 20% label noise, trained using Adam with learning rate 0.0001, and data augmentation. Right:Test error over (Model size × Epochs). Three slices of this plot are shown on the left.
Figure 10: Epoch-wise double descent for ResNet18 and CNN (width=128). ResNets trained usingAdam with learning rate 0.0001, and CNNs trained with SGD with inverse-squareroot learning rate.
Figure 11: Sample-wise non-monotonicity.
Figure 12: Left: Test Error as a function of model size and number of train samples, for 5-layerCNNs on CIFAR-10 + 20% noise. Note the ridge of high test error again lies along the interpolationthreshold. Right: Three slices of the left plot, showing the effect of more data for models ofdifferent sizes. Note that, when training to completion, more data helps for small and large models,but does not help for near-critically-parameterized models (green).
Figure 13: Scaling of model size with our parameterization of width & embedding dimension.
Figure 14: Random Fourier Features on the Fashion MNIST dataset. The setting is equivalentto two-layer neural network with e-ix activation, with randomly-initialized first layer that is fixedthroughout training. The second layer is trained using gradient flow.
Figure 15: Sample-wise double-descent slice for Random Fourier Features on the Fashion MNISTdataset. In this figure the embedding dimension (number of random features) is 1000.
Figure 16:	Epoch-wise double descent for ResNet18 trained with Adam and multiple learning rateschedulesA practical recommendation resulting from epoch-wise double descent is that stopping the trainingwhen the test error starts to increase may not always be the best strategy. In some cases, the test errormay decrease again after reaching a maximum, and the final value may be lower than the minimumearlier in training.
Figure 17:	Epoch-wise double descent for ResNet18 trained with SGD and multiple learning rateschedules18Published as a conference paper at ICLR 2020IO2 IO3 IO4 IO5 IO6Iterations(a) Constant learning rateIO2 IO3 IO4 IO5 IO6Iterations(b) Inverse-square root learningrateIO2 IO3 IO4 IO5 IO6Iterations(c) Dynamic learning rateFigure 18:	Epoch-wise double descent for ResNet18 trained with SGD+Momentum and multiplelearning rate schedulesE.2 Model-Wise Double Descent: Additional ResultsE.2. 1 Clean Settings With Model-wise Double DescentCIFAR100, ResNet18Figure 19: Top: Train and test performance as a function of both model size and train epochs.
Figure 18:	Epoch-wise double descent for ResNet18 trained with SGD+Momentum and multiplelearning rate schedulesE.2 Model-Wise Double Descent: Additional ResultsE.2. 1 Clean Settings With Model-wise Double DescentCIFAR100, ResNet18Figure 19: Top: Train and test performance as a function of both model size and train epochs.
Figure 19: Top: Train and test performance as a function of both model size and train epochs.
Figure 20: Top: Train and test performance as a function of both model size and train epochs.
Figure 21: Left: Test error dynamics with weight decay of 5e-4 (bottom left) and without weightdecay (top left). Right: Test and train error and test loss for models with varying amounts ofweight decay. All models are 5-Layer CNNs on CIFAR-10 with 10% label noise, trained withdata-augmentation and SGD for 500K steps.
Figure 22: Generalized double descent for weight decay. We found that using the same initiallearning rate for all weight decay values led to training instabilities. This resulted in some noise inthe Test Error (Weight Decay × Epochs) plot shown above.
Figure 23: Model-wise test error dynamics for a subsampled IWSLT‘14 dataset. Left: 4k samples,Right: 18k samples. Note that with optimal early-stopping, more samples is always better.
Figure 24: Model-wise test error dynamics for a IWSLT‘14 de-en and subsampled WMT‘14 en-frdatasets. Left: IWSLT‘14, Right: subsampled (200k samples) WMT‘14. Note that with optimalearly-stopping, the test error is much lower for this task.
Figure 25: Top: Train and test performance as a function of both modelBottom: Test error dynamics of the same model (CNN, on CIFAR-10 withaugmentation and SGD optimizer with learning rate Z 1∕√T).
Figure 26: Model-wise double descent for adversarial training ResNet18s on CIFAR-10 (sub-sampled to 25k train samples) with no label noise. We train for L2 robustness of radius = 0.5 and= 1.0, using 10-step PGD (Goodfellow et al. (2014); Madry et al. (2017)). Trained using SGD(batch size 128) with learning rate 0.1 for 400 epochs, then 0.01 for 400 epochs.
Figure 28: Effect of Ensembling (ResNets, 15% label noise). Test error of an ensemble of 5models, compared to the base models. The ensembled classifier is determined by plurality vote overthe 5 base models. Note that emsembling helps most around the critical regime. All models areResNet18s trained on CIFAR-10 with 15% label noise, using Adam for 4K epochs (same settingas Figure 1). Test error is measured against the original (not noisy) test set, and each model in theensemble is trained using a train set with independently-sampled 15% label noise.
Figure 29: Effect of Ensembling (CNNs, no label noise). Test error of an ensemble of 5 models,compared to the base models. All models are 5-layer CNNs trained on CIFAR-10 with no labelnoise, using SGD and no data augmentation. (same setting as Figure 7).
