Figure 1: The graphical models for Markov decision processes (MDPs) (a) and partially observableMarkov processes (POMDPs) (b). Grey nodes are observed, white nodes are hidden. In POMDPs,the state s is not observable and must be inferred from past observations. The Ot is a binary randomvariable, where Ot = 1 denotes that the action is optimal at time t, and Ot = 0 denotes that theaction is not optimal.
Figure 2: The structure of sequential variational soft Q-learning networks (SVQNs). Black solidlines represent forward paths of the neural network, gray dashed lines represent reconstruction pathsand blue arrows stand for sampling latent variables using the re-parameterization trick. Double-arrows indicate that the algorithm needs to minimize the KL-divergence between two probability dis-tributions. The model takes the observation ot, previous action at-1 and reward rt-1 as inputs, andit uses a neural network fθ(ot, at-1, rt-1) to extract the low-dim hidden feature wt. The recurrentunit rnnθ(wt, ht-1) is used to capture the historical information ht. qθ(st ∣ht) and pθ(st ∣st-1, at-1)are proposed distributions of hidden states. The hidden state St and inner hidden state St are sampledfrom qθ(st∣ht) and Pprior(St∣st-1, at-1) respectively. The Q-function Qθ(st, at) is learned via thetemporal difference (TD) algorithm with soft update.
Figure 3: Screen shots of Atari games and ViZDoom tasks. From left to right, they are Pong, Chop-perCommand, DoubleDunk, Asteroids, Health Gather,Health Gather v2 and Defend Center. Whenonly given one frame as the input, these Atari games are POMDPs because we can not obtain the ve-locity of the moving object from a single observation. Because the agent in ViZDoom environmentcan only see in just one direction, it naturally introduces partial observability to these tasks.
Figure 4: (a) Training curves on Health Gather. Both SVQN models outperform other baselines.
