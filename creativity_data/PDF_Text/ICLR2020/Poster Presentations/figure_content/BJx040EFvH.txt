Figure 1: Cyclic learning rates used for FGSM adversarial training on CIFAR10 and ImageNet overepochs. The ImageNet cyclic schedule is decayed further by a factor of 10 in the second and thirdphases.
Figure 2: Performance of models trained on CIFAR10 at = 8/255 with cyclic learning rates andhalf precision, given varying numbers of epochs across different adversarial training methods. Eachpoint denotes the average model performance over 3 independent runs, where the x axis denotes thenumber of epochs N the model was trained for, and the y axis denotes the resulting accuracy. Theorange dots measure accuracy on natural images and the blue dots plot the empirical robust accuracyon adversarial images. The vertical dotted line indicates the minimum number of epochs needed totrain a model to 45% robust accuracy.
Figure 3: Robust test performance of FGSM adversarial training over different step sizes for8/255.
Figure 4: Learning curves for FGSM adversarial training plotting the training loss and error ratesincurred by an FGSM and PGD adversary when trained with zero-initialization FGSM at = 8/255,depicting the catastrophic overfitting where PGD performance suddenly degrades while the modeloverfits to the FGSM performance.
Figure 5: Histogram of the resulting perturbations from a PGD adversary for each feature for asuccessfully trained robust CIFAR10 model and a catastrophically overfitted CIFAR10 model.
Figure 6: Robust test performance of FGSM adversarial training over different step sizes for8/255 with early stopping to avoid catastrophic overfitting.
