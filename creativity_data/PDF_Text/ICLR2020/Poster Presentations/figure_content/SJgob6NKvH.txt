Figure 1: RTFM requires jointly reasoning over the goal, a document describing environment dy-namics, and environment observations. This figure shows key snapshots from a trained policy onone randomly sampled environment. Frame 1 shows the initial world. In 4, the agent approaches“fanatical sword”, which beats the target “fire goblin”. In 5, the agent acquires the sword. In 10, theagent evades the distractor “poison bat” while chasing the target. In 11, the agent engages the targetand defeats it, thereby winning the episode. Sprites are used for visualisation — the agent observescell content in text (shown in white). More examples are in appendix A.
Figure 2: The FiLM2 layer.
Figure 3: txt2π models interactions between the goal, document, and observations.
Figure 4: Ablation training curves on simplestvariant of RTFM. Individual runs are in lightcolours. Average win rates are in bold, dark lines.
Figure 5: txt2π attention on the full RTFM. These include the document attention conditioned onthe goal (top) as well as those conditioned on summaries produced by intermediate FiLM2 layers.
Figure 6: The initial world is shown in 1. In 4, the agent avoids the target “lightning shaman”because it does not yet have “arcane spear”, which beats the target. In 7 and 8, the agent is corneredby monsters. In 9, the agent is forced to engage in combat and loses.
Figure 7: The initial world is shown in 1. In 5 the agent evades the target “cold ghost” because itdoes not yet have “soldier’s knife”, which beats the target. In 11 and 13, the agent obtains “soldier’sknife” while evading monsters. In 14, the agent defeats the target and wins.
Figure 8: The convolutional network baseline. The FiLM baseline has the same structure, but withconvolutional layers replaced by FiLM layers.
Figure 10: Performance on the Rock-paper-scissors task across models. Left shows final perfor-mance on environments whose goals and dynamics were seen during training. Right shows perfor-mance on the environments whose goals and dynamics were not seen during training.
Figure 9: The Rock-paper-scissors task requires jointlyreasoning over the game observations and a documentdescribing environment dynamics. The agent observescell content in the form of text (shown in white).
Figure 11: Learning curve while transferring to the development environments. Win rates of indi-vidual runs are shown in light colours. Average win rates are shown in bold, dark lines.
Figure 12: Ablation training curves. Win rates ofindividual runs are shown in light colours. Aver-age win rates are shown in bold, dark lines.
Figure 13: Curriculum learning results for txt2π on RTFM. Win rates of individual runs are shownin light colours. Average win rates are shown in bold, dark lines.
