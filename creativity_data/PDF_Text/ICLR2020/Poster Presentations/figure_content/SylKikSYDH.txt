Figure 1: The Compressive Transformer keeps a fine-grained memory of past activations, which arethen compressed into coarser compressed memories. The above model has three layers, a sequencelength ns = 3, memory size nm = 6, compressed memory size ncm = 6. The highlighted memoriesare compacted, with a compression function fc per layer, to a single compressed memory â€” insteadof being discarded at the next sequence. In this example, the rate of compression c = 3.
Figure 2: Attention weight on Enwik8. Av-erage attention weight from the sequence overthe compressed memory (oldest), memory, andsequence (newest) respectively. The sequenceself-attention is causally masked, so more at-tention is placed on earlier elements in the se-quence. There is an increase in attention at thetransition from memory to compressed memory.
Figure 3: Learning rate analysis. Reducing thelearning rate (e.g. to zero) during training (onEnwik8) harms training performance. Reduc-ing the frequency of optimisation updates (ef-fectively increasing the batch size) is preferable.
Figure 4: Speech Modelling. We see the Com-pressive Transformer is able to obtain competi-tive results against the state-of-the-art WaveNetin the modelling of raw speech sampled at24kHz.
Figure 5: Vision and RL. We see the Com-pressive Transformer integrates visual informa-tion across time within an IMPALA RL agent,trained on an object matching task.
Figure 6: Model analysis. Compression loss broken down by layer.
