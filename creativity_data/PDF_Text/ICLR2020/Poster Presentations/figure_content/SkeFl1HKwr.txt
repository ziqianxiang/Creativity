Figure 1: Linear regions and classification regions of models trained with different optimizationtechniques. The gray curves in the top row are transition boundaries separating different linearregions, and the color represents the activation rate of the corresponding linear region. In the bottomrow, different colors represent different classification regions, separated by the decision boundaries.
Figure 2: Depth-wise exclusion process to search for the linear region of x*. Each node in eachlayer provides a hyperplane to cut the polytope defined by the preceding layers.
Figure 3: Inradius distributions for different learning rates and optimization techniques. The modelsin the top row were trained with a larger learning rate (1e-3), and the bottom row a smaller learningrate (1e-4). The three columns show the inradius distributions of the manifold regions, the decisionregions, and the adversarial regions, respectively.
Figure 4: Angles between the directions of different hyperplanes of a particular manifold region.
Figure 5: Different Xt of the point X*, which is classified into Class 10 (number 9). From leftto right, t varies from 1 to 10 (number 0 to number 9). Images with green borders are correctlyclassified into Class 10, and red borders misclassified into Class t. Different rows represent themodels trained with different optimization techniques or learning rates. The models whose namesend with “-L” were trained with a larger learning rate (1e-3), and “-S” a smaller learning rate (1e-4).
Figure 6: (a): boxplot of the number of unique surrounding regions for 1000 test points. Differentcolors represent different learning rates (LRs). (b) and (c): distributions of the cosine similarity ρ ofall unique surrounding regions. Models in (b) were trained with a larger learning rate (1e-3), and (c)a smaller learning rate (1e-4).
Figure 7: Visualization of the toy dataset. Different colors represent different classes.
Figure 8: Linear regions and classification regions of models trained with different optimizationtechniques. The gray curves in the top row are transition boundaries separating different linearregions, and the color represents the activation rate of the corresponding linear region. In the bottomrow, different colors represent different classification regions, separated by the decision boundary.
Figure 9:	Inradius distributions of different optimization techniques for CNN models.(a) Manifoldregion; (b) Decision region; (c) Adversarial region.
Figure 10:	Angles between directions of different hyperplanes of a single manifold region. The in-dices on both axes indicate different hyperplanes: 0-7,199 represent the hyperplanes of the first con-volutional layer, 7,200-10,335 the second convolutional layer, 10,336-11,487 the third convolutionalTlayer, and 11,488-12,511 the fully-connected hidden layer. The value of a pixel is arccos 口 W.] Wl,i.e., the angle between wi and wj . (a) Vanilla; (b) BN; (c) Dropout.
Figure 12: (a) Boxplot of the number of unique surrounding regions for 1000 test points; (b) Distri-butions of the cosine similarity ρ of all unique surrounding regions for the 1000 test points.
