Figure 1: The framework of our proposed FNA. Firstly, we select an artificially designed networkas the seed network Ns and expand Ns to a super network Nsup which is the representation of thesearch space. Then parameters of Ns are remapped to Nsup . We utilize the NAS method to startthe architecture adaptation with the super network and obtain the target architecture Archt . Beforeparameter adaptation, we remap the parameters of Ns to Archt . Finally, we adapt the parameters ofArcht to get the target network Nt.
Figure 2: Our parameters are remapped on three levels. (a) shows the depth-level remapping. Theparameters of existing corresponding layers are mapped from the original network. The parametersof new layers are mapped from the last layer in the original network. (b) shows the width-levelremapping. For each channels-diminished dimension, the parameters are copied from the originalexisting ones. (c) shows the kernel-level remapping. The original parameters are mapped to thecentral part of the new larger kernel. The values of the other parameters are assigned with 0.
Figure 3: Parameter Remapping on thekernel-level with a dilation setting.
Figure 4: Visualization of our searched architectures on different frameworks. MB: inverted residualblock proposed in MobilenetV2 (Sandler et al., 2018). Kx_Ey: the kernel size of the depthwiseconvolution is x and the expansion ratio is y.
Figure 5: Visualization of semantic segmentation results on the Cityscapes validation dataset.
Figure 6: Visualization of object detection results on the MS-COCO validation dataset.
