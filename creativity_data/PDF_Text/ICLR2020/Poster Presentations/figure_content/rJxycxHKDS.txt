Figure 1: A Domain Adaptive Multibranch Network is a sequence of computational units f(i), each of whichprocesses the data in parallel branches, whose outputs are then aggregated in a weighted manner by a gate toobtain a single response. To allow for domain-adaptive computations, each domain has its own set of gates, onefor each computational unit, which combine the branches in different ways. As a result, some computations areshared across domains while others are domain-specific.
Figure 2: A computational unit f (i) is an aggregationof the outputs of parallel computations, or branches,fj(i).
Figure 3: Computational graphs for the source (top)and target (bottom) domains, for the same network.
Figure 4: Evolution of the gatesâ€™ activations for each of the computational units in a multibranch ResNet-50 network, for the Office DSLR + Webcam . Amazon domain adaptation problem. In the top two rows, weshow the gates for the source domains and in the bottom row for the target one. All branches are initialized toparameters obtained from a single ResNet-50 trained on ImageNet. Note how for the first computational unit,conv1, each domain chooses to process the data with different branches. In the remaining units, the two sourcedomains, which have similar appearance, share all the computations. By contrast, the target domain still usesits own branches in conv3_x, and conv4_x to account for its significantly different appearance. When arrivingat conv_5x, the data has been converted to a domain-agnostic representation, and hence the same branch canoperate on all domains.
Figure 1: Multibranch LeNet. This architecture is a multibranch extension to the LeNet used by DANN (Ganin& Lempitsky, 2015).
Figure 2: Multibranch SVHNet. This architecture is a multibranch extension to the SVHNet used byDANN (Ganin & Lempitsky, 2015).
Figure 3:	Multibranch architecture for drone detection. This architecture is a multibranch extension to theone used by Rozantsev et al. (2019).
Figure 4:	Multibranch ResNet-50. This architecture is adapted from the original ResNet-50 (He et al., 2016).
Figure 5:	Gate evolution for a multibranch SVHN network with branches of different capacities. Branch 1 is theoriginal branch that applies 5x5 convolutions to the image, whereas branch 2 is a similar architecture but with1x1 convolutions instead. The network quickly recognizes that SVHN requires a more complex processing andhence assigns the respective branch to it for computational units 1 and 3.
Figure 6: Gate evolution for a multibranch LeNet network with branches of different capacities. We havesimplified the architecture to encapsulate the feature extraction into a single computational unit in this case.
Figure 7: Effect of adding extra branches to a LeNet multibranch network. We augment the original multibranchLeNet with a third branch under the same branch architecture as the original one. The network rapidly decidesto ignore this overparametrization. The additional branch does not have an effect in the final activation of thegates, nor does it help during training.
Figure 8: Augmenting a multibranch ResNet-50 has a similar effect as the above. One of the branches isdiscarded early on by each computational unit.
