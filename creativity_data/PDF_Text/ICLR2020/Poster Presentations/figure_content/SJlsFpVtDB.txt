Figure 1: Posterior predictive distribution in the xy-plane (grey) of a Bayesian neural network with 2 layers of16 units, tanh activations, prior p0 (w) = N(w; 0, 1), and Bernoulli likelihood. In case of variational Bayes(Figs. 1b, 1c), the KL divergence of the ELBO is annealed from β = 0 to β = 1 over many iterations (450kannealing, 50k ELBO). Fig. 1d shows that the approximation trades off the expected log-likelihood for a betterKL divergence as β is increased. With 70 data points, the annealed KL jumps to a significantly lower value,resulting in an almost linear decision boundary. By contrast, MCMC yields a much better predictive distributionfor the same number of samples. Data is visualised in red and blue.
Figure 2: Time-evolution of distribution parameters of Bayesian Forgetting (top) and the Ornstein-Uhlenbeckprocess (bottom) for different adaptation parameter values. The initial distribution (at t = 0) can be seen as theapproximate posterior at some time-step tk .
Figure 3: Two-moons dataset. Predictive distribution (Figs. 3a - 3c) of a BNN (gray) and running memory(rectangular shapes, size is proportional to the score), chosen by the memory update proposed in Sec. 3.2. Datafrom tk and t<k-1 is visualised as large circles and small dots, respectively. Fig. 3d shows the one-step-ahead(predictive) LML (divided by the number of samples) for data that will be selected for the memory and data thatwill be evicted. Data that will be selected in the memory tends to have a significantly lower predictive likelihood.
Figure 4: Average test LML, evaluated for several memory sizes (top), and evaluated over time (bottom) for aspecific memory size (cf. corresponding legend). Cf. Sec. 6.1 for details and App. A for further results.
Figure 5: Mean and std deviation of the approximate posterior distributions of a logistic regression model over720 time-steps. The model is trained on a toy classification problem with rotating class boundaries (cf. Sec. 6.2).
Figure 6: One-step ahead LML on Covertype dataset. Subplots show 3 different adaptation methods (3 leftplots), evaluated for several values of the respective adaptation parameter, and Bayesian forgetting with = 0.11,evaluated for multiple memory sizes (right).
Figure 7:	Average test LML on further datasets not included in the main text. Evaluated for several memorysizes (top), and evaluated over time (bottom) for a specific memory size. Cf. Sec. 6.1, App. A.1 for details.
Figure 8:	Average test LML on UCI Energy (top) and UCI Concrete (bottom). GRS denotes our approach(Sec. 3), GRS with k-center replaces our memory update by the k-center method; GRS with refit replaces ourGaussian update by the optimization of Eq. (2) with Dtk ∪ Mtk-1 \Mtk . Evaluated for several memory sizes(left), and evaluated over time (right) for 3 different memory sizes. Hyperparameters are chosen as in Sec. 6.1.
Figure 9:	Running memory at different time-steps on MNIST (cf. Sec. 6.1), with a memory size N = 100. Thememory update tends to select non-typical data while showing diversity.
Figure 10: One-step ahead LML on Gas Sensor Array Drift dataset (top) and Weather dataset (bottom). Subplotsshow 3 different adaptation methods (left), evaluated for several values of the respective adaptation parameter,and Bayesian forgetting with = 0.0095 (Gas Sensory Array Drift) and = 0.031 (Weather), evaluated formultiple memory sizes (right).
Figure 11: LML for toy classification problem with rotating class boundaries (cf.Sec. A)B Experiment SetupThe following is an explanatory list of the update methods used in Sec. 6.1:•	k-center (VCL): Uses the k-center method (Sec.2.2) for the memory update and Eq. (2) with(Dtk ∪ Mtk-1) \ Mtk for the Gaussian update.
