Figure 1: Learned latent space trajectories in generative adversarial networks correspond to visualtransformations like camera shift and zoom. Take the “steering wheel”, drive in the latent space, andexplore the natural image manifold via generative transformations!-Zoom +-Rotate2D +This paper seeks to quantify the degree to which we can achieve basic visual transformations bynavigating in GAN latent space. In other words, are GANs “steerable” in latent space?1 We analyzethe relationship between the data distribution on which the model is trained and the success inachieving these transformations. From our experiments, it is possible to shift the distribution ofgenerated images to some degree, but we cannot extrapolate entirely out of the dataset’s support.
Figure 2: We aim to find a path in z space to transform the generated image G(z) to its editedversion edit(G(z, α)), e.g., an α× zoom. This walk results in the generated image G(z + αw)when we choose a linear walk, or G(f (f (...(z))) when we choose a non-linear walk.
Figure 3: Transformation limits. As We increase the magnitude of w*, the operation either doesnot transform the image any further, or the image becomes unrealisitic. Below each figure we alsoindicate the average LPIPS perceptual distance betWeen 200 sampled image pairs of that category.
Figure 4: Each row shows how a single latent direction w* affects two different ImageNet classes.
Figure 5: Quantifying the extent of transformations. We compare the attributes of generated im-ages under the raw model output G(z), compared to the distribution under a learned transformationmodel(α). We measure the intersection between G(z) and model(α), and also compute the FIDon the transformed image to limit our transformations to the natural image manifold.
Figure 6: Understanding per-class biases. We observe a correlation between the variability in thetraining data for ImageNet classes, and our ability to shift the distribution under latent space trans-formations. Classes with low variability (e.g., robin) limit our ability to achieve desired transfor-mations, in comparison to classes with a broad dataset distribution (e.g., laptop). To the right, weshow the distribution of the zoom attribute in the dataset (black) and under +α (red) and -α (green)transformations for these two examples.
Figure 7: Comparison of linear and nonlinear walks for the zoom operation. The linear walk under-shoots the targeted level of transformation, but maintains more realistic output.
Figure 8: Distribution for luminance transformation learned from the StyleGAN cars generator, andqualitative examples of color transformations on various datasets using StyleGAN.
Figure 9: Reducing the effect of transformation limits. Using aDCGAN model on MNIST digits, Wecompare the L2 reconstruction errors on latent space walks for models trained with vanilla GANSwithout (argmin W) and with data augmentation (argmin W + aug). We also compare to jointlyoptimizing the generator and the walk parameters with data augmentation (argmin G,W), whichachieves the lowest L2 error.
Figure 10: Comparing model versus dataset distribution. We plot statistics of the generated underthe color (luminance), zoom (object bounding box size), and shift operations (bounding box center),and compare them to the statistics of images in the training dataset.
Figure 11: LPIPS Perceptual distances between images generated from pairs of consecutive ai anda^+ι. We sample 1000 images from randomly selected categories using BigGAN, transform themaccording to the learned linear trajectory for each transformation. We plot the mean perceptualdistance and one standard deviation across the 1000 samples (shaded area), as well as 20 individualsamples (scatterplot). Because the Rotate 3D operation undershoots the targeted transformation, Weobserve more visible effects when we increase the α magnitude.
Figure 12: Bounding boxes for random selected classes using ImageNet training images.
Figure 13: Bounding boxes for random selected classes using model-generated images for zoomand horizontal and vertical shift transformations under random values of α.
Figure 14: Linear walks in BigGAN, trained to minimize LPIPS loss. For comparison, We show thesame samples as in Fig. 1 (which used a linear walk with L2 loss).
Figure 15: Nonlinear walks in BigGAN, trained to minimize L2 loss for color and LPIPS loss forthe remaining transformations. For comparison, we show the same samples in Fig. 1 (which used alinear walk with L2 loss), replacing the linear walk vector W with a nonlinear walk.
Figure 16: Quantitative experiments for nonlinear walks in BigGAN. We show the attributes ofgenerated images under the raw model output G(z), compared to the distribution under a learnedtransformation model(α), the intersection area between G(z) and model(α), FID score on trans-formed images, and scatterplots relating dataset variability to the extent of model transformation.
Figure 17: Qualitative examples for randomly selected categories in BigGAN, using the linear tra-jectory and L2 objective.
Figure 18: Qualitative examples for randomly selected categories in BigGAN, using the linear tra-jectory and LPIPS objective.
Figure 19: Qualitative examples for randomly selected categories in BigGAN, using a nonlineartrajectory.
Figure 20: Qualitative examples for learned transformations using the StyleGAN car generator.
Figure 21: Quantitative experiments for learned transformations using the StyleGAN car generator.
Figure 22: Qualitative examples for learned transformations using the StyleGAN cat generator.
Figure 23: Quantitative experiments for learned transformations using the StyleGAN cat generator.
Figure 24: Qualitative examples for learned transformations using the StyleGAN FFHQ face gener-ator.
Figure 25: Quantitative experiments for learned transformations using the StyleGAN FFHQ facegenerator. For the zoom operation not all faces are detectable; we plot the distribution as zeros forα values in which no face is detected. We use the dlib face detector (King, 2009) for bounding boxcoordinates.
Figure 26: Qualitative examples for learned transformations using the Progressive GAN CelebaA-HQ face generator.
Figure 27: Quantitative experiments for learnedαCelebA-HQ face generator.
Figure 28: Comparison of optimizing for color transformations in the Stylegan w and z latent spaces.
Figure 29: Qualitative examples of optimizing for a color walk with a segmented target using Style-GAN in left column and a contrast walk for both BigGAN and StyleGAN in the right column.
Figure 30: Qualitative examples of a linear walk combining the zoom, shift X, and shift Y transfor-mations. First row shows the target image, second row shows the result of learning a walk for thethree transformations jointly, and the third row shows results for combining the separately trainedwalks. Green vertical line denotes image center.
Figure 31:	Quantitative experiments on steerability with an MNIST DCGAN for the Zoom transfor-mation. Odd rows are the target images and even rows are the learned transformations.
Figure 32:	Quantitative experiments on steerability with an MNIST DCGAN for the Shift X trans-formation. Odd rows are the target images and even rows are the learned transformations.
Figure 33: Quantitative experiments on steerability with an MNIST DCGAN for the Rotate 2Dtransformation. Odd rows are the target images and even rows are the learned transformations.
