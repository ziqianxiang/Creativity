Figure 1: Illustration of the proposed BasisGAN. The diversity generated images are achieved bythe parameter generation in the stochastic sub-model, where basis generators take samples froma prior distribution and generate low dimensional basis elements from the learned spaces. Thesampled basis elements are linearly combined using the deterministic bases coefficients and usedto reconstruct the convolutional filters. Filters in each stochastic layer are modeled with a separatebasis generator. By convolving the same feature from the deterministic sub-model using differentconvolutional filters, images with diverse appearances are generated.
Figure 2: Stochastic auto-encoder: one-to-many conditional image generation without paired sam-ple. The network is trained directly to reconstruct the input real-world images, and the inherentstochasticity of the proposed method successfully promotes diverse output appearances with strongfidelity and correspondence to the inputs.
Figure 3: BasisGAN adapted from Pix2Pix. The network is trained without any auxiliary lossfunctions or regularizations. From top to bottom, the image to image translation tasks are: edges→ handbags, edges → shoes, maps → satellite, nights → days, facades → buildings. Additionalexamples are provided in the supplementary material, Figure A.2.
Figure 4: High resolution conditional image generation. Additional examples are provided in thesupplementary material, Figure A.4.
