Figure 1: AssembleNet with multiple intermediate streams. Example learned architecture. Darkercolors of connections indicate stronger connections. At each convolutional block, multiple 2D and(2+1)D residual modules are repeated alternatingly. Our network has 4 block levels (+ the stem levelconnected to raw data). Each convolutional block has its own output channel size (i.e., the number offilters) C and the temporal resolution r controlling the 1D temporal convolutional layers in it.
Figure 2: An example showing a sequence of architecture evolution. These architectures have actualparent-child relationships. The fitness of the third model was worse than the second model (due torandom mutations), but it was high enough to survive in the population pool and eventually evolveinto a better model.
Figure 3: More AssembleNet examples. Similarly good performing diverse architectures, all withhigher-than-50% mean-average precision on Charades. For instance, even our simpler two-stemAssembleNet-50 (left) got 51.4% mAP on Charades. Darker edges mean higher weights.
Figure 4: Comparison of different searchmethods.
Figure 5: Visualization of the super-graph corresponding to our video architecture search space.
Figure 6: An illustration of the node split mutation operator, used for both evolution and initialarchitecture population generation.
Figure 7: Illustration of hand-designed baseline (2+1)D CNN models used in our ablation study.
