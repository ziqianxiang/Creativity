Figure 1: The training (left) and fine-tuning (right) stages of our GNN.
Figure 2: An illustration of our proposed Wasserstein super-class clustering algorithm.
Figure 3: Visualization: t-SNE plots of the computed embeddings of test graphs on 20-shot scenariofrom OurMethod-GAT (left), GIN (middle) and WL Kernel (right) on TRIANGLES dataset. Theembeddings for both our model and GIN are taken from the final layers of the respective models.
Figure 4: Visualization: t-SNE plots of the computed embeddings of test graphs on 20-shot scenariofrom OurMethod-GAT (left), GIN (middle) and WL Kernel (right) on ENZYMES dataset.
Figure 5:	Visualization: t-SNE plots of the computed embeddings of test graphs on 20-shot scenariofrom OurMethod-GAT (left), GIN (middle) and WL Kernel (right) on Reddit dataset.
Figure 6:	Visualization: t-SNE plots of the computed embeddings of test graphs on 20-shot scenariofrom OurMethod-GAT (left), GIN (middle) and WL Kernel (right) on Letter-High dataset.
Figure 7:	Visualization: t-SNE plots of the computed embeddings of test graphs on 20-shot sce-nario from OurMethod-GAT with only 1 super-class (left), GIN (middle) and WL Kernel (right) onTRIANGLES dataset.
Figure 8:	Visualization: t-SNE plots of the computed embeddings of test graphs on 20-shot scenariofrom OurMethod-GAT with only 1 super-class (left), GIN (middle) and WL Kernel (right) on Redditdataset.
Figure 9:	Visualization: t-SNE plots of the computed embeddings of test graphs on 20-shot scenariofrom OurMethod-GAT with only 1 super-class (left), GIN (middle) and WL Kernel (right) on Letter-High dataset.
