Figure 1: A base model generates an output Yt given parameters θ and input Xt , while a hypermodelgenerates base model parameters gν(z) given hypermodel parameters ν and an index z.
Figure 2: Computation required for ensemble hypermodels versus diagonal linear hypermodels toperform well on Gaussian bandits with independent arms.
Figure 3: Compare (i) ensemble hypermodels, (ii) linear hypermodels, (iii) annealing -greedy, and(iv) an agent assuming independent actions on a neural network bandit.
Figure 4: Cumulative regret of IDS and TS with with one-sparse models.
Figure 5: Compare ensembles with and without additive priors on a neural network bandit.
Figure 6: Performance of linear hypermodel with varying strengths in observation noise and pertur-bationIn Figure 7 we present results on how mis-specification in prior can affect the performance of thelinear hypermodel. Plot shows the regret of linear hypermodel after 100k steps, when the prior ismis-specified. Prior is mis-speicified by drawing weights of the prior network from a distributionsuch that the variance of these weights are a factor m times that of the variance of the weights ofthe generator, we call this value m as the prior weight multiplier. We can see that a very small value14Published as a conference paper at ICLR 2020of m does not induce sufficient exploration and leads to a huge regret. Similarly, a large value of mcan also induce more exploration than desired and leads to some additional regret.
Figure 7: Performance of linear hypermodels for different values of multiplier mIn Figure 8, we show how performance of a linear hypermodel is affected by the index dimension.
Figure 8: Performance of linear hypermodels for different index dimensionsD Diagonal linear hypermodels and Bayes by BackpropAn alternative approach for approximating posterior distributions for neural networks is variationalmethods such as Bayes by Backprop (Blundell et al., 2015). Bayes by Backprop assumes a diagonalGaussian distribution as the variational posterior of neural network weights, which in effect uses adiagonal linear hypermodel. Its training algorithm follows the variational inference framework andaims to minimize a KL loss.
Figure 9: Compare (i) a diagonal hypermodel agent trained with perturbed SGD, (ii) a diagonalhypermodel agent trained with Bayes by Backprop, (iii) a linear hypermodel agent trained withperturbed SGD, and (iv) a dropout agent on a small neural network bandit.
