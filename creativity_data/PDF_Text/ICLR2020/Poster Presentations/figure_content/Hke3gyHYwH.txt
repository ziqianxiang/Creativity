Figure 1: Performance on binary classification using `2 loss. Setting 1: MNIST; Setting 2: CIFAR.
Figure 2: Test accuracy duringCIFAR-10 training (noise 0.4).
Figure 3: Setting 2, }W p4q }F and }Wp4q ´ W p4q p0q}F during training. Noise “ 20%, λ “ 4.
Figure 4: Plot of test error for fully connected two-layer network on MNIST (binary classificationbetween “5” and “8”) with difference trick and different mixing coefficients α, where f(θ1, θ2, x) “aαg(θι, x) — ʌ/ɪ´ɪg(θι, x). Note that this parametrization preserves the NTK. The network has10,000 hidden neurons and we train both layers with gradient descent with fixed learning rate for5,000 steps. The training loss is less than 0.0001 at the time of stopping. We observe that when αincreases, the test error drops because the scale of the initial output of the network goes down.
Figure 5: Training (dashed) & test (solid) errors vs. epoch for Setting 2. Noise rate “ 20%,λ P t0.25, 0.5, 1, 2, 4, 8u. Training error of AUX is measured with auxiliary variables.
Figure 6:	Setting 2, }W p7q }F and }W p7q ´ Wp7qp0q}F during training. Noise rate “ 20%, λ “ 4.
Figure 7:	Setting 2, }W p4q }F and }W p4q ´ Wp4qp0q}F during training. Noise rate “ 0, λ “ 2.
Figure 8: Setting 2, }W p7q }F and }W p7q ´ Wp7qp0q}F during training. Noise rate “ 0, λ “ 2.
