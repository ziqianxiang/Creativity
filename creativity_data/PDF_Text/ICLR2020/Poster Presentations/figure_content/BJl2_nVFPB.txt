Figure 1: Overview of our three step learning pipeline. The first step of the training consists inlearning an unbiased image representation via self-supervision using both labelled and unlabelleddata, which learns well the early layers of the representation; in the second step, we fine-tune only thelast few layers of the model using supervision on the labelled set; finally, the fine-tuned representationis used, via rank statistics, to induce clusters in the unlabelled data, while maintaining a goodrepresentation on the labelled set.
Figure 2: Evolution of the t-SNEduring the training of CIFAR-10.
Figure 3: t-SNE on CIFAR-10: impact of incre-mental Learning. (a) representation on the la-belled and (b) Unlabelled CIFAR classes. Colorsof data points denote their groUnd-trUth labels. Weobserve a bigger overlap in (a) between the “old”class 3 and the “new” class 5 when not incorporat-ing Incremental Learning.
Figure 4:{1, 2, 3, 5, 7, 10, 15, 20, 50}.
