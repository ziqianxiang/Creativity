Figure 1: Test accuracy and label precision vs. number of epochs on MNIST dataset.
Figure 2: Test accuracy (%) on Tiny-ImageNet dataset with symmetric noise(a) Symmetry-0%(c) Symmetry-20%(b) Symmetry-10%(d) Symmetry-30%(e) Symmetry-40%(f) Symmetry-50%Figure 3: Test accuracy vs. number of epochs on CIFAR10 dataset.
Figure 3: Test accuracy vs. number of epochs on CIFAR10 dataset.
Figure 4: Training/Test accuracy for soft and hard hinge loss with different optimizer on CIFAR100H Multi-Class ExtensionFor multi-class classification, denote the groudtruth label as y ∈ {1, ..., K}. Denote the classifica-tion prediction (the last layer output of networks before loss function) as ti, i ∈ {1, ..., K}. Then,the classification margin for multi-class classification can be defined as followsu = ty - max ti .	(80)We can see that 1 u < 0 = 1 ty - max ti < 0 is indeed the 0-1 loss for multi-class classification.
Figure 5: Test accuracy and label precision vs. number of epochs on CIFAR10 dataset.
Figure 6: Test accuracy and label precision vs. number of epochs on CIFAR100 dataset.
Figure 7: Test accuracy vs. number of epochs on MNIST dataset.
Figure 8: Test accuracy vs. number of epochs on CIFAR100 dataset.
