Figure 1: The importance of temperature for evaluating NLG models on quality and diversity. Eachsub-figure plots inverse quality against inverse diversity (lower is better for both metrics). Left:current way of comparing NLG models. In this case, it is impossible to come to any meaningfulconclusions about which model (red or blue) dominates the other. Middle: With our proposedNLG evaluation framework, the temperature sweep shines a light on the relative performance of themodels: the red model should be used for high-diversity samples and the blue model for high-qualitysamples. Right: A second simulated scenario (consistent with the left Figure) where the temperaturesweep reveals that the blue model dominates the red. That is, for any desired diversity-level, there isa temperature for which the blue model outperforms the red in terms of quality (and vice versa).
Figure 2: Negative BLEU-5 versus SBLEU-5 (lower is better for both metrics) on theEMNLP2017 News dataset taken from (Lu et al.,2018b) and this work (train data and FM-GAN).
Figure 3: Effect of temperature tuning on theglobal metrics (lower is better for both metrics)for the synthetic task.
Figure 4: Results on the EMNLP 2017 News dataset. (lower is better for all metrics). MLE under atemperature sweep achieves better quality-diversity trade-off compared to the GAN approaches.
Figure 6: Analysis of decoding methods. (lower is better for all metrics). Left: Less biased methodsprovided a better quality/diversity trade-off. Right: However, they are computationally much moreexpensive.
Figure 5: Dotted line indicates the start ofGAN training. We notice a clear drop inentropy (spike in NLLtest ) when movingfrom maximum-likelihood to adversarialupdates.
Figure 7: Negative BLEU-5 on test data against SBLEU5 for models with different temperatureapplied at training time. The redder the dot, the higher the Î± i.e. more pressure to increase entropy.
Figure 8: Discriminator Rejection Sampling is not a great tool to navigate in quality-diversity space.
Figure 9: Different strategies to modulate the quality/diversity trade-off in FM-GAN.
Figure 10: MLE still outperforms RL-GAN under different decoding mechanisms.
