Figure 1: Illustrations of the two new pre-training objectives2.2	Transformer EncoderWe use a multi-layer bidirectional Transformer encoder (Vaswani et al., 2017) to encode contextualinformation for input representation. Given the input vectors X = {xi}iN=1, an L-layer Transformeris used to encode the input as:Hl = T ransf ormerl(Hl-1)	(1)where l ∈ [1,L], H0 = X and HL = [hL,…，hN]. We use the hidden vector hL as the contextualizedrepresentation of the input token ti.
Figure 2: Loss and accuracy of word and sentence prediction over the number of pre-training steps7Published as a conference paper at ICLR 2020compared with BERT, the augmented shuffled token prediction in StructBERT’s word structuralobjective had little effect on the loss and accuracy of masked token prediction. On the other hand, theintegration of the simpler task of shuffled token prediction (lower loss and higher accuracy) providesStructBERT with the capability of word reordering. In contrast, the new sentence structural objectivein StructBERT leads to a more challenging prediction task than that in BERT, as shown in the twofigures at the bottom. This new pre-training objective enables StructBERT to exploit inter-sentencestructures, which benefits sentence-pair downstream tasks.
