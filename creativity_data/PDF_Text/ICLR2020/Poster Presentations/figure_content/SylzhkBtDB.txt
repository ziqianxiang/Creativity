Figure 1: An illustration ofthe multi-task learning architecturewith a shared lower module B andk task-specific modules {Ai}ik=1.
Figure 2: Positive vs. Negative transfer is affected by the data一not just the model. See lower right-vs-mid. Task 2 and 3have the same model (dotted lines) but different data distribu-tions. Notice the difference of data in circled areas.
Figure 3: Performance improvement of a target task (Task 1) by MTL with a source task vs. STL.
Figure 4: Illustration of the covariance alignment module on task embeddings.
Figure 5: Performance improvements of Algorithm 1 by aligning task embeddings.
Figure 6: Covariance similarity score vs.
Figure 7: Comparing MTL model performance over different task similarity. For (a) and (c), MTLtrains two regression tasks; For (b) and (d), MTL trains two classification tasks. For regressiontasks, we use spearman correlation as model performance indicator. For classification tasks, weuse accuracy as the metric. We report the average model performance over two tasks. The x-axisdenotes the cosine distance, i.e. 1 - cos(θ1 , θ2).
Figure 8:	The performance improvement on the target task (MTL minus STL) by varying the cosinesimilarity of the two tasks’ STL models. We observe that higher similarity between the STL modelsleads to better improvement on the target task.
Figure 9:	Comparing Algorithm 1 to the baseline MTL training on the synthetic example in Section2.3. Algorithm 1 corrects the negative transfer phenomenon observed in Figure 3.
Figure 10:	Cross validation to choose the best performing model capacity for each model.
Figure 11:	Validation on MLP, CNN and LSTM models for sentiment analysis tasks.
Figure 12:	Comparing Algorithm 2 to the unweighted scheme and Kendall et al. (2018).
