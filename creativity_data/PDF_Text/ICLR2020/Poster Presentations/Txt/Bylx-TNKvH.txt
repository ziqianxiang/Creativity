Published as a conference paper at ICLR 2020
Functional vs. parametric equivalence
of ReLU networks
Mary Phuong & Christoph H. Lampert
IST Austria
Am Campus 1, Klosterneuburg, Austria
{bphuong,chl}@ist.ac.at
We address the following question: How redundant is the parameterisation of ReLU net-
works? Specifically, we consider transformations of the weight space which leave the func-
tion implemented by the network intact. Two such transformations are known for feed-
forward architectures: permutation of neurons within a layer, and positive scaling of all
incoming weights of a neuron coupled with inverse scaling of its outgoing weights. In this
work, we show for architectures with non-increasing widths that permutation and scaling
are in fact the only function-preserving weight transformations. For any eligible architec-
ture we give an explicit construction of a neural network such that any other network that
implements the same function can be obtained from the original one by the application of
permutations and rescaling. The proof relies on a geometric understanding of boundaries
between linear regions of ReLU networks, and we hope the developed mathematical tools
are of independent interest.
1	Introduction
Ever since its early successes, deep learning has been a puzzle for machine learning theorists. Mul-
tiple aspects of deep learning seem at first sight to contradict common sense: single-hidden-layer
networks suffice to approximate any continuous function (Cybenko, 1989; Hornik et al., 1989), yet in
practice deeper is better; the loss surface is highly non-convex, yet it can be minimised by first-order
methods; the capacity of the model class is immense, yet deep networks tend not to overfit (Zhang
et al., 2017).
Recent investigations into these and other questions have emphasised the role of over-
parameterisation, or highly redundant function representation. It is now known that over-
parameterised networks enjoy both easier training (Allen-Zhu et al., 2019; Du et al., 2019; Frankle
& Carbin, 2019), and better generalisation (Belkin et al., 2019; Neyshabur et al., 2019; Novak et al.,
2018). However, the specific mechanism by which over-parameterisation operates is still largely a
mystery.
In this work, we study one particular aspect of over-parameterisation, namely the ability of neural
networks to represent a target function in many different ways. In other words, we ask whether many
different parameter configurations can give rise to the same function. Such a notion of parameteri-
sation redundancy has so far remained unexplored, despite its potential connections to the structure
of the loss landscape, as well as to the literature on neural network capacity in general.
Specifically, we consider feed-forward ReLU networks, with weight matrices W1, . . . , WL, and
biases b1, . . . , bL,. We study parameter transformations which preserve the output behaviour of the
network h(z) = WLσ(WL-1σ(. . . W1z + b1 . . . ) + bL-1) + bL for all inputs z in some domain
Z. Two such transformations are known for feed-forward ReLU architectures:
1.	Permutation of units (neurons) within a layer, i.e. for some permutation matrix P,
Wl J PWi,	bl J Pbι,	(1)
W1+1 J Wι+ιP-1.	⑵
2.	Positive scaling of all incoming weights of a unit coupled with inverse scaling of its out-
going weights. Applied to a whole layer, with potentially different scaling factors arranged
into a diagonal matrix M, this can be written as
Wl J MWl,	bl J Mbl,	(3)
Wl+1 J Wl+1M-1.	(4)
1
Published as a conference paper at ICLR 2020
Our main theorem applies to architectures with non-increasing widths, and shows that there are
no other function-preserving parameter transformations besides permutation and scaling. Stated
formally:
Theorem 1. Consider a bounded open nonempty domain Z ⊆ Rd0 and any architecture
(do,...,瓦)with do ≥ di ≥ •… ≥ "l-i ≥ 2,“工 =1. For this architecture, there exists a
ReLU network hθ : Z → R, or equivalently a setting of the weights θ , (W1, b1, . . . , WL, bL),
such that for any ‘general’ ReLU network hη : Z → R (with the same architecture) satisfying
hθ (z) = hη (z) for all z ∈ Z, there exist permutation matrices P1, . . . PL-1, and positive diagonal
matrices M1,	. . . , ML-1, such that		
W1	M1P1W01,	b1	M1P1b01,
Wl	MlPlWl0Pl--11Ml--11,	bl	MlPlb0l,
WL	W0LPL--11ML--11,	bL	b0L,
l∈{2,...,L-1},	(5)
where η , (W01, b01, . . . , WL0 , b0L) are the parameters of hη.
In the above, ‘general’ networks is a class of networks meant to exclude degenerate cases. We give a
more precise definition in Section 3; for now it suffices to note that almost all networks are general.
The proof of the result relies on a geometric understanding of prediction surfaces of ReLU networks.
These surfaces are piece-wise linear functions, with non-differentiabilities or ‘folds’ between linear
regions. It turns out that folds carry a lot of information about the parameters of a network, so much
in fact, that some networks are uniquely identified (up to permutation and scaling) by the function
they implement. This is the main insight of the theorem.
In the following sections, we introduce in more detail the concept of a fold-set, and describe its
geometric structure for a subclass of ReLU networks. The paper culminates in a proof sketch of the
main result. The full proof, including proofs of intermediate results, is included in the Appendix.
2	Related work
The functional equivalence of neural networks is a well-researched topic in classical connectionist
literature. The problem was first posed by Hecht-Nielsen (1990), and soon resolved for feed-forward
networks with the tanh activation function by Chen et al. (1993), who showed that any smooth
transformation of the weight space that preserves the function of all neural networks is necessarily
a composition of permutations and sign flips. For the same class of networks, Fefferman & Markel
(1994) showed a somewhat stronger result: knowledge of the input-output mapping of a neural
network determines both its architecture and its weights, up to permutations and sign flips. Similar
results have been proven for single-layer networks with a saturating activation function such as
sigmoid or RBF (Kurkova & Kainen, 1994), as well as single-layer recurrent networks with a smooth
activation function (Albertini & Sontag, 1993a;b).
To the best of our knowledge, no such theoretical results exist for networks with the ReLU activation,
which is non-saturating, asymmetric and non-smooth. Broadly related is the recent work by Petersen
et al. (2018) and Berner et al. (2019) who study whether two neural networks (ReLU or otherwise)
that are close in the functional space have parameterisations that are close in the weight space. This
is called inverse stability. In contrast, we are interested in ReLU networks that are functionally
identical, and ask about all their possible parameterisations.
In terms of proof technique, our approach is based on the geometry of piece-wise linear functions,
specifically the boundaries between linear regions. The intuition for this kind of analysis has pre-
viously been presented by Raghu et al. (2017) and Serra et al. (2018), and somewhat similar proof
techniques to ours have been used by Hanin & Rolnick (2019) in the context of counting the number
of linear decision regions.
Finally, the sets of equivalent parametrisations can be viewed as symmetries in the weight space,
with implications for optimisation. Multiple authors, including e.g. Neyshabur et al. (2015); Badri-
narayanan et al. (2016); Stock et al. (2019), have observed that the naive loss gradient is sensitive to
reparametrisation by scaling, and proposed alternative, scaling-invariant optimisation procedures.
2
Published as a conference paper at ICLR 2020
3	ReLU networks
This section introduces notation and two important classes of ReLU networks that we refer to
throughout the manuscript. We denote by σ the ReLU function: σ(u)i = max {0, ui} for
i ∈ [dim(u)]; the subscript index denotes the corresponding vector element.
ReLU network. Let Z ⊆ Rd0 with d0 ≥ 2 be a nonempty open set, and let θ ,
(W1, b1, . . . , WL, bL) be the network’s parameters, with Wl ∈ Rdl×dl-1,bl ∈ Rdl, and dL = 1.
We denote the corresponding ReLU network by hθ : Z → R, where
hθ , hθL◦ σ ◦ hL-1 ◦•••◦ σ ◦h1θ ,	(6)
and hθ(Z) = W? ∙ Z + b(. For 1 ≤ l ≤ k ≤ L, we also introduce notation for truncated networks,
hlθk，hθ ◦ σ ◦ hk-1 …。σ ◦ hθ.	(7)
We will omit the subscript θ when it is clear from the context.
General ReLU network. In this work, we restrict our attention to so-called general ReLU net-
works. Intuitively, a general network is one that satisfies a number of non-degeneracy properties,
such as all weight matrices having non-zero entries and full rank, no two network units exactly
cancelling each other out, etc. It can be shown1 that almost all ReLU networks are general. In
other words, a sufficient condition for a ReLU network to be general with probability one is that its
weights are sampled from a distribution with a density.
More formally, a general ReLU network is one that satisfies the following three conditions.
1.	For any unit (l, i), the local optima of hi1:l do not have value exactly zero.
2.	For all k ≤ l and all diagonal matrices (Ik, . . . , Il) with entries in {0, 1},
rank(LWlL-I …IkWk) = min {dk-i, rank(Ik),..., rank(L-i), rank(L)}.	(8)
3.	For any two units (l, i), (k, j), any linear region R1 ⊆ Z of hi1:l, and any linear region
R2 ⊆ Z of hj1:k, the linear functions implemented by hi1:l on R1 and hj1:k on R2 are not
multiples of each other.
General networks are convenient to study, as they exclude many degenerate special cases.
The second important class of ReLU networks are so-called transparent networks. Their signifi-
cance as well as their name will become clear in the next section. For now, we state the definition.
Transparent ReLU network. A ReLU network h : Z → R is called transparent if for all Z ∈ Z
and l ∈ [L - 1], there exists i ∈ [dl] such that hi1:l(Z) ≥ 0. In words, we require that for any input,
at least one unit on each layer is active.
4	Fold-sets
In this section we introduce the concept of fold-sets, which is key to our understanding of ReLU
networks and their prediction surfaces. Since ReLU networks are piece-wise linear functions, a
great deal about them is revealed by the boundaries between individual linear regions. A network’s
fold-set is simply the union of all these boundaries.
More formally, if Z is an open set, and f : Z → R is any continuous, piece-wise linear function, we
define the fold-set of f, denoted by F(f), as the set of all points at which f is non-differentiable.
It turns out there is a class of networks whose fold-sets are especially easy to understand; these are
the ones we have termed transparent. For transparent networks, we have the following characterisa-
tion of the fold-set (which also motivates the name ‘transparent’).
1See Appendix, Lemmas A.10, A.11 and A.12.
3
Published as a conference paper at ICLR 2020
Lemma 1. If h : Z → R is a general and transparent ReLU network, then
F(h) = [{z | h1"(z) = 0}.	(9)
To appreciate the significance of the lemma, suppose we are given some transparent ReLU network
function h and we want to infer its parameters. This lemma shows that the knowledge of the end-
to-end mapping h , h1:L in fact gives us information about the network’s hidden units hi1:l (hence
‘transparent’). Moreover, this information is very explicit: we observe the units’ zero-level sets,
which in the case of a linear unit on a full-dimensional space already determines the unit’s parame-
ters up to scaling2 3. Of course, dealing with piece-wise linearity and disambiguating the union into
its constituent zero-level sets remains a challenge for upcoming sections.
5	Piece-wise linear surfaces
In this section, we provide a geometric description of fold-sets of transparent networks. Intuitively,
the fold-sets look like the sets shown in Figure 1. The first-layer units of a network are linear, so the
component Si z | hi1:1(z) = 0 of the fold-set (9) is a union of hyperplanes, illustrated by the blue
lines in Figure 1. These hyperplanes partition the input space into a number of regions that each
correspond to a different activation pattern. For a fixed activation pattern, or equivalently on each
region, the second-layer units are linear, so their zero-level sets Si z | hi1:2 (z) = 0 are composed
of piece-wise hyperplanes on the partition induced by the first-layer units. This is shown by the
orange lines in Figure 1. More generally, the lth-layer zero-level sets Si z | hi1:l(z) = 0 consist of
piece-wise hyperplanes on the partition induced by all lower-layer units. This yields a fold-set that
looks like the set in the right pane of Figure 1, but potentially much more complicated.
We now define these concepts more precisely.
Piece-wise hyperplane. Let P be a partition of Z. We say H ⊆ Z is a piece-wise hyperplane
with respect to partition P, if H is nonempty and there exist (w, b) 6= (0, 0) and P ∈ P such that
H = {z ∈ P | w|z + b = 0}.
Piece-wise linear surface. A set S ⊆ Z is called a piece-wise linear surface on Z of order κ if
it has a representation of the form S = Sl∈[κ],i∈[n ] Hil , where each Hil is a piece-wise hyperplane
with respect to the partition induced by Sk∈[l-1],j∈[n ] Hjk, and no number smaller than κ admits
such a representation.
Using these definitions, the following lemma formalises the intuition behind Figure 1.
Lemma 2. If h is a general and transparent ReLU network, then its fold-set is a piece-wise linear
surface of order at most L - 1.
2See Appendix, Lemma A.19.
3A similar figure has appeared in the work of Raghu et al. (2017).
4
Published as a conference paper at ICLR 2020
Figure 2: A piece-wise linear surface with few intersections be-
tween piece-wise hyperplanes. From the fold-set alone (right)
it is not possible to determine if a hyperplane emerged from the
first layer (left, blue) or from the second one (left, orange).
Figure 3: Greedy layer as-
signment to the piece-wise
linear surface in Figure 1.
The final ingredient we will need to be able to reason about the parameterisation of ReLU networks
is a more precise characterisation of the fold-set, in particular, the dependence structure between
individual piece-wise hyperplanes. For example, consider the piece-wise linear surface in Figure 1
and compare it to the one in Figure 2. Suppose as before that the blue hyperplanes come from
first-layer units, the orange hyperplanes come from second-layer units, and the black hyperplanes
come from third-layer units. The difference between Figure 1 and Figure 2 is that if we observe only
the fold-set, i.e. only the union of the zero-level sets over all layers (as shown in the right pane of
Figure 2), then in the case of Figure 2, it is impossible to know which folds come from which layers.
For instance, the blue folds and the orange folds could be assigned to the first and second layer
almost arbitrarily; there is not enough information (i.e. intersection) in the fold-set to tell which is
which. In contrast, the piece-wise linear surface in the right pane of Figure 1 could in principle be
disambiguated into first-, second- and third- layer folds by the following procedure:
1.	Take the largest possible union of hyperplanes that is a subset of the fold-set, and assign
the hyperplanes to layer one.
2.	Take all piece-wise hyperplanes with respect to the partition induced by the first-layer folds,
and assign them to layer two.
3.	Take all piece-wise hyperplanes with respect to the partition induced by the first- and
second- layer folds, and assign them to layer three.
This procedure is not guaranteed to assign all folds to their original layers because it ignores how
piece-wise hyperplanes are connected; for example for the piece-wise linear surface in Figure 1, the
procedure yields the layer assignment shown in Figure 3. However, it is sufficient for our purposes,
and it is easier to work with mathematically.
Formally, for a piece-wise linear surface S, we denote
kS :=	{S0 ⊆ S | S0 is a piece-wise linear surface of order at most k}.	(10)
One can show4 that kS is itself a piece-wise linear surface of order at most k, so one can think of
kS as the ‘largest possible’ subset of S that is a piece-wise linear surface of order at most k. For
the piece-wise linear surface in Figure 3, the set 1S consists of the blue hyperplanes, 2S consists
of the blue and the orange (piece-wise) hyperplanes, and 3S = S.
This definition allows us to uniquely decompose S into its piece-wise hyperplanes. Let S =
Sl∈[κ],i∈[n ] Hli be any representation of S in terms of its piece-wise hyperplanes. We say the rep-
resentation is canonical if each Hli is distinct and Sl∈[k],i∈[n ] Hli = kS for all k ∈ [κ]. One can
show5 that such a representation exists and is unique up to subscript indexing. Importantly, it assigns
a unique ‘layer’ to each piece-wise hyperplane, its superscript.
4See Appendix, Lemma A.1.
5See Appendix, Lemmas A.5 and A.6.
5
Published as a conference paper at ICLR 2020
Figure 4: A piece-wise linear surface in canonical form and its dependency graph.
The dependency graph (See also Figure 4) is a way to formally describe the dependencies between
piece-wise hyperplanes.
Dependency graph. Let S = Uι∈[κ] i∈[nl] Hi be the canonical representation of S. The depen-
dency graph of S is the directed graph that has the piece-wise hyperplanes Hli}l i as vertices, and
has an edge Hi → Hk iff l < k and relint Hi ∩ Cl Hk = 0. That is, there is and edge Hi → Hk if
Hjk ‘depends on’ or ‘bends at’ Hli .
6	Main res ult
With all the necessary concepts in place, we now put the pieces together and explain the proof idea
behind the main result. We restate the theorem here for the reader’s convenience.
Theorem 1. Consider a bounded open nonempty domain Z ⊆ Rd0 and any architecture
(do,..., d，L)with do ≥ di ≥ …≥ dL-ι ≥ 2, dL = L For this architecture, there exists a
ReLU network hθ : Z → R such that for any general ReLU network hη : Z → R (with the same ar-
chitecture) satisfying hθ (z) = hη (z) for all z ∈ Z, there exist permutation matrices P1, . . . PL-1,
and positive diagonal matrices M1, . . . , ML-1, such that
W1 = M1P1W10 ,
Wl = MlPlW0lPl--11Ml--11,
WL = WL0 PL--11ML--11,
b1 = M1P1b01,
bl = MlPlb0l ,
bL = b0L ,
l∈{2,...,L-1},	(11)
where (W1, b1, . . . , WL, bL) are the parameters of hθ, and (W01, b01, . . . , WL0 , b0L) are the pa-
rameters of hη.
In other words, for architectures with non-increasing widths, there exists a ReLU network h such
that knowledge of the input-output mapping h determines the network’s parameters uniquely up to
permutation and scaling.
The idea behind the proof is as follows. Suppose we are given the function h. Then we also know
its fold-set F(h), and if h is general and transparent, the fold-set is a piece-wise linear surface (by
Lemma 2) of the form F(h) = Ul,i z | hi1:l(z) = 0 . As we have mentioned earlier, this union
of zero-level sets contains a lot of information about the network’s parameters, provided we can
disambiguate the union to obtain the zero-level sets of individual units.
This disambiguation of the union is crucial, but is impossible in general. To see why, con-
sider the first-layer units: given F(h), we want to identify Ui z | hi1:1(z) = 0 . We know that
Ui z | hi1:1(z) = 0 is a union of d1 hyperplanes that is a subset of 1F(h), so if 1F(h) is a
union of d1 hyperplanes, we are done. In general however, F(h) may contain more than d1 hyper-
planes, such as for example in Figure 2. In such a setting it is impossible to tell which hyperplanes
come from the first layer.
6
Published as a conference paper at ICLR 2020
The key insight here is the following: even though, say, a last-layer unit can create a fold that looks
like a hyperplane, this hyperplane cannot have any dependencies, or descendants in the dependency
graph. This follows from the fact that the layer is the last. More generally, if a (piece-wise) hyper-
plane has a chain of descendants of length m, it must come from a layer that is at least m layers
below the last one. Formally, we have the following lemma.
Lemma 3. Let h : Z → R be a general ReLU network. Denote S := l∈[λ],i∈[d ] z | hi1:l(z) = 0
and let S = k∈[κ],j∈[n ] Hjk be the canonical representation of S. Then for all Hjk there exists a
unit (l, i) with l ≥ k such that Hjk ⊆ z | hi1:l(z) = 0 . Moreover, if the dependency graph of S
contains a directed path of length m starting at Hjk, then l ≤ λ - m.
Main proof idea. This lemma motivates the main idea of the proof. We explicitly construct a
network h such that the dependency graph of its fold-set is well connected. More precisely, we
ensure that each of the hyperplanes corresponding to first-layer units has a chain of descendants of
length L - 2. This implies by Lemma 3 that the first-layer hyperplanes can be identified as such,
using only the information contained in the fold-set. One can show that this is sufficient to recover
the parameters W1, b1, up to permutation and scaling. To extend the argument to higher-layers, we
then consider the truncated network hl:L . In hl:L, layer l becomes the first layer, and we apply the
same reasoning as above to recover Wl , bl .
The next lemma shows that a network with a ‘well connected’ dependency graph exists. In what
follows, f |a denotes the restriction of a function f to a domain A, and Z§，{σ(hθ:1 (Z)) | Z ∈ Z}
is the set of all possible inputs to the truncated network hl:L. For notational convenience, we define
Z0 , Z
.
Lemma 4. For a bounded open nonempty domain Z and architecture (d0, . . . , dL) with d0 ≥ d1 ≥
∙∙∙ ≥ dL-ι ≥ 2, dL = 1, there exists a general transparent ReLU network h : Z → R such that
for l ∈ [L - 1], the fold-set F(hl:L|intZl-1) is a piece-wise linear surface whose dependency graph
contains dl directed paths of length (L - 1 - l) with distinct starting vertices.
Theorem 1 then follows by the inductive argument outlined above.
Proof sketch of Theorem 1. Let hθ be the network from Lemma 4. One can show that if hθ is
transparent, and hη (Z) = hθ (Z) for all Z ∈ Z, then also hη is transparent, and all the truncated
networks hθL, hnL are transparent.
We proceed by induction. Let l = 1. Then we have
hθ'lint Zθ^1 ≡ hθ ≡ hη ≡ %'卜m Zθ^1	(12)
which implies F(hθL∣atz-ι) = F(hηL∣int z1 ). (For notational convenience, we will omit the
domain restriction for now.) Because both networks are general and transparent, the fold-sets are
representable as unions of the respective zero-level sets, and we obtain
U	{z | hθlτ+kj](z)=0}= U {z | hηlτ+k [j](z) = 0}	(13)
k∈[L-l],j∈[dk]	k∈[L-l],j∈[dk]
This is a piece-wise linear surface, whose dependency graph by Lemma 4 contains dl directed paths
of length (L- 1-l) with distinct starting vertices. Denote these vertices H1, . . . , Hdl . By Lemma 3,
Hi ⊆ {z | hθl-1+λ[∣](z) = 0} for some (λ, ι) with λ ≤ (L - l) - (L - 1 - l) = 1. We thus
obtain Si∈[d ] Hi ⊆ Si∈[d ] Z | hlθ[ι](Z) = 0 , where on the left-hand side we have a union of dl
hyperplanes, and on the right-hand side we have a union of at most dl hyperplanes. It follows that
the two sides are equal, and by applying the same argument to hη, we get
U {z I hlθ[i](z) = 0} = U {z I hlη[i](z)=0}.	(14)
i∈[dl]	i∈[dl]
Therefore there must exist a permutation π : [dl] → [dl] such that
{z I hlθ [i](z) = 0} = {z I hlη [π (i)](z) = 0}	(15)
7
Published as a conference paper at ICLR 2020
for all i. One can show6 that this implies the existence of scalars m1, . . . mdl, such that
(Wl[i, ：], bι[i]) = mi(WO [π(i), :],b0 [π(i)]).	(16)
We know that mi 6= 0 because the folds z | hlθ[i](z) = 0 , z | hlη[i](z) = 0 , are nonempty; oth-
erwise Si∈[d ] Hi could not be a union of dl hyperplanes. We have thus shown that there exists
a permutation matrix Pl ∈ Rdl×dl and a nonzero-entry diagonal matrix Ml ∈ Rdl ×dl such that
Wl = MlPlWl0 and bl = MlPlb0l. One can also show that the scalars mi are positive.7
For the inductive step, let l ∈ {2, . . . , L - 1}, and assume that there exist permutation matrices
P1, . . . , Pl-1, and positive-entry diagonal matrices M1, . . . , Ml-1, such that (65) holds up to layer
l - 1. Then hθ:l-1 ≡ Ml-IPl-Ih1jl-1. Since the end-to-end mappings are the same, hθL ≡ h^L,
it follows that the truncated mappings satisfy
hθLlint Zθ-1 ≡ (hηL ◦ P-IIM-l) Lt Zθ-1 ≡ hηLlint ZL	⑺
Where η := (W0P-1ιM-1ι, b0, W0+ι, b3,… , WL0 , b0L). We therefore apply the same argu-
ment to hθL 卜口1 zi-ι and hl-L ∖int 工― as we presented above for the case l = 1. We obtain that there
exists a permutation matrix Pl ∈ Rdl ×dl and a positive-entry diagonal matrix Ml ∈ Rdl ×dl such
that
Wl =MlPlWl0Pl--11Ml--11,	bl =MlPlb0l.	(18)
Finally, consider the last layer. We know that hθ:LT ≡ ML-IPL-ih*L-1, which implies hθ ≡
hηL ◦ PL--1 1ML--1 1, i.e. hθL and hηL ◦ PL--1 1ML--1 1 are identical linear functions supported on the
full-dimensional domain ZL-1. It follows that WL = WLP--ιM--ι and bL = bL∙	□
Discussion of assumptions. Most of the theorem’s assumptions have their origin in Lemma 4. The
reason we restrict the domain of hl:L to the interior of Zl-1 is that we want hl:L to be defined on an
open set (otherwise fold-sets become unwieldy). For similar reasons, we study only architectures
with non-increasing widths; otherwise int Zl-1 may be empty. We conjecture that the theorem does
not hold for more general architectures. If it does, the proof will likely go beyond fold-sets.
To guarantee transparency, our construction is such that for each input z ∈ Z and layer l ∈ [L - 1],
either h11:l (z) > 0 or h21:l (z) > 0. Transparency could in principle be achieved with just a single
unit, but it would have to be positive everywhere. This is why we impose dl ≥ 2. Guaranteeing
transparency for the first layer (whose inputs are not constrained to the positive quadrant) also ne-
cessitates boundedness of Z. Boundedness can be lifted ifwe consider a slightly modified definition
of transparency; proofs become more complicated though and we do not consider this crucial.
Almost all of the proof carries over to the case of leaky ReLU activations (where σ is defined as
σ(u)i = max {αui, ui} for some small α > 0). The part that does not carry over is our proof that
Ml has only positive entries on the diagonal: In this part, we compare the slope of hθL for inputs
on the positive and negative side of a given ReLU unit, and notice that the negative-side slope is
‘singular’ in the sense that some basis directions have zero magnitude. This particular argument
does not work for the leaky ReLU, though we cannot rule out that a simple workaround exists.
7 Discussion & future work
In this work, we have shown that for architectures with non-increasing widths, certain ReLU net-
works are almost uniquely identified by the function they implement. The result suggests that the
function-equivalence classes of ReLU networks are surprisingly small, i.e. there may be only little
redundancy in the way ReLU networks are parameterised, contrary to what is commonly believed.
This apparent contradiction could be explained in a number of ways:
• It could be the case that even though exact equivalence classes are small, approximate
equivalence is much easier to achieve. That is, it could be that khθ - hη k ≤ is satisfied
6See Appendix, Lemma A.19.
7See Appendix, Theorem A.1.
8
Published as a conference paper at ICLR 2020
by a disproportionately larger class of parameters η than khθ - hη k = 0. This issue is
related to the so-called inverse stability of the realisation map of neural nets, which is not
yet well understood.
•	Another possibility is that the kind of networks we consider in this paper is not represen-
tative of networks typically encountered in practice, i.e. it could be that ‘typical networks’
do not have well connected dependency graphs, and are therefore not easily identifiable.
•	Finally, we have considered only architectures with non-increasing widths, whereas some
previous theoretical work has assumed much wider intermediate layers compared to the
input dimension. It is possible that parameterisation redundancy is much larger in such
a regime compared to ours. However, gains from over-parameterisation have also been
observed in practical settings with architectures not unlike those considered here.
We consider these questions important directions for further research. We also hypothesise that our
analysis could be extended to convolutional and recurrent networks, and to other piece-wise linear
activation functions such as leaky ReLU.
References
Francesca Albertini and Eduardo D. Sontag. For neural networks, function determines form. Neural
networks, 6(7):975-990,1993a.
Francesca Albertini and Eduardo D. Sontag. Identifiability of discrete-time neural networks. In
European Control Conference, 1993b.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learing (ICML), 2019.
Vijay Badrinarayanan, Bamdev Mishra, and Roberto Cipolla. Symmetry-invariant optimization in
deep networks. In International Conference on Learning Representations (ICLR), 2016.
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-
learning practice and the classical bias-variance trade-off. In Proceedings of the National
Academy of Sciences (PNAS), 2019.
Julius Berner, Dennis ElbraChter, and PhiliPP Grohs. HoW degenerate is the parametrization of neu-
ral networks with the ReLU activation function? In Conference on Neural Information Processing
Systems (NeurIPS), 2019.
An Mei Chen, HaW-minn Lu, and Robert Hecht-Nielsen. On the geometry of feedforWard neural
netWork error surfaces. Neural computation, 5(6):910-927, 1993.
George Cybenko. Approximation by superpositions ofa sigmoidal function. Mathematics of control,
signals and systems, 2(4):303-314, 1989.
Simon S. Du, Jason D. Lee, Haochuan Li, LiWei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural netWorks. In International Conference on Machine Learing (ICML), 2019.
Charles Fefferman and Scott Markel. Recovering a feed-forWard net from its output. In Conference
on Neural Information Processing Systems (NIPS), 1994.
Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural
netWorks. In International Conference on Learning Representations (ICLR), 2019.
Boris Hanin and David Rolnick. Complexity of linear regions in deep netWorks. In International
Conference on Machine Learing (ICML), 2019.
Robert Hecht-Nielsen. On the algebraic structure of feedforWard netWork Weight spaces. In Ad-
vanced Neural Computers, pp. 129-135. Elsevier, 1990.
Kurt Hornik, MaxWell Stinchcombe, and Halbert White. Multilayer feedforWard netWorks are uni-
versal approximators. Neural networks, 2(5):359-366, 1989.
9
Published as a conference paper at ICLR 2020
Vera Kurkova and Paul C. Kainen. Functionally equivalent feedforward neural networks. Neural
Computation, 6(3):543-558,1994.
Behnam Neyshabur, Ruslan R. Salakhutdinov, and Nati Srebro. Path-SGD: Path-normalized op-
timization in deep neural networks. In Conference on Neural Information Processing Systems
(NIPS), 2015.
Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan Srebro. Towards
understanding the role of over-parametrization in generalization of neural networks. In Interna-
tional Conference on Learning Representations (ICLR), 2019.
Roman Novak, Yasaman Bahri, Daniel A. Abolafia, Jeffrey Pennington, and Jascha Sohl-Dickstein.
Sensitivity and generalization in neural networks: an empirical study. In International Conference
on Learning Representations (ICLR), 2018.
Philipp Petersen, Mones Raslan, and Felix Voigtlaender. Topological properties of the set of func-
tions generated by neural networks of fixed size. In arXiv:1806.08459, 2018.
Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, and Jascha Sohl Dickstein. On the
expressive power of deep neural networks. In International Conference on Machine Learing
(ICML), 2017.
Thiago Serra, Christian Tjandraatmadja, and Srikumar Ramalingam. Bounding and counting linear
regions of deep neural networks. In International Conference on Machine Learing (ICML), 2018.
Pierre Stock, Benjamin Graham, Remi Gribonval, and Herve Jegou. Equi-normalization of neural
networks. In International Conference on Learning Representations (ICLR), 2019.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In International Conference on Learning Rep-
resentations (ICLR), 2017.
10
Published as a conference paper at ICLR 2020
A Appendix
Use the following look-up table to find a particular lemma or theorem and its proof.
Lemma 1
Lemma 2
Lemma 3
Lemma 4
Theorem 1
→ Lemma A.15
→ Corollary A.1
→ Lemma A.18
→ Lemma A.17
→ Theorem A.1
A. 1 Piece-wise linear surfaces
Definition A.1 (Partition). Let S ⊆ Z. We define the partition of Z induced by S, denoted PZ (S), as
the set of connected components of Z \ S.
Definition A.2 (Piece-wise hyperplane). Let P be a partition of Z. We say H ⊆ Z is a piece-wise
hyperplane with respect to partition P, if H = 0 and there exist (w, b) = (0,0) and P ∈ P such
that H = {z ∈ P | w|z + b = 0}.
Definition A.3 (Piece-wise linear surface / pwl. surface). A set S ⊆ Z is called a piece-wise linear
surface on Z of order κ ifit can be written as S = l∈[κ],i∈[n ] Hli, where each Hil is a piece-wise
hyperplane with respect to PZ ( k∈[l-1],j∈[n ] Hjk), and no number smaller than κ admits such a
representation.
Lemma A.1. If S1, S2 are piece-wise linear surfaces on Z of order k1 and k2, then S1 ∪ S2 is a
piece-wise linear surface on Z of order at most max {k1, k2}.
Proof. Let S1 = Sl∈[k1],i∈[nl] Hli and S2 = Sl∈[k2],i∈[ml] Gli be the pwl. surface representations of
S1, S2. Given Hli, consider the partition
P:=PZ	[	Hjk∪	[	Gjk .	(1)
k∈[l-1],j∈[nk]	k∈[max {l-1,k2}],j∈[mk]
We can write Hi = UP∈p Hi ∩ P and denote the nonempty intersections Hi ∩ P as {Hi,j}j∙
Similarly, we decompose Gi = Sj Gι,j. Then SI ∪ S2 = Sι∈[maχ {加也}] (Uij H Iij ∪ Si,j 僦力
where each HIlj and SIij is a piece-wise hyperplane wrt. P = Pz(Uk∈[i-i](Siο jο Hk0 jο ∪
Ui0 j0 TjO )).	'	^	'	' 口
Given sets Z and S ⊆ Z, we introduce the notation
iS := [ {S0 ⊆ S | S0 is a pwl. surface on Z of order at most i}.	(2)
(The dependence on Z is suppressed.) By Lemma A.1, iS is itself a pwl. surface on Z of order at
most i.
Lemma A.2. For i ≤ j and any set S, we have ijS = j iS = iS.
Proof. We will need these definitions:
iS = [ {S00	⊆	S | S00 is a pwl. surface of order at most i},	(3)
jS = [ {S00	⊆	S | S00 is a pwl. surface of order at most j},	(4)
ijS = [ {S00	⊆	jS | S00 is a pwl. surface of order at most i},	(5)
jiS = [ {S00	⊆ iS | S00 is a pwl. surface of order at mostj}.	(6)
Consider first the equality j iS = iS. We know that j iS ⊆ iS because the square operator
always yields a subset. At the same time, iS ⊆ j iS, because iS satisfies the condition for
membership in (6).
11
Published as a conference paper at ICLR 2020
To prove the equality ijS = iS, we use the inclusion jS ⊆ S to deduce ijS ⊆ iS. Now
let S00 ⊆ S be one of the sets under the union in (3), i.e. it is a pwl. surface of order at most i. Then it
is also a pwl. surface of order at most j, implying S00 ⊆ jS. This means S00 is also one of the sets
under the union in (5), proving that □iS ⊆ □i□7-S.	□
Lemma A.3. Let Z and S ⊆ Z be sets. Then one can write k+1S = kS ∪ i Hi where Hi are
piece-wise hyperplanes wrt. PZ (□k S).
Proof. Let □k+1S = Sl∈[κ],i∈[n ] Hli be the pwl. surface representation of □k+1S. If κ ≤ k, then
□k+1S = □kS and we are done. Otherwise, Sl∈[k],i∈[nk] Hil ⊆ □kS, implying
□k+1S ⊆ □kS∪ [ Hik+1.	(7)
i∈[nk+1]
At the same time, □kS ∪ Si∈[n ] Hik+1 is a pwl. surface of order at most k + 1 because □kS is
a pwl. surface of order at most k and Hik+1 can be decomposed into piece-wise hyperplanes wrt.
Pz(□kS). Therefore, □kS ∪ Ui∈[nk+ι] Hk+1 ⊆ □k+ιS, implying in fact equality.	□
Definition A.4 (Canonical representation of a pwl. surface). Let S be a pwl. surface on Z. The pwl.
surface representation S =	l∈[κ],i∈[n	]	Hil	is called canonical if	l∈[k],i∈[n	]	Hil	=	□kS for all
k ∈ [κ], and each Hli is distinct.
Lemma A.4. If S = l∈[κ],i∈[n ] Hil is a pwl. surface in canonical form, then κ is the order of S.
Proof. Denote the order of S by λ. By the definition of order, λ ≤ κ, and S = □λS. Then, since
S = Ul∈[κ],i∈[n ] Hil is a canonical representation, we have
[	Hil = □λS = S =	[	Hil .	(8)
l∈[λ],i∈[nl]	l∈[κ],i∈[nl]
It follows that K = λ.	□
Lemma A.5. Every pwl. surface has a canonical representation.
Proof. The inclusion Ul∈[k],i∈[n ] Hil ⊆ □kS holds for any representation. We will show the other
inclusion by induction in the order of S. If S is order one, □1S ⊆ S = Ui∈[n1]Hi1 holds for
any representation and we are done. Now assume the lemma holds up to order κ - 1, and let
S be order κ. Then by Lemma A.3, S = □κS = □κ-1S ∪ Ui Hiκ, where Hiκ are piece-wise
hyperplanes wrt. PZ(□κ-1S). By the inductive assumption, □κ-1S has a canonical representation,
say □κ-1S = Ul∈[κ-1],i∈[nl] Hil. We claim that S = Ul∈[κ],i∈[nl] Hil is a canonical representation
of S. If k = κ, then clearly □kS ⊆ S = Ul∈[κ],i∈[n ] Hli. If k ∈ [κ - 1], then by Lemma A.2,
□kS = □k□κ-1S = Ul∈[k],i∈[n ] Hil, where we have used the canonical representation of □κ-1S.
Finally, distinctness of Hi can be ensured by throwing away duplicates.	□
Lemma A.6. Let Z be an open set. If S is a piece-wise linear surface on Z, and if S =
l∈[κ],i∈[n ] Hli and S = k∈[κ],j∈[m ] Gjk are two canonical representations of S, then for all
l ∈ [κ], nl = ml and there exists a permutation π : [nl] → [nl] such that Hli = Glπ(i). In other
words, the canonical representation is unique up to within-order indexing.
Proof. Let k ∈ [κ]. Because both representations are canonical, we have
□k-1S ∪ [ Hik =□kS=□k-1S∪ [ Gjk,	(9)
i∈[nk]	j∈[mk]
where Hik and Gjk are piece-wise hyperplanes wrt. PZ(□k-1S). Then for each P ∈ PZ (□k-1S),
P ∩ [ Hik = P ∩ [ Gjk,	(10)
i∈[nk]	j∈[mk]
where on both sides above We have a union of hyperplanes on an open set. The claim follows. □
12
Published as a conference paper at ICLR 2020
Definition A.5 (Dependency graph of a pwl. surface). Let S be a piece-wise linear surface on Z,
and let S = l∈[κ],i∈[n ] Hli be its canonical representation. We define the dependency graph of
S as the directed graph that has the piece-wise hyperplanes Hil l i as vertices, and has an edge
Hl → Hk iff l < k and Hi ∩ Cl Hk = 0.	，
A.2 ReLU networks and folds
We denote by σ the ReLU function: σ(u)i = max {0, ui} for i ∈ [dim(u)].
Definition A.6 (ReLU network). Let Z ⊆ Rd0 with d0 ≥ 2 be a nonempty open set, and let
θ , (W1, b1, . . . , WL, bL) be the network’s parameters, with Wl ∈ Rdl×dl-1, bl ∈ Rdl, and
dL = 1. A ReLU network parameterised by θ is the function hθ : Z → R, defined by
hθ，hL ◦ σ ◦ hL-1 ◦•••◦ σ ◦ hθ,	(11)
where hθ (Z) = Wi ∙ Z + bι. For 1 ≤ l ≤ k ≤ L, we also denote
hlθk，hθ ◦ σ ◦ hk-1 …。σ ◦ hθ,	(12)
hlθk，σ ◦ hθk.	(13)
For a ReLU network hθ : Z → R and l ∈ [L 一 1], denote Zθ，{hθ"(Z) | Z ∈ Z}. Also, for
convenience, define Z0θ , Z. (We will omit the subscript θ when it is clear from the context.) We
write f|A to denote the restriction of the function f to the domain A.
Definition A.7 (Activation indicator). A tuple I , (I1, . . . , IL-1) is called an activation indicator
ifIl = diag(il) ∈ Rdl ×dl and il ∈ {0, 1}dl for l ∈ [L 一 1]. It is called non-trivial ifil 6= 0for all
l ∈ [L 一 1] and non-trivial up to k ifil 6= 0for all l ∈ [k].
Given a parameter vector θ , (W1 , b1 , . . . , WL , bL ) and an activation indicator I, we introduce
the notation
wl(θ, I) , e∣WιIι-ιWι-ι …IιWι,	(14)
l
bi(θ,I)，e| X WιIι-ι …Wk+ιIkbk.	(15)
k=1
(We will omit the argument θ when itis clear from the context.) These quantities characterise the dif-
ferent linear pieces implemented by the network’s units. Also define Iθ (Z) , (I1θ(Z), . . . , IθL-1(Z))
as the activation indicator for a specific input: If (z)[i, i]，1 {h}ι(Z) ≥ 0} for all (l,i).
Lemma A.7. In a ReLU network with parameters θ = (W1, b1, . . . , WL, bL), the pre-activations
satisfy h1"(Z) ∈ {wι(θ, I) ∙ Z + bi(θ,I)}『where the indexing runs over all possible activation
indicators I. MorepreciSely, h;ι(Z) = wi(θ, Iθ(z)) ∙ Z + bi(θ, Iθ(z)).
Proof. Left as exercise.	□
Definition A.8 (Fold-set). Let Z be an open set, and f : Z → R a continuous, piece-wise linear
function. We define the fold-set of f, denoted by F(f), as the set of all points at which f is non-
differentiable.
Definition A.9 (Positive / negative in a neighbourhood). Let Z be an open set. The function f :
Z → R is positive (negative) in the neighbourhood ofZ ∈ Z iffor any > 0 there exists Z0 ∈ B(Z)
such that f(Z0) > 0 (f(Z0) < 0).
Definition A.10 (Unit fold-set). Let hθ : Z → R be a ReLU network. We define the unit (l, i)
fold-set of hf, denoted Fι(hf), as the set of all Z ∈ Z where hfl [i](Z) = 0 and hfl [i] is positive in
the neighbourhood of Z.
Lemma A.8. Let Z be an open set, and f : Z → R a continuous piece-wise linear function. Then
F(σ ◦ f ) consists of those Z ∈ Z that satisfy
•	f(Z) > 0 and Z ∈ F(f), or
13
Published as a conference paper at ICLR 2020
•	f(z) = 0 and f is positive in the neighbourhood of z.
Proof. We will prove that if Z satisfies any of the two conditions, then Z ∈ F(σ ◦ f), and if it violates
both, then z ∈ F(σ ◦ f)c. We begin with the latter implication.
Let Z be such that f(Z) > 0 and Z ∈/ F(f), i.e. f is differentiable at Z. Since f is piece-wise linear,
there exists > 0 such that all of B (Z) lies inside a single linear region off and f (B (Z)) ⊆ (0, ∞].
Then, on B(Z), the ReLU behaves like an identity, implying σ ◦ f is differentiable at Z, proving
that Z ∈ F(σ ◦ f)c. Next, consider Z such that f(Z) = 0. For it to violate the second condition, there
must exist a ball B(Z) around Z such that f(B (Z)) ⊆ (-∞, 0]. (This is also true if f(Z) < 0.)
Then, on B(Z), the ReLU behaves like a constant zero, implying that σ ◦ f is differentiable at Z.
We now prove the other implication. If f(Z) > 0 and Z ∈ F(f), then there exists > 0 such that
f (B (Z)) ⊆ (0, ∞], which guarantees that the ReLU behaves like an identity on B(Z). In this ball,
we have σ ◦ f = f, so Z ∈ F(σ ◦ f).
If f(Z) = 0 and f is positive in the neighbourhood of Z, we distinguish several cases. If Z ∈/ F(f),
then there exists a ball Bδ(Z) on which f behaves linearly, i.e. σ(f (Z)) = σ(wlz + b), implying
Z ∈ F(σ ◦ f). If Z ∈ F(f) and, in addition, there exists a ball Bδ(Z) such that f (Bδ (Z)) ⊆ [0, ∞),
then the ReLU behaves like an identity on Bδ(Z) and Z ∈ F(σ ◦ f). The final case is Z ∈ F(f) such
that f attains both positive and negative values in its neighbourhood. Since f is piece-wise linear,
there exist p, n such that f(Z + n) < 0 < f(Z + p), and Z + p, Z + n ∈/ F(f) for all ∈ (0, 1].
Then V(σ ◦ f )(z + EP) = 0 and V(σ ◦ f )(z + En) = 0, yielding Z ∈ F(σ ◦ f).	□
Lemma A.9. Let Z be an open set, and let f1, . . . , fn : Z → R be continuous, piece-wise linear
functions. For any w1, . . . , wn ∈ R, define f = Pin=1 wifi. Then F(f) ⊆ Sin=1 F(fi).
Proof Left as exercise.	□
A.3 General and transparent ReLU networks
Lemma A.10. For all θ except a closed zero-measure set,
rank(Wι I- ∙∙∙Ik Wk) = min {dk-i, rank(Ik),..., rank(L-i), dι},	(16)
rank(LWlI1…IkWk) = min {dk-i, rank(Ik),..., rank(L-i), rank(L)},	(17)
for all activation indicators I and all k ≤ l.
Proof. First, notice that (16) is just a special case of (17) with Il equal to the identity matrix. It
therefore suffices to prove (17).
To further simplify, we will prove the statement for a single fixed activation indicator I. Then if
Θ(I) is the set of networks for which (17) holds given I, and Θ(I) contains all networks except a
closed zero-measure set, then also TI Θ(I) contains all networks except a closed zero-measure set,
proving the lemma.
Let us hence fix I, and let k ∈ [L]. We proceed by induction. For the initial step, notice that the
matrix IkWk is just Wk with some rows replaced by zeroes. The rank of such a matrix is the same
as the matrix obtained by removing the zero rows, which has size (rank(Ik), dk-1). For all Wk
except a closed zero-measure set, this matrix has rank min {dk-1, rank(Ik)}.
For the inductive step, denote Wi := IiWi …IkWk and
ri := min {dk-1, rank(Ik), . . . , rank(Ii)}.	(18)
We assume that rank(W一)= r— and want to prove the same for i. Notice that for all Wi except
a closed zero-measure set, any ri rows ofWi are linearly independent and their span intersects with
ker(W∣-ι) only at 0. To see this, recall that by the inductive assumption, rank(W∣-ι) = ri-ι, so
ker( W ∣-ι) has dimension d%-∖ 一 ri-ι. We can concatenate any ri-subset of rows of Wi to the basis
of ker(W∣-ι) to obtain a matrix of size (r + di-ι 一 ri-ι,di-ι), which is a wide matrix, because
ri ≤ ri-1. Hence, its rows are linearly independent for all Wi except a closed zero-measure set.
14
Published as a conference paper at ICLR 2020
We now prove that rank(IiWiWi-ι) = min {rank(Wi-ι), rank(L)}，ri. The "≤"directionis
immediate. For the "≥" direction, We distinguish between two cases. If rank(Ii) ≤ rank(Wi-ι),
let v1 , . . . vri be the (linearly independent) nonzero rows of IiWi . We want to show that
{v∣Wi-ι匕 are linearly independent, i.e. that IiWiWi-ι has at least r linearly independent
rows. If pr=ι λjv|Wi-ι = 0, then P；=i λjVj ∈ ker(W∣-ι), which by assumption implies
P λjVj = 0. By the independence of {vj}, we obtain λj = 0, i.e. {v∣Wi-ι匕 are linearly
independent, and rank(IiWiWi-ι) = r%.
If rank(Ii) > rank(Wi-ι), we can reduce the problem to the case rank(L) ≤ rank(Wi-ι) by
observing that rank(IiWiWi-ι) ≥ rank(JiWiWi-ι) if Ji equals Ii only with some 1's replaced
by 0’s. We can thus take any such Ji and apply the argument from the previous paragraph to obtain
rank(IiWiW-)≥ rank(JiWiW一)≥ ri.	□
Lemma A.11. For all θ except a closed zero-measure set, the following holds. Let (l, i), (k, j) be
any units, letI be an activation indicator non-trivial up to l - 1, and let J be an activation indicator
non-trivial up to k - 1, such that (l, i, I1:l-1) 6= (k, j, J1:k-1). Then, for all scalars c ∈ R, it holds
that [wil(θ, I), bli(θ, I)] 6= c[wjk(θ, J), bjk(θ, J)].
Proof. First, we exclude from consideration all θ = (W1, b1, . . . , WL, bL) such that
e|W山-1W1-1 …IkWkej = 0 for some l, k, i, j, and some I non-trivial up to l - 1. Since
for any fixed (l, k, i,j, I), the set of θ satisfying the above is the set of roots of a non-trivial poly-
nomial in θ, it is zero-measure and closed. Because there are only finitely many configurations of
(l, k, i, j, I), we have thus excluded a closed zero-measure set of parameters. We will denote its
complement Θ*.
From now on, we assume θ ∈ Θ*. Notice that the case C = 0 of the lemma is thus automatically
satisfied, since w∣(θ, I)，e∣W1L-1W1-1 …IiWi = 0 by the definition of Θ*. In the following,
we can therefore assume c 6= 0 and treat (l, i, I) and (k, j, J) symmetrically.
Denote by Θ- ⊆ Θ* the set of parameters θ for which the lemma does not hold; we need to show
that Θ- is closed and zero-measure. We start by showing the latter property by contradiction.
Suppose Θ- is positive-measure. We know that for all θ ∈ Θ-, there exist triples (l, i, I), (k, j, J)
as stated in the lemma, and a scalar c ∈ R such that [wil(θ, I), bli(θ, I)] = c[wjk(θ, J), bjk(θ, J)]. Let
C denote the set of all triplet-pairs ((l, i, I), (k, j, J)) satisfying the conditions of the lemma; then
the previous statement can be written as
Θ- ⊆	U	{θ ∈ Θ*∣∃c ∈ R : [wi(θ,I),bi(θ,I)] = c[wk(θ, J),bj(θ,J)]}.	(19)
((l,i,I),(k,j,J))∈C
Since C is finite, there exist ((l, i, I), (k, j, J)) ∈ C for which the set under the union (call it Θ0) is
positive-measure.
We now consider two cases. If (l, i) 6= (k, j), then observe that Θ0 must contain some θ, θ0 such
that θ = (Wi,bi,.. .,WL,bL)andθ0 = (Wi, bi, .. .,Wl,bl+δei, .. .,WL,bL),whereδ 6= 0
and l ≥ k. By membership in Θ0, there exist c, c0 ∈ R such that
[wil(θ, I), bli(θ, I)] = c[wjk(θ, J), bjk(θ, J)],	(20)
[wli(θ0, I), bli(θ0, I)] =c0[wjk(θ0,J),bjk(θ0,J)].	(21)
Notice that wli, wjk do not depend on the bl [i]-component of θ, and neither does bjk because k ≤ l
and (k, j) 6= (l, i). It follows that [wjk(θ, J), bjk (θ, J)] = [wjk (θ0, J), bjk(θ0, J)] =: V. Notice also
that [wli(θ0, I), bli(θ0, I)] = [wli(θ, I), bli(θ, I) + δ]. Putting everything together, we have that
cV = [wil(θ, I), bli(θ, I)],	(22)
c0V= [wil(θ, I), bli(θ, I) + δ],	(23)
which implies (c0 - c)V = [0, δ], and in particular wjk(θ, J) = 0. This contradicts the assumption
that θ ∈ Θ* and completes the proof for the case (l, i) = (k,j).
15
Published as a conference paper at ICLR 2020
If (l, i) = (k, j), then it must be that I1:l-1 6= J1:l-1. Wlog, let (λ, ι) ∈ [l - 1] × [dλ] be such that
Iλ[ι, ι] = 1 and Jλ[ι, ι] = 0. Then there exist θ, θ0 ∈ Θ0 such that θ = (W1, b1, . . . , WL, bL) and
θ0 = (W1, b1, . . . , Wλ, bλ + δeι, . . . , WL, bL), where δ 6= 0. Then there exist c, c0 ∈ R such that
[wli(θ, I), bli(θ, I)] = c[wli(θ, J), bli(θ, J)],	(24)
[wil(θ0, I), bli(θ0, I)] =c0[wil(θ0,J),bli(θ0,J)],	(25)
where as before, wil does not depend on the bλ [ι]-component. For bli we now have bli (θ0, J) =
bli(θ, J) and b[(θ0, I) = b[(θ, I) + d, where d = δe∣W1L-1 …Wλ+ιe∣ and by membership in
Θ*, d = 0. From here, we can proceed as in the case (l,i) = (k,j), completing the proof for Θ-
being zero-measure.
Finally, we show that Θ- is closed. Let θ ∈ Θ* \ Θ-, i.e. for all ((l,i,I), (k,j,J)) ∈ C,
the vectors [wil (θ, I), bli(θ, I)] and [wjk(θ, J), bjk(θ, J)] are non-colinear. Since wli, bli, wjk, bjk are
continuous functions in θ, there exists a small enough > 0 such that [wil (θ0, I), bli (θ0, I)] and
[wjk(θ0, J), bjk(θ0, J)] are non-colinear for all θ0 ∈ B(θ) and all ((l, i, I), (k, j, J)) ∈ C. Hence,
Θ* \ Θ- is open, and Θ- is closed.	□
Lemma A.12. For all ReLU nets h : Z → R except a closed zero-measure set,
Fi(h) = {z ∈ Z | h}l(z)=0}	(26)
= z ∈ Z | hi1:l is positive and negative in the neighbourhood of z	(27)
for all units (l, i).
Proof. We provide a proof for a single unit (l, i); the extension to all units follows from the finite
number of units.
Let G, H, denote the sets defined on the right-hand sides of (26) and (27) respectively. Clearly,
H ⊆ Fil (h) ⊆ G. We will show that G ⊆ H. Let Y ⊆ R denote the set of all local optima of
the function Z → Wι[i,:] ∙ h1:l-1 (z). Due to piece-wise linearity of the function, and the finite
number of pieces, Y is finite. It follows that for all ReLU networks except a closed zero-measure
set, -bl [i] ∈/ Y. It is thus guaranteed that hi1:l never attains a local maximum or minimum at zero.
No z ∈ G can therefore be a local maximum or minimum, implying that hi1:l is both positive and
negative in the neighbourhood of z. Hence, Z ∈ H.	□
Definition A.11 (General ReLU network). A ReLU network is general if it satisfies Lemmas A.10,
A.11 and A.12.
All ReLU networks except a closed zero-measure set are general.
Lemma A.13. If h is a generalReLUnetwork, then F(h1:l) = Sd=-1 F(h1:1-1) for all (I, i).
Proof. The inclusion F(h}l) ⊆ Ud=j F(hj:l-1) follows from Lemma A.9. For the other inclu-
sion, let z ∈ F(hk:1-1) for some k ∈ [dι-ι]. Then there exist sequences of points zι(e), Z2(e) ∈
Be(z) \ Udl-11 F(h1：l-1) such that I(zι(e)) =: I and I(z2(e)) =: J are independent of e, and
▽h]"T(ZI(E)) = Vhk：l-1(z2(€)). We consider three cases based on the (non-)triviality of I and J.
First, suppose both I and J are trivial up to l - 1. Then by Lemma A.7,
Vhk:lT(ZI(E))= I1-1[k, k] WkT(I)= 0,	(28)
and similarly Vh]:1-1 (z2(e)) = 0, which contradicts Vh]:1T(ZI(E) = Vh]:1-1 (z2(e). Hence, at
least one ofI, J, must be non-trivial up to l - 1.
Second, say both I and J are non-trivial up to l _ 1. From Vh]:1T(ZI(E)) = VhkI-'(zz(e)1 it
follows that I1:1-1 6= J1:1-1, we can therefore apply Lemma A.11 to (l, i, I) and (l, i, J). We obtain
Wi1(I) 6= Wi1(J), implying Vhi1:1(Z1(E)) 6= Vhi1:1(Z2(E)). Thus, Z must be a fold-point of hi1:1.
Finally, say I is trivial up to l - 1 and J is non-trivial up to l - 1. Then Vhi1:1(Z1(E)) = Wi1(I) = 0,
whereas Lemma A.11 applied to (l, i, J) with c = 0 yields Vhi1:1(Z2(E)) = Wi1 (J) 6= 0. Hence,
Vh；:1 (ZI(E)) = VhIl(Z2(Ey) and Z must be a fold-point of hɪ:1.	□
16
Published as a conference paper at ICLR 2020
Definition A.12 (Transparent ReLU network). A ReLU network h : Z → R is called transparent up
to layer m, if for all z ∈ Z and l ∈ [m], there exists i ∈ [dl] such that hi1:l(z) ≥ 0, or in other words,
rank(Il(z)) ≥ 1. If h is transparent up to layer L - 1, we say it is transparent.
LemmaA.14. Let h : Z → R be a ReLU network, and let λ ∈ [L]. If h isgeneral, then hλ:LIint zλ-ι
is general. If h is transparent, then hλ:LIint zλ-ι is transparent.
Proof. We will abbreviate hλ:LIint zλ-ι as hλ'L. Assume h is general. Then hλ:L clearly satisfies
Lemma A.10, and for all (l, i), Wι[i,:] = 01. Next, We prove that hλ:L satisfies Lemma A.11.
Suppose this was not the case; then there exist units (λ - 1 + l, i), (λ - 1 + k, j), and non-trivial
activation indicators I = (Iλ, . . . , Iλ-1+l ), J = (Jλ, . . . , Iλ-1+k), with (l, i, I) 6= (k, j, J), and a
scalar C ∈ R such that
elWλ-1+lIλ-2+l ∙ ∙ ∙ IλWλ = C ∙ eJWλ-1+kJλ-2 + k …jλwλ,	(29)
and
λ-1+l
ei| X w
λ-1+lIλ-2+ι …Wιo+ιlιobio =	(30)
l0=λ
λ-1+k
C ∙ e| E Wλ-1+kJλ-2+k …wk0 + 1Ikobk0.	(31)
k0=λ
Then for any non-trivial indicator (I1, . . . , Iλ-1) , (J1, . . . , Jλ-1), we obtain by post-multiplying
(29),
e∣Wλ-i+ιIλ-2+1 …IiWi = C ∙ e∣Wλ-i+kJλ-2+k …J1W1,	(32)
and for all ι ∈ [λ - 1],
e∣Wλ-i+ιIλ-2+ι ∙∙∙ W∣+ιl∣b∣ = C ∙ e∣Wλ-i+k Jλ-2+k …W∣+ι J∣b∣.	(33)
The first equality means that Wi(I) = C ∙ wk(J), and the second equality implies bi (I)= C ∙ bk (J).
However, that contradicts the fact that h satisfies Lemma A.11.
The last condition of generality is Lemma A.12. Suppose hλ:L does not satisfy the lemma. Then
there exists a unit (l, i) such that
{z ∈ int Zl-i ∣ hλ(Z) =0} ⊆
{z ∈ int ZlTI hλ:1 is positive and negative in the neighbourhood of z},
i.e. there exists Z ∈ int ZlT such that hʌ:l(z) = 0, and for some e > 0 either h，：1 (Be(Zy) ⊆
(-∞, 0] or h，:l(Be(Z)) ⊆ [0, ∞). However, then there exists z0 ∈ Z such that h1:l-1 (z0) = z,
and for z0 we obtain hɪ:l(z0) = 0, and by continuity, there is δ > 0 such that either h}l(Bδ(z0)) ⊆
(-∞, 0] or h1”(Bδ(z0)) ⊆ [0, ∞). This contradicts the fact that h satisfies Lemma A.12. We have
thus shown that if h is general, then hλ:LIint zλ-ι is general.
Finally, assume h is transparent, i.e. for all Z ∈ Z and l ∈ [L - 1], there exists i ∈ [dl] such that
hii:l (Z) ≥ 0. Then also for all Z ∈ int Zλ-i and l ∈ {λ, . . . , L - 1}, there exists i ∈ [dl] such that
hʌ:l(z) ≥ 0. Hence, hλ:L is transparent.	□
Lemma A.15. a) For all ReLU networks h : Z → R and all l ∈ [L], i ∈ [dl], we have F(hii:l) ⊆
Sk∈[l-i],j ∈[dk] Fjk(h). In particular, F(h) ⊆ Sk∈[L-i],j∈[dl] Fjk(h).
b) For all general ReLU networks h : Z → R transparent up to layer l - 1, we have
F(hii:l) = k∈[l-i],j∈[d ] Fjk(h). In particular, for all general transparent ReLU networks,
F(h) = Sk∈[L-i],j∈[dk] Fjk(h).
Proof. We give a proof of b) only. A proof of a) can be obtained by replacing some equalities by
inclusions. We will prove by induction that F(hii:l) = Sk∈[l-i],j ∈[d ] Fjk(h) if h is general and
transparent up to layer l - 1. For l = 1, the function hɪ:l is linear, so F(h) = 0 and the claim holds
17
Published as a conference paper at ICLR 2020
trivially. Now assume that F(hi1:l) = Sk∈[l-1],j∈[d ] Fjk(h) holds; we will prove the same statement
for l + 1. By Lemma A.8 and Lemma A.13, we have
F(h1:l )= ({z ∈ Z | hl:l (Z) > 0} ∩ F(h1:l)) ∪ Fi(h)	(34)
=({z ∈ Z | hl:l(z) > 0} ∩ U	Fk(h)) ∪ Fli(h),	(35)
k∈[l-1],j∈[dk]
dl
F(h1a+1) = [({z ∈ Z I hl:l(Z) > 0}∩ U	Fk(h)) ∪ Fi(h	(36)
i=1	k∈[i-1],j∈[dk]
dl	dl
= ([{z ∈ Z I hl:l(z) > 0} ∩ U	Fj(h) ∪ U Fi(h).	(37)
i=1	k∈[i-1],j ∈[dk]	i=1
Since Sid=l 1 {z ∈ Z I hi1:i(z) > 0} ⊆ Z, we obtain
F(h1:i+1) ⊆	U	Fj(h) ∪ U Fj(h)=	U	Fj(h).	(38)
k∈[i-1],j∈[dk]	j∈[dl]	k∈[i],j ∈[dk]
It remains to show the reverse inclusion; we do so by contradiction.
Suppose Z ∈ Sj∈[i],j∈[dk] Fj(h) \ F(h1：i+1), or equivalently
Z ∈	U	Fj(h)	∩ (Z \	U {z ∈ Z ∣ hl:l (z) > 0})	(39)
j∈[i-1],j∈[dk ]	i∈[dl ]
=	U	Fjj(h)∩ \	{z∈ ZIhi1:i(z) ≤0}.	(40)
j∈[i-1],j∈[dk]	i∈[dl]
Because h is transparent, there exists i ∈ [di] : h}i (Z) ≥ 0, so for this i We have hi：1 (Z) = 0.
However, by Lemma A.12, this implies Z ∈ Fll(h) ⊆ F(h1l+1).	□
Lemma A.16. Let h : Z → R be a ReLU network. Then Fii+1(h) is a union of piece-wise hyper-
planes wrt. PZ (Sj∈[i],j∈[dk] Fjj (h)).
Proof. Since PZ(F(hi1:i+1)) is the partition of the input space into the linear regions of hi1:i+1, and
F(hi1:i+1) ⊆ Sj∈[i],j∈[d ] Fjj(h) by Lemma A.15, the function hi1:i+1 is also linear on the regions of
PZ (Sj∈[i],j∈[dk] Fjj(h)). For any P ∈ PZ(Sj∈[i],j∈[dk] Fjj(h)), denote the slope and bias of hi1:i+1
on P by w(P), b(P ). Then
P ∩ Fii+1(h) = {z ∈ P Iw(P)|z+ b(P) = 0	(41)
and hi1:i+1 is positive in the neighbourhood of z}.	(42)
The positivity condition guarantees that (w(P), b(P)) 6= (0, 0), so P ∩ Fii+1 is either an empty set
or a piece-wise hyperplane.	□
Corollary A.1. Let h : Z → R be a ReLU network. Then the set i∈[κ],i∈[d ] Fii(h) is a pwl. surface
of order at most κ. In particular, if h is general and transparent, then F(h) = i∈[L-1],i∈[d ] Fii (h)
is a pwl. surface of order at most L - 1.
A.4 Main result
Lemma A.17. For any bounded domain X and any architecture (d1, . . . , dL-1) with d0 ≥ d1 ≥
∙∙∙ ≥ “l-i ≥ 2, there exists a nonempty open set of transparent ReLU networks h : X → R such
that for l ∈ [L - 1],
•	dim Zi = di, and
18
Published as a conference paper at ICLR 2020
•	the set F(hl:L |intZl-1) is a pwl. surface whose dependency graph contains dl directed paths
of length (L - 1 - l) with distinct starting vertices.
Proof. We give an explicit construction; we first state it and then we prove its properties. For l = 1,
we choose the parameters (Wl, bl) as follows. Let P1 be some nonempty open convex subset of X,
and define
Wl1 := (w, b) ∈ Rdl-1 × R inf w|z + b < 0 < sup w|z + b .	(43)
z∈Pl	z∈Pl
Since Pl has at least two elements, the strict separation theorem implies that Wl1 is nonempty. It is
also open. We can therefore choose (Wl[1, :], bl [1]) from Wl1, and define
Wl2 := {(w, b) ∈ Wl1 |w|z+b > 0forallz ∈ Zl
such that Wl [1, :]z + bl[1] ≤ 0}.
(44)
The set A := z ∈ conv cl Zl | Wl [1, :]z + bl [1] ≤ 0 is nonempty, convex and compact, and by
construction there exists z ∈ Pl \ A. Again, the strict separation theorem implies Wl2 is nonempty.
It is also open by the boundedness ofA. We then choose (Wl[2, :], bl[2]) from Wl2, and define
Q∣ , {z ∈ Pl | Wι[i, :]z + bι[i] ≥ 0 forall i ∈ [∣]}.	(45)
Note that int Q2 is nonempty, open and convex. Also, there exist zι, z ∈ Q2 such that
Wl [i, :]zi + bl [i] = 0,	(46)
Wl[j, :]zi + bl[j] > 0 forj 6= i.	(47)
For i	∈ {3,	.	.	. , dl},	we	then	choose (Wl [i, :], bl [i]) as follows.	Let	A	:=	conv{z1,	.	.	.	, zi-1}
and let Z ∈	int Qi-ι	\	A.	Such Z exists because dim Qi-ι =	dim	Pl	=	dl-ι,	and	dim A ≤
i - 2 ≤ dl - 2 ≤ dl-1 - 2. By strict separation and boundedness, there exists a nonempty open
set of hyperplanes that strictly separate A and Z. Let (Wl [i, :], bl [i]) be any of them, oriented
such that Wl [i, :]zj + bl[i] > 0 for j ∈ [i - 1]. Then denote by Zi ∈ int Qi-ι any point that
satisfies Wl [i, :]zi + bl [i] = 0; it exists by convexity. Then int Qi is nonempty and convex, and the
construction for {i + 1, . . . , dl} goes through.
For layers l ∈ {2, . . . , L - 1}, we use a similar construction as for l = 1. Denote
Pl := (Wl-1Pl-1 + bl-1) ∩ {Z >0},	(48)
Pl := (Wl-IPl-1 + bl-1)∩{z ≥ 0}.	(49)
Pl is nonempty (because int Qdj is nonempty), open (because Wl-ι is wide) and convex. We
assume Wl-1 is full-rank; this holds for all choices of Wl-1 except a closed zero-measure set. By
the construction of Qdj, there exist zι,...,工由一 ∈ Qdj ⊆ Pl-ι such that or i ∈ [dl-ι],
Wl-1[i,:]Zi+bl-1[i]=0,	(50)
Wl-1[j, :]zi + bl-1[j] > 0 forj 6= i.	(51)
Denote their images z0i := Wl-1zi + bl-1. Then by openness, there exists a ball around each z0i
such that
B(z0i)	⊆	(Wl-1Pl-1	+ bl-1)	∩ {z0[j]	>0forj	6=	i},	(52)
implying that each z0i has a dl-1 -dimensional, relatively open neighbourhood B(z0i)∩{z0[i] = 0} ⊆
Pl whose elements satisfy z0[i] = 0 and z0[j] > 0 for j = i. It follows that the set
Wl1 := (w, b) ∈ Rdlj1 × R inf w|z + b < 0 < sup w|z + b, and
z∈Pl	z∈Pl
∀i ∈ [dl-ι] ∃z ∈ Pl : z[i] = 0, z[j] > 0 for j = i, and WTZ + b = 0 j (53)
contains a nonempty open subset. We can therefore choose (Wl [1, :], bl [1]) from Wl1. For the choice
of (Wl [i, :], bl [i]) for i ∈ {2, . . . , dl}, we use the same procedure as in the first layer. Finally, choose
(WL , bL ) arbitrarily.
19
Published as a conference paper at ICLR 2020
We will show that this construction satisfies the lemma. The networks are transparent because of
how we define Wl2: for all x ∈ X and l ∈ [L - 1], either h11:l (x) > 0 or h12:l (x) > 0. Also,
dim Zl = di because Zl contains int Qdl, which is nonempty and open.
Now let l ∈ [L]. We can think of the function hl:L|int Zl-1 as an (L - l + 1)-layer ReLU network
parameterised by (Wl, bl, . . . , WL, bL). Because h is transparent, also hl:L|int Zl-1 is transpar-
ent. Corollary A.1 then implies that S := F(hl:L|int Zl-1) = Sk∈[L-l],j∈[dk] Fjk(hl:L|intZl-1) is
a pwl. surface. Let S = Sk∈[L-l],j∈[n ] Hjk be its canonical representation, and let G denote its
dependency graph.
To find the required paths in G, we first identify some important vertices. For λ ∈ [L - l], denote
Z+ := {z ∈ Pl | hl"+kT(z) > 0 for k ∈ [λ - 1]}.	(54)
This set is nonempty and open because Pl+λ is nonempty and open. Next, for any unit (λ, ι),
Fλ(hl:LIint Zl-1) ∩ Z+ = {z ∈ Pl I h∣"+λ-1(z) = 0,	(55)
hl:l+k-1(z) > 0fork ∈ [λ - 1]}.
By the definition of W1+λ-1 and the fact that hl"+λ-1(Z+) = Pl+λ, the set A :=
Fλ(hl:LIint ZlT) ∩ Z+ is nonempty. Also, by the definition of {Plo}l,, h∣"+λ-1 is in fact linear
on Zλ+, so A is a hyperplane on Zλ+. Therefore there exists a piece-wise hyperplane Hjk((λλ,,ιι)) from
the canonical representation ofS that contains A; by Lemma A.11, all {Hjk((λλ,,ιι))}λ,ι are distinct from
each other.
k(λ,i)	k(λ+1,ι)
We now show that G contains the edge Hj(λ,i) → Hj(λ+1,ι) for λ ∈ [L -l - 1] and all (i, ι). By the
definition of Wl+λ, there exists Z ∈ Pl+λ such that z[i] = 0, z[j] > 0 for j = i, and Wl+λ[∣, :]z +
bl+λ[∣] = 0. There also exists a ball Be(Z) ⊆ (Wl+λ-1Pl+λ-1 + bl+λ-ι) ∩ {z[j] > 0 for j = i}.
Then because of how {Plo}l, are defined, there exists Z0 ∈ Pl such that hl"+λT(z0) = Z, so it
satisfies
hi"+λT(z0) =0,	(56)
h/kT(z0) > 0, for k ∈ [λ],(k,j) = (λ,i),	(57)
h∣"十λ(z0) =0.	(58)
It follows that z0 ∈ F?(hl:L 卜① zl-ι) ∩ Zf ⊆ H；(：：). At the same time, the preimage
(hl"+λ-1 )-1(Be(Z)) is open by continuity, and contains z0. So there exists a ball Be(z0) ⊆ Pl
such that all z0 ∈ Be(Z0) satisfy
hlj:l+k-1(z0) >0, fork∈ [λ], (k, j) 6= (λ, i).	(59)
On this ball, hi:l+?-1 is linear, so the set A := Be(Z0) ∩ {hi"+λ-1 (z0) > 0} is an open half-ball.
On A, h!la+λ is linear as well, and the set {z01 h∣"+λ(z0) = 0} intersects the center of the half-ball,
Z0. Therefore there exists a sequence of points {zn} ⊆ Pl such that Zn → Z0 and
hl:l+k-1(z0) > 0, for k ∈ [λ],	(60)
h∣"+λ(z0) =0.	(61)
0	λ+1 l:L	+	k(λ+1,ι)
We obtain that z ∈ cl(Fι	(h	Iint Zl-1 ) ∩ Zλ+1 ) ⊆ cl Hj (λ+1 ι) , which implies
Cl Hk(λ,i) ∩ Cl Hk(λ+1,1)=仍	(62)
cl Hj(λ,i) ∩ cl Hj(λ+1,ι) = ".	(62)
Itremains to show that k(λ,i) < k(λ+1,ι). Consider again the ball Be(Z0). ByLemmaA.11, h∣"十λ
is a different linear function on Be(Z0) ∩ {hi"+λ-1(z0) > 0} and on Be(z0) ∩ {hi"+λ-1(z0) < 0}.
,,
Hence, Hj (λ+1 ι) is not a piece-wise hyperplane wrt. any partition that does not include Hj (λi) .
k(λ,i)	k(λ+1,ι)
We obtain that k(λ, i) < k(λ + 1, ι), proving that G contains the edge Hj(λ,,i) → Hj(λ+1,,ι) .
k(1,i)	k(2,1)	k(L-l,1)
Finally, observe that the dl paths	^Hj(I	i)	→	^hj(2	1)	∙ ∙ ∙ ∙ →	^Hj(L	l 1)have length	(L	1	1),
and distinct starting vertices. This proves the theorem.	□
20
Published as a conference paper at ICLR 2020
Lemma A.18. For all general ReLU networks h : Z → R, the following holds. Denote S =
l∈[λ],i∈[d ] Fil(h) and let S = k∈[κ],j∈[n ] Hjk be the canonical representation of S. Then Hjk ⊆
Fil(h) for some (l, i) with l ≥ k. Moreover, if the dependency graph of S contains a directed path of
length m starting at Hjk, then l ≤ λ - m.
Proof. Because the representation is canonical, we have
Hjk 6⊆ k-1S ⊇	[	Fil(h),	(63)
l∈[k-1],i∈[dl]
which implies Hjk ⊆ Sl≥k,i Fil(h). By piece-wise linearity, we can write
U	Fi(h)	= U	Fi(h)	∩ P = U	{z	∈ P |	Wli(P)	∙ Z + bi (P )=0},	(64)
l≥k,i	l≥k,i,P	l≥k,i,P
where P runs over the linear regions of hi1:l. Moreover, by the definition of Fil (h), all wil (P) 6= 0.
Combined with Lemma A.11, we obtain that each nonempty set on the right-hand side of (64) is a
different hyperplane on an open set. Therefore there exists one for which Hjk ⊆ Fil(h) ∩P ⊆ Fil(h).
Now assume that the dependency graph of S contains a directed path of length m starting at Hjk =:
Hk0; denote the path Hfk° → H；1 → ∙∙∙ → Hkm. By the first part	of the	lemma, We know	that
Hjkι ⊆ Filι (h) for some lι. Let ι ∈ [m]; we will show that lι-1 < lι.
Because of the edge Hjkι-1 → Hjkι, we know that Hjkι-1 and cl Hjkι	intersect,	and that Hjkι	is	a
jι-1	jι	jι-1	jι	jι
piece-wise hyperplane wrt. some partition P for which HkI-I is a boundary. Let Z be any point
of intersection. By openness, there exists a ball Be(Z) such that Hk：—； is a hyperplane on Be(Z),
and H; is a hyperplane on one half-ball defined by Be(Z) and Hk：-；. If it was the case that
l∣-ι ≥ l∣, then Fl： (h) would be a hyperplane on Be(Z), i.e. there would have to exist some piece-
kι 1
wise hyperplane on the opposite half-ball as Hj ι-1 , but included in the same hyperplane. However,
by Lemma A.11, no two piece-wise hyperplanes in S are included in a single hyperplane, so we get
a contradiction.
Hence, we obtain l0 < lι < •…< lm ≤ λ, which yields lo ≤ λ - m.	□
Lemma A.19. Let (w, b), (c, a) ∈ Rd × R and let F ⊆ Rd with dim F = d - 1. If w|Z + b = 0
and c|Z + a = 0 for all Z ∈ F, then either (w, b) = (0, 0), (c, a) = (0, 0), or there exists
β ∈ R : (c, a) = β (w, b).
Proof. Since dimF = d - 1, there exist d affinely independent vectors f0, . . . , fd-1 in F. Hence
there are d - 1 linearly independent vectors v1 := f1 - f0, . . . , vd-1 := fd-1 - f0, such that
w|vi = c|vi = 0. In other words, both w and c lie in the orthogonal complement of the span
of v1, . . . , vd-1. If w = 0, then necessarily b = 0, and similarly for (c, a). If w 6= 0 6= c, then
because the orthogonal complement is one-dimensional, there exists β ∈ R such that c = βw. Then
CTZ + a — β(wlZ + b) = a — βb = 0 and the lemma follows.	□
Theorem A.1. Consider a bounded domain X and any architecture (d1, . . . , dL-1) with d0 ≥ d1 ≥
∙∙∙ ≥ dL-ι ≥ 2. Let hθ : X → R be a general ReLU network satisfying Lemma A.17, and let
hη : X → R be any general ReLU network such that hθ (x) = hη (x) for all x ∈ X. Denote
η , (W10,b01,...,WL0 ,b0L). Then there exist permutation matrices P1, . . . PL-1, and positive-
entry diagonal matrices M1, . . . , ML-1, such that
W1	M1P1W10 ,	b1	M1P1b01
Wl	MlPlWl0Pl--11Ml--11,	bl	MlPlb0l,
WL	WL0 PL--11ML--11,	bL	b0L.
l ∈ {2,...,L- 1},	(65)
21
Published as a conference paper at ICLR 2020
Proof. First, notice that hη is transparent. To see this, observe that hθ is transparent, i.e.
rank(lθ(x)) ≥ 1 for all l ∈ [L 一 1] and X ∈ X. By Lemma A.10, Vχhη(x) = Vχhθ(x) = 01,
implying that hη is transparent.
We proceed by induction. Let l = 1. Then we have
hθLlint Zθ^1 ≡ hθ ≡ hη ≡ hηLlint Zθ^1	(66)
which implies F(hθL∣atzi-ι) = F(hηL∣int z-ι). (For notational convenience, We will omit the
domain restriction for now.) Because both networks are general and transparent, Corollary A.1
implies that the set
U	Fk (hθL) = F(hθL) = F(hlηL)= U	Fj(%L	(67)
k∈[L-l],j ∈[dk]	k∈[L-l],j∈[dk]
is a pwl. surface of order at most L-l. By Lemma A.17, its graph contains dι directed paths oflength
(L 一 1 一 l) with distinct starting vertices. Denote these vertices H1, . . . , Hdl . By Lemma A.18,
Hi ⊆ Fλ(hθL) for some (λ, ι) with λ ≤ (L 一 l) 一 (L 一 1 一 l) = 1. We thus obtain Ui∈[d] Hi ⊆
Ui∈[d] FlML), where on the left-hand side we have a union of dι hyperplanes, and on the right-
hand side we have a union of at most dι hyperplanes. It follows that Ui∈[d匕]Hi = Ui∈[d] F1 (hθL),
and by applying the same argument to hη, we get Ui∈[d] F1 (hθL) = Ui∈[d匕]F1(hηL). Therefore
there must exist a permutation π : [dι] → [dι] such that F1(hθL)= 叫⑴⑺2工)for all i. Then by
Lemma A.19, there exist scalars m1, . . . mdl, such that
(Wι[i, ：], bι[i]) = mi(WO [π(i), [bι[n(i)]).	(68)
We know that mi = 0 because the folds F1 (%L), F1(hηL), are nonempty; otherwise Ui∈[d] Hi
could not be a union of dι hyperplanes. We have thus shown that there exists a permutation matrix
Pι ∈ Rdl ×dl and a nonzero-entry diagonal matrix Mι ∈ Rdl ×dl such that Wι = Mι Pι Wι0 and
bι = MιPιb0ι.
Next, we show that the diagonal entries of Mι are positive. Let z-, z+ ∈ int Zιθ-1 be such that
Iθ(z-) and Iθ(z+) differ only in Iιθ[i, i]. Wlog, let Iιθ[i, i](z-) = 0 and Iιθ[i, i](z+) = 1, and
denote the row span ofIιθ(z-)Wι by W. Then
VzhθL(z-) = WLm …Wι+ιlθ(z-)Wι ∈ W,
VzhθL(z+) = WLIL-ι(z+) ∙∙∙ Wι+1 Iθ (z+)Wι ∈ Span(W ∪ Wι[i, ：]).
Since hθ is general and dι ≤ dι-1, the matrix Wι has full row rank. This means that the rows ofWι
form a basis, in which the representation of VzhθL(z-) has one more zero coefficient compared
to VzhθL(z+). In other words, for two points z-, z+ ∈ int Zj1 whose indicators differ only in
Iθ [i, i], the point for which I? [i, i](z) = 0 is also the one for which VzhθL(z) has more zero coeffi-
cients when expressed in the row basis ofWι. Now, observe that ifIθ(z-) and Iθ(z+) differ only in
Iιθ[i, i], then Iη(z-) andIη(z+) differ only in Iιη[π(i), π(i)]. Because Wι = MιPιWι0, the number
of zero coefficients of VZhjL(z-) = VZh?L(z-) in the row basis of WI is the same as the number
of zero coefficients of Vzh?L(z-) in the row basis of Wι. It follows that In[π(i),π(i)](z-) = 0
and Iιη[π(i), π(i)](z+) = 1. Hence, mi is positive.
For the inductive step, let l ∈ {2, . . . , L 一 1}, and assume that there exist permutation matrices
P1, . . . , Pι-1, and positive-entry diagonal matrices M1, . . . , Mι-1, such that (65) holds up to layer
l 一 1. Then h?"T ≡ Mι-ιPι-ιhn"T. Since h?:L ≡ h*L, it follows that
hθLIint Zθ-1 ≡ (hnL ◦ P-IIM-1l) Lt Zθ-1 ≡ hnLlint ZL	(69)
where η := (W0P-IIM-1「bj, W0+ι,耳十],…，WL, bL). We can therefore apply the same ar-
gument to h?L∣int zi-ι and 九号工黑工-as we presented above for the case l = 1. We obtain that
there exists a permutation matrix Pι ∈ Rdl ×dl and a positive-entry diagonal matrix Mι ∈ Rdl ×dl
such that
Wι =MιPιWι0Pι--11Mι--11,	bι =MιPιb0ι.	(70)
22
Published as a conference paper at ICLR 2020
Finally, consider the last layer. We know that h；LT ≡ ML-IPL-ιh*Lτ, which implies hθ ≡
hηL ◦ PL--1 1ML--1 1, i.e. hθL and hηL ◦ PL--1 1ML--1 1 are identical linear functions supported on the
full-dimensional domain ZLT. It follows that WL = WLP--ιM--ι and bL = b：.	□
23