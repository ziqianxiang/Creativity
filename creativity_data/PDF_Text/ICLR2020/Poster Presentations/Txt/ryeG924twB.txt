Published as a conference paper at ICLR 2020
Learning Expensive Coordination:
An Event-Based Deep RL Approach
Runsheng Yu； Xinrun Wang； Rundong Wang, Youzhi Zhang, & Bo Ant
School of Computer Science and Engineering,
Nanyang Technological University,
Singapore
runshengyu@gmail.com,
{xwang033,rundong001,yzhang137}@e.ntu.edu.sg,
boan@ntu.edu.sg
ZhenYu Shi；& Hanjiang Lait
School of Data and Computer Science,
Sun Yat-sen University
Guangzhou, China
shizhy6@mail2.sysu.edu.cn, laihanj3@mail.sysu.edu.cn
Ab stract
Existing works in deep Multi-Agent Reinforcement Learning (MARL) mainly fo-
cus on coordinating cooperative agents to complete certain tasks jointly. However,
in many cases of the real world, agents are self-interested such as employees in
a company and clubs in a league. Therefore, the leader, i.e., the manager of the
company or the league, needs to provide bonuses to followers for efficient coordi-
nation, which we call expensive coordination. The main difficulties of expensive
coordination are that i) the leader has to consider the long-term effect and predict
the followers’ behaviors when assigning bonuses, and ii) the complex interactions
between followers make the training process hard to converge, especially when
the leader’s policy changes with time. In this work, we address this problem
through an event-based deep RL approach. Our main contributions are threefold.
(1) We model the leader’s decision-making process as a semi-Markov Decision
Process and propose a novel multi-agent event-based policy gradient to learn the
leader’s long-term policy. (2) We exploit the leader-follower consistency scheme
to design a follower-aware module and a follower-specific attention module to
predict the followers’ behaviors and make accurate response to their behaviors.
(3) We propose an action abstraction-based policy gradient algorithm to reduce
the followers’ decision space and thus accelerate the training process of follow-
ers. Experiments in resource collections, navigation, and the predator-prey game
reveal that our approach outperforms the state-of-the-art methods dramatically.
1	Introduction
Deep Multi-Agent Reinforcement Learning (MARL) has been widely used in coordinating cooper-
ative agents to jointly complete certain tasks where the agent is assumed to be selfless (fully coop-
erative), i.e., the agent is willing to sacrifice itself to maximize the team reward. However, in many
cases of the real world, the agents are self-interested, such as taxi drivers in a taxi company (fleets)
and clubs in a league. For instance, in the example of taxi fleets (Miao et al., 2016), drivers may pre-
fer to stay in the area with high customer demand to gain more reward. It is unfair and not efficient
to compel the taxi driver to selflessly contribute to the company, e.g., to stay in the low customer
demand area. Forcing the drivers to selflessly contribute may increase the income for the company
in a short-term but it will finally causes the low efficient and unsustainable of that company in the
* Indicates equal contribution.
,Co-corresponding authors.
1
Published as a conference paper at ICLR 2020
long run because the unsatisfied drivers may be demotivated and even leave the company. Another
important example is that the government wants some companies to invest on the poverty area to
achieve the fairness of the society, which may inevitably reduce the profits of companies. Similar to
previous example, the companies may leave when the government forces them to invest. A better
way to achieve coordination among followers and achieve the leader’s goals is that the manager of
the company or the government needs to provide bonuses to followers, like the taxi company pays
extra bonuses for serving the customers in rural areas and the government provides subsidies for
investing in the poverty areas, which we term as expensive coordination. In this paper, we solve the
large-scale sequential expensive coordination problem with a novel RL training scheme.
There are several lines of works related to the expensive coordination problem, including mecha-
nism design (Nisan & Ronen, 2001) and the principal-agent model (Laffont & Martimort, 2009).
However, these works focus more on static decisions (each agent only makes a single decision). To
consider sequential decisions, the leader-follower MDP game (Sabbadin & Viet, 2013; 2016) and the
RL-based mechanism design (Tang, 2017; Shen et al., 2017) are introduced but most of their works
only focus on matrix games or small-scale Markov games, which cannot be applied to the case with
the large-scale action or state space. The most related work is M3RL (Shu & Tian, 2019) where
the leader assigns goals and bonuses by using a simple attention mechanism (summing/averaging
the features together) and mind (behaviors) tracking to predict the followers’ behaviors and makes
response to the followers’ behaviors. But they only consider the rule-based followers, i.e., followers
with fixed preference, and ignore the followers’ behaviors responding to the leader’s policy, which
significantly simplifies the problem and leads the unreasonability of the model.
In the expensive coordination problem, there are two critical issues which should be considered: 1)
the leader’s long-term decision process where the leader has to consider both the long-term effect
of itself and long-term behaviors of the followers when determining his action to incentivise the
coordination among followers, which is not considered in (Sabbadin & Viet, 2013; Mguni et al.,
2019); and 2) the complex interactions between the leader and followers where the followers will
adapt their policies to maximize their own utility given the leader’s policy, which makes the training
process unstable and hard, if not unable, to converge in large-scale environment, especially when
the leader changes his actions frequently, which is ignored by (Tharakunnel & Bhattacharyya, 2007;
Shu & Tian, 2019). In this work, we address these two issues in the expensive coordination problem
through an abstraction-based deep RL approach.
Our main contributions are threefold. (1) We model the leader’s decision-making process as a semi-
Markov Decision Process (semi-MDP) and propose a novel event-based policy gradient to learn
the leader’s policy considering the long-term effect (leader takes actions at important points rather
than at each step to avoid myopic decisions.) (Section 4.1). (2) A well-performing leader’s policy
is also highly dependent on how well the leader knows the followers. To predict the followers’
behaviors precisely, we show the leader-follower consistency scheme. Based on the scheme, the
follower-aware module, the follower-specific attention module, and the sequential decision module
are proposed to capture these followers’ behaviors and make accurate response to their behaviors
(Section 4.2). (3) To accelerate the training process, we propose an action abstraction-based policy
gradient algorithm for the followers. This approach is able to reduce followers’ decision space and
thus simplifies the interaction between the leader and followers as well as accelerates the training
process of followers (Section 4.3). Experiments in resource collections, navigation and predator-
prey show that our method outperforms the state-of-the-art methods dramatically.
2	Related Works
Our works are closely related to leader-follower RL, temporal abstraction RL, and event-based RL.
Leader-follower RL. The leader-follower RL targets at addressing the issue of expensive coordina-
tion where the leader wants to maximize the social benefit (or the leader’s self-benefit) by coor-
dinating non-cooperative followers through providing them bonuses. Previous works have investi-
gated different approaches to solve the expensive coordination, including the vanilla leader-follower
MARL (Sabbadin & Viet, 2013; LaUmonier & Chaib-draa, 2005), leader Semi-MDP (TharakUnnel
& Bhattacharyya, 2007), multiple followers and sub-followers MARL (Cheng et al., 2017), follow-
ers abstraction (Sabbadin & Viet, 2016), and Bayesian optimization (MgUni et al., 2019). BUt most
of them focUs on simple tabUlar games or small-scale Markov games. The most related work (ShU
2
Published as a conference paper at ICLR 2020
Figure 1: Overview of our framework. The details of the leader’s module and the follower’s module
can be found in Section 4.2 and Section 4.3, respectively. The implement details of each module can
be found in Appendix D.2.1.
& Tian, 2019) leverages the deep RL approach to compute the leader’s policy of assigning goals
and bonuses to rule-based followers. But their method performs poorly when the followers are RL-
based. In this work, we aim to compute the leader’s policy against the RL-based followers in the
complex and sequential scenarios.
Temporal abstraction RL. Our methods are also related to temporal abstraction method (Sutton et al.,
1998; Daniel et al., 2016; Bacon et al., 2017; Smith et al., 2018; Zhang & Whiteson, 2019; Vezhn-
evets et al., 2016). The basic idea of temporal abstraction is to divide the original one-level decision
process into a two-level decision process where the high-level part is to decide the meta goal while
the low-level policy is to select the primitive actions. Our leader’s decision process is different from
those methods mentioned above because the leader’s policy can naturally form as an intermittent
(temporal abstraction) decision process (semi-MDP) (Tharakunnel & Bhattacharyya, 2007) and it
is unnecessary to design the two-level decision process for the leader (since the low-level decision
process is the follower). Based on the nature of the leader, a novel training method is introduced.
Event-based RL & Planning. Previous studies also focus on using events to capture important
elements (e.g., whether agent reaches a goal) during the whole episode. Upadhyay et al. (2018)
regard the leader’s action and the environment feedback as events in the continuous time environ-
ment. Becker et al. (2004); Gupta et al. (2018) leverage events to capture the fact that an agent
has accomplished some goals. We adopt this idea by depicting the event as the actions taken by
the leader at some time steps and design a novel event-based policy gradient to learn the long-term
leader’s policy.
3	Stackelberg Markov Games
Our research focuses on single-leader multi-follower Stackelberg Markov Games (SMG) (Mguni
et al., 2019; Sabbadin & Viet, 2013), which can be formulated as a tuple G =〈N, S, A, Ω, P, R, Y).
N is the set of N followers, i.e., |N| = N. S is the set of states. s0 ∈ S0 ⊂ S is an initial state and
S0 is the set of initial states. A = ×k∈N Ak is the set of joint actions for followers where ak ∈ Ak is
an action for the k-th follower. ω ∈ Ω = X k∈NΩk is an action for the leader and ωk = {gk, bk} ∈
Ωk is a goal and a bonus that the leader assigns to the k-th follower. P : S × A → ∆(S) is the
transition function1 and R = ×k∈Nrk × rl is the reward function set where rk : S × A × Ω → R
is the reward function for the k-th follower and rl : S × A × Ω → R is the reward function for the
leader. γ is the discount factor and a is a joint action of followers.
The leader,s policy is defined as μ = (μkik∈N where μk : Ω × S → ∆(Ωk) is the leader,s action
to the k-th follower given the leader’s action in the previous timestep ωt-1 and the current state
st. ∆(∙) is a probability distribution. The followers, joint policy is defined as π =(nk〉where
πk : Ωk × S → ∆(Ak) is the k-th follower policy given the leader,s action ωk and the current state
st. Given the policy profile of the leader and followers (μ, π)，the follower,s utility is defined as
Jk(μ, ∏) = ElPT=O Ytrk (St, Qt, ωt)] and the leader,s utility is J(μ, π) = EIPT=O Ytrl (St, Qt, ωt)].
We assume that the leader and followers aim to maximize their own utilities. We define the trajectory
τ as a sequence of state, leader,s action, and followers, actions hω-1, (st, at, ωt)tT=0i where ω-1 is
the first step leader,s action and is set to zero.
1Notice that the transition function does not depend on the leader,s action.
3
Published as a conference paper at ICLR 2020
◎ Commit new action ■ Maintain previous action
Time step
For Follower I
For Follower II
(a) A simple example for the illustration of AT . Sup-
pose that the whole step is 4, the AT = {eI1 =
h0, ω0I i, eI1I = h1, ω1IIi, eI2I = h3, ω3IIi}.
(b) The probabilistic graphical model of the pro-
posed framework. Dotted line means that β affects
the final result of ω indirectly. ω-1 is set to be zero.
Figure 2: An example and a probabilistic graphical model to illustrate our method.
4	Methodology
In this section, we propose a novel training scheme to train a well-performing leader policy against
both rule-based and RL-based followers in the expensive coordination problem. We address the
two issues, the leader’s long-term decision process and the complex interactions between the leader
and followers, with three key steps: (a) we model the leader’s decision-making process as a semi-
Markov Decision Process (semi-MDP) and propose a novel event-based policy gradient to take
actions only at important time steps to avoid myopic policy; (b) to accurately predict followers’ be-
haviors, we construct a follower-aware module based on the leader-follower consistency, including a
novel follower-specific attention mechanism, and a sequential decision module to predict followers’
behaviors precisely and make accurate response to these behaviors; and (c) an action abstraction-
based policy gradient method for followers is proposed to simplify the decision process for the
followers and thus simplify the interaction between leader and followers, and accelerate the conver-
gence of the training process.
4.1	Event-Based Trajectory Optimization for Leader
We first describe the event-based trajectory optimization for the leader. As we mentioned
above, the leader’s decision process can be naturally formulated as a semi-MDP (Tharakun-
nel & Bhattacharyya, 2007). Therefore, we firstly describe the basic ideas of semi-
MDP using the modified option structure. We define the modified option as a tuple:
hμ, (βk)k∈Ni where μ is the leader's policy as We defined above and βk(St,ωt-1) : S X
Ω → [0,1] is the termination function for the k-th follower, to indicate the Probabil-
ity whether the leader’s action to the k-th follower changes (ωtk-1 6= ωtk). Based on
these definitions, we formulate the one-step option-state transition function with decay as:
PY (st+ι, ωt, at®, ωt—i) = YP (st+ι|st, at) ∏ (at®, ωt) QR。一 βk (St, ωt-i))此之1 =ωk +
βk (st, ωt-1) μk (ωk∣st, ωt-1)}, where 1 is the indicator function and π (at∣st, ωt)	=
口k∈N πk (ak ∣st, ωk) is the joint policy for followers. Notice that this is an extension of the aug-
mented process mentioned in (Bacon et al., 2017). Differently, we do not have the low-level policy
here (the low-level policy is the follower) and since we only focus on the finite time horizon, γ is
set to be 1. Our modified option is used to depict the long-term decision process for the leader as
shown in Fig. 2.
Now we start to discuss our leader’s policy gradient. In fact, it is not easy to directly optimize
the leader’s utility based on this multi-agent option-state transition function since this form in-
cludes leader’s different action stages to different followers. Notice that for a sampled trajec-
tory, the occurrence of the leader actions is deterministic. Therefore, we can regard the time
step and the action the leader takes at that step as an event and define the (universal) event set
UT = {hti,ωtk i|ti ≤ T, k ∈ N}. We use the notation eik = hti,ωtk i to represent the leader’s action
to the k-th follower at step ti, i is the index of the event. Since we focus on the change of the actions
from the leader, we further define a set that represents a collection of new actions (ωtk 6= ωtk-1)
taken by the leader within that trajectory: AT = {ek ∣ωtk = ωtk-1,ti ≤ T, k ∈ N} ⊆ UT, where
ti - 1 is the previous time step. AT represents when and how the leader commits to a new action
(an example can be found in Fig. 2a). For brevity, ejk 6∈ AT means ejk ∈ UT \AT . The probability of
4
Published as a conference paper at ICLR 2020
AT can be represented as:
P (AT )= Yk∈N Yek ∈Ατ βk(sti，ωti-1)μk (ωki lsti Mt—) Ye* (1 - βk H)，
where tj - 1 is the previous time step for tj . This equation illustrates that the probability of the
occurrence ofa certain leader’s event set within a trajectory. Concretely, the leader changes action to
the k-th follower at ti ∈ eik while maintaining the same action within the interval from ti - 1 ∈ eik-1
to ti (s.t., ti ∈ eik). Similarly, we can further define the probability of the whole trajectory τ as:
P(τ) = P(s0)Yk∈N {[Yek∈Aτ βk(sti,ωti-ι)μk (ωki^ti,ωt-) ∏k 体|s"")] ×
Yek6∈AT (1 - βk(stj,ωtj-1))πk (atkj |stj, ωtkj )	YtT=0P(st+1|st,at).
Comparing with P(AT), P(τ) includes the probability of the followers as well as the state transition.
Do note that our goal is to maximize maxAT EP (τ) [Rτ (T)] , indicating that the leader is required
to select an action that can maximize the accumulated reward, where Rτ (T ) = PtT=0 γtrtl is the
accumulated reward and τ is to stress that its accumulated reward is from the trajectory τ . Following
the REINFORCE trick (Sutton & Barto, 1998), the policy gradient for the termination function and
the leader’s policy function can be formulated under the following proposition:
Proposition 1. The policy gradients for the termination function βk (sti , ωti ) and leader’s policy
function μk (ωkk ∣s%, ω%-ι) can be written as:
Vθ J (θ) ≈ Efτ(∙) {[Xk∈N XT=0 I (e )i RT (T)} ； ▽"⑼ ≈ EipT(∙) {[Xk∈N Xi(ek)i Rτ(T)};
where θ and H are the parameters for the termination function βk and leader's policy μ3. I (∙) and
10( ∙) are the piece-wise functions:
{-▽g βk (Sti ,ωti-1 )	k Λ
1-βk(sti ,ωti-ι )	ei	∈ T,	I 0(	k∖ _ ʃv^ log μk	(ωki |sti ,	ωti-l)	ek	∈	AT,
▽。卜,3ti-?	ek ∈ AT.	& )=10	ek ∈ AT.
βk(sti ,ωti-l)	i ∈ T
All the proofs can be found in Appendix A. Proposition 1 implies that under the event-based method,
whether the leader’s commitment to a new action will induce different policy gradients for both
termination function and the policy function.
However, from the empirical results, we find that the leader’s policy function updates rarely during
the whole episode because the policy only updates when the leader commits to a new action, which
causes the sample inefficiency. Notice that in fact the leader commits to the same action when
ek / at. Therefore, the policy indication function 10(∙) can be formulated in an alternative way:
Mek) = VO logμk (ωki |sti, ωti-ι) ,ek / at； Jμk (ωki = ""/st, ω%-ι) ,ek / at. This
form considers both committing to a new action and maintaining the same actions (Details can be
found in Remark 2), which we call the Event-Based Policy Gradient (EBPG) and the previous one
as the sparse EBPG respectively.
Intuitively, the dense EBPG is better than the sparse EBPG because it updates the leader’s policy
function more frequently than the sparse one. For example, in time step t, supposing that the leader
chooses a wrong action for follower k and receives a negative reward. Then, the leader should
learn to diminish the action chosen that state by EBPG. The sparse EBPG only do one PG during
before terminating the action (at the committing action step) while the dense one does PG in each
step before terminating the action. The latter can provide more signal to correct the wrong action.
Experiments also reveal that the dense one is better (Sec. D.3.3).
4.2	Neural Network based Leader
The EBPG approach is able to improve leader’s performance. However, it is still very hard for
the leader to choose actions considering long-term effect only based on the current state informa-
tion. This is because the followers change their behaviors over time according to the leader’s policy.
5
Published as a conference paper at ICLR 2020
Therefore, we introduce new modules and training schemes so as to capture the change of the fol-
lowers’ behaviors as well as the global state. To abstract the complicated state information, we
use neural networks to learn the state representation. To capture the followers’ behaviors and make
accurate response to their behaviors, we design three modules: (1) we exploit the leader-follower
consistency under game regularization and policy bound conditions, (2) based on the consistency, a
follower-aware module is introduced and (3) based on the follower-aware module, a novel attention
mechanism, and sequential decision making module is designed to make accurate response to these
followers’ behaviors as shown in Fig. 1.
Leader-Follower Consistency. In previous works, a surge of researches focus on predicting other
agents’ behaviors through historical information, where the other agents are assumed to be oppo-
nents of that agent, which is only suitable for zero-sum games (Zheng et al., 2018; Foerster et al.,
2018; He et al., 2016). However, these methods cannot be directly applied to our case because SMG
is not zero-sum. We note that Shu & Tian (2019) attempt to directly use the followers’ behavior
prediction module (use the history of the followers to predict their future actions) but do not analyze
when and how it works. To ensure that the leader can predict the followers’ behaviors, we introduce
the following assumptions.
Assumption 1. (Game regularization) The leader-follower state-action space (AX Ω XS) is com-
pact and rk is a ContinuousfUnCtion w.r.t. μ bounded by RmaX.
This assumption is inspired by (Antos et al., 2008). We only extend it into the multi-agent forms.
This assumption indicates that the action and states space should be limited and the reward function
for the leader action should be smooth.
Assumption 2. (Policy Bound) For any agent k, reward function rk and policy is consistency, i.e.,
| Jk([πk, π-k] , μ) - Jk([πk, π-k] , μ0)∣ ≥ C2∣πk - π0k∣
Where C2 is a constant that satisfies C2 > 0. π0k is the k-thfollower，s new policy. μ0 is the leader's
new policy.
This assumption is inspired by (Mguni et al., 2019). π-k indicates the joint policy without the k-th
agent’s. This assumption indicates that the change of the leader causes only slightly changes on each
followers policy.
Based on these two assumptions, we propose a proposition here:
Proposition 2. (Leader-Follower Consistency.) If both the assumptions of game regularization
and policy bound are satisfied, for ∀e > 0, k ∈ N, there exists δ > 0, such that ∣μ 一 μ0∣ ≤ E
implies Ink 一 π0k∣ ≤ δ, where μ0 and π0k are the new policies for the leader and the k-th follower
respectively.
This proposition reveals that the change of the leader causes only slightly changes on each follower’s
policy under the game regularization assumption and the policy bound assumption, which is fun-
damental to follower-aware learning. Roughly speaking, the game regularization requires that the
states, actions, and rewards are bounded while the policy bound states that a little change of a fol-
lower’s policy does not change its utility so much. The former is from the game itself and we only
focus on the latter. To satisfy the latter, one possible method is to make the μ and μ0 close because
the followers always find the best response to the leader’s policy and if the leader changes a little,
the followers do not change too much since the new best response to μ0 is not far away from best
response to μ. One direct method is to slow down the learning rate of the leader to make μ0 and μ
close. Moreover, for the leader part, taking the right actions is also an important way to guarantee
the second assumption because the taken action will more precisely decrease the probability of huge
change of the whole process and stabilize the training process. There is an interesting phenomenon
that on one hand, knowing more about the followers can diminish the wrong decision and thus aids
the establishment of the consistency. On the other hand, the consistency will further guarantee the
accuracy of the follower-aware module. Therefore, they form a positive feedback.
Follower-Aware Module. Based on the leader-follower consistency, we can safely implement the
follower-aware module to our network. Before we discuss this module in details, we first define
the history for both the leader and followers. For the k-th follower, its history at time step t is a
sequence of states, its own actions, and the leader’s actions to it, i.e., htk = h(st0, atk0, ωtk0)t0≤ti ∈ Htk
6
Published as a conference paper at ICLR 2020
while the leader’s history is the stack of all followers histories ht = hhtkik∈N ∈ Htl . Then,
Wedefinethehistory-based leader's policy as: μht(St) = Z-1p(ωf |st, at, ht)p(at∣st, ht) H
p(ωk |st, at, ht) Qkpk (^k |st, hk), where Z is the normalization term, pk is the predicted action
probability of the k-th follower and a defines the predicted action (predicted by the leader). P is
to stress the output is a probability. Since we cannot directly obtain an accurate estimation ofak,
we adopt an alternative way to leverage history information and imitation learning to make a pre-
diction of other agents, action probability function pk (^f |st, hf) (Implementation details can be
found in Appendix B) andp(ωk |st, at, ht) is designed using the attention mechanism as well as the
sequential decision module presented below.
Follower-Specified Attention Mechanism. Inspired by (Chen et al., 2018), we introduce a
follower-specific attention mechanism to identify the important followers where the important fol-
lowers are followers who has just finished a task and the leader has to commit new actions to these
followers. The attention mechanism is as follows:
exp f (A (st,^k, hk)))	.
Pk∈N exP f (A (st, αk,hk)));
Ct = ∑k∈N. WkA (st, ^k, hk) ； ck = [ct,A (st, ak, hk)].
Where wk is the weight of the k-th follower, A(∙) : Rds×ak× ×hkt → RdC is a function to blend
various information of an agent together (dc means the dimension of the output of the A(∙)), f (∙):
RdC → R1 is a function to map the blending information to a real number, and ctk is the k-th
agent attention value (the compression of history, states and actions for follower k as well as other
followers). ak is the output of pk (^k |st, hk), the predicted the k-th follower,s action. This attention
mechanism is better because it quantifies the importance of each follower in each state through
learning while the original methods only adds/averages all the features (st, ak,hf) together (ShU
& Tian, 2019). Another advantage is that its weights can be visualized to see which follower is
important to the leader at current step. (Details can be found in Appendices D.2.1 & D.3.4). ctk then
is used by βk and μk.
Sequentially Determining Goals and Bonuses. Also notice that the goal and the bonus are sequen-
tially correlated. Therefore, it is better for the leader to choose the bonus and the goal sequentially
rather than select them independently. Therefore, to consider the goal and bonus jointly when mak-
ing a decision, we build a probabilistic graph-based model as: p(ωk |st, at, ht) ≈ p(gk; bk|ck) h
p(btk|gtk, ctk ) × p(gtk |ctk ), the first approximate equation is established because ctk is the compression
of (st, at, ht). p(bk∣gk, ck) andp(gk|ck) means the policy for bonuses and goals (Implementation
details can be found in Appendix D.2.1).
4.3	Follower Action Abstraction Policy Gradient
These methods mentioned above are fully implemented can enhance the performance dramatically.
But when facing the RL-based followers, the SMG is still hard to converge. This is because in SMG,
the policies of the leader and followers are always changing depending on other agents, performance.
To guarantee convergence, the leader can only update its policy when the followers reach (or are near
to) the best response policy (Fiez et al., 2019). However, when the followers are RL-based agents,
there is no way to ensure the followers, policies are (near) the best response policies in large-scale
SMG and the commonly-seen idea is to provide enough training time but it is unbearable in practice
due to the limitation of computing power (Mguni et al., 2019).
To accelerate the training process, inspired by the action abstraction approach which is commonly-
seen in Poker (Brown & Sandholm, 2019; Tuyls et al., 2018) and action abstraction RL (Chandak
et al., 2019), we collect the followers, primitive actions sharing the same properties together as a
meta policy. Then, the followers only need to select the meta action to make a decision. Therefore,
the original game is converted into a meta game, which is easy to solve.
Specifically, we define the policy for the k-th follower as:	∏k (ak|s)	=
Pz πmeta(zls)πkοwer (α惶 z), where s = hs, ωki is the augmented state for the follower (the
combination of current state and the leader,s action). nk,eta(zls) is the meta policy for the k-th
follower and z is the high-level (meta) action. We hypothesize that the lower-level policy (the
policy to choose the primitive actions) is already known (rule-based) and deterministic, i.e.,
∏kower (αk∣s, z) = 1. For instance, given the example of the navigation task, the ∏keta can be the
selection to which landmark to explore while πlkower is a specific route planning algorithm (such
7
Published as a conference paper at ICLR 2020
----Ours
-Ours w/o EBPG
-Ours w/o Attention
M3RL
o sβooo iooooo ωβooo 200000	2s0000
Episodes
(a) Resource Collections.
Ours
Ours w/o EBPG
Ours w/o Attention
M3RL
0 SeOoO 100000 UeOoO 200000	2S0000
Episodes
100000 ωβooo 200000
Episodes
3	OUrS
——Ours w/o EBPG
-2M I /	--- Ours VtiO Attention
M3ΛL
-300
0 SeOoO 100000 UeOoO 200000	25β00β
Episodes
(b) Multi-Bonus Resource
Collections.
(c) Navigation.
(d) Predator-Prey.

Figure 3: Leader’s reward curves for different tasks (rule-based followers).
as Dijkstra Algorithm). Based on this assumption, we can design a novel policy gradient to train
the meta policy:	Jk 叱 E Nλk log∏kιeta(z∖^)Rk], where λk is the parameter for meta-Policy
πmk eta (Details can be found in Lemma 3).
4.4 Loss Functions
In this section, we discuss how to design the leader’s and followers’ loss functions.
Loss Functions for the Leaders. The basic structure for the leader is the actor-critic structure (Sut-
ton & Barto, 1998). We find that adding regularizers can enhance the leader’s performance and we
implement the maximum entropy for the leader’s policy function as well as the L2 regularization
for the termination function, i.e., Lenp = 一 Pk Pωk μk(ωk ∖s, h) logμk(ωk ∖s, h) and Lreg = β2.
We also use imitation learning to learn the predicted action function pk . Following the same logic
of (Shu & Tian, 2019), two baseline functions φg(ct) and φb(ct) are also introduced to further reduce
the variance. Details can be found in Appendix B.
Loss Functions for the RL-Based Followers. The basic structure for each follower is also based
on the actor-critic structure. We leverage the action abstraction policy gradient as we mentioned
above. The learning rate between the leader and follower should satisfy the two time-scale principle
(Roughly speaking, the leader learns slower than the follower(s)), similar to (Borkar, 1997). Details
can be found in Appendix B and the pseudo-code can be found in Appendix C.
5	Experimental Results
5.1	Setup
Tasks. We evaluate the follow-
ing tasks to testify the performance
of our proposed method. All of
these tasks are based on SMG men-
tioned above. (1) resource collec-
tions: each follower collects three
PjeMəH
Resource Collection MulU-bonus Resource Collection Navigation
Tasla
Predator-prey
Figure 4: The final reward for RL-based followers. No ab-
straction means the vanilla RL-based followers.
types of resources including its pre-
ferred one and the leader can choose
two bonuses levels (Shu & Tian,
2019); (2) multi-bonus resources col-
lections: based on (1), the leader can choose four bonuses levels; (3) modified navigation: followers
are required to navigate some landmarks and after one of the landmarks is reached, the reached land-
mark disappears and new landmark will appear randomly. (4) modified predator-prey: followers are
required to capture some randomly moving preys, prizes will be given after touching them. Both (3)
and (4) are based on (Lowe et al., 2017) and we modify them into our SMG setting. Moreover, to
increase the difficulty, in each episode, the combinations of the followers will change, i.e., in each
task, there are 40 different followers and at each episode, we randomly choose some followers to
play the game. More details can be found in Appendix D.
Baselines & Ablations. To evaluate our method, we compare a recently proposed method as our
baseline: M3RL (Shu & Tian, 2019). We do not include other baselines because other methods
8
Published as a conference paper at ICLR 2020
cannot be used in our problems, as justified in (Shu & Tian, 2019). For the ablations of the leader
part, we choose: (1) ours: the full implementation of our method. (2) ours w/o EBPG: removing
the event-based policy gradient part; (3) ours w/o Attention: replacing follower-specified attention
model by the original attention model mentioned in (Shu & Tian, 2019). For the follower part,
we choose (a) with rule-based follower (b) with vanilla RL-based follower, and (c) with action
abstraction RL-based follower to testify the ability of our methods when facing different followers.
Hyper-Parameters. Our code is implemented in Pytorch (Paszke et al., 2017). If no special men-
tion, the batch size is 1 (online learning). Similar to (Shu & Tian, 2019), we set the learning rate as
0.001 for the leader’s critic and followers while 0.0003 for the leader’s policy. The optimization al-
gorithm is Adam (Kingma & Ba, 2014). Our method takes less than two days to train on a NVIDIA
Geforce GTX 1080Ti GPU in each experiment.
For the loss function, we set the λ1 = 0.01 and λ2 = 0.001. The total training episode is 250, 000
for all the tasks (including both the rule-based followers and the RL-based followers). To encourage
exploration, we use the ι-greedy2. For the leader, the exploration rate is set to 0.1 and slightly
decreases to zero (5000 episode). For the followers, the exploration rate for each agent is always 0.3
(except for the noise experiments).
5.2 Learning Efficiency
Table 1: Robustness results in multi-bound resource collections.
b% is the probability that followers randomly choose actions.
Methods	0% Noise	30% Noise	50% Noise
Ours total incentive	-^1832^^	17.63	17.28
M3RL total incentive	4.06	3.85	4.02
Ours total reward	-^1006^^	5.36	5.30
M3RL total reward	-1.58	-3.23	-8.96
The quantitative results with dif-
ferent tasks are shown in Figs. 3
& 4. For the rule-based fol-
lowers, from Fig. 3, we find
that our method outperforms the
state-of-the-art method in all the
tasks, showing that our method
is sample efficient and fast to coverage. There is an interesting phenomenon that in the task of
multi-bonus resource collections and navigation, only our method obtains a positive reward, indi-
cating that our method can work well in complicated environments. For ablations, we can see that
ours w/o attention and ours w/o EBPG are worse than ours, representing these components do en-
hance the performance. For the RL-based followers, from Fig. 4, we observe that when facing the
RL-based method with action abstraction, our approach outperforms the baseline method in all the
tasks (in predator-prey game, the reward for ours is twice as that of the state-of-the-art). We also
find that without action abstraction, the reward is less than zero, revealing that the abstraction does
play a crucial role in stabilizing training.
5.3	Robustness
This experiment is to evaluate whether our method is robust to the noise, i.e., the follower randomly
takes actions. We make this experiment by introducing noise into the follower decision. From
Table 1, we can find that our method reaches a higher total reward (more than 5) among all the
environment with noise than the state-of-the-art, indicating that our method is robust to the noise.
We also observe that the total reward for the baseline method becomes lower with the increase of the
noise while our method is more robust to the change. Moreover, for the incentive (the total gain),
we find that our method gains much more incentive than the state-of-the-art method, showing that
our method coordinates have a better coordination the followers than the state-of-the-art method.
5.4	More Experiments
We also do a substantial number of experiments. However, due to the space limitation, we can
only provide some results here: (1) The total incentives: incentive can reveal the performance of
successful rate interacting with the followers. Our method outperforms the state-of-the-art method,
indicating that our method has a better ability to interact with the followers. (2) Sparse EBPG: we
compare the performance gap between sparse EBPG and (dense) EBPG. This results show that the
sparse one is worse than the dense one, supporting the assumption that the dense signal can improve
2Normally it is called the decayed -greedy. We use ι instead of to avoid notation abuse.
9
Published as a conference paper at ICLR 2020
the sample efficiency. (3) Visualizing attention: We visualize the attention module to find what it ac-
tually learns and the result indicates that our attention mechanism does capture the followers whom
the leader needs to assign bonuses to. (4) Two time-scale training: We testify whether our two time-
scale training scheme works and the ablation shows that this scheme does play an important role in
improving the performance of both the leader and the followers. (5) The committing interval: We
observe that the dynamic committing interval (our method) performs better than the one with fixed
committing intervals. (6) Reward for RL-based followers: we show the reward for the followers,
which can provide the situation of the followers. The result represents that our method aids the
followers to gain more than the state-of-the-art method. (7) Number of RL-based followers: finally,
we testify our method in cases with different number of RL-based followers. The result shows that
our method always performs well. The full results can be found in Appendix D.
6	Conclusion Remarks
This paper proposes a novel RL training scheme for Stackelberg Markov Games with single leader
and multiple self-interested followers, which considers the leader’s long-term decision process and
complicated interaction between followers with three contributions. 1) To consider the long-term
effect of the leader’s behavior, we develop an event-based policy gradient for the leader’s policy. 2)
To predict the followers’ behaviors and make accurate response to their behaviors, we exploit the
leader-follower consistency to design a novel follower-aware module and follower-specific attention
mechanism. 3) We propose an action abstraction-based policy gradient algorithm to accelerate the
training process of followers. Experiments in resource collections, navigation, and predator-prey
game reveal that our method outperforms the state-of-the-art methods dramatically.
We are willing to highlight that SMGs contribute to the RL (especially MARL) community with
three key aspects: 1). As we mentioned in the Introduction, most of the existing MARL methods
assume that all the agents are willing to sacrifice themselves to maximize the total rewards, which
is not true in many real-world non-cooperative scenarios. On the contrary, our proposed method
realistically assumes that agents are self-interested. Thus, SMGs provide a new scheme focusing
more on the self-interested agents. We think this aspect is the most significant contribution to the
RL community. 2). The SMGs can be regarded as the multi-agent system with different roles (the
leader and the followers) (Wilson et al., 2008) and our method provides a solution to that problem.
3). Our methods also contribute to the hierarchical RL, i.e., it provides a non-cooperative training
scheme between the high-level policy (the leaders) and the low-level policy (the followers), which
plays an important role when the followers are self-interested. Moreover, our EBPG also propose
an novel policy gradient method for the temporal abstraction structure.
There are several directions we would like to investigate to further extend our SMG model: i) we
will consider multiple cooperative/competitive leaders and multiple self-interested followers, which
is the case in the labor market, ii) we will consider multi-level leaders, which is the case in the
hierarchical organizations and companies and iii) we will consider the adversarial attacks to our
SMG model, which may induce extra cost to the leader for efficient coordination. We believe that
our work is a preliminary step towards a deeper understanding of the leader-follower scheme in both
research and the application to society.
Acknowledgements
This research is supported by NRF AISG-RP-2019-0013, NSOE-TSS2019-01, MOE and NTU.
Also, this work is supported by the National Natural Science Foundation of China under Grants
(U1611264, U1811261,61602530, 61772567, U1811262 and U1711262). This work is also sup-
ported by the Pearl River Nova Program of Guangzhou (201906010080).
We would like to thank Tianming Shu, Darren Chua, Suming Yu, Enrique Munoz de Cote, and Xu
He for their kind suggestions and helps. We also appreciate the anonymous reviewers for their useful
suggestions.
10
Published as a conference paper at ICLR 2020
References
Andras Antos, Csaba Szepesvari, and Remi Munos. Fitted Q-iteration in continuous action-space
mdps. In NeurIPS,pp. 9-16, 2008.
Pierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic architecture. In AAAI., pp. 1726-
1734, 2017.
Raphen Becker, Shlomo Zilberstein, Victor Lesser, and Claudia V Goldman. Solving transition
independent decentralized Markov decision processes. Journal of Artificial Intelligence Research,
22:423-455, 2004.
Vivek S Borkar. Stochastic approximation with two time scales. Systems & Control Letters, 29(5):
291-294, 1997.
Noam Brown and Tuomas Sandholm. Superhuman AI for multiplayer poker. Science, 365(6456):
885-890, 2019.
Yash Chandak, Georgios Theocharous, James Kostas, Scott Jordan, and Philip S Thomas. Learning
action representations for reinforcement learning. arXiv preprint arXiv:1902.00183, 2019.
Changan Chen, Yuejiang Liu, Sven Kreiss, and Alexandre Alahi. Crowd-robot interaction:
Crowd-aware robot navigation with attention-based deep reinforcement learning. arXiv preprint
arXiv:1809.08835, 2018.
Chi Cheng, Zhangqing Zhu, Bo Xin, and Chunlin Chen. A multi-agent reinforcement learning
algorithm based on Stackelberg game. In DDCLS, pp. 727-732, 2017.
Christian Daniel, Herke Van Hoof, Jan Peters, and Gerhard Neumann. Probabilistic inference for
determining options in reinforcement learning. Machine Learning, 104(2-3):337-357, 2016.
Tanner Fiez, Benjamin Chasnov, and Lillian J Ratliff. Convergence of learning dynamics in Stack-
elberg games. arXiv preprint arXiv:1906.01217, 2019.
Jakob Foerster, Richard Y Chen, Maruan Al-Shedivat, Shimon Whiteson, Pieter Abbeel, and Igor
Mordatch. Learning with opponent-learning awareness. In AAMAS, pp. 122-130, 2018.
FA Gers, J Schmidhuber, and F Cummins. Learning to forget: Continual prediction with LSTM.
Neural Computation, 12(10):2451, 2000.
Tarun Gupta, Akshat Kumar, and Praveen Paruchuri. Planning and learning for decentralized MDPs
with event driven rewards. In AAAI, pp. 6186-6194, 2018.
He He, Jordan Boyd-Graber, Kevin Kwok, and Hal Daume III. Opponent modeling in deep rein-
forcement learning. In ICML, pp. 1804-1813, 2016.
Shariq Iqbal and Fei Sha. Actor-attention-critic for multi-agent reinforcement learning. In ICML,
pp. 2961-2970, 2019.
Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In
ICML, pp. 267-274, 2002.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Jean-Jacques Laffont and David Martimort. The Theory of Incentives: The Principal-Agent Model.
Princeton University Press, 2009.
Julien LaUmOnier and Brahim Chaib-draa. Multiagent q-learning: Preliminary study on dominance
between the nash and stackelberg equilibriums. In aAAi workshop, 2005.
Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent
actor-critic for mixed cooperative-competitive environments. In NeurIPS, pp. 6379-6390, 2017.
11
Published as a conference paper at ICLR 2020
David Mguni, Joel Jennings, Emilio Sison, Sergio Valcarcel Macua, Sofia Ceppi, and Enrique
Munoz de Cote. Coordinating the crowd: Inducing desirable equilibria in non-cooperative sys-
tems. In AAMAS,pp. 386-394, 2019.
Fei Miao, Shuo Han, Shan Lin, John A Stankovic, Desheng Zhang, Sirajum Munir, Hua Huang,
Tian He, and George J Pappas. Taxi dispatch with real-time sensing data in metropolitan areas: A
receding horizon control approach. IEEE Transactions on Automation Science and Engineering,
13(2):463-478, 2016.
Noam Nisan and Amir Ronen. Algorithmic mechanism design. Games and Economic Behavior, 35
(1-2):166-196, 2001.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
PyTorch. In Autodiff Workshop NeurIPS, 2017.
Neil Rabinowitz, Frank Perbet, Francis Song, Chiyuan Zhang, SM Ali Eslami, and Matthew
Botvinick. Machine theory of mind. In International Conference on Machine Learning, pp.
4215-4224, 2018.
Regis Sabbadin and Anne-France Viet. A tractable leader-follower MDP model for animal disease
management. In AAAI, pp. 1320-1326, 2013.
RegiS Sabbadin and Anne-France Viet. Leader-follower MDP models with factored state space and
many followers-followers abstraction, structured dynamics and state aggregation. In ECAI, pp.
116-124, 2016.
Weiran Shen, Binghui Peng, Hanpeng Liu, Michael Zhang, Ruohan Qian, Yan Hong, Zhi Guo,
Zongyao Ding, Pengjun Lu, and Pingzhong Tang. Reinforcement mechanism design, with ap-
plications to dynamic pricing in sponsored search auctions. arXiv preprint arXiv:1711.10279,
2017.
Tianmin Shu and Yuandong Tian. M3RL: Mind-aware multi-agent management reinforcement
learning. In ICLR, 2019.
Matthew Smith, Herke Hoof, and Joelle Pineau. An inference-based policy gradient method for
learning options. In ICML, pp. 4710-4719, 2018.
Richard S Sutton and Andrew G Barto. Introduction to Reinforcement Learning. MIT Press Cam-
bridge, 1998.
Richard S Sutton, Doina Precup, and Satinder P Singh. Intra-option learning about temporally
abstract actions. In ICML, pp. 556-564, 1998.
Pingzhong Tang. Reinforcement mechanism design. In IJCAI, pp. 26-30, 2017.
Kurian Tharakunnel and Siddhartha Bhattacharyya. Leader-follower semi-Markov decision prob-
lems: Theoretical framework and approximate solution. In 2007 IEEE International Symposium
on Approximate Dynamic Programming and Reinforcement Learning, pp. 111-118, 2007.
Karl Tuyls, Julien Perolat, Marc Lanctot, Joel Z Leibo, and Thore Graepel. A generalised method
for empirical game theoretic analysis. In AAMAS, pp. 77-85, 2018.
Utkarsh Upadhyay, Abir De, and Manuel Gomez Rodriguez. Deep reinforcement learning of marked
temporal point processes. In NeurIPS, pp. 3168-3178, 2018.
Alexander Vezhnevets, Volodymyr Mnih, Simon Osindero, Alex Graves, Oriol Vinyals, John Aga-
piou, et al. Strategic attentive writer for learning macro-actions. In NeurIPS, pp. 3486-3494,
2016.
Aaron Wilson, Alan Fern, Soumya Ray, and Prasad Tadepalli. Learning and transferring roles in
multi-agent reinforcement. In Proc. AAAI-08 Workshop on Transfer Learning for Complex Tasks,
2008.
12
Published as a conference paper at ICLR 2020
Shangtong Zhang and Shimon Whiteson. DAC: The double actor-critic architecture for learning
options. arXiv preprint arXiv:1904.12691, 2019.
Yan Zheng, Zhaopeng Meng, Jianye Hao, Zongzhang Zhang, Tianpei Yang, and Changjie Fan. A
deep Bayesian policy reuse approach against non-stationary agents. In NeurIPS, pp. 954-964,
2018.
13
Published as a conference paper at ICLR 2020
A Proofs
Proposition 1. The policy gradients for termination function βk (sti , ωti) and leader’s policy func-
tion μk (ωkk |s行, ω%-ι) can be written as:
Vθ J(θ) ≈ ET〜pτ(∙)
X XT I(eik)
k∈N i=0
Rτ(T) ;
VJ⑻ ≈ EipT (∙)
T
X XI0(eik)
k∈N i=0
Rτ(T) ;
where θ and H are the parameters for the termination function βk and the leader's policy μk.. I (∙)
and 10(∙) are the piece-wise functions:
(Zeβk(sti,ωti-1)
I (Jk ) = J 1-βk(Sti ,ωti-1)
I (ei )= ∖ Ne βk (Sti i-I)
I βk(Sti ,ωti-ι)
eik ∈ AT
eik ∈/ AT .
eik ∈ AT
eik ∈/ AT
Proof. First recall the utility for the leader:
J(θ) = EP(τ) [R*(T)]
=XXP(|AT|=m)P(s0)Yk∈N	Yeik∈Aτ βk(Sti, ωti-1)μk (ωkilsti, ωti-l) πk (akilsti ,ωki )] ×
τm	i T
]Yek∈Aτ(1-βk (stj, ωtj-i)) ∏k (ajMY)] } Yt=0 P (st+ι∣st, at)Rτ (T )；
Where m is the number of the times taking new action, m ≤ T . If m = T , implying that the leader
has taken new action at each time. P(lAT l = m) means the probability of times taking new action
within an episode.
Take derivatives on both LHS and RHS, we get:
VθJ(θ) = XX VθP(∣Aτ∣ = m)P(so) Y { Y βk(Sti,ωtτ)μk 32, ωt-) πk 忌隰")
τ m	k∈N	eik ∈AT
×
T
Y (l-βk (stj, ωtj-i)) ∏k (akj∣stj,ωkj)	Y P (st+ι∣st, at)Rτ (T)
ejk 6∈AT	t=0
τm
VθP(∣AT∣ = m)Qk∈N Qek∈At βk(sti, ωti-i)Qek∈Aτ(1-βk(stj, ωtj-i))
P (∣Aτ ∣ = m)Qk∈N Qek∈Aτ βk (Sti, ωti-i)Qek∈Aτ (1-βk (stj, ωtj-i))
× P(IAT1 = m)P(s0) Y I Y βk(sti,ωti-i)μk HiM,ωti-i) π (ɑkjsti,ωki)	×
k∈N	eik ∈AT
T
Y (l-βk (stj, ωtj-i)) ∏k (akj∣Stj,ωj)	Y P(st+ι∣st, aW (T)
ejk 6∈AT	t=0
FOrbreVity,weuseP(T) torePresentP(SO)Qk∈N { IQek∈at βk(Sti, ωti-ι)μk HiM, ωti-ι) πk (akiM,ωki)] ×
IQek∈at (1 - βk(Stj,ωtj-ι)) ∏k (akj∣Stj, T=OP(St+ι∣St, at), the trajectory probabil-
ity. And we use i ∈ eik and j ∈ ejk to rePresent i ∈ eik ∈ AT to j ∈ ejk ∈/ AT with a slight abuse of
14
Published as a conference paper at ICLR 2020
notation. Thus the equation mentioned above can be further simplified as:
ΣΣP (|At | = m)Vθ I E ∑lθg(l - βk (Sti, ωti-1))+E £log βk (sj, 3j-i) I P(T W (T)
T m	∖k∈N i∈ek	k6N j∈ek	)
≈ 叫"Vθ EElog(1 - βk (Sti,ωti-1))+E EIOgβk (Sj,3jτ)∣ R(T)
1	k∈N i∈ek	k∈N j∈ek
[ςς
[k∈N i∈ek
-vθβk (Sti,ωti-1) + 'X `X vθβk (sj,ωj-1)
1 - βk (Sti,ωti-ι) 幺 y βk IStjωj-ι)
k∈N j∈ek	' J j /
Rr (T)
The equation above is exactly the REINFORCE trick (Sutton & Barto, 1998) and the rule of deriva-
tions. The approximation indicates that one trajectory only has one AT3. Also based on the definition
of efk and ej, the equation can be rewritten in a more compact form:
Vθ J (θ)= Ei”.)
[XX
[k∈N i∈ek
-vθβk (Sti, ωti-1)
1 - βk (Sti, ωti-1)
+
k∈N j∈eJ
Rr (T)
“XX I(ek)
I Lk∈N i=0
RT (T)}
Where I(∙) is the piece-wise function:
一▽e βk (Sti ,ωti-ι)
1-βk (Sti ,ωti-ι)
▽gβ (Sti ,ωti-1)
βk (Sti ,ωti-1)
ek ∈ At
ek ∈ At .
This is the first part of the proof (the policy gradient for the termination function). Here, we start
proving the second part (the policy gradient for the leader,s action).
The proof of the second part is similar to the first part.
VJ ⑼=XX P (I At I = m)P(So) Y [ Y βk (Sti, %-" 3"%,必一)小(磋际,叫
T m	k∈N I |_ej∈Aτ
×
]T
∏ (1-βk(Stj, ωtj-1)))	∏P(St+1∣St, αt)Rτ(T);
ek ∈At	J t=0
旺
VOP(IAT1 = m) Qk∈N Qek∈Aτ 4k 33 |Sti, ωti-1)
P(IAT1 = m) ∏k∈" Qek∈Aτ ^k (ωki |Sti, ωti-1)
× P (I At I = m) × P (So) ∏ ∏ βk (Sti,必一川(就防,必一)πk (磋际,就)
k∈N i∈ek
× Y (1 - βk(Stj, ωtj-1))∏k (aj|Stj,4J)RT(T)
eJeAτ
=XX(XX Vo log μk (ωki I Sti, ωti-1)) P(T)Rτ(T)
T m ∖k∈N ek	)
=ET〜PT(∙) ∖ I X X Vo log μk (ωki∣Sti, ωti-1) Rt(T)}
([k∈N i∈ek	J	J
3We find that Upadhyay et al. (2018) also implement this approximation but use different explanations.
15
Published as a conference paper at ICLR 2020
We rewrite it to a more compact form:
▽方 J (叼=ET 〜pτ(∙)
([
T
X X I0(etk)
k∈N i=1
Rτ(T)
I，(。k) — ∫S logμk (ωkiIsti, ωti-ι) ek ∈ At
I (ei) = 0	eik ∈/ AT
□
Remark 1. Some researches also focus on event-based RL but either on single-agent continuous
time (Upadhyay et al., 2018) or reward representation (Gupta et al., 2018). We are the first to
develop and implement the event-based policy gradient into the multi-agent system.
Remark 2. In fact, the policy gradient for the leader actions might be somewhat sparse, i.e., we only
update the policy when the leader changes its actions. Notice that the leader commits to the same
action when eik ∈/ AT. Therefore, the probability of leader’s action P(AT) can also represented as:
P(AT)= ∏ ∏ βk(sti,ωti-i)μk QkiIsti,3—)
k∈N eik ∈AT
X Y (1 - βk(stj, ωj-1))) μk (ωj = ωj-ιlstj, ωj-ι) ;
ejk 6∈AT
Then the policy gradient for leader's policy ▽户 J(H) can thus be ▽方J(H)	≈
EipT(∙) {[Pk∈N PT=0 I0(ek)i RT(T)}，where
Io(ek) = [S logμk (ωkilsti, ωti-ι)	ek ∈ at
( i )= 'B (ωk = ωki-i ∣st, ωti-i) ef ∈ AT
Lemma 1. (Reward Bound) For any agent k, the corresponding reward function rk w.r.t ω is C-
Lipschitz continuous.
∣rk (st, ak, a-k, μ)-建(st, af, a-k, μ0) ∣ ≤ C∣μ - μ0∣, st, at = [ak, a-k].
Where C is a constant that satisfies C > 0.4 μ0 is leader's new policy. a-k is the joint action
without agent k 's.
Proof. Based on Assumption 1, We can build a compact metric space (A × Ω × S, Rk). From the
Heine-Cantor theorem we know that the compact metric space induce uniformly continuous.
That is, for every e > 0, there exists a δ > 0, such that ∣μ 一 μ0∣ ≤ e implies ∣rk (st, ak, a-k, μ) -
rk (st, ak, a-k, μ0) ∣ ≤ δ. There exists at least one positive Constance C such that δ ≤ C/e. Then
Irk(St, ak, a-k, μ) - rk (st, ak, a-k, μ0) ∣ ≤ δ ≤ C ≤ C∣μ 一 μ0∣.	□
This Lemma is similar to the assumption in (Mguni et al., 2019). We prove it rather than make it an
assumption.
Lemma 2. If Assumption 1 is satisfied, the inequality is established:
∣Jk([∏k,∏-k] , μ) - Jk([∏k,∏-k] , μ0)∣ ≤C∣μ 一 μ0∣
Proof. Expand the utility function by the bellman equation:
4The follower,s reward is originally defined based on the rk (St, ak, a-k, ωt 〜μ). Here to emphasize the
relationship between leader,s policy μ and reward, with a little abuse of the notation, we use rk (St, ak, a-k, μ)
to represent the reward function.
16
Published as a conference paper at ICLR 2020
|Jk ([∏k ,∏-k ] , μ) - J k ([∏k ,∏-k ] , μ )|	(1)
=| maχE rk (so,a0,a-k, μ) + Y X P(sι∣so, ao) Vk([∏k,∏-k] , μ, si)
π∈Π
s1∈S
— maχ E rk (s0,a0,a-k, μ0) + Y X P (sι∣so, ao) Vk ([∏k ,∏-k ] , μ0,s1) |	⑵
π∈Π
s1∈S
≤ max ∣E{rk (so, ak, a-k, μ) - Irk (so, ak, a-k, μ0)
π∈Π
+ Y X P(S0|s, a) [Vk([∏k ,∏-k] , μ,sι) - Vk ([∏k ,∏-k] , μ0,sι)]}∣,	(3)
s1∈S
∞
=XX YtPπ (st+i∣St, ∏) max ∣E [rk (st,ak,a-k, μ) - rk (st,ak, a-k, μ0)] ∣	(4)
π∈Π
s∈S t=o
The first relaxation is due to the property of max: max |A(x) - B(x)| ≥ | max A(x) - maxB(x)|,
where A(x) and B(x) are both the real functions. The Eq. (4) is the result of recursive iteration.
≤(1 - γ)-1 max ∣E∏ [rk (st, ak, a-k, μ) -Tk(St, ak, a-k, 〃')] | ≤ C∣μ - 〃1	(5)
Where C = (1 - Y)-1C.The last equation is drawn form the Assumption 1 and the inequality of a
geometric Series: |(I - Ypn)-1∣ ≤ (1 - Y)-i. Some parts follow the same logic of (Bacon et al.,
2017; MgUni et al., 2019; Kakade"& Langford, 2002).	□
Proposition 2. (Leader-Follower Consistency.) If both Assumptions 1 and 2 are satisfied, for every
e > 0, k ∈ N, there exists δ > 0, such that ∣μ 一 μ0∣ ≤ E implies ∣πk 一 π0k∣ ≤ δ, where μ0 and π0k
are the new policies for the leader and follower k respectively.
Proof. By combining Lemma 2 and AssUmption 2, we can draw that:
∣∏k - ∏0k∣ ≤ (1 - Y)C∣μ - μ0l
If there exist ∣ω - ω0∣ < e and We have:
∣∏k - ∏0k∣ ≤ (1 - Y)C∣μ - μ0∣ ≤ e(1 - Y)C
And we set δ ≥ e(1 - Y)C, the consistency is established.	□
Lemma 3. (Action Abstraction Policy Gradient.) Under the assumption that the low-level follower
policy ∏kower (at∣st, Zt) is fixed and deterministic, the policy gradient for action abstraction-based
follower can be formulated as:
VλkJk fx E[Vλk log ∏meta(z∣^)Rk],
Where Rk is the accumulated reward for the k-th follower.
Proof. Recall the Utility for the k-th follower:
Jk(μ, π) = E ]xT=0 ytrk (st, at, ωt) |at 〜π (∙∣st) ,st+i 〜P (∙∣st, at), so = s, ωt 〜μ(∙∣st, ωt-i)
For brevity, we rewrite the object as: Jk = E PtT=o Ytrtk (st, at, ωt) with a slight abUse of
notation. The standard policy gradient can be:
Vλk Jk = E[Vλk log * (ak∣st)Rk]
17
Published as a conference paper at ICLR 2020
When ∏kower (a|s, Z) is fixed and deterministic, the equation can be:
Vλk Jk = E Vλk log[£
πmeta(zt∖St)∏lower(at∖St,Zt)]Rk
Vλk[	ztπ
Pzt πketa(zt∖st)πkower
zt)
E
Vlk Pzt πLta(Zt ∖^t)πkθwer3岛，zt)] Rk = E Pzt Vlk πLta (Zt ∖ 的)πkθwer (at Mzt)]
Pzt nLta(Zt 舟)πkθwer (at∖st ,zt)	_	_	Pzt πLta (Zt ∖ 和)πkθwer (at∖st,zt)
[Pzt Vλk
πmeta(Zt 怛tHkOweKat怛t/t)] Rk]
Z
(X E [Vλk lθg[∏meta(Zt∖^t)] Rk]
Where Z = Pz πmk eta(Zt∖St)∏iower(at∖St,Zt) = 1 is the partition function. For brevity, with a
slight abuse of notation, we omit the superscript for variables a and Z which represents the index of
an agent.
B	Loss Functions
B.1 Leader Loss Functions
We add a baseline function to reduce the variance of the event-based policy gradient for the leader.
We adopt the idea of successor representation (Rabinowitz et al., 2018; Shu & Tian, 2019) as two
expected baseline functions: φg(ct) and φb(ct). For the gain baseline function:
φg = E ∑ I (g = gk) I (Sk = Sg) Vg,
g∈G k∈N
For the bonus-based baseline function:
Φb = -∑ ∑i(g = gk)bk,
g∈G k∈N
Two baseline neural network functions with parameters θg and θb are trained through minimizing
the mean square error:
Lbaseline =(Og (Ct; Og) - φg )2 + (Ob(Ct; θb) - φb)2,
Where ct is the attention-based latent variable.
To this end, the gradient for the leader can be formulated as:
Vθ Lpo0icy = -Vθ J 0(θ) + V*(λιLreg );	V ^ Lpolicy = -VJ0 ⑼ + Vo(λ2Lenp ).
Where
Vθ J0(θ) ≈ ET〜pτ(•)
VJ0⑻ ≈ ET〜pτ(∙)
X X I(etk)
k∈N t
("XXI1(etk)
k∈N t
(RT (T) - φg(q; θg ) + φb(ct
(RT(T) - φg(ct; θg) + φb(ct
are the baseline policy gradients.
We also leverage the imitation learning to learn the action probability function pk (af ∖st, hf, θ/
similar to (Shu & Tian, 2019), where θI is the parameters for follower-aware module:
LIL = E
-N Xlogpk (ak∖st, hk; θι)
k∈N
The history encoder, the state encoder and the attention module are updated with the leader’s policy
gradient end-to-end.
18
Published as a conference paper at ICLR 2020
B.2 Follower Loss Functions
The follower policy gradient is from Lemma 3 and we find that adding the history hk can improve
the performance:
Vλk Jk fx E [Vλk log ∏Lta(z∣S, hk )Rk],
and the learning rate of the follower α and the leader β satisfy
X: αt = X:β = ∞, X[ αt + β < ∞, lim 8t/αt = 0.
t→∞
t≥0	t≥0	t≥0
-«-» Tl ♦ 1 ♦	,	,1 CIl	， 1	♦	.	1 1 • 1	. 1	. 1 1	1 A • .1 1	♦	. /'
Which indicates the follower’s learning rate is much higher than the leader. β is the learning rate for
the leader’s critic function, which is the same as α. Followers do not have critic function.
C Algorithm
Algorithm 1: EBPG
Input: The initialized leader's parameters θ, H and followers' parameters λ;
Output: Well-trained leader and followers;
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
1
2
3
4
5
6
while not converge do
if Rollout Stage then
for t ≤ T do
leader commits to the goals g and bonuses b according to Algorithm 2;
send the goals and bonuses to each agent separately;
for k in N do
The k-th follower receives its own bonus bk and goal gk;
each agent make a decision ak = πk (ak ∣^t);
end
st+1, rt* l * *, {rtk}k∈N = step(st, at) ;	// transition function.
end
store hst, at, ωt, st+1, rtl, {rtk}k∈Ni in episode buffer;
else if Training Stage then
for k ∈ N do
I λk - λk - αVλk Jk;
end
// the follower’s parameters
end
θg《-θg - βv θg Lbaseline, θb《-θb - β V ^b Lbaseline ;	// the critic's
parameters
θɪ J θɪ 一 βLiL V^ιLil；	// the follower-aware module's parameters
θ J θ 一 βV θL1p00lcy ；	// the termination function's parameters
Id J Id — βV® LpOliCy;	// the parameters of leader's policy;
Algorithm 2: Action Choices for Leader
Input: Leader's policy μ
Output: goals g and bonuses b；
for k ∈ N do
sample the termination function βk;
if βtk terminates previous leader’s action ωtk-1 then
I choose new action ωf;
else
[ωk = ωk-ι;	// maintain previous action.
19
Published as a conference paper at ICLR 2020
(a) Resource Collections &	(b) Navigation.	(c) Predator-Prey.
Multi-bonus Resource Collec-
tions. This figure is inspired by
(Shu & Tian, 2019).
Figure 5:	Illustration of different tasks.
D	Experiment Details
D.1 Tasks Details
The illustration of experimental scenarios can be found in Figure 5. Here we give some details about
these environments:
Resource Collections. This task is similar to (Shu & Tian, 2019), which is based on the scene that
the leader and the followers collect some resources. Each follower has its own preference which
might be the same (or against) to the leader’s preference. In order to make the followers obey the
leader’s instruction, the leader should pay the followers bonuses. There are total 4 types of resources
and for different resources each agent has different preferences. The leader owns two type of bonus
(1 or 2) and 4 types of goals (each resource is a goal). The number of leader is 1 while the number
of followers is 4.
Multi-Bonus Resource Collections. This task is similar to Resource Collections. Except that the
leader can take 4 level bonuses (a bonus from 1 to 4) while each agent owns one skill. The number
of leader is 1 while the number of followers is 4.
Modified Navigation. This task is original from (Lowe et al., 2017). We make some modifications
here to make it suitable our SMG: the leader and the followers are going to navigate some land-
marks. Each follower has its own preference which might be the same (or against) to the leader’s
preference. When a landmark has been navigated, it disappears immediately and a new landmark
will appear. There are total 6 types of landmarks and for different landmarks, each agent has differ-
ent preferences. The leader owns two type of bonuses and 6 types of goals (each landmark is a goal).
The number of leader is 1 while the number of followers is 8 and the number of the landmarks is 6.
Modified Predator-Prey. This task is also original from (Lowe et al., 2017). We make some modifi-
cation here to make it suitable our SMG: the leader and the followers are going to catch some preys.
Each follower has its own preference which might be the same (or against) to the leader’s preference.
In each step, whether a prey has been caught, it randomly chooses a direction to go. Catching a prey
will not make it disappear, which means that the preys can exist until the game ends. There are total
8 types of preys and for different preys, each agent has different preferences. The leader owns two
type of bonuses and 8 types of goals (each prey is a goal). The followers are faster than the preys.
The number of leader is 1 while the number of followers is 10 and the number of the landmarks is 8.
Reward Design. The rewards mentioned in Section 3 are the general forms. Here, we define two
specified forms of the leader and followers reward function in our experiments:
Leader Reward. We define vg as the prize (utility) for finishing task g. We set the reward function
for the leader at step t as: rtl = g∈G k∈N I g = gtk I stk = sg vg - btk , formulated by the
total gain g∈G k∈N I g = gtk I stk = sg vg (the total prizes got by the leader at time t) minus
the total payment - g∈G k∈N I g = gtk btk (the total bonuses paid to the followers). We should
20
Published as a conference paper at ICLR 2020
emphasize that our leader reward is total different from the (Shu & Tian, 2019): in their approaches,
the leader changes its mind after signing a contract will not be punished. To make it suitable to the
real world, we modify the reward as the leader should pay the followers bonuses immediately after
signing the contract and cannot get back if it gives up the contract.
Follower Reward. For the followers, we set the reward for the k-th follower as: rtk = Pg rgk,t =
uk,g I stk = sg + I gtl = gtk × btk , where uk,g reveals the payoff of the k-th follower when
finishing task g (the preference). Specifically, rtk indicates that the follower can either follow the
leader’s instruction or just do what it prefers to. The followers will receive reward immediately after
signing the contract (the leader and the followers achieve an agreement). I stk = sg means that the
follower finishes the task g at step t. A penalty is added to the followers if the followers betray the
leader (the followers and the leader sign the contract but the followers do not agree to work).
D.2 Training Details
D.2.1 Network Design
Our network is based on (Shu & Tian, 2019). Some do not suit our method. We do some modification
here: (1) We change the vanilla attention mechanism (sum/average all the history and action of
each follower together) to a follower-specified one: each follower has a weight which indicates
how important the follower is at the current step. (2) The output for g and b are changed into the
sequential form, i.e., we first calculate p(gtk|ctk) to get gtk, then based on gtk, we choose p(btk |gtk, ctk).
(3) The history information htk is compressed by the LSTM (Gers et al., 2000).
Leader’s Network. For the history information, we are willing to stress that the history consists of
two parts, one is the statistical information similar to (Shu & Tian, 2019) and the other is the past
information given by the neural networks. We leverage the LSTM with 128 hidden units and two
fully-connected layers to compress the past information and the statistical information and obtain
htk. Each layer contains 128 neurons.
For the state information, we encode it into a 128-dimension hidden state by a state encoder. For
resource collections task, the state encoder consist of a convolutional layer with 64 channels and
kernels of 1×1 and two fully connected layers with 128 neurons. For navigation and predator-prey
task, we encode the state using two fully connected layers with 128 neurons.
For the follower-specific attention module, We firstly predict the ^k from the follower-aware mod-
ule which contains an LSTM with 128 hidden units and two fully-connected layers with 128 neu-
rons. Secondly, we transform ak, hf and the output of the state encoder into A(st, ^k, hf) using
a fully connected layer with 128 neurons. The hidden vector is generated by concatenating all the
A(st, ak, hk). Then the attention weight W is calculated using two fully connected layers with soft-
max activation. The Ct is calculated as a weighted summation of W and A(∙). Finally, we obtain the
k-th attention value Ck by concatenating Ct and A(st, ^k, hk).
For sequential graph-based model, we firstly calculate p(gtk |ctk) using two fully connected layers.
Each layer consists of 128 neuron units. The input of layers is ctk and the output is gtk . Then we
obtain btk by two fully connected layers. The input of layers is the concatenation of ctk and gtk .
Follower’s Network. We construct the RL-based followers by a state encoder and two fully con-
nected layers. Each fully connected layer contains 128 neurons. For resource collections task, the
state encoder consists of a convolutional layer with 64 channels and kernels of 1×1, two fully con-
nected layers with 128 neurons and an LSTM with 128 hidden units. For navigation and predator-
prey task, we encode the state using two fully connected layers with 128 neurons and an LSTM
with 128 hidden units. Notice that the network structure of action-abstraction RL-based follower
and non-action-abstraction RL-based follower is similar. The only difference is that the output of
the former is high-level action z and the output of the latter is low-level action atk .
21
Published as a conference paper at ICLR 2020
120
>0QgQ:0
0 8 6 4 2
əawəuu-
15.0
12.5
>
W 10.0
(U
C 7.5
5.0
17.5
0.0
0	50000	100000	150000	200000	250000
Episodes
(b) Total incentive for the multi-bonus task (rule-
based agent).
0
0	50000	100000	150000	200000	250000
Episodes
(a) Total incentive for the predator-prey task (rule-
based agent).
Figure 6:	Total incentives for predator-prey task and multi-bonus resource collections task.
0	50000	100000	150000	200000	250000
Episodes
① >J3U8U-
10
0
0	50000	100000	150000	200000
Episodes
0 g Q 。 。 Q
7 6 5 4 3 2
(a)	Total incentive for the resource collections task
(rule-based agent).
(b)	Total incentives for the navigation task (rule-
based agent).
Figure 7:	Total incentives for resource collections task and navigation task.
D.3 More Experimental Results
D.3.1 Total Incentive Function
Total incentive (income) functions Rin = Pt rtl +Pt Pk btk can reveal how well the leader interacts
with the followers; the higher the Rin is, the more successful the coordination between the leader
and the followers.
From Figure 6 and 7, comparing with the state-of-the-art method, we can see that our method far
outperforms the state-of-the-art method, which reveals that our method does have a better ability to
coordinate with the followers. In all of the scenarios, without the EBPG, the performance of our
method is worse than ours with EBPG. Specifically, in some scenarios (e.g., multi-bonus resource
collections, navigation), without the EBPG, the performance of our method is (or nearly) similar to
the performance of M3RL, showing the effectiveness of our novel policy gradient. Moreover, we can
notice that in navigation environment, without follower-specified attention, the performance of our
method diminishes rapidly, which implies that in some scenarios, attention does play an important
role.
22
Published as a conference paper at ICLR 2020
0	50000	100000	150000	200000	250000
Episodes
(a) The reward curves for multi-bonus resource
collections.
0.0
0	50000	100000	150000	200000	250000
Episodes
(b) The total incentive curves for multi-bonus re-
source collections.
50000	100000	150000	200000	250000
Episodes
(a) The reward curves for predator-prey.
Figure 9: The ablation study of sparse EBPG in the predator-prey task.
Figure 8: The ablation study of sparse EBPG in the multi-bonus resource collections task.
0
0	50000
100000	150000	200000	250000
Episodes
Sparse EBPG w/o Attention
(b) The total incentive curves for predator-prey.
D.3.2 Robust of Our Method
In this section, we are going to testify the robust of our method. Specifically, we evaluate whether
our method is robust to the noise. We make this experiment by introducing noise into the follower
decision. For example, if we set the noise function as 30%, indicating that there is 30% probability
that the followers will choose action randomly.
D.3.3 Sparse Event-based Policy Gradient
This experiment testifies whether the dense event-based policy gradient increases the leader’s per-
formance comparing with the sparse event-based policy gradient. We make ablations here: (1) Ours:
the full structure of our method; (2) Sparse Event-Based Policy Gradient (sparse EBPG): the fully
structure of ours except that the EBPG is replaced by sparse event-based policy gradient; (3) sparse
EBPG w/o attention: replacing the follower-specified attention mechanism by averaging the input
features.
From Figure 8 and 9 we can find that if the policy gradient is sparse, its performance is worse than
the dense one, implying that the dense method does improve the leader’s performance. There is also
an interesting phenomenon that sparse EBPG with follower-specified attention mechanism performs
better than that without, revealing that the attention can stabilize training when the training signal is
sparse.
23
Published as a conference paper at ICLR 2020
s-9M Uo-WaHV
follower 1
follower 2
■ follower 3
follower 4
ιllower 1
Illower 2
)ɪɪower 3
)IIower4
follower 1
follower 2
follower 3
follower 4
~s一。M Uo-WaHV
Commlt to Follower 4
(d)
G>mmlt to Follower 1
(a)
G>mmlt to Follower 2
(b)
G>mmlt to Follower 3
(c)
10.0
7.5
Figure 10: Visualization of the attention. The x-axis indicates the commit time for different agents
and the y-axis indicates the corresponding weights w k when the leader commits to that agent.
2.5
0.0 1 ∕ft
-2.5	Ours (different learning-rate)
Ours (same learning-rate)
—5.0
M3RL (different learning-rate)
M3RL (same learning-rate)
-10.0
0	50000	100000	150000	200000	250000
Episodes
10
O T
p-IBM33
50000
100000	150000	200000	250000
Episodes
(b) The ablation study of reward curves for two
time-scale update method (two RL-based follow-
ers).
(a) The ablation study of reward curves for two
time-scale update method (one RL-based fol-
lower).
Figure 11:	The ablation study of reward curves for two time-scale update method in resource col-
lections task.
D.3.4 Visualizing Attention
Following the same logic of (Iqbal & Sha, 2019), we visualize the weight of the attention when the
leader takes actions. From Figure 10, we find that the attention mechanism does learn to strongly
attend to the followers that the leader needs to take actions. The followers with leader’s commitment
obtain much higher attention weight than others, showing that the attention module actually learn to
identify the important followers while leader committing new action. Thus, the attention mechanism
does play an important role in improving the performance.
D.3.5 Two Time-Scale Updating
In order to evaluate the performance of our two time-scale update scheme (TTSU), we do an ablation
study as shown in Fig 11. We can find that the performance where the followers’ learning rate α
(1 × 10-3) is much larger than the leader’s β (3 × 10-4) is better than the performance where
the leader’s learning rate is similar to the followers (1 × 10-3). Moreover, without TTSU, the
reward curves of training methods become unstable, revealing that TTSU can stabilize the training
process. In fact, TTSU improves the rate of convergence and play an important role in improving
performance.
D.3.6 Committing Interval
We evaluate the leader’s performance between static committing interval and our dynamic commit-
ting interval. As shown in Figure 12, we observe that all the different fixed committing intervals
only change the rate of convergence and do not enhance the leader’s performance. All the fixed
24
Published as a conference paper at ICLR 2020
20
0
p-lraM33
-100
Dynamic interval commit (ours)
1 step interval commit (M3RL)
-----3 step interval commit (M3RL)
6 step interval commit (M3RL)
5 0 5 0 5 0
2 5 7 0 2 5
一 --T T T
p-IBM33
Dynamic committing interval (ours)
1 step committing interval (M3RL)
3 step committing interval (M3RL)
6 step committing interval (M3RL)
0	50000	100000	150000	200000	250000
Episodes
(a)	The ablation study of reward curves for differ-
ent committing interval in resource collections.
0	50000	100000	150000	200000	250000
Episodes
(b)	The ablation study of reward curves for fixed
committing interval method in navigation.
p」BMəɑ
Dynamic interval commit (ours)
-IOO
-120
1 step interval commit (M3RL)
3 step interval commit (M3RL)
6 step interval commit (M3RL)
p-IBM33
Dynamic committing interval (ours)
1 step committing interval (M3RL)
3 step committing interval (M3RL)
6 step committing interval (M3RL)
0	50000	100000	150000	200000	250000
Episodes
-250
-300
0	25000 50000 75000 100000125000150000175000 200000
Episodes
(c)	The ablation study of reward curves for fixed
committing interval method in multi-bonus re-
source collections.
(d) The ablation study of reward curves for fixed
committing interval method in predator-prey.
Figure 12:	The ablation study of reward curves for fixed commitment time in resource collections
task.
committing intervals are much worse than our dynamic committing approach, revealing the fact that
our dynamic committing approach aids a lot in improving the leader’s performance.
Table 2: Ablation results of RL-based followers for resource collections. ’X’ means the module is
used.
Modules								
EBPG	X	X		X		X		
Leader-follower consistency	X		X		X		X	
Action abstraction policy gradient	X	X	X	X				
Final reward	8.12	6.26	418	3.48	-0.02?	-0.03?	-0.05?	-0.05?
?The last four terms are so close to zero and we use the rewards from last episode as the final results.
D.3.7 Reward for RL-based Followers
We are interesting in the reward for the RL-based follower(s). Intuitively, a well-performing leader
can make the follower gain more. As shown in Figure 13, the reward for RL follower is higher than
M3RL follower in all the tasks. This represents the leader can coordinate the followers better and
make them gain more reward than other methods, which forms a win-win strategy.
25
Published as a conference paper at ICLR 2020
0 SeOoO 100000 UeOoO 20000«
Episodes
PJaMəɑ:
8 ---- Ours
M3ΛL	j"Λ∣ (I√⅛∙3ψ⅛*z	20
w	∕r√*	U
LSWW
BS
50000 IOOOOO IWOOO 200000	2S0000
Episodes
(c) Navigation.
PJaMəɑ:
20 ---- Ours
M3RL
u>
(d) Predator-Prey.
(a) Resource Collections. (b) Multi-Bonus Resource
Collections.
-10.0
I
---Ours
-- Ours (no abstraction)
----M3RL
----M3AL (no abstraction)
Figure 13: Reward curves for RL-based followers in different tasks.
PJeM9⅛j
O	50000 IOOOOO IWOOO 200000	2S0000
Episodes
SeOoO IOOOOO UeOoO 200000	2S0000
Episodes
50000 IOOOOO IWOOO 200000	2S0000
Episodes

(a) Resource Collections. (b) Multi-Bonus Resource (c) Navigation.	(d) Predator-Prey.
Collections.
Figure 14:	Leader’s reward curves for different tasks (RL-based followers).
D.3.8 Number of the follower agents
Finally, we evaluate the leader’s performance with different number of RL-based followers. As
shown in Figure 15, we find that our method outperforms the state-of-the-art method when facing
different number of RL-based workers.
D.3.9 Different combinations study
To further illustrate the performance of different combinations of our methods, we make an extra
ablation here. We choose the resource collections as the environments. Our analysis is as follows:
For the RL based followers scenario, As shown in Table 2, we find that the action abstraction policy
gradient is very important to converge. Additionally, adding different modules can improve the
performance and the method with all the modules reach the highest reward than other combinations.
The contribution ranking for each module is: Action abstraction policy gradient > EBPG > Leader-
follower consistency.
PJ∙M9H
Ours
M3RL
Ours (no abstraction)
M3KL (no abstraction)
Ours
M3RL
Ours (no abstract!。
M3ΛL (no abstractic
PJ∙M9≈
Ours
M3RL
Ours (no abstraction)
M3HL (no abstraction)
PJ∙M9≈
1	2	1	4	1	8	1	10
N umber of followers	Number of ft>l lowers	N umber of followers	N umber of followers
(a) Resource Collections. (b) Multi-Bonus Resource (c) Navigation.	(d) Predator-Prey.
Collections.
Figure 15:	The final reward of different number of follower agents in different tasks.
26