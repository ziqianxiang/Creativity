Published as a conference paper at ICLR 2020
Mixed-curvature Variational Autoencoders
Ondrej SkopekI Octavian-Eugen Ganea1,2 & Gary Bedgneul1,2
1	Department of Computer Science, ETH Zurich
2	Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology
oskopek@oskopek.com, oct@mit.edu, gary.becigneul@inf.ethz.ch
Ab stract
Euclidean geometry has historically been the typical “workhorse” for machine
learning applications due to its power and simplicity. However, it has recently
been shown that geometric spaces with constant non-zero curvature improve rep-
resentations and performance on a variety of data types and downstream tasks.
Consequently, generative models like Variational Autoencoders (VAEs) have been
successfully generalized to elliptical and hyperbolic latent spaces. While these ap-
proaches work well on data with particular kinds of biases e.g. tree-like data for a
hyperbolic VAE, there exists no generic approach unifying and leveraging all three
models. We develop a Mixed-curvature Variational Autoencoder, an efficient way
to train a VAE whose latent space is a product of constant curvature Riemannian
manifolds, where the per-component curvature is fixed or learnable. This gener-
alizes the Euclidean VAE to curved latent spaces and recovers it when curvatures
of all latent space components go to 0.
1	Introduction
Generative models, a growing area of unsupervised learning, aim to model the data distribution
p(x) over data points x from a space X, which is usually a high-dimensional Euclidean space Rn.
This has desirable benefits like a naturally definable inner-product, vector addition, or a closed-
form distance function. Yet, many types of data have a strongly non-Euclidean latent structure
(Bronstein et al., 2017), e.g. the set of human-interpretable images. They are usually thought to
live on a “natural image manifold” (Zhu et al., 2016), a continuous lower-dimensional subset of the
space in which they are represented. By moving along the manifold, one can continuously change
the content and appearance of interpretable images. As noted in Nickel & Kiela (2017), changing
the geometry of the underlying latent space enables better representations of specific data types
compared to Euclidean spaces of any dimensions, e.g. tree structures and scale-free networks.
Motivated by these observations, a range of recent methods learn representations in different spaces
of constant curvatures: spherical or elliptical (Batmanghelich et al., 2016), hyperbolic (Nickel &
Kiela, 2017; Sala et al., 2018; Tifrea et al., 2019) and even in products of these spaces (Gu et al.,
2019; Bachmann et al., 2020). Using a combination of different constant curvature spaces, Gu et al.
(2019) aim to match the underlying geometry of the data better. However, an open question remains:
how to choose the dimensionality and curvatures of each of the partial spaces?
A popular approach to generative modeling is the Variational Autoencoder (Kingma & Welling,
2014). VAEs provide a way to sidestep the intractability of marginalizing a joint probability model
of the input and latent space p(x, z), while allowing for a prior p(z) on the latent space. Recently,
variants of the VAE have been introduced for spherical (Davidson et al., 2018; Xu & Durrett, 2018)
and hyperbolic (Mathieu et al., 2019; Nagano et al., 2019) latent spaces.
Our approach, the Mixed-curvature Variational Autoencoder, is a generalization of VAEs to products
of constant curvature spaces1. It has the advantage of a better reduction in dimensionality, while
maintaining efficient optimization. The resulting latent space is a “non-constantly” curved manifold
that is more flexible than a single constant curvature manifold.
1Code is available on GitHub at https://github.com/oskopek/mvae.
1
Published as a conference paper at ICLR 2020
Our contributions are the following: (i) we develop a principled framework for manipulating rep-
resentations and modeling probability distributions in products of constant curvature spaces that
smoothly transitions across curvatures of different signs, (ii) we generalize Variational Autoencoders
to learn latent representations on products of constant curvature spaces with generalized Gaussian-
like priors, and (iii) empirically, our models outperform current benchmarks on a synthetic tree
dataset (Mathieu et al., 2019) and on image reconstruction on the MNIST (LeCun, 1998), Omniglot
(Lake et al., 2015), and CIFAR (Krizhevsky, 2009) datasets for some latent space dimensions.
2	Geometry and Probability in Riemannian manifolds
To define constantly curved spaces, we first need to define the notion of sectional curvature K(τx) of
two linearly independent vectors in the tangent space at a point x ∈ M spanning a two-dimensional
plane τx (Berger, 2012). Since we deal with constant curvature spaces where all the sectional
curvatures are equal, We denote a manifold,s curvature as K. Instead of curvature K, We sometimes
use the generalized notion of a radius: R = 1 /，|K|.
There are three different types of manifolds M We can define With respect to the sign of the curva-
ture: a positively curved space, a “flat” space, and a negatively curved space. Common realizations
of those manifolds are the hypersphere SK, the Euclidean space E, and the hyperboloid HK:
(SK = {x ∈ Rn+1 : hx, x〉2 = 1/K},	for K > 0
M = En = Rn,	forK = 0
[hK = {x ∈ Rn+1 : hx, XiL = 1/K}, for K< 0
where(∙, )？ is the standard Euclidean inner product, and(•,•)£ is the Lorentz inner product,
n+1
hx, yiL = -x1y1 +	xiyi ∀x, y ∈ Rn+1.
i=2
We Will need to define the exponential map, logarithmic map, and parallel transport in all spaces We
consider. The exponential map in Euclidean space is defined as expx(v) = x + v, for all x ∈ En
and v ∈ TxEn. Its inverse, the logarithmic map is logx (y) = y - x, for all x, y ∈ En. Parallel
transport in Euclidean space is simply an identity PTx→y(v) = v, for all x, y ∈ En and v ∈ TxEn.
An overvieW of these operations in the hyperboloid HnK and the hypersphere SnK can be found in
Table 1. For more details, refer to Petersen et al. (2006), Cannon et al. (1997), or Appendix A.
2.1	Stereographically projected spaces
The above spaces are enough to cover any possible value of the curvature, and they define all the
necessary operations We Will need to train VAEs in them. HoWever, both the hypersphere and the
hyperboloid have an unsuitable property, namely the non-convergence of the norm of points as the
curvature goes to 0. Both spaces groW as K → 0 and become locally “flatter”, but to do that, their
points have to go aWay from the origin of the coordinate space 0 to be able to satisfy the manifold’s
definition. A good example of a point that diverges is the origin of the hyperboloid (or a pole of the
hypersphere) μo = (1∕P∣K|, 0,..., 0)T. In general, we can see that kμok2 = 1/K —→→ 土∞.
Additionally, the distance and the metric tensors of these spaces do not converge to their Euclidean
variants as K → 0, hence the spaces themselves do not converge to Rd. This makes both of these
spaces unsuitable for trying to learn sign-agnostic curvatures.
Luckily, there exist well-defined non-Euclidean spaces that inherit most properties from the hyper-
boloid and the hypersphere, yet do not have these properties - namely, the POinCare ball and the
projected sphere, respectively. We obtain them by applying stereographic conformal projections,
meaning that angles are preserved by this transformation. Since the distance function on the hyper-
boloid and hypersphere only depend on the radius and angles between points, they are isometric.
We first need to define the projection function ρK. For a point (ξ; xT)T ∈ Rn+1 and curvature
K ∈ R, where ξ ∈ R, x,y ∈ Rn
1	1	1 - K kyk2.
IpKi 1 + K kyk2 ;
ρK((ξ; xT)T)
x
1 + PKξ,
ρ-K1(y)
2yT
1 + K kyk2
T
2
Published as a conference paper at ICLR 2020
The formulas correspond to the classical stereographic projections defined for these models (Lee,
1997). Note that both of these projections map the point μo = (1/y∕∖K|, 0,..., 0) in the original
space to μo = 0 in the projected space, and back.
Since the stereographic projection is conformal, the metric tensors of both spaces will be conformal.
In this case, the metric tensors of both spaces are the same, except for the sign of K : gDxK = gPxK =
(λxK)2gE, for all x in the respective manifold (Ganea et al., 2018a), and gEy = I for all y ∈ E.
The conformal factor λxK is then defined as λxK = 2/(1 + K kxk22). Among other things, this
form of the metric tensor has the consequence that we unfortunately cannot define a single unified
inner product in all tangent spaces at all points. The inner product at x ∈ M has the form of
hu, vix = (λxK)2 hu, vi2 for all u, v ∈ TxM.
We can now define the two models corresponding to K > 0 and K < 0. The curvature of the
projected manifold is the same as the original manifold. An n-dimensional projected hypersphere
(K > 0) is defined as the set DK = PK(SK \ {-μo}) = Rn, where μo = (1∕p∖K∖, 0,..., 0)t ∈
SK, along with the induced distance function. The n-dimensional Poincare ball PK (also called
the Poincare disk when n = 2) for a given curvature K < 0 is defined as PK = PK (HK)=
{x ∈ Rn : hx, x〉2 < 一KK } , with the induced distance function.
2.2	Gyrovector spaces
An important analogy to vector spaces (vector addition and scalar multiplication) in non-Euclidean
geometry is the notion of gyrovector spaces (Ungar, 2008). Both of the above spaces DK and PK
(jointly denoted as MK) share the same structure, hence they also share the following definition of
addition. The Mobius addition ㊉K of x, y ∈ MK (for both signs of K) is defined as
X㊉K y
(I 一 2K hx, y% — K kyk2)χ + (1 + K kxk2)y
1一 2K hx, yi2 + K2 kx∣∣2 ky∣∣2
We can, therefore, define “gyrospace distances” for both of the above spaces, which are alternative
curvature-aware distance functions
一 ,	.	2 一 二「	，	、	2 一一	…
dDgyr(x, y) = √K tan 1(√K k-x ㊉K y∣b), dpsτ(x, y) = √-K tanh 1(√-K k—x ㊉K y∣b).
These two distances are equivalent to their non-gyrospace variants dM (x, y) = dMgyr (x, y), as is
shown in Theorem A.4 and its hypersphere equivalent. Additionally, Theorem A.5 shows that
dMgyr(x, y) -K→→ 2 ∣∣χ 一 y∣b,
which means that the distance functions converge to the Euclidean distance function as K → 0.
We can notice that most statements and operations in constant curvature spaces have a dual statement
or operation in the corresponding space with the opposite curvature sign. The notion of duality
is one which comes up very often and in our case is based on Euler’s formula eix = cos(x) +
i Sin(X) and the notion of principal square roots √-K = i√K. This provides a connection between
trigonometric, hyperbolic trigonometric, and exponential functions. Thus, we can convert all the
hyperbolic formulas above to their spherical equivalents and vice-versa.
Since Ganea et al. (2018a) and Tifrea et al. (2019) used the same gyrovector spaces to define an
exponential map, its inverse logarithmic map, and parallel transport in the POinCare ball, We can
reuse them for the projected hypersphere by applying the transformations above, as they share the
same formalism. For parallel transport, we additionally need the notion of gyration (Ungar, 2008)
gyr[x, y]v =㊀K(x ㊉K y)㊉K (x ㊉K (y ㊉K v)).
Parallel transport in the both the projected hypersphere and the Poincare ball then is PTK→y (v)=
(λK∕λK)gyr[y, -x]v, for all x, y ∈ MK and v ∈ Tx MnK . Using a curvature-aware definition
scalar products (hx, yiK = hx, yi2 if K ≥ 0, hx, yiL if K < 0) and of trigonometric functions
sin if K > 0
sinK =
sinh if K < 0
cos if K > 0
cosK = cosh ifK<0
tan if K > 0
anK	tanh if K < 0
we can summarize all the necessary operations in all manifolds compactly in Table 1 and Table 2.
3
Published as a conference paper at ICLR 2020
	Table 1: Summary of operations in SK and HK .
Distance	d(x, y) = PKi cosκ1(IK I hx, yiκ)
Exponential map	eχpK (V) = CoSK (PKi ιιvkκ)X+SinK (PKi ιιvkκ) PKv kvk
Logarithmic map	logK(y) = . CoSK -K■ hχ,yiK) I) (y — K hx, yiκ χ) SinK (CoSK (K hx, yiκ))	
Parallel transport	PTK→y(v) = V - I KKj；iK	(x + y) 1 + K hx, yiK
Table 2: Summary of operations in projected spaces DK and PK .
Distance	1	1	2K ιX - y ι22 d(x, y) =	c CoSK 12 PK	Kl (1 + K kχk2)(i + K kyk2)
Gyrospace distance	2 2	2	2	一	... dgyr(x, y) = p=K= tanK (a/|K 1 Il-x ㊉K yk2)
Exponential map	eχpK(v) = X ㊉K (tanK(PKλx kvk2) 焉	! 2	IKI ιvι2
Logarithmic map	2	—X Q^ y logx (y) = PWtanK (pk1 k-x ㊉K yk2)k-x ㊉K y∣2
Parallel transport	λK PTK→y (v) = λK gyr[y, -χ]v λy	
2.3	Products of spaces
Previously, our space consisted of only one manifold of varying dimensionality and fixed curvature.
Like Gu et al. (2019), we propose learning latent representations in products of constant curvature
spaces, contrary to existing VAE approaches which are limited to a single Riemannian manifold.
Our latent space M0 consists of several component spaces M0 = ×ik=1 MnKi , where ni is the di-
mensionality of the space, Ki is its curvature, and M ∈ {E, S, D, H, P} is the model choice. Even
though all components have constant curvature, the resulting manifold M0 has non-constant curva-
ture. Its distance function decomposes based on its definition dM (x, y) = Pk=I d242(x(i), y(i)),
Ki
where x(i) represents a vector in MnKi , corresponding to the part of the latent space representa-
tion of x belonging to MnKi . All other operations we defined on our manifolds are element-wise.
Therefore, we again decompose the representations into parts x(i), apply the operation on that part
Kii)(x(i)) and concatenate the resulting parts back X = Jk=I x(i).
The signature of the product space, i.e. its parametrization, has several degrees of freedom per
component: (i) the model M, (ii) the dimensionality ni , and (iii) the curvature Ki . We need to
select all of the above for every component in our product space. To simplify, we use a shorthand
notation for repeated components: (MnKi )j = ×lj=1 MnKi . In Euclidean spaces, the notation is
redundant. For n1 , . . . , nk ∈ Z, such that Pik=1 ni = n ∈ Z, it holds that the Cartesian product
k
of Euclidean spaces Eni is En = ×i=1 Eni . However, the equality does not hold for the other
considered manifolds. This is due to the additional constraints posed on the points in the definitions
of individual models of curved spaces.
4
Published as a conference paper at ICLR 2020
2.4	Probability distributions on Riemannian manifolds
To be able to train Variational Autoencoders, we need to chose a probability distribution p as a
prior and a corresponding posterior distribution family q. Both of these distributions have to be
differentiable with respect to their parametrization, they need to have a differentiable Kullback-
Leiber (KL) divergence, and be “reparametrizable” (Kingma & Welling, 2014). For distributions
where the KL does not have a closed-form solution independent on z, or where this integral is too
hard to compute, we can estimate it using Monte Carlo estimation
DKL (q || Pp ≈ L X log (W) if L=Ilog (qp⅞)，
where ZQ)〜q for all l = 1,...,L. The Euclidean VAE uses a natural choice for a prior on its
latent representations - the Gaussian distribution (Kingma & Welling, 2014). Apart from satisfying
the requirements for a VAE prior and posterior distribution, the Gaussian distribution has addi-
tional properties, like being the maximum entropy distribution for a given variance (Jaynes, 1957).
There exist several fundamentally different approaches to generalizing the Normal distribution to
Riemannian manifolds. We discuss the following three generalizations based on the way they are
constructed (Mathieu et al., 2019).
Wrapping This approach leverages the fact that all manifolds define a tangent vector space at
every point. We simply sample from a Gaussian distribution in the tangent space at μo with mean
0, and use parallel transport and the exponential map to map the sampled point onto the manifold.
The PDF can be obtained using the multivariate chain rule ifwe can compute the determinant of the
Jacobian of the parallel transport and the exponential map. This is very computationally effective at
the expense of losing some theoretical properties.
Restriction The “Restricted Normal” approach is conceptually antagonal - instead of expanding
a point to a dimensionally larger point, we restrict a point of the ambient space sampled from a
Gaussian to the manifold. The consequence is that the distributions constructed this way are based
on the “flat” Euclidean distance. An example of this is the von Mises-Fisher (vMF) distribution
(Davidson et al., 2018). A downside of this approach is that vMF only has a single scalar covariance
parameter κ, while other approaches can parametrize covariance in different dimensions separately.
Maximizing entropy Assuming a known mean and covariance matrix, we want to maximize the
entropy of the distribution (Pennec, 2006). This approach is usually called the Riemannian Nor-
mal distribution. Mathieu et al. (2019) derive it for the POinCare ball, and Hauberg (2018) derive
the Spherical Normal distribution on the hypersphere. Maximum entropy distributions resemble the
Gaussian distribution’s properties the closest, but it is usually very hard to sample from such distribu-
tions, compute their normalization constants, and even derive the specific form. Since the gains for
VAE performance using this construction of Normal distributions over wrapping is only marginal,
as reported by Mathieu et al. (2019), we have chosen to focus on Wrapped Normal distributions.
To summarize, Wrapped Normal distributions are very computationally efficient to sample from and
also efficient for computing the log probability ofa sample, as detailed by Nagano et al. (2019). The
Riemannian Normal distributions (based on geodesic distance in the manifold directly) could also
be used, however they are more computationally expensive for sampling, because the only methods
available are based on rejection sampling (Mathieu et al., 2019).
2.4.	1 Wrapped Normal distribution
First of all, We need to define an “origin” point on the manifold, which We will denote as μo ∈
MK. What this point corresponds to is manifold-specific: in the hyperboloid and hypersphere it
corresponds to the point μo = (1/，|K|, 0,..., 0)t, and in the POinCare ball, projected sphere, and
Euclidean space it is simply μo = 0, the origin of the coordinate system.
Sampling from the distribution WN(μ, Σ) has been described in detail in Nagano et al. (2019) and
Mathieu et al. (2019), and we have extended all the necessary operations and procedures to arbitrary
curvature K. Sampling then corresponds to:
V 〜N(μo,Σ) ∈ TμoMK, U = PTK0→μ(v) ∈ TμMκ, Z = expμK(U) ∈ MK.
5
Published as a conference paper at ICLR 2020
The log-probability of samples can be computed by the reverse procedure:
U = IogK (Z) ∈ TμMκ, V = PTK-μo (U) ∈ Tμo MK,
log WN(μ, ∑) = logN(V μo, Σ) - log det (源),
where f = expK ◦ PTK,→μ . The distribution can be applied to all manifolds that We have intro-
duced. The only differences are the specific forms of operations and the log-determinant in the PDF.
The specific forms of the log-PDF for the four spaces H, S, D, and P are derived in Appendix B. All
variants of this distribution are reparametrizable, differentiable, and the KL can be computed using
Monte Carlo estimation. As a consequence of the distance function and operations convergence the-
orems for the Poincare ball A.5 (analogously for the projected hypersphere), A.17, A.18, and A.20,
we see that the Wrapped Normal distribution converges to the Gaussian distribution as K → 0.
3	Variational Autoencoders in products spaces
To be able to learn latent representations in Riemannian manifolds instead of in Rd as above, we
only need to change the parametrization of the mean and covariance in the VAE forward pass, and
the choice of prior and posterior distributions. The prior and posterior have to be chosen depending
on the chosen manifold and are essentially treated as hyperparameters of our VAE. Since we have
defined the Wrapped Normal family of distributions for all spaces, we can use WN(μo, σ2I) as the
posterior family, and WN(μo, I) as the prior distribution. The forms of the distributions depend on
the chosen space type. The mean is parametrized using the exponential map expK as an activation
function. Hence, all the model’s parameters live in Euclidean space and can be optimized directly.
In experiments, we sometimes use vMF(μ, K) for the hypersphere SK (or a backprojected variant of
vMF for DnK) with the associated hyperspherical uniform distribution U(SnK) as a prior (Davidson
et al., 2018), or the Riemannian Normal distribution RN(μ, σ2) and the associated prior RNμo, 1
for the Poincare ball PnK (Mathieu et al., 2019).
3.1	Learning curvature
We have already seen approaches to learning VAEs in products of spaces of constant curvature.
However, we can also change the curvature constant in each of the spaces during training. The
individual spaces will still have constant curvature at each point, we just allow changing the constant
in between training steps. To differentiate between these training procedures, we will call them fixed
curvature and learnable curvature VAEs respectively.
The motivation behind changing curvature of non-Euclidean constant curvature spaces might not
be clear, since it is apparent from the definition of the distance function in the hypersphere and
hyperboloid d(x, y) = R ∙ θχ,y, that the distances between two points that stay at the same angle
only get rescaled when changing the radius of the space. Same applies for the POinCare ball and
the projected spherical space. However, the decoder does not only depend on pairwise distances,
but rather on the specific positions of points in the space. It can be conjectured that the KL term
of the ELBO indeed is only “rescaled” when we change the curvature, however, the reconstruction
process is influenced in non-trivial ways. Since that is hard to quantify and prove, we devise a series
of practical experiments to show overall model performance is enhanced when learning curvature.
Fixed curvature VAEs In fixed curvature VAEs, all component latent spaces have a fixed curva-
ture that is selected a priori and fixed for the whole duration of the training procedure, as well as
during evaluation. For Euclidean components it is 0, for positively or negatively curved spaces any
positive or negative number can be chosen, respectively. For stability reasons, we select curvature
values from the range [0.25, 1.0], which corresponds to radii in [1.0, 2.0]. The exact curvature value
does not have a significant impact on performance when training a fixed curvature VAE, as motivated
by the distance rescaling remark above. In the following, we refer to fixed curvature components
with a constant subscript, e.g. H1n .
6
Published as a conference paper at ICLR 2020
Learnable curvature VAEs In all our manifolds, we can differentiate the ELBO with respect to
the curvature K. This enables us to treat K as a parameter of the model and learn it using gradient-
based optimization, exactly like we learn the encoder/decoder maps in a VAE.
Learning curvature directly is badly conditioned - We are trying to learn one scalar parameter that
influences the resulting decoder and hence the ELBO quite heavily. Empirically, we have found that
Stochastic Gradient Descent Works Well to optimize the radius of a component. We constrain the
radius to be strictly positive in all non-Euclidean spaces by applying a ReLU activation function
before We use it in operations.
Universal curvature VAEs HoWever, We must still a priori select the “partitioning” of our latent
space - the number of components and for each of them select the dimension and at least the sign of
the curvature of that component (signature estimation).
The simplest approach Would be to just try all possibilities and compare the results on a specific
dataset. This procedure Would most likely be optimal, but does not scale Well.
To eliminate this, We propose an approximate method - We partition our space into 2-dimensional
components (if the number of dimensions is odd, one component Will have 3 dimensions). We
initialize all of them as Euclidean components and train for half the number of maximal epochs We
are alloWed. Then, We split the components into 3 approximately equal-sized groups and make one
group into hyperbolic components, one into spherical, and the last remains Euclidean. We do this
by changing the curvature of a component by a very small . We then train just the encoder/decoder
maps for a feW epochs to stabilize the representations after changing the curvatures. Finally, We
alloW learning the curvatures of all non-Euclidean components and train for the rest of the alloWed
epochs. The method is not completely general, as it never uses components bigger than dimension
2, but the approximation has empirically performed satisfactorily.
We also do not constrain the curvature of the components to a specific sign in the last stage of
training. Therefore, components may change their type of space from a positively curved to a
negatively curved one, or vice-versa.
Because of the divergence of points as K → 0 for the hyperboloid and hypersphere, the universal
curvature VAE assumes the positively curved space is D and the negatively curved space is P. In all
experiments, this universal approach is denoted as Un .
4	Experiments
For our experiments, We use four datasets: (i) Branching diffusion process (Mathieu et al., 2019,
BDP) - a synthetic tree-like dataset With injected noise, (ii) Dynamically-binarized MNIST digits
(LeCun, 1998) - We binarize the images similarly to Burda et al. (2016); Salakhutdinov & Murray
(2008): the training set is binarized dynamically (uniformly sampled threshold per-sample bin(x) ∈
{0, 1}D = x > U[0, 1], x ∈ x ⊆ [0, 1]D), and the evaluation set is done With a fixed binarization
(x > 0.5), (iii) Dynamically-binarized Omniglot characters (Lake et al., 2015) doWnsampled to
28 × 28 pixels, and (iv) CIFAR-10 (Krizhevsky, 2009).
All models in all datasets are trained With early stopping on training ELBO With a lookahead of 50
epochs and a Warmup of 100 epochs (BoWman et al., 2016). All BDP models are trained for a 1000
epochs, MNIST and Omniglot models are trained for 300 epochs, and CIFAR for 200 epochs. We
compare models With a given latent space dimension using marginal log-likelihood With importance
sampling (Burda et al., 2016) With 500 samples, except for CIFAR, Which uses 50 due to memory
constraints. In all tables, We denote it as LL. We run all experiments at least 3 times to get an
estimate of variance When using different initial values.
In all the BDP, MNIST, and Omniglot experiments beloW, We use a simple feed-forWard encoder
and decoder architecture consisting of a single dense layer With 400 neurons and element-Wise
ReLU activation. Since all the VAE parameters {θ, φ} live in Euclidean manifolds, We can use
standard gradient-based optimization methods. Specifically, We use the Adam (Kingma & Ba, 2015)
optimizer With a learning rate of 10-3 and standard settings for β1 = 0.9, β2 = 0.999, and = 10-8.
For the CIFAR encoder map, We use a simple convolutional neural netWorks With three convolutional
layers With 64, 128, and 512 channels respectively. For the decoder map, We first use a dense layer
7
Published as a conference paper at ICLR 2020
Table 3: Summary of results (mean and standard deviation), latent space dimension 6, spherical
covariance, on the BDP (left) and MNIST (right) datasets.
Model		LL	Model		LL
S61		-55.81±0.35	S61			—96.71±o.17
D61		-55.78±o.07	vMF S61		—97.03±o.14
E6		-56.28±0.56	D61		—98.21±o.23
(H2-1)3		-56.08±o.52	E6		—97.16±o.15
H6-1		—56.18±o.32	H6-1		—97.10±o.44
(P2-1)3		-55.98±o.62	(P2-1)3		—97.56±o.04
P6-1 (RN P2-1)	3	—56.74±0.55	(RN P2-1)3		—92.54±o.19
		—54.99±0.12	(S2)3		—96.46±o.12
(S2)3		—56.05±o.21	S6		—96.72±o.15
(D2)3		—56.06±o.36	vMF S6		—96.72±o.18
(H2)3		—55.80±0.32	D6		—97.72±o.15
(P2)3		—56.29±o.05	(H2)3		—97.37±o.13
(RN P2)3		—56.25±o.56	(RN P2)3		—94.I6±0.68
D2 × E2 ×	P2	—55.87±0.22	D2 × E2 ×	P2	—97.48±o.18
E2 × H2 ×	S2	—55.92±o.42	D2 × E2 ×	(RN P2)	—96.43±o.47
E2 × H2 ×	(vMF S2)	—55.82±o.43	D12 × E2 ×	(RN P2-1)	—96.18±o.21
E2 × H2-1	× (vMF S21)	—55.77±0.51	E2 × H2 × E2 × H2-1 E2 × H2 ×	S2	—96.80±o.20
(U2)3 U6		—55.56±o.15 —55.84±o.38		× S21 (vMF S2)	—96.76±o.O9 —96.56±o.27
	—		(U2)3		—97.12±o.04
			U6		—97.26±o.16
of dimension 2048, and then three consecutive transposed convolutional layers with 256, 64, and
3 channels. All layers are followed by a ReLU activation function, except for the last one. All
convolutions have 4 × 4 kernels with stride 2, and padding of size 1.
The first 10 epochs for all models are trained with a fixed curvature starting at 0 and increasing
in absolute value each epoch. This corresponds to a burn-in period similarly to Nickel & Kiela
(2017). For learnable curvature approaches we then use Stochastic Gradient Descent with learning
rate 10-4 and let the optimizers adjust the value freely, for fixed curvature approaches it stays at the
last burn-in value. All our models use the Wrapped Normal distribution, or equivalently Gaussian in
Euclidean components, unless specified otherwise. All fixed curvature components are denoted with
a M1 or M-1 subscript, learnable curvature components do not have a subscript. The observation
model for the reconstruction loss term were Bernoulli distributions for MNIST and Omniglot, and
standard Gaussian distributions for BDP and CIFAR.
As baselines, we train VAEs with spaces that have a fixed constant curvature, i.e. assume a single
Riemannian manifold (potentially a product of them) as their latent space. It is apparent that our
models with a single component, like S1n correspond to Davidson et al. (2018) and Xu & Durrett
(2018), Hn-1 is equivalent to the Hyperbolic VAE of Nagano et al. (2019), Pn-c corresponds to the
Pc-VAE of Mathieu et al. (2019), and En is equivalent to the Euclidean VAE. In the following, we
present a selection of all the obtained results. For more information see Appendix E. Bold numbers
represent values that are particularly interesting. Since the Riemannian Normal and the von Mises-
Fischer distribution only have a spherical covariance matrix, i.e. a single scalar variance parameter
per component, we evaluate all our approaches with a spherical covariance parametrization as well.
Binary diffusion process For the BDP dataset and latent dimension 6 (Table 3), we observe that
all VAEs that only use the von Mises-Fischer distribution perform worse than a Wrapped Normal.
However, when a VMF spherical component was paired with other component types, it performed
better than if a Wrapped Normal spherical component was used instead. Riemannian Normal VAEs
did very well on their own - the fixed Poincare VAE (RN P-J3 obtains the best score. It did not
fare as well when we tried to learn curvature with it, however.
8
Published as a conference paper at ICLR 2020
Table 4: Summary of selected models (mean and standard deviation), latent space dimension 6 (left)
and 12 (right), diagonal covariance, on the MNIST dataset.
Model	LL	Model	LL
s1	-96.51±0.09	ISIp	-79.92±o.21
Dl	-97.89±0.10	(D21)6	-80.53±o.ιo
E6	-96.88±0.16	E12	-79.51±o.09
H-ι	-97.38±0.73	(H2-1)6	-80.54±o.23
P-1	-97.33±0.15	H1-21	-79.37±o.14
S6	-96.44±0.20	(P2-1)6	-80.39±o.O7
D6	-97.53±0.22	^s12	-79.99±o.27
(H2)3	-96.86±0.31	D12	-80.37±o.16
H6	-96.90±0.26	H12	-79.77±o.io
P6	-97.26±0.16	(P2)6	-80.31±o.O8
D2 x E2 x P2	-97.37±0.14	(D2)2 X (E2)2 X (P-I)2	-80.14±o.ii
D2 x E2 x P-ι	-97.29±0.16	D14 x E4 x P4-1	-80.14±o.2O
E2 x H2 x S2	-96.71±0.19	(E2)2 x (H2)2 x (S2)2	-79.59±o.25
E2 X H-ι X S2	-96.66±0.27	E4 x H4 x S4	-79.69±o.14
(U2)3	-97.06±0.13	(U2)6	-79.61±o.06
U6	-96.90±0.10	U12	-80.01±o.3O
An interesting observation is that all single-component VAEs M6 performed worse than product
VAEs (M2)3 when curvature was learned, across all component types. Our universal curvature VAE
(U2 )3 managed to get better results than all other approaches except for the Riemannian Normal
baseline, but it is within the margin of error of some other models. It also outperformed its single-
component variant U6. However, we did not find that it converged to specific curvature values, only
that they were in the approximate range of (-0.1, +0.1).
Dynamically-binarized MNIST reconstruction On MNIST (Table 3) with spherical covariance,
we noticed that VMF again under-performed Wrapped Normal, except when it was part of a product
like E2 X H2 X (VMF S2). When paired with another Euclidean and a Riemannian Normal Poincare
disk component, it performed well, but that might be because the RN P-1 component achieved best
results across the board on MNIST. It achieved good results even compared to diagonal covariance
VAEs on 6-dimensional MNIST. Several approaches are better than the Euclidean baseline. That
applies mainly to the above mentioned Riemannian Normal Poincare ball components, but also S6
both with Wrapped Normal and VMF, as well as most product space VAEs with different curvatures
(third section of the table). Our (U2)3 performed similarly to the Euclidean baseline VAE.
With diagonal covariance parametrization (Table 4), we observe similar trends. With a latent di-
mension of 6, the Riemannian Normal POinCare ball VAE is still the best performer. The Euclidean
baseline VAE achieved better results than its spherical covariance counterpart. Overall, the best
result is achieved by the single-component spherical model, with learnable curvature S6 . Interest-
ingly, all single-component VAEs performed better than their (M2)3 counterparts, except for the
H6 hyperboloid, but only by a tiny margin. Products of different component types also achieve
good results. Noteworthy is that their fixed curvature variants seem to perform marginally better
than learnable curvature ones. Our universal VAEs perform at around the Euclidean baseline VAE
performance. Interestingly, all of them end up with negative curvatures -0.3 < K < 0.
Secondly, we run our models with a latent space dimension of 12. We immediately notice, that
not many models can beat the Euclidean VAE baseline E12 consistently, but several are within the
margin of error. Notably, the product VAEs of H, S, and E, fixed and learnable H12 , and our
universal VAE (U2)6. Interestingly, products of small components perform better when curvature is
fixed, whereas single big component VAEs are better when curvature is learned.
Dynamically-binarized Omniglot reconstruction For a latent space of dimension 6 (Table 5),
the best of the baseline models is the Poincare VAE of (Mathieu et al., 2019). Our models that come
9
Published as a conference paper at ICLR 2020
Table 5: Summary of selected models (mean and standard deviation), latent space dimension 6,
diagonal covariance, on the Omniglot (left) and CIFAR-10 dataset (right), respectively.
Model	LL Model	LL
S61	-136.69±o.94	E6	-1896.19±2.54
D61	-137.42±i.20	H-1	-1888.23±2.12
E6	-136.05±o.29	P-1	-1893.27±o.61
H6-1	-137.09±o.O6 ~6^		-1893.85±o.36
P6-1	-135.86±0.20	s6	-1889.76±1.62
(S2)3	-136.14±o.27	P6	-1891.40±2.14
S6	-136.20±o.44	D2 X E2 X P2	-1899.90±4.60
(D2)3	-136.13±0.17	E2 x H2 x S2	-1895.46±o.92
D6	-136.30±0.08	(U2)3	-1895.09±4.27
(H2)3	-136.17±o.09			
H6	-136.24±o.32	
(P2)3	-136.09±o.O7	
P6	-136.05±o.44	
D2 × E2 × P2	-135.89±0.40	
E2 × H2 × S2	-135.93±o.48	
(U2)3	-136.21±o.07	
U6	-136.04±o.17	
very close to the average estimated marginal log-likelihood, and are definitely within the margin of
error, are mainly (S2)3, D2 × E2 × P2, and U6. However, with the variance of performance across
different runs, we cannot draw a clear conclusion. In general, hyperbolic VAEs seem to be doing
a bit better on this dataset than spherical VAEs, which is also confirmed by the fact that almost all
universal curvature models finished with negative curvature components.
CIFAR-10 reconstruction For a latent space of dimension 6, we can observe that almost all non-
Euclidean models perform better than the euclidean baseline E6 . Especially well-performing is the
fixed hyperboloid H6-1, and the learnable hypersphere S6. Curvatures for all learnable models on
this dataset converge to values in the approximate range of (-0.15, +0.15).
Summary In conclusion, a very good model seems to be the Riemannian Normal Poincare ball
VAE RN Pn . However, it has practical limitations due to a rejection sampling algorithm and an
unstable implementation. On the contrary, von Mises-Fischer spherical VAEs have almost consis-
tently performed worse than their Wrapped Normal equivalents. Overall, Wrapped Normal VAEs in
all constant curvature manifolds seem to perform well at modeling the latent space.
A key takeaway is that our universal curvature models Un and (U2)bn/2c seem to generally out-
perform their corresponding Euclidean VAE baselines in lower-dimensional latent spaces and, with
minor losses, manage to keep most of the competitive performance as the dimensionality goes up,
contrary to VAEs with other non-Euclidean components.
5	Conclusion
By transforming the latent space and associated prior distributions onto Riemannian manifolds of
constant curvature, it has previously been shown that we can learn representations on curved space.
Generalizing on the above ideas, we have extended the theory of learning VAEs to products of con-
stant curvature spaces. To do that, we have derived the necessary operations in several models of
constant curvature spaces, extended existing probability distribution families to these manifolds, and
generalized VAEs to latent spaces that are products of smaller “component” spaces, with learnable
curvature. On various datasets, we show that our approach is competitive and additionally has the
property that it generalizes the Euclidean variational autoencoder - if the curvatures of all compo-
nents go to 0, we recover the VAE of Kingma & Welling (2014).
10
Published as a conference paper at ICLR 2020
Acknowledgments
We would like to thank Andreas Bloch for help in verifying some of the formulas for constant
curvature spaces and for many insightful discussions; Prof. Thomas Hofmann and the Data Analytics
Lab, the Leonhard cluster, and ETH Zurich for GPU access.
Work was done while all authors were at ETH Zurich.
Ondrej Skopek (oskopek@google.com) is now at Google.
Octavian-Eugen Ganea (oct@mit.edu) is now at the Computer Science and Artificial Intelligence
Laboratory, Massachusetts Institute of Technology.
Gary Becigneul (gary.becigneul@inf.ethz.ch) is funded by the Max Planck ETH Center
for Learning Systems.
References
Herve Abdi and Lynne J. Williams. Principal component analysis. Wiley Interdisciplinary Reviews:
Computational Statistics, 2(4):433-459, 2010. doi: 10.1002∕wics.101.
Georgios Arvanitidis, Lars Kai Hansen, and S0ren Hauberg. Latent space oddity: on the curvature
of deep generative models. In International Conference on Learning Representations, 2018.
Gregor Bachmann, Gary Becigneul, and Octavian-Eugen Ganea. Constant Curvature Graph Convo-
lutional Networks, 2020. URL https://openreview.net/forum?id=BJg73xHtvr.
Kayhan Batmanghelich, Ardavan Saeedi, Karthik Narasimhan, and Sam Gershman. Nonparametric
Spherical Topic Modeling with Word Embeddings. In Proceedings of the 54th Annual Meeting of
the Association for Computational Linguistics (Volume 2: Short Papers), pp. 537-542. Associa-
tion for Computational Linguistics, 2016. doi: 10.18653/v1/P16-2087.
Gary BeCigneUI and Octavian-Eugen Ganea. Riemannian Adaptive Optimization Methods. In In-
ternational Conference on Learning Representations, 2019.
Marcel Berger. A panoramic view of Riemannian geometry. Springer Science & Business Media,
2012.
Janos Bolyai. Appendix, Scientiam Spatii absolute veram exhibens: a veritate aut falsitate Axioma-
tis XI. Euclidei (a priori haud unquam decidenda) independentem; adjecta ad casum falsitatis,
quadratura circuli geometrica. Auctore Johanne Bolyai de eadem, Geometrarum in Exercitu Cae-
sareo Regio Austriaco Castrensium Capitaneo. Coll. Ref., 1832.
Silvere Bonnabel. Stochastic Gradient Descent on Riemannian Manifolds. IEEE Transactions on
Automatic Control, 58:2217-2229, 2013.
Samuel R. Bowman, Luke Vilnis, Oriol Vinyals, Andrew Dai, Rafal Jozefowicz, and Samy Bengio.
Generating Sentences from a Continuous Space. In Proceedings of The 20th SIGNLL Confer-
ence on Computational Natural Language Learning, pp. 10-21. Association for Computational
Linguistics, 2016. doi: 10.18653/v1/K16-1002.
M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and P. Vandergheynst. Geometric Deep Learning:
Going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4):18-42, July 2017. ISSN
1053-5888. doi: 10.1109/MSP.2017.2693418.
Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance Weighted Autoencoders. In
Proceedings of the 4th International Conference on Learning Representations, ICLR 2016, 2016.
James W Cannon, William J Floyd, Richard Kenyon, Walter R Parry, et al. Hyperbolic geometry.
Flavors of geometry, 31:59-115, 1997.
Xi Chen, Diederik P. Kingma, Tim Salimans, Yan Duan, Prafulla Dhariwal, John Schulman, Ilya
Sutskever, and Pieter Abbeel. Variational Lossy Autoencoder. In Proceedings of the 2nd Interna-
tional Conference on Learning Representations, ICLR 2014, 2014.
11
Published as a conference paper at ICLR 2020
Tim R. Davidson, Luca Falorsi, Nicola De Cao, Thomas Kipf, and Jakub M. Tomczak. Hyperspher-
ical Variational Auto-Encoders. In UAI, 2018.
Bhuwan Dhingra, Christopher J Shallue, Mohammad Norouzi, Andrew M Dai, and George E Dahl.
Embedding Text in Hyperbolic Spaces. arXiv preprint arXiv:1806.04313, 2018.
Robert L. Foote. A Unified Pythagorean Theorem in Euclidean, Spherical, and Hyperbolic Geome-
tries. MathematicsMagazine, 90(1):59-69, 2017. ISSN0025570X,19300980.
Octavian Ganea, Gary BecigneUL and Thomas Hofmann. Hyperbolic Neural Networks. In S. Ben-
gio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in
Neural Information Processing Systems 31, pp. 5345-5355. Curran Associates, Inc., 2018a.
Octavian-Eugen Ganea, Gary Becigneul, and Thomas Hofmann. Hyperbolic Entailment Cones for
Learning Hierarchical Embeddings. arXiv preprint arXiv:1804.01882, 2018b.
Mevlana C Gemici, Danilo Rezende, and Shakir Mohamed. Normalizing Flows on Riemannian
Manifolds. arXiv preprint arXiv:1611.02304, 2016.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative Adversarial Nets. In Advances in Neural Infor-
mation Processing Systems, pp. 2672-2680, 2014.
Albert Gu, Frederic Sala, Beliz Gunel, and Christopher Re. Learning Mixed-Curvature Representa-
tions in Product Spaces. In International Conference on Learning Representations, 2019.
S0ren Hauberg. Directional Statistics with the Spherical Normal Distribution. In 201821st Interna-
tional Conference on Information Fusion (FUSION), pp. 704-711. IEEE, 2018.
Chin-Wei Huang, David Krueger, Alexandre Lacoste, and Aaron Courville. Neural autoregressive
flows. arXiv preprint arXiv:1804.00779, 2018.
Edwin T Jaynes. Information theory and statistical mechanics. Physical review, 106(4):620, 1957.
Yoon Kim, Kelly Zhang, Alexander M Rush, Yann LeCun, et al. Adversarially regularized autoen-
coders. arXiv preprint arXiv:1706.04223, 2017.
Diederik P. Kingma and Jimmy Lei Ba. Adam: A Method for Stochastic Optimization. International
Conference on Learning Representations (ICLR), 2015.
Diederik P Kingma and Max Welling. Auto-Encoding Variational Bayes. In Proceedings of the 2nd
International Conference on Learning Representations, ICLR 2014, 2014.
Max Kochurov, Sergey Kozlukov, Rasul Karimov, and Viktor Yanush. Geoopt: Adaptive Rieman-
nian optimization in PyTorch. https://github.com/geoopt/geoopt, 2019.
Alex Krizhevsky. Learning Multiple Layers of Features from Tiny Images. Technical report, Uni-
versity of Toronto, 2009.
Brenden M. Lake, Ruslan Salakhutdinov, and Joshua B. Tenenbaum. Human-level concept learning
through probabilistic program induction. Science, 350(6266):1332-1338, 2015. ISSN 0036-8075.
doi: 10.1126/science.aab3050.
Johann H Lambert. Observations trigonometriques. Mem. Acad. Sci. Berlin, 24:327-354, 1770.
Marc T Law, Jake Snell, and Richard S Zemel. Lorentzian distance learning, 2019.
Yann LeCun. The MNIST database of handwritten digits, 1998. URL http://yann.lecun.
com/exdb/mnist/.
J.M. Lee. Riemannian Manifolds: An Introduction to Curvature. Graduate Texts in Mathematics.
Springer New York, 1997. ISBN 9780387983226.
Hongbo Li, David Hestenes, and Alyn Rockwood. A Universal Model for Conformal Geometries
of Euclidean, Spherical and Double-Hyperbolic Spaces, pp. 77-104. Springer Berlin Heidelberg,
Berlin, Heidelberg, 2001. ISBN 978-3-662-04621-0. doi: 10.1007/978-3-662-04621-0_4.
12
Published as a conference paper at ICLR 2020
Francesco Locatello, Stefan Bauer, Mario Lucic, Sylvain Gelly, Bernhard SchOlkopf, and Olivier
Bachem. Challenging common assumptions in the unsupervised learning of disentangled repre-
sentations. arXiv preprint arXiv:1811.12359, 2018.
Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey. Adversarial
autoencoders. arXiv preprint arXiv:1511.05644, 2015.
Emile Mathieu, Charline Le Lan, Chris J Maddison, Ryota Tomioka, and Yee Whye Teh. Hierarchi-
cal Representations with Poincare Variational Auto-Encoders. arXivpreprint arXiv:190L06033,
2019.
Lolc Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed,
and Alexander Lerchner. beta-VAE: Learning Basic Visual Concepts with a Constrained Varia-
tional Framework. In Proceedings of the 5th International Conference on Learning Representa-
tions, ICLR 2017, 2017.
Yoshihiro Nagano, Shoichiro Yamaguchi, Yasuhiro Fujita, and Masanori Koyama. A differen-
tiable gaussian-like distribution on hyperbolic space for gradient-based learning. arXiv preprint
arXiv:1902.02992, 2019.
Maximilian Nickel and Douwe Kiela. Learning Continuous Hierarchies in the Lorentz Model of
Hyperbolic Geometry. In Proceedings of the 35th International Conference on International
Conference on Machine Learning - Volume 50, ICML’18, 2018.
Maximillian Nickel and Douwe Kiela. Poincare Embeddings for Learning Hierarchical Represen-
tations. In Advances in Neural Information Processing Systems 30, pp. 6338-6347. Curran Asso-
ciates, Inc., 2017.
Shirui Pan, Ruiqi Hu, Guodong Long, Jing Jiang, Lina Yao, and Chengqi Zhang. Adversarially
regularized graph autoencoder for graph embedding. arXiv preprint arXiv:1802.04407, 2018.
Xavier Pennec. Intrinsic Statistics on Riemannian Manifolds: Basic Tools for Geometric Measure-
ments. Journal of Mathematical Imaging and Vision, 25(1):127, Jul 2006. ISSN 1573-7683. doi:
10.1007/s10851-006-6228-4.
Peter Petersen, S Axler, and KA Ribet. Riemannian Geometry, volume 171. Springer, 2006.
John Ratcliffe. Foundations of Hyperbolic Manifolds, volume 149. Springer Science & Business
Media, 2006.
Danilo Jimenez Rezende and Shakir Mohamed. Variational Inference with Normalizing Flows.
arXiv preprint arXiv:1505.05770, 2015.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic Backpropagation and
Approximate Inference in Deep Generative Models. arXiv preprint arXiv:1401.4082, 2014.
Frederic Sala, Chris De Sa, Albert Gu, and Christopher Re. Representation tradeoffs for hyper-
bolic embeddings. In Proceedings of the 35th International Conference on Machine Learning,
volume 80 of Proceedings of Machine Learning Research, pp. 4460-4469, Stockholmsmassan,
Stockholm Sweden, 10-15 Jul 2018. PMLR.
Ruslan Salakhutdinov and Iain Murray. On the Quantitative Analysis of Deep Belief Networks. In
Proceedings of the 25th International Conference on Machine Learning, ICML’08, pp. 872-879,
New York, NY, USA, 2008. ACM. ISBN 978-1-60558-205-4. doi: 10.1145/1390156.1390266.
Martin Simonovsky and Nikos Komodakis. GraphVAE: Towards generation of small graphs using
variational autoencoders. In International Conference on Artificial Neural Networks, pp. 412-
422. Springer, 2018.
John Parr Snyder. Map ProjeCtions-A working manual, volume 1395. US Government Printing
Office, 1987.
Alexandru Tifrea, Gary Becigneul, and Octavian-Eugen Ganea. Poincare Glove: Hyperbolic Word
Embeddings. In International Conference on Learning Representations, 2019.
13
Published as a conference paper at ICLR 2020
Abraham Albert Ungar. A Gyrovector Space Approach to Hyperbolic Geometry. Synthesis Lectures
on Mathematics and Statistics, 1(1):1-194, 2008.
Benjamin Wilson and Matthias Leimeister. Gradient descent in hyperbolic space. arXiv preprint
arXiv:1805.08207, 2018.
Richard C. Wilson and Edwin R. Hancock. Spherical embedding and classification. In Edwin R.
Hancock, Richard C. Wilson, Terry Windeatt, Ilkay Ulusoy, and Francisco Escolano (eds.), Struc-
tural, Syntactic, and Statistical Pattern Recognition, pp. 589-599, Berlin, Heidelberg, 2010.
Springer Berlin Heidelberg. ISBN 978-3-642-14980-1.
Jiacheng Xu and Greg Durrett. Spherical Latent Spaces for Stable Variational Autoencoders. In
Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp.
4503-4513. Association for Computational Linguistics, 2018.
JUn-Yan Zhu, PhiliPP Krahenbuhl, Eli Shechtman, and Alexei A. Efros. Generative Visual Manip-
ulation on the Natural Image Manifold. In Proceedings of European Conference on Computer
Vision (ECCV), 2016.
14
Published as a conference paper at ICLR 2020
A Geometrical details
A.1 Riemannian geometry
An elementary notion in Riemannian geometry is that of a real, smooth manifold M ⊆ Rn , which
is a collection of real vectors x that is locally similar to a linear space, and lives in the ambient
space Rn . At each point of the manifold x ∈ M a real vector space of the same dimensionality as
M is defined, called the tangent space at point x: TxM. Intuitively, the tangent space contains all
the directions and speeds at which one can pass through x. Given a matrix representation G(x) ∈
Rn×n of the Riemannian metric tensor g(x), we can define a scalar product on the tangent space:
h∙, ∙iχ : TxM X TxM → M, where ha, b〉x = g(x)(a, b) = aτG(x)b for any a, b ∈ TXM. A
Riemannian manifold is then the tuple (M, g). The scalar product induces a norm on the tangent
space TxM: ∣∣a∣∣χ = ʌ/ha, a)χ ∀a ∈ TxM (Petersen et al., 2006).
Although it seems like the manifold only defines a local geometry, it induces global quantities by
integrating the local contributions. The metric tensor induces a local infinitesimal volume element
on each tangent space TxM and hence a measure is induced as well dM(x) =，|G(x)|dx where
dx is the Lebesgue measure. The length of a curve γ : t 7→ γ(t) ∈ M, t ∈ [0, 1] is given by
L(Y)=R01 qιι ddt γ(t)ι∣γ(t)dt.
Straight lines are generalized to constant speed curves giving the shortest path between pairs of
points x, y ∈ M, socalled geodesics, for which it holds that γ* = argminγ L(Y), such that γ(0)=
x, Y⑴ = y, and ∣∣今7(力)||)伯)= 1. Global distances are thus induced on M by dM(x, y) =
infγ L(Y). Using this metric, we can go on to define a metric space (M, dM). Moving from a
point x ∈ M in a given direction v ∈ TxM with constant velocity is formalized by the exponential
map: expx : TxM → M. There exists a unique unit speed geodesic Y such that Y(0)
x and
dγ⑶I
dt lt=0
v, where v ∈ TxM.
The corresponding exponential map is then defined as expx(v) = Y(1). The logarithmic map is the
inverse logx = expx-1 : M → TxM. For geodesically complete manifolds, i.e. manifolds in which
there exists a length-minimizing geodesic between every x, y ∈ M, such as the Lorentz model,
hypersphere, and many others, expx is well-defined on the full tangent space TxM.
To connect vectors in tangent spaces, we use parallel transport PTx→y : TxM → TyM, which is
an isomorphism between the two tangent spaces, so that the transported vectors stay parallel to the
connection. It corresponds to moving tangent vectors along geodesics and defines a canonical way
to connect tangent spaces.
A.2 B rief comparison of constant curvature space models
We have seen five different models of constant curvature space, each of which has advantages and
disadvantages when applied to learning latent representations in them using VAEs.
A big advantage of the hyperboloid and hypersphere is that optimization in the spaces does not
suffer from as many numerical instabilities as it does in the respective projected spaces. On the
other hand, we have seen that when K → 0, the norms of points go to infinity. As we will see in
experiments, this is not a problem when optimizing curvature within these spaces in practice, except
if we’re trying to cross the boundary at K = 0 and go from a hyperboloid to a sphere, or vice versa.
Intuitively, the points are just positioned very differently in the ambient space of H- and S, for a
small > 0.
Since points in the n-dimensional projected hypersphere and POinCare ball models can be repre-
sented using a real vector of length n, it enables us to visualize points in these manifolds directly
for n = 2 or even n = 3. On the other hand, optimizing a function over these models is not very
well-conditioned. In the case of the POinCare ball, a significant amount of points lie close to the
boundary of the ball (i.e. with a squared norm of almost 1/K), which causes numerical instabilities
even when using 64-bit float precision in computations.
A similar problem occurs with the projected hypersphere with points that are far away from the
origin 0 (i.e. points that are close to the “South pole” on the backprojected sphere). Unintuitively,
15
Published as a conference paper at ICLR 2020
all points that are far away from the origin are actually very close to each other with respect to the
induced distance function and very far away from each other in terms of Euclidean distance.
Both distance conversion theorems (A.5 and its projected hypersphere counterpart) rely on the points
being fixed when changing curvature. If they are somehow dependent on curvature, the convergence
theorem does not hold. We conjecture that if points stay close to the boundary in P or far away
from 0 in D as K → 0, this is exactly the reason for numerical instabilities (apart from the standard
numerical problem of representing large numbers in floating-point notation).
Because of the above reasons, we do some of our experiments with the projected spaces and others
with the hyperboloid and hypersphere, and aim to compare the performance of these empirically as
well.
A.3 Euclidean geometry
A.3.1 Euclidean space
Distance function The distance function in En is
dE(x, y) = kx -yk2 .
Due to the Pythagorean theorem, we can derive that
kx - yk22 = hx - y,x - yi2 = kxk22 - 2 hx, yi2 + kyk22
= kxk22 + kyk22 - 2 kxk2 kyk2 cos-1 θx,y
Exponential map The exponential map in En is
expx (v) = x + v.
The fact that the resulting points belong to the space is trivial. Deriving the inverse function, i.e. the
logarithmic map, is also trivial:
logx (y) = y - x.
Parallel transport We do not need parallel transport in the Euclidean space, as we can directly
sample from a Normal distribution. In other words, we can just define parallel transport to be an
identity function.
A.4 Hyperbolic geometry
A.4. 1 Hyperboloid
Do note, that all the theorems for the hypersphere are essentially trivial corollaries of their equiva-
lents in the hypersphere (and vice-versa) (Section A.5.1). Notable differences include the fact that
R2 = - -K, not R2 = -K, and all the operations use the hyperbolic trigonometric functions sinh,
cosh, and tanh, instead of their Euclidean counterparts. Also, we often leverage the “hyperbolic”
Pythagorean theorem, in the form cosh2 (α) - sinh2 (α) = 1.
Projections Due to the definition of the space as a retraction from the ambient space, we can
project a generic vector in the ambient space to the hyperboloid using the shortest Euclidean distance
by normalization:
xx
ProjHK (X) = R PiL = √K∣iχiL.
Secondly, the n + 1 coordinates of a point on the hyperboloid are co-dependent; they satisfy the
relationhx, XiL = 1/K. This implies, that if we are given a vector with n coordinates X =
(x2, . . . , xn+1), we can compute the missing coordinate to place it onto the hyperboloid:
χι = JkXk2- K.
This is useful for example in the case of orthogonally projecting points from Tμ° HK onto the mani-
fold.
16
Published as a conference paper at ICLR 2020
Distance function The distance function in HnK is
dK(x, y) = R ∙ θχ,y = RCoshT (-hxRyiL) = √=-κ CoshT (-K {χ, y L .
Remark A.1 (About the divergence of points in HnK). Since the points on the hyperboloid x ∈ HnK
are norm-constrained to
1
hx, XiL = K，
all the points on the hyperboloid go to infinity as K goes to 0- from below:
lim hx, xiL= -∞.
K→0-	L
This confirms the intuition that the hyperboloid grows “flatter”, but to do that, it has to go away
from the origin of the coordinate space 0. A good example of a point that diverges is the origin of
the hyperboloid μK = (1/K, 0,..., 0)t = (R, 0,..., 0)t . That makes this model unsuitable for
trying to learn sign-agnostic curvatures, similarly to the hypersphere.
Exponential map The exponential map in HnK is
expK(V) = Cosh (啥)x + sinh (牛)|RL,
and in the case of X := μo = (R, 0,..., 0)t:
expK0(V) = (cosh (喈)R; sinh (||vr) ∏⅞vT)T,
where V = (0； VT)T and ||v||l = ∣∣v∣∣2 = ∣∣V∣∣2.
Theorem A.2 (Logarithmic map in HnK). For all X, y ∈ HnK, the logarithmic map in HnK maps y
to a tangent vector at X:
logK(y) = Cosh2 (α) (y - αX),
α2- 1
where α = K hx, yiL.
Proof. We show the detailed derivation of the logarithmic map as an inverse function to the expo-
nential map logx(y) = expx-1(y), adapted from (Nagano et al., 2019).
As mentioned previously,
y = expK (V) = Cosh (%)x + sinh (%) kRvL.
Solving for V, we obtain
V=Hfc (y -Cosh (%)x
However, we still need to rewrite ∣∣V∣∣L in evaluatable terms:
0=hx, ViL Rdhv⅛
hx, y〉c - Cosh (kRL) hx,xi}
^R2^^
hence
Cosh (k⅛) = -R2 hx, yiL,
and therefore
∣∣V∣∣L = R CoshT (-R2 hx, y〉J = √-K CoshT(K hx, y)£) = dK (x, y).
17
Published as a conference paper at ICLR 2020
Plugging the result back into the first equation, we obtain
v
||v||L
R Sinh k kvk
y - cosh ( kvRL ) X
----RcoSh Im)_ (y - Cosh (1 Rcosh-1 (α)) X
Rsinh (RRRcosh-1 (α))<	RR	'
——CoSɪ―(α)——(y - CoSh(COShT(α))χ)
sinh(cosh-1(α))
cosh-1(α)
√02-T(y -ax)，
where ɑ = -R2(x, y}c = Khx, y}c , and the last equality assumes ∣α∣ > 1. This assumption
holds, since for all points X, y ∈ HnK it holds that hX, yiL ≤ -R2, and hX, yiL = -R2 if and only
if x = y, due to Cauchy-Schwarz (Ratcliffe, 2006, Theorem 3.1.6). Hence, the only case where this
would be a problem would be if X = y, but it is clear that the result in that case is U = 0.	□
Parallel transport Using the generic formula for parallel transport in manifolds for x, y ∈ M
and v ∈ TxM
K	DlogxK (y), v E	K	K
PTx→y (V) = V - -dM (χ, y) X (IOgx (y) + logy (X)),	⑴
and the logarithmic map formula from Theorem A.2
logK(y) = CoSh2 (α) (y - ax),
α2 - 1
where α =-焉(x, y)£ , We derive parallel transport in HK:
PTK→y(V) =V + R2-hXLyL(x + y).
A special form of parallel transport exists for when the source vector is μo = (R, 0,..., 0)t:
∕yι + R∖
PTK	(V)= V + hy, vi2 I	y2
μo→y (V)	v + R2 + Ry1	..
yn+1
A.4.2 POINCARE BALL
Do note, that all the theorems for the projected hypersphere are essentially trivial corollaries of their
equivalents in the Poincare ball (and vice-versa) (Section A.5.2). Notable differences include the fact
that R2 = - KK, not R2 = K, and all the operations use the hyperbolic trigonometric functions sinh,
Cosh, and tanh, instead of their Euclidean counterparts. Also, we often leverage the “hyperbolic”
Pythagorean theorem, in the form Cosh2 (α) - sinh2 (α) = 1.
Stereographic projection
Theorem A.3 (Stereographic backprojected points of PnK belong to HnK). For all y ∈ PnK,
llρK1(y)l∣L = K.
Proof.
T l2
K kyk22 - 1	2yT
llρ-K1(y)ll2L
IKIKkyk2 + ι; Kkyk2 + ι
L
18
Published as a conference paper at ICLR 2020
-1 ɪ K kyk2- 1! 2 +	4 kyk2
∖P∣Ki K kyk2 + 1)	(K kyk2 + 1)2
ι -(K kyk2 -1)2 + 4∣κ∣kyk2
IKI	(K kyk2 + i)2
1 -(K kyk2-1)2-4K kyk2
-K (K kyk2 + i)2
1 (K kyk2- 1)2 +4K kyk2
K	(K kyk2 + 1)2
ɪ K 2 kyk2 + 2K kyk2 + 1
K (K kyk2 + 1)2
1 (K kyk2 + 1)2 _ 1
-------------- - -.
K (K kyk2 + 1)2 K
□
Distance function The distance function in PnK is (derived from the hyperboloid distance function
using the stereographic projection ρK):
dP(x, y) = dH(ρ-K1 (x), ρ-K1 (y))
=，COsh-I 11______________2K kx - yk2______
=√-K	\	(1 + K kxk2)(1 + K kyk2)
=R cosh-1(1+	2R2kx - yk2	!
V	(R2 -kχk2)(R2 -kyk2);
Theorem A.4 (Distance equivalence in PnK). For all K < 0 and for all pairs of points x, y ∈ PnK,
the Poincare distance between them equals the gyrospace distance
dP(x, y) = dPgyr (x, y).
Proof. Proven using Mathematica (File: distance_limits.ws), proof involves heavy algebra.
□
Theorem A.5 (Gyrospace distance converges to Euclidean in PnK). For any fixed pair of points
x, y ∈ PK, the Poincare gyrospace distance between them converges to the Euclidean distance in
the limit (up to a constant) as K → 0-:
lim
K→0-
dPgyr (x, y)
2kx-yk2.
Proof.
lim dP r(x, y) = 2 lim
K→0- gyr	K→0-
tanhT(√-K k-x ㊉K y∣∣2)
2 lim
K→0-
tanhT(√-K ky - x∣∣2)
2ky-xk2,
where the second equality holds because of the theorem of limits of composed functions, where
tanh-1(a ʌ/-K)
"-√-K-
19
Published as a conference paper at ICLR 2020
g(K) = k-x ㊉ K yk2
We see that
Kl→im0- g(K) = ky - xk2
due to Theorem A.14, and
lim	f(a)
a→kx-yk2
tanh-1(a ʌ/-K)
Additionally for the last equality, we need the fact that
tanh-1(a ∙∖∕∣XI)
lim--------∣—----= a.
x→0	E
□
Theorem A.6 (Distance converges to Euclidean as K → 0- in PnK). For any fixed pair of points
x, y ∈ PK, the Poincare distance between them converges to the Euclidean distance in the limit (Up
to a constant) as K → 0- :
lim dP(x, y)
K→0-
2kx-yk2.
Proof. Theorem A.4 and A.5.
□
Exponential map As derived and proven in Ganea et al. (2018a), the exponential map in PnK and
its inverse is
expK (V)=X ㊉K (tanh (√-KλK臂)√-⅛ξ)
logK ⑹=√-⅛ tanh-1 -k-x ㊉K yk2)F≡K⅛
In the case of X := μo = (0,..., 0)T they simplify to:
expK0 (V) = tanh (L kvθ √-Kvkvk2
logKo (y) = tanh-1 (√-K kylb) ∣7-y-.
kyk2
Parallel transport Kochurov et al. (2019); Ganea et al. (2018a) have also derived and imple-
mented the parallel transport operation for the Poincare ball:
λK
PTxTy(V) = λK gyr[y, -x]v,
λy
2
PTμo→y (V) = λK v，
y
λK
PTK→μo (V) =十V，
where
gyr[x, y]V = -(x ㊉K y) ㊉K (x ㊉K (y ㊉K V))
is the gyration operation (Ungar, 2008, Definition 1.11).
Unfortunately, on the Poincare ball,〈•，∙)x has a form that changes with respect to x, unlike in the
hyperboloid.
20
Published as a conference paper at ICLR 2020
A.5 Spherical geometry
A.5.1 Hypersphere
All the theorems for the hypersphere are essentially trivial corollaries of their equivalents in the
hyperboloid (Section A.4.1). Notable differences include the fact that R2 = K, not R2 = - K,
and all the operations use the Euclidean trigonometric functions sin, cos, and tan, instead of their
hyperbolic counterparts. Also, we often leverage the Pythagorean theorem, in the form sin2 (α) +
cos2(α) = 1.
Projections Due to the definition of the space as a retraction from the ambient space, we can
project a generic vector in the ambient space to the hypersphere using the shortest Euclidean distance
by normalization:
xx
Proj 殳 n-ι (x) = R-—— = -^=----.
SK	l∣χ∣∣2	√K ∣∣χ∣∣2
Secondly, the n + 1 coordinates of a point on the sphere are co-dependent; they satisfy the relation
hx, x〉2 = 1/K. This implies, that if We are given a vector with n coordinates X = (x2,..., xn+ι),
we can compute the missing coordinate to place it onto the sphere:
χ1 = ∖JK - kxk2.
This is useful for example in the case of orthogonally projecting points from Tμo SK onto the mani-
fold.
Distance function The distance function in SnK is
dKK (x, y) = R ∙ θχ,y = R cos-1
√K CoST(K hx, yi2).
Remark A.7 (About the divergence of points in SnK). Since the points on the hypersphere x ∈ SnK
are norm-constrained to
1
hx, xi2 = K，
all the points on the sphere go to infinity as K goes to 0+ from above:
lim hx, xi = ∞.
K→0+	2
This confirms the intuition that the sphere grows “flatter”, but to do that, it has to go away from the
origin of the coordinate space 0. A good example of a point that diverges is the north pole of the
sphere μK = (1/K, 0,..., 0)T = (R, 0,..., 0)t. That makes this model unsuitable for trying to
learn sign-agnostic curvatures, similarly to the hyperboloid.
Exponential map The exponential map in SnK is
expK (V)=Cos (呼)x+Sin (呼)高.
Theorem A.8 (Logarithmic map in SnK). For all x, y ∈ SnK, the logarithmic map in SnK maps y to
:	logK (y)「y …
1 - α2
where α = K hx, yi2.
Proof. Analogous to the proof of Theorem A.2.
As mentioned previously,
y = expKK (V) = cos k -ɪ ) x + sin k kR2
Rv
⅛ .
21
Published as a conference paper at ICLR 2020
Solving for v, we obtain
l∣v∣∣2	(
、 y -cos
乎X
v
RSin (*) '	'
However, we still need to rewrite ||v||2 in evaluatable terms:
0=hx, vi2=d⅛
hx, yi2 一cos (kR2) ∣χ,χi}
R2
hence
cos (端)=R hx, yi2,
and therefore
||v||2 = R COST	hx, y%) = √k COST(K hx, yi2) = dK(X, y).
Plugging the result back into the first equation, we obtain
v =	1 吁“ ∖ (y 一 cos (kvk2) X)
R sin (*) V RRJJ
_R COST 3	(y - cos (1R cos-1
Rsin (RRcos-1 (α)) ∖	RR
X
cos-1 (α)
sin(cos-1(α))
(y - cos(cos-1(α))x)
cos-1(α)
√1 — α2
(y - αx),
where α = 击(x, y)？ = K〈x, y)？, and the last equality assumes ∣α∣ > 1. This assumption holds,
since for all points x, y ∈ SnK it holds that hx, yi2 ≤ R2, and hx, yi2 = R2 if and only if x
y,
due to Cauchy-Schwarz (Ratcliffe, 2006, Theorem 3.1.6). Hence, the only case where this would be
a problem would be if x = y, but it is clear that the result in that case is u = 0.
□
Parallel transport Using the generic formula for parallel transport in manifolds (Equation A.4.1)
for x, y ∈ SnK and v ∈ TxSnK and the spherical logarithmic map formula
logxK(y)
cos-1 (α)
√1 — a2
(y - αx),
where α = K hx, yi2 , we derive parallel transport in SnK :
PTxK→y(v) =v—
v—
hy, vi2
R2 + (χ, y)
K hy, vi2
(x + y)
2
1+Khx,yi2
(x+y).
A special form of parallel transport exists for when the source vector is μo = (R, 0,..., 0)T:
PTK	(V) = v — hy, vi2
PT μ0→y (V)= V - R2 + Ryi
∕yι + R∖
y2
yn+1
A.5.2 Projected hypersphere
Do note, that all the theorems for the projected hypersphere are essentially trivial corollaries of their
equivalents in the Poincare ball (and vice-versa) (Section A.4.2). Notable differences include the
fact that R2 = 六,not R2 = 一 K, and all the operations use the Euclidean trigonometric functions
sin, cos, and tan, instead of their hyperbolic counterparts. Also, we often leverage the Pythagorean
theorem, in the form sin2(α) + cos2(α) = 1.
22
Published as a conference paper at ICLR 2020
Stereographic projection
Remark A.9 (Homeomorphism between SnK and Rn). We notice that ρK is not a homeomorphism
between the n-dimensional sphere and Rn, as it is not defined at —μo = (—R; 0T)T. Ifwe addi-
tionally changed ComPactfied the plane by adding a point “at infinity” and set it equal to PK (μo),
ρK would become a homeomorphism.
Theorem A.10 (Stereographic backprojected points of DnK belong to SnK). For all y ∈ DnK,
ι∣ρκ1(y)ι∣2 = K.
Proof.
T ι2
K kyk2—1.	2yT
ιιρ-K1(y)ιι22
K K kyk2 + 1; K kyk2 + 1
2
1 ɪ K IM2 — 1 1 2 +	4 ky∣∣2
\P|K| K kyk2 + 1)	(K kyk2 + 1)2
1 (K kyk2 — 1)2 +4∣KIkyk2
阳	(K kyk2 + 1)2
1 (K kyk2 — 1)2 +4K kyk2
K (K kyk2 + 1)2
ɪ K2 kyk2 + 2K kyk2 + 1
K	(K kyk2 + 1)2
1 (K kyk2 + 1)2	1
=	.
K (K kyk2 + 1)2 K
□
Distance function The distance function in DnK is (derived from the spherical distance function
using the stereographic projection ρK):
dD(x, y) = dS(ρ-K1(x), ρ-K1(y))
= LOs-I 11_____________2K kx - yk2_____!
√K	V (1 + K kxk2)(1 + K kyk2”
=Rcos-1 11---------2R2 kx — yk2~!
V	(R2 + kxk2)(R2 + kyk2)J
Theorem A.11 (Distance equivalence in DnK). For all K > 0 and for all pairs of points x, y ∈ DnK,
the spherical projected distance between them equals the gyrospace distance
dD(x, y) = dDgyr (x, y).
Proof. Proven using Mathematica (File: distance_limits.ws), proof involves heavy algebra.
□
Theorem A.12 (Gyrospace distance converges to Euclidean in DnK). For any fixed pair of points
x, y ∈ DnK, the spherical projected gyrospace distance between them converges to the Euclidean
distance in the limit (up to a constant) as K → 0+:
lim+ dDgyr (x, y) = 2 kx — yk2 .
23
Published as a conference paper at ICLR 2020
Proof.
tan-1(√K k-x ㊉K y∣b)
√K
tan-1(√K ∣∣y - x∣b)
√K
2ky-xk2,
lim dD r(x, y) = 2 lim
K→0+	gyr	K→0+
= 2 lim
K→0+
where the second equality holds because of the theorem of limits of composed functions, where
f(a)
tan-1(a√,K)
√K
g(K) = Il-x ㊉K yk2 .
We see that
lim g(K) = ky - xk2
K→0-
due to Theorem A.14, and
lim	f(a)
a→kx-yk2
tan-1(a√K)
√K
Additionally for the last equality, we need the fact that
lim
x→0
tanh-1(a ∙∖∕∣x∣)
VZM
a.
□
Theorem A.13 (Distance converges to Euclidean as K → 0+ in DnK). For any fixed pair of points
x, y ∈ DnK, the spherical projected distance between them converges to the Euclidean distance in
the limit (up to a constant) as K → 0+:
lim dD(x, y) = 2 Ix - yI2 .
K→0+
Proof. Theorem A.11 and A.12.
□
Exponential map Analogously to the derivation of the exponential map in PnK in Ganea et al.
(2018a, Section 2.3-2.4), We can derive MobiUs scalar multiplication in DK:
r 0k X
1
i√K
1
i√K
tanh(r tanh-1 (i√K |罔卜))
x
Ilxk2
x
kxk2
1	,	1 , r—....... X
=√K tan(r tan (VZK Ilxk2)) ∣χ∣ ,
Where We use the fact that tanh-1(ix) = i tan-1(x) and tanh(ix) = i tan(x). We can easily see
that 1 0k x = x.
Hence, the geodesic has the form of
Yx→y(t) = X ㊉K t 乳K (-X ㊉K y),
and therefore the exponential map in DnK is:
expK(v) = x ㊉K tan
√K kvk2	.
v
24
Published as a conference paper at ICLR 2020
The inverse formula can also be computed:
logK (y) = √⅛ tan-1 (√Kk-x ㊉K yk2)F≡⅛
In the case of X := μo = (0,..., 0)T they simplify to:
expKo (V) = tan (√K 同2)√KVlvk2
logKo(y) = tan-1 (√Kkyk2)√κyy2
Parallel transport Similarly to the Poincare ball, We can derive the parallel transport operation
for the projected sphere:
λK
PTK→y (v) = λ-K gyr[y, -x]v,
λy
2
PTμo→y (V) = λΚ v，
y
λK
PTΚ→μo (V) = ɪV，
Where
gyr[χ, y]v = -(χ ㊉K y)㊉K (X ㊉K (y ㊉K v))
is the gyration operation (Ungar, 2008, Definition 1.11).
Unfortunately, on the projected sphere, h∙, ∙iχ has a form that changes with respect to x, similarly to
the Poincare ball and unlike in the hypersphere.
A.6 Miscellaneous properties
Theorem A.14 (Mobius addition converges to EUCL vector addition).
lim (x ㊉K y) = X + y.
K→0
Note: This theorem Worksfrom both sides, hence applies to the Poincare ball as well as the projected
spherical space. Observe that the Mobius addition has the Sameformfor both spaces.
Proof.
lim(x ㊉ K y) = lim "(I-2K E 喻厂 K 皿2)0 +(1 + K 同2加#
K→0	K→0[	1 — 2Κ hx, y? + K2 ∣x∣2 ∣y∣2	J
= x + y.
□
Theorem A.15 (ρ-K1 is the inverse stereographic projection).
For all (ξ; xT)T ∈ MnK, ξ ∈ R
ρK1(ρ((ξ;XT )T )) = x,
where M ∈ {S, H}.
Proof.
PK1(PK ((ξ; XT )T )) = PK1 (I	P^l
V - v|KlξJ
25
Published as a conference paper at ICLR 2020
/
K
1
Pm K
∖
X
i-√Γκ∣ξ
X
ι-√Wk
2
-1
2
2	；
2
2XT
1-√W⅛
X
ι-√Wξ
∖τ
K
2
2 + 1
2
1/加
1→≡l2 + 1
ι-V∖K∖ ξ
X
K
K
2
∖ T
12√m XT ʌ
-;i-√mξ)
1∕√m	K K 同2	-1.2√mXT
(d√wfc+1V1 - PKξ)2	；1 - PKξ
We observe that ∣∣x∣∣2 =%-ξ2, because X e MK. Therefore
PK(PK((ξ; XT)τ))
=...	(above)
_	1∕√m
K
+1
T
XT T
1/VWi
(i-√⅞)(i+√W⅛)
(1 - vWiξ)2
-1.‰∕∖K
；1 - PKξ
(1-" ξ)(1 + VWi ξ)
T
XT	T
+1
(1 - "ξ)2
-1,^V∖K
；1 - PKξ
1
ɪr 一
K
1∕√m	(1+√mξ -1.2√mXT!t
1+κ + 1 V - PKξ	； 1 - PKξ)
i-Λ∕Γκ∣ξ
1∕√m	( 1+Pm ξ -1+√Kξ. 2所 XT V
；1 - PK ξ
ι+√W⅛+ι-√⅞
1 -V∖K∖ ξ
-‰ (2pKξ; 2p∖K∖xτ)T = (ξ; xτ)τ .
2MlK∖ '	，
□
Lemma A.16 (λK converges to 2 as K → 0). For all X in PK or DK，it holds that
lim λK = 2.
K→0
Proof.
lim λK = lim ------------百=2.
K→0 X	K→0 1 + K ∣∣X∣∣2
□
Theorem A.17 (expK (V) converges to X + v as K → 0). For all X in the Poincare ball PK or the
projected sphere DK and V ∈ TxM, it holds that
Km0 exPK(V) = exPX(V) = X + v,
hence the exponential map converges to its Euclidean variant.
26
Published as a conference paper at ICLR 2020
Proof. For the positive case K > 0
lim expK (v)
K→0+	x
))
due to several applications of the theorem of limits of composed functions, Lemma A.16, and the
fact that
tan( ʌ/ɑa)
lim----------= a.
α→0	α
The negative case K < 0 is analogous.
□
Theorem A.18 (logK (y) converges to y - X as K → 0). For all x, y in the Poincare ball PK or
the projected sphere DnK, it holds that
lim logxK(y) = logx(v) = y - x,
K→0
hence the logarithmic map converges to its Euclidean variant.
Proof. Firstly,
K→0
Z = -X ㊉K y--> y - X,
due to Theorem A.14. For the positive case K > 0
Kl→im0+logxK(y)
lim
K→0+
tanK1 (PKi kzk2
lim
K→0+
2 tanK1 (PKI kzk2)
λxK
M kzk2
z
lim ⅛
K→0+ λxK
tan-1 (√K kzk2)
lim ------ʌ=------- ∙ lim Z
K→0+	K kzk2	K→0+
1 ∙ 1 ∙ (x - Vy) = x - y,
due to several applications of the theorem of limits of composed functions, product rule for limits,
Lemma A.16, and the fact that
lim
α→0
tan-1(√αa)
a.
The negative case K < 0 is analogous.
□
Lemma A.19 (gyr[x, y]v converges to V as K → 0). For all x, y in the Poincare ball PK or the
projected sphere DnK and v ∈ TxM, it holds that
lim gyr[x, y]x = v,
K→0
hence gyration converges to an identity function.
27
Published as a conference paper at ICLR 2020
Proof.
lim gyr[x, y]v = lim (㊀K(x ㊉K y)㊉K (X ㊉K (y ㊉K V)))
K→0	K→0
= -(x+y)+(x+ (y+v))
= -x - y + x + y + v = v,
due to Theorem A.14 and the theorem of limits of composed functions.
□
Theorem A.20 (PTxK→y (v) converges to V as K → 0). For all x, y in the Poincare ball PK or the
projected sphere DnK and v ∈ TxM, it holds that
Kli→m0PTxK→y(V)
V.
Proof.
Kli→m0PTxK→y(V)=Kli→m0
λK
λχK gyr[y, -χ]v
λy
lim
K→0
λK
λK
|{z}
K-→-→0 1
• KmO gyr[y, -χ]v
_- /
{z
K→0
--→v
V,
due to the product rule for limits, Lemma A.16, and Lemma A.19.
□
B	Probability details
B.1 Wrapped Normal distributions
Theorem B.1 (Probability density function of WN(z; μ, Σ) in HK).
(RSinh (kURL ) ∖
log WN(z; μ, ∑) = log N(v; 0, ∑) — (n — 1)log I -kU------ I ,
where U = log/ (z), V = PTK→μ0 (U), and R = 1∕√-K.
Proof. This was shown for the case K = 1 by Nagano et al. (2019). The difference is that We
do not assume unitary radius R = 1 = 1/√-K. Hence, our tranformation function has the form
f = expK ◦ PTKo→μ, and f-1 = PTKT“. ◦ logf.
The derivative of parallel transport P TxK→y (V) for any x, y ∈ HnK and V ∈ Tx HnK is a map
d PTxK→y (V) : Tv (Tx HnK). Using the orthonormal basis (with respect to the Lorentz product)
{ξ1, . . . ξn}, we can compute the determinant by computing the change with respect to each basis
vector.
d PTxK→y (ξ)
∂
∂e
PTxK→y (V +ξ)
=0
∂
∂e
=0
ξ+
(V + ξ) +
hy, V + eξ L
R2 — hχ, y〉L
(x + y)
hy, ξL
R2 - hx, yiL
(x+y)
=0
PTxK→y (ξ).
Since parallel transport preserves norms and vectors in the orthonormal basis have norm 1, the
change is dPTxK→y(ξ)L= PTxK→y(ξ)L = 1.
28
Published as a conference paper at ICLR 2020
For computing the determinant of the exponential map Jacobian, we choose the orthonormal basis
{ξι = u/ IlUIlL , ξ2, ∙ ∙ ∙, ξn}, where wejust completed the basis based on the first vector. We again
look at the change with respect to each basis vector. For the basis vector ξ1:
dexPK (ξ1)=
∂
∂e
exPK ( U + e TTur
e=0	∖	IIukL
dexpK (ξ)
∂
∂e
∂
∂e
e=0
expK (U + eξ)
e=0
cosh (∣⅛ξ∣k) X +
(U + eξ)
Rsinh (ku+RξkL
IIu + eξk
∂
∂e
e=0
J 而下 ^ R sinh	Q
cosh	x +(u + eξ)
rrJ	∣∖∣2∖∖+ + &
e cosh( Mk嗜+声)
-----ɜ.....— (U + eξ)
IukL + 图
(R2 IlUkL ξ - R2EU + E(IlUkL + e2)x)sinh ( VZkuRL+m
+-----------------------n-----------------------
R(kUkL + E2)3/2
e=0
R2 ∣∣u∖L sinh (叱L ) Rsinh (kRL)
-R(IukL)3/2- ξ = -FE- ξ,
where the third equality holds because
kU + EξkL = kUkL + E2kξkL - 2 (U,Eξ>L
=kUkL + E- 2e hU, ξL
=kUkL + E2,
where the last equality relies on the fact that the basis is orthogonal, and U is parallel to ξ1
U/ kUkL, hence it is orthogonal to all the other basis vectors.
29
Published as a conference paper at ICLR 2020
Because the basis is orthonormal the determinant is a product of the norms of the computed change
for each basis vector. Therefore,
det (d PTx→y(V)) = ιn = 1.
Additionally, the following two properties hold:
2
2
and
d expxK
d expxK (ξ)2L
sinh
sinh2
- sinh2
2
R sinh
R2 sinh2 k kuk
kukL	'
u
+ co
x
—+ cosh
R
ukL
k
L
uk2L
kukL
+ cosh2 (kuL) =L
kukL
R2 sinh2 (kuk
kξk2L
L
L

Therefore, we obtain
det
∂ expxK (u)
∂u
(RSinh (kuRkL,rn 1
1 ^ ( ~⅛- J
Finally,
det
det
∂ expK (U)
∂u
∂PTK	(v)
.det ——μ0→μ ；
∂v
Rsinh (kuRL)y-1
~⅛- J
□
Theorem B.2 (Probability density function of WN(z; μ, Σ) in SK).
log WN(z; μ, Σ) = log N(v; 0, Σ) - (n - 1) log
R sin
k
where U = logf (z), V = PTK→μo (U), and R = 1∕√K.
Proof. The theorem is very similar to Theorem B.1. The difference is that in this one, our
manifold changes from HnK to SnK , hence K > 0. Our tranformation function has the form
f = expK ◦ PT%→μ, and f-1 = PTK^ ◦ logf.
The derivative of parallel transport P TxK→y (V) for any x, y ∈ SnK and V ∈ TxSnK is a map
d PTxK→y(V) : Tv(TxSnK). Using the orthonormal basis (with respect to the Lorentz product)
{ξ1, . . . ξn}, we can compute the determinant by computing the change with respect to each ba-
sis vector.
∂
dPTXK→y (ξ)= ∂	PTK→y (V + ⑹
∂	=0
30
Published as a conference paper at ICLR 2020
∂
% e=0
hy, V + eξ2
R2 + 8 y)2
(X + y)
hy, ξ2
R2 + H y)2
(X + y)
e=0
ξ -
=PTK→y(ξ).
Since parallel transport preserves norms and vectors in the orthonormal basis have norm 1, the
changeis MPTKTy(ξ)∣∣2 = IIPTK→y(ξ)∣∣2 = 1.
For computing the determinant of the exponential map Jacobian, we choose the orthonormal basis
{ξι = u/ ||uk2, ξ2,..., ξn}, where we just completed the basis based on the first vector. We again
look at the change with respect to each basis vector. For the basis vector ξ1:
dexpK (ξι)=
NJPK (U + £ 舟
色 [cos (lkuk2+ d ʌ X + RSin (吗卢1)
dey I R χ +kuk2 Ikuk2 + e|
—
(kuk2 + e) sin ("uR+^)	cos ("uR+^
------------------LX +----------
RIkuk2 + e|	+	kuk2
u
(Huk2+ e) u
e=0
cos (吟)就-Sin(啥)R,
where the second equality is due to
u
u + e-∏-∏-
kuk2
kuk2
kuk2 = Ikuk2 + e|.
For every other basis vector ξk where k > 1:
deχpK (S) =
∂
de
∂
de
expK (u + eξ)
e=0
-cos () X +
e=0	∖ R /
d
de
e=0
cos') X + R Sin∖ ) (u + 胤
R	kuk22 + e2
(u + eξ)
(R2 lluk2 ξ - R2eu - e(kuk2 + e2)X)sin ( S 噌+声
R(kuk2 + e2)3/2
R2 ku∣∣2 sin (kuk2)	Rsin (kuk2)
R(kuk2)3/2- ξ = -⅛- ξ,
e=0
e cos (√u∣Ξ≡)
-kuk2 + e2-
+
R sin(3处)
FW(U + ⑹
31
Published as a conference paper at ICLR 2020
where the third equality holds because
ku+ξk22= kuk22 + 2 kξk22 - 2 hu, ξi2
= kuk22 +2 - 2 hu, ξi2
= kuk22 + 2 ,
where the last equality relies on the fact that the basis is orthogonal, and u is parallel to ξ1 =
u/ kuk2, hence it is orthogonal to all the other basis vectors.
Because the basis is orthonormal the determinant is a product of the norms of the computed change
for each basis vector. Therefore,
det
∂ PTχ→y (V)
∂v
1n = 1.
Additionally, the following two properties hold:
d expxK
2
and
d expxK (ξ)22
RSin (*)
-kuk2- ξ
R2 sin2
kuk
R2 sin2
kuk
kuk
R
2
2
'M
R
2
2
kξk22
Therefore, we obtain
det
∂ expxK (u)
∂u
kuk2
Finally,
det (f2)=det
d exPK(U)
∂u
∂PTK	(v)
• det ——μ°→μ∖2
∂v
R Isin (⅛ [I]
kuk2
1 ∙
2
2
2
R Isin (⅛ )「
□
Theorem B.3 (Probability density function of WN(z; μ, Σ) in PK).
log WNPK (z; μ, ∑) = log WNHK (Pκ1(z); PK1 (μ), ∑).
Proof. Follows from Theorem B.1 and A.3.
Also proven by (Mathieu et al., 2019) in a slightly different form for a scalar scale parameter
WN(z; μ, σ2I). Given
log N (z; μ,σ2I)
- 2log(2∏σ2)
—
32
Published as a conference paper at ICLR 2020
log WN(z; μ, σ2I) = - dP ；；2Z) - 2 log (2πσ2)
+ (n - 1) log (	√=Kdκ (μ, z)-).
∖sinh(√-KdK(μ, ZXJ
□
Theorem B.4 (Probability density function of WN(z; μ, Σ) in DK).
log WNDK (z; μ, ∑) = log WNSK (Pκ1(z); Pκ1(μ), ∑).
Proof. Follows from Theorem B.2 and A.3 adapted from P to D.	□
C Related work
Universal models of geometry Duality between spaces of constant curvature was first noticed by
Lambert (1770), and later gave rise to various theorems that have the same or similar forms in all
three geometries, like the law of sines (Bolyai, 1832)
sin A	sin B	sin C
Pκ (a)	Pκ (b)	Pκ (c),
wherepK(r) = 2π sinK (r) denotes the circumference of a circle of radius r in a space of constant
curvature K, and
Kx3 K2x5
sinκ(X) = X —3!—I—5!—
χ∞ (-1)iKix2i+1
i=0	(2"1)!
—
Other unified formulas for the law of cosines, or recently, a unified Pythagorean theorem has also
been proposed (Foote, 2017):
K
A(C) = A(a) + A(b) - -A(a)A(b),
2π
where A(r) is the area of a circle of radius r in a space of constant curvature K. Unfortunately, in
this formulation A(r) still depends on the sign ofK w.r.t. the choice of trigonometric functions in
its definition.
There also exist approaches defining a universal geometry of constant curvature spaces. Li et al.
(2001, Chapter 4) define a unified model of all three geometries using the null cone (light cone)
of a Minkowski space. The term “Minkowski space” comes from special relativity and is usually
denoted as R1,n, similar to the ambient space of what we defined as Hn, with the Lorentz scalar
product(•,•)£. The hyperboloid Hn corresponds to the positive (upper, future) null cone of R1,n.
All the other models can be defined in this space using the appropriate stereographic projections and
pulling back the metric onto the specific sub-manifold. Unfortunately, we found the formalism not
useful for our application, apart from providing a very interesting theoretical connection among the
models.
Concurrent VAE approaches The variational autoencoder was originally proposed in Kingma
& Welling (2014) and concurrently in Rezende et al. (2014). One of the most common improve-
ments on the VAE in practice is the choice of the encoder and decoder maps, ranging from linear
parametrizations of the posterior to feed-forward neural networks, convolutional neural networks,
etc. For different data domains, extensions like the GraphVAE (Simonovsky & Komodakis, 2018)
using graph convolutional neural networks for the encoder and decoder were proposed.
The basic VAE framework was mostly improved upon by using autoregressive flows (Chen et al.,
2014) or small changes to the ELBO loss function (Matthey et al., 2017; Burda et al., 2016). An
important work in this area is β-VAE, which adds a simple scalar multiplicative constant to the
KL divergence term in the ELBO, and has shown to improve both sample quality and (if β >
1) disentanglement of different dimensions in the latent representation. For more information on
disentanglement, see Locatello et al. (2018).
33
Published as a conference paper at ICLR 2020
Geometric deep learning One of the emerging trends in deep learning has been to leverage non-
Euclidean geometry to learn representations, originally emerging from knowledge-base and graph
representation learning (Bronstein et al., 2017).
Recently, several approaches to learning representations in Euclidean spaces have been generalized
to non-Euclidean spaces (Dhingra et al., 2018; Ganea et al., 2018b; Nickel & Kiela, 2017). Since
then, this research direction has grown immensely and accumulated more approaches, mostly for
hyperbolic spaces, like Ganea et al. (2018a); Nickel & Kiela (2018); Tifrea et al. (2019); Law et al.
(2019). Similarly, spherical spaces have also been leveraged for learning non-Euclidean representa-
tions (Batmanghelich et al., 2016; Wilson & Hancock, 2010).
To be able to learn representations in these spaces, new Riemannian optimization methods were
required as well (Wilson & Leimeister, 2018; BonnabeL 2013; BecigneUl & Ganea, 2019).
The generalization to products of constant curvature Riemannian manifolds is only natural and has
been proposed by Gu et al. (2019). They evaluated their approach by directly optimizing a distance-
based loss function using Riemannian optimization in products of spaces on graph reconstruction
and word analogy tasks, in both cases reaping the benefits of non-Euclidean geometry, especially
when learning lower-dimensional representations. Further use of product spaces with constant cur-
vature components to train Graph Convolutional Networks was concurrently with this work done by
Bachmann et al. (2020).
Geometry in VAEs One of the first attempts at leveraging geometry in VAEs was Arvanitidis et al.
(2018). They examine how a Euclidean VAE benefits both in sample quality and latent represen-
tation distribution quality when employing a non-Euclidean Riemannian metric in the latent space
using kernel transformations.
Hence, a potential improvement area of VAEs could be the choice of the posterior family and prior
distribution. However, the Gaussian (Normal) distribution works very well in practice, as it is the
maximum entropy probability distribution with a known variance, and imposes no constraints on
higher-order moments (skewness, kurtosis, etc.) of the distribution. Recently, non-Euclidean geom-
etry has been used in learning variational autoencoders as well. Generalizing Normal distributions
to these spaces is in general non-trivial..
Two similar approaches, Davidson et al. (2018) and Xu & Durrett (2018), used the von Mises-
Fischer distribution on the unit hypersphere to generalize VAEs to spherical spaces. The von Mises-
Fischer distribution is again a maximum entropy probability distribution on the unit hypersphere, but
only has a spherical covariance parameter, which makes it less general than a Gaussian distribution.
Conversely, two approaches, Mathieu et al. (2019) and Nagano et al. (2019), have generalized VAEs
to hyperbolic spaces - both the Poincare ball and the hyperboloid, respectively. They both adopt
a non-maximum entropy probability distribution called the Wrapped Normal. Additionally, Math-
ieu et al. (2019) also derive the Riemannian Normal, which is a maximum entropy distribution on
the Poincare disk, but in practice performs similar to the Wrapped Normal, especially in higher
dimensions.
Our approach generalizes on the afore-mentioned geometrical VAE work, by employing a “products
of spaces” approach similar to Gu et al. (2019) and unifying the different approaches into a single
framework for all spaces of constant curvature.
D Extended future work
Even though we have shown that one can approximate the true posterior very well with Normal-
like distributions in Riemannian manifolds of constant curvature, there remain several promising
directions of explorations.
First of all, an interesting extension of this work would be to try mixed-curvature VAEs on graph
data, e.g. link prediction on social networks, as some of our models might be well suited for sparse
and structured data. Another very beneficial extension would be to investigate why the obtained
results have a relatively big variance across runs and try to reduce it. However, this is a problem that
affects the Euclidean VAE as well, even if not as flagrantly.
34
Published as a conference paper at ICLR 2020
Secondly, we have empirically noticed that it seems to be significantly harder to optimize our models
in spherical spaces - they Seem more prone to divergence. In discussions, other researchers have also
observed similar behavior, but a more thorough investigation is not available at the moment. We have
side-stepped some optimization problems by introducing products of spaces - previously, it has been
reported that both spherical and hyperbolic VAEs generally do not scale well to dimensions greater
than 20 or 40. For those cases, we could successfully optimize a subdivided space (S2)36 instead
of one big manifold S72 . However, that also does not seem to be a conclusive rule. Especially in
higher dimensions, we have noticed that our VAEs (S2)36 with learnable curvature and D172 with
fixed curvature seem to consistently diverge. In a few cases S72 with fixed curvature and even the
product (E2)12 × (H2)12 × (S2)12 with learnable curvature seemed to diverge quite often as well.
The most promising future direction seems to be the use of “Normalizing Flows” for variational
inference as presented by Rezende & Mohamed (2015) and Gemici et al. (2016). More recently, it
was also combined with “autoregressive flows” in Huang et al. (2018). Using normalizing flows,
one should be able to achieve the desired level of complexity of the latent distribution in a VAE,
which should, similarly to our work, help to approximate the true posterior of the data better. The
advantage of normalizing flows is the flexibility of the modeled distributions, at the expense of being
more computationally expensive.
Finally, another interesting extension would be to extend the defined geometrical models to allow for
training generative adversarial networks (GANs) (Goodfellow et al., 2014) in products of constant
curvature spaces and benefit from the better sharpness and quality of samples that GANs provide.
Finally, one could synthesize the above to achieve adversarially trained autoencoders in Riemannian
manifolds similarly to Pan et al. (2018); Kim et al. (2017); Makhzani et al. (2015) and aim to achieve
good sample quality and a well-formed latent space at the same time.
E Extended results
uJU.Zi-wiiip w ± U,Zj
3d2-comp 002 d2
3h2-comp 000 h2
, 3h2-comp001h2
3h2-comp 002 h2
, 3p2-comp 000 p2
3p2-comp001p2
, 3p2-comp 002 p2
3s2-comp 000 s2
, 3s2-comp001s2
3s2-comp 002 s2
3u2-comp 000 u2
3u2-comp001u2
, 3u2-comp 002 u2
d2,e2,p2-comp 000 e2
d2,e2,p2-comp001p2
d2,e2,p2-comp 002 d2
d6-comp 000 d6
e2,h2,s2-comp_000_e2
•	e2,h2,s2-comp_001_h2
e2,h2,s2-comp_002_s2
h6-comp 000 h6
p6-comp 000 p6
•	s6-comp 000 s6
u6-comp 000 u6
Figure 1:	Learned curvature across epochs (with standard deviation) with latent space dimension of
6, diagonal covariance parametrization, on the MNIST dataset.
35
Published as a conference paper at ICLR 2020
Table 6: Summary of results (mean and standard-deviation) with latent space dimension of 6, spher-
ical covariance parametrization, on the BDP dataset.
Model	LL	ELBO	BCE	KL
1SI)3	-55.89±0.36	-56.72±o.4O	51.01±o.31	5.72±o.O9
si	-55.81±o.35	-56.57±o.44	51.16±o.78	5.41±o.42
(vMF S2)3	-57.87±1.52	-58.64±i.63	53.96±2.i6	4.68±o.53
vMF S?	-58.78±o.83	-60.74±2.29	56.03±2.64	4.71±o.48
(Di)3	-56.01±o.24	-56.67±o.31	51.02±o.4O	5.65±o.io
D?	—55.78±0.07	-56.38±o.O6	50.85±o.2O	5.53±o.24
(E2 )3	-56.34±o.45	-56.94±o.5O	51.32±o.55	5.62±o.19
E6	-56.28±o.56	-56.99±o.59	51.58±o.69	5.41±o.29
(H-I)3	-56.08±o.52	-56.80±o.54	50.94±o.38	5.86±o.25
H-1	-56.18±o.32	-57.10±o.21	51.48±o.47	5.62±o.31
(P-I)3	-55.98±o.62	-56.49±o.62	50.96±o.61	5.52±o.31
P-I	-56.74±o.55	-57.61±o.74	52.01±o.71	5.60±o.24
(RN P-1)3	-54.99±o.12	-55.90±o.13	52.42±o.71	3.48±o.6O
(S2)3	-56.05±o.21	-56.69±o.36	51.07±o.21	5.61±o.22
(vMF S2 )3	-57.56±o.88	-57.80±o.89	52.68±i.62	5.12±o.84
S6	-56.06±o.51	-56.65±o.64	50.93±o.38	5.72±o.4O
vMF S6	-58.21±o.92	-59.87±1.51	54.99±i.79	4.88±o.39
(D2)3	-56.06±o.36	-56.69±o.54	50.95±o.4O	5.74±o.17
D6	-56.10±o.25	-56.69±o.17	50.90±o.19	5.79±o.O3
(H2)3	-55.80±o.32	-56.72±o.16	51.14±o.39	5.58±o.28
H6	-56.03±o.21	-56.82±o.2O	50.99±o.16	5.83±o.27
(P2)3	-56.29±o.O5	-57.11±o.22	51.41±o.19	5.69±o.3O
P6	-56.40±o.31	-57.13±o.25	51.17±o.33	5.96±o.27
(RN P2)3	-56.25±o.56	-57.26±o.45	53.16±i.O7	4.11±0.64
D2 × E2 × P2	-55.87±o.22	-56.35±o.22	50.67±o.57	5.69±o.43
D2 × E2 × P-1	-56.06±o.41	-56.86±o.65	51.23±o.67	5.64±o.ii
D2 × E2 × (RN P2)	-56.35±o.82	-57.06±o.78	51.89±o.71	5.17±o.12
D2 × E2 × (RN P-1)	-56.17±o.43	-56.75±o.56	51.80±o.8O	4.95±o.32
E2 × H2 × S2	-55.92±o.42	-56.54±o.45	51.13±o.74	5.41±o.4O
E2 × H-1 × S2	-56.04±o.57	-56.71±o.77	51.09±o.86	5.62±o.12
E2 × H2 × (vMF S2)	-55.82±o.43	-56.32±o.47	51.10±o.67	5.21±o.2O
E2 × H-1 × (vMF S2)	—55.77±0.51	-56.34±o.65	51.33±o.57	5.01±o.17
(U2)3	-55.56±o.15	-56.05±o.32	50.68±o.23	5.37±o.io
U6	-55.84±o.38	-56.46±o.41	50.66±o.38	5.81±o.18
36
Published as a conference paper at ICLR 2020
Table 7: Summary of results (mean and standard-deviation) with latent space dimension of 6, spher-
ical covariance parametrization, on the MNIST dataset.
Model	LL	ELBO	BCE	KL
1SI)3	-96.77±0.26	-101.66±o.32	87.04±o.49	14.62±o.18
(vMF S2)3	-97.72±o.22	-102.98±o.15	87.77±o.18	15.21±o.O7
S?	-96.71±o.17	-101.55±o.3O	86.90±o.3O	14.65±o.io
vMF S?	-97.03±o.14	-102.12±o.26	87.42±o.28	14.69±o.O3
(D?)3	-97.84±o.io	-102.75±o.22	88.43±o.12	14.33±o.13
D?	-98.21±o.23	-103.02±o.14	88.44±o.O5	14.58±o.ii
(E2 )3	-97.04±o.14	-101.44±o.18	86.77±o.22	14.67±o.22
E6	-97.16±o.15	-101.67±o.14	87.17±o.26	14.50±o.2O
(H-I)3	-97.31±o.O9	-102.20±o.29	87.81±o.23	14.39±o.13
H-1	-97.10±o.44	-101.89±o.33	87.32±o.22	14.56±o.2O
(P-I)3	-97.56±o.O4	-102.33±o.22	87.93±o.32	14.40±o.io
(RN P-1)3	-92.54±o.19	-97.19±o.21	88.42±o.2O	8.76±o.O4
P-1	-97.80±o.O5	-102.60±o.O4	88.14±o.O8	14.46±o.O7
(S2)3	-96.46±o.12	-101.30±o.17	86.79±o.25	14.51±o.O9
(vMF S2 )3	-97.62±o.3O	-102.72±o.37	87.48±o.37	15.24±o.O3
S6	-96.72±o.15	-101.39±o.16	86.69±o.15	14.70±o.13
vMF S6	-96.72±o.18	-101.55±o.21	86.82±o.23	14.73±o.O2
(D2)3	-97.68±o.24	-102.51±o.44	88.11±o.34	14.41±o.ii
D6	-97.72±o.15	-102.31±o.16	87.70±o.22	14.61±o.O6
(H2)3	-97.37±o.13	-102.07±o.24	87.56±o.3O	14.51±o.ii
H6	-97.47±o.16	-102.18±o.2O	87.64±o.23	14.53±o.O7
(P2)3	-97.62±o.O5	-102.34±o.16	87.92±o.16	14.43±o.O6
(RN P2)3	-94.16±o.68	-98.65±o.66	89.27±o.79	9.38±o.15
P6	-97.71±o.24	-102.55±o.21	88.24±o.23	14.32±o.O4
D2 × E2 × P2	-97.48±o.18	-102.22±o.29	87.85±o.17	14.37±o.13
D2 × E2 × P-1	-97.58±o.13	-102.23±o.15	87.75±o.15	14.49±o.15
D2 × E2 × (RN P2)	-96.43±o.47	-101.31±o.51	88.82±o.5O	12.50±o.O3
D1 × E2 × (RN P-1)	-96.18±o.21	-100.91±o.31	88.58±o.47	12.33±o.19
E2 × H2 × S2	-96.80±o.2O	-101.60±o.33	87.13±o.19	14.47±o.17
E2 × H-1 × S2	-96.76±o.O9	-101.48±o.13	86.99±o.17	14.49±o.O5
E2 × H2 × (vMF S2 )	-96.56±o.27	-101.49±o.28	86.58±o.36	14.91±o.14
E2 × H-I × (vMF S2)	-96.76±o.39	-101.82±o.13	87.08±o.O6	14.74±o.13
(U2)3	-97.12±o.O4	-101.68±o.O6	87.13±o.14	14.55±o.16
U6	-97.26±o.16	-102.05±o.18	87.54±o.21	14.51±o.ii
37
Published as a conference paper at ICLR 2020
Table 8: Summary of results (mean and standard-deviation) with latent space dimension of 6, diag-
onal covariance parametrization, on the MNIST dataset.
Model	LL	ELBO	BCE	KL
1S1)3	-96.57±0.04	-101.34±o.12	86.88±0.17	14.45±0.10
Sl	-96.51±0.09	-101.29±0.18	86.71±0.20	14.58±0.13
(D1 )3	-97.81±0.14	-102.58±0.23	88.31±0.25	14.27±0.02
Dl	-97.89±0.10	-102.65±0.10	88.39±0.16	14.26±0.08
(E2 )3	-96.94±0.34	-101.34±0.41	86.89±0.36	14.44±0.11
E6	-96.88±0.16	-101.36±0.08	86.90±0.14	14.46±0.07
(H-1 )3	-97.19±0.32	-102.06±0.28	87.63±0.37	14.42±0.10
H-ι	-97.38±0.73	-102.22±0.95	87.75±0.59	14.47±0.37
(P-1 )3	-97.57±0.12	-102.22±0.18	87.83±0.30	14.39±0.13
P-1		-97.33±0.15	-102.02±0.35	87.71±0.36	14.31±0.04
(S2 )3	-96.78±0.35	-101.43±o.24	86.93±0.28	14.50±0.05
S6	-96.44±0.20	-101.18±0.36	86.74±0.38	14.44±0.05
(D2 )3	-97.61±0.19	-102.37±0.26	87.96±0.21	14.41±0.06
D6	-97.53±0.22	-102.31±0.38	87.97±0.37	14.34±0.08
(H2 )3	-96.86±0.31	-101.61±0.30	87.13±0.30	14.48±0.08
H6	-96.90±0.26	-101.48±0.35	87.18±0.48	14.30±0.15
(p2 )3	-97.52±0.02	-102.30±0.07	88.11±0.07	14.19±0.12
P6	-97.26±0.16	-102.00±0.17	87.58±0.16	14.42±0.08
D2 X E2 X P2	-97.37±0.14	-102.12±o.19	87.78±0.23	14.34±0.12
D2 x E2 x P-1	-97.29±0.16	-101.86±0.16	87.54±0.17	14.32±0.04
E2 x H2 x S2	-96.71±0.19	-101.34±0.16	86.91±0.17	14.43±0.06
E2 x H-1 x Sl	-96.66±0.27	-101.46±0.44	87.02±0.38	14.44±0.08
(U2 )3	-97.06±0.13	-101.66±o.19	87.22±0.12	14.44±0.07
U6	-96.90±0.10	-101.68±0.07	87.27±0.11	14.42±0.12
。。。
/ LAqG 9” A
“3y5G7"
∖ 434fQ9g夕
O / 乙当 “ U S 7，A
0/237567^?
Oi 23T5G”，
(a) Original
0 / Zf G G 匕”
O∕P5ty5G7S
OlZ3U5b18
(b) (E2)36
(d) E2 × H2 × S2
(c) (E2)3
O / 乙+"S7S%
O / 23,367g夕
(e) (E2)12 × (H2)12 × (S2)12
K彳夕
3 gg
77 n/
S 6。
<5f
“ Y，
¾3 3
乙22<
(f) (U2)36

Figure 2:	Qualitative comparison of reconstruction quality of randomly selected runs of a selection
of well-performing models on MNIST test set digits.
38
Published as a conference paper at ICLR 2020
Table 9: Summary of results (mean and standard-deviation) with latent space dimension of 12,
diagonal covariance parametrization, on the MNIST dataset.
Model	LL	ELBO	BCE	KL
1st	-79.92±0.21	-84.88±0.14	62.83±0.21	22.06±0.07
S12	-80.72±0.34	-85.73±0.36	63.86±0.32	21.87±0.04
(d)6	-80.53±0.10	-85.59±0.08	63.62±0.12	21.97±0.16
Dl2	-80.81±0.12	-86.40±0.17	64.42±0.19	21.98±0.06
(E2 )6	-79.51±0.10	-83.91±0.12	61.84±0.06	22.07±0.13
E12	-79.51±0.09	-83.95±0.06	61.66±0.10	22.29±0.04
(H-I)6	-80.54±0.23	-86.05±0.52	63.78±0.26	22.27±0.26
H-I	-79.37±0.14	-84.76±0.08	62.32±0.05	22.44±0.10
(P-I)6	-80.39±0.07	-85.46±0.15	63.48±0.22	21.98±0.17
P-I	-80.88±0.20	-85.87±0.45	63.66±0.59	22.21±0.17
(S2)6	-79.95±0.14	-84.90±0.25	62.83±0.34	22.07±0.17
S12	-79.99±0.27	-84.78±0.26	62.89±0.29	21.89±0.18
(D2)6	-80.40±0.09	-85.38±0.08	63.49±0.12	21.89±0.18
D12	-80.37±0.16	-85.26±0.19	63.24±0.15	22.02±0.13
(H2)6	-80.13±0.08	-85.22±0.24	63.32±0.34	21.90±0.10
H12	-79.77±0.10	-84.58±0.15	62.49±0.10	22.09±0.20
(P2)6	-80.31±0.08	-85.35±0.10	63.57±0.17	21.79±0.07
P12	-80.66±0.09	-85.55±0.03	63.55±0.17	22.00±0.14
—80.30±0.31
-80.14±0.11
—80.17±0.11
-80.14±0.20
-79.59±0.25
-79.87±0.45
-79.69±0.14
-85.22±0.40
-85.00±0.08
-84.95±0.27
-84.99±0.26
-84.43±0.20
-84.82±0.61
-84.45±0.12
63.52±0.48
62.99±0.16
62.87±0.39
63.06±0.26
62.68±0.20
62.66±0.42
62.64±0.28
21.70±0.11
22.01±0.24
22.08±0.18
21.92±0.08
21.75±0.20
22.17±0.20
21.81±0.21
X × E E X x0
产产X X产产X
2 21 2 2
fmD4叫(1e(1ee4
(U2)6	-79.61±0.06 -84.13±0.04 61.92±0.22 22.21±0.23
U12	-80.01±0.30 - 84.86±0.51 62.90± 0.63 21.96±0.16
39
Published as a conference paper at ICLR 2020
Table 10: Summary of results (mean and standard-deviation) with latent space dimension of 72,
diagonal covariance parametrization, on the MNIST dataset.
Model		LL	ELBO	BCE	KL
(S21)36		-78.43±0.44	—84.99±0.49	56.88±0.28	28.11±0.56
(D21)36		—76.03±o.17	—83.04±0.25	54.35±0.15	28.69±0.17
(E2)36		-74.53±o.06	—80.05±0.10	50.91±0.17	29.15±0.07
E72		—74.42±0.06	—80.09±0.12	51.45±0.30	28.63±0.20
(H2-1)36		—77.92±0.32	—84.76±0.55	56.85±0.60	27.91±0.42
H7-21		—77.30±o.12	—86.98±0.09	58.04±0.29	28.94±0.25
(P2-1)36		—76.11±o.08	—82.63±0.19	53.89±0.36	28.74±0.30
P7-21		—77.50±o.05	—84.53±0.13	55.80±0.20	28.73±0.18
S72		—75.24±0.oι	—81.39±0.14	53.03±0.27	28.36±0.16
(D2)36		—75.66±o.06	—81.94±0.09	53.32±0.16	28.61±0.11
D72		—77.11±2.21	—83.94±2.81	54.94±2.55	29.00±1.31
(H2)36		—77.87±o.02	—83.95±0.02	55.71±0.35	28.24±0.36
H72		—75.03±o.ιι	—81.23±0.14	52.63±0.10	28.61±0.11
(P2)36		—75.77±o.12	—82.07±0.02	53.65±0.38	28.43±0.39
P72		—75.71±o.08	—81.95±0.09	53.29±0.14	28.67±0.05
(D2)12 × (E2)12 ×	(P2)12	—77.40±o.55	—83.35±0.41	53.90±0.40	29.45±0.12
(D12)12 × (E2)12 ×	(P2-1)12	—75.36±o.23	—81.53±0.42	53.02±0.39	28.51±0.45
D24 × E24 × P24		—75.11 ±0.05	—80.99±0.07	52.48±0.19	28.52±0.16
(E2)12 × (H2-1)12	× (S21)12	—77.53±0.34	—83.95±0.40	55.54±0.43	28.42±0.08
E24 × H24 × S24		—75.04±o.16	—81.17±0.18	52.61±0.32	28.55±0.38
(U2)36		—74.64±o.08	—80.52±0.10	52.04±0.10	28.48±0.07
U72		—75.46±o.09	—81.76±0.09	53.27±0.18	28.49±0.18
40
Published as a conference paper at ICLR 2020
乙Oo。。00。
Cao。。。。。
6 £ O O ɔ ə ə
S S-SGG 9 2 9
S S ⅞ 9 OJ Otf 2 0f
尸；f J 尸了 7 7 O* ?
£; ¢/ GIJ Ti n/ 7—
ff J I J I *⅛ f b f 4f 4f- ⅞f-
⅛Γ444 444d
AG444。〃/
qq<roof /√
q q q。夕√J√
S S S 5 ? √ √ Z
∣r 3 3 r∏「，？ /√
pnmm?/
3333m7?z
(b) Second component of (E2)3.
VV√Q3222
VyVOT 3 222
夕Vyqs883
77Lds 5 3 3 3
7l∕ 5 5 5 5 3 3
7 k√5 5 5 5 5 3
(a) First component of (E2)3.
(c) First (negative) component of (U2)3 .
(d) Second (negative) component of (U2)3.
Γ⅛ zr⅛ /b Cv AMW ¾v ⅞≡0
Λ⅛∕⅛zb<bs4us3
?ι¢££8883
L L 4 G Oo Oo 2 2
⅛“*<⅛a2z2
〃工C
y "夕 777tx
A' HQOOOOd
HqROOOCd
IrqqGo 0/
WCrqeS
(e) P2 ofE2 × P2 × D2.	(f) D2 ofE2 × P2 × D2.
Figure 3: Samples from various models of a grid search around 0 of a single component’s latent
space on MNIST test digits.
41
Published as a conference paper at ICLR 2020
(a) H2 of E2 × H2 × S2
(b) H6
(c) E2 of E2 × H2 × S2
(e) S2 of E2 × H2 × S2
(d) E6
(f) S6
Figure 4: Illustrative latent space visualization of a randomly selected run of the models E2 ×H2 ×S2,
E6, H6, and S6 on MNIST with spherical covariance. E2 is visualized directly, S2 is visualized
using a Lambert azimuthal equal-area projection (Snyder, 1987, Chapter 24), H2 is transformed to
the Poincare ball model using a stereographic conformal projection. All other latent space sizes
were first projected using the respective transformation (to PoinCare ball, Lambert projection) if
applicable, and then projected to R2 using Principal Component Analysis (Abdi & Williams, 2010,
PCA) and visualized directly.
42
Published as a conference paper at ICLR 2020
Table 11: Summary of results (mean and standard-deviation) with latent space dimension of 6,
diagonal covariance parametrization, on the Omniglot dataset.
Model	LL	ELBO	BCE	KL
(S21)3	-136.80±1.31	—141.68±1.52	131.73±5.65	9.95±4.33
S61	-136.69±o.94	—141.46±0.92	129.52±0.74	11.94±0.19
(D21)3	—136.21±o.12	—140.44±0.17	128.93±0.14	11.51±0.04
D61	-137.42±i.20	—141.95±1.94	130.70±2.18	11.25±0.26
(E2)3	—136.08±o.21	—140.46±0.24	128.85±0.34	11.62±0.14
E6	—136.05±o.29	—140.50±0.35	128.95±0.41	11.55±0.14
(H2-1)3	—137.14±o.13	—141.87±0.16	130.18±0.21	11.69±0.10
H6-1	—137.09±o.O6	—142.22±0.19	130.37±0.21	11.85±0.12
(P2-1)3	—136.16±o.20	—140.63±0.32	129.29±0.34	11.34±0.03
P6-1	—135.86±o.20	—140.36±0.19	128.92±0.23	11.44±0.16
(S2)3	—136.14±o.27	—140.68±o.32	128.98±0.27	11.70±0.13
S6	—136.20±o.44	—140.76±0.45	129.10±0.37	11.66±0.13
(D2)3	—136.13±o.17	—140.59±0.15	129.10±0.20	11.49±0.12
D6	—136.30±o.08	—140.74±0.14	129.35±0.16	11.39±0.05
(H2)3	—136.17±o.09	—140.65±0.17	129.26±0.07	11.39±0.16
H6	—136.24±o.32	—140.92±0.33	129.48±0.27	11.45±0.12
(P2)3	—136.09±o.O7	—140.41±0.08	129.04±0.05	11.37±0.08
P6	—136.05±o.44	—140.42±0.47	129.04±0.53	11.38±0.07
D2 × E2 × P2	—135.89±o.40	—140.28±o.42	128.75±0.40	11.53±0.04
D12 × E2 × P2-1	—136.01±o.31	—140.52±0.35	129.02±0.27	11.50±0.11
E2 × H2 × S2	—135.93±o.48	—140.51±0.53	128.85±0.48	11.66±0.14
E2 × H2-1 × S21	—136.34±o.41	—141.02±0.46	129.24±0.47	11.78±0.10
(U2)3	—136.21±o.07	—140.65±o.30	129.14±0.34	11.52±0.15
U6	—136.04±o.17	—140.43±0.14	129.07±0.27	11.36±0.13
43
Published as a conference paper at ICLR 2020
Table 12: Summary of results (mean and standard-deviation) with latent space dimension of 72,
diagonal covariance parametrization, on the Omniglot dataset.
Model	LL	ELBO	BCE	KL
(s2)36	-112.33±o.14	-118.94±o.14 91.04±o.37 27.90±o.23
(D2)36	-108.66±0.24	-116.06±0.18 85.95±0.16	30.11±0.04
(E2)36	-105.96±0.33	-112.41±0.35 79.80±0.72 32.61±0.41
E72	-105.89±0.16	-112.40±0.17 79.52±0.19 32.89±0.20
(H-I 产	-112.22±0.11	-119.06±0.15 91.30±0.47 27.76±0.35
H-I	-111.19±0.42	-120.49±0.35 91.11±0.73 29.38±0.40
(P-1)36	-109.05±0.09	-115.99±0.10 85.81±0.42 30.18±0.34
P-I	-111.24±0.28	-118.36±0.24 89.53±0.38	28.84±0.18
^S72	-109.39±o.32 -116.42±o.32 87.22±o.58 29.20±o.28
(D2 )36	-108.89±0.36	-115.65±0.45 85.29±0.74 30.37±0.30
D72	-108.81±0.08	-115.71±0.09 85.68±0.10 30.03±0.09
(H2)36	-112.21±0.28	-118.74±0.30 91.03±0.76 27.71±0.47
H72	-108.62±0.40	-115.54±0.30 85.18±0.62 30.37±0.34
(P2)36	-108.78±0.66	-115.54±0.70 85.16±1.38 30.38±0.69
P72	-109.66±0.61	-116.50±0.68 87.09±1.43 29.42±0.75
(D2)12 × (E2)12 × (P2)12	-107.02±1.56 -115.62±1.76 88.52±8.24 27.10±6.48
(d2)12 × (e2)12 × (P-I)12	-108.06±0.47 -114.92±0.39 83.95±0.58 30.97±0.22
(E2)12 × (H2)12 × (S2)12	-114.85±0.38	-120.98±0.15 95.12±0.49 25.86±0.40
(E2)12 × (H-I)12 × (S1)12	-110.28±0.37	-116.90±0.42 87.71±0.82 29.19±0.41
(U2)36	-105.98±0.05	-112.70±o.19	79.85±o.80 32.85±o.61
U72	-106.58±0.12	-113.68±0.11 81.53±0.34 32.15±0.36
Table 13: Summary of results (mean and standard-deviation) with latent space dimension of 6,
diagonal covariance parametrization, on the CIFAR dataset. All nan standard deviation values
below indicate the repeated experiment was not stable enough to produce a meaningful estimate of
spread.
Model	LL	ELBO	BCE	KL
S61	—1893.16±nan	—1901.32±nan	1885.97±nan	15.35±nan
D61	-1891.69±nan	-1897.77±nan	1884.12±nan	13.64±nan
E6	—1896.19±2.54	-1905.75±3.19	1889.97±2.88	15.78±0.32
H6-1	-1888.23±2.12	-1896.56±2.93	1882.05±2.65	14.51±0.34
P6-1	-1893.27±o.61	-1902.67±0.74	1887.44±0.83	15.23±0.16
D6	-1893.85±o.36	-1902.67±o.69	1887.37±0.74	15.30±0.08
S6	-1889.76±1.62	-1897.31±1.71	1882.55±1.48	14.76±0.24
P6	-1891.40±2.14	-1899.68±2.74	1884.58±2.56	15.10±0.18
D2 × E2 × P2	-1899.90±4.60	-1904.63±1.46	1889.13±1.38	15.50±0.08
E2 × H2 × S2	—1895.46±o.92	-1897.57±0.94	1882.84±0.70	14.73±0.24
(U2)3	-1895.09±4.27	-1904.46±5.21	1888.89±4.71	15.57±0.51
44