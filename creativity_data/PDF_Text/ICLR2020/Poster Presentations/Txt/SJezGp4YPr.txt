Published as a conference paper at ICLR 2020
Geometric Insights into the Convergence of
Nonlinear TD Learning
David Brandfonbrener
Courant Institute of Mathematical Sciences
New York University
david.brandfonbrener@nyu.edu
Joan Bruna
Courant Institute of Mathematical Sciences
Center for Data Science
New York University
bruna@cims.nyu.edu
Ab stract
While there are convergence guarantees for temporal difference (TD) learning
when using linear function approximators, the situation for nonlinear models is
far less understood, and divergent examples are known. Here we take a first step
towards extending theoretical convergence guarantees to TD learning with non-
linear function approximation. More precisely, we consider the expected learning
dynamics of the TD(0) algorithm for value estimation. As the step-size converges
to zero, these dynamics are defined by a nonlinear ODE which depends on the
geometry of the space of function approximators, the structure of the underlying
Markov chain, and their interaction. We find a set of function approximators that
includes ReLU networks and has geometry amenable to TD learning regardless of
environment, so that the solution performs about as well as linear TD in the worst
case. Then, we show how environments that are more reversible induce dynam-
ics that are better for TD learning and prove global convergence to the true value
function for well-conditioned function approximators. Finally, we generalize a di-
vergent counterexample to a family of divergent problems to demonstrate how the
interaction between approximator and environment can go wrong and to motivate
the assumptions needed to prove convergence.
1 Introduction
The instability of reinforcement learning (RL) algorithms is well known, but not well characterized
theoretically. Notably, there is no guarantee that value estimation by temporal difference (TD) learn-
ing converges when using nonlinear function approximators, even in the on-policy case. The use of
a function approximator introduces a projection of the tabular TD update into the class of repre-
sentable functions. Since the dynamics of TD do not follow the gradient of any objective function,
the interaction of the geometry of the function class with that of the TD algorithm in the space of all
functions potentially eliminates any convergence guarantees.
This lack of convergence has motivated many authors to seek variants of TD learning that re-
establish convergence guarantees, such as two timescale algorithms. In contrast, in this work we
focus on TD learning directly and examine its behavior under generic function approximation. We
consider the simplest case: on-policy discounted value estimation. To further simplify the analy-
sis, we only consider the expected learning dynamics in continuous time as opposed to the online
algorithm with sampling. This means that we are eschewing discussions of off-policy data, explo-
ration, sampling variance, and step size. In this continuous limit, the dynamics of TD learning are
modeled as a (nonlinear) ODE. Stability of this ODE is a pre-requisite for convergence of the algo-
rithm. However, for general approximators and MDPs it can diverge as demonstrated by Tsitsiklis
& Van Roy (1997). Today, the convergence of this ODE is known in two regimes: under linear
function approximation for general environments (Tsitsiklis & Van Roy, 1997) and under reversible
environments for general function approximation (Ollivier, 2018). We significantly close this gap
through the following contributions:
1.	We prove that the set of smooth homogeneous functions, including ReLU networks, is
amenable to the expected dynamics of TD. In this case, the ODE is attracted to a compact
1
Published as a conference paper at ICLR 2020
set containing the true value function. Moreover, when we use a parametrization inspired
by ResNets, nonlinear TD will have error comparable to linear TD in the worst case.
2.	We prove global convergence to the true value function when the environment is “more
reversible” than the function approximator is “poorly conditioned”.
3.	We generalize a divergent TD example to a broad class of non-reversible environments.
These results begin to explain how the geometry of nonlinear function approximators and the struc-
ture of the environment interact with TD learning.
2 Setup
2.1	Deriving the dynamics ODE
We consider the problem of on-policy value estimation. Define a Markov reward process (MRP)
M = (S, P, r, γ) where S is the state space with |S| = n finite, P(s0|s) is the transition matrix,
r(s, s0) is the finite reward function, and γ ∈ [0, 1) is the discount factor. This is equivalent to a
Markov decision process with a fixed policy. Throughout we assume that P defines an irreducible,
aperiodic Markov chain with stationary distribution μ. We want to find the true value function:
V * (S) := E[P∞=o γtr(st, st+ι)∣so = s], where the expectation is taken over transitions from P 1.
The true value function satisfies the Bellman equation:
V*(s) = Es0~P(∙∣s)[r(s, s0) + YV*(s0)].	(1)
It will be useful to think of the value function as a vector in Rn and to define R(s) =
Es0 ~p (∙∣s) [r(s, s0)] so that the Bellman equation becomes V * = R + YPV *.
The most prominent algorithm for estimating V* is temporal difference (TD) learning, a form of dy-
namic programming (Sutton & Barto, 2018). In the tabular setting (with no function approximation)
the TD(0) variant of the algorithm with learning rates αk makes the following update at iteration
k+1:
Vk+1(s) = Vk(s) + αk(r(s, s0) + YVk(s0) - Vk(s)).	(2)
Under appropriate conditions on the learning rates and noise we have that Vk → V* as k → ∞
(Robbins & Monro, 1951; Sutton, 1988). Moreover, under these conditions the algorithm is a dis-
cretized and sampled version of the expected continuous dynamics of the following ODE.
, -	..,	..
V(S) = μ(s)(R(s) + γEso~p(∙∣s)[V(S)] - V(s)).	(3)
Letting Dμ be the matrix with μ along the diagonal and applying the Bellman equation We get
, ..
V = Dμ(R + YPV - V) = Dμ(V* - YPV* + YPV - V) = -Dμ(I - YP)(V - V*).	(4)
We now define
A := Dμ(I - YP).	(5)
While A is not necessarily symmetric, it is positive definite in the sense that xTAx > 0 for nonzero
X since 2 (A + AT) is positive definite (Sutton, 1988). This can be seen by showing that A is a
non-singular M-matrix (Horn & Johnson, 1994). This then implies convergence of the ODE defined
in (4) to V * regardless of initial conditions.
In practice, the state space may be too large to use a tabular approach or we may have a feature
representation of the states that we think we can use to efficiently generalize. So, we can parametrize
a value function Vθ by θ ∈ Rd . Then, the “semi-gradient” TD(0) algorithm is
θk+ι = θk + αkVVθk(s)(r(s, s0) + y^⅝(s0)-叽(s)).	(6)
Now by an abuse of notation, define V : Rd → Rn tobe the function that maps parameters θ to value
functions V(θ) so that V (θ)s = Vθ(S). Now, the associated ODE which we will study becomes:
θ = -VV (θ)T Dμ(I - YP )(V (θ) - V *) = -VV (θ)T A(V (θ) - V *).	(7)
The method is called “semi-gradient” because it is meant to approximate gradient descent on the
squared expected Bellman error, which is not feasible since it would require two independent sam-
ples of the next state to make each update (Sutton & Barto, 2018). This approximation is what
results in the lack of convergence guarantees, as elaborated below.
1In RL the true value function is often denoted V π for a policy π. Our MRP can be thought of as an MDP
with fixed policy, so π is part of the environment and We use V * to emphasize that the objective is to find V *.
2
Published as a conference paper at ICLR 2020
2.2 Convergence for linear functions and reversible environments
There are two main regimes where the above dynamics are known to converge. The first is when
V (θ) is linear and the second when the MRP is reversible so that A is symmetric.
It is a classic result of Tsitsiklis & Van Roy (1997) that under linear function approximation, where
V (θ) = Φθ for some full rank feature matrix Φ, TD(0) converges to a unique fixed point (the result
also applies to the more general TD(λ)). The proof uses the fact that in the linear case, letting
θ* = (ΦTAΦ)-1ΦTAV*, the dynamics (7) become:
θ= -ΦT AΦ(θ - θ*).	(8)
The positive definiteness of A gives global convergence to θ*.
Recent work has extended this result to give finite sample bounds for linear TD (Bhandari et al.,
2018). Making such an extension in the nonlinear case is beyond the scope of this paper since even
the simpler-to-analyze expected continuous dynamics are not understood for the nonlinear case.
Some concurrent work has also extended linear convergence to particular kinds of neural networks
in the lazy training regime where the networks behave like linear models (Agazzi & Lu, 2019; Cai
et al., 2019). The relationship to our work will be discussed in Section 6.
The other main regime where the dynamics converge is when P defines a reversible Markov chain
which makes TD(0) gradient descent (Ollivier, 2018). In that case, A is symmetric so (7) becomes:
1
θ = - 2 VkV (θ) - V * kA	(9)
where A must be symmetric to define an inner product. Then for any function approximator this
gradient flow will approach some local minima. Note that without this symmetry the TD dynamics
in (7) are provably not gradient descent of any objective since differentiating the dynamics we get a
non-symmetric matrix which cannot be the Hessian of any objective function (Maei, 2011).
2.3 An example of divergence
To better understand the challenge of proving convergence, Tsitsiklis & Van Roy (1997) provide
an example of an MRP with zero reward everywhere and a nonlinear function approximator where
both the parameters and estimated value function diverge under the expected TD(0) dynamics. We
provide a description of this idea in Figure 1.
a state. Here we see only a two-dimensional slice of the 3-dimensional function space corresponding
to the 3-state MRP. There is no reward so V* = 0. The blue vector field represents the dynamics
defined by the linear system V = -A(V — V*). The red spiral represents the one parameter
family of functions defined for the divergent counterexample. Using an approximator constrains
the dynamics to the red curve by projecting the ambient dynamics (blue arrows) onto the tangent
space of the curve (note this projection is not explicitly illustrated in the diagrams). The yellow dots
indicate stable fixed points and pink dots unstable fixed points. For the tabular approximator, global
convergence to V * is guaranteed since the dynamics are unconstrained. For the divergent example,
projecting the vector field onto the tangent space of the curve causes the dynamics to spiral outwards
regardless of initial conditions. However, if we use the same function approximator but make the
environment reversible, the dynamics on the curve will converge to a local optimum.
3
Published as a conference paper at ICLR 2020
3 TD with homogeneous approximators including neural
NETWORKS
Our first result is that the expected dynamics of TD(0) are attracted to a neighborhood of the true
value function in any irreducible, aperiodic environment when we use a smooth and homogeneous
function approximator.
Definition 1 (Homogeneous). f : Rk → Rm is h-homogeneous for h ∈ R if f(x) = hVf (x)x.
Note that by Euler’s homogeneous function theorem, this is equivalent to f (αx) = αhf(x) for all
positive α.
Remark 1. The ReLU activation as well as the square and several others are homogeneous. More-
over, neural networks of any depth with such activations remain homogeneous. This can be found in
Lemma 2.1 of (Liang et al., 2017) and we include a proof in Appendix Efor completeness.
Note that linear functions are also homogeneous and we will show that much like linear functions,
the set of homogeneous functions works well with TD learning. At a high level, the intuition is that
the image of a homogeneous mapping from parameters to functions is a set in function space who’s
geometry prevents the sort of divergence seen in the spiral example. When V is homogeneous, then
the point V (θ) in the space of functions must lie in the span of the columns of VV (θ) which define
the tangent space to the manifold of functions. This prevents examples like the spiral where the
tangent space is nearly orthogonal to V (θ) for all V (θ) in the manifold of functions. However, since
homogeneous functions are a much more general class than linear functions, the following result is
not quite as strong as the global convergence in the linear setting.
Theorem 1.	Let V : Rd → Rn be an h-homogeneousfunction such that kV(θ)kμ ≤ C∣∣θ∣∣'. Let
B = k(I-YPYV kμ = ki-Yμ. Then,for any initial conditions θo, if θ follows the dynamics defined by
(7) we have
liminf IlV(θ)∣∣* ≤ B.
t→∞
(10)
The full proof is found in Appendix A.1. The main technique is to use homogeneity to see that
dkθk2 = θτ θ = -θτ VV (θ)T A(V (θ) — V *) = —1V (θ)T A(V (θ) — V *).	(11)
dt	h
This allows us to relate the norm of the value function to the dynamics in parameter space. We can
also extend this result to prove that the limsup of the dynamics attains the same bound if we add a
stronger assumption that the approximator is bi-Holder continuous. This result is in Appendix A.2.
One way to think about the theorem is to say that using a homogeneous approximator does at least
as well as a baseline given by the zero function. This is because B is a potentially tight bound on
k V * 一 0∣μ, but cannot be known a priori since We do not know the expected rewards in advance.
Using this intuition, we can change the parametrization of the function to include a stronger baseline.
We can use a linear baseline since we understand how TD behaves with linear approximators. This
gives a parametrization that resembles residual neural networks (He et al., 2016) and which we will
call residual-homogeneous when the network is also homogeneous.
Definition 2 (Residual-homogeneous). A function f : Rk1 × Rk2 → Rm is residual-homogeneous
iff(x1, x2) = Φx1 + g(x2) where Φ ∈ Rm×k1 andg is h-homogeneous.
For this function class, we prove the following theorem, which extend the ideas from Theorem 1.
Theorem 2.	Let V : Rd1 × Rd2 → Rn be a residual-homogeneous function where Φ is a full
rank feature matrix and k V (θ)kμ ≤ C ∣∣θ∣'. Let ∏φ be the projection onto the span of Φ and let
Bφ = k(I-γP)(V_-πφV )kμ. Thenfor any initial conditions θ0, if θ follows the dynamics defined
by (7) we have
liminf ∣∣V(θ) - ∏φV*∣∣μ ≤ Bφ.
t→∞
(12)
The proof is in Appendix A.3. This allows us to bound the quality of the value function found by
TD learning as compared to the linear baseline, but we may want to also bound the actual distance
to the true value function. Using the above bound relative to the baseline in conjunction with the
quality of the baseline, we can derive the following corollary, with proof in Appendix A.3
4
Published as a conference paper at ICLR 2020
Corollary 1. Under the same assumptions as Theorem 2 we have
1+γ
lit→nfkV⑻-V *kμ≤ (I + 曰 )kV *- πΦV *kμ.	CB)
Remark 2. For linear TD, we get that limt→∞ ∣∣Φθ 一 V * k* ≤ kV —n；V kμ at the fixed point θ*
(Tsitsiklis & Van Roy, 1997). So, our result shows that in terms of the worst case bound, residual-
homogeneous approximators perform similarly to linear functions, especially for large γ.
These are the first results that characterize the behavior of TD for a broad class of nonlinear functions
including neural networks regardless of initialization under the same assumptions on the environ-
ment as used in the analysis of linear TD. Our current results resemble those established in the
context of non-convex optimisation using residual networks (Shamir, 2018), also obtained under
weak assumptions. One direction for future work would be to extend the results from the liminf to
limsup or even to show that the limit exists. Another direction for future work is to try to strengthen
the assumptions and leverage structure in V* itself to reduce the space of possible solutions and be
able to make stronger conclusions.
4 The interaction between approximator and environment
In the previous section we considered a class of approximators for which we can provide guarantees
in all irreducible, aperiodic environments. Now we consider how the function approximator inter-
acts with the environment during TD learning. Recall that prior work has shown that in reversible
environments, TD learning is performing gradient descent (Ollivier, 2018). Our insight is that strict
reversibility is not necessary to make similar guarantees if we also have more information about the
function approximator. As seen in the spiral example, the geometric problem with TD arises from
the combination of “spinning” linear dynamics from an asymmetric A matrix (i.e. a non-reversible
environment) with a poorly conditioned function approximator that “kills” some directions of the
update towards the true value function in function space. In this section we will formalize this no-
tion by showing how we can trade off environment reversibility and approximator conditioning and
still guarantee convergence. First we need a way to quantify how reversible an environment is and
offer the following definition.
Definition 3 (Reversibility coefficient). Let SA := 1/2(A + AT ) and RA := 1/2(A 一 AT) be
the symmetric and anti-symmetric parts of A, as defined in equation (5). Then the reversibility
coefficient ρ(M) is
ρ(M) :=	min
V ∈Rn∖{0}
∣SaV k2 + ∣AVk2
∣RaV k2
(14)
Note that when the environment is reversible this coefficient is infinite since in that case A is sym-
metric and RA is zero. The antisymmetric part RA captures the spinning behavior of the linear
dynamical system in function space (as in the spiral example). At a high level, more spinning means
a less reversible environment and larger RA which lowers the reversibility coefficient.
Now we need a compatible way to quantify the effect of the function approximator. To do this,
We have to examine the matrix W(θ)W(θ)T. This matrix shapes the dynamics of TD and in
the case of neural networks under particular assumptions it is known as the neural tangent kernel
(Jacot et al., 2018). The condition number of this matrix gives us one Way to quantify hoW much
the approximator prefers updates along the directions in function space corresponding to maximal
eigenvalues over those corresponding to minimal eigenvalues. This gives us a Way to quantify hoW
Well-behaved the function approximator is and alloWs us to prove the folloWing theorem.
Theorem 3.	Let κ(M) be the condition number of a matrix M. Assume that for all θ,
K(VV(θ)W(θ)T) < ρ(M).
Then if θ evolves according to (7) we have that for all θ
d∣V(θ)- V* ∣Sa < 0
dt
where SA := 1/2(A + AT). Thus, V(θ) → V* regardless of initial conditions.
(15)
(16)
5
Published as a conference paper at ICLR 2020
The proof is relatively simple and proceeds by using the chain rule to write out the time derivative
of the Lyapunov function and applying the Courant-Fischer-Weyl min-max theorem along with the
assumption to get the result. The full details can be found in Appendix B.
This result provides strong global convergence guarantees, albeit under fairly strong assumptions.
It nevertheless provides intuition about how the environment interacts with the function approxi-
mation. We show that we can use a nonlinear approximator that generalizes across states by using
gradient information so long as the condition number of the tangent kernel of the approximator is
bounded by the reversibility coefficient. Note that for the condition number of the kernel to be finite,
the Jacobian must be full rank which means that the function approximator has more parameters
than the number of states. Such an approximator has more parameters than a tabular approximator,
but can be nonlinear and generalize across states using the structure of the input representation. This
opens a few directions for future work to formalize the relationship between the environment and
approximator in the regime when there are less parameters than states or when the state space is infi-
nite. It may be fruitful to connect this to the literature from supervised learning on over-parametrized
neural networks (see for example Oymak & Soltanolkotabi (2019) and references therein), especially
in the case of value estimation from a finite dataset (i.e. a replay buffer). Another direction would
be to leverage structure in V * and the input representation so that the approximator is effectively
over-parametrized and similar arguments can be made, but it is not clear how to formalize such
assumptions.
4.1	Extension to k-step returns
We now consider how the analysis strategy presented above applies to a classical variation on the TD
learning algorithm. We find that k-step returns have better convergence guarantees by increasing the
reversibility of the effective environment. This is not completely surprising since in the limit k → ∞
We recover gradient descent in the μ-norm with the Monte Carlo algorithm for value estimation.
However, we show that our sufficient condition to guarantee convergence in the well-conditioned
regime weakens exponentially with k . In Appendix C we show that with k step returns the dynamics
of the algorithm become
θ = -VV(θ)TDμ(I -(YP)k)(V(θ) - V*).	(17)
We can define a notion of effective reversibility that scales exponentially with k such that we recover
the same type of convergence as Theorem 3 whenever
κ(VV(θ)VV(θ)T)1/2 < μmin(1 - Yk) (γλ2(P))-k	(18)
μmax
where λ2 (P) is the second largest eigenvalue ofP, which is strictly less than 1 under our irreducible
and aperiodic assumption. This result shows how using k-step returns to increase the effective
reversibility of the environment can lead to better convergence properties. See Appendix C for a
more precise statement of the result and its proof.
4.2	Numerical experiment on the divergent example
We perform a small set of experiments on the divergent spiral example from Section 2.3 which
support our conclusions about reversibility and k-step returns. We integrate the expected dynamics
ODEs in two settings, one where we introduce reversibility into the environment and the other
where we increase the value of k in the algorithm. The function approximator is always the spiral
approximator from the example. We can introduce reversibility by adding reverse connections to the
environment with probability δ ∈ {0, 0.1, 0.2, 0.23} as shown in Figure 2. This effectively reduces
the spinning of the linear dynamical system in function space defined by the Bellman operator. We
find that increasing reversibility eventually leads to convergence. We also validate the result that
increasing k will lead to convergence by increasing the effective reversibility without changing the
MRP. Note that the spiral example is outside the assumptions of the theory in this section since the
function is not well-conditioned, but we wanted to show that the connection between reversibility
and convergence may extend beyond the well-conditioned setting.
6
Published as a conference paper at ICLR 2020
Figure 2: Left: the Markov chain used for the experiments labeled with the transition probabilities.
Center: the spiral divergence example in progressively more reversible environments. A stronger re-
verse connection makes the environment more reversible and eventually causes convergence. Right:
The impact of using k-step returns. We use δ = 0 for all values of k and get convergence for k = 3.
140
120
100
80
60
40
20
5	A generalized divergent example
To motivate the necessity of assumptions similar to the ones that we have made we can look again
to the spiral example of (Tsitsiklis & Van Roy, 1997). Here we generalize this example to arbitrary
number of states for most non-reversible MRPs. Our construction allows for approximators with
arbitrary number of parameters, but restricts them to have rank deficient tangent kernels to mimic
the spiral in a 2-D subspace of function space. The construction can be found in Appendix D, and
the result can be described formally as follows.
Proposition 1. Ifthe MRP is not reversible such that A = Dμ (I - YP) has at least one non-real
eigenvalue, then there exists a function approximator V such that TD learning will diverge. That is,
for any initial parameters θ0, as t → ∞ we have ∣∣ V (θ) — V * k → ∞. Moreover, W (θ) can have
rank up to n - 1, where n is the number of states, for all θ.
The construction of the approximator in the counterexample is somewhat pathological, but any con-
vergence proofs have to make assumptions to rule out these divergent examples. In this work we
avoid these by using either smooth homogeneous functions or by using well-conditioned functions
in nearly-reversible environments. While there may be other assumptions that yield convergence,
they must also account for this class of divergent examples.
6	Related work
6.1	Connections to work in the lazy training regime
Concurrent work (Agazzi & Lu, 2019) has proven convergence of expected TD in the nonlinear,
non-reversible setting in the so-called “lazy training” regime, in which nonlinear models (including
neural networks) with particular parametrization and scaling behave as linear models, with a kernel
given by the linear approximation of the function at initialization. Whereas this kernel captures some
structure from the function approximation, the lazy training regime does not account for feature
selection, since parameters are confined in a small neighborhood around their initialization (Chizat
& Bach, 2018). Another result in a similar direction is from concurrent work (Cai et al., 2019)
which considers two-layer networks (one hidden layer) in the large width regime where only the
first layer is trained. They show that this particular type of function with fixed output layer is nearly
linear and derive global convergence in the limit of large width with an additional assumption on
the regularity of the stationary distribution. In contrast with these works, our results account for
feature selection with more general nonlinear functions. Our homogeneous results hold for a broad
class of approximators much closer to those used in practice and our well-conditioned results hold
for general nonlinear parametrization and provide useful intuition about the relationship between
approximator and environment.
7
Published as a conference paper at ICLR 2020
6.2	Connections to work on fitted value iteration
Another line of work provides convergence rates for fitted value iteration or fitted Q iteration under
the assumption of small optimization error at each iteration (Munos, 2007; Munos & Szepesvari,
2008; Yang et al., 2019b). These papers give bounds that depend on the maximum difference be-
tween the function returned by the Bellman operator applied to the current iterate and its projection
into the space of representable functions (which they call the inherent Bellman residual). This as-
sumption means that a priori the function class has geometry amenable to the MDP being evaluated.
Here we do not rely on any assumptions about successful optimization or an assumption that the
projection of the tabular TD update into the space of representable functions is uniformly small.
Instead we find scenarios where we can guarantee that the difference between the tabular update and
projection cannot be too far so that the optimization procedure succeeds.
6.3	Alternative value estimation algorithms
There are several papers that introduce new algorithms inspired by TD learning but modified so as
to have provable convergence with nonlinear approximators. To our knowledge, all of them use
either a two timescale argument where the optimization procedure at the faster timescale views the
slower timescale as fixed (Borkar, 1997; 2008) or they attempt to optimize a different objective
function (Baird, 1995). Most of the algorithms have not seen widespread use, potentially because
these modifications make optimization more difficult or decrease the quality of solutions. More
specifically, Baird (1995) present residual algorithms, which attempt to optimize a different objective
which avoids double sampling, but has incorrect value functions as solutions (Sutton & Barto, 2018).
Bhatnagar et al. (2009) present GTD2/TDC which uses two timescales to perform gradient descent
on the norm of the TD(0) dynamics projected onto the image of the nonlinear approximator in
function space and thus has the same fixed points as TD(0). More recently, Dai et al. (2018) and
Chung et al. (2019) present two timescale algorithms which are provably convergent. Yang et al.
(2019a) characterize target networks, which have seen widespread use (Mnih et al., 2015), as a
two timescale algorithm. Finally, while they do not provide guaranteed convergence with nonlinear
functions, Achiam et al. (2019) present an algorithm that uses a similar observation to ours about
the connection between TD-learning with nonlinear functions and the neural tangent kernel. Their
algorithm then estimates a preconditioning matrix that serves a similar function as the two-timescale
argument. In contrast to this line of work on algorithmic modifications, our work is a first step
towards characterizing the behavior of nonlinear TD without two timescales or a modified objective.
6.4	Empirical work
Recent empirical work by Fu et al. (2019) empirically investigates the interaction between nonlinear
function approximators (neural networks) with Q-learning, which is a semi-gradient algorithm that
is similar to TD. They find that divergence is rare and that more expressive approximators reduce
approximation error and reduce the chances of divergence. Since these more expressive approxima-
tors are more likely to be well-conditioned, this gives reason to believe it may be possible to extend
our convergence results from the well-conditioned setting for very expressive approximators to more
realistic approximators.
7	Discussion
We have considered the expected continuous dynamics of the TD algorithm for on policy value es-
timation from the perspective of the interaction of the geometry of the function approximator and
environment. Using this perspective we derived two positive results and one negative result. First,
we showed attraction to a compact set when homogeneous approximators like ReLU networks. The
worst case solution in this set is comparable to the worst case linear TD solution for a particular
parametrization inspired by ResNets. Second, we showed global convergence when the environ-
ment is more reversible than the approximator is poorly-conditioned. Finally, we provided a gener-
alized counterexample to motivate the assumptions necessary to rule out bad interactions between
approximator and environment.
There are several possible directions for future work. First, while our results extend both the linear
and reversible convergence regimes, they do not close the gap between the two. One direction for
8
Published as a conference paper at ICLR 2020
future work is thus to provide a unifying analysis that would neatly connect all of the convergent
regimes. Next, it may be possible to find a more precise notion of the well-conditioning necessary to
get local convergence rather than global convergence which would allow extension to more realistic
settings. It may be possible to leverage assumptions about regularity of the true value function so
that the function class is effectively well-conditioned. Another direction is that, while it was beyond
the scope of this paper, it would be instructive to extend the results to finite sample results as has
recently been done for linear TD. It would also be interesting to extend the results to off-policy
and Q-learning settings, but likely would require stronger assumptions. Finally, we would like to
motivate future work by noting that the ultimate goal of this line of work is to put TD on the same
solid footing as optimization in supervised learning where we can characterize easy problems (by
convexity), can guarantee convergence even in hard problems (to local minima), and have some
notions of how to make optimization easier (like over-parametrization). Here we have taken a step
towards this kind of analysis, but the precise characterization of what makes a problem easy and
whether and where TD converges on hard problems remain incomplete.
Acknowledgments
We would like to thank Yann Ollivier for helping to inspire this project and for sharing some of his
notes with us. We also thank the lab mates, especially Will Whitney, Aaron Zweig, and Min Jae
Song, who provided useful discussions and feedback.
This work was partially supported by the Alfred P. Sloan Foundation, NSF RI-1816753, NSF CA-
REER CIF 1845360, and Samsung Electronics.
References
Joshua Achiam, Ethan Knight, and Pieter Abbeel. Towards Characterizing Divergence in Deep
Q-Learning. arXiv e-prints, art. arXiv:1903.08894, Mar 2019.
Andrea Agazzi and Jianfeng Lu. Temporal-difference learning for nonlinear value function approxi-
mation in the lazy training regime. CoRR, abs/1905.10917, 2019. URL http://arxiv.org/
abs/1905.10917.
Leemon Baird. Residual algorithms: Reinforcement learning with function approximation. In
Machine Learning Proceedings 1995,pp. 30-37. Elsevier, 1995.
Jalaj Bhandari, Daniel Russo, and Raghav Singal. A finite time analysis of temporal difference
learning with linear function approximation. arXiv preprint arXiv:1806.02450, 2018.
Shalabh Bhatnagar, Doina Precup, David Silver, Richard S Sutton, Hamid R. Maei, and Csaba
Szepesvari. Convergent temporal-difference learning with arbitrary smooth function approxima-
tion. In Y. Bengio, D. Schuurmans, J. D. Lafferty, C. K. I. Williams, and A. Culotta (eds.),
Advances in Neural Information Processing Systems 22, pp. 1204-1212. Curran Associates, Inc.,
2009.
Vivek S. Borkar. Stochastic approximation with two time scales. Syst. Control Lett., 29(5):291-
294, February 1997. ISSN 0167-6911. doi: 10.1016/S0167-6911(97)90015-3. URL http:
//dx.doi.org/10.1016/S0167-6911(97)90015-3.
V.S. Borkar. Stochastic Approximation: A Dynamical Systems Viewpoint. Cambridge University
Press, 2008. ISBN 9780521515924. URL https://books.google.com/books?id=
QLxIvgAACAAJ.
Qi Cai, Zhuoran Yang, Jason D. Lee, and Zhaoran Wang. Neural temporal-difference learning
converges to global optima. CoRR, abs/1905.10027, 2019. URL http://arxiv.org/abs/
1905.10027.
LenaIc Chizat and Francis Bach. A Note on Lazy Training in Supervised Differentiable Pro-
gramming. working paper or preprint, December 2018. URL https://hal.inria.fr/
hal-01945578.
9
Published as a conference paper at ICLR 2020
Wesley Chung, Somjit Nath, Ajin Joseph, and Martha White. Two-timescale networks for nonlinear
value function approximation. ICLR, 2019.
Bo Dai, Albert Shaw, Lihong Li, Lin Xiao, Niao He, Zhen Liu, Jianshu Chen, and Le Song. SBEED:
Convergent reinforcement learning with nonlinear function approximation. In Jennifer Dy and
Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning,
volume 80 of Proceedings of Machine Learning Research, pp. 1125-1134, Stockholmsmssan,
Stockholm Sweden, 10-15 JUl 2018. PMLR. URL http://Proceedings.mlr.ρress/
v80/dai18c.html.
Justin Fu, Aviral Kumar, Matthew Soh, and Sergey Levine. Diagnosing Bottlenecks in Deep Q-
learning Algorithms. arXiv e-prints, art. arXiv:1902.10250, Feb 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Roger A. Horn and Charles R. Johnson. Topics in matrix analysis. Cambridge University Press,
Cambridge, 1994. ISBN 0-521-46713-6. Corrected reprint of the 1991 original.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In Advances in neural information processing systems, pp. 8571-
8580, 2018.
Tengyuan Liang, Tomaso A. Poggio, Alexander Rakhlin, and James Stokes. Fisher-rao metric,
geometry, and complexity of neural networks. CoRR, abs/1711.01530, 2017. URL http://
arxiv.org/abs/1711.01530.
Hamid R. Maei. Gradient temporal-difference learning algorithms. University of Alberta, 2011.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G.
Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Pe-
tersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran,
Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep rein-
forcement learning. Nature, 518(7540):529-533, February 2015. ISSN 00280836. URL
http://dx.doi.org/10.1038/nature14236.
Remi Munos. Performance bounds in lp-norm for approximate value iteration. SIAM J. Control and
Optimization, 46(2):541-561, 2007.
Remi Munos and Csaba Szepesvari. Finite-time bounds for fitted value iteration. J. Mach. Learn.
Res., 9:815-857, June 2008. ISSN 1532-4435. URL http://dl.acm.org/citation.
cfm?id=1390681.1390708.
Yann Ollivier. Approximate temporal difference learning is a gradient descent for reversible policies.
CoRR, abs/1805.00869, 2018. URL http://arxiv.org/abs/1805.00869.
Samet Oymak and Mahdi Soltanolkotabi. Towards moderate overparameterization: global conver-
gence guarantees for training shallow neural networks. arXiv preprint arXiv:1902.04674, 2019.
H. Robbins and S. Monro. A stochastic approximation method. Annals of Mathematical Statistics,
22:400-407, 1951.
Ohad Shamir. Are resnets provably better than linear predictors? In Advances in neural information
processing systems, pp. 507-516, 2018.
Richard S. Sutton. Learning to predict by the methods of temporal differences. Mach. Learn., 3
(1):9-44, August 1988. ISSN 0885-6125. doi: 10.1023/A:1022633531479. URL http://dx.
doi.org/10.1023/A:1022633531479.
Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT Press,
Cambridge, MA, USA, 2nd edition, 2018.
10
Published as a conference paper at ICLR 2020
John N. Tsitsiklis and Benjamin Van Roy. Analysis of temporal-diffference learning with func-
tion approximation. In M. C. Mozer, M. I. Jordan, and T. Petsche (eds.), Advances in Neural
Information Processing Systems 9, pp. 1075-1081. MIT Press,1997.
Zhuoran Yang, Zuyue Fu, Kaiqing Zhang, and Zhaoran Wang. Convergent reinforcement learn-
ing with function approximation: A bilevel optimization perspective, 2019a. URL https:
//openreview.net/forum?id=ryfcCo0ctQ.
Zhuoran Yang, Yuchen Xie, and Zhaoran Wang. A theoretical analysis of deep q-learning. CoRR,
abs/1901.00137, 2019b. URL http://arxiv.org/abs/1901.00137.
11
Published as a conference paper at ICLR 2020
A TD with homogeneous approximators
A.1 Proof of Theorem 1
Theorem 1. Let V : Rd → Rn be an h-homogeneousfunction such that kV(θ)kμ ≤ C∣∣θ∣∣'∙ Let
B = k(I-YPYV kμ = kιR-Yμ. Then,for any initial conditions θ0, if θ follows the dynamics defined by
(7) we have
liminf ∣∣V(θ)kμ ≤ B.	(10)
t→∞
Proof. Applying the chain rule, (7), and the homogeneity assumption we get that:
dkθ∣2 = θτ θ = -θτ VV (θ)T A(V (θ) — V *) = —1V (θ)T A(V (θ) — V *).
dt	h
For any e > 0, whenever B + e = k(I-Y-YV k" + e < ∣∣ V(θ)∣μ We have that
∣V(θ)TAV*∣≤∣∣V(θ)kμ∣(I - γP)V*∣∣μ
< (1-γ)kV(θ)∣μ -e(1-γ)∣V(θ)kμ
≤∣V(θ)∣μ -Y∣V(θ)∣μ -e(1-γ)(B + e)
≤ V(θ)TDμV(θ) - γV(θ)TDμPV(θ) - C
= V (θ)T AV (θ) - c
for c = (1 - γ)(B + ) > 0. The last inequality follows from an application of Lemma 1 of
(Tsitsiklis & Van Roy, 19§7) that ∣PV∣∣μ ≤ ∣∣ V∣∣μ along with Cauchy-Schwarz to show that:
V(θ)TDμPV(θ) = V(θ)TDN2D/PV(θ) ≤ ∣V(θ)∣μ∣PV(θ)∣μ ≤ ∣V(θ)∣μ.
Putting this all together we get that whenever B + e ‹ ∣∣ V(θ)∣μ we have that
d∣θ∣2 ≤ - 1(V (θ)T AV (θ) - |V (θ)T AV *|) < - C < 0.
dt h	h
Put another way, we can define U =	{θ	:	dkdtk	≥	-c/h}	and W = {V :	∣ V∣∣μ	≤ B + e}.	By the
above, we have that V(U) ⊆ W where V(U) is the image of U under V, i.e. {V : ∃θ ∈ U s.t. V =
V(θ)}. Now define O = {θ : C∣∣θ∣∣' < B + e} which contains an open ball around the origin. By
our Holder continuity assumption, we know that V(O) ⊆ W.
Putting it all together, if θ 6∈ U at time t, the dynamics of ∣θ∣2 are contracting faster than c/h so that
there exists a finite T > t such that at time T the parameters θ(T) will either be in O or U, either
way implying V(θ) will be in W. Since the above reasoning held for any > 0, taking the liminf
we can take E → 0 which yields the result.	□
A.2 Bi-Holder homogeneous stability
Proposition 2. Let V : Rd → Rn be h-homogeneous. Moreover, assume that c∣θ∣s ≤ ∣∣V(θ)∣μ ≤
C∣∣θ∣r for some c, s, r, C > 0. Let B = k(I-Y-YV kμ = k-R-Yμ. Then, for any initial conditions θo,
if θ follows the dynamics defined by (7) we have
lim SUp ∣V(θ)∣μ ≤ C(B/c)r/s.
t→∞
(19)
Proof. Using the same reasoning as above without the additional E, we get that whenever B
k(I-YPYV*k* < ∣∣V(θ)∣μ,we have
|V(θ)TAV*| ≤ V(θ)TAV(θ).
So, whenever B < ∣∣V(θ)∣μ, we have that dkθk2 < 0.
12
Published as a conference paper at ICLR 2020
Define O = {θ : c∣∣θ∣∣s ≤ B}. Thus, by the lower Holder bound, dkθk < 0 for all θ ∈ O. So, We
have that as t → ∞, θ ∈ O. And, by the upper Holder bound, if θ ∈ O then
kV(θ)kμ≤ m∈oCkθ0kr ≤ C(B/c)r/s
Since, θ ∈ O in the limit, we get the desired result.	□
A.3 Proof of Theorem 2 and Corollary 1
Theorem 2. Let V : Rd1 × Rd2 → Rn be a residual-homogeneous function where Φ is a full
rank feature matrix and ∣∣ V (θ)kμ ≤ C ∣∣θ∣∣'. Let ∏φ be the projection onto the span of Φ and let
Bφ = k(I-γP)([-πφV )kμ. Thenfor any initial conditions θ0, if θ follows the dynamics defined
by (7) we have
liminf ∣∣V(θ) — ΠφV*∣∣μ ≤ Bφ.
t→∞
(12)
Proof. Let f be homogeneous and
V(θ)=V(θ1,θ2)=Φθ1+f(θ2)
Let θ1 = ΠφV * be the best linear predictor. In fact, the proof would go through for any θɪ, only the
bound gets worse for worse choice of θ*.
Note that
dkθ - F 0)k2 = -(θ - (θ*,0))>DV(θ)>A(V(θ) - V*)
= -(Φ(θ1 - θ1*) + f (θ2 - 0))>A(V(θ) -V*)
= -(V (θ) - Φθ1*)>A(V (θ) - Φθ1*) - (V (θ) - Φθ1*)>A(Φθ1* - V*).
The residual parametrization is necessary here to have the linear part of DV (θ) independent from θ
so that we can turn the difference of θ1 - θ1* into a difference between functions.
As in the proof of Theorem 1, for any e > 0 we get that whenever Bφ + e < ∣∣ V (θ) — Φθ*∣μ,
I(V(θ) - Φθ*)TA(Φθ* - V*)1 ≤ ∣V(θ) - Φθ*kμk(i - γP)(Φθ* - V*)kμ
<	(i - γ)∣V(θ) - Φθ*∣μ - e(i - γ)∣v(θ) - Φθ*∣μ
≤	nV ⑹一φθ*kμ - γ nV (θ) - φθ*kμ - e(I-Y)(Bφ+°
≤	∣V(θ) - Φθ*∣μ - γ(V(θ) - Φθ*)TDμP(V(θ) - Φθ*) - c
=	(V (θ) - Φθ1*)T A(V (θ) - Φθ1*) - c.
As a consequence we can conclude that whenever Bφ + e < ∣∣V(θ) - Φθ* ∣∣μ
d∣θ-(θ1*,0)∣2
dt
< -c/h.
To conclude the proof, we can define U = {θ : dkθ-dt1,0)k ≥ -c/h} and W = {V : ∣∣V -
Φθ*∣∣μ ≤ B + e}. Then we have that V(U) ⊆ W. Now define O = {(θ*, 0) + θ : C∣∣θ∣∣' < B + e}
which contains an open ball around (θ1*, 0). By our Holder continuity assumption and the residual
parametrization of V, we know that V (O) ⊆ W .
Putting it all together, ifθ 6∈ U at time t, the dynamics of ∣θ∣2 are contracting faster than c/h so that
there exists a finite T > t such that at time T the parameters θ(T) will either be in O or U, either
way implying V(θ) will be in W. Since the above reasoning held for any > 0, taking the liminf
we can take E → 0 which yields the result.	□
Corollary 1. Under the same assumptions as Theorem 2 we have
1+γ
liminf ∣V(θ) - V*||“ ≤ (1 + I)∣V* - ΠφV*||“.	(13)
t→∞	1 - γ
13
Published as a conference paper at ICLR 2020
Proof. Applying the triangle inequality and the result of the Theorem, we get that
liminf IlV(θ) - V*∣∣μ ≤ liminf(∣∣V(θ) - ΠφV*∣∣μ + k∏φV* - V*∣∣μ)
t→∞	t→∞
=liminf ∣V(θ) - ΠφV*||“ + ∣ΠφV* - V*||“
t→∞
≤ BΦ + k∏φV* - V*kμ
Then, we note that II - γP I ≤ 1 + γ since P is a stochastic matrix which lets us bound BΦ by
1+γ∣∣ΠφV* — V*kμ. Putting this together with the above yields the result.	□
B Convergence in the well-conditioned setting
B.1 Proof of the theorem
Theorem 3. Let κ(M) be the condition number of a matrix M. Assume that for all θ,
K(VV(θ)VV(θ)T) < ρ(M).	(15)
Then if θ evolves according to (7) we have that for all θ
dkV⑻-V*kSA < 0	(16)
dt
where SA := 1/2(A + AT). Thus, V(θ) → V* regardless of initial conditions.
Proof. To simplify notation, we use S for SA and R for RA . We define L : Rd → R which will be
the Lyapunov function for the dynamical system as
L(θ)=∣V(θ)-V*∣2S
By applying the chain rule and the polarization identity to (7) we have
L(θ) = -(VV(θ)T(A + AT)(V(θ) - V*), VV(θ)TA(V(θ) - V*),
= -∣VV (θ)T A(V (θ) -V*)∣2 - VV (θ)T AT (V (θ) -V*),VV(θ)TA(V(θ) -V*)
= -∣VV (θ)T A(V (θ) - V*)∣2
-1 (∣VV(θ)T(A + AT)(V(θ) - V*)k2 -kVV(θ)T(A - AT)(V(θ) - V*)∣∣2)
= -∣VV (θ)T A(V (θ) - V *)∣2 - ∣VV(θ)TS(V(θ)-V*)∣2+∣VV(θ)TR(V(θ) -V*)∣2
where R := 1/2(A - AT). So using the assumption and then the min-max theorem,
κ(VV (θ)VV (θ)T) < ρ(M)
λmax(VV (θ)VV (θ)T)∣R(V (θ) - V *)∣2 < λmin(VV(θ)VV(θ)T) ∣A(V(θ)-V*)∣2+∣S(V(θ)-V*)∣2
∣VV(θ)TR(V(θ) -V*)∣2 < ∣VV(θ)TA(V(θ) -V*)∣2 + ∣VV(θ)TS(V(θ) -V*)∣2
where λmax, λmin are the maximal and minimal eigenvalues. Combining, we conclude that L(θ) <
0 and the result follows.	□
B.2 Calculating the reversibility coefficient
The reversibility coefficient can be seen as the minimizer of a generalized Rayleigh quotient (Horn
& Johnson, 1994). Such a quotient for Hermitian matrices B, C is defined as Rb,c(U) = UTBu.
And when C is full rank, we know that this is maximized by the maximal eigenvalue of C-1 B . In
our case, this gives us a way to calculate the reversibility coefficient as
P(M)=(V∈ma∖{0} ∣SaVRA;I∣AV∣2)	= (λmaXaSTSA + ATAlTRARA))	. (20)
14
Published as a conference paper at ICLR 2020
C k-step returns in the well-conditioned setting
C.1 Expected dynamics derivation
The k-step return variant of TD learning replaces r(s, s0) + γVθ(s0) in the original algorithm (6) by
Ptk=-01 γtr(st, st+1) + γkVθ(sk). To get the continuous matrix version of the dynamics we replace
R with V * - YPV * in the following expression to get a telescoping series so that
k-1
V-X(γP)tR+(γP)kV=V-V* -(γP)k(V-V*) = (I- (γP)k)(V - V*).
t=0
Thus, we can define the TD(k) dynamics by
θ = -VV(θ)TDμ(I -(YP)k)(V(θ) - V*).	(21)
C.2 Effective reversibility
To draw the comparison to the TD(0) algorithm analyzed in the paper, we define the matrix
Ak := Dμ(I -(YP)k).
Now We take symmetric and asymmetric parts Sk = 1 (Ak + AT) and Rk = 2 (Ak - AT). Then
we can define reversibility in this effective environment.
Definition 4 (Effective reversibility coefficient).
ρk(M) :
min
V ∈Rn∖{0}
kSk V k2 + kAk V k2
kRk V k2
(22)
C.3 Convergence Proposition
Proposition 3. Let λ2 (P) be the modulus of the second largest eigenvalue of P, which is strictly
less than 1 by our irreducible, aperiodic assumption. Then
“min(1 - Yk) (γλ2(P))-k ≤ Pk(M)1/2	(23)
μmax
矶 V (θ)-v *k2
And if θ follows the ODE defined by (21), then-瓦--k < 0 whenever
κ(VV(θ)VV(θ)T)1/2 < μmin(I - Yk) (γλ2(P))-k.	(24)
μmax
Asa consequence, as long as κ(VV (θ)VV (θ)T) is finite there exists k sufficiently large to guarantee
convergence to V * regardless of initial conditions.
Proof. To extend Theorem 3 to this result, we will show that
“min(I-Yk) (Yλ2(P))-k < min	≡V∣ ≤ Pk(M)1/2
μmax	V∈Rn∖{0} kRkVk
First, since Rk = 2((YPT)kDμ - DμgP)k) We have Rk 1 = 0 since Dμ is the stationary distribu-
tion so that (PT)kDμ1 = DμPk1 = μ. So We only need to consider V ⊥ 1. Now, We have
k
kRkVk ≤ Yr (k(PT)kDμVk + kDμPkVk) ≤ μmaxYk(λ2(P))kkVk
where λ2 is the eigenvalue with second largest magnitude (since 1 is the largest eigenvalue, but it
has eigenvector 1 for P and μ for PT). And we have
kSkV k ≥ λmin(Sk )kV k ≥ (μmin - Ykμmin)kV k
by the Gershgorin circle theorem since Pk defines a distribution with stationary distribution Dμ so
that subtracting the off diagonal from the diagonal of Ak we get λmin(Ak) ≥ μi - Pj YkDμPj =
15
Published as a conference paper at ICLR 2020
μi - Ykμi for all i. Since an analogous argument applies to the columns of Ak, We get the above
bound on the eigenvalues of Sk . Dividing the bounds we get the first result.
dkv (θ)-v *kS
Then by Theorem 3, we get the second result that--------币-----k < 0 when the condition holds.
Moreover, the global convergence folloWs for some sufficiently large k since the effective reversibil-
ity coefficient is increasing exponentially with k.	□
D Generalized divergence construction
Proposition 1. Ifthe MRP is not reversible such that A = Dμ(I — YP) has at least one non-real
eigenvalue, then there exists a function approximator V such that TD learning will diverge. That is,
for any initial parameters θo, as t → ∞ we have ∣∣ V (θ) — V * k → ∞. Moreover, VV (θ) can have
rank up to n - 1, where n is the number of states, for all θ.
Proof. Since A is real, having one non-real eigenvalue implies that A has a pair of complex eigen-
values a + bi, a - bi corresponding to eigenvectors V1 , V2. We know that a > 0 since A is a
non-singular M-matrix and that b 6= 0 by assumption. Define U1 = Re(V1) and U2 = Im(V1) and
U = (U1 U2 ). Note that U1 , U2 are linearly independent since V1 , V2 are so that U is full rank and
that AU = U -ab ab . To construct a divergent approximator, define
so that Q is a matrix of rank 2 with range equal to E := span{U1, U2}.
For any V ∈ EsothatV = U xy we have kVk2
≤ C(x2 + y2) with C > 0. Then
VTQTAV = (x y) UTU(UTU)-1
a2 + b2
- (x y)	0 a
-a
-b
0
2 + b2
-ba (UTU)-1UT U
-(a2 + b2)
ab	x
-b a	y
ITVf
C
Thus, QTA is a matrix of rank 2 with eigenvectors V1 , V2 and eigenvalues (a2 + b2)i, (a2 + b2)i.
Choosing V0 in the span of V1, V2 we define for θ ∈ R:
V(θ) = e(Q+I)θV0 ∈ span{V1,V2}
Then for V * = 0, we aply the above to (7) to get
θ = -VV(θ)TAV(θ) = -V(θ)T(U + QT)AV(θ) > 0
as long as < (a2 + b2 )/C and V(θ) 6= 0. This gives us divergence since as θ → ∞ we have
kV(θ)k→∞.
Now we just need to show that we can make VV (θ) rank n - 1. To do this we will note that
we can define any function V(H) from Rd → E⊥ for any number of parameters and we still get
divergence if we use the function approximator V0(θ, θ) = V(θ) + V@. This is because dV∂θ,θ) =
(I+ Q)V (θ) ⊥ AV (θ) so that θ remains strictly above 0 as before, implying divergence. With this
construction, we can build a divergent approximator V0 on n states with VV0(θ, θH) of rank n - 1
everywhere. For example, set d = n — 2 and let V(θ) be the identity function.	□
E Neural networks are homogeneous
Lemma 1. (Liang et al., 2017) Let σ : R → R be h-homogeneous. Then define a homogeneous
network f : Rd → Rn by
f (θι,…，Θl)= σ(∙∙∙ σ(σ(Φθ1)θ2) ∙∙∙ Θl)	(25)
16
Published as a conference paper at ICLR 2020
where θi is a di-1 × di+1 matrix and Φ is the m × p matrix of data points and σ is applied compo-
nentwise. Then for all 1 ≤ i ≤ L
f(θ) =	hL-i+1	df (θ).	VeC (θi)	and	f (θ)	=	Vf(θ)θ XX hL-i+1∕L	(26)
∂vec(θi )	i=1
where veC maps the matriCes θi to veCtors while θ ∈ Rd is already a veCtor.
Proof. We will consider n = 1 and note that the result applies for arbitrary n by applying the result
to each component function of f .
Define gi(θι,...,θi) = σ(…σ(σ(Φθ1)θ2)…θi-ι)θi, which We will denote simply gi so that
we have f = σ(gL). Note that gi is a di+1 dimensional vector and dL+1 = 1. Define
fi(θ1,...,θi-1) = σ(…σ(σ(Φθ1)θ2)…θi-ι), and denote it fi so that fi = σ(gi). Let f 0 be
Φ. Now we proceed by induction on the gap j - i for j ≥ i from 1 to L - 1.
As the base case, consider i = j . Then we have that
f vec(θi) = f 占vec(θi) = XX f M(仇联
∂vec(θi)	∂gi ∂vec(θi)	e b,c ∂gei ∂(θi)bc
=XX ∂f ∂⅛(θi)bc = σ0(ga) XX ∂⅛(θi)ba
=σ(g) X fb-1(θi)bα = σ'(gia)gia = hσ(gia) = 1 f；
b
Make the inductive assumption that for j - i = k ≥ 0 we have for a ≤ da+1
∂fj	1
f VecR) = h+fa
Now consider j - i = k + . We have
f vec(θa) = ∂vec(θi )	JfL JdjL vec(θ.) = ' X JfL 产-i ∂fj-1 ∂vec(θa)	(θi)	hk+1 彳 ∂fj-1 fb = hk+l XX f fɪ fb-1 = h+ X σ0(ga )(θ )bafbT 1	0jj	1	j hk+1 σ (ga)ga	hk+2 fa
Taking j = L gives us the first result that f(θ) = hL-i+1 ∂f(θ)) vec(θa). Summing over the L
choices of i gives f(θ) = Vf (θ)θ PL=I hL-i+1∕L.	口
17