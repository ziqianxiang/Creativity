Published as a conference paper at ICLR 2020
Mixout: Effective Regularization to Finetune
Large-scale Pretrained Language Models
Cheolhyoung Lee* * * §
Kyunghyun Chot t §
Wanmo Kang*
cheolhyoung.lee@kaist.ac.kr
kyunghyun.cho@nyu.edu
wanmo.kang@kaist.ac.kr
Ab stract
In natural language processing, it has been observed recently that generalization
could be greatly improved by finetuning a large-scale language model pretrained
on a large unlabeled corpus. Despite its recent success and wide adoption, finetun-
ing a large pretrained language model on a downstream task is prone to degenerate
performance when there are only a small number of training instances available.
In this paper, we introduce a new regularization technique, to which we refer as
“mixout”, motivated by dropout. Mixout stochastically mixes the parameters of
two models. We show that our mixout technique regularizes learning to minimize
the deviation from one of the two models and that the strength of regularization
adapts along the optimization trajectory. We empirically evaluate the proposed
mixout and its variants on finetuning a pretrained language model on downstream
tasks. More specifically, we demonstrate that the stability of finetuning and the av-
erage accuracy greatly increase when we use the proposed approach to regularize
finetuning of BERT on downstream tasks in GLUE.
1	Introduction
Transfer learning has been widely used for the tasks in natural language processing (NLP) (Collobert
et al., 2011; Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019; Phang et al., 2018). In particu-
lar, Devlin et al. (2018) recently demonstrated the effectiveness of finetuning a large-scale language
model pretrained on a large, unannotated corpus on a wide range of NLP tasks including question an-
swering and language inference. They have designed two variants of models, BERTLARGE (340M
parameters) and BERTBASE (110M parameters). Although BERTLARGE outperforms BERTBASE
generally, it was observed that finetuning sometimes fails when a target dataset has fewer than 10,000
training instances (Devlin et al., 2018; Phang et al., 2018).
When finetuning a big, pretrained language model, dropout (Srivastava et al., 2014) has been used
as a regularization technique to prevent co-adaptation of neurons (Vaswani et al., 2017; Devlin et al.,
2018; Yang et al., 2019). We provide a theoretical understanding of dropout and its variants, such
as Gaussian dropout (Wang & Manning, 2013), variational dropout (Kingma et al., 2015), and drop-
connect (Wan et al., 2013), as an adaptive L2-penalty toward the origin (all zero parameters 0) and
generalize dropout by considering a target model parameter u (instead of the origin), to which we
refer as mixout(u). We illustrate mixout(u) in Figure 1. To be specific, mixout(u) replaces
all outgoing parameters from a randomly selected neuron to the corresponding parameters of u.
mixout(u) avoids optimization from diverging away from u through an adaptive L2-penalty to-
ward u. Unlike mixout(u), dropout encourages a move toward the origin which deviates away
from u since dropout is equivalent to mixout(0).
We conduct experiments empirically validating the effectiveness of the proposed mixout(wpre)
where wpre denotes a pretrained model parameter. To validate our theoretical findings, we train
a fully connected network on EMNIST Digits (Cohen et al., 2017) and finetune it on MNIST. We
observe that a finetuning solution of mixout(wpre) deviates less from wpre in the L2-sense than
* Department of Mathematical Sciences, KAIST, Daejeon, 34141, Republic of Korea
tNew York University
^ Facebook AI Research
§ CIFAR Azrieli Global Scholar
1
Published as a conference paper at ICLR 2020
(a) Vanilla network at U
(b) Dropout network at W
(c) mixout(u) network at W
Figure 1: Illustration of mixout(u). Suppose that u and w are a target model parameter and a
current model parameter, respectively. (a): We first memorize the parameters of the vanilla network
at u. (b): In the dropout network, we randomly choose an input neuron to be dropped (a dotted
neuron) with a probability of p. That is, all outgoing parameters from the dropped neuron are
eliminated (dotted connections). (c): In the mixout(u) network, the eliminated parameters in (b)
are replaced by the corresponding parameters in (a). In other words, the mixout(u) network at w
is the mixture of the vanilla network at u and the dropout network at w with a probability of p.
that of dropout. In the main experiment, we finetune BERTLARGE with mixout(wpre) on small
training sets of GLUE (Wang et al., 2018). We observe that mixout(wpre) reduces the number
of unusable models that fail with the chance-level accuracy and increases the average development
(dev) scores for all tasks. In the ablation studies, we perform the following three experiments for
finetuning BERTLARGE with mixout(wpre): (i) the effect of mixout(wpre) on a sufficient num-
ber of training examples, (ii) the effect of a regularization technique for an additional output layer
which is not pretrained, and (iii) the effect of probability of mixout(wpre) compared to dropout.
From these ablation studies, we observe that three characteristics of mixout(wpre): (i) finetuning
with mixout(wpre) does not harm model performance even with a sufficient number of training
examples; (ii) It is beneficial to use a variant of mixout as a regularization technique for the addi-
tional output layer; (iii) The proposed mixout(wpre) is helpful to the average dev score and to the
finetuning stability in a wider range of its hyperparameter p than dropout.
1.1	Related Work
For large-scale pretrained language models (Vaswani et al., 2017; Devlin et al., 2018; Yang et al.,
2019), dropout has been used as one of several regularization techniques. The theoretical analysis
for dropout as an L2-regularizer toward 0 was explored by Wan et al. (2013) where 0 is the ori-
gin. They provided a sharp characterization of dropout for a simplified setting (generalized linear
model). Mianjy & Arora (2019) gave a formal and complete characterization of dropout in deep
linear networks with squared loss as a nuclear norm regularization toward 0. However, neither Wan
et al. (2013) nor Mianjy & Arora (2019) gives theoretical analysis for the extension of dropout which
uses a point other than 0.
Wiese et al. (2017), Kirkpatrick et al. (2017), and Schwarz et al. (2018) used L2-penalty toward
a pretrained model parameter to improve performance. They focused on preventing catastrophic
forgetting to enable their models to learn multiple tasks sequentially. They however do not discuss
nor demonstrate the effect of L2-penalty toward the pretrained model parameter on the stability of
finetuning. Barone et al. (2017) introduced tuneout, which is a special case of mixout. They applied
various regularization techniques including dropout, tuneout, and L2-penalty toward a pretrained
model parameter to finetune neural machine translation. They however do not demonstrate empirical
significance of tuneout compared to other regularization techniques nor its theoretical justification.
2	Preliminaries and Notations
Norms and Loss Functions Unless explicitly stated, a norm ∣∣ ∙ ∣∣ refers to L2-norm. A loss
function of a neural network is written as L(W) = n PZi Li(w), where W is a trainable model
parameter. Li is “a per-example loss function” computed on the i-th data point.
2
Published as a conference paper at ICLR 2020
Strong Convexity A differentiable function f is strongly convex if there exists m > 0 such that
f (y) ≥ f (x) + Vf (χ)>(y - x) + mmky - xk2,	(1)
for all x and y .
Weight Decay We refer as “wdecay(u, λ)" to minimizing
L(W) + λ kw — uk2,
instead of the original loss function L(w) where λ is a regularization coefficient. Usual weight
decay of λ is equivalent to wdecay(0, λ).
Probability for Dropout and Dropconnect Dropout (Srivastava et al., 2014) is a regularization
technique selecting a neuron to drop with a probability ofp. Dropconnect (Wan et al., 2013) chooses
a parameter to drop with a probability of p. To emphasize their hyperparameter p, we write dropout
and dropconnect with a drop probability ofp as “dropout(p)” and “dropconnect(p)”, respectively.
dropout(p) is a special case of dropconnect(p) ifwe simultaneously drop the parameters outgoing
from each dropped neuron.
Inverted Dropout and Dropconnect In the case of dropout(p), a neuron is retained with a proba-
bility of 1 - p during training. If we denote the weight parameter of that neuron as w during training,
then we use (1 - p)w for that weight parameter at test time (Srivastava et al., 2014). This ensures
that the expected output of a neuron is the same as the actual output at test time. In this paper,
dropout(p) refers to inverted dropout(p) which uses w/(1 - p) instead of w during training. By
doing so, we do not need to compute the output separately at test time. Similarly, dropconnect(p)
refers to inverted dropconnect(p).
3	Analysis of Dropout and Its Generalization
We start our theoretical analysis by investigating dropconnect which is a general form of dropout
and then apply the result derived from dropconnect to dropout. The iterative SGD equation for
dropconnect(p) with a learning rate ofη is
w(t+1) = W㈤-ηB㈤VL ((EB(t)) 1B⑴W⑴)，t = 0, 1, 2,…，	(2)
where B(t) = diag(B(t), B(t), …，Bdt)) and B(t)'s are mutually independent Bernoulli(1 — P)
random variables with a drop probability of p for all i and t. We regard equation 2 as finding a
solution to the minimization problem below:
minEL ((EBI)-IBW) ,	(3)
where B = diag(Bι, B2, …，Bd) and Bis are mutually independent Bernoulli(1 — P) random
variables with a drop probability of p for all i.
Gaussian dropout (Wang & Manning, 2013) and variational dropout (Kingma et al., 2015) use other
random masks to improve dropout rather than Bernoulli random masks. To explain these variants of
dropout as well, We set a random mask matrix M = diag(Mι, M2, …，Md) to satisfy EMi = μ
and Var(Mi) = σ2 for all i. Now we define a random mixture function with respect to W from u
and M as
Φ(w; u, M) = μ-1((I — M)u + MW — (1 — μ)u),	(4)
and a minimization problem with “mixconnect(u, μ, σ2)''as
minEL(Φ(w; u, M)).	(5)
We can view dropconnect(P) equation 3 as a special case of equation 5 where u = 0 and M = B.
We investigate how mixconnect(u, μ, σ2) differs from the vanilla minimization problem
min EL(W).	(6)
w
If the loss function L is strongly convex, we can derive a lower bound of EL Φ(W; u, M) as in
Theorem 1:
3
Published as a conference paper at ICLR 2020
Theorem 1. Assume that the loss function L is strongly convex. Suppose that a random mixture
function with respect to w from u and M is given by Φ(w; u, M ) in equation 4 where M is
diag(Mι, M2,…，Md) satisfying EMi = μ and Var(Mi) = σ2 for all i. Then, there exists
m > 0 such that
2
EL(Φ(w; u, M)) ≥ L(W) + 2μ2-∣∣w - u∣∣2,
for all w (Proof in Supplement A).
(7)
Theorem 1 shows that minimizing the l.h.s. of equation 7 minimizes the r.h.s. of equation 7 when
the r.h.s. is a sharp lower limit of the l.h.s. The strong convexity of L means that L is bounded from
below by a quadratic function, and the inequality of equation 7 comes from the strong convexity.
Hence, the equality holds if L is quadratic, and mixconnect(u, μ, σ2) is an L2-regularizer with a
regularization coefficient of mσ2∕μ2.
3.1	Mixconnect to Mixout
We propose mixout as a special case of mixconnect, which is motivated by the relationship between
dropout and dropconnect. We assume that
w	w(N1)	w(N1) w(N2)	w(N2)	w(Nk)	w(Nk)
W = ( w1	,	, wdι , w1	,	, wd2 ,	, w1	,	, Wdk J ,
where wj(Ni) is the jth parameter outgoing from the neuron Ni. We set the corresponding M to
M = diag (M(NI),…，M(NI), M(N2),…，M(N2), .............,M(Nk),…，M(Nk)) , (8)
where EM(Ni) = μ and Var(M(Ni)) = σ2 for all i. In this paper, We set M(Ni) to Bernoulli(1 — P)
for all i and mixout(u) hereafter refers to this correlated version of mixconnect with Bernoulli
random masks. We write it as “mixout(u, p)” when we emphasize the mix probability p.
Corollary 1.1. Assume that the loss function L is strongly convex. We denote the random mix-
ture function of mixout(u, p), which is equivalent to that of mixconnect(u, 1 - p, p - p2), as
Φ(W; u, M) where M is defined in equation 8. Then, there exists m > 0 such that
EL(Φ(w; u, B)) ≥ L(w) + 2(1mpp) ∣∣w - u∣2,	(9)
for all W.
Corollary 1.1 is a straightforward result from Theorem 1. As the mix probability p in equation 9
increases to 1, the L2-regularization coefficient of mp/(1 - p) increases to infinity. It means that p
of mixout(u, p) can adjust the strength of L2 -penalty toward u in optimization. mixout(u) differs
from wdecay(u) since the regularization coefficient of mixout(u) depends on m determined by the
current model parameter W. mixout(u, p) indeed regularizes learning to minimize the deviation
from u. We validate this by performing least squares regression in Supplement D.
We often apply dropout to specific layers. For instance, Simonyan & Zisserman (2014) applied
dropout to fully connected layers only. We generalize Theorem 1 to the case in which mixout is
only applied to specific layers, and it can be done by constructing M in a particular way. We
demonstrate this approach in Supplement B and show that mixout for specific layers adaptively
L2-penalizes their parameters.
3.2	Mixout for Pretrained Models
Hoffer et al. (2017) have empirically shown that
∣∣wt - wo∣∣~logt,	(10)
where Wt is a model parameter after the t-th SGD step. When training from scratch, we usually
sample an initial model parameter W0 from a normal/uniform distribution with mean 0 and small
variance. Since W0 is close to the origin, Wt is away from the origin only with a large t by equa-
tion 10. When finetuning, we initialize our model parameter from a pretrained model parameter
Wpre . Since we usually obtain Wpre by training from scratch on a large pretraining dataset, Wpre
is often far away from the origin. By Corollary 1.1, dropout L2-penalizes the model parameter for
deviating away from the origin rather than Wpre . To explicitly prevent the deviation from Wpre , we
instead propose to use mixout(Wpre).
4
Published as a conference paper at ICLR 2020
4 Verification of Theoretical Results for Mixout on MNIST
Wiese et al. (2017) have highlighted that wdecay(wpre) is an effective regularization technique to
avoid catastrophic forgetting during finetuning. Because mixout(wpre) keeps the finetuned model
to stay in the vicinity of the pretrained model similarly to wdecay(wpre), we suspect that the pro-
posed mixout(wpre) has a similar effect of alleviating the issue of catastrophic forgetting. To empir-
ically verify this claim, we pretrain a 784-300-100-10 fully-connected network on EMNIST Digits
(Cohen et al., 2017), and finetune it on MNIST. For more detailed description of the model architec-
ture and datasets, see Supplement C.1.
In the pretraining stage, We run five random experiments with a batch size of 32 for {1, 2, …，20}
training epochs. We use Adam (Kingma & Ba, 2014) with a learning rate of 10-4, β1 = 0.9,
β2 = 0.999, wdecay(0, 0.01), learning rate warm-up over the first 10% steps of the total steps,
and linear decay of the learning rate after the warm-up. We use dropout(0.1) for all layers except
the input and output layers. We select wpre whose validation accuracy on EMNIST Digits is best
(0.992) in all experiments.
For finetuning, most of the model hyperparameters are kept same as in pretraining, with the excep-
tion of the learning rate, number of training epochs, and regularization techniques. We train with
a learning rate of 5 × 10-5 for 5 training epochs. We replace dropout(p) with mixout(wpre, p).
We do not use any other regularization technique such as wdecay(0) and wdecay(wpre). We mon-
itor kwft - wpre k1 2 ,1 validation accuracy on MNIST, and validation accuracy on EMNIST Digits to
compare mixout(wpre, p) to dropout(p) across 10 random restarts.2
(a) kwft - wpre k2
(b) MNIST
(c) EMNIST Digits
Figure 2: We present kwft - wpre k2, validation accuracy on MNIST (target task), and validation
accuracy on EMNIST Digits (source task), as the function of the probability p where wft and wpre
are the model parameter after finetuning and the pretrained model parameter, respectively. We report
mean (curve) ± std. (shaded area) across 10 random restarts. (a): mixout(wpre, p) L2-penalizes
the deviation from wpre, and this penalty becomes strong as p increases. However, with dropout(p),
wft becomes away from wpre asp increases. (b): After finetuning on MNIST, both mixout(wpre, p)
and dropout(p) result in high validation accuracy on MNIST for p ∈ {0.1, 0.2, 0.3}. (c): Valida-
tion accuracy of dropout(p) on EMNIST Digits drops more than that of mixout(wpre, p) for all
p. mixout(wpre, p) minimizes the deviation from wpre and memorizes the source task better than
dropout(p) for all p.
As shown in Figure 2 (a), after finetuning with mixout(wpre , p), the deviation from wpre is min-
imized in the L2-sense. This result verifies Corollary 1.1. We demonstrate that the validation
accuracy of mixout(wpre, p) has greater robustness to the choice of p than that of dropout(p).
In Figure 2 (b), both dropout(p) and mixout(wpre, p) result in high validation accuracy on the
target task (MNIST) for p ∈ {0.1, 0.2, 0.3}, although mixout(wpre, p) is much more robust
with respect to the choice of the mix probability p. In Figure 2 (c), the validation accuracy of
mixout(wpre, p) on the source task (EMNIST Digits) drops from the validation accuracy of the
model at wpre (0.992) to approximately 0.723 regardless of p. On the other hand, the validation
accuracy of dropout(p) on the source task respectively drops by 0.041, 0.074 and 0.105 which are
more than those of mixout(wpre, p) forp ∈ {0.1, 0.2, 0.3}.
1 wft is a model parameter after finetuning.
2Using the same pretrained model parameter wpre but perform different finetuning data shuffling.
5
Published as a conference paper at ICLR 2020
5	Finetuning a Pretrained Language Model with Mixout
In order to experimentally validate the effectiveness of mixout, we finetune BERTLARGE on a sub-
set of GLUE (Wang et al., 2018) tasks (RTE, MRPC, CoLA, and STS-B) with mixout(wpre). We
choose them because Phang et al. (2018) have observed that it was unstable to finetune BERTLARGE
on these four tasks. We use the publicly available pretrained model released by Devlin et al. (2018),
ported into PyTorch by HuggingFace.3 We use the learning setup and hyperparameters recom-
mended by Devlin et al. (2018). We use Adam with a learning rate of 2 × 10-5, β1 = 0.9,
β2 = 0.999, learning rate warmup over the first 10% steps of the total steps, and linear decay
of the learning rate after the warmup finishes. We train with a batch size of 32 for 3 training epochs.
Since the pretrained BERTLARGE is the sentence encoder, we have to create an additional output
layer, which is not pretrained. We initialize each parameter ofit with N(0, 0.022). We describe our
experimental setup further in Supplement C.2.
The original regularization strategy used in Devlin et al. (2018) for finetuning BERTLARGE is
using both dropout(0.1) and wdecay(0, 0.01) for all layers except layer normalization and in-
termediate layers activated by GELU (Hendrycks & Gimpel, 2016). We however cannot use
mixout(wpre) nor wdecay(wpre) for the additional output layer which was not pretrained and
therefore does not have wpre . We do not use any regularization for the additional output layer
when finetuning BERTLARGE with mixout(wpre) and wdecay(wpre). For the other layers, we
replace dropout(0.1) and wdecay(0, 0.01) with mixout(wpre) and wdecay(wpre), respectively.
Phang et al. (2018) have reported that large pretrained models (e.g., BERTLARGE) are prone to
degenerate performance when finetuned on a task with a small number of training examples, and that
multiple random restarts4 are required to obtain a usable model better than random prediction. To
compare finetuning stability of the regularization techniques, we need to demonstrate the distribution
of model performance. We therefore train BERTLARGE with each regularization strategy on each
task with 20 random restarts. We validate each random restart on the dev set to observe the behaviour
of the proposed mixout and finally evaluate it on the test set for generalization. We present the test
score of our proposed regularization strategy on each task in Supplement C.3.
We finetune BERTLARGE with mixout(wpre , {0.7, 0.8, 0.9}) on RTE, MRPC, CoLA, and STS-
B. For the baselines, we finetune BERTLARGE with both dropout(0.1) and wdecay(0, 0.01) as
well as with wdecay(wpre, {0.01, 0.04, 0.07, 0.10}). These choices are made based on the exper-
iments in Section 6.3 and Supplement F. In Section 6.3, we observe that finetuning BERTLARGE
with mixout(wpre, p) on RTE is significantly more stable with p ∈ {0.7, 0.8, 0.9} while fine-
tuning with dropout(p) becomes unstable as p increases. In Supplement F, we demonstrate that
dropout(0.1) is almost optimal for all the tasks in terms of mean dev score although Devlin et al.
(2018) selected it to improve the maximum dev score.
In Figure 3, we plot the distributions of the dev scores from 20 random restarts when finetuning
BERTLARGE with various regularization strategies on each task. For conciseness, we only show
four regularization strategies; Devlin et al. (2018)’s: both dropout(0.1) and wdecay(0, 0.01),
Wiese et al. (2017)’s: wdecay(wpre, 0.01), ours: mixout(wpre, 0.7), and ours+Wiese et al.
(2017)'s: both mixout(wpre, 0.7) and Wdecay(wp® 0.01). As shown in Figure 3 (a-c), We ob-
serve many finetuning runs that fail with the chance-level accuracy when we finetune BERTLARGE
with both dropout(0.1) and Wdecay(0, 0.01) on RTE, MRPC, and CoLA. We also have a bunch of
degenerate model configurations when we use Wdecay(wpre, 0.01) without mixout(wpre, 0.7).
Unlike existing regularization strategies, when we use mixout(wpre, 0.7) as a regularization tech-
nique with or without Wdecay(wpre, 0.01) for finetuning BERTLARGE, the number of degenerate
model configurations that fail with a chance-level accuracy significantly decreases. For example, in
Figure 3 (c), we have only one degenerate model configuration when finetuning BERTLARGE with
mixout(wpre, 0.7) on CoLA while we observe respectively seven and six degenerate models with
Devlin et al. (2018)’s and Wiese et al. (2017)’s regularization strategies.
3https : / / s3 . amazonaws . com / models . huggingface . co / bert /
bert-large-uncased-pytorch_model.bin
4Using the same pretrained model parameter wpre but each random restart differs from the others by shuf-
fling target data and initializing the additional output layer differently.
6
Published as a conference paper at ICLR 2020
70
60
50
ə,lous >ΦQ
92.5
90.0
ω
o 87.5
(j
S
成 85.0
Q
82.5
80.0
Devlin Wiese
Devlin Wiese Our Our+W
Our Our+W
(a)	RTE (Accuracy)
60 q步
40
20
0	―
Devlin Wiese Our Our+W
(b)	MRPC (F1 accuracy)
100
80
60
40
20
° Devlin Wiese Our Our+W
(c)	CoLA (Mattew’s correlation)
(d)	STS-B (Spearman correlation)
Figure 3: Distribution of dev scores on each task from 20 random restarts when finetuning
BERTLARGE with Devlin et al. (2018)’s: both dropout(0.1) and wdecay(0, 0.01), Wiese et al.
(2017)’s: wdecay(wpre, 0.01), ours: mixout(wpre, 0.7), and ours+Wiese et al. (2017)’s: both
mixout(wpre, 0.7) and wdecay(wpre, 0.01). We write them as Devlin (blue), Wiese (orange), Our
(green), and Our+W (red), respectively. We use the same set of 20 random initializations across all
the regularization setups. Error intervals show mean±std. For all the tasks, the number of finetuning
runs that fail with the chance-level accuracy is significantly reduced when we use our regularization
mixout(wpre, 0.7) regardless of using wdecay(wpre, 0.01).
In Figure 3 (a), we further improve the stability of finetuning BERTLARGE by using both
mixout(wpre, 0.7) and wdecay(wpre, 0.01). Figure 3 (d) shows respectively two and one degener-
ate model configurations with Devlin et al. (2018)’s and Wiese et al. (2017)’s, but we do not have any
degenerate resulting model with ours and ours+Wiese et al. (2017)’s. In Figure 3 (b, c), we observe
that the number of degenerate model configurations increases when we use wdecay(wpre, 0.01)
additionally to mixout(wpre, 0.7). In short, applying our proposed mixout significantly stabi-
lizes the finetuning results of BERTLARGE on small training sets regardless of whether we use
wdecay(wpre, 0.01).
In Table 1, we report the average and the best dev scores across 20 random restarts for each task
with various regularization strategies. The average dev scores with mixout(wpre, {0.7, 0.8, 0.9})
increase for all the tasks. For instance, the mean dev score of finetuning with mixout(wpre, 0.8) on
CoLA is 57.9 which is 49.2% increase over 38.8 obtained by finetuning with both dropout(p) and
wdecay(0, 0.01). We observe that using wdecay(wpre, {0.01, 0.04, 0.07, 0.10}) also improves
the average dev scores for most tasks compared to using both dropout(p) and wdecay(0, 0.01).
We however observe that finetuning with mixout(wpre, {0.7, 0.8, 0.9}) outperforms that with
wdecay(wpre, {0.01, 0.04, 0.07, 0.10}) on average. This confirms that mixout(wpre) has a dif-
ferent effect for finetuning BERTLARGE compared to wdecay(wpre) since mixout(wpre) is an
adaptive L2 -regularizer along the optimization trajectory.
7
Published as a conference paper at ICLR 2020
Since finetuning a large pretrained language model such as BERTLARGE on a small training set fre-
quently fails, the final model performance has often been reported as the maximum dev score (Devlin
et al., 2018; Phang et al., 2018) among a few random restarts. We thus report the best dev score for
each setting in Table 1. According to the best dev scores as well, mixout(wpre, {0.7, 0.8, 0.9})
improves performance for all the tasks compared to using both dropout(p) and wdecay(0, 0.01).
For instance, using mixout(wpre , 0.9) improves the maximum dev score by 0.9 compared to
using both dropout(p) and wdecay(0, 0.01) on MRPC. Unlike the average dev scores, the
best dev scores achieved by using wdecay(wpre, {0.01, 0.04, 0.07, 0.10}) are better than those
achieved by using mixout(wpre, {0.7, 0.8, 0.9}) except RTE on which it was better to use
mixout(wpre, {0.7, 0.8, 0.9}) than wdecay(wpre, {0.01, 0.04, 0.07, 0.10}).
Table 1: Mean (max) dev scores across 20 random restarts when finetuning BERTLARGE with
various regularization strategies on each task. We show the following baseline results on the first and
second cells: Devlin et al. (2018)’s regularization strategy (both dropout(p) and wdecay(0, 0.01))
and Wiese et al. (2017)’s regularization strategy (wdecay(wpre, {0.01, 0.04, 0.07, 0.10})). In the
third cell, we demonstrate finetuning results with only mixout(wpre, {0.7, 0.8, 0.9}). The results
with both mixout(wpre, {0.7, 0.8, 0.9}) and wdecay(wpre, 0.01) are also presented in the fourth
cell. Bold marks the best of each statistics within each column. The mean dev scores greatly increase
for all the tasks when we use mixout(wpre, {0.7, 0.8, 0.9}).
TECHNIQUE 1	TECHNIQUE 2	RTE	MRPC	CoLA	STS-B
dropout(0.1)	wdecay(0, 0.01)	56.5 (73.6)	83.4 (90.4)	38.8 (63.3)	82.4 (90.3)
-	wdecay(wpre, 0.01)	56.3 (71.5)	86.2 (91.6)	41.9 (65.6)	85.4 (90.5)
-	wdecay(wpre, 0.04)	51.5( 70.8)	85.8 (91.5)	35.4 (64.7)	80.7 (90.6)
-	wdecay(wpre, 0.07)	57.0 (70.4)	85.8 (91.0)	48.1 (63.9)	89.6 (90.3)
-	wdecay(wpre, 0.10)	54.6 (71.1)	84.2 (91.8)	45.6 (63.8)	84.3 (90.1)
mixout(wpre, 0.7)	-	61.6 (74.0)	87.1 (91.1)	57.4 (62.1)	89.6 (90.3)
mixout(wpre, 0.8)	-	64.0 (74.0)	89.0 (90.7)	57.9 (63.8)	89.4 (90.3)
mixout(wpre, 0.9)	-	64.3 (73.3)	88.2 (91.4)	55.2 (63.4)	89.4 (90.0)
mixout(wpre, 0.7)	wdecay(wpre, 0.01)	65.3 (74.4)	87.8 (91.8)	51.9 (64.0)	89.6 (90.6)
mixout(wpre, 0.8)	wdecay(wpre, 0.01)	62.8 (74.0)	86.3 (90.9)	58.3 (65.1)	89.7 (90.3)
mixout(wpre, 0.9)	wdecay(wpre, 0.01)	65.0 (755)	88.6 (91.3)	58.1 (65.1)	89.5 (90.0)
We investigate the effect of combining both mixout(wpre) and wdecay(wpre) to see whether
they are complementary. We finetune BERTLARGE with both mixout(wpre, {0.7, 0.8, 0.9})
and wdecay(wpre, 0.01). This leads not only to the improvement in the average dev scores but
also in the best dev scores compared to using wdecay(wpre, {0.01, 0.04, 0.07, 0.10}) and us-
ing both dropout(p) and wdecay(0, 0.01). The experiments in this section confirm that using
mixout(wpre) as one of several regularization techniques prevents finetuning instability and yields
gains in dev scores.
6	Ablation S tudy
In this section, we perform ablation experiments to better understand mixout(wpre). Unless explic-
itly stated, all experimental setups are the same as in Section 5.
6.1	Mixout with a Sufficient Number of Training Examples
We showed the effectiveness of the proposed mixout finetuning with only a few training examples
in Section 5. In this section, we investigate the effectiveness of the proposed mixout in the case
of a larger finetuning set. Since it has been stable to finetune BERTLARGE on a sufficient number
of training examples (Devlin et al., 2018; Phang et al., 2018), we expect to see the change in the
behaviour of mixout(wpre) when we use it to finetune BERTLARGE on a larger training set.
We train BERTLARGE by using both mixout(wpre, 0.7) and wdecay(wpre, 0.01) with 20 random
restarts on SST-2.5 We also train BERTLARGE by using both dropout(p) and wdecay(0, 0.01)
with 20 random restarts on SST-2 as the baseline. In Table 2, we report the mean and maximum of
5For the description of SST-2, see Supplement C.2.
8
Published as a conference paper at ICLR 2020
SST-2 dev scores across 20 random restarts with each regularization strategy. We observe that there
is little difference between their mean and maximum dev scores on a larger training set, although
using both mixout(wpre, 0.7) and wdecay(wpre, 0.01) outperformed using both dropout(p) and
wdecay(0, 0.01) on the small training sets in Section 5.
Table 2: Mean (max) SST-2 dev scores across 20 random restarts when finetuning BERTLARGE
with each regularization strategy. Bold marks the best of each statistics within each column. For a
large training set, both mean and maximum dev scores are similar to each other.
TECHNIQUE 1	TECHNIQUE 2	SST-2
dropout(0.1)	Wdecay(0, 0.01)	93.4 (94.0)
mixout(wpre, 0.7)	wdecay(wpre, 0.01)	93.5 (943)
6.2	Effect of a Regularization Technique for an Additional Output Layer
In this section, we explore the effect of a regularization technique for an additional output layer.
There are two regularization techniques available for the additional output layer: dropout(p) and
mixout(w0 , p) where w0 is its randomly initialized parameter. Either of these strategies differs
from the earlier experiments in Section 5 where we did not put any regularization for the additional
output layer.
Table 3: We present mean (max) dev scores across 20 random restarts with various regularization
techniques for the additional output layers (ADDITIONAL) when finetuning BERTLARGE on each
task. For all cases, we apply mixout(wpre, 0.7) to the pretrained layers (PRETRAINED). The
first row corresponds to the setup in Section 5. In the second row, we apply mixout(w0, 0.7) to
the additional output layer where w0 is its randomly initialized parameter. The third row shows
the results obtained by applying dropout(0.7) to the additional output layer. In the fourth row, we
demonstrate the best of each result from all the regularization strategies shown in Table 1. Bold
marks the best of each statistics within each column. We obtain additional gains in dev scores by
varying the regularization technique for the additional output layer.
PRETRAINED		ADDrnONAL	RTE	MRPC	CoLA	STS-B
mixout(wpre,	0.7)	-	61.6 (74.0)	87.1 (91.1)	57.4 (62.1)	89.6 (90.3)
mixout(w re, pre	0.7)	mixout(wo, 0.7)	66.5 (75.5)	88.1 (92.4)	58.7 (65.6)	89.7 (90.6)
mixout(wpre,	0.7)	dropout(0.7)	57.2 (70.8)	85.9 (92.5)	48.9 (64.3)	89.2 (89.8)
The best of each result from Table 1			65.3( 755)	89.0 (91.8)	58.3 (65.6)	89.7 (90.6)
We report the average and best dev scores across 20 random restarts when finetuning BERTLARGE
with mixout(wpre, 0.7) while varying the regularization technique for the additional output layer in
Table 3.6 We observe that using mixout(w0, 0.7) for the additional output layer improves both the
average and best dev score on RTE, CoLA, and STS-B. In the case of MRPC, we have the highest
best-dev score by using dropout(0.7) for the additional output layer while the highest mean dev
score is obtained by using mixout(w0, 0.7) for it. In Section 3.2, we discussed how mixout(w0)
does not differ from dropout when the layer is randomly initialized, since we sample w0 from w
whose mean and variance are 0 and small, respectively. Although the additional output layer is
randomly initialized, we observe the significant difference between dropout and mixout(w0) in this
layer. We conjecture that kw0 - 0k is not sufficiently small because Ekw - 0k is proportional to
the dimensionality of the layer (2,048). We therefore expect mixout(w0) to behave differently from
dropout even for the case of training from scratch.
In the last row of Table 3, we present the best of the corresponding result from Table 1. We
have the highest mean and best dev scores when we respectively use mixout(wpre, 0.7) and
mixout(w0, 0.7) for the pretrained layers and the additional output layer on RTE, CoLA, and STS-
B. The highest mean dev score on MRPC is obtained by using mixout(wpre, 0.8) for the pretrained
layers which is one of the results in Table 1. We have the highest best dev score on MRPC when
we use mixout(wpre, 0.7) and dropout(0.7) for the pretrained layers and the additional output
6In this experiment, we use neither wdecay(0) nor wdecay(wpre).
9
Published as a conference paper at ICLR 2020
layer, respectively. The experiments in this section reveal that using mixout(w0) for a randomly
initialized layer of a pretrained model is one of the regularization schemes to improve the average
dev score and the best dev score.
6.3	Effect of Mix Probability for Mixout and Dropout
We explore the effect of the hyperparameter p when finetuning BERTLARGE with mixout(wpre , p)
and dropout(p). We train BERTLARGE With mixout(wpre, {0.0, 0.1,…，0.9}) on RTE with
20 random restarts. We also train BERTLARGE after replacing mixout(wpre, p) by dropout(p)
with 20 random restarts. We do not use any regularization technique for the additional output
layer. Because we use neither wdecay(0) nor wdecay(wpre) in this section, dropout(0.0) and
mixout(wpre, 0.0) are equivalent to finetuning without regularization.
Probability
Figure 4: Distribution of RTE dev scores (Accuracy) from 20 random restarts when finetun-
ing BERTLARGE with dropout(p) (orange) or mixout(wpre, p) (blue). Error intervals show
mean±std. We do not use wdecay(0) nor wdecay(wpre). In the case of mixout(wpre, p), the
number of usable models after finetuning with mixout(wpre, {0.7, 0.8, 0.9}) is significantly more
than the number of usable models after finetuning with dropout(p) for all p.
It is not helpful to vary p for dropout(p) while mixout(wpre, p) helps significantly in a wide range
of p. Figure 4 shows distributions of RTE dev scores across 20 random restarts when finetuning
BERT large with dropout(p) and mixout(wpre, P) for P ∈ {0.0, 0.1, •一，0.9}. The mean
dev score of finetuning BERTLARGE with mixout(wpre, p) increases as p increases. On the other
hand, the mean dev score of finetuning BERTLARGE with dropout(P) decreases as P increases. If
P is less than 0.4, finetuning with mixout(wpre, P) does not improve the finetuning results of using
dropout({0.0, 0.1, 0.2}). We however observe that mixout(wpre, {0.7, 0.8, 0.9}) yields better
average dev scores than dropout(P) for all P, and significantly reduces the number of finetuning
runs that fail with the chance-level accuracy.
We notice that the proposed mixout spends more time than dropout from the experiments in this
section. It takes longer to finetune a model with the proposed mixout than with the original dropout,
although this increase is not significant especially considering the waste of time from failed finetun-
ing runs using dropout. In Supplement E, we describe more in detail the difference between mixout
and dropout in terms of wall-clock time.
7	Conclusion
The special case of our approach, mixout(wpre), is one of several regularization techniques mod-
ifying a finetuning procedure to prevent catastrophic forgetting. Unlike wdecay(wpre) proposed
earlier by Wiese et al. (2017), mixout(wpre) is an adaptive L2-regularizer toward wpre in the sense
that its regularization coefficient adapts along the optimization path. Due to this difference, the pro-
posed mixout improves the stability of finetuning a big, pretrained language model even with only
a few training examples of a target task. Furthermore, our experiments have revealed the proposed
approach improves finetuning results in terms of the average accuracy and the best accuracy over
multiple runs. We emphasize that our approach can be used with any pretrained language models
such as RoBERTa (Liu et al., 2019) and XLNet (Yang et al., 2019), since mixout does not depend
on model architectures, and leave it as future work.
10
Published as a conference paper at ICLR 2020
Acknowledgments
The first and third authors’ work was supported by the National Research Foundation of Korea
(NRF) grants funded by the Korea government (MOE, MSIT) (NRF-2017R1A2B4011546, NRF-
2019R1A5A1028324). The second author thanks support by AdeptMind, eBay, TenCent, NVIDIA
and CIFAR and was partly supported by Samsung Electronics (Improving Deep Learning using
Latent Structure).
References
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450, 2016.
Antonio Valerio Miceli Barone, Barry Haddow, Ulrich Germann, and Rico Sennrich. Regularization
techniques for fine-tuning in neural machine translation. arXiv preprint arXiv:1707.09920, 2017.
Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017 task
1: Semantic textual similarity-multilingual and cross-lingual focused evaluation. arXiv preprint
arXiv:1708.00055, 2017.
Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre van Schaik. Emnist: an extension of
mnist to handwritten letters. arXiv preprint arXiv:1702.05373, 2017.
Ronan Collobert, Jason Weston, Leon Bottou, Michael Karlen, Koray KavUkcUoglu, and Pavel
Kuksa. Natural language processing (almost) from scratch. Journal of machine learning research,
12(Aug):2493-2537, 2011.
Ido Dagan, Oren Glickman, and Bernardo Magnini. The PASCAL recognising textual entailment
challenge. In Machine learning challenges. evaluating predictive uncertainty, visual object clas-
Sfication, and recognising tectual entailment, pp. 177-190. Springer, 2006.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
William B Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases.
In Proceedings of the International Workshop on Paraphrasing, 2005.
Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint
arXiv:1606.08415, 2016.
Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the generaliza-
tion gap in large batch training of neural networks. In Advances in Neural Information Processing
Systems, pp. 1731-1741, 2017.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Durk P Kingma, Tim Salimans, and Max Welling. Variational dropout and the local reparameteri-
zation trick. In Advances in Neural Information Processing Systems, pp. 2575-2583, 2015.
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A
Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcom-
ing catastrophic forgetting in neural networks. Proceedings of the national academy of sciences,
114(13):3521-3526, 2017.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
approach. arXiv preprint arXiv:1907.11692, 2019.
Poorya Mianjy and Raman Arora. On dropout and nuclear norm regularization. In Kamalika Chaud-
huri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Ma-
chine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 4575-4584, Long
Beach, California, USA, 09-15 Jun 2019. PMLR. URL http : / / proceedings . mlr .
press/v97/mianjy19a.html.
11
Published as a conference paper at ICLR 2020
Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In
Proceedings of the 27th international conference on machine learning (ICML-10), pp. 807-814,
2010.
Jason Phang, ThibaUlt Fevry, and Samuel R Bowman. Sentence encoders on stilts: Supplementary
training on intermediate labeled-data tasks. arXiv preprint arXiv:1811.01088, 2018.
Jonathan Schwarz, Jelena Luketina, Wojciech M Czarnecki, Agnieszka Grabska-Barwinska,
Yee Whye Teh, Razvan Pascanu, and Raia Hadsell. Progress & compress: A scalable frame-
work for continual learning. arXiv preprint arXiv:1805.06370, 2018.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng,
and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment
treebank. In Proceedings of EMNLP, pp. 1631-1642, 2013.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. The Journal of Machine
Learning Research, 15(1):1929-1958, 2014.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998-6008, 2017.
Li Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, and Rob Fergus. Regularization of neural
networks using dropconnect. In International conference on machine learning, pp. 1058-1066,
2013.
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.
Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv
preprint arXiv:1804.07461, 2018.
Sida Wang and Christopher Manning. Fast dropout training. In international conference on machine
learning, pp. 118-126, 2013.
Alex Warstadt, Amanpreet Singh, and Samuel R. Bowman. Neural network acceptability judgments.
arXiv preprint 1805.12471, 2018.
Georg Wiese, Dirk Weissenborn, and Mariana Neves. Neural domain adaptation for biomedical
question answering. arXiv preprint arXiv:1706.03610, 2017.
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V
Le. Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprint
arXiv:1906.08237, 2019.
12
Published as a conference paper at ICLR 2020
Supplementary Material
A Proofs for Theorem 1
Theorem 1. Assume that the loss function L is strongly convex. Suppose that a random mixture
function with respect to w from u and M is given by
Φ(w; u, M) = μ-1((I — M)u + MW — (1 — μ)u),
where M is diag(Mι, M2,…，Md) satisfying EMi = μ and Var(Mi) = σ2 for all i. Then,
there exists m > 0 such that
2
EL(Φ(w; u, M)) ≥ L(W) + ~2μ2∣∣w — u∣∣2,	(11)
for all W.
Proof. Since L is strongly convex, there exist m > 0 such that
EL(Φ(w; u, M)) = EL (W + (Φ(w; u, M) — W))
≥ L(W) + VL(w)>E[Φ(w; u, M) — w] + mE∣Φ(w; u, M) — w∣2,
2	(12)
for all W by equation 1. Recall that EMi = μ and Var(Mi) = σ2 for all i. Then, We have
E[Φ(W; u, M) — W] = 0,
and
1 2
E∣Φ(w; u, M) — w∣2 = E 一(W — u)(M — μI)
μ
1d
=-2 ɪz(wi - Ui) E(Mi — μ)
μ2 i=1
2
=-IIw - u∣2.
μ2
By using equation 13 and equation 14, We can reWrite equation 12 as
2
EL(Φ(w; u, M)) ≥ L(W) + ~2μ2 IIW — u∣∣2.
(13)
(14)
□
B Applying to S pecific Layers
We often apply dropout to specific layers. For instance, Simonyan & Zisserman (2014) applied
dropout to fully connected layers only. We generalize Theorem 1 to the case in Which mixconnect is
only applied to specific layers, and it can be done by constructing M in a particular Way. To better
characterize mixconnect applied to specific layers, We define the index set I as I = {i : Mi = 1}.
Furthermore, We use W and U to denote (Wi)i/ɪ and (ui)i∈1, respectively. Then, we generalize
equation 7 to
2
EL(Φ(w; u, M)) ≥ L(w) + ~2μ2IIW — u∣2.
(15)
From equation 15, applying mixconnect(u, μ, σ2) is to use adaptive Wdecay(U) on the weight
parameter of the specific layers W. Similarly, we can regard applying miχout(u, P) to specific
layers as adaptive Wdecay(u).
13
Published as a conference paper at ICLR 2020
C Experimental Details
C.1 From EMNIST Digits to MNIST
Model Architecture The model architecture in Section 4 is a 784-300-100-10 fully connected
network with a softmax output layer. For each hidden layer, we add layer normalization (Ba et al.,
2016) right after the ReLU (Nair & Hinton, 2010) nonlinearity. We initialize each parameter with
N(0, 0.022) and each bias with 0.
Regularization In the pretraining stage, we use dropout(0.1) and wdecay(0, 0.01). We apply
dropout(0.1) to all hidden layers. That is, we do not drop neurons of the input and output layers.
wdecay(0, 0.01) does not penalize the parameters for bias and layer normalization. When we
finetune our model on MNIST, we replace dropout(p) with mixout(wpre, p). We use neither
wdecay(0) nor wdecay(wpre) for finetuning.
Dataset For pretraining, we train our model on EMNIST Digits. This dataset has 280,000 char-
acters into 10 balanced classes. These characters are compatible with MNIST characters. EMNIST
Digits provides 240,000 characters for training and 40,000 characters for test. We use 240,000 char-
acters provide for training and split these into the training set (216,000 characters) and validation set
(24,000 characters). For finetuning, we train our model on MNIST. This has 70,000 characters into
10 balance classes. MNIST provide 60,000 characters for training and 10,000 characters for test.
We use 60,000 characters given for training and split these into the training set (54,000 characters)
and validation set (6,000 characters).
Data Preprocessing We only use normalization after scaling pixel values into [0, 1]. We do not
use any data augmentation.
C.2 Finetuning BERT on partial GLUE tasks
Model Architecture Because the model architecture of BERTLARGE is identical to the original
(Devlin et al., 2018), we omit its exhaustive description. Briefly, BERTLARGE has 24 layers, 1024
hidden size, and 16 self-attention heads (total 340M parameters). We use the publicly available pre-
trained model released by Devlin et al. (2018), ported into PyTorch by HuggingFace.7 We initialize
each weight parameter and bias for an additional output layer with N (0, 0.022) and 0, respectively.
Regularization In the finetuning stage, Devlin et al. (2018) used wdecay(0, 0.01) for all param-
eters except bias and layer normalization. They apply dropout(0.1) to all layers except each hidden
layer activated by GELU (Hendrycks & Gimpel, 2016) and layer normalization. We substitute
wdecay(wpre) and mixout(wpre) for wdecay(0, 0.01) and dropout(0.1), respectively.
Dataset We use a subset of GLUE (Wang et al., 2018) tasks. The brief description for each dataset
is as the following:
•	RTE (2,500 training examples): Binary entailment task (Dagan et al., 2006)
•	MRPC (3,700 training examples): Semantic similarity (Dolan & Brockett, 2005)
•	CoLA (8,500 training examples): Acceptability classification (Warstadt et al., 2018)
•	STS-B (7,000 training examples): Semantic textual similarity (Cer et al., 2017)
•	SST-2 (67,000 training examples): Binary sentiment classification (Socher et al., 2013)
In this paper, we reported F1 accuracy scores for MRPC, Mattew’s correlation scores for CoLA,
Spearman correlation scores for STS-B, and accuracy scores for the other tasks.
Data Preprocessing We use the publicly available implementation of BERTLARGE by Hugging-
Face.8
7https : / / s3 . amazonaws . com / models . huggingface . co / bert /
bert-large-uncased-pytorch_model.bin
8 https://github.com/huggingface/pytorch- transformers
14
Published as a conference paper at ICLR 2020
C.3 Test Results on GLUE tasks
We expect that using mixout stabilizes finetuning results of BERTLARGE on a small training set.
To show this, we demonstrated distributions of dev scores from 20 random restarts on RTE, MRPC,
CoLA, and STS-B in Figure 3. We further obtained the highest average/best dev score on each task
in Table 3. To confirm the generalization of the our best model on the dev set, we demonstrate the
test results scored by the evaluation server9 in Table 4.
Table 4: We present the test score when finetuning BERTLARGE with each regularization strat-
egy on each task. The first row shows the test scores obtained by using both dropout(p) and
wdecay(0, 0.01). These results in the first row are reported by Devlin et al. (2018). They used
the learning rate of {2 × 10-5, 3 × 10-5, 4 × 10-5, 5 × 10-5} and a batch size of 32 for 3
epochs with multiple random restarts. They selected the best model on each dev set. In the second
row, we demonstrate the test scores obtained by using the proposed mixout in Section 6.2: using
mixout(wpre, 0.7) for the pretrained layers and mixout(w0, 0.7) for the additional output layer
where w0 is its randomly initialized weight parameter. We used the learning rate of 2 × 10-5 and a
batch size of 32 for 3 epochs with 20 random restarts. We submitted the best model on each dev set.
The third row shows that the test scores obtained by using both dropout(p) and wdecay(0, 0.01)
with same experimental setups of the second row. Bold marks the best within each column. The
proposed mixout improves the test scores except MRPC compared to the original regularization
strategy proposed by Devlin et al. (2018).
STRATEGY	RTE	MRPC	CoLA	STS-B
Devlinetal.(2018)	=	70.1	89.3	60.5	86.5
mixout(wpre, 0.7) & mixout(wo, 0.7)	70.2	89.1	62.1	87.3
dropout(p) + Wdecay(0, 0.01)	68.2	88.3	59.6	86.0
For all the tasks except MRPC, the test scores obtained by the proposed mixout10 are better than
those reported by Devlin et al. (2018). We explored the behaviour of finetuning BERTLARGE with
mixout by using the learning rate of 2 × 10-5 while Devlin et al. (2018) obtained their results
by using the learning rate of {2 × 10-5 , 3 × 10-5 , 4 × 10-5, 5 × 10-5}. We thus present the
test scores obtained by the regularization strategy of Devlin et al. (2018) when the learning rate is
2 × 10-5 . The results in this section show that the best model on the dev set generalizes well, and
all the experiments based on dev scores in this paper are proper to validate the effectiveness of the
proposed mixout. For the remaining GLUE tasks such as SST-2 with a sufficient number of training
instances, we observed that using mixout does not differs from using dropout in Section 6.1. We
therefore omit the test results on the other tasks in GLUE.
D Verification of Corollary 1.1 with Least S quares Regression
Corollary 1.1 shows that mixout(u, p) regularizes learning to minimize the deviation from the tar-
get model parameter u, and the strength of regularization increases as p increases when the loss func-
tion is strongly convex. In order to validate this, we explore the behavior of least squares regression
with miχout(u, P) on a synthetic dataset. For randomly given wɪ and w2, We generated an obser-
vation y satisfying y = Wix + W + E where E is Gaussian noise. We set the model to y = wιx + w2.
That is, the model parameter w is given by (w1, w2). We randomly pick u as a target model param-
eter for mixout(u, p) and perform least squares regression with mixout(u, {0.0, 0.3, 0.6, 0.9}).
As shown in Figure 5, w converges to the target model parameter u rather than the true model
parameter wi = (w1i, w2i) as the mix probability p increases.
9https://gluebenchmark.com/leaderboard
10The regularization strategy in Section 6.2: using mixout(wpre , 0.7) for the pretrained layers and
mixout(w0 , 0.7) for the additional output where w0 is its randomly initialized weight parameter.
15
Published as a conference paper at ICLR 2020
X
(c) mixout(u, 0.6)
(d) mixout(u, 0.9)
Figure 5: Behavior of mixout(u, p) for a strongly convex loss function. We plot the line obtained
by least squares regression with mixout(u, {0.0, 0.3, 0.6, 0.9}) (each green line) on a synthetic
dataset (blue dots) generated by the true line (each blue dotted line). As p increases, the regression
line (each green line) converges to the target line generated by the target model parameter u (each
orange dotted line) rather than the true line (each blue dotted line).
E	Time Usage of Mixout C ompared to Dropout
We recorded the training time of the experiment in Section 6.3 to compare the time usage of mixout
and that of dropout. It took about 843 seconds to finetune BERTLARGE with mixout(wpre). On
the other hand, it took about 636 seconds to finetune BERTLARGE with dropout. mixout(wpre)
spends 32.5% more time than dropout since mixout(wpre) needs an additional computation with
the pretrained model parameter wpre . However, as shown in Figure 4, at least 15 finetuning runs
among 20 random restarts fail with the chance-level accuracy on RTE with dropout(p) for all p
while only 4 finetuning runs out of 20 random restarts are unusable with mixout(wpre, 0.8). From
this result, it is reasonable to finetune with the proposed mixout although this requires additional
time usage compared to dropout.
F Extensive Hyperparameter Search for Dropout
Devlin et al. (2018) finetuned BERTLARGE with dropout(0.1) on all GLUE (Wang et al., 2018)
tasks. They chose it to improve the maximum dev score on each downstream task, but we have
reported not only the maximum dev score but also the mean dev score to quantitatively compare
various regularization techniques in our paper. In this section, we explore the effect of the hyper-
parameter p when finetuning BERTLARGE with dropout(p) on RTE, MRPC, CoLA, and STS-B
to show dropout(0.1) is optimal in terms of mean dev score. All experimental setups for these
experiments are the same as Section 6.3.
16
Published as a conference paper at ICLR 2020
(a) RTE (Accuracy)
92.5
(b) MRPC (F1 accuracy)
Ooo
6 4 2
①」OUS >ΦQ
0.3	0.2	0.1	0.0
Drop probability
IOQ Q 。 O
W 8 6 4 2
aJ8s >ΦQ
0.5	0.4	0.3	0.2	0.1	0.0
Drop probability
(c)	CoLA (Mattew’s correlation)
(d)	STS-B (Spearman correlation)
0
Figure 6: Distribution of dev scores on each task from 20 random restarts when finetuning
BERTLARGE with dropout({0.0, 0.1, …,0.5}). Error intervals show mean±std. When We use
dropout(0.1), we have the highest average dev scores on MRPC and STS-B and the second-highest
average dev scores on RTE and CoLA. These results show that dropout(0.1) is almost optimal for
all tasks in terms of mean dev score.
As shown in Figure 6, we have the highest average dev score on MRPC with dropout(0.1) as well as
on STS-B. We obtain the highest average dev scores with dropout(0.0) on RTE and CoLA, but we
get the second-highest average dev scores with dropout(0.1) on them. These experiments confirm
that the drop probability 0.1 is almost optimal for the highest average dev score on each task.
17