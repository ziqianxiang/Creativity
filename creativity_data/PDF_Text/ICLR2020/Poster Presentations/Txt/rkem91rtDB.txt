Published as a conference paper at ICLR 2020
Inductive and Unsupervised Representation
Learning on Graph Structured Objects
Lichen Wang1, Bo Zong2, Qianqian Ma3, Wei Cheng2, Jingchao Ni2, Wenchao Yu2,
Yanchi Liu2, Dongjin Song2, Haifeng Chen2, and Yun Fu1
1Northeastern University, Boston, USA
2NEC Laboratories America, Princeton, USA
3Boston University, Boston, USA
wanglichenxj@gmail.com, maqq@bu.edu, yunfu@ece.neu.edu,
{bzong,weicheng,jni,wyu,yanchi,dsong,heifeng}@nec-labels.com
Ab stract
Inductive and unsupervised graph learning is a critical technique for predictive
or information retrieval tasks where label information is difficult to obtain. It is
also challenging to make graph learning inductive and unsupervised at the same
time, as learning processes guided by reconstruction error based loss functions
inevitably demand graph similarity evaluation that is usually computationally in-
tractable. In this paper, we propose a general framework SEED (Sampling, Encod-
ing, and Embedding Distributions) for inductive and unsupervised representation
learning on graph structured objects. Instead of directly dealing with the com-
putational challenges raised by graph similarity evaluation, given an input graph,
the SEED framework samples a number of subgraphs whose reconstruction errors
could be efficiently evaluated, encodes the subgraph samples into a collection of
subgraph vectors, and employs the embedding of the subgraph vector distribution
as the output vector representation for the input graph. By theoretical analysis,
we demonstrate the close connection between SEED and graph isomorphism. Us-
ing public benchmark datasets, our empirical study suggests the proposed SEED
framework is able to achieve up to 10% improvement, compared with competitive
baseline methods.
1	Introduction
Representation learning has been the core problem of machine learning tasks on graphs. Given a
graph structured object, the goal is to represent the input graph as a dense low-dimensional vec-
tor so that we are able to feed this vector into off-the-shelf machine learning or data manage-
ment techniques for a wide spectrum of downstream tasks, such as classification (Niepert et al.,
2016), anomaly detection (Akoglu et al., 2015), information retrieval (Li et al., 2019), and many
others (Santoro et al., 2017b; Nickel et al., 2015).
In this paper, our work focuses on learning graph representations in an inductive and unsupervised
manner. As inductive methods provide high efficiency and generalization for making inference
over unseen data, they are desired in critical applications. For example, we could train a model
that encodes graphs generated from computer program execution traces into vectors so that we can
perform malware detection in a vector space. During real-time inference, efficient encoding and the
capability of processing unseen programs are expected for practical usage. Meanwhile, for real-life
applications where labels are expensive or difficult to obtain, such as anomaly detection (Zong et al.,
2018) and information retrieval (Yan et al., 2005), unsupervised methods could provide effective
feature representations shared among different tasks.
Inductive and unsupervised graph learning is challenging, even compared with its transductive or
supervised counterparts. First, when inductive capability is required, it is inevitable to deal with the
problem of node alignment such that we can discover common patterns across graphs. Second, in
the case of unsupervised learning, we have limited options to design objectives that guide learning
processes. To evaluate the quality of the learned latent representations, reconstruction errors are
1
Published as a conference paper at ICLR 2020
Figure 1: SEED consists of three components: sampling, encoding, and embedding distribution.
Given an input graph, its vector representation can be obtained by going through the components.
commonly adopted. When node alignment meets reconstruction error, We have to answer a basic
question: Given two graphs Gi and G2, are they identical or isomorphic (Chartrand, 1977)? To this
end, it could be computationally intractable to compute reconstruction errors (e.g., using graph edit
distance (Zeng et al., 2009) as the metric) in order to capture detailed structural information.
Previous deep graph learning techniques mainly focus on transductive (Perozzi et al., 2014) or su-
pervised settings (Li et al., 2019). A few recent studies focus on autoencoding specific structures,
such as directed acyclic graphs (Zhang et al., 2019), trees or graphs that can be decomposed into
trees (Jin et al., 2018), and so on. From the perspective of graph generation, You et al. (2018) pro-
pose to generate graphs of similar graph statistics (e.g., degree distribution), and Bojchevski et al.
(2018) provide a GAN based method to generate graphs of similar random walks.
In this paper, we propose a general framework SEED (Sampling, Encoding, and Embedding Dis-
tributions) for inductive and unsupervised representation learning on graph structured objects. As
shown in Figure 1, SEED consists of three major components: subgraph sampling, subgraph encod-
ing, and embedding subgraph distributions. SEED takes arbitrary graphs as input, where nodes and
edges could have rich features, or have no features at all. By sequentially going through the three
components, SEED outputs a vector representation for an input graph. One can further feed such
vector representations to off-the-shelf machine learning or data management tools for downstream
learning or retrieval tasks.
Instead of directly addressing the computational challenge raised by evaluation of graph reconstruc-
tion errors, SEED decomposes the reconstruction problem into the following two sub-problems.
Q1: How to efficiently autoencode and compare structural data in an unsupervised fashion? SEED
focuses on a class of subgraphs whose encoding, decoding, and reconstruction errors can be eval-
uated in polynomial time. In particular, we propose random walks with earliest visiting time
(WEAVE) serving as the subgraph class, and utilize deep architectures to efficiently autoencode
WEAVEs. Note that reconstruction errors with respect to WEAVEs are evaluated in linear time.
Q2: How to measure the difference of two graphs in a tractable way? As one subgraph only covers
partial information ofan input graph, SEED samples a number of subgraphs to enhance information
coverage. With each subgraph encoded as a vector, an input graph is represented by a collection
of vectors. If two graphs are similar, their subgraph distribution will also be similar. Based on this
intuition, we evaluate graph similarity by computing distribution distance between two collections
of vectors. By embedding distribution of subgraph representations, SEED outputs a vector repre-
sentation for an input graph, where distance between two graphs’ vector representations reflects the
distance between their subgraph distributions.
Unlike existing message-passing based graph learning techniques whose expressive power is upper
bounded by Weisfeiler-Lehman graph kernels (Xu et al., 2019; Shervashidze et al., 2011), we show
the direct relationship between SEED and graph isomorphism in Section 3.5.
We empirically evaluate the effectiveness of the SEED framework via classification and clustering
tasks on public benchmark datasets. We observe that graph representations generated by SEED
are able to effectively capture structural information, and maintain stable performance even when
the node attributes are not available. Compared with competitive baseline methods, the proposed
SEED framework could achieve up to 10% improvement in prediction accuracy. In addition, SEED
2
Published as a conference paper at ICLR 2020
achieves high-quality representations when a reasonable number of small subgraph are sampled. By
adjusting sample size, we are able to make trade-off between effectiveness and efficiency.
2	Related work
Kernel methods. Similarity evaluation is one of the key operations in graph learning. Conventional
graph kernels rely on handcrafted substructures or graph statistics to build vector representations for
graphs (BorgWardt & Kriegel, 2005; Kashima et al., 2003; VishWanathan et al., 2010; Horvath et al.,
2004; Shervashidze & Borgwardt, 2009; Kriege et al., 2019). Although kernel methods are poten-
tially unsupervised and inductive, it is difficult to make them handle rich node and edge attributes in
many applications, because of the rigid definition of substructures.
Deep learning. Deep graph representation learning suggests a promising direction Where one can
learn unified vector representations for graphs by jointly considering both structural and attribute
information. While most of existing Works are either transductive (Perozzi et al., 2014; Kipf &
Welling, 2016; Liu et al., 2018) or supervised settings (Scarselli et al., 2008; Battaglia et al., 2016;
Defferrard et al., 2016; DUvenaUd et al., 2015; Kearnes et al., 2016; Velickovic et al., 2018; Santoro
et al., 2017a; Xu et al., 2018; Hamilton et al., 2017), a feW recent studies focus on autoencoding
specific strUctUres, sUch as directed acyclic graphs (Zhang et al., 2019), trees or graphs that can be
decomposed into trees (Jin et al., 2018), and so on. In the case of graph generation, YoU et al. (2018)
propose to generate graphs of similar graph statistics (e.g., degree distribUtion), and Bojchevski
et al. (2018) provide a method to generate graphs of similar random Walks. In addition, Li et al.
(2019) propose a sUpervised method to learn graph similarity, and XU et al. (2019) theoretically
analyses the expressive poWer of existing message-passing based graph neUral netWorks. Micali &
ZhU (2016) propose anonymoUs Walks for reconstrUction tasks. It reconstrUcts a Markov process
from the records collected by limited/partial observations. In an anonymoUs Walk procedUre, the
states are visited according to the Underlying transition probabilities, bUt no global state names are
knoWn. Ivanov & BUrnaev (2018) deploy anonymoUs Walks as a crUcial strategy for obtaining data-
driven and featUre-based graph representations. An efficient sampling approach is designed Which
approximates the distribUtions for large netWorks.
Unlike existing kernel or deep learning methods, oUr SEED frameWork is UnsUpervised With indUc-
tive capability, and natUrally sUpports complex attribUtes on nodes and edges. Moreover, it Works
for arbitrary graphs, and provides graph representations that simUltaneoUsly captUre both strUctUral
and attribUte information.
3	SEED: Sampling, Encoding, and Embedding Distributions
The core idea of SEED is to efficiently encode sUbgraphs as vectors so that We can Utilize sUbgraph
distribUtion distance to reflect graph similarity. We first give an abstract overvieW on the SEED
frameWork in Section 3.1, and then discUss concrete implementations for each component in Sec-
tion 3.2, 3.3, and 3.4, respectively. In Section 3.5, We share the theoretical insights in SEED. For
the ease of presentation, We focUs on Undirected graphs With rich node attribUtes in the folloWing
discUssion. With minor modification, oUr techniqUe can also handle directed graphs With rich node
and edge attribUtes.
3.1	Overview
SEED encodes an arbitrary graph into a vector by the folloWing three major components, as shoWn
in FigUre 1.
•	Sampling. A nUmber of sUbgraphs are sampled from an inpUt graph in this component. The
design goal of this component is to find a class of sUbgraphs that can be efficiently encoded and
decoded so that We are able to evalUate their reconstrUction errors in a tractable Way.
•	Encoding. Each sampled sUbgraph is encoded into a vector in this component. IntUitively, if
a sUbgraph vector representation has good qUality, We shoUld be able to reconstrUct the original
sUbgraph Well based on the vector representation. Therefore, the design goal of this component
is to find an aUtoencoding system that provides sUch encoding fUnctionality.
3
Published as a conference paper at ICLR 2020
•	Embedding distribution. A collection of subgraph vector representations are aggregated into
one vector serving	as	the	input graph,s representation.	For	two graphs,	their	distance	in	the
output vector space approximates their subgraph distribution distance. The design goal of this
component is to find such a aggregation function that preserves a pre-defined distribution distance.
Although there could be many possible implementations for the above three components, We propose
a competitive implementation in this paper, and discuss them in details in the rest of this section.
3.2	Sampling
Vanina	J b-a-b-a-a-b
random walk:	b-a-b-a-a-b
WEAVE:
Figure 2: Expressive power comparison between WEAVEs and vanilla random walks: while blue
and orange walks cannot be differentiated in terms of vanilla random walks, the difference under
WEAVEs is outstanding.
［初:］用用弋］｛］
卧卧卧卧卧B
In this paper, we propose to sample a class of subgraphs called WEAVE (random Walk with EArIieSt
Visit timE). Let G be an input graph of a node set V(G) and an edge set E(G). AWEAVE of length
k is sampled from G as follows.
•	Initialization. A starting node v(0) is randomly drawn from V(G) at timestamp 0, and its earliest
visiting time is set to 0.
•	Next-hop selection. Without loss of generality, assume v (p) is the node visited at timestamp p
(0 ≤ p < k). We randomly draw a node v(p+1) from v(p)’s one-hop neighborhood as the node
to be visited at timestamp p + 1. If v(p+1) is a node that we have not visited before, its earliest
visiting time is set to p + 1; otherwise, its earliest visiting is unchanged. We hop to v(p+1).
•	Termination. The sampling process ends when timestamp reaches k.
In practical computation, a WEAVE is denoted as a matrix X = [x(0), x(1),…，x(k)]. In particular,
x(p) = [x(ap), x(tp)] is a concatenation of two vectors, where x(ap) includes attribute information
for the node visited at timestamp p, and xt(p) contains its earliest visit time. As earliest visit time is
discrete, we use one-hot scheme to represent such information, where xt(p) is a k-dimensional vector
and xt(p) [q] = 1 means the earliest visit time is timestamp q. If one aims to sample s WEAVEs from
an input graph, the output of this component is a set of s matrices {X1, X2, ..., Xs}.
Difference between WEAVEs and vanilla random walks. The key distinction comes from the
information of the earliest visit time. Vanilla random walks include coarser-granularity structural
information, such as neighborhood density and neighborhood attribute distribution (Perozzi et al.,
2014). As vanilla random walks have no memory on visit history, detailed structural information
related to loops or circles is ignored. While it is also efficient to encode and decode vanilla ran-
dom walk, it is difficult to evaluate finer-granularity structural difference between graphs. Unlike
vanilla random walks, WEAVEs utilize earliest visit time to preserve loop information in sampled
subgraphs. As shown in Figure 2, while we cannot tell the difference between walk w1 and walk
w2 using vanilla random walk, the distinction is outstanding under WEAVEs. Note that it is equally
efficient to encode and decode WEAVEs, compared with vanilla random walks.
In addition, WEAVE is also related to anonymous random walks (Ivanov & Burnaev, 2018; Micali
& Zhu, 2016). By excluding attribution information, a WEAVE is reduced to an anonymous random
walk.
4
Published as a conference paper at ICLR 2020
3.3	Encoding
Given a set of sampled WEAVEs of length k {X1, X2, ..., Xs}, the goal is to encode each sam-
pled WEAVE into a dense low-dimensional vector. As sampled WEAVEs share same length, their
matrix representations also have identical shapes. Given a WEAVE X , one could encode it by an
autoencoder (Hinton & Salakhutdinov, 2006) as follows.
Z = f(X； θe),	X = g(z; θd),	(1)
where Z is the dense low-dimensional representation for the input WEAVE, f (∙) is the encoding
function implemented by an MLP with parameters θe, and g(∙) is the decoding function implemented
by another MLP with parameters θd . The quality of z is evaluated through reconstruction errors as
follows,
L = kx - X k2.	⑵
By conventional gradient descent based backpropagation (Kingma & Ba, 2014), one could optimize
θe and θd via minimizing reconstruction error L. After such an autoencoder is well trained, the latent
representation z includes both node attribute information and finer-granularity structural information
simultaneously. Given s sampled WEAVEs of an input graph, the output of this component is s dense
low-dimensional vectors {zι, z2,…，z§}.
3.4	Embedding distribution
Let G and H be two arbitrary graphs. Suppose subgraph (e.g., WEAVE) distributions for G and
H are PG and PH , respectively. In this component, we are interested in evaluating the distance
between PG and PH . In this work, we investigate the feasibility of employing empirical estimate
of the maximum mean discrepancy (MMD) (Gretton et al., 2012) to evaluate subgraph distribution
distances, without assumptions on prior distributions, while there are multiple candidate metrics for
distribution distance evaluation, such as KL-divergence (Kullback & Leibler, 1951) and Wasserstein
distance (Arjovsky et al., 2017). We leave the detailed comparison among different choices of
distance metrics in our future work.
Given S subgraphs sampled from G as {zι,…，z§} and S subgraphs sampled from H as
{hi,…，hs}, we can estimate the distance between PG and PH under the MMD framework:
ss	ss
MMMD(Pg ,Ph) =	-1)XXk(zi, zj) + S(S 彳 XX k(hi, hj)
i=1 j 6=i	i=1 j6=i
ss
— ∑2 XX k(Zi, hj)
S i=1 j=1
= kμG - μHk2∙	⑶
μG and μ∏ are empirical kernel embeddings of PG and PH, respectively, and are defined as follows,
1s
μG = - >2φ(Zi),
S
i=1
1s
μH =-y^φ(hi),
S i=1
(4)
where φ(∙) is the implicit feature mapping function with respect to the kernel function k(∙, ∙). To
this end, μ^ and μ∏ are the output vector representation for G and H, respectively.
In terms of kernel selection, we find the following options are effective in practice.
Identity kernel. Under this kernel, pairwise similarity evaluation is performed in the original input
space. Its implementation is simple, but surprisingly effective in real-life datasets,
1s	1s
μG = 一 Tzi,	μH = 一 ɪ^hi.	⑸
SS
i=1	i=1
where output representations are obtained by average aggregation over subgraph representations.
Commonly adopted kernels. For popular kernels (e.g., RBF kernel, inverse multi-quadratics ker-
nel, and so on), it could be difficult to find and adopt their feature mapping functions. While approx-
imation methods could be developed for individual kernels (Ring & Eskofier, 2016), we could train
5
Published as a conference paper at ICLR 2020
a deep neural network that approximates such feature mapping functions. In particular,
1s
μg = — £B(Zi； θm),
s i=1
1s
Mh = — £B(hi； θm),	D (PG ,PH) = k^G - μHl2,⑹
s i=1
where φ(∙; θm) is an MLP with parameters θm, and D(∙, ∙) is the approximation to the empirical
estimate of MMD. Note that μG and μH are output representations for G and H, respectively. To
train the function φ(∙; θm), we evaluate the approximation error by
J(θm)	= kD(PG,PH)	- M\MD(PG, PH)k22,	(7)
where θm	is optimized by minimizing J(θm).
3.5 Theoretical insights
In this section, we sketch the theoretical connection between SEED and well-known graph isomor-
phism (Chartrand, 1977), and show how walk length in WEAVE impacts the effectiveness in graph
isomorphism tests. The full proof of theorems and lemmas is detailed in Appendix.
To make the discussion self-contained, we define graph isomorphism and its variant with node at-
tributes as follows.
Graph isomorphism. G = (V (G), E(G))	and H = (V (H), E(H)))	are isomorphic if there is a
bijection function f :	V	(G)	⇔	V (H) such that ∀(u, v) ∈ E(G)	⇔ (f (u), f(v))	∈	E(H).
Graph isomorphism with node attributes. Let G = (V (G), E(G), l1), H = (V (H), E(H), l2) be
two attributed graphs, where l1, l2 are attribute mapping functions l1 : V (G) → Rd, l2 : V (H) →
Rd, and node attributes are denoted as d-dimensional vectors. Then G and H are isomorphic with
node attributes if there is a bijection f :	V (G)	⇔	V (H), s.t.,	∀(u,	v) ∈	E(G)	⇔	(f (u), f (v))	∈
E(H), and ∀u ∈ V(G),l1(u) = l2(f(u)).
Identical distributions. Two distributions P and Q are identical if and only if their 1st order
Wasserstein distance (RUSchendorf, 1985) Wι(Ρ, Q) = 0.
The following theory suggests the minimum walk length for WEAVEs, if every edge in a graph is
expected to be visited.
Lemma 1. Let G = (V (G), E(G)) be a connected graph, then there exists a walk of length k which
can visit all the edges of G, where k ≥ 2|E(G)| - 1.
Now, we are ready to present the connection between SEED and graph isomorphism.
Theorem 1. Let G = (V (G), E(G)) and H = (V (H), E(H)) be two connected graphs.
Suppose we can enumerate all possible WEAVEs from G and H with a fixed-length k ≥
2 max{|E (G)|, |E(H)|} - 1, where each WEAVE has a unique vector representation generated from
a well-trained autoencoder. The Wasserstein distance between G’s and H’s WEAVE distributions is
0 if and only if G and H are isomorphic.
The following theory shows the connection in the case of graphs with nodes attributes.
Theorem 2. Let G = (V (G), E(G)) and H = (V (H), E (H)) be two connected graphs with node
attributes. Suppose we can enumerate all possible WEAVEs on G and H with a fixed-length k ≥
2 max{|E (G)|, |E(H)|} - 1, where each WEAVE has a unique vector representation generated from
a well-trained autoencoder. The Wasserstein distance between G’s and H’s WEAVE distributions is
0 if and only if G and H are isomorphic with node attributes.
Note that similar results can be easily extended to the cases with both node and edge attributes, the
corresponding details can be found in Appendix E.
The theoretical results suggest the potential power of the SEED framework in capturing structural
difference of graph data. As shown above, in order to achieve the same expressive power of graph
isomorphism, we need to sample a large number of WEAVEs with a long walk length so that all pos-
sible WEAVEs can be enumerated. The resource demand is impractical. However, in the empirical
study in Section 4, we show that SEED can achieve state-of-the-art performance, when we sample a
small number of WEAVEs with a reasonably short walk length.
6
Published as a conference paper at ICLR 2020
4 Experiments
4.1	Datasets
We employ seven public benchmark datasets to evaluate the effectiveness of SEED. The brief intro-
ductions of the datasets are listed below.
•	Deezer User-User Friendship Networks (Deezer) (Rozemberczki et al., 2018) is a social net-
work dataset which is collected from the music streaming service Deezer. It represents friendship
network of users from three European countries (i.e., Romania, Croatia and Hungary). There
are three graphs which corresponds to the three countries. Nodes represent the users and edges
are the mutual friendships. For the three graphs, the numbers of nodes are 41, 773, 54, 573, and
47, 538, respectively, and the number of edges are 125, 826, 498, 202, and 222, 887, respectively.
There exist 84 distinct genres, and genre notations are considered as node features. Thus, node
features are represented as a 84-dimensional multi-hot vectors.
•	Mutagenic Aromatic and Heteroaromatic Nitro Compounds (MUTAG) (Debnath et al., 1991)
is a chemical bioinformatics dataset, which contains 188 chemical compounds. The compounds
can be divided into two classes according to their mutagenic effect on a bacterium. The chemical
data can be converted to graph structures, where each node represents an atom. Explicit hydrogen
atoms have been removed. In the obtained graph, the node attributes represent the atom types
(i.e., C, N, O, F, I, Cl and Br) while the edge attributes represent bond types (i.e., single, double,
triple or aromatic).
•	NCI1 (Wale et al., 2008) represents a balanced subsets of datasets of chemical compounds
screened for activity against non-small cell lung cancer and ovarian cancer cell lines, respec-
tively. The label is assigned based on this characteristic. Each compound is converted to a graph.
There are 4, 110 graphs in total with 122, 747 edges.
•	PROTEINS (Borgwardt et al., 2005) is a bioinformatics dataset. The proteins in the dataset
are converted to graphs based on the sub-structures and physical connections of the proteins.
Specifically, nodes are secondary structure elements (SSEs), and edges represent the amino-acid
sequence between the two neighbors. PROTEINS has 3 discrete labels (i.e., helix, sheet, and
turn). There are 1, 113 graphs in total with 43, 471 edges.
•	COLLAB (Leskovec et al., 2005) is a scientific collaboration dataset. It belongs to a social
connection network in general. COLLAB is collected from 3 public collaboration datasets (i.e.,
Astro Physics, Condensed Matter Physics, and High Energy Physics). The ego-networks are
generated for individual researchers. The label of each graph represents the field which this
researcher belongs to. There are 5, 000 graphs with 24, 574, 995 edges.
•	IMDB-BINARY (Yanardag & Vishwanathan, 2015) is a collaboration dataset of film industry.
The ego-network of each actor/actress is converted to a graph data. Each node represents an
actor/actress and each edge is the indication if two actors/actresses if they appear in the same
movie. IMDB-BINARY has 1, 000 graphs associated with 19, 773 edges in total.
•	IMDB-MULTI extends the IMDB-BINARY dataset to a multi-class version. It contains a bal-
anced set of ego-networks derived from Sci-Fi, Romance, and Comedy genres. Specifically, there
are 1, 500 graphs with 19, 502 edges in total.
4.2	Baselines
Three state-of-the-art representative techniques are implemented as baselines in the experiments.
•	Graph Sample and Aggregate (GraphSAGE) (Hamilton et al., 2017) is an inductive graph rep-
resentation learning approach in either supervised or unsupervised manner. GraphSAGE explores
node and structure information by sampling and aggregating features from the local neighborhood
of each node. A forward propagation algorithm is specifically designed to aggregates the infor-
mation together. We evaluate GraphSAGE in its unsupervised setting.
•	Graph Matching Network (GMN) (Li et al., 2019) utilizes graph neural networks to obtain
graph representations for graph matching applications. A novel Graph Embedding Network is
designed for better preserving node features and graph structures. In particular, Graph Matching
7
Published as a conference paper at ICLR 2020
Network is proposed to directly obtain the similarity score of each pair of graphs. In our imple-
mentation, we utilize the Graph Embedding Networks and deploy the graph-based loss function
proposed in (Hamilton et al., 2017) for unsupervised learning fashion.
•	Graph Isomorphism Network (GIN) (Xu et al., 2019) provides a simple yet effective neural
network architecture for graph representation learning. It deploys the sum aggregator to achieve
more comprehensive representations. The original GIN is a supervised learning method. Thus,
we follow the GraphSAGE approach, and modify its objective to fit an unsupervised setting.
Setting		Methods	SAGE	GIN	GMN	SEED	SAGE	GIN	GMN	SEED
	Datasets	Metric	Node Feature Excluded				Node Feature Included			
		ACC	0.3853	0.4913	0.4924	0.4927	0.3840	0.4930	0.4808	0.4810
	Dezzer	NMI	0.0079	0.0958	0.0726	0.1277	0.0003	0.0893	0.0651	0.0566
	Λ∕ττ TTAC	ACC	0.6649	0.4997	0.4990	0.8014	0.6649	0.4963	0.4910	0.7260
	MUTAG	NMI	0.0150	0.0946	0.0825	0.3214	0.0070	0.0933	0.0917	0.1567
	NTCT 1	ACC	0.5098	0.5221	0.5022	0.5510	0.5070	0.5204	0.5005	0.5441
	NCI1	NMI	0.0003	0.0015	0.0034	0.0073	0.0002	0.0013	0.0042	0.0089
Clustering	DVMTΠTNTC1	ACC	0.5657	0.5957	0.5966	0.5957	0.5657	0.5957	0.5957	0.5957
	PROTEINS	NMI	0.0013	0.0038	0.0117	0.0518	0.0004	0.0034	0.0067	0.0689
	f'/ ∖τ τ ʌ ŋ	ACC	0.5208	0.5458	0.5173	0.5973	-	-	-	-
	COLLAB	NMI	0.0025	0.0729	0.0193	0.2108	-	-	-	-
		ACC	0.5069	0.6202	0.5010	0.5776	-	-	-	-
	IMDB-BINARY	NMI	0.0002	0.0459	0.0093	0.0241	-	-	-	-
	TΛΛTΛT3 ʌæt TT tj	ACC	0.3550	3607	0.3348	0.3816	-	-	-	-
	IMDB-MULTI	NMI	0.0019	0.0185	0.0112	0.0214	-	-	-	-
	Dezzer	ACC	0.3775	0.5094	0.5427	0.6327	0.3754	0.5270	0.5627	0.7451
	MUTAG	ACC	0.6778	0.6778	0.6889	0.8112	0.6889	0.6778	0.6889	0.8222
	NCI1	ACC	0.5410	0.5571	0.5123	0.6105	0.5328	0.5231	0.5133	0.6151
Classification	PROTEINS	ACC	0.6846	0.7387	0.6216	0.7207	0.7027	0.7207	0.6357	0.7462
	COLLAB	ACC	0.5650	0.6170	0.5460	0.6720	-	-	-	-
	IMDB-BINARY	ACC	0.5400	0.7310	0.5140	0.7660	-	-	-	-
	IMDB-MULTI	ACC	0.3866	0.3843	0.3478	0.4466	-	-	-	-
Table 1: Evaluating graph representation quality by classification and clustering tasks
Two downstream tasks, classification and clustering, are deployed to evaluate the quality of the
learned graph representations.
For classification task, a simple multi-layer fully connected neural network is built as a classifier. We
report the average accuracy (ACC) for classification performance. For clustering task, an effective
conventional clustering approach, Normalized Cuts (NCut) (Jianbo Shi & Malik, 2000), is used to
cluster graph representations. We consider two widely used metrics for clustering performance,
including Accuracy (ACC) and Normalized Mutual Information (NMI) (Wu et al., 2009). ACC
comes from classification with the best mapping, and NMI evaluates the mutual information across
the ground truth and the recovered cluster labels based on a normalization operation. Both ACC and
NMI are positive measurements (i.e., the higher the metric is, the better the performance will be).
4.3	Performance Analysis
In this section, we discuss the performance of SEED and its baselines in the downstream tasks. The
performance with and without the node features are reported. In this set of experiments, SEED
adopts identity kernel in the component of embedding distributions.
As shown in Table 1, SEED consistently outperforms the baseline methods in both classification
and clustering tasks. For GIN and GMN, supervision information could be crucial in order to dif-
ferentiate structural variations. As GraphSAGE mainly focuses on aggregating feature information
from neighbor nodes, it could be difficult for GraphSAGE to extract effective structural information
from an unsupervised manner. In the unsupervised setting, SEED is able to differentiate structural
difference at finer granularity and capture rich attribute information, leading to high-quality graph
representations with superior performance in downstream tasks. Interestingly, for NCI and PRO-
TEINS datasets, we see node features bring little improvement in the unsupervised setting. One
8
Published as a conference paper at ICLR 2020
possible reason could be node feature information has high correlation with structural information
in these cases.
Sampling Number	Classification Accuracy	Clustering					
		ACC	NMI	Walk	Classification	Clustering	
25	0.6832	0.6649	0.0031	Length	Accuracy	ACC	NMI
50	0.6778	0.6649	0.0005	5	0.7278	0.6649	0.0534
100	0.7778	0.6649	0.0537	10	0.7778	0.7633	0.2100
150	0.7889	0.6968	0.1081	15	0.8167	0.7723	0.2495
200	0.7778	0.7633	0.2100	20	0.8778	0.8245	0.3351
300	0.7833	0.7502	0.1995	25	0.8722	0.8218	0.3380
400	0.8389	0.7628	0.1928	30	0.8743	0.8285	0.3321
800	0.8111	0.7660	0.1940				
Table 3: Representation quality with differ-
Table 2: Representation quality with differ-	ent walk lengths
ent sampling numbers
10
5
0
-5
-10
-10	-5	0	5
(a) Samples = 50
10
5
0
-5
-10
-10	0	10
(b) Samples = 100
(c) Samples = 150
Figure 3: t-SNE VisUalziation of the MUTAG representations with different sampling numbers
(a) Walk length = 5	(b) Walk length = 10	(C) Walk length = 15	(d) Walk length = 20	(e) Walk length = 25
Figure 4: t-SNE Visualziation of MUTAG representations with different walk lengths
-10	0	10
10
0
-10
4.4	Ablation S tudy
Walk length and sample numbers are two meta-parameters in the SEED framework. By adjusting
these two meta-parameters, we can make trade-off between effectiVeness and computational effi-
ciency. In the experiment, we empirically eValuate the impact of the two meta-parameters on the
MUTAG dataset. In Table 2, each row denotes the performance with different sampling numbers
(from 25 to 800) while the walk length is fixed to 10. MoreoVer, we adjust the walk length from 5
to 25 while sampling number is fixed to 200 in Table 3. We can see that the performance of SEED
in both classification and clustering tasks increases as there are more subgraphs sampled, especially
for the changes from 25 to 200. Meanwhile, we obserVe the increasing rates diminish dramatically
when sampling number ranges from 200 to 800. Similarly, the performance of SEED increase as
the walk length grows from 5 to 20, and the performance starts to conVerge when the length goes
beyond 20.
More empirical results are proVided in Appendix. In particular, we demonstrate the effectiVeness
of using DeepSets (Zaheer et al., 2017) for embedding distribution in Appendix F. In Appendix G,
we consider a pure feature based random walk baseline with earliest Visit time remoVed, and eVal-
uate its performance. Moreover, We present how to leverage Nystrom approximation in embedding
distribution, and its performance is reported in Appendix H.
9
Published as a conference paper at ICLR 2020
10
5
0
-5
-10
-10 -5	0	5	10
(a) Identity Kernel
(b) RBF Kernel
Figure 5: t-SNE visualization of the learned representations from different kernels on MUTAG
5 Embedding Distribution
We employ t-SNE (Maaten & Hinton, 2008) to visualize learned graph representations in Figure 3
and Figure 4. Red and blue colors indicate two labels. We observe that the boundary becomes
clearer when sample number or walk length increases.
Embedding	Classification ACC	Clustering ACC	Clustering NMi
identity kernel	0.8112	0.8014	0.3214
RBF kernel	0.7958	0.7984	0.3115
Table 4: Graph representation quality comparison between identity and RBF kernel on MUTAG
identity kernels or commonly adopted kernels could be deployed in the component of embedding
subgraph distributions. in our implementation, we utilize a multi-layer deep neural network to
approximate a feature mapping function, for kernels whose feature mapping function is difficult to
obtain. Figure 5 shows the t-SNE visualization of learned graph representations based on identity
kernel and RBF kernel. As shown in Table 4, SEED variants with different kernels for distribution
embedding could distinguish different classes with similar performance on the MUTAG dataset.
6 Conclusion
in this paper, we propose a novel framework SEED (Sampling, Encoding, and Embedding distri-
bution) framework for unsupervised and inductive graph learning. instead of directly dealing with
the computational challenges raised by graph similarity evaluation, given an input graph, the SEED
framework samples a number of subgraphs whose reconstruction errors could be efficiently evalu-
ated, encodes the subgraph samples into a collection of subgraph vectors, and employs the embed-
ding of the subgraph vector distribution as the output vector representation for the input graph. By
theoretical analysis, we demonstrate the close connection between SEED and graph isomorphism.
Our experimental results suggest the SEED framework is effective, and achieves state-of-the-art
predictive performance on public benchmark datasets.
10
Published as a conference paper at ICLR 2020
References
Leman Akoglu, Hanghang Tong, and Danai Koutra. Graph based anomaly detection and description:
a survey. Data mining and knowledge discovery, 29(3):626-688, 2015.
Martin Arjovsky, SoUmith Chintala, and Leon Bottou. Wasserstein gan. arXiv preprint
arXiv:1701.07875, 2017.
Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, et al. Interaction net-
works for learning about objects, relations and physics. In Proceedings of Advances in neural
information processing systems, pp. 4502-4510, 2016.
Aleksandar Bojchevski, Oleksandr Shchur, Daniel ZUgner, and Stephan GUnnemann. Netgan: Gen-
erating graphs via random walks. In Proceedings of International Conference on Machine Learn-
ing, 2018.
Karsten M Borgwardt and Hans-Peter Kriegel. Shortest-path kernels on graphs. In Proceedings of
IEEE International conference on data mining, pp. 8-pp, 2005.
Karsten M Borgwardt, Cheng Soon Ong, Stefan Schonauer, SVN Vishwanathan, Alex J Smola, and
Hans-Peter Kriegel. Protein function prediction via graph kernels. Bioinformatics, 21(SUPPL1):
i47-i56, 2005.
Gary Chartrand. Introductory graph theory. Courier Corporation, 1977.
Asim Kumar Debnath, Rosa L Lopez de Compadre, Gargi Debnath, Alan J Shusterman, and Cor-
win Hansch. Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro com-
pounds. correlation with molecular orbital energies and hydrophobicity. Journal of medicinal
chemistry, 34(2):786-797, 1991.
Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on
graphs with fast localized spectral filtering. In Proceedings of Advances in neural information
processing systems, pp. 3844-3852, 2016.
David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alan
Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning molecular
fingerprints. In Proceedings of Advances in neural information processing systems, pp. 2224-
2232, 2015.
Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Scholkopf, and Alexander Smola.
A kernel two-sample test. Journal of Machine Learning Research, 13(Mar):723-773, 2012.
Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs.
In Proceedings of Advances in Neural Information Processing Systems, pp. 1024-1034, 2017.
Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural
networks. science, 313(5786):504-507, 2006.
TamaS Horvath, Thomas Gartner, and Stefan Wrobel. Cyclic pattern kernels for predictive graph
mining. In Proceedings ofACM SIGKDD international conference on Knowledge discovery and
data mining, pp. 158-167, 2004.
Sergey Ivanov and Evgeny Burnaev. Anonymous walk embeddings. In Jennifer Dy and Andreas
Krause (eds.), Proceedings of International Conference on Machine Learning, volume 80 of Pro-
Ceedings of Machine Learning Research, pp. 2186-2195, Stockholmsmassan, Stockholm Swe-
den, 10-15 Jul 2018. PMLR.
Jianbo Shi and J. Malik. Normalized cuts and image segmentation. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 22(8):888-905, Aug 2000.
Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Junction tree variational autoencoder for
molecular graph generation. In Proceedings of International Conference on Machine Learning,
2018.
11
Published as a conference paper at ICLR 2020
Hisashi Kashima, Koji Tsuda, and Akihiro Inokuchi. Marginalized kernels between labeled graphs.
In Proceedings Of International conference on machine learning (ICML-03), pp. 321-328, 2003.
Steven Kearnes, Kevin McCloskey, Marc Berndl, Vijay Pande, and Patrick Riley. Molecular graph
convolutions: moving beyond fingerprints. Journal of computer-aided molecular design, 30(8):
595-608, 2016.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Thomas N Kipf and Max Welling. Variational graph auto-encoders. arXiv preprint
arXiv:1611.07308, 2016.
Nils M. Kriege, Fredrik D. Johansson, and Christopher Morris. A survey on graph kernels. CoRR,
abs/1903.11835, 2019. URL http://arxiv.org/abs/1903.11835.
Solomon Kullback and Richard A Leibler. On information and sufficiency. The annals of mathe-
matical statistics, 22(1):79-86, 1951.
Jure Leskovec, Jon Kleinberg, and Christos Faloutsos. Graphs over time: densification laws, shrink-
ing diameters and possible explanations. In Proceedings of the eleventh ACM SIGKDD interna-
tional conference on Knowledge discovery in data mining, pp. 177-187. ACM, 2005.
Yujia Li, Chenjie Gu, Thomas Dullien, Oriol Vinyals, and Pushmeet Kohli. Graph matching net-
works for learning the similarity of graph structured objects. In Kamalika Chaudhuri and Ruslan
Salakhutdinov (eds.), Proceedings of International Conference on Machine Learning, volume 97,
pp. 3835-3845, 09-15 Jun 2019.
Qi Liu, Miltiadis Allamanis, Marc Brockschmidt, and Alexander Gaunt. Constrained graph vari-
ational autoencoders for molecule design. In Proceedings of Advances in Neural Information
Processing Systems, pp. 7795-7804, 2018.
Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of machine
learning research, 9(Nov):2579-2605, 2008.
Silvio Micali and Zeyuan Allen Zhu. Reconstructing markov processes from independent and
anonymous experiments. Discrete Applied Mathematics, 200:108-122, 2016.
Maximilian Nickel, Kevin Murphy, Volker Tresp, and Evgeniy Gabrilovich. A review of relational
machine learning for knowledge graphs. Proceedings of IEEE, 104(1):11-33, 2015.
Mathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov. Learning convolutional neural net-
works for graphs. In Proceedings of International Conference on Machine Learning, pp. 2014-
2023, 2016.
Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social representa-
tions. In Proceedings of ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining, pp. 701-710. ACM, 2014.
Matthias Ring and Bjoern M Eskofier. An approximation of the gaussian rbf kernel for efficient
classification with svms. Pattern Recognition Letters, 84:107-113, 2016.
Benedek Rozemberczki, Ryan Davies, Rik Sarkar, and Charles Sutton. Gemsec: Graph embedding
with self clustering. arXiv preprint arXiv:1802.03997, 2018.
LUdger RUschendorf. The Wasserstein distance and approximation theorems. Probability Theory
and Related Fields, 70(1):117-129, 1985.
Adam Santoro, David Raposo, David G Barrett, MateUsz MalinoWski, Razvan PascanU, Peter
Battaglia, and Timothy Lillicrap. A simple neUral netWork modUle for relational reasoning. In
Proceedings of Advances in neural information processing systems, pp. 4967-4976, 2017a.
Adam Santoro, David Raposo, David G Barrett, MateUsz MalinoWski, Razvan PascanU, Peter
Battaglia, and Timothy Lillicrap. A simple neUral netWork modUle for relational reasoning. In
Proceedings of Advances in neural information processing systems, pp. 4967-4976, 2017b.
12
Published as a conference paper at ICLR 2020
Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini.
The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61-80, 2008.
Nino Shervashidze and Karsten Borgwardt. Fast subtree kernels on graphs. In Proceedings of
Advances in neural information processing systems, pp. 1660-1668, 2009.
Nino Shervashidze, Pascal Schweitzer, Erik Jan van Leeuwen, Kurt Mehlhorn, and Karsten M Borg-
wardt. Weisfeiler-lehman graph kernels. Journal of Machine Learning Research, 12(Sep):2539-
2561, 2011.
Petar VeliCkovic, Guillem CUcurulL Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. In Proceedings of International Conference on Learning
Representations, 2018.
S Vichy N Vishwanathan, Nicol N Schraudolph, Risi Kondor, and Karsten M Borgwardt. Graph
kernels. Journal of Machine Learning Research, 11(Apr):1201-1242, 2010.
Nikil Wale, Ian A Watson, and George Karypis. Comparison of descriptor spaces for chemical com-
pound retrieval and classification. Knowledge and Information Systems, 14(3):347-375, 2008.
Christopher KI Williams and Matthias Seeger. Using the nystrom method to speed up kernel ma-
chines. In Advances in neural information processing systems, pp. 682-688, 2001.
Junjie Wu, Hui Xiong, and Jian Chen. Adapting the right measures for k-means clustering. In
Proceedings of the ACM SIGKDD international conference on Knowledge discovery and data
mining, pp. 877-886, 2009.
Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken ichi Kawarabayashi, and Stefanie
Jegelka. Representation learning on graphs with jumping knowledge networks. In Proceedings
of International Conference on Machine Learning, 2018.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? In Proceedings of International Conference on Learning Representations, 2019.
Xifeng Yan, Philip S Yu, and Jiawei Han. Substructure similarity search in graph databases. In
Proceedings of ACM SIGMOD international conference on Management of data, pp. 766-777.
ACM, 2005.
Pinar Yanardag and SVN Vishwanathan. Deep graph kernels. In Proceedings of the ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining, pp. 1365-1374. ACM,
2015.
Jiaxuan You, Rex Ying, Xiang Ren, William L Hamilton, and Jure Leskovec. Graphrnn: Generating
realistic graphs with deep auto-regressive models. In Proceedings of International Conference on
Machine Learning, 2018.
Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan R Salakhutdinov,
and Alexander J Smola. Deep sets. In Advances in neural information processing systems, pp.
3391-3401, 2017.
Zhiping Zeng, Anthony KH Tung, Jianyong Wang, Jianhua Feng, and Lizhu Zhou. Comparing stars:
On approximating graph edit distance. Proceedings of VLDB Endowment, 2(1):25-36, 2009.
Muhan Zhang, Shali Jiang, Zhicheng Cui, Roman Garnett, and Yixin Chen. D-vae: A variational
autoencoder for directed acyclic graphs. arXiv preprint arXiv:1904.11088, 2019.
Bo Zong, Qi Song, Martin Renqiang Min, Wei Cheng, Cristian Lumezanu, Daeki Cho, and Haifeng
Chen. Deep autoencoding gaussian mixture model for unsupervised anomaly detection. In Pro-
ceedings of International Conference on Learning Representations, 2018.
13
Published as a conference paper at ICLR 2020
A	Proof for Lemma 1
Proof. We will use induction on |E(G)| to complete the proof.
Basic case: Let |E(G)| = 1, the only possible graph is a line graph of length 1. For such a graph, the
walk from one node to another can cover the only edge on the graph, which has length 1 = 2 ∙ 1 - 1.
Induction: We assume for all the connected graphs on less than m edges (i.e., |E(G)| ≤ m - 1),
there exist a walk of length k which can visit all the edges if k ≥ 2|E(G)| - 1. Then we will show
for any connected graph with m edges, there also exists a walk which can cover all the edges on the
graph with length k ≥ 2|E(G)| - 1.
Let G = (V (G), E(G)) be a connected graph with |E(G)| = m. Firstly, we assume G is not a tree,
which means there exist a cycle on G. By removing an edge e = (vi , vj ) from the cycle, we can
get a graph G0 on m - 1 edges which is still connected. This is because any edge on a cycle is not
bridge. Then according to the induction hypothesis, there exists a walk w0 = v1v2 . . . vi . . . vj . . . vt
of length k0 ≥ 2(m - 1) + 1 which can visit all the edges on G0 (The walk does not necessarily start
from node 1, v1 just represents the first node appears in this walk). Next, we will go back to our
graph G, as G0 is a subgraph of G, w0 is also a walk on G . By replacing the first appeared node vi on
walk w0 with a walk vivjvi, we can obtain a new walk w = v1v2 . . . vivjvi . . . vj . . . vt on G. As w
can cover all the edges on G0 and the edge e with length k = k0 + 2 ≥ 2(m - 1) - 1 + 2 = 2m - 1,
which means it can cover all the edges on G with length k ≥ 2|E(G)| - 1.
Next, consider graph G which is a tree. In this case, we can remove a leaf vj and its incident edge
e = (vi, vj) from G, then we can also obtain a connected graph G0 with |E(G0)| = m - 1. Similarly,
according to the induction hypothesis, We can find a walk w0 = v1v2 ...v% ...vt on G0 which can
visit all the τn — 1 edges of Q, of length ⅛z, where k! ≥ 2(Tn —1) — 1. As Q, is a subgraph of Q, any
walk on Q, is also a walk on Q including walk w!. Then we can also extend walk w! on Q by replacing
the first appeared Vi with a wal k Vivj vi, which produce a new walk W = v1v2 ...viv Vi ...vt. W can
visit all the edges of G0 as well as the edge e with length k = k0 + 2 ≥ 2(m - 1) - 1 + 2 = 2m - 1.
In other words, W can visit all the edges on G with length k ≥ 2|E(G) | - 1. Now, we have verified
our assumption works for all the connected graphs with m edges, hence we complete our proof. (To
give an intuition for our proof of lemma 1, we provide an example of 5 edges in Figure 6)
5
(a1) Graph Q which is not a tree (a2) Graph Q, corresponds to Q in (a1)	(b1) Graph Q which is a tree (b2) Graph Q, corresponds to Q in (b1)
Figure 6: Different types of graphs with random walk W which can visit all the edges.
Figure 6 (a1) illustrates an example graph G which is a connected graph on 5 edges but not a tree.
By removing an edge (v2,v5) from the cycle, we can get a connected graph G0 (Figure 6 (a2)) with
4 edges. G0 has a walk w0 = V1V2V3V4V5 which covers all the edges of G0, as w0 is also a walk on
G, by replacing v5 with walk v5v2v5 in w0, we can get W = v1v2v3v4v5v2v5 which can visit all the
edges of G. Figure 6 (b1) shows an example graph G which is a tree on 5 edges. By removing the
leaf v4 and its incident edge (v4, v3), we can get a tree G0 with 4 edges (Figure 6 (b2)). G0 has a
walk W0 = v1v2v3v5 which covers all the edges of G0, as W0 is also a walk on G, by replacing v3
with v3v4v3 in W0 we can get a walk W = v1v2v3v4v3v5 which can cover all the edges of G.
B Lemma 2
The following lemma is crucial for the proof of Theorem 1.
14
Published as a conference paper at ICLR 2020
Lemma 2. Suppose that w, w0 are two random walks on graph G and graph H respectively, if the
representation of w and w0 are the same, i.e., rw = rw0, the number of the distinct edges on w and
w0 are the same, as well as the number of the distinct nodes on w and w0.
Proof. Let n1 , n2 be the number of distinct nodes on w, w0 respectively, let m1 , m2 be the number
of distinct edges on w and w0 respectively. First, let’s prove n1 = n2 . We will prove this by
contradiction. Assume n1 6= n2, without loss of generality, let n1 > n2. According to our encoding
rule, the largest number appears in a representation vector is the number of the distinct nodes in
the corresponding walk. Hence, the largest element in vector rw is n1 while the largest element in
vector rw0 is n2. Thus, rw 6= rw0, which contradicts our assumption. Therefore, we have n1 = n2.
Next, we will show m1 = m2 . We will also prove this point by contradiction. Assume m1 6= m2,
without loss of generality, let m1 > m2 . As we have proved n1 = n2, each edge on w and w0
will be encoded as a vector like [k1, k2]>, where k1, k2 ∈ [n1]. A walk consists of edges, hence the
representation of a walk is formed by the representation of edges. Since m1 > m2 , which means
there exists at least two consecutive element [k1 , k2]> in rw which will not appear in rw0 , thus
rw 6= rw0 , which is a contradiction of our assumption. As a result, we can prove m1 = m2 .
C Proof for Theorem 1
Proof. We will first prove the sufficiency of the theorem, i.e., suppose graphs G = (V (G), E(G))
and H = (V (H), E(H)) are two isomorphic graphs, we will show that the WEAVE’s distribution
on G and H are the same.
Let A be the set of all the possible walks with length k on G, B be the set of all the possible walks
with length k on H. Each element of A and B represents one unique walk on G and H respectively.
As we have assumed a WEAVE is a class of subgraphs, which means a WEAVE may corresponds
to multiple unique walks in A or B . Consider a walk w = v1 v2 . . . vi . . . vt ∈ A (vi represent the ith
node appears in the walk), for any edge e = (vi, vj) on wi, as e ∈ E(G), according to the definition
of isomorphism, there exists a mapping f : V (G) → V (H) such that (f (vi), f(vj)) ∈ E(H). If we
map each node on wi to graph H, we can get a new walk wi0 = f(v1)f(v2)...f(vt) on H as each
edge (f(vi), f(vj)) ∈ E(H), besides, as the length of wi0 is also k, we have wi0 ∈ B. Hence, we can
define a new mapping g : A → B, s.t.
∀wi = v1v2 . . .vt ∈ A, g(wi) = f(v1)f(v2) . . . f(vt) = wi0 ∈ B.	(8)
Next, we will show that g is a bijective mapping. Firstly, we will show that f is injective. Sup-
pose g(w1) = g(w2), we want to show w1 = w2. Assume w1 6= w2, there must exists one
step i such that w1 (i) 6= w2(i), let w1 (i) = (vi(1) , vj(1)), w2 (i) = (vi(2) , vj(2)), then we have
(f (vi(1)), f (vj(1))) 6= (f (vi(2)), f (vj(2))) due to the definition of isomorphism. According to the map-
pingruleoff, (f(vi(1)),f(vj(1))) is the ith step off(w1), (f(vi(2)),f(vj(2))) is the ith step of g(w2),
thus the walk g(w1) 6= g(w2), which contradicts our assumption. Therefore, the assumption is false,
we have w1 = w2. Then we will show that g is surjective, i.e., for any w0 ∈ B, there exists a w ∈ A
such that g(w) = w0. We will also prove this by contradiction, suppose there exists a walk w0 ∈ B
such that we can’t find any w ∈ A to make g(w) = w0. Let w0 = v1v2 . . . vt, according to the defi-
nition of isomorphism, for any edge (vi, vj) ∈ E(H) on w0, we have (f-1 (vi), f-1 (vj)) ∈ E(G),
where f-1 represents the inverse mapping of f. Hence
w = f-1(v1)f-1(v1)... f-1(vt) ∈A,	(9)
as w is a walk on graph H with length k. Now consider g(w), based on the mapping rule of g, we
need to map each node on w via f, i.e.,
g(w) = f(f-1(v1))f(f-1(v1))... f(f-1(vt)) = v1v2 ...vt = w0,	(10)
which is contradiction to our assumption. Thus we have proved g is an injective mapping as well as
a surjective mapping, then we can conclude that g is a bijective mapping.
Then we will show the WEAVEs’ distribution of G and H are the same. Since in our assumption,
|E(G)| is limited, then |A| and |B| are limited, besides, according to our encoding rule, different
15
Published as a conference paper at ICLR 2020
walks may correspond to one specific WEAVE while each WEAVE corresponds a unique represen-
tation vector, thus the number of all the possible representation vectors is limited for both G and H.
Thus, the representation vector’s distributions PG for graph G and representation’s distributions PH
for graph H are both discrete distributions. To compare the similarity of two discrete probability
distributions, we can adopt the following equation to compute the Wasserstein distance and check if
it is 0.
mn
i=1 j=1
m
s.t.	π(i, j) = wqj , ∀j,
i=1
n
π(i, j) = wpi, ∀i,
j=1
π(i, j) ≥ 0, ∀i, j,
(11)
where W1 (P, Q) is the Wasserstein distance of probability distribution P and Q, π(i, j) is the cost
function and s(i, j) is a distance function, wqj and wpj are the probabilities ofqj and pj respectively.
Since we have proved g : A → B is a bijection, besides, according to our encoding rule, g(w) and
w will corresponds to the same WEAVE, hence they will share the same representation vector. As
a consequence, for each point (gi, wgi) (gi corresponds to a representation vector, wgi represents
the probability of gi) in the distribution PG, we can find a point (hi, whi) in PH such that gi =
hi, and wgi = whi . Then consider (11), for PG and PH, if we let π be a diagonal matrix with
[wp1 , wp2 , . . . , wpm] on the diagonal and all the other elements be 0, we can make each element in
the sum Pim=1 Pjn=1 π(i, j)s(i, j) be 0, as this sum is supposed to be nonnegative, its minimum
is 0, hence W1 (PG, PH) = 0, which means for two isomorphic graphs G and H, their WEAVE’s
distributions PG and PH are the same.
Next we will prove the necessity of this theorem. Suppose that the Wasserstein distance between the
walk representation distributions PG and PH is 0, we will show that graph G and H are isomorphic.
Let the number of the nodes of graph G is n1 , the number of the nodes of graph H is n2, let the
number of the edges on graph G is m1 , the number if the edges on graph H is m2 . Let k =
2 max{m1, m2} - 1.
Now, we will give a bijective mapping f : V (G) → v(H). First, consider the walks on graph G, as
k = 2 max{m1, m2} - 1 ≥ 2m1 - 1, according to Lemma 1, there exists at least one walk of length
k on graph G which can cover all the edges of G. Consider such a walk wG, let rG = [1, 2, 3, ..., t]>
be the representation vector (corresponds to a WEAVE) we obtained according to our encoding rule.
Now, we will use this representation to mark the nodes on graph G . Mark the first node in this walk
as u1 (corresponds to 1 in the representation), the second node as u2, the ith appearing node in wG
is ui, continue this process untill we marked all the new appearing nodes in this walk. Since wG can
visit all the edges of graph G, all the nodes on this graph will definitely be marked, hence the last
new appearing node will be marked as un1 . Now, let’s consider the walks on graph H. As we have
assumed that W1(PG, PH) = 0, which means that for each point (gi, wgi) on PG, we can find a point
(hi, whi) in PH such that gi = hi, and wgi = whi. As a consequence, as rg is a point on PG, there
must be a point rh on H such that rh = rg = [1, 2, 3, ..., t]>. Then choose any walk wh on H which
produce rh, and apply the same method to mark the nodes in this walk in order as v1, v2, ..., vn1 .
Now we can define the mapping f, let f : V (G) → V (H), s.t., f(ui) = vi for ∀i ∈ [n1], which is
exactly the mapping we are looking for.
Next, we just need show for each edge (ui , uj ) ∈ E(G), we have (f (ui), f (uj )) ∈ E(H), and
vice versa, then we can prove G and H are isomorphic. The first direction is obviously true as wG
covers all the edges on G, for any edge (ui, uj) in wG, we have (f (ui), f(uj)) = (vi, vj) which
belongs to wh, since wh is walk on H, we have (vi, vj) ∈ E(H). Then we will prove the reverse
direction, i.e., for any (vi, vj) = (f(ui), f(uj)) ∈ E(H), we have (ui, uj ) ∈ E(G). To prove
this, we will first show that the number of edges of graph G and H are the same, i.e., m1 = m2.
Suppose this is not true, without loss of generality, let m1 > m2 . Since PG and PH are the results of
random walks for infinite times. Then there must exists some walks which can visit the additional
edges on G, as a consequence, we can obtain some representation vector which will not appear
16
Published as a conference paper at ICLR 2020
AEIlqEqoJd
∣1	Encoding
L1	Representations
Ared
CD	C
gree九	Cred
Q
c1.2
AEIlqEqoJd
4
16
「「厂「r
PLR	1G	1R	1G
2G	2R	2G	2R	Encoding
Hr	3G	1	血	Representations
AEIlqEqoJd
尸°	1-1∙1
2-1.1	2-1.2	∙
1.3 - 1.2	3 - 13
1
16
..Encoding
Representations
'gree九
(a) Graph without node attributes	(b) Graph with discrete node attributes (C) Graph With continuous node attributes
Figure 7: Walk representation distributions of graphs without attributes, graphs with discrete at-
tributes, and graphs with continuous attributes.
in Ph, which contradicts our assumption. Hence, We have mi = m2. Besides, since We have
rg = rh, according to Lemma 2, We can derive that the number of distinct edges on Wg and Wh
are the same. As wg covers all the edges on G, hence the number of distinct edges on wg is m1 .
Therefore, the number of distinct edges on Wh is also m1, which means Wh also has visited all the
edges on H. As for any edge (vi , vj) on Wh, we have (ui , uj) on Wh, in other words, we have
(ui, uj) = (f-1(vi), f-1(vj)) ∈ E(G). Hence we complete the proof.
Figure 7 shows the walk representation distributions for a 4 nodes ring with walk length k = 2 in
three different cases: without node attributes, with discrete node attributes, and with continuous node
attributes. We can see the attributes will have an influence to the distributions, more specifically, the
probability of each unique walk keeps the same no matter what the attributes are, however, the
probability of each representation vector may vary as different unique walks may correspond to one
representation vector, and the attributes may influence how many representation vectors there will
be and how many unique walks correspond to a representation vector. To clarify, in Figure 7 (a), the
ring graph does not have nodes attributes, there exists 16 unique walks in total, among them walk
ABD, BDC, DCA, CAB, DBA, CDB, ACD, BAC will all be encoded as r1 = [1 2 3]>, walk
ABA, BAB, BDB, DBD, CDC, DCD, CAC, ACA will be encoded as r2 = [1 2 1]>. Hence,
for a graph in Figure 7 (a), We have Pr(r1) = 岛,Pr(r2) = 岛.In Figure 7 (b), each node
has a discrete attribute, i.e., red or green, there are still 16 unique walks in total. However, in this
case, there exits four different representation vectors, walk ABC, CBA, ADC, CDA will be encoded
as r1 = [1R 2G 3R]>, where R represents Red while G represents Green; walk BCD, DCB,
DAB, DCB correspond to r2 = [1G 2R 3G]>; walk ABA, ADA, CDC, CBC correspond to
r3 = [1R 2G 3R]>; walk BAB, BCB, DCD, DAD correspond to r3 = [1R 2G 3R]>. In this
case, We have Pr(ri) = Pr(r2) = Pr(r3) = Pr(r4)= =.In the last, let's consider the case when
there exists continuous nodes attributes, for such a graph, the value of nodes attributes has infinite
choices, hence, it is very likely that each node may have different attribute. As a consequence, each
unique walk will correspond to a unique representation vector. In our example Figure 7 (c), there
also exists 16 unique walks, each walk has a particular representation vector, hence, the probability
of each representation vector is A.
D	Proof for Theorem 2
Proof. The proof for Theorem 2 is quite similar as the proof of Theorem 1, this is because the
attributes just influence the representation vector form and how many unique walks correspond to a
representation vector, however, the probability of each unique walk keeps same. Hence, we can use
a similar method to complete the proof. Similarly, we will first prove the sufficiency. Let G and H
17
Published as a conference paper at ICLR 2020
be two isomorphic graphs with attributes, we will prove that the walk representations distribution of
G and H are the same. Suppose that A and B are the sets of possible walks of length k on G and H
respectively. By applying the same analysis method as in the proof of Theorem 1, we can show that
there exists a bijective mapping g : A → B such that for ∀wi = v1v2v3 . . . vt ∈ A, we have
g(wi) = f(v1)f(v2) . . .f(vt) ∈ B,	(12)
where f : V (G) → V (H) satisfies ∀(vi, vj) ∈ E(G), we have (f(vi), f(vj)) ∈ E(H) and for ∀vi ∈
V (G), the attribute of vi and f (vi) are the same. Hence, according to our encoding rule, wi and
f(wi) will be encoded as the same representation vector, which means for each point (rgi , P r(rgi))
in the representation distribution of G, we can find a point (rhi , P r(rhi)) in the distribution of
H such that rgi = rhi , P r(rgi) = P r(rhi ). Thus, we can obtain the Wasserstein distance of
distribution PG and the distribution PH is W1 (PG, PH) = 0 via a similar approach as in Theorem
1. In other words, we have PG = PH . In addition, the necessity proof of Theorem 2 is the same as
Theorem 1.
E Graphs with node attributes and edge attributes
If both the nodes and edges in a graph have attributes, the graph is an attributed graph denoted by
G = (V, E, α, β), where α : V → LN and β : E → LE are nodes and edges labeling functions,
LN, LE are sets of labels for nodes and edges. In this case, the graph isomorphism are defined as:
Definition . Given two graphs G = (V (G), E(G), αG, βg) and H = (V (H), E(H), αH, βH), then
G and H are isomorphic with node attributes as well as edge attributes if there is a bijection f :
V(G)⇔V(H)
∀uv ∈ E(G) ⇔f(u)f(v) ∈E(H),	(13)
αG (u) = αH(f(u)),∀u ∈ V (G),	(14)
βG (u, v) = βH(f(u), f (v)).	(15)
Corollary 1. Let G = (V (G), E(G)) and H = (V (H), E(H)) be two connected graphs with node
attributes. Suppose we can enumerate all possible WEAVEs on G and H with a fixed-length k ≥
2 max{|E (G)|, |E(H)|} - 1, where each WEAVE has a unique vector representation generated from
a well-trained autoencoder. The Wasserstein distance between G’s and H’s WEAVE distributions is
0 if and only if G and H are isomorphic with both node attributes and edge attributes.
Proof. When both nodes and edges of a graph are given attributes, the representation vectors of ran-
dom walks will be different. However, just like the cases with only nodes attributes, the probability
of each unique walk on the graph keeps same. Hence, we can follow a similar analysis method as
Theorem 2 to complete this proof.
Dataset	Identity Kernel			DeepSet-MMD		
	Classification ACC	Clustering		Classification ACC	Clustering	
		ACC	NMI		ACC	NMI
NCI1	0.6105	0.5510	0.0073	0.6382	0.5630	0.0095
PROTEINS	0.7207	0.5957	0.0518	0.7103	0.5965	0.0438
COLLAB	0.6720	0.5973	0.2108	0.6572	0.5668	0.2015
IMDB-BINARY	0.7660	0.5776	0.0241	0.7210	0.5219	0.0225
IMDB-MULTI	0.4466	0.3816	0.0214	0.4258	0.3647	0.0168
Table 5: Representation evaluation based on classification and clustering down-stream tasks
F DeepSet in the Component of Embedding Distributions
In this section, we investigate whether DeepSet (Zaheer et al., 2017) is an effective technique for
distribution embedding. In particular, we employ DeepSet to replace the multi-layer neural net-
work for feature mapping function approximation, and similarity values generated by MMD serve
18
Published as a conference paper at ICLR 2020
as supervision signals to guide DeepSet training. In our experiments, we compare the SEED imple-
mentation based on DeepSet with MMD (DeepSet in Table 5) with the SEED implementation based
on the identity kernel (Identity Kernel in Table 5). We also observe that the MMD does not have
significant performance difference. The result confirms that DeepSet could be a strong candidate for
the component of Embedding subgraph distributions.
G Ablation Study on WEAVE
Feature utilized	Classification ACC	Clustering ACC	Clustering NMI
Only node feature	0.6444	0.6744	0.0625
Only earliest visit time	0.8112	0.8014	0.3214
Node feature + Earliest visit time	0.8222	0.7260	0.1567
Table 6: The impact of node feature and earliest visit time in WEAVE based on MUTAG dataset
In this section, we investigate the impact of node features and earliest visit time in WEAVE. In
Table 6, Only node feature means only node features in WEAVE are utilized for subgraph encoding
(which is equivalent to vanilla random walks), only earliest visit time means only earliest visit time
information in WEAVE is used for subgraph encoding, and Node feature + earliest visit time means
both information is employed. We evaluate the impact on the MUTAG dataset. As shown above,
it is crucial to use both node feature and earliest visit time information in order to achieve the best
performance. Interestingly, on the MUTAG dataset, We observe that clustering could be easier if
We only consider earliest visit time information. On the MUTAG dataset, node features seem to
be noisy for the clustering task. As the clustering task is unsupervised, noisy node features could
negatively impact its performance when both node features and earliest visit time information are
considered.
H NYSTROM APPROXIMATION IN THE SEED FRAMEWORK
In this section, we evaluate the impact OfNyStrOm based kernel approximation (Williams & Seeger,
2001) to the component of embedding distributions.
Figure 8: Response time comparison between exact MMD and its Nystrom approximation
First, we investigate the impact to the effectiveness in the downstream tasks. In this set of exper-
iment, we implement a baseline named SEED-Nystrom, where the Nystrom method is applied to
approximate RBF kernel based MMD during training phases with 200 sampled WEAVEs. In partic-
ular, top 30 eigenvalues and the corresponding eigenvectors are selected for the approximation. As
shown in Table 7, across five datasets, SEED-NyStrOm achieves comparable performance, compared
with the case where an identity kernel is adopted.
In addition, we evaluate the response time of exact RBF kernel based MMD and its Nystrom ap-
proximation. Top 30 eigenvalues and the corresponding eigenvectors are selected for the Nystrom
19
Published as a conference paper at ICLR 2020
Dataset	RBF Kernel			SEED-Nystrom		
	Classification ACC	Clustering		Classification ACC	Clustering	
		ACC	NMI		ACC	NMI
NCI1	0.6211	0.5610	0.0079	0.6281	0.5518	0.0081
PROTEINS	0.7161	0.5857	0.0476	0.7054	0.5738	0.0389
COLLAB	0.6718	0.5212	0.1831	0.6447	0.5217	0.1983
IMDB-BINARY	0.7421	0.5582	0.0218	0.7280	0.5018	0.0211
IMDB-MULTI	0.4541	0.3985	0.0241	0.4148	0.3676	0.0172
Table 7: Representation evaluation based on classification and clustering down-stream tasks
approximation. As shown in Figure 8, when we range the number of WEAVE samples from 100 to
2000, the Nystrom approximation scales better than the exact MMD evaluation.
In summary, the Nystrom method is a promising method that can further improve the scalability of
the SEED framework in training phases, especially for the cases where a large number of WEAVE
samples are required.
20