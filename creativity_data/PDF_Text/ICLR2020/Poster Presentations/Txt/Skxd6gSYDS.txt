Published as a conference paper at ICLR 2020
Query-efficient Meta Attack to Deep
Neural Networks
JiaWei Du1,3 ∖ Hu Zhang2∖Joey Tianyi Zhou3, Yi Yang2, Jiashi Feng1
1Dept. ECE, National University of Singapore, Singapore
2ReLER, University of Technology Sydney, Australia
3Institute of High performance Computing, A*STAR, Singapore
dujiawei@u.nus.edu,Hu.Zhang-1@student.uts.edu.au
joey.tianyi.zhou@gmail.com,Yi.Yang@uts.edu.au
elefjia@nus.edu.sg
Ab stract
Black-box attack methods aim to infer suitable attack patterns to targeted DNN
models by only using output feedback of the models and the corresponding input
queries. However, due to lack of prior and inefficiency in leveraging the query and
feedback information, existing methods are mostly query-intensive for obtaining
effective attack patterns. In this work, we propose a meta attack approach that is
capable of attacking a targeted model with much fewer queries. Its high query-
efficiency stems from effective utilization of meta learning approaches in learning
generalizable prior abstraction from the previously observed attack patterns and
exploiting such prior to help infer attack patterns from only a few queries and
outputs. Extensive experiments on MNIST, CIFAR10 and tiny-Imagenet demon-
strate that our meta-attack method can remarkably reduce the number of model
queries without sacrificing the attack performance. Besides, the obtained meta
attacker is not restricted to a particular model but can be used easily with a fast
adaptive ability to attack a variety of models. The code of our work is available at
https://github.com/dydjw9/MetaAttack_ICLR2020/.
1	Introduction
Despite the great success in various tasks, deep neural networks (DNNs) are found to be suscepti-
ble to adversarial attacks and often suffer dramatic performance degradation in front of adversarial
examples, even if only tiny and invisible noise is imposed on the input (Szegedy et al., 2014). To in-
vestigate the safety and robustness of DNNs, many adversarial attack methods have been developed,
which apply to either a white-box (Goodfellow et al., 2015; Moosavi-Dezfooli et al., 2016; Carlini
& Wagner, 2017; Madry et al., 2018) or a black-box setting (Papernot et al., 2017; Brendel et al.,
2018; Narodytska & Kasiviswanathan, 2017). In the white-box attack setting, the target model is
transparent to the attacker and imperceptible adversarial noise can be easily crafted to mislead this
model by leveraging its gradient information (Goodfellow et al., 2015). In contrast, in the black-
box setting, the structure and parameters of the target DNN model are invisible, and the adversary
can only access the input-output pair in each query. With a sufficient number of queries, black-box
methods utilize the returned information to attack the target model generally by estimating gradient
(Chen et al., 2017; Ilyas et al., 2018a; Narodytska & Kasiviswanathan, 2017; Cheng et al., 2019).
Black-box attack is more feasible in realistic scenarios than white-box attack but it is much more
query-intensive. Such a drawback is largely attributed to the fact that returned information for each
queried example is sparse and limited. During inferring attack patterns, existing black-box methods
simply integrate the information between two sequential iterations brutally and ignore the implicit
but profound message, thus not fully exploiting the returned information. Although query-efficient
algorithms for generating attack examples are very meaningful in practice (Ilyas et al., 2018a), how
to enhance query-efficiency for black-box attack remains underexplored.
* Equal contribution.
1
Published as a conference paper at ICLR 2020
In this work, we address a query-efficiency concerned attack problem. Particularly, we consider
only top-k probability scores accessible from the target black-box model. With this practical but
challenging scenario, we aim at three important objectives: lower query number, higher success
rate and smaller noise magnitude. We develop a meta-learning based attack method, which applies
meta learning to obtaining prior information from the successful attack patterns, and uses the prior
for efficient optimization. Specifically, we propose to train a meta attacker model through meta
learning (Nichol et al., 2018), inspired by its success in solving few-shot learning problems. We first
deploy several existing classification models to get pairs of (images, gradients) with the max-margin
logit classification loss. Then we use the data pairs of each classification model to train the meta
attacker. After obtaining the attacker, we use it to attack a new black-box model for accelerating the
search process for adversarial examples by optimizing it with coordinate-wise gradient estimation.
Different from previous methods, we use the estimated gradient not only to update adversarial noise
but to fine-tune the well-trained attacker. After few-shot fine-tuning, the attacker is able to simulate
the gradient distribution of the target model.
We evaluate our method on MNIST, CIFAR10 and tiny-ImageNet datasets by comparing it
with state-of-the-art black-box attack methods including Zoo (Chen et al., 2017), Decision-
Boundary (Brendel et al., 2018), AutoZoom (Tu et al., 2019), Opt-attack (Cheng et al., 2019) and
Bandits (Ilyas et al., 2018b). In both targeted and untargeted settings, our proposed method achieves
comparable attack success rate and adversarial perturbation to all baselines but with a significantly
reduced query number. The detailed experiment results demonstrate our superior query-efficiency.
2	Related Work
Classical white-box attack methods include Fast-Gradient Sign Method (FGSM) (Goodfellow et al.,
2015), IFGSM (Madry et al., 2018), DeepFool (Moosavi-Dezfooli et al., 2016) and C&W at-
tack (Carlini & Wagner, 2017), following a setting where detailed information about the target model
(gradients and losses) is provided. Comparatively, the black-box setting better accords with the real
world scenarios in that little information about the target model is visible to the attacker. The pi-
oneer work on black-box attack (Papernot et al., 2017) tries to construct a substitute model with
augmented data and transfer the black-box attack problem to a white-box one. However, its attack
performance is very poor due to the limited transferability of adversarial examples between two dif-
ferent models. (Brendel et al., 2018) considers a more restricted case where only top-1 prediction
classes are returned and proposes a random-walk based attack method around the decision boundary.
It dispenses class prediction scores and hence requires extensive model queries. Zoo (Chen et al.,
2017) is a black-box version of C&W attack, achieving a similar attack success rate and comparable
visual quality as many white-box attack methods. However, its coordinate-wise gradient estimation
requires extensive model evaluations. More recently, (Ilyas et al., 2018a) proposes a query-limited
setting with L∞ noise considered, and uses a natural evolution strategy (NES) to enhance query effi-
ciency. Though this method successfully controls the query number, the noise imposed is larger than
average. (Narodytska & Kasiviswanathan, 2017) proposes a novel local-search based technique to
construct numerical approximation to the network gradient, which is then carefully used to construct
a small set of pixels in an image to perturb. It suffers a similar problem as in (Chen et al., 2017) for
pixel-wise attack. (Cheng et al., 2019) considers a hard-label black-box setting and formulates the
problem as real-valued optimization that is solved by a zeroth order optimization algorithm. Ilyas
et al. (2018b) reduce the queries by introducing two gradient priors, the time-independent prior and
the data-dependent prior, and reformulating the optimization problem.
We then briefly introduce some works on meta-learning related to our work. Meta-learning is a pro-
cess of learning how to learn. A meta-learning algorithm takes in a distribution of tasks, each being
a learning problem, and produces a quick learner that can generalize from a small number of exam-
ples. Meta-learning is very popular recently for its fast adaptive ability. MAML (Finn et al., 2017) is
the first to propose this idea. Recently, a simplified algorithm Reptile (Nichol et al., 2018) which is
an approximation to the first-order MAML is proposed, achieving higher efficiency in computation
and consuming less memory. With these superior properties, meta learning is applied to adversarial
attack methods (Zgner & Gnnemann, 2019; Edmunds et al., 2017). Zgner & Gnnemann (2019) try
to attack the structure of a graph model in the training process to decrease the model generalization
performance. Edmunds et al. (2017) investigate the susceptibility of MAML to adversarial attacks
and the transferability of the obtained meta model to a specific task.
2
Published as a conference paper at ICLR 2020
3	Method
3.1	Preliminaries: Black-box Attack Schemes
We first formulate the black-box attack problem and introduce the widely used solutions. We use
(x,t) to denote the pair of a natural image and its true label, and X and £ to denote the adversarial
perturbed version ofx and the returned label by the target classification model Mtar. The black-box
attack aims to find an adversarial example X with imperCeivabIe difference from X to fail the target
model , i.e., t 6= t through querying the target model for multiple times. It can be formulated as
min '(X, Mtar (X),t)
X	(1)
s.t. kχ - Xkp ≤ ρ, #queries ≤ Q.
Here ∣∣ ∙ ∣∣p denotes the 'p norm that measures how much perturbation is imposed. Mtar(X) is the
returned logit or probability by the target model Mtar. The loss function '(X, Mtar (X),t) measures
the degree of certainty for model Mtar assigning the input X into class t. One common used
adversarial loss is the probability of class t: '(X, Mtar(X),t) = PMtar(t∣X). The first constraint
enforces high similarity between the clean image X and the adversarial one Xadv and the second
imposes a fixed budget Q for the number of queries allowed in the optimization.
In the white-box attack setting, the adversary can access the true gradient Vχt'(Xt) and perform
gradient descent ^t+ι = Xt — Nxt'(Xt). But in the black-box setting, the gradient information
Vχt'(Xt) is not attainable. In this case, the attacker can estimate the gradient using only queried
information from model evaluation such as hard label, logits and probability scores. This kind
of estimator is the backbone of so-called zeroth-order optimization approaches (Chen et al., 2017;
Narodytska & Kasiviswanathan, 2017; Tu et al., 2019; Ilyas et al., 2018a;b). The estimation is
done via finite difference method (Chen et al., 2017; Narodytska & Kasiviswanathan, 2017; Tu
et al., 2019), which finds the k components of the gradient by estimating the inner products of the
gradients with all the standard basis vector e1, ..., ek:
V'(X) ≈ XX f(X + hei)2∕(X -hei)ei,	⑵
i=1
where step size h controls the quality of the estimated gradient. Another strategy is to reformulate
the loss function (Ilyas et al., 2018a;b). Instead of computing the gradient of '(x) itself, the ex-
pected value of loss function '(x) under the search distribution is minimized and when the search
distribution of random Guassian noise is adopted, the gradient estimation problem transfers into a
zeroth-order estimation problem,
1n
VE['(x)] ≈——，2'(x + σδi)δi	(3)
i=1
where n is the amount of noise sampled from the distribution. After obtaining the estimated gradient,
classical optimization algorithms (Nesterov, 2013; Johnson & Zhang, 2013) can be used to infer the
adversarial examples. Though the estimated gradient may not be accurate, it is still proved useful
enough in adversarial attack. The convergence of these zeroth-order methods is guaranteed under
mild assumptions (Ghadimi & Lan, 2013; Nesterov & Spokoiny, 2017; Hazan et al., 2016).
Since each model evaluation consumes a query, naively applying the above gradient estimation to
black-box attack is quite query expensive due to its coordinate or noise sampling nature. Take the
first strategy on tiny-Imagenet dataset for example. It consumes more than 20,000 queries for each
image to obtain a full gradient estimate, which is not affordable in practice. In this work, we address
such a limitation via developing a query-efficient meta-learning based attack model.
3.2	Learning of Meta Attacker
To reduce the query cost for black-box attack, we apply meta learning to training a meta attacker
model, inspired by its recent success in few-shot learning problems (Finn et al., 2017; Nichol et al.,
2018). The meta attacker learns to extract useful prior information of the gradient of a variety of
models w.r.t. specific input samples. It can infer the gradient for a new target model using only a
3
Published as a conference paper at ICLR 2020
Algorithm 1 Meta Attacker Training
Input: Input images X, groundtruth gradients Gi generated from classification models Mi to serve
as task Ti ;
1:	Randomly initialize θ;
2:	while not done do
3:	for all Ti do
4:	Sample K samples from (X, Gi) for training, denoted as (Xs , Gis);
5:	Evaluate Vθ Li(Aθ) = Vθ ∣∣Aθ (Xs) - Gfk2 WithreSPectto(Xs, GS);
6:	Update θi := θ - αVθLi(Aθ);
7:	end for
8:	Update θ := θ + E 1 pn=1(θi - θ);
9:	end while
Output: Parameters θ of meta model A.
feW queries. After obtaining such a meta attacker, We replace the zeroth-order gradient estimation
in traditional black box attack methods With it to directly estimate the gradient.
We collect a set of existing classification models M1, ..., Mn to generate gradient information for
universal meta attacker training. Specifically, We feed each image x into the models M1, ..., Mn
respectively and compute losses `1, ..., `n by using folloWing max-margin logit classification loss:
'i(x) = max
log[Mi(x)]t - max log[Mi(x)]j, 0 .
j6=t
(4)
Here t is the groundtruth label and j indexes other classes. [Mi (x)]t is the probability score of
the true label predicted by the model Mi, and [Mi(x)]j denotes the probability scores of other
classes. By performing one step back-propagation of losses `1, ..., `n W.r.t. the input images x, the
corresponding gradients gi = Vχ'i(x), i = 1,…,n are obtained. Finally, we collect n groups of
data X = {x}, Gi = {gi}, i = 1, ..., n to train the universal meta attacker.
We design a meta attacker A which has a similar structure as an autoencoder, consisting of sym-
metric convolution and de-convolution layers and outputs a gradient map with the same size as the
input. Meta attacker model A is parameterized with parameters θ.
Due to the intrinsic difference between selected classification models, each obtained set (X, Gi) is
treated as a task Ti in meta attacker training. During the training process, for each iteration we
only draw K samples from task Ti and feedback the loss Li to update model parameters from θ to
θi0. θi0 is then computed through one or multiple gradient descents: θi0 := θ - αVθLi (Aθ). For
a sensitive position of the meta attacker, the meta attacker parameters are optimized by combining
each θi0 across all tasks {Ti}i=1,...,n , following the update strategy of Reptile (Nichol et al., 2018)
in meta learning,
1n
θ ：= θ + e-E(θi - θ).	(5)
n i=1
We adopt mean-squared error (MSE) as the training loss in the inner update,
Li (Aθ) = ∣Aθ(Xs ) - Gis ∣22 .	(6)
The set (Xs, Gis) denotes the K samples used for each inner update from θ to θi0. Since the number
of K sampled each time is very small, the update strategy above tries to find good meta attacker
parameters θ as an initial point, from which the meta attacker model can fast adapt to new data
distribution through gradient descent based fine-tuning within limited samples. Therefore, this char-
acteristic can be naturally leveraged in attacking new black-box models by estimating their gradient
information through a few queries. Detailed training process of our meta attacker is described in
Algorithm 1.
3.3	Query-efficient Attack via Meta Attacker
An effective adversarial attack relies on optimizing the loss function equation 1 w.r.t. the input image
to find the adversarial example of the target model Mtar. Differently, our proposed method applies
the meta attacker A to predicting the gradient map of a test image directly.
4
Published as a conference paper at ICLR 2020
Algorithm 2 Adversarial Meta Attack Algorithm
Input: Test image x0 with label t, meta attacker Aθ, target model Mtar, iteration interval m,
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
selected top-q coordinates;
for t = 0, 1, 2, ... do
if (t + 1) mod m = 0 then
Perform zeroth-order gradient estimation on top q coordinates, denoted as It and obtain gt ;
Fine-tune meta attacker A with (xt, gt) on It by loss L = k[Aθ(xt)]It - [gt]It k22;
else
Generate the gradient map gt directly from meta attacker A with xt , select coordinates It ;
end if
Update [x0]It = [xt]It + β[gt]It;
if Mtar (x0) 6= t then
xadv = x ;
break;
else
xt+1 = x ;
end if
end for
Output: adversarial example xadv .
We use the obtained meta attacker model A to predict useful gradient map for attacking, which
should be fine tuned to adapt to the new gradient distribution under our new target model. Par-
ticularly, instead of finetuning once, for each given image x, we update A by leveraging query
information with the following periodic scheme. Suppose the given image is perturbed to xt ∈ Rp
at iteration t. If (t + 1) mod m = 0, our method performs zeroth-order gradient estimation to ob-
tain gradient map gt for fine-tuning. As each pixel value for estimated gradient map consumes two
queries, for further saving queries, we just select q ofthep coordinates to estimate, q p, instead of
the full gradient map through all p coordinates. The indexes of chosen coordinates are determined by
the gradient map gt-1 obtained in iteration t- 1. We sort the coordinate indexes by the value of gt-1
and select top-q indexes. The set of these indexes are denoted as It . We feed image xt in iteration
t into meta attacker A and compute the MSE loss on indexes It, i.e. L = k[Aθ(xt)]It - [gt]It k22.
Then we perform gradient descent for the MSE loss with a few steps to update the parameters θ of
meta attacker A. For the rest iterations, we just use the periodically updated attacker Aθ to directly
generate the gradient gt = Aθ(xt). When we have the estimated gradient map gt in iteration t, we
update to get the adversarial sample x0t by [x0t]It = [xt]It + β[gt]It where β is a hyperparameter to
be tuned. The details are summarized in Algorithm 2.
In our method, the following operations contribute to reducing the query number needed by the
attacker. First, though we just use q coordinates to fine-tune our meta attacker A every m iterations,
the meta attacker A is trained to ensure that it can abstract the gradient distribution of different xt
and learn to predict the gradient from a few samples with simple fine-tuning. Secondly, the most
query-consuming part lies in zeroth-order gradient estimation, due to its coordinate-wise nature.
In our algorithm, we only do this every m iterations. When we use the finetuned meta attacker A
directly, no query is consumed in gradient estimation in these iterations. Intuitively, larger m implies
less gradient estimation computation and fewer queries. Besides, just as mentioned above, even in
zeroth-order gradient estimation, only top-q coordinates are required. Normally, q is much smaller
than dimension p of the input.
4	Experiments
We compare our meta attacker with state-of-the-art black-box attack methods including Zoo (Chen
et al., 2017), Decision-Boundary (Brendel et al., 2018), AutoZoom (Tu et al., 2019), Opt-
attack (Cheng et al., 2019), FW-black (Chen et al., 2018) and Bandits (Ilyas et al., 2018b) to evaluate
its query efficiency. We also study its generalizability and transferability through a Meta transfer
attacker, as detailed in the following sections.
5
Published as a conference paper at ICLR 2020
4.1	Settings
Datasets and Target Models We evaluate the attack performance on MNIST (LeCun, 1998) for
handwritten digit recognition, CIFAR10 (Krizhevsky & Hinton, 2009) and tiny-Imagenet (Rus-
sakovsky et al., 2015) for object classification. The architecture details of meta attack models on
MNIST, CIFAR10 and tiny-Imagenet are given in Table 6. For MNIST, we train a separate meta
attacker model since the images have different channel numbers from other natural image datasets.
For CIFAR10 and tiny-Imagenet, we use a common meta attacker model. On CIFAR-10, we choose
ResNet18 (He et al., 2016) as the target model Mtar and use VGG13, VGG16 (Simonyan & Zisser-
man, 2014) and GoogleNet (Szegedy et al., 2015) for training our meta attacker. On tiny-Imagenet,
we choose VGG19 and ResNet34 as the target model separately, and use VGG13, VGG16 and
ResNet18 for training the meta attacker together.
Attack Protocols For a target black-box model Mtar , obtaining a pair of (input-output) is consid-
ered as one query. We use the mis-classification rate as attack success rate; we randomly select 1000
images from each dataset as test images. To evaluate overall noise added by the attack methods,
We use the mean L? distance across all the samples noise(Mtar) = n Pin=1 kxia,dAv,Mtar - xik2,
Where xia,dAv,M denotes the adversarial version for the authentic sample xi.
Meta-training Details For all the experiments, We use the same architecture for the meta attacker
A as shoWn in Table 6. We use Reptile (Nichol et al., 2018) With 0.01 learning rate to train meta
attackers. We use 10000 randomly selected images from the training set to train the meta-attackers
in three datasets. The proportion of the selected images to the Whole training set are 16%, 20%, and
10% respectively. Fine-tuning parameters are set as m = 5 for MNIST and CIFAR10, and m = 3
for tiny-Imagenet. Top q = 128 coordinates are selected as part coordinates for attacker fine-tuning
and model attacking on MNIST; and q = 500 on CIFAR10 and tiny-Imagenet.
4.2	Comparison with Baselines
We compare our meta attacker With baselines for both the untargeted and targeted black-box attack
on the three datasets. The results are reported in detail as beloW.
Untargeted Attack Untargeted attack aims to generate adversarial examples that Would be mis-
classified by the attacked model into any category different from the ground truth one. The overall
results are shoWn in Table 1, in Which, Meta transfer denotes that meta attacker trained on one
dataset is used to attack target models on another dataset . Our method is competitive With baselines
in terms of adversarial perturbation and success rate, but our query number is reduced.
We also compare the results of our method With Zoo (Chen et al., 2017) and AutoZoom (Tu et al.,
2019) from a query-efficiency perspective. We use these models to conduct untargeted attack on
CIFAR10 and tiny-Imagenet by limiting a maximum number of queries for each adversarial example
and compare their success rate. The results are shoWn in Fig. 1. We notice that for different query
thresholds, the success rate of our method is alWays higher than Zoo and AutoZoom. This is possibly
because the testing samples have different L2 distances to the decision boundary. Higher success
rate of our method indicates our meta attacker can predict correct gradient even When the query
information is limited. These results give strong evidence on effectiveness of our proposed method
for enhancing query efficiency.
Ss38ns XUeUV
Figure 1: Comparison with limited queries.
seα Ss38ns Xuew
SeH Ssguns
SucfessRate
,Lz, QUeneS vs. t。P~q se［版ιo r∣ajo
~~∙	∣0∙55 3500
--Success Rate
→- L2
Queries
Success Rate
→- L2
T- Queries
0-50
0-35
0-30
OJS
«0 6sS211
MOO
3S00
3000
2500 OT
且
2000 0
woo O'
ιaoo
sao
0
Figure 2: Top-q and β selection.


Targeted Attack Targeted attack aims to generate adversarial noise such that the perturbed sample
would be mis-classified into any pre-specified category. It is a more strict setting than the untargeted
6
Published as a conference paper at ICLR 2020
Table 1: MNIST, CIFAR10 and tiny-ImageNet untargeted attack comparison: Meta attacker attains compara-
ble success rate and L2 distortion as baselines, and significantly reduces query numbers.
Dataset / Target model	Method	Success Rate	Avg. L2	Avg. Queries
	Zoo (Chen et al., 2017)	1.00	1.61	21,760
	Decision Boundary (Brendel et al., 2018)	1.00	1.85	13,630
	Opt-attack (Cheng et al., 2019)	1.00	1.85	12,925
MNIST / Net4	AutoZoom (Tu et al., 2019),	1.00	1.86	2,412
	Bandits (Ilyas et al., 2018b)	0.73	1.99	3,771
	Meta attack (ours)	1.00	1.77	749
	Zoo (Chen et al., 2017)	1.00	0.30	8,192
	Decision Boundary (Brendel et al., 2018)	1.00	0.30	17,010
	Opt-attack (Cheng et al., 2019)	1.00	0.33	20,407
CIFAR10 / Resnet18	AutoZoom (TU et al., 2019)	1.00	0.28	3,112
	Bandits (Ilyas et al., 2018b)	0.91	0.33	4,491
	FW-black (Chen et al., 2018)	1.00	0.43	5,021
	Meta transfer (ours)	0.92	0.35	1,765
	Meta attack (ours)	0.94	0.34	1,583
	Zoo (Chen et al.,2017)	1.00	0.52	27,827
	Decision Boundary (Brendel et al., 2018)	1.00	0.52	49,942
	Opt-attack (Cheng et al., 2019)	1.00	0.53	71,016
tiny-ImageNet / VGG19	AutoZoom (TU et al., 2019)	1.00	0.54	8,904
	Bandits (Ilyas et al., 2018b)	0.78	0.54	9,159
	Meta transfer (ours)	0.99	0.56	3,624
	Meta attack (ours)	0.99	0.53	3,278
	Zoo (Chen et al.,2017)	1.00	0.47	25,344
	Decision Boundary (Brendel et al., 2018)	1.00	0.48	49,982
tiny-ImageNet / Resnet34	AutoZoom (TU et al., 2019) Opt-attack (Cheng et al., 2019)	1.00 1.00	0.45 0.52	9,770 60,437
	Bandits (Ilyas et al., 2018b)	0.73	0.49	9,978
	Meta transfer (ours)	0.99	0.56	3,540
	Meta attack (ours)	0.99	0.53	3,268
one. For fair comparison, we define the target label for each sample —a sample with label ` gets the
target label (`+ 1) mod #classes. We deploy our meta attacker the same as above. The results on
MNIST, CIFAR10 and tiny-ImageNet are shown in Table 2. Similar to results of untargeted attack,
we achieve comparable noise and success rate to baselines but with reduced query numbers.
4.3	Model Analysis
Meta Training We first test the benefits of meta training by comparing performance of a meta-
trained attacker with a Gaussian randomly initialized attacker without meta training on the three
datasets. Fig. 3 shows their success rate, L2 distortion and query count results for initial success. The
meta pre-trained attacker achieves averagely 7% higher success rate with 16% lower L2 distortion
and 30% less queries, compared with the randomly initialized one. This justifies the contributions
of meta training to enhancing query efficiency and also attack performance.
Guaranteed by fine-tuning, the randomly initialized attacker succeeds over many testing samples.
The fine-tuning works like an inner training in meta training. With sufficient fine-tuning iterations,
the randomly initialized attacker functions like a well-trained meta attacker. This explains the effec-
tiveness of the randomly initialized meta attacker on many testing samples compromised by more
queries. However, it could not predict gradient as accurate as the well-trained meta attacker during
earlier iterations. Such inaccuracy leads to larger L2 distortion at the beginning. On the contrary, the
meta training process enables the well-trained meta attacker to fast-adapt to current testing samples.
These results highlight the significant advantages of our meta model towards to black-box attack.
The process of meta training makes it familiar with gradient patterns of various models.
Generalizability Here we show that our meta attacker trained on one dataset can be transferred to
other datasets. We conduct this experiment between CIFAR10 and tiny-Imagenet, denoted as Meta
transfer in Table 1 and 2. We first apply the meta attacker trained on CIFAR10 to attack VGG19,
ResNet34 on tiny-Imagenet respectively, which are different from models used for training meta
attacker. Note the meta attacker tested on CIFAR10 has no privileged prior and is not familiar with
7
Published as a conference paper at ICLR 2020
Table 2: MNIST, CIFAR10 and tiny-ImageNet targeted attack comparison: Meta attack significantly outper-
forms other black-box methods in query numbers.
Dataset / Target model	Method	Success Rate	Avg. L2	Avg. Queries
	Zoo (Chen et al., 2017)	1.00	2.63	23,552
	Decision Boundary (Brendel et al., 2018)	0.64	2.71	19,951
MNIST / Net4	AutoZoom (Tu et al., 2019)	0.95	2.52	6,174
	Opt-attack (Cheng et al., 2019)	1.00	2.33	99,661
	Meta attack (ours)	1.00	2.66	1,299
	Zoo (Chen et al.,2017)	1.00	0.55	66,400
	Decision Boundary (Brendel et al., 2018)	0.58	0.53	16,250
CIFAR10 / Resnet18	AutoZoom (Tu et al., 2019)	1.00	0.51	9,082
	Opt-attack (Cheng et al., 2019)	1.00	0.50	121,810
	FW-black (Chen et al., 2018)	0.90	0.73	6,987
	Meta transfer (ours)	0.92	0.74	3,899
	Meta attack (ours)	0.93	0.77	3,667
	Zoo (Chen et al.,2017)	0.74	1.26	119,648
	AutoZoom (Tu et al., 2019)	0.87	1.45	53,778
tiny-ImageNet / VGG19	Opt-attack (Cheng et al., 2019)	0.66	1.14	252,009
	Meta transfer (ours)	0.55	1.37	12,275
	Meta attack (ours)	0.54	1.24	11,498
	Zoo (Chen et al., 2017)	0.60	1.03	88,966
	AutoZoom (Tu et al., 2019)	0.95	1.15	52,174
tiny-ImageNet / Resnet34	Opt-attack (Cheng et al., 2019)	0.78	1.00	214,015
	Meta transfer (ours)	0.69	1.40	13,435
	Meta attack (ours)	0.54	1.21	12,897
Mnist Net4
CifarlO Resnetl8
Figure 3: Comparison of randomly initialized and well-trained meta attackers.
ι.o
0.8
0.6
0.4
0.2
0.0
0.99
tiny-lmagenet VGG19
Success Rate L2 Dist Queries
neither tiny-Imagenet dataset nor the corresponding classification models. Similarly, we also use
the meta attacker trained on tiny-Imagenet to attack the target ResNet18 model on CIFAR10. The
results show the good generalizability and robustness of our proposed meta attacker.
Parameters Selection We test the selection of top-q and β on CIFAR10. We choose q ranging
from 350 to 600 and give the results with β ranging from 3e-3 to 5e-3, as shown in Fig. 2. When q
increases, query number, success rate and L2 will all increase. In order to balance the overall result,
we choose q to be 500. When β increases, success rate and L2 will increase and query number
decreases. In order to balance success rate and query, we choose β to be 4e-3 in the experiment.
5	Conclusion
We propose a meta-based black-box attack method that largely reduces demanded query numbers
without compromising in attack success rate and distortion. We train a meta attacker to learn useful
prior information about gradient and incorporate it into the optimization process to decrease the
number of queries. Specifically, the meta attacker is finetuned to fit the gradient distribution of target
model and each update is based on the output of finetuned meta attaker. Extensive experimental
results confirm the superior query-efficiency of our method over baselines.
8
Published as a conference paper at ICLR 2020
Acknowledgement
This research is partially supported by Programmatic grant no. A1687b0033 from the Singapore
government’s Research, Innovation and Enterprise 2020 plan (Advanced Manufacturing and Engi-
neering domain).
Hu Zhang (No. 201706340188) is partially supported by the Chinese Scholarship Council.
Jiashi Feng was partially supported by NUS IDS R-263-000-C67-646, ECRA R-263-000-C87-133,
MOE Tier-II R-263-000-D17-112 and AI.SG R-263-000-D97-490.
9
Published as a conference paper at ICLR 2020
References
W. Brendel, J. Rauber, and M. Bethge. Decision-based adversarial attacks: Reliable attacks against
black-box machine learning models. In International Conference on Learning Representations,
2018. URL https://arxiv.org/abs/1712.04248.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017
IEEE Symposium on Security and Privacy (SP), pp. 39-57. IEEE, 2017.
Jinghui Chen, Jinfeng Yi, and Quanquan Gu. A frank-wolfe framework for efficient and effective
adversarial attacks. arXiv preprint arXiv:1811.10828, 2018.
Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. Zoo: Zeroth order opti-
mization based black-box attacks to deep neural networks without training substitute models. In
Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, pp. 15-26. ACM,
2017.
Minhao Cheng, Thong Le, Pin-Yu Chen, Huan Zhang, JinFeng Yi, and Cho-Jui Hsieh. Query-
efficient hard-label black-box attack: An optimization-based approach. In International Confer-
ence on Learning Representations, 2019. URL https://openreview.net/forum?id=
rJlk6iRqKX.
Riley Edmunds, Noah Golmant, Vinay Ramasesh, Phillip Kuznetsov, Piyush Patil, and Raul Puri.
Transferability of adversarial attacks in model-agnostic meta-learning. Deep Learning and Secu-
rity Workshop (DLSW) in Singapore, 2017.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. In Proceedings of the 34th International Conference on Machine Learning-
Volume 70, pp. 1126-1135. JMLR. org, 2017.
Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochas-
tic programming. SIAM Journal on Optimization, 23(4):2341-2368, 2013.
Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In International Conference on Learning Representations, 2015. URL http://
arxiv.org/abs/1412.6572.
Elad Hazan et al. Introduction to online convex optimization. Foundations and TrendsR in Opti-
mization, 2(3-4):157-325, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. Black-box adversarial attacks with
limited queries and information. In Proceedings of the 35th International Conference on Machine
Learning, ICML 2018, July 2018a. URL https://arxiv.org/abs/1804.08598.
Andrew Ilyas, Logan Engstrom, and Aleksander Madry. Prior convictions: Black-box adversarial
attacks with bandits and priors. arXiv preprint arXiv:1807.07978, 2018b.
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance
reduction. In Advances in neural information processing systems, pp. 315-323, 2013.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Tech-
nical report, Citeseer, 2009.
Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. To-
wards deep learning models resistant to adversarial attacks. In International Conference on Learn-
ing Representations, 2018. URL https://openreview.net/forum?id=rJzIBfZAb.
10
Published as a conference paper at ICLR 2020
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and
accurate method to fool deep neural networks. In Proceedings of the IEEE conference on com-
Puter vision and pattern recognition, pp. 2574-2582, 2016.
Nina Narodytska and Shiva Kasiviswanathan. Simple black-box adversarial attacks on deep neural
networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops
(CVPRW), pp. 1310-1318. IEEE, 2017.
Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer
Science & Business Media, 2013.
Yurii Nesterov and Vladimir Spokoiny. Random gradient-free minimization of convex functions.
Foundations of Computational Mathematics, 17(2):527-566, 2017.
Alex Nichol, Joshua Achiam, and John Schulman. On first-order meta-learning algorithms. arXiv
preprint arXiv:1803.02999, 2018.
Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram
Swami. Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM
on Asia conference on computer and communications security, pp. 506-519. ACM, 2017.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.
ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision
(IJCV), 115(3):211-252, 2015. doi: 10.1007/s11263-015-0816-y.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfel-
low, and Rob Fergus. Intriguing properties of neural networks. In International Conference on
Learning Representations, 2014. URL http://arxiv.org/abs/1312.6199.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1-9, 2015.
Chun-Chen Tu, Paishun Ting, Pin-Yu Chen, Sijia Liu, Huan Zhang, Jinfeng Yi, Cho-Jui Hsieh, and
Shin-Ming Cheng. Autozoom: Autoencoder-based zeroth order optimization method for attack-
ing black-box neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 33, pp. 742-749, 2019.
Daniel Zgner and Stephan Gnnemann. Adversarial attacks on graph neural networks via meta
learning. In International Conference on Learning Representations, 2019. URL https:
//openreview.net/forum?id=Bylnx209YX.
11
Published as a conference paper at ICLR 2020
6	Appendix
6.1	More experimental results
6.1.1	Cosine similarity between estimated gradients and white-box gradients
To demonstrate the ability of estimating gradient of our meta attacker, we have conducted exper-
iments with the Resnet-34 model on the tiny-Imagenet dataset to compare the cosine similarity
between the estimated gradients from our proposed meta-attacker and the accurate white-box gradi-
ents. We also compare the cosine similarity between ZOO (Chen et al., 2017) estimated gradients
and the white-box gradients as reference. The results of cosine similarity and required number of
queries are shown in Table 3. We can observe that our meta-attacker does fast-adapt to the target
model and generate accurate gradients. Not only it estimates gradients with positive cosine similar-
ity to the true gradients, but it also performs closer to the ZOO estimated results with the same small
standard deviation.
Table 3: Cosine similarity between estimated gradients and white-box gradients.
Ours meta-attaCker	∣ ZOO (Chen et al., 2017)
Task Type	Similarity	Queries	Similarity	Queries
Untargeted	0.356 ± 0.074	3,268	0.395 ± 0.079	25,344
Targeted	0.225 ± 0.090	12,897	0.363 ± 0.069	88,966
6.1.2	Vanilla training autoencoder and meta-attacker learning from
ESTIMATED GRADIENTS
We have ConduCted experiments in seCtion 4.3 to demonstrate the benefits of meta training. However,
we only Compare the performanCe of a meta-trained attaCker with a Gaussian randomly initialized
attaCker. We ConduCt two more experiments on meta training here to further investigate the benefits
of meta training. First we Compare our meta-trained attaCker with a vanilla autoenCoder that learns to
map images to gradients of one white-box model. The vanilla autoenCoder has the same arChiteCture
with our meta-attaCker, but it is trained in one white-box model. Then, we train a new meta-attaCker
in four blaCk-box models, i.e., the gradients used for training are estimated via ZOO (Chen et al.,
2017). The experiment results are presented in Table 4, 5 .
Table 4: MNIST untargeted attaCk Comparison.
Method	Success Rate	Avg. L2	Avg. Queries
Ours(reported)	1.000	-178-	∏Γ03
ZOO estimated	1.000	1.77	1,130
Vanilla AutoenCoder	1.000	1.93	1,899
Initialised attaCker	0.912	1.96	1,721
Table 5: MNIST targeted attaCk Comparison.
Method	Success Rate	Avg. L2	Avg. Queries
Ours(reported)	1.000	-266-	1,971
ZOO estimated	1.000	2.42	2,105
Vanilla Autoencoder	1.000	2.80	2,905
Initialised attacker	0.895	2.81	3,040
The two experiments are ConduCted on 1000 randomly seleCted images from the MNIST testing set.
As for the four approaCh settings: ”Ours (reported)” is the results we report in our paper; ”ZOO esti-
mated” is the meta-attaCker trained from ZOO estimated gradients; ”Vanilla AutoenCoder” is the au-
toenCoder maps image to gradient trained in one different MNIST ClassifiCation model; ”Initialised
attaCker” is the meta-attaCker with randomly initialized weights. We Can see the ”ZOO estimated”
model performs Closer to our reported meta-attaCker. However, the ”Vanilla AutoenCoder” performs
muCh worse (with larger L2-norm and more queries), whiCh performs similarly to randomly initial-
ized meta-attaCker. The two experiments verify the effeCtiveness of our first meta-training phase.
12
Published as a conference paper at ICLR 2020
6.2	Structure of Meta Attacker
Table 6: Structure of meta attacker. Conv: convolutional layer, Convt: de-convolutional layer.
Meta attacker (MNIST)	Meta attacker (CIFAR10, tiny-ImageNet)
Conv(16, 3, 3, 1) + ReLu + bn	Conv(32,3,3, 1) + ReLu +bn
Conv(32, 4, 4, 2) + ReLu + bn	Conv(64, 4, 4, 2) + ReLu + bn
Conv(64, 4, 4, 2) + ReLu + bn	Conv(128, 4, 4, 2) + ReLu + bn
Conv(64, 4, 4, 2) + ReLu + bn	Conv(256, 4, 4, 2) + ReLu + bn
Convt(64, 4, 4, 2) + ReLu + bn	Convt(256, 4, 4, 2) + ReLu + bn
Convt(32, 4, 4, 2) + ReLu + bn	Convt(128, 4, 4, 2) + ReLu + bn
Convt(16, 4, 4, 2) + ReLu + bn	Convt(64, 4, 4, 2) + ReLu + bn
Convt(8, 3, 3, 1) + ReLu + bn	Convt(32, 3, 3, 1) + ReLu +bn
6.3	Structure of Target Model Used in MNIST
Table 7:	NeUraI network architecture Used on MNIST.
MNIST Model (Conv: convolutional layer, FC: fully connected layer.)
Conv(128,3,3) + Tanh
MaxPool(2,2)
Conv(64, 3, 3) + Tanh
MaxPool(2,2)
FC(128) + Relu
FC(10) + Softmax
6.4 Accuracy of Target Models on Original Datasets
Table 8:	Accuracy of each target model on each dataset
DataSet ∣ MNIST ∣ CIFAR10 ∣ tiny-ImageNet
Model I MNIST Model ∣ Resnet18 ∣ VGG19 ∣ Resnet34
Accuracy ∣	0.9911	∣	0.9501	∣ 0.6481 ∣	0.6972
6.5 Adversarial Examples Generated by Our Method
13
Published as a conference paper at ICLR 2020
Target class
0123456789
C7l23q56 39 9
©12345 6 C* 夕 O/
燧IN 3q53。3 9
Ols3q54 qs 9
◎ 1G 3 q 5 4 r* S 9
。\2>4569，O/
⅞J>l23q56 Γ*s 3
0∖23q56。夕 0/
43123 q 6 6 o<s>q
Cl23q56。夕彳
Figure 4: Adversarial examples generated by our method on MNIST. The groundtruth images are
shown in the diagonal and the rest are adversarial examples that are misclassified to the targeted
class shown on the top.
14
Published as a conference paper at ICLR 2020
Target class
Figure 5: Adversarial examples generated by our method on CIFAR10. The groundtruth images
are shown in the diagonal and the rest are adversarial examples that are misclassified to the targeted
class shown on the top.
15