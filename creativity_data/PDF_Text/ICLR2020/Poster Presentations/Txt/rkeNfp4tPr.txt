Published as a conference paper at ICLR 2020
Escaping Saddle Points Faster with
Stochastic Momentum
Jun-Kun Wang, Chi-Heng Lin, & Jacob Abernethy
Georgia Institute of Technology
{jimwang,cl3385,prof}@gatech.edu
Ab stract
Stochastic gradient descent (SGD) with stochastic momentum is popular in non-
convex stochastic optimization and particularly for the training of deep neural
networks. In standard SGD, parameters are updated by improving along the path
of the gradient at the current iterate on a batch of examples, where the addition of
a “momentum” term biases the update in the direction of the previous change in
parameters. In non-stochastic convex optimization one can show that a momentum
adjustment provably reduces convergence time in many settings, yet such results
have been elusive in the stochastic and non-convex settings. At the same time, a
widely-observed empirical phenomenon is that in training deep networks stochas-
tic momentum appears to significantly improve convergence time, variants of it
have flourished in the development of other popular update methods, e.g. ADAM
(Kingma & Ba (2015)), AMSGrad (Reddi et al. (2018b)), etc. Yet theoretical
justification for the use of stochastic momentum has remained a significant open
question. In this paper we propose an answer: stochastic momentum improves
deep network training because it modifies SGD to escape saddle points faster and,
consequently, to more quickly find a second order stationary point. Our theoretical
results also shed light on the related question of how to choose the ideal momentum
Parameter-our analysis suggests that β ∈ [0,1) should be large (close to 1), which
comports with empirical findings. We also provide experimental findings that
further validate these conclusions.
1	Introduction
SGD with stochastic momentum has been a de facto algorithm in nonconvex oPtimization and deeP
learning. It has been widely adoPted for training machine learning models in various aPPlications.
Modern techniques in comPuter vision (e.g.Krizhevsky et al. (2012); He et al. (2016); Cubuk et al.
(2018); Gastaldi (2017)), sPeech recognition (e.g. Amodei et al. (2016)), natural language Processing
(e.g. Vaswani et al. (2017)), and reinforcement learning (e.g. Silver et al. (2017)) use SGD with
stochastic momentum to train models. The advantage of SGD with stochastic momentum has been
widely observed (Hoffer et al. (2017); Loshchilov & Hutter (2019); Wilson et al. (2017)). Sutskever
et al. (2013) demonstrate that training deeP neural nets by SGD with stochastic momentum helPs
achieving in faster convergence comPared with the standard SGD (i.e. without momentum). The
success of momentum makes it a necessary tool for designing new oPtimization algorithms in
oPtimization and deeP learning. For examPle, all the PoPular variants of adaPtive stochastic gradient
methods like Adam (Kingma & Ba (2015)) or AMSGrad (Reddi et al. (2018b)) include the use of
momentum.
DesPite the wide use of stochastic momentum (Algorithm 1) in Practice, 1 justification for the clear
emPirical imProvements has remained elusive, as has any mathematical guidelines for actually setting
the momentum Parameter—it has been observed that large values (e.g. β = 0.9) work well in
Practice. It should be noted that Algorithm 1 is the default momentum-method in PoPular software
1Heavy ball momentum is the default choice of momentum method in PyTorch and Tensorflow, instead
of Nesterov’s momentum. See the manual Pages https://pytorch.org/docs/stable/_modules/
torch/optim/sgd.html and https://www.tensorflow.org/api_docs/python/tf/
keras/optimizers/SGD.
1
Published as a conference paper at ICLR 2020
Algorithm 1: SGD with stochastic heavy ball momentum
1
2
3
4
5
6
7
Required: Step size parameter η and momentum parameter β.
Init: w0 ∈ Rd and m-1 = 0 ∈ Rd.
for t = 0 to T do
Given current iterate wt, obtain stochastic gradient gt := Vf (wt; ξt).
Update stochastic momentum mt := βmt-1 + gt.
Update iterate wt+1 := wt - ηmt .
end for
packages such as PyTorch and Tensorflow. In this paper we provide a theoretical analysis for SGD
with momentum. We identify some mild conditions that guarantees SGD with stochastic momentum
will provably escape saddle points faster than the standard SGD, which provides clear evidence for
the benefit of using stochastic momentum. For stochastic heavy ball momentum, a weighted average
of stochastic gradients at the visited points is maintained. The new update is computed as the current
update minus a step in the direction of the momentum. Our analysis shows that these updates can
amplify a component in an escape direction of the saddle points.
In this paper, we focus on finding a second-order stationary point for smooth non-convex optimization
by SGD with stochastic heavy ball momentum. Specifically, we consider the stochastic nonconvex
optimization problem, mi□w∈Rd f (W) := Eξ~D [f (w; ξ)], where We overload the notation so that
f(w; ξ) represents a stochastic function induced by the randomness ξ while f(w) is the expectation
of the stochastic functions. An (, )-second-order stationary point w satisfies
∣∣Vf(w)k ≤ e and V2f(w)占-eI.	⑴
Obtaining a second order guarantee has emerged as a desired goal in the nonconvex optimization
community. Since finding a global minimum or even a local minimum in general nonconvex
optimization can be NP hard (Anandkumar & Ge (2016); Nie (2015); Murty & Kabadi (1987);
Nesterov (2000)), most of the papers in nonconvex optimization target at reaching an approximate
second-order stationary point with additional assumptions like Lipschitzness in the gradients and the
Hessian (e.g. Allen-Zhu & Li (2018); Carmon & Duchi (2018); Curtis et al. (2017); Daneshmand
et al. (2018); Du et al. (2017); Fang et al. (2018; 2019); Ge et al. (2015); Jin et al. (2017; 2019);
Kohler & Lucchi (2017); Lei et al. (2017); Lee et al. (2019); Levy (2016); Mokhtari et al. (2018);
Nesterov & Polyak (2006); Reddi et al. (2018a); Staib et al. (2019); Tripuraneni et al. (2018); Xu
et al. (2018)). 2 We follow these related works for the goal and aim at showing the benefit of the use
of the momentum in reaching an (, )-second-order stationary point.
We introduce a required condition, akin to a model assumption made in (Daneshmand et al. (2018)),
that ensures the dynamic procedure in Algorithm 2 produces updates with suitable correlation with
the negative curvature directions of the function f .
Definition 1. Assume, at some time t, that the Hessian Ht = V2f(wt) has some eigenvalue smaller
than - and ∣Vf (wt)∣ ≤ . Let vt be the eigenvector corresponding to the smallest eigenvalue of
V2f(wt). The stochastic momentum mt satisfies Correlated Negative Curvature (CNC) at t with
parameter γ > 0 if
Et[hmt, vti2] ≥ γ.	(2)
As we will show, the recursive dynamics of SGD with heavy ball momentum helps in amplifying the
escape signal γ, which allows it to escape saddle points faster.
Contribution: We show that, under CNC assumption and some minor constraints that upper-bound
parameter β, if SGD with momentum has properties called Almost Positively Aligned with Gradient
(APAG), Almost Positively Correlated with Gradient (APCG), and Gradient Alignment or Curvature
Exploitation (GrACE), defined in the later section, then it takes T = O((1 一 β)log(1∕(1 — β)e)e-10)
iterations to return an (e, E) second order stationary point. Alternatively, one can obtain an (e, √e)
second order stationary point in T = O((1 - β) log(1/(1 - β))-5) iterations. Our theoretical
result demonstrates that a larger momentum parameter β can help in escaping saddle points faster.
As saddle points are pervasive in the loss landscape of optimization and deep learning (Dauphin et al.
2We apologize that the list is far from exhaustive.
2
Published as a conference paper at ICLR 2020
(2014); Choromanska et al. (2015)), the result sheds light on explaining why SGD with momentum
enables training faster in optimization and deep learning.
Notation: In this paper We use Et[∙] to represent conditional expectation E[∙∣wι, w2,..., wt], which
is about fixing the randomness upto but not including t and notice that wt was determined at t - 1.
2	Background
2.1	A thought experiment.
Let us provide some high-level intuition about the benefit
of stochastic momentum with respect to escaping saddle
points. In an iterative update scheme, at some time t0 the
parameters wt0 can enter a saddle point region, that is a
place where Hessian V2f (wt°) has a non-trivial negative
eigenvalue, say λmin(V2f (wt0)) ≤ —e, and the gradient
Vf (wt0) is small in norm, say kVf (wt0)k ≤ . The
challenge here is that gradient updates may drift only very
slowly away from the saddle point, and may not escape
this region; see (Du et al. (2017); Lee et al. (2019)) for
additional details. On the other hand, if the iterates were
Figure 1: The trajectory of the standard
SGD (left) and SGD with momentum (right).
to move in one particular direction, namely along vt0 the
direction of the smallest eigenvector of V2f(wt0), then a
fast escape is guaranteed under certain constraints on the
step size η; see e.g. (Carmon et al. (2018)). While the negative eigenvector could be computed
directly, this 2nd-order method is prohibitively expensive and hence we typically aim to rely on
gradient methods. With this in mind, Daneshmand et al. (2018), who study non-momentum SGD,
make an assumption akin to our CNC property described above that each stochastic gradient gt0 is
strongly non-orthogonal to vt0 the direction of large negative curvature. This suffices to drive the
updates out of the saddle point region.
In the present paper we study stochastic momentum, and our CNC property requires that the update
direction mt0 is strongly non-orthogonal to vt0; more precisely, Et0 [hmt0, vt0i2] ≥ γ > 0. We are
able to take advantage of the analysis of (Daneshmand et al. (2018)) to establish that updates begin
to escape a saddle point region for similar reasons. Further, this effect is amplified in successive
iterations through the momentum update when β is close to 1. Assume that at some wt0 we have mt0
which possesses significant correlation with the negative curvature direction vt0, then on successive
rounds mt0+1 is quite close to βmt0, mt0+2 is quite close to β2mt0, and so forth; see Figure 1 for an
example. This provides an intuitive perspective on how momentum might help accelerate the escape
process. Yet one might ask does this procedure provably contribute to the escape process and, if so,
what is the aggregate performance improvement of the momentum? We answer the first question in
the affirmative, and we answer the second question essentially by showing that momentum can help
speed up saddle-point escape by a multiplicative factor of 1 — β . On the negative side, we also show
that β is constrained and may not be chosen arbitrarily close to 1.
2.2	Momentum Helps Escape Saddle Points: an Empirical View
Let us now establish, empirically, the clear benefit of stochastic momentum on the problem of
saddle-point escape. We construct two stochastic optimization tasks, and each exhibits at least one
significant saddle point. The two objectives are as follows.
minf(w)	:= 1 X (1 w>Hw + b>w + IlwI110),	⑶
w	n2
i=1
mwnf (w)	：= n X ((a>w)2 — y)2∙	⑷
n i=1
Problem (3) of these was considered by (Staib et al. (2019); Reddi et al. (2018a)) and represents a very
straightforward non-convex optimization challenge, with an embedded saddle given by the matrix
3
Published as a conference paper at ICLR 2020
(a) Solving objective (3).
Figure 2: Performance of SGD with different values of β = {0, 0.3, 0.5, 0.7, 0.9}; β = 0 corresponds to the
standard SGD. Fig. 4a: We plot convergence in function value f (∙) given in (3). Initialization is always set as
w0 = 0. All the algorithms use the same step size η = 5 × 10-5 . Fig. 4b: We plot convergence in relative
distance to the true model w*, defined as min(∣∣wt — w*k, ∣∣wt + w*k)∕kw*k, which more appropriately
captures progress as the global sign of the objective (4) is unrecoverable. All the algorithms are initialized at the
same point wo 〜N(0, Id/(10000d)) and use the same step size η = 5 X 10-4.
(b) Solving objective (4). (phase retrieval)
H := diag([1, -0.1]), and stochastic gaussian perturbations given by b 〜N(0, diag([0.1,0.001]));
the small variance in the second component provides lower noise in the escape direction. Here we
have set n = 10. Observe that the origin is in the neighborhood of saddle points and has objective
value zero. SGD and SGD with momentum are initialized at the origin in the experiment so that
they have to escape saddle points before the convergence. The second objective (4) appears in
the phase retrieval problem, that has real applications in physical sciences (CandeS et al. (2013);
Shechtman et al. (2015)). In phase retrieval3, one wants to find an unknown w* ∈ Rd with access
to but a few samples yi = (a>w*)2; the design vector a% is known a priori. Here We have sampled
w* 〜N(0, Id/d) and 电〜N(0, Id) with d =10 and n = 200.
The empirical findings, displayed in Figure 2, are quite stark: for both objectives, convergence
is significantly accelerated by larger choices of β. In the first objective (Figure 4a), we see each
optimization trajectory entering a saddle point region, apparent from the “flat” progress, yet we
observe that large-momentum trajectories escape the saddle much more quickly than those with
smaller momentum. A similar affect appears in Figure 4b. To the best of our knowledge, this is the
first reported empirical finding that establishes the dramatic speed up of stochastic momentum for
finding an optimal solution in phase retrieval.
2.3	Related works.
Heavy ball method: The heavy ball method was originally proposed by Polyak (1964). It has been
observed that this algorithm, even in the deterministic setting, provides no convergence speedup over
standard gradient descent, except in some highly structure cases such as convex quadratic objectives
where an “accelerated” rate is possible (Lessard et al. (2016); Goh (2017); Ghadimi et al. (2015); Sun
et al. (2019); Loizou & Richtarik (2017; 2018); Gadat et al. (2016); Yang et al. (2018); Kidambi et al.
(2018); Can et al. (2019)). We provide a comprehensive survey of the related works about heavy ball
method in Appendix A.
Reaching a second order stationary point: As we mentioned earlier, there are many works aim at
reaching a second order stationary point. We classify them into two categories: specialized algorithms
and simple GD/SGD variants. Specialized algorithms are those designed to exploit the negative
curvature explicitly and escape saddle points faster than the ones without the explicit exploitation
(e.g. Carmon et al. (2018); Agarwal et al. (2017); Allen-Zhu & Li (2018); Xu et al. (2018)). Simple
GD/SGD variants are those with minimal tweaks of standard GD/SGD or their variants (e.g. Ge et al.
(2015); Levy (2016); Fang et al. (2019); Jin et al. (2017; 2018; 2019); Daneshmand et al. (2018);
3It is known that phase retrieval is nonconvex and has the so-called strict saddle property: (1) every local
minimizer {w*, —w*} is global up to phase, (2) each saddle exhibits negative curvature (see e.g. (Sun et al.
(2015; 2016); Chen et al. (2018)))
4
Published as a conference paper at ICLR 2020
Staib et al. (2019)). Our work belongs to this category. In this category, perhaps the pioneer works
are (Ge et al. (2015)) and (Jin et al. (2017)). Jin et al. (2017) show that explicitly adding isotropic
noise in each iteration guarantees that GD escapes saddle points and finds a second order stationary
point with high probability. Following (Jin et al. (2017)), Daneshmand et al. (2018) assume that
stochastic gradient inherently has a component to escape. Specifically, they make assumption of the
Correlated Negative Curvature (CNC) for stochastic gradient gt so that Et[hgt, vti2] ≥ γ > 0. The
assumption allows the algorithm to avoid the procedure of perturbing the updates by adding isotropic
noise. Our work is motivated by (Daneshmand et al. (2018)) but assumes CNC for the stochastic
momentum mt instead. In Appendix A, we compare the results of our work with the related works.
3 Main Results
We assume that the gradient Vf is L-Lipschitz; that is, f is L-Smooth. Further, We assume that the
Hessian V2f is ρ-Lipschitz. These two properties ensure that kVf (W) -Vf (w0)k ≤ Lkw 一 w0k and
thatkV2f(w)-V2f(w0)k ≤ ρkw - w0k,∀w, w0 . The L-Lipschitz gradient assumption implies that
|f(w0)-f(w)-hVf(w),w0-wi∣ ≤ LLkw-w0k2,∀w,w0,whiletheρ-LipschitzHessianassumption
implies that |f (w0) — f (w) 一(Vf (w), w0-w〉一 (w0-w)> V2f (w)(w0-w)| ≤ P∣∣w-w0k3, ∀w, w0.
Furthermore, we assume that the stochastic gradient has bounded noise kVf(w) - Vf(w; ξ)k2 ≤ σ2
and that the norm of stochastic momentum is bounded so that kmt k ≤ cm. We denote ΠiMi as the
matrix product of matrices {Mi} and we use σmaχ(M) = ∣∣M∣∣2 := supχ=° hχχMχi to denote the
spectral norm of the matrix M .
3.1	Required Properties with Empirical Validation
Our analysis of stochastic momentum relies on three properties of the stochastic momentum dynamic.
These properties are somewhat unusual, but we argue they should hold in natural settings, and later
we aim to demonstrate that they hold empirically in a couple of standard problems of interest.
Definition 2. We say that SGD with stochastic momentum satisfies Almost Positively Aligned with
Gradient (APAG) 4 if we have
EtKVf(Wt),mt -gti] ≥ -1 kVf(Wt)k2.
(5)
We say that SGD with stochastic momentum satisfies Almost Positively Correlated with Gradient
(APCG) with parameter τ if ∃c0 > 0 such that,
Et[hVf(wt), Mtmti] ≥ -c0ησmax(Mt)∣Vf (wt)∣2,
where the PSD matrix Mt is defined as
(6)
Mt = (∏T-lGs,t )(∏T-1Gs,t)	with Gs,t ：= I-P；=i βs-j V2f(wt) = I - η⅛≠) V2f(wt)
for any integer 1 ≤ k ≤ τ - 1, and η is any step size chosen that guarantees each Gs,t is PSD.
Definition 3. We say that the SGD with momentum exhibits Gradient Alignment or Curvature
Exploitation (GrACE) if ∃ch ≥ 0 such that
Et[η(Vf (wt),gt - mti + Tnm>V2f(wt)mt] ≤ η2ch.
(7)
APAG requires that the momentum term mt must, in expectation, not be significantly misaligned
with the gradient Vf(wt). This is a very natural condition when one sees that the momentum term is
acting as a biased estimate of the gradient of the deterministic f . APAG demands that the bias can
not be too large relative to the size of Vf(wt). Indeed this property is only needed in our analysis
when the gradient is large (i.e. ∣Vf (wt)∣ ≥ ) as it guarantees that the algorithm makes progress;
our analysis does not require APAG holds when gradient is small.
APCG is a related property, but requires that the current momentum term mt is almost positively
correlated with the the gradient Vf (wt), but measured in the Mahalanobis norm induced by Mt. It
4Note that our analysis still go through if one replaces 2 on r.h.s. of (5) with any larger number c < 1; the
resulted iteration complexity would be only a constant multiple worse.
5
Published as a conference paper at ICLR 2020
(a) Gradient norm ∣∣Vf (wt)k.
(b) About APAG.
(c) About APCG.
(d) Gradient norm ∣Vf (wt)∣.
(e) About APAG.
1000	2000	3000	4000
iterations
(f) About APCG.
Figure 3: Plots of the related properties. Sub-figures on the top row are regarding solving (3) and sub-figures on
the bottom row are regarding solving (4) (phase retrieval). Note that the function value/relative distance to w* are
plotted on Figure 2. Above, sub-figures (a) and (d): We plot the gradient norms versus iterations. Sub-figures (b)
and (e): We plot the values ofhVf (wt),mt — gti∕∣Vf (Wt) ∣∣2 versus iterations. For (b), we only report them
when the gradient is large (∣Vf (wt)∣ ≥ 0.02). It shows that the value is large than -0.5 except the transition.
For (e), we observe that the value is almost always nonnegative. Sub-figures (c) and (f): We plot the value of
hVf(wt),Mtmti/ησmaχ(Mt)kVf(wt)k2. For (c), we let Mt = (∏3=105Gs,t)(∏3=105Gs,t) and we only
report the values when the update is in the region of saddle points. For (f), we let Mt = (Πs5=001Gs,t)(Πs5=001Gs,t)
and we observe that the value is almost always nonnegative. The figures implies that SGD with momentum has
APAG and APCG properties in the experiments. Furthermore, an interesting observation is that, for the phase
retrieval problem, the expected values might actually be nonnegative.
may appear to be an unusual object, but one can view the PSD matrix Mt as measuring something
about the local curvature of the function with respect to the trajectory of the SGD with momentum
dynamic. We will show that this property holds empirically on two natural problems for a reasonable
constant c0 . APCG is only needed in our analysis when the update is in a saddle region with significant
negative curvature, kVf (w)k ≤ e and λmin(V2f (W)) ≤ —e. Our analysis does not require APCG
holds when the gradient is large or the update is at an (, )-second order stationary point.
For GrACE, the first term on l.h.s of (7) measures the alignment between stochastic momentum
mt and the gradient Vf(wt), while the second term on l.h.s measures the curvature exploitation.
The first term is small (or even negative) when the stochastic momentum mt is aligned with the
gradient Vf(wt), while the second term is small (or even negative) when the stochastic momentum
mt can exploit a negative curvature (i.e. the subspace of eigenvectors that corresponds to the
negative eigenvalues of the Hessian V2f(wt) if exists). Overall, a small sum of the two terms (and,
consequently, a small ch) allows one to bound the function value of the next iterate (see Lemma 8).
On Figure 3, we report some quantities related to APAG and APCG as well as the gradient norm
when solving the previously discussed problems (3) and (4) using SGD with momentum. We also
report a quantity regarding GrACE on Figure 4 in the appendix.
3.2	Convergence results
The high level idea of our analysis follows as a similar template to (Jin et al. (2017); Daneshmand et al.
(2018); Staib et al. (2019)). Our proof is structured into three cases: either (a) kVf (w)k ≥ , or (b)
kVf (w)k ≤ and λmin(V2f (w)) ≤ —, or otherwise (c) kVf (w)k ≤ and λmin(V2f (w)) ≥ —,
6
Published as a conference paper at ICLR 2020
Algorithm 2: SGD with stochastic heavy ball momentum
1:	Required: Step size parameters r and η, momentum parameter β, and period parameter Tthred .
2:	Init: w0 ∈ Rd and m-1 = 0 ∈ Rd.
3:	for t = 0 to T do
4:	Get stochastic gradient gt at wt, and set stochastic momentum mt := βmt-1 + gt.
5:	Set learning rate: η := η unless (t mod Tthred) = 0 in which case η := r
6:	wt+ι = wt - ηmt.
7:	end for
meaning we have arrived in a second-order stationary region. The precise algorithm we analyze is
Algorithm 2, which identical to Algorithm 1 except that we boost the step size to a larger value r on
occasion. We will show that the algorithm makes progress in cases (a) and (b). In case (c), when the
goal has already been met, further execution of the algorithm only weakly hurts progress. Ultimately,
we prove that a second order stationary point is arrived at with high probability. While our proof
borrows tools from (Daneshmand et al. (2018); Staib et al. (2019)), much of the momentum analysis
is entirely novel to our knowledge.
Theorem 1. Assume that the stochastic momentum satisfies CNC. Set 5 r = O(2), η = O (5), and
T,ι 7 —— C(I___β) lotʃ( Lcmσ_Pc Ch ) —— O((1 _ β) locc( Lcmσ_Pc Ch )f-6) for some constant c > 0 If
Tthred = ηe log( (1-β)δγe ) = O((I β) log( (1-β)δγe )fc ) Jor some Constant c > 0. I
SGD with momentum (Algorithm 2) has APAG property when gradient is large (∣∣Vf (w)k ≥ E),
APCGTthred property when it enters a region of saddle points that exhibits a negative curvature
(∣Vf (w)∣ ≤ E and λmin(V2f (w)) ≤ -E), and GrACE property throughout the iterations, then it
reaches an (e, E) second order stationary point in T = 2Tthred(f (wo) — mi□w f (W)) / (δ Fthred)=
O((1 — β) log( L(Cm-β)ρδcc )e-10) iterations with high probability 1 — δ, where Fthred = O(e4).
The theorem implies the advantage of using stochastic momentum for SGD. Higher β leads to
reaching a second order stationary point faster. As we will show in the following, this is due to that
higher β enables escaping the saddle points faster. In Subsection 3.2.1, we provide some key details
of the proof of Theorem 1. The interested reader can read a high-level sketch of the proof, as well as
the detailed version, in Appendix G.
Remark 1: (constraints on β) We also need some minor constraints on β so that β cannot be too
close to 1. They are 1) L(1 - β)3 > 1, 2) σ2(1 - β)3 > 1, 3) C(1 - β)2 > 1, 4) η ≤ 1-β, 5)
η ≤ ~~, and 6) Tthred ≥ 1 + 12ββ. Please see Appendix E.1 for the details and discussions.
Remark 2: (escaping saddle points) Note that Algorithm 2 reduces to CNC-SGD of Daneshmand
et al. (2018) when β = 0 (i.e. without momentum). Therefore, let us compare the results. We show
that the escape time of Algorithm 2 is Tthred ：= O( (1-β)) (see Appendix E.3.3, especially (81-82)).
On the other hand, for CNC-SGD, based on Table 3 in their paper, is Tthred = O(能).One can
clearly see that Tthred of our result has a dependency 1 - β, which makes it smaller than that of
Daneshmand et al. (2018) for any same η and consequently demonstrates escaping saddle point faster
with momentum.
Remark 3: (finding a second order stationary point) Denote ` a number such that ∀t, kgt k ≤ `. In
Appendix G.3, we show that in the high momentum regime where (1 - β) << P '2 0, Algorithm 2
cmchc
is strictly better than CNC-SGD of Daneshmand et al. (2018), which means that a higher momentum
can help find a second order stationary point faster. Empirically, we find out that c0 ≈ 0 (Figure 3)
and ch ≈ 0 (Figure 4) in the phase retrieval problem, so the condition is easily satisfied for a wide
range of β .
3.2.1 Escaping saddle points
In this subsection, we analyze the process of escaping saddle points by SGD with momentum. Denote
t0 any time such that (t0 mod Tthred) = 0. Suppose that it enters the region exhibiting a small
5See Table 3 in Appendix E for the precise expressions of the parameters. Here, we hide the parameters’
dependencies on γ, L, cm, c0, σ2, ρ, ch, and δ. W.l.o.g, we also assume that cm, L, σ2, c0, ch, and ρ are not less
than one and ≤ 1.
7
Published as a conference paper at ICLR 2020
gradient but a large negative eigenvalue of the Hessian (i.e. kVf (wt0 )k ≤ E and λmin(V2f (wt0)) ≤
-). We want to show that it takes at most Tthred iterations to escape the region and whenever it
escapes, the function value decreases at least by Fthred = O(E4) on expectation, where the precise
expression of Fthred will be determined later in Appendix E. The technique that we use is proving
by contradiction. Assume that the function value on expectation does not decrease at least Fthred
in Tthred iterations. Then, we get an upper bound of the expected distance Et0 [kwt0+Tthred -
wt0 k2] ≤ Cupper. Yet, by leveraging the negative curvature, we also show a lower bound of the
form Et0 [kwt0+Tthred - wt0 k2] ≥ Clower. The analysis will show that the lower bound is larger
than the upper bound (namely, Clower > Cupper), which leads to the contradiction and concludes
that the function value must decrease at least Fthred in Tthred iterations on expectation. Since
Tthred = O((1 — β)log( (i-β)e )e6), the dependency on β suggests that larger β can leads to smaller
Tthred, which implies that larger momentum helps in escaping saddle points faster.
Lemma 1 below provides an upper bound of the expected distance. The proof is in Appendix C.
Lemma 1. Denote t0 any time such that (t0 mod Tthred) = 0. Suppose that Et0 [f (wt0 ) -
f(wt0+t)] ≤ Fthred for any 0	≤	t ≤	Tthred. Then,	Et0	[kwt0+t	- wt0 k	] ≤ Cupper,t	:=
MtlFthred+2r2 ch + 3 r3cm) + 8 2	tσ2	+ 4	2( ɪ ∖2 2 +	2 2	2
(1-β)2	+8η (1-β)2 +4η 1-ββ) cm + 2r cm.
We see that Cupper,t in Lemma 1 is monotone increasing with t, so we can define Cupper := Cupper,Tthred.
Now let us switch to obtaining the lower bound of Et0 [kwt0+Tthred - wt0 k2]. The key to get the
lower bound comes from the recursive dynamics of SGD with momentum.
Lemma 2. Denote t0 any time such that (t0 mod Tthred) = 0. Let us define a quadratic ap-
Proximation at wt0, Q(W) := f (wt0) +hw — wto, Vf (wt0 ))+ 11 (w 一 wt° )>H(w 一 wt°), where
H := V2f(wt0). Also, define Gs := (I - η sk=1 βs-kH). Then we can write wt0+t - wt0 exactly
using the following decomPosition.
qm,t-1
qv,t- 1	~^^^^^^^}^^^^^^^~^^^^^^^{
z--------λ----------{	t-1
(∏j=lGj)( — rmto) +η (T) E (∏j=S+ιGj)βsmt.
s=1
t-1
qq,t-1
ʌ
s
+ η (T)E (∏j=S+ιGj) ∑βs-k(Vf (wto+k) -VQ(wto+s))
s=1	k=1
qw,t-1
Z 一	人
r
t-1	s
qξ,t-1
Z---------------------A--------
t-1	s
+ η (-1) E	(∏j=S+ιGj)	Ees-kVf (wt0)+η (-1)	E	(∏j=S+ιGj)	Eeiξt0+k	.
s=1	k=1	s=1	k=1
The proof of Lemma 2 is in Appendix D. Furthermore, we will use the quantities
qv,t-1, qm,t-1, qq,t-1, qw,t-1, qξ,t-1 as defined above throughout the analysis.
Lemma 3. Following the notations of Lemma 2, we have that
Eto [k wto + t一wto Il ] ≥ Eto [kqv,t—1 k ]+2ηEto [hqv,t—1, qm,t—1+qq,t—1+qw,t—1 +qξ,t-1i] =: Clower.
We are going to show that the dominant term in the lower bound of Et0 [kwt0+t 一 wt0 k2] is
Eto [Iqv,t—1 I2], which is the critical component for ensuring that the lower bound is larger than
the upper bound of the expected distance.
Lemma 4. Denote θj := Pk=I βj~k = Pk=I ek-1 and λ := —λmin(H). Following the Condi-
tions and notations in Lemma 1 and Lemma 2, we have that
Eto [kqv,t-ιk2] ≥ (Πj=1(1+ ηθjλ))2r2γ.	(8)
Proof. We know that λma (H) ≤ —e < 0. Let V be the eigenvector of the Hessian H with unit norm
that corresponds to λmin(H) so that Hv = λmin(H)v. We have (I 一 ηH)v = v 一 ηλmin(H)v =
8
Published as a conference paper at ICLR 2020
(1 - ηλmin(H))v. Then,
Eto[kqv,t-ik2] =) Eto [kqv,t-i k2kvk2] ≥) Eto [hqv,t-i, vi2] = Eto [h(∏j=1Gj'VmtO,vi2]
=) EtOK (πj=1 (I-ηθjH ArmtO ,vi2] = EtO h(πj = 1(1 - ηθj λmin(H )))rmto , vi2]	⑼
(e)
≥ (Πj=1(i + ηθjλ))2r2γ,
where (a) is because V is with unit norm, (b) is by CaUchy-SchWarz inequality, (c), (d) are by the
definitions, and (e) is by the CNC assumption so that Eto [hmto, vi2] ≥ Y.	口
Observe that the lower bound in (8) is monotone increasing with t and the momentum parameter
β . Moreover, it actually grows exponentially in t. To get the contradiction, we have to show that
the lower bound is larger than the upper bound. By Lemma 1 and Lemma 3, it suffices to prove the
following lemma. We provide its proof in Appendix E.
Lemma 5. Let Fthred = O(4) and η2 Tthred ≤ r2. By following the conditions and notations in
Theorem 1, Lemma 1 and Lemma 2, we conclude that if SGD with momentum (Algorithm 2) has the
APCG property, then we have that Clower := EtO[kqv,Tthred-1k2]+2ηEtO[hqv,Tthred-1, qm,Tthred-1 +
qq,Tthred-1 + qw,Tthred-1 + qξ,Tthred-1i] > Cupper.
4 Conclusion
In this paper, we identify three properties that guarantee SGD with momentum in reaching a second-
order stationary point faster by a higher momentum, which justifies the practice of using a large value
of momentum parameter β . We show that a greater momentum leads to escaping strict saddle points
faster due to that SGD with momentum recursively enlarges the projection to an escape direction.
However, how to make sure that SGD with momentum has the three properties is not very clear. It
would be interesting to identify conditions that guarantee SGD with momentum to have the properties.
Perhaps a good starting point is understanding why the properties hold in phase retrieval. We believe
that our results shed light on understanding the recent success of SGD with momentum in non-convex
optimization and deep learning.
Acknowledgments
We gratefully acknowledge financial support from NSF IIS awards 1910077 and 1453304.
References
Naman Agarwal, Zeyuan Allen-Zhu, Brian Bullins, Elad Hazan, and Tengyu Ma. Finding approximate
local minima faster than gradient descent. STOC, 2017.
Zeyuan Allen-Zhu and Yuanzhi Li. Neon2: Finding local minima via first-order oracles. NeurIPS,
2018.
Dario Amodei, Sundaram Ananthanarayanan, Rishita Anubhai, and et al. Deep speech 2 : End-to-end
speech recognition in english and mandarin. ICML, 2016.
Anima Anandkumar and Rong Ge. Efficient approaches for escaping higher order saddle points in
non-convex optimization. COLT, 2016.
Bugra Can, Mert Gurbuzbalaban, and Lingjiong Zhu. Accelerated linear convergence of stochastic
momentum methods in wasserstein distances. ICML, 2019.
Emmanuel J. Candes, Yonina Eldar, Thomas Strohmer, and Vlad Voroninski. Phase retrieval via
matrix completion. SIAM Journal on Imaging Sciences, 2013.
Yair Carmon and John C. Duchi. Gradient descent efficiently finds the cubic-regularized non-convex
newton step. NeurIPS, 2018.
9
Published as a conference paper at ICLR 2020
Yair Carmon, John Duchi, Oliver Hinder, and Aaron Sidford. Accelerated methods for nonconvex
optimization. SIAM Journal of Optimization, 2018.
Yuxin Chen, Yuejie Chi, Jianqing Fan, Cong Ma, and Yuling Yan. Gradient descent with random
initialization: Fast global convergence for nonconvex phase retrieval. Mathematical Programming,
2018.
Anna Choromanska, Mikael Henaff, Michael Mathieu, Gerard Ben Arous, and Yann LeCun. The
loss surfaces of multilayer networks. AISTAT, 2015.
Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment:
Learning augmentation policies from data. arXiv:1805.09501, 2018.
Frank E. Curtis, Daniel P. Robinson, and Mohammadreza Samadi. A trust region algorithm with a
worst-case iteration complexity of o(-3/2 ) for nonconvex optimization. Mathematical Program-
ming, 2017.
Hadi Daneshmand, Jonas Kohler, Aurelien Lucchi, and Thomas Hofmann. Escaping saddles with
stochastic gradients. ICML, 2018.
Yann Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua
Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex
optimization. NIPS, 2014.
Simon S. Du, Chi Jin, Jason D. Lee, Michael I. Jordan, Barnabas Poczos, and Aarti Singh. Gradient
descent can take exponential time to escape saddle points. NIPS, 2017.
Cong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang. Spider: Near-optimal non-convex
optimization via stochastic path-integrated differential estimator. NeurIPS, 2018.
Cong Fang, Zhouchen Lin, and Tong Zhang. Sharp analysis for nonconvex sgd escaping from saddle
points. COLT, 2019.
Sebastien Gadat, Fabien Panloup, and Sofiane Saadane. Stochastic heavy ball. arXiv:1609.04228,
2016.
Xavier Gastaldi. Shake-shake regularization. arXiv:1705.07485, 2017.
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points — online stochastic
gradient for tensor decomposition. COLT, 2015.
Euhanna Ghadimi, Hamid Reza Feyzmahdavian, and Mikael Johansson. Global convergence of the
heavy-ball method for convex optimization. ECC, 2015.
Saeed Ghadimi and Guanghui Lan. Stochastic first- and zeroth-order methods for nonconvex
stochastic programming. SIAM Journal on Optimization, 2013.
Saeed Ghadimi and Guanghui Lan. Accelerated gradient methods for nonconvex nonlinear and
stochastic programming. Mathematical Programming, 2016.
Gabriel Goh. Why momentum really works. Distill, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. Conference on Computer Vision and Pattern Recognition (CVPR), 2016.
Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the generaliza-
tion gap in large batch training of neural networks. NIPS, 2017.
Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M. Kakade, and Michael I. Jordan. How to escape
saddle points efficiently. ICML, 2017.
Chi Jin, Praneeth Netrapalli, and Michael I. Jordan. Accelerated gradient descent escapes saddle
points faster than gradient descent. COLT, 2018.
10
Published as a conference paper at ICLR 2020
Chi Jin, Praneeth Netrapalli, Rong Ge, Sham M. Kakade, and Michael I. Jordan. Stochastic gradient
descent escapes saddle points efficiently. arXiv:1902.04811, 2019.
Rahul Kidambi, Praneeth Netrapalli, Prateek Jain, and Sham M. Kakade. On the insufficiency of
existing momentum schemes for stochastic optimization. ICLR, 2018.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. ICLR, 2015.
Jonas Moritz Kohler and Aurelien Lucchi. Sub-sampled cubic regularization for non-convex opti-
mization. ICML, 2017.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolu-
tional neural networks. NIPS, 2012.
Jason D. Lee, Ioannis Panageas, Georgios Piliouras, Max Simchowitz, Michael I. Jordan, and
Benjamin Recht. First-order methods almost always avoid strict saddle-points. Mathematical
Programming, Series B, 2019.
Lihua Lei, Cheng Ju, Jianbo Chen, and Michael I. Jordan. Nonconvex finite-sum optimization via
scsg methods. NIPS, 2017.
Laurent Lessard, Benjamin Recht, and Andrew Packard. Analysis and design of optimization
algorithms via integral quadratic constraints. SIAM Journal on Optimization, 2016.
Kfir Y. Levy. The power of normalization: Faster evasion of saddle points. arXiv:1611.04831, 2016.
Nicolas Loizou and Peter Richtarik. Momentum and stochastic momentum for stochastic gradient,
newton, proximal point and subspace descent methods. arXiv:1712.09677, 2017.
Nicolas Loizou and Peter Richtarik. Accelerated gossip via stochastic heavy ball method. Allerton,
2018.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. ICLR, 2019.
Aryan Mokhtari, Asuman Ozdaglar, and Ali Jadbabaie. Escaping saddle points in constrained
optimization. NeurIPS, 2018.
Katta G Murty and Santosh N Kabadi. Some np-complete problems in quadratic and nonlinear
programming. Mathematical programming, 1987.
Yurii Nesterov. Squared functional systems and optimization problems. High performance optimiza-
tion, Springer, 2000.
Yurii Nesterov. Introductory lectures on convex optimization: a basic course. Springer, 2013.
Yurii Nesterov and B.T. Polyak. Cubic regularization of newton method and its global performance.
Math. Program., Ser A 108, 177-205, 2006.
Jiawang Nie. The hierarchy of local minimums in polynomial optimization. Mathematical program-
ming, 2015.
Peter Ochs, Yunjin Chen, Thomas Brox, and Thomas Pock. ipiano: Inertial proximal algorithm for
nonconvex optimization. SIAM Journal of Imaging Sciences, 2014.
B.T. Polyak. Some methods of speeding up the convergence of iteration methods. USSR Computa-
tional Mathematics and Mathematical Physics, 1964.
Sashank Reddi, Manzil Zaheer, Suvrit Sra, Barnabas Poczos, Francis Bach, Ruslan Salakhutdinov,
and Alex Smola. A generic approach for escaping saddle points. AISTATS, 2018a.
Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. ICLR,
2018b.
Yoav Shechtman, Yonina C. Eldar, Oren Cohen, Henry Nicholas Chapman, Jianwei Miao, and
Mordechai Segev. Phase retrieval with application to optical imaging: a contemporary overview.
IEEE signal processing magazine, 2015.
11
Published as a conference paper at ICLR 2020
David Silver, Julian Schrittwieser, Karen Simonyan, and et al. Mastering the game of go without
human knowledge. Nature, 2017.
Matthew Staib, Sashank J. Reddi, Satyen Kale, Sanjiv Kumar, and Suvrit Sra. Escaping saddle points
with adaptive gradient methods. ICML, 2019.
Ju Sun, Qing Qu, and John Wright. When are nonconvex problems not scary? NIPS Workshop on
Non-convex Optimization for Machine Learning: Theory and Practice, 2015.
Ju Sun, Qing Qu, and John Wright. A geometrical analysis of phase retrieval. International
Symposium on Information Theory, 2016.
Tao Sun, Penghang Yin, Dongsheng Li, Chun Huang, Lei Guan, and Hao Jiang. Non-ergodic
convergence analysis of heavy-ball algorithms. AAAI, 2019.
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization
and momentum in deep learning. ICML, 2013.
T. Tieleman and G. Hinton. Rmsprop: Divide the gradient by a running average of its recent
magnitude. COURSERA: Neural Networks for Machine Learning, 2012.
Nilesh Tripuraneni, Mitchell Stern, Chi Jin, Jeffrey Regier, and Michael I Jordan. Stochastic cubic
regularization for fast nonconvex optimization. NeurIPS, 2018.
Ashish Vaswani, Noam Shazeer, Niki Parmar, and et al. Attention is all you need. NIPS, 2017.
Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nathan Srebro, , and Benjamin Recht. The
marginal value of adaptive gradient methods in machine learning. NIPS, 2017.
Yi Xu, Jing Rong, and Tianbao Yang. First-order stochastic algorithms for escaping from saddle
points in almost linear time. NeurIPS, 2018.
Tianbao Yang, Qihang Lin, and Zhe Li. Unified convergence analysis of stochastic momentum
methods for convex and non-convex optimization. IJCAI, 2018.
A	Literature survey
Heavy ball method: The heavy ball method was originally proposed by Polyak (1964). It has been
observed that this algorithm, even in the deterministic setting, provides no convergence speedup
over standard gradient descent, except in some highly structure cases such as convex quadratic
objectives where an “accelerated” rate is possible (Lessard et al. (2016); Goh (2017)). In recent years,
some works make some efforts in analyzing heavy ball method for other classes of optimization
problems besides the quadratic functions. For example, Ghadimi et al. (2015) prove an O(1/T)
ergodic convergence rate when the problem is smooth convex, while Sun et al. (2019) provide a
non-ergodic convergence rate for certain classes of convex problems. Ochs et al. (2014) combine the
technique of forward-backward splitting with heavy ball method for a specific class of nonconvex
optimization problem. For stochastic heavy ball method, Loizou & Richtarik (2017) analyze a class
of linear regression problems and shows a linear convergence rate of stochastic momentum, in which
the linear regression problems actually belongs to the case of strongly convex quadratic functions.
Other works includes (Gadat et al. (2016)), which shows almost sure convergence to the critical
points by stochastic heavy ball for general non-convex coercive functions. Yet, the result does not
show any advantage of stochastic heavy ball over other optimization algorithms like SGD. Can et al.
(2019) show an accelerated linear convergence to a stationary distribution under Wasserstein distance
for strongly convex quadratic functions by SGD with stochastic heavy ball momentum. Yang et al.
(2018) provide a unified analysis of stochastic heavy ball momentum and Nesterov’s momentum for
smooth non-convex objective functions. They show that the expected gradient norm converges at rate
O(1/ʌ/t). Yet, the rate is not better than that of the standard SGD. We are also aware of the works
(Ghadimi & Lan (2016; 2013)), which propose some variants of stochastic accelerated algorithms
with first order stationary point guarantees. Yet, the framework in (Ghadimi & Lan (2016; 2013))
does not capture the stochastic heavy ball momentum used in practice. There is also a negative result
about the heavy ball momentum. Kidambi et al. (2018) show that for a specific strongly convex and
12
Published as a conference paper at ICLR 2020
(a) About GrACE for problem (3).
Figure 4: Plot regarding the GrACE property. We plot the values of ηh∙f(Wt),gt-m1+2η mt Htmt versus
iterations. An interesting observation is that the value is well upper-bounded by zero for the phase retrieval
problem. The results imply that the constant ch is indeed small.
iterations
(b) About GrACE for problem (4) (phase re-
trieval).
strongly smooth problem, SGD with heavy ball momentum fails to achieving the best convergence
rate while some algorithms can.
Reaching a second order stationary point: As we mentioned earlier, there are many works aim at
reaching a second order stationary point. We classify them into two categories: specialized algorithms
and simple GD/SGD variants. Specialized algorithms are those designed to exploit the negative
curvature explicitly and escape saddle points faster than the ones without the explicit exploitation
(e.g. Carmon et al. (2018); Agarwal et al. (2017); Allen-Zhu & Li (2018); Xu et al. (2018)). Simple
GD/SGD variants are those with minimal tweaks of standard GD/SGD or their variants (e.g. Ge et al.
(2015); Levy (2016); Fang et al. (2019); Jin et al. (2017; 2018; 2019); Daneshmand et al. (2018);
Staib et al. (2019)). Our work belongs to this category. In this category, perhaps the pioneer works
are (Ge et al. (2015)) and (Jin et al. (2017)). Jin et al. (2017) show that explicitly adding isotropic
noise in each iteration guarantees that GD escapes saddle points and finds a second order stationary
point with high probability. Following (Jin et al. (2017)), Daneshmand et al. (2018) assume that
stochastic gradient inherently has a component to escape. Specifically, they make assumption of the
Correlated Negative Curvature (CNC) for stochastic gradient gt so that Et[hgt, vti2] ≥ γ > 0. The
assumption allows the algorithm to avoid the procedure of perturbing the updates by adding isotropic
noise. Our work is motivated by (Daneshmand et al. (2018)) but assumes CNC for the stochastic
momentum mt instead. Very recently, Jin et al. (2019) consider perturbing the update of SGD and
provide a second order guarantee. Staib et al. (2019) consider a variant of RMSProp (Tieleman
& Hinton (2012)), in which the gradient gt is multiplied by a preconditioning matrix Gt and the
update is wt+1 = wt - Gt-1/2gt. The work shows that the algorithm can help in escaping saddle
points faster compared to the standard SGD under certain conditions. Fang et al. (2019) propose
average-SGD, in which a suffix averaging scheme is conducted for the updates. They also assume an
inherent property of stochastic gradients that allows SGD to escape saddle points.
We summarize the iteration complexity results of the related works for simple SGD variants on
Table 1. 6 The readers can see that the iteration complexity of (Fang et al. (2019)) and (Jin et al.
(2019)) are better than (Daneshmand et al. (2018); Staib et al. (2019)) and our result. So, we want
to explain the results and clarify the differences. First, we focus on explaining why the popular
algorithm, SGD with heavy ball momentum, works well in practice, which is without the suffix
averaging scheme used in (Fang et al. (2019)) and is without the explicit perturbation used in (Jin
et al. (2019)). Specifically, we focus on studying the effect of stochastic heavy ball momentum and
showing the advantage of using it. Furthermore, our analysis framework is built on the work of
(Daneshmand et al. (2018)). We believe that, based on the insight in our work, one can also show
the advantage of stochastic momentum by modifying the assumptions and algorithms in (Fang et al.
(2019)) or (Jin et al. (2019)) and consequently get a better dependency on .
6We follow the work (Daneshmand et al. (2018)) for reaching an (e, e)-stationary point, while some works
are for an (e, √e)-stationary point. We translate them into the complexity of getting an (e, e)-stationary point.
13
Published as a conference paper at ICLR 2020
Algorithm	Complexity
Perturbed SGD (Ge et al. (2015))	O(Ci)	=
Average-SGD (Fang et al. (2019))	^F7)
Perturbed SGD (Jin et al. (2019))	O(e-8)
CNC-SGD (Daneshmand et al.(2018))	O(e-10)
AdaPtive SGD (Staib et al.(2019))-	O(e-10)
SGD+momentum (this work)	O((1- β)log(』)e-10)
Table 1: Iteration complexity to find an (, ) second-order stationary point .
B Lemma 6, 7, and 8
In the following, Lemma 7 says that under the APAG property, when the gradient norm is large, on
expectation SGD with momentum decreases the function value by a constant and consequently makes
progress. On the other hand, Lemma 8 upper-bounds the increase of function value of the next iterate
(if happens) by leveraging the GrACE property.
Lemma 6. If SGD with momentum has the APAG property, then, considering the update step
22
wt+1 = Wt - ηmt, we have that Et[f(wt+ι)] ≤ f(wt) - 2 ∣∣Vf (Wt)Il2 + η2 m .
Proof. By the L-smoothness assumption,
f(wt+ι) ≤ f(wt) — nhVf(wt),mt + Ln-l∣mtk2
Lη2c2
≤f (Wt) — nhvf (Wt), gti — nhvf (Wt), mt — gti +-2-.	(IO)
Taking the expectation on both sides. We have
Et[f(Wt+ι)] ≤ f(Wt) - n∣Vf(Wt)k2 - nEt[hVf(Wt),mt - gti] + Ln2cm
≤ f(Wt) - 2∣Vf(Wt)k2 + Ln2cm.	(11)
where we use the APAG property in the last inequality.
□
2
Lemma 7. Assume that the step size n satisfies n ≤ 甚丁∙ If SGD with momentum has the
m
APAG property, then, considering the update step Wt+1 = Wt - ηmt, we have that Et [f (Wt+1)] ≤
f (Wt) - 4e2 when ∣∣Vf(Wt)∣ ≥ 匕
Lemma 6	r 2 2	∣∣Vf(wt)k≥e
Proof. Et[f(Wt+1)-f(Wt)]	≤	-2∣Vf(Wt)k2 + ⅛m	≤ 一
where the last inequality is due to the constraint of η.
η .2 i Lη2cm W η .2
-2e +	2- ≤ - 4 e，
□
Lemma 8. If SGD with momentum has the GrACE property, then, considering the update step
3
Wt+1 = Wt - nmt, we have that Et[f(Wt+ι)] ≤ f (Wt) + n2ch + ρ6~cmm.
Proof. Consider the update rule Wt+1 = Wt - ηmt, where mt represents the stochastic momentum
and η is the step size. By ρ-Lipschitzness of Hessian, we have f(Wt+1) ≤ f(Wt) - ηhVf(Wt), gti +
nhVf (Wt), gt - mti + η22m>V2 f (Wt)mt + ρη3 IlmtIl3. Taking the conditional expectation, one has
23
Et[f(Wt+i)] ≤ f (Wt)- Et[nllVf(Wt)Il2] + Et[nhVf(Wt),gt - mti + ym>V2f(Wt)mt] + ɪcm.
≤ f (Wt) +0 + n2ch +	cm.
6m
(12)
14
□
Published as a conference paper at ICLR 2020
C Proof of Lemma 1
Lemma 1 Denote	t0	any time such that	(t0	mod	Tthred)	=	0.	Suppose that	Et0	[f (wt0)	-
f(wt0+t)] ≤ Fthred for any 0 ≤ t ≤ Tthred. Then,
Et0 [kwt0+t - wt0 k ] ≤ Cupper,t
8ηt (Ftthred+ 2r2ch + 3r3cm) , q 2 tσ2	2l β x2 2 ɪɔ 2 2
:= ----------D2--------------- +8η	D2	+4η	(l-β)	cm + 2r	cm.
(13)
Proof. Recall that the update is wt0+1 = wt0-rmt0, and wt0+t = wt0+t-1-ηmt0+t-1, for t > 1.
We have that
kwt0+t -wt0 k2 ≤ 2(kwt0+t -wt0+1 k2 + kwt0+1 - wt0 k2) ≤ 2kwt0+t -wt0+1 k2 + 2r2c2m,
(14)
where the first inequality is by the triangle inequality and the second one is due to the assumption
that kmt k ≤ cm for any t. Now let us denote
•	αs := Ptj-=10-s βj
•	At-1 := Pts-=11 αs
and let Us rewrite gt = Vf (wt) + ξt, where ξt is the zero-mean noise. We have that
t-1	t-1 s
Eto[kw⅛o+i - wto+ιk2] = Eto[k X -ηmto+sk2] = Eto[η2k X ((Xβs-jgto+j) + βmto)k2]
s=1	s=1 j=1
t-1 s	t-1
≤Eto[2η2kXXβs-jgto+jk2+2η2kXβsmtok2]
s=1 j=1	s=1
t-1 s
≤ Eto [2η2k XX βs-j gto+j k2] + 2η2(二)2cm
s=1 j=1
t-1	β
=Eto [2η2k Easgto+sk2] + 2η2( iʒ )2 cm
s=1	1- β
t-1	β
=Eto [2η2k Εas(Vf (wto+s) + ξto+s)k2] + 2η2(占)2cm
s=1	1- β
t-1	t-1
≤ Eto[4η2k X as Vf (wto+s)k2] + Eto[4η2k X asξto+sk2] + 2η2 (三)%.
s=1	s=1	1- β
(15)
To proceed, we need to Upper boUnd Eto [4η2 k Pts-=11 asVf(wto+s)k2]. We have that
t-1	(a)	t-1 a
Eto [4η2k 汇 asVf(wto+s )k2] ≤ Et0 [4η2 A2-C£「kVf (wto+s)k2]
s=1	s=1 At-1
≤ Eto [4η2A-1 X kVf (wto+s)k2] ≤ Eto [4η2 —
o 1 - β s=1	o	o	(1 - β)2
t-1
X kVf(wto +s)k2].
s=1
(16)
15
Published as a conference paper at ICLR 2020
where (a) is by Jensen,s inequality, (b) is by maxs αs ≤ i⅛， and (c) is by At-ι ≤ ɪ-e. Now let
us switch to bound the other term.
t-1	t-1	t-1
Eto [4η2k Xαsξto+sk2] = 4η2(Etc, X aiajξ>"to+j]+ % X O⅛>+sξto+s])
s=1	i=j	s=1
=)
t-1
4η2(0 + Eto X α2ξ>+sξto+s]),
s=1
(b)	2 tσ2
≤ 4η (1-βp .
(17)
where (a) is because Et° [ξ>+iξto+j] =0 for i = j, (b) is by that ∣∣ξt∣∣2 ≤ σ2 and maxt αt ≤ 击.
Combining (14), (15), (16), (17),
C	Ct	t-1	C	C tσ2	C , β 、OC	CC
EtO[kwto+t	— wtoll2] ≤	EtO[8η2∩^^2	X l∣v∕(Wto+S)Il2] +	8η2∙^^+4η2(^―^)	Cm	+ 2r	cm.
(i — p)(i — βr	1 — β
J	(18)
Now we need to bound Eh [PS=1 IVf (wto+s)k2]. By using ρ-Lipschitzness of Hessian, we have
that
f (wto + s) ≤ f (WtO +s-1) — η"f (wto + s-1), mto+s-1i + 1 η2m> + s-1V2f (wto + s-1)mto + s-1 + pη3 kmto+s-1 k3∙
2 o	6
(19)
By adding ηhVf (wto+s-ι), gto+s-1〉on both sides, we have
η(Vf (Wto + s-1), gto+s-1i ≤ f (WtO +s-1) — f (wto + s) + ηhvf (wto+s-1), gto+s-1 — mto + s-1i
+1 η2m>+s-ιv2f(wto+s-ι)mto+s-ι+ ρη3kmto+s-ιk3∙
2 o	6
(20)
Taking conditional expectation on both sides leads to
Eto + s-1[η∣∣Vf(Wto + s-1)l∣2] ≤ Eto+s-1[f (wto + s-1) — f (wto + s)] + η2ch + p^cL	(21)
where Eto + s-1[η"f (Wto+s-1)，gto+s-1 — mto+s-1i + 2η2m> + s-iv2f (Wto+s-1)mto+s-1] ≤
η2ch by the GrACE property. We have that for to ≤ to + S — 1
Eto [η∣Vf (Wto+s-i)k2] = Eto [Eto+s-i[η∣Vf (Wt0+s-i)∣∣2]]
(21)	P
≤ EtO [Eto + s-1[f (Wto+s-1) — f(Wto+s)]] + η ch + 6 η3cm
=EtO [f (Wto + s-1) — f (Wto + s )] + η2ch + pη3cm ∙	(22)
Summing the above inequality from S = 2, 3,...，t leads to
t-1
EtO [X ηkvf (Wto+s)k2] ≤ Eto[f(Wto + 1) — f(Wto + t)]+ η2(t — 1)Ch + 6η3 (t — 1)Cm
s = 1
=Eto [f (Wto + 1) — f (Wto ) + f (Wto ) — f (Wto+t)]+ η2(t — 1)Ch + pη3(t — 1)Cm
(a)	p
≤ EtO [f (Wto + 1) — f (Wto )]+ Fthred + η2 (t — 1)Ch + Rη3 (t — 1)。"，
(23)
where (a) is by the assumption (made for proving by contradiction) that Eto [f (Wto) 一 f (w%+s )] ≤
Fthred for any 0 ≤ S ≤ Tthred. By (21) with S = 1 and η = r, we have
EtO [r|Vf(Wto )|2] ≤ EtO [f (Wto ) ― f (Wto + 1)] + r%h + 6 r3 Cm.	(24)
16
Published as a conference paper at ICLR 2020
By (23) and (24), we know that
t-1	t-1
EtoXηkVf(wto+s)k2] ≤ Et0[rkf(wt0)k2]+ EtoXη∣∣Vf(wt0+s)∣∣2]
s=1	s=1
≤ Fthred + r2 ch + ^r,cn + η2tch + g^tcm
6m	6m
≤ Fthred + 2r2ch + Pr3cm + %小
6m 6m
≤ Fthred + 2r2ch + 3『^*,	(25)
where (a) is by the constraint that η2t ≤ r2 for 0 ≤ t ≤ Tthred and (b) is by the constraint that r ≥ η.
By combining (25) and (18)
t	t-1	tσ2	β
EtO[kwto+t — wtok2] ≤ EtO[8η2∩-m2 X kVf(wto+s)k2] + 8η2∩~^m2 +4η2(τj-∑) cm + 2r cm
(1 - β) s=1	(1 - β)	1 - β
≤ 8ηt(Fthred + 2r2ch + Pr3cm) + 8n2 tσ2	+ 4n2( β )2c2 + 2r2c2
≤	(1-β)2	+ η (1- β)2 +4η 4 - 8)cm + 2r cm.
(26)
□
17
Published as a conference paper at ICLR 2020
D Proof of Lemma 2 and Lemma 3
Lemma 2 Denote to any time such that (t0 mod Tthred) = 0∙ Let us define a quadratic ap-
proximation at wt0, Q(W) := f (wt0) + hw — wt0, V/(wt0 )〉+ 1 (W — wt0 )>H(W — wt0), where
H := V2f (wto). Also, define GS := (I — η Pk=I βs-kH) and
•	qv,t-1 := (∏j = 1Gj)( - rmto )∙
•	qm,t-ι ：= 一 ES=I (∏jzS+1Gj)βsmto.
•	qq,t-ι ：= - Es=1 (∏j=S+ιGj) Ek=I βs-k (▽/(wto+k) - VQ(wto+s)).
•	qw,t-ι ：= - Es=1 (∏j=s+1Gj) Ek=I βs-kVf (wto).
•	qξ,t-ι ：= - Es=I (∏t=s+1Gj) Ek=I βs-kξto+k.
Then, wto+t — w% = qv,t-i + ηqm,t-i + ηq%t-i + ηqw,t-i + ηqξ,t-i
Notations:
Denote to any time such that (to mod Tthred) = 0. Let us define a quadratic approximation at wto , Q(W) := f (WtO ) + hw — wto , Vf (WtO )〉+ 2 (W — wto )>H(w — wto ),	(27) where H := V2 f (wt°). Also, we denote S Gs :=(I — η X βs-kH) k=1 vm,s := β mto s vq,s := Xβs-k(Vf (wto+k) — VQ(wto+s)) S Vw,s := X βs-kVf (Wto )	( ) S %,s := X βiξto + k k = 1 S θs := X βs-k. k = 1
Proof. First, We rewrite mto+j for any j ≥ 1 as follows.
j
mto+j = βj mto + ∑βj-k gto+k
j
=βj mto + ^βj-k(Vf(wto+k) + ξto + k ) ∙
(29)
18
Published as a conference paper at ICLR 2020
We have that
Wto+t — Wto
wt0+t-1 - wto - ηmt0+t-1
t-1
=wto+t-1 - WtO- η(βt-1mto + E βt-1-k(V/(wto + k) + ξto + k))
k=1
t-1
=wto + t-1 - WtO- η E βt-1-k VQ(wto + t-1)
k=1
t-1
-n(et-1mto ÷ ^X βt-1-k (V/(wto + k) - VQ(wto+t-1) + ξto + k))
k=1
t-1
=wto + t-1 - wto - η E βt-1-k(H (wto+t-1 - wto )÷ Vf(WtO ))
k=1
t-1
-n(β t-1mto ÷ ^X βt-1-k (Vf(Wto + k ) - VQ(Wto+t-1) ÷ ξto + k ))
k=1
t-1
=(I -n E βt-1-k H )(Wto+t-1 - WtO)
k=1
t-1
-n(β t 1mto ÷ ^X βt 1 k (Vf(Wto + k ) - VQ(Wto+t-1) ÷ Vf(WtO ) ÷ ξto + k)),
k=1
(30)
where (a) is by using (29) with j = t - 1, (b) is by subtracting and adding back the same term, and
(C) is by VQ(Wto+t-1) = Vf(WtO) ÷ H(Wto+t-1 - Wto).
To continue, by using the nations in (28), we can rewrite (30) as
Wto + t - Wto = Gt-1(Wto+t-1 - Wto) - η(Vm,t-1 ÷ Vq,t-1 ÷ ^w,t-1 ÷ Vξ,t-1)∙
Recursively expanding (31) leads to
Wto+t - Wto = Gt-I (Wto+t-1 - Wto) - η(vm,t-1 ÷ vq,t-1 ÷ vw,t-1 ÷ vξ,t-1)
=Gt-I(Gt-2 (Wto+t-2 - Wto ) - η(vm,t-2 ÷ vq,t-2 ÷ vw,t-2 ÷ vξ,t-2))
-η(vm,t-1 ÷ vq,t-1 ÷ vw,t-1 ÷ vξ,t-1)
t-1
=(口；=1 Gj)(Wto+ 1 - Wto ) - n£ (πj=S + 1Gj)(vm,s ÷ vq,s ÷ vw,s ÷ vξ,s )，
s = 1
t-1
=(πj=1Gj)(- rmto)- η E (πj=S+1Gj )(vm,s ÷ vq,s ÷ vw,s ÷ vξ,s),
s=1
(31)
(32)
where (a) we use the notation that ∏j=SGj := GS × Gs+1 ×................Gt-1 and the notation that
Πj=tGj = 1 and (b) is by the update rule. By using the definitions of {q*,t-1} in the lemma
statement, we complete the proof.
□
Lemma 3 Following the notations ofLemma 2, we have that
Eto [kWto + t - Wto k ] ≥ Eto [kqv,t-1 k ] ÷ 2ηEto [hqv,t-1, qm,t-1 ÷ qq,t-1 ÷ qw,t-1 ÷ qξ,t-1i] := Clower
(33)
Proof. Following the proof of Lemma 2, we have
Wto+t - Wto = %,t-1 ÷ η(qm,t-1 ÷ qq,t-1 ÷ qw,t-1 ÷ qξ,t-1)∙	(34)
19
Published as a conference paper at ICLR 2020
Therefore, by using ka + bk2 ≥ kak2 + 2ha, bi,
Et0 [kwt0+t - wt0 k2] ≥ Et0 [kqv,t-1 k2] + 2ηEt0 [hqv,t-1, qm,t-1 + qq,t-1 + qw,t-1 + qξ,t-1 i].
(35)
□
20
Published as a conference paper at ICLR 2020
E Proof of Lemma 5
Lemma 5 Let Fthred = O(4) and η2Tthred ≤ r2. By following the conditions and notations in
Theorem 1, Lemma 1 and Lemma 2, we conclude that if SGD with momentum (Algorithm 2) has the
APCG property, then we have that Clower := Et0[kqv,Tthred-1k2]+2ηEt0[hqv,Tthred-1, qm,Tthred-1 +
qq,Tthred -1 qw,Tthred -1 qξ,Tthred -1	upper .
Table 3: Constraints and choices of the parameters.
Parameter	Value	Constraint origin	constant
r	δγ2 cr	(64), (65), (66)	co c0_	Co _	r*c ——	1 Cr ≤ CmPLσ2Ch, C0 = 1152 ^o— ≤ c' CmPLσ2c0 (1-β)2ch — r 		CO	„— ≤ c 	CmPLσ4(ι-β)3ch — r	
r	”	r ≤ JδF⅛red from (89)	”
η	δ272e5 cη	764)	-C ≤	C1	Cl — CO cη ≤ cmρL2σ2c0Ch , C1 = 24
η	”	η ≤ r∕√Tthred from (25),(39),(87),(89)	”
η	”	n ≤ min{ (IL), (1-β) } from (45), (78)7	”
Fthred	δγ 2 64 CF	(65)	-C口 ≤	C2	CC - -CO- CF ≤ CmP2 Lσ4 Ch , C2 = 576 、	8c0 	CF — CmP	
Fthred	”	-ZZ	2^^ Fthred ≤ ⅞r from (88)		”
Tthred		Tthred ≥ F lθg( ⅞⅞⅛h ) from (82)	
W.l.o.g, we assume that cm, L, σ2, c0, ch, and ρ are not less than one and that ≤ 1.
E.1 SOME CONSTRAINTS ON β.
We require that parameter β is not too close to 1 so that the following holds,
•	1)L(1-β)3 > 1.
•	2) σ2(1 - β)3 > 1.
•	3) c0(1 - β)2 > 1.
•	4) η ≤ 1-β.
•	5) η ≤ T.
c C(1 cΓ1 T、C(I-e) ICc√LCm σ PC Ch、、1 I 2β
•	6) Tthred ≥ ~~^lo lθg( (f)δγe ) ≥ 1 + T-，
The constraints upper-bound the value of β. That is, β cannot be too close to 1. We note that the β
dependence on L, σ, and c0 are only artificial. We use these constraints in our proofs but they are
mostly artefacts of the analysis. For example, if a function is L-smooth, and L < 1, then it is also
1-smooth, so we can assume without loss of generality that L > 1. Similarly, the dependence on σ is
not highly relevant, since we can always increase the variance of the stochastic gradient, for example
by adding an O(1) gaussian perturbation.
21
Published as a conference paper at ICLR 2020
E.2 Some lemmas
To prove Lemma 5, We need a series of lemmas with the choices of parameters on Table 3.
Upper bounding EtCl [|0"-4|]:
Lemma 9. Following the conditions in Lemma 1 and Lemma 2, we have
Etc[gq,T∣∣] ≤(∏j=1 (1+ 汹 λ)) 77β⅛
t(1 - P)
(∏j=1(1 + ηθjλ)) P 8(Fthred + 2r2Ch + 3r3*)
+	1 - β 滔	(1 - P)2
(∏j=1(I + ηθj λ)) ρ(8 (r-β)2 +4η2( ɪ )2cm + 2r2c2n)
+	1 - β	河
Proof.
t— 1	s
Etc[gq,t-ι∣∣] = EtC [k- X(∏j=S+ιGj)Xβs-k(Vf(wto+k) -VQ(wto+s))k]
s=1	k=1
(a)	t—I	二
≤ Eto[£ k(∏j=s+ιG) ∑Ps-k(Vf(Wto+k) -VQ(wto+s))k]
s=1	k=1
(b)	t—1	S
≤ EtoX k(∏j=s+1G)k2k Xβs-k(Vf(wto+k) -VQ(wto+s))k]
s=1	k=1
(c)	t-1	S
≤ Eto [∑ Mnj=S+1G)k2 ∑βs-kk(Vf(wto+k) -VQ(wto+s))k]
s=1	k=1
(d)	t—1	S
≤ Eto [X k(∏j=s+1Gj)k2 X βs-k(∣∣Vf (wto+k) -Vf(wto+s)k + ∣∣Vf(wto+s) -VQ(wto+s)k)]
s=1	k=1
(37)
where (a), (c), (d) is by triangle inequality, (b) isbythefactthat ∣∣Ax∣∣2 ≤ IlAIl2∣∣x∣∣2 for any matrix
A and vector x. Now that we have an upper bound of ∣∣Vf (wto+k) 一 Vf (u⅛o+s)∣∣,
(a)	(b)
llVf(wto+k)-Vf(wto+s)k ≤ Lkwto+k - wto+s∣ ≤ Lη(S- k)cm.
(38)
where (a) is by the assumption of L-Lipschitz gradient and (b) is by applying the triangle inequality
(s - k) times and that ∣∣wt - wt—1∣∣ ≤ η∣∣mt-1∣∣ ≤ ηcm, for any t. We can also derive an upper
bound of Eto [∣Vf(wt0+s) - VQ(wt0+s)k],
Eto[∣Vf(wto+s) -VQ(wto+s)∣]
(a)	„ rρll	ll21 (b) P ∕8ηs(Fthred + 2r2 Ch + 3 r3 cm)	r2σ2	λ 2i β 、2 2 c22 ∖
≤ Eto[2 llwto+s - wto Il ] ≤ 2(	(1 - β)2	+ 8(1 - β)2 +4η (f-^β) Cm + 2r cm∙)
(39)
Above, (a) is by the fact that if a function f (∙) has P Lipschitz Hessian, then
l∣Vf (y) - Vf (x) - V2f (x)(y - x)∣ ≤ P∣∣y - x∣2	(40)
(c.f. Lemma 1.2.4 in (Nesterov (2013))) and using the definition that
Q(w):= f (WtO ) + hw - wto , Vf (wto )〉+ 2(w - wto )>H(W - wto ),
22
Published as a conference paper at ICLR 2020
(b)	is by Lemma 1 and η2t ≤ r2 for 0 ≤ t ≤ Tthred
Trl rii	Ii 21d 8ηt (Fthred + 2r^ ch + 3 rɜɑm)	C 2 tθ?	2 / β ∖ 2 2 C 22
Et0 [kwt0 + t - wt0 k ] ≤	(] _ β)2	+ 8η (] _ β)2 +4η (] _ β ) Cm + 2r Cm
≤ 8MFthred + 2r2ch + Pr%m) + 8 r%2	+ 4 2 (ɪ)2c2 + 2r2c2
≤	(1 -β)2	+8(i-β)2 +4η (i-βJcm + 2rcm.
(41)
Combing (37), (38), (39), We have that
Et0 [gq,t-1 k]
(37) t-1	JL
≤ Et0 E k(∏j 二+Cj )k2 fβi (kV∕ (wt0+k )-Vf(wt0+s)k + kVf(wt0+s)-VQ(wt0+s )k)]
s = 1	k=1
(38),(39)
≤
t— 1	s
X k (∏j=S+ιGj) k2 X βs-kLη(s - k)cm
s = 1	k=1
t—1	s
+ X k(∏j=S+ιGj )k2 X βs-k ρ (
s=1	k=1
8ηs (Fthred + 2r2ch, + P r%m)
(1- β)2
+ ⅛r‰ +4η2(三)2cm + 2r2cm)
(1 - β)	1 - β
t—1	s	t—1	s
:=X k (∏j=S+ιGj) k2 X βiLη(s- k)cm + X k (∏j=S+ιGj) k2 X βs-”(VS + V),
s=1	k=1	s=1	k=1
(42)
Where on the last line We use the notation that
8ηs(Fthred + 2r2ch + P r3c3n)
VS ：=	^-W
V = 8ττr⅞w+4η2( τβ^ )%m+2r2cm.
(1 - β)	1 - β
To continue, let us analyze ∣∣ (∏j=g+ιGj) ∣∣2 first.
(43)
j
k(∏j=s+1Gj )∣2 = k∏j=s+ι(i-η X β j—kH )∣2
k=1
(a)
≤ ∏j=s+1(1 + ηθj λ)
∏j=1(1 + ηθj λ) (≤) ∏j=1(1 + ηθj λ)
∏j=1(1 + ηθjλ) ≤ -(1 + ηe)s
(44)
Above, we use the notation that θj := Pk=I βj-k. For (a), it is due to that λ := -λmin(H),
λmax(H) ≤ L, and the choice of η so that 1 ≥ 1—e, or equivalently,
η ≤ 1-β.	(45)
L
For (b), it is due to that θj ≥ 1 for any j and λ ≥ e. Therefore, we can upper-bound the first term on
r.h.s of (42) as
t—1	s	t—1	s—1
X k(∏j=s+1 Gj)∣2 Xβs-kLη(s - k)cm = X k(∏j=s+1Gj)∣2 XβkkLηcm
s=1	k=1	s=1	k=1
(a) —	β
≤ Σk(∏j=s + 1Gj )∣2 (ɪɪ^ LWm
9 (∏t-1	∩	1	θ ∖	βLηcm	X 1
≤ (∏j=1(I + ηθjλ))(1 - β)2 N (1 + ηe)s
(c) /TTt—1/1	I	β ^l	βLnCm	1	/-rrt-1 ∕1	, β ʌʌʌ βLCm
≤(%=1(1+ηθj λ))(τ-β)2 n =他=1(1+η% λ)) i(i-β)2,	(46)
23
Published as a conference paper at ICLR 2020
where (a) is by that fact that ∑∞=1 βkk ≤(、'y for any 0 ≤ β < 1, (b) is by us-
ing (44), and (C) is by using that P∞=1(ɪ^^)s ≤ a. Now let us switch to bound
PS=I Il (∏j 二+ 1Gj) k2 Pk = I βs-k2 (VS + ν)on * (42). We have that
t ——1	s	()] t — 1
X M∏j=s+1Gj )k2 Xβs-k P (VS+V) ≤ 口 X 叫=s+ιGjmP (VS+V)
(≤) (∏j=1(1 + 汹λ)) X 1 P + (∏j=1(1 + 汹λ)) X 1 P
―	1 - β M(1 + ηe)s 2VS	1 - β ⅛f(1 + ηe)s 2V
(C (Πj=1(1+ 汹λ)) X 1 P	(∏j=1(1 + 汹λ)) PV
≤	1 - β M (1 + ηe)s 2VS +	1 - β 河
= (∏j=1(1 + ηθjλ)) tX	1 P	(∏j=1(1+ ηθjλ)) P(8+4η2(ɪ)2* +2厂2*)
=	1 - β M (1 + ηe)s 2VS +	1 - β	际
(d)(∏j=1(1 + ηθj λ)) P 8η (Fthred + 2r2ch + P r3cm)
≤	1 - β	(^	(1 - β)2
(∏j=1(1+ηθj λ)) p(8 (i⅛2 +4η2( 1ββ )2cm+2r2cm)
+	1 - β	际
(47)
where (a) is by the fact that Pk=1 βs-k ≤ 1/(1 一 β), (b) is by (44), (c) is by using that
P∞=1( 1+⅛Y ≤ 壶,(d) is by Σ∞=1 Zkk ≤ (Γ⅛2 for any Izl ≤ 1 and SUbStitUting Z =七,
which leads to P∞=1 zkk ≤ (ɪ-zz^ =(J，也焉产=(1+yf ≤(n2y in which the last inequality is
by chosen the step size η so that ηe ≤ 1.
By combining (42), (46), and (47), we have that
(42) t-1	二
Eto[gq,t-1k ≤ EMnj=S+1Gj)∣2 Eβs-kLη(s- k)cm
S=1	k=1
t— 1	s
+ X k(∏j=S+1Gj )∣2 X βs-k P(Vs + V)
S=1	k=1
(46),(47)
≤
(∏j=1(1+ηθj λ))「
+
(∏j=1(1+ ηθjλ)) P 8(Fthred+ 2r2Ch + 家3噫)
1 - β 谓	(1 - β)2
+
(∏j=1(1+ηθj λ)) p(8 (r—加 +4η2( 1—e )2Cm+2r2cm)
1 - β	泰
which completes the proof.
(48)
□
24
Published as a conference paper at ICLR 2020
Upper bounding Ilqv,t-ι∣∣:
Lemma 10. Following the conditions in Lemma 1 and Lemma 2, we have
Ilqv,t-1∣∣ ≤ (∏j=i(l + ηθjλ))rcm∙	(49)
Proof.
kqv,t-ik ≤ k(∏j=lGj)(-rmto )k ≤ k(∏j=1G )∣2∣- rmto ∣ ≤ (∏j=1(1 + ηθj λ))rcm,
(50)
where the last inequality is because η is chosen so that 1 ≥ 1LLe and the fact that λmax(H) ≤ L.
□
Lower bounding E% [2η(qv,t-ι, qq,t-ι)]:
Lemma 11. Following the conditions in Lemma 1 and Lemma 2, we have
Eto [2ηhqv,t-1, qq,t-1i]
≥ -2η(∏j=1(1+ ηθjλ))2rcm×
r βLcm	P 8(Fthred + 2r2Ch + Pr3c3n)	ρ(8之知 + 4η2(!⅜/Cm + 2r2cm)
L(1 - β) + 覆	(1 - β)3	+	2ηe(1 - β)	].
(51)
Proof. By the results of Lemma 9 and Lemma 10
EtO[2ηhqv,t-ι,qq,t-ιi] ≥ -Eto[加叽,1|||%1||]
Lemma 10	z , .	、
≥	-EtO [2η(∏7-=1(1 + ηθj X))rcmkqq,t-ι ll]
Lemr 9-2η(∏j=1(1 + ηθjλ))2rcm×	(52)
r βLcm	ρ 8(Fthred + 2r2ch, + 3r3cm)	ρ(8(r-β)2 +4η2(TLee/cm + 2r2cm)
L(1 - β)2 + 市	(1 - β)3	+	2ηe(1 - β)	L
□
25
Published as a conference paper at ICLR 2020
Lower bounding Et0 [2ηhqv,t-1, qξ,t-1i]:
Lemma 12. Following the conditions in Lemma 1 and Lemma 2, we have
Eto [2ηhqv,t-ι,qξ,t-ιi] = 0.	(53)
Proof.
t-1	s
Et0 [2ηhqv,t-1, qξ,t-1i] = Et0 [2ηhqv,t-1, -X(∏j=S+ιGj )X βs-k ξto+ki]
s=1	k=1
s
(=a) Et0 [2ηhqv,t-1, Xαkξt0+ki]
s
(=) Et0 [2η X Et0 +k-1 [hqv,t-1 , αk ξt0 +k i]]
k=1	(54)
s
= Et0 [2η	hqv,t-1, Et0+k-1 [αk ξt0 +k]i]
k=1
s
= Et0 [2η	αk hqv,t-1 , Et0+k-1 [ξt0 +k]i]
k=1
(=d) 0,
where (a) holds for some coefficients αk, (b) is by the tower rule, (c) is because qv,t-1 is measureable
with to, and (d) is by the zero mean assumption of ξ's.
□
Lower bounding Et0 [2ηhqv,t-1, qm,t-1i]:
Lemma 13. Following the conditions in Lemma 1 and Lemma 2, we have
Eto[2ηhqv,t-ι,qm,t-ιi] ≥ 0.	(55)
Proof.
Eto [2ηhqv,t-1, qm,t-1 i]
t-1
=2ηrEto [h(∏j=lGj)mto, X (∏j=S+ιGj)βsmt0i]
s=1
(56)
(=a) 2ηrEto [hmto , Bmtoi] (≥) 0,
where (a) is by defining the matrix B := (∏j=1 Gj)> (Ps=1 (∏j=S+ιGj)βs)∙ For(b), notice that
the matrix B is symmetric positive semidefinite. To see that the matrix B is symmetric positive
semidefinite, observe that each Gj := (I - η Pjk=1 βj-kH) can be written in the form of Gj =
U Dj U > for some orthonormal matrix U and a diagonal matrix Dj . Therefore, the matrix product
(∏j=1 Gj)> (∏j=S+ιGj) = U(Πj=IDj)(∏j=s+]Dj)U> is symmetric positive semidefinite as long
as each Gj is. So, (b) is by the property ofa matrix being symmetric positive semidefinite.
□
26
Published as a conference paper at ICLR 2020
Lower bounding 2ηEt0 包,1,qw,t-ιi]:
Lemma 14. Following the conditions in Lemma 1 and Lemma 2, ifSGD with momentum has
the APCG property, then
2ηEt0[hqv,t-i,qw,t-ii] ≥ -72ηrc0vCnj=1(1 + ηθjλ))2e.	(57)
(1 - β)
Proof. Define Ds := Πtj-=11 Gj Πtj-=1s+1 Gj.
t-1	s
2ηEt0 [hqv,t-1, qw,t-ii] = 2ηEt0 [h(∏j=1GjWmtO), X (∏j=S+ιGj) X βSiNf(WtO )i]
s=1	k=1
t-1	S
=2ηEto [hrmto, X (∏j=Gj∏j=S+ιGj) Xβs-kNf (wto)i]
S=1	k=1
t-1 S
2ηrXXβS-kEtO[hmtO,DSNf(wtO)i]
S=1 k=1
(a)	t-1 S
≥ -2η2rc0	βS-kkDSk2kNf(wtO)k2
S=1 k=1
≥-2¾ X kDsk2∣Nf (wto)k2,
1	- β S=1
where (a) is by the APCG property. We also have that
t-1	t-1	t-1	t-1
kDSk2 = kΠj=1GjΠj=S+1Gjk2 ≤ kΠj=1Gjk2kΠj=S+1Gjk2
(58)
(a)
≤ kΠtj-=11Gjk2
Πj=1(i + ηθjλ) (≤) (Πj=1(i + ηθjλ))2
(1 + η)S
(1 + η)S
(59)
where (a) and (b) is by (44). Substituting the result back to (58), we get
2η2rc0 t-1
2ηEto [hqv,t-1, qw,t-1i] ≥ - 1 _ β EkDSIl2kNf (WtO )∣∣
1 - β S=1
≥- τ-rβ0X Cnjmj" kNf(WtO )k2 ≥- cr--β⅛ (πj=1(1+汹 λ))2kNf(WtO )k2
(60)
Using the fact that kNf (WtO )k ≤ completes the proof.
□
E.3 Proof of Lemma 5
Recall that the strategy is proving by contradiction. Assume that the function value does not decrease
at least Fthred in Tthred iterations on expectation. Then, we can get an upper bound of the expected
distance EtO [kWtO+Tthred - WtO k2] ≤ Cupper but, by leveraging the negative curvature, we can also
show a lower bound of the form EtO [kWtO+Tthred - WtO k2] ≥ Clower. The strategy is showing that
the lower bound is larger than the upper bound, which leads to the contradiction and concludes
that the function value must decrease at least Fthred in Tthred iterations on expectation. To get the
contradiction, according to Lemma 1 and Lemma 3, we need to show that
EtO [kqv,Tthred-1k2] + 2ηEtO [hqv,Tth
red-1, qm,Tthred-1 + qq,Tt
hred -1 + qw,Tthred -1 + qξ,Tthred-1i] > Cupper.
(61)
Yet, by Lemma 13 and Lemma 12, we have that ηEtO [hqv,Tthred-1, qm,Tthred-1i] ≥ 0 and
ηEtO [hqv,Tthred-1, qξ,Tthred-1i] = 0. So, it suffices to prove that
EtO [kqv,Tthred-1k2] + 2ηEtO[hqv,Tth
red -1 , qq,Tthred -1 + qw,Tthred -1 i] > Cupper ,	(62)
and it suffices to show that
27
Published as a conference paper at ICLR 2020
•	4 Et0 [kqV,Tthred-ik2 ] + 2nEt0 Kqv,Tthred-1, qq,TthredTi] ≥0∙
•	1 Eto [kqv,τthred-1k2] +2ηEt0[hqv,Tt
hred -1 , qw,Tthred -1	.
•	4 Eto [kqv,Tthred-Ik 2] ≥ Cupper.
E.3.1	PROVING THAT 4Eto [kqv,Τthred-ik2]+2ηEtθ [hqv,Tthred-1,qq,Tthred-li] ≥ 0：
By Lemma 4 and Lemma 11, we have that
4 Et0 [kqv,Tthred-ik2] + Et0 [2ηhqv,Tthred -1 , qq,Tthred -1 i]
≥ L (~∏Tthred-1 (1	+	nθ∙λ)12r2 〜-2r)(∏τthred-1 (1 +	∏θ∙λ)12rc
≥ 4 V1j = 1	(1	+	ηθjλ)) r Y 2η V1j = 1	(1+	ηθj λ)) rcm
r βLCm + P 8(Fthred + 2r2ch + Pr%m)	ρ(8(f⅛ + 4η2 (1-β)2* + 2τrcn)
× ∣e(1 - β)2 + 谓	(1 - β)3	+	2%(1 - β)	].
(63)
To show that the above is nonnegative, it suffices to show that
r2 ≥ 24ηrβLcm
Y ≥ e(1- β)2 ,
and
2 ≥ 24ηrcmρ 8 (Fthred + 2r ch + 3r Cm)
r Y - (1 一 β)η62	(1 一 β)2	，
and
2	24ηrcm ρ(8 (r-β)2 + 4η2( 1ββ)2cm + 2r2cm)
r γ ≥
1 - 1 一 β	2%
(64)
(65)
(66)
Now w.l.o.g, we assume that cm, L, values of parameters on Table 3, we	σ2, c0, and ρ are not less than one and that E ≤ 1. By using the have the following results; a sufficient condition of (64) is that CT、24Lcme2 C ≥「F.	(67)
A sufficient condition of (65) is that	cr	576cmρ ≥ ~7~. q∖3,	(68) cF (1 一 β)3
and	1 ≥ 1152cmβchcr,	(69)
and	1 ≥ 192cmρ2cr	(70) 1 ≥ (1 - β)3 .	()
A sufficient condition of (66) is that	1 > 96cmP°2 +3Cm)Cre 1 ≥	(1 一 β )3	,	()
and a sufficient condition for the ab	ove (71), by the assumption that both σ2 ≥ 1 and cm ≥ 1, is 1、576cmPσ2cre	S 1 ≥	(1 - β)3 .	()
Now let us verify if (67), (68), (69), (70), (72) are satisfied. For (67), using the constraint of
cη on Table 3, We have that ： ≥ CmPL； C Ch. Using this inequality, it suffices to let cr∙ ≥
c3 ρLσ2^:(1-β)2 for getting (67), which holds by using the constraint that c0(1 一 β)2 > 1 and E ≤ 1.
28
Published as a conference paper at ICLR 2020
For (68), using the constraint of CF on Table 3, We have that * ≥ CmP L Ch. Using this inequality,
it suffices to let Cr ≥ c3。工/5尸,which holds by using the constraint that σ2(1 - β)3 > 1. For
(69), it needs ι1(1-βPs ≥ c3。力勿 ≥ Cr, which hold by using the constraint that σ2(1 - β)3 > 1.
For (70), it suffices to let (1-βP ≥ c3 p%% ≥ Cr which holds by using the constraint that
σ2(1 - β)3 > 1. For (72), it suffices to let 57(6c-^^ ≥ 脸PLσ2ch ≥ Cr, which holds by using the
constraint that L(1 - β)3 > 1 and ≤ 1. Therefore, by choosing the parameter values as Table 3, we
CangUaranteethat 1 EtCl [|包,小1-1口2] + 2ηEt(o [d,小1-1, qq,Tthred-ιi] ≥ 0.
E∙3∙2 PROVING THAT 4Et0 [kqv,Tthred-1k2]+2ηEt0 [hqv,τthred-1,qw,τthred-1i] ≥ 0:
By Lemma 4 and Lemma 14, we have that
4Eto [kqv,Tthred -1k2] + 2ηEt0[hqv,Tth
red -1, qw,Tthred -1i]
≥ 1 (Π⅛ed-1(1+ ηθjλ))2r2γ - 72η⅛Cn=fei(1+ ηθjλ))2e.
4	(1 - β)
(73)
To show that the above is nonnegative, it suffices to show that
r2γ ≥ ⅞-¾.	(74)
A sufficient condition is cr ≥ 8⅜⅛. Using the constraint of Cn on Table 3, we have that 1- ≥
Cη	1-β	Cη
5 L2 2 0	4
mρ c]---h. So, it suffices to let Cr ≥ 3c5。二号葭(一6),which holds by using the constraint that
LC1 - β)3 > 1 (so that LC1 - β) > 1) and ≤ 1.
E.3.3 PROVING THAT 1 Eto [kqv,Tthred-1k2] ≥ Cupper:
From Lemma 4 and Lemma 1, we need to show that
4 (∏T=Γd-1(1 + ηθjλ))2r2Y
8ηt (Fthred + 2r2Ch + P r3C3n)
≥	(1- β)2
We know that 1 (∏T={ed-1(1 + ηθjλ))2r2Y
that
r2σ2
+ 8CT-罚 +
1	Tthred -1
≥ 4(lj=1
4η2( τβh )2Cm + 2r%m.	(75)
1-β
C1 + ηθj))2r2γ. It suffices to show
4 (∏⅛ed-1(1 + ηθje))2r2Y
8ηt(Fthred + 2r2Ch + 3 r3点)
≥	(1- β)2
r2σ2
+ 8CT-Iy
+ 4η2( ɪ )%m + 2r%m.	(76)
1-β
Note that the left hand side is exponentially growing in Tthred . We can choose the number of
iterations Tthred large enough to get the desired result. Specifically, we claim that Tthred ≥
c(1-β) log( Lcm-βpCyCh) for some constant c > 0. To see this, let us first apply log on both sides of
(76),
Tthred-1
2( X log(1 + ηθjE)) + log(r2Y) ≥ log(8aTthred + 8b)	(77)
j=1
where we denote a := 4n(Fthred+-βCh+3r Cm) and b := 4(；： + 2η2(T-β)2Cm + r2C2m. To
proceed, we are going to use the inequality log(1 + x) ≥ X2, for X ∈ [0,〜2.51]. We have that
1≥
ηE
Cr-^
(78)
29
Published as a conference paper at ICLR 2020
as guaranteed by the constraint of η. So,
Tthred -1	(a)
2( X Iog(I + ηθjE)) ≥
j=1
Tthred -1	Tthred-1 j-1
X ηθjE= X	XβkηE
j=1	j=1	k=0
Tthred-1 1 - βj	1
j=1	τ-βη ≥ 1-β
(Tthred - 1 - ɪ)ηe.
1 -β
(b) Tthred - 1
≥ 2(1- β)
ηE,
(79)
where (a) is by using the inequality log(1 + x) ≥ χ with X = ηθjE ≤ 1 and (b) is by making
Tthr--I ≥(i-β)2, Which is equivalent to the condition that
Tthred ≥ 1 + 1-β	(80)
Now let us substitute the result of (79) back to (77). We have that
T	[	2(I- β) 1 FaTthred + 8b∖	zδn
Tthred ≥ 1 +--------log(--------2----),	(81)
ηE	γr2
which is what we need to show. By choosing Tthred large enough,
Tthred ≥ c(1-β) log( Luh ) = O((1 - β) log(7Γ⅛)E-6)	(82)
ηE	(1 - β)δγE	(1 - β)E
for some constant c > 0, we can guarantee that the above inequality (81) holds.
F Proof of Lemma 15
Lemma 15 (Daneshmand et al. (2018)) Let Us define the event Yk := {∣Nf(WkTthred)k ≥
E Or λmin(V2f (WkTthred )) ≤ -Eh The complement is Yk := {kVf (WkTthred )k ≤
E and λmin(V2f (WkTthred)) ≥ -e}, which suggests that WkTthred is an (e, e)-second order sta-
tionary points. Suppose that
E[f(W(k+1)Tthred)-f(WkTthred)|Yk] ≤-∆
E[f (W(k + 1)兀hred ) - f (Wk 兀hred )|Yk ] ≤ δ ɪ ∙
(83)
Set T = 2Tthred f(W0) - minw f(W) /(δ∆).	We return W uniformly randomly from
W0, WTthred, W2Tthred, . . ., WkTthred, . . ., WKTthred, where K := bT /Tthredc. Then, with probability
at least 1 - δ, we will have chosen a Wk where Yk did not occur.
Proof. Let Pk be the probability that Yk occurs.
E[f (W(k+1)Tthred ) - f (WkTthred )]
= E[f (W(k+1)Tthred) - f (WkTthred)|Yk]Pk +E[f(W(k+1)Tthred) - f (WkTthred)|Yck](1 - Pk)
≤ -∆Pk + δ∆∕2(1 - Pk)
=δ∆∕2 - (1 + δ∕2)∆Pk
≤ δ∆∕2 - ∆Pk.
(84)
Summing over all K, we have
1K	1K
K+1∑Ef(W(k+1)Tthred ) - f (Wk 兀hred )] ≤ ' K +]	(δ∕2-Pk)
k=0	k=0
⇒ K⅛Γ X Pk ≤ δ∕2+ Win∆f (W) ≤ δ	(85)
1K
⇒ κ+1∑(1 - Pk) ≥ 1 - δ∙
□
30
Published as a conference paper at ICLR 2020
G Proof of Theorem 1
Theorem 1 Assume that the stochastic momentum satisfies CNC. Set r = O(2), η = O(5), and
T7j =—— c(1——β) log∙(LCmσ_PC Ch ) —— 0((1 — β) loρ,(LCmσ_PC Ch )^-6) for some constant c > 0 If
Jthred 一 麻 log( (1 -β)δγe ) = Ly((J■ β) log( (1 -β)δγe )t ) for some constant c > 0. f
SGD with momentum (Algorithm 2) has APAG property when gradient is large (∣∣Vf (w)k ≥ E),
APCGTthred property when it enters a region of saddle points that exhibits a negative curvature
(∣Vf (w)∣ ≤ E and λmin(V2f (w)) ≤ -E), and GrACE property throughout the iterations, then it
reaches an (e, E) second order stationary point in T = 2Tthred(f (wo) 一 mi□w f (W)) / (δ Fthred) =
0((1 — β)log( Lcm-e)p；Ch )e-10) iterations with high probability 1 — δ, where Fthred = O(e4).
G.1 Proof sketch of Theorem 1
In this subsection, we provide a sketch of the proof of Theorem 1. The complete proof is available in
Appendix G. Our proof uses a lemma in (Daneshmand et al. (2018)), which is Lemma 15 below. The
lemma guarantees that uniformly sampling a w from {wkTthred }, k = 0, 1, 2, . . . , bT /Tthredc gives
an (E, E)-second order stationary point with high probability. We replicate the proof of Lemma 15 in
Appendix F.
Lemma 15. (Daneshmand et al. (2018)) Let us define the event Υk := {∣Vf (wkTthred)∣ ≥
E or λmin(V2f (wkTthred)) ≤ —E}. The complement is ΥCk := {∣Vf(wkTthred)∣ ≤
E and λmin(V2f(wkTthred)) ≥ —E}, which suggests that wkTthred is an (E, E)-second order sta-
tionary points. Suppose that
E[f (W(k+1)Tthred ) — f (WkTthred )^] ≤ -△ ^ E [f (W(&+1)飞1) - f"hred )3] ≤ δ W .
(86)
Set T = 2Tthred(/(wo) — mi□w f (W))/(δ∆).	8 We return W uniformly randomly from
W0, WTthred, W2Tthred, . . . , WkTthred, . . . , WKTthred, where K := bT /Tthredc. Then, with probability
at least 1 — δ, we will have chosen a Wk where Υk did not occur.
To use the result of Lemma 15, we need to let the conditions in (86) be satisfied. We can bound
E[f (w(k+i)τthred) — f(wkTthred )∣Υk ] ≤ —Fthred, based on the analysis of the large gradient norm
regime (Lemma 7) and the analysis for the scenario when the update is with small gradient norm but a
large negative curvature is available (Subsection 3.2.1). For the other condition, E[f (w(k+1)Tthred) —
f (WkTthred) I Υk] ≤ δFthred, it requires that the expected amortized increase of function value due
to taking the large step size r is limited (i.e. bounded by δFthred) when WkTthred is a second order
stationary point. By having the conditions satisfied, we can apply Lemma 15 and finish the proof of
the theorem.
G.2 Full proof of Theorem 1
Proof. Our proof is based on Lemma 15. So, let us consider the events in Lemma 15,
Υk := {∣Vf(WkTthred)∣ ≥ E or λmin(V2f(WkTthred)) ≤ —E}. We first show that
E[f (W(k+1)Tthred ) — f (WkTthred ) | Yk] ≤ Fthred.
When ∣Vf (WkTt
hred )∣ ≥ E:
8One can use any upper bound of f (w0) - minw f (w) as f (w0) - minw f (w) in the expression of T .
31
Published as a conference paper at ICLR 2020
Consider that Yk is the case that kVf (WkTthred )k ≥ & Denote to ：= kTthred in the following. We
have that
Tthred -1
Et0 [f(wt0+Tthred) - f(wt0)] = X Et0[E[f(wt0+t+1) - f(wt0+t)|w0:t0+t]]
t=0
Tthred-1
= Et0[f(wt0+1) - f(wt0)] + X	Et0[E[f(wt0+t+1) - f(wt0+t)|w0:t0+t]]
t=1
(a)	r	Lr2 c2	Tthred-1
≤ -2kVf(wt0)k2 + -2-m + E	Eto[E[f(wt0 +t+ι) — f(Wto+t)|wo：to+t]]
t=1
(b)	r	-r2-2	Tthred-1	ρ
≤ -5kvf(WtO)k2 +	m+ + X	(n%h+q3cm)
22	6
(87)
t=1
≤) -2kVf(wto)k2 + -2cm + r2ch + Pr3cm
(d)	r
≤ — 2 kvf (WtO )k2+-r2cm+r2-h
(e)	r 2	2 2	2	(f) r 2 (g)
≤	- 2E + - cm + r -h ≤ - 4E ≤ -Fthred,
where (a) is by using Lemma 6 with step size r, (b) is by using Lemma 8, (c) is due to the constraint
that η2Tthred ≤ r2 ,
2
, (d) is by the choice of r, (e) is by kVf(wt)k ≥ , (f) is by the choice of r so
, and (g ) is by
that r ≤ 4(Lcm+ch)
4 2 ≥ Fthred.
(88)
When kVf (WkTthred)k ≤ Eand λmin(V2f (WkTthred)) ≤ -E:
The scenario that Υk is the case that kVf (WkTthred)k ≤ E and λmin(V2f (WkTthred)) ≤ -E has been
analyzed in Appendix E, which guarantees that E[f (WtO+Tthred) - f (WtO)] ≤ -Fthred under the
setting.
When kVf (WkTthred)k ≤ Eand λmin(V2f (WkTthred)) ≥ -E:
Now let Us switch to show that E[f (w(k+i)τthred) - f (wkTthred)|Yk] ≤ δFhred. Recall that Yk
means that kVf (WkTthred )k ≤ E and λmin(V2f (WkTthred)) ≥ -E. Denote t0 := kTthred in the
following. We have that
Tthred -1
EtO [f(WtO+Tthred) - f (WtO)] = X EtO [E[f(WtO+t+1) - f(WtO+t)|W0:tO+t]]
t=0
Tthred -1
= EtO[f(WtO+1) - f (WtO)] + X EtO[E[f(WtO+t+1) - f(WtO+t)|W0:tO+t]]
t=1
(a)	ρ	Tthred -1
≤ r2-h + ar3cm +	2∑	Eto [E[f (wto+t+1)- f (wto + t)|w0:to + t]]
6m	O	O	O	O
t=1
(b)	Tthred -1
≤ r2ch+Pr3cm + x	(η2-h+Pη3-m)
t=1
(c)	2 P 3 3	2	(d) δFthred
≤	2r	-h	+	3 r	cm	≤	4r	-h	≤ —2一.	(89)
where (a) is by Using Lemma 8 with step size r, (b) is by Using Lemma 8 with step step size η, (c) is
by setting η2Tthred ≤ r2 and η ≤ r, (d) is by the choice of r so that 8r2 ch ≤ δFthred.
32
Published as a conference paper at ICLR 2020
Now we are ready to use Lemma 15, since both the conditions are satisfied. According to
the lemma and the choices of parameters value on Table 3, we can set T = 2Tthred f(w0) -
minw f (w))/(δFthred) = O((1 - β) log( Lcr-2)烧)e-10), which Will return a W that is an (e, e)
second order stationary point. Thus, we have completed the proof.
□
G.3 Comparison to Daneshmand et al. (2018)
Theorem 2 in Daneshmand et al. (2018) states that, for CNC-SGD to find an (e, p1/2e) stationary point,
the total number of iterations is T = O('；：L log2(盖)e-10), where ' is the bound of the stochastic
gradient norm kgtk ≤ ` which can be viewed as the counterpart ofcm in our paper. By translating their
result for finding an (e, e) stationary point, it is T = O(' f5Lγf log2(ρδLL)e-10). On the other hand,
using the parameters value on Table 3, we have that T = 2Tthred f(w0) - minw f(w) /(δFthred) =
O((I-田。咋/4("2)3。μ log(LI-⅛⅛)e-10) for Algorithm 2.
Before making a comparison, we note that their result does not have a dependency on the variance
of stochastic gradient (i.e. σ2), which is because they assume that the variance is also bounded by
the constant ` (can be seen from (86) in the supplementary of their paper where the variance terms
kζik are bounded by `). Following their treatment, if we assume that σ2 ≤ cm, then on (71) we can
instead replace (σ2 + 3c2rι) with 4* and on (72) it becomes 1 ≥ 5；6Gm)3re. This will remove all
the parameters, dependency on σ2. Now by comparing O((1 -e)点或。0 ∙ ρ4L3e-10) of ours and
T = O(ρ2'10 ∙ ρ4L3e-10) of Daneshmand et al. (2018), we see that in the high momentum regime
where (1 - β) << 9 '2 0, Algorithm 2 is strictly better than that of Daneshmand et al. (2018), which
cmchc
means that a higher momentum can help to find a second order stationary point faster.
33