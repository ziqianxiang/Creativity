Published as a conference paper at ICLR 2020
Extreme Tensoring for Low-Memory Precon-
DITIONING
Xinyi Chen
Google AI Princeton
Princeton, NJ
xinyic@google.com
Elad Hazan
Princeton University & Google AI Princeton
Princeton, NJ
ehazan@cs.princeton.edu
Yi Zhang
Princeton University
Princeton, NJ
y.zhang@cs.princeton.edu
Naman Agarwal
Google AI Princeton
Princeton, NJ
namanagarwal@google.com
Cyril Zhang
Princeton University & Google AI Princeton
Princeton, NJ
cyril.zhang@princeton.edu
Ab stract
State-of-the-art models are now trained with billions of parameters, reaching hard-
ware limits in terms of memory consumption. This has created a recent demand
for memory-efficient optimizers. To this end, we investigate the limits and per-
formance tradeoffs of memory-efficient adaptively preconditioned gradient meth-
ods. We propose extreme tensoring for high-dimensional stochastic optimization,
showing that an optimizer needs very little memory to benefit from adaptive pre-
conditioning. Our technique applies to arbitrary models (not necessarily with
tensor-shaped parameters), and is accompanied by regret and convergence guaran-
tees, which shed light on the tradeoffs between preconditioner quality and expres-
sivity. On a large-scale NLP model, we reduce the optimizer memory overhead
by three orders of magnitude, without degrading performance.
1	Introduction
Among the most influential and important optimization techniques in machine learning are adaptive
learning-rate methods, otherwise known as diagonal-matrix adaptive preconditioning. Essentially all
of the most-commonly used incarnations of adaptive preconditioning (AdaGrad, Adam, RMSprop,
Adadelta, etc.) accumulate second-moment estimators of each coordinate of the gradient, then scale
the parameter updates by the square roots of these accumulators. These methods come with an
overhead memory cost of storing these accumulators, thereby doubling the memory consumption.
In the regime where model size encroaches upon the same order of magnitude as the total amount
of RAM, a need has arisen to view memory as a limited resource in large-scale optimization.
We address the question of whether the benefits of adaptive preconditioning can be attained with-
out significant memory overhead. To this end, we introduce extreme tensoring, a family of generic
modifications to any second-moment-based adaptive optimizer. Our method uses a compressed pre-
conditioner which takes the form of a tensor product of arbitrary order, with simple updates, without
necessarily requiring the parameters to be tensor-shaped. In our regret analysis, we quantify how
extreme tensoring competes provably with full-memory AdaGrad in the online convex optimization
framework, with a multiplicative data-dependent constant that can be measured empirically.
In a large-scale language modeling setting, we demonstrate that an optimizer requires very little
additional memory to benefit from adaptive preconditioning. Furthermore, the inherent flexibility
1
Published as a conference paper at ICLR 2020
of our method enables us to conduct, to the first of our knowledge, the first empirical study of the
tradeoff between training convergence and memory in the optimizer.
1.1	Related work
The most widely-adopted form of adaptive preconditioning are second-moment-based: for example,
AdaGrad (Duchi et al., 2011), Adam (Kingma and Ba, 2014), RMSprop (Tieleman and Hinton,
2012), and Adadelta (Zeiler, 2012). Some recent preconditioning methods are not based on second
moments (Bello et al., 2017; Chen and Gu, 2018; Bernstein et al., 2018), and fall beyond our scope.
Tensor-factorized preconditioners. Many works (Martens and Grosse, 2015; Gupta et al., 2018;
Martens et al., 2018) investigate tensor-factorized preconditioners. These are presented in the view
of restricted full-matrix preconditioning (vs. diagonal for us) of tensor parameters (vs. general-
purpose for us). In parameter count regimes relevant to this paper, these full-matrix adaptive meth-
ods suffer from prohibitive time and memory overhead issues. Extreme tensoring can be viewed as
the diagonal version of Shampoo. The matrix version of diagonal Shampoo was discussed in the
appendix of (Gupta et al., 2018); in this paper we extend the algorithm to arbitrary tensor factor-
izations, and focus on the tradeoff between memory consumption and optimizer performance. The
theoretical part of our work follows proof techniques seen in (Gupta et al., 2017) and (Gupta et al.,
2018), although our diagonal restriction results in distinct updates and incomparable regret bounds.
Optimizer memory reduction in deep learning. Perhaps the most closely related work to ours is
Adafactor (Shazeer and Stern, 2018), an empirical work which achieves sublinear-memory adap-
tive regularization by restricting preconditioners on matrix-shaped gradients to “row” and “column”
learning rates, with a similar update rule. Similar algorithms have appeared in prior work (Gupta
et al., 2014; Shazeer et al., 2017). As a special case of our regret analysis, we provide some theory
for this line of work, while proposing a more general and versatile memory-reduction method. Con-
current to this work, Anil et al. (2019) are also motivated by memory efficiency, and the algorithm
uses a different approximation to the preconditioner than a tensor product. Their algorithms’ index
partitions are compatible with our tensor indices, and we provide a comparison in Appendix C.
Optimizer memory reduction outside of deep learning. Older than our work is the classic L-
BFGS algorithm (Liu and Nocedal, 1989), a quasi-Newton method with a tunable memory window
hyperparameter. This does not immediately apply to our setting, as the tunable memory overhead is
(window size)×, rather than (1 + ε)×. Furthermore, second-order methods turn out to be brittle un-
der stochastic gradients; it remains an open empirical problem to robustly exploit Hessian curvature
of the loss surface in modern deep learning settings.
Intrinsic dimensionality in deep learning. More broadly, there are various empirical attempts
to locate and characterize low-dimensional phenomena hiding within high-dimensional parameter
spaces in deep learning (Li et al., 2018a;b). Our comparison provides a lens on measuring the
intrinsic dimensionality of the preconditioner. Another is (Agarwal et al., 2020), a different way to
impose parameter bottlenecks on adaptive preconditioners; however, the proposed method does not
reduce memory consumption.
Our contributions:
•	Using the user-selectable degree of memory reduction, we conduct an empirical study of
training convergence vs. preconditioner memory usage. We find, in a realistic large-scale
language modeling setting, that reallocating memory savings from the optimizer to the
model itself can pay off in terms of end-to-end performance.
•	We propose extreme tensoring as a modification to AdaGrad, Adam, etc. for reducing
the overhead memory cost in adaptive preconditioning, accompanying algorithms such as
(Shazeer and Stern, 2018; Anil et al., 2019).
•	We derive a regret bound for extreme-tensored AdaGrad, and provide the first numerical
comparisons of regret bounds in deep learning. Though they obviously do not provide
convergence guarantees in the non-convex setting, they act as a proxy for understanding the
AdaGrad accumulator overestimation induced by memory constraints. Via this diagnostic
lens, we see that the regret competes with AdaGrad within a multiplicative constant, which
we measure to be small in practice.
2
Published as a conference paper at ICLR 2020
Figure 1: Illustration of tensor index I: the vector x is reshaped into tensor x of d dimensions, and
the bijection I maps the ith coordinate of x, x[i], to x[I (i)].
2	Preliminaries
2.1	Stochastic optimization and adaptive methods
We will state our algorithmic contribution in the usual framework of stochastic optimization of a
differentiable function f (∙), equipped With an unbiased stochastic gradient oracle Vf (∙). At each
of T iterations, the algorithm queries a stochastic gradient gt := Vf(xt). In practice, our target is
the large-scale ERM setting, Where f is the population loss on training data, and Vf is a mini-batch
stochastic gradient With respect to the model parameters.
HoWever, since the introduction of AdaGrad (Duchi et al., 2011), the standard analysis of adaptive
regularization has been in the online convex optimization (OCO) frameWork (see, e.g. Gupta et al.
(2017; 2018)). In this language, We vieW the xt queried by the learning algorithm as a sequence of
online decisions chosen from a convex set K, and the gt as a sequence of gradients of adversarially
chosen convex loss functions ft . We are interested in minimizing the regret, defined as
TT
Xft(xt) - mx∈iKnXft(x).
This is a generalization of stochastic convex optimization; for a broad survey, see Hazan (2016).
A frameWork for reducing the stochastic non-convex setting to the online convex setting is intro-
duced in AgarWal et al. (2018), translating a sublinear regret bound to convergence to approximate
stationary points.
2.2	Tensors and indexing conventions
We introduce some notation and terminology for clarity and convenience.
By [n] We denote the index set {1, . . . , n}. Id and 0d refer to the d × d identity matrix and d-
dimensional zero vector, respectively. We use • to denote the trace inner product (Hadamard prod-
uct) betWeen tWo matrices, and ∙ to denote the element-Wise product of tWo vectors, or a vector and
a scalar.
As is standard in optimization contexts, a tensor of order p is an element of Rd1 ×...×dp: a p-
dimensional table of real numbers, indexed by a p-tuple I = (I1, ∙ ∙ ∙ Ip) of integers, Where Ii ∈ [di].
Our methods Will require the user to specify a relabeling of gradient vectors, Which We Will call a
tensor index:
Definition 2.1. Let d1, . . . , dp be positive integers whose product is d. A tensor index is a bijection
I : [d] → ×ip=1 [di] between indices for Rd and Rd1 ×...×dp.
We Will refer to the “reshape” conversion betWeen vectors x ∈ Rd and tensors x ∈ Rd1 ×...×dp
specified by the index bijection I. For this, We Will use the shorthand notation x := I(x) and
x := I-1 (x). Throughout this paper, We Will use square brackets to refer to vector and tensor
indices: for example, the previous definition can be restated as enforcing x[i] = x[I (i)]. See
Figure 1 for an illustration of such a conversion.
Although We do not need them to state the simple algorithm, it Will be useful in the analysis to
introduce a feW other pieces of notation. We denote the tensor (or Kronecker) product of matrices
3
Published as a conference paper at ICLR 2020
A, B by A 0 B. Given a positive definite matrix A ∈ Rd×d, for any X ∈ Rd, define the matrix norm
kxk a := X>τAx and its dual kXkA := √χ>A-1 x. For a square matrix M, diag(M) refers to the
diagonal matrix of the same dimensions and diagonal entries as M .
3	Preconditioning by extreme tensoring
We begin by presenting our proposal for black-box memory reduction in adaptive preconditioning.
We call this extreme tensoring, as it takes the view of restricting a high-dimensional preconditioner
to an arbitrary-order tensor product.
The algorithm implicitly maintains a preconditioner which is a rank-one tensor product of the same
dimension as a given tensor index. A second-moment accumulator is maintained for sums of squared
gradient magnitudes across each (p - 1)-dimensional tensor slice; the adaptive step size for each
coordinate is the inverse square root of the geometric mean of its p corresponding slice sums. The
formal specification of the base algorithm (AdaGrad with extreme tensoring) is given in Algorithm 1.
Algorithm 1 AdaGrad with extreme tensoring
1:	Input: Initializer x1, learning rate schedule {ηt}, tensor index I with dimensions (d1, . . . , dp),
ε>0
2:	Initialize (S(1), . . . , S(p)) := (0d1, . . . , 0dp)
3:	for t = 1, . . . , T do
4:	Receive stochastic gradient gt
5:	Reshape: gt := I(gt)
6:	Accumulate slice sums:
∀i ∈ [p], S(i)[j] 一 S⑴[j]+ X gt[I]2
I:Ii=j
7:	Get step sizes: δt[I] := (ε + QP=ι S(i)[Ii])- 2p
8:	Update: xt+i — Xt — ηt ∙ I 1(δt) ∙ gt
9:	end for
We make a few important remarks:
•	AdaGrad is a special case of Algorithm 1, with p = 1, d1 = d. The analogues for Adam,
RMSprop, Adadelta, etc. are obtained straightforwardly by decaying the accumulator
(S(i)[j] J β2 ∙ S(i)[j] + ...). Extreme tensoring is compatible with first-moment esti-
mation (i.e. momentum), although the memory savings disappear. In the setting for our
main empirical result, removing momentum did not degrade performance, corroborating
the findings of Shazeer and Stern (2018).
•	If the tensor dimensions (d1, . . . , dp) in the decomposition are close to equal, the memory
overhead scales as O(P ∙ d1/p).
•	As is standard in matrix and tensor optimization methods (Gupta et al., 2018; Shazeer and
Stern, 2018; Martens et al., 2018), independent copies of Algorithm 1 can be run on each
tensor-shaped parameter group; optimizer APIs in standard deep learning packages pro-
mote this convention. This can be viewed as maintaining a tensor sum of preconditioners,
each of which is a p-wise tensor product.
•	The accumulator update step can be implemented concisely and efficiently using re-
indexing procedures (typically called reshape or view) in standard deep learning and
numerical linear algebra packages.
Lemma 4.3 in the analysis shows that the per-coordinate learning rates are underestimates of those
prescribed by AdaGrad. This interpretation is used in our proof, and serves as the basis of the
empirical measurements in Section 5.3.
4
Published as a conference paper at ICLR 2020
4 Regret analysis
In this section, we justify the choice of update rules for the compressed preconditioner using the
regret-minimizing adaptive regularization framework introduced in the original analysis of Ada-
Grad (Duchi et al., 2011). Throughout the analysis, we will adopt the conventions of online convex
optimization, as discussed in Section 2.1. This gives rise to an intuitively appealing and inter-
pretable regret bound, which we discuss in this section and revisit in the empirical measurements of
Section 5.3.
4.1	Statement and discussion of regret bound
First, we define and prove the main regret bound. As is standard in the regret analysis of diagonal
AdaGrad (see Duchi et al. (2011)), this is most naturally stated with a diameter term on xt: D∞ :=
maxt,x* ∣∣χt 一 x* k∞. This can be constrained using a projection onto an '∞ ball in the algorithm,
but this is seldom seen in practice. It will also be helpful to define Git to be the diagonal matrix with
values
Git[j,j] = X gt[I]2
I:Ii=j
on the diagonal.
Theorem 4.1. Define HT = W=ιKdi + PT=ι Gt)1/2P, and HT = diag(εI + PT=I gtg>)1/2.
Then, there exists a choice of constant learning rate schedule η1 = . . . = ηt := η and ε > 0 such
that the {xt} chosen by Algorithm 1 satisfy the regret bound
X ft(xt) 一 ft(x*) ≤ D∞,2Tr(HT)Tr(HT),
t=1
where x* := minx PtT=1 ft(x) is the loss-minimizing decision in hindsight.
As a direct consequence, we recover the AdaGrad regret bound as a special case where p = 1.
Noting further that HT = HT, we restate this well-known result in our notation, for clarity and
comparisons:
Corollary 4.2. In setting of Theorem 4.1, when p = 1, the {xt} chosen by Algorithm 1 satisfy
T
X ft(xt) - ft(x*) ≤ √2D∞Tr(HT).
t=1
Thus, AdaGrad with extreme tensoring satisfies a regret bound at most
JTr(HTMTr(HT) times
than that of AdaGrad. This quantity can be as large as Ω(√d); this is the worst-case price of the
dramatic memory savings in the regularizer. However, in the presence of sparsity, this ratio can be
much smaller; we include a discussion and empirical measurements in Section 5.3.
4.2	Proofs
The derivation of the regret bound uses the proof framework introduced by Gupta et al. (2017).
Our first important lemma states that the regularizer in Algorithm 1 is a spectral (per-coordinate)
upper bound for the diagonal AdaGrad preconditioner. In other words, this lemma establishes that
the per-coordinate learning rates under extreme tensoring are underestimates of those dictated by
AdaGrad.
Lemma 4.3. Suppose gι, g2,…，gT are tensors ofdimension di X … X dp, and let gt = I-1(gt)
for all t, where I is any tensor index whose image is ×ip=1 [di]. Then for t ∈ [T],
tt
diag(εI + X gsg>)1/2 W 唠 =ι(εQ + X Gs)1/2p.
s=1	s=1
5
Published as a conference paper at ICLR 2020
Proof. Let d = d1d2 …dp. Let j ∈ [d] and let aι,...,ap be such that I(j) = [aι, a2,…，ap].
tt	t
diag(εI + Xgsg>)[j,j]p = (ε + Xgs[a1,a2,…，ap]2)p ≤ Πp=ι(ε + X X gs[I]2).
s=1	s=1	s=1 I:Ii =ai
Taking the pth root on both sides,
tt	t
diag(εI + Xgsg>)[j,j] ≤ ∏P=ι(ε + X X gs[I]2)1/p = ∏P=ι(ε + X G：[ai,ai])1/p
t
=唠 =1(εIdi + X GS)1∕p[j,j]∙
s=1
Taking square roots of the above inequality yields the lemma.	□
4.3	Regret bound
The following lemma bounds the regret in terms of the quadratic norms in the regularizer and its
dual:
Lemma 4.4. Denote by Ht the d-by-d diagonal matrix whose entries are (I-1(δt))-1. Take η1 =
...=ηt := η to be the ConStant learning rate, and let x* be the loss minimizing choice in hindsight.
Then, the regret of Algorithm 1 is bounded by
1T	T
2η X(Hxt- x*kHt - kxt+ι - x*kHt)+ 2 X(HgtkHt)2.
Proof. By the definition of Algorithm 1, for any x*,
xt+1 - x* = xt - x* - ηHt-1gt, and Ht(xt+1 - x*) = Ht(xt - x*) - ηgt.
Taking the inner product of the above vectors,
(xt+1 - x*)>Ht(xt+1 - x*) = (xt - x*)>Ht(xt - x*) - 2η(xt - x*)>gt + η2gt>Ht-1gt.
Rearranging and dividing by 2η, we have
gt (Xt- x*) = 2n(kxt- χ*kHt - kxt+ι- χ*kHt) + 2(HgtkHt)2.
Using convexity of the loss functions and summing over t yields the lemma.	□
Next, we present a lemma that is later used to bound the second term in the regret.
Lemma 4.5. (Gupta et al., 2018) Let g1,g2,…，gτ be a Sequence of vectors, and let Mt =
diag(Ptx=1 gs gs> ). Let S+ denote the set of diagonal positive definite matrices. Given a func-
tion Φ over S+, define At = argminA∈S+ {Mt • A-1 + Φ(A)} and assume that the minimum is
attained for all t. Then
TT
X(kgtk*At)2 ≤ X(kgtk*AT)2 + Φ(AT) - Φ(A0).
We proceed to prove Theorem 4.1.
Proof. (Theorem 4.1) Recall the definition of Ht = Xp=《几 + P：=i G：)1/2p, we bound the first
term in Lemma 4.4 as follows,
TT
X(kxt - x*k2Ht - kxt+1 - x*k2Ht) ≤ X(xt -x*)T(Ht - Ht-1)(xt -x*) + kx1 - x*k2H1
t=1	t=2
T
≤D∞2 X Tr(Ht - Ht-1) + D∞2 Tr(H1) = D∞2 Tr(HT)
t=2
6
Published as a conference paper at ICLR 2020
The last inequality is due to the fact that Ht-ι W Ht. Recall Ht = diag(εI + P：=i gsg>)1/2, and
by Lemma 4.3, Ht W Ht. Now we use Lemma 4.5 with function Φ(A) = Tr(A) + εTr(A-1).
Let Mt = diag(Pts=1 gsgs> ) and let S+ denote the set of diagonal positive definite matrices, we
have argmi□A∈s+ {Mt • A-1 + Φ(A)} = argmi□A∈s+ Tr(H2A-1 + A) = Ht. The last equality
can be derived by minimizing each diagonal entry of H individually.
By Lemma 4.5,
TT	T
X(kgtkHt )2 ≤ X(kgtkHT )2 + Φ(HT) - Φ(H0)≤ X(kgtkH T )2 + Φ(HT)
T
=diag(X gtg>) • H-1 + Tr(HT) + εTr(H-1 ) = 2Tr(HT)
t=1
We can take the diagonal of PL gtg> in the first equality since HT1 is a diagonal matrix as well.
We proceed to bound the second term in Lemma 4.4. Since Ht W Ht and both Ht and Ht are
full-rank, H-1 占 H-1. Therefore PT=1(kgtkHt)2 ≤ PT=I(kgtkHt)2 ≤ 2Tr(HT). Summing UP
the two terms and taking η = D∞ 丫Tr(HT) , We conclude that the regret of Algorithm 1 is bounded
√2Tr(2HT)__________________________________
by 2ηD∞Tr(HT) + ηTr(HT) = D∞^2Tr(Hτ)Tr(H^t).	□
5 Experiments
In this section, we Provide several emPirical studies on extreme tensoring. Our main exPeriment
interPolates between the memorylessness of SGD and the full memory consumPtion of AdaGrad
on a large-scale language model; we additionally isolate the effects of Preconditioner exPressivity
in a synthetic exPeriment of the same form, and Provide a Parallel CIFAR-10 exPeriment in the
aPPendix.
To accomPany the main exPerimental result, we Provide emPirical measurements of the comPetitive
ratio in the regret bound, as well as a quick comParison in which the memory savings are used to
train a larger model.
5.1	Memory-performance tradeoff in large-scale NLP
Our main emPirical study focuses on large-scale language modeling with the Transformer architec-
ture (Vaswani et al., 2017) on the Google Billion Words (GBW) dataset (Chelba et al., 2013), and
the results are shown in Figure 2. We use the PiPeline from the oPen-source Tensor2Tensor Package
(Vaswani et al., 2018) for PreProcessing the GBW dataset, and use the base Transformer architec-
ture in the same rePository as our base architecture. The decoder-only model has 6 identical layers
with hidden dimension dmodel = 512, and feedforward dimension dff = 2048. In addition, the
weights are shared between the embedding and softmax layers, and the model has a total of 〜35M
Parameters.
For our experiments, we use a learning rate schedule of ηt = C ∙ min(10-6 ∙ t, √1t), and C is a
hyPerParameter we tune for each exPeriment. The learning rate schedule is the same as the one used
in (Shazeer and Stern, 2018): a linear warmup stage, followed by inverse square root decay. We
train each model for 500K steps on GPUs, with a max sequence length of 256 tokens, and a max
number of 4096 tokens in a batch. Global learning rates are selected by hyperparameter search.
Except in our comparison with Adam, we do not use momentum in our extreme tensoring experi-
ments. Momentum incurs an overhead memory cost linear in the model dimension. In addition, we
empirically tested the benefit of exponentially decaying the second moment estimator (β2 < 1 in
the language of Adam and Adafactor). We found that in language modeling experiments, decaying
the second moment estimator did not contribute to better performance; on the other hand, the vision
experiments in the appendix do use this decay (β2 = 0.99).
7
Published as a conference paper at ICLR 2020
Figure 2: Memory-performance tradeoff for
GBW language modeling with a Transformer
network. Cyan points: performance after dou-
bling model size using optimizer RAM savings.
Optimizer	Parameter count	Final PPL
AdaGrad	3.5 × 107	^4TTT8
^ET1	1.2 × 105	-39.84
^ET2	1.0 × 104	ɪl^
ET3	5.0 × 103	^457I9
ET ∞	^90	-47.94
SGD		-49.80
Adam	7.0 × 107	38.47
Adafactor	1.2 × 105	38.86
Table 1: Performance and memory comparison
between adaptive optimizers for GBW language
modeling with a Transformer network.
Extreme tensoring gives us a class of optimizers which interpolate smoothly between AdaGrad
and SGD. Between these endpoints, we choose three levels of extreme tensoring (denoted by
ET{1, 2, 3}), with tensor index dimensions fully specified in the appendix. While extreme ten-
soring beyond level 2 have limited memory savings in most scenarios, this family of optimizers
allow us to have additional interpolating points to investigate the memory tradeoff. The dark blue
dots in Figure 2 records the training performance for these interpolating algorithms. Note that the
horizontal scale is logarithmic, so extreme tensoring saves memory by orders of magnitude. We
also provide training results for Adam (which consumes more memory for first-moment estimation)
and Adafactor (similar to ET1 but with a different step size scaling) in Table 1, for completeness;
however, these are not part of the interpolation study.
To provide an additional interpolating point, we include in the comparison a closely related algo-
rithm, which selects a single learning rate per parameter group (e.g. embedding, bias, convolutional
layer); thus, the preconditioner is a tensor sum of scalar multiples of the identity matrix. Each such
scalar is chosen to be the inverse square root of the sum of squared `2 norms of the parameter group
over time; it is easily seen that this achieves the same regret as online gradient descent (Zinkevich,
2003), so we include it as the least granular adaptive optimizer, and call it ET∞. Note that this
algorithm is known, and Ward et al. (2018) in particular provide a fine-grained analysis.
Finally, in Appendix 6, we re-run the ET{1, 2, 3} experiments with the SM3 update rule (Anil et al.,
2019), finding the same monotonic trend. In addition to model performance, we also provide in the
appendix wall clock comparisons for the algorithms we have benchmarked.
5.2	Doubling the memory allowance
As an extension of the above comparison, we argue that the memory consumption freed up by
choosing a compressed adaptive optimizer can be usefully reallocated to training a larger model.
We double the number of layers of the Transformer network used in the previous section, keeping
all embedding sizes the same; this results in a network with 〜56M parameters.
Results are shown in Figure 2 and Table 2. Table 2 reports performance from the larger model,
where final perplexities are given with the same running time allowance as the corresponding main
experiment (middle column), as well as the same iteration count (500K steps; right column). The
light blue dots in Figure 2 plots the final evaluation perplexity of the larger model in the third column
of Table 2 for a more intuitive visualization. Holding the memory consumption constant, we find
that it pays off to use a larger model rather than an adaptive optimizer that stores all of the second-
moment accumulators. Training the larger model with extreme tensoring consumes essentially the
same amount of memory as training the original model with Adam or AdaGrad, but the performance
improves significantly. Even holding the running time constant, the larger models are competitive
with the fully-converged smaller ones.
8
Published as a conference paper at ICLR 2020
Optimizer	Final ppl. (time)	Finalppl. (iters)
^ET1	19:25	16:23
^ET2	^43781	-40.04
ET3	^44^	-40.45
ET ∞	42.95	41.68
Table 2: Comparison of memory-efficient op-
timizers on a double-sized model, so that to-
tal memory consumption is lower than that of
AdaGrad/Adam on the smaller model. Final
perplexities are given with the same running
time allowance as the corresponding main ex-
periment (middle column), as well as the same
iteration count (500K steps; right column).
9 8 7 6 5 4
Oooooo
,Jzμn6ΘJ ⅛ueJJ.
Figure 3: Comparison of quantities in the nu-
merical regret bounds. Since the vertical scale
is logarithmic, the multiplicative regret bound
gap compared to AdaGrad is half the height dif-
ference between blue and red bars.
Ss-a*j-60- u9a-ndod
IO0
o
IO2
IO1
200	400	600	800	1000
Iteration
1 O
O O
1 1
SSO- U-ndod -eu
Figure 4: Training curves and final loss comparison for a convex problem with synthetic data. Left:
Loss curve for each optimizer. Right: Final loss vs. optimizer parameter count.
5.3	Empirical measurements of the regret bound
An appealing part of our regret analysis is the interpretability of the trace quantities in Theorem 4.1.
Figure 3 shows a comparison of traces Tr(HT) and Tr(HT) of the final regularizers in the language
modeling experiment. Then, by Theorem 4.1 and Corollary 4.2, the upper bound for AdaGrad's
regret scales with the latter quantity, while that of extreme tensoring scales with the geometric mean
of the two traces. Note that the vertical scale of the figure is logarithmic, so the multiplicative regret
bound gap compared to AdaGrad is half the height difference between blue and red bars.
Intriguingly, the multiplicative gap, which depends on the loss function and training trajectory via
the sequence of gradients, appears to be very small in practice. In the case of one-level extreme
JTr(HT)∕Tr(HT) ≈ 5.7.
tensoring, we have
5.4	Comparison on synthetic data
In this section, we exhibit a simple experiment with a convex optimization problem, in which there
is a clear tradeoff between preconditioner quality and expressivity. We generated Gaussian data
Xi ∈ R512 with a large condition number (〜104), and a Gaussian matrix W ∈ R10×512. Labels
are generated according to the log-linear model Pr[yi = j] a exp((Wχi j). Then, the optimization
problem in W is to minimize the empirical negative log-probability under the same model. Our
findings are robust with respect to the batch size; we use the full gradient on 104 samples in our
plots for clarity.
As in the large-scale experiment, we use successively deeper tensor factorizations of the precondi-
tioner, along the feature dimension of the matrix W. For depths 1, 2, and 3, we choose tensor indices
of dimensions (10, 512), (10,16,32), and (10,8, 8, 8), respectively. Global learning rates are tuned
individually. Results are shown in Figure 4.
9
Published as a conference paper at ICLR 2020
Algorithm	Adafactor	Adam	AdaGrad	ET1	ET2	ET3	ET∞	SGD
Wall-clock time (hrs)	20.64	18.26	17.84	18.61	20.17	24.82	19.00	17.50
Table 3: Wall clock comparisons for optimizing the base Transformer model.
5.5	Implementation Details
For completeness, we provide some details on our implementation of the algorithm. Table 3 gives
the wall clock comparisons for optimizing the base Transformer model. We point out that the
ET algorithms incur some computational overhead due to additional tensor operations. We have
not optimized our implementations, and expect that the time efficiency gap could be reduced with
architecture-aware optimizations. We trained the model on one V100 GPU, with parallel hyperpa-
rameter tuning on the global learning rate multiplier. We used the Tensor2Tensor (Vaswani et al.,
2018) framework for training, and Vizier (Golovin et al., 2017) for hyperparameter tuning.
6 Conclusion
We have introduced extreme tensoring, a modification to any second-moment-based adaptive opti-
mizer which drastically shrinks memory overhead. Our experiments characterize a performance-
memory tradeoff in the optimizer, and demonstrate the possibility of negligible memory overhead
without degrading convergence. Our regret analysis provides a competitive ratio with uncompressed
adaptive methods, giving an additional empirical lens on data-dependent preconditioner quality.
References
Naman Agarwal, Brian Bullins, Xinyi Chen, Elad Hazan, Karan Singh, Cyril Zhang, and Yi Zhang.
The case for full-matrix adaptive regularization. arXiv preprint arXiv:1806.02958, 2018.
Naman Agarwal, Rohan Anil, Elad Hazan, Tomer Koren, and Cyril Zhang. Disentangling adaptive
gradient methods from learning rates. arXiv preprint arXiv:2002.11803, 2020.
Rohan Anil, Vineet Gupta, Tomer Koren, and Yoram Singer. Memory-efficient adaptive optimiza-
tion for large-scale learning, 2019.
Irwan Bello, Barret Zoph, Vijay Vasudevan, and Quoc V Le. Neural optimizer search with rein-
forcement learning. arXiv preprint arXiv:1709.07417, 2017.
Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Anima Anandkumar. signsgd:
compressed optimisation for non-convex problems. arXiv preprint arXiv:1802.04434, 2018.
Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony
Robinson. One billion word benchmark for measuring progress in statistical language modeling.
arXiv preprint arXiv:1312.3005, 2013.
Jinghui Chen and Quanquan Gu. Closing the generalization gap of adaptive gradient methods in
training deep neural networks. arXiv preprint arXiv:1806.06763, 2018.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal ofMachine Learning Research, 12:2121-2159, 2011.
Daniel Golovin, Benjamin Solnik, Subhodeep Moitra, Greg Kochanski, John Karro, and D Sculley.
Google vizier: A service for black-box optimization. In Proceedings of the 23rd ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining, pages 1487-1495. ACM,
2017.
Maya R Gupta, Samy Bengio, and Jason Weston. Training highly multiclass classifiers. The Journal
of Machine Learning Research, 15(1):1461-1492, 2014.
Vineet Gupta, Tomer Koren, and Yoram Singer. A unified approach to adaptive regularization in
online and stochastic optimization. arXiv preprint arXiv:1706.06569, 2017.
10
Published as a conference paper at ICLR 2020
Vineet Gupta, Tomer Koren, and Yoram Singer. Shampoo: Preconditioned stochastic tensor opti-
mization. In International Conference on Machine Learning, 2018.
Elad Hazan. Introduction to online convex optimization. Foundations and TrendsR in Optimization,
2(3-4):157-325, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages
770-778, 2016.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.
Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. Measuring the intrinsic dimension
of objective landscapes. arXiv preprint arXiv:1804.08838, 2018a.
Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss land-
scape of neural nets. In Advances in Neural Information Processing Systems, pages 6389-6399,
2018b.
Dong C Liu and Jorge Nocedal. On the limited memory bfgs method for large scale optimization.
Mathematical Programming, 45(1-3):503-528, 1989.
James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate
curvature. In International Conference on Machine Learning, pages 2408-2417, 2015.
James Martens, Jimmy Ba, and Matt Johnson. Kronecker-factored curvature approximations for
recurrent neural networks. 2018.
Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost.
In International Conference on Machine Learning, 2018.
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,
and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.
arXiv preprint arXiv:1701.06538, 2017.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26-
31, 2012.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor-
mation Processing Systems, pages 5998-6008, 2017.
Ashish Vaswani, Samy Bengio, Eugene Brevdo, Francois Chollet, Aidan N. Gomez, Stephan
Gouws, Llion Jones, Eukasz Kaiser, Nal Kalchbrenner, Niki Parmar, Ryan Sepassi, Noam
Shazeer, and Jakob Uszkoreit. Tensor2tensor for neural machine translation. CoRR,
abs/1803.07416, 2018. URL http://arxiv.org/abs/1803.07416.
Rachel Ward, Xiaoxia Wu, and Leon Bottou. Adagrad stepsizes: Sharp convergence over nonconvex
landscapes, from any initialization. arXiv preprint arXiv:1806.01811, 2018.
Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701,
2012.
Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In
Proceedings of the 20th International Conference on Machine Learning (ICML-03), pages 928-
936, 2003.
11
Published as a conference paper at ICLR 2020
A CIFAR- 1 0 experiment
In this section, we evaluate the memory-performance trade-off of our proposed algorithm on the
CIFAR-10 dataset (Krizhevsky, 2009). Specifically, we compare the test accuracy of a ResNet-18
model (He et al., 2016) trained with SGD, Adam and 3 levels of extreme tensoring.
A.1 Setup
With each optimizer, the model is trained for 150 epochs with optimally tuned constant learning rate.
To prevent a 2X memory overhead, no momentum is used for any of the optimizers (i.e. β1 = 0 in
Adam). We use batch size 128, weight decay 5 × 10-4 for all the experiments in this section.
A.2 Tensor indices
We show our tensor decomposition scheme in the table below. Note that bias parameters are not
shown in each layer in the table since are treated as vectors and thus not decomposed at all.
Parameter Shape	Shape (ET1)	Shape (ET2)	Shape (ET3)
(64, 3, 3, 3)	一	(64, 3, 9)	(8, 8, 3, 9)	一	(8, 8, 3, 9)
(64, 64, 3, 3)	(64, 64, 9)	(8, 8, 8, 8, 9)	一	(8, 8, 8, 8, 9)
(128, 64, 3, 3)	(128, 64, 9)	(8,16, 8, 8, 9)一	(8,4,4, 8, 8, 9)
(128,128, 3, 3)-	(128,128, 9)	(8,16, 8,16, 9)	(8,4,4, 8, 4,4, 9^
(256,128, 3, 3)-	(256,128, 9)	(16,16, 8,16, 9)	(4,4,4, 4, 8,4,4；9―
(256, 256, 3, 3)-	(256, 256, 9)	(16,16,16,16, 9)	(4,4,4, 4, 4,4,4741厂
(512, 256, 3, 3)-	(512, 256, 9)	(32,16,16,16, 9)	(8,4,4, 4, 4,4,4741厂
(512, 512, 3, 3)-	(512, 512, 9)	(32,16, 32,16, 9)	(8,4,4, 4, 8,4,4741厂
(128,64,1,1)一	(128, 64)	(16, 8, 8, 8)	一	(4,4, 8, 8, 8)
(256,128,1,1)	(256,128)-	(16,16,16, 8)	(4,4,4, 4, 4,4, 8^^
(512,128,1,1)~~	(512,128)一	(32,16,16, 8)	(8,4,4, 4, 4,4, a)
Table 4: Tensor indices used for different levels of extreme tensoring for the ResNet-18 model on
Cifar-10.
A.3 Results
In this section, we report the best test errors seen in the first 150 epoch along with the parameter
count in the optimizer. A similar trend of trade-offs between performance and parameter count is
observed.
Optimizer	Parameter count	Final test error
Adam(β1 = 0)	1.1 × 107	^840
^ET1	2.3 × 104	^22
ET2	1.6 × 104	^849
ET3	1.5 × 104	^852
^ET∞	^62	^857
SGD	一	1	9.27
Table 5: Performance and memory comparison between adaptive optimizers for Cifar-10 classifica-
tion with a ResNet-18 network.
12
Published as a conference paper at ICLR 2020
0.070
0.085
0080
0.075
-eu-u
	SGD							
		•日	■		ΓΓ3			
					ΓΓ2			β Adam
								
								
IO1 102
IO9 IO4 IO5
Pa rametercount
IO7 IO8
Figure 5: Memory-performance comparison for CIFAR-10 classification with an 18-layer ResNet:
final test error vs. optimizer parameter count. Note that the horizontal scale is logarithmic.
B Language modeling experiment details
In this section, we provide more details on the tensor decomposition in the main empirical result.
The following table records parameter shapes for the Transformer model in our main experiments,
and the tensor shapes of the reshaped parameters.
Parameter	Parameter Shape	Shape (ET1)	Shape (ET2)	Shape (ET3)
Q,K,V, position-wise FF	(512,512)	(512, 512)-	(16, 32,16, 32)	(4, 4, 4, 8,4, 4, 4可一
Embedding Weights	(2000,512)	(2000,512)	(40, 50,16, 32)	(5, 8, 5,10,4,4,4‹^
Layer Norm	(512,)、	一	(512,)、	(16,32)	一	(4, 4, 4, 8)
FC Weights	(512, 2048)	(512, 2048)	(16, 32, 32, 64)	(4, 4, 4, 8,4, 8, 8可一
FC Bias	一	(2048,)	(2048,)	(32,64)	一	(4, 8, 8, 8)
FC Weights	(2048, 512)	(2048,512)	(32, 64,16, 32)	(4, 8, 8, 8,4, 4, 4可一
FC BiaS	一	(512,)	一	(512,)	(16,32)	一	(4, 4, 4, 8)
B.1 Wall Clock Comparisons
In this subsection we provide the wall clock comparisons for optimizing the base Transformer model.
We trained the model on one V100 GPU, with parallel hyperparameter tuning on the global learning
rate multiplier. We used the Tensor2Tensor (Vaswani et al., 2018) framework for training, and Vizier
(Golovin et al., 2017) for hyperparameter tuning.
Algorithm	Adafactor	Adam	AdaGrad	ET1	ET2	ET3	ET∞	SGD
Wall-clock time (hrs)	20.64	18.26	17.84	18.61	20.17	24.82	19.00	17.50
C SM3 Comparison
An alternate way to modulate the memory-performance tradeoff is to use the user-selectable parti-
tions in SM3 (Anil et al., 2019), which provide a alternate scheme for overestimating the AdaGrad
preconditioners. These are compatible with the tensor factorizations in our work, so we provide
in Table 6 the final test perplexities for the same Transformer experiments on the ET{1, 2, 3} de-
compositions (denoted as S{1, 2, 3}). We used SM3-II as reported in the pseudocode of Anil et al.
(2019). Note that we have not attempted to retune any hyperparameters other than the global scalar
learning rate multiplier. As such, these experiments should not be viewed as a comparison between
these families of optimizers; rather, these experiments corroborate a loss of adaptivity as the precon-
ditioner is constrained to an extremely low dimension.
13
Published as a conference paper at ICLR 2020
Optimizer	Parameter count	Final PpL
AdaGrad	3.5 X 107	1∏8
^^1	1.2 X 105	H59
^S2	1.0 × 104	^43^
^S3	5.0 X 103	"49.45
SGD	1	49.80
Table 6: Results of Transformer experiments with SM3 preconditioner estimates. The same mono-
tonic behavior is exhibited as when the extreme tensoring granularity is varied.
14