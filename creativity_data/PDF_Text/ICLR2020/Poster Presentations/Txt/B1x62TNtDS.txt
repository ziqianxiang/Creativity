Published as a conference paper at ICLR 2020
Understanding the Limitations of Variational
Mutual Information Estimators
Jiaming Song & Stefano Ermon
Stanford University
{tsong, ermon}@cs.stanford.edu
Ab stract
Variational approaches based on neural networks are showing promise for estimat-
ing mutual information (MI) between high dimensional variables. However, they
can be difficult to use in practice due to poorly understood bias/variance tradeoffs.
We theoretically show that, under some conditions, estimators such as MINE ex-
hibit variance that could grow exponentially with the true amount of underlying
MI. We also empirically demonstrate that existing estimators fail to satisfy basic
self-consistency properties of MI, such as data processing and additivity under in-
dependence. Based on a unified perspective of variational approaches, we develop
a new estimator that focuses on variance reduction. Empirical results on standard
benchmark tasks demonstrate that our proposed estimator exhibits improved bias-
variance trade-offs on standard benchmark tasks.
1	Introduction
Mutual information (MI) estimation and optimization are crucial to many important problems in
machine learning, such as representation learning (Chen et al., 2016; Zhao et al., 2018b; Tishby &
Zaslavsky, 2015; Higgins et al., 2018) and reinforcement learning (Pathak et al., 2017; van den Oord
et al., 2018). However, estimating mutual information from samples is challenging (McAllester &
Statos, 2018) and traditional parametric and non-parametric approaches (Nemenman et al., 2004;
Gao et al., 2015; 2017) struggle to scale up to modern machine learning problems, such as estimating
the MI between images and learned representations.
Recently, there has been a surge of interest in MI estimation with variational approaches (Barber
& Agakov, 2003; Nguyen et al., 2010; Donsker & Varadhan, 1975), which can be naturally com-
bined with deep learning methods (Alemi et al., 2016; van den Oord et al., 2018; Poole et al., 2019).
Despite their empirical effectiveness in downstream tasks such as representation learning (Hjelm
et al., 2018; Velickovic et al., 2018), their effectiveness for MI estimation remains unclear. In par-
ticular, higher estimated MI between observations and learned representations do not seem to indi-
cate improved predictive performance when the representations are used for downstream supervised
learning tasks (Tschannen et al., 2019).
In this paper, we discuss two limitations of variational approaches to MI estimation. First, we
theoretically demonstrate that the variance of certain estimators, such as MINE (Belghazi et al.,
2018), could grow exponentially with the ground truth MI, leading to poor bias-variance trade-offs.
Second, we propose a set of self-consistency tests over basic properties of MI, and empirically
demonstrate that all considered variational estimators fail to satisfy critical properties of MI, such as
data processing and additivity under independence. These limitations challenge the effectiveness of
these methods for estimating or optimizing MI.
To mitigate these issues, we propose a unified perspective over variational estimators treating vari-
ational MI estimation as an optimization problem over (valid) density ratios. This view highlights
the role of partition functions estimation, which is the culprit of high variance issues in MINE.
To address this issue, we propose to improve MI estimation via variance reduction techniques for
partition function estimation. Empirical results demonstrate that our estimators have much better
bias-variance trade-off compared to existing methods on standard benchmark tasks.
1
Published as a conference paper at ICLR 2020
2	Background and Related Work
2.1	Notations
We use uppercase letters to denote a probability measure (e.g., P , Q) and corresponding lowercase
letters to denote its density1 functions (e.g., p, q) unless specified otherwise. We use X, Y to denote
random variables with separable sample spaces denoted as X and Y respectively, and P(X) (or
P(Y)) to denote the set of all probability measures over the Borel σ-algebra on X (or Y).
Under Q ∈ P(X), the p-norm of a function r : X → R is defined as krkp := (R |r|pdQ)1/p
with krk∞ = limp→∞ krkp. The set of locally p-integrable functions is defined as Lp(Q) := {r :
X → R | krkp < ∞}. The space of probability measures wrt. Q is defined as ∆(Q) := {r ∈
L1 (Q) | krk1 = 1, r ≥ 0}; we also call this the space of “valid density ratios” wrt. Q. We use
P《Q to denote that P is absolutely continuous with respect to Q. We use IE to denote an
estimator for IE where we replace expectations with sample averages.
2.2	Variational Mutual Inforamtion Estimation
The mutual information between two random variables X and Y is the KL divergence between the
joint and the product of marginals:
I(X;Y)=DKL(P(X,Y)kP(X)P(Y))	(1)
which we wish to estimate using samples from P(X, Y ); in certain cases we may know the density
of marginals (e.g. P(X)). There are a wide range of variational approaches to variational MI esti-
mation. Variational information maximization uses the following result (Barber & Agakov, 2003):
Lemma 1 (Barber-Agakov (BA)). For two random variables X and Y :
I(X; Y) = SUp {Ep(X,Y) [logqφ(x∣y) - logp(x)] =: IBA®)}	⑵
qφ
where qφ : Y → P(X) is a valid conditional distribution over X given y ∈ Y and p(x) is the
probability density function of the marginal distribution P(X).
Another family of approaches perform MI estimation through variational lower bounds to KL di-
vergences. For example, the Mutual Information Neural Estimator (MINE, Belghazi et al. (2018))
applies the following lower bound to KL divergences (Donsker & Varadhan, 1975).
Lemma 2 (Donsker-Varadahn (DV)). ∀P, Q ∈ P(X) such that P	Q,
DKL(PkQ) =	sUp	{EP[T] - logEQ[eT] =: IMINE(T)} .	(3)
T∈L∞(Q)
One could set P = P(X, Y) and Q = P(X)P(Y), T as a parametrized neural network (e.g.
Tθ(x, y) parametrized by θ), and obtain the estimate by optimizing the above objective via stochastic
gradient descent over mini-batches. However, the corresponding estimator IMINE (where we replace
the expectations in Eq. (3) with sample averages) is biased, leading to biased gradient estimates;
Belghazi et al. (2018) propose to reduce bias via estimating the partition function EQ [eT] with
exponential moving averages of mini-batches.
The variational f -divergence estimation approach (Nguyen et al., 2010; Nowozin et al., 2016) con-
siders lower bounds on f -divergences which can be specialize to KL divergence, and subsequently
to mutual information estimation:
Lemma 3 (Nyugen et al. (NWJ)). ∀P, Q ∈ P(X ) such that P	Q,
DKL(PkQ) =	sUp	{EP[T] - EQ[eT-1] =: INWJ (T)}	(4)
T∈L∞ (Q)
andDKL(PkQ) = INWJ(T) when T =log(dP/dQ) +1.
1In the remainder of the paper, we slightly overload “density” for discrete random variables.
2
Published as a conference paper at ICLR 2020
The supremum over T is a invertible function of the density ratio dP/ dQ, so one could use this
approach to estimate density ratios by inverting the function (Nguyen et al., 2010; Nowozin et al.,
2016; Grover & Ermon, 2017). The corresponding mini-batch estimator (denoted as INWJ) is Unbi-
ased, so unlike MINE, this approach does not require special care to reduce bias in gradients.
Contrastive Predictive Coding (CPC, van den Oord et al. (2018)) considers the following objective:
ICPC(fθ) := EPn (X,Y)
1X log _fθ (X y)_
n ⅛1 1 Pn=I fθ (χi, yj)
(5)
where fθ : X × Y → R≥0 is a neUral network parametrized by θ and P n(X, Y ) denotes the joint
pdf for n i.i.d. random variables sampled from P(X, Y ). CPC generally has less variance bUt is
more biased becaUse its estimate does not exceed log n, where n is the batch size (van den Oord
et al., 2018; Poole et al., 2019). While one can fUrther redUce the bias with larger n, the nUmber
of evalUations needed for estimating each batch with fθ is n2, which scales poorly. To address the
high-bias issUe of CPC, Poole et al. proposed an interpolation between ICPC and INWJ to obtain
more fine-grained bias-variance trade-offs.
3	Variational mutual information estimation as optimization
OVER DENSITY RATIO S
In this section, we Unify several existing methods for variational mUtUal information estimation.
We first show that variational mUtUal information estimation can be formUlated as a constrained
optimization problem, where the feasible set is ∆(Q), i.e. the valid density ratios with respect to Q.
Theorem 1. ∀P, Q ∈ P(X) such that P Q we have
DKL(P kQ) = sup EP [log r]	(6)
r∈∆(Q)
where the supremum is achived when r = dP/ dQ.
We defer the proof in Appendix A. The above argUment works for KL divergence between general
distribUtions, bUt in this paper we focUs on the special case of mUtUal information estimation. For
the remainder of the paper, we Use P to represent the short-hand notation for the joint distribUtion
P(X, Y ) and Use Q to represent the short-hand notation for the prodUct of marginals P(X)P(Y ).
3.1	A summary of existing variational methods
From Theorem 1, we can describe a general approach to variational MI estimation:
1.	Obtain a density ratio estimate - denote the solution as r;
2.	Project r to be close to ∆(Q) - in practice We only have samples from Q, so we denote the
solution as Γ(r; Qn), where Qn is the empirical distribution ofn i.i.d. samples from Q;
3.	Estimate mutual information with EP [log Γ(r; Qn)].
We illustrate two examples of variational mutual information estimation that can be summarized
with this approach. In the case of Barber-Agakov, the proposed density ratio estimate is rBA =
qφ(x∣y)∕p(x) (assuming that P(X) is known), which is guaranteed to be in ∆(Q) because
Eq [qφ(x∣y)∕p(x)] = / qφ(x∣y)∕p(x)dP(x)dP(y) = 1,	Γba(tba, Qn)=『ba	⑺
for all conditional distributions qφ . In the case of MINE / Donsker-Varadahn, the logarithm of the
density ratio is estimated with Tθ(x, y); the corresponding density ratio might not be normalized,
so one could apply the following normalization for n samples:
EQn eTθ∕EQn[eTθ] =1,	ΓMINE(eTθ,Qn)=eθT∕EQn[eTθ]	(8)
where EQn [eTθ] (the sample average) is an unbiased estimate of the partition function EQ [eTθ];
ΓDV (eTθ, Qn) ∈ ∆(Q) is only true when n → ∞. Similarly, we show ICPC is a lower bound to MI
in Corollary 2, Appendix A, providing an alternative proof to the one in Poole et al. (2019).
3
Published as a conference paper at ICLR 2020
These examples demonstrate that different mutual information estimators can be obtained in a pro-
cedural manner by implementing the above steps, and one could involve different objectives at each
step. For example, one could estimate density ratio via logistic regression (Hjelm et al., 2018; Poole
et al., 2019; Mukherjee et al., 2019) while using INWJ or IMINE to estimate MI. While logistic re-
gression does not optimize for a lower bound for KL divergence, it provides density ratio estimates
between P and Q which could be used for subsequent steps.
Table 1: Summarization of variational estimators of mutual information. The ∈ ∆(Q) column
denotes whether the estimator is a valid density ratio wrt. Q. (X) means any parameterization is
valid; (n → ∞) means any parameterization is valid as the batch size grows to infinity; (tr → ∞)
means only the optimal parametrization is valid (infinite training cost).
Category	Estimator	Params	Γ(r; Qn)		∈ ∆(Q)
Gen.	T IBA IGM (Eq.(9))	qφ Pθ ,Pφ,Pψ	qφ(x∖y)∕p(x) pθ (χ, y)∕pφ(χ)pψ (y)		X tr → ∞
Disc.	f IMINE f ICPC ISMILE (Eq. (17))	Tθ fθ Tθ ,τ	eTθ (χ,y)∕EQn [eTθ (x,y)] fθ (x, y)/EPn(Y ) [fθ (x, y)] eTθ (x,y) ∕EQ [eclip(Tθ(x,y),-τ,τ)]	n → ∞ X n, τ → ∞
3.2	Generative and discriminative approaches to MI estimation
The above discussed variational mutual information methods can be summarized into two broad
categories based on how the density ratio is obtained.
•	The discriminative approach estimates the density ratio dP/dQ directly; examples include
the MINE, NWJ and CPC estimators.
•	The generative approach estimates the densities of P and Q separately; examples include
the BA estimator where a conditional generative model is learned. In addition, we describe
a generative approach that explicitly learns generative models (GM) for P (X, Y ), P (X)
andP(Y):
IGM(pθ,pφ,pψ) := EP [log pθ (x, y) - log pφ (x) - log pψ (y)],	(9)
where pθ, pφ, pψ are maximum likelihood estimates of P(X, Y ), P(X) and P(Y ) respec-
tively. We can learn the three distributions with generative models, such as VAE (Kingma
& Welling, 2013) or Normalizing flows (Dinh et al., 2016), from samples.
We summarize various generative and discriminative variational estimators in Table 1.
Differences between two approaches While both generative and discriminative approaches can
be summarized with the procedure in Section 3.1, they imply different choices in modeling, estima-
tion and optimization.
•	On the modeling side, the generative approaches might require more stringent assumptions
on the architectures (e.g. likelihood or evidence lower bound is tractable), whereas the
discriminative approaches do not have such restrictions.
•	On the estimation side, generative approaches do not need to consider samples from the
product of marginals P (X)P (Y ) (since it can model P(X, Y ), P(X), P(Y ) separately),
yet the discriminative approaches require samples from P(X)P(Y ); ifwe consider a mini-
batch of size n, the number of evaluations for generative approaches is Ω(n) whereas that
for discriminative approaches it could be Ω(n2).
•	On the optimization side, discriminative approaches may need additional projection steps to
be close to ∆(Q) (such as IMINE), while generative approaches might not need to perform
this step (such as IBA ).
4
Published as a conference paper at ICLR 2020
4	Limitations of existing variational estimators
4.1	Good discriminative estimators require exponentially large batches
In the INWJ and IMINE estimators, one needs to estimate the “partition function” EQ [r] for some
density ratio estimator r; for example, IMINE needs this in order to perform the projection step
ΓMINE(r, Qn) in Eq (8). Note that the INWJ and IMINE lower bounds are maximized when r takes
the optimal value r? = dP/ dQ. However, the sample averages IMINE and INWJ of EQ [r?] could
have a variance that scales exponentially with the ground-truth MI; we show this in Theorem 2.
Theorem 2. Assume that the ground truth density ratio r? = dP/ dQ and VarQ [r?] exist. Let Qn
denote the empirical distribution of n i.i.d. samples from Q and let EQn denote the sample average
over Qn. Then under the randomness of the sampling procedure, we have:
VarQ [EQn [r?]] ≥ eDK二Q)- 1	(10)
lim nVarQ [log EQn [r?]] ≥ eDKL (P kQ) - 1.	(11)
n→∞	n
We defer the proof to Appendix A. Note that in the theorem above, we assume the ground truth
density ratio r? is already obtained, which is the optimal ratio for NWJ and MINE estimators. As
a natural consequence, the NWJ and MINE estimators under the optimal solution could exhibit
variances that grow exponentially with the ground truth MI (recall that in our context MI is a KL
divergence). One could achieve smaller variances with some r 6= r? , but this guarantees looser
bounds and higher bias.
Corollary 1. Assume that the assumptions in Theorem 2 hold. Let Pm and Qn be the empirical
distributions of m i.i.d. samples from P andn i.i.d. samples from Q, respectively. Define
INmW,nJ :=EPm[logr?+1] - EQn [r?]	(12)
IMmI,NnE :=EPm[logr?] - log EQn [r?]	(13)
where r? = dP /dQ. Then under the randomness of the sampling procedure, we have ∀m ∈ N:
VarP,Q[INmW,nJ] ≥ (eDKL(PkQ) - 1)/n
lim nVarP,Q [IMmI,NnE] ≥ eDKL(PkQ) - 1.
n→∞
(14)
(15)
This high variance phenomenon has been empirically observed in Poole et al. (2019) (Figure 3) for
INWJ under various batch sizes, where the log-variance scales linearly with ML We also demonstrate
this in Figure 2 (Section 6.1). In order to keep the variance of IMINE and INWJ relatively constant
with growing MI, one would need a batch size of n = Θ(eDKL(PkQ)). ICPC has small variance, but
it would need n ≥ eDKL (P kQ) to have small bias, as its estimations are bounded by log n.
4.2	Self-consistency issues for mutual information estimators
If we consider X, Y to be high-dimensional, estimation of mutual information becomes more diffi-
cult. The density ratio between P(X, Y ) and P (X)P (Y ) could be very difficult to estimate from fi-
nite samples without proper parametric assumptions (McAllester & Statos, 2018; Zhao et al., 2018a).
Additionally, the exact value of mutual information is dependent on the definition of the sample
space; given finite samples, whether the underlying random variable is assumed to be discrete or
continuous will lead to different measurements of mutual information (corresponding to entropy
and differential entropy, respectively).
In machine learning applications, however, we are often more interested in maximizing or min-
imizing mutual information (estimates), rather than estimating its exact value. For example, if an
estimator is off by a constant factor, it would still be useful for downstream applications, even though
it can be highly biased. To this end, we propose a set of self-consistency tests for any MI estimator
I, based on properties of mutual information:
1.	(Independence) if X and Y are independent, then I(X; Y ) = 0;
5
Published as a conference paper at ICLR 2020
.	.	. .	.	一 4 , ______ 0，	___ 一 ------- - 0，________
2.	(Data processing) for all functions g,h, I(X; Y) ≥ I(g(X); h(Y)) and I(X; Y) ≈
I([X,g(X)]; [Y, h(Y)]) where [∙, ∙] denotes concatenation.
3.	(Additivity) denote X1, X2 as independent random variables that have the same distribution
^,, 一 一 一、 ^. .
as X (similarly define Y1,Y2), then .I([X1,X2]; [Y1, Y2]) ≈ 2 ∙ I(X, Y).
These properties holds under both entropy and differential entropy, so they do not depend on the
choice of the sample space. While these conditions are necessary but obviously not sufficient for
accurate mutual information estimation, we argue that satisfying them is highly desirable for appli-
cations such as representation learning (Chen et al., 2016) and information bottleneck (Tishby &
Zaslavsky, 2015). Unfortunately, none of the MI estimators we considered above pass all the self-
consistency tests when X, Y are images, as we demonstrate below in Section 6.2. In particular, the
generative approaches perform poorly when MI is low (failing in independence and data process-
ing), whereas discriminative approaches perform poorly when MI is high (failing in additivity).
5	Improved MI estimation via clipped density ratios
To address the high-variance issue in the INWJ and IMINE estimators, we propose to clip the density
ratios when estimating the partition function. We define the following clip function:
clip(v, l, u) = max(min(v, u), l)
(16)
For an empirical distribution of n samples Qn, instead of estimating the partition function via
EQn [r], we instead consider EQn [clip(r, e-τ, eτ)] where τ ≥ 0 is a hyperparameter; this is equiva-
lent to clipping the log density ratio estimator between -τ and τ.
We can then obtain a following estimator with smoothed partition function estimates:
ISMILE(Tθ,τ) := EP [Tθ(x, y)] - log EQ[clip(eTθ(x,y), e-τ, eτ)]	(17)
where Tθ is a neural network that estimates the log-density ratio (similar to the role of Tθ in IMINE).
We term this the Smoothed Mutual Information “Lower-bound” Estimator (SMILE) with hyperpa-
rameter τ; ISMILE converges to IMINE when τ → ∞. In our experiments, we consider learning the
density ratio with logistic regression, similar to the procedure in Deep InfoMax (Hjelm et al., 2018).
The selection of τ affects the bias-variance trade-off when estimating the partition function; with a
smaller τ, variance is reduced at the cost of (potentially) increasing bias. In the following theorems,
we analyze the bias and variance in the worst case for density ratio estimators whose actual partition
function is S for some S ∈ (0, ∞).
Theorem 3.	Let r(x) : X → R≥0 be any non-negative measurable function such that rdQ = S,
S ∈ (0, ∞) and r(x) ∈ [0, eK]. Define rτ (x) = clip(r(x), eτ, e-τ) for finite, non-negative τ. If
τ < K, then the bias for using rτ to estimate the partition function of r satisfies:
∖Eq[t] — Eq[rτ]| ≤ max e-τ|1 - Se-τ|,
1 - eκe-τ + S(eK - eτ)
eκ - e-τ
if τ ≥ K, then
∖EQ[r] - EQ[rτ]∖ ≤ e-τ(1 - Se-K).
Theorem 4.	The variance of the estimator EQn [rτ] (using n samples from Q) satisfies:
eτ	e-τ
Var[EQn [rτ]] ≤
(18)
We defer the proofs to Appendix A. Theorems 3 and 4 suggest that as we decrease τ, variance is
decreased at the cost of potentially increasing bias. However, if S is close to 1, then we could use
small τ values to obtain estimators where both variance and bias are small. We further discuss the
bias-variance trade-off for a fixed r over changes of τ in Theorem 3 and Corollary 3.
6
Published as a conference paper at ICLR 2020
6 Experiments
6.1 Benchmarking on multivariate Gaussians
First, we evaluate the performance of MI bounds on two toy tasks detailed in (Poole et al., 2019;
Belghazi et al., 2018), where the ground truth MI is tractable. The first task (Gaussian) is where
(x, y) are drawn from a 20-d Gaussian distribution with correlation ρ, and the second task (Cubic)
is the same as Gaussian but we apply the transformation y 7→ y3 . We consider three discrimina-
tive approaches (ICPC , INWJ, ISMILE) and one generative approach (IGM). For the discriminative
approaches, we consider the joint critic in (Belghazi et al., 2018) and the separate critic in (van den
Oord et al., 2018). For IGM we consider invertible flow models (Dinh et al., 2016). We train all
models for 20k iterations, with the ground truth mutual information increasing by 2 per 4k itera-
tions. More training details are included in Appendix B2.
10
0
Steps
CPC
10-
8-
Joint critic
Separable critic
True MI
log(bs)
NWJ
8
6
4
2
0
5000 10000 15000 20000
10
8
6
4
2
0
SMILE (τ = 5.0)
0	5000 10000 15000 20000
⅛> 6-
4-
1--------------------------------ɪ
■
2-
0--
0
5000 10000 15000 20000
StePS
NWJ
10
8
6
4
2
0
5000 10000 15000 20000
8
6
4
2
0
SMILE (τ = 5.0)
0	5000 10000 15000 20000
10
8
6
4
2
0
10
8
6
4
2
0
SMILE (τ = ∞)	GM (Flow)
0	5000 10000 15000 20000	0	5000 10000 15000 20000
SMILE (τ = ∞)	GM (Flow)
0	5000 10000 15000 20000	0	5000 10000 15000 20000

0
Figure 1:	Performance of mutual information estimation approaches on Gaussian (top row) and
Cubic (bottom row). Left two columns are ICPC and INWJ , next three columns are ISMILE with
τ = 1.0, 5.0, ∞ and the right column is IGM with flow models.
Figure 1 shows the estimated mutual information over the number of iterations. In both tasks,
ICPC has high bias and INWJ has high variance when the ground truth MI is high, whereas ISMILE
has relatively low bias and low variance across different architectures and tasks. Decreasing τ
in the SMILE estimator decreases variances consistently but has different effects over bias; for
example, under the joint critic bias is higher for τ = 5.0 in Gaussian but lower in Cubic. IGM with
flow models has the best performance on Gaussian, yet performs poorly on Cubic, illustrating the
importance of model parametrization in the generative approaches.
30-
ω 20-
Σ
10-
0-
2	4	6	8	10
MI
Figure 2:	Bias / Variance / MSE of various estimators on Cubic (right). We display more results for
Gaussian in Appendix B.
In Figure 2, we compare the bias, variance and mean squared error (MSE) of the discriminative
methods. We observe that the variance of INWJ increases exponentially with mutual information,
which is consistent with our theory in Corollary 1. On the other hand, the SMILE estimator is able
to achieve much lower variances with small τ values; in comparison the variance of SMILE when
τ = ∞ is similar to that of INWJ in Cubic. In Table 2, we show that ISMILE can have nearly two
orders of magnitude smaller variance than INWJ while having similar bias. Therefore ISMILE enjoys
lower MSE in this benchmark MI estimation task compared to INWJ and ICPC .
2We release our code in
7
Published as a conference paper at ICLR 2020
6.2 Self-consistency tests on Images
Same image	different images
Setting 2 (data processing)
Setting 3 (additivity)
Setting 1 (baseline)
Figure 3:	Three settings in the self-consistency experiments.
Next, We perform our proposed self-consistency tests on high-dimensional images (MNIST and
CIFAR10) under three settings, where the ground truth MI is difficult to obtain (if not impossible).
These settings are illustrated in Figure 3.
1.	The first setting is where X is an image and Y is the same image where we mask the bot-
tom rows, leaving the top t rows from X (t is selected before evaluation). The rationale
behind this choice of Y is twofold: 1) I(X; Y) should be non-decreasing with t; 2) it is eas-
ier (compared to low-d representations) to gain intuition about the amount of information
remaining in Y .
2.	In the second setting, X corresponds to two identical images, and Y to the top t1, t2 rows
of the two images (t1 ≥ t2); this considers the “data-processing” property.
3.	In the third setting, X corresponds to two independent images, and Y to the top t rows of
both; this considers the “additivity” property.
We compare four approaches: ICPC, IMINE, ISMILE and IGM . We use the same CNN architecture
for ICPC, IMINE and ISMILE, and use VAEs (Kingma & Welling, 2013) for IGM. We include more
experimental details and alternative image processing approaches in Appendix B.
Figure 4: Evaluation of I(X; Y)/I(X; , X). X is an image and Y contains the top t rows ofX.
Rows used
Baselines We evaluate the first setting with Y having varying number of rows t in Figure 4, where
the estimations are normalized by the estimated I(X; X). Most methods (except for IGM) predicts
zero MI when X and Y are independent, passing the first self-consistency test. Moreover, the
estimated MI is non-decreasing with increasing t, but with different slopes. As a reference, we show
the validation accuracy of predicting the label where only the top t rows are considered.
Data-processing In the second setting we set t2 = t1 - 3. Ideally, the estimator should satisfy
I ([X, X]; [Y, h(Y)])/I(X, Y) ≈ 1, as additional processing should not increase information. We
show the above ratio in Figure 5 under varying t1 values. All methods except for IMINE and IGM
performs well in both datasets; IGM performs poorly in CIFAR10 (possibly due to limited capacity
of VAE), whereas IMINE performs poorly in MNIST (possibly due to numerical stability issues).
8
Published as a conference paper at ICLR 2020
MNIST (Data PrOceSSing)
Q5
2 1
0e
5	10	15	20	25
Rows USed
CIFAR10 (Data PrOceSSing)
1.5-
1.0--
0.5-
10	15	20	25
Rows USed
-♦— CPC	—♦— GM (VAE)
→- MINE →- SMILE (τ = ∞)
-SMILE (τ = 5.0)
---Ideal
△ ，一_ -_______ ___ 、 . ^ , _
Figure 5: Evaluation of I([X, X];[Y, h(Y)])∕I(X, Y), where the ideal value is 1.
MNIST (Additivity)
1
- - -
4 3 2
Ow
10	15	20	25
Rows used
CIFAR10 (Additivity)
5Q5Q5
2 2 110
Ow
5	10	15	20	25	30
Rows used
Figure 6: Evaluation of I([Xι, X2]; [Yl, Y2])∕I(X, Y), where the ideal value is 2.
Additivity In the third setting, the estimator should double its value compared to the baseline with
the same t, i.e. I([X1, X2]; [Y1, Y2])∕I (X, Y) ≈ 2. Figure 6 shows the above ratio under different
values of t. None of the discriminative approaches worked well in this case except when t is very
small, when t is large this ratio converges to 1 (possibly due to initialization and saturation of the
training objective). IGM however, performs near perfectly on this test for all values of t.
7 Discussion
In this work, we discuss generative and discriminative approaches to variational mutual information
estimation and demonstrate their limitations. We show that estimators based on INWJ and IMINE
are prone to high variances when estimating with mini-batches, inspiring our ISMILE estimator that
improves performances on benchmark tasks. However, none of the approaches are good enough to
pass the self-consistency tests. The generative approaches perform poorly when MI is small (failing
independence and data-processing tests) while the discriminative approaches perform poorly when
MI is large (failing additivity tests).
These empirical evidences suggest that optimization over these variational estimators are not nec-
essarily related to optimizing MI, so the empirical successes with these estimators might have little
connections to optimizing mutual information. Therefore, it would be helpful to acknowledge these
limitations and consider alternative measurements of information that are more suited for modern
machine learning applications (Ozair et al., 2019; Tschannen et al., 2019).
Acknowledgements
This research was supported by AFOSR (FA9550-19-1-0024), NSF (#1651565, #1522054,
#1733686), ONR, and FLI. The authors would like to thank Shengjia Zhao, Yilun Xu and Lantao
Yu for helpful discussions.
References
Alexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. Deep variational information
bottleneck. arXiv preprint arXiv:1612.00410, December 2016.
David Barber and Felix V Agakov. The IM algorithm: a variational approach to information max-
imization. In Advances in neural information processing systems, pp. None. researchgate.net,
2003.
9
Published as a conference paper at ICLR 2020
Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, Aaron
Courville, and R Devon Hjelm. MINE: Mutual information neural estimation. arXiv preprint
arXiv:1801.04062, January 2018.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. InfoGAN:
Interpretable representation learning by information maximizing generative adversarial nets. In
D D Lee, M Sugiyama, U V Luxburg, I Guyon, and R Garnett (eds.), Advances in Neural Infor-
mation Processing Systems 29, pp. 2l72-2180. Curran Associates, Inc., 2016.
L Dinh, D Krueger, and Y Bengio. NICE: Non-linear independent components estimation. arXiv
preprint arXiv:1410.8516, 2014.
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real NVP. arXiv
preprint arXiv:1605.08803, May 2016.
Monroe D Donsker and SR Srinivasa Varadhan. Asymptotic evaluation of certain markov process
expectations for large time, i. Communications on Pure and Applied Mathematics, 28(1):1T7,
1975.
Shuyang Gao, Greg Ver Steeg, and Aram Galstyan. Efficient estimation of mutual information for
strongly dependent variables. In Artificial intelligence and statistics, pp. 277-286, 2015.
Weihao Gao, Sreeram Kannan, Sewoong Oh, and Pramod Viswanath. Estimating mutual informa-
tion for Discrete-Continuous mixtures. arXiv preprint arXiv:1709.06212, September 2017.
Aditya Grover and Stefano Ermon. Boosted generative models. arXiv preprint arXiv:1702.08484,
February 2017.
Irina Higgins, David Amos, David Pfau, Sebastien Racaniere, Loic Matthey, Danilo Rezende,
and Alexander Lerchner. Towards a definition of disentangled representations. arXiv preprint
arXiv:1812.02230, 2018.
R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam
Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation
and maximization. arXiv preprint arXiv:1808.06670, 2018.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, December 2014.
Diederik P Kingma and Max Welling. Auto-Encoding variational bayes. arXiv preprint
arXiv:1312.6114v10, December 2013.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet classification with deep con-
volutional neural networks. In F Pereira, C J C Burges, L Bottou, and K Q Weinberger (eds.),
Advances in Neural Information Processing Systems 25, pp. 1097-1105. Curran Associates, Inc.,
2012.
Y LeCun, L eon Bottou, Y Bengio, and others. Gradient-Based learning applied to document recog-
nition. PROC. OF THE, 1998.
David McAllester and Karl Statos. Formal limitations on the measurement of mutual information.
arXiv preprint arXiv:1811.04251, 2018.
Sudipto Mukherjee, Himanshu Asnani, and Sreeram Kannan. Ccmi: Classifier based conditional
mutual information estimation. arXiv preprint arXiv:1906.01824, 2019.
Ilya Nemenman, William Bialek, and Rob De Ruyter Van Steveninck. Entropy and information in
neural spike trains: Progress on the sampling problem. Physical Review E, 69(5):056111, 2004.
X Nguyen, M J Wainwright, and M I Jordan. Estimating divergence functionals and the likelihood
ratio by convex risk minimization. IEEE transactions on information theory / Professional Tech-
nical Group on Information Theory, 56(11):5847-5861, November 2010. ISSN 0018-9448. doi:
10.1109/TIT.2010.2068870.
10
Published as a conference paper at ICLR 2020
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-GAN: Training generative neural sam-
plers using variational divergence minimization. arXiv preprint arXiv:1606.00709, June 2016.
Sherjil Ozair, Corey Lynch, Yoshua Bengio, Aaron van den Oord, Sergey Levine, and Pierre
Sermanet. Wasserstein dependency measure for representation learning. arXiv preprint
arXiv:1903.11780, March 2019.
Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration
by self-supervised prediction. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition Workshops,pp.16-17, 2017.
Ben Poole, Sherjil Ozair, Aaron van den Oord, Alexander A Alemi, and George Tucker. On varia-
tional bounds of mutual information. arXiv preprint arXiv:1905.06922, May 2019.
Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. arXiv
preprint arXiv:1503.02406, March 2015.
Michael Tschannen, Josip Djolonga, Paul K Rubenstein, Sylvain Gelly, and Mario Lucic. On mutual
information maximization for representation learning. arXiv preprint arXiv:1907.13625, July
2019.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-
tive coding. arXiv preprint arXiv:1807.03748, July 2018.
Petar Velickovic, William Fedus, William L Hamilton, Pietro Lio, Yoshua Bengio, and R De-
von Hjelm. Deep graph infomax. arXiv preprint arXiv:1809.10341, September 2018.
Shengjia Zhao, Hongyu Ren, Arianna Yuan, Jiaming Song, Noah Goodman, and Stefano Ermon.
Bias and generalization in deep generative models: An empirical study. In Advances in Neural
Information Processing Systems, pp. 10792-10801, 2018a.
Shengjia Zhao, Jiaming Song, and Stefano Ermon. The information autoencoding family: A la-
grangian perspective on latent variable generative models. arXiv preprint arXiv:1806.06514, June
2018b.
11
Published as a conference paper at ICLR 2020
A Proofs
A.1 Proofs in Section 3
Theorem 1. ∀P, Q ∈ P(X) such that P	Q we have
DKL(P kQ) = sup EP [log r]
r∈∆(Q)
(6)
where the supremum is achived when r = dP/ dQ.
Proof. For every T ∈ L∞ (Q), define rT
Varadhan inequality (Donsker & Varadhan, 1975)
T
E或£=],then rτ ∈ ∆(Q) and from the Donsker-
DKL(P kQ) = sup EP[T] -logEQ[eT]
T∈L∞ (Q)
(19)
Moreover, we have:
sup EP log
T∈L∞ (Q)
eT
EQ [eT]
sup EP [log rT]
rT ∈∆(Q)
(20)
dP
DKL(PkQ) = Ep[1ogdP - 1ogdQ] = EP IogdQ
which completes the proof.
Corollary 2. ∀P,Q ∈ P(X) such that P	Q, ∀fθ : X → R≥0 we have
I(X; Y ) ≥ ICPC(fθ) := EPn(X,Y)
1 IX 1	fθ (Xi, yi)
n ⅛ °g n Pn=ιfθ(xi, yj)
(21)
(22)
Proof.
nICPC(fθ) := EPn (X,Y)
EPn(X,Y)
n
X log
i=1
n
X log
i=1
.	fθ (χi, yi)
n Pn=I fθ(Xi, yj)
nfθ(Xi, yi)
Pn=ifθ (Xi, yj)
(23)
(24)
□
Since
EP(X)Pn(Y)
nfθ(x, y)
Pn=I fθ(x, yj)
1,
(25)
we can apply Theorem 1 to obtain:
nICPC (fθ) = EPn (X,Y )
i=1
n
n
X log
i=1
EP (Xi ,Y1n) log
nfθ(xi, yi)
Pn=if (Xi yj)
.nfθ(Xi, yi)
'Pn=I fθ(xi, yj)
(26)
(27)
≤	I(Xi;Y1n) =nI(X;Y)
i=1
(28)
n
where Y1n denotes the concatenation of n independent random variables (Y1 , . . . , Yn ) and
P(Xi,Y1n)=P(Xi,Yi)P(Y1i-1)P(Yin+1)
is the joint distribution of P (Xi, Y1n).
□
12
Published as a conference paper at ICLR 2020
A.2 Proofs in Section 4
Theorem 2. Assume that the ground truth density ratio r? = dP/ dQ and VarQ [r?] exist. Let Qn
denote the empirical distribution of n i.i.d. samples from Q and let EQn denote the sample average
over Qn. Then under the randomness of the sampling procedure, we have:
VarQ [EQn [r?]] ≥
eDKL(PkQ) - 1
nl→im∞ nVarQ [log EQn [r?]]
n
≥ eDKL(PkQ) - 1.
(10)
(11)
Proof. Consider the variance of r?(x) when X 〜Q:
eDKL(PkQ) - 1
(29)
(30)
(31)
(32)
where (29) uses the definition of variance, (30) uses the definition of Radon-Nikodym derivative to
change measures, (31) uses Jensen’s inequality over log, and (32) uses the definition of KL diver-
gences.
The variance of the mean ofn i.i.d. random variables then gives us:
VarQ[EQn[r]] = VarM ≥ eDKL(PkQ^	(33)
nn
which is the first part of the theorem.
As n → ∞, VarQ[EQn [r]] → 0, so we can apply the delta method:
VarQ [f (X)] ≈ (f0(E(X)))2VarQ [X]	(34)
Applying f = log and E[X] = 1 gives us the second part of the theorem:
lim nVarQ [log EQn [r]] = lim nVar[EQn [r]] ≥ eDKL (P kQ) - 1
n→∞	n	n→∞	n
which describes the variance in the asymptotic sense.
(35)
□
Corollary 1. Assume that the assumptions in Theorem 2 hold. Let Pm and Qn be the empirical
distributions ofm i.i.d. samples from P and n i.i.d. samples from Q, respectively. Define
INmW,nJ :=EPm[logr?+1] - EQn [r?]	(12)
IMmI,NnE :=EPm[logr?] - log EQn [r?]	(13)
where r? = dP /dQ. Then under the randomness of the sampling procedure, we have ∀m ∈ N:
VarP,Q[INmW,nJ] ≥ (eDKL(PkQ) - 1)/n
lim nVarP,Q[IMmI,NnE] ≥ eDKL(PkQ) - 1.
n→∞
(14)
(15)
Proof. Since Pm and Qn are independent, we have
Var[INmW,nJ] ≥ Var[EQn [r?]]	(36)
=Var[EQn[r?]] ≥ eDKL(PkQ)- 1	(37)
and
lim nVar[IMmI,nNE] ≥ lim nVar[log EQn [r?]] ≥ eDKL(PkQ) - 1	(38)
n→∞	MINE	n→∞	n
which completes the proof.
□
13
Published as a conference paper at ICLR 2020
A.3 Proofs in Section 5
Theorem 3. Let r(x) : X → R≥0 be any non-negative measurable function such that rdQ = S,
S ∈ (0, ∞) and r(x) ∈ [0, eK]. Define rτ (x) = clip(r(x), eτ, e-τ) for finite, non-negative τ. If
τ < K, then the bias for using rτ to estimate the partition function of r satisfies:
∣Eq[r] - EqKt]∣ ≤ max e-τ|1 — Se-τ|,
if τ ≥ K, then
1 - eκe-τ + S(eK - eτ)
eκ - e-τ
[Eq[『]-Eq[rτ]∣≤ e-τ(1 - SeT).
Proof. We establish the upper bounds by finding a worst case r to find the largest |Eq [r] - EQ [r/] |.
First, without loss of generality, we may assume that r(x) ∈ (-∞, e-τ] ∪ [eτ, ∞) for all x ∈ X.
Otherwise, denote Xτ (r) = {x ∈ X : e-τ < r(x) < eτ} as the (measurable) set where the r(x)
values are between e-τ and eτ . Let
Vr(r) = /	()r(x)dx ∈ (e-τ∣Xτ(r)∣,eτ∣Xτ(r)∣)
(39)
be the integral of r over Xτ (r). We can transform r(x) for all x ∈ Xτ (r) to have values only in
{e-τ, eτ} and still integrate to Vτ (r), so the expectation under Q is not changed.
Then we show that we can rescale all the values above eτ and below eτ to the same value without
changing the expected value under Q. We denote
K1=log I(r(x) ≤ e-τ)r(x)dQ(x) - log	I(r(x) ≤ e-τ)dQ(x)
K2 = log I (r(x) ≥ eτ)r(x)dQ(x) - log	I (r(x) ≥ eτ)dQ(x)
(40)
(41)
where eK1 and eK2 represents the mean of r(x) for all r(x) ≤ e-τ and r(x) ≥ eτ respectively. We
then have:
EQ [r] = eK1	I (r(x) ≤ e-τ)dQ(x) + eK2	I (r(x) ≥ eτ)dQ(x)
1=ZI(r(x)≤e-τ)dQ(x)+ZI(r(x)≥eτ)dQ(x)
so we can parametrize EQ [r] via K1 and K2 . Since EQ [r] = S by assumption, we have:
(42)
(43)
I (r(x) ≤ e-τ)dQ(x)
and from the definition of rτ (x):
eK2 - S
eK2 - e-Kι
(44)
EQ[rτ]
eK2 e-τ -Se-τ+Seτ
eK2 - e-K1
- e-K1eτ
:= g(K1 , K2)
(45)
;
We can obtain an upper bound once we find max g(K1, K2) and ming(K1, K2). First, we have:
∂g(K1, K2)	e-K1 eτ (eK2 - e-K1) - e-K1 (eK2 e-τ - Se-τ + Seτ - e-K1 eτ)
∂K1
(e
e-Kι(eτ - e-τ)(eK - S)
(eK2 - e-Ki )2
K2 - e-K1)2
≥0
(46)
∂g(K1, K2)	eK2 e-τ (eK2 - e-K1) - eK2 (eK2 e-τ - Se-τ + Seτ - e-K1 eτ)
∂K2
(e
eK2(eτ - e-τ)(e-Kι - S)
(eK2 - e-K )2
K2 - e-K1)2
≤0
(47)
14
Published as a conference paper at ICLR 2020
Therefore, g(K1, K2) is largest when K1 → ∞, K2 = τ and smallest when K1 = τ, K2 → ∞.
maxg(K,K2) = lim 1 - e Ke + S(e - e T) = S + e-τ - Se-2τ
K→∞	eτ - e-K
ming(K1,K2)= lim ee T -K+ S(f - T) =「
K→∞	eK - e-τ
Therefore,
|EQ[r] - EQ [rT]| ≤ max(| maxg(K1, K2) - S|, |S - min g(K1, K2)|)
=max (∣e-τ — Se-2τ∣, |S — e-τ∣)
(48)
(49)
(50)
(51)
The proof for Theorem 3 simply follows the above analysis for fixed K. When τ < K, we consider
the case when K1 → ∞, K2 = τ and K1 = τ, K2 = K ; when τ > K only the smaller values will
be clipped, so the increased value is no larger than the case where K1 → ∞, K2 = K:
eK S
—∙ eτ = e-τ (1 — Se-K)	(52)
eK
where eκ ≥ S from the fact that R r dQ = S.	口
Theorem 4. The variance of the estimator EQn [rT] (using n samples from Q) satisfies:
Var[EQn [rT]] ≤ e J	(18)
Proof. Since rT (x) is bounded between eT and e-T, we have
eT — e-T
Var[rτ] ≤ ——4——	(53)
Taking the mean of n independent random variables gives Us the result.	口
Combining Theorem 3 and 4 with the bias-variance trade-off argument, we have the following:
Corollary 3. Let r(x) : X → R≥0 be any non-negative measurable function such that rdQ = S,
S ∈ (0, ∞) and r(x) ∈ [0, eK]. Define rT (x) = clip(r(x), eT, e-T) for finite, non-negative τ and
EQn as the sample average of n i.i.d. samples from Q. If τ < K, then
Eq [(r - EQn [rτ ])2] ≤ max (e-τ |1 - Se-τ |, 1 - / ：K + Se - ^	+ e^-f: ；
e — e	4n
If τ ≥ K, then:
Eq[(r - EQn[rτ])2] ≤ e-2τ(1 - Se-κ)2 +	-； T	(54)
15
Published as a conference paper at ICLR 2020
B Additional Experimental Details
B.1	Benchmark Tasks
Tasks We sample each dimension of (x, y) independently from a correlated Gaussian with mean 0
and correlation of ρ, where X = Y = R20 . The true mutual information is computed as:
I(X, y) = - dlog (1 - P)	(55)
The initial mutual information is 2, and we increase the mutual information by 2 every 4k iterations,
so the total training iterations is 20k.
Architecture and training procedure For all the discriminative methods, we consider two types
of architectures - joint and separable. The joint architecture concatenates the inputs x, y, and then
passes through a two layer MLP with 256 neurons in each layer with ReLU activations at each layer.
The separaable architecture learns two separate neural networks for x and y (denoted as g(x) and
h(y)) and predicts g(x)>h(y); g and h are two neural networks, each is a two layer MLP with 256
neurons in each layer with ReLU activations at each layer; the output of g and h are 32 dimensions.
For the generative method, we consider the invertible flow architecture described in (Dinh et al.,
2014; 2016). pθ, pφ, pψ are flow models with 5 coupling layers (with scaling), where each layer
contains a neural network with 2 layers of 100 neurons and ReLU activation. For all the cases, we
use with the Adam optimizer (Kingma & Ba, 2014) with learning rate 5 × 10-4 and β1 = 0.9, β2 =
0.999 and train for 20k iterations with a batch size of 64, following the setup in Poole et al. (2019).
Additional results We show the bias, variance and mean squared error of the discriminative ap-
proaches in Table 2. We include additional results for ISMILE with τ = 10.0.
Gaussian	∣	Cubic
	MI	2	4	6	8	10	2	4	6	8	10
		CPC	0.25	0.99	2.31	4.00	5.89	0.72	1.48	2.63	4.20	5.99
	NWJ	0.12	0.30	0.75	2.30	2.97	0.66	1.21	2.04	3.21	4.70
Bias	SMILE (τ = 1.0)	0.15	0.30	0.32	0.18	0.03	0.47	0.77	1.16	1.64	2.16
	SMILE (τ = 5.0)	0.13	0.11	0.19	0.54	0.86	0.71	1.22	1.55	1.84	2.16
	SMILE (τ = 10.0)	0.14	0.21	0.22	0.11	0.19	0.70	1.28	1.83	2.44	3.02
	SMILE (τ = ∞)	0.15	0.21	0.22	0.12	0.22	0.71	1.29	1.82	2.35	2.81
	GM (Flow)	0.11	0.14	0.15	0.14	0.17	1.02	0.47	1.85	2.93	3.55
	CPC	0.04	0.04	0.02	0.01	0.00	0.03	0.04	0.03	0.01	0.01
	NWJ	0.06	0.22	1.36	16.50	99.0	0.04	0.10	0.41	0.93	3.23
Var	SMILE (τ = 1.0)	0.05	0.12	0.20	0.28	0.34	0.04	0.10	0.14	0.20	0.30
	SMILE (τ = 5.0)	0.05	0.11	0.19	0.31	0.51	0.04	0.07	0.12	0.18	0.26
	SMILE (τ = 10.0)	0.05	0.13	0.31	0.69	1.35	0.03	0.10	0.21	0.46	0.79
	SMILE (τ = ∞)	0.05	0.14	0.36	0.75	1.54	0.03	0.12	0.24	0.65	0.94
	GM (Flow)	0.05	0.10	0.13	0.16	0.19	0.56	0.72	0.92	1.02	1.02
	CPC	0.10	1.02	5.33	16.00	34.66	0.55	2.22	6.95	17.62	35.91
	NWJ	0.07	0.32	2.19	33.37	28.43	0.47	1.55	4.56	11.13	27.00
MSE	SMILE (τ = 1.0)	0.08	0.21	0.30	0.32	0.31	0.26	0.69	1.49	2.90	4.98
	SMILE (τ = 5.0)	0.07	0.13	0.22	0.57	1.26	0.54	1.56	2.53	3.58	4.92
	SMILE (τ = 10.0)	0.07	0.18	0.36	0.67	1.33	0.52	1.75	3.54	6.41	9.91
	SMILE (τ = ∞)	0.08	0.19	0.40	0.76	1.62	0.54	1.75	3.55	6.09	8.81
	GM (Flow)	0.07	0.11	0.14	0.17	0.22	1.65	0.91	4.36	9.70	13.67
Table 2: Bias, Variance and MSE of the estimators under the joint critic.
We show the bias, variance and MSE results in Figure 7. We also evaluate the variance of estimating
EQn [rτ] (partition function with clipped ratios) for different values of τ in the SMILE estimator in
Figure 8b. With smaller τ we seea visible decrease in terms of variance in this term; this is consistent
with the variance estimates in Figure 7, as there the variance of EPn [log r] is also considered.
16
Published as a conference paper at ICLR 2020
2	4	6	8	10
MI
GauSSian
2	4	6	8	10
MI
Cubic
2	4	6	8	10
MI
6 4
s-8
0 12
O - -
loo
1 1
8ueμe>
Figure 7: Bias / Variance / MSE of various estimators. on Gaussian (top) and Cubic (down).
(a) Additional benchmark results on MINE estimator.
Figure 8: Additional benchmark results.
100
GauSSian
12 3
- - -
Ooo
111
8u」e>
(b) Variance of EQn [rτ]
B.2	Self-consistency Experiments
Tasks We consider three tasks with the mutual information estimator I :
4 , ____ _	_.	.	..	...
1.	I(X; Y) where X is an image from MNIST ( CUn et al , 199 ) or CIFAR10 ( Knzhevsky
et al., 2012) and Y is the top t rows of X. To simplify architecture designs, we simply
mask oUt the bottom rows to be zero, see FigUre 3.
2.	I([X,X]; [Y; h(Y)]) where X is an image, Y is the top t rows of X, h(Y) is the top
(t - 3) rows of Y and [∙, ∙] denotes concatenation. Ideally, the prediction should be close
^. .
to I(X; Y).
3.	I([X1,X2], [Yι,Y2]) where Xi and X2 are independent images from MNIST or CIFAR10,
Y1 and Y2 are the top t rows of X1 and X2 respectively. Ideally, this prediction should be
close to 2 ∙ I(X; Y).
Architecture and training procedure We consider the same architecture for all the discriminative
approaches. The first layer is a convolutional layer with 64 output channels, kernel size of 5, stride
of2 and padding of2; the second layer is a convolutional layer with 128 output channels, kernel size
of 5, stride of 2 and padding of 2. This is followed another fully connected layer with 1024 neurons
and finally a linear layer that produces an output of 1. All the layers (except the last one) use ReLU
activations. We stack variables over the channel dimension to perform concatenation.
For the generative approach, we consider the following VAE architectures. The encoder architec-
ture is identical to the discriminative approach except the last layer has 20 outputs that predict the
mean and standard deviations of 10 Gaussians respectively. The decoder for MNIST is a two layer
MLP with 400 neurons each; the decoder for CIFAR10 is the corresponding transposed convolution
network for the encoder. All the layers (except the last layers for encoder and decoder) use ReLU ac-
tivations. For concatenation we stack variables over the channel dimension. For all the cases, we use
with the Adam optimizer (Kingma & Ba, 2014) with learning rate 10-4 and β1 = 0.9, β2 = 0.999.
17
Published as a conference paper at ICLR 2020
For IGM we train for 10 epochs, and for the discriminative methods, we train for 2 epochs, due to
numerical stability issues of IMINE .
Additional experiments on scaling, rotation and translation We consider additional benchmark
experiments on MNIST where instead of removing rows, we apply alternative transformations such
as random scaling, rotation and translations. For random scaling, we upscale the image randomly
by 1x to 1.2x; for random rotation, we randomly rotate the image between ±20 degrees; for random
translation, we shift the image randomly by no more than 3 pixels horizontally and vertically. We
consider evaluating the data processing and additivity properties, where the ideal value for the former
is no more than 1, and the ideal value for the latter is 2. From the results in Table 3, none of the
considered approaches achieve good results in all cases.
	I		CPC	MINE	GM (VAE)	SMILE (τ = 5.0)	SMILE (τ =	∞)
Scaling	1.00	1.03	1.12	1.19	1.04	
Data-ProCessing Rotation	1.00	1.30	1.13	1.03	1.27	
Translation	1.00	1.28	1.01	1.07	1.08	
Scaling	1.00	1.55	1.89	1.04	1.18	
Additivity	Rotation	1.00	2.09	1.58	1.50	1.78	
Translation	1.00	1.41	1.28	1.32	1.33	
Table 3: Self-consistency experiments on other image transforms.
18