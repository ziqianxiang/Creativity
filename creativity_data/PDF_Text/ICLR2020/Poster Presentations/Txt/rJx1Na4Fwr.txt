Published as a conference paper at ICLR 2020
MACER: Attack-free and Scalable Robust
Training via Maximizing Certified Radius
Runtian Zhai1*, Chen Dan2*, Di He1*, Huan Zhang3,
Boqing Gong4, Pradeep Ravikumar2, Cho-Jui Hsieh3 & Liwei Wang1
1Peking University 2CMU 3UCLA 4Google *Equal Contribution
{zhairuntian, di_he}@pku.edu.cn, {cdan, Pradeepr}@cs.cmu.edu,
huanzhang@ucla.edu, boqinggo@outlook.com, chohsieh@cs.ucla.edu, wanglw@cis.pku.edu.cn
Ab stract
Adversarial training is one of the most popular ways to learn robust models but is
usually attack-dependent and time costly. In this paper, we propose the MACER
algorithm, which learns robust models without using adversarial training but per-
forms better than all existing provable l2-defenses. Recent work (Cohen et al.,
2019) shows that randomized smoothing can be used to provide a certified l2 ra-
dius to smoothed classifiers, and our algorithm trains provably robust smoothed
classifiers via MAximizing the CErtified Radius (MACER). The attack-free char-
acteristic makes MACER faster to train and easier to optimize. In our experi-
ments, we show that our method can be applied to modern deep neural networks
on a wide range of datasets, including Cifar-10, ImageNet, MNIST, and SVHN.
For all tasks, MACER spends less training time than state-of-the-art adversarial
training algorithms, and the learned models achieve larger average certified radii.
Our code is available at https://github.com/RuntianZ/macer.
1	Introduction
Modern neural network classifiers are able to achieve very high accuracy on image classification
tasks but are sensitive to small, adversarially chosen perturbations to the inputs (Szegedy et al., 2013;
Biggio et al., 2013). Given an image x that is correctly classified by a neural network, a malicious
attacker may find a small adversarial perturbation δ such that the perturbed image x + δ, though
visually indistinguishable from the original image, is assigned to a wrong class with high confidence
by the network. Such vulnerability creates security concerns in many real-world applications.
Researchers have proposed a variety of defense methods to improve the robustness of neural net-
works. Most of the existing defenses are based on adversarial training (Szegedy et al., 2013; Madry
et al., 2017; Goodfellow et al., 2015; Huang et al., 2015; Athalye et al., 2018; Ding et al., 2020).
During training, these methods first learn on-the-fly adversarial examples of the inputs with multiple
attack iterations and then update model parameters using these perturbed samples together with the
original labels. However, such approaches depend on a particular (class of) attack method. It cannot
be formally guaranteed whether the resulting model is also robust against other attacks. Moreover,
attack iterations are usually quite expensive. As a result, adversarial training runs very slowly.
Another line of algorithms trains robust models by maximizing the certified radius provided by ro-
bust certification methods (Weng et al., 2018; Wong & Kolter, 2018; Zhang et al., 2018; Mirman
et al., 2018; Wang et al., 2018; Gowal et al., 2018; Zhang et al., 2019c). Using linear or convex
relaxations of fully connected ReLU networks, a robust certification method computes a “safe ra-
dius” r for a classifier at a given input such that at any point within the neighboring radius-r ball
of the input, the classifier is guaranteed to have unchanged predictions. However, the certification
methods are usually computationally expensive and can only handle shallow neural networks with
ReLU activations, so these training algorithms have troubles in scaling to modern networks.
In this work, we propose an attack-free and scalable method to train robust deep neural networks.
We mainly leverage the recent randomized smoothing technique (Cohen et al., 2019). A randomized
smoothed classifier g for an arbitrary classifier f is defined as g(x) = Enf (X + η), in which η 〜
N(0, σ2I). While Cohen et al. (2019) derived how to analytically compute the certified radius of the
randomly smoothed classifier g, they did not show how to maximize that radius to make the classifier
1
Published as a conference paper at ICLR 2020
g robust. Salman et al. (2019) proposed SmoothAdv to improve the robustness ofg, but it still relies
on the expensive attack iterations. Instead of adversarial training, we propose to learn robust models
by directly taking the certified radius into the objective. We outline a few challenging desiderata any
practical instantiation of this idea would however have to satisfy, and provide approaches to address
each of these in turn. A discussion of these desiderata, as well as a detailed implementation of our
approach is provided in Section 4. And as we show both theoretically and empirically, our method
is numerically stable and accounts for both classification accuracy and robustness.
Our contributions are summarized as follows:
• We propose an attack-free and scalable robust training algorithm by MAximizing the CErti-
fied Radius (MACER). MACER has the following advantages compared to previous works:
-	Different from adversarial training, We train robust models by directly maximizing
the certified radius without specifying any attack strategies, and the learned model
can achieve provable robustness against any possible attack in the certified region.
Additionally, by avoiding time-consuming attack iterations, our proposed algorithm
runs much faster than adversarial training.
-	Different from other methods (Wong & Kolter, 2018) that maximize the certified ra-
dius but are not scalable to deep neural netWorks, our method can be applied to archi-
tectures of any size. This makes our algorithm more practical in real scenarios.
• We empirically evaluate our proposed method through extensive experiments on Cifar-10,
ImageNet, MNIST, and SVHN. On all tasks, MACER achieves better performance than
state-of-the-art algorithms. MACER is also exceptionally fast. For example, on ImageNet,
MACER uses 39% less training time than adversarial training but still performs better.
2 Related work
Neural networks trained by standard SGD are not robust - a small and human imperceptible pertur-
bation can easily change the prediction of a netWork. In the White-box setting, methods have been
proposed to construct adversarial examples with small '∞ or '2 perturbations (Goodfellow et al.,
2015; Madry et al., 2017; Carlini & Wagner, 2016; Moosavi-Dezfooli et al., 2015). Furthermore,
even in the black-box setting where the adversary does not have access to the model structure and
parameters, adversarial examples can be found by either transfer attack (Papernot et al., 2016) or
optimization-based approaches (Chen et al., 2017; Rauber et al., 2017; Cheng et al., 2019). It is thus
important to study how to improve the robustness of neural networks against adversarial examples.
Adversarial training So far, adversarial training has been the most successful robust training
method according to many recent studies. Adversarial training was first proposed in Szegedy et al.
(2013) and Goodfellow et al. (2015), where they showed that adding adversarial examples to the
training set can improve the robustness against such attacks. More recently, Madry et al. (2017) for-
mulated adversarial training as a min-max optimization problem and demonstrated that adversarial
training with PGD attack leads to empirical robust models. Zhang et al. (2019b) further decomposed
the robust error as the sum of natural error and boundary error for better performance. Finally, Gao
et al. (2019) proved the convergence of adversarial training. Although models obtained by adversar-
ial training empirically achieve good performance, they do not have certified error guarantees.
Despite the popularity of PGD-based adversarial training, one major issue is that its speed is too
slow. Some recent papers propose methods to accelerate adversarial training. For example, Free-
m (Shafahi et al., 2019) replays an adversarial example several times in one iteration, YOPO-m-n
(Zhang et al., 2019a) restricts back propagation in PGD within the first layer, and Qin et al. (2019)
estimates the adversary with local linearization.
Robustness certification and provable defense Many defense algorithms proposed in the past
few years were claimed to be effective, but Athalye et al. (2018) showed that most of them are based
on “gradient masking” and can be bypassed by more carefully designed attacks. It is thus important
to study how to measure the provable robustness of a network. A robustness certification algorithm
takes a classifier f and an input point x as inputs, and outputs a “safe radius” r such that for any δ
subject to kδk ≤ r, f (x) = f (x + δ). Several algorithms have been proposed recently, including
the convex polytope technique (Wong & Kolter, 2018), abstract interpretation methods (Singh et al.,
2018; Gehr et al., 2018) and the recursive propagation algrithms (Weng et al., 2018; Zhang et al.,
2
Published as a conference paper at ICLR 2020
2018). These methods can provide attack-agnostic robust error lower bounds. Moreover, to achieve
networks with nontrivial certified robust error, one can train a network by minimizing the certified
robust error computed by the above-mentioned methods, and several algorithms have been proposed
in the past year (Wong & Kolter, 2018; Wong et al., 2018; Wang et al., 2018; Gowal et al., 2018;
Zhang et al., 2019c; Mirman et al., 2018). Unfortunately, they can only be applied to shallow
networks with limited activation and run very slowly.
More recently, researchers found a new class of certification methods called randomized smoothing.
The idea of randomization has been used for defense in several previous works (Xie et al., 2017;
Liu et al., 2018) but without any certification. Later on, Lecuyer et al. (2018) first showed that if
a Gaussian random noise is added to the input or any intermediate layer. A certified guarantee on
small `2 perturbation can be computed via differential privacy. Li et al. (2018) and Cohen et al.
(2019) then provided improved ways to compute the `2 certified robust error for Gaussian smoothed
models. In this paper, we propose a new algorithm to train on these `2 certified error bounds to
significantly reduce the certified error and achieve better provable adversarial robustness.
3	Preliminaries
Problem setup Consider a standard classification task with an underlying data distribution pdata
over pairs of examples X ∈ X ⊂ Rd and corresponding labels y ∈ Y = {1,2,…，K}. Usually
Pdata is unknown and We can only access a training set S = {(χι,yι), ∙∙∙ , (χn,yn)} in which
(xi, yi) is i.i.d. drawn from pd&ta, i = 1,2,…，n. The empirical data distribution (uniform distri-
bution over S) is denoted by pd&ta. Let f ∈ F be the classifier of interest that maps any X ∈ X to
Y. Usually f is parameterized by a set of parameters θ, so we also write it as fθ.
We call X0 = X + δ an adversarial example of X to classifier fθ if fθ can correctly classify X but
assigns a different label to X0. Following many previous works (Cohen et al., 2019; Salman et al.,
2019), we focus on the setting where δ satisfies `2 norm constraint kδ k2 ≤ . We say that the model
fθ is l2-robust at (X, y) if it correctly classifies X as y and for any kδk2 ≤ , the model classifies X+δ
as y. In the problem of robust classification, our ultimate goal is to find a model that is l2 -robust at
(x, y) with high probability over (x, y)〜Pdata for a given e > 0.
Neural network In image classification we often use deep neural networks. Let uθ : X → RK be
a neural network, whose output at input X is a vector (uθ1(X), ..., uθK (X)). The classifier induced by
uθ (X) is fθ (X) = arg maxc∈Y ucθ (X).
In order to train θ by minimizing a loss function such as cross entropy, we always use a softmax layer
on uθ to normalize it into a probability distribution. The resulting network is z§ (∙; β) : X → P (K )1,
which is given by zθc(X; β) =eβuθ(x)/Pc0∈γ eβuθ (x), ∀c ∈ Y, β is the inverse temperature. For
simplicity, we will use zθ(X) to refer to zθ(X; β) when the meaning is clear from context. The vector
zθ(x) = (z1(x),…，ZK(x)) is commonly regarded as the “likelihood vector”, and Zc(X) measures
how likely input X belongs to class c.
Robust radius By definition, the l2-robustness of fθ at a data point (X, y) depends on the radius
of the largest l2 ball centered at X in which fθ does not change its prediction. This radius is called
the robust radius, which is formally defined as
inf	kX0 - Xk2 , when fθ (X) = y
R(fθ; χ,y) = f fθ(x0)=fθ(x)	(1)
0	, when fθ (X) 6= y
Recall that our ultimate goal is to train a classifier which is l2 -robust at (X, y) with high probability
over the sampling of (x, y)〜Pd&ta. Mathematically the goal can be expressed as to minimize the
expectation of the 0/1 robust classification error. The error is defined as
l-robust(fθ;x,y) := 1 - 1{R(fθ[χ,y)≥e},	⑵
and the goal is to minimize its expectation over the population
Le-robust(fθ):= E(x,y)^Pdatale-robust(fθ; Xly)	(3)
1The probability simplex in RK.
3
Published as a conference paper at ICLR 2020
It is thus quite natural to improve model robustness via maximizing the robust radius. Unfortunately,
computing the robust radius (1) of a classifier induced by a deep neural network is very difficult.
Weng et al. (2018) showed that computing the l1 robust radius of a deep neural network is NP-hard.
Although there is no result for the l2 radius yet, it is very likely that computing the l2 robust radius
is also NP-hard.
Certified radius Many previous works proposed certification methods that seek to derive a
tight lower bound of R(fθ ; x, y) for neural networks (see Section 2 for related work). We call
this lower bound certified radius and denote it by CR(fθ; x, y). The certified radius satisfies
0 ≤ CR(fθ; χ, y) ≤ R(fθ; χ, y) for any fθ, χ, y.
The certified radius leads to a guaranteed upper bound of the 0/1 robust classification error, which
is called 0/1 certified robust error. The 0/1 certified robust error of classifier fθ on sample (x, y) is
1/certifiedfθ； x, y) := 1 - 1{CR(fθ/,y)≥e}	(4)
i.e. a sample is counted as correct only if the certified radius reaches . The expectation of certified
robust error over (x, y)〜Pdata serves as a performance metric of the provable robustness:
Le-certified(fθ):= E(x,y)〜Pdatacertified(fθ; x，y)	(5)
Recall that CR(fθ; x, y) is a lower bound of the true robust radius, which immediately implies that
L0-certified(fθ) ≥ L0-1obust(fθ). Therefore, a small 0/1 certified robust error leads to a small 0/1
robust classification error.
Randomized smoothing In this work, we use the recent randomized smoothing technique (Cohen
et al., 2019), which is scalable to any architectures, to obtain the certified radius of smoothed deep
neural networks. The key part of randomized smoothing is to use the smoothed version of fθ, which
is denoted by gθ, to make predictions. The formulation of gθ is defined as follows.
Definition 1. For an arbitrary classifier fθ ∈ F and σ > 0, the smoothed classifier gθ of fθ is
defined as
gθ (X) = arg max Pn〜N(o,σ2ι)(fθ(X + η) = C)	⑹
c∈Y
In short, the smoothed classifier gθ(x) returns the label most likely to be returned by fθ when its
input is sampled from a Gaussian distribution N(x, σ2I) centered at x. Cohen et al. (2019) proves
the following theorem, which provides an analytic form of certified radius:
Theorem 1. (Cohen et al., 2019) Let fθ ∈ F, and η 〜N(0, σ2I). Let the smoothed classifier gθ
be defined asin (6). Let the ground truth of an input x bey. If gθ classifies x correctly, i.e.
Pη(fθ(x + η) = y) ≥ maxPη(fθ(x + η) = y0)
y0 6=y
Then gθ is provably robust at x, with the certified radius given by
CR(gθ; χ,y) = 2[Φ-1(Pn(fθ(χ + η) = y)) - Φ-1(maxPn(fθ(χ + η) = y0))]
=2[φ I(En 1{fθ(χ+n)=y}) - φ 1 (m=xEn 1{fθ(χ+n)=y0})]
where Φ is the c.d.f. of the standard Gaussian distribution.
(7)
(8)
4 Robust training via maximizing the certified radius
As we can see from Theorem 1, the value of the certified radius can be estimated by repeatedly
sampling Gaussian noises. More importantly, it can be computed for any deep neural networks.
This motivates us to design a training method to maximize the certified radius and learn robust
models.
To minimize the 0/1 robust classification error in (3) or the 0/1 certified robust error in (5), many
previous works (Zhang et al., 2019b; Zhai et al., 2019) proposed to first decompose the error. Note
that a classifier gθ has a positive 0/1 certified robust error on sample (x, y) if and only if exactly one
of the following two cases happens:
4
Published as a conference paper at ICLR 2020
•	gθ (x) 6= y, i.e. the classifier misclassifies x.
•	gθ(x) = y, but CR(gθ; x, y) < , i.e. the classifier is correct but not robust enough.
Thus, the 0/1 certified robust error can be decomposed as the sum of two error terms: a 0/1 classifi-
cation error and a 0/1 robustness error:
l-certified(gθ; x，y) = 1 - 1{CR(gθ∙,χ,y)≥e)
=1{gθ (x)=y}	+ 1{gθ (χ)=y,CR(gθ ;x,y)<e}
'------{z--}	'------------{z------------}
0/1 Classification Error	0/1 Robustness Error
(9)
4.1	Desiderata for objective functions
Minimizing the 0-1 error directly is intractable. A classic method is to minimize a surrogate loss
instead. The surrogate loss for the 0/1 classification error is called classification loss and denoted by
lC(gθ; x, y). The surrogate loss for the 0/1 robustness error is called robustness loss and denoted by
lR(gθ; x, y). Our final objective function is
l(gθ; x, y) = lC(gθ; x, y) + lR(gθ; x, y)	(10)
We would like our loss functions lC (gθ; x, y) and lR(gθ; x, y) to satisfy some favorable conditions.
These conditions are summarized below as (C1) - (C3):
•	(C1) (Surrogate condition): Surrogate loss should be an upper bound of the original er-
ror function, i.e. lC (gθ; x, y) and lR(gθ; x, y) should be upper bounds of 1{gθ(x)6=y} and
1{gθ (x)=y,CR(gθ ;x,y)<} , respectively.
•	(C2) (Differentiablity): lC (gθ; x, y) and lR(gθ; x, y) should be (sub-)differentiable with
respect to θ.
•	(C3) (Numerical stability): The computation oflC(gθ; x, y) and lR(gθ; x, y) and their (sub-
)gradients with respect to θ should be numerically stable.
The surrogate condition (C1) ensures that l(gθ; x, y) itself meets the surrogate condition, i.e.
l(gθ ； x,y) = IC (gθ ； x,y) + lR(gθ ； x,y) ≥ l0-Certified (ge; x,y)	(II)
Conditions (C2) and (C3) ensure that (10) can be stably minimized with first order methods.
4.2	Surrogate losses (for Condition C1)
We next discuss choices of the surrogate losses that ensure we satisfy condition (C1). The classifi-
cation surrogate loss is relatively easy to design. There are many widely used loss functions from
which we can choose, and in this work we choose the cross-entropy loss as the classification loss:
1{gθ(χ)=y} ≤ IC(gθ；x, y) := ICE(gθ(X), y)	(12)
For the robustness surrogate loss, we choose the hinge loss on the certified radius:
1{gθ (X)=y,CR(gθ ;x,y)<e}
≤ λ ∙ max {e + Z- CR(gθ； x, y), 0}∙ 1{gθ(x)=y} ：= 3(gθ； x, y)
(13)
where Z > 0 and λ ≥ 4. We use the hinge loss because not only does it satisfy the surrogate
condition, but also it is numerically stable, which we will discuss in Section 4.4.
4.3	Differentiable certified radius via soft randomized smoothing (for
Condition C2)
The classification surrogate loss in (12) is differentiable with respect to θ, but the differentiability
of the robustness surrogate loss in (13) requires differentiability of CR(gθ; x, y). In this section We
5
Published as a conference paper at ICLR 2020
will show that the randomized smoothing certified radius in (8) does not meet condition (C2), and
accordingly, we will introduce soft randomized smoothing to solve this problem.
Whether the certified radius (8) is sub-differentiable with respect to θ boils down to the differ-
entiablity of Eη1{fθ (x+η)=y}. Theoretically, the expectation is indeed differentiable. However,
from a practical point of view, the expectation needs to be estimated by Monte Carlo sampling
En 1{fθ(χ+η)=y} ≈ 1 Pk=I 1f (x+rlj)=y}, where ηj is i.i.d Gaussian noise and k is the number
of samples. This estimation, which is a sum of indicator functions, is not differentiable. Hence,
condition (C2) is still not met from the algorithmic perspective.
To tackle this problem, we leverage soft randomized smoothing (Soft-RS). In contrast to the original
version of randomized smoothing (Hard-RS), Soft-RS is applied to a neural network zθ (x) whose
last layer is Softmax. The soft smoothed classifier gθ is defined as follows.
Definition 2. Fora neural network zθ : X → P(K) whose last layer is softmax and σ > 0, the soft
smoothed classifier gθ of zθ is defined as
gθ (x) = arg max En〜N(oσI)[zc(x + η)]	(14)
c∈Y
Using Lemma 2 in Salman et al. (2019), we prove the following theorem in Appendix A:
Theorem 2. Let the ground truth of an input X be y. If gθ classifies X correctly, i.e.
0
En[zθy(X + η)] ≥ maxEn[zθy (X + η)]	(15)
y0 6=y
Then gθ is Provably robust at x, with the certified radius given by
CR(gθ; x,y) = 2[Φ-1(En[zy(X + η)]) - Φ-1(noaxEn[zy(x + η)])]	(16)
where Φ is the c.d.f. of the standard Gaussian distribution.
We notice that in Salman et al. (2019) (see its Appendix B), a similar technique was introduced to
overcome the non-differentiability in creating adversarial examples to a smoothed classifier. Dif-
ferent from their work, our method uses Soft-RS to obtain a certified radius that is differentiable
in practice. The certified radius given by soft randomized smoothing meets condition (C2) in the
algorithmic design. Even if we use Monte Carlo sampling to estimate the expectation, (16) is still
sub-differentiable with respect to θ as long as zθ is sub-differentiable with respect to θ.
Connection between Soft-RS and Hard-RS We highlight two main properties of Soft-RS.
Firstly, it is a differentiable approximation of the original Hard-RS. To see this, note that when
β → ∞, Zy(x; β) -→ 1{y=argmaχc u&(x)}, So gθ converges to gθ almost everywhere. Conse-
quently, the Soft-RS certified radius (16) converges to the Hard-RS certified radius (8) almost every-
where as β goes to infinity. Secondly, Soft-RS itself provides an alternative way to get a provable
robustness guarantee. In Appendix A, we will provide Soft-RS certification procedures that certify
gθ with the Hoeffding bound or the empirical Bernstein bound.
4.4	Numerical Stability (for Condition C3)
In this section, we will address the numerical stability condition (C3). While Soft-RS does provide
us with a differentiable certified radius (16) which we could maximize with first-order optimization
methods, directly optimizing (16) suffers from exploding gradients. The problem stems from the
inverse cumulative density function Φ-1(X), whose derivative is huge when X is close to 0 or 1.
Fortunately, by minimizing the robustness loss (13) instead, we can maximize the robust radius free
from exploding gradients. The hinge loss restricts that samples with non-zero robustness loss must
satisfy 0 < CR(gθ; x,y) < e + e, which is equivalent to 0 < ξθ(x,y) < Y where ξθ (x,y)=
Φ-1 (En [zyy (x + η)]) - Φ-1(maxyo=y En [zyy (X + η)]) and Y = 2(V+^). Under this restriction, the
derivative of Φ-1 is always bounded as shown in the following proposition. The proof can be found
in Appendix B.
Proposition 1. Given anyp1,p2, ...pK satisfies p1 ≥ p2 ≥ ... ≥ pK ≥ 0 andp1+p2 + ...+pK = 1,
let γ = 2(e+e), the derivative of min{[Φ-1(pι) — Φ-1(p2)], γ} with respect to pi and p2 is bounded.
6
Published as a conference paper at ICLR 2020
4.5	Complete implementation
We are now ready to present the complete MACER algorithm. Expectations over Gaussian samples
are approximated with Monte Carlo sampling. Let ηι,…，ηk be k i.i.d. samples from N(0, σ2I).
The final objective function is
l(gθ; χ,y) = IC (gθ; χ,y) + Lr(Gθ ; χ,y)
-log Zy (x) + λ ∙ max{e + N - CR(gθ； x, y), 0} ∙ 1{gθ(χ)=y}
-log Zy (x) + λσ max{γ - ξ(x,y), 0}∙ 1{gθ(x)=y}
(17)
where Zθ(x) = k Pjk=I z§ (x + %) is the empirical expectation of z§ (x + η) and ξθ (χ,y) =
y	y0
Φ 1 (zy (x)) - Φ 1(maxy0=y ^y (x)). During training we minimize E(x,y)〜Pdatal(gθ； x,y). De-
tailed implementation is described in Algorithm 1. To simplify the implementation, we choose γ to
be a hyperparameter instead of N. The inverse temperature of softmax β is also a hyperparameter.
Algorithm 1 MACER: robust training via MAximizing CErtified Radius
1:	Input: Training set Pdata, noise level σ, number of Gaussian samples k, trade-off factor λ,
hinge factor γ, inverse temperature β, model parameters θ
2:	for each iteration do
3:	Sample a minibatch (xι,yι),…,(xn,yn)〜Pdata
4:	For each xi, sample k i.i.d. Gaussian samples xn, ∙∙∙ ,x%k 〜N (x, σ2I)
5:	Compute the empirical expectations: Zθ(Xi) J Pk=I zθ(Xij)/k for i = 1, ∙∙∙ ,n
6:	Compute Gθ = {(xi,yi) : gθ(Xi) = yi}: (xi,yi) ∈ Gθ ⇔ y = argmaxc∈γ	Zc(Xi)
7:	For each (xi,yi) ∈ Gθ, compute yi J arg maxc∈γ∖{yi} Zc(Xi)
8:	For each (xi,yi) ∈ Gθ, compute ξ(xi,yi): ξ(xi,yi) J Φ-1(Zyi (Xi))- Φ-1(Zyi(Xi))
9:	Update θ with one step of any first-order optimization method to minimize
-1 xx log Zyi (Xi)+λσ
i=1
E max{γ - ξθ(xi,yi), 0}
(xi,yi)∈Gθ
10:	end for
Compare to adversarial training Adversarial training defines the problem as a mini-max game
and solves it by optimizing the inner loop (attack generation) and the outer loop (model update)
iteratively. In our method, we only have a single loop (model update). As a result, our proposed
algorithm can run much faster than adversarial training because it does not require additional back
propagations to generate adversarial examples.
Compare to previous work The overall objective function of our method, a linear combination
of a classification loss and a robustness loss, is similar to those of adversarial logit pairing (ALP)
(Kannan et al., 2018) and TRADES (Zhang et al., 2019b). In MACER, the λ in the objective
function (17) can also be viewed as a trade-off factor between accuracy and robustness. However,
the robustness term of MACER does not depend on a particular adversarial example X0, which makes
it substantially different from ALP and TRADES.
5 Experiments
In this section, we empirically evaluate our proposed MACER algorithm on a wide range of tasks.
We also study the influence of different hyperparameters in MACER on the final model performance.
5.1	Setup
To fairly compare with previous works, we follow Cohen et al. (2019) and Salman et al. (2019) to
use LeNet for MNIST, ResNet-110 for Cifar-10 and SVHN, and ResNet-50 for ImageNet.
7
Published as a conference paper at ICLR 2020
MACER Training For Cifar-10, MNIST and SVHN, we train the models for 440 epochs using
our proposed algorithm. The learning rate is initialized to be 0.01, and is decayed by 0.1 at the
200th/400th epoch. For all the models, we use k = 16, γ = 8.0 and β = 16.0. The value of λ trades
off the accuracy and robustness and we find that different λ leads to different robust accuracy when
the model is injected by different levels (σ) of noise. We find setting λ = 12.0 for σ = 0.25 and
λ = 4.0 for σ = 0.50 works best. For ImageNet, we train the models for 120 epochs. The initial
learning rate is set to be 0.1 and is decayed by 0.1 at the 30th/60th/90th epoch. For all models on
ImageNet, we use k = 2, γ = 8.0 and β = 16.0. More details can be found in Appendix C.
Baselines We compare the performance of MACER with two previous works. The first work
(Cohen et al., 2019) trains smoothed networks by simply minimizing cross-entropy loss. The second
one (Salman et al., 2019) uses adversarial training on smoothed networks to improve the robustness.
For both baselines, we use checkpoints provided by the authors and report their original numbers
whenever available. In addition, we run Cohen et al. (2019)’s method on all tasks as it is a speical
case of MACER by setting k = 1 and λ = 0.
Certification Following previous works, we report the approximated certified test set accuracy,
which is the fraction of the test set that can be certified to be robust at radius r. However, the
approximated certified test set accuracy is a function of the radius r. It is hard to compare two
models unless one is uniformly better than the other for all r. Hence, we also use the average
certified radius (ACR) as a metric: for each test data (x, y) and model g, we can estimate the certified
radius CR(g; x, y). The average certified radius is defined as ∣sJ P(X y)∈s丘St CR(g; x, y) where
Stest is the test set. To estimate the certified radius for data points, we use the source code provided
by Cohen et al. (2019).
5.2	Results
We report the results on Cifar-10 and ImageNet in the main body of the paper. Results on MNIST
and SVHN can be found in Appendix C.2.
Table 1: Approximated certified test accuracy and ACR on Cifar-10: Each column is an l2 radius.
σ	Model	0.00	0.25	0.50	0.75	1.00	1.25	1.50	1.75	2.00	2.25	ACR
	Cohen-0.25	0.75	0.60	0.43	0.26	0	0	0	0	0	0	0.416
0.25	Salman-0.25	0.74	0.67	0.57	0.47	0	0	0	0	0	0	0.538
	MACER-0.25	0.81	0.71	0.59	0.43	0	0	0	0	0	0	0.556
	Cohen-0.50	0.65	0.54	0.41	0.32	0.23	0.15	0.09	0.04	0	0	0.491
0.50	Salman-0.50	0.50	0.46	0.44	0.40	0.38	0.33	0.29	0.23	0	0	0.709
	MACER-0.50	0.66	0.60	0.53	0.46	0.38	0.29	0.19	0.12	0	0	0.726
	Cohen-1.00	0.47	0.39	0.34	0.28	0.21	0.17	0.14	0.08	0.05	0.03	0.458
1.00	Salman-1.00	0.45	0.41	0.38	0.35	0.32	0.28	0.25	0.22	0.19	0.17	0.787
	MACER-1.00	0.45	0.41	0.38	0.35	0.32	0.29	0.25	0.22	0.18	0.16	0.792
MACER-0.25
Cohen-0.25
Salman-0.25
_ _ __
AOE.InOOE p。一三」。。
0
0	0.5
& radius
(a) σ = 0.25
_ _ __
AOE.InOOE pos」。。
0
0	0.5
1	1.5	2
⅛ radius
(b) σ = 0.50
AOE.InOOE pos」。。
—MACER-1.00
- -Cohen-1.00
..Salman-1.00
0∣--.---' - -z=≈s∙^
0	12	3	4
Iz radius
(c) σ = 1.00
Figure 1:	Radius-accuracy curves of different Cifar-10 models.
8
Published as a conference paper at ICLR 2020
Table 2: Approximated certified test accuracy and ACR on ImageNet: Each column is an l2 radius.
σ	Model	0.0	0.5	1.0	1.5	2.0	2.5	3.0	ACR
	Cohen-0.25	0.67	0.49	^^0^^	0	0	0	0	0.470
0.25	Salman-0.25	0.65	0.56	0	0	0	0	0	0.528
	MACER-0.25	0.68	0.57	0	0	0	0	0	0.544
	Cohen-0.50	0.57	0.46	0.37	0.29	0	0	0	0.720
0.50	Salman-0.50	0.54	0.49	0.43	0.37	0	0	0	0.815
	MACER-0.50	0.64	0.53	0.43	0.31	0	0	0	0.831
	Cohen-1.00	0.44	0.38	0.33	0.26	0.19	0.15	0.12	0.863
1.00	Salman-1.00	0.40	0.37	0.34	0.30	0.27	0.25	0.20	1.003
	MACER-1.00	0.48	0.43	0.36	0.30	0.25	0.18	0.14	1.008
MACER-0.25
Cohen-0.25
Salman-0.25
_ __ __ __
>0m300mp。一三」。。
0
0	0.5
l∙i radius
(a) σ = 0.25
0.8
MACER-0.50
Cohen-0.50
1
AOE.InOOE p。一三」。。
(b) σ = 0.50
AOE.IngE
(c) σ = 1.00
Figure 2:	Radius-accuracy curves of different ImageNet models.
Performance The performance of different models on Cifar-10 are reported in Table 1, and in
Figure 1 we display the radius-accuracy curves. Note that the area under a radius-accuracy curve is
equal to the ACR of the model. First, the plots show that our proposed method consistently achieves
significantly higher approximated certified test set accuracy than Cohen et al. (2019). This shows
that robust training via maximizing the certified radius is more effective than simply minimizing
the cross entropy classification loss. Second, the performance of our model is different from that of
Salman et al. (2019) for different r. For example, for σ = 0.25, our model achieves higher accuracy
than Salman et al. (2019)’s model when r = 0/0.25/0.5, but the performance of ours is worse when
r = 0.75. For the average certified radius, our models are better than Salman et al. (2019)’s models2
in all settings. For example, when σ = 0.25/0.50, the ACR of our model is about 3% larger than
that of Salman et al. (2019)’s. The gain of our model is relatively smaller when σ = 1.0. This is
because σ = 1.0 is a very large noise level (Cohen et al., 2019) and both models perform poorly.
The ImageNet results are displayed in Table 2 and Figure 2, and the observation is similar. All
experimental results show that our proposed algorithm is more effective than previous ones.
Training speed Since MACER does not require adversarial attack during training, it runs much
faster to learn a robust model. Empirically, we compare MACER with Salman et al. (2019) on the
average training time per epoch and the total training hours, and list the statistics in Table 3. For a
fair comparison, we use the codes34 provided by the original authors and run all algorithms on the
same machine. For Cifar-10 we use one NVIDIA P100 GPU and for ImageNet we use four NVIDIA
P100 GPUs. According to our experiments, on ImageNet, MACER achieves ACR=0.544 in 117.90
hours. On the contrary, Salman et al. (2019) only achieves ACR=0.528 but uses 193.10 hours, which
clearly shows that our method is much more efficient.
One might question whether the higher performance of MACER comes from the fact that we train
for more epochs than previous methods. In Section C.3 we also run MACER for 150 epochs and
compare it with the models in Table 3. The results show that when run for only 150 epochs, MACER
still achieves a performance comparable with SmoothAdv, and is 4 times faster at the same time.
2Salman et al. (2019) releases hundreds of models, and we select the model with the largest average certified
radius for each σ as our baseline.
3https://github.com/locuslab/smoothing
4https://github.com/Hadisalman/smoothing-adversarial
9
Published as a conference paper at ICLR 2020
Table 3: Training time and performance of σ = 0.25 models.
Dataset		Model		sec/epoch	Epochs	Total hrs	ACR
	-Cohen-0.25 (Cohen et al., 2019)-	-314-	-150-	-TM-	0.416
Cifar-10	Salman-0.25 (Salman et al., 2019)	1990.1	150	82.92	0.538
	MACER-0.25(ours)	504.0	440	61.60	0.556
	Cohen-0.25(Cohen et al., 2019)	2154.5	-90-	-53.86-	0.470
ImageNet	Salman-0.25 (Salman et al., 20i9)	7723.8	90	193.10	0.528
	MACER-0.25(ours)	3537.1	120	117.90	0.544
Figure 3:	Effect of hyperparameters on Cifar-10 (σ = 0.25).
5.3 Effect of hyperparameters
In this section, we carefully examine the effect of different hyperparameters in MACER. All exper-
iments are run on Cifar-10 with σ = 0.25 or 0.50. The results for σ = 0.25 are shown in Figure 3.
All details can be found in Appendix C.4.
Effect of k We sample k Gaussian samples for each input to estimate the expectation in (16). We
can see from Figure 3(a) that using more Gaussian samples usually leads to better performance. For
example, the radius-accuracy curve of k = 16 is uniformly above that of k = 1.
Effect of λ The radius-accuracy curves in Figure 3(b) demonstrate the trade-off effect of λ. From
the figure, we can see that as λ increases, the clean accuracy drops while the certified accuracy at
large radii increases.
Effect of γ γ is defined as the hyperparameter in the hinge loss. From Figure 3(c) we can see that
when γ is small, the approximated certified test set accuracy at large radii is small since γ “truncates”
the large radii. As γ increases, the robust accuracy improves. It appears that γ also acts as a trade-off
between accuracy and robustness, but the effect is not as significant as the effect of λ.
Effect ofβ Similar to Salman et al. (2019)’s finding (see its Appendix B), we also observe that using
a larger β produces better results. While Salman et al. (2019) pointed out that a large β may make
training unstable, we find that if we only apply a large β to the robustness loss, we can maintain
training stability and achieve a larger average certified radius as well.
6 Conclusion and future work
In this work we propose MACER, an attack-free and scalable robust training method via directly
maximizing the certified radius of a smoothed classifier. We discuss the desiderata such an algo-
rithm would have to satisfy, and provide an approach to each of them. According to our extensive
experiments, MACER performs better than previous provable l2-defenses and trains faster. Our
strong empirical results suggest that adversarial training is not a must for robust training, and de-
fense based on certification is a promising direction for future research. Moreover, several recent
papers (Carmon et al., 2019; Zhai et al., 2019; Stanforth et al., 2019) suggest that using unlabeled
data helps improve adversarially robust generalization. We will also extend MACER to the semi-
supervised setting.
10
Published as a conference paper at ICLR 2020
Acknowledgments
We thank Tianle Cai for helpful discussions and suggestions. This work was done when Runtian
Zhai was visiting UCLA under the Top-Notch Undergraduate Program of Peking University school
of EECS. Chen Dan and Pradeep Ravikumar acknowledge the support of Rakuten Inc., and NSF
via IIS1909816. Huan Zhang and Cho-Jui Hsieh acknowledge the support of NSF via IIS1719097.
Liwei Wang acknowledges the support of Beijing Academy of Artificial Intelligence.
References
Anish Athalye, Nicholas Carlini, and David A. Wagner. Obfuscated gradients give a false sense of
security: Circumventing defenses to adversarial examples. CoRR, abs/1802.00420, 2018. URL
http://arxiv.org/abs/1802.00420.
Battista Biggio, Igmo Corona, Davide Maiorca, Blame Nelson, Nedim Srndic, Pavel Laskov, Gior-
gio Giacinto, and Fabio Roli. Evasion attacks against machine learning at test time. In Joint
European conference on machine learning and knowledge discovery in databases, pp. 387-402.
Springer, 2013.
Nicholas Carlini and David A. Wagner. Towards evaluating the robustness of neural networks.
CoRR, abs/1608.04644, 2016. URL http://arxiv.org/abs/1608.04644.
Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, Percy Liang, and John C Duchi. Unlabeled data
improves adversarial robustness. arXiv preprint arXiv:1905.13736, 2019.
Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. Zoo: Zeroth order opti-
mization based black-box attacks to deep neural networks without training substitute models. In
Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, pp. 15-26. ACM,
2017.
Minhao Cheng, Thong Le, Pin-Yu Chen, Jinfeng Yi, Huan Zhang, and Cho-Jui Hsieh. Query-
efficient hard-label black-box attack: An optimization-based approach. 2019.
Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized
smoothing. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th
International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning
Research, pp. 1310-1320, Long Beach, California, USA, 09-15 Jun 2019. PMLR.
Gavin Weiguang Ding, Yash Sharma, Kry Yik Chau Lui, and Ruitong Huang. Mma training:
Direct input space margin maximization through adversarial training. In International Confer-
ence on Learning Representations, 2020. URL https://openreview.net/forum?id=
HkeryxBtPB.
Ruiqi Gao, Tianle Cai, Haochuan Li, Cho-Jui Hsieh, Liwei Wang, and Jason D Lee. Convergence
of adversarial training in overparametrized neural networks. In Advances in Neural Information
Processing Systems 32. 2019.
Timon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat Chaudhuri, and Mar-
tin Vechev. Ai2: Safety and robustness certification of neural networks with abstract interpreta-
tion. In 2018 IEEE Symposium on Security and Privacy (SP), pp. 3-18. IEEE, 2018.
Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In International Conference on Learning Representations, 2015. URL http://
arxiv.org/abs/1412.6572.
Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan Ue-
sato, Timothy Mann, and Pushmeet Kohli. On the effectiveness of interval bound propagation for
training verifiably robust models. arXiv preprint arXiv:1810.12715, 2018.
RUitong Huang, Bing Xu, Dale Schuurmans, and Csaba Szepesvari. Learning with a strong adver-
sary. arXiv preprint arXiv:1511.03034, 2015.
11
Published as a conference paper at ICLR 2020
Harini Kannan, Alexey Kurakin, and Ian J. Goodfellow. Adversarial logit pairing. CoRR,
abs/1803.06373, 2018. URL http://arxiv.org/abs/1803.06373.
Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. Certified
robustness to adversarial examples with differential privacy. arXiv preprint arXiv:1802.03471,
2018.
Bai Li, Changyou Chen, Wenlin Wang, and Lawrence Carin. Second-order adversarial attack
and certifiable robustness. CoRR, abs/1809.03113, 2018. URL http://arxiv.org/abs/
1809.03113.
Xuanqing Liu, Minhao Cheng, Huan Zhang, and Cho-Jui Hsieh. Towards robust neural networks via
random self-ensemble. In Proceedings of the European Conference on Computer Vision (ECCV),
pp. 369-385, 2018.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017.
Andreas Maurer and Massimiliano Pontil. Empirical Bernstein Bounds and Sample Variance Penal-
ization. arXiv e-prints, art. arXiv:0907.3740, Jul 2009.
Matthew Mirman, Timon Gehr, and Martin Vechev. Differentiable abstract interpretation for prov-
ably robust neural networks. In International Conference on Machine Learning, pp. 3575-3583,
2018.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and
accurate method to fool deep neural networks. CoRR, abs/1511.04599, 2015. URL http:
//arxiv.org/abs/1511.04599.
Nicolas Papernot, Patrick McDaniel, Arunesh Sinha, and Michael Wellman. Towards the science of
security and privacy in machine learning. arXiv preprint arXiv:1611.03814, 2016.
Chongli Qin, James Martens, Sven Gowal, Dilip Krishnan, Alhussein Fawzi, Soham De, Robert
Stanforth, Pushmeet Kohli, et al. Adversarial robustness through local linearization. arXiv
preprint arXiv:1907.02610, 2019.
Jonas Rauber, Wieland Brendel, and Matthias Bethge. Foolbox: A python toolbox to benchmark the
robustness of machine learning models. arXiv preprint arXiv:1707.04131, 2017. URL http:
//arxiv.org/abs/1707.04131.
Hadi Salman, Greg Yang, Jerry Li, Pengchuan Zhang, Huan Zhang, Ilya P. Razenshteyn, and
Sebastien Bubeck. Provably robust deep learning via adversarially trained smoothed classifiers.
CoRR, abs/1906.04584, 2019. URL http://arxiv.org/abs/1906.04584.
Ali Shafahi, Mahyar Najibi, Amin Ghiasi, Zheng Xu, John P. Dickerson, Christoph Studer, Larry S.
Davis, Gavin Taylor, and Tom Goldstein. Adversarial training for free! CoRR, abs/1904.12843,
2019. URL http://arxiv.org/abs/1904.12843.
Gagandeep Singh, Timon Gehr, Matthew Mirman, Markus PUScheL and Martin Vechev. Fast and
effective robustness certification. In Advances in Neural Information Processing Systems, pp.
10802-10813, 2018.
Robert Stanforth, Alhussein Fawzi, Pushmeet Kohli, et al. Are labels required for improving adver-
sarial robustness? arXiv preprint arXiv:1905.13725, 2019.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfel-
low, and Rob Fergus. Intriguing properties of neural networks. CoRR, abs/1312.6199, 2013. URL
http://arxiv.org/abs/1312.6199.
Shiqi Wang, Yizheng Chen, Ahmed Abdou, and Suman Jana. Mixtrain: Scalable training of formally
robust neural networks. arXiv preprint arXiv:1811.02625, 2018.
12
Published as a conference paper at ICLR 2020
Lily Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Luca Daniel, Duane Boning,
and Inderjit Dhillon. Towards fast computation of certified robustness for ReLU networks. In
Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on
Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 5276-5285,
Stockholmsmssan, Stockholm Sweden, 10-15 JUl 2018. PMLR.
Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer
adversarial polytope. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th In-
ternational Conference on Machine Learning, volume 80 of Proceedings of Machine Learning
Research, pp. 5286-5295, Stockholmsmssan, Stockholm Sweden, 10-15 Jul 2018. PMLR.
Eric Wong, Frank Schmidt, Jan Hendrik Metzen, and J. Zico Kolter. Scaling provable adversarial
defenses. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett
(eds.), Advances in Neural Information Processing Systems 31, pp. 8400-8409. Curran Asso-
ciates, Inc., 2018.
Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, and Alan Yuille. Mitigating adversarial
effects through randomization. arXiv preprint arXiv:1711.01991, 2017.
Runtian Zhai, Tianle Cai, Di He, Chen Dan, Kun He, John E. Hopcroft, and Liwei Wang. Adversar-
ially robust generalization just requires more unlabeled data. CoRR, abs/1906.00555, 2019. URL
http://arxiv.org/abs/1906.00555.
Dinghuai Zhang, Tianyuan Zhang, Yiping Lu, Zhanxing Zhu, and Bin Dong. You only propagate
once: Accelerating adversarial training via maximal principle. arXiv preprint arXiv:1905.00877,
2019a.
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan.
Theoretically principled trade-off between robustness and accuracy. In Kamalika Chaudhuri
and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine
Learning, volume 97 of Proceedings of Machine Learning Research, pp. 7472-7482, Long Beach,
California, USA, 09-15 Jun 2019b. PMLR. URL http://proceedings.mlr.press/
v97/zhang19p.html.
Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel. Efficient neural net-
work robustness certification with general activation functions. In Advances in neural information
processing systems, pp. 4939-4948, 2018.
Huan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Duane Boning, and Cho-Jui Hsieh. Towards sta-
ble and efficient training of verifiably robust neural networks. arXiv preprint arXiv:1906.06316,
2019c.
13
Published as a conference paper at ICLR 2020
A Soft randomized smoothing
In this section we provide theoretical analysis and certification procedures for Soft-RS.
A.1 Proof of theorem 2
Our proof is based on the following lemma:
Lemma 1. For any measurable function f : X → [0,1], define f (x) = En〜N(o,σ2 I)f (X + η), then
x → Φ 1(f (x)) Is 1∕σ-Lipschitz.
This lemma is the generalized version of Lemma 2 in Salman et al. (2019).
0
Proof of Theorem 2. Let y* = arg maXyo=y En [zyy (x + η)]. For any C ∈ Y, define Zc as:
Zc(X)= En 〜N (0,σ2 I)[zc(x + η)]	(18)
Because Zc : X → [0,1], by Lemma 1 We have X → Φ-1(Zc(x)) is 1∕σ-Lipschitz. Thus, ∀y0 = y,
for any δ such that1冏卜 ≤ σ [Φ-1(En区(X + η)]) — Φ-1(maxy0=y En[zy (x + η)])]:
IT-
φ-1(zy(χ+δ)) ≥ Φ-1(zy(X))- 2[Φ-1(zy(X))- Φ-1(zy 3)]
φ-1(zy0 (χ+δ)) ≤ Φ-1(zy0 (x)) + 2[Φ-1(zy (X))- Φ-1(zy* ⑺1	(19)
≤ Φ-1(zy* (x)) + 2[φ-1 (Zy (x)) - Φ-1(zy* ⑺力
Therefore, Φ-1 (En Zθy (X + δ + η)) ≥ Φ-1(EnZθy0(X + δ + η)). Due to the monotonicity of Φ-1, We
have EnZy (x + δ + η) ≥ EnZy (x + δ + η), which implies that gθ(X + δ) = y.	□
A.2 Soft-RS certification procedure
0
Let ZA = En[Zyy(x + η)] and ZB = maxyo=y En[Zyy (x + η)]. If there exist ZA,ZB ∈ [0,1] such
that P(ZA ≥ ZA ∧ ZB ≤ ZB) ≥ 1 - α, then with probability at least 1 - α, CR(gy; x,y) ≥
σ[Φ-1(za) - Φ-1(ZB)]. Meanwhile, ZB ≤ 1 - za, so we can take ZB = 1 - za, and
P(CR(gθ; x,y) ≥ σΦ-1(ZA)) ≥ 1 - α	(20)
It reduces to find a confidence lower bound of ZA . Here we provide two bounds:
Hoeffding Bound The random variable Zy (χ + η) has mean za, and Zy,…，Zyy are its k observa-
tions. Because Zy ∈ [0,1] for any j = 1, ∙∙∙ ,k,we can use Hoeffding's inequality to obtain a lower
confidence bound:
Lemma 2. (Hoeffding’s Inequality) Let X1, ..., Xk be independent random variables bounded by
the interval [0,1] .Let X = 1 Pj=I Xj, thenfor any t ≥ 0
P(X - EX ≥ t) ≤ e-2kt2	(21)
Denote Zy = ɪ Pj=ι Zy. By Hoeffding,s inequality we have
P(Zy - ZA ≥ r-Oga) ≤ α	(22)
Hence, a 1 - a confidence lower bound ZA of ZA is
ZA = Zy -汽＞	(23)
14
Published as a conference paper at ICLR 2020
Empirical Bernstein Bound Maurer & Pontil (2009) provides us with a tighter bound:
Theorem 3. (Theorem 4 in Maurer & Pontil (2009)) Under the conditions of Lemma 2, with prob-
ability at least 1 一 α,
X 一 EX ≤ S2S2P +i7⅛	(24)
k 3(k 一 1)
where S2 is the sample variance of Xi, •…,Xk, i.e.
S2
(25)
Consequently, a 1 一 a confidence lower bound ZA of ZA is
ZA = Zy -J
2S2 log 飞 _ 7log 2
k 3(k 一 1)
(26)
The full certification procedure with the above two bounds is described in Algorithm 2.
Algorithm 2 Soft randomized smoothing certification
1:	# Certify the robustness of g around an input X with Hoefding bound
2:	function CERTIFYHOEFFDING(Z, σ2, x, n0, n, α)
3:	ZQ — SAMPLEUNDERNOISE(Z,x,no,σ2)[1, :]/nŋ
4:	Ca — arg maXc Zqc
5:	ZA — SAMPLEUNDERNOISE(z, x, n, σ2)[1, ^a]∕n
6:	ZA J ZA - q- 2ogα
7:	if ZA > 1 then return prediction ^a and radius σΦ-i (za)
8:	elsereturn ABSTAIN	一
9:	end function
10:	# Certify with empirical Bernstein bound
11:	function CERTIFYBERNSTEIN(Z, σ2, x, no, n, α)
12:	Zq — SAMPLEUNDERNOISE(Z,x,no,σ2)[1, :]/no
13:	^a — arg maXc Zqc
14:	A J SAMPLEUNDERNOISE(Z, x, n, σ2)
15:	ZA 一 A[1, ^A]∕n, SA 一 a[2,Ca]-”,Ca]2加
16.	7, J"	q 2SA log(2∕α)	7log(2∕α)
16:	ZA — ZA -V n	3(n-1)
17:	if ZA > 1 then return prediction ^a and radius σΦ-1(ZA)
18:	else return ABSTAIN
19:	end function
20:	# Helper function: draw num samples from Z(x +η) and return the 1st and 2nd sample moment
21:	function SAMPLEUNDERNOISE(Z, x, num, σ2)
22:	Initialize a 2 X K matrix A J(0, •…，0;0, ∙∙∙ , 0)
23:	for j = 1 to num do
24:	Sample noise η 〜N(0, σ2I)
25:	Compute: Zj = Z(x + ηj )
26:	Increment: A[1, :] J A[1, :] + Zj, A[2, :] J A[2, :] + Zj2
27:	end for
28:	return A
29:	end function
A.3 Comparing Soft-RS with Hard-RS
We use Soft-RS during training and use Hard-RS during certification. In this section, we empirically
compare these two certification methods. We certify the nine models in Table 4. For each model,
15
Published as a conference paper at ICLR 2020
we certify with both Hard-RS and Soft-RS. For Hard-RS, we use Clopper-Pearson bound and for
Soft-RS, we use the empirical Bernstein bound with different choices ofβ. The results are displayed
in Figure 4. The results show that Hard-RS consistently gives a larger lower bound of robust radius
than Soft-RS. We also observe that there is a gap between Soft-RS and Hard-RS when β → ∞,
which implies that the empirical Bernstein bound, though tighter than the Hoeffding bound, is still
looser than the Clopper-Pearson bound.
B Proof of Proposition 1
Proof of Proposition 1. We only need to consider the case when Φ-1(p1) - Φ-1(p2) ≤ γ since the
derivative is zero when Φ-1(p1) - Φ-1(p2) > γ. Obviously, p2 ≤ 0.5 and thus Φ-1(p2) ≤ 0. So
Φ-1 (p1) ≤ γ.
Define p* ∈ (0,1) such that Φ-1(p*) = Y. Since Φ-1(p) is a strictly increasing function of p, p*
is unique, and pi ≤ p*. min{[Φ-1 (pi) - Φ-1(p2)], γ} = Φ-1(p1) - Φ-1(p2). Since pi is the
largest value and pi + p + ... + PK = 1, we have 春 ≤ pi ≤ p*. Since [Φ-1]0(p) is continuous
in any closed interval of (0, 1), the derivative of Φ-i(pi) - Φ-i(p2) with respect topi is bounded.
Similarly, p2 is the largest amongp2, ...pK and (K - 1)p2 ≥ p2 + ... +pK = 1 -pi ≥ 1 -p*. Thus
1 - p* ≥ p2 ≥ Kpi, and the derivative of Φ-i(pi) - Φ-i(p2) with respect top is bounded. 口
C S upplementary material for experiments
C.1 Compared models
In this section we list all compared models in the main body of this paper. Cifar-10 models are listed
in Table 4, and ImageNet models are listed in Table 5.
Table 4: Models for comparison on Cifar-10.
Q	Model	Description
Cohen-{0.25,0.50,1.00}		Cohen et al. (2019)‘s models
0.25	Salman-0.25 MACER-0.25	8-sample 10-step SmoothAdvpGD with E = 1.00 MACER with k = 16, λ = 12.0, β = 16.0 and Y = 8.0
0.50	Salman-0.50 MACER-0.50	2-sample 10-step SmoothAdvpGD with E = 2.00 MACER with k = 16, λ = 4.0, β = 16.0 and Y = 8.0
1.00	Salman-1.00 MACER-1.00	2-sample 10-step SmoothAdvpGD with E = 2.00 MACER with k = 16, dynamic λ5, β = 16.0 and Y = 8.0
Table 5: Models for comparison on ImageNet.
Q	Model	Description
COhen-{0.25,0.50,1.00}		Cohen et al. (2019)‘s models
0.25	Salman-0.25 MACER-0.25	1-sample 2-step SmoothAdvddn with E = 1.0 MACER with k = 2, λ = 6.0, β = 16.0 and Y = 8.0
0.50	Salman-0.50 MACER-0.50	1-sample 1-step SmoothAdvpGD with E = 1.0 MACER with k = 2, λ = 3.0, β = 16.0 and Y = 8.0
1.00	Salman-1.00 MACER-1.00	1-sample 1-step SmoothAdvpGD with E = 2.0 MACER with k = 2, λ = 3.0, β = 16.0 and Y = 8.0
C.2 Results on MNIST and SVHN
Here we present experimental results on MNIST and SVHN. For comparison we also report the
results produced by Cohen et al. (2019)’s method.
5We first train with λ = 0.0, and then change to λ = 12.0 after learning rate decay.
16
Published as a conference paper at ICLR 2020
Certified accuracy	Certified accuracy	Certified accuracy
r~^	r~^	r~^	r~^
(a) Cohen-0.25
8 6 4 2
Oooo
Aoe-nooe pə三一」əo
(d) Salman-0.25
(g) MACER-0.25
--/3=1.0 I
——0 = 4.0
—0=16.0
“…Hard-RS
--β = 1.0
---β = 4.0
—β = 16.0
.•…Hard-RS
0-----------------------------------
0	0.5	1	1.5	2
I2 radius
(b) Cohen-0.50
(e) Salman-0.50
(h) MACER-0.50
I2 radius
(C) Cohen-1.00
(f) Salman-1.00
(i) MACER-1.00
O
8 6 4 2 0
Aoe-nooe pə三一」əo
Figure 4: Comparing Soft-RS with Hard-RS on Cifar-10. The three Columns Correspond to σ
0.25, 0.50, 1.00 respeCtively.
17
Published as a conference paper at ICLR 2020
C.2.1 MNIST
The results are reported in Table 6. For all σ, we use k = 16, λ = 16.0, γ = 8.0 and β = 16.0.
Table 6: Approximated certified test accuracy and ACR on MNIST: Each column is an l2 radius.
Q	Method	0.00	0.25	0.50	0.75	1.00	1.25	1.50	1.75	2.00	2.25	ACR
0.25	Cohen	^099^	0.97	0.94	0.89	^^0-^	0	0	0	0	0	0.887
	MACER	0.99	0.99	0.97	0.95	0	0	0	0	0	0	0.918
0.50	Cohen	^099^	0.97	0.94	0.91	0.84	0.75	0.57	0.33	0	0	1.453
	MACER	0.99	0.98	0.96	0.94	0.90	0.83	0.73	0.50	0	0	1.583
1.00	Cohen	^095"	0.92	0.87	0.81	0.72	0.61	0.50	0.34	0.20	0.10	1.417
	MACER	0.89	0.85	0.79	0.75	0.69	0.61	0.54	0.45	0.36	0.28	1.520
C.2.2 SVHN
The results are reported in Table 7. We use k = 16, λ = 12.0, γ = 8.0 and β = 16.0.
Table 7: Approximated certified test accuracy and ACR on SVHN: Each column is an l2 radius.
σ	Method	0.00	0.25	0.50	0.75	1.00	1.25	1.50	1.75	2.00	2.25	ACR
0.25	Cohen	^090"^	0.70	0.44	0.26	^^0^^	0	0	0	0	0	0.469
	MACER	0.86	0.72	0.56	0.39	0	0	0	0	0	0	0.540
0.50	Cohen	^067^	0.48	0.37	0.24	0.14	0.08	0.06	0.03	0	0	0.434
	MACER	0.61	0.53	0.44	0.35	0.24	0.15	0.09	0.04	0	0	0.538
C.3 MACER Training for 150 epochs
In Table 8 we report the performance and training time of MACER on Cifar-10 when it is only run
for 150 epochs, and compare with SmoothAdv (Salman et al., 2019) and MACER (440 epochs).
The learning rate is decayed by 0.1 at epochs 60 and 120. All other hyperparameters are kept the
same as in Table 4.
Table 8: Approximated certified test accuracy and ACR on Cifar-10: Each column is an l2 radius.
σ	Method	0.00	0.25	0.50	0.75	1.00	1.25	1.50	1.75	ACR	Epochs	Total hrs
	SmoothAdv	0.74	0.67	0.57	0.47	-0-	0	0	0	0.538	-150-	-82.92-
0.25	MACER	0.76	0.67	0.57	0.42	0	0	0	0	0.531	150	21.00
	MACER	0.81	0.71	0.59	0.43	0	0	0	0	0.556	440	61.60
	SmoothAdv	0.50	0.46	0.44	0.40	0.38	0.33	0.29	0.23	0.709	-150-	-82.92-
0.50	MACER	0.62	0.57	0.50	0.44	0.38	0.29	0.21	0.13	0.712	150	21.00
	MACER	0.66	0.60	0.53	0.46	0.38	0.29	0.19	0.12	0.726	440	61.60
C.4 Effect of hyperparameters
All experiments are run on Cifar-10 with σ = 0.25 or 0.50. See Table 9 for detailed experimental
settings. Results are reported in Tables 10-13.
18
Published as a conference paper at ICLR 2020
Table 9: Experimental setting for examining the effect of hyperparameters.
Experiment	k	λ		Y			β	
Effect of k	1/2/4/8/16	120	8.0	16.0
Effect of λ	16	0.0/1.0/2.0/4.0/8.0/16.0	8.0	160
Effect of Y	16	120	2.0/4.0/6.0/8.0/10.0/12.0/14.0/16.0	16.0
Effect of 8一	16	12.0	8.0	1.0/2.0/4.0/8.0/16.0/32.0/640-
Table 10: Effect of k: Approximated certified test accuracy and ACR on Cifar-10.
σ	k	0.00	0.25	0.50	0.75	1.00	1.25	1.50	1.75	2.00	2.25	ACR
		0.77	0.65	0.45	0.27	^^0^^	0	0	0	0	^^0~~	0.448
	2	0.81	0.69	0.53	0.38	0	0	0	0	0	0	0.519
0.25	4	0.79	0.69	0.55	0.38	0	0	0	0	0	0	0.517
	8	0.78	0.69	0.57	0.41	0	0	0	0	0	0	0.538
	16	0.81	0.71	0.59	0.43	0	0	0	0	0	0	0.556
	~~T~	0.36	0.31	0.26	0.19	0.12	0.09	0.05	0.02	0	^^0~~	0.306
	2	0.60	0.53	0.47	0.37	0.28	0.19	0.13	0.08	0	0	0.589
0.50	4	0.60	0.55	0.50	0.42	0.34	0.26	0.18	0.11	0	0	0.665
	8	0.61	0.56	0.50	0.43	0.37	0.28	0.22	0.14	0	0	0.699
	16	0.60	0.56	0.50	0.46	0.38	0.29	0.23	0.14	0	0	0.712
Table 11: Effect of λ: Approximated certified test accuracy and ACR on Cifar-10.
Q	λ	0.00	0.25	0.50	0.75	1.00	1.25	1.50	1.75	2.00	2.25	ACR
	~0TΓ	0.82	0.67	0.51	0.32	^^0^^	0	0	0	0	0	0.488
	1.0	0.81	0.68	0.54	0.35	0	0	0	0	0	0	0.516
	2.0	0.81	0.71	0.54	0.36	0	0	0	0	0	0	0.523
0.25	4.0	0.82	0.71	0.56	0.41	0	0	0	0	0	0	0.540
	8.0	0.80	0.70	0.59	0.43	0	0	0	0	0	0	0.550
	16.0	0.78	0.69	0.57	0.46	0	0	0	0	0	0	0.550
	-0.0-	0.69	0.56	0.44	0.31	0.21	0.13	0.06	0.02	0	0	0.515
	1.0	0.68	0.59	0.52	0.43	0.33	0.23	0.15	0.07	0	0	0.662
n cr∖	2.0	0.70	0.61	0.53	0.43	0.36	0.27	0.18	0.09	0	0	0.709
0.50	4.0	0.66	0.60	0.53	0.46	0.37	0.29	0.19	0.12	0	0	0.726
	8.0	0.62	0.56	0.50	0.45	0.37	0.29	0.21	0.13	0	0	0.700
	16.0	0.60	0.56	0.51	0.45	0.36	0.30	0.21	0.14	0	0	0.711
19
Published as a conference paper at ICLR 2020
Table 12: Effect of γ : Approximated certified test accuracy and ACR on Cifar-10.
σ	Y	0.00	0.25	0.50	0.75	1.00	1.25	1.50	1.75	2.00	2.25	ACR
	2~0：0~	0.82	0.67	0.47	0.26	^^0^^	0	0	0	0	0	0.455
	4.0	0.83	0.70	0.53	0.35	0	0	0	0	0	0	0.521
	6.0	0.80	0.70	0.57	0.40	0	0	0	0	0	0	0.539
∩ ɔe	8.0	0.81	0.71	0.59	0.43	0	0	0	0	0	0	0.556
0.25	10.0	0.78	0.69	0.57	0.44	0	0	0	0	0	0	0.543
	12.0	0.76	0.69	0.58	0.42	0	0	0	0	0	0	0.534
	14.0	0.76	0.68	0.55	0.44	0	0	0	0	0	0	0.536
	16.0	0.75	0.67	0.56	0.44	0	0	0	0	0	0	0.534
	-20-	0.70	0.61	0.49	0.33	0.23	0.13	0.06	0.03	0	0	0.549
	4.0	0.68	0.60	0.53	0.44	0.34	0.25	0.15	0.06	0	0	0.676
	6.0	0.65	0.58	0.51	0.43	0.35	0.28	0.20	0.13	0	0	0.706
n cn	8.0	0.60	0.56	0.50	0.46	0.38	0.29	0.23	0.14	0	0	0.712
0.50	10.0	0.58	0.54	0.49	0.44	0.37	0.31	0.24	0.15	0	0	0.707
	12.0	0.56	0.51	0.48	0.43	0.38	0.34	0.26	0.16	0	0	0.712
	14.0	0.51	0.48	0.44	0.38	0.33	0.28	0.22	0.16	0	0	0.634
	16.0	0.57	0.54	0.47	0.41	0.34	0.30	0.24	0.17	0	0	0.695
Table 13: Effect of β: Approximated certified test accuracy and ACR on Cifar-10.
σ	β	0.00	0.25	0.50	0.75	1.00	1.25	1.50	1.75	2.00	2.25	ACR
	1.0	0.79	0.67	0.54	0.37	^^0^^	0	0	0	0	0	0.513
	2.0	0.81	0.69	0.56	0.41	0	0	0	0	0	0	0.534
	4.0	0.79	0.70	0.59	0.43	0	0	0	0	0	0	0.549
0.25	8.0	0.79	0.71	0.57	0.43	0	0	0	0	0	0	0.541
	16.0	0.81	0.71	0.59	0.43	0	0	0	0	0	0	0.556
	32.0	0.79	0.70	0.58	0.43	0	0	0	0	0	0	0.549
	64.0	0.74	0.65	0.54	0.42	0	0	0	0	0	0	0.517
	~TQ~	0.67	0.59	0.52	0.43	0.35	0.26	0.19	0.10	0	0	0.696
	2.0	0.63	0.57	0.52	0.45	0.37	0.31	0.22	0.13	0	0	0.719
	4.0	0.60	0.55	0.50	0.45	0.36	0.30	0.22	0.14	0	0	0.703
0.50	8.0	0.60	0.56	0.52	0.43	0.38	0.30	0.24	0.14	0	0	0.713
	16.0	0.60	0.56	0.50	0.46	0.38	0.29	0.23	0.14	0	0	0.712
	32.0	0.59	0.55	0.49	0.44	0.36	0.30	0.23	0.14	0	0	0.705
	64.0	0.45	0.41	0.38	0.35	0.32	0.26	0.21	0.16	0	0	0.579
20