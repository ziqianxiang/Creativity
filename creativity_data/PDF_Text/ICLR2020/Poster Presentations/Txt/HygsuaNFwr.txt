Published as a conference paper at ICLR 2020
Order Learning and Its Application to
Age Estimation
Kyungsun Lim, Nyeong-Ho Shin, Young-Yoon Lee, and Chang-Su Kim
School of Electrical Engineering, Korea University and Samsung Electronics Co., Ltd
{kslim, nhshin, cskim}@mcl.korea.ac.kr, yy77lee@gmail.com
Ab stract
We propose order learning to determine the order graph of classes, representing
ranks or priorities, and classify an object instance into one of the classes. To this
end, we design a pairwise comparator to categorize the relationship between two
instances into one of three cases: one instance is ‘greater than,’ ‘similar to,’ or
‘smaller than’ the other. Then, by comparing an input instance with reference in-
stances and maximizing the consistency among the comparison results, the class
of the input can be estimated reliably. We apply order learning to develop a facial
age estimator, which provides the state-of-the-art performance. Moreover, the per-
formance is further improved when the order graph is divided into disjoint chains
using gender and ethnic group information or even in an unsupervised manner.
1	Introduction
To measure the quality of something, we often compare it with other things of a similar kind. Before
assigning 4 stars to a film, a critic would have thought, “It is better than 3-star films but worse than
5-stars.” This ranking through pairwise comparisons is done in various decision processes (Saaty,
1977). It is easier to tell the nearer one between two objects in a picture than to estimate the distance
of each object directly (Chen et al., 2016; Lee & Kim, 2019a). Also, it is easy to tell a higher pitch
between two notes, but absolute pitch is a rare ability (Bachem, 1955).
Ranking through comparisons has been investigated for machine learning. In learning to rank (LTR),
the pairwise approach learns, between two documents, which one is more relevant to a query (Liu,
2009). Also, in ordinal regression (Frank & Hall, 2001; Li & Lin, 2007), to predict the rank of
an object, binary classifications are performed to tell whether the rank is higher than a series of
thresholds or not. In this paper, we propose order learning to learn ordering relationship between
objects. Thus, order learning is related to LTR and ordinal regression. However, whereas LTR and
ordinal regression assume that ranks form a total order (Hrbacek & Jech, 1984), order learning can
be used for a partial order as well. Order learning is also related to metric learning (Xing et al.,
2003). While metric learning is about whether an object is ‘similar to or dissimilar from’ another
object, order learning is about ‘greater than or smaller than.’ Section 2 reviews this related work.
In order learning, a set of classes, Θ = {θι, θ2,…，θn}, is ordered, where each class θi represents
one or more object instances. Between two classes θi and θj , there are three possibilities: θi > θj
or θi < θj or neither (i.e. incomparable). These relationships are represented by the order graph.
The goal of order learning is to determine the order graph and then classify an instance into one
of the classes in Θ. To achieve this, we develop a pairwise comparator that determines ordering
relationship between two instances x and y into one of three categories: x is ‘greater than,’ ‘similar
to,’ or ‘smaller than’ y. Then, we use the comparator to measure an input instance against multiple
reference instances in known classes. Finally, we estimate the class of the input to maximize the
consistency among the comparison results. It is noted that the parameter optimization of the pair-
wise comparator, the selection of the references, and the discovery of the order graph are jointly
performed to minimize a common loss function. Section 3 proposes this order learning.
We apply order learning to facial age estimation. Order learning matches age estimation well, since it
is easier to tell a younger one between two people than to estimate each person’s age directly (Chang
et al., 2010; Zhang et al., 2017a). Even when we assume that age classes are linearly ordered, the
proposed age estimator performs well. The performance is further improved, when classes are
1
Published as a conference paper at ICLR 2020
divided into disjoint chains in a supervised manner using gender and ethnic group information or
even in an unsupervised manner. Section 4 describes this age estimator and discusses its results.
Finally, Section 5 concludes this work.
2	Related work
Pairwise comparison: It is a fundamental problem to estimate the priorities (or ranks) of objects
through pairwise comparison. In the classic paper, Saaty (1977) noted that, even when direct esti-
mates of certain quantities are unavailable, rough ratios between them are easily obtained in many
cases. Thus, he proposed the scaling method to reconstruct absolute priorities using only relative
priorities. The scaling method was applied to monocular depth estimation (Lee & Kim, 2019a) and
aesthetic assessment (Lee & Kim, 2019b). Ranking from a pairwise comparison matrix has been
studied to handle cases, in which the matrix is huge or some elements are noisy (Braverman & Mos-
sel, 2008; Jamieson & Nowak, 2011; Negahban et al., 2012; Wauthier et al., 2013). On the other
hand, the pairwise approach to LTR learns, between two documents, which one is more relevant to
a query (Liu, 2009; Herbrich et al., 1999; Burges et al., 2005; Tsai et al., 2007). The proposed order
learning is related to LTR, since it also predicts the order between objects. But, while LTR sorts
multiple objects with unknown ranks and focuses on the sorting quality, order learning compares a
single object x with optimally selected references with known ranks to estimate the rank of x.
Ordinal regression: Ordinal regression predicts an ordinal variable (or rank) of an instance. Sup-
pose that a 20-year-old is misclassified as a 50-year old and a 25-year old, respectively. The former
error should be more penalized than the latter. Ordinal regression exploits this characteristic in the
design of a classifier or a regressor. In Frank & Hall (2001) and Li & Lin (2007), a conversion
scheme was proposed to transform an ordinal regression problem into multiple binary classification
problems. Ordinal regression based on this conversion scheme has been used in various applications,
including age estimation (Chang et al., 2010; 2011; Niu et al., 2016; Chen et al., 2017) and monoc-
ular depth estimation (Fu et al., 2018). Note that order learning is different from ordinal regression.
Order learning performs pairwise comparison between objects, instead of directly estimating the
rank of each object. In age estimation, ordinal regression based on the conversion scheme is con-
Cemed with the problem, “Is a person's age bigger than a threshold θ?'' for each θ. In contrast,
order learning concerns “Between two people, who is older?” Conceptually, order learning is easier.
Technically, if there are N ranks, the conversion scheme requires N - 1 binary classifiers, but order
learning needs only a single ternary classifier. Moreover, whereas ordinal regression assumes that
ranks form a total order, order learning can be used even in the case of a partial order (Hrbacek &
Jech, 1984).
Metric learning: A distance metric can be learned from examples of similar pairs of points and
those of dissimilar pairs (Xing et al., 2003). The similarity depends on an application and is implic-
itly defined by user-provided examples. Ifa learned metric generalizes well to unseen data, it can be
used to enforce the desired similarity criterion in clustering (Xing et al., 2003), classification (Wein-
berger et al., 2006), or information retrieval (McFee & Lanckriet, 2010). Both metric learning and
order learning learn important binary relations in mathematics: metric and order (Hrbacek & Jech,
1984). However, a metric decides whether an object x is similar to or dissimilar from another object
y, whereas an order tells whether x is greater than or smaller than y. Thus, a learned metric is useful
for grouping similar data, whereas a learned order is suitable for processing ordered data.
Age estimation: Human ages can be estimated from facial appearance (Kwon & da Vitoria Lobo,
1994). Geng et al. (2007) proposed the aging pattern subspace, and Guo et al. (2009) introduced
biologically inspired features to age estimation. Recently, deep learning has been adopted for age
estimation. Niu et al. (2016) proposed OR-CNN for age estimation, which is an ordinal regressor
using the conversion scheme. Chen et al. (2017) proposed Ranking-CNN, which is another ordinal
regressor. While OR-CNN uses a common feature for multiple binary classifiers, Ranking-CNN
employs a separate CNN to extract a feature for each binary classifier. Tan et al. (2018) grouped
adjacent ages via the group-n encoding, determined whether a face belongs to each group, and com-
bined the results to predict the age. Pan et al. (2018) proposed the mean-variance loss to train a CNN
classifier for age estimation. Shen et al. (2018) proposed the deep regression forests for age esti-
mation. Zhang et al. (2019) developed a compact age estimator using the two-points representation.
Also, Li et al. (2019) proposed a continuity-aware probabilistic network for age estimation.
2
Published as a conference paper at ICLR 2020
◎ ◎ ◎
: ; :
O。。
Figure 1: Examples of order graphs, in which node n precedes node m (n → m), if n divides m. For
clarity, self-loops for reflexivity and edges deducible from transitivity are omitted from the graphs.
3	Order learning
3.1	What is order?
Let us first review mathematical definitions and concepts related to order. An order (Hrbacek &
Jech, 1984; Bartle, 1976), often denoted by ≤, is a binary relation on a set Θ = {θι, θ2,…，θn}
that satisfies the three properties of
•	Reflexivity: θi ≤ θi for every θi ∈ Θ;
•	Antisymmetry: If θi ≤ θj and θj ≤ θi , then θi = θj ;
•	Transitivity: If θi ≤ θj and θj ≤ θk, then θi ≤ θk.
In real-world problems, an order describes ranks or priorities of objects. For example, in age esti-
mation, θi ≤ θj means that people in age class θi look younger than those in θj .
We may use the symbol →, instead of ≤, to denote an order on a finite set Θ. Then, the order can be
represented by a directed graph (Gross & Yellen, 2006) using elements in Θ as nodes. If θi → θj ,
there is a directed edge from node θi to node θj. The order graph is acyclic because of antisymmetry
and transitivity. For example, for n, m ∈ N, let n → m denote that m is a multiple of n. Note that
it is an order on any subset ofN. Figure 1(a) is the graph representing this order on {1, . . . , 9}.
Elements θi and θj are comparable if θi → θj or θj → θi , or incomparable otherwise. In Fig-
ure 1(a), 6 and 8 are incomparable. In age estimation, it is difficult to compare apparent ages of
people in different ethnic groups or of different genders.
An order on a set Θ is total (or linear) if all elements in Θ are comparable to one another. In such
a case, Θ is called a linearly ordered set. In some real-world problems, orders are not linear. In this
work, a subset Θc of Θ is referred to as a chain, ifΘc is linearly ordered and also maximal, i.e. there
is no proper superset of Θc that is linearly ordered. In Figure 1(a), nodes 1, 2, 4, and 8 form a chain.
In Figure 1(b), the entire set is composed of three disjoint chains.
3.2	Order learning — Basics
Let Θ = {θ1,θ2,…，θn} be an ordered set of classes, where each class θi represents one or more
object instances. For example, in age estimation, age class 11 is the set of 11-year-olds. The objec-
tive of order learning is to determine the order graph, such as Figure 1(a) or (b), and categorize an
object instance into one of the classes. However, in many cases, order graphs are given explicitly
or obvious from the contexts. For example, in quality assessment, there are typically five classes
(poor → satisfactory → good → very good → excellent), forming a single chain. Also, in age
estimation, suppose that an algorithm first classifies a person’s gender into female or male and then
estimates the age differently according to the gender. in this case, implicitly, there are separate age
classes for each gender, and the age classes compose two disjoint chains similarly to Figure 1(b).
Thus, in this subsection, we assume that the order graph is already known. Also, given an object
instance, we assume that the chain to which the instance belongs is known. Then, we attempt to cat-
egorize the instance into one of the classes in the chain. section 3.4 will propose the order learning
in the case of an unknown order graph, composed of disjoint chains.
instead of directly estimating the class of each instance, we learn pairwise ordering relationship be-
tween two instances. Let Θc = {0, 1, . . . , N - 1} be a chain, where N is the number of classes. Let
3
Published as a conference paper at ICLR 2020
Figure 2: Illustration of the pairwise comparator, where c denotes concatenation.
x y
x ≈ y
T Y y
X and y be two instances belonging to classes in Θc. Let θ(∙) denote the class of an instance. Then,
x and y are compared and their ordering relationship is defined according to their class difference as
x y	if	θ(x) - θ(y)	> τ,	(1)
x ≈ y	if	∣θ(χ) - θ(y)∣ ≤ τ,	(2)
x Y y	if	θ(x) — θ(y)	< —τ,	(3)
where T is a threshold. To avoid confusion,	we use '冷, ≈,	Y' for the instance ordering, while '>,
=,<’ for the class order. In practice, the categorization in (1)~(3) is performed by a pairwise
comparator in Figure 2, which consists of a Siamese network and a ternary classifier (Lee & Kim,
2019b). To train the comparator, only comparable instance pairs are employed.
We estimate the class θ(x) of a test instance x by comparing it with reference instances ym, 0 ≤
m ≤ M — 1, where M is the number of references. The references are selected from training data
such that they are from the same chain as x. Given x and ym, the comparator provides one of three
categories ‘, ≈, Y’ as a result. Let θ0 be an estimate of the true class θ(x). Then, the consistency
between the comparator result and the estimate is defined as
φcon(x, ym, θ0) =	(4)
[x A ym][θ0 — θ(ym) > T] + [x ≈ ym][∣θ0 — θ(ym)∣ ≤ τ] + [x Y ym][θ0 一 θ(ym) < -T]
where [∙] is the indicator function. The function φcon(x, ym, θ0) returns either 0 for an inconsistent
case or 1 for a consistent case. For example, suppose that the pairwise comparator declares x Y ym
but θ0 — θ(ym) > τ. Then, φcon(χ, ym,θ0) = 0 ∙ 1+0 ∙ 0+1 ∙ 0 = 0. Due to a possible classification
error of the comparator, this inconsistency may occur even when the estimate θ0 equals the true class
θ(x). To maximize the consistency with all references, we estimate the class ofx by
M-1
Θmc(x) = arg max V" Φcon(x,ym,θ0),	(5)
θ0∈Θc
m=0
which is called the maximum consistency (MC) rule. Figure 3 illustrates this MC rule.
It is noted that 'A, ≈, Y' is not an mathematical order. For example, if θ(χ) + 3T = θ(y)=
θ(z) — 3τ, then X ≈ y and y ≈ Z but X Y z. This is impossible in an order. More precisely, due to
the quantization effect of the ternary classifier in (1)~(3),'a, ≈, Y' is quasi-transitive (Sen, 1969),
and ‘≈‛ is symmetric but intransitive. We use this quasi-transitive relation to categorize an instance
into one of the classes, on which a mathematical order is well defined.
3.3	Order learning — Supervised chains
3.3.1	Single-chain hypothesis (1CH)
in the simplest case of 1CH, all classes form a single chain Θc = {0, 1, . . . , N — 1}. For example,
in 1CH age estimation, people's ages are estimated regardless of their ethnic groups or genders.
We implement the comparator in Figure 2 using CNNs, as described in Section 4.1. Let qxy =
(q0xy , q1xy , q2xy ) be the one-hot vector, indicating the ground-truth ordering relationship between
training instances x and y. Specifically, (1, 0, 0), (0, 1, 0), and (0, 0, 1) represent x A y, x ≈ y,
and x Y y. Also, pxy = (p0xy, p1xy, p2xy) is the corresponding softmax probability vector of the
comparator. We train the comparator to minimize the comparator loss
2
'co = — XXX qxy log Pxy	(6)
x∈T y∈Rj=0
4
Published as a conference paper at ICLR 2020
θ, = 7	θ, = 9
ClaSS θ(ym) 012 3 4 5 6 7 8 9 10 11 12 13 14	012 3 4 5 6 7 8 9 10 11 12 13 14
iiiiiiiiiioiiii	Iiiooiiiiioiooi
111111111110111	111001111110001
110111111111111	110001111111001
110111111111011	110001111111101
101111111111111	101001111111001
Figure 3: Consistency computation in the MC rule: It is illustrated how to compute the sum in (5) for
two candidates θ0 = 7 and 9. Each box represents a reference ym . There are 5 references for each
class in {0, . . . , 14}. Comparison results are color-coded (yellow for x ym, gray for x ≈ ym, and
green for X Y ym). The bold black rectangle encloses the references satisfying ∣θ0 一 θ(ym)∣ ≤ T,
where τ = 4. The computed consistency φcon(x, ym , θ0) in (5) is written within the box. For
θ0 = 7, there are six inconsistent boxes. For θ0 = 9, there are 24 such boxes. In this example, θ0 = 7
minimizes the inconsistency, or equivalently maximizes the consistency. Therefore, Θmc(x) = 7.
where T is the set of all training instances and R ⊂ T is the set of reference instances. First,
We initialize R = T and minimize 'co via the stochastic gradient descent. Then, We reduce the
reference set R by sampling references from T. Specifically, for each class in Θc, we choose M/N
reference images to minimize the same loss 'co, where M is the number of all references and N is
the number of classes. In other Words, the reliability score ofa reference candidate y is defined as
2
α(y) = X X qjxy log pjxy	(7)
x∈T j=0
and the M/N candidates With the highest reliability scores are selected. Next, after fixing the
reference set R, the comparator is trained to minimize the loss 'co. Then, after fixing the comparator
parameters, the reference set R is updated to minimize the same loss 'co , and so forth.
In the test phase, an input instance is compared With the M references and its class is estimated
using the MC rule in (5).
3.3.2 K-CHAIN HYPOTHESIS (KCH)
In KCH, We assume that classes form K disjoint chains, as in Figure 1(b). For example, in the
supervised 6CH for age estimation, We predict a person’s age according to the gender in {female,
male} and the ethnic group in {African, Asian, European}. Thus, there are 6 chains in total. In this
case, people in different chains are assumed to be incomparable for age estimation. It is supervised,
since gender and ethnic group annotations are used to separate the chains. The supervised 2CH or
3CH also can be implemented by dividing chains by genders only or ethnic groups only.
The comparator is trained similarly to 1CH. HoWever, in computing the comparator loss in (6), a
training instance x and a reference y are constrained to be from the same chain. Also, during the
test, the type (or chain) of a test instance should be determined. Therefore, a K-Way type classifier
is trained, Which shares the feature extractor With the comparator in Figure 2 and uses additional
fully-connected (FC) layers. Thus, the overall loss is given by
'='co+'ty	(8)
Where 'co is the comparator loss and 'ty is the type classifier loss. The comparator and the type
classifier are jointly trained to minimize this overall loss '.
During the test, given an input instance, We determine its chain using the type classifier, and compare
it With the references from the same chain, and then estimate its class using the MC rule in (5).
3.4 Order learning — UNSUPERVISED chains
This subsection proposes an algorithm to separate classes into K disjoint chains When there are no
supervision or annotation data available for the separation. First, We randomly partition the training
set T into 70,71,..., TK-ι, where T = T ∪... ∪Tk-i and Tk ∩Tι = 0 for k = l. Then, similarly
5
Published as a conference paper at ICLR 2020
Algorithm 1 Order Learning with Unsupervised Chains
Input: T = training set of ordinal data, K = # of chains, N = # of classes in each chain, and M = # of
references in each chain
1:	Partition T randomly into T0 , . . . , TK-1 and train a pairwise comparator
2:	for each chain k do	. Reference Selection (Rk)
3:	From Tk, select M/N references y with the highest reliability scores αk (y)
4:	end for
5:	repeat
6:	for each instance x do	. Membership Update (Tk)
7:	Assign it to Tk*, where k* = arg maxfc βk (x) subject to the regularization constraint
8:	end for
9:	Fine-tune the comparator and train a type classifier using T0,..., TK-I to minimize ' = 'co + 'ty
10:	for each instance x do	. Membership Refinement (Tk)
11:	Assign it to Tk0 where k0 is its type classification result
12:	end for
13:	for each chain k do	. Reference Selection (Rk)
14:	From Tk, select M/N references y with the highest reliability scores αk (y)
15:	end for
16:	until convergence or predefined number of iterations
Output: Pairwise comparator, type classifier, reference sets R0 , . . . , RK-1
to (6), the comparator loss 'to can be written as
K-1	2	K-1	K-1
'co = -XXXXjlog时=-X Xαk (y) = - X X βk (x)	(9)
k=0 x∈Tk y∈Rk j=0	k=0 y∈Rk	k=0 x∈Tk
where Rk ⊂ Tk is the set of references for the kth chain, αk (y) = Px∈T Pjqjxy log pjxy is the
reliability of a reference y in the kth chain, and βk(x) = Py∈R Pjqjxy log pjxy is the affinity
of an instance x to the references in the kth chain. Note that βk(x) = - Py∈R D(qxykpxy)
where D is the Kullback-Leibler distance (Cover & Thomas, 2006). Second, after fixing the chain
membership Tk for each chain k, we select references y to maximize the reliability scores αk(y).
These references form Rk. Third, after fixing R0, . . . , RK-1, we update the chain membership
T0, . . . , TK-1, by assigning each training instance x to the kth chain that maximizes the affinity
score βk (x). The second and third steps are iteratively repeated. Both steps decrease the same loss
'co in (9).
The second and third steps are analogous to the centroid rule and the nearest neighbor rule in the K-
means clustering (Gersho & Gray, 1991), respectively. The second step determines representatives
in each chain (or cluster), while the third step assigns each instance to an optimal chain according to
the affinity. Furthermore, both steps decrease the same loss alternately.
However, as described in Algorithm 1, we modify this iterative algorithm by including the mem-
bership refinement step in lines 10 〜 12. Specifically, We train a K-way type classifier using
T0, . . . , TK-1. Then, we accept the type classification results to refine T0, . . . , TK-1. This refine-
ment is necessary because the type classifier should be used in the test phase to determine the chain
of an unseen instance. Therefore, it is desirable to select the references also after refining the chain
membership. Also, in line 7, if we assign an instance x to maximize βk (x) only, some classes may
be assigned too few training instances, leading to data imbalance. To avoid this, we enforce the
regularization constraint so that every class is assigned at least a predefined number of instances.
This regularized membership update is described in Appendix A.
4	Age estimation
We develop an age estimator based on the proposed order learning. Order learning is suitable for age
estimation, since telling the older one between two people is easier than estimating each person’s
age directly (Chang et al., 2010; Zhang et al., 2017a).
4.1	Implementation details
It is less difficult to distinguish between a 5-year-old and a 10-year-old than between a 65-year-
old and a 70-year-old. Therefore, in age estimation, we replace the categorization based on the
6
Published as a conference paper at ICLR 2020
Table 1: A summary of the balanced dataset, formed from MORPH II, AFAD, and UTK. An element
mn means that, out of m images in the original dataset, n images are sampled for the balanced dataset.
MORPH II	AFAD	UTK	Balanced
	Male	Female	Male	Female	Male	Female	Male	Female
African	4,022 36,772	4,446 5,748	0 0	0 0	2,047 2,319	1,871 2,209	6,069	6,317
Asian	153	17	5,000	5,000	1,015	1,200	6,168	6,217
	153	17	100,752	63,680	1,575	1,859		
European	1,852 7,992	2,602 2,602	0 0	0 0	4,487 5,477	3,437 4,601	6,339	6,039
arithmetic difference in (1)~(3) with that	based on the geometric ratio as follows.
x y	if	log θ(x) - log θ(y) > τage,	(10)
X ≈ y	if	| log θ(χ) - log θ(y)∣ ≤「age,	(11)
X Y y	if	log θ(x) - log θ(y) < -Tage,	(12)
which represent ‘older,’ ‘similar,’ and ‘younger.’ The consistency in (4) is also modified accordingly.
There are 5 reference images for each age class within range [15, 80] in this work (M = 330, N =
66). Thus, a test image should be compared with 330 references. However, we develop a two-
step approach, which does at most 130 comparisons but performs as good as the method using 330
comparisons. The two-step estimation is employed in all experiments. It is described in Appendix B.
We align all facial images using SeetaFaceEngine (Zhang et al., 2014) and resize them into 256 ×
256 × 3. Then, we crop a resized image into 224 × 224 × 3. For the feature extractors in Figure 2, we
use VGG16 without the FC layers (Simonyan & Zisserman, 2014). They yield 512-channel feature
vectors. Then, the vectors are concatenated and input to the ternary classifier, which has three FC
layers, yielding 512-, 512-, and 3-channel vectors sequentially. The 3-channel vector is normalized
to the softmax probabilities of the three categories '冷, ≈, Y.’ In (10)~(12), Tage is set to 0.1.
In KCH with K ≥ 2, the type (or chain) of a test image should be determined. Thus, we design a
type classifier, which shares the feature extractor with the comparator. Similarly to the ternary classi-
fier, the type classifier uses three FC layers, yielding 512-, 512-, and K-channel vectors sequentially.
The comparator and the type classifier are jointly trained.
To initialize the feature extractors, we adopt the VGG16 parameters pre-trained on ImageNet (Deng
et al., 2009). We randomly initialize all the other layers. We update the parameters using the Adam
optimizer (Kingma & Ba, 2014). We set the learning rate to 10-4 for the first 70 epochs. Then, we
select 5 references for each age class. Using the selected references, we fine-tune the network with a
learning rate of 10-5. We repeat the reference selection and the parameter fine-tuning up to 3 times.
In the case of unsupervised chains, we enforce the regularization constraint (line 7 in Algorithm 1).
By default, for each age, all chains are constrained to be assigned the same number of training
images. If there are L training images of θ-year-olds, the age classes θ in the K chains are assigned
L/K images, respectively, according to the affinity scores βk(X) by Algorithm 2 in Appendix A.
4.2	Datasets and evaluation metrics
MORPH II (Ricanek & Tesafaye, 2006) is the most popular age estimation benchmark, containing
about 55,000 facial images in the age range [16, 77]. IMDB-WIKI (Rothe et al., 2018) is another
dataset containing about 500,000 celebrity images obtained from IMDB and Wikipedia. It is some-
times used to pre-train age estimation networks. Optionally, we also select 150,000 clean data from
IMDB-WIKI to pre-train the proposed pairwise comparator.
Although several facial age datasets are available, most are biased to specific ethnic groups or gen-
ders. Data unbalance restricts the usability and degrades the generalization performance. Thus, we
form a ‘balanced dataset’ from MORPH II, AFAD (Niu et al., 2016), and UTK (Zhang et al., 2017b).
Table 1 shows how the balanced dataset is organized. Before sampling images from MORPH II,
AFAD, and UTK, we rectify inconsistent labels by following the strategy in Yip et al. (2018). For
each combination of gender in {female, male} and ethnic group in {African, Asian, European}, we
sample about 6,000 images. Also, during the sampling, we attempt to make the age distribution as
7
Published as a conference paper at ICLR 2020
Table 2: Performance comparison on the MORPH II dataset: * means that the networks are pre-
trained on IMDB-WIKI, and f the values are read from the reported CS curves or measured by
experiments. The best results are boldfaced, and the second best ones are underlined.
Setting A	Setting B Setting C (SE) Setting D (RS)
	MAE	CS(%)	MAE	CS(%)	MAE	CS(%)	MAE	CS(%)
OHRank (Chang et al., 2011)	-	-	-	-	-	-	6.07	56.3
OR-CNN (Niu et al., 2016)	-	-	-	-	-	-	3.27	73.0*
Ranking-CNN (Chen et al., 2017)	-	-	-	-	-	-	2.96	85.0*
DMTL (Han et al., 2018)	-	-	-	-	3.00	85.3	-	-
DEX* (Rothe et al., 2018)	2.68	-	-	-	-	-	-	-
DRFs (Shen et al., 2018)	2.91	82.9	2.98	-	-	-	2.17	91.3
MO-CNN* (Tan et al., 2018)	2.52	85.0*	2.70	83.0*	-	-	-	-
MV (Pan et al., 2018)	-	-	-	-	2.80	87.0*	2.41	90.0*
MV*	-	-	-	-	2.79	-	2.16	-
BridgeNet* (Li et al., 2019)	2.38	91.0*	2.63	86.0*	-	-	-	-
Proposed (1CH)	2.69	89.1	3.00	85.2	2.76	88.0	2.32	92.4
Proposed* (1CH)	2.41	91.7	2.75	88.2	2.68	88.8	2.22	93.3
uniform as possible within range [15, 80]. The balanced dataset is partitioned into training and test
subsets with ratio 8 : 2.
For performance assessment, we calculate the mean absolute error (MAE) (Lanitis et al., 2004) and
the cumulative score (CS) (Geng et al., 2006). MAE is the average absolute error between predicted
and ground-truth ages. Given a tolerance level l, CS computes the percentage of test images whose
absolute errors are less than or equal to l. In this work, l is fixed to 5, as done in Chang et al. (2011),
Han et al. (2018), and Shen et al. (2018).
4.3	Experimental results
Table 2 compares the proposed algorithm (1CH) with conventional algorithms on MORPH II. As
evaluation protocols for MORPH II, we use four different settings, including the 5-fold subject-
exclusive (SE) and the 5-fold random split (RS) (Chang et al., 2010; Guo & Wang, 2012). Ap-
pendix C.1 describes these four settings in detail and provides an extended version of Table 2.
OHRank, OR-CNN, and Ranking-CNN are all based on ordinal regression. OHRank uses traditional
features, yielding relatively poor performances, whereas OR-CNN and Ranking-CNN use CNN fea-
tures. DEX, DRFs, MO-CNN, MV, and BridgeNet employ VGG16 as backbone networks. Among
them, MV and BridgeNet achieve the state-of-the-art results, by employing the mean-variance loss
and the gating networks, respectively. The proposed algorithm outperforms these algorithms in set-
ting C, which is the most challenging task. Furthermore, in terms of CS, the proposed algorithm
yields the best performances in all four settings. These outstanding performances indicate that order
learning is an effective approach to age estimation.
In Table 3, we analyze the performances of the proposed algorithm on the balanced dataset according
to the number of hypothesized chains. We also implement and train the state-of-the-art MV on the
balanced dataset and provide its results using supervised chains.
Let us first analyze the performances of the proposed algorithm using ‘supervised’ chains. The
MAE and CS scores on the balanced dataset are worse than those on MORPH II, since the balanced
dataset contains more diverse data and thus is more challenging. By processing facial images sepa-
rately according to the genders (2CH), the proposed algorithm reduces MAE by 0.05 and improves
CS by 0.2% in comparison with 1CH. Similar improvements are obtained by 3CH or 6CH, which
consider the ethnic groups only or both gender and ethnic groups, respectively. In contrast, in the
case of MV, multi-chain hypotheses sometimes degrade the performances; e.g., MV (6CH) yields
a lower CS than MV (1CH). Regardless of the number of chains, the proposed algorithm trains a
single comparator but uses a different set of references for each chain. The comparator is a ternary
classifier. In contrast, MV (6CH) should train six different age estimators, each of which is a 66-way
classifier, to handle different chains. Thus, their training is more challenging than that of the single
ternary classifier. Note that, for the multi-chain hypotheses, the proposed algorithm first identifies
the chain ofa test image using the type classifiers, whose accuracies are about 98%. In Table 3, these
8
Published as a conference paper at ICLR 2020
Table 3: Comparison of the proposed algorithm with MV on the balanced dataset. In MV and the
supervised algorithm, multi-chain hypotheses divide data by the genders and/or the ethnic groups.
	MAE				CS(%)			
	1CH	2CH	3CH	6CH	1CH	2CH	3CH	6CH
MV (Pan et al., 2018)	4.49	4.52	4.44	4.40	69.9	70.1	70.3	69.6
Proposed (supervised)	4.23	4.18	4.19	4.18	73.2	73.4	73.4	73.4
Proposed (unsupervised)	-	4.16	4.17	4.16	-	74.0	73.9	74.0
Test image Reference images
\自士
22
Male
Asian
15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50
Figure 4: Age estimation in 6CH: Only the references of ages from 15 to 50 are shown. Comparison
results are color-coded. Cyan, yellow, and magenta mean that the test subject is older than (),
similar to (≈), and younger than (Y) a reference. The age is estimated correctly as 22.
type classifiers are used to obtain the results of the proposed algorithm, whereas the ground-truth
gender and ethnic group of each test image are used for MV.
Figure 4 shows how to estimate an age in 6CH. In this test, the subject is a 22-year-old Asian male.
He is compared with the references who are also Asian males. Using the comparison results, the age
is correctly estimated as 22 by the MC rule in (5).
Table 4 lists the MAE results for each test chain. Europeans yield poorer MAEs than Africans or
Asians. However, this is not due to inherent differences between ethnic groups. It is rather caused
by differences in image qualities. As listed in Table 1, more European faces are sampled from UTK.
The UTK faces were crawled from the Internet and their qualities are relatively low. Also, from the
cross-chain test results using 6CH, some observations can be made:
•	Except for the As-F test chain, the lowest MAE is achieved by the references in the same chain.
•	Eu-M and Eu-F are mutually compatible. For Eu-M, the second best performance is obtained by
the Eu-F references, and vice versa. On the other hand, some chains, such as Af-M and Eu-F, are
less compatible for the purpose of the proposed age estimation.
Table 3 also includes the performances of the proposed algorithm using ‘unsupervised’ chains. The
unsupervised algorithm outperforms the supervised one, which indicates that the gender or ethnic
group is not the best information to divide data for age estimation. As in the supervised case, 2CH,
3CH, and 6CH yield similar performances, which means that two chains are enough for the balanced
set. Compared with MV (1CH), the unsupervised algorithm (2CH) improves the performances
significantly, by 0.33 in terms of MAE and 4.1% in terms of CS.
Figure 5 shows how training images are divided into two chains in the unsupervised 2CH. During the
membership update, for each age, each chain is regularized to include at least a certain percentage
(κ) of the training images. In the default mode, the two chains are assigned the same number of
images with κ = 50%. However, Appendix C.3 shows that the performance is not very sensitive
to κ. At κ = 10%, MAE = 4.17 and CS = 73.7%. From Figure 5, we observe
•	The division of the chains is not clearly related to genders or ethnic groups. Regardless of genders
or ethnic groups, about half of the images are assigned to chain 1 and the others to chain 2.
•	At κ = 10%, chain 1 mostly consists of middle ages, while chain 2 of 10s, 20s, 60s, and 70s.
•	At κ = 50%, there is no such strong age-dependent tendency. But, for some combinations of
gender, ethnic group, and age band, it is not equal division. For example, for Asian females, a
majority of 40s are assigned to chain 1 but a majority of 50s and 60s are assigned to chain 2.
The unsupervised algorithm is designed to divide instances into multiple clusters when gender and
ethnic group information is unavailable. As shown in Appendix C.3, different K's yield various
clustering results. Surprisingly, these different clusters still outperform the supervised algorithm.
9
Published as a conference paper at ICLR 2020
Table 4: Cross-chain tests on the balanced dataset (MAEs). For example, in 6CH, when African
male references are used to estimate the ages of Asian females, the resultant MAE is 3.82.
Test chain
Method	Reference chain	Af-M	Af-F	As-M	As-F	Eu-M	Eu-F
1CH	All	3.87	3.82	3.98	3.79	5.21	4.69
6CH	African-Male	3.85	3.79	4.03	3.82	5.50	5.00
	African-Female	4.02	3.65	4.18	3.85	5.42	5.02
	Asian-Male	3.97	3.75	3.97	3.81	5.48	4.87
	Asian-Female	4.06	3.78	4.05	3.78	5.69	4.89
	European-Male	3.99	3.71	4.02	3.80	5.13	4.66
	European-Female	4.45	3.79	4.11	3.77	5.21	4.65
Af-M	k= 10%	k = 50%	■ chain 1 chain 2 :■■■■■■ ■
Af-F		:■■■■■■■
As-M		:"■■■■■■
As-F	:■.一■■■■	:・■■■■■■
Eu-M		:■■■■■■■
Eu-F	10s	20s	30s	40s	50s	60s	70s Age	:・■■■■■■ 10 s	20 s	30s	40s	50s	60s	70s Age
Figure 5: Distributions of training images in the unsupervised algorithm (2CH).
For example, at κ = 10%, let us consider the age band of 20s and 30s. If the references in chain 2
are used to estimate the ages of people in chain 1, the average error is 4.6 years. On the contrary, if
the references in chain 1 are used for chain 2, the average error is -5.4 years. These opposite biases
mean that people in chain 1 tend to look older than those in chain 2. These ‘looking-older’ people in
20s and 30s compose the blue cluster (chain 1) together with most people in 40s and 50s in Figure 5.
In this case, ‘looking-older’ people in 20s and 30s are separated from ‘looking-younger’ ones by the
unsupervised algorithm. This is more effective than the gender-based or ethnic-group-based division
of the supervised algorithm. Appendix C presents more results on age estimation.
5 Conclusions
Order learning was proposed in this work. In order learning, classes form an ordered set, and each
class represents object instances of the same rank. Its goal is to determine the order graph of classes
and classify a test instance into one of the classes. To this end, we designed the pairwise comparator
to learn ordering relationships between instances. We then decided the class of an instance by
comparing it with reference instances in the same chain and maximizing the consistency among the
comparison results. For age estimation, it was shown that the proposed algorithm yields the state-
of-the-art performance even in the case of the single-chain hypothesis. The performance is further
improved when the order graph is divided into multiple disjoint chains.
In this paper, we assumed that the order graph is composed of disjoint chains. However, there are
more complicated graphs, e.g. Figure 1(a), than disjoint chains. For example, it is hard to recognize
an infant’s sex from its facial image (Porter et al., 1984). But, after puberty, male and female take
divergent paths. This can be reflected by an order graph, which consists of two chains sharing
common nodes up to a certain age. It is an open problem to generalize order learning to find an
optimal order graph, which is not restricted to disjoint chains.
Acknowledgements
This work was supported by ‘The Cross-Ministry Giga KOREA Project’ grant funded by the Korea
government (MSIT) (No. GK19P0200, Development of4D reconstruction and dynamic deformable
action model based hyperrealistic service technology), and by the National Research Foundation of
Korea (NRF) grant funded by the Korea government (MSIP) (No. NRF-2018R1A2B3003896).
10
Published as a conference paper at ICLR 2020
References
A. Bachem. Absolute pitch. J. Acoust. Soc. Am., 27(6):1180-1185, November 1955.
Robert G. Bartle. The Elements of Real Analysis. John Wiley & Sons, 2nd edition, 1976.
Mark Braverman and Elchanan Mossel. Noisy sorting without resampling. In Symp. Discrete Algo-
rithms, pp. 268-276, 2008.
Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, and Greg Hul-
lender. Learning to rank using gradient descent. In ICML, pp. 89-96, 2005.
Kuang-Yu Chang, Chu-Song Chen, and Yi-Ping Hung. A ranking approach for human ages estima-
tion based on face images. In ICPR, pp. 3396-3399, 2010.
Kuang-Yu Chang, Chu-Song Chen, and Yi-Ping Hung. Ordinal hyperplanes ranker with cost sensi-
tivities for age estimation. In CVPR, pp. 585-592, 2011.
Shixing Chen, Caojin Zhang, Ming Dong, Jialiang Le, and Mike Rao. Using Ranking-CNN for age
estimation. In CVPR, pp. 5183-5192, 2017.
Weifeng Chen, Zhao Fu, Dawei Yang, and Jia Deng. Single-image depth perception in the wild. In
NIPS, pp. 730-738, 2016.
Thomas M. Cover and Joy A. Thomas. Elements of Information Theory. Wiley, 2006.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale
hierarchical image database. In CVPR, pp. 248-255, 2009.
Eibe Frank and Mark Hall. A simple approach to ordinal classification. In ECML, pp. 145-156,
2001.
Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Batmanghelich, and Dacheng Tao. Deep ordinal
regression network for monocular depth estimation. In CVPR, pp. 2002-2011, 2018.
Xin Geng, Zhi-Hua Zhou, Yu Zhang, Gang Li, and Honghua Dai. Learning from facial aging
patterns for automatic age estimation. In ACM Multimedia, pp. 307-316, 2006.
Xin Geng, Zhi-Hua Zhou, and Kate Smith-Miles. Automatic age estimation based on facial aging
patterns. IEEE Trans. Pattern Anal. Mach. Intell., 29(12):2234-2240, December 2007.
A. Gersho and R. M. Gray. Vector Quantization and Signal Compression. Kluwer Academic Pub-
lishers Norwell, 1991.
Jonathan L. Gross and Jay Yellen. Graph Theory and Its Applications. Chapman & Hall, 2nd
edition, 2006.
Guodong Guo and Guowang Mu. Simultaneous dimensionality reduction and human age estimation
via kernel partial least squares regression. In CVPR, pp. 657-664, 2011.
Guodong Guo and Xiaolong Wang. A study on human age estimation under facial expression
changes. In CVPR, pp. 2547-2553, 2012.
Guodong Guo, Guowang Mu, Yun Fu, and Thomas S. Huang. Human age estimation using bio-
inspired features. In CVPR, pp. 112-119, 2009.
Hu Han, Anil K. Jain, Fang Wang, Shiguang Shan, and Xilin Chen. Heterogeneous face attribute
estimation: A deep multi-task learning approach. IEEE Trans. Pattern Anal. Mach. Intell., 40
(11):2597-2609, November 2018.
Ralf Herbrich, Thore Graepel, and Klaus Obermayer. Support vector learning for ordinal regression.
In ICANN, pp. 97-102, 1999.
Karel Hrbacek and Thomas Jech. Introduction to Set Theory. Marcel Dekker, Inc., 2nd edition,
1984.
11
Published as a conference paper at ICLR 2020
Ivan Huerta, Carles Fernandez, Carlos Segura, Javier Hernando, and Andrea Prati. A deep analysis
on age estimation. Pattern Recog. Lett., 68:239-249, December 2015.
Kevin G. Jamieson and Robert D. Nowak. Active ranking using pairwise comparisons. In NIPS, pp.
2240-2248, 2011.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Young Ho Kwon and Niels da Vitoria Lobo. Age classification from facial images. In CVPR, pp.
762-767, 1994.
Andreas Lanitis, Chrisina Draganova, and Chris Christodoulou. Comparing different classifiers for
automatic age estimation. IEEE Trans. Syst., Man, Cybern. B, Cybern., 34(1):621-628, February
2004.
Jae-Han Lee and Chang-Su Kim. Monocular depth estimation using relative depth maps. In CVPR,
pp. 9729-9738, 2019a.
Jun-Tae Lee and Chang-Su Kim. Image aesthetic assessment based on pairwise comparison - a
unified approach to score regression, binary classification, and personalization. In ICCV, pp.
1191-1200, 2019b.
Ling Li and Hsuan-Tien Lin. Ordinal regression by extended binary classification. In NIPS, pp.
865-872, 2007.
Wanhua Li, Jiwen Lu, Jianjiang Feng, Chunjing Xu, Jie Zhou, and Qi Tian. BridgeNet: A continuity-
aware probabilistic network for age estimation. In CVPR, pp. 1145-1154, 2019.
Tie-Yan Liu. Learning to rank for information retrieval. Foundations and Trends in Informatioon
Retrieval, 3(3):225-331, 2009.
Brian McFee and Gert Lanckriet. Metric learning to rank. In ICML, pp. 775-782, 2010.
Sahand Negahban, Sewoong Oh, and Devavrat Shah. Iterative ranking from pair-wise comparisons.
In NIPS, pp. 2474-2482, 2012.
Zhenxing Niu, Mo Zhou, Le Wang, Xinbo Gao, and Gang Hua. Ordinal regression with multiple
output CNN for age estimation. In CVPR, pp. 4920-4928, 2016.
Hongyu Pan, Hu Han, Shiguang Shan, and Xilin Chen. Mean-variance loss for deep age estimation
from a face. In CVPR, pp. 5285-5294, 2018.
Gabriel Panis, Andreas Lanitis, Nicholas Tsapatsoulis, and Timothy F. Cootes. Overview of research
on facial ageing using the FG-NET ageing database. IET Biometrics, 5(2):37-46, 2016.
Richard H. Porter, Jennifer M. Cernoch, and Rene D. Balogh. Recognition of neonates by facial-
visual characteristics. Pediatrics, 74(4):501-504, October 1984.
Karl Ricanek and Tamirat Tesafaye. MORPH: A longitudinal image database of normal adult age-
progression. In FGR, pp. 341-345, 2006.
Rasmus Rothe, Radu Timofte, and Luc Van Gool. Deep expectation of real and apparent age from a
single image without facial landmarks. Int. J. Comput. Vis., 126(2):144-157, April 2018.
Thomas L. Saaty. A scaling method for priorities in hierarchical structures. J. Math. Psychol., 15
(3):234-281, June 1977.
Amartya Sen. Quasi-transitivity, rational choice and collective decisions. Rev. Econ. Stud., 36(3):
381-393, July 1969.
Wei Shen, Yilu Guo, Yan Wang, Kai Zhao, Bo Wang, and Alan Yuille. Deep regression forests for
age estimation. In CVPR, pp. 2304-2313, 2018.
12
Published as a conference paper at ICLR 2020
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. In ICLR, 2014.
Zichang Tan, Shuai Zhou, Jun Wan, Zhen Lei, and Stan Z. Li. Age estimation based on a single
network with soft softmax of aging modeling. In ACCV, 2016.
Zichang Tan, Jun Wan, Zhen Lei, Ruicong Zhi, Guodong Guo, and Stan Z. Li. Efficient group-n
encoding and decoding for facial age estimation. IEEE Trans. Pattern Anal. Mach. Intell., 40(11):
2610-2623, November 2018.
Ming-Feng Tsai, Tie-Yan Liu, Tao Qin, Hsin-Hsi Chen, and Wei-Ying Ma. FRank: A ranking
method with fidelity loss. In SIGIR, pp. 383-390, 2007.
Fabian L. Wauthier, Michael I. Jordan, and Nebojsa Jojic. Efficient ranking from pairwise compar-
isons. In ICML, pp. 109-117, 2013.
Kilian Q. Weinberger, John Blitzer, and Lawrence K. Saul. Distance metric learning for large margin
nearest neighbor classification. In NIPS, pp. 1473-1480, 2006.
Eric P. Xing, Andrew Y. Ng, Michael I. Jordan, and Stuart Russell. Distance metric learning with
application to clustering with side-information. In NIPS, pp. 521-528, 2003.
Dong Yi, Zhen Lei, and Stan Z. Li. Age estimation by multi-scale convolutional network. In ACCV,
2014.
Benjamin Yip, Garrett Bingham, Katherine Kempfert, Jonathan Fabish, Troy Kling, Cuixian Chen,
and Yishi Wang. Preliminary studies on a large face database. In ICBD, pp. 2572-2579, 2018.
ByungIn Yoo, Youngjun Kwak, Youngsung Kim, Changkyu Choi, and Junmo Kim. Deep facial age
estimation using conditional multitask learning with weak label expansion. IEEE Signal Process.
Lett., 25(6):808-812, June 2018.
Chao Zhang, Shuaicheng Liu, Xun Xu, and Ce Zhu. C3AE: Exploring the limits of compact model
for age estimation. In CVPR, pp. 12587-12596, 2019.
Jie Zhang, Shiguang Shan, Meina Kan, and Xilin Chen. Coarse-to-fine auto-encoder networks
(CFAN) for real-time face alignment. In ECCV, pp. 1-16, 2014.
Yunxuan Zhang, Li Liu, Cheng Li, and Chen Change Loy. Quantifying facial age by posterior of
age comparisons. In BMVC, 2017a.
Zhifei Zhang, Yang Song, and Hairong Qi. Age progression/regression by conditional adversarial
autoencoder. In CVPR, pp. 5810-5818, 2017b.
13
Published as a conference paper at ICLR 2020
A Regularized membership update
During the chain membership update in Algorithm 1, we assign an instance x to chain k to maximize
βk(x) subject to the regularization constraint. As mentioned in Section 4.1, in age estimation, this
regularization is enforced for each age. Let X denote the set of θ-year-olds for a certain θ. Also,
let K = {0, 1, . . . , K - 1} be the set of chains. Suppose that we should assign at least a certain
number (L) of instances in X to each chain. This is done by calling RegularAssign(K, X, L) in
Algorithm 2, which is a recursive function. Algorithm 2 yields the membership function c(x) as
output. For example, c(x) = 1 means that x belongs to chain 1.
Algorithm 2 RegularAssign(K, X, L)
Input: K = set of chains, X = set of instances, and L = minimum number
1:	for each k ∈ K do
2:	Xk = 0
3:	end for
4:	for each x ∈ X do
5:	c(x) = arg maxk∈K βk(x)
6:	Xc(x) = Xc(x) ∪ {x}
7:	end for
8:	km = arg mink∈K |Xk|
9:	if |Xkm | ≥ L then
10:	return
11:	else
12:	X =X-Xkm
13:	while |Xkm | <	L do
14:	x0 = maxx∈X βkm (x)
15:	X= X-{x0}
16:	Xkm =Xkm∪{x0}
17:	end while
18:	RegularAssign(K - {km}, X, L)
19:	end if
Output: Membership function c(x)
. Initialize chains
. Irregular partitioning
. Chain of the minimum size
. Increase Xkm
. Recursion
B	Two-step estimation
There are 5 reference images for each age within range [15, 80] in this work. Thus, for the age
estimation of a test image using the MC rule in (5), the test image should be compared with M = 330
reference images. However, we reduce the number of comparisons using a two-step approach. First,
the test image is compared with the 35 references of ages 15, 25, . . . , 75 only, and a rough age
estimate θ1 is obtained using the MC rule. Second, it is compared with the 105 references of all ages
within [θ1 - 10, θ1 + 10], and the final estimate θ2 is obtained. Since there are at least 10 common
references in the first and second steps, the two-step estimation requires at most 130 comparisons.
14
Published as a conference paper at ICLR 2020
C More experiments
C.1 Performance comparison on MORPH II
Four experimental settings are used for performance comparison on MORPH II (Ricanek &
Tesafaye, 2006).
•	Setting A: 5,492 images of Europeans are randomly selected and then divided into training
and testing sets with ratio 8:2 (Chang et al., 2011).
•	Setting B: About 21,000 images are randomly selected, while restricting the ratio between
Africans and Europeans to 1:1 and that between females and males to 1:3. They are di-
vided into three subsets (S1, S2, S3). The training and testing are done under two sub-
settings (Guo & Mu, 2011).
-(B1) training on S1, testing on S2 + S3
-(B2) training on S2, testing onS1 + S3
•	Setting C (SE): The entire dataset is randomly split into five folds, subject to the con-
straint that the same person’s images should belong to only one fold, and the 5-fold cross-
validation is performed.
•	Setting D (RS): The entire dataset is randomly split into five folds without any constraint,
and the 5-fold cross-validation is performed.
Table 5 is an extended version of Table 2. It includes the results of more conventional algorithms.
Table 5: Performance comparison on the MORPH II dataset: * means that the networks are pre-
trained on IMDB-WIKI, and f the values are read from the reported CS curves or measured by
experiments. The best results are boldfaced, and the second best ones are underlined.
	Setting A		Setting B		Setting C (SE)		Setting D (RS)	
	MAE	CS(%)	MAE	CS(%)	MAE	CS(%)	MAE	CS(%)
RED-SVM (Chang et al., 2010)	-	-	-	-	-	-	6.49	49.0*
OHRank (Chang et al., 2011)	-	-	-	-	-	-	6.07	56.3
KPLS (Guo & Mu, 2011)	-	-	4.18	-	-	-	-	-
CPLF (Yi et al., 2014)	-	-	3.63	-	-	-	-	-
Huerta et al. (Huerta et al., 2015)	-	-	-	-	3.88	-	-	-
OR-CNN (Niu et al., 2016)	-	-	-	-	-	-	3.27	73.0*
Tan et al. (Tan et al., 2016)	-	-	3.03	-	-	-	-	-
Ranking-CNN (Chen et al., 2017)	-	-	-	-	-	-	2.96	85.0*
DMTL (Han et al., 2018)	-	-	-	-	3.00	85.3	-	-
DEX (Rothe et al., 2018)	3.25	-	-	-	-	-	-	-
DEX*	2.68	-	-	-	-	-	-	-
CMT (Yoo et al., 2018)	-	-	-	-	2.91	-	-	-
DRFs (Shen et al., 2018)	2.91	82.9	2.98	-	-	-	2.17	91.3
MO-CNN (Tan et al., 2018)	2.93	83.0*	2.86	82.0*	-	-	-	-
MO-CNN*	2.52	85.0*	2.70	83.0*	-	-	-	-
MV (Pan et al., 2018)	-	-	-	-	2.80	87.0*	2.41	90.0*
MV*	-	-	-	-	2.79	-	2.16	-
C3AE (Zhang et al., 2019)	-	-	-	-	-	-	2.78	-
C3AE*	-	-	-	-	-	-	2.75	-
BridgeNet* (Li et al., 2019)	2.38	91.0*	2.63	86.0*	-	-	-	-
Proposed (1CH)	2.69	89.1	3.00	85.2	2.76	88.0	2.32	92.4
Proposed* (1CH)	2.41	91.7	2.75	88.2	2.68	88.8	2.22	93.3
15
Published as a conference paper at ICLR 2020
C.2 Generalization performance of comparator on FG-NET
We assess the proposed age estimator (1CH) on the FG-NET database (Panis et al., 2016). FG-NET
is a relatively small dataset, composed of 1,002 facial images of 82 subjects. Ages range from 0 to
69. For FG-NET, the leave one person out (LOPO) approach is often used for evaluation. In other
words, to perform tests on each subject, an estimator is trained using the remaining 81 subjects.
Then, the results are averaged over all 82 subjects.
In order to assess the generalization performance, we do not retrain the comparator on the FG-NET
data. Instead, we fix the comparator trained on the balanced dataset and just select references from
the remaining subjects’ faces in each LOPO test. For the comparator, the arithmetic scheme in
(1)~(3) is tested as well as the default geometric scheme in (10)~(12).
For comparison, MV (Pan et al., 2018) is tested, but it is trained for each LOPO test.
Table 6 summarizes the comparison results. MV provides better average performances on the entire
age range [0, 69] than the proposed algorithm does. This is because the balanced dataset does not
include subjects of ages between 0 and 14. Ifwe reduce the test age range to [15, 69], the proposed
algorithm outperforms MV, even though the comparator is not retrained. These results indicate
that the comparator generalizes well to unseen data, as long as the training images cover a desired
age range. Also, note that the geometric scheme provides better performances than the arithmetic
scheme.
Table 6: Performance comparison on FG-NET. The average performances over test ages within
ranges [0, 69] and [15, 69] are reported, respectively.
	0 to 69		15to69	
	MAE	CS(%)	MAE	CS(%)
MV	3.98	79.5	6.00	63.7
Proposed (1CH, Geometric τage = 0.15)	8.04	41.4	4.90	64.3
Proposed (1CH, Arithmetic τ = 7)	9.26	33.1	5.32	64.1
Figure 6 compares MAEs according to a test age. Again, within the covered range [15, 69], the
proposed algorithm significantly outperforms MV especially when test subjects are older than 45.
Figure 6: MAEs of the proposed algorithm (1CH) and MV on FG-NET in terms of a test age.
16
Published as a conference paper at ICLR 2020
C.3 PERFORMANCE ACCORDING TO κ
Table 7: MAE and CS performances of the unsupervised algorithm (2CH) on the balanced dataset,
according to the minimum percentage (κ) constraint during the regularized membership update. The
performances are not very sensitive to κ. The best performances are achieved in the default mode,
i.e. at K = 50%.	_____________________________________
K (%)	10	20	30	40	50
Figure 7: Distributions of training images in the unsupervised algorithm (2CH) at κ = 20%, 30%,
and 40%. From Figures 5 and 7, we see that stronger age-dependent tendencies are observed, as κ
gets smaller.
17
Published as a conference paper at ICLR 2020
C.4 PERFORMANCE ACCORDING TO THRESHOLDS τ AND τae
age
The ordering relationship between two instances can be categorized via the arithmetic scheme in
(1)~(3) using a threshold T or the geometric scheme in (10)~(12) using a threshold Tage. Ta-
ble 8 lists the performances of the proposed algorithm (1CH) according to these thresholds. We see
that the geometric scheme outperforms the arithmetic scheme in general. The best performance is
achieved with Tage = 0.1, which is used in all experiments in the main paper. Note that the scores
are poorer than those in Table 3, since the comparator is trained for a smaller number of epochs to
facilitate this test. At Tage = 0.1, two teenagers are declared to be not ‘similar to’ each other if their
age difference is larger than about 1. Also, two forties are not ‘similar’ if the age difference is larger
than about 5.
Table 8: The performances of the proposed algorithm (1CH) on the balanced dataset according to
thresholds T and Tage .
τ for arithmetic scheme	τage for geometric scheme
	0	2	5	7	9	0.05	0.10	0.15	0.20	0.25
MAE	4.42	4.36	4.33	4.32	4.33	4.38	4.31	4.32	4.41	4.41
CS(%)	71.0	71.7	72.2	72.2	72.5	71.4	72.8	72.2	71.8	71.7
C.5 Performance according to number of references
Table 9: The performances of the proposed algorithm (supervised) on the balanced dataset according
to the number of references for each age class (M/N). In general, the performances get better with
more references. However, the performances are not very sensitive to M/N . They saturate when
M/N ≥ 5. Therefore, we set M/N = 5 in this work.
M/N	1CH		2CH		3CH		6CH		Average	
	MAE	CS(%)	MAE	CS(%)	MAE	CS(%)	MAE	CS(%)	MAE	CS(%)
1	4.321	72.43	4.180	72.98	4.199	73.20	4.168	73.76	4.217	73.09
2	4.318	72.43	4.182	73.00	4.200	73.23	4.170	73.64	4.218	73.08
3	4.313	72.61	4.175	73.04	4.200	73.29	4.170	73.68	4.214	73.16
4	4.311	72.58	4.178	72.96	4.197	73.24	4.176	73.62	4.215	73.10
5	4.309	72.61	4.177	73.02	4.197	73.27	4.168	73.72	4.213	73.16
6	4.308	72.66	4.178	73.01	4.195	73.20	4.170	73.76	4.213	73.16
7	4.308	72.70	4.179	73.00	4.196	73.24	4.167	73.81	4.213	73.19
8	4.306	72.69	4.178	72.94	4.196	73.31	4.172	73.71	4.213	73.16
9	4.305	72.63	4.180	73.04	4.194	73.36	4.172	73.72	4.213	73.19
10	4.305	72.65	4.180	73.07	4.193	73.35	4.173	73.75	4.213	73.21
C.6 Reference images
Figure 8 shows all references in the supervised 6CH.
18
Published as a conference paper at ICLR 2020
6CH Reference images
融M融融廊足拉门心："二
ela
nacirf
elame
6CN8CN]I>CN]9CNE 昌 K ZZ
09 6lrJ8sl>lrJ9lrJκ 工
寻 .9寸 2 百 A
。寸 6E 8E /ʃ
S S 5 S S S 3 S


naeporu
ela
naeporu
elame
Figure 8: All reference images in the supervised 6CH. For some ages in certain chains, the balanced dataset includes less than 5 faces. In such cases, there are less
than 5 references.
19
Published as a conference paper at ICLR 2020
C.7 Age estimation examples
22 Af-M (22 Af-M)
16 Af-F (16 Af-F)
20 Af-M (20 Af-M)
竺-M(18 AS-M)
20

As-F (20A⅞F)
23AS~F (23 AS-F)
15 Eu-F (15 EU-F)

1 Feu-M(17 Eu-M)
26 As-M (26 As-M) ^As~F (30 史)
39 As-F (39
29 Eu-M (29 Eu-M) 34 Eu-M (34 Eu-M)
35 Af-M (35 Af-M) 42 Af-M (42 Af-M)' 59 Af-M (59 Af-M)
27 Af-F (27 Af-F) 30 Af-M (30 Af-M)
--一， 一----- `——-> -—----- `————--
治目I
28 As-F (28 Eu-F)

44 As-M (44 As-M) 60 As-M (60 As-M)
36 Eu-F (36 Eu-F) 43 Eu-F (43 Eu-F) 58*Eu-M (58 Eu-M) 65 Eu-F (65 Eu-F)
(a)	Success cases
39 Af-F (28 AtF)
66 Af-F (53 Af-F)
31 Af-F (42 Af-F) 74 Af-M (61 Af-M)
________________ 23 Af-M (32 Af-M)
24 As-F (38 As-F) 27 As-M (38 As-M
28 ElbM (15 Eu-M)
24 Af-F (35 Af-F)
27 As-M (17
25 Eu-F (18 Eu-F) 35 Eu-F (24 EU-F)
37 Eu-F (25 Eu-F) 42 Ell-M (30 Eu-M)
50 Eu-F (33 Eu-F)
25 As-F (18 As-F)
35 Eu-M (74 Eu-M)
27 As-F (17 As-F) 66 Ag-M (80 AS∙M)
38 As-F (60 Eu-F) 67 Eu-M (45 Eu-M)
(b)	Failure cases
Figure 9: Age estimation results of the proposed algorithm (supervised 6CH). For each face, the es-
timated label is provided together with the ground-truth in parentheses. In (a), the ages are estimated
correctly. In the last row, third column, the ethnic group is misclassified. This happens rarely. In
(b), failure cases are provided. These are hard examples due to various challenging factors, such as
low quality photographs and occlusion by hairs, hats, hands, and stickers.
20