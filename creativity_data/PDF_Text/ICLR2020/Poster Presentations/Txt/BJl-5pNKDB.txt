Published as a conference paper at ICLR 2020
On Computation and Generalization of Gener-
ative Adversarial Imitation Learning
Minshuo Chen* Yizhou Wangt Tianyi Liu* Zhuoran Yang^ Xingguo L/
Zhaoran Wang** TUo Zhao*
* Georgia Tech,t Xian Jiaotong University, ^ Princeton University, ** Northwestern University
Ab stract
Generative Adversarial Imitation Learning (GAIL) is a powerful and practical ap-
proach for learning sequential decision-making policies. Different from Rein-
forcement Learning (RL), GAIL takes advantage of demonstration data by experts
(e.g., human), and learns both the policy and reward function of the unknown en-
vironment. Despite the significant empirical progresses, the theory behind GAIL
is still largely unknown. The major difficulty comes from the underlying temporal
dependency of the demonstration data and the minimax computational formula-
tion of GAIL without convex-concave structure. To bridge such a gap between
theory and practice, this paper investigates the theoretical properties of GAIL.
Specifically, we show: (1) For GAIL with general reward parameterization, the
generalization can be guaranteed as long as the class of the reward functions is
properly controlled; (2) For GAIL, where the reward is parameterized as a repro-
ducing kernel function, GAIL can be efficiently solved by stochastic first order
optimization algorithms, which attain sublinear convergence to a stationary solu-
tion. To the best of our knowledge, these are the first results on statistical and
computational guarantees of imitation learning with reward/policy function ap-
proximation. Numerical experiments are provided to support our analysis.
1	Introduction
As various robots (Tail et al., 2018), self-driving cars (Kuefler et al., 2017), unmanned aerial ve-
hicles (Pfeiffer et al., 2018) and other intelligent agents are applied to complex and unstructured
environments, programming their behaviors/policy has become increasingly challenging. These in-
telligent agents need to accommodate a huge number of tasks with unique environmental demands.
To address these challenges, many reinforcement learning (RL) methods have been proposed for
learning sequential decision-making policies (Sutton et al., 1998; Kaelbling et al., 1996; Mnih et al.,
2015). These RL methods, however, heavily rely on human expert domain knowledge to design
proper reward functions. For complex tasks, which are often difficult to describe formally, these RL
methods become impractical.
The Imitation Learning (IL, Argall et al. (2009); Abbeel & Ng (2004)) approach is a powerful
and practical alternative to RL. Rather than having a human expert handcrafting a reward function
for learning the desired policy, the imitation learning approach only requires the human expert to
demonstrate the desired policy, and then the intelligent agent (a.k.a. learner) learns to match the
demonstration. Most of existing imitation learning methods fall in the following two categories:
• Behavioral Cloning (BC, Pomerleau (1991)). BC treats the IL problem as supervised learning.
Specifically, it learns a policy by fitting a regression model over expert demonstrations, which di-
rectly maps states to actions. Unfortunately, BC has a fundamental drawback. Recall that in super-
vised learning, the distribution of the training data is decoupled from the learned model, whereas
in imitation learning, the agent’s policy affects what state is queried next. The mismatch between
training and testing distributions, also known as covariate shift (Ross & Bagnell, 2010; Ross et al.,
2011), yields significant compounding errors. Therefore, BC often suffers from poor generalization.
• Inverse Reinforcement Learning (IRL, Russell (1998); Ng et al. (2000); Finn et al. (2016); Levine
& Koltun (2012)). IRL treats the IL problem as bi-level optimization. Specifically, it finds a reward
1
Published as a conference paper at ICLR 2020
function, under which the expert policy is uniquely optimal. Though IRL does not have the error
compounding issue, its computation is very inefficient. Many existing IRL methods need to solve
a sequence of computationally expensive reinforcement learning problems, due to their bi-level
optimization nature. Therefore, they often fail to scale to large and high dimensional environments.
More recently, Ho & Ermon (2016) propose a Generative Adversarial Imitation Learning (GAIL)
method, which obtains significant performance gains over existing IL methods in imitating complex
expert policies in large and high-dimensional environments. GAIL generalizes IRL by formulating
the IL problem as minimax optimization, which can be solved by alternating gradient-type algo-
rithms in a more scalable and efficient manner.
Specifically, we consider an infinite horizon Markov Decision Process (MDP), where S denotes the
state space, A denotes the action space, P denotes the Markov transition kernel, r* denotes the
reward function, and p0 denotes the distribution of the initial state. We assume that the Markov
transition kernel P is fixed and there is an unknown expert policy ∏*: S → P (A), where P (A)
denotes the set of distributions over the action space. As can be seen, {st}tT=-01 essentially forms a
Markov chain with the transition kernel induced byπ* as Pπ* (S, SO) = Ea∈A π*(a | s)∙P(s0 | s,a).
Given n demonstration trajectories from ∏* denoted by {St(i),at(i)}tT=-01
,where i = 1,..., n, so 〜po,
at 〜π* (∙ | st), and st+ι 〜P(∙ | st,at), GAIL aims to learn π* by solving the following minimax
optimization problem,
minπ maxr∈R[Eπr(s, a) - Eπ* r(s, a)],	(1)
where En[r(s, a)] = limτ→∞ E[T PT-o1 r(st, at)∣π] denotes the average reward under the policy
π when the reward function is r, and E∏*[r(s,a)] = nT Pn=I PT-o1[r(s(",a(")] denotes the
empirical average reward over the demonstration trajectories. As shown in (1), GAIL aims to find a
policy, which attains an average reward similar to that of the expert policy with respect to any reward
belonging to the function class R.
For large and high-dimensional imitation learning problems, we often encounter infinitely many
states. To ease computation, we need to consider function approximations. Specifically, suppose
that for every s ∈ S and a ∈ A, there are feature vectors ψs ∈ RdS and ψa ∈ RdA associated with
a and s, respectively. Then we can approximate the policy and reward as
∏(∙∣s) = eω(ψs) and r(s, a) = eθ(ψs,ψa),
where πe and re belong to certain function classes (e.g. reproducing kernel Hilbert space or deep
neural networks, Ormoneit & Sen (2002); LeCun et al. (2015)) associated with parameters ω and θ,
respectively. Accordingly, we can optimize (1) with respect to the parameters ω and θ by scalable
alternating gradient-type algorithms.
Although GAIL has achieved significant empirical progresses, its theoretical properties are still
largely unknown. There are three major difficulties when analyzing GAIL: 1). There exists tempo-
ral dependency in the demonstration trajectories/data due to their sequential nature (Howard, 1960;
Puterman, 2014; Abounadi et al., 2001); 2). GAIL is formulated as a minimax optimization prob-
lem. Most of existing learning theories, however, focus on empirical risk minimization problems,
and therefore are not readily applicable (Vapnik, 2013; Mohri et al., 2018; Anthony & Bartlett,
2009); 3). The minimax optimization problem in (1) does not have a convex-concave structure, and
therefore existing theories in convex optimization literature cannot be applied for analyzing the al-
ternating stochastic gradient-type algorithms (Willem, 1997; Ben-Tal & Nemirovski, 1998; Murray
& Overton, 1980; Chambolle & Pock, 2011; Chen et al., 2014). Some recent results suggest to use
stage-wise stochastic gradient-type algorithms (Rafique et al., 2018; Dai et al., 2017). More specif-
ically, at every iteration, they need to solve the inner maximization problem up to a high precision,
and then apply stochastic gradient update to the outer minimization problem. Such algorithms, how-
ever, are rarely used by practitioners, as they are inefficient in practice (due to the computationally
intensive inner maximization).
To bridge such a gap between practice and theory, we establish the generalization properties of
GAIL and the convergence properties of the alternating mini-batch stochastic gradient algorithm for
solving (1). Specifically, our contributions can be summarized as follows:
2
Published as a conference paper at ICLR 2020
•	We formally define the generalization of GAIL under the “so-called” R-reward distance, and then
show that the generalization of GAIL can be guaranteed under reward distance as long as the class
of the reward functions is properly controlled;
•	We provide sufficient conditions, under which an alternating mini-batch stochastic gradient algo-
rithm can efficiently solve the minimax optimization in (1), and attains sublinear convergence to a
stationary solution.
To the best of our knowledge, these are the first results on statistical and computational theories of
imitation learning with reward/policy function approximations.
Our work is related to Syed et al. (2008); Cai et al. (2019). Syed et al. (2008) study the generalization
and computational properties of apprenticeship learning. Since they assume that the state space of
the underlying Markov decision process is finite, they do not consider any reward/policy function
approximations; Cai et al. (2019) study the computational properties of imitation learning under a
simple control setting. Their assumption on linear policy and quadratic reward is very restrictive,
and does not hold for many real applications.
N	otation. Given a vector x = (x1, ..., xd)> ∈ Rd, we define kxk22 = Pjd=1 xj2. Given a function
f : Rd → R,we denote its '∞ norm as ∣∣f k∞ = max、|f (x)].
2	Generalization of GAIL
To analyze the generalization properties of GAIL, we first assume that we can access an infinite num-
ber of the expert’s demonstration trajectories (underlying population), and that the reward function
is chosen optimally within some large class of functions. This allows us to remove the maximum
operation from (1), which leads to an interpretation of how and in what sense the resulting policy is
close to the true expert policy. Before we proceed, we first introduce some preliminaries.
Definition 1 (Stationary Distribution). Note that any policy π induces a Markov chain on S × A.
The transition kernel is given by
Pn(s0, a0 | s, a) = π(a01 S) ∙ P(s0 | s, a), ∀(s, a), (s0, a0) ∈ S × A.
When such a Markov chain is aperiodic and recurrent, we denote its stationary distribution as ρπ .
Note that a policy π is uniquely determined by its stationary distribution ρπ in the sense that
π(a | S) = Pn (S,a)/Pa∈A Pn(S,a).
Then we can write the expected average reward of r(s, a) under the policy π as
En [r(s,a)] =limT →∞ E[ T PT-1 r(st,at)∣π] = EPn 卜(s,a)] = P(s,a)∈s×A Pn (s,a) ∙ r(s,a).
We further define the R-distance between two policies π and π0 as follows.
Definition 2. Let R denote a class of symmetric reward functions from S × A to R, i.e., if r ∈ R,
then -r ∈ R. Given two policy π0 and π, the R-distance for GAIL is defined as
dR(π, π0) = supr∈R[Enr(S, a) - En0 r(S, a)].
The R-distance over policies for Markov decision processes is essentially an Integral Probability
Metric (IPM) over stationary distributions (Muller, 1997). For different choices of R, We have vari-
ous R-distances. For example, we can choose R as the class of all 1-Lipschitz continuous functions,
which yields that dR(π, π0) is the Wasserstein distance between Pn and Pn0 (Vallender, 1974). For
computational convenience, GAIL and its variants usually choose R as a class of functions from
some reproducing kernel Hilbert space, or a class of neural network functions.
Definition 3. Given n demonstration trajectories from time 0 to T - 1 obtained by an expert policy
π* denoted by (s(iLaf))T-1, where i = 1,..., n, a policy b learned by GAIL generalizes under the
R-distance "r(∙, ∙) with generalization error e, if with high probability, we have
∣dR(∏n,b) - dR(∏*,b)∣ ≤ e,
where dR∏n, b) is the empirical R-distance between ∏* and b defined as
dR(πn ,b) = suPr ∈R[En: r(S,a) - Ebbr(S,a)] with En* [r(S, a)]=春 P=ι PT-1[r(S(i) ,a(i))].
3
Published as a conference paper at ICLR 2020
The generalization of GAIL implies that the R-distance between the expert policy ∏ and the learned
policy πb is close to the empirical R-distance between them. Our analysis aims to prove the former
distance to be small, whereas the latter one is what we attempts to minimize in practice.
We then introduce the assumptions on the underlying Markov decision process and expert policy.
Assumption 1. Under the expert policy ∏*, (St, at)T- forms a stationary and exponentially β-
mixing Markov chain, i.e.,
β(k) = supnEB∈σ0n supA∈σn∞+k |P(A|B) - P(A)| ≤ β0 exp(-β1kα),
where β0, β1, α are positive constants, and σij is the σ-algebra generated by (st, at)tj=i for i ≤ j.
Moreover, for every s ∈ S and a ∈ A, there are feature vectors ψs ∈ RdS and ψa ∈ RdA associated
with a and s, respectively, and ψs and ψa are uniformly bounded, where
kψsk2 ≤ 1 and kψak2 ≤ 1, ∀s ∈ S and ∀a ∈ A.
Assumption 1 requires the underlying MDP to be ergodic (Levin & Peres, 2017), which is a com-
monly studied assumption in exiting reinforcement learning literature on maximizing the expected
average reward (Strehl & Littman, 2005; Li et al., 2011; Brafman & Tennenholtz, 2002; Kearns &
Singh, 2002). The feature vectors associated with a and s allow us to apply function approxima-
tions to parameterize the reward and policy functions. Accordingly, we write the reward function as
r(s, a) = re(ψs, ψa), which is assumed to be bounded.
Assumption 2. The reward function class is uniformly bounded, i.e., krk∞ ≤ Br for any r ∈ R.
Now we proceed with our main result on generalization properties of GAIL. We use N (R, e, k∙ ∣∣∞)
to denote the covering number of the function class R under the '∞ distance ∣∣ ∙ ∣∣∞.
Theorem 1 (Main Result). Suppose Assumptions 1-2 hold, and the policy learned by GAIL satisfies
dR(∏n, bb) 一 inf∏ d.(∏n, ∏) < e,
where the infimum is taken over all possible learned policies. Then with probability at least 1 - δ
over the joint distribution of {(a(ti), s(ti))tT=-01}in=1, we have
dR(π*,π) 一 inf dR(π*,π) ≤ O (PByZJbgN(r, qT, k ∙ k∞) + Br j lθT/;)) + e,
where ζ = (β-1 log 警)α ∙
Theorem 1 implies that the policy πb learned by GAIL generalizes as long as the complexity of the
function class R is well controlled. To the best of our knowledge, this is the first result on the
generalization of imitation learning with function approximations. As the proof of Theorem 1 is
involved, we only present a sketch due to space limit. More details are provided in Appendix A.1.
Proof Sketch. Our analysis relies on characterizing the concentration property of the empirical av-
erage reward under the expert policy. For notational simplicity, we define
φ = E∏*r(S, a) - nτ Pi=ι PT=01 r(s(i),a(i)).
The key challenge comes from the fact that (st(i) , a(ti) )’s are dependent. To handle such a depen-
dency, we adopt the independent block technique from Yu (1994). Specifically, we partition every
trajectory into disjoint blocks (where the block size is of the order O((log(T) +log(1∕δ))"ɑ), and
construct two separable trajectories: One contains all blocks with odd indices (denoted by Bodd),
and the other contains all those with even indices (denoted by Beven). We define
φ1 = E∏*r(s,a) - 等 Pn=I P(s(i),ati))∈Boddr(Sti),a(i)),
and analogously for φ2 with (St(i), at(i)) ∈ Beven. Then we have
P(SuPr∈R φ ≥ ε) ≤ P(SuPr∈R φ + suPr∈R φ ≥ ε) ≤ P(SuPr∈r Φι ≥ ε) + P(SuPr∈R Φ2 ≥ ε).
4
Published as a conference paper at ICLR 2020
We consider a block-wise independent counterpart of φ1 denoted by φ1, where each block is sampled
independently from the same Markov chain as φ1, i.e., φe1 has independent blocks of samples from
the same exponentially β-mixing Markov chain . Accordingly, we denote φe1 as
el = E∏* r(s, a) - nT Pn=I P(s(i),ati))∈Bodd r (S(i), a(i))，
where Beodd denotes i.i.d. blocks of samples. Now we bound the difference between φ1 and φe1 by
P(sup φ1 -	E[sup φ1]	≥ ε - E[ sup φ1])	≤ P( sup φ1	- E[sup φ1]	≥ ε - E[ sup φ1])
r∈R	r∈R	r∈R	r∈R	r∈R	r∈R
+ CβT∕(log(T ) + log(1∕δ)E
where C is a constant, and β is the mixing coefficient, and P(supr∈R φ1 - E[supr∈R φ1] ≥
ε - E[supr∈R φ1]) can be bounded using the empirical process technique for independent random
variables. The details of the above inequality can be found in Corollary 3 in Appendix A.1, where
the proof technique is adapted from Lemma 1 in Mohri & Rostamizadeh (2009). Let φe2 be defined
analogously as φe1 . With a similar argument further applied to φ2 and φe2 , we obtain
P(SuP φ ≥ ε) ≤ 2P(sup φι 一 E[sup φι] ≥ ε 一 E[sup φι]) + 2CβT∕(log(T) + log(1∕δ))"α.
The rest of our analysis follows the PAC-learning framework using Rademacher complexity and is
omitted (Mohri et al., 2018). We complete the proof sketch.	□
Example 1:	Reproducing Kernel Reward Function. One popular option to parameterize the
reward by functions is the reproducing kernel Hilbert space (RKHS, Kim & Park (2018); Li et al.
(2018)). There have been several implementations of RKHS, and we consider the feature mapping
approach. Specifically, we consider g : RdS × RdA → Rq, and the reward can be written as
r(S, a) = reθ(ψs, ψa) = θ>g(ψs, ψa),
where θ ∈ Rq. We require g to be Lipschitz continuous with respect to (ψa, ψs).
Assumption 3. The feature mapping g satisfies g(0, 0) = 0, and there exists a constant ρg such that
for any ψa, ψa0 , ψs and ψs0, we have
kg(ψs,ψa) — g(ψS ,ψa )k2 ≤ Pg Jkψs — ψS k2 + kψa — ψa k2 .
Assumption 3 is mild and satisfied by popular feature mappings, e.g., random Fourier feature map-
ping1 * (Rahimi & Recht, 2008; Bach, 2017). The next corollary presents the generalization bound of
GAIL using feature mapping.
Corollary 1. Suppose kθk2 ≤ Bθ. For large enough n and T, with probability at least 1 一 δ over
the joint distribution of {(at(i), S(ti))tT=-01}in=1, we have
"r(∏", b) — inf "r(∏*, ∏) ≤ O
π
(pB∕ζ MKT而+PgB ∕og≡)
+ .
Corollary 1 indicates that with respect to a class of properly normalized reproducing kernel reward
functions, GAIL generalizes in terms of the R-distance.
Example 2:	Neural Network Reward Function. Another popular option to parameterize the
reward function is to use neural networks. Specifically, let σ(v) = [max{v1, 0}, ..., max{vd, 0}]>
denote the ReLU activation for v ∈ Rd . We consider a D-layer feedforward neural network with
ReLU activation as follows,
r(S, a) = reW(ψs,ψa) = WD>σ(WD-1σ(...σ(W1[ψa>, ψs>]>))),
where W = {Wk | Wk ∈ Rdk-1 ×dk, k = 1, ..., D 一 1, WD ∈ RdD-1} and d0 = dA + dS. The
next corollary presents the generalization bound of GAIL using neural networks.
1More precisely, Assumption 3 actually holds with overwhelming probability over the distribution of the
random mapping.
5
Published as a conference paper at ICLR 2020
Corollary 2. Suppose kWik2 ≤ 1, where i = 1, ..., D. For large enough n and T, with probability
at least 1 - δ over the joint distribution of{(a(ti),st(i))tT=-01}in=1, we have
"r(∏", b) — inf "r(∏*, ∏) ≤ O
π
(PnT/ζ rd2Dlog (DpnT/ζJ+j⅛*)
+ .
Corollary 2 indicates that with respect to a class of properly normalized neural network reward
functions, GAIL generalizes in terms of the R-distance.
Remark 1 (The Tradeoff between Generalization and Representation of GAIL). As can be seen
from Definition 2, the R-distances are essentially differentiating two policies. For the Wasserstein-
type distance, i.e., R contains all 1-Lipschitz continuous functions, if dR(π, π0) is small, it is safe
to conclude that two policies π and π0 are nearly the same almost everywhere. However, when we
choose R to be the reproducing kernel Hilbert space or the class of neural networks with relatively
small complexity, dR(π, π0) can be small even if π and π0 are not very close. Therefore, we need to
choose a sufficiently diverse class of reward functions to ensure that we recover the expert policy.
As Theorem 1 suggests, however, that we need to control the complexity of the function class R
to guarantee the generalization. This implies that when parameterizing the reward function, we
need to carefully choose the function class to attain the optimal tradeoff between generalization and
representation of GAIL.
3 Computation of GAIL
To investigate the computational properties of GAIL, we parameterize the reward by functions be-
longing to some reproducing kernel Hilbert space. The implementation is based on feature mapping,
as mentioned in the previous section. The policy can be parameterized by functions belonging to
some reproducing kernel Hilbert space or some class of deep neural networks with parameter ω .
Specifically, We denote π(a∣s) = eω(ψs), where ∏ω(ψs) is the parametrized policy mapping from
RdS to a simplex in RdA with |A| = d. For computational convenience, we consider solving a
slightly modified minimax optimization problem:
minω max∣∣θ∣∣2≤κ Ee- [eθ(s, a)] - E∏*eθ[(Ψs, ψa)] - λH(∏ω) — 2 ∣∣θ∣∣2 ,	(2)
where reθ(s, a) = θ>g(ψs, ψa), H(πeω) is some regularizer for the policy (e.g., causal entropy regu-
larizer, Ho & Ermon (2016)), and λ > 0 and μ > 0 are tuning parameters. Compared with (1), the
additional regularizers in (2) can improve the optimization landscape, and help mitigate computa-
tional instability in practice.
We apply the alternating mini-batch stochastic gradient algorithm to (2). Specifically, we denote the
objective function in (2) as F(ω, θ) for notational simplicity. At the (t + 1)-th iteration, we take
θ(t+1) =∏κ(θ(t) + ηθ Pj∈M(t) Vθfj(ω㈤,θ⑴))and	(3)
ω(HI)= ω⑴—ηω Pj∈Mωt) Vω fj(ω(t),θ(t+1)),	(4)
where η and ηω are learning rates, the projection ∏κ(v) = 1(∣∣v∣∣2 ≤ K) ∙ V + 1(∣∣v∣∣2 > κ) ∙ K ∙
v/ kvk2 , Vfj’s and Vfj’s are independent stochastic approximations of VF (Sutton et al., 2000),
and M(θt), M(ωt) are mini-batches with sizes qθ and qω, respectively. Before we proceed with the
convergence analysis, we impose the follow assumptions on the problem.
Assumption 4. There are two positive constants Mω and Mθ such that for any ω and kθk2 ≤ K,
Unbiased : EVfj(ω, θ) = EVfej (ω, θ) = VF (ω, θ),
Bounded : E∣∣Vωfj(ω,θ) — VωF(ω,θ)k2 ≤ Mω and E∣∣Vθfj(ω,θ) — VθF(ω,θ)k2 ≤ Mθ.
Assumption 4 requires the stochastic gradient to be unbiased with a bounded variance, which is a
common assumption in existing optimization literature (Nemirovski et al., 2009; Ghadimi & Lan,
2013; Duchi et al., 2011; Bottou, 2010).
6
Published as a conference paper at ICLR 2020
Assumption 5. (i) For any ω, there exists some constant χ > 0 and υ ∈ (0, 1) such that
k(Pπeω )tρ0 - ρπeω kTV ≤ χυt ,
where Pπeω (s0, a0 | s, a) = πeω (a0 |s0)P (s0 | s, a) is the transition kernel induced by πeω, ρ0 is the initial
distribution of (s0, a0), and ρπeω is the stationary distribution induced by πeω.
(ii)	There exist constants Sπe , Bω , Lρ , LQ > 0 such that for any ω, ω0, we have
∣∣Vω lθg(eω (a∣s)) - Vω lθ>g,(∏ωjt (a∣s))∣∣2 ≤ Sn ∣∣ω - ω0∣∣2 ,	∣∣Vω log ∏ (a∣s)k2 ≤ Bω,
.. .. _ ,.. .. . ~ . ~ _ ,..
∣ρπeω - ρπeω0 ∣TV ≤ Lρ ∣ω - ω ∣2 ,	∣Q ω - Q ω ∣∞ ≤ LQ ∣ω - ω ∣2 ,
where Qπeω (s, a) = Pt∞=0 E [re(st, at) - Eπeω [re] | s0 = s, a0 = a,πeω] is the action-value function.
(iii)	There exist constants BH and SH > 0 such that for any ω, ω 0 , we have
H(πeω) ≤BH, and	∣VωH(πeω) -VωH(πeω0)∣2 ≤SH∣ω-ω0∣2.
Note that (i) of Assumption 5 requires the Markov Chain to be geometrically mixing. (ii) and (iii)
state some commonly used regularity conditions for policies (Sutton et al., 2000; Pirotta et al., 2015).
We then define L-Stationary points of F. Specifically, We say that (ω*,θ*) is a stationary point of
F, if and only if, for any fixed α > 0,
VωF(ω*, θ*) = 0 and θ* - ∏κ(θ* + αVθF(ω*, θ*)) = 0.
The L-stationarity is a generalization of the stationary point for unconstrained optimization, and is
a necessary condition for optimality. Accordingly, We take α = 1 and measure the sub-stationarity
of the algorithm at the iteration N by
JN = min1≤t≤N E∣θ(t) - Πκ(θ(t) + VθF (ω(t), θ(t)))∣22 + E∣VωF (ω(t), θ(t+1))∣22.
We then state the global convergence of the alternating mini-batch stochastic gradient algorithm.
Theorem 2. Suppose Assumptions 1-5 hold. We choose step sizes ηθ, ηω satisfying
Lω	1 ]	<i ʃ 1	7 Lω + 1	1
Sω(8Lω +2), 2Lω j, ηθ ≤ mm l≡μ, 150Sω , 100(2μ + Sω)
and meanwhile ηω/ηθ ≤ μ/(30Lω + 5), where Lω = 2√2(S∏ + 2BωLP)KPgχ∕(1 - υ) + BωLq,
and Sω = 2√2qκρgχBω/(1 - υ). Given any e > 0, we choose batch sizes qθ = O(1∕e) and
qω = O(1/e). Then we need at most
N = η(C0 + 4√2pg κ + μκ2 + 2λBH )e-1
iterations such that JN ≤ e, where C0 depends on the initialization, and η depends on ηω and ηθ .
Here O hides linear or quardic dependence on some constants in Assumptions 1-5. Theorem 2 shows
that though the minimax optimization problem in (2) does not have a convex-concave structure, the
alternating mini-batch stochastic gradient algorithm still guarantees to converge to a stationary point.
We are not aware of any similar results for GAIL in existing literature.
Proof Sketch. We prove the convergence by showing
PiN=1 Eθ(t) - Πκ(θ(t) + VθF (ω(t), θ(t)))22 + EVωF (ω(t), θ(t+1))22 ≤ C + Ne/2,	(5)
where C is a constant and Ne/2 is the accumulation of noise in stochastic approximations of VF.
Then we straightforwardly have NJN ≤ C + N e/2. Dividing both sizes by N, we can derive the
desired result. The main difficulty of showing (5) comes from the fact that the outer minimization
problem is nonconvex and we cannot solve the inner maximization problem exactly. To overcome
this difficulty, we construct a monotonically decreasing potential function:
E⑴=EF(ω㈤,θ(t)) + s((1 + 2ηωLω)/2 ∙ E∣∣ω(t) - ω(I)Il；
+ (ηω∕2ηθ - μηω∕4 + 3ηωηθμ2∕2) ∙ E∣∣θ(t+1) - θ(t) 俏 + μηω∕8 ∙ E∣∣θ⑴-θ(I)俏),
for a constant s to be chosen later. Denote ξθ(t) and ξω(t) as the i.i.d. noise of the stochastic gradients.
The following lemma characterizes the decrement of the potential function at each iteration.
ηω ≤ min
7
Published as a conference paper at ICLR 2020
Lemma 1. With the step sizes ηθ and ηω chosen as in Theorem 2, we have
E(t+1) -E(t) ≤ - k1Ekω(t+1) -ω(t)k22 - k2Ekω(t) - ω(t-1)k22 - k3Ekθ(t+2) - θ(t+1)k22
- k4Ekθ(t+1) - θ(t)k22 - k5Ekθ(t) - θ(t-1)k22 + ν(Eξω(t)k22 + Ekξθ(t)k22),
where ν is a constant depending on F, ηθ, and ηω. Moreover, we have constants k1 , k2 , k3, k4, k5 >
0 for S = 8∕(*(58Lω + 9)).
Let k = 1/min{k1, k4} and φ = max{1,1∕η22,1∕ηω}. We obtain
PN=IEMt)- Πκ(θ㈤ + vθF(ω⑴,θ㈤))俏 + E∣∣VωF(ω⑴”t+1))俏
≤(i)φ PtN=1 Ekθ(t+1) - θ(t)k22 + kω(t+1) - ω(t)k22 (≤ii)kφ(E(1) - E(N)) + kφNνEkξω(t)k22 + kξθ(t)k22,
where (i) follows from plugging in the update (3) as well as the contraction property of projection,
and (ii) follows from Lemma 1. Choosing qθ = 4kφνMθ∕ and qω = 4kφνMω ∕, we obtain
Pg1 E∣∣θ㈤一Πκ(θ⑴ + VθF(ω⑴,θ㈤))∣∣2 + E∣∣VωF(ω⑴，θ(HI))俏 ≤ kφ(E⑴-E(N)) + N.
We have E(N ) ≥ EF (ω(N ), θ(N )) by the construction of E(N ). It is easy to verify that F is lower
bounded (Lemma 10 in Appendix B). Eventually, We complete the proof by substituting the lower
bound and choosing N = kφ(2E⑴ + 4√2ρgκ + μκ2 + 2λBH)e-1.	□
4	Experiment
To verify our theory in Section 3, we conduct experiments in three reinforcement learning tasks:
Acrobot, MountainCar, and Hopper. For each task, we first train an expert policy using the proximal
policy optimization (PPO) algorithm in (Schulman et al., 2017) for 500 iterations, and then use the
expert policy to generate the demonstration data. The demonstration data for every task contains 500
trajectories, each of which is a series of state action pairs throughout one episode in the environment.
When training GAIL, we randomly select a mini-batch of trajectories, which contain at least 8192
state action pairs. We use PPO to update the policy parameters. This avoids the instability of the
policy gradient algorithm, and improves the reproducibility of our experiments.
-100
-200
-300
-400
-500
(.ɔueuj.IO七∙υd) P-IeMfυ-l ΦMra.-φ><
Figure 1: Performance of GAIL on three different tasks. The plotted curves are averaged over
5 independent runs with the vertical axis being the average reward and horizontal axis being the
number of iterations.
We use the same neural network architecture for all the environments. For policy, we use a fully
connected neural network with two hidden layers of 128 neurons in each layer and tanh activation.
For reward, we use a fully connected ReLU neural network with two hidden layers of 1024 and
512 neurons, respectively. To implement the kernel reward, we fix the first two layers of the neural
network after random initialization and only update the third layer, i.e., the first two layers mimic
the random feature mapping. We choose K = 1 and μ = 0.3. When updating the neural network
reward, we use weight normalization in each layer (Salimans & Kingma, 2016).
When updating the kernel reward at each iteration, we choose to take the stochastic gradient ascent
step for either once (i.e., alternating update in Section 3) or 10 times. When updating the neural
network reward at each iteration, we choose to take the stochastic gradient ascent step for only once.
We tune step size parameters for updating the policy and reward, and summarize the numerical
results of the step sizes attaining the maximal average episode reward in Figure 1.
8
Published as a conference paper at ICLR 2020
As can be seen, using multiple stochastic gradient ascent steps for updating the reward at each
iteration yields similar performance as that of one step. We present the convergence analysis of
using multiple stochastic gradient ascent steps for updating the reward in Appendix C. Moreover,
we observe that parameterizing the reward by neural networks slightly outperform that of the kernel
reward. However, its training process tends to be unstable and takes longer time to converge.
5	Discussions
Our proposed theories of GAIL are closely related to Generative Adversarial Networks (Goodfellow
et al., 2014; Arjovsky et al., 2017): (1) The generalization of GANs is defined based on the integral
probabilistic metric (IPM) between the synthetic distribution obtained by the generator network and
the distribution of the real data (Arora et al., 2017). As the real data in GANs are considered as
independent realizations of the underlying distribution, the generalization of GANs can be analyzed
using commonly used empirical process techniques for i.i.d. random variables. GAIL, however,
involves dependent demonstration data from experts, and therefore the analysis is more involved. (2)
Our computational theory of GAIL can be applied to MMD-GAN and its variants, where the IPM
is induced by some reproducing kernel Hilbert space (Li et al., 2017; Binkowski et al., 2018; Arbel
et al., 2018). The alternating mini-batch stochastic gradient algorithm attains a similar sublinear rate
of convergence to a stationary solution.
Moreover, our computational theory of GAIL only considers the policy gradient update when learn-
ing the policy (Sutton et al., 2000). Extending to other types of updates such as natural policy
gradient (Kakade, 2002), proximal policy gradient (Schulman et al., 2017) and trust region policy
optimization (Schulman et al., 2015) is a challenging, but important future direction.
References
Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In
Proceedings of the twenty-first international conference on Machine learning, pp. 1. ACM, 2004.
Jinane Abounadi, D Bertsekas, and Vivek S Borkar. Learning algorithms for markov decision pro-
cesses with average cost. SIAM Journal on Control and Optimization, 40(3):681-698, 2001.
Martin Anthony and Peter L Bartlett. Neural Network Learning: Theoretical Foundations. Cam-
bridge University Press, 2009.
Michael Arbel, Dougal Sutherland, Mikolaj BinkoWski, and Arthur Gretton. On gradient regular-
izers for mmd gans. In Advances in Neural Information Processing Systems, pp. 6700-6710,
2018.
Brenna D Argall, Sonia Chernova, Manuela Veloso, and Brett Browning. A survey of robot learning
from demonstration. Robotics and autonomous systems, 57(5):469-483, 2009.
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein generative adversarial networks.
In International Conference on Machine Learning, pp. 214-223, 2017.
Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium
in generative adversarial nets (gans). In Proceedings of the 34th International Conference on
Machine Learning-Volume 70, pp. 224-232. JMLR. org, 2017.
Francis Bach. On the equivalence between kernel quadrature rules and random feature expansions.
The Journal of Machine Learning Research, 18(1):714-751, 2017.
Aharon Ben-Tal and Arkadi Nemirovski. Robust convex optimization. Mathematics of operations
research, 23(4):769-805, 1998.
Mikolaj BinkOWski, Dougal J Sutherland, Michael Arbel, and Arthur Gretton. Demystifying mmd
gans. arXiv preprint arXiv:1801.01401, 2018.
Leon Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of
COMPSTAT’2010, pp. 177-186. Springer, 2010.
9
Published as a conference paper at ICLR 2020
Ronen I Brafman and Moshe Tennenholtz. R-max-a general polynomial time algorithm for near-
optimal reinforcement learning. Journal ofMachine Learning Research, 3(Oct):213-231, 2002.
Qi Cai, Mingyi Hong, Yongxin Chen, and Zhaoran Wang. On the global convergence of imitation
learning: A case for linear quadratic regulator. arXiv preprint arXiv:1901.03674, 2019.
Antonin Chambolle and Thomas Pock. A first-order primal-dual algorithm for convex problems
with applications to imaging. Journal of mathematical imaging and vision, 40(1):120-145, 2011.
Yunmei Chen, Guanghui Lan, and Yuyuan Ouyang. Optimal primal-dual methods for a class of
saddle point problems. SIAM Journal on Optimization, 24(4):1779-1814, 2014.
Bo Dai, Albert Shaw, Lihong Li, Lin Xiao, Niao He, Zhen Liu, Jianshu Chen, and Le Song.
Sbeed: Convergent reinforcement learning with nonlinear function approximation. arXiv preprint
arXiv:1712.10285, 2017.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.
Chelsea Finn, Sergey Levine, and Pieter Abbeel. Guided cost learning: Deep inverse optimal control
via policy optimization. In International Conference on Machine Learning, pp. 49-58, 2016.
Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochas-
tic programming. SIAM Journal on Optimization, 23(4):2341-2368, 2013.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems, pp. 2672-2680, 2014.
Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in Neural
Information Processing Systems, pp. 4565-4573, 2016.
Ronald A Howard. Dynamic programming and markov processes. 1960.
Leslie Pack Kaelbling, Michael L Littman, and Andrew W Moore. Reinforcement learning: A
survey. Journal of artificial intelligence research, 4:237-285, 1996.
Sham M Kakade. A natural policy gradient. In Advances in neural information processing systems,
pp. 1531-1538, 2002.
Michael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time. Ma-
chine learning, 49(2-3):209-232, 2002.
Kee-Eung Kim and Hyun Soo Park. Imitation learning via kernel mean embedding. In Thirty-Second
AAAI Conference on Artificial Intelligence, 2018.
Alex Kuefler, Jeremy Morton, Tim Wheeler, and Mykel Kochenderfer. Imitating driver behavior
with generative adversarial networks. In 2017 IEEE Intelligent Vehicles Symposium (IV), pp.
204-211. IEEE, 2017.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436, 2015.
David A Levin and Yuval Peres. Markov chains and mixing times, volume 107. American Mathe-
matical Soc., 2017.
Sergey Levine and Vladlen Koltun. Continuous inverse optimal control with locally optimal exam-
ples. arXiv preprint arXiv:1206.4617, 2012.
Chun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, and Barnabas Poczos. Mmd gan:
Towards deeper understanding of moment matching network. In Advances in Neural Information
Processing Systems, pp. 2203-2213, 2017.
Lihong Li, Michael L Littman, Thomas J Walsh, and Alexander L Strehl. Knows what it knows: a
framework for self-aware learning. Machine learning, 82(3):399-443, 2011.
10
Published as a conference paper at ICLR 2020
Shuang Li, Shuai Xiao, Shixiang Zhu, Nan Du, Yao Xie, and Le Song. Learning temporal point
processes via reinforcement learning. In Advances in Neural Information Processing Systems, pp.
10781-10791,2018.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. Nature, 518(7540):529, 2015.
Mehryar Mohri and Afshin Rostamizadeh. Rademacher complexity bounds for non-iid processes.
In Advances in Neural Information Processing Systems, pp. 1097-1104, 2009.
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning.
MIT press, 2018.
Alfred Muller. Integral probability metrics and their generating classes of functions. Advances in
Applied Probability, 29(2):429-443, 1997.
Walter Murray and Michael L Overton. A projected lagrangian algorithm for nonlinear minimax
optimization. SIAM Journal on Scientific and Statistical Computing, 1(3):345-370, 1980.
Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic
approximation approach to stochastic programming. SIAM Journal on optimization, 19(4):1574-
1609, 2009.
Andrew Y Ng, Stuart J Russell, et al. Algorithms for inverse reinforcement learning. In Icml,
volume 1, pp. 2, 2000.
Dirk Ormoneit and Saunak Sen. Kernel-based reinforcement learning. Machine learning, 49(2-3):
161-178, 2002.
Mark Pfeiffer, Samarth Shukla, Matteo Turchetta, Cesar Cadena, Andreas Krause, Roland Siegwart,
and Juan Nieto. Reinforced imitation: Sample efficient deep reinforcement learning for mapless
navigation by leveraging prior demonstrations. IEEE Robotics and Automation Letters, 3(4):
4423-4430, 2018.
Matteo Pirotta, Marcello Restelli, and Luca Bascetta. Policy gradient in lipschitz markov decision
processes. Machine Learning, 100(2-3):255-283, 2015.
Dean A Pomerleau. Efficient training of artificial neural networks for autonomous navigation. Neu-
ral Computation, 3(1):88-97, 1991.
Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming. John
Wiley & Sons, 2014.
Hassan Rafique, Mingrui Liu, Qihang Lin, and Tianbao Yang. Non-convex min-max optimization:
Provable algorithms and applications in machine learning. arXiv preprint arXiv:1810.02060,
2018.
Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in
neural information processing systems, pp. 1177-1184, 2008.
Stephane RoSS and Drew Bagnell. Efficient reductions for imitation learning. In Proceedings ofthe
thirteenth international conference on artificial intelligence and statistics, pp. 661-668, 2010.
StePhane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and struc-
tured prediction to no-regret online learning. In Proceedings of the fourteenth international con-
ference on artificial intelligence and statistics, pp. 627-635, 2011.
Stuart J Russell. Learning agents for uncertain environments. In COLT, volume 98, pp. 101-103,
1998.
Tim Salimans and Durk P Kingma. Weight normalization: A simple reparameterization to accelerate
training of deep neural networks. In Advances in Neural Information Processing Systems, pp.
901-909, 2016.
11
Published as a conference paper at ICLR 2020
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International Conference on Machine Learning, pp. 1889-1897, 2015.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Alexander L Strehl and Michael L Littman. A theoretical analysis of model-based interval estima-
tion. In Proceedings of the 22nd international conference on Machine learning, pp. 856-863.
ACM, 2005.
Richard S Sutton, Andrew G Barto, et al. Introduction to reinforcement learning, volume 135. MIT
press Cambridge, 1998.
Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient
methods for reinforcement learning with function approximation. In Advances in neural informa-
tion processing systems, pp. 1057-1063, 2000.
Umar Syed, Michael Bowling, and Robert E Schapire. Apprenticeship learning using linear pro-
gramming. In Proceedings of the 25th international conference on Machine learning, pp. 1032-
1039. ACM, 2008.
Lei Tail, Jingwei Zhang, Ming Liu, and Wolfram Burgard. Socially compliant navigation through
raw depth inputs with generative adversarial imitation learning. In 2018 IEEE International Con-
ference on Robotics and Automation (ICRA), pp. 1111-1117. IEEE, 2018.
SS Vallender. Calculation of the wasserstein distance between probability distributions on the line.
Theory of Probability & Its Applications, 18(4):784-786, 1974.
Vladimir Vapnik. The nature of statistical learning theory. Springer science & business media,
2013.
Michel Willem. Minimax theorems, volume 24. Springer Science & Business Media, 1997.
Bin Yu. Rates of convergence for empirical processes of stationary mixing sequences. The Annals
of Probability, pp. 94-116, 1994.
12
Published as a conference paper at ICLR 2020
A Proofs in Section 2
A.1 Proof of Theorem 1
≤ 2 sup E∏*r(s,a) -	r(st,at)
r∈R	T t=0
We first consider n = 1. For notational simplicity, we denote xt = (st, at) and τ = {xt}tT=-01. Then
the generalization gap is bounded by
dR(π*,b) - inf dR(∏*,∏) = dR(π^,τr) - dR(∏n,b)
π
+ dR(∏n, b) - inf dR(∏n, ∏)
π
+ inf dR(∏n,∏) - inf "r(∏*,∏)
ππ
T-1
+ e.
T-1	T-1
DenoteΦ(τ) = supr∈RE∏*r(s,a)-T P r(st, at) = supr∈RE∏*r(x)-T P r(xt). Weutilize
t=0	t=0
the independent block technique first proposed in Yu (1994) to show the concentration of Φ(τ).
Specifically, we partition τ into 2m blocks of equal size b. We denote two alternating sequences as
τ0 = (X1, X2,…,Xm)	Xi = (X(2i-1)b+1,…,x(2i-1)b+b)
τ1 = (X(I),x21), …，Xm))	Xi = (x2ib+1, ∙∙∙ , x2ib+b),
We now define a new sequence
一. ,- r≤	r≤
丁0 = (X1, x2,…，Xm),	⑹
where X’s are i.i.d blocks of size b, and each block Xi follows the same distribution as Xi.
We define r : X → R as rb(X) = 1b Pk=ι r(xk). Note that r is essentially the average reward on
a block. Accordingly, we denote Rb as the set of all rb ’s induced by r ∈ R.
Before we proceed, we need to introduce a lemma which characterizes the relationship between the
expectations of a bounded measurable function with respect to τ0 and τe0 .
Lemma 2. (Yu, 1994) Suppose h is a measurable function bounded by M > 0 defined over the
blocks Xi , then the following inequality holds:
∣Eτo(h) - Eeo(h)∣≤ (m - 1)Mβ(b),
where Eτ0 denotes the expectation with repect to τ0, and Eτe0 denotes the expectation with respect
to τe0 .
Corollary 3. Applying Lemma 2, we have
Pτ (Φ(τ) >e)	≤2Pτe0(Φ(τe0)-Eτe0[Φ(τe0)]	>e-Eτe0[Φ(τe0)])+2(m-1)β(b).	(7)
Proof. Consider P(Φ(τ) > e),we have
1T
Pτ(Φ(τ) > e) = PT(SupE∏*r(x) - τ}r(xt) > e)
≤ Pτ
SsuPr∈R E∏*r(x) - T P r(xt)	suPr∈R E∏* r(x) - T P r(xt)
2	+	2
∖
= Pτ(Φ(τ0) + Φ(τ1) > 2e)
≤ Pτ0 (Φ(τ0) > e) + Pτ1 (Φ(τ1) > e)
= 2Pτ0 (Φ(τ0) > e)
=2Pτ0(Φ(τ0)-Eτe0[Φ(τe0)] >e-Eτe0[Φ(τe0)]),
13
Published as a conference paper at ICLR 2020
where the first inequality (8) follows from the convexity of supremum.
Applying Lemma 2 and setting h = 1 {(Φ(τe0) - Eτe0 [Φ(τe0)] > - Eτe0 [Φ(τe0)]}, we obtain
Pτ0(Φ(τ0)-Eτe0[Φ(τe0)] > - Eτe0 [Φ(τe0)])
≤ Pτe0((Φ(τe0)-Eτe0[Φ(τe0)] > - Eτe0 [Φ(τe0)]) + 2(m - 1)β(b).
□
Now since τe0 consists of independent blocks, we can apply McDiarmid’s inequality to rb by viewing
Xi’s as i.i.d samples. We rewrite Φ(τe0) as
m
1
Φ(τo) = sup E∏*r∙b(Xi) -	r∙b(Xi).
rb∈Rb	m i=1
Given samples Xi, ∙∙∙ , Xi,…,Xm and Xi, ∙∙∙ , X0,…,Xm , We have
∣Φ(Xl,…，Xi,…，Xm)- Φ(Xi,…，Xi,…，Xm)∣ ≤ m2 卜 b(Xi)∣ ≤ 2mr.
Then by McDiarmid’s inequality, We have
Peo (φ(e0) - EeOgO)]> e -闻国亍O)D ≤ exp (…V )
NoW combining (7) and (9), We obtain
Pτ (Φ(τ) > ) ≤ 2exp
-m(— Eeo [Φ(e0)])2
2B2
+ 2(m - 1)β(b).
(9)
(10)
By the argument of symmetrization, We have
Eτeo [Φ(τeO)] ≤ 2Eτeo ,σ
m
1
SUp〉」QiTb(Xi)
m rb ∈Rb i=i
(11)
where Qi's are i.i.d. Rademacher random variables. Now we relate the Rademacher complexity (11)
to its counterpart taking i.i.d samples. Specifically, We denote x(jt) as the j-th point of the t-th block.
j
Denote τeOj as the collection of the j-th sample from each independent block Xi for i = 1, . . . , m.
Plugging in the definition of rb, we have
2Eτeo,σ
1
SUP
m r∈R
mb
X σ b X r(χjt))
1 b 1 m	(t)
≤ 2Eeo,σ b X - sup Xσtr(Xj))
m
j=i	r∈R t=i
2b	1 m
≤	EEeo,σ	SUPE σtr(xj))
bj=i o m r∈R t=i	j
2b	1 m
=b X EeO ,σ - SUR X σtr(Xj)
j=i	r∈R t=i
1m
=2Ee0,σ -supXσtr(XIt)).
m r∈R t=i
(12)
Setting the right-hand side of (10) to be 2 and substituting (12), we obtain, with probability at least
1 - δ, for all r ∈R,
Φ(τ) ≤ 2Eτeo1,σ
1
—sup
m r∈R
t=i
X Qtr(X(it)) + 2Br
(13)
m
where δ0 = δ - 4(m - 1)β(b).
14
Published as a conference paper at ICLR 2020
Then we denote
Rademacher complexity for Xi’s:
τr∏ ♦ ♦	1 c F	1	i ∙ , i` ^xr ，
Empirical Rademacher complexity for Xi’s:
Empirical Rademacher complexity for Xi’s:
1m
Rm = Eeι,σ [m SuRXσir(χ1t))
Rτe01
m
sup ^X σir(eIt))
m r∈R t=1	1
Rb m = Eσ
1
—sup
m r∈R
ir
t=1
^
m
~
^
Applying Lemma 2 to the indicator function 1{RDm - Rbm > }, we obtain
P(RDm-Rbm>) ≤	P(RDm -	Rb τe01	>)+(m-1)β(2b-1) ≤P(RDm-Rbτe01	>)+(m-1)β(b).
It is straightforward to verify that given x(11),. . . , x(1i) ,. . . , x(1m) and x(11) ,. . . , x01(i)
Rademacher complexity satisfies
, x(1m), the
RI- R e1l ≤ 2mr.
Then by applying McDiarmid’s Inequality again, we obtain
≤ exp
—me2 )
+ (m - 1)β(b).
ThUs With probability at least 1 — δ, we have
Rm — Rm ≤ 2Br jlθg，/2 2mmτ品
(14)
Combining (13) and (14), we have with probability 1 — δ,
Φ(τ) ≤ 2Rbm +6Br
2m
(15)
We apply the DUdley’s entropy integral to boUnd Rm. Specifically, we have
Rb
m
4α	12 ∕*√mBr	，-3-----
≤√m + m /	PlogNRZFO de
≤√√m+1√mr Plog N (R,α, k∙k∞).
(16)
It sUffices to pick ɑ = √m . By combining (6), (15), and (16), we have with probability at least
1 — δ,
dR(π* ,π) — inf dR(π^,π) ≤----1--η= qlθg N (R, 1/√m, k ∙ k∞) + 12 Br J δ δ0 + e,
π	m m	2m
where δ0 = δ — 4(m — 1)β(b) and 2bm = T. SUbstitUting m = T /2b, we have
dR(π ,π) — inf dR(π^,π) ≤ ~T~ + PT //2 b qlog N (R, 1/PT72b, k ∙ k∞) + 12Br { T/b + e.
(17)
Now we instantiate a choice of b and m for expotentially β-mixing seqUences, where the mixing
coefficient β(b) ≤ βo exp(—βιba) for constants βo, βι, α > 0. We set δ0 > δ — 4(m — 1)β(b) = 22.
15
Published as a conference paper at ICLR 2020
By a simple calculation, it is enough to choose b = (log，4；*，) )1∕α. Substituting such a b into (17).
We have with probability at least 1 - δ:
dR(π*,π)—inf dR (π*,π) ≤ O (PTr∕ζ ylog N (R, rɪ, k ∙ k∞)+Br S^T/zɪɪ j+e,
(18)
where Z = (β-1 log β0δT)1.
When n > 1, we concatenate n trajectories to form a sequence of length nT, and such a sequence
is still exponentially β-mixing. Applying the same technique, we partition the whole sequence into
2nm blocks of equal size b. Then with probability at least 1 - δ, we have
dR(∏*,b) - inf dR(∏*,∏) ≤ O ( -Aj= JlogN(R, ʌ/ɪ, k∙k∞) + BrJl0g^) + e.
π	nT∕ζ	nT	nT∕ζ
A.2 Proof of Corollary 1
The reward function can be bounded by
Ir(S,a)| = lθ>g(ψs,ψO)I ≤ kθk2kg(ψs,ψO)k2 ≤ √2BθPg,
where the first inequality comes from Cauchy-Schwartz inequality.
To compute the covering number, we exploit the Lipschitz continuity of r(S, a) with respect to
parameter θ. Specifically, for two different parameters θ and θ0 , we have
kr(S, a) - r0(S,a)k∞ = (θ - θ0)>g(ψs, ψO)∞
(i)
≤ kθ - θ0k2 sup	kg(ψs, ψO)k2
(s,O)∈S×A
≤) √2kθ -θ0k2Pg	suP	/Mk2 + kψak2 (≤) √2Pg kθ -θ0k2 ,
(s,a)∈S×A
where (i) comes from Cauchy-Schwartz inequality, (ii) comes from the Lipschitz continuity of g,
and (iii) comes from the boundedness of ψs and ψa .
Denote Θ = {θ ∈ Rq : kθk2 ≤ Bθ}. By the standard argument of the volume ratio, we have
N(θ,e,∣H∣2) ≤ °+2B )q.
Accordingly, we have
NR,
2(β-1 log(4βoT∕δ))
二k∙k∞ ≤N ∣Θ,
2(β-1log(4βoT∕δ))1
,k∙k2
≤ 1+ + 2√2ρg Bθj
2(β-1 log(4βoT∕δ)) 1
q
. (19)
T
T
T
Plugging (19) into (18), we have
dR(π*,b) - inf dR(π*,π)
π
=O(ppgB⅜Uqlog (ρgBθS! + PgBθSog苧)+ e
hold, with probability at least 1 - δ.
16
Published as a conference paper at ICLR 2020
A.3 Proof of Corollary 2
We investigate the LiPschitz continuity of r with respect to the weight matrices Wι, ∙ ∙ ∙ , Wd.
Specifically, given two different sets of matrices Wι,…，Wd and W0,…，WD, We have
kr(s, a) - r0(s, a)k∞
≤ WD>σ(WD-1σ(...σ(W1[ψa>, ψs>]>)...)) - (WD0 )>σ(WD0 -1σ(...σ(W10[ψa>, ψs>]>)...))2
≤ WD>σ(WD-1σ(...σ(W1[ψa>, ψs>]>)...)) - (WD0 )>σ(WD-1σ(...σ(W1[ψa>, ψs>]>)...))2
+ (WD0 )>σ(WD-1σ(...σ(W1[ψa>, ψs>]>)...)) - (WD0 )>σ(WD0 -1σ(...σ(W10[ψa>,ψs>]>)...))2
≤ kWD - WD0 k2 σ(WD-1σ(...σ(W1[ψa>, ψs>]>)...))2
+ kWD0 k2 σ(WD-1σ(...σ(W1[ψa>, ψs>]>)...)) - σ(WD0 -1σ(...σ(W10[ψa>,ψs>]>)...))2 .
Note that we have
σ(WD-1σ(...σ(W1[ψa>, ψs>]>)...))2 ≤ WD-1σ(...σ(W1[ψa>, ψs>]>)...)2
≤∣WD-ik2K(…σ(Wι[ψ>,ψ>]>)…)∣∣2≤h[ψ>,ψ>]>∣∣2(≤, √2,
where (i) comes from the definition of the ReLU activation, (ii) comes from kWi k2 ≤ 1 and recur-
sion, and (iii) comes from the boundedness of ψs and ψa . Accordingly, we have
kr(s,a) - r0(s,a)k∞ ≤ √2 kWD - WD k2 + kWD ∣∣2 ∣∣σ(WD-1σ(...) - σ(WD-1σ(...)∣∣2
≤ √2 IIWD - WDII2 + ∣∣WD-ισ(...)- WD-ισG∙J∣∣2
(ii)	2
≤ X √2kWi- Wi'k2,
i=1
where (i) comes from the Lipschitz continuity of the ReLU activation, and (ii) comes from the
recursion. We then derive the covering number ofR by the Cartesian product of the matrix covering
of W1 , ..., WD :
N(R, e, ∣∣∙ ∣∣∞) ≤ ∏N (Wi, d√2, k ∙ l∣2 j ≤ (1+	^ J ,	(20)
where the second inequality comes from the standard argument of the volume ratio. Plugging (20)
into (18), we have
dR(π*,b) — inf dR(π*,π)
π
hold, with probability at least 1 — δ .
B Proof of Theorem 2
The proof is arranged as follows. We first prove the bounded ness of Q fucntion and characterize
the Lipschitz properties of the gradients of F with respect to ω and θ, respectively, in Section B.2.
Then we provide some important lemmas in Section B.3. Using these lemmas, we prove Lemma 1
in Section B.4. We prove Theorem 2 in Section B.5. For notational simplicity, We denote(,, •)as
the vector inner product throughout the rest of our analysis.
B.1	BOUNDEDNESS OF Q FUNCTION
Lemma 3. For any ω , we have
where BQ = 2√⅛χ.
∣∣Qπeω∣∣∞≤BQ,
17
Published as a conference paper at ICLR 2020
Proof.
∞
Qπeω (s, a) =	E[reθ(st,at) - Eπeω reθ | s0 = s, a0 = a,πeω]
(s, a)d(s, a)
∞
≤ X2kreθk∞ ρ0(Pπω)t -ρπeωTV
t=0
≤ 2√2κρg XX χυt = 2√⅛,
t=0	1 - υ
where the first inequality comes from the definition of Total Variance distance of probability mea-
SUres and the second inequality results from (i) of Assumption 5.	□
B.2	Lipschitz properties of the gradients
Lemma 4. Suppose that Assumption 1, 3 and 5 hold. For any ω, ω0 , θ and θ0, we have
kVωF(ω,θ) - VωF(ω0,θ)k2 ≤ Lω kω - ω0k2 ,
∣∣VθF(ω,θ)-VθF(ω,θ0)k2 ≤ μ kθ -θ0k2 ,
where Lω = 2√2(Sπ+2BωLP)KPgX + BωLq.
Proof. By the Policy Gradient Theorem (Sutton et al., 2000), we have
VωF(ω, θ) = Eπeω V log(πeω (a | s))Qπeω (s, a).
Therefore,
kVωF (ω, θ) - VωF (ω0, θ)k2
= Eπeω V log(πeω (a | s))Qπeω (s, a) - Eπeω0 V log(πeω0 (a | s))Qπeω0 (s, a)
≤ Eπeω V log(πeω (a | s))Qπeω (s, a) - Eπeω V log(πeω0 (a | s))Qeπω0 (s, a)
+ Eπeω V log(πeω0 (a | s))Qπeω0 (s, a) - Eπeω0 V log(πeω0 (a | s))Qeπω0 (s, a)
≤ (SπeBQ + BωLQ) kω - ω0k2 + 2BωBQ ρπeω - ρπeω0 TV
≤ (Sπe BQ + BωLQ + 2BωBQLρ) kω - ω0k2 ,	(21)
where the second and the third inequality results from (ii) of Assumption 5. Plugging BQ =
2√2κUgχ into (21) yields the desired result.
Similarly, we have
∣∣VθF(ω, θ) - VθF(ω, θ0)∣∣2 ≤ ∣∣-μ(θ -叽 ≤ μ ∣∣θ -叫.
□
We then characterize the Lipschitz continuity of VθF with respect to ω.
Lemma 5. Suppose Assumptions 1, 3 and 5 hold. For any ω, ω0 and θ, we have
kVθF(ω, θ) - VθF(ω0, θ)k2 ≤ Sωkω - ω0k2,
where Sω
2√2ξ^KPg χBω
1—υ
18
Published as a conference paper at ICLR 2020
Proof. We have
Vθ F (ω,θ) = -μθ - Vθ hEρ∏ω [θ>g(ψst, ψat )] - Eρ* [θ>g(ψst, ψaJ]]
=-μθ - [Eρ∏ω [g(ψst ,ψat)] - Eρ* [g(ψst ,ψat )]].
Therefore, we have
Vθ F (ω, θ) - VθF (ω0, θ)
= Eρπeω [g(ψst, ψat)] - Eρπeω0 [g (ψst, ψat)]
≤ √q max IEρπω g(ψst, ψat )j - EPn 0 g(ψst, ψat ) j I ∙	(22)
1≤j≤q	ω	ω0
Suppose j* = argmaxι≤j≤q ∣Eρ∏ωg(ψst,ψajj — Eρ∏ω0g(ψs*,ψa,)j〔，by Mean Value Theorem,
there exists vector ωe, which is some interpolation between vectors ω and ω0, such that
Eρπeω g(ψst, ψat)j - Eρπeω0 g(ψst, ψat)j = hVωEρπeωe g(ψst, ψat)j,ω - ω0i.	(23)
By Policy Gradient Theorem, we have
Hvω Eρ5eω g。, ,ψa, ) j (=胆PneV log eω (a | s)Qgeω (S,a)(
___ 一. , . . . . . ~ , ..
≤ SUp	|Vlog∏ω(a | s)∣∣Qeeω(s,a)∣
(s,a)∈S×A
≤ BQBω ,	(24)
where Q∏ω (s, a) = P∞=0 E[g(st, at)j* - E∏ωgj* | so = s, aο = a,∏]. Combining (22), (23), (24)
and using Cauchy-Schwartz Inequality, We prove the lemma.	□
B.3 Some Important Lemmas for Proving Lemma 1
We denote
ξ(t) = VθF(ω⑴,θ(t)) - -1 X Vθfj(ω⑴”)
qθ j∈M(θt)
and g = VωF(ω⑶”t+1)) - -1 X Vωfj(ω⑴”t+1))
ω j∈M(ωt)
as the i.i.d. noise of the stochastic gradient, respectively. Throughout the rest of the analysis, the
expectation E is taken with respect to all the noise in each iteration of the alternating mini-batch
stochastic gradient descent algorithm. The next lemma characterizes the progress at the (t + 1)-th
iteration. For notational simplicity, we define vector function
G(π) = EPπg(ψs, ψa)	(25)
Lemma 6. At the (t + 1)-th iteration, we have
EF (ω(t+1), θ(t+1)) - EF (ω(t), θ(t))
≤ (Lω - η1 )Ekω(HI)- ω㈤k2 + Sω ∙ E ∣∣ω(t) - ω(t-1)[
+ (套+SS+μ)E ∣∣ …-叫∣2+(⅛∙+2)E ∣∣θ(t)-θ(Tu2
+一周2+# ∣∣ξ(tT∣2∙
Proof. We have
EF (ω(t+1), θ(t+1)) - EF (ω(t), θ(t))
=EF(ω(t+1),θ(t+1)) - EF (ω(t), θ(t+1)) +EF(ω(t),θ(t+1)) - EF (ω(t), θ(t)).
19
Published as a conference paper at ICLR 2020
By the mean value theorem, we have
(VωF(ω(t),θ(t+1)),ω(t+1) - ω⑴)=F(ω(t+1), θ(t+1)) - F(ω⑴”t+1)),
where ωe (t) is some interpolation between ω(t+1) and ω(t). Then we have
EF (ω(t+1), θ(t+1)) - EF (ω(t), θ(t+1))
= EhVωF(ωe (t), θ(t+1)) - Vω F (ω(t), θ(t+1)), ω(t+1) - ω(t)i
+ EhVωF(ω(t), θ(t+1)), ω(t+1) - ω(t)i.	(26)
By Cauchy- Swartz inequality, we have
EhVω F (ωe (t), θ(t+1)) - VωF (ω(t), θ(t+1)), ω(t+1) - ω(t)i
≤ EkVωF(ωe (t), θ(t+1)) - VωF (ω(t), θ(t+1))k2kω(t+1) - ω(t)k2
≤ LωEkω(t+1) -ω(t)k22,
where the last inequality comes from Lemma 5. Morever, (4) implies
ω(t+1) -ω(t) = -ηω(VωF (ω(t), θ(t+1)) + ξω(t)).
Therefore, we have
EhVωF(ω(t),θ(t+1)),ω(t+1) - ω⑴〉=--1 E∣∣ω(HI)- ω㈤∣∣2 — Ehξωt), ω(t+1) - ω⑴)
ηω
= - Ehξω(t), -ηω(VωF (ω(t), θ(t+1)) + ξω(t))i
--1 Ekω(t+1) - ω(t)k2
ηω	2
=-η1 Ekω(t+1) - ω(t)k2 + ηωE∣H]∣2∙
Thus, we have
EF (ω(t+1) ,θ(HI)) - EF (ω(t),θ(t+1)) ≤ (Lω -:)E∣∣ω-1)- ω㈤∣∣2 + 加四|口]|2 . (27)
By (3), the increment of F(ω, θ) takes the form
F (ω(t), θ(t+1)) - F (ω(t), θ(t))
=D‰t)) - G(∏*),θ(t+1) - θ(t)E - 2(kθ(t+1)k2 -kθ(t)k2)
≤〈。(港⑷)-G(∏*)- μθ(t),θ(t+1) - θ(t)E .	(28)
For notational simplicity, we define
(t+1) = θ(t+1) - θ(t) +ηθ VθF (ω(t), θ(t)) +ξθ(t)	.	(29)
Note that we have
VθF(ω⑴，θ㈤)=G(∏ω(t)) - G(∏*) - 〃。"	(30)
Plugging (29) and (30) into (28), we obtain
F(ω(t), θ(t+1)) - F(ω(t),θ(t)) ≤( θ(t+1) - ∖ - "1) - ξ(t), θ(t+1)-。㈤).
Since θ belongs to the convex set {θ | kθk2 ≤ κ}, we have
h(t), θ(t+1) - θ(t)i ≥0.
20
Published as a conference paper at ICLR 2020
Then we obtain
F (ω(t), θ(t+1)) - F (ω(t), θ(t))
≤ -1 hθ(t+1) - θ⑴ + e(t) - e(t+1),θ(t+1) - θ(t)i - hξ(t),θ(t+1) - θ(t)i
ηθ	θ
=-1 he(t) - e(t+1),θ(t+1) - θ(t)i + -1 ∣∣θ(t+1) - θ(t)∣∣2 - hξ(t),θ(t+1) - θ(t)i.
ηθ	ηθ	(31)
By the definition of (t) in (29), we have
e(t+1) = θ(t+1) - (θ㈤+ η (VθF(ω⑴,θ㈤)+ ξ(t)))	(32)
and
e(t) = θ(t) - θ(t-1) +ηθ VθF (ω(t-1), θ(t-1)) + ξθ(t-1)	.	(33)
Subtracting (33) from (32),we obtain
e(t) -	e(t+1) =	(θ(t)	- θ(t+1)) -	(θ(t-1)	- θ(t)) -ηθ	VθF (ω(t-1),	θ(t-1)) - VθF (ω(t),	θ(t))
- ηθ (ξθ(t-1) - ξθ(t) ).	(34)
Plugging (34) into the first term on the right hand side of (31), we obtain
-1 he(t) - e(t+1),θ(t+1) - θ㈤)
ηθ
-1 hθ(t) - θ(I),θ(HI) - θ⑴i + hVθF(ω(t),θ(t)) - VF(ω(t-1), θ(t-1)), θ(t+1) - θ(t)i
∣θ} 、
(A)
^{z
(B)
—-∣∣θ(t+1) - θ(t)∣∣2 + hξ(t) - ξ(tτ),θ(t+1)- θ(t)i.
For term (A), we apply the Cauchy-Schwarz inequality to obtain an upper bound as follows.
1-hθ⑴一θ(t-ι),θ(t+1) - θ⑴i ≤ L Mt)- θ(t-1)∣∣2 ∙ ∣∣θ(t+1) - θ(t)∣∣2
≤ 2⅛ Mt)-θ(tT∣2+2⅛ ∣∣ …-叫∣2∙
To derive the upper bound of (B), we apply Lemma 5 to obtain
hVθF (ω(t), θ(t)) - VθF (ω(t-1), θ(t-1)), θ(t+1) - θ(t)i
= hVθF (ω(t), θ(t)) - VθF (ω(t-1), θ(t)), θ(t+1) - θ(t)i
+ hVθF (ω(t-1), θ(t)) - VθF (ω(t-1), θ(t-1)), θ(t+1) - θ(t)i
≤ Sω	∣∣ω⑴一ω(t-1)∣∣ ∙	∣∣θ(t+1) -	θ(t)∣∣	+ μ ∙ ∣∣θ(t)	-	θ(t-1)∣∣	∙	∣∣θ(t+1)	- θ(t)∣∣
≤ Sω ∣∣ω(t)-ω(tT∣2 + Sω ∙∣∣ …—叫∣2
+2 ∙∣∣θ(t)-θ(T)∣∣2+2 ∙∣∣θ(t+1)-。啡.
(35)
(36)
(37)
Plugging (36) and (37) into (35), we obtain
1- he㈤-e(t+1),θ(t+1) - θ⑴i ≤ (-ɪ + μ + Sω) Mt+1) - θ⑴ ∣∣2
+ Sω ∣∣ω⑴-ω(I) ∣∣2 + hξ(t) - ξ(t-1),θ(t+1) - θ(t)i.
(38)
21
Published as a conference paper at ICLR 2020
Further plugging (38) into (31), we obtain
F (ω(t), θ(t+1)) - F (ω(t), θ(t))
≤ + S2ω
+Sω Mt)
≤	+ Sω
+Sω II
2
2
2
2
(39)
Finally, taking expectation of (39) with respect to the noise and together with (27) , we prove the
final result.
-ω(t)k2 + S2ωE II
IIIθ(t+1) - θ(t) III2
- θ(t-1)III2 +ηωE
ω(t) - ω(t-1) III2
IH1I2+』E 'Ti:
□
We then characterize the update of ω.
Lemma 7. The update of ω satisfies
E ω(t+1) - ω(t) - (ω(t) - ω(t-1)), ω(t+1) - ω(t)
≤ - ηω ∙ E ((θ(t+2) - θ(t+1)) — (θ(t+1) — θ(t)) — (e(t+2) — e(t+1)), θ(t+1)-。㈤〉
-等∙ E^(t+1) - θ(t)12 + ηω ω(LLω + I) ∙ E^(t+1) - ω(t)j2 + ηωLω ∙ Jω(t) - ω(tτ)[
+(∣+宿 )E 同2+ηωE 忖-唯.
22
Published as a conference paper at ICLR 2020
Proof. By the update policy, we have
E ω(t+1) - ω(t) - (ω(t) - ω(t-1)), ω(t+1) - ω(t)
=-ηωE (VωF(ω(t),θ(t+1)) + 寸t - VωF(ωd),θ⑴)-ξω j),s(t+1)- ω⑴E
=-ηωE DVωF(ω⑴,θ(HI))- VωF(ω㈤,θ⑴),ω(HI)- ω⑴E
-	ηωE DVωF (ω(t), θ(t)) - VωF (ω(t-1), θ(t)), ω(t+1) -ω(t)E
-	ηωE Dξω(t), -ηω(VωF (ω(t), θ(t+1)) + ξω(t))E + ηωE Dξω(t-1), ω(t+1) - ω(t)E
≤ -ηωE DVωF (ω(t), θ(t+1)) - VωF (ω(t), θ(t)), ω(t+1) -ω(t)E
、-----------------------------{-----------------------}
(C)
-ηωE DVωF (ω(t), θ(t)) - VωF (ω(t-1), θ(t)), ω(t+1) - ω(t)E
、---------------------------V--------------------------}
(D)
+ηω E∣∣ξωt)∣∣2+ηω E∣∣ξωt-1)∣∣2+ηω E∣∣ω(t+1) - ω(t)∣∣2.	(40)
Then for (C), by the definition of objective function, we have
(C)=-ηωE VωXG(πeω(t))j(θ(t+1) - θ(t))j, ω(t+1) - ω(t)
=-ηω E XD Vω G(eω(t))j -Vω G(∏e(t) )j + Vω G(∏e(t) )j ),ω(t+1) - 31(t∖ ∙
(θ(t+1) - θ(t))j
-	ηωE X(θ(t+1) - θ(t))j DVωG(πeω(t))j -VωG(πeωe(t))j),ω(t+1) - ω(t)E
jj
-	ηωE X(θ(t+1) - θ(t))j DVωG(πeωe (t))j), ω(t+1) - ω(t)E,
jj
(41)
where for every j, πeωe(t) is some interpolation between vectors πeω(t) and πeω(t+1) such that
VωG(πeωe(t))j, ω(t+1) - ω(t) =G(πeω(t+1))j -G(πeω(t))j.	(42)
For the first term in the right hand side of (41), by Lemma 4, we have
-ηωE X(θ(t+1) - θ(t))j DVωG(πeω(t))j -VωG(πeωe(t))j),ω(t+1) - ω(t)E
jj
≤2ηωLωE∣∣∣ω(t+1) - ω(t)∣∣∣2 .	(43)
For the second term in the right hand side of (41), by (42) we have
-ηωEX(θ(t+1) -θ(t))jDVωG(πeωe (t))j,ω(t+1) - ω(t)E
jj
=-ηωEDG(πeω(t+1)) - G(πeω(t)), θ(t+1) - θ(t)E.
23
Published as a conference paper at ICLR 2020
Then by the defintion of objective function, We have
-ηωEX(即+1) -θ㈤)j<VωG(∏ω(t))j,“(t+1) -ω⑴)
j
-ηωEDɪ (…-…-…)-ξ尸)+ 〃…
-ɪ (θ(t+1)-。㈤-e(t+1)) + $)- μθ(t∖θ(t+D - θ(
=-ηωE ((θ(t+2) - θ(t+1)) - (θ(t+1) - θ(t)) - (e(t+2) - e(t+1)),θ(t+1) - θ(t))
+ ηωE<ξ('+1) -ξ,,θ(t+1) -θg -μηωE∣∣θ(t+1) -θ([∣2
≤- ηω E D(θ(t+2) - θ(t+1)) - (θ(t+1) - θ⑴)一(e(t+2) - e(t+1)), θ^+1)- θ㈤)
-X e吠I)-θ啡+ημ EIw2∙
For (D), applying Cauchy-Schwartz inequality we have
-ηωEhVωF(ω㈤,θ⑴)-VωF(ω(T), θ⑴),ω(HI)- 3*
≤ηωLωE∣∣ω(t+1)-ω(t)∣∣2∣∣ω(t)-ω(tτ)∣∣2
≤ ηωLω E∣∣ω(t+1)- 3(t)∣∣2+ηωLω E∣∣3(t) - 3(t-ι)∣∣2
(44)
(45)
Finally, combining (40)-(45), we prove Lemma 7.	□
For notational simplicity, we define
j(t+2) = (θ(t+2) - θ(t+1)) - (θ(t+1) - θ(t))
Lemma 8. The first term on the right hand side of Lemma (7) satisfies
-ηωE ((θ(t+2) - θ(t+1)) - (θ(t+1) - θ(t)) - (e(t+2) - e(t+1)),θ(t+1) - θ(t))
≤-崇 E∣∣θ(t+2) - θ(t+1)
2+("+⅛)EMt+1)一叫2
+ 3ηω ηθ S2
+ -2—
- E ∣∣3(t+1) - 3([1 + 2ηωηθ
(啡尸 ∣∣2+ENt) ∣∣)
(46)
(47)
(48)
Proof. Plugging (46) into (47), we obtain
-也E <(θ(t+2) - θ(t+1)) - (θ(t+1) - θ(t)) - (e(t+2) - e(t+1)),θ(t+1) - θ(t))
ηωE <S(t+2)-(
ηωE Dδ(t+2) -(
ηθ
+ ηωE D(e(t+2) .
ηθ
(t+2) - e(t+1)) j(t+2) - (θ(t+2) - θ(t+1))y
(t+2) - e(t+1)) 6(t+2)))- ηωE DJ(t+2) θ(t+2) - θ(t+I)E
-e(t+1)),θ(t+2) - θ(t+1)) .
(49)
e
e
By applying the equality
hu,vi = 2(Huk2 + Ilvk2 -ku -vk2)
24
Published as a conference paper at ICLR 2020
to the first two terms on the right hand side of (49), we obtain
-ηωe D(θ(t+2) - θ(t+1)) - (θ(t+1) - θ(t)) - (e(t+2) - e(t+1)), θ(t+1) - θ(t)E
=蚩E (∣∣δ(t+2)-(e(t+2)-e(F2 + MT2-")-…[)
-⅛E (llδ(tτ∣2+llθ(t+2)-…∣∣2-∣∣ …-叫2)
+ ηωEhe(t+2) - e(t+1),θ(t+2) - θ(t+1)i.
ηθ
(50)
Recall that
θ(t+2) = Πκ (θ(t+1) + ηθ (VθF(ω(t+1)θ(t+1)) + ξ(t+1)))
and
θ(t+1) = Πκ
+ ηθ (VθF(ω㈤，θ⑴)+ ξ(0))
Following from the convexity of {θ∣∣∣θ∣∣2 ≤ κ}, We have
h(t+2), θ(t+2) - θ(t+1)i ≤ 0 and h(t+1), θ(t+2) - θ(t+1)i ≥ 0.
Thus, the last term on the right side of (50) is negative. By rearranging the terms in (50), we obtain
ηθ
ηω
≤ — •
一2ηθ
D(θ(t+2) - θ(t+1)) - (θ(t+1) - θ(t)) - (e(t+2) - e(t+1)), θ(t+1) - θ(t) E
.E 忖+2) -(e(t+2) -e(HI))H
-ηω-E llθ(t+2) - θ(t+1)
2ηθ	ll
+ -ηωE Hθ(t+1) — θ⑴
2ηθ	H
2
(51)
-也E
2
2
2
2
2
By definition of δ(t+2) in (46), we have
δ(t+2) - ((t+2) - (t+1)) = (θ(t+2) - θ(t+1) - (t+2)) - (θ(t+1) - θ(t) - (t+1))
=ηθ -—氏㈤)-μ(θ(t+1) - θ(t)) + ξ(t+1)
Using the Cauchy-Schwarz inequality, we obtain

2
ηω- ∙ E 俨+2)-"2)-€(HI))H
2
≤ 3ηωηθ- ∙(E kG(eω(t+i)) - G(eω(t))k2 + E ∣∣μ(θ(t+1) - θ㈤)∣∣2 + E ∣∣ξ尸-ξ(t)
2
2
≤ 3ηωηS2 ∙ E lω(t+1) - ω(t) h2 + 3μ2ηωη ∙ E Mt+1) - θ(t) i∣2 + 竽∙ E 忖包
Plugging (52) into (51) yields (48), which concludes the proof of Lemma 8.
Lemma 9. For the update of ω, we have
1 ∙ E∣∣ω(t+1) - ω(t)∣∣2 - 1 ∙ E∣∣ω(t) - ω(t-1)∣∣2
≤
-ξθ(t)lll22.
(52)
□
ηω (5Lω + 1)	3ηω"θS^
2	+
- ⅛ ∙E llθ(t+2)-…l∣2+(3
+(ηω+ηω )E Wll
+η2ω ∙ E∣∣ξωt-1)
E lω(t+1)
2
2
2
25
Published as a conference paper at ICLR 2020
Proof. Combing Lemma 7 and Lemma 8, we obtain
Ehω(t+1) - ω(t) - (ω(t) - ω(t-1)), ω(t+1) - ω(t)i
≤-2ηθ E Mt+2)-θ(t+唯+ ( 2 ηω ηθ μ2 + ⅛ ) EIL 叫2
+ 3ηωηSS E ∣∣ω(t+1) - ω㈤ ∣∣2 -等E ∣∣θ(t+1) - θ㈤ ∣∣2
+ 2ηω Lω E∣∣ω(t+1) - ω(t)∣∣2 + 狐(L； + I) E ∣∣ω(t+1) - ω㈤ ∣∣j + ηω2Lω E∣∣ω㈤-ω(t-1)∣∣2
+(*+ηω )E ∣∣ξωt) ∣∣2+ηS E k” ∣∣2+2 η ηθ YE ∣∣ξ 尸 ∣∣2+E ∣∣ξ(t) b	(53)
Note that we have
Ehω(t+1) - ω(t) - (ω(t) - ω(t-1)), ω(t+1) - ω(t)i
=1 E∣∣ω(t+1) - ω㈤∣∣2 - 1 E∣∣ω⑴-ω(t-1)∣∣2 + ∣E∣∣(ω(t+1) - ω㈤)-(ω㈤-ω(t-1))[
≥ 1 E ∣∣ω(t+1) - ω(t) ∣∣2 - 1 E a - ω( j) ∣∣2.	(54)
Combining (53) and (54), We conclude the proof of Lemma 9.	□
Lemma 10. Suppose Assumption 1, 3 and 5 hold. F (ω(t), θ(t)) is lower bounded throughout all
iterations.
Proof. By definition of F in (2), We have
F(ω(t),θ(t)) ≥ E∏	eθ(t) (s,a) - E∏*eθ(t) (ψs,ψa) - μ kθk2 - λBH
ω2
≥ - (2√2pgκ + 2κ2 + λBH) .	(55)
□
B.4 Proof of Lemma 1
Recall that We construct a potential function that dacays monotonically along the solution path,
Which takes the form
E(t+1)= EF (ω(t+1),θ(t+1)) + S ∙ ( 1 + 2；3L E ∣∣ω(t+1) - ω⑴ ∣∣j
+
32
+ 2 ηω ηθ μ
E∣∣θ(t+2) -θ(t+1)∣∣2 + μηωE∣∣θ(t+1) - θ(t)∣∣2)
(56)
for some constant S > 0. We define five constants k1, k2, k3, k4, k5 as
nω (7Lω + 1) + 3ηω ηθSS )
k1
1
------S ∙
2ηω
2 ηω ηθ μ2
Sω + 2μ )
Here, We restate Lemma 1 and then prove it.
Lemma 1. We choose step sizes ηθ, ηω satisfying
1
ηω ≤ min
Sω (8Lω +2) , 2Lω∫,
ηθ ≤ min{1⅛,
7Lω + 1	1
150Sω , 100(2μ + Sω)
26
Published as a conference paper at ICLR 2020
and meanwhile ηω/η ≤ μ∕(30Lω + 5), where Lω = √2κρgS∏ |A| + λSH. Then We have
E(t+1) -E(t)	≤ -	k1E	ω(t+1) - ω(t)2 - k2E	ω(t)	- ω(t-1)2 -	k3E	θ(t+2) -	θ(t+1)
2
2
-	k4E θ(t+1) - θ(t)2 - k5E θ(t) - θ(t-1)2
+ ηω E 依 ∣∣2 + 2 E iξ(t-1t	2
+S ∙ (* E IHt) ∣∣2 + ηω E IHtT) ∣∣2 + 丝∙ (E |卜”∣∣2+e ∣k("2)).	(57)
Moreover, we have k1 , k2 , k3, k4, k5 > 0 for
8
S = -ɪ-- ------.
ηω (58Lω + 9)
Proof. For notational simplicity, we define
K (t+1)
1 + 2尸3 EL(HI)- ω⑴[
+ (蒲-华 + 3加72)∙ E ∣θ(t+2) - θ(t+i) ∣l2 + 皆E ∣θ(t+i) - θ(t) ∣l2
(58)
By rearranging the inequality in Lemma (9), we obtain
K(t+1) - K(t) ≤ (ηω(7Ls + I) + 3ηωηθS2)E I∣ω(t+1)
2
2
- ω(t)
ηωLωE ]ω(t) - ω(t-i) l∣2 - (牛 - 3μ⅛)e 收+2) -。912
-	μηωE∣lθ(t+1) - θ(t)∣l2 - μηωE∣θ⑴- θ(I)∣∣2
+	定+2μ )E e)[+ηωE 归T) [+” ∙ (E k 尸 H：+E w [)
By definition of P(t) in (56), we have
E(t) =F(ω(t),θ(t))+S∙K(t)
(59)
for some constant s > 0. Combining (59) and Lemma 6, since & < ɪ^, we obtain
E(t+1) -E ⑴ ≤ - (ɪ - s ∙ (ηω I' + 1) + 3ηω ： SS )) ,(t+1) - ω⑴ ∣2

- ?)]ω(t) - ω(t-D]2 - s ∙(牛 - 3〃”sηθ) »(t+2) - θ(t+i)∣∣2
∙ (E ∣∣ξθ(t+1) ∣∣
+E∣∣∣ξθ(t)∣∣∣22).
(60)
2
2
Since η < i50μ, we have k3 > 0. Now we choose a proper constant S such that k1,k2,k4, k5 are
positive. Note that they are positive if and only if
1∕(2ηω) > S ∙((ηω(7Lω + 1)/2 +	)),	(61)
s∙ηωLω∕2 >Sω∕2,	(62)
S ∙(μηω∕8) > 1∕(2ηθ) + (2μ + Sω)/2,	(63)
S ∙ (μηω)∕8 > 1∕(2ηθ) + μ∕2.	(64)
27
Published as a conference paper at ICLR 2020
Rearranging the terms in (61), (62), (63), (64) , we obtain
Sω / /	1/(2加)
----< s < -------------------
ηω Lω	ηω (7Lω + 1 + 3ηθ sω )/2
1/(23)+ (Sω + 2μ)∕2	1∕(2ηω)
------------------< s < -------------------
μηω/8	ηω (7Lω + 1 + 3ηθ sω )/2
(65)
(66)
Since	7Lω +1	1	 ηθ ≤ 150Sω an ηθ < 100(2μ + Sω),
by rearranging the terms in (65) and (66) and taking the leading terms, we obtain
	η ,< -L— and 也 < _比_ ηω Sω (8Lω + 2)	ηθ ‹ 30Lω + 5
Therefore, We have	ηω ,	μ —<	^^rτ, ηω < ηω and ηθ < ηθ, ηθ	30Lω + 5
Where	- min] Sω /+a) , +} , , =	. J ɪ 7Lω + 1	1	1 ηθ = min [150μ, 150Sω , 100(2μ + 跖 ∫ .
□
B.5 Proof of Theorem 2
Let k = 1/min{kι, k4}, and φ = max{1, \/危,1加2}. Then We have
N
NJN ≤ X φ ∙ E(∣∣
t=1
N
≤ φk X(E(t)
t=1
2
2
+ S ∙ (ηω/2 ∙ E
+ θ(t+1) - θ(t) 2)
ω(t+1) - ω(t) 2
≤ φk (E(1) - E(N)
ξθ(t-1)

2
+ 3max{2μ, "}E W I))
(67)
Now set
ν = max 2 max ηω + sηω2 +
sηω sηω
1	3ηωηθ
,3maxt2μ,—
2μ , 2
and divide both sides of (67) by N , we have
JN ≤
kφ(E(1) - E(N))
+ kφν MG + M
qω	qθ
(68)
N
By definition of E in (56) and Lemma 10, We have
E(N) ≥ F(ω(N),θ(N)) ≥ -(2√2ρgκ + μκ2 + λBH) > -∞.
NoW for any given > 0, We take
4kφνMθ	4kφνMω	, nT	- 2E⑴ + 4√2ρ0K + μκ2 + 2XBh
qθ = --------, qω = ------and N = kφ ------------------------------
and obtain
JN ≤ .
28
Published as a conference paper at ICLR 2020
C Empirical Maximizer Case
Notice that the object function (2) is quadratic in θ, and thus we are able to get the empirical maxi-
mizer by simple calculation instead of performing stochastic gradient ascent at each iteration. This
allows Us to avoid using large batches while achieving the same sample complexity O( *).
Given a fixed ω(t), by definition of G in (25), We can get the optimal θ*(ω(t)) in population form:
θ*(ω(t)) = 1[G(∏ω(t)) - G(∏*)].	(69)
μ
At the (t + 1)-th iteration, we can achieve the closed form of the empirical maximizer by sampling
one trajectory randomly and then apply stochastic gradient descent to the outer minimization. More
specifically, the updating rule is defined as
e(t+1)(ω(t)) = 1[G(∏ω(t)) - G(n*)],	(70)
μ
ω(t+1) = ω⑴一ηωS e(t+1)),	(71)
where G is the empirical version of G obtained by sampling, and Vω f is the stochastic approxima-
tion of Vω F at the (t + 1)-the iteration.
We define the stationary point as follow.
Definition 4. We call ω* an stationary point if VωF(ω*,θ*(ω*)) =0.
Before we proceed with the convergence analysis, we impose the following assumptions on the
problem.
Assumption 6. There is some constant MG > 0 s.t. for any ω, θ andπ, the following two conditions
hold.
. _________~ , " 一 一 , " — 一，、 一 ..,,
Unbiased : EVω fet (ω, θe) = VωF(ω, θe), EGe(π) = EG(π).
Gradient bounded : E Vωfet (ω, θ)	≤ MG.
Assumption 6 requires the stochastic gradient to be unbiased with bounded second moment.
12ρ2
Corollary 4. Under Assumption 1, 3 and 5, there exists BF = -μg + XBh such that for any ω, we
have |F(ω,θ*(ω))∣ < Bf.
Proof. By Equation (69),we have ∣∣θ*(ω)k2 ≤ 2√ρg. Plugging this into F(ω,θ* (ω)), we have for
any ω,
lFdθ* (M)I = lE∏ω hθ (ω),g(ψst ,ψat )i - E∏* hθ(ω),g(ψst ,ψat )i
—	λH(∏ω(t))- 2 kθ*(ω)∣2 I
≤	2 kθ*(ω)∣∣2 ∙ maX kg(ψs, ψa)∣2 + XH (∏ω(t)) + 2 kθ*(ω)k2
12ρ2
≤	—-g + XMh .
μ
□
For notational simplicity, We define ξω(t) = VθF(ω(t), θe(t+1)) - Vθfet(ω(t), θe(t+1)) as the i.i.d
stochastic noise and σ%t = θ*(ω(t)) 一 θ(t) as the i.i.d error of empirical maximizer. We denote E as
the expectation over ξω(t)(t ≥ 1) and σθ(t) (t ≥ 1). We measure the sub-stationarity of the algotithm
at the iteration N by
IN = IminV E∣Vω F (ω㈤,θ*(ω㈤))[.
Then we state the global convergence of the above mentioned optimization method.
29
Published as a conference paper at ICLR 2020
Theorem 3. Suppose Assumptions 1, 3, 5, 6 hold. Given any > 0, we take ηω
√2(2Lω+Sω/μ)MG ,then We need at most
N = O ((Pg/μ + λBH) (Lω + Sω/MMG !
iterations to have IN < .
Proof. By employing the first inequality in Lemma 4, We have
F(ω(t+1),θ*(ω⑴))-F(ω㈤,θ*(ω㈤))-hVωF(ω⑴,θ*(ω(t))), ω(t+1) - ω⑴)
≤L2ω ∣∣ω(t+1) - ω(t)∣∣2 .	(72)
Note that
EhVωF(ω⑴,θ*(ω㈤)),ω(t+1) - ω(t)i
=EhVωF(ω㈤,θ*(ω㈤)),-加(VωF(ω㈤,刖)+
=EhVωF(ω㈤,θ*(ω㈤)),-加(VωF(ω㈤,θ*(ω㈤)-σθ) + 苴)》
=EhVωF(ω(t),θ*(ω⑴)),-ηω(VωF(ω⑴,θ*(ω㈤))+ 4八)
(=) EhVωF(ω㈤,θ*(ω㈤)),-%VωF(ω㈤,θ*(ω㈤)))
=-ηωE∣∣Vω F (ω⑴,θ*(ω⑴))∣∣2,	(73)
Where (i) comes from the unbiased property of θe(t) and the fact that VωF(ω, θ) is linear in θ, and
(ii) comes from the unbiased property of fj(ω, θ). NoW taking the expectation on both sides of (72)
and plugging (73) in, We obtain
EF (ω(t+1),θ*(ω(t))) - F (ω⑴,θ*(ω⑴))+ 加 E∣∣Vω F (ω㈤,,θ*(ω㈤)∣∣j ≤ Lω 宿 Mg. (74)
Dividing both sides by ηω and rearranging the terms in (74), We get
E ∣∣ V“F”), θ*(ω(t)))∣∣2 ≤EF…"-F…θ*” + L&MG
2	ηω	2
EF(ω⑴,θ*(ω⑶))-F(ω(t+1), θ*(ω(t+1)))	Lω	M
≤------------------η------------------+ ɪ ηω MG
EF(ω(t+D,θ*(ω(t+1))) - F(ω(t+1), θ*(ω(t)))
+	.	(75)
ηω
NoW consider F(ω(t+1), θ*(ω(t+1))) — F(ω(t+1), θ* (ω(t))), We have
F(ω(t+1),θ*(ω(t+1))) - F(ω(t+1),θ*(ω(t)))
=hG(∏ω(t+i)) - G(∏*),θ*(ω(t+1))i-hG(∏ω(t+i)) - G(π*), θ*(ω(t))i
-μ(I∣2+1))∣∣2-∣∣θ3)∣∣2)
=hμθ*(ω(t+1)),θ*(ω(t+1)) - θ*(ω㈤))
-μhθ*(ω(t+1)) + θ*(ω(t)),θ*(ω(t+1)) - θ*(ω(t)))
=μ ∣∣θ*(ωU)) -θ*(ω⑴∣∣2
1	2
=∣ 一(G(πω(t+1) ) - G(πω(t) ))
2	μ	2
≤ Shω(t+1)-叫2∙	(76)
30
Published as a conference paper at ICLR 2020
Taking expectation on both sides of (76) with respect to the noise introduced by SGD, we have
EF(ω(t+1),θ*(ω(t+1))) - F(ω(HI), θ*(ω㈤))≤ 宗*Mg∙
Summing the equation(75) up, we have
N2
X E∣∣VωF(ω,θ*(ω)4
t=1	2
1N	L
≤ — EEF(ω⑴，θ*(ω㈤))-F(ω(t+1), θ*(ω(t))) + LNηωMG
1N
≤ — XEF(ω⑴,θ*(ω㈤))-F(ω(t+1), θ*(ω(t+1)))
ηω i=1
1 N	L
+ 1- XEF(ω(t+1),θ*(ω(t+1))) - F(ω(HI), θ*(ω⑴))+ LNηωMG
1N
≤ — XEF(ω⑴,θ*(ω㈤))-F(ω(t+1), θ*(ω(t+1)))
ηω i=1
1 N S2	L
+ -X Sω危MG +	NηωMg.
ηω i=1 2μ	2
Dividing both sides of the above equation by N, we get
Iminr E ∣∣Vω F (ω(t),θ*(ω(t)))∣∣2 JF Q(1),θ*3(I)))- EF,N+1),θ*3(N+I)))I
+(Lω+Sμμ )ηω MG.
Since |F(ω⑴，θ*(ω(I))) - EF(ω(N+1),θ*(ω(N +1)))∣ ≤ 2Bf. Take ηω =，L/2+⅞⅛mgN,
then we have
IminV 盟口F Q(t),θ*3(t)))∣∣2 ≤ 4r BF (Lω+⅞/μ)MG,
12ρ2
where BF = ——g + XMh . ThiS implies that when ηω = BCr~~'	- ,We need at most
μ	√2(2Lω +Sω∕μ)MG
N = O ((Pg/μ + λMH ) (Lω + Sω ∕μ)MG !
such that IN <e .	□
31