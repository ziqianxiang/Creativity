Published as a conference paper at ICLR 2020
Sharing Knowledge in Multi-Task
Deep Reinforcement Learning
Carlo D’Eramo & Davide Tateo
Department of Computer Science
TU Darmstadt, IAS
HochschUlstraβe 10, 64289, Darmstadt, Germany
{carlo.deramo,davide.tateo}@tu-darmstadt.de
Andrea Bonarini & Marcello Restelli
Politecnico di Milano, DEIB
Piazza Leonardo da Vinci 32, 20133, Milano
{andrea.bonarini,marcello.restelli}@polimi.it
Jan Peters
TU Darmstadt, IAS
Hochschulstraβe 10, 64289, Darmstadt, Germany
Max Planck InstitUte for Intelligent Systems
Max-Planck-Ring 4, 72076, Tubingen, Germany
jan.peters@tu-darmstadt.de
Ab stract
We study the benefit of sharing representations among tasks to enable the effective
use of deep neural networks in Multi-Task Reinforcement Learning. We leverage
the assumption that learning from different tasks, sharing common properties, is
helpful to generalize the knowledge of them resulting in a more effective feature ex-
traction compared to learning a single task. Intuitively, the resulting set of features
offers performance benefits when used by Reinforcement Learning algorithms.
We prove this by providing theoretical guarantees that highlight the conditions
for which is convenient to share representations among tasks, extending the well-
known finite-time bounds of Approximate Value-Iteration to the multi-task setting.
In addition, we complement our analysis by proposing multi-task extensions of
three Reinforcement Learning algorithms that we empirically evaluate on widely
used Reinforcement Learning benchmarks showing significant improvements over
the single-task counterparts in terms of sample efficiency and performance.
1	Introduction
Multi-Task Learning (MTL) ambitiously aims to learn multiple tasks jointly instead of learning them
separately, leveraging the assumption that the considered tasks have common properties which can be
exploited by Machine Learning (ML) models to generalize the learning of each of them. For instance,
the features extracted in the hidden layers of a neural network trained on multiple tasks have the
advantage of being a general representation of structures common to each other. This translates into
an effective way of learning multiple tasks at the same time, but it can also improve the learning
of each individual task compared to learning them separately (Caruana, 1997). Furthermore, the
learned representation can be used to perform Transfer Learning (TL), i.e. using it as a preliminary
knowledge to learn a new similar task resulting in a more effective and faster learning than learning
the new task from scratch (Baxter, 2000; Thrun & Pratt, 2012).
The same benefits of extraction and exploitation of common features among the tasks achieved
in MTL, can be obtained in Multi-Task Reinforcement Learning (MTRL) when training a single
agent on multiple Reinforcement Learning (RL) problems with common structures (Taylor & Stone,
2009; Lazaric, 2012). In particular, in MTRL an agent can be trained on multiple tasks in the same
1
Published as a conference paper at ICLR 2020
domain, e.g. riding a bicycle or cycling while going towards a goal, or on different but similar
domains, e.g. balancing a pendulum or balancing a double pendulum1. Considering recent advances
in Deep Reinforcement Learning (DRL) and the resulting increase in the complexity of experimental
benchmarks, the use of Deep Learning (DL) models, e.g. deep neural networks, has become a popular
and effective way to extract common features among tasks in MTRL algorithms (Rusu et al., 2015;
Liu et al., 2016; Higgins et al., 2017). However, despite the high representational capacity of DL
models, the extraction of good features remains challenging. For instance, the performance of the
learning process can degrade when unrelated tasks are used together (Caruana, 1997; Baxter, 2000);
another detrimental issue may occur when the training of a single model is not balanced properly
among multiple tasks (Hessel et al., 2018).
Recent developments in MTRL achieve significant results in feature extraction by means of algorithms
specifically developed to address these issues. While some of these works rely on a single deep
neural network to model the multi-task agent (Liu et al., 2016; Yang et al., 2017; Hessel et al., 2018;
Wulfmeier et al., 2019), others use multiple deep neural networks, e.g. one for each task and another
for the multi-task agent (Rusu et al., 2015; Parisotto et al., 2015; Higgins et al., 2017; Teh et al., 2017).
Intuitively, achieving good results in MTRL with a single deep neural network is more desirable
than using many of them, since the training time is likely much less and the whole architecture is
easier to implement. In this paper we study the benefits of shared representations among tasks. We
theoretically motivate the intuitive effectiveness of our method, deriving theoretical guarantees that
exploit the theoretical framework provided by Maurer et al. (2016), in which the authors present
upper bounds on the quality of learning in MTL when extracting features for multiple tasks in a
single shared representation. The significancy of this result is that the cost of learning the shared
representation decreases with a factor O(1∕√t), where T is the number of tasks for many function
approximator hypothesis classes. The main contribution of this work is twofold.
1.	We derive upper confidence bounds for Approximate Value-Iteration (AVI) and Approximate
Policy-Iteration (API)2 (Farahmand, 2011) in the MTRL setting, and we extend the approx-
imation error bounds in Maurer et al. (2016) to the case of multiple tasks with different
dimensionalities. Then, we show how to combine these results resulting in, to the best
of our knowledge, the first proposed extension of the finite-time bounds of AVI/API to
MTRL. Despite being an extension of previous works, we derive these results to justify
our approach showing how the error propagation in AVI/API can theoretically benefit from
learning multiple tasks jointly.
2.	We leverage these results proposing a neural network architecture, for which these bounds
hold with minor assumptions, that allow us to learn multiple tasks with a single regressor
extracting a common representation. We show an empirical evidence of the consequence of
our bounds by means of a variant of Fitted Q-Iteration (FQI) (Ernst et al., 2005), based on our
shared network and for which our bounds apply, that we call Multi Fitted Q-Iteration (MFQI).
Then, we perform an empirical evaluation in challenging RL problems proposing multi-
task variants of the Deep Q-Network (DQN) (Mnih et al., 2015) and Deep Deterministic
Policy Gradient (DDPG) (Lillicrap et al., 2015) algorithms. These algorithms are practical
implementations of the more general AVI/API framework, designed to solve complex
problems. In this case, the bounds apply to these algorithms only with some assumptions,
e.g. stationary sampling distribution. The outcome of the empirical analysis joins the
theoretical results, showing significant performance improvements compared to the single-
task version of the algorithms in various RL problems, including several MuJoCo (Todorov
et al., 2012) domains.
2	Preliminaries
Let B(X ) be the space of bounded measurable functions w.r.t. the σ-algebra σX, and similarly
B(X, L) be the same bounded by L < ∞.
A Markov Decision Process (MDP) is defined as a 5-tuple M =< S, A, P, R, γ >, where S is the
state space, A is the action space, P : S × A → S is the transition distribution where P (s0 |s, a)
1For simplicity, in this paper we refer to the concepts of task and domain interchangeably.
2All proofs and the theorem for API are in Appendix A.2.
2
Published as a conference paper at ICLR 2020
is the probability of reaching state s0 when performing action a in state s, R : S × A × S →
R is the reward function, and γ ∈ (0, 1] is the discount factor. A deterministic policy π maps,
for each state, the action to perform: π : S → A. Given a policy π, the value of an action
a in a state s represents the expected discounted cumulative reward obtained by performing a
in s and following π thereafter: Qπ(s, a) , E[Pk∞=0 γkri+k+1 |si = s, ai = a, π], where ri+1
is the reward obtained after the i-th transition. The expected discounted cumulative reward is
maximized by following the optimal policy π* which is the one that determines the optimal action
values, i.e., the ones that satisfy the Bellman optimality equation (Bellman, 1954): Q*(s,a)，
JS P (s0∣s,a)[R(s,a,s0) + Y max。，Q*(s0 ,a0)] ds0. The solution of the Bellman optimality equation
is the fixed point of the optimal Bellman operator T* : B(S XA) → B(S X A) defined as
(T * Q)(s,a)，RS P (s0∣s, a)[R(s, a, s0) + Y max。，Q(s0, a0)]ds0. In the MTRL setting, there are
multiple MDPs M(t) =< S(t), A(t), P(t), R(t), γ(t) > where t ∈ {1, . . . , T} and T is the number
of MDPs. For each MDP M(t), a deterministic policy πt : S(t) → A(t) induces an action-value
function Qtπt (s(t), a(t)) = E[Pk∞=0 Ykri(+t)k+1 |si = s(t), ai = a(t), πt]. In this setting, the goal is to
maximize the sum of the expected cumulative discounted reward of each task.
In our theoretical analysis of the MTRL problem, the complexity of representation plays a central role.
As done in Maurer et al. (2016), we consider the Gaussian complexity, a variant of the well-known
Rademacher complexity, to measure the complexity of the representation. Given a set XX ∈ XTn of n
input samples for each task t ∈ {1, . . . , T}, and a class H composed of k ∈ {1, . . . , K} functions,
the Gaussian complexity of a random set H(XX) = {(hk(Xti)) : h ∈ H} ⊆ RKTn is defined as
follows:
G(H(X)) = E sup X γtkihk(Xti)Xti ,	(1)
h∈H tki
where Ytki are independent standard normal variables. We also need to define the following quantity,
taken from Maurer (2016): let γ be a vector of m random standard normal variables, and f ∈ F :
Y → Rm , with Y ⊆ Rn , we define
hγ, f(y) - f(y0)i
O(F) =	SUP E SUP ---------------η---- .	(2)
y,y0∈Y,y=y0	f ∈F ky - y k
Equation 2 can be viewed as a Gaussian average of Lipschitz quotients, and appears in the bounds
provided in this work. Finally, we define L(F) as the upper bound of the Lipschitz constant of all the
functions f in the function class F.
3	Theoretical analysis
The following theoretical study starts from the derivation of theoretical guarantees for MTRL in the
AVI framework, extending the results of Farahmand (2011) in the MTRL scenario. Then, to bound
the approximation error term in the AVI bound, we extend the result described in Maurer (2006)
to MTRL. As we discuss, the resulting bounds described in this section clearly show the benefit of
sharing representation in MTRL. To the best of our knowledge, this is the first general result for
MTRL; previous works have focused on finite MDPs (Brunskill & Li, 2013) or linear models (Lazaric
& Restelli, 2011).
3.1	Multi-task representation learning
The multi-task representation learning problem consists in learning simultaneously a set of T tasks
μt, modeled as probability measures over the space of the possible input-output pairs (x, y), with
x ∈ X and y ∈ R, being X the input space. Let w ∈ W : X → RJ, h ∈ H : RJ → RK and
f ∈ F : RK → R be functions chosen from their respective hypothesis classes. The functions
in the hypothesis classes must be Lipschitz continuous functions. Let Z = (Zι,..., ZT) be the
multi-sample over the set of tasks μ = (μι,..., μτ), where Zt = (Ztι,..., Ztn)〜 μn and
Zti = (Xti, Yti) ~ μt. We can formalize our regression problem as the following minimization
3
Published as a conference paper at ICLR 2020
problem:
1TN
min] nTΣΣ'(ft(h(wt(Xti))),Yti) ： f ∈ 产，h ∈H W ∈ W T∖ ,	⑶
where we use f = (fι,..., fτ), W = (wι,..., WT), and define the minimizers of Equation (3) as W,
h, and f. We assume that the loss function ` : R × R → [0, 1] is 1-Lipschitz in the first argument for
every value of the second argument. While this assumption may seem restrictive, the result obtained
can be easily scaled to the general case. To use the principal result of this section, for a generic loss
function '0, it is possible to use '(∙) = '0G)/emx, where EmaX is the maximum value of '0. The expected
loss over the tasks, given W, h and f is the task-averaged risk:
1T
εavg(W, h,f) = T 52E ['(ft(h(wt(X ))),Y)]
(4)
t=1
The minimum task-averaged risk, given the set of tasks μ and the hypothesis classes W, H and F is
ε*vg, and the corresponding minimizers are w*, h and f*.
3.2	Multi-task Approximate Value Iteration bound
We start by considering the bound for the AVI framework which applies for the single-task scenario.
Theorem 1. (Theorem 3.4 ofFarahmand (2011)) Let Kbea positive integer, and Qmαχ ≤ RRmaχ. Then
for any sequence (Qk)kK=0 ⊂ B(S × A, Qmax) and the corresponding sequence (εk)kK=-01, where
εk = kQk+ι - T*QkkV, we have:
kQ*- QnK kι,ρ≤ 7τ^γ52 [ infι 1C3I,ρ,ν (K ； r)E 1(εo,...,εκ-i; r) +ɪ YK RmJ ,⑸
(1 - γ)2 r∈[0,1]	,,	1 - γ
where
11	、2	K-1
CVI,ρ,ν(K； r) = I	) SUp	X ak(1-r)	X Ym ccVIι,ρ,ν(m,K - k; πK)
π10 ,...,πK0 k=0	m≥0
2
+c VI2,ρ,ν (m + l; πk + 1,...,πK )),	⑹
with E(ε0, . . . , εK-1; r) = PkK=-01 αk2r εk, the two coefficients cVI1,ρ,ν, cVI2,ρ,ν, the distributions ρ
and ν, and the series αk are defined as in Farahmand (2011).
In the multi-task scenario, let the average approximation error across tasks be:
1T
εavg,k (W k, h k, fk ) = T ^"^kQt,k + 1 -TrQt,kkV,	⑺
t=1
where Qt,k+ι = ft,k ◦ hk ◦ Wt,k, and 7t* is the optimal Bellman operator of task t.
In the following, we extend the AVI bound of Theorem 1 to the multi-task scenario, by computing
the average loss across tasks and pushing inside the average using Jensen’s inequality.
Theorem 2. Let Kbea positive integer, and Q max ≤ R-aχ. Thenfor any sequence (Qk)K=0 ⊂ B(SX
A, Qmax) and the corresponding sequence (εavg,k)kK=-01, where ε avg ,k = T PT=ikQt,k+1 -Tt^Qt,k IlV，
we have:
TK
ɪ XkQ： - Q∏κkι,ρ ≤ 7r⅛ inf, ICVI (K ； r)Ea! (ε avg ,0,∙∙∙,ε avg ,k-i； r)+ ∖, max, avg
T t=1 (1 - Y)	r∈[0,1]	1 - Y
(8)
with Eavg = pK=01 α ε avg ,k, Y = max Yt, CVI (K ； r) = max	CVIPV (K ； t,r), Rmax, avg
t∈{1,...,T}	t∈{1,...,T}
1	μi-7)7κ-k-1
T PT=I Rmax,t and αk = ( (1-Y)YK+1
I 1-γK+1
0 ≤ k < K,
k=K
4
Published as a conference paper at ICLR 2020
Remarks Theorem 2 retains most of the properties of Theorem 3.4 of Farahmand (2011), except
that the regression error in the bound is now task-averaged. Interestingly, the second term of the
sum in Equation (8) depends on the average maximum reward for each task. In order to obtain this
result we use an overly pessimistic bound on γ and the concentrability coefficients, however this
approximation is not too loose if the MDPs are sufficiently similar.
3.3 Multi-task approximation error bound
We bound the task-averaged approximation error εavg at each AVI iteration k involved in (8) following
a derivation similar to the one proposed by Maurer et al. (2016), obtaining:
Theorem 3. Let μ, W, H and F be defined as above and assume 0 ∈ H and f (0) = 0,∀f ∈ F.
Thenfor δ > 0 with probability at least 1 一 δ in the draw of Z 〜QT=I μf we have that
εavg(W, h, f) ≤ L(F) (ci "H)SUpi”...，T} G(W(Xl)) + C2 SUpwkw(X)kO(H)
n	nT
l	minp∈p G(H(p))∖ ι	suph,wkh(w(X))kO(F)	88ln(3) , *
+c3 —nτ— ++c4----------------n√τ---------+ V ~tγ++%.
(9)
Remarks The assumptions 0 ∈ H and f(0) = 0 for all f ∈ F are not essential for the proof and
are only needed to simplify the result. For reasonable function classes, the Gaussian complexity
G(W(Xl)) is O(√n). If SupW∣∣w(X)k and suph,w∣∣h(w(X))k can be uniformly bounded, then
they are O(√nT). For some function classes, the Gaussian average of Lipschitz quotients O(∙) can
be bounded independently from the number of samples. Given these assumptions, the first and the
fourth term of the right hand side of Equation (9), which represent respectively the cost of learning the
meta-state space W and the task-specific f mappings, are both O(1∕√n). The second term represents
the cost of learning the multi-task representation h and is O(1∕√nτ), thus vanishing in the multi-task
limit T → ∞. The third term can be removed if ∀h ∈ H, ∃p0 ∈ P : h(p) = 0; even when this
assumption does not hold, this term can be ignored for many classes of interest, e.g. neural networks,
as it can be arbitrarily small.
The last term to be bounded in (9) is the minimum average approximation error εa*vg at each AVI
iteration k. Recalling that the task-averaged approximation error is defined as in (7), applying
Theorem 5.3 by Farahmand (2011) we obtain:
Lemma 4. Let Q*,k, ∀t ∈ {1,...,T} be the mιnιmιzers of ε * vg ,k, tk = arg max⅛∈{i,...,τ }kQ*,k+ι 一
TtQt,k kV, and bk,i = kQ%+ι - T*Q您,ikν, then:
with CAE defined as in Farahmand (2011).
Final remarks The bound for MTRL is derived by composing the results in Theorems 2 and 3, and
Lemma 4. The results above highlight the advantage of learning a shared representation. The bound
in Theorem 2 shows that a small approximation error is critical to improve the convergence towards
the optimal action-value function, and the bound in Theorem 3 shows that the cost of learning the
shared representation at each AVI iteration is mitigated by using multiple tasks. This is particularly
beneficial when the feature representation is complex, e.g. deep neural networks.
3.4 Discussion
As stated in the remarks of Equation (9), the benefit of MTRL is evinced by the second component
of the bound, i.e. the cost of learning h, which vanishes with the increase of the number of tasks.
Obviously, adding more tasks require the shared representation to be large enough to include all
of them, undesirably causing the term Suph Wkh(W(X ))k in the fourth component of the bound to
increase. This introduces a tradeoff between the number of features and number of tasks; however, for
5
Published as a conference paper at ICLR 2020
Output
Input
fl
wI
f,
x2
w2
y?
xT
yτ
(a) Shared network
xι
y1

0.50
= 0.45
^0.40
Y 0.35
*0.30
= 0.25
0.20
0.15
Figure 1: (a) The architecture of the neural network we propose to Ieam T tasks simultaneously.
The wt block maps each input xt from task μt to a shared set of layers h which extracts a common
representation of the tasks. Eventually, the shared representation is specialized in block ft and the
output yt of the network is computed. Note that each block can be composed of arbitrarily many
layers. (b) Results of FQI and MFQI averaged over 4 tasks in Car-On-Hill, showing ∣∣Q* — Q7τκ ∣∣ on
the left, and the discounted cumulative reward on the right, (c) Results of MFQI showing ∖∖Q*-Q7τκ ∣∣
for increasing number of tasks. Both results in (b) and (c) are averaged over 100 experiments, and
show the 95% confidence intervals.
a reasonable number of tasks the number of features used in the single-task case is enough to handle
them, as we show in some experiments in Section 5. Notably, since the AVI/API framework provided
by Farahmand (2011) provides an easy way to include the approximation error of a generic function
approximator, it is easy to show the benefit in MTRL of the bound in Equation (9). Despite being just
multi-task extensions of previous works, our results are the first one to theoretically show the benefit
of sharing representation in MTRL. Moreover, they serve as a significant theoretical motivation,
besides to the intuitive ones, of the practical algorithms that we describe in the following sections.
4	Sharing representations
We want to empirically evaluate the benefit of our theoretical study in the problem of jointly learning
T different tasks μt, introducing a neural network architecture for which our bounds hold. Following
our theoretical framework, the network we propose extracts representations Wt from inputs xt for each
task μt, mapping them to common features in a set of shared layers h, specializing the learning of
each task in respective separated layers ft, and finally computing the output 机=(扭。h。=
ft{h{wt{xt))) (Figure 1(a)). The idea behind this architecture is not new in the literature. For
instance, similar ideas have already been used in DQN variants to improve exploration on the same
task via bootstrapping (Osband et al., 2016) and to perform MTRL (Liu et al., 2016).
The intuitive and desirable property of this architecture is the exploitation of the regularization effect
introduced by the shared representation of the jointly learned tasks. Indeed, unlike learning a single
task that may end up in Overfitting, forcing the model to compute a shared representation of the tasks
helps the regression process to extract more general features, with a consequent reduction in the
variance of the learned function. This intuitive justification for our approach, joins the theoretical
benefit proven in Section 3. Note that our architecture can be used in any MTRL problem involving a
regression process; indeed, it can be easily used in value-based methods as a Q-function regressor,
or in policy search as a policy regressor. In both cases, the targets are learned for each task μt
in its respective output block ft. Remarkably, as we show in the experimental Section 5, it is
straightforward to extend RL algorithms to their multi-task variants only through the use of the
proposed network architecture, without major changes to the algorithms themselves.
5	Experimental results
To empirically evince the effect described by our bounds, we propose an extension of FQI (Ernst
et al., 2005; Riedmiller, 2005), that we call MFQI, for which our AVI bounds apply. Then, to
empirically evaluate our approach in challenging RL problems, we introduce multi-task variants
of two well-known DRL algorithms: DQN (Mnih et al., 2015) and DDPG (Lillicrap et al., 2015),
which we call Multi Deep Q-Network (MDQN) and Multi Deep Deterministic Policy Gradient
(MDDPG) respectively. Note that for these methodologies, our AVI and API bounds hold only with
6
Published as a conference paper at ICLR 2020
(a) Multi-task
Figure 2: Discounted cumulative reward averaged over 100 experiments of DQN and MDQN for
each task and for transfer learning in the Acrobot problem. An epoch consists of 1, 000 steps, after
which the greedy policy is evaluated for 2, 000 steps. The 95% confidence intervals are shown.
(b) Transfer
the simplifying assumption that the samples are i.i.d.; nevertheless they are useful to show the benefit
of our method also in complex scenarios, e.g. MuJoCo (Todorov et al., 2012). We remark that in
these experiments we are only interested in showing the benefit of learning multiple tasks with a
shared representation w.r.t. learning a single task; therefore, we only compare our methods with
the single task counterparts, ignoring other works on MTRL in literature. Experiments have been
developed using the MushroomRL library (D’Eramo et al., 2020), and run on an NVIDIAR DGX
StationTM and IntelR AI DevCloud. Refer to Appendix B for all the details and our motivations
about the experimental settings.
5.1	MULTI FITTED Q-ITERATION
As a first empirical evaluation, we consider FQI, as an example of an AVI algorithm, to show the
effect described by our theoretical AVI bounds in experiments. We consider the Car-On-Hill problem
as described in Ernst et al. (2005), and select four different tasks from it changing the mass of the
car and the value of the actions (details in Appendix B). Then, we run separate instances of FQI
with a single task network for each task respectively, and one of MFQI considering all the tasks
simultaneously. Figure 1(b) shows the L1-norm of the difference between Q* and QnK averaged
over all the tasks. It is clear how MFQI is able to get much closer to the optimal Q-function, thus
giving an empirical evidence of the AVI bounds in Theorem 2. For completeness, we also show the
advantage of MFQI w.r.t. FQI in performance. Then, in Figure 1(c) we provide an empirical evidence
of the benefit of increasing the number of tasks in MFQI in terms of both quality and stability.
5.2	MULTI DEEP Q-NETWORK
As in Liu et al. (2016), our MDQN uses separate replay memories for each task and the batch
used in each training step is built picking the same number of samples from each replay memory.
Furthermore, a step of the algorithm consists of exactly one step in each task. These are the only
minor changes to the vanilla DQN algorithm we introduce, while all other aspects, such as the use of
the target network, are not modified. Thus, the time complexity of MDQN is considerably lower than
vanilla DQN thanks to the learning of T tasks with a single model, but at the cost of a higher memory
complexity for the collection of samples for each task. We consider five problems with similar
state spaces, sparse rewards and discrete actions: Cart-Pole, Acrobot, Mountain-Car, Car-On-Hill,
and Inverted-Pendulum. The implementation of the first three problems is the one provided by the
OpenAI Gym library Brockman et al. (2016), while Car-On-Hill is described in Ernst et al. (2005)
and Inverted-Pendulum in Lagoudakis & Parr (2003).
Figure 2(a) shows the performance of MDQN w.r.t. to vanilla DQN that uses a single-task network
structured as the multi-task one in the case with T = 1. The first three plots from the left show good
performance of MDQN, which is both higher and more stable than DQN. In Car-On-Hill, MDQN is
slightly slower than DQN to reach the best performance, but eventually manages to be more stable.
Finally, the Inverted-Pendulum experiment is clearly too easy to solve for both approaches, but it is
still useful for the shared feature extraction in MDQN. The described results provide important hints
about the better quality of the features extracted by MDQN w.r.t. DQN. To further demonstrate this,
we evaluate the performance of DQN on Acrobot, arguably the hardest of the five problems, using
a single-task network with the shared parameters in h initialized with the weights of a multi-task
7
Published as a conference paper at ICLR 2020
Inverted-Double-Pendulum Inverted-Pendulum-Swingup
Inverted-Pendulum
yueuuojjed
φuueuuQμ,ud
0	50	100	0	50	100	0	50	100
⅛Epochs	⅛Epochs	⅛Epochs
(a) Multi-task for pendulums
Walker	Half-Cheetah
Hopper
<υ
占30
(o
∣2θ
(υ
d 10
0	50	100	0	50	100	0	50	100
⅛Epochs	⅛Epochs	⅛Epochs
(c) Multi-task for walkers
0	50	ιoo
#Epochs
(b) Transfer for pendulums
(er
50	100
#Epochs
(d) Transfer for walkers
Figure 3: Discounted cumulative reward averaged over 40 experiments of DDPG and MDDPG for
each task and for transfer learning in the Inverted-Double-Pendulum and Hopper problems. An
epoch consists of 10, 000 steps, after which the greedy policy is evaluated for 5, 000 steps. The 95%
confidence intervals are shown.
network trained with MDQN on the other four problems. Arbitrarily, the pre-trained weights can be
adjusted during the learning of the new task or can be kept fixed and only the remaining randomly
initialized parameters in w and f are trained. From Figure 2(b), the advantages of initializing the
weights are clear. In particular, we compare the performance of DQN without initialization w.r.t.
DQN with initialization in three settings: in Unfreeze-0 the initialized weights are adjusted, in No-
Unfreeze they are kept fixed, and in Unfreeze-10 they are kept fixed until epoch 10 after which they
start to be optimized. Interestingly, keeping the shared weights fixed shows a significant performance
improvement in the earliest epochs, but ceases to improve soon. On the other hand, the adjustment of
weights from the earliest epochs shows improvements only compared to the uninitialized network
in the intermediate stages of learning. The best results are achieved by starting to adjust the shared
weights after epoch 10, which is approximately the point at which the improvement given by the
fixed initialization starts to lessen.
5.3	Multi Deep Deterministic Policy Gradient
In order to show how the flexibility of our approach easily allows to perform MTRL in policy search
algorithms, we propose MDDPG as a multi-task variant of DDPG. As an actor-critic method, DDPG
requires an actor network and a critic network. Intuitively, to obtain MDDPG both the actor and critic
networks should be built following our proposed structure. We perform separate experiments on two
sets of MuJoCo Todorov et al. (2012) problems with similar continuous state and action spaces: the
first set includes Inverted-Pendulum, Inverted-Double-Pendulum, and Inverted-Pendulum-Swingup as
implemented in the pybullet library, whereas the second set includes Hopper-Stand, Walker-Walk,
and Half-Cheetah-Run as implemented in the DeepMind Control SuiteTassa et al. (2018). Figure 3(a)
shows a relevant improvement of MDDPG w.r.t. DDPG in the pendulum tasks. Indeed, while in
Inverted-Pendulum, which is the easiest problem among the three, the performance of MDDPG is
only slightly better than DDPG, the difference in the other two problems is significant. The advantage
of MDDPG is confirmed in Figure 3(c) where it performs better than DDPG in Hopper and equally
good in the other two tasks. Again, we perform a TL evaluation of DDPG in the problems where
it suffers the most, by initializing the shared weights of a single-task network with the ones of a
multi-task network trained with MDDPG on the other problems. Figures 3(b) and 3(d) show evident
advantages of pre-training the shared weights and a significant difference between keeping them fixed
or not.
8
Published as a conference paper at ICLR 2020
6	Related works
Our work is inspired from both theoretical and empirical studies in MTL and MTRL literature. In
particular, the theoretical analysis we provide follows previous results about the theoretical properties
of multi-task algorithms. For instance, Cavallanti et al. (2010) and Maurer (2006) prove the theoretical
advantages of MTL based on linear approximation. More in detail, Maurer (2006) derives bounds on
MTL when a linear approximator is used to extract a shared representation among tasks. Then, Maurer
et al. (2016), which we considered in this work, describes similar results that extend to the use of
non-linear approximators. Similar studies have been conducted in the context of MTRL. Among the
others, Lazaric & Restelli (2011) and Brunskill & Li (2013) give theoretical proofs of the advantage
of learning from multiple MDPs and introduces new algorithms to empirically support their claims,
as done in this work.
Generally, contributions in MTRL assume that properties of different tasks, e.g. dynamics and reward
function, are generated from a common generative model. About this, interesting analyses consider
Bayesian approaches; for instance Wilson et al. (2007) assumes that the tasks are generated from a
hierarchical Bayesian model, and likewise Lazaric & Ghavamzadeh (2010) considers the case when
the value functions are generated from a common prior distribution. Similar considerations, which
however does not use a Bayesian approach, are implicitly made in Taylor et al. (2007), Lazaric et al.
(2008), and also in this work.
In recent years, the advantages of MTRL have been empirically evinced also in DRL, especially
exploiting the powerful representational capacity of deep neural networks. For instance, Parisotto
et al. (2015) and Rusu et al. (2015) propose to derive a multi-task policy from the policies learned by
DQN experts trained separately on different tasks. Rusu et al. (2015) compares to a therein introduced
variant of DQN, which is very similar to our MDQN and the one in Liu et al. (2016), showing how
their method overcomes it in the Atari benchmark Bellemare et al. (2013). Further developments,
extend the analysis to policy search (Yang et al., 2017; Teh et al., 2017), and to multi-goal RL (Schaul
et al., 2015; Andrychowicz et al., 2017). Finally, Hessel et al. (2018) addresses the problem of
balancing the learning of multiple tasks with a single deep neural network proposing a method that
uniformly adapts the impact of each task on the training updates of the agent.
7	Conclusion
We have theoretically proved the advantage in RL of using a shared representation to learn multiple
tasks w.r.t. learning a single task. We have derived our results extending the AVI/API bounds (Farah-
mand, 2011) to MTRL, leveraging the upper bounds on the approximation error in MTL provided
in Maurer et al. (2016). The results of this analysis show that the error propagation during the
AVI/API iterations is reduced according to the number of tasks. Then, we proposed a practical way of
exploiting this theoretical benefit which consists in an effective way of extracting shared representa-
tions of multiple tasks by means of deep neural networks. To empirically show the advantages of our
method, we carried out experiments on challenging RL problems with the introduction of multi-task
extensions of FQI, DQN, and DDPG based on the neural network structure we proposed. As desired,
the favorable empirical results confirm the theoretical benefit we described.
9
Published as a conference paper at ICLR 2020
Acknowledgments
This project has received funding from the European Union’s Horizon 2020 research and innovation
programme under grant agreement No. #640554 (SKILLS4ROBOTS) and No. #713010 (GOAL-
Robots). This project has also been supported by grants from NVIDIA, the NVIDIA DGX Station,
and the IntelR AI DevCloud. The authors thank Alberto Maria Metelli, Andrea Tirinzoni and Matteo
Papini for their helpful insights during the development of the project.
References
Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob
McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay.
In Advances in Neural Information Processing Systems, pp. 5048-5058, 2017.
Jonathan Baxter. A model of inductive bias learning. Journal of Artificial Intelligence Research, 12:
149-198, 2000.
Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environ-
ment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:
253-279, 2013.
Richard Bellman. The theory of dynamic programming. Technical report, RAND Corp Santa Monica
CA, 1954.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym, 2016.
Emma Brunskill and Lihong Li. Sample complexity of multi-task reinforcement learning. In
Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence, 2013.
Rich Caruana. Multitask learning. Machine learning, 28(1):41-75, 1997.
Giovanni Cavallanti, Nicolo Cesa-Bianchi, and Claudio Gentile. Linear algorithms for online
multitask classification. Journal of Machine Learning Research, 11(Oct):2901-2934, 2010.
Carlo D’Eramo, Davide Tateo, Andrea Bonarini, Marcello Restelli, and Jan Peters. Mushroomrl:
Simplifying reinforcement learning research. arXiv:2001.01102, 2020.
Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning.
Journal of Machine Learning Research, 6(Apr):503-556, 2005.
Amir-massoud Farahmand. Regularization in reinforcement learning. 2011.
Matteo Hessel, Hubert Soyer, Lasse Espeholt, Wojciech Czarnecki, Simon Schmitt, and Hado van
Hasselt. Multi-task deep reinforcement learning with popart. arXiv:1809.04474, 2018.
Irina Higgins, Arka Pal, Andrei Rusu, Loic Matthey, Christopher Burgess, Alexander Pritzel, Matthew
Botvinick, Charles Blundell, and Alexander Lerchner. Darla: Improving zero-shot transfer in
reinforcement learning. In International Conference on Machine Learning, pp. 1480-1490, 2017.
Michail G Lagoudakis and Ronald Parr. Least-squares policy iteration. Journal of machine learning
research, 4(Dec):1107-1149, 2003.
Alessandro Lazaric. Transfer in reinforcement learning: a framework and a survey. In Reinforcement
Learning, pp. 143-173. Springer, 2012.
Alessandro Lazaric and Mohammad Ghavamzadeh. Bayesian multi-task reinforcement learning. In
ICML-27th International Conference on Machine Learning, pp. 599-606. Omnipress, 2010.
Alessandro Lazaric and Marcello Restelli. Transfer from multiple mdps. In Advances in Neural
Information Processing Systems, pp. 1746-1754, 2011.
Alessandro Lazaric, Marcello Restelli, and Andrea Bonarini. Transfer of samples in batch rein-
forcement learning. In Proceedings of the 25th international conference on Machine learning, pp.
544-551. ACM, 2008.
10
Published as a conference paper at ICLR 2020
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015.
Lydia Liu, Urun Dogan, and Katja Hofmann. Decoding multitask dqn in the world of minecraft. In
European Workshop on Reinforcement Learning, 2016.
Andreas Maurer. Bounds for linear multi-task learning. Journal of Machine Learning Research, 7
(Jan):117-139, 2006.
Andreas Maurer. A chain rule for the expected suprema of gaussian processes. Theoretical Computer
Science, 650:109-122, 2016.
Andreas Maurer, Massimiliano Pontil, and Bernardino Romera-Paredes. The benefit of multitask
representation learning. The Journal of Machine Learning Research, 17(1):2853-2884, 2016.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control
through deep reinforcement learning. Nature, 518(7540):529, 2015.
Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via
bootstrapped dqn. In Advances in neural information processing systems, pp. 4026-4034, 2016.
Emilio Parisotto, Jimmy Lei Ba, and Ruslan Salakhutdinov. Actor-mimic: Deep multitask and
transfer reinforcement learning. arXiv preprint arXiv:1511.06342, 2015.
Martin Riedmiller. Neural fitted q iteration-first experiences with a data efficient neural reinforcement
learning method. In European Conference on Machine Learning, pp. 317-328. Springer, 2005.
Andrei A Rusu, Sergio Gomez Colmenarejo, Caglar Gulcehre, Guillaume Desjardins, James Kirk-
patrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, and Raia Hadsell. Policy
distillation. arXiv preprint arXiv:1511.06295, 2015.
Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approximators.
In International Conference on Machine Learning, pp. 1312-1320, 2015.
Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden,
Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, Timothy P. Lillicrap, and Martin A. Riedmiller.
Deepmind control suite. CoRR, abs/1801.00690, 2018.
Matthew E Taylor and Peter Stone. Transfer learning for reinforcement learning domains: A survey.
Journal of Machine Learning Research, 10(Jul):1633-1685, 2009.
Matthew E Taylor, Peter Stone, and Yaxin Liu. Transfer learning via inter-task mappings for temporal
difference learning. Journal of Machine Learning Research, 8(Sep):2125-2167, 2007.
Yee Teh, Victor Bapst, Wojciech M Czarnecki, John Quan, James Kirkpatrick, Raia Hadsell, Nicolas
Heess, and Razvan Pascanu. Distral: Robust multitask reinforcement learning. In Advances in
Neural Information Processing Systems, pp. 4496-4506, 2017.
Sebastian Thrun and Lorien Pratt. Learning to learn. Springer Science & Business Media, 2012.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE, 2012.
Aaron Wilson, Alan Fern, Soumya Ray, and Prasad Tadepalli. Multi-task reinforcement learning: a
hierarchical bayesian approach. In Proceedings of the 24th international conference on Machine
learning, pp. 1015-1022. ACM, 2007.
Markus Wulfmeier, Abbas Abdolmaleki, Roland Hafner, Jost Tobias Springenberg, Michael Neunert,
Tim Hertweck, Thomas Lampe, Noah Siegel, Nicolas Heess, and Martin Riedmiller. Regularized
hierarchical policies for compositional transfer in robotics. arXiv:1906.11228, 2019.
Zhaoyang Yang, Kathryn E Merrick, Hussein A Abbass, and Lianwen Jin. Multi-task deep reinforce-
ment learning for continuous action control. In IJCAI, pp. 3301-3307, 2017.
11
Published as a conference paper at ICLR 2020
A Proofs
A. 1 Approximated Value-Iteration bounds
Proof of Theorem 2. We compute the average expected loss across tasks:
1T
T EkQ；- QnKhP
t=1
T
1X	2Yt_
T = (1 - Yt)2
≤
inf , CV2I PV (K ； t, r)E 2 (εt,0,..., εt,κ-1; t, r) +
r∈[0,1]
2K
彳	Yt Rmax,t
1 - γt
2γ	1
厂Y2 T
T
X
t=1
1
inf CVι,ρ,ν
r∈[0,1]
(K; t, r)E2 (εt,0,..., εt,κ-i; t, r) +
2K
Y	Yt Rmax,t
1 - γt t
2γ
(1-γ)2
CVI,ρ,ν(K； t, r)E2(εt,0,…，εt,κ-1; t, r)) + 1-―YKRmax,avg
2γ
(1-γ)2
inf 1
r∈[0,1] T t=1
inf
r∈[0,1]
T
12
I,ρ,ν (K; t,*E2 (εt,0, ..., εt,K-1;t, r)) + 1 YY Rmax,avg
2γ
(1-γ)2
-	1 .	. 1 N / 1 .	.∖	2	_
if, CVl(K ； r)不 £ (E 2(%。…，εt,K-1;t, r)) + ；-------------------------------YK Rmax,avg
r∈[0,1]	T	1 - Y
(11)
with Y = max
t∈{1,...,T}
11	T
Yt, CVI(K ； r) = max CVI PV (K ； t,r),and Rmax,avg = 1∕τ∑t=ι Rmax,t.
t∈{1,...,T}
1
Considering the term. 1/T P Pt=1 [e 2 (εt,o,..., ε∙t,κ— ι；t,r)] = 1/t ΣT=1 (∑K=01 α2,rkεt,k) 2 let
≤
≤
≤
≤
TX (
αk
(1-γ)γκ-k-1
1-γκ + 1
(1-γ)γκ
1-γκ + 1
0 ≤ k < K,
k=K
(
then we bound
T /K-1	\ 2	T /K-1	\ 2
T X X akεt,k	≤ T X X αkrεt,k
t=1	k=0	t=1	k=0
Using Jensen’s inequality:
So, now we can write (11) as
1T
T EkQ- Q∏κk1,ρ≤
t=1
2Y
(1 - Y )2
1	1
inf ] CVI(K； r)Eavg(εavg,0,..∙, εavg,K-1； r)
r∈[0,1]
2
- Y
+
1
Y Rmax,avg
With εavg,k = 1/t Pt=1 εt,k and Eavg(εavg,0, ..., εavg,K-1； r) = Pk=0 αkrεavg,k.
□
Proof of Lemma 4. Let us start from the definition of optimal task-averaged risk:
1T
ε嬴,k = T EkQzk+1-Tt*Qt,k kV,
t=1
12
Published as a conference paper at ICLR 2020
where Q↑k, with t ∈ [1, T], are the minimizers of εavg,k.
z-x ∙ 1	. 1	.	1	7	1.1
Consider the task t such that
tk = arg , maχ JIQ⅛,k+ι -Tt*Qt,kkV,
t∈{1,...,T }
we can write the following inequality:
Kk ≤kQl,k+ι -TiQik,kkν.
By the application of Theorem 5.3 by Farahmand (2011) to the right hand side, and defining
bk,i = kQik,i+ι - TtiQtk,ikν, we obtain:
k-1
Kk ≤kQik,k+1-(T*)k+1Qik ,0kν + X(Ytk CAE(V ； tk ,P ))i+1bk,k-i-i.
i=0
Squaring both sides yields the result:
εavg,k ≤
k-1
-(TI*)k+1Qtk,θkν + X(Ytk CAE(V ； tk,P ))i+1bk,k-1-i
i=0
2
□
A.2 Approximated Policy-Iteration bounds
kQ*-QπKHρ≤ (i⅛
We start by considering the bound for the API framework:
Theorem 5. (Theorem 3.2 ofFarahmand (2011)) Let Kbea positive integer, and Qmαχ ≤ R-ax. Then
for any sequence (Qk)kK=-01 ⊂ B(S × A, Qmax) and the corresponding sequence (εk)kK=-01, where
εk = IQk - Qπk Iν2, we have:
inf CP	(K; r)E2 (εo,..., εκ-i; r) + γK-rRmmax ,	(12)
∈[0,1] PI,ρ,ν
where
CPI,ρ,ν (K; r) =
2	K-1	/
,suP, X ak(1-r)	X YYmCPIι,ρ,ν(K — k — 1,m + 1； πk+1)+
π00 ,...,πK0 k=0	m≥0
γmc YmCPI2,ρ,ν(K — k — 1,m； πk+1,πk) + CPI3,ρ,ν I ；
m≥1
(13)
with E (εo,..., £K-i； r) = PK-II a2J' εk, the three coefficients CPi 巾产,CPI 2,ρ,ν, CPI3,ρ,ν ,the distri-
butions ρ and V, and the series αk are defined as in Farahmand (2011).
From Theorem 5, by computing the average loss across tasks and pushing inside the average using
Jensen’s inequality, we derive the API bounds averaged on multiple tasks.
Theorem6. LetKbeapositive integer, andQmαx ≤ R-χ. ThenfOranySequenCe (Qk)K- ⊂ B(SX
A, Qmax) and the corresponding sequence (εavg,k)kK=-01, where εavg,k = T PT=IkQt,k - Qnk kV, we
have:
T
TXkQ： - Q∏Kki, ≤ (T-yf
inf CPI(K； r)Eavg (εavg,0,..., εavg,K-1 ； r)
r∈[0,1]
+Y	Rmax,avg ,
(14)
1
1
13
Published as a conference paper at ICLR 2020
with Eavg = PK=-01 αkrεavg,k, Y = max Yt, CPI(K; r)
t∈{1,...,T}
1
max	CPI,ρ,ν (K; t, r)，Rmax,avg
t∈{1,...,T}
T PPt=I Rmax,t and ak
(1-γ)γK-k-1
1-γκ+1
(1-γ)γK
1-γκ+1
0 ≤ k < K,
k=K
Proof of Theorem 6. The proof is very similar to the one for AVI. We compute the average expected
loss across tasks:
1T
TXkQ： - QnKkι,ρ
t=1
T
1 X _2Yt_
t = (1 - Yt)2
.r c1	/ ” ,	∖ cl /	,	∖ , K-1 C
r∈i[0f1] CPI，P，V (K;t，r)E (εt,0,...,εt,κ-1; t,r)+ Yt	Rmax,t
2γ	1
Gf T
T
X
t=1
.r c1	/ ” ,	∖ cl /	,	∖ , K-1 C
r∈in)f1] CPI，P，V (K; t，r)E (εt,0,...,εt,κ-1; t,r)+ Yt Rmax,t
2γ
(1-γ)2
2γ
(1-γ)2
inf 1
r∈[0,1] T
inf CP2I PV (K ； t, r)E1 (εt,),..., εt,κ-i; t, r) ) + YKTRmaχ,avg
r∈[),1]
T
：X (C力,ν(K; t，r)E1 (εt,0,..., εt,κ-i; t, r)) + YKTRmax,avg
t=1
2γ
(1-Y)2
■	ɪ	ι t
r∈inf1] CPI (K; r) T X (E 1 (εt,0,..., εt,K-1; t，r)) + Y KTRwg
Using Jensen’s inequality as in the AVI scenario, we can write (15) as:
T
T∑kQ^- QπKkι,ρ ≤ (Γ⅛
1	1
inf CPI (K； r)EaVg(εavg,) , . . . , εavg,K-1; r)
r∈[),1]
(15)
{
≤
≤
≤
≤
≤
先（
+YK-1
with εavg,k = 1/T Pt=1 εt,k and Eavg (εavg,) , . . . , εavg,K-1 ; r) = Pk=) αkr εavg,k .
(16)
□
A.3 Approximation bounds
ProofofTheorem 3. Let wɪ,..., WT, h and f；,...,fT be the minimizers of £嬴，then:
1
εavg (W ,h, f) - εavg = "g(w ,h, f)-蒿£ '(ft(h(wt(XGY) ,Yti)
n ti
X---------------------{z-------------------
A
}
+ (ɪ X '(ft(h(Wt(Xti ))),Yti) - ɪ X '(ft* (h*(w； (Xti))),匕i)
nT	nT
ti	ti
'----------------------------------V--------------------------------
B
+ (nT X '(f*(h*(wt(Xti))),Yti)-ε*vg)	(17)
S------------------V----------------}
C
We proceed to bound the three components individually:
• C can be bounded using Hoeffding's inequality, with probability 1 - δ∕2 by ,ln%)∕(2nτ),
as it contains only nT random variables bounded in the interval [0, 1];
14
Published as a conference paper at ICLR 2020
•	B can be bounded by 0, by definition of W, h and f, as they are the mmιmιzers of EqUa-
tion (3);
•	the bounding of A is less straightforward and is described in the following.
We define the following auxiliary function spaces:
•	W0 = {x ∈ X → (wt(xti)) : (w1, . . . , wT) ∈ WT},
•	F0 = {y ∈ RKTn → (ft(yti)) : (fι,...,fτ) ∈ FT},
and the following auxiliary sets:
•	S = {('(ft(h(wt(Xti)X,Yti) : f ∈ F T ,h ∈H,w ∈ W T } ⊆ RTn,
•	S = F0(H(W0(X))) = {(ft(h(wt(Xti)))) : f ∈ FT,h ∈H,w ∈ WT} ⊆ RTn,
•	S00 = H(W0(X)) = {(h(wt(Xti))) : h ∈ H,w ∈ WT} ⊆ RKTn,
which will be useful in our proof.
Using Theorem 9 by Maurer et al. (2016), we can write:
1
εavg(w, h, f )-斤 5S'(ft(h(Wt(Xtiy)) Yti)
nT
ti
≤ SUp ( εavg (w,h, f) - ɪ X '(ft(h(wt(Xti))),Yti)∖
w∈WT,h∈H,f∈FT	nT ti
≤√πGS) + S9H),	(18)
nT	2nT
then by Lipschitz property of the loss function ` and the contraction lemma Corollary 11 Maurer et al.
(2016): G(S) ≤ G(S0). By Theorem 12 by Maurer et al. (2016), for universal constants c01 and c02:
G(S0) ≤ c01L(F0)G(S00) + c02D(S00)O(F0) +minG(F(y)),	(19)
y∈Y
where L(F0) is the largest value for the Lipschitz constants in the function space F0, and D(S00) is
the Euclidean diameter of the set S00 .
Using Theorem 12 by Maurer et al. (2016) again, for universal constants c010 and c020:
G(S00) ≤ C[L(H)G(W0(X))+ C2D(W0(X))O(H)+minG(H(p)).	(20)
p∈P
Putting (19) and (20) together:
G(S0) ≤ clL(F0) (cLl,^G(W0(X)) + c2zD(W0(X))O(H) + minG(H(p)))
p∈P
+ c02D(S00)O(F0) +minG(F(y))
y∈Y
=c1c[L(F 0)L(H)G(W 0(X)) + c1c20L(F 0)D(W ' (X))O(H) + c；L(F0) min G(H(p))
p∈P
+ c02D(S00)O(F0) +minG(F(y)).	(21)
y∈Y
At this point, we have to bound the individual terms in the right hand side of (21), following the same
procedure proposed by Maurer et al. (2016).
15
Published as a conference paper at ICLR 2020
Firstly, to bound L(F0), let y, y0 ∈ RKTn, where y = (yti) with yti ∈ RK and y0 = (yt0i) with
yt0i ∈ RK . We can write the following:
kf(y)-f(y0)k2=X(ft(yti)-ft(yt0i))2
ti
≤L(F)2Xkyti-yt0ik2
=L(F)2ky-y0k2,	(22)
whence L(F0) ≤ L(F).
Then, we bound:
G(W0(X)) = E sup TYktiwtk(Xti) Xti
w∈WT kti
≤ sup E sup	γkliwk(Xli)Xli
t l∈{1,...,T}	w∈W ki
= T sup	G(W(Xl)).	(23)
l∈{1,...,T}
Then, since it is possible to bound the Euclidean diameter using the norm of the supremum value in
the set, we bound D(Sfr) ≤ 2sup%,wkh(w(X))k and D(W0(X)) ≤ 2supw∈wτ ∣∣w(X)k.
Also, we bound O(F0):
E sup hγ, g(y) - g(y0)i = E sup	γti (ft(yti) - ft(yt0i))
g∈F0	f∈FT ti
E
t
sup	γi (f(yti) - f(yt0i))
f∈F i
≤√t∣EE SuF?Yi(f(yti)-f(y0i))
1
O(F)2 Xkyti-y0ik2)
√Tθ(F)ky -y0k,
(24)
whence O(F0) ≤ √TO(F).
To minimize the last term, it is possible to choose y0 = 0, as f(0) = 0, ∀f ∈ F, resulting in
miny∈Y G(F(y)) = G(F (0)) = 0.
Then, substituting in (21), and recalling that G(S) ≤ G(S0):
G(S) ≤ CdL(F)L(H)T	SuP	G(W(Xl))+ 2c1c,L(F) SuP IlW(X)IIO(H)
l∈{1,...,T}	w∈WT
+ clL(F) minG(H(P)) + 2c2 SuPkh(W(X))∣∣√Tθ(F).	(25)
p∈P	h,w
16
Published as a conference paper at ICLR 2020
Now, the first term A of (17) can be bounded substituting (25) in (18):
1
εavg(W, h, f) - nτ	'(ft(h(Wt(Xti))), Yti)
ti
≤浮(CICIL(F)L(H)T	SUp	G(W(Xl)) + 2c1cgL(F) SUp kw(X)∣∣O(H)
nT	l∈{1,...,T}	w∈WT
9ln( 2)
2nT
+ CIL(F) min G(H(P)) + 2c2 SUpkh(W(X))k√TO(F)
p∈P	h,w
CIL(F)L(H)supl∈{i,…,t} G(W(Xl))
+ C2
supw kw(X)kL(F )O(H)
nT
+ C3
L(F )minρ∈ρ G(H(P))
nT
+ C4
suph,w kh(w(X))k O(F)
n√τ
+∕9›.
n
A union bound between A, B and C of (17) completes the proof:
εavg(W, h,f) - ε嬴
≤ JL(F)L(H)SUpl∈{i,...,τ} G(W(Xl))
1n
SUpwkW(X) kL(F)O(H)
+ c2----------不--------
nT
L(F) minp∈P G(H(P))
+c3--------r-----------
nT
,SUph,w kh(w(X))k O(F)
+ C4---,------尸--------
+不.
nT
□
B	Additional details of empirical evaluation
B.1	MULTI FITTED Q-ITERATION
We consider Car-On-Hill problem with discount factor 0.95 and horizon 100. Running Adam
optimizer with learning rate 0.001 and using a mean squared loss, we train a neural network composed
of 2 shared layers of 30 neurons each, with sigmoidal activation function, as described in Riedmiller
(2005). We select 8 tasks for the problem changing the mass of the car m and the value of the
discrete actions a (Table 1). Figure 1(b) is computed considering the first four tasks, while Figure 1(c)
considers task 1 in the result with 1 task, tasks 1 and 2 for the result with 2 tasks, tasks 1, 2, 3, and 4
for the result with 4 tasks, and all the tasks for the result with 8 tasks. To run FQI and MFQI, for each
task we collect transitions running an extra-tree trained following the procedure and setting in Ernst
et al. (2005), using an -greedy policy with = 0.1, to obtain a small, but representative dataset. The
optimal Q-function for each task is computed by tree-search3 for 100 states uniformly picked from
the state space, and the 2 discrete actions, for a total of 200 state-action tuples.
B.2	MULTI DEEP Q-NETWORK
The five problems we consider for this experiment are: Cart-Pole, Acrobot, Mountain-Car, Car-On-
Hill, and Inverted-Pendulum4. The discount factors are respectively 0.99, 0.99, 0.99, 0.95, and 0.95.
The horizons are respectively 500, 1, 000, 1, 000, 100, and 3, 000. The network we use consists of80
ReLu units for each wt, t ∈ {1, . . . , T} block, with T = 5. Then, the shared block h consists of one
3We follow the method described in Ernst et al. (2005).
4The IDs of the problems in the OpenAI Gym library are: CartPole-v0, Acrobot-v1, and MountainCar-v0.
17
Published as a conference paper at ICLR 2020
Task	Mass	Action set
-1-	1.0	-{-4.0;4.0}-
-2-	0.8	-{-4.0；4.0}-
-3-	1.0	-{-4.5;4.5;-
-4-	1.2	-{-4.5;4.5}-
-5~	1.0	{-4.125；4.125}
6	1.0	{-4.25;4.25}
7	0.8	{-4.375;4.375}
8	0.85	{-4.0；4.0}~~
Table 1: Different values of the mass of the car and available actions chosen for the Car-On-Hill tasks
in the MFQI empirical evaluation.
layer with 80 ReLu units and another one with 80 sigmoid units. Eventually, each ft has a number of
linear units equal to the number of discrete actions a(t),i ∈ {1,..., #A(t)} of task μt which outputs
the action-value Qt(s, ai(t)) = yt(s, ai(t)) = ft(h(wt(s)), ai(t)), ∀s ∈ S(t). The use of sigmoid units
in the second layer of h is due to our choice to extract meaningful shared features bounded between 0
and 1 to be used as input of the last linear layer, as in most RL approaches. In practice, we have also
found that sigmoid units help to reduce task interference in multi-task networks, where instead the
linear response of ReLu units cause a problematic increase in the feature values. Furthermore, the use
of a bounded feature space reduces the suph,w ∣∣h(w(JX))k term in the upper bound of Theorem 3,
corresponding to the upper bound of the diameter of the feature space, as shown in Appendix A.
The initial replay memory size for each task is 100 and the maximum size is 5, 000. We use Huber
loss with Adam optimizer using learning rate 10-3 and batch size of 100 samples for each task. The
target network is updated every 100 steps. The exploration is ε-greedy with ε linearly decaying from
1 to 0.01 in the first 5, 000 steps.
B.3	Multi Deep Deterministic Policy Gradient
The two set of problems we consider for this experiment are: one including Inverted-Pendulum,
Inverted-Double-Pendulum, and Inverted-Pendulum-Swingup, and another one including Hopper-
Stand, Walker-Walk, and Half-Cheetah-Run5. The discount factors are 0.99 and the horizons are
1, 000 for all problems. The actor network is composed of 600 ReLu units for each wt, t ∈ {1, . . . , T}
block, with T = 3. The shared block h has 500 units with ReLu activation function as for MDQN.
Finally, each ft has a number of tanh units equal to the number of dimensions of the continuous
actions a⑴ ∈ A⑴ of task μt which outputs the policy ∏t(s) = yt(s) = ft(h(wt(s))), ∀s ∈ S⑶.
On the other hand, the critic network consists of the same wt units of the actor, except for the use of
sigmoidal units in the h layer, as in MDQN. In addition to this, the actions a(t) are given as input to
h. Finally, each ft has a single linear unit Qt(s, a(t)) = yt(s, a(t)) = ft(h(wt(s), a(t))), ∀s ∈ S(t).
The initial replay memory size for each task is 64 and the maximum size is 50, 000. We use Huber
loss to update the critic network and the policy gradient to update the actor network. In both cases
the optimization is performed with Adam optimizer and batch size of 64 samples for each task. The
learning rate of the actor is 10-4 and the learning rate of the critic is 10-3. Moreover, we apply
'2-penalization to the critic network using a regularization coefficient of 0.01. The target networks are
updated with soft-updates using τ = 10-3. The exploration is performed using the action computed
by the actor network adding a noise generated with an Ornstein-Uhlenbeck process with θ = 0.15
and σ = 0.2. Note that most of these values are taken from the original DDPG paper Lillicrap et al.
(2015), which optimizes them for the single-task scenario.
5The IDs of the problems in the pybullet library are: InvertedPendulumBulletEnv-v0,
InvertedDoublePendulumBulletEnv-v0, and InvertedPendulumSwingupBulletEnv-v0. The names of the
domain and the task of the problems in the DeepMind Control Suite are: hopper-stand, walker-walk, and
cheetah-run.
18