Published as a conference paper at ICLR 2020
Actor-Critic Provably Finds Nash Equilibria
of Linear-Quadratic Mean-Field Games
Zuyue Fu
Northwestern University
zuyue.fu@u.northwestern.edu
Zhuoran Yang
Princeton University
zy6@princeton.edu
Yongxin Chen
Georgia Institute of Technology
yongchen@gatech.edu
Zhaoran Wang
Northwestern University
zhaoranwang@gmail.com
Ab stract
We study discrete-time mean-field Markov games with infinite numbers of agents
where each agent aims to minimize its ergodic cost. We consider the setting
where the agents have identical linear state transitions and quadratic cost func-
tions, while the aggregated effect of the agents is captured by the population mean
of their states, namely, the mean-field state. For such a game, based on the Nash
certainty equivalence principle, we provide sufficient conditions for the existence
and uniqueness of its Nash equilibrium. Moreover, to find the Nash equilibrium,
we propose a mean-field actor-critic algorithm with linear function approxima-
tion, which does not require knowing the model of dynamics. Specifically, at
each iteration of our algorithm, we use the single-agent actor-critic algorithm to
approximately obtain the optimal policy of the each agent given the current mean-
field state, and then update the mean-field state. In particular, we prove that our
algorithm converges to the Nash equilibrium at a linear rate. To the best of our
knowledge, this is the first success of applying model-free reinforcement learn-
ing with function approximation to discrete-time mean-field Markov games with
provable non-asymptotic global convergence guarantees.
1	Introduction
In reinforcement learning (RL) (Sutton and Barto, 2018), an agent learns to make decisions that
minimize its expected total cost through sequential interactions with the environment. Multi-agent
reinforcement learning (MARL) (Shoham et al., 2003; 2007; Busoniu et al., 2008) aims to extend RL
to sequential decision-making problems involving multiple agents. In a non-cooperative game, we
are interested in the Nash equilibrium (Nash, 1951), which is a joint policy of all the agents such that
each agent cannot decrease its expected total cost by unilaterally deviating from its Nash policy. The
Nash equilibrium plays a critical role in understanding the social dynamics of self-interested agents
(Ash, 2000; Axtell, 2002) and constructing the optimal policy of a particular agent via fictitious self-
play (Bowling and Veloso, 2000; Ganzfried and Sandholm, 2009). With the recent development in
deep learning (LeCun et al., 2015), MARL with function approximation achieves tremendous empir-
ical successes in applications, including Go (Silver et al., 2016; 2017), Poker (Heinrich and Silver,
2016; Moravclk et al., 2017), Star Craft (Vinyals et al., 2019), Dota (OPenAL 2018), autonomous
driving (Shalev-Shwartz et al., 2016), multi-robotic systems (Yang and Gu, 2004), and solving social
dilemmas (de Cote et al., 2006; Leibo et al., 2017; Hughes et al., 2018). However, since the caPacity
of the joint state and action sPaces grows exPonentially in the number of agents, such MARL aP-
Proaches become comPutationally intractable when the number of agents is large, which is common
in real-world aPPlications (Sandholm, 2010; Calderone, 2017; Wang et al., 2017a).
Mean-field game is ProPosed by Huang et al. (2003; 2006); Lasry and Lions (2006a;b; 2007) with
the idea of utilizing mean-field aPProximation to model the strategic interactions within a large PoP-
ulation. In a mean-field game, each agent has the same cost function and state transition, which
dePend on the other agents only through their aggregated effect. As a result, the oPtimal Policy of
1
Published as a conference paper at ICLR 2020
each agent depends solely on its own state and the aggregated effect of the population, and such an
optimal policy is symmetric across all the agents. Moreover, if the aggregated effect of the popula-
tion corresponds to the Nash equilibrium, then the optimal policy of each agent jointly constitutes a
Nash equilibrium. Although such a Nash equilibrium corresponds to an infinite number of agents, it
well approximates the Nash equilibrium for a sufficiently large number of agents (Bensoussan et al.,
2016). Also, as the aggregated effect of the population abstracts away the strategic interactions be-
tween individual agents, it circumvents the computational intractability of the MARL approaches
that do not exploit symmetry.
However, most existing work on mean-field games focuses on characterizing the existence and
uniqueness of the Nash equilibrium rather than designing provably efficient algorithms. In par-
ticular, most existing work considers the continuous-time setting, which requires solving a pair of
Hamilton-Jacobi-Bellman (HJB) and Fokker-Planck (FP) equations, whereas the discrete-time set-
ting is more common in practice, e.g., in the aforementioned applications. Moreover, most existing
approaches, including the ones based on solving the HJB and FP equations, require knowing the
model of dynamics (Bardi and Priuli, 2014), or having the access to a simulator, which generates
the next state given any state-action pair and aggregated effect of the population (Guo et al., 2019),
which is often unavailable in practice.
To address these challenges, we develop an efficient model-free RL approach to mean-field game,
which provably attains the Nash equilibrium. In particular, we focus on discrete-time mean-field
games with linear state transitions and quadratic cost functions, where the aggregated effect of the
population is quantified by the mean-field state. Such games capture the fundamental difficulties
of general mean-field games and well approximates a variety of real-world systems such as power
grids (Minciardi and Sacile, 2011), swarm robots (Fang, 2014; Araki et al., 2017; Doerr et al.,
2018), and financial systems (Zhou and Li, 2000; Huang and Li, 2018). In detail, based on the
Nash certainty equivalence (NCE) principle (Huang et al., 2006; 2007), we propose a mean-field
actor-critic algorithm which, at each iteration, given the mean-field state μ, approximately attains
the optimal policy ∏μ of each agent, and then updates the mean-field state μ assuming that all the
agents follow ∏μ. We parametrize the actor and critic by linear and quadratic functions, respectively,
and prove that such a parameterization encompasses the optimal policy of each agent. Specifically,
we update the actor parameter using policy gradient (Sutton et al., 2000) and natural policy gradient
(Kakade, 2002; Peters and Schaal, 2008; Bhatnagar et al., 2009) and update the critic parameter
using primal-dual gradient temporal difference (Sutton et al., 2009a;b). In particular, we prove that
given the mean-field state μ, the sequence of policies generated by the actor converges linearly to the
optimal policy ∏*. Moreover, when alternatingly update the policy and mean-field state, We prove
that the sequence of policies and its corresponding sequence of mean-field states converge to the
unique Nash equilibrium at a linear rate. Our approach can be interpreted from both “passive” and
“active” perspectives: (i) Assuming that each self-interested agent employs the single-agent actor-
critic algorithm, the policy of each agent converges to the unique Nash policy, which characterizes
the social dynamics of a large population of model-free RL agents. (ii) For a particular agent,
our approach serves as a fictitious self-play method for it to find its Nash policy, assuming the other
agents give their best responses. To the best of our knowledge, our work establishes the first efficient
model-free RL approach with function approximation that provably attains the Nash equilibrium
of a discrete-time mean-field game. As a byproduct, we also show that the sequence of policies
generated by the single-agent actor-critic algorithm converges at a linear rate to the optimal policy
ofa linear-quadratic regulator (LQR) problem in the presence of drift, which may be of independent
interest.
Related Work. Mean-field game is first introduced in Huang et al. (2003; 2006); Lasry and Li-
ons (2006a;b; 2007). In the last decade, there is growing interest in understanding continuous-time
mean-field games. See, e.g., GUeant et al. (2011); Bensoussan et al. (2013); Gomes et al. (2014);
Carmona and Delarue (2013; 2018) and the references therein. Due to their simple structures,
continuous-time linear-quadratic mean-field games are extensively studied under various model as-
sumptions. See Li and Zhang (2008); Bardi (2011); Wang and Zhang (2012); Bardi and Priuli
(2014); Huang et al. (2016a;b); Bensoussan et al. (2016; 2017); Caines and Kizilkale (2017); Huang
and Huang (2017); Moon and BaSar (2018); Huang and Zhou (2019) for examples of this line of
work. Meanwhile, the literature on discrete-time linear-quadratic mean-field games remains rela-
tively scarce. Most of this line of work focuses on characterizing the existence of a Nash equi-
librium and the behavior of such a Nash equilibrium when the number of agents goes to infinity
2
Published as a conference paper at ICLR 2020
(Gomes et al., 2010; Tembine and Huang, 2011; Moon and Bayar, 2014; Biswas, 2015; Saldi et al.,
2018a;b; 2019). See also Yang et al. (2018a), which applies maximum entropy inverse RL (Ziebart
et al., 2008) to infer the cost function and social dynamics of discrete-time mean-field games with
finite state and action spaces. Our work is most related to Guo et al. (2019), where they propose
a mean-field Q-learning algorithm (Watkins and Dayan, 1992) for discrete-time mean-field games
with finite state and action spaces. Such an algorithm requires the access to a simulator, which,
given any state-action pair and mean-field state, outputs the next state. In contrast, both our state
and action spaces are infinite, and we do not require such a simulator but only observations of trajec-
tories under given mean-field state. Correspondingly, we study the mean-field actor-critic algorithm
with linear function approximation, whereas their algorithm is tailored to the tabular setting. Also,
our work is closely related to Mguni et al. (2018), which focuses on a more restrictive setting where
the state transition does not involve the mean-field state. In such a setting, mean-field games are
potential games, which is, however, not true in more general settings (Li et al., 2017; Briani and
Cardaliaguet, 2018). In comparison, we allow the state transition to depend on the mean-field state.
Meanwhile, they propose a fictitious self-play method based on the single-agent actor-critic algo-
rithm and establishes its asymptotic convergence. However, their proof of convergence relies on
the assumption that the single-agent actor-critic algorithm converges to the optimal policy, which
is unverified therein. In addition, our work is related to Jayakumar and Aditya (2019), where the
proposed algorithm is only shown to converge asymptotically to a stationary point of the mean-field
game.
Our work also extends the line of work on finding the Nash equilibria of Markov games using
MARL. Due to the computational intractability introduced by the large number of agents, such a
line of work focuses on finite-agent Markov games (Littman, 1994; 2001; Hu and Wellman, 1998;
Bowling, 2001; Lagoudakis and Parr, 2002; Hu and Wellman, 2003; Conitzer and Sandholm, 2007;
Perolat et al., 2015; Perolat et al., 2016b;a; 2018; Wei et al., 2017; Zhang et al., 2018; Zou et al.,
2019; Casgrain et al., 2019). See also Shoham et al. (2003; 2007); Busoniu et al. (2008); Li (2018)
for detailed surveys. Our work is related to Yang et al. (2018b), where they combine the mean-field
approximation of actions (rather than states) and Nash Q-learning (Hu and Wellman, 2003) to study
general-sum Markov games with a large number of agents. However, the Nash Q-learning algorithm
is only applicable to finite state and action spaces, and its convergence is established under rather
strong assumptions. Also, when the number of agents goes to infinity, their approach yields a variant
of tabular Q-learning, which is different from our mean-field actor-critic algorithm.
For policy optimization, based on the policy gradient theorem, Sutton et al. (2000); Konda and Tsit-
siklis (2000) propose the actor-critic algorithm, which is later generalized to the natural actor-critic
algorithm (Peters and Schaal, 2008; Bhatnagar et al., 2009). Most existing results on the conver-
gence of actor-critic algorithms are based on stochastic approximation using ordinary differential
equations (Bhatnagar et al., 2009; Castro and Meir, 2010; Konda and Tsitsiklis, 2000; Maei, 2018),
which are asymptotic in nature. For policy evaluation, the convergence of primal-dual gradient tem-
poral difference is studied in Liu et al. (2015); Du et al. (2017); Wang et al. (2017b); Yu (2017);
Wai et al. (2018). However, this line of work assumes that the feature mapping is bounded, which
is not the case in our setting. Thus, the existing convergence results are not applicable to analyzing
the critic update in our setting. To handle the unbounded feature mapping, we utilize a truncation
argument, which requires more delicate analysis.
Finally, our work extends the line of work that studies model-free RL for LQR. For example, Bradtke
(1993); Bradtke et al. (1994) show that policy iteration converges to the optimal policy, Tu and
Recht (2017); Dean et al. (2017) study the sample complexity of least-squares temporal-difference
for policy evaluation. More recently, Fazel et al. (2018); Malik et al. (2018); Tu and Recht (2018)
show that the policy gradient algorithm converges at a linear rate to the optimal policy. See as also
Hardt et al. (2016); Dean et al. (2018) for more in this line of work. Our work is also closely related
to Yang et al. (2019), where they show that the sequence of policies generated by the natural actor-
critic algorithm enjoys a linear rate of convergence to the optimal policy. Compared with this work,
when fixing the mean-field state, we use the actor-critic algorithm to study LQR in the presence of
drift, which introduces significant difficulties in the analysis. As we show in §3, the drift causes the
optimal policy to have an additional intercept, which makes the state- and action-value functions
more complicated.
3
Published as a conference paper at ICLR 2020
Notations. We denote by ∣∣Mk* the spectral norm, P(M) the spectral radius, σmin(M) the min-
imum singular value, and σmax (M) the maximum singular value of a matrix M. We use kαk2 to
represent the '2-norm of a vector α, and (α)j to denote the sub-vector (αi, αi+ι,..., a§)>, where
αk is the k-th entry of the vector α. For scalars a1, . . . , an, we denote by poly(a1, . . . , an) the
polynomial of a1 , . . . , an, and this polynomial may vary from line to line. We use [n] to denote the
set {1, 2, . . . , n} for any n ∈ N.
2	Linear-Quadratic Mean-Field Game
A linear-quadratic mean-field Na-player game involves Na ∈ N agents, whose state transitions are
given by
x；+i = Axtt + Butt + Axt + di + ωi,	∀t ≥ 0, i ∈ [Na].
Here A ∈ Rm×m, B ∈ Rm×k, and A ∈ Rm×m are matrices, Xt ∈ Rm and ut ∈ Rk are the state and
action vectors of agent i, respectively, the vector dt ∈ Rm is a drift term, ωtt ∈ Rm is an independent
random noise term following the Gaussian distribution N(0, Ψω), and Xt = 1/Na ∙ PN= 1 XjiS the
mean-field state. The agents are coupled through the mean-field state χt. In the linear-quadratic
mean-field Na-player game, the cost of agent i ∈ [Na] at time t ≥ 0 is given by
Ct = (χt)>Qχi + (ut)>Rui + x>QXt,
where Q ∈ Rm×m, R ∈ Rk×k, and Q ∈ Rm×m are matrices, and ut is generated by ∏i i.e., the
policy of agent i. To measure the performance of agent i following its policy πt under the influence
of the other agents, we define the expected total cost of agent i as
J i(π1,π2,...,πNa )= Tlimo E G XX ct).
We are interested in finding a Nash equilibrium (π1, π2, . . . , πNa), which is defined by
Jt (π1, . . . , πt-1, πt , πt+1, . . . , πNa) ≤ Jt (π1, . . . , πt-1, πet , πt+1, . . . , πNa),	∀πet , i ∈ [Na].
That is, agent i cannot further decrease its expected total cost by unilaterally deviating from its Nash
policy.
For the simplicity of discussion, we assume that the drift term dt is identical for each agent. We
consider taking the infinite-population limit Na → ∞, where each agent has an infinitesimal contri-
bution to the dynamics of the system. Thus, the joint policy of all the agents except agent i can be
modeled as a mean-field policy ∏*, and all the agents following such a mean-field policy ∏* generate
the mean-field state ExJ, where {χj}t≥o is generated following the policy ∏L By the symmetry of
the agents in terms of their state transitions and cost functions, we focus on a fixed agent and drop
the superscript i hereafter.
Before we formally present the formulation of linear-quadratic mean-field games, we first introduce
the following mean-field LQR (MF-LQR) problem, which aims to find an optimal policy for the
fixed agent given the mean-field policy πJ .
Problem 2.1 (MF-LQR). Given the mean-field policy πJ, we consider the following formulation,
Xt+1 = Axt + But + AExJ + d + ωt,
c(χt,ut) = x>Qxt + u>Rut + (ExJ)>Q(Exj),
1T
J(∏,∏J)=Jim E - Vc(xt,ut),
T →o T
t=0
where xt ∈ Rm is the state vector, ut ∈ Rk is the action vector generated by the policy π, {xtJ}t≥0 is
the trajectory generated by the policy πJ, ωt ∈ Rm is an independent random noise term following
the Gaussian distribution N(0, Ψω), and d ∈ Rm is a drift term. Here the expectation ExtJ is taken
across all the agents. We aim to find ∏* such that J(π*, πj) = inf∏∈∏ J(π, πj).
4
Published as a conference paper at ICLR 2020
Note that a controllable linear system using linear quadratic optimal control is always stable. Further
combining the fact that our linear closed-loop dynamics in Problem 2.1 is driven by the Gaussian
noise term ωt, We know that the Markov chain of states generated by the policy ∏* admits a stationary
distribution and converges to this stationary distribution. This implies that the mean-field state Ext
converges to a constant vector μt as t → ∞, which serves as a time-invariant mean-field state. As we
consider the ergodic setting, it then suffices to study Problem 2.1 with t sufficiently large. Therefore,
the influence of the mean-field policy ∏t is captured by the mean-field state μt. By re-formulating
Problem 2.1, with slight abuse of notations, we obtain the following drifted-LQR (D-LQR).
Problem 2.2 (D-LQR). Given a mean-field state μ ∈ Rm, we consider the following formulation,
xt+1 = Axt + But + Aμ + d + ωt,
cμ(xt, ut) = x> Qxt + u> Rut + μ> Qμ,
T
Jμ(π) = lim E
μ
T→∞
1
TEc”(xt,ut)
t=0
where xt ∈ Rm is the state vector, ut ∈ Rk is the action vector generated by the policy π, ωt ∈ Rm
is an independent random noise term following the Gaussian distribution N(0, Ψω), and d ∈ Rm is
a drift term. We aim to find an optimal policy ∏μ such that Jμ(∏μ) = inf∏∈∏ Jμ(∏).
Compared with the most studied LQR problem (Lewis et al., 2012), both the state transition and
the cost function in Problem 2.2 have drift terms, which act as the mean-field “force” that drives
the states away from zero. Such a mean-field “force” introduces additional challenges when solving
Problem 2.2 in the model-free setting (see §3.3 for details). On the other hand, the unique optimal
policy ∏μ of Problem 2.2 admits a linear form ∏μ(xt) = -Kn方xt + b∏* under certain regularity
conditions (Anderson and Moore, 2007), where the matrix K∏* ∈ Rk×m and the vector b∏* ∈ Rk
μ	μ
are the parameters of ∏*. Motivated by such a linear form of the optimal policy, we define the class
of linear-Gaussian policies as
Π = {π(x) = -Knx + b∏ + σ ∙ η: Kn ∈ Rk×m, b∏ ∈ Rk},
(2.1)
where σ ∈ R and the standard Gaussian noise term η ∈ Rk is included to encourage exploration. To
solve Problem 2.2, it suffices to find the optimal policy ∏μ within Π. We define Λι(μ) = ∏μ as the
optimal policy under the mean-field state μ.
Assume that all the agents follow the linear policy π(x) = -Knx + bn under the mean-field state
μ. By plugging ut = π(xt) into the state transition in Problem 2.2, as t → ∞, we know that these
agents generate a new mean-field state μ∩ew such that
μ∏ew = (I — A + BKn ) 1 (Bbn + Aμ + d).
We define Λ2 (μ, ∏) = μ□ew as such a new mean-field state.
Now, we are ready to present the following linear-quadratic mean-field game (LQ-MFG).
Problem 2.3 (LQ-MFG). We consider the following formulation,
xt+1 = Axt + But + Aμ + d + ωt,
c(xt, ut) = x>Qxt + u>Rut + μ>Qμ,
T
J(π, μ) = lim E
T→∞
1T
TEc(Xt,ut)
t=0
where xt ∈ Rm is the state vector, ut ∈ Rk is the action vector generated by the policy ∏, μ ∈ Rm
is the mean-field state, ωt ∈ Rm is an independent random noise term following the Gaussian
distribution N(0, Ψω), and d ∈ Rm is a drift term. We aim to find a pair (μ*,π*) such that (i)
J(π*,μ*) = infn∈∏ J(π, μ*); (ii) Exj= converges to μ* as t → ∞, where {xJ= }t≥o is the Markov
chain of states generated by the policy ∏*.
The formulation in Problem 2.3 is studied by Lasry and Lions (2007); Bensoussan et al. (2016); Saldi
et al. (2018a;b). We propose a more general formulation in Problem C.2 (see §C of the appendix for
5
Published as a conference paper at ICLR 2020
details), where an additional interaction term between the state vector Xt and the mean-field state μ
is incorporated into the cost function. According to our analysis in §C, up to minor modification, the
results in the following sections also carry over to Problem C.2. Therefore, for the sake of simplicity,
we focus on Problem 2.3 in the sequel.
In Problem 2.3, condition (i) is equivalent to the optimality of the policy ∏* under the mean-field
state μ*, namely, Λι(μ*) = π*. Meanwhile, condition (ii) is equivalent to the invariance of the
mean-field state μ* given the policy ∏*, namely, Λ2(μ*,∏*) = μ*. Such equivalence follows from
the NCE principle (Huang et al., 2006; 2007), which also motivates the following definition of the
Nash equilibrium pair (Saldi et al., 2018a;b).
Definition 2.4 (Nash Equilibrium Pair). The pair (μ*,π*) ∈ Rm X Π constitutes a Nash equilibrium
pair of Problem 2.3 if it satisfies π* = Λι(μ*) and μ* = Λ2(μ*,π*). Here μ* is called the Nash
mean-field state and ∏* is called the Nash policy.
By Definition 2.4, Problem 2.3 aims to find a Nash equilibrium pair (μ*,π*).
3	Mean-Field Actor-Critic
We first characterize the existence and uniqueness of the Nash equilibrium pair of Problem 2.3 under
mild regularity conditions, and then propose a mean-field actor-critic algorithm to obtain such a Nash
equilibrium. As a building block of the mean-field actor-critic, we propose the natural actor-critic to
solve Problem 2.2.
3.1	Existence and Uniqueness of Nash Equilibrium Pair
We now establish the existence and uniqueness of the Nash equilibrium pair defined in Definition
2.4. We impose the following regularity conditions.
Assumption 3.1. We assume that the following statements hold:
(i)	The algebraic Riccati equation X = A>XA + Q - A>XB(B>XB + R)-1B>XA admits a
unique symmetric positive definite solution X*;
(ii)	It holds for L0 = L1 L3 + L2 that L0 < 1, where
Li = Il [(I — A)QT(I — A)> + BR-1B>]-1A∣∣2 ∙∣∣ [K*Q-1(I - A)> - RTB>]∣∣2,
L = [1 — P(A — BK*)]-l∙kAk2,	L3 = [1 - P(A - BK*)]-1∙kB∣∣2.
Here K * = -(B>X *B + R)-1B>X *A.
The first assumption is implied by mild regularity conditions on the matrices A, B, Q, and R, which
are (1) the positivity of R; (2) the non-negativity of Q = C>C; (3) the observability of (A, C);
(4) the stability of (A, B). See De Souza et al. (1986); Lewis et al. (2012) for more details. The
second assumption is standard in the literature (Bensoussan et al., 2016; Saldi et al., 2018b), which
ensures the stability of the LQ-MFG. In the following proposition, we show that Problem 2.3 admits
a unique Nash equilibrium pair.
Proposition 3.2 (Existence and Uniqueness of Nash Equilibrium Pair). Under Assumption 3.1, the
operator Λ(∙) = Λ2(∙, Λι(∙)) is Lo-Lipschitz, where Lo is given in Assumption 3.1. Moreover, there
exists a unique Nash equilibrium pair (μ*,π*) of Problem 2.3.
Proof. See §E.1 for a detailed proof.	□
3.2	Mean-Field Actor-Critic for LQ-MFG
The NCE principle motivates a fixed-point approach to solve Problem 2.3, which generates a se-
quence of policies {∏s}s≥o and mean-field states {μs}s≥o satisfying the following two properties:
(i) Given the mean-field state μs, the policy ∏ is optimal. (ii) The mean-field state becomes μs+ι
as t → ∞, if all the agents follow ∏ under the current mean-field state μs. Here (i) requires solving
Problem 2.2 given the mean-field state μs, while (ii) requires simulating the agents following the
6
Published as a conference paper at ICLR 2020
Algorithm 1 Mean-Field Actor-Critic for solving LQ-MFG.
1:	Input:
•	Initial mean-field state μo and Initial policy ∏o with parameters Ko and b0.
•	Numbers of iterations S,	{Ns}s∈[S] ,	{Hs}s∈[S] ,	{Ts,n, Ts,n}s∈[S],n∈[Ns] ,
{Tesb,h, Tsb,h}s∈[S],h∈[Hs] .
•	Stepsizes {γs}s∈[S] , {γsb}s∈[S] , {γs,n,t}s∈[S],n∈[Ns],t∈[Ts,n] , {γsb,h,t}s∈[S],h∈[Hs],t∈[Tsb,h] .
2:	for s = 0, 1, 2, . . . , S - 1 do
3:	Policy Update: Solve for the optimal policy πs+1 with parameters Ks+1 and bs+1 of Problem
2.2 Via Algorithm 2 With 〃5, ∏s, N3, Hs, {Ts,n, Ts,n}n∈[Ns], {τb,h,Tb,h}h∈Hs], Ys, Yb,
{γs,n,t}n∈[Ns],t∈[Ts,n],and{γsb,h,t}h∈[Hs],t∈[Tsbh],whichgivestheestimatedmean-fieldstate
μKs + ι ,bs + ι.
4:	Mean-Field State Update: Update the mean-field state via μs+ι — μκs+ι,bs+ι.
5:	end for
6:	Output: Pair (∏s,μs).
policy ∏s given the current mean-field μs. Based on such properties, we propose the mean-field
actor-critic in Algorithm 1.
Algorithm 1 requires solving Problem 2.2 at each iteration to obtain ∏ = Λι(μs) and μs+ι =
Λ2(μs, ∏s). To this end, we introduce the natural actor-critic in §3.3 that solves Problem 2.2.
3.3	Natural Actor-Critic for D-LQR
Now we focus on solving Problem 2.2 for a fixed mean-field state μ, we thus drop the subscript
μ hereafter. With slight abuse of notations, we write ∏κ,b(x) = -Kx + b + σ ∙ η to emphasize
the dependence on K and b, and J(K, b) = J (πK,b) consequently. Now, we propose the natural
actor-critic to solve Problem 2.2.
For any policy πK,b ∈ Π, by the state transition in Problem 2.2, we have
xt+1 = (A — BK)xt + (Bb + Aμ + d) + q, Et 〜N(0, Ψ∈),	(3.1)
where Ψ = σBB> + Ψω. It is known that if ρ(A - BK) < 1, then the Markov chain {xt}t≥0
induced by (3.1) has a unique stationary distribution N(μκ,b, Φk) (Anderson and Moore, 2007),
where the mean-field state μκ,b and the covariance Φk satisfy that
μκ,b = (I — A + BK )-1 (Bb + Aμ + d),	(3.2)
ΦK= (A - BK)ΦK(A - BK)> + Ψ.	(3.3)
Meanwhile, the Bellman equation for Problem 2.2 takes the following form
PK = (Q + K>RK) + (A - BK)>PK(A -BK).	(3.4)
Then by calculation (see Proposition B.2 in §B.1 of the appendix for details), it holds that the ex-
pected total cost J(K, b) is decomposed as
J(K, b) = Ji (K) + J (K, b) + σ2 ∙ tr(R) + μ>Qμ,	(3.5)
where J1(K) and J2(K, b) are defined as
J1(K) =tr(Q+K>RK)ΦK = tr(PKΨ),
J2(K,b) = ("K,b)> (Q+-KKRK-K>R) ("K,b) .	(3.6)
Here J1(K) is the expected total cost in the most studied LQR problems (Yang et al., 2019; Fazel
et al., 2018), where the state transition does not have drift terms. Meanwhile, J2(K, b) corresponds
to the expected cost induced by the drift terms. The following two propositions characterize the
properties of J2 (K, b).
First, we show that J2(K, b) is strongly convex in b.
7
Published as a conference paper at ICLR 2020
Proposition 3.3. Given any K, the function J2 (K, b) is νK -strongly convex in b. Here νK =
σmin(Y1>,KY1,K + Y2>,KY2,K), where Y1,K = R1/2K(I - A + BK)-1B - R1/2 and Y2,K =
Q1/2(I - A + BK)-1B. Also, J2(K, b) has ιK -Lipschitz continuous gradient in b, where ιK is
upper bounded as ∣k ≤ [1 - P(A - BK)]-2 ∙ (∣∣Bk2 ∙ kK∣2 ∙ ∣∣R∣∣2 + ∣∣B∣∣2 ∙ ∣∣Qk2).
Proof. See §E.4 for a detailed proof.	口
Second, we show that minb J2(K, b) is independent of K.
Proposition 3.4. We define bK = argminb J2 (K, b), where J2 (K, b) is defined in (3.6). It holds
that
bκ = [KQ-1(I - A)> - R-1B>] ∙ [(I - A)QT(I - A)> + BR-1B>]-1 ∙ (Aμ + d).
Moreover, J2(K, bK) takes the form of
J2(K, bκ) = (Aμ + d)> [(I - A)QT(I - A)> + BR-1B>]-1 ∙ (Aμ + d),
which is independent of K .
Proof. See §E.2 for a detailed proof.
□
Since minb J2(K, b) is independent of K by Proposition 3.4, it holds that the optimal K * is the
same as argminK J1(K). This motivates us to minimize J(K, b) by first updating K following the
gradient direction NK J (K) to the optimal K*, then updating b following the gradient direction
VbJ2(K*, b). We now design our algorithm based on this idea.
We define ΥK, pK,b, and qK,b as
YK=(QB>PPKa rABKPKB) = (YKI	Υf),
Pκ,b = A>	[PK	∙ (Aμ + d)	+ fK,b],	qK,b =	B> [PK	∙ (Aμ +	d)	+	fK,b],
(3.7)
where f&b = (I 一 A + BK)->[(A 一 BK)>PK(Bb + Aμ + d) 一 K>Rb]. By calculation (see
Proposition B.3 in §B.1 of the appendix for details), the gradients of J1(K) and J2(K, b) take the
forms of
Vk Ji(K) = 2(Y22K - Y汕∙ Φk,	VbJ2(K, b) = YK(-KμK,b + b) + YKμK,b + q&b.
Our algorithm follows the natural actor-critic method (Bhatnagar et al., 2009) and actor-critic
method (Konda and Tsitsiklis, 2000). Specifically, (i) To obtain the optimal K*, in the critic up-
date step, we estimate the matrix YK by YK via a policy evaluation algorithm, e.g., Algorithm 3 or
Algorithm 4 (see §B.2 and §B.3 of the appendix for details); in the actor update step, we update K
via K J K - Y ∙ (Y K2K - Y K), where the term Y K2K - Y K is the estimated natural gradient. (ii)
To obtain the optimal b* given K*, in the critic update step, We estimate Yk*, qκ*,b, and μκ*,b by
Yk*, qκ*,b, and μκ*,b Viaa policy evaluation algorithm; In the actor update step, We update b Via
b J b - γ ∙V b J2 (K *, b), where V bJ (K *, b) = Y K* (-K* μκ* ,b + b)+ Y K μκ * ,b + qκ* ,b is the
estimated gradient. Combining the above procedure, we obtain the natural actor-critic for Problem
2.2, which is stated in Algorithm 2.
One may want to apply gradient method to J(K, b) directly in the joint space of K and b. However,
the gradient dominance property of J1(K) in the most studied LQR problem (Yang et al., 2019)
no longer holds for J(K, b). Therefore, the convergence of the gradient method to J(K, b) is not
guaranteed in our problem.
4	Global Convergence Results
The following theorem establishes the rate of convergence of Algorithm 1 to the Nash equilibrium
pair (μ*,π*) of Problem2.3.
8
Published as a conference paper at ICLR 2020
Algorithm 2 Natural Actor-Critic Algorithm for D-LQR.
1:	Input:
•	Mean-field state μ and initial policy ∏κ0,bo.
•	Numbers of iterations N, H, {Ten, Tn}n∈[N] , {Tehb, Thb}h∈[H] .
•	Stepsizes γ, γb, {γn,t}n∈[N],t∈[Tn], {γhb,t}h∈[H],t∈[Thb] .
2:	for n = 0, 1, 2, . . . , N - 1 do
3： Critic Update： Compute YKn Via Algorithm 3 With ∏Kn,b0, μ, Tn,Tn, {γn,t}t∈[τn], Ko, and
b0 as inputs.
4:	Actor Update: Update the parameter via
Kn+1 - Kn -Y ∙(YKnKn - YKn )∙
5:	end for
6:	for h = 0, 1, 2, . . . , H - 1 do
7:	Critic Update: Compute °Kn 6,YKN, 谟欢 及 via Algorithm 3 with ∏KN,bh, μ, Tb, Th,
{γhb,t}t∈[Thb], K0, and b0.
8:	Actor Update: Update the parameter via
bh+1 J bh - Yb ∙ [yK2n (-KNbK,bh + bh) + YKNbKN,bh + GKN,bh].
9:	end for
10:	Output: Policy ∏K,b = ∏KnRh, estimated mean-field state μK,b = Gkn2笈.
Theorem 4.1 (Convergence of Algorithm 1). For a sufficiently small tolerance ε > 0, we set the
number of iterations S in Algorithm 1 such that
S	log(kμ0 - μ* *k2 ∙ ε-1)
>	lθg(1∕Lθ)
(4.1)
For any s ∈ [S], we define
εs= min{ [1 — ρ(A - BK*)]4(kB∣∣2 + |国2)-4(|底||-2 + |回-2) CmM) ∙ σmm(R) ∙ ε2,
νκ* ∙ [1 - P(A - BK*)]4 ∙ kBk-2 ∙ Mb(μs) ∙ ε2, ε} ∙ 2-s-10,	(4.2)
where νκ* is defined in Proposition 3.3 and
Mb(μs) = 4∣∣Q-1(I - A)> ∙ [(i - A)QT(I - A)> + BR-1B>]-1 ∙ (Aμs + d)(
• [ν-* + σι-n(ψe) ∙。—需(R)] ” .	(4.3)
In the s-th policy update step in Line 3 of Algorithm 1, we set the inputs via Theorem B.4 such that
Jμs (∏s+ι) 一 Jμs (∏μs) < £§, Where the expected total cost Jμs (•) is defined in Problem 2.2, and
∏*s = Λι(μs) is the optimal policy under the mean-field state μs. Then it holds with probability at
least 1 - ε5 that
kμs - μ*k2 ≤ ε,	IlKS - K*∣∣f ≤ ε,	IlbS - b*∣∣2 ≤ (1 + Li) ∙ ε.
Here μ* is the Nash mean-field state, KS and bS are parameters of the policy πS, and K* and b* are
parameters of the Nash policy π*.
Proof Sketch. The proof of the theorem is based on the convergence of the natural actor-critic al-
gorithm 2 and a contraction argument. First, we prove in Theorem B.4 that Algorithm 2 converges
linearly to the optimal policy of Problem 2.2. By this, in each iteration of Algorithm 1, we control
the error between μs+1 and μs*+1 to be εs > 0 with high probability, where μs*+1 is the mean-field
state generated by the optimal policy Λ1(μs); in other words, μs*+1 = Λ(μs). Combining the fact
from Proposition 3.2 that Λ(∙) is a contraction, we deduce that
Iμs+1 - μ* I2 ≤ ∣∣Λ(μs) - Λ(μ*)∣∣2 + εes ≤ L0 • Iμs - μ* I2 + εes
9
Published as a conference paper at ICLR 2020
with high probability, where εes > 0 is some error term and is specified in the detailed proof. More-
over, by telescoping sum, and note that the sum PsS=1 εes is upper bounded by the desired error ε,
We conclude the theorem. See §D.1 for a detailed proof.	□
We highlight that if the inputs of Algorithm 1 satisfy the conditions stated in Theorem B.4, it holds
that Jμs (∏s+ι) - Jμs (∏*s) < ε for any S ∈ [S]. See Theorem B.4 in §B.1 of the appendix
for details. By Theorem 4.1, Algorithm 1 converges linearly to the unique Nash equilibrium pair
(μ*, ∏*) of Problem 2.3. To the best of our knowledge, this theorem is the first successful attempt to
establish that reinforcement learning With function approximation finds the Nash equilibrium pairs
in mean-field games with theoretical guarantee, which lays the theoretical foundations for applying
modern reinforcement learning techniques to general mean-field games.
5 Conclusion
For the discrete-time linear-quadratic mean-field games, we provide sufficient conditions for the
existence and uniqueness of the Nash equilibrium pair. Moreover, we propose the mean-field actor-
critic algorithm with linear function approximation that is shown converges to the Nash equilibrium
pair with linear rate of convergence. Our algorithm can be modified to use other parametrized
function classes, including deep neural networks, for solving mean-field games. For future research,
we aim to extend our algorithm to other variations of mean-field games including risk-sensitive
mean-field games (Saldi et al., 2018a; Tembine et al., 2014), robust mean-field games (Bauso et al.,
2016), and partially observed mean-field games (Saldi et al., 2019).
References
Alizadeh, F., Haeberly, J.-P. A. and Overton, M. L. (1998). Primal-dual interior-point methods for
semidefinite programming: convergence rates, stability and numerical results. SIAM Journal on
Optimization, 8 746-768.
Anderson, B. D. and Moore, J. B. (2007). Optimal control: linear quadratic methods. Courier
Corporation.
Araki, B., Strang, J., Pohorecky, S., Qiu, C., Naegeli, T. and Rus, D. (2017). Multi-robot path plan-
ning for a swarm of robots that can both fly and drive. In 2017 IEEE International Conference on
Robotics and Automation (ICRA). IEEE.
Ash, C. (2000). Social-self-interest. Annals of public and cooperative economics, 71 261-284.
Axtell, R. L. (2002). Non-cooperative dynamics of multi-agent teams. In Autonomous Agents and
Multiagent Systems.
Bardi, M. (2011). Explicit solutions of some linear-quadratic mean field games. Networks and
heterogeneous media, 7 243-261.
Bardi, M. and Priuli, F. S. (2014). Linear-quadratic n-person and mean-field games with ergodic
cost. SIAM Journal on Control and Optimization, 52 3022-3052.
Bauso, D., Tembine, H. and Bayar, T. (2016). Robust mean field games. Dynamic games and appli-
cations, 6 277-303.
Bensoussan, A., Chau, M., Lai, Y. and Yam, S. C. P. (2017). Linear-quadratic mean field Stack-
elberg games with state and control delays. SIAM Journal on Control and Optimization, 55
2748-2781.
Bensoussan, A., Frehse, J. and Yam, P. (2013). Mean field games and mean field type control theory.
Springer.
Bensoussan, A., Sung, K., Yam, S. C. P. and Yung, S.-P. (2016). Linear-quadratic mean field games.
Journal of Optimization Theory and Applications, 169 496-529.
10
Published as a conference paper at ICLR 2020
Bhandari, J., Russo, D. and Singal, R. (2018). A finite time analysis of temporal difference learning
with linear function approximation. arXiv preprint arXiv:1806.02450.
Bhatnagar, S., Sutton, R. S., Ghavamzadeh, M. and Lee, M. (2009). Natural actor-critic algorithms.
Automatica, 45 2471-2482.
Biswas, A. (2015). Mean field games with ergodic cost for discrete time markov processes. arXiv
preprint arXiv:1510.08968.
Borkar, V. S. and Meyn, S. P. (2000). The ODE method for convergence of stochastic approximation
and reinforcement learning. SIAM Journal on Control and Optimization, 38 447-469.
Bowling, M. (2001). Rational and convergent learning in stochastic games. In International Con-
ference on Artificial Intelligence.
Bowling, M. and Veloso, M. (2000). An analysis of stochastic game theory for multiagent reinforce-
ment learning. Tech. rep., Carnegie Mellon University.
Bradtke, S. J. (1993). Reinforcement learning applied to linear quadratic regulation. In Advances in
Neural Information Processing Systems.
Bradtke, S. J., Ydstie, B. E. and Barto, A. G. (1994). Adaptive linear quadratic control using policy
iteration. In American Control Conference, vol. 3. IEEE.
Briani, A. and Cardaliaguet, P. (2018). Stable solutions in potential mean field game systems. Non-
linear Differential Equations and Applications, 25 1.
Busoniu, L., Babuska, R. and De Schutter, B. (2008). A comprehensive survey of multiagent rein-
forcement learning. IEEE Transactions on Systems, Man, and Cybernetics, Part C: Applications
and Reviews, 38 156-172.
Caines, P. E. and Kizilkale, A. C. (2017). -nash equilibria for partially observed LQG mean field
games with a major player. IEEE Transactions on Automatic Control, 62 3225-3234.
Calderone, D. J. (2017). Models of Competition for Intelligent Transportation Infrastructure: Park-
ing, Ridesharing, and External Factors in Routing Decisions. University of California, Berkeley.
Carmona, R. and Delarue, F. (2013). Probabilistic analysis of mean-field games. SIAM Journal on
Control and Optimization, 51 2705-2734.
Carmona, R. and Delarue, F. (2018). Probabilistic Theory of Mean Field Games with Applications
I-II. Springer.
Casgrain, P., Ning, B. and Jaimungal, S. (2019). Deep Q-learning for Nash equilibria: Nash-DQN.
arXiv preprint arXiv:1904.10554.
Castro, D. D. and Meir, R. (2010). A convergent online single time scale actor critic algorithm.
Journal of Machine Learning Research, 11 367-410.
Conitzer, V. and Sandholm, T. (2007). AWESOME: A general multiagent learning algorithm that
converges in self-play and learns a best response against stationary opponents. Machine Learning,
67 23-43.
de Cote, E. M., Lazaric, A. and Restelli, M. (2006). Learning to cooperate in multi-agent social
dilemmas. In International Conference on Autonomous Agents and Multiagent Systems. ACM.
De Souza, C., Gevers, M. and Goodwin, G. (1986). Riccati equations in optimal filtering of non-
stabilizable systems having singular state transition matrices. IEEE Transactions on Automatic
Control, 31 831-838.
Dean, S., Mania, H., Matni, N., Recht, B. and Tu, S. (2017). On the sample complexity of the linear
quadratic regulator. arXiv preprint arXiv:1710.01688.
Dean, S., Mania, H., Matni, N., Recht, B. and Tu, S. (2018). Regret bounds for robust adaptive
control of the linear quadratic regulator. In Advances in Neural Information Processing Systems.
11
Published as a conference paper at ICLR 2020
Doerr, B., Linares, R., Zhu, P. and Ferrari, S. (2018). Random finite set theory and optimal control
for large spacecraft swarms. arXiv preprint arXiv:1810.00696.
Du, S. S., Chen, J., Li, L., Xiao, L. and Zhou, D. (2017). Stochastic variance reduction methods for
policy evaluation. In Proceedings of the 34th International Conference on Machine Learning-
Volume 70. JMLR. org.
Fang, J. (2014). The LQR controller design of two-wheeled self-balancing robot based on the parti-
cle swarm optimization algorithm. Mathematical Problems in Engineering, 2014.
Fazel, M., Ge, R., Kakade, S. M. and Mesbahi, M. (2018). Global convergence of policy gradient
methods for the linear quadratic regulator. arXiv preprint arXiv:1801.05039.
Ganzfried, S. and Sandholm, T. (2009). Computing equilibria in multiplayer stochastic games of
imperfect information. In Twenty-First International Joint Conference on Artificial Intelligence.
Gomes, D. A., Mohr, J. and Souza, R. R. (2010). Discrete time, finite state space mean field games.
Journal de mathCmatiques Pures et appliquCes, 93 308-328.
Gomes, D. A. et al. (2014). Mean field games models—a brief survey. Dynamic Games and Appli-
cations, 4 110-154.
GUeanL O., Lasry, J.-M. and Lions, P.-L. (2011). Mean field games and applications. In Paris-
Princeton lectures on mathematical finance 2010. Springer, 205-266.
GUo, X., HU, A., XU, R. and Zhang, J. (2019). Learning mean-field games. arXiv PrePrint
arXiv:1901.09585.
Hardt, M., Ma, T. and Recht, B. (2016). Gradient descent learns linear dynamical systems. arXiv
PrePrint arXiv:1609.05191.
Heinrich, J. and Silver, D. (2016). Deep reinforcement learning from self-play in imperfect-
information games. arXiv PrePrint arXiv:1603.01121.
HU, J. and Wellman, M. P. (1998). MUltiagent reinforcement learning: Theoretical framework and
an algorithm. In International Conference on Machine Learning. Morgan KaUfmann PUblishers
Inc.
HU, J. and Wellman, M. P. (2003). Nash Q-learning for general-sUm stochastic games. Journal of
machine learning research, 4 1039-1069.
HUang, J. and HUang, M. (2017). RobUst mean field linear-qUadratic-gaUssian games with Unknown
L2-distUrbance. SIAM Journal on Control and OPtimization, 55 2811-2840.
HUang, J. and Li, N. (2018). Linear-qUadratic mean-field game for stochastic delayed systems.
IEEE Transactions on Automatic Control, 63 2722-2729.
HUang, J., Li, X. and Wang, T. (2016a). Mean-field linear-qUadratic-GaUssian (LQG) games for
stochastic integral systems. IEEE Transactions on Automatic Control, 61 2670-2675.
HUang, J., Wang, S. and WU, Z. (2016b). Backward mean-field linear-qUadratic-GaUssian (LQG)
games: FUll and partial information. IEEE Transactions on Automatic Control, 61 3784-3796.
Huang, M., Caines, P. E. and Malhame, R. P. (2003). Individual and mass behaviour in large PoPu-
lation stochastic wireless power control problems: centralized and nash eqUilibriUm solUtions. In
Conference on Decision and Control. IEEE.
Huang, M., Caines, P. E. and Malhame, R. P. (2007). Large-population cost-coupled LQG problems
with nonuniform agents: individual-mass behavior and decentralized ε-Nash equilibria. IEEE
transactions on automatic control, 52 1560-1571.
Huang, M., Malhame, R. P., Caines, P. E. et al. (2006). Large population stochastic dynamic games:
closed-loop Mckean-Vlasov systems and the Nash certainty equivalence principle. Communica-
tions in Information & Systems, 6 221-252.
12
Published as a conference paper at ICLR 2020
Huang, M. and Zhou, M. (2019). Linear quadratic mean field games: Asymptotic solvability and
relation to the fixed point approach. arXiv preprint arXiv:1903.08776.
Hughes, E., Leibo, J. Z., Phillips, M., Tuyls, K., Duenez-Guzman, E.,	Castafieda, A. G.,
Dunning, I., Zhu, T., McKee, K., Koster, R. et al. (2018). Inequity aversion improves co-
operation in intertemporal social dilemmas. In Advances in Neural Information Processing
Systems.
Jayakumar, S. and Aditya, M. (2019). Reinforcement learning in stationary mean-field games. In
International Conference on Autonomous Agents and Multiagent Systems.
Kakade, S. M. (2002). A natural policy gradient. In Advances in neural information processing
systems.
Konda, V. R. and Tsitsiklis, J. N. (2000). Actor-critic algorithms. In Advances in neural information
processing systems.
Korda, N. and La, P. (2015). On TD(0) with function approximation: Concentration bounds and a
centered variant with exponential convergence. In International Conference on Machine Learn-
ing.
Kushner, H. and Yin, G. G. (2003). Stochastic approximation and recursive algorithms and appli-
cations. Springer Science & Business Media.
Lagoudakis, M. G. and Parr, R. (2002). Value function approximation in zero-sum Markov games.
In Uncertainty in Artificial Intelligence.
Lasry, J.-M. and Lions, P.-L. (2006a). Jeux a champ moyen. I-Ie cas Stationnaire. Comptes Rendus
Mathematique, 343 619-625.
Lasry, J.-M. and Lions, P.-L. (2006b). Jeux a champ moyen. II-horizon fini et contrOle optimal.
Comptes Rendus Mathematique, 343 679-684.
Lasry, J.-M. and Lions, P.-L. (2007). Mean field games. Japanese journal of mathematics, 2 229-
260.
LeCun, Y., Bengio, Y. and Hinton, G. (2015). Deep learning. Nature, 521 436-444.
Leibo, J. Z., Zambaldi, V., Lanctot, M., Marecki, J. and Graepel, T. (2017). Multi-agent reinforce-
ment learning in sequential social dilemmas. In International Conference on Autonomous Agents
and Multiagent Systems. International Foundation for Autonomous Agents and Multiagent Sys-
tems.
Lewis, F. L., Vrabie, D. and Syrmos, V. L. (2012). Optimal control. John Wiley & Sons.
Li, S., Zhang, W. and Zhao, L. (2017). Connections between mean-field game and social welfare
optimization. arXiv preprint arXiv:1703.10211.
Li, T. and Zhang, J.-F. (2008). Asymptotically optimal decentralized control for large population
stochastic multiagent systems. IEEE Transactions on Automatic Control, 53 1643-1660.
Li, Y. (2018). Deep reinforcement learning. arXiv preprint arXiv:1810.06339.
Littman, M. L. (1994). Markov games as a framework for multi-agent reinforcement learning. In
Machine Learning Proceedings 1994. Elsevier, 157-163.
Littman, M. L. (2001). Friend-or-foe Q-learning in general-sum games. In Proceedings of the Eigh-
teenth International Conference on Machine Learning.
Liu, B., Liu, J., Ghavamzadeh, M., Mahadevan, S. and Petrik, M. (2015). Finite-sample analysis of
proximal gradient TD algorithms. In Conference on Uncertainty in Artificial Intelligence.
Maei, H. R. (2018). Convergent actor-critic algorithms under off-policy training and function ap-
proximation. arXiv preprint arXiv:1802.07842.
13
Published as a conference paper at ICLR 2020
Magnus, J. R. (1979). The expectation of products of quadratic forms in normal variables: the
practice. Statistica Neerlandica, 33 131-136.
Magnus, J. R. et al. (1978). The moments of products of quadratic forms in normal variables. Univ.,
Instituut voor Actuariaat en Econometrie.
Malik, D., Pananjady, A., Bhatia, K., Khamaru, K., Bartlett, P. L. and Wainwright, M. J. (2018).
Derivative-free methods for policy optimization: Guarantees for linear quadratic systems. arXiv
preprint arXiv:1812.08305.
Mguni, D., Jennings, J. and de Cote, E. M. (2018). Decentralised learning in systems with many,
many strategic agents. In Thirty-Second AAAI Conference on Artificial Intelligence.
Minciardi, R. and Sacile, R. (2011). Optimal control in a cooperative network of smart power grids.
IEEE Systems Journal, 6 126-133.
Moon, J. and Bayar, T. (2014). Discrete-time LQG mean field games with unreliable CommUnica-
tion. In Conference on Decision and Control. IEEE.
Moon, J. and Basyar, T. (2018). Linear quadratic mean field stackelberg differential games. Automat-
ica, 97 200-213.
MoravClk, M., Schmid, M., Burch, N., Lisy, V., Morrill, D., Bard, N., Davis, T., Waugh, K.,
Johanson, M. and Bowling, M. (2017). Deepstack: Expert-level artificial intelligence in heads-up
no-limit poker. Science, 356 508-513.
Nash, J. (1951). Non-cooperative games. Annals of mathematics 286-295.
OpenAI (2018). Openai five. https://blog.openai.com/openai-five/.
Perolat, J., Piot, B., Geist, M., Scherrer, B. and Pietquin, O. (2016a). Softened approximate policy
iteration for markov games. In International Conference on Machine Learning.
Perolat, J., Piot, B. and Pietquin, O. (2018). Actor-critic fictitious play in simultaneous move multi-
stage games. In International Conference on Artificial Intelligence and Statistics.
Perolat, J., Piot, B., Scherrer, B. and Pietquin, O. (2016b). On the use of non-stationary strategies
for solving two-player zero-sum Markov games. In International Conference on Artificial Intelli-
gence and Statistics.
Perolat, J., Scherrer, B., Piot, B. and Pietquin, O. (2015). Approximate dynamic programming for
two-player zero-sum Markov games. In International Conference on Machine Learning (ICML
2015).
Peters, J. and Schaal, S. (2008). Natural actor-critic. Neurocomputing, 71 1180-1190.
Rudelson, M., Vershynin, R. et al. (2013). Hanson-wright inequality and sub-Gaussian concentra-
tion. Electronic Communications in Probability, 18.
Saldi, N., Basar, T. and Raginsky, M. (2018a). Discrete-time risk-sensitive mean-field games. arXiv
preprint arXiv:1808.03929.
Saldi, N., Basar, T. and Raginsky, M. (2018b). Markov-Nash equilibria in mean-field games with
discounted cost. SIAM Journal on Control and Optimization, 56 4256-4287.
Saldi, N., Basar, T. and Raginsky, M. (2019). Approximate Nash equilibria in partially observed
stochastic games with mean-field interactions. Mathematics of Operations Research.
Sandholm, W. H. (2010). Population Games and Evolutionary Dynamics. MIT Press.
Shalev-Shwartz, S., Shammah, S. and Shashua, A. (2016). Safe, multi-agent, reinforcement learning
for autonomous driving. arXiv preprint arXiv:1610.03295.
Shoham, Y., Powers, R. and Grenager, T. (2003). Multi-agent reinforcement learning: a critical
survey.
14
Published as a conference paper at ICLR 2020
Shoham, Y., Powers, R. and Grenager, T. (2007). If multi-agent learning is the answer, what is the
question? Artificial Intelligence, 171 365-377.
Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser, J.,
Antonoglou, I., Panneershelvam, V., Lanctot, M. et al. (2016). Mastering the game of Go with
deep neural networks and tree search. Nature, 529 484-489.
Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T.,
Baker, L., Lai, M., Bolton, A. et al. (2017). Mastering the game of Go without human knowl-
edge. Nature, 550 354.
Sutton, R. S. and Barto, A. G. (2018). Reinforcement learning: An introduction. MIT Press.
Sutton, R. S., Maei, H. R., Precup, D., Bhatnagar, S., Silver, D., Szepesvari, C. and Wiewiora, E.
(2009a). Fast gradient-descent methods for temporal-difference learning with linear function ap-
proximation. In Proceedings of the 26th Annual International Conference on Machine Learning.
ACM.
Sutton, R. S., Maei, H. R. and Szepesvari, C.(2009b). A convergent o(n) temporal-difference algo-
rithm for off-policy learning with linear function approximation. In Advances in neural informa-
tion processing systems.
Sutton, R. S., McAllester, D. A., Singh, S. P. and Mansour, Y. (2000). Policy gradient methods for
reinforcement learning with function approximation. In Advances in neural information process-
ing systems.
Sznitman, A.-S. (1991). Topics in propagation of chaos. In Ecole d,ete de Probabilites de Saint-
Flour XIX—1989. Springer, 165-251.
Tembine, H. and Huang, M. (2011). Mean field difference games: Mckean-Vlasov dynamics. In
Conference on Decision and Control and EuroPean Control Conference. IEEE.
Tembine, H., Zhu, Q. and Bayar, T. (2014). Risk-sensitive mean-field games. IEEE Transactions on
Automatic Control, 59 835-850.
Tu, S. and Recht, B. (2017). Least-squares temporal difference learning for the linear quadratic
regulator. arXiv PrePrint arXiv:1712.08642.
Tu, S. and Recht, B. (2018). The gap between model-based and model-free methods on the linear
quadratic regulator: An asymptotic viewpoint. arXiv PrePrint arXiv:1812.03565.
Vinyals, O., Babuschkin, I., Chung, J., Mathieu, M., Jaderberg, M., Czarnecki, W., Dudzik, A.,
Huang, A., Georgiev, P., Powell, R. et al. (2019). Alphastar: Mastering the real-time strategy
game starcraft ii.
Wai, H.-T., Yang, Z., Wang, P. Z. and Hong, M. (2018). Multi-agent reinforcement learning via dou-
ble averaging primal-dual optimization. In Advances in Neural Information Processing Systems.
Wang, B.-C. and Zhang, J.-F. (2012). Mean field games for large-population multiagent systems
with Markov jump parameters. SIAM Journal on Control and OPtimization, 50 2308-2334.
Wang, J., Zhang, W., Yuan, S. et al. (2017a). Display advertising with real-time bidding (RTB) and
behavioural targeting. Foundations and TrendsR in Information Retrieval, 11 297-435.
Wang, Y., Chen, W., Liu, Y., Ma, Z.-M. and Liu, T.-Y. (2017b). Finite sample analysis of the GTD
policy evaluation algorithms in Markov setting. In Advances in Neural Information Processing
Systems.
Watkins, C. J. and Dayan, P. (1992). Q-learning. Machine learning, 8 279-292.
Wei, C.-Y., Hong, Y.-T. and Lu, C.-J. (2017). Online reinforcement learning in stochastic games. In
Advances in Neural Information Processing Systems.
Yang, E. and Gu, D. (2004). Multiagent reinforcement learning for multi-robot systems: A survey.
ManuscriPt.
15
Published as a conference paper at ICLR 2020
Yang, J., Ye, X., Trivedi, R., Xu, H. and Zha, H. (2018a). Deep mean field games for learning opti-
mal behavior policy of large populations. In International Conference on Learning Representa-
tions.
Yang, Y., Luo, R., Li, M., Zhou, M., Zhang, W. and Wang, J. (2018b). Mean field multi-agent rein-
forcement learning. arXiv preprint arXiv:1802.05438.
Yang, Z., Chen, Y., Hong, M. and Wang, Z. (2019). On the global convergence of actor-critic: A
case for linear quadratic regulator with ergodic cost. arXiv preprint arXiv:1907.06246.
Yu, H. (2017). On convergence of some gradient-based temporal-differences algorithms for off-
policy learning. arXiv preprint arXiv:1712.09652.
Zhang, K., Yang, Z., Liu, H., Zhang, T. and Bayar, T. (2018). Finite-sample analyses for fully de-
centralized multi-agent reinforcement learning. arXiv preprint arXiv:1812.02783.
Zhou, X. Y. and Li, D. (2000). Continuous-time mean-variance portfolio selection: A stochastic LQ
framework. Applied Mathematics and Optimization, 42 19-33.
Ziebart, B. D., Maas, A. L., Bagnell, J. A. and Dey, A. K. (2008). Maximum entropy inverse rein-
forcement learning. In AAAI Conference on Artificial Intelligence, vol. 3.
Zou, S., Xu, T. and Liang, Y. (2019). Finite-sample analysis for SARSA and Q-learning with linear
function approximation. arXiv preprint arXiv:1902.02234.
16
Published as a conference paper at ICLR 2020
A Notations in the Appendix
In the proof, for convenience, for any invertible matrix M, we denote by M-> = (M-1)> =
(M>)-1 and kMkF the Frobenius norm. We also denote by svec(M) the symmetric vectoriza-
tion of the symmetric matrix M, which is the vectorization of the upper triangular matrix of the
symmetric matrix M, with off-diagonal entries scaled by √2. We denote by smat(∙) the inverse
operation. For any matrices G and H, We denote by G 0 H the Kronecker product, and G Zs H
the symmetric Kronecker product, which is defined as a mapping on a vector svec(M) such that
(G 0s H)svec(M) = 1/2 ∙ svec(HMG> + GMH>).
For notational simplicity, we write En(∙) to emphasize that the expectation is taken following the
policy π .
B	Auxiliary Algorithms and Analysis
B.1	Results in D-LQR
In this section, we provide auxiliary results in analyzing Problem 2.2. First, we introduce the value
functions of the Markov decision process (MDP) induced by Problem 2.2. We define the state- and
action-value functions VK,b(x) and QK,b(x, u) as follows
∞
VK,b(x) =X Ec(xt, ut) | x0 = x - J(K, b) ,	(B.1)
t=0
QK,b(x, u) = c(x, u) - J(K,b) +E VK,b(x1) | x0 = x,u0 = u ,	(B.2)
where xt follows the state transition, and ut follows the policy πK,b given xt. In other words, we
have Ut = -Kxt + b + σ%, where ηt 〜N(0, I). The following proposition establishes the close
forms of these value functions.
Proposition B.1. The state-value function VK,b(x) takes the form of
VK,b(x) = x>Pκx - tr(PκΦk) + 2fK,b(x - μκ,b) - μK,bPκμκ,b,	(B.3)
and the action-value function QK,b (x, u) takes the form of
Qκ,b(x,u) = (U)> γκ (x) + 2 (pK,b)> (x) - tr(Pκφκ )-σ2 ∙ tr(R + pκBB >)
-b>Rb + 2b>RKμκ,b- μκ,b(Q + K>RK + Pκ)μκ,b
+ 2fκ,b[(Aμ + d) - μκ,b] +(Aμ + d)>pκ(Aμ + d),	(B.4)
where fκ,b = (I - A + BK)->[(A - BK)>Pκ(Bb + Aμ + d) - K>Rb], and YK, pκ,b, and
qκ,b are defined in (3.7).
Proof. See §E.6 for a detailed proof.	□
By Proposition B.1, we know that Vκ,b(x) is quadratic in x, while Qκ,b(x, u) is quadratic in
(x> , u> )> . Now, we show that (3.5) holds.
Proposition B.2. The expected total cost J(K, b) defined in Problem 2.2 takes the form of
J(K, b) = JI(K) + J2(K, b) + σ2 ∙ tr(R) + μ>Qμ,
where
J1(K) = tr(Q + K>RK)Φκ] =tr(PκΨ),
J(K,b)=(μκ，b)> (Q+-KKRK -KRR (μκ，b).
Here μκ,b is defined in (3.2), Φκ is defined in (3.3), and PK is defined in (3.4).
17
Published as a conference paper at ICLR 2020
Proof. See §E.3 for a detailed proof.	□
The following proposition establishes the gradients of J1(K) and J2(K, b), respectively.
Proposition B.3. The gradient of J1 (K) and the gradient of J2 (K, b) with respect to b take the
forms of
VK Jι(K) = 2(CYKK - YK) ∙ Φk,	VbJ2(K, b) = 2[Υ%(-Kμκ,b + b) + ΥKμκ,b + qκ,b],
where ΥK and qK,b are defined in (3.7).
Proof. See §E.5 for a detailed proof.	□
The following theorem establishes the convergence of Algorithm 2.
Theorem B.4 (Convergence of Algorithm 2). Assume that ρ(A - BK0) < 1. Let ε > 0 be a
sufficiently small tolerance. We set
Y ≤ [kR∣∣2 + kBk2 ∙ J(K0,b0) ∙σ-i1n(Ψe)]T,
N ≥ C ∙ kΦκ* k2 ∙ γ-1 ∙ log{4[J(Ko, bo) - J(K*, b*)] ∙ ε-1},
Tn ≥ poly(kKnkF,	kb0k2,	kμk2,	J(K0, b0))	∙	λK4n	∙	[1 -	P(A - BKn)]	∙ ε-5 ,
Tn ≥ poly(kKn∣∣F,	kb0k2,	kμk2,	J(Ko, bo))	∙	λ/	∙	[1 -	P(A - BKn)]-12 ∙ ε-12,
Yn,t = Y0 "-V2,
Y Y ≤ min{1 - ρ(A - BKN), [1 - ρ(A - BKN)] —2 ∙ (|出||2』Kn k2 ∙ ∣R∣2 + ∣∣B∣∣2 ∙∣∣Qk2)},
H ≥ Co ∙ VKN ∙ (Yb)-1 ∙ log{4[J(Kn, bo) - J(KN,bKN)] ∙ ε-1},
Th ≥ poly(kKNkF,	kbhk2,	kμk2, J(KN, bo)) ∙ λκN	∙ ν-N	∙	[1 -	P(A - BKN)]	11 ∙ ε-5,
Th ≥ poly(kKNkF,	kbhk2,	kμk2, J(KN, bo)) ∙ λκN	∙ ν-N	∙	[1 -P(A - bkN)]	17 ∙ ε-8,
Yb,t = Yo ∙t-1∕2,
where C, Co, and Yo are positive absolute constants, {Kn}n∈[N] and {bh}h∈[H] are the sequences
generated by Algorithm 2, λKn is specified in Proposition B.6, and νKN is specified in Proposition
3.3. Then it holds with probability at least 1 - ε1o that
J(Kn, bH) - J(K*, b*) < ε,	IlbH - b*∣∣2 ≤ Mγ(μ) ∙ ε1/2,
kKN - K*kF ≤ [σm1n(We) ∙ σm1n(R) ∙ ε]"2,	IIbKN,Yh - MK*,b*k2 ≤ ε,
where Mγ(μ) is defined in (4.3).
Proof. See §D.2 for a detailed proof.	□
By Theorem B.4, given any mean-field state μ, Algorithm 2 converges linearly to the optimal policy
π* of Problem 2.2.
μ
B.2 Primal-Dual Policy Evaluation Algorithm
Note that the critic update steps in Algorithm 2 are built upon the estimators of the matrix ΥK and
the vector qK,Y. We now derive a policy evaluation algorithm to establish the estimators of ΥK and
qK,Y, which is based on gradient temporal difference algorithm (Sutton et al., 2009a).
We define the feature vector as
(	2(x,u)	ʌ
ψ(x,u)=(	X - μκ,b	I ,	(B.5)
∖u - (-Kμκ,b + b)/
18
Published as a conference paper at ICLR 2020
where
U Svec (U -KKbb + b)
X - μκ,b )>
(-Kμκ,b + b) J
Recall svec(M) gives the symmetric vectorization of the symmetric matrix M. We also define
/	Svec(YK)	ʌ
αK,b=IYK Us+b)+(PK,b))	(B.6)
where ΥK, pK,b, and qK,b are defined in (3.7). To eStimate ΥK and qK,b, it SufficeS to eStimate
αK,b. Meanwhile, we define
ΘK,b = EπK,b ψ(x, u)ψ(x, u) - ψ(x0, u0)> ,	(B.7)
where (x0, u0) iS the State-action pair after (x, u) following the policy πK,b and the State tranSition.
The following propoSition characterizeS the connection between ΘK,b and αK,b.
Proposition B.5. It holdS that
1	0 ) J (K, b)) =	J(K,b)	)
EπK,b ψ(x, u)	ΘK,b	αK,b	EπK,b c(x, u)ψ(x, u)	,
where ψ(x, u) iS defined in (B.5), αK,b iS defined in (B.6), and ΘK,b iS defined in (B.7).
Proof. See §E.7 for a detailed proof.
□
By PropoSition B.5, to obtain αK,b, it SufficeS to Solve the following linear SyStem in ζ = (ζ1, ζ2>)>,
θK,b ∙ ζ
J(K,b)
EπK,b c(x, u)ψ(x, u)
(B.8)
where for notational convenience, we define
ΘK,b =	EπK,b ψ(x,u)	ΘK,b
(B.9)
The following propoSition ShowS that ΘK,b iS invertible.
Proposition B.6. If ρ(A - BK) < 1, then the matrix ΘK,b iS invertible, and kΘK,bk2 ≤ 4(1 +
∣∣KkF)2 ∙ kΦκk2. Also, σmin(ΘK,b) ≥ λκ, where λκ only depends on ∣∣K∣∣2 and P(A - BK).
Proof. See §E.8 for a detailed proof.
□
By Proposition B.6, ΘK,b is invertible. Therefore, (B.8) admits the unique solution ζK,b
(J(K,b),α>K,b)>.
Now, we present the primal-dual gradient temporal difference algorithm.
Primal-Dual Gradient Method. Instead of solving (B.8) directly, we minimize the following loss
function with respect to ζ = ((ζ1)>, (ζ2 )>),
ζ1 - J(K, b)2 + EπK,b ψ (x, u)ζ1 + ΘK,bζ2 - EπK,b c(x, u)ψ(x, u)	.	(B.10)
By Fenchel’s duality, the minimization of (B.10) is equivalent to the following primal-dual min-max
problem,
minmaxF(ζ,ξ) = nEπK,b ψ(x, u)ζ1 + ΘK,bζ2 - EπK,b c(x,u)ψ(x,u)o ξ2	(B.11)
+ [ζ 1 - J(K,b)] ∙ξ1-kξ∣2∕2,
19
Published as a conference paper at ICLR 2020
where we restrict the primal variable ζ in a compact set Vζ and the dual variable ξ in a compact set
Vξ, which are specified in Definition B.7. It holds that
NQ F = ξ1 + E∏κ,b [ψ(x,u)]>ξ2,	RZF = θK,bξ2,	Vξi F = Z1 - J(K,b)-ξ1,
Vξ2F = E∏κ,b [ψ(x,u)]Z1 + Θκ,bZ2 一 E∏κ,b [c(x, u)ψ(x,u)] 一 ξ2.	(B.12)
The primal-dual gradient method updates ζ and ξ via
Z 1 J Z 1 一 Y WOF(Z, ξ),	Z2 J Z2 一 Y ∙ VF(Z, ξ)
ξ1 J ξ1 — Y ∙ VξiF(Z, ξ),	ξ2 J ξ2 — γ ∙Vξ2F(Z, ξ).	(B.13)
Estimation of Mean-Field State μκ,b. To utilize the primal-dual gradient method in (B.13), it
remains to evaluate the feature vector ψ(x, u). Note that by (B.5), the evaluation of the feature
vector ψ(χ, U) requires the mean-field state μκ,b. In What follows, We establish the estimator μκ,b
of the mean-field state μK,b by simulating the MDP folloWing the policy πK,b for T steps, and
calculate the estimated feature vector ψ(x, u) by
ʌ	(	b(Xlu)
ψ(x, u) = x 一 μbK,b
∖u 一 (-Kμκ,b + b)
(B.14)
where b(x, u) takes the form of
b(x, U) = Svec "(u - (:谥鼠 + b)) (u 一 (一谥鼠 + b))
We now define the sets Vζ and Vξ in (B.11).
Definition B.7. Given K0 and b0 such that ρ(A 一 BK0) < 1 and J(K0, b0) < ∞, we define the
sets Vζ and Vξ as
VZ = {Z: 0 ≤ Z 1 ≤ J(Kο, bο), kZ2k2 ≤ Mζ,1 + Mζ,2 ∙ (1 + 1阳周∙ [1 一 P(A - BK)] -1},
Vξ = {ξ: ∣ξ1l ≤ J(K0,b0), kξ2k2 ≤ Mξ ∙ (1 + kKk2)3 ∙ [-P(A — BK)]-1}∙
Here Mζ,1, Mζ,2, and Mξ are constants independent of K and b, which take the forms of
Mζ,1= h(kQkF + kRkF) + (kAkF + kBkF) ∙ √d ∙ J(Kο, bο) ・。^^网)]
+ (kAk2 + kBk2) ∙ J (Kθ, bθ)2 ∙ σmi1n(Ψω ) ∙ σmi1n (Q),
+ h(kQk2 + kRk2) + (kAk2 + kBk2)2 ∙ J(Kο, bο) ∙ σmi1n(Ψω)i
• J(Kο,bο) ∙ [σmi1n(Q) + σ-1n(R)]
Mζ,2 = (kAk2 + kBk2) • (KQ + kr),	Mξ = C • (Mζ,1 + M4,2) • J(Kο, bο)2 • σ1-2n(Q),
where C is a positive absolute constant, and κQ and κR are condition numbers of Q and R, respec-
tively.
We summarize the primal-dual gradient temporal difference algorithm in Algorithm 3. Hereafter,
for notational convenience, we denote by ψt the estimated feature vector ψ(xt, ut).
We now characterize the rate of convergence of Algorithm 3.
Theorem B.8 (Convergence of Algorithm 3). Given K0, b0, K, and b such that ρ(A 一 BK0) < 1
and J(K, b) ≤ J(K0, b0), we define the sets Vζ and Vξ through Definition B.7. Let Yt =
Y0t-1/2, where Y0 is a positive absolute constant. Let ρ ∈ (ρ(A 一 BK), 1). For T ≥
poly0(kKkF, kbk2, kμk2, J(K0, b0)) • (1 一 ρ)-6 and a sufficiently large T, it holds with probability
at least 1 一 T-4 一 Te-6 that
kbκ,b — ακ,bk2 ≤ λκ2 • poly1(kKI∣f, kbk2, kμk2, j(Kο,bο)) •
log6 T + log T
T 1/2 • (1 一 ρ)4 + T1∕4 .(1 一 ρ)2
20
Published as a conference paper at ICLR 2020
Algorithm 3 Primal-Dual Gradient Temporal Difference Algorithm.
1:	Input: Policy ∏κ,b, mean-field state μ, numbers of iteration T and T, stepsizes {γt}t∈[τ],
parameters K0 and b0 .
2:	Define the sets Vζ and Vξ via Definition B.7 with K0 and b0 .
3:	Initialize the parameters by ζ0 ∈ Vζ and ξ0 ∈ Vξ .
4:	Sample eo from the the stationary distribution N(μκ,b, Φk).
5:	for t = 0, . . . , Te - 1 do
6:	Given the mean-field state μ, take action et following ∏κ,b and generate the next state et+ι.
7:	end for
„	,._.τ, _	_	_	_	^
8:	Set μκ,b J 1/T ∙ Et=I Xt and compute the estimated feature vector ψ Via (B.14).
9:	Sample xo from the the stationary distribution N(μκ,b, Φk).
10:	for t = 0, . . . , T - 1 do
11:	Given the mean-field state μ, take action Ut following ∏κ,b, observe the cost q, and generate
the next state xt+1.
12:	Set δt+1	J	ζt	+	(ψbt -	ψbt+1)	ζt	-	ct.
13:	Update parameters via
ζ1+1 J	ζ1 - γt+1 ∙ (ξt	+ ψ> ξ2),	ζ2+1 J ζ2	- γt+1 ∙ ψt(ψt - ψt+1)> ξ2,
ξ1+ι J	(I — γt+1) ∙ ξ1	+ Yt+1 ∙ (ZI	- Ct),	ξ2+ι J (I	— Yt+1) ∙ ξ2 + Yt+1 ∙ δt+ι ∙ ψt.
14:	Project ζt+1 and ξt+1 to Vζ and Vξ , respectively.
15:	end for
16:	Set ακ,b J (PT=I γt)-1∙ (PT=I Yt ∙ ζ2), and
YKJSmat(bK,b,1),	(PK；) J bK,b,2 - YK Qk黑;+ b)，
(k+d+1)(k+d)/2	(k+d+3)(k+d)/2
where αK,b,1 = (αK,b)l	and αK,b,2 = (αK,b)(k+d+ι)(k+d)∕2 + ι∙
17:	Output: Estimators μK,b, YK, and qKb.
where λK is defined in Proposition B.6. Same bounds for kYb K — YKkF2, kpbK,b — pK,bk22, and
kqbK,b — qK,b k22 hold. Meanwhile, it holds with probability at least 1 — Te-6 that

kμK,b - μK,b∣∣2
log T
≤ ~τz:-----
Te1/4
, (I-P)-2 ∙poly2(llφK ∣∣2, IIK kF, ∣∣b∣2, ∣∣μ∣∣2, J (κ0,b0)).
Proof. See §D.3 for a detailed proof.
□
B.3 Temporal Difference Policy Evaluation Algorithm
Besides the primal-dual gradient temporal difference algorithm, we can also evaluate αK,b by TD(0)
method (Sutton and Barto, 2018) in practice, which is presented in Algorithm 4.
Note that in related literature (Bhandari et al., 2018; Korda and La, 2015), non-asymptotic conver-
gence analysis of TD(0) method with linear function approximation is only applied to discounted
MDP. As for our ergodic setting, the convergence of TD(0) method is only shown asymptotically
(Borkar and Meyn, 2000; Kushner and Yin, 2003) using ordinary differential equation method.
Therefore, in the convergence theorem proposed in §3, we only focus on the primal-dual gradient
temporal difference method (Algorithm 3) to establish non-asymptotic convergence result.
C General Formulation
Compared with Problem 2.3, a more general formulation includes an additional term x>Pμ in the
cost function. For the completeness of this paper, we define this general formulation here. Following
from a same argument as in §2, it suffices to study the setting where t is sufficiently large. First, we
propose the following general drifted LQR (general D-LQR) problem, which is parallel to Problem
2.2.
21
Published as a conference paper at ICLR 2020
Algorithm 4 Temporal Difference Policy Evaluation Algorithm.
1:	Input: Policy πK,b, number of iteration T and T, stepsizes {γt}t∈[T] .
2:	Sample eo from the stationary distribution N(μκ,b, Φk).
3:	for t = 0, . . . , T - 1 do
4:	Take action uet under the policy πK,b and generate the next state xet+1.
5:	end for
_	, r≤ _rr
6:	Set μκ,b J 1/Te ∙ Pt=ι xt.
7:	Sample x0 from the the stationary distribution N(μK,b, ΦK).
8:	for t = 0, . . . , T do
9:	Given the mean-field state μ, take action ut following πK,b, observe the cost ct, and generate
the next state xt+1 .
10:	Set δt+1 J ζt + (ψbt - ψbt+1 ) ζt - ct.
11:	Update parameters via《吊一(1 — γt+ι)	∙	Zt	+ γt+ι	∙ Ct	and	Zt+ι	—	Z2	—	Yt+ι ∙	δt+ι	∙ ψt.
12:	Project ζt to Vζ0 , where Vζ0 is a compact set.
13:	end for
14:	Set bbK,b — (PT=I Yt)j (PT=I Yt ∙ Z2), and
YK Jsmat(&仆,1)，	(PK,；) J bK,b,2 - YK ]—尴；+ b
(k+d+1)(k+d)/2	,	(k+d+3)(k+d)/2,
where αK,b,1 = (αK,b)1	and αK,b,2 = Sκ,b)(k+d+1)(k+d)∕2 + 1∙
15:	Output: Estimators μbK,b, YK, and qbK,b. * **
Problem C.1 (General D-LQR). For any given mean-field state μ ∈ Rm , consider the following
formulation
xt+1 = Axt + But + Aμ + d + ωt,
cμ(xt, ut) = x>Qxt + u>Rut + μ>Qμ + 2x>Pμ,
1T
Jμ(π) = Iim E - Ee”(xt,ut),
T →∞ T
t=0
where xt ∈ Rm is the state vector, ut ∈ Rk is the action vector generated by the policy π, ωt ∈ Rm
is an independent random noise term following the Gaussian distribution N(0, Ψω), and d ∈ Rm is
a drift term. We aim to find an optimal policy ∏μ such that Jμ(∏μ) = inf∏∈∏ Jμ(∏).
In Problem C.1, the unique optimal policy π* (∙) still admits a linear form ∏μ(xt) = -K∏* xt + b∏*
(Anderson and Moore, 2007), where the matrix K∏* ∈ Rk×m and the vector b∏* ∈ Rk are the
**
parameters of the policy π. It then suffices to find the optimal policy in the class Π defined in (2.1).
Parallel to Problem 2.3, we define the general LQ-MFG problem as follows.
Problem C.2 (General LQ-MFG). We consider the following formulation
xt+1 = Axt + But + Aμ + d + ωt,
e(xt, ut) = x>Qxt + u>Rut + μ>Qμ + 2x>Pμ,
T
= . _
J(π, μ) = lim E
T→∞
1T
TEe(Xt,ut),
t=0
where xt ∈ Rm is the state vector, ut ∈ Rk is the action vector generated by the policy π , μ ∈ Rm
is the mean-field state, ωt ∈ Rm is an independent random noise term following the Gaussian
distribution N(0, Ψω), and d ∈ Rm is a drift term. We aim to find a pair (μ*,π*) such that (i)
J(π*,μ*) = inf∏∈∏ J(π, μ*); (ii) E球 converges to μ* as t → ∞, where {球}t≥o is the Markov
chain of states generated by the policy ∏*.
One can see that Problem C.2 aims to find a Nash equilibrium pair (μ*,∏*).
22
Published as a conference paper at ICLR 2020
Similar to the discussions in §3.2, to solve Problem C.2, one can design an algorithm similar to
Algorithm 1, which solves Problem C.1 and obtain the new mean-field state at each iteration. We
omit the detailed algorithm here. Now, we focus on solving Problem C.1 in the sequel.
Similar to §3.3, We drop the subscript μ when We focus on Problem C.1 for a fixed μ. We write
∏κ,b(x) = -Kx + b + σ ∙ η to emphasize the dependence on K and b, and J(K, b) = J(∏κ,b)
consequently. We derive an explicit form of the expected total cost J(K, b) in the following propo-
sition.
Proposition C.3. The expected total cost Je(K, b) in Problem C.1 is decomposed as
J(K, b) = Ji(K) + Je2(K, b)+ σ2 ∙ tr(R) + μ>Qμ,
where J1(K) and J2(K, b) take the forms of
Je1(K) =tr(Q+K>RK)ΦK =tr(PKΨ),
J2(K,b) =CK，b)> (Q +-RKRK -KRR CK，b) +2μ>Pμκ,b.
Here μκ,b is given in (3.2), Φk is given in (3.3), and PK is given in (3.4).
Proof. The proof is similar to the one of Proposition B.2. Thus we omit it here.	□
Compared with the form of J(K, b) given in (3.5), we see that the only difference is that J(K, b)
contains an extra term 2μ>Pμκ,b in J (K, b), which is only a linear term in b (recall that μκ,b is
linear in b by (3.2)). Thus, J2(K, b) is still strongly convex in b, as shown in the proposition below.
Proposition C.4. Given any K, the function J2 (K, b) is νK -strongly convex in b, here νK =
σmin(Y1>,KY1,K + Y2>,KY2,K), where Y1,K = R1/2K(I - A+BK)-1B - R1/2 and Y2,K =
Q1/2 (I - A + BK)-1B. Also, Je2 (K, b) has ιK -Lipschitz continuous gradient in b, where ιK is
upper bounded such that ∣k ≤ [1 - P(A - BK)]-2 ∙ (|田般∙ ∣∣Kk2 ∙ ∣∣R∣∣* + ∣∣Bk! ∙ kQk*)∙
Proof. The proof is similar to the one of Proposition 3.3. Thus we omit it here.
□
Parallel to Proposition 3.4, we derive a similar proposition in the sequel.
KK
Proposition C.5. Denote by bK = argminb J2(K, b), then J2(K, bK) takes the form
J2(K,bK)=(*>+/)	(QT(I -A)>S
S(I - A)Q-1
3Q-1 (I - A)>S(I - A)Q-1
-Q
which is independent of K. Here S = [(I - A)Q-1(I - A)> + BR-1B>]-1. And ebK takes the
form
bK = [KQ-1(I - A)> - R-1B>] ∙ S ∙ [(Aμ + d) + (I - A)Q-1P>μ] - KQTP>μ.
Proof. The proof is similar to the one of Proposition 3.4. Thus we omit it here.
□
Similar to Problem 2.2, we define the state- and action-value functions as
∞
VeK,b(x) = X E[ec(xt, ut) |
t=0
x0
x, ut
-Kxt + b + σηt] - Je(K, b) ,
0
QK,b(x, u) = ce(x, u) - J(K, b) + E VK,b(x0) | x, u ,
where the x0 is the state generated by the state transition after the state-action pair (x, u). A slight
modification of Proposition B.1 gives the proposition below.
23
Published as a conference paper at ICLR 2020
Proposition C.6. For Problem C.1, the state-value function VK,b (x) takes the form
Veκ,b(χ) = χ>PKX - tr(PκΦk) + 2fK,b(χ - μκ,b) - (μκ,b)>Pκμκ,b,
and the action-value function QK,b(x, u) takes the form
>>
Qκ,b(x,u)=	U	YK U +2 PK：	U	Tr(PKφκ )-σ2 ∙ tr(R + pκ BBD- b>Rb
+ 2b>RKμκ,b - (μκ,b)>(Q + K>RK + pκ)μκ,b + 2fK,b[(Aμ + d) - μκ,b]
+ (Aμ + d)，PK (Aμ + d) — 2μ^∣^P μκ,b.
Here the matrix Υκ is given in (3.7), and the vectors peκ,b, qeκ,b are given as
pκ,b∖ = (A> [PK ∙(Aμ + d) + fκ,b] + Pμ
eκ,by	y	B> [Pκ ∙ (Aμ + d) + fκ,b]
(C.1)
where the vector fκ,b = (I - A + BK)->[(A - BK)>Pκ(Bb + Aμ + d) - K>Rb + Pμ].
Proof. The proof is similar to the one of Proposition B.1. Thus we omit it here.
Now we establish the gradients of J(K, b) for Problem C.1.
Proposition C.7. The gradient of J1(K) and the gradient of J2(K, b) w.r.t. b takes the form
VK JL(K) = 2(YKK - YK) ∙ Φκ,	VbJ2(K, b) = 2[Y22(-Kμκ,b + b) + YKμκ,b + eκ,b],
where the matrix Υκ is given in (3.7), and the vector qeκ,b is given in (C.1).
Proof. The proof is similar to the one of Proposition B.3. Thus we omit it here.
Equipped with above results, parallel to the analysis in §3, it is clear that by slight modification
of Algorithms 1, 2, and 3, we derive similar actor-critic algorithms to solve both Problem C.2 and
Problem C.1, where all the non-asymptotic convergence results hold. We omit the algorithms and
the convergence results here.
D	Proofs of Theorems
D.1 Proof of Theorem 4.1
We define μ*+ι = Λ(μs), which is the mean-field state generated by the optimal policy
∏κ*(μs),b*(μs) = Λι(μs) under the current mean-field state μs. By Proposition 3.4, the optimal
K *(μ) is independent of the mean-field state μ. Therefore, We write K * = K *(μ) hereafter for
notational convenience. By (3.2), we know that
μ*+ι = (I — A + BK*) 1 ∙ [Bb*(μs) + Aμs + d]
We define
μs+ι = (I — A + BKS) 1 (BbS + Aμs + d),
which is the mean-field state generated by the policy ∏s under the current mean-field state μs, where
KS and bS are the parameters of the policy πS . By triangle inequality, we have
kμs + 1 - μ*k2 ≤ kμs + 1 - μs+1k2 + kμs+1 -
μ*+1k2 + kμ*+1 - μ*k2,
(D.1)
"{z
E1
{z^
E2
} '-----------------'
where μs+ι is generated by Algorithm 1. We upper bound Ei, E?, and E3 in the sequel.
Upper Bound of E1. By Theorem B.4, it holds with probability at least 1 - ε10 that
EI = kμs+1 - μs + 1k2 < εs ≤ ε/8 ∙ 2 s,
(D.2)
24
Published as a conference paper at ICLR 2020
where εs is given in (4.2).
Upper Bound of E⅛. By the triangle inequality, we have
E? = U (I — A + BKS) 1 (BbS + Aμ + d) — (I — A + BK*) 1 ∙ [Bb* (μs) + Aμ + d] ∣∣^
≤ ∣∣Bb*(μs) + Aμs + d∣∣2 ∙ ∣∣[I — A + BK* + B(Ks — K*)] T — (I — A + BK*)T(
+ ∣∣(I — A + BKS)T∣∣2 ∙∣∣B∣∣2 ∙ ∣∣bs — b*(μs)∣∣2.	(D.3)
By Taylor,s expansion, we have
∣∣[I — A + BK * + B(Ks — K *)]-1 — (I — A + BK *)-1∣∣2
=∣∣(I — A + BK *)-1[I + (I — A + BK *)TB(KS — K* )]-1 — (I — A + BK *)-1∣∣2
≤ 2∣∣(I — A + BK*)-1B(Ks — K*)(I — A + BK*)-1∣∣2.	(D.4)
Meanwhile, by Taylor,s expansion, it holds with probability at least 1 — ε10 that
∣∣(I — a + BKs)-1∣∣2 = U(I — A + BK * + B(Ks — K *))-1∣∣2
=∣∣(I — A + BK *)T(I +(I — A + BK *)-1B(Ks — K *))-1∣∣2
≤ [1 — P(A — BK*)]-1 ∙ (1 + ∣∣(I — A + BK*)-1B∣∣2 ∙ ∣∣K* — Ks∣∣2)
≤ 2[1 — P(A — BK*)]-2,	(D.5)
where the last inequality comes from Theorem B.4. By plugging (D.4) and (D.5) in (D.3), it holds
with probability at least 1 — ε10 that
E2 ≤ 2 ∣ ∣Bb*(μs) + Aμs + d 11 2 ∙ ∣∣(I — A + BK *)-1 B(Ks — K *)(I — A + BK * )-1∣∣2
+ ∣∣(I — A + BKs)-1∣∣2 ∙∣∣b∣∣2 ∙ ∣∣bs — b*(μs)∣∣2
≤ 2 ∣ ∣Bb*(μs)+ Aμs + d 11 2 ∙ [1 — P(A — BK*)]-2 ∙∣∣B∣∣2 ∙ |照—K*心	(D.6)
+ 2[1 — ρ(A — BK*)]-2 ∙∣∣B∣∣2 ∙ ∣∣bs — b*(μs)∣∣2∙
By Proposition 3.4, it holds that
IlBb*(μS) + Aμs + d∣ ∣ 2 ≤ Li ∙∣lBIl2 Tlμs∣∣2 + IlAIl2 Tlμs∣∣2+ IldIl2
≤ (L1∙IB∣∣2 + ∣∣A∣2) ∙Ms∣∣2 + ∣∣d∣∣2,	(D.7)
where the scalar Li is defined in Assumption 3.1. Meanwhile, by Theorem B.4, it holds with
probability at least 1 — εi0 that
IKs — K*If ≤ [σmi1n(Φe) ∙ σmi1n(R) ∙ £,]1/2,	帆—b*(μ3)∣∣2 ≤ Mbg ∙ ε*, (D.8)
where Mb(μs) is defined in (4.3). Combining (D.6), (D.7), (D.8), and the choice of £§ in (4.2), it
holds with probability at least 1 — εi0 that
E2 ≤ ε∕8 ∙ 2-s.	(D.9)
Upper Bound of E⅛. By Proposition 3.2, we have
E3 = ∣∣μ*+ι — μ*∣∣2 = ∣∣A(μ3) - A(μ*) ∣ ∣ 2 ≤ L0 ∙ kμ3 - μ*∣∣2,	(D.10)
where L° = LiL3 + L2 by Assumption 3.1.
By plugging (D.2), (D.9), and (D.10) in (D.1), we know that
l∣μs+ι — μ*II2 ≤ L0 ∙∣∣μs — μ*II2 + ε ∙ 2-s-2,	(D.U)
25
Published as a conference paper at ICLR 2020
which holds with probability at least 1 - ε10. Following from (D.11) and a union bound argument
with S = O(log(1∕ε)), it holds with probability at least 1 - ε5 that
kμS - μ*k2 ≤ LS ∙kμ0 - μ*∣∣2 + ε/2,
where we use the fact that L0 < 1 by Assumption 3.1. By the choice of S in (4.1), it further holds
with probability at least 1 - ε6 that
kμs - μ*k ≤ ε.	(D.12)
By Theorem B.4 and the choice of εs in (4.2), it holds with probability at least 1 - ε5 that
kKs - K*kF = IlKS - K*(μs)∣∣F ≤ [σ1-1n(Ψe) ∙ σ^(R) ∙ ε]	≤ ε.	(D.13)
Meanwhile, by the triangle inequality and the choice of εs in (4.2), it holds with probability at least
1 - ε5 that
kbs -b*k2 ≤ libs - b(μs)∣∣2 + ∣∣b*(μs)-b*H2
≤ Mb(μs) ∙ εS/2 + Li ∙ kμs - μ* k2
≤ (1 + Li) ∙ ε,	(D.14)
where the second inequality comes from Theorem B.4 and Proposition 3.4, and the last inequality
comes from (D.12). By (D.12), (D.13), and (D.14), we conclude the proof of the theorem.
D.2 Proof of Theorem B.4
Proof. We first show that JI(KN) - Ji(K*) ‹ ε∕2 with a high probability, then show that
J2(Kn, bH) - J2(K*, b*) < ε∕2 with ahigh probability. Then We have
J(Kn, bN) - J(K*, b*) = JI(KN) + J2(KN, bH) - Ji(K*) - J2(K*,b*) < ε
with a high probability, which proves Theorem B.4.
Part 1. We show that Ji(Kn) - Ji(K*) < ε∕2 with a high probability.
We first bound Ji(Ki) - Ji(K2) for any Ki and K2. By Proposition B.2, Ji(K) takes the form of
Ji (K) = tr(Pκ Ψe) = Ey 〜N (0,Ψe)(y>Pκ y).	(D.15)
The following lemma calculates y>PK1y - y>PK2 y for any Ki and K2.
Lemma D.1. Assume that ρ(A - BKi) < 1 and ρ(A - BK2) < 1. For any state vector y, we
denote by {yt}t≥0 the sequence generated by the state transition yt+i = (A - BK2)yt with initial
state y0 = y . It holds that
y>PK2 y - y>PK1 y =	DK1,K2(yt),
t≥0
where
DK1,K2(y) = 2y>(K2 -Ki)(Υ2K21Ki -Υ2Ki1)y+y>(K2-Ki)>Υ2K21(K2 -Ki)y.
Here ΥK is defined in (3.7).
Proof. See §F.1 for a detailed proof.	□
The following lemma shows that Ji(K) is gradient dominant.
Lemma D.2. Let K * be the optimal parameter and K be a parameter such that Ji (K) < ∞, then
it holds that
Ji(K) - Ji(K*) ≤ σ-in(R) ∙ kΦκ* k2 ∙ tr[(ΥK2K - YKy(YK2K - 丫沏，	(D.16)
Ji(K) - Ji(K*) ≥ σmin(Ψω) ∙kYK2k-i ∙ tr[(YK2K - YK)T(YKK - Y沏.	(D.17)
26
Published as a conference paper at ICLR 2020
Proof. See §F.2 for a detailed proof.	□
Recall that from Algorithm 2, the parameter K is updated via
Kn+1= Kn - Y ∙(Υ Kn Kn - Y 鼠)，	(D.18)
where YKn is the output of Algorithm 3. We upper bound | J1(Kn+1) - Jι(K*)| m the sequel.
First, We show that if JI(Kn) — Jι(K*) ≥ ε∕2 holds for any n ≤ N, we obtain that
JI(KN) ≤ JI(KN-I) ≤ ∙∙∙ ≤ JI(KO),	(D.19)
which holds with probability at least 1 - ε13. We prove (D.19) by mathematical induction. Suppose
that
JI(Kn) ≤ JI(Kn-1) ≤ …≤ Jι(Ko),	(D.20)
which holds for n = 0. In what follows, we define Kn+1 as
Kn+1= Kn - Y ∙ (YKn Kn - YKn),	(DZ)
where YKn is given in (3.7). By (D.21), we have
JI(Kn+1)- JI(Kn ) = Ey 〜N (0,Ψe)[y> (PKn+1 - PKn)y]
=-2Y ∙ tr[φKn+1 ∙ (YKn Kn - YKn (YKn Kn- Y^n )]
+ Y2 ∙ tr[φKn+1 ∙ (YKn Kn - YKn『曜n (YKn Kn- YKn )]
≤ -2Y ∙ "[φKn+1 ∙ (YKn Kn - YKn	Kn- Y^n )]
+ Y2 ^ kYKn k2 ∙ "[φKn+1 ∙ (YKn Kn- YKn ^5^ Kn- YKn )],
where the first equality comes from (D.15), the second equality comes from Lemma D.1, and the
last inequality comes from the trace inequality. By the definition of YK in (3.7), we obtain that
kY2K2nk2 ≤ kRk2+kBk22∙kPKnk2 ≤ kRk2 + kBk22 ∙ J1(Kn) ∙ σm-i1n(Ψ)
≤ kRk2 + kBk22 ∙J1(K0)∙σm-i1n(Ψ),	(D.23)
where the second inequality comes from Proposition B.2. By plugging (D.23) and the choice of
stepsize Y ≤ [kRk2 + kBk22 ∙ J1(K0) ∙ σm-i1n(Ψ)]-1 into (D.22), we obtain that
J1(Ken+1)-J1(Kn) ≤ -Y ∙ trΦKen+1 ∙(Y2K2nKn-Y2K1n)>(Y2K2nKn-Y2K1n)]
≤ -Y ∙ σmin(Ψ) ∙tr(Y2K2nKn-Y2K1n)>(Y2K2nKn-Y2K1n)]
≤ -Y ∙ σmin(ψe) ∙ σmin(R) ∙ kφK* k2 1 ∙ [J1(Kn)- JI(K*)] < 0, (D∙24)
where the last inequality comes from Lemma D.2.
The following lemma upper bounds |J1(Kn+1) - J1(Kn+1)|.
Lemma D.3. Assume that J1(Kn) ≤ J1(K0). It holds with probability at least 1 - ε15 that
I J1(Kn+l) - J1(Kn+1)∣ ≤ Y ∙ σmin(Ψe) Pmg(R) ∙ ∣∣Φk* k-1 ∙ ε∕4,
where Kn+1 and Kn+1 are defined in (D.18) and (D.21), respectively.
Proof. See §F.3 for a detailed proof.	□
Combining (D.24) and Lemma D.3, if J1(Kn) - J1 (K*) ≥ ε∕2, it holds with probability at least
1 - ε15 that
J1(Kn+1) -J1(Kn) ≤ J1(Ken+1) -J1(Kn)+ ∣∣J1(Ken+1) -J1(Kn+1)∣∣
≤ -Y ∙ σmin(Ψe) Pmg(R) ∙ ∣∣Φk* k-1 ∙ ε∕4 < 0.	(D.25)
27
Published as a conference paper at ICLR 2020
Combining (D.20) and (D.25), it holds with probability at least 1 - ε15 that
Jl(Kn+l) ≤ JI(Kn) ≤∙∙∙≤ Jl(Kθ).
Finally, following from a union bound argument and the choice of N in Theorem B.4, if J1(Kn) -
Jι(K*) ≥ ε∕2 holds for any n ≤ N, we have
JI(KN) ≤ JI(KN-1) ≤ ∙∙∙ ≤ J1(K0),
which holds with probability at least 1 - ε13. Thus, we complete the proof of (D.19).
Combining (D.24)and (D.25), for JI(Kn) - J1(K*) ≥ ε∕2,we have
JI(Kn+1)- JI(K *) ≤ [1 - Y ∙ σmin (We) ∙ σmin (R) ∙ ∣∣φK* ∣∣-1] ∙ [J1(Kn)- JI(K *)],
which holds with probability at least 1 - ε13 . Meanwhile, following from a union bound argument
and the choice of N in Theorem B.4, it holds with probability at least 1 - ε11 that
Ji(Kn) - J1(K*) ≤ ε∕2.	(D.26)
The following lemma upper bounds ∣∣Kn - K *∣f.
Lemma D.4. For any K, we have
∣K - K*kF ≤ σm1n(Ψe) ∙。—；口(冗)∙ [Jl(K) - Jl(K*)].
Proof. See §F.4 for a detailed proof.	□
Combining (D.26) and Lemma D.4, we have
∣Kn - K*∣F ≤ [σmi1n(Ψe) P-ljR) ∙ ε∕2]1"	(D.27)
which holds with probability 1 - ε11.
Part 2. We show that J?(Kn, bH - J2(K*, b) < ε∕2 with high probability. Following from
Proposition 3.4, it holds that J2(K*,b*) = J2(Kn,bKN). Therefore, it suffices to show that
J2(KN, bH) - J2(KN, bKN) < ε∕2.
First, we show that if J2(KN, bh) - J2(KN, bKN) ≥ ε∕2 for any h ≤ H, we obtain that
Jz(Kn, bH) ≤ J2(Kn, bH-i) ≤ …≤ J(KN bi) ≤ J2(KN bo),	(D.28)
which holds with probability at least 1 - ε13 . We prove (D.28) by mathematical induction. Suppose
that
J2(KN, bh) ≤ J2(KN, bh-1) ≤ ∙∙∙ ≤ J2(KN, b0),	(D.29)
Recall that by Algorithm 2, the parameter b is updated via
bh+i = bh - Yb ∙ VbJ2(KN,bh).	(D.30)
Here
VbJ2(KN, bh) = YYKN(-Kn;⅛N,bh + bh) + YKNμκN,bh + GKN,bh,	(D.31)
where YKN and qbKN,bh are the outputs of Algorithm 3. We define bh+1 as
eh+i = bh - Yb ∙VbJ2(KN,bh).	(D.32)
Here
VbJ2(KN, bh) = YKN(-KnμκN,bh + bh) + YKNμκN,bh + qKN,bh,	(D.33)
where YKN and qKN,bh are defined in (3.7). We upper bound J2(KN, bh+1) - J2(KN, bKN) in the
sequel. Following from (D.32) and Proposition 3.3, we have
J2 (KN, eh+i) - J2 (KN, bh) ≤ -Y /2 ∙ IlVbJ2 (KN, bh)∣∣2
≤ -VKN ∙ Yb ∙ [Jy(,Kn, bh) - Jy(Kn, bκN)]
≤ -VKN ∙ Y ∙ ε< 0,	(D.34)
where νKN is specified in Proposition 3.3. The following lemma upper bounds |J2 (KN, bh+i) -
J2(KN, bh+i)|.
28
Published as a conference paper at ICLR 2020
Lemma D.5. Assume that J2(KN, bh) ≤ J2(KN, b0). It holds with probability at least 1 - ε15 that
I J2(KN, bh+ι) — Jz(Kn, eh+ι)∣ ≤ VKN ∙ Yb ∙ ε∕2,
where bh+1 and ebh+1 are defined in (D.30) and (D.32), respectively.
Proof. See §F.5 for a detailed proof.	□
Combining (D.34) and Lemma D.5, we know that if J2(KN, bh) - J2(KN, bKN) ≥ ε, it holds with
probability at least 1 - ε15 that
J2(KN, bh+1) - J2(KN, bh) ≤ J2(KN, ebh+1) - J2(KN, bh) + IIJ2(KN, bh+1) - J2(KN, ebh+1)II
≤ -VKN ∙ Yb ∙ ε∕2 < 0.	(D.35)
Combining (D.29) and (D.35), it holds with probability at least 1 - ε15 that
Jz(Kn, bh+ι) ≤ Jz(Kn, bh) ≤ ∙∙∙ ≤ Jz(Kn, bo).
Following from a union bound argument and the choice of H in Theorem B.4, if J2 (KN, bh) -
J2(KN, bKN) ≥ ε holds for any h ≤ H, we have
J2(KN, bH) ≤ J2(KN, bH-I) ≤ ∙∙∙ ≤ J2(KN, bO),
which holds with probability at least 1 - ε13. Thus, we finish the proof of (D.28).
Combining (D.34) and Lemma D.5, for J2(KN, bh) - J2(KN, bKN) ≥ ε∕2, we have
J2(KN bh+ι) — J2(KN, bKN) ≤ (1 - VKN ∙ Yb) ∙ [J2(KN, bh) - J2(KN bKN)],
which holds with probability at least 1 - ε13. Meanwhile, following from a union bound argument
and the choice of H in Theorem B.4, it holds with probability at least 1 - ε11 that
J2(KN, bH) - J2 (KN, bKN) ≤ ε∕2.	(D.36)
By Proposition 3.3 and (D.36), it holds with probability at least 1 - ε11 that
kbH — bKNk2 ≤ (2ε∕νκ*)1/2.	(D.37)
Following from Proposition 3.4, we know that
bKN — b* = (KN — K *)QT(I - A)>	(D.38)
• [(I — A)QT(I — A)> + BR-1B>]-1 ∙ (Aμ + d).
Combining (D.27), (D.37), and (D.38), it holds with probability 1 — ε10 that
kbH — bKNk2 ≤ Mb • ε1/2,
where
Mb(μ) = 4∣∣Q-1(I — A)> ∙ [(I — A)QT(I — A)> + BR-1B>]-1 ∙ (Aμ + d)(
• [v-* + σm1n(ψe) • σ-1n (R)] ” ∙
We finish the proof of the theorem.	□
D.3 Proof of Theorem B.8
Proof. We follow the proof of Theorem 4.2 in Yang et al. (2019), where they only consider LQR
without drift terms. Since our proof requires much more delicate analysis, we present it here.
Part 1.	We denote by ζ and ξ the primal and dual variables generated by Algorithm 3. We define
the primal-dual gap of (B.11) as
gap(ζb, ξb) = maxF(ζb,ξ) — minF(ζ,ξb).	(D.39)
ξ∈Vξ	ζ∈Vζ
29
Published as a conference paper at ICLR 2020
In the sequel, we upper bound kαbK,b - αK,bk2 using (D.39).
We define ζK,b and ξ(ζ) as
Zκ,b = (J (K,b),αK,b)>,	ξ(Z) = argmax F (Z,ξ).	(D.40)
Following from (B.12), we know that
ξ1 (ζ) = ζ1 - J(K, b), ξ2(ζ) = EπK,b ψ(x, u)ζ1 + ΘK,bζ2 - EπK,b c(x, u)ψ (x, u). (D.41)
The following lemma shows that ζK,b ∈ Vζ and ξ(ζ) ∈ Vξ for any ζ ∈ Vζ.
Lemma D.6. Under the assumptions in Theorem B.8, it holds that ζK,b = (J (K, b), α>K,b)> ∈ Vζ.
Also, for any ζ ∈ Vζ, the vector ξ(ζ) defined in (D.40) satisfies that ξ(ζ) ∈ Vξ.
Proof. See §F.6 for a detailed proof.
□
By (B.12), We know that NZF(Zκ,b, 0) = 0 and VξF(Zκ,b, 0) = 0. Combining Lemma D.6, it
holds that (ζK,b, 0) is a saddle point of the function F(ζ, ξ) defined in (B.11).
Following from (D.39), it holds that
EπK,b	ψ(x, u)ζb1 +	ΘK,bζb2	- EπK,b	c(x,u)ψ(x,u)	+	ζb1 -	J(K, b)2
=F ζb,ξ(ζb) =maxF(ζb,ξ) = gap(ζb, ξb) + minF(ζ,ξb),	(D.42)
ξ∈Vξ	ζ∈Vζ
where the first equality comes from (D.41), and the second equality comes from the fact that ξ(ζ) =
argmaxξ∈V F(ζ, ξ) by (D.40) and Lemma D.6. We upper bound the RHS of (D.42) and lower
bound the LHS of (D.42) in the sequel.
As for the RHS of (D.42), it holds for any ξ ∈ Vξ that
min F(Z, ξ) ≤ min max F(Z, ξ) = min F(Z, ξ(Z))
=2 mVn∣∣∣E∏κ,b [ψ(x,u)]ζ 1 + θκ,bζ2 - E∏κ,b[c(X, U)ψ(X,u)] ∣∣2 + IZ1 - J(K,切2}
= 0,	(D.43)
where the first equality comes from the fact that ξ(ζ) = argmaxξ∈V F(ζ, ξ) by (D.40) and Lemma
D.6, the second equality comes from (D.41), and the last equality holds by taking ζ = ζK,b ∈ Vζ.
Meanwhile, we lower bound the LHS of (D.42) as
∣∣∣EπK,b [ψ(X, u)]ζb1 + ΘK,bζb2 - EπK,b [c(X, u)ψ(X, u)]∣∣∣ + IIζb1 - J(K, b)II2
=∣∣θK,b(ζ - Cκ,b) ∣∣2 ≥ λK ∙ kb - ζK,bk2 ≥ λK ∙ IlbK,b - aK,bk2, (D.44)
where the first equality comes from the definition of ΘK,b in (B.9), and the first inequality comes
from Proposition B.6. Here λK is defined in Proposition B.6. Combining (D.42), (D.43), and (D.44),
it holds that
kακ,b - ακ,b∣2 ≤ λκ2 ∙ gap(b,b),	(D.45)
which finishes the proof of this part.
Part 2.	We now upper bound gap(ζb, ξb). We denote by zet = (Xet>, uet>)> for t ∈ [Te], where Xet and uet
are generated in Line 6 of Algorithm 3. Following from the state transition in Problem 2.3 and the
form of the linear policy, {zet }t∈[Te] follows the following transition,
zet+1 = Lzet + ν + δt ,	(D.46)
30
Published as a conference paper at ICLR 2020
where
_ A Aμ + d ʌ ω — ( ωt ʌ 丁 _ (A B、
V = I-K(Aμ + d) + b) , t = 1-Kωt + ση) ,	= I-KA -KB) .
Note that we have
L= -KAA -KBB = -IK(A B).
Then by the property of spectral radius, it holds that
ρ(L)=ρ(A B) -IK	=ρ(A-BK)<1.
Thus, the Markov chain generated by (D.46) admits a unique stationary distribution N(μ, ∑z),
where
μz = (I-L)TV,	ςZ= LfZL> + (-Kψω KψωKωK σ2l) .	(D.47)
The following lemma characterizes the average
Te
bz = ι∕τ ∙ X e
t=1
(D.48)
Lemma D.7. It holds that
bz 〜N (μz+e^ μτ,
where ∣∣μτ∣∣2 ≤ Mμ ∙ (1 - ρ)-2 ∙ ∣∣μz∣∣2 and k∑TIlF ≤ M∑ ∙ (1 - ρ)-1 ∙ k∑z∣∣F∙Here Mμ and M∑
6
are positive absolute constants. Moreover, it holds with probability at least 1 - T -6 that
e
IIbZ - μzk2 ≤ T1/4 ∙ (1 - ρ)-2 ∙ poly(kφK k2, IIKkF, ∣∣b∣∣2, kμ∣∣2)∙
Proof. See §F.7 for a detailed proof.
□
Lemma D.7 gives that

kbK,b - μK,bk2
log T
≤ -—一
Te1/4
•(1 - Pr-
∙poly(kφκ k2,kK kF,kbk2,kμk2),
which holds with probability at least 1 - Te-6 .
We now apply a truncation argument to show that gap(ζ, ξ) is upper bounded. We define the event
E in the sequel. Following from Lemma D.7, it holds for any Z 〜N(μz, ∑z) that
z — bz + 1/T • μτ 〜N(0, ∑z + 1/T • ΣT).
By Lemma G.3, there exists a positive absolute constant C0 such that
PhIkZ- bz + 1∕T • μτk2 - tr(Σ z )∣ >τ i ≤ 2exp[-Co ∙ min(τ2kΣ Zk-2, τ kΣ Zk-1)], (D.49)
where we write Σz = Σz + 1/T • ΣTe for notational convenience. By taking τ = C1 • logT • kΣzkF
in (D.49) for a sufficiently large positive absolute constant C1, it holds that
PhIkZ - bz + 1∕T • μτk2 - tr(Σz)∣ >C1 • logT ∙ kΣZkFi ≤ T-6.	(D.50)
We define the event Et,1 for any t ∈ [T] as
Et,ι = n∣kzt - bz + 1∕T • μτk2 - tr(ΣZ)∣ ≤ Ci ・ logT ∙k∑z∣∣f}∙
31
Published as a conference paper at ICLR 2020
Then by (D.50), it holds for any t ∈ [T] that
P(Et,1) ≥ 1 - T-6.	(D.51)
Also, we define
E1 = \ Et,1.	(D.52)
t∈[T]
Following from a union bound argument and (D.51), it holds that
P(E1) ≥ 1 - T -5 .	(D.53)
Also, conditioning on E1, it holds for sufficiently large T that
max Ilzt- bzk2
t∈[T]
• ••~■一	,~■'	.~ .. <->
≤ Ci ∙ logT ∙k∑z∣F + tr(∑z) + I1/T • 〃别2
≤ 2Ci ∙ [1 + M∑(1 - ρ)-1∕T2] ∙ logT ∙∣∑z∣2 + Mμ(1 - PT-IT ∙ ∣μz∣2
≤ C- ∙logT ∙ (1 + IIKkF) ∙ kφκII- ∙(I-P)-1 + C3 ∙ (kbk2 + IlΜk2) ∙(I-P)-4 ∙ T-2
≤ 2C- ∙ logT ∙ (1 + kKkF) ∙ kΦκk2 ∙ (1 - ρ)-i,	(D.54)
where Ce1 , C-, and C3 are positive absolute constants. Here, the first inequality comes from the
definition of E1 in (D.52), the second inequality comes from Lemma D.7, and the third inequality
comes from (D.47). Also, we define the following event
E = {kbz- μz + 1∕T ∙ 〃别2 ≤ Ci}.	(D.55)
Then by Lemma D.7, we know that
P(E2) ≥ 1 - Te-6	(D.56)
for T sufficiently large. We define the event E as
E=Ei\E2.
Then following from (D.53), (D.56), and a union bound argument, we know that
P(E) ≥ 1 - T-5 - Te-6.
Now, we define the truncated feature vector ψ(x, u) as ψ(x, u) = ψ(x, u) 1E, the truncated cost
function ec(x, u) as ce(x, u) = c(x, u) 1E, and also the truncated objective function F (ζ, ξ) as
Fe(Z,ξ) = {e@Z 1 + E[(Ψe- Ψ0)Ψe>]Z2	-	E(ee)}>ξ2 +	[Z1	- E(e)]	∙ξ1	-	kξk-∕2,	(d.57)
where we write ψ = ψ (x, u) and ce = ce(x, u) for notational convenience. Here the expectation
is taken following the policy πK,b and the state transition. The following lemma establishes the
upper bound of |F(Z, ξ) - F(Z, ξ)∣, where F(Z, ξ) and F(Z, ξ) are defined in (B.11) and (D.57),
respectively.
Lemma D.8. It holds with probability at least 1 - Te-6 that
e
∣F(Z,ξ) - F(Z, ξ)∣ ≤ (2T +	∙ (1 - ρ)-2 ∙ poly(kKkF, kbk2, k〃k2, J(Ko, b°)).
Proof. See §F.8 for a detailed proof.
□
6
Following from (D.39) and Lemma D.8, it holds with probability at least 1 - T-6 that
∣gap(
Zb, ξb) - gfap(Zb, ξb)∣∣
e
≤ (2T + T/4) ∙ (1 - P)-2 ∙ poly(kKkF, kbk2, kμk2, J(Ko, bo))∙
(D.58)
32
Published as a conference paper at ICLR 2020
1	1 C	-~~^ /U 4
where we define gfap(ζ, ξ) as
gfap(ζb, ξb) = max Fe(ζb, ξ) - minFe(ζ,ξb).
ξ∈Vξ	ζ∈Vζ
Therefore, to upper bound of gap(ζ, ξ), we only need to upper bound gfap(ζ, ξ).
Part 3. We upper bound gfap(ζ, ξ) in the sequel. We first show that the trajectory generated by the
policy πK,b and the state transition in Problem 2.2 is β-mixing.
Lemma D.9. Consider a linear system yt+ι = Dyt + H + υt, where {yt}t≥o ⊂ Rm, the matrix
D ∈ Rm×m satisfying P(D) < 1, the vector H ∈ Rm, and Ut 〜N(0, Σ) is the Gaussians. We
denote by $t the marginal distribution of yt for any t ≥ 0. Meanwhile, assume that the stationary
distribution of {yt}t≥0 is a Gaussian distribution N((I - D)-1H, Σ∞), where Σ∞ is the covariance
matrix. We define the β-mixing coefficients for any n ≥ 1 as follows
β(n) = SupEy~$t [∣∣Pyn(∙ | y0 = y) - PN((I-D)T@∑∞)S IlTv] ∙
Then, for any ρ ∈ (ρ(D), 1), the β-mixing coefficients satisfy that
β(n) ≤ Cρ,D泌∙ [MςQ + m ∙ (1 - P) 2] / ∙ Pn,
where CρD,曾 is a constant, which only depends on ρ, D, and H. We say that the sequence {yt}t≥o
is β-mixing with parameter P.
Proof. See Proposition 3.1 in Tu and Recht (2017) for details.
□
Recall that by (3.1), the sequence {xt}t≥0 follows
xt+1 = (A — BK)xt + (Bb + Aμ + d) + Q, Et 〜N(0, Ψ∈),
where the matrix A - BK satisfies that P(A - BK) < 1. Therefore, by Lemma D.9, the sequence
{zt}t≥0 is β-mixing with parameter P ∈ (P(A - BK), 1), where zt = (xt>, ut>)>. The following
lemma upper bounds the primal-dual gap for a convex-concave problem.
Lemma D.10. Let X and Y be two compact and convex sets such that kx - x0 k2 ≤ M and ky -
y0 k2 ≤ M for any x, x0 ∈ X and y, y0 ∈ Y . We consider solving the following minimax problem
min max H(x, y) = Ee〜$」G(x, y； e)],
where the objective function H(x, y) is convex in x and concave in y. In addition, we assume that
the distribution $e is β-mixing with β (n) ≤ Ce ∙ ρn, where Ce is a constant. Meanwhile, We assume
that it holds almost surely that G(x, y; E) is Lo-Lipschitz in both X and y, the gradient VχG(x, y; E)
is L1-Lipschitz in x for any y ∈ Y, the gradient VyG(x, y; E) is L1-Lipschitz in y for any x ∈ X,
where Ce, L0, L1 > 1. Each step of our gradient-based method takes the following forms,
Xt+1 = Γχ	Ixt	一	Yt+1	∙	VxG(Xt,yt； Et)],	yt+1	= Γγ [yt	一	γt+ι ∙ VyG(xt,yt; Et)],
where the operators ΓX and ΓY projects the variables back to X and Y, respectively, and
the stepsizes take the form Yt = γo ∙ t-1/2 for a constant γo > 0. Moreover, let b =
(PtT=1 γt)-1(PtT=1 γtXt) and yb = (PtT=1 γt)-1(PtT=1 γtyt) be the final output of the gradi-
ent method after T iterations, then there exists a positive absolute constant C, such that for any
δ ∈ (0, 1), the primal-dual gap to the minimax problem is upper bounded as
max H (Xb, y) 一 min H(X, yb) ≤
x∈X	y∈Y
C ∙ (M2 + L0 + L0L1M)
log(1/P)
log2 T + log(1∕δ)	C ∙ CeL0M
√T	+ —T―
which holds with probability at least 1 一 δ.
Proof. See Theorem 5.4 in Yang et al. (2019) for details.
□
33
Published as a conference paper at ICLR 2020


To use Lemma D.10, we define the function G(ζ, ξ; ψ, ψ0) as
G(ζ, ξ; ψ, ψ0) = [ψζ1 + (ψ - ψ0)ψ>ζ Z-eψ]> ξ2 + (ζ1 - ee) ∙ ξ1 -1/2 ∙ kξk
2
2,






where ψ = ψ(x, u) and ψ0 = ψ(x0, u0). Note that the gradients of G(ζ, ξ; ψ, ψ0) take the form
VZG(ζ,ξ; ψ,ψ0)
ʌ-- _
ψe>ξ2 + ξ1
-c-
ψ(ψ -
~	-,	~	~■一	~-l- C	_ .~	Cl 一
ψeζ1 + (ψe -	ψe0)ψe>ζ2 -	ceψe - ξ2


~.

~.





~-

By Definition B.7 and Lemma D.6, we know that
IIVZG(Z, ξ; ψe, ψe0)∣∣2 ≤ poly(kK∣F, J(Ko, bo)) ∙ log2 T ∙ (1 - ρ)-2,
Il VξG(ζ, ξ; ψe, ψe0) ∣∣2 ≤ poly(kK∣F, kμk2, J(Ko, bo)) ∙ log2 T ∙ (1 - ρ)-2.
(D.59)



This gives the Lipschitz constant Lo in Lemma D.10 for G(ζ, ξ; ψ, ψ 0). Also, the Hessians of


G(ζ, ξ; ψ, ψ0) take the forms of
0,
which follows that
I∣v2ζ G(Z,ξ; ψe,ψe0)∣∣2
0,
1.
(D.60)


~.

~.
This gives the Lipschitz constant L1 in Lemma D.10 for VZG(ζ, ξ; ψ, ψ0) and VξG(ζ, ξ; ψ, ψ 0).
Moreover, note that (D.54) provides an upper bound of M, combining (D.59), (D.60) and Lemma
D.10, it holds with probability at least 1 - T-5 that
gfap(ζb, ξb) ≤
poly(kK∣F, kμk2,J(Ko, bo)) ∙ log6 T
(1 - ρ)4 ∙ Vt
(D.61)
^
^
Combining (D.45), (D.58), and (D.61), we know that

kbκ,b - ακ,bk2 ≤ λκ2 ∙ polyι (kK∣∣F, kb∣∣2, kμ∣∣2, J(Ko, bo)) ∙
log6 T
log T
T1/2 ∙ (1 - p)4 + Te1∕4 ∙ (1 - ρ)2 一.
Same bounds for kΥbK - ΥKk2F, kpbK,b - pK,bk22, and kqbK,b - qK,bk22 hold. We finish the proof of
□
the theorem.
E	Proofs of Propositions
E.1 Proof of Proposition 3.2
Proof. We follow a similar proof as in the one of Theorem 1.1 in Sznitman (1991) and Theorem 3.2
in Bensoussan et al. (2016). Note that for any policy πK,b ∈ Π, the parameters K and b uniquely
determine the policy. We define the following metric on Π.
Definition E.1. For any πK1,b1 , πK2,b2 ∈ Π, we define the following metric,
kπKι,bι - πK2,b2 k2 = c1 ∙ ∣∣K1 - KtZ k2 + c2 ∙ kb1 - b2 k2,
where c1 and c2 are positive constants.
One can verify that Definition E.1 satisfies the requirement of being a metric. We first evaluate the
forms of the operators Λι(∙) and Λ2 (∙, ∙).
Forms of the operators Λι(∙) and Λ2(∙, ∙). By the definition of Λι(μ), which gives the optimal
policy under the mean-field state μ, it holds that
八皿=πμ,
where ∏μ solves Problem 2.2. This gives the form of Λι(∙). We now turn to Λ2(μ, π), which gives
the mean-field state μnew generated by the policy ∏ under the current mean-field state μ. In Problem
34
Published as a conference paper at ICLR 2020
2.2, the sequence of states {xt}t≥0 constitutes a Markov chain, which admits a unique stationary
distribution. Thus, by the state transition in Problem 2.2 and the form of the linear-Gaussian policy,
we have
Nnew = (A - BKn)μnew + (Bbn + Aμ + d),	(E∙D
where Kn and b∏ are parameters of the policy ∏. By solving (E.1) for μnew, it holds that
Λ2 (μ, π) = μnew = (I — A + BKn) 1 (Bbn + Aμ + d).
This gives the form of Λ2(∙, ∙).
Next, we compute the LiPSchitz constants for Λι(∙) and Λ2(∙, ∙).
Lipschitz constant for Λι(∙). By Proposition 3.4, for any μ1,μ2 ∈ Rm, the optimal K * is fixed for
Problem 2.2. Therefore, by the form of the optimal bK given in Proposition 3.4, it holds that
∣∣Λ1(μ1) - Λ1(μ2)∣∣2 ≤ C2 ∙∣∣ [(I - A)QT(I - A)> + br-1b>]-1a∣∣2
∙ Il [K*Q-1(I - A)> - R-1b>] ∣∣2 ∙kμι - μ2k2
=c2Lι ∙ kμι - μ2∣∣2,	(E.2)
where L1 is defined in Assumption 3.1.
Lipschitz constants for Λ2(∙, ∙). By Proposition 3.4, for any μ1,μ2 ∈ Rm, the optimal K * is fixed
for Problem 2.2. Thus, for any π ∈ Π such that π is an optimal policy under some μ ∈ Rm, it holds
that
∣∣A2(μi, π) - λ2(M2, π) ∣∣2 = ∣∣ (I - A + BKn )-1 ∙ A ∙ (p1 - μ2) ∣∣2
≤	[1	-	P(A - bk*)]	1	∙ IIAk2	∙	I∣μ1	-	μ2∣∣2
=L2 ∙ kμι - μ2k2,	(E.3)
where L2 is defined in Assumption 3.1, and Kn = K* is the parameter of the policy π. Meanwhile,
for any mean-field state μ ∈ Rm, and any poicies ∏1,∏2 ∈ Π that are optimal under some mean-field
states μ1, μ2, respectively, we have
∣∣A2(μ,πI)-八2(也开2) ∣∣2 = ∣∣(I - A + bk *)-1B ∙ (b∏ι - b∏2 )∣∣2
≤ [1-ρ(A-BK*)]-1∙IBI2∙Ibn1-bn2I2
= c2-1L3 ∙ Iπ1 - π2 I2,	(E.4)
where in the last equality, we use the fact that Kn1 = Kn2 = K* by Proposition 3.4. Here L3 is
defined in Assumption 3.1, and bn1 and bn2 are the parameters of the policies π1 and π2.
Now we show that the operator Λ(∙) is a contraction. For any μ1,μ2 ∈ Rm, it holds that
∣∣A(μI)- A(μ2"∣2 =∣∣A2(μ1, A1(μI)) - λ2 (μ2, AMμ2D∣∣2
≤∣∣Λ2(μ1, Λ1(μ1)) — Λ2 (μ1, Λ1(μ2)) ∣∣2+∣∣Λ2 (μ1, Λ1(μ2)) — Λ2 (μ2, Λ1(μ2)) ∣∣?
≤ c2 1L3∙∣∣A1(μI)- A1(μ2"∣2 + L2 ∙ kμ1 - μ2∣∣2
≤ c-1L3 ∙ c2L1 ∙∣∣μ1 - μ2k2 + L ∙ kμ1 - μ2k2 = (L1L3 + L2) ∙∣∣μ1 - μ2k2,
where the first inequality comes from triangle inequality, the second inequality comes from (E.3)
and (E.4), and the last inequality comes from (E.2). By Assumption 3.1, we know that L0 =
L1L3 + L2 < 1, which shows that the operator Λ(∙) is a contraction. Moreover, by Banach fixed-
point theorem, we obtain that Λ(∙) has a unique fixed point, which gives the unique equilibrium pair
of Problem 2.3. We finish the proof of the proposition.	□
35
Published as a conference paper at ICLR 2020
E.2 Proof OF Proposition 3.4
Proof. By the definition of J (K, b) in (3.6) and the definition of μκ,b in (3.2), the problem
min J2(K, b)
b
is equivalent to the following constrained problem,
min
μ,b
(μ∖ τ (Q + KTRK
∖b) [	-RK
-KτR∖ (μ
Rb
s.t. (I - A + BK)μ - (Bb + Aμ + d)=0.
Following from the KKT conditions of (E.5), it holds that
2Mk (μ) + Nkλ = 0,	NK W + Aμ + d = 0,
where
MK =(Q+RKrk -KTR) ,	Nk =LI-A+BK))
By solving (E.6), the minimizer of (E.5) takes the form of
(KKKK) = -MKINK (NKMKINK )-1(Aμ + d).
By substituting (E.7) into the definition of J (K, b) in (3.6), We have
J2(K, bK) = (Aμ + d)τ(NKMKINK)-1(Aμ + d).
Meanwhile, by calculation, we have
M-1 = ( QT	Q-1κτ )
MK = IKQT KQ-1KT + RTJ .
Therefore, the term NKMKINK in (E.8) takes the form of
NKMKINK = (I - A)Q-1(I - Aτ)+ BR-1Bτ.
By plugging (E.9) into (E.8), we have
J2(K, bκ) = (Aμ + d)τ[(I - A)Q-1(I - Aτ) + BRTBT] -1(Aμ + d).
(E.5)
(E.6)
(E.7)
(E.8)
(E.9)
Also, by plugging (E.9) into (E.7), we have
KQ-IQT-(A-A)R-IBT) [(I - A)Q-1(I - A)τ + BRTBT]-1(Aμ + d)
We finish the proof of the proposition.
□
E.3 Proof of Proposition B.2
Proof. By the definition of the cost function c(x, U) in Problem 2.2 (recall that we drop the subscript
μ when we focus on Problem 2.2), we have
ECt = E(XTQxt + UTRUt + μτQμ)
=E(XT Qxt + XT K τRKxt _ 2bτRKxt + bτRb + σ2ηj Rnt + μτQμ)
=E[xτ(Q + KτRK)xt — 2bτRKxt] + bτRb + σ2 ∙ tr(R) + μτQμ, (E.10)
where we write Ct = c(xt, Ut) for notational convenience. Here in the second line we use Ut =
∏κ,b(xt) = -Kxt + b + σηt. Therefore, combining (E.10) and the definition of J(K, b) in Problem
2.2, we have
1
J(K, b) = lim — X{ElxT(Q + KτRK)xt - 2bTRKxt] + bτRb + σ2 ∙ tr(R) + μτQμ∣
T→∞ T	J	J
=Ex〜N(μκ,b,φκ) [xτ(Q + KτRK)x - 2bτRKx] + bτRb + σ2 ∙ tr(R) + μτQμ
=tr[(Q + KTRK)Φκ] + μK,b(Q + KTRK)μκ,b - 2bτRKμκ,b	(E.11)
+ bτRb + σ2 ∙ tr(R) + μτQμ.
36
Published as a conference paper at ICLR 2020
Now, by iteratively applying (3.3) and (3.4), we have
tr(Q + K>RK)ΦK =tr(PKΨ),	(E.12)
where PK is given in (3.4). Combining (E.11) and (E.12), we know that
J(K, b) = JI(K) + J2(K, b) + σ2 ∙ tr(R) + μ>Qμ,
where
J1(K) =tr(Q+K>RK)ΦK = tr(PKΨ),
j2 (KM = ("K,b)> (Q+-KKRK-K>R) CK,b).
We finish the proof of the proposition.	□
E.4 Proof of Proposition 3.3
Proof. By calculating the Hessian matrix of J2(K, b), we have
V2bJ2(K, b) =B>(I - A + BK)->(Q + K>RK)(I - A + BK)TB
- RK(I -A+BK)-1B+B>(I - A + BK)->K>R +R
=R1/2K(I - A + BK)-1B - R1/2> R1/2K(I - A + BK)-1B - R1/2
+B>(I-A+BK)->Q(I-A+BK)-1B,
which is a positive definite matrix independent of b. We denote by its minimum singular value as
νK. Also, note that kVb2bJ2(K, b)k2 is upper bounded as
MbJ2(K,b)∣∣2 ≤ [1 - ρ(A - BK)]-2 ∙ (∣∣B∣∣2 ∙ ∣∣Kk2 ∙∣∣R∣∣2 + |网2 ∙∣∣Qk2).
Therefore, it holds that
∣k ≤ [1 - P(A - BK)]-2 ∙ (kBk2 ∙ kKk2 ∙kRk2 + kBk2 ∙kQk2),
where ∣k is the maximum singular value of V^J2 (K, b). We finish the proof of the proposition. □
E.5 Proof of Proposition B.3
Proof. Following from Proposition B.2, it holds that
JI(K)= tr(PK We)= Ey 〜N (0,Ψe)(y>PK y) = Ey 〜N (0,Ψe) [fK ⑻],	(E.13)
where fK (y) = y>PKy. By the definition of PK in (3.4), we obtain that
VKfK(y) = VKny>(Q + K>RK)y + [(A - BK)y]>PK[(A - BK)y]>o
=2RKyy> + VK [fκ ((A - BK)y)].	(E.14)
Also, we have
VK [fκ ((A - BK)y)i = VK fκ ((A - BK)y) - 2B>Pk(A - BK)yy>.	(E.15)
By plugging (E.15) into (E.14), we have
VK fκ(y) = 2[(R + B>PKB)K - BiPKA]yy> + VKfκ ((A - BK)y).	(E.16)
By iteratively applying (E.16), it holds that
∞
Vkfκ(y)=2[(R + BTPKB)K - BTPKA] ∙ X yty>,	(E.17)
t=0
where yt+1 = (A - BK)yt with y0 = y. Now, combining (E.13) and (E.17), it holds that
Vk Ji(K) = 2[(R + B>PκB)K - B>PκA]Φκ = 2(Υ%K - YK) ∙ Φκ,
where YK is defined in (3.7). Meanwhile, combining the form of μκ,b in (3.2), it holds by calcula-
tion that
Vb J2 (K, b) = 2 [γ22(-Kμκ,b + b) + YKμκ,b + qκ,b],
where qκ,b is defined in (3.7). We finish the proof of the proposition.	□
37
Published as a conference paper at ICLR 2020
E.6 Proof of Proposition B.1
Proof. From the definition of VK,b (x) in (B.1) and the definition of the cost function c(x, u) in
Problem 2.2, it holds that
∞
VK,b(x) = X Ext>(Q + K>RK)xt - 2b>RKxt
t=0
+ b>Rb + σ2η>Rnt + μ>Qμ | x0 = x] — J(K, b)}.
Combining (3.1), we know that VK,b (x) is a quadratic function taking the form of VK,b (x) =
x>Gx + r>x + h, where G, r, and h are functions of K and b. Note that VK,b(x) satisfies that
VK,b(x) = c(x, —Kx + b) — J(K, b) + EVK,b(x0) | x],	(E.18)
by substituting the form of c(x, —Kx+b) in Problem 2.2 and J(K, b) in (3.5) into (E.18), we obtain
that
x>Gx + r>x + h
=x>(Q + K > RK )x — 2b>RKx + b>Rb + μ>Qμ	(E.19)
—	[tr(Pκ Ψe) + μK,b(Q + K >RK)μκ,b — 2b>RKμκ,b + μ>Qμ + b> Rb]
+	[(A — BK )x + (Bb + Aμ + d)]>G[(A — BK )x + (Bb + Aμ + d)]
+	tr(GΨe) + r> [(A — BK)X + (Bb + Aμ + d)] + h — σ2 ∙ tr(R).
By comparing the quadratic terms and linear terms on both the LHS and RHS in (E.19), we obtain
that
G = PK,	r = 2fK,b,
where fκ,b = (I — A + BK)->[(A — BK)>Pk(Bb + Aμ + d) — K>Rb]. Also, by the definition
of VK,b(x) in (B.1), we know that E[VK,b(x)] = 0, where the expectation is taken following the
stationary distribution generated by the policy πK,b and the state transition. Therefore, we have
h = -2fκ,bμκ,b — μK,bpκ μκ,b — tr(pκ φk ),
which shows that
Vκ,b(x) = XTPK x - tr(Pκ Φk ) + 2fK,b(x — μκ,b) — μK,bPκ μκ,b.	(E.20)
For the action-value function QK,b(x, u), by plugging (E.20) into (B.2), we obtain that
Qκ,b(x,u) = (u)> YK (X) + 2 (pK,b)> (U) - tr(pκφκ) - σ2 ∙ S(R + PKBB>)
—b> Rb + 2b> RKμκ,b — μK,b(Q + K >RK + PK )μκ,b
+ 2fK,b ^Aμ + d) — μκ,b] +(Aμ + d)> pκ(Aμ + d).
We finish the proof of the proposition.	口
E.7 Proof of Proposition B.5
Proof. By Proposition B.1, it holds that QK,b takes the following linear form
QK,b(x, u) = ψ(x, u)>αK,b + βK,b,	(E.21)
where βK,b is a scalar independent of x and u. Note that QK,b(x, u) satisfies that
QK,b(x, u) = c(x, u) — J(K, b) + EπK,b QK,b(x0, u0) |x,u ,	(E.22)
where (x0, u0) is the state-action pair after (x, u) following the policy πK,b and the state transition.
Combining (E.21) and (E.22), we obtain that
ψ(x, u)>αK,b = c(x, u) — J(K, b) + EπK,b [ψ(x0, u0) | x, u]>αK,b.	(E.23)
38
Published as a conference paper at ICLR 2020
By left multiplying ψ(x, u) to both sides of (E.23), and taking the expectation, we have
E∏κ,b {ψ(x,u)[ψ(x,u) - ψ(x0,u0)]>} ∙ ακ,b + E∏κ,b [ψ(x,u)] ∙ J(K, b) = E∏κ,b [c(x,u)ψ(x,u)].
Combining the definition of the matrix ΘK,b in (B.7), we have
1	0	J(K, b) =	J(K, b)
EπK,b ψ(x, u)	ΘK,b	αK,b	EπK,b c(x, u)ψ(x, u)	,
which concludes the proof of the proposition.	□
E.8 Proof of Proposition B.6
Proof. Invertibility and Upper Bound. We denote by zt = (xt> , ut> )> for any t ≥ 0. Then
following from the state transition and the policy πK,b, the transition of {zt}t≥0 takes the form of
zt+1 = Lzt + ν + δt ,
(E.24)
where L, ν and δ are defined as
(A B、	_ A Aμ + d ∖	f	ωt	∖
L = -KKA -KB) , V = --K(Aμ + d)+ b) ,	δt = --ωωt + σηj .
Note that L also takes the form of
L= -IK (AB	).
Combining the fact that ρ(UV ) = ρ(V U) for any matrices U and V , we know that ρ(L) = ρ(A -
K) < 1, which verifies the stability of (E.24). Following from the stability of (E.24), we know
that the Markov chain generated by (E.24) admits a unique stationary distribution N(μz, ∑z), where
μz and ∑z satisfy that
μz = Lμz + ν, ∑z = L∑z L> + Ψδ.
where
Ψ =	Ψω	-ΨωK>
Ψδ = -KΨω KΨωK> + σ2I .
Also, we know that Σz takes the form of
>
Σz =Cov	ux =	-KKΦK	KΦKKK>+σ2I	= 0	σ2I	+ -K	ΦK	-K	,
(E.25)
where ΦK is defined in (3.3).
The following lemma establishes the form of ΘK,b.
Lemma E.2. The matrix ΘK,b in (B.7) takes the form of
C _ (2(ςz 与S ςz)(I - L 与S L)>	0	、
ΘK,b =	0	Σz(I-L)> .
Proof. See §F.9 for a detailed proof.
□
Note that since P(L) < 1, both I - L XS L and I - L are positive definite. Therefore, by Lemma
E.2, the matrix ΘK,b is invertible. This finishes the proof of the invertibility of ΘK,b. Moreover, by
(E.25) and Lemma E.2, we upper bound the spectral norm of ΘK,b as
kΘκ,bk2 ≤ 2max{|&k2 ∙(1 + ∣∣L∣∣2), k∑zk2 ∙(1 + kLk2)} ≤ 4(1 + |阳图2 ∙ ∣∣Φkk2,
which proves the upper bound of kΘK,bk2 .
39
Published as a conference paper at ICLR 2020
Minimum singular value. To lower bound σmin(ΘK,b), we only need to upper bound
11	1
σmax(Θ-K1,b) = kΘ-K1,bk2. We first calculate Θ-K1,b. Recall that the matrix ΘK,b in (B.8) takes the
form of
1
0
Θe K,b
EπK,b ψ(x, u) ΘK,b
By the definition of the feature vector ψ(x, u) in (B.5), the vector σez = EπK,b [ψ(x, u)] takes the
form of
σez = EπK,b ψ(x, u) = sv0ekc+(Σmz) ,
where 0k+m denotes the all-zero column vector with dimension k + m. Also, since ΘK,b is invert-
ible, the matrix ΘK,b is also invertible, whose inverse takes the form of
Θe -1	1	0
θκ,b = - -Θ-1 ∙ e	θ-1 J.
,	∖ θK,b σz	θK,bJ
1
The following lemma upper bounds the spectral norm of Θ-K1,b.
1
Lemma E.3. The spectral norm of the matrix Θ-K1,b is upper bounded by a positive constant λK,
IT	1	1	1	Il T^ll	1	/ Λ	I > τ^∖
where λK only depends on kKk2 and ρ(A - BK).
Proof. See §F.10 for a detailed proof.	□
By Lemma E.3, We know that σmin(Θκ,b) is lower bounded by a positive constant λκ = 1∕λκ,
which only depends on ∣∣K∣∣2 and P(A - BK). This concludes the proof of the proposition. □
F	Proofs of Lemmas
F.1 Proof of Lemma D.1
Proof. Following from (3.4), it holds that
y>PK2 y = X y>(A - BK2)t>(Q + K2>RK2)(A - BK2)ty.	(F.1)
t≥0
Meanwhile, by the state transition yt+1 = (A - BK2)yt, we know that
yt = (A - BK2)ty0 = (A - BK2)ty.	(F.2)
By plugging (F.2) into (F.1), it holds that
y>PK2 y = Xyt>(Q + K2>RK2)yt = X(yt>Qyt + yt>K2>RK2yt).	(F.3)
t≥0	t≥0
Also, it holds that
y>PK1 y = X(yt>+1PK1yt+1 - yt>PK1yt)	(F.4)
t≥0
Combining (F.3) and (F.4), we have
y>PK2y - y>PK1y = X(yt>Qyt + yt>K2>RK2yt + yt>+1PK1yt+1 - yt>PK1yt).	(F.5)
t≥0
Also, by the state transition yt+1 = (A - BK2)yt, it holds for any t ≥ 0 that
yt>Qyt + yt>K2>RK2yt + yt>+1PK1yt+1 - yt>PK1yt
= yt>Q + (K2 - K1 + K1)>R(K2 - K1 + K1)yt
+ yt>A - BK1 -B(K2 -K1)>PK1A-BK1 -B(K2 -K1)yt -yt>PK1yt
= 2yt>(K2 -K1)>(R+B>PK1B)K1 -B>PK1Ayt
+yt>(K2 -K1)>(R+B>PK1B)(K2 -K1)yt
= 2yt>(K2 -K1)>(Υ2K21K1 -Υ2K11)yt+yt>(K2 -K1)>Υ2K21(K2 -K1)yt,	(F.6)
40
Published as a conference paper at ICLR 2020
where the matrix ΥK1 is defined in (3.7). By plugging (F.6) into (F.5), we have
y>PK2 y - y>PK1y
=X2yt>(K2-K1)>(Υ2K21K1-Υ2K11)yt+yt>(K2-K1)>Υ2K21(K2-K1)yt
t≥0
=	DK1,K2 (yt),
t≥0
where Dki,k(y) = 2yγ(K - KI)(YKIKi - YK1)y + y>(K2 - K)>ΥK1(K2 - Kι)y. We
finish the proof of the lemma.	□
F.2 Proof of Lemma D.2
Proof. We prove (D.16) and (D.17) separately in the sequel.
Proof of (D.16). From the definition of J1(K) in (3.6), we have
JI(K)- JI(K*) = tr(PKψe - pκ*We) = Ey〜N(o,Ψe)(y>Pκy - y>pκ*y)
=-E [χ Dk,k* (yt)l,	(F.7)
t≥0
where in the last equality, we apply Lemma D.1 and the expectation is taken following the transition
yt+i = (A — BK*)yt with initial state yo 〜N(0, Ψe). Here we denote by Dk,k* (y) as
Dk,k*(y) = 2y>(K* - K)(YKiK - YK)y + y>(K* - K)>YK(K* - K)y.
Also, we write DK,K* (y) as
DK,K*(y) = 2y>(K* -K)(Y2K2K-Y2K1)y+y>(K* -K)>Y2K2(K* -K)y	(F.8)
= y>K* -K+(Y2K2)-1(Y2K2K-Y2K1)>Y2K2K* - K + (Y2K2)-1(Y2K2K - Y2K1)y
- y>(Y2K2K - Y2K1)>(Y2K2)-1(Y2K2K - Y2K1)y.
Note that the first term on the RHS of (F.8) is positive, due to the fact that it is a quadratic form of a
positive definite matrix, we lower bound DK,K* (y) as
DK,K* (y) ≥ -y>(Y2K2K - Y2K1)>(Y2K2)-1(Y2K2K - Y2K1)y.	(F.9)
Combining (F.7) and (F.9), it holds that
JI(K)- Ji(K*) ≤ E(Xyty>)	∙ tr[(YK2K - YK)>(YK)-1(YKK - 丫沏
t≥0	2
=kΦκ* k2 ∙ tr[(YK2K - YK0>(YK2)T(YK2K - 丫2]
≤	Il(YK2)T∣∣2 ∙kΦκ*k2 ∙tr[(YK2K- YKy(YKK- 丫沏
≤	σ-i1n(R) ∙kΦκ* k2 ∙ tr[(YK2K - YK)>(YKK - 丫2],
where the last line comes from the fact that Y2K2 = R + B>PKB	R. This complete the proof of
(D.16).
-	CC ______ 一	.	C	≈.............. ∙	-.	C -.
Proof of (D.17). Note that for any K, it holds by the optimality of K* that
Ji(K) -Ji(K*) ≥ Ji(K)-Ji(Ke)= -E X DK,Ke (yt) ,	(F.10)
t≥0
where the expectation is taken following the transition yt+i = (A - BK)yt With initial state yo 〜
N(0, Ψe). By taking K = K - (Y2K2)-i(Y2K2K - Y2Ki) and following from a similar calculation as
in (F.8), the function DK,Ke (y) takes the form of
DK,Ke(y) = -y>(Y2K2K - Y2Ki)>(Y2K2)-i(Y2K2K - Y2Ki)y.	(F.11)
41
Published as a conference paper at ICLR 2020
Combining (F.10) and (F.11), it holds that
J(K)- J(K*) ≥ tr[Φκ<(ΥKK2K - Y沿>(Υ泊T(YKK2K - Υκ)]
≥ bmin(Ψe) • ||丫和-1 ∙ "[(Y^K - Y^)>(YK2K - Y2],
where We use the fact that ΦKK = (A — BK)ΦKK(A — BK) 1 + Ψe 占 Ψe m the last line. This
finishes the proof of (D.17).	□
F.3 Proof of Lemma D.3
Proof. By Proposition B.2, we have
IJI(Kn+1)— J1(Kn+1) ∣= "[(「太…- PKn+ι )Ψ e] ≤ kPjKn+1 - Pk”+1∣∣2 ∙ kΨJ∣F∙	(F.12)
The following lemma upper bounds the term kPKe	- PKn+1 k2 .
Lemma F.1. Suppose that the parameters K and K satisfy that
kK — K k2 ∙ (kA - BK k2 + 1) ∙kΦκ k2 ≤ σmin(Ψω )/4 ∙ |田|[-1,	(F.13)
then it holds that
-1	. .	....	....	. . '~
kPK - PK k2 ≤ 6 ∙ σmi1n (Ψω) ∙ ∣∣Φk ∣∣2 ∙ kK∣2 ∙ |倒2 ∙ ∣∣K - K∣2	(F.14)
• (kBk2 ∙kKk2) ∙kA - BKk2 + kBk2 ∙kKk2 + 1).
Proof. See Lemma 5.7 in Yang et al. (2019) for a detailed proof.	□
To use Lemma F.1, it suffices to verify that Kn+1 and Kn+1 satisfy (F.13). Note that from the
definitions of Kn+1 and Kn+1 in (D.18) and (D.21), respectively, we have
kKn+1 - Kn+1k2 • (IlA - BKn+1k2 + 1) • ∣∣φKKn+ι k2
≤ Y • ∣YKn - YKnkF • (1 + ∣Kn∣2) • (kA - BKn+1k2 + 1) • kΦκn+1k2.	(F.15)
Now, we upper bound the RHS of (F.15). For the term kA - BKn+1 k2, it holds by the definition of
二 . 一— 一、■
Kn+1 in (D.21) that
kA - BKen+1k2 ≤ kA - BKnk2 + γ • kBk2 • kY2K2n Kn - Y2K1n k2
≤ kA - BKnk2 + Y • kB∣2 • kYKn∣2 • (1 + kKnk2).	(F.16)
By the definition of YKn in (3.7), we upper bound kYKn k2 as
kYKnk2 ≤kQk2 + kRk2 + (∣a∣f + ∣b∣f)2 ∙kPKnk2
≤ kQk2 + kRk2 + (kAkF + ∣B∣f)2 • J1(K0) ∙ σm1n(Ψe),	(F.17)
where the last line comes from the fact that
J1(K0) ≥ JI(Kn) = tr[(Q + K>RKn)Φκn] = tr(PKnΨe) ≥ kPKnk2 • σmin(Ψe).
As for the term kΦKe	k2 in (F.15), from the fact that
J1(K0) ≥ JI(Kn+1) = tr[(Q + K>+1RKn+l)Φκn+1] ≥ kΦKn+1k2 • σmin(Q),
it holds that
kΦKn+1k2 ≤ J1(K0) • σ-i1n(Q).	(F.18)
Therefore, combining (F.15), (F.16), (F.17), and (F.18), we know that
kKn+1 - Kn+1∣∣2 • (IlA - BKn+1∣∣2 + 1) ∙ ∣∣φKn+ι l∣2
≤ Polyi(kKnk2) ∙kYKn - YKnkF.	(F.19)
42
Published as a conference paper at ICLR 2020
一	一 •	一 ................. .......... -	-	F /1	~ C	.
From Theorem B.8, it	holds with probability at least	1	- Tn-4	- Tn-6	that
kΥbKn - ΥKnkF ≤
POM#KnkF, k〃k2)log3 Tn
--------:----:~~:- , --：_Γ- 
λKn • (1 - P)	Tl/4
+ Pθly4(kKnkF,kb0k2,kμk2)
λKn
(F.20)
log1/2 Tn
Tl/8 ∙(1-ρ),
which holds for any ρ ∈ (ρ(A - BKn), 1). Note that from the choice of Tn and Tn in the statement
of Theorem B.4 that
Tn	≥ p0ly5(∣∣KnlIF,	∣∣b0∣∣2, Mg)	∙ λK4n，[1 -P(A - BKn)]	∙ ε-5,
Tn	≥ P0ly6(kKnkF,	kb0∣2, k〃k2)∙ λ∕	∙	[l - ρ(A - BKn)]	-" ∙ ε-12,
it holds that
POly3(kKnkF, k〃k2)log3 Tnl P0% ( k Kn k F, k M 2, k〃k 2 )	log" Tn
λKn∙(1-ρ)2	尸 +	λKn	Tl/8 ∙ (1 - ρ)
≤ min I ∣P0lyι(kKnk2)i	Pmm (Ψω )/4 ∙ ∣B∣-1,	(F.21)
hP0ly2(kKn k2)i-1 ∙ ε∕8 ∙ Y ∙ σmin(Ψe) Pmm(R) ∙∣Φk * ∣-1 ∙ |&|曰｝.
Combining (F.19), (F.20), and (F.21), we know that (F.13) holds with Probability at least 1 - ε15
for sufficiently small ε > 0. Meanwhile, by (F.16), (F.17), and (F.18), the RHS of (F.14) is uPPer
bounded as
6 ∙ σ-1n(ψω) ∙ kφKn+ιk2 ∙ kKn+1k2 ∙ ∣∣R∣∣2 ∙ kKen+l - Kn+1k2
• (kBk2 ∙ IIKn+1∣^ ∙ IlA - BKn+1k2 + ∣∣B∣∣2 ∙ kKn+1∣∣2 + 1)
,..	..、 ..^ ..
≤ Pθly2(kKnk2) ∙kΥKn- YKnkF.	(F.22)
Now, by Lemma F.1, it holds with Probability at least 1 - ε15 that
kPKen+1 - PKn+1 k2 ≤ 6 • σmin(Ψω) • kΦKen+1 k2 • kKen+1k2 • kRk2 • kKen+1 - Kn+1k2
• (∣∣B∣∣2 • kKn+1∣^ • kA - BKn+1∣∣2 + ∣∣B∣∣2 • ∣∣Kn+1∣∣2 + 1)
≤ PθM(kKnk2 ) ∙kY Kn- YKnkF
≤ ε∕8 • Y • σmin(Ψe) ∙ σmin(R) ∙ ∣∣Φk* k-1 • ∣∣Ψ∈k-1,	(F.23)
where the second inequality comes from (F.22), and the last inequality comes from (F.20) and (F.21).
Combining (F.12) and (F.23), it holds with Probability at least 1 - ε15 that
IJI(Kn+1)- JI(Kn+I)I ≤ Y • σmin(We) • σmin(R) ∙ ∣∣φK* ∣∣2 1 ∙ ε/4,
which concludes the Proof of the lemma.	□
F.4 Proof of Lemma D.4
Proof. Note that YK*K* 一 YK* is the natural gradient of J at the minimizer K*, which implies
that
Y2K2*K* - Y2K1* = 0.	(F.24)
By Lemma D.1, it holds that
JI(K)- JI(K *) = tr(PK ψe - pK * ψe) = Ey 〜N (0,Ψe)(y>PK Iy - y>PK* y)
=E Xh2yt>(K-K*)(Y2K2*K* -Y2K1*)yt+yt>(K-K*)>Y2K2*(K-K*)yti
t≥0
=E Xyt>(K-K*)>Y2K1*(K-K*)yt ,	(F.25)
t≥0
43
Published as a conference paper at ICLR 2020
where we use (F.24) in the last line. Here the expectations are taken following the transition yt+1 =
(A — BK)yt with initial state yo 〜N(0, Ψe). Also, We have
e{X y> (K - K *)>ΥK2* (K - K * )y∕
t≥0
=tr[Φκ (K — K *)> Υ%* (K — K *)]
≥kΦκk2 ∙kΥKM∣2 ∙ tr[(K - K*)>(K - K*)]
≥ σmin(Ψe) ∙ σmin(R) ∙ ∣∣K — K*∣∣F,	(F.26)
where we use the fact that Φk = (A - BK )Φk (A - BK )+Ψe 占 Ψe and Υ%* = R+B>Pk* B 占
R in the last line. Combining (F.25) and (F.26), we have
JI(K)- Jl(K*) ≥ σmm(Ψe) Pmg(R) ∙ ∣K - K*|信
We conclude the proof of the lemma.	□
F.5 Proof of Lemma D.5
Proof. Following from Proposition 3.3, we have
J2(KN, bh+1) - J2(KN, bh+1)
≤ Yb ∙ VbJ2(KN, eh+I)T [VbJ2(KN, bh) - ▽ b J2(KN, bh)]
+ (Yb)2 ∙ VKN/2 ∙ IlVbJ2(Kn, bh) - VbJ2(KN, bh)∣∣2,
J2(KN, bh+1) - J2(KN, bh+1)
≤ -γb ∙VbJ2(KN,eh+ι)τ [VbJ2(KN, bh) - VbJ2(KN, bh)]	(F.27)
-(Yb)2 ∙ ιKN/2 ∙ ∣∣ VbJ2 (KN, bh) - V b j2 (KN, bh)∣∣2,
where νKN and ιKN are defined in Proposition 3.3. Also, following from Proposition B.3, it holds
that
∣∣VbJ2(KN, eh+1)∣∣2 ≤ poly/kKN∣∣f, ∣∣bh∣∣2, kμ∣∣2, J(KN, b°)) ∙ [1 - P(A - BKN)] 1.
(F.28)
Combining (F.27), (F.28), and the fact that VKN ≤ ∣Kn ≤ [1 一 P(A — BKN)]-2 ∙ poly2(∣∣KN∣∣2),
we know that
J2 (KN, bh+1) - J2(KN, ebh+1)	(F.29)
≤ (γb)2 ∙ Poly2(kKNl∣2) ∙ ∣∣VbJ2(KN, bh) - Vbj2(kn, bh)∣∣2 ∙ [1 - P(A - bkn)] 2
+ Yb ∙poly1(∣∣KN∣∣f, ∣bh∣∣2, ∣∣μ∣∣2,J(Kn, bo)) ∙ ∣∣VbJ2(KN,bh) - VbJ2(KN, bh)∣∣2
• [1 - ρ(A - BKn)]-1.
Note that from the definition of VbJ2(KN, bh) and VbJ2(KN, bh) in (D.31) and (D.33), respec-
tively, it holds by triangle inequality that
∣∣VbJ2(KN,bh)-VbbJ2(KN,bh)∣∣2
≤ llγK2N - YKNl∣2 ∙ IIKNl∣2 ∙ IlbKN,bh∣∣2 + IlYKN∣∣2 ∙ IlKN∣∣2 ∙ IlbKN,bh - μκN瓦 ∣∣2
+ HyKN- yK2nII2 , IlbhIl2 + llγKN- yKnII2 • IIbKN,bhIl2 + IIbKN及-qKNmIl2
+ llγK1N∣∣2 • IIbKN,bh -mkn,bhl∣2.
By Theorem B.8, combining the fact that J2(KN, bh) ≤ J2(KN, bo) and the fact that ∣∣mkn,b∣2 ≤
J(KN, bo)∕σmin(Q), we know that with probability at least 1 - (Tb)-4 - (Tb)-6, it holds for any
P ∈ (P(A - BKN), 1) that
∣∣VbJ2(KN,bh)-VbbJ2(KN,bh)∣∣2	(F.30)
log3 Tb	log1/2 Teb
≤ λKN ∙poly3(∣KN 1^^2/^2,,2(KN ,b0)) ∙ [ (Tb )1∕g(1- ρy +(聋工.(In-P) ∙
44
Published as a conference paper at ICLR 2020
Following from the choices of γb, Tnb, and Tenb in the statement of Theorem B.4, it holds that
Yb ∙ polyi(kKN l∣F, kbhk2, kμk2, J(KN, bO)) ∙ λKN ∙ p0ly3(llκN l∣F, kbhk2, kμk2, J2 (KN, bO))
」(TnH + (Tlb- P)卜[1- P(A - BKN)] -1+ [1 -P(A - BKNr
. p°ly3(kKN kF, kbhk2, kμ∣2, J2(KN, bθ)) ∙ [ (TT-P) 4 + (Tb)17 T - ρ)2
• (Yb)2 ∙poly2(kKNk2)∙λκN
≤ VKN • Yb • ε∕2.
Further combining (F.29) and (F.30), it holds with probability at least 1 - ε15 that
J2 (KN, bh+1) - J2(KN, ebh+1) ≤ νKN • Yb • ε∕2.
We then finish the proof of the lemma.	□
F.6 Proof of Lemma D.6
Proof. We show that ζK,b ∈ Vζ and ξ(ζ) ∈ Vξ for any ζ ∈ Vζ separately.
Part 1.	First we show that ζK,b ∈ Vζ. Note that from Definition B.7, we know that ζK1 ,b = J(K, b)
satisfies that 0 ≤ ζK1 ,b ≤ J(KO, bO). It remains to show that ζK2 ,b = αK,b satisfies that lζK2 ,b l2 ≤
Mζ. By the definition of αK,b in (B.6), we know that
kακ,bk2 ≤kΥκkF + kΥκk2 • (kμκ,bk2 + kμK,bk2)
+ (kA∣2 + kBk2)2 • (kPκk2 ∙kAμ + dk2 + kfκ,bk2)2	(F.31)
where fκ,b = (I — A + BK)->[(A 一 BK)>PK(Bb + Aμ + d) — K>Rb] and for notational
simplicity, we denote by μK b = -Kμκ,b + b. We only need to bound YK, μκ,b, μK b, PK, and
fK,b. Note that by Proposition B.2, the expected total cost J(K, b) takes the form of
J(K, b) = tr(PκΨe) + μK,bQμκ,b + (μK,b)>RμK,b + σ2 ∙ tr(R) + μ>Qμ.
Thus, we have
J(KO, bO) ≥ J(K, b) ≥ σmin(Ψω) • tr(PK) ≥ σmin(Ψω) • kPKk2,
J(Ko,bo) ≥ J(K, b) ≥ μK,bQμκ,b ≥ σmin(Q) ∙ ∣∣μκ,b∣∣2,
J(Ko,bo) ≥ J(K,b) ≥ (〃K,b)>R〃K,b ≥ σmm(R) • kμK,bk2,
which imply that
kPKk2 ≤ J(Ko,bo)∕σmin(Ψω),
kμκ,b∣∣2 ≤ J(Ko, bo)∕σmin(Q),
kμK,b∣2 ≤ J(K0,b0)∕σmin(R).	(F.32)
For ΥK, it holds that
ΥK= Q0 R0+BA>>PK(A B),
which gives
kYκkF ≤ (kQ∣F + kR∣F) + (∣Ak2 + kBkF) ∙kPκkF,
kYκ k2 ≤ (kQk2 + kRk2)+ (kAk2 + kBk2)2 ∙kPκ k2.
Combining (F.32) and the fact that ∣Pk ∣∣f ≤ √m • ∣∣PK ∣∣2, we know that
k γK ∣∣F ≤ (kQI∣F + kRI∣F) + (kAIlF + kBk2) • √m • J(K0, bθ"σmin(ψω),
kYκ k2 ≤ (kQk2 + kRk2) + (kA∣2 + kBk2)2 • J (K0,b0)∕σmin(Ψω).	(F.33)
45
Published as a conference paper at ICLR 2020
Now, we upper bound the vector fK,b. Note that by algebra, the vector fK,b takes the form of
fκ,b = -Pκμκ,b + (I - A + BK)-T(Qμκ,b- K>RμK,b).
Therefore, we upper bound fK,b as
kfK,bk2 ≤	J(K0, bO)2	∙	σm1n(WS)	∙ σ-1n(Q)	+ [1 -P(A - BK)]	，(KQ + KR ∙ IIKkF)	(F.34)
Combining (F.31), (F.32), (F.33), and (F.34), it holds that
kζK,bk2 = kαK,bk2 ≤ MZ,1 + MZ,2 ∙ (1 + IIKkF) ∙ [1 -P(A - BK)]-1.
Therefore, it holds that ζK,b ∈ Vζ .
Part 2.	Now we show that for any ζ ∈ Vζ, we have ξ(ζ) ∈ Vξ. Recall that from (D.41), it holds that
ξ1 (ζ) = ζ1 - J(K, b), ξ2 (ζ) = EπK,b [ψ(x, u)]ζ1 + ΘK,bζ2 - EπK,b [c(x, u)ψ (x, u)]. (F.35)
Then we have
ξ1(ζ) = ζ1 -J(K,b) ≤J(K0,b0),
(F.36)
where we use the fact that since ζ ∈ Vζ, we have 0 ≤ ζ1 ≤ J(K0, b0) by Definition B.7. Also, by
(F.35), we have
ξ2(ζ)2 ≤
|
sz
B1
Note that we upper bound B1 as
E∏κ,b [ψ(x, u)] Z1 Il + ∣θκ,bk2 ∙ kζ2k2 + ∣∣E∏κ,b [c(x, u)ψ(x, u)]
-----------------} "	{	' ।----------..--------
|---------
B2
(F.37)
*{z
B3
2
}
B1 ≤ J(K0,b0) ∙ ∣∣E∏κ,b [ψ(x,u)] ∣∣2
Following from the definition of ψ(x, u) in (B.5), we know that
(F.38)
EπK,b ψ(x, u)	≤ I∑zIF,
(F.39)
where Σz is defined as
>
∑z = Cov
x
u
ΦK	-ΦKK>
-KΦK	KΦKK> + σ2I
00 σ02I + -IK ΦK -IK
Combining (F.38) and (F.39), we have
Bi ≤ J(Ko,bo) ∙k∑z∣∣F.
By Proposition B.6, we upper bound B2 as
B2 ≤ 4(1 + kKkF)3 ∙ kφκ k2 ∙ (MZ,1 + Mζ,2) ∙ [1 - P(A - BK)] 1,
(F.40)
(F.41)
where we use the fact that ζ ∈ Vζ and Definition B.7. As for the term B3 in (F.37), we utilize the
following lemma to provide an upper bound.
Lemma F.2. The vector EπK,b [c(x, u)ψ(x, u)] takes the following form,
∕2svec[∑z diag(Q,R)∑z + h∑z, diag(Q,R)i∑z ]ʌ
_	∕svec(∑z)
+ [μK,bQμK,b + (μK,b)TRμK,b + μTQμ] I 0m
0k
Here the matrix ∑z takes the form of
∑z =	-KΦKΦK
-ΦKK>
KΦk K> + σ2 ∙ I .
46
Published as a conference paper at ICLR 2020
Proof. See §F.11 for a detailed proof.
□
From Lemma F.2 and (F.32), it holds that
B3 ≤ 3[IIQI∣f + IlRIlF + J(Ko,bo) ∙ IlQIl2∕σmin(Q)	(f.42)
+ J(Ko,bo) ∙∣∣R∣∣2∕σmin(R)] ∙k∑z∣2.
Moreover, by the definition of Σz in (E.25), combining the triangle inequality, we have the following
bounds for IΣz IF and IΣz I2,
k∑zkF ≤ 2(d + kKkF) ∙kΦκk2,	k∑zk2 ≤ 2(1 + kKkF) ∙kΦκk2.	(F.43)
Also, we have
J(Ko, bo) ≥ J(K, b) ≥ tr[(Q + K>RK)Φk] ≥ ∣Φκk2 ∙ σmin(Q),
which gives the upper bound for ΦK as follows,
IΦK I2 ≤ J(Ko, bo)∕σmin(Q).	(F.44)
Therefore, combining (F.37), (F.40), (F.41), (F.42), (F.43), and (F.44), we know that
∣∣ξ2(Z)∣∣2 ≤ C ∙ (Mζ,ι + Mζ,2) ∙ J(Ko,bo)2∕σmm(Q)	(F.45)
• (1 + kKkF)3 ∙ [1-P(A - BK)]-1.
By (F.36) and (F.45), we know that ξ(ζ) ∈ Vξ for any ζ ∈ Vζ. We conclude the proof of the
lemma.	□
F.7 Proof of Lemma D.7
Proof. Assume that 茄 〜N(μ↑, Σ∣). Following from the fact that
zet+1
Lzet + ν + δt ,
it holds that
t-1
Z 〜N Ltμt + £Li”,
i=o
(L>)tΣtLt + Xt-1(L>)iΨδLi ,
i=o
(F.46)
where
Ψ = Ψω
Ψδ =	KΨω
KΨω
KΨωK> + σ2I .
From (D.47), We know that μz takes the form of
∞
μz = (I — L)-1 ν =〉： Ljν.
j=o
(F.47)
Therefore, combining (F.46) and (F.47), we have
1 Te
E(bz )= μz + 不 ELt
T t=1
Te	∞
μt- TXX Liν∙
t=1 i=t
(F.48)
We denote by
Te
Te	∞
μT
ELt μt -£ ∑Liν.
t=1
t=1 i=t
47
Published as a conference paper at ICLR 2020
Meanwhile, it holds that
Te	Te ∞
X Ltμt - XX Liv2
Te	Te ∞
≤ Xρ(L)t ∙kμtk2 + XXρ(L)i ∙kvk2
≤ [1 -P(L)] 1 ∙ kμtk2 + [1 - ρ(L)] 2 ∙ kvk2
≤ Mμ ∙ (1 - ρ)-2 ∙kμzk2,	(F.49)
where Mμ is a positive absolute constant.
For the covariance, note that for any random variables X 〜N(μι, ∑ι) and Y 〜N(μ2, ∑2), We
know that Z = X + Y 〜N(μι + μ2, Σ), where k∑∣∣F ≤ 2k∑ιkr + 2k∑2∣∣F. Combining (F.46),
we know that bz 〜N(Ebz, Σt/T), where ΣT satisfies that
Te	Te t-1
T∕2 ∙k∑TkF ≤ Xρ(L)2t ∙ k∑tkF + XXPlLw ∙kΨδkF
≤ [1 - p(l)2]-1 ∙k∑tkF + T ∙ [1 - p(L)2]-1 ∙kΨδkF,
which implies that
k∑TkF ≤ M∑ ∙(1-ρ)-1∙k∑zkF,
(F.50)
where MΣ is a positive absolute constant. Combining (F.48), (F.49), and (F.50), we obtain that
bz
〜N (μz + 于 μTe,
where ∣∣μTek2 ≤ Mμ ∙ (1 - p)-2 ∙ ∣∣μz∣∣2 and ∣∣ΣTkF ≤ M∑ ∙ (1 - ρ)-1 ∙ k∑z∣∣F∙ Moreover, by the
6
Gaussian tail inequality, it holds with probability at least 1 - T-6 that
e
Ilbz- μzk2 ≤ T1/4 ∙ (1 - ρ)-2 ∙ poly(kφK k2, IIKkF, ∣∣b∣∣2, kμ∣∣2)∙
Then we finish the proof of the lemma.
□
F.8 Proof of Lemma D.8
Proof. We continue using the notations given in §D.3. We define
F(ζ,ξ) = {e(Ψ)Z 1 + E[(ψb - ψ0)ψ>]ζ2 - E(cψ)}>ξ2 + [ζ1 - E(c)] ∙ ξ1 - 1∕2 ∙kξk2,
where ψ = ψ (x, u) is the estimated feature vector. Here the expectation is only taken over the
trajectory generated by the state transition and the policy πK,b, conditioning on the randomness
induced when calculating the estimated feature vectors. Thus, the function F (ζ, ξ) is still random,
where the randomness comes from the estimated feature vectors. Note that |F(Z, ξ) - F(Z, ξ)∣ ≤
|F(ζ,ξ) - F(Z,ξ)∣ + ∣F(Z,ξ) - F(Z,ξ)∣. Thus, we only need to upper bound |F(Z,ξ) - F(Z,ξ)∣
.^ ,	~:	...
and ∣F(Z,ξ)- F(Z,ξ)∣.
Part 1. First we upper bound |F (Z,ξ) - F (Z, ξ)∣. Note that by algebra, we have
F (ζ, ξ) - Fb(ζ, ξ)
= nE(ψ	-	ψb)ζ1	+	E[(ψ	-	ψ0)ψ> -	(ψb -	ψb0)ψb>]ζ2 - E[c(ψ	- ψb)]o	ξ2
≤ E(kψ - b∣2) ∙ h∣Z 1| + E(kψ - ψ0k2 + 2∣∣ψ∣∣2) ∙ IIZ2k2 + E(c)i ∙ kξ2k2,	(F.51)
48
Published as a conference paper at ICLR 2020
where the expectation is only taken over the trajectory generated by the state transition and the policy
πK,b. From Lemma D.7, it holds that
P(kμz - μz + 1/T ∙ μτk2 ≤ CI) ≥ 1 - T 6.	(F.52)
Therefore, combining (F.52), it holds with probability at least 1 - Te-6 that
E(∣∣ψ - ψ0k2 + 2∣∣ψ∣∣2) ≤ Poly(I∣Φkk2, kKIlF, Wk2, kμk2, J(Ko, bo)),	(F.53)
where the expectation is conditioned on the randomness induced when calculating the estimated
feature vectors. Also, we know that
E(C) ≤ Poly(kΦκk2, kKkF, kbk2, kμk2, J(Ko, bo)).	(F.54)
Therefore, combining (F.51), (F.53), (F.54), and Definition B.7, it holds with Probability at least
r≤ C
1 - Te-6 that
IF(Z,ξ) - F(Z,ξ)∣ ≤ E(kψ - ψb∣∣2) ∙poly(kΦκk2, kK∣∣F, kb∣2, kμk2,J(K0,b0)).	(F.55)
1 - ΛΛ ♦ C	.1 Ie	C / /	∖ ∙ /ɪʌ L、 Λ j7^' /	∖ ♦ /ɪʌ 1 Λ∖	F	1 Il ʃ /	∖
Following from the definitions ofψ(x, u) in (B.5) and ψ(x, u) in (B.14), we upper bound kψ(x, u) -
G/ Ml C	I
ψ(x, u)k2 for any x and u as
kψ(X, U) - ψ (X, U) k 2 = kμbz - μz k2 + Ilz(Rz - μz )> + (μbz - μz )z> IIf + kμz μ> - μz /b> k2
≤ Poly(kΦκk2, kKkF, kb∣2, kμk2, J(Ko, bo)) ∙ kbz — μzk2,	(F.56)
where μz is defined in (D.47), μz is defined in (D.48), and z = (x>,u>)>. Also, by Lemma D.7,
we know that
e
kbz - μz l∣2 ≤ T1/4 ∙ (1 - Pr-
• poly (llφκ l∣2, kK I∣f, kbl∣2, kμ∣∣2,J(Kο, bo)),
(F.57)
.......... •一	..... 一	： C — 一 .	.	____ _______ 一__________ •一 一一	•一
which holds with probability at least 1 - T-6 . Combining (F.55), (F.56), and (F.57), it holds with
probability at least 1 - Te-6 that
e
IF(ζ,ξ) - Fb(ζ,ξ)∣ ≤ T1/4 , (I-P)-2 • poly(kK∣∣F, kb∣∣2, kμ∣∣2,J(K0, bO)).	(F.58)
ɪʌ ，A -.-VT	1	1 I 言/月 八	六/月 >≈∖ I	.<	IClC	1
Part 2. We now upper bound |F(Z, ξ) 一 F(Z, ξ) | in the sequel. By definitions, we have
IIFe(ζ, ξ) - Fb(ζ, ξ)II
= IIInE(ψe - ψb)Z1 + E(ψe - ψe0)ψe>	-	(ψb	-	ψb0)ψb>Z2	- E(ceψe - cbψb)o	ξ2 + E(cb	-	ec)ξ1III
≤ IIInE(ψb)Z1+E(ψbψb>)Z2 -E(cbψb)o>ξ2+E(cb)ξ1III •1Ec	(F.59)
+ ∣[E(ψ0ψ>)Z2]>ξ2∣ ∙ 1(E0∩E)C,
where we define the event E 0 as
E 0= \ ∩ n∣kz0 - μz + 1/T • μτ k2 - tr(∑ z )∣ ≤ Cι∙iοg T ∙k∑ zk2})∩ E2,
t∈[T]
where E2 is defined in (D.55). Combining the fact that P(E2) ≥ 1 - T-6 and Lemma G.3, it holds
that P(E0) ≥ 1 - T-5 - T-6. Following a similar argument as in Part 1, it holds from (F.59) that
∣Fe(ζ,ξ) - Fb(ζ,ξ)∣ ≤ (T + T/4) , poly(kKl∣F, kbk2, kμk2, J(K0, bO))	(F.6。)
for sufficiently large T and T.
Now, combining (F.58) and (F.60), by triangle inequality, it holds with probability at least 1 - Te-6
that
e
∣F(Z,ξ) - F(Z, ξ)∣ ≤ (2T + TgZ) • (1 - P)-2 • poly(kKkF, kbk2, kμk2, J(Ko, %))∙
We finish the proof of the lemma.	□
49
Published as a conference paper at ICLR 2020
F.9 Proof of Lemma E.2
Proof. Recall that the feature vector ψ(x, u) takes the following form
ψ(χ,u)= (svec[(Z T- )μ(z - μz )T])
We then have
ψ(x, u) - ψ(x0, u0) = svec[yyT y--(L(yLy++δ)δ()Ly + δ)T]
(F.61)
where We denote by y = Z - μz, and (χ0, u0) is the state-action pair after (x, U) following the state
transition and the policy πK,b. Therefore, for any symmetric matrices M, N and any vectors m, n,
it holds from (B.7) and (F.61) that
svecm(M)TΘK,b
svec(yyT)	svec[yy T - (Ly + δ)(Ly + δ)T]
y	y- (Ly+δ)
Ey,δ{ (hM,yy>i + m>y) ∙ [hN,yy> - (Ly + δ)(Ly + δ)>i + n>(y - Ly - δ)] }
Ey(hyy>,Mi ∙ hyy> - LyyTLT - ψδ,Ni) + Ey(hyy>,Mi ∙ n>(y - Ly))	(F.62)
|
}|
{z^
A1
{z^
A2
}
+ Ey(m>y ∙ hyy> -LyyTLT -Ψδ,Ni) + Ey[m>y ∙n>(y-Ly)],
|
} I
*{z
A3
{z^
A4
}
where the expectations are taken over y 〜N(0, ∑z) and δ 〜N(0, Ψδ). We evaluate the terms Ai,
A2, A3, and A4 in the sequel.
For the terms A2 and A3 in (F62), by the fact that y = Z - μz 〜N(0, ∑z), we know that these two
terms vanish. For A4 , it holds that
A4 = Ey [m>y ∙ (y — Ly)>n] = Ey [m>yy>(I — L)>n] = m>∑z(I — L)>n.	(F.63)
For A1 , by algebra, we have
Ai = Ey(hyy>,Mi ∙ hyy> -LyyTLT -Ψδ,Ni)
=EyIhyyT, M ∙ hyyτ - LryyTLτ, Ni) - Ey(hyy'Mi ∙ hψδ,Ni)
=Ey [yTMy ∙ yT(N - LTNDy]-R, Mi ∙ hΨδ, Ni
=Eu〜N(0,i) [uT∑*M∑Zr2u ∙ ut∑1r2(N - LTNL)∑y2u] -h∑z, Mi ∙ hΨδ, Ni. (F.64)
Now, by applying Lemma G.1 to the first term on the RHS of (F.64), we know that
Ai = 2tr[∑y2M∑1/2 ∙ ∑1/2(N - LtNL)∑1/2]
+ tr(∑y2M∑z/2) ∙ tr[∑y2(N - L>NL)∑1/2] - h∑z,Mi ∙ (Ψδ,Ni
=2(M, ∑z(N - L>NL)∑zi + h∑z,M)∙ h∑z - L∑zL> - Ψδ,Ni
= 2M, Σz(N - LTNL)Σz,
where we use the fact that Σz = LΣzLT + Ψδ in the last equality. By using the property of the
operator svec(∙) and the definition of the symmetric Kronecker product, we obtain that
Ai = 2svec(M)Tsvec[Σz(N - LTNL)Σz]
=2svec(M)> [∑z 区S ∑z - (∑zL>)区S (∑zL>)]svec(N)
=2svec(M)> [(∑z ®S ∑z)(I - L ®S L)>]svec(N).	(F.65)
50
Published as a conference paper at ICLR 2020
Combining (F.62), (F.63), and (F.65), we obtain that
>
ΘK,b
Svec(M)> [2(∑z 与S ∑z)(I - L 0s L)>]svec(N) + m>∑z(I - L)Tn
SveC(M)、> (2(∑z 乳S Σz)(I - L 乳S L)>
m0
Thus, the matrix ΘK,b takes the following form,
Θκ b = (2('z 脸S ςz)(I - L Z)S L)>
0
Σz(I - L)
0
Σz(I-L)>
which concludes the proof of the lemma.
□
F.10 Proof of Lemma E.3
Proof. From the definition of ΘK,b in (B.9), it holdS that
kΘe-K1,bk22≤ 1+kΘ-K1,bk22+kΘ-K1,bσezk22,	(F.66)
where σez iS defined aS
σez = EπK,b [ψ(x, u)] = Sv0ekc+(Σmz) .
We bound the RHS of (F.66) in the Sequel. For the term Θ-K1,bσez, combining Lemma E.2, we have
Θ-K1,bσez =	1/2
(I - L ZS L)->(∑z ZS ∑z)-1 ∙ svec(∑z)、
0k+m
(I - L ZS L)->(Σ-1 ZS Σ-1) ∙ svec(∑z)
0k+m
(I - L ZS L)-> ∙ svec(Σ-1))
0k+m	,
(F.67)
where we uSe the property of the Symmetric Kronecker product in the Second and laSt line. By taking
the Spectral norm on both SideS of (F.67), it holdS that
kΘκ1bσz k2 = 1/2 ∙∣∣(I - L ZSvrJ svec(Σ-1)∣∣2
≤ 1/2 ∙∣∣(I- L ZsL)->∣∣2 ∙∣∣svec(Σ-1)∣∣2
≤ 1/2 • [1-P2(L)]-1 ∙k∑-1kF
≤ 1/2 ∙√k+m • [1-P2(L)]-1 ∙k∑-1k2
=1/2 ∙ √k+m ∙ [1 - P2(L)]-1 ∙ σ-1n(∑z),	(F.68)
where in the third line we uSe Lemma G.2 to the matrix LZS L. Similarly, we upper bound kΘ-K1,bk2
in the Sequel
kθκlbk2 ≤ min{1/2 ∙ [1-ρ2(L)]-1σm2n(∑z), [1 - ρ(L)]-%:(∑z)}.	(F.69)
ThuS, combining (F.66), (F.68), and (F.69), we obtain that
kΘκ1bk2 ≤ 1 + 1/2 ∙ √k + m • [1 - P2(L)]-1 ∙ σm11(∑z)
+ min{1/2 • [1-p2(L)]-1σm2n(∑z), [1 - ρ(L)]，—番①)}.	(F.70)
51
Published as a conference paper at ICLR 2020
Now it remains to characterize σmin(Σz). For any vectors s ∈ Rm and r ∈ Rk, we have
rs> Σz
Ex~N(μκ,b,Φκ),u~∏κ,b(∙∣ x) { [sɪ (X - μK,b) + rτ (U + KμK,b - b)] }
Eχ~N(μκ,b,Φκ),η~N(0,I) { [(s - K>r)>(x - μκ,b) + σr>η] 2 }
Eχ~N(μκ,b,Φκ){ [(s - KTr)T(X - μκ,b)]2} + Eη~N(o,i) [(σr>η)2]. (F∙71)
The first term on the RHS of (F.71) is lower bounded as
Eχ~N(μκ,b,Φκ) {[(s - KTr)T(X - μκ,b)]2 0 =(S- K>r)>φK(S- KTr)
≥ ks - KTrk2 ∙ σmin(Φκ) ≥ ks - KTr||2 ∙ σmm(Ψω),	(F.72)
where the last inequality comes from the fact that σmin(ΦK) ≥ σmin(Ψω) by (3.3). The second
term on the RHS of (F.71) takes the form of
Eη~N (0,i )[9rTn)2] = σ2 ∣∣rk2.	(F.73)
Therefore, combining (F.71), (F.72), and (F.73), we have
-KTrk2 ∙ σmin(Ψω) + σ2∣r∣2
≥ σmin(Ψω) ∙ ∣∣S∣2 + [σ2 - ∣K ∣2 ∙ bmin(Ψω)] ∙ ∣r∣2.
From this, we know that
σmin(∑z ) ≥ min{σmin(Ψω ),σ2 -∣K k2 ∙σmin(Ψω)}.	(F.74)
1
Thus, combining (F.70) and (F.74), we know that ∣Θ-K1,b∣2 is upper bounded by a constant λK, where
T 1 1	1 Il T^ll	t ∕, T ∖	/ Λ I > T^∖ EI ♦ C ♦ 1 .1	CCjl	I-I
λκ only depends on ∣∣K∣∣2 and P(L) = P(A — BK). This finishes the proof of the lemma.	口
F.11 Proof of Lemma F.2
Proof. First, note that the cost function c(X, u) takes the following form,
svec diag(Q, R)
c(x,u) = Ψ(x,u)t	2Qμκ,b
∖	2RμK,b
+ [μK,bQμκ,b + (MK,b)TRMK,b + μTQμ].
For any matrix V and vectors vx, vu, it holds that
svec(V)
vx
vu
I	/ svec [diag(Q, R)]∖	∕svec(V)
E∏κb< Ψ(x,u)t	2Qμκ,b	Ψ(x,u)t	Vx
,I	∖	2RμK,b	)	∖ Vu
|
{z^
D1
}
+ E∏κ,b ∖ ψ(X, u)T(μK,bQμK,b + (MK,b)TRMK,b + μTQμ)
(F.75)
svec(V)
vx
vu
I
{z^
D2
}
In the sequel, we calculate D1 and D2 respectively.
52
Published as a conference paper at ICLR 2020
Calculation of D1. Note that by the definition of ψ(x, u) in (B.5), it holds that
Di = EnKJ (z - μz)>diag(Q, R)(Z - 〃z) + (z - 〃z)> (2Q：K：)
• (Z - μz )>V(Z- μz ) + (Z- μz )> (Vx) }
=E∏κ,b [(z - μz)>diag(Q,R)(z - 〃z) ∙ (z - 〃z)>V(z - 〃z)]	(F.76)
>
+ Eπκ,b (2Rμκ,,b) (Z-μz)(Z-μz)> (V：).
Here Z = (x>,u>)> and μz = E∏κ,b(z). For the first term on the RHS of (F.76), note that
Z - μz 〜N(0, ∑z). Therefore, by Lemma G.1, we obtain that
E∏K,b [(Z - μz )>diag(Q, R)(Z - μz ) ∙ (z - μz )>V (z - μz )]
=2<∑zdiag(Q, R)∑z ,V)+ <∑z, diag(Q, R))∙(∑z, V)
= svech2Σz diag(Q, R)Σz + Σz, diag(Q, R) • Σzi svec(V).	(F.77)
Meanwhile, the second term on the RHS of (F.76) takes the form of
j "(2Rμκ,b)> (z -μzU)> (v：)# =卜(2朦:)#	(v:).	(F.78)
Combining (F.76), (F.77), and (F.78), we obtain that
∕2svec[∑zdiag(Q,R)∑z + h∑z, diag(Q, R)>∑z]∖ > ∕svec(V八
Di = I	∑ (2Qμκ,b)	)(	VX	I .	(F.79)
∖	z ∖2RμK,b/	/ V v： /
Calculation of D2. By the definition of the feature vector ψ(x, u) in (B.5), we know that
_	∕svec(∑z八 > ∕svec(V八
D2 = (μK,bQμκ,b + (μK,b)>RμK,b + 〃>Q〃)	0m	VX	.	(f.80)
0k	V：
Now, combining (F.75), (F.79), and (F.80), it holds that
(2svec[∑zdiag(Q,R)∑z + 限,diag(Q,R)i∑z]\
E∏κ,b[c(X,u)ψ(X,u)] = i	∑ (2Qμκ,b)	J
_	∕svec(∑z)
+ [μK,bQμK,b + (μK,b)τRμK,b + μTQμ] (	0m
0k
which concludes the proof of the lemma.
□
G Auxiliary Results
Lemma G.1. Assume that the random variable W 〜N(0, I), and let U and V be two symmetric
matrices, then it holds that
E(w>Uw • w>V w) = 2tr(UV) +tr(U) • tr(V).
53
Published as a conference paper at ICLR 2020
Proof. See Magnus et al. (1978) and Magnus (1979) for a detailed proof.	□
Lemma G.2. Let M, N be commuting symmetric matrices, and let α1 , . . . , αn, β1, . . . , βn denote
their eigenvalues with v1, . . . , vn a common basis of orthogonal eigenvectors. Then the n(n + 1)/2
eigenvalues of M Xs N are given by (αiβj + αjβi)∕2, where 1 ≤ i ≤ j ≤ n.
Proof. See Lemma 2 in Alizadeh et al. (1998) for a detailed proof.	□
Lemma G.3. For any integer m > 0, let A ∈ Rm×m and η 〜N(0, Im). Then, there exists some
absolute constant C > 0 such that for any t ≥ 0, we have
p[∣η>Aη - E(η>Aη)∣ >ti ≤ 2 ∙ exp[-C ∙ min(t2∣∣Ak-2, tlMk-1)].
Proof. See Rudelson et al. (2013) for a detailed proof.	□
54