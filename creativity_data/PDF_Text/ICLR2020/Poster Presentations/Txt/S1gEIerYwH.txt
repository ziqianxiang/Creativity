Published as a conference paper at ICLR 2020
Transferring Optimality Across Data Distri-
butions via Homotopy Methods
Matilde Gargiani1, Andrea Zanelli2, Quoc Tran-Dinh3, Moritz Diehl2,4, Frank Hutter1,5
1	Department of Computer Science, University of Freiburg
{gargiani, fh}@cs.uni-freiburg.de
2	Department of Microsystems Engineering (IMTEK), University of Freiburg
{andrea.zanelli, moritz.diehl}@imtek.uni-freiburg.de
3Department of Statistics and Operations Research, University of North Carolina
quoctd@email.unc.edu
4	Department of Mathematics, University of Freiburg
5	Bosch Center for Artificial Intelligence
Ab stract
Homotopy methods, also known as continuation methods, are a powerful math-
ematical tool to efficiently solve various problems in numerical analysis. In this
work, we propose a novel homotopy-based numerical method that can be used to
gradually transfer optimized parameters of a neural network across different data
distributions. This method generalizes the widely-used heuristic of pre-training
parameters on one dataset and then fine-tuning them on another dataset of inter-
est. We conduct a theoretical analysis showing that, under some assumptions, the
homotopy method combined with Stochastic Gradient Descent (SGD) is guaran-
teed to converge in expectation to an rθ-optimal solution for a target task when
started from an expected rθ-optimal solution on a source task. Empirical evalua-
tions on a toy regression dataset and for transferring optimized parameters from
MNIST to Fashion-MNIST and CIFAR-10 show substantial improvement of the
numerical performance over random initialization and pre-training.
1 Introduction
Homotopy methods (Allgower & Georg, 1980), also known as continuation methods, are a powerful
mathematical tool to efficiently solve various problems in numerical analysis (e.g., Tran-Dinh et al.
(2012), Zanelli et al. (2019)). The core idea consists in sequentially solving a series of parametric
problems, starting from an easy-to-solve problem and progressively deforming it, via a homotopy
function, to the target one. Homotopy methods are suitable to solve complex non-convex optimiza-
tion problems where no or only little prior knowledge regarding the localization of the solutions
is available. In addition, in contrast to state-of-the-art algorithms in deep learning (e.g., Bottou
(2010), Duchi et al. (2011), Kingma & Ba (2015)), these methods often achieve global convergence
guarantees by only exploiting local structures of the problem. Concepts, such as curriculum-learning
and warm-starting, that are related to different degrees to homotopy methods, have been explored
both in the deep learning (e.g., Gulcehre et al. (2016), Mobahi (2016), Gulcehre et al. (2017)) and
in the reinforcement learning (e.g., Narvekar (2017)) communities.
In this work, we propose a novel homotopy-based numerical method to transfer knowledge regarding
the localization of a minimizer across different task distributions in deep learning. This method grad-
ually tracks a neural network’s (close-to-)optimal parameters from one data distribution to another
one via the homotopy method (Allgower & Georg, 1980) and can be interpreted as a generalization
of the very common heuristic of fine-tuning a pre-trained network. After discussing related work
(Section 2) and background on homotopy methods (Section 3), our contributions are as follows:
1. We provide a general theoretical analysis of the homotopy method when using SGD as
an iterative solver, proving that under some local assumptions it tracks in expectation an
rθ-optimal solution from the source task to the target task (Section 4).
1
Published as a conference paper at ICLR 2020
2.	We introduce homotopy functions for transferring optimality across data distributions for
supervised regression and classification tasks (Section 5).
3.	For a toy regression dataset and for transferring optimized parameters from MNIST to
Fashion-MNIST and from MNIST to CIFAR-10, we show that our method obtains up to
two orders of magnitude better numerical performance than random initialization and sub-
stantial improvement of the numerical performance over pre-training (Section 6).
2	Related Work
Deep neural networks have led to establish a new state-of-the-art in many applications. Despite their
great success and the many theoretical studies that have been published in the last years (e.g., Bal-
duzzi et al. (2017), Li et al. (2018), Feizi et al. (2018), Kunin et al. (2019)), training these deep
models remains a big challenge. Various stochastic optimization algorithms (e.g., Duchi et al.
(2011), Kingma & Ba (2015), Reddi et al. (2018)) and initialization heuristics (e.g., Daniely et al.
(2016), Klambauer et al. (2017), Hanin & Rolnick (2018)) have been recently suggested in order
to improve and speed up the training procedure. We now briefly discuss the state-of-the-art deep
learning optimization techniques and initialization strategies that are most related with the proposed
homotopy-based method, drawing connections with existing and ongoing research works in the field.
Curriculum Learning. First introduced by Bengio et al. (2009) and then extended in different
works (e.g., Graves et al. (2017), Weinshall et al. (2018), Hacohen & Weinshall (2019)), curriculum
learning can also be listed among the optimization heuristics proposed to alleviate the complexity of
solving high dimensional and non-convex problems. In particular, taking inspiration from the fact
that humans and animals learn “better” when exposed to progressively more complex situations in
an organized manner, curriculum learning techniques guide the training by starting with “easy-to-
learn” samples and progressively introducing more “complex-to-learn” ones. This guided learning
process can also be rephrased in a homotopy-like fashion (see Algorithm 1) as solving a sequence
of optimization problems where the target training distribution gradually changes from considering
only the “easy” examples to the full original training distribution.
Meta-Learning and Transfer-Learning. Due to the massive amount of computational resources
required by the development of modern deep learning applications, the community has started to
explore the possibility of re-using learned parameters across different tasks, leading to the develop-
ment of many new transfer-learning (e.g., Rohrbach et al. (2013), Wang & Schneider (2014), Cui
et al. (2019)) and meta-learning (e.g., Schmidhuber (1987), Hochreiter et al. (2001), Finn et al.
(2017), Zintgraf et al. (2019)) algorithms. The simplest way to transfer knowledge across different
tasks consists in using warm-start initialization. This heuristic is amply used in computer vision ap-
plications, where it is also known as the fine-tuning technique (e.g., Krizhevsky et al. (2012), Yosin-
Ski et al. (2014), Reyes et al. (2015), Kading et al. (2016)). So far, there is no rigorous explanation
of why and when fine-tuning works. However, numerous empirical evaluations on different bench-
marks show that warm-starting the parameters of deep models often leads to faster convergence and
better generalization than using random initialization.
3	Background
In this work, we will focus on solving problems of the form
1N
θ* ∈ arg min — T 'j ⑼,
θ∈Rd N
j=1
X--V----}
:=J (θ)
(1)
where J : Rd → R is our target objective function and θ* is a minimizer. Problems as described
in (1) arise, for instance, in classification and regression scenarios.
In the following section we briefly review the main concepts of homotopy and continuation methods,
which the proposed technique to solve problem (1) is based on.
2
Published as a conference paper at ICLR 2020
3.1	Homotopic Functions and Continuation Methods for Optimization
Given two topological spaces Z and Y , a homotopy is a continuous deformation between two con-
tinuous functions g, f : Z → Y that fulfills certain properties. We can formalize this concept with
the following definition
Definition 3.1. Let g, f : Z → Y be continuous maps on the topological spaces Z, Y. A homotopy
from g to f is a continuous function H : Z × [0, 1] → Y such that
H(z, 0) = g(z) ,
H(z, 1) = f(z) ,
∀z ∈ Z .
(2)
If such function H exists, g is said to be homotopic of f, and this relation is denoted by g ' f.
It is straightforward to show that, A ⊆ Rn being a convex set, any two continuous maps g, f :
Z → A are homotopic (see (Suciu, 2016) for a derivation). From this fact it follows that any two
continuous and real functions are homotopic. See Figures 4a- 4b in the appendix for a graphical
representation of two different homotopy maps between the probability density functions of two
Gaussian distributions, where λ ∈ [0, 1] denotes the homotopy parameter. See also Section A in the
appendix for details on some of the main properties of homotopic functions.
Continuation methods (also known as homotopy methods) are a widely used mathematical tool to
solve complex non-convex optimization problems where no or only very limited prior knowledge
regarding the localization of optimal solutions is available (see (Allgower & Georg, 1980) for a
full characterization of continuation methods). The core idea of a homotopy approach consists in
defining a homotopy function H(θ, λ) with λ ∈ [0, 1] such that H(θ, 0) = J0(θ) is a trivial to
optimize smooth map (or a smooth map of which a surrogate θ0 of an optimal solution is available)
and H(θ, 1) = J(θ) is our target objective function. Instead of directly addressing problem (1), we
approximately and sequentially solve γ > 0 parametric optimization problems of the form
1N
θi ∈ arg min ʊ Y
θ∈Rd N
(3)
-.=H(θ,λi)
for increasing values of the parameter λi for i = 1, . . . , γ and warm-starting each problem with the
previously derived approximate solution. Conceptually, Algorithm 1 describes the basic steps of a
general homotopy algorithm. Under appropriate assumptions, if the increment ∆λ is sufficiently
small, then the iterative procedure in Algorithm 1 will converge to a neighborhood of an optimal
solution of the target objective J that depends in some sense on the number of iterations k > 0
performed (Allgower & Georg, 1980). Many different variations of Algorithm 1 exist. In particular,
Algorithm 1 A Conceptual Homotopy Algorithm
1:	θo ≈ θ0 ∈ argmi∏θ H(θ, 0)
2:	γ > 0 , γ ∈ Z
3:	λo = 0, ∆λ = 1∕γ
4:	k > 0 , k ∈ Z
5:	for i = 1, . . . , γ do
6:	λi - λi-1 + ∆λ
7:	procedure θi -ITERATIVESOLVER(θi-ι, k,H(θ, λi))
8:	return θγ
different update schemes for the homotopy parameter can be adopted (e.g., geometric or sublinear
rate of increase), various iterative solvers can be used under distinct and specific assumptions, and,
finally, also diverse levels of approximation for the solutions θ* can be considered, i.e. different k
values.
Before going into the details of two concrete formulations of the conceptual homotopy method
outlined in Algorithm 1 (see Section 5) when applied to transfer optimality knowledge in regression
and classification scenarios, we provide a general theoretical analysis in a simplified setting.
3
Published as a conference paper at ICLR 2020
4	Theoretical Analysis
In this section, we provide a local theoretical analysis of homotopy methods when Stochastic Gra-
dient Descent (SGD) (Bottou, 2010) is used as iterative solver in Algorithm 1. The locality of the
analysis consists in the definition of hyperspheres of radius B ≥ 0 around the optimal solutions of
each homotopy problem H(θ, λi) where it is possible to exploit certain structures of the problem.
In particular, we approximately and sequentially solve γ > 0 unconstrained optimization problems
of the form
θ* ∈ arg min H(θ, λi), ∀i = 1,...,γ,	(4)
θ∈Rd
where H(θ, λi) fulfills the assumptions described in Section 4.1 and λi ∈ [0, 1]. Let θi be an ap-
proximate solution of the problem associated with parameter λi derived by applying k > 0 iterations
of SGD (in the limit, k = 1) and also the starting point for the problem associated with parameter
λi+1, ∀i = 1, . . . , γ - 1. In addition, let θ0 denote an approximate solution for the source task, i.e.
λ0 = 0, that is used as initial point for the problem associated with λ1 . In this section we charac-
terize the maximum allowed variation of the homotopy parameter in order for the method to able to
track in expectation an rθ-optimal solution from source to target task.
4.1	Assumptions
We now expose the fundamental assumptions for our general local theoretical analysis on which
all the derivations in Sections 4.2 and 4.3 rely. In addition, throughout the analysis the '-functions
in (3) are implicitly assumed to be differentiable in θ. We start by giving the definition of the regions
around the optimal solutions of the homotopy problems where the analysis is conducted.
Definition 4.1. Given θ* and B ≥ 0, let Bb,θ* be thefollowing set ofvectors
BBq= {θ s.t. kθ - θ"∣≤ B} , ∀i = 0,...,γ.
Assumption 4.2 (local L-smoothness). Assume that there exists a constant L > 0 such that
.. ,~ , ʌ ...	.. ~	ʌ..	~ ʌ
kVθ H (θ, λi) - Vθ H(θ,")∣∣ ≤ Lkθ - θk, ∀θ,θ ∈Bb 储,∀i = 0,...,γ.	(5)
Corollary 4.2.1. IfH is locally L-smooth in θ, then the following inequality holds
H (θ*,λi) - H (θ,λi) ≤ - ɪ kVθ H (θ,λi)k2, ∀θ ∈Bb外 ∀i = 0,...,γ.	(6)
2L
Proof. See Lemma 1.1 in (Gower, 2018) for a proof.	□
Assumption 4.3 (local μ-strong convexity). Assume that there exists μ > 0 such that
H (θ,%) ≥ H (θ,%) + Vθ H(θ,%)τ (θ - θ) + 2IB - θk2, ∀θ, θ ∈Bb,θq ∀i = 0,...,γ. (7)
Assumption 4.4 (bounded `-derivative). Assume that there exists ν > 0 such that
_____	.	, O .	. . O . _	__
∣∣Vθ 'j (θ, λi)k≤ ν,	∀θ ∈ BB 盟,∀i = 0,...,γ,	∀j	= 1,...,N.	(8)
Assumption 4.5 (local bounded “variance”). Let g(θ, λi) denote an unbiased estimate of the gra-
dient VθH(θ, λi). Assume that there exists a constant C ≥ 0 such that the following bound on the
expected squared norm of the estimate of the gradient holds
E h∣g(θ, λi)∣2i ≤ C 2, ∀θ ∈ Bb,θ* , ∀i = 0,...,γ.	(9)
Remark 4.6. Assumption 4.5 is standard for proving error bounds on SGD iterates (see (Schmidt,
2014)). In addition, notice that, since
E h∣g(θ, λi)∣2i = Var (kg(θ, %)k) + E h∣g(θ, %)k] 2 ,
the C constant is proportional to the variance and the squared expected value of the norm of the
gradient estimate. Therefore, it decreases when the iterates approach a minimizer and by reducing
the noise in the estimate of the gradient. In the limit (i.e. exact gradient and convergence to a
minimizer), C = 0.
4
Published as a conference paper at ICLR 2020
Recall that θ*(λi) ≡ θ*.
Assumption 4.7 (strong regularity). Assume that there exists δ > 0 such that the following inequal-
ity holds
kθ*(λi+ι) - θ*(%)k ≤ δ∣λi+ι - λi∣, ∀i = 0,..., γ - 1.
Remark 4.8. Assumption 4.7 follows directly from the application of the Implicit Function Theorem
by introducing some milder assumptions on the problem structure (see Lemma 2.1.8 in (Allgower &
Georg, 1980)).
4.2	Fundamental Theoretical Preliminaries
Before proceeding with the main theoretical contributions, we extend the existing results in the
literature on global error bounds for the iterates of Stochastic Gradient Descent such that they can
be applied when the underlying assumptions are only required to hold locally. The derived local
error bounds for SGD iterates are used in Proposition 4.11 and Theorem 4.12.
Proposition 4.9. Let θi ∈ Bb,θ* be the starting point for the problem described in (3), and let
θi := θi,0 and θi+1 := θi,k denote the iterate after k > 0 SGD steps, where an SGD step is defined
as
θi,k = θi,k-1 - αg(θi,k-1, λi) .
Under Assumptions 4.2- 4.5 and by setting the batch size 0 < M ≤ N to a value such that
(NNM) ≤ (1-αVd) B with Kd = p(1 — αμ) and the learning rate a to a constant value such that
0 < α ≤ min (泰,1), the following error bound on the iterates holds
α C2
E [kθi+ι — θΓ+ιk2] ≤ (1 — 2αμ)k ∙ E [∣∣θi - θ"∣2] + 不.	(10)
Proof. See Section D in the appendix.	□
Remark 4.10. The expectation in (10) is taken w.r.t. all the random variables, i.e. estimates of the
gradients and initial point θ0, involved in the optimization procedure up to the current i+ 1 iteration
of the algorithm.
4.3	Main Theoretical Contributions
Under the considered assumptions and by exploiting the previously derived results on local error
bounds for SGD iterates, we show that, if the approximate solution θi for the problem with parameter
λi is “sufficiently close" to a minimizer θ* in expectation, i.e. E - θ*k2] ≤ 吟,then, for a
“sufficiently small" change in the homotopy parameter, the same vicinity to a minimizer θ*+ι is
preserved in expectation for the approximate solution θi+1 of the problem with parameter λi+1, i.e.
E [∣∣θi+ι - θ*+1k2] ≤ 吟.In particular, with Theorem 4.12 We characterize the maximum allowed
variation of the homotopy parameter based on the properties of the parametric problems and the
convergence characteristics of the adopted iterative solver, i.e. rate of convergence and number of
iterations.
First, in order to apply the results derived in Theorem 4.12, given a realization of θi ∈ Bb,θ》,we
have to derive the conditions on ∣∣θi - θ*k such that - θ*+ιk ≤ B. In addition, we derive the
necessary conditions in order to apply these results recursively across the iterations of Algorithm 1.
Proposition 4.11. Let θi ∈ BBq and ∣λi — λi+ι∣ ≤ G with 0 ≤ E ≤ B. If ∣∣θi — θ*∣ ≤ B — δe,
then ∣∣θi — θ*+ι∣ ≤ B. Moreover, let Kd =，(1 — αμ) and assume that
(N - M) ≤ (1- Kk)(1- Kd)B
N —	2αν
and
(1 - Kdk)B -
(N - M)	2αν 、
N	(I-Kd))
1
E ≤ δ
Then, after applying k iterations of SGD, we obtain that
kθi+1 - θi+1k ≤ B - δe .
5
Published as a conference paper at ICLR 2020
(11)
(12)
(13)
□
Proof. See Section E.1 in the appendix.	□
See Figure 9 in the appendix for a graphical representation of the results derived in Proposition 4.11,
where the continuous and dashed lines are used to represent the circles of radius B and B - δ,
respectively.
Theorem 4.12. Consider Algorithm 1 with Stochastic Gradient Descent as solver and let k > 0 be
the number ofiterations, 0 < a ≤ min (器,十) be the step size and 0 < M ≤ N be the batch size
such that
(N - M) ≤ (1- Kd)(1- Kd)B
N —	2αν	,
where Kd =，(1 — aμ). For θo ∈ BB-δe,θ* and rθ ∈ R such that
αC2
r2 ≥ Τ、
2μ
then, if E []® — θ*k2] ≤ r2 and ∣λi 一 λi+ι∣ ≤ e, where "= min{e, e} with
r __ _re_ 1 Srθ - αC2∕2μ
e = 一 了 + δy (1 — 2αμ)k ,
the following inequality holds
E [kθi+1 — θ*+1 k2] ≤rθ .
Proof. See Section E.2 in the appendix.
The results derived in Theorem 4.12 show that the homotopy method used in combination with
SGD allows to track in expectation an rθ-optimal solution across the parametric problems for “small
enough" variations of the homotopy parameter, i.e. ∆λ ≤ e. Notice that r can potentially be smaller
than B — δ and has to be bigger than the radius of the “noise-dominant” hypersphere centered at
the minimizers, i.e. 吟 ≥ αC. In particular, by exploiting the local structure of the parametric
problems we derive the maximum allowed variation of the homotopy parameter across the iterations
of Algorithm 1. The derived upper bound is inversely proportional to the strong regularity constant
δand depends on the number of iterations k performed with SGD, such that the more iterations we
perform on each parametric problem the more we are allowed to change the homotopy parameter.
Finally, notice that these results can be applied recursively across the parametric problems.
5	Transferring Optimality via Homotopy Methods
In this section we describe a possible application of homotopy methods to solve supervised regres-
sion and classification tasks. We address the case where deep neural networks are used as models.
We start by introducing the problem framework of supervised learning and then we propose two
different homotopy functions for the regression and classification scenarios, respectively.
5.1	Problem Formulation
Despite the generality of the proposed methodology, in this work we specifically address the su-
pervised learning framework, and, in particular, when the predictive model is constituted by a deep
neural network f(x; θ) parameterized by θ ∈ Rd.
In the supervised learning scenario, independently from the type of task t, we typically dispose of
a training set Dt consisting of N pairs of examples (xj, yj). The goal of the learning process is to
find a value of θ that minimizes an objective function which measures the discrepancy between the
outputs produced by the network y = f (x; θ) and the target outputs y. In particular, the learning
process consists in minimizing the following empirical objective function
J(θ) := N X	'(yj,f(Xj； θ)),
(xj,yj)∈Dt
(14)
6
Published as a conference paper at ICLR 2020
whose non-convexity originates from the high non-convexity of our model f .
In the classical setting, J is chosen based on the KL divergence between the target data distribution
Qx,y, with density qx,y = q(y|x)q(x), and the learned data distribution Px,y (θ), with density px,y =
p(y|x; θ)q(x), where p(y|x; θ) is modeled via a neural network, (Goodfellow et al., 2016). With the
appropriate approximations, this leads to the following form for the objective function
J(θ)
Σ
(xj,yj)∈Dt
q(y|x) log
q(y⑶
p(y∣χ; θ)
(15)
1
N
5.2	Homotopy Functions Across Data Distributions
Finding a value of θ that attains a local minimum of the objective function in (14) is often a hard
optimization task, given the high dimensionality and non-convexity of the problem. In addition,
prior knowledge regarding the localization of the solutions is rarely available. The complexity of
minimizing such functions also depends in some non-trivial way on the task distribution Qx,y that
is addressed (e.g., Ionescu et al. (2016), Zendel et al. (2017)). For some tasks, convergence to a
good approximate solution is achieved after a few epochs, while for other tasks, orders of magnitude
more iterations are required to reach the neighborhood of a solution. In this perspective, different
heuristics have been recently proposed in the attempt of re-using across different data distributions
the prior knowledge gained from approximately solving the learning problem associated with a
certain task. The question whether we could exploit easy-to-solve or already-solved tasks to speed
up and improve the learning of unsolved hard tasks arises. The method we propose in this paper
addresses this question and attempts to do so by using a rigorous and well-established mathematical
framework, with the goal of speeding up the learning process in presence of hard-to-solve tasks.
In the perspective of homotopy methods, this goal can be achieved under some assumptions by
defining a homotopy transformation between starting and target tasks and by following the procedure
described in Algorithm 1. Despite the flexibility and generality of the method, with this work we
only focus on homotopy deformations across different task distributions, but similar transformations
can be applied in numerous different manners that are also worth exploring, e.g., progressively
modifying the architecture of the network or the weights of the objective function terms.
Let S be the source task with training data Ds of pairs (χs,ys)〜Qχs,ys whose good approximate
solution θ* for the minimization of the objective in (14) is available (or cheaply computable), and
let t denote the target task with training data Dt of pairs (xt, yt)〜Qxt,yt whose conditional distri-
bution we aim to learn. We propose two different homotopy deformations from task s to task t for
regression and classification, respectively.
5.2.1	Supervised Regression
In the supervised regression scenario, by modeling the density of the conditional learned distribu-
tion as p(y|x; θ) = N y; f(x, θ), σ2 I and using the approximate KL divergence objective func-
tion described in (15), we recover the mean squared error as minimization criterion. The proposed
homotopy deformation is based on the following equations
yλ∣x = (1 - λ) ys |x + λyt∣x,	(16)
p(yλ∣χ) = N(yλ; f(χ; θ),σ2l).	(17)
Notice that the transformation described in (16) preserves the unimodality of the conditional distri-
bution (see caption of Figures 4a and 4b in the appendix), and, when used in combination with the
objective function defined in Equation (15), leads to the minimization w.r.t. θ of
H(θ, λ) := E(χ,yλ) k(1 - λ) (ys - f (x; θ)) + λ (yt - f (x; θ))『.	(18)
See Figure 6a in the appendix for a graphical representation of this homotopy deformation when
applied to gradually transform a one-dimensional sine wave function with a frequency of 1 radian
into a one-dimensional sine wave function with a frequency of 137 radians. A downside of this
homotopy deformation is that the same support for x is required (the absence of the subscripts s and
t on x stands to indicate that the same realization for xs and xt has to be considered). Alternatively,
it is possible to approximate (16) by using a Gaussian filter (see Figure 6b and Section B in the
appendix).
7
Published as a conference paper at ICLR 2020
5.2.2	Supervised Classification
In the case of supervised classification, by modeling the density of the conditional learned distribu-
tion as p(y|x; θ) = M ultinoulli(y; f(x; θ)), and using the approximate KL divergence objective
function described in (15), we recover the cross-entropy loss function, (Goodfellow et al., 2016). A
possible homotopy deformation for the classification case consists in applying the following trans-
formations
xλ = (1 -λ)xs +λxt,	(19)
yλ∣Xλ = (1 - λ) ys∣Xs + λyt∣xt,	(20)
which corresponds to the use of probabilistic labels. See Figure 8 in the appendix for a graphical
representation of the proposed homotopy deformation. The corresponding label vector for the de-
formed image represented in Figure 8b is y0.5 = [0, 0, 0.5, 0, 0, 0.5, 0, 0, 0, 0], given that λ = 0.5
and that the sampled realizations of xs and xt, represented in Figures 8a and 8c, belong to class 2
and 5, respectively.
6	Experimental Evaluation
In this section, we present some experimental evaluations of homotopy methods when applied to
solve supervised regression and classification tasks. As homotopy functions we adopt the ones
discussed in Section 5.2. We empirically show that homotopy methods outperform random and
warm-start initialization schemes in terms of numerical performance. In particular, when the target
task is complex and/or, in the transfer-learning scenario, when the data distributions are signifi-
cantly different, continuation methods can achieve significant speed-up compared to random and
warm-start initializations. We believe that their superior numerical performance relies on the use of
homotopy functions that progressively deform the data distribution from an easy-to-solve or already-
solved task to the target data distribution. In addition, consistently across all the benchmarks, our
homotopy-based method shows faster convergence than random-initialization and faster or compa-
rable convergence than warm-start initialization. When the source task is “similar” to the target one,
there is indeed no need to gradually vary the λ parameter in Algorithm 1, but it suffices to directly
set it to 1. In this extreme case, our homotopy method boils down to warm-start initialization.
6.1	Regression
For the supervised regression scenario, the problem we address is how to transfer “optimality knowl-
edge” across two tasks that involve regressing from the input to the output of two sine wave functions
with different values of phase ω. Each considered dataset has 10000 samples split across training
and testing, where x and y are defined as follows
X 〜U(0,1), y = sin(ωx) + ε , ε 〜N(0,0.01).	(21)
The goal is to start with an “easy-to-learn” task, i.e. ω ≈ 1 rad, whose optimum is available
by performing only few epochs with a first-order optimizer, e.g. SGD, Adam, and progressively
transfer the “optimality knowledge” to a more complex task, i.e. ω >> 1 rad, by approximately
solving the homotopy problems for increasing values of λ as described in Algorithm 1. We set
ω = 1 rad for our source task distribution, and study the performance of the proposed approach with
homotopy function as described in Equation (16) for different target distributions with ω >> 1 rad.
See Figures 5a and 5b in the appendix for a visualization of the source data distribution with ω = 1
rad and the target data distribution when ω = 137 rad, respectively. The regressor is a feedforward
neural network with 6 hidden layers of 100 units each and relu as activation function. In order to
make the experiments more robust with respect to the choice of the step size α, we use Adam as
optimizer. For the experiments in Figures 1a-1b, Figures 7a-7b in the appendix, and Figure 2a,
we set α = 0.001, γ = 10, k = 200 and then performed an additional 500 epochs on the final
target problem, while for the experiments in Figure 2b, we set γ = 10, k = 300 and performed an
additional 600 epochs on the final target problem. In this last scenario we set α = 0.001 and then
decrease it with a cosine annealing schedule to observe convergence to an optimum. As shown in
Figures 1a_1b, Figures 7a-7b in the appendix, and Figures 2a and 2b, the homotopy method leads
to faster convergence than the considered baselines by preserving the vicinity to an optimal solution
for problems H(θ, λ) across the different λ values. In particular, we achieve a training loss up to
two orders of magnitude better than the considered baselines.
8
Published as a conference paper at ICLR 2020
(a) ωs = 1 rad, ωt = 74 rad.
(b) ωs = 1 rad, ωt = 137 rad.
Figure 1: Median train loss across 100 runs versus epochs for sine wave regression tasks with
different frequency values. The shaded areas represent the 25th and 75th percentiles. See Section F.1
in the appendix for an evaluation of the test performance.
oogzɪUOda-SSo-U-£ U=P3E
(a) Median train loss versus omega values.
(b) ωs = 1 rad, ωt = 137 rad.
Figure 2: Comparison of homotopy method, warm start and random initialization on sine wave
regression tasks. The shaded areas represent the 25th and 75th percentiles. On the left, the median
train loss achieved by the considered methods after 2500 epochs across 100 runs versus different
omega values for the target task is plotted. For the homotopy method and warm-start initialization,
ωs = 1 rad is used. On the right, the median train loss across 100 runs versus epochs for target task
with ω = 137 rad is plotted. With respect to Figure 1b, in Figure 2b a cosine decay schedule is used
for the learning rate, and more epochs are performed to better observe the convergence properties of
the different methods.
6.2	Classification
For the supervised classification scenario, we first apply the continuation method with the homotopy
deformation described in Equations (19) and (20) in order to transfer optimality from the MNIST
task, a notoriously “easy-to-learn” task for neural networks, to the FashionMNIST task. Since the
two datasets have the same input dimensionality and the same number of classes, no additional pre-
processing of the data is required. As network architecture, we use a VGG-type network, (Simonyan
& Zisserman, 2015), and Adam as optimizer with a step size of α = 0.001.
Secondly, we consider CIFAR-10 as target data distribution. Differently from the previous scenario,
padding of the MNIST samples is required in order to apply Equation (19). The MNIST samples are
also replicated across three channels. Also in this case we adopt a VGG-type network, (Simonyan
& Zisserman, 2015), and Adam as optimizer with a step size of α = 0.0001.
As shown in Figures 3a and 3b, in both benchmarks the homotopy method leads to faster con-
vergence than random initialization. While in the second benchmark our method reaches a lower
value of training loss in fewer epochs than warm-start, in the MNIST-to-FashionMNIST case the
performance is comparable to using warm-start initialization. A possible interpretation is that,
when the source and target task distributions are “too similar”, as we hypothesize in the MNIST-
9
Published as a conference paper at ICLR 2020
(a) FashionMNIST.
Figure 3: Median train loss across 10 runs versus epochs for different target task distributions. In
both cases, the source task is the classification of the MNIST dataset. See Section F.2 in the appendix
for an evaluation of the test performance.
(b) CIFAR-10.
to-FashionMNIST scenario, then there is no need for homotopy deformations to be applied, i.e.
0 < λ < 1, but we can directly apply λ = 1 in our scheme, which corresponds to simply using
warm-start initialization.
7	Conclusions
In this paper we propose anew methodology based on homotopy methods in order to transfer knowl-
edge across different task distributions. In particular, our homotopy-based method allows one to
exploit easy-to-solve or already-solved learning problems to solve new and complex tasks, by ap-
proximately and sequentially solving a sequence of optimization problems where the task distribu-
tion is gradually deformed from the source to the target one. We conduct a theoretical analysis of
a general homotopy method in a simplified setting, and then we test our method on some popular
deep learning benchmarks, where it shows superior numerical performance compared to random
and warm-start initialization schemes. The proposed framework, in its limiting case, corresponds
to the widely used fine-tuning heuristic, allowing for a new and more rigorous interpretation of the
latter. Finally, the generality of homotopy methods also opens many novel and promising research
directions in fundamental fields for deep learning, such as stochastic non-convex optimization and
transfer-learning.
Acknowledgments
This work has partly been supported by the European Research Council (ERC) under the European
Union’s Horizon 2020 research and innovation programme under grant no. 716721 as well as by
the German Federal Ministry for Economic Affairs and Energy (BMWi) via DyConPV (0324166B),
and by DFG via Research Unit FOR 2401. In addition, Q. Tran-Dinh has partly been supported by
the National Science Foundation (NSF), grant. no. 1619884. The authors thank Stefan Falkner for
his helpful suggestions and comments.
References
Eugene L. AllgoWer and KUrt Georg. Numerical continuation methods. An introduction. SPringer-
Verlag, 1980.
David Balduzzi, Brian McWilliams, and Tony Butler-Yeoman. Neural taylor aPProximations: Con-
vergence and exPloration in rectifier netWorks. In Proceedings of the 34th International Confer-
ence on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, PP. 351-360,
2017. URL http://proceedings.mlr.press/v70/balduzzi17c.html.
Yoshua Bengio, Jerome Louradour, Ronan Collobert, and Jason Weston. Curriculum learn-
ing. In Proceedings of the 26th Annual International Conference on Machine Learning, ICML
10
Published as a conference paper at ICLR 2020
2009, Montreal, Quebec, Canada, June 14-18, pp. 41-48, 2009. URL https://doi.org/
10.1145/1553374.1553380.
Leon Bottou. Large-scale machine learning with stochastic gradient descent. In Pro-
ceedings of COMPSTAT’2010, pp. 177-186, 2010. URL https://leon.bottou.org/
publications/pdf/compstat-2010.pdf.
Wanyun Cui, Guangyu Zheng, Zhiqiang Shen, Sihang Jiang, and Wei Wang. Transfer learning for
sequences via learning to collocate. In International Conference on Learning Representations,
2019. URL https://openreview.net/pdf?id=ByldlhAqYQ.
Amit Daniely, Roy Frostig, and Yoram Singer. Toward deeper understanding of neural networks:
The power of initialization and a dual view on expressivity. In D. D. Lee, M. Sugiyama,
U. V. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing
Systems 29, pp. 2253-2261, 2016. URL http://papers.nips.cc/paper/6427-
toward- deeper- understanding- of- neural- networks- the- power- of-
initialization-and-a-dual-view-on-expressivity.pdf.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine Learning, 12:2121-2159, 2011. URL http:
//www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf.
Soheil Feizi, Hamid Javadi, Jesse Zhang, and David Tse. Porcupine neural networks:
Approximating neural network landscapes. In S. Bengio, H. Wallach, H. Larochelle,
K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Informa-
tion Processing Systems 31, pp. 4831-4841. Curran Associates, Inc., 2018. URL
https://papers.nips.cc/paper/7732-porcupine-neural-networks-
approximating-neural-network-landscapes.pdf.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast
adaptation of deep networks. In Doina Precup and Yee Whye Teh (eds.), Pro-
ceedings of the 34th International Conference on Machine Learning, volume 70, pp.
1126-1135. PMLR, 2017. URL http://proceedings.mlr.press/v70/finn17a/
finn17a.pdf?Source=post _page-------------------------------.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. URL
http://www.deeplearningbook.org.
Robert M. Gower. Convergence theorems for gradient descent, 2018. URL https://
perso.telecom-paristech.fr/rgower/pdf/M2,statistique_optimisation/
grad_conv.pdf.
Alex Graves, Marc G. Bellemare, Jacob Menick, Remi Munos, and Koray Kavukcuoglu. Automated
curriculum learning for neural networks. In Proceedings of the 34th International Conference on
Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pp. 1311-1320, 2017.
URL http://proceedings.mlr.press/v70/graves17a.html.
Caglar Gulcehre, Marcin Moczulski, Misha Denil, and Yoshua Bengio. Noisy activation functions.
arXiv 1603.00391, 2016. URL https://arxiv.org/pdf/1603.00391.pdf.
Caglar Gulcehre, Marcin Moczulski, Francesco Visin, and Yoshua Bengio. Mollifying net-
works. In International Conference on Learning Representations, 2017. URL https://
openreview.net/pdf?id=r1G4z8cge.
Guy Hacohen and Daphna Weinshall. On the power of curriculum learning in training deep
networks. In Proceedings of the 36th International Conference on Machine Learning, ICML
2019, 9-15 June 2019, Long Beach, California, USA, pp. 2535-2544, 2019. URL http:
//proceedings.mlr.press/v97/hacohen19a.html.
Boris Hanin and David Rolnick. How to start training: The effect of initialization and architec-
ture. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Gar-
nett (eds.), Advances in Neural Information Processing Systems 31, pp. 571-581. Curran As-
sociates, Inc., 2018. URL http://papers.nips.cc/paper/7338-how-to-start-
training-the-effect-of-initialization-and-architecture.pdf.
11
Published as a conference paper at ICLR 2020
Sepp Hochreiter, A. Steven Younger, and Peter R. Conwell. Learning to learn using gradient descent.
In Georg Dorffner, Horst Bischof, and Kurt Hornik (eds.), Artificial Neural Networks — ICANN
2001, pp. 87-94. Springer Berlin Heidelberg, 2001. URL https://link.Springer.com/
Chapter/10.10 07/3-54 0-4 4 668-0_13.
R. Tudor Ionescu, Bogdan Alexe, Marius Leordeanu, Marius Popescu, Dim P. Papadopoulos, and
Vittorio Ferrari. How hard can it be? Estimating the difficulty of visual search in an image. In
2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2157-2166,
2016. URL https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=
7780606&tag=1.
Diederik P. Kingma and Jimmy Ba. Adam: a method for stochastic optimization. In Proc. 3rd
International Conference for Learning Representations, 2015. URL http://arxiv.org/
abs/1412.6980.
Gunter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-normalizing
neural networks. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish-
wanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp.
971-980. Curran Associates, Inc., 2017. URL https://papers.nips.cc/paper/6698-
self-normalizing-neural-networks.pdf.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convo-
lutional neural networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger (eds.),
Advances in Neural Information Processing Systems 25, pp. 1097-1105. Curran Associates, Inc.,
2012. URL http://papers.nips.cc/paper/4824-imagenet-classification-
with-deep-convolutional-neural-networks.pdf.
Daniel Kunin, Jonathan M. Bloom, Aleksandrina Goeva, and Cotton Seed. Loss landscapes of
regularized linear autoencoders. In Proceedings of the 36th International Conference on Machine
Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, pp. 3560-3569, 2019. URL
http://proceedings.mlr.press/v97/kunin19a.html.
Christoph Kading, Erik Rodner, Alexander Freytag, and Joachim Denzler. Fine-tuning deep neural
networks in continuous learning scenarios. In ACCV Workshop on Interpretation and Visualiza-
tion of Deep Neural Nets, 2016.
Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss land-
scape of neural nets. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and
R. Garnett (eds.), Advances in Neural Information Processing Systems 31, pp. 6389-6399. Curran
Associates, Inc., 2018. URL http://papers.nips.cc/paper/7875-visualizing-
the-loss-landscape-of-neural-nets.pdf.
Hossein Mobahi. Training recurrent neural networks by diffusion. arXiv 1601.04114, 2016. URL
https://arxiv.org/pdf/1601.04114.pdf.
Sanmit Narvekar. Curriculum learning in reinforcement learning. In International Joint Conference
on Artificial Intelligence, 2017. URL https://www.ijcai.org/Proceedings/2017/
0757.pdf.
Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In Inter-
national Conference on Learning Representations, 2018. URL https://openreview.net/
forum?id=ryQu7f-RZ.
Angie K. Reyes, Juan C. Caicedo, and Jorge E. Camargo. Fine-tuning deep convolutional networks
for plant recognition. In Linda Cappellato, Nicola Ferro, Gareth J. F. Jones, and Eric SanJuan
(eds.), Working Notes of CLEF 2015 - Conference and Labs of the Evaluation forum, Toulouse,
France, September 8-11, 2015, volume 1391 of CEUR Workshop Proceedings. CEUR-WS.org,
2015. URL http://ceur-ws.org/Vol-1391/121-CR.pdf.
Marcus Rohrbach, Sandra Ebert, and Bernt Schiele. Transfer learning in a transductive set-
ting. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger (eds.),
Advances in Neural Information Processing Systems 26, pp. 46-54. Curran Associates, Inc.,
2013. URL http://papers.nips.cc/paper/5209-transfer-learning-in-a-
transductive-setting.pdf.
12
Published as a conference paper at ICLR 2020
Jurgen Schmidhuber. Evolutionary principles in self-referential learning. On learning now to learn:
The meta-meta-meta…-hook. Diploma thesis, Technische Universitat Munchen, Germany, May
1987. URL http://people.idsia.ch/ 〜juergen/diploma.html.
Mark Schmidt. Convergence rate of stochastic gradient with constant step size,
July 2014.	URL	https://www.cs.ubc.ca/~schmidtm/Documents/
2014_Notes_ConstantStepSG.pdf.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning
Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceed-
ings, 2015. URL http://arxiv.org/abs/1409.1556.
Alexandru I. Suciu. Lecture notes in topology, February 2016. URL www.northeastern.edu/
suciu/MATH4565/utop.sp16.html.
Quoc Tran-Dinh, Carlo Savorgnan, and Moritz Diehl. Adjoint-based predictor-corrector sequential
convex programming for parametric nonlinear optimization. SIAM Journal on Optimization, 22
(4):1258-1284, 2012.
Xuezhi Wang and Jeff Schneider. Flexible transfer learning under support and model shift.
In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger (eds.),
Advances in Neural Information Processing Systems 27, pp. 1898-1906. Curran Associates,
Inc., 2014. URL http://papers.nips.cc/paper/5632- flexible-transfer-
learning-under-support-and-model-shift.pdf.
Daphna Weinshall, Gad Cohen, and Dan Amir. Curriculum learning by transfer learning: Theory and
experiments with deep networks. In Proceedings of the 35th International Conference on Machine
Learning, ICML20l8, Stockholmsmassan, Stockholm, Sweden, July 10-15,2018,pp. 5235-5243,
2018. URL http://proceedings.mlr.press/v80/weinshall18a.html.
Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in
deep neural networks? In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and
K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 27, pp. 3320-
3328. Curran Associates, Inc., 2014. URL http://papers.nips.cc/paper/5347-how-
transferable-are-features-in-deep-neural-networks.pdf.
Andrea Zanelli, Quoc Tran-Dinh, and Moritz Diehl. Contraction estimates for abstract real-time
algorithms for NMPC. In Proceedings of the IEEE Conference on Decision and Control, Nice,
France, 2019.
Oliver Zendel, Katrin Honauer, Markus Murschitz, Martin Humenberger, and Gustavo
Fernandez Dominguez. Analyzing computer vision data - the good, the bad and the ugly.
In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6670-
6680, 2017. URL http://openaccess.thecvf.com/content_cvpr_2 017/papers/
Zendel_Analyzing_Computer_Vision_CVPR_2 017_paper.pdf.
Luisa M. Zintgraf, Kyriacos Shiarlis, Vitaly Kurin, Katja Hofmann, and Shimon Whiteson. Cavia:
Fast context adaptation via meta-learning. In Proceedings of the 36th International Conference on
Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, pp. 7693-7702,
2019. URL http://proceedings.mlr.press/v97/zintgraf19a.html.
13
Published as a conference paper at ICLR 2020
A Properties of Homotopic Functions
Among the numerous properties of homotopic functions, we recall the following ones
Proposition A.1. Suppose that there exists a homotopy H : Z × [0, 1] → Y from g to f, i.e. g ' f.
Then
•g'g
(reflexive property)
•	g ' f =⇒ f ' g
•	g ' f and f ' h =⇒ g ' h
(symmetric property)
(transitive property)
Proof. See proof of Theorem 1.5 in (SUciu, 2016).	□
Proposition A.2. Letg, g0 : Z → Y and f, f0 : Y → W be continuous maps, and let f ◦g, f0 ◦g0 :
Z → W be the respective composite maps. Ifg ' g0 and f ' f0, then f ◦ g ' f0 ◦ g0.
Proof. See proof of Proposition 1.7 in (SUciu, 2016).	□
B	Approximation via Gaussian Filter
For the sUpervised regression scenario, we propose the following homotopy deformation
yλ∣x = λys∣x + (1 - λ) yt|x.	(22)
A downside of this homotopy fUnction is that the same sUpport for x is reqUired (the absence of the
sUbscripts s andt on x stands to indicate that the same realization for xs and xt has tobe considered).
Alternatively, it is possible to approximate EqUation (22) by Using a GaUssian filter, as depicted in
FigUre 6b.
In particUlar, having sampled one realization z of the pair (xs , ys) from the training set Ds, 0 <
MGF ≤ N realizations of the pair (xt, yt) are sampled from Dt. Each yt,j realization is then
weighted based on the vicinity of xt,j to the sampled xs,z realization. This leads to the following
approximation of the z realization of yλ
λ	MGF
yλ,z = (I - λ) ys,z + M^ wjy Wj ytj ,
(23)
(24)
jp (-
2∏ξ2
||xs,z - Xtj ||2
2ξ2
where ξ > 0 is the standard deviation of the GaUssian filter.
14
Published as a conference paper at ICLR 2020
C Additional Figures
(a) Homotopy 1.
(b) Homotopy 2.
Figure 4: Two different homotopy deformations between the probability density functions of two
one-dimensional Gaussian distributions with mean and standard deviation given by μι = 1, σι = 1
and μ2 = 5, σ2 = 0.1, respectively. The homotopy represented in Figure 4a results in a mixture of
Gaussian distributions, with mixture coefficient given by the homotopy parameter λ. In Figure 4b
the deformation concerns instead the parameters μ and σ of the original distributions. Preserving
unimodality is a desirable property when the homotopy function is used in combination with a
continuation method since, as shown in Figure 4b, the location of the optimum moves together with
the function deformation, allowing the optimizer to track it and gradually reach the optimum of the
final target task. On the contrary, deforming the function as shown in Figure 4a does not lead to
a gradual shift of the optimal solutions. Consequently, approximately and sequentially solving the
problems corresponding to intermediate values of the homotopy parameter λ, i.e. 0 < λ < 1, will
not allow the homotopy method to gradually approach the desired final optimal solution.
l∙+(x∙3)U-∙,
-1.0-	1
0.0
(b)ω = 137 rad.
Figure 5: Graphical representations of the source (left) and target with ω = 137 rad (right) data
distributions used for the sine-wave regression evaluation.
(a) ω = 1 rad.
15
Published as a conference paper at ICLR 2020
(a)	Homotopy transformation described in
Equation (16).
(b)	Approximation of the homotopy transfor-
mation in Equation (16) (also Equation (22))
with a Gaussian filter as described in Equa-
tions (23) and (24).
Figure 7: Median train loss across 100 runs versus epochs for sine wave regression tasks with
different omega values.
Figure 6: Graphical representation of the proposed homotopy transformation for the supervised
regression scenario when applied to progressively deform a sine wave function with frequency of
1 radian into a sine wave function with frequency of 137 radians for different values of homotopy
parameter.
(b) ωs = 1 rad, ωt = 116 rad.
(a)	Sampled image of an
handwritten digit 2 (class 2)
from the MNIST dataset.
(b)	Homotopy deformation
of the images represented
in Figures 8a and 8c corre-
sponding to λ = 0.5.
(c)	Sampled image of a
sandal (class 5) from the
FahionMNIST dataset.
Figure 8: Graphical representation of the homotopy transformation from xs to xt as described in
Equation (19) for two sampled images from the MNIST and FashionMNIST datasets.
16
Published as a conference paper at ICLR 2020
D Local Error Bounds for SGD Iterates
Before proving local error bounds for SGD iterates in the considered framework, given the local
nature of our assumptions, we need to demonstrate two important facts, on which the proof relies.
In particular, we need to show:
•	local linear contraction of Gradient Descent (GD) iterates, and that
•	starting in a hypersphere of radius B around a minimizer and given a “big enough” batch
size, the next SGD iterate is also contained in this region for all possible realizations of the
gradient estimate.
Considering problem (4) with fixed parameter λi , in the following subsections we will refer to
θ* = θ*, θk = θi,k and gk = g(θk, λi), where We drop the subscript i and the explicit dependence
on λi in order to simplify the notation. The analysis holds for all fixed parameters λi.
D. 1 Local Linear Contraction of GD Iterates
Let us use GD to solve the following optimization problem
θ* ∈ arg min H(θ, λi),
θ
where the objective function H fulfills Assumptions 4.2 and 4.3.
We now derive error bounds on the iterates of GD
θk+ι = θk - αVθH(θk,λi),
where θk ∈ Bb,θ* and 0 < a ≤ L is the step size.
We start by applying the definition of GD iterates and then we exploit the introduced assumptions
kθk+ι - θ*k2 = kθk - αVθH(θk,λi) - θ*k2
=kθk - θ*∣∣2 - 2αVθH(θk, λi)T(θk - θ*) + α2kVθH限,")『
strong convexity
≤
corollary 4.2.1
≤
(1 - αμ)∣∣θk - θ*∣∣2 - 2α(H(θk,%)- H(θ*,%))+ α2∣MH®,%)∣∣2
(1 - αμ)kθk - θ*k2 - 2α(1 - αL)(H(θk, λi) - H(θ*, λi)).
Since H(θk, λi) - H(θ*, λi) ≥ 0 and -2α(1 - αL) ≤ 0 when 0 < α ≤ *, we can safely drop the
second term and obtain the final result
kθk+1- θ*k2 ≤ (1 - αμ)kθk - θ*k2.
See also Theorem 2.3 in (Gower, 2018) for a derivation where Assumptions 4.2 and 4.3 are required
to hold globally.
D.2 Realization of the SGD Iterates in the Strong Convexity and
L-Smoothness Region around a Minimizer
We address the following optimization problem
1N
θ* ∈ arg min 初£' (θ,λi)
j=1
'--------{z---
-. = H(θ,λi)
}
where H fulfills Assumptions 4.2— 4.4.
As proved in Section D.1, under Assumptions 4.2 and 4.3, whenever θo ∈ Bb,θ* and 0 < α ≤ L,
deterministic gradient descent iterates converge linearly with contraction rate Kd := a∕(1 - αμ).
In particular, the following inequality holds
kθk+ι-θ*k ≤ Kd∙kθk-θ*k ,
17
Published as a conference paper at ICLR 2020
for any θk such that k θk - θ*k ≤ B, and superscript D denotes iterates obtained by applying the full
gradient VHk := VH(θk, λi)
θD+ι = θk - avHk .
Let θk+1 denote the iterate obtained by applying one iteration of stochastic gradient descent
θk+1 = θk - αgk ,
where gk := M Pj∈M V'j(θk, λi) and M is a set of 0 < M ≤ N indexes randomly sampled
from N = {1, . . . , N}.
Given any realization of θk s.t. ∣∣θk - θ*k ≤ B and any realization of gk, by exploiting Assump-
tion 4.4 and the results derived in Section D.1, we have that
kθk+1 - θ* k = kθk - αgk - θ* k
=kθk - αVHk + αVHk - agk - θ*k
≤ kθk - αVHk - θ*k+ αkVHk - gkk
=kθk- aVHk- θ*k+α∣∣N x V'j+N x V'j - M x V'j||
j∈N∖M	j∈M	j∈M
=kθk- aVHk- θ*k+α∣∣N X v'- MNMNX vMI	K
j∈N∖M	j∈M	(25)
≤	kθk - αVHk-θ*k + α I N X kV'jk + NNMM X kV'jk ∣
j∈N ∖M	j∈M
≤	kθD+ι-θ*k +2α―V
≤	κdkθk - θ*k +2α (N NM) ν.
Since we have assumed that the current realization of θk lies in the hypersphere of radius B around
the optimal solution θ*, by solving for NNM the following inequality
(N-M)
Kd B + 2a--N一-ν ≤ B ,
We obtain that, whenever (NNM) ≤ (1-7) B, the realization of θk+ι will also lie in this region.
These derivations show that when the realization of the current iterate θk lies in the hypersphere
of radius B around the minimizer θ*, and (NNM) ≤ (1-}) B, then the next iterate θk+1 will also
lie in this region. Consequently, in our scenario, if we assume that the initial point θ0 lies in the
hypersphere of radius B around the minimizer θ*, then, by applying the derivations recursively,
we can show that the iterates will remain in this local region around the minimizer where strong
convexity and smoothness hold.
D.3 Proof of Proposition 4.9
Let us use SGD to solve the following optimization problem
θ* ∈ arg min H(θ, λi),
θ
where the objective function H fulfills Assumptions 4.2— 4.4. We now derive error bounds for the
iterates of SGD
θk+1 = θk - αgk ,
where gk is the unbiased estimate of VHk defined in the previous section and fulfills Assump-
tion 4.5, θk ∈ Bb,θ* , 0 < a ≤ min (击,十) is the step size and the batch size is set to a value M
such that (NNM) ≤ (⅛κd)B.
N	2αν
18
Published as a conference paper at ICLR 2020
We start by applying the definition of SGD iterates
kθk+ι-θ*k2 SGD=eratekθk-αgk-θ*k2
=kθk - θ*k2 - 2αgT (θk - θ*) + α2 kgk 112 .
We now take the expectation w.r.t. θ0, g0, . . . , gk-1, gk and, considering Assumptions 4.2- 4.5, we
obtain the following series of inequalities
Eθ0,g0,...,gk-1,gk [kθk+1-θ*k2] = Eθ0,g0,...,gk-1,gk [∣∣θk-θ*k2- 2αgT(θk- θ*)
+α2kgkk2
law0fiterat=expectati0ns Eθ0,g0,...,gk-1 [Egk [I® —叫2
-2αgT(θk — θ*) + α2kgkk2 | θo, g0, ..., gk-l]]
unbiased gk +bounded “variance”
≤	Eθ0,g0,..,gk-1 [kθk-θ*k2
-2αVHT (θk — θ*)] + α2C2
strong convexity
≤ (I- 2αμ) ∙ Eθo,g0 ,...,gk-1 [kθk-θ*k2] +α2C2 .
By applying this result recursively, we derive the following bound on the error for the SGD iterates
αC2
Eθ0,g0,...,gk-1,gk [kθk+ι - θ*k2] ≤ (1 — 2αμ)k+1 ∙ Eθo [I% - θ*k2] + α2μ-.
See also Section 3 in (Schmidt, 2014) for a derivation where Assumptions 4.2 and 4.3 are required
to hold globally.
E Main Theoretical Contributions
E.1 Proof of Proposition 4.11
Proposition E.1. Let θi ∈ Bb,θ* and ∣λi — λi+ι∣ ≤ e, with 0 ≤ E ≤ B. If ∣∣θi — θ*∣ ≤ B — δe,
then ∣∣θi — θ*+ι∣ ≤ B. Moreover, let Kd =，(1 - αμ) and assume that
(N - M) ≤ (1 — Kk)(1 — Kd)B
N
2αν
and
1	k (N — M)	2αν
E≤δ((I-Kd)B -	Ldy).
Then, after applying k iterations of SGD, we obtain that
kθi+1 — θi+1k ≤ B — δE .
Proof.
kθi — θi+ιk = kθi — θi + θ∙T-θ兀ιk
Triangle Ineq.
≤
Assumption 4.7
≤
kθi-θ" + kθ* — θ*+ιk
kθi — θi k + δ∣λi — λi+1 | .
Finally, using the fact that ∣λi — λi+ι | ≤ e, it follows that, if ∣∣θi — θ"∣ ≤ B — δe with 0 ≤ E ≤ B,
then ∣θi — θ*+ι∣ ≤ B.
We now derive the conditions on E such that ∣θi+ι — θ*+ι∣ ≤ B — δe. By applying recursively the
results derived in Section D.2 (25), we obtain that
kθi+ι 一 θ"∣ ≤ κk∣θi — θ*+ιk + 2α(NNMIV X Kd .
i=0
19
Published as a conference paper at ICLR 2020
By using the limit of the geometric series, we have that
kθi+ι- θ"l≤ Kdkθi-%ιk + (NNM) (12ανd)
Finally, by considering that |陶一θ"∣∣ ≤ B and by solving in e the following inequality
κkdB +
(N-M) } ≤ B - δe
N (1 - Kd)
we obtain the following upper bound on e
1
e ≤ δ
(1 - Kdk)B -
(N - M)	2αν ∖
N	(I-Kd)J
from which also the extra condition on the batch size
(N - M) ≤ (1- Kk)(1- Kd)B
N —	2αν
□
Figure 9: Graphical representation of the results derived in Proposition 4.11. The continuous and
dashed lines are used to represent the circles of radius B and B - δe around the optimal solutions,
respectively.
E.2 Proof of Theorem 4.12
Theorem E.2. Consider Algorithm 1 with Stochastic Gradient Descent as solver and let k > 0 be
the number ofiterations, 0 < a ≤ min (/,be the step size and 0 < M ≤ N be the batch size
such that
(N - M) ≤ (1- Kk)(1- Kd)B
N —	2αν ,
where Kd =，(1 — aμ). For θo ∈ Bβ-δε,θ^ and rθ ∈ R such that
2	αC 2
rθ ≥ k
then, if E [∣∣θi — θ*k2] ≤ r2 and ∣λi — λi+ι∣ ≤ e, where e := min {e, e} with
(26)
rθ 1 ∕rθ - αC72μ
e = -9 + δV (1- 2αμ)k ,	()
the following inequality holds
E [kθi+ι - θ+ k2] ≤rθ .	(28)
20
Published as a conference paper at ICLR 2020
Proof.
Ineq. 10	aC2
E [kθi+ι - θ*+1k2]	≤ (1 - 2α4)kE [帆-θ"∣2] + -2μ-
,一	…αC2
=(1 - 2αμ)kE [∖∖θi - θ↑ + θ↑ - θ*+1k2] + -2-
Triangle Ineq.	α	αC2
≤	(1 - 2αμ)kE [(∣∣θi - θ"∣ + ∖θ* - θ*+1∖)2] + -2μ-
=(1 - 2-μ)kE[(∣∣θi-θ"∣2 + I∣θ.θ"∣2
一	-C2
+2∖θi-θ 刘网-θ*+1∖)] + -2μ-
Assumption 4.7
≤	(1 - 2-μ) E [(∖θi - θ*∖ + δ ∣λi - λi+ι∣
-C 2
+2δ∖θi - θi ∖∣λi - λi+1∣)] +	-
2μ
-C2
≤ (1 - 2-μ)k (δ2e2 + 2δrθe + 币 + -μ-.
We now solve in e the following second degree inequality
(1 - 2-μ)k (δ2e2 + 2δrθI + r^) + -C- ≤ r2 .
(29)
The inequality (29) admits solutions if and only if rj ≥ OI2. In particular, inequality (29) holds
VW ∈ [0,也 where e =-臂 + 1，彳-Xff"
□
21
Published as a conference paper at ICLR 2020
F Experimental Evaluation: Test Performances
F.1 Regression
(a) ωs = 1 rad, ωt = 74 rad.
(c) ωs = 1 rad, ωt = 116 rad.
Figure 10: Median test loss across 100 runs versus epochs for target tasks with different ω values.
The shaded areas represent the 25th and 75th percentiles. For warm-start initialization and homotopy
method, ωs = 1 rad is used for the source task.
(d) ωs = 1 rad, ωt = 137 rad.
F.2 Classification
	Method		Final Mean Test Accuracy	Best Mean Test Accuracy
homotopy Y = 5, k = 2	0.89 ± 0.003	0.91 ± 0.002
homotopy Y = 5, k = 4	0.89 ± 0.002	0.91 ± 0.003
homotopy Y = 10, k = 1	0.89 ± 0.004	0.91 ± 0.001
homotopy Y = 10, k = 4	0.90 ± 0.002	0.91 ± 0.003
warm start	0.89 ± 0.003	0.90 ± 0.002
random init		0.89 ± 0.004		0.90 ± 0.003
Table 1: MNIST-FashionMNIST
Method	Final Mean Test Accuracy	Best Mean Test Accuracy
homotopy Y = 5, k = 2	0.55 ± 0.004	0.59 ± 0.003
homotopy Y = 10, k = 1	0.55 ± 0.005	0.60 ± 0.002
homotopy Y = 10, k = 2	0.56 ± 0.003	0.60 ± 0.003
homotopy Y = 10, k = 4	0.56 ± 0.005	0.61 ± 0.004
warm start	0.54 ± 0.006	0.59 ± 0.005
random init		0.64 ± 0.02			0.64 ± 0.02	
Table 2: MNIST-CIFAR-10
22