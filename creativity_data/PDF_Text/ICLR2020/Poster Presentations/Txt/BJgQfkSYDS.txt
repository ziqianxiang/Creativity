Published as a conference paper at ICLR 2020
Neural Policy Gradient Methods:
Global Optimality and Rates of Con-
VERGENCE
Lingxiao Wang* * * §t	Qi Cai哺 Zhuoran Yang§ Zhaoran Wang,
Ab stract
Policy gradient methods with actor-critic schemes demonstrate tremendous empir-
ical successes, especially when the actors and critics are parameterized by neural
networks. However, it remains less clear whether such “neural” policy gradient
methods converge to globally optimal policies and whether they even converge at
all. We answer both the questions affirmatively under the overparameterized two-
layer neural-network parameterization. In detail, assuming independent sampling,
we prove that neural natural policy gradient converges to a globally optimal pol-
icy at a sublinear rate. Also, we show that neural vanilla policy gradient converges
sublinearly to a stationary point. Meanwhile, by relating the suboptimality of the
stationary points to the representation power of neural actor and critic classes, we
prove the global optimality of all stationary points under mild regularity condi-
tions. Particularly, we show that a key to the global optimality and convergence
is the “compatibility” between the actor and critic, which is ensured by sharing
neural architectures and random initializations across the actor and critic. To the
best of our knowledge, our analysis establishes the first global optimality and con-
vergence guarantees for neural policy gradient methods. 1
1	Introduction
In reinforcement learning (Sutton and Barto, 2018), an agent aims to maximize its expected total
reward by taking a sequence of actions according to a policy in a stochastic environment, which
is modeled as a Markov decision process (MDP) (Puterman, 2014). To obtain the optimal policy,
policy gradient methods (Williams, 1992; Baxter and Bartlett, 2000; Sutton et al., 2000) directly
maximize the expected total reward via gradient-based optimization. As policy gradient methods
are easily implementable and readily integrable with advanced optimization techniques such as vari-
ance reduction (Johnson and Zhang, 2013; Papini et al., 2018) and distributed optimization (Mnih
et al., 2016; Espeholt et al., 2018), they enjoy wide popularity among practitioners. In particular,
when the policy (actor) and action-value function (critic) are parameterized by neural networks, pol-
icy gradient methods achieve significant empirical successes in challenging applications, such as
playing Go (Silver et al., 2016; 2017), real-time strategy gaming (Vinyals et al., 2019), robot manip-
ulation (Peters and Schaal, 2006; Duan et al., 2016), and natural language processing (Wang et al.,
2018). See Li (2017) for a detailed survey.
In stark contrast to the tremendous empirical successes, policy gradient methods remain much less
well understood in terms of theory, especially when they involve neural networks. More specifi-
cally, most existing work analyzes the REINFORCE algorithm (Williams, 1992; Sutton et al., 2000),
which estimates the policy gradient via Monte Carlo sampling. Based on the recent progress in non-
convex optimization, Papini et al. (2018); Shen et al. (2019); Xu et al. (2019a); Karimi et al. (2019);
* equal contribution
,Northwestern University; lingxiaowang2 0 22@u.northwestern.edu
^NorthWestern University; qicai2 0 22@u.northwestern.edu
§Princeton University; zy6@princeton.edu
^Northwestern University; zhaoranwang@gmail.com
1See https://arxiv.org/abs/1909.01150 for the full version.
1
Published as a conference paper at ICLR 2020
Zhang et al. (2019) establish the rate of convergence of REINFORCE to a first- or second-order
stationary point. However, the global optimality of the attained stationary point remains unclear.
A more commonly used class of policy gradient methods is equipped with the actor-critic scheme
(Konda and Tsitsiklis, 2000), which alternatingly estimates the action-value function in the policy
gradient via a policy evaluation step (critic update), and performs a policy improvement step using
the estimated policy gradient (actor update). The global optimality and rate of convergence of such
a class are even more challenging to analyze than that of REINFORCE. In particular, the policy
evaluation step itself may converge to an undesirable stationary point or even diverge (Tsitsiklis
and Van Roy, 1997), especially when it involves both nonlinear action-value function approximator,
such as neural network, and temporal-difference update (Sutton, 1988). As a result, the estimated
policy gradient may be biased, which possibly leads to divergence. Even if the algorithm converges
to a stationary point, due to the nonconvexity of the expected total reward with respect to the policy
as well as its parameter, the global optimality of such a stationary point remains unclear. The only
exception is the linear-quadratic regulator (LQR) setting (Fazel et al., 2018; Malik et al., 2018; Tu
and Recht, 2018; Yang et al., 2019a; Bu et al., 2019), which is, however, more restrictive than the
general MDP setting that possibly involves neural networks.
To bridge the gap between practice and theory, we analyze neural policy gradient methods equipped
with actor-critic schemes, where the actors and critics are represented by overparameterized two-
layer neural networks. In detail, we study two settings, where the policy improvement steps are
based on vanilla policy gradient and natural policy gradient, respectively. In both settings, the policy
evaluation steps are based on the TD(0) algorithm (Sutton, 1988) with independent sampling. In
the first setting, We prove that neural vanilla policy gradient converges to a stationary point of the
expected total reward ata 1/ √T-rate in the expected squared norm of the policy gradient, where T
is the number of policy improvement steps. MeanWhile, through a geometric characterization that
relates the suboptimality of the stationary points to the representation power of the neural networks
parameterizing the actor and critic, we establish the global optimality of all stationary points under
mild regularity conditions. In the second setting, through the lens of Kullback-Leibler (KL) diver-
gence regularization, we prove that neural natural policy gradient converges to a globally optimal
policy ata 1/√T-rate in the expected total reward. In particular, a key to such global optimality and
convergence guarantees is a notion of compatibility between the actor and critic, which connects the
accuracy of policy evaluation steps with the efficacy of policy improvement steps. We show that
such a notion of compatibility is ensured by using shared neural architectures and random initial-
izations for both the actor and critic, which is often used as a practical heuristic (Mnih et al., 2016).
To our best knowledge, our analysis gives the first global optimality and convergence guarantees for
neural policy gradient methods, which corroborate their significant empirical successes.
Related Work. In contrast to the huge body of empirical literature on policy gradient methods,
theoretical results on their convergence remain relatively scarce. In particular, Sutton et al. (2000)
and Kakade (2002) analyze vanilla policy gradient (REINFORCE) and natural policy gradient with
compatible action-value function approximators, respectively, which are further extended by Konda
and Tsitsiklis (2000); Peters and Schaal (2008); Castro and Meir (2010) to incorporate actor-critic
schemes. Most of this line of work only establishes the asymptotic convergence based on stochastic
approximation techniques (Kushner and Yin, 2003; Borkar, 2009) and requires the actor and critic to
be parameterized by linear functions. Another line of work (Papini et al., 2018; Xu et al., 2019a;b;
Shen et al., 2019; Karimi et al., 2019; Zhang et al., 2019) builds on the recent progress in nonconvex
optimization to establish the nonasymptotic rates of convergence of REINFORCE (Williams, 1992;
Baxter and Bartlett, 2000; Sutton et al., 2000) and its variants, but only to first- or second-order
stationary points, which, however, lacks global optimality guarantees. Moreover, when actor-critic
schemes are involved, due to the error of policy evaluation steps and its impact on policy improve-
ment steps, the nonasymptotic rates of convergence of policy gradient methods, even to first- or
second-order stationary points, remain rather open.
Compared with the convergence of policy gradient methods, their global optimality is even less
explored in terms of theory. Fazel et al. (2018); Malik et al. (2018); Tu and Recht (2018); Yang et al.
(2019a); Bu et al. (2019) prove that policy gradient methods converge to globally optimal policies
in the LQR setting, which is more restrictive. In very recent work, Bhandari and Russo (2019)
establish the global optimality of vanilla policy gradient (REINFORCE) in the general MDP setting.
However, they require the policy class to be convex, which restricts its applicability to the tabular
2
Published as a conference paper at ICLR 2020
and LQR settings. In independent work, Agarwal et al. (2019) prove that vanilla policy gradient
and natural policy gradient converge to globally optimal policies at 1 / √T-rates in the tabular and
linear settings. In the tabular setting, their rate of convergence of vanilla policy gradient depends
on the size of the state space. In contrast, we focus on the nonlinear setting with the actor-critic
scheme, where the actor and critic are parameterized by neural networks. It is worth mentioning
that when such neural networks have linear activation functions, our analysis also covers the linear
setting, which is, however, not our focus. In addition, Liu et al. (2019) analyze the proximal policy
optimization (PPO)and trust region policy optimization (TRPO)algorithms (Schulman et al., 2015;
2017), where the actors and critics are parameterized by neural networks, and establish their 1 /√T-
rates of convergence to globally optimal policies. However, they require solving a subproblem of
policy improvement in the functional space using multiple stochastic gradient steps in the parameter
space, whereas vanilla policy gradient and natural policy gradient only require a single stochastic
(natural) gradient step in the parameter space, which makes the analysis even more challenging.
There is also an emerging body of literature that analyzes the training and generalization error of
deep supervised learning with overparameterized neural networks (Daniely, 2017; Jacot et al., 2018;
Wu et al., 2018; Allen-Zhu et al., 2018a;b; Du et al., 2018a;b; Zou et al., 2018; Chizat and Bach,
2018; Jacot et al., 2018; Li and Liang, 2018; Cao and Gu, 2019a;b; Arora et al., 2019; Lee et al.,
2019), especially when they are trained using stochastic gradient. See Fan et al. (2019) for a detailed
survey. In comparison, our focus is on deep reinforcement learning with policy gradient methods.
In particular, the policy evaluation steps are based on the TD(0) algorithm, which uses stochastic
semigradient (Sutton, 1988) rather than stochastic gradient. Moreover, the interplay between the
actor and critic makes our analysis even more challenging than that of deep supervised learning.
Notation. For distribution μ on Ω and P > 0, we define kf (∙)kμ,p = (rω |f |pd〃)1/p as the Lp(μ)
norm of f. We define ∣∣f (∙)kμ,∞ = inf{C ≥ 0 : |f (x)| ≤ C for μ-almost every x} as the L∞(μ)-
norm of f. We write kf ∣μ,p for notational simplicity when the variable of f is clear from the
context. We further denote by ∣∣∙∣∣μ the L2(μ)-norm for notational simplicity. For a vector φ ∈ Rn
andp > 0, we denote by kφkp the `p-norm of φ. We denote by x = ([x]1>, . . . , [x]>m)> a vector in
Rmd, where [x]i ∈ Rd is the i-th block ofx for i ∈ [m].
2	Background
In this section, we introduce the background of reinforcement learning and policy gradient methods.
Reinforcement Learning. A discounted Markov decision process (MDP) is defined by tuple
(S, A, P, ζ, r, γ). Here S and A are the sets of all possible states and actions, respectively. Mean-
while, P is the Markov transition kernel and r is the reward function, which is possibly stochastic.
Specifically, when taking action a ∈ A at state s ∈ S, the agent receives reward r(s, a) and the
environment transits into a new state according to transition probability P(∙ | s,a). Meanwhile, Z
is the distribution of initial state S0 ∈ S and γ ∈ (0, 1) is the discount factor. In addition, policy
π(a | s) gives the probability of taking action a at state s. We denote the state- and action-value
functions associated with π by V π : S → R and Qπ : S × A → R, which are defined respectively
as
∞
Vπ (s) = (1-Y) ∙ E X Yt ∙ r(St ,At) So = s ,	Vs ∈S,	(2.1)
t=0
∞
Qn (s,a) = (1 — Y) ∙ E X Yt ∙ r(St ,At) So = s,Ao = a ,	∀(s,a) ∈S×A,	(2.2)
t=0
where So 〜Z(∙), At 〜∏(∙ | St), and St+1 〜P(∙ | St,At) for all t ≥ 0. Also, we define the
advantage function of policy π as the difference between Qπ and Vπ, i.e., Aπ (s, a) = Qπ(s, a) -
Vπ(s). By the definitions in (2.1) and (2.2), Vπ and Qπ are related via
Vπ (s) = En [Qπ (s,a)] = hQπ (s, ∙),∏(∙∣ s)i,
where〈•，•〉is the inner product in R1A1. Here we write Ea〜∏(∙∣ s)[Qπ(s, a)] as En [Qπ (s, a)] for
notational simplicity. Note that policy π together with transition probability P induces a Markov
3
Published as a conference paper at ICLR 2020
chain over state space S . We denote by %π the stationary state distribution of the Markov chain
induced by π. We further define ς∏ (s, a) = π(a | s) ∙ %∏ (S) as the stationary state-action distribution
over S × A. Meanwhile, policy π induces a state visitation measure over S and a state-action
visitation measure over S × A, which are denoted by νπ and σπ, respectively. Specifically, for all
(s, a) ∈ S × A, we define
∞∞
Vn(S) = (I- Y) ∙ X γt ∙ P(St = s),	σ∏(S, a) = (I- Y) ∙ X γt ∙ P(St = 3 * s, At = a),
t=0	t=0	(2.3)
where So 〜Z(∙), At 〜∏(∙ | St), and St+ι 〜P(∙ | St,At) for all t ≥ 0. By definition, we have
σ∏(s, a) = π(a | s) ∙ Vn (s). We define the expected total reward function J(π) by
∞
J(n) = (I-Y) ∙ E X Yt ∙ T(St, At) = EZ [Vn(s)] = Eσ∏ [r(s, a)], ∀π,	(2.4)
t=0
where We write Eσπ[r(s,a)] = E(s,。)〜σπ(∙,∙)[r(s, a)] for notational simplicity. The goal of re-
inforcement learning is to find the optimal policy that maximizes J(∏), which is denoted by ∏*.
When state space S is large, a popular approach is to find the maximizer of J(π) over a class of
parameterized policies {πθ : θ ∈ B}, where θ ∈ B is the parameter and B is the parameter space. In
this case, we obtain the optimization problem maxθ∈B J(πθ).
Policy Gradient Methods. Policy gradient methods maximize J(∏θ) using Vθ J(∏θ). These meth-
ods are based on the policy gradient theorem (Sutton and Barto, 2018), which states that
Vθ J(∏θ) = Eσ∏θ [Qnθ(s,a) ∙Vθ log∏θ(a | s)],	(2.5)
where σnθ is the state-action visitation measure defined in (2.3). Based on (2.5), (vanilla) policy gra-
dient maximizes the expected total reward via gradient ascent. Specifically, we generate a sequence
of policy parameters {θi }i∈[T] via
θi+1 - θi + η ∙ vθ J(πθi),	(2.6)
where η > 0 is the learning rate. Meanwhile, natural policy gradient (Kakade, 2002) utilizes natural
gradient ascent (Amari, 1998), which is invariant to the parameterization of policies. Specifically,
let F(θ) be the Fisher information matrix corresponding to policy πθ, which is given by
F(θ) = Eσπθ hVθlogπθ(a | s)[Vθ log πθ (a | s)]>i.	(2.7)
At each iteration, natural policy gradient performs
θi+ι J θi + η ∙ F-1(θi) ∙ Vθ J(a%),	(2.8)
where F-1(θi) is the inverse of F(θi) and η is the learning rate. In practice, both Qnθ in (2.5) and
F(θ) in (2.7) remain to be estimated, which yields approximations of the policy improvement steps
in (2.6) and (2.8).
3	Neural Policy Gradient Methods
In this section, we represent πθ by a two-layer neural network and study neural policy gradient
methods, which estimate the policy gradient and natural policy gradient using the actor-critic scheme
(Konda and Tsitsiklis, 2000).
3.1 Overparameterized Neural Policy
We now introduce the parameterization of policies. For notational simplicity, we assume that S ×
A ⊆ Rd with d ≥ 2. Without loss of generality, we further assume that k(s, a)k2 = 1 for all
(s, a) ∈ S × A. A two-layer neural network f ((s, a); W, b) with input (s, a) and width m takes the
form of
m
f((s,a); W,b) = —= X br ∙ ReLU((S,a)>[W ]r),	∀(s,a) ∈ S × A.	(3.1)
r=1
4
Published as a conference paper at ICLR 2020
Here ReLU : R → R is the rectified linear unit (ReLU) activation function, which is defined as
ReLU(U) = 1{u > 0} ∙ u. Also, {br}r∈[m] and W = IW]>,..., [W]')> ∈ Rmdin (3.1)
are the parameters. When training the two-layer neural network, we initialize the parameters via
[Winit]r 〜N(0,Id∕d) and b 〜Unif({-1,1}) for all r ∈ [m]. Note that the ReLU activation
function satisfies ReLU(C ∙ U) = C ∙ ReLU(U) for all c > 0 and u ∈ R. Hence, without loss of
generality, we keep br fixed at the initial parameter throughout training and only update W in the
sequel. See, e.g., Allen-Zhu et al. (2018b) for a detailed argument. For notational simplicity, we
write f ((s, a); W, b) as f ((s, a); W) hereafter.
Using the two-layer neural network in (3.1), we define
eχp[τ ∙ f((s,a);θ)]
πθ(a|s)= Pa0∈A exp[τ ∙ f((s,a0); θ)] ,	∀(s,a) ∈S'A,	(母
where f((∙,∙); θ) is defined in (3.1) with θ ∈ Rmd playing the role of W. Note that ∏θ defined
in (3.2) takes the form of an energy-based policy (Haarnoja et al., 2017). With a slight abuse of
terminology, We call T the temperature parameter and f ((∙, ∙); θ) the energy function in the sequel.
In the sequel, we investigate policy gradient methods for the class of neural policies defined in
(3.2). We define the feature mapping φθ = ([φθ]1>, . . . , [φθ]>m)> : Rd → Rmd ofa two-layer neural
network f((∙, ∙); θ) as
[φθ]r(s, a) = √m ∙ l{(s, a)>[θ]r > θ} ∙ (s, a),	∀(s, a) ∈ S ×A, Vr ∈ [m].	(3.3)
By (3.1), it holds that f((∙,∙); θ) = φθ(∙, ∙)>θ. Meanwhile, f((∙, ∙); θ) is almost everywhere differ-
entiable with respect to θ, and it holds that Vθf ((∙, ∙); θ) = φθ (∙, ∙). In the following proposition, we
calculate the closed forms of the policy gradient V J(∏θ) and the Fisher information matrix F(θ)
for πθ defined in (3.2).
Proposition 3.1 (Policy Gradient and Fisher Information Matrix). For πθ defined in (3.2), we have
vθ J (πθ) = T ∙ Eσ∏θ hQπθ (s,a) ∙ (φθ (s,a) - E∏θ [φθ (s,a0)])i,	(3.4)
F (θ) = τ 2 ∙ Eσ∏θ [(φθ (s, a) - E∏θ [φθ (s, a0)])(φθ (s, a) - E∏θ [φθ (s, a')]) i ,	(3.5)
where φθ(∙, ∙) is the feature mapping defined in (3.3), T is the temperature parameter, and
σπθ is the state-action visitation measure defined in (2.3). Here we write Eπθ [φθ(s, a0)] =
Ea0〜∏θ(∙∣ S) [φθ(s, a')] for notational simplicity.
Proof. See §H.1 for a detailed proof.	□
Since the action-value function Qπθ in (3.4) is unknown, to obtain the policy gradient, we use
another two-layer neural network to track the action-value function of policy πθ . Specifically, we
use a two-layer neural network Qω(∙, ∙) = f ((∙, ∙); ω) defined in (3.1) to represent the action-value
function Qπθ, where ω plays the same role as W in (3.1). Such an approach is known as the actor-
critic scheme (Konda and Tsitsiklis, 2000). We call πθ and Qω the actor and critic, respectively. We
highlight that in the overparameterized regime where the width of two-layer neural networks m is
large, a shared architecture and random initialization between the actor Qω and the energy function
of critic πθ ensures approximate compatible function approximations. See §B for details.
3.2 Neural Policy Gradient Methods
Now we present neural policy gradient and neural natural policy gradient. Following the actor-critic
scheme, they generate a sequence of policies {πθi}i∈[T +1] and action-value functions {Qωi}i∈[T].
3.2.1	Actor Update
As introduced in §2, we aim to solve the optimization problem maxθ∈B J(πθ) iteratively via
gradient-based methods, where B is the parameter space. We set B = {α ∈ Rmd : kα - Winitk2 ≤
R}, where R > 1 and Winit is the initial parameter defined in §3.1. For all i ∈ [T], let θi be the
5
Published as a conference paper at ICLR 2020
policy parameter at the i-th iteration. For notational simplicity, in the sequel, we denote by σi and ςi
the state-action visitation measure σπθ and the stationary state-action distribution ςπθ , respectively,
which are defined in §2. Similarly, we write	νi	=	νπθ	and	%i	=	%πθ	. To update	θi,	we set
Θi+1 J ∏b(θi + η ∙ G(θi) ∙ vθ J(∏θi)),	(3.6)
where we define ΠB : Rmd → B as the projection operator onto the parameter space B ⊆ Rmd .
Here G(θi) ∈ Rmd×md is a matrix specific to each algorithm. Specifically, we have G(θi) = Imd
for policy gradient and G(θi) = (F (θi))-1 for natural policy gradient, where F (θi) is the Fisher
information matrix in (3.5). Meanwhile, η is the learning rate and vθ J (πθi ) is an estimator of
vθJ(πθi), which takes the form of
1B
1
Vθ J(∏θi) = B •工 Qωi(s`,a`) ∙ Vθ log∏θi (a` | s`).	(3.7)
'=1
Here Ti is the temperature parameter of ∏θi, {(s', ag)}'∈[B] is sampled from the state-action Visita-
tion measure σi corresponding to the current policy πθi, and B > 0 is the batch size. Also, Qωi is
the critic obtained by Algorithm 2. Here we omit the dependency of vθJ(πθi) on ωi for notational
simplicity. See §C for the sampling from Visitation measures.
Inverting Fisher Information Matrix. Recall that G(θi) is the inVerse of the Fisher information
matrix used in natural policy gradient. In the oVerparameterized regime, inVerting an estimator F(θi)
of F (θi) can be infeasible as F (θi) is a high-dimensional matrix, which is possibly not inVertible.
To resolve this issue, We estimate the natural policy gradient G(θi) ∙ V J(∏θi) by solving
^,. , ^ ,..
mm ∣∣Fb(θi) ∙ α — τi ∙ Vθ J(∏θj∣∣2,	(3.8)
α∈B
1	© T/ ∖ ♦ F	♦ /C f	∙	.1	.	.	1 ⅛-J ∙	,1	,
Where VθJ(πθi) is defined in (3.7), τi is the temperature parameter in πθi, and B is the parameter
space. Meanwhile, F(θi) is an unbiased estimator of F(θi) based on {(s', ag)}'∈[B] sampled from
σi, Which is defined as
τ2	B	>
F(Oi) = B ∙ X(φθi(s',a') - E∏θi [hi(s',a')]) (φθi(s',a') - E∏θi [φθi(s',a')]) , (3.9)
'=1
where a' 〜∏θi (∙ | s`) and φθi is defined in (3.3) with θ = θ%. The actor update of neural natural
policy gradient takes the form of
Ti+1 J	Ti	+ η,	Ti+1	∙ θi+ι J Ti	∙ θi	+ η ∙ argmin ∣∣F(θi)	∙ α —	τi ∙	Vθ J(∏θj∣∣2,	(3.10)
α∈B
where we use an arbitrary minimizer of (3.8) if it is not unique. Note that we also update the
temperature parameter by Ti+1 J Ti + η, which ensures θi+1 ∈ B. It is worth mentioning that up
to minor modifications, our analysis allows for approximately solving (3.8), which is the common
practice of approximate second-order optimization (Martens and Grosse, 2015; Wu et al., 2017).
To summarize, at the i-th iteration, neural policy gradient obtains θi+1 via projected gradient ascent
using VθJ(πθi) defined in (3.7). Meanwhile, neural natural policy gradient solves (3.8) and obtains
θi+1 according to (3.10).
3.2.2	Critic Update
E 1 . ■ © T/ ∖ ∙ ,	∙	,	1 , ∙ ,1	♦ , ♦ /ʌ ♦ /C f 「	1∙	,1	,♦	1
To obtain VθJ(πθ), it remains to obtain the critic Qωi in (3.7). For any policy π, the action-value
function Qπ is the unique solution to the Bellman equation Q = TπQ (Sutton and Barto, 2018).
Here Tπ is the Bellman operator that takes the form of
TπQ(s, a) = E[(1 — Y) ∙ r(s, a) + Y ∙ Q(s0, a0)], ∀(s, a) ∈ S ×A,
where s0 ~ P(∙ | s, a) and a0 ~ π(∙ | s0). Correspondingly, we aim to solve the following optimiza-
tion problem
ωi
J argminEςJ(Qω(s,a) -TπθiQω(s,a))2],
ω∈B
(3.11)
6
Published as a conference paper at ICLR 2020
where ςi and Tπθi are the stationary state-action distribution and the Bellman operator associated
with πθi , respectively, and B is the parameter space. We adopt neural temporal-difference learning
(TD) studied in Cai et al. (2019), which solves the optimization problem in (3.11) via stochastic
semigradient descent (Sutton, 1988). Specifically, an iteration of neural TD takes the form of
ω(t + 1/2)
一ω⑴一ηTD ∙ (Qω(t)(4 s, a) - (1 - Y) ∙ r(s, a) - γQω(t) (S0, a0)) ∙ Vω Qω(t)(s, a), (3.12)
ω(t + 1) J argmin ∣∣α — ω(t + 1∕2)∣∣2,
α∈B
(3.13)
where (s, α)〜ςi(∙), s0 〜P(∙ | s, a), a0 〜 π(∙ | s0), and ητD is the learning rate of neural TD. Here
(3.12) is the stochastic semigradient step, and (3.13) projects the parameter obtained by (3.12) back
to the parameter space B. Meanwhile, the state-action pairs in (3.12) are sampled from the stationary
state-action distribution ςi , which is achieved by sampling from the Markov chain induced by πθi
until it mixes. See Algorithm 2 in §F for details. Finally, combining the actor updates and the
critic update described in (3.6), (3.10), and (3.11), respectively, we obtain neural policy gradient and
natural policy gradient, which are described in Algorithm 1.
Algorithm 1 Neural Policy Gradient Methods
Require: Number of iterations T, number of TD iterations TTd, learning rate η, learning rate ητD
of neural TD, temperature parameters {Ti}i∈[T +1], batch size B.
1:	Initialization: Initialize br 〜Unif({-1, 1}) and [卬.山 〜N(0,Id∕d) for all r ∈ [m]. Set
B J {α ∈ Rmd : ∣α — Winit ∣2 ≤ R} and θ1 J Winit.
2:	for i ∈ [T] do
3:
4:
5:
Update ωi using Algorithm 2 with πθi as the input, ω(0) J Winit and {br}r∈[m] as the
initialization, TTD as the number of iterations, and ηTD as the learning rate.
Sample {(s',α')}'∈[B] from the visitation measure σ%, and estimate VθJ(∏θ) and F(θi)
using (3.7) and (3.9), respectively.
If using policy gradient, update θi+1 by
θi+1 J πB (θi + η ∙vθ J(πθi )).
If using natural policy gradient, update θi+1 and Ti+1 by
^
^
τi+ι J Ti + η,	Ti+ι ∙ θi+ι - Ti ∙ θi + η ∙ argmin ∣∣F(θi) ∙ α — Ti ∙Vθ J(∏θj∣∣2.
α∈B
6:	end for
7:	Output: {πθi}i∈[T +1].
4 Main Results
In this section, we establish the global optimality and convergence for neural policy gradient meth-
ods. Hereafter, we assume that the absolute value of the reward function r is upper bounded by an
absolute constant Qmax > 0. As a result, we obtain from (2.1) and (2.2) that |V π(S, a)| ≤ Qmax,
∣Qπ(s, a)| ≤ Qmax, and ∣Aπ(s, a)| ≤ 2Qmaχ for all π and (s, a) ∈ S × A. In What follows, We
show that neural policy gradient converges to a stationary point of J(πθ) with respect to θ at a sub-
linear rate. We further characterize the geometry of J(πθ) and establish the global optimality of the
obtained stationary point. We defer the global optimality and convergence of neural natural policy
gradient to §A.
In the sequel, we study the convergence of neural policy gradient, i.e., Algorithm 1 with (3.6) as
the actor update, where G(θ) = Imd. In what follows, we lay out a regularity condition on the
action-value function Qπ .
Assumption 4.1 (Action-Value Function Class). We define
FR,∞
f(S, a) = f0(S, a) +
J l{w>(s, a) > 0} ∙ (s, a)>ι(w)dμ(w) : ∣ι(w)∣∞
7
Published as a conference paper at ICLR 2020
where μ: Rd → R is the density function of the Gaussian distribution N(0,Id/d), fo(∙, ∙) =
f ((∙, ∙); Winit) is the two-layer neural network corresponding to the initial parameter Wait, and
ι : Rd → Rd together with f0 parameterizes the element of FR,∞. We assume that Qπ ∈ FR,∞ for
all π.
Assumption 4.1 is a mild regularity condition on Qπ, as FR,∞ captures a sufficiently general family
of functions, which constitute a subset of the reproducing kernel Hilbert space (RKHS) induced
by the random feature 1{w>(s, a) > 0} ∙ (s, a) with W 〜N(0, Id/d) (Rahimi and Recht, 2008;
2009) up to the shift of f0. Similar assumptions are imposed in the analysis of batch reinforcement
learning in RKHS (Farahmand et al., 2016).
In what follows, we lay out a regularity condition on the state visitation measure νπ and the station-
ary state distribution %π .
Assumption 4.2 (Regularity Condition on νπ and %π). Let π and πe be two arbitrary policies. We
assume that there exists an absolute constant c > 0 such that
E∏∙ν∏ h1{|y>(S,a)| ≤ u}] ≤ C ∙U/∣y∣∣2,
e∏∙%∏ [ι{∣y>(s, a)l ≤ u}] ≤ c ∙ u∕kyk2,	∀y ∈ Rd, ∀u > 0.
Here the expectations are taken over the joint distributions e(∙ | ∙) ∙ Vn(∙) and e(∙ | ∙) ∙ %∏(∙) over
S × A, respectively.
Assumption 4.2 essentially imposes a regularity condition on the Markov transition kernel P of the
MDP as P determines νπ and %π for all π. Such a regularity condition holds if both νπ and %π have
upper-bounded density functions for all π .
After introducing these regularity conditions, we present the following proposition adapted from Cai
et al. (2019), which characterizes the convergence of neural TD for the critic update.
Proposition 4.3 (Convergence of Critic Update). We set ητD = min{(1 - γ)∕8, 1∕√Ttd} in
Algorithm 1. Let Qωi be the output of the i-th critic update in Line 3 of Algorithm 1, which is an
estimator of Qπθi obtained by Algorithm 2 with TTD iterations. Under Assumptions 4.1 and 4.2, it
holds for TTD = Ω(m) that
Einit [kQωi - Qπθi k" = O(R3 ∙ m-1∕2 + R5∕2 ∙ m-1∕4),	(4.1)
where ςi is the stationary state-action distribution corresponding to πθi . Here the expectation is
taken over the random initialization.
Proof. See §F.1 for a detailed proof.	□
Cai et al. (2019) show that the error of the critic update consists of two parts, namely the approxima-
tion error of two-layer neural networks and the algorithmic error of neural TD. The former decays as
the width m grows, while the latter decays as the number of neural TD iterations TTD in Algorithm 2
grows. By setting TTD = Ω(m), the algorithmic error in (4.1) ofProposition 4.3 is dominatedby the
approximation error. In contrast with Cai et al. (2019), we obtain a more refined convergence char-
acterization under the more restrictive assumption that Qπ ∈ FR,∞ . Specifically, such a restriction
allows us to obtain the upper bound of the mean squared error in (4.1) of Proposition 4.3.
It now remains to establish the convergence of the actor update, which involves the estimator
Vθ J(∏θj of the policy gradient Vθ J(∏θJ based on {(s', a')}'∈[B]. We introduce the following
regularity condition on the variance of VθJ(πθi).
Assumption 4.4 (Variance Upper Bound). Recall that σi is the state-action visitation measure cor-
responding to πθi for all i ∈ [T]. Let ξi = VθJ(πθi) - E[Vθ J (πθi)], where VθJ(πθi) is defined in
(3.7). We assume that there exists an absolute constant σξ > 0 such that E[∣∣ξi∣∣2] ≤ Ti ∙ σξ2∕B for
all i ∈ [T]. Here the expectations are taken over σi given θi and ωi.
Assumption 4.4 is a mild regularity condition. Such a regularity condition holds if the Markov chain
that generates {(s', a')}'∈[B] mixes sufficiently fast and Qωi(s, a) with (s,a)〜 σ% have upper
8
Published as a conference paper at ICLR 2020
bounded second moments for all i ∈ [T]. Zhang et al. (2019) verify that under certain regularity
conditions, similar unbiased policy gradient estimators have almost surely upper bounded norms,
which implies Assumption 4.4. Similar regularity conditions are also imposed in the analysis of
policy gradient methods by Xu et al. (2019a;b).
In what follows, we impose a regularity condition on the discrepancy between the state-action visi-
tation measure and the stationary state-action distribution corresponding to the same policy.
Assumption 4.5 (Regularity Condition on σi and ςi). We assume that there exists an absolute con-
stant κ > 0 such that
(4.2)
Here dσjdςi is the Radon-Nikodym derivative of σi with respect to ς%.
We highlight that if the MDP is initialized at the stationary distribution ςi , the state-action visitation
measure σi is the same as ςi. Meanwhile, if the induced Markov state-action chain mixes sufficiently
fast, such an assumption also holds. A similar regularity condition is imposed by Scherrer (2013),
which assumes that the L∞-norm of dσ∕dςi is upper bounded, whereas we only assume that its
L2-norm is upper bounded.
Meanwhile, we impose the following regularity condition on the smoothness of the expected total
reward J (πθ) with respect to θ.
Assumption 4.6 (Lipschitz Continuous Policy Gradient). We assume that Vθ J(∏θ ) is L-Lipschitz
continuous with respect to θ, where L > 0 is an absolute constant.
Such an assumption holds when the transition probability P(∙ | s, a) and the reward function r are
both Lipschitz continuous with respect to their inputs (Pirotta et al., 2015). Also, Karimi et al.
(2019); Zhang et al. (2019); Xu et al. (2019b); Agarwal et al. (2019) verify the Lipschitz continuity
of the policy gradient under certain regularity conditions.
Note that we restrict θ to the parameter space B. Here we call θ ∈ B a stationary point of J(πθ)
if it holds for all θ ∈ B that Vθ J (πθb)> (θ - θb) ≤ 0. We now show that the sequence {θi}i∈[T+1]
generated by neural policy gradient converges to a stationary point at a sublinear rate.
Theorem 4.7 (Convergence to Stationary Point). We set Ti = 1, η = 1∕√T, ητD = min{(1 -
Y)/8, 1∕√Ttd}, TTD = Ω(m), and B = {α : ∣∣α - Winitk2 ≤ R} by Algorithm 1, where the actor
update is given in (3.6) with G(θ) = Imd. For all i ∈ [T], we define
Pi = η-1 ∙ h∏B(θi + η ∙ VθJ(∏θi)) - M ∈ Rmd,	(4.3)
where ΠB : Rmd → B is the projection operator onto B ⊆ Rmd. Under the assumptions of Proposi-
tion 4.3 and Assumptions 4.4-4.6, for T ≥ 4L2 we have
min
i∈[T]
E[kPik2] ≤ 8∕√T ∙ EJ(∏θτ +1) - J(∏θι)] +8σξ∕B + eq(T),
where K is defined in (4.2) of Assumption 4.2 and £q(T) = K ∙ O(R5/2 ∙ m-1/4 ∙ T1/2 + R9/4 ∙
m-1/8 ∙ T1/2). Here the expectations are taken over all the randomness.
Proof. See §D.1 for a detailed proof.
□
By Theorem 4.7 with m = Ω(T8 ∙ R18) and B = Ω(√T), we obtain mini∈[τ] E[∣∣Pi∣∣2]=
O (1 / √T). Therefore, when the two-layer neural networks are sufficiently wide and the batch size
B is sufficiently large, neural policy gradient achieves a 1∕√T-rate of convergence. Moreover, Pi
defined in (4.3) is known as the gradient mapping at θi (Nesterov, 2018). It is known that θ ∈ B is a
stationary point if and only if the gradient mapping at θ is a zero vector. Therefore, (a subsequence
of) {θi}i∈[T+1] converges to a stationary point θb ∈ B as mini∈[T] E[kPik22] converges to zero. In
other words, neural policy gradient converges to a stationary point ata 1∕ √T-rate. Also, we remark
9
Published as a conference paper at ICLR 2020
that the projection operator in the actor update is adopted only for the purpose of simplicity, which
can be removed with more refined analysis. Moreover, the projection-free version of neural policy
gradient converges to a stationary point at a similar sublinear rate. See §G for details.
We now characterize the global optimality of the obtained stationary point θ. To this end, we com-
pare the expected total reward of ∏. with that of the global optimum ∏* of J(∏).
Theorem 4.8	(Global Optimality of Stationary Point). Let θ ∈ B be a stationary point of J(πθ). It
holds that
(I- Y) ∙ (J (π*) - J (πb)) ≤ 2Qmax ∙ lnf llub(∙, ∙) - φθ(∙, ∙)>θkσ∏ b ,
θ∈B	θ
where Qmax is the upper bound of |r| and Uθb: S × A → R is defined as
ubb(s, a) = —~~-- (s, a) — ——— (s) + φ-bb(s, a)ɪθ, ∀(s, a) ∈ S × A.
dσπθb	dνπθb
(4.4)
Here dσ∏*/dσ∏b and dν∏*/dν∏b are the Radon-Nikodym derivatives, and ∣∣ ∙ ∣∣σπg. is the L2(σ∏§)-
norm.	θ
Proof. See §D.2 for a detailed proof.
□
>
To understand Theorem 4.8, We highlight that for θ,θ ∈ B, the function φb(∙, ∙)1 θ is well approxi-
mated by the overparameterized two-layer neural network f ((∙, ∙); θ). See Corollary E.4 for details.
Therefore, the global optimality of πθb depends on the error of approximating uθb with an overparam-
eterized two-layer neural network. Specifically, if uθb is well approximated by an overparameterized
two-layer neural network, then n» is nearly as optimal as ∏*. In the following corollary, we formally
establish a sufficient condition for any stationary point θ to be globally optimal.
Theorem 4.9	(Global Optimality of Stationary Point). Let θ ∈ B be a stationary point of J(πθ).
We assume that uθb ∈ FR,∞ in Theorem 4.8. Under Assumption 4.2, it holds that
(1 — Y) ∙ Einit [J(∏*) - J(∏b)]=O(R3/2 ∙ m-1/4).
More generally, without assuming uθb ∈ FR,∞ in Theorem 4.8, under Assumption 4.2, it holds that
(I-Y) ∙ Einit [J(π*) - J(πb)]=O(R3/2 ∙ m-1/4) + Einit [∣πFr,∞Ub-Ubkσ∏b].
Here the expectations are taken over the random initialization, and ΠFR,∞ is the projection operator
onto FR,∞ with respect to the L2 (σπb)-norm.
Proof. See §H.2 for a detailed proof.
□
By Theorem 4.9, a stationary point θ is globally optimal if uθb ∈ FR,∞ and m → ∞. Moreover,
following from the definition ofρi in (4.3) of Theorem 4.7, we obtain that
VθJ(∏θi)>(θ - θi) ≤ (2R + 2η ∙ Qmax) ∙ kρik2, ∀θ ∈ B.
(4.5)
See §H.3 for a detailed proof of (4.5). Since ∣ρi ∣2 = 0 implies that θi is a stationary point, the
right-hand side of (4.5) quantifies the deviation of θi from a stationary point θ. Following similar
analysis to §D.2 and §H.2, if uθi ∈ FR,∞ for all i ∈ [T], we obtain that
(1 — Y) ∙ min E[J(∏*) - J(∏θj]=O(R3/2 ∙ m-1/4) + (2R + 2η ∙ Qmax) ∙ min E[kρik2].
Thus, by invoking Theorem 4.7, it holds for sufficiently large m and B that the expected total reward
J(∏θi) converges to the global optimum J(∏*) at a 1/T1/4-rate.
10
Published as a conference paper at ICLR 2020
References
Agarwal, A., Kakade, S. M., Lee, J. D. and Mahajan, G. (2019). Optimality and approximation with
policy gradient methods in Markov decision processes. arXiv preprint arXiv:1908.00261.
Allen-Zhu, Z., Li, Y. and Liang, Y. (2018a). Learning and generalization in overparameterized neu-
ral networks, going beyond two layers. arXiv preprint arXiv:1811.04918.
Allen-Zhu, Z., Li, Y. and Song, Z. (2018b). A convergence theory for deep learning via over-
parameterization. arXiv preprint arXiv:1811.03962.
Amari, S.-I. (1998). Natural gradient works efficiently in learning. Neural Computation, 10 251-
276.
Antos, A., Szepesvari, C. and Munos, R. (2008). Learning near-optimal policies with Bellman-
residual minimization based fitted policy iteration and a single sample path. Machine Learning,
71 89-129.
Arora, S., Du, S. S., Hu, W., Li, Z. and Wang, R. (2019). Fine-grained analysis of optimiza-
tion and generalization for overparameterized two-layer neural networks. arXiv preprint
arXiv:1901.08584.
Barron, A. R. (1994). Approximation and estimation bounds for artificial neural networks. Machine
Learning, 14 115-133.
Baxter, J. and Bartlett, P. L. (2000). Direct gradient-based reinforcement learning. In International
Symposium on Circuits and Systems.
Bhandari, J. and Russo, D. (2019). Global optimality guarantees for policy gradient methods. arXiv
preprint arXiv:1906.01786.
Borkar, V. S. (2009). Stochastic Approximation: A Dynamical Systems Viewpoint, vol. 48. Springer.
Bu, J., Mesbahi, A., Fazel, M. and Mesbahi, M. (2019). LQR through the lens of first order methods:
Discrete-time case. arXiv preprint arXiv:1907.08921.
Cai, Q., Yang, Z., D. Lee, J. and Wang, Z. (2019). Neural temporal-difference learning converges to
global optima. arXiv preprint arXiv:1905.10027.
Cao, Y. and Gu, Q. (2019a). Generalization bounds of stochastic gradient descent for wide and deep
neural networks. arXiv preprint arXiv:1905.13210.
Cao, Y. and Gu, Q. (2019b). A generalization theory of gradient descent for learning over-
parameterized deep ReLU networks. arXiv preprint arXiv:1902.01384.
Castro, D. D. and Meir, R. (2010). A convergent online single time scale actor critic algorithm.
Journal of Machine Learning Research, 11 367-410.
Chen, J. and Jiang, N. (2019). Information-theoretic considerations in batch reinforcement learning.
arXiv preprint arXiv:1905.00360.
Chizat, L. and Bach, F. (2018). A note on lazy training in supervised differentiable programming.
arXiv preprint arXiv:1812.07956.
Daniely, A. (2017). SGD learns the conjugate kernel class of the network. In Advances in Neural
Information Processing Systems.
Du, S. S., Lee, J. D., Li, H., Wang, L. and Zhai, X. (2018a). Gradient descent finds global minima
of deep neural networks. arXiv preprint arXiv:1811.03804.
Du, S. S., Zhai, X., Poczos, B. and Singh, A. (2018b). Gradient descent provably optimizes over-
parameterized neural networks. arXiv preprint arXiv:1810.02054.
11
Published as a conference paper at ICLR 2020
Duan, Y., Chen, X., Houthooft, R., Schulman, J. and Abbeel, P. (2016). Benchmarking deep rein-
forcement learning for continuous control. In International Conference on Machine Learning.
Espeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V., Ward, T., Doron, Y., Firoiu, V.,
Harley, T., Dunning, I. et al. (2018). IMPALA: Scalable distributed deep-RL with importance
weighted actor-learner architectures. arXiv preprint arXiv:1802.01561.
Fan, J., Ma, C. and Zhong, Y. (2019). A selective overview of deep learning. arXiv preprint
arXiv:1904.05526.
Farahmand, A.-m., Ghavamzadeh, M., Szepesvari, C. and Mannor, S. (2016). Regularized policy
iteration with nonparametric function spaces. Journal of Machine Learning Research, 17 4809-
4874.
Farahmand, A.-m., Szepesvari, C. and Munos, R. (2010). Error propagation for approximate policy
and value iteration. In Advances in Neural Information Processing Systems.
Fazel, M., Ge, R., Kakade, S. M. and Mesbahi, M. (2018). Global convergence of policy gradient
methods for the linear quadratic regulator. arXiv preprint arXiv:1801.05039.
Funahashi, K.-I. (1989). On the approximate realization of continuous mappings by neural networks.
Neural Networks, 2 183-192.
Haarnoja, T., Tang, H., Abbeel, P. and Levine, S. (2017). Reinforcement learning with deep energy-
based policies. In International Conference on Machine Learning.
Jacot, A., Gabriel, F. and Hongler, C. (2018). Neural tangent kernel: Convergence and generaliza-
tion in neural networks. In Advances in Neural Information Processing Systems.
Johnson, R. and Zhang, T. (2013). Accelerating stochastic gradient descent using predictive variance
reduction. In Advances in Neural Information Processing Systems.
Kakade, S. and Langford, J. (2002). Approximately optimal approximate reinforcement learning.
In International Conference on Machine Learning.
Kakade, S. M. (2002). A natural policy gradient. In Advances in Neural Information Processing
Systems.
Karimi, B., Miasojedow, B., Moulines, E. and Wai, H.-T. (2019). Non-asymptotic analysis of biased
stochastic approximation scheme. arXiv preprint arXiv:1902.00629.
Klusowski, J. M. and Barron, A. R. (2016). Risk bounds for high-dimensional ridge function com-
binations including neural networks. arXiv preprint arXiv:1607.01434.
Konda, V. (2002). Actor-Critic Algorithms. Ph.D. thesis, Massachusetts Institute of Technology.
Konda, V. R. and Tsitsiklis, J. N. (2000). Actor-critic algorithms. In Advances in Neural Information
Processing Systems.
Kushner, H. and Yin, G. G. (2003). Stochastic Approximation and Recursive Algorithms and Appli-
cations, vol. 35. Springer Science & Business Media.
Lazaric, A., Ghavamzadeh, M. and Munos, R. (2016). Analysis of classification-based policy itera-
tion algorithms. Journal of Machine Learning Research, 17 583-612.
Lee, J., Bahri, Y., Novak, R., Schoenholz, S. S., Pennington, J. and Sohl-Dickstein, J. (2018). Deep
neural networks as Gaussian processes. In International Conference on Learning Representa-
tions.
Lee, J., Xiao, L., Schoenholz, S. S., Bahri, Y., Sohl-Dickstein, J. and Pennington, J. (2019). Wide
neural networks of any depth evolve as linear models under gradient descent. arXiv preprint
arXiv:1902.06720.
12
Published as a conference paper at ICLR 2020
Li, Y. (2017). Deep reinforcement learning: An overview. arXiv preprint arXiv:1701.07274.
Li, Y. and Liang, Y. (2018). Learning overparameterized neural networks via stochastic gradient
descent on structured data. In Advances in Neural Information Processing Systems.
Liu, B., Cai, Q., Yang, Z. and Wang, Z. (2019). Neural proximal/trust region policy optimization
attains globally optimal policy. arXiv preprint arXiv:1906.10306.
Malik, D., Pananjady, A., Bhatia, K., Khamaru, K., Bartlett, P. L. and Wainwright, M. J. (2018).
Derivative-free methods for policy optimization: Guarantees for linear quadratic systems. arXiv
preprint arXiv:1812.08305.
Martens, J. and Grosse, R. (2015). Optimizing neural networks with kronecker-factored approxi-
mate curvature. In International Conference on Machine Learning.
Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D. and
Kavukcuoglu, K. (2016). Asynchronous methods for deep reinforcement learning. In
International Conference on Machine Learning.
Munos, R. and Szepesvari, C. (2008). Finite-time bounds for fitted value iteration. Journal of Ma-
chine Learning Research, 9 815-857.
Nesterov, Y. (2018). Lectures on Convex Optimization. Springer.
Pan, X. and Srikumar, V. (2016). Expressiveness of rectifier networks. In International Conference
on Machine Learning.
Papini, M., Binaghi, D., Canonaco, G., Pirotta, M. and Restelli, M. (2018). Stochastic variance-
reduced policy gradient. arXiv preprint arXiv:1806.05618.
Peters, J. and Schaal, S. (2006). Policy gradient methods for robotics. In International Conference
on Intelligent Robots and Systems.
Peters, J. and Schaal, S. (2008). Natural actor-critic. Neurocomputing, 71 1180-1190.
Pirotta, M., Restelli, M. and Bascetta, L. (2015). Policy gradient in Lipschitz Markov decision pro-
cesses. Machine Learning, 100 255-283.
Puterman, M. L. (2014). Markov Decision Processes: Discrete Stochastic Dynamic Programming.
John Wiley & Sons.
Rahimi, A. and Recht, B. (2008). Random features for large-scale kernel machines. In Advances in
Neural Information Processing Systems.
Rahimi, A. and Recht, B. (2009). Weighted sums of random kitchen sinks: Replacing minimization
with randomization in learning. In Advances in Neural Information Processing Systems.
Scherrer, B. (2013). On the performance bounds of some policy search dynamic programming
algorithms. arXiv preprint arXiv:1306.0539.
Scherrer, B., Ghavamzadeh, M., Gabillon, V., Lesner, B. and Geist, M. (2015). Approximate mod-
ified policy iteration and its application to the game of Tetris. Journal of Machine Learning
Research, 16 1629-1676.
Schulman, J., Levine, S., Abbeel, P., Jordan, M. and Moritz, P. (2015). Trust region policy optimiza-
tion. In International Conference on Machine Learning.
Schulman, J., Wolski, F., Dhariwal, P., Radford, A. and Klimov, O. (2017). Proximal policy opti-
mization algorithms. arXiv preprint arXiv:1707.06347.
Shen, Z., Ribeiro, A., Hassani, H., Qian, H. and Mi, C. (2019). Hessian aided policy gradient. In
International Conference on Machine Learning.
13
Published as a conference paper at ICLR 2020
Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser, J.,
Antonoglou, I., Panneershelvam, V., Lanctot, M. et al. (2016). Mastering the game of Go with
deep neural networks and tree search. Nature, 529 484.
Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T.,
Baker, L., Lai, M., Bolton, A. et al. (2017). Mastering the game of Go without human knowl-
edge. Nature, 550 354.
Sutton, R. S. (1988). Learning to predict by the methods of temporal differences. Machine Learning,
3 9-44.
Sutton, R. S. and Barto, A. G. (2018). Reinforcement Learning: An Introduction. MIT press.
Sutton, R. S., McAllester, D. A., Singh, S. P. and Mansour, Y. (2000). Policy gradient methods for
reinforcement learning with function approximation. In Advances in Neural Information Pro-
cessing Systems.
Szepesvari, C. and Munos, R. (2005). Finite time bounds for sampling based fitted value iteration.
In International Conference on Machine Learning.
Tsitsiklis, J. N. and Van Roy, B. (1997). Analysis of temporal-diffference learning with function
approximation. In Advances in Neural Information Processing Systems.
Tu, S. and Recht, B. (2018). The gap between model-based and model-free methods on the linear
quadratic regulator: An asymptotic viewpoint. arXiv preprint arXiv:1812.03565.
van Handel, R. (2014). Probability in High Dimension. Princeton University.
Vinyals, O., Babuschkin, I., Chung, J., Mathieu, M., Jaderberg, M., Czarnecki, W. M.,
Dudzik, A., Huang, A., Georgiev, P., Powell, R., Ewalds, T., Horgan, D., Kroiss, M.,
Danihelka, I., Agapiou, J., Oh, J., Dalibard, V., Choi, D., Sifre, L., Sulsky, Y., Vezhnevets, S.,
Molloy, J., Cai, T., Budden, D., Paine, T., Gulcehre, C., Wang, Z., Pfaff, T., Pohlen, T.,
Wu, Y., Yogatama, D., Cohen, J., McKinney, K., Smith, O., Schaul, T., Lillicrap, T.,
Apps, C., Kavukcuoglu, K., Hassabis, D. and Silver, D. (2019). AlphaStar: Master-
ing the Real-Time Strategy Game StarCraft II. https://deepmind.com/blog/
alphastar-mastering-real-time-strategy-game-starcraft-ii/.
Wagner, P. (2011). A reinterpretation of the policy oscillation phenomenon in approximate policy
iteration. In Advances in Neural Information Processing Systems.
Wagner, P. (2013). Optimistic policy iteration and natural actor-critic: A unifying view and a non-
optimality result. In Advances in Neural Information Processing Systems.
Wang, W. Y., Li, J. and He, X. (2018). Deep reinforcement learning for NLP. In Association for
Computational Linguistics.
Williams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist reinforce-
ment learning. Machine Learning, 8 229-256.
Wu, L., Ma, C. and Weinan, E. (2018). How SGD selects the global minima in over-parameterized
learning: A dynamical stability perspective. In Advances in Neural Information Processing Sys-
tems.
Wu, Y., Mansimov, E., Grosse, R. B., Liao, S. and Ba, J. (2017). Scalable trust-region method for
deep reinforcement learning using kronecker-factored approximation. In Advances in Neural
Information Processing Systems.
Xu, P., Gao, F. and Gu, Q. (2019a). An improved convergence analysis of stochastic variance-
reduced policy gradient. arXiv preprint arXiv:1905.12615.
Xu, P., Gao, F. and Gu, Q. (2019b). Sample efficient policy gradient methods with recursive variance
reduction. arXiv preprint arXiv:1909.08610.
14
Published as a conference paper at ICLR 2020
Yang, Z., Chen, Y., Hong, M. and Wang, Z. (2019a). On the global convergence of actor-critic: A
case for linear quadratic regulator with ergodic cost. arXiv preprint arXiv:1907.06246.
Yang, Z., Xie, Y. and Wang, Z. (2019b). A theoretical analysis of deep Q-learning. arXiv preprint
arXiv:1901.00137.
Zhang, K., Koppel, A., Zhu, H. and Bayar, T. (2019). Global convergence of policy gradient methods
to (almost) locally optimal policies. arXiv preprint arXiv:1906.08383.
Zou, D., Cao, Y., Zhou, D. and Gu, Q. (2018). Stochastic gradient descent optimizes over-
parameterized deep ReLU networks. arXiv preprint arXiv:1811.08888.
15
Published as a conference paper at ICLR 2020
A	Neural Natural Policy Gradient
In the sequel, we study the convergence of neural natural policy gradient. As shown in Algorithm
1, neural natural policy gradient uses neural TD for policy evaluation and updates the actor using
(3.10), where θi and τi in (3.2) are both updated. To analyze the critic update, we impose As-
sumptions 4.1 and 4.2, which guarantee that Proposition 4.3 holds. Meanwhile, to analyze the actor
update, we impose the following regularity conditions.
In parallel to Assumption 4.4, we lay out the following regularity condition on the variance of the
estimators of the policy gradient and the Fisher information matrix.
Assumption A.1 (Variance Upper Bound). Let B = {α ∈ Rmd : kα - Winit k2 ≤ R}, where Winit
is the initial parameter. We define
_	，	.	. . .	^ , . .	^ _ ,	. ..	・ 一r
δi =(Ti+1 ∙ θi+ι - Ti ∙ θi)∕η = argmin ∣∣Fb(θi) ∙ α -方∙ Vθ J(∏θj∣∣2, ∀i ∈ [T],
α∈B
1	© τ/ ∖	1 π / /-1 ∖	FC t ∙ /Cf 1 ZZ-, Γ∖∖	J 1 ɪɪ τ∙ ,1	1∙	1 ,	1	。	, •
where VθJ(πθi) and F (θi) are defined in (3.7) and (3.9), respectively. With slight abuse of notation,
for all i ∈ [T], we define the function ξi : Rmd → Rmd as
ξi(α) = F(θi) ∙ α - Ti ∙ Vθ J(∏θj - E[Fb(θi) ∙ α - τi ∙ Vθ J(∏θJ].
We assume that there exists an absolute constant σξ > 0 such that
E[kξi(δi)k2] ≤ T ∙ σ2∕B, E[kξi(ωi)∣2] ≤ Ti ∙ σ∖∕B, ∀i ∈ [T].
Here the expectations are taken over σi given θi and ωi .
Next, we lay out a regularity condition on the visitation measures σi , νi and the stationary distribu-
tions ςi , %i , respectively.
Assumption A.2 (Upper Bounded Concentrability Coefficient). We denote by ν and σ* the state
and state-action visitation measures corresponding to the global optimum ∏*. For all i ∈ [T], We
define the concentrability coefficients φi, ψi,夕i, and ψi as
ψi = {Eσi[(dσ*∕dσi)2]}，，ψi = {旧“科[(dν"dνi)2]}，，
d = {Eςi[(dσ"dςi)2]} / ,	ψi = {E%i[(dν"d%i)2]} / ,	(A.I)
where dσ*∕dσi, dν*∕d%, dσ*∕d<i, and dν*∕d%i are the Radon-Nikodym derivatives. We assume
that the concentrability coefficients defined in (A.1) are uniformly upper bounded by an absolute
constant c0 > 0.
The regularity condition on upper bounded concentrability coefficients is commonly imposed in the
reinforcement learning literature and is standard for theoretical analysis (SzePeSVari and Munos,
2005; Munos and Szepesvari, 2008; Antos et al., 2008; Lazaric et al., 2016; Farahmand et al., 2010;
2016; Scherrer, 2013; Scherrer et al., 2015; Yang et al., 2019b; Chen and Jiang, 2019).
Finally, we introduce the following regularity condition on the initial parameter Winit in Algorithm
1.
Assumption A.3 (Upper Bounded Moment at Random Initialization). Let φ0 (s, a) ∈ Rmd be the
feature mapping defined in (3.3) with θ = Winit . We assume that there exists an absolute constant
M > 0 such that
Einit	SUp	f((s,a); Winit)∣2 = Einit SUp ∣Φo (s, a)> Winit |2 ≤ M2.
(s,a)∈S×A	(s,a)∈S×A
Here the expectations are taken over the random initialization.
Note that as m → ∞, the two-layer neural network φ0(s, a)>Winit converges to a Gaussian pro-
cess indexed by (s, a) (Lee et al., 2018), which lies in a compact subset of Rd. It is known that
under certain regularity conditions, the maximum of a Gaussian process over a compact index set
is a sub-Gaussian random variable (van Handel, 2014). Therefore, the regularity condition that
max(s,a) ∣φo(s, α)>Winit∣ has a finite second moment is mild.
We now establish the global optimality and rate of convergence of neural natural policy gradient.
16
Published as a conference paper at ICLR 2020
Theorem A.4 (Global Optimality and Convergence). We set η = 1∕√T, ητD = min{(1 -
Y)/8, 1∕√Ttd}, TTD = Ω(m), τ = (i - 1) ∙ η, and B = {α : ∣∣α - Winitk2 ≤ R} in Algo-
rithm 1, where the actor update is given in (3.10). Under the assumptions of Proposition 4.3 and
Assumptions A.1-A.3, we have
min E[J(π*) - J(πθ.)] ≤ log AI + 9R2z+ M + -~ʒ——X Ei(T).	(A.2)
m[T] L ( ) ( θi )j ≤ (1-γ) ∙√T + (1 - γ) ∙ Tyc)	( )
Here M is defined in Assumption A.3 and Ei (T) satisfies
Ei(T) = √8c0 ∙ R1/2 ∙ (σ∣∕B)”	(A.3)
、--------{-----------}
(a)
+ O((Ti+1 ∙ T1/2 + 1) ∙ R3/ ∙ m-1/4 + R5/4 ∙ m-1/8) + εQ,,,
、-----------------------V-----------------------' |{z}
(b)	(c)
where co is defined in Assumption A.2 and εQ,i = co ∙ O(R3/2 ∙ m-1/4 + R5/4 ∙ m-1/8). Here the
expectation is taken over all the randomness.
Proof. See §D.3 for a detailed proof.
□
As shown in (A.2) OfTheorem A.4, the optimality gap mini∈[τ] E[J (π*) -J (∏θJ] is upper bounded
by two terms. Intuitively, the first O(1∕√T) term characterizes the convergence of neural natural
policy gradient as m, B → ∞. Meanwhile, the second term aggregates the errors incurred by both
the actor update and the critic update due to finite m and B. Specifically, in (A.3) of Theorem A.4,
(a) corresponds to the estimation error of F(θ) and Vθ J(∏θ) due to the finite batch Size B, which
vanishes as B → ∞. Also, (b) corresponds to the incompatibility between the parameterizations of
the actor and critic. As introduced in §3.1, we use shared architecture and random initialization to
ensure approximately compatible function approximations. In particular, (b) vanishes as m → ∞.
Meanwhile, (c) corresponds to the policy evaluation error, i.e., the error of approximating Qπθi
using Qωi . As shown in Proposition 4.3, such an error is sufficiently small when both m and
TTD are sufficiently large. To conclude, when m, B, and TTD are sufficiently large, the expected
total reward of (a subsequence of) {πθi }i∈[T +1] obtained from the neural natural policy gradient
converges to the global optimum J(∏*) at a 1∕√T-rate. Formally, We have the following corollary.
Corollary A.5 (Global Optimality and Convergence). Under the same assumptions of Theorem
A.4, it holds for m = Ω(R10 ∙ T6) and B = Ω(R2 ∙ T2 ∙ σ2) that
mnE[J(∏*)-J(∏θi)] = Ofn lθg|A|斤).
i∈[T]	(1 - γ) ∙ √T
Here the expectation is taken over all the randomness.
Proof. See §H.4 for a detailed proof.	□
Corollary A.5 establishes both the global optimality and rate of convergence of neural natural policy
gradient. Combining Theorem 4.7 and Corollary A.5, we conclude that when we use overparam-
eterized two-layer neural networks, both neural policy gradient and neural natural policy gradient
converge at 1∕√T-rates. In comparison, when m and B are sufficiently large, neural policy gra-
dient is only shown to converge to a stationary point under the additional regularity condition that
VθJ(πθ) is Lipschitz continuous (Assumption 4.6). Moreover, by Theorem 4.8, the global optimal-
ity of such a stationary point hinges on the representation power of the overparameterized two-layer
neural network. In contrast, neural natural policy gradient is shown to attain the global optimum
when both m and B are sufficiently large without additional regularity conditions such as Assump-
tion 4.6, which reveals the benefit of incorporating more sophisticated optimization techniques to
reinforcement learning. A similar phenomenon is observed in the LQR setting (Fazel et al., 2018;
Malik et al., 2018; Tu and Recht, 2018), where natural policy gradient enjoys an improved rate of
convergence.
17
Published as a conference paper at ICLR 2020
In recent work, Liu et al. (2019) study the global optimality and rates of convergence of neural
proximal policy optimization (PPO)and trust region policy optimization (TRPO)(Schulman et al.,
2015; 2017). Although Liu et al. (2019) establish a similar 1 /√T-rate of convergence to the global
optimum, neural PPO is different from neural natural policy gradient, as it requires solving a sub-
problem of policy improvement in the functional space by fitting an overparameterized two-layer
neural network using multiple stochastic gradient steps in the parameter space. In contrast, neural
natural policy gradient only requires a single stochastic natural gradient step in the parameter space,
which makes the analysis even more challenging.
B S hared Initialization and Compatible Function Approximation.
Sutton et al. (2000) introduce the notion of compatible function approximations. Specifically, the
action-value function Qω is compatible with ∏θ if We have Vω Aω (s, a) = Vθ log ∏θ(a | S) for all
(s, a) ∈ S × A, where Aω(s, a) = Qω(s, a) 一 hQω(s, ∙), ∏θ(∙ | s)i is the advantage function cor-
responding to Qω . Compatible function approximations enable us to construct unbiased estimators
of the policy gradient, which are essential for the optimality and convergence of policy gradient
methods (Konda and Tsitsiklis, 2000; Sutton et al., 2000; Kakade, 2002; Peters and Schaal, 2008;
Wagner, 2011; 2013).
To approximately obtain compatible function approximations when both the actor and critic are
represented by neural networks, we use a shared architecture between the action-value function Qω
and the energy function of πθ, and initialize Qω and πθ with the same parameter Winit, where
[Winit]r 〜 N(0, Id/d) for all r ∈ [m]. We show that in the overparameterized regime where m is
large, the shared architecture and random initialization ensure Qω to be approximately compatible
with ∏θ in the following sense. We define φ0 = ([Φo]>,..., [Φo]m)> : Rd → Rmd as the centered
feature mapping corresponding to the initialization, which takes the form of
[Φo]r(s,a) = √= ∙ l{(s,a)>[Winit]r > 0} ∙ (s,a)	(B.1)
一 E∏θ [√m ' 1 { (S,a0)> [Winit]r > 0} ∙ (s,a0) , ∀(s,a) ∈S×A,
where Winit is the initialization shared by both the actor and critic, and we omit the dependency on
θ for notational simplicity. Similarly, we define for all (S, a) ∈ S × Athe following centered feature
mappings,
Φθ(s,a) = φθ(s,a) - E∏θ[φθ(s,a0)], Φω (s, a) = Φω (s,a) - E∏θ [φω (s,a0)].	(B.2)
Here φθ(S, a) and φω (S, a) are the feature mappings defined in (3.3), which correspond to θ and ω,
respectively. By (3.1), we have
Aω(s,a) =	Qω (s, a)	- E∏θ [Qω (s,	a0)]	=	Φω(s, a)>ω,	Vθ log∏(a	| S)= φθ(s,	a),	(B.3)
which holds almost everywhere for θ ∈ Rmd. As shown in Corollary E.3 in §E, when the width
m is sufficiently large, in policy gradient methods, both φθ and φω are well approximated by φ0
defined in (B.1). Therefore, by (B.3), we conclude that in the overparameterized regime with shared
architecture and random initialization, Qω is approximately compatible with πθ .
C S ampling From Visitation Measure.
Recall that the policy gradient VθJ(πθ) in (3.4) involves an expectation taken over the state-action
visitation measure σπθ . Thus, to obtain an unbiased estimator of the policy gradient, we need to
sample from the visitation measure σπθ . To achieve such a goal, we introduce an artificial MDP
(S, A, P, ζ, r, γ). Such an MDP only differs from the original MDP in the Markov transition kernel
P , which is defined as
Pe(s01 s, a) = Y ∙ P(s0 | s, a) + (1 一 γ) ∙ Z(s0), ∀(s, a, s0) ∈ S ×A×S.
Here P is the Markov transition kernel of the original MDP. That is, at each state transition of the
artificial MDP, the next state is sampled from the initial state distribution ζ with probability 1 一 γ.
18
Published as a conference paper at ICLR 2020
In other words, at each state transition, we restart the original MDP with probability 1 - γ . As
shown in Konda (2002), the stationary state distribution of the induced Markov chain is exactly
the state visitation measure νπθ . Therefore, when we sample a trajectory {(St, At)}t≥0, where
So 〜Z(∙), At 〜 ∏(∙ | St), and St+ι 〜 P(∙ | St ,At) for all t ≥ 0, the marginal distribution of
(St, At) converges to the state-action visitation measure σπθ .
D	Proof of Main Results
In this section, we present the proof of Theorems 4.7, 4.8, and A.4. Our proof utilizes the following
lemma, which establishes the one-point convexity of J(∏) at the global optimum ∏*. Such a lemma
is adapted from Kakade and Langford (2002).
Lemma D.1 (Performance Difference (Kakade and Langford, 2002)). It holds for all π that
JI(∏*) - J (∏) = (1 - γ)-1 ∙ Eν*[hQπ (s, ∙),π*(∙ | S)- π(∙ | s)i],
where ν* is the state visitation measure corresponding to ∏*.
Proof. Following from Lemma J.1, which is Lemma 6.1 in Kakade and Langford (2002), it holds
for all π that
J (∏*) - J (∏) = (1 - γ)-1 ∙ Eσ*[Aπ (S, a)],	(D.1)
where σ* is the state-action visitation measure corresponding to ∏*, and An is the advantage function
associated with π. By definition, we have σ*(∙, ∙) = ∏*(∙∣∙)∙ ν*(∙). Meanwhile, it holds for all S ∈ S
that
E∏* [Aπ(s, a)] = E∏* [Qπ(s, a)] - Vπ(s) = W(s, ∙), π*(∙ | SD-W(s, ∙), ∏(∙ | s)i
=hQπ(s,∙),π*(∙∣ s) - π(∙∣ s)i.	(D.2)
Combining (D.1) and (D.2), we conclude that
JI(∏*) - J (∏) = (1 - γ)-1 ∙ Eν*[hQπ (s, ∙),π*(∙ | s) - π(∙ | s)i],
which concludes the proof of Lemma D.1.	□
D.1 Proof of Theorem 4.7
Proof. We first lower bound the difference between the expected total rewards of πθi+1 and πθi . By
Assumption 4.6, Vθ J(∏θ) is L-Lipschitz continuous. Thus, it holds that
J(∏θi+ι) - J(∏θi) ≥ η ∙Vθ J(∏θi)>δi - L/2 ∙ kθi+ι - θik2,	(D.3)
where δi = (θi+ι - θi)∕η. Recall that ξi = Vθ J(∏θJ - E[Vθ J(∏θJ], where the expectation is
taken over σi given θi and ωi . It holds that
VθJ(πθi)>δi = VθJ(πθi) - EVb θ J (πθi)]	δi - ξi> δi + VbθJ(πθi)>δi.	(D.4)
On the right-hand side of (D.4), the first term represents the error of estimating Vθ J (πθi ) using
E[Vθ J (∏θi)] = Eσi [Vθ log ∏θi(a | s) ∙ Qωi (s, a)], the second term is related to the variance of the
estimator VθJ(πθi) of the policy gradient VθJ(πθi), and the last term relates the increment δi of
the actor update to VθJ(πθi). In the following lemma, we establish a lower bound of the first term.
Lemma D.2. It holds that
∣(Vθ J(∏θi) - e[Vθ J(∏θi)])>δj ≤ 4κ ∙ R∕η ∙kQπθi - Qωi∣L,
where VJ (πθi) is defined in (3.7), ςi is the stationary state-action distribution, and κ is the absolute
constant defined in Assumption 4.5. Here the expectation is taken over σi given θi and ωi.
Proof. See §H.5 for a detailed proof.
□
19
Published as a conference paper at ICLR 2020
For the second term on the right-hand side of (D.4), we have
-ξ>δi ≥-kξik2∕2-kδik2∕2.	(D.5)
Now it remains to lower bound the third term on the right-hand side of (D.4). For notational sim-
plicity, we define
ei = θi+ι - (% + η ∙ ▽ J(πθi)) = πb(% + η ∙ ▽ J(πθi)) - (θi + η ∙ ▽ J5θJ),
where ΠB is the projection operator onto B. It then holds that
e> h∏B(θi	+ η ∙ VJ(∏θi)) - Xi	=	e>R+ι	- x)	≤	0,	∀x	∈	B.	(D.6)
Specifically, setting x = θi in (D.6), we obtain that ei> δi ≤ 0, which implies
VbJθ(πθi)>δi = (δi - ei∕η)> δi ≥ kδik22.	(D.7)
By plugging Lemma D.2, (D.5), and (D.7) into (D.4), we obtain that
VθJ(∏θi)>δi ≥ -4κ ∙ R∕η ∙ kQπθi - Qωi∣L + kδik2/2 -kξik2∕2.	(D.8)
Thus, by plugging (D.8) and the definition that δi = (θi+1 - θi)∕η into (D.3), we obtain for all
i ∈ [T] that
(1 - L ∙ η) ∙ E[kδik2∕2]
≤ η-1 ∙ E[J(∏θi+ι) - J(∏θi)] + 4κ ∙ R∕η ∙ E[kQπθi - Qωi∣L] + E[kξik2∕2],	(D.9)
where the expectations are taking over all the randomness.
Now we turn to characterize kρi - δik2. By the definition of ρi in (4.3), we have
IlPi- δik2 = η-1 ∙ ∣∣∏b(θi + η ∙ VθJ(∏θj) - θi - (∏b(θi + η ∙ VθJ(∏θj) - θ)(
=η-1 ∙ ∣∣∏b(θi + η ∙ VθJ(∏θi)) - ∏b(θi + η ∙ VθJ(∏θj)∣∣2
≤ IVθ J (πθi ) - VbθJ(πθi )I2.	(D.10)
The following lemma further upper bounds the right-hand side of (D.10).
Lemma D.3. It holds for all i ∈ [T] that
E[kVθJEi)- VθJ(∏θi)k2] ≤ 2E[kξik2] +8κ2 ∙ E[kQπθi - Q-ik".
Here the expectations are taken over all the randomness.
Proof. See §H.6 for a detailed proof.
□
Recall that we set η = 1∕√T. UPon telescoping (D.9), it holds for T ≥ 4L2 that
min
i∈[T]
E[kρik22] ≤
T
1∕T ∙ X E[kPi k2]
i=1
T
≤ 1∕T ∙ X(2E[kδik2] +2E[kρi - δik2])
i=1
T
≤ 1∕T ∙ X4(1 - L ∙ η) ∙ E[∣∣δik2] + 2E[∣∣ρi -切图
i=1
T
≤ 8∕√T ∙ E[J(∏θτ+1) - J(∏θι)] + 8∕T ∙ XE[kξik2] + εQ(T),
i=1
(D.11)
20
Published as a conference paper at ICLR 2020
where the third inequality follows from the fact that 1 - L ∙ η ≥ 1/2, while the fourth inequality fol-
lows from (D.9), (D.10), and Lemma D.3. Here the expectations are taken over all the randomness,
and εQ (T ) is defined as
T	T
εQ(T ) = 32κ ∙ R∕√T ∙ X E[kQπθi - QωikJ +16κ2∕T ∙ X E[kQπθi - Qωik".
i=1	i=1
By Proposition 4.3 and Assumption 4.4, it holds for all i ∈ [T] that
E[kQπθi - Qωik" = O(R3 ∙ m-1/2 + R5/2 ∙ m-1/4),	E[kξik2] ≤ σξ2∕B.
By plugging (D.12) into (D.11), we conclude that
min E[kρik2] ≤ 8∕√T ∙ E[J(∏θ, +J- J — + 8σ2∕B + eq(T),
i∈[T]
where
(D.12)
eq(T) = K ∙ O(R5/2 ∙ m-1/4 ∙ T1/2 + R9/4 ∙ m-1/8 ∙ T1/2).
Thus, we complete the proof of Theorem 4.7.
□
D.2 Proof of Theorem 4.8
Proof. Since θ is a stationary point of J(πθ), it holds that
VθJ(∏b)>(θ - b) ≤ 0, ∀θ ∈ B.	(D.13)
Therefore, by Proposition 3.1, we obtain from (D.13) that
vθJ(πb)> (θ - θ) = Eσπb [φb(S, a)> (θ - b) ∙ Qnb(S, a)] ≤ 0, ∀θ ∈ B.	(D.14)
Here φθb and φθb are defined in (3.3) and (B.2) with θ = θ, respectively. Note that
Eσ∏b [φb(s,a)>(θ - b) ∙ V"b(S)] = EVnb[E∏b [φb(s, a)]>(θ - b) ∙ Vπb(S)] = 0,
Eσ∏b[E∏b [Φb(s,a0)>(θ - b)] ∙ Aπb(s,a)i = EVnb [e∏θ∙ [Φb(s,a0)>(θ - b)] ∙ Enb [Aπb(s,a)] = 0,
which holds since Enb[φg(s, a)] = Enb[Anb(s, a)] = 0 for all s ∈ S. Thus, by (D.14), We have
Eσ∏一 [Φb(s,a)>(θ - b) ∙ Qnb(s,a)]
θ
=Eσ∏b [Φb(s, a)>(θ - b) ∙ Anb(s, a)] - Eσ∏b [e∏θ∙ [φg‹s, a0)>(θ - b)] ∙ Anb(s, a)]
+ Eσ∏b[Φb(s,a)>(θ - b) ∙ Vnb(s)]
θ
=Eσ∏b [Φb(s, a)>(θ - b) ∙ Anb(s, a)] ≤ 0, ∀θ ∈ B.	(D.15)
Meanwhile, by Lemma D.1 we have
(1 - γ) ∙ (J(∏*) - J(∏b)) = Eν* [hAnb(S, ∙),∏*(∙ | s) - ∏b(∙ | s)i].	(D.16)
In what follows, we write ∆θ = θ - θ. Combining (D.15) and (D.16), we obtain that
(1 - γ) ∙ (J(∏*)- J(∏b))
≤ EV* [hAnb(S, ∙), π*(∙ | s) - πb(∙ | S)i] - Eσ∏一 [φb(s, a)θ ∙ Anb(S, a)]
θ
=Eν* [hAnb(s, ∙), ∏*(∙ | s) - ∏b(∙ | s)i] - EVnb [hAnb(s, ∙),φ^(s, •)>△© ∙ π4 | s)i], (D.17)
θ
where We use the fact that σnb (∙, ∙) = ∏^(∙ | ∙) ∙ Vn看(∙). It remains to upper bound the right-hand side
of (D.17). By calculation, it holds for all (s, a) ∈ S × A that
(∏*(a | s) - ∏b(a | s))dν*(s) - φ.(s, a)>∆θ ∙ ∏.(a | s)dν.(s)
=(π (a Inb(-∣πb(a | S) ∙ dVb(S) - φbG a)>4) ∙ πb(a | S)dνnb(S)
= uθb(s, a) - φθb(s, a)>θ dσnθb(s, a),	(D.18)
21
Published as a conference paper at ICLR 2020
where uθb is defined as
uθ(s, a) = :∙π* (s, a) — ^VnI(S) + φbb(s, a)>θ, ∀(s, a) ∈ S ×A.
dσπθb	dνπθb
Here dσπ* /dσπb and dνπ* /dνπ b are the Radon-Nikodym derivatives. By plugging (D.18) into
(D.17), we obtain that
(1 - Y) ∙ (J(∏*)- J(∏b))
≤ Eν* [hAπb(s, ∙), ∏*(∙ | s) - ∏b(∙ | s)i] - EVnb [hAπb(S, ∙), Φθ(s, ∙)>∆θ ∙ ∏b(∙ | s)i]
θ
=X X Anb(s,a) ∙ ((∏*(a | S)- ∏bb(a | s))dν*(s) - φbb(s,a)>∆θ ∙ πbb(a | s)dνbb(s))
S a∈A
=/	Anb(s, a) ∙ (Ub(s, a) - Φb(s, a)>∆θ)dσ∏b(s, a)
S×A
≤kAπb (∙,∙)∣∣σ∏ b∙k"b(∙, ∙) — Φθ(∙, ∙)>θkσ∏",
θθ
(D.19)
where the second equality follows from (D.18) and the last inequality is from the Cauchy-Schwartz
inequality. Note that ∣Aπb(s, a)| ≤ 2Qmaχ for all (s, a) ∈ S×A. Therefore, it follows from (D.19)
that
(I- Y) ∙ (J (π*) - J (πb)) ≤ 2Qmax ∙ Ilub(∙, ∙) - φθ(∙, ∙)>θkσ∏. , ∀θ ∈ B.
θ
(D.20)
Finally, by taking the infimum of the right-hand side of (D.20) with respect to θ∈ B, we obtain that
(1-γ) ∙ (J (∏*) - J (∏b)) ≤ 2Qmax ∙ inf k"b(∙, ∙) - Φθ(∙, ∙)>θkσ∏ b,
θ∈B	θ
which concludes the proof of Theorem 4.8.
□
D.3 Proof of Theorem A.4
Proof. For notational simplicity, we write πi = πθi hereafter. In the following lemma, we charac-
terize the performance difference J(∏*) - J(∏i) based on Lemma D.1.
Lemma D.4. It holds that
(1 - Y) ∙η∙ (J (∏*)- J (∏i)) = Ev*[dkl(∏*(∙∣ S)Ilni(∙∣ s)) - Dkl(∏*(∙∣ S)Ilni+i(∙∣ s))
-DKL (ni+1(∙ Ι S)IIni卜 | s))] - Hi,
where Hi is defined as
Hi =	EV*	h<log(ni+ι(∙ |	s)∕ni(∙	|	S))	- η ∙ Q3i(s, ∙),n*(∙ | S)-	ni(∙	|	s)〉i	(D∙21)
S--------------------------------{z-------------------------------}
(i)
+ η ∙ Eν* [hQωi(S, ∙) - Qni(S, ∙), n*(∙ | S) - ∏i(∙ | S)i]
V-------------------------{z----------------------}
(ii)
+ Eν* [<log(∏i(∙ | S)∕∏i+ι(∙ | S)),∏i+ι(∙ | s) - ∏i(∙ | s)〉].
X-------------------------{--------------------------}
(iii)
Proof. See §H.7 for a detailed proof.	□
Here Hi defined in (D.21) of Lemma D.4 consists of three terms. Specifically, (i) is related to
the error of estimating the natural policy gradient using (3.8). Also, (ii) is related to the error of
estimating Qπi using Qωi . Meanwhile, (iii) is the remainder term. We upper bound these three
terms in §H.8. Combining these upper bounds, we obtain the following lemma.
22
Published as a conference paper at ICLR 2020
Lemma D.5. Under Assumptions 4.2 and A.3, we have
E	|Hi|- Eν+	hDKL (πi+1(∙	Ι S)IIni(I S))]	≤	η2	∙	(9R2	+ M2)	+ η 31 +	ψi)	∙ εQ,i + εi∙
Here the expectation is taken over allthe randomness. Meanwhile, * and ψi are the concentrability
coefficients defined in (A.1) of Assumption A.2, εQ,i is defined as εQ,i = E[kQπi -Qωikςi],Mis
the absolute constant defined in Assumption A.3, and εi is defined as
1/2
Ei = √2 ∙ R1/2 ∙ η Si + ψi) ∙ τ-1 ∙ {E[kξi(δi)k2] + E[kξi(ωi)k2] }	(D.22)
+ O((Ti+1 + η) ∙ R3/1 2 ∙ m-1/4 + η ∙ R5/4 ∙ m-1/8).
Here ξi(δi) and ξi(ω. are defined in Assumption A.1, where δi = η-1 ∙ (τi+ι ∙ θ%+ι - Ti ∙ θi), while
*i and ψi are the concentrability coefficients defined in (A.1) of Assumption A.2.
Proof. See §H.8 for a detailed proof.
□
By Lemmas D.4 and D.5, we obtain that
(1 - Y)	∙	E[J(n*)	- J(∏i)]	≤ η-1	∙ E Eν*	[Dkl(∏*(∙	| s)k∏i(∙	| S))	(D.23)
-Dkl(∏*(∙∣ s)k∏i+ι(∙∣ s))]
+ η ∙ (9R2 + M2) + η-1 ∙ ɛi + (* + ψ0) ∙ εQ,i,
where εQ,i is defined as εQ,i = E[kQπi -Qωi kςi], M is the absolute constant defined in Assumption
A.3, Ei is defined in (D.22) of Lemma D.5, and the expectations are taken over all the randomness.
Recall that we set η = 1/√T. Upon telescoping (D.23), We obtain that
(1 — Y) ∙ min E[J(π*) — J(πi)] ≤
i∈[T]
1T
--γ ∙ ∑E[J(n*)- J(∏i)]
T	i=1
(D.24)
1
E EVJDKL(n*(∙∣ s)∣∣∏ι(∙∣ s))] +9R2 + M2
1 T
+ T ∙ Ε(√T ∙ εi + (*i + ψi) ∙ εQ,i),
T i=1
where the expectations are taken over all the randomness and the last inequality follows from the
fact that
Dkl(∏*(∙ |	s)∣∣∏t +ι(∙ |	s))	≥ 0,	VS	∈	S,	∀θτ +1	∈	Rmd.
In what follows, we upper bound the right-hand side of (D.24). Note that we set T1 = 0. By the
parameterization of policy in (3.2), it then holds that ∏1(∙ | s) is uniform over A for all S ∈ S and
θ1 ∈ Rmd . Therefore, we obtain that
Dkl(∏*(∙∣ S)k∏1(∙∣ s)) ≤ log |A|, VS ∈S, ∀θ1 ∈ Rmd.	(D.25)
Meanwhile, by Assumption A.1, we have
E[kξi(δi)k2] ≤ nEhEσi [kξi(δi)k2]]01/2 ≤ Ti2 ∙σξ ∙ BT/2,
where the expectation Eσi [kξi(δi)k22] is taken over σi given θi and ωi, while the other expectations
are taken over all the randomness. A similar upper bound holds for E[kξi(ωi)k2]. Therefore, by
plugging the upper bounds of E[kξi(σi)k2] and E[kξi(ωi)k2] into Ei defined in (D.22) of Lemma
D.5, we obtain from Assumption A.2 that
√T ∙ εi ≤ 2√2c0 ∙ R1/2 V/ ∙ BT/4	(D.26)
+ O((Ti+1 ∙ T1/2 + 1) ∙ R3/2 ∙ m-1/4 + R5/4 ∙ m-1/8).
23
Published as a conference paper at ICLR 2020
Also, combining Assumption A.2 and Proposition 4.3, it holds that
Wi + ψ0)	∙ εQ,i ≤	2co	∙	E[kQπi -	Qωi∣L] = co	∙	O(R3/2 ∙ m-1/4	+ R5/4	∙	m-1/8).	(D.27)
Finally, by plugging (D.25), (D.26), and (D.27) into (D.24) and setting
&(T) = √T ∙ εi + (d + ψi) ∙ εQ,i,
we complete the proof of Theorem A.4.
□
E Linearization Error
In this section, we lay out a fundamental lemma that characterizes the distance between a two-layer
neural network φθ>θ and its linearization φ0>θ, where φθ is the feature mapping of the two-layer
neural network defined in (3.3) and φ0 is the feature mapping corresponding to the initial parameter
Winit.
We first introduce a function class that consists of lineaizations of f ((∙, ∙); W) defined in (3.1).
Definition E.1 (Function Class). Let R > 0 be an absolute constant. For all m ∈ N, we define
1m
FRm = f f((s, a)； W) = √m ∙ Ebr ∙ 1 {[Winit]>(s, a) > 0} ∙ [W]>(s, a)	(E.1)
r=1
: kW-Winitk2 ≤R ,
where [Winit]r ~ N(0, Id/d) and br ~ Unif({-1,1}) are the initial parameters of the two-layer
neural network defined in (3.1).
Note that FeR,m in (E.1) is a class of functions that are linear in W but nonlinear in (s, a).
Meanwhile, it holds that Vwf((s,a); W) = Vwf((s,a); W)|w=Winit for all (s,a) ∈ S × A,
where f ((∙, ∙); W) is the two-layer neural network defined in (3.1). Thus, f ((∙,∙); W) can be
viewed as the linearization of f ((∙, ∙); W) at the initial parameter 卬由上. Moreover, for a fixed
R, the linearization error of f((∙, ∙); W) decays to zero as the width m → ∞. Intuitively, since
kW - Winitk2 is upper bounded by R, the differences between blocks k[W]r - [Winit]rk2 are suf-
ficiently small for a sufficiently large m and all r ∈ [m]. As a result, for a sufficiently large m,
we have 1{[Winit]r>(s, a) > 0} = 1{[W]r> (s, a) > 0} with high probability for all r ∈ [m] and
(s, a) ∈ S ×A, and thus f ((∙, ∙); W) is well approximated by its linearization f((∙, ∙); W).
The following lemma formally characterizes the corresponding linearization error.
Lemma E.2 (Linearization Error (Cai et al., 2019)). Let Winit be the initial parameter of the two-
layer neural network defined in (3.1). Let B = {α ∈ Rmd : kα - Winitk2 ≤ R}. Under Assumption
4.2, it holds for all θ, θ0 ∈ B that
Einit [kΦθ(∙, ∙)>θ0 - φo(∙, ∙)>θ0kσ] = O(R3 ∙ m-1/2),
where the expectation is taken over the random initialization. Here φθ and φ0 are the feature map-
pings defined in (3.3), which correspond to θ and Wait, respectively, and σ(∙, ∙) = ∏(∙∣∙) ∙ V(∙) is
the distribution over S × A such that Assumption 4.2 holds.
Proof. By the definition of feature mapping in (3.3), we obtain that
φθ(s, a)>θ0 - φ0(s, a)>θ0
m
=√m ∙〉： (1{(s, a)>[θ]r > 0} - 1{(s, a)> [Winit]r > 0}) ∙ (s, a)> [θ0]r ∙
r=1
Meanwhile, for 1{(s, a)> [θ]r > 0} 6= 1{(s, a)> [Winit]r > 0}, we have
∣(s,a)>[Winit]r | ≤ ∣(s,a)>[θ]r-(s,a)>[Winit]r∣ ≤ k(s,a)∣∣2 ∙k[θ]r-[Winit]r∣∣2,
(E.2)
(E.3)
24
Published as a conference paper at ICLR 2020
where the last inequality follows from the Cauchy-Schwartz inequality. Recall that k(s, a)k2 ≤ 1
for all (s, a) ∈ S × A. Thus, it follows from (E.3) that
∣l{(s,a)>[θ]r > 0} - l{(s,a)>[Winit]r > 0}∣
≤ l{∣(s,a)>[Winit]r | ≤ k[θ]r — [Winit]rk2}.	(E.4)
By plugging (E.4) into (E.2), we obtain that
∣φθ(s, a)>θ0 - φo(s, a)>θ0∣
1m
≤√√m ∙ X 1 { | (s,a)> [Winit]r | ≤ k [θ]r - [Winit]r k 2 } ∙∣(s,a)> [θ0]r |
r=1
m
≤ √m ∙ ^X 1 {|(S, a)> [Winit]r | ≤ k [θ]r - [Winit]r k 2 }
r=1
∙	(|(s, a)> [Winit]r | + | (s, a)> ([θ0]r - [Winit]r) |)
1m
≤ √m ∙ ^X 1 {|(S, a)> [Winit]r | ≤ k [θ]r - [Winit]r k 2 }	(E，5)
r=1
∙	(∣(s,a)>[Winit]r | + k[θ0]r-[Winit]rk2),
where the last inequality follows from the Cauchy-Schwartz inequality and the fact that k(S, a)k2 ≤
1. Following from the fact that 1{|x| ≤ y} ∙ |x| ≤ 1{|x| ≤ y} ∙ y, we obtain from (E.5) that
∣φθ(s,a)>θ0 - φo(s,a)>θ0∣
m
≤	√m ∙ ^X 1{|(S, a)> [Winit]r | ≤ k [θ]r - [Winit]r k 2 }	(ES)
r=1
∙ (k[θ]r - [Winit]rk2 + k [θ0]r - [Winit]r k2).
Therefore, following from the Cauchy-Schwartz inequality, we obtain from (E.6) that
∣φθ(s,a)>θ0 - φo(s,a)>θ0∣2	(E.7)
≤ — ∙ ^X 1 {|(S, a)> [Winit ]r | ≤ k [θ]r - [Winit]r k2 }
m r=1
m
∙ X(2k[θ]r - [Winit]rk2 + 2k[θ0]r - [Winit]r k2)
r=1
≤ — ∙ ^X 1{|(s, a)> [Winit ]r | ≤ k [θ]r - [Winit]r k 2 } ∙ 2(kθ - Winitk2 + kθ0 - Winitk 2),
m r=1
where the first inequality follows from the fact that (x + y)2 ≤ 2x2 + 2y2. Recall that θ, θ0 ∈ B,
where B = {α ∈ Rmd : kα - Winit k2 ≤ R}. Thus, following from (E.7), we have
4R2	m
∣Φθ(s,a)>θ0 - Φo(s,a)>θ0∣2 ≤ ~m ∙ 】1{∣(s,a)>[Winit]r∣≤k[θ]r -[Winit]rk2}
By Assumption 4.2, we obtain from (E.8) that
kφθ(∙, ∙)>θ0 - φo(∙, ∙)>θ0kσ = Eσ [∣φθ(s, a)>θ0 - φo(s, a)>θ0∣2]
≤ 4C ∙ R ^ X k[θ]r -[Winit]rk2
—m	r=1	k[Winit]r k2	，
(E.8)
(E.9)
where c is the absolute constant defined by Assumption 4.2. It now suffices to take the expectation
of the right-hand side of (E.9) over the random initialization. Following from the Cauchy-Schwartz
25
Published as a conference paper at ICLR 2020
inequality, we obtain that
(X k⅞⅛k2 )2≤ (X 弛 LsM ∙ (X"n")
m
=k。- Winitk2 ∙ X 1∕kWinit]rk2
r=1
m
≤ R2 ∙ X 1∕kWinit]rk2 ,	(E.10)
r=1
where the last inequality follows from the fact that θ ∈ B. Therefore, combining (E.9) and (E.10),
we conclude that
Einit [∣∣φθ (∙, ∙)>θ0 - Φ0(∙, ∙)>θ0kσ ] ≤ 4cmR3 ∙ Einit ](X 1/k [Winit]rk2) 1/2
≤ 4cmR3 ∙ (X Einit [1/k [Winit]rk2]) W
=4cι ∙ R3 ∙ m-1/2,
where the second inequality follows from the Jensen,s inequality and ci = c∙Ex〜N(o,id/d)[1∕∣∣x∣∣2].
Thus, We complete the proof of Lemma E.2.	□
By Lemma E.2, the linearization φ0>θ converges to the two-layer neural network φθ>θ as the width
m → ∞. Based on Lemma E.2, the following corollary characterizes a similar convergence where
the feature mappings φθ and φo are replaced by the centered feature mappings φ° and φθ defined in
(B.1) and (B.2), respectively.
Corollary E.3. Let Winit be the initial parameter and B = {α ∈ Rmd : kα - Winit k2 ≤ R} be the
parameter space. Under Assumption 4.2, it holds for all θ, θ0 ∈ B that
Einit [kφθ(∙, ∙)>θ0 - Φ0(∙, ∙)>θ0kσ] = O(R3 ∙ m-1/2),
where the expectation is taken over the random initialization. Here φ° and φθ are the centered feature
mappings defined in (B.1) and (B.2), respectively, and σ(∙, ∙) = ∏(∙∣∙) ∙ V(∙) is the distribution over
S × A such that Assumption 4.2 holds.
Proof. By the definitions of φ° and φθ in (B.1) and (B.2), respectively, we obtain that
ι∣φθ (∙, ∙)>θ0—φo(∙, ∙)>θ0kσ = ∣∣φθ (∙, ∙)>θ0—φ0 (∙, ∙)>θ0—E∏θ [φθ (∙, aO)>θ0—φo(∙, a0)>θ0] ∣∣σ
≤ 2kφθ (∙, ∙)>θ0—φo(∙, ∙)>θ0kσ+2kφθ (∙, ∙)>θ0—φo(∙, ∙)>θ0k∏θ ∙ν,
where the second inequality follows from the Jensen’s inequality and the fact that kx + yk22 ≤
2kxk22 + 2kyk22. Therefore, by Assumption 4.2 and Lemma E.2, we obtain that
Einit [kφθ(∙, ∙)>θ0 — Φ0(∙, ∙)>θ0kσ]
≤ 2Einit[kΦθ(∙, ∙)>θ0 — Φ0(∙, ∙)>θ0kσ] + 2Einit[kΦθ(∙, ∙)>θ0 — Φ0(∙, ∙)>θ0k∏θ∙ν] = O(R3 ∙ m-1/2),
which concludes the proof of Corollary E.3.	□
In what follows, we present a corollary that quantifies the difference between the function φ^(∙, ∙)>θ
and the two-layer neural network f ((∙, ∙); θ) = φθ(∙, ∙)>θ by the L2(σ)-norm, where σ(∙, ∙) =
∏(∙ | ∙) ∙ v(∙) is the distribution over SXA such that Assumption 4.2 holds.
Corollary E.4. Let B = {α ∈ Rmd : kα —Winitk2 ≤ R}. Under Assumption 4.2, it holds for all
θ, θb ∈ B that
Einit [kΦb(∙, ∙)>θ — φθ(∙, ∙)>θkσ]=O(R3/2 ∙ m-1/4),
where the expectation is taken over the random initialization. Here φθ is the feature mapping defined
in (3.3), and σ(∙, ∙) = π(∙ | ∙) ∙ V(∙) is the distribution over S ×A such that Assumption 4.2 holds.
26
Published as a conference paper at ICLR 2020
Proof. By the triangle inequality, we have
Einit [kφb(∙, ∙)>θ - φθ(∙, ∙)>θkσ]
≤ Einit [kΦb(∙, ∙)>θ — Φθ(∙, ∙)>θkσ ] + Einit [kΦθ (∙, ∙)>θ - Φθ (∙, ∙)>θ∣∣σ ],	—
where φ0 is the feature mapping defined in (3.3) with θ = Winit . Meanwhile, for all θ, θ ∈ B =
{α ∈ Rmd : kα - Winitk2 ≤ R}, it follows from Assumption 4.2 and Lemma E.2 that
Einit [kΦb(∙, ∙)>θ — Φθ(∙, ∙)>θkσ]=O(R3/2 ∙ m-1/4),
Einit [kφθ (∙, ∙)>θ — φo(∙, ∙)>θkσ ] = OR3/ ∙ m-1/4),	(Ε.12)
where the expectations are taken over the random initialization. Combining (E.11) and (E.12), we
obtain that
Einit [kΦb(∙, ∙)>θ — Φθ (∙, ∙)>θkσ ] = OR3/ ∙ m-1/4),
which concludes the proof of Corollary E.4.	□
Corollary E.4 implies that when the width m is sufficiently large, φg(∙, ∙)>θ is well approximated
by the two-layer neural network f ((∙, ∙); θ) in L/(o)-norm, where σ(∙, ∙) = π(∙ | ∙) ∙ V(∙) is the
distribution over S × A such that Assumption 4.2 holds.
F Neural TD
In this section, we introduce the details of neural TD (Cai et al., 2019) for critic update in Algorithm
1. Neural TD solves the optimization problem in (3.11) using the TD iterations defined in (3.12) and
(3.13), which is summarized in Algorithm 2.
Algorithm 2 NeUraI TD (Cai et al., 2019)_____________________________________________________
Require: The policy ∏, number of TD iterations TTd, and learning rate ητD of neural TD.
1:	Initialization: Initialize b 〜Unif({-1,1}) and [Winit]r 〜N(0, Id/d). Set B 一 {α ∈
Rmd ： kα — Winitk/ ≤ R} and ω(O) - Winit.
2:	for t = 0, . . . , TTD — 1 do
3:	Sample a tuple (s, a, r, s0, a0), where (s, a)〜ςi, s0 〜P(∙ | s, a), r J r(s, a), and a0 〜
π(T s0).
4:	Compute the Bellman residue δ J Qω(t)(s, a) 一 (1 一 Y) ∙ r 一 Y ∙ Qω(t)(s0, a0).
5:	Perform	a TD update step: ω(t + 1/2) J ω(t) 一 η ∙ δ ∙ VωQω(t)(s,	a).
6:	Perform	a projection step: ω(t + 1) J ΠB(ω(t + 1/2)).
7:	Perform	an averaging step: ω J 扉∙ ω + 士 ∙ ω(t + 1).
8:	end for
9:	Output： Qout (∙) J Qω (∙).
The following theorem by Cai et al. (2019) characterizes the rate of convergence of Algorithm 2.
Theorem F.1 (Convergence of Neural TD (Cai et al., 2019)). We set ητD = min{(1 -
γ)/8,1/√Ttd} in Algorithm 2. Under Assumption 4.2, it holds that
Einit [kQout - Qπk2∏] ≤ 2Einit[k∏-eR,mQn - Qπk2∏]	(F.1)
+ O(R2 ∙ T-D/2 + R3 ∙ m-1∕/ + R5// ∙ m-1/4),
where ΠFe	is the projection operator onto FR,m, and ςπ is the stationary state-action distribution
corresponding to π .
Proof. See Proposition 4.7 in Cai et al. (2019) for a detailed proof.
□
27
Published as a conference paper at ICLR 2020
F.1 Proof of Proposition 4.3
Proof. By Theorem F.1, to establish the rate of convergence of neural TD, it suffices to characterize
the approximation error Einit[kΠFe Qπ - Qπ kς2π] in (F.1). To this end, we first define a new
function class
F Rm
m
• X br ∙ l{[Winit]> (s,a) > 0} ∙ W> (s, a)
r=1
k[W]r - [Winit]rk∞ ≤ R/
where [Winit]r 〜 N(0,Id∕d) and b 〜 Unif({-1,1}) are the initial parameters. By definition,
FR,m is a subset of FR,m defined in Definition E.1. The following lemma obtained from Rahimi
and Recht (2009) characterizes the deviation of FRm from Fr,∞ given in Assumption 4.1.
Lemma F.2 (Projection Error of FRm (Rahimi and Recht, 2009)). Let f ∈ Fr,∞, where Fr,∞ is
defined in Assumption 4.1. For any δ > 0, it holds with probability at least 1 - δ that
k∏FRm f - f kς ≤ R • m-1/2 • [1 + P2log(1∕δ)],	(F.2)
where ς is a distribution over S × A.
Proof. See Rahimi and Recht (2009) for a detailed proof.	□
Following from (F.2) in Lemma F.2, for all f ∈ FR,∞ and t > 0, we have
P(k∏FRm f - fkς ≥ t) ≤ exp(-1∕2 • (t ∙√m∕R - 1)2).	(F.3)
Meanwhile, by Assumption 4.1, we have Qπ ∈ FR,∞. Therefore, by setting f = Qπ and ς = ςπ in
(F.3), we obtain that
∞
Einit [kπF Rm Qn -Qnk2」=J0	P(kπF Rm Qn - Q"k2∏ ≥ t)dt
∞
≤	exp(-1/2 • (t • √m∕R - 1)2)dt = O(R • m-1/2),
0
(F.4)
where the expectation is taken over the random initialization. Also, note that FRm ⊆ JeR,m, where
FR,m is defined in Definition E.1. Therefore, it follows from (F.4) that
Einit [k∏FR,mQn - Qπk2∏] ≤ Einit [k∏FRmQn - Q"k2∏] = O(R • m-1/2).	(F.5)
Combining (F.5) and Theorem F.1, We obtain for ητD = min{(1 - γ)∕8, 1∕√Ttd} that
Einit [kQout - Qnkσπ] =	O(R • m-1∕2 + R2	•	T-D/2	+	R3 • m-1/2	+	R5/2	• m-1∕4).	(F.6)
Specifically, Qωi is the output of Algorithm 2 with ∏θi as the input. Finally, by setting TTD = Ω(m)
in (F.6), we obtain
Einit [kQωi - Qnθi k2i] = O(R3 • m-1/2 + R5/2 • m-1/4),
which concludes the proof of Proposition 4.3.	□
G Projection-Free Neural Policy Gradient
In this section, we study the convergence of neural policy gradient where we do not impose the
projection in the actor update. Specifically, the projection-free actor update takes the form of
θi+1 - θi + η • VθJ(πθi).
28
Published as a conference paper at ICLR 2020
Here Vθ J(不§) is an estimator of the policy gradient Vθ J(∏θi), which takes the form of
B
τi
VθJ(∏θj = B ∙ 2^Qωi(s`,a`) ∙ Vθlog∏θi(a` | s`).	(G.1)
'=1
Here Ti is the temperature parameter of ∏θ%, {(s', a')}'∈[B] is sampled from the state-action visita-
tion measure σi corresponding to the current policy πθi , and B > 0 is the batch size. Also, Qeωi is
the modified critic. Specifically, for all (s, a) ∈ S × A, we define
Qωi (s, a)
Qmax
∙ 1 {Qωi (s, a) ≥ Qmax}
Qmax
∙1 Qωi(s,a)≤-Q
max }
(G.2)
+ Qωi (S, a) ∙ 1{ ―QmaX < Qωi (s, a) < Qmax },
where Qωi is obtained from Algorithm 2 with πθi as the input. We summarize projection-free neural
policy gradient in Algorithm 3.
Algorithm 3 Projection-Free Neural Policy Gradient
Require: Number of iterations T, number of TD iterations TTd, learning rate η, learning rate ητD
of neural TD, temperature parameters {τi}i∈[T +1], and batch size B.
1:	Initialization: Initialize b 〜Unif({-1,1}) and [Winit]r 〜N(0, Id/d) for all r ∈ [m]. Set
B一{α ∈ Rmd ： kα - Winitk2 ≤ R} and θι J 卬山心
2:	for i ∈ [T] do
3:	Update ωi using Algorithm 2 with πθi as the input, ω(0) J Winit and {br}r∈[m] as the
initialization, TTD as the number of iterations, and ηTD as the learning rate.
4:	Sample {(s', a')}'∈[B] from the visitation measure σ%, and estimate Vθ J(∏θ) using (G.1)
and (G.2).
5:	Update θi+1 by θi+1 J θi + η ∙ Vθ J (πθi ).
6:	end for
7:	Output: {πθi}i∈[T +1] .
G. 1 Convergence of Projection-Free Neural Policy Gradient
In this section, we show that the sequence {θi}i∈[T+1] generated by projection-free neural policy
gradient converges to a stationary point at a sublinear rate. In parallel to Assumption 4.4, we lay out
the following regularity condition on the moments of the estimator Vθ J (πθi ).
Assumption G.1 (Moment Upper Bound). Recall that σi is the state-action visitation measure cor-
responding to πθi for all i ∈ [T]. Let ξi = VθJ(πθi) - E[Vθ J (πθi)], where VθJ(πθi) is defined
in (G.1). We assume that there exists absolute constants σp % > 0 such that E[∣∣ξi∣∣2] ≤ Ti ∙ σ-∕B
and E[∣∣ξi∣∣3] ≤ T ∙ ςg/B3/2 for all i ∈ [T]. Here the expectations are taken over σ% given θ% and
ωi.
Similar to Theorem 4.7, in the following theorem, we show that the sequence {θi}i∈[T+1] generated
by Algorithm 3 converges to a stationary point θ with Vθ J (πθb) = 0 at a sublinear rate.
Theorem G.2 (Convergence to Stationary Point). Let η = 1∕√T, Ti = 1, ητD = min{(1 -
Y)/8,1∕√TτD}, and TTD = Ω(m) in Algorithm 3. Under the assumptions of Proposition 4.3 and
Assumptions 4.5, 4.6, and G.1, it holds for T ≥ 4L2 and B =。忌∙ T1/2) that
min E[kVθJ(∏θJk2] ≤ 8∕√T ∙ E[J(∏θ,+J- J(∏θj + epG,
i∈[T]
where
epG =O(TT/2 + r3∕2 ∙ m-1∕4 ∙ t + R5∕4 ∙ m-1∕8 ∙ t).
Here the expectations are taken over all the randomness.
29
Published as a conference paper at ICLR 2020
Proof. Our proof aligns closely to that of Theorem 4.7 in §D.1. We first lower bound the difference
J (πθi+1) - J (πθi). By Assumption 4.6, we have
J(∏θi+ι) - J(∏θi) ≥ η ∙Vθ J(∏θi)>δi - L/2 ∙ kθi+ι - θik2,	(G.3)
where
_	，一	.,,	〜	,	. .	■ 一r
δi = (θi+ι - θi)∕η = Vθ J(∏θj,	∀i ∈ [T].
Following the proof of Lemma D.2 in §H.5, we obtain that
∣(Vθ J(∏θi) - E[Vθ J(∏θi)])>δi∣ ≤ κ∕η ∙ 2kθi+ι - θik2 ∙ kQπθi - Qωi∣L,	(G.4)
where the expectation is taken over σi given θi and ωi. Recall that ξi = Vθ J (πθi ) - E[Vθ J (πθi )],
where the expectation is taken over σi given θi and ωi . Following from (G.4), we obtain that
VθJ(πθi)>δi = VθJ(πθi) - E[Ve θ J (πθi)]	δi - (ξei)>δi + VeθJ(πθi)>δi
≥ -2κ ∙kθi+ι - θik2∕η ∙kQπθi - Qdki - kξik2∕2 + kδik2∕2,	(G.5)
where the second inequality follows similar analysis to §D.1. Hence, by plugging (G.5) into (G.3),
we have
J(πθi+1) - J(πθi)
≥ (η - L ∙ η2)∕2 ∙kδik2 - η ∙ kξik2∕2 - 2κ/为+1 -切心∙ kQπθi - Qωikςi.	(G.6)
It remains to upper bound kθi+1 - θik2. To this end, we use the fact that
kθi+1 - θi k2 ≤ kθi - Winit k2 + kθi+1 - Winit k2,
and upper bound kθi - Winitk2 and kθi+1 - Winitk2. By the actor update in Algorithm 3, we obtain
for all i > 1 that
i-1	i-1
kθi - Winitk2 ≤ X η ∙ kV θ J (∏θj)k2 ≤ X η ∙ (∣∣E[V θ J (∏θ" 1 + kj∣2),	(G.7)
j=1	j=1
where the expectation is taken over σi given θi and ωi . Meanwhile, it holds that
IIe[v θ J (πθj )] ∣∣2 = IIEσj[φθj (s,a) ∙ Q3j (s, a)]∣∣2 ≤ Eσj[kφθj (s,a)k2 ∙ lQωj (S,a)|], (G.8)
where Φθ, is the centered feature mapping defined in (B.2), and the last inequality follows from the
Jensen's inequality. We now upper bound the right-hand side of (G.8). Note that kφθ, (S, a)k2 ≤ 2
for all (S, a) ∈ S × A. Meanwhile, by (G.2), we obtain that
,cr	，，/	、一，	.一—
∣Qωj (s,a)∣ ≤ Qmax, ∀(s,a) ∈S ×A.	(G.9)
By plugging (G.9) into (G.8), we obtain for all j ∈ [T] that
∣∣E[VeθJ(πθj)]∣∣2 ≤ 2Qmax.	(G.10)
By further plugging (G.10) into (G.7), we obtain for all i > 1 that
i-1
kθi - Winitl∣2 ≤ 2Qmax ∙ η ∙ T + X η ∙ kξj ∣∣2∙	(G.11)
j=1
We now lower bound the right-hand side of (G.6) based on (G.11). Following from the Cauchy-
Schwartz inequality and Assumption G.1, we obtain that
E[kθi- Winitk2 Ti - QU,』
1/2
≤ 2Qmaχ F∙ T ∙ {E[kQπθi - Qωik"}
+ Xη ∙ {E[kξik2]}1/2 ∙ {E[kQπθi - Qωik2i] 01/2
j=1
1/2
≤ (2Qmax ∙ η ∙ T + σξ∙η∙ T ∙ B-1/2) ∙ {E[kQπθi - Qωj" }	,	(G.12)
30
Published as a conference paper at ICLR 2020
where the expectations are taken over all the randomness, and σξe is the absolute constant defined in
Assumptions G.1. By plugging (G.12) into (G.6), we obtain that
(η - L ∙ η2)∕2 ∙ E[kδi k2]
≤ E[J(∏θi+ι) - J(∏θi)] + η ∙ σe∕(2B) + Ro(T) ∙ {E[kQπθi - Qaik[]}"	(G.13)
where we use the fact that kθi+1 - θi k2 ≤ kθi+1 - Winit k2 + kθi - Winit k2 . Here the expectations
are taken over all the randomness, and R0(T) is defined by
Rθ(T) = 4Qmax ∙ η ∙ T + 2θξ ∙ η ∙ T ∙ B-1/2.
By Proposition 4.3 and Assumption 4.2, We obtain for η = 1/√T, B = Ω(σ* ∙ T1/2), and TTD =
Ω(m) that
Ro(T) = O(√T),	E[kQπθi - Qωik2i] ≤ E[kQπθi - Qωik2i]
=O(R3 ∙ m-1∕2 + R5∕2 ∙ m-1∕4),	(G.14)
where the inequality holds since ∣Qπ% (s, a)| ≤ QmaX for all (s, a) ∈ S ×A. By plugging (G.14)
into (G.13) with η = 1/√T and B = Ω(σ] ∙ T1/2), we obtain that
(1 - L∕√T)/2 ∙ E[kδik2] ≤√T ∙ E[J(∏θi+ι)- J(∏θi)] + epG,	(G.15)
where
ePG =O(TT/2 +	r3/2	∙	m-1∕4	∙ T +	R5/4	∙	m-1/8	∙	T).	(G.16)
It remains to upper bound ∣∣δi -Vθ J(∏θi) k 2, where δi = VJ(∏θi). Following from similar analysis
to §H.6, we obtain that
E[∣Vθ J(∏θi) - Vθ J(∏θi)k2] ≤ 2E[kξi∣2] +8κ2 ∙ E[∣Qπθi - Qωik",
where the expectations are taken over all the randomness. Therefore, following from Proposition
4.3 and Assumption G.1, it holds for η = 1∕√T, B = Ω(σ] ∙ T1/2), and TTD = Ω(m) that
E[∣VθJ(∏θi) - VθJ(∏θi)k2]=OL/ + R3 ∙ m-1/2 + R5/2 ∙ m-1/4).	(G.17)
Thus, combining (G.15) and (G.17), we obtain for all i ∈ [T] that
E[∣VθJ(∏θi)k2] ≤ 2E[∣δik2] +2E[∣VθJ(∏%)-VθJ(π%)k2]〜
≤ 4(1 - L∕√T) ∙ E[∣δik2] + 2E[∣Vθ JIn%) - Vθ J(π%)k2]
≤ 8√T ∙ E[J(∏θi+ι)- J(∏θi)] + epG,	(G.18)
where we use the fact that T ≥ 4L2 and we define ePG in (G.16). Finally, by telescoping (G.18),
we obtain that
1 L
min E[∣Vθ J(∏θi)k2] ≤ T ∙ ∑E[∣Vθ J(∏θi)k2] ≤ 8E[J(∏θτ+1) - J(∏θι)]/近 + epG,
where
epG =O(TT/2 + R3/2 ∙ m-1/4 . T + R5/4 ∙ m-1/8 ∙ T).
Here the expectations are taken over all the randomness. Thus, we complete the proof of Theorem
G.2.	□
Following from Theorem G.2, it holds for m = Ω(R10 ∙ T12) that
min
i∈[T]
E[kVθ J (∏θi)k2] = O(1∕√T).
Therefore, θ% converges to a stationary point at a 1∕ √T-rate if the width m of the two-layer neural
network and the batch size B are sufficiently large. We highlight that compared with neural policy
gradient with projection in the actor update, Algorithm 3 needs a larger width m to achieve the
1 ∕ √T-rate of convergence. Such a stronger requirement on m is the extra price to pay for using the
projection-free actor update.
31
Published as a conference paper at ICLR 2020
G.2 Global Optimality of Projection-Free Neural Policy Gradient
In this section, we characterize the global optimality of projection-free neural policy gradient. We
define a sequence of parameter spaces {Bi }i∈[T] as follows,
Bi = {α ∈ Rmd : kα -切必 ≤ R°}, ∀i ∈ [T],	(G.19)
where Ro ≥ 1 is an absolute constant. The sequence {Bi}i∈τ Characterizes the global optimality
of the parameter sequence {θi}i∈[T] . Specifically, similar to (4.5), we have
VθJ(∏θi)>(θ - θi) ≤ kθ - θik2 ∙ kVθJ(∏θi)k2 ≤ Ro ∙ kVθJ(∏θi)k2, ∀θ ∈ Bi, ∀i ∈ [T],
where the first inequality follows from the Cauchy-Schwartz inequality. Following similar analysis
to §D.2, we obtain for all i ∈ [T] that
(1-γ) ∙ (J(π*)- J(∏θi))
≤ 2Qmax ∙ inf kuθi (∙, ∙) - φθi (∙, ∙)>θkσi + Rθ ∙∣V J (∏θj∣∣2∙	(G.20)
θ∈Bi
We now introduce the parameter space BT that includes the sequence {θi}i∈[τ] and the parameter
space Bi as its subspace for all i ∈ [T] as follows,
BT = {α ∈ Rmd : ka - Winitk 2 ≤ R(T)+ Ro},	(G.21)
where
T
R(T) = 2Qmaχ ∙ η ∙ T + η ∙ X ∣∣ξi∣∣2∙
i=1
(G.22)
Here ξi is defined in Assumption G.1. Following from (G.7) and (G.10) in the proof of Theorem
G.2 in §G.1, we have θi ∈ BT for all i ∈ [T]. By Corollary E.4, φθi(∙, ∙)>θ is well approximated
by f((∙,∙); θ) for θ,θi ∈ BT when the width m is sufficiently large. Thus, following from (G.20),
for a sufficiently large m, the suboptimality of θi is characterized by kVθJ(πθi)k2, which is further
quantified by Theorem G.2, and the approximation error infθ∈Bi ||劭稔(∙, ∙) 一 f ((∙, ∙);。)|上,which
quantifies the representation power of the overparameterized two-layer neural networks. In the
following theorem, we present a sufficient condition for the output of projection-free neural policy
gradient to be globally optimal.
Theorem G.3 (Global Optimality of Projection-Free Neural Policy Gradient). Let η = 1∕√T,
Ti = 1, ητD = min{(1 — γ)∕8, l∕√TτD}, and TTD = Ω(m) in Algorithm 3. We define
ueθi (s, a) = uθi (s, a) + φθi (s, a) (Winit - θi),	∀(s, a) ∈ S × A.
Here uθi is defined in (4.4) of Theorem 4.8 with θ = θi, and φθi is the feature mapping defined in
(3.3) with θ = θi. Under the assumptions of Theorem G.2, ifit holds that
Uθi ∈Fro,∞,	∀i ∈ [T],
then for T ≥ 4L2, B = Ω(T1/2), and m = Ω(R10 ∙ T12), we have
(1 - Y) ∙ min E[J(π*) - J(π/)] = O(Ro ∙ TT/4).
Here the expectation is taken over all the randomness.
Proof. To prove Theorem G.3, it suffices to upper bound the expectation of the right-hand side of
(G.20) over all the randomness. We first upper bound the following term,
EhMIlu% ('I "/("T θkσi i，
where the expectation is taken over all the randomness. Note that
uθi (s, a) - φθi (s, a)>θ = ueθi (s, a) + φθi (s, a)>θi - φθi (s, a)>Winit - φθi (s, a)>θ
= ueθi (s, a) - φo(s, a)>(θ - θi + Winit)	(G.23)
-(φθi (s, a) - φ0(s, a)) (θ - θi + Winit),
32
Published as a conference paper at ICLR 2020
which holds for all (s, a) ∈ S × A and θ ∈ Bi with Bi defined in (G.19). Therefore, by the triangle
inequality, we obtain from (G.23) that
inf	kuθi (∙, ∙) - φθi (∙,	∙)>θkσi	≤ inf {∣∣eθi (∙, ∙)	-	φ0(∙, ∙)>(θ	-	θi +	Winit)kσi	(G.24)
θ∈Bi	θ∈Bi
+ Il (φθi (∙, ∙) - φ0(∙, ∙))τ (θ - θi + Winit)Ilσj.
We now upper bound the right-hand side of (G.24). In what follows, we define θi by
φ0(∙, ∙)>θi = π-F-	eθi(∙, ∙),
R0 ,m
1	T-T	•	,1	•	,.	ɪ, ,1	r	1	...	。U
where ∏-F- is the projection operator onto FRo rm. It then follows from the definition of FRo m
in Definition E.1 that θi ∈ Bi = {α ∈ Rmd : ∣∣α - Winitk2 ≤ Ro} for all i ∈ [T]. Meanwhile, by
the definition of Bi in (G.19), we have
θei + θi - Winit ∈ Bi ,	∀i ∈ [T].	(G.25)
Combining (G.24) and (G.25), we have
inf kuθi (∙, ∙) - φθi (∙, ∙)>θkσi ≤ keθi (∙, ∙) - φ0(∙, ∙)>θikσi + ∣l (φθi (∙, ∙) - φθd, B)Tθillσi .
θ∈Bi
(G.26)
Now, it suffices to upper bound the right-hand side of (G.26). Following from the proof of Proposi-
tion 4.3 in §F.1, We obtain for e^i ∈ FRo ∞ that
E[ke%(∙, ∙) - Φo(∙, ∙)>eikσ[ = E[∣∣e%(∙, ∙) - ∏FRo,m通(∙, ∙)Lii= O(Ro ∙ m-1/2), (G.27)
where the expectations are taken over all the randomness. Meanwhile, note that
..  .. ___________ _____ _ .  、
kθi - Winitk2 ≤ r0 ≤ R0 + R(T),
where R(T) is defined in (G.22). Therefore, We obaιn that θi,θi ∈ Bt. By Assumption G.1, We
obtain for η = 1/√T and B = Ω(T1/2) that
ER(T)2 = O(T),	ER(T)3 = O(T 3/2),	(G.28)
where the expectations are taken over all the randomness given Winit . Thus, following from (G.28),
Assumption 4.2, and Lemma E.2, we obtain for all θi, θ% ∈ BT that
E[kφ0(∙, ∙)>θi - φθi (∙, ∙)>θikσi]
≤ {E[kΦo(∙, ∙)>e - φθi(∙, ∙)>⅛]}1" = O(T3/4 ∙ m-14).	(G.29)
By plugging (G.27) and (G.29) into (G.26), we have
E[θinjuθi(∙,∙)- Φθi(∙, ∙)>θkσii
≤ E[keθi (∙, ∙) - φ0(∙, ∙)>θikσi ] + E[kφ0(∙, ∙)> Oi- φθi (∙, ∙)>θikσi ]
=O(R0 ∙ m-1/2 + T3/4 ∙ m-1/4),	(G.30)
which holds for all i ∈ [T].
Meanwhile, by Theorem G.2, we obtain for B = Ω(T1/2) and m = Ω(Ri0 ∙ T12) that
min E[kVθ J(π%)∣∣2] = O(T-1/4).	(G.31)
i∈[T]
Thus, by plugging (G.30) and (G.31) with m = Ω(Ri0 ∙ T12) into (G.20), we complete the proof of
Theorem G.3.	□
By Theorem G.3, it holds for sufficiently large width m and batch size B that the expected total
reward J(∏θi) converges to the global optimum J(∏*) at a 1/T 1/4-rate.
33
Published as a conference paper at ICLR 2020
H Proof of Auxiliary Results
In this section, we lay out the proof of the auxiliary results.
H.1 Proof of Proposition 3.1
Proof. The proof is based on the policy gradient theorem (Sutton and Barto, 2018) in (2.5) and the
definition of the Fisher information matrix in (2.7). It suffices to calculate Vθ log ∏θ(∙ ∣∙). By the
definition of ∏θ(∙ | ∙) in (3.2), it holds for all (s,a) ∈ S ×A that
Vθ log ∏θ(a | S) = T ∙Vθf((s,a); θ) — T ∙
Pa0∈A Vθf((s,a0); θ) ∙ exp[τ ∙ f((s,a0); θ)]
Pa0∈A exP[τ ∙ f((s,a0); θ)]
T ∙ vθf ((s, a)； θ) ― T ∙ E∏θ [vθf ((s, a)； θ),
(H.1)
where we write E∏>[Vθf ((s,a0); θ)] = E。，〜∏θ(∙∣ s)[Vθf ((s,a0); θ)] for notational simplicity.
Meanwhile, recall that Vθf ((∙, ∙); θ) = φθ(∙, ∙), where φθ is the feature mapping defined in (3.3).
Thus, (H.1) implies that
Vθ log ∏θ (a | S) = τ ∙ φθ (s, a) — τ ∙ E∏θ [φθ (s, a0)].	(H.2)
Finally, by plugging (H.2) into (2.5) and (2.7), we have
VθJ(∏θ) = T皿、∣Qπθ(s,a) ∙ (φθ(s,a) — E∏θ[Φθ(s,a0)])],
F (θ) = T 2[(φθ (S,a) - E∏θ [φθ (S,a')])(θθ (s,a) - E∏θ [φθ (S,a')]) i,
which concludes the proof of Proposition 3.1.	□
H.2 Proof of Theorem 4.9
Proof. By Theorem 4.8, we have
(1 —	γ)	∙	(J (∏*) — J (∏b))	≤	2Qmax ∙ inf	kub(∙,	∙) —	Φθ(∙,	∙)>θkσ∏ b,	(H.3)
θ∈B	θ
where uθb is defined in (4.4). It suffices to upper bound the right-hand side of (H.3) under the
expectation over the random initialization. Following from the triangle inequality, we obtain that
感 kub(∙, ∙) — Φb(∙, ∙)>θkσ∏b
θ∈B	θ
≤ θnB n llub(∙, ∙) — πeR,m ub(∙, ∙) Ln b + UnFR,m "^'	— M '	b }
=∣lub(∙, ∙) — nFeR,m ub(∙, ∙)∣∣σ∏ b + θnB l∣nFeR,m ub(∙, ∙) — φb(∙, ∙)>θL. b,	(H.4
where FR,m is defined in Definition E.1. It remains to upper bound the right-hand side of (H.4). In
what follows, we define θ by
>
φ0 (∙, ∙) θ = πFR,m ub(∙, ∙) ∈ FR,m,
where φ0 is the feature mapping defined in (3.3) with θ = Winit . By the definition of FR,m in
Definition E.1, it holds that θ ∈ B = {α ∈ Rmd : kα — Winitk2 ≤ R}. Thus, by (H.4) and the fact
that θ ∈ B , we have
θnB k"b(∙, •) — Φb(∙, ∙)>θkσ∏b ≤ k"b(∙, ∙) — Φθ(∙, ∙)>ekσ∏b + kΦθ(∙, ∙)>e — Φb(∙, ∙)>ekσ∏b .
(H.5)
Following from the proof of Proposition 4.3 in §F.1, it holds for uθb ∈ FR,∞ that
Einit [kub(∙, ∙) — φb(∙, ∙) θkσ∏b]
θ
≤ {Einit [k"b(∙, •) — Φb(∙, ∙)>ekσ∏b] 01/2 =O(R ∙ m-1∕2),	(H.6)
34
Published as a conference paper at ICLR 2020
where the first inequality follows from the Jensen’s inequality, and the expectations are taken over
the random initialization. Meanwhile, following from Lemma E.2, we obtain for all θ, θ ∈ B that
Einit [kφ0(∙, ∙) O- φb(∙, ∙) θ∣∣σ∏b]
θ
≤ {Einit [kΦo(∙, ∙)>e- Φb(∙, ∙)>e∣σ∏Jo1/2 = O(R3/2 ∙ m-1/4),	(H.7)
where the expectations are taken over the random initialization. Finally, by plugging (H.6) and (H.7)
into (H.5), we obtain that
(1 - Y) ∙ Einit [ J(∏*) - J(∏b)] ≤ 2Qmax ∙ Einith照 |以,•) — Φθ(, ∙)>θ∣σ∏ J = OR3/ ∙ m—1/4),
θ∈B	θ
where the first inequality follows from (H.3). Similarly, if the assumption that uθb ∈ FR,∞ is not
imposed, we conclude that
(1 - Y) ∙ Einit [ J(∏*) - J(∏b)] ≤ O(R3/ ∙ m-1/4) + Einit [k∏FR,∞Ub-Ubkσ∏b],
which completes the proof of Theorem 4.9.	□
H.3 Proof of Inequality (4.5)
Proof. Recall that we define ρi by
Pi = η-1 ∙ (∏b(θi + η ∙ V0J(∏θj) - θi),	(H.8)
where ΠB is the projection operator onto B. Following from (H.8) and the fact that (ΠBy -
y)>(ΠBy - x) ≤ 0 for all x ∈ B, we have
(η ∙ Pi- η ∙ Vθ J(∏θj)>(η ∙ Pi + θi - θ) ≤ 0, ∀θ ∈ B.	(h.9)
Thus, following from (H.9), we obtain that
vθJ(πθi)>(θ - θi) ≤ Pir(O -Oi)- η ∙ kPik/ + η ∙ PlvθJ-
≤kPik/ ∙ (kθ -θik/ + η∙kvθJ(∏θi)k/), ∀θ ∈b,	(H.10)
where the last inequality follows from the CaUchy-SchWartz inequality and the fact that -η ∙∣∣Pi k / ≤
0. It remains to upper bound the right-hand side of (H.10). For all O, Oi ∈ B = {α ∈ Rmd :
kα - Winitk/ ≤ R}, we have kO - Oik/ ≤ 2R. Meanwhile, recall that we set τi = 1. Therefore,
following from Proposition 3.1, we obtain that
∣vθ J (∏θi)k/ ≤ Eσi [∣Qπθi (s,a)∣∙kφθi (s,a)k/] ≤ 2Qmax,	(H.11)
where the first inequality follows from the Jensen's inequality, and the second inequality follows
from the facts that ∣Qπ%(s, a)| ≤ QmaX and ∣∣φθi (s, a)∣/ ≤ 2 for all (s, a) ∈ S ×A. By plugging
(H.11) and the upper bound kO - Oik/ ≤ 2R into (H.10), we conclude that
vθJ(∏θi)>(θ - θi) ≤ (2R + 2η ∙ QmaX) ∙ |同|/, ∀θ ∈ B,
which concludes the proof of (4.5).	□
H.4 Proof of Corollary A.5
Proof. It suffices to calculate «(T) defined in (A.3) in Theorem A.4. Note that we set Ti = (i -
1)∕√T. Therefore, we have Ti = O(√T) for all i ∈ [T]. Thus, it holds for m = Ω(R10 ∙ T6) that
O((Ti+1 ∙ T1/2 + 1) ∙ R3// ∙ m-1/4)=O(TT//), Vi ∈ [t],
O(R5∕4 ∙ m-1/8) = Ο(TT/).	(H.12)
Meanwhile, it holds for B = Ω(R/ ∙ T/ ∙ σ/) that
R1// ∙(σ∣∕B)1∕4 = Ο(TT/).	(H.13)
35
Published as a conference paper at ICLR 2020
Therefore, combining (H.12) and (H.13), we obtain that
q(T) = √8c0 ∙ R1/2 ∙ (σξ∕B)1/4 + O((1 + τi+1 ∙ T1/2) ∙ R3/2 ∙ m-1/4 + R5/4 ∙ m-1/8)
O(T -1/2).
By Theorem A.4, we have
min
i∈[T]
EJ(π*)- J(∏θi)]
log |A| + 9R2 + M
-(1 - γ) ∙√T
+ O((1 - Y)-1 ∙ Tτ∕2)
log |A| ʌ
(1 - Y) ∙√T)
which concludes the proof of Corollary A.5.
□
H.5 Proof of Lemma D.2
Proof. In the sequel, We write gi = E[Vθ J(∏θj] for notational simplicity, where Vθ J(∏θJ is
defined in (3.7), and the expectation is taken over σi given θi and ωi. Recall that we set τi = 1. By
Proposition 3.1, we obtain that
I(VθJ (πθi ) - gi)τδi∖ = ∣Eσi hφθi Ga) ∙ (QπθiGa)- Qωi(S,/)] δi∣
≤ kδik2 ∙EσJkΦθi (s,a)k2 ∙∖Qπθi (s,a) — Qωi (s,a)∖],	(H.14)
where φθi (s, a) is the centered feature mapping defined in (B.2) with θ = θ%, and the inequality
follows from the Jensen’s inequality. Note that θi , θi+1 ∈ B. It holds that
kδik2 = ∣∣θi+ι- θik2∕η ≤ 2R∕η.
Meanwhile, note that k≠θ. (s, a) k 2 ≤ 2 for all (s, a) ∈ S×A. Therefore, it follows from Assumption
4.5 and (H.14) that
∖(Vθ J(∏θi) - gi)τδi∖ ≤ 4R∕η ∙ Eσi[∖Qπθi (s,a) — Qωi(s,a)∖]
≤ 4R∕η ∙ {EςJ(dσi∕dςi)2] } 1/2 ∙ kQπθi - Qω, L
≤ 4κ ∙ R∕η ∙ kQπθi - Qωi∣L,
where the second inequality follows from the Cauchy-Schwartz inequality, dσi ∕dςi is the Radon-
Nikodym derivative, and κ is defined in Assumption 4.5. Thus, we complete the proof of Lemma
D.2.	□
H.6 Proof of Lemma D.3
Proof. In what follows, we write gi = E[VJ (πθi)] for notational simplicity, where the expectation
is taken over σi given θi and ωi . Note that
EkVθJ(πθi) -VbθJ(πθi)k22] ≤2Ekξik22] +2EkVθJ(πθi)-gik22],	(H.15)
where we use the fact that kx + yk22 ≤ 2kxk22 + 2kyk22, and the expectations are taken over all the
randomness. By Proposition 3.1, we have
kvθJ (πθi ) - gik2 = IIEσi hφθi (s,a) ∙ (Qπθi(s,a) - Qωi (s,a))] ∣l2
≤ EσJkΦθi (s, a) k 2 ∙ ∖Qπθi (s, a) - Qω, (s,a)∖],	(H.16)
where φθi is defined in (B.2) with θ = θ% and the second inequality follows from the Jensen's
inequality. Since ∣∣φθi(s, a)k2 ≤ 2 for all (s, a) ∈ S ×A,we obtain from (H.16) that
kvθJ (πθi) - gi∣2 ≤ {Eσi[kφθi(s, a)k2 ∙ ∖Qπθi (s,a) - Qωi(s,a)∖]}
≤ 4κ2 ∙kQπθi- Qωik2i,	(H.17)
where κ is defined in Assumption 4.5 and the inequality follows from the Cauchy-Schwartz inequal-
ity. By plugging (H.17) into (H.15), we obtain that
E[kVθ J (∏θi) -V θJ (∏θi)∣2] ≤ 2E[∣ξik2] +8κ2 ∙ E[∣Qπθi - Qaik!」，
which concludes the proof of Lemma D.3.	□
36
Published as a conference paper at ICLR 2020
H.7 Proof of Lemma D.4
Proof. By the definition of the KL divergence, it holds for all S ∈ S that
DKL (π*(∙ I s)∣∣πi(∙ I S)) — dkl(π*(∙ I S)IIni+i(∙ I S))
=Qog(∏i+1(∙ I s)∕∏i(∙ I s)),π*(∙ I s)〉.	(H.18)
Meanwhile, the right-hand side of (H.18) can be expanded as follows,
0°g(πi+ι(∙ i S)In (∙ i s)),π*(∙ i s)〉
=<log(∏i+ι(∙ I s)∕∏i(∙ I s)),π*(∙ I s) - Πi+1(∙ I s)〉+ (log(∏i+ι(∙ ∣ s)∕∏i(∙ ∣ s)),∏i+ι(∙ ∣ s)〉
=<log(πi+1(∙ 1 s)∕πi(∙ 1 s)),π*(∙ 1 s) - πi+1(∙ 1 s)〉+DKL(πi+1(∙ i s) I ∣ πi(∙ i s)). (H.19)
X--------------------〜--------------------'
Li
Combining (H.18) and (H.19), We obtain that
Li = Dkl(∏*(∙ I s) i H(∙ ∣ s)) - Dkl(∏*(∙ I s) i l∏i+ι(∙ ∣ s)) - DκL(∏i+ι(∙ ∣ s)1 H(∙ ∣ s)). (H.20)
In what follows, we calculate the difference
Eν* [Li] -(I- Y) ∙ η ∙ (J(n*) - J(ni)).
By Lemma D.1, we have
J(∏*) - J(∏i) = (1 - Y)-1 ∙ Eν* [hQπi(s, ∙),π*(s, ∙) - ∏i(s, ∙)>].	(H.21)
Meanwhile, for Li defined in (H.19), we obtain that
Li- η∙hQπi(s, ∙),π*(s,∙)- ∏i(s,∙)i
=<log(πi+ι(∙ IS)∕πi(∙ I S)),π*(∙ Is) -πi+ι(∙ Is)〉- η ∙ hQπi(s, ∙),π*(s, ∙) -πi(s, ∙)i
=(log(πi+ι(∙	i S)∕πi(∙l	s))	- η ∙	Qωi(s, ∙),π*(∙l s)	-	πi(∙	i	s)〉	(H.22)
+ η∙hQωi(S, ∙) - Qπi(s, ∙),π*(∙∣ s) - ∏i(∙∣ s)i
+ (log(∏i+ι(∙ I s)∕∏i(∙ I s)),∏i(∙ I s) - ∏i+1(∙ I s)〉.
Note that upon taking the expectation over S ~ ν* (∙) in (H.22), the right-hand side of (H.22) is equal
to Hi defined in (D.21) of Lemma D.4. Thus, combining (H.21) and (H.22), we obtain that
Eν* [Li] - (1 - Y) ∙ η ∙ (J(π*) - J(πi)) = H	(H.23)
where Hi is defined in (D.21). By plugging (H.20) into (H.23), we conclude that
(1 - Y) ∙ η ∙ (J(π*) - J(∏i)) = Eν* [Dkl(∏*(∙ I s) ∣∣∏i(∙ ∣ s)) - Dkl(∏*(∙ I s) ∣ ∣ πi+ι(∙ ∣ s))
-DκL(∏i+ι(∙ I s) l l ∏i(∙ I s)) - Hi,
which concludes the proof of Lemma D.4.	口
H.8 Proof of Lemma D.5
Proof. By (D.21), we have
EijHi口 ≤ E	Eν* [°og(πi+ι(∙l S)∕πi(∙l	s))	- η ∙Qωi(s, ∙),π*(∙l s)	- πi(∙l s)〉	(H.24)
+ η ∙ EIEV* [KQ-1s, ∙) - Qni(s, ∙),π*(∙l S)-πi(∙l s)i|]]
+ E Eν* [∣(log(∏i(∙ I s)∕∏i+ι(∙ I s)),∏i+ι(∙ I s) - πi(∙ ∣ s)〉[],
where the inequality follows from the Jensen,s inequality, and the expectations are taken over all
the randomness. To prove Lemma D.5, we establish the upper bounds of the three terms on the
right-hand side of (H.24) respectively in the following lemmas.
37
Published as a conference paper at ICLR 2020
Lemma H.1. It holds that
E[Eν*[∣hQωi(s, ∙) - Qni (s, ∙),∏*(∙ | S)- ∏i(∙ | s))|]] ≤ (φi + ψi) ∙ E[kQωi — Q/J
where φ0i, ψi0 are the concentrability coefficients defined in (A.1) of Assumption A.2. Here the
expectations are taken over all the randomness.
Proof. See §I.1 for a detailed proof.
□
Lemma H.2. Under Assumptions 4.2 and A.3, it holds that
E Eν* [∣q°g(∏i+ι(∙ I s)∕∏i(∙ I s)),∏i(∙ I S)- ∏i+ι(∙ | s))∣]
≤ E Eν*[DκL(∏i+ι(∙∣ s)∣∣∏i(∙∣ s))i + η2 ∙(9R2 + M2)+ Ogi ∙ R3/2 ∙ m-1/4),
where M is the absolute constant defined in Assumption A.3. Here the expectations are taken over
all the randomness.
Proof. See §I.2 for a detailed proof.	□
Lemma H.3. Under Assumption 4.2, it holds that
E Eν* h∣Oog(πi+ι(∙ | S)In(∙ | S)) - η ∙ Qωi(s, ∙),π*(∙ | S)- πi(∙ | s)>∣]
1/2
≤ √2% + ψi) ∙ η ∙ R1/ ∙ τ-i ∙ {E[kξi(δi)k2] + E[kξi(ωi)k2] }
+ O((Ti+1 + η) ∙ R3/2 ∙ m-1/4 + η ∙ R5/4 ∙ m-1∕8),
where 夕i and ψi are the concentrability coefficients defined in (A.1) of Assumption A.2 and ξi(δi),
ξi(ωi) are defined in Assumption A.1. Here the expectations are taken over all the randomness.
Proof. See §I.3 for a detailed proof.	□
Finally, applying Lemmas H.1, H.2, and H.3 to (H.24), it holds under Assumptions 4.2 and A.3 that
E |Hi| - Eν+ hDKL (πi+1(∙ Ι S) ∣∣πi(∙ | S))i ≤ η2 ∙ (6R2 + M2) + η ∙ (d + ψi) ∙ εQ,i + εi,
where
εQ,i=E[kQπi-Qωikςi]
εi = {E[∣∣ξi(δi)k2 + kξi(ωi)k2]}1/2 + O((Ti+1 + η) ∙ R3/2 ∙ m-1/4 + η ∙ R5/4 ∙ m-1/8).
Here the expectations are taken over all the randomness. Therefore, we complete the proof of
LemmaD.5.	□
I Proof of Supporting Lemmas
In this section, we provide the proof of the lemmas in §H.
38
Published as a conference paper at ICLR 2020
I.1	Proof OF Lemma H.1
Proof. We define ∆Q,i(s, a) = Qωi (s, a) - Qπi (s, a) forall (s, a) ∈ S×A. It holds for all i ∈ [T]
that
E"*[∣h∆Qi(s,∙),π*(∙∣ s)-∏i(∙∣ s))|]
/ ∆Q ∆Q,i(s,a) ∙ (π*(a ∣ S)- ∏i(a ∣ S)) dν*(s).
JS a∈A
Meanwhile, it holds for any S ∈ S that
E δqAs, a) ∙ (π*(a ∣ s) - πi(a ∣ S))
a∈A
=I I	∆Q,i(s, a)	∙	(π*(a ∣	s)	-	∏i(a	∣	S))Kila	∣	s)dπi(a ∣ s)
I a a∈A
≤ /	I ∆Q,i(s, a) ∙ (π*(a ∣ s) - ∏i(a ∣ s))∕πi(a ∣ s)∣dπi(a ∣ s),
JaAA
(I.1)
(I.2)
where the inequality follows from the Jensen,s inequality. By plugging (I.2) into (I.1), We have
E〃* [∣<∆Q,i(s,∙),π*(∙∣ s)-∏i(∙∣ s)>∣]
≤ /	1 ∆Q,i(s,a) ∙ (π*(a ∣ s) - ∏i(a ∣ S))∕∏i(a ∣ S)Ide(S,a),	ι
JSaA
(I.3)
where We define e(∙, ∙) = ∏ (∙∣∙) ∙ ν*(∙). Recall that ςi(∙, ∙) = ∏i(∙∣∙) ∙ %i(∙) and σ*(∙, ∙) = ∏*(∙∣∙) ∙
ν*(∙). Therefore, following from (I.3), it holds that
Eν* [KAnG ∙),π*(∙ ∣ s) - πi(∙ ∣ s)i∣]
Zdν* .、一 ，.
/△◎,退《)** + Js / nGNG⑪k -(S) dςi(s,a).
(I.4)
Finally, applying the Cauchy-Schwartz inequality to (I.4) yields that
E[Eν* [∣h∆Q,i(s, ∙),π*(∙∣ s)-∏i(∙∣ S)川
≤ ({E<⅛[(dσ*∕dςi)2]} / 十{E%J(dν*∕d%i)2]} / ) ∙ E {嗅/4乂与叫2]} /
=Wi十ψi) ∙E {Eς』∣△Q,i(s,ɑ)∣2]} /	=E+明)∙E[BqML],
where dσ*∕dςi and dν* ∕d%i are the Radon-Nikodym derivatives,夕i and ψi are the concentrability
coefficients defined in (A.1) of Assumption A.2, and the expectations are taken over allthe random-
ness. Thus, we complete the proof of Lemma H.1.	□
I.2	Proof of Lemma H.2
Proof. Following from the definition of ∏θ in (3.2), we obtain that
(log(∏i+ι(∙ ∣ s)∕∏i(∙ ∣ s)),∏i(∙ ∣ s) - ∏i+ι(∙ ∣ s))
=Si+1 ∙ f ((s, ∙); θi+ι) - Ti ∙ f ((s, ∙); θi),∏i(∙ ∣ s) - ∏i+ι(∙ ∣ s))	(I.5)
-Ci(S), πi(∙ ∣ s) - πi+1(∙ ∣ s)),
where f ((∙, ∙); θ) is the two-layer neural network defined in (3.1) and Ci(S) is defined by
Note that both ∏i(∙∣ s) and ∏i+ι(∙ ∣ s) are distributions over A, which implies that
(Ci(s), ∏i(∙ ∣ S) - Πi+1(∙ ∣ S)) = Ci(s) - Ci(S) =0,	∀s ∈ S.	(I.6)
39
Published as a conference paper at ICLR 2020
Meanwhile, recall that we define the feature mapping φθ(s, a) in (3.3). For the two-layer neural
network f ((∙, ∙); θ), We have
f((s,a); θ) = φθ(s,a)>θ,	∀(s,a) ∈ S × A.	(I.7)
In what follows, we write φi(s, a) = φθi (s, a) and ∆i(a | s) = πi(a | s) - πi+1(a | s) for notational
simplicity. By plugging (I.6) and (I.7) into (I.5), we obtain for all s ∈ S that
KIog(πi+M∙ Ι s)/ni(。 | S)), Ai(∙ | S))| = lhτi+ι ∙ φi+ι(s, ∙)>θi+ι - τi ∙ φi(s, ∙)>θi, δM∙ | S)il
≤ lhφi(s, ∙)>(τi+1 ∙ θi+1 - τi ∙ θi), δN | s)i|
+ τi+1 ∙ lhφi+1(s, ∙)>θi+1 - φi(s, ∙)>θi+1, δ∙ | s)i|
≤ ∣∣φi (s, ∙)>(τi+1 ∙ θi+1 - τi ∙ θi)k∞,A Tl δ,I s)k1,A	(I.8)
X--------------------------{----------------------}
(i)
+ τi+1 ∙ lhφi+1(s, ∙)>θi+1 - φi(s, ∙)>θi+1, δ∙ | S))|,
'------------------------{--------------------}
(ii)
where the last inequality follows from the Holder,s inequality. Here We denote by ∣∣ ∙ ∣∞,a and
k ∙ ∣∣i,a the '∞- and '1-norms defined on R|A|, respectively. In what follows, we upper bound (i)
and (ii) on the right-hand side of (I.8) respectively.
Upper Bounding (i) in (I.8). Recall that we define
1
δi = η ∙(Ti+1 ∙ θi+ι - Ti ∙ θi) = argmin ∣∣F(θi) ∙ α — τi ∙ VJ(∏θj∣∣2.
α∈B
Thus, it holds that δi ∈ B and ∣δi - Winit ∣2 ≤ R, where Winit is the initial parameter. In what
follows, we denote by φ0 the feature mapping defined in (3.3) with θ = Winit. Then for all (S, a) ∈
S × A, we have
∣φi(s,a)>(τi+ι ∙ θi+ι — Ti ∙ θi)∣ = η ∙ ∣φi(s,a)>δi∣
≤ η ∙ (∣φo(s, a)>Winitl + ∣φi(s, a)>δi — φi(s, a)>θ∕ + ∣φi(s,a)>θi — φo(s, a)>Winit|)
≤ η ∙ (M0 + kφi(s, a)k2 ∙ kδi — θik2 + ∣φi(s, a)>θi — φ0(s, a)>Winit|),	(I∙9)
where the first inequality follows from the triangle inequality, the second inequality follows from
the Cauchy-Schwartz inequality, and M0 is defined by
Mo =	sup	l Φ0 (s,a)> Winit |.	(I.10)
(s,a)∈S×A
In what follows, we upper bound the right-hand side of (I.9). Note that Ti-1 + η = Ti. Therefore,
we obtain that
∣∣θi - WinitI12 ≤ Ti-IE，k°i-1 — Winitl12 + η∕τi ∙ ∣∣δi-i - Winitk2,	(I/D
which holds for all i > 1. Recursively, since θ1 = Winit ∈ B and δi ∈ B for all i ∈ [T], it then
follows from (I.11) that θi ∈ B for all i ∈ [T]. Thus, it holds that ∣δi — θi∣2 ≤ 2R. Meanwhile,
following from (3.3), it holds for all θ ∈ Rmd and (S, a) ∈ S × A that ∣φθ(S, a)∣2 ≤ 1. Therefore,
we obtain that
∣∣φi(s, a)∣∣2 ∙ ∣∣δi — θi∣∣2 ≤ 2R,	∀(s, a) ∈S×A.	(I.12)
It remains to upper bound ∣φi(s, a)>θi — φo(s, a)>Winit∣ for all (s, a) ∈ S × A, which is equal
to ∣f ((s, a); θi) — f((s, a); Winit)∣ by (I.7). Recall that f((∙, ∙); θ) is differentiable with respect to
θ ∈ Rmd almost everywhere, and the gradient Vθf = ([Vθf]1>, . . . , [Vθf]>m)> is given by
[vθ f]r (s,a) = √= ∙ 1{(s, a)> [θ]r > 0} ∙ (S,a) = [φθ ]r(s, a) ,	∀(s,a) ∈ S × A,
40
Published as a conference paper at ICLR 2020
where φθ (s, a) is defined in (3.3). Since kφθ (s, a)k2 ≤ 1 for all θ ∈ Rmd and (s, a) ∈ S × A, we
obtain for all (s, a) ∈ S × A that
∣φi(s, a)>θi - φo(s, a)>Winit∣ = f ((s, a)；。，一 f ((s, a)； Winit) ∣
≤ sup ∣∣Vθ f((s,a); θ)∣∣2 ∙∣∣θi- Winitk 2
θ∈Rmd
=SUp kφθ (s,a)k2 ∙ kθi - Winitk2 ≤ R, (I.13)
θ∈Rmd
where the last inequality holds since θi ∈ B.
By plugging (I.12) and (I.13) into (I.9), we have
∣Ti+ι ∙ Φi(s,a)>θi+1 - Ti ∙ φi(s,a)>θi∣ ≤ η ∙ (Mo + 3R),	∀(s,a) ∈S×A,
where M0 is defined in (I.10). Therefore, it holds for all s ∈ S that
IlTi+1 ∙ φi(s, ∙)>θi+1 - τi ∙ φi (s, ∙)>θi k∞,A = SUP |Ti+1 ' φi(S, a/ θi+1 - τi ' φi(s, a)>θi |
a∈A
≤ η∙ (Mo + 3R).	(I.14)
Finally, by the Pinsker’s inequality, it follows from (I.14) that
kφi (S,。> (Ti+1 ∙ θi+1 - τi ∙ θi)k∞,A ∙ Ι s)k1,A - DKL (πi+1(∙ Ι S)IIni 卜 | S))
≤ η ∙	(M0	+ 3R)	∙	kπi+1(∙ | S)- πi(T S) k1,A	-	1/2	∙	kπi+1(∙ | S)- πi(∙∣ S)II2,A∙	(L15)
By completing the squares, we further upper bound the right-hand side of (I.15) by
kφi(S, ∙)>(Ti+1 ∙	θi+1 -	τi	∙	θi )k∞,A ∙	Ι	s)I∣1,A -	DKL (πi+1(∙	| S)||ni«	|	S))
=-1/2 ∙ (k∏i+ι(∙ | s) - πi(∙ | s)∣∣i,a - η ∙ (Mo + 3R))2 + 1/2 ∙ η2 ∙ (Mo + 3R)2
≤ 1/2 ∙ η2 ∙ (Mo + 3R)2 ≤ η2 ∙ (M02 + 9R2),	(I.16)
which holds for all S ∈ S. Here the last inequality follows from the fact that (x + y)2 ≤ 2x2 + 2y2 .
Upper Bounding (ii) in (I.8). It holds for all S ∈ S that
∣hφi+1 (S, ∙)>θi+1 - φi(S, ∙)>θi+1, δJ∙ | s))|
≤ lhφi+1 (s, ∙)>θi+1 - Oi(S, ∙)>θi+1,πi(∙ | S)i| + Ih0i+1(S, ∙)>θi+1 - φi (s, ∙)> θi+1 ,πi+1 (∙ | S)i|
≤ kφi+1(S, ∙)>θi+1 - φi(S, ∙)>θi+1k∏i,1 + kφi+1(S, ∙)>θi+1 - φi(S, ∙)>θi+1 k∏i+ι, 1 ∙ (L17)
Here for any distribution n ∈ P (A), We denote by ∣∣ ∙ ∣∣∏,p the Lp(n)-norm, which is defined by
∣∣v∣∏,p = [Pa∈A n(a) ∙ |v(a) |p]1/p. Following from Assumption 4.2 and Lemma E.2, it holds that
E[Eν* [∣∣φi+1(S, ∙)>θi+1 - φo(s, ∙)>θi+1∣∣∏i,1]]
≤ E[kφi+1(∙, ∙)>θi+1 - φo(∙, ∙)>θi+1k∏i∙ν*] = O(R3/2 ∙ mT/4),
EhEν* [∣∣φi(S, ∙)>θi+1 - φo(S, ∙)>θi+1 ∣∣∏i,1]i
≤ E[kφi(∙, ∙)>θi+1 - φo(∙, ∙)>θi+1∣∏i∙V*] = O(R3/2 ∙ m-1/4),	(I.18)
where the inequalities follow from the Cauchy-Schwartz inequality, and the expectations are taken
over all the randomness. Meanwhile, it holds that
kφi+1(S, ∙) θi+1 - φi(S, ∙) θi+1k∏i,1
≤ kφi+1(s, ∙)>θi+1 - φo(s, ∙)>θi+1k∏i,1 + kφi(s, ∙)>θi+1 - φo(s, ∙)>θi+1 k∏i,1.	(L19)
Combining (I.18) and (I.19), we obtain that
e[Ev* [kφi+1(s, ∙)>θi+1 - φi(s, ∙)>θi+1k∏i,1]i = O(R3/2 ∙ m-1/4).	(I.20)
Similarly, it holds that
e[Ev* [kφi+1(s, ∙)>θi+1 - φi(s, ∙)>θi+1k∏i+1,1]i = O(R3/2 ∙ m-1/4),	(I.21)
41
Published as a conference paper at ICLR 2020
where the expectation is taken over all the randomness. By plugging (I.20) and (I.21) into (I.17), we
obtain that
τi+ι ∙ E[Eν*[∣hΦi+ι (s, ∙)>θi+ι - φi(s, ∙)>θi+ι, ∆i(∙∣ s))|]] = O(τi+ι ∙ R3/2 ∙ m-1/4). (I.22)
Finally, by plugging (I.16) and (I.22) into (I.8), it holds under Assumptions 4.2 and A.3 that
E Eν* [∣<log(∏i+ι(∙ | s)∕∏i(∙ | s)),∏i(∙ | S)- ∏i+ι(∙ | s))∣i
≤ E Eν* h°KL(∏i+ι(∙ | S)Il∏i(∙ |	s))i	+	η2	. (9R2	+ M2)	+ O(τi+1 ∙	R3/2	∙	m-1/4),
where M is the absolute constant defined in Assumption A.3. Thus, we complete the proof of
Lemma H.2.	□
I.3 Proof of Lemma H.3
Proof. Note that Eπθ [φθi (S, a0)] and Eπθ [φωi (S, a0)] depend solely on S ∈ S, where we write
E∏θjφθi (s, a0)] = Ea0〜∏θ. (∙∣ s)[φθi (s, a0)] for notational simplicity. Thus, We have
(E∏θi [φθi(s, a)>δi - φωi(s, a0)>ωi],π*(∙ | S)- ∏i(∙ | s)) =0,	VS ∈ S.	(I.23)
MeanWhile, folloWing from the parameterization of πθ in (3.2) and (I.6) in §I.2, We obtain that
(log(πi+ι(∙ | s"πi(∙ | S)) - η ∙ Q3i(s, ∙), π*(∙ | S)- πi(∙ | s))
=hTi+ι ∙ φθi+ι (s, ∙)>θi+ι - τi ∙ φθi(S ∙)>θi - η ∙ φωi(s, ∙)>ωi,π*(∙ | S)- πi(∙ | S)i. (L24)
In what follows, We define ∆*(∙ | ∙) = ∏*(∙∣∙) - ∏(∙∣∙) for notational simplicity. Then, combining
(I.23) and (I.24), We obtain for all S ∈ S that
<bg(πi+ι(∙ | S)/ni( | S)) - η ∙ Q3i(s, ∙), Ni (∙ | s))
=η ∙ hφθi (s, ∙)>δi - φω (s, ∙)>ωi, △一 | S)i
+ τi+1 ∙ hφθi+ι (s, ∙)>θi+1 - φθi(S, ∙)>θi+1, △ (∙ | S)i
=η ∙ hφθi(s, ∙)>δi -φωi(s, ∙)>ωi,δ"(T S)i	(L25)
X---------------------{z------------------}
(iii)
+ τi+1 ∙ hφθi+ι (s, ∙)>θi+1 - φθi(S, ∙)>θi+1, △ (∙ | s)i,
S--------------------------{z---------------------}
(iv)
where φθi and φωi are the centered feature mappings defined in (B.2) that correspond to θi and ωi,
respectively, and δi is defined by
1
δi = η ∙	(τi+ι	∙	θi+ι -	τi	∙	θi)	= argmin ∣∣F(θi)ω -	τi	∙ VJ(∏θj∣∣2.	(I.26)
ω∈B
In what follows, we upper bound the expectations of (iii) and (iv) over all the randomness separately.
Upper Bounding (iii) in (I.25). It holds that
Eν* [∣hΦθi(S, ∙)>δi - φωi(S,∙)>ωi,π*(∙∣ S)i∣]
≤	∖φθi (s, a)>δi - φωi (s, a)>"i|db*(S, a)
S×A
= /	&i(S,a)>6i — φωi (S,a)>^i| ∙ df1 (s, a)dσi (S,a
S×A	dσi
≤ 夕i，kφθi(∙, B>δi - φωi (., ∙)>ωikσi ,
(I.27)
42
Published as a conference paper at ICLR 2020
where dσ*∕dσg is the Radon-Nikodym derivative,夕i is defined in (A.1) of Assumption A.2, and the
last inequality follows from the Cauchy-Schwartz inequality. Similarly, it holds that
Eν* [Kφθi (S, a)>δi - φωi (S, a)>ωi, πi(a | S))|]
≤ [ ∕lφθi (s,a)>δi - φωi (s,a)>ωildπi(a | S) ∙ ν* (S)
=/	lφθi (S,α0>δi - φωi(S,a)>^i| ∙壮(S)dσi(s,00
S ×A	dνi
≤ ψi ∙ kφθi (∙, ∙)>δi - φωi (∙, ∙)>ωi kσi ,	(I.28)
where dν"d% is the Radon-Nikodym derivative, ψi is defined in (A.1) of Assumption A.2, and
the last inequality follows from the Cauchy-Schwartz inequality. Combining (I.27) and (I.28), we
obtain that
Eν* [∣hΦθi (S, ∙)>δi- φωi (S, ∙)>ωi, ∆ (∙∣ S)i∣]
≤ (ψi	+	ψi )	∙	kφθi (∙,	∙)>δi	-	φωi (∙,	∙)>ωikσi .	(L29)
It now suffices to upper bound ∣∣≠θ. (∙, ∙)>δi - φωi(∙, ∙)>ωikσi. With a slight abuse of notation, We
write φθi = φθi (∙, ∙) and φω⅛ = Φω⅛ (∙, ∙) hereafter for notational simplicity. Note that
kδ>φθi - ω>φωikσi = /Ebi [(δ>φθi - ω>φωJ ∙ (δ>φθi - ω>φωJ]
≤ ,1(Si - Si? [φ% ∙ (δ>φ% - ω>φωj] I	(I.30)
'----------------------V---------------------}
(iii.a)
+ qEσi [(s>φθi - ω>φωj ∙ (S>φθi - ω>φωj] ∙
、---------------------------{----------------------}
(iii.b)
We now upper bound the expectations of the right-hand side of (I.30) over all the randomness.
Upper Bounding (iii.a) in (I.30). Note that ωi, δi ∈ B, where δi is defined in (I.26) and B = {α ∈
Rmd : ∣α - Winit ∣2 ≤ R}. Therefore, we obtain that
∣ωi - δi∣2 ≤ 2R.	(I.31)
Meanwhile, following from Proposition 3.1 and (3.7), it holds that
Eσi [F(Oi)] = F(Oi)= τi2 ∙ Eσi [φθi (φθi)>],
Eσi [Vθ J(∏θi)] = τi ∙ Eσi [φ% ∙ (Φωi)>ωi],	(I.32)
where the expectations are taken over σi given Oi and ωi . In what follows, we write gi =
Eσi [VJ (πθi )] for notational simplicity, where the expectation is taken over σi given Oi and ωi.
By plugging (I.31) and (I.32) into (iii.a) in (I.30), we obtain that
|(Si - ωi)>Eσa [(φθi ∙ (δJφθi - ω[φωj)] J = T-，| (Si- ωi)>(F(θi) ∙ δi - τi ∙ gi) |
≤ 2R ∙ τ-2 ∙∣∣F(θi) ∙ δi - Ti ∙ gi∣2,	(I.33)
where the last inequality follows from the Cauchy-Schwartz inequality and (I.31). By (I.33), we
have
Eh∣(δi - ωi)>Eσi [Φθi(δ>φθi- ω>φωi)]∣1/2] ≤ Ci ∙ Eh(∣∣F (θi) ∙ δi - T ∙ gi∖^/]
≤ Ci	∙	E[(kF(θi)	∙ δi	- Ti ∙ Vθ J(∏θi)∣2 + kξi(δi)∣∣2)1/2]
≤ Ci	∙	{E[kF(θi)	∙ δi	- Ti ∙ Vθ J(∏θi)k2] + E[kξi(δi)k2]	01/2,	(I.34)
43
Published as a conference paper at ICLR 2020
where the expectations are taken over all the randomness. Here the last inequality follows from the
Jensen's inequality, Ci = √2R ∙ τ-1, and ξi(δi) is defined by
ξi(δi) = F(θi)	∙ δi	- Ti	∙ Vθ J(∏θi)	- (F(θi)	∙	δi	- Ti	∙	gi).	(I.35)
In What follows, We upper bound ∣∣FR) ∙ δ% — τi ∙ Vθ J(∏θJ∣∣2 on the right-hand Side of (I.34).
Recall that we define δi by
1
δi = η ∙	(τi+ι	∙	θi+ι — Ti	∙	θi)	= argmin ∣∣F(θi)	∙	ωi	—	τi	∙	Re J(∏θj∣∣2.	(I.36)
ω∈B
Therefore, since ωi ∈ B, We obtain from (I.36) that
∣F(θi) ∙ δi - Ti ∙ Ve J(∏θi)∣∣2 ≤ ∣∣F(θi) ∙ ωi — Ti ∙ Ve J(∏θi)∣∣2
≤	IIF(Oi)	∙	ωi	-	Ti	∙	gik2 +	∣∣ξi(ωi)k2,	(L37)
Where recall that, similar to (I.35), We define ξi(ωi) by
ξi(ωi) = F(θi)	∙	ωi	-	Ti	∙	VeJ(∏θi)	-	(F(θi)	∙ ωi	- Ti	∙ g,.	(I.38)
By plugging (I.37) into (I.34), We obtain that
E[∣(δi - ωi)>Eσi [φθi ∙ (δ>φθi - ω> φωi)] 11/2]
≤ Ci	∙	{e[∣F(θi)	∙	3i-	Ti ∙ gik2]	+ E[∣ξi(δi)∣2]	+ E[kξi(ωi)k2]}1"	(I.39)
where Ci = 2RR ∙ TiT and ξi(δi), ξi(ωi) are defined in (I.35) and (I.38), respectively. To upper
bound the right-hand side of (I.39), it now suffices to upper bound the expectation E[∣F(θi) ∙ 3i -
Ti ∙ gik2]. By (I.32), we obtain that
kF(Oi) ∙ 3i - ti ∙ gi∣∣2 = Ti ∙ ∣∣Eσi [φθi ∙ (φθi - φωi)>3i]l∣2
≤ T ∙ Eσi [kφθi ∙ (φθi - φωi )>3iki] = τi2 ∙ Eσi [kφθi ∣∣2 ∙ |(以-九。>3/] ,	(L4O)
where the inequality follows from the Jensen,s inequality. In what follows, we upper bound the
right-hand side of (I.40). Note that ∣φθi(s, a)∣2 ≤ 2 for all (s, a) ∈ S × A. By further plugging
into (I.40), we obtain that
IlF(Oi) ∙ 3i - τi	∙ gik2 ≤	2τi2	∙ Eσi	[∣(φθi	-九。>3/]	≤	2τ2	' k(φθi	-	φωJ>3ikσi,	(1∙41)
where the last inequality follows from the Jensen’s inequality. Recall that 3i , Oi ∈ B. Therefore, by
Assumption 4.2 and Corollary E.3, we have
E[k(φθi - φωi )>3ikσi ]
≤ E[k(φθi - Φo)>3ikσi] + E[k(φo - Φωi)>3ikσi] = OR3/ ∙ m-1/4),	(I.42)
where the expectations are taken over all the randomness. Combining (I.41) and (I.42), we obtain
that
E[∣F(θi) ∙ 3i - Ti ∙ gik2] = O(2τi2 ∙ R3/2 ∙ m-1∕4),	(I.43)
where the expectation is taken over all the randomness. Finally, by plugging (I.43) into (I.39), we
conclude that
Eh∣(δi - 3i)>Eσi [φθi ∙ (δ> φθi - 3> φωi )] I"?]
≤ Ci ∙ {E[kF(Oi) ∙ 3i - τi ∙ gi∣∣2] + E[kξi(3i)k2] + E[kξi(δi)k2] }
=O(R5/4 ∙ m-1/8) + √2R ∙ T-1 ∙ {E[∣∣ξi(δi)∣2 + ∣ξi(3i)k2]}l"	(I.44)
where Ci = √2R ∙ TiT and ξi(δi), ξi(3i) are defined in Assumption A.1.
44
Published as a conference paper at ICLR 2020
Upper Bounding (iii.b) in (I.30). Following from the Cauchy-Schwartz inequality, it holds that
qEσi [(ω>φθi - ω>φωi) ∙ (δ>φθi - ω>φωi)]
≤ (kω>φθi- ω>φωikσi ∙kδ>φθi- ω>φωikσJ / .	(I.45)
To upper bound the right-hand side of (I.45), We first upper bound ∣∣ω>φθi - ω>Φω⅛ kσi. Recall that
ωi, θi ∈ B. Following from Assumption 4.2 and Corollary E.3, it holds that
E[kω>Φθi - ω>φo∣H] = O(R3 ∙ m—1/2),
E[kω>φωi - ω>Φo∣H] = O(R3 ∙ m—1/2),	(I.46)
where φ° is defined in (B.1) and the expectations are taken over all the randomness. Therefore,
folloWing from (I.46), We obtain that
E[kω>φθi - ω> φωikσi
≤ 2E[∣ω>φθi- ω>φo∣M] +2E[kω>φ,i- ω>φ°[上]=O(R3 ∙ m-1/2).	(I.47)
It remains to upper bound ∣∣δ>φθi - ω>φω⅛ 1展 on the right-hand side of (I.45). Since δ> ∈ B, by
Assumption 4.2 and Corollary E.3, we obtain that
E[kδ>Φθi - δ>φo∣∣σj = O(R3 ∙ m-1/2),	(I.48)
where the expectation is taken over all the randomness. Meanwhile, following from the fact that
kΦo(s, a)∣2 ≤ 2 for all (s, a) ∈ S ×A, we obtain that
lδ>φ0(s, a) - ω>φ0(s, a)|
≤ ∣∣φ0(s, a)∣∣2 ∙ kδ> - ω>k2 ≤ 4R,	∀(s, a) ∈ S × A,	(1.49)
where the first inequality follows from the Cauchy-Schwartz inequality and the second inequality
follows from the fact that δ> , ω> ∈ B. Combining (I.46), (I.48), and (I.49), we obtain that
E[kδ>Φθi - ω>φωi∣σi] ≤ 3E[∣δ>Φθi - δ>φ0kσ1 +3E[∣δ>φo- 9>方。||「」
+ 3E[∣ω>φωi- ω>φo∣∣σj =O(R + R ∙ m-1/2),	(I.50)
where the expectations are taken over all the randomness. Finally, plugging (I.47) and (I.50) into
(I.45), we obtain that
1/2]
E {Eσa [(ω> φθi - ω> φωj(δ> φθi - ω> φωi)] }
1	i_	i_	i_	i_	ι 1/2
≤ {E[kω> φθi - ω> φωi kσi ∙ kδ> φθi - ω> φωikσj}
≤ nE[kω> φθi- ω> φωi kσi ] ∙ E[kδ> φθi- ω> φωi kσi ] O
=O(R3∕2 ∙ m-1/4 + R5/4 ∙ m-1/8),	(I.51)
where the inequalities follow from the Cauchy-Schwartz inequality and the expectations are taken
over all the randomness.
Finally, by plugging (I.44), (I.51), and (I.30) into (I.29), we obtain that
E∣Eν* [∣hΦθi(s, ∙)>δ> - φωi(s, ∙)>ω>, △- | s)i∣]
=η ∙ (φ> + ψ>) ∙(O(R5/4 ∙ m-1/8 + R3/2 ∙ m-1/4)	(I.52)
+ √2R ∙ τ-1∙ {E[kξ>(δ>)k2 + kξ>(ω>)k2]}1/2),
where ξ>(δ>) and ξ>(ω>) are defined in Assumption A.1. Here the expectations are taken over all the
randomness.
45
Published as a conference paper at ICLR 2020
Upper Bounding (iv) in (I.25). The analysis of (iv) is similar to that of (ii) in §H.8. It holds that
Ihφθi+1 (s, ∙)>θi+ι - φθi(s, ∙)>θi+ι, ∆*(∙∣ s)i∣
≤ Ihφθi+1 (s, ∙)>θi+ι - φθi(s, ∙)>θi+ι,∏*(∙ | s)i∣ + Ihφθi+1 (s, ∙)>Θi+1 - φθi(s, ∙)>θi+ι, ∏i(∙ | s)i∣
≤ kφθi+ι (s, ∙)>θi+1 - φθi (S, ∙)>θi+1kπ*,1 + kφθi+ι (S, ∙)>θi+1 - φθi (S, ^>θi+1 ∣∣∏i,1∙ (L53)
Note that θi , θi+1 ∈ B. Following from Assumption 4.2 and Lemma E.2, it holds that
EhEν* [I∣φθi+1 (s, ∙)>θi+1 - φ0(S, ∙)>θi+1kπ*,l]i
≤ E[kφθi+1 (∙, ∙)>θi+1 - Φo(∙, ∙)>θi+ιkσ*] = O(R3/2 ∙ m-1/4),
e[Ev* [kφθi(s, ∙)>θi+1 - φo(s, ∙)>θi+1k∏*,1]i
≤ E[kφθi(∙, ∙)>θi+ι - Φo(∙, ∙)>θi+ιkσ*] = O(R3/2 ∙ m-1∕4),	(I.54)
where the inequalities follow from the Jensen’s inequality, φ0 is the feature mapping defined in (3.3)
with θ = Winit , and the expectations are taken over all the randomness. Following from (I.54), we
obtain that
EhEν* [kφθi+ι (S, ∙)>θi+1 - φθi (s, ∙)>θi+1kπ*,l]i
≤ EhEν* [kφθi+ι (S, ∙)>θi+1 - φ0(S, ∙)>θi+1kπ*,l]i
+ EhEν* [kφθi (s, ∙)>θi+1 - φ0(s, ∙)>θi+1kπ*,l]i
=O(R3/2 ∙ m-1/4),	(I.55)
where the expectations are taken over all the randomness. Similarly, it holds that
e[Ev* [kφθi+ι (s, ∙)>θi+ι - φθi(s, ∙)>θi+1k∏i,1] = O(R3/2 ∙ m-1∕4).	(I.56)
By plugging (I.55) and (I.56) into (I.53), we obtain that
e[e“* [Ni(s, ∙)>θi+ι - φθi(s, ∙)>θi+ι, △- | s)i∣]i = O(R3/2 ∙ m-1/4).	(I.57)
Finally, by plugging (I.52) and (I.57) into (I.25), we obtain that
E Eν* [Klθg(πi+l(∙ | S)In (∙ | S)) - η ∙ Qωi (s, ∙),π*(∙ | S)- πi(∙ | s)〉Ii
1/2
≤ √2% + ψi) ∙ η ∙ R1/2 ∙ τ-1 ∙ {E[kξi(δi)k2] + E[kξi(ωi)k2]}
+ O((Ti+1 + 1) ∙ R3/2 ∙ m-1/4 + η ∙ R5/4 ∙ m-1/8),
where 夕i, ψi are defined in Assumption A.2 and ξi(δi), ξi(ω. are defined in Assumption A.1. Thus,
We complete the proof of Lemma H.3.	□
J	Auxilliary Lemma
Lemma J.1 (Performance Difference (Kakade and Langford, 2002)). It holds for any π and πe that
J(e) - J(∏) = (1 - Y) - 1 ∙ Ee∙ν∏ [Aπ (s,a)]∙
Here νπe is the state visitation measure corresponding to πe, which is defined in (2.3).
Proof. See Kakade and Langford (2002) for a detailed proof.
□
46