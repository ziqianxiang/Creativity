Published as a conference paper at ICLR 2020
Neural tangent kernels, transportation map-
pings, and universal approximation
Ziwei Ji, Matus Telgarsky, Ruicheng Xian
Department of Computer Science
University of Illinois at Urbana-Champaign
{ziweiji2,mjt,rxian2}@illinois.edu
Ab stract
This paper establishes rates of universal approximation for the shallow neural
tangent kernel (NTK): network weights are only allowed microscopic changes from
random initialization, which entails that activations are mostly unchanged, and
the network is nearly equivalent to its linearization. Concretely, the paper has two
main contributions: a generic scheme to approximate functions with the NTK by
sampling from transport mappings between the initial weights and their desired
values, and the construction of transport mappings via Fourier transforms. Regard-
ing the first contribution, the proof scheme provides another perspective on how the
NTK regime arises from rescaling: redundancy in the weights due to resampling
allows individual weights to be scaled down. Regarding the second contribution,
the most notable transport mapping asserts that roughly 1∕δ10d nodes are sufficient
to approximate continuous functions, where δ depends on the continuity properties
of the target function. By contrast, nearly the same proof yields a bound of 1∕δ2d
for shallow ReLU networks; this gap suggests a tantalizing direction for future
work, separating shallow ReLU networks and their linearization.
1 Main res ult and overview
Consider functions computed by a single ReLU layer, meaning
m
x 7→	sj σ
j=1
(1.1)
where σ(z) := max{0, z}. While shallow networks are celebrated as being universal approximators
(Cybenko, 1989; Funahashi, 1989; Hornik et al., 1989) — they approximate continuous functions
arbitrarily well over compact sets — what is more shocking is that gradient descent can learn the
parameters to these networks, and they generalize (Zhang et al., 2016).
Working towards an understanding of gradient descent on shallow (and deep!) networks, researchers
began investigating the neural tangent kernel (NTK) (Jacot et al., 2018; Du et al., 2018; Allen-Zhu
et al., 2018), which replaces a network with its linearization at initialization, meaning
x 7→
where X = (x, 1) ∈ Rd+1, W = (w, b) ∈ Rd+1; (1.2)
here each Wj = (wj,bj) is frozen at Gaussian initialization (henceforth the bias is collapsed in for
convenience), and each transported weight τj is microscopically close to the corresponding initial
weight Wj, concretely ∣∣τj - Wjk = O(1∕∈√m), where e > 0 is a parameter and the scaling e∕√m is
conventional in this literature (Allen-Zhu et al., 2018).
As eq. (1.2) is merely affine in the parameters, it is not outlandish that gradient descent can be
analyzed. What is outlandish is firstly that gradient descent on eq. (1.1) with small learning rate will
track the behavior of eq. (1.2), and secondly the weights hardly change as a function ofm, specifically
kτj - Wj l∣2 =O(I∕e√m).
1
Published as a conference paper at ICLR 2020
Contributions. This work provides rates of function approximation for the NTK as defined in
eq. (1.2), moreover in the “NTK setting”: the transported weights must be near initialization, meaning
k7j - WjIl = O(1∕e√m). In more detail:
Continuous functions (cf. Theorem 1.5). The main theorem packages the primary tools here to
say: the NTK can approximate continuous functions arbitrarily well, so long as the width is
at least 1∕δ10d, where δ depends on continuity properties of the target function; moreover,
the transports satisfy ∣∣τj- - Wjk = O(1∕∈√m), and the ReLU network (eq. (1.1)) and its
linearization (eq. (1.2)) stay close. Re-using many parts of the proof, a nearly-optimal rate
1∕δ2d is exhibited for ReLU networks in Theorem E.1; this gap between ReLU networks and
their NTK poses a tantalizing gap for future work.
Approximation via sampling of transport mappings. The first component of the proof of Theo-
rem 1.5, detailed in Section 2, is a procedure which starts with an infinite width network,
and describes how sampling introduces redundancy in the weights, and automatically leads
to the desired microscopic transports ∣∣τj- - Wjk = O(1∕∈√m). As detailed in Theorem 2.1,
the error between the infinite width and sampled networks is O(e + 1∕√m). In this way, the
analysis provides another perspective on the scaling behavior and small weight changes of
the NTK.
Construction of transport mappings. The second component of the proof of Theorem 1.5, detailed
in Section 3, is to construct explicit transport mappings for various types of functions. As
detailed in Lemma 3.3, approximating continuous functions proceeds by constructing an
infinite width network not directly for the target function f, but instead its convolution
f * Ga with a Gaussian Ga with tiny variance α2. Care is needed in order to obtain a rate
of the form 1∕δO(d), rather than, say, 1∕δO(d∕δ). The main constructions are based on Fourier
transforms.
Rounding out the organization of this paper: this introduction will state the main summarizing result
and its intuition, and then close with related work; Section 4 will describe certain odds and ends
for approximating continuous functions which were left out from the main tools in Section 2 and
Section 3; Section 5 sketches abstract approaches to constructing transport mappings, including ones
based on a corresponding Hilbert space; Section 6 will conclude with open problems and related
discussion. Proofs are sketched in the paper body, but details are deferred to the appendices.
1.1	Basic notation, intuition, and main result
The NTK views networks as finite width realizations of intrinsically infinite width objects. In order
to transport an infinite number of parameters away from their initialization, one option is to use a
transport mapping T : Rd+1 → Rd+1 to show where weights should go:
X → Ew〈T(W), Φ(x; W))= Ew〈T(W),X)σ0(hW,X>) = Ew(T(W),X)1 [{W, X>≥ 0],
where Φ(x; W) = Xσ0({W, Xi) is a random feature representation of X (Rahimi & Recht, 2008). This
abstracts the individual transported weights (τj)jm=1 from before into transported weights defined
over arbitrary weights W ∈ Rd+1. These (augmented) weights W = (w, b) (with weight W ∈ Rd and
bias b ∈ R) will always be distributed according to a standard Gaussian with identity covariance,
with G denoting the density and probability law simultaneously.
A key message of this work, developed in Section 2, is (a) the infinite width network can be sampled
to give rates of approximation by finite width networks, (b) the microscopic adjustments of the NTK
setting arise naturally from the sampling process! Indeed, letting s ∈ {-1, +1} denote a uniformly
distributed random sign,
E〈T(W), Φ(x; W)) = E DsT(W) + sιWe√m, Φ(x; W)E
W	w,s ∖	/
= E s2 = 1, E S = 0
m
≈ —X SjsTT(Wj) + SjWje√m, Φ(x; Wj))	sampling (Wj, sj)	)
m j=1
=√m X( s√wj+wj ,sjφ(x; Wj)).
2
Published as a conference paper at ICLR 2020
As highlighted by the bolded terms: increasing the width m corresponds to resampling, and allows
the transported weights to be scaled down! Indeed, the distance moved is O(1∕∈√m) by construction.
To this end, for convenience define
Tj := Te(Wj, Sj) := S[√m ) + wj1[kwj Il ≤ R],
φj (x) := Φe(x; Wj, Sj ) :
√m φ(x Wj)
(1.4)
where R is a truncation radius included for purely technical reasons. The transport mappings
constructed in Section 3 satisfy B := SuPw ∣∣T(W)∣ < ∞, and thus maxj- ∣Tj 一 Wjk ≤ B∕e√m by
construction as promised (with high probability).
The key message of Section 2 is to control the deviations of this process, culminating in Theorem 2.1
and also Theorem 1.5 below, which yields upper bounds on the width necessary to approximate
infinite width networks. The notion of approximation here will follow (Barron, 1993) and use the
L2 (P) metric, where P is a probability measure on the ball{x ∈ Rd : IxI ≤ 1}:
IhIL2(P) =	h(x)2 dP (x).
Additionally IhIL2 = R h(x)2 dx and IhIL1 = R |h(x)| dx will respectively denote the usual L2
and L1 metrics over functions on Rd .
Theorem 1.5 (Simplification of Theorems 2.1 and 4.3). Let continuous function f : Rd → R be given,
along with δ ∈ (0,1] so that |f (x)-f (x0)∣ ≤ e whenever ∣∣x-x0∣2 ≤ δ andmax{∣x∣, ∣x0∣} ≤ 1+δ,
LetP be any probability distribution over IxI ≤ 1. Then there exists a transport mapping T (defining
T and Tj as in eq, (1.4)) and associated scalars
M5d(5d+9)/2
B =SuP IIT(w)∣2 = O 4 5(d+i)	, where M := SuP |f(x)|,
W	∖ e δ L))	kxk≤1+δ
so that with probability at least 1 — 3η over Gaussian weights (Wj)m=ι and uniform signs (Sj)m=ι,
then maxj ∣∣τj- - Wjk ≤ B∕e√m, and
m
f - X〈Tj,φjS)
j =1	L2(P)
m	m
m X sjσ(a ㈤) 一 Xsj ,φj(•)〉
j=1	j=1	L2(P)
〜
≤ Oe
√= + e√d pln(1∕η)
VZln(I∕η)
In words: given an arbitrary target function f and associated continuity parameter δ, width (B/e)2 =
O(d5d+9∕10δ10(d+1)) suffices for error O(e), parameters are close to initialization, and the NTK and
the original network behave similarly. The randomized construction does not merely give existence,
but holds with high probability: the sampling process is thus in a sense robust, and may be used
algorithmically!
As provided in Theorem 4.5, elements of the proof of Theorem 1.5 can be extracted and converted
into a direct approximation rate of continuous functions by ReLU networks, and the rate becomes
d+2
O(dd+2∕2δ2d+2). Since this rate is nearly tight, together these rates pose an interesting question: is
there a purely approximation-theoretic gap between shallow ReLU networks and their NTK?
1.2	Related work
Optimization literature; the NTK. This work is motivated and inspired by the optimization
literature, which introduced the NTK to study gradient descent in a variety of nearly-parallel works
(Jacot et al., 2018; Du et al., 2018; Du et al., 2018; Allen-Zhu et al., 2018; Arora et al., 2019; Oymak
3
Published as a conference paper at ICLR 2020
& Soltanolkotabi, 2019; Li & Liang, 2018; Cao & Gu, 2019). These works require the network width
to be polynomial in n, the size of the training set; by contrast, the analysis here studies closeness in
function space, and the width instead scales with properties of the target function.
One close relative to the present work is that of (Chizat & Bach, 2019), which provides an abstract
proof scheme following the preceding works, and explains the microscopic change of the weights
as a consequence of the scaling e∕√m. This is consistent with the resampling perspective here, as
summarized in eq. (1.3).
Random features and the mean-field perspective. The random features perspective (Rahimi &
Recht, 2008) studies a related convex problem: similarly to the NTK, the activations σ0(Ej,X)) are
held fixed, and what are trained are scalar weights aj ∈ R on each feature. The Fourier transport map
construction used both for the NTK here in Theorem 1.5 and for shallow networks in Theorem E.1
proceeds by constructing exactly such a reweighting, and thus the present work also establishes
universal approximation properties of random features. A related perspective is presented in the
mean-field literature, which relate gradient descent on (Wj)m=ι to a Wasserstein flow in the space of
distributions on these features (Chizat & Bach; Mei et al., 2018). The analysis here does not have
any explicit ties to the mean-field literature, however it is interesting and suggestive that transport
mappings appear in both.
Approximation literature. The closest prior work is due to Barron (1993), who gave good rates
of approximation for functions f : Rd → R when the associated quantity / ∣∣w∣∣∙ |f(w)| dw is small,
where f denotes the Fourier transform of f. The proofs in Section 3 will use elements from the
proofs in (Barron, 1993), but with many distinct components, and thus it is interesting that the same
quantity / k w∣∣∙ |f (W) | dw arises once again. Like the work of (Barron, 1993), the present work also
chooses to approximate in the L2 (P ) metric. Standard classical works in this literature are general
universal approximation guarantees without rates or attention to the weight magnitudes (Cybenko,
1989; Hornik et al., 1989; Funahashi, 1989; Leshno et al., 1993). The rate given here of roughly
1∕δ2d in Theorem 4.5 does not seem to appear rigorously in prior work, though it is mentioned as a
consequence of a proof in (Mhaskar & Micchelli, 1992), who also take the approach of approximation
via Gaussian convolutions; the use of convolutions is not only standard (Wendland, 2004), it is
moreover classical, having been used in Weierstrass’s original proof (Weierstrass, 1885).
Many related works use a RKHSes directly. Sun et al. (2018) prove universal approximation (with
rates) via an RKHS, however they do not consider the NTK (or the NTK setting of small weight
changes). Bach (2017a) (see also (Bach, 2017b; Basri et al., 2019; Bietti & Mairal, 2019)) studies a
variety of questions related to function fitting with the random features model, including establishing
rates of approximation for Lipschitz functions on the surface of the sphere (with a few further
conditions); the rates are better than those here (roughly Θ(1∕δd/2)), however they do not consider the
NTK setting, meaning either the setting of small changes from initialization nor the linearization.
Another close parallel work studies exact representation power of infinite width networks, developing
representations for functions with Ω(d) derivatives (Ongie et al., 2019); similarly, the constructions
here use an exact representation result for Gaussian convolutions, as developed in Section 3.
Regarding lower bounds from the literature, there are two lower bounds of the form 1∕δd/2 for general
shallow networks, not necessarily in the NTK setting (Yarotsky, 2016; Bach, 2017a). Interestingly,
Yarotsky (2016) also presents a lower bound of 1∕δd for approximations whose parameters vary
continuously with the target function; this seems to hold for the Fourier constructions here in
Section 3, though an argument needs to be made for the sampling step.
2 Sampling from a transport
This section establishes that by sampling from an infinite width NTK, the resulting finite width NTK
is close in L2(P) both to the infinite width idealization, and also to the finite width non-linearized
ReLU network; moreover, the sampling process introduces redundancy in the weights, allowing them
to be scaled down and lie close to initialization.
4
Published as a conference paper at ICLR 2020
Theorem 2.1. Suppose B ≥ max {2, SuPw IlT(W)Il2}, and Set R := ʌ/d +1 + 2,ln(m∕η). With
probability at least 1 一 3η, then maxj ∣τj 一 Wjk ≤ B∕e√m, and
m
XM,φj(∙) - Ew〈T(W), φ(∙;W))
j=1
≤2 (√m+ER) h1 + PIn(I/n)i,	(2.2)
L2(P)
XSj ,φj (.)〉一 X √⅛σ(Sj , ))	≤ 2	+ BR + √m + eR) h1 + Pln(1/n)i .
j	j	L2(P)
(2.3)
As discussed in the introduction, maxj∙ ∣τj 一 Wjk ≤ B∕e√m is essentially by construction. Next,
recall the sampling derivation in eq. (1.3), restated here as a lemma for convenience, the notation
(W, S) collecting all random variables together, meaning W = (Wι,..., Wm) and S = (si,..., sm).
Lemma 2.4. E〈T(W), Φ(x; W)〉=	X<∕(Wj, sj-), Φe(x; Wj∙,sj∙)〉= F X〈Tj,Φj(x)〉.
w	f,S j	f,S j
The proof of eq. (2.2) now follows from the classical Maurey sampling lemma (Pisier, 1980), which
was also used in the related work by Barron (1993). The following version additionally includes a
high probability control, which results from an application of McDiarmid’s inequality. Applying the
following sampling lemma to the present setting, the deviations will scale with B := SuPw ∣∣T(W)∣∣2.
Lemma 2.5 (Maurey). Letfunctions {g(∙; v) : V ∈ V} be given, where V ⊆ Rp is a set ofpossible
parameters. Let ν be a probability measure over V, let (v1, . . . , vm) be an iid random draw from ν,
and define
f(x) := E g(x; v)	and	gj (x) := g(x; vj).
V〜V
Then
2
1m	1	2	1	2
E	f — Xgj	≤ — Ellg(I V)IIL2(P) ≤ — SuPllg(I V)IIL2(P),
((sj,vj))m=1	m	m v	L2(P)	m v∈V	L2(P)
j=1	j=1	L2(P)
and with probability at least 1 一 η,
f 一
m
m X gj
j=1
≤ SuP kg(∙; v)kL2(P)
v∈V
L2(P)
1+√2ln(1∕η)
√m
Concretely, here gj(x) = m〈Tj, φj∙(x)〉，and suPv∈v ∣∣g(∙; W)∣∣L2(p)≤ √2supw ∣∣T(W)∣2 =
O(B + Re√m) by Cauchy-Schwarz. Before continuing, note also that there are other proof schemes
attaining similar bounds (Bach, 2017a;b, Proposition 1), and that similar bounds are possible for the
uniform norm, albeit with more sensitivity to the basis functions g (cf. Lemma B.2).
Turning now to the final bound in eq. (2.3), the first step is to note by positive homogeneity of the
ReLU that σ(〈Tj,X))= (Jj,X)σ0((Jj, x)), thus
m	mm
X ⑴,φj) 一 √m X sjσKτj, x〉)= X (Tj, φj — √mxσ0Kτj, x〉)
j=1	j=1	j=1
which boils down to checking the difference in activations, namely σ0(〈Wj, X))一 σ0(<τj, X)). As
is standard in the NTK literature, since Tj 一 Wj is (with high probability) microscopic compared to
〈Wj, X), the activations should also be close. The following lemma makes this precise.
Lemma 2.6. For any X
∈ Rp, if R ≥ √d +2
(as used in eq. (1.4)), then
E卜((W,X)) 一 σ0(6(W),X〉)| ≤ 2B^2.
w∣	I e√mπ
From here, the eq. (2.3) can be established with another application of Lemma 2.5. This completes
the proof of Theorem 2.1 after an application of Gaussian concentration to ensure maxj-1 Wj ∣∣ ≤ R.
This also establishes the first half of Theorem 1.5.
5
Published as a conference paper at ICLR 2020
3 Concrete transport mappings via Fourier transforms
The previous section showed function approximation in the NTK setting assuming the existence of
an infinite width NTK defined by a transport mapping T; this section will close the gap by providing
a variety of transport maps.
The transport mappings here will be constructed via Fourier transforms, with convention
f(x) = /
exp -2πixTw f(x) dx;
a few general properties are summarized in Appendix A. Interestingly, these transports are all random
feature transports: they have the form T(W) = (0, .…，0,p(W)) where P is a signed density over
random features, and Ew TΓ(W), Φ(χ; W))= EwP(W)σ0(hW,Xi). This perspective of a signed
density will be used to prove universal approximation — again via sampling! — of shallow ReLU
networks (and random features) later in Theorems 4.5 and E.1. (For constructions which are not
based on random features, see Section 5.)
The first steps of the approach here follow a derivation due to Barron (1993). Specifically, the inverse
Fourier transform gives a way to rewrite a function as an infinite with network with complex-valued
activations x 7→ exp(2πixTW):
f(x) =
T
exp(2πixTW)f (W) dW.
A key trick due to Barron (1993) is to force the right hand side to be real (since the left hand side
is real): specifically, letting f (W) = |f (w)| exp(2∏iθf (w)) With ∣θf(w)∣ ≤ 1 denote the radial
decomposition of f,
/
Ref (x) = Re
T
exp(2πixTW)f (W) dW
Re	exp(2πixTW
_	,	. . . O ,	. . _
+ 2∏iθf (w))∣∕(w)∣ dW
J cos (2π(xτW + θf (w))) ∣jT(w)∣ dW.
After this step, the proofs diverge: the approach here is to use the fundamental theorem of calculus to
rewrite cos in terms of σ0 :
cos(z) - cos(0) = -	sin(b) db = -	sin(b)1[z - b ≥ 0] db, = -	sin(b)σ0 (z - b) db;
00	0
plugging this back in gives an explicit representation of f in terms of an infinite width threshold
network! A similar approach can be used to obtain an infinite width ReLU network.
This is summarized in the following lemma, which includes a calculation of the error incurred
by truncating the weights; this truncation is necessary when applying the sampling of Section 2.
Interestingly, this truncation procedure leads to the quantity / kw∣∣∙ |f (W) | dW, which was explicitly
introduced as a key quantity by Barron (1993) via a different route, namely of introducing a factor
kW k to enforce decay on cos.
Lemma 3.1. Let f : Rd → R be given with Fourier transform f and truncation radius r ∈ [0, ∞].
1.	Define infinite width threshold network
Fr (x) :
f (0) + / If(W)I cos (2∏(θf (W)-∣∣W∣∣))dW
+ 2π
J σ0(<W, X>)∣∕(w)∣ sin(2π(θf (w) — b))1[∣b∣ ≤ ∣∣Wk ≤ r] dW.
Forany ∣∣x∣ ≤ 1, F∞ = f and∖f (x) — Fr(x)∣ ≤ 4∏ Rkwk>r I∣w∣∣∙ If(W)I dW.
6
Published as a conference paper at ICLR 2020
2.	Define infinite width ReLU network
Qr(x) :
f(0)+
∣f(w)∣ [cos(2π(θf(w) -IlwII)) — 2π∣∣wk sin(2π(θf(w) — ∣∣w∣∣))] dw
+ xT
J w∣f(w)∣ dw
+ 4π2
σ σ(WτX)∣∕(w)∣ cos(2π(∣w∣ — b))1[∣b∣ ≤ ∣∣w∣ ≤ r] dW.
Forany IlxIl ≤ 1, Q∞ = f and∖f (X)- Qr(x)∣ ≤ 12π2 Rlwk>r ∣∣w∣∣2 ∙ ∣f(w)∣ dw∙
(The second part of the shows that the same technique allows functions to be written with equality as
ReLU networks; this is included as a curiosity and used in a few places in the appendices, but is not
part of the main NTK story.)
The preceding constructions immediately yield transport mappings from Gaussian initialization to the
function f in a brute-force way: by introducing the fraction G(W)/g(w), calling the numerator part of
the integration measure, and the denominator part of the integrand. As stated before, these transport
maps are random feature maps: they zero out the coordinates corresponding to x!
Lemma 3.2. Let f : Rd → R be given with Fourier transform f. For any r ∈ [0, ∞], define
transport mapping Tr (w, b) :=(0,..., 0,pr (W)) with
Tr (W) b)d+1 = Pr (W)= 2
f(0) +
.ʌ , , ..................
If(V)Icos(2π(θf(V)- kvk))dv
+ 2π
ʌ
If(W)I
G(W)
cos(2π(θf (w) - b))1[IbI ≤ IwI ≤ r].
By this choice,for any ∣∣x∣ ≤ 1, f (x) = Ew T∞((W), Φ(x; W)), and
/ If(v)I dv + 2π
SUp kTr (W)∣2 ≤ 2∣f (0)∖ +2
W
∣f (x) — E〈T(W), Φ(x; W))|
≤ 4π
/
kwk>r
.ʌ............
If(W)i ∙ IIw| dW.
ʌ
If(W)I
SUp
kwk≤r G(W)
网≤kwk
The preceding construction may seem general, however it is quite loose, noting the final supremum
term within SUpw ∣∣7r(W)∣∣2; indeed, attempting to plug this construction into Theorem 2.1 does not
yield the 1∕δO(d) rate in Theorem 1.5, but instead a rate 1∕δO(d∕δ), which is disastrously larger!
Interestingly, a fix is possible for special functions of the form f * Gα, namely convolutions with
Gaussians of coordinate-wise variance α2. These are exactly the types of functions used in Section 4
to approximate continuous functions. The fix is simply to apply a change of variable so that, in a
sense, the target function and the initialization distribution have similar units.
Lemma 3.3. Let function f, variance α2 > 0, and r ∈ [0, ∞] be given, and define fα := f * Gα
and φ := (2πɑ)-1, and transport mapping Tr (w, b) :=(0,..., 0,pr.(W)) with
Tr (w, b)d+1 = Pr (W) := 2
fa (0) + / Ifa(V)I cos(2π(θfɑ (V)- IlvlD^v
+ 2π(2πφ2)(d+1"2I∕(φw)Ieb/2 sin(2π(θfα(φw) — b))1[I" ≤ ∣∣wI ≤ r].
Then fa(x) = Ew (T∞(W), Φ(x; W))for ∣∣xI ≤ 1, and for r ∈ [√d, ∞),
SUp ∣∣Tr(W)I ≤ 2 M + (2n02)d/2Mf(1 + ,2π3φ2er2/2^ ,
where M := SUpx If (x)I, and Mf = 1 when fa = Ga and Mf = ∣∣f (φ∙)∣Lι otherwise, and
sup ∣f (x) — E〈Tr(W), Φ(x; W)) ∣ ≤ 4π(2πφ2)(d+1”2Mf (√d + 3) exp (—(r — √d)2∕4
kxk≤1
7
Published as a conference paper at ICLR 2020
4 Approximating continuous functions
The final piece needed to prove Theorem 1.5 is to show that a function f is close to its Gaussian
convolution f * Ga, at least when α > 0 is chosen appropriately. This is a classical topic (Wendland,
2004), and indeed it was used in the original proof of the Weierstrass approximation theorem
(Weierstrass, 1885). The treatment here will include enough detail necessary to yield explicit rates.
The following definition will be used to replace the usual (, δ) conditions associated with continuous
functions with an exact quantity.
Definition 4.1. Let f : Rd → R be given, and define modulus of continuity ωf as
ωf (δ) := sup {f (x) — f (x0) : max{∣∣xk, ∣∣x0k} ≤ 1 + δ, ∣∣x — x0∣∣ ≤ δ} .	♦
If f is continuous, then ωf (defined here over a compact set) is not only finite for all inputs, but
moreover limδ→0 ωf (δ) → 0. It is also possible to use this definition with discontinuous functions;
note additionally that the convolution bounds in Section 3 only required an L1 bound on the pre-
convolution function f, and therefore the tools throughout may be applied to discontinuous functions,
albeit with some care to their Fourier transforms!
Lemma 4.2. Let f : Rd → R and δ > 0 be given, and define
M := SUp If(X)l,	f∣δ(X) = f (χ)1[kxk ≤ 1 + δ],	α := √-------τ=≡≡≡=≡÷.
∣∣xk≤ι+δ	√d + √2ln(2M∕ωf (δ))
Let Gα denote a Gaussian with the preceding variance α2. Then
sup If — f∣δ * Gal ≤ 2ωf(δ).
∣x∣≤1
The proof splits the integrand into two parts: points close to X, and points far from it. Points close to
X must behave like f(X) due to continuity, whereas points far from X are rare and do not matter due
to the Gaussian convolution. The full details are in the appendix.
Lemma 4.2 can be combined with the transport for f * Gα from Section 3 to give a transport for
approximating continuous functions.
Theorem 4.3. As in Lemma 4.2, let f : Rd → R and δ > 0 be given, and define
M := sup |f (X)|,
∣x∣≤1+δ
δ	之一、
α := —=------ = Oe(δ∕√d),
√d + √2ln(2M∕ωf (δ))
f∣δ(x) := f(χ)1[kχk≤ 1 + δ],
r := √d + 2t ln
4πMf(√d + 3)
(2πα2Xd+1"2ωf (δ)
Let Gα denote a Gaussian with the preceding variance α2, and let Tr denote the truncated Fourier
map constructed in Lemma 3.3for f∣δ * Ga, With preceding truncation choice r. Then
supkτr(w)k = O kf∣δkLι
W	T
sup If - Ew F(w), Φ(x; w))| ≤ ωf(δ).
∣x∣≤1
This completes all the pieces needed to prove Theorem 1.5.
Proof of Theorem 1.5. Let f be given, and let Tr denote the transport mapping provided by Theo-
rem 4.3 for f∣δ * Ga, whose various parameters match those in the statement of Theorem 1.5. The
proof is completed by plugging Tr into Theorem 2.1, and simplifying by noting that ≥ ωf(δ) by
definition, and ∣∣f∣δ∣∣Lι = O(M) since δ ≤ 1.	□
8
Published as a conference paper at ICLR 2020
As mentioned earlier, the infinite width network constructed in Lemma 3.1 via inverse Fourier
transforms can be used to succinctly prove (via Lemma 2.5 and Lemma 4.2) that threshold and ReLU
networks are universal approximators, with a rate vastly improving upon that of Theorem 1.5.
Before stating the result, one more tool is needed: a sampling semantics for signed densities; see also
(Bach, 2017a;b) for further development and references.
Definition 4.4. A sample from a signed (Lebesgue) density p : Rd+1 → R with kpkL1 < ∞ is a pair
(W, S) where W is sampled from the probability density 刖/|m|匕,and S := sgn(p(w)). Let Ep denote
the corresponding expectation over (W, S)〜p.	♦
This notion of signed sampling also has a corresponding Maurey lemma, and an analogue for the
uniform norm; both are provided in Appendix B. The full detailed universal approximation theorems
for threshold and ReLU networks are provided in Appendix E; a simplified form for threshold
networks alone is as follows. In either case, the proof proceeds by applying signed density sampling
bounds (e.g., appropriate generalizations of Lemma 2.5) to the infinite width networks constructed in
Lemma 3.1. Curiously, the simplified bound stated here for threshold networks for the uniform norm
is only a multiplicative factor √d larger than the L2 (P) bound in Theorem E.1.
Theorem 4.5 (Simplification of Theorem E.1). Let f : Rd → R and δ > 0 be given, and define
M := sup |f(x)|,	f∣δ(x) := f(x)1[kxk ≤ 1 + δ], α :
kxk≤1+δ
δ
Then there exist c ∈ R and p : Rd+1 → R with
|c| ≤ M + kf∣δkL1 (2∏ɑ2)d/2,	and |加匕 ≤ 2||加|匕 ^0^，
so that, with probability ≥ 1 一 3η over ((Sj, Wj ))m=ι drawn from P (cf. Definition 4.4),
sup f (x) -
kxk≤1I
ci + "'"1〉： Sjσ0(<Wj, x))I I ≤ 2ωf (δ) + ^√m1 ∣8pdln(m) + pln(1∕η)].
5 Ab stract transport mappings, and an RKHS
Section 3 provided concrete transport mappings via Fourier transforms: it was, for instance, easy to
use these constructions to develop approximation rates for continuous functions. These constructions
had a major weakness: they were random feature transport mappings, meaning they arguably did
not fully utilize the transport sampling provided in Section 2. This section will develop one abstract
approach via RKHSes, but first will revisit the random feature constructions.
Suppose f (x) = P p(W)σ0(hW, X))dW for some density p; as in the proof of Lemma 3.2, introducing
the term G(W)∕g(w) gives f (x) = R Gw))σ0(hW,Xi)dG(W), which is now in the desired form,
however the ratio term can be large (and a truncation is needed to make it finite in Lemma 3.2). The
refined construction in Lemma 3.3 achieved a better bound on SuPw ∣∣T(W) k by being careful about
the scaling of the Gaussian, and then standardizing it with a change-of-variable transformation, but
still it yields a random feature transport.
Another approach would be to start from the second construction in Lemma 3.1, which writes f(x) =
∕p(W)σ(hW,Xi)dW = Jp(W)(W,X)σ0(hW,Xi), and thus build a transport around W → p(W)W,
which now uses all coordinates. This transport mapping is still just a rescaling, however, and does
not lead to improvements when plugged into the other parts of this work.
Consider the following approach to building a general T and an associated Reproducing Kernel
Hilbert Space (RKHS). To start, define an inner product(•，∙)h and norm ∣∣ ∙ ∣∣h via IlTkH =
hT, TiH = ʃ ∣T(W)k2 dG(W) = ∣∣T∣L2(g)； to justify this Hilbert space, note that it gives rise to
the usual kernel product (Cho & Saul, 2009), namely
(x,x0) → EwΦ(x; W)TΦ(x0; W) = (Φ(x;), Φ(x0; ∙))h ,
and moreover our earlier predictors can be written as Ew TΓ(W), Φ(x; W))= TΓ, Φ(χ; ∙)Hu.
9
Published as a conference paper at ICLR 2020
The utility of these definitions is highlighted in the following bounds; specifically, while a given T
may have SuPw ∣∣T(W)Il2 = ∞, truncation can make this quantity finite (and thus Theorem 2.1 may
be applied), and the approximation error can be bounded with kTkH .
Proposition 5.1.	1. The output-truncated transport TB(W) := T(W)I [∣∣T(W)∣∣2 ≤ B] has
approximation error
UPJ〈7b, Φ(χr)>H -〈T, φ(χ; ∙))H∣ ≤ kτBH√2.
2.	The input-truncated transport T (W) := T(W)1[∣W∣ ≤ r] with r > dh has approximation
error
IIsup1l<7r, φ(x; ∙“h - T, φ(X； ∙))H∣ ≤ e(r-√dd)2∕4 .
kxk≤1	e
Unfortunately, this formalism can not be applied to the pre-truncation mapping T∞ from Lemma 3.3,
since ∣T∞∣H = ∞. Consequently, this approach is left as an interesting direction for future work.
6 Open problems
The main open question is: how much can the rates 1∕δ2d for ReLU networks and 1∕δ10d for their
NTK be tightened, and is there a genuine gap? Expanding this inquiry, firstly there are three relevant
choices regarding which layers are trained: training just the output layer as with random features
(Bach, 2017b;a), training just the input layer (as in this work), and training both layers. Secondly,
for each of these choices, there is a question of norm; e.g., by requiring the maximum over node
weight Euclidean norms to be small, the NTK regime is enforced. Are there genuine separations
between these settings? Which settings are most relevant empirically? What happens beyond the
NTK (Allen-Zhu & Li, 2019)?
Another direction is to use the Fourier tools of Section 2, as well as other tools for constructing
transportation maps, and identify function classes with good approximation rates by the NTK and by
shallow networks, in particular rates with a merely polynomial dependence on dimension.
Connecting back to the optimization literature, the referenced NTK optimization works for the
squared loss seem to require a width which scales with n, and the test error sometimes scales with
detailed functions of the observed labels, which require a further argument to go to 0 (see, e.g.,
yT(H∞)-1y in (Arora et al., 2019)). Perhaps such quantities can be replaced with a function space
or other approximation theoretic perspective on the conditional mean function (and samples thereof)?
Lastly, what are connections to optimal transport? It seems natural to choose T as an optimal
transport, in which case one would hope the parameter B := supw ∣∣T(W)∣2 can be small, and
moreover easily bounded by the optimal transport cost, ideally in ways similarly easy to the bounding
by the Hilbert norm in Proposition 5.1.
Acknowledgements
The authors are grateful for support from the NSF under grant IIS-1750051, and from NVIDIA via a
GPU grant.
References
Zeyuan Allen-Zhu and Yuanzhi Li. What can resnet learn efficiently, going beyond kernels? 2019.
arXiv:1905.10337 [cs.LG].
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and Generalization in Overparameterized
Neural Networks, Going Beyond Two Layers. arXiv e-prints, art. arXiv:1811.04918, Nov 2018.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. 2018. arXiv:1811.03962 [cs.LG].
10
Published as a conference paper at ICLR 2020
Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-Grained Analysis
of Optimization and Generalization for Overparameterized Two-Layer Neural Networks. arXiv
e-prints, art. arXiv:1901.08584, Jan 2019.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis
of optimization and generalization for overparameterized two-layer neural networks. 2019.
arXiv:1901.08584 [cs.LG].
Francis Bach. Breaking the curse of dimensionality with convex neural networks. Journal of Machine
Learning Research,18(19):1-53, 2017a.
Francis R. Bach. On the equivalence between kernel quadrature rules and random feature expansions.
J. Mach. Learn. Res., 18:21:1-21:38, 2017b.
Andrew R. Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE
Transactions on Information Theory, 39(3):930-945, May 1993.
Ronen Basri, David Jacobs, Yoni Kasten, and Shira Kritchman. The convergence rate of neural
networks for learned functions of different frequencies. In NeurIPS, 2019.
Alberto Bietti and Julien Mairal. On the inductive bias of neural tangent kernels. In NeurIPS, 2019.
Yuan Cao and Quanquan Gu. Generalization Error Bounds of Gradient Descent for Learning
Over-parameterized Deep ReLU Networks. 2019. arXiv:1902.01384 [cs.LG].
Lenaic Chizat and Francis Bach. On the Global Convergence of Gradient Descent for Over-
parameterized Models using Optimal Transport. art. arXiv:1805.09545. arXiv:1805.09545
[cs.LG].
Lenaic Chizat and Francis Bach. A Note on Lazy Training in Supervised Differentiable Programming.
arXiv:1812.07956v2 [math.OC], 2019.
Youngmin Cho and Lawrence K. Saul. Kernel methods for deep learning. In NIPS, 2009.
George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control,
Signals and Systems, 2(4):303-314, 1989.
Simon S. Du, Jason D. Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. 2018. arXiv:1811.03804 [cs.LG].
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. 2018. arXiv:1810.02054 [cs.LG].
Gerald B. Folland. Real analysis: modern techniques and their applications. Wiley Interscience, 2
edition, 1999.
Ken-ichi Funahashi. On the approximate realization of continuous mappings by neural networks.
Neural Networks, 2(3):183-192, 1989.
Leonid Gurvits and Pascal Koiran. Approximation and learning of convex superpositions. In Paul
Vitanyi (ed.), Computational Learning Theory, pp. 222-236. Springer, 1995.
K. Hornik, M. Stinchcombe, and H. White. Multilayer feedforward networks are universal approxi-
mators. Neural Networks, 2(5):359-366, July 1989.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in neural information processing systems, pp.
8571-8580, 2018.
Moshe Leshno, Vladimir Ya. Lin, Allan Pinkus, and Shimon Schocken. Multilayer feedforward
networks with a nonpolynomial activation function can approximate any function. Neural Networks,
6(6):861-867, 1993.
11
Published as a conference paper at ICLR 2020
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. In Advances in Neural Information Processing Systems, pp. 8157-8166,
2018.
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A Mean Field View of the Landscape of
Two-Layers Neural Networks. art. arXiv:1804.06561, 2018. arXiv:1804.06561 [cs.LG].
Hrushikesh N Mhaskar and Charles A Micchelli. Approximation by superposition of sigmoidal and
radial basis functions. Advances in Applied mathematics, 13(3):350-373, 1992.
Greg Ongie, Rebecca Willett, Daniel Soudry, and Nathan Srebro. A function space view of bounded
norm infinite width relu nets: The multivariate case. 2019. arXiv:1910.01635v1 [cs.LG].
Samet Oymak and Mahdi Soltanolkotabi. Towards moderate overparameterization: global conver-
gence guarantees for training shallow neural networks. 2019. arXiv:1902.04674 [cs.LG].
Gilles Pisier. Remarques sur un resultat non publie de b. maurey. Seminaire Analysefonctionnelle
(dit), pp. 1-12, 1980.
Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In NIPS, pp.
1177-1184. 2008.
Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From Theory to
Algorithms. Cambridge University Press, 2014.
Yitong Sun, Anna Gilbert, and Ambuj Tewari. On the approximation properties of random relu
features. arXiv preprint arXiv:1810.04374 [cs.LG], 2018.
Martin J. Wainwright. UC Berkeley Statistics 210B, Lecture Notes: Basic tail and concentration
bounds, Jan 2015. URL https://www.stat.berkeley.edu/~mjwain/stat210b/.
Karl Weierstrass. Uber die analytische darstellbarkeit sogenannter WillkUrliCher functionen eιner
reellen Veranderlichen. Sitzungsberichte derAkademie ZU Berlin, pp. 633-639, 789-805, 1885.
Holger Wendland. Scattered Data Approximation. Cambridge Monographs on Applied and Compu-
tational Mathematics. Cambridge University Press, 2004.
Dmitry Yarotsky. Error bounds for approximations With deep relu netWorks. 2016.
arXiv:1610.01145 [cs.LG].
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv:1611.03530 [cs.LG], 2016.
A Technical lemmas
Gaussian Concentration. The folloWing lemma collects a feW properties of Gaussian concentra-
tion needed throughout.
Lemma A.1. Let W 〜 Gd be a standard Gaussian in Rd, and let r ≥ db be given.
1.
k ∣∣w∣∣ dG(W) ≤ √d.
2. P[∣∣w∣∣ > r] ≤ exp(-(r — √d)2/2); alternatively, with probability at least 1 一 η, ∣∣Wil ≤
√d + P2ln(1∕η).
3.
/
kwk>r
∣∣w∣∣ dG(W) ≤ (r + 2) exp (—(r — √d)2∕2^
≤ 2(√d + 3) exp (-(r — √d)2∕4^.
4.
/
kwk>r
7)2 exp
Wk2 dG(W)	≤
(—(r — √d)2∕4).
2(r + 2)2 exp (—(r — √d)2∕2^	≤	2(√d +
12
Published as a conference paper at ICLR 2020
The more convenient form of some of the inequalities will need to following technical lemma.
Lemma A.2. Given b ≥ 0 and c > 0 and a ≥ 0 with a + b ≥ 2c and x ≥ b, then
(x + a) exp(-(x - b)2/c) ≤ (a + b) exp(-(x - b)2/(2c)),
and if moreover a + b ≥ 4c,
(x + a)2 exp(-(x - b)2/c) ≤ (a + b)2 exp(-(x - b)2/(2c))
Proof. Since ln(x + a) ≤ ln(b + a) + (x - b)/(b + a),
(x + a) exp(-(x - b)2/c) ≤ (a + b) exp(-(x - b)2/c + (x - b)/(b + a))
≤ (a + b) exp(-(x - b)2/c + (x - b)2/(2c))
≤ (a + b) exp(-(x - b)2/(2c)).
Similarly, multiplying the preceding Taylor expansion by 2,
(x + a)2 exp(-(x -	b)2/c)	≤	(a +	b)2 exp(-(x -	b)2/c + 2(x -	b)/(b + a))
≤	(a +	b)2 exp(-(x -	b)2/c + 2(x -	b)2/(4c))
≤ (a + b)2 exp(-(x - b)2/(2c)).
□
Proof of Lemma A.1.
1. By Jensen’s inequality,
kwk dG(w) ≤
Zʃ kwk2 dG(w) = √d.
2. The claim follows from Gaussian concentration with Lipschitz mappings (Wainwright, 2015,
Theorem 2.4), specifically since w 7→ kwk is 1-Lipschitz, meaning
and since E ∣∣wk < √d.
3.	Note that
Zkwk>r ∣w∣ dG(w) ≤ Xi=0 Zr+i<kwk≤r+i+1
(r + i + 1) dG(w),
whereas the Gaussian concentration from the preceding part grants
P[∣w∣ > r + i] ≤ exp(-(r + i — √d)2∕2) ≤ exp(-(r — √d)2∕2) exp(-i2/2),
whereby
∞
∣w∣ dG(w) ≤ (r + 1)	dG(w) + X i	dG(w)
∞
≤ (r + 1) exp(-(r — √d)2∕2) + ^X i exp(-(r — √d)2∕2) exp(-i2/2)
i=0
∞
≤ exp(-(r - √d)2/2) r + 1 + ^X iexp(-i2∕2)
i=0
≤ exp(-(r — √d)2/2) [r + 2].
The final inequality follows by applying Lemma A.2 with (a, b, c, x) = (3, √d, 2, r)
13
Published as a conference paper at ICLR 2020
4.	Proceeding similarly,
Z	kwk dG(w) ≤ X Z	(r + i + 1)2 dG(w),
≤ 2(r+ 1)2Z	dG(w) + 2X∞ i2Z	dG(w)
∞
≤ 2(r + 1)2 exp(-(r — √d)2∕2) + 2 ^X i2 exp(-(r — √d)2∕2) exp(-i2∕2)
i=0
∞
≤ 2exp(-(r — √d)2∕2) (r + 1)2 + ^Xi2 exp(—i2/2)
i=0
≤ 2exp(-(r - √d)2∕2) [r + 2]2 .
The final inequality follows by applying Lemma A.2 with (a, b, c, x) = (7, √d, 2, r).
□
Fourier transforms. The convention for the Fourier transform used here is
f(w) =	f(x) exp(2πiwTx) dx;
see for instance (Folland, 1999, Section 8.8) for a discussion of other conventions, and the resulting
tradeoffs. Note also the polar decomposition notation f (W) = |f (w)| exp(2∏iθf (w)) with ∣θf(w)∣ ≤
1. The following lemma collects a few properties used throughout.
_	. O. .....
LemmaA.3.	1. |f| ≤ |f£.
-——	ʌ . —---.	..... ..
2.	[ g = fg and |f * g| ≤ IlfllL11^∣.
3.	Let a > 0 be g^ven and define φ := (2πɑ)-1. Then |Gα∣ = Gα (meaning Gα has no radial
component, thus θGα (w) = 0), and
Ga(W) = (2πɑ2)-"2Gφ(w) = (2πφ2 )d/2Gg(w) = (2n)d/2G(w/。).
Proof. 1. Directly,
Ih(W)I ≤ / ∣h(x)∣ ∙ ∣ exp(2πiwτx)∣ dx ≤ |向£,
2.	The first equality is standard (Folland, 1999, Theorem 8.22c), and the inequality combines
it with the preceding bound.
3.	The form of Gα and the first displayed inequality are standard (Folland, 1999, Proposition
8.24). The second and third inequalities use the choice of φ and the form of Gφ.
□
ReLU representation. Lastly, the exact ReLU representation constructions (e.g., Lemma 3.1) will
use the following folklore lemma to write a univariate twice continuously differentiable function as
an infinite width ReLU network.
Lemma A.4. Let f : R → R be given with f(0) = f0 (0) = 0 and f00 continuous. For any z ≥ 0,
∞
f(z) =
0
σ(z - b)f 00 (b) db.
14
Published as a conference paper at ICLR 2020
Proof. Using integration by parts,
σ(z - b)f00 (b) db =	(z - b)f 00 (b) db
00
= z zf00(b)db- zbf00(b)db
00
—
zf0(b)|z0
|z0-Zzf0(b)db
zf0(z) - zf0(0) - zf0(z)-0-f(z)+f(0)
f(z).
□
B S ampling tools: Maurey’ s lemma and co-VC dimension
This section collects various sampling tools used as a basis for Section 2. First is a proof of Lemma 2.5,
which here is combined with an application of McDiarmid’s inequality to give a high probability
guarantee.
Proof of Lemma 2.5. Following the usual Maurey scheme (Pisier, 1980),
E
(vj)jm=1
2 2
f - - X gj	= ~-2 E	X (f - gj)
m j	m2 (vj)jm=1 j
=3 E Xf - gj<2 (P)
m2 (vj)m	L2 (P)
12
=m Ekf - g1kL2(p)
=m E (kg1kL2(p)-kfkL2(p))
≤ m EvIkgIkL2(P)
≤ m Sup 加V)直(P).
The high probability bound will follow from McDiarmid’s inequality. To establish the bounded
differences property, define
F(V ):=F((VI,…,Vm))= f - m X g(∙; Vj)
j	L2(P)
and note from the general metric space inequality kpk - kqk ≤ kp - qk that for any V
(V1, . . . , Vm) and V0 = (V10 , . . . , Vm0 ) differing only on a single Vk 6= Vk0 ,
IlF(V)-F(V0)∣∣ ≤ mXg(∙;Vj)- mXg(,Vj)
j	j	L2(P)
=m∣lg(∙; Vk)- g(∙; Vk )I∣L2(P)
≤ m (Hg(∙; Vk )IlL2(P) +llg(∙; Vk )IlL2(P))
2
≤ m vupllg(∙; V)M(P).
15
Published as a conference paper at ICLR 2020
Thus, with probability at least 1 - η, McDiarmid’s inequality grants,
F (V) ≤ EV F (V) + sup kg(∙; v)kL2(p )∖ /小⑴哈,
v∈V	m
and the statement follows by Jensen's inequality, specifically EVF(V) ≤，EVF(V)2 *.	□
Maurey’s lemma also applies to sampling from signed densities in the sense of Definition 4.4.
Lemma B.1. Let f (x) = JP(W)g(〈W,X))dW be given with ∣∣p∣∣Lι < ∞ and P is supported
on a ball of radius B, and let ((sj,Wj ))m=ι be sampled from P as in Definition 4.4, and define
gj (x) ：= g(hW, Xi). With probability at least 1 一 η,
k	kPkLι XX
f - -m~ 工 Sj gj
j=1	L2(P)
≤ SUpllg(hW, ∙i)kL2(P)kPkLι
kWk≤B
1 + /2ln(1∕η)
√m
Proof. Since
J P(W)g(〈W,X))dW
kPkLJSgn(P) ≡L: g(hw,xi)dw
l∣PkLι E sg(hw,xi),
s,W
the sampling procedure indeed provides an unbiased estimate of the integral, and thus by Maurey’s
Lemma (cf. Lemma 2.5), with probability at least 1 一 η,
k	kPkLi X	_ Ii Ii
f ^Σ~ jg Sjgj	= kPkLi
mj1
j=1	L2(P)
1m
Es,Wsg(hwJ, ∙i)-j sjgj
m
j=1
≤ kPkLi suP ksg(hw, ∙i)kL2(P)
s∈{±1}
kWk≤B
kPkLi sup kg((W, ∙i)kL2(P)
kWk≤B
L2(P)
1 + /2ln(1∕η)
√m
1 + √2ln(1∕η)
Lastly, here is a uniform norm analog of the preceding L2 (P ) signed density sampling bound.
Interestingly, the bound only gives a √d degradation with σ0, and no degradation for σ. The method
of proof is to use uniform convergence, but with data and parameters switched; consequently, this has
been called “co-VC dimension” (Gurvits & Koiran, 1995; Sun et al., 2018). The proof is somewhat
more complicated than the proof of the Maurey lemma, and in particular needs to be a bit more
attentive to the fine-grained structure of the functions being sampled.
Lemma B.2. Let density P ： Rd+1 → R with kPkLi < ∞, and let ((Sj, Wj))jm=1 be a sample from P
in the sense of Definition 4.4.
1. With probability at least 1 一 2η,
sup
kxk≤1
J P(W)σ0((W,Xi)dW 一
吗LL X sw0 (<w¼χ>)
j
≤ k√Li [p8(d + 1)ln(m + 1) + pln(1∕η)].
□
2. Suppose P is supported on the set W ：= {W ∈ Rd+1 : kwk ≤ r, |b| ≤ kwk}. With
probability at least 1 一 2η,
sup
kxk≤1
J P(W)σ((W,X))dW —
业m1 X sjσ(<w¼χ>)
j
≤ 4rkPkLi
一	√m
[1 + PIn(I加)].
16
Published as a conference paper at ICLR 2020
Proof of Lemma B.2. In both cases, letting g denote either of σ0 or σ,
sup
kxk≤1
/p(W)g(hW, xi)dW ― kpkL1 X Sjg((Wj,x〉)
m
j
IlpllLI sup Esg(hw,xi)dw
kxk≤1
-m1 X SjgKwj,xA ,
p
j
and at this point it is a classical uniform deviations problem, but with the role of parameter and data
swapped, an approach which has been used before (sometimes under the heading “co-VC dimension”
(Gurvits & Koiran, 1995; Sun et al., 2018)). Continuing, with probability at least 1— 2η, standard
Rademacher complexity (Shalev-Shwartz & Ben-David, 2014) grants
sup E sg(hW,Xi)dW —
kxk≤1
≤ 2Rad( {⑶g(<Wj∙,x>))m=ι : kx∣ ≤ 1
+ 3 SUP ∣g(hW,xi)∣ J1，1/n).
w∈W	V	2m
kxk≤1
where W is a constraint set on W (when g = σ0, it is Rd+1, whereas with g = σ it is ∣b∣ ≤ ∣∣w∣ ≤ r).
To simplify further, note that a Rademacher random vector (1, . . . , m ) is distributionally equivalent
to (S11, . . . , Sm m ) for any fixed vector of signs (S1, . . . , Sm ), and therefore
Rad (1(sjg(<Wj∙,xy))j=ι : kx∣ ≤ 1	1 Ee SUP XSjqg(hW,xi)
n	kxk≤1 j
1 Ee SUP X ej'g(hW, xi)
n	kxk≤1 j
Rad {(g(〈Wj,x)))m=ι : kx∣≤ 1
Combining these steps, with probability at least 1— 2η,
sup
kxk≤1
Zp(W)g(hW, xi)dW ― kpkL1 X Sjg((Wj,x〉)
mj
≤ ∣p∣L1
2Rad ({(g(〈Wj,x>))m=ι : kx∣ ≤ 1}) +3 黑p |g(hW,xi)|ʌ/ln^
kwx∈k≤W1
The proof now splits into two cases g ∈ {σ0, σ}, bounding the remaining terms.
1.	Since the range of σ0 is {0, 1},
sup ∣σ0(hW,xi)∣ ≤ 1,
w∈w
kxk≤1
and the Rademacher complexity is the VC dimension of linear predictors, thus
Rad ({(σ0(<Wj∙,x>))m=ι : kx∣ ≤ 1
2(d + 1) 1n(m + 1)
m
2.	Inthecase g = σ, since W := {W ∈ Rd+1 : ∣w∣ ≤ r, |b| ≤ ∣∣w∣},
sup ∣σ(hW,Xi)∣≤ sup IWTx + b| ≤ 2r.
w∈w
kxk≤1
kwk≤r
网≤kwk
kxk≤1
p
17
Published as a conference paper at ICLR 2020
Moreover, the Rademacher complexity is a standard combination of the Lipschitz composi-
tion rule and linear prediction rules (Shalev-Shwartz & Ben-David, 2014), and thus
Rad ({(σ'(<Wj ,X〉))m=i ： kxk ≤ l}) ≤ Rad ({(㈤㈤雁]：kxk ≤ l}) ≤√
□
C Deferred proofs from Section 2
For convenience throughout this appendix, define
B
B := sup IIT(W)Il	and	Be := SuP k兀(w, s)k2 ≤ —+ + R.
W	W,s	€y m
The first step is to prove eq. (2.2), restated here as follows.
Lemma C.1. With probability at least 1 - η over (W, S),
m
X S, φj 3〉- Ew〈T(W), φ(∙;W)〉
j=1
≤ ∈Be h√2 + 2pln(1∕η)].
L2(P)
Proof. The proof proceeds by applying Maurey sampling (cf. Lemma 2.5) to the functions gj (x) :=
m τj, φj(x) , noting by Lemma 2.4 that
f (x) := E — X gj (X)= E X〈Tj ,φj (x)〉= E(7；(f,S), Φe(x; f ,S)E = E(T(W), Φ(x; W)〉.
f,S m j	f,Sj	f ,S ∖	Iw
Applying Lemma 2.5, with probability at least 1 - η,
¥	1 3	/ supw,s mkG(w, s), φe(∙; W,	s)〉∣∣L2(P)	Γ1 ι ATTTTTvI
f - m 工 ％	≤------------------√m-----------------[1 +，2 ln(1/n)]，
j=1	L2(P)
where
2	2	22B2
sup k 伍(w,s), Φe(∙; W,s)〉∣L2(P) ≤ sup Ex 帜(W)∣∣2∣∣Φe(x; W,s)∣∣2 ≤ -----∙
w,s	w,s	m
□
Next, the restatement of eq. (2.3) is as follows.
Lemma C.2. With probability at least 1
least 1 - η,
—η ,If R ≥ d+ + 2
then with probability at
E<τj ,φj 3〉-
jj
≤ ―-J= + eBe h√2 + 2pln(1∕η)].
mπ
L2(P)
Recall that the proof of Lemma C.2, as discussed in the body, must calculate the fraction of activations
which change, which was collected into Lemma 2.6.
Proof of Lemma 2.6. Consider an idealized T0 which does not truncate, whereby
IKTO(W),x〉| τ hW,xi || ≤∣ T(W)Ix + hW,xi — hW,xi ≤ B√x∣.
m	m
18
Published as a conference paper at ICLR 2020
The event 卜gn(hW,X))= sgn(〈T0(W),X〉)] implies the event [|hW,X)| ≤ Bkx∣∣∕e√m], and thus,
additionally using rotational invariance of the Gaussian,
E∣σ0 "w,χi)- σ0KT7(W)㈤)∣ = p hsgn"w,xi) = sgnKT7(W),x〉)]
≤ P [∣(W,xi∣ ≤ Bkxk/e√m]
W
=P [∣W1∣ ∙ kxk ≤ Bkxk∕e√m]
W
2/2dz
Returning to the general case with truncation, by Lemma A.1, using the assumed lower bound on R,
P[kwk > R] ≤ eχp(-(R - √d)2/2) ≤ —ʌ/—,
mπ
which gives the final bound via triangle inequality.
□
With Lemma 2.6 in hand, the proof of Lemma C.2 is now an application of Maurey’s lemma, with an
invocation of positive homogeneity to massage terms.
Proof of Lemma C.2. The approach is once again to apply Maurey sampling (cf Lemma 2.5). To this
end, define
g(x; W, s) := m
and f (x) = E g(x; W,s),
W,s
—
as well as gj (x) := g(x; Wj, Sj). Using this notation, the goal of this proof is to upper bound
—
L2(P)
m χ %
j
L2(P)
By Lemma 2.5, with probability at least 1 - η,
m X %
j
L2(P)
≤ kfkL2(p)+ f—m χ gj
j
L2(P)
≤ kfkL2(P) +suP kg(∙; W,s)kL2(P)
W,s
1 + ,2ln(1∕η)
√m
To control these terms, fixing any (W, s), it holds by positive homogeneity of σ that
∣∣g(x; W, s) ∣∣L2(p)= m2 E(工(W, s), Φe(x; W, s) — √mΦ(x; T(W, s)))≤ m2B2 E ' "x" ≤ 2mc2B2.
On the other hand, by Lemma 2.6, for any kxk ≤ 1,
|f (x)| ≤ Ew,s∣(∕(W, s), Φe(x; W,s)-S^ Φ(x"(W,s))
≤ BeEw,s
Ckxk ∣σ0(乙(W)Tx) — σ0(WTx) ∣
√m
≤型
which also upper bounds kfkL2(P).
□
19
Published as a conference paper at ICLR 2020
The proof of Theorem 2.1 now follows by combining Lemmas C.1 and C.2.
ProofofTheorem 2.1. By Lemma A.1 and a union bound on (Wι,..., Wm), maxj ∣∣Wjk ≤ R, thus
max kTe(^Wj) - Wj k ≤ max k丁(Wj) k ≤ B
j	j m	m
Moreover, R ≥ √d + 2 Jln (^√∣∏), and thus the two other bounds are from Lemmas C.1 and C.2.
□
D Deferred proofs from Section 3
The first core lemma shows how to write a target function f as an infinite-width network via its
Fourier transform.
Proof of Lemma 3.1. The first steps are the same for σ and σ0, and indeed match the initial steps of
(Barron, 1993), namely
f(x)-f(0)=Re	exp(2πixTW)/(W) dW
=Re/ exp(2πixTW + 2πiθf (w))∣∕(w)∣ dW
=J cos (2π(xtw + θf (w))) ∣f(W)∣dW.
For convenience, define h(z) := cos(2πz), whereby
f (x) - f (0) = / h(xTW + Θ∕(w))∣∕(w)∣ dW,
(D.1)
and the proofs not differ for both activations and from (Barron, 1993).
1.	Consider first σ0. Since kxk ≤ 1, by Cauchy-Schwarz it suffices to approximate h along the
interval [-kWk + θf (W), kWk+	θf (W)]. By the fundamental theorem of calculus,
h(hW, Xi + θf (w)) - h (-∣IWk + θf (w))
hw,xi+θf (w)
=	h0(b) db
-kwk+θf (w)
=	h0(b)1[hW, xi + θf (W) ≥ b]1[b ≥ -kWk + θf (W)] db
-
-Z
h0(θf (W) - b)1[xTW+	b ≥ 0]1[kWk ≥ b] db,
h0(θf (W) - b)1[xTW+	b ≥ 0]1[kWk ≥ |b|] db,
b 7→ θf (W ) - b
where the last step follows since 1[xTW+	b ≥ 0] implies b ≥ -kWk. Plugging this back in
to eq. (D.1) and still using h(z) = cos(2πz),
f(x) - f(0) =
=Z
∣f(w)∣ cos (2π(xTw + θf (w))) dw
.ʌ, .......... ..
If(W)I h(-kwk 十 θf(W))-
h0(θf (W) - b)1[xTW+	b ≥ 0]1[kWk ≥ IbI] db dW,
20
Published as a conference paper at ICLR 2020
which after pushing more terms onto the left hand side gives
f (χ) — f(0) - / f (W)Ih(f (W)TlWk)dw
—
JJ ∖f(w)∖h0(θf (w) — b)1[xτw + b ≥ 0]1[∣∣w∣∣ ≥ |b|] dbdw
2π J f(w)∣ sin(2π(θf(w) — b))σ0((w, X))1[||wk ≥ |b|]dw,
which gives F∞ = f for ∣∣x∣ ≤ 1. To bound the error of Fr, note by the form of F∞ for
any ∣∣x∣ ≤ 1 that
∣f (x) — Fr(x)∣ = 2π [	/|f(w)| sin(2π(θf (w) — b))σ0((W, Xi)1[∣w∣ ≥ |b|] dbdw
∖w∖>r
≤ 2π f /	If(W)|| sin(2π(θf (w) — b))|σ0({WJ, X}) dbdw
∕∣w∣∣>r J∖b∖<^∖w∖∖
≤ 2π ∕w∖>rf(W)4<「dw
=4π /	∣∣w∣ ∙ | f(w)|dw.
∖w∖>r
2.	Now consider σ. Rather than using FTC as above, this proof replaces h with ReLUs via
Lemma A.4, which requires a function which is both zero and flat at 0. To this end, define
H(b) = h(b + q) — (h(q) + bh0(q))	with q = —||w|| + θf (w),
whereby H(0) = 0 = H0(0). Invoking Lemma A.4 on H gives, for any Z := wτx +
θf (w) ≥ q,
h(z) — (h(q) + (z — q)h0(q)) = H(Z — q)
J H00(b)σ(z — q — b)1 [b ≥ 0] db
J H00(b)σ(WTX + θf (w) + ∣∣w∣ — θf (w) — b)1[b ≥ 0] db
J H”(∣IWk- b)σ(wτx + b)1[∣w∣ ≥ b] db
b → ∣∣w∣ — b
J H00(∣∣Wk- b)σ(wτx + b)1[∣w∣ ≥ |b|] db,
the final equality since —b > ∣∣w∣ implies wτx + b ≤ ∣∣w∣ + b < 0, thus σ(Wτx) = 0 and
this case has no effect. Plugging this back into eq.(D.1),
f (x) — f (0) = /
=/
=/
.ʌ.......
|f(w)|h(z)dw
.ʌ, , . .,.
|f (w)| h(q) + (z — q)h (q)—
J H”(131 — 90^^)111311 ≥ |b|] db dw
.ʌ , , . . ..........................
|f (w)| h(q) + (w x + ∣∣w∣)h (q)—
J H00(∣w∣ — 90^^)111311 ≥ |b|] db dw,
21
Published as a conference paper at ICLR 2020
which gives Q∞ = f for kxk ≤ 1 after expanding h and H . To bound the error of Qr , for
any kxk ≤ 1
f (x) - Qr(x)
≤ /wk∕(w"lHθ0(kwk
H00(kwk - b)σ(WτX)1[∣∣wk ≥ ∣b∣]db
dw
—b)∣σ(WτX)1 [kwk ≥ ∣b∣]dbdw
≤ 4π2
/
kwk>r
kwk
|f(w)| /	σ(WτX)dbdw
-kwk
≤ 4π2
kwk
/wjw)/wk
(kwk + |b|) db dw
≤ 12π2
/
kwk>r
..C . ʌ ,
l∣wk2 ∙ ∣f(w)∣dw.
□
Next, Lemma 3.2 converts Lemma 3.1 into a (random feature) transport map by introducing the
fraction G(W)/g(w).
Proof of Lemma 3.2. Starting from the construction in Lemma 3.1, again using h(z) = cos(2πz) for
convenience, and manually introducing a factor G(W),
f (x) -
f(0)-
.ʌ, .. ............
lf(w)lh(θf(W)- kwk)dw
—
JJ ∣f(w)∣h0(θf (w) — b)1[xTw + b ≥ 0]1[∣w∣ ≥ |b|] dbdw
/ Wh0(θf (w) - b)σ0(hW,X>)1[∣H∣ ≥ ∣b∣]dG(W).
(w)
To construct T∞, rotational invariance of the Gaussian gives E1 [wTX ≥ 0] = 1∕2, thus
f(0)+
.ʌ, .. ..............
lf(w)lh(θf (w)-kwk)dw
/2 f(0) + / lf(v)lh(θf(V)-kvk)dv
σ0(hw, Xi) dG(w),
and transport mapping is T∞(w, b) = (0,p∞(w)) ∈ Rd X R with
Γ	^	1	l^工,∖l
p∞(w)=2 f(0)+ 八f(v)∣h(θf(v)-H)dv -黜h0(θf(w)-b)1[∣wk≥ 矶
(w)
By construction, Ew F (w), Φ(x; w))= Fr (x), and therefore Lemma 3.1 grants for all ∣χ∣ ≤ 1
f (x) = Ew <T∞(w), Φ(x; w))
and
∣f (x) - E (Tr(w), Φ(x; w))|
≤ 4π
/
kwk>r
.ʌ.......
If(W)HlWkdw∙
□
With more care (in particular, a crucial change of variable), a much better bound is possible for
convolutions with Gaussians.
Proof of Lemma 3.3. By Lemma A.3, setting φ := (2πσ)-1,
∣fα(w)∣ = ∣f(w)∣Gα(w) = (2∏)"2∣∕(w)∣G(w∕φ).
Plugging this into Lemma 3.1 and again defining h(z) := cos(2πz) for convenience, but unlike
Lemma 3.2 performing a change of variable to directly introduce G(W), and then manually introducing
22
Published as a conference paper at ICLR 2020
G(b),
fα(X)- fα (0) - / lfα(w)lh(θfα (w) - kwk)dw
—
∣fα(w)∣h0(θfa(w) - b)σ0(WτX)1[kwk ≥ ∣b∣]dbdw
=-(2π)d∕2" ∣∕(w)∣G(w∕φ)h0(θfa(W)- b)σ0(WτX)1[kwk ≥ ∣b∣]dbdw
=—(2πφ2)"2φ J ∣f(φw)∣G(w)h0(θfα (φw) — b)σ0(φ(W,X〉)1[0kwk ≥ φ∣b∣]db dw
=-(2πφ*d+1”2/∣∕(φw)∣eb2∕2h0(θfɑ(φw) - b)σ0(hW,Xi)1[∣∣w∣∣ ≥ ∣b∣]dG(W).
As in Lemma 3.2, the transport is constructed by using Eσ0(hW, Xi) = 1∕2 to model constants:
Tr(w, b) = (0,..., 0,pr(W)), where
Pr(W) := 2 fα(0) + / lfα(v)lh(θfα (V)- kvk)dv
-(2πφ2)3+1”2∣∕(φw)∣eb2 /2h0(θfɑ (φw) - b)1 [|b| ≤ ∣∣w∣∣ ≤ r],
with f (x) = Ew (T∞(W), Φ(x; W))for ∣∣xk ≤ 1 by construction.
When r < ∞, by construction
喟kTr(w)k
≤ 2fα(0) +2
∣fα(v)∣ dv + 2π(2πφ2)(d+1)/2 sup
kwk≤r
网≤kwk
∣f(φw)∣eb2/2,
where |f (φw)∣ = 1 when fa = Ga (meaning f itself is the DiraC at 0), and more generally
Lemma A.3 grants |f (φw)∣ ≤ ∣∣f (φ∙)∣∣L1; as in the lemma statement, these cases are summarized
with |f (φw)∣ ≤ Mf. Plugging this in and simplifying further via Lemma A.3,
SUP ∣∣τr(W)k ≤ 2
w
f (X)Gα (-X) dX
+ 2(2π)d∕2/
∣f(v)∣G(v∕φ) dv + 2π(2πφ2)3+1"2Mf sUP eb2∕2
∣b∣≤r
≤ 2 M + 2(2πφ2)d∕2Mf + 2π(2πφ2)(d+1)∕2Mf sUP eb2∕2
∣b∣≤r
For the approximation estimate, for any ∣X∣ ≤ 1, the preceding derivation and Lemma A.1 grant
f (x) - E I(Tr (W), Φ(x; W))|
—
≤ 2π(2πφ2)(d+1)/2 ʃ	J	∣f(φw)∣∣ sin(2π(wτx + θfα(w)))σ0(WTX)∣ dbdG(w)
≤ 2π(2πφ2)(d+1)∕2Mf	db dG(W)
≤ 4π(2πφ2)(d+1)∕2Mf	∣W∣ dG(W)
kwk>r
≤ 4π(2πφ2)5+1”2Mf (√d + 3) exp (-(r — √d)2∕4^ .
□
E Deferred proofs from Section 4
The first proof is of the approximation properties of Gaussian convolution; as stated in the body, the
proof proceeds by splitting the error into two terms, one for nearby points, the other for distant points.
23
Published as a conference paper at ICLR 2020
Proof of Lemma 4.2. Splitting the integral into two terms, for any kXk ≤ 1,
f (x) - (f∣δ * Ga)(X)	f∣δ (X)Ga(Z) dz - / f∣δ (Z)Ga(X - Z) dz
f∣δ(X)Ga(z)dz - / f∣δ(x - Z)Ga(Z)dz
≤ I ∣f∣δ(x) - f∣δ(x - z)∣ Gα(z)dz
f∣δ(X)- f∣δ(X - Z)I Gα(z)dz
f∣δ(x) - f∣δ(x - z)I Gα(z)dz.
Analyzing these terms separately, the definition of ωf (δ) gives
f∣δ(x) - f∣δ(x - z)∣ Ga(Z)dZ ≤ [	ωf(δ)Ga(Z)dZ ≤ ωf(δ),
kzk≤δ
whereas Gaussian concentration (cf. Lemma A.1) gives
f∣δ(x) - f∣δ(x - Z)IGa(Z)dZ ≤ 2MP[∣∣αZ∣∣ >δ] ≤ 2Mexp(-(δ∕α-√d)2∕2) ≤ ωf (δ).
□
This now combines with Lemma 3.3 to prove Theorem 4.3.
Proof of Theorem 4.3. Plugging the choice of r into Lemma 3.3, for any kxk ≤ 1,
∣f (x) - E(Tr (W), Φ(x; w))I ≤ 4π(2πφ2)3+1"2Mf (√d + 3) exp(-(r -√d)2∕4) ≤ ωf (δ).
Moreover, plugging r into the estimate on SuPw ∣∣Tr (W) ∣∣ provided by Lemma 3.3 gives
SUp ∣∣τr (W)k ≤ 2 M + (2n02)d/2Mf(1 + P2π3φ2er2/2)],
where Lemma A.3 and the choice of r give
Mf ≤kf∣δ∣Lι,	and	£2/ ≤ ede(r-√d)2,
where
4π(2πφ2)(d+1)22Mf (√d +3) !4 _	( M Mf √d
ωf(δ)	J	[ (ωf3)ɑd+1
and noting moreover that α = Oe(δ∕√d).
□
To close this section comes the full version of Theorem 4.5, which gives explicit constructions for
both threshold σ0 and ReLU σ. Interestingly, in the case of σ0 , it is not necessary to truncate the
density, as is the case everywhere else in this work.
Theorem E.1. As in Lemma 4.2, let f : Rd → R and δ > 0 be given, and define
M := Sup |f (x)|,
kxk≤1+δ
f∣δ(x) := f (x)1[∣xk ≤ 1 + δ],
Let Ga denote a Gaussian with the preceding variance a2, and define h := f∣δ * Ga With Fourier
transform h satisfying radial decomposition h(W) = |h(W)| exp(2πiθh (W). Lastly, let P be a
probability measure supported on ∣x∣ ≤ 1.
24
Published as a conference paper at ICLR 2020
1.
Additionally define
ci :=	h(0)+J	|h(w)| cos (2π(θh(w)	-∣∣Wk))	dw,	pi ：=	2π∣h(w)∣ sin(2π(θ%(w)-b))1	[|b|	≤	∣∣w∣].
Then
lci∣≤ M + kf∣δ∣L1 (2∏ɑ2)d/2,	and 1血必 ≤ 2||加以 I^od+,
and with probability at least 1 — 3η over a draw of ((sj, wj ))m=ι from Pi (cf. Definition 4.4),
f-
m
ci + l∣PιkLι Esjc0Hwj, ∙))
j=i
≤ 2ωf(δ) + kp1kL1
L2(P)
1 + ,2ln(1∕η)
√m
sup f (x) -
kxk≤i
ci + ∣∣pιkLι ^X sjσ0(<wj,X)) 11 ≤ 2ωf (δ) + "√m1 [p8(d +I)ln(m +I) + pin(1/n)i.
2.
Additionally define
c2 ：= f (0)f (0) + / |h(w)| [cos(2π(θh(w) -kwk)) - 2∏kwk sin(2π(θ%(w) -∣∣w∣∣))] dw,
a2 := J w∣h(w)∣ dw,
r2 := √d + 2^JIn
24π2( √d +7)2 ∣f∣δ kLι
ωf (δ)
p2(W):= 4π2∣h(w)∣ cos(2π(kwk - b))1[∣b∣ ≤ IHl ≤ r2],
and for convenience create fake (weight, bias, sign) triples
(w, b, s)m+ι ：= (0, El, m ∙ sgn(c2)),	(w,b, s)m+2 ：= (a2,0, +m), (w,b, s)m+3 ：= (-。2, 0, -m).
Then
ka2k2 ≤ √dkf∣δ∣L1 φ(2πα2厂d/2,
kP2kL1 ≤ 2kf∣δ ∣L1 j(2Π0⅞+i,
∣C2∣ ≤ M + 2√dkf∣δ∣L1 (2πα2厂d/2,
and with probability at least 1 — 3η over a draw of ((sj, wj ))m=ι from P2 (cf. Definition 4.4),
m+3
f - m X Sjσ(Vwj, ∙))
j=1
sup f (x) -
kxk≤i
L2(P)
≤ 3ωf (δ) + r2kpkL1
1 m+3
m X sjσ(Vwj, •〉)≤ 3ωf⑷ +
j=1
1 + ,2ln(1∕η)
√m
4r⅛ hi + pιn≡i.
Proof.
1. By Lemma 3.1 and the choice of b1, for any kxk ≤ 1,
h(x) = c1 +J Pι(W)σ0(hW,xi)dW,
25
Published as a conference paper at ICLR 2020
thus by Lemma 4.2 and Lemma B.1, defining hj := ∣∣p∣∣L1 σ0(〈Wj, •))for convenience, with
probability at least 1 — η,
f— (c1 +	hj/m)
j
≤kf — hkL2(P) +
L2(P)
h — (c1 + hj /m)
j
L2(P)
≤ 2ωf 3) + llp1kLι SUp kσKw, ∙i∣L2(P)
≤ 2ωf (δ) + kp1kL1
kwk≤r2
"1 + P2ln(i∕η)
√m
1 + ,2ln(1∕η)
√m
Similarly, the uniform norm bound follows by Lemma 4.2 and Lemma B.2: with probability
at least 1 - 2η, for any kxk ≤ 1,
f(x) -	(c1	+	hj (x)/m)	≤ f (x) - h(x) +h(x)	-	(c1	+	hj (x)/m)
≤ 2ωf (δ) + H^√mL1 [p8(d + 1)ln(m + 1) + pln(1∕η)].
For the estimates on |c1| and kp1 kL1, note setting φ := (2πα)-1, note by Lemma A.3 and a
change of variable W 7→ φW and Lemma A.1 that
IlplkLi ≤ 2π / |/\ * Ga(W)I / 1[|b| ≤ ∣∣wk] db dw
≤ 4∏kf∣δ∣∣Lι / kwk(2πφ2)d∕2Gφ(w)dw
=4π(2π)"2kf∣δ∣∣Lι / kφw∣φdG(w) dw
≤ 4π(2π)"2φd+1kf∣δ∣Lι / IHIG(W)dw
≤ 4√dπ(2π)d^φd+1kf∣δ∣Lι,
≤ 2kf∣δ ∣Lij .9 272dd+ι .
1	(2πα2)d+1
Similarly,
|ci| ≤ M + kf∣δ ∣L1 / G α(w)dw ≤ M + kf∣δ ∣L1 (2∏φ2)d/2.
2.	By Lemma 3.1 and Lemma A.1 and the various chosen parameters, for any kxk ≤ 1,
b2 + hx, a* + Jp2(W)σ(hW, Xi) dW — h(x) ≤ 12π
2 /	∣∣w∣2∣h(w)∣ dw
kwk>r2
j
j
≤ 24π2(√d + 7)2 exp( —(r2 — √d)2∕4)
≤ ωf(δ).
Thus by Lemma 4.2 and Lemma B.1, defining hj := IlpkLI Sjσ((Wj, )) for convenience,
with probability at least 1 — η,
I	m+3	I	I
≤kf — hkL2(P)
+ Ilf - (b2 + (∙)Tc2 + Ep2 s1h1 Il +	EP2 s1 h1 - ɪ2 Sjhj/m
L2(P)
j=1
L2(P)
m
≤ 3ωf (δ) + kp2 kLl k：Upr2 kσ(hw, ∙ikL2(P)
1 + √2ln(1∕η)
√m
≤ 3ωf (δ) +2r2kpkL1
1 + VZln(I加
√m
26
Published as a conference paper at ICLR 2020
Similarly, the uniform norm bound follows by Lemma 4.2 and Lemma B.2: with probability
at least 1 - 2η, for any kxk ≤ 1,
m+3	m
f (x) -	hj (x)/m	≤	f (x)	- h(x)	+f(x)	-	(b2 +xTc2 + Ep2s1h1(x)	+E s1h1(x)	-	sj hj (x)/m
p2
j=1	j=1
≤ 3ωf (δ) + TpmLL [l + PlnWn)i .
For the estimates on |c1| and kp1kL1, note setting φ := (2πα)-1, note by Lemma A.3 and a
change of variable w 7→ φw and Lemma A.1 that
∣∣P2∣∣L1 ≤ 4∏2 / |/\ * Ga(w)∣ / 1[|b| ≤ IlwIl ≤ r2] dbdw
≤ 8∏2kf∣δ∣∣L1 [	kwk(2πφ2)"2Gφ(w)dw
kwk≤r2
= 8∏2(2∏)"2∣∣f∣δ∣∣Lι [	∣∣φw∣∣φdG(w)dw
kφwk≤r2
≤ 8∏2(2π)"2φd+1kf∣δ∣Lι [	IHq(W) dw
kφwk≤r2
≤ 8√d∏2(2π严φd+1 ∣f∣δ∣Lι
≤ 2kfiδ kL1 S (2∏αjd+1.
Similarly,
∣C2∣ ≤ M + ∣∣f∣δ ∣∣L1 /(1 + kwk)Gα(w)dw
≤ M + kf∣δIL1 (2∏φ2)d/2 /(1 + ∣φwk)dG(w)
≤ M + 2√d∣f∣δ∣Lι(2πφ2)d/2,
Ia2I2
=J w∣h(w)∣ dw
≤ / ∣∣w∣∣∣h(w)∣dw
≤ kf∣δIL1 (2∏φ2)d/2/ ∣φwk∣h(w)∣dw
≤ √d∣f∣δ∣L1 φ(2πφ2严.
□
F Deferred proofs from Section 5
Lastly, the two short proofs leading to the RKHS bounds.
Proof of Proposition 5.1.	1. By Markov’s inequality
/ 1[∣T(w)∣2 >B]dG(w) ≤ kB≡,
27
Published as a conference paper at ICLR 2020
thus by Cauchy-Schwarz, for any ∣∣x∣∣ ≤ 1,
,Φ(x; ∙)	〉H -〈T, Φ3∙)>H∣= / (TB(W)-T(W))TXσ0(hW,Xi)dG(W) ≤/ ∣ 1 [∣∣T(W)∣∣2 >B]∣ ∙ ∣T(W)TX I ∙∣σ0(hW,Xi)IdG(W) ≤ j/1 [∣∣T(W)∣∣2 >B]2 dG(W) ∙ J (T(W)Tx)2 dG(W) ≤ 勺∙ j/2∣∣T(W)∣∣2dG(W) _ kTkH√2 =B .
2. Proceeding similarly, but now using Lemma A.1 to control the indicator,
k%, Φ(x; ∙))H - (T, Φ(x; ∙))h I = / (Tr(W)- T(W))T Xσ0(hW,X))dG(W)
≤ / ∣ 1 [∣∣W∣∣2 > r] ∣ ∙ IT(W)TX i ∙ ∣σ0(<W,X))IdG(W)
≤
W∣∣2 > r]2 dG(W) ∙ J / (T(W)Tx)2 dG(W)
≤ jexp (-(r -√d)2∕2) ∙ j/ 2∣∣T(W)∣∣2 dG(W)
=kT∣∣H √2
e(r-√d)2∕4 .
□
28