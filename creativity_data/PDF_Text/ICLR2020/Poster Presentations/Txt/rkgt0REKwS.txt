Published as a conference paper at ICLR 2020
Curriculum Loss: Robust Learning and Gen-
eralization against Label Corruption
Yueming Lyu & Ivor W. Tsang
Centre for Artificial Intelligence, University of Technology Sydney
yueminglyu@gmail.com, Ivor.Tsang@uts.edu.au
Ab stract
Deep neural networks (DNNs) have great expressive power, which can even mem-
orize samples with wrong labels. It is vitally important to reiterate robustness and
generalization in DNNs against label corruption. To this end, this paper stud-
ies the 0-1 loss, which has a monotonic relationship with empirical adversary
(reweighted) risk (Hu et al., 2018). Although the 0-1 loss has some robust proper-
ties, it is difficult to optimize. To efficiently optimize the 0-1 loss while keeping its
robust properties, we propose a very simple and efficient loss, i.e. curriculum loss
(CL). Our CL is a tighter upper bound of the 0-1 loss compared with conventional
summation based surrogate losses. Moreover, CL can adaptively select samples
for model training. As a result, our loss can be deemed as a novel perspective
of curriculum sample selection strategy, which bridges a connection between cur-
riculum learning and robust learning. Experimental results on benchmark datasets
validate the robustness of the proposed loss.
1	Introduction
Noise corruption is a common phenomenon in our daily life. For instance, noisy corrupted (wrong)
labels may be resulted from annotating for similar objects (Su et al., 2012; Yuan et al., 2019), crawl-
ing images and labels from websites (Hu et al., 2017; Tanaka et al., 2018) and creating training
sets by program (Ratner et al., 2016; Khetan et al., 2018). Learning with noisy labels is thus an
promising area.
Deep neural networks (DNNs) have great expressive power (model complexity) to learn challenging
tasks. However, DNNs also undertake a higher risk of overfitting to the data. Although many regu-
larization techniques, such as adding regularization terms, data augmentation, weight decay, dropout
and batch normalization, have been proposed, generalization is still vitally important for deep learn-
ing to fully exploit the super-expressive power. Zhang et al. (2017) show that DNNs can even fully
memorize samples with incorrectly corrupted labels. Such label corruption significantly degenerates
the generalization performance of deep models. This calls a lot of attention on robustness in deep
learning with noisy labels.
Robustness of 0-1 loss: The problem resulted from data corruption or label corruption is that test
distribution is different from training distribution. Hu et al. (2018) analyzed the adversarial risk
that the test distribution density is adversarially changed within a limited f -divergence (e.g. KL-
divergence) from the training distribution density. They show that there is a monotonic relationship
between the (empirical) risk and the (empirical) adversarial risk when the 0-1 loss function is used.
This suggests that minimizing the empirical risk with the 0-1 loss function is equivalent to minimize
the empirical adversarial risk (worst-case risk). When we train a model based on the corrupted
training distribution, we want our model to perform well on the clean distribution. Since we do not
know the clean distribution, we want our model to perform well for the worst case estimate of the
clean distribution in some constrained set. It is thus natural to employ the worst-case classification
risk of the estimated clean distribution as the objective. Note that the worst-case classification risk
is an upper bound of the classification risk of the true clean distribution, minimizing the worst-case
risk can usually decrease the true risk. When we employ the 0-1 loss, because of the equivalence
between the classification risk and the worst-case classification risk, we can directly minimize the
1
Published as a conference paper at ICLR 2020
classification risk under the corrupted training distribution instead of minimizing the worst-case
classification risk.
From the learning perspective, the 0-1 loss is more robust to outliers compared with an unbounded
(convex) loss (e.g. hinge loss) (Masnadi-Shirazi & Vasconcelos, 2009). This is due to unbounded
convex losses putting much weight on the outliers (with a large loss value) when minimizing the
losses (Masnadi-Shirazi & Vasconcelos, 2009). If the unbounded (convex) loss is employed in deep
network models, this becomes more prominent. Since training loss of deep networks can often be
minimized to zero, outlier with a large loss has a large impact on the model. On the other hand, the
0-1 loss treats each training sample equally. Thus, each sample does not have too much influence on
the model. Therefore, the model is tolerant of a small number of outliers.
Although the 0-1 loss has many robust properties, its non-differentiability and zero gradients make
it difficult to optimize. One possible way to alleviate this problem is to seek an upper bound of
the 0-1 loss that is still efficient to optimize but tighter than conventional (convex) losses. Such a
tighter upper bound of the 0-1 loss can reduce the influence of the noisy outliers compared with
conventional (convex) losses. At the same time, it is easier to optimize compared with the 0-1 loss.
When minimizing the upper bound surrogate, we expect that the 0-1 loss objective is also minimized.
Learnability under large noise rate: The 0-1 loss cannot deal with large noise rate. When the
noise rate becomes large, the systematic error (due to label corruption) grows up and becomes
not negligible. As a result, the model’s generalization performance will degenerate due to this
systematic error. To reduce the systematic error produced by training with noisy labels, several
methods have been proposed. They can be categorized into three kinds: transition matrix based
method (Sukhbaatar et al., 2014; Patrini et al., 2017; Goldberger & Ben-Reuven, 2017), regular-
ization based method (Miyato et al., 2016) and sample selection based method (Jiang et al., 2018;
Han et al., 2018b). Among them, sample selection based method is one promising direction that se-
lects samples to reduce noisy ratio for training. These methods are based on the idea of curriculum
learning (Bengio et al., 2009) which is one successful method that trains the model gradually with
samples ordered in a meaningful sequence. Although they achieve success to some extents, most of
these methods are heuristic based.
To efficiently minimize the 0-1 loss while keeping the robust properties, we propose a novel
loss that is a tighter upper bound of the 0-1 loss compared with conventional surrogate losses.
Specifically, giving any base loss function l(u) ≥ 1 u < 0 , u ∈ R, our loss Q(u) satisfies
pn=ι 1 (Ui < 0)≤ Q(U) ≤ Pn=ι l(u，i), Where U = [uι,…，un] with Ui being the classifica-
tion margin of ith sample, and 1(∙) is an indicator function. We name it as Curriculum Loss (CL)
because our loss automatically and adaptively selects samples for training, which can be deemed as
a curriculum learning paradigm.
Our contributions are listed as follows:
•	We propose a novel loss (i.e. curriculum loss) for robust learning against label corruption.
We prove that our CL is a tighter upper bound of 0-1 loss compared with conventional
summation based surrogate loss. Moreover, CL can adaptively select samples for stagewise
training, which bridges a connection between curriculum learning and robust learning.
•	We prove that CL can be performed by a simple and fast selection algorithm with
O(n log n) time complexity. Moreover, our CL supports mini-batch update, which is con-
venient to be used as a plug-in in many deep models.
•	We further propose a Noise Pruned Curriculum Loss (NPCL) to address label corruption
problem by extending CL to a more general form. Our NPCL automatically prune the
estimated noisy samples during training. Moreover, NPCL is also very simple and efficient,
which can be used as a plug-in in deep models as well.
2	Curriculum Loss
In this section, we present the framework of our proposed Curriculum Loss (CL). We begin with
discussion about robustness of the 0-1 loss in Section 2.1. We then show that our CL is a tighter
upper bound of the 0-1 loss compared with conventional summation based surrogate losses in Sec-
tion 2.2. A tighter bound of the 0-1 loss means that it is less sensitive to the noisy outliers, and it
2
Published as a conference paper at ICLR 2020
better preserves the robustness of the 0-1 loss with a small rate of label corruption. For a large rate
of label corruption, we extend our CL to a Noise Pruned Curriculum Loss (NPCL) to address this
issue in Section 2.3. A simple multi-class extension and a novel soft multi-hinge loss are included
in the Appendix. All the detailed proofs can be found in the Appendix as well.
2.1	Robustness of 0-1 loss against label corruption
We rephrase Theorem 1 in (Hu et al., 2018) from a different perspective, which motivates us to
employ the 0-1 loss for training against label corruption.
Theorem 1. (Monotonic Relationship) (Hu et al. (Hu et al., 2018)) Let p(x, y) and q(x, y) be the
training and test density,respectively. Define r(x, y) = q(x, y)/p(x, y) and ri = r(xi, yi). Let
l(yb, y) = 1 sign(yb) 6= y and l(yb, y) = 1 argmaxk (ybk) 6= y be 0-1 loss for binary classification
and multi-class classification, respectively. Let f (∙) be convex with f (1) = 0. Define risk R(θ),
empirical risk R(θ), adversarial risk Radv (θ) and empirical adversarial risk Radv (θ) as
R(θ) = Ep(x,y) [l(gθ(x), y)]	(1)
1n
R(θ) = n Ei=I l(gθ (xi),yi)	(2)
Radv(θ) = sup Ep(x,y) [r(x, y)l(gθ(x), y)]	(3)
r∈Uf
1n
Radv(θ)= sup —〉/ Iril(gθ(Xi),yi),	(4)
n	i=1
r∈Uf
where Uf	=	r(x, y) Ep(x,y)	[f	(r(x, y))] ≤	δ, Ep(x,y) [r(x, y)] = 1, r(x, y) ≥	0, ∀(x, y) ∈ X × Y
andUf =	{r ∣ n Pn=I f (ri)	≤	δ, n1 Pn=I &	= 1, r ≥ 0 }. Then we have that
If Radv (θ1) <	1,	then R(θ1)	< R(θ2) Q⇒ Radv (θ1) < Radv (θ2).	(5)
If Radv(θι) =	1,	then R(θι)	≤ R(θ2) 0 Radv(θ2) = 1.	(6)
The same monotonic relationship holds between their empirical approximation: R(θ) and Radv.
Theorem 1 (Hu et al., 2018) shows that the monotonic relationship between the (empirical) risk
and the (empirical) adversarial risk (worst-case risk) when 0-1 loss function is used. It means that
minimizing (empirical) risk is equivalent to minimize the (empirical) adversarial risk (worst-case
risk) for 0-1 loss. When we train a model based on the corrupted training distribution p(x, y),
we want our model to perform well on the clean distribution q(x, y). Since we do not know the
clean distribution q, we want our model to perform well for the worst-case estimate of the clean
distribution, with the assumption that the f -divergence between the corrupted distribution p and the
clean distribution q is bounded by δ. Note that the underlying clean distribution is fixed but unknown,
given the corrupted training distribution, the smallest δ that bounds the divergence between the
corrupted distribution and clean distribution measures the intrinsic difficulty of the corruption, and
it is also fixed and unknown. The corresponding worst-case distribution w.r.t the smallest δ is an
estimate of the true clean distribution, and this worst-case risk upper bounds the risk of the true
clean distribution. In addition, this bound is tighter than the other worst-case risks w.r.t larger δ. It
is natural to use this upper bound as the objective for robust learning. When we use 0-1 loss (that
is commonly employed for evaluation), because of the equivalence of the risk and the worst-case
risk, we can directly minimize risk under training distribution p instead of directly minimizing the
worst-case risk (i.e., the upper bound). Moreover, this enables us to minimize the upper bound
without knowing the true δ beforehand. When the true δ is small, i.e., the corruption of the training
data is not heavy, the upper bound is not too pessimistic. Usually, minimizing the upper bound can
decrease the true risk under clean distribution. Particularly, when the clean distribution coincides
with the worst-case estimate w.r.t the smallest δ, minimizing the risk under the corrupted training
distribution leads to the same minimizer as minimizing the risk under the clean distribution.
2.2	Tighter upper bounds of the 0-1 Loss
Unlike commonly used loss functions in machine learning, the non-differentiability and zero gradi-
ents of the 0-1 loss make it difficult to optimize. We thus propose a tighter upper bound surrogate
3
Published as a conference paper at ICLR 2020
loss. We use the classification margin to define the 0-1 loss. For binary classification, classification
margin is u = yby, where yb and y ∈ {+1, -1} denotes the prediction and ground truth, respectively.
(A simple multi-class extension is discussed in the Appendix.) Let ui ∈ R be the classification
margin of the ith sample for i ∈ {1, ..., n}. Denote u = [u1, ..., un]. The 0-1 loss objective can be
defined as follows:
J(U)= Pi=1 1(ui < 0).	⑺
Given a base upper bound function l(u) ≥ 1 u < 0 , u ∈ R, the conventional surrogate of the 0-1
loss can be defined as
Jb(u) = Pin=1 l(ui).	(8)
Our curriculum loss Q(u) can be defined as Eq.(9). Q(u) is a tighter upper bound of 0-1 loss J(u)
compared with the conventional surrogate loss J (u), which is summarized in Theorem 2:
Theorem 2. (Tighter Bound) Suppose that base loss function l(u) ≥ 1 u < 0 , u ∈ R is an
upper bound of the 0-1 loss function. Let ui ∈ R be the classification margin of the ith sample for
i ∈ {1,…,n}. Denote max(∙, ∙) as the maximum between two inputs. Let U = [uι,…,Un]. Define
Q (u) as follows:
Q (U)=	minιmax (En	.	vil(ui ),n-En	,	vi	+ EnlI(Ui	<	0)).	⑼
v∈{0,1}n	i=1	i=1	i=1
EI T/ ∖	,∕Λ∕	∖	，今/	∖ 1	1 1	.
Then J(U) ≤ Q (U) ≤ J (U) holds true.
Remark: For any fixed u, We can obtain an optimum solution V of the partial optimization. The
index indicator V can naturally select samples as a curriculum paradigm for training models. The
partial optimization W.r.t index indicator v can be solved by a very simple and efficient algorithm
(Algorithm 1) in O(n log n). Thus, the loss is very efficient to compute. Moreover, since Q (U) is
tighter than conventional surrogate loss J (U), it is less sensitive to outliers compared With J (U).
Furthermore, it better preserves the robust property of the 0-1 loss against label corruption.
The difficulty of optimizing the 0-1 loss is that the 0-1 loss has zero gradients in almost everyWhere
(except at the breaking point). This issue prevents us from using first-order methods to optimize
the 0-1 loss. Eq.(9) provides a surrogate of the 0-1 loss With non-zero subgradient for optimization,
While preserving robust properties of the 0-1 loss. Note that our goal is to construct a tight upper
bound of the 0-1 loss While maintaining informative (sub)gradients. Eq.(9) balances the 0-1 loss and
conventional surrogate by selecting (the trust) samples (index) for training progressively.
Updating With all the samples at once is not efficient for deep models, While training With mini-batch
is more efficient and Well supported for many deep learning tools. We thus propose a batch based
curriculum loss Q(U) given as Eq.(10). We shoW that Q(U) is also a tighter upper bound of 0-1 loss
objective J(U) compared With conventional loss J (U). This property is summarized in Corollary 1.
Corollary 1. (Mini-batch Update) Suppose that base loss function l(U)≥1U<0,U ∈ R is an
upper bound of the 0-1 loss function. Let b, m be the number of batches and batch size, respectively.
Let Uij ∈ R be the classification margin of the ith sample in batch j for i ∈ {1, ..., m} and j ∈
{1, ..., b}. Denote U = [U11 , ..., Umb]. Letn = mb. Define Q (U) as follows:
Q (U)=XbTUminIm max (Xmι vijl(Uij ),m-Xm 1 vij + Xmι1(uij< 0)). (IO)
j=1 v∈{0,1}	i=1	i=1	i=1
Then J(U) ≤ Q (U) ≤ Q (U) ≤ J (U) holds true.
Remark: Corollary 1 shoWs that a batch-based curriculum loss is also a tighter upper bound of
0-1 loss J(U) compared With the conventional surrogate loss J (U). This enables us to train deep
models With mini-batch update. Note that random shuffle in different epoch results in a different
batch-based curriculum loss. Nevertheless, We at least knoW that all the induced losses are upper
bounds of 0-1 loss objective and are tighter than J (U). Moreover, all these losses are induced by the
same base loss function l(∙). Note that, our goal is to minimize the 0-1 loss. Random shuffle leads
to a multiple surrogate training scheme. In addition, training deep models Without shuffle does not
have this issue.
We noW present another curriculum loss E (U) Which is tighter than Q(U). E (U) is an (scaled)
upper bound of 0-1 loss. This property is summarized as Theorem 3.
4
Published as a conference paper at ICLR 2020
Algorithm 1 Partial Optimization
Input: ui for i ∈ {1, ..., n}, the selection threshold C;
Output: Index set v = (v1, v2, . . . , vn);
Compute the losses li = l(ui) for i = 1, ..., n;
Sort samples (index) w.r.t. the losses {li}n=ι in a non-decreasing order; // Get lι ≤ ∙∙∙ ≤ ln
Initialize L0 = 0;
for i = 1 to n do
Li = Li-1 + li;
if Li ≤ (C + 1 - i) then
Set vi = 1;
else
Set vi = 0;
end if
end for
Theorem 3. (Scaled Bound) Suppose that base loss function l(u) ≥ 1 u < 0 , u ∈ R is an upper
bound of the 0-1 loss function. Let ui ∈ R be the classification margin of the ith sample for
i ∈ {1, ..., n}. Denote u = [u1, ..., un]. Define E (u) as follows:
nn
E (u) =v∈m{0i,n1}n max	i=1vil(ui),n-	i=1 vi .	(11)
Then J(u) ≤ 2E (u) ≤ 2J (u) holds true.
Remark: E(u) has similar properties to Q(u) discussed above. Moreover, it is tighter than Q(u),
i.e. E(u) ≤ Q(u). Thus, it is less sensitive to outliers compared with Q(u). However, Q(u) can
construct more adaptive curriculum by taking 0-1 loss into consideration during the training process.
Directly optimizing E(u) is not as efficient as that optimizing Q(u). We now present a batch loss
objective E(u) given as Eq.(12). E(u) is also a tighter upper bound of 0-1 loss objective J(u)
compared with conventional surrogate loss J (u).
Corollary 2. (Mini-batch Update for Scaled Bound) Suppose that base loss function l(u) ≥
1 u < 0 , u ∈ R is an upper bound of the 0-1 loss function. Let b, m be the number of batches
and batch size, respectively. Let uij ∈ R be the classification margin of the ith sample in batch j for
i ∈ {1, ..., m} andj ∈ {1, ..., b}. Denote u = [u11, ..., umb]. Let n = mb. Define E(u) as follows:
Eb (U)=Xj=I v∈min}m max ( Xm=1 Vijl(Uij ), m-Xm=1 Vij ) .	(12)
Then J(U) ≤ 2E (U) ≤ 2Eb (U) ≤ 2bJ (U) holds true.
All the curriculum losses defined above rely on minimizing a partial optimization problem (Eq.(13))
to find the selection index set v*. We now show that the optimization of V with given classification
margin ui ∈ R, i ∈ {1, ..., n} can be done in O(n log n).
Theorem 4. (Partial Optimization) Suppose that base loss function l(u) ≥ 1 u < 0 , u ∈ R is an
upper bound of the 0-1 loss function. For fixed ui ∈ R, i ∈ {1, ..., n}, an minimum solution v* of
the minimization problem in Eq. (13) can be achieved by Algorithm 1:
v∈m{0i,n1}nmax Pin=1 Vil(ui), C - Pin=1 Vi ,
(13)
where C is the threshold parameter such that 0 ≤ C ≤ 2n.
Remark: The time complexity of Algorithm 1 is O(n log n). Moreover, it does not involve complex
operations, and is very simple and efficient to compute.
Algorithm 1 can adaptively select samples for training. It has some useful properties to help us
better understand the objective after partial minimization, we present them in Proposition 1.
5
Published as a conference paper at ICLR 2020
Proposition 1. (Optimum of Partial Optimization) Suppose that base loss function l(u) ≥
1 u < 0 , u ∈ R is an upper bound of the 0-1 loss function. Let ui ∈ R for i ∈ {1, ..., n} be
fixed values. Without loss of generality, assume l(uι) ≤ l(u2)… ≤ l(un). Let v* be an optimum
solution ofthe partial optimization problem in Eq.(13). Let T* = PZi v* and LT* = PT=I I(Ui).
Then we have
LT* ≤ C + 1 - T*	(14)
LT*+1 > C-T*	(15)
LT*+1 > max(LT * , C - T*)	(16)
nn
min max	vi l(ui ), C -	vi = max(LT * , C - T*).	(17)
Remark: When C ≤ n + PZi 1(U < 0), Eq.(17) is tighter than the conventional loss J(u).
When C ≥ n, Eq. (17) is a scaled upper bound of 0-1 loss J(u) . From Eq.(17) , we know the
optimum of the partial optimization problem (13) (i.e. our objective) is max(LT * , C - T*). When
LT* ≥ C - T* , we can directly optimize LT* with the selected samples for training. When LT* <
C - T*, note that LT*+i > max(LT * , C - T*) from Eq.(16), we can optimize LT*+i for training.
Note that when T* < n, we have that LT*+i ≤ Ln = Pin=i l(ui ), which is still tighter than the
conventional loss Jb(u). When T* = n, for the parameter C ≤ n + Pin=i 1 ui < 0 , we have that
LT* = Jb(u) ≥ J(u) ≥ C - n = C - T*. Thus we can optimize max(LT * , C - T*) = Jb(u). In
practice, when training with random mini-batch, we find that optimizing LT* in both cases instead
of LT* +i does not make much influence.
2.3	Noise Pruned Curriculum Loss
The curriculum loss in Eq.(9) and Eq.(11) expect to minimize the upper bound of the 0-1 loss for all
the training samples. When model capability (complexity) is high, (deep network) model will still
attain small (zero) training loss and overfit to the noisy samples.
The ideal model is that it correctly classifies the clean training samples and misclassifies the noisy
samples with wrong labels. Suppose that the rate of noisy samples (by label corruption) is ∈ [0, 1].
The ideal model is to correctly classify the (1 - )n clean training samples, and misclassify the n
noisy training samples. This is because the label is corrupted. Correctly classify the training samples
with corrupted (wrong) label means that the model has already overfitted to noisy samples. This will
harm the generalization to the unseen data.
Considering all the above reasons, we thus propose the Noise Pruned Curriculum Loss (NPCL) as
L(U)= minιmaχ (En Ivil(Ui),C- En Ivi),	(18)
v∈{0,i}n	i=i	i=i
where C = (1 - e)n or C = (1 - e)2n +(1 - C) Pn=i 1(Ui < 0).
When we know there are n noisy samples in the training set, we can leverage this as our prior.
(The impact of misspecification of the prior is included in the supplement.) When C = (1 - C)n
(assume C, Cn are integers for simplicity), from the selection procedure in Algorithm 1, we know
Cn1 samples with largest losses l(U) will be pruned. This is because C - Pin=i vi + 1 ≤ 0 when
in=i vi ≥ (1 - c)n + 1. Without loss of generality, assume l(ui) ≤ l(u2) … ≤ I(Un). After
pruning, We have v(i-e)n+i =…=vn = 0, the pruned loss becomes
L (U)= min	max (X(	v vil(ui), (1 — c)n — X(	v vi).	(19)
v∈{0,i}(1-)n	i=i	i=i
It is the basic CL for (1 一 c)n samples and it is the upper bound of P(=-e)n 1 (Ui < 0). If we prune
more noisy samples than clean samples, it will reduce the noise ratio. Then the basic CL can handle.
Fortunately, this assumption is supported by the "memorization" effect in deep networks (Arpit et al.,
2017), i.e. deep networks tend to learn clean and easy pattern first. Thus, the loss of noisy or hard
1 When Pi(=1-1)n+1 l(ui) 6= 0, n samples will be pruned. Otherwise, n - 1 samples will be pruned.
6
Published as a conference paper at ICLR 2020
Algorithm 2 Training with Batch Noise Pruned Curriculum Loss
Input: Number of epochs N, batch size m, noise ratio e;
Output: The model parameter w;
Initialize model parameter w.
for k = 1 to N do
Shuffle training set D;
while Not fetch all the data from D do
Fetch a mini-batch Db from D;
Compute losses {li}im=1 for data in Db;
Compute the selection threshold C according to Eq.(21).
Compute selection index v* by Algorithm 1;
Update w = w - αVl (Dv*) w.r.t the subset Dv*
end while
^ 一一
of D selected by v*;
end for
data tend to remain high for a period (before being overfitted). Therefore, the pruned samples with
largest loss are more likely to be the noisy samples. After the rough pruning, the problem becomes
optimizing basic CL for the remaining samples as in Eq.(19). Note that our CL is a tight upper bound
approximation to the 0-1 loss, it preserves the robust property to some extent. Thus, it can handle
case with small noise rate. Specifically, our CL(Eq.19) further select samples from the remaining
samples for training adaptively according to the state of training process. This generally will further
reduce the noise ratio. Thus, we may expect our NPCL to be robust to noisy samples. Note that, all
the above can be done by the simple and efficient Algorithm 1 without explicit pruning samples in
a separated step. Namely, our loss can do all these automatically under a unified objective form in
Eq.(18).
When C = (1 - )n, the NPCL in Eq.(18) reduces to basic CL E(u) in Eq.(11) with = 0. When
C = (1 - )2n+ (1 -) Pin=1 1 ui < 0 , for an ideal target model (that misclassifies noisy samples
only), we know that E[C] = (1 - e)2n +(1 - E)EPn=I l(u <。)] = (1 - e)2n +(1 - E)En =
(1 - )n. It has similar properties as choosing C = (1 - )n. Moreover, it is more adaptive by
considering 0-1 loss during training at different stages. In this case, the NPCL in Eq.(18) reduces to
the CL Q(u) in Eq.(9) when E = 0. Note that C is a prior, users can defined it based on their domain
knowledge.
To leverage the benefit of deep learning, we present the batched NPCL as
b	mm
L(u) = j=1 v∈m{0i,n1}m max	i=1vijl(uij),Cj-	i=1 vij ,	(20)
where Cj = (1 - E)m or as in Eq.(21):
Cj = (1 -E)2m + (1 -E) ^Xi=ι 1(uj < 0).	QI)
Similar to Corollary 1, we know that L (u) ≤ L (u). Thus, optimizing the batched NPCL is indeed
minimizing the upper bound of NPCL. This enables us to train the model with mini-batch update,
which is very efficient for modern deep learning tools. The training procedure is summarized in
Algorithm 2. It uses Algorithm 1 to select a subset of samples from every mini-batch. Then, it uses
the selected samples to perform gradient update.
3 Empirical S tudy
3.1 Evaluation of Robustness against Label Corruption
We evaluate our NPCL by comparing Generalized Cross-Entropy (GCE) loss (Zhang & Sabuncu,
2018), Co-teaching (Han et al., 2018b), Co-teaching+ (Yu et al., 2019), MentorNet (Jiang et al.,
2018) and standard network training on MNIST, CIFAR10 and CIFAR100 dataset as in (Han et al.,
2018b; Patrini et al., 2017; Goldberger & Ben-Reuven, 2017). Two types of random label corrup-
tion, i.e. Symmetry flipping (Van Rooyen et al., 2015) and Pair flipping (Han et al., 2018a), are
7
Published as a conference paper at ICLR 2020
(b)
(a)
(c)
mnist pairflip 0.35
mnist_symmetric_0_5
mnist_sym metric_0.2
25	50	75	100	125	350	175	200
Epoch
(f) Symmetry-20%
(d) Pairflip-35%
Eooch
(e) Symmetry-50%
Figure 1: Test accuracy and label precision vs. number of epochs on MNIST dataset.
considered in this work. Symmetry flipping is that the corrupted label is uniformly assign to one
of K - 1 incorrect classes. Pair flipping is that the corrupted label is assign to one specific class
similar to the ground truth. The noise rate of label flipping is chosen from {20%, 50%, 35%} as
a representative. As a robust loss function, we further compare NPCL with GCE loss in detail with
noise rate in {0%, 10%, 20%, 30%, 40%, 50%}. We employ same network architecture and network
hyperparameters as in Co-teaching (Han et al., 2018b) for all the methods in comparison. Specif-
ically, the batch size and the number of epochs is set to m = 128 and N = 200, respectively. The
Adam optimizer with the same parameter as (Han et al., 2018b) is employed. The architecture of
neural network is presented in Appendix L. For NPCL, we employ hinge loss as the base upper
bound function of 0-1 loss. In the first few epochs, we train model using full batch with soft hinge
loss (in the supplement) as a burn-in period suggested in (Jiang et al., 2018). Specifically, we start
NPCL at 5th epoch on MNIST and 10th epoch on CIFAR10 and CIFAR100, respectively. For Co-
teaching (Han et al., 2018b) and MentorNet in (Jiang et al., 2018), we employ the open sourced code
of Co-teaching (Han et al., 2018b). For Co-teaching+ (Yu et al., 2019), we employ the code pro-
vided by the authors. We implement NPCL by Pytorch. For NPCL, Co-teaching and Co-teaching+,
we employ the true noise rate as parameter. Experiments are performed five independent runs. The
error bar for STD is shaded.
For performance measurements, we employ both test accuracy and label precision as in (Han et al.,
2018b). Label precision is defined as : number of clean samples / number of selected samples, which
measures the selection accuracy for sample selection based methods. A higher label precision in the
mini-batch after sample selection can lead to a update with less noisy samples, which means that
model suffers less influence of noisy samples and thus preforms more robustly to label corruption.
The pictures of test accuracy and label precision vs. number of epochs on MNIST are presented in
Figure 1. The results on CIFAR10 and CIFAR100 are shown in Figure 5 and Figure 6 in Appendix,
respectively. It shows that NCPL achieves superior performance compared with GCE loss in terms
of test accuracy. Particularly, NPCL obtains significant better performance compared with GCE loss
in hard cases: Symmetry-50% and Pair-flip-35%, which shows that NPCL is more robust to label
corruption compared with GCE loss. Moreover, NPCL obtains better performance on MNIST, and
competitive performance on CIFAR10 and CIFAR100 compared with Co-teaching. Furthermore,
NPCL achieves better performance than Co-teaching+ on CIFAR10 and two cases on MNIST. In
addition, we find that Co-teaching+ is not stable on CIFAR100 with 50% symmetric noise. Note
that NPCL is a simple plug-in for a single network, while Co-teaching/Co-teaching+ employs two
networks to train the model concurrently. Thus, both the space complexity and time complexity of
Co-teaching/Co-teaching+ is doubled compared with our NPCL.
8
Published as a conference paper at ICLR 2020
Both our NPCL and Generalized Cross Entropy (GCE) loss are robust loss functions as plug-in
for single network. Thus, we provide a more detailed comparison between our NPCL and GCE
loss with noise rate in {0%, 10%, 20%, 30%, 40%, 50%}. The experimental results on CIFAR10 are
presented in Figure 3. The experimental results on CIFAR100 and MNIST are provided in Figure 8
and Figure 7 in Appendix.From Figure 3, Figure 8 and Figure 7, we can observe that NPCL obtains
similar and higher test accuracy in all the cases. Moreover, from Figure 3 and Figure 7, we can
see that NPCL achieves similar test accuracy compared with the GCE loss when the noise rate is
small. The improvement increases with the increase of the noise rate. Particularly, NPCL obtains
remarkable improvement compared with the GCE loss on CIFAR10 with noise rate 50%. It shows
that NPCL is more robust compared with GCE loss against label corruption. GCE loss employs all
samples for training, while NPCL prunes the noisy samples adaptively. As a result, GCE loss still
employs samples with wrong labels for training, which misleads the model. Thus, NPCL obtains
better performance when the noise rate becomes large.
3.2 More experiments with different network architectures
We follow the experiments setup in (Lee et al., 2019). We use the online code of (Lee et al., 2019) ,
and only change the loss for comparison. We cite the numbers of Softmax, RoG and D2L (Ma et al.,
2018) in (Lee et al., 2019) for comparison.
The test accuracy results on uniform noise, semantic noise and open-set noise are shown in Table 1,
Table 2 and Table 3, respectively. From Table 1, we can observe that both NPCL and CL outperforms
Softmax (cross-entropy) and RoG (cross-entropy) on five cases for uniform noise. Note that RoG is
an ensemble method, while CL/NPCL is a single loss for network training, one can combine them
to boost the performance. From Table 2, we can see that CL obtains consistently better performance
than cross-entropy and D2L (Ma et al., 2018) for the semantic noise. Table 3 shows that NPCL
achieves competitive performance compared with RoG for open-set noise.
Table 1: Test accuracy(%) OfDenseNet on CIFAR10 and CIFAR100.
Noise type		 CIFAR10				CIFAR100			
	NPCL	CL	Softmax	RoG	NPCL	CL	Softmax	RoG
uniform (20%)	89.49	89.32	81.01	87.41	64.88	67.92	61.72	64.29
uniform (40%)	83.24	85.57	72.34	81.83	56.34	58.63	50.89	55.68
uniform (60%)	66.2	68.52	55.42	75.45	44.49	46.65	38.33	44.12
Table 2: Test accuracy(%) OfDenseNet on CIFAR10 and CIFAR100 With Semantic noise.
Dataset	Label generator (noise rate)	NPCL	CL	Cross-entropy	D2L
CIFAR10	DenSeNet(32%)	66.5	67.45	67:24	66.91
	ReSNet(38%)	61.88	62.88	6226	59.10
	VGG(34%)	68.37	69.61	68:77	57.97
CIFAR100	DenSeNet(34%)	57.59	55.14	5072	5.00
	ReSNet(37%)	54.49	53.20	50:68	23.71
	VGG(37%)	一	55.41	52.71	51.08 —	40.97
Table 3: Test accuracy(%) of DenseNet on CIFAR10 With open-set noise.			
Open-set Data	NPCL	Softmax	RoG
CIFAR100	82.85	79.01	83.37
ImageNet	87.95	86.88	87.05
CIFAR100-ImageNet	84.28	81.58	84.35
We further evaluate the performance of CL/NPCL on the Tiny-ImageNet dataset. We use the
ResNet18 netWork as the test-bed. For GCE loss, We employ the default hyper-parameter q = 0.7 in
all cases. All the methods are performed five runs With seeds {1, 2, 3, 4, 5}. The curve of mean test
accuracy (shaded in std) are provided in Figure 2. We can see that NPCL and CL obtain higher test
accuracy than generalized cross-entropy loss and stand cross-entropy loss on both cases. Note that
CL does not have parameters, it is much convenient to use.
9
Published as a conference paper at ICLR 2020
40302010
Aue-muu<-kəi
3025201510
AUe-nuu4 】SBH
(a) Symmetric-20%
(b) Symmetric-50%
dfarlO_sym metric_O
Figure 2: Test accuracy (%) on Tiny-ImageNet dataset with symmetric noise
(a) Symmetry-0%
(c) Symmetry-20%
(b) Symmetry-10%
(d) Symmetry-30%
(e) Symmetry-40%
(f) Symmetry-50%
Figure 3: Test accuracy vs. number of epochs on CIFAR10 dataset.
4 Conclusion and Further Work
In this work, we proposed a curriculum loss (CL) for robust learning. Theoretically, we analyzed the
properties of CL and proved that it is tighter upper bound of the 0-1 loss compared with conventional
summation based surrogate losses. We extended our CL to a more general form (NPCL) to handle
large rate of label corruption. Empirically, experimental results on benchmark datasets show the
robustness of the proposed loss. As a further work, we may improve our CL to handle imbalanced
distribution by considering diversity for each class. Moreover, it is interesting to investigate the
influence of different base loss functions in CL and NPCL.
Acknowledgement
We sincerely thank the reviewers for their insightful comments and suggestions. This paper was
supported by Australian Research Council grants DP180100106 and DP200101328.
10
Published as a conference paper at ICLR 2020
References
Devansh Arpit, StanisIaW Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxin-
der S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. A closer look
at memorization in deep networks. In ICML, pp. 233-242, 2017.
Peter L Bartlett, Michael I Jordan, and Jon D McAuliffe. Convexity, classification, and risk bounds.
Journal of the American Statistical Association, 101(473):138-156, 2006.
Yoshua Bengio, J6r6me Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In
ICML, pp. 41-48. ACM, 2009.
Jacob Goldberger and Ehud Ben-Reuven. Training deep neural-networks using a noise adaptation
layer. In ICLR, 2017.
Bo Han, Jiangchao Yao, Gang Niu, Mingyuan Zhou, Ivor Tsang, Ya Zhang, and Masashi Sugiyama.
Masking: A new perspective of noisy supervision. In Advances in Neural Information Processing
Systems, pp. 5836-5846, 2018a.
Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi
Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. In
Advances in Neural Information Processing Systems, pp. 8527-8537, 2018b.
Mengqiu Hu, Yang Yang, Fumin Shen, Luming Zhang, Heng Tao Shen, and Xuelong Li. Robust
web image annotation via exploring multi-facet and structural knowledge. IEEE Transactions on
Image Processing, 26(10):4871-4884, 2017.
Weihua Hu, Gang Niu, Issei Sato, and Masashi Sugiyama. Does distributionally robust supervised
learning give robust classifiers? 2018.
Lu Jiang, Deyu Meng, Shoou-I Yu, Zhenzhong Lan, Shiguang Shan, and Alexander Hauptmann.
Self-paced learning with diversity. In Advances in Neural Information Processing Systems, pp.
2078-2086, 2014.
Lu Jiang, Deyu Meng, Qian Zhao, Shiguang Shan, and Alexander G Hauptmann. Self-paced cur-
riculum learning. In Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015.
Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. Mentornet: Learning data-
driven curriculum for very deep neural networks on corrupted labels. 2018.
Ashish Khetan, Zachary C Lipton, and Anima Anandkumar. Learning from noisy singly-labeled
data. ICLR, 2018.
M Pawan Kumar, Benjamin Packer, and Daphne Koller. Self-paced learning for latent variable
models. In Advances in Neural Information Processing Systems, pp. 1189-1197, 2010.
Kimin Lee, Sukmin Yun, Kibok Lee, Honglak Lee, Bo Li, and Jinwoo Shin. Robust inference via
generative classifiers for handling noisy labels. In International Conference on Machine Learning,
2019.
Xingjun Ma, Yisen Wang, Michael E Houle, Shuo Zhou, Sarah M Erfani, Shu-Tao Xia, Sudanthi
Wijewickrema, and James Bailey. Dimensionality-driven learning with noisy labels. In Interna-
tional Conference on Machine Learning, 2018.
Hamed Masnadi-Shirazi and Nuno Vasconcelos. On the design of loss functions for classification:
theory, robustness to outliers, and savageboost. In Advances in neural information processing
systems, pp. 1049-1056, 2009.
Takeru Miyato, Andrew M Dai, and Ian Goodfellow. Virtual adversarial training for semi-supervised
text classification. In ICLR, 2016.
Robert Moore and John DeNero. L1 and l2 regularization for multiclass hinge loss models. In
Symposium on Machine Learning in Speech and Language Processing, 2011.
11
Published as a conference paper at ICLR 2020
Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and Lizhen Qu. Making
deep neural networks robust to label noise: A loss correction approach. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition ,pp.1944-1952, 2017.
Alexander J Ratner, Christopher M De Sa, Sen Wu, Daniel Selsam, and Christopher R6. Data
programming: Creating large training sets, quickly. In Advances in neural information processing
systems, pp. 3567-3575, 2016.
Hao Su, Jia Deng, and Li Fei-Fei. Crowdsourcing annotations for visual object detection. In Work-
shops at the Twenty-Sixth AAAI Conference on Artificial Intelligence, 2012.
Sainbayar Sukhbaatar, Joan Bruna, Manohar Paluri, Lubomir Bourdev, and Rob Fergus. Training
convolutional networks with noisy labels. arXiv preprint arXiv:1406.2080, 2014.
Daiki Tanaka, Daiki Ikami, Toshihiko Yamasaki, and Kiyoharu Aizawa. Joint optimization frame-
work for learning with noisy labels. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 5552-5560, 2018.
Brendan Van Rooyen, Aditya Menon, and Robert C Williamson. Learning with symmetric label
noise: The importance of being unhinged. In Advances in Neural Information Processing Systems,
pp. 10-18, 2015.
Yichao Wu and Yufeng Liu. Robust truncated hinge loss support vector machines. Journal of the
American Statistical Association, 102(479):974-983, 2007.
Xingrui Yu, Bo Han, Jiangchao Yao, Gang Niu, Ivor Tsang, and Masashi Sugiyama. How does dis-
agreement help generalization against label corruption? In International Conference on Machine
Learning, pp. 7164-7173, 2019.
Yuan Yuan, Yueming Lyu, Xi Shen, Ivor W. Tsang, and Dit-Yan Yeung. Marginalized average
attentional network for weakly-supervised learning. In ICLR, 2019.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. ICLR, 2017.
Zhilu Zhang and Mert Sabuncu. Generalized cross entropy loss for training deep neural networks
with noisy labels. In Advances in neural information processing systems, pp. 8778-8788, 2018.
12
Published as a conference paper at ICLR 2020
A Explanation of Theorem 1 for robust learning
Theorem. (Monotonic Relationship) ((Hu et al., 2018) Let p(x, y) and q(x, y) be the train-
ing and test density,respectively. Define r(x, y) = q(x, y)/p(x, y) and ri = r(xi, yi). Let
l(yb, y) = 1 sign(yb) 6= y and l(yb, y) = 1 argmaxk (ybk) 6= y be 0-1 loss for binary classifi-
cation and multi-class classification, respectively. Let f (∙) be convex with f (1) = 0. Define risk
R(θ), empirical risk R(θ), adversarial risk Radv (θ) and empirical adversarial risk Radv (θ) as
R(θ) = Ep(x,y) [l(gθ(x), y)]	(22)
1n
R(θ) = n y^i=ι l(gθ (xi),yi)	(23)
Radv(θ) = sup Ep(x,y) [r(x, y)l(gθ(x), y)]	(24)
r∈Uf
1n
Radv(θ) =SuP —〉J Jit(gθ(Xi),yi),	(25)
r∈Ubf n	i=1
where Uf	=	r(x, y) Ep(x,y)	[f	(r(x, y))] ≤	δ, Ep(x,y)	[r(x, y)] = 1, r(x, y) ≥ 0, ∀(x,	y) ∈ X × Y
andUf =	{r ∣ § Pn=ι f g	≤	δ, § Pin=1 ri	= 1, r ≥	0 }. Then we have that
If Radv (θl) <	1,	then R(θl)	< R(θ2)	O Radv (θl) < Radv (θ2).	(26)
If Radv(θι) =	1,	then R(θι)	≤ R(θ2)	Q⇒ Radv(θ2) = 1.	(27)
The same monotonic relationship holds between their empirical approximation: R(θ) and Radv.
Hu et al. (2018) show that minimizing (empirical) risk is equivalent to minimize the (empirical)
adversarial risk (worst-case risk) for 0-1 loss. Thus, we can directly optimize the risk instead of
the worst-case risk. Specifically, suppose we have an observable training distribution p(x, y). The
observable distribution p(x, y) may be corrupted from an underlying clean distribution q(x, y). We
train a model based on the training distribution p(x, y), and we want our model to perform well
on the clean distribution q(x, y). Since we do not know the clean distribution q(x, y), we want our
model to perform well for the worst-case estimate of the clean distribution, with the assumption
that the f -divergence between the corrupted distribution p and the clean distribution q is bounded
by δ. Note that the underlying clean distribution is fixed but unknown, given the corrupted training
distribution, the smallest δ that bounds the divergence between the corrupted distribution and clean
distribution measures the intrinsic difficulty of the corruption, and it is also fixed and unknown. The
corresponding worst-case distribution w.r.t the smallest δ is an estimate of the true clean distribution,
and this worst-case risk upper bounds the risk of the true clean distribution. In addition, this bound
is tighter than the other worst-case risks w.r.t larger δ. Formally, the upper bound w.r.t the smallest
δ is given as
G(θ) := suP Eq(x,y) [l(gθ(x), y)]	(28)
q∈Uef
where Uf is an equivalent constrainted set w.r.t Uf for q(x, y). Then, we have
G(θ) := suP Eq(x,y) [l(gθ(x), y)] = suP Ep(x,y) [r(x, y)l(gθ(x), y)]	(29)
q∈Uef	r∈Uf
5T1	7/ ∖ ♦ C < 1	Γ∙	EI	Λ	1	.1 . ... χ-v / /-i∖ ♦	∙ 1 . .	♦ ♦ ♦	Pζ / ∕λ∖
When l(∙) is0-1 loss, from Theorem 1, we know that minimize G(θ) is equivalent to minimize G(θ).
Thus, we can minimize G(θ) instead of G(θ).
Ge(θ) := Ep(x,y) [l(gθ(x), y)]	(30)
Minimize the Eq.(30) enables us to minimize the Eq.(28) without knowing the true divergence pa-
rameter δ beforehand. Usually, minimizing the upper bound can decrease the true risk under clean
distribution. Particularly, when the clean distribution coincides with the worst-case estimate w.r.t the
smallest δ, minimizing the risk under the corrupted training distribution leads to the same minimizer
as minimizing the risk under the clean distribution.
Relationship between label corruption and general corruption
13
Published as a conference paper at ICLR 2020
Label corruption is a special case of general corruption. Label corruption restricts the corruption in
the space Y instead of the space X × Y. That is to say, the training distribution p(x) is same as the
clean distribution q(x) over X. Then, we have the robust risk for label corruption as
Gy (θ) :=	sup Eq(x,y) [l(gθ(x), y)]
-Γ~; 一
q∈Uf ∩H
(31)
where H := {q(x, y) |q(x) = p(x), ∀(x, y) ∈ X × Y }. The supremum in Gy(θ) is taken over
Uf ∩ H, while the supremum in G(θ) is taken over Uf . Due to the additional constrain q(x) =
p(x), ∀(x, y) ∈ X × Y, we thus know that the robust risk Gy (θ) is bounded by G(θ), i.e., Gy (θ) ≤
G(θ). Moreover, it is more piratical and important to be robust for both label corruption and feature
corruption.
B Proof of Theorem 2
Proof. Because l(u < 0)≤ l(u), we have Pn=ι l(ui) ≥ Pn=ι 1(% < 0). Then
Q (U) = min max v∈{0,1}n	n vil(ui), n i=1	n -	i=1vi+	χn=ι i(υ	i <0))	(32)
≤ max (X^n.]	n l(ui), n-	i=1	1 + XnJ	ui < 0))		(33)
= max (Xn=I	l(ui)En,1(U i=1	i <0))			(34)
= Xin=1 l(ui)					(35)
n
Since loss J(u) =	i=1 l(ui), we obtain Q (u) ≤ J (u).
On the other hand, we have that
Q (U)= moinι}n max (E=Ivil(Ui),n-	in=1 vi +	in= 1(ui < 0))
v∈{0,1}n	i=1	i=1	i=1
≥ min n-,χ	vi +	1(u < 0)
v∈{0,1}n	i=1 i	i=1 i
=Xn=ι 1(Ui < H
(36)
(37)
Since J(U) = Pn=I Mui < 0), We obtain Q (U) ≥ J(U)
□
C Proof of Corollary 1
Proof. Since n = mb, similar to the proof of Q (U) ≤ J (U), we have
b	m	mm
Q(U)	= j=1 v∈m{0i,n1}m	max i=1 vijl(uij), m-	i=1 vij + i=11	uij	<0
b	m	mm
≤Σ j=1 max(Ei=Il(Uij ),m - ΣSi=11+ΣSi=11(uij < 0))	(38)
b	mm
= j=1 max i=1 l(uij), i=1 1 uij < 0	(39)
=Xjb=1 Xim=1 l(uij) = Jb(U)	(40)
14
Published as a conference paper at ICLR 2020
On the other hand, since the group (batch) separable sum structure, we have that
Q(U) =Xb=I v∈min}mmax (Xm=1 Vjl(Uj ),m-Xm=1 Vj + Xm=11(uj < 0))
b	m	mm
=v∈m{0i,n1}n	j=1max i=1Vijl(uij),m-	i=1 Vij + i=11 uij	<0	(41)
bm	bm	bm
≥ Vmn}n max (XX Vjl(Uj ),n - XX vij + XX 1(uj <0))	(42)
= Q (U) ≥ J (U)	(43)
□
D Proof of Partial Optimization Theorem (Theorem 4)
Proof. For simplicity, let li, = l(ui), i ∈ {1,…，n}. Without loss of generality, assume lι ≤ l2 … ≤
ln. Let V be the solution obtained by Algorithm 1. Assume there exits a V such that
nn	n	n
max ( P Vili,C - P Vi) <	max ( P ViIi,C	- P v*).	(44)
i=1	i=1	i=1	i=1
nn
Let T = P Vi and Ti = P Vii .
i=1	i=1
Case 1:	If T = Ti, then there exists an Vk = 1 and Vki = 0. From Algorithm 1, we know k > Ti
nn
(Vki = 0 ⇒ k > Ti) and lk ≥ lj, j ∈ {1, ..., Ti}. Then we know P Viili ≤ P Vili. Thus, we can
i=1	i=1
achieve that
nn	nn
max (^X Vili,C - ^X Vi) = max(^X Vili,C - ^X Vi)	(45)
i=1	i=1	i=1	i=1
nn
≤ max (E Vili,C - Evi).	(46)
i=1	i=1
This contradicts the assumption in Eq.(44)
τ *
Case 2:	If T > Ti, then there exists an Vk = 1 and Vk = 0. Let LT* = P li. Since lk ≥ 0, We
i=1
have LT* + lk ≥ LT* . From Algorithm 1, We knoW that LT* + lk > C - Ti . Thus We obtain that
nn
max	Vili , C -	Vi ≥ LT* + lk	(47)
≥ max (LT*, C — Ti)	(48)
nn
= max (XVili,C - XVi)	(49)
i=1	i=1
This contradicts the assumption in Eq.(44)
Case 3:	IfT < Ti, We obtain C - T ≥ C - Ti + 1. Then We can achieve that
nn
max (^X Vili,C — ^X Vi) = max (LT*, C — Ti)	(50)
i=1	i=1
≤ C + 1 - Ti	(51)
≤ C - T	(52)
Xn
i=1 Vi	(53)
nn
≤ max	Vili , C - Vi .	(54)
15
Published as a conference paper at ICLR 2020
This contradicts the assumption in Eq.(44).
Finally, We conclude that V obtained by Algorithm 1 is the minimum of the optimization problem
given in (13).	口
E	Proof of Proposition 1
n
Proof. Note that T* = P v*, from the condition of v* = 1 in Algorithm 1, we know that LT* ≤
i=1
C+ 1 - T*. From the condition of vk* = 0 in Algorithm 1, we know that LT*+1 > C -T*. Because
l(ui) ≥ 1 ui < 0 ≥ 0 for i ∈ {1, ..., n}, we have LT*+1 = LT* + l(uT*+1) ≥ LT* . Thus, we
obtain LT*+1 > max(LT * , C - T*). By substitute the optimum v* into the optimization function,
we obtain that
nn
v∈m{0i,n1}nmax	i=1 vil(ui), C -	i=1 vi
nn
= maχ(E Ml(Ui),C-E [v*)
i=1	i=1
= maχ(LT * , C - T*)
(55)
(56)
(57)
□
F Proof of Theorem 3
>Λ	C -I-V T r`	.1	.	1 ∙	/1 1 ∖ ∙	.∙	1 .	.1	.1	1	1 ∙	Gf ∖ ♦	1 -	∕c∖ AC, .) ∙
Proof. We first prove that objective (11) is tighter than the loss objective J(u) in Eq.(8). After this,
we prove that objective (11) is an upper bound of the 0/1 loss defined in equation (7).
For simplicity, let li = l(ui), we obtain that
	n E (u) = min maχ(	Vil(ui), n - v∈{0,1}n	i=1	n X Vi) i=1	(58)
	≤ maχ(	l(ui), (n -	1)) i=1	i=1 n		(59)
一	.	G，、 Note that J (u)	= X l(ui). i=1 n l(ui), thus, we have E (u) ≤ J (u). i=1		(60)
Without loss of	generality, assume lι ≤ l2 … ≤ ln. Let Li	i = P lj, T j=1	n P Vi* , where i=1
n
v* = [v*,v* …Vn]t is the optimum of V for fixed u. Let k = P 1(% ≥ 0). Then we achieve
i=1
that the 0/1 loss J(u) is as follows:
n
J(u) = X 1(ui < 0)=n - k.	(61)
i=1
From Algorithm 1 with C = n, we achieve that LT ≤ n - T + 1 and LT+1 > n - T.
Case 1:	If k ≥ T, we can achieve that
2E (u) - J(u) = 2 maχ(LT, n - T) - (n - k)	(62)
≥ 2(n-T) - (n-k)	(63)
=n+k-2T ≥ 0.
16
Published as a conference paper at ICLR 2020
Case 2:	If k < T, n - T ≥ LT , we can obtain that
2E (u) - J(u) = 2(n - T) - (n - k) = n + k - 2T.	(64)
Since k < T, if follows that
TT
LT = Lk+ X lj ≥ Lk+ X 1	(65)
j=k+1	j=k+1
= Lk + T - k	(66)
≥ T - k.	(67)
Together with n - T ≥ LT , we can obtain that
n - T ≥ LT ≥ T - k ⇒ n + k - 2T ≥ 0.	(68)
Thus, we can achieve that
2E (u) - J(u) = n+k- 2T ≥ 0.	(69)
Case 3:	If k < T, n - T < LT , we can obtain that
2E (u) - J(u) = 2 max(LT, n - T) - (n - k)	(70)
= 2LT - (n-k)	(71)
> (n - T) + LT - n+k.	(72)
From (67), we have LT ≥ T - k. Together with (72), it follows that
2E (u) -	J(u) > (n - T) + (T	- k)	- n+k ≥	0.	(73)
Finally, We can achieve that J(U)	≤ 2E (U) ≤ 2J (U) .	□
G Proof of Corollary 2
Proof. Since n = mb, similar to the proof of Q (U) ≤ J (U), We have
E (U) =Xb=1 v∈min}m max ( Xm=i Vjl(Uj )，m-XL Vi)
≤Xb=ι max (Xm=ιl(Uij),m-X：=1D	(74)
=Xb=Imax (Xm=Il(Ui), 0)	(75)
=Xjb=1 Xim=1 l(uij) = Jb(U)	(76)
On the other hand, since the group (batch) separable sum structure, We have that
E (U) =Xj=I v∈min}m max ( Xm=1 Vil(Ui ), m-XL Vi )
=v∈m⅛n Xb=ImaX (Xm=1 Vil(Ui)，m-Xm=1 Vi)	(77)
bm	bm
≥ V emj% n max(E E Vil(Ui ),n -E EVi)	(78)
v∈{ , }	i=1i=1	i=1i=1
= E (U)	(79)
_ 一 .一 一	一 .	一、 一一 ，、 一a,、 C,、
Together With Theorem 3, We obtain that J(U) ≤ 2E (U) ≤ 2E (U) ≤ 2J (U)
□
17
Published as a conference paper at ICLR 2020
QFARlOO-Ad am
0080604020
AUe.Inuu<
Epoch
(a) Training with Adam optimizer
8c604020
*uejnuu<
(b) Training with SGD optimizer
Figure 4: Training/Test accuracy for soft and hard hinge loss with different optimizer on CIFAR100
H Multi-Class Extension
For multi-class classification, denote the groudtruth label as y ∈ {1, ..., K}. Denote the classifica-
tion prediction (the last layer output of networks before loss function) as ti, i ∈ {1, ..., K}. Then,
the classification margin for multi-class classification can be defined as follows
u = ty - max ti .	(80)
We can see that 1 u < 0 = 1 ty - max ti < 0 is indeed the 0-1 loss for multi-class classification.
With the classification margin u, we can compute the base loss l(u) ≥ 1 u < 0 . In this work,
we employ the hinge loss. As we need the upper bound of 0-1 loss, the multi-class hard hinge loss
function Moore & DeNero (2011) can be defined as
H(t, y) = max(1 - u, 0) = max(1 - ty + maxti, 0).	(81)
i6=y
The multi-class hard hinge loss in Eq.(81) is not easy to optimize for deep networks. We propose a
novel soft multi-class hinge loss function as follows:
S(t, y) =
max(1 - ty + max ti , 0)
i6=y
max(1 - ty + LogSumExp(t), 0)
, ty - max ti ≥ 0
, ty - max ti < 0.
(82)
The soft hinge loss employs the LogSumExp function to approximate the max function when the
classification margin is less than zero, i.e., misclassification case. Intuitively, when the sample
is misclassified, it is far away from being correctly separate by a positive margin (e.g. margin
u ≥ 1). In this situation, a smooth loss function can help speed up gradient update. Because
LogSUmExp(t) > maxi∈{i,…k} t We know that the soft hinge loss is an upper bound of the hard
hinge loss, i.e., S(t, y) ≥ H(t, y) . Moreover, we can obtain a new weighted loss F(t, y; β) =
βS(t, y) + (1 - β)H(t, y), β ∈ [0, 1] that is also an upper bound of 0-1 loss.
I	Evaluation of Efficiency of the Proposed S oft- hinge Loss
We compare our soft multi-class hinge loss with hard multi-class hinge loss Moore & DeNero (2011)
on CIFAR100 dataset training with Adam and SGD optimizer, respectively. We keep both the net-
work architecture and hyperparameters same. We employ the default learning rate and momentums
of Adam optimizer in PyTorch toolbox, i.e. lr = 10-3, β1 = 0.9, β2 = 0.999. For SGD optimizer,
the learning rate (lr) and momentum (ρ) are set to lr = 10-2 and ρ = 0.9 respectively.
The pictures of training/test accuracy v.s number of epochs are presented in Figure 4. We can observe
that both the training accuracy and the test accuracy of our soft hinge loss increase greatly fast as the
number of epochs increase. In contrast, the training and test accuracy of hard hinge loss grow very
slowly. The training accuracy of soft hinge loss can arrive 100% trained with both optimizers. Both
18
Published as a conference paper at ICLR 2020
training and test accuracy of soft hinge loss are consistently better than hard hinge loss. In addition,
training accuracy of hard hinge loss can also reach 100% when SGD optimizer is used. However,
its test accuracy is lowever than that of soft hinge loss.
J IMPACT OF MIS SPECIFIED ESTIMATION OF NOISE RATE
We empirically analyze the impact of misspecified prior for the noise rate . The average test accu-
racy over last ten epochs on MNIST for different priors are reported in Table 4. We can observe that
NPCL is robust to misspecified prior for small noise cases (Symmetry-20%). Moreover, it becomes
a bit more sensitive on large noise case (Symmetry-50%) and on the pair flipping case (Pair-35%).
Table 4: Average test accuracy of NPCL with different E on MNIST over last ten epochs
Flipping Rate	0.5e	0.75e	E	1.25e	1.5e
Symmetry-20%	96.31% ± 0.17%	97.72% ± 0.09%	99.41% ± 0.01%	99.55% ± 0.02%	99.10% ± 0.04%
Symmetry-50%	78.67% ± 0.36%	87.36% ± 0.29%	98.53% ± 0.02%	97.92% ± 0.06%	67.61% ± 0.06%
Pair-35%	80.59% ± 0.40%~	87.86% ± 0.48%~	97.90% ± 0.04%	99.33% ± 0.02%	86.66% ± 0.08%~
Flipping-Rate
Symmetry-20%
Symmetry-50%
Pair-35%
Table 5: Average test accuracy on MNIST over the last ten epochs.
Standard
93.78% ± 0.04%
65.81% ± 0.14%
70.50% ± 0.16%
MentorNet
96.68% ± 0.05%
90.53% ± 0.07%
89.62% ± 0.15%
Co-teaching
97.14% ± 0.03%
91.35% ± 0.09%
90.96% ± 0.18%
Co-teaching+
99.41% ± 0.01%
97.79% ± 0.03%
93.81% ± 0.20%
GCE
99.40 ± 0.01%
92.48 ± 0.13%
72.26 ± 0.06%
NPCL
99.41% ± 0.01%
98.53% ± 0.02%
97.90% ± 0.04%
Table 6: Average test accuracy on CIFAR10 over the last ten epochs.
Flipping-Rate	Standard	MentorNet	Co-teaching	Co-teaching+	GCE	NPCL
Symmetry-20%	76.62% ± 0.07%	81.20% ± 0.09%	82.13% ± 0.08%	80.64% ± 0.15%	84.68% ± 0.05%	84.30% ± 0.07%
Symmetry-50%	49.92% ± 0.09%	72.09% ± 0.06%	74.28% ± 0.11%	58.43% ± 0.30%	61.80% ± 0.11%	77.66% ± 0.09%
Pair-35%	62.26% ± 0.09%	71.52% ± 0;06%~	77.77% ± 0J14%~	62.72% ± 0.23%	60.86% ± 0.05%	76.52% ± 0.11%
K Related Literature
Curriculum Learning: Curriculum learning is a general learning methodology that achieves suc-
cess in many area. The very beginning work of curriculum learning (Bengio et al., 2009) trains a
model gradually with samples ordered in a meaningful sequence, which has improved performance
on many problems. Since the curriculum in (Bengio et al., 2009) is predetermined by prior knowl-
edge and remained fixed later, which ignores the feedback of learners, Kumar et al. (Kumar et al.,
2010) further propose Self-paced learning that selects samples by alternative minimization of an
augmented objective. Jiang et al. (Jiang et al., 2014) propose a self-paced learning method to select
samples with diversity. After that, Jiang et al. (Jiang et al., 2015) propose a self-paced curricu-
lum strategy that takes different priors into consideration. Although these methods achieve success,
the relation between the augmented objective of self-paced learning and the original objective (e.g.
cross entropy loss for classification) is not clear. In addition, as stated in (Jiang et al., 2018), the
alternative update in self-paced learning is not efficient for training deep networks.
Learning with Noisy Labels: The most related works are the sample selection based methods
for robust learning. This kind of works are inspired by curriculum learning (Bengio et al., 2009).
Among them, Jiang et al. (Jiang et al., 2018) propose to learn the curriculum from data by a mentor
net. They use the mentor net to select samples for training with noisy labels. Co-teaching (Han et al.,
2018b) employs two networks to select samples to train each other and achieve good generalization
performance against large rate of label corruption. Co-teaching+ (Yu et al., 2019) extends Co-
teaching by selecting samples with disagreement of prediction of two networks. Compared with
Co-teaching/Co-teaching+, our CL is a simple plugin for a single network. Thus both space and
time complexity of CL are half of Co-teaching’s. Recently, Zhang & Sabuncu (2018) propose a
generalized Cross-entropy loss for robust learning.
Construction of tighter bounds of 0-1 loss: Along the line of construction of tighter bounds of
the 0-1 loss, many methods have been proposed. To name a few, Masnadi-Shirazi et al. (Masnadi-
Shirazi & Vasconcelos, 2009) propose Savage loss, which is a non-convex upper bound of the 0-1
19
Published as a conference paper at ICLR 2020
Flipping-Rate
Symmetry-20%
Symmetry-50%
Pair-35%
Table 7: Average test accuracy on CIFAR100 over the last ten epochs.
Standard
47.05% ± 0.11%
25.47% ± 0.07%
39.91% ± 0.11%
MentorNet
51.58% ± 0.15%
39.65% ± 0.10%
40.42% ± 0.07%
Co-teaching
53.89% ± 0.09%
41.08% ± 0.07%
43.36% ± 0.08%
Co-teaching+
56.15% ± 0.09%
37.88% ± 0.06%
40.88% ± 0.16%
GCE
51.86% ± 0.09%
37.60% ± 0.08%
36.64% ± 0.07%
NPCL
55.30% ± 0.09%
42.56% ± 0.06%
44.43% ± 0.15%
Aue.Inyq lsυl
(a)
(b)	(c)
Figure 5: Test accuracy and label precision vs. number of epochs on CIFAR10 dataset.
loss function. Bartlett et al. (Bartlett et al., 2006) analyze the properties of the truncated loss for
conventional convex loss. Wu et al. (Wu & Liu, 2007) study the truncated hinge loss for SVM.
Although the results are fruitful, these works are mainly focus on loss function at individual data
point, they do not have sample selection property. In contrast, our curriculum loss can automatically
select samples for training. Moreover, it can be constructed in a tighter way than these individual
losses by employing them as the base loss function.
L Architecture of Neural Networks
CNN on MNIST	CNN on CIFAR-10	CNN on CIFAR-100
28×28 Gray Image	32×32 RGB Image	32×32 RGB Image
3×3 conv, 128 LReLU
3×3 conv, 128 LReLU
3×3 conv, 128 LReLU
2×2 max-pool, stride 2
dropout, P = 0.25
3×3conv,256 LReLU
3×3 conv, 256 LReLU
3×3 conv, 256LReLU
2×2 max-pool, stride 2
dropout, P = 0.25
3 × 3 conv, 512 LReLU
3×3 conv, 256 LReLU
3×3 conv,128 LReLU
avg-pool
dense 128→10	I	dense 128→10	I	dense 128→100
20
Published as a conference paper at ICLR 2020
Epoch
(d) Symmetry-20%
(c)
dfarl00symmetric0.5
(b)
(e) Symmetry-50%
Epoch
(f) Pairflip-35%
Figure 6: Test accuracy and label precision vs. number of epochs on CIFAR100 dataset.
mnist_Symmet rijθ.l
mnist SymmetriC-O
mnist SymnletriC-0-2
(b) Symmetry-10%
(c) Symmetry-20%
Figure 7: Test accuracy vs. number of epochs on MNIST dataset.
Eooch
(e) Symmetry-40%
(a) Symmetry-0%
Eooch
(d) Symmetry-30%
Epoch
(f) Symmetry-50%
21
Published as a conference paper at ICLR 2020
(a) Symmetry-0%
----GeneralizedCrossEntropy
=1-----i-i-----i-Fj
IOO 125 ISO	175	200
Epoch
(b) Symmetry-10%
(c) Symmetry-20%
dfarlOO symmetric 0.3
4030覆10
>uro^≡uu<⅛aj.
Aue-Inuuq⅛3J.
(d) Symmetry-30%
(e) Symmetry-40%
Figure 8: Test accuracy vs. number of epochs on CIFAR100 dataset.
dfarl00_symmetric_0.5
(f) Symmetry-50%
22