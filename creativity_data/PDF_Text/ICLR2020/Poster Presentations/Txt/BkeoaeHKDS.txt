Published as a conference paper at ICLR 2020
Gradients as Features
for Deep Representation Learning
Fangzhou Mu, Yingyu Liang
Department of Computer Sciences
University of Wisconsin-Madison
{fmu, yliang,}@cs.wisc.edu
Yin Li
Departments of Biostatistics & Computer Sciences
University of Wisconsin-Madison
yin.li@wisc.edu
Ab stract
We address the challenging problem of deep representation learning - the effi-
cient adaption of a pre-trained deep network to different tasks. Specifically, we
propose to explore gradient-based features. These features are gradients of the
model parameters with respect to a task-specific loss given an input sample. Our
key innovation is the design of a linear model that incorporates both gradient and
activation of the pre-trained network. We demonstrate that our model provides
a local linear approximation to an underlying deep model, and discuss important
theoretical insights. Moreover, we present an efficient algorithm for the training
and inference of our model without computing the actual gradients. Our method is
evaluated across a number of representation-learning tasks on several datasets and
using different network architectures. Strong results are obtained in all settings,
and are well-aligned with our theoretical insights1.
1 Introduction
Despite tremendous success of deep learning, training deep neural networks requires a massive
amount of labeled data and computing resources. The recent development of representation learn-
ing holds great promises for improving data efficiency of training, and enables an easy adaption to
different tasks using the same feature representation. These features can be learned via either unsu-
pervised learning using deep generative models (Kingma & Welling, 2014; Dumoulin et al., 2016),
or self-supervised learning with “pretext” tasks and pseudo-labels (Noroozi & Favaro, 2016; Zhang
et al., 2016; Gidaris et al., 2018), or transfer learning from another large-scale dataset (Yosinski
et al., 2014; Oquab et al., 2014; Girshick et al., 2014). After learning, the per-sample activation of
the network is considered as generic features. By leveraging these features, simple classifiers such
as linear models can be learned for different tasks. However, given sufficient amount of training
data, the performance of representation-learning methods lags behind fully supervised deep models.
As a step towards bridging this gap, we propose to make use of gradient-based features from a pre-
trained network, i.e., gradients of the model parameters with respect to a task-specific loss given an
input sample. Our key intuition is that these per-sample gradients contain task-relevant discrimina-
tive information. More importantly, we design a novel linear model that accounts for both gradient-
and activation-based features. The design of our linear model stems from the recent advances in
the theoretical analysis of deep models. Specifically, our gradient-based features are inspired by the
neural tangent kernel (NTK) (Jacot et al., 2018; Lee et al., 2019; Arora et al., 2019b), and adapt
NTK in the setting of finite-width networks. Therefore, our model provides a local approximation
of fine-tuning an underlying deep model, and the accuracy of the approximation is controlled by
the semantic gap between the representation-learning and the target tasks. Finally, the structure of
the gradient features and the linear model allows us to derive an efficient and scalable algorithm for
training and inference.
To evaluate our method, we focus on visual representation learning in this paper, although our model
can be easily modified for natural language or speech data. To this end, we consider a number of
learning tasks in vision, including unsupervised, self-supervised and transfer learning. Our method
1Project webpage at http://pages.cs.wisc.edu/' fmu∕gradfeat20
1
Published as a conference paper at ICLR 2020
is evaluated across tasks, datasets and network architectures and compared against a set of baseline
methods. We observe empirically that our model with the gradient features outperforms the tradi-
tional activation-based logistic regressor by a significant margin in all settings. Moreover, our model
compares favorably against fine-tuning of network parameters.
Our main contributions are thus summarized as follows.
•	We propose a novel representation-learning method. At the core of our method lies in a linear
model that builds on gradients of model parameters as the feature representation.
•	From a theoretical perspective, we claim that our linear model provides a local approximation
of fine-tuning an underlying deep model. From a practical perspective, we devise an efficient
and scalable algorithm for the training and inference of our method.
•	We demonstrate strong results of our method across various representation-learning tasks, dif-
ferent network architectures and several datasets. Furthermore, these empirical results are well-
aligned with our theoretical insight.
2	Related Work
Representation Learning. Learning good representation of data without expensive supervision
remains a challenging problem. Representation learning using deep models has been recently ex-
plored. For example, different types of deep latent variable models (Kingma & Welling, 2014;
Higgins et al., 2017; Berthelot et al., 2019; Dumoulin et al., 2016; Donahue et al., 2016; Dinh
et al., 2017; Kingma & Dhariwal, 2018; Grathwohl et al., 2019) were considered for representa-
tion learning. These models were designed to fit to the distribution of data, yet their intermediate
responses were found useful for discriminative tasks. Another example is self-supervised learning.
This paradigm seeks to learn from a discriminative pretext task whose supervision comes almost for
free. These pretext tasks for images include predicting rotation angles (Gidaris et al., 2018), solv-
ing jigsaw puzzles (Noroozi & Favaro, 2016) and colorizing grayscale images (Zhang et al., 2016).
Finally, the idea of transfer learning hinges on the assumption that features learned from a large
and generic dataset can be shared across closely related tasks and datasets (Girshick et al., 2014;
Sharif Razavian et al., 2014; Oquab et al., 2014). The most successful models for transfer learning
so far are those pre-trained on the ImageNet classification task (Yosinski et al., 2014).
As opposed to proposing new representation-learning tasks, our work studies how to get the most
out of the existing tasks. Hence, our method is broadly applicable - it offers a generic framework
that can be readily combined with any representation-learning paradigm.
Gradient of Deep Networks. Our method makes use of the Jacobian matrix of a deep network
as feature representation for a downstream task. Gradient information is traditionally employed for
visualizing and interpreting convolutional networks (Simonyan et al., 2013), and more recently for
generating adversarial samples (Szegedy et al., 2013), crafting defense strategies (Goodfellow et al.,
2015), facilitating knowledge distillation (Sinha et al., 2018; Srinivas & Fleuret, 2018), and boosting
multi-task and meta learning (Sinha et al., 2018; Achille et al., 2019).
Our work draws inspiration from Fisher vectors (FVs) (Jaakkola & Haussler, 1999) - gradient-based
features from a probabilistic model (e.g., mixture of Gaussians). The FV pipeline has demonstrated
its success for visual recognition based on hand-crafted local descriptors (Perronnin & Dance, 2007).
More recently, FVs have shown promising results with deep models, first as an ingredient of a hybrid
system (Perronnin & Larlus, 2015), and then as task embeddings for meta-learning (Achille et al.,
2019). Our method differs from the FV approaches in two folds. First, it is not built around a prob-
abilistic model, hence has distinct theoretical motivation as we describe later. Second, our method
enjoys exact gradient computation with respect to network parameters and allows scalable training,
whereas Achille et al. (2019) employs heuristics in their method to aggressively approximate the
computation of FVs.
Another line of relevant research is from Zinkevich et al. (2017). They also explored similar gra-
dient features of deep networks, which they call holographic features, by drawing inspiration from
generalized linear models (GLM). They showed that a linear model trained on the gradient features
can faithfully recover the output of the original network for the same task, and also showed some
other desired properties of gradient features. In contrast, our work is motivated by the NTK theory,
2
Published as a conference paper at ICLR 2020
+#
(Linear weight)
uθ1B>B4
!#%"?())
(Gradient-based features)
w2
s6∙M,!ee∙~l

+
×
×
Figure 1: (a) An illustration of our parameterization. We consider a deep network F (x; θ, ω) ,
ωT fθ (x) that consists of a ConvNet fθ with its parameters θ , (θ1, θ2) and linear weights ω. (b)
An overview of our proposed model. Our model takes the activation fθ (x) and the gradient.% (x)
as input (see illustration in (a)), and learns linear weights w1 (matrix) and w2 (vector) for prediction.
and we study the more challenging problem of adapting the features for a different task. It would
be interesting to investigate the connection between GLM and NTK.
Neural Tangent Kernel (NTK) for Wide Networks. Jacot et al. (2018) established the connection
between deep networks and kernel methods by introducing the neural tangent kernel (NTK). Lee
et al. (2019) further showed that a network evolves as a linear model in the infinite width limit
when trained on certain losses under gradient descent. Similar ideas have been used to analyze wide
networks (see, e.g., Arora et al. (2019b;a); Li & Liang (2018); Allen-Zhu et al. (2019a); Du et al.
(2019); Allen-Zhu et al. (2019b); Cao & Gu (2020); Mei et al. (2019)). Our method is, to our best
knowledge, the first attempt to materialize the theory in the regime of practical networks. Instead of
assuming random initialization of network parameters as all the prior works do, we for the first time
empirically evaluate the implication of pre-training on the linear approximation theory.
3	Gradient-based Features for Representation Learning
We start by introducing the setting of representation learning using deep models. Consider a feed-
forward deep neural network Fθ,ω (x) , ωTfθ(x) that consists of a backbone fθ(x) with its vector-
ized parameters θ and a linear model defined by ω (italic for vectors and bold for matrix). Specif-
ically, fθ encodes the input x into a vector representation fθ(x) ∈ Rd. ω ∈ Rd×c are thus linear
weights that map a feature vector into c output dimensions. For this work, we focus on convolutional
networks (ConvNets) for classification tasks. With trivial modifications, our method can easily ex-
tend beyond ConvNets and classification, e.g., for a recurrent network as the backbone and/or for a
regression task.
Following the setting of representation learning, We assume that a pre-trained fθ is given with θ
as the learned parameters. The term representation learning refers to a set of learning methods that
do not make use of discriminative signals from the task of interest. For example, f can be the
encoder of a deep generative model (Kingma & Welling, 2014; Dumoulin et al., 2016; Donahue
et al., 2016), or a ConvNet learned by using proxy tasks (self-supervised learning) (Goyal et al.,
2019; Kolesnikov et al., 2019) or from another large-scale labeled dataset such as ImageNet (Deng
et al., 2009). Given a target task, it is common practice to regard % as a fixed feature extractor
(activation-based features) and train a set of linear weights, given by
gω (X)= ω T fs(χ),	(1)
We omit the bias term for clarity. Note that ω and θ are instantiations of ω and θ, where ω is the
solution of the linear model and θ is given by representation learning. Based on this setup, we now
describe our method, discuss the theoretical implications and present an efficient training scheme.
3.1	Method Outline
Our method assumes a partition of θ , (θ1, θ2), where θ1 and θ2 parameterize the bottom and
top layers of the ConvNet f (see Figure 1(a) for an illustration). Importantly, we propose to use
gradient-based features ▽&Fθ,ω (x) = ωT J⑥ (x) in addition to activation-based features fθ(x).
Specifically, J% (x) ∈ Rd×lθ2l is the Jacobian matrix of fθ with respect to the pre-trained parameter
3
Published as a conference paper at ICLR 2020
& from the top layers of f. Given the features (fθ(x), ω T J% (x)) for x, our linear model g, hereby
considered as a classifier for concreteness, takes the form
gwι ,W2 (x) = WT fθ(x) + ωT Js2 (x)w2 = gwι(x) + ωT Jθ2 (x)w2,	(2)
where wɪ ∈ Rd×c are linear classifiers initialized from ω, w2 ∈ Rlθ2l are shared linear weights for
gradient features, and ∣Θ21 is the size of the parameter θ2. Both w1 and w2 are our model parameters
that need to be learned from a target task. An overview of the model is shown in Figure 1(b).
Our model subsumes the linear model in Eq. (1) as the first term, and includes a second term that is
linear in the gradient-based features. We note that this extra linear term is different from standard
linear classifiers as in Eq. (1). In this case, the gradient-based features form a matrix and the weight
vector w2 is multiplied to each row of the feature matrix. Therefore, w2 is shared for all output di-
mensions. Similar to standard linear classifiers, the output of g is further normalized by the softmax
function and trained with the cross-entropy loss using labeled data from the target dataset.
Conceptually, our method can be summarized into three steps.
•	Pre-train the ConvNet f@. This is accomplished by substituting in any existing representation-
learning algorithm. Examples include unsupervised, self-supervised and transfer learning.
•	Train linear classifiers ω using fθ(x). This is a standard step in representation learning by
fitting a linear model using “generic features”.
•	Learn the linear model gw1,w2 (x). A linear model in the form of Eq. (2) is learned using
gradient- and activation-based features. Note that the features are obtained when θ = G is kept
fixed, hence our method requires no extra tuning ofthe parameter θ.
3.2	Theoretical Insight
The key insight is that our model provides a local linear approximation to Fθ2,ω (x). This
approximation comes from Eq. (2) 一 the crux of our approach. Importantly, our linear model is
mathematically well motivated - it can be interpreted as the first-order Taylor expansion of Fθ,ω
with respect to its parameters (θ2, ω) around the point of (θ2, ω). More formally, We note that
Fθ,ω (x) ≈ ωT fθ(x) + ωT Jθ2 (x)(θ2 - θ2) + (ω - ω)T fθ(x)
=ωT fθ(x) + ωT Jθ2 (x)(θ2 - θ2)	⑶
=gω,θ2-θ2 (x).
With ω = w1and θ2 -θθ2 = w2, Eq. (2) provides a linear approximation of the deep model Fθ2,ω(x)
around the initialization (&, ω). Our key intuition is that given a sufficiently strong base network,
training our model approximates fine-tuning Fθ2 ,ω.
The quality of the linear approximation can be theoretically analyzed via the recent neural tangent
kernel approach (Jacot et al., 2018; Lee et al., 2019; Arora et al., 2019b) or some related ideas (Arora
et al., 2019a; Li & Liang, 2018; Allen-Zhu et al., 2019a; Du et al., 2019; Allen-Zhu et al., 2019b;
Cao & Gu, 2020; Mei et al., 2019) when the base network Fθ,ω is sufficiently wide and at random
initialization. Unlike prior works, we apply the linear approximation on pre-trained networks of
practical sizes. We argue that such an approximation is useful in practice for the following reasons:
•	The pre-trained network provides a strong starting point. Thus, the pre-trained network
parameter θθ is close to a good solution for the downstream task. The key to good linear ap-
proximation is that the network output is stable with respect to small changes in the network
parameters. The pre-trained base network also has such stability properties, which are sup-
ported by empirical observations. For example, the pre-trained network has similar predictions
for a significant fraction of data in the downstream task as a fine-tuned network.
•	The network width required for the linearization to hold decreases as data becomes more
structured. An assumption made in existing analysis is that the network is sufficiently or even
infinitely wide compared to the size of the dataset, so that the approximation can hold for any
dataset. We argue that this is not necessary in practice, since the practical datasets are well-
structured, and theoretically it has been shown that as long as the trained network is sufficiently
4
Published as a conference paper at ICLR 2020
wide compared to the effective complexity determined by the structure of the data, then the
approximation can hold (Li & Liang, 2018; Allen-Zhu et al., 2019a). Our approach thus takes
advantage of the bottom layers to reduce data complexity in the hope that linearization of the
top (and often the widest) layers can be sufficiently accurate.
3.3	Scalable Training
Moving beyond the theoretical aspects, a practical challenge of our method is the high cost of com-
PUting g during training and inference. A naive approach requires evaluating and storing ωT J% (x)
for all x. This is computationally expensive and can become infeasible as the number of output di-
mensions C and the parameter size ∣Θ21 grow. Inspired by Pearlmutter (1994), we design an efficient
training and inference scheme for g. Thanks to this scheme, the complexity of training our model
using gradient features is of the same magnitude as training a linear classifier on network activation.
Central to our scalable approach is the inexpensive evaluation of the Jacobian-vector product (JVP)
ωT Jθ2 (x)w2, whose size is the same as c. First, We note that
Fθ+rw2,ω (x) = Fθ,ω (x) + rωT jθ2 (x)w2 + o(r2)	(4)
by first-order Taylor expansion around a scalar r = 0. Rearrange and take the limit ofr to 0, we get
—T t_ / ʌ _ ]. Fθ+rw2 ,ω)(X)- Fθ,ω(x)	∂ I	/、
ω J& (X)W2 = r→0 ------------：—r--------：— = ∂ LOF⅛+rw2,ω (X),	(5)
which can be conveniently evaluated via forward-mode automatic differentiation.
More precisely, let us consider the basic building block of f - convolutional layers. These layers are
defined as a linear function h(zc; wc, bc) = wcT zc + bc, where wc and bc, which are part of θ, are
the weight and bias respectively, and zc is the layer input. We denote the counterparts of wc and bc
in w2 as Wc and bc, i.e., Wc and bc are the linear weights applied to Wc and bc. It can be shown that
∂h(zc; wc + rwc,bc + rbc)	~ K - Mdzc
------------ 	=h(zc; Wc,bc) + h(ɪ; wc, 0),	(6)
∂r-------------------------------------------------------------------∂r
where 争 is the JVP coming from the upstream layer.
When a nonlinearity is encountered, we have, using the ReLU function as an example,
∂ReLU(zc)	∂zc
—dr -=而 θ 1zc≥0
(7)
where θ is the element-wise product, 1 is the element-wise indicator function and zc is the layer
input. Other nonlinearities as well as pooling layers can be handled in the same spirit. For batch
normalization, we fold them into their preceding convolutions.
Importantly, Eqs. (6) and (7) provide an efficient approach to evaluating the desired JVP in Eq. (5)
by successively evaluating a set of JVPs on the fly. This chain of evaluations starts with the seed
∂0 = 0, where zo is the output of the section of f parameterized by θι and can be pre-computed.
ωT Jθ2 (x)w2 can be computed along with the standard forward propagation through f. Moreover,
during the training of g, its parameters wι and w2 can be learned via standard back-propagation. In
summary, our approach only requires a single forward pass through the fixed f for evaluating g, and
a single backward pass for updating the parameters W1 and w2.
Complexity Analysis. We further discuss the complexity of our method in training and inference,
and contrast our method to the fine-tuning of network parameters θ2. Our forward pass, as demon-
strated by Eqs. (6) and (7), is a chain of linear operations intertwined by element-wise multipli-
cations. The second term in Eq. (6) forms the “main stream” of computation, while the first term
merges into the main stream at every layer of the ConvNet f . The same reasoning holds for the
backward pass. Overall, our method requires twice as many linear operations as fine-tuning θ2 of
the ConvNet. Note, however, that half of the linear operations by our method are slightly cheaper
due to the absence of the bias term. Moreover, in the special case where θ2 only includes the topmost
layer of f, our method carries out the same number of operations as fine-tuning since the second
term in Eq. (6) can be dropped. For memory consumption, our method requires to store an addi-
tional “copy” (linear weights) of the model parameters compared to fine-tuning. As the size of θ2
is typically small, this minor increase of computing and memory cost puts our method on the same
page as fine-tuning.
5
Published as a conference paper at ICLR 2020
4	Experiments
Our experimental results are organized into two parts. We first perform ablation studies to under-
stand the representation power of the gradient features. Next, we evaluate our method on three
representation-learning tasks: learning deep generative models, self-supervised learning using a
pretext task, and transfer learning from ImageNet.
We report results on several datasets and using different base networks to demonstrate the strength
of our method. In all cases, our method achieves significant better performance than the standard
logistic regressor on network activation. In the most interesting scenario where the pre-training
task is similar to the target task, our method achieves comparable or even better performance than
fine-tuning. Before we present our results, we summarize our implementation details.
Implementation Details. We adopt the NTK parametrization (Jacot et al., 2018) for θ2 and fold
batch normalization into their preceding convolutional layers prior to training our model. For the
SVHN and CIFAR-10/100 experiments, We train the models for 80K iterations with initial learning
rate 1e-3, halved every 20K iterations. For the VOC07 and COCO2014 experiments, we train the
models for 50 epochs with initial learning rate 1e-3, halved every 20 epochs. All models are trained
with the Adam optimizer (Kingma & Ba, 2015) with batch size 64, β1 = 0.5, β2 = 0.999 and
weight decay 1e-6. In addition to Adam, we also use the SGD optimizer with weight decay 5e-5,
momentum 0.9 and the same learning rate schedule for fine-tuning, and we report the better result
between the two runs.
4.1	Ablation study
We conduct ablation studies to address the following two questions: Does pre-training encourage
more powerful gradient features? What is the optimal size of the gradient features? We answer
the first question by comparing gradient features derived from various combinations of random and
pre-trained network parameters, and the second by varying the number of layers that contribute to
the gradient features.
Baselines. We compare our linear model in Eq. (2), referred to as the full model in the sequel,
against two baselines. The first baseline gw1 (x), or the activation model, is the first term in the full
model. It is a logistic regressor on network activation and is widely used for representation learning
in practice. The second baseline ωT J& (x)w2, or the gradient model, is the second term in the full
model. It is linear in the gradient features and can serve as a linear classifier on its own. Moreover,
we compare our method against fine-tuning in order to assess the validity of our theoretical insight.
Settings. For ablation study, we consider both unsupervised- and supervised-learning settings and
report classification accuracy. In the unsupervised setting (Table 1), we transfer features learned
from a deep generative model for image classification. Specifically, we train a BiGAN on CIFAR-
10 (Krizhevsky et al., 2009) and use its encoder as the base network for classification of the same
dataset. In the supervised setting (Table 2), we consider fine-tuning ImageNet pre-trained models
on a different classification tasks. Concretely, we use the PyTorch (Paszke et al., 2017) distribution
of ImageNet pre-trained ResNet18 (He et al., 2016) as the base network for VOC07 (Everingham
et al., 2010) object classification. For both settings, activation features are always output from the
pre-trained base network. In contrast, gradient features are evaluated with respect to all possible
combinations of random and pre-trained network parameters. In this way, we ensure that variation
in performance of the full model can be solely attributed to the gradient features.
Results and Conclusions. Our results are shown in Table 1 (unsupervised) and Table 2 (supervised).
We summarize our main conclusions from the ablation study.
•	The discriminative power of the gradient features is a consequence of pre-training. In
particular, the full model’s performance gain is not a consequence of an increase in parameter
size. The full model supplied with random gradients is no better than the activation baseline.
•	Gradient from the topmost layer of a pre-trained network suffices to ensure a reasonable
performance gain. Further inflating the gradient features from bottom layers has diminishing
returns, introducing little accuracy gain and sometimes hurting the performance. The results in
Zinkevich et al. (2017) also suggest that gradient features from top layers are the most effective
in terms of approximating the underlying network.
6
Published as a conference paper at ICLR 2020
Table 1: Ablation study in the unsupervised setting. We train a BiGAN on CIFAR-10 and use
its encoder as the base network for classification of the same dataset. All results are reported using
classification accuracy. We obtain the pre-trained & and a from the pre-trained base network, and
ω by first training the activation baseline for the target task. The specifications of θι, θ2 and ω only
affects gradient evaluation. Activation features are always output from the pre-trained base network.
	θ2: conv5 grad full	θ2: conv4-5 grad full	θ2 conv3-5 grad full
random	random ω Random	θ	pre-trained ω	23.09^^62.83 40.23	63.11	26.30~62.83 43.36 63.42	27.52~62.85 44.36 63.38
θι	pre-trained random ω θ2	pre-trained ω	-32.50^^62.98 44.07 63.99	^20^^6∏0^ 45.87 64.21	ɪɪ3^^63TI0^ 48.88	64.51
random	random ω Pre-trained	θ	pre-trained ω	53.39^^63.74 59.98	65.46	56.24^^64.30 61.47 65.38	57.20^^64.60 59.89 64.73
θι	pre-trained random ω θ2	pre-trained ω	^028^^69.84 70.14 70.51	^08^^70.IF 70.89 70.78	^60^^70:64- 71.55 71.37
activation		62.87	
fine-tuning	71.78 —	73.18 —	74.30 —
Table 2: Ablation study in the supervised setting. We use the PyTorch distribution of ImageNet
pre-trained ResNet18 as the base network for VOC07 object classification. All results are reported
using mean average precision (mAP). Predictions are based on a single center crop at test time.
			θ2: layer4block2		θ2: layer4block1-2	
			grad	full	grad	full
	random	random ω	15.63	82.84	17.96	82.85
Random	θ2	pre-trained ω	17.43	82.79	19.90	82.82
θ1	pre-trained	random ω	18.15	82.85	20.15	82.21
	θθ2	pre-trained ω	20.78	82.76	23.57	82.73
	random	random ω	62.35	82.84	69.34	82.82
Pre-trained	θ2	pre-trained ω	65.75	82.84	71.73	82.86
θθ1	pre-trained	random ω	80.74	83.15	80.30	82.97
	θθ2	pre-trained ω	83.05	83.50	83.12	83.40
activation				82.65				
fine-tuning			82.97		82.50	
•	Our results improve as the dataset and base network grow in complexity. Our method
always outperforms the activation baseline by a significant margin when the gradient features
are derived from pre-trained parameters. Moreover, our method consistently beats the gradi-
ent baseline and even fine-tuning on the more challenging VOC07 dataset when using a more
complicated residual network.
Remarks. We obtain the pre-trained θ= (θ1,θ2) from representation learning, and the pre-trained
ω by first training the standard logistic regressor for the target task. We follow the same training
procedure in later experiments. According to the ablation study, a good heuristic for our method is to
use the gradients from the topmost convolutional layer (or residual block) of a pre-trained network.
A more principled strategy for selecting gradient-contributing layers is left for future work.
4.2	Results on Representation Learning
We now present results on three different representation-learning tasks: unsupervised learning using
generative modeling, self-supervised learning using pretext tasks and transfer learning from pre-
trained models. For all experiments in this section, we contrast our proposed linear classifier (i.e.,
the full model in Eq. (2)) with the activation and gradient baselines as well as fine-tuning θ2 for the
7
Published as a conference paper at ICLR 2020
Table 3: Unsupervised Learning Results. We consider two base networks, namely the encoder of
a BiGAN and that of a VAE, trained on three datasets. Our target task is image classification on the
same datasets. All results are reported using classification accuracy.
	SVHN	CIFAR-10	CIFAR-100
activation	82.04	62.87	34.30
BiGAN	gradient	89.91	70.14	38.99
电：conv5)	full	89.96	70.51	39.37
fine-tuning	^911	7178	41.05
activation	81.95	52.05	29.20
VAE	gradient	91.44	61.47	35.10
(θ2: conv8)	full	90.90	61.16	34.83
fine-tuning	936	—	65.16	—	37.86
Table 4: Self-supervised and Transfer Learning Results. We consider a ResNet50 pre-trained on
the jigsaw pretext task, and a ResNet18 pre-trained on the ImageNet classification task. Our target
task is VOC07 and COCO2014 object classification. All results are reported using mean average
precision (mAP). Predictions are averaged over ten random crops at test time.
		VOC07	COCO2014
	activation	57.83	42.10
Self-supervised	gradient	57.73	41.02
(Jigsaw)	full	61.70	45.62
	fine-tuning	67.88	—	526	―
Self-supervised (Colorization)	activation	52.30	n/a
	activation	83.59	62.98
Transfer	gradient	84.63	66.33
(ImageNet)	full	84.95	66.45
	fine-tuning	84.14	—	63.43	—
target task. Following our conclusion from the ablation study, we make use of gradient features only
from the topmost layer (or residual block) of a pre-trained base network.
Unsupervised Learning. Similar to our ablation study, we consider unsupervised representation
learning using deep generative models. We train both BiGAN and VAE, and use their encoders as
the pre-trained ConvNet f . Our BiGAN and VAE models follow the architecture and training setup
from Dumoulin et al. (2016) and Berthelot et al. (2019). We average-pool the ConvNet output for
activation features, and obtain gradient features from the topmost convolutional layer. We train on
the train split of CIFAR-10/100 and the extra split of SVHN, and report classification accuracy
on their test splits.
Unsupervised Learning Results. Our results are summarized in Table 3. Our model consistently
improves the activation baseline by over 10% on all three datasets. Moreover, we observe good
agreement of performance between our model and fine-tuning. Finally, we notice that our model
does not have significant advantage over the gradient baseline. We think this is because the datasets
and base networks used here are so simple that the gradient features alone can be equally effective.
Self-supervised Learning. Moving beyond unsupervised learning, we also consider self-supervised
setting, where the representation is learned via a pretext task and pseudo-labels. In this case, we
consider a ResNet50 pre-trained on the jigsaw self-supervising task on ImageNet provided by Goyal
et al. (2019). See Noroozi & Favaro (2016) for details on the jigsaw task. We average-pool the
ConvNet output for activation features, and use gradient features from the last residual block. We
train on the trainval split of VOC07 and the train split of COCO2014 for object classification,
and report the mean average precision (mAP) scores on their respective test and val splits.
8
Published as a conference paper at ICLR 2020
Self-supervised Learning Results. Our results are summarized in the top part of Table 4. We
again observe a significant improvement of up to 4.5% over the two baselines on both datasets. Our
method also outperforms a competitive self-supervised learning method (Colorization) (Zhang et al.,
2016) using the activation from the same backbone network (ResNet50) and pre-trained on the same
dataset (ImageNet). We note that our method still lags behind fine-tuning in this setting with a large
gap (-6% on VOC07 and -7% on COCO2014).
Transfer Learning. Finally, we consider the most widely used transfer learning setting, where
the representation is learned by pre-training on a large scale labeled dataset, e.g., ImageNet. We
experiment with an ImageNet pre-trained ResNet18 from PyTorch. We follow the same setting as
self-supervised learning and report results on VOC07 and COCO2014 datasets.
Transfer Learning Results. Our results are presented in the bottom part of Table 4. Not surpris-
ingly, our results consistently outperforms the two baselines across both datasets. However, the gap
between our method and gradient baseline is much smaller. To our surprise, our method outperforms
fine-tuning on both datasets by a large margin. In comparison to fine-tuning, our method has a gain
of +0.8% and +3.0% on VOC07 and the larger COCO2014 dataset.
4.3	Further Discussions
We provide further discussions of our results. Specifically, we analyze the performance gap between
out method and fine-tuning under three different settings. We then compare our method with learning
a linear classifier only using gradient features. Moreover, we contrast our method with the recent
effort of learning infinitely wide networks using exact NTK (Arora et al., 2019b).
The performance gap between our method and fine-tuning. We argue that this gap is controlled
by the degree of semantic overlap between the representation-learning and target tasks. The jigsaw
pretext task, though proven effective for learning features, is still semantically distant to an image
classification task, and hence results in the large performance gap we observe in the self-supervised
setting. On the other hand, it is reasonable to expect classifiers for datasets with overlapping cate-
gories to share common semantics. This may explain why our method outperforms fine-tuning in
the transfer-learning experiments, where the source and target datasets are conceptually close.
Comparing our method with training a linear classifier on gradient features alone. In general,
both methods are considerably more powerful than the standard activation-based logistic regressor.
In the unsupervised setting, the gradient model performs on par with, or even better than the full
model, which consumes both activation and gradient features (see Table 3). This raises the question
of whether network activations are redundant in the presence of gradient features. We argue that this
is not necessarily the case. In fact, as we demonstrate in the self-supervised and transfer-learning
experiments (see Table 4), combining activation and gradient features is a formula for delivering
the best performance on challenging representation-learning tasks (jigsaw pretext task) and datasets
(VOC07 and COCO2014). It would be interesting to further characterize the “representation bias”
induced by the two types of features.
Contrasting our method with learning infinitely wide networks from scratch. Arora et al.
(2019b) recently proposed exact NTK computation algorithms for learning infinitely wide Con-
vNets, and proved that such networks are equivalent to kernel regression using NTK. They obtained
66% accuracy on CIFAR-10 classification using a vanilla network and achieved an extra 11% boost
after including customized network modules. Their approach, at its current stage, only supports
full-batch learning and is limited to single output dimension. Our method, though not directly com-
parable with theirs, does not suffer from these limitations and delivers competitive results (e.g.,
we obtain 70.5% accuracy on CIFAR-10 using a four-layer BiGAN encoder as the base network,
see Table 3). Hence, out work highlights another promising direction to capitalize on the insights
generated by the theoretical analysis of NTK.
5	Conclusion
We presented a novel method for deep representation learning. Specifically, given a pre-trained deep
network, we explored as features the per-sample gradients of the network parameters relative to a
task-specific loss, and constructed a linear model that combines network activation with the gradient
9
Published as a conference paper at ICLR 2020
features. We showed that our model can be very efficient in training and inference, and may be
understood as a local linear approximation to an underlying deep model by an appeal to the neural-
tangent-kernel (NTK) theory. We empirically demonstrated that the gradient features are highly
discriminative for downstream tasks, and our method can significantly improve over the baseline
method of representation learning across pre-training tasks, network architectures and datasets. We
believe that our work offers a novel perspective to improving deep representation learning. Future
research directions include using NTK as a distance metric for measuring sample similarity in low-
shot classification and assessing model similarity in knowledge distillation.
Acknowledgement: This work was supported in part by FA9550-18-1-0166. The authors would
also like to acknowledge the support provided by the University of Wisconsin-Madison Office of
the Vice Chancellor for Research and Graduate Education with funding from the Wisconsin Alumni
Research Foundation.
References
Alessandro Achille, Michael Lam, Rahul Tewari, Avinash Ravichandran, Subhransu Maji, Charless
Fowlkes, Stefano Soatto, and Pietro Perona. Task2vec: Task embedding for meta-learning. In
ICCV, 2019.
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameter-
ized neural networks, going beyond two layers. In NeurIPS, 2019a.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In ICML, 2019b.
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of opti-
mization and generalization for overparameterized two-layer neural networks. In ICML, 2019a.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. On
exact computation with an infinitely wide neural net. In NeurIPS, 2019b.
David Berthelot, Colin Raffel, Aurko Roy, and Ian Goodfellow. Understanding and improving
interpolation in autoencoders via an adversarial regularizer. In ICLR, 2019.
Yuan Cao and Quanquan Gu. Generalization error bounds of gradient descent for learning over-
parameterized deep relu networks. In AAAI, 2020.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In CVPR, 2009.
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. In
ICLR, 2017.
JeffDonahue, PhiliPP Krahenbuhl, and Trevor Darrell. Adversarial feature learning. In ICLR, 2016.
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deeP neural networks. In ICML, 2019.
Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Olivier MastroPietro, Alex Lamb, Martin Ar-
jovsky, and Aaron Courville. Adversarially learned inference. In ICLR, 2016.
Mark Everingham, Luc Van Gool, ChristoPher KI Williams, John Winn, and Andrew Zisserman.
The pascal visual object classes (VOC) challenge. IJCV,, 88(2):303-338, 2010.
SPyros Gidaris, Praveer Singh, and Nikos Komodakis. UnsuPervised rePresentation learning by
Predicting image rotations. In ICLR, 2018.
Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accu-
rate object detection and semantic segmentation. In CVPR, 2014.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. ExPlaining and harnessing adversarial
examPles. In ICLR, 2015.
10
Published as a conference paper at ICLR 2020
Priya Goyal, Dhruv Mahajan, Abhinav Gupta, and Ishan Misra. Scaling and benchmarking self-
supervised visual representation learning. In ICCV, 2019.
Will Grathwohl, Ricky TQ Chen, Jesse Betterncourt, Ilya Sutskever, and David Duvenaud. Ffjord:
Free-form continuous dynamics for scalable reversible generative models. In ICLR, 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In CVPR, 2016.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. beta-VAE: Learning basic visual concepts with a
constrained variational framework. In ICLR, 2017.
Tommi Jaakkola and David Haussler. Exploiting generative models in discriminative classifiers. In
NeurIPS, 1999.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In NeurIPS, 2018.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2014.
Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. In
NeurIPS, 2018.
Alexander Kolesnikov, Xiaohua Zhai, and Lucas Beyer. Revisiting self-supervised visual represen-
tation learning. In CVPR, 2019.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
Technical report, 2009.
Jaehoon Lee, Lechao Xiao, Samuel S Schoenholz, Yasaman Bahri, Jascha Sohl-Dickstein, and Jef-
frey Pennington. Wide neural networks of any depth evolve as linear models under gradient
descent. In NeurIPS, 2019.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. In NeurIPS, 2018.
Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Mean-field theory of two-layers neural
networks: dimension-free bounds and kernel limit. In COLT, 2019.
Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw
puzzles. In ECCV, 2016.
Maxime Oquab, Leon Bottou, Ivan Laptev, and Josef Sivic. Learning and transferring mid-level
image representations using convolutional neural networks. In CVPR, 2014.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
pytorch. In NeurIPS, 2017.
Barak A Pearlmutter. Fast exact multiplication by the hessian. Neural computation, 6(1):147-160,
1994.
Florent Perronnin and Christopher Dance. Fisher kernels on visual vocabularies for image catego-
rization. In CVPR, 2007.
Florent Perronnin and Diane Larlus. Fisher vectors meet neural networks: A hybrid classification
architecture. In CVPR, 2015.
Ali Sharif Razavian, Hossein Azizpour, Josephine Sullivan, and Stefan Carlsson. Cnn features off-
the-shelf: an astounding baseline for recognition. In CVPR Workshops, 2014.
11
Published as a conference paper at ICLR 2020
Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: Vi-
sualising image classification models and saliency maps. arXiv preprint arXiv:1312.6034, 2013.
Ayan Sinha, Zhao Chen, Vijay Badrinarayanan, and Andrew Rabinovich. Gradient adversarial train-
ing of neural networks. arXiv preprint arXiv:1806.08028, 2018.
Suraj Srinivas and Francois Fleuret. Knowledge transfer with Jacobian matching. In ICML, 2018.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep
neural networks? In NeurIPS, 2014.
Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In ECCV, 2016.
Martin A Zinkevich, Alex Davies, and Dale Schuurmans. Holographic feature representations of
deep networks. In UAI, 2017.
12