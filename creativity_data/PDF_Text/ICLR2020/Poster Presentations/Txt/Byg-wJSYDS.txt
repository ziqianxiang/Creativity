Published as a conference paper at ICLR 2020
Discrepancy Ratio: Evaluating Model Perfor-
mance When Even Experts Disagree on the
Truth
Igor Lovchinsky Alon Daks Israel Malkin Pouya Samangouei Ardavan Saeedi
ilovchinsky	adaks	imalkin	psamangouei	asaeedi
Yang Liu	Swami Sankaranarayanan
yliu	ssankaranarayanan
Tomer Gafner
tgafner
Benjamin Sternlieb
bsternlieb
Patrick Maher
pmaher
Nathan Silberman
nsilberman
Butterfly Network
@butterflynetwork.com
Ab stract
In most machine learning tasks unambiguous ground truth labels can easily be
acquired. However, this luxury is often not afforded to many high-stakes, real-
world scenarios such as medical image interpretation, where even expert human
annotators typically exhibit very high levels of disagreement with one another.
While prior works have focused on overcoming noisy labels during training, the
question of how to evaluate models when annotators disagree about ground truth
has remained largely unexplored. To address this, we propose the discrepancy
ratio: a novel, task-independent and principled framework for validating machine
learning models in the presence of high label noise. Conceptually, our approach
evaluates a model by comparing its predictions to those of human annotators,
taking into account the degree to which annotators disagree with one another.
While our approach is entirely general, we show that in the special case of binary
classification, our proposed metric can be evaluated in terms of simple, closed-
form expressions that depend only on aggregate statistics of the labels and not on
any individual label. Finally, we demonstrate how this framework can be used
effectively to validate machine learning models using two real-world tasks from
medical imaging. The discrepancy ratio metric reveals what conventional metrics
do not: that our models not only vastly exceed the average human performance,
but even exceed the performance of the best human experts in our datasets.
1	Introduction
The canonical supervised machine learning paradigm assumes the presence of both inputs and their
corresponding, unambiguous outputs. In practice, the vast majority of datasets exhibit some degree
of label noise. Acknowledgement of this fact has resulted in the development of algorithms tailored
to training models in the presence of noisy ground truth labels (Sukhbaatar et al., 2015; Frenay &
Kaban, 2014; Zhu & Wu, 2004). These approaches include cleansing noisy labels before training
begins (Hodge & Austin, 2004), making the training process robust to noise (Patrini et al., 2017;
Vahdat, 2017; Rolnick et al., 2017) and actively modeling label noise (Sukhbaatar et al., 2015; Joseph
et al., 1995). These methods almost exclusively address only the problem of noisy labels during
training and tend to assume that at least some noise-free ground truth is available at test time in
order to properly evaluate different approaches. Indeed, without an unambiguous yard-stick, how can
competing schemes possibly be evaluated in a standardized manner?
There exist many domains, such as medical imaging (Rosenkrantz et al., 2013; Lazarus et al., 2006)
and machine translation (Wang et al., 2018), in which access to unambiguous ground truth, even
1
Published as a conference paper at ICLR 2020
at evaluation time, is not practically achievable. In these domains not only do annotators disagree
with each other, but they often even disagree with themselves (Gulshan et al., 2016; Hansegard et al.,
2009). This, however, does not imply that the annotators are not trustworthy, but rather that these
tasks are difficult even for domain experts. Datasets in these regimes are, in practice, quite different
from those typically used in machine learning research (which are usually geared towards content that
can be easily curated by lay people). As a consequence, studies utilizing public data often resort to
simulating label noise. Indeed, a theoretical understanding of evaluation in the presence of ambiguous
test data has remained largely unexplored.
In this work, we propose a new framework for evaluating the performance of machine learning
models in the presence of high label noise. Our main contributions are as follows:
1.	We propose a novel, task-independent metric, the discrepancy ratio, that allows us to a)
quantify model performance, taking into account how much humans disagree with each
other, and b) set a precise threshold at which the model performance can be said to exceed
the average human performance.
2.	We perform a detailed analysis of the discrepancy ratio for the special case of binary
classification and demonstrate that it can be evaluated in terms of simple, closed-form
expressions that depend only on aggregate statistics (mean and variance) of the annotations
and not on any individual annotation.
3.	We demonstrate the effectiveness of our approach on models that we trained on two important
medical imaging tasks with real-world clinical data and show that it reveals that our models
not only significantly exceed the average human performance, but even beat the best human
experts in our datasets.
2	Related Work
There are several well-known strategies for dealing with ambiguous or noisy evaluation labels. The
most popular approach is to simply use a single annotator (Jafari et al., 2019; Rajchl et al., 2017;
Cobzas et al., 2007; Roth et al., 2018). An approach taken by many popular, large-scale datasets,
such as ImageNet (Deng et al., 2009), Places (Zhou et al., 2014), COCO (Lin et al., 2014) and
Open Images V4 (Kuznetsova et al., 2018), is to take the average or majority vote. These efforts
typically employ crowdsourcing in some fashion, since the labeling of this data does not require
expert-level knowledge. While majority vote reduces label noise, in domains that require specialized
knowledge it is often not practical to get enough experts that the noise is removed to the point where
the annotations can be assumed to be “correct” (in the sense that MNIST annotations are assumed
to be correct). A related approach obtains a ground-truth dataset by enforcing a resolution criterion
on labels where annotators disagree. This criterion can, for example, involve a live session where
annotators deliberate and come to a consensus or an adjudication process where a third party (usually
a more senior, expert annotator) is tasked with resolving disagreements (Krause et al., 2018; Hannun
et al., 2019; Rajpurkar et al., 2018b). This approach may be effective at removing erroneous labels
(reducing variance) at the expense of annotator independence (increasing bias).
The techniques outlined above fall short in providing a unified method that can both a) be compared
across different studies/tasks and b) answer the essential question of how the model performance
relates to an average human annotator (i.e. how a prototypical human currently performs at a task).
In developing high-stakes, real-world machine learning systems, the goal of evaluating a model is
often to determine whether performance is safely substitutable for individual human behavior rather
than comparing to an idealized oracle.
Several studies have compared model performance with inter-annotator variability. Some have
visualized model performance and inter-annotator variability (e.g. via ROC curves (Krause et al.,
2018; Gulshan et al., 2016) or Bland-Altman plots (Hansegard et al., 2009; Altman & Bland, 1983))
without providing an explicit metric. Several works have constructed a metric by comparing the
model to the average annotator performance, though each has done so in a completely different way
(Wei et al., 2019; Hannun et al., 2019; Rajpurkar et al., 2018a;b; 2017). These metrics, however,
generally depend on the number of annotators used in the study and/or require that all annotators
label the entire dataset (see Section 4 and Appendix C for more details). Moreover, the metrics are
task-specific and cannot be interpreted outside the context of a particular study.
2
Published as a conference paper at ICLR 2020
3	The Discrepancy Ratio Metric
In this section, we define the discrepancy ratio and show that it has numerous desirable properties
that overcome the limitations of prior art detailed in Section 2.
3.1	Definitions
Definition 1. For any two arbitrary sets of labels Y = {y1, ...yn} and Y0 = {y01, ...y0n0}, the mean
pairwise deviation (MPD) is defined by:
ψ(γ, Y 0)=XX	,	⑴
y∈Yy0∈Y0
where δ can be chosen to correspond to any measure of agreement appropriate to the task in question
(e.g. squared error, absolute error, Jaccard Index (Jaccard, 1912), Cohen’s kappa (Cohen, 1960)).
Consider a dataset of N samples where we have a prediction mi , for each sample i, by the model we
are trying to evaluate. Further assume that each sample is labeled by at least two annotators and that
each annotator may label the same sample multiple times. Formally, let Yji = {yji k } be the set of
annotations provided by annotator j on sample i where k indexes the annotation number. Finally, let
Ai indicate the number of annotators that have annotated sample i.
Definition 2. We define the annotator discrepancy, which quantifies the average annotator disagree-
ment, to be the average of the MPD over all annotator pairs and all samples in the dataset:
N	Ai
α({Yj}) = NX Ai(AL I) X ψ(γj, Yjo).	⑵
i=1	j,j0=1
j06=j
Without access to unambiguous ground-truth labels, a natural way to define human performance
is by comparing an annotator’s labels to those provided by other humans on the same samples.
Consequently, we use the annotator discrepancy as a measure of average human performance.
Definition 3. Similarly, the model discrepancy is defined to be
N	Ai
μ(mi,{Yj}) = N X Ai Xψ(mi,Yj).	⑶
i=1	j=1
Definition 4. We define the discrepancy ratio to be the ratio of the model and annotator discrepancies:
△= μ.	(4)
α
Intuitively, this metric is the ratio of the average model-annotator disagreement and the annotator-
annotator disagreement. For example, a discrepancy ratio of 0.8 would mean that the model is
20% closer to the annotators than they are to each other. When △ is above (below) 1, the model
performance can be said to be worse (better) than the average human performance. Since the measure
of agreement δ is arbitrary, the discrepancy ratio can be thought of as a “meta-metric” that reports
how the model performs (relative to humans) with respect to that particular measure of agreement.
The dependence of the discrepancy ratio on the choice of δ is further explored in Section 4.2.2.
3.2	Properties
The discrepancy ratio metric, as formulated above, has several desirable properties:
Dimensionless: Dimensionless quantity whose meaning is invariant across different task categories
(regression, classification, segmentation etc.) and whose value is interpretable and highly intuitive.
Minimal dataset coverage requirements: The discrepancy ratio does not require that all annotators
label all samples (see Fig. 2 and Appendix C), which is often not practical in real-world situations,
3
Published as a conference paper at ICLR 2020
Sample Index	Model	Annotator 1	Annotator 2	...	Annotator A
1	0	0,1	0, 0	...	1,0,1
2	1	-	1,1	...	0
... N	... 0	... 1	... 1,1,1	... ...	... 1,1,1,1,1
Table 1: An Example Dataset for a Binary Classification Task.
but only that each sample is labeled by at least two annotators (two annotators are needed since it is
meaningless to measure the annotator disagreement with only a single annotator).
Insensitive to number of labels per-sample: The discrepancy ratio is expressed in terms of averages
over all model/annotation pairs and therefore does not depend on how many annotators are used (see
Fig. 2), nor how many times each annotator labels each image. This property makes it convenient to
compare the discrepancy ratio across studies.
Since the discrepancy ratio is formulated as a relative error that the model makes over that of the
annotators, it can be naturally incorporated as a test statistic in a non-inferiority framework (Ahn
et al., 2013), often used in medical research. The goal of non-inferiority testing is to show that a new
method is not statistically worse than an existing standard, up to some chosen equivalence bound. For
the discrepancy ratio, we can choose the equivalence bound to correspond to an acceptable fraction of
the error that the model is allowed to make on top of the one that annotators already make (e.g. setting
the equivalence bound to be 0.1 can be interpreted as saying that the model can make an error no
more than 10% larger than the ones humans typically make). Additional properties of the discrepancy
ratio, as well as comparisons to alternative formulations, are discussed in detail in Appendix C and E.
3.3	An Illustrative Example
In this section, we compute expressions for the model and annotator discrepancies on a generic binary
classification task using the squared error as an agreement measure. This calculation will serve to
build intuition for these metrics and can be used as a blueprint for evaluating them on other tasks.
Consider a dataset (see Table 1) containing N samples, where each sample is given a binary score
by a model and a set of A annotators (an annotator may label an image more than once, or not at
all). We assume all samples are labeled by at least two annotators. In the analysis below we adopt
the convention that yjik denotes the kth label provided by the jth annotator on the ith sample, and
removing an index denotes averaging over the values of that index. For example, yji is the average of
the labels provided by the jth annotator on the ith sample and y is the average annotation over all
labels, annotators and samples. The order of the averages should be clear from the context.
As shown in Appendix B, given two sets of binary labels Y = {y1, y2, ..., yn} and Y0 =
{y1 ,y2,…,Un，} the MPD is given by the expression y + y0 - 2yy0, where y = ⅛ Pn=I yk. Using
this result and Eq. 2, the annotator discrepancy for a single sample is then given by (Appendix B):
2σ2i
αi = 2yi(1 - yi) + Ar-I.	(5)
where σy2i is the variance of the labels yi and Ai is the number of annotators that have labeled sample
i. For the important special case where each annotator provided no more than a single label per
image, this expression further simplifies to
(6)
where we see that annotator discrepancy is simply proportional to the variance of the labels. Similarly
we can write the single-sample model discrepancy as
Ai
μi =	A1- Xmi +	yj-	2miyii	=	mi	+ yi	-	2miyi.	⑺
j=1
Averaging this result over all samples, the model discrepancy becomes
4
Published as a conference paper at ICLR 2020
μ = (m + y — 2my) — 2Cov[mi, yi],	(8)
where the covariance is taken over the samples i and m and y correspond to the model predictions
and annotator labels averaged over the dataset. The expression in parentheses in Eq.8 is invariant
under permutations of the model predictions mi and annotations yi . It can be interpreted as the
best baseline performance that can be expected when the model and annotators are simply guessing.
The covariance term represents a correction to this baseline performance that takes into account the
correlation of the predictions and labels (i.e. when the model and annotators provide correlated labels,
the performance is improved). The expressions in Eqs. 5-7 only depend on aggregate statistics (i.e.
mean and variance) of the labels and not on any particular annotation and can be used, for example,
as metrics during model training.
4	Experiments
We illustrate the ideas discussed in Section 3 on three datasets. First, we use MNIST (LeCun et al.,
2010) with simulated models and annotators to objectively compare different strategies for evaluating
model performance in the presence of noisy test data. Next, we train and evaluate models on two
real-world, medical image datasets, which exhibit high degrees of expert annotator disagreement, and
evaluate them using our framework.
4.1	MNIST Digit Classification
For our MNIST experiments, we use the standard train/test splits and original (10-class) labels. Rather
than train models, we synthetically generate them by corrupting the ground truth labels. Each model
is designated as model X where X designates the percentage of the ground truth which the model
preserves. For example, model 1.0 leaves the ground truth as is whereas model 0.8 chooses 20% of
the labels randomly from amongst the other 9 classes.
We explicitly differentiate between true labels and observed labels, which are noisy and may not
match the true labels. Our goal is to understand how different evaluation strategies fare in the presence
of label noise, e.g. when we do not have direct access to the true labels. To this end, we evaluate
several strategies: Naive Evaluation, Majority Vote, Relative F1 and our Discrepancy Ratio metric.
(a)ɪ,o
Naive Evaluation
>o≡⊃uora p3Λj3sqo
model 1.0
model 0.9
model 0.8
A°婚」nu°婚 poΛJ3sqo
----model 1.0
----model 0.9
----model 0.8
°,0.0	0.2	0.4	0.6	0.8	1.0
label swap probability
0.2	0.4	0.6	0.8
label swap probability
(b)	Majority Vote
1.0
Relative FI	(d)
---model 0.9
10	15	20	25	30	35
number of annotators
0.200
0.175-
0.150
0.125-
0.100
0.075-
0.050
0.025
Relative Fl
τu∙
single annotator strictness
Figure 1: Performance of baseline metrics on the MNIST test set. Curves show mean value and 2-
standard deviation bounds obtained from multiple simulations of the experiment. (a) Naive Evaluation
exhibits linear decay of observed accuracy but cannot compare model to inter-annotator variability
(b) Majority Vote is more invariant to label noise but also does not compare model to inter-annotator
variability. Panel (c) shows that the relative F1 score depends sensitively on the number of annotators.
Here, the number of annotators refers to the total number used in the analysis (6 annotators are used
for the hold-out set while the number used in the ground truth set is varied). Panel (d) shows that the
relative F1 score (where we used 6 annotators in the ground-truth set and 3 hold-out annotators) also
gets skewed when annotators are allowed to voluntarily skip images (or equivalently not all annotators
get images of equivalent difficulty). For panels (c) and (d) we have fixed the swap probability at 0.3.
4.1.1	Naive Evaluation
In this regime, we obtain each observed label by either selecting the true label (probability 1 - p)
or random sampling (probability p). As Fig. 1a illustrates, the degree to which model performance
is underestimated increases monotonically and linearly with the label swap probability. The Naive
Evaluation strategy is limited in its reliance on one label per sample. It has no way to evaluate the
degree of label noise (a priori) nor a mechanism to compare model to annotator performance.
5
Published as a conference paper at ICLR 2020
4.1.2	Majority Vote
In this regime, we assume we have 9 different annotators, each of whom selects the true label with
probability p or randomly samples a label with probability 1 - p. The final observed label is obtained
by taking the mode of the annotators’ labels. Fig. 1b shows the effect of this aggregation on the
observed accuracy. Majority Vote has an advantage over Naive Evaluation in that the narrowing
gaps between models as the label noise is increased can, to some extent, be recovered by averaging
multiple labels. Additionally, one can use the entropy of the label distribution to get a sense of
inter-annotator variability. However, although Naive Evaluation and Majority Vote are sufficient for
the task of ranking models (deciding which one is better), these approach do not indicate whether the
model performance is better or worse than the average human performance.
4.1.3	Relative F1
We additionally consider the method used in (Rajpurkar et al., 2018b; Hannun et al., 2019; Rajpurkar
et al., 2018a). Here, a model and a set of held-out annotators are compared (using the F1 score) to a
gold-standard ground truth set obtained using the majority-vote of multiple annotators. The model F1
score is then compared to the average F1 score of all the annotators in the hold-out set. As can be
seen in Fig. 1c, the difference in the model and average annotator F1 scores, which we refer to as
Relative F1, is dependent on the number of annotators used in the ground truth set (see Appendix C
for intuition about this effect).
The Relative F1 score metric has another major shortcoming. As we discuss in Appendix C, in many
applications annotators must be allowed to voluntarily skip samples when they feel that there is
not enough information to provide an accurate label. Consequently, as shown in Fig. 1d a single
annotator can bias the relative F1 score by simply annotating only the easier samples (on which there
is high agreement among annotators) and skipping the hard ones (on which the agreement is low).
Here we assigned to each image in the MNIST test set its own swap probability p, which ranges from
lower values (i.e. images on which there is high agreement) to high values (i.e. images on which
there is high disagreement). We find that as we increase the single annotator strictness (the fraction
of the dataset they skip), the relative F1 score is correspondingly reduced. It should be noted that this
same exact bias can arise when comparing the model and all annotators pairwise (as in the methods
used in (Wei et al., 2019; Rajpurkar et al., 2017)) rather than to the majority vote.
4.1.4	Discrepancy Ratio
The discrepancy ratio introduces a number of improvements over the baselines. First, unlike Naive
Evaluation and Majority Vote, the discrepancy ratio explicitly compares the model performance to the
inter-annotator variability. As illustrated in Figs. 2a and 2b, when the swap probability is close to 0,
the annotators are in almost perfect agreement and the discrepancy ratio is higher than 1 (for models
other than model 1.0). When p increases past a certain point, however, the annotators become more
discrepant relative to each other than relative to the models and the discrepancy ratio falls below 1. It
is in this regime that the model performance can be said to be better than the average human
performance. Once p approaches 1, the labels are completely random and the annotators are again
as different from each other as they are from the model, yielding a discrepancy ratio of 1. Naturally,
the discrepancy ratio for the ground truth curve (model 1.0) always lies below 1. We note that the
large gap in performance between models in the low-noise regime represents an important and subtle
feature of our approach: models that have similar performance as measured by the Naive Evaluation
and Majority Vote methods can in fact be dramatically different when human performance is taken
into account. As a specific example, consider models 1.0 and 0.9 at a label swap probability of 0.05.
These models appear to only be different by 〜10% when evaluated using accuracy metrics (Figs. 1a
and 1b). However, as shown in Figs. 2a and 2b, model 0.9 is twice as far from annotators as they are
from each other, while model 1.0 is twice as close, thus resulting in a discrepancy ratio performance
gap of 〜400%.
Relative F1 does allow for evaluating the model by comparing it to annotator performance but the
discrepancy ratio has several advantages. First, as illustrated in Fig. 2c, the discrepancy ratio is
insensitive to the number of annotators making it a more robust metric to compare across different
published studies. Second, the discrepancy ratio allows for a highly varying number of annotators per
sample. This feature permits its use in scenarios where annotators have the ability to skip samples
and makes it a more practically viable solution than Relative F1. The presence of strict annotators
(who skip hard samples) has no effect on the discrepancy ratio (Fig. 2d).
6
Published as a conference paper at ICLR 2020
(a)
(d)
Auuedα,.luS-P
label swap probability
(b)
00%,0	0.2	0.4	0.6	0.8	1.0
label swap probability
---model 0.95
---model 0.9
10	15	20	25	30	35
number of annotators
OWAuuedα,-D--
Figure 2: Comparison of the discrepancy ratio with baseline metrics on the MNIST test set. Curves
show mean value and 2-standard deviation bounds obtained from multiple simulations of the experi-
ment. Panel (a) shows the model discrepancy (for three models) and annotator discrepancy. The red
and blue points occur at label swap probabilities of 0.1 * 10/9 and 0.2 * 10/9, respectively, which
correspond precisely to when the accuracies of the simulated human annotators equal that of the
models (0.9 and 0.8, respectively). The discrepancy ratios (Eq. 4) for the same three models are
shown in (b). The blue and red points correspond to those shown in (a). The shaded region denotes
the area where the model is superior to the average human performance. Panels (c) and (d) show that
the discrepancy ratio is not sensitive to the number of annotators used, nor is subject to the strict
annotator bias illustrated in Fig 1d. Here we have used the same parameters as in Figs 1c and 1d.
Dataset	No. of Images	No. of Patients
Train	60,411	1,915
Validation	5,996	200
Test	17,659	600
Table 2: Training and Evaluation Datasets for PLAX Measuraility Classification Task.
4.2	Echocardiographic Image Classification and Regression
An echocardiogram is a common medical test that uses high-frequency sound waves (ultrasound) to
visualize the structure of the heart. In this work, we consider two important and ubiquitous tasks that
are performed as part of virtually any echocardiogram (Lang et al., 2015).
PLAX measurability classification: The exam typically begins with acquiring a parasternal long
axis (PLAX) view, which is used to get an overview of general cardiac function, as well as to make
specific measurements (e.g. fractional shortening (Lang et al., 2015), presence of pericardial effusion,
etc). Before any measurement can be performed, however, a clinician must first perform a binary
classification task and determine whether the image in question is of sufficient quality to be used in
the calculation (a similar binary classification screening step must be performed for most medical
imaging tasks).
LVEF regression: The left ventricular ejection fraction (LVEF), which measures the fraction of
blood the left ventricle expels with each heart cycle, is one of the key metrics used to quantify heart
functionality. This quantity is often measured in the apical 4-chamber view of the heart by segmenting
the left ventricular endocardial border at end-systole and end-diastole (see Appendix A).
Both tasks described above currently require the significant expertise of a trained sonographer. We
have developed deep neural network models to automate these tasks. The datasets for these tasks
are summarized in Tables 2 and 3. All data was collected using the Butterfly iQ ultrasound probe.
Details of the data collection and annotations for both tasks are described in Appendix A.
Dataset	No. of Images	No. of Patients
Train	890,541	2,964
Validation	407,151	1,180
Test	149,992	322
Table 3: Training and Evaluation Datasets for LVEF Regression Task.
7
Published as a conference paper at ICLR 2020
Figure 3: Panel (a) shows that human experts exhibit a high amount of disagreement on which images
meet the threshold for measurability (i. e. with perfect agreement all labels would be 0 or 1). This
plot displays the normalized histogram of the average labels on the PLAX measurability classification
dataset. The discrepancy ratio on the test set as the output threshold of the network output is varied is
shown in (b). Panel (c) shows the labels (blue) and model predictions (red) on the LVEF regression
task test set, sorted by the mean LVEF label. Each patient was labeled by an average of 5 annotators.
The black crosses signify the points on which the model predictions were artificially corrupted to
create large errors.
4.2.1	Evaluation using Discrepancy Ratio
In order to explore the properties of the discrepancy ratio in a real-world setting, we first consider
the PLAX measurability classification dataset. The neural network used for this task is described in
Appendix D. When evaluating the performance of our network using Naive Evaluation (where we
evaluate on a single annotator, chosen at random), we find that we achieve an accuracy of 77.8%,
while the majority vote method gives an accuracy of 88.0%. These numbers are quite poor by the
standards of most machine learning applications. Indeed, we might otherwise conclude that the model
performs at a sub-standard level.
However, when we evaluate using the discrepancy ratio and consider the inter-annotator variability, a
very different story emerges. As can be seen in the distribution of the average label (yi from Eq. 5) in
Fig. 3a, there is significant disagreement (〜25% on average) among the annotators as to whether a
given image meets the threshold of measurability. While this level of disagreement may seem high, it
is typical of that found in the medical applications literature (Wei et al., 2019; Hannun et al., 2019).
Fig. 3b shows the discrepancy ratio as a function of the threshold of the network output. Contrary
to the impression suggested by using the conventional metrics, we find that for a wide range of
thresholds the model dramatically exceeds the average human performance (as defined by ∆ = 1).
Table 4 shows the discrepancy ratio, with 95% confidence intervals, for the model as compared to
those achieved by the individual annotators on the test set. Here we find that the discrepancy ratio has
another nice property: it can be used to evaluate the performance of individual annotators. We find
that there is a significant spread in the annotator performance and that our model outperforms even
the best annotator in the dataset.
4.2.2	DEPENDENCE ON δ
As described in Section 2, the discrepancy ratio can be thought ofas a “meta-metric” that evaluates the
model performance for any desired measure of agreement δ. Here we use the LVEF regression dataset
to explore this dependence on δ. The neural network we used is described in Appendix D. We consider
three choices of δ: the absolute difference: δ = |y - y0|, squared difference: δ = (y - y0)2 and the
hinge loss: δ = max(0, λ - |y - y0|), where a penalty is incurred only past a minimum deviation
threshold 1. As with the PLAX measurability classification task, we find that our model exceeds the
performance of even the best annotator in our dataset on all three measures of agreement, obtaining
discrepancy ratios of 0.89, 0.78 and 0.67 respectively. In order to demonstrate the dependence on δ,
we then artificially corrupt the model so that it occasionally makes errors that are far outside the range
of those made by humans (Fig. 3c). How does the model fare with respect to human performance? It
depends on the type of errors one cares most about. When we set δ to be the absolute difference, the
discrepancy ratio is 0.95 and one may conclude that the model is performing well. If the squared
difference is used for δ instead, the outliers receive higher cost and the discrepancy ratio becomes
1We used λ = 15 as a clinically relevant measure of a significant Ejection Fraction error.
8
Published as a conference paper at ICLR 2020
∆ (95% CI)
A1	1.160(1.117to 1.213)
A2	0.904 (0.875 to 0.932)
A3	0.907 (0.858 to 0.964)
A4	0.922 (0.896 to 0.946)
A5	1.105 (1.052 to 1.161)
A6	0.959 (0.914 to 1.010)
A7	0.952 (0.923 to 0.984)
A8	1.131 (1.065 to 1.215)
A9	0.932 (0.897 to 0.967)
A10	1.030 (0.983 to 1.073)
A11	1.009 (0.973 to 1.049)
A12	1.069 (1.029 to 1.112)
Model	0.805 (0.786 to 0.820)
Table 4: The discrepancy ratio ∆ for the model and annotators (who have labeled at least 100 patients)
on the PLAX measurability test set. Here each annotator was individually taken out of the dataset
and treated as if they were the model for the purpose of calculating their discrepancy ratio. The
confidence intervals were obtained using the bootstrap method where the sampling was performed by
patient. We used the validation set to choose the threshold corresponding to the lowest discrepancy
ratio (as in Fig. 3b).
1.02. When a hinge-loss is used for δ, where deviations below λ are considered acceptable, we
observe that the performance is worse still (1.08).
5	Limitations
While our approach provides for a simple and general way of quantifying model performance in the
presence of label noise, it has several limitations. First, although the flexibility in choosing δ allows
us to examine various aspects of the model performance (illustrated in the example above), our metric
needs to be applied with caution since it is only as good as the underlying measure of agreement.
Second, in situations when annotators are allowed to skip samples they feel unsure about, samples
that get fewer labels may tend to be more ambiguous. Since the discrepancy ratio is undefined
for samples with fewer than two labels, this can cause the discrepancy ratio to overweight easier
examples. Consequently, one should ensure that all samples in the domain of interest are labeled by
at least two annotators. Finally, the discrepancy ratio can yield misleading results when the labels
are provided by poor annotators, though this is also true of any metric (e.g. the relative F1) that
attempts to normalize model performance to account for annotator disagreement. To mitigate this,
the annotators should be properly vetted as skilled for the task (see e.g. Hovy & Lavid (2010) or our
vetting procedure in Appendix A) and allowed to skip when they are unsure of the label. In some
situations annotators may adhere to several discrete “schools of thought” and hence their annotations
may be more similar within each group than between groups. In these situations it may make sense
to evaluate the discrepancy ratio independently within each group.
6	Conclusion
We present a unified and principled approach to evaluating machine learning models when humans
disagree on the truth. Our metric, the discrepancy ratio, is highly intuitive and can be adapted to
virtually any task. We have shown that the discrepancy ratio has numerous desirable properties
including insensitivity to annotator count and insensitivity to annotator/image overlap. At the same
time, we have evaluated the shortcomings of alternative implementations and show that our method
has numerous advantages that make it a convenient standard by which models can be evaluated in the
presence of ambiguous ground truth labels. Finally, to demonstrate our method we have trained deep
neural networks on two real-world tasks from medical imaging and have used our metric to show that
we achieve performance better than even the best annotators in our datasets.
Acknowledgments
The authors would like to thank Gal Koplewitz for his input and several helpful discussions.
9
Published as a conference paper at ICLR 2020
References
Soyeon Ahn, Seong Ho Park, and Kyoung Ho Lee. How to demonstrate similarity by using non-
inferiority and equivalence statistical testing in radiology research. Radiology, 267(2):328-338,
2013.
Douglas G Altman and J Martin Bland. Measurement in medicine: the analysis of method comparison
studies. Journal of the Royal Statistical Society: Series D (The Statistician), 32(3):307-317, 1983.
Dana Cobzas, Neil Birkbeck, Mark Schmidt, Martin Jagersand, and Albert Murtha. 3d variational
brain tumor segmentation using a high dimensional feature set. IEEE 11th International Conference
on Computer Vision, 2007.
Jacob Cohen. A coefficient of agreement for nominal scales. Educational and Psychological
Measurement, 20(1):37, 1960.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248-255. Ieee, 2009.
Benot Frenay and Ata Kaban. A comprehensive introduction to label noise. ESANN 2014 proceedings,
European Symposium on Artificial Neural Networks, Computational Intelligence and Machine
Learning, pp. 23-25, 2014.
Varun Gulshan, Lily Peng, Marc Coram, Martin C. Stumpe, Derek Wu, Arunachalam Narayanaswamy,
Subhashini Venugopalan, Kasumi Widner, Tom Madams, Jorge Cuadros, Ramasamy Kim, Rajiv
Raman, Philip C. Nelson, Jessica L. Mega, and Dale R. Webster. Development and validation of a
deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs. The
Journal of the American Medical Association, 316:2402-2410, 2016.
Awni Y. Hannun, Pranav Rajpurkar, Masoumeh Haghpanahi, Geoffrey H. Tison, Codie Bourn,
Mintu P. Turakhia, and Andrew Y. Ng. Cardiologist-level arrhythmia detection and classification
in ambulatory electrocardiograms using a deep neural network. Nature Medicine, 25:65-69, 2019.
Joger Hansegard, Stig Urheim, Ketil Lunde, Siri Malm, and Stein Inge Rabben. Semi-automated
quantification of left ventricular volumes and ejection fraction by real-time three-dimensional
echocardiography. Cardiovascular Ultrasound, 7, 2009.
Victoria Hodge and Jim Austin. A survey of outlier detection methodologies. Artif. Intell. Rev., 22:
85-126, 2004.
Eduard Hovy and Julia Lavid. Towards a science of corpus annotation: A new methodological
challenge for corpus linguistics. International Journal of Translation, 22, 2010.
Paul Jaccard. The distribution of the flora in the alpine zone. New Phytologist, 11:37-50, 1912.
Mohammad H. Jafari, Hany Girgis, Nathan Van Woudenberg, Zhibin Liao, Robert Rohling, Ken
Gin, Purang Abolmaesumi, and Terasa Tsang. Automatic biplane left ventricular ejection fraction
estimation with mobile point-of-care ultrasound using multi-task learning and adversarial training.
International Journal of Computer Assisted Radiology and Surgery, 2019.
Lawrence Joseph, Theresa W. Gyorkos, and Louis Coupal. Bayesian estimation of disease prevalence
and the parameters of diagnostic tests in the absence of a gold standard. Am. J. Epidemiol., 141:
263-272, 1995.
Jonathan Krause, Varun Gulshan, Ehsan Rahimy, Peter Karth, Kasumi Widner, Greg S. Corrado,
Lily Peng, and Dale R. Webster. Grader variability and the importance of reference standards for
evaluating machine learning models for diabetic retinopathy. Opthamology, 125:1264-1272, 2018.
Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab
Kamali, Stefan Popov, Matteo Malloci, Tom Duerig, et al. The open images dataset v4: Unified
image classification, object detection, and visual relationship detection at scale. arXiv preprint
arXiv:1811.00982, 2018.
10
Published as a conference paper at ICLR 2020
Roberto M Lang, Luigi P Badano, Victor Mor-Avi, Jonathan Afilalo, Anderson Armstrong, Laura
Ernande, Frank A Flachskampf, Elyse Foster, Steven A Goldstein, Tatiana Kuznetsova, et al.
Recommendations for cardiac chamber quantification by echocardiography in adults: an update
from the american society of echocardiography and the european association of cardiovascular
imaging. European Heart Journal-Cardiovascular Imaging, 16(3):233-271, 2015.
Elizabeth Lazarus, Martha B Mainiero, Barbara Schepps, Susan L Koelliker, and Linda S Livingston.
Bi-rads lexicon for us and mammography: interobserver variability and positive predictive value.
Radiology, 239(2):385-391, 2006.
Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT Labs [Online].
Available: http://yann. lecun. com/exdb/mnist, 2, 2010.
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr
Dollar, and C LaWrence Zitnick. Microsoft coco: Common objects in context. In European
conference on computer vision, pp. 740-755. Springer, 2014.
Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and Lizhen Qu. Making
deep neural netWorks robust to label noise: a loss correction approach. CVPR, pp. 1944-1952,
2017.
Martin Rajchl, MattheW C.H. Lee, Ozan Oktay, Konstantinos Kamnitsas, Jonathan Passerat-Palmbach,
Wenjia Bai, Mellisa Damodaram, Mary A. Rutherford, Joseph V. Hajnal, Bernhard Kainz, and
Daniel Rueckert. Deepcut: Object segmentation from bounding box annotations using convolu-
tional neural netWorks. IEEE Transactions on Medical Imaging, 36, 2017.
Pranav Rajpurkar, Jeremy Irvin, Kaylie Zhu, Brandon Yang, Hershel Mehta, Tony Duan, Daisy
Ding, Aarti Bagul, Robyn L. Ball, Curtis Langlotz, Katie Shpanskaya, MattheW P. Lungren, and
AndreW Y. Ng. Chexnet: Radiologist-level pneumonia detection on chest x-rays With deep learning.
arXiv preprint arXiv:1711.05225v3, 2017.
Pranav Rajpurkar, Jeremy Irvin, Aarti Bagul, Daisy Ding, Tony Duan, Hershel Mehta, Brandon Yang,
Kaylie Zhu, Dillon Laird, Robyn L. Ball, Curtis Langlotz, Katie Shpanskaya, MattheW P. Lungren,
and AndreW Y. Ng. Mura: Large dataset for abnormality detection in musculoskeletal radiographs.
arXiv preprint arXiv:1712.06957v4, 2018a.
Pranav Rajpurkar, Jeremy Irvin, Robyn L. Ball, Kaylie Zhu, Brandon Yang, Hershel Mehta, Tony
Duan, Daisy Ding, Aarti Bagul, Curtis P. Langlotz, Bhavik N. Patel, Kristen W. Yeom, Katie
Shpanskaya, Francis G. Blankenberg, Jayne Seekins, Timothy J. Amrhein, David A. Mong,
SafWan S. Halabi, Evan J. Zucker, AndreW Y. Ng, and MattheW P. Lungren. Deep learning for
chest radiograph diagnosis: A retrospective comparison of the chexnext algorithm to practicing
radiologists. PLOS Medicine, 15, 2018b.
David Rolnick, Andreas Veit, Serge Belongie, and Nir Shavit. Deep learning is robust to massive
label noise. arXiv preprint arXiv:1705.10694v3, 2017.
AndreW B Rosenkrantz, Ruth P Lim, Mershad Haghighi, Molly B Somberg, James S Babb, and
Samir S Taneja. Comparison of interreader reproducibility of the prostate imaging reporting and
data system and likert scales for evaluation of multiparametric prostate mri. American journal of
roentgenology, 201(4):W612-W618, 2013.
Holger R. Roth, Chen Shen, Hirohisa Oda, Masahiro Oda, Yuichiro Hayashi, Kazunari MisaWa, and
Kensaku Mori. Deep learning and its application to medical image segmentation. arXiv preprint
arXiv:1803.08691v1, 2018.
Sainbayar Sukhbaatar, Joan Bruna, Manohar Paluri, Lubomir Bourdev, and Rob Fergus. Training
convolutional netWorks With noisy labels. International Conference on Learning Representations,
2015.
Arash Vahdat. ToWard robustness against label noise in training deep discriminative neural netWorks.
NIPS, 2017.
11
Published as a conference paper at ICLR 2020
Wei Wang, Taro Watanabe, Macduff Hughes, Tetsuji Nakagawa, and Ciprian Chelba. Denoising
neural machine translation training with trusted data and online data selection. Proceedings of the
Third Conference on Machine Translation, 1:133-143, 2018.
Jason W Wei, Laura J Tafe, Yevgeniy A Linnik, Louis J Vaickus, Naofumi Tomita, and Saeed
Hassanpour. Pathologist-level classification of histologic patterns on resected lung adenocarcinoma
slides with deep neural networks. Scientific reports, 9(1):3358, 2019.
Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, and Aude Oliva. Learning deep
features for scene recognition using places database. In Advances in neural information processing
systems, pp. 487-495, 2014.
Xingquan Zhu and Xindong Wu. Class noise vs. attribute noise: A quantitative study of their impacts.
Artificial Intelligence Review, 22:177-210, 2004.
12
Published as a conference paper at ICLR 2020
(a)	(b)
Figure S1: Examples of images deemed measurable (a) and not measurable (b) by majority vote of
annotators. Several important features characteristic of a measurable image are noted in (a). These
same features are obscured or entirely absent in (b).
A Data Collection, Datasets and Annotations
In this section we describe the data collection, dataset splits and annotation procedure for the tasks
presented in Section 4.2 of the main text.
A. 1 Data Collection
For model development and evaluation, images were collected in several hospitals in the United
States on patients presenting for a standard echocardiogram exam. All data was collected using the
Butterfly iQ probe. For the PLAX measurability classification dataset we have additionally included
images taken on healthy adults outside the hospital setting (approximately 10% of the dataset).
A.2 PLAX Measurability Classification Task
In order to collect labels, echocardiogram images were presented to 14 professional, US-licensed
sonographers. The images in the test set received an average of 9 labels per image. The sonographers
were given the following task:
Mark as MEASURABLE the frames that satisfy ALL of the following criteria:
•	The frame shows a parasternal long axis (PLAX) view of diagnostic
quality.
•	The frame can be used for a measurement of the left ventricular internal
diameter (LVID), as shown in the diagram. Note that the LVID can be
measurable irrespective of where the heart is in its cycle.
Mark all other frames as NOT MEASURABLE.
The above prompt refers to a diagram showing a canonical image of a PLAX view. Examples of
images deemed measurable and not measurable by the majority of annotators are shown in Fig. S1.
13
Published as a conference paper at ICLR 2020
(a)
(b)
ES segmentation
mask ∖
Figure S2: An example LVEF calculation in the apical 4-chamber view. Panel (a) shows the
end-diastole frame with the overlaid segmentation mask while panel (b) shows the corresponding
end-systole frame within the same heart cycle. The calculated LVEF from these segmentation masks
is 66%
A.3 LVEF Regression Task
Here we collected three separate types of labels:
1.	Segmentation Masks: annotators were asked to segment the endocardial border of the left
ventricle.
2.	Measurability: annotators were asked to classify frames into measurable/non-measurable
classes using a procedure virtually identical to that used in the PLAX measurability classifi-
cation task above.
3.	LVEF labels: annotators were asked to provide LVEF labels by segmenting the endocardial
border at the end-systole and end-diastole frames.
Since the model predicts segmentations and measurability scores only as intermediate states (see
Appendix D for more details), they are not relevant for the evaluation framework presented in this
work and we do not consider these predictions further. Instead we focus on the LVEF regression
predictions. For this task, short echocardiogram video clips containing several heart cycles were
presented to 19 professional, US-licensed sonographers. The videos in the test set received an average
of 5 labels per video. The sonographers were given the following task:
14
Published as a conference paper at ICLR 2020
Apical Endocardial contours should be labeled according to ASE guidelines:
•	Identify a heart cycle within the cine.
•	Navigate to the ED frame and trace the endocardial border of the left
ventricle.
•	Navigate to the ES frame and trace the endocardial border of the left
ventricle.
•	Make sure the ED frame is to the left of the ES frame but part of the
same heart cycle.
•	Observe the calculated EF just above the cine. If this EF score looks cor-
rect, hit Submit. Otherwise, adjust the ED and ES frames accordingly.
The above prompt refers to “ASE guidelines”, which are published consensus recommendations and
can be found at Lang et al. (2015). An example of a LVEF prediction is shown in Fig S2, where the
model segments the end-systole and end-diastole frames and calculates the LVEF via the Modified
Simpson’s Method (Lang et al., 2015).
A.4 Annotator Inclusion Criteria
We relied on two senior cardiac sonographers (each having more than 15 years of experience) to source
annotators. For both tasks, each annotator was interviewed and given an onboarding assignment.
Annotators that were deemed by our sonographers to comply with ASE standards were then cleared
to label images. During the first several weeks of label collection, our sonographers conducted several
detailed reviews of each annotator’s images. Examples in which our sonographers disagreed with
the assessment of the annotator were discussed with the annotator in live sessions to ensure that the
disagreements resulted in genuine differences of medical opinion, rather than annotation errors. In
addition, the disagreement metrics for each annotator (as calculated Section 4.2.1) were tracked over
time and were used to schedule additional review sessions with the annotators, as necessary.
B MPD and Annotator Discrepancy using Squared-Error
Agreement Function
As a preliminary result that is used in Section 3.3, we show that given any two sets of labels
Y = {y1,y2, ..., yn} and Y0 = {y10 ,y20 , ..., yn0 0} the MPD is given by the expression
ψ(Y, Y0) = (y - y0)2 + σy2 + σy2
where y = n Pn=I y and * = (1 Pn=I y2) - y2 are the mean and variance of yk, respectively.
Proof.
n	n0
ψ(Y,Y0)=XX
k=1 k0=1
(yk - yk00)2
nn0
00
n	n	nn
nno hn0 X y2 + η X y2o- 2 X y X y/
k=1	k0=1	k=1 k0=1
nn0 卜0n(σ2 + y2) + nn0(σ2, + y02) - 2nn0yy0]
(y - y0)2 + σy2 + σy20 .
15
Published as a conference paper at ICLR 2020
□
As a special case, when the labels can only take on the values 0 and 1, the variance is fully determined
by the mean:
σy2 = y - y2 ,
and the MPD simplifies to
ψ(Y, Y0) = y + y0 - 2yy0.
Using this result and the definition in Eq. 2, the annotator discrepancy for a single sample is then
given by:
αi
Ai
X j y∙o一网y∙o
Ai(	Ai(Ai - 1)
j,j0=1
j6=j0
Ai	Ai
Ai(A⅛ [2(Ai - DXj2 X yj yjo
j=1	j,j0=1
j6=j0
2yi (1-yi) + A⅛.
where σy2i is the variance of the labels yi . Here we have used the fact that
Ai	Ai
X yji yji 0 = (Aiyi)2-X(yji)2.
j,j0=1	j=1
j6=j0
For the special case where each annotator provided no more than a single label per image, the above
relationship between the mean and variance can be used to further simplify the annotator discrepancy
to
i	2Ai
α =-------
i - 1
C Alternative Formulations of the Discrepancy Ratio
In constructing the discrepancy ratio metric, we made several seemingly arbitrary design choices.
In this section we discuss the reasons for these decisions and why the alternative formulations are
problematic.
C.1 The Order of the Averages over Annotations and Samples
Consider an alternative definition for the model discrepancy, where we swap the order of the averages
over samples and annotations:
μ0
AN
XX
j=1 i=1
Yi = 0
N
16
Published as a conference paper at ICLR 2020
Here A is the total number of annotators in the dataset and the sum over samples has to go over
only those i for which the jth annotator has labeled it at least once (i.e. Yji = 0). Conceptually,
this expression calculates a discrepancy score for each model/annotator pair, and then averages them
together to arrive at an aggregated measure of agreement.
An analogous alternative definition can be constructed for the annotator discrepancy:
α0
A
X
j,j0=1
j6=j0
N
X
i=1
ψ(Yji,Yji0)
NA(A - 1).
Yji,Yji06=0
As in the case of the model discrepancy, this expression computes a discrepancy for each annotator
pair and then averages all the pairs to yield an aggregate measure of annotator agreement. The
methods presented in Wei et al. (2019); Rajpurkar et al. (2017) use essentially these definitions,
though they do not compare them using a ratio.
When all annotators label all samples, these definitions are perfectly valid, and are in fact equivalent
to μ and α. However, in most real-world scenarios, data come from disparate sources (e.g. different
hospitals with different annotators) and itis not practical to ensure that the exact same set of annotators
labeled all the samples. Worse still, in many domains it is necessary to allow human annotators to
voluntarily skip samples when they feel there is not enough information to provide an accurate label.
In such a scenario, formulating the model and annotator discrepancies using the alternative definitions
above allows an annotator to improve their own discrepancy score by simply being strict and only
annotating “easy” samples. This will generally result in a bias being introduced into the discrepancy
ratio. To illustrate this point, consider the following toy example:
Example C.1 A dataset contains one “easy” image (on which the MPD between
any two annotators is δe) and one “hard” image (on which the MPD between
any two annotators is δh). Similarly, a machine learning model gets an MPD of
δe0 on the easy image and δh0 on the hard image. Three annotators (Alice, Bob
and Charlie) are asked to label the data. Alice and Bob label both images, but
Charlie wants to improve his discrepancy and only labels the easy image. Using
the original definitions of a and μ, the discrepancy ratio is
μ =	(δe	+ δh )/2 =	δe	+	δh
A= α =	(δe	+ δh)∕2 =	δe	+	δh	.
Note that the value of ∆ would have been exactly the same without Charlie. On
the other hand, using the alternative definition the discrepancy ratio becomes
biased and is given by
μ =(2(se+δh )/2+δ ”3 =	2δe + δh
=α0 = ((δe + δh)∕2 + 2δe)∕3 = (5δe + δh)∕2.
In the special case where the annotators and model are in perfect agreement on
the easy image (δe = δe0 = 0), Charlie was able to double the discrepancy ratio
by simply skipping the difficult image:
2δ0
δ0=2δh=2∆.
δh
It should be noted that even if we weigh the average of annotators by the inverse of the number of
annotations they provided, the bias does not go away, whereas the discrepancy ratio, as defined in the
main text, is completely insensitive to this effect. The effect illustrated in the example above is not
limited to the presence of an adversarial annotator, but can creep into the data whenever there exist
any systematic differences in how the data got partitioned among annotators, which in practice may
be challenging or even impossible to control.
17
Published as a conference paper at ICLR 2020
C.2 Computing Pairwise Deviations vs. Comparisons to the Mean
In the definition of the annotator discrepancy, we compute the MPD over all pairs of annotators. A
seemingly valid alternative would be to instead compare each annotator to the mean of all the other
annotations. In this alternative definition, the annotator discrepancy is given by the expression
α
00
N Ai
XX
Ψ(Yj ,ωj)
i=1 j=1
NAi
where
Ai
ωi = X y
j =乙 Ai - 1.
j0=1
j06=j
Here, ωji is a scalar that represents the average of all annotations for the ith sample, leaving out the
jth annotator. We can analogously construct an alternative definition for the model discrepancy:
N Ai
μ00 = XX
i=1 j=1
ψ(mi, ωji )
NAi
Note that we cannot simply compare the model predictions to the average of all the annotations since
then the number of annotators that go into the average for μ00 would be one higher than that used in
α00, which would artificially lower the discrepancy ratio. The methods presented in Hannun et al.
(2019); Rajpurkar et al. (2018a;b) are similar to this averaging approach.
While μ0 and α00 may seem like valid formulations for the model and annotator discrepancies, they
lead to a major problem: The discrepancy ratio becomes dependent on how many annotators are
used in the study.
To see why this is, notice that ωj is an average over Ai annotators and will generally scale as 1/√Ai.
This implies that both μ00 and a00 will get smaller as the number of annotators is increased. They will
generally scale at different rates, however, depending on the distribution of the model predictions and
annotator labels, and therefore will make the ratio dependent on the number of annotators (for a
specific example, see Section 4 of the main text).
D	Algorithm Development and Training
The deep learning algorithm used for the PLAX measurability classification task is a 10-layer
convolutional neural network, where each 2-layer block subsamples the inputs by a factor of 2. After
the final layer we apply a global average pooling, followed by a sigmoid function that is thresholded
at a particular level (see Section 4). We trained the network de novo using the Adam optimizer with a
learning rate of 0.00005 and a mini batch size of 32.
The algorithm used for estimating the LVEF employs a U-Net style network tasked with segmenting
the left ventricular endocardial border. The network consists of a 6-layer encoder followed by 6-layer
decoder where downsampling and upsampling by a factor of 2 occur at each layer, respectively. We
trained the network de novo using the Adam optimizer with a learning rate of 0.0001 and mini batch
size of 64. The network contains an additional output head that predicts the measurability of the input
image. The per-frame segmentations and measurability values are then used to find the end-systole
and end-diastole frames, which are in turn used to calculate the LVEF via the Modified Simpson’s
Method (Lang et al., 2015). The per-frame measurability scores are aggregated to produce a quality
score for each LVEF. The model only outputs LVEF predictions for which the predicted quality is
18
Published as a conference paper at ICLR 2020
above a predefined threshold.
E	Extensions of the Discrepancy Ratio
When there exists an imbalance of labels in the dataset, the discrepancy ratio metric can be stratified
to give a rebalanced measure of agreement.
For example, in a classification task where the class labels are imbalanced, we can define the
mean-per-class discrepancy ratio by separately computing ∆ for each class and averaging the results
together (in this case, the stratification into different classes can be determined by majority vote).
19