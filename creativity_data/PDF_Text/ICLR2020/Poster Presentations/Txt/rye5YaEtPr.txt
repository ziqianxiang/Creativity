Published as a conference paper at ICLR 2020
SAdam: A Variant of Adam for
Strongly Convex Functions
Guanghui Wang* 1, Shiyin Lu1, Quan Cheng1, Wei-Wei Tu2 and Lijun Zhang1，*
1 National Key Laboratory for Novel Software Technology, Nanjing University, China
24Paradigm Inc.， Beijing， China
{wanggh,lusy,chengq,zhanglj}@lamda.nju.edu.cn,tuwwcn@gmail.com
Ab stract
The Adam algorithm has become extremely popular for large-scale machine learn-
ing. Under convexity condition, it has been proved to enjoy a data-dependent
O(√T) regret bound where T is the time horizon. However, whether strong Con-
vexity can be utilized to further improve the performance remains an open problem.
In this paper, we give an affirmative answer by developing a variant of Adam
(referred to as SAdam) which achieves a data-dependent O(log T) regret bound
for strongly convex functions. The essential idea is to maintain a faster decaying
yet under controlled step size for exploiting strong convexity. In addition, under a
special configuration of hyperparameters, our SAdam reduces to SC-RMSprop, a
recently proposed variant of RMSprop for strongly convex functions, for which
we provide the first data-dependent logarithmic regret bound. Empirical results on
optimizing strongly convex functions and training deep networks demonstrate the
effectiveness of our method.
1	Introduction
Online Convex Optimization (OCO) is a well-established learning framework which has both theoret-
ical and practical appeals (Shalev-Shwartz et al., 2012). It is performed in a sequence of consecutive
rounds: In each round t, firstly a learner chooses a decision xt from a convex set D ⊆ Rd , at the
same time, an adversary reveals a loss function ft(∙) : D → R, and consequently the learner suffers a
loss ft(xt). The goal is to minimize regret, defined as the difference between the cumulative loss of
the learner and that of the best decision in hindsight (Hazan et al., 2016):
TT
R(T) :=	ft (xt) - min	ft(x).
x∈D
t=1	t=1
The most classic algorithm for OCO is Online Gradient Descent (OGD) (Zinkevich, 2003), which
attains an O(√T) regret.OGD iteratively performs descent step towards gradient direction with a
predetermined step size, which is oblivious to the characteristics of the data being observed. As a
result, its regret bound is data-independent, and can not benefit from the structure of data. To address
this limitation, various of adaptive gradient methods, such as Adagrad (Duchi et al., 2011), RMSprop
(Tieleman & Hinton, 2012) and Adadelta (Zeiler, 2012) have been proposed to exploit the geometry
of historical data. Among them, Adam (Kingma & Ba, 2015), which dynamically adjusts the step size
and the update direction by exponential average of the past gradients, has been extensively popular
and successfully applied to many applications (Xu et al., 2015; Gregor et al., 2015; Kiros et al.,
2015; Denkowski & Neubig, 2017; Bahar et al., 2017). Despite the outstanding performance, Reddi
et al. (2018) pointed out that Adam suffers the non-convergence issue, and developed two modified
versions, namely AMSgrad and AdamNC. These variants are equipped with data-dependent regret
bounds, which are O(√T) in the worst case and become tighter when gradients are sparse 1.
* Lijun Zhang is the corresponding author.
1We note that, as very recently pointed out by Tran et al. (2019), there still exists a minor theoretical flaw in the
analysis of Reddi et al. (2018), and such issue in fact appears in many of recent variants of AMSgrad/AdamNC.
In this paper, we provide a simple way to fix this problem. The details can be found in Appendix G.
1
Published as a conference paper at ICLR 2020
While the theoretical behavior of Adam in convex cases becomes clear, it remains an open problem
whether strong convexity can be exploited to achieve better performance. Such property arises, for
instance, in support vector machines as well as other regularized learning problems, and it is well-
known that the vanilla OGD with an appropriately chosen step size enjoys a much better O(log T)
regret bound for strongly convex functions (Hazan et al., 2007). In this paper, we propose a variant
of Adam adapted to strongly convex functions, referred to as SAdam. Our algorithm follows the
general framework of Adam, yet keeping a faster decaying step size controlled by time-variant
heperparameters to exploit strong convexity. Theoretical analysis demonstrates that SAdam achieves
a data-dependent O(log T) regret bound for strongly convex functions, which means that it converges
faster than AMSgrad and AdamNC in such cases, and also enjoys a huge gain in the face of sparse
gradients.
Furthermore, under a special configuration of heperparameters, the proposed algorithm reduces
to the SC-RMSprop (Mukkamala & Hein, 2017), which is a variant of RMSprop algorithm for
strongly convex functions. We provide an alternative proof for SC-RMSprop, and establish the first
data-dependent logarithmic regret bound. Finally, we evaluate the proposed algorithm on strongly
convex problems as well as deep networks, and the empirical results demonstrate the effectiveness of
our method.
Notation. Throughout the paper, we use lower case bold face letters to denote vectors, lower case
letters to denote scalars, and upper case letters to denote matrices. We use ∣∣ ∙ ∣∣ to denote the
'2-norm and ∣∣ ∙ ∣∞ the infinite norm. For a positive definite matrix H ∈ Rd×d, the weighted
'2-norm is defined by IlxkH = x>Hx. The H-weighted projection ∏H (x) of X onto D is defined by
∏H(x) = argmi%∈D ∣∣y 一 x∣H. We use gt to denote the gradient of ft(∙) at Xt. For vector sequence
{vt}tT=1, we denote the i-th element of vt by vt,i. For diagonal matrix sequence {Mt}tT=1, we use
mt,i to denote the i-th element in the diagonal of Mt. We use g±t,i = [gι,i,…,gt,i] to denote the
vector obtained by concatenating the i-th element of the gradient sequence {gt}tT=1.
2	Related Work
In this section, we briefly review related work in online convex and strongly convex optimization.
In the literature, most studies are devoted to the minimization of regret for convex functions. Under the
assumptions that the infinite norm of gradients and the diameter of the decision set are bounded,OGD
with step size on the order of O(1/ʌ/t) (referred to as convex OGD) achieves a data-independent
Ο(d√T) regret (Zinkevich, 2003), where d is the dimension. To conduct more informative updates,
Duchi et al. (2011) introduce Adagrad algorithm, which adjusts the step size of OGD in a per-
dimension basis according to the geometry of the past gradients. In particular, the diagonal version of
the algorithm updates decisions as
Xt+1 = Xt - √t%-“2gt	(1)
where α > 0 is a constant factor, Vt is a d × d diagonal matrix, and ∀i ∈ [d], vt,i = (Ptj=1
gj2,i)/t
is the arithmetic average of the square of the i-th elements of the past gradients. Intuitively, while
the step size of Adagrad, i.e., (α∕√t)Vt-1/2, decreases generally on the order of Ο(1∕√t) as that in
convex OGD, the additional matrix Vt-1/2 will automatically increase step sizes for sparse dimensions
in order to seize the infrequent yet valuable information therein. For convex functions, Adagrad
enjoys an O(Pd=1 ∣∣gLτ,ik) regret, which is Ο(d√T) in the worst case and becomes tighter when
gradients are sparse.
Although Adagrad works well in sparse cases, its performance has been found to deteriorate when
gradients are dense due to the rapid decay of the step size since it uses all the past gradients in the
update (Zeiler, 2012). To tackle this issue, Tieleman & Hinton (2012) propose RMSprop, which
alters the arithmetic average procedure with Exponential Moving Average (EMA), i.e.,
Vt = βVt-1 + (1 一 β)diag(gtgt> )
where β ∈ [0,1] is a hyperparameter, and diag(∙) denotes extracting the diagonal matrix. In this
way, the weights assigned to past gradients decay exponentially so that the reliance of the update is
2
Published as a conference paper at ICLR 2020
essentially limited to recent few gradients. Since the invention of RMSprop, many EMA variants of
Adagrad have been developed (Zeiler, 2012; Kingma & Ba, 2015; Dozat, 2016). One of the most
popular algorithms is Adam (Kingma & Ba, 2015), where the first-order momentum acceleration,
shown in (2), is incorporated into RMSprop to boost the performance:
^t = β1^t-1 + (1- βι)gt	(2)
Vt = β2Vt-1 + (1 - β2)diag(gtgt>)	(3)
xt+ι = Xt - √tV722gt	(4)
While it has been successfully applied to various practical applications, a recent study by Reddi
et al. (2018) shows that Adam could fail to converge to the optimal decision even in some simple
one-dimensional convex scenarios due to the potential rapid fluctuation of the step size. To resolve
this issue, they design two modified versions of Adam. The first one is AMSgrad,
Vt = max {E-1,%}
χt+1 = Xt - √tV-1∕2gt
where an additional element-wise maximization procedure is employed before the update of xt to
ensure a stable step size. The other is AdamNC, where the framework of Adam remains unchanged,
yet a time-variant β2 (i.e., β2t) is adopted to keep the step size under control. Theoretically, the two
algorithms achieve data-dependent O(√TPd=ι vτ,i+Pd=ι kg±τ,ik log T) and O(√TPd=ι vτ,i +
Pid=1 kg1:T,ik) regrets respectively. In the worst case, they suffer O(d√TlogT) and O(d√T)
regrets respectively, and enjoy a huge gain when data is sparse.
Note that the aforementioned algorithms are mainly analysed in general convex settings and suffer
at least O(d√T) regret in the worst case. For online strongly convex optimization, the classical
OGD with step size proportional to O(1/t) (referred to as strongly convex OGD) achieves a data-
independent O(dlogT) regret (Hazan et al., 2007). Inspired by this, Mukkamala & Hein (2017)
modify the update rule of Adagrad in (1) as follows
α -1
Xt+1 = Xt - -Vtt gt
so that the step size decays approximately on the order of O(1/t), which is similar to that in strongly
convex OGD. The new algorithm, named SC-Adagrad, is proved to enjoy a data-dependent regret
bound of O(Pid=1 log(kg1:T,ik2)), which is O(dlogT) in the worst case. They further extend this
idea to RMSprop, and propose an algorithm named SC-RMSprop. However, as pointed out in Section
3, their regret bound for SC-RMSprop is in fact data-independent, and in this paper we provide the
first data-dependent regret bound for this algorithm.
Very recently, several modifications of Adam adapted to non-convex settings have been developed
(Chen et al., 2019; Basu et al., 2018; Zhang et al., 2018; Shazeer & Stern, 2018). However, to our
knowledge, none of these algorithms are particularly designed for strongly convex functions, nor
enjoy a logarithmic regret bound.
3	SAdam
In this section, we first describe the proposed algorithm, then state its theoretical guarantees, and
finally compare it with the SC-RMSprop algorithm.
3.1	The Algorithm
Before proceeding to our algorithm, following previous studies, we introduce some standard defini-
tions (Boyd & Vandenberghe, 2004) and assumptions (Reddi et al., 2018).
Definition 1. Afunction f (∙) : D → R is λ-strongly convex if ∀xι, χ ∈ D,
f(xι) ≥ f(x2) + Vf (X2)>(xι - X2) + 2∣∣X1 - X2k2.	(5)
3
Published as a conference paper at ICLR 2020
Algorithm 1 SAdam
1:	Input: {β1t}tT=1,{β2t}tT=1,δ
2:	Initialize: ^0 = 0,忆=0d×d, xι = 0.
3:	for t = 1, . . . , T do
4：	gt = Vft(Xt)
5:	gt = β1tgt-1 + (I - βIt)gt
6:	Vt = β2tVt-1 + (1 - β2t)diag(gtgt>)
7:	Vt = Vt + δ Id
8：	χt+ι = ∏Vt (Xt- αVtTgt)
9: end for
Assumption 1. The infinite norm of the gradients of all loss functions are bounded by G∞, i.e., their
exists a constant G∞ > 0 such that maxX∈D kVft(X)k∞ ≤ G∞ holds for all t ∈ [T].
Assumption 2. The decision set D is bounded. Specifically, their exists a constant D∞ > 0 such
that maxX1,X2∈D kX1 - X2k∞ ≤ D∞.
We are now ready to present our algorithm, which follows the general framework of Adam and is
summarized in Algorithm 1. In each round t, we firstly observe the gradient at Xt (Step 4), then
compute the first-order momentum gt (Step 5). Here βιt a time-variant hyperparameter. Next, We
calculate the second-order momentum Vt by EMA of the square of past gradients (Step 6). This
procedure is controlled by β2t, Whose value Will be discussed later. After that, We add a vanishing
factor δ to the diagonal of Vt and get Vt (Step 7), which is a standard technique for avoiding too large
steps caused by small gradients in the beginning iterations. Finally, we update the decision by ^t and
Vt, which is then projected onto the decision set (Step 8).
While SAdam is inspired by Adam, there exist two key differences: One is the update rule of Xt in
Step 8, and the other is the configuration of β2t in Step 6. Intuitively, both modifications stem from
strongly convex OGD, and jointly result in a faster decaying yet under controlled step size which
helps utilize the strong convexity while preserving the practical benefits of Adam. Specifically, in the
first modification, we remove the square root operation in (4) of Adam, and update Xt at Step 8 as
follows
α
χt+ι = χt — -Vtt 1gt.	(6)
In this way, the step size used to update the i-th element of Xt is 及V-1, which decays in general on
the order of O(1/-), and can still be automatically tuned in a per-feature basis via the EMA of the
historical gradients.
The second modification is made to β2t, which determines the value of Vt and thus also controls the
decaying rate of the step size. To help understand the motivation behind our algorithm, we first revisit
Adam, where β2t is simply set to be constant, which, however, could cause rapid fluctuation of the
step size, and further leads to the non-convergence issue. To ensure convergence, Reddi et al. (2018)
propose that β2t should satisfy the following two conditions:
Condition 1.	∀-∈ [T] and i ∈ [d],
√-v1/2	√- — 1v1-1 i
__t,i ________t 1,i ≥ o
αα
Condition 2.	For some ζ > 0 and ∀- ∈ [T], i ∈ [d],
∖
t X πk=j1β2(t-k+1)(1 - β2j )g2,i ≥ Z UX gj,i.
j=1	ζ	j=1
The first condition implies that the difference between the inverses of step sizes in two consecutive
rounds is positive. It is inherently motivated by convex OGD (i.e., OGD with step size √t, where
α > 0 is a constant factor), in which
√t — √-ΞI ≥ O, ∀t ∈ [t ]
αα
4
Published as a conference paper at ICLR 2020
is a key condition used in the analysis. We first modify Condition 1 by mimicking the behavior
of strongly convex OGD as we are devoted to minimizing regret for strongly convex functions. In
strongly convex OGD (Hazan et al., 2007), the step size at each round t is set as 午 with ɑ ≥ 1 for
λ-strongly convex functions. Under this configuration, we have
--------≤ λ,∀t ∈ [T].	(7)
αα
Motivated by this, we propose the following condition for our SAdam, which is an analog to (7).
Condition 3.	Their exists a constant C > 0 such thatfor any a ≥ C, we have ∀t ∈ [T] and i ∈ [d],
tvti - (t - 1)vtτ,i ≤ λ(1- βι).	(8)
αα
Note that the extra (1 - β1) in the righthand side of (8) is necessary because SAdam involves the
first-order momentum in its update.
Finally, since the step size of SAdam scales with 1/t rather than 1∕√t in Adam, We modify Condition
2 accordingly as follows:
Condition 4.	For some ζ > 0, ∀t ∈ [T] and i ∈ [d],
t t-j	1 t
t XY β2(t-k+1) (I- β2j)g2,i ≥ ZX gj,i	⑼
3.2 Theoretical Guarantees
In the following, we give a general regret bound when the two conditions are satisfied.
Theorem 1. Suppose Assumptions 1 and 2 hold, and all loss functions fι (•),..., fτ(∙) are λ-
strongly convex. Let δ > 0, β1t = β1νt-1, where β1 ∈ [0, 1), ν ∈ [0, 1), and {β2t}tT=1 ∈ [0, 1]T be
a parameter sequence such that Conditions 3 and 4 are satisfied. Let ɑ ≥ C. The regret ofSAdam
satisfies
R(T) ≤ dD∞δ	+ dβιD∞ (G∞ + δ) +
(2 -2α(1 - β1)	2α(1 - β1)(ν - 1)2
αζ
(1 -β1)3
Xlog ( Zδ X g2,i + 1).	(10)
Remark 1. The above theorem implies that our algorithm enjoys an O(Pid=1 log(kg1:T,ik2)) regret
bound, which is O(dlogT) in the worst case, and automatically becomes tighter whenever the
gradients are small or sparse such that kg1:T,ik2 G2∞T for some i ∈ [d]. The superiority of
data-dependent bounds have been witnessed by a long list of literature, such as Duchi et al. (2011);
Mukkamala & Hein (2017); Reddi et al. (2018). In the following, we give some concrete examples:
Consider a one-dimensional sparse setting where non-zero gradient appears with probability
c/T and c > 1 is a constant. Then E
factor.
log PtT=1 gt2,1	= O(log(c))
which is a constant
• Consider a high-dimensional sparse setting where in each dimension of gradient non-zero
element appears with probability p = T (m-d)/d with m ∈ [1, d) being a constant. Then,
E Pid=1 log PjT=1gj2,ii = O(m log T), which is much tighter than O(d log T).
Remark 2. In practice, first-order momentum is a powerful technique that can significantly boost
the performance (Reddi et al., 2018), and our paper is the first to show that algorithms equipped with
such technique can achieve logarithmic regret bound for strongly convex functions. However, since
the regret bound of SAdam is data-dependent, it is difficult to rigorously analyse the influence of the
first-order momentum parameter β1 as it affects all the gradients appearing in the last term of the
regret of Theorem 1. We will further investigate this in the feature work. We note that the regret
bounds of adaptive algorithms with first-order momentum (e.g., Reddi et al., 2018; Chen et al., 2019)
all share a similar structure as our regret bound with respect to β1.
5
Published as a conference paper at ICLR 2020
Next, we provide an instantiation of {β2t }tT=1 such that Conditions 3 and 4 hold, and derive the
following Corollary.
Corollary 2. Suppose Assumptions 1 and 2 hold, and all loss functions fι(∙),...,fτ(∙) are λ-
StrongIy convex. Let δ > 0, βιt = βινt-1 where ν, βι ∈ [0,1), and 1 一 1 ≤ β2t ≤ 1 一 +, where
γ ∈ (0, 1]. Then we have:
1.	For any α ≥ (λ——)G∞, ∀t ∈ [T] and i ∈ [d],
tvti 一 (t 一 1)vtτ,i ≤ λ(1- βι).
αα
2.	For all t ∈ [T] andj ∈ [d],
t t-j	t
t XY β2(t-k+1) (1-β2j )g2,i ≥ Y X gj,i.
j=1 k=1	j=1
(2-γ )G2
Moreover, let α ≥ \(i—囱∞, and the regret ofSAdam satisfies:
R(T) ≤
dD∞ δ
2α(1 — βI)
+ dβιD∞ (G∞ + δ)
+ 2α(1 一 βι)(ν 一 1)2
dT
+ 证言和 Xlog (δ X g2,i + 1
Furthermore, as a special case, by setting βιt = 0 and 1 一 ɪ ≤ β2t ≤ 1 一 Y, our algorithm
reduces to SC-RMSprop (Mukkamala & Hein, 2017), which is a variant of RMSprop for strongly
convex functions. Although Mukkamala & Hein (2017) have provided theoretical guarantees for this
algorithm, we note that their regret bound is in fact data-independent. Specifically, the regret bound
provided by Mukkamala & Hein (2017) takes the following form:
R(T) ≤ 嘤+2YX log (TF+1)+YX f∈Y),T1 +lθ+δ
(11)
Focusing on the denominator of the last term in (11), we have
inf jvj,i + δ ≤ 1v1,i + δ ≤ G∞+δ
j∈[1,T]	,	,
thus
a X (1 一 Y)(1 + logT) ≥ dα (1 一 Y)(logT + 1)
Y i=ι inf j∈[1,T] jvj,i + δ — Y	G∞ + δ
which implies that their regret bound is of order O(dlogT), and thus data-independent. In contrast,
based on Corollary 2, we present a new regret bound for SC-RMSprop in the following, which is
O(Pid=1log(kg1:T,ik2 )), and thus data-dependent.
Corollary 3. Suppose Assumptions 1 and 2 hold, and all loss functions fι(∙),...,fτ(∙) are λ-
strongly convex. Let δ > 0, βιt = 0, and 1 一 ɪ ≤ β2t ≤ 1 一券,where Y ∈ (0,1]. Let a ≥ (2-Y)G∞.
Then SAdam reduces to SC-RMSprop, and its regret satisfies
R(T) ≤dD∞δ + Y X log(γ X j + 1).	(12)
i=1	j=1
Finally, we note that Mukkamala & Hein (2017) also consider a more general version of SC-RMSprop
which uses a time-variant non-increasing δ for each dimension i. In Appendix D we introduce the
δ-variant technique to our SAdam, and provide the corresponding theoretical guarantee.
4 Experiments
In this section, we present empirical results on optimizing strongly convex functions and training
deep networks. More results can be found Appendix.
Algorithms. In both experiments, we compare the following algorithms:
6
Published as a conference paper at ICLR 2020

0.2	0.4	0.6	0.8	1.0
Dataset proportion
(a) MNIST
3,
O -
1 .
0.2	0.4	0.6	0.8	1.0
Dataset proportion
(b) CIFAR10
0.2	0.4	0.6	0.8	1.0
Dataset proportion
(c) CIFAR100

Fig. 1: Regret v.s. data proportion for '2-regularized Softmax regression
•	SC-Adagrad (Mukkamala & Hein, 2017), with step size α= = a∕t.
•	SC-RMSProP (Mukkamala & Hein, 2017), with step size at = α∕t and βt = 1 - 0.9.
•	Adam (Kingma & Ba, 2015) and AMSgrad (Reddi et al., 2018), both with βι = 0.9,
β2 = 0.999, at = ɑ∕√t for convex problems and time-invariant at = ɑ for non-convex
problems.
•	AdamNC (Reddi et al., 2018), with βι = 0.9, β2t = 1 - 1∕t, and at = a∕√t for convex
problems and a time-invariant at = a for non-convex problems.
•	Online Gradient Descent (OGD), with step size at = a∕t for strongly convex problems and
a time-invariant at = a for non-convex problems.
•	Our proposed SAdam, with βι = 0.9, β2t = 1 - 0.9.
For Adam, AdamNC and AMSgrad, we choose δ = 10-8 according to the recommendations in
their papers. For SC-Adagrad and SC-RMSprop, following Mukkamala & Hein (2017), we choose a
time-variant δt,i = ξ2e-ξ1tvt,i for each dimension i, with ξ1 = 0.1, ξ2 = 1 for convex problems and
ξ1 = 0.1, ξ2 = 0.1 for non-convex problems. For our SAdam, since the removing of the square root
procedure and very small gradients may cause too large step sizes in the beginning iterations, we use
a rather large δ = 10-2 to avoid this problem. To conduct a fair comparison, for each algorithm, we
choose a from the set {0.1, 0.01, 0.001, 0.0001} and report the best results.
Datasets. In both experiments, we examine the performances of the aforementioned algorithms on
three widely used datasets: MNIST (60000 training samples, 10000 test samples), CIFAR10 (50000
training samples, 10000 test samples), and CIFAR100 (50000 training samples, 10000 test samples).
We refer to LeCun (1998) and Krizhevsky (2009) for more details of the three datasets.
4.1 Optimizing S trongly Convex Functions
In the first experiment, we consider the problem of mini-batch '2-regularized softmax regression,
which belongs to the online strongly convex optimization framework. Let K be the number of classes
and m be the batch size. In each round t, firstly a mini-batch of training samples {(xm, ym)}im=1
arrives, where yi ∈ [K], ∀i ∈ [m]. Then, the algorithm predicts parameter vectors {wi, bi}iK=1, and
suffers a loss which takes the following form:
1 m	ewy>ixi+byi	K	K
J(W㈤=-mXlog	PK , wMbj	+ λ1 X kwkk2+ λ2 Xbk.
i=1	j=1 e j	k=1	k=1
The value of λ1 and λ2 are set to be 10-2 for all experiments. The regret (in log scale) v.s. dataset
proportion is shown in Fig. 1. It can be seen that our SAdam outperforms other methods across all
the considered datasets. Besides, we observe that data-dependent strongly convex methods such as
SC-Adagrad, SC-RMSprop and SAdam preform better than algorithms for general convex functions
such as Adam, AMSgrad and AdamNC. Finally, OGD has the overall highest regret on all three
datasets.
7
Published as a conference paper at ICLR 2020
Epoch
(c) CIFAR100
(a) MNIST	(b) CIFAR10
0.99
>uflh3uu<4jsωl
---SAdam
---SC_ Ada grad
——SJRMSPrOP
OGD
---Adam
AdamNC
---Amsgrad
10	20	30	40	50
Epoch
(a) MNIST
Fig. 2: Training loss v.s. number of epochs for 4-layer CNN
>uflh3uu<4jsωl
80	100
20	40	60
Epoch
(b) CIFAR10
>uflh3uu<4jsωl
20	40	60	80	100
Epoch
(c) CIFAR100
Fig. 3: Testing accuracy v.s. number of epochs for 4-layer CNN
4.2 Training Deep Networks
Following Mukkamala & Hein (2017), we also conduct experiments on a 4-layer CNN, which consists
of two convolutional layers (each with 32 filters of size 3 × 3), one max-pooling layer (with a 2 × 2
window and 0.25 dropout), and one fully connected layer (with 128 hidden units and 0.5 dropout).
We employ ReLU function as the activation function for convolutional layers and softmax function
as the activation function for the fully connected layer. The loss function is the cross-entropy. The
training loss v.s. epoch is shown in Fig. 2, and the testing accuracy v.s. epoch is presented in Fig. 4.
As can be seen, our SAdam achieves the lowest training loss on the three data sets. Moreover, this
performance gain also translates into good performance on testing accuracy. The experimental results
show that although our proposed SAdam is designed for strongly convex functions, it could lead to
superior practical performance even in some highly non-convex cases such as deep learning tasks.
5	Conclusion and Future Work
In this paper, we provide a variant of Adam adapted to strongly convex functions. The proposed
algorithm, namely SAdam, follows the general framework of Adam, while keeping a step size
decaying in general on the order of O(1/t) and controlled by data-dependent heperparameters
to exploit strong convexity. Theoretical analysis shows that SAdam achieves a data-dependent
O(dlogT) regret bound for strongly convex functions, which means that it converges much faster
than Adam, AdamNC, and AMSgrad in such cases, and can enjoy a huge gain in the face of sparse
gradients. In addition, we also provide the first data-dependent logarithmic regret bound for SC-
RMSprop. Finally, we test the proposed algorithm on optimizing strongly convex functions as well
as training deep networks, and the empirical results demonstrate the effectiveness of our method.
Since SAdam enjoys a data-dependent O(d log T) regret for online strongly convex optimization, it
can be easily translated into a data-dependent O(dlogT/T) convergence rate for stochastic strongly
convex optimization (SSCO) by using the online-to-batch conversion (Kakade & Tewari, 2009).
However, this rate is not optimal for SSCO, and it is sill an open problem how to achieve a data-
dependent O(d/T) convergence rate for SSCO. Recent development on adaptive gradient method
(Chen et al., 2018) has proved that Adagrad combined with the multi-stage scheme (Hazan & Kale,
2014) can achieve this rate, but it is highly non-trivial to extend this technique to SAdam, and we
leave it as a future work.
8
Published as a conference paper at ICLR 2020
6	Acknowledgement
This work was partially supported by NSFC (61976112), NSFC-NRF Joint Research Project
(61861146001), and the Collaborative Innovation Center of Novel Software Technology and In-
dustrialization.
References
Parnia Bahar, Tamer Alkhouli, Jan-Thorsten Peter, Christopher Jan-Steffen Brix, and Hermann Ney.
Empirical investigation of optimization algorithms in neural machine translation. The Prague
Bulletin of Mathematical Linguistics, 108(1):13-25, 2017.
Amitabh Basu, Soham De, Anirbit Mukherjee, and Enayat Ullah. Convergence guarantees for
rmsprop and adam in non-convex optimization and their comparison to nesterov acceleration on
autoencoders. arXiv preprint arXiv:1807.06766, 2018.
Sebastian Bock, Josef Goppold, and Martin Wei. An improvement of the convergence proof of the
adam-optimizer. arXiv preprint arXiv:1804.10587, 2019.
Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.
Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong. On the convergence of a class of adam-type
algorithms for non-convex optimization. In Proceedings of the 7th International Conference on
Learning Representations, 2019.
Zaiyi Chen, Yi Xu, Enhong Chen, and Tianbao Yang. Sadagrad: Strongly adaptive stochastic gradient
methods. In Proceedings of 35th International Conference on Machine Learning, pp. 912-920,
2018.
Michael Denkowski and Graham Neubig. Stronger baselines for trustable results in neural machine
translation. In Proceedings of the 1st Workshop on Neural Machine Translation, pp. 18-27, 2017.
Timothy Dozat. Incorporating nesterov momentum into adam. In Proceedings of 4th International
Conference on Learning Representations, Workshop Track, 2016.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12:2121-2159, 2011.
Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Jimenez Rezende, and Daan Wierstra. Draw: a
recurrent neural network for image generation. In Proceedings of the 32nd International Conference
on Machine Learning, pp. 1462-1471, 2015.
Elad Hazan and Satyen Kale. Beyond the regret minimization barrier: optimal algorithms for
stochastic strongly-convex optimization. Journal of Machine Learning Research, 15:2489-2512,
2014.
Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex
optimization. Machine Learning, 69:169-192, 2007.
Elad Hazan et al. Introduction to online convex optimization. Foundations and TrendsR in Opti-
mization, 2(3-4):157-325, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the 29th IEEE Conference on Computer Vision and Pattern
Recognition, pp. 770-778, 2016.
Sham M Kakade and Ambuj Tewari. On the generalization ability of online strongly convex
programming algorithms. In Advances in Neural Information Processing Systems 21, pp. 801-808,
2009.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings of
3rd International Conference on Learning Representations, 2015.
9
Published as a conference paper at ICLR 2020
Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. Skip-thought vectors. In Advances in Neural Information Processing Systems 27,
pp. 3294-3302, 2015.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, Citeseer,
2009.
Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998.
H Brendan McMahan and Matthew Streeter. Adaptive bound optimization for online convex op-
timization. In Proceedings of the 23rd Annual Conference on Learning Theory, pp. 224-256,
2010.
Mahesh Chandra Mukkamala and Matthias Hein. Variants of rmsprop and adagrad with logarithmic
regret bounds. In Proceedings of the 33rd International Conference on Machine Learning, pp.
2545-2553, 2017.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In
Proceedings of 6th International Conference on Learning Representations, 2018.
Shai Shalev-Shwartz et al. Online learning and online convex optimization. Foundations and
TrendsR in Machine Learning, 4(2):107-194, 2012.
Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost.
In Proceedings of the 35th International Conference on Machine Learning, pp. 4596-4604, 2018.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning, pp. 26-31,
2012.
Phuong Thi Tran et al. On the convergence proof of amsgrad and a new version. IEEE Access, 7:
61706-61716, 2019.
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich
Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual
attention. In Proceedings of 32nd International Conference on Machine Learning, pp. 2048-2057,
2015.
Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701,
2012.
Jiawei Zhang, Limeng Cui, and Fisher B Gouza. Gadam: Genetic-evolutionary adam for deep neural
network optimization. arXiv preprint arXiv:1805.07500, 2018.
Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In
Proceedings of the 20th International Conference on Machine Learning, pp. 928-936, 2003.
10
Published as a conference paper at ICLR 2020
A Proof of Theorem 1
From Definition 1, we can upper bound regret as
T	T	(5) T	λ
R(T) = Eft(Xt)- Eft(X*)≤ £g> (Xt-X*)- 2 kxt -x*k2
(13)
t=1
t=1
t=1
where x* := minx∈D PT=I ft(x) is the best decision in hindsight. On the other hand, by the update
rule of xt+1 in Algorithm 1, we have
kxt+ι - x*kvt=∣∣πVt (Xt- at%-%) -x*∣∣^
≤kxt - atVT1gt - x*kVt
=- 2atg> (xt - x*) + kxt - x*kvt + a2 值t||V-ι
(14)
=-2αt(BltWt-1 + (I - βIt)gt) (Xt - XJ
+ kxt- x*kVt+α2llgtllV-ι
where at := α∕t, and the inequality is due to the following lemma, which implies that the weighted
projection procedure is non-expansive.
Lemma 1. (McMahan & Streeter, 2010) Let A ∈ Rd×d be a positive definite matrix and D ⊆ Rd be
a convex set. Then we have, ∀x, y ∈ Rd ,
kΠAD(x)-ΠAD(y)kA≤ kx - ykA.
(15)
Rearranging (14), we have
g>(xt - x*) ≤
kxt - x*kVt -kxt+1 - x*k
2αt(1 - β1t)
ɪ J g3(x*-xt)
αt
2(1 - β1t)
k^tkV--1
kxt - x*kVt - kxt+1 - x*k
2αt(1 - β1t)
2
-J g3(x*-xt)
(16)
αt
2(1 - β1)
k^tk^-ι
≤
+
+
where the last inequality is due to β1t ≤ β1. We proceed to bound the second term of the inequality
above. When t ≥ 2, by Young’s inequality and the definition of β1t, we get
τ⅛ gL(x*-xt) ≤
αt-1β1t
2(1 - β1t)
αt-1
k^t-lkV-ι +
β1t
≤ 2(1-βι)
kgt-lkV-ι +
2αt-1 (1 - β1t)
β1t
2αt-1 (1 - β1t)
kxt- x*kVt-1
(17)
kxt- x*kVt-1.
When t = 1, this term becomes 0 since ^0 = 0 in Algorithm 1. Plugging (16) and (17) into (13), We
get the following inequality, of which we divide the righthand side into three parts and upper bound
each of them one by one.
R(T)<XX k kxt - x*kVt -kxt+1 - x*kVt	λ∣∣x x ∣∣2
RT )≤	2αt(1-βιt)	2 kxt-x*k
'--------------------------------------
}
{z
P1
1 晨	2	Pt=I atkgtkV-1	晨
+ 2(1 - βι) §at-lkgt-lkV-1 + -2(1 - βι) 4	+ ∑
X-------------------------------------J X—
β1t
{z
P2
2αt-1(1 - β1t)
\^^~
P3
kxt-x*kVt-ι∙
11
Published as a conference paper at ICLR 2020
To bound P1 , we have
kx1 - x*kVι - kxT +1 - x*kVτ - XX ( λ
2αι(1 — βι)	2ατ(1 — βιτ)	∖2
--------{z------} t=1
≤0
+XX (kxt—χ*kVt -	kχt-x*kVt-1	!
t=2 12αt(I- βIt)	2αt-1(1 - β1(t-1)) J
≤kx1-x*% -XF(λkx	-x宿+XF(kxt-x*kVt	kxt-x**-ι
—2αι(1 — βI)	= V2 t *	) 与 1 - β1t 1	2αt	2αt-1
T
F
t=2
2α(1 — β1t)
(tkxt - x*kVt - (t - I) kxt - x*kVt-ι - λα(I - βIt) kxt - x*k2
1
+ (2≡⅛) - 2kx1-x*k2
(18)
where the inequality is derived from β1(t-1) ≥ β1t. For the first term in the last equality of (18), we
have
tkxt - x*kVt - (t - 1)kxt - x*kVt-ι - λα(I - β1t)kxt - x*k2
d
=^X(Xt,i - x*,i )2(tvt,i - (t - 1)vt-1,i - λα(1 - β1t))
i=1
d
(xt,i — x*,i)2 (tvt,i — (t — 1)vt-1,i — λα(1 — β1t))
i=1
(19)
(8) d
≤	(xt,i —
i=1
x*,i)2 (αλ(1 — β1) — λα(1 — β1t)) ≤ 0
where the second equality is because Vt,i =vt,i + δ, and the second inequality is due to 1 — β1 ≤
1 — β1t .
For the second term of (18), we have
(kx1 - x*kV1
(2αι(1 - Bi)
-2kxι - x*k2
d
(x1,i — x*,i)2
i=1
v1,i - λα(1 - Bi) )
2α(1 - βι)	)
d
(x1,i — x*,i)2
i=1
vi,i — λα(1 — Bi) + δ
2α(1 - β1)
d
≤	(xi,i — x*,i)2
i=i
δ
2α(1 - β1)
(20)
≤ dD∞ δ
-2α(1 - βι)
where first inequality is due to Condition 3, and the second inequality follows from Assumption 2.
Combining (18), (19) and (20), we get
Pl ≤ 2 dD∞δ)∙	(21)
2α(1 — Bi)
To bound P2 , we introduce the following lemma.
Lemma 2. The following inequality holds
T	dT
X atkgtkVt-1 ≤ (1-⅛ Xlog ( ζ1δ Xgj + 1∣ ∙	(22)
12
Published as a conference paper at ICLR 2020
By Lemma 2, we have
1 G	2	PL atkgtkV--1
P2 = 2(1 - β1) ^otTkgtTk Vtu + ~2(1 - β1)
1T
≤ (1 - β1) X αt kgtkVt-1
(23)
(22)
≤
d
(1⅛ X log
T
Xgj2,i+1
j=1
Finally, we turn to upper bound P3 :
T
P3=X
t=2
2αt-1(1 - βIt) " t
x*kVt
dT
XX
i=1 t=2
βlt
2α(I- βIt)
-x*,i)2(t - I)Vt-1,i
≤ D∞ (G∞ + 6 X X Bit t
≤ 一2α — Z ==工
≤ βD∞ (G∞ + 6 X X VtT t
≤ —2α — M N 厂面
β D∞(G∞ + 6
2α(1 -尸I)
d T-1
XXνt(t+1).
i=1 t=0
'---{----}
P30
To further bound P30, following Bock et al. (2019), we have
T-1
P30 =Xνtt+νt
t=0
_ ((T - 1)νt +1 - TVt + v 1 - vt \
(	(V-1)2	+ 1-VJ
1 - T(vT - VT +1) - vT
=	O-^P
/	1
≤ (1 - V)2
where the inequality follows from VT ≥ VT+1 . Thus,
P V dβ1D∞(G∞ + 6
3 ≤ 2α(1- βi)(V -1)2 .
We complete the proof by combining (21), (23) and (25).
B Proof of Corollary 2
For the first condition, we have
tvt,i - (t - 1)vt-1,i =tβ2tvt-1,i + t(1 - β2t)gt2,i - (t - 1)vt-1,i
≤t (1 - ;) vt-1,i + ttg2,i - (t - 1)vt-1,i
≤(t - γ - (t - 1))vt-1,i + G2∞
≤(2-γ)G2∞
(24)
(25)
(26)
13
Published as a conference paper at ICLR 2020
where the first inequality is derived from the definition of β2t , the second and the third inequalities are
due to Assumption 1. Based on (26), for any α ≥ (λ--)G∞, We have tv-t,i - (t-Djii ≤ λ(1 - βι)
holds for all t ∈ [T] and i ∈ [d].
For the second condition, We have
t t-j	t t-j
t X Y β2(t-k+1) (1 - β2j )j ≥t XY 1 - t-⅛
j=1 k=1	j=1 k=1
t - k Y 2
t - k +1 jgj,i
(27)
t t-j
=tXY
j=1 k=1
t
=γXgj2,i
j=1
where the inequality follows from βt ≥ 1 - 1 and 1 - βt ≥ Y.
C Proof of Lemma 2
We begin with the following lemma that is central to our analysis.
Lemma 3. For all i ∈ [d] and t ∈ [T], we have
(28)
Proof. For any a ≥ b > 0, the inequality 1 +x ≤ ex implies that
1a
a (a - b) ≤ log b.
(29)
Let m0 = ζδ, and mj = Pjk=1 gk2,i + ζδ > 0. By (29), we have
gj,i	= mj - mj-1 ≤ log mj
Pk=I g2,i + ζδ — mj - mj-1
Summing over 1 to T , we have
X gj,i	— mt 1
S Pk=I g2,i + Zδ ≤ log mo = log
+1
Now we turn to the proof of Lemma 2. First, expending the last term in the summation by the update
rule of Algorithm 1, we get
αT kgT 信 1 = αTX v⅛
d	PjT=1(1 - β1j) QkT=-1j β1(T -k+1)gj,i
α
i=1 T Pj=1(1 - β2j)Πk=1 β2(T -k+1)gj,i + δ
(30)
□
14
Published as a conference paper at ICLR 2020
The above equality can be further bounded as
d
αΤ kgT kV-1 ≤α X
T	i=1
d
≤αX
i=1
d
≤αX
i=1
2
PjT=1 QkT=-1j β1(T -k+1)gj,i
T PT=I(I- β2j)πT=l β2(Τ-k + 1)gj,i + δ
PjT=1QkT=-1j β1(T -k+1)PjT=1QkT=-1j β1(T-k+1)gj2,i
T PT=I(I- β2j)πT=l β2(T-k+1)gj,i + δ
PjT=1β1T-jPjT=1QkT=-1j β1(T-k+1)gj2,i
(31)
T PT=I (I- β2j )πT=l β2(T-k+1)gj,i + δ
α
d
(1 - βI)工 T PT=I(I- β2j)πkT=1 β2(Τ-k + 1)gj,i + δ
≤)	aZ X PT=1 βT - g2,i ≤ aZ X X βT-j	g2,i
一(I - βI) = Pj=I gj,i + ζδ — (I- βI) i=ι j=ι 1	Pk=I g2,i + ζδ
αζ
αζ
where the first inequality is due to 1 - β1j ≤ 1, the second inequality follows from Cauchy-Schwarz
inequality, the third inequality is due to β1t ≤ β1. Let rj = gj2,i/(Pjk=1 gk2,i + ζδ). Using similar
arguments for all time steps and summing over 1 to T, we have
αζ
鼻⑷置员-1 ≤ (1 - β1)
αζ
(1 - β1 )
αζ
(1 - β1 )
dTt
XXXβ1t-jrj
i=1 t=1 j=1
d T T-j
XXXβ1lrj
i=1 j=1 l=0
X X	PTO β1 gj
i=1 j=ι Pk=I g2,i+ζδ
(32)
αζ
≤ (1-βι)
(28) αZ
≤ (1 - βι)
dT
2XX
i=1 j=1
d
2 X log
i=1
Pjk=1 gk2,i + ζδ
+1
T
D SAdam with a decaying regularization factor
Algorithm 2 SAdam with time-variant δt (SAdamD)
1： Input: {β1t}T=1, {β2t}T=1, {δt}T=1
2: Initialize: ^0 = 0, V⅛ = 0d×d, x1 = 0.
3： for t = 1, . . . , T do
4：	gt = Vft(Xt)
5:	gt = β1tgt-1 +(1 - β1t)gt
6:	Vt = β2tVt-1 + (1 - β2t)diag(gtgt>)
7：	Vt = Vt + diag 件)
8：	χt+1 = ∏Vt (Xt- αVrIgt)
9: end for
In this section, we establish a generalized version of SAdam, which employs a time-variant regulariza-
tion factor δt,i for each dimension i, instead of a fixed one for all i ∈ [d] and t ∈ [T] as in the original
SAdam. The algorithm is referred to as SAdamD and summarized in Algorithm 2. It can be seen that
15
Published as a conference paper at ICLR 2020
our SAdamD reduces to SC-RMSProP with time-variant δt when βιt = 0 and 1 - -ɪ ≤ β2t ≤ 1 -券,
For SAdamD, we prove the following theoretical guarantee:
Theorem 4. Suppose Assumptions 1 and2 hold, and all lossfunctions fι(∙),...,fτ (∙) are λ-strongly
convex. Let {δt,i}tT=1 ∈ (0, 1]T be a non-increasing sequence for all i ∈ [d], β1t = β1νt-1 where
β1 ∈ [0, 1), ν ∈ [0, 1), and {β2t}tT=1 ∈ [0, 1]T be a parameter sequence such that Conditions 3 and
4 are satisfied. Let ɑ ≥ C. The regret of SAdamD satisfies
R(T) ≤
D∞ Pi= Mi
2α(I - βI)
d
+ ∏⅛ X log
ζ⅛ X g2,i+ 1
β1D∞ (dG∞ + Pd=I δ1,i)
+ —2α(1 - βι)(ν - 1)2—
(33)
By setting βιt = 0 and 1 - 1 ≤ β2t ≤ 1 - Y, we can derive the following regret bound for
SC-RMSProP:
Corollary 5. Suppose Assumptions 1 and 2 hold, and all loss functions fι(∙),...,fτ(∙) are λ-
StrongIy convex. Let {δt,i}T=I ∈ (0, 1]t be a non-increasing sequencefor all i ∈ [d], and 1 一 -ɪ ≤
β2t ≤ 1 — γY, where Y ∈ (0,1]. Let α ≥ (2-Y)g∞ . Then SAdamD reduces to SC-RMSprop, and the
regret satisfies
R(T) ≤ Do P=1	"i	+ α	X log	(ɪ	X gj	+ ] .	(34)
2α	γ	i=1	δT,i	j=1 j,i
Finally, we Provide an instantiation of δt and derive the following Corollary,
Corollary 6. Suppose Assumptions 1 and 2 hold, and all loss functions fι(∙),...,fτ(∙) are λ-
strongly convex. Let δt,i = 中】P2-r,where ξ2 ∈ (0,1] and ξι ≥ 0 are hypeparameters. Then
we have δt,i ∈ (0,1] and is non-increasing ∀i ∈ [d],t ∈ [T]. Let 1 一 1 ≤ β2t ≤ 1 一 +, where
γ ∈ (0,1], and α ≥ (2-Y)g∞ . Then SAdamD reduces to SC-RMSprop, and the regret satisfies
RT) ≤ * + Y X log(γ X g2,i+ ξ2) + YXlOgE X g2,i+ ξ2) .	(35)
E Proof of Theorem 4
By similar arguments as in the Proof of Theorem 1, we can uPPer bound regret as
R(T)<X QXt-X*kVt-kxt+ι-x*kVt	λllx x ll2
RT )≤ ⅛l,	2αt(1-βιt)	2 kxt-x*k
X-------------------------------------
{z
P1
1 G	2	P= αtkgtkV- -1
+ 2(1 - βι)与 MTkgtTkVI + -2(1 - βι)'
X-------------------------------
-z
P2
T
+ X 2αt-i(1 - βιt) kxt- x*kVt-ι∙
S--------------------------------}
P3
To bound P1 , based on (18), we have
T1
PI ≤ E 2α(1 - β11) (tn* - x*K - (t - 1)kχt - x*kVt-ι - λα(1 - βIt)kχt - xJ2)
(ι∣xι- XjVI
(2ɑι(I- βI)
-2l∣xι - xJ2
(36)
}
}
16
Published as a conference paper at ICLR 2020
For the first term in (36), we have
tkxt - x*kVt - (t - 1)kxt - x*kVt-ι - λα(I - βIt) kxt - x*k2
d
二 ^X(Xt,i - x*,i)2(tvt,i - (t - I)Vt-1,i - λα(1 - βIt))
i=1
d
y^(xt,i - x*,i)2(tvt,i - (t - I)Vt-1,i - λα(1 - βIt) + δt,i - δt-1,i)
i=1
(37)
d
≤	(xt,i -
x*,i)2(λα(1 - β1) - λα(1 - β1t) + δt,i - δt-1,i)
X------------------{-----------} '-----V-----}
≤0	≤0
≤0
For the second term of (36), we have
(kx1 - x*kV1
(2αι(1 - βι)
-2kχι - x*k2
d
E(xι,i-x*,i)2
i=1
v1,i - λα(1 - β1)
2α(1 - β1)
d
E(xι,i-x*,i)2
i=1
v1,i - λα(1 - β1) + δ1,i
2α(1 - β1)
(38)
≤ D∞ P= Mi
2α(1 - β1)
where the inequality follows from Condition 3. Combining (36), (37) and (38), we have
P ≤ D∞ P= Mi
1 ≤ 2α(1 - βι).
(39)
To bound P2 , we first introduce the following lemma.
Lemma 4. The following inequality holds
Xαtk∙tkVτ ≤ (1 -β1)
W log (ζδT- X g2,i+1∣.
(40)
The proof of Lemma 4 can be found in Appendix F. Based on Lemma 4, we have
1 X iia ll2 l P= αtk^tkVt
2(1 - β1) 2->αt-lkgt-1 kVt-1ι +	2(1 - β1)
1T
≤ (1 - β1) X ɑt kgtkVT
(41)
(40)
≤
d
(1⅛ X log
T
Xgj2,i+1
j=1
17
Published as a conference paper at ICLR 2020
Finally, we turn to upper bound P3 :
dT β
P3 ≤ X X 2ɑ(f-tβιt) (xt,i - x*,i)2tvt,i
≤ T X X i⅛t(G∞ + δι,i)
i=1 t=1	1t
≤ βD∞ X X 二t(G∞ + Mi)
2α	i=1 t=1 1 - β1
=β1D∞ X (
-2ɑ 上一
i=1
(24) βιD∞∞ X
一 2ɑ乙
i=1
(G2∞ + δ1,i)
1 - β1
T-1
Xνt(t+1)
t=0
(42)
(G∞ + δ1,i)
(1 - βι)(ν - 1)2
β1D∞(UG∞ + Pd=I δ1,i)
2α(1 - βι)(ν - 1)2
We finish the proof by combining (39), (41) and (42).
F Proof of Lemma 4
Expending the last term in the summation by using the update rule of Algorithm 2, we have
d	g2	d
ɑTkgT k^ =αT x vT-⅛=α x
PjT=1(1 - β1j ) QkT=-1j β1(T -k+1)gj,i
T PT=I(I - β2j)πT=l β2(T-k+1)gj,i + δT,i
(43)
The above equality can be further bounded as
d
αTk&TkV-1 ≤α X
T	i=1
PjT=1 QkT=-1j β1(T-k+1)	PjT=1 QkT=-1j β1(T-k+1)gj2,i
d
≤αX
i=1
T PT=I(I- β2)nT-Ijβ2(T-k + 1)gj,i + δT,i
PjT=1β1T-j	PjT=1 QkT=-1j β1(T-k+1)gj2,i
T PT=I(I - β2j)πT = l β2(T-k+1)gj,i + δT,i
d
一(I- βI)匚 TPT=I(I- β2j)nT-jβ2(T-k+1)g2,i + δT,i
≤ aZ X Pj=ι βT j gj,i
一(I- βI) h Pj=ι g2,i + ZδT,i
α
(44)
αζ
≤ aZ X X βT-j ,	g2,i
一(1-β1) i=1 j=1	1	Pk=I g2,i + ζδT,i'
The first inequality follows from Cauchy-Schwarz inequality and 1 - β1t ≤ 1, and the second
inequality is due to βιt ≤ βι. Let rj = Pj-h十的—.By using similar arguments as in (32), We
18
Published as a conference paper at ICLR 2020
have
T
αζ
占 αtkgtk VtT ≤ (1 - β1)
≤ αζ
一(1 - βι)
αζ
(1 - β1)
αζ
≤ (1-βι)
X XXe 厂 jfc-
X X X βT-j Pj	F+ ⑼
i=1 t=1 j=1	k=1 gk,i +ζδT,i
dTt
XXXrj
i=1 t=1 j=1
X X	PTO β1 无
=⅛ Pk=I 脸+①
(45)
≤	°ζ	X X__________j________
一(1 - βI)2 = j=1 Pjk=I 次,i + ζδτ,i
(28)
αζ
≤ (1 - βι)
2 x log (士 X 疯+ 1
G On the convergence of Amsgrad
In this section, we firstly provide the AMSgrad algorithm and its theoretical guarantees (Reddi et al.,
2018), then state a theoretical flaw in their analysis revealed by Tran et al. (2019), and finally propose
a simple solution to fix this problem.
The AMSgrad algorithm developed in Reddi et al. (2018) is summarized in Algorithm 3.
Algorithm 3 AMSgrad
1:	Input: {β1t}tT=1, β2
2:	Initialize: ^0 = 0, VQ = 0d×d, xι = 0.
3:	for t = 1, . . . , T do
4：	gt = Vft(Xt)
5:	gt = β1tgt-1 + (1 - βIt)gt
6:	Vt = β2Vt-1 + (1 - β2)diag(gtgt>)
7：	Vt = max{Vt, Vt-i}
8：	xt+ι = ∏DVt (Xt — at%-"gj,where at = √
9: end for
For AMSgrad, Reddi et al. (2018) provide the following regret bound.
Theorem 7 (Theorem 4 in Reddi et al. (2018), problematic). Suppose Assumptions 1 and 2 hold, and
all loss functions fι (•),..., fτ(∙) are convex. Let δ > 0, βι > 0, βιt ≤ βι, and Y = √β= ≤ 1. The
regret of AMSgrad satisfies
RT) ≤ O(M)X vτ/2+
D∞2
2(1 - β1)
Td
Xt=1Xi=1
βιtV1,/2
at
a √1 + log T
(I - βl)2(1 - γ)p(1 一尸2)
d
X kg1:T,i k.
i=1
(46)
+
19
Published as a conference paper at ICLR 2020
Recently, Tran et al. (2019) point out a mistake in the proof of Theorem 7. Specifically, in Reddi et al.
(2018), the following inequality is utilized (Proof of Lemma 2, Page 18):
T
X
t=1
2αt(i1-βιt)［陀 1/4(Xtf)k2 TM"(Xt+1 - x*)k2］
+_____β1t
2αt(1 - βIt)
kVt1∕4(x1
α √1 + log T
(I - β1 )2(1 - Y)P(I - β2)
d
X kg1:T,i k
i=1
1
-2α1(1 - βI)
1T
-x*)k +2(1- βι) X=2
IM1∕4(xt-x*)k2	kVt-/4(Xt-X*)k2
----------------	
αt-------------αt-1
T
+X
t=1
β1t
2αt(1 - βI)
陀1/4 (xt -
xJk2
α √1 + log T
(I- β1)2(1 - Y)(√1 - β2)
d
X	kg1:T,i k
i=1
(47)
+
+
which, however, may not hold. To see this, we note that essentially (47) uses
T
X 2αt(1-β1t)［陀1/4(Xt-X*)k2 -kVt1∕4(xt+1 -x*)k2i
T
≤ (T-IT) X 江 hkVt1/4(Xt-X*)k2 -k Vt1/4(xt+1- x*)k2i
(48)
which holds only if β1t ≤ β1 and ||匕1/4(Xt - x*)k2 - kVt1/4(xt+1 - x*)k2 is non-negative.
However, as empirically shown by Tran et al. (2019), the letter requirement can be violated in some
counterexamples. Note that similar problems exist in many recent proposed Adam variants. To
address this issue, Tran et al. (2019) establish a new convergence proof of AMSGrad, which indicates
an O(d√T) data-independent regret bound. Moreover, as an alternative, they also propose a variant
of AMSgrad, called AdamX, which alters the stricture of AMSgrad to force the inequality been
satisfied. For AdamX, they also give a new theoretical analysis and an O(d√T) data-independent
regret bound.
In this paper, we find out that the above problem can be solved by simply configuring β1t of AMSgrad
in a non-increasing manner, i.e., ∀t ≥ 2, β1t ≤ β1(t-1). Specifically, when β1t is non-increasing, we
can rewrite (47) as
T
X
t=1
2O⅛ hkVt1/4(Xt-X*)k2-kVt1/4(Xt+1-x*)k2i
+_____β1t
2αt(1 - β1t)
kVi1∕4(xt-x*)k2
α √1 + log T
(I - β1 )2(1 - Y)p(1 - β2)
T
X kg1:T,ik
i=1
+
≤	1	kV l/4(x _ X )k2 + T X kVt1/% Xt - X*)k2 _ kVt-/l (Xt - 3H2
I 2α1(l - β1) 1	1	*	2 t=2	at(l - β1t)	αt-i(l - β1(t-l))
T
+X
t=1
β1t
2αt(1 - β1)
kvt1/4(Xt-X*)k2
α √1 + log T
(I- β1)2(1 - Y)(√1 - β)
d
X kg1:T,i k
i=1
-2α1(1 - β1)
T
kV11∕4(x1-x*)k2 + 2 X
2 t=2
』匕 1∕4(xt-x.)k2 IIVL/4(xt-x*)k2
αt(1 - β1t)
αt-1(1 - β1t)
T
+X
t=1
β1t
2αt(1 - β1)
kvt1/4(Xt-X*)k2
α √1 + log T
(I- β1)2(1 - Y)(√1 - β)
d
X kg1:T,i k
i=1
+
1
—
+
20
Published as a conference paper at ICLR 2020
1
-2αl(1 - βI)
1T
- x*)k2 + 2(1 — βι) tXχ
I 它/4(xt-x*)k2	kV/4(Xt-X*)k2
---------------	
αt------------αt-1
T
+X
t=1
βιt
2αt(I- βI)
kVt1/4 (Xt-Xjk2
α √T÷TogT
(I- β1)2(1 - Y)(√1 - β)
d
X kg1:T,i k
i=1
+
where the second inequality is derived from β1t ≤ β1(t-1), and the last inequality is due to the fact
that βιt ≤ βι and kVt / (：1x*)' - 优-1：：：* is non-negative. In this way, the proof of
AMSgrad can proceed, and the algorithm structure as well as the conclusion in Theorem 7 remain
unchanged.
To summarize, we restate Theorem 7 as follows.
Theorem 8 (Fixed theoretical guarantee of AMSgrad). Suppose Assumptions 1 and 2 hold, and all
loss functions fι(•),..., fτ(∙) are convex. Let δ > 0, βι > 0, βιt ≤ βi(t-ι) where t ≥ 2, βιι 二 βι,
and γ = -√= ≤ 1. The regret of AMSgrad satisfies
R(T) ≤
D∞ √T
α(1 - β1)
d
X vT/2
i=1
D∞
2(1 - βι)
Td
Xt=1Xi=1
βιtV1,/2
αt
α √1 + log T
(1 - βl)2(1 - γ)p(1 一 尸2)
d
X kg1:T,ik.
i=1
(49)
+
+
H Experiments on ResNet 1 8
Epoch
(a) Training Loss v.s. number of epochs
Fig. 4: experimental results of ResNet18 on CIFAR10 dataset
Epoch
(b) Testing accuracy v.s. number of epochs
In this section, we conduct the image classification task on CIFAR10 dataset using ResNet18 (He
et al., 2016). The parameter configuration of algorithms is the same as that in Section 4. We repeat
each experiment 10 times and take their average. The training loss v.s. epoch is shown in Fig. 4a, and
the testing accuracy v.s. epoch is shown in Fig. 4b. As can be seen, our proposed SAdam outperforms
other algorithms.
21