Published as a conference paper at ICLR 2020
Meta Reinforcement Learning with
Autonomous Inference of Subtask Dependencies
Sungryull Sohn1	Hyunjae Woo1	Jongwook Choi1	Honglak Lee2,1
1 University of Michigan	2Google Brain
{srsohn,hjwoo,jwook}@umich.edu	honglak@google.com
Ab stract
We propose and address a novel few-shot RL problem, where a task is characterized
by a subtask graph which describes a set of subtasks and their dependencies that
are unknown to the agent. The agent needs to quickly adapt to the task over few
episodes during adaptation phase to maximize the return in the test phase. Instead
of directly learning a meta-policy, we develop a Meta-learner with Subtask Graph
Inference (MSGI), which infers the latent parameter of the task by interacting with
the environment and maximizes the return given the latent parameter. To facilitate
learning, we adopt an intrinsic reward inspired by upper confidence bound (UCB)
that encourages efficient exploration. Our experiment results on two grid-world
domains and StarCraft II environments show that the proposed method is able
to accurately infer the latent task parameter, and to adapt more efficiently than
existing meta RL and hierarchical RL methods 1.
1 Introduction
Recently, reinforcement learning (RL) systems have achieved super-human performance on many
complex tasks (Mnih et al., 2015; Silver et al., 2016; Van Seijen et al., 2017). However, these works
mostly have been focused on a single known task where the agent can be trained for a long time (e.g.,
Silver et al. (2016)). We argue that agent should be able to solve multiple tasks with varying sources
of reward. Recent work in multi-task RL has attempted to address this; however, they focused on
the setting where the structure of task are explicitly described with natural language instructions (Oh
et al., 2017; Andreas et al., 2017; Yu et al., 2017; Chaplot et al., 2018), programs (Denil et al., 2017),
or graph structures (Sohn et al., 2018). However, such task descriptions may not readily be available.
A more flexible solution is to have the agents infer the task by interacting with the environment.
Recent work in Meta RL (Hochreiter et al., 2001; Duan et al., 2016; Wang et al., 2016; Finn et al.,
2017) (especially in few-shot learning settings) has attempted to have the agents implicitly infer tasks
and quickly adapt to them. However, they have focused on relatively simple tasks with a single goal
(e.g., multi-armed bandit, locomotion, navigation, etc.).
We argue that real-world tasks often have a hierarchical structure and multiple goals, which require
long horizon planning or reasoning ability (Erol, 1996; Xu et al., 2017; Ghazanfari & Taylor, 2017;
Sohn et al., 2018). Take, for example, the task of making a breakfast in Figure 1. A meal can be
served with different dishes and drinks (e.g., boiled egg and coffee), where each could be considered
as a subtask. These can then be further decomposed into smaller substask until some base subtask
(e.g., pickup egg) is reached. Each subtask can provide the agent with reward; if only few subtasks
provide reward, this is considered a sparse reward problem. When the subtask dependencies are
complex and reward is sparse, learning an optimal policy can require a large number of interactions
with the environment. This is the problem scope we focus on in this work: learning to quickly infer
and adapt to varying hierarchical tasks with multiple goals and complex subtask dependencies.
To this end, we formulate and tackle a new few-shot RL problem called subtask graph inference
problem, where the task is defined as a factored MDP (Boutilier et al., 1995; Jonsson & Barto, 2006)
with hierarchical structure represented by subtask graph (Sohn et al., 2018) where the task is not
known a priori. The task consists of multiple subtasks, where each subtask gives reward when
completed (see Figure 1). The complex dependencies between subtasks (i.e., preconditions) enforce
agent to execute all the required subtasks before it can execute a certain subtask. Intuitively, the
agent can efficiently solve the task by leveraging the inductive bias of underlying task structure
(Section 2.2).
1The demo videos are available at https://bit.ly/msgi-videos.
1
Published as a conference paper at ICLR 2020
Figure 1: Overview of our method in the context of prepare breakfast task. This task can be broken down into
subtasks (e.g., pickup mug) that composes the underlying subtask graph G. (Left) To learn about the unknown
task, the agent collects trajectories over K episodes through a parameterized adaptation policy πθadapt that learns
to explore the environment. (Center) With each new trajectory, the agent attempts to infer the task’s underlying
ground-truth subtask graph G with Gb. (Right) A separate test policy πGtbest uses the inferred subtask graph Gb to
produce a trajectory that attempts to maximize the agent’s reward P rt (e.g., the green trajectory that achieves
the boil egg subtask). The more precise Gb, the more reward the agent would receive, which implicitly improves
the adaptation policy πθadapt to better explore the environment and therefore better infer Gb in return.
Inspired by the recent works on multi-task and few-shot RL, we propose a meta reinforcement
learning approach that explicitly infers the latent structure of the task (e.g., subtask graph). The agent
learns its adaptation policy to collect as much information about the environment as possible in order
to rapidly and accurately infer the unknown task structure. After that, the agent’s test policy is a
contextual policy that takes the inferred subtask graph as an input and maximizes the expected return
(See Figure 1). We leverage inductive logic programming (ILP) technique to derive an efficient task
inference method based on the principle of maximum likelihood. To facilitate learning, we adopt an
intrinsic reward inspired by upper confidence bound (UCB) that encourages efficient exploration. We
evaluate our approach on various environments ranging from simple grid-world (Sohn et al., 2018) to
StarCraft II (Vinyals et al., 2017). In all cases, our method can accurately infer the latent subtask
graph structure, and adapt more efficiently to unseen tasks than the baselines.
The contribution of this work can be summarized as follows:
•	We propose a new meta-RL problem with more general and richer form of tasks compared to the
recent meta-RL approaches.
•	We propose an efficient task inference algorithm that leverages inductive logic programming,
which accurately infers the latent subtask graph from the agent’s experience data.
•	We implement a deep meta-RL agent that efficiently infers the subtask graph for faster adaptation.
•	We compare our method with other meta-RL agents on various domains, and show that our method
adapts more efficiently to unseen tasks with complex subtask dependencies.
2	Problem Definition
2.1	Background: Few-shot Reinforcement Learning
A task is defined by an MDP MG = (S, A, PG, RG) parameterized by a task parameter G with a
set of states S, a set of actions A, transition dynamics PG, reward function RG. In the K-shot RL
formulation (Duan et al., 2016; Finn et al., 2017), each trial under a fixed task MG consists of an
adaptation phase where the agent learns a task-specific behavior and a test phase where the adapted
behavior is evaluated. For example, RNN-based meta-learners (Duan et al., 2016; Wang et al., 2016)
adapt to a task MG by updating its RNN states (or fast-parameters) φt, where the initialization and
update rule of φt is parameterized by a slow-parameter θ: φ0 = g(θ), φt+1 = f(φt; θ). Gradient-
based meta-learners (Finn et al., 2017; Nichol et al., 2018) instead aim to learn a good initialization
of the model so that it can adapt to a new task with few gradient update steps. In the test phase, the
agent’s performance on the task MG is measured in terms of the return:
RMG (πφH) = EπφH,MG hPtH=01 rti ,	(1)
2
Published as a conference paper at ICLR 2020
Algorithm 1 Adaptation policy optimization during meta-training
Require: p(G): distribution over subtask graph
1:
2:
3:
4:
5:
6:
7:
while not done do
Sample batch of task parameters {Gi}M=ι 〜P(G)
for all Gi in the batch do
Rollout K episodes T = {st, ot, rt, dt}H=ι 〜∏ɑdapt in task MGi	. adaptation phase
8:
Compute rtUCB as in Eq.(7)
Gbi = ILP(τ )
Sample T0 〜∏eχe in task MGi
UPdate θ 一 θ + nV© Pg RMGUCB 卜外)Using RM-
. subtask graph inference
. test phase
+UCB in Eq.(9)
where πφH is the policy after K episodes (or H Update steps) of adaptation, H0 is the horizon of test
phase, and rt is the reward at time t in the test phase. The goal is to find an optimal parameter θ that
maximizes the expected return EG [RMG (πφH)] over a given distribution of tasks p(G).
2.2	The Subtask Graph Inference Problem
We formulate the subtask graph inference problem, an instance of few-shot RL problem where a
task is parameterized by subtask graph (Sohn et al., 2018). The details of how a subtask graph
parameterizes the MDP is described in Appendix B. Our problem extends the subtask graph execution
problem in (Sohn et al., 2018) by removing the assumption that a subtask graph is given to the agent;
thus, the agent must infer the subtask graph in order to perform the complex task. Following few-shot
RL settings, the agent’s goal is to quickly adapt to the given task (i.e., MDP) in the adaptation phase
to maximize the return in the test phase (see Figure 1). A task consists of N subtasks and the subtask
graph models a hierarchical dependency between subtasks.
Subtask: A subtask Φi can be defined by a tuple (completion set Sciomp ⊂ S, precondition Gic :
S 7→ {0, 1}, subtask reward function Gir : S → R). A subtask Φi is complete if the current state is
contained in its completion set (i.e., St ∈ SComp), and the agent receives a reward r 〜Gr upon the
completion of subtask Φi. A subtask Φi is eligible (i.e., subtask can be executed) if its precondition
Gic is satisfied (see Figure 1 for examples). A subtask graph is a tuple of precondition and subtask
reward of all the subtasks: G = (Gc, Gr). Then, the task defined by the subtask graph is a factored
MDP (Boutilier et al., 1995; Schuurmans & Patrascu, 2002); i.e., the transition model is factored as
p(s0|s, a) = Qi pGi (s0i|s, a) and the reward function is factored as R(s, a) = Pi RGri (s, a) (see
Appendix for the detail). The main benefit of factored MDP is that it allows us to model many
hierarchical tasks in a principled way with a compact representation such as dynamic Bayesian
network (Dean & Kanazawa, 1989; Boutilier et al., 1995). For each subtask Φi, the agent can learn
an option Oi (Sutton et al., 1999b) that executes the subtask2.
Environment: The state input to the agent at time step t consists of st = {xt, et, stept, epit, obst}.
•	Completion: xt ∈ {0, 1}N indicates whether each subtask is complete.
•	Eligibility: et ∈ {0, 1}N indicates whether each subtask is eligible (i.e., precondition is satisfied).
•	Time budget: stept ∈ R is the remaining time steps until episode termination.
•	Episode budget: epit ∈ R is the remaining number of episodes in adaptation phase.
•	Observation: obst ∈ RH×W ×C is a (visual) observation at time t.
At time step t, we denote the option taken by the agent as ot and the binary variable that indicates
whether episode is terminated as dt .
3	Method
We propose a Meta-learner with Subtask Graph Inference (MSGI) which infers the latent subtask
graph G. Figure 1 overviews our approach. Our main idea is to employ two policies: adaptation
policy and test policy. During the adaptation phase, an adaptation policy πθadapt rolls out K episodes
2As in Andreas et al. (2017); Oh et al. (2017); Sohn et al. (2018), such options are pre-learned with curriculum
learning; the policy is learned by maximizing the subtask reward, and the initiation set and termination condition
are given as Ii = {s|Gic(s) = 1} and βi = I(xi = 1)
3
Published as a conference paper at ICLR 2020
Agent
trajectory
Input
x
Subtask ABCDE
t
0^
1
2
H
X
ABCDE
00000
10000
11000
11110
e
ABCDE
11100
11100
11101
■ ⋮ ■
11111
00000
10000
11000
out
O
A
11110
Decision tree of A
Precondition of A
Subtask
E
Input
X
ABCDE
00000
10000
11000
11110
丁 CART
1
1	train
I e『1 I
Logic
expression
out
立
E
ɪ
0
1
1
CART
train
Decision tree of E
[%B>0口
-F≥^Z>
eE=0
转0 T=C
盯	Logic
Build
〉	graph
Precondition of E
AB + BC
Simplify
xc>0 Ce=1 expression
FXX^T
I <E=0 I I <E=1 ]
Build
AB + ABC graph
J t
True
Inferred
subtask graph
1
F
Figure 2: Our inductive logic programming module infers the precondition Gc from adaptation trajectory. For
example, the decision tree of subtask E (bottom row) estimates the latent precondition function fGE : x 7→ eE
by fitting its input-output data (i.e., agent’s trajectory {xt, etE}tH=1). The decision tree is constructedcby choosing
a variable (i.e., a component of x) at each node that best splits the data. The learned decision trees of all the
subtasks are represented as logic expressions, and then transformed and merged to form a subtask graph.
of adaptation trajectories. From the collected adaptation trajectories, the agent infers the subtask
graph G using inductive logic programming (ILP) technique. A test policy πtbest, conditioned on the
inferred subtask graph Gb, rolls out episodes and maximizes the return in the test phase. Note that the
performance depends on the quality of the inferred subtask graph. The adaptation policy indirectly
contributes to the performance by improving the quality of inference. Intuitively, if the adaptation
policy completes more diverse subtasks during adaptation, the more “training data” is given to the
ILP module, which results in more accurate inferred subtask graph. Algorithm 1 summarizes our
meta-training procedure. For meta-testing, see Algorithm 2 in Appendix D.
3.1	Subtask Graph Inference
Let τH = {s1, o1, r1, d1, . . . , sH} be an adaptation trajectory of the adaptation policy πθadapt for K
episodes (or H steps in total) in adaptation phase. The goal is to infer the subtask graph G for
this task, specified by preconditions Gc and subtask rewards Gr . We find the maximum-likelihood
estimate (MLE) of G = (Gc, Gr) that maximizes the likelihood of the adaptation trajectory τH:
GMLE = arg maχGc,Gr P(TH ∖Gc,Gr).
The likelihood term can be expanded as
H
p(τH∖Gc,Gr) = p(s1∖Gc)	πθ (ot∖τt)p(st+1∖st,ot,Gc)p(rt∖st,ot,Gr)p(dt∖st,ot)	(2)
t=1
H
H p(sι∣Gc) "p(st+ι∣st, ot, Gc)p(rt∣st, ot, Gr),
(3)
t=1
where we dropped the terms that are independent of G. From the definitions in Section 2.2, precondi-
tion Gc defines the mapping x → e, and the subtask reward Gr determines the reward as rt 〜Gri if
subtask i is eligible (i.e., eit = 1) and option Oi is executed at time t. Therefore, we have
GbMLE = (GbcMLE, GbrMLE) = argmaxYp(et∖xt, Gc), argmaxYp(rt∖et,ot,Gr) .	(4)
Gc	t=1	Gr	t=1
We note that no supervision from the ground-truth subtask graph G is used. Below we explain how to
compute the estimate of preconditions GbcMLE and subtask rewards GbrMLE .
Precondition inference via logic induction Since the precondition function fGc : x 7→ e (see
Section 2.2 for definition) is a deterministic mapping, the probability termp(et∖xt, Gc) in Eq.(4) is 1
if et = fGc (xt) and 0 otherwise. Therefore, we can rewrite GbcMLE in Eq.(4) as:
H
GbcMLE = arg max Y I(et = fGc (xt)),
Gc t=1
(5)
where I(∙) is the indicator function. Since the eligibility e is factored, the precondition function fGci
for each subtask is inferred independently. We formulate the problem of finding a boolean function
4
Published as a conference paper at ICLR 2020
that satisfies all the indicator functions in Eq.(5) (i.e., QtH=1 I(et = fGc (xt)) = 1) as an inductive
logic programming (ILP) problem (Muggleton, 1991). Specifically, {xt}tH=1 forms binary vector
inputs to programs, and {eit}tH=1 forms Boolean-valued outputs of the i-th program that denotes the
eligibility of the i-th subtask. We use the classification and regression tree (CART) to infer the
precondition function fGc for each subtask based on Gini impurity (Breiman, 1984). Intuitively, the
constructed decision tree is the simplest boolean function approximation for the given input-output
pairs {xt , et }. Then, we convert it to a logic expression (i.e., precondition) in sum-of-product (SOP)
form to build the subtask graph. Figure 2 summarizes the overall logic induction process.
Subtask reward inference To infer the subtask reward function GbrMLE in Eq.(4), we model each
component of subtask reward as a Gaussian distribution Gr 〜N(bi, bi). Then, RMle becomes the
empirical mean of the rewards received after taking the eligible option Oi in the trajectory τH :
GMLE,i _ ^i	_ E	r I	_ Oi	i	_ 1] _ PH=IrtI(Ot = Oi, et = I)	符
Gr =μMLE= E	Lrtlot	= O ,	et	= 1= PH=I IQ = 0., et = 1).	⑹
3.2	Test phase: Subtask Graph Execution Policy
Once a subtask graph Gb has been inferred,
we can derive a subtask graph execution (SGE) policy
∏exe(o∣x) that aims to maximize the cumulative reward in the test phase. Note that this is precisely
the problem setting used in Sohn et al. (2018). Therefore, we employ a graph reward propagation
(GRProp) policy (Sohn et al., 2018) as our SGE policy. Intuitively, the GRProp policy approximates
a subtask graph to a differentiable form such that we can compute the gradient of modified return
with respect to the completion vector to measure how much each subtask is likely to increase the
modified return. Due to space limitation, we give a detail of the GRProp policy in Appendix I.
3.3	Learning: Optimization of the Adaptation Policy
We now describe how to learn the adaptation policy πθadapt, or its parameters θ. We can directly
optimize the objective RMG (π) using policy gradient methods (Williams, 1992; Sutton et al., 1999a),
such as actor-critic method with generalized advantage estimation (GAE) (Schulman et al., 2016).
However, we find it challenging to train our model for two reasons: 1) delayed and sparse reward
(i.e., the return in the test phase is treated as if it were given as a one-time reward at the last step of
adaptation phase), and 2) large task variance due to highly expressive power of subtask graph. To
facilitate learning, we propose to give an intrinsic reward rtUCB to agent in addition to the extrinsic
environment reward, where rtUCB is the upper confidence bound (UCB) (Auer et al., 2002)-inspired
exploration bonus term as follows:
UCB	N log(ni (0) + ni (1))
rtjc = Wucb ∙ I(xt is novel),	WUCB = ɪ2----------TΠλ---------,	(7)
t	i=1	ni(eti)
where N is the number of subtasks, eti is the eligibility of subtask i at time t, and ni (e) is the
visitation count of ei (i.e., the eligibility of subtask i) during the adaptation phase until time t. The
weight WUCB is designed to encourage the agent to make eligible and execute those subtasks that have
infrequently been eligible, since such rare data points in general largely improve the inference by
balancing the dataset that CART (i.e., our logic induction module) learns from. The conditioning
term I(xt is novel) encourages the adaptation policy to visit novel states with a previously unseen
completion vector xt (i.e., different combination of completed subtasks), since the data points with
same xt input will be ignored in the ILP module as a duplication. We implement I(xt is novel) using
a hash table for computational efficiency. Then, the intrinsic objective is given as follows:
RUMCGB πθadapt = Eπθadapt,MG hPtH=1rtUCBi ,	(8)
where H is the horizon of adaptation phase. Finally, we train the adaptation policy πθadapt using an
actor-critic method with GAE (Schulman et al., 2016) to maximize the following objective:
RPMG+GUCB πθadapt =RMG πGGbRProp+βUCBRUMCGB πθadapt,	(9)
where RMG (∙) is the meta-learning objective in Eq.(1), βucB is the mixing hyper-parameter, and G
is the inferred subtask graph that depends on the adaptation policy πθadapt. The complete procedure for
training our MSGI agent with UCB reward is summarized in Algorithm 1.
5
Published as a conference paper at ICLR 2020
4	Related Work
Meta Reinforcement Learning. There are roughly two broad categories of meta-RL approaches:
gradient-based meta-learners (Finn et al., 2017; Nichol et al., 2018; Gupta et al., 2018; Finn et al.,
2018; Kim et al., 2018) and RNN-based meta-learners (Duan et al., 2016; Wang et al., 2016). Gradient-
based meta RL algorithms, such as MAML (Finn et al., 2017) and Reptile (Nichol et al., 2018), learn
the agent’s policy by taking policy gradient steps during an adaptation phase, where the meta-learner
aims to learn a good initialization that enables rapid adaptation to an unseen task. RNN-based
meta-RL methods (Duan et al., 2016; Wang et al., 2016) updates the hidden states of a RNN as a
process of adaptation, where both of hidden state initialization and update rule are meta-learned.
Other variants of adaptation models instead of RNNs such as temporal convolutions (SNAIL) (Mishra
et al., 2018) also have been explored. Our approach is closer to the second category, but different
from existing works as we directly and explicitly infer the task parameter.
Logic induction. Inductive logic programming systems (Muggleton, 1991) learn a set of rules from
examples. (Xu et al., 2017) These works differ from ours as they are open-loop LPI; the input data to
LPI module is generated by other policy that does not care about ILP process. However, our agent
learns a policy to collect data more efficiently (i.e., closed-loop ILP). There also have been efforts
to combine neural networks and logic rules to deal with noisy and erroneous data and seek data
efficiency, such as (Hu et al., 2016; Evans & Grefenstette, 2017; Dong et al., 2019).
Autonomous Construction of Task Structure. Task planning approaches represented the task
structure using Hierarchical Task Networks (HTNs) (Tate, 1977). HTN identifies subtasks for a given
task and represent symbolic representations of their preconditions and effects, to reduce the search
space of planning (Hayes & Scassellati, 2016). They aim to execute a single goal task, often with
assumptions of simpler subtask dependency structures (e.g., without NOT dependency (Ghazanfari &
Taylor, 2017; Liu et al., 2016)) such that the task structure can be constructed from the successful
trajectories. In contrast, we tackle a more general and challenging setting, where each subtask gives a
reward (i.e., multi-goal setting) and the goal is to maximize the cumulative sum of reward within an
episode. More recently, these task planning approaches were successfully applied to the few-shot
visual imitation learning tasks by constructing recursive programs (Xu et al., 2017) or graph (Huang
et al., 2018). Contrary to them, we employ an active policy that seeks for experience useful in
discovering the task structure in unknown and stochastic environments.
5	Experiments
In the experiment, we investigate the following research questions: (1) Does MSGI correctly infer
task parameters G? (2) Does adaptation policy πθadapt improve the efficiency of few-shot RL? (3) Does
the use of UCB bonus facilitate training? (See Appendix H.1) (4) How well does MSGI perform
compared with other meta-RL algorithms? (5) Can MSGI generalize to longer adaptation horizon,
and unseen and more complex tasks?
We evaluate our approach in comparison with the following baselines:
•	Random is a policy that executes a random eligible subtask that has not been completed.
•	RL2 is the meta-RL agent in Duan et al. (2016), trained to maximize the return over K episodes.
•	HRL is the hierarchical RL agent in Sohn et al. (2018) trained with the same actor-critic method as
our approach during adaptation phase. The network parameter is reset when the task changes.
•	GRProp+Oracle is the GRProp policy (Sohn et al., 2018) provided with the ground-truth subtask
graph as input. This is roughly an upper bound of the performance of MSGI-based approaches.
•	MSGI-Rand (Ours) uses a random policy as an adaptation policy, with the task inference module.
•	MSGI-Meta (Ours) uses a meta-learned policy (i.e., πθadapt) as an adaptation policy, with the task
inference module.
For RL2 and HRL, we use the same network architecture as our MSGI adaptation policy. More details
of training and network architecture can be found in Appendix J. The domains on which we evaluate
these approaches include two simple grid-world environments (Mining and Playground) (Sohn
et al., 2018) and a more challenging domain SC2LE (Vinyals et al., 2017) (StarCraft II).
5.1	Experiments on Mining and Playground Domains
Mining (Sohn et al., 2018) is inspired by Minecraft (see Figure 3) where the agent receives reward by
picking up raw materials in the world or crafting items with raw materials. Playground (Sohn et al.,
6
Published as a conference paper at ICLR 2020
Figure 3: Left: A visual illustration of Playground domain and an example of underlying subtask graph. The
goal is to execute subtasks in the optimal order to maximize the reward within time budget. The subtask graph
describes subtasks with the corresponding rewards (e.g., transforming a chest gives 0.1 reward) and dependencies
between subtasks through AND and OR nodes. For instance, the agent must first transform chest AND transform
diamond before executing pick up duck. Right: A warfare scenario in SC2LE domain (Vinyals et al., 2017).
The agent must prepare for the upcoming warfare by training appropriate units, through an appropriate order of
subtasks (see Appendix for more details).
2018) is a more flexible and challenging domain, where the environment is stochastic and subtask
graphs are randomly generated (i.e., precondition is an arbitrary logic expression). We follow the
setting in Sohn et al. (2018) for choosing train/evaluation sets. We measure the performance in terms
of normalized reward R = (R - Rmin)/(Rmax -Rmin) averaged over 4 random seeds, where Rmin and
Rmax correspond to the average reward of the Random and the GRProp+Oracle agent, respectively.
5.1.1	Training Performance
Figure 4 shows the learning curves of MSGI-Meta and RL2, trained
on the D1-Train set of Playground domain. We set the adaptation
budget in each trial to K = 10 episodes. For MSGI-Rand and HRL
(which are not meta-learners), we show the average performance
after 10 episodes of adaptation. As training goes on, the performance
of MSGI-Meta significantly improves over MSGI-Rand with a large
margin. It demonstrates that our meta adaptation policy learns to
explore the environment more efficiently, inferring subtask graphs
more accurately. We also observe that the performance of RL2 agent
improves over time, eventually outperforming the HRL agent. This
indicates that RL2 learns 1) a good initial policy parameter that
captures the common knowledge generally applied to all the tasks
and 2) an efficient adaptation scheme such that it can adapt to the
given task more quickly than standard policy gradient update in HRL.
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.Q
Playground (Dl)
MSGl-Meta(OUrSj
Γr* --- MSGI-Rand(Ours)
■■■ HRL
Trial (thousand)
Figure 4: Learning curves on the
Playground domain. We mea-
sure the normalized reward (y-
axis) in a test phase, after a certain
number of training trials (x-axis).

5.1.2	Adaptation and Generalization Performance
Adaptation efficiency. In Figure 5, we measure the test performance (in terms of the normalized
reward R) by varying episode budget K (i.e., how many episodes are used in adaptation phase), after
8000 trials of meta-training (Figure 4). Intuitively, it shows how quickly the agent can adapt to the
given task. Our full algorithm MSGI-Meta consistently outperforms MSGI-Rand across all the tasks,
showing that our meta adaptation policy can efficiently explore informative states that are likely
to result in more accurate subtask graph inference. Also, both of our MSGI-based models perform
better than HRL and RL2 baselines in all the tasks, showing that explicitly inferring underlying task
structure and executing the predicted subtask graph is more effective than learning slow-parameters
and fast-parameters (e.g., RNN states) on those tasks involving complex subtask dependencies.
Generalization performance. We test whether the agents can generalize over unseen task and longer
adaptation horizon, as shown in Figure 5. For Playground, we follow the setup of (Sohn et al., 2018):
we train the agent on D1-Train with the adaptation budget of 10 episodes, and test on unseen graph
distributions D1-Eval and larger graphs D2-D4 (See Appendix C for more details about the tasks in
Playground and Mining). We report the agent’s performance as the normalized reward with up to 20
episodes of adaptation budget. For Mining, the agent is trained on randomly generated graphs with
25 episodes budget and tested on 440 hand-designed graphs used in (Sohn et al., 2018), with up to
7
Published as a conference paper at ICLR 2020
Budget (episodes)
Budget (episodes)
Budget (episodes)
— MSGI-Meta(Ours)
—MSGI-Rand(Ours)
HRL
一RL2
Figure 5: Generalization performance on unseen tasks (D1-Eval, D2, D3, D4, and Mining-Eval) with varying
adaptation horizon. We trained agent with the fixed adaptation budget (K = 10 for Playground and K = 25
for Mining) denoted by the vertical dashed line, and tested with varying unseen adaptation budgets. We report
the average normalized return during test phase, where GRProp+Oracle is the upper bound (i.e., Rb = 1) and
Random is the lower bound (i.e., R = 0). The shaded area in the plot indicates the range between R + σ and
Rb - σ where σ is the standard error of normalized return.
Adaptation (episodes)	Adaptation (episodes)	Adaptation (episodes)	Adaptation (episodes)
Figure 6: Adaptation performance with different adaptation horizon on SC2LE domain.
50 episodes of adaptation budget. Both of our MSGI-based models generalize well to unseen tasks
and over different adaptation horizon lengths, continuingly improving the agent’s performance. It
demonstrates that the efficient exploration scheme that our meta adaptation policy can generalize to
unseen tasks and longer adaptation horizon, and that our task execution policy, GRProp, generalizes
well to unseen tasks as already shown in (Sohn et al., 2018). However, RL2 fails to generalize to
unseen task and longer adaptation horizon: on D2-D4 with adaptation horizons longer than the length
the meta-learner was trained for, the performance of the RL2 agent is almost stationary or even
decreases for very long-horizon case (D2, D3, and Mining), eventually being surpassed by the HRL
agent. This indicates (1) the adaptation scheme that RL2 learned does not generalize well to longer
adaptation horizons, and (2) a common knowledge learned from the training tasks does not generalize
well to unseen test tasks.
5.2 Experiments on S tarCraft II Domain
SC2LE (Vinyals et al., 2017) is a challenging RL domain built upon the real-time strategy game
StarCraft II. We focus on two particular types of scenarios: Defeat Enemy and Build Unit. Each
type of the scenarios models the different aspect of challenges in the full game. The goal of Defeat
Enemy is to eliminate various enemy armies invading within 2,400 steps. We consider three different
combinations of units with varying difficulty: Defeat Zerglings, Defeat Hydralisks, Defeat Hydralisks
& Ultralisks (see Figure 9 and demo videos at https://bit.ly/msgi-videos). The goal of
Build Unit scenario is to build a specific unit within 2,400 steps. To showcase the advantage of MSGI
infering the underlying subtask graph, we set the target unit as Battlecruiser, which is at the highest
rank in the technology tree of Terran race. In both scenarios, the agent needs to train the workers,
collect resources, and construct buildings and produce units in correct sequential order to win the
game. Each building or unit has a precondition as per the technology tree of the player’s race (see
Figure 11 and Appendix E for more details).
8
Published as a conference paper at ICLR 2020
Agents. Note that the precondition of each subtask is determined by the domain and remains fixed
across the tasks. If we train the meta agents (MSGI-Meta and RL2), the agents memorize the subtask
dependencies (i.e., over-fitting) and does not learn any useful policy for efficient adaptation. Thus,
we only evaluate Random and HRL as our baseline agents. Instead of MSGI-Meta, we used MSGI-
GRProp. MSGI-GRProp uses the GRProp policy as an adaptation policy since GRProp is a good
approximation algorithm that works well without meta-training as shown in (Sohn et al., 2018). Since
the environment does not provide any subtask-specific reward, we set the subtask reward using the
UCB bonus term in Eq. (7) to encourage efficient exploration (See Appendix for detail).
Subtask graph inference. We quantitatively evaluate the inferred subtask graph in terms of the
precision and recall of the inferred precondition function fbc : x 7→ be. Specifically, we compare the
inference output be with the GT label e generated by the GT precondition function fc : x 7→ e for all
possible binary assignments of input (i.e., completion vector x). For all the tasks, our MSGI-GRProp
agent almost perfectly infers the preconditions with more than 94% precision and 96% recall of all
possible binary assignments, when averaged over all 163 preconditions in the game, with only 20
episodes of adaptation budget. We provide the detailed quantitative and qualitative results on the
inferred subtask graph in supplemental material.
Adaptation efficiency. Figure 6 shows the adaptation efficiency of MSGI-GRProp, HRL agents, and
Random policy on the four scenarios. We report the average victory or success rate over 8 episodes.
MSGI-GRProp consistently outperforms HRL agents with a high victory rate, by (1) quickly figuring
out the useful units and their prerequisite buildings and (2) focusing on executing these subtasks in
a correct order. For example, our MSGI-GRProp learns from the inferred subtask graph that some
buildings such as sensor tower or engineering bay are unnecessary for training units and avoids
constructing them (see Appendix F for the inferred subtask graph).
6 Conclusion
We introduced and addressed a few-shot RL problem with a complex subtask dependencies. We
proposed to learn the adaptation policy that efficiently collects experiences in the environment, infer
the underlying hierarchical task structure, and maximize the expected reward using the execution
policy given the inferred subtask graph. The empirical results confirm that our agent can efficiently
explore the environment during the adaptation phase that leads to better task inference and leverage
the inferred task structure during the test phase. In this work, we assumed that the option is pre-
learned and the environment provides the status of each subtask. In the future work, our approach
may be extended to more challenging settings where the relevant subtask structure is fully learned
from pure observations, and options to execute these subtasks are also automatically discovered.
Acknowledgments
We would like to thank Wilka Carvalho for valuable feedback on the manuscript. This work was partly
supported by Institute for Information & communications Technology Promotion (IITP) grant funded
by the Korea government (MSIT) (No. 2016-0-00563, Research on Adaptive Machine Learning
Technology Development for Intelligent Autonomous Digital Companion) and Korea Foundation for
Advanced Studies.
References
Jacob Andreas, Dan Klein, and Sergey Levine. Modular multitask reinforcement learning with policy
sketches. In ICML, 2017.
Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit
problem. Machine learning, 47(2-3):235-256, 2002.
M.K. Bloch. Hierarchical reinforcement learning in the taxicab domain. (Report No. CCA-TR-2009-
02). Ann Arbor, MI: Center for Cognitive Architecture, University of Michigan, 2009.
Craig Boutilier, Richard Dearden, Moises Goldszmidt, et al. Exploiting structure in policy construc-
tion. In IJCAI, volume 14, pp. 1104-1113, 1995.
Leo Breiman. Classification and regression trees. Routledge, 1984.
9
Published as a conference paper at ICLR 2020
Devendra Singh Chaplot, Kanthashree Mysore Sathyendra, Rama Kumar Pasumarthi, Dheeraj
Rajagopal, and Ruslan Salakhutdinov. Gated-attention architectures for task-oriented language
grounding. In AAAI, 2018.
Thomas Dean and Keiji Kanazawa. A model for reasoning about persistence and causation. Compu-
Iational intelligence, 5(2):142-150, 1989.
Misha DeniL Sergio G6mez Colmenarejo, Serkan Cabi, David Saxton, and Nando de Freitas. Pro-
grammable agents. arXiv preprint arXiv:1706.06383, 2017.
Honghua Dong, Jiayuan Mao, Tian Lin, Chong Wang, Lihong Li, and Denny Zhou. Neural logic
machines. In ICLR, 2019.
Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. Rl 2 : Fast
reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016.
Kutluhan Erol. Hierarchical task network planning: formalization, analysis, and implementation.
PhD thesis, 1996.
Richard Evans and Edward Grefenstette. Learning Explanatory Rules from Noisy Data. arXiv
preprint arXiv:1711.04574, 2017.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of
deep networks. In Proceedings of the 34th International Conference on Machine Learning-Volume
70, pp. 1126-1135. JMLR. org, 2017.
Chelsea Finn, Kelvin Xu, and Sergey Levine. Probabilistic model-agnostic meta-learning. In NeurIPS,
pp. 9516-9527, 2018.
Behzad Ghazanfari and Matthew E Taylor. Autonomous extracting a hierarchical structure of tasks in
reinforcement learning and multi-task reinforcement learning. arXiv preprint arXiv:1709.04579,
2017.
Abhishek Gupta, Russell Mendonca, YuXuan Liu, Pieter Abbeel, and Sergey Levine. Meta-
reinforcement learning of structured exploration strategies. arXiv preprint arXiv:1802.07245,
2018.
Bradley Hayes and Brian Scassellati. Autonomously constructing hierarchical task networks for
planning and human-robot collaboration. In 2016 IEEE International Conference on Robotics and
Automation (ICRA), pp. 5469-5476. IEEE, 2016.
Sepp Hochreiter, A Steven Younger, and Peter R Conwell. Learning to learn using gradient descent.
In International Conference on Artificial Neural Networks, pp. 87-94. Springer, 2001.
Zhiting Hu, Xuezhe Ma, Zhengzhong Liu, Eduard Hovy, and Eric Xing. Harnessing deep neural
networks with logic rules. arXiv preprint arXiv:1603.06318, 2016.
De-An Huang, Suraj Nair, Danfei Xu, Yuke Zhu, Animesh Garg, Li Fei-Fei, Silvio Savarese, and
Juan Carlos Niebles. Neural task graphs: Generalizing to unseen tasks from a single video
demonstration. arXiv preprint arXiv:1807.03480, 2018.
Anders Jonsson and Andrew Barto. Causal graph based decomposition of factored mdps. Journal of
Machine Learning Research, 7(Nov):2259-2301, 2006.
Taesup Kim, Jaesik Yoon, Ousmane Dia, Sungwoong Kim, Yoshua Bengio, and Sungjin Ahn.
Bayesian model-agnostic meta-learning. arXiv preprint arXiv:1806.03836, 2018.
Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Daniel
Gordon, Yuke Zhu, Abhinav Gupta, and Ali Farhadi. AI2-THOR: An Interactive 3D Environment
for Visual AI. arXiv, 2017.
Changsong Liu, Shaohua Yang, Sari Iaba-Sadiya, Nishant Shukla, Yunzhong He, Song-chun Zhu,
and Joyce Chai. Jointly learning grounded task structures from language instruction and visual
demonstration. In EMNLP, 2016.
10
Published as a conference paper at ICLR 2020
Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive meta-
learner. In ICLR, 2018.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control
through deep reinforcement learning. Nature, 518(7540):529-533, 2015.
Stephen Muggleton. Inductive logic programming. New Gen. Comput., 8(4):295-318, February
1991. ISSN 0288-3635. doi: 10.1007/BF03037089. URL http://dx.doi.org/10.1007/
BF03037089.
Alex Nichol, Joshua Achiam, and John Schulman. On first-order meta-learning algorithms. arXiv
preprint arXiv:1803.02999, 2018.
Junhyuk Oh, Satinder Singh, Honglak Lee, and Pushmeet Kohli. Zero-shot task generalization with
multi-task deep reinforcement learning. In ICML, 2017.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research,
12:2825-2830, 2011.
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional
continuous control using generalized advantage estimation. In ICLR, 2016.
Dale Schuurmans and Relu Patrascu. Direct value-approximation for factored MDPs. In NIPS, pp.
1579-1586, 2002.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. Nature, 529(7587):484-489, 2016.
Sungryull Sohn, Junhyuk Oh, and Honglak Lee. Hierarchical reinforcement learning for zero-shot
generalization with subtask dependencies. In NeurIPS, pp. 7156-7166, 2018.
Richard S. Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods
for reinforcement learning with function approximation. In NIPS, 1999a.
Richard S Sutton, Doina Precup, and Satinder Singh. Between MDPs and semi-MDPs: A framework
for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1-2):181-211,
1999b.
Austin Tate. Generating project networks. In Proceedings of the 5th international joint conference
on Artificial intelligence-Volume 2, pp. 888-893. Morgan Kaufmann Publishers Inc., 1977.
Harm Van Seijen, Mehdi Fatemi, Joshua Romoff, Romain Laroche, Tavian Barnes, and Jeffrey Tsang.
Hybrid reward architecture for reinforcement learning. In NIPS, pp. 5392-5402, 2017.
Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets, Michelle
Yeo, Alireza Makhzani, Heinrich Kuttler, John Agapiou, Julian Schrittwieser, et al. Starcraft II: A
new challenge for reinforcement learning. arXiv preprint arXiv:1708.04782, 2017.
Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos,
Charles Blundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn. arXiv
preprint arXiv:1611.05763, 2016.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine learning, 8(3-4):229-256, 1992.
Danfei Xu, Suraj Nair, Yuke Zhu, Julian Gao, Animesh Garg, Li Fei-Fei, and Silvio Savarese.
Neural task programming: Learning to generalize across hierarchical tasks. arXiv preprint
arXiv:1710.01813, 2017.
Haonan Yu, Haichao Zhang, and Wei Xu. A deep compositional framework for human-like language
acquisition in virtual environment. arXiv preprint arXiv:1703.09831, 2017.
11
Published as a conference paper at ICLR 2020
Appendix: Meta Reinforcement Learning with Autonomous
Inference of Subtask Dependencies
A Subtask graph and factored MDP
A. 1 Background: Factored Markov Decision Processes
A factored MDP (FMDP) (Boutilier et al., 1995; Jonsson & Barto, 2006) is an MDP M =
(S, A, P, R), where the state space S is defined by a set of discrete state variables s = {s1, . . . , sd}.
Each state variable si ∈ s takes on a value in its domain D(si). The state set S is a (subset of)
Cartesian product of the domain of all state variables ×si∈sD(si). In FMDP, the state variables si
are conditionally independent, such that the transition probability can be factored as follows:
p(st+1 |st , at) = p(st+1 |st , at)p(st+1 |st , at) . . . p(st+1 |st , at).	(10)
Then, the model of FMDP can be compactly represented by the subtask graph (Sohn et al., 2018) or
dynamic Bayesian network (DBN) (Dean & Kanazawa, 1989; Boutilier et al., 1995). They represent
the transition of each state variable p(sit+1|st, at) in either a Boolean expression (i.e., subtask graph)
or a binary decision tree (i.e., DBN). For more intuitive explanation, see the subtask graph paragraph
in Section 2.2 and Figure 1.
Jonsson & Barto (2006); Sohn et al. (2018) suggested that the factored MDP can be extended to
the option framework. Specifically, the option is defined based on the change in state variable (e.g.,
completion of subtask in Sohn et al. (2018)), and the option transition model and option reward
function are assumed to be factored. Similar to Eq. 10, the transition probability can be factored as
follows:
p(s0|s,o) = Y p(s0i |s, o),	R(s, o) = X Ri(s, o).	(11)
In (Sohn et al., 2018), the option Oi completes the subtask Φi by definition; thus, p(s0i|s, o) = 0 and
Ri(s, o) = 0 if o 6= Oi. By introducing the eligibility vector e, the transition and reward functions
are further expanded as follows:
p(x0i|x,o = Oi) = p(x0i|ei = 1)p(ei = 1|x),	(12)
Ri(x,o= Oi) = GriI(ei = 1),	(13)
where p(x0i|ei = 1) indicates that the subtask is completed x0i if the subtask is eligible ei = 1, p(ei|x)
is the precondition Gc, and I(ei = 1) indicates that the reward is given only if the subtask i is eligible.
B Details of task in subtask graph inference problem
For self-containedness, we repeat the details of how the task (i.e., MDP) is defined by the subtask graph
G from (Sohn et al., 2018). We define each task as an MDP tuple MG = (S, A, PG, RG, ρG, γ)
where S is a set of states, A is a set of actions, PG : S × A × S → [0, 1] is a task-specific state
transition function, RG : S × A → R is a task-specific reward function and ρG : S → [0, 1] is a
task-specific initial distribution over states. We describe the subtask graph G and each component of
MDP in the following paragraphs.
Subtask and Subtask Graph Formally, a subtask Φi is a tuple (completion set Sciomp ⊂ S,
precondition Gic, subtask reward Gir ∈ R). The subtask Φi is eligible (i.e., subtask can be executed)
if the precondition Gic is satisfied (see the State Distribution and Transition Function paragraph
below for detail). The subtask Φi is complete if the current state is in the completion set st ∈ Sciomp,
and the agent receives the reward rt 〜P(GQ. Subtasks are shared across tasks. The subtask graph
is a tuple of precondition and subtask reward of N subtasks: G = (Gc, Gr) (see Appendix B for
the detail). One example of subtask graph is given in Figure 3. A subtask graph G is a tuple of the
subtask reward Gr ∈ RN, and the precondition Gc of N subtasks.
State The state st consists of the observation obst ∈ {0, 1}W×H×C, the completion vector
xt ∈ {0, 1}N, the eligibility vector et ∈ {0, 1}N, the time budget stept ∈ R and number of episode
left during the adaptation epit ∈ R. An observation obst is represented as H × W × C tensor,
where H and W are the height and width of map respectively, and C is the number of object types in
12
Published as a conference paper at ICLR 2020
the domain. The (h, w, c)-th element of observation tensor is 1 if there is an object c in (h, w) on
the map, and 0 otherwise. The time budget indicates the number of remaining time-steps until the
episode termination. The completion vector and eligibility vector provides additional information
about N subtasks. The details of completion vector and eligibility vector will be explained in the
following paragraph.
State Distribution and Transition Function Given the current state
(obst, xt, et), the next step state (obst+1, xt+1, et+1) is computed from
the subtask graph G. Figure 7 describes the dependency between subtask
graph and MDP. In the beginning of episode, the completion vector xt is
initialized to a zero vector in the beginning of the episode x0 = [0, . . . , 0]
and the observation obs0 is sampled from the task-specific initial state dis-
tribution ρG . Specifically, the observation is generated by randomly placing
the agent and the N objects corresponding to the N subtasks defined in the
subtask graph G. When the agent executes subtask i, the i-th element of
completion vector is updated by the following update rule:
i	1 if	eit = 1
xt+1	xit otherwise	.
Figure 7: Dependency
between subtask graph
and MDP
(14)
The observation is updated such that agent moves on to the target object, and perform corresponding
primitive action. The eligibility vector et+1 is computed from the completion vector xt+1 and
precondition Gc as follows:
eit+1 = j∈COhRildi yAj ND
(15)
yAND = AND	xbt,+1 ,	(16)
j∈Childi
xbit,+j1 =xtj+1wi,j+(1-xtj+1)(1-wi,j),	(17)
where wi,j = 0 if there is a NOT connection between i-th node andj-th node, otherwise wi,j = 1, and
wi,j’s are defined by the Gc. Intuitively, xbit,j = 1 when j-th node does not violate the precondition of
i-th node. Executing each subtask costs different amount of time depending on the map configuration.
Specifically, the time cost is given as the Manhattan distance between agent location and target object
location in the grid-world plus one more step for performing a primitive action.
Task-specific Reward Function The reward function is defined in terms of the subtask reward
vector Gr ∈ RN and the eligibility vector et , where the subtask reward vector Gr is the component
of subtask graph G the and eligibility vector is computed from the completion vector xt and subtask
graph G as Eq. 17. Specifically, when agent executes subtask i, the amount of reward given to agent
at time step t is given as follows:
=	Gir	if	eit = 1
0 otherwise
(18)
Learning option The option framework can be naturally applied to the subtask graph-based tasks.
Consider the (optimal) option Oi = (Ii, πoi , βi) for subtask Φi. Its initiation set is Ii = {s|ei = 1},
where s is the state, ei is the i-th component of eligibility vector e, and e is an element of s. The
termination condition is βi = I(xit = 1), where xi is the i-th component of completion vector x. The
policy πoi maximizes the subtask reward Gir. Similar to Andreas et al. (2017); Oh et al. (2017); Sohn
et al. (2018), the option for each subtask is pre-learned via curriculum learning; i.e., the agent learns
options from the tasks consisting of single subtask by maximizing the subtask reward.
13
Published as a conference paper at ICLR 2020
C Details of the Playground and Mining Domain
For self-containedness, we provide the details of Playground and Mining (Sohn et al., 2018) domain.
Table 1 summarizes the complexity of the subtask graph for each task sets in Playground and Mining
domain.
Subtask Graph Setting					
	Playground				Mining
Ta^	D1	D2	D3	D4	-Eval-
Depth	-1-	-1-	"ɪ	6~6~	4-10-
Subtask	13	15	16	16	10-26
Table 1: (Playground) The subtask graphs in D1 have the same graph structure as training set, but the graph
was unseen. The subtask graphs in D2, D3, and D4 have (unseen) larger graph structures. (Mining) The subtask
graphs in Eval are unseen during training.
Legend
Subtask:
(OR)
AND:	O
NOT: -----------►
IEIigibIe 一
subtask IIneIigiblM
status ISUCCeSS I
Figure 8: An example observation and subtask graph of the Mining domain (Sohn et al., 2018). The precondition
of each subtask has semantic meaning based on the Minecraft game, which closely imitates the real-world tasks.
Obj: Name
Reward
Subtasks The set of subtasks in Playground and Mining are implemented as O = Aint × X,
where Aint is a set of interactions with objects, and X is a set of all types of interactive objects in the
domain. To execute a subtask (aint, obj) ∈ Aint × X, the agent should move on to the target object
obj and take the primitive action aint .
Mining The Mining (Sohn et al., 2018) is a domain inspired by Minecraft (see Figure 8) for example
task). The agent may pickup raw materials scattered in the world. The obtained raw materials may
be used to craft different items on different craft stations. There are two forms of preconditions: 1)
an item may be an ingredient for building other items (e.g. stick and stone are ingredients of stone
pickaxe), and 2) an item may be a required tool to pick up some objects (e.g. agent need stone pickaxe
to mine iron ore). To simplify the environment, it assumes that the agent can use the item multiple
times after picking it once. The subtasks in the higher layer in task graph are designed to give larger
reward. The pool of subtasks and preconditions are hand-coded similar to the crafting recipes in
Minecraft, and used as a template to generate 640 random task graphs. Following (Sohn et al., 2018),
we used 200 for training and 440 for testing.
Playground The Playground (Sohn et al., 2018) domain is a more flexible domain (see Figure 3,
left) which is designed to evaluate the strong generalization ability of agents on unseen task de-
pendencies under delayed reward in a stochastic environment. More specifically, the task graph in
Playground was randomly generated, hence its precondition can be any logical expression. Some of
the objects randomly move, which makes the environment stochastic. The agent was trained on small
14
Published as a conference paper at ICLR 2020
task graphs that consists of 4 layers of task dependencies, while evaluated on much larger task graphs
that consists of up to 6 layers of task dependencies (See Table 1). Following (Sohn et al., 2018), we
randomly generated 500 graphs for training and 500 graphs for testing. The task in the playground
domain is general such that it subsumes many other hierarchical RL domains such as Taxi (Bloch,
2009), Minecraft (Oh et al., 2017) and XWORLD (Yu et al., 2017).
D	Algorithm in meta-testing
The Algorithm 2 describes the process of our MSGI at meta-testing time for single trial.
Algorithm 2 Process of single trial for a task MG at meta-test time
Require: The current parameter θ
Require: A task MG parametrized by a task parameter G (unknown to the agent)
1:	Roll out K train episodes TH = {st, ot, rt, dt}H=I 〜∏adapt in task MG . adaptation phase
2:	Infer a subtask graph: G = (Gc, Gr) = (ILP(τH), RI(τH))	. task inference
3:	Roll out a test episode T0 = {st, o[, r0, dt}H=ι 〜∏exe in task MG	. test phase
4:	Measure the performance R = Pt rt0 for this task
E Details of the SC2LE Domain
The SC2LE domain (Vinyals et al., 2017) provides suite of mini-games focusing on specific aspects
of the entire StarCraft II game. In this paper, we custom design two types of new, simple mini-games
called Build Unit and Defeat Zerg troops. Specifically, we built Defeat Zerglings, Defeat Hydralisks,
Defeat Hydralisks & Ultralisks and Build Battlecruiser mini-games that compactly capture the most
fundamental goal of the full game. The Build Unit mini-game requires the agent to figure-out the
target unit and its precondition correctly, such that it can train the target unit within the given short
time budget. The Defeat Zerg troops mini-game mimics the full game more closely; the agent is
required to train enough units to win a war against the opponent players. To make the task more
challenging and interesting, we designed the reward to be given only at the end of episode depending
on the success of the whole task. Similar to the standard Melee game in StarCraft II, each episode is
initialized with 50 mineral, 0 gas, 7 and 4 SCVs that start gathering mineral and gas, respectively, 1
idle SCV, 1 refinery, and 1 Command Center (See Figure 9). The episode is terminated after 2,400
environment steps (equivalent to 20 minutes in game time). In the game, the agent is initially given
50 mineral, 0 gas, 7 and 4 SCVs that start gathering mineral and gas, respectively, 1 idle SCV, 1
refinery, and 1 Command Center (See Figure 9) and is allowed to prepare for the upcoming battle
only for 2,400 environment steps (equivalent to 20 minutes in game time). Therefore, the agent must
learn to collect resources and efficiently use them to build structures for training units. All the four
custom mini-games share the same initial setup as specified in Figure 9.
Defeat Zerg troops scenario: At the end of the war preparation, different combinations of enemy
unit appears: Defeat Zerglings and Defeat Hydralisks has 20 zerglings and 15 hydralisks, respectively,
and Defeat Hydralisks & Ultralisks contains a combination of total 5 hydralisks and 3 ultralisks.
When the war finally breaks out, the units trained by the agent will encounter the army of Zerg units
in the map and combat until the time over (240 environment steps or 2 minutes in the game) or either
side is defeated. Specifically, the agent may not take any action, and the units trained by the agent
perform an auto attack against the enemy units. Unlike the original full game that has ternary reward
structure of +1 (win) / 0 (draw) / -1 (loss), we use binary reward structure of +1 (win) and -1
(loss or draw). Notice that depending on the type of units the agent trained, a draw can happen. For
instance, if the units trained by the agent are air units that cannot attack the ground units and the
enemy units are the ground units that cannot attack the air units, then no combat will take place,
so we consider this case as a loss. Build unit scenario: The agent receives the reward of +1 if the
target unit is successfully trained within the time limit, and the episode terminates. When the episode
terminates due to time limit, the agent receives the reward of -1. We gave 2,400 step budget for the
Build Battlecruiser scenario such that only highly efficient policy can finish the task within the time
limit.
15
Published as a conference paper at ICLR 2020
The transition dynamics (i.e., build tech-tree) in SC2LE domain has a hierarchical characteristic
which can be inferred by our MSGI agent (see Figure 9). We conducted the experiment on Terran
race only, but our method can be applied to other races as well.
Subtask. There are 85 subtasks: 15 subtasks of constructing each type of building (Supply depot,
Barracks, Engineeringbay, Refinery, Factory, Missile turret, Sensor tower, Bunker, Ghost academy,
Armory, Starport, Fusioncore, Barrack-techlab, Factory-techlab, Starport-techlab), 17 subtasks
of training each type of unit (SCV, Marine, Reaper, Marauder, Ghost, Widowmine, Hellion, Hell-
bat, Cyclone, Siegetank, Thor, Banshee, Liberator, Medivac, Viking, Raven, Battlecruiser), one
subtask of idle worker, 32 subtasks of selecting each type of building and unit, gathering min-
eral, gathering gas, and no-op. For gathering mineral, we set the subtask as (mineral≥ val) where
val ∈ {50, 75, 100, 125, 150, 300, 400}. Similarly for gathering gas, we set the subtask as (gas≥ val)
where val ∈ {25, 50, 75, 100, 125, 150, 200, 300}. For no-op subtask, the agent takes the no-op ac-
tion for 8 times.
Figure 9: (Top) The agent starts the game initially with limited resources of 50 minerals, 0 gases, 3 foods, 11
SCVs collecting resources, 1 idle SCV and pre-built Refinery. (Middle) From the initial state, the agent needs to
strategically collect resources and build structures in order to be well prepared for the upcoming battle. (Bottom)
After 2,400 environment steps, the war breaks; all the buildings in the map are removed, and the enemy units
appear. The agent’s units should eliminate the enemy units within 240 environment steps during the war.
16
Published as a conference paper at ICLR 2020
Eligibility. The eligibility of the 15 building construction subtasks and 17 training unit subtasks is
given by the environment as an available action input. For the selection subtasks, we extracted the
number of corresponding units using the provided API of the environment. Gathering mineral, gas,
and no-op subtasks are always eligible.
Completion. The completion of the 15 construction subtasks and 17 training subtasks is 1 if the
corresponding building or unit is present on the map. For the selection subtasks, the completion is
1 if the target building or unit is selected. For gathering mineral and gas subtasks, the subtask is
completed if the condition is satisfied (i.e., gas≥ 50). The no-op subtask is never completed.
Subtask reward. In SC2LE domain, the agent does not receive any reward when completing a
subtask. The only reward given to agent is the binary reward rHepi = {+1, -1} at the end of episode
(i.e., t = Hepi). Therefore, the subtask reward inference method described in Eq.(4) may not be
applied. Instead, we tried to infer the subtask reward Gbr ∈ RN (see Section 3 for definition) from
a victory reward rHepi by building a binary classifier that predicts the victory reward rHepi from
the option count vector n ∈ NN using a logistic model (i.e., logistic regression), where N is the
number of subtasks and the option count vector n counts how many times each option had been
executed within an episode. Intuitively speaking, we assume that the execution of each subtask (i.e.,
option) gives an implicit subtask reward that is un-observable by the agent, and the victory reward is
determined by thresholding the sum of subtask rewards within an episode as follows:
rHepi = I(Gr>n > β),	(19)
where I(∙) is the indicator function and β is the threshold. Then, We approximate it using a sigmoid
function σ(∙) as follows:
rHepi = σ(Gr>n - β).	(20)
In the adaptation phase, we randomly sampled the subtask reward vector Gr from the uniform
distribution in [0, 1]N, and used it for running MSGI-GRProp agent while recording the option count
vector n. Then, the option count vectors n and the victory rewards rHepi from the K episodes in
adaptation phase form a training data {必,『用优,}K=r for estimating the parameters of logistic model
as follows:
log ^HP- = Gr>n - β,	(21)
1	- rHepi
where Gbr ∈ RN and β ∈ R are the weight and bias parameters to be learned. Finally, we used the
learned Gbr as the subtask reward vector for running our MSGI-GRProp agent in test phase. We used
the scikit-learn (Pedregosa et al., 2011) implementation of logistic regression.
F More results on the SC2LE Domain
Accuracy of inferred subtask graph. Figure 10 shows the accuracy of the subtask graph inferred
by MSGI-GRProp agent (Section 5.2), in terms of precision and recall over different adaption horizon.
Episodes
Episodes
Defeat Hydra- & Ultra-Iisks
Episodes
Figure 10: Precision and recall of binary assignments on the inferred subtask graph’s precondition.
Qualitative Examples. Figure 12 shows a simplified form of the subtask graph inferred by our MSGI-
GRProp agent after 20 episodes of adaptation. For better readability, we removed the preconditions
of resources (food, mineral, gas); Figure 13 depicts the full subtask graph. Compared to the actual
tech-tree of the game, we can see the dependency between buildings and units are correctly inferred.
17
Published as a conference paper at ICLR 2020
Figure 11: The actual tech-tree of Terran race in StarCraft II. There exists a hierarchy in the task, which can be
autonomously discovered by our MSGI agent.
OmCenter
<⅛Thor∣
JSiegeTankI
：yclone
LiberatOrl
Medivacl
hostAcad
Reaper
Marine
rmory
Factory
Barracks
SupDepO
Fa cto r
向
Bunker
ellion
issTurret
iking
EnginBay
SensTower
Raven
FusionCore
Refinery
Battlecruiser
BanSheel
SeleCt
StarpOr
select
Barracks
Barracks
TechLab
select
Factor
echLab
Figure 12: A simplified version of subtask graph inferred by our MSGI-GRProp agent after 10 episodes of
adaptation.
Starport
Marauder
idowMine
StarpOr
18
Published as a conference paper at ICLR 2020
1
idle SCV>0∣
Barracks
ComCenter
GhoStACad
sensτower
actor
tarport
50
75
TechL ab
attleCruiSerl
WidowMine
Figure 13: The full subtask graph inferred by our MSGI agent.
acιory
echL ab
tarport
echL ab
select
OmCenter
nera
125
nera
150
nera
300
nera
50
nera
75
minera
100
FuSionCore
19
Published as a conference paper at ICLR 2020
Budget (episodes)
一HRL
—MsGI-GRProp(Ours)
—Random
Figure 14:	Adaptation performance of MSGI-GRProp, Random, and HRL agents with different adaptation
horizon on AI2-THOR domain. The episode terminates after the agent executes 20 subtasks or when there is
no subtask available to execute. Our MSGI-GRProp achieves around 2.5 total reward within an episode by
executing roughly two serve subtasks, while the baseline methods almost never get any reward.
G Experiment on AI2-THOR Domain
The AI2-THOR (Kolve et al., 2017) is an interactive 3D environment where the agent can both
navigate and interact with the objects within the environment through variety of actions that can
change the states of the object (i.e., the PickupObject action changes the object’s isPickedUp state).
Among the several scenes provided by the environment, we focus on the kitchen scene and evaluate
agents on the task similar to the breakfast preparation task described in the introduction: The agent is
required to prepare the dishes by directly manipulating the objects given in the scene. See Figure 15
for an example task.
There are two different types of objects in the scene: The first type is the plain object (i.e., Bread,
Apple, Potato, Tomato, Egg, Lettuce, Cup, Mug, Plate, Pan, Bowl) that agents can move around with,
and the second type is the receptacle object (i.e., Pan, Plate, Bowl, Cabinet, Microwave, StoveBurner,
CounterTop, DiningTable, SideTable, Toaster, CoffeeMachine, Fridge) which can contain other objects
depending on their sizes. With these objects and the subtasks defined, the agent is required to cook
and serve foods through long sequence of subtasks. For instance, the agent can prepare a fried egg
dish by (1) placing a Pan object on StoveBurner, (2) placing Egg on Pan, (3) slicing (or, cracking)
Egg to EggCracked, (4) turning on the StoveKnob to cook the cracked egg, and finally (5) serving
the cooked Egg on the Plate. Rewards are given only when the agent successfully serves the cooked
object on the appropriate receptacles. Similar to the SC2LE domain (see Section 5.2), the task
structure in AI2-THOR is fixed as well (i.e., Egg cannot be cooked in the Fridge), and thus instead
of training meta agents, we evaluate and compare MSGI-GRProp, HRL, and Random agents on the
cooking tasks. The visualization of the subtask graph (i.e., underlying task structure) inferred by
MSGI-GRProp agent is available on the Figure 16.
Subtask. There are total 148 subtasks: 17 subtasks for picking up all the possible objects in
the scene (Tomato, Potato, Lettuce, Apple, Egg, Bread, TomatoSliced, PotatoSliced, LettuceSliced,
AppleSliced, EggCracked, BreadSliced, Pan, Plate, Cup, Mug, Bowl), 113 subtasks for putting
down each pickupable objects into pre-defined putdownable receptacles, 6 subtasks for slicing the
sliceable objects, 6 subtasks for cooking cookable objects or filling up liquid in the Mug or Cup, and
6 subtasks for serving the cooked or filled objects on the proper receptacles such as Plate, Bowl, and
DiningTable.
Completion. The completion of the 17 pick up subtasks is 1 if the corresponding object is in the
agent’s hand or inventory. For all the put down subtasks, the completion is 1 if the corresponding
object is in the target receptacle (i.e., TomatoSliced on Plate). For slice and cook subtasks, the
completion is 1 if the corresponding object is sliced or cooked, respectively. The serve subtasks are
complete if the corresponding objects are placed on the target receptacles (i.e., Cooked PotatoSliced
on Plate). When the agent executes the serve subtask, the served food is removed to simulate the user
eating the served dish, such that the agent can execute the same serve subtask at most once within the
episode.
Eligibility. The eligibility of the subtasks is computed based the corresponding subtask completion
vector. The eligibility of the pickup subtasks is always set to 1, and the putdown subtasks are eligible
if the corresponding object is picked up (i.e., Pickup Tomato is complete). The slice subtasks are
eligible if the sliceable objects are on any receptacle. The cook subtasks are eligible if the cookable
20
Published as a conference paper at ICLR 2020
objects are placed on the corresponding cooking station (i.e., Mug is on the CoffeeMachine). The
eligibility of the serve subtasks is 1 if the corresponding objects are either cooked or sliced.
Subtask reward. To make the task more challenging and realistic, we assigned a non-zero reward
only to the six Serve subtasks, that have the most complex precondition (i.e., sparse reward setting).
Similar to other environments, we randomly set the subtask reward of each subtask from the predefined
range when we sample a new task (i.e., trial). Table 2 specifies the range of subtask reward for the six
serve subtasks; intuitively speaking, we set a higher subtask reward for the subtask that has more
complex precondition.
Subtask name	Min reward	Max reward
Serve cooked potato	0.6	1.2
Serve cooked and sliced potato	1.0	2.0
Serve cooked and sliced bread	1.0	2.0
Serve cooked and cracked egg	1.0	2.0
Serve coffee	0.6	1.2
Serve water	0.4	1.0
Table 2: The range from which the subtask reward of serve subtask was sampled, in the AI2-THOR environment.
Result. On the AI2-THOR environment, we compared our MSGI-GRProp agent with two baseline
agents: Random and HRL. Figure 14 summarizes the performance of each agent with varying
adaptation budgets. We observed that both the Random and HRL agent almost never receives any non-
zero reward during 20 episodes of adaptation, since the serve subtasks have a complex precondition
that is not easy to satisfy for random policy. In contrast, our MSGI-GRProp agent achieves around 2.4
total reward on average after 20 episodes of adaptation. As specified in Table 2, each serve subtask
gives around 1.0〜1.5 reward, so it means MSGI-GRProP agent executes around two serve subtasks
within an episode. Also, considering that the minimum number of subtasks required for serve subtask
is around 6, being able to execute around two subtasks within only 20 steps means the agent does not
waste its time for executing other subtasks that are irrelevant to the target serve subtasks. The HRL
agent’s performance does not improve during adaptation since it seldom observes any reward. On the
other hand, our MSGI-GRProP agent can quickly find a way to execute the serve subtasks by inferring
the precondition of them; as shown in Figure 16, our ILP module can accurately and efficiently infer
the precondition of subtasks in AI2-THOR environment after only 20 episodes of adaptation.
21
Published as a conference paper at ICLR 2020
(a) Pick up Potato from Fridge.
(b) Slice Potato to PotatoSliced.
(c) Place PotatoSliced on Pan and cook.
(d) Serve Cooked PotatoSliced on Plate.
Figure 15:	(a) - (d) demonstrates an example task of preparing fried potato in the AI2-THOR domain. The
serve PotatoSliced on Plate subtask requires slicing the potato (e.g., (b)) and frying the sliced potato on the
pan (e.g., (c)) before serving the dish. The agent receives a reward after finishing the final subtask (e.g., (d)) of
serving the dish on the plate.
22
Published as a conference paper at ICLR 2020
Nonel—^&―⅛Serve
Nonel—⅛∣&~*4Serve
Nonel_►(&_BFickup-
Utdown EggCracked @ SinkBaSinl
PUtdown BreadSliCed @ Plate
—*&-HPUtdoWn EggCraCked @ DiningTablel
^^^A■&-APutdoWn EggCraCked @ Side Tablel
Dining Table	&-⅛C0
None & PiC kup
Utdownl Tomato @ SinkBaSinl
TomatoSliCed @ SinkBaSinl
Nonel■(&~~*pj
UtdoWn TomatoSliCed @ DiningTablel
UtdoWn BreadSliCed @ CounterTop
DiningT abe∣~~»4&
PiCkUp Tomato @ Nonef(&----HPUtdoWn Tomato @ Fridge
H&l—>Putdow
tdown MUg @ CoffeeMai:hinel->*(&-MCook 1
PUtdownl MUg @ MiCroWavel
PUtdown EggCracked @ Microwavel
_-vl&l-H=Utdown Bread @ DiningTable
e
-L→(&~MPUtdownl Bread @ SideTabel~►(&-
__►&----HPutdown BreadSliCed @ Fridge]
—►&-⅜Putdown BreadSliCed @ DiningTabl
Nonel—⅛&~~⅜jSe∏(e
ettuce @ Nonel-&»PliC kup
PiCkUp LettUCe @ Nonel~»|&|~~HPUtdownl LettUCe @ SideTable
'、'、了~~峭、JHUtdoWn Panl @ DiningTaFel
、&ðftaowi LettUCe @ SinkBaini
^__⅛& FUtIdoWn LettUCe @ DiningTabel
J&~~»Putdown LettUCeSliCed @ CoUrterTopl
UtdoWn Potato @ SideTable
PUtdoWn BoW @ Cabine
'、*&-计UtdoWn Potato @ SinkBaSinil
∖ *f&	PUtdoWn Potato @ CoUrterTop
Rdownl Apple @ DiningTagΓe∣~~>⅜∖
Utdownl Apple @ SideTabre~►&∙、
- UtdoWn Apple @ MiCroWavel—►!&—_
'''∖^^^fcl⅛l-----HPUtdoWn LettUCeSliCed @ Fridge
^"^*[&→PUiΓdown LettUCeSliCed @ Dining Tabl
PUtdownl PotatoSliCed @ Fridgel
PiCkUp Apple @ None
Apple @ Nione~►&~APiCkup AppleSliCed @ Nonel
，l @ Nonel-►&-MPUtdoWn BowI @ MiCrowv
■i@-----HPUtdoWn BoW @ Fridgel
No ne∣—►&_MSenve，
Nonel~~»4&~MPiCkUp
-IUtdoWn PotatoSliCed @ SideTable
Nonel_Mai-HPUtdown PotatoSliCed @ CoUnterTopl
∙^^^~^*∣S∣_⅜P∣utdown PlotatoSliCed @ SinkBasiin
>UtdoWn PotatoSliCed @ PanIl
CroWvel-►&—►Co
PUtdownl AppleSliCed @ Fridg^e
----■―►&~HPutdowni AppleSliCed @ SideTabiel
_*&-计UtdoWn AppleSliCed @ CoUnterTopl
、ɔ*@-APUtdownl AppleSliCed @ SinkBasin
,Putdown AppleSliCed @ Panl
_»&-MPUtdownl
图
倒
Figure 16:	The subtask graph inferred by our MSGI-GRProp agent in the AI2-THOR environment after 20
episodes.
23
Published as a conference paper at ICLR 2020
Playground (Dl)
0.85
0.80
0.75
0.70
佝 0.65
0.60
0.55
050O 12345678
Trial (thousand)
Figure 17: Comparison of meta-training MSGI-Meta agent that was trained with UCB bonus and extrinsic
reward, and MSGI-Meta without UCB agent that was trained with extrinsic reward only in the Playground
and Mining domain. In both domains, adding UCB bonus improves the meta-training performance of our
MSGI-Meta agent.
H More results on Mining and Playground
H. 1 Ablation study on the intrinsic reward
We conducted an ablation study comparing our MSGI-Meta with and without UCB bonus. We will
refer our method with UCB bonus as MSGI-Meta, and our method without UCB bonus as MSGI-Meta
without UCB. Figure 17 shows that UCB bonus facilitates the meta-training of our MSGI-Meta
agents in both Playground and Mining domains.
H.2 Qualitative result on the subtask graph inference
Figure 18 illustrates a qualitative example of the inferred subtask graphs inferred by MSGI-Meta
and MSGI-Rand agents on the Mining-Eval set. The adaptation budget was K = 50 episodes
and episode length was T = 80 steps. Both of MSGI-Meta and MSGI-Rand correctly inferred
most of subtasks in the lower hierarchy (e.g., Get stone, Cut wood, Get string) of the subtask
graph. However, only MSGI-Meta was successful in inferring the preconditions of subtasks in the
highest hierarchy (e.g., Smelt gold, Make goldware, and Craft necklace); MSGI-Rand never had an
experience where their preconditions are all satisfied, and thus failed to learn the preconditions of
these task. It demonstrates that MSGI-Meta with a meta-learned adaptation policy is able to collect
more comprehensive experience for accurate subtask graph inference.
H.3 Quantitative analysis of the adaptation policy
We measured the portion of subtasks being eligible or
completed at least once (i.e., coverage) during adapta-
tion to measure how exploratory MSGI-Meta and ran-
dom policy are. We report the averaged coverage over
	Coverage (%)				
Method	D1	D2	D3	D4	Eval
MSGI-Meta	-89^	-87^	81	^^75^	^^90-
MSGI-Rand.	~83~	7jT	68	^38^	~8T~
the evaluation graph set and 8 random seeds. The table shows that MSGI-Meta can make more
diverse subtasks complete and eligible than the random policy thanks to more accurate subtask graph
inference.
24
Published as a conference paper at ICLR 2020
Get string
Smelt gold
∖ ⅜ : Light furnace
J ∖∣ .	石09
a : Get gold ore
+1.11
a:Get coal
+0.27
: Craft necklace 11 曾：Make goldware
IIIl +5QO I I	运83	I
¾t : Make stone pickaxe
-0.17
■ : Make iron pickaxe
-0.48
(a) A ground-truth subtask graph.
(b) A subtask graph inferred by MSGI-Meta.
(c) A subtask graph inferred by MSGI-Rand.

Figure 18: A qualitative example of subtask graph inference, in the Mining domain.
25
Published as a conference paper at ICLR 2020
I Details of GRProp policy
For self-containedness, we provide the description of GRProp policy from Sohn et al. (2018). We
also make a few modifications on OR (x) and AND (x) in Eqs. 29 and 30.
Intuitively, GRProp policy modifies the subtask graph to a differentiable form such that we can
compute the gradient of modified return with respect to the subtask completion vector in order to
measure how much each subtask is likely to increase the modified return. Let xt be a completion
vector and Gr be a subtask reward vector (see Section 2 for definitions). Then, the sum of reward
until time-step t is given as:
Ut = Gr>xt.	(22)
We first modify the reward formulation such that it gives a half of subtask reward for satisfying the
preconditions and the rest for executing the subtask to encourage the agent to satisfy the precondition
of a subtask with a large reward:
Ubt = Gr> (xt + et)/2.	(23)
Let yAj ND be the output of j-th AND node. The eligibility vector et can be computed from the subtask
graph G and xt as follows:
eit = j∈COhRild	yAjND,	yAj ND =	k∈ACNhiDld	xbtj,k,	xbtj,k	=	xtkwj,k	+ NOT(xtk)(1 -	wj,k),
(24)
where wj,k = 0 if there is a NOT connection between j-th node and k-th node, otherwise wj,k = 1.
Intuitively, xbtj,k = 1 when k-th node does not violate the precondition ofj-th node. The logical AND,
OR, and NOT operations in Eq. 24 are substituted by the smoothed counterparts as follows:
pi eei	λor eei + (1 - λor ) xi ,	(25) OfR	(yeAjND) ,	(26) j ∈C hildi
yeAj ND	= A]D (Xjk),	(27) k∈Childj
Xj,k = wj,k Pk	+ (1 - wj,k)N]T (Pk),	(28)
where X ∈ Rd is the input completion vector,		
OfR (X) =	SoftmaX(worx) ∙ x,	(29)
AND (X) =	Z(x, Wand) Z(l∣X∣l,Wand) ,	(30)
-ɔ--,. NOT (X) =	-wnotx,	(31)
∣∣x∣∣ = d, Z(x,β) = 1 log(1 + exp(βx)) is a soft-plus function, and λοr = 0.6,w∩r = 2,wand =
3, wnot = 2 are the hyper-parameters of GRProp. Note that we slightly modified the implementation
of OR and AND from sigmoid and hyper-tangent functions in (Sohn et al., 2018) to softmax and
softplus functions for better performance. With the smoothed operations, the sum of smoothed and
modified reward is given as:
Uet = Gr>p,	(32)
where p = [p1, . . . , pd] and pi is computed from Eq. 25. Finally, the graph reward propagation policy
is a softmax policy,
∏(θt|G,Xt) = Softmax (TVxtUt) = Softmax (TGj(λ°rVχtet + (1 - λ°J)),	(33)
where we used the softmax temperature T = 40 for Playground and Mining domain, and linearly
annealed the temperature from T = 1 to T = 40 during adaptation phase for SC2LE domain.
Intuitively speaking, we act more confidently (i.e., higher temperature T) as we collect more data
since the inferred subtask graph will become more accurate.
26
Published as a conference paper at ICLR 2020
Adaptation (K episodes) Test phase
Figure 19: (Left) Our MSGI model and (Right) the architecture of adaptation policy πθadapt.
J Implementation Details
J.1 Details of MSGI architecture
Figure 19 illustrates the architecture of our MSGI model. Our adaptation policy takes the agent’s
trajectory τt = {st , ot , rt , dt} at time step t as input, where s = {obs, x, e, step, epi}. We used
convolutional neural network (CNN) and gated rectifier unit (GRU) to encode both the temporal and
spatial information of observation input obs. For other inputs, we simply concatenated all of them
along the dimension after normalization, and encoded with GRU and fully-connected (FC) layers.
Finally, the flat embedding and observation embedding are concatenated with separate heads for the
value and policy output respectively (See supplemental material for more detail).
Our MSGI architecture encodes the observation input using CNN module. Specifically, the ob-
servation embedding is computed by Conv1(16x1x1-1/0)-Conv2(32x3x3-1/0)-Conv3(64x3x3-1/1)-
Conv4(32x3x3-1/1)-Flatten-FC(512)-GRU(512). Other inputs are all concatenated into a single
vector, and fed to GRU(512). In turn, we extracted two flat embeddings using two separate FC(512)
heads for policy and value outputs. For each output, the observation and flat embeddings and concate-
nated into single vector, and fed to FC(512)-FC(d) for policy output and FC(512)-FC(1) for value
output, where d is the policy dimension. We used ReLU activation function in all the layers.
J.2 DETAILS OF TRAINING MSGI-Meta
Algorithm 1 describes the pseudo-code for training our MSGI-Meta model with and without UCB
bonus term. In adaptation phase, we ran a batch of 48 parallel environments. In test phase, we
measured the average performance over 4 episodes with 8 parallel workers (i.e., average over 32
episodes). We used actor-critic method with GAE (Schulman et al., 2016) as follows:
VθL = EG 〜Gtrain Es 〜∏.
∞
-Vθ log πθ
l=0
lY-1 (γλ)kn	δt+l
(34)
δt = rt + γktVθπ(st+1) - Vθπ(st),
(35)
where we used the learning rate η = 0.002, γ = 1, and λ = 0.9. We used RMSProp optimizer with
the smoothing parameter of 0.99 and epsilon of 1e-5. We trained our MSGI-Meta agent for 8000
trials, where the agent is updated after every trial. We used the best hyper-parameters chosen from
the sets specified in Table 4 for all the agents. We also used the entropy regularization with annealed
parameter βent. We started from βent = 0.05 and linearly decreased it after 1200 trials until it reaches
βent = 0 at 3200 trials. During training, we update the critic network to minimize E[(Rt - Vθπ (st))2],
where Rt is the cumulative reward at time t with the weight of 0.03. We clipped the magnitude of
gradient to be no larger than 1.
J.3 Details of training RL2 and HRL
For training RL2 and HRL, we used the same architecture and algorithm with MSGI-Meta. For RL2,
we used the same hyper-parameters except the learning rate η = 0.001 and the critic loss weight of
0.005. For HRL, we used the learning rate η = 0.001 and the critic loss weight of 0.12. We used the
best hyper-parameters chosen from the sets specified in Table 4 for all the agents.
27
Published as a conference paper at ICLR 2020
Hyperparameter	Notation	MSGI-Meta	Methods RL2	HRL
Learning Rate (LR)	η	2e-3	1e-3	1e-3
LR multiplier		0.999	0.999	0.999
GAE	λ	0.9	0.9	0.9
Critic	βcritic	0.12	0.005	0.12
Entropy	βent	0.1	1.0	0.03
UCB	βUCB	1.0	-	-
Architecture	(dflat, dgru)	(512, 512)	(512, 512)	(512, 512)
Table 3: Summary of hyper-parameters used for MSGI-Meta, RL2, and HRL agents.
Hyperparameter	Notation	Values
Learning rate (LR)	η	{1.0, 2.5, 5.0}×{e-5, e-4, e-3}
LR multiplier		{0.96, 0.98, 0.99, 0.993, 0.996, 0.999, 1.0}
GAE	λ	{0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.98, 1.0}
Critic	βcritic	{0.005, 0.01, 0.03, 0.06, 0.12, 0.25}
Entropy	βent	{0.02, 0.05, 0.1, 0.2, 0.5, 1.0, 2.0}
UCB	βuCB	{1.0,3.0}
Architecture	(dflat, dgrU)	{(128, 128), (256, 256), (512, 512)}
Table 4: The range of hyper-parameters we searched over. We did beam-search to find the best parameter with
the priority of η, λ, β, βent, (dflat, dgru), LR-decay.
28