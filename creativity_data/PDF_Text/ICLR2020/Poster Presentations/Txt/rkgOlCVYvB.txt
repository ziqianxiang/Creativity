Published as a conference paper at ICLR 2020
Pure and Spurious Critical Points:
a Geometric Study of Linear Networks
Matthew Trager*
New York University
Kathlen Kohn*
KTH Stockholm
Joan Bruna
New York University
Ab stract
The critical locus of the loss function of a neural network is determined by the
geometry of the functional space and by the parameterization of this space by
the network’s weights. We introduce a natural distinction between pure critical
points, which only depend on the functional space, and spurious critical points,
which arise from the parameterization. We apply this perspective to revisit and
extend the literature on the loss function of linear neural networks. For this type
of network, the functional space is either the set of all linear maps from input to
output space, or a determinantal variety, i.e., a set of linear maps with bounded
rank. We use geometric properties of determinantal varieties to derive new results
on the landscape of linear networks with different loss functions and different
parameterizations. Our analysis clearly illustrates that the absence of “bad” local
minima in the loss landscape of linear networks is due to two distinct phenomena
that apply in different settings: it is true for arbitrary smooth convex losses in the
case of architectures that can express all linear maps (“filling architectures”) but
it holds only for the quadratic loss when the functional space is a determinantal
variety (“non-filling architectures”). Without any assumption on the architecture,
smooth convex losses may lead to landscapes with many bad minima.
1 Introduction
A fundamental goal in the theory of deep learning is to explain why the optimization of the non-
convex loss function of a neural network does not seem to be affected by the presence of non-
global local minima. Many papers have addressed this issue by studying the landscape of the loss
function (Baldi & Hornik, 1989; Choromanska et al., 2015; Kawaguchi, 2016; Venturi et al., 2018).
These papers have shown that, in certain situations, any local minimum for the loss is in fact always
a global minimum. Unfortunately, it is also known that this property does not apply in more general
realistic settings (Yun et al., 2018; Venturi et al., 2018). More recently, researchers have begun
to search for explanations based on the dynamics of optimization. For example, in certain limit
situations, the gradient flow of over-parameterized networks will avoid local minimizers (Chizat &
Bach, 2018; Mei et al., 2018). We believe however that the study of the static properties of the loss
function (the structure of its critical locus) is not settled. Even in the case of linear networks, the
existing literature paints a purely analytical picture of the loss, and provides no sort of explanation as
to “why” such architectures exhibit no bad local minima. A complete understanding of the critical
locus should be a prerequisite for investigating the dynamics of the optimization.
The goal of this paper is to revisit the loss function of neural networks from a geometric perspective,
focusing on the relationship between the functional space of the network and its parameterization.
In particular, we view the loss as a composition
{parameter space} → {functional space} → R.
In this setting, the function ' is almost always convex, however the composition L = ' ◦ μ is not.
Critical points for L can in fact arise for two distinct reasons: either because we are applying `
to a non-convex functional space, or because the parameterizing map μ is locally degenerate. We
distinguish these two types of critical points by referring to them, respectively, as pure and spurious.
* Equal contribution.
1
Published as a conference paper at ICLR 2020
Table 1: Bad local minima in loss landscapes for linear networks
	quadratic loss	other smooth convex loss
filling	no bad minima	no bad minima
non-filling	no bad minima ↑	bad minina exist
Sspecial property of'∣
[determinahtal VarietieS J
<___「Convex optimization'
[over VeCtor SPaCe
Intuitively, pure critical points actually reflect the geometry of the functional space associated with
the network, while SpuriouS CritiCal pointS ariSe aS “artifaCtS” from the parameterization. After
defining pure and CritiCal pointS for arbitrary networkS, we inveStigate in detail the ClaSSifiCation
of CritiCal pointS in the CaSe of linear networkS. The funCtional SpaCe for SuCh networkS Can be
identified with a family of linear mapS, and we Can deSCribe itS geometry uSing algebraiC toolS.
Many of our StatementS rely on a Careful analySiS of the differential of the matrix multipliCation map.
In partiCular, we prove that non-global local minima are necessarily pure CritiCal pointS for Convex
loSSeS, whiCh meanS that many propertieS of the loSS landSCape Can be read from the funCtional
SpaCe. On the other hand, we emphaSize that even for linear networkS it iS poSSible to find many
Smooth Convex loSSeS with non-global loCal minima. ThiS happenS when the funCtional SpaCe iS a
determinantal variety, i.e., a (non-Smooth and non-Convex) family of matriCeS with bounded rank. In
thiS Setting, the abSenCe of non-global minima aCtually holdS in the partiCular CaSe of the quadratiC
loSS, beCauSe of very SpeCial geometriC propertieS of determinantal varietieS that we diSCuSS.
Related Work. Baldi & Hornik (1989) firSt proved the abSenCe of non-global (“bad”) loCal min-
ima for linear networkS with one hidden layer (autoenCoderS). Their reSult waS generalized to the
CaSe of deep linear networkS by KawaguChi (2016). Many paperS have SinCe then Studied the loSS
landSCape of linear networkS under different aSSumptionS (Hardt & Ma, 2016; Yun et al., 2017; Zhou
& Liang, 2017; Laurent & von BreCht, 2017; Lu & KawaguChi, 2017; Zhang, 2019). In partiCular,
Laurent & von BreCht (2017) Showed that linear networkS with “no bottleneCkS” have no bad loCal
minima for arbitrary Smooth loSS funCtionS. Lu & KawaguChi (2017) and Zhang (2019) argued that
“depth doeS not Create loCal minima”, meaning that the abSenCe of loCal minima of deep linear net-
workS iS implied by the Same property of Shallow linear networkS. Our Study of pure and SpuriouS
CritiCal pointS Can be uSed aS a framework for explaining all theSe reSultS in a unified way. The opti-
mization dynamics of linear networkS are alSo an aCtive area of reSearCh (Arora et al., 2019; 2018),
and our analySiS of the landSCape in funCtion SpaCe SetS the Stage for Studying gradient dynamiCS on
determinantal varietieS, aS in Bah et al. (2019). Our work iS alSo CloSely related to objeCtS of Study
in applied algebraiC geometry, partiCularly determinantal varieties and ED discriminants (DraiSma
et al., 2013; Ottaviani et al., 2013). Finally, we mention other reCent workS that Study neural net-
workS uSing algebraiC-geometriC toolS (Mehta et al., 2018; Kileel et al., 2019; Jaffali & Oeding,
2019).
Main contributions.
•	We introduCe a natural diStinCtion between “pure” and “SpuriouS” CritiCal pointS for the loSS funC-
tion of networkS. TheSe notionS provide an intuitive and uSeful language for Studying a Central
aSpeCt in the theory of neural networkS, namely the (over)parameterization of the funCtional SpaCe
and itS effeCt on the optimization landSCape. While moSt of the paper foCuSeS on linear networkS,
thiS viewpoint applieS to more general SettingS aS well (See alSo our diSCuSSion in Appendix A.3).
•	We Study the pure and CritiCal loCuS for linear networkS and arbitrary loSS funCtionS. We Show
that non-global loCal minima are alwayS pure for Convex loSSeS, unifying many known propertieS
on the landSCape of linear networkS.
•	We explain that the abSenCe of “bad” loCal minima in the loSS landSCape of linear networkS iS due
to two diStinCt phenomena and doeS not hold in general: it iS true for arbitrary Smooth Convex
loSSeS in the CaSe of arChiteCtureS that Can expreSS all linear mapS (“filling arChiteCtureS”) and
it holdS for the quadratiC loSS when the funCtional SpaCe iS a determinantal variety (“non-filling
arChiteCtureS”). Without any aSSumption on the arChiteCture, Smooth Convex loSSeS may lead to
many loCal minima. See Table 1.
2
Published as a conference paper at ICLR 2020
Figure 1: Pure and spurious critical points: θ1 is a pure critical point, while θ2 is a spurious critical
point (the level curves on the manifold MΦ describe the landscape in functional space). Note that
θ3 is mapped to the same function as θ2, but it is not a critical point.
•	We provide a precise description of the number of topologically connected components of the
set of global minima. This relates to recent work on “mode connectivity” in loss landscapes of
neural networks (Garipov et al., 2018).
•	We spell out connections between the loss landscape and classical geometric objects such as
caustics and ED discriminants. We believe that these concepts may be useful in the study of more
general functional spaces.
Differential notation. Our functional spaces will be manifolds with singularities, so we will make
use of elementary notions from differential geometry. If M and N are manifolds and g : M → N
is a smooth map, then we write dg(x) for the differential of g at the point x. This means that
dg(x) : TxM → Tg(x)N is the first order linear approximation of g at the point x ∈ M. If M and
N have singularities, then the same definitions apply if we restrict g to smooth points in M whose
image is also smooth in N . For most of our analysis, manifolds will be embedded in Euclidean
spaces, say M ⊂ Rm and N ⊂ Rn, so we can view the tangent spaces TxM and Tg(x)N as also
embedded in Rm and Rn . When N = R, the critical locus of a map g : M → R is defined as
Crit(g) = {x ∈ S mooth(M) | dg(x) = 0}.
2 Preliminaries
2.1	Pure and spurious critical points
A neural network (or any general “parametric learning model”) is defined by a continuous mapping
Φdθ dx	dy	dx	dθ
: R × R x → R y that associates an input vector x ∈ R x and a set of parameters θ ∈ R to
an output vector y = Φ(θ, x) ∈ Rdy. In other words, Φ determines a family of continuous functions
parameterized by θ ∈ Rdθ :
Mφ = {fθ : Rdχ → Rdy | fθ = Φ(θ, ∙)} ⊂ C(Rdx, Rdy).
Even though MΦ is naturally embedded in an infinite-dimensional functional space, it is itself finite
dimensional. In fact, if the mapping Φ is smooth, then MΦ is a finite-dimensional manifold with
singularities, and its intrinsic dimension is upper bounded by dθ . It is also important to note that
neural networks are often non-identifiable models, which means that different parameters can rep-
resent the same function (i.e., fθ = fθ0 does not imply θ = θ0). The manifold MΦ is sometimes
known as a neuromanifold (Amari, 2016). We now consider a general loss function of the form
L = ' ◦ μ, where μ : Rdθ → Mφ is the (over)parameterization of Mφ by θ and ' is a functional
defined on a subset of C(Rdx, Rdy) containing Mφ:1
L : Rdθ -→ Mφ 'lM→ R.	(1)
Definition 1. A critical point θ* ∈ Crit(L) is a pure critical point if μ(θ*) is a critical point
for the restriction '∣Mφ (note that this implicitly requires μ(θ*) to be a smooth point of Mφ). If
θ* ∈ Crit(L) but μ(θ*) ∈ Crit('∣Mφ), We say that θ* is a spurious critical point.
1This setting applies to both the empirical loss and the population loss.
3
Published as a conference paper at ICLR 2020
It is clear from this definition that pure critical points reflect the geometry of the functional space,
while spurious critical points do not have an intrinsic functional interpretation. For example, if
θ* ∈ Crit(L) is a spurious critical point, then it may be possible to find another parameter θ0 that
represents the same function fθ* = fe，and is not a critical point for L (see Figure 1). In contrast,
if θ* is a pure critical point, then all parameters θ0 such that μ(θ0) = μ(θ*) are automatically in
Crit(L), simply because dL(θ0) = d'∣Mφ (μ(θ0)) ◦ dμ(θ0). This will motivate us to study the fiber
{θ | μ(θ) = f} of all parameters mapped to the same function f (particularly when the function f is
a critical point of '∣Mφ).
We note that a sufficient condition for θ* ∈ Crit(L) to be a pure critical point is that the differential
dμ(θ*) at θ* has maximal rank (namely dim Mφ), i.e., that μ is locally a submersion at θ*. Indeed,
we have in this case
0 = dL(θ*) = d'∣Mφ(μ(θ*)) ◦ dμ(θ*) ⇒ d'∣Mφ (μ(θ*)) = 0,
so μ(θ*) is critical for the restriction of ' to Mφ. We also point out a special situation when Mφ is
a convex set (as a subset of C(Rdx, Rdy)) and ` is a smooth convex functional. In this case, the only
critical points of '∣Mφ are global minima, so We deduce that any critical point of L = ' ◦ μ is either
a global minimum or a spurious critical point. The following simple observation gives a sufficient
condition for critical points to be saddles (i.e., they are not local minima or local maxima).
Lemma 2. Let θ* ∈ Crit(L) be a (necessarily spurious) critical point with thefollowing property:
for any open neighborhood U of θ, there exists θ0 in U such that μ(θr) = μ(θ) and θ ∈ Crit(L).
Then θ* is a saddlefor L.
Proof. Assume that θ* is a local minimum (the reasoning is analogous if θ* is a local maximum).
This means that there exists a neighborhood U of θ* such that L(θ) ≥ L(θ*) for all θ ∈ U. In
particular, if θ0 ∈ U is such that μ(θ0) = μ(θ), then θ0 must also be a local minimum. This
contradicts θ0 ∈ Crit(L).	□
This general discussion on pure and spurious critical points applies to any smooth network map Φ
(with possible extensions to the case of piece-wise smooth mappings), and we believe that the dis-
tinction can be a useful tool in the study of the optimization landscape of general networks. In the
remaining part of the paper, we use this perspective for an in-depth study of the critical points of
linear networks. For this type of network, the functional set MΦ can be embedded in a finite di-
mensional ambient space, namely the space of all linear maps Rdx → Rdy . Furthermore, MΦ is
an algebraic variety (a manifold that can have singularities and that can be described by algebraic
equations). We will use basic tools from algebraic geometry to provide a complete description of
pure and spurious critical points, and to prove new results on the landscape of linear networks.
2.2 Linear networks and determinantal varieties
A linear network is a map Φ : Rdθ × Rdx → Rdy of the form
Φ(θ, x) = Wh...W1x,	θ = (Wh,...,W1) ∈ Rdθ,
(2)
where Wi ∈ Rdi×di-1 are matrices (so d0 = dx, dh = dy, and dθ = d0d1 + d1d2 + . . . + dh-1dh).
The functional space is in this case a subset of the space of all linear maps Rd0 → Rdh . As in (1),
we can decompose a loss function L for a linear network Φ as
Rdh×dh-1 × ... × Rdι×d0 -→ _ Rdh×d0	-→ R_
(Wh ,...,W1) I—› W = Wh ...W1 I—> '(W).
(3)
Here μd is the matrix multiplication map for the sequence of widths d = (dh,..., do), and ' is a
functional on the space of (dh × d0)-matrices. In practice, itis typically a functional that depends on
the training data, e.g. '(W) = ∣∣ WX 一 Y∣∣2 for fixed matrices X,Y.2 Note that even if ' is a convex
functional, the set Mφ will often not be a convex set. In fact, it is easy to see that the image of μd is
the space Mr of (dh × d0)-matrices of rank at most r = min{d0, . . . , dh}. Ifr < min(d0, dh), this
2Our setting can also be applied when ' includes a regularizer term defined in function space, e.g., '(W)=
∣∣WX - Yk2 + λR(W).
4
Published as a conference paper at ICLR 2020
set is known as a determinantal variety, a classical object of study in algebraic geometry (Harris,
1995). It is in fact an algebraic variety, i.e., it is described by polynomial equations in the matrix
entries (namely, it is the zero-set of all (r + 1) × (r + 1)-minors), and it is well known that the
dimension of Mr is r(m + n - r). Furthermore, for r > 0, the variety Mr has many singularities:
its singular locus is exactly Mr-1 ⊂ Mr, the set of all matrices with rank strictly smaller than r.
We refer the reader to Appendix A.1 for more details on determinantal varieties.
3 Main results
In this section, we investigate the critical locus Crit(L) of general functions L : Rdθ → R of the
form L = ' ◦ μd where ' : Rdh ×d0 → R is a (often convex) smooth map, and μd is the matrix
multiplication map introduced in (3). By studying the differential of μd, We will characterize pure
and spurious critical points of L. As previously noted, the image of μd is Mr ⊂ Rdh×d0 where
r = min{di}. In particular, we distinguish between two cases:
•	We say that the map μd is filling if r = min{d0, dh}, so Mr = Rdh×d0. In this case, the
functional space is smooth and convex.
•	We say that the map μd is non-filling if r < min{do, dh}, so Mr ( Rdh ×d0 is a determi-
nantal variety. In this case, the functional space is non-smooth and non-convex.
3.1 Properties of the matrix multiplication map
We present some general results on the matrix multiplication map μd, which we will apply to linear
networks in the next subsection. These facts may also be useful in other settings, for example, to
study the piece-wise linear behavior of ReLU networks.
We begin by noting that the differential map of μd can be written explicitly as
• ∙ ∙ ∙ ∙
dμd(θ)(Wh, ...,Wι) = WhWh-ι ...Wι + WhWh-1 ...Wι + ... + Wh ... W2W1.	(4)
Given a matrix M ∈ Rm×n, we denote by Row(M) ⊂ Rn and Col(M) ⊂ Rm the vector spaces
spanned by the rows and columns of M, respectively. Writing W>i = WhWh-1 . . . Wi+1 and
W<i = Wi-IWi-ι... Wι, the image of dμd(θ) in (4) is
Rdh 0 Row(W<h) + ... + Col(W>i) 0 Row(W<i) + ... + Col(W>ι) 0 Rd0.	(5)
From this expression, we deduce the following useful fact.
Lemma 3. The dimension of the image ofthe differential dμd at θ = (Wh,..., Wι) is given by
h	h-1
rk(dμd(θ)) = Xrk(W>i) ∙ rk(W<i) - X rk(W>i) ∙ rk(W<i+ι),
i=1	i=1
where we use the convention that W<1 = Id0, W>h = Idh are the identity matrices of size d0, dh.
We can use Lemma 3 to characterize all cases when the differential dμd at θ = (Wh,...,Wι) has
full rank (i.e., when the matrix multiplication map is a local submersion onto Mr).
Theorem 4. Let r = min{di}, θ = (Wh,..., Wι), and W = μd(θ).
•	(Filling case) If r = min{dh,d0}, the differential dμd(θ) has maximal rank equal to
dim Mr = dhd0 if and only if, for every i ∈ {1, 2, . . . , h - 1}, either rk(W>i) = dh
or rk(W<i+1) = d0 holds.
•	(Non-filling case) If r < min{dh, d0}, the differential dμd(θ) has maximal rank equal to
dim Mr = r(dh + do - r) ifand only if rk( W) = r.
Furthermore, in both situations, if rk(W) = e < r, then the image of dμd(θ) always contains the
tangent space TWMe of Me ⊂ Mr at W.
5
Published as a conference paper at ICLR 2020
We note that dμd(θ) has always maximal rank when rk(W) = r = min{di}, however in the filling
case it is possible to obtain a local submersion even when rk( W) < r (see Example 19 in appendix).
We next describe the fiber of the matrix multiplication map, that is, the set
μ-1 (W) = {(Wh, ...,Wι) | W = Wh ...Wι, Wi ∈ Rdi×diτ }.
It will be convenient to refer to μ-1(W) as the set of d-factorizations of W. We are interested in
understanding the structure of μ-1(W) since, as argued in Section 2.1, pure critical loci consist of
fibers of “critical functions”. The following result completely describes the connectivity of μ-1(W).
Theorem 5. Let r = min{di}. If rk(W) = r, then the set of d -factorizations μ-1(W) of W has
exactly 2b path-connected components, where b = #{i | d = r, 0 < i < h}. If rk(W) < r, then
μ-1(W) is always path-connected.
3.2	Application to linear networks
We now apply the general results from the previous subsection to study the critical locus Crit(L)
with L = ' ◦ μd, where ' is any smooth function. In the following, we always use r = min{ri} and
W = μd(θ). The next two facts follow almost immediately from Theorem 4.
Proposition 6. If θ is such that dμd(θ) has maximal rank (see Theorem 4), then θ ∈ Crit(L) if
and only if W ∈ Crit('∣Mr), and θ is a minimum (resp., saddle, maximum) for L if and only if W
is a minimum (resp., saddle, maximum) for '∣Mr. If rk(W) = r (WhiCh implies that dμd(θ) has
maximal rank) and θ ∈ Crit(L), then all d-factorizations of W also belong to Crit(L).
Proposition 7. If θ ∈ Crit(L) with rk(W) = e ≤ r, then W ∈ Crit('Me). In other words,
if rk(W) < r, then θ ∈ Crit(L) implies that W is a critical point for the restriction of ' to a
smaller determinantal variety Me (which is in the singular locus of the functional space Mr in the
non-filling case).
Note that if dh = 1, then either W = 0 or rk(W) = 1, and in the latter case Proposition 7 implies
that W ∈ Crit('∣Rdo∖{o}). If ' is convex, we immediately obtain that all critical points (not just
local minima, as in Laurent & von Brecht (2017)) below a certain energy level are global minima.
Corollary 8. Assume that ` is a smooth convex function and that dh = 1. If θ ∈ C rit(L), then
either W = μd(θ) = 0 or θ is global minimum for L.
Proposition 7 shows that critical points for L such that rk(W) < r correspond to critical points for '
restricted to a smaller determinantal variety. Using Lemma 2, it is possible to show that these points
are essentially always saddles for L.
Proposition 9. Let θ ∈ Crit(L) be such that rk(W) < r, and assume that d'(W) = 0. Then,
for any neighborhood U of θ, there exists θ0 in U such that μd(θ0) = W but θ0 ∈ Crit(L). In
particular, θ is a saddle point.
Proposition 10. Let' be any smooth convex function, and let L = ' ◦ μd. If θ is a non-global local
minimum for L, then necessarily rk(W) = r (so θ is a pure critical point). In particular, L has
non-global minima ifand only if '∣Mr has non-global minima.
This statement succinctly explains many known facts on the landscape of linear networks. For
example, we recover the main result from (Laurent & von Brecht, 2017), which states that when ` is
a smooth convex function and μd is filling (r = min{dh, do}), then all local minima for L are global
minima: indeed, this is because Mr = Rdh ×d0 is a linear space, so '∣Mr does not have non-global
minima. On the other hand, when μd is not filling, the functional space is not convex, and multiple
local minima may exist even when ` is a convex function. We will in fact present many examples of
smooth convex functions ' such that L = ' ◦ μd has non-global local minima (see Figure 3). In the
special case that ` is a quadratic loss (for any data distribution), then it is a remarkable fact that there
are no non-global local minima even when μd is not filling (Baldi & Hornik, 1989; Kawaguchi,
2016). In the next section, we will provide an intrinsic geometric justification for this property.
Remark 11. In Laurent & von Brecht (2017), the authors observe that their “structural hypothesis”
(i.e., for us, the fact that the network is filling) is a necessary assumption for their main result,
6
Published as a conference paper at ICLR 2020
as otherwise critical points of ` might not lie in the functional space of the network. This last
observation however does not imply the necessity of the filling assumption, and indeed in the case
of the quadratic loss there are no local bad minima despite the fact Mr ( Rdh×d0.
Finally, we conclude this section by pointing out that although the pure critical locus is determined
by the geometry of the functional space, the “lift” from function space to parameter space is not com-
PIetely trivial. In particular, there is always a large positive-dimensional set of critical parameters
associated with a critical linear function W (all possible d-factorizations of W). More interestingly,
this set may be topologically disconnected into a large number of components that are all function-
ally equivalent (see Theorem 5). This observation agrees with the folklore knowledge that neural
networks can have many disconnected valleys where the loss function achieves the same value.
3.3	The quadratic loss
We now assume that ' : Rdy ×d → R is of the form '(W) = ∣∣WX - Y∣∣2, where X ∈ Rdχ×s
and Y ∈ Rdy ×s are fixed data matrices. As mentioned above, it is known that L = ' ◦ μd has no
non-global local minima, even when μd is non-filling (Baldi & Hornik, 1989; Kawaguchi, 2016).
In this section, we discuss the intrinsic geometric reasons for this special behavior.
It is easy to relate the landscape of L with the Euclidean distance function from a determinantal
variety (or, equivalently, to the problem low-rank matrix approximation). Indeed, we know from
Proposition 10 that L has non-global local minima if and only if the same is true for '∣Mr. Further-
more, assuming that XXT has full rank, we use its square root P = (XXT)1/2 as a positive definite
matrix to derive
∣WX -Y∣2 = hWX,WXiF - 2hWX,YiF + hY,YiF
= hWP,WPiF - 2hWP,YXTP-1iF + hY,YiF
= ∣WP - Q0∣2 + const.,
where Q0 = YXTP-1 and “const.” only depends on the data matrices X and Y. Hence, minimiz-
ing '(W) is equivalent to minimizing ∣∣Wp - Qo∣∣2. Since the bijection Mr → Mr, W → WP
is also a bijective on the tangent spaces, it provides a one-to-one correspondence from the crit-
ical points of minW ∈Mr ∣WP - Q0 ∣2 to the critical points of minW ∈Mr ∣W - Q0 ∣2 . All in
all, studying the critical points of '∣Mr is equivalent to studying the critical points of the function
hQ0(W) = ∣W - Q0∣2 where W is restricted to the determinantal variety Mr.
The function hQ0 (W) is described by following generalization of the classical Eckart-Young The-
orem. The formulation we prove is an extension of Example 2.3 in Draisma & Horobet (2014) and
Theorem 2.9 in Ottaviani et al. (2013). We consider a fixed matrix Q0 ∈ Rdy ×dx and a singular
value decomposition (SVD) Q0 = UΣVT, where we assume Σ ∈ Rdy ×dx has decreasing diagonal
entries σ1, . . . , σm, with m = min(dy, dx). For any I ⊂ {1, 2, . . . , m} we write ΣI ∈ Rdy ×dx for
the diagonal matrix with entries σI,1, . . . , σI,m where σI,i = σi if i ∈ I and σI,i = 0 otherwise.
Theorem 12. If the singular values of Q0 are pairwise distinct and positive, hQ0 |Mr has exactly
mr critical points, namely the matrices QI = UΣIV T with #(I) = r. Moreover, its unique local
and global minimum is Q{1,...,r}. More precisely, the index of QI as a critical point of hQ0 |Mr (i.e.,
the number of negative eigenvalues of the Hessian matrix for any local parameterization) is
index(QI) = #{(j, i) ∈ I × Ic | j > i},	where Ic = {1, . . . , m} \ I.
In the appendix we present a more general version of this statement without the assumption that
the singular values of Q0 are pairwise distinct and positive. The surprising aspect of this result
is that the structure of the critical points is the same for almost all choices of Q0 . We want to
emphasize that this is a special behavior of determinantal varieties with respect to the Euclidean
distance, and the situation changes drastically ifwe apply even infinitesimal changes to the quadratic
loss function. More precisely, any linear perturbation of the Euclidean norm will result in a totally
different landscape, as the following example shows (more details are given in Appendix A.2).
Example 13. Let us consider the variety M1 ⊆ R3×3 of rank-one (3 × 3)-matrices. By Theorem 12,
for almost all Q0, the function hQ0 |M1 has three (real) critical points. Applying a linear change of
coordinates to Rdy ×dx = Rdydχ yields a different quadratic loss Bq。. Using tools from algebraic
7
Published as a conference paper at ICLR 2020
Figure 2: Left: If V ⊂ R2 is an ellipse, the distance function hu(p) = ∣∣p - u∣∣2 restricted to V
generally has 2 or 4 real critical points, depending on whether U lies inside or outside the diamond-
shaped region bounded by the caustic curve. Right: If V ⊂ R2 is a circle, then the caustic curve
degenerates to a point and the distance function generically has always 2 real critical points.
geometry, it is possible to show that for almost all linear coordinate changes (an open dense set),
the function Bq。∣Mι has 39 critical points over the complex numbers.3 The number of real critical
points however varies, depending on whether Q0 belongs to different open regions separated by a
caustic hypersurface in R3×3. Furthermore, the number of local minima varies as well; in particular,
it is no longer true that all Q0 admit a unique local minimum. Figure 3 presents some simple
computational experiments illustrating this behavior.
For all determinantal varieties, the situation is similar to the description in Example 13. More
generally, given an algebraic variety V ⊂ Rn and a point u ∈ Rn, the number of (real) critical points
of the distance function hu(p) = ∣p - u∣2 restricted to V is usually not constant as u varies: the
behavior changes when u crosses the caustic hypersurface, orED (Euclidean distance) discriminant,
of V; see Figure 2. In the case of determinantal varieties with the standard Euclidean distance, this
caustic hypersurface (more precisely its real locus) degenerates toa set of codimension 2, which does
not partition the space into different regions. This is analogous to the case of the circle in Figure 2.
13	∙
11	w	«
9	∙	∙	®
suδd -eo⅞oH-O-9CIEnU
1
0	1	2	3	4	5
number of local minima
Figure 3: Real critical points and local minima for random choices of hQ0 ∣M1 as defined in Exam-
ple 13. The size of each disk is proportional to the number of instances we found with that number
of critical points and local minima. This shows that linear networks with a convex loss may indeed
have multiple non-global local minima. More details in Appendix A.2 (Table 2 and Experiment 1).
3.4 Using different parameterizations: normalized networks
In the simple linear network model (2), the functional space Mr ⊂ Rdh×d0 is parameterized using
the matrix multiplication map μd. On the other hand, one can envision many variations of this
model that are network architectures with the same functional space but parameterized differently.
Examples include linear networks with skip connections, or convolutional linear networks. In this
3This means that the algebraic equations corresponding to the vanishing of the differential have exactly 39
complex solutions.
8
Published as a conference paper at ICLR 2020
subsection, we take a look at a model for normalized linear networks: these are maps of the form
Wh 1	W1
ψ(θ,x)=WhkWh-1k...kW⅛χ,	θ=(Wh,…心
(6)
where Wi ∈ Rdi×di-1 as before. This is a simple model for different types of weight normalization
schemes often used in practice. It is easy to see that the difference between (6) and our previous
linear network lies only in the parameterization of linear maps, since for normalized networks the
matrix multiplication map is replaced by
ω→Rdh×d0,	(Wh,…，W0-W=WhkWh-1k...kW1k,
where Ω = {(Wh,..., Wι) | Wi = 0, i = 1,...,h - 1} ⊂ Rdθ. According to our definitions, if
L = 'oμd and L0 = 'qv& are losses respectively for linear networks and normalized linear networks,
then the pure critical loci of L and L0 will correspond to each other (since these only depend on
the functional space), but a priori the spurious critical loci induced by the two parameterizations
may be different. In this particular setting, however, we show that this is not the case: the new
paramerization effectively does not introduce different critical points, and in fact makes the critical
locus slightly smaller.
Proposition 14. If L0 = ` ◦ Vd and L = ' ◦ μd, then the critical locus Crit(Lr) is in “correspon-
dence” with Crit(L) ∩ Ω, meaning that
{νd(θ0) | θ0 ∈ Crit(L0)} = {μd(θ) | θ ∈ Crit(L) ∩ Ω}.
4 Conclusions
We have introduced the notions of pure and spurious critical points as general tools for a geomet-
ric investigation of the landscape of neural networks. In particular, they provide a basic language
for describing the interplay between a convex loss function and an overparameterized, non-convex
functional space. In this paper, we have focused on the landscape of linear networks. This simple
model is useful for illustrating our geometric perspective, but also exhibits several interesting (and
surprisingly subtle) features. For example, the absence of non-global minima in the loss landscape
is a rather general property when the architecture is “filling”, while in the “non-filling” setting it is
a special property that holds for the quadratic loss. Furthermore, we have observed that even in this
simple framework global minima can have (possibly exponentially) many disconnected components.
In the future, we hope to extend our analysis to different network models. For example, we can
use our framework to study networks with polynomial activations (Kileel et al., 2019), which are a
direct generalization of the linear model. We expect that an analysis of pure and spurious critical
points in this context can be used to address a conjecture in Venturi et al. (2018) regarding the gap
between “upper” and “lower” dimensions in functional space. A geometric investigation of networks
with smooth non-polynomial activations is also possible; in that setting, the parameter space and
the functional space are usually of the same dimension (i.e., dθ = dim(MΦ)), however there is
still an interesting stratification of singular loci, as explained for example in (Amari, 2016, Section
12.2.2). General “discriminant hypersurfaces” can also be used to describe qualitative changes in
the landscape as the data distribution varies. Finally, extending our analysis to networks with ReLU
activations will require some care because of the non-differentiable setting. On the other hand, it
is clear that ReLU networks behave as linear networks when restricted to appropriate regions of
input space: this suggests that our study of ranks of differentials may be a useful building block for
pursuing in this important direction.
Acknowledgements. We thank James Mathews for many helpful discussions in the beginning of
this project. We are gratuful to ICERM (NSF DMS-1439786 and the Simons Foundation grant
507536) for the hospitality during the academic year 2018/2019 where many ideas for this project
were developed. MT and JB were partially supported by the Alfred P. Sloan Foundation, NSF RI-
1816753, NSF CAREER CIF 1845360, and Samsung Electronics. KK was partially supported by
the Knut and Alice Wallenbergs Foundation within their WASP AI/Math initiative.
9
Published as a conference paper at ICLR 2020
References
Shun-ichi Amari. Information Geometry and Its Applications, volume 194 of Applied Mathematical
Sciences. Springer Japan, Tokyo, 2016. ISBN 978-4-431-55977-1 978-4-431-55978-8. doi:
10.1007/978-4-431-55978-8.
Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient
descent for deep linear neural networks. arXiv preprint arXiv:1810.02281, 2018.
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix
factorization. arXiv preprint arXiv:1905.13655, 2019.
Bubacarr Bah, Holger Rauhut, Ulrich Terstiege, and Michael Westdickenberg. Learning deep
linear neural networks: Riemannian gradient flows and convergence to global minimizers.
arXiv:1910.05505 [cs, math], November 2019.
Pierre Baldi and Kurt Hornik. Neural networks and principal component analysis: Learning from
examples without local minima. Neural Networks, 2(1):53-58, January 1989. ISSN 08936080.
doi: 10.1016/0893-6080(89)90014-2.
Lenaic Chizat and Francis Bach. On the Global Convergence of Gradient Descent for Over-
parameterized Models using Optimal Transport. arXiv:1805.09545 [cs, math, stat], May 2018.
Anna Choromanska, Mikael Henaff, Michael Mathieu, Gerard Ben Arous, and Yann LeCun. The
Loss Surfaces of Multilayer Networks. In Proceedings of the Eighteenth International Conference
on Artificial Intelligence and Statistics, AISTATS 2015, San Diego, California, USA, May 9-12,
2015, 2015.
Jan Draisma and Emil Horobet. The average number of critical rank-one approximations to a tensor.
arXiv:1408.3507 [math], August 2014.
Jan Draisma, Emil Horobet, Giorgio Ottaviani, Bernd Sturmfels, and Rekha R. Thomas. The Eu-
clidean distance degree of an algebraic variety. arXiv:1309.0049 [math], August 2013.
Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry Vetrov, and Andrew Gordon Wilson.
Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs. arXiv:1802.10026 [cs, stat],
October 2018.
Daniel R. Grayson and Michael E. Stillman. Macaulay2, a software system for research in algebraic
geometry. Available at http://www.math.uiuc.edu/Macaulay2/, 2019.
Moritz Hardt and Tengyu Ma. Identity Matters in Deep Learning. arXiv:1611.04231 [cs, stat],
November 2016.
Joe Harris. Algebraic Geometry: A First Course. Number 133 in Graduate Texts in Mathematics.
Springer, New York, corr. 3rd print edition, 1995. ISBN 978-0-387-97716-4.
Hamza Jaffali and Luke Oeding. Learning algebraic models of quantum entanglement. arXiv
preprint arXiv:1908.10247, 2019.
Kenji Kawaguchi. Deep Learning without Poor Local Minima. CoRR, abs/1605.07110, 2016.
Joe Kileel, Matthew Trager, and Joan Bruna. On the expressive power of deep polynomial neural
networks. arXiv preprint arXiv:1905.12207, 2019.
Thomas Laurent and James von Brecht. Deep linear neural networks with arbitrary loss: All local
minima are global. arXiv:1712.01473 [cs, stat], December 2017.
John M. Lee. Introduction to Smooth Manifolds. Number 218 in Graduate Texts in Mathematics.
Springer, New York, 2003. ISBN 978-0-387-95495-0 978-0-387-95448-6.
Haihao Lu and Kenji Kawaguchi. Depth Creates No Bad Local Minima. arXiv:1702.08580 [cs,
math, stat], February 2017.
10
Published as a conference paper at ICLR 2020
Dhagash Mehta, Tianran Chen, Tingting Tang, and Jonathan D. Hauenstein. The loss surface of
deep linear networks viewed through the algebraic geometry lens. arXiv:1810.07716, 2018. URL
http://arxiv.org/abs/1810.07716.
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A Mean Field View of the Landscape of
Two-Layers Neural Networks. arXiv:1804.06561 [cond-mat, stat], April 2018.
Giorgio Ottaviani, Pierre-Jean Spaenlehauer, and Bernd Sturmfels. Exact Solutions in Structured
Low-Rank Approximation. arXiv:1311.2376 [cs, math, stat], November 2013.
Luca Venturi, Afonso S Bandeira, and Joan Bruna. Spurious valleys in two-layer neural network
optimization landscapes. arXiv preprint arXiv:1802.06384, 2018.
Chulhee Yun, Suvrit Sra, and Ali Jadbabaie. Global optimality conditions for deep neural networks.
arXiv preprint arXiv:1707.02444, 2017.
Chulhee Yun, Suvrit Sra, and Ali Jadbabaie. Small nonlinearities in activation functions create bad
local minima in neural networks. arXiv preprint arXiv:1802.03487, 2018.
Li Zhang. Depth creates no more spurious local minima. arXiv preprint arXiv:1901.09827, 2019.
Yi Zhou and Yingbin Liang. Critical Points of Neural Networks: Analytical Forms and Landscape
Properties. arXiv:1710.11205 [cs, stat], October 2017.
A Appendix
A. 1 Determinantal varieties
We present some additional properties of determinantal varieties. For proofs and more details, we
refer the reader to Harris (1995). Given r < min(m, n), the r-th determinantal variety Mr ⊂ Rm×n
is defined as the set of matrices with rank at most r:
Mr = {P ∈ Rm×n | rk(P) ≤ r} ⊂ Rm×n.
As mentioned in the main part of the paper, Mr is an algebraic variety of dimension r(m + n - r),
that can be described as the zero-set of all (r + 1) × (r + 1) minors. For r > 0, the the singular
locus of Mr is exactly Mr-1 ⊂ Mr. Some of our proofs will rely on the following explicit
characterization of tangent space of determinantal varieties: given a a matrix P ∈ Rm×n of rank
exactly r (so P is a smooth point on Mr) we have that
TP Mr = Rm 0 Row (P) + Col (P) 0 Rn ⊂ Rm×n.
We will also make use of the normal space to the tangent space TPMr at P, with respect to the
Frobenius inner product. This is given by
(TPMr)⊥ = Col(P)⊥ 0 Row(P)⊥,
where Col(P)⊥ and Row(P)⊥ are the orthogonal spaces to Col(P) and Row(P), respectively.
A.2 Euclidean distance degrees and discriminants
In this section, we informally discuss some algebraic notions related to ED (Euclidean distance)
degrees and discriminants. A detailed presentation can be found in Draisma et al. (2013). Given
an algebraic variety V ⊂ Rn and a point u ∈ Rn , the number of real critical points of the distance
function hu(p) = kp - uk2 restricted to V is only locally constant as u varies. In general, the
behavior changes when u crosses the caustic hypersurface, or ED (Euclidean distance) discriminant,
of V . The ED discriminant can be defined over the complex numbers, and in this setting it is indeed
always a hypersurface (i.e., it has codimension one), however it can have higher codimension over
the real numbers. For instance, for a circle in the complex plane with the origin as its center, a point
(u1, u2) ∈ C2 is on the ED discriminant if and only if u12 + u22 = 0. This defines a curve in the
complex plane whose real locus is a point (see right side of Figure 2). By the Eckart-Young Theorem
(Theorem 12), the ED discriminant of the determinantal variety Mr is the locus of all matrices Q0
11
Published as a conference paper at ICLR 2020
with at least two coinciding singular values, so it is defined by the discriminant of Q0Q0T . As in the
case of the circle, the ED discriminant of Mr has codimension two in Rdy ×dx.
Over the complex numbers, the number of critical points of the distance function hu restricted to V is
actually the same for every point u ∈ Cn not on the ED discriminant of V. This quantity is known as
the ED degree of the variety V . For instance, a circle has ED degree two whereas an ellipse has ED
degree four (on the left side of Figure 2, points u outside of the caustic curve yield two real critical
points and two imaginary critical points). The Eckart-Young Theorem (Theorem 12) tells us that the
ED degree of the determinantal variety Mr ⊂ Rdy ×dx is mr where m = min(dx, dy). As argued
in the main part of the paper, this does not hold any longer after perturbing either the determinantal
variety or the Euclidean distance slightly, even using only a linear change of coordinates. For an
algebraic variety V ⊂ Cn, a linear change of coordinates is given by an automorphism 夕：Cn →
Cn. For almost all such automorphisms (i.e., for all 夕 except those lying in some subvariety of
GL(n, C)) the ED degree of 夕(V) is the same; see Theorem 5.4 in Draisma et al. (2013). This
quantity is known as the general ED degree of V. For instance, almost all linear coordinate changes
will deform a circle into an ellipse, such that the general ED degree of the circle is four.
In the above definition of the general ED degree, we fixed the standard Euclidean distance and per-
turbed the variety. Alternatively, we can fix the variety and change the standard Euclidean distance
k ∙ k to distφ = k夕(∙)∣∣. The new distance function hφ,u(p) = distφ(p - u)2 from U satisfies
hφ(u)(φ(p)) = hidq(u)(夕(P)) = hφ,u(p). Hence, the ED degree of 夕(V) with respect to the stan-
dard Euclidean distance distid = k ∙ k equals the ED degree of V with respect to the perturbed
Euclidean distance distφ. In particular, the general ED degree of V can be obtained by comput-
ing the ED degree after applying a sufficiently random linear change of coordinates on either the
Euclidean distance or the variety V itself.
As in the case of a circle, the general ED degree of the determinantal variety Mr is not equal to the
ED degree ofMr. Furthermore, there is no known closed formula for the general ED degree ofMr
only involving the parameters dx, dy and r. In the special case of rank-one matrices, one can derive
a closed expression from the Catanese-Trifogli formula (Theorem 7.8 in Draisma et al. (2013)): the
general ED degree of M1 is
dx +dy
s=0
(-1)s(2dx+dy+1-s - 1)(dx +dy -s)!
i+j=s
i≤dx, j≤dy
(dx - i)!(dy - j)!
Σ
This expression yields 39 for dx = dy = 3, as mentioned in Example 13. For general r, formulas
for the general ED degree of Mr involving Chern and polar classes can be found in Ottaviani et al.
(2013); Draisma et al. (2013). A short algorithm to compute the general ED degree of Mr is given
in Example 7.11 of Draisma et al. (2013); it uses a package for advanced intersection theory in the
algebro-geometric software Macaulay2 (Grayson & Stillman, 2019).
This discussion shows that the Eckart-Young Theorem is indeed very special. The intrinsic reason
for this is that the determinantal variety Mr intersects the “isotropic quadric” associated with the
standard Euclidean distance (i.e., zero locus of X12,1 + . . . + Xd2 ,d in Cdx ×dy) in a particular way
(i.e., non-transversely). Performing a random linear change of coordinates on either Mr or the
isotropic quadric makes the intersection transverse. So the ED degree after the linear change of
coordinates is the general ED degree of Mr, and the Eckart-Young Theorem does not apply.
In summary, we have observed that the degeneration from an ellipse to a circle is analogous to the
degeneration from a determinantal variety with a perturbed Euclidean distance to the determinantal
variety with the standard Euclidean distance: in both cases, the ED degree drops because the situa-
tion becomes degenerate. Moreover, the ED discriminant drops dimension, which causes the special
phenomenon that the number of real critical points is almost everywhere the same.
Experiment 1. In general, it is very difficult to describe the open regions in Rn that are separated by
the ED discriminant ofa variety V ⊂ Rn. Finding the “typical” number of real critical points for the
distance function hu restricted to V, requires the computation of the volumes of these open regions.
In the current state of the art in real algebraic geometry, this is only possible for very particular
varieties V. For these reasons, and to get more insights on the typical number of real critical points
of determinantal varieties with a perturbed Euclidean distance, we performed computational exper-
iments with Macaulay2 (Grayson & Stillman, 2019) in the situation of Example 13. We fixed the
12
Published as a conference paper at ICLR 2020
Table 2: Number of critical points (columns) and number of minima (rows) in our experiments
	#(critical points)							13
	1		3	5	7	9	11	
	1	0	476	120	1	0	0	0
#(local	2	0	0	805	190	10	0	0
minima)	3	0	0	0	228	116	21	0
	4	0	0	0	0	16	12	5
determinantal variety M1 ⊆ R3×3 of rank-one (3 × 3)-matrices. In each iteration of the experi-
ment, We picked a random automorphism 夕：R3×3 → R3×3 and a random matrix Qo ∈ R3×3. We
first verified that the number of complex critical points of the perturbed quadratic distance function
hφ,Qo restricted to Mi is the expected number 39. After that, We computed the number of real
critical points and the number of local minima among them. Our results for 2000 iterations can
be found in Table 2 and Figure 3. Although this is a very rudimentary experiment in an extremely
simple setting, it provides clear evidence that the number of local minima of the perturbed distance
function is generally not one.
Implementation details: We note that our computations of real critical points and local minima
involved numerical methods and might thus be affected by numerical errors. In our implementation
We used several basic tests to rule out numerically bad iterations, so that We can report our results
With high confidence. The entries of the random matrix Q0 are independently and uniformly chosen
among the integers in Z = {-10, -9,..., 9,10}. The random automorphism 夕 is given by a matrix
in Z9×9 Whose entries are also chosen independently and uniformly at random.
A.3 Pure and spurious critical points in predictor space
We illustrate a variation of our functional setting Where the notions of pure and spurious can also be
naturally applied. We consider a training sample x1, . . . , xN ∈ Rdx, y1, . . . , yN ∈ R (for notational
simplicity We use dy = 1 but this is not necessary). We then Write an empirical risk of the form
,. ʌ , .
L(θ)= g(Y(θ),Y),
where Y(θ) = (Φ(θ, xi),..., Φ(Θ,xn)) ∈ RN, Y = (yi,..., yN) ∈ RN and g : RN X RN → R is
a convex function. As θ varies, Y(θ) defines a “predictor manifold” Y ⊂ RN, which depends only
on the input data xi, . . . , xN, but not on θ. The function L(θ) can be naturally seen as a composition
Rdθ →η Y →g R,
where η(θ) = Y(θ) ∈ Y. We may now distinguish again between “pure” and “spurious” critical
points for L. In an underparameterized regime dθ < N, or if the input data xi, . . . , xN is in
some way special, then Y ( RN is a submanifold (with singularities), and critical points may
arise because we are restricting g to Y (pure), or because of the parameterization map η (spurious).
In a highly overparameterized regime dθ N (which is usually the case in practice), we expect
Y = RN. This can be viewed as analogous to the “filling” situation described for linear networks in
this paper. In particular, all critical points that are not global minima for L are necessarily spurious,
since g |Y = g is convex.
A.4 Proof of Theorem 4
We first show Lemma 3 with help of the following general observation:
Proposition 15. Let Vi+ ⊆ V2+ ⊆ . . . ⊆ Vh+ and Vi- ⊇ V2- ⊇ . . . ⊇ Vh- be vector spaces with
dimensions ri+ := dim(Vi+) and ri- := dim(Vi-) for i = 1, . . . , h. Then we have
h	h-i
dim ((V+ 乳 V-) + (V+ 乳 V-) + ... + (V+ 乳 V-)) = X r+r- - X r+、+「
i=i	i=i
13
Published as a conference paper at ICLR 2020
Proof. We prove this assertion by induction on h. The base case (h = 1) is clear: dim(V1+ 0
V-) = r+r-. For the induction step, we set V := (%+ 0 %-) + ... + (Vh+-1 0 Vh-1). The key
observation is that the inclusions V1+ ⊆ V2+ ⊆ . . . ⊆ Vh+ and V1- ⊇ V2- ⊇ . . . ⊇ Vh- imply that
V∩ (Vh+ 0 Vh- ) = Vh+-1 0 Vh- . Hence, applying the induction hypothesis to V, we derive
dim (V + (V+ 0 V-)) = dim(V) + dim(V++ 0 V-) — dim(V+-ι 0 V-)
hX-1h-2
ri+ri- —	ri+ri-+1 + rh+ rh- — rh+-1 rh-
i=1
h	h-1
=Er+r-- Er+ri+ι.	口
i=1	i=1
Lemma 3. The dimension of the image ofthe differential dμd at θ = (Wh,..., Wι) is given by
h	h-1
rk(dμd(θ)) = X rk(W>i) ∙ rk(W<i) — X rk(W>i) ∙ rk(W<i+1),
i=1	i=1
where we use the convention that W<1 = Id0, W>h = Idh are the identity matrices of size d0, dh.
Proof. The image of the differential dμd(θ) is given in (5). Due to
Col(W) ⊆ Col(W>ι) ⊆ ... ⊆ Col(W>h-ι) = Col(Wh),
—	(7)
Row(W) ⊆ Row(W<h) ⊆ ... ⊆ Row(W<2) = Row(W1),
We can apply Proposition 15, which concludes the proof.	口
Now we provide a proof for Theorem 4, starting from a refinement of the last statement.
Proposition 16. Let r = min{di}, θ = (Wh,..., Wι), W = μd(θ), and e = rkW. The image of
the differential dμd at θ contains the tangent space TWMe of Me at W. Furthermore, for ^very
W ∈ Me∖Me-ι there exists θ0 such that μd(θ0) = W and the image of dμd(θ0) is exactly TWMe.
Proof. Due to ⑺ the image (5) of dμd(θ) always contains Rdh 0 Row (W) + Col(W) 0 Rd0 =
TWMe. Furthermore, there always exists (Wh,..., W1) ∈ μ-1(W) such that each Wi has rank
exactly r and the containments in (7) are all equalities. For example, one way to achieve this is
to consider any decomposition W = UVT where U ∈ Rdh×r and V = Rd0 ×r and then set
W1 = [V | 0]T, Wh = [U | 0], and Wi= I0r 00 for 2 ≤ i ≤ h — 1, where Ir is the (r × r)-identity
matrix and the zeros fill in the dimensions (di X di-ι) of Wi.	口
The next two propositions discuss the first part of Theorem 4, which distinguishes between the filling
and the non-filling case.
Proposition 17. Let r = min{di} and θ = (Wh, . . . , W1). In the non-filling case (i.e., if r <
min{dh, do}) we have that rk(dμd(θ)) < dimMr ifand only if rk(μd(θ)) < r.
Proof. If rk(μd(θ)) = r, then Proposition 16 implies that the image of the differential dμd(θ) is the
whole tangent space of Mr at μd(θ). To prove the other direction of the assertion, we assume that
rk(μd(θ)) < r. Since r < min{dh, do}, there is some i ∈ {1,...,h — 1} such that d = r. We
view μd as the following concatenation of the matrix multiplication maps:
Rdh×dh-1 X ... X Rd1 ×d0 —→ Rdh × di × Rdi×d0 —→ Rdh×d0 ,	(8)
where μi,ι = 〃(兀...4)X 〃(d”…,d0) and μi,2 = 〃(加心口。). Since rk(μd(θ)) < r, we have that
rk(W>i) < r or rk(W<i+1) < r. Without loss of generality, we may assume the latter. So applying
Lemma 3 to μi,2 and θ0 := μi,ι(θ) yields
rk(dμd(θ)) ≤ rk(dμi,2(θ0)) = rk(W<i+ι)(dh — rk(W>i)) + rk(W>i)do
< r (dh — rk(W>i)) + rk(W>i)do = rk(W>i) (do — r) + r dh
≤ r (do 一 r) + rdh = dim (Mr).	口
14
Published as a conference paper at ICLR 2020
Proposition 18. Let r = min{di} and θ = (Wh, . . . , W1). In the filling case (i.e., if r =
min{dh, d0}) we have that rk(dμd(θ)) < dhdo if and only if there is some i ∈ {1,...,h — 1}
with rk(W>i) < dh and rk(W<i+1) < d0.
Proof. Let us first assume that rk(W>i) < dh and rk(W<i+1) < d0 for some i ∈ {1, . . . , h — 1}.
We view μd as the concatenation of the matrix multiplication maps in (8). Applying Lemma 3 to
μi,2 and θ0 ：= μi,ι(θ) yields
rk(dμd(θ)) ≤ rk(dμi,2(θ0)) = rk(W<i+ι)(dh — rk(W>i)) + rk(W>i)d0
< d0 (dh — rk(W>i)) + rk(W>i)d0 = dhd0 .
Secondly, we assume the contrary, i.e., that every i ∈ {1, . . . , h — 1} satisfies rk(W>i) = dh or
rk(W<i+1) = d0. We observe the following 2 key properties which hold for all i ∈ {1, . . . , h — 1}:
rk(W>i) =dh	⇒	∀j	≥ i ：rk(W>j) =dh,	(9)
rk(W<i+1) =	d0	⇒	∀j	≤ i ： rk(W<j+1) = d0.
We consider the index set I := {i ∈ {1,...,h — 1} | rk(W<i+ι) = d0}. If I = 0, our assumption
implies that rk(W>i) = dh for every i ∈ {1, . . . , h}. So due to Lemma 3 we have
h	h-1
rk(dμd(θ)) = dh Xrk(W<i) — dh Xrk(W<i+ι) = dhrk(W<ι) = dhdo.
i=1	i=1
IfI 6= 0, we define k := max I. So for every i ∈ {k + 1, . . . , h — 1} we have rk(W<i+1) < d0,
and thus rk(W>i) = dh by our assumption. Moreover, due to (9), every j ∈ {0, . . . , k} satisfies
rk(W<j+1) = d0. Hence, Lemma 3 yields
kh
rk(dμd(θ)) = Xrk(W>j)do + dhdo + X dhrk(W<i)
j=1	i=k+2
k	h-1
—Erk(W>j)do — E dhrk(W<i+ι) = d%do.	□
j=1	i=k+1
Example 19. According to Proposition 18, the differential of the matrix multiplication map is sur-
jective whenever rk(W) = r, but also for certain θ when rk(W) < r. For example, let US con-
sider the map μ(2,2,2) ： R2×2 X R2×2 → R2×2 and the two factorizations θ = ([ 1 1 ], [ 1 0])
and θ0 = ([ 11 00] , [ 10 01 ]) of the rank-one matrix [ 11 11 ]. According to Proposition 18, the differen-
tial dμ(2,2,2)(θ) has maximal rank 4. So it is surjective, whereas dμ(2,2,2)(θ0) is not. In fact, by
Lemma 3, we have rk(dμ(2,2,2)(θ0)) = 3.
Theorem 4.	Let r = min{di}, θ = (Wh,..., Wι), and W = μd(θ).
•	(Filling case) If r = min{dh,d0}, the differential dμd(θ) has maximal rank equal to
dim Mr = dhd0 if and only if, for every i ∈ {1, 2, . . . , h — 1}, either rk(W>i) = dh
or rk(W<i+1) = d0 holds.
•	(Non-filling case) If r < min{dh, d0}, the differential dμd(θ) has maximal rank equal to
dim Mr = r(dh + do — r) ifand only if rk( W) = r.
Furthermore, in both situations, if rk(W) = e < r, then the image of dμd(θ) always contains the
tangent space TWMe of Me ⊂ Mr at W.
Proof. This is an amalgamation of Propositions 16, 17 and 18.
□
A.5 Proof of Theorem 5
In the following we use the notation from Theorem 5:
15
Published as a conference paper at ICLR 2020
Theorem 5.	Let r = min{di}. If rk(W) = r, then the set of d-factorizations μ-1(W) of W has
exactly 2b path-connected components, where b = #{i | d = r, 0 < i < h}. If rk(W) < r, then
μ-1(W) is always path-connected.
We also write GL+ (r) for the set of matrices in GL(r) with positive determinant. Analogously, we
set GL-(r) := {G ∈ GL(r) | det(G) < 0}.
We first prove Theorem 5 in the case that b = 0. To show that μ-1(W) is path-connected in this
case, we show the following stronger assertion: given two matrices W and W0 of arbitrary rank and
factorizations θ ∈ μ-1(W) and θ0 ∈ μ-1 (W0), each path in the codomain of μd from W to W0 can
be lifted to a path in the domain of μd from θ to θ0.
Proposition 20 (Path Lifting Property). If b = 0, then for every W, W0 ∈ Rdy ×dx, every θ ∈
μ-1(W), every θ0 ∈ μ-1(W0) and every continuous function f : [0,1] → Rdy ×dx with f (0) = W
and f(1) = W0, there is a continuous function F : [-1, 2] → Rdy ×dh-1 × . . . × Rd1×dx such that
F(-1) = θ, F(2) = θ0, μd(F(t)) = W for every t ∈ [-1,0], μd(F(t)) = W0 for every t ∈ [1, 2]
and μd(F(t)) = f (t) for every t ∈ [0,1].
Proof. Without loss of generality, we may assume that dy ≤ dx . Then the assumption b = 0 means
that di > dy for all i = 1, . . . , h - 1.
We prove the assertion by induction on h. For the induction beginning, we consider the cases h = 1
and h = 2. If h = 1, then μd is the identity and Proposition 20 is trivial. For h = 2, we construct
explicit lifts of the given paths. We first show that there is a path in μ-1(W) from θ = (W2, Wι) to
some (B2, B1) such that B2 has full rank.
Claim L Let h = 2, W ∈ Rdy ×dx and (W2, Wι) ∈ μ-1(W). Then there is (B2, Bi) ∈ μ-1(W)
with rk(B2) = dy and a continuous function g : [0,1] → μ-1(W) such that g(0) = (W2,W1) and
g(1) = (B2,B1).
Proof. If rk(W2) = dy, we have nothing to show. So we assume that s := rk(W2) < dy. Without
loss of generality, we may further assume that the first s rows ofW2 have rank s. As s < d1, we find
a matrix G ∈ GL+(d1) such that W2G = MIs 00, where Is ∈ Rs×s is the identity matrix and M ∈
R(dy-s)×s. Since GL+(d1) is path-connected, there is a continuous function g10 : [0, 1] → GL+(d1)
with g1(0) = Idi and gi(1) = G. Concatenation with GL(di) → μ-1(W), H → (W2H, H-1W1)
yields a continuous path gi in μ-1(W) from (W2,Wi) to (W2G, GTW1).
Since (W2G)(G-1W1) = W, we see that G-1W1 = h WN(s) i, where W(s) ∈ Rs×dx is the first
s rows of W and N ∈ R(dy -s)×dx. Replacing N by an arbitrary matrix N0 still yields that
W2G h W(0) i = W. Hence, we find a continuous path g2 in μ-1(W) from (W2G,G-1W1) to
(W2G,Bi := h W0(s) i).
Finally, we can replace the 0-columns in W2G by arbitrary matrices Mi ∈ Rs×(d1-s) and M2 ∈
R(dy-s)×(d1-s) such that MIs MM1 Bi = W still holds. In particular, we can pick Mi = 0 and
M2 = [Idy-s 0 ] such that B2 := MIs M0 has full rank dy, and we find a continuous path g3 in
μ-1(W) from (W2G, Bi) to (B2, Bi). Putting gi, g2 and g3 together yields a path g as desired in
Claim 1.	♦
As B2 has full rank, we find a matrix H ∈ GL+(di) such that B2H = [ Idy 0 ]. As in the
proof of Claim 1, we construct a continuous path in μ-i(W) from (B2,Bi) to (B2H, H-iBi).
Since H-iBi = [NN] for some N ∈ R(d1-dy)×dx, We also find a continuous path in μ-i(W)
from (B2H, H-iBi) to (B2H, [ W0 ]). All in all, we have constructed a continuous path Fi in
μ-i(W) from (W2, Wi) to ([Idy 0], [ W ]). Analogously, we find a continuous path 玛 in μ-i(W0)
between (W20, Wi0) and ([ Idy 0] , W00 ). Finally, we define F2 : [0, 1] → Rdy ×d1 × Rd1 ×dx,
t 7→ ([ Idy 0] , f(0t) ) such that putting Fi, F2 and F3 together yields a path F as desired in Propo-
sition 20.
16
Published as a conference paper at ICLR 2020
For the induction step, We view μd as the concatenation of the following two matrix multiplication
maps:
Rdy×dh-1 X	X Rdι×dχ μ(dh,…,dp ×id Rdy ×d∖ X Rdi ×dχ μ(dy-→dx) Rdy ×dχ
We consider θ = (Wh,..., Wι) and θ0 = (Wh,..., W0), as well as W>ι = Wh …W2 and W> 1 =
Wh …W0. Given a path f : [0,1] → Rdy ×dx from W = W>1W1 to W0 = W> 1 Wj, we apply the
induction beginning (h = 2) to μ(d di d)to get a path F2 : [-0.5,1.5] → Rdy ×d1 X Rd1 ×dx such
that F2(-0.5) = (W>1,W1), F2(l.5) = (W>ι,W1), μ^心心旧O W forall t ∈ [-0.5,0],
μ(dy ,d1,dχ)(F2(t)) = W 0 for all t ∈ [1,1.5] and 仙@ ,心化)(尸2(%))=f(t) forall t ∈ [0,1]. Now we
apply the induction hypothesis on μ(dh,…&)and the path from W>ι to W> 1 given by the first factor
of F2. This yields a path F1 : [-1, 2] → Rdy ×dh-i X . . . X Rd2×di with F1(-1) = (Wh, . . . W2),
Fι(2) = (Wh ,...W2), Mg,…4(F*)) = W>ι forall t ∈ [-1, -0.5],颔加…,d1)(F1(t)) = W> 1
for all t ∈ [1.5, 2] and Μ⑷,…,d1)(F1(t)) is the first factor of F2(t) for all t ∈ [-0.5,1.5]. ThiS
allows us to define a continuous path F : [-1, 2] → Rdy ×dh-i X . . . X Rdi ×dx from θ to θ0 by
setting F(t) = (F1(t), W1) for all t ∈ [-1, -0.5], F(t) = (F1(t), W1) for all t ∈ [1.5, 2] and for
all t ∈ [-1, -0.5] we let F(t) consist of F1(t) and the second factor of F2(t).	□
Corollary 21. If b = 0, then μ-1(W) is Path-Connectedforeach W ∈ Rdy ×dx.
Proof. Apply Proposition 20 to the constant function f : [0,1] → Rdy ×dx, t → W.	□
Now we study the case b > 0. We write 0 < i1 < . . . < ib < h for those indices ij such that
dj = r. Then we view μd as the concatenation of the following two matrix multiplication maps:
Rdy ×dh-i X ... X	Rdi×dχ	-→	Rdy ×dib	X Rdib	×dib-i	X ... X Rd” ×dχ	-→	Rdy ×dχ,	(10)
where μ1 = μ(dh,...,dib )χμ(dib,…,dib-1)*...*仙@1,…，do)and μ2 = μ(dy,%,…，叫3。).APPIying,e
path lifting property described above to the map μ1, we will show in Proposition 26 that μ-1(W)
and μ-1(W) have the same number of (path-)connected components. So it remains to study the
connected components of μ-1(W). We can shortly write the map μ2 as
μ2 : Rdy ×r X (Rr×r)b-1 X Rr×dχ —› Rdy ×dχ.
In the case that rk(W) = r, we use the following natural action of GL(r)b on μ-1(W):
GL(r)b X μ-1(W) -→ μ-1(W),
((Gb, . . . , GI), (Ab+1, . . . , AI)) 1-→ (Ab+1Gb, GbIAbGb-1, . . . , G1 1A1).
In fact, we show now that μ-1(W) is the orbit of any element under this action if rk(W) = r. From
this we will deduce in Corollaries 23 and 24 that μ-1(W) is homeomorphic to GL(r)b and thus has
2b (path-)connected components if the matrix W has maximal rank r.
Proposition 22. Let b > 0 and θ = (Ab+1,.. .,A1) ∈ Rdy ×r X (Rr×r)b-1 X Rr×dx such that
W = μ2(θ) has maximal rank r. Then μ-1(W) is the orbit of θ under the action defined in (11),
i.e.,
μ-1(W) = {(Ab+1Gb,G-1AbGb-1,...,G[1 A1) | G1,...,Gb ∈ GL(r)}.
Proof. One inclusion, namely "⊇”, is trivial. We prove the other inclusion "⊆" by induction on b.
For the induction beginning (b = 1), we write W = [ W11 W12 ] where W11 ∈ Rr×r. Without loss of
generality, we may assume that rk(W11) = r. Similarly, we write A2 = AA2i and A1 = [Aii Ai2]
where A〃 ∈ Rr×r for i = 1,2. For (A2,A；) ∈ μ-1(W), we write analogously A2 = [A21
and A01 = [ A0ii A0i2]. Due to rk(W11) = r, we have that rk(A21) = r = rk(A021). Hence, there
is a matrix G ∈ GL(r) such that A021 = A21G. This implies that A21GA011 = W11 = A21A11,
so A011 = G-1A11. Due to A21A12 = W12 = A21GA012, we get that A012 = G-1A12. Finally,
17
Published as a conference paper at ICLR 2020
rk(W11) = r implies that rk(A11) = r, so A22A11 = W21 = A022G-1A11 shows A022 = A22G.
Thus we have shown that A02 = A2G and A01 = G-1A1.
For the induction step (b > 1), We consider (Ab+ι,... ,A1) ∈ μ-1(W) and A>ι =
Ab+ι •…A2,A>1 = Ab+1 •…A2 ∈ Rdy×r. Now we can apply the induction beginning to
find G1 ∈ GL(r) such that A0>1 = A>1G1 and A01 = G1-1A1. As A0>1 has rank r and
Ab+1 ∙∙∙ A2 = A>ι = Ab+1 •…(A2G1), we can apply the induction hypothesis on the map
which multiplies the left-most b matrices. This yields Gb, . . . G2 ∈ GL(r) such that A0b+1 =
Ab+1Gb, ..., A3 = G- 1A3G2, A2 = G2 I(A2GI).	口
Corollary 23. If b > 0 and W ∈ Rdy ×dx has maximal rank r, then μ-1(W) is homeomorphic to
GL(r)b.
Proof. Wefix θ = (Ab+1,..., Ai) ∈ μ-1(W). The map 夕：GL(r)b → μ-1(W) given by
the action (11) on θ is continuous. We now construct its inverse. As rk(W) = r, we have
that rk(Ai) = r for all i = 1, . . . , b + 1. Without loss of generality, we may assume that the
first r rows of Ab+1 have rank r. We write π : Rdy ×r → Rr×r for the projection which for-
gets the last dy - r rows of a given matrix. For θ0 = (Ab+i, ...,A；) ∈ μ-1(W), Proposi-
tion 22 shows that θ0 = φ(Gb,..., Gi) for some (Gb,..., Gi) ∈ GL(r)b. So we have that
Gb = π(Ab+i)-iπ(A0b+i), Gb-i = Ab-iGbA0b, . . . , Gi = A2-iG2A02, which defines a map
ψ ： μ-1(W) -→ GL(r)b,
(Ab+i,.. .,A!) --~> (G(Ab+ι), A-1G(Ab+i)Ab, ..., A-I …A-IG(Ab+i)Ab …a2),
where G(Ab+；) := π(Ab+1)-1π(Ab+1). By construction, ψ is the inverse of 夕.Since ψ is contin-
uous, it is a homeomorphism between μ-i (W) and GL(r)b.	□
Corollary 24. If b > 0 and W ∈ Rdy ×dx has maximal rank r, then μ-i (W) has 2b connected
components. Each of these components is path-connected.
Proof. The group GL(r) has two connected components, namely GL+ (r) and GL- (r). Both com-
ponents are path-connected. Hence, GL(r)b has 2b connected components, each of them path-
connected. By Corollary 23, the same holds for μ-i(W).	□
To complete our understanding of the connected components of μ-i (W), we consider the case that
the matrix W ∈ Rdy×dx does not have maximal rank r. Inthatcase, it turns out that μ-1(W) is path-
connected, which we show by constructing explicit paths between any two elements of μ-i (W).
Proposition 25. Let W ∈ Rdy ×dx .If b > 0 and rk(W) < r, then μ-i (W) is path-connected.
Proof. We write W = ∣^ W1 ^∣ where W； ∈ Rr×dx, and denote by e the rank of W. If rk( W；) = e,
LW 2
then W2 = MW； for some M ∈ R(dy-r)×r.
Claim 2. If b = 1, (A2, A；) ∈ μ-1(W), rk(W；) = e and W2 = MW；, then there is a continuous
function f : [0,1] → μ-1(W) with f(0) = (A2,A1) andf(1) = ([IM] ,W 1).
Proof. Since rk(W) < r, we have that rk(A2) < r or rk(Ai) < r. If rk(A2) < r, we proceed as
in the proof of Claim 1 to find a path in μ-1(W) from (A2, A；) to (A2, A；) such that rk(A2) = r.
Hence, we may assume that rk(A2) = r. This implies that rk(Ai) = e. So K := ker(AiT ) ⊆ Rr
has positive dimension r - e. We write A2 = AA21 where A2； ∈ Rr×r , and denote by r2 the rank
of A2；. So the rowspace R ⊆ Rr of A2； has dimension r2. We now show that K + R = Rr. To
see this we set δ := dim(K ∩ R). Without loss of generality, we may assume that the first e rows of
W1 are linearly independent. Then the first e rows of A21 must also be linearly independent, so we
might further assume that the first r rows of A21 are linearly independent. We denote by A211 and
W11 the matrices formed by the first r rows of A21 and W1, respectively. In particular, we have
18
Published as a conference paper at ICLR 2020
that A211A1 = W11. Now We Chooseabasis (b1,...,br2) for R such that (bι,...,bδ) is a basis for
K ∩ R. Since R is the rowspace ofA211, there is some G ∈ GL(r2) such that the i-th row of GA211
is bi. So the first δ rows of GW11 = GA211A1 are zero, which shows that e = rk(GW 11) ≤ r - δ.
Thus, dim(K + R) = (r - e) + r2 - δ ≥ r, which proves that K + R = Rr.
If r2 < r, we now show that there is a path in μ-1(W) from (A2 ,A1) to (A2' = [A2,1] ,A1)
such that rk(A0201) = r. We may assume again that the first r2 rows a1, . . . , ar2 of A21 are linearly
independent, i.e., that they form a basis for R. Since K+R = Rr, we can extend this basis to a basis
(a1, . . . , ar) for Rr such that ai ∈ K for all i > r2. We define A0201 such that its first r2 rows are
a1, . . . , ar2 and such that its i-th row, for r2 < i ≤ r, is the sum of ai ∈ K and the i-th row ofA21.
Then A2 = [ AJ1 ] satisfies A2A1 = W. Moreover, the straight line from (A2, A1) to (A2, A1) is a
path in μ-1 (W). Since the last r - r rows of A21 are contained in the linear span R of the first τ2
rows of A21, the linearity of the determinant implies that det(A,J = det([ aι …a") = 0.
Thus, we may assume that τ = τ. If det(A21) < 0, we now construct a path in μ-1(W) from
(A2, A1) to (A0200 = AA02010 , A1) such that det(A02010 ) > 0. For this, we pick a vector v ∈ K \ {0}.
Since the rows of A21 form a basis for Rr, there is an index i ∈ {1, . . . , τ} such that the matrix
D ∈ Rr×r obtained from A21 by replacing its i-th row with V has full rank. We pick μ ∈ R such
that det(A21) + μ det(D) > 0 and define A21 to be the matrix obtained from A21 by adding μv to
its i-th row. Then det(A201) = det(A21) + μ det(D) > 0 and A2! = [ A，'。satisfies A20A1 = W.
Moreover, the straight line from (A2, A1) to (A200, A1) is a path in μ-1(W).
Therefore, we may assume that det(A21) > 0, so G := A2-11 ∈ GL+ (τ). Any path in GL+ (τ)
from Ir to G yields a path in μ-1(W) from (A2,A1) to (A2G,G-1A1) = ([ AIrG ] , W1). Since
(A22G)W 1 = W2 = MW1, the straight line from ([ArG] , W1) to ([Ir] , W1) is a path in
μ-1(W).	22	♦
Claim 3. If θ = (Ab+1,...,A1) ∈ μ-1(W), rk(W 1) = e and W2 = MW1, then there is a
continuous function F : [0,1] → μ-1 (W) with F(0) = θ and F(1) = ([M] ,Ir,...,Ir, W1).
Proof. As e < τ, at least one of the Ai has rank smaller than τ. We set k := min{i ∈ {1, . . . , b+1} |
rk(Ai) < τ}. If k < b + 1, we first show that there is a path in μ-1(W) from θ to (Ab+1,...,A1)
such that min{i ∈ {1, . . . , b + 1} | rk(A0i) < τ} = b + 1. Since rk(Ak) < τ, the rank of
WZ := Ak+1Ak is smaller than τ. We write WZ = [ W1 W2 ] where W； has T columns. Without
loss of generality, we may assume that rk(W；) = rk(W0). Then there is a matrix N such that
W2 = W；N. Hence, we can apply the transposed version of Claim 2, which yields a path from
(Ak+1,Ak) to (W 1, [ Ir N ]) inthe set of factorizations of W 0. Defining Ak+1 := W ；, Ak := [ Ir N ]
1	1	1
and Ai := Ai for all i ∈ {1,...,b + 1} \ {k, k + 1}, extends this path to a path in μ- (W)
from θ to (Ab+1, . . . , A1) such that min{i ∈ {1, . . . , b + 1} | rk(Ai) < τ} = k + 1. We note
that this construction increased the number k. So we can repeat the construction until we reach
(AZb+1 , . . . , AZ1 ) as desired.
Hence, we may assume that k
smaller than τ. We write W”
b + 1. Since rk(Ab+1) < τ, the rank of W" := Ab+1Ab is
hWI,i where WIZhaS T rows. Since W1 = WIZAb-I …A1
0 z	z	JZ
and the matrices Ab-1, . . . , A1 have rank τ, we have that rk(W 1 ) = rk(W1) = e. Analogously,
rk(WZZ) = e. So there is matrix MZ such that W； = MzW；. Applying Claim 2 yields a path
from (Ab+1,Ab) to ([ r, ] , W10) in the set of factorizations of W 00. This path can be extended to a
path in μ-1(W) from θ to θz := ([MP] , W10, Ab-1,..., A1). Applying the same construction on
W 000 := W 10Ab_1 yields a path in μ-1(W) from θz to θ00 := ([ r, ] ,Ir, W 10Ab-1,...,A1). We
repeat the contruction until ([ MO ] , Ir,. ..,Ir, W1) is reached. Since M0W1 = W2 = MW1, the
straight line from ([ r, ] ,Ir ,...,Ir, W1) to ([ M ] ,Ir ,...,Ir, W1) is apath in μ-1(W).	♦
19
Published as a conference paper at ICLR 2020
Now We finally show that μ-1(W) is path-connected. Without loss of generality, We may assume
that rk(W 1) = e, and we write W2 = MW1. For θ, θ0 ∈ μ-1(W), there are paths in μ-1(W)
from θ resp. θ0 to ([Mr] , Ir ,...,Ir, W ι),so there is a path from θ to θ0 in μ-1(W).	口
To settle the proof of Theorem 5, it is left to show that μ-1 (W) and μ-1(W) have indeed the same
number of (path-)connected components, as we promised earlier.
Proposition 26. Let b > 0 and W ∈ Rdy×dx, Then μ-1(W) and μ-1(W) have the same number
of connected components. Moreover, each of these components is path-connected.
Proof. Let US denote the connected components of μ-1(W) be Ci,..., Ck. By Corollary 24 and
Proposition 25, each of these components is path-connected. Using the notation in (10), we have
that μ-1(W) = Uk=I μ-1(Ci). Since the μ-1(C1),..., μ-1(Ck) are pairwise disconnected, we
see that μ-1(W) has at least k disconnected components. It is left to show that each μ-1(Ci) is
path-connected. For this, letθ,θ0 ∈ μ-1(Ci) and σ := μι(θ),σ0 := μι(θ0) ∈ Ci. As CiiS
path-connected, there is a path in Ci from σ to σ0. The map μι is a direct product of b + 1 matrix
multiplication maps. To each factor we can apply Proposition 20, which yields a path in μ-1(Ci)
from θ to θ0.	□
Corollary 27. Let b > 0 and W ∈ Rdy ×dx. If rk(W) = r, then μ-1(W) has 2b connected
components, and each of these components is path-connected. If rk(W) < r, then μ-1(W) is
path-connnected.
Proof. Combine Corollary 24 and Propositions 25 and 26.	口
Proofof Theorem 5. Theorem 5 is an amalgamation of Corollaries 21 and 27.	口
A.6 Proofs of Propositions 6, 7, 9, 10 and 14
Proposition 6. If θ is such that dμd(θ) has maximal rank (see Theorem 4), then θ ∈ Crit(L) if
and only if W ∈ Crit('∣Mr), and θ is a minimum (resp., saddle, maximum) for L if and only if W
is a minimum (resp., saddle, maximum) for '∣Mr. If rk(W) = r (which implies that dμd(θ) has
maximal rank) and θ ∈ Crit(L), then all d-factorizations of W also belong to Crit(L).
Proof. If μd is a local submersion at θ onto Mr, then there exists an open neighborhood U of W
in Mr and an open neighborhood V of θ with the property that μd acts as a projection from V
onto U (see, e.g., (Lee, 2003, Theorem 7.3)). From this, we easily deduce that θ is a minimum (resp.
saddle, maximum) for L if and only if W = μd(θ) isaminimum (resp. saddle, maximum) for '∣Mr.
Finally, if rk(W) = r, then dμd(θ) has maximal rank for all θ ∈ μ-1(W) by Theorem 4. 口
Proposition 7. If θ ∈ Crit(L) with rk(W) = e ≤ r, then W ∈ Crit('∣Me). In other words,
if rk(W) < r, then θ ∈ Crit(L) implies that W is a CritiCal point for the restriction of ' to a
smaller determinantal variety Me (which is in the singular locus of the functional space Mr in the
non-filling case).
Proof. According to Theorem 4, if μd(θ) = W with rk(W) = e, then Im(dμd(θ)) ⊃ TWMe.
This means that θ ∈ Crit(L) implies that W is critical for '∣Me.	口
Proposition 9. Let θ ∈ Crit(L) be such that rk(W) < r, and assume that d'(W) = 0. Then,
for any neighborhood U of θ, there exists θ0 in U such that μd(θ0) = W but θ0 ∈ Crit(L). In
particular, θ is a saddle point.
Proof. Our proof is a modification of an argument used in Zhang (2019). Let us first consider the
case that μd is filling, so r = min{ dh, d0}. Without loss of generality, we assume r = d0. Recall
that the image of dμd(θ) is given by
Rdh 0 Row(W<h) + ... + Col(W>i) 0 Row(W<i) + ... + Col(W>ι) 0 Rd0.
20
Published as a conference paper at ICLR 2020
We first note that Row(W<h) = Rd0, for otherwise dμd(θ) would be surjective, implying that
d'(W) = 0. We define i = max{j | Row(W<j) = Rd0}, so 1 ≤ i < h (writing W<ι = Id。). We
have that _	_
d'(W)(Col(W>i) 0 Row(W<i)) = d'(W)(Col(W>i) 0 Rd0) = 0.	(12)
Since Row(W<i+1) ( Rd0, we may find wi ∈ Rdi , wi 6= 0 such that wiT Wi . . . W1 = 0. We now
fix e > 0 and vi+ι ∈ Rdi+1 arbitrarily, and define Wi+ι = Wi+ι + e(vi+ι 0 wi). Clearly, We have
that Wi+1Wi . . . W1 = Wi+1Wi . . . W1. If (Wh, . . . , Wi+1, Wi, . . . , W1) is also a critical point
of L, then
d'(W )(Col(Wh,...,lWii+1) 0 Rd。)= 0.	(13)
Combining (12) and (13), we have that
d'(W )(Col(Wh,..., Wi+2(vi+1 0 Wi)) 0 Rd0) = 0.
If this were true for all vi+1 , then it would imply
d'(W )(C0l(Wh,...,Wi+2) 0 Rd0 ) = 0.	(14)
Hence, we have either found an arbitrarily small perturbation θ0 of θ as required in Proposition 9,
or (14) must hold. In the latter case, We reapply the same argument for Wi+2 =用+2 + c(vi+2 0
wi+1) where wi+1 6= 0 and wiT+1Wi+1 . . . W1 = 0. Again, we can either construct an arbitrarily
small perturbation θ0 of θ as required in Proposition 9, or We have d'(W)(C0l(W>i+2) 0 Rd0)=
0. Proceeding this way we eventually arrive at d'(W)(Rdh 0 Rd0) = 0 so d'(W) = 0, which
contradicts the hypothesis. Thus, at some point we must find an arbitrarily small perturbation θ0 of θ
as required in Proposition 9, which concludes the proof in the case that μd is filling.
We now consider the case that μd is not filling. We pick i ∈ {1,..., h 一 1} such that di = r, and
write for simplicity A = Wh ... Wi+1 and B = Wi... Wι. The assumption rk(W) < r implies
that rk(A) < r or rk(B) < r, and we assume without loss of generality that rk(A) < r. We define
the map LB (Wh,..., Wi+1) = '(Whh... Wi+1B). We also introduce the map 'b (A0) = '(A0B)
and the matrix multiplication map 仙d人 where da = (dh,..., di+ι), so that LB = 'b ◦仙&人.If θ
is a critical point for L, then θA = (Wh, . . . , Wi+1) must be a critical point for LB. We are thus
in the position to apply the analysis carried out in the filling case. In particular, we have that either
Θa can be perturbed to Θa such that 乩色人(Θa)=乩&人(Θa) but Θa ∈ Crit(LB), or d'B (A) = 0. In
the former case, we have that θ0 = (θA, θB) is not a critical point for L, and we are done. If instead
d'B (A) = 0, then we have that
d'(W)(Rdh 0 Row(B)) =0,
because the image of the differential of the map A0 7→ A0B is given by Rdh 0 Row(B). We
now proceed in a similar manner as before. We have that Col(W>i) ( Rdh, because we assumed
that W>i = A had rank less than r ≤ dh . Thus, we may find wi+1 ∈ Rdi , wi+1 6= 0 such that
Wh . . . Wi+1wi+1 = 0. We fix > 0 and vi arbitrarily, and define Wi = Wi+(wi+10vi). We have
that Wh ... Wi+ι Wi = Wh ... Wi+ι Wi. Ifforallchoices of Vi we have that (Wh,..., Wi,..., Wι)
is still a critical point for L, then we can deduce that
d'(W)(Rdh 0 Row(Wi-ι ... Wι)) = 0.
Repeating this reasoning, we obtain our result as before.
□
Proposition 10. Let' be any smooth convexfunction, and let L = ' ◦ μd. If θ is a non-global local
minimum for L, then necessarily rk(W) = r (so θ is a pure critical point). In particular, L has
non-global minima if and only if '|Mr has non-global minima.
Proof. The first statement follows immediately from Proposition 9: if θ ∈ Crit(L) is a non-global
local minimum, then necessarily d'(W) = 0, and we conclude that rk(W) = r. For the second
statement, we observe that if ' is a convex function, then θ is a local minimum for L if and only if
W = μd(θ) is a local minimum for '∣Mr. Indeed, if W = μd(θ) is a local minimum for '∣Mr, then
it is always true that any θ ∈ μ-1(W) is a local minimum. Conversely, if θ is a local minimum,
21
Published as a conference paper at ICLR 2020
then from Proposition 9 We see that either d'(W) = 0, in which case W is a (global) minimum
because ' is convex, or W must have maximal rank. In the latter case, dμd(θ) would be surjective
(by Theorem 4), so W would also be a local minimum for '∣Mr (see Proposition 6). Finally, it is
clear that θ is also a global minimum for L if and only if W is a global minimum for 'Mr. 口
Proposition 14. If L0 = ` ◦ Vd and L = ' ◦ μd, then the critical locus Crit(Lr) is in “correspon-
dence” with Crit(L) ∩ Ω, meaning that
{νd(θ0) | θ0 ∈ Crit(U)} = {μd(θ) | θ ∈ Crit(L) ∩ Ω}.
Proof. Let us define
P: ω → Rdθ, (Wh,...,WI) → (Wh, ∣Wh-ik'….kW⅛),
q : Ω → Rdθ, (Wh,..., Wι) → (WhkWh-ιk... kWιk, kWh-1k,...,高).
The image of both of these maps is N = {(Wh, . . . , W1) | kWik = 1, i = 1, . . . , h - 1}. In
fact, both maps are submersions onto N. Since Vd = μd ◦ P and μd ◦ q = μd∣Ω = Vd ◦ q, it is
enough to show the following two assertions: 1) θ0∈ Crit(L0) if and only if p(θ0) ∈ Crit(L); and
2) θ ∈ Crit(L) ∩ Ω if and only if q(θ) ∈ Crit(Li0).
For 1), we deduce from L0 = L ◦ P that dL0(θ 0) = dL(P(θ0)) ◦ dP(θ0) = 0 if dL(P(θ0)) = 0,
but this also holds conversely: if dL0(θ0) = 0, then Im(dP(θ0)) is contained in Ker(dL(P(θ 0))).
Since q ◦ P = P and both maps P and q are submersions, we have that Im(dP(θ 0)) = Tp(θ0)N =
Im(dq(p(θ0))). Now it follows from L ◦ q = L∣ω that dL(p(θ0)) = dL(p(θ0)) ◦ dq(p(θ0)) = 0. For
2), we can argue analogously, exchanging the roles of L and L0 as well as P and q.	口
A.7 Proof of Theorem 12
We consider a fixed matrix Q0 ∈ Rdy×dx and a singular value decomposition (SVD) Q0 = UΣV T.
Here U ∈ Rdy ×dy and V ∈ Rdx ×dx are orthogonal and Σ ∈ Rdy ×dx is diagonal with decreasing di-
agonal entries σ1, σ2, . . . , σm where m = min(dx, dy). We also write shortly [m] = {1, 2, . . . , m}
and denote by [m]r the set of all subsets of [m] of cardinality r. For I ∈ [m]r, we define ΣI ∈
Rdy×dx to be the diagonal matrix with entries σI,1, σI,2, . . . , σI,m where σI,i = σi if i ∈ I and
σI,i = 0 otherwise. These matrices yield the critical points of the function hQ0(P) = kP - Q0k2
restricted to the determinantal variety Mr .
Theorem 28. If Q0 ∈/ Mr, the critical points of hQ0 |Mr are all matrices of the form UΣIV T
where Q0 = UΣVT is a SVD andI ∈ [m]r. The local minima are the critical points with I = [r].
They are all global minima.
Proof. A matrix P ∈ Mr is a critical point if and only if (Qo - P) ∈ TPM⊥ = Col(P)⊥ 0
Row (P )⊥. If P = Pr=ι σ0 (Ui 0 v；) and Qo - P = pe=ι σj(uj 0 vj0) are SVD decompositions
with σi0 6= 0 and σj00 6= 0, the column spaces of P and Q0 - P are spanned by the u0i and u0j0,
respectively. Similarly, the row spaces ofP and Q0 - P are spanned by the vi0 and vj00, respectively.
So P is a critical point if and only if the vectors u0i, u0j0 and vi0, vj00 are orthonormal, i.e., if
re
Q0 = P + (Q0 - P) = Xσi0(u0i 0vi0) + X σj00(u0j0 0 vj00)
i=1	j=1
is a SVD of Q0. This proves that the critical points are of the form UΣIV T where Q0 = UΣV T is
a SVD and I ∈ [m]r .
Since Aq°(U∑iVT) = ∣∣U∑[m]∖ιVT∣∣2 = k∑[m]∖ιk2 = Pi∈ισ2, we see that the global minima
are exactly the critical points selecting r of the largest singular values of Q0, i.e., with I = [r]. It is
left to show that there are no other local minima. For this, we consider a critical point P = UΣIVT
such that at least one selected singular value σi for i ∈ I is strictly smaller than σr . We will show
now that P cannot be a local minimum. Since σi < σr , there is some j ∈ [r] such that j ∈/ I. As
22
Published as a conference paper at ICLR 2020
above, We write Uk and Vk for the columns of U and VT such that Qo = Pm=I σk(uk Xvk) and P =
Pk∈ισk(uk 0 vk). We consider rotations in the planes spanned by Ui, Uj and vi, vj, respectively:
for a ∈ [0, ∏2], we set u(α) = cos(α)ui + sin(α)uj and v(α) = cos(α)vi + sin(α)vj. Note that
U(O) = Ui and U 2) = Uj; analogously for v(α). Next we define σ(α) =cos2(α)σi + sin2(α)σj- and
Pα = X σk(Uk0vk) + σ(α) U(α) 0 v(α) ∈ Mr.
k∈I∖{i}
We note that Po = P and P∏ = U∑ι∖{i}∪{j-}VT are both critical points of Aq0 ∣Mr.
It remains to show that Aq° (Pa) as a function in α is strictly decreasing on the interval [0, ∏2]. From
hQ0 (Pα) = k X σk(Uk 0 vk) + σi(Ui 0 vi) - σ(α) (U(α) 0 v(α))k2
k∈/ I
and U(α) 0 v(α) = cos2 (α)(Ui 0 vi) + cos(α) sin(α)(Ui 0 vj + Uj 0 vi) + sin2 (α)(Uj 0 vj), we
deduce that
hQ0(Pα) =	X	σk2	+	σi	-	σ(α) cos2 (α)	+ 2	σ(α) cos(α)	sin(α)	+	σj	- σ(α)	sin2 (α)
k∈/ I ,k6=j
=	σk2 + σi2 + 2σj (σj - σi) cos2 (α) - (σj - σi)2 cos4(α).
k∈/ I,k6=j
The graph of the function f(x) = σi2 + 2σj (σj -σi)x- (σj -σi)2x2, for x ∈ R, is a parabola with a
unique local and global maximum at xo =不 j". Since xo ≥ 1, the function f is strictly increasing
on the interval [0,1]. Hence, Aq° (Pa) = Pk/i k=j∙ σk + f (cos2(α)) is strictly decreasing on [0, π2],
which concludes the proof.	□
If the singular values of Qo are pairwise distinct and positive, the singular vectors of Qo are unique
up to sign. So for each index set I ∈ [m]r the matrix QI = UΣIVT is the unique critical point of
hQ0 |Mr whose singular values are the σi for i ∈ I. Hence, Theorem 28 implies immediately the
following:
Corollary 29. If the singular values of Qo are pairwise distinct and positive, hQ0 |Mr has exactly
mr critical points, namely the QI = UΣIVT forI ∈ [m]r. Moreover, its unique local and global
minimum is Q[r].
We can strengthen this result by explicitly calculating the index of each critical point, i.e., the number
of negative eigenvalues of the Hessian matrix.
Theorem 30. If the singular values of Qo are pairwise distinct and positive, the index of QI as a
critical point of hQ0 |Mr is
index(QI) = #{(j, i) ∈ I × ([m] \I) |j > i}.
To prove this assertion, we may assume without loss of generality that dy ≤ dx, so m = dy. We may
further assume that Qo is a diagonal matrix, so Qo = Σ. Let 以凡,r,dx) : Rdy ×r X Rr×dx → Rdy ×dx
be the matrix multiplication map, and L = h∑ ◦ μ(dy,r,dx). For (A, B) ∈ μ-1 r d)(∑i ), Theorem4
implies that the condition ∑i ∈ Crit(h∑∣Mr) is equivalent to dL(A,B) = 0. Moreover, the
number of negative eigenvalues of the Hessian of L at any such factorization (A, B) of ΣI is the
same. This number is the index of ΣI. So we can compute it by fixing one specific factorization
(A, B) of ΣI.
To compute the Hessian of L at (A, B), we compute the partial derivatives of first and second order
of L:
∂L =2 [(AB - *)BT]ij,	∂j = 2 [AT(AB - ^ij,
23
Published as a conference paper at ICLR 2020
∂2L	_	0	, if i 6= k
∂aij∂aki	2[BBT]jl	, if i = k ,
∂2L	_	0	, if j 6= l
∂bij ∂bki	2[AT A]ik	, if j = l ,
∂2L	_	2aik bjl	, if j 6= k
∂aij ∂bki	2 (aik bjl +	[AB - Σ]il) , ifj = k
(15)
(16)
(17)
To assemble these second order partial derivatives into a matrix, we choose the following order of
the entries of (A, B):
a11 , . . . , a1r, a21 , . . . , a2r, . . . , ady 1 , . . . , adyr , b11 , . . . , br1 , b12, . . . , br2, . . . , b1dx , . . . , brdx .
We denote by H the Hessian matrix of L with respect to this ordering at the following specifically
chosen matrices (A0, B0): denoting by i1, i2, . . . , ir the entries of I in decreasing order, we pick
the j-th column of A0 to be the ij -th standard basis vector in Rdy and the j-th row of B0 to be the
σij -multiple of the ij -th standard basis vector in Rdx . Note that A0B0 = ΣI, A0T A0 = Ir is the
r × r-identity matrix, and B0B0T is the r × r-diagonal matrix with entries σi2 , σi2 , . . . , σi2 . We write
DM
MT N
H
where D ∈ Rrdy×rdy, N ∈ Rrdx×rdx, M ∈ Rrdy×rdx
Our first observation is that N, whose entries are described by (16), is twice the identity matrix,
so N = 2Irdx . Similarly, we see from (15) that D is a diagonal matrix. According to our fixed
ordering, the entries of D are indexed by pairs (ij, kl) of integers i, k ∈ [dy] and j, l ∈ [r]. With
this, the diagonal entries of D are Dij,ij = 2σi2 . Analogously, the entries of M are indexed by pairs
(ij, kl) of integers i ∈ [dy], j, k ∈ [r] and l ∈ [dx].
Lemma 31. Let i ∈ [dy] and j ∈ [r]. The ij-th row of M has exactly one non-zero entry. If i ∈ I,
there is some k ∈ [r] with i = ik and the non-zero entry is Mij,kij = 2σij. Otherwise, so if i ∈/ I,
the non-zero entry is Mij,ji = -2σi.
Proof. The entries of M are given by (17). We first observe that (A0)ik(B0)jl is non-zero if and
only if i = ik and l = ij. Moreover, we have that (A0)ikk (B0)jij = σij . Similarly, [A0B0 - Σ]il
is non-zero if and only i = l ∈/ I. For i ∈/ I we have that [A0B0 - Σ]ii = -σi.
Now we fix i and j and consider the ij-th row of M . We apply our observations above to the
following three cases.
If i = ij, then Mij,kl is non-zero if and only if k = j and l = i. In that case, Mij,ji = 2σi.
If i ∈ I, but i 6= ij , then there is some n 6= j such that i = in . Now Mij,kl is non-zero if and only
if k = n and l = ij. In that case, Mij,kij = 2σij .
Finally, if i ∈/ I, then Mij,kl is non-zero if and only if k = j and l = i. In that case, we have that
Mijji = -2σi.	口
Corollary 32. The square matrix ∆ := MMT ∈ Rrdy ×rdy is a diagonal matrix. For i ∈ [dy] and
j ∈ [r], its ij-th diagonal entry is ∆ij,ij = 4σi2 if i ∈ I and ∆ij,ij = 4σi2 if i ∈/ I.
Proof. The computation of the diagonal entries follows directly from Lemma 31. To see that all
other entries of ∆ are zero, we need to show that no column of M has more than one non-zero entry.
So let us assume for contradiction that the kl-th column of M has non-zero entries in the ij-th row
and in the lj-th row for (i,j) = (1, g).
If i, 1 ∈ I, then Lemma 31 implies i = ik = 1 and j = l = i1, which contradicts (i,j) = (1,习.
If i, 1 ∈ I, we see from Lemma 31 that j = k = g and i = l = 1, which contradicts (i,j) = (1,办
Finally, if i ∈ I and 1 ∈ I, then Lemma 31 yields that 1 = l = j ∈ I; a contradiction.	□
Corollary 33. The characteristic polynomial of H is
(t- 2)r|dx-dy1	∙tr2	∙ γ(t- 2(σ2 + 1))r∙ Y	Y(t2- 2t(σj	+ 1)+4(σ2	-	σf)) .	(18)
k∈I	i∈[m]∖I j∈I
24
Published as a conference paper at ICLR 2020
Proof. Using Schur complements, we can compute the characteristic polynomial of H as follows:
XH (t) = det (tIr(dχ + dy)- H
=det (tIrdx - 2Irdx ) det ((tIrdy - D)- M(tIrdx - 2Irdx ) IMT)
=(t - 2)rdx det ((tIrdy - D) - (t - 2)-1∆)
=(t - 2)r(dx-dy) det ((t - 2)(tIrdy - D)- ∆).
By Corollary 32, the matrix (t - 2)(tIrdy - D) - ∆ is a diagonal matrix whose ij-th diagonal entry
is (t - 2)(t - Dij,ij) - ∆ij,ij. We write shortly δij := ∆ij,ij and use the identity Dij,ij = 2σi2 to
further derive
dy r
χH(t)= (t - 2)r(dx-dy) Y Y (t-2)(t-2σi2j) -δij
i=1 j=1
dy r
= (t - 2)r(dx-dy) Y Y t2 - 2t(σi2j + 1) + (4σi2j -δij)
i=1 j=1
= (t - 2)r(dx-dy)	YYr	t t - 2(σi2j + 1)
i∈I j=1
• ( Y Y (t2 - 2t(σj+1)+4(σj-σ2))].
∖i∈[dy]∖I j = 1	)
The latter equality was derived by substituting specific values into the δij according to Corollary 32.
Rearranging the terms of this last expression of XH (t) yields (18).	口
Lemma 34. Let x, y > 0. The polynomial f(t) = t2 - 2t(x + 1) + 4(x - y) has two real roots and
at least one of them is positive. Moreover, f(t) has a negative root if and only if x < y.
Proof. The roots of f (t) are X +1 ±，(X + 1)2 — 4(x — y) = X +1 ±，(X — 1)2 + 4y. So the
discriminant is positive and f(t) has two real roots. Clearly, one of these is positive. The other one
is negative if and only if X + 1 <，(x — 1)2 + 4y, which is equivalent to (x + 1)2 < (x - 1)2 + 4y
and thus to x ‹y.	口
Proof of Theorem 30. It is left to count the number of negative roots of the univariate polyno-
mial (18). All the linear factors of (18) have non-negative roots. The ij-th quadratic factor of (18),
for i ∈ [dy] \I and j ∈ I, has at most one negative root due to Lemma 34. Moreover, it has exactly
one negative root if and only if σj2 < σi2 , which is equivalent to j > i. Hence, the polynomial (18)
has exactly #{(j, i) ∈ I × [dy] \I | j > i} many negative roots.	口
Theorem 12. If the singular values ofQ0 are pairwise distinct and positive, hQ0 |Mr has exactly
mr critical points, namely the matrices QI = UΣIVT with #(I) = r. Moreover, its unique local
and global minimum is Q{1,...,r}. More precisely, the index ofQI as a critical point of hQ0 |Mr (i.e.,
the number of negative eigenvalues of the Hessian matrix for any local parameterization) is
index(QI) = #{(j, i) ∈ I × Ic | j > i},	where Ic = {1, . . . , m} \ I.
Proof. This is an amalgamation of Corollary 29 and Theorem 30.	口
25