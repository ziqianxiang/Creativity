Published as a conference paper at ICLR 2020
Variational Autoencoders for Highly Multi-
variate Spatial Point Processes Intensities
Baichuan Yuan1, Xiaowei Wang2, Jianxin Ma2, Chang Zhou2,
Andrea L. Bertozzi1 , Hongxia Yang2
1	Department of Mathematics, University of California, Los Angeles
2	DAMO Academy, Alibaba Group
ybcmath@gmail.com, daemon.wxw@alibaba-inc.com,
majx13fromthu@gmail.com, ericzhou.zc@alibaba-inc.com,
bertozzi@math.ucla.edu, yang.yhx@alibaba-inc.com
Ab stract
Multivariate spatial point process models can describe heterotopic data over space.
However, highly multivariate intensities are computationally challenging due to
the curse of dimensionality. To bridge this gap, we introduce a declustering based
hidden variable model that leads to an efficient inference procedure via a varia-
tional autoencoder (VAE). We also prove that this model is a generalization of
the VAE-based model for collaborative filtering. This leads to an interesting ap-
plication of spatial point process models to recommender systems. Experimental
results show the method’s utility on both synthetic data and real-world data sets.
1	Introduction
Multivariate point processes are widely used to model events of multiple types occurring in an n
dimensional continuum. This paper focuses on multivariate spatial point processes (SPP), which
can uncover hidden connections between subprocesses based on the correlations of their spatial
point patterns. Often we encounter missing data problems, where some subprocesses are not fully
observable. The underlying connections could further contribute to the prediction of these subpro-
cesses over the unobserved areas. Moreno-Munoz et al. (2018) has shown the effectiveness of this
joint model for Gaussian processes with heterotopic data. Multi-output models in Lian et al. (2015)
such as coregionalization and cokriging can outperform independent predictions. However, there is
limited literature on the statistical methodology of the highly multivariate spatial point processes,
according to the very recent paper (Choiruddin et al., 2019).
Inference for multivariate spatial point processes intensities is still a challenging problem (Taylor
et al., 2015), especially with a large number of subprocesses. For popular Gaussian processes-based
approaches (Williams & Rasmussen, 2006), the multivariate intensity often consists of independent
and multi-output Gaussian processes. The complexity of the models and the curse of dimensional-
ity hinder this approach for highly multivariate data, such as friendship networks and recommender
systems with millions of users. In these problems, we only partially observe the events (e.g. users in-
teract with items, locations) for each subprocess (user). It is necessary to jointly infer the preference
of each user based on their hidden correlations. For example, a common approach in recommender
systems, collaborative filtering (He et al., 2017), predicts the item interests of each user with the
help of the collection of item preferences for a large number of users.
To address these problems, we propose a multivariate spatial point process model with a nonparamet-
ric intensity. We extend the well-known kernel estimator in Diggle (1985) to the multivariate case.
This generalization is achieved through the introduction of hidden variables inspired by stochastic
declustering (Zhuang et al., 2002). The latent variables naturally lead to a variational Bayesian in-
ference approach, which is different from the frequentist point estimation in the kernel estimator. To
reduce the complexity in the highly multivariate case, we consider an alternative set of hidden vari-
ables that are designed to work well as latent variables for a variational autoencoder (VAE) (Kingma
& Welling, 2014). This amortized inference (Gershman & Goodman, 2014) approach leads to fast
inference once the model is fully trained. Further, we show the equivalence for these two different
1
Published as a conference paper at ICLR 2020
settings of hidden variables using the properties of the spatial point process. This efficient approach
makes it possible to apply multivariate spatial point processes in many areas, including location-
based social networks and recommender systems with many users. Moreover, the nonparametric
method for analyzing spatial point data patterns is not related to specific parametric families of
models, which only requires the intensity to be well-defined.
Our approach is not a direct replacement for current inference methods on few-variate spatial point
processes (Jalilian et al., 2015). In contrast to the classical methodology, VAE requires a large
number of training data. The highly multivariate data that are widely available in social networks
and recommender systems can be ideal applications for our approach. In fact, it can be shown that
our model is a generalization of a state-of-the-art VAE-based collaborative filtering model (Liang
et al., 2018). Our model nonparametrically fits the underlying intensity function. Compared with
the multinomial distribution used in Liang et al. (2018), this leads to not only a smoother intensity
over space but also better predictions in terms of ranking-based losses. Compared to a univariate
model, such as the trans-Gaussian Cox processes (Williams & Rasmussen, 2006), our multivariate
model enhances the predictive ability on missing or unobserved areas, which is consistent with the
results of heterogeneous multi-output Gaussian processes (Moreno-MUnoz et al., 2018).
The contributions of this paper are three-fold. We first build a novel multivariate spatial point pro-
cess model and find a direct connection with the VAE-based collaborative filtering through detailed
theoretical analysis. Secondly, this connection introduces amortized inference for an efficient mul-
tivariate point process estimation. Finally, point processes generalize the discrete distribution used
in (Liang et al., 2018) and lead to a better modeling of spatial heterogeneity. We validate these
benefits through experiments for multiple multivariate data sets, showing improvement over classic
SPP methods and potentials on collaborative filtering applications.
2	Preliminaries
Spatial point process A point process (PP) is a random counting measure N (x) on a com-
plete separable metric space R (here we always assume that R ⊂ Rn) that takes values on
{0, 1, 2, ...} S{∞}. While the major theory of point processes centers around the temporal dynam-
ics, spatial point process models (Diggle et al., 1983) are established in forestry and seismology,
focusing on the stationary and isotropic case. We focus on the (first-order) intensity function λ(x),
which is the expected rate of the accumulation of points around a particular spatial location x. We
write
E [N (∆x)]	小
λ(x) = lim ————.——,	(1)
J	∣∆χ∣即	∣∆x∣
where ∆x is a small ball in the metric space, e.g. the Euclidean space Rn, with the centre x and
measure ∣∆χ∣ . The second-order intensity function is naturally defined as
E [N (∆x)N (∆y)]
λ(2)(χ, y) = lim ----------i-——―.-------,	(2)
∣∆χ∣,∣∆y∣ψo	∣∆x∣∣∆y∣
measuring the chance of points co-occurring in both ∆x and ∆y. Normalizing this leads to the
Pair-CorrelationfUnction g(x, y) = λ(2)(x, y)∕λ(x)λ(y). g(x, y) > 1 indicates that points are more
likely to form clusters than the simple Poisson process where g(x, y) = 1.
Common models in SPPs include the Poisson process with a non-stationary rate λ(x), and the Cox
process with a nonnegative-valued intensity process Λ(x), which is also a stochastic process. Cox
processes conditional on a realization of the intensity process Λ(x) = λ(x) are Poisson processes
with intensity λ(x). To model the aggregated points patterns, Poisson cluster (Neyman-Scott) pro-
cesses generate parent events from a Poisson process. Then each parent independently generates a
random number of offsprings. The relative positions of these offsprings to the parent are distributed
according to some p.d.f Kσ (x) in space (Diggle et al., 1983). Many point process models, including
most Cox processes, are in fact Poisson cluster processes. The duality between Cox processes and
cluster processes is widely used to construct Cox process models. For example, the kernel-based
intensity process Λ(x) = Pi∞=1 Kσ(x - xi) with xi from a Poisson process, is essentially a Poisson
cluster process. The number of offsprings is from a Poisson distribution with λ = 1 and the relative
position distribution is Kσ(x). Repulsive SPPs, on the other hand, model that nearby points of the
2
Published as a conference paper at ICLR 2020
process tend to repel each other. Higher order intensities are often considered in this case, such as
determinantal PPs.
Alternatively, if we are more interested in the realization intensity λ(x) than the mechanical in-
terpretation, the trans-Gaussian Cox process provides a tractable way to construct the Cox process
using a nonlinear transformation on a Gaussian process S(x). Popular choices for Λ(x) include the
log-Gaussian Cox process (LGCP) with exp(S(x)) and the permanental process with S(x)2. Re-
cent works on Cox processes have been extensively focused on the cases that are modulated via the
Gaussian random field, due to its capability in modeling the intensity and pair-correlations between
subprocesses. In the next section, we develop a more explicit approach to model interactions for fast
inference and the generalization ability for new subprocesses.
Inference for Point Processes Inference methods for point processes are mainly based on the or-
der statistics or likelihood function. The order statistics are often estimated nonparametrically, such
as the kernel estimator (Diggle, 1985) of the intensity function. For the likelihood-based inference,
we assume that one observes events X = {xi }iN=1 of the underlying spatial point process over the
area R. The log-likelihood for the inhomogeneous Poisson process over space R is
log P(X ∣Θ)
N
log(λ(xi)) -	λ(x) dx .
(3)
The integration term is the log void probability and can be viewed as a normalization term for the
likelihood. For Cox processes, the likelihood is the expectation over the Poisson likelihood. It is
difficult to directly integrate over the distribution of Λ. Monte Carlo methods (Adams et al., 2009)
are commonly used to approximate the expectation. To improve the scalability of the expensive
sampling, many methods such as variational inference (Lloyd et al., 2015), Laplace approximation
(Williams & Rasmussen, 2006) and reproducing kernel Hilbert spaces (Flaxman et al., 2017) are
proposed.
Variational Autoencoder As a stochastic variational inference algorithm, VAE (Kingma &
Welling, 2014) is maximizing the evidence lower bound (ELBO) of the log-likelihood function
logP(X∣Θ) ≥ Eqφ(^z∣χ)[log(pθ(X|z)] - KL(qφ(z∖X)∣p(z)).	(4)
The hidden variables z have a simple multivariate Gaussian prior p(z) = N (z; 0, I). The true pos-
terior, which is often intractable as in the Cox process, is approximated via a multivariate Gaussian
qφ(z|X) = N(z; μφ(X),σφ(X)). The KL divergence term in the ELBO can be calculated analyti-
cally. VAE uses a multilayer perceptron (MLP) to learn the mean and variance of the approximated
posterior directly from the data. The most related work here is a recent VAE-based model for col-
laborative filtering (VAE-CF) (Liang et al., 2018). They assume that each user is a multinomial
distribution over items with the log-likelihood logpθ(Xu∣zu) = PN=I Xiu log∏i(zu) for each user
u. Here Xu is the observed data of user clicking items, πi(zu) is the probability that user u clicks
the item i and Xiu is an indicator function on whether the user u clicked the item i.
3	Multivariate Spatial Point Processes
Here we consider a multivariate case of the SPP, with U interdependent univariate point processes
on the sample space R. The intensity function is measured in a similar way as the univariate case
via λu(x) = lim∣∆χ∣ψo (E [Nu(∆x)] /∖∆x∖), where Nu(∆x) is the number of events within a set
∆x for the subprocess u.
3.1	A nonparametric model
The observed data of multivariate SPP include the location of Nu events Xu = {xiu }iN=u1 associated
with each subprocess u. For each u, the observed event locations follow a Poisson process with spa-
tial intensity λu(x), which is a realization of the random intensity Λu(x). Using the nonparameteric
kernel estimator, the intensity of the subprocess u is estimated by
Nu
λu(x) =	Kσ(x - xiu).	(5)
i=1
3
Published as a conference paper at ICLR 2020
Here Kσ (x) is a kernel function and we usually adopt the radial basis function kernel (RBF) where
Kσ(x) = exp(-∣∣xk2∕2σ2). We ignore the end-correction (Diggle, 1985) for now.
In real-world applications, however, one often encounters the missing data problem, where we can-
not directly observe points in certain areas for some subprocesses. Instead, we seek to infer the
hidden data from other fully observed subprocesses. In our model, we assume that each subpro-
cess reflects the stochastic and heterogeneous patterns. For example, users in an e-commerce plat-
form usually prefer different categories. As in Poisson cluster processes, this naturally leads to
events clustering in specific areas. Another real-world example is the aggregation of check-in ac-
tivities around the home and workplaces for social network users (Cho et al., 2011). Note that
N = PuU=1 Nu is the total number of events. We introduce hidden variables Yiu for each event
xi = 1, ..., N and subprocess u = 1, ..., U, where Yiu = 1 if the subprocess u includes event xi and
Yiu = 0 otherwise. EYiu = piu is the probability that event xi is from the subprocess u. Then the
intensity process for our multivariate SPP model is
N
Λu(x) = XYiuKσ(x - xi),	(6)
i=1
for each subprocess u. This model generalizes the kernel density-based intensity to the missing data
case. Similarly to the original method, it can be applied to estimate the intensity for both cluster
processes such as Cox processes and repulsive ones like determinantal PPs. In order to incorporate
prior information and model the data uncertainty, we adopt a variational inference approach for the
hidden variables.
3.2	Variational inference
A major drawback of current inference methods for SPP is the introduction of a large number of
parameters in the highly multivariate case. For our model, we use an amortized inference approach
- VAE (Kingma & Welling, 2014) to avoid the computational complexity of directly estimating the
posterior for each subprocess u.
The generative process of our model can be described as follows: For each subprocess u, it has
a K-dimensional hidden variable Zu with a multivariate normal prior Zu 〜 N(0, IK). Here we
use a low-dimensional representation and then a nonlinear mapping fθ (zu) = {piu}iN=1 transforms
Zu so that it has the same dimension as the number of events N . Finally the spatial points of the
subprocess u are sampled according to the intensity λu(x) = PiN=1 piuKσ(x -xi). We approximate
the intractable posterior distribution of z, q(z∣X) with a multivariate Gaussian N(μφ(X), o@(X)).
As in Liang et al. (2018), we use MLPs to learn the nonlinear function fθ (Z) with parameters θ and
the mean and variance with parameters φ. The variational bound of our multivariate Cox process
model is then
logp(Xu∣Θ) ≥ Eqφ(zu∣Xu)[log(pθ(Xu∣zu)] - KL(qφ(zu∖Xu)∖p(zu)) = L.	⑺
The first term in L is essentially a complete likelihood function. For each subprocess u, it has the
following (expected) intensity function
Nu
Eqφ (zυ∣Xυ)Au(X) =	piuKσ(x-xiu)	(8)
i=1
and a Poisson process log-likelihood function from (8) and (3)
Nu	Nu	Nu
Eqφ(zu |Xu) log pθ (Xu ∖Zu) =	log(	piuKσ(x-xiu)) -	piuKσ(x- xiu)dx.	(9)
i=1	i=1	R i=1
For applications without explicit spatial information, we embed each event into a latent space as a
vector. First, we obtain a similarity graph for all events. Then the embedding xi of ith event in this
graph is obtained via graph neural networks (GNNs) such as GraphSAGE (Hamilton et al., 2017).
See Figure 1 for an illustration of our framework. Both xi and Zu are learned jointly combining the
information of item embedding and user hidden variable.
4
Published as a conference paper at ICLR 2020
Figure 1: Visual illustration of our spatial point process model via VAE during the training.
3.3	Alternative Model
Recall that the hidden variables Yiu describe whether the event xi is from the subprocess u. By
definition, we have PuU=1 Yiu = 1 and PuU=1 piu = 1 for any i. During the training process, it is dif-
ficult to normalize the probability piu over all subprocesses (have to use the full data). Moreover, this
constraint leads to guv < 1 for u 6= v, implying mutual-inhibition behaviors between subprocesses.
Instead, we consider an alternative model where piu is the probability that the subprocess u generates
an event xi i.e. PiN=u1 piu = 1 for each u. During the training, the total number of events Nu is not
viewed as a hidden variable for each subprocess. Thus the alternative model essentially normalizes
λu by a constant. With the reparameterization trick in Kingma & Welling (2014), we sample the
log-likelihood function using all events within a mini user batch and compute the gradient. This
approach incorporates all information about the user so that negative sampling is not needed. See
Algorithm 1 for our training procedure. For the model prediction, the normalized intensity of a
new subprocess can be efficiently calculated in O(N) using the approximated posterior qφ (ZXnew)
and nonlinear function fθ (z) with parameters θ, φ inferred from data. We can further reduce the
computational challenge (Liang et al., 2018) for large N due to fθ(z) by discretizing the space.
Now we show the equivalence of our multivariate model and the alternative one. There are two
probabilities to consider. The first one is the conditional probability of observed events Xu in the
subprocess u with an intensity function λu(x), given that there are Nu events within the metric
space R. The second one is the probability of sample Xu of size Nu from the normalized density
hu(x) = λu(x)/ R λu(s)ds. For general SPPs data, we have
Theorem 1. A spatial point process on a measurable set R ⊂ Rn with an intensity function λu(x)
is equivalent to Nu i.i.d samples within R with p.d.f hu(x) = λu(x)/ R λu(s)ds, given we know
Nu = λu(s)ds, which is the number of points within R for the point process model.
Proof. See Section B in Appendix.	□
According to this theorem (see supplementary material), we can replace the log-likelihood function
(9) in the ELBO with
Nu
Eqφ(zuXu) logPθ(Xu∣Zu) =£log(hu(xU)) + C.	(10)
i=1
Here C is related to the log-likelihood on the number of events Nu , which is a constant because
R λu(s)ds = Nu is observed. One drawback of this approach is that, for the prediction of actual
missing data, we cannot infer the number of missing points. Instead, our VAE-based model gener-
ates the normalized intensity predicting the possible locations for the missing events. We use the
alternative definition of piu from now on.
5
Published as a conference paper at ICLR 2020
This result shows that VAE-CF is a special case of this multivariate SPP model over a discrete
space X of events. In fact, VAE-CF is the alternative model with a delta function as the kernel
(hu(xi) = λu(xi)/ X λu(s)ds = piu), which is equivalent to the SPP model according to the
theorem. To better model the spatial heterogeneity of events, one can replace the delta function with
other kernels or use more advanced SPP intensities. We simply use a RBF kernel here, resulting in
hu(x) = PN=Ipu exp(kx - xik2∕2σ* 2 * 4).
Algorithm 1: Training VAE SPP with stochastic gradient descent.
Input: Traning subprocesses U ∈ UT with their point locations Xu
Result: Parameters θ and φ
Initialize θ and φ randomly;
while not converged do
Sample a subprocesses batch Ub from UT and their points Xb = Su∈U Xu ;
forall u ∈ Ub do
Sample Zu 〜 N(μφ(Xu), σφ(Xu)) with reparameterization trick;
Compute fθ(zu) = {piu}xi∈Xb ;
forall x ∈ Xu do
I Compute sampled normalized intensity hu(χ) ≈ Pxa∈χb puKσ(X - Xi);
end
Compute noisy gradients of the ELBO L w.r.t θ and φ
end
Average noisy gradients over batch;
Update θ and φ with the Adam optimizer (Kingma & Ba, 2015);
end
One benefit of this alternative model is its resulted consistency. The nonparametric kernel estimation
for the point process intensity is unbiased. To see this, for any measurable set R, we take the
expectation of the estimated intensity λ(X) over the Poisson point process distribution
N
E	λ(X)dX =	E X Kσ(X - Xi)dX =	Kσ(X - y)ρ(y)dydX =	ρ(y)dy,	(11)
where ρ(y) is the true intensity function. Then Eλ(X) = ρ(X) under mild conditions, e.g., a spa-
tially continuous assumption on ρ. But it is inconsistent due to the non-vanishing variance without
normalization. For our alternative model, the normalized intensity function hu (X) is still unbiased.
And according to the standard theory of the multivariate kernel density estimation (KDE), the con-
sistency of hu(X) is also guaranteed. Another benefit of using this alternative form can be seen from
the cross pair-correlation function. For the alternative model, we remove the undesirable restriction
of negative correlations between all users (guv < 1 for u 6= v) and can incorporate more diverse
relationships between users. To see this, we first consider the auto and cross pair-correlation func-
tion guv = EΛuΛv∕EΛuEΛv . For our original model, it is straightforward to prove that guu > 1
and guv < 1, u 6= v (see supplementary material). The auto pair-correlation functions show that our
model is more aggregate than the simple Poisson process.
4 Experiments
We compare our model (with the RBF kernel, VAE-SPP) with both VAE-CF (Liang et al., 2018) and
univariate spatial point process models using a standard KDE (Diggle, 1985) or TGCP (Williams &
Rasmussen, 2006) as intensity functions. We adopt the experiment setting in VAE-CF. We split the
data into training, validation and testing sets. For the multivariate model, the training data is used to
learn the parameters θ, φ. For KDE and TGCP models, we omit the training data because different
subprocesses are assumed to be independent and also because of the computational complexity of
fitting a highly multivariate TGCP. We assume that only 80% of the events in the validation and test
sets are observed. The remaining 20% are viewed as missing data to be inferred by different models.
Hyperparameters are selected on the validation data as in Liang et al. (2018). Finally, we compare
the prediction performance of different models on the missing data given the partially-observed
events. We use standard ranking losses such as NDCG@K and Recall@K defined in Appendix D.1.
6
Published as a conference paper at ICLR 2020
4.1	Multivariate SPP on s patial data
Synthetic data sets We simulate two different data sets using multiexponential and multisine
models. For the multiexponential data set, we simulate 5,000 Poisson processes with λk (x) =
ake-bkx, k = 1, ..., 5000, x ∈ [0, 30] as training data. Here ak and bk are uniformly sampled be-
tween [5, 10] and [0.1, 0.2] separately. 500 validation and 500 test subprocesses are generated in the
same way with parameters sampled from ak and bk . The multisine data set is generated via replac-
ing the intensity function with λk(x) = max(ak * sin(bkx), 5) and sampling ak and bk uniformly
between [5, 10] and [1, 2] separately. Each realization of the spatial point process is discretized using
a uniform grid over x with grid spacing 0.01.
Table 1: Testing results on the simulation data sets. Both the mean and variance are percentages
(Same below).________________________________________________________________________________
Name	Multiexp			Multisine		
	NDCG@100	Recall@50	Recall@100	NDCG@100	Recall@50	Recall@100
VAE-CF	6.78(0.28)	7.25(0.40)	14.5(0.52)	3.30(0.15)	2.49(0.13)	4.64(0.18)
VAE-SPP	7.11(0.31)	7.34(0.40)	14.9(0.54)	3.53(0.15)	2.58(0.13)	4.90(0.18)
KDE	5.27(0.15)	5.85(0.12)	11.8(0.17)	3.23(0.15)	2.29(0.12)	4.55(0.27)
TCGP	3.11(0.14)	3.32(0.11)	6.44(0.11)	3.77(0.14)	1.88(0.11)	3.92(0.17)
Location-based Social Network. We consider the Gowalla data set (Cho et al., 2011) in New
York City (NYC) and California (CA). We use a bounding box of -124.4096, 32.5343, -114.1308,
42.0095 for CA and -74.0479, 40.6829, -73.9067, 40.8820 for NYC (both from flickr1). Each user
with at least 20 events (check-ins) is viewed as a subprocess. There are 673,183 events and 6,728
users for Gowalla-CA. We randomly select 500 users as the validation set and 500 users as the
testing set. We use the remaining users for training. For Gowalla NYC, there are 86,703 events from
1,171 users. We set the size of both validation and testing sets to 100. For the spatial tessellation,
we use uniform grids (32 × 32 for NYC and 64 × 64 for CA). Both our model and VAE-CF can
work without grids. We further compare the performance of our model with VAE-CF by viewing
each location as an item.
Table 2: Testing results on the Gowalla data sets with uniform grids.
Name		CA				NYC		
	NDCG@100	Recall@50	Recall@100	NDCG@100	Recall@50	Recall@100
VAE-CF	41.8(1.5)	64.8(2.0)	70.0(2.0)	43.6(2.3)	73.9(2.9)	86.2(2.2)
VAE-SPP	42.3(1.5)	65.2(2.0)	70.2(1.9)	44.8(2.4)	74.5(2.9)	86.2(2.2)
KDE	34.5(1.5)	59.2(2.0)	64.0(2.0)	41.2(1.5)	69.9(2.0)	83.6(2.0)
TCGP	31.8(1.3)	56.5(2.0)	60.9(2.0)	37.3(2.3)	59.9(3.3)	75.9(2.8)
Table 3: Testing results on the Gowalla data sets without discretization.
Name		CA				NYC		
	NDCG@100	Recall@20	Recall@100	NDCG@100	Recall@20	Recall@100
VAE-CF	21.3(0.77)	16.6(0.74)	32.8(0.97)	16.0(1.7)	13.2(1.7)	26.3(2.4)
VAE-SPP	21.6(0.77)	17.0(0.80)	33.5(0.76)	16.1(1.7)	13.7(1.8)	27.1(2.5)
In Table 1, we summarize the performance of both multivariate and univariate models on the sim-
ulation data sets. It is clear that the multivariate models outperform the univariate ones. Moreover,
testing on multivariate models takes less time because it only evaluates the posterior probability
and intensity function. This illustrates the power of multivariate models using amortized inference.
Within the multivariate models, our continuous model further improves upon the discrete VAE-CF.
This is due to the fact that these simulation intensities are continuous over R. For real-world appli-
cations, the results on the location-based social network prediction and recommendation with and
without grids are presented in Table 2 and 3. We observe the same pattern in both NYC and CA.
1https://www.flickr.com/places/info/
7
Published as a conference paper at ICLR 2020
Figure 2: Estimated density functions for a Gowalla user in NYC (log scale). The first row from
left to right: observed check-in locations (in red), held-out check-in locations (in blue, as missing
data) and the estimated intensity from VAE-SPP. The second row from left to right: the estimated
intensity (or density) from VAE-CF, KDE and TGCP.
We stop using univariate models from now on due to their inferior performances, especially for col-
laborative filtering applications. Moreover, our model improves discrete VAE-CF regardless of the
choice of spatial grids. For visualization purposes, in Figure 2, we plot a user’s check-in locations in
Gowalla-NYC and intensities estimated via different methods. Comparing with VAE-CF, our model
generates a continuous intensity. The univariate models overfit the training data and lead to inferior
predictions of the missing data.
4.2	Multivariate SPP with a latent space
MovieLens data sets (ML-100K and ML-1M) include the movie (item) rating by users and we
binarize the rating with a threshold of 4. In the spatial point process setting, we view each user
as a subprocess over the latent space of item embeddings. Here the item embedding is generated
via a GNN. This framework is a natural generalization of the multimodal distribution over items.
The item-item graph is constructed based on item-item similarities. We use the Jaccard distance
to measure the similarities between items, which are further viewed as the sampling probabilities
for GNN. Currently, we only consider 1-hop connections. Both GNN and VAE are trained jointly,
which is more expensive than VAE-CF but leads to better performance compared to separate training
(see Appendix D). For movie recommendation tasks, we compare the discrete VAE-CF to our joint
model with GNN. The results in Table 4 show again the improvement of our model over the baseline.
Table 4: Testing results on the MoVieLens data sets.
Name		ML100K				ML1M		
	NDCG@100	ReCall@20	ReCall@100	NDCG@100	ReCall@20	ReCall@100
VAE-CF	40.8(2.8)	32.3(2.8)	57.6(3.3)	41.6(0.76)	33.1(0.81)	56.8(0.88)
VAE-SPP	41.5(2.9)	31.3(2.7)	59.0(3.5)	42.3(0.77)	33.9(0.82)	57.6(0.88)
8
Published as a conference paper at ICLR 2020
5 Conclusion
In this paper, we introduce a novel spatial point process model for efficient inference on the highly
multivariate case. Through amortized inference, our model makes it possible to investigate correla-
tions between a myriad of point patterns based on a large number of training data, and the theoretical
analysis on the density and intensity for SPPs builds the connection between our model and VAE-
CF. There are many promising directions of future works including the extension for multivariate
spatiotemporal PPs (Mohler et al., 2011; Yuan et al., 2019) and using features as covariances. There
are multiple ways to estimate the mean rate (hu(x)) of a spatial point process overall events, includ-
ing Gaussian mixture models, Gaussian processes and flow-based models. For future work, we can
investigate the connections between our model and other density-based estimations for point pro-
cesses. Another interesting application is to handle real-world recommender systems via improving
the joint training efficiency and comparing thoroughly with simpler algorithms as in Dacrema et al.
(2019).
Acknowledgments
We would like to thank the comments from Frederic P. Schoenberg and George Mohler. Andrea
L. Bertozzi and Baichuan Yuan want to thank the support of NIJ fellowship 2018-R2-CX-0013 and
NSF DMS-1737770.
References
Ryan Prescott Adams, Iain Murray, and David JC MacKay. Tractable nonparametric bayesian in-
ference in poisson processes with gaussian process intensities. In Proceedings of the 26th Annual
International Conference on Machine Learning, pp. 9-16. ACM, 2009.
Eunjoon Cho, Seth A Myers, and Jure Leskovec. Friendship and mobility: user movement in
location-based social networks. In Proceedings of the 17th ACM SIGKDD international con-
ference on Knowledge discovery and data mining, pp. 1082-1090. ACM, 2011.
Achmad Choiruddin, Francisco Cuevas-Pacheco, Jean-Francois Coeurjolly, and Rasmus
Waagepetersen. Regularized estimation for highly multivariate log gaussian cox processes. arXiv
preprint arXiv:1905.01455, 2019.
Maurizio Ferrari Dacrema, Paolo Cremonesi, and Dietmar Jannach. Are we really making much
progress? a worrying analysis of recent neural recommendation approaches. In Proceedings of
the 13th ACM Conference on Recommender Systems, pp. 101-109. ACM, 2019.
Peter Diggle. A kernel method for smoothing point process data. Journal of the Royal Statistical
Society: Series C (Applied Statistics), 34(2):138-147, 1985.
Peter J Diggle et al. Statistical analysis of spatial point patterns. Academic press, 1983.
Seth Flaxman, Yee Whye Teh, Dino Sejdinovic, et al. Poisson intensity estimation with reproducing
kernels. Electronic Journal of Statistics, 11(2):5081-5104, 2017.
Samuel Gershman and Noah Goodman. Amortized inference in probabilistic reasoning. In Pro-
ceedings of the annual meeting of the cognitive science society, volume 36, 2014.
Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs.
In Advances in Neural Information Processing Systems, pp. 1024-1034, 2017.
Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. Neural col-
laborative filtering. In Proceedings of the 26th International Conference on World Wide Web, pp.
173-182. International World Wide Web Conferences Steering Committee, 2017.
Abdollah Jalilian, Yongtao Guan, Jorge Mateu, and Rasmus Waagepetersen. Multivariate product-
shot-noise cox point process models. Biometrics, 71(4):1022-1033, 2015.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. ICLR, 2015.
9
Published as a conference paper at ICLR 2020
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. ICLR, 2014.
Wenzhao Lian, Ricardo Henao, Vinayak Rao, Joseph Lucas, and Lawrence Carin. A multitask point
process predictive model. In International Conference on Machine Learning, pp. 2030-2038,
2015.
Dawen Liang, Rahul G Krishnan, Matthew D Hoffman, and Tony Jebara. Variational autoencoders
for collaborative filtering. In Proceedings of the 2018 World Wide Web Conference on World Wide
Web, pp. 689-698. International World Wide Web Conferences Steering Committee, 2018.
Chris Lloyd, Tom Gunter, Michael Osborne, and Stephen Roberts. Variational inference for gaussian
process modulated poisson processes. In International Conference on Machine Learning, pp.
1814-1822, 2015.
George O Mohler, Martin B Short, P Jeffrey Brantingham, Frederic Paik Schoenberg, and George E
Tita. Self-exciting point process modeling of crime. Journal of the American Statistical Associa-
tion, 106(493):100-108, 2011.
Pablo Moreno-Munoz, Antonio Artes, and Mauricio Alvarez. Heterogeneous multi-output gaussian
process prediction. In Advances in Neural Information Processing Systems, pp. 6711-6720, 2018.
Benjamin Taylor, Tilman Davies, Barry Rowlingson, and Peter Diggle. Bayesian inference and data
augmentation schemes for spatial, spatiotemporal and multivariate log-gaussian cox processes in
r. Journal of Statistical Software, 63:1-48, 2015.
Christopher KI Williams and Carl Edward Rasmussen. Gaussian processes for machine learning,
volume 2. MIT Press Cambridge, MA, 2006.
Baichuan Yuan, Hao Li, Andrea L Bertozzi, P Jeffrey Brantingham, and Mason A Porter. Multivari-
ate spatiotemporal hawkes processes and network reconstruction. SIAM Journal on Mathematics
of Data Science, 1(2):356-382, 2019.
Jiancang Zhuang, Yosihiko Ogata, and David Vere-Jones. Stochastic declustering of space-time
earthquake occurrences. Journal of the American Statistical Association, 97(458):369-380, 2002.
10
Published as a conference paper at ICLR 2020
A Table of Notations
Table 5: Notations.
Notation	Definition or Descriptions
N(x)	counting measure on a metric space R
Nu	the number of events of subprocess U
λu (x)	intensity function of a subprocess U
Λu(x)	intensity process of a subprocess U
Kσ(x)	kernel function
U	# of subprocesses on the space
Ub	subprocesses in a batch
X	events set
Xu	observed events of subprocess U
Xb	all the observed events in a batch of sub- processes
xi	embedding/location of the ith event
Yiu	hidden variables indicate whether the subprocess U includes the ith event
zu	K-dimensional hidden variable repre- sents subprocess U
piu	probability of the ith event occurs in subprocess U
φ, θ	parameters of encoder(μφ,σφ) and decoder(fθ)
guv = EΛuΛv∕EΛuEΛv	auto and cross pair-correlation function
hu(x)	normalized density
B Proof of Theorem 1
Proof. We define our model as a point process on R with the intensity function λu (x).
The alternative model is Nu i.i.d samples within R with p.d.f hu(x), given that we know Nu
λu(s)ds is the number of points within the point process model.
1) Our model has the following probability generating functional
G(v) = exp(-	[1 - v(x)] Λ(dx))
Rd
2) Given Nu ,
Nu
p(x1, ..., xNu|Nu) =	hu(xi)
i=1
(12)
(13)
3) Our alternative model (a counting r.v. N(x) with locations according to hu(x)) has the following
characteristic functional
∞
Gc (v) = X p(N (R) = n)E[exp(	log(v(s))N (ds))|N (R) = n]
n=0	R
(14)
11
Published as a conference paper at ICLR 2020
Using 2), we can evaluate this conditional probability
E[exp( ∕log(v(s))N(ds))∣N(R) = n] = (RR ∖(S)T)ds )n
R	λu (s)ds
Using 2) again and because the point process observation probability is
p(ω) = p(N (R) = Nu)p(x1, ..., xNu |Nu)
(15)
(16)
1n
京叮 λu(xi)] exp(- J λ(x)dx),
we have
Gc(V)= exp(- Z λ(x)dx)(1 + ^X ɪ( f λ(s)v(s)ds)n) = exp( / λ(s)(v(s) — 1)ds). (17)
R	n=1 n R	R
The theorem follows from Gc(v) = G(v) as the probability generating functional completely deter-
mines the probability structure of the point process.	□
We show that (10) holds in the main paper.
Corollary 1.1.
Nu
Eqφ(zu∣Xu) log p(Xu∣Zu) = X log(hu (xu)) + C.	(18)
i=1
Proof. Define λu(x) = Eqφ(zu∣Xu)Λu(x).
Eqφ(zu∣Xu) logp(Xu∣Zu) =log(p(N(R) = Nu)P(Xu,…,χNu∣Nu))
Nu
= X log(hu(xiu)) + log(p(N(R) = Nu))
i=1
log(p(N (R) = Nu)) = n log(	λ(x)dx) - log(n!) -	λ(x)dx
is only a function of Nu .
(19)
(20)
(21)
□
C Auto and cross pair-correlation functions
We show that gu,v(x, y) > 1 for u = v and gu,v(x, y) < 1 for u 6= v for our orginal model.
	EΛu(x)Λv (x) gu,v(X,y)= EΛu(x)EΛv(x)	(22)
We have	EΛu(x)Λv(x) = E(XN YiuKh(x-xi))(XN YjvKh(y-xj))	(23)
	i=1	j=1 NN = XX EYiuYjvKh(X - Xi)Kh(y - Xj).	(24)
Similarly,	i=1 j=1 NN EΛu(X)EΛv(X) = (E XYiuKh(X-Xi))(E XYjvKh(y-Xj))	(25)
	i=1	j=1 NN = X XpiupjvKh (X - Xi)Kh(y - Xj).	(26)
	i=1 j=1	
Note that PuU=1 Yiu = 1 and PuU=1 piu = 1. When i 6= j, we have EYiuYjv = piupjv for any u, v.
When i = j, EYiuYiu = piu > (piu)2 for u = v and EYiuYiv = 0 < piupiv. Then it is easy to see
gu,v(x, y) > 1 for u = v and gu,v(x, y) < 1 for u 6= v.
12
Published as a conference paper at ICLR 2020
D More on Experiments
D.1 Metrics Definition
The ranking performance is evaluated through recall at K (Recall@K) and normalized discounted
cumulative gain at K (NDCG@K). In our VAE-SPP model, the predicted rank of the held-out items
Iu for each user u are obtained from sorting the intensity function λu(x).
Here we keep the definition in Liang et al., (2018). Recall@K is defined as
ReCan@K = Pi=I ：(； ∈ Iu .
|Iu|
(27)
NDCG@K is CalCulated by normalizing disCounted Cumulative gain (DCG@K) with ideal DCG@K
(IDCG@K). The definition are as follows
DCG@K
XX 2I(ri∈Iu) — ι
=log2(i + I) ,
NDCG@K
DCG@K
IDCG@K
(28)
where I is the indiCator funCtion and ri is the ith item among held-out items; IDCG@K is the ideal
DCG@K when the ranked list is perfeCtly ranked.
D.2 Hyperparameters
We implement our models in Tensorflow based on VAE-CF2. We keep the same MLP arChiteCtures
and hyperparameters for both of them. We use β-VAE with as suggested. The only additional
hyperparameters for our model is the σ2 in the kernel funCtion, whiCh is determined using grid
searCh on the validation set. We ConduCted the experiments on a single GTX 1080 TI 11GB GPU.
For simulation data, we train both models for 200 epoChs using Adam optimizer with β = 0.2,
lr = 5 × 10-5. We use mini-batChes of size 20. Our arChiteCtures Consist of a one layer MLP
with K = 50. For VAE-SPP, σ2 = 0.001. For Gowalla-NYC data, we train both models for
200 epoChs using Adam optimizer with β = 0.2, lr = 5 × 10-4. We use mini-batChes of size
20. Our arChiteCtures Consist of a one layer MLP with K = 50. For VAE-SPP, σ2 = 1 × 10-5.
For Gowalla-LA data, we train both models for 200 epoChs using Adam optimizer with β = 0.2,
lr = 1 × 10-3. We use mini-batChes of size 20. Our arChiteCtures Consist of a one layer MLP with
K = 50. For VAE-SPP, σ2 = 0.001. For ML-1M data, we train both models for 100 epoChs using
Adam optimizer with β = 0.2, lr = 1 × 10-3. We use mini-batChes of size 5. Our arChiteCtures
Consist of a one layer MLP with K = 200. For VAE-SPP, σ2 = 1 × 10-5. For ML-100K data,
we train both models for 100 epoChs using Adam optimizer with β = 0.2, lr = 1 × 10-3. We use
mini-batChes of size 5. Our arChiteCtures Consist of a one layer MLP with K = 200. For VAE-
SPP, σ2 = 1 × 10-5. The one-layer GNN in ML data is trained using GraphSAGE, for whiCh the
embedding dimension is 32 and the number of neighborhood is 10 for item and 5 for user. The graph
is Consist of the edges between users and items as well as the edges between items based on their
JaCCard similarity.
We use python statsmodel for the KDE and GPy for TGCP. Bandwidth for KDE is seleCted auto-
matiCally. The hyperparameters for TGCP are determined with a grid searCh on the validation set.
For simulation data sets, we set an RBF kernel with varianCe=1, lengthsCale=0.1 for TGCP. For
Gowalla-CA data sets, we set an Matern32 kernel with varianCe=1e-3, lengthsCale=0.1 for TGCP.
For Gowalla-NYC data sets, we set an Matern32 kernel with varianCe=1e-4, lengthsCale=0.01 for
TGCP.
D.3 Additional Experiments
On the training of VAE and GNN, we tried different training settings (separately or jointly) and
Choose to train them jointly. We also tested the point estimate version of the VAE-CF Called DAE-
CF (Mult-DAE in Liang et al., (2018), with the same setting), whiCh Can improve the result under
2https://github.com/dawenl/vae_cf
13
Published as a conference paper at ICLR 2020
certain metrics. One can easily extend our work to a DAE-SPP to obtain a point estimation for the
SPP intensity.
Table 6: Testing results on MovieLens-100K. These methods share the same network and trained
with 100 epochs. The test data is evaluating the model with the best performance during the valida-
tion. Separate means that GNN is trained separately with VAE-SPP.
	NDCG@100	Recall@20	Recall@100
VAE-CF	40.88	32.32	57.63
DAE-CF	40.98	29.29	58.80
VAE-SPP	41.50	31.34	58.99
VAE-SPP-Separate	41.43	31.15	58.82
We also did experiments on the MLPs for VAE. For Movie Lens 1M, the larger network in VAE-CF
leads to a 40.3 NDCG@100 for VAE-CF and 41.9 for VAE-SPP. As a result, we use the smaller one
instead.
14