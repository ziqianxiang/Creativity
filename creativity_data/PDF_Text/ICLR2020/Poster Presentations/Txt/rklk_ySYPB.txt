Published as a conference paper at ICLR 2020
Provable robustness against all adversarial
lp-perturbations for p ≥ 1
Francesco Croce
University of Tubingen, Germany
Matthias Hein
University of Tubingen, Germany
Abstract
In recent years several adversarial attacks and defenses have been pro-
posed. Often seemingly robust models turn out to be non-robust when
more sophisticated attacks are used. One way out of this dilemma are
provable robustness guarantees. While provably robust models for specific
lp-perturbation models have been developed, we show that they do not
come with any guarantee against other lq -perturbations. We propose a new
regularization scheme, MMR-Universal, for ReLU networks which enforces
robustness wrt l1- and l∞-perturbations and show how that leads to the
first provably robust models wrt any lp-norm for p ≥ 1.
1 Introduction
The vulnerability of neural networks against adversarial manipulations (Szegedy et al., 2014;
Goodfellow et al., 2015) is a problem for their deployment in safety critical systems such
as autonomous driving and medical applications. In fact, small perturbations of the input
which appear irrelevant or are even imperceivable to humans change the decisions of neural
networks. This questions their reliability and makes them a target of adversarial attacks.
To mitigate the non-robustness of neural networks many empirical defenses have been
proposed, e.g. by Gu & Rigazio (2015); Zheng et al. (2016); Papernot et al. (2016); Huang
et al. (2016); Bastani et al. (2016); Madry et al. (2018), but at the same time more
sophisticated attacks have proven these defenses to be ineffective (Carlini & Wagner, 2017;
Athalye et al., 2018; Mosbach et al., 2018), with the exception of the adversarial training
of Madry et al. (2018). However, even these l∞-adversarially trained models are not more
robust than normal ones when attacked with perturbations of small lp-norms with p = ∞
(Sharma & Chen, 2019; Schott et al., 2019; Croce et al., 2019b; Kang et al., 2019). The
situation becomes even more complicated if one extends the attack models beyond lp -balls
to other sets of perturbations (Brown et al., 2017; Engstrom et al., 2017; Hendrycks &
Dietterich, 2019; Geirhos et al., 2019).
Another approach, which fixes the problem of overestimating the robustness of a model, is
provable guarantees, which means that one certifies that the decision of the network does
not change in a certain lp-ball around the target point. Along this line, current state-of-the-
art methods compute either the norm of the minimal perturbation changing the decision
at a point (e.g. Katz et al. (2017); Tjeng et al. (2019)) or lower bounds on it (Hein &
Andriushchenko, 2017; Raghunathan et al., 2018; Wong & Kolter, 2018). Several new training
schemes like (Hein & Andriushchenko, 2017; Raghunathan et al., 2018; Wong & Kolter, 2018;
Mirman et al., 2018; Croce et al., 2019a; Xiao et al., 2019; Gowal et al., 2018) aim at both
enhancing the robustness of networks and producing models more amenable to verification
techniques. However, all of them are only able to prove robustness against a single kind of
perturbations, typically either l2- or l∞-bounded, and not wrt all the lp-norms simultaneously,
as shown in Section 5. Some are also designed to work for a specific p (Mirman et al., 2018;
Gowal et al., 2018), and it is not clear if they can be extended to other norms.
The only two papers which have shown, with some limitations, non-trivial empirical robustness
against multiple types of adversarial examples are Schott et al. (2019) and Tramer & Boneh
1
Published as a conference paper at ICLR 2020
(2019), which resist to l0- resp. l1-, l2- and l∞-attacks. However, they come without provable
guarantees and Schott et al. (2019) is restricted to MNIST.
In this paper we aim at robustness against all the lp-bounded attacks for p ≥ 1. We study
the non-trivial case where none of the lp-balls is contained in another. If tp is the radius
of the lp-ball for which we want to be provably robust, this requires: dP-q j > tp > j for
p < q and d being the input dimension. We show that, for normally trained models, for the
l1- and l∞-balls we use in the experiments none of the adversarial examples constrained to
be in the l1-ball (i.e. results of an l1 -attack) belongs to the l∞-ball, and vice versa. This
shows that certifying the union of such balls is significantly more complicated than getting
robust in only one of them, as in the case of the union the attackers have a much larger
variety of manipulations available to fool the classifier.
We propose a technique which allows to train piecewise affine models (like ReLU networks)
which are simultaneously provably robust to all the lp -norms with p ∈ [1, ∞]. First, we show
that having guarantees on the l1 - and l∞ -distance to the decision boundary and region
boundaries (the borders of the polytopes where the classifier is affine) is sufficient to derive
meaningful certificates on the robustness wrt all lp-norms for p ∈ (1, ∞). In particular, our
guarantees are independent of the dimension of the input space and thus go beyond a naive
approach where one just exploits that all lp-metrics can be upper- and lower-bounded wrt any
other lq -metric. Then, we extend the regularizer introduced in Croce et al. (2019a) so that we
can directly maximize these bounds at training time. Finally, we show the effectiveness of our
technique with experiments on four datasets, where the networks trained with our method
are the first ones having non-trivial provable robustness wrt l1-, l2- and l∞-perturbations.
2 Local properties and robustness guarantees of ReLU
networks
It is well known that feedforward neural networks (fully connected, CNNs, residual networks,
DenseNets etc.) with piecewise affine activation functions, e.g. ReLU, leaky ReLU, yield
continuous piecewise affine functions (see e.g. Arora et al. (2018); Croce & Hein (2018)).
Croce et al. (2019a) exploit this property to derive bounds on the robustness of such networks
against adversarial manipulations. In the following we recall the guarantees of Croce et al.
(2019a) wrt a single lp-perturbation which we extend in this paper to simultaneous guarantees
wrt all the lp-perturbations for p in [1, ∞].
2.1	ReLU networks as piecewise affine functions
Let f : Rd → RK be a classifier with d being the dimension of the input space and K the
number of classes. The classifier decision at a point x is given by arg max fr (x). In this paper
r=1,...,K
we deal with ReLU networks, that is with ReLU activation function (in fact our approach
can be easily extended to any piecewise affine activation function e.g. leaky ReLU or other
forms of layers leading to a piecewise affine classifier as in Croce et al. (2019b)).
Definition 2.1 A function f : Rd → R is cal led piecewise affine if there exists a finite set
of polytopes {Qr}rM=1 (referred to as linear regions of f) such that ∪rM=1Qr = Rd and f is an
affine function when restricted to every Qr .
Denoting the activation function as σ (σ(t) = max{0, t} if ReLU is used) and assuming L
hidden layers, we have the usual recursive definition of f as
g(l)(x) = W(l)f(l-1)(x) + b(l), f(l)(x)=σ(g(l)(x)), l=1,...,L,
with f(0) (x) ≡ x and f(x) = W (L+1) f (L) (x) + b(L+1) the output of f. Moreover, W (l) ∈
Rnl ×nl-1 and b(l) ∈ Rnl , where nl is the number of units in the l-th layer (n0 = d, nL+1 = K).
For the convenience of the reader we summarize from Croce & Hein (2018) the description
of the polytope Q(x) containing x and affine form of the classifier f when restricted to Q(x).
We assume that x does not lie on the boundary between polytopes (this is almost always
2
Published as a conference paper at ICLR 2020
true as faces shared between polytopes are of lower dimension). Let ∆(l) , Σ(l) ∈ Rnl ×nl for
l = 1, . . . , L be diagonal matrices defined elementwise as
∆(l)(x)ij = sign(fi(l)(x))
if i = j,
else.
Σ(l)(x) = 1 if i = j and fi(l) (x) > 0,
j 0 else.
This allows us to write f(l) (x) as composition of affine functions, that is
f(l)(x) =W(l)Σ(l-1)(x) W (l-1) Σ(l-2) (x) × ... W(1)x + b(1) ... + b(l-1) +b(l),
which we simplify as f(l) (x) = V (l)x + a(l), with V (l) ∈ Rnl ×d and a(l) ∈ Rnl given by
l-1	l-1 l-j
V(l) = W(l)( 口 Σ(l-j)(x)^(l-j)) and a(l) = b(l) + £ ( 口 W(I+1-^)∑Q-m)(X))b(j).
j=1	j=1	m=1
A forward pass through the network is sufficient to compute V(l) and b(l) for every l. The
polytope Q(x) is given as intersection of N = EL=I n half spaces defined by
Q(x)= ∩	∩	卜 ∈ R[∆(l)(x)ii (VCl)Z + a?) ≥0},
l=1,...,L i=1,...,nl
Finally, the affine restriction of f to Q(x) is f(z)|Q(x) = f (L+1) |Q(x) (z) = V(L+1)z + a(L+1) .
Let q be defined via P + ɪ = 1 and C the correct class of x. We introduce
dpB,l,j(x)
I (VF ,x + ajl)l
and
fc ( X ) - fs ( X )
vc( l+1) - v( l+1)∣∣
(1)
for every l = 1,. . . ,L, j = 1,. . . ,nL , s = 1,. . . ,K and s = c, which represent the N
lp -distances of x to the hyperplanes defining the polytope Q(x) and the K - 1 lp-distances
of x to the hyperplanes defining the decision boundaries in Q(x). Finally, we define
min min
l=1,...,L j=1,...,nl
,j (x) and
min
s=1,...,K
s=c
(2)
as the minimum values of these two sets of distances (note that dpD (x) < 0 if x is misclassified).
2.2	Robustness guarantees inside linear regions
The lp-robustness rp(x) of a classifier fat a point x, belonging to class c, wrt the lp-norm is
defined as the optimal value of the following optimization problem
rp(x) = min Il6Il , s.th. max fl (x + δ) ≥ f (x + δ), x + δ ∈ S,	(3)
δ∈Rd	p	l=c
where is S a set of constraints on the input, e.g. pixel values of images have to be in [0, 1].
The lp-robustness rp(x) is the smallest lp-distance to x of a point which is classified differently
from c. Thus, rp(x) = 0 for misclassified points. The following theorem from Croce et al.
(2019a), rephrased to fit the current notation, provides guarantees on rp(x).
Theorem 2.1 (Croce et al. (2019a)) If dpB (x) < dpD (x), then rp(x) ≥ dpB (x), while if
|dpD (x)| ≤ dpB (x), then rp(x) = max{dpD (x), 0}.
Although Theorem 2.1 holds for any lp-norm with p ≥ 1, it requires to compute dpB (x) and
dpD (x) for every p individually. In this paper, exploiting this result and the geometrical
arguments presented in Section 3, we show that it is possible to derive bounds on the
robustness rp(x) for any p ∈ (1, ∞) using only information on r1(x) and r∞ (x).
In the next section, we show that the straightforward usage of standard lp-norms inequalities
does not yield meaningful bounds on the lp-robustness inside the union of the l1- and l∞-ball,
since these bounds depend on the dimension of the input space of the network.
3
Published as a conference paper at ICLR 2020
Figure 1: Visualization of the 12-ball contained in the union resp. the convex hull of the
union of 11 - and l∞-balls in R3. First column: co-centric 11 -ball (blue) and l∞-ball (black).
Second: in red the largest l2-ball completely contained in the union of l1- and l∞-ball.
Third: in green the convex hull of the union of the l1- and l∞-ball. Fourth: the largest l2-
ball (red) contained in the convex hull. The l2-ball contained in the convex hull is significantly
larger than that contained in the union of l1- and l∞-ball.
3 Minimal lp-norm of the complement of the union of l1- and
l∞-ball and its convex hull
Let B 1 = {x ∈ Rd : Ilx∣∣1 ≤ e 1} and B∞ = {x ∈ Rd : ∣∣x∣∣∞ ≤ e∞} be the 11 -ball of radius
e 1 > 0 and the 1 ∞-ball of radius e∞ > 0 respectively, both centered at the origin in Rd. We
also assume e 1 ∈ (e∞, de∞), so that B 1 * B∞ and B∞ * B 1.
Suppose we can guarantee that the classifier does not change its label in U1,∞ = B1 ∪ B∞ .
Which guarantee does that imply for all intermediate 1p-norms? This question can be simply
answered by computing the minimal 1p-norms over Rd \ U1,∞, namely minX∈rd\U 1 ∞ ∣∣X∣∣p.
By the standard norm inequalities it holds, for every x ∈ Rd , that
1-p
IlXIP ≥ IX∣∣∞	and	IXIP ≥ IX∣1 d~p,
and thus a naive application of these inequalities yields the bound
x∈Rmin1 ^ ∞ i X i P ≥ max 卜∞，e 1d 1-p } ∙	⑷
However, this naive bound does not take into account that We know that ∣∣X∣∣1 ≥ e 1 and
Ix∣∞ ≥ e∞. Our first result yields the exact value taking advantage of this information.
Proposition 3.1 If d ≥ 2 and e 1 ∈ (e∞, de∞), then
x ∈Rm\U1, ∞ i X i P = (e∞ + (≡)∞-p ；	⑸
Thus a guarantee both for 11- and 1∞-ball yields a guarantee for all intermediate 1P-norms.
However, for affine classifiers a guarantee for B1 and B∞ implies a guarantee wrt the convex
hull C of their union B1 ∪ B∞ . This can be seen by the fact that an affine classifier generates
two half-spaces, and the convex hull of a set A is the intersection of all half-spaces containing
A. Thus, inside C the decision of the affine classifier cannot change if it is guaranteed not to
change in B1 and B∞, as C is completely contained in one of the half-spaces generated by
the classifier (see Figure 1 for illustrations of B1 , B∞, their union and their convex hull).
With the following theorem, we characterize, for any p ≥ 1, the minimal 1P-norm over Rd \ C.
Theorem 3.1 Let C be the convex hull of B 1 ∪ B∞. If d ≥ 2 and e 1 ∈ (e∞ ,de∞), then
min ∣ x L
X ∈R d \ C	P
__________eɪ_________
(e 1/c∞ — α + αq) /q
(6)
Where α =且——I d I and 1 + 1 = 1.
c ∞	c ∞	p q
4
Published as a conference paper at ICLR 2020
Figure 2: Comparison of the minimal l2-norm over Rd\C (6) (blue), Rd \ U1,∞ (5) (red) and
its naive lower bound (4) (green). We fix e∞ = 1 and show the results varying e 1 ∈(1 ,d), for
d = 784 and d = 3072. We plot the value (or a lower bound in case of (4)) of the minimal
Ilx∣∣2, depending on e 1, given by the different approaches (first and third plots). The red
curves are almost completely hidden by the green ones, as they mostly overlap, but can be
seen for small values of ∣∣x∣∣1. Moreover, we report (second and fourth plots) the ratios of
the minimal ∣∣X∣∣2 for Rd \ conv(B 1 ∪ B∞) and Rd \ (B 1 ∪ B∞). The values provided by (6)
are much larger than those of (5).
Note that our expression in Theorem 3.1 is exact and not just a lower bound. Moreover,
the minimal lp-distance of Rd \ C to the origin in Equation (6) is independent from the
dimension d, in contrast to the expression for the minimal lp-norm over Rd \ U1,∞ in (5) and
its naive lower bound in (4), which are both decreasing for increasing d and p > 1. In Figure
1 we compare visually the largest l2-balls (in red) fitting inside either U1,∞ or the convex hull
C in R3 , showing that the one in C is clearly larger. In Figure 2 we provide a quantitative
comparison in high dimensions. We plot the minimal l2-norm over Rd \ C (6) (blue) and
over Rd \ U 1,∞ (5) (red) and its naive lower bound (4) (green). We fix ∣∣x∣∣∞ = e∞ = 1 and
vary e 1 ∈ [1, d], with either d = 784 (left) or d = 3072 (right), i.e. the dimensions of the
input spaces of MNIST and CIFAR-10. One sees clearly that the blue line corresponding
to (6) is significantly higher than the other two. In the second and fourth plots of Figure 2
we show, for each e 1, the ratio of the 12-distances given by (6) and (5). The maximal ratio
is about 3.8 for d = 784 and 5.3 for d = 3072, meaning that the advantage of (6) increases
with d (for a more detailed analysis see A.3).
These two examples indicate that the lp-balls contained in C can be a few times larger than
those in U1,∞ . Recall that we deal with piecewise affine networks. If we could enlarge the
linear regions on which the classifier is affine so that it contains the l1- and l∞-ball of some
desired radii, we would automatically get the lp-balls of radii given by Theorem 3.1 to fit in
the linear regions. The next section formalizes the resulting robustness guarantees.
4 Universal provable robustness with respect to all lp-norms
Combining the results of Theorems 2.1 and 3.1, in the next theorem we derive lower bounds
on the robustness of a continuous piecewise affine classifier f, e.g. a ReLU network, at a
point x wrt any lp-norm with p ≥ 1 using only d1B (x), d1D (x), dB∞ (x) and dD∞ (x) (see (2)).
Theorem 4.1 Let dpB (x), dpD (x) be defined as in (2) and define ρ1 = min{d1B (x), |d1D (x)|}
and P∞ = min{d∞(X), |d∞(X)|}. If d ≥ 2 and X is correctly classified, then
rp(x) ≥
___________ρ1___________
(P 1∕p ∞ — α + αq )1/q
(7)
for any P ∈ (1, ∞), with α = P——[~ρpρ~ J and P + ɪ = 1.
Croce et al. (2019a) add a regularization term to the training objective in order to enlarge
the values of dpB (X) and dpD (X) for a fixed p, with X being the training points (note that they
optimize dpD(X) and not |dpD (X)| to encourage correct classification).
Sorting in increasing order dpB,l,j and dpD,s , (see (1)), that is the lp-distances to the hyperplanes
defining Q(X) and to decision hyperplanes, and denoting them as dpB,πB and dpD,πD respectively,
5
Published as a conference paper at ICLR 2020
the Maximum Margin Regularizer (MMR) of Croce et al. (2019a) is defined as
1 kB	dB B(X)	1 kD	dD D (X)
MMR-Ip(χ) =	kB	玄 max	(o, 1-	3TYB	) +	kD	M max	(o,1 -	"；D	).	⑻
i=1	i=1
It tries to push the kB closest hyperplanes defining Q(X) farther than γB from X and the
kD closest decision hyperplanes farther than γD from X both wrt the lp-metric. In other
words, MMR-lp aims at widening the linear regions around the training points so that they
contain lp-balls of radius either γB or γD centered in the training points. Using MMR-lp wrt
a fixed lp-norm, possibly in combination with the adversarial training of Madry et al. (2018),
leads to classifiers which are empirically resistant wrt lp-adversarial attacks and are easily
verifiable by state-of-the-art methods to provide lower bounds on the true robustness.
For our goal of simultaneous lp-robustness guarantees for all p ≥ 1, we use the insights
obtained from Theorem 4.1 to propose a combination of MMR-l1 and MMR-l∞ , called MMR-
Universal. It enhances implicitly robustness wrt every lp-norm without actually computing
and modifying separately all the distances dpB (X) and dpD (X) for the different values of p.
Definition 4.1 (MMR-Universal) Let x
be a training point. We define the regularizer
1 kB
MMR-Universal(x) = —	λ 1 max(0, 1 —
B i=1
d1B,π1B,i(x)
γ1
+ λ∞ max 0, 1 -
,πB (X)
, ∞,i
Y ∞	)
1	K-1
+ K-I 工 λιmax 仅，1 -
i=1
d1D,π1D,i(x)
γ1
+ λ∞ max 0, 1 -
,πD (X)
, ∞,i
Y ∞	×
(9)
where kB ∈ {1, . . . , N}, λ1 , λ∞, γ1, γ∞ > 0.
We stress that, even if the formulation of MMR-Universal is based on MMR-lp , it is just
thanks to the novel geometrical motivation provided by Theorem 3.1 and its interpretation
in terms of robustness guarantees of Theorem 4.1 that we have a theoretical justification
of MMR-Universal. Moreover, we are not aware of any other approach which can enforce
simultaneously l1- and l∞-guarantees, which is the key property of MMR-Universal.
The loss function which is minimized while training the classifier f is then, with {(xi , yi)}iT=1
being the training set and CE the cross-entropy loss,
1T
L ({(Xi,yi)}T=J = T £CE(f (Xi),yi) + MMR-Universal(Xi).
i=1
During the optimization our regularizer aims at pushing both the polytope boundaries and
the decision hyperplanes farther than γ1 in l1-distance and farther than γ∞ in l∞-distance
from the training point X, in order to achieve robustness close or better than γ1 and γ∞
respectively. According to Theorem 4.1, this enhances also the lp-robustness for p ∈ (1, ∞).
Note that if the projection of X on a decision hyperplane does not lie inside Q(X), dpD (X) is
just an approximation of the signed distance to the true decision surface, in which case Croce
et al. (2019a) argue that it is an approximation of the local Cross-Lipschitz constant which
is also associated to robustness (see Hein & Andriushchenko (2017)). The regularization
parameters λ1 and λ∞ are used to balance the weight of the l1- and l∞-term in the regularizer,
and also wrt the cross-entropy loss. Note that the terms of MMR-Universal involving the
quantities dD D (X) penalize misclassification, as they take negative values in this case.
p,πp,i
Moreover, we take into account the kB closest hyperplanes and not just the closest one as
done in Theorems 2.1 and 4.1. This has two reasons: first, in this way the regularizer enlarges
the size of the linear regions around the training points more quickly and effectively, given
the large number of hyperplanes defining each polytope. Second, pushing many hyperplanes
influences also the neighboring linear regions of Q(X). This comes into play when, in order
to get better bounds on the robustness at X, one wants to explore also a portion of the input
space outside of the linear region Q(X), which is where Theorem 4.1 holds. As noted in
6
Published as a conference paper at ICLR 2020
Raghunathan et al. (2018); Croce et al. (2019a); Xiao et al. (2019), established methods to
compute lower bounds on the robustness are loose or completely fail when using normally
trained models. In fact, their effectiveness is mostly related to how many ReLU units have
stable sign when perturbing the input x within a given lp-ball. This is almost equivalent
to having the hyperplanes far from x in lp-distance, which is what MMR-Universal tries
to accomplish. This explains why in Section 5 we can certify the models trained with
MMR-Universal with the methods of Wong & Kolter (2018) and Tjeng et al. (2019).
5	Experiments
We compare the models obtained via our MMR-Universal regularizer1 to state-of-the-art
methods for provable robustness and adversarial training. As evaluation criterion we use
the robust test error, defined as the largest classification error when every image of the
test set can be perturbed within a fixed set (e.g. an lp-ball of radius tp). We focus on the
lp -balls with p ∈ {1, 2, ∞}. Since computing the robust test error is in general an NP-hard
problem, we evaluate lower and upper bounds on it. The lower bound is the fraction of
points for which an attack can change the decision with perturbations in the lp-balls of
radius tp (adversarial samples), that is with lp-norm smaller than 5 For this task We use
the PGD-attack (Kurakin et al. (2017); Madry et al. (2018); Tramer & Boneh (2019)) and
the FAB-attack (Croce & Hein (2019)) for l1 , l2 and l∞, MIP (Tjeng et al. (2019)) for l∞
and the Linear Region Attack (Croce et al. (2019b)) for l2 and apply all of them (see C.3 for
details). The upper bound is the portion of test points for which we cannot certify, using the
methods of Tjeng et al. (2019) and Wong & Kolter (2018), that no lp-perturbation smaller
than 6p can change the correct class of the original input.
Smaller values of the upper bounds on the robust test error indicate models with better
provable robustness. While lower bounds give an empirical estimate of the true robustness,
it has been shown that they can heavily underestimate the vulnerability of classifiers (e.g.
by Athalye et al. (2018); Mosbach et al. (2018)).
5.1	Choice of 用
In choosing the values of tp for P ∈ {1, 2, ∞}, we try to be consistent with previous literature
(e.g. Wong & Kolter (2018); Croce et al. (2019a)) for the values of E∞ and E2. Equation (6)
provides, given e 1 and E∞, a value at which one can expect 12-robustness (approximately
E2 = √E1E∞). Then we fix e 1 such that this approximation is slightly larger than the desired
E2. We show in Table 1 the values chosen for Ep, P ∈ {1, 2, ∞}, and used to compute the
robust test error in Table 2. Notice that for these values no lp-ball is contained in the others.
Table 1: The values chosen for Ep on the different datasets and the expected 12-robustness
level (last column) given E1 and E∞, computed according to (6).
dataset	e 1	e ∞	e 2	e2 by (6)
MNIST / F-MNIST	1	01	0.3	0.3162
GTS	3	4/255	0.2	0.2170
CIFAR-10	2	2∕255	0.1	0.1252
Moreover, we compute for the plain models the percentage of adversarial examples given by
an 11-attack (we use the PGD-attack) with budget E1 which have also l∞-norm smaller than
or equal to E∞, and vice versa. These percentages are zero for all the datasets, meaning that
being (provably) robust in the union of these lp-balls is much more difficult than in just one
of them (see also C.1).
7
Published as a conference paper at ICLR 2020
Table 2: We report, for the different datasets and training schemes, the test error (TE) and
lower (LB) and upper (UB) bounds on the robust test error (in percentage) wrt the union of
lp -norms for p ∈ {1, 2, ∞} denoted as l1 + l2 + l∞ (that is the largest test error possible if
any perturbation in the union l1 + l2 + l∞ is allowed). The training schemes compared are
plain training, adversarial trainings of Madry et al. (2018); Tramer & Boneh (2019) (AT),
robust training of Wong & Kolter (2018); Wong et al. (2018) (KW), MMR regularization
of Croce et al. (2019a), MMR combined with AT (MMR+AT) and our MMR-Universal
regularization. The models of our MMR-Universal are the only ones which have non trivial
upper bounds on the robust test error for all datasets.
provable robustness against multiple perturbations						
model	TE	l1 + l2 + l∞			l1 + l2 + l∞	
		LB	UB	TE	LB	UB
plain	0.85	-88.5	~100	9.32	-100	~100
AT-l∞	0.82	4.7	100	11.54	26.3	100
AT-l2	0.87	25.9	100	8.10	98.8	100
AT-(l1,l2,l∞)	0.80	4.9	100	14.13	29.6	100
KW-l∞	1.21	4.8	100	21.73	43.6	100
KW-l2	1.11	10.3	100	13.08	66.7	86.8
MMR-l∞	1.65	10.4	100	14.51	36.7	100
MMR-l2	2.57	78.6	99.9	12.85	95.8	100
MMR+AT-l∞	1.19	4.1	100	14.52	31.8	100
MMR+AT-12	1.73	15.3	99.9	13.40	66.5	99.1
MMR-Universal	3.04	12.4	20.8	18.57	43.5	52.9
plain	6.77	-71.5	^^100	23.29	-88.6	^^100
AT-l∞	6.83	64.0	100	27.06	52.5	100
AT-l2	8.76	59.0	100	25.84	62.1	100
AT-(l1 , l2 , l∞)	8.80	45.2	100	35.41	57.1	100
KW-l∞	15.57	87.8	100	38.91	51.9	100
KW-l2	14.35	57.6	100	40.24	54.0	100
MMR-l∞	13.32	71.3	99.6	34.61	58.7	100
MMR-l2	14.21	62.6	80.9	40.93	72.9	98.0
MMR+AT-l∞	14.89	82.8	100	35.38	50.8	100
MMR+AT-12	15.34	58.1	84.8	37.78	61.3	99.9
MMR-Universal	15.98	51.6	52.4	46.96	63.8	64.6
5.2	Main results
We train CNNs on MNIST, Fashion-MNIST (Xiao et al. (2017)), German Traffic Sign (GTS)
(Stallkamp et al. (2012)) and CIFAR-10 (Krizhevsky et al. (2014)). We consider several
training schemes: plain training, the PGD-based adversarial training (AT) of Madry et al.
(2018) and its extension to multiple lp-balls in Tramer & Boneh (2019), the robust training
(KW) of Wong & Kolter (2018); Wong et al. (2018), the MMR-regularized training (MMR) of
Croce et al. (2019a), either alone or with adversarial training (MMR+AT) and the training
with our regularizer MMR-Universal. We use AT, KW, MMR and MMR+AT wrt l2 and l∞ ,
as these are the norms for which such methods have been used in the original papers. More
details about the architecture and models in C.3.
In Table 2 we report test error (TE) computed on the whole test set and lower (LB) and
upper (UB) bounds on the robust test error obtained considering the union of the three
lp -balls, indicated by l1 + l2 + l∞ (these statistics are on the first 1000 points of the test set).
The lower bounds l1 + l2 + l∞-LB are given by the fraction of test points for which one of
the adversarial attacks wrt l1, l2 and l∞ is successful. The upper bounds l1 + l2 + l∞-UB
are computed as the percentage of points for which at least one of the three lp-balls is not
certified to be free of adversarial examples (lower is better). This last one is the metric of
main interest, since we aim at universal ly provably robust models. In C.2 we report the lower
and upper bounds for the individual norms for every model.
1	Code available at https://github.com/fra31/mmr-universal.
8
Published as a conference paper at ICLR 2020
MMR-Universal is the only method which can give non-trivial upper bounds on the robust
test error for all datasets, while almost all other methods aiming at provable robustness
have l1 + l2 + l∞-UB close to or at 100%. Notably, on GTS the upper bound on the robust
test error of MMR-Universal is lower than the lower bound of all other methods except
AT-(l1 , l2, l∞), showing that MMR-Universal provably outperforms existing methods which
provide guarantees wrt individual lp-balls, either l2 or l∞, when certifying the union l1+l2+l∞.
The test error is slightly increased wrt the other methods giving provable robustness, but
the same holds true for combined adversarial training AT-(l1, l2, l∞) compared to standard
adversarial training AT-12/l∞. We conclude that MMR-Universal is the only method so far
being able to provide non-trivial robustness guarantees for multiple lp -balls in the case that
none of them contains any other.
6	Conclusion
With MMR-Universal we have proposed the first method providing provable robustness
guarantees for all lp-balls with p ≥ 1. Compared to existing works guaranteeing robustness
wrt either l2 or l∞ , providing guarantees wrt the union of different lp-balls turns out to be
considerably harder. It is an interesting open question if the ideas developed in this paper
can be integrated into other approaches towards provable robustness.
Acknowledgements
We would like to thank Maksym Andriushchenko for helping us to set up and adapt the
original code for MMR. We acknowledge support from the German Federal Ministry of
Education and Research (BMBF) through the Tubingen AI Center (FKZ: 01IS18039A).
This work was also supported by the DFG Cluster of Excellence “Machine Learning - New
Perspectives for Science”, EXC 2064/1, project number 390727645, and by DFG grant
389792660 as part of TRR 248.
References
R. Arora, A. Basuy, P. Mianjyz, and A. Mukherjee. Understanding deep neural networks
with rectified linear unit. In ICLR, 2018.
A. Athalye, N. Carlini, and D. A. Wagner. Obfuscated gradients give a false sense of security:
Circumventing defenses to adversarial examples. In ICML, 2018.
O. Bastani, Y. Ioannou, L. Lampropoulos, D. Vytiniotis, A. Nori, and A. Criminisi. Measuring
neural net robustness with constraints. In NIPS, 2016.
T. B. Brown, D. Mane, A. Roy, M. Abadi, and J. Gilmer. Adversarial patch. In NIPS 2017
Workshop on Machine Learning and Computer Security, 2017.
N. Carlini and D. Wagner. Adversarial examples are not easily detected: Bypassing ten
detection methods. In ACM Workshop on Artificial Intel ligence and Security, 2017.
F. Croce and M. Hein. A randomized gradient-free attack on relu networks. In GCPR, 2018.
F. Croce and M. Hein. Minimally distorted adversarial examples with a fast adaptive
boundary attack. preprint, arXiv:1907.02044, 2019.
F. Croce, M. Andriushchenko, and M. Hein. Provable robustness of relu networks via
maximization of linear regions. In AISTATS, 2019a.
F. Croce, J. Rauber, and M. Hein. Scaling up the randomized gradient-free adversarial attack
reveals overestimation of robustness using established attacks. International Journal of
Computer Vision, 1-19, 2019b.
L. Engstrom, B. Tran, D. Tsipras, L. Schmidt, and A. Madry. A rotation and a translation
suffice: Fooling CNNs with simple transformations. In NIPS 2017 Workshop on Machine
Learning and Computer Security, 2017.
9
Published as a conference paper at ICLR 2020
R.	Geirhos, P. Rubisch, C.Michaelis, M. Bethge, F. A. Wichmann, and W. Brendel. Imagenet-
trained cnns are biased towards texture; increasing shape bias improves accuracy and
robustness. In ICLR, 2019.
I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples.
In ICLR, 2015.
S.	Gowal, K. Dvijotham, R. Stanforth, R. Bunel, C. Qin, J. Uesato, R. Arandjelovic, T. A.
Mann, and P. Kohli. On the effectiveness of interval bound propagation for training
verifiably robust models. preprint, arXiv:1810.12715v3, 2018.
S.	Gu and L. Rigazio. Towards deep neural network architectures robust to adversarial
examples. In ICLR Workshop, 2015.
M. Hein and M. Andriushchenko. Formal guarantees on the robustness of a classifier against
adversarial manipulation. In NIPS, 2017.
D. Hendrycks and T. Dietterich. Benchmarking neural network robustness to common
corruptions and perturbations. In ICLR, 2019.
R. Huang, B. Xu, D. Schuurmans, and C. Szepesvari. Learning with a strong adversary. In
ICLR, 2016.
D. Kang, Y. Sun, T. Brown, D. Hendrycks, and J. Steinhardt. Transfer of adversarial
robustness between perturbation types. preprint, arXiv:1905.01034, 2019.
G. Katz, C. Barrett, D. Dill, K. Julian, and M. Kochenderfer. Reluplex: An efficient smt
solver for verifying deep neural networks. In CAV, 2017.
D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. preprint,
arXiv:1412.6980, 2014.
A. Krizhevsky, V. Nair, and G. Hinton. Cifar-10 (canadian institute for advanced research).
2014. URL http://www.cs.toronto.edu/~kriz/cifar.html.
A. Kurakin, I. J. Goodfellow, and S. Bengio. Adversarial examples in the physical world. In
ICLR Workshop, 2017.
A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Valdu. Towards deep learning models
resistant to adversarial attacks. In ICLR, 2018.
M. Mirman, T. Gehr, and M. Vechev. Differentiable abstract interpretation for provably
robust neural networks. In ICML, 2018.
M.	Mosbach, M. Andriushchenko, T. Trost, M. Hein, and D. Klakow. Logit pairing methods
can fool gradient-based attacks. In NeurIPS 2018 Workshop on Security in Machine
Learning, 2018.
N.	Papernot, P. McDonald, X. Wu, S. Jha, and A. Swami. Distillation as a defense to
adversarial perturbations against deep networks. In IEEE Symposium on Security &
Privacy, 2016.
A. Raghunathan, J. Steinhardt, and P. Liang. Certified defenses against adversarial examples.
In ICLR, 2018.
L. Schott, J. Rauber, M. Bethge, and W. Brendel. Towards the first adversarially robust
neural network model on MNIST. In ICLR, 2019.
Y. Sharma and P. Chen. Attacking the madry defense model with l1 -based adversarial
examples. In ICLR Workshop, 2019.
J. Stallkamp, M. Schlipsing, J. Salmen, and C. Igel. Man vs. computer: Benchmarking
machine learning algorithms for traffic sign recognition. Neural Networks, 32:323-332,
2012.
10
Published as a conference paper at ICLR 2020
C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus.
Intriguing properties of neural networks. In ICLR, pp. 2503-2511, 2014.
V. Tjeng, K. Xiao, and R. Tedrake. Evaluating robustness of neural networks with mixed
integer programming. In ICLR, 2019.
F. Tramer and D. Boneh. Adversarial training and robustness for multiple perturbations. In
NeurIPS, 2019.
E. Wong and J. Zico Kolter. Provable defenses against adversarial examples via the convex
outer adversarial polytope. In ICML, 2018.
E. Wong, F. Schmidt, J. H. Metzen, and J. Z. Kolter. Scaling provable adversarial defenses.
In NeurIPS, 2018.
H. Xiao, K. Rasul, and R. Vollgraf. Fashion-MNIST: a novel image dataset for benchmarking
machine learning algorithms. preprint, arXiv:1708.07747, 2017.
K. Y. Xiao, V. Tjeng, N. M. Shafiullah, and A. Madry. Training for faster adversarial
robustness verification via inducing relu stability. In ICLR, 2019.
S. Zheng, Y. Song, T. Leung, and I. J. Goodfellow. Improving the robustness of deep neural
networks via stability training. In CVPR, 2016.
11
Published as a conference paper at ICLR 2020
A Minimal lp-norm of the complement of the union of l1- and
l∞-ball and its convex hull
A.1 Proof of Proposition 3.1
Proof. We first note that for C1 < C∞ it holds B1 ⊂ B∞ and the proof follows from the
standard inequality ∣∣x∣∣P ≥ ∣∣x∣∣∞ where equality is attained for x = C∞e%, where ei are
standard basis vectors. Moreover, if C1 > de∞ it holds B∞ ⊂ B1 as max方∈B∞ ∣∣x∣∣ 1 = de∞
1 — P
and the result follows by ∣∣x∣∣P ≥ ∣∣x∣∣ 1 d~p~. The equality is realized by the vector with all
the entries equal to 号.
For the second case we first note that using Holder inequality |〈u, V)| ≤ ∣∣UIlP Ilv∣∣q where
-+ - = 1, it holds
Pq ,
k	k	1
工 |Xi| ≤ (工 |Xi|P)Pkq.
i=1	i=1
Let x ∈ Rd. Without loss of generality after a potential permutation of the coordinates it
holds |xd| = Ilx∣∣∞. Then we get
d	d -1	(Ed-11 Xi Λ
IlXIlP =工|Xi|p = |xd|p + 工|Xi|p ≥ |xd|p +、
i=1	i=1	(d - i)q
We have
min | Xd | p + (E i=1 । x D
IlXll∞≥e∞,∣∣X∣∣1 ≥e 1	(d — 1) q
noting that |Xd| = ∣∣x∣∣∞ and Ed-1 |x/ ≥ e 1 — C∞.
Finally, we note that the vector
(c 1 - c∞)p
(d — 1)PT ,
d-1
Σc 1 — c ∞ l
-e	：-ei + c ∞ ed,
d—1
i=1
realizes equality. Indeed, ∣∣v∣∣P = (d — 1)(7d-e∞P——+ c∞, which finishes the proof.	□
A.2 Proof of Theorem 3.1
Proof. We first note that the minimum of the lP-norm over Rd \ C lies on the boundary of C
(otherwise any point on the segment joining the origin and y and outside C would have
lP -norm smaller than y). Moreover, the faces of C are contained in hyperplanes constructed
as the affine hull of a subset of d points from the union of the vertices of B1 and B∞ .
The vertices of B1 are V1 = {C1 e%, — C1 e% | i = 1,..., d}, where e% is the i-th element of the
standard basis of Rd, and that of B∞ are V∞ , consisting of the 2d vectors whose components
are elements of {C∞, — C∞}. Note that V1 ∩ V∞ = 0. Any subset of d vertices from V1 ∪ V∞
defines a hyperplane which contains a face of C if it does not contain any point of the
interior of C .
Let S be a set of vertices defining a hyperplane containing a face
derive conditions on the vertices contained in S . Let k
of C . We first
=f1-----k ∈ [0, 1).
U GO

∈ N and α
Note that k + 1 > —. Then no more than k vertices of B1 belong to S, that is to a face of
u go
C. In fact, if we consider k + 1 vertices of B1, namely wlog {C1 e 1,..., C1 ek +1}, and consider
their convex combination Z = C1 E31 k+1 ei then ∣∣z∣∞ = k+1 < C∞ by the definition of k.
Thus S cannot contain more than k vertices of B1 .
Second, assume C1 ej is in S. If any vertex v of B∞ with vj = - C∞ is also in S
then, with α/ = U∞ ∈ (0, 1), we get
,	U1 十 U∞
Ila/c 1 ej + (1 — aL)v∣∣∞ = max{∣α/c 1 — (1 — aL)C∞|, (1 — αL)C∞},
12
Published as a conference paper at ICLR 2020
where (1 — α/) E∞ < E∞ and
|α'Eι — (1 — az)E∞| = |αz(eι + E∞) — E∞| = 0 < E∞.
Thus S would not span a face as a convex combination intersects the interior of C . This
implies that if Eιej is in S then all the vertices V of B∞ in S need to have Vj = E∞, otherwise
S would not define a face of C. Analogously, if — E ι ej ∈ S then any vertex V of B∞ in S has
Vj = — E∞. However, we note that out of symmetry reasons we can just consider faces of
C in the positive orthant and thus we consider in the following just sets S which contain
vertices of “positive type” eiej.
Let now S be a set (not necessarily defining a face of C) containing h ≤ k vertices
of B1 and d — h vertices of B∞ and P the matrix whose columns are these points. The
matrix P has the form
(E ι	0 ... 0 e ∞
0	e ι ... 0 e∞
...
P =	0	. . .	0 E1 E ∞
^^0^^TΞ	0
...
0	...	0
e ∞、
e ∞
e ∞
A
/
where A ∈ Rd-h,d-h is a matrix whose entries are either E∞ or — E∞. If the matrix P does
not have full rank then the origin belongs to any hyperplane containing S, which means it
cannot be a face of C . This also implies A has full rank if S spans a face of C .
We denote by π the hyperplane generated by the affine hull of S (the columns of
P) assuming that A has full rank. Every point b belonging to the hyperplane π generated
by S is such that there exists a unique a ∈ Rd which satisfies
	/ 1	...	1 ∖	
Pl a = ( IPd ) a =	^71 ^^0^^TΞ^^0~ 0	E1	... 0 ... 0	...	0	E1	e ∞	.	.	.	e	∞ e ∞	.	.	.	e	∞ e ∞	.	.	.	e	∞	a=( b)=b b
	^^0^^TΞ	0^ ... 0	...	0	A /	
where 1d1,d2 is the matrix of size d1 × d2 whose entries are 1.
The matrix (P', bz) ∈ Rd +1 ,d +1 need not have full rank, so that
rank P P = rank( P P, br) = dim a = d
and then the linear system P/a = b/ has a unique solution.
We define the vector V ∈ Rd as solution of PTV = 1d,1, which is unique as P has full rank.
From their definitions we have Pa = b and 1Ta = 1, so that
1 = 1Ta = (PTV)Ta = VTPa = VTb,
and thus
b VV = 1,	(IO)
noticing that this also implies that any vector b ∈ Rd such that (b, VV = 1 belongs to π
(suppose that ∃q ∈ π with (q, vV = 1, then define C as the solution of Pc = q and then
1 = (q, V V = (Pc, V V = ccP P T V)= (c, 1)which contradicts that q ∈ π).
Applying Holder inequality to (10) we get for any b ∈ π,
网 P ≥K⅛,	(U)
where P + ɪ = 1. Moreover, as P ∈ (1, ∞) there exists always a point b* for which (11) holds
as equality.
13
Published as a conference paper at ICLR 2020
In the rest of the proof We compute Ilv∣∣q for any q > 1 when S is a face of C and then (11)
yields the desired minimal value of IIb∣∣P over all b lying in faces of C.
Let v = (v1, v2), v1 ∈ Rh, v2 ∈ Rd-h and Ih denotes the identity matrix of Rh,h.
It holds
PTv =(	∖11h	Pt	V ν 1 = = ( 11 h,1 = = 1 d,ι,
1t∞ 1 d—h,h A J ∖ v2 J ∖ 1 d-h, 1 J
which implies
v 1=a,..., η
V1 C1)
and
AT v 2 = 1 d—h, 1 -	∞ 1 d—h, 1
c 1
(1-h⅞)1 ….
Moreover, we have
Iv∣1 = Iv 1∣1+1v211 = h +1v2∣1, Iv∣∞ = maxa,Iv2∣∞}.	(12)
If S generates a face, then by definition π does not intersect with the interior of C and thus
it holds for all b ∈ π: ∣ b ∣∣ 1 ≥ c 1 and ∣∣ b ∣∣∞ ≥ C ∞. Suppose ∣∣ v ∣∣ 1 = c> ；. Then there exists
b* ∈ π such equality in Holder,s equality is realized, that is 1 = (b*, v)= ∣∣b*∣∞ ∣v∣1, and
thus Ilb*∣∞ = C < C∞, which contradicts ∣∣b∣∣∞ ≥ C∞ for all b ∈ π and thus it must hold
Ilv11 ≤ ；. Similarly, one can derive ∣∣v∣∞ ≤ 六. Combining (12) with the just derived
inequalities we get upper bounds on the norms of v2,
1h	1
1vM ≤ C∞ - C1 and 1v21∞ ≤ C1.
(13)
Furthermore v2 is defined as the solution of
ATv 2 =(j) 1 d-h,1.
c ∞	∖ c ∞	c 1 )
We note that all the entries of AT are either 1 or — 1, so that the inner product between
each row of AT and v2 is a lower bound on the l1-norm of v2 . Since every entry of the r.h.s.
of the linear system is ɪɪ -hh we get ∣∣v2∣1 ≥ ^ɪ -hh, which combined with (13) leads to
∣ v 2∣1
1 h
e∞ - 1
This implies that ATv2 = ∣∣v2%. In order to achieve equality (u, v) = ∣∣v∣∣1 it has
to hold ui = sgn(vi) for every vi = 0. If at least two components of v were non-zero, the
corresponding columns of AT would be identical, which contradicts the fact that AT has full
rank. Thus v2 can only have one non-zero component which in absolute value is equal to
ɪɪ h Thus, after a potential reordering of the components, v has the form
/ ∖
1	11
c 1	c 1 c ∞
V~V~'
h times
—h, 0,..., 0
/
From the second condition in (13), we have —------ ≤ — and h + 1 ≥ U = k + α. Recalling
e ∞	€ 1 e 1	& ∞
h ≤ k, we have
h ∈ [k + α — 1 ,k] ∩ N.
This means that, in order for S to define a face of C, we need h = k if α > 0, h ∈ {k — 1, k}
if α = 0 (in this case choosing h = k — 1 or h = k leads to the same v, so in practice it is
possible to use simply h = k for any α).
Once we have determined v, we can use again (10) and (11) to see that
1
网 p ≥ ∣⅛
(寺 + (ɪ T )q)1 =(呼 ∞
c 1
—α + αq )1/q
(14)
14
Published as a conference paper at ICLR 2020
Finally, for any V there exists b* ∈ π for which equality is achieved in (14). Suppose that this
b* does not lie in a face of C. Then one could just consider the line segment from the origin
to b* and the point intersecting the boundary of C would have smaller lp-norm contradicting
the just derived inequality. Thus the b* realizing equality in (14) lies in a face of C.
A.3 Comparison of the robustness guarantee for the union of B1 and B∞ in
(5) and the convex hull of B1 and B∞ in (6)
We compare the robustness guarantees obtained by considering the union of B1 and B∞
(denoted by bU in the following), see (5), and the convex hull of B1 and B∞ (denoted by bC
in the following), see (6). In particular, We want to determine the ratio δ = U ∈ [1 ,d] for
which the gain in the robustness guarantee bC for the convex hull is maximized compared to
just considering the robustness guarantee bU as a function of the dimension d of the input
space. We restrict here the analysis to the case of p = 2, that is computing the radius of the
largest l2-ball fitting inside U1,∞ or its convex hull C. Let us denote
1
bu =	min	IlxIl	=	fe∞	+	∙^1-:∞1 ] ,	be	= min	IlxIl	=-----------^1--------τ-
χ∈Rd\Uι,∞	k (d - I)PTJ	χ∈Rd\C	(ei/e∞ — α + αq) /q
the two bounds from (5) and (6) respectively, which can be rewritten as
bU (δ) = C∞ (1 + (d -11P-1 Y , be (δ)
E∞δ	c∙1
------------T	〜C ∞ δ p ,
(δ — α + αq) q
where α = δ — [_δJ. We note that
δ — 1 ≤ δ — α + αq = [δJ +(δ — [δJ)q ≤ δ.
As the differences are very small, we use instead the lower bound
b e (S) = (C∞δ =C ∞δ 1.
We want to find the value δ* which maximizes bC (δ) varying d (a numerical evaluation is
presented in Figure 2). Notice first that δ* maximizes also
= δ	「＞ 1
(bU (δ)) P	I +(d - 1) P TJ	≥
and can be found as the solution of
∂ (bC(δ))p	∂
- :---:~~— ———— l
∂δ (bU(δ))P	∂δ
δJJ Γ
=「(S - 1)Pδ「(S — 1)P「p(S — 1)PT
=V + (d — 1) P-1J	V + (d — 1) P-1J	(d — 1) PT
which is equivalent to
(d — 1) p T + (δ — 1) p — pδ (δ — 1)PT = 0.
bC
bU
≥生
δ* bU
δ*
Y2〜送
√2.
d 4 (
Restricting the analysis to p = 2 for simplicity, we get
d — 1 + (δ — 1)2 — 2 δ (δ — 1) = — δ2 + d = 0 =⇒ δ * = √d,
and one can check that δ* is indeed a maximizer. Moreover, at δ* we have a ratio between
the two bounds	1
1 . (√d — 1)2
+ (d ― 1)
We observe that the improvement of the robustness guarantee by considering the convex
hull instead of the union is increasing with dimension and is ≈ 3.8 for d = 784 and ≈ 5.3 for
d = 3072. Thus in high dimensions there is a considerable gain by considering the convex
hull.
15
Published as a conference paper at ICLR 2020
B Universal provable robustness with respect to all lp-norms
B.1 Proof of Theorem 4.1
Proof. From the definition of dpB (x) and dpD (x) we know that none of the hyperplanes {πj }j
(either boundaries of the polytope Q(x) or decision hyperplanes) identified by V (l) and v(l) ,
l = 1, . . . , L + 1, is closer than min{dpB (x), |dpD (x)|} in lp-distance. Therefore the interior
of the l1-ball of radius ρ1 (namely, B1 (x, ρ1)) and of the l∞ -ball of radius ρ∞ (B∞(x, ρ∞))
centered in x does not intersect with any of those hyperplanes. This implies that {πj }j are
intersecting the closure of Rd \ conv(B1 (x, ρ1) ∪ B∞(x, ρ∞)). Then, from Theorem 3.1 we
get
min{dB(x),ldD(x)l}≥ (P 1ρ∞ -； + αq产∙
Finally, exploiting Theorem 2.1, rp (x) ≥ min{dpB (x), |dpD (x)|} holds.

C Experiments
C.l Choice of 用
In Table 3 we compute the percentage of adversarial perturbations given by the PGD-attack
Wrt lp with budget Ep which have lq-norm smaller than j, for q = P (the values of tp and j
used are those from Table 1). We used the plain model of each dataset.
The most relevant statistics of Table 3 are about the relation between the l1- and l∞-
perturbations (first two rows). In fact, none of the adversarial examples wrt l1 is contained
in the l∞-ball, and vice versa. This means that, although the volume of the l1 -ball is much
smaller, even because of the intersection with the box constraints [0, 1]d, than that of the
l∞ -ball in high dimension, and most of it is actually contained in the l∞ -ball, the adversarial
examples found by l1-attacks are anyway very different from those got by l∞-attacks. The
choice of such Ep is then meaningful, as the adversarial perturbations we are trying to prevent
wrt the various norms are non-overlapping and in practice exploit regions of the input space
significantly diverse one from another.
Moreover, one can see that also the adversarial manipulations wrt l1 and l2 do not overlap.
Regarding the case of l2 and l∞ , for MNIST and F-MNIST it happens that the adversarial
examples wrt l2 are contained in the l∞-ball. However, as one observes in Table 4, being
able to certify the l∞-ball is not sufficient to get non-trivial guarantees wrt l2 . In fact, all
the models trained on these datasets to be provably robust wrt the l∞-norm, that is KW-l∞,
MMR-l∞ and MMR+AT-l∞ , have upper bounds on the robust test error in the l2-ball larger
than 99%, despite the values of the lower bounds are small (which means that the attacks
could not find adversarial perturbations for many points).
Such analysis confirms that empirical and provable robustness are two distinct problems,
and the interaction of different kinds of perturbations, as we have, changes according to
which of these two scenarios one considers.
Table 3: Percentage of l1-adversarial examples contained in the l∞-ball and vice versa.
	MNIST	F-MNIST	GTS	CIFAR-10
lI-Perturbations with l∞-norm ≤ e∞ (%)	00	0.0	0.0	0.0
l∞-perturbations with l ι-norm ≤ e 1 (%)	0.0	0.0	0.0	0.0
lI-Perturbations with 12-norm ≤ e2 (%)	45	7.4	0.0	0.0
12-perturbations with 11-norm ≤ e 1 (%)	0.0	0.0	0.0	0.0
12-perturbations with 1 ∞-norm ≤ e∞ (%)	100.0~~	100.0	0.3	16.0
1 ∞-perturbations with 12-norm ≤ e2 (%)	0.0	0.0	0.0	0.0
16
Published as a conference paper at ICLR 2020
C.2 Main results
In Table 4 we report, for each dataset, the test error and upper and lower bounds on the
robust test error, together with the Ep used, for each norm individually. It is clear that
training for provable lp-robustness (expressed by the upper bounds) does not, in general,
yield provable lq -robustness for q = p, even in the case where the lower bounds are small for
both p and q.
In order to compute the upper bounds on the robust test error in Tables 2 and 4 we use the
method of Wong & Kolter (2018) for all the three lp-norms and that of Tjeng et al. (2019)
only for the l∞-norm. This second one exploits a reformulation of the problem in (3) in
terms of mixed integer programming (MIP), which is able to exactly compute the solution of
(3) for p ∈ {1, 2, ∞}. However, such technique is strongly limited by its high computational
cost. The only reason why it is possible to use it in practice is the exploitation of some
presolvers which are able to reduce the complexity of the MIP. Unfortunately, such presolvers
are effective just wrt l∞ . On the other hand, the method of Wong & Kolter (2018) applies
directly to every lp-norm. This explains why the bounds provided for l∞ are tighter than
those for l1 and l2 .
C.3 Experimental details
The convolutional architecture that we use is identical to Wong & Kolter (2018), which
consists of two convolutional layers with 16 and 32 filters of size 4 × 4 and stride 2, followed
by a fully connected layer with 100 hidden units. The AT-l∞ , AT-l2, KW, MMR and
MMR+AT training models are those presented in Croce et al. (2019a) and available at https:
//github.com/max- andr/provable- robustness- max- linear- regions. We trained the
AT-(l1, l2, l∞) performing for each batch of the 128 images the PGD-attack wrt the three
norms (40 steps for MNIST and F-MNIST, 10 steps for GTS and CIFAR-10) and then
training on the point realizing the maximal loss (the cross-entropy function is used), for
100 epochs. For all experiments with MMR-Universal we use batch size 128 and we train
the models for 100 epochs. Moreover, we use Adam optimizer of Kingma & Ba (2014) with
learning rate of 5 × 10-4 for MNIST and F-MNIST, 0.001 for the other datasets. We also
reduce the learning rate by a factor of 10 for the last 10 epochs. On CIFAR-10 dataset we
apply random crops and random mirroring of the images as data augmentation.
For training we use MMR-Universal as in (9) with kB linearly (wrt the epoch) decreasing
from 20% to 5% of the total number of hidden units of the network architecture. We also
use a training schedule for λp where we linearly increase it from λp/10 to λp during the first
10 epochs. We employ both schemes since they increase the stability of training with MMR.
In order to determine the best set of hyperparameters λ1, λ∞, γ1 , and γ∞ of MMR, we
perform a grid search over them for every dataset. In particular, we empirically found that
the optimal values of Yp are usually between 1 and 2 times the Ep used for the evaluation of
the robust test error, while the values of λp are more diverse across the different datasets.
Specifically, for the models we reported in Table 4 the following values for the (λ1 , λ∞) have
been used: (3.0, 12.0) for MNIST, (3.0, 40.0) for F-MNIST, (3.0, 12.0) for GTS and (1.0,
6.0) for CIFAR-10.
In Tables 2 and 4, while the test error which is computed on the full test set, the statistics
regarding upper and lower bounds on the robust test error are computed on the first 1000
points of the respective test sets. For the lower bounds we use the FAB-attack with the
original parameters, 100 iterations and 10 restarts. For PGD we use also 100 iterations
and 10 restarts: the directions for the update step are the sign of the gradient for l∞ , the
normalized gradient for 12 and the normalized sparse gradient suggested by Tramer & Boneh
(2019) with sparsity level 1% for MNIST and F-MNIST, 10% for GTS and CIFAR-10. Finally
we use the Liner Region Attack as in the original code. For MIP (Tjeng et al. (2019)) we
use a timeout of 120s, that means if no guarantee is obtained by that time, the algorithm
stops verifying that point.
17
Published as a conference paper at ICLR 2020
Table 4: We report, for the different datasets and training schemes, the test error (TE) and
lower (LB) and upper (UB) bounds on the robust test error (in percentage) wrt the lp-norms
at thresholds 与,with P = 1, 2, ∞ (that is the largest test error possible if any perturbation
of lp-norm equal to tp is allowed). Moreover We show the 11 + 12 + l∞-UB, that is the upper
bound on the robust error when the attacker is allowed to use the union of the three lp-balls.
The training schemes compared are plain training, adversarial training of Madry et al. (2018);
Tramer & Boneh (2019) (AT), robust training of Wong & Kolter (2018); Wong et al. (2018)
(KW), MMR regularization of Croce et al. (2019a), MMR combined with AT (MMR+AT)
and our MMR-Universal regularization. One can clearly see that our MMR-Universal models
are the only ones which have non trivial upper bounds on the robust test error wrt all the
considered norms.
provable robustness against multiple perturbations
l1	l2	l∞	l1 + l2 + l∞
model	TE	LB	UB	LB	UB	LB	UB	LB	UB
MNIST		e 1 二	二 1	e 2 =	0.3	e ∞ =	二 0.1		
plain	-0.85	2.3	~~100-	3T-	100	88.5	~~100	88.5	^^100
AT-l∞	0.82	1.8	100	1.7	100	4.7	100	4.7	100
AT-l2	0.87	2.1	100	2.2	100	25.9	100	25.9	100
AT-(l1,l2,l∞)	0.80	2.1	100	1.7	100	4.9	100	4.9	100
KW-l∞	1.21	3.6	100	2.8	100	4.4	4.4	4.8	100
KW-l2	1.11	2.4	100	2.3	6.6	10.3	10.3	10.3	100
MMR-l∞	1.65	10.0	100	5.2	100	6.0	6.0	10.4	100
MMR-l2	2.57	4.5	62.3	6.7	14.3	78.6	99.9	78.6	99.9
MMR+AT-l∞	1.19	3.6	100	2.4	100	3.6	3.6	4.1	100
MMR+AT-12	1.73	3.6	99.9	3.7	12.1	15.3	76.8	15.3	99.9
MMR-Universal	3.04	6.4	20.8	6.2	10.4	12.4	12.4	12.4	20.8
F-MNIST		e 1 二	二 1	e 2 =	0.3	e ∞ =	二 0.1		
plain	-9.32	-31.3	~~100-	65.8	100	100	~~100	100	~100
AT-l∞	11.54	19.0	100	17.1	100	25.4	73.0	26.3	100
AT-l2	8.10	15.9	100	20.6	100	98.8	100	98.8	100
AT-(l1,l2,l∞)	14.13	22.2	100	20.3	100	28.3	98.6	29.6	100
KW-l∞	21.73	42.7	100	30.5	99.2	32.4	32.4	43.6	100
KW-l2	13.08	15.8	19.8	15.9	19.9	66.7	86.8	66.7	86.8
MMR-l∞	14.51	28.5	100	23.5	100	33.2	33.6	36.7	100
MMR-l2	12.85	18.2	39.4	24.8	33.2	95.8	100	95.8	100
MMR+AT-l∞	14.52	27.3	100	22.9	100	27.5	30.7	31.8	100
MMR+AT-12	13.40	17.2	55.4	20.2	37.8	66.5	99.1	66.5	99.1
MMR-Universal	18.57	25.0	52.4	24.3	37.4	43.5	44.3	43.5	52.9
GTS		e 1 二	二 3	e 2 =	0.2	e ∞ = 4/255			
plain	-6.77	60.5	~~100-	38.4	^^99.3	71.1	98.4	71.5	~100
AT-1∞	6.83	64.0	100	24.9	99.2	31.7	82.3	64.0	100
AT-12	8.76	44.0	100	27.2	98.4	58.9	97.1	59.0	100
AT-(11,12,1∞)	8.80	41.8	100	24.0	93.7	41.2	79.4	45.2	100
KW-1∞	15.57	87.8	100	41.1	77.7	36.1	36.6	87.8	100
KW-12	14.35	46.5	100	30.8	35.3	57.0	63.0	57.6	100
MMR-1∞	13.32	71.3	99.6	40.9	41.7	49.5	49.6	71.3	99.6
MMR-12	14.21	54.6	80.4	36.3	36.6	62.3	63.6	62.6	80.9
MMR+AT-1∞	14.89	82.8	100	39.9	44.7	38.3	38.4	82.8	100
MMR+AT-12	15.34	49.4	84.3	33.2	33.8	57.2	60.2	58.1	84.8
MMR-Universal	15.98	49.7	51.5	34.3	34.6	47.0	47.0	51.6	52.4
CIFAR-10		e 1 二	二 2	e 2 =	0.1	e ∞ 一	2/255		
plain	23.29	-61.0	~~100-	48.9	100	88.6	~~100	88.6	~100
AT-1∞	27.06	39.6	100	33.3	99.2	52.5	88.5	52.5	100
AT-12	25.84	41.9	100	35.3	99.9	62.1	99.4	62.1	100
AT-(11,12,1∞)	35.41	47.7	100	41.7	88.2	57.0	76.8	57.1	100
KW-1∞	38.91	51.9	100	39.9	66.1	46.6	48.0	51.9	100
KW-12	40.24	47.3	100	44.6	49.3	53.6	54.7	54.0	100
MMR-1∞	34.61	54.1	100	42.3	68.4	57.7	61.0	58.7	100
MMR-12	40.93	58.9	98.0	50.4	56.3	72.9	86.1	72.9	98.0
MMR+AT-1∞	35.38	50.6	100	1481.2	84.7	48.7	54.2	50.8	100
MMR+AT-12	37.78	50.4	99.9	46.1	54.2	61.3	74.1	61.3	99.9
MMR-Universal	46.96	56.4	63.4	51.9	53.6	63.8	63.8	63.8	64.6
Published as a conference paper at ICLR 2020
Figure 3: We show, for each dataset, the evolution of the test error (red), upper bound (UB)
on the robust test error wrt l1 (black), l2 (cyan) and l∞ (blue) during training. Moreover, we
report in green the upper bounds on the test error when the attacker is allowed to exploit the
union of the three lp-balls. The statistics on the robustness are computed at epoch 1, 2, 5, 10
and then every 10 epochs on 1000 points with the method of Wong & Kolter (2018), using
the models trained with MMR-Universal.
C.4 Evolution of robustness during training
We show in Figure 3 the clean test error (red) and the upper bounds on the robust test error
wrt l1 (black), l2 (cyan), l∞ (blue) and wrt the union of the three lp-balls (green), evaluated
at epoch 1, 2, 5, 10 and then every 10 epochs (for each model we train for 100 epochs) for
the models trained with our regularizer MMR-Universal. For each dataset used in Section 5
the test error is computed on the whole test set, while the upper bound on the robust test
error is evaluated on the first 1000 points of the test set using the method introduced in
Wong & Kolter (2018) (the thresholds e 1, e2, e∞ are those provided in Table 1). Note that
the statistics wrt l∞ are not evaluated additionally with the MIP formulation of Tjeng et al.
(2019) as the results in the main paper which would improve the upper bounds wrt l∞.
For all the datasets the test error keeps decreasing across epochs. The values of all the upper
bounds generally improve during training, showing the effectiveness of MMR-Universal.
C.5 Other combinations of MMR and AT
We here report the robustness obtained training with MMR-lp +AT-lq with p = q on MNIST.
This means that MMR is used wrt lp, while adversarial training wrt lq . In particular we test
p, q ∈ {1, ∞}. In Table 5 we report the test error (TE), lower (LB) and upper bounds (UB)
on the robust test error for such model, evaluated wrt l1 , l2 , l∞ and l1 + l2 + l∞ as done
in Section 5. It is clear that training with MMR wrt a single norm does not suffice to get
provable guarantees in all the other norms, despite the addition of adversarial training. In
fact, for both the models analysed the UB equals 100% for at least one norm. Note that
the statistics wrt l∞ in the plots do not include the results of the MIP formulation of Tjeng
et al. (2019).
19
Published as a conference paper at ICLR 2020
Table 5: Robustness of other combinations of MMR and AT.
model	TE	l1		l2		l∞ LB	UB	l1 + l2 + LB	l∞ UB
		LB	UB	LB	UB				
MMR-l1+AT-l∞	0.99	2.5	15.7	26~~	25.4	9.4	100	9.4	100
MMR-l∞+AT-l1	1.43	2.7	100	2.3	100	5.6	30.2	5.6	100
C.6 Larger models
We trained models with MMR-Universal also on the "Large" architecture from Wong et al.
(2018), but we could not achieve a significant improvement compared to the smaller networks
reported in the main paper. Note that the verification becomes the more expensive the larger
the network is. Thus for the results in Table 6 we use only the method of Wong & Kolter
(2018) to compute the UB on the robust test error (we do not additionally use Tjeng et al.
(2019) for the statistics relative to the l∞-robustness which yields tighter upper bounds),
which explains why the results are seemingly worse than those in Table 2.
Table 6: MMR-Universal models trained on the "Large" architecture from Wong et al. (2018)
with the same tp as in the experiments in Section 5. * For the l∞-robustness in this case the
method of Tjeng et al. (2019) is not used.
MMR-Universal models with larger architecture
model	TE	l1		l2		l∞		11 + 12 +	1∞ UB
		LB	UB	LB	UB	LB	UB	LBr	
MNIST	2.20	4.6	11.2	42^^	6.9	8.8	48.8*	88	48.8
F-MNIST	19.20	29.8	47.6	25.5	34.2	43.6	64.4*	43.7	64.5
GTS	17.40	45.8	54.8	32.1	36.9	47.5	58.3*	49.8	60.1
CIFAR-10	45.09	48.6	63.3	46.0	48.9	57.1	69.6*	57.1	69.9
20