Published as a conference paper at ICLR 2020
Economy Statistical Recurrent Units For
Inferring Nonlinear Granger Causality
Saurabh Khanna	Vincent Y. F. Tan
Department of Electrical and Computer Engineering Department of Electrical and Computer Engineering
National University of Singapore	Department of Mathematics
elesaur@nus.edu.sg	National University of Singapore
vtan@nus.edu.sg
Ab stract
Granger causality is a widely-used criterion for analyzing interactions in large-
scale networks. As most physical interactions are inherently nonlinear, we con-
sider the problem of inferring the existence of pairwise Granger causality between
nonlinearly interacting stochastic processes from their time series measurements.
Our proposed approach relies on modeling the embedded nonlinearities in the
measurements using a component-wise time series prediction model based on
Statistical Recurrent Units (SRUs). We make a case that the network topology
of Granger causal relations is directly inferrable from a structured sparse estimate
of the internal parameters of the SRU networks trained to predict the processes’
time series measurements. We propose a variant of SRU, called economy-SRU,
which, by design has considerably fewer trainable parameters, and therefore less
prone to overfitting. The economy-SRU computes a low-dimensional sketch of its
high-dimensional hidden state in the form of random projections to generate the
feedback for its recurrent processing. Additionally, the internal weight parameters
of the economy-SRU are strategically regularized in a group-wise manner to fa-
cilitate the proposed network in extracting meaningful predictive features that are
highly time-localized to mimic real-world causal events. Extensive experiments
are carried out to demonstrate that the proposed economy-SRU based time series
prediction model outperforms the MLP, LSTM and attention-gated CNN-based
time series models considered previously for inferring Granger causality.
1	Introduction
The physical mechanisms behind the functioning of any large-scale system can be understood in
terms of the networked interactions between the underlying system processes. Granger causality
is one widely-accepted criterion used in building network models of interactions between large en-
sembles of stochastic processes. While Granger causality may not necessarily imply true causality,
it has proven effective in qualifying pairwise interactions between stochastic processes in a variety
of system identification problems, e.g., gene regulatory network mapping (Fujita et al. (2007)), and
the mapping of human brain connectome (Seth et al. (2015)). This perspective has given rise to
the canonical problem of inferring pairwise Granger causal relationships between a set of stochastic
processes from their time series measurements. At present, the vast majority of Granger causal in-
ference methods adopt a model-based inference approach whereby the measured time series data is
modeled using with a suitable parameterized data generative model whose inferred parameters ul-
timately reveal the true topology of pairwise Granger causal relationships. Such methods typically
rely on using linear regression models for inference. However, as illustrated in the classical bivariate
example by Baek & Brock (1992), linear model-based Granger causality tests can fail catastrophi-
cally in the presence of even mild nonlinearities in the measurements, thus making a strong case for
our work which tackles the nonlinearities in the measurements by exploring new generative models
of the time series measurements based on recurrent neural networks.
1
Published as a conference paper at ICLR 2020
2	Problem formulation
Consider a multivariate dynamical system whose evolution from an initial state is fully characterized
by n distinct stochastic processes which can potentially interact nonlinearly among themselves. Our
goal here is to unravel the unknown nonlinear system dynamics by mapping the entire network of
pairwise interactions between the system-defining stochastic processes, using Granger causality as
the qualifier of the individual pairwise interactions.
In order to detect the pairwise Granger causal relations between the stochastic processes, we as-
sume access to their concurrent, uniformly-sampled measurements presented as an n-variate time
series x = {xt : t ∈ N} ⊂ Rn. Let xt,i denote the ith component of the n-dimensional vector mea-
surement xt , representing the measured value of process i at time t. Motivated by the framework
proposed in Tank et al. (2017), we assume that the measurement samples xt , t ∈ N are generated
sequentially according to the following nonlinear, component-wise autoregressive model:
xt,i = fi (xt-p:t-1,1 , xt-p:t-1,2, . . . , xt-p:t-1,n) + et,i, i = 1, 2, . . . n,	(1)
where xt-p:t-1,j , {xt-1,j, xt-2,j, . . . , xt-p,j} represents the most recent p measurements of the
jth component of x in the immediate past relative to current time t. The scalar-valued component
generative function fi captures all of the linear and nonlinear interactions between the n stochastic
processes up to time t - 1 that decide the measured value of the ith stochastic process at time t. The
residual ei,t encapsulates the combined effect of all instantaneous and exogenous factors influenc-
ing the measurement of process i at time t, as well as any imperfections in the presumed model.
Equation 1 may be viewed as a generalization of the linear vector autoregressive (VAR) model in
the sense that the components of x can be nonlinearly dependent on one another across time. The
value p is loosely interpreted to be the order of the above nonlinear autoregressive model.
2.1	Granger causality in nonlinear dynamical systems
We now proceed to interpret Granger causality in the context of the above component-wise time
series model. Recalling the standard definition by Granger (1969), a time series v is said to Granger
cause another time series u if the past of v contains new information above and beyond the past
of u that can improve the predictions of current or future values of u. For x with its n components
generated according to equation 1, the concept of Granger causality can be extended as suggested by
Tank et al. (2018) as follows. We say that series j does not Granger cause series i if the component-
wise generative function fi does not depend on the past measurements in series j, i.e., for all t ≥ 1
and all distinct pairs xt-p:t-1,j and x0t-p:t-1,j,
fi (xt-p:t-1,1 , . . . , xt-p:t-1,j , . . . , xt-p:t-1,n ) = fi xt-p:t-1,1 , . . . , xt-p:t-1,j , . . . , xt-p:t-1,n .
(2)
From equation 1, it is immediately evident that under the constraint in equation 2, the past of series j
does not assert any causal influence on series i, in alignment with the core principle behind Granger
causality. Based on the above implication of equation 2, the detection of Granger noncausality
between the components of x translates to identifying those components ofx whose past is irrelevant
to the functional description of each individual fi featured in equation 1.
Note that any reliable inference of pairwise Granger causality between the components of x is fea-
sible only if there are no unobserved confounding factors in the system which could potentially
influence x. In this work, we assume that the system of interest is causally sufficient (Spirtes &
Zhang (2016)), i.e., none of the n stochastic processes (whose measurements are available) have a
common Granger-causing-ancestor that is unobserved.
2.2	Inferring Granger causality using component-wise recurrent models
We undertake a model-based inference approach wherein the time series measurements are used
as observations to learn an autoregressive model which is anatomically similar to the component-
wise generative model described in equation 1 except for the unknown functions fi replaced with
their respective parameterized approximations denoted by gi. Let Θi , 1 ≤ i ≤ n denote the com-
plete set of parameters encoding the functional description of the approximating functions {gi}in=1.
Then, the pairwise Granger causality between series i and the components of x is deduced from Θi
which is estimated by fitting gi ’s output to the ordered measurements in series i. Specifically, if
the estimated Θi suggests that gi’s output is independent of the past measurements in series j, then
2
Published as a conference paper at ICLR 2020
we declare that series j is Granger noncausal for series i. We aim to design the approximation
function gi to be highly expressive and capable of well-approximating any intricate causal coupling
between the components of x induced by the component-wise function fi , while simultaneously
being easily identifiable from underdetermined measurements.
By virtue of their universal approximation property (Schafer & Zimmermann (2006)), recurrent neu-
ral networks or RNNs are a particularly ideal choice for gi towards inferring the pairwise Granger
causal relationships in x. In this work, we investigate the use of a special type of RNN called
the statistical recurrent unit (SRU) for inferring pairwise Granger causality between multiple non-
linearly interacting stochastic processes. Introduced by Oliva et al. (2017), an SRU is a highly
expressive recurrent neural network designed specifically for modeling multivariate time series data
with complex-nonlinear dependencies spanning multiple time lags. Unlike the popular gated RNNs
(e.g., long short-term memory (LSTM) (Hochreiter & Schmidhuber (1997)) and gated recurrent
unit (GRU)) (Chung et al. (2014)), the SRU’s design is completely devoid of the highly nonlinear
sigmoid gating functions and thus less affected by the vanishing/exploding gradient issue during
training. Despite its simpler ungated architecture, an SRU can model both short and long-term tem-
poral dependencies in a multivariate time series. It does so by maintaining multi-time scale summary
statistics of the time series data in the past, which are preferentially sensitive to different older por-
tions of the time series x. By taking appropriate linear combinations of the summary statistics at
different time scales, an SRU is able to construct predictive causal features which can be both highly
component-specific and lag-specific at the same time. From the causal inference perspective, this
dual-specificity of the SRU’s predictive features is its most desirable feature, as one would argue
that causal effects in reality also tend to be highly localized in both space and time.
The main contributions of this paper can be summarized as follows:
1.	We propose the use of statistical recurrent units (SRUs) for detecting pairwise Granger
causality between the nonlinearly interacting stochastic processes. We show that the entire
network of pairwise Granger causal relationships can be inferred directly from the regu-
larized block-sparse estimate of the input-layer weight parameters of the SRUs trained to
predict the time series measurements of the individual processes.
2.	We propose a modified SRU architecture called economy SRU or eSRU in short. The first of
the two proposed modifications is aimed at substantially reducing the number of trainable
parameters in the standard SRU model without sacrificing its expressiveness. The second
modification entails regularizing the SRU’s internal weight parameters to enhance the inter-
pretability of its learned predictive features. Compared to the standard SRU, the proposed
eSRU model is considerably less likely to overfit the time series measurements.
3.	We conduct extensive numerical experiments to demonstrate that eSRU is a compelling
model for inferring pairwise Granger causality. The proposed model is found to outperform
the multi-layer perceptron (MLP), LSTM and attention-gated convolutional neural network
(AG-CNN) based models considered in the earlier works.
3	Proposed Granger causal inference framework
In the proposed scheme, each of the unknown generative functions fi , 1 ≤ i ≤ n in the presumed
component-wise model of x in (1) is individually approximated by a distinct SRU network. The
ith SRU network sequentially processes the time series measurements x and outputs a next-step
prediction sequence X+ = {Xi,2, Xi,3,..., ^i,t+ι,...} ⊂ R, where ^i,t+ι denotes the predicted
value of component series i at time t + 1. The prediction ^i,t+ι is computed in a recurrent fashion
by combining the current input sample xt at time t with the summary statistics of past samples of x
up to and including time t - 1 as illustrated in Figure 1.
The following update equations describe the sequential processing of the input time series x within
the ith SRU network in order to generate a prediction of xi,t+1.
Feedback: ri,t = hWr(i)ui,t-1 +b(ri)	∈Rdr.	(3a)
Recurrent statistics: φi,t = h Wi(ni)xt + Wf(i)ri,t-1 + bi(ni)	∈ Rdφ .	(3b)
3
Published as a conference paper at ICLR 2020
Multi-scale summary statistics: ui,t = [(u：； )T (u：2 )T …(Uam )T] ∈ Rmdφ, αj ∈ A, ∀j.
(3c)
Single-scale summary statistics: uia,jt = (1	- αj )uia,tj-1 + αj φi,t, ∈ Rdφ	, αj	∈	[0,	1].	(3d)
Output features: oi,t = h	Wo(i)ui,t + b(oi)	∈ Rdo .	(3e)
Output prediction: ^i,t+ι = (w[") θi,t + % ∈ R.	(3f)
The function	h in the above updates is the	elementwise Rectified Linear	Unit (ReLU) operator,
h(∙) := max(∙, 0), which serves as the nonlinear activation in the three dedicated single layer
neural networks that generate the recurrent statistics φi,t, the feedback ri,t and the output fea-
tures oi,t in the ith SRU network. In order to generate the next-step prediction of series i at
time t, the ith SRU network first prepares the feedback ri,t by nonlinearly transforming its last
hidden state ui,t-1. As stated in equation 3a, a single layer ReLU network parameterized by
weight matrix Wr(i) and bias br(i) maps the hidden state ui,t-1 to the feedback ri,t. Another
single layer ReLU network parameterized by weight matrices Wi(ni), Wf(i) and bias bi(ni) takes the
input xt and the feedback ri,t and tranforms them into the recurrent statistics φi,t as described
in equation 3b. Equation 3d describes how the network’s multi-timescale hidden states uia,t for
α ∈ A = {α1, α2, . . . , αm} ⊂ [0, 1] are updated in parallel by taking exponentially weighted
moving averages of the recurrent statistics φi,t corresponding to m different scales in A. A third
single layer ReLU network parameterized by Wo(i) and b(oi) transforms the concatenated multi-
timescale summary statistics ui,t = (uia,t1 )T (uia,t2)T . . . (uia,mm)TT to generate the nonlinear causal
features oi,t which, according to Oliva et al. (2017), are arguably highly sensitive to the input time
series measurements at specific lags. Finally, the network generates the next-step prediction of se-
ries i as Xi,t+ι by linearly combining the nonlinear output features in Oi,t, as depicted in equation 3f.
xt
一 COnCatenate 一
φi,t
Wi(ni), Wf(i), b(oi)
」Ham (Z)
ri,t
ReLU 士
Wr(i), br(i)
ui,t-1
T HaI(Z)
T Hα2 (Z)
ZT
Concatenate
θi,t	> Linear
xi,t+1
Figure 1:
The ith SRU
approximat-
ing fi in
equation 1.
Ha(Z)= 1-(1 I
For values of scale α ≈ 1, the single-scale summary statistic uia,t in equation 3d is more sensitive
to the recent past measurements in x. On the other hand, α ≈ 0 yields a summary statistic that is
more representative of the older portions of the input time series. Oliva et al. (2017) elaborates on
how the SRU is able to generate output features (Oi,t , 1 ≤ i ≤ n) that are preferentially sensitive to
the measurements from specific past segments of x by taking appropriate linear combinations of the
summary statistics corresponding to different values of α in A.
3.1	Inferring pairwise Granger causality using SRUs
Let Θ(SiR)U , {Wf(i), Wi(ni), bi(ni), Wr(i), b(ri), Wo(i), b(oi), wy(i), b(yi)} denote the complete set of param-
eters of the ith SRU network approximating fi in the presumed component-wise model of x. From
equation 3b, we observe that the weight matrix Wi(ni) regulates the influence of the individual com-
ponents of the input time series x on the generation of the recurrent statistics φi,t, and ultimately
the next-step prediction of series i. In real-world dynamical systems, the networked interactions are
typically sparse which implies that very few dimensions of the input time series x actually play a
role in the generation of its individual components. Bearing this property of the networked interac-
tions in mind, we are interested in learning the parameters Θ(SiR)U such that the ith SRU’s sequential
4
Published as a conference paper at ICLR 2020
output closely matches with series i’s measurements, while simultaneously seeking a column-sparse
estimate of the weight matrix Wi(ni) .
We propose to learn the parameters Θ(SiR)U of the ith SRU network by minimizing the penalized mean
squared prediction error loss as shown below.
1	T-1	n
θSRu := arg min Tɪɪ X (Xi,t - χi,t+ι)2 + λι X kW『(:,j)k2.	(4)
ΘS(iR)U T - 1 t=1	j=1
In the above, the network output ^i,t depends nonlinearly on Wini) according to the composite
relation described by the updates (3a)-(3f) and Wi(ni) (:, j) denotes the jth column in the weight
matriχ Wi(ni) . The `1 -group norm penalty in the objective is known to promote column sparsity in
(i)
the estimated Win (Simon et al. (2013)). From equation 3b, a straightforward implication of the
column vector Wi(ni) (:, j ) being estimated as the all-zeros vector is that the past measurements in
series j do not influence the predicted future value of series i. In this case, we declare that series
j does not Granger-cause series i. Moreover, the indeχ set supporting the non-zero columns in the
estimated weight matrix Wini) enumerates the components of X which are likely to Granger-cause
series i. Likewise, the entire network of pairwise Granger causal relationships in x can be deduced
from the non-zero column support of the estimated weight matrices Wi(ni), 1 ≤ i ≤ n in the n SRU
networks trained to predict the components of X.
The component-wise SRU optimization problem in equation 4 is nonconvex and potentially has mul-
tiple local minima. To solve for ΘSRu，We use first-order gradient-based methods such as stochastic
gradient descent which have been found to be consistently successful in finding good solutions of
nonconvex deep neural network optimization problems (Allen-Zhu et al. (2019)). Since our ap-
proach of detecting Granger noncausality hinges upon correctly identifying the all-zero columns
of Wi(ni) , it is important that the first-order gradient based parameter updates used for minimizing
the penalized SRU loss ensure that majority of the coefficients in Wi(ni) iterates become exactly
zero after a certain number of iterations. Seeking exact column sparsity in the converged solution
of Wi(ni), we follow the same approach as Tank et al. (2018) and resort to a first-order proximal gra-
dient descent algorithm to find a regularized solution of the SRU optimization. The gradients needed
for executing the gradient descent updates of the SRU network parameters are computed efficiently
using the backpropagation through time (BPTT) procedure (Jaeger (2002)).
4	Economy SRU: A remedy for overfitting
By computing the summary statistics of past measurements at sufficiently granular time scales,
an SRU can learn predictive causal features which are highly localized in time. While a higher
granularity of α in A translates to a more general SRU model that fits better to the time series
measurements, it also entails substantial increase in the number of trainable parameters. Since
measurement scarcity is typical in causal inference problems, the proposed component-wise SRU
based time series prediction model is usually overparameterized and thus susceptible to overfitting.
The typical high dimensionality of the recurrent statistic φt accentuates this issue.
To alleviate the overfitting concerns, we propose two modifications to the standard SRU (Oliva et al.
(2017)) aimed primarily at reducing its likelihood of overfitting the time series measurements. The
modifications are relevant regardless of the current Granger causal inference context, and henceforth
we refer to the modified SRU as Economy-SRU (eSRU).
4.1	Modification-I: Generating feedback from a low-dimensional sketch of
SUMMARY STATISTICS
We propose to reduce the number of trainable parameters in the ith SRU network by substitut-
ing the feedback ReLU network parameterized by Wr(i) and b(ri) with the two stage network
shown in Fig. 2. The first stage implements the linear matrix-vector multiplication operation
Dr(i)ui,t to generate the output vi,t ∈ Rd0r, where Dr(i) ∈ Rd0r×mdφ is a fixed, full row-rank
5
Published as a conference paper at ICLR 2020
Stage-1	Stage-2
(Encoder)	(Decoder)
Figure 2: Proposed two-stage feedback in
economy-SRU.
matrix with d0r	mdφ . The d0r-dimensional output of the first stage can be viewed as a low-
dimensional, stable embedding of the multi-timescale summary statistics ui,t. The entries of the
constant matrix Dr* (i) are drawn independently from a zero mean Gaussian distribution with vari-
ance d. The stage-1 processing is based on the premise that for most real-world systems and
the associated time series measurements, their high-
dimensional summary statistics learned by the SRU
network as ui,t tend to be highly structured, and
thus ui,t has significantly fewer degrees of free-
dom relative to its ambient dimension. Thus, by
projecting the mdφ-dimensional ui,t onto the d0r (
mdφ) rows of Dr(i), we obtain its low-dimensional
embedding vi,t which nonetheless retains most of
the contextual information conveyed by the un-
compressed ui,t1 Johnson & Lindenstrauss (1984);
Dirksen (2014). The second stage of the proposed
feedback network is a single/multi-layer ReLU net-
work which maps the sketched summary statis-
tics vi,t to the feedback vector ri,t . The second stage ReLU network is parameterized by weight
matrix Wr0,(i) ∈ Rdr×d0r and bias b0r,(i) ∈ Rdr. Compared to the standard SRU’s feedback whose
generation is controlled by mdφdr + dr trainable parameters, the proposed feedback network has
only d0rdr + dr trainable parameters, which is substantially fewer when d0r	mdφ . Consequently,
the modified SRU is less susceptible to overfitting.
4.2	Modification-II: Grouped-sparse mixing of multi-scale summary statistics
for learning time-localized predictive features
In the standard SRU proposed by Oliva et al. (2017), there are no restrictions on the weight ma-
trix Wo(i) parameterizing the ReLU network that maps the summary statistics ui,t to the final predic-
tive features in oi,t. Noting that the number of parameters in the mdφ × do sized weight matrix Wo(i)
usually dominates the overall number of trainable parameters in the SRU, any meaningful effort to-
wards addressing the model overfitting concerns must consider regularizing the weights in Wo(i) .
In this spirit, we propose the following penalized optimization problem to estimate the parameters
θ∖SRu = (θSRU∖{Wr(i)}) ∪ {Wr0(i)} of the eSRU model equipped with the two-stage feedback
proposed in Section 4.1:
T-1	n	do dφ
θLsRu = argminT-1 X (Xi,t — xi,t+ι)2 + λl X kWi",j )k2 + λ2 XX k Wo⑴(j, Gj,k )k2.
Θe(SiR)U	t=1	j=1	j=1 k=1
(5)
Here λ1 and λ2 are positive constants that bias the group sparse penalizations against the eSRU’s fit
to the measurements in the ith component series. The term Woi(j, Gj,k) (1 ≤ j ≤ do, 1 ≤ k ≤ dφ)
denotes the subvector of the jth row in Wo(i) obtained by extracting the weight coefficients indexed
by set Gj,k. As shown via an example in Fig. 3, the index set Gj,k enumerates the m weight
coefficients in the row vector Wo(i)(j, :) which are multiplied to the exponentially weighted running
averages of kth recurrent statistic φi,t(k) corresponding to the m timescales in A prior to being
transformed by the neural unit generating the jth predictive feature in oi,t . Compared to equation 4,
the second penalty term in equation 5 promotes a group-sparse solution for Wo(i) to the effect that
each predictive feature in oi,t depends on only a few components of the recurrent statistic φi,t via
their linearly mixed multi-scale exponentially weighted averages. We opine that the learned linear
mixtures, represented by the intermediate products Wo(i) (j, Gj,k)ui,t(Gk,j), are highly sensitive to
certain past segments of the input time series x. Consequently, the output features in oi,t are both
time-localized and component-specific, a common trait of real-world causal effects.
1Gaussian random matrices of appropriate dimensions are approximately isometries with overwhelming
probability (Johnson & Lindenstrauss (1984)). However, instead of using n independent instantiations of a
(i)
Gaussian random matrix for initializing Dr , 1 ≤ i ≤ n, we recommend initializing them with the same
random matrix, as the latter strategy reduces the probability that any one of them is spurious encoder by n-fold.
6
Published as a conference paper at ICLR 2020
Figure 3: An illustration of the proposed group-wise mixing of the multi-timescale summary statis-
tics ui,t in the ith SRU (with dφ = 5) towards generating thejth predictive feature in oi,t. The weights
corresponding to the same colored connections belong to the same group.
The above group-sparse regularization of the weight coefficients in Wo(i), combined with the
(i)
column-sparsity of Win , is pivotal to enforcing that the occurrence of any future pattern in a time
series can be attributed to the past occurrences of a few highly time-localized patterns in the an-
cestral time series. The results of our numerical experiments further confirm that by choosing λ1
(i)
and λ2 appropriately, the proposed group-wise sparsity inducing regularization of Wo ameliorates
overfitting, and the optimization in equation 5 yields an estimate of Wi(ni) whose column support
closely reflects the true pairwise Granger causal relationships between the components of x.
5 Experiments
We evaluate the performance of the proposed SRU- and eSRU-based component-wise time series
models in inferring pairwise Granger causal relationships in a multivariate time series. The proposed
models are compared to the existing MLP- and LSTM-based models in Tank et al. (2018) and the
attention-gated CNN-based model (referred hereafter as Temporal Causal Discovery Framework
(TCDF)) in Nauta et al. (2019). To ensure parity between the competing models, the maximum
size of all the input/hidden/output layers in the different NN/RNN time series models is fixed to 10,
unless specified otherwise. The complete list of tuned hyperparameters of the considered models
used for different datasets is provided in Appendix G. The performance of each method is qualified
in terms of its AUROC (Area Under the Receiver Operating Characteristic curve). Here, the ROC
curve illustrates the trade off between the true-positive rate (TPR) and the false-positive rate (FPR)
achieved by the methods towards the detection of n2 pairwise Granger causal relationships between
the n measured processes in the experiment. The ROC curves of SRU and eSRU models are obtained
by sweeping through different values of the regularization parameter λ1 in equation 4 and equation 5,
respectively. Likewise, the ROCs of component-wise MLP and LSTM models are obtained by
varying λι's counterpart in Tank et al. (20l8). For TCDF, the ROC curve is obtained by varying the
threshold that is applied to attention scores of the trained AG-CNN model in Nauta et al. (2019).
5.1	Lorenz-96 simulations
In the first set of experiments, the time series measurements x intended for Granger causal inference
are generated according to the Lorenz-96 model which has been extensively used in climate science
for modeling and prediction purposes (Schneider et al. (2017)). In the Lorenz-96 model of an n-
variable system, the individual state trajectories of the n variables are governed by the following set
of odinary differential equations:
∂xt,i
合, = -xt,i-1 (xt,i-2 - xt,i+1) - xt,i + F,	1 ≤ i ≤ n∙
(6)
where the first and the second terms on the RHS represent the advection and the diffusion in the
system, respectively, and the third term F is the magnitude of the external forcing. The system
dynamics becomes increasingly chaotic for higher values of F (Karimi & Paul (2010)). We evaluate
and compare the accuracy of the proposed methods in inferring pairwise Granger causal relation-
ships between n = 10 variables with Lorenz-96 dynamics. We consider two settings: F = 10 and
F = 40 in order to simulate two different strengths of nonlinearity in the causal interactions between
7
Published as a conference paper at ICLR 2020
Table 1: Averaged AUROC for 5 independent Lorenz-96 datasets
(a)F = 10	(b) F =40
MODEL	AVERAGE AUROC		MODEL	AVERAGE AUROC	
	T = 250	T=500		T = 250	T= 500
MLP	0.93 ± 0.02	0.96 ± 0.03	MLP	0.85 ± 0.08	0.94 ± 0.03
LSTM	0.90 ± 0.02	0.95 ± 0.05	LSTM	0.78 ± 0.09	0.90 ± 0.05
TCDF	0.70 ± 0.01	0.72 ± 0.04	TCDF	0.62 ± 0.01	0.68 ± 0.04
SRU	0.84 ± 0.03	0.90 ± 0.02	SRU	1.0 ± 0.0	1.0 ± 0.0
eSRU	0.95 ± 0.02	0.98 ± 0.01	eSRU	0.99 ± 0.0	1.0 ± 0.0
the variables. Here, the ground truth is straightforward i.e., for any 1 ≤ i ≤ n, the ith component of
time series x is Granger caused by its components with time indices from i - 2 to i + 1.
In the case of weak nonlinear interactions (F = 10), from Table 1a, we observe that eSRU achieves
the highest AUROC among all competing models. The gap in performance is more pronounced
when fewer time series measurements (T = 250) are available. In case of stronger nonlinear inter-
actions (F = 40), we observe that both SRU and eSRU are the only models that are able to perfectly
recover the true Granger causal network (Table 1b). Surprisingly, the SRU and eSRU models per-
form poorer when F is small. This could be attributed to the proposed models not sufficiently
regularized when fitted to weakly-interacting time series measurements that are less nonlinear.
5.2	VAR simulations
In the second set of simulations, we consider the time series measurements x to be generated ac-
cording to a 3rd order linear VAR model:
xt = A(1)xt-1 + A(2)xt-2 + A(3)xt-3 + wt, t ≥ 1,	(7)
where the matrices A(i) , i = 1, 2, 3 contain the regression coefficients which model the linear in-
teractions between its n = 10 components. The noise term wt is Gaussian distributed with zero
mean and covariance 0.01I. We consider a sparse network of Granger causal interactions with
only 30% of the regression coefficients in Ai selected uniformly being non-zero and the regression
matrices Ai being collectively joint sparse (same setup as in Bolstad et al. (2011)). All non-zero
regression coefficients are set equal to 0.0994 which guarantees the stability of the simulated VAR
process.
From Table 2, we observe that all time series models generally achieve a higher AUROC as the
number of measurements available increases. For T = 500, the component-wise MLP and the
proposed eSRU are statistically tied when comparing their average AUROCs. For T = 1000, eSRU
significantly outperforms the rest of the time series models and is able to recover the true Granger
causal network almost perfectly.
Table 2: Averaged AUROC for 5 indepen- dently generated VAR datasets MODEL	AVERAGE AUROC			Table 3: Average AUROC corresponding to in- ferred brain connectivity for 5 human subjects	
			MODEL	AVERAGE AUROC
	T = 500	T= 1000		T = 200
MLP	0.94 ± 0.03	0.93 ± 0.02	MLP	0.81 ± 0.04
LSTM	0.79 ± 0.12	0.8 ± 0.09	LSTM	0.70 ± 0.03
TCDF	0.77 ± 0.07	0.78 ± 0.04	TCDF	0.75 ± 0.04
SRU	0.82 ± 0.06	0.91 ± 0.04	SRU	0.78 ± 0.02
eSRU	0.93 ± 0.05	0.98 ± 0.01	eSRU	0.84 ± 0.03
5.3 In Silico estimation of brain connectivity using BOLD signals
In the third set of experiments, we apply the different learning methods to estimate the con-
nections in the human brain from simulated blood oxygenation level dependent (BOLD) imag-
ing data. Here, the individual components of x comprise T = 200 time-ordered samples of
8
Published as a conference paper at ICLR 2020
the BOLD signals simulated for n = 15 different brain regions of interest (ROIs) in a human
subject. To conduct the experiments, we use simulated BOLD time series measurements corre-
sponding to the five different human subjects (labelled as 2 to 6) in the Sim-3.mat file shared at
https://www.fmrib.ox.ac.uk/datasets/netsim/index.html. The generation of
the Sim3 dataset is described in Smith et al. (2011). The goal here is to detect the directed connec-
tivity between different brain ROIs in the form of pairwise Granger causal relationships between the
components of x.
From Table 3, it is evident that eSRU is more robust to overfitting compared to the standard SRU and
detects the true Granger causal relationships more reliably. Interestingly, a single-layer cMLP model
is found to outperform more complex cLSTM and attention gated-CNN (TCDF) models; however
we expect the latter models to perform better when more time series measurements are available.
5.4 Dream-3 in silico network inference challenge
In the final set of experiments, we evaluate the performance of the different time series models in
inferring gene regulation networks synthesized for the DREAM-3 In Silico Network Challenge (Prill
et al. (2010); Marbach et al. (2009)). Here, the time series x represents the in silico measurements of
the gene expression levels ofn = 100 genes, available for estimating the gene regulatory networks of
E.coli and yeast. A total of five gene regulation networks are to be inferred (two for E.coli and three
for yeast) from the networks’ gene expression level trajectories recorded while they recover from 46
different perturbations (each trajectory has 21 time points). All NN/RNN models are implemented
with 10 neurons per layer, except for the componentwise MLP model which has 5 neurons per layer.
From Table 4, we can observe that the proposed SRU and eSRU models are generally more accurate
Table 4: AUROCs for the inferred gene regulatory networks
MODEL	AUROC
	E.coli-1	E.coli-2	Yeast-1	Yeast-2	Yeast-3
MLP	0.644	0.568	0.585	0.506	0.528
LSTM	0.629	0.609	0.579	0.519	0.555
TCDF	0.614	0.647	0.581	0.556	0.557
SRU	0.657	0.666	0.617	0.575	0.55
eSRU	0.66	0.629	0.627	0.557	0.55
compared to the MLP, LSTM, and attention-gated CNN (TCDF) models in inferring the true gene-
gene interactions. For four out of the five gene regulatory networks, either SRU or eSRU was the
best performing model among the competing ones.
6 Conclusion
In this work, we addressed the problem of inferring pairwise Granger causal relationships between
stochastic processes that interact nonlinearly. We showed that the such causality between the pro-
cesses can be robustly inferred from the regularized internal parameters of the proposed eSRU-based
recurrent models trained to predict the time series measurements of the individal processes. Future
work includes:
i Investigating the use of other loss functions besides the mean-square error loss which can capture
the exogenous and instantaneous causal effects in a more realistic way.
ii	Incorporating unobserved confounding variables/processes in recurrent models.
iii	Inferring Granger causality from multi-rate time series measurements.
Acknowledgements
This work is supported by a Singapore Ministry of Education (MOE) Tier 2 Grant (R-263-000-C83-
112).
9
Published as a conference paper at ICLR 2020
References
Zahra Abbasvandi and Ali Motie Nasrabadi. A self-organized recurrent neural network for esti-
mating the effective connectivity and its application to EEG data. Computers in Biology and
Medicine,110:93 - 107, 2019.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In Proceedings of the 36th International Conference on Machine Learning,
volume 97, pp. 242-252, Jun 2019.
E. Baek and W. Brock. A general test for Granger causality: Bivariate model. In Technical Report.
Korean Development Institute and University of Wisconsin-Madison, 1992.
M. T. Bahadori and Y. Liu. An examination of practical Granger causality inference. In Proc. SIAM
Int. Conf. Data Min., pp. 467-475, 2013.
Zhidong Bai, Wing-Keung Wong, and Bingzhi Zhang. Multivariate linear and nonlinear causality
tests. Mathematics and Computers in Simulation, 81(1):5-17, September 2010.
A. Bolstad, B. D. Van Veen, and R. Nowak. Causal network inference via group sparse regulariza-
tion. IEEE Transactions on Signal Processing, 59(6):2628-2641, June 2011.
Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of
gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.
Cees Diks and Valentyn Panchenko. A new statistic and practical guidelines for nonparametric
Granger causality testing. Journal of Economic Dynamics and Control, 30(9):1647 - 1669, 2006.
Cees Diks and Marcin Wolski. Nonlinear Granger causality: Guidelines for multivariate analysis.
Journal of Applied Econometrics, 31(7):1333-1351, 2016.
Sjoerd Dirksen. Dimensionality reduction with subgaussian matrices: A unified theory. Foundations
of Computational Mathematics, 16:1367-1396, 2014.
Andrea Duggento, Maria Guerrisi, and Nicola Toschi. Echo state network models for nonlinear
Granger causality. bioRxiv, 2019.
Andre Fujita, Joao R Sato, MigUel Garay, RUi Yamaguchi, SatorU Miyano, Mari Sogayar, and Carlos
E Ferreira. Modeling gene expression regulatory networks with the sparse vector autoregressive
model. BMC Systems Biology, 1, Feb 2007.
C.	W. J. Granger. Investigating causal relations by econometric models and cross-spectral methods.
Econometrica, 37(3):424-438, 1969.
Craig Hiemstra and Jonathan D. Jones. Testing for linear and nonlinear Granger causality in the
stock price- volume relation. The Journal of Finance, 49(5):1639-1664, 1994.
Sepp Hochreiter and Jargen Schmidhuber. Long short-term memory. Neural Computation, 9(8):
1735-1780, 1997.
Herbert Jaeger. Tutorial on training recurrent neural networks, covering BPPT, RTRL, EKF and the
echo state network approach. GMD-Forschungszentrum Informationstechnik, 5, Jan 2002.
William Johnson and Joram Lindenstrauss. Extensions of lipschitz maps into a hilbert space. Con-
temporary Mathematics, 26:189-206, 01 1984.
A. Karimi and M. R. Paul. Extensive chaos in the Lorenz-96 model. Chaos: An Interdisciplinary
Journal of Nonlinear Science, 20(4):043105, 2010.
Nehemy Lim, Florence d,Alche Buc, Cedric Auliac, and George Michailidis. Operator-valued
kernel-based vector autoregressive models for network inference. Machine Learning, 99:489-
513, 2014.
Daniel Marbach, Thomas Schaffter, Claudio Mattiussi, and Dario Floreano. Generating realistic
in silico gene networks for performance assessment of reverse engineering methods. Journal of
Computational Biology, 16(2):229-239, 2009.
10
Published as a conference paper at ICLR 2020
D.	Marinazzo, M. Pellicoro, and S. Stramaglia. Kernel-Granger causality and the analysis of dy-
namical networks. Physical Review E, 77, May 2008.
Meike Nauta, Doina Bucur, and Christin Seifert. Machine Learning and Knowledge Extraction, 1
(1):312-340, Jan 2019.
JUnier B. Oliva, Barnabas Poczos, and Jeff Schneider. The Statistical Recurrent Unit. In Proceedings
of the 34th International Conference on Machine Learning, volume 70, pp. 2671-2680, Aug 2017.
Robert J. Prill, Daniel Marbach, Julio Saez-Rodriguez, Peter K. Sorger, Leonidas G. Alexopoulos,
Xiaowei Xue, Neil D. Clarke, Gregoire Altan-Bonnet, and Gustavo Stolovitzky. Towards a rig-
orous assessment of systems biology models: The DREAM3 challenges. PLOS ONE, 5(2):1-18,
Feb 2010.
Anton Maximilian SChafer and Hans Georg Zimmermann. Recurrent neural networks are universal
approximators. In Proceedings of the 16th International Conference on Artificial Neural Networks
- Volume Part I, pp. 632-640, 2006.
Tapio Schneider, Shiwei Lan, Andrew Stuart, and Joao Teixeira. Earth system modeling 2.0: A
blueprint for models that learn from observations and targeted high-resolution simulations. Geo-
physical Research Letters, 44(24):12,396-12,417, 2017.
Anil K. Seth, Adam B. Barrett, and Lionel Barnett. Granger causality analysis in neuroscience and
neuroimaging. Journal of Neuroscience, 35(8):3293-3297, 2015.
Yanning Shen, Brian Baingana, and Georgios B. Giannakis. Nonlinear structural vector autoregres-
sive models for inferring effective brain network connectivity. arXiv preprint arXiv:1610.06551,
Oct 2016.
Noah Simon, Jerome Friedman, Trevor Hastie, and Robert Tibshirani. A sparse-group LASSO.
Journal of Computational and Graphical Statistics, 22(2):231-245, 2013.
Vikas Sindhwani, Ha Quang Minh, and AUreIie C. Lozano. Scalable matrix-valued kernel learning
for high-dimensional nonlinear multivariate regression and Granger causality. In Proceedings of
the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence, pp. 586-595, 2013.
Stephen M. Smith, Karla L. Miller, Gholamreza Salimi-Khorshidi, Matthew Webster, Christian F.
Beckmann, Thomas E. Nichols, Joseph D. Ramsey, and Mark W. Woolrich. Network modelling
methods for FMRI. NeuroImage, 54(2):875 - 891, 2011.
Peter Spirtes and Kun Zhang. Causal discovery and inference: concepts and recent methodological
advances. Applied Informatics, 3(1):3, Feb 2016.
Alex Tank, Ian Cover, Nicholas Foti, Ali Shojaie, and Emily Fox. An interpretable and sparse neural
network model for nonlinear Granger causality discovery. NIPS Time Series Workshop, Nov 2017.
Alex Tank, Ian Cover, Nicholas Foti, Ali Shojaie, and Emily Fox. Neural Granger causality for
nonlinear time series. arXiv:1802.05842v1, 2018.
Y. Wang, K. Lin, Y. Qi, Q. Lian, S. Feng, Z. Wu, and G. Pan. Estimating brain connectivity with
varying-length time lags using a recurrent neural network. IEEE Transactions on Biomedical
Engineering, 65(9):1953-1963, Sep 2018.
A Review of related work
Initial efforts in testing for nonlinear Granger causality focused mostly on the nonparameteric ap-
proach. Baek & Brock (1992) proposed a general statistical test to detect nonlinear Granger causal-
ity between two variables under the assumption that the linear VAR modeling of their time series
measurements results in i.i.d residual errors. The test involves computing correlation integral es-
timators of the conditional probabilities concerning distances between carefully selected lead-lag
subsequences of the input bivariate time series. Successive works by Hiemstra & Jones (1994); Diks
& Panchenko (2006); Bai et al. (2010); Diks & Wolski (2016) proposed their improved variants of
11
Published as a conference paper at ICLR 2020
the Baek-Brock test. Hiemstra & Jones (1994) modified and extended the original Baek-Brock test
to allow for weakly dependent residual errors in linear VAR modeling of the time series measure-
ments. Diks & Panchenko (2006) proposed a rectified version of the Heimstra-Jones’ test statistic,
making it unbiased while fixing the issue of overrejection of the null hypothesis. Later works by
Bai et al. (2010); Diks & Wolski (2016) extended the bi-variate test in Diks & Panchenko (2006) to
the multivariate setting. The biggest common drawback of these nonparameteric tests is the large
sample sizes required to robustly estimate the conditional probabilities that constitute the test statis-
tic. Furthermore, the prevalent strategy in these methods of testing each one of the variable-pairs
individually to detect pairwise Granger causality is unappealing from a computational standpoint,
especially when a very large number of variables are involved.
In the model driven approach, the Granger causal relationships are inferred directly from the pa-
rameters of a data generative model fitted to the time series measurements. Compared to the non-
parameteric approach, the model-based inference approach is considerably more sample efficient,
however the scope of inferrable causal dependencies is dictated by the choice of data generative
model. Nonlinear kernel based regression models have been found to be reasonably effective in test-
ing of nonlinear Granger causality. Kernel methods rely on linearization of the causal interactions in
a kernel-induced high dimensional feature space; the linearized interactions are subsequently mod-
eled using a linear VAR model in the feature space. Based on this idea, Marinazzo et al. (2008)
proposes a kernel Granger causality index to detect pairwise nonlinear Granger causality in the mul-
tivariate case. In Sindhwani et al. (2013); Lim et al. (2014), the nonlinear dependencies in the time
series measurements are modeled using nonlinear functions expressible as sums of vector valued
functions in the induced reproducing kernel Hilbert space (RKHS) of a matrix-valued kernel. In
Lim et al. (2014), additional smoothness and structured sparsity constraints are imposed on the ker-
nel parameters to promote consistency of the time series fitted nonlinear model. Shen et al. (2016)
proposes a nonlinear kernel-based structural VAR model to capture instantaneous nonlinear inter-
actions. The existing kernel based regression models are restrictive as they consider only additive
linear combinations of the RKHS functions to approximate the nonlinear dependencies in the time
series. Furthermore, deciding the optimal order of kernel based regression models is difficult as
it requires prior knowledge of the mimimum time delay beyond which the causal influences are
negligible.
By virtue of their universal approximation ability, RNNs offer a pragmatic way forward in modeling
of complex nonlinear dependencies in the time series measurements for the purpose of inferring
Granger causality. Mutiple recent works by Wang et al. (2018); Duggento et al. (2019); Abbasvandi
& Nasrabadi (2019) have investigated the use of different types of RNNs for inferring nonlinear
Granger causal relationships. However, they all adopt the same naive strategy whereby each pair-
wise causal relationship is tested individually by estimating its causal connection strength. The
strength of the causal connection from series j to series i is determined by the ratio of mean-squared
prediction errors incurred by unrestricted and restricted RNN models towards predicting series i us-
ing the past measurement sequences of all n component including and excluding the jth component
alone, respectively. The pairwise testing strategy however does not scale well computationally as
the number of component series becomes very large. This strategy also fails to exploit the typical
sparse connectivity of networked interactions between the processes which has unlocked significant
performance gains in the existing linear methods (Bahadori & Liu (2013); Bolstad et al. (2011)).
In a recent work by Tank et al. (2018), the pairwise Granger causal relationships are inferred di-
rectly from the weight parameters of component-wise MLP or LSTM networks fitted to the time
series measurements. By enforcing column-sparsity of the input-layer weight matrices in the fitted
MLP/LSTM models, their proposed approach returns a sparsely connected estimate of the underly-
ing Granger causal network. Due to its feedforward architecture, a traditional MLP network is not
well-suited for modeling ordered data such as a time series. Tank et al. (2018) demonstrated that the
MLP network can learn short range temporal dependencies spanning a few time delays by letting the
network’s input stage process multi-lag time series data over sliding windows. However, modeling
long-range temporal dependencies using the same approach requires a larger sliding window size
which entails an inconvenient increase in the number of trainable parameters. The simulation results
in Tank et al. (2018) indicate that MLP models are generally outperformed by LSTM models in
extracting the true topology of pairwise Granger causality, especially when the processes interact in
a highly nonlinear and intricate manner. While purposefully designed for modeling short and long
term temporal dependencies in a time series, the LSTM (Hochreiter & Schmidhuber (1997)) is very
12
Published as a conference paper at ICLR 2020
general and often too much overparameterized and thus prone to overfitting. While using overparam-
eterized models for inference is preferable when there is abundant training data available to leverage
upon, there are several applications where the data available for causal inference is extremely scarce.
It is our opinion that using a simpler RNN model combined with meaningful regularization of the
model parameters is the best way forward in inferring Granger causal relationships from underde-
termined time series measurements. Building on the ideas put forth by Tank et al. (2018), this paper
investigates the use of Statistical Recurrent Units (SRUs) towards inferring Granger causality.
B Proximal gradient descent updates for estimating the
REGULARIZED WEIGHT PARAMETERS IN THE SRU AND eSRU MODELS
Noting that the proximal operator corresponding to mixed '1-'2 norm (group-norm) is the group-
wise soft-thresholding operator, we use the following proximal gradient-descent updates to minimize
the ith SRU’s regularized loss in equation 4:
Wini),t+1(：,j) = Sλιη (Wini)k,j)-ηVwi",j)li(θSRW , ∀j ∈ H	(8)
Here, li(Θi)，T-I PT-I (xi,t - xi,t+ι)2 is the unregularized SRU loss function, η is the gradient-
descent stepsize and Sλ1η is the elementwise soft-thresholding operator defined below.
Sλιη(W)，(W -"ηkwwk2,	kwk2 >λ1η, ∀w ∈ Rn.	(9)
1	0,	kwk2 ≤ λ1η
The columns of weight matrix Wi(i) in the ith eSRU model are also updated in exactly the same
fashion as above.
Likewise, the jth row of the group-norm regularized weight matrix Wo(i) in the eSRU optimization
in equation 5 is updated as shown below.
W"t+1(j,Gj,k) = Sλ2η (W(i),t(j,Gj,k) -ηVwoi)(j,Gj,k)li(θ(SRU)) , ∀j = 1,2,...do. (10)
The gradient of the unregularized loss function li, 1 ≤ i ≤ n associated with the SRU and eSRU
models used in the above updates is evaluated via the backpropagation through time (BPTT) proce-
dure (Jaeger (2002)).
C Ablation Study
C.1 CHOICE OF ENCODER FOR eSRU FEEDBACK’ S STAGE-1: FIXED OR DATA DEPENDENT
As a possible further enhancement of the proposed eSRU time series model, one may consider
learning the encoding map, Dr(i), in the feedback path, as trainable parameters of the ith eSRU. In
Table 5, we compare the Granger causality detection performance of this particular eSRU variant
and the proposed design wherein Dr(i) is taken to be a random matrix with i.i.d. Gaussian entries.
The experimental setup is kept the same as in Section 5, and the entries of Dr(i) in the eSRU variant
are `2 -norm penalized during training.
We observe that the performance of these two models is statistically tied, which indicates that the
randomly constructed Dr(i) is able to distill the necessary information from the high-dimensional
summary statistics ui,t-1 required for generating the feedback ri,t . Based on these results, we
recommend using the proposed eSRU design with its randomly constructed encoding map Dr(i),
because of its simpler design and reduced training complexity.
C.2 IMPACT OF GROUP-SPARSE REGULARIZATION OF Wo(i)
In order to highlight the importance of learning time-localized predictive features in detecting
Granger causality, we compare the following two time series models:
13
Published as a conference paper at ICLR 2020
Table 5: Average AUROC for eSRU variants
DATASET
Average AUROC
Randomly constructed Dri)	Dri) as trainable parameters
Lorenz (T	= 250, F =	10)
Lorenz (T	= 500, F =	10)
Lorenz (T	= 250, F =	40)
Lorenz (T	= 500, F =	40)
VAR (T =	500)	
VAR (T =	1000)	
NetSim		
0.95 ± 0.02
0.98 ± 0.01
0.99 ± 0.0
1.0±0
0.93 ± 0.05
0.98 ± 0.01
0.84 ± 0.03
0.97 ± 0.01
0.99 ± 0.0
0.98 ± 0.01
1.0 ± 0.0
0.91 ± 0.04
0.98 ± 0.01
0.80 ± 0.02
i.	proposed eSRU (with group-sparse regularization of Wo(i) as described in Section 4.2)
ii.	eSRU variant with ridge-regularized Wo(i)
Once again, we use the same experimental settings as mentioned in Section 5. From Table 6, we
observe that barring the Lorenz-96(T =250/500,F =40) datasets, for which nearly perfect re-
covery of the Granger causal network is achieved, the average AUROC improves consistently for
the other datasets by switching from unstructured ridge regularization to the proposed group-sparse
regularization of the output weight matrix Wo(i) .
Table 6: Performance of eSRU variants with different regularizations of Wo(i)
DATASET
Average AUROC
Proposed group-sparse Ridge regularization
	regularization of W0(i)	of Wo(i)
Lorenz (T = 250, F = 10)	0.95 ± 0.02	0.93 ± 0.03
Lorenz (T = 500, F = 10)	0.98 ± 0.01	0.94 ± 0.04
Lorenz (T = 250, F = 40)	0.99 ± 0.0	0.99 ± 0.0
Lorenz (T = 500, F = 40)	1.0 ± 0.0	1.0 ± 0.0
VAR (T = 500)	0.93 ± 0.05	0.90 ± 0.03
VAR (T = 1000)	0.98 ± 0.01	0.96 ± 0.01
NetSim	0.84 ± 0.03	0.83 ± 0.03
D Implementation details
• Activation function for SRU and eSRU models
While the standard SRU proposed by Oliva et al. (2017) uses ReLU neurons, we found in
our numerical experiments that using the Exponential Linear Unit (ELU) activation resulted
in better performance. The ELU activation function is defined as
ELU(x) = x x x>0, α>0.	(11)
α(ex - 1) x ≤ 0
In our simulations, the constant α is set equal to one.
•	Number of neural layers in SRU model
To approximate the generative functions fi in equation 1, we consider the simplest ar-
chitecture for the SRU networks, whereby the constituent ReLU networks generating the
recurrent features, output features and feedback have a single layer feedforward design with
equal number of neurons.
•	Number of neural layers in Economy-SRU model
The ReLU networks used for generating the recurrent and output features in the proposed
14
Published as a conference paper at ICLR 2020
eSRU model have a single-layer feedforward design. However, the second stage of eSRU’s
modified feedback can be either single or multi-layered feedforward network. Provided
that d0r mdφ, a multi-layer implementation of the second stage of eSRU’s feedback can
still have fewer trainable parameters overall compared to the SRU’s single layer feedback
network. The simulation results in Section 5 are obtained using a two-layer ReLU network
in the second stage of eSRU’s feedback for the DREAM-3 experiments, and while using a
three-layer design for the Lorenz-96, VAR and NetSim experiments.
•	Self-interactions in Dream-3 gene networks
The in-silico gene networks synthesized for the DREAM-3 challenge have no self-
connections. Noting that none of the Granger causal inference methods evaluated in our
experiments intentionally suppress the self-interactions, the reported AUROC values are
computed by ignoring any self-connections in the inferred Granger causal networks.
E Python codes
cMLP & cLSTM models
Pytorch implementation of the componentwise MLP and LSTM models are taken from https:
//github.com/icc2115/Neural-GC.
Temporal Causal Discovery Framework (TCDF)
Pytorch implementation of the attention-gated CNN based Temporal Causal Discovery Framework
(TCDF) is taken from https://github.com/M-Nauta/TCDF.
Proposed SRU and Economy-SRU models
Pytorch implementations of the proposed componentwise SRU and eSRU models are shared at
https://github.com/sakhanna/SRU_for_GCI.
F ROC plots
The receiver operating characteristics (ROC) of different Granger causal inference methods are com-
pared in Figures 4-7. Here, an ROC curve represents the trade-off between the true-positive rate
(TPR) and the false-positive rate (FPR) achieved by a given method while inferring the underlying
pairwise Granger causal relationships.
15
Published as a conference paper at ICLR 2020
(a) F = 10,T =250
(b)F = 10,T =500
(c) F = 40, T = 250
(d) F = 40, T = 500
Figure 4: Average ROC curves for Lorenz-96 datasets
(a) 30% sparsity, T = 500
(b) 30% sparsity, T = 1000
Figure 5: Average ROC curves for VAR datasets
16
Published as a conference paper at ICLR 2020
Q≤
1
0.8
0.6
0.4
0.2
0
0	0.2	0.4	0.6	0.8
FPR
Figure 6: Average ROC curves for the NetSim experiment.
(b) E.coli-2
(a) E.coli-1
(e) Yeast-3
Figure 7: ROC curves for Dream-3 datasets
17
Published as a conference paper at ICLR 2020
G	Tuned hyperparameters
Tables 7 to 11 summarize the chosen hyperparameters and configurations of the different NN/RNN
models used for generating the results reported in Section 5. For the Dream-3 experiments, the
model configurations used for inferring the E.coli-1 gene regulatory network Prill et al. (2010) have
been provided in the tables.
Parameters	Tuning range	Dataset			
		LorenZ (F = 10/40)	VAR	Dream-3	NetSim
# Neural units per layer	-	10	10	5	10
Batch size	-	-250/500/1000	500/1000	21	200
Learning rate	Two-fold cross- validation across [5e-5, 1e-1]	~0.0005 (F = 10),~ 0.001 (F = 40)	0.1	0.0005	0.0005
Ridge regularization bias	Two-fold cross- validation across [5e-5, 10]	0.232079 (F = 10), 10.0 (F = 40)	0.002	5.0	0.464159
Block-sparse regu- larization bias (in- put layer)	-	[0.1, 10] (F = 10), [1, 100] (F=40)	[0.0001, 0.01]	[0.1, 100]	[0.1, 3.162]
# Lags		一	5	5	2	5
# Training epochs	一	2000	2000	2000	2000
Table 7: Componentwise MLP model configuration (Refer Tank et al. (2017) for detailed description
of the model parameters).
Parameters	TUning strategy	Dataset			
		Lorenz (F = 10/40)	VAR	Dream-3	NetSim
# units per layer	一	-Tc	^Tq	-To	~70
Batch size	一	250/500/1000	500/1000	^21	200
Learning rate	Two-fold cross- validation across [5e-5, 1e-1]	0.0005 (F = 10), 0.001 (F = 40)	0.1	0.0005	0.001
Ridge regularization bias	Two-fold cross- validation across [5e-5, 10]	0.021544 (F = 10), 5.0(F=40)	0.0005	5.0	0.010772
Group-sparse regu- larization bias	一	[1, 56.234]	[0.003162, 0.01]	[0.1, 17.52]	[0.1, 3.162]
Truncation	一	^^No	^^No	^^No	^^No
# Training epochs	一	2000	2000	2000	4000
Table 8: Componentwise LSTM model configuration (Refer Tank et al. (2017)) for detailed descrip-
tion of the model parameters).
Parameters	TUning strategy	Dataset				
		Lorenz (F = 10)	Lorenz (F = 40)	VAR	Dream-3	NetSim
Kernel size	{2,4}	4	2	2	2	4
Batch size	一	250/500/1000	250/500/1000	500/1000	21	-200-
Layers	{2,3,4}	2	2	3	3	2
Learning rate	{10-1, 10-2, 10-3}	0.01	0.01	-0.001-	-0.01-	0.001
Dilation	{1, 2,4}	1	2	1	4	2
Significance	一	08	0.8	8	08	-08-
# Training epochs	{1000,2000,4000}	2000	2000	2000	-4000-	2000-
Table 9: TCDF’s Attention-gated CNN model parameters (Refer Nauta et al. (2019) for detailed
description of the model parameters).
18
Published as a conference paper at ICLR 2020
Parameters	Tuning strategy	Dataset			
		LorenZ (F = 10/40)	VAR	Dream-3	NetSim
# units per layer	—	^70	-Tc	^7q	-To
"^A	-	{0.0, 0.01, 0.1, 0.99}	{0.0,0.01,0.1, 0.99}	{0.0,0.01,0.1, 0.5, 0.99}	{0.0,0.01,0.1, 0.99}
Learning rate	Two-fold cross- validation across [5e-4, 1e-1]	0.005 (F = 10), 0.01 (F=40)	0.04	0.005	0.001
Ridge regularization bias	Two-fold cross- validation across [0.01, 10]	0.021544 (F = 10), 0.464159 (F=40)	0.021544	0.2	0.464159
Group-sparse regu- larization bias λ1 (input layer)	—	[0.1, 1] (F = 10), [0.0631, 1] (F=40)	[0.001, 1]	[0.01, 1.0]	[0.1, 3.162]
Batch size	—	125	125	^21	^5
# Training epochs	—	2000	2000	1000	2000
Table 10: Componentwise SRU model configuration
Parameters	TUning strategy	Dataset			
		Lorenz (F = 10/40)	VAR	Dream-3	NetSim
# units per layer	—	-To	-To	-To	-To
"^A	—	{0.0, 0.01, 0.1, 0.99}	{0.0,0.01,0.1, 0.99}	{0.05, 0.1, 0.2, 0.99}	{0.0,0.01,0.1, 0.99}
Learning rate	Two-fold cross- validation across [5e-4, 1e-1]	0.01	0.01	0.001	0.001
Ridge regularization parameter	Two-fold cross- validation across [0.01, 10]	0.001 (F = 10), 0.043088 (F = 40)	0.021544	0.1	0.232
Group-sparse regu- larization bias λ1 (input layer)	—	[0.03162, 0.1]	[0.03162, 0.3162]	[0.1, 3.162]	[0.1, 3.162]
Group-sparse reg- ularization	bias λ2 (output feature layer)	Two-fold cross- validation across [0.01, 10]	0.232079 (F = 10), 0.928318 (F = 40)	0.464159	1.0	0.005
# layers in the sec- ond stage of feed- back network	—	F	2	1	2
Batch size	—	125	125	^21	^5
# Training epochs	—	2000	2000	2000	2000
Table 11: Economy-SRU model configuration
19