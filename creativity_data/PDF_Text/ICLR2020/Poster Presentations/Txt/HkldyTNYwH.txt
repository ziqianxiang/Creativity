Published as a conference paper at ICLR 2020
AE-OT: A new Generative Model based on ex-
tended semi-discrete optimal transport
Dongsheng An1, Yang Guo1,Na LeiEl2, Zhongxuan Luo2, Shing-TUng Yau3, and Xianfeng Gu1
1Department of Computer Science, Stony Brook University
2DUT-RU ISE, Dalian University of Technology
3Department of Mathematics, Harvard University
{doan, yangguo}@cs.stonybrook.edu , {nalei, zxluo}@dlut.edu.cn , yau@math.harvard.edu,
gu@cs.stonybrook.edu
Ab stract
Current generative models like generative adversarial networks (GANs) and vari-
ational autoencoders (VAEs) have attracted huge attention due to its capability
to generate visual realistic images. However, most of the existing models suffer
from the mode collapse or mode mixture problems. In this work, we give a the-
oretic explanation of the both problems by Figalli’s regularity theory of optimal
transportation maps. Basically, the generator compute the transportation maps
between the white noise distributions and the data distributions, which are in gen-
eral discontinuous. However, deep neural networks (DNNs) can only represent
continuous maps. This intrinsic conflict induces mode collapse and mode mixture.
In order to tackle the both problems, we explicitly separate the manifold embedding
and the optimal transportation; the first part is carried out using an autoencoder
(AE) to map the images onto the latent space; the second part is accomplished
using a GPU-based convex optimization to find the discontinuous transportation
maps. Composing the extended optimal transport (OT) map and the decoder, we
can finally generate new images from the white noise. This AE-OT model avoids
representing discontinuous maps by DNNs, therefore effectively prevents mode
collapse and mode mixture.
1	Introduction
Generative adversarial networks (GANs) (Goodfellow et al. (2014)) and variational autoencoders
(VAEs) (Kingma & Welling (2013)) emerge as the dominant approaches for unconditional image
generation. When trained on appropriate datasets, they are able to produce realistic and visual
appealing samples. GAN methods train an unconditional generator that regresses real images from
random noise and a discriminator that measures the difference between generated samples and real
images. Despite GANs’ advantages, they have critical drawbacks. 1) Training of GANs are tricky
and sensitive to hyperparameters. 2) GANs suffer from mode collapse, in which the generator only
learns to generate few modes of data distribution while missing others, although samples from the
missing modes occur throughout the training data (see e.g. Goodfellow (2016)). While for the
VAEs, the encoder is used to map the data distribution to a Gaussian latent distribution, which is then
mapped back to the data distribution by the decoder. While standard VAEs tend to capture all modes,
they often generate ambiguous images on multimodal real data distributions. We propose that these
phenomena relates deeply with the singularities of distribution transport maps.
Manifold Distribution Hypothesis In deep learning, the manifold distribution hypothesis is well
accepted, which assumes the distribution of a specific class of natural data is concentrated on a
low dimensional manifold embedded in the high dimensional data space Tenenbaum et al. (2000).
Therefore, GANs and VAEs implicitly aim to accomplish two major tasks: 1) manifold embedding:
to find the encoding/decoding maps between the data manifold embedded in the image space and the
latent space; 2) probability distribution transport: to transport a given white noise distribution to the
data distribution, either in the latent or in the image space.
1
Published as a conference paper at ICLR 2020
(a) convex support (b) concave support: single mode (c) concave support: multi modes
Figure 1: Mode collapse/mixture caused by the discontinuity of the transport map. Top row shows
real data distributions, the bottom row gives the noise distributions. On top, each cluster represents a
mode, the spurious generated samples are red crosses (mode mixture); at the bottom, red dotted lines
are the singularity set, red crosses are mapped to be spurious samples by DNNs. (a) If the support of
ʌ
the target distribution is convex, DNN (f1) is able to approximate the transport map f1 well. When
the support of the target distributions are concave, there are two situations: (b) single mode and (c)
ʌ
multi modes. In (b), DNN, represented by f2 , cannot approximate the transport map f2 well and
generates some spurious samples. f3 gives the transport map of multi-mode, when approximating it
with continuous DNNs, either mode collapse f31 or mode mixture f32 will happen.
Distribution Transformation The generator of GAN model and the decoder of VAE model are
trained to compute a transport map that transforms a known continuous distribution (e.g. Gaussian
white noise) to the real data distribution. Namely, the transport map pushes forward the given noise
distribution to a generated distribution to approximate the real data distribution, the similarity between
the two distributions determines the generalization ability of the generator Ben-David et al. (2010).
Discontinuity and Mode Collapse/Mixture It is a common practice among GAN/VAE models that
the generators/decoders are expressed by deep neural networks, which can only represent continuous
mappings. Unfortunately, as pointed out by works Nagarajan & Kolter (2017); Khayatkhoei et al.
(2018); Xiao et al. (2018), the transport maps may be discontinuous when there are multiple modes
in the data distribution. This intrinsic conflict can cause mode collapse or mode mixture. The later
means that the generated samples are mixtures of multiple modes and look spurious or ambiguous. 1
Furthermore, even when the real data distribution has a single mode, ambiguous data (e.g. a human
face image with mismatched eye colors) can still present. This can be explained by Brenier’s polar
factorization theorem Brenier (1991b; 1987; 1991a) and Figalli’s regularity theorem Figalli (2010);
Chen & Figalli (2017) (Thm. 5 in Appendix B), which asserts that if the support of the target
distribution is not convex, then there will be singularity sets on the support of the source distribution,
such that the transport map is discontinuous on these sets. This shows the intrinsic difficulties of
conventional GANs/VAEs cannot be eliminated, as shown in Fig. 1.
Conquering Mode Collapse/Mixture However, according to Brenier (1987; 1991a) theorem, the
optimal transport map can be represented as the gradient map of the Brenier potential. At the regular
points, the Brenier potential is differentiable, its gradient map (the transport map) is continuous; at
the singularities, the Brenier potential is continuous but not differentiable, and its gradient map is
discontinuous. Conventional GANs and VAEs model the gradient map directly and encounter the
trouble of discontinuity. In contrast, we propose to model the globally continuous Brenier potential
to avoid mode collapse/mixture.
More specifically, our proposed AE-OT model separates the manifold embedding step and the
probability distribution transformation step, the former is carried out by an autoencoder (AE), the
latter is accomplished by a convex optimization framework (OT). The OT step computes the Brenier
potential explicitly and is able to locate the singularity set (the discontinuous points of the gradient
map) based on Figalli’s theory. Our experimental results demonstrate that the proposed method can
not only cover all of the modes, but also avoid generating spurious samples (mode mixture).
Contributions (i) From theoretical aspect, this work gives a thorough explanation of mode collapse
and mode mixture by the regularity theory of optimal transportation developed by Figalli (2018
1For example, a generator generates obscure digits mixing 0 and 8 but neither 0 nor 8 on the MNIST dataset.
2
Published as a conference paper at ICLR 2020
Fields medalist) and the reasons why standard GANs/VAEs cannot solve this problem perfectly.
(ii) From practical aspect, this work proposes a novel model called AE-OT, which first encodes
the data manifold into the latent space, then compute the Brenier potential to represent the optimal
transportation map in the latent space. The Figalli’s singularity set can be located efficiently and
avoided when generating new samples. In this way, our model eliminates mode collapse and
mode mixture successfully. (iii) The algorithm for finding the Brenier potential and the optimal
transportation map can be accelerated with GPU based convex optimization algorithm. The method
converges to the unique global optimum with bounded error estimate. (iv) Our experiment results
demonstrate the efficiency and efficacy of the proposed method.
2	Related Work
Optimal Transport Optimal transport (OT) has been successfully applied in the areas like economy,
optics and machine learning, as surveyed in Solomon (2018) and Peyre & CUtUri (2018). In Gu
et al. (2016) , the intrinsic connection between Brenier theory in OT and Alexandroff theory in
convex geometry was established, and applied for deep learning in Lei et al. (2017) by an convex
optimization. Figalli and the collaborators Figalli (2010); Chen & Figalli (2017) proposed that when
the support of the data distribution is non-convex, the transport map will be discontinuous.
Generative models Generative model is one of the main tasks in the machine learning field. One of
the most used image generation methods is Variational Autoencoders (VAEs), where the decoders
approximate real data distributions from a Gaussian distribution in a variational approach (e.g Kingma
& Welling (2013) and Rezende et al. (2014)). Later, Adversarial Autoencoders (AAEs) Makhzani
et al. (2015) and Wasserstein Autoencoders (WAEs) Tolstikhin et al. (2018) are proposed following
the similar scheme. Although VAEs are relatively simple to train, images they generate look blurry.
Generative Adversarial Networks (GANs) Goodfellow et al. (2014) proposed by Goodfellow et.al can
produce better quality images. While being a powerful tool in generating realistically looking samples,
GANs suffer from the mode collapse problem. To solve this problem, a huge number of methods,
including changing the loss function (e.g. Wasserstein GAN Arjovsky et al. (2017)), regularizing the
discriminators to be Lipschitz (clipping Arjovsky et al. (2017), gradient regularization Gulrajani et al.
(2017), Mescheder et al. (2018) or spectral normalization Miyato et al. (2018)), were proposed.
Besides, various non-adversarial methods has also been proposed recently. GLO Bojanowski et al.
(2017) adopts the “encoder-less autoencoder" method to generate new images with a non-adversarial
loss function. IMLE Li & Malik (2018) proposed an ICP related generative model training approach.
Later GLANN Hoshen & Malik (2019) combines the advantages of GLO and IMLE, an embedding
from image space to latent space was first found using GLO and then a transformation between an
arbitrary distribution and latent code was computed using IMLE.
Mitigating Mode Collapsing Recently, Nagarajan & Kolter (2017); Khayatkhoei et al. (2018);
Xiao et al. (2018) also realize the training difficulties of GANs come from the approximation of
discontinuous functions with continuous DNNs. By the gradient-based regularization, GDGAN
Nagarajan & Kolter (2017) do relieve the mode collapse phenomenon of GANs, but mode mixture
still exists. Khayatkhoei et al. (2018) proposes to use multiple GANs to overcome the mode collapse.
Xiao et al. (2018) proposed to embed the images into a latent space according to Bourgain’s theorem,
and train the generator by sampling a Gaussian mixture distribution in the latent space instead of a
unimodal Gaussian. The recently introduced normalized diversification by Liu et al. (2018) can also
help overcome mode collapse successfully. However, all of them cannot solve the mode mixture well.
All these works Nagarajan & Kolter (2017); Khayatkhoei et al. (2018); Xiao et al. (2018) explain that
if the target data distribution has multiple modes, the transport map is discontinuous, but DNNs can
only represent continuous mappings, the intrinsic conflict causes mode collapse.
3	Computational Algorithms
Overview of AE-OT Model Our AE-OT model is summarized in Fig. 2, it has two major compo-
nents: i) (AE) An autoencoder is trained to encode (fθ) the data manifold from the image space X
to the latent space Z, and map the data distribution to the latent code distribution; then the decoder
gξ decodes the latent code back to the data manifold. ii) (OT) This module computes the optimal
transportation map T from the noise distribution to the latent code distribution. First, the Brenier
potential is found by a convex optimization process according to Gu et al. (2016), whose gradient is
the semi-discrete optimal transport map, where the target is the discrete set of latent codes of training
3
Published as a conference paper at ICLR 2020
Image Space X
Latent Space Z
Encoder
fθ
Input
Images
N Noise Distribution μ
Recon. Generated
Images Images
Image Space X ∣
∏∏
Figure 2:	AE-OT model. AE: fθ and gξ represent the encoding and decoding maps respectively,
where θ and ξ are their corresponding network parameters. In the latent space Z, the latent codes are
clustered into three different modes, represented as marks with different shapes (i.e. disks, squares
and circles). OT: The singular set between different modes is plotted with dashed lines. Finally,
the generator of our model, which generates realistic images from random noise samples, is the
composition of the extended OT map T and the decoding map gξ .
samples; then the transport map is piece-wise linearly extended to a global continuous map T , where
the image domain becomes a simplicial complex obtained by triangulating the above latent codes.
Finally, the singularity set in the source domain is located and avoided when generating new samples.
As a result, given a random noise x, we can get the generated image by gξ ◦ T (x).
Semi-Discrete OT Map Suppose the source measure μ (Gaussian or uniform distribution) is ab-
solutely continuous defined on a convex domain Ω ⊂ Rd, the target domain is a discrete set,
Y = {yι, y2,…，yn}, Iyi ∈ Rd, the target measure is a Dirac measure, V = PZi %δ(y - yj, i =
1,2,...,n, with the equal total mass as the source measure, μ(Ω) = PZi %. Under a semi-
discrete transport map T : Ω → Y, a cell decomposition is induced Ω = U2i Wi, such that
every x in each cell Wi is mapped to the target yi , T : x ∈ Wi 7→ yi . The map T is measure
preserving, denoted as T#m = V, if the μ-volume of each cell Wi equals to the ν-measure of
the image T(Wi) = y%, μ(Wi) = %. The cost function is given by C : Ω X Y → R, where
c(x, y) represent the cost for transporting a unit mass from x to y . The total cost of T is given
by ∕ω C(X,t (X))dμ(X) = ∑XJwi c(x, yi)dμ(x). Semi-discrete optimal transport map is the
measure-preserving map that minimizes the total cost, T* := arg min^μ=ν Jω c(x, T(x))dμ(x).
When the cost function is the L2 distance C(X, y) = 1/2kX - yk2, Brenier’s theorem claims that
the semi-discrete OT map is given by the gradient map of a piece-wise (PL) convex function, the
so-called Brenier potential Uh : Ω → R, uh(x) := maχn=ι{∏h,i(x)}, where ∏h,i(x) = hx,yii + hi
is the hyperplane corresponding to yi ∈ Y. As shown in Fig. 3(a), the projection of the graph of uh
decomposes Ω into cells Wi(h), each cell Wi(h) is the projection of the supporting plane ∏h,i(χ).
The height vector h is the unique optimizer of the following convex energy under the condition that
Pi hi = 0,
E(h)
hn	n
wi(η)dηi -	hiVi,
0 i=i	i=i
(1)
where wi(η) is the μ-volume of Wi(η). The convex energy E(h) can be optimized simply by gradient
descend method with VE(h) = (wi(h) - Vi)T.
The key is to compute the μ-volume wi(h) of each cell Wi(h), which can be estimated using
conventional Monte Carlo method. We draw N random samples from μ distribution, {xj}〜i.i.d. μ,
∀j ∈ J, the estimated μ-volume of each cell is Wi(h) = #{j ∈ J | Xj ∈ Wi(h)}∕N. Given
Xj , we can find Wi in which Xj ∈ Wi by i = arg maxi {hXj, yii + hi}, i = 1, 2, . . . , n. When
N is large enough, W%(h) converges to Wi(h). Then the gradient of the energy is approximated as
VE ≈ (Wi(h) - Vi)T. Once the gradient is estimated, we can use Adam algorithm Kingma & Ba
(2015) to minimize the energy. Sampling of X is independent of each other and finding the cell that
4
Published as a conference paper at ICLR 2020
〜
T
∩	Y
(a)	(b)
Figure 3:	Illustration of the proposed algorithm with two modes in Y . (a) Brenier potential and the
corresponding power diagram. Each cell Wi is mapped to yi, which is the slope of πh,i . The red line
in Ω gives the singular set. (b) The extended semi-discrete map. By computing the weighted center
of each cell, and then triangulating the centers according to the power diagram, we get the PL map
T(x) from Ω to Y. If the samples x1 is not in the triangles transverse the singular set, we map it to
the corresponding T(x1) in Y.
x is located only involves matrix multiplication and sorting. Hence the Monte Carlo method has a
natural parallel computation implementation on GPUs.
The approximation error is proportional to the inverse of the square root the amount of Monte Carlo
samples. Asymptotically the number of Monte Carlo samples increases exponentially with respect to
the dimension d (see e.g. Weed & Bach (2017)). This brings huge computational burdens. To find
a good balance between precision and speed, we adaptively adjust the number of random samples.
In practice, we apply the following strategy: if the energy E(h) ceases decreasing for a number
of consecutive steps, we double the amount of Monte Carlo samples. The algorithmic details of
semi-discrete OT map are summarized in Alg. 1.
Algorithm 1 Semi-discrete OT Map
1: Input: Latent codes Y = {yi }i∈I , empirical latent code dis-
tribution V = 击 ɪ2i∈ι δyi, number ofMonte Carlo samples
N, positive integer s.
2: Output: Optimal transport map T(∙).
3: Initialize h = (hi ,h2,..., h∣ι∣) . (0, 0,..., 0).
4: repeat
5:	Generate N uniformly distributed samples {xj }jN=1 .
6:	Calculate Vh = (Wi(h) - Vi)t.
7:	Vh = Vh — mean(Vh).
8:	Update h by Adam algorithm with β1 = 0.9, β2 = 0.5.
9:	if E(h) has not decreased for s steps then
10:	N — N X 2.
11:	end if
12: until Converge
13: OT map T (∙) . V (maxi〈•，yi〉+ hi).
Algorithm 2 Generate latent code
1: Input: Optimal transport map T(∙), number of samples to gen-
erate n, angle threshold θ.
2: Output: Generated latent code P .
3: Compute Ci by Monte Carlo method.
4: repeat
5:	Sample X 〜μ, Find the smallest d + 1 vertex around X as
{d(x, Cio ), d(x, cii),..., d(x, Cid)}.
6:	Compute dihedral angles θik between πi0 and πik .
7:	Select θik with θik ≤ θ, result in ik = 0, 1, . . . , d1.
8:	if ∀k, θik > θ then Abandon X
9:	else Generate latent code T(X) = Pd=0 λkT(Cik ) With
λk = d-i 11 1 2 3 4 5 6 7 8 9 10 11 (x, cik )/Pd=0 d-i (x, cij).
10:	end if
11: until Generate n new latent code
Piece-wise Linear Extension The semi-discrete OT map Vuh : Ω → Y maps all X ∈ Ω to the
latent codes of training samples {yi}’s and won’t generate new samples. Therefore, we extend the
semi-discrete OT map T = Vuh to a piecewise linear (PL) mapping T as follows. The projection
of Uh in the source domain induces a cell decomposition of Ω, of which each cell is of μ-volume
Vi and is mapped to the corresponding yi. By representing the cells by their μ-mass centers as
Ci := JW (h) xdμ(x), we can get the point-wise map t : ci → yi. The Poincare of the cell
decomposition induces a triangulation of the centers C = {ci}: if Wi ∩ Wj = 0, then Ci is connected
with Cj to form an edge [ci, Cj]. Similarly, if Wi0 ∩ Wi1 ∙∙∙ ∩ Wik = 0, then there is a k-dimensional
simplex [Ci0, Ci1 , . . . , Cik]. All these simplices form a triangulation of C (a simplicial complex),
denoted as T(C) (the green triangles in the left of Fig. 3(b)). We can triangulate Y in the same way
to obtain the triangulation T(Y) (the green triangles in the right of Fig. 3(b)). Once a random sample
x is drawn from the distribution μ, we can find the simplex σ in T(C) containing x. Assume the
simplex σ has d + 1 vertices {Ci0, Ci1 , . . . , Cid}, the bary-centric coordinates of x in σ is defined as
x = Pdk=0 λkCik, and Pdk=0 λk = 1 with all λk non-negative. Then the generated latent code of x
d
under this piece-wise linear map is given by T(x) =	k=0 λkyik (In Fig. 3(b), the green dot x1 is
5
Published as a conference paper at ICLR 2020
∙-v
mapped to be T (x1)). Because all of the yis are used to construct the simplicial complex T(Y ) in
the support of the target distribution, we can guarantee that no mode is lost.
In practice, the μ-mass center Ci is approximated by the mean value of all the Monte-Carlo samples
inside Wi(h), Ci = Pχ,.∈w. xj/#{xj ∈ Wi}, where Xj 〜μ. The connectivity information T(C)
is too complicated to construct and to store in high dimensional space, thus T(C) is not explicitly
built. Instead, we find the simplex σ ∈ T(C) containing X as follows: given a random point X ∈ Ω,
evaluate and sort its Euclidean distances to the centers d(x, Ci), i = 1, 2,...,n in the ascending order.
Suppose the first d + 1 items are {d(x, ^i0), d(x, ^^),..., d(x, Cid)}, then σ is formed by {^ik}. The
d
bary-centric coordinates λik are estimated as λik = d 1(x, Cik)/Ek=。d 1(x, Cik). However, this
may generate some spurious samples. To overcome it, we need further to detect the singular set.
Singular Set Detection According to Figalli’s theory Figalli (2010); Chen & Figalli (2017), if there
are multiple modes or the support of the target distribution is concave, there will be singular sets
Σ ⊂ Ω, where the Brenier potential is continuous but not differentiable, making its gradient map, i.e.
the transport map, discontinuous.
As shown in Fig. 3(a), the source distribution is uniformly defined on Ω, and the target empirical
distribution has two modes. There is one ridge (the red line) on the Brenier potential uh, whose
projection is the singular set Σ (the red line in Ω). Ω \ Σ consists of two connected components,
each of them is mapped onto a single mode. Σ consists of codimension 1 facets of cells. If
Wi (h) ∩ Wj (h) ⊂ Σ, then the dihedral angle between two supporting planes πh,i and πh,j of
uh is prominently large. Therefore, on the graph of Brenier potential, we pick the pairs of facets
whose dihedral angles are larger than a given threshold, the projection of their intersection gives a
co-dimension 1 cell in the singular set Σ. During the generation process, if a random sample X is
around Σ, it will be mapped by T to the gaps among the modes. When generating new latent codes,
we just abandon such samples. And this help our method prevent the mode mixture phenomenon.
Given the extended OT map T (X), some of the polyhedrons transverse the singular set (the red lines
of Fig .3(b)), which means that different vertices of the polyhedron belongs to different mode. If the
sample X falls into such a polyhedron (the dotted red triangle), we just abandon it (as shown in Fig.
3(b), the red dot X2 is just abandoned). Specifically, given X, we can detect if it belongs to the singular
set by checking the angles θik between πi° and π⅛ ,k = 1,2,...,d as θik =(yi。,yik〉/|"。∣∣∙ Ilyikl∣.
ʌ
If all of the angles θik is larger than a threshold θ, we say X belongs to the singular set and just
ʌ
abandon it. Or wejust select a subset {∏ik} with θik ≤ θ, denoted as {∏%,k = 0,1,...,dι}. Then
we can compute λk = d-1(x, ^ifc)/Pd=。d-1(x, Ci.) and T(X) = Pd=。λkT(CiJ. Intuitively, T(∙)
smooths the discrete function T (∙) in regions where latent codes are dense and keep the discontinuity
of T(∙) where latent codes are very sparse. In this way we avoid generating spurious latent code and
thus improve the generation quality. The algorithm to generate new code is shown in Alg. 2 and the
effect of threshold filtering is further investigated in Appendix 4.1.
4	Experiments
In order to validate that the proposed method can solve the mode collapse/mixture problems and
generate controllable high quality images, several experiments are conducted.
The first experiment explores the influence of the angle thresholds for the singularity detection on
synthetic dataset.
The second experiment focuses on toy sets, so that the complexity of the tasks can be manually
controlled and the mode and quality of the generated samples can be accurately computed. Lin et al.
(2018) did a large-scale comparison with previous methods that explicitly proposed to mitigate mode
collapse and thus established a baseline for comparison. For consistent evaluation, we set up our
experiment on the same benchmark dataset as theirs, and make the comparison.
In the last experiment, we run the proposed method mainly on 4 public datasets, MNIST LeCun &
Cortes (2010), MNIST-FANSION Han Xiao & Vollgraf (2017), CIFAR-10 Krizhevsky (2009) and
CelebA Zhang et al. (2018), just like the authors of Hoshen & Malik (2019) Sajjadi et al. (2018)
Lucic et al. (2018) did in their papers. Besides, the architecture of the decoder is the same as Lucic
et al. (2018), in which the authors did a large-scale study to evaluated the best performance of 8
6
Published as a conference paper at ICLR 2020
different generative models including various GAN models and VAE, and the encoder is set to be the
mirror of decoder.
4.1	Single Parameter Selective Interpolation
On synthetic datasets, effects of angle threshold filtering can be visually inspected. As illustrated
in Fig. 4, number of mode is a monotonically increasing function with respect to angle threshold
ʌ ʌ ʌ
θ . Quality of generated samples is effected directly by choosing different θ . Generally, small θ
ʌ
encourages interpolation in between closely related real samples while too large θ will result in
interpolation between samples from different modes, which might in turn lower generation quality.
ʌ
On synthetic datasets, where modes are isotropic and different modes are clearly separable, an ideal θ
that captures all modes while avoids generating low quality samples can be chosen within a relatively
ʌ
wide band. For real world datasets of unknown modes, an ideal θ needs to be hand tuned as the
separability of different modes depends largely on input data pattern and quality of the embedding
map.
(a) Target Distribution
ʌ
(b) θ too small
ʌ ʌ
(c) θ too large	(d) Proper θ
ʌ
Figure 4: Effect of increasing angle threshold θ . (a) shows target distribution. (b) and (c) shows
ʌ ʌ
AE-OT results when θ is too small (as in (b)) or too large (as in (c)). (d) shows a proper choice of θ
that precisely captured and generalized all modes.
4.2	Mitigation of mode collapse and mode mixture in synthetic dataset
Since synthetic dataset consists of explicit distributions and known modes, mode collapse and the
quality of the generated sample can be accurately measured. We choose the same synthetic datasets
and metrics as in Lin et al. (2018). Specifically, we use 2D-ring and 2D-grid for test sets and Number
of modes, Percentage of high-quality samples, reverse Kullback-Leibler (KL) divergence as evaluation
metrics. Number of modes counts the amount of modes captured by samples produced a generative
model. Percentage of high-quality samples measures the proportion of samples that generated within
three standard deviations of the nearest mode. reverse KL divergence measures how well generated
samples balance among all modes regarding the real distribution. In Lin et al. (2018), the authors
evaluated GAN, ALI, MD and PacGAN on synthetic sets with above three metrics. Each networks are
trained under the same generator architecture with a total of approximated 400K training parameters.
For AE-OT test, since the source space and the target space are both 2-dimensional, there is no need
to train any autoencoder. A two dimensional extended OT map is directly computed. Our results are
included in table 1, and benchmarks of previous methods are copied from Lin et al. (2018) and Xiao
et al. (2018). Generally speaking, the samples generated by the proposed method can not only cover
all of the modes, the quality of them is also better than others.
Besides, we experiment on stack MNIST dataset and CelebA dataset to further illustrate the perfor-
mance of the proposed method, and the results are shown in Section C.1 and C.2 in the Appendix.
4.3	Quantitative comparison with FID
FID is computed by: (1) extract the visual-meaningful features of both the generated and real images
through the inception network, (2) fit the features in both the generated and real feature spaces with
Gaussian distribution, and then (3) compute the distance between the two Gaussian distributions with
the following formula FID = ∣∣μr - μg ∣∣2 + Tr(Σr + Σg - 2(ΣrΣg) 1), where μr, μg mean the
means of the real and generated features, Σr , Σg represent the variances of both distributions.
We report our results in Tab. (2), in which the compared data comes from Lucic et al. (2018)Hoshen
& Malik (2019). In general, the proposed model achieves better than or comparable scores to other
7
Published as a conference paper at ICLR 2020
Table 1: Experiments on synthetic datasets. Under standard benchmark settings, AE-OT achieves
best performances over an average of 10 independent experiment results in terms of modes captured,
probability of high quality samples and reverse KL divergence. The mean values and standard
deviations of the experiment results are reported here.
	2D-ring			2D-grid		
	Modes (Max 8)	high quality samples	reverse KL	Modes (Max 25)	high quality samples	reverse KL
GAN	6.3±0.5	98.2±0.2%	0.45±0.09	17.3±0.8	94.8±0.7%	0.70±0.07
ALI	6.6±0.3	97.6±0.4%	0.36±0.04	24.1±0.4	95.7±0.6%	0.14±0.03
MD	4.3±0.8	36.6±8.8%	1.93±0.11	23.8±0.5	79.9±3.2%	0.18±0.03
PacGAN2	7.9±0.1	95.6±2.0%	0.07±0.03	23.8±0.7	91.3±0.8%	0.13±0.04
PacGAN3	7.8±0.1	97.7±0.3%	0.10±0.02	24.6±0.4	94.2±0.4%	0.06±0.02
PacGAN4	7.8±0.1	95.9±1.4%	0.07±0.02	24.8±0.2	93.6±0.6%	0.04±0.01
BourGAN	8.0±0.0	99.8±2.9%	4e-4±2e-4	24.9±0.3	95.9±0.2%	0.01±0.02
AE-OT	8.0±0.0	99.6±0.3%	0.004±0.001	25.0±0.0	99.8±0.2%	0.007±0.002
Table 2: Quantitative comparison with FID
	Adversarial				Non-Adversarial		Reference	
Dataset	NS GAN	LSGAN	WGAN	BEGAN	VAE	GLANN	AE	Ours
MNIST	6.8±0.5	7.8±0.6	6.7±0.4	13.1±1.0	23.8±0.6	8.6±0.1	5.5	6.2±0.2
Fansion	26.5±1.6	30.7±2.2	21.5±1.6	22.9±0.9	58.7±1.2	13.0±0.1	4.7	10.1±0.3
CIFAR-10	58.5±1.9	87.1 ±47.5	55.2±2.3	71.4±1.6	65.4±0.2	46.5±0.2	28.2	38.3±0.5
CelebA	55.0±3.3	53.9±2.8	41.3±2.0	38.9±0.9	85.7±3.8	46.3±0.1	67.5	68.4± 0.5
state-of-the-art generative models. Theoretically, the FID scores of our proposed generative models
should be close to that of the pre-trained autoencoders, and this is also validated in our experiments.
The autoencoder architecture we use here cannot find a good encoding for the CelebA dataset due to
the limited capacity. But the FID score of the generation model is still approach to the autoencoder.
In order to verify that with appropriate capacity of autoencoder, the proposed model works. We use
the generator of DCGAN Radford et al. (2016) as the decoder of the autoencoder, then the reported
FID score is 28.6, outperforming other models. Also, some of the generated images are displayed in
Fig. 7 of the Appendix.
We also display the generating results for the four dataset in Fig. 5. It includes the original images,
the best generating results of Lucic et al. (2018), including various GANs and VAE, the results of
Hoshen & Malik (2019) and ours, row by row.
5	Conclusion
This work gives a theoretic explanation for mode collapse/mixture by Brenier’s theory and Figalli’s
regularity theory of optimal transport maps. When the target measure has concave support, the OT
map is discontinuous on the signular sets. But DNNs can only represent continuous functions, this
conflict causes the both problems. In order to solve this problem, the AE-OT model is proposed by
separating manifold embedding and measure transformation. The former step is computed using an
autoencoder, the latter is carried out using the extended semi-discrete OT map based on GPUs. The
model is tested thoroughly and extensively by both synthetic and real data sets. The experimental
results validates the discontinuity of the OT maps and demonstrate the advantages comparing to the
state-of-the-arts.
Acknowledgements
The project is partially supported by the National Natural Science Foundation of China (61936002,
61772105, 61720106005), NSF CMMI-1762287 collaborative research: computational framework for
designing conformal stretchable electronics, Ford URP topology optimization of cellular mesostruc-
tures’ nonlinear behaviors for crash safety, and NSF DMS-1737812 collaborative research: ATD:
theory and algorithms for discrete curvatures on network data from human mobility and monitoring.
8
Published as a conference paper at ICLR 2020
S7∖4G，6Go
3<r牙 J0gG√
3。夕4 56 3 1
¾,S43O3 I
2 a 夕 7，3 73
ft∙夕少 83。/ G
53∕∖ 久3qJ
。9r 7I5 ɛ π
5 g√「6，⅛xp
,。o√* i q√
51772 /15
Ign¾4 q7q
2∕3r/ 34。
L3fh*3q5
N3-70ygooM
Oz/ 7 Σ
s q 3，V台
OOfg 6 q
H Qu oɔ >3 τ/
3 2og G(9
q 37、夕9
夕Q/305Z3
O夕5 4。q49
5515S76⅛∙
/72ʃa/ 2，
JG sʌa 06<6
q0o57≤⅛-α
071->3 夕 D9
567。。7ΓΓ6
Figure 5: The comparison between the proposed method and the state-of-the-art on MNIST LeCun &
Cortes (2010), Fashion MNIST Han Xiao & Vollgraf (2017), Cifar10 Krizhevsky (2009) and CelebA
Zhang et al. (2018). The first row shows some of the real images in each dataset. The second row
corresponds to the best results of Lucic et al. (2018); The third row gives the results of Hoshen &
Malik (2019); then we display our generating results in the last row.
References
Martin Arjovsky, Soumith Chintala, and Leon Bottou. WaSSerStein generative adversarial networks.
In ICML,pp. 214-223, 2017.
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman
Vaughan. A theory of learning from different domains. Machine learning, 79(1-2):151-175, 2010.
Piotr Bojanowski, Armand Joulin, David Lopez-Paz, and Arthur Szlam. Optimizing the latent space
of generative networks. arXiv:1707.05776, 2017.
Y. Brenier. Polar decomposition and increasing rearrangement of vector fields. C. R. Acad. Sci. Paris
Sr. I Math., 305(19):805-808, 1987.
Y. Brenier. Polar factorization and monotone rearrangement of vector-valued functions. Comm. Pure
Appl. Math., 44(4):375-417, 1991a.
Yann Brenier. Polar factorization and monotone rearrangement of vector-valued functions. Commu-
nications on pure and applied mathematics, 44(4):375-417, 1991b.
9
Published as a conference paper at ICLR 2020
LA Caffarelli. A localization property of viscosity solutions to the monge-amp6re equation and
their strict convexity. Ann. ofMath., 131(2):129-134, 1990a.
L.A. Caffarelli. Interior w2,p estimates for solutions ofthe monge-amp6re equation. Ann. ofMath.,
131(2):135-150, 1990b.
L.A. Caffarelli. Some regularity properties of solutions of monge amp6re equation. Comm. Pure
Appl. Math., 44(8-9):965-969, 1991.
Shibing Chen and Alessio Figalli. Partial w2,p regularity for optimal transport maps. Journal of
Functional Analysis, 272:4588-4605, 2017.
Alessio Figalli. Regularity properties of optimal maps between nonconvex domains in the plane.
Communications in Partial Differential Equations, 35(3):465-479, February 2010.
Ian Goodfellow. Nips 2016 tutorial: Generative adversarial networks. arXiv:1701.00160, 2016.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, pp. 2672-2680, 2014.
David Xianfeng Gu, Feng Luo, jian Sun, and Shing-Tung Yau. Variational principles for minkowski
type problems, discrete optimal transport, and discrete monge-ampere equations. Asian Journal of
Mathematics, 2016.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville.
Improved training of wasserstein gans. In NIPS, pp. 5769-5779, 2017.
Kashif Rasul Han Xiao and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking
machine learning algorithms. https://arxiv.org/abs/1708.07747, 2017.
Yedid Hoshen and Jitendra Malik. Non-adversarial image synthesis with generative latent nearest
neighbors. In CVPR, 2019.
Mahyar Khayatkhoei, Maneesh K. Singh, and Ahmed Elgammal. Disconnected manifold learning
for generative adversarial networks. In Advances in Neural Information Processing Systems, 2018.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. 2015.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv:1312.6114, 2013.
Jun Kitagawa, Quentin Merigot, and Boris Thibert. Convergence of a newton algorithm for semi-
discrete optimal transport. JOURNAL OF THE EUROPEAN MATHEMATICAL SOCIETY, 2019.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Tech report, 2009.
Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http://yann.
lecun.com/exdb/mnist/.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Na Lei, Kehua Su, Li Cui, Shing-Tung Yau, and David Xianfeng Gu. A geometric view of optimal
transportation and generative mode. Computer Aided Geometric Design, 2017.
Na Lei, Dongsheng An, Yang Guo, Kehu Su, Shixia Liu, Zhongxuan Luo, Shing-Tung Yau, and
Xianfeng Gu. A geometric understanding of deep learning. Engineering, 2020.
Ke Li and Jitendra Malik. Implicit maximum likelihood estimation. arXiv:1809.09087, 2018.
Zinan Lin, Ashish Khetan, Giulia Fanti, and Sewoong Oh. Pacgan: The power of two samples
in generative adversarial networks. In Advances in Neural Information Processing Systems, pp.
1505-1514, 2018.
Shaohui Liu, Xiao Zhang, Jianqiao Wangni, and Shi Jianbo. Normalized diversification. In CVPR,
2018.
10
Published as a conference paper at ICLR 2020
Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, and Olivier Bousquet. Are gans created
equal? a large-scale study. In Advances in neural information processing Systems, pp. 698-707,
2018.
Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey. Adversarial
autoencoders. arXiv:1511.05644, 2015.
Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for gans do
actually converge? In ICML, pp. 3478-3487, 2018.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for
generative adversarial networks. In ICLR, 2018.
Vaishnavh Nagarajan and J. Zico Kolter. Gradient descent gan optimization is locally stable. In
Advances in Neural Information Processing Systems, 2017.
Gabriel Peyr6 and Marco Cuturi. Computational Optimal Transport. arxiv: 1803.00567, 2018.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. In ICLR, 2016.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative models. arXiv:1401.4082, 2014.
Mehdi SM Sajjadi, Olivier Bachem, Mario Lucic, Olivier Bousquet, and Sylvain Gelly. Assessing
generative models via precision and recall. arXiv:1806.00035, 2018.
Justin Solomon. Optimal Transport on Discrete Domains. https://arxiv.org/abs/1801.07745, 2018.
J B Tenenbaum, V Silva, and J C Langford. A global geometric framework for nonlinear dimension-
ality reduction. Science, 290(5500):2391-232, 2000.
Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bernhard Schoelkopf. Wasserstein auto-encoders.
In ICLR, 2018.
CedriC Villani. Optimal transport: old and new. Springer Science & Business Media, 2008.
Jonathan Weed and Francis Bach. Sharp asymptotic and finite-sample rates of convergence of
empirical measures in wasserstein distance. arXiv:1707.00087, 2017.
Chang Xiao, Peilin Zhong, and Changxi Zheng. Bourgan: Generative networks with metric embed-
dings. In Advances in Neural Information Processing Systems, 2018.
Zhanpeng Zhang, Ping Luo, Chen Change Loy, and Xiaoou Tang. From facial expression recognition
to interpersonal relation prediction. International Journal of Computer Vision, 2018.
11
Published as a conference paper at ICLR 2020
A Brenier’s Theory
In this subsection, we briefly introduce the basic concepts and theorems in optimal transport theory,
which comes from Brenier theory Villani (2008); Brenier (1987; 1991a), and discrete theory Gu et al.
(2016).
Optimal transport Problem Suppose X, Y ⊂ Rd are two subsets of n-dimensional Euclidean
space, μ, V are two probability measured defined on X and Y respectively, with equal total measure,
μ(X) = v(Y). A map T : X → Y is measure preserving, denoted as T⅛μ = V, if for any
measurable set B ⊂ Y, μ(TT(B)) = V(B). Given a cost function c(χ,y) : X X Y → R≥o,
indicating the cost of moving each unit mass from the source to the target, the total transport cost of
the map T is defined to be JX c(x, T(x))dμ(x).
The Monge’s problem of optimal transport arises from finding the measure-preserving map that
minimizes the total transport cost.
(MP)
Wc(μ,ν):= min C c(x,T (x))dμ(x).
T⅛μ=νJx
(2)
The solutions to the Monge’s problem is called the optimal transport map, whose total transport cost
is called the Wasserstein distance between μ and V, denoted as Wc(μ, V).
Brenier’s Approach Brenier Brenier (1987; 1991a) proved the following theorem:
Theorem 1 (Brenier Brenier (1987; 1991b)). Suppose X and Y are the Euclidean space Rd and
the transport cost is the quadratic Euclidean distance c(x, y) = 1∕2∣∣x — y∣∣2. Furthermore μ is
absolutely continuous and μ and V have finite SeCond order moments, then there exists a convex
function U : X → R, the so-called Briener potential, its gradient map Vu gives the solution to the
Monge’s problem. The Brenier potential is unique up to a constant.
Brenier,s polar factorization theorem claims that: for any measure preserving map 7#仙 = v, T can
be uniquely decomposes into the forms T = Vu ◦ s, where s : X → X is a volume preserving map
and Vu is the optimal transport map under L2 cost. Therefore, the regularity of T can be determined
by that of Vu.
Discrete Brenier’s Theorem Brenier theorem can be directly generalized to discrete target measure.
Suppose the source measure μ is defined on a compact convex set Ω, the target measure V =
pn=ι Viδ(y — yi), μ(Ω) = Pi %. The discrete Brenier potential is a piecewise linear function,
uh(x) = mnax{πh,i(x)} = mnax {hx, yi i + hi} .
(3)
As shown in Fig. 3(a), the projection of the Brenier potential induces a cell decomposition of Ω, each
cell Wi(h) := {p ∈ Ω∣Vuh(p) = yi}, whose μ-measure is denoted as wi(h).
Theorem 2 (Discrete Brenier Theorem Gu et al. (2016)). For any V1, V2, . . . , Vn > 0 with Pin=1 Vi =
μ(Ω), there exists h = (hi, h2,..., hn) ∈ Rn, unique up to adding a constant (c,c,..., C), so that
wi (h) = Vi, for all i. The vector h is the unique minimum argument of the following convex energy
E(h)
hn	n
wi(η)dηi —	hiVi,
(4)
defined on an open convex set H = {h ∈ Rn : wi(h) > 0, i = 1, 2, . . . , n}. Furthermore, Vuh
minimizes the quadratic cost Jω ∣∣x — T(x)k2dμ(x) among all transport maps 7#仙 = V. The
gradient of above energy is given by VE (h) = (w1(h) — V1, w2(h) — V2, . . . , wn (h) — Vn)T. The
Hessian of the energy is given by
∂wi =	μ(Wi ∩ Wj) ∂wi = X ∂wi
dhj kyi - yj Il , dhi Ji ah/
The optimal transport map can be obtained by convex optimization. Furthermore, the optimization
can be carried out using Newton’s method. The global linear convergence rate is guaranteed by the
following theorem:
12
Published as a conference paper at ICLR 2020
Theorem 3 (KitagaWa-Merigot-Thibert KitagaWa et al. (2019)). Assume the costfunction is quadratic
distance, μ has convex support and also that (i) The probability density of μ is C0,α(Ω) for a in
(0; 1]. (ii) μ has positive Poincare-Wirtinger constant. Then the Newton algorithmfor Semi-discrete
optimal transport converges globally with linear rate and locally with rate 1 + α.
Though in our method the gradient descend method is applied, the above theorem also ensures its
convergence because of the convexity of the energy function E(h) We adopted.
B Figalli’ s Theory
In this section, We shoW the fact that even for the case of single mode, the transport map may still be
discontinuous, Which Will cause the instability of the training process of GANs. The arguments are
mainly based on the regularity theory of transport maps developed by Figalli Chen & Figalli (2017);
Figalli (2010) and so on.
According to Brenier’s Theorem Brenier (1987; 1991a), any transport map can be decomposed into a
measure preserving map and a solution to the Monge-Ampere equation, Which is the optimal transport
map under the L2 cost function. Therefore, the continuity of the transport map can be reduced to
the regularity (smoothness) of the solution to the Monge-Ampere equation. When the support of
the target measure is convex and the density functions are smooth, Caffarelli shoWed the map is
differentiable; otherWise if the target domain is not convex, Figalli shoWed the map is discontinuous,
and gave precise description of the singularity set. In this section, We briefly introduce Figallis’
theory, and conduct an experiment using CelebA data set to shoW the existence of the singularity set,
hence demonstrate the fact that the transport maps computed in GANs are discontinuous.
B.1	Convex Domains - Caffarelli Theorem
Let Ω and Λ are two bounded open sets in Rn, and let f : Rn → R and g : Rn → R be two positive
functions such that f = 0 in Rn \ Ω, g = 0 in Rn \ Λ, and
Zf = Zλ g = 1.
According to Brenier’s Theorem Brenier (1987; 1991a), there exists a globally Lipschitz convex
function 夕：Rn → R such that ▽夕#f = g and and ▽夕(x) ∈ Λ for L2-a.e. X ∈ Rn. We say 夕
weakly solves the Monge-Ampere equation
det(D2 夕)=——f—— in Rn,	(6)
g ◦▽夕
together with the boundary condition ▽夕(Rn) ⊂ Λ.夕 is called the Briener potential.
As shown by Caffarelli [9], if Λ is convex, then 夕 is strictly convex, and it solves the Monge-Ampere
equation 6. The regularity theory has been estabilished (see Caffarelli (1990a;b; 1991)), such as
1.	if λ ≤ f, g ≤ 1∕λ for some λ > 0, then 夕 ∈ CIoa(λ)(Ω).
2.	if f ∈ Cka(Ω) andg ∈ C^(Λ), then 中 ∈ C^2,α(Ω), (k ≥ 0,α ∈ (0,1)).
B.2	Non-convex Domains - Figalli Theorem
However, if Λ is not convex, the regularity of the Brenier potential can not be guaranteed. Figalli
gave examples in Figalli (2010), such that
1.	Ω is convex, Λ is simply connected, but non-convex;
2.	the density functions f and g are smooth, f ∈ C∞(Ω) and g ∈ C∞(Λ);
3.	the Brenier potential 夕 ∈ C 1(Ω), the transport map ▽夕 is not continuous.
In this scenario, the transport map can not be learned using DNNs, and training process is unstable or
the GAN model generates unrealistic samples.
13
Published as a conference paper at ICLR 2020
Figalli's construction Let φ : Rn → R be a convex function. Its subdifferential at a point X is
defined by
∂夕(x) := {y ∈ Rn3(z) ≥ 夕(x) + y ∙ (z — x),∀z ∈ Rn}.
夕 is differentiable at a point X if and only if ∂夕(x) is a singleton. Figalli decomposes the set of
non-differentiability points according to the dimension of the singular set:
∑k(ψ) := {x ∈ Rn∣dim(∂夕)=k}, k = 0,...,n.	(7)
For any k = 0,...,n, the set ∑k (夕)is (n — k)-reCtifiable. The set of reachable subgradients at X as
▽4=V lim ▽g(Xk)∣Xk ∈ ∑0,Xk → X ..
k→+∞
It is known that the convex hull of V*^(x), coincides with ∂夕(x).
Theorem 4 (Figalli). Assume that there exists λ > 0 such that λ ≤ f ≤ 1∕λ in Ω, λ ≤ g ≤ 1∕λ in
Λ, and that ∂Ω and ∂Λ are continuous. Then 夕 is strictly convex inside Ω. Moreover there exist two
open sets Ω0 ⊂ Ω andN ⊂ Λ, with L2(Ω \ Ω0) = L2(Λ \ Λ0) = 0, such that φ ∈ C 1,α(Ω0), V夕 is
a bi-Holder homeomorphism between Ω0 and N, and 夕 is an Alexandrov solution of6 inside Ω0. In
particular Caffarelli's regularity theoryfor strictly convex Alexandrov solutions ofthe Monge-Ampere
equations applies to 夕 inside Ω0.
Figalli studies the singular set of 夕 in Ω, i.e. the set of points X ∈ Ω where 夕 is not differentiable,
denoted as Sing. Figalli shows the singularity set has the following characterization,
Sing = {x ∈ Ω∣∂夕(x) ∩ Λ = 0, V*夕(x) ⊂ ∂Λ, ∂夕(x) ⊂ Λ}.
it can be decomposed into connected components Sing := ∪iSi. For planar case,
Theorem 5 (Figalli Singularity Set). The number of connected components of Sing is at most
countable. Moreover:
1.	either Si coincides with an isolated point {∕i} for some Xi ∈ Ω, and in this case the
boundary of ∂(Xi) is entirely contained inside ∂ Λ (so that ∂夕(Xi) completely fills a hole in
N);
2.	or Si can be written as a disjoint union as follows:
Si = [ γij ,
j
where γij : Iij → Sing are embedded Lipschitz curves parameterized by arc-lengh, Iij is
an interval.
B.3 Elementary Experiments
We have designed a numerical experiment to verify Figalli’s theorems in low dimensional case.
Fig. 6 shows another computational result, which demonstrates the singularity structure in Figalli’s
theorem. The source domain Ω is the unit disk, the target domain Λ is with complicated geometry.
The singularity set of the optimal transport map satisfies the description of Figalli’s theorem 4,
31
Σ1 =	γk, Σ2 =	Xj .
i=0	j=0
∂夕(xo) fills the hole on Λ. For any interior point P ∈ γι, ∂夕(P) is a line segment connecting two
points on the boundary of N.
C Additional Experiments
C.1 Stack MNIST Experiment
Experiments of varisou GAN models on stacked MNIST dataset are in consistent with Lin et al.
(2018). For AE-OT model, we use the architecture shown in table 3 and table 4, with the decoder
14
Published as a conference paper at ICLR 2020
Figure 6: Figalli’s example: Singularity structure of an optimal transport map, Fig.3.2 in Figalli
(2010), Lei et al. (2020).
Table 3: EnCoder architecture for StaCk MNIST
layer	number of outputs	kernel size	stride	BN	activation
Input X 〜Pdata	28*28*3				
Convolution	14*14*16	5*5	2		LeakyReLU
Convolution	7*7*32	5*5	2	Yes	LeakyReLU
Convolution	4*4*64	5*5	2	Yes	LeakyReLU
Convolution	2*2*128	5*5	2	Yes	LeakyReLU
Fully connected	100				
architecture same as the consistent generator architectures in GANs, and encoder having mirrored
architecture.
We test diversity of generated samples from our AE-OT method on stack MNIST dataset that consists
of 128,000 samples in 1,000 modes with each sample stacking three handwritten digit images from
MNIST dataset LeCun et al. (1998). Number of modes counts the amount of modes captured by
samples produced a generative model. The reverse KL divergence is computed by first assign each
samples to their nearest mode, and compute the KL divergence between histogram of sample count
ʌ
on each mode and the histogram of real data. We choose angle threshold θ = 0.5 for AE-OT
method. Details of network architectures are listed in supplementary materials. Experiments results
are summarized in table 5, which show our method achieves best performance in terms of modes
captured and reverse KL divergence on stacked MNIST dataset.
C.2 CelebA Experiment
we evaluate our method on CelebA dataset by measuring collision probability in a batch of 1024
generated images of size 64-by-64. If a pair of identical images appear, a collision is declared, and
thus higher collision probability means lower generation diversity. The same metric has been used
in Lin et al. (2018) for evaluation of PacGAN. To make a consistent comparison, we design our
autoencoder network with encoder having the same architecture as in previous work and decoder
ʌ
having a mirrored architecture of encoder. Angle threshold θ is chosen to be 0.7 for AE-OT test.
Results are listed in table 6, with corresponding images can be downloaded here. Results have
shown that our method achieves best result in terms of probability of collision. Autoencoder network
structures can be found at table 7 and 8.
15
Published as a conference paper at ICLR 2020
Table 4: Decoder architecture for Stack MNIST
layer	number of outputs	kernel size	stride	BN	activation
Input Z 〜PIatent	100				
Fully connected	2*2*128			Yes	ReLU
Transposed Convolution	4*4*64	5*5	2	Yes	ReLU
Transposed Convolution	7*7*32	5*5	2	Yes	ReLU
Transposed Convolution	14*14*16	5*5	2	Yes	ReLU
Transposed Convolution	28*28*3	5*5	2		Tanh
Table 5: Experiments on stacked MNIST. Results have shown that our method achieves best results
in termS of mode captured and reverSe KL divergence. (*) In WGAN, PacWGAN and AE-OT
experiments, number of feature maps in each network layer is a quarter of those in other experiments.
Stacked MNIST
	Modes	KL
DCGAN	99.0	3.40
ALI	16.0	5.40
Unrolled GAN	48.7	4.32
VEEGAN	150.0	2.95
MD	24.5 ± 7.67	5.49 ± 0.42
PacDCGAN4	1000.0 ± 0.00	0.07 ± 0.005
WGAN(*)	314.3 ± 38.54	2.44 ± 0.170
PacWGAN4(*)	965.7 ± 19.07	0.42 ± 0.094
AE-OT(*)	1000.0 ± 0.0	0.03 ± 0.0008
C.3 Linear Interpolation in the latent space
Given any two images in the dataset, we can find the images between them by linear interpolation in
the noise space because the one to one correspondence between μ mass centers in the noise space and
the images in the dataset is provided by the proposed algorithm. For other generation models, though
the interpolation can be done successfully in the noise space, they cannot find the correspondence
from the noise space and the image space. The results of the linear interpolation are shown in Fig. 8.
16
Published as a conference paper at ICLR 2020
Table 6: Probability of identical images in a batch of 1024 generated images from DCGAN, PacGAN2
and AE-OT. Results have shown that our method achives best result in terms of collision probability
on CelebA dataset._______________________________________________________________
Discriminator size (Decoder size)	Probability of collision		
	DCGAN	PaCDCGAN2	AE-OT
273K	1	0.33	0
4×273K	0.42	0	0
16×273K	0.86	0	0
25×273K	0.65	0.17	0
Table 7: Encoder architecture in CelebA experiment
layer	number of outputs	kernel size	stride	BN	activation
Input X 〜Pdata	64*64*3				
Convolution	32*32*dim_f	4*4	2		LeakyReLU
Convolution	16*16*dim_f*2	4*4	2	Yes	LeakyReLU
Convolution	8*8*dim_f*4	4*4	2	Yes	LeakyReLU
Convolution	4*4*dim_f*8	4*4	2	Yes	LeakyReLU
Convolution	100	4*4	1		
Table 8: Decoder architecture in CelebA experiment
layer	number of outputs	kernel size	stride	BN	activation
Input Z 〜Platent	"Ig0				
Transposed Convolution	4*4*dim_f*8				
Transposed Convolution	8*8*dim_f*4	4*4	2	Yes	ReLU
Transposed Convolution	16*16*dim_f*2	4*4	2	Yes	ReLU
Transposed Convolution	32*32*dim_f	4*4	2	Yes	ReLU
Transposed Convolution	64*64*3	4*4	2		Tanh
17
Published as a conference paper at ICLR 2020
Figure 7: The generated human faces with the architecture originated from DCGAN Radford et al.
(2016)	18
Published as a conference paper at ICLR 2020
要耍型鳖瞽Sy0,曹
,ι⅛翻覆金金畜龄需
电♦■亨亨7歹7
Figure 8: The linear interpolation between given two faces in the dataset.
19