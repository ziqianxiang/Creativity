Published as a conference paper at ICLR 2020
Short and Sparse Deconvolution — A Geomet
ric Approach
Yenson Lau*
Electrical Engineering
Columbia University
y.lau@columbia.edu
Pengcheng Zhou
Department of Statistics
Columbia University
zhoupc2018@gmail.com
Qing Qu*
Center for Data Science
New York University
qq213@nyu.edu
Yuqian Zhang
Electrical & Computer Engineering
Rutgers University
yqz.zhang@rutgers.edu
Han-wen Kuo
Electrical Engineering
Columbia University
hk2673@columbia.edu
John Wright
Electrical Engineering
Columbia University
jw2966@columbia.edu
Ab stract
Short-and-sparse deconvolution (SaSD) is the problem of extracting localized,
recurring motifs in signals with spatial or temporal structure. Variants of this
problem arise in applications such as image deblurring, microscopy, neural spike
sorting, and more. The problem is challenging in both theory and practice, as natu-
ral optimization formulations are nonconvex. Moreover, practical deconvolution
problems involve smooth motifs (kernels) whose spectra decay rapidly, resulting
in poor conditioning and numerical challenges. This paper is motivated by recent
theoretical advances (Zhang et al., 2017; Kuo et al., 2019), which characterize the
optimization landscape of a particular nonconvex formulation of SaSD and give
a provable algorithm which exactly solves certain non-practical instances of the
SaSD problem. We leverage the key ideas from this theory (sphere constraints, data-
driven initialization) to develop a practical algorithm, which performs well on data
arising from a range of application areas. We highlight key additional challenges
posed by the ill-conditioning of real SaSD problems, and suggest heuristics (accel-
eration, continuation, reweighting) to mitigate them. Experiments demonstrate the
performance and generality of the proposed method.
1	Introduction
Many signals arising in science and engineering can be modeled as superpositions of basic, recurring
motifs, which encode critical information about a physical process of interest. Signals of this type
can be modeled as the convolution of a zero-padded short kernel a0 P Rp0 (the motif) with a longer
sparse signal x0 P Rm (m " p0) which encodes the locations of the motifs in the sample* 1:
y “ ιa0 f x0.	(1)
We term this a short-and-sparse (SaS) model. Since often only y is observed, short-and-sparse
deconvolution (SaSD) is the problem of recovering both a0 and x0 from y. Variants of SaSD arise in
areas such as microscopy (Cheung et al., 2018), astronomy (Briers et al., 2013), and neuroscience
(Song et al., 2018). SaSD is a challenging inverse problem in both theory and practice. Natural
formulations are nonconvex, and very little algorithmic theory was available. Moreover, practical
instances are often ill-conditioned, due to the spectral decay of the kernel a0 (Cheung et al., 2018).
This paper is motivated by recent theoretical advances in nonconvex optimization and, in particular,
on the geometry of SaSD. Zhang et al. (2017) and Kuo et al. (2019) study particular optimization
*YL and QQ contributed equally to this work. The full version of this work can be found at https:
//arxiv.org/abs/1908.10959.
1For simplicity, (1) uses cyclic convolution; algorithms are results also apply to linear convolution with minor
modifications. Here ι denotes the zero padding operator.
1
Published as a conference paper at ICLR 2020
formulations for SaSD and show that the landscape is largely driven by the problem symmetries of
SaSD. They derive provable methods for idealized problem instances, which exactly recover pa0, x0q
up to trivial ambiguities. While inspiring, these methods are not practical and perform poorly on real
problem instances. Where the emphasis of Zhang et al. (2017) and Kuo et al. (2019) is on theoretical
guarantees, here we focus on practical computation. We show how to combine ideas from this theory
with heuristics that better address the properties of practical deconvolution problems, to build a novel
method that performs well on data arising in a range of application areas. A critical issue in moving
from theory to practice is the poor conditioning of naturally-occurring deconvolution problems: we
show how to address this with a combination of ideas from sparse optimization, such as momentum,
continuation, and reweighting. The end result is a general purpose method, which we demonstrate on
data from neural spike sorting, calcium imaging and fluorescence microscopy.
Notation. The zero-padding operator is denoted by ∣: Rp → Rm. Projection of a vector V P Rp
onto the sphere is denoted by P&p—i (V) “ v{ ∣∣v}2, and Pz(V) “ V —〈v, Zy Z denotes projection
onto the tangent space of Z P sp´1. The Riemannian gradient of a function f : SpT → R at point Z
on the sphere is given by grad f (z) “ Pz (Vf (z)).
Reproducible research. The code for implementations of our algorithms can be found online:
https://github.com/qingqu06/sparse_deconvolution.
For more details of our work on SaSD, we refer interested readers to our project website
https://deconvlab.github.io/.
2	S ymmetry and Geometry in SaSD
In this section, we begin by describing two intrinsic properties for SaSD. Later, we show how these
play an important role in the geometry of optimization and the design of efficient methods.
An important observation of the SaSD problem is that it admits multiple equivalent solutions. This is
purely due to the cyclic convolution between a0 and x0, which exhibits the trivial ambiguity2
y “ ∣ao f X0 “ (ɑs` r∣aoS) f (ɪs´` [x。]),
for any nonzero scalar α and cyclic shift s` [-]. These scale and shift symmetries create several
acceptable candidates for a0 and x0, and in the absence of further information we only expect to
recover a0 and x0 up to symmetry. Furthermore, they largely drive the behavior of certain nonconvex
optimization problems formulated for SaSD. Since the success of SaSD requires distinguishing
between overlapping copies of a0 , its difficulty also depends highly on the “similarity” of the a0 to
its shifts. Here we capture this notion using the shift-coherence of a0,
μ(ao) “ max ∣X∣ao, s` [∣ao]〉| P [0,1] .	(2)
Intuitively, the shifts of a。become closer together as μ(ao) increases (Figure 10), making objective
landscapes for optimization less favorable for recovering any specific shift of a0 .
2.1	Landscape geometry under shift-incoherence
A natural approach to solving SaSD is to formulate it as a suitable optimization problem. In this
paper we will focus on the Bilinear Lasso (BL) problem, which minimizes the squared error between
the observation y and its reconstruction a f x, plus a '「norm sparsity penalty on x,
min	[ψBL(a, x) “ 1 }y — ∣a f x}2 ' λ }x}J .	(3)
aeSpT,xpRm
Later in this section, we will see that the kernel length p should be set slightly larger than p。 .
The Bilinear Lasso is a nonconvex optimization problem, as the shift symmetries of SaSD create dis-
crete local minimizers in the objective landscape. The regularization created by problem symmetries
2We therefore assume w.l.o.g. that }a0 }2 “ 1 in this paper.
2
Published as a conference paper at ICLR 2020
(a) Near one shift
(b) Two shifts
(c) Multiple shifts
(d)/ABL	(e)3BL
Figure 1: Geometry of gABL near superpositions of shifts of a0 (Kuo et al., 2019). (a) Regions near single
shifts are strongly convex. (b) Regions between two shifts contain a saddle-point, with negative curvature
towards each shift and positive curvature orthogonally. (c) The span of three shifts. For each figure, the top shows
the function value in height, and the bottom shows function value over the sphere. (d,e) When μs(ao) « 0, the
Bilinear Lasso gBL (Q) “ minx Ψbl (q, x) and ABL gABL(Q) are empirically similar in the span of three shifts.
in nonconvex inverse problems are a fairly general phenomenon (Sun et al., 2015) and, as Kuo et al.
(2019) shows, its influence in SaSD extends beyond the neighborhoods of these local minimizers.
Kuo et al. analyzed an Approximate Bilinear Lasso (ABL) objective3 ΨABL, which satisfies
Ψabl(a, x) » Ψbl(a, x),	when μ(a) » 0.
This non-practical objective serves as a valid simplification of the Bilinear Lasso for analysis when
the true kernel is itself incoherent, i.e. μ(a0) » 0 (Figures 1d and 1e). Under its marginalization4
夕ABL(a) “ minχpRm Ψabl(a, x),	(4)
certain crucial properties regarding its curvature can be characterized for generic choices of x. The
reason we choose to partial minimize x instead of a is because (i) the problem (4) is convex w.r.t. x,
and (ii) the dimension of the subspace of a is significantly smaller than that of x (i.e., p ! m), which
is the place that the measure concentrates.
Curvature in the span of a few shifts. Suppose we set P > p0, which ensures that we can find
an a » α1 s'1[a0S + α2 S'2[a0S P sp´1 that lies near the span of two shifts of a0. If α1 » ±1
(or α2 » 0) then, under suitable conditions on a0 and x0, Kuo et al. (2019) asserts that a lies
in a strongly convex region of DABL, containing a single minimizer near S'1 [a0S (Figure 1a); the
converse is also true. A saddle-point exists nearby when α1 » α2 is balanced, characterized by large
negative curvature along the two shifts and positive curvature in orthogonal directions (Figure 1b).
Interpolating between these two cases, large negative gradients point towards individual shifts.
The behavior of DABL between two shifts of a0 — strong convexity near single shifts, and saddle-
points near balanced points — extends to regions of the sphere spanned by several shifts (Figure 1c);
we elaborate on this further in Appendix A.1. This regional landscape guarantees that a0 can be
efficiently recovered up to a signed shift using methods for first and second-order descent, as soon as
a can be brought sufficiently close to the span of a few shifts.
Optimization over the sphere. For both the Bilinear Lasso and ABL, a unit-norm constraint on
a is enforced to break the scaling symmetry between a0 and x0. Choosing the '2-norm, however,
has surprisingly strong implications for optimization. The ABL objective, for example, is piecewise
concave whenever a is sufficiently far away from any shift of a0 , but the sphere induces positive
curvature near individual shifts to create strong convexity. These two properties combine to ensure
recoverability of a0. In contrast, enforcing '1 -norm constraints often leads to spurious minimizers
for deconvolution problems (Levin et al., 2011; Benichoux et al., 2013; Zhang et al., 2017).
Initializing near a few shifts. The landscape of DABL makes single shifts of a0 easy to locate if a
is initialized near a span of a few shifts. Fortunately, this is a relatively simple matter in SaSD, as y is
3As the intention here is apply some key intuition from the ABL objective towards the Bilinear Lasso itself,
we intentionally omit the concrete form of ΨABL(Q). Readers may refer to Appendix A for more details.
4Minimizing ^abl, this is equivalent to minimizing Ψabl as X can be recovered via convex optimization.
3
Published as a conference paper at ICLR 2020
itself a sparse superposition of shifts. Therefore, one initialization strategy is to randomly choose a
length-p0 window yji “ [yi yi+1 ... yi'p0 ´1 ST from the observation and set
ap0q = Psp-1 'r 0po´i ； y ； 0pLi]).	(5)
This brings ap0q suitably close to the sum of a few shifts of a0 (Appendix A.2); any truncation effects
are absorbed by padding the ends of yri, which also sets the length for a to be p = 3p0 ´ 2.
Implications for practical computation. The (regionally) benign optimization landscape of gABL
guarantees that efficient recovery is possible for SaSD when a0 is incoherent. Applications of sparse
deconvolution, however, are often motivated by sharpening or resolution tasks (Huang et al., 2009;
Candes & Fernandez-Granda, 2014; Campisi & Egiazarian, 2016) where the motif a0 is smooth and
coherent (i.e. μ(a0) is large). The ABL objective is a poor approximation of the Bilinear Lasso in
such cases and fails to yield practical algorithms, so we should optimize the Bilinear Lasso directly.
From Figures 1d and 1e, we can see that low-dimensional subspheres spanned by shifts of a0 are
empirically similar when a0 is incoherent. Although this breaks down in the coherent case, as we
illustrate in Appendix A.3, the symmetry breaking properties of 夕BL remain present. This allows US
to apply the geometric intuition discussed here to create an optimization method that, with the help of
a number of computational heuristics, performs well in for SaSD even in general problem instances.
Algorithm 1 Inertial Alternating Descent Method (iADM)
Input:	Initializations ap0q P sp´1, X P Rm; observation y P Rm; penalty λ ≥ 0; momentum a P [0,1).
Output:	papkq , xpkq), a local minimizer of ΨBL.
Initialize ap1q “ ap0q , xp1q “ xp0q .
for k “ 1, 2, . . . until converged do
Update x with accelerated proximal gradient step:
Wpkq D XPkq + α ∙ (XPkq ´ xpk—1q)
xpk+1q D softλtk [wpkq ´ tk ∙ Vxψλ(apkq, Wpkq)‰,
where softλ(V) “ Sign(V) d max(∣v — λ∣, 0) denotes the soft-thresholding operator.
Update a with accelerated Riemannian gradient step:
Zpkq D PSP — 1 (apkq +〈a(k) ,；(k — 1)y ∙ PaPkTq (。⑻))
apk+1q D Psp—1 (Zpk)—Tk ∙ grada ψλ (Z pkq, xpk+1q)).
end for
(a) Gradient descent
Figure 2: Momentum acceleration. a) Iterates of gradient descent oscillate on ill-conditioned functions; each
marker denotes one iteration. b) Momentum dampens oscillation and speeds up convergence.
(b) GD with momentum
3	Designing a practical SaSD algorithm
Several algorithms for SaSD-type problems have been developed for specific applications, such as
image deblurring (Levin et al., 2011; Briers et al., 2013; Campisi & Egiazarian, 2016), neuroscience
(Rey et al., 2015; Friedrich et al., 2017; Song et al., 2018), and image super-resolution (Baker &
Kanade, 2002; Shtengel et al., 2009; Yang et al., 2010), or are augmented with additional structure
(Wipf & Zhang, 2014; Ling & Strohmer, 2017; Walk et al., 2017).
Here, we instead leverage the theory from Section 2 to build an algorithm for general practical
settings. In addition to applying an appropriate initialization scheme (5) and optimizing on the sphere,
we minimize the Bilinear Lasso (3) instead of the ABL (4) to more accurately account for interactions
between shifts of a0 in highly shift-coherent settings. Furthermore, we also address the negative
effects of large coherence using a number of heuristics, leading to an efficient algorithm for SaSD.
4
Published as a conference paper at ICLR 2020
Momentum acceleration. In shift-coherent settings, the Hessian of ΨBL becomes ill-conditioned5
near shifts of a0, a situation known to cause slow convergence for first-order methods (Nesterov,
2013). A remedy is to add momentum (Polyak, 1964; Beck & Teboulle, 2009) to first-order iterations,
for instance, by augmenting gradient descent on some smooth f pzq with stepsize τ with the term w,
Wpk) D ZPkq + α ∙(zpkq ´ ZPkTq)	(6)
zpk'1q D WPk) ´ T - Vf (WPkq).	(7)
Here, α controls the momentum added6. As illustrated in Figure 2, this additional term improves
convergence by reducing oscillations of the iterates for ill-conditioned problems. Momentum has
been shown to improve convergence for nonconvex and nonsmooth problems (Pock & Sabach, 2016;
Jin et al., 2018). Here we provide an inertial alternating descent method (iADM) for finding local
minimizers of ΨBL (Algorithm 1), which modifies iPALM (Pock & Sabach, 2016) to perform updates
on a via retraction on the sphere (Absil et al., 2009)7.
Algorithm 2 SaS-BD with homotopy continuation
Input: Observation y P Rm, motif size po； momentum a P [0,1); initial λp1q final λ<, penalty decrease
η P p0, 1q; precision factor δ P p0, 1q.
Output:	Solution path {(apnq, xpnq; λpnq)( for SaSD.
Set number of iterations N D [iog(λ*∕λ ⑴){log η∖.
Initialize apoq P R3p0´2 using (5), xpoq = 0 P Rm.
for n “ 1, . . . , N do
Minimize Ψλpnq to precision δλpnq with Algorithm 1:
'aPnq, Xpnq) D iADM' apnτ), χ(nτ) ； y, λpnq ,a ).
Update λpn'1q D ηλpnq.
end for
(a) λ “ 5 X 10—1	(b) λ “ 5 X lθ´2	(C) λ “ 5 X lθ´3
Figure 3: BiHnear-lasso objective 夕λ on the sphere SP´1, for P “ 3 and varying λ; brighter colors indicate
higher values. The function landscape of 夕λ flattens as sparse penalty λ decreases from left to right.
Homotopy continuation. It is also possible to improve optimization by modifying the objective
ΨBL directly through the sparsity penalty λ. Variations of this idea appear in both Zhang et al. (2017)
and Kuo et al. (2019), and can also help to mitigate the effects of large shift-coherence.
When solving (3) in the noise-free case, it is clear that larger choices of λ encourage sparser
solutions for x. Conversely, smaller choices of λ place local minimizers of the marginal objective
夕 bl (a) “ minx ΨBL(a, x) closer to signed-shifts of a° by emphasizing reconstruction quality.
When μ(ao) is large, however,夕BL becomes ill-conditioned as λ → 0 due to the poor spectral
conditioning of a0, leading to severe flatness near local minimizers and the creation spurious local
minimizers when noise is present (Figure 3). Conversely, larger values of λ limit x to a small set of
support patterns and simplify the landscape of夕bl, at the expense of precision.
It is therefore important both for fast convergence and accurate recovery for λ to be chosen appro-
priately. When problem parameters — such as noise level, p0, or θ — are not known a priori, a
homotopy continuation method (Hale et al., 2008; Wright et al., 2009; Xiao & Zhang, 2013) can be
used to obtain a range of solutions for SaSD. Using initialization (5), a rough estimate (ap1q, Xp1q)
5This is because the circulant matrix Ca0 is ill-conditioned.
6Setting α “ 0 removes momentum and reverts to standard gradient descent.
7The stepsizes tk and τk are obtained by backtracking (Nocedal & Wright, 2006; Pock & Sabach, 2016) to
ensure sufficient decrease for Ψbl (apkq, Wpkq) — Ψbl (apkq, xpk'1q), and vice versa.
5
Published as a conference paper at ICLR 2020
is obtained by solving (3) with iADM using a large choice for λp1q . This estimate is refined via a
solution path {(aPnq, XPnq; λpnq)} by gradually decreasing λpnq. By ensuring that x remains sparse
along the solution path, the objective ΨBL enjoys restricted strong convexity w.r.t. both a and x
throughout optimization (Agarwal et al., 2010). As a result, homotopy achieves linear convergence
for SaSD where sublinear convergence is expected otherwise (Figures 4c and 4d). We provide a
complete algorithm for SaSD combining Bilinear Lasso and homotopy continuation in Algorithm 2.
4 Experiments
4.1	Synthetic experiments
Here we perform SaSD in simulations on both coherent and incoherent settings. Coherent kernels
are discretized from the GaUSSian window function a0 “ gp0,0.5, where gp,σ “ PSp´i ([exp (一
[i´p´ɪ) )‰P=1). Incoherent kernels a0 〜UnifpSp0T) are sampled uniformly on the sphere.
(a) Incoherent a0
(b) Coherent a0
IoglOmO)
(c) Incoherent a0
(d) Coherent a0
(a) Simulated kernel recovery
Figure 4: Synthetic experiments for Bilinear Lasso. Success probability (a, b): x0 „i.i.d. BRpθq, the
success probability of SaS-BD by solving (3), shown by increasing brightness, is large when the sparsity
rate θ is sufficiently small compared to the length of a0 , and vice versa. Success with a fixed sparsity rate
is more likely when a0 is incoherent. Algorithmic convergence (c, d): iterate convergence for iADM with
αk “ pk ´ 1q{pk ` 1q vs. αk “ 0 (ADM); with and without homotopy. Homotopy significantly improves
convergence rate, and momentum improves convergence when a0 is coherent.
(c) Real calcium signal vs. reconstruction
Times [s]
(b) Spike train estimates (simulated)
(d) Spike train estimates (real data)
Figure 5: Deconvolution for calcium imaging using Algorithm 2 with iADM and with reweighting (Ap-
pendix B). Simulated data: (a) recovered AR2 kernel; (b) estimate of spike train. Real data: (c) reconstructed
calcium signal (d) estimate of spike train. Reweighting improves estimation quality in each case.
Recovery performance. We test recovery probability for varying kernel lengths p0 and sparsity
rates θ. To ensure the problem size is sufficiently large, we set m “ 100p0. For each p0 and θ, we
randomly generate8 x „i.i.d. BRpθ) for both coherent and incoherent a0. We solve ten trials of (3) on
lθ´2
clean observation data a0 f x0 using iADM With λ “ ?0^. The probability of recovering a signed
8BR(θ) denotes the Bernoulli-Rademacher distribution, which has values ±1 w.p. θ/2 and zero w.p. 1 一 θ.
6
Published as a conference paper at ICLR 2020
shift of a0 is shown in Figure 4. Recovery is likely when sparsity is low compared to the kernel
length. The coherent problem setting has a smaller success region compared to the incoherent setting.
Momentum and homotopy. Next, we test the performance of Algorithm 1 with momentum
(αk “ k´2 ； see Pock & Sabach (2016)) and without (α “ 0). This is done by minimizing Ψbl with
initialization (5), using clean observations with po = 102, m “ 104, and θ “ pJ3/4 for coherent and
incoherent a°. We also apply homotopy (Algorithm 2) with λp1q “ max' |〈S'[ap0qS，y〉| — see Xiao
& Zhang (2013), λ< = ?p.^, η “ 0.8, and δ = 0.1. The final solve of (3) uses precision ε< “ lθ´6,
regardless of method. Figures 4c and 4d show the comparison results on coherent problem settings.
Comparison to existing methods. Finally, we compare iADM, and iADM with homotopy, against
a number of existing methods for minimizing 夕bl. The first is alternating minimization (KUo et al.,
2019), which at each iteration k minimizes apkq with xpkq fixed using accelerated (Riemannian)
gradient descent with backtracking, and vice versa. The next method is the popular alternating
direction method of multipliers (Boyd et al., 2011). Finally, we compare against iPALM (Pock &
Sabach, 2016) with backtracking, using the unit ball constraint on a0 instead of the unit sphere.
For each method, we deconvolve signals with po = 50,m = 100po, and θ “ pj3/4 for both coherent
and incoherent a0. For both iADM, iADM with homotopy, and iPALM we set α “ 0.3. For
homotopy, we set λp1q “ max"〈s'[ap0qS, y〉|, λ< “ ?^, and δ “ 0.5. Furthermore we set η “ 0.5
or η “ 0.8 and for ADMM, we set the slack parameter to ρ “ 0.7 or ρ “ 0.5 for incoherent and
coherent a0 respectively. From Figure 6, we can see that ADMM performs better than iADM in the
incoherent case, but becomes less reliable in the coherent case. In both cases, iADM with homotopy
is the best performer. Finally, we observe roughly equal performance between iPALM and iADM.
Figure 6: Algorithmic comparison. (a) Convergence of various methods minimizing ΨBL with incoherent a0
over FFT operations used (for computing convolutions). The y-axis denotes the log of the angle between apkq
and the nearest shift of a0 , and each marker denotes five iterations. (b) Convergence for coherent a0 , and (c)
with an AR2 kernel for modeling calcium signals.
4.2	Imaging applications
Here we demonstrate the performance and generality of the proposed method. We begin with calcium
fluorescence imaging, a popular modality for studying spiking activity in large neuronal populations
(Grienberger & Konnerth, 2012), followed by stochastic optical reconstruction microscopy (STORM)
(Rust et al., 2006； Huang et al., 2008； 2010), a superresolution technique for in vivo microscopy9.
Sparse deconvolution of calcium signals. Neural spike trains created by action potentials, each
inducing a transient response in the calcium concentration of the surrounding environment. The
aggregate signal can be modeled as a convolution between the transient a0 and the spike train x0 .
Whilst a0 and x0 both encode valuable information, neither are perfectly known ahead of time.
Here, we first test our method on synthetic data generated using an AR2 model for a0 , a shift-
coherent kernel that is challenging for deconvolution, see e.g. Friedrich et al. (2017). We set
xo 〜i.i.d. BernoUlli(p´"5) P R104 with additive noise n 〜i.i.d. N(0, 5 ∙ 10^2). Figures 5a and 5b
demonstrate accurate recovery of a0 and x0 in this synthetic setting. Next, we test our method on
real data10； Figures 5c and 5d demonstrate recovery of spike locations. Although iADM provides
9Other superresolution methods for microscopy include photoactivated localization microscopy (PALM)
(Betzig et al., 2006), and fluorescence photoactivation localization microscopy (fPALM) (Hess et al., 2006).
10Obtained at http://spikefinder.codeneuro.org.
7
Published as a conference paper at ICLR 2020
decent performance, in the presence of large noise estimation quality can be improved by stronger
sparsification methods, such as the reweighting technique by Candes et al. (2008), which we elaborate
on in Appendix B. Additionally, Figure 6c shows that the proposed method converges to higher
precision in comparison with state-of-the-art methods.
(a) Frame 100, time = 4s
(b) Frame 200, time = 8s
(c) Original (d) Resolved
Figure 7: SaSD for STORM imaging. (a, b) Individual frames (left) and predicted point process map using
SaSD (right). (c, d) shows the original microscopy and the super-resolved image obtained by our method.
(a) Calcium image Y
(c) Reconstruction Ak f Xk pk “ 1, 2q
(b) Estimated kernels Ak
(d) Predicted activation maps Xk
Figure 8: Classification of calcium images. (a) Original calcium image; (b) respective kernel estimates; (c)
reconstructed images with the (left) neuron and (right) dendrite kernels; (d) respective occurence map estimates.
Super-resolution for fluorescence microscopy. Fluorescence microscopy is often spatially limited
by the diffraction of light; its wavelength (several hundred nanometers) is often larger than typical
molecular length-scales in cells, preventing a detailed characterization of subcellular structures. The
STORM technique overcomes this resolution limit by using photoswitchable fluorescent probes
to multiplex the image into multiple frames, each containing a subset of the molecules present
(Figure 7). If the location of these molecules can be precisely determined for each frame, synthesizing
all deconvolved frames will produce a super-resolution microscopy image with nanoscale resolution.
For each image frame, the localization task can be formulated via the SaS model
Yt	“	ιA0	f	X0,t	`	Nt ,	(8)
loomoon	loomoon	loomoon	loomoon
STORM frame	point spread function	sparse point sources	noise
where f denotes 2D convolution. Here we will solve this task on the single-molecule localization
microscopy (SMLM) benchmarking dataset11 via SaSD, recovering both the PSF A0 and the point
source maps X0,t simultaneously. We apply iADM with reweighting (Appendix B) on frames of size
128 X 128 from the video sequence “Tubulin”； each pixel is of 100nm2 resolution11 12, the fluorescence
wavelength is 690nm, and the framerate is f “ 25Hz. Figure 7 shows examples of recovered
activation maps, and the aggregated super-resolution image from all 500 frames, accurately predicting
the PSF (see Appendix D) and the activation map for each video frame to produce higher resolution
microscopy images.
Localization in calcium images. Our methods are easily extended to handle superpositions of
multiple SaS signals. In calcium imaging, this can potentially be used to track the neurons in
video sequences, a challenging task due to (non-) rigid motion, overlapping sources, and irregular
11Data can be accessed at http://bigwww.epfl.ch/smlm/datasets/index.html.
12Here We solve SaSD on the same 128 X 128 grid. In practice, the localization problem is solved on a finer
grid, so that the resulting resolution can reach 20 ´ 30 nm.
8
Published as a conference paper at ICLR 2020
background noise Pnevmatikakis et al. (2016); Giovannucci et al. (2019). We consider frames video
obtained via the two-photon calcium microscopy dataset from the Allen Institute for Brain Science13,
shown in Figure 8. Each frame contains the cross section of several neurons and dendrites, which
have distinct sizes. We model this as the SaS signal Yt “ ιA1 f X1,t ` ιA2 f X2,t, where each
summand consists of neurons or dendrites exclusively. By extending Algorithm 2 to recover each of
the kernels Ak and maps Xk, we can solve this convolutional dictionary learning (SaS-CDL; see
Appendix C) problem which allows us to separate the dendritic and neuronal components from this
image for localization of firing activity, etc. As a result, the application of SaS-CDL as a denoising or
analysis tool for calcium imaging videos provides a very promising direction for future research.
5 Discussion
Many nonconvex inverse problems, such as SaSD, are strongly regulated by their problem symmetries.
Understanding this regularity and when or how it breaks down is important for developing effective
algorithms. We illustrate this by combining geometric intuition with practical heuristics, motivated
by common challenges in real deconvolution, to produce an efficient and general purpose method that
performs well on data arising from a range of application areas. Our approach, therefore, can serve
as a general baseline for studying and developing extensions to SaSD, such as SaS-CDL (Bristow
& Lucey, 2014; Chun & Fessler, 2017; Garcia-Cardona & Wohlberg, 2018), Bayesian approaches
(Babacan et al., 2008; Wipf & Zhang, 2014), and hierarchical SaS models (Chen et al., 2013).
Acknowledgments
This work was funded by NSF 1343282, NSF CCF 1527809, and NSF IIS 1546411. QQ also
acknowledges supports from Microsoft PhD fellowship and the Moore-Sloan fellowship. We would
like to thank Gongguo Tang, Shuyang Ling, Carlos Fernandez-Granda, Ruoxi Sun, and Liam Paninski
for fruitful discussions.
References
Pierre-Antoine. Absil, Robert Mahoney, and Rodolphe Sepulchre. Optimization Algorithms on Matrix
Manifolds. Princeton University Press, 2009.
Alekh Agarwal, Sahand Negahban, and Martin J Wainwright. Fast global convergence rates of
gradient methods for high-dimensional statistical recovery. In Advances in Neural Information
Processing Systems,pp. 37-45, 2010.
S Derin Babacan, Rafael Molina, and Aggelos K Katsaggelos. Variational bayesian blind decon-
volution using a total variation prior. IEEE Transactions on Image Processing, 18(1):12-26,
2008.
Simon Baker and Takeo Kanade. Limits on super-resolution and how to break them. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence, 24(9):1167-1183, 2002.
Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse
problems. SIAM Journal on Imaging Sciences, 2(1):183-202, 2009.
Alexis Benichoux, Emmanuel Vincent, and Remi GribonvaL A fundamental pitfall in blind de-
convolution with sparse and shift-invariant priors. In ICASSP-38th International Conference on
Acoustics, Speech, and Signal Processing-2013, 2013.
Eric Betzig, George H Patterson, Rachid Sougrat, O Wolf Lindwasser, Scott Olenych, Juan S
Bonifacino, Michael W Davidson, Jennifer Lippincott-Schwartz, and Harald F Hess. Imaging
intracellular fluorescent proteins at nanometer resolution. Science, 313(5793):1642-1645, 2006.
Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, Jonathan Eckstein, et al. Distributed optimization
and statistical learning via the alternating direction method of multipliers. Foundations and
TrendsR in Machine learning, 3(1):1-122, 2011.
13Obtained at http://observatory.brain-map.org/visualcoding/.
9
Published as a conference paper at ICLR 2020
David Briers, Donald D Duncan, Evan R Hirst, Sean J Kirkpatrick, Marcus Larsson, Wiendelt Steen-
bergen, Tomas Stromberg, and Oliver B Thompson. Laser speckle contrast imaging: theoretical
and practical limitations. Journal of biomedical optics, 18(6):066018, 2013.
Hilton Bristow and Simon Lucey. Optimization methods for convolutional sparse coding. arXiv
preprint arXiv:1406.2407, 2014.
Patrizio Campisi and Karen Egiazarian. Blind image deconvolution: theory and applications. CRC
press, 2016.
Emmanuel J Candes and Carlos Fernandez-Granda. Towards a mathematical theory of super-
resolution. Communications on pure and applied Mathematics, 67(6):906-956, 2014.
Emmanuel J Candes, Michael B Wakin, and Stephen P Boyd. Enhancing sparsity by reweighted `1
minimization. Journal of Fourier analysis and applications, 14(5-6):877-905, 2008.
Bo Chen, Gungor Polatkan, Guillermo Sapiro, David Blei, David Dunson, and Lawrence Carin. Deep
learning with hierarchical convolutional factor analysis. IEEE transactions on pattern analysis
and machine intelligence, 35(8):1887-1901, 2013.
Sky C Cheung, John Y Shin, Yenson Lau, Zhengyu Chen, Ju Sun, Yuqian Zhang, John N Wright,
and Abhay N Pasupathy. Dictionary learning in fourier transform scanning tunneling spectroscopy.
arXiv preprint arXiv:1807.10752, 2018.
Il Yong Chun and Jeffrey A Fessler. Convolutional dictionary learning: Acceleration and convergence.
IEEE Transactions on Image Processing, 27(4):1697-1712, 2017.
Chaitanya Ekanadham, Daniel Tranchina, and Eero P Simoncelli. A blind sparse deconvolution
method for neural spike identification. In Advances in Neural Information Processing Systems, pp.
1440-1448, 2011.
Johannes Friedrich, Pengcheng Zhou, and Liam Paninski. Fast online deconvolution of calcium
imaging data. PLoS Computational Biology, 13(3):e1005423, 2017.
Cristina Garcia-Cardona and Brendt Wohlberg. Convolutional dictionary learning: A comparative
review and new algorithms. IEEE Transactions on Computational Imaging, 4(3):366-381, 2018.
Andrea Giovannucci, Johannes Friedrich, Pat Gunn, Jeremie Kalfon, Brandon L Brown, Sue Ann
Koay, Jiannis Taxidis, Farzaneh Najafi, Jeffrey L Gauthier, Pengcheng Zhou, et al. Caiman an
open source tool for scalable calcium imaging data analysis. Elife, 8:e38173, 2019.
Christine Grienberger and Arthur Konnerth. Imaging calcium in neurons. Neuron, 73(5):862-885,
2012.
Elaine T Hale, Wotao Yin, and Yin Zhang. Fixed-point continuation for zell_1-minimization:
Methodology and convergence. SIAM Journal on Optimization, 19(3):1107-1130, 2008.
Samuel T Hess, Thanu PK Girirajan, and Michael D Mason. Ultra-high resolution imaging by
fluorescence photoactivation localization microscopy. Biophysical journal, 91(11):4258-4272,
2006.
Bo Huang, Wenqin Wang, Mark Bates, and Xiaowei Zhuang. Three-dimensional super-resolution
imaging by stochastic optical reconstruction microscopy. Science, 319(5864):810-813, 2008.
Bo Huang, Mark Bates, and Xiaowei Zhuang. Super-resolution fluorescence microscopy. Annual
Review of Biochemistry, 78:993-1016, 2009.
Bo Huang, Hazen Babcock, and Xiaowei Zhuang. Breaking the diffraction barrier: super-resolution
imaging of cells. Cell, 143(7):1047-1058, 2010.
Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape saddle
points efficiently. In Proceedings of the 34th International Conference on Machine Learning, pp.
1724-1732, 2017.
10
Published as a conference paper at ICLR 2020
Chi Jin, Praneeth Netrapalli, and Michael I Jordan. Accelerated gradient descent escapes saddle
points faster than gradient descent. In Conference On Learning Theory, pp. 1042-1085, 2018.
Han-Wen Kuo, Yuqian Zhang, Yenson Lau, and John Wright. Geometry and symmetry in short-and-
sparse deconvolution. In International Conference on Machine Learning (ICML), June 2019.
Anat Levin, Yair Weiss, Fredo Durand, and William T Freeman. Understanding blind deconvolution
algorithms. IEEE Transactions on Pattern Analysis and Machine Intelligence, 33(12):2354-2367,
2011.
Shuyang Ling and Thomas Strohmer. Blind deconvolution meets blind demixing: Algorithms and
performance bounds. IEEE Transactions on Information Theory, 63(7):4497-4520, 2017.
Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer
Science & Business Media, 2013.
Jorge Nocedal and Stephen Wright. Numerical optimization. Springer Science & Business Media,
2006.
Eftychios A Pnevmatikakis, Daniel Soudry, Yuanjun Gao, Timothy A Machado, Josh Merel, David
Pfau, Thomas Reardon, Yu Mu, Clay Lacefield, Weijian Yang, et al. Simultaneous denoising,
deconvolution, and demixing of calcium imaging data. Neuron, 89(2):285-299, 2016.
Thomas Pock and Shoham Sabach. Inertial proximal alternating linearized minimization (ipalm) for
nonconvex and nonsmooth problems. SIAM Journal on Imaging Sciences, 9(4):1756-1787, 2016.
Boris T Polyak. Some methods of speeding up the convergence of iteration methods. USSR
Computational Mathematics and Mathematical Physics, 4(5):1-17, 1964.
Hernan Gonzalo Rey, Carlos Pedreira, and Rodrigo Quian Quiroga. Past, present and future of spike
sorting techniques. Brain Research Bulletin, 119:106-117, 2015.
Michael J Rust, Mark Bates, and Xiaowei Zhuang. Sub-diffraction-limit imaging by stochastic optical
reconstruction microscopy (storm). Nature Methods, 3(10):793, 2006.
Gleb Shtengel, James A Galbraith, Catherine G Galbraith, Jennifer Lippincott-Schwartz, Jennifer M
Gillette, Suliana Manley, Rachid Sougrat, Clare M Waterman, Pakorn Kanchanawong, Michael W
Davidson, et al. Interferometric fluorescent super-resolution microscopy resolves 3d cellular
ultrastructure. Proceedings of the National Academy of Sciences, 106(9):3125-3130, 2009.
Andrew H Song, Francisco Flores, and Demba Ba. Spike sorting by convolutional dictionary learning.
arXiv preprint arXiv:1806.01979, 2018.
Ju Sun, Qing Qu, and John Wright. When are nonconvex problems not scary? arXiv preprint
arXiv:1510.06096, 2015.
Philipp Walk, Peter Jung, Gotz E Pfander, and Babak Hassibi. Blind deconvolution with additional
autocorrelations via convex programs. arXiv preprint arXiv:1701.04890, 2017.
David Wipf and Haichao Zhang. Revisiting bayesian blind deconvolution. The Journal of Machine
Learning Research, 15(1):3595-3634, 2014.
Stephen J Wright, Robert D Nowak, and Mdrio AT Figueiredo. Sparse reconstruction by separable
approximation. IEEE Transactions on Signal Processing, 57(7):2479-2493, 2009.
Lin Xiao and Tong Zhang. A proximal-gradient homotopy method for the sparse least-squares
problem. SIAM Journal on Optimization, 23(2):1062-1091, 2013.
Jianchao Yang, John Wright, Thomas S Huang, and Yi Ma. Image super-resolution via sparse
representation. IEEE Transactions on Image Processing, 19(11):2861-2873, 2010.
Florence Yellin, Benjamin D Haeffele, and Rene Vidal. Blood cell detection and counting in
holographic lens-free imaging by convolutional sparse dictionary learning and coding. In IEEE
14th International Symposium on Biomedical Imaging, pp. 650-653. IEEE, 2017.
11
Published as a conference paper at ICLR 2020
Yuqian Zhang, Yenson Lau, Han-Wen Kuo, Sky Cheung, Abhay Pasupathy, and John Wright. On
the global geometry of sphere-constrained sparse blind deconvolution. In Computer Vision and
PatternRecognition (CVPR), 2017 IEEE Conference on,pp. 4381-4389. IEEE, 2017.
Yin Zhou, Hang Chang, Kenneth Barner, Paul Spellman, and Bahram Parvin. Classification of
histology sections via multispectral convolutional sparse coding. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pp. 3081-3088, 2014.
A Approximate Bilinear Lasso Objective
Recall from Section 2.2 of the main text that SaSD can be formulated as the Bilinear Lasso problem
aPSpm1inpRm [ψBLpa, xq = 2 }y Ta f x}2 + λ }X}l].	⑼
Unfortunately, this objective is challenging for analysis. A major culprit is that its marginalization
夕 BL(a) “ mχin{ 1 }y — ιa f x}2 + λ }x}ι },
(10)
generally does not admit closed form solutions due the convolution with a in the squared error term.
This motivates Kuo et al. (2019) to study the nonconvex formulation
占PminC ,ABL(a, xq “ 2 }x}2 ´ X∣a f x, y>+}y}2 + λ }x}J .	(11)
We refer to (11) as the Approximate Bilinear Lasso formulation, and it is quite easy to see that
ΨABL(a, xq « ΨBL(a, xq When }a f x}2 « }x}2, i.e. if a is shift-incoherent, or μ(a) « 0. The
marginalized objective function 夕bl (a) = minx Wdq(。, xq now has the closed form expression
^ABL (aq = — 2 }softλ [q f yS}2 .	(12)
Here soft denotes the elementwise soft-thresholding operator Softt(Xiq = Sign(χj∙ max(∣xi | — t, 0q,
and aq denotes the adjoint kernel of a, i.e. the kernel s.t. xιa f u, vy = xu, aq f vy @u, v P Rm .
A. 1 Landscape Geometry
The rest of Section 2.2 discusses the regional characterization of 夕ABL in the span of a small number
of shifts from a0 . This language is made precise in the form of the subsphere
SI = { ∑'PI ɑ`s` [ιaoS : ɑ` P R ( ∩ sp´1,	(13)
spanned by a small set of cyclic shifts of ιa0 . Although we will not discuss the explicit distance
function here, the characterization by Kuo et al. (2019) holds whenever a is close enough to such
a subsphere with |I| ≤ 4θpo, where θ is the probability that any individual entry of xo is nonzero.
Suppose we have a « ∑'pI ɑ`s` [ιao] for some appropriate index set I. Note that if μsa0 « 0,
then μsa « 0, Va P SI. Now let apιq and ap2q be the first and second largest coordinates of the
shifts participating in a, and let sp1q ra0s and sp2q ra0s be the corresponding shifts. Then
•	If Iααp2q∣ « 0, then a is in a strongly convex region of 夕abl, containing a single local
minimizer corresponding to sp1q ra0s.
•	If ∣ ααp2q ∣ « 1, then a is near a saddle-point, with negative curvature pointing towards spιq [a。]
and s(2)[a。]. If ∣ααp3q ∣ « 0 , i.e. spιq [a。] and sp2q [a。] are the only two participating shifts,
αp2q
then 夕ABL is also characterized by positive curvature in all orthogonal directions.
•	Otherwise,〈一grad夕ABL(aq, Z — a〉takes on a large positive value, for either U = s(i)[a。]
or u = sp2q [a。], i.e. the negative Riemannian gradient is large and points towards one of the
participating shifts.
12
Published as a conference paper at ICLR 2020
一/yA|« z —
Initialization ap0q	aisi ra0s ' aj SjraOs
Figure 9: Data-driven initialization for a: using a piece of the observed data y to generate a good initial point
ap0q. Top: data y “ a0 f x0 is a superposition of shifts of the true kernel a0. Bottom: a length-p0 window
contains pieces of just a few shifts. Bottom-center: one step of the generalized power method approximately fills
in the missing pieces, yielding an initialization that is close to a linear combination of shifts of a0 (right).
This is an example of a ridable saddle property (Jin et al., 2017) that allows many first and second-
order methods to locate local minimizers. Since all local minimizers of 夕ABL near SI must correspond
to signed-shifts of a0 , this guarantees that the Approximate Bilinear Lasso formulation can be
efficiently solved to recover a0 (and subsequently x0) for incoherent a0, as long as a is initialized
near some appropriate subsphere and the sparsity coherence tradeoff poθ W (μs (ao))T{2 is satisfied.
We note that this is a poor tradeoff rate, which reflects that the Approximate Bilinear Lasso formulation
is non-practical and cannot handle SaSD problems involving kernels with high shift-coherence.
A.2 Data-driven initialization
For the SaS-BD problem, we usually initialize x by xp0q “ 0, so that our initialization is sparse.
For the optimization variable a P Rn, recall from Section 2.2 in the main text that it is desirable to
obtain an initialization a0 which is close to the intersection of SpT and a subsphere SI spanned by
a few shifts of a0. When x0 is sparse, our measurement y is a linear combination of a few shifts
of ao. Therefore, an arbitrary consecutive po-length window 负 “ [yi ymi...期匕上―、ST of the
data y should be not far away from such a subspace SI . As illustrated in Figure 9, one step of the
generalized power method (Kuo et al., 2019)
rp0q= PSpT `r op´i ； ri ； op´is)	(14)
ap0q “ Psp-i (´v^abl (rp°q))	(15)
produces a refined initialization that is very close to a subspace SI spanned by a few shifts of a0 with
|I| « θp0. However, (15) is a relatively complicated for a simple idea. In practice, we find that the
simple initialization ap0q “ arp0q from (14) works suitably well for solving SaSD with (9).
A.3 Comparison to the Bilinear Lasso
Although it is easy to see that WablP。) and WblP。) are similar as long as μ(a) « 0, it is also clear
that these two quantities can be very different when μ(a) is large. This is especially significant when
μ(ao) is itself large, as the desired solutions for a are then also coherent.
From Figure 10, we can see that these changes are reflected in the low-dimensional subspheres (13)
spanned by adjacent shifts of a°. Compared to the incoherent case,夕BL also takes on small values in
regions between adjacent shifts, creating a “global valley” on the subsphere. Theoretically, this makes
it difficult to ensure exact recovery of up to symmetry when a0 is coherent, and the objective function
becomes much more complicated. This is not a significant issue in terms of practical computation,
however, since adjacent shifts of a° become indistinguishable as μ(ao) → 1, meaning that one only
needs to ensure that a lands in the “global valley” to achieve good estimates of a0 up to symmetry.
13
Published as a conference paper at ICLR 2020
(a)夕ABL, incoherent
(b)夕bl, incoherent
(c)夕bl, coherent
Figure 10: Low-dimensional subspheres spanned by shifts of a0. Subfigures (a,b) present the optimization
landscapes of 夕ABL(a) and 夕BL(a), for a P SpT ∩span{ao,sι [ao],S2 [a。U, with higher values being brighter.
The red dots denote the shifts of a。. Subfigure (c) shows the landscape 夕BL when a。is coherent, which
significantly departs from the landscapes of (a,b), but still retains symmetry breaking curvature.
B Reweighted sparse penalization
When a0 is shift-coherent, minimization of the objective ΨBL with respect to x becomes sensitive
to perturbations, creating “smudging” effects on the recovered map x. These resolution issues can
be remedied with stronger concave regularizers. A simple way of facilitating this with the Bilinear
Lasso is to use a reweighting technique (Candes et al., 2008). The basic idea is to adaptively adjust
the penalty by considering a weighted variant of the original Bilinear Lasso problem from (9),
min	ΨwL(a, x) “ 1 }y — a f x}2 ' λ }w d x}1	(16)
apSP—1, XPRm
where W P Rm and d denotes the Hadamard product. Here We will set the weights W to be roughly
inverse to the magnitude of the true signal x0 , i.e.,
1
Wi = 1 I I L
|x0,i| ` ε
(17)
Algorithm 3 Reweighted Bilinear Lasso
Input: Initializations ap0q, X p0q, penalty λ > 0
Output: Local minimizers aPjq, XPjq of ΨWLpjq.
Initialize wp1q “ 1m, j D L
while not converged do
Using the initialization 'apj´1), Xpj´1)) and weight WPjq, solve (16) — e.g. with iADM — to
obtain solution 'aPjq, XPjq);
Set ε with (19) and update the weights as
WPj'1q “ ∣-.	(18)
IXPjq I ' ε
Update ' D ' ` L
end while
In addition to choosing λ > 0, here ε > 0 trades off between sparsification strength (small ε) and
algorithmic stability (large ε). Let |x|Piq denote the i-th largest entry of |X|. For experiments in the
main text, we set
ε “ max { |x|(rn/log(m/n)S) , 10—3}.	(19)
Starting with the initial weights WP0q “ 1m, Algorithm 3 successively solves (16), updating the
weights using (17) at each outer loop iteration j. As j → 8, this method becomes equivalent to
replacing the '1 -norm in (9) with the nonconvex penalty Xi log(∣xi | ' ε) (Candes et al., 2008).
We can easily adopt our iADM algorithm to solve this subproblem, by taking the proximal gradient
on x with a different penalty λi for each entry xi. Figure 11, as well as calcium imaging experiments
in Section 4.2, Figure 5 of the main text, demonstrate improved estimation as a result of this method.
14
Published as a conference paper at ICLR 2020
(a) True map x0	(c) Noisy y , `1 only	(e) Noisy y, reweighted
(b) True motif a0	(d) Noisy a, `1 only	(f) Noisy a, reweighted
Figure 11: Recovery of xo with '1 -reweighting. (a, b) Truth signals. (c) Solving min。Ψbl(q, x) With noisy
data and coherent a0 leads to low-quality estimates of x; (d) performance suffers further when a is a noisy
estimate of a0 . (e, f) ReWeighted `1 minimization alleviates this issue significantly.
C Extension for convolutional dictionary learning
y
a0,1
a0,2 f
x0,2
h 9 T
f
Figure 12:	Convolutional dictionary learning. Simultaneous recovery for multiple unknoWn kernels
{a0,kUN“1 and sparse activation maps {x0,k}N“1 from y “ EN=I a。# f xo,k.
(a) PSF in 2D
(b) PSF in 3D
Figure 13:	Estimated PSF for STORM imaging. The left hand side shows the estimated 8 X 8 PSF in 2D,
the right hand side visualizes the PSF in 3D.
The optimization methods we introduced for SaSD here can be naturally extended for sparse blind
deconvolution problems with multiple kernels/motifs (a.k.a. convolutional dictionary learning; see
Garcia-Cardona & Wohlberg (2018)), which have broad applications in microscopy data analysis
(Yellin et al., 2017; Zhou et al., 2014; Cheung et al., 2018) and neural spike sorting (Ekanadham
et al., 2011; Rey et al., 2015; Song et al., 2018). As illustrated in Figure 12, the new observation y is
15
Published as a conference paper at ICLR 2020
the sum of N convolutions between short kernels ta0,kukN“1 and sparse maps tx0,kukN“1,
N
y “ X ∣ao,k f xo,k, ao,k P Rp0,	xo,k P Rm,	(1 ≤ k ≤ N).	(20)
k“1
The natural extension of SaSD, then, is to recover ta0,kukN“1 and tx0,kukN“1 up to signed, shift, and
permutation ambiguities, leading to the SaS convolutional dictionary learning (SaS-CDL) problem.
The SaSD problem can be seen as a special case of SaS-CDL with N “ 1. Based on the Bilinear
Lasso formulation in (9) for solving SaSD, we constrain all kernels a0,k over the sphere, and consider
the following nonconvex objective:
1
min -
takukN“1, txkukN“1 2
N
y ´ ak f xk
k“1
+ λ ∑ }xk}1,
s.t. ak P SpT	(1 ≤ k ≤ N). (21)
Similar to the idea of solving the Bilinear Lasso in (9), we optimize (21) via iADM, by taking
alternating descent steps on takukN“1 and txkukN“1 with the other variable fixed.
D Super-resolution with STORM imaging
For point source localization in STORM frames, recall that we use the SaS model from Section 4.2.2,
Yt	“	ιA0	f	X0,t	+	Nt .	(22)
lomon	lomon	lomon	lomon
STORM frame	point spread function	sparse point sources	noise
We then apply our SaSD method to recover both A0 and X0,t from Yt. We show our recovery of
X0,t as well as the super-resolved image using all available frames in Figure 6 of the main text. Since
the main objective of STORM imaging is to recover the point sources, we have deferred the recovered
PSF A0 to Figure 13 here.
16