Published as a conference paper at ICLR 2020
ProxSGD: Training Structured Neural Net-
works under Regularization and Constraints
Yang Yang	Yaxiong Yuan
Fraunhofer ITWM	University of Luxembourg
Fraunhofer Center Machine Learning	yaxiong.yuan@uni.lu
yang.yang@itwm.fraunhofer.de
Avraam Chatzimichailidis
Fraunhofer ITWM
TU Kaiserslautern
avraam.chatzimichailidis@itwm.fraunhofer.de
Ruud JG van Sloun	Lei Lei, Symeon Chatzinotas
Eindhoven University of Technology University of Luxembourg
r.j.g.v.sloun@tue.nl	{lei.lei, symeon.chatzinotas}@uni.lu
Ab stract
In this paper, we consider the problem of training structured neural networks (NN)
with nonsmooth regularization (e.g. '1 -norm) and constraints (e.g. interval Con-
straints). We formulate training as a constrained nonsmooth nonconvex optimiza-
tion problem, and propose a convergent proximal-type stochastic gradient descent
(ProxSGD) algorithm. We show that under properly selected learning rates, with
probability 1, every limit point of the sequence generated by the proposed Prox-
SGD algorithm is a stationary point. Finally, to support the theoretical analysis
and demonstrate the flexibility of ProxSGD, we show by extensive numerical tests
how ProxSGD can be used to train either sparse or binary neural networks through
an adequate selection of the regularization function and constraint set.
1 Introduction
In this paper, we consider the problem of training neural networks (NN) under constraints and reg-
ularization. It is formulated as an optimization problem
1m
minimize - Vfi(x, yi) +r(x),
x∈X⊆Rn m
i=1
X----{z---}
,f(x)
(1)
where x	is the parameter vector to optimize, yi is the i-th training example which consists of the
training input and desired output, and m is the number of training examples. The training loss f
is assumed to be smooth (but nonconvex) with respect tox, the regularization r is assumed to be
convex (but nonsmooth), proper and lower semicontinuous, and the constraint set X is convex and
compact (closed and bounded).
When r(x)	= 0 andX = Rn, stochastic gradient descent (SGD) has been used to solve the optimiza-
tion problem (1). At each iteration, a minibatch of the m training examples are drawn randomly,
and the obtained gradient is an unbiased estimate of the true gradient. Therefore SGD generally
moves along the descent direction, see Bertsekas & Tsitsiklis (2000). SGD can be accelerated by
replacing the instantaneous gradient estimates by a momentum aggregating all gradient in past it-
erations. Despite the success and popularity of SGD with momentum, its convergence had been an
open problem. Assuming f is convex, analyzing the convergence was first attempted in Kingma &
Ba (2015) and later concluded in Reddi et al. (2018). The proof for a nonconvex f was later given
in Chen et al. (2019); Lei et al. (2019).
1
Published as a conference paper at ICLR 2020
In machine learning, the regularization function r is typically used to promote a certain structure
in the optimal solution, for example sparsity as in, e.g., feature selection and compressed sensing,
or a zero-mean-Gaussian prior on the parameters (Bach et al., 2011; Boyd et al., 2010). It can be
interpreted as a penalty function since at the optimal point x? of problem (1), the value r(x?) will
be small. One nominant example is the Tikhonov regularization r(x) = μ∣∣x∣∣2 for some predefined
constant μ, and it can be used to alleviate the ill-conditioning and ensure that the magnitude of the
weights will not become exceedingly large. Another commonly used regularization, the '1 -norm
where r(x) = μ∣∣x∣∣ι = μP；=i |xj| (the convex surrogate of the 'o-norm), would encourage a
sparse solution. In the context of NN, it is used to (i) promote a sparse neural network (SNN) to
alleviate overfitting and to allow a better generalization, (ii) accelerate the training process, and (iii)
prune the network to reduce its complexity, see Louizos et al. (2018) and Gale et al. (2019).
Technically, it is difficult to analyze the regularizations as some commonly used convex regularizers
are nonsmooth, for example, `1 -norm. In current implementations of TensorFlow, the gradient of
|x| is simply set to 0 when x = 0. This amounts to the stochastic subgradient descent method and
usually exhibits slow convergence. Other techniques to promote a SNN includes magnitude pruning
and variational dropout, see Gale et al. (2019).
Although regularization can be interpreted as a constraint from the duality theory, sometimes it may
still be more desirable to use explicit constraints, for example, P xj2 ≤ α, where the summation is
over the weights on the same layer. This is useful when we already know how to choose α. Another
example is the lower and upper bound on the weights, that is, l ≤ w ≤ u for some predefined l
and u. Compared with regularization, constraints do not encourage the weights to stay in a small
neighborhood of the initial weight, see Chapter 7.2 of Goodfellow et al. (2016) for more details.
The set X models such explicit constraints, but it poses an additional challenge for stochastic gradient
algorithms as the new weight obtained from the SGD method (with or without momentum) must be
projected back to the set X to maintain its feasibility. However, projection is a nonlinear operator,
so the unbiasedness of the random gradient would be lost. Therefore the convergence analysis for
constrained problems is much more involved than unconstrained problems.
In this paper, we propose a convergent proximal-type stochastic gradient algorithm (ProxSGD) to
train neural networks under nonsmooth regularization and constraints. It turns out momentum plays
a central role in the convergence analysis. We establish that with probability (w.p.) 1, every limit
point of the sequence generated by ProxSGD is a stationary point of the nonsmooth nonconvex
problem (1). This is in sharp contrast to unconstrained optimization, where the convergence of the
vanilla SGD method has long been well understood while the convergence of the SGD method with
momentum was only settled recently. Nevertheless, the convergence rate of ProxSGD is not derived
in the current work and is worth further investigating.
To test the proposed algorithm, we consider two applications. The first application is to train a SNN,
and we leverage `1 -regularization, that is,
1m
minimize 一 旌 fi(x, yi) + μ∣∣x∣∣ι.	(2)
xm
i=1
The second application is to train a binary neural network (BNN) where the weights (and activations)
are either 1 or -1 (see Courbariaux et al. (2015; 2016); Hou et al. (2017); Yin et al. (2018); Bai et al.
(2019) for more details). To achieve this, we augment the loss function with a term that penalizes
the weights if they are not +1 or -1:
1m	n
minimize — Efi(x, yi) + μ f(a,j (Xj + 1)2 + (1 - aj )(xj - 1)2)
x,a	m	4
i=1	j =1
subject to aj = 0 or 1,	j = 1, . . ., n,
where μ is a given penalty parameter. The binary variable aj can be interpreted as a switch for
weight xj: when aj = 0, (1 - aj)(xj - 1)2 is activated, and there is a strong incentive for xj to be
1 (the analysis for aj = 1 is similar). Since integer variables are difficult to optimize, we relax aj
to be a continuous variable between 0 and 1. To summarize, a BNN can be obtained by solving the
2
Published as a conference paper at ICLR 2020
following regularized optimization problem under constraints with respect to x and a
1m	n
minimize — Efi(x, yi) + μ f(a,j (Xj + 1)2 + (1 - aj )(xj - 1)2)
x,a m	4
i=1	j =1
subject to 0 ≤ aj ≤ 1, j = 1, . . ., n.	(3)
If μ is properly selected (or sufficiently large), the optimal aj- will be exactly or close to 0 or 1.
Consequently, regularization and constraints offer interpretability and flexibility, which allows us to
use more accurate models to promote structures in the neural networks, and the proposed convergent
ProxSGD algorithm ensures efficient training of such models.
2 The proposed Prox S GD algorithm
In this section, we describe the ProxSGD algorithm to solve (1).
Background and setup. We make the following blanket assumptions on problem (1).
•fi (x, y(i)) is smooth (continuously differentiable) but not necessarily convex.
•	Vχ fi(x, y(i)) is LiPSChitz continuous with a finite constant Li for any yi. Thus V/(x) is
LiPSChitz continuous with constant L，ml Pm=I Li.
•	r(x) is convex, proper and lower semicontinuous (not necessarily smooth).
•	X is convex and compact.
We are interested in algorithms that can find a stationary point of (1). A stationary point x? satisfies
the optimality condition: atx =x? ,
(x	-x?)TVf	(x?)	+ r(x)	- r(x?) ≥ 0,	∀x	∈ X.	(4)
When r(x) = 0 and X = Rn, the deterministic optimization problem 1 can be solved by the (batch)
gradient descent method. When m, the number of training examples, is large, it is computationally
expensive to calculate the gradient. Instead, we estimate the gradient by a minibatch of m(t) training
examples. We denote the minibatch by M(t): its elements are drawn uniformly from {1， 2， . . . ， m}
and there are m(t) elements. Then the estimated gradient is
g(t),焉 X Vfi(X(t)，y(i))
i∈M(t)
(5)
and it is an unbiased estimate of the true gradient.
The proposed algorithm. The instantaneous gradient g(t) is used to form an aggregate gradient
(momentum) v(t), which is updated recursively as follows
v(t) = (1 — ρ(t))v(t - 1) + ρ(t)g(t),	(6)
where ρ(t) is the stepsize (learning rate) for the momentum and ρ(t) ∈ (0， 1].
At iteration t, we propose to solve an approximation subproblem and denote its solution as
b((t)， v(t)， τ (t)), or simply xb(t)
b(t)，argmin I(X - x(t))Tv(t) + 1(x - x(t))Tdiag(τ(t))(x - x(t)) + r(x)∣ .	(7)
x∈X	2
A quadratic regularization term is incorporated so that the subproblem (7) is strongly convex and its
modulus is the minimum element of the vector T(t), denoted as T(t) and T(t) = minj=ι,…,n Tj(t).
Note that T(t) should be lower bounded by a positive constant that is strictly larger than 0, so that
the quadratic regularization in (7) will not vanish.
The difference between two vectors Xb(t) and X(t) specifies a direction starting at X(t) and ending
at Xb(t). This update direction is used to refine the weight vector
X(t + 1) = X(t) + (t)(Xb(t) - X(t))，	(8)
3
PUbHShed as a COnferenCe PaPer at ICLR 2020
ADABound	AMSGrad		RMSProP I		AdaGrad	|	SGD (w. momentum)]	ProXSGD	I		algorithm	∣
飞	飞	飞	1	1	1	P 3	ImomelItUml
1	C		6	6	6	、土	IWeightl
	i	* dip(e(t) / ^r(t) ,τ1ι(t) ,τ1u(t))	N 、上 Il ⅜ ++	⅞ + ++	，r⑴十短	+ f	1	T 3	I quadratic gain in SUbProblem ∣
O	O	O	O	O	O	I COnVeX |	IregUlariZatkml
石 3	石 3	得	石 3	石 3	石 3	I convex, ComPaet	I CCmStrailIt Set
TablerCornlect5'!! between the PrOPOSed framework and existing method-where PJ 产 C and b are
SOmePredefiIledCOlIStant∙~rsH r(E — 1) +gsΘg(-HrsH ∕⅛(EI 1) + (1 — 玄)gsΘgs∙
Where C(E) is a StePSiZe (learning rate) for the Weight and C(E) m (Oj-• NOte thath(e + 1) is feasible
as long as(E) is feasibley as it is the COIlVeX COmbiIlatiOIl OftWOfeaSibIe POiiltS(e) and(E) WhiIe
the Set 次 is convex，
The above StePS (5)—(8) are SUmmariZed in Algorithm LWhiCh is termed PrOXimalItyPe Stochastic
Gmdient DeSCent (PrOXSGDfor the reason that the explicit COlIStraint 2 m 次 in (7) Can also be
formulated impHCitIy as a regularization functiony more SPeCifiCaπyy the indicator function b(If
all elements OfTare equalythenE)is exactly the proximal OPemtOr
H>s
AIgo=∙thm 1 PrOXimalItyPe Stochastic GmdientDeSCent(PrOXSGD) MethOd
Input:^0) m 悌~—l) UaIOV≈s'2sʌ-工s
forE U O 二 4do
L COmPUte the instantaneous gradient gbased on the minibatch 富-
gsu+MF"Hsy≡∙
m£ M
/ )冷 M
2∙ UPdate the momentumU(Il P)EI 1) + Pg∙
COmPUteE) by solving the approXimation SUbPrOemi
(E) H arg min ( (—(e))(e)(E) ÷ q(i(e) ) Tdiag(TG) )(—(e)) ÷)
4∙ UPdate the Weigh(e + 1) u(e) +——(e)
end for
PrOXSGDAlgOrithm IbearS a similar StrUetUreaS SeVemI SGD algorithmwithout and With
momentumy See TabIe L and it allowsinterpret SOme existing algorithms as SPeCiaI CaSeS Ofthe
PrOPOSed framework，For exampl 尸 no momentum is USed in SGDy and this amountStO Setting
P (tj UliIl Algorithm LlIl ADAMythe learning rate for momentum is a COllStant P and the learning
rate for the Weight VeCtOriS given by c∕(l —) for SOmeand this simply amounts to Settg
P (t) H P and CUe/(1 —in AlgOrithm L ThiSterpretatn also implies that the COnVergenCe
COnditions to be PrOPOSed ShOrtIy Iaterare also SUffCient for existing algorithms (although they are
IIOt mea!πbe the WeakeSt COlIditionS available in HtemtUre )，
4
Published as a conference paper at ICLR 2020
Solving the approximation subproblem (7). Since (7) is strongly convex, xb(t) is unique. Gener-
ally xb(t) in (7) does not admit a closed-form expression and should be solved by a generic solver.
However, some important special cases that are frequently used in practice can be solved efficiently.
•	The trivial case is X = Rn and r = 0, where
xb(t)
X⑴-弛
x( ) T⑴ ,
(9)
where the vector division is understood to be element-wise. When X = Rn and r(x) = μ∣∣x∣∣ι,
xb(t) has a closed-form expression that is known as the soft-thresholding operator
b(t) = S品(x(t) - M
(10)
where Sa(b) , max(b - a, 0) - max(-b - a, 0) (Bach et al., 2011).
•	If X = Rn and r(x) = μ∣∣x∣∣2 and T(t) = TI for some T, then (Parikh & Boyd, 2014)
b(t)= ʃ(1 - μ∕kτx⑴-v(t)k2)(x(t)- v(t)/T),
if kτx(t) - v(t)∣∣2 ≥ μ,
otherwise.
(11)
If x is divided into blocks xι, x2,..., the '2-regularization is commonly used to promote block
sparsity (rather than element sparsity by `1 -regularization).
•	When there is a bound constraint l ≤ x ≤ u, xb(t) can simply be obtained by first solving the
approximation subproblem (7) without the bound constraint and then projecting the optimal point
onto the interval [l, u]. For example, when X = Rn and r = 0,
u
xb(t)
X⑴-四
x()	T (t)J
(12)
l
with [X]lu = clip(X, l, u) , min(max(X, l), u).
•	If the constraint function is quadratic: X = {X : kXk22 ≤ 1}, Xb(t) has a semi-analytical expression
(up to a scalar Lagrange multiplier which can be found efficiently by the bisection method).
Approximation subproblem. We explain why we update the weights by solving an approxi-
mation subproblem (7). First, we denote f as the smooth part of the objective function in
(7). Clearly it depends on X(t) and v(t) (and thus M(t)), while X(t) and v(t) depend on
the old weights X(0), . . . , X(t - 1) and momentum and v(0), . . . , v(t - 1). Define F(t) ,
{X(0), . . . , X(t), M(0), . . . , M(t)} as a shorthand notation for the trajectory generated by ProxSGD.
We formally write f as
f(x; F(t)) , (X - x(t))Tv(t) + 1(x - x(t))τdiag(τ(t))(x - x(t)).	(13)
It follows from the optimality of Xb(t) that
fe(X(t); F(t)) + r(X(t)) ≥ fe(Xb(t); F(t)) + r(Xb(t)).
After inserting (13) and reorganizing the terms, the above inequality becomes
(Xb(t) - X(t))τv(t) + r(Xb(t)) - r(X(t)) ≤ -T (t)kXb(t) - X(t)k22.	(14)
Since Nf (x) is Lipschitz continuous with constant L, We have
f (X(t + 1)) + r(X(t + 1)) - (f (X(t)) + r(X(t)))
≤ (x(t + 1) - x(t))τVf (x(t)) + L I∣x(t + 1) - x(t)k2 + r(x(t + 1)) -r(x(t))	(15)
≤ e(t) ((b(t) - x(t))TVf(X(t)) + r(x(t)) - r(x(t)) + Le(t)kb(t) - x(t)k2) ,	(16)
where the first inequality follows from the descent lemma (applied to f) and the second inequality
follows from the Jensen’s inequality of the convex function r and the update rule (8).
5
Published as a conference paper at ICLR 2020
If v(t) = Vf (x(t)) (which is true asymptotically as We show shortly later), by replacing Vf (x(t))
in (16) by v(t) and inserting (14) into (16), we obtain
f (X(t + I)) + r(x(t + I))Tf (x(t)) + r(x(t))) ≤ e(t) ɑe(t) - T⑴)kb⑴一X⑴k2. (17)
The right hand side (RHS) will be negative when e(t) < 2τL(t): this will eventually be satisfied as
we shall use a decaying (t). This implies that the proposed update (8) will decrease the objective
value of (1) after each iteration.
Momentum and algorithm convergence. It turns out that the momentum (gradient averaging step)
in (6) is essential for the convergence of ProxSGD. Under some mild technical assumptions we
outline now, the aggregate gradient v(t) will converge to the true (unknown) gradient Vf (X(t)).
This remark is made rigorous in the following theorem.
Theorem 1. Assume that the unbiased gradient g(t) has a bounded second moment
Ekg(t)k22|F(t) ≤C,	(18)
for some finite and positive constant C, and the sequence of stepsizes {ρ(t)} and {(t)} satisfy
∞	∞	∞	∞	(t)
Xρ(t) =	∞, Xρ(t)2	< ∞, X e(t)	= ∞, X e(t)2	< ∞, t→m∞	——	= 0.	(19)
t=0	t=0	t=0	t=0	∞	ρ(t)
Then limt→∞ kv(t) - Vf (X(t))k = 0, and every limit point of the sequence {X(t)} is a stationary
point of (1) w.p.1.
Proof. Under the assumptions (18) and (19), it follows from Lemma 1 of RUSzczynSki (1980) that
v(t) → Vf (X(t)). Since the descent direction Xb(t) - X(t) is a descent direction in view of (14),
the convergence of the ProxSGD algorithm can be obtained by generalizing the line of analysis in
Theorem 1 of Yang et al. (2016) for smooth optimization problems. The detailed proof is included
in the appendix to make the paper self-contained.	□
We draw some comments on the convergence analysis in Theorem 1.
The bounded second moment assumption on the gradient g in (18) and decreasing stepsizes in (19)
are standard assumptions in stochastic optimization and SGD. What is noteworthy is that (t) should
decrease faster than ρ(t) to ensure that v(t) → Vf (X(t)). But this is more of an interest from the
theoretical perspective, and in practice, we observe that e(t)∕ρ(t) = a for some constant a that is
smaller than 1 usually yields satisfactory performance, as we show numerically in the next section.
According to Theorem 1, the momentum v(t) converges to the (unknown) true gradient Vf (X(t)),
so the ProxSGD algorithm eventually behaves similar to the (deterministic) gradient descent algo-
rithm. This property is essential to guarantee the convergence of the ProxSGD algorithm.
To guarantee the theoretical convergence, the quadratic gain τ(t) in the approximation subproblem
(7) should be lower bounded by some positive constant (and it does not even have to be time-
varying). In practice, there are various rationales to define it (see Table 1), and they lead to different
empirical convergence speed and generalization performance.
The technical assumptions in Theorem 1 may not always be fully satisfied by the neural networks
deployed in practice, due to, e.g., the nonsmooth ReLU activation function, batch normalization
and dropout. Nevertheless, Theorem 1 still provides valuable guidance on the algorithm’s practical
performance and the choice of the hyperparameters.
3 Simulation Results
In this section, we perform numerical experiments to test the proposed ProxSGD algorithm1. In
particular, we first train two SNN to compare ProxSGD with ADAM (Kingma & Ba, 2015), AMS-
Grad (Reddi et al., 2018), ADABound (Luo et al., 2019) and SGD with momentum. Then we train
1The simulations in Setion 3.1 and 3.3 are implemented in TensorFlow and available at https://
github.com/optyang/proxsgd. The simulations in Section 3.2 are implemented in PyTorch and avail-
able at https://github.com/cc-hpc-itwm/proxsgd.
6
Published as a conference paper at ICLR 2020
a BNN to illustrate the merit of regularization and constraints. To ensure a fair comparison, the
hyperparameters of all algorithms are chosen according to either the inventors’ recommendations or
a hyperparameter search. Furthermore, in all simulations, the quadratic gain τ (t) in ProxSGD is
updated in the same way as ADAM, with β = 0.999 (see Table 1).
3.1	Sparse Neural Network: Training Convolution Neural Networks on
CIFAR- 1 0
We first consider the multiclass classification problem on CIFAR-10 dataset (Krizhevsky, 2009)
with convolution neural network (CNN). The network has 6 convolutional layers and each of them
is followed by a batch normalization layer; the exact setting is shown in Table 2.
Table 2: CNN Settings
parameter	value
data set number of convolution layers size of convolution kernels number of output filters in convolution layers 1-2, 3-4, 5-6 operations after convolution layers 1-2, 3-4, 5-6 kernel size, stride, padding of maxing pooling activation function for convolution/output layer loss function and regularization function	CIFAR-10 6 3×3 32, 64,128 max pooling, dropout (rate=0.2) 2×2, 2, valid elu/softmax cross entropy and 'ι-norm
Following the parameter configurations of ADAM in Kingma & Ba (2015), AMSGrad in Reddi
et al. (2018), and ADABound in Luo et al. (2019), we set ρ = 0.1, β = 0.999 and = 0.001 (see
Table 1), which are uniform for all the algorithms and commonly used in practice. Note that we have
also activated '1-regularization for these algorithms in the built-in function in TensorFlow/PyTorch,
which amounts to adding the subgradient of the 'ι-norm to the gradient of the loss function. For the
proposed ProxSGD, (t) and ρ(t) decrease over the iterations as follows,
0.06	0.9
e⑴=(t+4)O5，P⑴=(t+4)O5.	(20)
Recall that the `1 -norm in the approximation subproblem naturally leads to the soft-thresholding
proximal mapping, see (10). The regularization parameter μ in the soft-thresholding then permits
controlling the sparsity of the parameter variable x; in this experiment we set μ = 5 ∙ 10-5.
(a) Training Loss	(b) Test Accuracy	(c) CDF of Weights
Figure 1: Performance comparison for CNN on CIFAR-10.
In Figure 1, we compare the four algorithms (ProxSGD, ADAM, AMSGrad, ADABound) in terms
of three metrics, namely, the training loss, the test accuracy and the achieved sparsity. On the
one hand, Figure 1(a) shows that ProxSGD outperforms ADAM, AMSGrad and ADABound in the
achieved loss value. On the other hand, the achieved accuracy is comparable, see Figure 1(b).
The sparsity of the trained model is measured by the cumulative distribution function (CDF) of the
weights’ value, which specifies the percentage of weights below any given value. For the proposed
7
Published as a conference paper at ICLR 2020
ProxSGD in Figure 1(c), we can observe at 0 in the x-axis the abrupt change of the CDF in the
y-axis, which implies that more than 90% of the weights are exactly zero. By comparison, only
40%-50% are exactly zero by the other algorithms. What is more, for this experiment, the soft-
thresholding proximal operator in ProxSGD does not increase the computation time: ADAM 17.24s
(per epoch), AMSGrad 17.44s, ADABound 16.38s, ProxSGD 16.04s. Therefore, in this experi-
ment, the proposed ProxSGD with soft-thresholding proximal mapping has a clear and substantial
advantage than other stochastic subgradient-based algorithms.
3.2	Sparse Neural Network: Training DenseNet- 20 1 on CIFAR- 1 00
In this subsection, the performance of ProxSGD is evaluated by a much more complex network
and dataset. In particular, we train the DenseNet-201 network (Huang et al., 2017) for CIFAR-100
(Krizhevsky, 2009). DenseNet-201 is the deepest topology of the DenseNet family and belongs
to the state of the art networks in image classification tasks. We train the network using Prox-
SGD, ADAM and SGD with momentum. To ensure a fair comparison among these algorithms, the
learning rate is not explicitly decayed during training for all algorithms. Furthermore, the ideal hy-
perparameters for each algorithm were computed by grid-search and the curves are averaged over
five runs. A batch-size of 128 is adopted. For ProxSGD, the regularization parameter is μ = 10-5,
the learning rate for the weight and momentum is, respectively,
小 _	0.15 小 _	0.9
Kt) = (t + 4)0.5 , P⑴=(t + 4)0.5 .
For ADAM, e = 6 ∙ 10-4 and P = 0.1. SGD with momentum uses a learning rate of e = 6 ∙ 10-3
and a momentum of 0.9 (so P = 0.1). The regularization parameter for both ADAM and SGD with
momentum is μ = 10-4.
Figure 2: Performance comparison for DenseNet-201 on CIFAR-100.
Figure 2 shows the performance of ProxSGD and other algorithms for DenseNet-201 trained on
CIFAR-100. We see from Figure 2(a) that ProxSGD achieves the lowest training loss after Epoch
10. The test accuracy in Figure 2(b) shows that all algorithms achieve similar accuracy and ProxSGD
outperforms the other two during the early stage of training. We remark that this is achieved with
a much sparser network as shown in Figure 2(c). In particular, we can see from the zoomed-in part
of Figure 2(c) that SGD with momentum has approximately 70% of their weights at zero, while
most weights learned by ADAM are not exactly zero (although they are very small). In contrast,
ProxSGD reaches the sparsity of 92-94%.
In Figure 3, we demonstrate that ProxSGD is much more efficient in generating a SNN, irrespective
of the hyperparameters (related to the learning rate). In particular, we try many different initial
learning rate of the weight vector e(0) for ProxSGD and test their performance. From Figure 3(a)-
(b) we see that, as expected, the hyperparameters affect the achieved training loss and test accuracy,
and many lead to a worse training loss and/or test accuracy than ADAM and SGD with momentum.
However, Figure 3(c) shows that most of them (except when they are too small: e(0) = 0.01 and
0.001) generate a much sparser NN than both ADAM and SGD with momentum. These observations
are also consistent with the theoretical framework in Section 2: interpretating ADAM and SGD with
8
Published as a conference paper at ICLR 2020
IProx-SGDe(O)-OJ-
Pnox-SGDe(O)-OJ
Pnox-SGDe(O)-O-IS
Pnox-SGDe(O)-O.1
Pnox-SGD ¢(0)-0.09
Pnox-SGD ¢(0)-0.08
Pnox-SGD ¢(0)-0.07
Pnox-SGD ¢(0)-0.06
Pnox-SGDe(O)-O-Ol
Prox-SGDe(O)-OMl
Adam
SGO wtth momentum
60
50
40
30
20
10
O
O 10	20	30	40	50	60
Epochs
O 10	20	30	40	50	60
1.0
0.9
0.8
0.7
0.6
LL
0 o.5
U
0.4
0.3
0.2
0.1
0.0
—Prox-SGD ε(0)-0.3
— Prvx-SGDe(O)-OJ
:——Prvx-SGDe(O)-O-IS
——Prvx-SGDe(O)-O.1
——Pntx-SGDe(O)-O-Od
——Prvx-SGD e(0)-0.07
---Prvx-SGDe(O)-O-OS
——Pntx-SGDe(O)-O-OI
---Prvx-SGDe(O)-O-Ml
---Adern
---SGD with momentum
Epochs
-0.0∞4	-0.0∞2	0.0000	0.0002	0.0004
Weights value
(a) Training Loss
(b) Test Accuracy	(c) CDF of Weights
Figure 3:	Hyperparameters and sparsity for DenseNet-201 on CIFAR-100.
momentum as special cases of ProxSGD implies that they have the same convergence rate, and the
sparsity is due to the explicit use of the nonsmooth '1 -norm regularization.
For this experiment, the soft-thresholding proximal operator in ProxSGD increases the training time:
the average time per epoch for ProxSGD is 3.5 min, SGD with momentum 2.8 min and ADAM 2.9
min. In view of the higher level of sparsity achieved by ProxSGD, this increase in computation time
is reasonable and affordable.
3.3 Binary Neural Networks: Training Deep Neural Networks on MNIST
In this subsection, we evaluate the proposed algorithm ProxSGD in training the BNN by solving
problem (3). We train a 6-layer fully-connected deep neural network (DNN) for the MNIST dataset,
and we use the tanh activation function to promote a binary activation output; see Table 3. The
algorithm parameters are the same as Sec. 3.1, except that μ = 2 ∙ 10-4. The chosen setup is
particularly suited to evaluate the merit of the proposed method, since MNIST is a simple dataset
and it allows us to investigate soly the effect of the proposed model and training algorithm.
Table 3: DNN Settings
parameter	Value
dataset	MNIST
number of hidden layers	6
number of nodes per hidden layer	200
activation function in hidden/output layer	tanh/softmax
loss function	cross entropy
After customizing Algorithm 1 to problem (3), the approximation subproblem is
(X - X(U)Tvx⑴ + 2(X - Mt))Tdiag(Tx(U)(X - x(t))	[
x ,a	蕾a≤1 j +(a - a(t))Tva(t) + 1 (a - a(t))Tdiag(τa(t))(a - a(t)) ʃ .
Both Xb(t) and ab(t) have a closed-form expression (cf. (9) and (12))
b(t) =	x(t)	— vx(?,	and b(t)	= a(t)	— va(?
D	D Tx(t) , D 「， Ta(t)J 0
(21)
where vx(t) and va(t) are the momentum updated in the spirit of (6), with the gradients given by
gx⑴=m⅛ X Vfi(X(t)，y(i)) + 2(x(t)+2a(t) -1),andga(t) = μx(t).
i∈M(t)
The training loss is shown in Figure 4(a). We remark that during the training process of ProxSGD,
the weights are not binarized, for the reason that the penalty should regularize the problem in a way
such that the optimal weights (to which ProxSGD converges) are exactly or close to 1 or -1. After
training is completed, the CDF of the learned weights is summarized in Figure 4(c), and then the
9
Published as a conference paper at ICLR 2020
(a) Training Loss
(b) Test Accuracy	(c) CDF of Weights
Figure 4:	Performance comparison for BNN on MNIST
learned weights are binarized to generate a full BNN whose test accuracy is in Figure 4(b). On the
one hand, we see from Figure 4(a)-(b) that the achieved training loss and test accuracy by BNN is
worse than the standard full-precision DNN (possibly with soft-thresholding). This is expected as
BNN imposes regularization and constraints on the optimization problem and reduces the search
space. However, the difference in test accuracy is quite small. On the other hand, we see from
Figure 4(c) that the regularization in the proposed formulation (3) is very effective in promoting
binary weights: 15% of weights are in the range (-1,-0.5) and 15% of weights are in the range
(0.5,1), and all the other weights are either -1 or 1. As all weights are exactly or close to 1 or -1, we
could just binarize the weights to exactly 1 or -1 only once by hard thresholding, after the training
is completed, and thus the incurred performance loss is small (98% versus 95% for test accuracy).
In contrast, the weights generated by the full-precision DNN (that is, without regularization) are
smoothly distributed in [-2, 2].
Even though the proposed formulation (3) doubles the number of parameters to optimize (from x in
full-precision DNN to (x, a) in BNN ProxSGD), the convergence speed is equally fast in terms of
the number of iterations. The computation time is also roughly the same: full-precision DNN 13.06s
(per epoch) and ProxSGD 12.21s. We remark that ga(t), the batch gradient w.r.t. a, has a closed-
form expression and it does not involve the back-propagation. In comparison with the algorithm in
Courbariaux et al. (2016), the proposed ProxSGD converges much faster and achieves a much better
training loss and test accuracy (95% versus 89%, the computation time per epoch for Courbariaux
et al. (2016) is 13.56s). The notable performance improvement is due to the regularization and
constraints. Naturally We should make an effort of searching for a proper regularization parameter μ,
but this effort is very well paid off. Furthermore, we observe in the simulations that the performance
is not sensitive to the exact value of μ, as long as it is in an appropriate range.
4 Concluding Remarks
In this paper, We proposed ProxSGD, a proximal-type stochastic gradient descent algorithm With
momentum, for constrained optimization problems Where the smooth loss function is augmented
by a nonsmooth and convex regularization. We considered tWo applications, namely the stochastic
training of SNN and BNN, to shoW that regularization and constraints can effectively promote struc-
tures in the learned netWork. More generally, incorporating regularization and constraints alloWs us
to use a more accurate and interpretable model for the problem at hand and the proposed convergent
ProxSGD algorithms ensures efficient training. Numerical tests shoWed that ProxSGD outperforms
state-of-the-art algorithms, in terms of convergence speed, achieved training loss and/or the desired
structure in the learned neural netWorks.
Acknowledgement
The Work of Yang is supported by DFG Project DeTol. The Work of van Sloun is part of the research
programme Rubicon ENW 2018-3 With project number 019.183EN.014, Which is financed by the
Dutch Research Council (NWO). The Work of Yuan, Lei and Chatzinotas is supported by ERC
AGNOSTIC, FNR CORE ASWELL and FNR-AFR LARGOS.
10
Published as a conference paper at ICLR 2020
References
Francis Bach, Rodolphe Jenatton, Julien Mairal, and Guillaume Obozinski. Optimization with
SParsity-IndUcing Penalties. Foundations and Trends in Machine Learning, 4(1):1-106, 2011.
doi: 10.1561/2200000015. URL http://arxiv.org/abs/1108.0775.
YU Bai, YU-Xiang Wang, and Edo Liberty. ProxqUant: QUantized neUral networks via Proximal
oPerators. In International Conference on Learning Representations, 2019. URL https://
openreview.net/forum?id=HyzMyhCcK7.
Dimitri P. Bertsekas and John N. Tsitsiklis. Gradient convergence in gradient methods with errors.
SIAM Journal on Optimization, 10(3):627-642, 2000. doi: 10.1137/S1052623497331063. URL
http://link.aip.org/link/SJOPE8/v10/i3/p627/s1&Agg=doi.
StePhen Boyd, Neal Parikh, Eric ChU, Borja Peleato, and Jonathan Eckstein. DistribUted oPti-
mization and statistical learning via the alternating direction method of mUltiPliers. Foundations
and Trends in Machine Learning, 3(1), 2010. doi: 10.1561/2200000016. URL http://www.
nowpublishers.com/product.aspx?product=MAL{&}doi=2200000016.
Xiangyi Chen, Sijia LiU, RUoyU SUn, and Mingyi Hong. On the Convergence of A Class of Adam-
TyPe Algorithms for Non-Convex OPtimization. In International Conference on Learning Repre-
sentations, 2019. URL http://arxiv.org/abs/1808.02941.
MatthieU CoUrbariaUx, YoshUa Bengio, and Jean-Pierre David. Binaryconnect: Training deeP neUral
networks with binary weights dUring ProPagations. In Advances in Neural Information Processing
Systems 28, PP. 3123-3131, 2015.
MatthieU CoUrbariaUx, Itay HUbara, Daniel SoUdry, Ran El-Yaniv, and YoshUa Bengio. Bina-
rized NeUral Networks. In Advances in Neural Information Processing Systems 29, 2016. URL
https://papers.nips.cc/paper/6573- binarized- neural- networks.pdf.
Trevor Gale, Erich Elsen, and Sara Hooker. The State of SParsity in DeeP NeUral Networks. 2019.
URL http://arxiv.org/abs/1902.09574.
Ian Goodfellow, YoshUa Bengio, Aaron CoUrville, and YoshUa Bengio. Deep learning, volUme 1.
MIT Press, 2016.
LU HoU, QUanming Yao, and James T. Kwok. Loss-aware binarization of deeP networks. In Interna-
tional Conference on Learning Representations, 2017. URL https://openreview.net/
forum?id=S1oWlN9ll.
Gao HUang, ZhUang LiU, LaUrens van der Maaten, and Kilian Q. Weinberger. Densely connected
convolUtional networks. In The Conference on Computer Vision and Pattern Recognition, 2017.
URL https://arxiv.org/abs/1608.06993.
Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic OPtimization. In Inter-
national Conference on Learning Representations, 2015. URL http://arxiv.org/abs/
1412.6980.
Alex Krizhevsky. Learning mUltiPle layers of featUres from tiny images, 2009. URL https:
//www.cs.toronto.edu/~kriz∕learning-features-2009-TR.pdf.
YUnwen Lei, Ting HU, and Ke Tang. Stochastic Gradient Descent for Nonconvex Learning withoUt
BoUnded Gradient AssUmPtions. to aPPear in IEEE Transactions on Neural Networks and Learn-
ing Systems, 2019. URL https://ieeexplore.ieee.org/document/8930994.
Christos LoUizos, Max Welling, and Diederik P. Kingma. Learning SParse NeUral Networks throUgh
L0 RegUlarization. In International Conference on Learning Representations, 2018. URL http:
//arxiv.org/abs/1712.01312.
Liangchen LUo, YUanhao Xiong, Yan LiU, and XU SUn. AdaPtive Gradient Method with Dynamic
BoUnd of Learning Rate. In International Conference on Learning Representations, 2019. URL
http://arxiv.org/abs/1902.09843v1.
11
Published as a conference paper at ICLR 2020
Neal Parikh and Stephen Boyd. Proximal algorithms. Foundations and Trends in Optimization, 1
(3):127-239,2014. doi: 10.1561/2400000003. URL http://www.nowpublishers.com/
articles/foundations- and- trends- in- optimization/OPT- 003.
Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of ADAM and beyond.
In International Conference on Learning Representations, 2018. URL http://arxiv.org/
abs/1904.09237.
Andrzej RUszczynski. Feasible direction methods for stochastic programming problems. Math-
ematical Programming, 19(1):220-229, December 1980. doi: 10.1007/BF01581643. URL
http://www.springerlink.com/index/10.1007/BF01581643.
Yang Yang, GesUaldo ScUtari, Daniel P Palomar, and MariUs Pesavento. A parallel decomposi-
tion method for nonconvex stochastic mUlti-agent optimization problems. IEEE Transactions
on Signal Processing, 64(11):2949-2964, JUne 2016. doi: 10.1109/TSP.2016.2531627. URL
http://ieeexplore.ieee.org/document/7412752/.
Penghang Yin, ShUai Zhang, Jiancheng LyU, Stanley Osher, Yingyong Qi, and Jack Xin. Bina-
ryRelax: A Relaxation Approach for Training Deep NeUral Networks with QUantized Weights.
SIAM Journal on Imaging Sciences, 11(4):2205-2223, JanUary 2018. URL https://epubs.
siam.org/doi/10.1137/18M1166134.
12
Published as a conference paper at ICLR 2020
A Appendix: Proof of Theorem 1
Proof. The claim limt→∞ ∣∣v(t) -Vf (x(t))k = 0 is a consequence of (Ruszczyilski, 1980, Lemma
1). To see this, we just need to verify that all the technical conditions therein are satisfied by the
problem at hand. Specifically, Condition (a) of (RUSzCzynski, 1980, Lemma 1) is satisfied because
X is closed and bounded. Condition (b) of (Ruszczynski, 1980, Lemma 1) is exactly (18). Con-
ditions (c)-(d) of (RUSZCZynski, 1980, Lemma 1) come from the stepsize rules in (19) of Theorem
1. Condition (e) of (RUSZCZynski, 1980, Lemma 1) comes from the Lipschitz property of Vf and
stepsize rule in (19) of Theorem 1.
We need the following intermediate result to prove the limit point of the sequence x(t) is a stationary
point of (1).
Lemma 1. There exists a constant L such that
∣xb(x(t1), ξ(t1)) - xb(x(t2), ξ(t2))∣ ≤ Lb ∣x(t1) - x(t2)∣ + e(t1,t2),
and limt1,t2→∞ e(t1, t2) = 0 w.p.1.
Proof. We assume without loss of generality (w.l.o.g.) that τ (t) = τ1, and the approximation
subproblem (7) reduces to
b(t)，argmin {(x — x(t))Tv(t) + T∣∣x — x(t)∣2 + r(x)).
x∈X	2
It is further equivalent to
FinV {(X - x(t))τv(t)+ Tkx - X⑴k2 + y0,	(22)
x∈X,r(x)≤y	2
where the (unique) optimal X and y is (Xb(t) and r(Xb(t)), respectively.
We assume w.l.o.g. that t2 > t1. It follows from first-order optimality condition that
(X — Xb(t1))τ (v(t1)	+ T (Xb(t1) —	X(t1))) + y —	r(Xb(t1)) ≥	0, ∀X, y such that r(X) ≤ y	(23a)
(X — Xb(t2))τ (v(t2)	+ T (Xb(t2) —	X(t2))) + y —	r(Xb(t2)) ≥	0, ∀X, y such that r(X) ≤ y.	(23b)
Setting (X, y) = (Xb(t2), r(Xb(t2))) in (23a) and (X, y) = (Xb(t1), r(Xb(t1))) in (23b), and adding
them up, we obtain
(b(tι) — b(t2))τ(v(tι) — v(t2))-τ(x(tι)-x(t2))τ(b(tι) — b(t2)) ≤ —T∣∣b(tι) — b(t2)k2,. (24)
The term on the left hand side can be lower bounded as follows:
Xb(t1) — Xb(t2), v(t1) — Vf (X(t1)) — v(t2) + Vf (X(t2))
+ Xb(t1) — Xb(t2), Vf (X(t1)) — Vf(X(t2)) — T Xb(t1) — Xb(t2), X(t1) — X(t2)
≥ —	Xb(t1) — Xb(t2)(ε(t1) + ε(t2)) — (L + T)Xb(t1) — Xb(t2)X(t1) —	X(t2)	(25)
where the	inequality comes from the Lipschitz	continuity of	Vf (X),	with	ε(t)	,
kv(t) — Vf (x(t))k.
Combining the inequalities (24) and (25), we have
Xb(t1) — Xb(t2) ≤ (L + T)T-1X(t1) — X(t2) + T-1 (ε(t1) + ε(t2)),
which leads to the desired (asymptotic) Lipschitz property:
Xb(t1) — Xb(t2) ≤ LbX(t1) — X(t2) + e(t1, t2),
with L，TT(L + τ) and e(t1,t2)，τ-1(ε(t1) + ε(t2)), and limt1→∞,t2→∞ e(t1,t2) = 0
w.p.1.	口
13
Published as a conference paper at ICLR 2020
Define U(x) , f(x) + r(x). Following the line of analysis from (15) to (16), we obtain
U(x(t+1)) -U(x(t))	(26)
≤ e(t)((b(t) - x(t))T(Vf(X(t)) + r(χ(t)) - r(χ(t))) + Le(t)2∣∣b(t) - χ(t)∣∣2
=e⑴(b⑴一x(t))T(Vf(X⑻-v⑴+ V⑴+ r(x(t)) - r(x(t))) + Le⑴2∣∣b⑴一x(t)∣∣2
≤ - e⑴(T- Le⑴)∣∣b⑴一x(t)∣∣2 + e⑴∣∣b⑴一X⑴∣∣∣∣Vf(X(U)-V⑴∣∣,	(27)
where in the last inequality we used (14) and the Cauchy-Schwarz inequality.
Let us show by contradiction that lim inft→∞ ∣Xb(t)-X(t)∣ = 0 w.p.1. Suppose lim inft→∞ ∣Xb(t) -
X(t)∣ ≥ χ > 0 with a positive probability. Then we can find a realization such that at the same time
∣∣Xb(t) - X(t)∣∣ ≥ χ > 0 for all t and limt→∞ ∣∣Vf (X(t)) - V(t)∣∣ = 0; we focus next on such a
realization. Using ∣Xb(t) - X(t)∣ ≥ χ > 0, the inequality (27) is equivalent to
U(x(t + 1)) — U(x(t)) ≤ Y" (T- Le(t) - ɪ ∣∣Vf (x(t)) - v(t)k) ∣∣b(t) - x(t)∣∣2. (28)
Since limt→∞ ∣Vf (X(t)) - V(t)∣ = 0, there exists a t0 sufficiently large such that
T — Le(t) - - IlVf(X(t)) - v(t)k ≥ T > 0,	∀t ≥ to.	(29)
2χ
Therefore, it follows from (28) and (29) that
U(X(t)) - U(Xt0) ≤ -Tχ2Pn=t0 (t)en+1,	(30)
which, in view of Pn∞=t n+1 = ∞, contradicts the boundedness of {U (X(t))}. Therefore it must
be lim inf t→∞ IXb(t) - X(t)I = 0 w.p.1.
Let us show by contradiction that lim supt→∞ IXb(t) - X(t)I =	0 w.p.1. Suppose
lim supt→∞ IXb(t) - X(t)I > 0 with a positive probability. We focus next on a realization along
with lim supt→∞ IXb(t) - X(t)I > 0, limt→∞ ∣∣Vf (X(t)) - V(t)∣∣ = 0, lim inft→∞ ∣∣Xb(t) -
X(t)∣ = 0, and limti,t2→∞ e(t1, t2) = 0, where e(t1, t2) is defined in Lemma 1. It follows from
lim supt→∞ IXb(t) - X(t)I > 0 and lim inft→∞ ∣Xb(t) - X(t)∣ = 0 that there exists a δ > 0 such
that I4X(t)I	≥	2δ	(with 4X(t)	,	Xb(t)	-	X(t))	for infinitely many t and also I4X(t)I	< δ
for infinitely many t. Therefore, one can always find an infinite set of indexes, say T, having the
following properties: for any t ∈ T, there exists an integer it > t such that
I4X(t)I < δ,	I4X(it)I > 2δ,	δ ≤ I4X(n)I ≤ 2δ, t < n < it.	(31)
Given the above bounds, the following holds: for all t ∈ T,
δ ≤ I4X(it)I - I4X(t)I
≤ I4X(it) - 4X(t)I = I(Xb(it) - X(it)) - (Xb(t) - X(t))I
≤ ∣∣Xb(it) - Xb(t)∣∣ + ∣∣X(it) - X(t)∣∣
≤ (1 + Lb)∣∣X(it) - X(t)∣∣ + e(it, t)
≤ (1+Lb)Pint=-t1(n)I4X(n)I + e(it, t)
≤ 2δ(1+Lb)Pint=-t1(n)+e(it,t),	(32)
implying that
1
liminf Pn=te(n) ≥ δι，------ > 0.
T 3t→∞	2(1 + Lb )
(33)
Proceeding as in (32), we also have: for all t ∈ T,
I4X(t + 1)I - I4X(t)I ≤ I4X(t + 1) - 4X(t)I ≤ (1 + Lb)(t) I4X(t)I +e(t,t+1),
14
Published as a conference paper at ICLR 2020
which leads to
(1+(1+Lb)(t))k4x(t)k+e(t,t+1) ≥ k4x(t + 1)k ≥δ,	(34)
where the second inequality follows from (31). It follows from (34) that there exists a a > 0 such
that for sufficiently large t ∈ T,
k4x(t)k≥ ：-mi ≥ δ > 0.	(35)
1	+ (1 + Lb)(t)
Here after we assume w.l.o.g. that (35) holds for all t ∈ T (in fact one can always restrict {x(t)}t∈T
to a proper subsequence).
We show now that (33) is in contradiction with the convergence of {U (x(t))}. Invoking (27), we
have for all t ∈ T,
U(x(t + 1)) — U(x(t)) ≤ Y" (T- Le(t)) ∣∣b(t) - x(t)∣∣2 + e(t)δ∣∣Vf (x(t)) - v(t)∣∣
≤γt1- 2-W(x(t；)-v削!∣∣b(t)-χ(t)∣∣2
+ (t)δVf (x(t)) - v(t)2,	(36)
and for t < n < it ,
U(x(n + 1)) — U(x(n)) ≤ -e(n) (T — Le(n) — 'Vf(χ(n1 J：；" ! ∣∣b(n) — x(n)∣∣2
2	x(n) - x(n)
≤ Yn) (T - LUn) - IVf(X(nδ)~VnI! ∣∣b(n) - x(n)∣∣2,
(37)
where the last inequality follows from (31). Adding (36) and (37) over n = t + 1, . . . , it - 1
and, for t ∈ T sufficiently large (so that T - L(t)/2 - δ-1 ∣∣Vf (x(n)) - v(n)∣∣ ≥ Tb > 0 and
∣∣Vf (x(t)) — v(t)∣∣ <bj2∕δ), we have
(a)
U(x(it)) - U(x(t)) ≤
(b)
≤
(c)
≤
-TbPint=-t1(n)∣∣xb(n) - x(n)∣∣2 + (t)δ ∣∣Vf (x(t)) - v(t)∣∣
-bS2 Pn=+ιe(n) - e(t)(选一δ ∣∣ Vf (x(t)) - v(t) ∣∣)
-b尾 Pn=+ιe(n),
(38)
where (a) follows from T - L(t)/2 - δ-1 ∣∣Vf (x(n)) - v(n)∣∣ ≥ Tb > 0; (b) is due to (35);
and in (c) We used ∣∣Vf(x(t)) 一 v(t)∣∣ < bS2∕δ. Since {U(x(t))} converges, it must be
lim inf Pint=-t1+1(n) = 0, which contradicts (33). Therefore, it must be lim supt→∞ ∣xb(t) -
x(t)∣∣ = 0 w.p.1.
Finally, let us prove that every limit point of the sequence {x(t)} is a stationary solution of (1). Let
x? be the limit point of the convergent subsequence {x(t)}t∈T . Taking the limit of (23a) over the
index set T (and replacing w.l.o.g. y by r(x)), we have
lim (x - xb(t))T (v(t) + T (xb(t) - x(t))) + r(x) - r(xb(t))
T 3t→∞
= (x - x? )T Vf (x? ) + r (x) - r(x? ) ≥ 0, ∀ x ∈ X,
where the last equality follows from: i) limt→∞ ∣Vf (x(t)) - v(t)∣ = 0, and ii) lim t→∞ ∣xb(t) -
x(t) ∣∣ = 0. This is the desired first-order optimality condition and x? is a stationary point of (1). 口
15