Published as a conference paper at ICLR 2020
Probabilistic Connection Importance Infer-
ence and Lossless Compression of Deep Neu-
ral Networks
Xin Xing	Long Sha	Pengyu Hong
Harvard University	Brandeis University	Brandeis University
Zuofeng Shang	Jun S. Liu
New Jersey Institute of Technology	Harvard University
Ab stract
Deep neural networks (DNNs) can be huge in size, requiring a considerable amount
of energy and computational resources to operate, which limits their applications
in numerous scenarios. It is thus of interest to compress DNNs while maintaining
their performance levels. We here propose a probabilistic importance inference
approach for pruning DNNs. Specifically, we test the significance of the relevance
of a connection in a DNN to the DNN’s outputs using a nonparemetric scoring test
and keep only those significant ones. Experimental results show that the proposed
approach achieves better lossless compression rates than existing techniques.
1	Introduction
Deep neural networks (DNNs) have many successful applications ranging from computer vision,
natural language processing to computational biology. However, large DNNs usually require sig-
nificant memory and storage overhead and sometimes a large network bandwidth, which hinges
their usages on mobile devices. Running large-size neural networks also consumes a considerable
amount of energy, making their deployments on battery-constrained devices difficult. Furthermore,
the over-parameterization issue in DNN architectures can impair its performances. Recent works
(Han et al. (2015); Ullrich et al. (2017); Louizos et al. (2017) and references therein) show ways to
reduce the network complexity by using proper regularization or network pruning to significantly
reduce the number of parameters. One way to convert a dense DNN into a sparse one is by applying
L0/L1 regularization on the model parameters. The L1 penalty is computationally efficient, but it
also introduces more bias on the large weights and may lead to significant degradation in model
performances (Han et al., 2015). In contrast, L0 regularization shows better performance, but incurs
much higher computational complexity due to its combinatorial nature. Pruning, as shown in Han et al.
(2015) and Tartaglione et al. (2018), is another promising strategy to sparsify DNNs by only keeping
network connections more relevant to the final output. The importance of a network connection (i.e.,
the connection between two network nodes) can be approximated by the magnitudes or gradients of
its weights. However, such an approximation may not be accurate since it does not consider the highly
nonlinear relationships between network connections and the final output induced by the multi-layer
convolutions of DNNs.
Some available network compression methods improve the computational performance with moderate
to high losses in accuracy, which can be highly undesirable in many critical missions (such as
autonomous driving). In order to achieve lossless compression, we need to correctly decipher the
relationships between the network connections and the final output. This is a challenging task because
the structural nature of DNNs makes the dependence between a network connection and the network
output highly nonlinear. In this paper, we propose a probabilistic connection importance inference
(PCII) method for testing whether a connection in a DNN is relevant to the DNN output. Specifically,
we introduce a new technique called probabilistic tensor product decomposition to decompose the
association of two connected network nodes into two components: one related to the DNN output and
the other independent of the DNN output. If the strength of the first component is high, we keep the
1
Published as a conference paper at ICLR 2020
network connection. Otherwise, we delete it. The inference is carried out by a nonparametric score
test, which is based on modeling the log-transformed joint density of the connected nodes and the final
output in a tensor product reproducing kernel Hilbert space (RKHS). We further derive the asymptotic
distribution of the proposed test statistics, thus avoiding the computationally intensive resampling
step encountered in most nonparametric inference settings. We implemented the method and tested it
on image classification tasks, in which our approach achieved the highest lossless compression rates.
Section 2 reviews relevant literature; Section 3 introduces the PCII method and algorithm; Section
4 establishes theoretical properties for using the method to infer dependence between a network
connection and the DNN output; and Section 5 shows the experimental results and concludes with a
short discussion.
2	Related Works
Zhu & Gupta (2017) found that a DNN can be greatly sparsified with minimal loss in accuracy. One
strategy for sparsifying DNNs is to shrink small weights to zero. Along this line of thinking, Louizos
et al. (2017) introduced a smoothed version of L0 regularization aiming to be both computationally
feasible and beneficial to generalizability. There are also some regularization methods based on
Bayesian formulations. Ullrich et al. (2017) proposed to add a Gaussian mixture prior to the weights.
The sparsity is achieved by concentrating weights to cluster centers. Molchanov et al. (2017) proposed
variational dropout, which learns individual dropout rate based on the empirical Bayes principle.
Also, as shown in Han et al. (2015), the performance of pruning and retraining is related to choice the
correct regularization, such as L1 or L2 regularization.
PCII offers a means to prune a DNN by keeping network connections that are the most relevant to
the DNN output. This idea goes back to the optimal brain damage work by LeCun et al. (1990). It
shows that among the many parameters in the network, many are redundant and do not contribute
significantly to the output. Later, Hassibi et al. (1993) proposed the optimal brain surgeon method,
which leverages a second-order Taylor expansion to select parameters for deletion. Most recently,
Han et al. (2015) proposed a three-step pruning method. Tartaglione et al. (2018) proposed to prune
a DNN based on the sensitivity score. There are also several approaches targeting at sparsifying
convolution layers. For example, Anwar et al. (2017) proposed to prune feature maps and kernels in
convolution layers.
Comparing with existing pruning methods based on the magnitude or gradient of weights, our
approach directly models the relationship between a network connection and the network output in
a nonparametric way. In addition, our inference is based on a statistical hypothesis testing, which
outputs p-values to quantify the dependence strength of each network connection to the DNN output.
The use of p-values allows network connections to be treated in a unified way and alleviates the need
of ad hoc weights normalization required in some existing approaches.
3	Probabilistic Connection Importance Inference
In this section, we establish a general procedure for building the probabilistic connection structure,
in which the connections are inferred by the nonparametric score test in tensor product RKHS. As
shown in our experiments (see Section 5), our technique not only sparsifies the network but improves
its generalization ability.
3.1	PCII on fully connected layers
Without loss of generality, we let a feed-forward acyclic DNN (Figure 1) be composed of T layers
with Xt being the input of the t-th network layer, where t = 0, 1, . . . , T . Let t = 0 and t = T
indicate the input and output layers, respectively, and let 0 < t < T index the hidden layers. The
collection of all the nodes is denoted as G and the collection of all network connections is denoted
as E. We use a pair of nodes to denote a connection in E. For example, (Xt,1, Xt+1,1) denotes an
edge from the 1st node in the t-th layer to the 1st node in the (t + 1)-th layer. For simplicity, we use
Y to denote the output layer, who takes on categorical values for a classification task, and takes on
continuous values for regression.
2
Published as a conference paper at ICLR 2020
Figure 1: Flowchart for probabilistic connection inference. We have two nodes a and β from a fully
connected neural network. The connection of α and β is expressed as ηα,β + ηα,β,y. The importance
of the connection is inferred by testing whether the three way interaction ηα,β,y is zero or not.
The output of the (t + 1)-th layer can be described as
mt
Xt+1,j = gt+1(	wti,,tj+1Xt,i), for j = 1, . . . ,mt+1,
i=1
where mt denotes the number of nodes in the t-th layer, and gt+1 is the activation function. It should
be noted that the magnitude of weights is not necessarily the best indication of the impact of the
corresponding network connection. A more preferable way is to directly model the relationship
between a network connection and the final output of a DNN. For simplicity, we use (α, β) to denote
an arbitrary network connection. Due to the high non-linearity of a DNN, we use the nonparametric
function to model the relationship among α, β and Y .
Let us denote the joint density ofα, β and Y as f(α, β, y) and the log-transferred density as η(α, β, y).
We assume that η(α, β, y) can be deposed as
η(α, β, y) = ηα + ηβ + ηy + ηα,y + ηβ,y + ηα,β + ηα,β,y ,	(1)
、-----{z-----}
interaction between α and β
where ηα, ηβ, and ηy are marginal effects, ηα,y, ηβ,y, and ηα,β are the two-way interaction terms,
ηα,β,y is the three-way interaction term. Here, we interpret the connection as the interaction of α and
β, i.e. ηα,β + ηα,β,y. Specifically, ηα,β is the interaction effect of α and β without the impact of y,
and ηα,β,y characterizes the interaction of α, β impacted by y. Our aim is to measure the significance
of the connection by how much it is related to Y . To model this problem mathematically, we measure
the association between the connection and Y by the significance of the three-way interaction ηα,β,y .
Therefore, inferring whether the connection is related to the final output Y or not is equivalent to
testing whether ηα,β,y = 0 or not. As shown in Figure 1, the connection is important for the network
model if and only if the three-way interaction ηα,β,y 6= 0. We propose a score test to qualify the
significance of this term. The detailed construction of the statistical test is explained in Section 4.2.
3.2	PCII on convolutional layers
For different activation functions and type of layers, we have modifications to adopt the specific
structure. Here, we generalize the proposed PCII test to handle convolutional layers, which are
critical in many modern neural network architectures such as VGG16 Simonyan & Zisserman (2014).
As demonstrated in Li et al. (2016), sparsifying the filter has little effect on reducing the computation
cost. Nevertheless, reducing the volume of filters can greatly increase computing efficiency. For
example, the volume of a filter is 3 × 3 × 4. There are four 3 × 3 slices. PCII can be generalized to
infer the importance of the slices, which can be treated as the connection between one channel in the
current layer and one channel in the next layer.
3
Published as a conference paper at ICLR 2020
Figure 2: Flowchart for probabilistic connection inference for convolutional layers. Without loss
generality, we use same notation of α and β to denote two channels from a convolutional neural
network. The connection between α and β is corresponding to the convolutional operator ht1,,tm+t1+1
(shown as one slice with the red border). The importance of this connection is inferred by testing
whether the three way interaction ηα,β,y is zero or not.
Convolutional filters can be applied to transform channels in the t-th layer to channels in the (t+ 1)-th
layer. We denote the j-th channel in the t-th layer as Xt,j forj = 1, . . . , mt, where mt is the number
of channels in t-th layer and t = 0, . . . , T. Each filter connects all channels in the t-th layer to one
channel in the t + 1-th layer. Let hit,,jt+1 denote a convolution operator that connects two channels
Xt,i and Xt+1,j. Then, the filter connected to Xt+1,j is {hit,,jt+1}im=t1 where we denote each hit,,tj+1
as a slice of the filter. For example, when we choose a 3 × 3 × 4 filter and set stride as one, this
operator is the filter convolved (slided) across the width and height of the input {Xt,i}im=t1 and offset
with the bias. For one slice in the t + 1-th layer, we can write its connection with the slices in the
previous layer as
mt
Xt+1,j = gt+1 (X ht,,jt+1 (Xt,i) + btj,t+1)
i=1
where gt+1 is an activation function and btj,t+1 is the bias for the j-th filter. As illustrated in Figure 2,
the red arrow from Xt,1 to Xt+1,mt+1 represents ht1,,tm+t1+1, which is one slice of the filter {hit,,tm+t1+1}im=t1
connecting the channels in the t-th layer to channel Xt+1,mt+1 in the t + 1-th layer. For simplicity,
we denote α as one channel in the current layer and β as one channel in the next layer. Since
the relationship among the triplet (α, β, Y ) is highly nonlinear, we model its log-transformed joint
density as a nonparametric function η(α, β, y). Similar to the fully connected layers, we assume that
η(α, β, y) has a decomposition as in (1). As shown in Figure 2, the connection between α and β is
decomposed to two parts: one unrelated to the output Y , ηα,β, and another related to the output Y ,
ηα,β,y. We aim to select connections that have greater contributions to Y , which is mathematically
translated into the task of assessing the significance of the three-way interaction term ηα,β,y against
the null hypothesis ηα,β,y = 0.
3.3	Algorithm
In real applications, both fully connected layers and convolutional layers are used in a complex neural
network architecture. We integrate the PCII procedure described in the previous two subsections to
simultaneously infer the connections in both fully connected and convolutional layers, as summarized
in Algorithm 1. We use pi(jt) to denote the p-value for testing the i-th node in t-th layer and j-th node
in (I 1 ∖ fh lc∖7ατ* ∙f<^Aτ* fhα Glll∖r CcnnaCfaA l^1∖7ατ*C' ^Rcτ* CCnCTIcl l^1∖7ατ*C'	/4∕~ιf∕240 fhα n ∖7^1l11α
in (t + 1)-th layer for the fully connected layers. For convolutional layers, pij denotes the p-value
for inferring the importance of the filter connecting the i-th slice in the t-th layer and the j -th slice in
4
Published as a conference paper at ICLR 2020
the t + 1-th layer. The calculation of the p-values are given in Section 4 based on our proposed Score
test.
Algorithm 1 PCII: Probabilistic Connection Importance Inference for Lossless Neural Network
Compression
Input: A training dataset, a DNN architecture, and the desired model compression rate
Step 1: Use the training dataset to train a DNN of the given architecture.
Step 2: (a). Importance inference of connections in fully connected layers: infer the three-way
dependence effect of a network connection by testing the hypothesis H0 : ηα,β,y = 0 v.s H1 :
ηα,β,y 6= 0, and calculate the test statistics (or p-values) for all network connections as pf =
{pit,,jt+1, i = 1, . . . , mt, j = 1, . . . , mt+1 | t and t + 1 layers are fully connected}.
(b). Importance inference of connections in convolutional layers: infer the three-way dependence
effect of a network connection by testing the hypothesis H0 : ηα,β,y = 0 v.s H1 : ηα,β,y 6= 0,
and calculate the test statistics (or p-values) for all network connections as pc = {pit,,jt+1, i =
1, . . . , mt , j = 1, . . . , mt+1 | t and t + 1 layers are convolutionally connected}.
Step 3: Rank all network connections by their test statistics (or p-values), and select a threshold ρf
for deleting network connections in fully connected layers and ρc in convolutional layers to achieve
the desired model compression rate (Strategies for choosing ρf and ρc are given in Section 5).
Step 4: Set the same initial value for non-zero weights and filters. Retrain the sparsified DNN.
4 Score Test and Theoretical Properties
4.1 Background on tensor product RKHS
Consider two random variables α and β for fully connected layers or two random vectors α and β for
convolutional layers. Let Y be a random variable as the final output. The domains for α, β and Y are
A, B, and Y, respectively. Here, we assume that the log-transformed joint density function η belongs
to a tensor product RKHS H = Hhai 0 Hhei 0 HhYi where 0 denotes the tensor product of two
linear space.
For marginal RKHS, Hhl* i with an inner product(•，)旬1)for l = α,β, Y, there exists a symmetric
and square integrable function Kl such that
hf,Kl (x,∙)iHhii = f (x), for all f ∈ Hhli	⑵
where Kl is called the reproducing kernel of Hhli for l = α, β, Y. By Mercer’s theorem, any
continuous kernel has the following decomposition
∞
K (χ,y) = £〃v φν (x)φν (y)	⑶
ν=0
where the μν,s are non-negative descending eigenvalues and the φν,s are eigen-functions. For the
discrete domain {1,..., a}, We define the kernel as K(x, y) = l{x = y} for x,y ∈ {1,..., a}. For
a continuous domain, there are different choice of kernels such as Gaussian kernels and Sobolev
kernels. Note that the eigenvalues for different kernels on continuous domain have different decay
rate. For example, the eigenvalues of the Gaussian kernel have the exponential decay rate, i.e., there
exists some c > 0 such that μν X exp(-cν) (Sollich & Williams, 2004); and the eigenvalues of the
m-th Sobolev kernels have the polynomial decay rate, i.e., μν x V-2m (Gu, 2013).
4.2 Probabilistic Decomposition of tensor product RKHS
Next we propose the probabilistic tensor sum decomposition for each marginal RKHS, Hhli, for
l = α, β, Y. We first use Euclidean space as a simple example to illustrate the basic idea of tensor
sum decomposition. Recall that the tensor sum decomposition is often called ANOVA decomposition
in linear model. For example, for the d-dimensional Euclidean space, we denote f as a vector and let
f(x) be the x-th entry of the vector for x = 1, . . . , d. We denote A as an average operator defined as
Af (x) = (11, f). The tensor sum decomposition of the Euclidean space Rd is
1d
Rd = Rd ㊉ Rd := span{? 1}㊉{f ∈ Rd | Ef (d) = 0}
i=1
(4)
5
Published as a conference paper at ICLR 2020
where the first space is called the grand mean and the second space is called the main effect. Then,
we construct the kernel for R0d and R1d in the following lemma.
Lemma 1. For a RKHS space H, there corresponds a unique reproducing kernel K, which is
non-negative definite. Based on the tensor sum decomposition H = Ho ㊉ Hi, where Ho = {1∕d1}
and H1 = {g ∈ H : EX (g(X)) = 0}, we have the kernel for H0 as
ko (x, y) = 1/d	(5)
and kernel for H1 as
ki(x,y) = l{x=y} - 1/d
where 1 denotes the indicator function.
However, in RKHS with infinite dimension, the grand mean is not a single vector. Here, we set
the average operator A as A := f → Exf(x) = Ex hkx, fiH = hEkx, fiH where k is the kernel
function in H and the first equality is due to the reproducing property. Exkx plays the same role as
d 1 in Euclidean space. Then, We have the tensor sum decomposition in a functional space defined as
H = Ho ㊉ Hi := span{Eχkχ}㊉{f ∈ H : Af = 0}.	(6)
FolloWing the same fashion, We call Ho as the grand mean space and H1 as the main effect space.
Notice that Exkx is also knoWn as the kernel mean embedding Which is Well established in the
statistics literature Berlinet & Thomas-Agnan (2011). Then We introduce the folloWing lemma to
construct the kernel function for Ho and Hi .
Lemma 2. For RKHS space H, there corresponds an unique reproducing kernel K, which is non-
negative definite. Based on the tensor sum decomposition H = Ho ㊉ Hi where Ho = {Eχkχ} and
Hi = {g ∈ H : Ex(g(x)) = 0}, we have the kernel for Ho as
ko(x, y) = Ex[k(x, y)] + Ey[k(x, y)] - Ex,yk(x, y)	(7)
and the kernel for Hi as
ki(x, y) = hkx - Ekx,ky - EkyiH
= k(x, y) - Ex [k(x, y)] - Ey [k(x, y)] + Ex,yk(x, y).
In neural netWorks, A, B are in a continuous domain. The final output Y can be either in con-
tinuous domain or discrete domain, Which depends on the tasks. Here, We construct the tensor
sum decomposition for discrete domain and continuous domain based on Lemma 1 and Lemma
2 respectively. Specifically, We have spaces Hhαi, Hhβi and HhYi decomposed as tensor sums of
subspaces Hhai = HOai ㊉ Hhαi, Hhei = Hoβi ㊉ Hhβi, HhYi = HOYi ㊉ HIYi. Following Gu
(2013), We apply the distributive laW and have the decomposition of H as
H =(HOai ㊉ Hhai)乳(HOe ㊉ HFi)乳(HOYi ㊉ HhYi)
≡Hooo ㊉ Hioo ㊉ HOio ㊉ HOOi ㊉ Hiio ㊉ Hioi ㊉ Hoii ㊉ Hiii.	(8)
where Hijk = Hhai ㊉ Hj ㊉ HkY i.
Lemma 3. Suppose Khii is the reproducing kernel of Hhii on Xi, and Kh2i is the reproducing
kernel of H⑵ on X?. Then the reproducing kernels of H⑴ 0 H⑵ on X = Xi X X? is K(x, y)=
Khii (xhii, yhii )K h2i (xh2i, yh2i ) with x = (xhii, xh2i ) and y = (yhii, yh2i ).
Lemmas 3 can be easily proved based on Theorems 2.6 in Gu (2013). Lemma 3 implies that the
reproducing kernels of the tensor product space is the product of the reproducing kernels. Based on
these three lemmas, we can construct kernel for each subspace defined in (8).
4.3	Score Test
Based on (8), the log-transformed density function η ∈ H can be correspondingly decomposed as (1).
Thus, ηa,β,Y = 0 if and only if η* ∈ Ho := Hooo ㊉ Hioo ㊉ Hoio ㊉ Hooi ㊉ Hiio ㊉ Hioi ㊉ Hoii
where η* is the underlying truth. Hence, we focus on the following hypothesis testing problem:
Ho : η* ∈ Ho vs. Hi : η* ∈ H\Ho,	(9)
6
Published as a conference paper at ICLR 2020
where H\H0 denotes set difference ofH and H0. We next propose a likelihood-ratio based procedure
to test (9). Suppose that t = (α, β, y) and ti = (αi, βi, yi), i = 1, . . . , n, are iid observations
generated from T = (A, B, Y). Let LRn be the likelihood ratio functional defined as
1n
LRn(η) = 'n㈤-'n(PHOη) = -n ɪ^nei)- PHOη(ti)}, η ∈H,	(10)
where PHO is a projection operator from H to H0. Using the reproducing property, we rewrite (10)
as
1n
LRjn) = - n E{IK ,niH -(Kt ,n)H}	(II)
n i=1
where K is the kernel for H and K0 is the kernel for H0 .
Then We calculate the Frechet derivative of the likelihood ratio functional as
DLRngAn = 1X XX(Kti- K0i), ∆n)	=(nKI, ∆n)丸	(12)
Where K1 is the kernel for H111. We define our test statistics as the squared norm of the score
function of the likelihood ratio functional as
1n
Sn = ii n X κii iih
i=1
(13)
By the reproducing property, We can expand the left hand side of (13) as
nn
Sn =今 ΣΣ κ 1(ti, tj)	(14)
i=1 j=1
We observe an interesting phenomenon that Sn2 has a similar expression With the MMD (Gretton
et al., 2012) When Y is a binary variable. When Y ∈ {0, 1}, the kernel on HhYi is K1hY i (yi, yj) =
l{χ = y} - 1/2. Assume that the kernel for Hhai 0 H『i is Kha,βi(x, x) for X ∈ A × B. By
Lemma 3, We have K1(ti, tj) = K1hY i (yi, yj)K1h1α,βi (xi, xj). Then We have 8Sn2 as
8S	=	ɪ(	ς	K ha,βi	(χ X .)	_ 2	ς	Kha,β(χ, X )+ ς	K ha,βi(χ,	X))
8Sn	=	n2 (	2-^/	K11	(Xi，Xj)	2	ʌ,	K11	(xi, Xj) + ʌ,	K11	(xi,	Xj)).	(15)
{i,j | yi =yj =0}	{i,j | yi 6=yj}	{i,j | yi =yj =1}
If We replace K1h1a,βi With Kha,βi, i.e., the kernel on the Hhai 0 Hhβi, the right hand side of (15) is
the MMD measuring the distance betWeen the joint distribution of α and β in the group With y = 0
and the joint distribution of α and β in the group With y = 1.
MMD2 [(α,β),Y ] = n2 ( ∑ K ha，ei(Xi, Xj) - 2 ∑ K (aβ M, Xj)+ ∑ K (aβ M, Xj))
{i,j | yi =yj =0}	{i,j | yi 6=yj}	{i,j | yi =yj =1}
(16)
Since We Want to infer the importance of the connection betWeen the α and β, We are only interested
in comparing Whether the interaction effect betWeen α and β changes or not in the tWo groups. The
main effects of α and β only contribute to the importance of the nodes or slices and are not relevant
to the connection betWeen α and β .
4.4	Calculation of the Test Statistics
We introduce a matrix form of the squared norm of score function for manifesting the computation
process. In (14), Sn2 is determined by the kernel on th H1hai 0 Hh1βi 0 H1hY i . By Lemma 1 and
Lemma 2, We replace the expectation by the sample average and get the kernel function for Hlhli as
k1l (x, y)
1n
-n^2kl(xi,y) -
n	nn
n ∑ kl(x, yi) + n ΣΣ k (χi, yj)
7
Published as a conference paper at ICLR 2020
where kl is the kernel function for Hhli for l = α, β, Y . Some popular choices of the kernel functions
are Gaussian kernel, Laplace kernel and polynomial kernel. Let Kl be the empirical kernel matrix.
We can rewrite (14) as
Sn = 3 [(HKαH) ◦ (HKβH) ◦ (HKYH)]++	(17)
n2
where H = In - n 11T, In is a n × n identity matrix and 1n is a n × 1 vector of ones, and
[A]++ = Pin=1 Pjn=1 Aij . This test statistics is also related to three-way Lancaster’s interaction
measure (Lancaster, 2002).
Fortunately, the computation of (17) only involves the matrix multiplication, the computational
procedure can be executed in parallel on GPU. In our experiment, the highly parallelized matrix
operation tremendously alleviated the computing time: essentially reducing from quadratic to almost
linear. In addition, we can reduce the sample size by sub-sampling r << n data points. Sub-sampling
the dataset is a popular technique to reduce computational burden and can efficiently approximate the
full data likelihood in a regression setting (Ma et al., 2015; Drineas et al., 2011). However, for very
large data set, the sub-sampling is not efficient. We consider a mini-batch algorithm to calculate the
score in each batch and used the averaged score as the final test statistics. This is also related to the
divide-and-conquer method which is widely used in kernel-based learning (Zhang et al., 2013; Shang
& Cheng, 2017; Liu et al., 2018).
4.5	Asymptotic distribution
The asymptotic distribution of Sn2 depends on the decay rate of the kernel of the product of RKHS. Sup-
pose that {μVαi, φVαi }∞=ι is a series of eigenvalue and eigenfunction pairs for HF, {μVβi, φVβi }∞=ι
is a sequence of basis for Hh1βi . If Y is continuous, we suppose that HhYi has the eigensystem,
{μVYi, φVαi}∞=ι. If Y is categorical, we suppose that HhYi has the eigensystem, {μVY i,φVαi}a=1.
Then We have the eigenvalue eigenfuntion pair for the tensor product space Hhai 0 HIe 乳 HhYi as
{μVαiμVβi μVY i,ΦVaiΦVβiΦVY i}
where να = 1, . . . , ∞, νβ = 1, . . . , ∞, νY = 1, . . . , ∞ (Y is continuous), and νY = 1, . . . , a - 1
(Y is categorical with a categories). For simplicity, we order the pairs in the decreasing order
of μρ, P = 1,..., ∞. The null hypothesis could be interpreted as factorization hypothesis, i.e.,
(X, Y) ⊥ Z ∨ (X, Z) ⊥ Y ∨ (Y, Z) ⊥ X or X, Y, Z are mutually independent.
Theorem 1. Suppose the kernel on H111 is square integrable. If Y is continuous variable, then
under H0, we have
∞
nsn → X μρep	(18)
ρ=1
where ρ are i.i.d. standard Gaussian random variables.
The proof of this theorem is shown in the Supplementary Materials A.1. The asymptotic distribution
of nSn2 only depends on the eigenvalues of the kernel. Theorem 1 is related to Wilks’ phenomenon
demonstrated in the classic nonparametric/semiparametric regression framework (Fan et al., 2001;
Shang & Cheng, 2013), i.e., the asymptotic distribution is independent of the nuisance parameters.
In practice, we fix the same kernel for the fully connected layers and the same kernel for the
convolutional layers. Thus, it provides a unified importance measure for all connections in the same
type of layer, which avoids the scaling problem faced by those pruning methods that use magnitudes
of weights as an importance measure. In addition, we can use the value of the test statistics as an
importance measure for pruning and bypass the effort of calculating the p-values since the order is
the same according to either of these two measures.
5	Results
We conducted experiments to test out the PCII method in two supervised image classification tasks:
MNIST and CIFAR10. We used TensorFlow to train DNNs. The back-end of PCII was implemented
in Fortran and R language. Programming interfaces were implemented to connect the back-end
calculations to TensorFlow. The experiments were run on a computer with one Nvidia Titan V GPU
and 48 CPU cores.
8
Published as a conference paper at ICLR 2020
PCII offers a convenient way to adjust compression rate by changing the p-value threshold ρ. For
other compression methods in consideration, the compression rate is usually controlled by some
hyper-parameters, which need to be tuned via an ad hoc trial and error strategy. Two types of
comparisons were carried out. In the first type, we adjust the compression rate of a method while
requiring its compressed DNN to achieve the test accuracy of the original uncompressed DNN. We
term this the lossless compression rates (LCR). Since it is very time-consuming to obtain an exact
test accuracy, we allow a 0.01% deviation from the desired test error rates. In the second type, we
compared the minimum testing error (MTE) of the compressed DNNs produced by different model
compression methods. MTE shows how a compression method can help increase the generalizability
and robustness of a DNN. We only included the results of the tested methods that we could obtain
their working codes to reproduce the results reported in their original papers. In addition, we did not
include methods that are not able to achieve the LCR of the corresponding test dataset.
5.1	MNIST DATASET
We tried two network architectures for MNIST (60k training images and 10k test images): a multilayer
perceptron (MLP) with two hidden layers of sizes 300 and 100, respectively, and a convolutional
neural network LeNet-5 (LeCun et al. (1998)). We trained LeNet-300-100 for 100 epochs to achieve
a test error of 1.69%. These approaches include two regularization based methods in Louizos et al.
(2017), as well as several pruning based methods in Han et al. (2015), and Guo et al. (2016). The
results are summarized in Table 1. PCII achieved the lowest MTE when the compression rate was
10x. Then, we further increased the compression rate until the error rate reached 1.70%. The results
show that PCII not only compressed a medium-sized MLP better than existing methods but also was
able to improve generalizability of a MLP via compression (i.e., the MTE is better than the LCR).
Dataset: MNIST, Network: LeNet-300-100
Criterion	Method	Error %	Compression Rate
	Original	1.69%	1x
	PCir	1.70%	26x
LCR	Han et al. (2015)	1.69%	15.1x
	Louizos et al.(2017)	1.70%	3.3x
	Guo et al. (2016)	1.70%	15x
	PCir	1.58%	10x
MTE	Han et al. (2015)	1.59%	12.2x
	Louizos et al.(2017)	1.70%	3.3x
	GUo et al.(2016)	1.70%	15x
Table 1: Experimental results for LeNet-300-100 on MNIST dataset.
Figure 3: The heatmap showing the PLR test result of the 784 × 300 connections between the input
layer and the first FC layer in Lenet-300-100. Each pixel represents a p-value of the corresponding
pair. The brighter color representing a smaller p-value. The width and height of the heatmap
correspond to the input dimension (784) and the size (300) of the first FC layer.
Interestingly, our inference results can also help in interpreting the fitted neural network. For example,
through inferring the importance of the connections between input layer and the the first hidden layer,
we can visualize the importance of the features in the input layer. Figure 3 plots the p-values of the
9
Published as a conference paper at ICLR 2020
associations between the network connections in the first layer and the final output. The heatmap
shows that a banded structure repeated 28 times, in which the central region tends to have smaller
p-values. The left and right margins of the heatmap show that the connections on the first and last few
channels are less relevant to the final output (i.e., have large p-values). This phenomenon is observed
because a written digit is usually located in the central part of a image.
Dataset: MNIST, Network: LeNet-5
Criterion	Method	Error %	Compression Rate
	Original	0.70%	1x
	PCir	0.69%	38x
LCR	Han et al. (2015)	0.71%	12.1x
	Louizos et al.(2017)	0.69%	1.4x
	Guo et al. (2016)	0.70%	32x
	PCIT	0.65%	10.7x
MTE	Han et al. (2015)	0.70%	6x
	Louizos et al.(2017)	0.68%	1.1x
	GUo et al.(2016)	0.70%	32x
Table 2: Experimental results for LeNet-5-caffe on MNIST dataset.
The LeNet-5 model (https://goo.gl/4yI3dL) is a modified version of LeCun et al. (1998). It
includes two convolutional layers with 20 and 50 filters, respectively, and a fully connected layer
with 500 nodes. The results are summarized in Table 2. PCII achieved both the lowest MTE and
highest LCR, again, for this model, demonstrating the broad applicability of the PCII strategy for
various neural network architectures.
5.2	CIFAR10 DATASET
To demonstrate the applicability of PCII to complex DNNs, we applied it to VGG16 (Zagoruyko,
2015), which was adapted for the CIFAR-10 dataset (Krizhevsky & Hinton, 2009). The network
consists of 13 convolutional and two fully-connected layers. The results are summarized in Table
3. The test error gradually decreases as the compression rate increases from 1x to 3x, and achieves
the MTE at 6.01%. When the compression rate is further increased, the test error begins to increase.
As the compression rate reaches 10x, the test error increases to 7.56% that is comparable to the test
error of the uncompressed VGG16. For this dataset, we only include the result for PCII due to the
limited resources. In fact, we could not obtain the results for other methods in comparison in three
days’ computing time.
Dataset: CIFAR10, Network: VGG16
Criterion	Method	Error	Compression Rate
	Original	7.55%	1x
LCR	PCir	7.56%	10x
MTE	PCn	6.01%	3x
Table 3: Experimental results for VGG16 on CIFAR10 dataset.
6	Discussion
We propose a statistically principled strategy to directly infer the importance of a connection in a
neural network via PCII, a hypothesis testing framework. The proposed PCII test provides a p-value
based measure on the importance of the connection through the connection’s association with the
final output. This type of direct quantification cannot be easily accomplished by the magnitude-based
pruning method. Although the two examples are relatively small in size, they demonstrated the broad
applicability of the PCII method and its improved power in network compression. Last but not least
we note that the PCII testing method can be easily generalized to a broad class of connection types
including the skip layer connections in RNN.
10
Published as a conference paper at ICLR 2020
7	Acknowledgement
XX and JL would like to acknowledge NSF and NIH for providing partial support or this work. PH
and LS would like to acknowledge NSF (NSF OAC 1920147) for providing partial support of this
work.
References
Sajid Anwar, Kyuyeon Hwang, and Wonyong Sung. Structured pruning of deep convolutional neural
networks. ACM Journal on Emerging Technologies in Computing Systems (JETC), 13(3):32, 2017.
Alain Berlinet and Christine Thomas-Agnan. Reproducing kernel Hilbert spaces in probability and
statistics. Springer Science & Business Media, 2011.
Petros Drineas, Michael W Mahoney, S Muthukrishnan, and Tamgs Sarl6s. Faster least squares
approximation. Numerische mathematik,117(2):219-249, 2011.
Jianqing Fan, Chunming Zhang, and Jian Zhang. Generalized likelihood ratio statistics and wilks
phenomenon. Annals of statistics, 29:153-193, 2001.
Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Scholkopf, and Alexander Smola. A
kernel two-sample test. Journal of Machine Learning Research, 13(Mar):723-773, 2012.
Chong Gu. Smoothing spline ANOVA models, volume 297. Springer Science & Business Media,
2013.
Yiwen Guo, Anbang Yao, and Yurong Chen. Dynamic network surgery for efficient dnns. In Advances
In Neural Information Processing Systems, pp. 1379-1387, 2016.
Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for
efficient neural network. In Advances in neural information processing systems, pp. 1135-1143,
2015.
Babak Hassibi, David G Stork, and Gregory J Wolff. Optimal brain surgeon and general network
pruning. In Neural Networks, 1993., IEEE International Conference on, pp. 293-299. IEEE, 1993.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.
Technical report, Citeseer, 2009.
H Lancaster. 0.(1969). the chi-squared distribution. Wiley, New York. MR, 40:6667, 2002.
Yann LeCun, John S Denker, and Sara A Solla. Optimal brain damage. In Advances in neural
information processing systems, pp. 598-605, 1990.
Yann LeCun, Leon Bottou, Yoshua Bengio, Patrick Haffner, et al. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for
efficient convnets. arXiv preprint arXiv:1608.08710, 2016.
Meimei Liu, Zuofeng Shang, and Guang Cheng. How many machines can we use in parallel
computing for kernel ridge regression? arXiv preprint arXiv:1805.09948, 2018.
Christos Louizos, Max Welling, and Diederik P Kingma. Learning sparse neural networks through
l_0 regularization. arXiv preprint arXiv:1712.01312, 2017.
Ping Ma, Michael W Mahoney, and Bin Yu. A statistical perspective on algorithmic leveraging. The
Journal of Machine Learning Research, 16(1):861-911, 2015.
Dmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov. Variational dropout sparsifies deep neural
networks. arXiv preprint arXiv:1701.05369, 2017.
Zuofeng Shang and Guang Cheng. Local and global asymptotic inference in smoothing spline models.
The Annals of Statistics, 41(5):2608-2638, 2013.
11
Published as a conference paper at ICLR 2020
Zuofeng Shang and Guang Cheng. Computational limits of a distributed algorithm for smoothing
spline. The Journal of Machine Learning Research ,18(1):3809-3845, 2017.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Peter Sollich and Christopher KI Williams. Understanding gaussian process regression using the
equivalent kernel. In International Workshop on Deterministic and Statistical Methods in Machine
Learning, pp. 211-228. Springer, 2004.
Enzo Tartaglione, Skjalg Leps0y, Attilio Fiandrotti, and Gianluca Francini. Learning sparse neural
networks via sensitivity-driven regularization. In Advances in Neural Information Processing
Systems, pp. 3882-3892, 2018.
Karen Ullrich, Edward Meeds, and Max Welling. Soft weight-sharing for neural network compression.
arXiv preprint arXiv:1702.04008, 2017.
Sergey Zagoruyko. 92.45 on cifar-10 in torch. Technical report, 2015.
Yuchen Zhang, John Duchi, and Martin Wainwright. Divide and conquer kernel ridge regression. In
Conference on Learning Theory, pp. 592-617, 2013.
Michael Zhu and Suyog Gupta. To prune, or not to prune: exploring the efficacy of pruning for model
compression. arXiv preprint arXiv:1710.01878, 2017.
A Supplementary Materials
A.1 Poof of Theorem 1
Let {μρ, φρ}∞=ι be the eigenvalues and eigenvectors for the tensor product space Hhai xH『i XHIY i.
By the dedomposition defined (8), we have
Et[φρ(t)] = 0,	(19)
for ρ = 1, . . . , ∞, i.e., the mean of the eigenfunction φρ is zero. By simple calculation, we have
∞
X μρ < ∞	(20)
ρ=1
for the exponential decayed kernels and polynomal decayed kernel with decay rate as i-m for m > 1.
Thus, the commonly used kernels such as Gausssian kernel, Laplase Kernel and linear or qudratic
Solblev kernel satisfy this requirement.
By Mercer’s theorem, we have the decomposition as
nn
nsn = 1 XX K 1(ti, tj)
n n∞
=	μρφρ(ti)φρ(tj )
i=1 j=1 ρ=1
∞n
=X(μρ(X Φ(ti))2)
ρ=1	i=1
∞
→ ∑μρeρ
ρ=1
where ρ are i.i.d. standard normal random variables. The last row is proved by applying the
Lindeberg-LGvy CLT to have PNi φ(ti) → EP since (1) and (2) holds. Then, by the Kolmogorov's
inequality and P∞=ι μρ < ∞, we have the last row holds.
12