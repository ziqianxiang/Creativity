Published as a conference paper at ICLR 2020
Deep Orientation Uncertainty Learning
based on a Bingham Loss
Igor Gilitschenski1, Roshni Sahoo1, Wilko Schwarting1, Alexander Amini1,
Sertac Karaman2, Daniela Rus1
1	Computer Science and Artificial Intelligence Lab, MIT
2	Laboratory for Information and Decision Systems, MIT
{igilitschenski, rsahoo, wilkos, amini, sertac, rus}@mit.edu
Ab stract
Reasoning about uncertain orientations is one of the core problems in many per-
ception tasks such as object pose estimation or motion estimation. In these scenar-
ios, poor illumination conditions, sensor limitations, or appearance invariance may
result in highly uncertain estimates. In this work, we propose a novel learning-
based representation for orientation uncertainty. By characterizing uncertainty
over unit quaternions with the Bingham distribution, we formulate a loss that nat-
urally captures the antipodal symmetry of the representation. We discuss the inter-
pretability of the learned distribution parameters and demonstrate the feasibility
of our approach on several challenging real-world pose estimation tasks involving
uncertain orientations.
1 Introduction
Reasoning about uncertain poses and orientations, specifically 3-dimensional (3d) positions
and 3-axes orientations, is one of the main inference tasks in computer vision (Sattler et al.,
2019), robotics (Glover et al., 2011), aerospace (Crassidis & Markley, 2003), and other fields.
Proper representation and estimation of uncertainty is im-
portant, e.g. when dealing with structural ambiguities in
object pose estimation or coping with sensor corruption.
In vision and robotics tasks, high levels of pose uncer-
tainty may occur due to potentially adversarial conditions
that arise in real-world scenarios. A principled approach
to uncertainty quantification allows for better execution of
planning and situation-awareness tasks such as grasping,
tracking, and motion estimation.
When representing uncertainties over poses, the position
can be modeled using a Gaussian distribution. This ap-
proach is well-motivated by the Central Limit Theorem
and widely used in probabilistic deep learning models.
However, this paradigm cannot be as easily applied to
modeling periodic quantities, such as the orientation of
an object. Therefore, Gaussian models become unsuit-
able particularly in learning regimes involving high un-
certainties where one cannot assume local linearity of the
underlying space. In this work, we set out to develop a
principled probabilistic deep learning approach capable
of coping with uncertain orientations.
Currently, most deep learning approaches that predict
poses or rigid-body motions suffer from at least one of
three drawbacks: 1) they do not model the uncertainty at
Figure 1: Objects from the T-LESS
dataset and the corresponding orienta-
tion uncertainty predicted by the model
trained on the newly proposed Bingham
loss, which is capable of capturing rota-
tional symmetries.
all and merely focus on the accuracy of the predicted pose, 2) they make simplifying assumptions
not taking into account that the orientation is defined on a periodic manifold, making the approach
1
Published as a conference paper at ICLR 2020
only suitable in low-noise regimes, or 3) even when trying to account for periodicity, no dependency
is assumed between the orientation axes and usually an Euler angle-based representation is required.
To this point, there are no probabilistic deep learning models for uncertainty of orientations that take
the geometry of the underlying domain into account.
In this work, we close this research gap by proposing a probabilistic deep learning model inspired
by Directional Statistics (Mardia & Jupp, 1999). We present a loss based on the Bingham distri-
bution (Bingham, 1974), an antipodally symmetric distribution on the sphere. With this loss, we
represent uncertain orientations by modeling uncertainty over unit quaternions. Our contributions
involve Bingham parameter learning using backpropagation through a Gram-Schmidt method to en-
sure orthonormalization, efficient approximate evaluation of the normalization constant of the Bing-
ham distribution from a lookup table, and backpropagating through an interpolation scheme during
learning. We also discuss interpretability of the Bingham distribution parameters and establish the
feasibility of the approach through extensive evaluations.
In summary, this work makes the following contributions: 1) We propose the Bingham loss, a novel
loss function for deep learning-based predictions of orientations and their uncertainty. 2) We provide
a methodology for making the newly proposed loss and its normalization constant computationally
tractable in a deep learning pipeline. 3) We demonstrate multi-modal orientation prediction using a
Bingham variant of Mixture Density Networks. 4) We demonstrate how our approach outperforms
the state-of-the-art on challenging pose and orientation estimation tasks1.
2	Background: Bingham Distribution for Uncertain
Orientations
Unit quaternions are a widely used representation for object orientation in 3d space. They are
more compact than rotation matrices, and unlike Euler angles, do not suffer from degeneracies
such as Gimbal lock. Additionally, quaternions provide a convenient mathematical notation where
the quaternion product, q1 q2, of two unit quaternions q1 , q2 ∈ H1 results in a concatenation
of the rotations represented by each of the quaternions individually. A full introduction to this
representation by given in Kuipers (1999) and notational aspects are discussed by Sommer et al.
(2018). In this work, a quaternion q1i + q2j + q3k + q4 will be interpreted as a vector q ∈ R4. It
is important to note that the definition of unit quaternions is equivalent to the vector q being of unit
length ||q|| = 1. Furthermore, the quaternions q and -q represent the same orientation. Therefore,
representing uncertain orientations using quaternions requires a probability distribution on the 4d
hypersphere that exhibits antipodal symmetry, i.e. for the density function f (∙) of this distribution
f(q) = f (-q) has to hold.
A probability distribution exhibiting these properties was proposed by Bingham (1974). It arises
by conditioning a zero mean Gaussian to unit length. The Bingham distribution is given in terms
of its p.d.f. as p(x; M, Z) = N (MZM>)-1 exp(x> MZM> x) ,where x ∈ R4 with ||x|| = 1,
N (MZM>) is a normalization constant, M ∈ R4×4 orthogonal, and Z = diag(z1, z2, z3, 0) ∈
R4×4 diagonal, with diagonal entries zi <= 0 and the last entry being zero. We use the notation
Bingham(M, Z). The restriction on the range of the diagonal entries in Z has numerical and rep-
resentational convenience reasons. It can be shown that Bingham(M, Z) = Bingham(M, Z + cI)
for all c ∈ R with I ∈ R4×4 denoting the identity matrix. Similarly, changing the order of diagonal
entries in Z has no effect on the distribution as long as the columns in M are permuted accordingly.
In the definition above, the parameters M and Z bear some similarity to the mean and variance
of a Gaussian. The density obtains its maxima at ±M:,4 (the fourth column of M) which can
be thought of as a mean orientation respecting the manifold structure. The diagonal entries of Z
can be interpreted as dispersion parameters, and the first three columns of M can be interpreted as
the directions of the dispersion (the Gaussian analog is the orientation of the covariance ellipsoid).
Bingham distributions allow for representation of uniform priors over individual axes or even the
entire space, making them superior to Gaussians in any of the usual orientation representations.
1Code available at https://github.com/igilitschenski/deep_bingham
2
Published as a conference paper at ICLR 2020
(a) S1
(b) S2
Figure 2: Densities of the Bingham distribution represented for different dimensionality. For the
circular case (a), the density is shown as a function of unit vectors on the plane. For the spherical
case (b), it is shown as a heatmap on a 3d unit sphere. For the 4d case (c), which is of our particular
interest, we visualize the mode of the Bingham in terms of the coordinate system orientation repre-
sented by the corresponding quaternion. Then, we draw samples from the distribution and visualize
each sample as a potential coordinate arrow endpoint for each axis (i.e. each sample drawn from
the Bingham distribution is represented by three points in the plot). This representation allows us to
simultaneously represent the orientation and the corresponding uncertainty.
One of the main challenges of using the Bingham distribution is the computation of its normalization
constant
N(MZM>)
||q||=1
exp(q>MZM>q) dq ,
which is a Hypergeometric function of matrix argument (Herz, 1955). Evaluating these functions
imposes a high computational burden and is still an area of active research (Koev & Edelman, 2006;
Kume et al., 2013; Koyama et al., 2014; Kume & Sei, 2018). Using the transformation theorem and
the fact that M is orthogonal, the normalization constant can be simplified as N (MZM> ) = N (Z),
making it merely a function of the three parameters zi (i = 1, 2, 3) and motivating the use of pre-
computed lookup tables in practice.
Furthermore, to make the uncertainty of a Bingham Distribution more interpretable in practice, we
propose the use of Expected Absolute Angular Deviation (EAAD) which is defined as
EAAD(Z) =
||q||=1
θ(q, e) ∙ p(q; I, Z) dq ,
where p(∙) is the Bingham(L Z) density, I is the identity matrix, e = [0, 0, 0,1] is the vector
corresponding to the unit quaternion representing the identity and θ(q, e) = 2 ∙ arccos(∣hq, e)|)
denotes the angular distance between q and e. The EAAD describes the expected angular deviation
from the “mean” orientation. It can be loosely thought of as the orientation counterpart to the
standard deviation in Euclidean space. For the same reason as in the normalization constant, the
EAAD computation does not involve the parameter M.
3	Deep Orientation Uncertainty Learning
The Bingham distribution is the main component of the proposed probabilistic framework for rep-
resenting deep learned uncertain orientations. Drawing inspiration from Mixture Density Net-
works (Bishop, 1994), we propose using the Bingham distribution’s negative log-likelihood as a
loss function
L(y, M, Z) = - log p(y; M,Z) = -y>MZM>y + log N (Z) ,
with M, Z as defined above and y being the orientation label given in the training data. We use a
neural network to learn M and Z, end-to-end, directly from the input data (e.g. RGB images). From
3
Published as a conference paper at ICLR 2020
Input
Figure 3: The proposed orientation uncertainty estimation pipeline predicts the parameters of a
Bingham distribution for representing uncertain unit quaternions. Backpropagation through an in-
terpolator and use of a lookup table allows for avoiding evaluations of the computationally expensive
Bingham normalization constant.
this prediction, the point estimate of y is obtained as y = M：,4 as the last column corresponds to
the highest diagonal entry of Z and thus represents one of the modes of the distribution (the other
being -y due to antipodal symmetry).
No costly evaluation of the normalization constant is required and no major computational chal-
lenges arise in the special case where the dispersion parameter Z is known and not predicted by
a neural network. However, as our goal is the modeling of uncertainty, we propose methods for
modeling M and Z as well as backpropagating through N (Z).
3.1	Modeling of distribution parameters
In order to obtain predictions M and Z, We require a 19 dimensional output (o ∈ R19) of the pre-
dictor network (3 outputs for Z, 16 outputs for M). On its own, these outputs do not satisfy the
above-mentioned constraints on the Bingham distribution parameters. Thus, We define the differen-
tiable transforms TM : R16 → R4×4 and TZ : R3 → R4×4 that transform these outputs such that
the constraints are satisfied.
The transform TZ is obtained as TZ(o1,o2,o3) = diag(Z1,Z2,Z3,0) with Zi = - exp(θi). For
computing M, we first subdivide 04,..., 019 into four vectors Vi ∈ R4 (i = 1,..., 4). Then,
we apply the Gram-Schmidt orthonormalization method to these vectors according to mi =
Normalize(Vi — Pk-=ILhmk, Vii ∙ Imk) with i ∈ {1, 2, 3, 4} and Normalize(X) = x/ ∣∣x∣∣. Fi-
nally, the prediction M is obtained as TM(03,..., 019) = [m 1,..., m4], and M is orthogonal by
construction.
3.2	Backpropagation through the Bingham Normalization Constant
As mentioned earlier, computation of the Bingham normalization constant is numerically burden-
some. This is also true for its derivatives which can be shown to be proportional to the normalization
constant of Bingham distributions of higher dimension (Kume & Wood, 2007). A forward-backward
pass for one single data point requires 4 evaluations of hypergeometric functions of matrix argument.
We avoid this by precomputing a lookup table for N (Z) at L different locations ti (with Zi =
diag([t>, 0]). This table is then used to build an interpolator fN(z) = PL-I Wiφ(∣∣z - ti||) with
z ∈ R3 and φ denoting a radial basis function. The weights wi can also be precomputed during
generation of the interpolator. Thus, we can approximate N (Z) ≈ /n (z) and RzN (Z) ≈ VzfN (z).
To the best of our knowledge, this is the first time that a lookup table based interpolation mechanism
has been included in the computation graph of a neural network.
3.3	Multi-Modal Prediction
A Bingham variant of Mixture Density Networks can be used to obtain multi-modal predictions.
However, MDNs are hard to train even in the Gaussian case. Following the discussion in Makansi
4
Published as a conference paper at ICLR 2020
et al. (2019), we separate the training in two stages. In the first stage, we only learn to predict M and
assume the dispersion to be fixed with Z = diag(-a, -a, -a, 0). In practice a ∈ R+ can usually
be set to 1 as it merely scales the cost term. In the second stage, we train to predict M and Z jointly.
Our evaluation will show that in high uncertainty regimes, this training method is also helpful for
the unimodal case.
4	Experiments
In this section we evaluate the proposed Bingham loss on its ability to learn calibrated uncertainty
estimates for orientations. This goes beyond comparing point estimates of orientations; we evaluate
how well the estimated distribution of orientations can explain the data. We will also show that
the Bingham distribution representation is capable of capturing ambiguity and uncertainty in SO(3)
better than state-of-the-art approaches.
We investigate characteristics and behaviors by training neural networks on two head-pose datasets,
IDIAP (Odobez, 2003) and UPNA (Ariz et al., 2016), as well as the object pose dataset T-
LESS (Hodan et al., 2017). We show the capability of calibrated uncertainty estimation by applying
artificial label-noise to IDIAP and UPNA and observing that the Bingham parametrization allows
for accurate prediction of uncertainty. In addition to calibrated uncertainty estimation, we demon-
strate advanced capabilities in the face of object orientation ambiguity on the T-LESS dataset by
visualizing the predicted distributions for different orientation ambiguous objects, e.g. symmetric,
and comparing to objects with clear orientation.
4.1	Architecture and Experimental Setup
We seek to estimate the Bingham distribution parameters directly from image data. Our pipeline is
shown in Figure 3 and begins by passing an image input to a convolutional encoder, in our case a
standard ResNet-18 network followed by a fully connected layer, populating the entries of o1, o2, o3
and v1 , v2, v3, v4. Subsequently, Z is computed by constrained diagonalization of o1, o2, o3, and
Gram-Schmidt orthonormalization of v1, v2, v3, v4 yields M, as described in Section 3.1. To eval-
uate the Bingham loss, the normalizer N (Z) needs to be queried from the RBF lookup table,
Section 3.2. Differentiation of the interpolator via finite differences enables us to back-propagate
through the entire pipeline. All models were implemented in PyTorch and optimized with the Adam
optimizer.
We create the lookup table by numerical integration. More precisely, we use Scipy’s tplquad
method to compute a triple integral for each Z in the table. We set the relative error tolerance to 1e-3
and the absolute error tolerance to 1e-7. The actual computed integral is
N(Z)= /	/	/ exp (t(Φ1,Φ2,Φ3)>Zt(Φ1,Φ2,03)) ∙ sin(Φι)2 ∙ sin(φ2)dφ1 dφ2 dφ3 ,
with
t(φ1,φ2,φ3)
^sin(φι) ∙ sin(Φ2) ∙ sin(φ3)-
sin(φι) ∙ sin(Φ2) ∙ cos(φ3)
sin(φι) ∙ cos(φ2)
cos(φ1)
to account for a transformation of coordinates from unit quaternions to 4d spherical coordinates.
Because we use the Bingham log likelihood as our optimization objective, we compute the logarithm
before the interpolation to avoid failure at locations where the interpolator wrongly outputs negative
values.
4.2	Baselines
We compare our work with the approach proposed by Prokudin et al. (2018). It also uses a loss
based on directional statistics, specifically the Von Mises distribution. The Von Mises distribution
can be thought of as a circular analog of the Normal distribution. In order to apply this approach
to our setting, orientations are modeled with Euler angles. The loss then consists of the sum of
log-likelihoods for each angle. While this approach can properly account for periodicity of the
5
Published as a conference paper at ICLR 2020
underlying data, we expect it to fail in cases where the underlying uncertainty is not axis aligned
because it does not account for dependencies between uncertain rotation axes.
Furthermore, we also evaluate several different representations of the parameter matrix M. We
consider the classical Gram-Schmidt (CGS), modified Gram-Schmidt (MGS), and the matrix rep-
resentation of the quaternion (QM) used by Birdal et al. (2018). Finally, we also include two non-
probabilistic orientation prediction baselines. The first one is based on a Mean Square Error (MSE)
between the predicted and ground truth quaternion. The second one is based on a cosine loss applied
to each angle’s biternion as discussed by Prokudin et al. (2018).
4.3	Evaluation Metrics
To evaluate error metrics over predicted orientations, it is unsuitable to compute the RMSE over
angles, since it does not sufficiently consider the spherical nature of the underlying data. Instead, we
make use of the Mean Absolute Angular Deviation (MAAD) which has also been used by Prokudin
et al. (2018). It is based on the angular distance between two angles defined above. We also com-
pute the EAAD to assess the quality of the results. Additionally, the difference between EAAD
and MAAD serves as an indicator of the quality of the predicted uncertainty. The acceptable differ-
ence in practice is application dependent. For the cases of the Von Mises distribution parameters,
EAAD computation is carried out in a similar way as for the Bingham defined above. EAAD is
calculated over the learned dispersion parameters for each example and averaged. The quality of the
respective model is measured in terms of log-likelihood to indicate the goodness of an individual
fit. For MDNs, we additionally report a Mean Minimum Absolute Angular Deviation (MMAAD),
which uses the component closest to ground-truth for absolute angular deviation computation. The
MAAD and EAAD for MDNs are computed in a per-component fashion and then weighted using
the predicted mixture weights.
4.4	Calibrated Uncertainty Estimation
We evaluate the distribution fit on the head pose datasets UPNA and IDIAP, which consist of head
images from a video of several people inside a room. Each image is annotated with head orientation
given by pan, tilt and roll angles. We use these datasets as they provide accurate labels and allow for
carrying out experiments involving artificial label noise.
The results on the raw dataset are shown in Table 1. They demonstrate that the general		EAAD	UPNA MAAD	LL	EAAD	IDIAP MAAD	LL
performance for point esti- mates, indicated by MAAD, of the Bingham distribution remains on a similar level as the Von Mises distribu- tion and the non-probabilistic approaches. In this setting, most motions of the subjects’ heads are aligned with the grav- ity axis allowing both distri- butions to successfully capture	BD-CGS BD-MGS BD-QM VM MSE Cosine Table 1: Bi ror (MSE), a UPNA and I	0.10	0.11	4.70 0.10	0.13	3.87 0.10	0.16	0.31 0.13	0.11	3.69 -	0.12	- -	0.12	- ngham (BD), Von Mises nd cosine based loss pred DIAP datasets.			0.10	0.09	4.49 0.10	0.10	4.58 0.10	0.09	4.74 0.12	0.09	2.08 -	0.10	- -	0.10	- (VM), Mean Square Er- iction performance on raw		
the noise. However, the Bingham still attains a higher log-likelihood and a smaller gap between
MAAD and EAAD. Similarly, the parametrization of the concentration matrix M has a relatively
small impact on the estimation performance. Although MGS has stronger robustness guarantees
than CGS (the latter has a quadratic dependency on the condition number of the input matrix, see
Giraud et al. (2005) for a discussion of both), the condition of the input is not poor enough to impact
performance. While the quaternion matrix approach is easier to train, it also loses some of the ex-
pressiveness of the Bingham distribution because the underlying mapping (from quaternions to the
space of orthogonal matrices) is not surjective.
To estimate how well the predicted uncertainties are calibrated, we add artificial noise by drawing
random perturbations from the Bingham distribution with varying z1, z2, and z3 parameters and
applying them to the quaternion labels before training. Both UPNA and IDIAP contain negligible
6
Published as a conference paper at ICLR 2020
EAAD
EAAD
-z3 MAAD -z1	-z2 -z3 MAAD
-z1
-Zl -Z2 -Z3 EAAD -zι	-z2
Label noise No noise 0	20	20
UPNA 497 497 497 0.10-^19	19-
±0.4 ±0.4
20	0.52
19	0.54
±0.5 0.69
250~150 50~~0.22
T86~105^^63	0.23
±78 ±30 ±15 0.29
IDIAP 499 499 499 0.10	19	19	18	0.55	167	164	47	0.24
±0.5	±	0.5 ±0.3	0.59	±17	±20	±3	0.29
150 100 75	0.23
130 114 74	0.23
±35 ±10 ±14 0.28
93	87	76	0.25
±8	±8	±7 0.28
EAAD
-z2	-z3 MAAD
300 300 300 0.13
303 300 295 0.13
±16 ±16 ±17 0.20
300 294 280 0.13
±24 ±25 ±35 0.20
Table 2: Testing accuracy of uncertainty calibration. Prior to training, we perturb the labels with
noise sampled from the Bingham distribution with M equal to the identity and varying z1, z2, z3.
The figures represent the different noise distributions.
amounts of noise, so the dispersion of the noise distribution should be captured by the learned Z to
high accuracy. An evaluation of uncertainty and label noise is shown in Table 2. For the case of
no noise, the Bingham uncertainty parameters approximate the highest certainty levels represented
in the lookup table. Thus, the maximum and minimum values in the lookup table automatically
become the bounds of what certainty levels can be represented by the proposed loss. When noise
is applied to the training labels, the learned uncertainty parameters closely match the dispersion of
label noise, so the predicted EAAD accurately captures the EAAD corresponding to the dispersion
of the label noise distribution. We note that the MAAD is slightly higher than the true and estimated
EAAD values. This overconfidence effect is typical in probabilistic deep learning and also arises
when predicting the parameters of a Gaussian (Amini et al., 2019). In addition, we evaluated a
scenario where the noise is newly sampled and applied to the true labels in each iteration (rather than
corrupting the labels with the sampled noise prior to training). In this scenario, the EAAD computed
from the learned dispersion parameters, the true EAAD, and the MAAD are approximately equal in
value. While this scenario is less realistic in practice (and thus not visualized), it provides further
evidence for representational consistency of the loss.
4.5	Handling Ambiguous Data
We use the T-LESS dataset for evaluating the proposed model using ambiguous data. It contains
images of 30 different textureless objects taken from different cameras. We use the Kinect RGB
single-object images all of which are split into training, test, and validation sets. At a coarse scale
most of the objects in the dataset exhibit rotational or other symmetries. At a finer scale some of
these ambiguities disappear due to smaller structures. On the one hand, we expect those to be more
challenging to learn. On the other hand, capturing these structures allows for very precise orientation
estimation. To be able to disregard these structures, we create a variant of T-LESS where we add blur to each image using a uniform 10px × 10px kernel.				
We carry out two sets of experiments. In the first set of experiments, we train orientation estimation	Method	Log-likelihood MAAD EAAD		
models for 5 epochs using the Bingham loss (BD-5)	VM-5	-0.12	0.48	0.33
and the Von Mises loss (VM-5) on the blurred and	BD-5	2.82	1.57	1.58
original set of images. This allows to investigate the	VM-5 w. blur	-0.03~~	0.56	0.44
uncertainty estimation properties before the network	BD-5 w. blur	2.71	1.59	1.58
captures the finer grained structures. In the second
set of experiments, we use the original set of images
to evaluate multi-modal orientation prediction using
the two-stage training approach for models with 1
Table 3: Results on the T-LESS dataset in the
high uncertainty regime.
(BD-MDN-1), 2 (BD-MDN-2), and 4 (BD-MDN-4) mixture components. Each stage is carried
out for 30 epochs. The comparison methods use Von Mises (VM), Mean Square Error (MSE), and
Cosine losses with an overall training duration of 60 epochs (or until convergence if that is earlier).
The results for the first set of experiments are visualized in Table 3. As expected, both approaches
are on average far off in terms of the true orientation. While Von Mises performs better on the
MAAD, we observe that there is a larger difference between the MAAD and EAAD values for the
7
Published as a conference paper at ICLR 2020
Von Mises distribution than the Bingham distribution. This indicates that the uncertainty estimates
of the Von Mises distribution may be overconfident. On the other hand the Bingham distribution
better captures the uncertainty over individual axes. One interesting insight is that allowing for
uniform distributions over individual non-aligned periodic axes can make it hard for the learning
method to pick up on the proper pose and thus may require pre-training on the pure pose estimation
task in such regimes.
In the second set of experiments, as visu-
alized in Table 4, we use this training strat-
egy for all Bingham MDN models result-
ing in robust convergence behavior. How-
ever, the unimodal Bingham (BD-MDN-
1) converges slower than Von Mises (VM)
thus achieving a higher MAAD, which
is adequately captured by the Bingham’s
EAAD. For multiple mixture components,
we obtain a very low MAAD and can ob-
serve again the phenomenon of the lookup
table limitations in the EAAD. Thus, the
Method	Log-likelihood MAAD MMAAD EAAD			
VM	3.73	0.10	-	0.17
BD-MDN-1	5.00	0.20	-	0.21
BD-MDN-2	6.17	0.07	0.06	0.12
BD-MDN-4	6.19	0.06	0.05	0.10
MSE	-	0.22	-	-
Cosine	-	0.10	-	-
Table 4: Results on the T-LESS dataset involving multi
modal prediction.
MAAD achieved during the first training stage can not only be used for inspecting the network’s
accuracy but also for determining the minimum Z parameter values stored in the lookup table. An-
other interesting phenomenon can be observed in the EAAD and MAAD of the VM loss. As the
representation required by Von Mises assumes that each axis is independent, EAAD is computed
per rotation axis. This results in an overapproximation of the uncertainty overall. For the non-
probabilistic losses, the cosine loss achieves better performance which is probably due to better
consideration of the underlying geometry. In summary, while the proposed Bingham loss shares
the general challenges of training Mixture Density Networks, it better captures the underlying noise
structure by explicitly modeling dependencies between rotation axes.
5	Discussion and Related Work
Quantifying and representing uncertainty by and in neural networks has been a subject of extensive
research initially focused on modeling probability distribution parameters (Nix & Weigend, 1994)
and mixture distributions (Bishop, 1994) as neural network outputs. More recent approaches fo-
cus on improving understanding of the underlying uncertainties (Kendall & Gal, 2017), providing
scalable techniques for estimating predictive uncertainty (Lakshminarayanan et al., 2017), and sta-
bilizing training to avoid mode collapse (Makansi et al., 2019). The present work is orthogonal to
these approaches in the sense that it focuses on proper modeling of the underlying geometric domain
and coping with a computationally demanding normalization constant.
Handling of poses and orientations has been extensively studied in the context of Bayesian filtering
for applications such as spacecraft attitude estimation (Crassidis & Markley, 2003) and ego-motion
estimation (Bloesch et al., 2015), where one can often assume the underlying uncertainties to be
small. This allows for leveraging local-linearity and using the Gaussian distribution. Recently,
methods based on directional statistics enabled modeling of high uncertainty levels for inferring
orientations (Gilitschenski et al., 2016) and full poses (Glover et al., 2011; Glover & Kaelbling,
2014; Srivatsan et al., 2016) by using the Bingham distribution. Drawing inspiration from these
results, this work extends the applicability of these approaches to probabilistic deep learning models.
Particularly in computer vision, deep learning has been applied to spherical regression and pose
estimation problems (Liao et al., 2019; Huang et al., 2018). These applications involve inferring
object (Brachmann et al., 2014; Hodan et al., 2018; Li et al., 2018b;a; Manhardt et al., 2019; SUn-
dermeyer et al., 2018; Tekin et al., 2018; Wang et al., 2019b;a), body (Yang et al., 2019), and camera
poses (Clark et al., 2017; Sattler et al., 2019; Wang et al., 2017; 2018). In all of these scenarios there
is a mUltitUde of soUrces for potentially high Uncertainties sUch as the Use of low-resolUtion data
(e.g. tracking pose of distant pedestrians), absence of textUres (e.g. when operating on depth data),
or motion blUr (e.g. dUe to high speeds in ego-motion estimation). However, most of the existing
approaches merely focUs on inferring the pose bUt do not accoUnt for the Underlying Uncertainty.
8
Published as a conference paper at ICLR 2020
The representation proposed in our work closes this gap by allowing for neural networks to output
well-calibrated orientation uncertainty estimates.
Only a few approaches consider modeling the uncertainty of orientations for deep learning based
pose estimation. PoseRBPF by Deng et al. (2019) discretizes the orientation space into over 190 000
bins and learns a codebook to allow for tractable inference. In contrast to that approach, we do
not require an a priori discretization and can directly obtain interpretable estimates. Similarly to us,
Prokudin et al. (2018) propose a loss based on directional statistics. By making use of the Von Mises
distribution, their work can properly account for periodicity of circular data. However, as we have
shown in our evaluations, this approach cannot properly account for dependencies between different
axes and thus, struggles when the underlying uncertainty is not axis aligned.
6	Conclusion
In this work, we introduced the Bingham loss, a loss function based on the Bingham distribution
that enables neural networks to predict uncertainty over unit quaternions and thus uncertain orienta-
tions. This allows for using (rotation-)symmetric objects and ambiguous sensor data in the context
of pose and orientation estimation. In addition, we demonstrate how to cope with intractable likeli-
hoods in deep learning pipelines by using non-linear interpolation and lookup tables as part of the
computation graph.
The presented approach is directly usable in existing probabilistic deep learning techniques. More-
over, we demonstrate its applicability for mixture density models. The choice of parametrization
remains one of the main design decisions in pose and orientation estimation pipelines. Our work
supports the case for using quaternions over other parametrizations for deep learning. It also mo-
tivates further research on how to properly model dependencies between uncertain periodic and
non-periodic quantities.
Acknowledgments
This work was supported in part by NSF Grant 1723943, the Office of Naval Research (ONR) Grant
N00014-18-1-2830, and Toyota Research Institute (TRI). This article solely reflects the opinions
and conclusions of its authors and not TRI, Toyota, or any other Toyota entity. Their support is
gratefully acknowledged.
References
Alexander Amini, Wilko Schwarting, Ava Soleimany, and Daniela Rus. Deep Evidential Regression.
arXiv preprint:1910.02600, 2019.
Mikel Ariz, Jose J. Bengoechea, Arantxa Villanueva, and Rafael Cabeza. A novel 2D/3D database
with automatic face annotation for head tracking and pose estimation. Computer Vision and Image
Understanding,148:201-210, 2016.
Christopher Bingham. An Antipodally Symmetric Distribution on the Sphere. The Annals of Statis-
tics, 2(6):1201-1225, 1974.
Tolga Birdal, Umut Simsekli, Mustafa Onur Eken, and Slobodan Ilic. Bayesian Pose Graph Op-
timization via Bingham Distributions and Tempered Geodesic MCMC. In Advances in Neural
Information Processing Systems (NeurIPS), 2018.
Christopher M. Bishop. Mixture Density Networks. Technical report, Neural Computing Research
Group, Aston University, 1994.
Michael Bloesch, Sammy Omari, Marco Hutter, and Roland Siegwart. Robust visual inertial odom-
etry using a direct EKF-based approach. In Proceedings of the International Conference on Intel-
ligent Robots and Systems (IROS), 2015.
Eric Brachmann, Alexander Krull, Frank Michel, Stefan Gumhold, Jamie Shotton, and Carsten
Rother. Learning 6D Object Pose Estimation Using 3D Object Coordinates. In Proceedings of
the European Conference on Computer Vision (ECCV), 2014.
9
Published as a conference paper at ICLR 2020
Ronald Clark, Sen Wang, Hongkai Wen, Andrew Markham, and Niki Trigoni. VINet: Visual-Inertial
Odometry as a Sequence-to-Sequence Learning Problem. In Proceedings of the AAAI Conference
on Artificial Intelligence (AAAI), 2017.
John L. Crassidis and F. Landis Markley. Unscented Filtering for Spacecraft Attitude Estimation.
Journal of Guidance, Control, and Dynamics, 26(4):536-542, 2003.
Xinke Deng, Arsalan Mousavian, Yu Xiang, Fei Xia, Timothy Bretl, and Dieter Fox. PoseRBPF:
A Rao-Blackwellized Particle Filter for 6D Object Pose Estimation. In Proceedings of Robotics:
Science and Systems (RSS), 2019.
Igor Gilitschenski, Gerhard Kurz, Simon J. Julier, and Uwe D. Hanebeck. Unscented Orientation
Estimation Based on the Bingham Distribution. Transactions on Automatic Control, 61(1), 2016.
LUc Giraud, Julien Langou, Miroslav RozloZnik, and Jasper van den Eshof. RoUnding error analysis
of the classical Gram-Schmidt orthogonalization process. Numerische Mathematik, 101(1), 2005.
Jared Glover and Leslie Kaelbling. Tracking the spin on a ping pong ball with the quaternion
Bingham filter. In Proceedings of the International Conference on Robotics and Automation
(ICRA), 2014.
Jared Glover, Radu Rusu, and Gary Bradski. Monte Carlo Pose Estimation with Quaternion Kernels
and the Bingham Distribution. In Proceedings of Robotics: Science and Systems (RSS), 2011.
Carl S. Herz. Bessel Functions of Matrix Argument. The Annals of Mathematics, 61(3):474, 1955.
TomaS Hodan, Pavel Haluza, Stepan Obdrzalek, Jin Matas, ManoliS Lourakis, and Xenophon ZabU-
lis. T-LESS: An RGB-D dataset for 6D pose estimation of texture-less objects. Winter Conference
on Applications of Computer Vision (WACV), 2017.
Tomas Hodan, Frank Michel, Eric Brachmann, Wadim Kehl, Anders Glent Buch, Dirk Kraft,
Bertram Drost, Joel Vidal, Stephan Ihrke, Xenophon Zabulis, Caner Sahin, Fabian Manhardt,
Federico Tombari, Tae-Kyun Kim, jir´ Matas, and Carsten Rother. BOP: Benchmark for 6D Ob-
ject Pose Estimation. In Proceedings of the European Conference on Computer Vision (ECCV),
2018.
Siyuan Huang, Siyuan Qi, Yinxue Xiao, Yixin Zhu, Ying Nian Wu, and Song-Chun Zhu. Cooper-
ative holistic scene understanding: Unifying 3d object, layout, and camera pose estimation. In
Advances in Neural Information Processing Systems (NeurIPS), pp. 206-217, 2018.
Alex Kendall and Yarin Gal. What Uncertainties Do We Need in Bayesian Deep Learning for
Computer Vision? In Advances in Neural Information Processing Systems (NeurIPS), 2017.
Plamen Koev and Alan Edelman. The efficient evaluation of the hypergeometric function ofa matrix
argument. Mathematics of Computation, 75(254):833-847, 2006.
Tamio Koyama, Hiromasa Nakayama, Kenta Nishiyama, and Nobuki Takayama. Holonomic gra-
dient descent for the FisherBingham distribution on the d-dimensional sphere. Computational
Statistics, 29(3-4):661-683, 2014.
Jack B. Kuipers. Quaternions and rotation sequences : a primer with applications to orbits,
aerospace, and virtual reality. Princeton University Press, 1999.
Alfred Kume and Tomonari Sei. On the exact maximum likelihood inference of FisherBingham
distributions using an adjusted holonomic gradient method. Statistics and Computing, 28(4):
835-847, 2018.
Alfred Kume and Andrew T.A. Wood. On the derivatives of the normalising constant of the Bingham
distribution. Statistics & Probability Letters, 77(8):832-837, 2007.
Alfred Kume, Simon P. Preston, and Andrew T. A. Wood. Saddlepoint approximations for the nor-
malizing constant of Fisher-Bingham distributions on products of spheres and Stiefel manifolds.
Biometrika, 100(4):971-984, 2013.
10
Published as a conference paper at ICLR 2020
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and Scalable Predictive
Uncertainty Estimation using Deep Ensembles. In Advances in Neural Information Processing
Systems (NeurIPS), 2017.
Chi Li, Jin Bai, and Gregory D Hager. A Unified Framework for Multi-view Multi-class Object Pose
Estimation. In Proceedings of the European Conference on Computer Vision (ECCV), 2018a.
Yi Li, Gu Wang, Xiangyang Ji, Yu Xiang, and Dieter Fox. DeepIM: Deep Iterative Matching for
6D Pose Estimation. In Proceedings of the European Conference on Computer Vision (ECCV),
2018b.
Shuai Liao, Efstratios Gavves, and Cees G M Snoek. Spherical Regression: Learning Viewpoints,
Surface Normals and 3D Rotations on N-Spheres. In Proceedings of the Conference on Computer
Vision and Pattern Recognition (CVPR), 2019.
Osama Makansi, Eddy Ilg, Ozgun Cicek, and Thomas Brox. Overcoming Limitations of Mixture
Density Networks: A Sampling and Fitting Framework for Multimodal Future Prediction. In
Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR), 2019.
Fabian Manhardt, Wadim Kehl, and Adrien Gaidon. ROI-10D: Monocular Lifting of 2D Detection
to 6D Pose and Metric Shape. In Proceedings of the Conference on Computer Vision and Pattern
Recognition (CVPR), 2019.
Kanti V. Mardia and Peter E. Jupp. Directional Statistics. Wiley, 1999.
David A. Nix and Andreas S. Weigend. Estimating the Mean and Variance of the Target Probability
Distribution. In Proceedings of the International Conference on Neural Networks (ICNN), 1994.
Jean Marc Odobez. Idiap Head Pose Database. http://www.idiap.ch/dataset/
headpose, 2003.
Sergey Prokudin, Peter Gehler, and Sebastian Nowozin. Deep Directional Statistics: Pose Estima-
tion with Uncertainty Quantification. In Proceedings of the European Conference on Computer
Vision (ECCV), 2018.
Torsten Sattler, Qunjie Zhou, Marc Pollefeys, and Laura Leal-Taixe. Understanding the Limitations
of CNN-Based Absolute Camera Pose Regression. In Proceedings of the Conference on Computer
Vision and Pattern Recognition (CVPR), 2019.
Hannes Sommer, Igor Gilitschenski, Michael Bloesch, Stephan Weiss, Roland Siegwart, and Juan
Nieto. Why and How to Avoid the Flipped Quaternion Multiplication. Aerospace, 5(3):72, 2018.
Rangaprasad A. Srivatsan, Gillian T. Rosen, D. Feroze Naina Mohamed, and Howie Choset. Esti-
mating SE(3) Elements Using a Dual Quaternion Based Linear Kalman Filter. In Proceedings of
Robotics Science and Systems (RSS), 2016.
Martin Sundermeyer, Zoltan-Csaba Marton, Maximilian Durner, Manuel Brucker, and Rudolph
Triebel. Implicit 3D Orientation Learning for 6D Object Detection from RGB Images. In Pro-
ceedings of the European Conference on Computer Vision (ECCV), 2018.
Bugra Tekin, Sudipta N Sinha, and Pascal Fua. Real-Time Seamless Single Shot 6D Object Pose Pre-
diction. In Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR),
2018.
Chen Wang, Danfei Xu, Yuke Zhu, Roberto Martin-Martin, Cewu Lu, Li Fei-Fei, and Silvio
Savarese. DenseFusion: 6D Object Pose Estimation by Iterative Dense Fusion. In Proceedings of
the Conference on Computer Vision and Pattern Recognition (CVPR), 2019a.
He Wang, Srinath Sridhar, Jingwei Huang, Julien Valentin, Shuran Song, and Leonidas J Guibas.
Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size Estimation. In
Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR), 2019b.
Sen Wang, Ronald Clark, Hongkai Wen, and Niki Trigoni. DeepVO: Towards end-to-end visual
odometry with deep Recurrent Convolutional Neural Networks. In Proceedings of the Interna-
tional Conference on Robotics and Automation (ICRA), 2017.
11
Published as a conference paper at ICLR 2020
Sen Wang, Ronald Clark, Hongkai Wen, and Niki Trigoni. End-to-end, sequence-to-sequence prob-
abilistic visual odometry through deep neural networks. The International Journal of Robotics
Research, 2018.
Tsun-Yi Yang, Yi-Ting Chen, Yen-Yu Lin, and Yung-Yu Chuang. FSA-Net: Learning Fine-Grained
Structure Aggregation for Head Pose Estimation From a Single Image. In Proceedings of the
Conference on Computer Vision and Pattern Recognition (CVPR), 2019.
12