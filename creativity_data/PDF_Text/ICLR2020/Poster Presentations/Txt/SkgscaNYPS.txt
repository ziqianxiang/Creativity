Published as a conference paper at ICLR 2020
The asymptotic spectrum of the Hessian of
DNN throughout training
Arthur Jacot, Franck Gabriel & Clement Hongler
Chair of Statistical Field Theory
Ecole Polytechnique Federale de Lausanne
{arthur.jacot,franck.grabriel,clement.hongler}@epfl.ch
Abstract
The dynamics of DNNs during gradient descent is described by the so-called
Neural Tangent Kernel (NTK). In this article, we show that the NTK allows
one to gain precise insight into the Hessian of the cost of DNNs. When
the NTK is fixed during training, we obtain a full characterization of the
asymptotics of the spectrum of the Hessian, at initialization and during
training. In the so-called mean-field limit, where the NTK is not fixed during
training, we describe the first two moments of the Hessian at initialization.
1	Introduction
The advent of deep learning has sparked a lot of interest in the loss surface of deep neural
networks (DNN), and in particular its Hessian. However to our knowledge, there is still no
theoretical description of the spectrum of the Hessian. Nevertheless a number of phenomena
have been observed numerically.
The loss surface of neural networks has been compared to the energy landscape of different
physical models (Choromanska et al., 2015; Geiger et al., 2018; Mei et al., 2018). It appears
that the loss surface of DNNs may change significantly depending on the width of the network
(the number of neurons in the hidden layer), motivating the distinction between the under-
and over-parametrized regimes (Baity-Jesi et al., 2018; Geiger et al., 2018; 2019).
The non-convexity of the loss function implies the existence of a very large number of saddle
points, which could slow down training. In particular, in (Pascanu et al., 2014; Dauphin
et al., 2014), a relation between the rank of saddle points (the number of negative eigenvalues
of the Hessian) and their loss has been observed.
For overparametrized DNNs, a possibly more important phenomenon is the large number of
flat directions (Baity-Jesi et al., 2018). The existence of these flat minima is conjectured
to be related to the generalization of DNNs and may depend on the training procedure
(Hochreiter & Schmidhuber, 1997; Chaudhari et al., 2016; Wu et al., 2017).
In (Jacot et al., 2018) it has been shown, using a functional approach, that in the infinite-
width limit, DNNs behave like kernel methods with respect to the so-called Neural Tangent
Kernel, which is determined by the architecture of the network. This leads to convergence
guarantees for DNNs (Jacot et al., 2018; Du et al., 2019; Allen-Zhu et al., 2018; Huang &
Yau, 2019) and strengthens the connections between neural networks and kernel methods
(Neal, 1996; Cho & Saul, 2009; Lee et al., 2018).
Our approach also allows one to probe the so-called mean-field/active limit (studied in
(Rotskoff & Vanden-Eijnden, 2018; Chizat & Bach, 2018a; Mei et al., 2018) for shallow
networks), where the NTK varies during training.
This raises the question: can we use these new results to gain insight into the behavior of
the Hessian of the loss of DNNs, at least in the small region explored by the parameters
during training?
1
Published as a conference paper at ICLR 2020
1.1	Contributions
Following ideas introduced in (Jacot et al., 2018), we consider the training of L + 1-layered
DNNs in a functional setting. For a functional cost C , the Hessian of the loss RP 3 θ 7→
C F(L) (θ) is the sum of two P × P matrices I and S. We show the following results for
large P and for a fixed number of datapoints N :
•	The first matrix I is positive semi-definite and its eigenvalues are given by the
(weighted) kernel PCA of the dataset with respect to the NTK. The dominating
eigenvalues are the principal components of the data followed by a high number
of small eigenvalues. The “flat directions” are spanned by the small eigenvalues
and the null-space (of dimension at least P - N when there is a single output).
Because the NTK is asymptotically constant (Jacot et al., 2018), these results apply
at initialization, during training and at convergence.
•	The second matrix S can be viewed as residual contribution to H , since it vanishes
as the network converges to a global minimum. We compute the limit of the first
moment Tr (S) and characterize its evolution during training, of the second moment
Tr S2 * which stays constant during training, and show that the higher moments
vanish.
•	Regarding the sum H = I + S, we show that the matrices I and S are asymptotically
orthogonal to each other at initialization and during training. In particular, the
moments of the matrices I and S add up: tr(Hk) ≈ tr(Ik) + tr(Sk).
These results give, for any depth and a fairly general non-linearity, a complete description of
the spectrum of the Hessian in terms of the NTK at initialization and throughout training.
Our theoretical results are consistent with a number of observations about the Hessian
(Hochreiter & Schmidhuber, 1997; Pascanu et al., 2014; Dauphin et al., 2014; Chaudhari
et al., 2016; Wu et al., 2017; Pennington & Bahri, 2017; Geiger et al., 2018), and sheds a
new light on them.
1.2	Related works
The Hessian of the loss has been studied through the decomposition I + S in a number of
previous works (Sagun et al., 2017; Pennington & Bahri, 2017; Geiger et al., 2018).
For least-squares and cross-entropy costs, the first matrix I is equal to the Fisher matrix
(Wagenaar, 1998; Pascanu & Bengio, 2013), whose moments have been described for shallow
networks in (Pennington & Worah, 2018). For deep networks, the first two moments and the
operator norm of the Fisher matrix for a least squares loss were computed at initialization
in (Karakida et al., 2018) conditionally on a certain independence assumption; our method
does not require such assumptions. Note that their approach implicitly uses the NTK.
The second matrix S has been studied in (Pennington & Bahri, 2017; Geiger et al., 2018)
for shallow networks, conditionally on a number of assumptions. Note that in the setting of
(Pennington & Bahri, 2017), the matrices I and S are assumed to be freely independent,
which allows them to study the spectrum of the Hessian; in our setting, we show that the
two matrices I and S are asymptotically orthogonal to each other.
2 Setup
We consider deep fully connected artificial neural networks (DNNs) using the setup and
NTK parametrization of (Jacot et al., 2018), taking an arbitrary nonlinearity σ ∈ Cb4(R)
(i.e. σ : R → R that is 4 times continuously differentiable function with all four derivatives
bounded). The layers are numbered from 0 (input) to L (output), each containing n` neurons
for ' = 0,...,L. The P = £'品 (n` + 1) n'+ι parameters consist of the weight matrices
W(') ∈ Rn'+1×n' and bias vectors b(') ∈ Rn'+1 for ' = 0,...,L — 1. We aggregate the
parameters into the vector θ ∈ RP .
2
Published as a conference paper at ICLR 2020
The activations and pre-activations of the layers are defined recursively for an input x ∈ Rn0 ,
setting α(0) (x; θ) = x :
α('+1)(x; θ)=
W(')α(')(x; θ)+ βb⑶,
α('+1)(x; θ) = σ(α('+1)(x; θ)).
The parameter β is added to tune the influence of the bias on training1 . All parameters are
initialized as iid N(0, 1) Gaussians.
We will in particular study the network function, which maps inputs x to the activation of
the output layer (before the last non-linearity):
fθ (χ) = α(L)(x; θ).
In this paper, we will study the limit of various ob jects as n1 , . . . , nL → ∞ sequentially, i.e.
we first take n1 → ∞, then n2 → ∞, etc. This greatly simplifies the proofs, but they could
in principle be extended to the simultaneous limit, i.e. when n1 = ... = nL-1 → ∞. All
our numerical experiments are done with ‘rectangular’ networks (with n1 = ... = nL-1) and
match closely the predictions for the sequential limit.
In the limit we study in this paper, the NTK is asymptotically fixed, as in (Jacot et al., 2018;
Allen-Zhu et al., 2018; Du et al., 2019; Arora et al., 2019; Huang & Yau, 2019). By rescaling
the outputs of DNNs as the width increases, one can reach another limit where the NTK
is not fixed (Chizat & Bach, 2018a;b; Rotskoff & Vanden-Eijnden, 2018; Mei et al., 2019).
Some of our results can be extended to this setting, but only at initialization (see Section
3.3). The behavior during training becomes however much more complex.
2.1	Functional viewpoint
The network function lives in a function space fθ ∈ F := [Rn0 → RnL] and we call the
function F (L) : RP → F that maps the parameters θ to the network function fθ the
realization function. We study the differential behavior of F(L) :
•	The derivative DF(L) ∈ RP 0 F is a function-valued vector of dimension P. The
p-th entry DpF(L) = ∂θp fθ ∈ F represents how modifying the parameter θp modifies
the function fθ in the space F.
•	The Hessian HF (L) ∈ RP 0 RP 0 F is a function-valued P × P matrix.
The network is trained with respect to the cost functional:
1N
C(f ) = NN Eci (f(xi)),
i=1
for strictly convex ci , summing over a finite dataset x1 , . . . , xN ∈ Rn0 of size N . The
parameters are then trained with gradient descent on the composition C ◦ F(L), which defines
the usual loss surface of neural networks.
In this setting, we define the finite realization function Y (L) : RP → RNnL mapping
parameters θ to be the restriction of the network function fθ to the training set yik = fθ,k (xi).
The Jacobian DY (L) is hence an N nL × P matrix and its Hessian HY (L) is a P × P × N nL
tensor. Defining the restricted cost C(y) = N Pi ci(yi), We have C ◦ F(L) = C ◦ Y(L).
For our analysis, we require that the gradient norm kDC k does not explode during training.
The folloWing condition is sufficient:
Definition 1. A loss C : RNnL → R has bounded gradients over sublevel sets (BGOSS) if
the norm of the gradient is bounded over all sets Ua = Y ∈ RN nL : C (Y ) ≤ a .
For example, the Mean Square Error (MSE) C(Y) = 2n ∣∣Y * 一 Y ∣∣2 for the labels Y * ∈ RNnL
has BGOSS because ∣∣VC (Y )∣2 = N ∣∣ Y * — Y ∣∣2 = 2C(Y). For the binary and softmax
cross-entropy the gradient is uniformly bounded, see Proposition 2 in Appendix A.
1In our experiments, we take β = 0.1.
3
Published as a conference paper at ICLR 2020
2.2	Neural Tangent Kernel
The behavior during training of the network function fθ in the function space F is described
by a (multi-dimensional) kernel, the Neural Tangent Kernel (NTK)
P
Θ(kL,k)0(x,x0) = X ∂θpfθ,k(x)∂θpfθ,k0(x0).
p=1
During training, the function fθ follows the so-called kernel gradient descent with respect to
the NTK, which is defined as
1N
dtfθ(t)(X) = -Vθ(L) C∣fθ(t) (X) = - N W)(X,Xi)vcifθ(t)(Xi)).
i=1
In the infinite-width limit (letting n1 → ∞, . . . , nL-1 → ∞ sequentially) and for losses with
BGOSS, the NTK converges to a deterministic limit Θ(L) → Θ(∞L) 0 IdnL, which is constant
during training, uniformly on finite time intervals [0, T] (Jacot et al., 2018). For the MSE
loss, the uniform convergence of the NTK was proven for T = ∞ in (Arora et al., 2019).
The limiting NTK Θ(∞L) : Rn0 × Rn0 → R is constructed as follows:
1.	For f, g : R → R and a kernel K : Rn0 × Rn0 → R, define the kernel LfK,g :
Rn0 × Rn0 → R by
LfK,g(X0,X1) = E(a0,a1) [f(a0)g(a1)] ,
for (a0, a1 ) a centered Gaussian vector with covariance matrix (K(Xi, Xj ))i,j=0,1 . For
f = g , we denote by LfK the kernel LfK,f .
2.	We define the kernels Σ∞∞) for each layer of the network, starting with Σ∞∞)(Xo, xi)=
1∕no(XTXi) + β2 and then recursively by Σ∞∞+1) = Lσ⑷ + β2, for ' = 1,...,L — 1,
Σ∞
where σ is the network non-linearity.
3.	The limiting NTK Θ(∞L) is defined in terms of the kernels Σ∞∞) and the kernels
Σ(')_ L,σ
ς∞ = lv,('-1).
Σ∞
L
Θ(L) = X ∑(')∑('+1)	∑(L)
∞	∞	∞	... ∞ .
'=1
The NTK leads to convergence guarantees for DNNs in the infinite-width limit, and connect
their generalization to that of kernel methods (Jacot et al., 2018; Arora et al., 2019).
2.3	Gram Matrices
For a finite dataset xi,...,xn ∈ Rn0 and a fixed depth L ≥ 1, we denote by Θ ∈ RNnL × NnL
the Gram matrix of X1 , . . . , XN with respect to the limiting NTK, defined by
Θik,jm = Θ∞L) (Xi,Xj) δkm.
It is block diagonal because different outputs k 6= m are asymptotically uncorrelated.
Similarly, for any (scalar) kernel K(L) (such as the limiting kernels Σ(∞L), Λ(∞L), Υ(∞L), Φ(∞L), Ξ(∞L)
introduced later), we denote the Gram matrix of the datapoints by K.
3 Main Theorems
3.1	Hessian as I+ S
Using the above setup, the Hessian H of the loss C ◦ F (L) is the sum of two terms, with the
entry Hp,p0 given by
Hp,p0 = HC∣fθ (∂θp F,∂θp0 F) + DC∣fθ MSpOF).
4
Published as a conference paper at ICLR 2020
For a finite dataset, the Hessian matrix H C ◦ Y (L) is equal to the sum of two matrices
I = (DY(L))T HCDY(L) and S = VC ∙ HY(L)
where DY (L) is a NnL × P matrix, HC is a NnL × NnL matrix and HY (L) is a P × P × NnL
tensor to which We apply a scalar product (denoted by ∙) in its last dimension with the NnL
vector VC to obtain a P × P matrix.
Our main contribution is the following theorem, which describes the limiting moments
Tr Hk in terms of the moments of I and S :
Theorem 1. For any loss C with BGOSS and σ ∈ Cb4(R), in the sequential limit n1 →
∞, . . . , nL-1 → ∞, we have for all k ≥ 1
Tr (H(t)k ≈ Tr (I(t)k +Tr (S(t)k .
The limits of Tr I (t)k and Tr S (t)k can be expressed in terms of the NTK Θ(∞L), the
kernels Υ(∞L), Ξ(∞L) and the non-symmetric kernels Φ(∞L), Λ(∞L) defined in Appendix C:
•	The moments Tr I (t)k converge to the following limits (with the convention that
ik+1 = i1):
Nk
Tr (l(t)k) → Tr ((HC(Y ⑴冶)k) = N X Y Cim(fθ(t)"))Θ∞L)(Xim,%+)
i1,...,ik=1 m=1
•	The first moment Tr (S (t)) converges to the limit:
Tr (S (t)) = (G(t))T VC(Y (t)).
At initialization (G(0), Y (0)) form a Gaussian pair of NnL -vectors, independent
for differing output indices k = 1, ..., nL and with covariance E [Gik (0)Gi0k0 (0)] =
δkk0 Ξ(∞L)
(xi, xi0) and E [Gik (0)Yi0k0 (0)] = δkk0 Φ(∞L) (xi, xi0) for the limiting kernel
Ξ(∞L) (x, y) and non-symmetric kernel Φ(∞L) (x, y). During training, both vectors follow
the differential equations
_ , . ~ . ...
∂tG(t) = -Λ VC(Y (t))
∂tY (t) = -Θ VC (Y (t)).
•	The second moment Tr S (t)2 converges to the following limit defined in terms of
the Gram matrix Υ:
Tr (S2) → (VC(Y(t)))T YVC(Y(t))
•	The higher moments Tr S (t)k for k ≥ 3 vanish.
Proof. The moments of I and S can be studied separately because the moments of their sum
is asymptotically equal to the sum of their moments by Proposition 5 below. The limiting
moments of I and S are respectively described by Propositions 1 and 4 below.	□
In the case of a MSE loss C (Y)=击 ∣∣ Y — Y *『,the first and second derivatives take simple
forms VC(Y) = N (Y — Y*) and HC(Y) = NIdNnL and the differential equations can be
solved to obtain more explicit formulae:
5
Published as a conference paper at ICLR 2020
IO2
ε-EU
10^1
IO1
Figure 1: Comparison of the theoretical prediction of Corollary 1 for the expectation of the
first 4 moments (colored lines) to the empirical average over 250 trials (black crosses) for a
rectangular network with two hidden layers of finite widths n1 = n2 = 5000 (L = 3) with the
smooth ReLU (left) and the normalized smooth ReLU (right), for the MSE loss on scaled
down 14x14 MNIST with N = 256. Only the first two moments are affected by S at the
beginning of training.
£EE
IO2
6 x IO1
4× 101
3 x 101
lθ-ɪ
IO1
Corollary 1. For the MSE loss C and σ ∈ Cb4(R), in the limit n1 , ..., nL-1 → ∞, we have
uniformly over [0, T]
Tr (H(t)k) → NTr (θk) + Tr (S(t)k)
where
Tr (S(t)) →-((Y * - Y(0))T (IdNnL + e-tθ) Θ-1Λ T e-tθ (Y * - Y (0))
+ N g(0)t e-tθ (Y * — Y (0))
Tr (S(t)2) →上(Y* - Y(0))te-tθYe-tθ(Y* - Y(0))
N2
Tr (S(t)k) →0 when k > 2.
In expectation we have:
E [Tr (S(t))] → - NTr ((IdNnL + e-tθ) Θ-1ΛTe-tθ (∑ + Y*Y*T)) + NTr (e-tθΦT)
E [Tr (S(t)2)] → WTr (e-tθYe-tθ (∑ + Y*Y*T)).
Proof. The moments of I are constant because HC = NIdNnL is constant. For the moments
of S, we first solve the differential equation for Y (t):
Y (t) = Y * - e-tθ (Y * - Y (0)).
Noting Y(t) — Y(0) = -Θ Rt PC(S)ds, We have
.. .. ~
G(t) = G(0) - A
Zt
0
PC(s)ds
. . ~ ~ -1 . .. ..
G(0) + AΘT(Y(t) - Y(0))
G(0) + AΘT (IdNnL + e* (Y* - Y(0))
The expectation of the first moment of S then folloWs.
□
6
Published as a conference paper at ICLR 2020
3.2	Mutual Orthogonality of I and S
A first key ingredient to prove Theorem 1 is the asymptotic mutual orthogonality of the
matrices I and S
Proposition (Proposition 5 in Appendix D). For any loss C with BGOSS and σ ∈ Cb4 (R),
we have uniformly over [0, T]
lim …lim ∣∣IS∣∣f = 0.
nL-1 →∞	n1→∞
As a consequence lim”-1→∞ …limnι→∞ Tr ([I + S]k) — [Tr (Ik) + Tr (Sk)] = 0.
Remark 1. If two matrices A and B are mutualy orthogonal (i.e. AB = 0) the range of A is
contained in the nullspace of B and vice versa. The non-zero eigenvalues of the sum A + B
are therefore given by the union of the non-zero eigenvalues of A and B . Furthermore the
moments of A and B add up: Tr ([A + B]k) = Tr (Ak) + Tr (Bk). Proposition 5 shows that
this is what happens asymptotically for I and S.
Note that both matrices I and S have large nullspaces: indeed assuming a constant width
w = n1 = ... = nL-1, we have Rank (I) ≤ NnL and Rank(S) ≤ 2(L — 1)wNnL (see
Appendix C), while the number of parameters P scales as w2 (when L > 2).
Figure 2 illustrates the mutual orthogonality of I and S . All numerical experiments are done
for rectangular networks (when the width of the hidden layers are equal) and agree well with
our predictions obtained in the sequential limit.
3.3	Mean-field Limit
For a rectangular network with width w, if the output of the network is divided by √w and
the learning rate is multiplied by w (to keep similar dynamics at initialization), the training
dynamics changes and the NTK varies during training when w goes to infinity. The new
parametrization of the output changes the scaling of the two matrices:
H]C (⅛Y(L)
1 (DY(L))T HCDY(L) + √1= VC ∙HY(L) = ɪI + √= S.
The scaling of the learning rate essentially multiplies the whole Hessian by w. In this setting,
the matrix I is left unchanged while the matrix S is multiplied by ʌ/w (the k-th moment of
S is hence multiplied by wk/2). In particular, the two moments of the Hessian are dominated
by the moments of S, and the higher moments of S (and the operator norm of S) should
not vanish. This suggests that the active regime may be characterised by the fact that
∣S∣F ∣I∣F . Under the conjecture that Theorem 1 holds for the infinite-width limit of
rectangular networks, the asymptotic of the two first moments of H is given by:
ι∕√wTr (H) → N(0, VCTΞVC)
1/wTr (H2) → VCT YVC,
where for the MSE loss we have VC = —Y*.
3.4	The matrix S
The matrix S = VC ∙ HY(L) is best understood as a perturbation to I, which vanishes as
the network converges because VC → 0. To calculate its moments, we note that
Tr
where the vector G = PkP=1 ∂θ22 Y ∈ RNnL is the evaluation of the function gθ (x)
PkP=1 ∂θ22 fθ (x) on the training set.
7
Published as a conference paper at ICLR 2020
For the second moment we have
Tr ((vc ∙ HY(L)) 2) = VCT	XX ∂2pθp0Y (∂2pθp, Y)T VC = VCTYVC
p,p0=1
for Y the Gram matrix of the kernel Y(L)(x,y) = PPp0=ι 32必0fθ(x)(9卷必0于8(y)).
The following proposition desribes the limit of the function gθ and the kernel Υ(L) and the
vanishing of the higher moments:
Proposition (Proposition 4 in Appendix C). For any loss C with BGOSS and σ ∈ Cb4 (R),
the first two moments of S take the form
Tr (S(t)) = G(t)T VC (t)
Tr (S(t)2) = VC(t)TY(t)VC(t)
-	At initialization, gθ and fθ converge to a (centered) Gaussian pair with covariances
E[gθ,k(x)gθ,k0 (x0)] = δkk0 Ξ(∞L) (x, x0)
E[gθ,k (x)fθ,k0 (x0)] = δkk0Φ(∞L)(x, x0)
E[fθ,k(x)fθ,k0 (x0)] = δkk0 Σ(∞L) (x, x0)
and during training gθ evolves according to
N
∂tgθ,k (X)= X Λ∞L(x,Xi)∂ik C (Y (t))∙
i=1
-	Uniformly over any interval [0, T], the kernel Y(L) has a deterministic and fixed limit
limnL-ι→∞ …limnι→∞ Y(L)(x, X) = 6卜卜；Y∞L(x, x0) with limiting kernel:
L-1
Y	∞)(x, x0) = X (θ∞)(x, X0)2∑∞(x, X0) + 2Θ∞)(x, x0)Σ∞(x, X0)) Σ∞+1)(x, x0)…Σ∞LT)(x, x0).
2=1
-The higher moment k > 2 vanish: limnL-ι→∞ ∙∙∙ limnι→∞ Tr (Sk) = 0.
This result has a number of consequences for infinitely wide networks:
1.	At initialization, the matrix S has a finite Frobenius norm ∣∣S∣∣F = Tr (S2)=
VCTYVC, because Y converges to a fixed limit. As the network converges, the
derivative of the cost goes to zero VC (t) → 0 and so does the Frobenius norm of S.
2.	In contrast the operator norm of S vanishes already at initialization (because for all
even k, we have ∣S∣op ≤ k Tr (Sk ) → 0). At initialization, the vanishing of S in
operator norm but not in Frobenius norm can be explained by the matrix S having
a growing number of eigenvalues of shrinking intensity as the width grows.
3.	When it comes to the first moment of S, Proposition 4 shows that the spectrum of
S is in general not symmetric. For the MSE loss the expectation of the first moment
at initialization is
E [Tr(S)] = E [(Y - Y*)TG] = E [YTG] - (Y*)T E [G] = Tr (Φ) - 0
which may be positive or negative depending on the choice of nonlinearity: with a
smooth ReLU, it is positive, while for the arc-tangent or the normalized smooth
ReLU, it can be negative (see Figure 1).
This is in contrast to the result obtained in (Pennington & Bahri, 2017; Geiger et al.,
2018) for the shallow ReLU networks, taking the second derivative of the ReLU to
be zero. Under this assumption the spectrum of S is symmetric: if the eigenvalues
are ordered from lowest to highest, λi = -λP -i and Tr(S) = 0.
8
Published as a conference paper at ICLR 2020
Iargestelg. aft
> Iargestelg. of S
-Q-IQQ -0.075 -0.050 -0.025 0.00C
VtsV
Figure 2: Illustration of the mutual
orthogonality of I and S. For the 20
first eigenvectors of I (blue) and S
(orange), we plot the Rayleigh quo-
tients vTIv and vTSv (with L = 3,
n1 = n2 = 1000 and the normal-
ized ReLU on 14x14 MNIST with
N = 256). We see that the direc-
tions where I is large are directions
where S is small and vice versa.
Figure 3: Plot of the loss surface around a global mini-
mum along the first (along the y coordinate) and fourth
(x coordinate) eigenvectors of I . The network has L = 4,
width n1 = n2 = n3 = 1000 for the smooth ReLU (left)
and the normalized smooth ReLU (right). The data is
uniform on the unit disk. Normalizing the non-linearity
greatly reduces the narrow valley structure of the loss
thus speeding up training.
These observations suggest that S has little influence on the shape of the surface, especially
towards the end of training, the matrix I however has an interesting structure.
3.5 The matrix I
At a global minimizer θ*, the spectrum of I describes how the loss behaves around θ*. Along
the eigenvectors of the biggest eigenvalues of I , the loss increases rapidely, while small
eigenvalues correspond to flat directions. Numerically, it has been observed that the matrix
I features a few dominating eigenvalues and a bulk of small eigenvalues (Sagun et al., 2016;
2017; Gur-Ari et al., 2018; Papyan, 2019). This leads to a narrow valley structure of the loss
around a minimum: the biggest eigenvalues are the ‘cliffs’ of the valley, i.e. the directions
along which the loss grows fastest, while the small eigenvalues form the ‘flat directions’or
the bottom of the valley.
Note that the rank of I is bounded by NnL and in the overparametrized regime, when
NnL < P , the matrix I will have a large nullspace, these are directions along which the
value of the function on the training set does not change. Note that in the overparametrized
regime, global minima are not isolated: they lie in a manifold of dimension at least P - NnL
and the nullspace of I is tangent to this solution manifold.
The matrix I is closely related to the NTK Gram matrix:
Θ = DY(L) (DY(L))T and I = (DY(L))THCDY(L).
As a result, the limiting spectrum of the matrix I can be directly obtained from the NTK2 *
Proposition 1. For any loss C with BGOSS and σ ∈ Cb4 (R), uniformly over any interval
[0, T ], the moments Tr Ik converge to the following limit (with the convention that ik+1 =
i1):
1Nk
nL-i11→∞∙∙nl→∞ Tr (Ik) = Tr ( (HC (Yt )θ) k) = N X Y Cim (fθ(t)(Xim ))θ∞1(x,m ,x,m+l)
i1,...,ik =1 m=1
2This result was already obtained in (Karakida et al., 2018), but without identifying the NTK
explicitely and only at initialization.
9
Published as a conference paper at ICLR 2020
Proof. It follows from Tr (Ik) = Tr (((DY(L))THCDY(L)) = Tr ((HCΘ)k) and the
asymptotic of the NTK (JacOt et al., 2018).	□
3.5.1	Mean-Square Error
1
When the loss is the MSE, HC is equal to NIdNnL ∙ AS a result, Θ and I have the same
non-zero eigenvalues up to a scaling of 1/N . Because the NTK is assymptotically fixed, the
spectrum of I is also fixed in the limit.
The eigenvectors of the NTK Gram matrix are the kernel principal components of the
data. The biggest principal components are the directions in function space which are most
favorised by the NTK. This gives a functional interpretation of the narrow valley structure in
DNNs: the cliffs of the valley are the biggest principal components, while the flat directions
are the smallest components.
Remark 2. As the depth L of the network increases, one can observe two regimes (Poole
et al., 2016; Jacot et al., 2019): Order/Freeze where the NTK converges to a constant and
Chaos where the NTK converges to a Kronecker delta. In the Order/Freeze the NnL × NnL
Gram matrix approaches a block diagonal matrix with nL constant blocks, and as a result
nL eigenvalues of I dominate the other ones, corresponding to constant directions along each
outputs (this is in line with the observations of (Papyan, 2019)). This leads to a narrow
valley for the loss and slows down training. In contrast, in the Chaos regime, the NTK Gram
matrix approaches a scaled identity matrix, and the spectrum of I should hence concentrate
around a positive value, hence speeding up training. Figure 3 illustrates this phenomenon:
with the smooth ReLU we observe a narrow valley, while with the normalized smooth ReLU
(which lies in the Chaos according to (Jacot et al., 2019)) the narrowness of the loss is
reduced. A similar phenomenon may explain why normalization helps smoothing the loss
surface and speed up training (Santurkar et al., 2018; Ghorbani et al., 2019).
3.5.2	Cross-Entropy Loss
For a binary cross-entropy loss with labels Y * ∈ { — 1, +1}N
N
C(Y) = N X log(1 + e-Yi*Yi
i=1
HC is a diagonal matrix whose entries depend on Y (but not on Y * ):
HiiC(Y)
1	1
N 1 + e-M + eγ
The eigenvectors of I then correspond to the weighted kernel principal component of the
data. The positive weights .£-1i+£了稔 approach 1∕3 as Y goes to 0, i.e. when it is close to
the decision boundary from one class to the other, and as Yi → ±∞ the weight go to zero.
The weights evolve in time through Yi , the spectrum of I is therefore not asymptotically
fixed as in the MSE case, but the functional interpretation of the spectrum in terms of the
kernel principal components remains.
4 Conclusion
We have given an explicit formula for the limiting moments of the Hessian of DNNs throughout
training. We have used the common decomposition of the Hessian in two terms I and S and
have shown that the two terms are asymptotically mutually orthogonal, such that they can
be studied separately.
The matrix S vanishes in Frobenius norm as the network converges and has vanishing operator
norm throughout training. The matrix I is arguably the most important as it describes the
narrow valley structure of the loss around a global minimum. The eigendecomposition of I
is related to the (weighted) kernel principal components of the data w.r.t. the NTK.
10
Published as a conference paper at ICLR 2020
Acknowledgements
Clement Hongler acknowledges support from the ERC SG CONSTAMIS grant, the NCCR
SwissMAP grant, the NSF DMS-1106588 grant, the Minerva Foundation, the Blavatnik
Family Foundation, and the Latsis foundation.
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A Convergence Theory for Deep Learning
via Over-Parameterization. CoRR, abs/1811.03962, 2018. URL http://arxiv.org/abs/
1811.03962.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang.
On exact computation with an infinitely wide neural net. arXiv preprint arXiv:1904.11955,
2019.
Marco Baity-Jesi, Levent Sagun, Mario Geiger, Stefano Spigler, Gerard Ben Arous, Chiara
Cammarota, Yann LeCun, Matthieu Wyart, and Giulio Biroli. Comparing Dynamics:
Deep Neural Networks versus Glassy Systems. In Jennifer Dy and Andreas Krause (eds.),
Proceedings of the 35th International Conference on Machine Learning, volume 80, pp.
314—323. PMLR, 10—15 Jul 2018. URL http://Proceedings.mlr.press∕v80∕baity-
jesi18a.html.
Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Chris-
tian Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-sgd: Biasing
gradient descent into wide valleys. arXiv preprint arXiv:1611.01838, 2016.
Lenaic Chizat and Francis Bach. On the Global Convergence of Gradient Descent
for Over-parameterized Models using Optimal Transport. In Advances in Neural
Information Processing Systems 31, pp. 3040-3050. Curran Associates, Inc., 2018a. URL
http://papers.nips.cc/paper/7567-on-the-global-convergence-of-gradient-
descent-for-over-parameterized-models-using-optimal-transport.pdf.
Lenaic Chizat and Francis Bach. A note on lazy training in supervised differentiable
programming. arXiv preprint arXiv:1812.07956, 2018b.
Youngmin Cho and Lawrence K. Saul. Kernel Methods for Deep Learning. In Advances in
Neural Information Processing Systems 22, pp. 342-350. Curran Associates, Inc., 2009. URL
http://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning.pdf.
Anna Choromanska, Mikael Henaff, Michael Mathieu, Gerard Ben Arous, and Yann LeCun.
The Loss Surfaces of Multilayer Networks. Journal of Machine Learning Research, 38:
192-204, nov 2015. URL https://arxiv.org/pdf/1412.0233.pdf.
Yann N. Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and
Yoshua Bengio. Identifying and Attacking the Saddle Point Problem in High-dimensional
Non-convex Optimization. In Proceedings of the 27th International Conference on Neural
Information Processing Systems - Volume 2, NIPS’14, pp. 2933-2941, Cambridge, MA,
USA, 2014. MIT Press.
Simon S. Du, Xiyu Zhai, BarnabaS Poczos, and Aarti Singh. Gradient Descent Provably
Optimizes Over-parameterized Neural Networks. 2019.
Mario Geiger, Stefano Spigler, StePhane d,Ascoli, Levent Sagun, Marco Baity-Jesi, Giulio
Biroli, and Matthieu Wyart. The jamming transition as a paradigm to understand the
loss landscape of deep neural networks. arXiv preprint arXiv:1809.09349, 2018.
Mario Geiger, Arthur Jacot, Stefano Spigler, Franck Gabriel, Levent Sagun, StePhane
d,Ascoli, Giulio Biroli, Clement Hongler, and Matthieu Wyart. Scaling description of
generalization with number of parameters in deep learning . abs/1901.01608, 2019. URL
http://arxiv.org/abs/1901.01608.
11
Published as a conference paper at ICLR 2020
Behrooz Ghorbani, Shankar Krishnan, and Ying Xiao. An investigation into neural net opti-
mization via hessian eigenvalue density. In Kamalika Chaudhuri and Ruslan Salakhutdinov
(eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97
of Proceedings of Machine Learning Research, pp. 2232-2241, Long Beach, California, USA,
09-15 Jun 2019. PMLR. URL http://Proceedings.mlr.press∕v97∕ghorbani19b.html.
Guy Gur-Ari, Daniel A. Roberts, and Ethan Dyer. Gradient descent happens in a tiny
subspace. CoRR, abs/1812.04754, 2018. URL http://arxiv.org/abs/1812.04754.
Sepp Hochreiter and Jurgen Schmidhuber. Flat minima. Neural Computation, 9(1):1-42,
1997.
Jiaoyang Huang and Horng-Tzer Yau. Dynamics of deep neural networks and neural tangent
hierarchy. arXiv preprint arXiv:1909.08156, 2019.
Arthur Jacot, Franck Gabriel, and CIement Hongler. Neural Tangent Ker-
nel: Convergence and Generalization in Neural Networks. In Advances in
Neural Information Processing Systems 31, pp. 8580-8589. Curran Associates,
Inc., 2018. URL http://papers.nips.cc/paper/8076-neural-tangent-kernel-
convergence-and-generalization-in-neural-networks.pdf.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Freeze and chaos for dnns: an NTK
view of batch normalization, checkerboard and boundary effects. CoRR, abs/1907.05715,
2019. URL http://arxiv.org/abs/1907.05715.
Ryo Karakida, Shotaro Akaho, and Shun-Ichi Amari. Universal Statistics of Fisher In-
formation in Deep Neural Networks: Mean Field Approach. jun 2018. URL http:
//arxiv.org/abs/1806.01316.
Jae Hoon Lee, Yasaman Bahri, Roman Novak, Samuel S. Schoenholz, Jeffrey Pennington,
and Jascha Sohl-Dickstein. Deep Neural Networks as Gaussian Processes. ICLR, 2018.
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape
of two-layer neural networks. Proceedings of the National Academy of Sciences, 115(33):
E7665-E7671, 2018.
Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Mean-field theory of two-layers
neural networks: dimension-free bounds and kernel limit. arXiv preprint arXiv:1902.06015,
2019.
Radford M. Neal. Bayesian Learning for Neural Networks. Springer-Verlag New York, Inc.,
Secaucus, NJ, USA, 1996. ISBN 0387947248.
Vardan Papyan. Measurements of three-level hierarchical structure in the outliers in the
spectrum of deepnet hessians. CoRR, abs/1901.08244, 2019. URL http://arxiv.org/
abs/1901.08244.
Razvan Pascanu and Yoshua Bengio. Revisiting Natural Gradient for Deep Networks. jan
2013. URL http://arxiv.org/abs/1301.3584.
Razvan Pascanu, Yann N Dauphin, Surya Ganguli, and Yoshua Bengio. On the saddle point
problem for non-convex optimization. arXiv preprint, 2014. URL https://arxiv.org/
pdf/1405.4604.pdf.
Jeffrey Pennington and Yasaman Bahri. Geometry of Neural Network Loss Surfaces via
Random Matrix Theory. In Proceedings of the 34th International Conference on Machine
Learning, volume 70, pp. 2798-2806. PMLR, 06-11 Aug 2017. URL http://proceedings.
mlr.press/v70/pennington17a.html.
Jeffrey Pennington and Pratik Worah. The Spectrum of the Fisher Informa-
tion Matrix of a Single-Hidden-Layer Neural Network. In Advances in Neu-
ral Information Processing Systems 31, pp. 5415-5424. Curran Associates, Inc.,
2018. URL http://papers.nips.cc/paper/7786- the- spectrum- of- the- fisher-
information- matrix- of- a- single- hidden- layer- neural- network.pdf.
12
Published as a conference paper at ICLR 2020
Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Gan-
guli. Exponential expressivity in deep neural networks through transient chaos. In
D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (eds.), Ad-
Vances in Neural Information Processing Systems 29, pp. 3360-3368. Curran Associates,
Inc., 2016. URL http://papers.nips.cc/paper/6322- exponential- expressivity-
in-deep-neural-networks-through-transient-chaos.pdf.
Grant Rotskoff and Eric Vanden-Eijnden. Parameters as interacting particles: long
time convergence and asymptotic error scaling of neural networks. In Advances in
Neural Information Processing Systems 31, pp. 7146-7155. Curran Associates, Inc.,
2018. URL http://papers.nips.cc/paper/7945-parameters-as-interacting-
particles- long- time- convergence- and- asymptotic- error- scaling- of- neural-
networks.pdf.
Levent Sagun, Leon Bottou, and Yann LeCun. Singularity of the hessian in deep learning.
CoRR, abs/1611.07476, 2016. URL http://arxiv.org/abs/1611.07476.
Levent Sagun, Utku Evci, V. Ugur Guney, Yann Dauphin, and Leon Bottou. Empirical
Analysis of the Hessian of Over-Parametrized Neural Networks. CoRR, abs/1706.04454,
2017.
Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch
normalization help optimization? In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing
Systems 31, pp. 2483-2493. Curran Associates, Inc., 2018. URL http://papers.nips.
cc/paper/7515-how-does-batch-normalization-help-optimization.pdf.
Daniel Wagenaar. Information geometry of neural networks. 1998. ISSN 0302-9743.
Lei Wu, Zhanxing Zhu, and Weinan E. Towards Understanding Generalization of Deep
Learning: Perspective of Loss Landscapes. CoRR, abs/1706.10239, 2017. URL http:
//arxiv.org/abs/1706.10239.
A Proofs
For the proofs of the theorems and propositions presented in the main text, we reformulate
the setup of (Jacot et al., 2018). For a fixed training set x1 , ..., xN, we consider a (possibly
random) time-varying training direction D(t) ∈ RNnL which describes how each of the
outputs must be modified. In the case of gradient descent on a cost C(Y ), the training
direction is D(t) = VC(Y(t)). The parameters are updated according to the differential
equation
∂tθ(t) = (∂θY(t))T D(t).
Under the condition that R0T kD(t)k2 dt is stochastically bounded as the width of the network
goes to infinity, the NTK Θ(L) converges to its fixed limit uniformly over [0, T].
The reason we consider a general training direction (and not only a gradient of a loss) is
that we can split a network in two at a layer ` and the training of the smaller network will
be according to the training direction D(')(t) given by
D(')(t) = diag (σ (a(')(xi)
T
W(') )…diag * (α(L-I)
√⅛ W(LT))T Dtt
because the derivatives σ are bounded and by Lemma 1 of the Appendix of (Jacot et al.,
2018), this training direction satisfies the constraints even though it is not the gradient of a
loss. As a consequence, as nι → ∞,..., n'-ι → ∞ the NTK of the smaller network Θ⑶ also
converges to its limit uniformly over [0, T]. As We let n` → ∞ the pre-activations α(') and
weights Wj move at a rate of 1∕√n∑. We will use this rate of change to prove that other
types of kernels are constant during training.
13
Published as a conference paper at ICLR 2020
When a network is trained with gradient descent on a loss C with BGOSS, the integral
R0T kD(t)k2 dt is stochastically bounded. Because the loss is decreasing during training, the
outputs Y (t) lie in the sublevel set UC(Y (0)) for all times t. The norm of the gradient is
hence bounded for all times t. Because the distribution of Y (0) converges to a multivariate
Gaussian, b(C(Y (0))) is stochastically bounded as the width grows, where b(a) is a bound
on the norm of the gradient on Ua. We then have the bound R0T kD(t)k2 dt ≤ Tb(C(Y (0)))
which is itself stochastically bounded.
For the binary and softmax cross-entropy losses the gradient is uniformly bounded:
Proposition 2. For the binary cross-entropy loss C and any Y ∈ RN, kVC(Y)|卜 ≤ √=.
For the softmax cross-entropy loss C on C ∈ N classes and any Y ∈ RNc, ∣∣VC(Y)|卜 ≤ √2C,
Proof. The binary cross-entropy loss with labels Y * ∈ {0,1}N is
1 A	eW*	1 A ,	…
C(Y) = -NN X log f+^τ = NN X log(1 + eYi) - YY*
and the gradient at an input i is
∂iC(Y)
1 e居-Yi*(1 + e居)
N 1 + eγ
which is bounded in absolute value by N for both Yi* = 0,1 such that ∣∣VC(Y)∣b ≤ √=.
The softmax cross-entropy loss over c classes with labels Y * ∈ {1, . . . , c}N is defined by
C(Y)=-事 Xlog Pc⅛k=事 X log (X U- Yγ*.
The gradient is at an input i and output class m is
1	eYim
dimC(Y) = N (Pc=IeYik - δY*m)
which is bounded in absolute value by N such that ∣∣VC(Y)∣2 ≤ √2c.
□
B Preliminaries
To study the moments of the matrix S , we first have to show that two tensors vanish as
n1, ..., nL-1 → ∞:
喷九也(x0,x1,x2) = (Vfθ,k0 (XO))T Hfθ,kι (x1)vfθ,k2 (x2)
Γ(kL0,)k1,k2,k3(x0,x1,x2,x4) = (Vfθ,k0(x0))THfθ,k1(x1)Hfθ,k2(x2)Vfθ,k3(x3).
We study these tensors recursively, for this, we need a recursive definition for the first
derivatives ∂θp fθ,k (x) and second derivatives ∂θ2 θ 0 fθ,k (x). The value of these derivatives
depend on the layer ` the parameters θp and θp0 belong to, and on whether they are connection
weights Wm') or biases bf). The derivatives with respect to the parameters of the last layer
are
dW (LT) fθ,k0 (x) = , 1	αm-1) (x)δkk0
mk	nL-1
∂b(L-1)fθ,k0(x) = β2δkk0
k
for parameters θp which belong to the lower layers the derivatives can be defined recursively
by
nL-1
∂θp fθ,k(X) = √n= X ∂θp&mLT)(X)“amLT)(X)) WmkT).
L-1 m=1
14
Published as a conference paper at ICLR 2020
For the second derivatives, we first note that if either of the parameters θp or θp0 are bias of
the last layer, or if they are both connection weights of the last layer, then ∂θ2 θ fθ,k (x) = 0.
Two cases are left: when one parameter is a connection weight of the last layer and the
others belong to the lower layers, and when both belong to the lower layers. Both cases can
be defined recursively in terms of the first and second derivatives of α(L-1):
∂θ2 W(L)fθ,k0 (x)
p mk
∂θ2p θp0 fθ,k0 (x)
√n1=∂θp &mLT)(X) “&mLT)(X))δkko
nL-1
n— X d2pθpo αLτ)3σ (aLτ) (X)) WmLT)
L-1 m=1
+
nL-1
加—X dθp&mLT)(X)泡，⅛Lτ)(X)σ (α(Lτ) (X))
nL-1 m=1
W (L-1)
mk
Using these recursive definitions, the tensors Ω(L+1) and Γ(L+1) are given in terms of
Θ(L),Ω(L) and F(L), in the same manner that the NTK Θ(L+1) is defined recursively in terms
of Θ(L) in (Jacot et al., 2018).
Lemma 1. For any loss C with BGOSS and σ ∈ Cb4(R), we have uniformly over [0, T]
lim
nL-1 →∞
lim Ω
n1→∞
(L)
k0,k1,k2
(X0,X1,X2)
0
Proof. The proof is done by induction. When L = 1 the second derivatives ∂θ2 θ 0 fθ,k (X) = 0
and ωL)k1,k2 (x0,x1,x2) =0.
For the induction step, We write Ωk'"+1)	(xo, X1,X2) recursively as
n-3/2	x	θmmo,mi W, Xi)emm)i,m2(^,⑹“碌(⑹向湍(叼加碌(⑹)WJmOkOWmfl)MWmx
m0,m1,m2
+ n-3/2	X	ΩmO,m1,m2(X0,X1,X2)σ(αmO(X0))σ(aml(X1))σ(αm2 (x2))WmXWmI)kιWm2k2
mO,ml,m2
+ n-3/2 X θmmO,mι (20,21)6(⅛2(20))6(或1(21))6或1 (⑹)WmOk0δk1 k2
mO,ml
+n-3/2 X θmml,m2(戏⑹6湍(XO))σ(⅛m1 (XI))万(碌(⑹州府瓦W)(X.
m1,m2
As nι,..., n`-i → ∞ and for any times t < T, the NTK θ(' converges to its limit while Ω(')
vanishes. The second summand hence vanishes and the others converge to
n-3/2 X弥加叫用如叫应际端) (XO))σ(α(m) (XI))“滞如))WmX WmXWmX
m
+n-3/2 X 涯的血际端)(XO))σ(⅛m)(XI))b(ʒrn)s))WmX δk1k2
m
+n-3/2 X ^,(叫处问端)(XO))σ(⅛m)(XI))wʒrn)s))skoki WmX.
m
At initialization, all terms vanish as n` → ∞ because all summands are independent with zero
mean and finite variance: in the nι → ∞,..., n`-i → ∞ limit, the αm)(X) are independent
for different m, see (Jacot et al., 2018). During training, the weights W(') and preactivations
α(') move at a rate of 1∕√n' (see the proof of convergence of the NTK in (Jacot et al., 2018)).
Since 6 is Lipschitz, we obtain that the motion during training of each of the sums is of
order n—3/2+1/2 = n-1. As a result, uniformly over times t ∈ [0, T], all the sums vanish. □
Similarily, we have
15
Published as a conference paper at ICLR 2020
Lemma 2. For any loss C with BGOSS and σ ∈ Cb4 (R), we have uniformly over [0, T]
lim …lim rkL)k k k (χ0,χι,χ2,χ3) = 0
nL-1→∞	n1→∞ k0,k1,k2,k3
Proof. The proof is done by induction. When L = 1 the hessian HF (1) = 0, such that
Γ(kL0,)k1,k2,k3(x0,x1,x2,x3) =0.
For the induction step, Γ('+1) can be defined recursively:
Γ(kL0,+k11,)k2,k3(x0,x1,x2,x3)
=n-2	E	rmo),mι,m2,m3(x0,x1,x2,x3)σ(αm0)(x0))σ(αmι)(x1 ))6(^^ (⑹加⑺曾? (x3))
m0 ,m1 ,m2 ,m3
W(L) W(L) W(L) W(L)
m0 k0 m1 k1 m2 k2 m3 k3
+nL-	E	θm0),mι(20,21)^1)^2^3(21,22,23)6(0^0)(20))^(0^1)(21))
m0 ,m1 ,m2 ,m3
σgm2)(22))σ(αm3)(23))WmxWmXWmx Wχ
+nL	X :。m。),m1 ,m2(20,21,22)θm2),m3 (22,23)σ(αmO)(XO))6(amI)(XI))
m0 ,m1 ,m2 ,m3
σ(α(L)(29))6(α(L)(XQ))W(L), W(L), W(L), W(L))
6(0m2 (22 ))6(0m3 (23 )) m0 k0 m1 k1 m2 k2 m3 k3
+n-2	X	θmo),mι (20,21)θmι),m2 (21,22 )θm2),m3 (22,23 )6(。*(XO))况湍? (XI))
m0 ,m1 ,m2 ,m3
σ(α(L)(X))σ(α(L)(XQ))W(L), W(L), W(L), W(L))
6(0m2 (22 ))6(0m3 (23 )) m0 k0 m1 k1 m2 k2 m3 k3
+n-2	X	amι),m2,m3 (x1,22,23)σ(αmI)(XO))σ(αmI)(XI))σ(αm2)(22))σ(αm3)(23))
m1 ,m2 ,m3
δk0k1Wm(L2)k2Wm(L3)k3
+n-2	X	θmι),m2 (21, 22)θm2),m3 (22, 23)σ(αm) (20))(σ(αw (XI))6(am? (22))<σ(αm3) (23))
m1 ,m2 ,m3
δk0k1Wm(L2)k2Wm(L3)k3
+n-2	X	*OmIm2 (20,X1,X2)6(amO)(XO))6(amI)(XI))6(am2)(X2))6(am2)(X3))
m0 ,m1 ,m2
Wm(LO)kOWm(L1)k1δk2k3
+n-2	X	θmo),mι (20,21)θmι),m2 (x1 ,22 )6(αmO)(XO))6(αmI)(XI))σ(αm? (x2 ))σ(αm? (23))
mO ,m1 ,m2
Wm(LO)kOWm(L1)k1δk2k3
+ n-2 X θm,m2 (x1,x2)σ(αmι)(xO))σ(αmI)(XI))σ(αm2)(x2))σ(αm2)(x3))δkokι δk2k3
m1 ,m2
+n-2	X	θmo),mι (xO,x1)θmι),m3 (22,x3)σ(αmO)(XO))6(αmI)(XI))6(αm? (x2 ))σ(αm? (23))
mO ,m1 ,m3
Wm(Lo)koδk1k2Wm(L3)k3
As nι,..., n'-ι → ∞ and for any times t < T, the NTK Θ(') converges to its limit while Ω(')
and Γ(') vanishes. rkL+1)k2 晅(xo, X1,X2,X3) therefore converges to:
+n-2 X ^L&O^C^L)(X1,x2)θ∞L)(X2,x3)σ(aL)(XO))σ(0L)(XI))σ(0L)(X2))σ(aL)(X3))
m
Wm(Lk)oWm(Lk)1Wm(Lk)2Wm(Lk)3
16
Published as a conference paper at ICLR 2020
+n-2 ɪ2θ∞)(x1,x2)θ∞)(x2,x3)σ(amL)(x0))fσ(αm)(x1 ))σ(αm)(x2))fσ(αmL)(x3))
m
δk0k1Wm(Lk)2Wm(Lk)3
+n-2 X θ∞L)(X0,xI)θ∞L)(X1,x22mmL)(XO))σmmL)(XI ))<σ(α^ (x2))σ(α(L) (x3))
m
Wm(Lk)0Wm(Lk)1δk2k3
+n-2 £ θ∞L)(X1,x2^mmL)(XO))σmmL) (x1))<σ(αm)(x2))σ(arn)(x3))δkokι δk2k3
m
+n-2 X 03)氏0,21)03)3,23)69(^)^0))69^)3 ))69^)3))69(^)氏3))
m
Wm(Lk)0δk1k2Wm(Lk)3
For the convergence during training, we proceed similarily to the proof of Lemma 1. At
initialization, all terms vanish as n` → ∞ because all summands are independent (after
taking the n1 , . . . , nL-1 → ∞ limit) with zero mean and finite variance. During training,
the weights W(') and preactivations α(') move at a rate of 1∕√n' which leads to a change of
order n—2+1/2 = n-1.5, which vanishes for all times t too.	口
C The Matrix S
We now have the theoretical tools to describe the moments of the matrix S. We first give a
bound for the rank of S :
Proposition 3. Rank(S) ≤ 2(n1 + ... + nL—1)N nL
Proof. We first observe that S is given by a sum of NnL matrices:
N nL
Spp0 = X X ∂ikC∂θ2pθp fθ,k(Xi).
It is therefore sufficiant to show that the rank of each matrices Hfθ,k (X) = ∂θ2 θ 0 fθ,k(Xi)
is bounded by 2(n1 + ... + nL).
The derivatives ∂θp fθ,k (X) have different definition depending on whether the parameter θp
is a connection weight ^W(') or a bias bj'):
dW(') fθ,k(X) = √=α(')(χ; θ)dα('+1)3θ)fθ,k(X)
db(') fθ,k (x) = βda('+1)3θ)fθ,k (x)
These formulas only depend on θ through the values (α(')(X; θ))	and (d4(')(x.e) fθ,k (x))
for ` = 1, ..., L-1 (note that both α(0) (X) = Xi and ∂ (L)	fθ k (X) = δik do not depend on θ).
i	i	αi γχ[θ)八，
Together there are 2(n1 + ... + nL—1) of them. As a consequence, the map θ 7→ ∂θp fθ,k(Xi) p
can be written as a composition
θ ∈ RP → (α(')(X; θ),∂a('/θ)fθ,k(X))e 产及如+…+”)→ @,/趾(Xi))° e RP
and the matrix Hfθ,k (x) is equal to the Jacobian of this map. By the chain rule, Hfθ,k (x)
is the matrix multiplication of the Jacobians of the two submaps, whose rank are bounded
by 2(n1 + ... + nL—1), hence bounding the rank of Hfθ,k (X). And because S is a sum
of NnL matrices of rank smaller than 2(n1 + ... + nL—1), the rank of S is bounded by
2(nι + ... + nL-i)NnL.	口
17
Published as a conference paper at ICLR 2020
C.1 Moments
Let us now prove Proposition 4:
Proposition 4. For any loss C with BGOSS and σ ∈ Cb4(R), the first two moments of S
take the form
Tr(S(t)) = G(t)T VC(t)
Tr (S(t)2) = VC(t)TΥ(t)VC(t)
-	At initialization, gθ and fθ converge to a (centered) Gaussian pair with covariances
E[gθ,k(x)gθ,k0 (x0)] = δkk0 Ξ(∞L) (x, x0)
E[gθ,k(x)fθ,k0(x0)] = δkk0Φ(∞L)(x, x0)
E[fθ,k(x)fθ,k0 (x0)] = δkk0 Σ(∞L) (x, x0)
and during training gθ evolves according to
N
∂tgθ,k (x) = X Λ∞L(x,Xi)∂ik C(Y(t))∙
i=1
-	Uniformly over any interval [0, T] where R0T kVC (t)k2 dt is stochastically bounded, the kernel
Y(L) has a deterministic and fixed limit limnL-ι→∞ …limnι→∞ YLO(x,x0) = δkk> Υ∞L)(x, x0)
with limiting kernel:
L-1
Y∞L)(x, x0) = X (θ∞)(x, x0)2Σ(')(x, x0) + 2Θ∞)(x, x0)Σ(')(x, x0)) Σ('+1)(x, x0)…Σ(L-1)(x, X).
2=1
-The higher moment k > 2 vanish: limnL-ι→∞ ∙∙∙ limnι→∞ Tr (Sk) = 0.
Proof. The first moment of S takes the form
Tr(S)=X(VC)THp,pY= (VC)T G
p
where G is the restriction to the training set of the function gθ (x) = p ∂θ2 θ fθ (x). This
process is random at initialization and varies during training. Lemma 3 below shows that, in
the infinite width limit, it is a Gaussian process at initialization which then evolves according
to a simple differential equation, hence describing the evolution of the first moment during
training.
The second moment of S takes the form:
PN
Tr(S2) =XX
∂θ2p1,θp2 fθ,k1 (x1)∂θ2p2,θp1 fθ,k2 (x2)c0i1 (xi1)c0i2 (xi2)
p1 ,p2=1 i1 ,i2=1
=(VC)T Y VC
where Υ(kL1,)k2(x1,x2) = PpP1,p2=1 ∂θ2p1,θp2fθ,k1(x1)∂θ2p2,θp1fθ,k2(x2) is a multidimensional ker-
nel and Y is its Gram matrix. Lemma 4 below shows that in the infinite-width limit,
Y(kL,)k (x1, x2) converges to a deterministic and time-independent limit Y(∞L) (x1, x2)δk1k2 .
To show that Tr(Sk) → 0 for all k > 2, it suffices to show that S2 F → 0 as Tr(S k) <
IlS2 ∣∣f ∣∣SkF—2 and We know that ∣∣S∣∣f → (∂γC)T Y∂γC is finite. We have that
N	nL
∣∣S2 ∣∣F = X	X	Ψ(k0,)k1,k2,k3(xi0, xi1, xi2, xi3)∂fθ,k0(xi0)C∂fθ,k1(xi1)C
i0,i1,i2,i3 =1 k0,k1,k2,k3=1
18
Published as a conference paper at ICLR 2020
∂fθ,k2(xi2)C∂fθ,k3(xi3)C
=Ψ ∙ (∂γC严
for Ψ the NnL × NnL × NnL × NnL finite version of
P
Ψk0,k1,k2,k3 (xi0 , xi1 , xi2 , xi3 ) =	∂θp0 ,θp1 fθ,k0 (x0)∂θp1 ,θp2 fθ,k1 (x1)
p0,p1,p2,p3=1
∂θ2p2,θp3fθ,k2(x2)∂θ2p3,θp0fθ,k3(x3).
which vanishes in the infinite width limit by Lemma 5 below.	□
Lemma 3. For any loss C with BGOSS and σ ∈ Cb4 (R), at initialization gθ and fθ converge
to a (centered) Gaussian pair with covariances
E[gθ,k(x)gθ,k0 (x0)] = δkk0 Ξ(∞L) (x, x0)
E[gθ,k(x)fθ,k0 (x0)] = δkk0 Φ(∞L) (x, x0)
E[fθ,k(x)fθ,k0 (x0)] = δkk0 Σ(∞L) (x, x0)
and during training gθ evolves according to
N
∂tgθ(x) = XΛ(∞L)(x,xi)Di(t)
i=1
Proof. When L = 1, gθ (x) is 0 for any x and θ.
For the inductive step, the trace gθ(L,k+1) (x) is defined recursively as
nL
下 X g鼠(x)σ (&mL)(X)) W(Lk + Tr (Vfθ,m(x)(Vfθ,m(x))τ) σ (&mL)(X)) WmL
L m=1
First note that Tr Vfθ,m (X) (Vfθ,m (X))T = Θ(mLm) (X, X). Now let n1, ...nL-1 → ∞, by the
induction hypothesis, the pairs (g(m, αmL)) converge to iid Gaussian pairs of processes with
covariance Φ(∞L) at initialization.
At initialization, conditioned on the values of gmL), αmL) the pairs (g((L+1), fθ) follow a centered
Gaussian distribution with (conditioned) covariance
nL
E⅛+1)(χ)g(L+1)(χ0) Ig(Lm ,αmL)] =詈 X (gθLm (χ)”⅛L)(χ))+θ∞L)(χ,χ)σ 卜(L)(X)))
L m=1
(g(Lm(χ0)"⅛L)(χ0)) +θ∞L)(χ0,χ0)σ (αm)(χ0)))
nL
E[g(Lfc+l)(χ)fθ,ko (x0)|g(Lmm ,amL)] =看 X (gθLm (χ)”⅛L)(χ))+θ∞L)(χ,χ)σ 卜 (L)(X)))
L m=1
σ kmL) (XO))
nL
E[fθ,k(X)fθ,k0(XxgθLm ,aL)]=肃 χσ (αmL) (X))U限3)+β 2.
L m=1
As nL → ∞, by the law of large number, these (random) covariances converge to their
expectations which are deterministic, hence the pairs (gk(L+1), fθk) have asymptotically the
same Gaussian distribution independent of g(L), amL):
E hgθ(L,k)(X)gθ(L,k)0(X0)i → δkk0 Ξ(∞L)(X, X0)
19
Published as a conference paper at ICLR 2020
E gθ(L,k)(x)fθ(,Lk)0(x0) → δkk0 Φ(∞L)(x, x)
E hfθ(,Lk)(x)fθ(,Lk)0(x0)i → δkk0 Σ(∞L)(x, x)
with Ξ(∞1) (x, x0) = Φ(∞1) (x, x0) = 0 and
Ξ∞L+1)(χ,χ0) = E[gg0σ(α)σ(α0)]
+ Θ∞L)(x0, x0)E [gσ(α)σ(α0)]
+ Θ∞L)(x, X)E [g0σ(α0)σ(α)]
+ Θ∞L)(x, x)Θ∞L)(x0, x0)E [σ(α0)σ(α)]
=Ξ∞)(x,x0)Σ∞L)(x,x0) + (Φ∞L)(X,x0)Φ∞L)(X0,x) +Φ∞L)(X,x)Φ∞L)(X0,x0)) Σ∞)(x,x0)
+ Φ∞L)(X,x0)Φ∞L)(X0,x0)E[σ(α)σ.(α0)] + Φ∞L) (X,X)Φ∞L)(X0,X)E[σ∙(α)σ(α0)]
+ Θ∞L)(x0, x0) (φ∞L)(x, x)∑∞L)(x, x0) + Φ∞L)(x, x0)E [σ(α)σ∙(α0)])
+ Θ∞L)(X,X) (φ∞L)(X0,X0)Σ∞L)(X,X0) + Φ∞L)(X0,X)E [σ(α)<σ(ɑ0)]^
+ θ∞L)(X,x)θ∞L)(X0,x0)∑ 臣也犷)
and
Φ∞L+1)(X, X0) = E [g<σ(α)σ(α0)] + Θ∞L)(X, X)E [σ(α)σ(a0)]
=φ∞L)(X,x')∑(L+1)(X,X0) + (φ∞L)(X,x) + θ∞L)(X,x)) e [σ(α)σ(α0)]
where (g, g0 , α, α0 ) is a Gaussian quadruple of covariance
'Ξ∞L)(X,x)	Ξ∞L)(X,x0)	Φ∞L)(X,x)	Φ∞L)(X,x0)、
Ξ∞L)(X, x0) Ξ∞L)(X0,x0) Φ∞L)(X0,x) Φ∞L)(X0,x0)
Φ∞L) (x, x) Φ∞L) (x0, x)	∑∞L) (x, x)	∑∞L) (x, x0)
Φ(∞L)(X, X0) Φ(∞L)(X0, X0)	∑(∞L)(X, X0)	∑(∞L)(X0, X0)
During training, the parameters follow the gradient ∂tθ(t) = (∂θY (t))T D(t). By the
induction hypothesis, the traces gθ(L,m) then evolve according to the differential equation
N nL	T
dtgθLm(χ)=√n= x x Amm，(χ,χi)σ(⅛LO)(X)) (WmL)) Di(t)
L i=1 m=1
and in the limit as n1, ..., nL-1 → ∞, the kernel Λ(mLm) 0 (X, Xi) converges to a deterministic
and fixed limit δmmoA∞L)(x, Xi). Note that as nL grows, the g；Lm(x) move at a rate of 1∕√nL
just like the pre-activations aJmL). Even though they move less and less, together they affect
the trace gθ(L,k+1) which follows the differential equation
N nL
∂tgθ(L,k+1)(X) = x x A(kLk+0 1)(X, Xi)Dik0 (t)
i=1 k0=1
where
AkL+1) (X,X0) = nz X Amm0 (X,x0)σ ^mL)(X)) σ 卜察3)) WJmk)WJ(Lk0
m,m0
+n1- x g(Lm(X)0mm o (χ,χ0)"⅛L)(X)) ”⅛LO)(XO)) WmL)WmL ko
m,m0
+: x g(Lm(X)σ (⅛L)(X))σ km>L)(XO)) δkk0
Lm
20
Published as a conference paper at ICLR 2020
+	_2_	X	Ω(L)	(xo x χ)σ (a(L)(X)) σ	(α(L)(XO)) W(L)W(L)
十 nL	/ /	"m0mm(X ,x,x)σ [αm (X)J σ	(αm0 (X )J Wmk Wm0k0
m,m0
+	_1	X	θ(L) (x x)θ(L) (x xo)σ (a(L)(X)) σ (α(L)(XO)) W(L)W(L)
+	n^	^	θmm(x,x)θmm0(X,x )σ (αm	(X)J Q (αm0 (X )J Wmk	Wm0k0
L m,m0
+； X θmmm (x,x)q (⅛L) (X)) q (⅛L)(XO)) δkk0.
Lm
As n1, ..., nL-1 → ∞, the kernels Θ(mLm) 0(X, X0) and Λ(mLm) 0 (X, X0) converge to their limit and
am0mm(x0,x,x) vanishes:
ΛL (x,x0) → n- X Λ∞L) (χ,χ0)σ ^mL)(X)) σ QmL)(XO)) Wm)Wmk)0
Lm
+ n- X gθLm (x)θ∞L)(X3泄(⅛L)(X))σ (⅛L)(XO)) Wmk)Wmk)o
Lm
+n- x gθLm (x)"⅛L)(X))σ 卜^L) (XO)) δkk0
Lm
+ nL x Θ∞L)(X,X)Θ∞L)(X,X0HkmL)(X)) σ ^mL)(XO)) Wm)Wm)0
Lm
+ n^ X θ∞L) (x, x涧 (⅛L) (X))σ (⅛L) (XO)) δkk0
Lm
By the law of large numbers, as nL → ∞, at initialization Λ(kLk+0 1)(x, x0) → δkk0 Λ(∞L+1)(x, x0)
where
Λ∞L+I)(X,x0) = Λ∞L)(X,x0)Σ ∞L+1)(x,x0)
+ Θ∞L)(x, x0)E [gσ (α) σ (a0)]
+ E [gσ (α) σ (a0)]
+ θ∞L) (x, x)θ∞L) (x, χ0)E [σ (a) σ (α')]
+ Θ∞L) (x, x)E [σ (α) σ (α0)]
=Λ∞L)(X,x0)Σ ∞L+I)(X,x0)
+ Θ∞L)(X,X0) (φ∞L)(χ,X0)∑∞L+1)(X,X0) + Φ∞L)(X,x)E[σ∙(α)σ(α0)])
+ Φ∞L)(x, x0)∑∞L+1)(x, x0) + Φ∞L)(x, x)E [σ (α) σ (α0)]
+ Θ∞L) (x, x)Θ∞L) (x, x0)E [σ (α) σ (α0)]
+ Θ∞L) (x, x)E [σ (α) σ (α0)]
During training Θ(∞L) and Λ(∞L) are fixed in the limit n1, .., nL-1 → ∞, and the values
gθLmm (x), α(L)(x) and WmLk) vary at a rate of 1∕√nL which induce a change of the same rate
to Λ(L(x, x0), which is therefore asymptotically fixed during training as n^ → ∞.	□
The next lemma describes the asymptotic limit of the kernel Υ(L) :
Lemma 4. For any loss C with BGOSS and σ ∈ Cb4(R), the second moment of the
Hessian of the realization function HF (L) converges uniformly over [0, T] to a fixed limit as
n1 , ...nL-1 → ∞
L-1
YkLO(X, X0) → δfcfc0 X (θ∞∞)(X, xo)2Σ∞∞)(X, X0) + 2Θ∞∞)(X, xo)Σ∞∞)(X, X0)) Σ∞∞+1)(x, xo)…Σ∞L-1)(x, xo).
'=1
21
Published as a conference paper at ICLR 2020
Proof. The proof is by induction on the depth L. The case L = 1 is trivially true because
∂θ2 θ fθ,k (x) = 0 for all p, p0 , k, x. For the induction step we observe that
Υ(kL,k)0 (x, x0)
P
= X ∂θ2p1,θp2 fθ,k(x)∂θ2p2,θp1 fθ,k0(x0)
p1 ,p2 =1
1
nL
1
nL
1
nL
+
+
nL
X Y(L) (X χ0)σ αα(L)(x)) σ (α(L)(x0)∖ W(L)W(L)
Υm,m0 (x, x )σ αm (x) σ αm0 (x ) Wmk Wm0k0
m,m0 =1
nL
ΣΩ(L)	(χ0 x χ0)σ αα(L)(x)) σ αα(L)(x0)∖ W(L)W(L)
Ωm0,m,m0 (x，x，x )σ (αm (X)J σ ^αm0 (X ) J Wmk Wm0k0
m,m0 =1
nL
ΣΩ(L)	(x χ0 χ)σ αα(L)(x)) σ (α(L)(x0)∖ W(L)W(L)
Ωm,m0,m(X，X，x)σ (αm (X)J σ ^αm0 (X )J Wmk Wm0k0
m,m0 =1
1	nL
+____X Θ(L) (x x0)θ(L) (x0 x)σ(α(L)(X)) σ(α(L)(X0)) W(L)W(L)
十 nj	θm,m0 (X，X )θm0,mW ")σ (αm (X)J σ (αm0 (X )J Wmk Wm0k0
L m,m0 =1
nL
十 ~ X θm,mo E“加卜曾)3)σ km，)(#))δkk0
L m=1
if We now let the width of the lower layers grow to infinity n1,...nL-1 → ∞, the tensor Ω(L)
vanishes and Υ(mL,)m0 and the NTK Θ(mL,)m0 converge to limits which are non-zero only when
m = m0. As a result, the term above converges to
1 nL
嬴 X Y∞L)(X,X0)σ km⅜)) σ (⅛L)(X0)) WmLkWmLk,
L m=1
nL
+高 X θ∞L)(X,X0)2σ km⅜)) σ ^mv)) Wmk)Wmk)o
L m=1
2 nL
+ — X 嗯)也 X )σ 卜 mL)(X))σ 卜 mL)(X)) δkk,
L m=1
At initialization, we can apply the law of large numbers as nL → ∞ such that it converges
to Υ(∞L+1) (X, X0)δkk0 , for the kernel Υ(∞L+1) (X, X0) defined recursively by
Y∞L+1)(X,X0) =Y∞L)(X,X0)∑∞L)(X, X0) + Θ∞L)(X,X0)2Σ∞L)(X,X0) + 2Θ∞L)(X,X0)Σ∞L)(X,X0)
and Υ(∞1) (X, X0) = 0.
For the convergence during training, we proceed similarily to the proof of Lemma 1: the
activations αmL)(X) and weights WmLk move at a rate of 1/√nL and the change to YkL+I) is
therefore of order 1/√nL and vanishes as nL → 0.	□
Finally, the next lemma shows the vanishing of the tensor Ψ(kL,)k ,k ,k to prove that the
higher moments of S vanish.
Lemma 5. For any loss C with BGOSS and σ ∈ Cb4(R), uniformly over [0, T]
lim
nL-1 →∞
lim Ψ(L)	(Xi , Xi , Xi , Xi ) = 0
n1→∞	k0,k1,k2,k3	i0 , i1 , i2 , i3
Proof. When L = 1 the Hessian is zero and Ψ(k1),k ,k ,k (Xi0 , Xi1 , Xi2 , Xi3) = 0.
For the induction step, we write Ψ(kL,+k1,)k ,k (Xi0 , Xi1 , Xi2 , Xi3) recursively, because
it contains many terms, we change the notation, writing
X0	X1
m0	m1
for
22
Published as a conference paper at ICLR 2020
Θm0 ,m1 (XO , X1 ),
XO	X1	X2
mO	m1	m2
for αm0,mι ,m2 (X0, x1, x2 ) and
XO	X1	X2	X3
mO	m1	m2	m3
for Γm0 ,m1 ,m2 ,m3 (XO , X1 , X2 , X3 ). The value Ψk0 ,k1 ,k2 ,k3 (Xi0 , Xi1 , Xi2 , Xi3 ) is then equal to
n-2	X	ψm0),m1,m2,m3 (xO,x1,x2,x3)<σ (am0)(xO)) <σ (ami)(x1)) <σ (am2)(x2))
m0 ,m1 ,m2 ,m3
“am3)(χ3)) WmIWmIWm2)k2 WmL
+nL-2
Σ
m0 ,m1 ,m2 ,m3
XO	X1
mO	m1
x1
m1
x2
m2
X2	X3
m2	m3
x3
m3
x0
m0
σ 卜m0) (XO))
+nL-2
Σ
m0 ,m1 ,m2 ,m3
σ (am)
x0
m0
(Xl)) σ (a(L)(Xo)) σ (a(L)(xq)) W(')
(X1) σ αm2 (X2) σ αm3 (X3)	Wm0k0
W(L)
m1 k1
Wm(L2)k2Wm(L3)k3
x1
m1
x2
m2
X2	X3
m2	m3
x3
m3
x0
m0
σ 卜m0)(XO)) σ (ami)(XI))
σ
(X3)	W(L) W(L) W(L)	W(L)
(X3 )	m0 k0	m1 k1	m2 k2	m3 k3
(am2)(χ2)) σ (am?
+nL-2
Σ
m0 ,m1 ,m2 ,m3
+nL-2
Σ
m0 ,m1 ,m2 ,m3
x0
m0
x0
m0
x1
m1
x1
m1
x1
m1
x1
m1
x2
m2
x2
m2
x3
m3
x3
m3
x0
m0
X2	X3	XO
m2	m3	mO
σ km? (XO))σ gm? (XI))
σ 卜m0)(XO)) σ (ami)(XI))
σ
σ
(a(L)(X)) σ (a(L)(XRW(L)) W(L)) W(L)) W(L))
am2 (X2 )	σ am3 (X3)	m0k0	m1k1	m2k2	m3k3
+nL-2
Σ
m0 ,m1 ,m2 ,m3
x1
m1
x2
m2
x2
m2
x3
m3
X3	XO	X1
m3	mO	m1
“am0)(XO)) σ (ami)(XI))
σ
(a(L)(X2)、σ (a(L)(X3)> W(L)) W(L)) W(L)) W(L))
am2 (X2 )	σ am3 (X3)	m0k0	m1k1	m2k2	m3k3
+nL-2
Σ
m0 ,m1 ,m2 ,m3
x0
m0
x1
m1
x2
m2
x2
m2
X3	XO
m3	mO
σ (am0) (XO)) “&mι) (XI))
σ
卜m?(X2)) σ
+nL-2
Σ
m0 ,m1 ,m2 ,m3
x1
m1
x2
m2
x3
m3
x3
m3
XO	X1
mO	m1
σ (dm? (XO)) σ 卜mi) (XI))
σ
卜m2) (X2 D σ
aa(L) (XR)[ W(L)) W(L)) W(L)) W(L))
am3 (X3 )	m0 k0	m1 k1	m2 k2	m3 k3
+nL-2
Σ
m0 ,m1 ,m2 ,m3
x0
m0
x1
m1
X2	X3
m2	m3
x3
m3
x0
m0
σ (am0)(Xo)) “ami)(Xi))
σ
(am2) (X2。σ
(a(L)(X3)> w(L)) w(L)) w(L)) w(L))
αm3 (X3 )	m0 k0	m1 k1	m2 k2	m3 k3
+nL-2
Σ
m0 ,m1 ,m2 ,m3
x0
m0
x1
m1
X1 X2
m1	m2
x3
m3
x0
m0
σ Gm? (XO))σ kmi)(Xi))
“αm2) (X2“ σ
+nL-2
Σ
m0 ,m1 ,m2 ,m3
x1
m1
x2
m2
X2	X3
m2	m3
XO	X1
mO	m1
σ (amo) (XO)) σ 卜mi)(Xi))
σ 卜m2) (X2 D σ
aa(L) (XR)[ W(L)) W(L)) W(L)) W(L))
am3 (X3 )	m0 k0	m1 k1	m2 k2	m3 k3
23
Published as a conference paper at ICLR 2020
+n-2	E
m0,m1 ,m2,m3
+n-2	X	m
m,mi,m2 L
+n-2	X	尸1
L	2—/	m
m,m2,m3 L
+n-2	X [ m0
m,m3,m0 L
+n-2	E
m,m0 ,mi
X0
m0
+n-2	∑ m
m,mi,m2 L
+n-2 X	X1
L	2—/	m
m,m2,m3 L
+n-2	E
m,m3,m0
+n-2	E
m,m0 ,mi
X2
m2
X1
m1
X2
m2
X1
m
X1
m1
X1
m1
X2
m2
X0
m0
xι
m1
+n-2	X m
m,mi ,m2 L
+n-2	X	[x1
Lm
m,m2,m3 L
+n-2	X [x2
Lm
m,m3,m0
X1
m
X2
m
X1
m1
X2
m2
X3
m3
X3	X3	X0	X1	X2
m3	m3	m0	m1	m2
σ Qm?(x0))σ ^mI)(XI))
σ(<(X2))σ (am; m)) WmlWmlWmlWml
X1
m1
X2
m2
X2
m
X1
m1
X2	X2
m2	m2
σ Q m?(X2))
X3	X3
m3	m3
σ (碌经))
X3	X3
m3	m3
X3
m
σ 卜(L)(X°))σ GmLI(XI))
σ(⅛L)(X3))WmXWmX 必晅
X0
m
X0
m0
“&(L)(Xο))σ km¼1))
σ 卜 (^(⑹)σ km¼1))
σ 卜^L)(X2))σ (<(X3)) WmIWml a
X2	X3
mm
X0
m0
σ 卜 (^(⑹)σ ^mI)
σ (α(L)(X2)) σ (α(L)(XR)) W(L), W(L),
σ Iam (X2) J σ Iam (X3) J m 0n0ko *「miki
X2	X2
m2	m2
X3
m
σ同以⑹)σ卜
X3	X3
m3	m3
X2	X3
m m3
σ km¼0))σ (&mI)(XI))
X0
m
X0
m0
碌)
σ
σ
(χ1))
i δk2k3
(X3))WmIWml *3
(«m)(X0)) σ («m)(X1)) ^ʃ («m72)(X2))
σ(&m?(X3))WmlWml ⅛⅛
(碌05)) σ (⅛L)(XI))
(L)	(L)	(L)	(L)
σ (am)(X2)) σ (%3)(23)) Wm。% 吗3兀 δkik2
X3 X0 X1
m m0 m1
σ (就此。))σ (碌I)(XI))
σ 卜(L)(X2))σ ^m⅜3)) WmiWml a®3
X1
m1
X2
m2
X3
m
σ km¼0))σ (&mI)(XI))
σ卜曾以⑹)
X2	X3
m2	m3
σ^m⅜3)) WmIWml ⅛lk3
m σ(αm¼0))σ (碌)(XI))
(L)	(L)	(L)	(L)
σ ^2)(X2)J σ (4^5)) Wm2k2 叱3k3 δk0ki
X3	X0
m3	m0
X1
m
σ (明此。))σ (碌)(x1))
σ(碌)5))σ (c)(χ3)} WmiWmI
24
Published as a conference paper at ICLR 2020
+n-2 X [ x0	x1 x2][x3 x0]σ (α(L)(XO)) σ(α(L)(XI))
L	m0 m1 m m m0 m0 0	m1 1
m,m0,m1
σ ^mL)(X2)) σ 卜鼎X3)) WmLkOWLkIδk2k3
+nL-2
m,m1,m2
+nL-2
m,m2,m3
+nL-2
m,m3,m0
+nL-2
m,m0,m1
+nL-2
m,m0
+nL-2
m,m0
X0	X1
X2	X3
m m1	m2	m
X1	X2	X3	X0
m m2	m3	m
X2	X3	X0	X1
m	m3	m0	m
X3	X0	X1 X2
m m0 m1 m
X0 X1	X2 X3
m m0	m0 m
X1 X2	X3 X0
m m0	m0 m
σ kmL)(XO))σ km? (XI))σ km?(X2" “&mL)(X3 D
Wm(L1)k1Wm(L2)k2δk0k3
σ kmL)(XO))σ kmL) (XI))万卜取2))万卜思出))
Wm(L2)k2Wm(L3)k3δk0k1
σ km? (XO))σ kmL) (XI))σ kmlL)(X2))“砒(X3))
WmmL0)k0WmmL3)k3δk1k2
σ卜窝⑹)σ km? (XI))σ卜鼎2)) “碇3))
WmmL0)k0WmmL1)k1δk2k3
“&mL)(XO)) σ (am，)(X1)) σ km，)(X2)) σ 卜(L)(X3))
δk0 k1 δk2 k3
σ kmL)(XO)) σ (amL)(XI)) σ kmO)(X2)) σ 卜(LO)(X3))
δk0 k3 δk1 k2
Even though this is a very large formula one can notice that most terms are “rotation of
each other”. Moreover, as n1,...,nL-1 → ∞, all terms containing either an ΨmL), an ΩmL) or
a ΓmL) vanish. For the remaining terms, we may replace the NTKs ΘmL) by their limit and
as a result ΨmkL0,+k11,)k2,k3 (Xi0, Xi1 , Xi2, Xi3) converges to
n-2 X θ∞L) (xO, x1 )θ∞L) (x1, x2)θ∞L) (x2 , x3)θ∞L) (x3,⑹万 kmL) (XO)) σ kmL) (x1 ))
m
σ (amL) (X2)) σ kmL)(X3)) Wm)O Wmk)IWm)2 Wm)3
+n-2 XΘ∞L)(XO,X1)Θ∞L)(X1,X2)Θ∞L)(X2,x3)σ kmL)(xo)) σ 卜(L)(XI))
m
σ kmL)(X2)) σ kmL)(X3)) Wm)IWmk)2 δk0k3
+n-2 X θ∞L)(X1,x2)θ∞L)(X2,x3)θ∞L)(X3,xO)σ (αmL) (XO)) σ (⅛L)(XI))
m
σ kmL)(X2)) σ kmL)(X3)) WmI Wmk)3 s^
+n-2 XΘ∞L)(XO,X1)Θ∞L)(X2,X3)Θ∞L)(X3,XO)σ 卜mL)(xo)) σ 卜(L)(XI))
m
σ kmL)(X2)) σ kmL)(X3)) Wm)OWm)3 δk1k2
+n-2 X θ∞L)(XO,χ1)θ∞L)(X1,χ2)θ∞L)(X3,χO)σ kmL) (XO)) σ 卜(L)(XI))
m
σ kmL)(X2)) σ kmL)(X3)) Wm)OWmk)I δfc2fc3
25
Published as a conference paper at ICLR 2020
+n-2 XΘ∞)(x0,xι)Θ∞)(x2,X3)σr α(mL)(χoo)) σ (⅛L)(xι))
m
"⅛L)(X2" “&mL)(X3“ ⅛lk1δk2k3
+n-2 X θ∞)(x1,x2)θ∞)(x3,x0)<σ (am)(x0D σ (⅛L)(XI))
m
"⅛L)(X2" “&mL)(X3“ δk0k3 δk1k2
And all these sums vanish as nL → ∞ thanks to the prefactor nL-2 , proving the vanishing of
Ψ(kL0,+k11,)k2,k3 (Xi0, Xi1, Xi2, Xi3) in the infinite width limit.
During training, the activations α(jL)(X) and weights WmLk move at a rate of 1/√nL which
induces a change to Ψ(L+1) of order n-3/2 which vanishes in the infinite width limit. □
D Orthogonality of I and S
From Lemma 2 and the vanishing of the tensor Γ(L) as proven in Lemma 2, we can easily
prove the orthogonality of I and S of Proposition 5:
Proposition 5. For any loss C with BGOSS and σ ∈ Cb4(R), we have uniformly over [0, T]
lim …lim ∣∣IS∣∣f = 0.
nL-1 →∞	n1 →∞
As a consequence limnr-ι→∞ ∙ ∙ ∙ limnι→∞ Tr ([I + S]k) — [Tr(Ik) + Tr (Sk)] = 0.
Proof. The Frobenius norm of IS is equal to
IlISkF = ∣∣DYHC(DY)T (VC ∙HY)∣∣2
P / PNnL	∖ 2
=E (EE E dθp1 fθ,kι (XiI)Ckl (XQdθpfθ,kι (Xiι)*p3fθ,k2 (X2)(Xi2 )九(Xi2 ))
p1,p2=1 p=1 i1,i2=1 k1,k2=1
N	nL
=	c0k01 (Xi1)c0k001 (Xi01)c0k2 (Xi2)c0k20 (Xi02)Θk1,k10 (Xi1, Xi01)Γk1,k2,k20,k01 (Xi1, Xi2, Xi02, Xi01)
i1 ,i2 ,i01 ,i02=1 k1 ,k2 ,k10 ,k20 =1
and Γ vanishes as n1 , ..., nL-1 → ∞ by Lemma 2.
The k-th moment of the sum Tr (I + S)k is equal to the sum over all Tr (A1 …Ak) for any
word A1. ..Ak of Ai ∈ {I, S}. The difference Tr ([I + S]k) — [Tr (Ik) + Tr (Sk)] is hence
equal to the sum over all mixed words, i.e. words A1 . . . Ak which contain at least one I and
one S . Such words must contain two consecutive terms Am Am+1 one equal to I and the
other equal to S . We can then bound the trace by
∣Tr(A1 …Ak)| ≤ ∣A1∣F …kAm-1∣F ∣AmAm+1∣F ∣Am+2∣F …∣Ak∣f
which vanishes in the infinite width limit because ∣I ∣F and ∣S ∣F are bounded and
kAmAm+1∣F = ∣∣IS∣F vanishes.	□
26