Published as a conference paper at ICLR 2020
Dynamically Pruned Message Passing Net-
works for Large-scale Knowledge Graph
Reasoning
Xiaoran Xu1, Wei Feng1, Yunsheng Jiang1 , Xiaohui Xie1, Zhiqing Sun2, Zhi-Hong Deng3
1	Hulu, {xiaoran.xu, wei.feng, yunsheng.jiang, xiaohui.xie}@hulu.com
2	Carnegie Mellon University, zhiqings@andrew.cmu.edu
3	Peking University, zhdeng@pku.edu.cn
Ab stract
We propose Dynamically Pruned Message Passing Networks (DPMPN) for large-
scale knowledge graph reasoning. In contrast to existing models, embedding-
based or path-based, we learn an input-dependent subgraph to explicitly model
reasoning process. Subgraphs are dynamically constructed and expanded by ap-
plying graphical attention mechanism conditioned on input queries. In this way,
we not only construct graph-structured explanations but also enable message pass-
ing designed in Graph Neural Networks (GNNs) to scale with graph sizes. We
take the inspiration from the consciousness prior proposed by Bengio (2017) and
develop a two-GNN framework to encode input-agnostic full structure represen-
tation and learn input-dependent local one coordinated by an attention module.
Experiments show the reasoning capability of our model to provide clear graph-
ical explanations as well as predict results accurately, outperforming most state-
of-the-art methods in knowledge base completion tasks.
1 Introduction
Modern deep learning systems should bring in explicit reasoning modeling to complement their
black-box models, where reasoning takes a step-by-step form about organizing facts to yield new
knowledge and finally draw a conclusion. Particularly, we rely on graph-structured representation
to model reasoning by manipulating nodes and edges where semantic entities or relations can be
explicitly represented (Battaglia et al., 2018). Here, we choose knowledge graph scenarios to study
reasoning where semantics have been defined on nodes and edges. For example, in knowledge base
completion tasks, each edge is represented by a triple hhead, rel, taili that contains two entities and
their relation. The goal is to predict which entity might be a tail given query hhead, rel, ?i.
Existing models can be categorized into embedding-based and path-based model families. The
embedding-based (Bordes et al., 2013; Sun et al., 2018; Lacroix et al., 2018) often achieves a high
score by fitting data using various neural network techniques but lacks interpretability. The path-
based (Xiong et al., 2017; Das et al., 2018; Shen et al., 2018; Wang, 2018) attempts to construct an
explanatory path to model an iterative decision-making process using reinforcement learning and
recurrent networks. A question is: can we construct structured explanations other than a path to
better explain reasoning in graph context. To this end, we propose to learn a dynamically induced
subgraph which starts with a head node and ends with a predicted tail node as shown in Figure 1.
Graph reasoning can be powered by Graph Neural Networks. Graph reasoning needs to learn
about entities, relations, and their composing rules to manipulate structured knowledge and produce
structured explanations. Graph Neural Networks (GNNs) provide such structured computation and
also inherit powerful data-fitting capacity from deep neural networks (Scarselli et al., 2009; Battaglia
et al., 2018). Specifically, GNNs follow a neighborhood aggregation scheme to recursively aggregate
information from neighbors to update node states. After T iterations, each node can carry structure
information from its T -hop neighborhood (Gilmer et al., 2017; Xu et al., 2018a).
GNNs need graphical attention expression to interpret. Neighborhood attention operation is a
popular way to implement attention mechanism on graphs (Velickovic et al., 2018; Hoshen, 2017) by
1
Published as a conference paper at ICLR 2020
(a) The AthletePlaysForTeam task.
(b) The OrganizationHiredPerson task.
Figure 1: Subgraph visualization on two examples from NELL995’s test data. Each task has ten
thousands of nodes and edges. The big yellow represents a given head and the big red represents a
predicted tail. Color indicates attention gained along T -step reasoning. Yellow means more attention
during early steps while red means more attention at the end. Grey means less attention.
focusing on specific interactions with neighbors. Here, we propose a new graphical attention mech-
anism not only for computation but also for interpretation. We present three considerations when
constructing attention-induced subgraphs: (1) given a subgraph, we first attend within it to select a
few nodes and then attend over those nodes’ neighborhood for next expansion; (2) we propagate at-
tention across steps to capture long-term dependency; (3) our attention mechanism models reasoning
process explicitly through pipeline disentangled from underlying representation computing.
GNNs need input-dependent pruning to scale. GNNs are notorious for their poor scalability.
Consider one message passing iteration on a graph with |V | nodes and |E| edges. Even if the graph
is sparse, the complexity of O(|E|) is still problematic on large graphs with millions of nodes and
edges. Besides, mini-batch based training with batch size B and high dimensions D would lead to
O(BD|E|) making things worse. However, we can avoid this situation by learning input-dependent
pruning to run computation on dynamical graphs, as an input query often triggers a small fraction
of the entire graph so that it is wasteful to perform computation over the full graph for each input.
Cognitive intuition of the consciousness prior. Bengio (2017) brought the notion of attentive
awareness from cognitive science into deep learning in his consciousness prior proposal. He pointed
out a process of disentangling high-level factors from full underlying representation to form a low-
dimensional combination through attention mechanism. He proposed to use two recurrent neural
networks (RNNs) to encode two types of state: unconscious state represented by a high-dimensional
vector before attention and conscious state by a derived low-dimensional vector after attention.
We use two GNNs instead to encode such states on nodes. We construct input-dependent subgraphs
to run message passing efficiently, and also run full message passing over the entire graph to acquire
features beyond a local view constrained by subgraphs. We apply attention mechanism between
the two GNNs, where the bottom runs before attention, called Inattentive GNN (IGNN), and the
above runs on each attention-induced subgraph, called Attentive GNN (AGNN). IGNN provides
representation computed on the full graph for AGNN. AGNN reinforces representation within a
cohesive group of nodes to produce sharp semantics. Experimental results show that our model
attains very competitive scores on HITS@1,3 and the mean reciprocal rank (MRR) compared to the
best embedding-based method so far. More importantly, we provide explanations while they do not.
2 Addressing the Scale-Up Problem
Notation. We denote training data by {(xi , yi)}iN=1. We denote a full graph by G = hV, Ei
with relations R and an input-dependent subgraph by G(x) = hVG(x) , EG(x)i which is an in-
duced subgraph of G. We denote boundary of a graph by ∂G where V∂G = N (VG) - VG and
N(VG) means neighbors of nodes in VG. We also denote high-order boundaries such as ∂2G where
V∂2G = N(N(VG)) ∪ N(VG) - VG. Trainable parameters include node embeddings {ev}v∈V,
2
Published as a conference paper at ICLR 2020
relation embeddings {er }r∈R, and weights used in two GNNs and an attention module. When per-
forming full or pruned message passing, node and relation embeddings will be indexed according to
the operated graph, denoted by θG or θG(x). For IGNN, we use Ht of size |V| × D to denote node
hidden states at step t; for AGNN, we use Ht (x) of size |VG(x) | × D to denote. The objective is
written as PiN=1 l(xi, yi; θG(xi), θG), where G(xi) is dynamically constructed.
The scale-up problem in GNNs. First, we write the full message passing in IGNN as
Ht = fIGNN(Ht-1; θG),	(1)
where fIGNN represents all involved operations in one message passing iteration over G , including:
(1) computing messages along each edge with the complexity1 of O(BD|E|), (2) aggregating mes-
sages received at each node with O(BD|E|), and (3) updating node states with O(BD|V|). For
T -step propagation, the per-batch complexity is O(BDT(|E| + |V|)). Considering that backpropa-
gation requires intermediate computation results to be saved during one pass, this complexity counts
for both time and space. However, since IGNN is input-agnostic, node representations can be shared
across inputs in one batch so that we can remove B to get O(DT (|E | + |V |)). If we use a sampled
edge set E from E such that |E| ≈ k|V|, the complexity can be further reduced to O(DT |V|).
The pruned message passing in AGNN can be written as
Ht(X) = fAGNN(Ht-1(x), Ht； θG(x))∙
(2)
Its complexity can be computed similarly as above. However, we cannot remove B . Fortunately,
subgraph G(x) is not G. If we let x be a node v, G(x) grows from a single node, i.e., G0(x) = {v},
and expands itself each step, leading to a sequence of (G0(x), G1(x), . . . , GT (x)). Here, we de-
scribe the expansion behavior as consecutive expansion, which means no jumping across neighbor-
hood allowed, so that we can ensure that
Gt(x) ⊆ Gt-1(x) ∪∂Gt-1(x) ⊆ Gt-2(x) ∪∂2Gt-2(x).	(3)
Many real-world graphs follow the small-world pattern, and the six degrees of separation implies
G0(x) ∪ ∂6G0(x) ≈ G. The upper bound of Gt(x) can grow exponentially in t, and there is no
guarantee that Gt(x) will not explode.
Proposition. Given a graph G (undirected or directed in both directions), we assume the probability
of the degree of an arbitrary node being less than or equal to d is larger than p, i.e., P (deg(v) ≤
d) > p, ∀v ∈ V. Considering a sequence of consecutively expanding subgraphs (G0, G1, . . . , GT),
starting with G0 = {v}, for all t ≥ 1, we can ensure
P |VGt| ≤
d(d - 1)t - 2
d - 2
d(d-1)t-1-2
> P	d-2
(4)
The proposition implies the guarantee of upper-bounding |VGt(x) | becomes exponentially looser and
weaker as t gets larger even if the given assumption has a small d and a large p (close to 1). We
define graph increment at step t as ∆Gt(x) such that Gt(x) = Gt-1(x) ∪ ∆Gt(x). To prevent
Gt(x) from explosion, we need to constrain ∆Gt (x).
Sampling strategies. A simple but effective way to handle the large scale is to do sampling.
1.
2.
3.
4.
∆Gt(x) = ∂Gt-1 (x), wherewe sample nodes from the boundary of GtT(x).
ʌ z->≠ / ∖	Cx√7^^"'T~Γ~ ∖	t	. 1 .11	1	C	1 1 IC ∕~d-1 / ∖
∆Gt(X) = ∂Gt-1(X), where we take the boundary of sampled nodes from Gt-1(X).
t	t1
∆Gt(x) = ∂Gt 1(x), where we sample nodes twice from Gt 1(x) and from ∂Gt 1(x).

ʌ
∆Gt(x) = ∂Gt-1(x), where we sample nodes three times with the last from ∂ Gt-1 (x).
----------- 一 一 一
Obviously, We have ∂Gt-1(x) ⊆ ∂Gt-1(x) ⊆ ∂Gt-1(x) and GtT(X) ∪ ∂Gt-1(x) ⊆ GtT(X) ∪
t1
∂Gt-1(X). Further, we let N1 and N3 be the maximum number of sampled nodes in ∂Gt-1(X) and
................................................ . ∙ ..................................
the last sampling of ∂Gt-1(X) respectively and let N2 be per-node maximum sampled neighbors in
∂Gt-1(X), and then we can obtain much tighter guarantee as follow:
1We assume per-example per-edge per-dimension time cost as a unit time.
3
Published as a conference paper at ICLR 2020
Figure 2: Model architecture used in knowledge graph reasoning.
1.	P(∣V∆Gt(x)∣ ≤ Nι(d - 1)) >PN1 for ∂Gt-1(x).
2.	P(∣V∆Gt(x)∣ ≤ N1N2) = 1 andP(∣V∆"(x)∣ ≤ N1∙min(d-I,N2)) > pN1 for ∂Gt-1(x).
..
3.	P(∣V∆Gt(x)∣ ≤ min(N1N2,N3)) = 1 for ∂Gt-1(x).
.......................................   .5，	一一 E .	，W	1
Attention strategies. Although we guarantee ∣VGT (x) ∣ ≤ 1 + T min(N1N2, N3) by ∂Gt-1(x) and
constrain the growth of Gt-1(x) by decreasing either N1N2 or N3, smaller sampling size means
less area explored and less chance to hit target nodes. To make efficient selection rather than ran-
dom sampling, we apply attention mechanism to do the top-K selection where K can be small. We
t1
change ∂Gt 1(x) to ∂Gt 1(x) where 〜represents the operation of attending over nodes and Pick-
ing the top-K. There are two types of attention operations, one applied to Gt-1(x) and the other
			
			
applied to ∂Gt-1(x). Note that the size of ∂Gt-1(x) might be much larger if we intend to sample
more nodes with larger N2 to sufficiently explore the boundary. Nevertheless, we can address this
problem by using smaller dimensions to compute attention, since attention on each node is a scalar
requiring a smaller capacity compared to node representation vectors computed in message passing.
3	DPMPN MODEL
3.1	Architecture design for knowledge graph reasoning
Our model architecture as shown in Figure 2 consists of:
•	IGNN module: performs full message passing to compute full-graph node representations.
•	AGNN module: performs a batch of pruned message passing to compute input-dependent node
representations which also make use of underlying representations from IGNN.
•	Attention Module: performs a flow-style attention transition process, conditioned on node repre-
sentations from both IGNN and AGNN but only affecting AGNN.
IGNN module. We implement it using standard message passing mechanism (Gilmer et al., 2017).
If the full graph has an extremely large number of edges, We sample a subset of edges, ET ⊂ E,
randomly each step. For a batch of input queries, we let node representations from IGNN be shared
across queries, containing no batch dimension. Thus, its complexity does not scale with batch size
and the saved resources can be allocated to sampling more edges. Each node v has a state Hτv,: at
step τ, where the initial H0v,: = ev. Each edge hv0, r, vi produces a message, denoted by Mhτv0,r,vi,:
at step τ . The computation components include:
4
Published as a conference paper at ICLR 2020
•	Message function: Mhv,内),：=Ψignn(HT,,:, er, HT,：)，Where hv0,r,v) ∈ Eτ.
Message aggregation: MT ：
√N1T(V) Pv0,r Mτv0,r,vi,:, where hv0, r，Vi ∈ El
•	Node state update function: HT：1 = HT,： + δiGNN(HT,：, MV,：, ev), where V ∈ V.
We compute messages only for sampled edges, hv0, r, v)∈ ET, each step. Functions Ψignn and
δIGNN are implemented by a two-layer MLP (using leakyReLu for the first layer and tanh for the
second) with input arguments concatenated respectively. Messages are aggregated by dividing the
sum by the square root of NT(v), the number of neighbors that send messages to v, preserving the
scale of variance. We use a residual adding to update each node state instead of a GRU or a LSTM.
After running for T steps, we output a pooling result or simply the last, denoted by H = HT , to
feed into downstream modules.
AGNN module. AGNN is input-dependent, which means node states depend on input query
x = hhead, rel, ?i, denoted by Hvt,： (x). We implement pruned message passing, running on small
subgraphs each conditioned on an input query. We leverage the sparsity and only save Hvt,： (x) for
visited nodes v ∈ VGt(x). When t = 0, we start from node head with VG0(x) = {vhead}. When
computing messages, denoted by Mhtv0,r,vi,：(x), we use an attending-sampling-attending procedure,
explained in Section 3.2, to constrain the number of computed edges. The computation components
include:
•	Message function: Mhu,rv、,：(X) = @agnn(HV，,：(x),Cr(x),HV,：(x)), where hv0,r,v) ∈
EGt(x)2, and cr (x) = [er, qhead, qrel] represents a context vector.
•	MeSSage aggregation: mV,： (X) = √Nt(V) Pv0,r Mhv0,r,vh (X) Where hv0, r, Vi ∈ EGt(X)
•	Node state attending function: HVh+,： 1(X) = ahV+1W HV,：, where ahV+1 is an attention score.
•	Node state update function: HV：1(x) = HV,：(x) + 6agnn(HV,：(x), MV,：(x), cV+1(χ)), where
chV+1(X) = [HVh+,： 1(X), qhead, qrel] also represents a context vector.
Query context is defined by its head and relation embeddings, i.e., qhead = ehead and qrel = erel .
We introduce a node state attending function to pass node representation information from IGNN
to AGNN weighted by a scalar attention score ahV+1 and projected by a learnable matrix W . We
initialize HV0,：(X) = HV,： for node V ∈ VG0(x), letting unseen nodes hold zero states.
Attention module. Attention over T steps is represented by a sequence of node probability dis-
tributions, denoted by ah (t = 1, 2 . . . , T). The initial distribution a0 is a one-hot vector with
a0 [Vhead] = 1. To spread attention, we need to compute transition matrices Th each step. Since
it is conditioned on both IGNN and AGNN, we capture two types of interaction between V 0 and V:
Hto ： (x)〜HV,：(x), and Htvo ： (x)〜Hv,：. The former favors visited nodes, while the latter is used
to attend to unseen neighboring nodes.
工tv，= SoftmaXv∈Nt(v0)( Er ai(HVo,：(x), Cr(X), Ht,：(x)) + α2(Hlυ0,：(x), Cr(X), Hv,：))
α1(∙) = MLP(HV0,：(x), Cr(X))TWIMLP(HV,：(x), Cr(x))	⑸
α2(∙) =MLP(Hv，,：(x), Cr(X))TW2MLP(Hv,：, Cr(X))
where W1 and W2 are two learnable matrices. Each MLP uses one single layer with the leakyReLu
activation. To reduce the complexity for computing T h , we use nodes V 0 ∈ V , which contains
Gh(X)
nodes with the k-largest attention scores at step t, and use nodes V sampled from V0’s neighbors
to compute attention transition for the next step. Due to the fact that nodes V0 result from the
top-k pruning, the loss of attention may occur to diminish the total amount. Therefore, we use a
renormalized version, ah+1 = T h ah /kT h ah k, to compute new attention scores. We use attention
scores at the final step as the probability to predict the tail node.
2In practice, we can use a smaller set of edges than EGt (x) to pass messages as discussed in Section 3.2
5
Published as a conference paper at ICLR 2020
Figure 3: Iterative attending-sampling-attending procedure balancing coverage and complexity.
3.2	Complexity reduction by iterative attending, sampling and attending
AGNN deals with each subgraph relying on input x and keeps a few selected nodes in VGt(x), called
visited nodes. Initially, VG0 (x) contains only one node vhead, and then VGt (x) is enlarged by adding
new nodes each step. When propagating messages, we can just consider the one-hop neighborhood
each step. However, the expansion goes so rapidly that it covers almost all nodes after a few steps.
The key to address the problem is to constrain the scope of nodes we can expand the boundary from,
i.e., the core nodes which determine where we can go next. We call it the attending-from horizon,
tt
Gt (x), selected according to attention scores at . Given this horizon, we may still need node sam-
t
pling over the neighborhood N (Gt(x)) in some cases where a hub node of extremely high degree
exists to cause an extremely large neighborhood. We introduce an attending-to horizon, denoted by
tt
Nb (Gt(x)), inside the sampling horizon, denoted by Nb (Gt(x)). The attention module runs within
the sampling horizon with smaller dimensions exchanged for sampling more neighbors for a larger
coverage. In one word, we face a trade-off between coverage and complexity, and our strategy is
to sample more but attend less plus using small dimensions to compute attention. We obtain the
attending-to horizon according to newly computed attention scores at+1. Then, message passing
------------------------------------
tt
iteration at step t in AGNN can be further constrained on edges between Gt(x) and N(Gt(x)), a
smaller set than EGt (x) . We illustrate this procedure in Figure 3.
4	Experiments
Datasets. We use six large KG datasets: FB15K, FB15K-237, WN18, WN18RR, NELL995, and
YAGO3-10. FB15K-237 (Toutanova & Chen, 2015) is sampled from FB15K (Bordes et al., 2013)
with redundant relations removed, and WN18RR (Dettmers et al., 2018) is a subset of WN18 (Bor-
des et al., 2013) removing triples that cause test leakage. Thus, they are both considered more
challenging. NELL995 (Xiong et al., 2017) has separate datasets for 12 query relations each corre-
sponding to a single-query-relation KBC task. YAGO3-10 (Mahdisoltani et al., 2014) contains the
largest KG with millions of edges. Their statistics are shown in Table 1. We find some statistical
differences between train and validation (or test). In a KG with all training triples as its edges, a
triple (head, rel, tail) is considered as a multi-edge triple if the KG contains other triples that also
connect head and tail ignoring the direction. We notice that FB15K-237 is a special case compared
to the others, as there are no edges in its KG directly linking any pair of head and tail in valida-
tion (or test). Therefore, when using training triples as queries to train our model, given a batch,
for FB15K-237, we cut off from the KG all triples connecting the head-tail pairs in the given batch,
ignoring relation types and edge directions, forcing the model to learn a composite reasoning pattern
rather than a single-hop pattern, and for the rest datasets, we only remove the triples of this batch
and their inverse from the KG to avoid information leakage before training on this batch. This can
be regarded as a hyperparameter tuning whether to force a multi-hop reasoning or not, leading to a
performance boost of about 2% in HITS@1 on FB15-237.
Experimental settings. We use the same data split protocol as in many papers (Dettmers et al.,
2018; Xiong et al., 2017; Das et al., 2018). We create a KG, a directed graph, consisting of all
train triples and their inverse added for each dataset except NELL995, since it already includes
reciprocal relations. Besides, every node in KGs has a self-loop edge to itself. We also add inverse
relations into the validation and test set to evaluate the two directions. For evaluation metrics, we use
HITS@1,3,10 and the mean reciprocal rank (MRR) in the filtered setting for FB15K-237, WN18RR,
6
Published as a conference paper at ICLR 2020
Table 1: Statistics of the six KG datasets. PME (tr) means the proportion of multi-edge triples in
train; PME (va) means the proportion of multi-edge triples in validation; AL (va) means the average
length of shortest paths connecting each head-tail pair in validation.
Dataset	#EntitieS	#RelS	#Train	#Valid	#Test	PME (tr)	PME (va)	AL (va)
FB15K	14,951	1,345	483,142	50,000	59,071	81.2%^^	80.6%	1.22
FB15K-237	14,541	237	272,115	17,535	20,466	38.0%	0%	2.25
WN18	40,943	18	141,442	5,000	5,000	93.1%	94.0%	1.18
WN18RR	40,943	11	86,835	3,034	3,134	34.5%	35.5%	2.84
NELL995	74,536	200	149,678	543	2,818	100%	31.1%	2.00
YAGO3-10	123,188	37	1,079,040	5,000	5,000	56.4%	56.0%	1.75
Table 2: Comparison results on the FB15K-237 and WN18RR datasets. Results of Q] are taken
from (Nguyen et al., 2018),$] from (DettmerS et al., 2018), [^] from (Shen et al., 2018), [♦] from
(Sun et al., 2018), [4] from (Das et al., 2018), and [z] from (Lacroix et al., 2018). Some collected
results only have a metric score while some including ours take the form of “mean (std)”.
Metric (%)	H@1	FB15K-237			WN18RR			
		H@3	H@10	MRR	H@1	H@3	H@10	MRR
TranSE Q]	-	-	46.5	29.4	-	-	50.1	22.6
DistMult [4]	15.5	26.3	41.9	~4AΛ	"39	44	49	43
DiStMult [8]	20.6 (.4)	31.8 (.2)	-	29.0 (.2)	38.4 (.4)	42.4 (.3)	-	41.3 (.3)
ComplEx [4]	15.8	27.5	42.8	~4AΠ	^7I	46	51	44
ComplEx [8]	20.8(.2)	32.6 (.5)	-	29.6 (.2)	38.5 (.3)	43.9 (.3)	-	42.2 (.2)
ConvE [4]	^37	35.6	50.1	"325	^40	44	52	43
ConvE [8]	23.3 (.4)	33.8 (.3)	-	30.8 (.2)	39.6 (.3)	44.7 (.2)	-	43.3 (.2)
RotatE [♦]	24.1	37.5	53.3	-33.8	42.8	49.2	57.1	47.6
ComplEx-N3[z]	-	-	56	^^7	-	-	57	48
NeuralLP [8]	18.2(.6)	27.2 (.3)	-	24.9 (.2)	37.2(.1)	43.4 (.1)	-	43.5 (.1)
MINERVA [8]	14.1(.2)	23.2 (.4)	-	20.5 (.3)	35.1(.1)	44.5 (.4)	-	40.9 (.1)
MINERVA [4]	-	-	45.6	-	41.3	45.6	51.3	-
M-Walk [8]	16.5 (.3)	24.3 (.2)	-	23.2(.2)	41.4(.1)	44.5 (.2)	-	43.7 (.1)
DPMPN	28.6 (.1)	40.3 (.1)	53.0 (.3)	36.9(.1)	44.4 (.4)	49.7 (.8)	55.8(.5)	48.2 (.5)
FB15K, WN18, and YAGO3-10, and use the mean average precision (MAP) for NELL995’s single-
query-relation KBC tasks. For NELL995, we follow the same evaluation procedure as in (Xiong
et al., 2017; Das et al., 2018; Shen et al., 2018), ranking the answer entities against the negative
examples given in their experiments. We run our experiments using a 12G-memory GPU, TITAN
X (Pascal), with Intel(R) Xeon(R) CPU E5-2670 v3 @ 2.30GHz. Our code is written in Python
based on TensorFlow 2.0 and NumPy 1.16 and can be found by the link3 below. We run three
times for each hyperparameter setting per dataset to report the means and standard deviations. See
hyperparameter details in the appendix.
Baselines. We compare our model against embedding-based approaches, including TransE (Bordes
et al., 2013), TransR (Lin et al., 2015b), DistMult (Yang et al., 2015), ConvE (Dettmers et al.,
2018), ComplE (Trouillon et al., 2016), HolE (Nickel et al., 2016), RotatE (Sun et al., 2018), and
ComplEx-N3 (Lacroix et al., 2018), and path-based approaches that use RL methods, including
DeepPath (Xiong et al., 2017), MINERVA (Das et al., 2018), and M-Walk (Shen et al., 2018), and
also that uses learned neural logic, NeuralLP (Yang et al., 2017).
Comparison results and analysis. We report comparison on FB15K-23 and WN18RR in Table 2.
Our model DPMPN significantly outperforms all the baselines in HITS@1,3 and MRR. Compared
to the best baseline, we only lose a few points in HITS@10 but gain a lot in HITS@1,3. We
speculate that it is the reasoning capability that helps DPMPN make a sharp prediction by exploiting
graph-structured composition locally and conditionally. When a target becomes too vague to predict,
reasoning may lose its advantage against embedding-based models. However, path-based baselines,
with a certain ability to do reasoning, perform worse than we expect. We argue that it might be
inappropriate to think of reasoning, a sequential decision process, equivalent to a sequence of nodes.
The average lengths of the shortest paths between heads and tails as shown in Table 1 suggests a very
short path, which makes the motivation of using a path almost useless. The reasoning pattern should
be modeled in the form of dynamical local graph-structured pattern with nodes densely connected
3https://github.com/anonymousauthor123/DPMPN
7
Published as a conference paper at ICLR 2020
(B) IGNN Component Analysis
42086420
55544444
(％) £0 US =w
(A) Convergence Analysis (eval on test)
50505050
Z 5 ZeiZ 5 Zei
55554444
{%) £0 US ew
=20
=50
=100
=200
=400
57.5
55.0
50.0
45.0
42.5
40.0
(D) Attending-to Horizon Analysis
52.5
47.5
W/o IGNN
匚二I With IGNN
(C) Sampling Horizon Analysis
Maχ-samplιng-per-node
Max-sampling-per-node
Max-sampling-per-node
Max-sampling-per-node
Max-sampling-per-node
=20
=50
=100
=200
=400
Maχ-attedιng-to-per-step
Maχ-atteding-to-per-step
Maχ-atteding-to-per-step
Maχ-atteding-to-per-step
Maχ-atteding-to-per-step
(E) Attending-from Horizon Analysis
H@1	H@3	H@10 MMR
42086420
55544444
(％) £0 US =w
(F) Searching Horizon Analysis
Jii



Figure 4:	Experimental analysis on WN18RR. (A) Convergence analysis: We pick six model snap-
shots during training and evaluate them on test. (B) IGNN component analysis: w/o IGNN uses zero
step to run message passing, while with IGNN uses two; (C)-(F) Sampling, attending-to, attending-
from and searching horizon analysis. The charts on FB15K-237 can be found in the appendix.
with each other to produce a decision collectively. We also run our model on FB15K, WN18,
and YAGO3-10, and the comparison results in the appendix show that DPMPN achieves a very
competitive position against the best state of the art. We summarize the comparison on NELL995's
tasks in the appendix. DPMPN performs the best on five tasks, also being competitive on the rest.
Convergence analysis. Our model converges very fast during training. We may use half of train-
ing queries to train model to generalize as shown in Figure 4(A). Compared to less expensive
embedding-based models, our model need to traverse a number of edges when training on one
input, consuming much time per batch, but it does not need to pass a second epoch, thus saving a lot
of training time. The reason may be that training queries also belong to the KG's edges and some
might be exploited to construct subgraphs during training on other queries.
Component analysis. Given the stacked GNN architecture, we want to examine how much each
GNN component contributes to the performance. Since IGNN is input-agnostic, we cannot rely
on its node representations only to predict a tail given an input query. However, AGNN is input-
dependent, which means it can be carried out to complete the task without taking underlying node
representations from IGNN. Therefore, we can arrange two sets of experiments: (1) AGNN + IGNN,
and (2) AGNN-only. In AGNN-only, we do not run message passing in IGNN to compute Hv,：
but instead use node embeddings as Hv,：, and then we run pruned message passing in AGNN as
usual. We want to be sure whether IGNN is actually useful. In this setting, we compare the first
set which runs IGNN for two steps against the second one which totally shuts IGNN down. The
results in Figure 4(B) (and Figure 7(B) in Appendix) show that IGNN brings an amount of gains in
each metric on WN18RR (and FB15K-23), indicating that representations computed by full-graph
message passing indeed help subgraph-based message passing.
Horizon analysis. The sampling, attending-to, attending-from and searching (i.e., propagation
steps) horizons determine how large area a subgraph can expand over. These factors affect com-
putation complexity as well as prediction performance. Intuitively, enlarging the exploring area by
sampling more, attending more, and searching longer, may increase the chance of hitting a target
to gain some performance. However, the experimental results in Figure 4(C)(D) show that it is not
always the case. In Figure 4(E), we can see that increasing the maximum number of attending-
8
Published as a conference paper at ICLR 2020
(A) Attention Flow Analysis (on entroy) (B) Attention Flow Analysis (topi's proportion) (C) Attention Flow Analysis (top3's proportion) (D) Attention Flow Analysis (top5's proportion)
—AthletePbiysFoiTeain
-∙- AthIetePIeysInLeague
—AthIeteHomeStedium
τ- AthletePlaysSport
-∙- TesinPiaysSpcrt
OrgHeadQuarteredInCity
—∙- Wtor⅛For
→- XiSQnBoEMLaeation
-∙- ⅛ιsoπLeadΛt5
Step
1.00
0.75
0.50
0.25
0.00
-0.25
-0.50
-0.75
-1.00
Aih Iete Ptays F⅛ ITea m
AthIetePieysInLeague
AthletcHomeSUidium
-∙- AthIetePieysSpart
-∙- Te∙ mMaysSpcrt
OrgHeadQuarteredInCity
—WtoricsFor
→- PeisanBomInUcatioi
-∙- fcιsoπLeadΛt5
—OrgHiivdPeisan
AgentaeiOngSToθɪŋ
IRMmPlawlnl
1.00
0.75
0.50
0.25
0.00
-0.25
-0.50
-0.75
-1.00
Aih Iete PIeysForTeem
AthletePiaysInlcague
AthletcHomeSUidium
AthletepIsysSport
-∙- Te∙ mMaysSpcrt
τ- OiQHeadQuarteivdInCity
—WtoricsFor
→- PeisanBomInLocatia
-∙- ⅛ιsoπLea<isO<s
.OigHiEdPeEon
—AgentBeIongsTaOis
IRMmPlawlnl
1.00
0.75
0.50
0.25
0.00
-0.25
-0.50
-0.75
-1.00
—AthIetePIeysForream
—∙- AthIetePiaysInLeague
—AthIeteHomeSUidium
—AthletepIsysSport
-*- TeamPiayjSpart
τ- OiQHeadQuarteivdlnCity
—WtorlQFor
→- ⅝ιsαπBαmlπLocatioπ
-∙- ⅛ιsαπLeadsO<s
T- OigHiredPeison
—∙- AgentBeIongsTaOis
IRMmPlawlnl ∙*crs
Step
Step
Step
0	1	2	3	4	0	1	2	3	4	0	1	2	3	4
Figure 5:	Analysis of attention flow on NELL995 tasks. (A) The average entropy of attention
distributions changing along steps for each single-query-relation KBC task. (B)(C)(D) The changing
of the proportion of attention concentrated at the top-1,3,5 nodes per step for each task.
Training Time for One Epoch (h)
B 6 4 2
)右昌 20 ,JOJ BE_16ejl
8 6 4 2
ɪ) quot⅛ auoJ aE 一I-mc-c-eH
B 6 4 2
wuoʤ。UoJoJ BE-X 6ejl
(E) Time Cost on Batch Sizes
B 6 4 2
)右昌 20 ,JOJ BE_16ejl
Figure 6: Analysis of time cost on WN18RR: (A)-(D) measure the one-epoch training time on dif-
ferent horizon settings corresponding to Figure 4(C)-(F); (E) measures on different batch sizes us-
ing horizon setting Max-sampling-per-node=20, Max-attending-to-per-step=20, Max-attending-from-per-
step=20, and #Steps-in-AGNN=8. The charts on FB15K-237 can be found in the appendix.
from nodes per step is useful. That also explains why we call nodes in the attending-from horizon
the core nodes, as they determine where subgraphs can be expanded and how attention will be
propagated to affect the final probability distribution on the tail prediction. However, GPUs with
a limited memory do not allow for a too large number of sampled or attended nodes especially for
Max-attending-from-per-step. The detailed explanations can be found in attention strategies in Section
2 where the upper bound is controlled by N1 N2 and N3 (Max-attending-from-per-step corresponding
to N1, Max-sampling-per-node to N2, and Max-attending-to-per-step to N3). In N1N2, Section 3.2 sug-
gests that we should sample more by a large N2 but attend less by a small N1. Figure 4(F) suggests
that the propagation steps of AGNN should not go below four.
Attention flow analysis. If the flow-style attention really captures the way we reason about the
world, its process should be conducted in a diverging-converging thinking pattern. Intuitively, first,
for the diverging thinking phase, we search and collect ideas as much as we can; then, for the
converging thinking phase, we try to concentrate our thoughts on one point. To check whether the
attention flow has such a pattern, we measure the average entropy of attention distributions changing
along steps and also the proportion of attention concentrated at the top-1,3,5 nodes. As we expect,
attention is more focused at the final step and the beginning.
Time cost analysis. The time cost is affected not only by the scale ofa dataset but also by the horizon
setting. For each dataset, we list the training time for one epoch corresponding to our standard
hyperparameter settings in the appendix. Note that there is always a trade-off between complexity
and performance. We thus study whether we can reduce time cost a lot at the price of sacrificing a
little performance. We plot the one-epoch training time in Figure 6(A)-(D), using the same settings
as we do in the horizon analysis. We can see that Max-attending-from-per-step and #Steps-in-AGNN
affect the training time significantly while Max-sampling-per-node and Max-attending-to-per-step affect
very slightly. Therefore, we can use smaller Max-sampling-per-node and Max-attending-to-per-step in
order to gain a larger batch size, making the computation more efficiency as shown in Figure 6(E).
Visualization. To further demonstrate the reasoning capability, we show visualization results of
some pruned subgraphs on NELL995’s test data for 12 separate tasks. We avoid using the training
data in order to show generalization of the learned reasoning capability. We show the visualization
results in Figure 1. See the appendix for detailed analysis and more visualization results.
9
Published as a conference paper at ICLR 2020
Discussion of the limitation. Although DPMPN shows a promising way to harness the scalability
on large-scale graph data, current GPU-based machine learning platforms, such as TensorFlow and
PyTorch, seem not ready to fully leverage sparse tensor computation which acts as building blocks
to support dynamical computation graphs which varies from one input to another. Extra overhead
caused by extensive sparse operations will neutralize the benefits of exploiting sparsity.
5	Related Work
Knowledge graph reasoning. Early work, including TransE (Bordes et al., 2013) and its analogues
(Wang et al., 2014; Lin et al., 2015b; Ji et al., 2015), DistMult (Yang et al., 2015), ConvE (Dettmers
et al., 2018) and ComplEx (Trouillon et al., 2016), focuses on learning embeddings of entities and
relations. Some recent works of this line (Sun et al., 2018; Lacroix et al., 2018) achieve high accu-
racy. Another line aims to learn inference paths (Lao et al., 2011; Guu et al., 2015; Lin et al., 2015a;
Toutanova et al., 2016; Chen et al., 2018; Lin et al., 2018) for knowledge graph reasoning, especially
DeepPath (Xiong et al., 2017), MINERVA (Das et al., 2018), and M-Walk (Shen et al., 2018), which
use RL to learn multi-hop relational paths. However, these approaches, based on policy gradients
or Monte Carlo tree search, often suffer from low sample efficiency and sparse rewards, requiring
a large number of rollouts and sophisticated reward function design. Other efforts include learning
soft logical rules (Cohen, 2016; Yang et al., 2017) or compostional programs (Liang et al., 2016).
Relational reasoning in Graph Neural Networks. Relational reasoning is regarded as the key for
combinatorial generalization, taking the form of entity- and relation-centric organization to reason
about the composition structure of the world (Craik, 1952; Lake et al., 2017). A multitude of recent
implementations (Battaglia et al., 2018) encode relational inductive biases into neural networks to
exploit graph-structured representation, including graph convolution networks (GCNs) (Bruna et al.,
2014; Henaff et al., 2015; Duvenaud et al., 2015; Kearnes et al., 2016; Defferrard et al., 2016; Niepert
et al., 2016; Kipf & Welling, 2017; Bronstein et al., 2017) and graph neural networks (Scarselli et al.,
2009; Li et al., 2016; Santoro et al., 2017; Battaglia et al., 2016; Gilmer et al., 2017). Variants of
GNN architectures have been developed. Relation networks (Santoro et al., 2017) use a simple
but effective neural module to model relational reasoning, and its recurrent versions (Santoro et al.,
2018; Palm et al., 2018) do multi-step relational inference for long periods; Interaction networks
(Battaglia et al., 2016) provide a general-purpose learnable physics engine, and two of its variants are
visual interaction networks (Watters et al., 2017) and vertex attention interaction networks (Hoshen,
2017); Message passing neural networks (Gilmer et al., 2017) unify various GCNs and GNNs into
a general message passing formalism by analogy to the one in graphical models.
Attention mechanism on graphs. Neighborhood attention operation can enhance GNNs’ repre-
sentation power (Velickovic et al., 2018; Hoshen, 2017; Wang et al., 2018; Kool, 2018). These
approaches often use multi-head self-attention to focus on specific interactions with neighbors when
aggregating messages, inspired by (Bahdanau et al., 2015; Lin et al., 2017; Vaswani et al., 2017).
Most graph-based attention mechanisms attend over neighborhood in a single-hop fashion, and
(Hoshen, 2017) claims that the multi-hop architecture does not help to model high-order interac-
tion in experiments. However, a flow-style design of attention in (Xu et al., 2018b) shows a way to
model long-range attention, stringing isolated attention operations by transition matrices.
6	Conclusion
We introduce Dynamically Pruned Message Passing Networks (DPMPN) and apply it to large-scale
knowledge graph reasoning tasks. We propose to learn an input-dependent local subgraph which
is progressively and selectively constructed to model a sequential reasoning process in knowledge
graphs. We use graphical attention expression, a flow-style attention mechanism, to guide and prune
the underlying message passing, making it scalable for large-scale graphs and also providing clear
graphical interpretations. We also take the inspiration from the consciousness prior to develop a
two-GNN framework to boost experimental performances.
10
Published as a conference paper at ICLR 2020
References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. CoRR, abs/1409.0473, 2015.
Peter W. Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, and Koray
Kavukcuoglu. Interaction networks for learning about objects, relations and physics. In NIPS,
2016.
Peter W. Battaglia, Jessica B. Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Flo-
res Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan
Faulkner, Caglar GUlCehre, Francis Song, Andrew J. Ballard, Justin Gilmer, George E. Dahl,
Ashish Vaswani, Kelsey R. Allen, Charles Nash, Victoria Langston, Chris Dyer, Nicolas Heess,
Daan Wierstra, Pushmeet Kohli, Matthew Botvinick, Oriol Vinyals, Yujia Li, and Razvan Pas-
canu. Relational inductive biases, deep learning, and graph networks. CoRR, abs/1806.01261,
2018.
Yoshua Bengio. The consciousness prior. CoRR, abs/1709.08568, 2017.
Antoine Bordes, Nicolas Usunier, Alberto Garcla-Duran, Jason Weston, and Oksana Yakhnenko.
Translating embeddings for modeling multi-relational data. In NIPS, 2013.
Michael M. Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. Geomet-
ric deep learning: Going beyond euclidean data. IEEE Signal Processing Magazine, 34:18-42,
2017.
Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally
connected networks on graphs. CoRR, abs/1312.6203, 2014.
Wenhu Chen, Wenhan Xiong, Xifeng Yan, and William Yang Wang. Variational knowledge graph
reasoning. In NAACL-HLT, 2018.
William W. Cohen. Tensorlog: A differentiable deductive database. CoRR, abs/1605.06523, 2016.
Kenneth H. Craik. The nature of explanation. 1952.
Rajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer, Luke Vilnis, Ishan Durugkar, Akshay Krish-
namurthy, Alexander J. Smola, and Andrew McCallum. Go for a walk and arrive at the answer:
Reasoning over paths in knowledge bases using reinforcement learning. CoRR, abs/1711.05851,
2018.
Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on
graphs with fast localized spectral filtering. In NIPS, 2016.
Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. Convolutional 2d
knowledge graph embeddings. In AAAI, 2018.
David K. Duvenaud, Dougal Maclaurin, Jorge Aguilera-Iparraguirre, Rafael Gomez-Bombarelli,
Timothy Hirzel, Alan Aspuru-Guzik, and Ryan P. Adams. Convolutional networks on graphs for
learning molecular fingerprints. In NIPS, 2015.
Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural
message passing for quantum chemistry. In ICML, 2017.
Kelvin Guu, John Miller, and Percy S. Liang. Traversing knowledge graphs in vector space. In
EMNLP, 2015.
Mikael Henaff, Joan Bruna, and Yann LeCun. Deep convolutional networks on graph-structured
data. CoRR, abs/1506.05163, 2015.
Yedid Hoshen. Vain: Attentional multi-agent predictive modeling. In NIPS, 2017.
Guoliang Ji, Shizhu He, Liheng Xu, Kang Liu, and Jian Zhao. Knowledge graph embedding via
dynamic mapping matrix. In ACL, 2015.
11
Published as a conference paper at ICLR 2020
Steven M. Kearnes, Kevin McCloskey, Marc Berndl, Vijay S. Pande, and Patrick Riley. Molecular
graph convolutions: moving beyond fingerprints. Journal of computer-aided molecular design,
30 8:595-608,2016.
Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. CoRR, abs/1609.02907, 2017.
Wouter Kool. Attention solves your tsp , approximately. 2018.
Timothee Lacroix, Nicolas Usunier, and Guillaume Obozinski. Canonical tensor decomposition for
knowledge base completion. In ICML, 2018.
Brenden M. Lake, Tomer D. Ullman, Joshua B. Tenenbaum, and Samuel J Gershman. Building
machines that learn and think like people. The Behavioral and brain sciences, 40:e253, 2017.
Ni Lao, Tom Michael Mitchell, and William W. Cohen. Random walk inference and learning in a
large scale knowledge base. In EMNLP, 2011.
Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard S. Zemel. Gated graph sequence neural
networks. CoRR, abs/1511.05493, 2016.
Chen Liang, Jonathan Berant, Quoc V. Le, Kenneth D. Forbus, and Ni Lao. Neural symbolic ma-
chines: Learning semantic parsers on freebase with weak supervision. In ACL, 2016.
Xi Victoria Lin, Richard Socher, and Caiming Xiong. Multi-hop knowledge graph reasoning with
reward shaping. In EMNLP, 2018.
Yankai Lin, Zhiyuan Liu, and Maosong Sun. Modeling relation paths for representation learning of
knowledge bases. In EMNLP, 2015a.
Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. Learning entity and relation
embeddings for knowledge graph completion. In AAAI, 2015b.
Zhouhan Lin, Minwei Feng, Clcero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and
Yoshua Bengio. A structured self-attentive sentence embedding. CoRR, abs/1703.03130, 2017.
Farzaneh Mahdisoltani, Joanna Asia Biega, and Fabian M. Suchanek. Yago3: A knowledge base
from multilingual wikipedias. In CIDR, 2014.
Dai Quoc Nguyen, Tu Dinh Nguyen, Dat Quoc Nguyen, and Dinh Q. Phung. A novel embedding
model for knowledge base completion based on convolutional neural network. In NAACL-HLT,
2018.
Maximilian Nickel, Lorenzo Rosasco, and Tomaso A. Poggio. Holographic embeddings of knowl-
edge graphs. In AAAI, 2016.
Mathias Niepert, Mohammed Hassan Ahmed, and Konstantin Kutzkov. Learning convolutional
neural networks for graphs. In ICML, 2016.
Rasmus Berg Palm, Ulrich Paquet, and Ole Winther. Recurrent relational networks. In NeurIPS,
2018.
Adam Santoro, David Raposo, David G. T. Barrett, Mateusz Malinowski, Razvan Pascanu, Peter W.
Battaglia, and Timothy P. Lillicrap. A simple neural network module for relational reasoning. In
NIPS, 2017.
Adam Santoro, Ryan Faulkner, David Raposo, Jack W. Rae, Mike Chrzanowski, TheOPhane Weber,
Daan Wierstra, Oriol Vinyals, Razvan Pascanu, and Timothy P. Lillicrap. Relational recurrent
neural networks. In NeurIPS, 2018.
Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini.
The graph neural network model. IEEE Transactions on Neural Networks, 20:61-80, 2009.
Yelong Shen, Jianshu Chen, Pu Huang, Yuqing Guo, and Jianfeng Gao. M-walk: Learning to walk
over graphs using monte carlo tree search. In NeurIPS, 2018.
12
Published as a conference paper at ICLR 2020
Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. Rotate: Knowledge graph embedding
by relational rotation in complex space. CoRR, abs/1902.10197, 2018.
Kristina Toutanova and Danqi Chen. Observed versus latent features for knowledge base and text
inference. In Proceedings of the 3rd Workshop on Continuous Vector Space Models and their
Compositionality, 2015.
Kristina Toutanova, Victoria Lin, Wen tau Yih, Hoifung Poon, and Chris Quirk. Compositional
learning of embeddings for relation paths in knowledge base and text. In ACL, 2016.
Theo Trouillon, Johannes WelbL Sebastian Riedel, Enc GaUssier, and GUillaUme Bouchard. Com-
plex embeddings for simple link prediction. In ICML, 2016.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
LUkasz Kaiser, and Illia PolosUkhin. Attention is all yoU need. In NIPS, 2017.
Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Alejandro Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. CoRR, abs/1710.10903, 2018.
William Wang. Knowledge graph reasoning: Recent advances, 2018.
Xiaolong Wang, Ross B. Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks.
2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7794-7803, 2018.
Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. Knowledge graph embedding by trans-
lating on hyperplanes. In AAAI, 2014.
Nicholas Watters, Daniel Zoran, TheoPhane Weber, Peter W. Battaglia, Razvan Pascanu, and Andrea
Tacchetti. Visual interaction networks: Learning a physics simulator from video. In NIPS, 2017.
Wenhan Xiong, Thien Hoang, and William Yang Wang. DeePPath: A reinforcement learning method
for knowledge graPh reasoning. In EMNLP, 2017.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How Powerful are graPh neural
networks? ArXiv, abs/1810.00826, 2018a.
Xiaoran Xu, SongPeng Zu, Chengliang Gao, Yuan Zhang, and Wei Feng. Modeling attention flow
on graPhs. CoRR, abs/1811.00497, 2018b.
Bishan Yang, Wen tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. Embedding entities and
relations for learning and inference in knowledge bases. CoRR, abs/1412.6575, 2015.
Fan Yang, Zhilin Yang, and William W. Cohen. Differentiable learning of logical rules for knowl-
edge base reasoning. In NIPS, 2017.
13
Published as a conference paper at ICLR 2020
Appendix
7	Proof
Proposition. Given a graph G (undirected or directed in both directions), we assume the probability
of the degree of an arbitrary node being less than or equal to d is larger than p, i.e., P (deg(v) ≤
d) > p, ∀v ∈ V. Considering a sequence of consecutively expanding subgraphs (G0, G1, . . . , GT),
starting with G0 = {v}, for all t ≥ 1, we can ensure
P |VGt| ≤
d(d - 1)t - 2
d - 2
d(d-1)t-1-2
> P	d-2
(6)
Proof. We consider the extreme case of greedy consecutive expansion, where Gt = Gt-1 ∪ ∆Gt =
Gt-1 ∪ ∂Gt-1, since if this case satisfies the inequality, any case of consecutive expansion can also
satisfy it. By definition, all the subgraphs Gt are a connected graph. Here, we use ∆V t to denote
V∆Gt for short. In the extreme case, we can ensure that the newly added nodes ∆V t at step t only
belong to the neighborhood of the last added nodes ∆V t-1. Since for t ≥ 2 each node in ∆V t-1
already has at least one edge within Gt-1 due to the definition of connected graphs, we can have
P(∣∆V t∣ ≤ ∣∆V t-1∣(d - 1)) >piδv t-11.
Fort = 1,we have P(∣∆V11 ≤ d) > p and thus
P(∣⅜ι| ≤ 1 + d) >P.
For t ≥ 2, based on ∣Vct | = 1 + ∣∆V 1∣ + ... + ∣∆Vt|, we obtain
P(∣Vct | ≤ 1 + d + d(d - 1) + ... + d(d - 1)t-1) >p1+d+WdT)+…+d(dT)t-2
which is
(7)
(8)
(9)
P |VGt| ≤
d(d- 1)t - 2
d-2
d(d-1)t-1-2
)>p	d-2
(10)
We can find that t = 1 also satisfies this inequality.
□
14
Published as a conference paper at ICLR 2020
8 Hyperparameter Settings
Table 3: Our standard hyperparameter settings we use for each dataset plus their one-epoch training
time. For experimental analysis, we only adjust one hyperparameter and keep the remaining fixed
as the standard setting. For NELL995, the one-epoch training time means the average time cost of
the 12 single-query-relation tasks.
Hyperparameter	FB15K-237	FB15K	WN18RR	WN18	YAGO3-10	NELL995
batchsize	80	^^80^^	100	100	100	10
n_dims_att	50	50	50	50	50	200
n_dims	100	100	100	100	100	200
maxsampling_per_step (in IGNN)	-10000~~	10000	10000	10000	10000	10000
max attending 木om-per step	20	-^20^^	20	20	20	100
maxsampling_perJnode (in AGNN)	200	200	200	200	200	1000
maxqttending Jo-Per-SteP	200	200	200	200	200	1000
n steps _in」GNN	2	1	2	1	1	1
n_steps _in-AGNN	6	6	8	8	6	5
learning-rate	-0.001	0.001	0.001	0.001	0.0001	0.001
optimizer	Adam	Adam	Adam	Adam	Adam	Adam
grad_clipnorm	1	1	1	1	1	1
n_epochs	1	1	1	1	1	3
One-epoch training time (h)	25.7	63.7	4.3	8.5	185.0	0.12
The hyperparameters can be categorized into three groups:
•	Normal hyperparameters, including batchsize, n_dims_att, ndims, learning_rate, grad_clipnorm, and
nepochs. We Set smaller dimensions, n_dims_att, for computation in the attention module, as it
uses more edges than the message passing uses in AGNN, and also intuitively, it does not need to
propagate high-dimensional messages but only compute scalar scores over a sampled neighbor-
hood, in concert with the idea in the key-value mechanism (Bengio, 2017). We set nepochs = 1
in most cases, indicating that our model can be trained well by one epoch only due to its fast
convergence.
•	The hyperparameters in charge of the sampling-attending horizon, including
maxdampling_PeJSteP that controls the maximum number to sample edges per step in IGNN, and
maxsamPling-PerJWde, max attending 木om卫er step and max attending _to _per_step that control the
maximum number to sample neighbors of each selected node per step per input, the maximum
number of selected nodes for attending-from per step per input, and the maximum number of
selected nodes in a sampled neighborhood for attending-to per step per input in AGNN.
•	The hyperparameters in charge of the searching horizon, including n_steps_inJGNN representing
the number of propagation steps to run standard message passing in IGNN, and n_StePSjnAGNN
representing the number of propagation steps to run pruned message passing in AGNN.
Note that we tune these hyperparameters according to not only their performances but also the
computation resources available to us. In some cases, to deal with a very large knowledge graph with
limited resources, we need to make a trade-off between efficiency and effectiveness. For example,
each of NELL995’s single-query-relation tasks has a small training set, though still with a large
graph, so we can reduce the batch size in favor of affording larger dimensions and a larger sampling-
attending horizon without any concern for waiting too long to finish one epoch.
15
Published as a conference paper at ICLR 2020
9 More Experimental Results
Table 4: Comparison results on the FB15K and WN18 datasets. Results of [4] are taken from
(Nickel et al., 2016),[4]from (Dettmers et al., 2018), [◊] from (SUn et al., 2018), [8] from (Yang
et al., 2017), and [z] from (Lacroix et al., 2018). Our results take the form of”mean (std)”.
Metric (%)	H@1	FB15K		MRR	WN18			
		H@3	H@10		H@1	H@3	H@10	MRR
TranSE Q]	29.7	57.8	74.9	46.3	11.3	88.8	94.3	49.5
HolE Q]	-402^^	61.3	73.9	-^52.4	-93.0^^	94.5	94.9	93.8
DiStMUlt [⅜	54.6^^	73.3	82.4	^^65.4	-728^^	91.4	93.6	82.2
ComplEx 阳	59.9^^	75.9	84.0	^^69.2	-936^^	93.6	94.7	94.1
ConvE [4]	55.8^^	72.3	83.1	^^65.7	-935^^	94.6	95.6	94.3
RotatE [♦]	74.6	83.0	88.4	-^797-	94.4	95.2	95.9	94.9
ComplEx-N3 [z]	-	-	91	-^86-	-	-	96	95
NeUralLP [8]	-	-	83.7	76-	-	-	94.5	94
DPMPN	72.6 (.4)	78.4 (.4)	83.4 (.5)	76.4 (.4)	91.6(.8)	93.6(.4)	94.9 (.4)	92.8(.6)
Table 5: Comparison results on the YAGO3-10 dataset. Results of [4] are taken from (Dettmers
et al., 2018),"] from (Lacroix et al., 2018), and [Z] from (Lacroix et al., 2018).
Metric (%)	YAGO3-10			
	H@1	H@3	H@10	MRR
DiStMUlt Qi	24	38	54	34
ComplEx [匐	26	40	55	36
COnvE [制	35	49	62	44
ComplEx-N3 [z]	-	-	71	58
DPMPN	48.4	59.5	67.9	55.3
Table 6: Comparison results of MAP scores (%) on NELL995’s single-query-relation KBC tasks.
We take our baselines’ results from (Shen et al., 2018). No reports found on the last two in the paper.
Tasks	NeuCFlow	M-Walk	MINERVA	DeepPath	TransE	TransR
AthletePlaysForTeam	83.9 (0.5)	84.7 (1.3)	82.7 (0.8)	72.1 (1.2)	62.7	67.3
AthletePlaysInLeagUe	97.5 (0.1)	97.8 (0.2)	95.2 (0.8)	92.7 (5.3)	77.3	91.2
AthleteHomeStadiUm	93.6 (0.1)	91.9 (0.1)	92.8 (0.1)	84.6 (0.8)	71.8	72.2
AthletePlaysSport	98.6 (0.0)	98.3 (0.1)	98.6 (0.1)	91.7 (4.1)	87.6	96.3
TeamPlayssport	90.4 (0.4)	88.4(1.8)	87.5 (0.5)	69.6 (6.7)	76.1	81.4
OrgHeadQUarteredInCity	94.7 (0.3)	95.0 (0.7)	94.5 (0.3)	79.0 (0.0)	62.0	65.7
WorksFor	86.8 (0.0)	84.2 (0.6)	82.7 (0.5)	69.9 (0.3)	67.7	69.2
PersonBornInLocation	84.1 (0.5)	81.2 (0.0)	78.2 (0.0)	75.5 (0.5)	71.2	81.2
PersonLeadsOrg	88.4 (0.1)	88.8 (0.5)	83.0 (2.6)	79.0 (1.0)	75.1	77.2
OrgHiredPerson	84.7 (0.8)	88.8 (0.6)	87.0 (0.3)	73.8 (1.9)	71.9	73.7
AgentBelongsToOrg	89.3 (1.2)	-	-	-	-	-
TeamPlaysInLeague	97.2 (0.3)	-	-	-	-	-
16
Published as a conference paper at ICLR 2020
65
60
55
g50
ω
o 45
S
,y 40
1 35
30
25
20
45.0
42.5
40.0
≡37.5
φ
⅛35.0
整.5
Σ
30.0
27.5
25.0
(A) Convergence Analysis (eval on test)
0.5	1.0	1.5	2.0	2.5	3.0
Epoch
(D) Attending-to Horizon Analysis
5 0 5 0 5 0 5
5 5 4 4 3 3 2
(％) £0 US =w
Max-atteding-to-per-step	=	20
I	I	Max-atteding-to-per-step	=	50
I	I	Max-atteding-to-per-step	=	100
I	I	Max-atteding-to-per-step	=	200
MMR
45.0
42.5
40.0
≡375
φ
§35.0
U
§32.5
乏
30.0
27.5
25.0
(B) IGNN Component Analysis
W/o IGNN
匚二I With IGNN
H@1	H@3	H@10 MMR
(E) Attending-from Horizon Analysis
MMR
—=)=8sUco VE-F πc-c-o>r
5 0 5 0 5 0 5
3 3 2 2 11
)8Uco VE-F πc-c-o>r
(C) Sampling Horizon Analysis
Q 5 Q 5 Q 5 Q 5 Q
5 ZciZ 5 ZciZ 5
444333322
(％) £。US 2w
(F) Searching Horizon Analysis
5 0 5 0 5 0 5
4 4 3 3 2 2 1
(％) £0 US =w
Figure 7: Experimental analysis on FB15K-237. (A) Convergence analysis: we pick six model
snapshots at time points of 0.3, 0.5, 0.7, 1, 2, and 3 epochs during training and evaluate them on test;
(B) IGNN component analysis: w/o IGNN uses zero step to run message passing, while with IGNN
uses two steps; (C)-(F) Sampling, attending-to, attending-from and searching horizon analysis.
Figure 8: Analysis of time cost on FB15K-237: (A)-(D) measure the one-epoch training time on dif-
ferent horizon settings corresponding to Figure 7(C)-(F); (E) measures on different batch sizes using
horizon setting Max-sampled-edges-per-node=20, Max-seen-nodes-per-step=20, Max-attended-nodes-per-
step=20, and #Steps-of-AGNN=6.
10	More Visualization Resutls
10.1	Case study on the AthletePlaysForTeam task
In the case shown in Figure 9, the query is (concept_PersOnnOrthamerica_michaelJurner,
concept:athleteplays-forteam, ?) and a true answer is conceptsportsteamfalcons. From Figure 9, We
can see our model learns that (concept_personnorthamericaJnichael-turner, concept:athletehOmestadium,
concept _stadiumoreventvenue _georgia_dome ) and (concept _stadiumoreventvenue_georgia_dome, con-
cept:teamhOmestadiumdnv, concept^sportsteamfalcons) are two important facts to support the an-
swer of concept.sportsteam falcons. Besides, other facts, SUch as (concept athlete joey _harrington,
concept:athletehOmestadium,	concept Ktadiumoreventvenue _georgia_dome ) and ( concept _athlete-
joey_harrington, concept:athletePlaysfOrteam, concept^sportsteamfalcons), provide a vivid example
that a person or an athlete with concept_stadiumOreVentVenue_georgia_dome as his or her home
stadium might play for the team concept_SPOrtsteamfaIcOns. We have such examples more than
one, like concept-athlete_roddy^white's and concept_athlete_quarterback_matt_ryan's. The entity con-
17
Published as a conference paper at ICLR 2020
ceptsportsleaguejnfl cannot help Us differentiate the true answer from other NFL teams, but it can
at least exclude those non-NFL teams. In a word, our subgraph-structured representation can well
capture the relational and compositional reasoning pattern.
Figure 9: AthletePlaysForTeam. The head is Concept-PersonnorthamericaMichael.turner, the query
relation is concept:athletePlaysforteam, and the tail is conceptsportsteam.falcons. The left is a full sub-
graph derived with max.attendingfromφer.step=20, and the right is a further pruned subgraph from
the left based on attention. The big yellow node represents the head, and the big red node represents
the tail. Color on the rest indicates attention scores over a T -step reasoning process, where grey
means less attention, yellow means more attention gained during early steps, and red means gaining
more attention when getting closer to the final step.
For the AthletePlaysForTeam task
Query : (ConcePt-PerSonnorthamerica_michael-turner , concept: athleteplaysforteam , ConcePt-SPortSteam-falcons)
Selected key edges :
concept_personnorthamerica_michael_turner , concept :agentbelongstoorganization , concept_sportsleague_nfl
concept_personnorthamerica_michael_turner , concept: athletehomestadium , concept_stadiumoreventvenue_georgia_dome
concept_sportsleague_nfl ,
ConCePt-SPortSIeague_nfl ,
ConCePt-SPortSIeague_nfl ,
ConCePt-SPortSIeague_nfl ,
ConCePt-SPortSIeague_nfl ,
ConCePt-SPortSIeague_nfl ,
ConCePt-Stadiumoreventvenue-georgia_dome ,
ConCePt-Stadiumoreventvenue-georgia_dome ,
ConCePt-Stadiumoreventvenue-georgia_dome ,
ConCePt-Stadiumoreventvenue-georgia_dome ,
ConCePt-Stadiumoreventvenue-georgia_dome ,
ConCePt-Stadiumoreventvenue-georgia_dome ,
concept: agentcompeteswithagent, concept_sportsleague_nfl
concept: agentComPeteSwithagentJnv , concept_sportsleague_nfl
concept :teamplaysinleague_inv , concept_sportsteam_sd_chargers
concept: leaguestadiums , concept_stadiumoreventvenue_georgia_dome
concept :teamplaysinleague_inv , concept_sportsteam_falcons
concept :agentbelongstoorganization_inv, concept_personnorthamerica_michael.turner
concept: leaguestadiums_inv , concept_sportsleague_nfl
concept: teamhomestadium_inv , concept_sportsteam.falcons
concept: athletehomestadium-
concept :athletehomestadium-
concept :athletehomestadium-
concept :athletehomestadium_
nv,
nv,
nv,
nv,
concept .athlete _joey_harrington
concept _athlete_roddy .white
concept_coach_deangelo_hall
concept _personnorthamerica_michael .turner
ConcePt-SPortSIeague_nfl , concept: SubPartoforganiZation_inv , concePt-SPortSteam-oakland_raiders
Concept.Sportsteam_sd,chargers ,
Concept.Sportsteam_sd,chargers ,
Concept.Sportsteam_sd,chargers ,
Concept.Sportsteam_sd,chargers ,
ConCePt-SPortSteam-Sd-ChargerS ,
ConCePt-SPortSteam-falcons ,
ConCePt-SPortSteam-falcons ,
Concept.SportsteamJalcons ,
Concept.SportsteamJalcons ,
Concept.SportsteamJalcons ,
Concept.SportsteamJalcons ,
Concept.SportsteamJalcons ,
concept: teamplaysinleague , concept_sportsleague_nfl
concept: teamplaysagainstteam , concept_sportsteam Jalcons
concept :teamplaysagainstteam_inv , concept_sportsteam.falcons
concept: teamplaysagainstteam , concept_sportsteam_oakland.raiders
concept :teamplaysagainstteam_inv , concept_sportsteam_oakland_raiders
concept: teamplaysinleague , concept_sportsleague_nfl
concept: teamplaysagainstteam , concept_sportsteam_sd_chargers
concept: teamplaysagainstteam_inv , concept_sportsteam_sd,chargers
concept :teamhomestadium , concept_stadiumoreventvenue_georgia_dome
concept: teamplaysagainstteam , concept_sportsteam_oakland.raiders
concept: teamplaysagainstteam_inv , concept_sportsteam_oakland.raiders
concept :athleteledsportsteam_inv , concept.athlete_joey_harrington
concept .athlete _joey_harrington , concept: athletehomestadium , ConCePt-Stadiumoreventvenue-georgia_dome
concept .athlete _joey_harrington , concept :athleteledsportsteam , concept _sportsteam Jalcons
concept .athlete _joey_harrington , concept: athleteplaysforteam , concept _sportsteam Jalcons
concept_athlete_roddy_white , concept: athletehomestadium , concept_stadiumoreventvenue_georgia_dome
concept_athlete_roddy_white , concept: athleteplaysforteam , concept_sportsteam_falcons
concept_coach_deangelo_hall , concept: athletehomestadium , concept_stadiumoreventvenue_georgia_dome
18
Published as a conference paper at ICLR 2020
ConcePt-Coach-deangelo_hall , concept: athleteplaysforteam , ConcePt-SPortSteam-oakland_raiders
ConcePt-SPortSIeague_nfl , concept :teamplaysinleague_inv , concePt-SPortSteam_new_york_giants
Concept-Sportsteam-Sd-Chargers , concePt:teamPIaySagainStteam_inv , concePt-SPortSteam_new_york_giantS
concePt-SPortSteam-falconS , concept: teamplaysagainstteam , concePt-SPortSteam_new_york_giants
concePt-SPortSteam-falcons , concept: teamplaysagainstteam_inv , concept_sportsteam_new_york_giants
concept_sportsteam_oakland.raiders ,
concept_sportsteam_oakland.raiders ,
concept_sportsteam_oakland.raiders ,
concept_sportsteam_oakland.raiders ,
concept_sportsteam_oakland.raiders ,
concept_sportsteam_oakland.raiders ,
concept_sportsteam_oakland.raiders ,
concept_sportsteam_new_york_giants ,
concept_sportsteam_new_york_giants ,
concept_sportsteam_new_york_giants ,
concept_sportsteam_new_york_giants ,
concept: teamplaysagainstteam_inv , concept_sportsteam_new_york_giants
concept :teamplaysagainstteam , concept_sportsteam_sd.chargers
concept: teamplaysagainstteam_inv , concept_sportsteam_sd.chargers
concept :teamplaysagainstteam , concept_sportsteam.falcons
concept: teamplaysagainstteam_inv , concept_sportsteam.falcons
concept: agentcompeteswithagent, concept_sportsteam_oakland.raiders
concept: agentcompeteswithagent_inv , concept_sportsteam_oakland.raiders
concept :teamplaysagainstteam , concept_sportsteam_sd.chargers
concept :teamplaysagainstteam , concept_sportsteam_falcons
concept: teamplaysagainstteam_inv , concept_sportsteam_falcons
concept :teamplaysagainstteam , concept_sportsteam_oakland_raiders
10.2	More results
Figure 10: AthletePlaysInLeague. The head is concept-personnorthamericamattJtreanor, the query
relation is eoneept:ɑthletePlaysinleague, and the tail is concept^sportsleague.mlb. The left is a full sub-
graph derived with max_attendingfromper_step=20, and the right is a further pruned subgraph from
the left based on attention. The big yellow node represents the head, and the big red node represents
the tail. Color on the rest indicates attention scores over a T -step reasoning process, where grey
means less attention, yellow means more attention gained during early steps, and red means gaining
more attention when getting closer to the final step.
For the AthletePlaysInLeague task
Query : (ConCePt-PerSonnorthameriCa_matt_treanor , concept: athleteplaysinleague , concept _sportsleague_mlb)
Selected key edges
ConCePt-PerSonnorthameriCa_matt_treanor , concept: athleteflyouttosportsteamposition , ConCePt-SPortSteamPoSition-Center
ConCePt.PerSonnorthameriCa_matt_treanor , ConCePt :athleteplayssport , ConCePt.SPort.baseball
Concept.Sportsteamposition.Center
Concept.Sportsteamposition.Center
Concept.Sportsteamposition.Center
ConCePt-SPortSteamPoSition-Center
concept: athleteflyouttosportsteamposition_inv
concept: athleteflyouttosportsteamposition_inv
concept: athleteflyouttosportsteamposition_inv
concept: athleteflyouttosportsteamposition_inv
ConCePt.SPort.baseball , concept:
ConCePt.SPort.baseball , concept:
ConCePt.SPort.baseball , concept:
ConCePt.SPort.baseball , concept:
concept _personus_orlando_hudson,
concept _personus_orlando_hudson,
concept _athlete_ben_hendrickson,
concept _athlete_ben_hendrickson,
athleteplayssport_inv
athleteplayssport_inv
athleteplayssport_inv
athleteplayssport_inv
concept _personus_orlando_hudson
concept _athlete_ben_hendrickson
ConCePt.CoaCh_j_j __hardy
concept _athlete_hunter_pence
ConCePt-PerSonus_o"ando_hudson
concept _athlete_ben_hendrickson
concept _coach_j_j__hardy
concept _athlete_hunter_pence
concept: athleteplaysinleague , concept _sportsleague_mlb
concept: athleteplayssport , ConCePt-SPort-baseball
concept:coachesinleague , ConCePt-SPortSIeagUe_mlb
concept: athleteplayssport , ConCePt-SPort-baseball
19
Published as a conference paper at ICLR 2020
concept_coach_j_j__hardy , concept:coachesinleague, concept_sportsleague_mlb
concept_coach_j_j__hardy , Conceptiathleteplaysinleague , concept_sportsleague_mlb
concept_coach_j_j__hardy , concept Iathleteplayssport , ConcePt_sport_baseball
concept_athlete_hunter_pence , concept Iathleteplaysinleague , ConCePt-SPortsleague_mlb
concept_athlete_hunter_pence , concept Iathleteplayssport , ConCePt_sport_baseball
ConCePt.sportsleague_mlb , ConCePt:coaChesinleagUe_inv , concept_athlete_ben_hendrickson
ConCePt.sportsleague_mlb , COnCePt: COaChesinleagUe_inv , COnCePt.Coach_j_j__hardy
concept_male_archie_manning
ge a-g ca H	。na I
OTlBF*-≠βd I iʃno rwvβ r*VegJEadO “ an *_如 α u m
ιoβ-sUtβs
冲 t_3p crt-hockey
t^t-5PS⅛⅛W‰jq⅛
OTl oBpk,⅛ke-∏ ew
OTIC<τ*-S⅛d∣ErwrtrtWEj∣NwLBrtarκ6j∞W1**>∙P

concept_sport_footba Il
concept_athlete_eli_manning
concept-ath∣eteJOeJmdley
ewɪ c*∣*-s⅛d ι⅛mo
OTiσBf*-cαatfιJ⅛ιπ-mcgrwM
laπ⅛-3⅛1uπι
OTiσBf*-atH*e-barry-b∞ds
OTic*∣*jwr3σιusJerwny-shockβy
OTIBEaIM∙-∙ært⅜ιe⅛ffl我
BrK^lt-PeEnJndI 5 dr
BnCTPUIMaq_Hal 匚藤耳
OTθBf*-atHe⅛JβwwισBja><or
ronwp
BEV>	d U ETB E tv«i 8_mca % JEIS« 5
CTICTfl t_5 FwWM rrγια q H	r*fl r
JB-
CT∏OTf*-spαrt3⅛am-CT⅛⅛
concept_sportsteam_new_york_giants	ConcepLstsdiumoreventvenue
co∏cept-sportste a g ue_nfl
BEVlldty-NlnXd m a
0>ncfFtjφw⅛team-pa⅛
CviBKjspartsIeawjn,
OTlBf<≠βd I ιmα rwve rtv« E«_p au I Jrrg 丽 _如 d u m
CTl 冲 t_a»i I 必」CIFC«s d I
OTlBF*-βwa r⅛rcφ hytq 所 ame r*_s * b_bqw1_M Il
nt.su pβr,bαwi
OTlCt.βtħ，一TLmanTng
ll^tβ-stedlwn
:ity_east_rutherford
concept-stedium∂
_giants_stadium
co∏cept-sta diumo reventvenuej
BnCCVIl如 d u mTBEtVVI ue」UCaSqL如 d u E
c∞ctp t_a trt—_■ sb F
BnCt_maIJarCħ le_man T n g
BnC5 C_ba	all

ConcepIJakeJleW
ConCePJstadiumoreventvenuejncafeeqliseum
co ∏cept-sportslea gue_nhl
Figure 11: AthleteHomeStadium. The head is concept_athlete_eli_manning, the query relation is
concept:athletehOmestadium, and the tail is conceptstadiumoreventvenue_giantsstadium. The left is a full
subgraph derived with max_attendingfom_PejsteP=20, and the right is a further pruned subgraph
from the left based on attention. The big yellow node represents the head, and the big red node
represents the tail. Color on the rest indicates attention scores over a T -step reasoning process,
where grey means less attention, yellow means more attention gained during early steps, and red
means gaining more attention when getting closer to the final step.
For the AthleteHomeStadium task
Query : (concept.athlete_eli_manning
Conceptiathletehomestadium , COnCePt-StadiUmOreventvenue_giants_stadium)
Selected key edges
concept_athlete_e
concept_athlete_e
concept_athlete_e
concept_athlete_e
concept_athlete_e
.manning , concept
.manning , concept
.manning , concept
.manning , concept
.manning , concept
Personbelongstoorganization , concept_sportsteam_new_york_giants
athleteplaysforteam , COnCePt-SPOrtSteam.new_york_giants
athleteledsportsteam , concept_sportsteam_new_york_giants
athleteplaysinleague , COnCePt_sportsleague_nfl
fatherOfPerSOnJnv , concept_male_archie.manning
concept_sportsteam_new_york_giants ,
concept_sportsteam_new_york_giants ,
concept_sportsteam_new_york_giants ,
concept_sportsteam_new_york_giants ,
concept_sportsteam_new_york_giants ,
Conceptiteamplaysinleague , COnCePt.SPOrtSleagUe_nfl
concept:teamhomestadium , concept.Stadiumoreventvenue_giants_stadium
concept: PersonbelongstoorganizationJnv , concept .athlete _eli_manning
concept: athleteplaysforteam_inv , concept_athlete_eli_manning
concept: athleteledsportsteam_inv , concept .athlete _eli_manning
COnCePt.SPOrtSIeagUe_nfl
COnCePt.SPOrtSIeagUe_nfl
COnCePt.SPOrtSIeagUe_nfl
COnCePt_sportsleague_nfl
COnCePt.SPOrtSIeagUe_nfl
COnCePt :teamPlaySinIeagUeJnv , COnCePt.SPOrtSteam.new_york_giants
concept: agentcompeteswithagent , COnCePt.SPOrtSleagUe_nfl
COnCePt :agentCOmPeteSwithagent_inv , COnCePt-SPOrtSleague_nfl
concept: leaguestadiums , COnCePt-StadiUmOreventvenUe _giants_stadium
concept: athleteplaysinleague_inv , concept_athlete_eli manning
concept_male_archie_manning , concept: fatherofperson , concept_athlete_eli_manning
COnCePt.SPOrtSIeagUe_nfl , concept: leaguestadiums , COnCePt.StadiUmOreventvenUe_paul_brown.StadiUm
concept.Stadiumoreventvenue_giants_stadium , concept: teamhOmeStadium_inv , concept_sportsteam_new_york_giants
concept.Stadiumoreventvenue_giants_stadium , concept:leaguestadiums_inv , COnCePt.SPOrtSleagUe_nfl
concept.Stadiumoreventvenue_giants_stadium , concept: proxyfor_inv , COnCePt-City_east_rutherford
concept_city_east_rutherford , concept : proxyfor , Concept.Stadiumoreventvenue_giants_stadium
COnCePt.StadiUmOreventvenUe_paul_brown.StadiUm , concept:leaguestadiums_inv , COnCePt-SPOrtSleague_nfl
For the AthletePlaysSport task
20
Published as a conference paper at ICLR 2020
Figure 12: AthletePlaysSport. The head is concept-athletejvernonjwells, the query relation is con-
eept:athletePlayssPOrt, and the tail is concepgport_basebaU. The left is a full subgraph derived with
max-attending from^erstep=20, and the right is a further pruned subgraph from the left based on at-
tention. The big yellow node represents the head, and the big red node represents the tail. Color on
the rest indicates attention scores over a T -step reasoning process, where grey means less attention,
yellow means more attention gained during early steps, and red means gaining more attention when
getting closer to the final step.
Query: (concept .athlete _vernon_wells , concept: athleteplayssport , concept_sport_baseball)
Selected key edges
COnCePt-athlete _vernon_wells
COnCePt_athlete_vernon_wells
COnCePt-athlete _vernon_wells
ConCePt-athlete _vernon_wells
ConCePt-athlete _vernon_wells
ConCePt-athlete _vernon_wells
ConCePt_sportsleague_mlb
ConCePt.sportsleague_mlb
ConCePt.sportsleague_mlb
concept :athleteplaysinleague , ConCePt-SPortsleague_mlb
concept: Coachwontrophy , ConCePt_awardtroPhytoUrnament-world_series
ConCePt :agentCoHaborateswithagentJnv , ConCePt.sportsteam_blue_jays
ConCePt Ipersonbelongstoorganization , ConCePt.sportsteam_blue_jays
concept :athleteplaysforteam , ConCePt-sportsteam_blue_jays
concept :athleteledsportsteam , ConCePt.sportsteam_blue_jays
ConCePt:teamPgysinleagUeJnv
ConCePt:teamPgysinleagUeJnv
ConCePt:teamPgysinleagUeJnv
ConCePt.awardtroPhytoUrnament.world.series ,
ConCePt.awardtroPhytoUrnament.world.series ,
ConCePt_awardtroPhytoUrnament-world-series ,
ConCePt.sport_baseball
ConCePt.awardtroPhytoUrnament.World.series ,
concept _sportsteam .dodgers
concept _sportsteam_yankees
ConCePt.sportsteam-PittsbUrgh.Pirates
concept: teamwontrophy_inv , concept_sportsteam.dodgers
concept: teamwontrophy_inv , ConCePt.sportsteam_yankees
ConCept : awardtrophytoUrnamentistheChampionshipgameofthenationalsport
ConCePt.sportsteam_blue_jays
ConCePt_sportsteam_blue_jays
ConCePt.sportsteam_blue_jays
concept: teamwontrophy_inv , ConCePt.sportsteam-PittsbUrgh.Pirates
ConCePt.sportsteam_dodgers ,
ConCePt.sportsteam_dodgers ,
ConCePt.sportsteam_dodgers ,
ConCePt_sportsteam_dodgers ,
concept_sportsteam_yankees ,
concept_sportsteam_yankees ,
concept_sportsteam_yankees ,
concept_sportsteam_yankees ,
concept_sportsteam_yankees ,
concept_sportsteam_yankees ,
concept :teamplaysinleague , ConCePt.sportsIeagUe_mlb
concept :teamplaysagainstteam , ConCePt_sportsteam_yankees
concept :teamplayssport , ConCePt.sport_baseball
concept:teamplaysagainstteam , concept_sportsteam_yankees
ConCePt:teamPlaysagainstteam_inv, ConCePt.sportsteam_yankees
concept: teamwontrophy , ConCePt.awardtroPhytoUrnament.World.series
concept:teamplayssport , ConCePt_sport_baseball
concept:teamplaysagainstteam , ConCePt-sportsteam.dodgers
ConCePt :teamPlaysagainstteamJnv , concept_sportsteam.dodgers
concept: teamwontrophy , concept_awardtrophytournament_world.series
concept:teamplayssport , ConCePt.sport_baseball
concept:teamplaysagainstteam , concept_sportsteam_pittsburgh_pirates
ConCePt:teamPlaysagainstteam_inv, ConCePt.sportsteam.PittsbUrgh.Pirates
ConCePt.sport_baseball , ConCePt IteamplayssportJnv , concept_sportsteam.dodgers
ConCePt.sport_baseball , ConCePt IteamplayssportJnv , concept_sportsteam_yankees
ConCePt.sport_baseball , concept: awardtroPhytoUrnamentistheChamPionshiPgameofthenationaIsPort_inv ,
concept_awardtrophytournament_world .series
ConCePt.sport_baseball , ConCePt IteamplayssportJnv , ConCePt.sportsteam-PittsbUrgh.Pirates
concept_sportsteam_pittsburgh_pirates , concept :teamplaysagainstteam , concept_sportsteam_yankees
concept_sportsteam_pittsburgh_pirates , ConCePt :teamPlaysagainstteam_inv , concept_sportsteam_yankees
21
Published as a conference paper at ICLR 2020
Concept.Sportsteam.PittsbUrgh.Pirates , concept: teamwontrophy , concept_awardtrophytournament_world_series
Concept.Sportsteam.PittsbUrgh.Pirates , ConcePt:teamPIayssport, ConcePt.sport_baseball
Figure 13: TeamPlaysSport. The head is Conceptdportsteam_red^wings, the query relation is con-
cept:teamplayssport, and the tail is concept sport ±ockey. The left is a full subgraph derived with
max-attending from^erstep=20, and the right is a further pruned subgraph from the left based on
attention. The big yellow node represents the head, and the big red node represents the tail. Color
on the rest indicates attention scores over a T -step reasoning process, where grey means less atten-
tion, yellow means more attention gained during early steps, and red means gaining more attention
when getting closer to the final step.
For the TeamPlaysSport task
Query : (concept_sportsteam_red_wings , ConCePt :teamPgyssPort , ConCePt.sport_hockey)
Selected key edges
ConCePt.sportsteam_red_wings ,
ConCePt.sportsteam_red_wings ,
ConCePt.sportsteam_red_wings ,
ConCePt.sportsteam_red_wings ,
ConCePt_sportsteam_red_wings ,
ConCePt.sportsteam_red_wings ,
ConCePt.sportsteam_red_wings ,
ConCePt.sportsteam_red_wings ,
ConCePt.sportsteam _montreal.Canadiens,
ConCePt.sportsteam _montreal.Canadiens,
ConCePt.sportsteam _montreal.Canadiens,
ConCePt.sportsteam _montreal.Canadiens,
ConCePt.sportsteam _montreal.Canadiens,
concept :teamplaysagainstteam , concept_sportsteam_montreal_canadiens
ConCePt :teamPIaysagainstteam_inv , ConCePt-sportsteam_montreal.Canadiens
concept :teamplaysagainstteam , ConCePt-sportsteam_blue_jackets
ConCePt :teamPIaysagainstteam_inv , ConCePt.sportsteam_blueJackets
concept :worksfor_inv , concept_athlete_lidstrom
concept :organizationhiredperson , ConCePt-athlete_lidstrom
concept :athleteplaysforteam_inv , ConCePt-athlete_lidstrom
ConCePt IathleteledsportsteamJnv , ConCePt.athlete_lidstrom
ConCePt_sportsteam_blue Jackets
concept_sportsteam_blue ,jackets
concept_sportsteam_blue ,jackets
concept :teamplaysagainstteam , ConCePt.sportsteam_red_wings
ConCePt :teamPIaysagainstteam_inv , concept_sportsteam_red_wings
concept :teamplaysinleague , ConCePt-sportsIeagUe_nhl
concept :teamplaysagainstteam , ConCePt.sportsteam_leafs
ConCePt :teamPIaysagainstteam_inv , ConCePt-sportsteam_leafs
concept _athlete_lidstrom
concept _athlete_lidstrom
concept _athlete_lidstrom
concept _athlete_lidstrom
concept:teamplaysagainstteam , ConCePt_sportsteam_red_wings
ConCePt :teamPgysagainstteam_inv , ConCePt.sportsteam_red_wings
ConCePt :teamPIaysinIeagUe , ConCePt.sportsIeagUe_nhl
ConCePt.sportsteam_red_wings ,
ConCePt.sportsteam_red_wings ,
ConCePt.sportsteam_red_wings ,
concept: worksfor , ConCePt.sportsteam_red_wings
concept: organizationhiredperson_inv, ConCePt-sportsteam _red_wings
ConCePt :athleteplaysforteam , ConCePt_sportsteam_red_wings
ConCePt :athleteledsportsteam , ConCePt-sportsteam_red_wings
ConCePt_sportsIeagUe_nhl ,
ConCePt.sportsIeagUe.nhl ,
ConCePt.sportsIeagUe.nhl ,
ConCePt.sportsteam_leafs ,
ConCePt.sportsteam_leafs ,
concept :teamplaysinleague , ConCePt.sportsIeagUe_nhl
concept :teamplaysagainstteam , ConCePt.sportsteam_leafs
ConCePt :teamPIaysagainstteam_inv , ConCePt.sportsteam_leafs
concept: agentcompeteswithagent , ConCePt_sportsIeagUe_nhl
ConCePt :agentComPeteswithagent_inv , ConCePt.sportsIeagUe_nhl
ConCePt :teamPgysinIeagUeJnv , ConCePt.sportsteam_leafs
concept:teamplaysinleague , ConCePt.sportsIeagUe_nhl
concept:teamplayssport , concept_sport_hockey
22
Published as a conference paper at ICLR 2020
.	.... concept recordlabel dreamworks Skg
co nce pt_ceo_george_boden heι me r	κ -	-	- s
concept_ceqJeffrey_katzenberg
concept_website_network
concept_biotechcompany_the_walt_disney_co_
n_burbank_glendale_pasadena
concept_ceo_robert_iger	concept_city_ne
Figure 14: OrganizationHeadQuarteredInCity. The head is ConcePt.companyDisney, the query
relation is Conceptiorganizationheadquarteredincity, and the tail is conceptJcityJburbank. The left is a
full subgraph derived with max_attending fom_perdtep=20, and the right is a further pruned subgraph
from the left based on attention. The big yellow node represents the head, and the big red node
represents the tail. Color on the rest indicates attention scores over a T -step reasoning process,
where grey means less attention, yellow means more attention gained during early steps, and red
means gaining more attention when getting closer to the final step.
For the OrganizationHeadQuarteredInCity task
Query : (concept .company _disney ,
Conceptiorganizationheadquarteredincity , concept_city_burbank)
Selected key edges :
concept .company _disney ,
concept .company _disney ,
concept .company _disney ,
concept .company _disney ,
concept .company _disney ,
concept .company _disney ,
concept .company _disney ,
concept .company _disney ,
concept .company _disney ,
concept: headquarteredin , concept_city_burbank
concept: SubpartoforganizationJnv , concept_website_network
concept: worksfor_inv , concept _ceo_robert_iger
concept: proxyfor_inv , concept_ceo_robert_iger
concept: personleadsorganization_inv , COnCePt_Ceo_robert_iger
concept: ceoof_inv , concept_ceo_robert_iger
concept: PersOnleadsOrganiZatiOnJnv , COnCePt_Ceo_jeffrey_katzenberg
COnCePt:organiZatiOnhiredPerson, concept_ceo_jeffrey_katzenberg
COnCePt:organiZatiOnterminatedPerson, concept_ceo_jeffrey_katzenberg
concept _city_burbank , COnCePt:headquarteredin_inv , concept .company _disney
concept_city_burbank , COnCePt :headquarteredin_inv , concept_biotechcompany_the_walt_disney_co_
:subpartoforganization , concept .company _disney
:worksfor , concept .company _disney
:proxyfor , concept .company _disney
:personleadsorganization , concept .company _disney
:ceoof , concept .company _disney
:topmemberoforganization , concept_biotechcompany_the_walt_disney_co.
:organizationterminatedperson_inv, concept_biotechcompany_the_walt_disney_co.
concept : Personleadsorganization , concept .company _disney
COnCeP"OrganizationhiredpersonJnv , concept .company _disney
COnCeP"OrganizationterminatedpersonJnv , concept .company _disney
concept: worksfor , COnCePt.recordlabel_dreamworks_skg
concept :topmemberoforganization , concept_recordlabel_dreamworks_skg
COnCeP"OrganizationterminatedpersonJnv , concept_recordlabel_dreamworks_skg
concept : ceoof , concept.recordlabel_dreamworks_skg
COnCePt.Website.network , concept
concept_ceo_robert_iger , concept
concept_ceo_robert_iger , concept
concept_ceo_robert_iger , concept
concept_ceo_robert_iger , concept
concept_ceo_robert_iger , concept
concept_ceo_robert_iger , concept
concept_ceo_jeffrey_katzenberg ,
concept_ceo_jeffrey_katzenberg ,
concept_ceo_jeffrey_katzenberg ,
concept_ceo_jeffrey_katzenberg ,
concept_ceo_jeffrey_katzenberg ,
concept_ceo_jeffrey_katzenberg ,
concept_ceo_jeffrey_katzenberg ,
COnCePt.biotechcompany _the_walt_disney_co_,
COnCePt.biotechcompany _the_walt_disney_co_,
concept_recordlabel_dreamworks_skg , concept
concept_recordlabel_dreamworks_skg , concept
concept_recordlabel_dreamworks_skg , concept
concept_recordlabel_dreamworks_skg , concept
concept: headquarteredin , concept_city_burbank
concept:organizationheadquarteredincity , concept_city_burbank
WorksforJnv , concept_ceo_jeffrey_katzenberg
topmemberoforganizationJnv , COnCePt_Ceo_jeffrey_katzenberg
Organizationterminatedperson , concept_ceo_jeffrey_katzenberg
CeoofJnv , concept_ceo_jeffrey_katzenberg
concept_city_burbank , concept: airportincity_inv , concept_transportation_burbank_glendale_pasadena
concept .transportation _burbank_glendale_pasadena, concept: airportincity , concept _city_burbank
23
Published as a conference paper at ICLR 2020
concept_retailstore_microsoft
concept_personus_steve_ba Ilmer
ConCePt-PerSona UStra lia_paul_allen
concept_company_adobe
ConCeP t_personmexicq
co∏cept-person robbie bach
ConCePJpolitkia n Jobs
CePt-Companyjnicrosoft
_SdentiSt_b a Imer
conceptjj(
^-microsoft
concept,sportstea m_StatejJ niversity
conce pt-perso n_bill
conceptusPOrtSteamJIa rva rd-divin ity-scho
Figure 15: WorksFor. The head is Concept_Scientist_balmer, the query relation is con-
cept:Worksfor, and the tail is concept .university Microsoft. The left is a full subgraph derived with
max-attending from^erstep=20, and the right is a further pruned subgraph from the left based on at-
tention. The big yellow node represents the head, and the big red node represents the tail. Color on
the rest indicates attention scores over a T -step reasoning process, where grey means less attention,
yellow means more attention gained during early steps, and red means gaining more attention when
getting closer to the final step.
For the WorksFor task
QUery : (concept-Scientist_balmer, concept: worksfor , concept-University_microsoft)
Selected key edges
ConCePt.scientist_balmer , ConCePt:topmemberoforganiZation , ConCePt.ComPany-microsoft
ConCePt.scientist_balmer , ConCePt:organiZationterminatedPersonJnv , ConCePt.University-microsoft
ConCePt-ComPany_microsoft , ConCePt ItopmemberoforganizationJnv , ConCePt-PersonUs-steve_ballmer
ConCePt.ComPany.microsoft , ConCePt ItopmemberoforganizationJnv , ConCePt.scientist_balmer
ConCePt.University.microsoft ,
ConCePt.University.microsoft ,
ConCePt.University.microsoft ,
ConCePt-University_microsoft ,
ConCePt.University.microsoft ,
ConCePt.PersonUs.steve_ballmer ,
ConCePt.PersonUs.steve_ballmer ,
ConCePt.PersonUs.steve_ballmer ,
ConCePt-PersonUs-steve_ballmer ,
ConCePt.PersonUs.steve_ballmer ,
ConCePt.PersonUs.steve_ballmer ,
ConCePt.PersonUs.steve_ballmer ,
concept :agentcollaborateswithagent , ConCePt.PersonUs-steve_ballmer
ConCePt :personIeadsorganiZation_inv , ConCePt.PersonUs.steve_ballmer
ConCePt :personIeadsorganiZation_inv , ConCePt.Person_bill
concept:organizationterminatedperson , ConCePt_scientist_balmer
ConCePt :personIeadsorganiZation_inv , ConCePt.Person.robbie_bach
concept :topmemberoforganization , ConCePt.ComPany.microsoft
ConCePt IagentcollaborateswithagentJnv , ConCePt-University.microsoft
concept :personleadsorganization , ConCePt-University.microsoft
concept:worksfor , ConCePt-University_microsoft
concept: proxyfor , ConCePt.retailstore_microsoft
concept: subpartof , ConCePt.retailstore_microsoft
concept :agentcontrols , ConCePt-retailstore_microsoft
ConCePt.Person_bill , concept:personleadsorganization , ConCePt.University-microsoft
ConCePt.Person_bill , concept: worksfor , ConCePt.University.microsoft
concept_person_robbie_bach , concept:personleadsorganization , ConCePt.University-microsoft
concept_person_robbie_bach , concept: worksfor , ConCePt.University.microsoft
ConCePt.retailstore_microsoft , concept
ConCePt-retailstore_microsoft , concept
ConCePt.retailstore_microsoft , concept
For the PersonBornInLocation task
Query : (concept .person _mark001 ,
SeleCted key edges :
concept .person _mark001
concept .person _mark001
concept .person _mark001
concept .person _mark001
concept .person _mark001
ProXyfor_inv , ConCePt.PersonUs.steve_ballmer
subpartofjnv , ConCePt_PersonUs_steve_ballmer
agentControls_inv , ConCePt-PersonUs.steve_ballmer
concept :personborninlocation , ConCePt.CoUnty_york.City)
concept:persongraduatedfromuniversity, ConCePt-University.college
concept: Persongraduatedschool , ConCePt-University-College
concept:persongraduatedfromuniversity, concept_university_state_university
concept: Persongraduatedschool , concept_university_state_university
concept: personbornincity , concept_city_hampshire
24
Published as a conference paper at ICLR 2020
Figure 16: PersonBornInLocation. The head is ConcePt_personmark001, the query relation is con-
cept:Personborninlocation, and the tail is concept-county _york_city. The left is a full subgraph derived
with max.attending fromperstep=20, and the right is a further pruned subgraph from the left based on
attention. The big yellow node represents the head, and the big red node represents the tail. Color on
the rest indicates attention scores over a T -step reasoning process, where grey means less attention,
yellow means more attention gained during early steps, and red means gaining more attention when
getting closer to the final step.
concept-person_mark001 , concept: hasspouse , concept_person_diane0 01
concept-person_mark001 , ConCePt:hasspouse_inv, ConCePt-Person.diane001
ConCePt.University .college
ConCePt.University.college
ConCePt.University.college
ConCePt-University-College
ConCePtIpersongraduatedfromuniversityJnv, ConCePt.Person_mark001
concept: PersongraduatedschooLinv , concept .person _mark001
COnCePt :persOngradUatedfrOmUniversityJnv , COnCePt-Person_bill
COnCePt :persOngradUatedsChoolJnv , COnCePt-Person_bill
COnCePt.University.state_University,
COnCePt.University.state_University,
COnCePt.University.state_University,
COnCePt-University_state_university,
COnCePt:persOngradUatedfrOmUniversity_inv, concept .person _mark001
concept: PersongraduatedschooLinv , concept .person _mark001
COnCePt :persOngradUatedfrOmUniversityJnv , COnCePt.Person_bill
COnCePt :persOngradUatedsChoolJnv , COnCePt-Person_bill
concept_city_hampshire ,
concept .person _diane001
concept .person _diane001
concept .person _diane001
concept _person _diane001
concept .person _diane001
concept: personbornincity_inv , concept .person _mark001
concept:persongraduatedfromuniversity , concept_university_state_university
concept : persongraduatedschool , concept_university_state_university
concept : hasspouse , concept .person _mark001
concept :hasspouse_inv , COnCePt-Person_mark001
concept :personborninlocation , concept_county_york_city
concept_university_state_university , COnCePt:persOngradUatedfrOmUniversityJnv , concept .person _diane001
concept_university_state_university , COnCePt :persOngradUatedsChoolJnv , COnCePt-Person_diane001
COnCePt.Person_bil
COnCePt-Person_bil
COnCePt.Person_bil
COnCePt.Person_bil
COnCePt.Person_bil
COnCePt.Person_bil
concept_city_york ,
concept_city_york ,
,concept:personbornincity , concept_city_york
,concept:personborninlocation , concept_city_york
,concept:persongraduatedfromuniversity , concept_university.college
,concept:persongraduatedschool , concept_university_college
,concept:persongraduatedfromuniversity , concept_university_state_university
,concept:persongraduatedschool , concept_university_state_university
concept: personbornincity_inv , COnCePt-Person_bill
concept: personbornincity_inv , COnCePt-Person_diane001
Concept.University .college , COnCePtIpersongraduatedfromuniversityJnv , concept .person _diane001
concept .person _diane001 , concept :personbornincity , concept_city_york
For the PersonLeadsOrganization task
Query : (concept .journalist _bill_plante , concept:personleadsorganization , concept .company _cnn__pbs)
Selected key edges :
COnCePt-journalist_bill_plante , concept: worksfor , concept_televisionnetwork_cbs
COnCePt-journalist_bill_plante , COnCePt IagentcollaborateswithagentJnv , concept_televisionnetwork_cbs
COnCePt-televisionnetwork_cbs , concept: worksfor_inv , concept .journalist _walter_cronkite
25
Published as a conference paper at ICLR 2020
Figure 17: PersonLeadsOrganization. The head is ConceptjournalistJilLplante, the query relation
is concept:OrganizatiOnheadquarteredincity, and the tail is concept_company_CnnVbs. The left is a full
subgraph derived with max_attending from^erstep=20, and the right is a further pruned subgraph
from the left based on attention. The big yellow node represents the head, and the big red node
represents the tail. Color on the rest indicates attention scores over a T -step reasoning process,
where grey means less attention, yellow means more attention gained during early steps, and red
means gaining more attention when getting closer to the final step.
concept_televisionnetwork_cbs ,
concept_televisionnetwork_cbs ,
concept_televisionnetwork_cbs ,
concept_televisionnetwork_cbs ,
concept_televisionnetwork_cbs ,
concept_televisionnetwork_cbs ,
concept_televisionnetwork_cbs ,
concept: agentcollaborateswithagent, concept-journalist _walter_cronkite
concept: WorksforJnv , ConCePt-PersonUs-scott_pelley
ConCePt: WorksforJnv , concept_actor_daniel_schorr
ConCePt: WorksforJnv , concept .person _edward_r__murrow
concept: agentcollaborateswithagent , concept .person _edward_r__murrow
concept: worksfor_inv , concept _journalist_bill_plante
concept: agentcollaborateswithagent, COnCePt_journalist_bill_plante
concept .journalist _walter_cronkite , concept: worksfor , concept _televisionnetwork_cbs
concept .journalist _walter_cronkite , concept :agentcollaborateswithagent_inv , COnCePt-televisionnetwork _cbs
concept .journalist _walter_cronkite , concept: worksfor , concept_nonprofitorganization_cbs ,evening
COnCePt-PerSonus_scott_pelley , concept: worksfor , concept_televisionnetwork_cbs
COnCePt.Personus_scott_pelley , concept:personleadsorganization , COnCePt.televisionnetwork_cbs
COnCePt.Personus_scott_pelley , concept:personleadsorganization , concept .company _cnn__pbs
COnCePt-actor_daniel_schorr , concept: worksfor , concept_televisionnetwork_cbs
COnCePt-actor_daniel_schorr , concept: Personleadsorganization , COnCePt-televisionnetwork_cbs
COnCePt_actor_daniel_schorr , concept: Personleadsorganization , concept .company _cnn__pbs
COnCePt.Person_edward_r__murrow ,
COnCePt.Person_edward_r__murrow ,
COnCePt.Person_edward_r__murrow ,
COnCePt.Person_edward_r__murrow ,
concept: worksfor , concept_televisionnetwork_cbs
concept: agentcollaborateswithagent_inv, concept _televisionnetwork_cbs
concept:personleadsorganization , concept_televisionnetwork_cbs
concept: Personleadsorganization , concept .company _cnn__pbs
COnCePt_televisionnetwork_cbs
COnCePt-televisionnetwork_cbs
COnCePt-televisionnetwork_cbs
COnCePt-televisionnetwork_cbs
concept: Organizationheadquarteredincity , concept _city_new_york
concept: headquarteredin , concept_city_new_york
concept: agentcollaborateswithagent, COnCePt.PerSoneurope_william_paley
COnCePt:topmemberoforganization_inv, COnCePt.PerSoneurope_william_paley
COnCePt.COmPany_Cnn__pbs , concept: headquarteredin , concept_city_new_york
COnCePt.company_cnn__pbs , COnCePt IpersonbelongstoorganizationJnv , COnCePt-PerSoneurope_william_paley
COnCePt.nonprofitorganization _cbs_evening , concept: worksfor_inv , concept .journalist _walter_cronkite
concept_city_new_york , concept
concept_city_new_york , concept
concept_city_new_york , concept
COnCePt-PerSoneurope _william_paley
COnCePt-PerSoneurope _william_paley
COnCePt-PerSoneurope _william_paley
COnCePt-PerSoneurope _william_paley
OrganizationheadquarteredincityJnv, concept_televisionnetwork_cbs
headquarteredinJnv , concept_televisionnetwork_cbs
headquarteredin _inv , concept .company _cnn__pbs
concept: agentcollaborateswithagent_inv, COnCePt-televisionnetwork_cbs
concept:topmemberoforganization , COnCePt-televisionnetwork_cbs
concept: Personbelongstoorganization , concept .company _cnn__pbs
concept: Personleadsorganization , concept .company _cnn__pbs
For the OrganizationHiredPerson task
26
Published as a conference paper at ICLR 2020
Figure 18: OrganizationHiredPerson. The head is conceptstateorprovince.afternoon, the query rela-
tion is concept:OrganizatiOnhiredpersOn, and the tail is concept-personmexico-ryanjwhitney. The left is a
full subgraph derived with max_attendingfrom.per^step=20, and the right is a further pruned subgraph
from the left based on attention. The big yellow node represents the head, and the big red node rep-
resents the tail. Color on the rest indicates attention scores over a T -step reasoning process, where
grey means less attention, yellow means more attention gained during early steps, and red means
gaining more attention when getting closer to the final step.
Query: (concept_stateorprovince_afternoon , concept:OrganiZatiOnhiredPersOn , concept_personmexico_ryan_whitney)
Selected key edges :
ConCePt.Stateorprovince .afternoon,
ConCePt-Stateorprovince .afternoon,
COnCePt.Stateorprovince .afternoon,
COnCePt-dateliteral_n2007 , concept
COnCePt-dateliteral_n2007 , concept
COnCePt-dateliteral_n2007 , concept
COnCePt_dateliteral_n2007 , concept
concept : atdate
concept : atdate
concept : atdate
concept _dateliteral_n20 07
concept _date_n2003
concept _dateliteral_n20 0 6
concept_date_n2003 ,
concept_date_n2003 ,
concept_date_n2003 ,
concept_date_n2003 ,
concept
concept
concept
concept
COnCePt_dateliteral_n2006 ,
COnCePt-dateliteral_n2006 ,
COnCePt-dateliteral_n2006 ,
COnCePt-dateliteral_n2006 ,
atdateJnv
atdateJnv
atdateJnv
atdateJnv
atdateJnv , COnCePt-COUntry.United.States
atdateJnv , COnCePt_City_home
atdateJnv , concept_city_service
atdateJnv , COnCePt_country_left_parties
concept _country_united_states
concept _country_united_states
concept _country_united_states
concept: atdate_inv
concept: atdate_inv
concept: atdate_inv
concept: atdate_inv
COnCePt-COUntry.United.States
COnCePt_City_home
concept _city_service
COnCePt.COUntry_left_parties
COnCePt_country_united_states
COnCePt_City_home
concept _city_service
COnCePt.COUntry_left_parties
COnCePt_City_home ,
COnCePt_City_home ,
COnCePt_city_home ,
concept: atdate , concept_year_n1992
concept: atdate , concept_year_n1997
concept: Organizationhiredperson , concept_personmexico_ryan_whitney
concept_city_service , concept
concept_city_service , concept
concept_city_service , concept
COnCePt-COUntry_left_parties ,
COnCePt_country_left_parties ,
concept: atdate , concept_year_n 1992
concept: atdate , concept_year_n 1997
concept:organizationhiredperson, COnCePt-PersOnmeXiCo_ryan_whitney
concept_year_n1992 ,
concept_year_n1992 ,
concept_year_n1992 ,
concept_year_n1992 ,
concept_year_n1997 ,
concept_year_n1997 ,
concept_year_n1997 ,
concept: atdate_inv ,
concept: atdate_inv ,
concept: atdate_inv ,
concept: atdate_inv ,
concept: atdate_inv ,
concept: atdate_inv ,
concept: atdate_inv ,
atdate , concept_year_n1992
atdate , concept_year_n1997
Organizationhiredperson , concept_personmexico_ryan_whitney
concept :worksfor_inv , COnCePt-PersOnmeXiCo_ryan_whitney
concept :organizationhiredperson , COnCePt-PersOnmeXiCo_ryan_whitney
COnCePt.governmentorganization_house
concept_country_united_states
COnCePt_City_home
concept _tradeunion .congress
COnCePt_governmentorganization_house
concept_country_united_states
COnCePt_City_home
COnCePt.PersOnmeXiCo_ryan_whitney , concept: worksfor , concept .governmentorganization .house
27
Published as a conference paper at ICLR 2020
concept_personmexico_ryan_whitney , concept: worksfor , concept_tradeunion.congress
concept_personmexico_ryan_whitney , concept: worksfor , ConCePt.CoUntry_left_parties
concept.governmentorganization.house , ConCePt: PersonbelongstoorganizationJnv , ConCePt.PerSonUS.Party
concept.governmentorganization .house , concept: worksfor_inv , concept _personmexico_ryan_whitney
concept-governmentorganization _house , concept: organizationhiredperson , concept _personmexico_ryan_whitney
concept _tradeunion .congress , concept: organizationhiredperson , COnCePt-PerSOnUS-Party
concept _tradeunion .congress , concept: worksfor_inv , COnCePt.PerSOnmeXiCo_ryan_whitney
concept _tradeunion .congress , concept: organizationhiredperson , concept _personmexico_ryan_whitney
COnCePt-COUntry_left_parties , concept :organizationhiredperson , concept_personus_party
Figure 19: AgentBelongsToOrganization. The head is ConcePt卫ersonjnark001, the query relation
is Conceptiagentbelongstoorganization, and the tail is concept.geopoliticallocation jworld. The left is a
full subgraph derived with max_attending fom_perdtep=20, and the right is a further pruned subgraph
from the left based on attention. The big yellow node represents the head, and the big red node
represents the tail. Color on the rest indicates attention scores over a T -step reasoning process,
where grey means less attention, yellow means more attention gained during early steps, and red
means gaining more attention when getting closer to the final step.
For the AgentBelongsToOrganization task
Query : (concept .person _mark001 ,
concept:agentbelongstoorganization , COnCePt.geopoliticallocation_world)
Selected key edges :
concept .person _mark001 ,
concept .person _mark001 ,
concept .person _mark001 ,
concept .person _mark001 ,
COnCePt.SPOrtSteam.State.UniVerSity
COnCePt.SPOrtSteam.State.UniVerSity
COnCePt.SPOrtSteam.State.UniVerSity
COnCePt.SPOrtSteam.State.UniVerSity
concept:personbelongstoorganization, COnCePt.SPOrtSteam-State-UniVerSity
COnCePtIagentcoHaborateswithagent, concept_male_world
concept: agentcollaborateswithagent_inv, concept _male_world
concept:personbelongstoorganization, COnCePt_politicalparty_college
COnCePt:personbelongstoorganization_inv
COnCePt:personbelongstoorganization_inv
COnCePt:personbelongstoorganization_inv
COnCePt:personbelongstoorganization_inv
concept_male.world ,
concept_male,world ,
concept_male,world ,
concept_male,world ,
concept_male,world ,
concept_male.world ,
concept_male,world ,
concept_male,world ,
concept_male,world ,
concept:agentcollaborateswithagent, COnCePt-politician_jobs
COnCePtIagentcollaborateswithagentJnv, COnCePt.politician_jobs
concept: agentcollaborateswithagent , concept .person _mark001
COnCePtIagentcollaborateswithagentJnv , concept .person _mark001
concept:agentcollaborateswithagent, concept .person _greg001
COnCePtIagentcollaborateswithagentJnv , concept_person_greg001
concept:agentcontrols , COnCePt.Person_greg001
concept:agentcollaborateswithagent, COnCePt-Person_michael002
COnCePtIagentcollaborateswithagentJnv, COnCePt.Person_michael002
COnCePt.politician _jobs
COnCePt.Person_mark001
COnCePt-Person_greg001
concept .person _michael002
concept_politicalparty_college , COnCePtIpersonbelongstoorganizationJnv , concept .person _mark001
concept_politicalparty_college , COnCePt IpersonbelongstoorganizationJnv , COnCePt-PerSon_greg001
concept_politicalparty_college , COnCePt IpersonbelongstoorganizationJnv , concept_person_michael002
COnCePt.politician_jobs , concept:personbelongstoorganization , COnCePt-SPOrtSteam.State.UniVerSity
COnCePt.politician_jobs , concept :agentcollaborateswithagent , concept_male_world
28
Published as a conference paper at ICLR 2020
concept_politician_jobs
concept_politician_jobs
concept .person _greg001 ,
concept .person _greg001 ,
concept _person _greg001 ,
ConCePt.Person.greg001 ,
ConCePt.Person.greg001 ,
concept .person _greg001 ,
concept .person _greg001 ,
ConCePt IagentcollaborateswithagentJnv , ConCePt_male_world
concept:worksfor , COnCePt.geopoliticallocation_world
COnCePtIpersonbelongstoorganization, COnCePt.SPOrtSteam-State-UniverSity
COnCePtIagentcoHaborateswithagent, concept_male_world
concept: agentcollaborateswithagent_inv, concept _male_world
concept: agentcontrols_inv , COnCePt_male_world
concept:agentbelongstoorganization, COnCePt-geopoliticallocation_world
COnCePtIpersonbelongstoorganization, COnCePt.politicalparty_college
concept:agentbelongstoorganization, COnCePt-recordlabel.friends
concept_person_michael002 , concept:
concept .person _michael002 , concept:
concept .person _michael002 , concept:
concept .person _michael002 , concept:
concept .person _michael002 , concept:
COnCePt.geopoliticallocation_world ,
COnCePt.geopoliticallocation_world ,
COnCePt.geopoliticallocation_world ,
Personbelongstoorganization , COnCePt_sportsteam_state_university
agentcollaborateswithagent , COnCePt_male_world
agentcollaborateswithagentJnv , concept_male_world
agentbelongstoorganization , COnCePt.geopoliticallocation_world
Personbelongstoorganization , concept_politicalparty_college
concept: worksfor_inv , COnCePt.PerSOnmeXiCo_ryan_whitney
concept: Organizationhiredperson , concept_personmexico_ryan_whitney
concept: worksfor_inv , COnCePt-politician_jobs
COnCePt.recordlabel_friends , concept: Organizationhiredperson , concept _personmexico_ryan_whitney
COnCePt-PerSOnmeXiCo_ryan_whitney , concept: worksfor , COnCePt_geopoliticallocation_world
COnCePt.PerSOnmeXiCo_ryan_whitney , concept: organizationhiredperson_inv , concept _geopoliticallocation .world
COnCePt.PerSOnmeXiCo_ryan_whitney , concept: organizationhiredperson_inv , COnCePt-recordlabel_friends
Figure 20: TeamPlaysInLeague. The head is concept_sportsteam-mavericks, the query relation is
eoneept:teamPlaysinleague, and the tail is conceptsportsleague.nba. The left is a full subgraph derived
with maχ.attendingfromφer.step=20, and the right is a further pruned subgraph from the left based on
attention. The big yellow node represents the head, and the big red node represents the tail. Color on
the rest indicates attention scores over a T -step reasoning process, where grey means less attention,
yellow means more attention gained during early steps, and red means gaining more attention when
getting closer to the final step.
For the TeamPlaysInLeague task
Query : (COnCePt-SPOrtSteam.mavericks, concept:teamplaysinleague , COnCePt.SPOrtSIeagUe_nba)
Selected key edges
Concept.Sportsteam.mavericks ,
concept_sportsteam_mavericks ,
Concept.Sportsteam.mavericks ,
Concept.Sportsteam.mavericks ,
Concept.Sportsteam.mavericks ,
concept :teamplayssport , COnCePt-Sport_basketball
concept :teamplaysagainstteam , concept_sportsteam_boston_celtics
concept:teamplaysagainstteam_inv , COnCePt-SPOrtSteam.boston_celtics
concept :teamplaysagainstteam , concept_sportsteam.spurs
concept:teamplaysagainstteam_inv , concept_sportsteam_spurs
COnCePt.Sport_basketball , concept:teamplayssport_inv , concept_sportsteam.college
COnCePt_sport_basketball , concept:teamplayssport_inv , COnCePt_sportsteam_marshall_university
concept_sportsteam_boston.Celtics , concept:teamplaysinleague , concept_sportsleague_nba
COnCePt.SPOrtSteam.SPUrS , concept:teamplaysinleague , concept.Sportsleague_nba
concept_sportsleague_nba , concept: agentcompeteswithagent , concept_sportsleague_nba
29
Published as a conference paper at ICLR 2020
concept_sportsleague_nba , Conceptiagentcompeteswithagenkinv , concept_sportsleague_nba
Concept.Sportsteam.college , ConCePt Iteampgysinleague , ConCePt-SPortsleagUeJnternational
30