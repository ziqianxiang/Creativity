Published as a conference paper at ICLR 2020
Observational Overfitting in Reinforcement
Learning
Xingyou Song*, Yiding Jiang±, Stephen Tu, Behnam Neyshabur
Google
{xingyousong,ydjiang,stephentu,neyshabur}@google.com
Yilun Du*
MIT
yilundu@mit.edu
Ab stract
A major component of overfitting in model-free reinforcement learning (RL) in-
volves the case where the agent may mistakenly correlate reward with certain
spurious features from the observations generated by the Markov Decision Process
(MDP). We provide a general framework for analyzing this scenario, which we
use to design multiple synthetic benchmarks from only modifying the observation
space of an MDP. When an agent overfits to different observation spaces even if
the underlying MDP dynamics is fixed, we term this observational overfitting. Our
experiments expose intriguing properties especially with regards to implicit regu-
larization, and also corroborate results from previous works in RL generalization
and supervised learning (SL).
1	Introduction
Generalization for RL has recently grown to be an important topic for agents to perform well in
unseen environments. Complication arises when the dynamics of the environments entangle with
the observation, which is often a high-dimensional projection of the true latent state. One particular
framework, which we denote by zero-shot supervised framework (Zhang et al., 2018a;c; Nichol
et al., 2018; Justesen et al., 2018) and is used to study RL generalization, is to treat it analogous to a
classical supervised learning (SL) problem - i.e. assume there exists a distribution ofMDP's, train
jointly on a finite “training set” sampled from this distribution, and check expected performance
on the entire distribution, with the fixed trained policy. In this framework, there is a spectrum of
analysis, ranging from almost purely theoretical analysis (Wang et al., 2019; Asadi et al., 2018) to
full empirical results on diverse environments (Zhang et al., 2018c; Packer et al., 2018).
However, there is a lack of analysis in the middle of this spectrum. On the theoretical side, previous
work do not provide analysis for the case when the underlying MDP is relatively complex and requires
the policy to be a non-linear function approximator such as a neural network. On the empirical side,
there is no common ground between recently proposed empirical benchmarks. This is partially caused
by multiple confounding factors for RL generalization that can be hard to identify and separate. For
instance, an agent can overfit to the MDP dynamics of the training set, such as for control in Mujoco
(Pinto et al., 2017; Rajeswaran et al., 2017). In other cases, an RNN-based policy can overfit to
maze-like tasks in exploration (Zhang et al., 2018c), or even exploit determinism and avoid using
observations (Bellemare et al., 2012; Machado et al., 2018). Furthermore, various hyperparameters
such as the batch-size in SGD (Smith et al., 2018), choice of optimizer (Kingma & Ba, 2014), discount
factor γ (Jiang et al., 2015) and regularizations such as entropy (Ahmed et al., 2018) and weight
norms (Cobbe et al., 2018) can also affect generalization.
* Work partially performed as an OpenAI Fellow.
^ Work performed during the Google AI Residency Program. http://g.co/airesidency
1
Published as a conference paper at ICLR 2020
Due to these confounding factors, it can be unclear what parts of the MDP or policy are actually
contributing to overfitting or generalization in a principled manner, especially in empirical studies
with newly proposed benchmarks. In order to isolate these factors, we study one broad factor affecting
generalization that is most correlated with themes in SL, specifically observational overfitting, where
an agent overfits due to properties of the observation which are irrelevant to the latent dynamics of
the MDP family. To study this factor, we fix a single underlying MDP’s dynamics and generate a
distribution of MDP’s by only modifying the observational outputs.
Our contributions in this paper are the following:
1.	We discuss realistic instances where observational overfitting may occur and its difference
from other confounding factors, and design a parametric theoretical framework to induce
observational overfitting that can be applied to any underlying MDP.
2.	We study observational overfitting with linear quadratic regulators (LQR) in a synthetic
environment and neural networks such as multi-layer perceptrons (MLPs) and convolutions
in classic Gym environments. A primary novel result we demonstrate for all cases is that
implicit regularization occurs in this setting in RL. We further test the implicit regularization
hypothesis on the benchmark CoinRun from using MLPs, even when the underlying MDP
dynamics are changing per level.
3.	In the Appendix, we expand upon previous experiments by including full training curve and
hyperparamters. We also provide an extensive analysis of the convex one-step LQR case
under the observational overfitting regime, showing that under Gaussian initialization of the
policy and using gradient descent on the training cost, a generalization gap must necessarily
exist.
The structure of this paper is outlined as follows: Section 2 discusses the motivation behind this
work and the synthetic construction to abstract certain observation effects. Section 3 demonstrates
numerous experiments using this synthetic construction that suggest implicit regularization is at work.
Finally, Section 3.4 tests the implicit regularization hypothesis on CoinRun, as well as ablates various
ImageNet architectures and margin metrics in the Appendix.
2	Motivation and Related Work
We start by showing an example of observational overfitting in Figure 1. This example highlights
the issues surrounding MDP’s with rich, textured observations - specifically, the agent can use any
features that are correlated with progress, even those which may not generalize across levels. This
is an important issue for vision-based policies, as many times it is not obvious what part of the
observation causes an agent to act or generalize.
Figure 1: Example of observational overfitting in Sonic from Gym Retro (Nichol et al., 2018).
Saliency maps (Greydanus et al., 2018) highlight (in red) the top-left timer and background objects
such as clouds and textures because they are correlated with progress, as they move backwards
while agent is moving forwards. The agent could memorize optimal actions for training levels
even if its observation was only from the timer, and “blacking-out” the timer consistently improved
generalization performance (see Appendix A.2.3).
Currently most architectures used in model-free RL are simple (with fewer than one million parame-
ters) compared to the much larger and more complex ImageNet architectures used for classification.
2
Published as a conference paper at ICLR 2020
This is due to the fact that most RL environments studied either have relatively simple and highly
structured images (e.g. Atari) compared to real world images, or conveniently do not directly force
the agent to observe highly detailed images. For instance in large scale RL such as DOTA2 (OpenAI,
2018) or Starcraft 2 (Vinyals et al., 2017), the agent observations are internal minimaps pertaining to
object xy-locations, rather than human-rendered observations.
2.1	What Happens in Observation Space?
Several artificial benchmarks (Zhang et al., 2018b; Gamrian & Goldberg, 2019) have been proposed
before to portray this notion of overfitting, where an agent must deal with a changing background -
however, a key difference in our work is that we explicitly require the “background” to be correlated
with the progress rather than loosely correlated (e.g. through determinism between the background
and the game avatar) or not at all. This makes a more explicit connection to causal inference
(Arjovsky et al., 2019; Heinze-Deml & Meinshausen, 2019; Heinze-Deml et al., 2019) where
spurious correlations between ungeneralizable features and progress may make training easy, but are
detrimental to test performance because they induce false attributions.
Previously, many works interpret the decision-making of an agent through saliency and other network
visualizations (Greydanus et al., 2018; Such et al., 2018) on common benchmarks such as Atari.
Other recent works such as (Igl et al., 2019) analyze the interactions between noise-injecting explicit
regularizations and the information bottleneck. However, our work is motivated by learning theoretic
frameworks to capture this phenomena, as there is vast literature on understanding the generalization
properties of SL classifiers (Vapnik & Chervonenkis, 1971; McAllester, 1999; Bartlett & Mendelson,
2002) and in particular neural networks (Neyshabur et al., 2015b; Dziugaite & Roy, 2017; Neyshabur
et al., 2017; Bartlett et al., 2017; Arora et al., 2018c). For an RL policy with high-dimensional
observations, we hypothesize its overfitting can come from more theoretically principled reasons, as
opposed to purely good inductive biases on game images.
As an example of what may happen in high dimensional observation space, consider linear least
squares regression task where given the set X ∈ Rm×d and Y ∈ Rm, we want to find w ∈ Rd
that minimizes 'χ,γ(W) = ∣∣Y - Xw∣∣1 2 where m is the number of samples and d is the input
dimension. We know that if X>X is full rank (hence d ≤ m), 'χ,γ(.) has a unique global minimum
w* = (X>X)-1X>Y. On the other hand if X>X is not full rank (eg. when m ‹ d), then there are
many global minima w* such that Y = Xw* 1. Luckily, if we use any gradient based optimization
to minimize the loss and initialize with w = 0, the solution will only span column spaces of X and
converges to minimum `2 norm solution among all global minima due to implicit regularization
(Gunasekar et al., 2017). Thus a high dimensional observation space with a low dimensional state
space can induce multiple solutions, some of which are not generalizable to other functions or MDP’s
but one could hope that implicit regularization would help avoiding this issue. We analyze this case
in further detail for the convex one-step LQR case in Section 3.1 and Appendix A.4.3.
2.2	Notation
In the zero-shot framework for RL generalization, we assume there exists a distribution D over MDP’s
M for which there exists a fixed policy πopt that can achieve maximal return on expectation over
MDP’s generated from the distribution. An appropriate finite training set Mtrain = {M1, . . . , Mn}
can then be created by repeatedly randomly sampling M 〜 D. Thus for a MDP M and any policy
π, expected episodic reward is defined as RM (π).
In many empirical cases, the support of the distribution D is made by parametrized MDP’s where
some process, given a parameter θ, creates a mapping θ → Mθ (e.g. through procedural generation),
and thus we may simplify notation and instead define a distribution Θ that induces D, which implies
a set of samples Θtrain = {θ1, . . . , θn} also induces a Mtrain = {M1, . . . , Mn}, and we may
redefine reward as RMθ (π) = Rθ (π).
1Given any X with full rank X>X, it is possible to create many global minima by projecting the data onto
high dimensions using a semi-orthogonal matrix Z ∈ Rd×d0 where d0 > m ≥ d and ZZ> = Id . Therefore, we
the loss 'χz,γ(W) = ∣∣Y — XZw∣∣2 will have many global optima w* with Y = XZw*.
3
Published as a conference paper at ICLR 2020
(a)
Figure 2: (a) Visual Analogy of the Observation Function. (b) Our combinations for 1-D (top) and
2-D (bottom) images for synthetic tasks.
As a simplified model of the observational problem from Sonic, we can construct a mapping θ → Mθ
by first fixing a base MDP M = (S, A, r, T), which corresponds to state space, action space, reward,
and transition. The only effect of θ is to introduce an additional observation function φθ : S → O,
where the agent receives input from the high dimensional observation space O rather than from
the state space S . Thus, for our setting, θ actually parameterizes a POMDP family which can be
thought of as simply a combination of a base MDP M and an observational function φθ, hence
Mθ = (M, φθ).
Let Θtrain = {θ1, . . . , θn} be a set of n i.i.d. samples from Θ, and suppose we train π to optimize
1
reward against {Mθ : θ 〜。加°泳}. The objective Jg(∏) =	∣ Eθ.∈θ. . Rθi (∏) IS the
| Θtrain |	i train
average reward over this empirical sample. We want to generalize to the distribution Θ, which can be
expressed as the average episode reward R over the full distribution, i.e. Jθ (π) = Eθ〜θ [Rθ (π)].
Thus we define the generalization gap as JΘb (π) - JΘ (π).
2.3	Setup
We can model the effects of Figure 1 more generally, not specific to sidescroller games. We assume
that there is an underlying state s (e.g. xy-locations of objects in a game), whose features may be
very well structured, but that this state has been projected to a high dimensional observation space by
φθ . To abstract the notion of generalizable and non-generalizable features, we construct a simple and
natural candidate class of functions, where
φθ (s) = h(f (s), gθ (s))
(1)
In this setup, f (∙) is a function invariant for the entire MDP population Θ, while gθ (∙) is a function
dependent on the sampled parameter θ. h is a ”combination” function which combines the two
outputs of f and g to produce a final observation. While f projects this latent data into salient and
important, invariant features such as the avatar, monsters, and items, gθ projects the latent data
to unimportant features that do not contribute to extra generalizable information, and can cause
overfitting, such as the changing background or textures. A visual representation is shown in Figure
2. This is a simplified but still insightful model relevant in more realistic settings. For instance, in
settings where gθ does matter, learning this separation and task-identification (Yu et al., 2017; Peng
et al., 2018) could potentially help fast adaptation in meta-learning (Finn et al., 2017). From now on,
we denote this setup as the (f, g)-scheme.
This setting also leads to more interpretable generalization bounds - Lemma 2 of (Wang et al., 2019)
provides a high probability (1 - δ) bound for the “intrinsic” generalization gap when m levels are
sampled: gap ≤ Radm (Rn) + O ( ʌ/ɪɑgm/^)'), where
Radm (Rn )= E(θι,…,θm)〜gm Eσ∈{-1,+1}
SUpL X σi Rθi (n)
π∈n m i=1
(2)
4
Published as a conference paper at ICLR 2020
is the Rademacher Complexity under the MDP, where θi are the ζi parameters used in the original
work, and the transition T and initialization I are fixed, therefore omitted, to accommodate our
setting.
The Rademacher Complexity term captures how invariant policies in the set Π with respect to θ. For
most RL benchmarks, this is not interpretable due to multiple confounding factors such as the varying
level dynamics. For instance, it is difficult to imagine what behaviors or network weights a policy
would possess in order to produce the same total rewards, regardless of changing dynamics.
However, in our case, because the environment parameters θ are only from gθ , the Rademacher
Complexity is directly based on how much the policy “looks at" gθ. More formally, let Π* be the
set of policies ∏* which are not be affected by changes in gθ; i.e. Vθ∏* (φθ(S)) = 0 ∀s and thus
Rθ(∏*) = Rconst ∀θ, which implies that the environment parameter θ has no effect on the reward;
hence Radm(R∏*) = Eσ∈{-1,+1} Isup∏*∈∏* mm Pm=I σiRconst∖ = 0.
2.4	Architecture and Implicit Regularization
Normally in a MDP such as a game, the concatenation operation may be dependent on time (e.g.
textures move around in the frame). In the scope of this work, we simplify the concatenation effect
and assume h(∙) is a static concatenation, but still are able to demonstrate insightful properties. We
note that this inductive bias on h allows explicit regularization to trivially solve this problem, by
penalizing a policy’s first layer that is used to “view” gθ(s) (Appendix A.1.1), hence we only focus
on implicit regularizations.
This setting is naturally attractive to analyzing architectural differences, as it is more closely related
in spirit to image classifiers and SL. One particular line of work to explain the effects of certain
architectural modifications in SL such as overparametrization and residual connections is implicit
regularization (Neyshabur et al., 2015a; Gunasekar et al., 2017; Neyshabur, 2017), as overparametriza-
tion through more layer depth and wider layers has proven to have no `p-regularization equivalent
(Arora et al., 2019), but rather precondition the dynamics during training. Thus, in order to fairly
experimentally measure this effect, we always use fixed hyperparameters and only vary based on
architecture. In this work, we only refer to architectural implicit regularization techniques, which do
not have a explicit regularization equivalent. Some techniques e.g. coordinate descent (Bradley et al.,
2011) are equivalent to explicit `1 -regularization.
3	Experiments
3.1	Overparamterized LQR
We first analyze the case of the LQR as a surrogate for what may occur in deep RL, which has been
done before for various topics such as sample complexity (Dean et al., 2019) and model-based RL
(Tu & Recht, 2019). This is analogous to analyzing linear/logistic regression (Kakade et al., 2008;
McAllester, 2003) as a surrogate to understanding extensions to deep SL techniques (Neyshabur et al.,
2018a; Bartlett et al., 2017). In particular, this has numerous benefits - the cost (negative of reward)
function is deterministic, and allows exact gradient descent (i.e. the policy can differentiate through
the cost function) as opposed to necessarily using stochastic gradients in normal RL, and thus can
cleanly provide evidence of implicit regularization. Furthermore, in terms of gradient dynamics and
optimization, LQR readily possesses nontrivial qualities compared to linear regression, as the LQR
cost is a non-convex function but all of its minima are global minima (Fazel et al., 2018).
To show that overparametrization alone is an important implicit regularizer in RL, LQR allows the use
of linear policies (and consequently also allows stacking linear layers) without requiring a stochastic
output such as discrete Gumbel-softmax or for the continuous case, a parametrized Gaussian. This
is setting able to show that overparametrization alone can affect gradient dynamics, and is not a
consequence of extra representation power due to additional non-linearities in the policy. There have
been multiple recent works on this linear-layer stacking in SL and other theoretical problems such as
matrix factorization and matrix completion (Arora et al., 2018b;a; Gunasekar et al., 2017), but to our
knowledge, we are the first to analyze this case in the context of RL generalization.
5
Published as a conference paper at ICLR 2020
We explicitly describe setup as follows: for a given θ, We let f (S) = Wc ∙ s, while gθ (S) = Wθ ∙ S
where Wc , Wθ are semi-orthogonal matrices, to prevent information loss relevant to outputting the
optimal action, as the state is transformed into the observation. Hence, if St is the underlying state
at time t, then the observation is ot =
St and thus the action is at = Kot , where K is the
policy matrix. While Wc remains a constant matrix, we sample Wθ randomly, using the “level ID”
integer θ as the seed for random generation. In terms of dimensions, if S is of shape dstate , then
f also projects to a shape of dstate, while gθ projects to a much larger shape dnoise , implying that
the observation to the agent is of dimension dsignal + dnoise . In our experiments, we set as default
(dsignal , dnoise) = (100, 1000).
If P? is the unique minimizer of the original cost function, then the unique minimizer of the population
cost is K? =
Wc0P?T
for instance
T
. However, if we have a single level, then there exist multiple solutions,
αWc P?T
(1 - α)WθP?
T
∀α. This extra bottom component Wθ P?T causes overfitting. In
Appendix A.4.3, we show that in the 1-step LQR case (which can be extended to convex losses whose
gradients are linear in the input), gradient descent cannot remove this component, and thus overfitting
necessarily occurs.
Furthermore, we find that increasing dnoise increases the generalization gap in the LQR setting. This
is empirically verified in Figure 3 using an actual non-convex LQR loss, and the results suggest
that the gap scales by O(VzdnOise). In terms of overparametrization, we experimentally added more
(100 × 100) linear layers K = K0K1, ..., Kj and increased widths for a 2-layer case (Figure 3), and
observe that both settings reduce the generalization gap, and also reduce the norms (spectral, nuclear,
Frobenius) of the final end-to-end policy K, without changing its expressiveness. This suggests that
gradient descent under overparametrization implicitly biases the policy towards a “simpler” model in
the LQR case.
Generalization Gap (Log) vs Noise Dimension (Log)
Figure 3: (Left) We show that the generalization gap vs noise dimension is tight as the noise dimension
increases, showing that this bound is accurate. (Middle and Right) LQR Generalization Gap vs
Number of Intermediate Layers. We plotted different Φ = Pj=0 kAkkL terms without exponents, as
powers of those terms are monotonic transforms since kkAk^ ≥ 1 ∀A and ∣∣A∣∣* = IlAkF , ∣∣A∣∣1∙ We
see that the naive spectral bound diverges at 2 layers, and the weight-counting sums are too loose.
As a surrogate model for deep RL, one may ask if the generalization gap of the final end-to-end
policy K can be predicted by functions of the layers K0, ..., Kj. This is an important question as
it is a required base case for predicting generalization when using stochastic policy gradient with
nonlinear activations such as ReLU or Tanh. From examining the distribution of singular values on
K (Appendix A.1.1), we find that more layers does not bias the policy towards a low rank solution
in the nonconvex LQR case, unlike (Arora et al., 2018b) which shows this does occur for matrix
completion, and in general, convex losses. Ultimately, we answer in the negative: intriguingly, SL
bounds have very little predictive power in the RL domain case.
6
Published as a conference paper at ICLR 2020
To understand why SL bounds may be candidates for the LQR case, we note that as a basic smoothness
bound C(K) - C(K0) ≤ O(kK - K0k) (Appendix A.4) can lead to very similar reasoning with SL
bounds. Since our setup is similar to SL in that “LQR levels” which may be interpreted as a dataset,
We use bounds of the form ∆ ∙ Φ, where ∆ is a “macro” product term ∆ = Qj=0 IlKik ≥ ∣∣ Qj=o Kil ∣
derivable from the fact that kAB k ≤ kAk kB k in the linear case, and Φ is a weight-counting term
which deals with the overparametrized case, such as Φ = Pj=O 片；,(Neyshabur et al., 2018a)
or Φ =(Pj=0 ( kKk1) / ) (Bartlett et al., 2017). However, the Φ terms increase too rapidly as
shown in Figure 3. Terms such as Frobenius product (Golowich et al., 2018) and Fischer-Rao (Liang
et al., 2019) are effective for the SL depth case, but are both ineffective in the LQR depth case. For
width, the only product which is effective is the nuclear norm product.
3.2	Projected Gym Environments
In Section 3.1, we find that observational overfitting exists and overparametrization potentially helps
in the linear setting. In order to analyze the case when the underlying dynamics are nonlinear,
we let M be a classic Gym environment and we generate a Mθ = (M, wθ) by performing the
exact same (f, g)-scheme as the LQR case, i.e. sampling θ to produce an observation function
wθ(s) =
Wc
Wθ
s. We again can produce training/test sets of MDPs by repeatedly sampling θ, and
for policy optimization, we use Proximal Policy Gradient (Schulman et al., 2017).
Although bounds on the smoothness term Rθ (π) - Rθ (π0) affects upper bounds on Rademacher
Complexity (and thus generalization bounds), we have no such theoretical guarantees in the Mujoco
case as it is difficult to analyze the smoothness term for complicated transitions such as Mujoco’s
physics simulator. However, in Figure 4, we can observe empirically that the underlying state
dynamics has a significant effect on generalization performance as the policy nontrivially increased
test performance such as in CartPole-v1 and Swimmer-v2, while it could not for others. This suggests
that the Rademacher complexity and smoothness on the reward function vary highly for different
environments.
P0江
500
400
300
200
100
CartPoIe-Vl
0
0.0 0.2 0.4 0.6 0.8 1.0
Reacher-v2
-200
-300
-400
-500
-600
-700
3000
2500
2000
1500
1000
500
0
-500
1e8
0.0 0.2 0.4 0.6 0.8 1.0
1e8
HalfCheetah-v2
2000
1750
1500
1250
1000
750
500
250
0
Hopper-v2
0.0 0.2 0.4 0.6 0.8 1.0
Walker2d-v2
1e8
0.0 0.2 0.4 0.6 0.8 1.0
Swimmer-v2
1e8
3500
3000
2500
2000
1500
1000
500
0.0 0.2 0.4 0.6 0.8 1.0
1e8
Total Timesteps
0.0 0.2 0.4 0.6 0.8 1.0
1e8

0
Figure 4: Each Mujoco task is given 10 training levels (randomly sampling gθ parameters). We used
a 2-layer ReLU policy, with 128 hidden units each. Dimensions of outputs of (f, g) were (30, 100)
respectively.
Even though it is common practice to use basic (2-layer) MLPs in these classic benchmarks, there are
highly nontrivial generalization effects from modifying on this class of architectures. Our results in
7
Published as a conference paper at ICLR 2020
Figures 5 and 6 show that increasing width and depth for basic MLPs can increase generalization
and is significantly dependent on the choice of activation, and other implicit regularizations such as
using residual layers can also improve generalization. Specifically, switching between ReLU and
Tanh activations produces different results during overparametrization. For instance, increasing Tanh
layers improves generalization on CartPole-v1, and width increase with ReLU helps on Swimmer-v2.
Tanh is noted to consistently improve generalization performance. However, stacking Tanh layers
comes at a cost of also producing vanishing gradients which can produce subpar training performance,
for e.g. HalfCheetah. To allow larger depths, we use ReLU residual layers, which also improves
generalization and stabilizes training.
CartPole-v1, 10 Levels, Varying Layers
HalfCheetah-v2, 50 Levels, Varying Layers
Figure 5: Effects of Depth.
Tanh Test
⅝ ■ ReLU Trail
ReLU Tesl
CartPole-v1, 10 Levels, Varying Width
Ooooo
Ooooo
5 4 3 2 1
p」BMəor
7	8	9	10	11	12	13
Width, 2x
Swimmer-v2, 30 Levels, Varying Width
Ooooo
2 0 8 6 4
p.jeməu
5	6	7	8	9	10	11
Width, 2x
Figure 6: Effects of Width.
Previous work (Zhang et al., 2018c) did not find such an architectural pattern for GridWorld environ-
ments, suggesting that this effect may exist primarily for observational overfitting cases. While there
have been numerous works which avoid overparametrization on simplifying policies (Rajeswaran
et al., 2017; Mania et al., 2018) or compactifying networks (Choromanski et al., 2018; Gaier &
Ha, 2019), we instead find that there are generalization benefits to overparametrization even in the
nonlinear control case.
3.3	Deconvolutional Projections
From the above results with MLPs, one may wonder if similar results may carry to convolutional
networks, as they are widely used for vision-based RL tasks. As a ground truth reference for our
experiment, we the canonical networks proven to generalize well in the dataset CoinRun, which are
from worst to best, NatureCNN Mnih et al. (2013), IMPALA Espeholt et al. (2018), and IMPALA-
LARGE (IMPALA with more residual blocks and higher convolution depths), which have respective
parameter numbers (600K, 622K, 823K).
We setup a similar (f, g)-scheme appropriate for the inductive bias of convolutions, by passing the
vanilla Gym 1D state corresponding to joint locations and velocities, through multiple deconvolutions.
We do so rather than using the RGB image from env.render() to enforce that the actual state is
indeed low dimensional and minimize complications in experimentation, as e.g. inference of velocity
information would require frame-stacking.
Specifically in our setup, we project the actual state to a fixed length, reshaping it into a square, and
replacing f and gθ both with the same orthogonally-initialized deconvolution architecture to each
produce a 84 X 84 image (but gθ's network weights are still generated by θι,..., θm similar to before).
We combine the two outputs by using one half of the ”image” from f, and one half from gθ, as shown
back in Figure 2.
8
Published as a conference paper at ICLR 2020
Total Timesteps
Figure 7: Performance of architectures in the synthetic Gym-Deconv dataset. To cleanly depict test
performance, training curves are replaced with horizontal (max env. reward) and vertical black lines
(avg. timestep when all networks reach max reward).
NatureCNN
IMPALA
IMPALA LARGE
Figure 8: We only show the observation from gθ (s), which tests memorization capacity on
Swimmer-v2.
Figure 7 shows that the same ranking between the three architectures exists as well on the Gym-
Deconv dataset. We show that generalization ranking among NatureCNN/IMPALA/IMPALA-
LARGE remains the same regardless of whether we use our synthetic constructions or CoinRun. This
suggests that the RL generalization quality of a convolutional architecture is not limited to real world
data, as our test purely uses numeric observations, which are not based on a human-prior. From these
findings, one may conjecture that these RL generalization performances are highly correlated and
may be due to common factors.
One of these factors we suggest is due to implicit regularization. In order to support this claim,
we perform a memorization test by only showing gθ ’s output to the policy. This makes the dataset
impossible to generalize to, as the policy network cannot invert every single observation function
{gθ1 (∙), gθ2 (∙),..., gθn (∙)} simultaneously. Zhang et al. (2018c) also constructs a memorization test
for mazes and grid-worlds, and showed that more parameters increased the memorization ability of
the policy. While it is intuitive that more parameters would incur more memorization, we show in
Figure 8 that this is perhaps not a complete picture when implicit regularization is involved.
Using the underlying MDP as a Swimmer-v2 environment, we see that NatureCNN, IMPALA,
IMPALA-LARGE have reduced memorization performances. IMPALA-LARGE, which has more
depth parameters and more residual layers (and thus technically has more capacity), memorizes less
than IMPALA due its inherent inductive bias. While memorization performance is dampened in 8, we
perform another deconvolution memorization test using an LQR as the underlying MDP in Appendix
A.1.1 that shows that there can exist specific hard limits to memorization, which also follows the
same ranking above.
3.4	Overparametrization in CoinRun
We further test our overparametrization hypothesis from Sections 3.1, 3.2 to the CoinRun benchmark,
using unlimited levels for training. For MLP networks, we downsized CoinRun from native 64 × 64
to 32 × 32, and flattened the 32 × 32 × 3 image for input to an MLP. Two significant differences
from the synthetic cases are that 1. Inherent dynamics are changing per level in CoinRun, and 2. The
relevant and irrelevant CoinRun features change locations across the 1-D input vector. Regardless,
in Figure 9, we show that overparametrization can still improve generalization in this more realistic
9
Published as a conference paper at ICLR 2020
RL benchmark, much akin to (Neyshabur et al., 2018b) which showed that overparametrization for
MLP's improved generalization on 32 X 32 X 3 CIFAR-10.
8
Figure 9: Overparametrization improves generalization for CoinRun.
While we also extend the case of large-parameter convolutional networks using ImageNet networks
in Appendix A.2.1, an important question is how to predict the generalization gap only from the
training phase. A particular set of metrics, popular in the SL community are margin distributions
(Jiang et al., 2018; Bartlett et al., 2017), as they deal with the case for softmax outputs which do
not explicitly penalize the weight norm of a network, by normalizing the ”confidence” margin of
the logit outputs. While using margins on state-action pairs (from an on-policy replay buffer) is not
technically rigorous, one may be curious to see if they have predictive power, especially as MLPs
are relatively simple to norm-bound. We plotted these margin distributions in Appendix A.2.2, but
found that the weight norm bounds used in SL are simply too dominant for this RL case. This, with
the bound results found earlier for the LQR case, suggests that current norm bounds are simply too
loose for the RL case even though we have shown overparametrization helps generalization in RL,
and hopefully this motivates more of the study of such theory.
Coinrun-Unlimited, MLP Depth (ReLU)
4	Conclusion
We have identified and isolated a key component of overfitting in RL as the particular case of “obser-
vational overfitting”, which is particularly attractive for studying architectural implicit regularizations.
We have analyzed this setting extensively, by examining 3 main components:
1.	The analytical case of LQR and linear policies under exact gradient descent, which lays the
foundation for understanding theoretical properties of networks in RL generalization.
2.	The empirical but principled Projected-Gym case for both MLP and convolutional networks
which demonstrates the effects of neural network policies under nonlinear environments.
3.	The large scale case for CoinRun, which can be interpreted as a case where relevant features
are moving across the input, where empirically, MLP overparametrization also improves
generalization.
We noted that current network policy bounds using ideas from SL are unable to explain over-
parametrization effects in RL, which is an important further direction. In some sense, this area of
RL generalization is an extension of static SL classification from adding extra RL components. For
instance, adding a nontrivial “combination function” between f and gθ that is dependent on time (to
simulate how object pixels move in a real game) is both an RL generalization issue and potentially
video classification issue, and extending results to the memory-based RNN case will also be highly
beneficial.
Furthermore, it is unclear whether such overparametrization effects would occur in off-policy methods
such as Q-learning and also ES-based methods. In terms of architectural design, recent works (Jacot
et al., 2018; Garriga-Alonso et al., 2019; Lee et al., 2019) have shed light on the properties of
asymptotically overparametrized neural networks in the infinite width and depth cases and their
performance in SL. Potentially such architectures (and a corresponding training algorithm) may be
used in the RL setting which can possibly provide benefits, one of which is generalization as shown
in this paper. We believe that this work provides an important initial step towards solving these future
problems.
10
Published as a conference paper at ICLR 2020
Acknowledgements
We would like to thank John Schulman for very helpful guidance over the course of this work. We
also wish to thank Chiyuan Zhang, Ofir Nachum, Aurick Zhou, Daniel Seita, Alexander Irpan, and
the OpenAI team for fruitful comments and discussions during the course of this work.
References
Zafarali Ahmed, Nicolas Le Roux, Mohammad Norouzi, and Dale Schuurmans. Understanding the
impact of entropy on policy optimization. CoRR, abs/1811.11214, 2018.
Martin Arjovsky, Leon Bottou,Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization.
CoRR, abs/1907.02893, 2019. URL http://arxiv.org/abs/1907.02893.
Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit
acceleration by overparameterization. CoRR, abs/1802.06509, 2018a. URL http://arxiv.
org/abs/1802.06509.
Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit
acceleration by overparameterization. In Proceedings of the 35th International Conference on
Machine Learning, ICML 2018, Stockholmsmdssan, Stockholm, Sweden, July 10-15, 2018, pp.
244-253, 2018b.
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep
nets via a compression approach. In Proceedings of the 35th International Conference on Machine
Learning, ICML 2018, Stockholmsmdssan, Stockholm, SWeden, July 10-15, 2018, pp. 254-263,
2018c.
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix
factorization. CoRR, abs/1905.13655, 2019. URL http://arxiv.org/abs/1905.13655.
Kavosh Asadi, Dipendra Misra, and Michael L. Littman. Lipschitz continuity in model-based
reinforcement learning. In Proceedings of the 35th International Conference on Machine Learning,
ICML2018, Stockholmsmassan, Stockholm, Sweden, July 10-15, 2018,pp. 264-273, 2018.
Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. Journal of Machine Learning Research, 3(Nov):463-482, 2002.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan,
and R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp. 6240-6249.
Curran Associates, Inc., 2017.
Marc G. Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environ-
ment: An evaluation platform for general agents. CoRR, abs/1207.4708, 2012.
Joseph K. Bradley, Aapo Kyrola, Danny Bickson, and Carlos Guestrin. Parallel coordinate descent
for l1-regularized loss minimization. In Proceedings of the 28th International Conference on
Machine Learning, ICML 2011, Bellevue, Washington, USA, June 28 - July 2, 2011, pp. 321-328,
2011. URL https://icml.cc/2011/papers/231_icmlpaper.pdf.
Krzysztof Choromanski, Mark Rowland, Vikas Sindhwani, Richard E. Turner, and Adrian Weller.
Structured evolution with compact architectures for scalable policy optimization. In Proceedings
of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmassan,
Stockholm, Sweden, July 10-15, 2018, pp. 969-977, 2018. URL http://proceedings.mlr.
press/v80/choromanski18a.html.
Karl Cobbe, Oleg Klimov, Chris Hesse, Taehoon Kim, and John Schulman. Quantifying generalization
in reinforcement learning. CoRR, abs/1812.02341, 2018.
Sarah Dean, Horia Mania, Nikolai Matni, Benjamin Recht, and Stephen Tu. On the sample complexity
of the linear quadratic regulator. Foundations of Computational Mathematics, Aug 2019.
11
Published as a conference paper at ICLR 2020
Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for
deep (stochastic) neural networks with many more parameters than training data. arXiv preprint
arXiv:1703.11008, 2017.
Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymyr Mnih, Tom Ward, Yotam
Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and Koray Kavukcuoglu. IMPALA:
scalable distributed deep-rl with importance weighted actor-learner architectures. In Proceedings
of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmdssan,
Stockholm, Sweden, July 10-15, 2018,pp.1406-1415, 2018.
Maryam Fazel, Rong Ge, Sham Kakade, and Mehran Mesbahi. Global convergence of policy gradient
methods for the linear quadratic regulator. In Proceedings of the 35th International Conference
on Machine Learning, ICML 2018, StoCkholmSmaSsan, Stockholm, Sweden, July 10-15, 2018, pp.
1466-1475, 2018.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. In Proceedings of the 34th International Conference on Machine Learning,
ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pp. 1126-1135, 2017. URL http:
//proceedings.mlr.press/v70/finn17a.html.
Adam Gaier and David Ha. Weight agnostic neural networks. CoRR, abs/1906.04358, 2019. URL
http://arxiv.org/abs/1906.04358.
Shani Gamrian and Yoav Goldberg. Transfer learning for related reinforcement learning tasks via
image-to-image translation. In Proceedings of the 36th International Conference on Machine
Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, pp. 2063-2072, 2019. URL
http://proceedings.mlr.press/v97/gamrian19a.html.
Adria Garriga-Alonso, Carl Edward Rasmussen, and Laurence Aitchison. Deep convolutional
networks as shallow gaussian processes. In 7th International Conference on Learning Rep-
resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019, 2019. URL https:
//openreview.net/forum?id=Bklfsi0cKm.
Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of neural
networks. In Conference On Learning Theory, COLT 2018, Stockholm, Sweden, 6-9 July 2018., pp.
297-299, 2018. URL http://proceedings.mlr.press/v75/golowich18a.html.
Samuel Greydanus, Anurag Koul, Jonathan Dodge, and Alan Fern. Visualizing and understanding
atari agents. In Proceedings of the 35th International Conference on Machine Learning, ICML
2018, Stockholmsmassan, Stockholm, Sweden, July 10-15, 2018, pp. 1787-1796, 2018.
Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro.
Implicit regularization in matrix factorization. In Advances in Neural Information Processing
Systems, pp. 6151-6159, 2017.
Christina Heinze-Deml and Nicolai Meinshausen. Conditional variance penalties and domain shift
robustness. CoRR, abs/1710.11469, 2019. URL https://arxiv.org/abs/1710.11469.
Christina Heinze-Deml, Jonas Peters, and Nicolai Meinshausen. Invariant causal prediction for
nonlinear models. CoRR, abs/1706.08576, 2019. URL https://arxiv.org/abs/1706.
08576.
Maximilian Igl, Kamil Ciosek, Yingzhen Li, Sebastian Tschiatschek, Cheng Zhang, Sam Devlin,
and Katja Hofmann. Generalization in reinforcement learning with selective noise injection and
information bottleneck. In Advances in Neural Information Processing Systems 30: Annual
Conference on Neural Information Processing Systems 2019,Long Beach, CA, USA, 2019.
Arthur Jacot, Clement Hongler, and Franck Gabriel. Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in Neural Information Processing Systems 31:
Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, 3-8 December
2018, Montreal, Canada.,pp. 8580-8589, 2018.
12
Published as a conference paper at ICLR 2020
Nan Jiang, Alex Kulesza, Satinder Singh, and Richard Lewis. The dependence of effective planning
horizon on model accuracy. In Proceedings of the 2015 International Conference on Autonomous
Agents andMultiagent Systems, AAMAS 'l5,pp. 1181-1189, Richland, SC, 2015. International
Foundation for Autonomous Agents and Multiagent Systems. ISBN 978-1-4503-3413-6.
Yiding Jiang, Dilip Krishnan, Hossein Mobahi, and Samy Bengio. Predicting the generalization
gap in deep networks with margin distributions. CoRR, abs/1810.00113, 2018. URL http:
//arxiv.org/abs/1810.00113.
Niels Justesen, Ruben Rodriguez Torrado, Philip Bontrager, Ahmed Khalifa, Julian Togelius, and
Sebastian Risi. Procedural level generation improves generality of deep reinforcement learning.
CoRR, abs/1806.10729, 2018. URL http://arxiv.org/abs/1806.10729.
Sham M. Kakade, Karthik Sridharan, and Ambuj Tewari. On the complexity of linear prediction: Risk
bounds, margin bounds, and regularization. In Advances in Neural Information Processing Systems
21, Proceedings of the Twenty-Second Annual Conference on Neural Information Processing
Systems, Vancouver, British Columbia, Canada, December 8-11, 2008, pp. 793-800, 2008.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR,
abs/1412.6980, 2014.
Jaehoon Lee, Lechao Xiao, Samuel S. Schoenholz, Yasaman Bahri, Jascha Sohl-Dickstein, and
Jeffrey Pennington. Wide neural networks of any depth evolve as linear models under gradient
descent. CoRR, abs/1902.06720, 2019. URL http://arxiv.org/abs/1902.06720.
Tengyuan Liang, Tomaso A. Poggio, Alexander Rakhlin, and James Stokes. Fisher-rao metric,
geometry, and complexity of neural networks. In The 22nd International Conference on Artificial
Intelligence and Statistics, AISTATS 2019, 16-18 April 2019, Naha, Okinawa, Japan, pp. 888-896,
2019. URL http://proceedings.mlr.press/v89/liang19a.html.
Marlos C. Machado, Marc G. Bellemare, Erik Talvitie, Joel Veness, Matthew J. Hausknecht, and
Michael Bowling. Revisiting the arcade learning environment: Evaluation protocols and open
problems for general agents. J. Artif. Intell. Res., 61:523-562, 2018. doi: 10.1613/jair.5699.
Horia Mania, Aurelia Guy, and Benjamin Recht. Simple random search provides a competitive
approach to reinforcement learning. CoRR, abs/1803.07055, 2018.
David A McAllester. Pac-bayesian model averaging. In COLT, volume 99, pp. 164-170. Citeseer,
1999.
David A. McAllester. Simplified pac-bayesian margin bounds. In Computational Learning Theory
and Kernel Machines, 16th Annual Conference on Computational Learning Theory and 7th Kernel
Workshop, COLT/Kernel 2003, Washington, DC, USA, August 24-27, 2003, Proceedings, pp.
203-215,2003. doi: 10.1007∕978-3-540-45167-9∖,16. URL https://doi.org/1O.10O7/
978-3-540-45167-9_16.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan
Wierstra, and Martin A. Riedmiller. Playing atari with deep reinforcement learning. CoRR,
abs/1312.5602, 2013.
Vaishnavh Nagarajan and J Zico Kolter. Generalization in deep networks: The role of distance from
initialization. arXiv preprint arXiv:1901.01672, 2019.
Behnam Neyshabur. Implicit regularization in deep learning. CoRR, abs/1709.01953, 2017. URL
http://arxiv.org/abs/17O9.O1953.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the
role of implicit regularization in deep learning. ICLR, 2015a.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural
networks. In Conference on Learning Theory, pp. 1376-1401, 2015b.
13
Published as a conference paper at ICLR 2020
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring gener-
alization in deep learning. In Advances in Neural Information Processing Systems 30: Annual
Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach,
CA, USA, pp. 5949-5958, 2017.
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A pac-bayesian approach to spectrally-
normalized margin bounds for neural networks. In 6th International Conference on Learning
Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track
Proceedings, 2018a. URL https://openreview.net/forum?id=Skz_WfbCZ.
Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan Srebro. Towards
understanding the role of over-parametrization in generalization of neural networks. CoRR,
abs/1805.12076, 2018b. URL http://arxiv.org/abs/1805.12076.
Alex Nichol, Vicki Pfau, Christopher Hesse, Oleg Klimov, and John Schulman. Gotta learn fast: A
new benchmark for generalization in RL. CoRR, abs/1804.03720, 2018.
OpenAI. Openai five. CoRR, 2018. URL https://openai.com/blog/openai-five/.
Charles Packer, Katelyn Gao, Jemej Kos, PhiliPP Krahenbuhl, Vladlen Koltun, and DaWn Song.
Assessing generalization in deep reinforcement learning. CoRR, abs/1810.12282, 2018. URL
http://arxiv.org/abs/1810.12282.
Xue Bin Peng, Marcin AndrychoWicz, Wojciech Zaremba, and Pieter Abbeel. Sim-to-real transfer of
robotic control With dynamics randomization. In 2018 IEEE International Conference on Robotics
and Automation, ICRA 2018, Brisbane, Australia, May 21-25, 2018, PP. 1-8, 2018. doi: 10.1109/
ICRA.2018.8460528. URL https://doi.org/10.1109/ICRA.2018.8460528.
Lerrel Pinto, James Davidson, Rahul Sukthankar, and Abhinav GuPta. Robust adversarial reinforce-
ment learning. In Proceedings of the 34th International Conference on Machine Learning, ICML
2017, Sydney, NSW, Australia, 6-11 August 2017, PP. 2817-2826, 2017.
Aravind RajesWaran, Kendall LoWrey, Emanuel Todorov, and Sham M. Kakade. ToWards generaliza-
tion and simPlicity in continuous control. In Advances in Neural Information Processing Systems
30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017,
Long Beach, CA, USA, PP. 6553-6564, 2017.
Adam Santoro, Ryan Faulkner, David RaPoso, Jack W. Rae, Mike ChrzanoWski, TheoPhane Weber,
Daan Wierstra, Oriol Vinyals, Razvan Pascanu, and Timothy P. LillicraP. Relational recurrent
neural netWorks. In Advances in Neural Information Processing Systems 31: Annual Conference
on Neural Information Processing Systems 2018, NeurIPS 2018, 3-8 December 2018, Montreal,
Canada., PP. 7310-7321, 2018.
John Schulman, FiliP Wolski, Prafulla DhariWal, Alec Radford, and Oleg Klimov. Proximal Policy
oPtimization algorithms. CoRR, abs/1707.06347, 2017.
Samuel L. Smith, Pieter-Jan Kindermans, Chris Ying, and Quoc V. Le. Don’t decay the learning
rate, increase the batch size. In 6th International Conference on Learning Representations, ICLR
2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings, 2018. URL
https://openreview.net/forum?id=B1Yy1BxCZ.
FeliPe Petroski Such, Vashisht Madhavan, Rosanne Liu, Rui Wang, Pablo Samuel Castro, Yulun
Li, LudWig Schubert, Marc G. Bellemare, Jeff Clune, and Joel Lehman. An atari model zoo for
analyzing, visualizing, and comParing deeP reinforcement learning agents. CoRR, abs/1812.07069,
2018.
StePhen Tu and Benjamin Recht. The gaP betWeen model-based and model-free methods on the linear
quadratic regulator: An asymPtotic vieWPoint. In Conference on Learning Theory, COLT 2019,
25-28 June 2019, Phoenix, AZ, USA, PP. 3036-3083, 2019. URL http://proceedings.
mlr.press/v99/tu19a.html.
Vladimir N VaPnik and A Ya Chervonenkis. On the uniform convergence of relative frequencies of
events to their Probabilities. In Theory of probability and its applications, PP. 11-30. SPringer,
1971.
14
Published as a conference paper at ICLR 2020
Roman Vershynin. Lectures in geometric functional analysis. CoRR, 2009.
Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets, Michelle
Yeo, Aiireza Makhzani, Heinrich Kuttler, John Agapiou, Julian Schrittwieser, John Quan, StePhen
Gaffney, Stig Petersen, Karen Simonyan, Tom Schaul, Hado van Hasselt, David Silver, Timothy P.
Lillicrap, Kevin Calderone, Paul Keet, Anthony Brunasso, David Lawrence, Anders Ekermo,
Jacob Repp, and Rodney Tsing. Starcraft II: A new challenge for reinforcement learning. CoRR,
abs/1708.04782, 2017. URL http://arxiv.org/abs/1708.04782.
Huan Wang, Stephan Zheng, Caiming Xiong, and Richard Socher. On the generalization gap in
reparameterizable reinforcement learning. In Proceedings of the 36th International Conference on
Machine Learning, ICML2019, 9-15 June 2019, Long Beach, California, USA, pp. 6648-6658,
2019. URL http://proceedings.mlr.press/v97/wang19o.html.
Wenhao Yu, Jie Tan, C. Karen Liu, and Greg Turk. Preparing for the unknown: Learning a universal
policy with online system identification. In Robotics: Science and Systems XIII, Massachusetts In-
stitute of Technology, Cambridge, Massachusetts, USA, July 12-16, 2017, 2017. doi: 10.15607/RSS.
2017.XIII.048. URL http://www.roboticsproceedings.org/rss13/p48.html.
Amy Zhang, Nicolas Ballas, and Joelle Pineau. A dissection of overfitting and generalization in
continuous reinforcement learning. CoRR, abs/1806.07937, 2018a.
Amy Zhang, Yuxin Wu, and Joelle Pineau. Natural environment benchmarks for reinforcement
learning. CoRR, abs/1811.06032, 2018b.
Chiyuan Zhang, Oriol Vinyals, Remi Munos, and Samy Bengio. A study on overfitting in deep
reinforcement learning. CoRR, abs/1804.06893, 2018c. URL http://arxiv.org/abs/
1804.06893.
15
Published as a conference paper at ICLR 2020
A.1 Full Plots for LQR and fg-Gym
A.1.1 LQR
100000
90000
80000
70000
60000
50000
40000
sθnω> -Ie-Π6uω
Total Timesteps
(b)
(c)	(d)
(a)
-----Test, Width 32
-----Test, Width 64
-----Test, Width 128
Test, Width 256
Test, Width 512
Test, Width 1024
-----Test, Width 2048
-----Test, Width 4096
(e)
Figure A1: (a,b): Singular Values for varying depths and widths. (c,d): Train and Test Loss for
varying widths and depths. (e): Train and Test Loss for varying Noise Dimensions.
16
Published as a conference paper at ICLR 2020
We further verify that explicit regularization (norm based penalties) also reduces generalization gaps.
However, explicit regularization may be explained due to the bias of the synthetic tasks, since the
first layer’s matrix may be regularized to only ”view” the output of f, especially as regularizing the
first layer’s weights substantially improves generalization.
Figure A2: Explicit Regularization on layer norms.
We provide another deconvolution memorization test, using an LQR as the underlying MDP. While
fg-Gym-Deconv shows that memorization performance is dampened, this test shows that there can
exist specific hard limits to memorization. Specifically, NatureCNN can memorize 30 levels, but not
50; IMPALA can memorize 2 levels but not 5; IMPALA-LARGE cannot memorize 2 levels at all.
Figure A3: Deconvolution memorization test using LQR as underlying MDP.
17
Published as a conference paper at ICLR 2020
A.2	Extended Large RL Results
A.2.1 Large ImageNet Models for CoinRun
For reference, we also extend the case of large-parameter convolutional networks using ImageNet
networks. We experimentally verify in Table 1 that large ImageNet models perform very differently
in RL than SL. We note that default network with the highest test reward was IMPALA-LARGE-BN
(IMPALA-LARGE, with Batchnorm) at ≈ 5.5 test score.
In order to verify that this is inherently a feature learning problem rather than a combinatorial
problem involving objects, such as in (Santoro et al., 2018), we show that state-of-the-art attention
mechanisms for RL such as Relational Memory Core (RMC) using pure attention on raw 32 × 32
pixels does not perform well here, showing that a large portion of generalization and transfer must be
based on correct convolutional setups.
Architecture	Coinrun-100 (Train, Test)
AlexNet-v2	(10.0,3.0)二
CifarNet	(10.0,3.0)一
IMPALA- LARGE-BN		(10.0, 5.5)
IncePtion-ResNet-v2	(10.0, 6.5)-
Inception-v4	(10.0, 6.0)-
MobileNet-v1	(10.0, 5.5)-
MobileNet-v2	(10.0,5.5)一
NASNet- CIFAR	(10.0, 4.0)
NASNet- Mobile		(10.0, 4.5)
ResNet-v2-50	(10.0, 5.5)-
ResNet-v2-101	(10.0, 5.0)-
ResNet-v2-152	(10.0, 5.5)-
RMC32x32	(9.0, 2.5)
ShakeShake	(10.0, 6.0)-
VGG-A	(9.0, 3.0)
VGG-16	(9.0,3.0)一
Table 1: Raw Network Performance (rounded to nearest 0.5) on CoinRun, 100 levels. Images
scaled to default image sizes (32 × 32 or 224 × 224) depending on network input requirement. See
Appendix A.2.1 for training curves.
18
Published as a conference paper at ICLR 2020
We provide the training/testing curves for the ImageNet/large convolutional models used. Note the
following:
1.
2.
3.
RMC32x32 projects the native image from CoinRun from 64 × 64 to 32 × 32, and uses all
pixels as components for attention, after adding the coordinate embedding found in (Santoro
et al., 2018). Optimal parameters were (mem_slots = 4, head_size = 32, num_heads = 4,
num_blocks = 2, gate_style = 'memory’).
Auxiliary Loss in ShakeShake was not used during training, only the pure network.
VGG-A is a similar but slightly smaller version of VGG-16.
Train and Test Performances of Special Architectures on CoinRun-100
①。UElWo七①CL
Normalized TimeSteps
Figure A4: Large Architecture Training/Testing Curves (Smoothed).
19
Published as a conference paper at ICLR 2020
A.2.2 Do State-Action Margin Distributions Predict Generalization in RL?
A key question is how to predict the generalization gap only from the training phase. A particular set
of metrics, popular in the SL community are margin distributions (Jiang et al., 2018; Bartlett et al.,
2017), as they deal with the case for softmax categorical outputs which do not explicitly penalize
the weight norm of a network, by normalizing the ”confidence” margin of the logit outputs. While
using margins on state-action pairs (from an on-policy replay buffer) is not technically rigorous, one
may be curious to see if they have predictive power, especially as MLP’s are relatively simple to
norm-bound, and as seen from the LQR experiments, the norm of the policy may be correlated with
the generalization performance.
For a policy, the the margin distribution will be defined as (x, y) →
Fn (X)y ―maxi=y Fn (X) i
Rn kSk2∕n
, where
Fπ (x)y is the logit value (before applying softmax) of output y given input x, and S is the matrix
of states in the replay buffer, and Rπ is a norm-based Lipschitz measure on the policy network
logits. In general, Rπ is a bound on the Lipschitz constant of the network but can also be simply
expressions which allow the margin distribution to have high correlation with the generalization
gap. Thus, we use measures inspired by recent literature in SL in which we designate Spectral-L1,
Distance, and Spectral-Frobenius measures for Rπ , and we replace the classical supervised learning
pair (x, y) = (s, a) with the state-action pairs found on-policy. 2
The expressions for Rπ (after removing irrelevant constants) are as follows, with their analogous
papers:
1.
2.
3.
Spectral-L1 measure:
Distance measure:
(n3kWik)(p3 " )3/2
(Bartlett et al., 2017)
JPd=IkWi- W0kF (Nagarajan & Kolter, 2019)
Spectral-Fro measure:
jn(d) Qd=IkWik2 Pd=IkWjWWOkF
(Neyshabur et al., 2018a)
We verify in Figure A5, that indeed, simply measuring the raw norms of the policy network is a poor
way to predict generalization, as it generally increases even as training begins to plateau. This is
inherently because the softmax on the logit output does not penalize arbitrarily high logit values, and
hence proper normalization is needed.
The margin distribution converges to a fixed distribution even long after training has plateaued.
However, unlike SL, the margin distribution is conceptually not fully correlated with RL generalization
on the total reward, as a policy overconfident in some state-action pairs does not imply bad testing
performance. This correlation is stronger if there are Lipschitz assumptions on state-action transitions,
as noted in (Wang et al., 2019). For empirical datasets such as CoinRun, a metric-distance between
transitioned states is ill-defined however. Nevertheless, the distribution over the on-policy replay
buffer at each policy gradient iteration is a rough measure of overall confidence.
Marain Distributions across time. 2-Laver Width 1024
Spectral-Ll
Spectral-Fro
Figure A5: Margin Distributions across training.
We note that there are two forms of modifications, network dependent (explicit modifications to the
policy - norm regularization, dropout, etc.) and data dependent (modifications only to the data in the
replay buffer - action stochasticity, data augmentation, etc.). Ultimately however, we find that current
2We removed the training sample constant m from all original measures as this is ill-defined for the RL case,
when one can generate infinitely many (s, a) pairs. Furthermore, we used the original kWi k1 in the numerator
found in the first version of (Bartlett et al., 2017) rather than the current kWi k1 2.
20
Published as a conference paper at ICLR 2020
norm measures Rπ become too dominant in the fraction, leading to the monotonic decreases in the
means of the distributions as We increase parametrization.
Figure A6: Margin Distributions at the end of training.
A.2.3 Gym-Retro (Sonic)
In the Gym-Retro benchmark using Sonic (Nichol et al., 2018), the agent is given 47 training levels
With reWards corresponding to increases in horizontal location. The policy is trained until 5k reWard.
At test time, 11 unseen levels are partitioned into starting positions, and the reWards are measured
and averaged.
We briefly mention that the agent strongly overfits to the scoreboard (i.e. an artifact correlated With
progress in the level), which may be interpreted as part of the output of gθ(∙). In fact, the agent is still
able to train to 5k reWard from purely observing the timer as the observation. By blacking out this
scoreboard with a black rectangle, we see an increase in test performance.
Settings	IMPALA	NatureCNN
Blackout	1250 ± 40	-1141 ± 40
NoBlackout	1130 ± 40	1052 ± 40
Table 2: IMPALA vs NatureCNN test rewards, with and without Blackout.
A.3 Hyperparameters and Exact Setups
A.3.1 Exact infinite LQR
For infinite horizon case, see (Fazel et al., 2018) for the the full solution and notations. Using the
same notation (A, B, Q, R), denote C(K) = Pχo〜D XTPKxo as the cost and Ut = -Kxt as the
21
Published as a conference paper at ICLR 2020
policy, where PK satisfies the infinite case for the Lyapunov equation:
PK = Q+KTRK+ (A-BK)TPK(A-BK)	(3)
We may calculate the precise LQR cost by vectorizing (i.e. flattening) both sides’ matrices and using
the Kroncker product a which leads to a linear regression problem on PK, which has a precise
solution, implementable in TensorFlow:
VeC(PK) = VeC(Q) + VeC(KTRK) + [(A - BK)T 0 (A - BK)T)] VeC(PK)	(4)
[In2 - (A - BK)T 0 (A - BK)T] VeC(PK) = vec(Q) + vec(KTRK)	(5)
Parameter	Generation
A B Q R n Ki ∀i	Orthogonal initialization, scaled 0.99 In In In 10 Orthogonal Initialization, scaled 0.5
Table 3: Hyperparameters for LQR
A.3.2 Projection Method
The basis for producing f, gθ outputs is due to using batch matrix multiplication operations, or”BMV”,
where the same network architecture uses different network weights for each batch dimension, and
thus each entry in a batchsize of B will be processed by the same architecture, but with different
network weights. This is to simulate the effect ofgθi. The numeric ID i of the enVironment is used as
an index to collect a specific set of network weights θi from a global memory of network weights (e.g.
using tensorflow.gather). We did not use nonlinear actiVations for the BMV architectures, as
they did not change the outcome of the results.
Architecture	Setup	
BMV-DeConV	(filtersize = 2, stride = 1, outchannel = 8, padding = (filtersize = 4, stride = 2, outchannel = 4, padding = (filtersize = 8, stride = 2, outchannel = 4, padding = (filtersize = 8, stride = 3, outchannel = 3, padding =	”VALID”) ”VALID”) ”VALID”) “VALID”)
BMV-DenSl	f : Dense 30, g : Dense 100	
A.3.3 ImageNet Models
For the networks used in the superVised learning tasks, we direct the reader to the following
repository: https://github.com/tensorflow/models/blob/master/research/
slim/nets/nets_factory.py. We also used the RMC: deepmind/sonnet/blob/
master/sonnet/python/modules/relational_memory.py
A.3.4 PPO Parameters
For the projected gym tasks, we used for PPO2 Hyperparameters:
22
Published as a conference paper at ICLR 2020
PPO2 Hyperparameters	Values
nsteps	2048
nenvs	T6
nminibatches	^64
λ	^095
Y	∙	^099
noptepochs	^10
entropy	0.0
learning rate	3∙10-4
vf coeffiicent	^03
max-grad-norm	0.5
total time steps	Varying
See (Cobbe et al., 2018) for the default parameters used for CoinRun. We only varied nminibatches in
order to fit memory onto GPU. We also did not use RNN additions, in order to measure performance
only from the feedforward network - the framestacking/temporal aspect is replaced by the option to
present the agent velocity in the image.
A.4	Theoretical (LQR)
In this section, we use notation consistent with (Fazel et al., 2018) for our base proofs. However, in
order to avoid confusion with a high dimensional policy K we described in 3.1, we denote our low
dimensional base policy as P and state as st rather than xt .
A.4. 1 Notation and Setting
Let k∙k be the spectral norm of a matrix (i.e. largest singular value). Suppose C(P) was the infinite
horizon cost for an (A, B, Q, R)-LQR where action at = -P ∙ st, St is the state at time t, state
transition is st+ι = A ∙ St + B ∙ at, and timestep cost is STQst + aTRat.
C(P) for an infinite horizon LQR, while known to be non-convex, still possess the property that
when VC (P *) = 0, P * is a global minimizer, or the problem statement is rank deficient. To
ensure that our cost C(P) always remains finite, we restrict our analysis when P ∈ P, where
P = {P : kPk ≤ α and kA - BP k ≤ 1} for some constant α, by choosing A, B and the
initialization of P appropriately, using the hyperparameters found in A.3.1. We further define the
observation modified cost as C(K; Wθ) = C K WWc	.
A.4.2 Smoothness B ounds
As described in Lemma 16 of (Fazel et al., 2018), we define
∞
TP(X) = X(A - BP)tX[(A - BP)T]t
and ∣∣T1p ∣∣ = SuPX TPXX) over all non-zero symmetric matrices X.
(6)
Lemma 27 of (Fazel et al., 2018) provides a bound on the difference C(P0) - C(P) for two different
policies P, P0 when LQR parameters A, B , Q, R are fixed. During the derivation, it states that when
IlP - P0k ≤ min (4c(p)kBinAQ-Bpk + 1)，kPk),then:
C(P0)-C(P)≤2∣TP∣ (2∣P∣ ∣R∣ ∣P0-P∣ +∣R∣ ∣P0-P∣2)+
2∣TP∣22∣B∣ (∣A-BP∣ +1)∣P-P0∣ ∣P ∣2 ∣R∣
(7)
Lemma 17 also states that:
∣TP ∣ ≤
C(P)
μσmiη (Q)
(8)
23
Published as a conference paper at ICLR 2020
where
μ = σmin (ExO 〜D [x0x0 ])
(9)
Assuming that in our problem setup, x0, Q, R, A, B were fixed, this means many of the parameters
in the bounds are constant, and thus we conclude:
C(P0)-C(P)≤O C(P)2 kPk2kP-P0k(kA-BPk+kBk+1)+kPkkP-P0k2
(10)
Since we assumed kA - BP k ≤ 1 or else TP(X) is infinite, we thus collect the terms:
C(P0)-C(P)≤OC(P)2hkPk2kP-P0k+kPkkP-P0k2i	(11)
Since α is a bound on kP k for P ∈ P, note that
kPk2kP-P0k+kPkkP-P0k2=kP-P0k(kPk2+kPk+kP-P0k)	(12)
≤kP-P0k(kPk2+kPk(kPk+kP0k)≤(3α2)kP-P0k	(13)
From (11), this leads to the bound:
C (P0) - C(P) ≤ O (C(P )2 kP - P0k)	(14)
Note that this directly implies a similar bound in the high dimensional observation case - in particular,
ifP=KWWθc	andP0=KWWθc	thenkP-P0k	≤	kK-K0kWWθc	=kK-K0k.
A.4.3 Gradient Dynamics in 1-Step LQR
We first start with a convex cost 1-step LQR toy example under this regime, which shows that linear
0T
components such as β W cannot be removed from the policy by gradient descent dynamics to
improve generalization. To shorten notation, let Wc ∈ Rn×n and Wθ ∈ Rp×n , where n p. This is
equivalent to setting dsignal = dstate = n and dnoise = p, and thus the policy K ∈ Rn×(n+p) .
In the 1-step LQR, We allow so 〜N(0,I), ao = K Wc so and si = so + ao with cost 11 ∣∣sι∣∣2,
then
C(K；Wθ)=Eso 2卜。+K[Wc]χo∣	=1∣i+K阳]	(15)
and
VC(K ； Wθ)= (I+K]Wc]) ]Wc
(16)
Define the population cost as C(K) := EWθ [C(K； Wθ)]. Let the notation O(p, n) denote the
following set of orthogonal matrices:
O(p, n) = {W ∈ Rp×n : WTW = I} .
We use the shorthand O(n) = O(n, n).
Proposition 1. Suppose that Wθ 〜Unif(O(p,n)) and Wc 〜Unif(O(n)). Then
(i)	The minimizer of C(K) is unique and given by K? = -WcT 0 .
(ii)	Thus, the minimizer cost is C(K?) = 0.
24
Published as a conference paper at ICLR 2020
Proof. By standard properties of the Haar measure on O(p, n), we have that E[Wθ] = 0 and
E[Wθ WfT] = PI. Therefore,
C(K)
+ WcT	0 .
K
K
I0
0	PI
We can now differentiate C(K):
▽C(K) = K
Wc
Wθ
Wc
0
Both claims now follow.
□
A.4.3. 1 Finite S ample Generalization Gap
As an instructive example, we consider the case when we only possess one sample Wθ. Note that if
K= K0+β
0
Wθ
T
,then VC(K; Wθ) = VC(K0; Wθ) 十 β
0
Wθ
T
. In particular, if we perform
gradient descent dynamics Kt+1 = Kt - ηVC(Kt; Wθ), then we have
Kt = Ko(I - ηM)t + B(I - ηM)t-1 + ... + B
(17)
where M
Wθ	Wθ
T
is the Hessian of C(K; Wθ) and B = -η
Wc
Wθ
. Note that M has rank
at most n n + p, and thus at a high level, K0(I - ηM)t does not diminish if some portion of K0
lies in the orthogonal complement of the range of M. If the initialization is Ko 〜N(0, I), then it is
highly likely we can find a subspace Q ⊆ range(M)⊥ for which K0Q 6= 0.
This is a specific example of the general case where if the Hessian of a function f (x) is degenerate
everywhere (e.g. has rank k < n), then an x0 initialized with e.g. an isotropic Gaussian distribu-
tion cannot converge under gradient descent to a minimizer that lives in the span of the Hessian,
as the non-degenerate components do not change. In particular, Proposition 4.7 in (Vershynin,
2009) points to the exact magnitude of the non-degenerate component in the relevant subspace Q:
Ex〜N(0,I) IjlPrOjQ(X)
n — k
n
The generalization gap may decrease if the number of level samples is high enough. This can
. . ^ ................................_ -l _______ _. , ____ 、 . <^> -l ___________ _ _
be seen by the sample Hessian of C(K) = -m Pj=I C(K; Wfi) being M =* Pi=ι Mi where
Mi
Wθi	Wθi
T
∀i. In particular, as m increases, the rank of Mc increases, which allows
gradient descent to recover the minimizer K? better.
We calculate the exact error of gradient descent on m samples of the 1-step LQR problem.
To simplify notation, we rewrite Wθi as Wi , and interchangeably use the abbreviations for the finite
Cm(K) = Cm(∙; {Wi}) = ml Pmm=I C(K； Wi) when necessary. We consider gradient descent
Kt+1 = Kt - ηVCm (Kt) starting from a random Ko with each entry iid N(0, ψ2) and η sufficiently
small so that gradient descent converges. Let K∞ denote the limit point of gradient descent. We
prove the following generalization gap between K∞ and Kopt .
Theorem 1. Suppose that n divides p. Fix an m ∈ {1, ..., p/n}. Suppose that the samples
{Wι,…，Wm} are generated by first sampling a W 〜 Unif(O(P)), and then partitioning the
first n ∙ m columns of W into Wι,…,Wm. Suppose gradient descent is run with a sufficiently small
step size η so that it converges to a unique limit point K∞. The limit point K∞ has the following
generalization error:
E[C(K∞)]
n	m2n
n2m
—+ —；---------TT + --；-------77
2	2(m + 1)2	2p(m + 1)2
'-------------------V---------
=:E1
m
―TTn +
m+1
ψ2 n2	m + 2 m2 n
2 m+1 m+1p
、--------V---------}
=:E2
—
—
25
Published as a conference paper at ICLR 2020
Here, the expectation is over the randomness of the samples {Wi}im=1 and the initalization K0. The
contribution from E1 is due to the generalization error of the minimum-norm stationary point of
Cm(∙; {Wi}) ∙ The contribution from E? is due to thefull-rank initialization of Ko.
Figure A7: Plot of E[C(K∞)] as a function of m,p, n with elsewhere fixed default values n = 10,
p = 1000, m = 10, and ψ = 1.
We remark that our proof specifically relies on the rank of the Hessian as m increases, rather than
a more common concentration inequality used in empirical risk minimization arguments, which
leads to a √m scaling. Furthermore, the above expression for E[C(K∞)] does not scale increasingly
with poly(p) for the convex 1-Step LQR case, while empirically, the non-convex infinite LQR case
does indeed increase from increasing the noise dimension p (as shown in Section 3.1). Interestingly,
this suggests that there is an extra contribution from the non-convexity of the cost, where the
observation-modified gradient dynamics tends to reach worse optima.
A.4.3.2 Proof of Theorem 1
Fix integers n,p with p ≥ n and suppose that n divides p. Draw a random W ∈ O(p) uniformly from
the Haar measure on O(p) and divide W column-wise into W1 , W2, ..., Wp/n (that is Wi ∈ Rp×n,
WiT Wi = In and WiT Wj = 0 when i 6= j). Also draw a Wc ∈ O(n) uniformly and independent
from W. Now We consider the matrix Zm ∈ Rm∙n×(n+p) for m = 1, ...,p/n defined as:
WcT
W1T
Zm :
WcT
WmT
Proposition 2. We have that Zm is given as:
1
1
m+1 Wc
Wi - m+1 Pi=ι Wi
m W_
m+1 Wm
FWc
m+1	c
-	1 P
m+1 乙.
i6=m Wi
26
Published as a conference paper at ICLR 2020
Proof. Using the fact that WcT Wc = I, WiT Wi = In , and WiT Wj = 0 for i 6= j, We can compute:
In
In
T
ZmZm = Im∙n +
In
In
By the matrix inversion formula We can compute the inverse (ZmZmT )-1 as the m × m block matrix
where the diagonal blocks are mmɪI and the off-diagonal blocks are 一 m^ɪI. We can now write Zm
using the formula:
Zm=Zm (ZmZm)-1
The result is seen to be:
m++1 Wc
一mmɪ W1 - m+1 Pi=I Wi
i+11 Wc
mmɪ Wm - m+1 Pi=m Wi
□
Proposition 3. We have that:
PZT
m
「mmɪ WcWT
[m⅛ Pi=ι Wi Wcτ
m+1 Pi=1 Wc WiT	]
mmɪ Pi=ι WiWiT - m+ι Pi=j WiWT一 ,
Proof. We use the formula PZT
previous proposition.
ZmT (ZmZmT )-1Zm, combined With the calculations in the
□
Now we consider the gradient of Cm :
m
VCm(K) = m X
i=1
T	1m
+ K -1X
∖n i=1
Setting PCm(K) = 0, We see that minimizers of Cm are solutions to:
K
m
- X
i=1
T
T
Using our notation above, this is the same as:
KZmTZm
—
The minimum norm solution, denoted Kmin , to this equation is:
Kmin
[In …In HZ* A
—
Next, we recall that C(K) = E[Cm(K)] is:
C(K) = 2 + 2tr
I0
+ tr K
Wc
0
0
n I
p
Our next goal is to compute E[C(Kmin)]. First, we observe that:
Wc
0
—
Wc
0
tr Kmin
Wc
0
—
ɪ tr(WTWc)
n+1 c
n
----n .
n+1
27
Published as a conference paper at ICLR 2020
Next, defining Bi ：=后 Wi
—
1
m+1
Kmin Kmin
I
I
0
inKmin
*
(m+1)2(Pim=1Wi)(Pim=1Wi)T
Therefore:
tr
---------Ln +	------------
(m + 1)2	p(m + 1)2
(nm) .
0
n I
p
2
m
n
Putting everything together, we have that:
C(Kmin)
m2n
n2 m
2 + 2(m + 1)2 + 2p(m + 1)2	m + 11rn
n
m
Notice that this final value is not a function of the actual realization of W .
We now consider the second source of error, which comes from the initialization K0. Recall that each
entry of K0 is drawn iid from N (0, ψ2).
Let us consider the gradient descent dynamics:
Kt+1 = Kt- NCm(Kt).
Unrolling the dynamics under our notation:
t-1
Kt = Ko (I	-	(η∕m)ZmZm) -	(η∕m)	[In	∙∙.	In]	Zm	£(I - (η∕m)ZmZm)k	.
k=0
It is not hard to see that for η sufficiently small, as t → ∞
t-1
lim 一(η∕m)[In ... In] Zm X(I - (η∕m)ZmZm)k = Kmin .
t→∞
k=0
Hence:
K∞ =K0(I- (η∕m)ZmT Zm)∞ + Kmin .
Call the matrix M∞ := (I - (η∕m)ZmT Zm)∞. We observe that:
C(K∞) = 2 + 2 tr (m∞K0KoM∞
I0
0 n I
+ 1 tr ((M∞KTKmm + KminKoM∞)
I
0
0
n I
p
+ 2 tr (KminKmin
I0
0	P I
+ tr K0M∞
Wc
0
+ tr KminM∞
Wc
0
Noticing that K0 is independent of Zm , taking expectations
E[C(K∞)] = E[C(Kmin)] + 1Etr (m∞K0KoM∞
I
0
0
n I
p
E[C(Kmin)] + ψ2nEtr (m∞M∞
0
n I
p
0
28
Published as a conference paper at ICLR 2020
Above, we use the fact that E[K0TK0] = ψ2nIn+p. Now it is not hard to see that for η sufficiently
m
small, we have that M∞ = I - PZT . On the other hand,
m.
using E[WiWjT] = 0, we have that
E[PZmT]
-m-I
m+11
0
0
m2 n I
m+1 P
Therefore:
E[M∞T M∞]
(1 - m/(m + 1))I
0
0
(1 - m2n/((m + 1)p)I .
Plugging in to the previous calculations, this yields
E[C(K∞)] = E[C(Kmin)] +
ψ2n2	m + 2 m2 n
—
2 m+1 m+1p
29