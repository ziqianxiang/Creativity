Published as a conference paper at ICLR 2020
Data-Independent Neural Pruning
via Coresets
Ben Mussay
Computer Science Department
University of Haifa
Haifa, Israel
bengordoncshaifa@gmail.com
Vladimir Braverman
Computer Science Department
Johns Hopkins University
Baltimore, MD., USA
vova@cs.jhu.edu
Dan Feldman
Computer Science Department
University of Haifa
Haifa, Israel
dannyf.post@gmail.com
Margarita Osadchy
Computer Science Department
University of Haifa
Haifa, Israel
rita@cs.haifa.ac.il
Samson Zhou
Computer Science Department
Carnegie Mellon University
Pittsburgh, IN., USA
samsonzhou@gmail.com
Ab stract
Previous work showed empirically that large neural networks can be significantly
reduced in size while preserving their accuracy. Model compression became a
central research topic, as it is crucial for deployment of neural networks on de-
vices with limited computational and memory resources. The majority of the
compression methods are based on heuristics and offer no worst-case guarantees
on the trade-off between the compression rate and the approximation error for an
arbitrarily new sample.
We propose the first efficient, data-independent neural pruning algorithm with a
provable trade-off between its compression rate and the approximation error for
any future test sample. Our method is based on the coreset framework, which
finds a small weighted subset of points that provably approximates the original
inputs. Specifically, we approximate the output of a layer of neurons by a coreset
of neurons in the previous layer and discard the rest. We apply this framework in
a layer-by-layer fashion from the top to the bottom. Unlike previous works, our
coreset is data independent, meaning that it provably guarantees the accuracy of
the function for any input x ∈ Rd , including an adversarial one. We demonstrate
the effectiveness of our method on popular network architectures. In particular,
our coresets yield 90% compression of the LeNet-300-100 architecture on MNIST
while improving classification accuracy.
1	Introduction
Neural networks today are the most popular and effective instrument of machine learning with nu-
merous applications in different domains. Since Krizhevsky et al. (2012) used a model with 60M
parameters to win the ImageNet competition in 2012, network architectures have been growing
wider and deeper. The vast overparametrization of neural networks offers better convergence (Allen-
Zhu et al., 2019) and better generalization (Neyshabur et al., 2018). The downside of the over-
parametrization is its high memory and computational costs, which prevent the use of these networks
in small devices, e.g., smartphones. Fortunately, it was observed that a trained network could be re-
duced to smaller sizes without much accuracy loss. Following this observation, many approaches to
1
Published as a conference paper at ICLR 2020
compress existing models have been proposed (see Gale et al. (2019) for a recent review on network
sparsification, and Mozer & Smolensky (1989); Srivastava et al. (2014); Yu et al. (2018); He et al.
(2017) for neural pruning).
Although a variety of model compression heuristics have been successfully applied to different neu-
ral network models, such as Jacob et al. (2018); Han et al. (2015); Alvarez & Salzmann (2017), these
approaches generally lack strong provable guarantees on the trade-off between the compression
rate and the approximation error. The absence of worst-case performance analysis can poten-
tially be a glaring problem depending on the application. Moreover, data-dependent methods for
model compression (e.g., Mozer & Smolensky (1989); Srivastava et al. (2014); Hu et al. (2016); Yu
et al. (2018); Baykal et al. (2018)) rely on the statistics presented in a data set. Hence, these methods
are vulnerable to adversarial attacks (Szegedy et al., 2014), which design inputs that do not follow
these statistics.
Ideally, a network compression framework should 1) provide provable guarantees on the trade-
off between the compression rate and the approximation error, 2) be data independent, 3) provide
high compression rate, and 4) be computationally efficient. To address these goals, we propose
an efficient framework with provable guarantees for neural pruning, which is based on the existing
theory of coresets such as (Braverman et al., 2016). Coresets decrease massive inputs to smaller
instances while maintaining a good provable approximation of the original set with respect to a given
function. Our main idea is to treat neurons of a neural network as inputs in a coreset framework.
Specifically, we reduce the number of neurons in layer i by constructing a coreset of neurons in
this layer that provably approximates the output of neurons in layer i + 1 and discarding the rest.
The coreset algorithm provides us with the choice of neurons in layer i and with the new weights
connecting these neurons to layer i+ 1. The coreset algorithm is applied layer-wise from the bottom
to the top of the network.
The size of the coreset, and consequently the number of remaining neurons in layer i, is provably
related to the approximation error of the output for every neuron in layer i + 1. Thus, we can
theoretically derive the trade-off between the compression rate and the approximation error of any
layer in the neural network. The coreset approximation of neurons provably holds for any input;
thus our compression is data-independent.
Similar to our approach, Baykal et al. (2018) used coresets for model compression. However, their
coresets are data-dependent; therefore, they cannot guarantee robustness over inputs. Moreover, they
construct coresets of weights, while our approach constructs coresets of neurons. Neural pruning
reduces the size of the weight tensors, while keeping the network dense. Hence the implementation
of the pruned network requires no additional effort. Implementing networks with sparse weights
(which is the result of weight pruning) is harder and in many cases does not result in actual compu-
tational savings.
Our empirical results on LeNet-300-100 for MNIST (LeCun et al., 1998) and VGG-16 (Simonyan
& Zisserman, 2014) for CIFAR-10 (Krizhevsky, 2009) demonstrate that our framework based on
coresets of neurons outperforms sampling-based coresets by improving compression without sacri-
ficing the accuracy. Finally, our construction is very fast; it took about 56 sec. to compress each
dense layer in the VGG-16 network using the platform specified in the experimental section.
Our Contributions: We propose an efficient, data-independent neural pruning algorithm with a
provable trade-off between the compression rate and the output approximation error. This is the first
framework to perform neural pruning via coresets. We provide theoretical compression rates for
some of the most popular neural activation functions summarized in Table 2.
2	Related Work
2.1	Coresets
Our compression algorithm is based on a data summarization approach known as coresets. Over the
past decade, coreset constructions have been recognized for high achievements in data reduction in
a variety of applications, including k-means, SVD, regression, low-rank approximation, PageRank,
convex hull, and SVM; see details in Phillips (2016). Many of the non-deterministic coreset based
2
Published as a conference paper at ICLR 2020
methods rely on the sensitivity framework, in which elements of the input are sampled according
to their sensitivity (Langberg & Schulman, 2010; Braverman et al., 2016; Tolochinsky & Feldman,
2018), which is used as a measure of their importance. The sampled elements are usually reweighted
afterwards.
2.2	Model Compression
State-of-the-art neural networks are often overparameterized, which causes a significant redundancy
of weights. To reduce both computation time and memory requirements of trained networks, many
approaches aim at removing this redundancy by model compression.
Weight Pruning: Weight pruning was considered as far back as 1990 (LeCun et al., 1990), but has
recently seen more study (Lebedev & Lempitsky, 2016; Dong et al., 2017). One of the most popular
approaches is pruning via sparsity. Sparsity can be enforced by L1 regularization to push weights
towards zero during training (Hu et al., 2016). However, it was observed (Han et al., 2015) that after
fine-tuning of the pruned network, L2 regularized network outperformed L1, as there is no benefit
to pushing values towards zero compared to pruning unimportant (small weight) connections.
The approach in Denton et al. (2014) exploits the linearity of the neural network by finding a low-
rank approximation of the weights and keeping the accuracy within 1% of the uncompressed model.
Jacob et al. (2018) performs quantization of the neural network’s weights and suggests a new training
procedure to preserve the model accuracy after the quantization.
These methods showed high compression rates, e.g., the compression rate of AlexNet can reach 35x
with the combination of pruning, quantization, and Huffman coding (Han et al., 2016). Nevertheless,
strong provable worst-case analysis is noticeably absent for most weight pruning methods.
Neural pruning: Weight pruning leads to an irregular network structure, which needs a special
treatment to deal with sparse representations, making it hard to achieve actual computational sav-
ings. On the other hand, neural pruning (Hu et al., 2016) and filter pruning in CNNs (e.g, Zhuang
et al. (2018); Li et al. (2017); Liu et al. (2017) simply reduce the size of the tensors.
The method in Hu et al. (2016) first identifies weak neurons by analyzing their activiations on a
large validation dataset. Then those weak neurons are pruned and the network is retrained. The
processes are repeated several times. Zhuang et al. (2018) introduces channel pruning based on
the contribution to the discriminative power. These methods are data-dependent; thus they cannot
provide guarantees of approximation error for any future input.
Li et al. (2017) measures the importance of channels by calculating the sum of absolute values of
weights. Other channel pruning methods either impose channel-wise sparsity in training, followed
by pruning channels with small scaling factors, and fine-tuning (e.g, Liu et al. (2017)) or perform
channel pruning by minimizing the reconstruction error of feature maps between the pruned and
pre-trained model (e.g., He et al. (2017).) These methods lack provable guarantees on the trade-offs
between their accuracy and compression.
Coreset-Based Model Compression Similar to our work, the approach in Baykal et al. (2018)
uses corests for model compression. However, they construct coresets of weights, while we con-
struct coresets of neurons. Their approach computes the importance of each weight, which is termed
sensitivity, using a subset from the validation set. The coreset is chosen for the specific distribution
(of data) so consequently, the compressed model is data-dependent. In our construction, the input
of the neural network is assumed to be an arbitrary vector in Rd and the sensitivity of a neuron is
computed for every input in Rd. This means that we create a data-independent coreset; its size is in-
dependent of the properties of the specific data at hand, and the compression provably approximates
any future test sample.
Dubey et al. (2018) builds upon k-means coresets by adding a sparsity constraint. The weighting
of the filters in the coreset is obtained based on their activation magnitudes over the training set.
The compression pipeline also includes a pre-processing step that follows a simple heuristic that
eliminates filters based on the mean of their activation norms over the training set. This construction
is obviously data-dependent and it uses corsets as an alternative mechanism for low-rank approxi-
mation of filters.
3
Published as a conference paper at ICLR 2020
(a)
(b)
Figure 1: Illustration of our neuron coreset construction on a toy example: (a) a full network, (b) the
compressed network. Both neurons in the second layer in (b) choose the same coreset comprising
neurons {1, 2, 3, 7} from layer 1, but with different weights. The compressed network has pruned
neurons {2, 5, 6} from layer 1.
3	Method
We propose an algorithm for compressing layer i and we apply it to all layers from the bottom to the
top of the network. We first give an intuitive description of the algorithm. We then formalize it and
provide a theoretical analysis of the proposed construction.
3.1	Data-Independent Coreset for Neural Pruning
Let aij = φ(pjT x) be the jth neuron in layer i, in which pj denotes its weights, and x denotes an
arbitrary input in Rd (see Figure 1, top). We first consider a single neuron in layer i + 1. The
linear part of this neuron is Z = PjP=I W(Pj)φ(p∕x). We would like to approximate Z by Z =
Pi∈j* U(Pl)φ(px) where J* ⊂ {1,…，|P∣} is a small subset, and We want this approximation
to be bounded by a multiplicative factor that holds for any x ∈ Rd . Unfortunately, our result in
Theorem 6 shows that this idealized goal is impossible. However, we show in Theorem 7 and
Corollary 8 that we can construct a small coreset C, such that |z - Z| ≤ ε for any input X ∈ Rd.
Algorithm 1 summarizes the coreset construction for a single neuron with an activation function φ,
(our results for common neural activation functions are summarized in Table 2). Algorithm 2 and
Corollary 9 show the construction of a single coreset with possibly different weights for all neurons
in layer i + 1 (see Figure 1, bottom).
3.2	Preliminaries
Definition 1 (weighted set). Let P ⊂ Rd be a finite set, and w be a function that maps every P ∈ P
to a weight w(P) > 0. The pair (P, w) is called a weighted set.
4
Published as a conference paper at ICLR 2020
Algorithm 1: CORESET(P, w, m, φ, β)
Input:	A weighted set (P, w),
an integer (sample size) m ≥ 1,
an (activation) function φ : R → [0, ∞),
an upper bound β > 0.
Output: A weighted set (C, u); see Theorem 7 and Corollary 8.
1
2
3
4
5
6
7
8
9
for every p ∈ P do
/_	W(P)φ(βkPk)
P(P) L Pq∈P w(q)φ(βkqk)
u(P) := 0
C JQ
for m iterations do
Sample a point q from P such that P ∈ P is chosen with probability pr(P).
C := C ∪{q}
w(q)
u(q) := u(q) +------L
m ∙ pr(q)
return (C, u)
A coreset in this paper is applied on a query space which consists of an input weighted set, an
objective function, and a class of models (queries) as follows.
Definition 2 (Query space). Let P0 = (P, w) be a weighted set called the input set. Let X ⊆ Rd be
a set, and f : P × X → [0, ∞) be a loss function. The tuple (P, w, X, f) is called a query space.
Given a set of points P and a set of queries X, a coreset of P is a weighted set of points that
provides a good approximation to P for any query x ∈ X. We state the definition of coresets with
multiplicative guarantees below, though we shall also reference coresets with additive guarantees.
Definition 3 (ε-coreset, multiplicative guarantee). Let (P, w, X, f) be a query space, and ε ∈ (0, 1)
be an error parameter. An ε-coreset of (P, w, X, f) is a weighted set (Q, u) such that for every
x∈X
w(P)f (P, x) -	u(q)f (q, x) ≤ ε	w(P)f (P, x)
p∈P	q∈Q	p∈P
The size of our coresets depends on two parameters: the complexity of the activation function which
is defined below, and the sum of a supremum that is defined later. We now recall the well-known
definition of VC dimension (Vapnik & Chervonenkis, 2015) using the variant from (Feldman &
Langberg, 2011).
Definition 4 (VC-dimension (Feldman & Langberg, 2011)). Let (P, w, X, f) be a query space.
For every x ∈ Rd, and r ≥ 0 we define rangeP,f (x, r) := {P ∈ P | f(P, x) ≤ r} and
ranges(P, X, f) := C ∩ rangeP,f (x, r) | C ⊆ P, x ∈ X, r ≥ 0 . For a set ranges of subsets of
Rd, the VC-dimension of (Rd, ranges) is the size |C| of the largest subset C ⊆ Rd such that
| {C ∩ range | range ∈ ranges} | = 2|C| .
The VC-dimension of the query space (P, X, f) is the VC-dimension of (P, ranges(P, X, f)).
The VC-dimension of all the query spaces that correspond to the activation functions in Table 2 is
O(d), as most of the other common activation functions (Anthony & Bartlett, 2009).
The following theorem bounds the size of the coreset for a given query space and explains how to
construct it. Unlike previous papers such as (Feldman & Langberg, 2011), we consider additive
error and not multiplicative error.
Theorem 5 (Braverman et al. (2016)). Let d be the VC-dimension of a query space (P, w, X, f).
Suppose s : P → [0, ∞) such that s(P) ≥ w(P) supx∈X f(P, x). Let t = p∈P s(P), and ε, δ ∈
5
Published as a conference paper at ICLR 2020
(0, 1). Let c ≥ 1 be a sufficiently large constant that can be determined from the proof, and let C be
a sample (multi-set) of
m ≥ c- d d log t + log
ε2
i.i.d. points from P, where for every p ∈ P and q ∈ C we have pr(p = q) = s(p)/t. Then, with
probability at least 1 - δ,
∀x ∈ X :
X W(Pf(P,X)- X mw⅛ ∙ f(q,X)
p∈P	q∈C	p q
≤ ε.
Algorithm 2: Coreset PER LAYER(P) wι,…，Wk,m, φ, β)
Input:	Weighted sets (P, wι),…，(P, Wk),
an integer (sample size) m ≥ 1,
an (activation) function φ : R → [0, ∞),
an upper bound β > 0.
Output:	A weighted set (C, u); see Theorem 7.
1
2
3
4
5
6
7
8
9
for every P ∈ P do
() :=	maxi∈[k] Wi(p)φ(β kpk)
pr P '	Pq∈p maxi∈[k] Wi(q)φ(β kqk)
u(P) := 0
C-0
for m iterations do
Sample a point q from P such that P ∈ P is chosen with probability pr(P).
C := C∪ {q}
∀i ∈ [k] : Ui(q) := Ui(q) +----w” 、
m ∙ pr(q)
return (C, uι,… ,Uk)
3.3	Main Theoretical Results
Most of the coresets provide a (1 + ε)-multiplicative factor approximation for every query that is
applied on the input set. The bound on the coreset size is independent or at least sub-linear in the
original number n of points, for any given input set. Unfortunately, the following theorem proves
that it is impossible to compute small coresets for many common activation functions such as ReLU.
This holds even if there are constraints on both the length of the input set and the test set of samples.
Theorem 6 (No coreset for multiplicative error). Let φ : R → [0, ∞) such that φ(b) > 0 if and
only if b > 0. Let α, β > 0, ε ∈ (0, 1) and n ≥ 1 be an integer. Then there is a set P ⊆ Bα(0) of n
points such that if a weighted set (C, u) satisfies C ⊆ P and
∀X ∈ Bβ(0) :	φ(PT X) -	u(q)φ(qTX) ≤ ε	φ(PT X),	(1)
p∈P	q∈C	p∈P
then C = P.
The proof of Theorem 6 is provided in Appendix A.1.
The following theorem motivates the usage of additive ε-error instead of multiplicative (1 +ε) error.
Fortunately, in this case there is a bound on the coreset’s size for appropriate sampling distributions.
Theorem 7. Let α, β > 0 and (P, W, Bβ (0), f) be a query space of VC-dimension d such that
P ⊆ Bα(0), the weights W are non-negative, f(P, X) = φ(PT X) and φ : R → [0, ∞) is a non-
decreasing function. Let ε, δ ∈ (0, 1) and
m ≥ c- d d log t + log
ε2
6
Published as a conference paper at ICLR 2020
where
t = φ(αβ)	w(p)
p∈P
and c is a sufficiently large constant that can be determined from the proof.
Let (C, u) be the output ofa call to CORESET(P, w, m, φ, β); see Algorithm 1. Then, |C| ≤ m and,
with probability at least 1 - δ,
w(p)φ(pT x) -	u(p)φ(pT x) ≤ ε.
p∈P	p∈C
The proof is provided in Appendix A.2.
As weights of a neural network can take positive and negative values, and the activation functions
φ : R → R may return negative values, we generalize our result to include negative weights and
any monotonic (non-decreasing or non-increasing) bounded activation function in the following
corollary.
Corollary 8. Let (P, w, Bβ (0), f) be a general query spaces, of VC-dimension O(d) such that
f(p, x) = φ(pT x) for some monotonic function φ : R → R and P ⊆ Bα(0). Let
s(p) = sup ∣w(p)Φ(pTx)|
x∈X
for every p ∈ P . Let c ≥ 1 be a sufficiently large constant that can be determined from the proof,
t = Pp∈P s(p), and
m ≥ c ( d log t + log
ε2
Let (C, u) be the output ofa call to CORESET(P, w, m, φ, β); see Algorithm 1. Then, |C| ≤ m and,
with probability at least 1 - δ,
∀x ∈
Bβ (0) :	w (p)φ(pT x) -	u(p)φ(pT x) ≤ ε.
p∈P
p∈C
The proof of Corollary 8 is provided in Appendix A.3.
3.4	From Coreset per Neuron to Coreset per Layer
Applying Algorithm 1 to each neuron in a layer i + 1 could result in the situation that a neuron in
layer i is selected to the coreset of some neurons in layer i + 1, but not to others. In this situation, it
cannot be removed. To perform neuron pruning, every neuron in layer i + 1 should select the same
neurons for its coreset, maybe with different weights. Thus, we wish to compute a single coreset for
multiple weighted sets that are different only by their weight function. Each such a set represents a
neuron in level i + 1, which includes k neurons. Algorithm 2 and Corollary 9 show how to compute
a single coreset for multiple weighted sets. Figure 1 provides an illustration of the layer pruning on
a toy example.
Corollary 9 (Coreset per Layer). Let (P, w1, Bβ(0), f), . . . , (P, wk, Bβ (0), f) be k query spaces,
each of VC-dimension O(d) such that f(p, x) = φ(pT x) for some non-decreasing φ : R → [0, ∞)
and P ⊆ Bα (0). Let
s(p) = max sup wi(p)φ(pTx)
i∈[k] x∈X
for every p ∈ P . Let c ≥ 1 be a sufficiently large constant that can be determined from the proof,
t = Pp∈P s(p)
m ≥ εct (dlog t + log (δ)).
Let (C, uι, •…,Uk) be the output of a call to CORESET(P) wι,…,wk, m, φ, β); SeeAlgorithm 2.
Then, |C | ≤ m and, with probability at least 1 - δ,
∀i ∈ [k], x ∈ Bβ (0) :	wi(p)φ(pTx) -	Ui(p)φ(pTx) ≤ ε.
p∈P	p∈C
7
Published as a conference paper at ICLR 2020
Network	Error(%)	# Parameters	Compression Ratio
LeNet-300-1W= LeNet-300-100 Pruned	2.16 2.03	267K = 26K	90%
VGG-16 VGG-16 PrUned	8.95 8.16	14M 350K	75%
Table 1: Empirical evaluations of our coresets on existing architectures for MNIST and CIFAR-10.
Note the improvement of accuracy in both cases!
The proof follows directly from the observation in Theorem 5 that s(p) ≥ w(p) supx∈X f(p, x).
4	Experiments
We first test our neural pruning with coresets on two popular models: LeNet-300-100 on
MNIST (LeCun et al., 1998), and VGG-16 (Simonyan & Zisserman, 2014) on CIFAR-
10 (Krizhevsky, 2009). We then compare the compression rate of our coreset (Neuron Coreset)
to the compression methods based on the following sampling schemes:
Baselines: uniform sampling, percentile (which deterministically retains the inputs with the highest
norms), and Singular Value Decomposition (SVD);
Schemes for matrix sparsification: based on L1 and L2 norms and their combination (Drineas &
Zouzias, 2011; Achlioptas et al., 2013; Kundu & Drineas, 2014);
Sensitivity sampling: CoreNet and CoreNet++ (Baykal et al., 2018).
In all experiments we used ReLU networks and we computed the average error of the tested al-
gorithms after performing each test ten times. For every layer, after applying neural pruning
the remaining weights were fine-tuned until convergence. The experiments were implemented
in PyTorch (Paszke et al., 2017) on a Linux Machine using an Intel Xeon, 32-core CPU with
3.2 GHz, 256 GB of RAM and Nvidia TitanX and Quadro M4000 GPUs . The source code of
our method can be found at: https://github.com/BenMussay/Data-Independent-
Neural-Pruning-via-Coresets.
4.1	Compressing LeNet and VGG
LeNet-300-100 network comprises two fully connected hidden layers with 300 and 100 neurons
correspondingly, trained on MNIST data set. Our coresets were able to prune roughly 90% of the
parameters and our compression did not have any associated accuracy cost - in fact, it slightly
improved the classification accuracy.
VGG-16 (Simonyan & Zisserman, 2014) includes 5 blocks comprising convolutional and pooling
layers, followed by 3 dense layers - the first two with 4096 neurons and the last with 1000 neurons.
The model was trained and tested on CIFAR-10. We applied our algorithm for neural pruning to the
dense layers, which have the largest number parameters. Our experiment showed slight improve-
ment in accuracy of classification while the number of parameters decreased by roughly 75%. We
summarize our findings in Table 1.
4.2	Coresets on ReLU
We analyzed the empirical trade-off between the approximation error and the size of the coreset,
constructed by Algorithm 1 and Corollary 8, in comparison to uniform sampling, which also imple-
ments Algorithm 1, but sets the probability of a point to 1/n (n is the size of the full set), and to
percentile, which deterministically retains the inputs with the highest norms (note that in percentile
the points are not weighted). We ran three tests, varying the distribution of weights. In the first and
second tests (Figure 2, (a) and (b)) the weights were drawn from the Gaussian and Uniform distri-
butions respectively. The total number of neurons was set to 1000. We selected subsets of neurons
8
Published as a conference paper at ICLR 2020
(a)	(b)	(c)
Figure 2: Approximation error of a single neuron on MNIST dataset across different coreset sizes.
The weights of the points in (a) are drawn from the Gaussian distribution, in (b) from the Uniform
distribution and in (c) we used the trained weights from LeNet-300-100. Our coreset, computed by
Algorithm 1 and Corollary 8, outperforms other reduction methods.
of increasing sizes from 50 to 1000 with a step of 50. In the third test (Figure 2, (c)) we used the
trained weights from the first layer of Lenet-300-100 including 300 neurons. We varied the core-
set size from 50 to 300 with a step 50. To evaluate the approximation error, we used images from
MNIST test set as queries. Each point in the plot was computed by 1) running the full network and
the compressed network (with corresponding compression level) on each image x in the test set, 2)
computing additive approximation error Pp∈P w(p)φ(pT x) - Pp∈C u(p)φ(pT x), 3) averaging
the resulting error over the test set. In all three tests, our coresets outperformed the tested methods
across all coreset sizes.
0.0
g.5Q 5.05
5.2.s7.5 2
」0t山φ>≡pp< ΦCTSΦ><
0	5	10	15	20	25	30
Percentage of N on-zero Parameters Retained
(a)
0.00
0
5 0 5 0 5 0 5
7.52.07.5.2
1.1.1.1.0.0.0.
」0t山①>EPP< φ^2φ><
5	10	15	20	25	30
(b)
Percentage of N on-zero Parameters Retained
Figure 3: Average accuracy of various algorithms on LeNet-200-105 on MNIST dataset across
different sparsity rates. Plot (a) shows the results of all tested methods. Plot (b) focuses on the
three top methods. Our method, which constructs neural coresets by applying the Algorithm 2 and
Corollary 8 in a layer-by-layer fashion, outperforms other coreset-based algorithms.
4.3	Comparison with Other Methods.
We compare the average approximation error vs. compression rates of our neural pruning coreset
with several other well-known algorithms (listed above). We run these tests on LeNet-200-105 ar-
chitecture, trained and tested on MNIST, and we measure the corresponding average approximation
error as defined in (Baykal et al., 2018):
errorPtest
1
Ptest
kφ^	llφθ(x)- φθ(x)kι,
x∈Pt est
where φ^(x) and φθ(x) are the outputs of the approximated and the original networks respectively.
9
Published as a conference paper at ICLR 2020
The results are summarized in Figure 3. As expected, all algorithms perform better with lower
compression, but our algorithm outperforms the other methods, especially for high compression
rates.
4.4	Ablation Analysis
The proposed compression framework includes for every layer, a selection of neurons using Al-
gorithm 2, followed by fine-tuning. We performed the following ablation analysis to evaluate the
contribution of different parts of our framework on LeNet-300-100 trained on MNIST. First, we
removed the fine-tuning, to test the improvement due to Algorithm 2 over the uniform sampling.
Figure 4, (a) shows the classification accuracy without fine-tuning as a function of the compression
rate. Figure 4, (b) shows that fine-tuning improves both methods, but the advantage of the coreset
is still apparent across almost all compression rates and it increases at the higher compression rates.
Note that the model selected by the coreset can be fine-tuned to 98% classification accuracy for any
compression rate, while the model chosen uniformly cannot maintain the same accuracy for high
compression rates.
These results demonstrate that our coreset algorithm provides better selection of neurons compared
to uniform sampling. Moreover, it requires significantly less fine-tuning: fine-tuning until conver-
gence of the uniform sampling took close to 2 epochs, while fine-tuning of our method required
about half of that time.
Figure 4: Average accuracy over 5 runs of the proposed framework and the uniform baseline on
LeNet-300-100 on MNIST dataset across different compression rates. Plot (a) shows the results
before fine-tuning, plot (b)-after fine-tuning. The fine-tuning was done until convergence. Fine-
tuning of the uniform sampling almost doubles in time compared to the fine-tuning of the coreset.
5	Conclusion
We proposed the first neural pruning algorithm with provable trade-offs between the compression
rate and the approximation error for any future test sample. We base our compression algorithm
on the coreset framework and construct coresets for most common activation functions. Our tests
on ReLU networks show high compression rates with no accuracy loss, and our theory guarantees
the worst case accuracy vs. compression trade-off for any future test sample, even an adversarial
one. In this paper we focused on pruning neurons. In future work, we plan to extend the proposed
framework to pruning filers in CNNs, to composition of layers, and to other architectures.
Acknowledgments
We thank Rafi Dalla-Torre, Matan Weksler and Benjamin Lastmann from Samsung Research Israel
for the fruitful debates and their technical support.
10
Published as a conference paper at ICLR 2020
References
Dimitris Achlioptas, Zohar Karnin, and Edo Liberty. Matrix entry-wise sampling: Simple is best,
2013.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In ICML, pp. 242-252, 2019.
Jose M Alvarez and Mathieu Salzmann. Compression-aware training of deep networks. In Advances
in Neural Information Processing Systems 30, pp. 856-867, 2017.
Martin Anthony and Peter L Bartlett. Neural network learning: Theoretical foundations. cambridge
university press, 2009.
Cenk Baykal, Lucas Liebenwein, Igor Gilitschenski, Dan Feldman, and Daniela Rus. Data-
dependent coresets for compressing neural networks with applications to generalization bounds.
CoRR, abs/1804.05345, 2018.
Vladimir Braverman, Dan Feldman, and Harry Lang. New frameworks for offline and streaming
coreset constructions. CoRR, abs/1612.00889, 2016. URL http://arxiv.org/abs/1612.
00889.
Emily L Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus. Exploiting linear
structure within convolutional networks for efficient evaluation. In Advances in neural informa-
tion processing systems, pp. 1269-1277, 2014.
Xin Dong, Shangyu Chen, and Sinno Pan. Learning to prune deep neural networks via layer-wise
optimal brain surgeon. In Advances in Neural Information Processing Systems, pp. 4857-4867,
2017.
Petros Drineas and Anastasios Zouzias. A note on element-wise matrix sparsification via a matrix-
valued bernstein inequality. Information Processing Letters, 111(8):385-389, 2011.
Abhimanyu Dubey, Moitreya Chatterjee, and Narendra Ahuja. Coreset-based neural network com-
pression. In ECCV, pp. 469-486, 2018.
Dan Feldman and Michael Langberg. A unified framework for approximating and clustering data.
In STOC, pp. 569-578, 2011.
Trevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity in deep neural networks. CoRR,
abs/1902.09574, 2019.
Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for
efficient neural network. In Advances in Neural Information Processing Systems 28, pp. 1135-
1143, 2015.
Song Han, Huizi Mao, and William J. Dally. Deep compression: Compressing deep neural network
with pruning, trained quantization and huffman coding. In ICLR, 2016.
Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for accelerating very deep neural net-
works. In ICCV 2017, Venice, Italy, October 22-29, 2017, pp. 1398-1406, 2017.
Hengyuan Hu, Rui Peng, Yu-Wing Tai, and Chi-Keung Tang. Network trimming: A data-driven
neuron pruning approach towards efficient deep architectures. CoRR, abs/1607.03250, 2016.
Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew G. Howard,
Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for
efficient integer-arithmetic-only inference. In CVPR, pp. 2704-2713, 2018.
Alex Krizhevsky. Learning multiple layers of features from tiny images, 2009. URL https:
//www.cs.toronto.edu/~kriz∕cifar.html.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
11
Published as a conference paper at ICLR 2020
Abhisek Kundu and Petros Drineas. A note on randomized element-wise matrix sparsification.
CoRR, abs/1404.0320, 2014.
Michael Langberg and Leonard J. Schulman. Universal epsilon-approximators for integrals. In
SODA, pp. 598-607, 2010.
Vadim Lebedev and Victor Lempitsky. Fast convnets using group-wise brain damage. In CVPR, pp.
2554-2564, 2016.
Yann LeCun, John S Denker, and Sara A Solla. Optimal brain damage. In Advances in neural
information processing systems, pp. 598-605, 1990.
Yann LeCun, Leon Bottou, Yoshua Bengio, Patrick Haffner, et al. Gradient-based learning applied
to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for effi-
cient convnets. In ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings,
2017.
Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang. Learn-
ing efficient convolutional networks through network slimming. In ICCV 2017, Venice, Italy,
October 22-29, 2017, pp. 2755-2763, 2017.
Michael C Mozer and Paul Smolensky. Skeletonization: A technique for trimming the fat from a
network via relevance assessment. In Advances in neural information processing systems, pp.
107-115, 1989.
Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan Srebro. To-
wards understanding the role of over-parametrization in generalization of neural networks. CoRR,
abs/1805.12076, 2018.
Adam Paszke, Sam Gross, Soumith Chintala, and Gregory Chanan. Pytorch: Tensors and dynamic
neural networks in python with strong gpu acceleration. PyTorch: Tensors and dynamic neural
networks in Python with strong GPU acceleration, 6, 2017.
Jeff M Phillips. Coresets and sketches. arXiv preprint arXiv:1601.00617, 2016.
K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recogni-
tion. CoRR, abs/1409.1556, 2014.
Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning
Research, 15(1):1929-1958, 2014.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfel-
low, and Rob Fergus. Intriguing properties of neural networks. ICLR, 2014.
Elad Tolochinsky and Dan Feldman. Coresets for monotonic functions with applications to deep
learning. CoRR, abs/1802.07382, 2018.
Vladimir N Vapnik and A Ya Chervonenkis. On the uniform convergence of relative frequencies of
events to their probabilities. In Measures of complexity, pp. 11-30. Springer, 2015.
Ruichi Yu, Ang Li, Chun-Fu Chen, Jui-Hsin Lai, Vlad I. Morariu, Xintong Han, Mingfei Gao,
Ching-Yung Lin, and Larry S. Davis. NISP: pruning networks using neuron importance score
propagation. In CVPR 2018, pp. 9194-9203, 2018.
Zhuangwei Zhuang, Mingkui Tan, Bohan Zhuang, Jing Liu, Yong Guo, Qingyao Wu, Junzhou
Huang, and Jinhui Zhu. Discrimination-aware channel pruning for deep neural networks. In
Advances in Neural Information Processing Systems 31, pp. 875-886, 2018.
12
Published as a conference paper at ICLR 2020
Figure 5: (left) Any point on a circle can be separated from the other points via a line. (right) The
same holds for a circle which is the intersection of a d-dimensional sphere and a hyperplane; see
Theorem 6.
A Appendix
A.1 Proof of Theorem 6
Consider the points on Ba(0) whose norm is α and last coordinate is a∕2. This is a (d - 1)-
dimensional sphere S that is centered at (0,…，0, α∕2). For every pointP on this sphere there is a
hyperplane that passes through the origin and separates p from the rest of the points in S. Formally,
there is an arbitrarily short vector xp (which is orthogonal to this hyperplane) such that xpTp > 0,
but xpTq < 0 for every q ∈ S \ {p}; see Fig. 5. By the definition of φ, we also have φ(xpT p) > 0,
but φ(xpT q) = 0 for every q ∈ S \ {p}.
Let P be an arbitrary set of n points in S, and C ⊂ P . Hence exists a point p ∈ P \ C. By the
previous paragraph,
φ(xpT q) -	u(q)φ(xpT q) = φ(xpT p) - 0 = φ(xpT p)
q∈P	q∈C
=	φ(xpT q) > ε	φ(xpT q).
q∈P	q∈P
Therefore C does not satisfy equation 1 in Theorem 6.
A.2 Proof of Theorem 7
We want to apply Algorithm 1, and to this end we need to prove a bound that is independent of x on
the supremum s, the total supremum t, and the VC-dimension of the query space.
Bound on f(p, x). Putp ∈ P and x ∈ Bβ(0). Hence,
f(p,x) = φ(pT x) ≤ φ(kpk kxk)	(2)
≤ φ(kpk β)	(3)
≤ φ(αβ),	(4)
where equation 2 holds by the Cauchy-Schwarz inequality and since φ is non-decreasing, equation 3
holds since x ∈ Bβ (0), and equation 4 holds since p ∈ Bα(0).
Bound on the total sup t. Using our bound on f (p, x),
t = X s(p) = X w(p)φ(kpk β) ≤ φ(αβ) X w(p),
p∈P	p∈P	p∈P
where the last inequality is by equation 4.
Bound on the VC-dimension of the query space (P, w, Bβ (0), f) is O(d) as proved e.g. in An-
thony & Bartlett (2009).
13
Published as a conference paper at ICLR 2020
Putting all together. By applying Theorem 1 with X = Bβ (0), we obtain that, with probability
at least 1 - δ,
∀x ∈ Bβ(0) : X w(p)f(p,x) - X u(q)f(q,x) ≤ ε.
p∈P	q∈C
Assume that the last equality indeed holds. Hence,
∀x ∈ Bβ (0) : X w(p)φ(pT x) - X u(q)φ(qTx) ≤ ε.
p∈P	q∈C
A.3 Proof of Corollary 8
We assume that φ is a non-decreasing function. Otherwise, we apply the proof below for the non-
decreasing function φ* = -φ and corresponding weight w*(p) = -w(p) for every P ∈ P. The
correctness follows since w(p)φ(pTx) = w*(p)φ*(pτx) for every P ∈ P.
Indeed, put x ∈ Bβ (0), and φ non-decreasing. Hence,
w(p)φ(pT x) -	u(p)φ(pT x)	(5)
p∈P	p∈C
≤	w(p)φ(pT x) -	u(p)φ(pT x)
p∈P	p∈C
w(p)φ(pT x)≥0	u(p)φ(pT x)≥0
+	w(p)φ(pT x) -	u(p)φ(pT x)
p∈P	p∈C
w(p)φ(pT x)<0	u(p)φ(pT x)<0
(6)
≤	w(p)φ(pT x) -	u(p)φ(pT x)
p∈P	p∈C
w(p)φ(pT x)≥0	u(p)φ(pT x)≥0
+ E	Iw(P)φ(PTx)I-	E	Iu(P)φ(PTx)I
p∈P	p∈C
w(p)φ(pT x)<0	u(p)φ(pT x)<0
(7)
Equation 6 is obtained by separating each sum into points with positive and negative weights and
applying Cauchy-Schwarz inequality. Next, we bound points with positive and negative weights
separately using Theorem 7.
A.4 Coreset for Different Activation Functions
Activation Function	Definition
ReLU	max(x, 0)
σ	1
	1+e-x
binary	ʃ 0 for x < 0
	(1 for X ≥ 0
Z	ln(1 + ex)
soft-clipping	1 log l++eX-ι)
Gaussian	e-x
Table 2: Examples of activation functions φ for which We can construct a coreset of size O( Oe) that
approximates ∣p∣ Pp∈p Φ(PTx) With ε-additive error.
14