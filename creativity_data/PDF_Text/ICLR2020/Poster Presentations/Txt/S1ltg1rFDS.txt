Published as a conference paper at ICLR 2020
Black-box Off-policy Estimation for
Infinite-Horizon Reinforcement Learning
Ali Mousavi*
Lihong Li*
* Google Research
{alimous,lihong,dennyzhou}@google.com
Qiang Liut
Dengyong Zhou*
tUniversity of Texas, Austin
lqiang@cs.utexas.edu
Ab stract
Off-policy estimation for long-horizon problems is important in many real-life
applications such as healthcare and robotics, where high-fidelity simulators may
not be available and on-policy evaluation is expensive or impossible. Recently,
Liu et al. (2018) proposed an approach that avoids the curse of horizon suffered
by typical importance-sampling-based methods. While showing promising results,
this approach is limited in practice as it requires data be drawn from the stationary
distribution of a known behavior policy. In this work, we propose a novel approach
that eliminates such limitations. In particular, we formulate the problem as solving
for the fixed point of a certain operator. Using tools from Reproducing Kernel
Hilbert Spaces (RKHSs), we develop a new estimator that computes importance
ratios of stationary distributions, without knowledge of how the off-policy data are
collected. We analyze its asymptotic consistency and finite-sample generalization.
Experiments on benchmarks verify the effectiveness of our approach.
1	Introduction
As reinforcement learning (RL) is increasingly applied to crucial real-life problems like robotics,
recommendation and conversation systems, off-policy estimation becomes even more critical. The
task here is to estimate the average long-term reward of a target policy, given historical data collected
by (possibly unknown) behavior policies. Since the reward and next state depend on what action
the policy chooses, simply averaging rewards in off-policy data does not estimate the target policy’s
long-term reward. Instead, proper correction must be made to remove the bias in data distribution.
One approach is to build a simulator that mimics the reward and next-state transitions in the real
world, and then evaluate the target policy against the simulator (e.g., Fonteneau et al., 2013; Ie et al.,
2019). While the idea is natural, building a high-fidelity simulator could be extensively challenging
in numerous domains, such as those that involve human interactions. Another approach is to use
propensity scores as importance weights, so that we could use the weighted average of rewards in
off-policy data as a suitable estimate of the average reward of the target policy. The latter approach is
more robust, as it does not require modeling assumptions about the real world’s dynamics. It often
finds more success in short-horizon problems like contextual bandits, but its variance often grows
exponentially in the horizon, a phenomenon known as “the curse of horizon” (Liu et al., 2018).
To address this challenge, Liu et al. (2018) proposed to solve an optimization problem of a minimax
nature, whose solution directly estimates the desired propensity score of states under the stationary
distribution, avoiding an explicit dependence on horizon. While their method is shown to give more
accurate predictions than previous algorithms, it is limited in several important ways:
• The method requires that data be collected by a known behavior policy. In practice, however,
such data are often collected over an extended period of time by multiple, unknown behavior
policies. For example, observational healthcare data typically contain patient records,
whose treatments were provided by different doctors in multiple hospitals, each following
potentially different procedures that are not always possible to specify explicitly.
• The method requires that the off-policy data reach the stationary distribution of the behavior
policy. In reality, it may take a very long time for a trajectory to reach the stationary
distribution, which may be impractical due to various reasons like costs and missing data.
1
Published as a conference paper at ICLR 2020
In this paper, we introduce a novel approach for the off-policy estimation problem that overcome
these drawbacks. The main contributions of our work are three-fold:
•	We formulate the off-policy estimation problem into one of solving for the fixed point of an
operator. Different from the related, and similar, Bellman operator that goes forward in time,
this operator is backward in time.
•	We develop a new algorithm, which does not have the aforementioned limitations of Liu
et al. (2018), and analyze its generalization bounds. Specifically, the algorithm does not
require that the off-policy data come from the stationary distribution, or that the behavior
policy be known.
•	We empirically demonstrate the effectiveness of our method on several classic control
benchmarks. In particular, we show that, unlike Liu et al. (2018), our method is effective
even if the off-policy data has not reached the stationary distribution.
In the next section, we give a brief overview of recent and related works. We then move to describing
the problem setting that we have used in the course of the paper and our off-policy estimation
approach. Finally, we present several experimental results to show the effectiveness of our method.
Notation. In the following, we use ∆(X) to denote the set of distributions over a set X. The `2 norm
of vector x is kxk. Given a real-valued function f defined on some set X , let
kfk2 := RXf f (x)2dx.
Finally, we denote by [n] the set {1, 2, . . . , n}, and 1{A} the indicator function.
2	Related works
Our work focuses on estimating a scalar (average long-term reward) that summarizes the quality
of a policy and has extensive applications in practice. This is different from value function or
policy learning from off-policy data (e.g., Precup et al., 2001; Maei et al., 2010; Sutton et al., 2016;
Munos et al., 2016; Metelli et al., 2018), where the major goal is to ensure stability and convergence.
Yet, these two problems share numerous core techniques, such as importance reweighting and
doubly robustness. Off-policy estimation and evaluation can also be used as a component for policy
optimization (e.g., Jiang & Li, 2016; Gelada & Bellemare, 2019; Liu et al., 2019; Zhang et al., 2019).
Importance reweighting, or inverse propensity scoring, has been used for off-policy RL (e.g., Precup
et al., 2001; Murphy et al., 2001; Li et al., 2015; Munos et al., 2016; Hanna et al., 2017; Xie et al.,
2019). Its accuracy can be improved by various techniques (Jiang & Li, 2016; Thomas & Brunskill,
2016; Guo et al., 2017; Farajtabar et al., 2018). However, these methods typically have a variance
that grows exponentially with the horizon, limiting their application to mostly short-horizon problems
like contextual bandits (DUdik et al., 2011; BottoU et al., 2013).
There have been recent efforts to avoid the exponential blow-up of variance in basic inverse propensity
scoring. A few aUthors explored the alternative to estimate the propensity score of a state’s stationary
distribution (LiU et al., 2018; Gelada & Bellemare, 2019), when behavior policies are known. Later,
NachUm et al. (2019) extended this idea to sitUations with unknown behavior policies. However, their
approach only works for the discoUnted reward criterion. In contrast, oUr work considers the more
general and challenging undiscounted criterion. In the next section, we briefly mention the setting
Under which we stUdy this problem and then present oUr black-box off-policy estimator.
OUr black-box estimator is inspired by previoUs work for black-box importance sampling (LiU & Lee,
2017). Interestingly, the aUthors show that it is beneficial to estimate propensity scores from data
withoUt Using knowledge of the behavior distribUtion (called proposal distribUtion in that paper), even
if it is available; see also Henmi et al. (2007) for related argUments. Similar benefits may exist for
oUr black-box off-policy estimator developed here, althoUgh a systematic stUdy is oUtside the scope
of this paper.
3	Problem Setting
Consider a Markov decision process (MDP) (PUterman, 1994) M = hS, A, P, R, p0, γi, where S and
A are the state and action spaces, P is the transition probability fUnction, R is the reward fUnction,
2
Published as a conference paper at ICLR 2020
p0 ∈ ∆(S) is the initial state distribution, and γ ∈ [0, 1] is the discount factor. A policy π maps
states to a distribution over actions: π : S → ∆(A), and π(a∣s) is the probability of choosing action
a in state s by policy π. With a fixed π, a trajectory τ = (s0, a0, r0, s1, a1, r1, . . .) is generated as
follows:1
S0 ~ po(∙),	at ~ ∏(∙∣St),	r = R(st,at),	st+ι ~ P (∙∣st,at),	∀t ≥ 0.
Given a target policy π, we consider two reward criteria, undiscounted (γ = 1) and discounted
(Y < 1), where En [∙] indicates the trajectory T is controlled by policy ∏:
(undiscounted)
(discounted)
1T
Pn	:= τlim∞ En TESrt = E(s,a)~d∏ [r],
∞	t=1
∞
ρn,λ	:= (1 - γ)En X γtrt	.
t=0
(1)
(2)
In the above, dn is the stationary distribution over S × A, which exists and is unique under certain
assumptions (Levin & Peres, 2017).
The γ < 1 case can be reduced to the undiscounted case of γ = 1, but not vice versa. Indeed, one can
show that the discounted reward in equation 2 can be interpreted as the stationary distribution of an
induced Markov process, whose transition function is a mixture of P and the initial-state distribution
p0. We refer interested readers to Appendix A for more details. Accordingly, in the following and
without the loss of generality, we will merely focus on the more general undiscounted criterion in
equation 1, and suppress the unnecessary dependency on p0 and γ .
In the off-policy estimation problem, we are interested in estimating ρn for a given target policy
π. However, instead of having access to on-policy trajectories generated by π, we have a set of n
transitions collected by some unknown (i.e., “black-box” or behavior-agnostic (Nachum et al., 2019))
behavior mechanism πBEH :
D := {(si, ai, ri, si)}1≤i≤n .
Therefore, the goal of off-policy estimation is to estimate ρn based on D, for a given target policy π .
The setting we described above is quite general, covering a number of situations. For example, πBEH
might be a single policy and D might consist of one or multiple trajectories collected by πBEH . In
this special case, s0i = si+1 for 1 < i < n; this is the off-policy RL scenario widely studied (e.g.,
Precup et al., 2001; Sutton et al., 2016; Munos et al., 2016; Liu et al., 2018; Gelada & Bellemare,
2019). Furthermore, if πBEH = π , we recover the on-policy setting. On the other hand, πBEH and
D can consist of multiple policies and their corresponding trajectories. In this situation, unlike the
single policy case s0i and si+1 might originate from two distinct policies. In general, one can consider
πBEH as a distribution over S × A where (si, ai) in D are sampled from. Having introduced the
general setting of the problem, we will describe our estimation approach in the next section.
4 Black-box estimation
Our estimator is based on the following operator defined on functions over S × A. For discrete
state-action spaces, given any d ∈ RS×A,
Bnd(s,a) := π(a∣s) ɪ2 P (slξ, α)d(ξ,α).
(3)
ξ∈S,α∈A
While we will develop the rest of the paper using the discrete version above for simplicity, the
continuous version can be similarly obtained without affecting the estimator and results:
Bnd(S,a) = π(a∣s) J dP(s∣ξ,α)d(ξ,α),
where P is now interpreted as the transition kernel.
(4)
1For simplicity in exposition, we assume rewards are deterministic. However, everything in this work
generalizes directly to the case of stochastic rewards.
3
Published as a conference paper at ICLR 2020
We should note that Bπ is indeed different from the Bellman operator (Puterman, 1994); although
they have some similarities. In particular, given some state-action pair (s, a), the Bellman operator
is defined using next state s0 of (s, a), while Bπ is defined using previous state-actions (ξ, α) that
transition to s. It is in this sense that Bπ is backward (in time). Furthermore, as we will show later,
d has the interpretation of a distribution over S × A. Therefore, Bπ describes how visitation flows
from (ξ, α) to (s, a) and hence, we call it the backward flow operator. Note that similar forms of Bπ
have appeared in the literature, usually used to encode constraints in a dual linear program for an
MDP (e.g., Wang et al., 2007; Wang, 2017; Dai et al., 2018). However, the application of Bπ for the
off-policy estimation problem as considered here appears new to the best of our knowledge.
An important property of Bπ is that, under certain assumptions, the stationary distribution dπ is the
unique fixed point that lies in ∆(S × A) (Levin & Peres, 2017):
dπ = Bπdπ and dπ ∈ ∆(S × A) .	(5)
This property is the key element we use to derive our estimator as we describe in the following.
4.1	Black-box estimator
In most cases, off-policy estimation involves a weighted average of observed rewards ri in D. We
therefore aim to directly estimate these (non-negative) weights which we denote by w = {wi } ∈
∆([n]); that is, wi ≥ 0 for i ∈ [n] and Pin=1 wi = 1. Note that the normalization of w may be
ensured by techniques such as self-normalized importance sampling (Liu, 2001). Once such a w is
obtained, the estimated reward is given by:
n
P∏ = E Wiri .	(6)
i=1
Effectively, any w ∈ ∆([n]) defines an empirical distribution which we denote by dw over S × A:
n
dw (s, a) :=	wi 1{si = s, ai = a} .	(7)
i=1
Equation 6 is equivalent to ρ∏ = E(s,Ο)〜dw [r]. Comparing it to equation 1, We naturally want
to optimize w so that dw is close to dπ . Therefore, inspired by the fixed-point property of dπ in
equation 5, the problem naturally becomes one of minimizing the discrepancy between dw and Bπdw .
In practice, w is often represented in a parametric way:
Wi = Wi/X WI ,	Wi ：= W(si,ai； ω) ≥ 0 ,	(8)
l
where W(.) is a parametric model, such as neural networks, with parameters ω ∈ Ω. We have now
reached the following optimization problem:
min D(dw k Bπdw) ,	(9)
ω∈Ω
where D(∙ ∣∣ ∙) is some discrepancy function between distributions. In practice, B is unknown, and
must be approximated by samples in the dataset D:
n
Bndw(s,a) := π(a∣s) ɪ2 Wi 1{si = s}.
i=1
Clearly, Bπdw is a valid distribution over S × A induced by W and D, and the black-box estimator
solves for W by minimizing D(dw ∣ Bπdw ).
4.2	Black-box estimator with MMD
There are different choices for D(∙ ∣∣ ∙) in equation 9, and multiple approaches to solve it (e.g.,
Nguyen et al., 2010; Dai et al., 2017). Here, we describe one such algorithm based on Maximum
Mean Discrepancy (MMD) (Muandet et al., 2017a). For simplicity, the discussion in this subsection
assumes S × A is finite, but the extension to continuous S × A is immediate.
4
Published as a conference paper at ICLR 2020
Let k(∙, ∙) be a positive definite kernel function defined on (S X A)2. Given two real-valued functions,
f and g, defined on S × A we define the following bilinear functional
k [f ；	g]	：=	E	f (s,a)	∙ k	((s,	a), («,«)) ∙	g(s,	a)	.	(10)
(s,a)∈S×A,(s,α)∈S×A
Clearly, we have k [f; f] ≥ 0 for any f due to the positive definiteness of k. In addition, k is called
strictly integrally positive definite if k [f; f] = 0 implies kf k2 = 0.
Let H be the reproducing kernel Hilbert space (RKHS) associated with the kernel function k. This is
the unique Hilbert space that includes functions that can be expressed as a sum of countably many
terms: f (∙) = Pi Ui k((si,ai), ∙), where {ui} ⊂ R, and {(si,ai)} ⊆ S × A. The space H is
equipped with an inner product defined as follow: given f,g ∈ H such that f = Pi Ui k((si, ai), ∙)
and g = Pi Vi k((si, ai), ∙), the inner product is hf, GH := Pi j Uivj k(si, ai), (sj, aj)), which
induces the norm defined by kf kH
PTjiu uiuj k((si, ai), (sj, aj)).
Given H, the maximum mean discrepancy between two distributions, μι and μ2, is defined by
Dk(μι k μ2) := sup {Eμι[f] — Eμ2 [f],	s.t. IlfllH ≤ 1}.
f∈H
Here, f may be considered as a discriminator, playing a similar role as the discriminator network in
generative adversarial networks (Goodfellow et al., 2014), to measure the difference between μι and
μ2. A useful property of MMD is that it admits a closed-form expression (Gretton et al., 2012):
Dk(μι k μ2) = k [μι — μ2; μι 一 μ2]
=k [μι; μι] — 2k [μι; μ2] + k [μ2; μ2],
where k [∙; ∙] is defined in equation 10, and we used the bilinear property k [∙; ∙]. Interested readers
are referred to surveys (e.g. Berlinet & Thomas-Agnan, 2011; Muandet et al., 2017b) for more
background on RKHS and MMD.
Applying MMD to our objective, we have
Dk(dw	k	Bndw)	= k	[dw；	dw]	—	2k 艮；B∏dw]	+ k	∖13πdw;	B∏dw].
In the above, both dw and Bπ dw are simply probability mass functions on a finite subset of S × A,
consisting of state-actions encountered in D. It follows immediately from equation 10 that
k [dw ; dw]	=	wiwj k((si, ai), (sj,aj)) 、	一 _z	J i,j	K(O)
k [dw； B∏dw[=	Ewiwj E∏(a0∣sj)k((si, ai), (Sj, a0)) i,j	a0 '	{	' K(Ij)
k Bπ dw ; Bπ dw	=	Ewiwj £ n(ai|si)n(aj|sj)k((Si,ai), (Sj,aj)). i,j	a0i,a0j 、	V	Z K(j)
Defining Ki,j := Ki(,0j) — 2Ki(,1j) + depends on ω; see equation 8):	Ki(,2j), we can express the objective as a function of ω (since {wi} '(ω) = EwiwjKij .	(11) i,j
Remark 4.1. Mini-batch training is an effective approach to solve large-scale problems. However,
the objective '(ω) is not in a form that is readyfor mini-batch training, as Wi requires normalization
(equation 8) that involves all data in D. Instead, we may equivalently minimize L(ω) := log '(ω),
which can be turned into a form that allow mini-batch training, using a trick that is also useful in
other machine learning contexts (e.g., Jean et al., 2015). See Appendix D for more details.
Algorithm 1 in Appendix E summarizes our estimator. We next present theoretical analysis of our
approach. We show the consistency of our result and provide a sample complexity bound.
5
Published as a conference paper at ICLR 2020
4.3	Theoretical Analysis
Consistency. The following theorem shows that the exact minimizer of equation 9 coincides with
the fixed point of Bπ , and the objective function measures the norm of the estimation error in an
induced RKHS. To simplify exposition, we assume x = (s, a) and x0 = (s0 , a0) a successive action-
state pair following x: x0 〜d∏(∙ | x), where d∏(x0 | x) is the transition probability from X to x0,
that is, d∏(x0 | x) = P(s0 | s,a)π(a0∣s0). Similarly, we denote by (x,X0) = ((s, a), (s0, a0)) an
independent copy of (x, x0).
Theorem 4.1.	Suppose k is strictly integrally positive definite, and dπ is the unique fixed point of Bπ
in equation 5. Then, for any d ∈ ∆(S × A),
Dk (d || Bn d) = 0	^⇒ d = d∏ .
Furthermore, Dk(d || Bπd) equals an MMD between d and dπ, with a transformed kernel:
Dk(d∣∣B∏d) = Dk(d|| d∏),
where k(x, xa) is a positive definite kernel, defined by
0	0	00
k(x, xa) = Eπ [k(x, xa) - k(x, xa0) - k(x0, xa) + k(x0, xa0) | (x, xa)],
where the expectation is with respect to x0 〜d∏(∙ | x) and x0 〜d∏(∙ | x), with x0 and x0 drawn
independently.
Generalization. We next give a sample complexity analysis. In practice, the estimated weight W
is based on minimizing the empirical loss Dk(dw || Bπdw), where Bπ is replaced by the empirical
approximation Bn. The following theorem compares the empirical weights W with the oracle weight
w* obtained by minimizing the expected loss Dk(dw || B∏dw), with the exact transition operator B∏.
Theorem 4.2.	Assume the weight function is decided by Wi = W (si, ai; ω)∕n. Denote by W =
{W(∙; ω): ω ∈ Ω} the model class of W(∙; ω). Assume W is the minimizer of the empirical loss
Dk(dw || Bndw) and W* the minimizer of expected loss Dk(dw || Bndw). Assume {xi}in=1 are i.i.d.
samples. Then, with probability 1 - δ we have
Dk(d^ || Bndw) — Dk(dw* || Bndw*) ≤ 16rmaχRn(W) + 16rmax + r2^8log(1∕δ),
where Rn(W) denotes the expected Rademacher complexity ofW with data size n, and rmax :=
max(kWk∞ , SuPx ∕k(x,x)), with ∣∣W∣∣∞ := sup{∣∣W∣∣∞ : W ∈ W}. This suggests a gener-
alization error of O(1∕√n) if Rn(W) = O(1∕√n), which is typical for parametric families of
functions.
5	Experiments
In this section, we present experiments to compare the performance of our proposed method with
other baselines on the off-policy evaluation problem. In general and for each experiment, we use a
behavior policy πBEH to generate trajectories of length TBEH. We then use these generated samples
from a behavior policy to estimate the expected reward of a given target policy π . To compare
our approach with other baselines, we use the root mean squared error (RMSE) with respect to the
average long-term reward of the target policy π . The latter is estimated using a trajectory of length
TTAR 1. In particular, we compare our proposed black-box approach with the following baselines:
• naive averaging baseline in which we simply estimate the expected reward of a target policy
by averaging the rewards over the trajectories generated by the behavior policy.
• model-based baseline where we use the kernel regression technique with a standard Gaussian
RBF kernel. We set the bandwidth of the kernel to the median (or 25th or 75th percentiles)
of the pairwise euclidean distances between the observed data points.
• inverse propensity score (IPS) baseline introduced by Liu et al. (2018).
We will first use a simple MDP from Thomas & Brunskill (2016) to highlight the IPS drawback we
previously mentioned in Section 1. We then move to classical control benchmarks.
6
Published as a conference paper at ICLR 2020
Figure 1: (a) ModelWin MDP from Thomas & Brunskill (2016). (b) The RMSE of different methods
as we change the length of horizon w.r.t the target policy reward. IPS depends on the horizon length
while our method is independent of the horizon length.
5.1	Toy Example
The ModelWin domain first introduced in Thomas & Brunskill (2016) is a fully observable MDP with
three states and two actions as denoted in Figure 1(a). The agent always begins in s1 and should
choose between two actions a1 and a2. If the agent chooses a1, then with probability ofp and 1 - p it
makes a transition to s2 and s3 and receives a reward of r = 1 and r = -1, respectively. On the other
hand, if the agent chooses a2, then with probability ofp and 1 - p it makes a transition to s3 with the
reward of r = -1 and s2 with the reward of r = 1, respectively. Once the agent is in either s2 or s3,
it goes back to the s1 in the next step without any reward. In our experiments, we set p = 0.4.
We define the behavior and target policies as the following. In the target policy, once the agent is
in s1, it chooses a1 and a2 with the probability of 0.9 and 0.1, respectively. On the other hand and
for the behavior policy, once the agent is in s1 , it chooses a1 and a2 with the probability of 0.7
and 0.3, respectively. We calculate the average on-policy reward from samples based on running a
trajectory of length TTAR = 50, 000 collected by the target policy. We estimate this on-policy reward
using trajectories of length TBEH ∈ {4, 8, 16, 32, 64, 128} collected by the behavior policy. In each
case, we set the number of trajectories such that the total number of transitions (i.e., TBEH times the
number of trajectories) is kept constant. For example, for TBEH = 4 and TBEH = 8 we use 50,000
and 25,000 trajectories, respectively. Since the problem has finitely many state-actions, we use the
tabular method and hence, equation 11 turns into a quadratic programming. We then report the result
of each setting based on 10 Monte-Carlo samples.
As we can see in Figure 1(b), the naive averaging method performs poorly consistently and indepen-
dent of the length of trajectories collected by the behavior policies. On the other hand, IPS performs
poorly when the collected trajectories have short-horizon and gets better as the horizon length of
trajectories get larger. This is expected for IPS — as mentioned in Section 1, it requires data be
drawn from the stationary distribution. In contrast, as shown in Figure 1(b), our black-box approach
performance is independent of the horizon length, and substantially better.
5.2	Classic Control
We now focus on four classic control problems. We begin by briefly describing each problem and
then compare the performance of our method with other approaches on these problems. Note that for
these problems are episodic, we convert them into infinite-horizon problems by resetting the state to
a random start state once the episode terminates.
Pendulum. In this environment, the goal is to control a pendulum in a vertical position. State
variables are the pole angle θ and velocity θ. The action a is the torque in the set {-2, -1, 0, 1, 2}
applied to the base. We set the reward function to -(θ2 + 0.lθ2 + 0.001a2).
Mountain Car. For this problem, the goal is to drive up the car to top of a hill. Mountain Car has a
state space of R2 (the position and speed of the car) and three possible actions (negative, positive, or
zero acceleration). We set the reward to +100 when the car reaches the goal and -1 otherwise.
7
Published as a conference paper at ICLR 2020
Figure 2: The RMSE of different methods w.r.t the target policy reward as we change the number of
trajectories. Our black-box approach outperforms other methods on three problems.
Figure 3: The median and error bars at 25th and 75th percentiles of different methods w.r.t the target
policy reward as we change the number of trajectories. The trend of results is similar to Figure 2.
Cartpole. The goal here is to prevent an attached pole to a cart from falling by changing the cart’s
velocity. Cartpole has a state space of R4 (cart position, velocity, pole angle and velocity) and two
possible actions (moving left or right). Reward is -100 when the pole falls and +1 otherwise.
Acrobot. In this problem, our goal is to swing a 2-link pendulum above the base. Acrobot has a
state space of R6 (sin(.) and cos(.) of both angles and angular velocities) and three possible actions
(applying +1, 0 or -1 torque on the joint). Reward is +100 for reaching the goal and -1 otherwise.
For each environment, we train a near-optimal policy π+ using the Neural Fitted Q Iteration algorithm
(Riedmiller, 2005). We then set the behavior and target policies as πBEH = α1π+ + (1 - α1)π- and
π = α2π+ + (1 - α2)π-, where π- denotes a random policy, and 0 ≤ α1, α2 ≤ 1 are two constant
values making the behavior policy distinct from the target policy. In our experiments, we set α1 = 0.7
and α2 = 0.9. In order to calculate the on-policy reward, we use a single trajectory collected by
π with TTAR = 50, 000. For off-policy data, we use multiple trajectories collected by πBEH with
TBEH = 200. In all the cases, we use a 3-layer (having 30, 20, and 10 hidden neurons) feed-forward
neural network with the sigmoid activation function as our parametric model in equation 8. For each
setting, we report results based on 20 Monte-Carlo samples.
Figure 2 shows the log of RMSE w.r.t. the target policy reward as we change the number of trajectories
collected by the behavior policy. We should note that all methods except the naive averaging method
have hyperparameters to be tuned. For each method, the optimal set of parameters might depend on
the number of trajectories (i.e., size of the training data). However, in order to avoid excessive tuning
and to show how much each method is robust to a change in the number of trajectories, we only tune
different methods based on 50 trajectories and use the same set of parameters for other settings. As
we can see, the naive averaging performance is almost independent of the number of trajectories. Our
method outperforms other approaches on three environments and it is only the Acrobot in which IPS
performs comparably to our black-box approach. In order to have a robust evaluation against outliers,
we have plotted the median and error bars at 25th and 75th percentiles in Figure 3. If we compare the
Figures 2 and 3, we notice that the trend of results is almost the same in both. In Appendix G, we
have studied how changing α1 of the behavior policy affects the final RMSE.
6 Conclusions
In this paper, we presented a novel approach for solving the off-policy estimation problem in the
long-horizon setting. Our method formulates the problem as solving for the fixed point of a “backward
flow” operator. We showed that unlike previous works, our approach does not require the knowledge
of the behavior policy or stationary off-policy data. We presented experimental results to show the
effectiveness of our approach compared to previous baselines. In the future, we plan to use structural
domain knowledge to improve the estimator and consider a random time horizon in episodic RL.
8
Published as a conference paper at ICLR 2020
References
Alain Berlinet and Christine Thomas-Agnan. Reproducing kernel Hilbert spaces in probability and
statistics. Springer Science & Business Media, 2011.
Leon Bottou, Jonas Peters, JoaqUin Quinonero-Candela, Denis Xavier Charles, D. Max Chickering,
Elon Portugaly, Dipankar Ray, Patrice Simard, and Ed Snelson. Counterfactual reasoning and
learning systems: The example of computational advertising. Journal of Machine Learning
Research,14:3207-3260, 2013.
Bo Dai, Niao He, Yunpeng Pan, Byron Boots, and Le Song. Learning from conditional distributions
via dual embeddings. In Proceedings of the 20th International Conference on Artificial Intelligence
and Statistics (AISTATS), pp. 1458-1467, 2017.
Bo Dai, Albert Shaw, Niao He, Lihong Li, and Le Song. Boosting the actor with dual critic. In
Proceedings of the 6th International Conference on Learning Representations (ICLR), 2018.
Miroslav Dudk, John Langford, and Lihong Li. Doubly robust policy evaluation and learning. In
Proceedings of the 28th International Conference on Machine Learning (ICML), pp. 1097-1104,
2011.
Mehrdad Farajtabar, Yinlam Chow, and Mohammad Ghavamzadeh. More robust doubly robust
off-policy evaluation. In Proceedings of the 35th International Conference on Machine Learning
(ICML), pp. 1446-1455, 2018.
Raphael Fonteneau, Susan A. Murphy, Louis Wehenkel, and Damien Ernst. Batch mode reinforcement
learning based on the synthesis of artificial trajectories. Annals of Operations Research, 208(1):
383-416, 2013.
Carles Gelada and Marc G. Bellemare. Off-policy deep reinforcement learning by bootstrapping the
covariate shift. In Proceedings of the 33rd AAAI Conference on Artificial Intelligence (AAAI), pp.
3647-3655, 2019.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural
Information Processing Systems 27 (NIPS), pp. 2672-2680, 2014.
Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Scholkopf, and Alexander Smola. A
kernel two-sample test. Journal of Machine Learning Research, 13(Mar):723-773, 2012.
Zhaohan Guo, Philip S. Thomas, and Emma Brunskill. Using options and covariance testing for long
horizon off-policy policy evaluation. In Advances in Neural Information Processing Systems 30
(NIPS), pp. 2489-2498, 2017.
Josiah P. Hanna, Peter Stone, and Scott Niekum. Bootstrapping with models: Confidence intervals
for off-policy evaluation. In Proceedings of the 31st AAAI Conference on Artificial Intelligence
(AAAI), pp. 4933-4934, 2017.
Masayuki Henmi, Ryo Yoshida, and Shinto Eguchi. Importance sampling via the estimated sampler.
Biometrika, 94(4):985-991, December 2007.
Eugene Ie, Chih-Wei Hsu, Martin Mladenov, Vihan Jain, Sanmit Narvekar, Jing Wang, Rui Wu, and
Craig Boutilier. RecSim: A configurable simulation platform for recommender systems, 2019.
CoRR abs/1909.04847.
Sebastien Jean, KyungHyun Cho, Roland Memisevic, and Yoshua Bengio. On using very large target
vocabulary for neural machine translation. In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics (ACL), pp. 1-10, 2015.
Nan Jiang and Lihong Li. Doubly robust off-policy evaluation for reinforcement learning. In
Proceedings of the 33rd International Conference on Machine Learning (ICML), pp. 652-661,
2016.
9
Published as a conference paper at ICLR 2020
David A. Levin and Yuval Peres. Markov Chains and Mixing Times. American Mathematical Society,
2nd edition, 2017. ISBN 1470429624.
Lihong Li, Remi Munos, and Csaba SzePesv陋.Toward minimax off-policy value estimation. In
Proceedings of the 18th International Conference on Artificial Intelligence and Statistics (AISTATS),
pp. 608-616, 2015.
Jun S. Liu. Monte Carlo Strategies in Scientific Computing. Springer Series in Statistics. Springer-
Verlag, 2001. ISBN 0387763694.
Qiang Liu and Jason D. Lee. Black-box importance sampling. In Proceedings of the 20th International
Conference on Artificial Intelligence and Statistics (AISTATS), pp. 952-961, 2017.
Qiang Liu and Dilin Wang. Stein variational gradient descent as moment matching. In Advances in
Neural Information Processing Systems (NIPS), pp. 8854-8863, 2018.
Qiang Liu, Lihong Li, Ziyang Tang, and Dengyong Zhou. Breaking the curse of horizon: Infinite-
horizon off-policy estimation. In Advances in Neural Information Processing Systems 31 (NeurIPS),
2018.
Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill. Off-policy policy gradient with
state distribution correction. In Proceedings of the 35th Conference on Uncertainty in Artificial
Intelligence (UAI), 2019.
Hamid Reza Maei, Csaba SzePesvðri, Shalabh Bhatnagar, and Richard S. Sutton. Toward off-policy
learning control with function approximation. In Proceedings of the 27th International Conference
on Machine Learning (ICML), pp. 719-726, 2010.
Alberto Maria Metelli, Matteo Papini, Francesco Faccio, and Marcello Restelli. Policy optimization
via importance sampling. In Advances in Neural Information Processing Systems 31 (NIPS), pp.
5447-5459, 2018.
Krikamol Muandet, Kenji Fukumizu, Bharath Sriperumbudur, and Bernhard Scholkopf. Kernel mean
embedding of distributions: A review and beyond. Foundations and Trends in Machine Learning,
10(1-2):1-141, 2017a.
Krikamol Muandet, Kenji Fukumizu, Bharath Sriperumbudur, Bernhard Scholkopf, et al. Kernel
mean embedding of distributions: A review and beyond. Foundations and TrendsR in Machine
Learning, 10(1-2):1-141, 2017b.
Remi Munos, Tom Stepleton, Anna Harutyunyan, and Marc G. Bellemare. Safe and efficient off-
policy reinforcement learning. In Advances in Neural Information Processing Systems 29 (NIPS),
pp. 1046-1054, 2016.
Susan A. Murphy, Mark van der Laan, and James M. Robins. Marginal mean models for dynamic
regimes. Journal of the American Statistical Association, 96(456):1410-1423, 2001.
Ofir Nachum, Yinlam Chow, Bo Dai, and Lihong Li. DualDICE: Behavior-agnostic estimation of
discounted stationary distribution corrections. 2019.
XuanLong Nguyen, Martin J. Wainwright, and Michael I. Jordan. Estimating divergence functionals
and the likelihood ratio by convex risk minimization. IEEE Transactions on Information Theory,
56(11):5847-5861, 2010.
Doina Precup, Richard S. Sutton, and Sanjoy Dasgupta. Off-policy temporal-difference learning with
funtion approximation. In Proceedings of the 18th Conference on Machine Learning (ICML), pp.
417-424, 2001.
Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming.
Wiley-Interscience, New York, 1994. ISBN 0-471-61977-9.
Martin Riedmiller. Neural fitted q iteration-first experiences with a data efficient neural reinforcement
learning method. In European Conference on Machine Learning, pp. 317-328. Springer, 2005.
10
Published as a conference paper at ICLR 2020
Richard S. Sutton, A. Rupam Mahmood, and Martha White. An emphatic approach to the problem
of off-policy temporal-difference learning. Journal ofMachine Learning Research, 17(73):1-29,
2016.
Philip S. Thomas and Emma Brunskill. Data-efficient off-policy policy evaluation for reinforcement
learning. In Proceedings of the 33rd International Conference on Machine Learning (ICML), pp.
2139-2148, 2016.
Mengdi Wang. Primal-dual π learning: Sample complexity and sublinear run time for ergodic Markov
decision problems, 2017. CoRR abs/1710.06100.
Tao Wang, Daniel J. Lizotte, Michael H. Bowling, and Dale Schuurmans. Stable dual dynamic
programming. In Advances in Neural Information Processing Systems 20 (NIPS), pp. 1569-1576,
2007.
Tengyang Xie, Yifei Ma, and Yu-Xiang Wang. Optimal off-policy evaluation for reinforcement
learning with marginalized importance sampling. In Advances in Neural Information Processing
Systems 32 (NeurIPS-19), 2019.
Shangtong Zhang, Wendelin Boehmer, and Shimon Whiteson. Generalized off-policy actor-critic. In
Advances in Neural Information Processing Systems 32 (NeurIPS-19), 2019.
11
Published as a conference paper at ICLR 2020
A	Reduction from Discounted to Undiscounted Reward
The same reduction is used in Liu et al. (2018). For completeness, we give the derivation details here,
for the case of finite state/actions. The derivation can be extended to general state-action spaces, with
proper adjustments in notation.
Let τ = (s0, a0, r0, s1, a1, . . .) be a trajectory generated by π, and dt ∈ ∆(S × A) be the distribution
of (st, at). Clearly,
do(s,a) = po(s)π(a∣s)
dt+i(s, a) = ^^dt(ξ,α)P(s∣ξ, α)π(a∣s),	∀t > 0 .
ξ,α
Using a matrix form, the recursion above can be written equivalently as dt+1 = PπTdt, where Pπ is
given by
P∏(s, a∣ξ, α) = P(s∣ξ, α)π(a∣s).
The discounted reward of policy π is
∞
ρ∏,γ = (I- Y)En〉： Ytrt = E(s,a)~d∏,γ [R(S,a)],
t=0
with
dπ,γ := (1 - Y) (d0 + Yd1 + Y2d2 + •…).
Multiplying both sides of above by YPπT , we have
YPTd∏,γ = (I- Y)(YPrd0 + Y2PTdι + Y3PTd2 + …)
=	(1 - Y)(YdI + Y2d2 + Y3d3 +——)
= dπ,γ - (1 - Y )d0 .
Therefore,
dπ,γ = YPπTdπ,γ + (1 - Y)d0
=	(YPπ + (1 - Y)d01T)Tdπ,γ .
Accordingly, dπ,γ is the fixed point of an induced transition matrix given by Pπ,λ := YPπ + (1 -
Y)d01T. This completes the reduction from the discounted to the undiscounted case.
B	Proof of Theorem 4.1
Note that
Dk(d k Bπd) = k [d - Bπd; d - Bπd] .
Following the definition of the strictly integrally positive definite kernels, we have thatDk(d k Bπd) =
0 implies d - Bπd = 0, which in turn implies d = dπ by the uniqueness assumption of dπ . We have
thus proved the first claim.
For the second claim, define δw = d - dπ . Since dπ - Bπ dπ = 0, we have
Dk(d k Bπd) = k[(d- Bπd); (d-Bπd)]
= k [(d - Bπd - (dπ - Bπ dπ)); (d - Bπd - (dπ - Bπ dπ))]
= k [(δw - Bπδw); (δw - Bπδw)] .
Recalling that Bnd(x) = Pχo Pn(χ∣χo)d(χo), We have
Dk(dkBπd)=k[(δw-Bπδw); (δw -Bπδw)]
=Ek(X,x)(δw (x) - Bn δw (x))(δw (x) - Bn 6仅(x))
x,x
=Ek(X,X) δw(χ) -EPn(χ∣χo)δw(χo) 11 δw(x) - EPn(χ∣χo)δw(x0) I.
χ,x	∖	xo	x ∖	χo	/
12
Published as a conference paper at ICLR 2020
Define the adjoint operator of Bπ ,
Pnf(χ) ：= XPn(Xlx)f(χ0).
x0
Denote by Pn the operator applied on k(χ,x) in terms of variable x, that is, Pχk(x,x)
PxO Pn(x0∣x)k(x0,x). This gives
Dk(d k Bnd) = £k(x,x0) δw(x) - EPn(x∣x0)δw(x0)	δw(x) - EPn(x∣xo)δw(xo)
X,X0	∖	X0	X ∖	Xq
=X δw (x) (k(x, x) - pχk(x, x) - Pnk(x, x) + PnPnk(x, x)) δ(x)
χ,X
=):δw (X)kn (x, X) δw (X),
χ,X
completing the proof.
C Proof of Theorem 4.2
First, note that the error can be decomposed as follows:
Dk(dw ∣∣Bndw) ≤ Dk(dw || Bndw) + Dk(Bndw || Bndw)
≤ Dk(dw* ||Bndw*) + Dk(Bndw ||Bndw)
_ , ..	_ , ʌ _	.. _ , ʌ - ，,一 _ 、
≤ Dk(dw* || Bndw*) + Dk(Bndw* ||Bndw*)+ Dk(BndW || Bnd1^)
≤ Dk(dw* || Bndw*) + 2 SUp Dk(Bndw || Bndw)
w∈W
= Dk(dw* || Bndw*) + 2Z,
where we define
Z := Dk(Bndw || Bndw).
Therefore, we just need to bound Z.
Denote by Bk := {f : f ∈ H, kfkH ≤ 1} the unit ball of RKHS. Define kBkk := sUpf∈B
and Rn(Bk) the expected Rademacher complexity of Bk of data size n. From classical RKHS
theory (see Lemma C.2 below), We know that IlBkk∞ ≤ rk and Rn(Bk) ≤ rk/√n, where rk :=
Psupx∈X k(x, XO).
It remains to calculate Z := sUpw∈W Dk(Bndw I Bndw)). Recall from the definition of Dk that
Dk(Bndw k BndW )) = SUp Ex~Bπdw [f (x)] - Eχ~B∏dw [f (x)].
Recall that {(xi, xi)}n=ι is a set of transitions with Xi ~ dn (∙ | Xi), where dn denotes the transition
probability under policy π. Following the definitions of Bndw and Bndw , we have
n
Bndw(x) =	w(Xi)1{X = X0i} ,
i=1
n
Bn dw (X) =	w(Xi)dn(X | Xi) .
i=1
Therefore,
. —，-△-，，一一、、
Z = SUp Dk(Bndw k Bndw))
w∈W
=suρ JuP Eχ~Bπ djf(x)] - EX~B∏ dw [f(x)]
1n
=sup	— Ew(Xi) (f (xi) - E水~d∏[f(xi)∣Xi]),
w∈W,f∈Bk n i=1	i
13
Published as a conference paper at ICLR 2020
where Xi ~ d∏ (∙ | Xi) is an independent copy of Xi that follows the same distribution. Note that Xi
is introduced only for the sample complexity analysis. Note that Z is a random variable, and E[Z]
denotes its expectation w.r.t. random data {Xi, X0i}in=1. We assume different (Xi, X0i) are independent
with each other. First, by McDiarmid inequality, we have
P (Z ≥ E[Z] + ) ≤ exp
n	ne2
I-2 kWk∞ kBkk∞
This is because when changing each data point (Xi , X0i), the maximum change on Z is at most
2 ∣∣Wk∞ ∣∣Bkk∞ In Therefore, we have Z ≤ E[Z] + ,2log(1∕δ) ∣∣W∣∣∞ ∣∣Bkk∞ /n with proba-
bility at least 1 - δ.
Accordingly, we now just need to bound E[Z]:
1n
E[Z] = EX SUp — Ew(Xi )(f(xi)- EX [f (Xi)∣xi ])
w∈W,f∈Bkn	i=1
1n
≤ ex,X	SUp	— Ew(Xi)(f(Xi)- f(Xi)
w∈W,f∈Bkn	i=1
1n
=EX,X,σ	sup	— Eσiw(Xixf(Xi) - f (Xi)
w∈W,f∈Bkn	i=1
(because {σi} are i.i.d. Rademacher random variables)
1n
≤ 2E sup	— V"σiw(Xi)f (Xi)
w∈W,f∈Bkn	i=1
=2Rn(W 0 Bk),
where
W0Bk={f (X)g(X0 ):f ∈W, g∈Bk}.
By Lemma C.1 below, we have
E[Z] ≤2Rn(W0Bk) ≤4(∣W∣∞+∣Bk∣∞)(Rn(W)+Rn(Bk)).
Combining the bounds, we have with probability 1 - δ,
2Z ≤ 4Rn(W 0 Bk) + J8l°g(I∕δ)kWk∞ kBkk∞
n
≤8(∣W∣∞+∣Bk∣∞)(Rn(W)+Rn(Bk)) +

8log(1∕δ)∣Wk∞ ∣Bk∣∞
n
Plugging Lemma C.2, we have
2Z≤8(∣W∣∞+rk)Rn(W)+
8rk (kWk∞ + rk + kWk∞ Plog(1∕δ)∕8)
√n
Assume rmax = max(∣W ∣∞ , rk). We have
2Z ≤ 16 rmaxRn(W) +
16rmax + r21ax ∙√8log(1∕δ)
Lemma C.1. Denote by ∣W∣∞ = SUp{∣f ∣∞ :f ∈ W} the super norm of a function set W. Then,
Rn(W0Bk) ≤2(∣W∣∞+∣Bk∣∞)(Rn(W)+Rn(Bk)) .
14
Published as a conference paper at ICLR 2020
Proof. Note that
f(X)g(XO)=If(X)+g(XO))2- 4(f(X)- g(XO))2.
Note that x2 is 2(kWk∞ + kBkk∞)-Lipschitzoninterval [- kWk∞ - kBkk∞ , kWk∞ + kBkk∞].
Applying Lemma C.1 of Liu & Wang (2018), we have
Rn(W 乳 Bk) ≤ 2(kWk∞ + kBkk∞)(Rn(W ㊉ Bk),
where W ㊉ Bk = {f (x) + g(X0): f ∈ W, g ∈ Bk}, and
Rn(W ㊉ Bk) = Evz[	Sup	X Zi(f(Xi) + g(Xi))]
f∈W,g∈Bk i
≤ Evz [ sup	zif(Xi)] + Evz [ sup	zig(XOi)]
f∈W i	g∈Bk i
=Rn(W)+Rn(Bk).
□
Remark The same result holds when w is defined as a function of the whole transition pair (X, XO),
that is, wi = w(Xi, XOi).
Lemma C.2. Let H be the RKHS with a positive definite kernel k(X, XO) on domain X. Let Bk =
{f∈Hk: kfkHk ≤ 1} be the Unit ball of Hk, and rk = 'suPχ∈χ k(X, xo). Then,
kBkk∞ ≤ rk,	Rn(Bk) ≤ √rkn.
Proof. These are standard results in RKHS theory, and we give a proof for completeness. For kBk k∞,
we just note that for any f ∈ Bk and X ∈ X,
f (X) = hf, k(X, ∙)iHk ≤ kf kHk kk(X, ∙)kHk ≤ kk(X, ∙)kHk The Rademacher complexity can be bounded as follows: Rn(Bk) = Eχ,σ Sup 1 X Oif(Xi) f∈Bk n i ≤ Eχ,σ ] sup *f,1 X σik(Xi, ∙) + =Eχ,σ Il1 X Oik(Xi, ∙) n i=1	Hk I	n	I2	1/2 ≤ Eχ,σ I Il n X Oik(Xi,∙) I	i=1	IHk 1/2 1n =Eχ,σ — E OiOjk(Xi,Xj) n i,j=1 1 n	1/2 =EX n Ek(Xi,g) i=1 ≤ ʃk.	=VZk(X,x) ≤ rk ∙ 1 -
□
15
Published as a conference paper at ICLR 2020
D Mini-batch training
The objective '(ω) is not in a form that is ready for mini-batch training. It is possible to yield better
scalability with a trick that has been found useful in other machine learning contexts (e.g., Jean et al.,
2015). We start with a transformed objective:
L(ω) := log '(ω)
=log X WiwjKij- 2log X Wl .
i,j	l
Then,
v7r _ Pi,j SwiWj)Kij	2 Pi Rw
VL =	—
Euv WuWv Kuv	El Wl
_ Pij WiWjKij V Iog(WiWj)	2 Pi WiV log Wi
= -------------------------------------------
Huv WuWvKuv	El Wl
ʌ	一_,	. . 一 、r	ʌ 一_ _ r
=Eij [V log(WiWj)] - Ei[V log Wi],
where EijH and Ei [∙] correspond to two properly defined discrete distributions defined on D2 and D,
respectively. Clearly, VL can now be approximated by mini-batches by drawing random samples
from D2 or D to approximate Eij and E
E Pseudo-Code of Algorithm
This section includes the pseudo-code of our algorithm that we described in Section 4.
Algorithm 1 Black-box Off-policy Estimator based on MMD
Inputs: Dataset D = {(si, ai, ri, s0i)}1≤i≤n of behavior policy πBEH, kernel function k, target
policy π, parametric model W (.).
Output: Target policy value estimate ρ∏.
1:	Estimate important weights {Wi}1≤i≤n by solving the optimization problem in equation 11.
2:	return ρ∏ = Pn=ι Wiri (see equation 6).
F	Bias-Variance Comparison
In this section, we compare the performance of our approach from the bias and variance perspective.
In particular, we focus on the ModelWin MDP (Figure 1). In order to see the impact of training data
size on the performance of different estimators, we consider different number of trajectories of length
4. For each setting, we have 200 independent runs and calculate the bias and variance of each method
based on them. We compare our approach with Liu et al. (2018) and Nachum et al. (2019) that are
two state-of-the-art methods in the off-policy estimation problem. Figure 4 shows the results of this
comparison.
As we can see, our method has a smaller bias but larger variance compared to other approaches on this
problem. As mentioned before, DualDICE (Nachum et al., 2019) does not cover the undiscounted
reward criterion. Therefore, instead of γ = 1, we have used γ = 0.9999 in the code shared by the
authors. This induces some bias but reduces the variance of this method. To confirm this observation,
we further decreased the γ to 0.9 in DualDICE and observed that reducing γ in this algorithm
increases the bias but reduces the variance at the same time.
The comparison to IPS of Liu et al. (2018) highlights the significance of an assumption needed by
IPS: the data must be drawn (approximately) from the stationary distribution of the behavior policy.
As the trajectory length is 4 in the experiment, this assumption is apparently violated, thus the high
bias of IPS. In contrast, our method does not rely on such an assumption, so is more robust.
16
Published as a conference paper at ICLR 2020
ω,≡巴 6。一
Figure 4: Bias and standard deviation of different methods for the ModelWin MDP (Fig. 1). Our
method has a smaller bias but larger variance compared to other algorithms.
Figure 5: The RMSE of different methods w.r.t the target policy reward as we change the behavior
policy. Our method outperform other approaches on different behavior policies.
G Sensitivity
Finally, in Figure 5 we measure how robust our approach is to changing the behavior policy compared
to other methods. In particular, we vary α1 that corresponds to the behavior policy to measure how the
RMSE is affected. While α2 is fixed to 0.9, in each experiment we choose α1 from {0.7, 0.5, 0.3, 0.1}.
For each experiment, we use data from 50 trajectories (with TBEH = 200) collected by the behavior
policy and report results based on 20 Monte-Carlo samples. According to Figure 5, as α1 diverges
more from α2, the performance of all the methods degrade while our method is the least affected. It
is worth mentioning that for the Mountain Car problem and α1 = 0.1, the behavior policy is close to
a random policy and hence the car has not been able to drive up to top of the hill. This means that all
the methods have constantly received a reward of -1 during all the steps and hence the estimated
on-policy reward has been -1 for all the methods as well. Therefore, the RMSE of all four methods
are equal in this case.
17