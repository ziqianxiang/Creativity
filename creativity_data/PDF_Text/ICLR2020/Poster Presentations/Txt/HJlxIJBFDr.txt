Published as a conference paper at ICLR 2020
Sample Efficient Policy Gradient Methods
with Recursive Variance Reduction
Pan Xu, Felicia Gao, Quanquan Gu
Department of Computer Science
University of California, Los Angeles
Los Angeles, CA 90094, USA
panxu@cs.ucla.edu, fxgao1160@engineering.ucla.edu, qgu@cs.ucla.edu
Ab stract
Improving the sample efficiency in reinforcement learning has been a long-
standing research problem. In this work, we aim to reduce the sample complexity
of existing policy gradient methods. We propose a novel policy gradient algo-
rithm called SRVR-PG, which only requires O(1/3/2)1 episodes to find an -
approximate stationary point of the nonconcave performance function J (θ) (i.e.,
θ such that kVJ(θ)k2 ≤ ). This sample complexity improves the existing result
O(1/5/3) for stochastic variance reduced policy gradient algorithms by a factor
of O(1/1/6). In addition, we also propose a variant of SRVR-PG with parameter
exploration, which explores the initial policy parameter from a prior probability
distribution. We conduct numerical experiments on classic control problems in
reinforcement learning to validate the performance of our proposed algorithms.
1	Introduction
Reinforcement learning (RL) (Sutton & Barto, 2018) has received significant success in solving
various complex problems such as learning robotic motion skills (Levine et al., 2015), autonomous
driving (Shalev-Shwartz et al., 2016) and Go game (Silver et al., 2017), where the agent progres-
sively interacts with the environment in order to learn a good policy to solve the task. In RL, the
agent makes its decision by choosing the action based on the current state and the historical rewards
it has received so far. After performing the chosen action, the agent’s state will change according
to some transition probability model and a new reward would be revealed to the agent by the envi-
ronment based on the action and new state. Then the agent continues to choose the next action until
it reaches a terminal state. The aim of the agent is to maximize its expected cumulative rewards.
Therefore, the pivotal problem in RL is to find a good policy which is a function that maps the state
space to the action space and thus informs the agent which action to take at each state. To optimize
the agent’s policy in the high dimensional continuous action space, the most popular approach is the
policy gradient method (Sutton et al., 2000) that parameterizes the policy by an unknown parameter
θ ∈ Rd and directly optimizes the policy by finding the optimal θ. The objective function J(θ) is
chosen to be the performance function, which is the expected return under a specific policy and is
usually non-concave. Our goal is to maximize the value of J(θ) by finding a stationary point θ*
such that kVJ (θ*)∣∣2 = 0 using gradient based algorithms.
Due to the expectation in the definition of J (θ), it is usually infeasible to compute the gradient
exactly. In practice, one often uses stochastic gradient estimators such as REINFORCE (Williams,
1992), PGT (Sutton et al., 2000) and GPOMDP (Baxter & Bartlett, 2001) to approximate the gradi-
ent of the expected return based on a batch of sampled trajectories. However, this approximation will
introduce additional variance and slow down the convergence of policy gradient, which thus requires
a huge amount of trajectories to find a good policy. Theoretically, these stochastic gradient (SG)
based algorithms require O(1/2) trajectories (Robbins & Monro, 1951) to find an -approximate
stationary point such that E[kVJ (θ)k22] ≤ . In order to reduce the variance of policy gradient
algorithms, Papini et al. (2018) proposed a stochastic variance-reduced policy gradient (SVRPG)
1O(∙) notation hides constant factors.
1
Published as a conference paper at ICLR 2020
Table 1: Comparison on sample complexities of different algorithms to achieve ∣∣VJ(θ)k2 ≤ e.
Algorithms	Complexity
REINFORCE (Williams, 1992)	O(1/2)
PGT (Sutton et al., 2000)	O(1/2)
GPOMDP (Baxter & Bartlett, 2001)	O(1/2)
SVRPG (Papini et al., 2018)	O(1/2)
SVRPG (Xu et al., 2019)	O(1/5/3)
SRVR-PG (This paper)	O(1/3/2)
algorithm by borrowing the idea from the stochastic variance reduced gradient (SVRG) (Johnson
& Zhang, 2013; Allen-Zhu & Hazan, 2016; Reddi et al., 2016a) in stochastic optimization. The
key idea is to use a so-called semi-stochastic gradient to replace the stochastic gradient used in SG
methods. The semi-stochastic gradient combines the stochastic gradient in the current iterate with a
snapshot of stochastic gradient stored in an early iterate which is called a reference iterate. In prac-
tice, SVRPG saves computation on trajectories and improves the performance of SG based policy
gradient methods. Papini et al. (2018) also proved that SVRPG converges to an -approximate sta-
tionary point θ of the nonconcave performance function J(θ) with E[∣VJ (θ)∣22] ≤ after O(1/2)
trajectories, which seems to have the same sample complexity as SG based methods. Recently, the
sample complexity of SVRPG has been improved to O(1/5/3) by a refined analysis (Xu et al.,
2019), which theoretically justifies the advantage of SVRPG over SG based methods.
This paper continues on this line of research. We propose a Stochastic Recursive Variance Reduced
Policy Gradient algorithm (SRVR-PG), which provably improves the sample complexity of SVRPG.
At the core of our proposed algorithm is a recursive semi-stochastic policy gradient inspired from
the stochastic path-integrated differential estimator (Fang et al., 2018), which accumulates all the
stochastic gradients from different iterates to reduce the variance. We prove that SRVR-PG only
takes O(1/3/2) trajectories to converge to an -approximate stationary point θ of the performance
function, i.e., E[∣VJ (θ)∣22] ≤ . We summarize the comparison of SRVR-PG with existing policy
gradient methods in terms of sample complexity in Table 1. Evidently, the sample complexity of
SRVR-PG is lower than that of REINFORCE, PGT and GPOMDP by a factor of O(1/1/2), and is
lower than that of SVRPG (Xu et al., 2019) by a factor of O(1/1/6).
In addition, we integrate our algorithm with parameter-based exploration (PGPE) method (Sehnke
et al., 2008; 2010), and propose a SRVR-PG-PE algorithm which directly optimizes the prior prob-
ability distribution of the policy parameter θ instead of finding the best value. The proposed
SRVR-PG-PE enjoys the same trajectory complexity as SRVR-PG and performs even better in some
applications due to its additional exploration over the parameter space. Our experimental results
on classical control tasks in reinforcement learning demonstrate the superior performance of the
proposed SRVR-PG and SRVR-PG-PE algorithms and verify our theoretical analysis.
1.1	Additional Related Work
We briefly review additional relevant work to ours with a focus on policy gradient based methods.
For other RL methods such as value based (Watkins & Dayan, 1992; Mnih et al., 2015) and actor-
critic (Konda & Tsitsiklis, 2000; Peters & Schaal, 2008a; Silver et al., 2014) methods, we refer the
reader to Peters & Schaal (2008b); Kober et al. (2013); Sutton & Barto (2018) fora complete review.
To reduce the variance of policy gradient methods, early works have introduced unbiased baseline
functions (Baxter & Bartlett, 2001; Greensmith et al., 2004; Peters & Schaal, 2008b) to reduce
the variance, which can be constant, time-dependent or state-dependent. Schulman et al. (2015b)
proposed the generalized advantage estimation (GAE) to explore the trade-off between bias and
variance of policy gradient. Recently, action-dependent baselines are also used in Tucker et al.
(2018); Wu et al. (2018) which introduces bias but reduces variance at the same time. Sehnke et al.
(2008; 2010) proposed policy gradient with parameter-based exploration (PGPE) that explores in
the parameter space. It has been shown that PGPE enjoys a much smaller variance (Zhao et al.,
2
Published as a conference paper at ICLR 2020
2011). The Stein variational policy gradient method is proposed in Liu et al. (2017). See Peters &
Schaal (2008b); Deisenroth et al. (2013); Li (2017) for a more detailed survey on policy gradient.
Stochastic variance reduced gradient techniques such as SVRG (Johnson & Zhang, 2013; Xiao &
Zhang, 2014), batching SVRG (Harikandeh et al., 2015), SAGA (Defazio et al., 2014) and SARAH
(Nguyen et al., 2017) were first developed in stochastic convex optimization. When the objective
function is nonconvex (or nonconcave for maximization problems), nonconvex SVRG (Allen-Zhu
& Hazan, 2016; Reddi et al., 2016a) and SCSG (Lei et al., 2017; Li & Li, 2018) were proposed and
proved to converge to a first-order stationary point faster than vanilla SGD (Robbins & Monro, 1951)
with no variance reduction. The state-of-the-art stochastic variance reduced gradient methods for
nonconvex functions are the SNVRG (Zhou et al., 2018) and SPIDER (Fang et al., 2018) algorithms,
which have been proved to achieve near optimal convergence rate for smooth functions.
There are yet not many papers studying variance reduced gradient techniques in RL. Du et al. (2017)
first applied SVRG in policy evaluation for a fixed policy. Xu et al. (2017) introduced SVRG into
trust region policy optimization for model-free policy gradient and showed that the resulting algo-
rithm SVRPO is more sample efficient than TRPO. Yuan et al. (2019) further applied the techniques
in SARAH (Nguyen et al., 2017) and SPIDER (Fang et al., 2018) to TRPO (Schulman et al., 2015a).
However, no analysis on sample complexity (i.e., number of trajectories required) was provided in
the aforementioned papers (Xu et al., 2017; Yuan et al., 2019). We note that a recent work by Shen
et al. (2019) proposed a Hessian aided policy gradient (HAPG) algorithm that converges to the sta-
tionary point of the performance function within O(H2 * * S/3/2) trajectories, which is worse than our
result by a factor of O(H2) where H is the horizon length of the environment. Moreover, they need
additional samples to approximate the Hessian vector product, and cannot handle the policy in a
constrained parameter space. Another related work pointed out by the anonymous reviewer is Yang
& Zhang (2019), which extended the stochastic mirror descent algorithm (Ghadimi et al., 2016) in
the optimization field to policy gradient methods and achieved O(H 2/2) sample complexity. Af-
ter the ICLR conference submission deadline, Yang & Zhang (2019) revised their paper by adding a
new variance reduction algorithm that achieves O(H2/3/2) sample complexity, which is also worse
than our result by a factor of O(H2).
Apart from the convergence analysis of the general nonconcave performance functions, there has
emerged a line of work (Cai et al., 2019; Liu et al., 2019; Yang et al., 2019; Wang et al., 2019) that
studies the global convergence of (proximal/trust-region) policy optimization with neural network
function approximation, which applies the theory of overparameterized neural networks (Du et al.,
2019b;a; Allen-Zhu et al., 2019; Zou et al., 2019; Cao & Gu, 2019) to reinforcement learning.
Notation kvk2 denotes the Euclidean norm of a vector v ∈ Rd and kAk2 denotes the spectral norm
of a matrix A ∈ Rd×d. We write an = O(bn) if an ≤ Cbn for some constant C > 0. The
Dirac delta function δ(x) satisfies δ(0) = +∞ and δ(x) = 0 if x 6= 0. Note that δ(x) satisfies
R+∞ δ(x)dx = 1. For any α > 0, we define the Renyi divergence (Renyi et al., 1961) between
distributions P and Q as
Da(P||Q) =」log2 Z P(x)(p(x) Y 1dx,
α- 1	2 x	Q(x)
which is non-negative for all α > 0. The exponentiated Renyi divergence is da(P||Q) = 2Da(P||Q).
2 Backgrounds on Policy Gradient
Markov Decision Process: A discrete-time Markov Decision Process (MDP) is a tuple M =
{S, A, P, r, γ, ρ}. S and A are the state and action spaces respectively. P(s0|s, a) is the tran-
sition probability of transiting to state s0 after taking action a at state s. Function r(s, a) :
S × A → [-R, R] emits a bounded reward after the agent takes action a at state s, where
R > 0 is a constant. γ ∈ (0, 1) is the discount factor. ρ is the distribution of the starting state.
A policy at state S is a probability function ∏(a∣s) over action space A. In episodic tasks, fol-
lowing any stationary policy, the agent can observe and collect a sequence of state-action pairs
τ = {s0, a0, s1, a1, . . . , sH-1, aH-1, sH}, which is called a trajectory or episode. H is called the
trajectory horizon or episode length. In practice, we can set H to be the maximum value among all
3
Published as a conference paper at ICLR 2020
the actual trajectory horizons we have collected. The sample return over one trajectory τ is defined
as the discounted cumulative reward R(τ) = PhH=-01 γhr(sh, ah).
Policy Gradient: Suppose the policy, denoted by πθ, is parameterized by an unknown parameter
θ ∈ Rd. We denote the trajectory distribution induced by policy ∏θ as p(τ∣θ). Then
H-1
p(τ∣θ) = P(S0)ɪɪ ∏θ(αh∣Sh)P(sh+ι∣Sh, ah).	(2.1)
h=0
We define the expected return under policy ∏θ as J(θ) = ET〜p(∙∣θ)[R(τ)|M], which is also called
the performance function. To maximize the performance function, we can update the policy param-
eter θ by iteratively running gradient ascent based algorithms, i.e., θk+ι = θk + ηVθ J(θk), where
η > 0 is the step size and the gradient Vθ J(θ) is derived as follows:
Vθ J(θ) = / R(T)Vθp(τ∣θ)dτ = / R(T)(Vθp(τ∣θ)∕p(τ∣θ))p(τ∣θ)dτ
=ET 〜p(∙∣θ)[Vθ log P(T ∣θ)R(τ )∣M].
(2.2)
However, it is intractable to calculate the exact gradient in (2.2) since the trajectory distribution
p(τ∣θ) is unknown. In practice, policy gradient algorithm samples a batch of trajectories {τi}N=ι to
approximate the exact gradient based on the sample average over all sampled trajectories:
1N
Vθ J(θ) = N	Vθ logp(τi∣θ)R(τi).
(2.3)
At the k-th iteration, the policy is then updated by θk+1 = θk + ηVθJ(θk). According to (2.1), we
know that Vθ logp(τ∕θ) is independent of the transition probability matrix P. Recall the definition
of R(T), we can rewrite the approximate gradient as follows
Vθ J(θ) = N XX ( HX Vθ log∏θ(ah∣sh))( Hχi Yhr(sih, ahɔ)
i=1	h=0	h=0
N
=f N X gm。),	Q4)
i=1
where Ti = {s0, a0, sɪ, a1,..., SH-ι,aH-ι, SH} for all i = 1,...,N and g(τ∕θ) is an unbiased
gradient estimator computed based on the i-th trajectory Ti . The gradient estimator in (2.4) is based
on the likelihood ratio methods and is often referred to as the REINFORCE gradient estimator
(Williams, 1992). Since E[Vθ log∏θ(a|s)] = 0, we can add any constant baseline bt to the reward
that is independent of the current action and the gradient estimator still remains unbiased. With
the observation that future actions do not depend on past rewards, another famous policy gradient
theorem (PGT) estimator (Sutton et al., 2000) removes the rewards from previous states:
H -1	H -1
g(τilθ) = X vθ logπθ(ah|sh)( X γtr(si,ai) - bt i,	(2.5)
h=0	t=h
where bt is a constant baseline. It has been shown (Peters & Schaal, 2008b) that the PGT estimator is
equivalent to the commonly used GPOMDP estimator (Baxter & Bartlett, 2001) defined as follows:
H -1	h
g(τilθ) = X I Xvθlogπθ(at|st) )(γhr(sh,ah) -bh).	(2.6)
h=0	t=0
All the three gradient estimators mentioned above are unbiased (Peters & Schaal, 2008b). It has
been proved that the variance of the PGT/GPOMDP estimator is independent of horizon H while
the variance of REINFORCE depends on H polynomially (Zhao et al., 2011; Pirotta et al., 2013).
Therefore, we will focus on the PGT/GPOMDP estimator in this paper and refer to them inter-
changeably due to their equivalence.
4
Published as a conference paper at ICLR 2020
3 The Proposed Algorithm
The approximation in (2.3) using a batch of trajectories often causes a high variance in practice.
In this section, we propose a novel variance reduced policy gradient algorithm called stochastic
recursive variance reduced policy gradient (SRVR-PG), which is displayed in Algorithm 1. Our
SRVR-PG algorithm consists of S epochs. In the initialization, we set the parameter of a reference
policy to be θe0 = θ0 . At the beginning of the s-th epoch, where s = 0, . . . , S - 1, we set the
initial policy parameter θ0s+1 to be the same as that of the reference policy θes . The algorithm
then samples N episodes {τi }iN=1 from the reference policy πθes to compute a gradient estimator
Vs = 1/NPN=Ig(τ,∣θs), where g(τ∕θs) is the PGT/GPOMDP estimator. Then the policy is
immediately update as in Line 6 of Algorithm 1.
Within the epoch, at the t-th iteration, SRVR-PG samples B episodes {τj }jB=1 based on the current
policy πθs+1 . We define the following recursive semi-stochastic gradient estimator:
vts+1
1B	1B
B X g(Tj ∣θs + 1) - B X gω (Tj ∣θs+1 ) + Vt-1,
j=1
j=1
(3.1)
where the first term is a stochastic gradient based on B episodes sampled from the current policy,
and the second term is a stochastic gradient defined based on the step-wise important weight between
the current policy πθs+1 and the reference policy πθes. Take the GPOMDP estimator for example,
for a behavior policy πθ1 and a target policy πθ2 , the step-wise importance weighted estimator is
defined as follows
H-1	h
gω (Tj lθl) = X "0:h(T lθ2, θl)( X Vθ2 log πθ2(aj |sj ) 卜hr (Sh,ah),	(3.2)
h=0	t=0
where ω0:h(τ∣Θ2, θι) = Qh,=。∏θ2(ah∣sh)∕∏θι (ah∣sh) is the importance weight fromp(τh∣θs+1)
to p(τh∣θs+-1) and Th is a truncated trajectory {(at, st)}h=° from the full trajectory T. It is easy to
Verifythat ET 〜p(τ ∣θι)[gω (Tjlθ1)] = ET 〜p(τ ∣Θ2) [g(τ lθ2 )].
The difference between the last two terms in (3.1) can be viewed as a control variate to reduce the
variance of the stochastic gradient. In many practical applications, the policy parameter space is
a subset of Rd, i.e., θ ∈ Θ with Θ ⊆ Rd being a convex set. In this case, we need to project
the updated policy parameter onto the constraint set. Base on the semi-stochastic gradient (3.1),
we can update the policy parameter using projected gradient ascent along the direction of vts+1:
θts++11 = PΘ(θts+1 + ηvts+1), where η > 0 is the step size and the projection operator associated
with Θ is defined as
Pθ(θ) = argmin ∣∣θ — u∣∣2 = argmin {lθ(u) + 用0 — u∣∣2},	(3.3)
u∈Θ	u∈Rd
where 1Θ (u) is the set indicator function on Θ, i.e., 1Θ (u) = 0 if u ∈ Θ and 1Θ (u) = +∞
otherwise. η > 0 is any finite real value and is chosen as the step size in our paper. It is easy
to see that 1θ(∙) is nonsmooth. At the end of the s-th epoch, We update the reference policy as
θs+1 = θms+1, where θms+1 is the last iterate of this epoch.
The goal of our algorithm is to find a point θ ∈ Θ that maximizes the performance function J(θ)
subject to the constraint, namely, maxθ∈Θ J(θ) = maxθ∈Rd {J (θ) - 1Θ(θ)}. The gradient norm
∣VJ (θ)∣2 is not sufficient to characterize the convergence of the algorithm due to additional the
constraint. Following the literature on nonsmooth optimization (Reddi et al., 2016b; Ghadimi et al.,
2016; Nguyen et al., 2017; Li & Li, 2018; Wang et al., 2018), we use the generalized first-order
stationary condition: Gη(θ) = 0, where the gradient mapping Gη is defined as follows
Gn(θ) = 1(Pθ(Θ + ηVJ(θ)) - θ).	(3.4)
η
We can view Gη as a generalized projected gradient at θ. By definition if Θ = Rd, we have
Gη(θ) ≡ VJ (θ). Therefore, the policy is update is displayed in Line 10 in Algorithm 1, where
5
Published as a conference paper at ICLR 2020
Algorithm 1 Stochastic Recursive Variance Reduced Policy Gradient (SRVR-PG)
1:	Input: number of epochs S, epoch size m, step size η, batch size N, mini-batch size B, gradient
estimator g, initial parameter θe0 = θ0 ∈ Θ
2:	for s = 0, . . . , S - 1 do
3:	θ0s+1 = θes
4:	Sample N trajectories {τi} fromp(∙∣θs)
5:	vS+1 = Vθ J(θs) := 1/NPN=i g(τi∣θ∙s)
6:	θ1s+1 = PΘ(θ0s+1 + ηv0s+1)
7:	for t = 1, . . . , m - 1 do
8:	Sample B trajectories {τ7-} fromp(∙∣θt+1)
9:	vS+1 = vS+1 + B PB=1 (g(τj ∣θS+1) -gω (Tj ∣θS+1))
10:	θts++11 = PΘ(θts+1 + ηvts+1)
11:	end for
12:	θes+1 = θms+1
13:	end for
14:	return θout, which is uniformly picked from {θts}t=0,...,m-1;s=0,...,S
prox is the proximal operator defined in (3.3). Similar recursive semi-stochastic gradients to (3.1)
were first proposed in stochastic optimization for finite-sum problems, leading to the stochastic re-
cursive gradient algorithm (SARAH) (Nguyen et al., 2017; 2019) and the stochastic path-integrated
differential estimator (SPIDER) (Fang et al., 2018; Wang et al., 2018). However, our gradient es-
timator in (3.1) is noticeably different from that in Nguyen et al. (2017); Fang et al. (2018); Wang
et al. (2018); Nguyen et al. (2019) due to the gradient estimator gω(τj∣θt+1) defined in (3.2) that is
equipped with step-wise importance weights. This term is essential to deal with the non-stationarity
of the distribution of the trajectory τ. Specifically, {τj}jB=1 are sampled from policy πθs+1 while the
PGT/GPOMDP estimator g(∙∣θt+1) is defined based on policy n®s+1 according to (2.6). This in-
consistency introduces extra challenges in the convergence analysis of SRVR-PG. Using importance
weighting, we can obtain
ET 〜p(τ ∣θs + 1)[gω (T lθs+1)] = ET 〜p(τ ∣θs+1)[g(T lθs+1 )]，
which eliminates the inconsistency caused by the varying trajectory distribution.
It is worth noting that the semi-stochastic gradient in (3.1) also differs from the one used in SVRPG
(Papini et al., 2018) because we recursively update vts+1 using vts-+11 from the previous iteration,
while SVRPG uses a reference gradient that is only updated at the beginning of each epoch. More-
over, SVRPG wastes N trajectories without updating the policy at the beginning of each epoch,
while Algorithm 1 updates the policy immediately after this sampling process (Line 6), which saves
computation in practice.
We notice that very recently another algorithm called SARAPO (Yuan et al., 2019) is proposed which
also uses a recursive gradient update in trust region policy optimization (Schulman et al., 2015a).
Our Algorithm 1 differs from their algorithm at least in the following ways: (1) our recursive gradient
vts defined in (3.1) has an importance weight from the snapshot gradient while SARAPO does not;
(2) we are optimizing the expected return while Yuan et al. (2019) optimizes the total advantage
over state visitation distribution and actions under KullbackLeibler divergence constraint; and most
importantly (3) there is no convergence or sample complexity analysis for SARAPO.
4 Main Theory
In this section, we present the theoretical analysis of Algorithm 1. We first introduce some common
assumptions used in the convergence analysis of policy gradient methods.
Assumption 4.1. Let ∏θ(a|s) be the policy parameterized by θ. There exist constants G,M > 0
such that the gradient and Hessian matrix of log ∏θ(a|s) with respect to θ satisfy
kVθlog∏θ(a∣s)k ≤ G,	∣∣Vθlog∏θ(a∣s)∣∣2 ≤ M,
for all a ∈ A and s ∈ S .
6
Published as a conference paper at ICLR 2020
The above boundedness assumption is reasonable since we usually require the policy function to
be twice differentiable and easy to optimize in practice. Similarly, in Papini et al. (2018), the au-
∂	∂2
thors assume that 品 log∏θ(a|s) and dθ-∂θ- log∏θ(a|s) are upper bounded elementwisely, which
is actually stronger than our Assumption 4.1.
In the following proposition, we show that Assumption4.1 directly implies that the Hessian ma-
trix of the performance function V2J(θ) is bounded, which is often referred to as the smoothness
assumption and is crucial in analyzing the convergence of nonconvex optimization (Reddi et al.,
2016a; Allen-Zhu & Hazan, 2016).
Proposition 4.2. Let g(τ∣θ) be the PGT estimator defined in (2.5). Assumption 4.1 implies:
(1)	. kg(τ∣θι) — g(τ∣θ2)k2 ≤ L∣∣θι — Θ2k2, ∀θι, θ2 ∈ Rd, where the smoothness parameter is
L = M R/(1 -γ)2 +2G2R/(1 - γ)3;
(2)	. J(θ) is L-smooth, namely kV2θ J (θ)k2 ≤ L;
(3)	. IIg(T∣θ)∣∣2 ≤ Cg forall θ ∈ Rd, with Cg = GR∕(1 — γ)2.
Similar properties are also proved in Xu et al. (2019). However, in contrast to their results, the
smoothness parameter L and the bound on the gradient norm here do not rely on horizon H . When
H ≈ 1∕(1 -γ) and γ is sufficiently close to 1, we can see that the order of the smoothness parameter
is O(1∕(1 -γ)3), which matches the order O(H2∕(1 -γ)) in Xu et al. (2019). The next assumption
requires the variance of the gradient estimator is bounded.
Assumption 4.3. There exists a constant ξ > 0 such that Var(g(τ∣θ)) ≤ ξ2, for all policy ∏θ.
In Algorithm 1, we have used importance sampling to connect the trajectories between two different
iterations. The following assumption ensures that the variance of the importance weight is bounded,
which is also made in Papini et al. (2018); Xu et al. (2019).
Assumption 4.4. Let ω(∙∣θι, θ2) = p(∙∣θ1)∕p(∙∣θ2). There is a constant W < ∞ such that for each
policy pairs encountered in Algorithm 1,
Var(ω(τ∣θι, θ2)) ≤ W, ∀θι, θ2 ∈ Rd,τ 〜p(∙∣θ2).
4.1	Convergence Rate and Sample Complexity of SRVR-PG
Now we are ready to present the convergence result of SRVR-PG to a stationary point:
Theorem 4.5. Suppose that Assumptions 4.1, 4.3 and 4.4 hold. In Algorithm 1, we choose the step
size η ≤ 1∕(4L) and epoch size m and mini-batch size B such that
B≥
72mηG2 (2G2 ∕M + 1)(W+1)γ
(1-γ)2
Then the generalized projected gradient of the output of Algorithm 1 satisfies
8[J(θ*) - J(θo) - 1θ(Θ*) + 1θ(Θo)] ɪ 6ξ2
+ ^N,
ηSm
where θ* = argmaxθ∈θ J(θ).
Remark 4.6. Theorem 4.5 states that under a proper choice of step size, batch size and epoch length,
the expected squared gradient norm of the performance function at the output of SRVR-PG is in the
order of
O(Sm+N).
Recall that S is the number of epochs and m is the epoch length of SRVR-PG, so Sm is the total
number of iterations of SRVR-PG. Thus the first term O(1∕(Sm)) characterizes the convergence
rate of SRVR-PG. The second term O(1∕N) comes from the variance of the stochastic gradient
used in the outer loop, where Nis the batch size used in the snapshot gradient v0s+1 in Line 5 of
SRVR-PG. Compared with the O(1∕(Sm) + 1∕N+ 1∕B) convergence rate in Papini et al. (2018),
7
Published as a conference paper at ICLR 2020
our analysis avoids the additional term O(1/B) that depends on the mini-batch size within each
epoch.
Compared with Xu et al. (2019), our mini-batch size B is independent of the horizon length H . This
enables us to choose a smaller mini-batch size B while maintaining the same convergence rate. As
we will show in the next corollary, this improvement leads to a lower sample complexity.
Corollary 4.7. Suppose the same conditions as in Theorem 4.5 hold. Set step size as η = 1/(4L),
the batch size parameters as N = O(1/) and B = O(1/1/2) respectively, epoch length as m =
O(1/1/2) and the number of epochs as S = O(1/1/2). Then Algorithm 1 outputs a point θout that
satisfies E[kGη(θout)k22] ≤ within O(1/3/2) trajectories in total.
Note that the results in PaPini et al. (2018); XU et al. (2019) are for ∣∣VθJ(θ)k2 ≤ e, while our
result in Corollary 4.7 is more general. In particular, when the policy parameter θ is defined on the
whole sPace Rd instead of Θ, our result reduces to the case for ∣Vθ J (θ)∣22 ≤ since Θ = Rd
and Gη(θ) = Vθ J (θ). In Xu et al. (2019), the authors imProved the samPle comPlexity of SVRPG
(PaPini et al., 2018) from O(1/2) to O(1/5/3) by a sharPer analysis. According to Corollary
4.7, SRVR-PG only needs O(1/3/2) number of trajectories to achieve ∣Vθ J (θ)∣22 ≤ , which is
lower than the samPle comPlexity of SVRPG by a factor of O(1/1/6). This imProvement is more
Pronounced when the required Precision is very small.
4.2 Implication for Gaussian Policy
Now, we consider the Gaussian Policy model and Present the samPle comPlexity of SRVR-PG in
this setting. For bounded action sPace A ⊂ R, a Gaussian Policy Parameterized by θ is defined as
1	(θ>φ(s) - a)2
πθ(a|S) = √2∏ eχp (----------2σ2—,,	(4.1)
where σ2 is a fixed standard deviation Parameter and φ : S 7→ Rd is a maPPing from the state
sPace to the feature sPace. For Gaussian Policy, under the mild condition that the actions and the
state feature vectors are bounded, we can verify that AssumPtions 4.1 and 4.3 hold, which can
be found in APPendix D. It is worth noting that AssumPtion 4.4 does not hold trivially for all
Gaussian distributions. In particular, Cortes et al. (2010) showed that for two Gaussian distribu-
tions ∏θι (a|s)〜 N(μι,σ2) and ∏θ2(a∣s)〜 N(μ2,σ2), if σ? > √2∕2σι, then the variance of
ω(τ∣θι, θ2) is bounded. For our Gaussian policy defined in (4.1) where the standard deviation σ2
is fixed, we have σ > √2∕2σ trivially hold, and therefore Assumption 4.4 holds for some finite
constant W > 0 according to (2.1).
Recall that Theorem 4.5 holds for any general models under Assumptions 4.1, 4.3 and 4.4. Based
on the above arguments, we know that the convergence analysis in Theorem 4.5 applies to Gaussian
policy. In the following corollary, we present the sample complexity of Algorithm 1 for Gaussian
policy with detailed dependency on precision parameter , horizon size H and the discount factor γ .
Corollary 4.8. Given the Gaussian policy defined in (4.1), suppose Assumption 4.4 holds and we
have |a| ≤ Ca for all a ∈ A and ∣φ(s)∣2 ≤ Mφ for all s ∈ S, where Ca, Mφ > 0 are constants. If
we set step size as η = O((1-γ)3), the mini-batch sizes and epoch length as N = O((1-γ)-3-1),
B = O((1 一 Y)TET/2) and m = O((1 一 γ)-2e-1/2), then the output of Algorithm 1 satisfies
E[∣Gη(θout)k2] ≤ E after O(1∕((1 一 Y)4e3/2)) trajectories in total.
Remark 4.9. For Gaussian policy, the number of trajectories Algorithm 1 needs to find an E-
approximate stationary point, i.e., E[∣Gη(θout)∣22] ≤ E, is also in the order of O(E-3/2), which
is faster than PGT and SVRPG. Additionally, we explicitly show that the sample complexity does
not depend on the horizon H, which is in sharp contrast with the results in Papini et al. (2018); Xu
et al. (2019). The dependence on 1∕(1 一 Y) comes from the variance of PGT estimator.
5	Experiments
In this section, we provide experiment results of the proposed algorithm on benchmark reinforce-
ment learning environments including the Cartpole, Mountain Car and Pendulum problems. In all
8
Published as a conference paper at ICLR 2020
×ιo3	×ιo1	×ιo3
00 b
_ ____ ____
Erq9sa,6",∙la,>a
625	1250	1875	2500
Number of Trajectories
(a) Cartpole
Enla,≈ωraEω><
-7.5-.	.	.	.	.
0	750	1500	2250	3000
Number of Trajectories
(b) Mountain Car
Enlα,≈ωraEω><
2.0
XlO5
0.0	0.5	1.0	1.5
Number of Trajectories
(c) Pendulum
XlO3
625	1250	1875	2500
Number OfTrajectories
(d) Cartpole
-7∙5- 0
XlO1
750	1500	2250
Number OfTrajectories
3000
(e) Mountain Car
XlO3
0.0	0.5	1.0	1.5	2.0
Number OfTrajectories ×w5
(f) Pendulum
Figure 1: (a)-(c): Comparison of different algorithms. Experimental results are averaged over 10
repetitions. (d)-(f): Comparison of different batch size B on the performance of SRVR-PG.
the experiments, we use the Gaussian policy defined in (4.1). In addition, we found that the proposed
algorithm works well without the extra projection step. Therefore, we did not use projection in our
experiments. For baselines, we compare the proposed SRVR-PG algorithm with the most relevant
methods: GPOMDP (Baxter & Bartlett, 2001) and SVRPG (Papini et al., 2018). For the learning
rates η in all of our experiments, we use grid search to directly tune η. For instance, we searched
η for the Cartpole problem by evenly dividing the interval [10-5, 10-1] into 20 points in the log-
space. For the batch size parameters N and B and the epoch length m, according to Corollary 4.7,
we choose N = O(1/), B = O(1/1/2) and thusm = O(1/1/2), where > 0 is a user-defined
precision parameter. In our experiments, we set N = C0/, B = C1/1/2 and m = C2/1/2
and tune the constant parameters C0, C1, C2 using grid search. The detailed parameters used in the
experiments are presented in Appendix E.
We evaluate the performance of different algorithms in terms of the total number of trajectories they
require to achieve a certain threshold of cumulative rewards. We run each experiment repeatedly
for 10 times and plot the averaged returns with standard deviation. For a given environment, all
experiments are initialized from the same random initialization. Figures 1(a), 1(b) and 1(c) show
the results on the comparison of GPOMDP, SVRPG, and our proposed SRVR-PG algorithm across
three different RL environments. It is evident that, for all environments, GPOMDP is overshadowed
by the variance reduced algorithms SVRPG and SRVR-PG significantly. Furthermore, SRVR-PG
outperforms SVRPG in all experiments, which is consistent with the comparison on the sample
complexity of GPOMDP, SVRPG and SRVR-PG in Table 1.
Corollaries 4.7 and 4.8 suggest that When the mini-batch size B is in the order of O(√N), SRVR-PG
achieves the best performance. Here N is the number of episodes sampled in the outer loop of Al-
gorithm 1 and B is the number of episodes sampled at each inner loop iteration. To validate our
theoretical result, We conduct a sensitivity study to demonstrate the effectiveness of different batch
sizes Within each epoch of SRVR-PG on its performance. The results on different environments are
displayed in Figures 1(d), 1(e) and 1(f) respectively. To interpret these results, We take the Pendu-
lum problem as an example. In this setting, We choose outer loop batch size N of Algorithm 1 to
be N = 250. By Corollary 4.8, the optimal choice of batch size in the inner loop of Algorithm 1 is
B = C√N, where C > 1 is a constant depending on horizon H and discount factor γ. Figure 1(f)
9
Published as a conference paper at ICLR 2020
shows that B = 50 ≈ 3√N yields the best convergence results for SRVR-PG on Pendulum, which
validates our theoretical analysis and implies that a larger batch size B does not necessarily result
in an improvement in sample complexity, as each update requires more trajectories, but a smaller
batch size B pushes SRVR-PG to behave more similar to GPOMDP. Moreover, by comparing with
the outer loop batch size N presented in Table 2 for SRVR-PG in Cartpole and Mountain Car envi-
ronments, we found that the results in Figures 1(d) and 1(e) are again in alignment with our theory.
Due to the space limit, additional experiment results are included in Appendix E.
6 Conclusions
We propose a novel policy gradient method called SRVR-PG, which is built on a recursively updated
stochastic policy gradient estimator. We prove that the sample complexity of SRVR-PG is lower than
the sample complexity of the state-of-the-art SVRPG (Papini et al., 2018; Xu et al., 2019) algorithm.
We also extend the new variance reduction technique to policy gradient with parameter-based ex-
ploration and propose the SRVR-PG-PE algorithm, which outperforms the original PGPE algorithm
both in theory and practice. Experiments on the classic reinforcement learning benchmarks validate
the advantage of our proposed algorithms.
Acknowledgments
We would like to thank the anonymous reviewers for their helpful comments. We would also like
to thank Rui Yuan for pointing out an error on the calculation of the smoothness parameter for the
performance function in the previous version. This research was sponsored in part by the National
Science Foundation IIS-1904183, IIS-1906169 and Adobe Data Science Research Award. The views
and conclusions contained in this paper are those of the authors and should not be interpreted as
representing any funding agencies.
References
Zeyuan Allen-Zhu and Elad Hazan. Variance reduction for faster non-convex optimization. In
International Conference on Machine Learning, pp. 699-707, 2016.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning, pp. 242-252, 2019.
Jonathan Baxter and Peter L Bartlett. Infinite-horizon policy-gradient estimation. Journal of Artifi-
cial Intelligence Research, 15:319-350, 2001.
Qi Cai, Zhuoran Yang, Jason D Lee, and Zhaoran Wang. Neural temporal-difference learning con-
verges to global optima. In Advances in Neural Information Processing Systems, 2019.
Yuan Cao and Quanquan Gu. A generalization theory of gradient descent for learning over-
parameterized deep relu networks. arXiv preprint arXiv:1902.01384, 2019.
Corinna Cortes, Yishay Mansour, and Mehryar Mohri. Learning bounds for importance weighting.
In Advances in Neural Information Processing Systems, pp. 442-450, 2010.
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method
with support for non-strongly convex composite objectives. In Advances in Neural Information
Processing Systems, pp. 1646-1654, 2014.
Marc Peter Deisenroth, Gerhard Neumann, Jan Peters, et al. A survey on policy search for robotics.
Foundations and TrendsR in Robotics, 2(1-2):1-142, 2013.
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In International Conference on Machine Learning, pp. 1675-
1685, 2019a.
Simon S Du, Jianshu Chen, Lihong Li, Lin Xiao, and Dengyong Zhou. Stochastic variance reduction
methods for policy evaluation. In Proceedings of the 34th International Conference on Machine
Learning-Volume 70, pp. 1049-1058. JMLR. org, 2017.
10
Published as a conference paper at ICLR 2020
Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In International Conference on Learning Representations,
2019b. URL https://openreview.net/forum?id=S1eK3i09YQ.
Cong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang. Spider: Near-optimal non-convex op-
timization via stochastic path-integrated differential estimator. In Advances in Neural Information
Processing Systems, pp. 686-696, 2018.
Saeed Ghadimi, Guanghui Lan, and Hongchao Zhang. Mini-batch stochastic approximation meth-
ods for nonconvex stochastic composite optimization. Mathematical Programming, 155(1-2):
267-305, 2016.
Evan Greensmith, Peter L Bartlett, and Jonathan Baxter. Variance reduction techniques for gradient
estimates in reinforcement learning. Journal of Machine Learning Research, 5(Nov):1471-1530,
2004.
Reza Harikandeh, Mohamed Osama Ahmed, Alim Virani, Mark Schmidt, Jakub Konecny, and Scott
Sallinen. Stopwasting my gradients: Practical svrg. In Advances in Neural Information Process-
ing Systems, pp. 2251-2259, 2015.
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance
reduction. In Advances in Neural Information Processing Systems, pp. 315-323, 2013.
Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey. The
International Journal of Robotics Research, 32(11):1238-1274, 2013.
Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In Advances in Neural Information
Processing Systems, pp. 1008-1014, 2000.
Lihua Lei, Cheng Ju, Jianbo Chen, and Michael I Jordan. Non-convex finite-sum optimization via
scsg methods. In Advances in Neural Information Processing Systems, pp. 2348-2358, 2017.
Sergey Levine, Nolan Wagener, and Pieter Abbeel. Learning contact-rich manipulation skills with
guided policy search. In 2015 IEEE International Conference on Robotics and Automation
(ICRA), pp. 156-163. IEEE, 2015.
Yuxi Li. Deep reinforcement learning: An overview. CoRR, abs/1701.07274, 2017. URL http:
//arxiv.org/abs/1701.07274.
Zhize Li and Jian Li. A simple proximal stochastic gradient method for nonsmooth nonconvex
optimization. In Advances in Neural Information Processing Systems, pp. 5569-5579, 2018.
Boyi Liu, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural proximal/trust region policy opti-
mization attains globally optimal policy. In Advances in Neural Information Processing Systems,
2019.
Yang Liu, Prajit Ramachandran, Qiang Liu, and Jian Peng. Stein variational policy gradient. CoRR,
abs/1704.02399, 2017. URL http://arxiv.org/abs/1704.02399.
Alberto Maria Metelli, Matteo Papini, Francesco Faccio, and Marcello Restelli. Policy optimization
via importance sampling. In Advances in Neural Information Processing Systems, pp. 5447-5459,
2018.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. Nature, 518(7540):529, 2015.
Lam M Nguyen, Jie Liu, Katya Scheinberg, and Martin Takac. Sarah: A novel method for machine
learning problems using stochastic recursive gradient. In Proceedings of the 34th International
Conference on Machine Learning-Volume 70, pp. 2613-2621. JMLR. org, 2017.
Lam M Nguyen, Marten van Dijk, Dzung T Phan, Phuong Ha Nguyen, Tsui-Wei Weng, and
Jayant R Kalagnanam. Optimal finite-sum smooth non-convex optimization with sarah. CoRR,
abs/1901.07648, 2019. URL http://arxiv.org/abs/1901.07648.
11
Published as a conference paper at ICLR 2020
Matteo Papini, Damiano Binaghi, Giuseppe Canonaco, Matteo Pirotta, and Marcello Restelli.
Stochastic variance-reduced policy gradient. In International Conference on Machine Learning,
pp. 4023-4032, 2018.
Jan Peters and Stefan Schaal. Natural actor-critic. Neurocomputing, 71(7-9):1180-1190, 2008a.
Jan Peters and Stefan Schaal. Reinforcement learning of motor skills with policy gradients. Neural
Networks, 21(4):682-697, 2008b.
Matteo Pirotta, Marcello Restelli, and Luca Bascetta. Adaptive step-size for policy gradient meth-
ods. In Advances in Neural Information Processing Systems, pp. 1394-1402, 2013.
Sashank J Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and Alex Smola. Stochastic variance
reduction for nonconvex optimization. In International Conference on Machine Learning, pp.
314-323, 2016a.
Sashank J Reddi, Suvrit Sra, Barnabas Poczos, and Alexander J Smola. Proximal stochastic methods
for nonsmooth nonconvex finite-sum optimization. In Advances in Neural Information Processing
Systems, pp. 1145-1153, 2016b.
Alfred Renyi et al. On measures of entropy and information. In Proceedings ofthe Fourth Berkeley
Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of
Statistics. The Regents of the University of California, 1961.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathemat-
ical Statistics, pp. 400-407, 1951.
John Schulman, Sergey Levine, Pieter Abbeel, Michael I Jordan, and Philipp Moritz. Trust region
policy optimization. In International Conference on Machine Learning, volume 37, pp. 1889-
1897, 2015a.
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-
dimensional continuous control using generalized advantage estimation. CoRR, abs/1506.02438,
2015b. URL https://arxiv.org/abs/1506.02438.
Frank Sehnke, Christian Osendorfer, Thomas RuCkstieβ, Alex Graves, Jan Peters, and Jurgen
Schmidhuber. Policy gradients with parameter-based exploration for control. In International
Conference on Artificial Neural Networks, pp. 387-396. Springer, 2008.
Frank Sehnke, Christian Osendorfer, Thomas RUCkStieβ, Alex Graves, Jan Peters, and JUrgen
Schmidhuber. Parameter-exploring policy gradients. Neural Networks, 23(4):551-559, 2010.
Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe, multi-agent, reinforcement
learning for autonomous driving. CoRR, abs/1610.03295, 2016. URL http://arxiv.org/
abs/1610.03295.
Zebang Shen, Alejandro Ribeiro, Hamed Hassani, Hui Qian, and Chao Mi. Hessian aided policy
gradient. In International Conference on Machine Learning, pp. 5729-5738, 2019.
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.
Deterministic policy gradient algorithms. In International Conference on Machine Learning,
2014.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go
without human knowledge. Nature, 550(7676):354, 2017.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient
methods for reinforcement learning with function approximation. In Advances in Neural Infor-
mation Processing Systems, pp. 1057-1063, 2000.
12
Published as a conference paper at ICLR 2020
George Tucker, Surya Bhupatiraju, Shixiang Gu, Richard Turner, Zoubin Ghahramani, and Sergey
Levine. The mirage of action-dependent baselines in reinforcement learning. In International
Conference on Machine Learning, pp. 5022-5031, 2018.
Lingxiao Wang, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural policy gradient methods: Global
optimality and rates of convergence. arXiv preprint arXiv:1909.01150, 2019.
Zhe Wang, Kaiyi Ji, Yi Zhou, Yingbin Liang, and Vahid Tarokh. Spiderboost: A class of faster
variance-reduced algorithms for nonconvex optimization. CoRR, abs/1810.10690, 2018. URL
http://arxiv.org/abs/1810.10690.
Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279-292, 1992.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine Learning, 8(3-4):229-256, 1992.
Cathy Wu, Aravind Rajeswaran, Yan Duan, Vikash Kumar, Alexandre M Bayen, Sham Kakade,
Igor Mordatch, and Pieter Abbeel. Variance reduction for policy gradient with action-dependent
factorized baselines. In International Conference on Learning Representations, 2018. URL
https://openreview.net/forum?id=H1tSsb-AW.
Lin Xiao and Tong Zhang. A proximal stochastic gradient method with progressive variance reduc-
tion. SIAM Journal on Optimization, 24(4):2057-2075, 2014.
Pan Xu, Felicia Gao, and Quanquan Gu. An improved convergence analysis of stochastic variance-
reduced policy gradient. In International Conference on Uncertainty in Artificial Intelligence,
2019.
Tianbing Xu, Qiang Liu, and Jian Peng. Stochastic variance reduction for policy gradient estimation.
CoRR, abs/1710.06034, 2017. URL http://arxiv.org/abs/1710.06034.
Long Yang and Yu Zhang. Policy optimization with stochastic mirror descent. arXiv preprint
arXiv:1906.10462, 2019.
Zhuoran Yang, Yongxin Chen, Mingyi Hong, and Zhaoran Wang. On the global convergence of
actor-critic: A case for linear quadratic regulator with ergodic cost. In Advances in Neural Infor-
mation Processing Systems, 2019.
Huizhuo Yuan, Chris Junchi Li, Yuhao Tang, and Yuren Zhou. Policy optimization via stochas-
tic recursive gradient algorithm, 2019. URL https://openreview.net/forum?id=
rJl3S2A9t7.
Tingting Zhao, Hirotaka Hachiya, Gang Niu, and Masashi Sugiyama. Analysis and improvement of
policy gradient estimation. In Advances in Neural Information Processing Systems, pp. 262-270,
2011.
Tingting Zhao, Hirotaka Hachiya, Voot Tangkaratt, Jun Morimoto, and Masashi Sugiyama. Efficient
sample reuse in policy gradients with parameter-based exploration. Neural computation, 25(6):
1512-1547, 2013.
Dongruo Zhou, Pan Xu, and Quanquan Gu. Stochastic nested variance reduced gradient descent for
nonconvex optimization. In Advances in Neural Information Processing Systems, pp. 3922-3933,
2018.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes
over-parameterized deep relu networks. Machine Learning, 2019.
13
Published as a conference paper at ICLR 2020
A Extension to Parameter-based Exploration
Although SRVR-PG is proposed for action-based policy gradient, it can be easily extended to the
policy gradient algorithm with parameter-based exploration (PGPE) (Sehnke et al., 2008). Unlike
action-based policy gradient in previous sections, PGPE does not directly optimize the policy param-
eter θ but instead assumes that it follows a prior distribution with hyper-parameter ρ: θ 〜 P(MP).
The expected return under the policy induced by the hyper-parameter ρ is formulated as follows2 *
J(ρ) =
p(θ∣ρ)p(τ ∣θ)R(τ )dτ dθ.
(A.1)
PGPE aims to find the hyper-parameter ρ* that maximizes the performance function J(P) Since
p(θ∣ρ) is stochastic and can provide sufficient exploration, We can choose ∏θ(a|s) = δ(a - μe(S))
to be a deterministic policy, where δ is the Dirac delta function and μθ(∙) is a deterministic function.
For instance, a linear deterministic policy is defined as ∏θ(a|s) = δ(a - θ>s) (Zhao et al., 2011;
Metelli et al., 2018). Given the policy parameter θ, a trajectory τ is only decided by the initial state
distribution and the transition probability. Therefore, PGPE is called a parameter-based exploration
approach. Similar to the action-based policy gradient methods, we can apply gradient ascent to find
ρ*. In the k-th iteration, we update Pk by ρk+ι = Pk + ηVρ J(ρ). The exact gradient of J(ρ) with
respect to P is given by
VρJ(ρ) =
p(θ∣ρ)p(τ ∣θ)Vρ log p(θ∣ρ)R(τ )dτ dθ.
To approximate VPJ(ρ), we first sample N policy parameters {θi} from p(θ∣ρ). Then we sample
one trajectory τi for each θi and use the following empirical average to approximate VρJ(ρ)
NH	N
V PJ (ρ) = N X Vρ log p(θi∣ρ) X Y hr(sh,ah) ：= N X gRρ),	(A.2)
i=1	h=0	i=1
where γ ∈ [0, 1) is the discount factor. Compared with the PGT/GPOMDP estimator in Section 2,
the likelihood term VP logp(θi∣ρ) in (A.2) for PGPE is independent of horizon H.
Algorithm 1 can be directly applied to the PGPE setting, where we replace the policy parameter θ
with the hyper-parameter ρ. When we need to sample N trajectories, we first sample N policy pa-
rameters {θi} from p(θ∣ρ). Since the policy is deterministic with given θ%, we sample one trajectory
Ti from each policy p(τ∣θi). The recursive semi-stochastic gradient is given by
1B	1B
vS+1 = B Eg(TjIρS+1) - B ∑gω(Tj∣ρs+1)+ vt-1,	(A.3)
j=1	j=1
where gω (τj ∣ρS+1) is the gradient estimator with step-wise importance weight defined in the way as
in (3.2). We call this variance reduced parameter-based algorithm SRVR-PG-PE, which is displayed
in Algorithm 2.
Under similar assumptions on the parameter distributionp(θ∣ρ), as Assumptions 4.1,4.3 and4.4, we
can easily prove that SRVR-PG-PE converges to a stationary point of J(ρ) with O(1/3/2) sample
complexity. In particular, we assume the policy parameter θ follows the distribution p(θ∣ρ) and
we update our estimation of ρ based on the semi-stochastic gradient in (A.3). Recall the gradient
VPJ(ρ) derived in (A.2). Since the policy in SRVR-PG-PE is deterministic, we only need to make
the boundedness assumption on p(θ∣ρ). In particular, we assume that
1.	∣∣Vρ logp(θ∣ρ)k2 and ∣∣Vp logp(θ∣ρ) ∣∣2 are bounded by constants in a similar way to As-
sumption 4.1;
2.	the gradient estimator g(τ∣ρ) = VP logp(θ∣ρ) PH=0 γhr(sh, ah) has bounded variance;
3.	and the importance weight ω(τj∣ρS+1, ρS+1) = p(θj ∣ρS+1)∕p(θj∣ρS+1) has bounded vari-
ance in a similar way to Assumption 4.4.
2We slightly abuse the notation by overloading J as the performance function defined on the hyper-
parameter ρ.
14
Published as a conference paper at ICLR 2020
Then the same gradient complexity O(1/3/2) for SRVR-PG-PE can be proved in the same way
as the proof of Theorem 4.5 and Corollary 4.7. Since the analysis is almost the same as that of
SRVR-PG, we omit the proof of the convergence of SRVR-PG-PE. In fact, according to the analysis
in Zhao et al. (2011); Metelli et al. (2018), all the three assumptions listed above can be easily
verified under a Gaussian prior for θ and a linear deterministic policy.
Algorithm 2 Stochastic Recursive Variance Reduced Policy Gradient with Parameter-based Explo-
ration (SRVR-PG-PE)
1: Input: number of epochs S, epoch size m, step size η, batch size N, mini-batch size B, gradient
estimator g, initial parameter ρ0m := ρe0 := ρ0
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
for s = 0, . . . , S - 1 do
ρs0+1 = ρs
Sample N policy parameters {θi} fromp(∙∣ρs)
Sample one trajectory τi from each policy πθi
vs+1 = VPJ(Ps) = N P=1 g(τi∣ρs)
ρs1+1 = ρs0+1 + ηvs0+1
for t = 1, . . . , m - 1 do
Sample B policy parameters {θj} fromp(∙∣ρs+1)
Sample one trajectory τj from each policy πθj
vs+1 = vs+1 + B Pf=1 (g(Tj ∣Ps+1) - gω (Tj ∣ρs+1))
ρts++11 = ρts+1 + ηvts+1
end for
end for
return ρout, which is uniformly picked from {ρts }t=0,...,m;s=0,...,S
B Proof of the Main Theory
In this section, we provide the proofs of the theoretical results for SRVR-PG (Algorithm 1). Before
we start the proof of Theorem 4.5, we first lay down the following key lemma that controls the
variance of the importance sampling weight ω.
LemmaB.1. For any θι, θ2 ∈ Rd, let s。：% (T ∣θι, θ2) = p(τh∣θ1)∕p(τh∣θ2),where Th is a truncated
trajectory of τ up to step h. Under Assumptions 4.1 and 4.4, it holds that
Var(so：h(T∣θι,θ2)) ≤ Cω∣∣θι - θ2∣∣2,
where Cω = h(2hG2 +M)(W + 1).
Recall that in Assumption 4.4 we assume the variance of the importance weight is upper bounded by
a constant W. Based on this assumption, Lemma B.1 further bounds the variance of the importance
weight via the distance between the behavioral and the target policies. As the algorithm converges,
these two policies will be very close and the bound in Lemma B.1 could be much tighter than the
constant bound.
Proof of Theorem 4.5. By plugging the definition of the projection operator in (3.3) into the update
rule ΘS+1 = Pθ(θS+1 + ηvS+1), we have
θs+1 = argmin 1θ(u) + 1∕(2η)∣∣u — θs+1∣∣2 - hvs+1, u〉.	(B.1)
u∈Rd
Similar to the generalized projected gradient Gη(θ) defined in (3.4), we define Gets+1 tobe a (stochas-
tic) gradient mapping based on the recursive gradient estimator vts+1:
Gt+1 = 1 (θS+1 - θS+1) = 1 (Pθ(θS+1 + ηvS+1) - θS+1).	(B.2)
The definition of Gets+1 differs from Gη(θts+1) only in the semi-stochastic gradient term vts+1, while
the latter one uses the full gradient VJ(θS+1). Note that 1θ(∙) is convex but not smooth. We
15
Published as a conference paper at ICLR 2020
assume that P ∈ ∂ 1©(。；+1) is a sub-gradient of 1θ(∙). According to the optimality condition of
(B.1), We have P + 1∕η(θt+1 -。；+1) - v；+1 = 0. Further by the convexity of 1θ(∙), We have
iθ(θS+1) ≤ 1θ(ΘS+1) + hp, θt+1 - θs+1i
=1θ(ΘS+1) - {1∕η(θS+1 - θs+1) - vs+1, ΘS+1 -毋+1〉.	(B.3)
By Proposition 4.2, J(θ) is L-smooth, which by definition directly implies
J(θ+1) ≥ J(θs+1) + <vj(θs+1), θs+1 - θs+1> - 2憧+IL- θs+⅛
For the simplification of presentation, let us define the notation Φ(θ) = J(θ) - 1θ(θ). Then ac-
cording to the definition of 1© we have argmax0e>d Φ(θ) = argmax0e© J(θ) := θ*. Combining
the above inequality With (B.3), We have
Φ(θs+1) ≥ Φ(θs+1) + (vj(θ;+1) -vs+1, θs+1 - θs+1) + G - 2) ∣∣θs+1 - θs+1 俏
=Φ(θS+1) + (vj(θs+1) - vS+1,ηGs+1> + η∣∣es+1∣∣2 - 2∣∣θs+1 - θs+1∣∣2
≥ φ(θs+1) - 2∣∣VJ(θs+1) - vs+1∣∣2 + 2∣甲1∣∣2 - 2∣∣θs+1 - θs+1∣∣2
=φ(θs+1) - 2∣∣VJ (θs+1) -VsT∣2 + 4∣∣es+1∣∣2 + (ɪ - 2) ∣∣θs+1 - θs+1∣∣2
η
≥ φ(θs+1) - 2∣∣vj (θs+1) -VsT∣2+8匠(θs+ι)∣∣2
-4 国(θs+1) - Gs+1k2 + (4η - 2) ∣ ∣ θs+L - θs+1∣∣2,	(B.4)
where the second inequality holds due to Young,s inequality and the third inequality holds due to
thefaCtthatMn(θs+1)∣∣2 ≤ 2∣腐+1∣∣2 +2|寓(θs+1) -Gs+1∣∣2. Denote卿 1 = prox,lθ(θs+1 +
ηVJ(θs+1)). By similar argument in (B.3) we have
1θ(θs+1) ≤ 1©(第 1) -h1∕η(θs+1 - θs+1) - vs+1, θs+1 - θs+1i,
1©(卿 1) ≤ 1θ(θs+1) -h1∕η(θs+1 - θs+1) - VJ (θs+1),卿1 - θs+1i.
Adding the above two inequalities immediately yields ∣∣θs+1 - θs+1∣∣2 ≤ η∣∣VJ(θs+1) - vs+1∣∣2,
which further implies ∣∣Gn(θs+1) -Gs+1∣∣2 ≤ ∣∣VJ(θs+1) - vs+1∣∣2. Submitting this result into
(B.4), we obtain
φ(θs+1) ≥ Φ(θs+1)-? ∣∣VJ (θs+1) - vs+1∣∣2 + 8属(θs+1)∣∣2
+ (⅛ - 2 )∣ ∣ θs+1 - θs+1∣∣2.	(B.5)
We denote the index set of {τj}^=I in the t-th inner iteration by Bt. Note that
I∣vj(θs+1) - vs+1∣∣2
1	2
=VJ(θs+1) - vs+1 + B X (gω (Tj∣θs+1) -g(Tj ∣θs+1))
B j∈Bt	2
1	2
=VJ(θs+1) - VJ(θs+1) + - E (gω (Tj ∣θs+1) - g(Tj ∣θs+1)) + VJ(θs+1) - vs+J
B j∈Bt	2
1 2
=VJ(θs+1) - VJ(θs+1) + - E (gω (Tj ∣θs+1) - g(Tj ∣θs+1))
j∈Βt
2
+ 万 X <VJ(θs+1) -VJ(θs+1) + gω (Tj ∣θs+1) - g(Tj ∣θs+1), VJ(θs+1)-vs+1>
B j∈Βt
16
Published as a conference paper at ICLR 2020
+ ∣∣ V J (θS+1)-vS+⅛.	(B.6)
Conditional on θ^+1, taking the expectation over Bt yields
EKVJ(θS+1) - 9(引毋+1), VJ(θS+1) - v；+1〉] = 0.
Similarly, taking the expectation over e；+1 and the choice of Bt yields
EKVJ(C⅛) -gωS优+1),VJ(犯1) -v=1〉] =0.
Combining the above equations with (B.6), we obtain
E[∣∣ VJ(θs+1) - vs+1∣∣2]
I	2
=E VJ®；+1) -VJ⑹+1) + 万 E ®(引/+1) -g(τj∣θs+1))
B j∈Bt	2
+ E ∣ ∣ VJ (毋+1)-vS+1∣∣2
=W X E ∣ ∣ VJ(θ;+1) -VJ(/+1) + gω (τj∣θS+1) - 9(万战+1) ∣ ∣ ；
j∈Bt
+ E ∣ ∣ VJ(毋+1)- vS+1∣∣2,	(B.7)
≤ W X E ∣ ∣ gω S∣θS+1) - g(Tj 战+1) ∣ ∣ 2 + ∣∣VJ (苏+1) - vS+1∣∣2,	(B.8)
j∈Bt
where (B.7) is due to the fact that E∣∣x1 + ... + xnk2 = E∣∣x11∣2 + ... + EkXnk2 for in-
dependent zero-mean random variables, and (B.8) holds due to the fact that x1,..., Xn is due
to EkX - Ex∣∣2 ≤ EkXk2. For the first term, We have ∣∣gω (τ7-∣θS+1) - g(τ7-优+1) ∣ ∣ 2 ≤
11gω (Tj ∣θt-1) - g(τ7-1θt+1) ∣∣2 + L∣∣。；+1 -。；+1112 by triangle inequality and Proposition 4.2.
2
E[ ∣ ∣ gω(τj∣θS+1) - g(τj∣θS+1) ∣ ∣ 2] = E
L h=0
H-1
H-1	h
X ("0：h - 1) X Ve logπθ(αt∣st) Yhr(Sh,αh)
EE	(30：h - 1) EVelogπθ(αt∣st) Yhr(Sh,αh)
h=0
H-1
≤ X h2(2G2 + M)(W + 1) ∣ ∣ θs+1 - θs+1∣∣2 ∙ h2G2γhR
h=0
24RG2(2G2 + M )(W +1)γ
(I-Y)5
θs+1∣∣2,	(B.9)
≤
—
2
2
2
where in the second equality we used the fact that E[V logπθ(α∣s)] = 0, the first inequality is due
to Lemma B.1 and in the last inequality we use the fact that P∞0 h4Yh = Y(Y3 + 11γ2 + 11γ +
1)/(1 - y)5 for ∣y∣ < 1. Combining the results in (B.8) and (B.9), we get
E ∣ ∣ VJ(θS+1) - vS+1∣∣2 ≤ C∣∣θS+1 - θS+1∣∣2 + ∣∣VJ(θS+1) - vS+1∣∣2
≤ C X∣∣θS+1 - θS+11∣∣2 + ∣∣VJ(θS+1) - vS+1∣∣2,	(B.10)
l=1
which holds for t = 1,..., m — 1, where CY = 24RG2(2G2 + M)(W + 1)y∕(1 — y)5. According
to Algorithm 1 and Assumption 4.3, we have
E∣∣ VJ(θ0+1) -vS+1∣∣2 ≤ N2.	(B.11)
Submitting the above result into (B.5) yields
EN,B [φ(θS+1)] ≥ EN,B [φ(θs+1)] + 8∣∣Gη(θS+1)∣∣2 + & - L) ∣∣θS+1 - θS+1∣∣2
17
Published as a conference paper at ICLR 2020
-专En,b 忙愉+1- *R - 喝,	(B.12)
L l=1	」
for t = 1,..., m - 1 .Recall Line 6 in Algorithm 1, where We update θij+1 With the average of a
mini-batch of gradients Vs = 1/NPN=I g(τ∕θs). Similar to (B.5), by smoothness of J(θ), we
have
巩好+1) ≥ φ(θs+1) - 3η∣∣vj(θs+1) -VsTl2 + 8∣∣Gη侬+1)∣∣2
+ (4⅛ - 2 >s+1-θs+1∣∣2.
Further by (B.11), it holds that
E[φ(θs+1)] ≥ E[φ(θs+1)]-誓 + 8∣∣Gη(θs+1)∣∣2 + (4⅛ - 2) llθs+1 - θs+1∣∣2. (B.13)
Telescoping inequality (B.12) from t = 1 to m - 1 and combining the result with (B.13), we obtain
. ”口 ...............  i
ENB [φ(θm+ )] ≥ EN,B [φ(θ0+ )] +8 E EN [ ∣ ∣ Gη (θs+ ) ∣ ∣ 2]-4N~
t=0
+ α - 2) EII叫-θs+1∣∣2
-3⅛ En,b] X⅛∣∣θs+1-笔 It
L t=0 1=1	」
≥ EN,B [φ(θ0+1)] + 8 E EN [ ∣ ∣ Gn (θs+1) ∣ ∣ 2] - 3；N
t=0
+ (} - 2 -当)E1∣*1-θs+ι∣∣2.
If We choose step size η and the epoch length B such that
1 B	3ηCγ _ 72ηG2(2G2 + M)(W + 1)γ
η ≤ 4L, m ≥ L =	M(1 - γ)2	,
and note that θs+1 = θs, θs+1 = θs+1, then (B.14) leads to
^m-1	*a2
EN [φ(θs+1)] ≥ EN [φ(θs)] + 8 E EN[ ∣ ∣ Gn(θs+1) ∣ ∣ 2] - -mf.
t=0
Summing up the above inequality over S = 0,..., S - 1 yields
(B.14)
(B.15)
(B.16)
S —1 m -1	Qq ¢2
8 EE E[ ∣ ∣ Gn (θs+1) ∣ ∣ 2] ≤ E[φ(/)]-E[φ(θ0)] + -Sm^
s=0 t=0
which immediately implies
E[∣∣Gn(θout) ∣ ∣ 2] ≤
8(叫Φ(θs)] - E[Φ(θ0)])	6ξ2
-----------—— + —
ηSm-----N
8(Φ(θ*)- Φ(θ0)) ɪ 6ξ2
------二------+ _ _
ηSm----------N
This completes the proof.
□
ProofofCorollary 4.7. Based on the convergence results in Theorem 4.5, in order to ensure
E[ ∣ ∣ VJ(θout) ∣ ∣ 2] ≤ GWe can choose S, m and N such that
8(J(θ*) - J(θ0))	E	6ξ2	E
	=—. --=—
ηSm----------------------2,	N 2
18
Published as a conference paper at ICLR 2020
which implies Sm = O(1/) and N = O(1/). Note that we have set m = O(B). The total
number of stochastic gradient evaluations Tg we need is
Tg=SN+SmB=O(B+f)
where we set B = 1/1/2.
□
C Proof of Technical Lemmas
In this section, we provide the proofs of the technical lemmas. We first prove the smoothness of the
performance function J (θ).
Proof of Proposition 4.2. Recall the definition of PGT in (2.5). We first show the Lipschitzness of
g(τ∣θ) with baseline b = 0 as follows:
kVg(τ ∣θ)∣∣2
X Vθlogπθ(ah|sh)f X γtr(st, at)
h=0	t=h	2
≤ (X Yhllvθlogπθ(at|st升2)占
t=0	- γ
/ MR
≤ (1-Y)2,
where we used the fact that 0 < γ < 1. When we have a nonzero baseline bh, we can simply scale
it with γh and the above result still holds up to a constant multiplier.
Since the PGT estimator is an unbiased estimator of the policy gradient vθJ(θ), we have vθJ(θ) =
Eτ[g(τ∣θ)] and thus
vθ J(θ) = / P(T∣θ)Vθg(τ∣θ)dτ + / P(T∣θ)g(τ∣θ)Vθ logP(T∣θ)dτ
=ET [Vθ g(τ ∣θ)] + ET [g(τ ∣Θ)Vθ log p(τ ∣θ)].
(C.1)
We have already bounded the norm of the first term by MR∕(1 - γ)2. NoW We take a look at the
second term. Plugging the equivalent definition of g(τ∣θ) in (2.6) yields
Eτ[g(τ ∣Θ)Vθ log p(τ ∣θ)]
H-1	h
I X I XVθlog∏θ(at∣st) IYhr(sh,a%)Vθlogp(τ∣θ) ∙p(τ∣θ)dτ
T h=0 t=0
H-1	h	H-1
I E I £Velog∏θ(at∣st) lγhr(sh,ah £ Vθlog∏θ侬Μ)∙p(τ∣θ)dτ
T h=0	t=0	t0=0
H-1	h	h
/ E I	Vθlog∏θ(at∣st) YhY(Sh,ah) £ Vθlogπ侬即，)∙p(τ∣θ)dτ,
T h=0	t=0	t0=0
(C.2)
where the second equality is due to VθP (st0+1 |st0, at0) = 0, and the last equality is due to the fact
that for all t0 > h it holds that
h
I	Vθlog∏Θ(at∣St)γhr(sh,ah)Vθlog∏θ(at，M) ∙p(τ∣θ)dτ = 0.
T t=0
Therefore, we have
H-1 h
∣∣Eτ[g(τ∣θ)Vθlogp(τ∣θ)]k2 ≤ ET XX
GYhR × (h+1)G
h=0 t=0
19
Published as a conference paper at ICLR 2020
H-1
= X G2R(h + 1)2γh
h=0
2G2R
’(I-Y)3.
Putting the above pieces together, we can obtain
2	MR	2G2R
llvθJ(θV∣2 ≤ (1 - γ)2 + (1 - γ)3 := L,
which implies that J(θ) is L-smooth with L = M R/(1 - γ)2 + 2G2R/(1 - γ)3.
Similarly, we can bound the norm of gradient estimator as follows
ll H-1	γhR(1 - γH-h) ll	GR
kg(Tlθ)k2 ≤	£ Re logπθ(ah∖sh)-------;--------- ≤ ∩-------τ2,
l h=0	1 - γ l2	(1 - γ)2
which completes the proof.	□
Lemma C.1 (Lemma 1 in Cortes et al. (2010)). Let ω(x) = P (x)/Q(x) be the importance weight
for distributions P and Q. Then E[ω] = 1, E[ω2] = d2(P ∖∖Q), where d2(P ∖∖Q) = 2D2 (P ||Q)
and D2(P∖∖Q) is the Renyi divergence between distributions P and Q. Note that this immediately
implies Var(ω) = d2(P∖∖Q) - 1.
Proof of Lemma B.1. According to the property of importance weight in Lemma C.1, we know
VarMh(T∖θs, θt+1)) = d2(p(τh∖θs)∖∖p(τh∖θS+1)) - 1.
To simplify the presentation, we denote θ1 = θes and θ2 = θts+1 in the rest of this proof. By
definition, we have
d2 (P(Th∖θι)∖∖p(Th∖θ2))=ZTP(Th∖θι) Po2jdτ
p(Th ∖θ1)2p(Th ∖θ2)-1dT.
Taking the gradient of d2(P(Th∖θ1)∖∖P(Th ∖θ2)) with respect to θ1, we have
Vθ1d2(P(Th∖θ1)∖∖P(Th∖θ2)) = 2	P(Th∖θ1)Vθ1P(Th∖θ1)P(Th∖θ2)-1dT.
In particular, if we set the value of θ1 to be θ1 = θ2 in the above formula of the gradient, we get
Rθ1d2(p(Th∖θ1)∖∖p(Th∖θ2))θ1=θ2
2	Vθ1P(Th∖θ1)dTθ1=θ2 = 0.
Applying mean value theorem with respect to the variable θ1 , we have
d2(p(τh∖θ1 )∖∖p(τh∖θ2)) = 1 + 1∕2(θι - θ2)>Vθd2(p(τh∖θ)∖∖p(τh∖θ2))(θ1 - θ2),	(C.3)
where θ = tθ1 + (1 - t)θ2 for some t ∈ [0, 1] and we used the fact that d2(p(Th∖θ2)∖∖p(Th ∖θ2)) = 1.
To bound the above exponentiated Renyi divergence, We need to compute the Hessian matrix. Taking
the derivative of Vθ1d2(p(Th ∖θ1)∖∖p(Th∖θ2)) with respect to θ1 further yields
R2θd2(p(Th∖θ)∖∖p(Th∖θ2)) = 2	Rθlogp(Th∖θ)Rθ log p(Th∖θ)
> p(Th ∖θ)2
p(Th∖θ2)
dT
+ 2	V2θp(Th∖θ)p(Th∖θ)p(Th∖θ2)-1dT.
(C.4)
Thus we need to compute the Hessian matrix of the trajectory distribution function, i.e., V2θp(Th∖θ),
which can further be derived from the Hessian matrix of the log-density function.
V2θ log p(Th∖θ) = -p(Th∖θ)-2Vθp(Th∖θ)Vθp(Th∖θ)> + p(Th∖θ)-1V2θp(Th∖θ).	(C.5)
20
Published as a conference paper at ICLR 2020
Submitting (C.5) into (C.4) yields
l∣Vθ d2 (P(Th⑼IIp(Th©2))k2
4τ
Vθ log P(Thm) vθ log p(τhlθ)> p∣≡ dτ
+ 2 Z vθlogP(ThIe)p(lθldτ
τ	p(Th Iθ2 )	2
≤ Z pτhθ2 (4kVθ logp(τh∣θ)k2 + 2∣Vθ logP(ThIθ)k2)dτ
τ p(Th Iθ2 )
≤ (4h2G2 +2hM)E[ω(TIθ,θ2)2]
≤ 2h(2hG2 + M)(W + 1),
where the second inequality comes from Assumption 4.1 and the last inequality is due to Assumption
4.4 and Lemma C.1. Combining the above result with (C.3), we have
Var30：h(T∣es, θS+1)) = d2(ρ(Th∣θs)∣∣p(Th∣θS+1)) - 1 ≤ Cωkes - ΘS+1∣2,
where Cω = h(2hG2 + M)(W + 1).	□
D Proof of Theoretical Results for Gaus sian Policy
In this section, we prove the sample complexity for Gaussian policy. According to (4.1), we can
calculate the gradient and Hessian matrix of the logarithm of the policy.
(a - θ > φ(s))φ(s)	2	φ(s)φ(s)>
V log ∏θ(a∣s) = --------2---------,	V2 log ∏θ(a∣s) =-------2——.	(D.1)
σ2	σ2
It is easy to see that Assumption 4.1 holds with G = CaMφ∕σ2 and M = Mφ∕σ2. Based on this
observation, Proposition 4.2 also holds for Gaussian policy with parameters defined as follows
r	RMΦ	…C	RCaMφ
L = σ2(1 - γ)3 , and Cg = σ2(1 - γ)2 .
(D.2)
The following lemma gives the variance ξ2 of the PGT estimator, which verifies Assumption 4.3.
Lemma D.1 (Lemma 5.5 in Pirotta et al. (2013)). Given a Gaussian policy ∏θ(a∣s) 〜
N(θ>φ(s), σ2), if the Ir(s, a)I ≤ R and lφ(s)l2 ≤ Mφ for all s ∈ S,a ∈ A and R,Mφ > 0
are constants, then the variance of PGT estimator defined in (2.5) can be bounded as follows:
Var(g(T Iθ)) ≤ ξ2
R2 M (3 -Hγ2H - 2γH」
(1 - γ)2σ2 V 1 - Y2 Y Y 1 - Y
Proof of Corollary 4.8. The proof will be similar to that of Corollary 4.7. By Theorem 4.5, to ensure
that E[lVJ(θout)l22] ≤ , we can set
8(J(θ*) - J(θo)) _ C	6ξ2 _ C
-------T--------=-.	--=一
ηSm	2 N 2
Plugging the value of ξ2 in Lemma D.1 into the second equation above yields N = O(C-1(1-Y)-3).
For the first equation, We have S = O(1∕(ηmc)). Therefore, the total number of stochastic gradient
evaluations Tg required by Algorithm 1 is
Tg = SN + SmB = O( -N- + B).
ηmC ηC
So a good choice of batch size B and epoch length m will lead to Bm = N. Combining this with
the requirement of B in Theorem 4.5, we can set
21
Published as a conference paper at ICLR 2020
Note that CY = 24RG2(2G2 + M)(W + 1)γ∕(1 - γ)5. Plugging the values of G, N and L into the
above equations yields
m=o((1-D, B=o((i-D
The corresponding sample complexity is
This completes the proof for Gaussian policy.
□
E	Additional details on experiments
Now, we provide more details of our experiments presented in Section 5. We first present the param-
eters for all algorithms we used in all our experiments in Tables 2 and 3. Among the parameters, the
neural network structure and the RL environment parameters are shared across all the algorithms.
As mentioned in Section 5, the order of the batch size parameters of our algorithm are chosen ac-
cording to Corollary 4.7 and we multiply them by a tuning constant via grid search. Similarly, the
orders of batch size parameters of SVRPG and GPOMDP are chosen based on the theoretical re-
sults suggested by Papini et al. (2018); Xu et al. (2019). Moerover, the learning rates for different
methods are tuned by grid search.
We then present the results of PGPE and SRVR-PG-PE on Cartpole, Mountain Car and Pendulum
in Figure 2. In all three environments, our SRVR-PG-PE algorithm shows improvement over PGPE
(Sehnke et al., 2010) in terms of number of trajectories. It is worth noting that in all these environ-
ments both PGPE and SRVR-PG-PE seem to solve the problem very quickly, which is consistent
with the results reported in (Zhao et al., 2011; 2013; Metelli et al., 2018). Our primary goal in this
experiment is to show that our proposed variance reduced policy gradient algorithm can be easily
extended to the PGPE framework. To avoid distracting the audience’s attention from the variance
reduction algorithm on the sample complexity, we do not thoroughly compare the performance of
the parameter based policy gradient methods such as PGPE and SRVR-PG-PE with the action based
policy gradient methods. We refer interested readers to the valuable empirical studies of PGPE based
algorithms presented in Zhao et al. (2011; 2013); Metelli et al. (2018).
EnttHc,6e∙lc,><
Entt≈c,6e∙lc,><
×102
×103
5 0 5
■ ■ ■
oil
- - -
En6e∙l><
O 500 IOOO 1500	2000	' O 125	250	375	500	6	350	700	1050	1400	1750
Number OfTrajectories	Number OfTrajectories	Number OfTrajectories
(a) Cartpole	(b) Mountain Car	(c) Pendulum
Figure 2: Performance of SRVR-PG-PE compared with PGPE. Experiment results are averaged over
10 runs.
22
Published as a conference paper at ICLR 2020
Table 2: Parameters used in the SRVR-PG experiments.
Parameters	Algorithm	Cartpole	Mountain Car	Pendulum
NN size	-	64	64	8×8
NN activation function	-	Tanh	Tanh	Tanh
Task horizon	-	100	1000	200
Total trajectories	-	2500	3000	2 × 105
	GPOMDP	0.99	0.999	0.99
Discount factor γ	SVRPG	0.999	0.999	0.995
	SRVR-PG	0.995	0.999	0.995
	GPOMDP	0.005	0.005	0.01
Learning rate η	SVRPG	0.0075	0.0025	0.01
	SRVR-PG	0.005	0.0025	0.01
	GPOMDP	10	10	250
Batch size N	SVRPG	25	10	250
	SRVR-PG	25	10	250
	GPOMDP	-	-	-
Batch size B	SVRPG	10	5	50
	SRVR-PG	5	3	50
	GPOMDP	-	-	-
Epoch size m	SVRPG	3	2	1
	SRVR-PG	3	2	1
Table 3: Parameters used in the SRVR-PG-PE experiments.
Parameters	Cartpole	Mountain Car	Pendulum
NN size	-	64	8×8
NN activation function	Tanh	Tanh	Tanh
Task horizon	100	1000	200
Total trajectories	2000	500	1750
Discount factor γ	0.99	0.999	0.99
Learning rate η	0.01	0.0075	0.01
Batch size N	10	5	50
Batch size B	5	3	10
Epoch size m	2	1	2
23