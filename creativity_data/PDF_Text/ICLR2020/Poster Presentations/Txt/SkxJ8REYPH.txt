Published as a conference paper at ICLR 2020
SlowMo: Improving Communication-Efficient
Distributed SGD with Slow Momentum
Jianyu Wang*
Department of Electrical and Computer Engineering
Carnegie Mellon University
Pittsburgh, PA 15213, USA
jianyuw1@andrew.cmu.edu
Vinayak Tantia, Nicolas Ballas & Michael Rabbat
Facebook AI Research
Montreal, Canada
{tantia, ballasn, mikerabbat}@fb.com
Ab stract
Distributed optimization is essential for training large models on large datasets.
Multiple approaches have been proposed to reduce the communication overhead
in distributed training, such as synchronizing only after performing multiple lo-
cal SGD steps, and decentralized methods (e.g., using gossip algorithms) to de-
couple communications among workers. Although these methods run faster than
AllReduce-based methods, which use blocking communication before every
update, the resulting models may be less accurate after the same number of up-
dates. Inspired by the BMUF method of Chen & Huo (2016), we propose a slow
momentum (SlowMo) framework, where workers periodically synchronize and
perform a momentum update, after multiple iterations of a base optimization algo-
rithm. Experiments on image classification and machine translation tasks demon-
strate that SlowMo consistently yields improvements in optimization and gen-
eralization performance relative to the base optimizer, even when the additional
overhead is amortized over many updates so that the SlowMo runtime is on par
with that of the base optimizer. We provide theoretical convergence guarantees
showing that SlowMo converges to a stationary point of smooth non-convex
losses. Since BMUF can be expressed through the SlowMo framework, our
results also correspond to the first theoretical convergence guarantees for BMUF.
1	Introduction
Distributed optimization (Chen et al., 2016; Goyal et al., 2017) is essential for training large models
on large datasets (Radford et al., 2019; Liu et al., 2019; Mahajan et al., 2018b). Currently, the
most widely-used approaches have workers compute small mini-batch gradients locally, in parallel,
and then aggregate these using a blocking communication primitive, AllReduce, before taking
an optimizer step. Communication overhead is a major issue limiting the scaling of this approach,
since AllReduce must complete before every step and blocking communications are sensitive to
stragglers (Dutta et al., 2018; Ferdinand et al., 2019).
Multiple complementary approaches have recently been investigated to reduce or hide communi-
cation overhead. Decentralized training (Jiang et al., 2017; Lian et al., 2017; 2018; Assran et al.,
2019) reduces idling due to blocking and stragglers by employing approximate gradient aggregation
(e.g., via gossip or distributed averaging). Approaches such as Local SGD reduce the frequency
of communication by having workers perform multiple updates between each round of communi-
cation (McDonald et al., 2010; McMahan et al., 2017; Zhou & Cong, 2018; Stich, 2019; Yu et al.,
2019b). It is also possible to combine decentralized algorithms with Local SGD (Wang & Joshi,
* Work performed while doing an internship at Facebook AI Research.
1
Published as a conference paper at ICLR 2020
2018; Wang et al., 2019). These approaches reduce communication overhead while injecting addi-
tional noise into the optimization process. Consequently, although they run faster than large mini-
batch methods, the resulting models may not achieve the same quality in terms of training loss or
generalization accuracy after the same number of iterations.
Momentum is believed to be a critical component for training deep networks, and it has been em-
pirically demonstrated to improve both optimization and generalization (Sutskever et al., 2013).
Yet, there is no consensus on how to combine momentum with communication efficient training
algorithms. Momentum is typically incorporated into such approaches by having workers maintain
separate buffers which are not synchronized (Lian et al., 2017; 2018; Assran et al., 2019; Koloskova
et al., 2019a). However, recent work shows that synchronizing the momentum buffer, using periodic
AllReduce or a decentralized method, leads to improvements in accuracy at the cost of dou-
bling the communication overhead (Yu et al., 2019a). In block-wise model update filtering (BMUF),
nodes perform multiple local optimization steps between communication rounds (similar to local
SGD), and they also maintain a momentum buffer that is only updated after each communication
round (Chen & Huo, 2016). Although it is now commonly used for training speech models, there
are no theoretical convergence guarantees for BMUF, and it has not been widely applied to other
tasks (e.g., in computer vision or natural language processing).
Inspired by BMUF, we propose a general framework called slow momentum (SlowMo) to improve
the accuracy of communication-efficient distributed training methods. SlowMo runs on top of a
base algorithm, which could be local SGD or a decentralized method such as stochastic gradient
push (SGP) (Nedic & Olshevsky, 2016; Assran et al., 2019). Periodically, after taking some number
τ of base algorithm steps, workers average their parameters using AllReduce and perform a mo-
mentum update. We demonstrate empirically that SlowMo consistently improves optimization and
generalization performance across a variety of base algorithms on image classification and neural
machine translation tasks—training ResNets on CIFAR-10 and ImageNet, and training a transformer
on WMT’16 En-De. Ultimately, SlowMo allows us to reap the speedup and scaling performance
of communication-efficient distributed methods without sacrificing as much in accuracy.
We also prove theoretical bounds showing that SLOWMO converges to a stationary point of smooth
non-convex functions at a rate O (1/√mTτ) after TT total inner optimization steps and T SLOWMO
updates with m worker nodes, for a variety of base optimizers. Thus, SlowMo is order-wise no
slower than stochastic gradient descent. BMUF and the recently-proposed Lookahead optimizer
(Zhang et al., 2019) can be expressed through the SlowMo framework, and so our results also
translate to the first theoretical convergence guarantees for both of these methods.
2	The Slow Momentum (SlowMo) Framework
SlowMo is a framework intended for solving stochastic optimization problems of the form
1m
min — VEξi^DiFi(x； ξi),	(1)
x∈Rd m	i i
i=1
using m worker nodes, where the loss function term Fi and samples ξi from the distribution Di are
available at the ith worker. SlowMo builds on top of a base optimization algorithm and has a nested
loop structure shown in Algorithm 1. Each worker maintains a local copy of the parameters, xt(,ik) at
worker i after the kth inner step of the tth outer iteration. We assume that all workers are initialized
to the same point x0,0, and the framework also uses a slow momentum buffer ut which is initialized
to u0 = 0; although each worker stores a copy of ut locally, these are always synchronized across
all nodes, so we omit the superscript to simplify the notation.
Within each outer iteration, workers first take τ steps of the base optimizer. The base optimizer
could be a method which involves no communication, such as SGD (with or without momentum)
or a decentralized algorithm which involves some communication, such as stochastic gradient push
(SGP) (Assran et al., 2019). We denote these updates by xt(,ik)+1 = xt(,ik) - γtdt(,ik) where γt is the
base optimizer (fast) learning rate and dt(,ik) is the update direction used at worker i. If the base
optimizer is SGD then d^k = VFi(x(ik; ξ(ik). For other base optimizers which may use additional
2
Published as a conference paper at ICLR 2020
1
2
3
4
5
6
7
8
9
Algorithm 1: Slow Momentum
Input: Base optimizer with learning rate γt; Inner loop steps T;
Slow learning rate α; Slow momentum factor β;
Number of worker nodes m. Initial point x0,0 and
initial slow momentum buffer u0 = 0.
for t ∈ {0, 1, . . . , T - 1} at worker i in parallel do
Reset/maintain/average base optimizer buffers
for k ∈ {0, 1, . . . , τ - 1} do
I Base OPtimiZer step: xti)+ι=x(ik - γtd(ik
end
Exact-Average: xt,τ = mm Pm=I x(iτ
Update slow momentum: ut+ι = βu + Y (xt,o 一 xt,τ)
Update outer iterates: xt+1,0 = xt,0 - αγtut+1
end
Figure 1: Illustration of one outer
iteration in the slow momentum
framework for m = 3 workers.
local momentum or communication, dt(,ik) represents the full update applied at worker i on this step.
Specific examples of dt(,ik) for different base optimizers are presented in Table C.1 in Appendix C.
After the τ base optimizer steps, the workers calculate the average xt,τ = xt,o 一 γmt Pm=I PT=1 d(ik
using ALLREDUCE (line 6), and then they perform a slow momentum update (lines 7-8),
ut+ι = βut +	(Xt,o 一 XtIT)	(2)
γt
Xt+1,o = Xt,o 一 αγtut+1.	(3)
Although the workers perform this update locally, in parallel, we again omit superscripts because
the values of Xt,o, Xt,τ, and hence ut+1 and Xt+1,o are always identical across all workers, since
they follow the AllReduce in line 6. Note that the difference xt,o — xt,τ is scaled by * in (2)
to make the slow momentum buffer invariant to the fast learning rate γt, which may change through
training, e.g., when using a learning rate schedule. The outer update in line 8 uses the product αγt
of the slow and fast learning rates. We use the distinction between slow and fast because the base
optimizer step is applied τ times for each outer update, but this is not intended to imply that one
learning rate is necessarily bigger or smaller than the other. We give specific examples of learning
rates and other hyperparameters used in the experiments in Section 4 below.
A specific SlowMo algorithm instance is obtained by specifying the base algorithm and the hyper-
parameters α, β, γ, andτ. We can recover a number of existing algorithms in this framework. When
the base algorithm is SGD, τ = 1, α = 1, and β ∈ [0, 1), we recover standard large mini-batch SGD
with learning rate γ and momentum β. When the base algorithm is SGD, τ > 1, α = 1, and β = 0,
we recover Local SGD (McDonald et al., 2010; Stich, 2019; Yu et al., 2019b; Wang & Joshi, 2018).
When the base algorithm does not involve communication among workers, τ > 1 and β > 0, we
recover BMUF (Chen & Huo, 2016).
We also obtain interesting novel distributed algorithms. In particular, the experiments in Section 4
demonstrate that using SlowMo with a decentralized base algorithm like SGP and reasonable val-
ues of τ consistently leads to improved optimization and generalization performance over the base
method alone, without a significant increase in runtime. We also observe empirically that, for a fixed
number of iterations, SlowMo combined with SGP is superior to SlowMo combined with SGD.
The above are all distributed algorithms. Perhaps surprisingly, SlowMo also encompasses a
recently-introduced non-distributed method: if we have m = 1 worker with SGD/Adam as the
base algorithm, α ∈ (0, 1], β = 0, and τ > 0, we recover the Lookahead optimizer of Zhang et al.
(2019), which also has a nested loop structure. Section 5 provides theoretical convergence guar-
antees when using the SlowMo framework to minimize smooth non-convex functions, and thus
provides the first theoretical convergence guarantees in the literature for BMUF and Lookahead in
this setting.
3
Published as a conference paper at ICLR 2020
3	Related Work
The idea of reducing communication overhead by using AllReduce to synchronize parameters
after every τ > 0 optimizer steps has been considered at least since the work of McDonald et al.
(2010), and has been more recently referred to as Local SGD in the literature. Elastic-average SGD
(Zhang et al., 2015) uses a related approach, but with a parameter server rather than AllReduce.
Lin et al. (2018) apply Local SGD for distributed training of deep neural networks and propose post-
local SGD, which starts by running AllReduce-SGD for some epochs before switching to Local
SGD, to improve generalization at the cost of additional communication.
Decentralized methods use approximate distributed averaging over a peer-to-peer topology, rather
than AllReduce. This decouples communication but also injects additional noise in the optimiza-
tion process since the models at different workers are no longer precisely synchronized. Lian et al.
(2017) present decentralized parallel SGD (D-PSGD), where each worker sends a copy of its model
to its peers at every iteration, and show it can be faster than parameter-server and AllReduce
methods for training deep neural networks. Lian et al. (2018) study an asynchronous extension,
AD-PSGD. Assran et al. (2019) study stochastic gradient push (SGP), and propose its asynchronous
counterpart overlap SGP (OSGP), which achieve a further speedup over D-PSGD and AD-PSGD
by using less coupled communication. D-PSGD, AD-PSGD, and SGP all have similar theoretical
convergence guarantees for smooth non-convex functions, showing a linear scaling relationship be-
tween the number of workers and the number of iterations to reach a neighborhood of a first-order
stationary point. Although the theory for all three methods only covers the case of SGD updates
without momentum, implementations use momentum locally at each worker, and workers only av-
erage their model parameters (not momentum buffers). Yu et al. (2019a) prove that linear scaling
holds when workers average their parameters and momentum buffers, although this doubles the
communication overhead. We refer to this approach as double-averaging below.
Scaman et al. (2019) establish optimal rates of convergence for decentralized optimization meth-
ods in the deterministic, convex setting. Richards & Rebeschini (2019) provide guarantees on the
generalization error of non-parametric least-squares regression trained using decentralized gradient
descent, showing that there are regimes where one can achieve a linear speedup. Neither of these
results apply directly to the setting considered in this paper, which focuses on smooth non-convex
stochastic optimization, and extending this line of work to non-convex settings is an interesting
direction.
Mahajan et al. (2018a) propose an approach to distributed learning of linear classifiers (i.e., convex
problems) where, in parallel, workers minimize locally formed approximate loss functions, and then
the resulting minimizers are averaged to determine a descent direction. Methods which fit in the
SlowMo framework, including Local SGD, BMUF (Chen & Huo, 2016), and the serial Lookahead
optimizer (Zhang et al., 2019), can be seen as related to this approach, where the actual loss function
at each worker is used rather than an approximate one, and where the descent direction is used in a
momentum update rather than a (deterministic) line search method.
Note that various approaches to gradient compression have been proposed to reduce the communica-
tion overhead for AllReduce and decentralized learning methods (Alistarh et al., 2007; Wen et al.,
2007; Bernstein et al., 2019; Karimireddy et al., 2019; Koloskova et al., 2019b; Vogels et al., 2019).
However, it is presently not clear to what extent compression may be beneficial for methods like
BMUF, D-PSGD, SGP, and OSGP, which perform averaging on the model parameters rather than
on gradients. Combining SlowMo with compression techniques is an interesting and important
direction for future work.
Finally, although momentum methods are known to achieve accelerated rates for deterministic opti-
mization, currently the theoretical understanding of the benefits of momentum methods (both serial
and parallel) is limited (BottoU et al., 2018). Loizou & Richtarik (2017) show that accelerated
convergence rates can be achieved when the objective is a quadratic finite sum (i.e., least squares
problem) that satisfies an interpolation condition. Can et al. (2019) show that accelerated rates can
also be achieved in the more general setting of smooth, strongly convex objectives, in a regime
where the noise level is below an explicit threshold, and where the rate of convergence is measured
in terms of the 1-Wasserstein metric of the distribution of trajectories produced by the momentum
method. In the setting of smooth non-convex functions, Gitman et al. (2019) establish stability and
asymptotic convergence results for the quasi-hyperbolic momentum method (Ma & Yarats, 2019),
4
Published as a conference paper at ICLR 2020
which can be viewed as interpolating between SGD and a stochastic momentum method. Extending
these results, which focus on the serial setting, to obtain decentralized momentum methods with
guaranteed acceleration, is an important direction for future work.
4	Experimental Results
We evaluate the effectiveness of SlowMo on three datasets: image classification on CIFAR-10
and ImageNet, and neural machine translation on WMT’16-En-De. All experiments use NVIDIA
DGX-1 servers as worker nodes. Each server contains 8 NVIDIA V100 GPUs and the servers are
internetworked via commodity 10 Gbps Ethernet.
On CIFAR-10 (Krizhevsky et al., 2009), we train a ResNet-18 (He et al., 2016) using 32 V100
GPUs, located on 32 different worker nodes. The total mini-batch size is 4096, and we train for 200
epochs. The learning rate (γt) linearly increases during the first 5 epochs, following the warm-up
strategy in Goyal et al. (2017), and then decays by a factor of 10 at epochs 100, 150, and 175. The
(fast) learning rate was tuned separately for each base optimizer. All experiments were run 5 times
with different random seeds, and the mean metrics are reported.
On ImageNet (Krizhevsky et al., 2012), we train a ResNet-50 (He et al., 2016) using 32 worker
nodes (i.e., 256 GPUs). The total mini-batch size is 8192, and we train for 90 epochs. The learning
rate schedule is identical to (Goyal et al., 2017), i.e., linear warm-up in the first 5 epochs and decay
by a factor of 10 at epochs 30, 60 and 80.
On WMT’16-En-De, we train a transformer model (Vaswani et al., 2017) using 8 worker nodes (i.e.,
64 GPUs). The model is trained with 200k token batches, and we train for 25 epochs. We follow the
experimental setting of Ott et al. (2018).
For each task, we consider several baselines: (i) Local SGD /Local Adam, where worker nodes
independently run single-node SGD (with Nesterov momentum) or Adam and periodically average
model parameters; (ii) stochastic gradient push (SGP), the state-of-the-art synchronous decentralized
training method; and (iii) Overlap-SGP (OSGP), an asynchronous version of SGP. For each baseline,
we examine its performance with and without SlowMo. Recall that Local SGD and Local Adam
with SlowMo are equivalent to BMUF. Local SGD and Local Adam do not involve communica-
tion during the inner loop (base optimizer) updates, while SGP and OSGP involve gossiping with
one peer at every step. In addition, we also evaluate the performance of AR-SGD/AR-Adam, the
traditional AllReduce implementation of parallel SGD/Adam. Details of all baseline methods are
provided in Appendices A and C.
In general, the hyperparameters of SlowMo (slow learning rate α, slow momentum β, and number
of inner loop steps τ) need to be tuned for each base optimizer and task. The results in Table 1 all
use α = 1, which we found to be consistently the best. For Local SGD (with or without SlowMo),
we set τ = 12, and for all other baseline methods we use τ = 48. Using τ > 12 for Local SGD
resulted in significantly worse loss/accuracy on ImageNet and WMT’16 En-De.
Note also that all of our baselines (or base optimizers) leverage a local momentum scheme, follow-
ing previous works (Assran et al., 2019; Koloskova et al., 2019a). When using these methods with
SlowMo, there are different ways to handle the base algorithm local momentum buffers at the be-
ginning of each outer loop (line 2 in Algorithm 1): zeroing, averaging among workers, or maintain-
ing the current local value. Appendix B.4 provides an empirical comparison. For the experiments
reported here, when using SGD with Nesterov momentum as the base algorithm (CIFAR-10 and Im-
agenet) we zero the base algorithm buffer, and when using Adam as the base algorithm (WMT’16
En-De) we maintain the current value of the Adam buffers. We also tried to apply SlowMo on top
of AR-SGD base optimizer, but we did not observe any improvement in that setting.
Optimization and Generalization Performance. Table 1 shows the best training loss and the
validation accuracy/BLEU score for each baseline, with and without SlowMo. Using SlowMo
consistently improves both the optimization and generalization performance across all training tasks
and baseline algorithms. Figure 2 presents validation error/loss per epoch to give a sense of conver-
gence speed. Observe that SGP with SlowMo substantially improves convergence, compared to
SGP alone. We observe a similar phenomenon when comparing the training curves; see Appendix B.
5
Published as a conference paper at ICLR 2020
Table 1: Comparisons to the original distributed optimization algorithms on various training tasks.
The best training loss, validation accuracy (for image classification), and BLEU score (for machine
translation) are reported. We fix slow learning rate α = 1. We set the number of local steps τ = 12
for CIFAR10. For ImageNet and WMT, we use τ = 48 for SGP and OSGP and τ = 12 for
Local SGD. The slow momentum β is tuned for each case. It typically ranges from 0.4 to 0.8.
Datasets	Baseline	Training Loss		Validation Acc./BLEU	
		Original	w/ SLOWMO	Original	w/ SlowMo
	Local SGD	0.122	0.006	91.73%	93.20%
CIFAR-10	OSGP	0.011	0.001	93.17%	93.74%
	SGP	0.002	0.001	93.90%	94.32%
	AR-SGD	0.002	-	92.66%	-
	Local SGD	1.43	1.21	69.94%	73.24%
ImageNet	OSGP	1.03	0.97	74.96%	75.54%
	SGP	1.07	1.00	75.15%	75.73%
	AR-SGD	0.96	—	76.00%	—
	Local Adam	2.520	2.480	26.62	27.14
WMT’16 En-De	SGP	2.500	2.447	26.92	27.84
	AR-Adam	2.468	—	27.17	—
Communication Cost. Table 2 shows the average training time per iteration on ImageNet and
WMT’16. For SGP/OSGP, since the additional communication cost due to averaging in line 6 of
Algorithm 1 is amortized over τ = 48 iterations, SlowMo maintains nearly the same speed as
the corresponding base algorithm. For methods like Local SGD or Local Adam, which already
compute an exact average every τ iterations, using SlowMo (i.e., using β > 0) does not increase the
amount of communication. In other words, using SlowMo on top of the base algorithm improves
training/validation accuracy at a negligible additional communication cost.
Effects of τ. The most important hyper-parameter in SlowMo is the number of base optimizer
steps τ before each SlowMo update, since it influences both the accuracy and the training time.
Figure 3 presents the validation accuracy and average iteration time of SGP-SlowMo for different
values of τ on ImageNet and WMT’16. It can be observed that the validation performance does not
monotonically increase or decrease with τ . Instead, there is a best value. On both ImageNet and
WMT’16, we find τ = 48 to be a good tradeoff between speed and accuracy. Moreover, SlowMo
is pretty robust to the choice of τ; even if τ = 96 for ImageNet and τ = 192 for WMT’16, SGP
With SLOWMO achieves better validation accuracy/loss than SGP alone.
We further investigate the effect of other hyperparameters (the slow learning rate a, slow momentum
β) as well as the different strategies for handling base algorithm buffers in Appendix B.
_----AR-SGD
----SGP
----SGP-SloMo
75	100 125 150 175 200
Number of Epochs
(a) CIFAR-10, batch size:4k.
Qqoooooo
09876543
.I0bωUO=EP=EA
0	20	40	60	80
Number of Epochs
(b) ImageNet, batch size:8k.
3jn) Ssoa u。=EP二EA
0	5	10	15	20	25
Number of Epochs
(c) WMT16 En-De, batch size:200k.
Figure 2: Validation curves for various tasks using SGP as the base algorithm. We fix α = 1, τ = 12
for these three plots. Shaded areas in (a) and (b) show the min-max values across all worker nodes.
The corresponding training curves are presented in Appendix B.2.
6
Published as a conference paper at ICLR 2020
Table 2: Average time per iteration with and without SlowMo. Recall that τ = 48 for the SGP and
OSGP base optimizer and τ = 12 for Local SGD/Local Adam. In some cases, with SlowMo was
faster than without; we hypothesize that this is due to statistical variations in timing and background
network traffic.
(a) ImageNet, batch size:8k, 32 nodes.
Baseline	Time/iterations (ms)	
	Original	w/ SlowMo
Local SGD	294	282
OSGP	271	271
SGP	304	302
AR-SGD	420	-
(b) WMT’16 En-De, batch size:200k, 8 nodes.
Baseline	Time/iterations (ms)	
	original	w/ SlowMo
Local Adam	503	505
SGP	1225	1279
AR-Adam	1648	-
(Sin) UoIsJsI⅛d OlUll
SGP-SLOMO(α = 1, β = 0.7)
AOEJnOOE uoaEPlIg
SGP-SLOMO(α = 1, β = 0.5)
(Sin) uo□EJ2IJ0od
25 50 75 100 125 150 175 200
T
CnN) SSδI u。
(a) Effect of τ on ImageNet.	(b) Effect of τ on WMT’16.
Figure 3: The effects of τ in SlowMo. We use SGP as the base algorithm. For ImageNet we
plot validation accuracy (higher is better), and for WMT’16 En-De we plot validation NLL (lower
is better). Increasing τ amortizes communication cost over more iterations, so the average time per
iteration decreases. We hypothesize that moderate values of τ have a regularizing effect, improving
loss and accuracy, and when τ is too large performance is degraded because workers’ local models
drift too far apart.
Comparison with Double-Averaging Momentum. As mentioned in Section 3, Yu et al. (2019a)
propose an alternative momentum scheme, double-averaging, to improve the convergence of Local
SGD and D-PSGD. We empirically compare it with SlowMo in terms of the validation accuracy
and average training time per iteration on ImageNet. When the base algorithm is SGP, double
averaging achieves 75.54% validation accuracy and takes 402 ms per iteration on average, while
SlowMo-SGP (τ = 48) reaches 75.73% validation accuracy while taking 302 ms per iteration
on average. Similarly, when the baseline algorithm is Local SGD with τ = 12, double-averaging
reaches 72.04% and takes 405 ms per iteration, while SlowMo reaches 73.24% and takes only 282
ms per iteration.
5 Theoretical Results
This section provides a convergence guarantee for SlowMo and shows that it can achieve a linear
speedup in terms of number of workers. Let fi(x) = Eξi〜Di [Fi(x; ξi)] denote the expected objec-
tive function at worker i, and let f (x) = ml Pm=I fi(x). Our analysis is conducted for a constant
learning rate γt = γ under the following standard assumptions.
Assumption 1 (L-smooth). Each local objective function fi (x) is L-smooth, i.e.,
∣∣Vfi(x) - Vfi(y)k ≤ L ∣∣x - yk,forall x, y ∈ Rd and i ∈{1, 2,..., m}.
Assumption 2 (Bounded variance). There exists a finite positive constant σ2 such that
Eξ 〜DiIlVFi(x; ξ) - Vfi(X) k2 ≤ σ2 ,for all i ∈{1, 2,..., m}.
In order to generalize the analysis to various base algorithms, We define dt,k = ml Pm=I dti) as the
average descent direction across the m workers and make the following assumption.
7
Published as a conference paper at ICLR 2020
Assumption 3. There exists a finite positive constant V such that E kdt,k - Et,k [dt,k]k2 ≤ V,
where Et,k denotes expectation conditioned on all randomness from stochastic gradients up to the
k-th step of t-th outer iteration.
As mentioned in Section 2, the analytic form of dt,k depends on the choice of base algorithm.
Therefore, the value of V also changes. For instance, when the base algorithm is Local-SGD, then
dt,k = mm Pm=ι VFi(x(i)； ξ(ik). It follows that
1 m	2	σ2
Ekdt,k - Et,k[dt,k]k2 = mXE 卜月/；ξ(ik) -Vfi(χ(i))∣∣ ≤ m = V.	(4)
i=1
The above value (V = σ2/m) can also be applied to other base algorithms, such as D-PSGD, SGP,
and OSGP. More details are provided in Appendix C.
Our main convergence result is stated next. Proofs of all results in this section appear in Appendix D.
Theorem 1 (General Result). Suppose all worker nodes start from the same initial point x0,0, and
the initial slow momentum is u0 = 0.
If we set α, β, Yt = γ, T and T so that 1-e = τ∕^^f
and the total iterations τT satisfies τT ≥
Assumptions 1 to 3, we have that:
mL2(1 + √3max { 3T(I-e-a), 14-ββ, ι}), then under
T -1τ	-1
TT X X EkVf(Xt,k)k2
≤ 2(f (x0,ο) — finf) + mVL
√	√mτT
T -1τ	-1
+ TT XXEkVf(xt,k)-Et,k[dt,k]k2
、--t—k=0--------V----------------}
Effect of base optimizer
4mVL2 (τ — 1) (1 — β 1A 2	8mVL2τ	β2
+ TT ( α ) T TT (1 — β2)
、-----------------------{-----------------------}
Effect of slow momentum
(5)
where finf = infx f (x).
Consistent with AR-SGD. Recall that AR-SGD is equivalent to taking T = 1, α = 1, and β = 0
and using SGD with learning rate Y as the base optimizer. In this case, all terms on the RHS but the
first one vanish, V = σ2∕m, and (5) is identical to the well-known rate of O(1/√mTr) for SGD.
Effect of the base optimizer. The second term in (5) only depends on the base optimizer. It mea-
sures the bias between the full batch gradient Vf (xt,k) and the expected update averaged across all
workers Et,k[dt,k]. For the base optimizers considered in this paper, this term relates to the discrep-
ancies among local models and can be easily found in previous distributed optimization literature.
In particular, under the same assumptions as Theorem 1, one can show that this term vanishes in a
rateof1/(TT) for D-PSGD, SGP, OSGP and Local-SGD; see Appendix C.
As an example, we provide the convergence analysis for the extreme case of Local SGD, where there
is no communication between nodes during each inner iteration. Intuitively, using other base algo-
rithms should only make this term smaller since they involve more communication than Local SGD.
Corollary 1 (Convergence of BMUF, i.e., Local SGD with SLOWMO). Under the same condi-
tions as Theorem 1, if the inner algorithm is Local SGD and there exists a positive finite constant ζ
such that m1 Pm=I l∣Vf (x) — Vfi(X)∣∣2 ≤ Z2 ,then
T -1τ -1
TT X XEkVf(xt,k)k2=O
1
√mrT
+ O(空)∙
(6)
Linear speedup. Corollary 1 shows that when the total number of steps TT is sufficiently large:
T ≥ m3T3, the convergence rate will be dominated by the first term O(1∕√mTr). That is, in order
to achieve an error, the algorithm requires m times less steps when using m times more worker
nodes. This also recovers the same rate as AR-SGD.
Extension to single-node case. As mentioned in Section 2, when there is only one node and the
slow momentum factor is β = 0, the SlowMo-SGD is the Lookahead optimizer. One can directly
apply Theorem 1 to this special case and get the following corollary.
8
Published as a conference paper at ICLR 2020
Corollary 2 (Convergence of Lookahead). Under the same conditions as Theorem 1, if the inner
optimizer is AR-SGD and β = 0, then one can obtain the following upper bound:
T-1 τ-1
τT XXEINf (χt,k )k2
τ t=0 k=0
< 2(f (x0,0) - finf) + σ2L	4σ2L2(τ -I)
≤	√TT	+ Tt
+O
(7)
(8)
6	Faster SlowMo: Removing the Periodic AllReduce
SlowMo helps improve both the optimization and generalization of communication-efficient algo-
rithms. When the base optimizer is SGP or OSGP, SlowMo also comes at the expense of higher
communication cost, since it requires performing an exact average every τ iterations. 1 Although the
communication cost can be amortized, here we go one step further and propose a SGP-SlowMo
variant, named SGP-SlowMo-noaverage, where we remove the exact average when we perform
the SlowMo update, i.e we skip line 6 in Algorithm 1. We empirically evaluate this variant on the
ImageNet and WMT’16 datasets, using α = 1, β = 0.6 and τ = 48.
Surprisingly, we observe that SGP-SlowMo-noaverage achieves similar performances on Imagenet
(75.78%, compared to 75.73% for SGP-SlowMo) and only slightly degrades the validation NLL on
WMT’16 (2.11, compared to 2.10), while preserving the iteration time of the base algorithm (298
ms per iteration on ImageNet and 1227 ms per iteration on WMT’16) since this variant does not
require additional communication. These results suggest that the slow momentum updates, and not
the momentum buffer synchronization, contribute the most to the performance gain of SlowMo.
We leave further investigation of SlowMo-SGP-noaverage for future work.
7	Concluding Remarks
In this paper, we propose a general momentum framework, SlowMo, for communication-efficient
distributed optimization algorithms. SlowMo can be built on the top of SGD, as well as decentral-
ized methods, such as SGP and (asynchronous) OSGP. On three different deep learning tasks, we
empirically show that SlowMo consistently improves the optimization and generalization perfor-
mance of the corresponding baseline algorithm while maintaining a similar level of communication
efficiency. Moreover, we establish a convergence guarantee for SlowMo, showing that it con-
verges to a stationary point of smooth and non-convex objective functions. Since BMUF (Chen &
Huo, 2016) can be expressed through SlowMo framework (by setting the base optimizer to be Lo-
cal SGD or Local Adam), to the best of our knowledge, we provide the first convergence guarantee
for BMUF in the literature.
References
Dan Alistarh, Demjan Grubic, Jerry Z. Li, Ryota Tomioka, and Milan Vojnovic. Qsgd:
Communication-efficient sgd via gradient quantization and encoding. In Advances in Neural
Information Processing Systems,pp. 1709-1720, 2007.
Mahmoud Assran, Nicolas Loizou, Nicolas Ballas, and Michael Rabbat. Stochastic gradient push
for distributed deep learning. In International Conference on Machine Learning, 2019.
Jeremy Bernstein, Jiawei Zhao, Kamyar Azizzadenesheli, and Anima Anandkumar. signSGD with
majority vote is communication efficient and fault tolerant. In International Conference on Learn-
ing Representations, 2019.
Leon Bottou, Frank E. Curtis, and Jorge NocedaL Optimization methods for large-scale machine
learning. Siam Review, 60(2):223-311, 2018.
1In Local SGD/Local Adam, an exact average is also required every τ iterations, hence in comparison, using
SlowMo does not increase the amount of communication required.
9
Published as a conference paper at ICLR 2020
BUgra Can, Mert Gurbuzbalaban, and Lingjiong Zhu. Accelerated linear convergence of stochastic
momentum methods in Wasserstein distances. arXiv preprint arxiv:1901.07445, 2019.
Jianmin Chen, Rajat Monga, Samy Bengio, and Rafal Jozefowicz. Revisiting distributed syn-
chronous SGD. In International Conference on Learning Representations Workshop Track, 2016.
Kai Chen and Qiang Huo. Scalable training of deep learning machines by incremental block train-
ing with intra-block parallel optimization and blockwise model-update filtering. In 2016 IEEE
International Conference on Acoustics, Speech and Signal Processing (ICASSP),pp. 5880-5884,
2016.
Sanghamitra Dutta, Gauri Joshi, Soumyadip Ghosh, Parijat Dube, and Priya Nagpurkar. Slow and
stale gradients can win the race: Error-runtime trade-offs in distributed SGD. In International
Conference on Artificial Intelligence and Statistics, pp. 803-812, 2018.
Nuwan Ferdinand, Haider Al-Lawati, Stark Draper, and Matthew Nokelby. Anytime minibatch:
Exploiting stragglers in online distributed optimization. In International Conference on Learning
Representations, 2019.
Igor Gitman, Hunter Lang, Pengchuan Zhang, and Lin Xiao. Understanding the role of momentum
in stochastic gradient methods. In Advances in Neural Information Processing Systems, 2019.
Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, An-
drew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: Training ImageNet
in 1 hour. arXiv preprint arXiv:1706.02677, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.
770-778, 2016.
Zhanhong Jiang, Aditya Balu, Chinmay Hegde, and Soumik Sarkar. Collaborative deep learning in
fixed topology networks. In Advances in Neural Information Processing Systems, pp. 5904-5914,
2017.
Sai Praneeth Karimireddy, Quentin Rebjock, Sebastian Stich, and Martin Jaggi. Error feedback
fixes SignSGD and other gradient compression schemes. In International Conference on Machine
Learning, 2019.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations, 2015.
Anastasia Koloskova, Tao Lin, Sebastian U Stich, and Martin Jaggi. Decentralized deep learning
with arbitrary communication compression. arXiv preprint arXiv:1907.09356, 2019a.
Anastasiia Koloskova, Sebastian Stich, and Martin Jaggi. Decentralized stochastic optimization and
gossip algorithms with compressed communication. In International Conference on Machine
Learning, 2019b.
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Learning multiple layers of features from tiny
images. CIFAR-10 (Canadian Institute for Advanced Research), 2009. URL http://www.cs.
toronto.edu/~kriz∕cifar.html.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in Neural Information Processing Systems, pp. 1097-1105,
2012.
Xiangru Lian, Ce Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji Liu. Can decentralized
algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic
gradient descent. In Advances in Neural Information Processing Systems, pp. 5330-5340, 2017.
Xiangru Lian, Wei Zhang, Ce Zhang, and Ji Liu. Asynchronous decentralized parallel stochastic
gradient descent. In Proceedings of the 35th International Conference on Machine Learning, pp.
3049-3058, 2018.
10
Published as a conference paper at ICLR 2020
Tao Lin, Sebastian U Stich, Kumar Kshitij Patel, and Martin Jaggi. Don’t use large mini-batches,
use local sgd. arXiv preprint arXiv:1808.07217, 2018.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pre-
training approach. arXiv preprint arXiv:1907.11692, 2019.
Nicolas Loizou and Peter Richtarik. Momentum and stochastic momentum for stochastic gradient,
Newton, proximal point and subspace descent methods. arXiv preprint arxiv:1712.09677, 2017.
Jerry Ma and Denis Yarats. Quasi-hyperbolic momentum and Adam for deep learning. In Interna-
tional Conference on Learning Representations, 2019.
DhrUv Mahajan, Nikunj Agrawal, S Sathiya Keerthi, Sundararajan Sellamanickam, and Leon Bot-
tou. An efficient distributed learning algorithm based on effective local functional approximations.
The Journal ofMachine Learning Research,19(1):2942-2978, 2018a.
Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li,
Ashwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised
pretraining. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 181-
196, 2018b.
Ryan McDonald, Keith Hall, and Gideon Mann. Distributed training strategies for the structured
perceptron. In Human Language Technologies: The 2010 Annual Conference of the North Amer-
ican Chapter of the Association for Computational Linguistics, pp. 456-464. Association for
Computational Linguistics, 2010.
H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Artificial Intelli-
gence and Statistics, pp. 1273-1282, 2017.
Angelia Nedic and Alex Olshevsky. Stochastic gradient-push for strongly convex functions on time-
varying directed graphs. IEEE Trans. Automatic Control, 61(12):3936-3947, 2016.
Myle Ott, Grangier David Edunov, Sergey, and Michael Auli. Scaling neural machine translation.
In Conference on Machine Translation (WMT), 2018.
Adam Paszke, Soumith Chintala, Ronan Collobert, Koray Kavukcuoglu, Clement Farabet, Samy
Bengio, Iain Melvin, Jason Weston, and Johnny Mariethoz. Pytorch: Tensors and dynamic neural
networks in python with strong gpu acceleration, 2017.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
models are unsupervised multi-task learners. Open AI tech. report, Feb. 2019.
Dominic Richards and Patrick Rebeschini. Optimal statistical rates for decentralised non-parametric
regression with linear speed-up. In Advances in Neural Information Processing Systems, 2019.
Kevin Scaman, Francis Bach, SebaStien Bubeck, Yin Tat Lee, and Laurent Massoulie. Optimal
convergence rates for convex distributed optimization in networks. Journal of Machine Learning
Research, pp. 1-31, 2019.
Sebastian U Stich. Local SGD converges fast and communicates little. In International Conference
on Learning Representations, 2019.
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initial-
ization and momentum in deep learning. In International Conference on Machine Learning, pp.
1139-1147, 2013.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infar-
mation Processing Systems, pp. 5998-6008, 2017.
Thijs Vogels, Sai Praneeth Karimireddy, and Martin Jaggi. PowerSGD: Practical low-rank gradient
compression for distributed optimization. In Advances in Neural Information Processing Systems,
2019.
11
Published as a conference paper at ICLR 2020
Jianyu Wang and Gauri Joshi. Cooperative SGD: A unified framework for the design and analysis
of communication-efficient SGD algorithms. arXiv preprint arXiv:1808.07576, 2018.
Jianyu Wang, Anit Kumar Sahu, Zhouyi Yang, Gauri Joshi, and Soummya Kar. MATCHA: Speeding
up decentralized SGD via matching decomposition sampling. arXiv preprint arXiv:1905.09435,
2019.
Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. TernGrad:
Ternary gradients to reduce communication in distributed deep learning. In Advances in Neural
Information Processing Systems,pp. 1509-1519, 2007.
Hao Yu, Rong Jin, and Sen Yang. On the linear speedup analysis of communication efficient mo-
mentum SGD for distributed non-convex optimization. In International Conference on Machine
Learning, 2019a.
Hao Yu, Sen Yang, and Shenghuo Zhu. Parallel restarted SGD with faster convergence and less
communication: Demystifying why model averaging works for deep learning. In Proceedings of
the AAAI Conference on Artificial Intelligence, volume 33, pp. 5693-5700, 2019b.
Michael R Zhang, James Lucas, Geoffrey Hinton, and Jimmy Ba. Lookahead optimizer: k steps
forward, 1 step back. arXiv preprint arXiv:1907.08610, 2019.
S. Zhang, A. Choromanska, and Y. LeCun. Deep learning with elastic averaged SGD. In Advances
in Neural Information Processing Systems, pp. 685-693, 2015.
Fan Zhou and Guojing Cong. On the convergence properties of a k-step averaging stochastic gradi-
ent descent algorithm for nonconvex optimization. In International Joint Conference on Artificial
Intelligence, 2018.
12
Published as a conference paper at ICLR 2020
A Experiment Details
A. 1 Implementation Details
All methods are implemented in PyTorch 1.0 (Paszke et al., 2017), and our experiments use
CUDA 9.2, CUDNN 7.3, and NCCL 2.2.13. The ImageNet experiments build on the example from
https://github.com/pytorch/examples/imagenet. The WMT’16 En-De experi-
ments build on https://github.com/pytorch/fairseq. For SGP and OSGP we use the
implementations available at https://github.com/facebookresearch/stochastic_
gradient_push.
A.2 CIFAR- 1 0
For the CIFAR-10 experiments, we train a ResNet-18, the implementation of which is avail-
able at https://github.com/kuangliu/pytorch-cifar/blob/master/models/
resnet.py. In all base algorithms, we use a Nesterov momentum parameter of 0.9 and set
the weight decay factor as 10-4. For each base algorithm, we tune the (fast) learning rate from
{0.01, 0.025, 0.05, 0.1, 0.15} and linearly scale it with the number of workers (i.e., 32). We found
that, with a total batch size 4096, the best learning rate for AR-SGD is 0.01, for OSGP/SGP is 0.05,
and for Local SGD is 0.025.
When applying SlowMo to these base algorithms, we fix α = 1 and τ = 12 and tune the value of
β from {0.4, 0.5, 0.6, 0.7, 0.8}. It turns out that for OSGP, SGP, and Local SGD, the best values of
β are all equal to 0.7. More discussion on the effects of α and β can be found in Appendix B.3.
A.3 ImageNet
For the ImageNet experiments, we use the same learning-rate, schedule, momentum, and weight
decay as those suggested in Goyal et al. (2017) for SGD. In particular, we use ResNet50 (He et al.
(2016)) and train it for 90 epochs with a reference learning-rate of 0.1 with respect to a 256 sample
batch, and scale this linearly with the batch-size. We decay the learning-rate by a factor of 10 at
epochs 30, 60, 80. We use a Nesterov momentum parameter of 0.9. We use weight decay 10-4.
When using SlowMo, we set the slow learning rate to α = 1 and explore different numbers of
inner steps, τ ∈ {12, 48} and different slow momentum value β ∈ {0.5, 0.6, 0.7} when the base
optimizer is SGP/OSGP and β = 0.7 when the base optimizer is LocalSGD. We also explore a
larger set of τ values τ ∈ {12, 24, 48, 96} in the ablation experiments.
A.4 WMT16 En-De
For the WMT16 En-De experiments, we follow the experimental protocol described in Ott et al.
(2018). All experiments are based on the big transformer model (Vaswani et al., 2017) with 6 blocks
in the encoder and decoder networks. For these experiments, the base optimizer used is Adam
(Kingma & Ba, 2015) with beta1 = 0.9, beta2 = 0.98, and = 10-8 and trained for 25 epochs.
We use the same learning rate schedule as Ott et al. (2018), i.e., the learning rate increases linearly
for 4, 000 steps to 10-3, after which it is decayed proportionally to the inverse square root of the
number of steps. We use label smoothing with weight 0.1 for the uniform prior distribution.
For SlowMo, we explore {0.5, 1.0} as the slow learning rate α. We observe that α = 1
gives better performance and therefore report results for α = 1 unless stated otherwise. We
explore different numbers of inner steps, τ ∈ {12, 48} and different slow momentum value
β ∈ {0.1, 0.3, 0.5, 0.6, 0.7}. We also explore a larger set of τ values, i.e. τ ∈ {12, 24, 48, 96, 192},
in the ablation experiments.
13
Published as a conference paper at ICLR 2020
Table B.1: Validation NLL (lower is better) with and without SlowMo on WMT’16 En-De. We
observe that SlowMo improves the validation NLL of SGP and Local Adam.
Baseline	Validation NLL	
	Original	w/ SlowMo
Local Adam	2.179	2.137
SGP	2.137	2.106
AR-Adam	2.108	—
B Additional Empirical Results
B.1	Validation NLL on WMT’16 En-De
We show the Validation NLL on WMT’16 En-De in Table B.1, corresponding to the experiments in
Table 1. We observe that SlowMo improves the validation NLL (along with BLEU score) of SGP
and Local Adam.
B.2	Additional Training Curves
We present the training loss-versus-epochs curves in Figure B.1, corresponding to the validation
curves in Figure 2. It can be observed that SlowMo substantially improves the convergence speed
of SGP.
a-1-2
Iww
SSOUIU.!
0	25
50 75 100 125 150 175 200
Number of Epochs
SSOjmiπusjH
4 2 0 8 6
■ ■ ■ ■ ■
3 3 3 2 2
(jjn) ssoj∞-a-aE.!H
(a) CIFAR-10, batch size:4k. (b) ImageNet, batch size:8k. (c) WMT16 En-De, batch size:200k.
Figure B.1: Training curves for various tasks using SGP as the base algorithm. We fix α = 1, τ = 12
for these three plots. Shaded areas in (a) and (b) show the min-max values across all worker nodes.
The corresponding validation curves are presented in Figure 2. Note that the training losses in these
three figures are evaluated right after the SlowMo update (i.e., Eq. (3)).
B.3	Effect of Slow Learning Rate α and Slow Momentum Factor β
In this section we evaluate the impact of slow learning rate α and slow momentum β hyperameters.
In Figure B.2a, we perform a parameter sweep over α and β on CIFAR-10 dataset, using OSGP as
the base algorithm of SlowMo. One can observe that when the value of β is fixed, α = 1 always
gives the highest validation accuracy; when the value of α is fixed, there is a best value of β ranging
from 0.4 to 0.8.
We further validate this claim on the WMT’16-En-De dataset. Figure B.2b shows that α = 1
gives lower validation loss than α = 0.5 for fixed β when using SGP or Local Adam as the base
algorithms. When running SlowMo-Adam with β > 0.5 and α = 1.0, or with β > 0.7 and
α = 0.5, the validation loss was substantially worse and so is not plotted here. Motivated by the
above observations, we stick to fix α = 1 and fine-tune β for SlowMo methods on all training
tasks.
14
Published as a conference paper at ICLR 2020
eəa°I)UIUJP-MoIS
9
89.47 88.36 86.54 84.17 80.76
90.05 89.04
90.95	91.62	91.32
92.03	92.50	92.32
92.66	92.63	92.80	92.51	92.08
92.94	93.32	93.05	93.04	92.87
91.70 91.17
0.8	0.6	0.4	0.2	0
Slow momentum β
-94.5
-93.0
-91.5
-90.0
-88.5
-87.0
(TlN) SSOa usap=BA
τ = 12, average local buffers
2.275 -
2.250
2.225-
2.200
2.175
2.150
2.125
2.100
0.0 0.1	0.2 0.3	0.4 0.5	0.6	0.7
Slow momentum β
(a)	Parameter sweep on CIFAR-10 training task
using OSGP as the base algorithm.
(b)	Effect of α and β on WMT’16 on
SlowMo-Adam.
Figure B.2: Impact on slow learning rate α and slow momentum β on SlowMo.
B.4	Base Optimizer Momentum Buffer S trategies
As described in Section 2, the base optimizer may have some associated buffers. SGD with mo-
mentum uses a momentum buffer, and Adam tracks estimates of the first and second moments of
the gradient. The slow momentum buffer is updated every τ steps according to Eq.(2). There are
several strategies that can be used to update the base optimizer buffers at the outer iteration level
(line 2 in Algorithm 1). Here, we explore three strategies: 1) reset the base optimizer buffers to
zero ; 2) maintain the base optimizer buffers to their current values; 3) average the base optimizer
buffers across workers, which requires additional communications. We evaluate the impact of these
strategies on ImageNet and WMT’16 in Table B.2 and Table B.3.
On ImageNet, we observe that the different buffer strategies achieve similar training and validation
performance. However, the averaging strategy comes at the cost of higher communication overhead
(an additional call to AllReduce for each buffer averaged). Based on these results, we choose the
reset strategy as the default in our experiments.
On WMT’16, we find that the reset buffer strategy underperforms both the maintain and average
approaches. When using Adam as base optimizer, reseting the second moment to zeros hurts the
optimization performance. This is not surprising since it is recognized that warming up the Adam
buffer is important. Averaging buffers achieves the best results but comes at a significantly higher
communication cost. We therefore select the maintain strategy as the default one when using Adam.
Table B.2: Effect of different buffer strategies: ImageNet, batch size:8k, 32 nodes.
Buffer Strategy	Training Loss	Validation Accuracy
Avg parameters + avg buffers	1.06	75.66%
Avg parameters + reset buffers	1.00	75.73%
Avg parameters + maintain buffers	0.98	75.78%
Table B.3: Effect of different buffer strategies: WMT’16 En-De, batch size:200k, 8 nodes.
Buffer Strategy	Training Loss	Validation Loss
Avg parameters + avg buffers	2.438	2.101
Avg parameters + reset buffers	5.093	4.732
Avg parameters + maintain buffers	2.447	2.106
B.5	Standard Deviations on CIFAR-10
Since experiments for CIFAR-10 were ran for 5 times with different random seeds, here we report
the standard deviations on the validation accuracy in Table B.4, as a complementary to Table 1.
15
Published as a conference paper at ICLR 2020
Table B.4: Validation Accuracy with and without SlowMo on CIFAR-10. Using SlowMo con-
sistently improves the performance of the base algorithms.
Baseline	Validation Accuracy Original	w/ SlowMo
Local SGD OSGP SGP AR-SGD	91.73 ± .14% 93.20 ± .23% 93.17 ± .11% 93.74 ± .17% 93.90 ±.13% 94.32 ±.21% 92.66 ±.16%	-
C Baseline Algorithms
In this section, we give a detailed description of each baseline algorithms used throughout the paper,
provide theoretical justification on how to incorporate the update rules of D-PSGD, SGP and OSGP
into the analysis of SlowMo, and also derive the analytic form of V used in Assumption 3 for each
method. A summary of the base optimizer update directions is given in Table C.1
Table C.1: Examples of update directions used by the base optimizer in SlowMo, where h(i)
and v(i) denote the first-order and second-order momentum buffers respectively and βlocal, β1 , β2
are the corresponding local momentum factors. When the local momentum buffers are reset at the
beginning of each inner loop, then ht(,i0) = 0, vt(,i0) = 0 and l = k; when the local momentum buffers
are maintained, then ht(,i	0) = ht(-)1,τ , vt(,0) = vt(-)1,τ and l = tτ + k.
Base Optimizer	Update directions d(t,ik) (and possible additional buffers h(i) , v(i))
SGD	h(ik+ι = βlocal htik + VFi(Xtik; ξfk) d(i) = βlocalhti" + VFi (Xtik； ξ(ik)	
SGP d(t,ik)	h(ik+1 = βlocalhtik + vFi(z(ik； ξ(ik) =YXtik- Y Pj∈Nin(i) p(i,j)[x(ik - Yt(βlocalh(ik+1 + VFi(Z(ik； ξ(ik))]
Adam	h(ik+ι=βιhtik+(1 - βι)vFi(χ(ik ； ξ(ik) .v(ik + 1 = β2v(ik + (1 - β2)vF2(xtik； ξ(ik) h(i) h	h⑴	/(1 _	βl)名⑴	一。,⑴	/(1 _	βl) ht,k + 1 =	ht,k + l∕(1 -	β1),	vt,k + 1	=	vt,k+l∕(1 -	β2) d(ik = h (ik+ι∕(Jv(ik+ι + e)	
SGP (Adam)	hfk+1=β1htik + (1 - βι)vFi(z(ik ； ξ(ik) ,v(i)+1 = β2V(ik + (1 - β2)VFi2(z(ik； ξ(ik) h(i) 一 h⑺	/(1 _ βl)名⑺	一。,⑺	/(1 _ βl) ht,k + 1 = ht,k + 1/(1 - β1), vt,k + 1 = vt,k+l∕ (1 - β2) d(i) = ɪχ(i) —工 P	- r. n(i,j)[χ(i) — Zh(i)	/(ʌ ^+ H dt,k = Yt xt,k	Yt 乙j∈Nkn(i) P	[xt,k	γtht,k+ι/(V vt,k+1 十 €)]
C.1 SGP and OSGP
Algorithms 2 and 3 present the pseudo-code of SGP and OSGP (Assran et al., 2019). To be con-
sistent with the experimental results of the original paper, we also use local Nesterov momentum
for each worker node. The communication topology among worker nodes is a time-varying di-
rected exponential graph Assran et al. (2019). That is, if all nodes are ordered sequentially, then,
according to their rank (0, 1, . . . , m - 1), each node periodically communicates with peers that are
20, 21, . . . , 2blog2(m-1)c hops away. We let each node only send and receive a single message (i.e.,
communicate with 1 peer) at each iteration.
16
Published as a conference paper at ICLR 2020
1
2
3
4
5
6
7
8
9
10
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
Algorithm 2: Stochastic Gradient Push with Nesterov Momentum (SGP)
Input: learning rate γ; momentum factor β0; Number of worker nodes m. Initial point x(0i) = z0(i),
h(0i) = 0 and w0(i) = 1 for all nodes i ∈ {1, 2, . . . , m}.
for k ∈ {0, 1, . . . , K - 1} at worker i in parallel do
Compute mini-batch gradients: VFi(zki); ξki))
Update local momentum: h：?] = βohki) + VFi(ZF； ξj(i))
xki++l = xki) - γ[β0hki++ι + VFi(Zki);ξki))]
Send (Pj,i)xk[ 1 ,pj,i)Wki)) to out-neighbors
Receive (Pkij) Xj) 1 ,Pki,j) w j)) from in-neighbors
Update model parameters: Xki++1 = P . Nin⑺ Pkij)Xj)ι
十	j∈N k	k十2
Update de-biased factors: Wk十1 = Pj∈N/⑸ Pkij)Wk+ 1
Update de-biased model parameters: Zk(i+) 1 = X(ki+) 1/wk(i+) 1
end
Algorithm 3: Overlap Stochastic Gradient Push with Nesterov Momentum (OSGP)
Input: learning rate γ; momentum factor β0; Number of worker nodes m. Initial point X(0i) = Z0(i),
h0i) = 0 and WOi) = 1 for all nodes i ∈ {1,2,..., m}; count_since_last = 0.
for k ∈ {0, 1, . . . , K - 1} at worker i in parallel do
Compute mini-batch gradients: VFi(Zk(i); ξk(i))
Update local momentum: h(ki+) 1 = β0h(ki) + VFi(Zk(i); ξk(i))
Xki+ 1 = Xki)-γ[βohki十ι + VFi(Zki); ξki))]
Non-blocking send (Pj,i)Xki) 1,Pj,i)Wki)) to out-neighbors
k+2
X(i) = (i,i)X(i)
Xk+1 = P Xk
Wk(i+) 1 = P(i,i)Wk(i)
if CountsinceJast = S then
Blockuntil (Pkij)Xk+ 1,Pkij,Wj)) isreceived from in-neighbors
count_since」ast = 0
else
I count_since」ast = count_since_last + 1
end
if Receive buffer non-empty then
for (Pj,％£[ 1 ,Pj'i)Wk?) in the receive buffer do
X(i) = X(2) + T)(ij)Xj)
Xk+1 = Xk + 1 + Pk0 Xk0 十 1
Wk+ι = Wk+i + PkiOj)Wj 十 i
end
end
Update de-biased model parameters: zki+] = Xk+ι∕Wki+ι
end
Note that although the implementation of SGP is with Nesterov momentum, the theoretical analysis
in Assran et al. (2019) only considers the vanilla case where there is no momentum. Accordingly,
the update rule can be written in a matrix form as
Xk+1 = (Xk - YVF(Zk； ξk)) P>
(9)
17
Published as a conference paper at ICLR 2020
where Xk = [x(k1), . . . , x(km)] ∈ Rd×m stacks all model parameters at different nodes and Zk =
[zk(1), . . . , zk(m)] ∈ Rd×m denotes the de-biased parameters. Similarly, we define the stochastic
gradient matrix as VF (Zk) = [VFι (z，); ξki)),..., VFm (z”; ξki))] ∈ Rd×m. Moreover, Pk ∈
Rm×m is defined as the mixing matrix which conforms to the underlying communication topology.
If node j is one of the in-neighbors of node i, then p(i,j) > 0 otherwise p(i,j) = 0. In particular,
matrix Pk is column-stochastic.
If we multiply a vector 1/m on both sides of the update rule (9), we have
m
xk+ι = Xk- 士 X VFi(Zki); ξki))	(10)
i=1
where xk = Xk1/m denotes the average model across all worker nodes. Recall that in SlowMo,
we rewrite the updates of the base algorithm at the kth steps of the tth outer iteration as
xt,k+1 = xt,k - γdt,k .	(11)
So comparing (10) and (11), We can conclude that ʤ = mm Pm=I VFi(z(ik; ξ(i)). As a conse-
quence, We have Et,k[dt,k]= 煮 Pm=I Vfi(Z(k). Since mini-batch gradients are independent, it
folloWs that
m 2
E kdt,k - Et,k[dt,k]k2 =E W XhVFi(Z*;馈)-Vfi(z(ik)]	(12)
i=1
m2
=m X EIlVFi(χ(ik ； ξ(ik )-wi/“	(13)
i=1
≤ σ2 = V.	(14)
m
Similarly, for OSGP, one can repeat the above procedure again. But the definition of Xk, Zk and
VF(Zk) Will change, in order to account for the delayed messages. In this case, We still have the
update rule Eq. (10). But xk is no longer the averaged model across all nodes. It also involves
delayed model parameters. We refer the interested reader to Assran et al. (2019) for futher details.
C.2 D-PSGD
In the case of decentralized parallel SGD (D-PSGD), proposed in Lian et al. (2017), the update rule
is quite similar to SGP. HoWever, the communication topology among Worker nodes is an undirected
graph. Hence, the mixing matrix Pk is doubly-stochastic. Each node Will exchange the model
parameters With its neighbors. The update rule can be Written as
Xk+1 = (Xk — YVF(Xk; ξk)) P>.
(15)
Again, We have that Xk = [x(k1), . . . , x(km)] ∈ Rd×m stacks all model parameters at different nodes
and VF(Xk) = [VF1(x(k1); ξk(i)), . . . , VFm(x(ki); ξk(i))] ∈ Rd×m denotes the stochastic gradient
matrix. By multiplying a vector 1/m on both sides of (15), We have
m
xk+ι=Xk- m x VFi(Xki)； ξki))
i=1
= Xk - γdk .
(16)
(17)
As a result, using the same technique as (12)-(14), we have V = σ2∕m for D-PSGD.
C.3 Local SGD
We further present the pseudo-code of Local SGD in Algorithm 4, and the pseudo-code of double-
averaging momentum scheme in Algorithm 5.
18
Published as a conference paper at ICLR 2020
Algorithm 4: Local SGD with Nesterov Momentum
1
2
3
4
5
6
7
8
9
10
1
2
3
4
5
6
7
8
9
10
11
12
Input: learning rate γ; momentum factor β0; Communication period τ; Number of worker nodes
m. Initial point x(0i) and h(0i) = 0for all nodes i ∈ {1, 2, . . . , m}.
for k ∈ {0, 1, . . . , K - 1} at worker i in parallel do
Compute mini-batch gradients: VFi(X(ki)； ξk(i))
Update local momentum: h(ki+) 1 = β0h(ki) + VFi(X(ki)； ξk(i))
xki+1 = Xki)-Y[βοhki+ι + VFi(zki); ξki))]
if k mod τ = 0 then
I ALLREDUCE model parameters: xki+1 = * Pm=I xj+1
else
I x(i) = x(i)
I xk+ι = xk+2
end
end
Algorithm 5: Local SGD with Double-Averaging Nesterov Momentum (Yu et al., 2019a)
Input: learning rate γ; momentum factor β0; Communication period τ; Number of worker nodes
m. Initial point x(0i) and h(0i) = 0for all nodes i ∈ {1, 2, . . . , m}.
for k ∈ {0, 1, . . . , K - 1} at worker i in parallel do
Compute mini-batch gradients: VFi(X(ki)； ξk(i))
Update local momentum: hk] ι
βohki) + VFi(Xki) ； ξki))
xki+1 = Xki)-Y[β0hki+1 + VFi(zki); ξki))]
if k mod τ = 0 then
AllReduce model parameters: X(ki+) 1 =
AllReduce momentum buffers: h(ki+) 1 =
else
(i) =	(i)
Xk + 1 = Xk+ 2
hki+ι =hki+ 2
ɪ Pm Xj)
m 乙j=1 xk+ 2
J Pm	h(j)
m 乙j=1 "k+ 2
end
end
D Proofs
D. 1 Equivalent Updates
To begin, recall that ʤ = ml Pm=I dtik, and that the local updates are
X(t,ik)+1 = Xt(,ik)
	
(18)
for k = 0,...,τ — 1, followed by an averaging step to obtain xt,τ =煮 Pm=I XtiT∙ Therefore, We
can write the update rule of the base optimizer as
τ-1
xt,τ = xt,0 - γ	dt,k.
k=0
(19)
19
Published as a conference paper at ICLR 2020
Combining this with (2) and (3), we have
τ-1
xt+1,0 = xt,0 - αγ dt,k - αγβut	(20)
k=0
τ-1
= xt,0 - αγ	dt,k + β (xt,0 - xt-1,0)	(21)
k=0
Let yt,o = xt,o + ι-ββ(xt,o - xt-ι,o), ∀t. Then by rearranging terms We get
τ-1
yt+1,0 = yt,0 - 1¾ ∑dt,k.	(22)
- β k=0
NoW, let us further extend the auxiliary sequence to all values of k 6= 0 as folloWs:
αγ
yt,k+1 = yt,k - 1 耳dt,k.	(23)
It is easy to shoW that yt,τ = yt+1,0 . In the sequel, We Will analyze the convergence of sequence
{yt,k} instead of {xt,k}.
D.2 Preliminaries
In the table beloW, We list all notations used in this paper.
Table D.1: List of notations.
Global learning rate	α
Global momentum factor	β
learning rate	γ
Outer iteration length	τ
Total number of outer iterations	T
Total number of steps	K
Liptschiz constant	L
Number of worker nodes	m
Throughout the theoretical analysis, We Will repeatedly use the folloWing facts:
•	Fact 1： ha, b〉= 2 IIall2 + 2 Ilbll2 - 1 Ila - b『；
•	Fact 2: According to Young’s Inequality, for any a > 0, We have
±ha, bi≤ ɪ ka∣2 + a kb∣2 .	(24)
2a	2
•	Fact 3： Ia + bI2 ≤ 2 IaI2 + 2 IbI2;
•	Fact 4： Suppose {ai}ik=1 is a set of non-negative scalars and s = Pik=1ai. Then according
to Jensen’s Inequality, We have
n
aibi
i=1
2
= s2
2k	k
≤ s2 ∙ X a kbik2 = S ∙ X a kbik2.	(25)
i=1 s	i=1
D.3 General Treatment
Since each local objective f is L-Smooth,the function f = m1- Pm=I f is also L-Smooth. It follows
that
Et,k [f (yt,k+1)] - f(yt,k ) ≤ -	：；-γΞ	Wf (yt,k ),	Et,k [dt,k D +	ς) 门	Y q、2	Et，k	hkdt,k ^]	(26)
1 - β	2(1 - β)
20
Published as a conference paper at ICLR 2020
where Et,k denotes a conditional expectation over the randomness in the (t, k)-th iteration, condi-
tioned on all past random variables. For the first term on the right hand side:
-Wf (yt,k ), Et,k [dt,k]i = -Wf (yt,k ) - Vf(xt,k), Et,k [dt,k])—〈▽/(Xt,k), Et,k [dt,kD
≤ 2a kVf (yt,k ) - Vf (xt,k )k2 + 2 kEt,k [dt,k ]k2
-	hVf(xt,k), Et,k[dt,k]i	(27)
=21a kVf(yt,k) - Vf(χt,k)k2 - F kEt,k[dt,k]k2
-	2 kVf (Xt,k )k2 + 2 kVf (Xt,k ) - Et,k[dt,k ]k2	(28)
≤ 2α kyt,k - xt,k k2------2 — kEt,k[dt,k ]k2
-	$ kVf (Xt,k )k2 + $ kVf (Xt,k ) - Et,k[dt,k ]k2	(29)
where (27) comes from Fact 4 (24) , and a > 0 is constant. For simplicity, we directly set a = 0.5.
Eqn. (28) uses Fact 2(a, b)= 1 ∣∣ak2 + 1 ∣∣bk2 一 2 ∣∣a 一 b∣∣2. Furthermore, according to the
definition of yt,k, it can be shown that
(30)
(31)
(32)
Substituting (32) into (29), it follows that
-hV f (yt,k ), Et,k [dt,k D ≤ - 2 kVf (Xt,k )k2 - 4 kEt,k [dt,k ]∣2 + 2 kVf (Xt,k ) - Et,k [dt,k ]∣2
+ 2γ2L2 (1 - ι-β) IXdt,j	+ (2-β)2 kXt,0 - Xt-1,0k2.
(33)
∣yt,k - Xt,k ∣
1———
1 - β
k-1
γ dt,j + yt,0 - Xt,0
≤2γ2 (1- ⅛
=2γ2 (1- ⅛
j=0
)2IIIIIIkXj=-01dt,j
)2IIIIIIkXj=-01dt,j
+ 2 ∣yt,0 - Xt,0∣2
ɪ 2β2 ll	ll2
+ (1 - β)2 kXt,0 - Xt-1,0k
2
2
2
Moreover, for the second term in (26), we have
Et,k h∣dt,k ∣ i = ∣Et,k [dt,k]∣ + Et,k h∣dt,k - Et,k [dt,k]∣ i .	(34)
Then, plugging (33) and (34) into (26),
Et,k[f (yt,k+1)] - f(yt,k) ≤ - Y2f IlVf(Xt,k)∣2 - Y2f Q - YeffL)IlEt,k[dt,k]k2
+ “f Et,k hkdt,k - Et,k [dt,k ]∣2i + Neff IlVf(Xt,k) - Et,k [dt,k ]∣2
+ 2γeffY2L2 (1 - τ-β)21Xdt,j	+ 2γe-ββL2 kXt,0 -Xt-1,0k2
(35)
21
Published as a conference paper at ICLR 2020
where Yeff = αγ∕(1 - β). Taking the total expectation,
E[f(yt,k+ι) - f(yt,k)] ≤-γ2fEkvf(χt,k)k2
E kEt,k [dt,k]k
+ ɔef E hkdt,k - Et,k[dt,k ]k2i + -ɪeff Ell▽/(Xt,k ) - Et,k [dt,k ]k2
X--------------------------------------------------------}
^^^{^^^
N1 (t,k)
2
+
+ 2γeffγ2L2
1-
2	k-1
金)E X dt,j
j=0
2γeffβ2L2	2
(1 - β)2 Ekxt,0 - xt-1,0k .
X----------------------------'
^^^{^^^
N2(t,k)
^^{^^
N3(t)
(36)
Summing from k = 0 to k = τ - 1, we have
E[f(yt+1,0) - f(yt,0)] =E[f (yt,τ) - f(yt,0)]
(37)
τ-1
≤-γ2f X E kVf (xt,k)k2
k=0
τ-1
X E kEt,k [dt,k]k2
k=0
YeffLTV
-2-
τ-1	τ-1	τ-1
+ ^yf X Ek▽/(xt,k ) - Et,k [dt,k ]k2 + X N2(t, k) + X N3 ⑴,
k=0	k=0	k=0
(38)
where N1(t, k), N2(t, k), and N3(t) are as defined in (36). Summing from t = 0 to t = T - 1 and
dividing both side by total iterations K = τT ,
E[f (yT,o) - f (yo,o)]
K
T-1 τ-1	T-1 τ-1
≤ - 2K X X Ek▽/(Xt,k)k2 - YKf (2 -YeffL) X X EkEt,k [dt,k]k2
+ fV + Yef ∑ X E kVf (xt,k) - Et,k[dt,k ]k2
t=0 k=0
1 T-1 τ-1	1 T-1 τ-1
+ K XX N2(t,k) + K XX N3(t)∙
t=0 k=0	t=0 k=0
(39)
Now, we are going to further expand the expressions of the last two terms in (39).
D.3.1 Bounding N2(t, k)
Using the fact ka + bk2 ≤ 2 kak2 + 2kbk2, we have
k-1
Xdt,j
j=0
2
≤2
≤2
k-1
X (dt,j -Et,j[dt,j])
j=0
k-1
X (dt,j - Et,j [dt,j])
j=0
2
k-1
+ 2	Et,j [dt,j ]
j=0
2
k-1
+ 2kX kEt,j[dt,j]k2
j=0
(40)
(41)
+
2
22
Published as a conference paper at ICLR 2020
where the last inequality comes from Fact 3. Then, taking the total expectation and summing over
the t-th outer iteration,
E
τ-1
X
k=0
k-1
dt,j
j=0
≤2E
τ-1
X
k=0
k-1	2	τ-1 k-1
X (dt,j- Et,j [dt,j ])∣∣∣ +2 X k X EkEtj [&，]『
j=0	k=0 j=0
τ-1 k-1	τ-1 k-1
=2XXE kdt,j-Et,j[dt,j]k2 +2XkXEkEt,j[dt,j]k2
k=0 j=0	k=0 j=0
τ-1	τ-1 τ-1
≤2V Xk+2XkXEkEt,j[dt,j]k2
k=0	k=0 j=0
τ-1
=τ(τ - 1)V + τ(τ - 1) XE kEt,k[dt,k]k2
k=0
(42)
(43)
(44)
(45)
2
where (43) uses the following fact:
E Jx(dt,j-Etj[dt,j])| j = XE Udtj-Et,j[dt,j]k2i
k-1 k-1
+ 2 X X E hdt,j - Et,j [dt,j], dt,l - Et,l [dt,l]i	(46)
j=0 l=j+1
k-1
=XE kdt,j -Et,j[dt,j]k2
j=0
k-1 k-1
+ 2 X X E hdt,j - Et,j [dt,j], Et,l [dt,l - Et,l [dt,l]]i (47)
j=0 l=j+1
k-1
=XE kdt,j -Et,j[dt,j]k2 .	(48)
j=0
As a result, we end up with the following
XN2(t,k) ≤2γ3ffL2τ(T- I)(I-β2-α产
α2
k=0
T-1 τ-1	2
XX
N2(t,k) ≤2γ3ffL2τ(T - 1)(1 - β- ɑ)
t=0 k=0	α
T-1τ-1	2
K xxN2(t,k) ≤2γ3ffL2τ(τ - 1)(1 - α- ɑ)
t=0 k=0
τ-1
V + XEkEt,k[dt,k]k2 ,
k=0
T-1τ-1
TV + XX
E kEt,k[dt,k]k2
V 1 T-1τ-1
7 + ⅛∑∑E kEt,k[dt,k]k2
t=0 k=0
(49)
(50)
(51)
where K = TT denotes the total steps.
23
Published as a conference paper at ICLR 2020
D.3.2 Bounding N3(t)
From the update rule (2), (3) and (18), we have
kxt,0 - xt-1,0 k2 =α2γ2
τ-1	2
dt-1,k + βut-1
k=0
τ-1
τ-1
2
α2γ2
α2γ2
≤2α2γ2
dt-1,k + β	dt-2,k + β2ut-2
k=0	k=0
Xt-1 βt-1-s τX-1ds,k!
s=0	k=0
I
Xt-1 βt-1-s	τX-1(ds,k -Es,k[ds,k])!
s=0	k=0
{z
T1
}
+ 2α2γ2
t-1	τ-1
Xβt-1-s	X Es,k [ds,k]
s=0	k=0
।----------------------
2
For the first term T1, taking the total expectation, we get
{z
T2
}
E[Tι] =E UXβtτ-s (XX(ds,k - Es,k[ds,k]))
= Xt-1β2(t-1-s)E	τX-1(ds,k -Es,k[ds,k])
2
s=0
k=0
t-1	τ -1
X β2(t-1-s) X E kds,k-Es,k[ds,k])k2
s=0	k=0
t-1
≤τVXβ2(t-1-s) = TV(1 + β2 + β4 + …+ e2(tT)) ≤
s=0
TV
1 - β2
(52)
(53)
(54)
(55)
(56)
(57)
(58)
(59)
where (57) and (58) are derived using the same routine as (46) to (48). Similarly, for the second
term T2 in (55), according to Fact 4,
E[T2] ≤
Xβt-1-sE [XX Es,kds,k| j
t-1	τ -1
X βt-1-s X E kEs,kds,kk2
s=0	k=0
t-1	τ -1
≤ 1-β Xβt-1-s X E 眄,kds,kk2].
s=0	k=0
(60)
(61)
(62)
24
Published as a conference paper at ICLR 2020
Substituting (59) and (62) back into (55) and summing over the t-th outer iteration, we have
X N3(t) ≤ 4Yf¾2V + 4f中 X β-X E hkEs,k ds,kk1,	(63)
k=0	( - β )	( - β) s=0	k=0
T-1 τ-1
XXN3(t)≤
t=0 k=0
4Kγ3ffL2β2τV
(1- β2)
+ fβ2T2 X X βt-1-s∑ E hl&k ds，k『i
t=0 s=0	k=0
(64)
4Kγ3⅛2^ + 4Y≡2F2 ∑ XE 眄，®dt,kk2i ∑ βT-S
(1 - β )	(1 - β)	t=0 k=0	s=t+1
(65)
≤ 4KYf 2伊口
≤	(1- β2)
+ 4fβ⅛2 X- X E hkEt,k dt,kk2i
(1 - β) t=0 k=0
4Kγf2βTVL
≤	(1- β2)
+「X XE hkEt,kdt,kk2i
(1 - β) t=0 k=0
T-1 τ-1
K XX NM ≤
t=0 k=0
4γ3fL2β 2τV
(1- β2)
+fβτ2 K X XE 眄,k % k2i.
(66)
(67)
(68)
D.3.3 Final Results
Plugging (51) and (68) back into (39), one can obtain
Ef(yT，0K- f(y0,0)] ≤ - 2K TXXEkVf (χt,k)k2 - 2KCi TXXE kEt,k[dt,k]k2
t=0 k=0	t=0 k=0
+ γfV "1+4YeffL(T- 1) (- - 1)2 + 8f⅞ #
T-1 τ-1
+ 2ef X X E kVf (xt,k ) - Et,k [dt,k ]k2	(69)
t=0 k=0
where Ci = 1/2 - YeffL - 4γ2ffL2τ(τ - 1)(1 - β - α)2∕α2 - 8γ2ffL2τ2β2∕(1 - β)2. When the
constants satisfy
1 - YeffL - 4Y2ffL2τ(τ - 1)
(1 — β — α)2
ɑ2
8γ2ffL2τ 2β2
(1 - β)2
≥ 0,
(70)
we have
E[f (yT,o) - f (yo,o)]
K
T-i τ-i
≤-Yf XX
≤	2K々乙
t=0 k=0
T-i τ-i
EkVf(Xt,k)k2 + 2K∙ XX EkVf (xt,k) - Et,k [dt,k]k2
t=0 k=0
+ Y2ffLV
+	2
1 + 4YeffL(τ - 1)
1 - β 1V , 8YeffLτβ2、
丁 - 1J + (1-β2 ) .
(71)
After rearranging terms, we get
T-i τ-i
K XXEkVf(xt,k)k2
t=0 k=0
≤ 2E[f (yo,o) - f (yT,o)]
一	YfK
T-i τ-i
+ K XXEkVf(xt,k)-Et,k[dt,k]k2
t=0 k=0
+ YeffLV
1 + 4YeffL(τ - 1)
1 - β 1V . MeffLTβ
丁 - 1J + 1-β2
. (72)
α
25
Published as a conference paper at ICLR 2020
Furthermore, since y0,0 = x0,0 - βx-1,0/(1 - β) = x0,0 and f(yτ,o) ≥ fi∏f, the above upper
bound can be simplified as
K X X Ekw(χt,k )k2 ≤ 2(f(f- fi∏f) + K X XEk▽/(Xt,k ) - Et,k [dt,k ]k2
t=0 k=0	γeff	t=0 k=0
+ YeffLV "l + 4YeffL(T- 1) (1-β - J + f2 #
α	1 - β2
(73)
If we set Yeff = Pm, then
T-1 τ-1
K XXE kVf (xt,k)k2
t=0 k=0
2 (f (x0,0) - finf)
T-1 τ-1
+K XXEkVf (xt,k) - Et,k [dt,k ]k2
t=0 k=0
+ mLV + 4mL2(τ — 1) ,1 — β
mK
ʌ2	8mL2τβ2
-1) + K (1- β2).
(74)
Recall the learning rate constraint is
J - YeffL - 4Y2ffL2τ(T - 1)
(1 - β - α)2
α2
-f2: ≥ 0.
(75)
≤
m mK
K
α
When γeff = √m, the constraint can be rewritten as
4τ(τ - 1)
1
—≥
2 ―
After rearranging, we have
K
mL2
(1 - β - α)2 mL2
α2
+ 8τ2
β2	mL2
(1 - β)2 K
(76)
(1 - β - α)2
β - α)2 q16 2 β2 ^1
ɑ2	+16τ (1-β)2 +1,
(77)
α2
β2
+ 16τ (1 - β)2 + 1
(78)
K
mL2
Furthermore, note that
≥1 + 8τ(τ - 1)
(1 - β - α)
α2
2	2	β2
一 + 16τ (1 - β)2 + 1
(79)
8τ (τ - 1)
(1 - β - α)2
α2
+ 16τ2 二 U
≤ 9τ
2(1 - β - α)2
α2
+ 16τ 2 二"Y
(80)
≤ √3max
3τ(1 - β - α) 4τβ
α ,1-β
,1 .
(81)
K
1
2
1
2
Therefore, when K ≥ mL2(1 + √3max {3τ(1-β-α), 4-β, 1∣), the condition (79) must be sat-
isfied.
D.4 Special Case 1: Blockwise Model Update Filtering (BMUF)
In this case, the inner optimizer is Local-SGD. That is,
mm
dt,k = m X VF(x(ik;Sk), Et,k[dt,k] = m XNf(Xtk).	(82)
i=1	i=1
Since all worker nodes are averaged after every T iterations, we have Xt(,i0) = Xt,0 , ∀i. Besides, it is
easy to validate that V = σ2∕m.
26
Published as a conference paper at ICLR 2020
According to previous literature on the convergence of Local-SGD (Wang & Joshi, 2018; Yu et al.,
2019a), we can directly get the following upper bound.
T-1τ-1
A=K xxE kVf (xt,k) - Et,k [dt,k]k
t=0 k=0
2
(83)
T-1 τ-1
T-1 τ-1
K xxE Il Vf (xt,k)-
t=0 k=0
T-1τ-1 m
≤ ——XXX E
mK
m
m X Vf(Xtik)
i=1
xt,k) - Vf (xt(,ik))III2
t=0 k=0 i=1
2 T-1τ-1 m	2
≤焉XXXE限-x制
t=0 k=0 i=1
2γ2L2σ2T	6γ2L2Z 2τ2
—1 — 12γ2L2τ2 + 1 — 12γ2L2τ2 '
When γLτ ≤ 6, we have 1/(1 - 12γ2L2τ2) ≥ 3/2. It follows that
T-1τ-1
K xxE kVf(xt,k) -Et,k[dt,k]k2 ≤3γ2L2σ2τ + 9γ2L2ζ2τ2.
t=0 k=0
(84)
(85)
(86)
(87)
(88)
Substituting (88) into (73) and setting 1-^YL =，m/K, We have
T-1τ-1
K xxEkVf(xt,k)k2≤
t=0 k=0
2L (f (x0,0) - finf) + σ
α2m 3σ2τ + 9ζ2τ2
(1-β)2
4σ2(τ - 1) β2	8σ2τ
K + (1 - β2) ~K~
(89)
=O
m mK
1-β -1)2
α
+O
(90)
2
+
K
+
D.5 Special Case 2: Lookahead
In this case, the inner optimizer is SGD and m = 1. Thus, we haveβ = 0, Et,k[dt,k] = Vf(xt,k),
and V = σ2 . Therefore,
ɪ X XEkVf(Xt,k)k2 ≤2(f(x0,0)- finf) + αγLσ2 + 4(1 - α)2γ2L2(τ - 1)σ2	(91)
K	αγK
t=0 k=0
It can be observed that when α = 1 or T = 1, the above upper bound reduces to the case of vanilla
mini-batch SGD. If we set αγL =，1/K, then we have
K x X EkVf(Xt,k)k2 ≤2L(f(x0,0√-finf) 十
K t=0 k=0	K
4(1 — α)2(τ — 1)σ2
+	α2K
+O
(92)
(93)
If the total iterations K is sufficiently large, then the first term will dominate the convergence rate.
27