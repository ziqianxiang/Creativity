Published as a conference paper at ICLR 2020
Prediction, Consistency, Curvature: Represen-
tation Learning for Locally-Linear Control
Nir Levine1 ∖ Yinlam Chow2*, Rui Shu3, Ang Li1, Mohammad Ghavamzadeh4, Hung Bui5
1DeepMind, 2 Google Research, 3Stanford University, 4Facebook AI Research, 5VinAI
Ab stract
Many real-world sequential decision-making problems can be formulated as op-
timal control with high-dimensional observations and unknown dynamics. A
promising approach is to embed the high-dimensional observations into a lower-
dimensional latent representation space, estimate the latent dynamics model, then
utilize this model for control in the latent space. An important open question is how
to learn a representation that is amenable to existing control algorithms? In this pa-
per, we focus on learning representations for locally-linear control algorithms, such
as iterative LQR (iLQR). By formulating and analyzing the representation learning
problem from an optimal control perspective, we establish three underlying princi-
ples that the learned representation should comprise: 1) accurate prediction in the
observation space, 2) consistency between latent and observation space dynamics,
and 3) low curvature in the latent space transitions. These principles naturally
correspond to a loss function that consists of three terms: prediction, consistency,
and curvature (PCC). Crucially, to make PCC tractable, we derive an amortized
variational bound for the PCC loss function. Extensive experiments on benchmark
domains demonstrate that the new variational-PCC learning algorithm benefits
from significantly more stable and reproducible training, and leads to superior
control performance. Further ablation studies give support to the importance of all
three PCC components for learning a good latent space for control.
1	Introduction
Decomposing the problem of decision-making in an unknown environment into estimating dynamics
followed by planning provides a powerful framework for building intelligent agents. This decomposi-
tion confers several notable benefits. First, it enables the handling of sparse-reward environments by
leveraging the dense signal of dynamics prediction. Second, once a dynamics model is learned, it can
be shared across multiple tasks within the same environment. While the merits of this decomposition
have been demonstrated in low-dimensional environments (Deisenroth & Rasmussen, 2011; Gal
et al., 2016), scaling these methods to high-dimensional environments remains an open challenge.
The recent advancements in generative models have enabled the successful dynamics estimation
of high-dimensional decision processes (Watter et al., 2015; Ha & Schmidhuber, 2018; Kurutach
et al., 2018). This procedure of learning dynamics can then be used in conjunction with a plethora of
decision-making techniques, ranging from optimal control to reinforcement learning (RL) (Watter
et al., 2015; Banijamali et al., 2018; Finn et al., 2016; Chua et al., 2018; Ha & Schmidhuber, 2018;
Kaiser et al., 2019; Hafner et al., 2018; Zhang et al., 2019). One particularly promising line of
work in this area focuses on learning the dynamics and conducting control in a low-dimensional
latent embedding of the observation space, where the embedding itself is learned through this
process (Watter et al., 2015; Banijamali et al., 2018; Hafner et al., 2018; Zhang et al., 2019). We refer
to this approach as learning controllable embedding (LCE). There have been two main approaches to
this problem: 1) to start by defining a cost function in the high-dimensional observation space and
learn the embedding space, its dynamics, and reward function, by interacting with the environment
in a RL fashion (Hafner et al., 2018; Zhang et al., 2019), and 2) to first learn the embedding space
and its dynamics, and then define a cost function in this low-dimensional space and conduct the
control (Watter et al., 2015; Banijamali et al., 2018). This can be later combined with RL for extra
fine-tuning of the model and control.
In this paper, we take the second approach and particularly focus on the important question of
what desirable traits should the latent embedding exhibit for it to be amenable to a specific class of
control/learning algorithms, namely the widely used class of locally-linear control (LLC) algorithms?
We argue from an optimal control standpoint that our latent space should exhibit three properties.
The first is prediction: given the ability to encode to and decode from the latent space, we expect
* Equal contribution. Correspondence to nirlevine@google.com
1
Published as a conference paper at ICLR 2020
the process of encoding, transitioning via the latent dynamics, and then decoding, to adhere to the
true observation dynamics. The second is consistency: given the ability to encode a observation
trajectory sampled from the true environment, we expect the latent dynamics to be consistent with
the encoded trajectory. Finally, curvature: in order to learn a latent space that is specifically amenable
to LLC algorithms, we expect the (learned) latent dynamics to exhibit low curvature in order to
minimize the approximation error of its first-order Taylor expansion employed by LLC algorithms.
Our contributions are thus as follows: (1) We propose the Prediction, Consistency, and Curvature
(PCC) framework for learning a latent space that is amenable to LLC algorithms and show that
the elements of PCC arise systematically from bounding the suboptimality of the solution of the
LLC algorithm in the latent space. (2) We design a latent variable model that adheres to the PCC
framework and derive a tractable variational bound for training the model. (3) To the best of our
knowledge, our proposed curvature loss for the transition dynamics (in the latent space) is novel. We
also propose a direct amortization of the Jacobian calculation in the curvature loss to help training
with curvature loss more efficiently. (4) Through extensive experimental comparison, we show that
the PCC model consistently outperforms E2C (Watter et al., 2015) and RCE (Banijamali et al., 2018)
on a number of control-from-images tasks, and verify via ablation, the importance of regularizing the
model to have consistency and low-curvature.
2	Problem Formulation
We are interested in controlling the non-linear dynamical systems of the form st+1 = fS (st , ut) + w,
over the horizon T . In this definition, st ∈ S ⊆ Rns and ut ∈ U ⊆ Rnu are the state and action
of the system at time step t ∈ {0, . . . , T - 1}, w is the Gaussian system noise, and fS is a smooth
non-linear system dynamics. We are particularly interested in the scenario in which we only have
access to the high-dimensional observation xt ∈ X ⊆ Rnx of each state st (nx ns). This scenario
has application in many real-world problems, such as visual-servoing (Espiau et al., 1992), in which
we only observe high-dimensional images of the environment and not its underlying state. We further
assume that the high-dimensional observations x have been selected such that for any arbitrary control
sequence U = {ut}tT=-01, the observation sequence {xt}tT=0 is generated by a stationary Markov
process, i.e., xt+i 〜P(∙∣χt, Ut), ∀t ∈ {0,...,T — 1}.1
A common approach to control the above dynamical system is to solve the following stochastic
optimal control (SOC) problem (Shapiro et al., 2009) that minimizes expected cumulative cost:
T-1
min L(U, P, c, x0) := E cT (xT) + X ct(xt, ut) | P, x0 , 2
(SOC1)
t=0
where ct : X × U → R≥0 is the immediate cost function at time t, cT ∈ R≥0 is the terminal cost, and
x0 is the observation at the initial state s0 . Note that all immediate costs are defined in the observation
space X, and are bounded by cmax > 0 and Lipschitz with constant clip > 0. For example, in visual-
servoing, (SOC1) can be formulated as a goal tracking problem (Ebert et al., 2018), where we control
the robot to reach the goal observation xgoal, and the objective is to compute a sequence of optimal
open-loop actions U that minimizes the cumulative tracking error E[Pt kxt - xgoal k2 | P, x0].
Since the observations X are high dimensional and the dynamics in the observation space P(∙∣χt, Ut)
is unknown, solving (SOC1) is often intractable. To address this issue, a class of algorithms has been
recently developed that is based on learning a low-dimensional latent (embedding) space Z ⊆ Rnz
(nz	nx) and latent state dynamics, and performing optimal control there. This class that we
refer to as learning controllable embedding (LCE) throughout the paper, include recently developed
algorithms, such as E2C (Watter et al., 2015), RCE (Banijamali et al., 2018), and SOLAR (Zhang et al.,
2019). The main idea behind the LCE approach is to learn a triplet, (i) an encoder E : X → P(Z); (ii)
a dynamics in the latent space F : Z × U → P(Z); and (iii) a decoder D : Z → P(X). These in turn
can be thought of as defining a (stochastic) mapping P : X × U → P(X) of the form P = D ◦ F ◦ E.
We then wish to solve the SOC in latent space Z :
min E ∣L(U, F, c,zo) | E, x°] + λ2 JRnPL
(SOC2)
such that the solution of (SOC2), U$, has similar performance to that of (SOC1), U；,
i.e., L(U；,P, c,xo) ≈ L(U2,P,c,xo). In (SOC2), zo is the initial latent state sampled from
the encoder E(∙∣χo); c : Z × U → R≥o is the latent cost function defined as Ct(Zt,uj =
R ct(xt, Ut)dD(xt|zt); R2(Pb) is a regularizer over the mapping Pb; and λ2 is the corresponding
1A method to ensure this Markovian assumption is by buffering observations (Mnih et al., 2013) for a number
of time steps.
2See Appendix B.3 for the extension to the closed-loop MDP problem.
2
Published as a conference paper at ICLR 2020
Figure 1: Evolution of the states (a)(blue) in equation SOC1 under dynamics P, (b)(green) in equa-
tion SOC2 under dynamics F, and (c)(red) in equation SOC3 under dynamics P.
regularization parameter. We will define R2 and λ2 more precisely in Section 3. Note that the
expectation in (SOC2) is over the randomness generated by the (stochastic) encoder E.
3	PCC Model: A Control Perspective
As described in Section 2, we are primarily interested in solving (SOC1), whose states evolve under
dynamics P, as shown at the bottom row of Figure 1(a) in (blue). However, because of the difficulties
in solving (SOC1), mainly due to the high dimension of observations x, LCE proposes to learn a
mapping Pbby solving (SOC2) that consists of a loss function, whose states evolve under dynamics
F (after an initial transition by encoder E), as depicted in Figure 1(b), and a regularization term.
The role of the regularizer R2 is to account for the performance gap between (SOC1) and the loss
function of (SOC2), due to the discrepancy between their evolution paths, shown in Figures 1(a)(blue)
and 1(b)(green). The goal of LCE is to learn P of the particular form P = D ◦ F ◦ E, described in
Section 2, such that the solution of (SOC2) has similar performance to that of (SOC1). In this section,
we propose a principled way to select the regularizer R2 to achieve this goal. Since the exact form
of (SOC2) has a direct effect on learning P, designing this regularization term, in turn, provides us
with a recipe (loss function) to learn the latent (embedded) space Z. In the following subsections, we
show that this loss function consists of three terms that correspond to prediction, consistency, and
curvature, the three ingredients of our PCC model.
Note that these two SOCs evolve in two different spaces, one in the observation space X under
dynamics P, and the other one in the latent space Z (after an initial transition from X to Z) under
dynamics F . Unlike P and F that only operate in a single space, X and Z, respectively, P can
govern the evolution of the system in both X and Z (see Figure 1(c)). Therefore, any recipe to learn
P, and as a result the latent space Z, should have at least two terms, to guarantee that the evolution
paths resulted from Pbin X and Z are consistent with those generated by P and F. We derive these
two terms, that are the prediction and consistency terms in the loss function used by our PCC model,
in Sections 3.1 and 3.2, respectively. While these two terms are the result of learning P in general
SOC problems, in Section 3.3, we concentrate on the particular class of LLC algorithms (e.g., iLQR
(Li & Todorov, 2004)) to solve SOC, and add the third term, curvature, to our recipe for learning Pb.
3.1	Prediction of the Next Observation
Figures 1(a)(blue) and 1(c)(red) show the transition in the observation space under P and Pb, where
Xt is the current observation, and xt+1 and Xt+ι are the next observations under these two dynamics,
respectively. Instead of learning a Pb with minimum mismatch with P in terms of some distribution
norm, we propose to learn P by solving the following SOC:
min L(U, Pb, c, x0) + λ3 R3 (Pb),	(SOC3)
U,Pb
whose loss function is the same as the one in (SOC1), with the true dynamics replaced by P. In
Lemma 1 (see Appendix A.1, for proof), we show how to set the regularization term R3 in (SOC3),
such that the control sequence resulted from solving (SOC3), Uξt, has similar performance to the
solution of (SOC1), U；, i.e., L(U；,P, c, x0) ≈ L(U3,P, c, x0).
_	_	___ _	...	____ 公,、一	_
Lemma 1. Let U^ be a solution to (SOC1) and (U3,P3) be a solution to (SOC3) with
R3(P) = Eχ,u[DKL(P(∙∣χ,u)∣∣P(∙∣χ,u))]	and	λ3 = vUu ∙ T2cfmχ.
Then, we have L(U；,P, c,x0) ≥ L(U3,P, c,xo) - 2λ3 JR3(P3).
(1)
In Eq. 1, the expectation is over the state-action stationary distribution of the policy used to generate
the training samples (uniformly random policy in this work), and U is the Lebesgue measure of U.3
3In the case when sampling policy is non-uniform and has no measure-zero set, 1/U is its minimum measure.
3
Published as a conference paper at ICLR 2020
3.2	Consistency in Prediction of the Next Latent State
In Section 3.1, we provided a recipe for learning P (in form of D ◦ F ◦ E) by introducing an
intermediate (SOC3) that evolves in the observation space X according to dynamics P. In this section
we first connect (SOC2) that operates in Z with (SOC3) that operates in X . For simplicity and
without loss generality, assume the initial cost c0 (x, u) is zero.4 Lemma 2 (see Appendix A.2, for
proof) suggests how we shall set the regularizer in (SOC2), such that its solution performs similarly
to that of (SOC3), under their corresponding dynamics models.
Lemma 2. Let (U3,P3) be a solution to (SOC3) and (U2,P2) be a solution to (SOC2) With
R2(P) = Ex,u[dKL ((E ◦ P)(∙∣x,u)∣∣(F ◦	E)(∙∣x,u))]	and	λ2	=	P2U	∙	T2cmax.(2)
Then, we have L(U3,P3,c, xo) ≥ L(U2,P2,c, xo) - 2λ2 y R2(P2).
Similar to Lemma 1, in Eq. 2, the expectation is over the state-action stationary distribution of the
policy used to generate the training samples. Moreover, E ◦ P (z0 |x, u) = x0 E(z0|x0)dP(x0|x, u)
and F ◦ E (z0|x, u) = z F (z0|z, u)dE(z|x) are the probability over the next latent state z0, given
the current observation X and action u, in (SOC2) and (SOC3) (see the paths Xt → Zt → ≡t+ι and
Xt → Zt → zt+1 → xt+1 → zt+1 in Figures 1(b)(green) and 1(c)(red)). Therefore R2 (P) can be
interpreted as the measure of discrepancy between these models, which we term as consistency loss.
Although Lemma 2 provides a recipe to learn P by solving (SOC2) with the regularizer (2), unfortu-
nately this regularizer cannot be computed from the data - that is of the form (xt, ut, xt+1) - because
the first term in the DKL requires marginalizing over current and next latent states (Zt and ≡t+1 in
Figure 1(c)). To address this issue, we propose to use the (computable) regularizer
R2,P) = Ex,”/，[Dkl (e(∙∣x0川(F ◦ E) (∙∣x,u))],	⑶
in which the expectation is over (X, u, X0 ) sampled from the training data. Corollary 1 (see Appendix
0
A.3, for proof) bounds the performance loss resulted from using R020(P) instead of R2(P), and shows
that it could be still a reasonable choice.
Corollary 1. Let (U3,Pb3) be a solution to (SOC3) and (U2,Pb2) be a solution to (SOC2) With
R2z(P) and and λ? defined by (3) and (2). Then, we have L(U3,P3,c, x0) ≥ L(U2,P2, c, x0)—
2λ2 ψR2(Pb2 ) + 2R3(PS).
Lemma 1 suggests a regularizer R3 to connect the solutions of (SOC1) and (SOC3). Similarly,
Corollary 1 shows that regularizer R200 in (3) establishes a connection between the solutions of (SOC3)
and (SOC2). Putting these results together, we achieve our goal in Lemma 3 (see Appendix A.4, for
proof) to design a regularizer for (SOC2), such that its solution performs similarly to that of (SOC1).
Lemma 3. Let U； be a solution to (SOC1) and (U2, P2) be a solution to (SOC2) with
R2(Pb) = 3R3(Pb) + 2R!2,(Pb)	and	λ2 = 2pU ∙ T2cmax,	(4)
1	C∕R∖ 1 TΛ∏ / rɔʌ	1 Γ 1 1	/ 1 ∖ JZOX r-r.1	1
where R3 (P) and R020(P) are defined by (1) and (3). Then, we have
L(UI ,P,c,xo) ≥L(U2 ,P,c,xo)—2λ2 qR2 (PS).
3.3	Locally-Linear Control in the Latent Space and Curvature Regularization
In Sections 3.1 and 3.2, we derived a loss function to learn the latent space Z. This loss function, that
was motivated by the general SOC perspective, consists of two terms to enforce the latent space to
not only predict the next observations accurately, but to be suitable for control. In this section, we
focus on the class of locally-linear control (LLC) algorithms (e.g., iLQR), for solving (SOC2), and
show how this choice adds a third term, that corresponds to curvature, to the regularizer of (SOC2),
and as a result, to the loss function of our PCC model.
The main idea in LLC algorithms is to iteratively compute an action sequence to improve the current
trajectory, by linearizing the dynamics around this trajectory, and use this action sequence to generate
4With non-zero initial cost, similar results can be derived by having an additional consistency term on x0 .
4
Published as a conference paper at ICLR 2020
the next trajectory (see Appendix B for more details about LLC and iLQR). This procedure implicitly
assumes that the dynamics is approximately locally linear. To ensure this in (SOC2), we further
restrict the dynamics P and assume that it is not only of the form P = D ◦ F ◦ E, but F, the latent
space dynamics, has low curvature. One way to ensure this in (SOC2) is to directly impose a penalty
over the curvature of the latent space transition function fZ (z, u). Assume F (z, u) = fZ (z, u) + w,
where w is a Gaussian noise. Consider the following SOC problem:
q
R2(P) + RLLC(P) ,	(SOC-LLC)
U,P
where R2 is defined by (4); U is optimized by a LLC algorithm, such as iLQR; RLLC(P ) is given by,
RLLC(Pb) = Ex,u [EJfZ (Z + ez , u + EU)- fZ (Z, U)- (Vz fZ (z, U) ∙ " + VufZ (z, U) Yu)Il2] | Ei ,⑸
where e = (e%,eu)> 〜N(0,δ2I), δ > 0 is a tunable parameter that characterizes the "di-
ameter" of latent state-action space in which the latent dynamics model has low curvature.
λLLC = 2√2T2Cmaχ√Umax "p(1 + p2log(2T∕η)) √X/2,1), where 1/X is the minimum
non-zero measure of the sample distribution w.r.t. X, and 1 - η ∈ [0, 1) is a probability threshold.
Lemma 4 (see Appendix A.5, for proof and discussions on how δ affects LLC performance) shows
that a solution of (SOC-LLC) has similar performance to a solution of (SOC1, and thus, (SOC-LLC)
is a reasonable optimization problem to learn P , and also the latent space Z .
Lemma 4. Let(ULlc, PLlc) be a LLC solution to (SOC-LLC) and U； be a solution to (SOC1).
Suppose the nominal latent state-action trajectory {(zt,Ut)}T01 Satisfies the condition: (zt,ut)〜
N(ZaQ u2,t), δ2I), where {(z2,t, u2,t)}T=01 is the optimal trajectory of (SOC2). Then with proba-
q
R2 (PbL；LC) + RLLC(PbL；LC) .
In practice, instead of solving (SOC-LLC) jointly for U and P, we treat (SOC-LLC) as a bi-level
optimization problem, first, solve the inner optimization problem for P, i.e.,
Pb； ∈ arg min λpR03(Pb) + λcR020(Pb) + λcurRLLC(Pb),	(PCC-LOSS)
Pb
1	Ct /含、	-∏-n	Γι 含/ / I \1 •	.1	ι TT TT TS ι .ι	ι .ι	.
where R30 (P) = -Ex,u,x0 [log P(x0 |x, u)] is the negative log-likelihood,5 and then, solve the outer
optimization problem, mmu L(U, F ,c, z0), where P； = D ◦ F； ◦ E；, to obtain the optimal control
sequence U； . Solving (SOC-LLC) this way is an approximation, in general, but is justified, when the
regularization parameter λLLC is large. Note that we leave the regularization parameters (λp, λc, λcur)
as hyper-parameters of our algorithm, and do not use those derived in the lemmas of this section.
Since the loss for learning P； in (PCC-LOSS) enforces (i) prediction accuracy, (ii) consistency in
latent state prediction, and (iii) low curvature over fZ, through the regularizers R03, R020, and RLLC,
respectively, we refer to it as the prediction-consistency-curvature (PCC) loss.
4 Instantiating the PCC Model in Practice
The PCC-Model objective in (PCC-LOSS) introduces the optimization problem
minPb λpR03(P) + λcR020(P) + λcurRLLC (P). To instantiate this model in practice, we de-
scribe P = D ◦ F ◦ E as a latent variable model that factorizes as P(xt+i, zt, zt+ι | χt,u)=
P(zt | xt)P(zt+1 | zt,ut)P(xt+i | ^t+ι). In this section, we propose a variational approximation
to the intractable negative log-likelihood R30 and batch-consistency R200 losses, and an efficient
approximation of the curvature loss RLLC.
4.1	Variational PCC
The negative log-likelihood 6 R03 admits a variational bound via Jensen’s Inequality,
b
R3(Pb) = -log P(xt+1 | xt,ut) = -log EQ(Zt,Zt+ι∣xt ,ut,xt+ι) Q(zt+Z, t,l X[1] Xi)
Q(Zt , Zt+1 | xt , Ut , xt+1 )
V	- l	、]crv P(Xt+ 1, zt, zt + 1 | xt,ut) — D0	b b θʌ ∕Λ∖
≤ -EQ(Zt，zt+l|xt，ut，xt+1) [log Q(zt,Zt+ι | χt,ut,χt+ι)j = R3, NLE-BOund(P，Q)， ⑹
5Since R3 (Pb) is the sum of R30 (Pb) and the entropy of P , we replaced it with R03(Pb) in (PCC-LOSS).
6For notation convenience, we drop the expectation over the empirical data that appears in various loss terms.
5
Published as a conference paper at ICLR 2020
which holds for any choice of recognition model Q. For simplicity, we assume the recog-
nition model employs bottom-up inference and thus factorizes as Q(zt, zt+ι∖xt, xt+ι,ut) =
Q(^t+1∣xt+1)Q(zt∣Zt+1, xt, ut). The main idea behind choosing a backward-facing model is to
allow the model to learn to account for noise in the underlying dynamics. We estimate the expec-
tations in (6) via Monte Carlo simulation. To reduce the variance of the estimator, we decompose
R03,NLE-Bound further into
-EQ(Zt+ι∣χt+ι) [log Pb(Xt+1 设t+。] + EQ(Zt+ι ∣χt+ι) [DKL (Q(ZtI 1t+ι,xt,Ut)kA(ZtI Xt))]
-H (Q(Zt+1 | xt+1)) - E Q(Zt+1∣xt+1) [log Pb(Zt+1 | zt,ut)],
Q(ZtIZt+1,xt,Ut) L
and note that the Entropy H(∙) and Kullback-Leibler Dkl(∙∣∣∙) terms are analytically tractable when
Q is restricted to a suitably chosen variational family (i.e. in our experiments, Q(Zt+ι ∖ xt+i) and
Q(zt ∖ Zt+i, xt, Ut) are factorized Gaussians). The derivation is provided in Appendix C.1.
Interestingly, the consistency loss R020 admits a similar treatment. We note that the consistency loss
seeks to match the distribution of ^t+ι | xt,u with zt+i ∖ xt+i, which we represent below as
R0(P) = DKL (P(Zt+1 | xt +I)IlP)t + 1 | xt, Ut)) = -H(P(Zt+1 | xt +1))- EP(Zt+ ι∣χt + ι) [log Pb(zt+1 | xt, Ut)i .
zt+1=zt + 1
Here, Pb(zt+ι | xt, Ut) is intractable due to the marginalization of zt. We employ the same procedure
as in (6) to construct a tractable variational bound
R20(P) ≤ -H(P(Zt+1 1 xt+1)) -EP(Zt+ι∣χt+ι)EQ(Zt∣^t+ι,χt,ut) "log P(Zt,⅛+1l,Xt,Ut).
Zt+1=Zt + 1	+
We now make the further simplifying assumption that QQt+i | xt+i) = P(zt+ι ∖ xt+i). This
allows us to rewrite the expression as
R2’(P) ≤ -H(Q(Zt+1 | xt+1))- E Q(Zt+1∣xt+1) [logP(2t+1 | Zt,ut)i
Q(ZtIZt+1,xt ,Ut) L	(7)
+ EQ(Zt+1∣xt+1) IDKL(Q(Zt | 2t+1,xt,Ut)kPb(Zt I Xt)) =R020,Bound(Pb,Q),
which is a subset of the terms in (6). See Appendix C.2 for a detailed derivation.
4.2 Curvature Regularization and Amortized Gradient
In practice we use a variant of the curvature loss where Taylor expansions and gradients are evaluated
at Z = Z + ∈z and U = U + Cu,
RLLC(Pb) = Ee〜N(0,δi)[kfz(z,U) - (Vzfz(z, U)ez + Vufz(z,u)eu) - fz(z,u)k2].	(8)
When nz is large, evaluation and differentiating through the Jacobians can be slow. To circumvent
this issue, the Jacobians evaluation can be amortized by treating the Jacobians as the coefficients of
the best linear approximation at the evaluation point. This leads to a new amortized curvature loss
RLLC-Amor (P,A, B) = Ee 〜N (0,δI )[kfZ (西 U)-(A(Z, u)ez + B(Z, u)eu - fz (z,u))∣∣2]∙	⑼
where A and B are function approximators to be optimized. Intuitively, the amortized curvature loss
seeks—for any given (z, U)—to find the best choice of linear approximation induced by A(z, U) and
B(z,u) such that the behavior of Fμ in the neighborhood of (z,u) is approximately linear.
5	Relation to Previous Embed-to-Control Approaches
In this section, we highlight the key differences between PCC and the closest previous works, namely
E2C and RCE. A key distinguishing factor is PCC’s use of a nonlinear latent dynamics model paired
with an explicit curvature loss. In comparison, E2C and RCE both employed “locally-linear dynamics”
of the form Z = A(z, U)z + B(z,u)u + c(z, U) where Z and U are auxiliary random variables meant
to be perturbations of z and U. When contrasted with (9), it is clear that neither A and B in the
E2C/RCE formulation can be treated as the Jacobians of the dynamics, and hence the curvature
of the dynamics is not being controlled explicitly. Furthermore, since the locally-linear dynamics
are wrapped inside the maximum-likelihood estimation, both E2C and RCE conflate the two key
elements prediction and curvature together. This makes controlling the stability of training much
more difficult. Not only does PCC explicitly separate these two components, we are also the first to
explicitly demonstrate theoretically and empirically that the curvature loss is important for iLQR.
6
Published as a conference paper at ICLR 2020
Figure 2: Top: Planar latent representations; Bottom: Inverted Pendulum latent representations (randomly
selected): left two: RCE, middle two: E2C, right two: PCC.
Furthermore, RCE does not incorporate PCC’s consistency loss. Note that PCC, RCE, and E2C are
all Markovian encoder-transition-decoder frameworks. Under such a framework, the sole reliance
on minimizing the prediction loss will result in a discrepancy between how the model is trained
(maximizing the likelihood induced by encoding-transitioning-decoding) versus how it is used at
test-time for control (continual transitioning in the latent space without ever decoding). By explicitly
minimizing the consistency loss, PCC reduces the discrapancy between how the model is trained
versus how it is used at test-time for planning. Interestingly, E2C does include a regularization term
that is akin to PCC’s consistency loss. However, as noted by the authors of RCE, E2C’s maximization
of pair-marginal log-likelihoods of (xt, xt+1) as opposed to the conditional likelihood of xt+1 given
xt means that E2C does not properly minimize the prediction loss prescribed by the PCC framework.
6	Experiments
In this section, we compare the performance of PCC with two model-based control algorithm
baselines: RCE7 (Banijamali et al., 2018) and E2C (Watter et al., 2015), as well as running a thorough
ablation study on various components of PCC. The experiments are based on the following continuous
control benchmark domains (see Appendix D for more descriptions): (i) Planar System, (ii) Inverted
Pendulum, (iii) Cartpole, (iv) 3-link manipulator, and (v) TORCS simulator8 (Wymann et al., 2000).
To generate our training and test sets, each consists of triples (xt, ut, xt+1), we: (1) sample an under-
lying state st and generate its corresponding observation xt, (2) sample an action ut, and (3) obtain
the next state st+1 according to the state transition dynamics, add it a zero-mean Gaussian noise with
variance σ2Ins, and generate corresponding observation xt+1.To ensure that the observation-action
data is uniformly distributed (see Section 3), we sample the state-action pair (st , ut) uniformly from
the state-action space. To understand the robustness of each model, we consider both deterministic
(σ = 0) and stochastic scenarios. In the stochastic case, we add noise to the system with different
values of σ and evaluate the models’ performance under various degree of noise.
Each task has underlying start and goal states that are unobservable to the algorithms, instead, the
algorithms have access to the corresponding start and goal observations. We apply control using
the iLQR algorithm (see Appendix B), with the same cost function that was used by RCE and E2C,
namely, C(Zt Jut) = (Zt - ZgoaI)TQ(Zt - ZgoaI) + ut Rut, and C(ZT) = (ZT - ZgoaI)TQ(ZT - ZgoaI),
where Zgoal is obtained by encoding the goal observation, and Q = K ∙ Inz, R = Inu9. Details of our
implementations are specified in Appendix D.3. We report performance in the underlying system,
specifically the percentage of time spent in the goal region10.
A Reproducible Experimental Pipeline In order to measure performance reproducibility, we
perform the following 2-step pipeline. For each control task and algorithm, we (1) train 10 models
7For the RCE implementation, we directly optimize the ELBO loss in Equation (16) of the paper. We also
tried the approach reported in the paper on increasing the weights of the two middle terms and then annealing
them to 1. However, in practice this method is sensitive to annealing schedule and has convergence issues.
8See a control demo on the TORCS simulator at https://youtu.be/GBrgALRZ2fw
9According to the definition of latent cost c(z, U) = D ◦ c(z, u), its quadratic approximation is given by
c(z,u) ≈ Z Ugoal ψU D ° C|z = zgoal，u=0 + 2 Z Ugoal Ozz 卷Zu] D ° C|z=zgoal,u=0
Yet for simplicity, we choose the same latent cost as in RCE and E2C with fixed, tunable matrices Q and R.
10Another possible metric is the average distance to goal, which has a similar behavior.
z - zgoal
U
7
Published as a conference paper at ICLR 2020
Table 1: Percentage of steps in goal state. Averaged over all models (left), and the best model (right).
Domain	RCE (all)	E2C (all)	PCC (all)	RCE (top 1)	E2C (top 1)	PCC (top 1)
Planar	2.1 ± 0.8 =	5.5 ± 1.7 =	35.7 ± 3.4=	9.2 ± 1.4 =	36.5 ± 3.6=	72.1 ± 0.4=
Pendulum	24.7 ± 3.1	46.8 ± 4.1	58.7 ± 3.7	68.8 ± 2.2	89.7 ± 0.5	90.3 ± 0.4
CartPole	59.5 ± 4.1	7.3 ± 1.5	54.3 ± 3.9	99.45 ± 0.1	40.2 ± 3.2	93.9 ± 1.7
3-link	-1.1 ± 0.4	4.7 ± 1.1-	18.8 ± 2.1	10.6 ± 0.8	20.9 ± 08	47.2 ± 1.7
TORCS	27.4 ± 1.8 一	28.2 ± 1.9 -	60.7 ± 1.1 -	39.9 ± 2.2 一	54.1 ± 2.3 -	68.6 ± 0.4 一
Table 2: Ablation analysis. Percentage of steps spent in goal state. From left to right: PCC including
all loss terms, excluding consistency loss, excluding curvature loss, amortizing the curvature loss.
Domain	PCC	PCC no Con	PCC no CUr	PCC Amor
Planar	35.7 ± 3.4	0.0 ± 0.0	29.6 ± 3.5	41.7 ± 3.7
Pendulum	58.7 ± 3.7	52.3 ± 3.5	50.3 ± 3.3	54.2 ± 3.1
Cartpole	54.3 ± 3.9	5.1 ± 0.4	17.4 ± 1.6	14.3 ± 1.2
3-link	18.8 ± 2.1	9.1 ± 1.5	13.1 ± 1.9	11.5 ± 1.8
independently, and (2) solve 10 control tasks per model (we do not cherry-pick, but instead perform
a total of 10 × 10 = 100 control tasks). We report statistics averaged over all the tasks (in addition,
we report the best performing model averaged over its 10 tasks). By adopting a principled and
statistically reliable evaluation pipeline, we also address a pitfall of the compared baselines where the
best model needs to be cherry picked, and training variance was not reported.
Results Table 1 shows how PCC outperforms the baseline algorithms in the noiseless dynamics case
by comparing means and standard deviations of the means on the different control tasks (for the case of
added noise to the dynamics, which exhibits similar behavior, refer to Appendix E.1). It is important
to note that for each algorithm, the performance metric averaged over all models is drastically different
than that of the best model, which justifies our rationale behind using the reproducible evaluation
pipeline and avoid cherry-picking when reporting. Figure 2 depicts 2 instances (randomly chosen
from the 10 trained models) of the learned latent space representations on the noiseless dynamics of
Planar and Inverted Pendulum tasks for PCC, RCE, and E2C models (additional representations can
be found in Appendix E.2). Representations were generated by encoding observations corresponding
to a uniform grid over the state space. Generally, PCC has a more interpretable representation of
both Planar and Inverted Pendulum Systems than other baselines for both the noiseless dynamics
case and the noisy case. Finally, in terms of computation, PCC demonstrates faster training with 64%
improvement over RCE, and 2% improvement over E2C.11
Ablation Analysis On top of comparing the performance of PCC to the baselines, in order to
understand the importance of each component in (PCC-LOSS), we also perform an ablation analysis
on the consistency loss (with/without consistency loss) and the curvature loss (with/without curvature
loss, and with/without amortization of the Jacobian terms). Table 2 shows the ablation analysis
of PCC on the aforementioned tasks. From the numerical results, one can clearly see that when
consistency loss is omitted, the control performance degrades. This corroborates with the theoretical
results in Section 3.2, which indicates the relationship of the consistency loss and the estimation error
between the next-latent dynamics prediction and the next-latent encoding. This further implies that
as the consistency term vanishes, the gap between control objective function and the model training
loss is widened, due to the accumulation of state estimation error. The control performance also
decreases when one removes the curvature loss. This is mainly attributed to the error between the
iLQR control algorithm and (SOC2). Although the latent state dynamics model is parameterized
with neural networks, which are smooth, without enforcing the curvature loss term the norm of
the Hessian (curvature) might still be high. This also confirms with the analysis in Section 3.3
about sub-optimality performance and curvature of latent dynamics. Finally, we observe that the
performance of models trained without amortized curvature loss are slightly better than with their
amortized counterpart, however, since the amortized curvature loss does not require computing
gradient of the latent dynamics (which means that in stochastic optimization one does not need to
estimate its Hessian), we observe relative speed-ups in model training with the amortized version
(speed-up of 6%, 9%, and 15% for Planar System, Inverted Pendulum, and Cartpole, respectively).
7	Conclusion
In this paper, we argue from first principles that learning a latent representation for control should be
guided by good prediction in the observation space and consistency between latent transition and
11Comparison jobs were deployed on the Planar system using Nvidia TITAN Xp GPU.
8
Published as a conference paper at ICLR 2020
the embedded observations. Furthermore, if variants of iterative LQR are used as the controller, the
low-curvature dynamics is desirable. All three elements of our PCC models are critical to the stability
of model training and the performance of the in-latent-space controller. We hypothesize that each
particular choice of controller will exert different requirement for the learned dynamics. A future
direction is to identify and investigate the additional bias for learning an effective embedding and
latent dynamics for other type of model-based control and planning methods.
References
E. Banijamali, R. Shu, M. Ghavamzadeh, H. Bui, and A. Ghodsi. Robust locally-linear controllable
embedding. In Proceedings of the Twenty First International Conference on Artificial Intelligence
and Statistics, pp.1751-1759, 2018.
Dimitri Bertsekas. Dynamic programming and optimal control, volume 1. Athena scientific, 1995.
Francesco Borrelli, Alberto Bemporad, and Manfred Morari. Predictive control for linear and hybrid
systems. Cambridge University Press, 2017.
Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.
Morten Breivik and Thor I Fossen. Principles of guidance-based path following in 2d and 3d. In
Proceedings of the 44th IEEE Conference on Decision and Control, pp. 627-634. IEEE, 2005.
Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement
learning in a handful of trials using probabilistic dynamics models. In Advances in Neural
Information Processing Systems, pp. 4754-4765, 2018.
Roy De Maesschalck, DeIPhine JoUan-Rimbaud, and Desire L Massart. The mahalanobis distance.
Chemometrics and intelligent laboratory systems, 50(1):1-18, 2000.
Marc Deisenroth and Carl E Rasmussen. Pilco: A model-based and data-efficient approach to policy
search. In Proceedings of the 28th International Conference on machine learning (ICML-11), pp.
465-472, 2011.
Frederik Ebert, Chelsea Finn, Sudeep Dasari, Annie Xie, Alex Lee, and Sergey Levine. Visual
foresight: Model-based deep reinforcement learning for vision-based robotic control. arXiv
preprint arXiv:1812.00568, 2018.
Bernard Espiau, FrangoiS Chaumette, and Patrick Rives. A new approach to visual servoing in
robotics. ieee Transactions on Robotics and Automation, 8(3):313-326, 1992.
Chelsea Finn, Xin Yu Tan, Yan Duan, Trevor Darrell, Sergey Levine, and Pieter Abbeel. Deep spatial
autoencoders for visuomotor learning. In 2016 IEEE International Conference on Robotics and
Automation (ICRA), pp. 512-519. IEEE, 2016.
Katsuhisa Furuta, Masaki Yamakita, and Seiichi Kobayashi. Swing up control of inverted pendulum.
In Proceedings IECON’91: 1991 International Conference on Industrial Electronics, Control and
Instrumentation, pp. 2193-2198. IEEE, 1991.
Yarin Gal, Rowan McAllister, and Carl Edward Rasmussen. Improving pilco with bayesian neural
network dynamics models. In Data-Efficient Machine Learning workshop, ICML, volume 4, 2016.
Shlomo Geva and Joaquin Sitte. A cartpole experiment benchmark for trainable controllers. IEEE
Control Systems Magazine, 13(5):40-51, 1993.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016.
David Ha and Jurgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018.
Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James
Davidson. Learning latent dynamics for planning from pixels. arXiv preprint arXiv:1811.04551,
2018.
Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H Campbell, Konrad
Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, et al. Model-based
reinforcement learning for atari. arXiv preprint arXiv:1903.00374, 2019.
9
Published as a conference paper at ICLR 2020
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Thanard Kurutach, Aviv Tamar, Ge Yang, Stuart J Russell, and Pieter Abbeel. Learning plannable
representations with causal infogan. In Advances in Neural Information Processing Systems, pp.
8733-8744, 2018.
Xuzhi Lai, Ancai Zhang, Min Wu, and Jinhua She. Singularity-avoiding swing-up control for under-
actuated three-link gymnast robot using virtual coupling between control torques. International
Journal of Robust and Nonlinear Control, 25(2):207-221, 2015.
Weiwei Li and Emanuel Todorov. Iterative linear quadratic regulator design for nonlinear biological
movement systems. In ICINCO (1), pp. 222-229, 2004.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan
Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602, 2013.
Kevin P Murphy. Machine learning: a probabilistic perspective. MIT press, 2012.
Erik Ordentlich and Marcelo J Weinberger. A distribution dependent refinement of pinsker’s inequality.
IEEE Transactions on Information Theory, 51(5):1836-1840, 2005.
Marek Petrik, Mohammad Ghavamzadeh, and Yinlam Chow. Safe policy improvement by minimizing
robust baseline regret. In Advances in Neural Information Processing Systems, pp. 2298-2306,
2016.
James Blake Rawlings and David Q Mayne. Model predictive control: Theory and design. Nob Hill
Pub. Madison, Wisconsin, 2009.
Alexander Shapiro, Darinka Dentcheva, and Andrzej RUszczynski. Lectures on Stochastic Program-
ming: modeling and theory. SIAM, 2009.
Mark W Spong. The swing Up control problem for the acrobot. IEEE control systems magazine, 15
(1):49-55, 1995.
ManUel Watter, Jost Springenberg, Joschka Boedecker, and Martin Riedmiller. Embed to control:
A locally linear latent dynamics model for control from raw images. In Advances in neural
information Processing systems, pp. 2746-2754, 2015.
Bernhard Wymann, Eric EsPia Christophe Guionneau, Christos Dimitrakakis, Remi Coulom, and An-
drew SUmner. Torcs, the open racing car simUlator. Software available at httP://torcs. sourceforge.
net, 4(6), 2000.
Marvin Zhang, Sharad Vikram, Laura Smith, Pieter Abbeel, Matthew J Johnson, and Sergey Levine.
Solar: Deep structured latent representations for model-based reinforcement learning. In Proceed-
ings of the 36th International Conference on Machine Learning, 2019.
10
Published as a conference paper at ICLR 2020
A	Technical Proofs of Section 3
A.1 Proof of Lemma 1
Following analogous derivations of Lemma 11 in Petrik et al. (2016) (for the case of finite-horizon
MDPs), for the case of finite-horizon MDPs, one has the following chain of inequalities for any given
control sequence {ut }tT=-01 and initial observation x0 :
. . ^ . ..
|L(U, Pb, x0) -L(U,P,x0)|
E cT
T-1
X
ct(xt,Ut) IP,x0
t=0
T-1
- E cT(xT)+	ct(xt,Ut) IP, x0
t=0
≤T2 ∙ cmax E
1 T-1
T E DTV(P(Ixt,Ut)IIP(Ixt,Ut)) | P,xo
T t=0
≤ 2TT2 ∙ Cmax E
TT X JKL(P(Ixt, Ut)IIP(Ixt, Ut)) | P, xo
T t=0
r1 T T	ʌ	■
≤ √2T 2
• cmax t E [t	kl(p(∙∣xt,ut)I∣P(∙∣xt,ut)) I P,xo
where DTV is the total variation distance of two distributions. The first inequality is based on the
result of the above lemma, the second inequality is based on Pinsker’s inequality (Ordentlich &
Weinberger, 2005), and the third inequality is based on Jensen's inequality (Boyd & Vandenberghe,
2004) of p∕(∙) function.
Now consider the expected cumulative KL cost: E，PT-1 KL(P(∙Ixt,ut)IIP(∙Ixt,ut)) I P,xo]
with respect to some arbitrary control action sequence {Ut}tT=-01. Notice that this arbitrary action
sequence can always be expressed in form of deterministic policy Ut = π0(xt, t) with some non-
stationary state-action mapping π0 . Therefore, this KL cost can be written as:
=E
=E
1 T-1
t EKL(P (Ixt,ut)“P(Ixt,ut)) i P,π,xo
t=0
1 T-1
T x
T t=0 ut∈U
1 T-1
TX
T t=0 ut∈U
KL(P(∙Ixt, ut)IIPb(∙Ixt, ut))d∏0(utIxt,t) I P,xo
KL(P(∙Ixt, ut)IIPb(∙Ixt, ut)) ∙
dπ0(UtIxt, t)
dU(Ut)
• dU (Ut ) I P, x0
(10)
E
≤U • Eχ,u [kl(P(∙Ix,u)IIP(∙Ix,u))],
where the expectation is taken over the state-action occupation measure TT PT-1 P(xt = x,ut =
UIx0, U) of the finite-horizon problem that is induced by data-sampling policy U. The last inequality
is due to change of measures in policy, and the last inequality is due to the facts that (i) π is a
deterministic policy, (ii) dU(Ut) is a sampling policy with lebesgue measure 1/U over
actions, (iii) the following bounds for importance sampling factor holds:
all control
U.
I dπ0(ut∣xt,t) I V
I	dU (ut)	J —
To conclude the first part of the proof, combining all the above arguments we have the following
inequality for any model P and control sequence U:
IL(U,P,xo)- L(U,P,xo)I ≤√2T2 •
cmaxU ∙ .,U [kL(P(
•Ix,u)IiP([x,u))].
(11)
11
Published as a conference paper at ICLR 2020
τ->	1	, ∕'	C	∙ 1	1 , ∙	C /C 八八 c∖	1 ∕rr* A+∖ TT ∙ ,ι
For the second part of the proof, consider the solution of (SOC3), namely (U3,P3'). Using the
optimality condition of this problem one obtains the following inequality:
L(U3,Pb3,X0) + √2T2 ∙ CmaxU
≤L(U^,Pb3,Xθ) + √2T2 ∙ CmaxU
• Je-,u [κL(P(∙∣χ,u)∣陶(∙∣χ,u))]
• ^Eχ,u [κL(P(∙∣x,u)∣b⅞(∙∣x,u))i.
(12)
Using the results in (11) and (12), one can then show the following chain of inequalities:
L(U1,P, C, X0)≥L(U1,P3,C, xo) - √2T2 • CmaxU ∙ .Eχ,u [kL(P(∙∣X,u)∣∣P式∙∣X,u))i
=L(U1,P3,c, xo) + √2T2 • CmaxU ∙ .Eχ,u [kL(P(∙∣x,u)∣∣P式∙∣x,u))i
-2√2T2 • CmaxU . ∖/Eχ,u IKL(P(∙∣x,u)∣∣P¾(∙∣x,u))]
V	(------------------------------ (13)
≥L(U3,P3,C,X0) + √2T2 • CmaxU ∙ yEχ,u ∣KL(P(∙∣X,u)∣∣Pb3 (∙∣X,u))i
-2√2T2 • CmaxU • ∖∣Kχ,u ∣KL(P(∙∣x,u)l∣P¾(∙∣x,u))]
≥L(U3,P, C, X0)- 2√2T2 • CmaxU • 4^Eχ,u ∣KL(P(∙∣x,u)∣∣P¾(∙∣x,u))],
where U； is the optimizer of (SOC1) and (Ug,Pg) is the optimizer of (SOC3).
Therefore by letting λ3 = √2T2 • CmaxU and R3(P)
Ex,u [KL(P(∙∣x,u川P(∙∣x,u))] and by
combining all of the above arguments, the proof of the above lemma is completed.
A.2 Proof of Lemma 2
For the first part of the proof, at any time-step t ≥ 1, for any arbitrary control action sequence
{ut}tT=-01, and any model Pb, consider the following decomposition of the expected cost :
t-1
E[c(Xt , ut) ∖ Pb,X0] =	ɪɪ dP(Xk∖Xk-1,Uk-1)∙
x0:t-1 ∈Xt k=1
/ /
zt ∈Z	zt0-1
dE(zt0-1|xt-1)F (zt|zt0-1, ut-1)	dD(xt|zt)c(xt, ut) .
^≡^{^^^^^^^^^^^≡
dG(zt |xt-1,ut-1)
}|
^^≡{^^^≡
c(zt ,ut)

■X T	∙ 1 .λ r 11	,	J	τm Γ /	∖ . /	∖ I	Ir , . rʌ ɪ τ ∙ .λ
Now consider the following cost function: E[C(xt-1 , ut-1 ) + C(xt, ut) | P, x0] for t > 2. Using the
above arguments, one can express this cost as
E[c(xt-1, ut-1) + c(xt, ut) | Pb, x0]
/
x0:t-2 ∈X t-1
t-2
ɪɪdP(Xk∖xk-1,uk-1) ∙ /	dE(zt-2∖xt-2) ∙ /	dF(zt-ι∖z0-2,Ut-2')
k=1	zt0-2∈Z	zt-1∈Z
c(zt-1,ut-1) + /	dD(Xt-ι∖zt-ι) /	dE(zt-1∖Xt-1)dF(zt∖zt-ι,ut-ι)c(zt,ut)
xt-1 ∈X	zt0-1 ,zt ∈Z
≤
x0:t-2 ∈X t-1
Zzt-1
t-2
dPb(xk |xk-1,Uk-ι) ∙ /	dE(zt-2∣Xt-2)∙
k=1	zt-2∈Z
dF(zt-1∣zt-2,ut-2) ( c(zt-1,ut-1) + /	dF(zt∖zt-1,ut-1)C(zt,ut)
zt∈Z
+ CmaX ∙ /	ɪɪdP(Xk ∖Xk-1,Uk-l) ∙ DTV ( /	dP(x0 ∖ Xt-2 ,Ut-2) E (∙∖ X0) ∖∖ /	dE(z∖xt-2)F (∙∖z,ut-2)
x0:t-2 ∈X t-1 k=1	x0∈X	z∈Z
12
Published as a conference paper at ICLR 2020
By continuing the above expansion, one can show that
∣E [L(U,F,c, zo) | E,xo] - L(U,P,c,xo)∣
1 T-1
≤T 2 ∙Cmax E T EDTV ((E ◦ P)(∙∣ Xt ,ut) || (F ◦ E)(∙∣xt ,ut)) | P,X0
Γ1 τ T l--------------------------------------
≤√2t2 ∙ Cmax E T E √KL((E ◦ P)(∙∣xt,ut)∣∣(F ◦ E)(∙∣xt,ut)) | P,X0
t=0
≤√2T2 ∙ Cmax
1 T-1
E T EKL((E ◦ P)(∙∣xt,ut )||(F ◦ E)(∙∣xt,ut)) | P,xo
t=0
where the last inequality is based on Jensen's inequality of a∕(∙) function.
For the second part of the proof, following similar arguments as in the second part of the proof of
Lemma 1, one can show the following chain of inequalities for solution of (SOC3) and (SOC2):
L(U3,P3,c,xo)
≥E [L(U3,,F3,,c,	zo)	|	E3,x0]	- √2T2	∙	CmaxU ∙	,Eχ,u [kL((E3	◦ J)(∙∣x,u)∣∣(球◦	EQ(∙∣x,u))]
=E [L(U*,F3,,c,	Z0)	|	E3,x0]	+ √2T2	∙	CmaxU ∙	^Eχ,u [kL((E3	◦ J⅜)(∙∣x,u)∣∣(碟◦	E30(∙∣x,u))]
-2√2T2 ∙ CmaxU ∙ jEχ,u IKL((Em ◦ Pf)(∙∣x,u 川(F3i ◦ E30(∙∣x,u))]
≥E [L(U∣,F2,,C, zo) | E2,xo] + √2T2 ∙ CmaxU ∙ JEx,u [kL((E$ ◦ Pf)(∙∣x,u)∣∣(Ff ◦ ES)(∙∣x,u))]
-2√2T2 ∙ CmaxU ∙ JEx,u ∣KL((E3: ◦ P^)(∙∣x,u川(F3i。E30(∙∣x,u))]
≥L(U2 , P2 , c, x0)- 2 √2t 2 .〜CmaxU ∙ JEx,u hKL((E2i。p2 )(1x, u)|| (F2。Emx, u))i ,
λ2	'----------------------{z--------------------}
...^ .
R20(P*)
(14)
where the first and third inequalities are based on the first part of this Lemma, and the second
inequality is based on the optimality condition of problem (SOC2). This completes the proof.
A.3 Proof of Corollary 1
To start with, the total-variation distance DTV (Rx0∈χ dPb(X∖x, u)E(∙∣x0)∣∣(F。E)(∙∣x, U)) can be
bounded by the following inequality using triangle inequality:
Dtv ( [	dPb(X∖x, u)E(∙∖x0)∖∖(F。E)(∙∖x,u))
x0∈X
≤Dtv ( [	dP(x0∖x,u)E(∙∖x0)∖∖(F。E)(∙∖x,u))+ Dtv ( [	dP(x0∖x, u)E(∙∖x0)∖∖ [	dPb(x0∖x, u)E(∙∖x0)
x0∈X	x0∈X	x0∈X
≤Dtv (/	dP(x0∖x, u)E(∙∖x0)∖∖(F。E)(∙∖x,u)) + DTV (P(∙∖x, u)∖∖Pb(∙∖x, U))
where the second inequality follows from the convexity property of the DTV-norm (w.r.t. convex
weights E(∙∖x0), ∀x0). Then by Pinsker,s inequality, one obtains the following inequality:
Dtv ( /	dPb(x0∖x, u)E(∙∖x0)∖∖(F。E)(∙∖x, u)
x0∈X
≤ j2KL (Z	dP(x0∖x, U)E(∙∖x0)∖∖(F。E)(∙∖x,u)
(15)
13
Published as a conference paper at ICLR 2020
We now analyze the batch consistency regularizer:
* * . ^.	- ,.... ... ..,
R2z(Pb) = Eχ,u,χ0 [KL(E(∙∣x0)∣∣(F ◦ E)(∙∣x,u))]
and connect it with the inequality in (15). Using Jensen’s inequality of convex function x log x, for
any observation-action pair (x, u) sampled from Uτ, one can show that
dP (x0|x, u)	dE(z0|x0) log	dP (x0|x, u)E(z0|x0)
x0∈X	z0∈Z	x0∈X
≤	dP(x0|x,u)
x0∈X	z0∈Z
dE(z0|x0) log (E(z0|x0)) .
Therefore, for any observation-control pair (x, u) the following inequality holds:
KL ( /	dP(x0∣x, u)E(∙∣x0) ||(F ◦ E)(∙∣x,u)
x0∈X
=	dP(x0|x, u)	dE(z0|x0) log	dP(x0 |x, u)E(z0 |x0)
x0∈X	z0∈Z	x0∈X
-	dP(x0 |x, u) log (g(x0 |x, u))
x0∈X
≤	dP(x0|x, u)	dE(z0|x0) log (E(z0|x0)) -	dP (x0 |x, u) log (g(x0 |x, u))
x0∈X	z0∈Z	x0∈X
=KL(E(∙∣x0)∣∣(F ◦ E)(∙∣x,u))
By taking expectation over (x, u) one can show that
Ex,u
KL(
x0∈X
dP(x0∣x, u)E(∙∣x0)∣∣(F ◦ E)(∙∣x,u))
(16)
(17)
is the lower bound of the batch consistency regularizer. Therefore, the above arguments imply that
DTVeZ XdP(XIx,u)E(∙∣x0)ll(F◦ E)(∙∣x,u)j ≤√/R2'(P)+ R3(P).	(18)
The inequality is based on the property that √a + V≤ ≤ √2√α + b.
Equipped with the above additional results, the rest of the proof on the performance bound
follows directly from the results from Lemma 2, in which here we further upper-bound
DTV (jχo∈χ dP(χ0∣χ, u)E(∙∣χ0)∣∣(F ◦ E)(∙∣χ, u)), when P = P$.
A.4 Proof of Lemma 3
For the first part of the proof, at any time-step t ≥ 1, for any arbitrary control action sequence
{ut }tT=-01 and for any model Pb, consider the following decomposition of the expected cost :
E[c(Xt, ut) | P, X0]
=CmaX ∙ I
x0:t-
1∈Xt
t-1
ɪɪ dP(Xk ∣Xk-1,Uk-1)DTV(P(∙∣χt-1,ut-1)∣∣P(∙∣χt-1,ut-1))
k=1
+
X0：t-1∈Xt
t-1
dP(Xk|Xk-1,uk-1)
k=1	zt∈Z
z0
zt-1
dE(zt0-1|Xt-1)F (zt|zt0-1, ut-1)	dD(Xt|zt)c(Xt, ut) .
^™^{^^^^^^^^^^
dG(zt |xt-1,ut-1)
}|
{z"^^^^^
c(zt,ut)
}
14
Published as a conference paper at ICLR 2020
■X T	∙ 1 .λ r 11	,	J τm Γ /	∖	.	/	∖ I	Ir , . rʌ ɪ τ ∙ .λ
Now consider the following cost function: E[c(xt-1,ut-1) + C(Xt, Ut) | P, xo] for t > 2. Using the
above arguments, one can express this cost as
E[c(xt-1,ut-1) + c(xt,ut) ∣ P,x0]
t-2
=/	ɪɪdP(Xk∣xk-1,uk-1) ∙ /	dE(z0-2∣xt-2) ∙ /	dF(zt-ι∣z0-2,ut-2)∙
Jx0ιt-2∈Xt-1 k = 1	JZt-2∈z	JZt-I
c(zt-1,ut-1) + /	dD(xt-1∣Zt-1) /
,	Jxt-I	z0-ι
dE(zt-1∖xt-1 )dF (zt∣z°-ι,ut-ι)c(zt,ut)
,zt∈Z
2 t	t-j
+ cmax ∙ T j .	ɪɪ dP(xk∣Xk-1,Uk-1)DTV(P(∙∖xt-j,ut-j)∣∣P(∙∣xt-j,ut-j))
≤/
,χ0H-2∈Xt 1
L
t-2	r
ɪɪ dP(xk∣Xk-1,Uk-1) ∙ /	dE(zt-2∣xt-2)∙
k=ι	zt-2∈Z
dF(zt-1∣zt-2,ut-2) ( c(zt-1,ut-1) + /	dF(zt∣zt-1,ut-1)&zt,ut)
zt∈Z	.
2	t	t-j
+ cmax ∙Ej ∙	∏dP(xk ∣Xk-1,Uk-1)DTV(P(∙∣xt-j,ut-j )∣∣P(∙∣xt-j,ut-j))
j=1	x0：t-j k=1
+ cmax ∙ I	ɪɪ dP(xk∣Xk-1,Uk-1) ∙ Dtv ( /	dP(xt∣xt-2,ut-2)E(∙∣xt)∣∣ /	dE(z∖xt-2)F(∙∣z,ut-2)).
x0:t-2∈Xt- 1 k=1	x0∈X	z∈Z
Continuing the above expansion, one can show that
∣E [L(U, F,c, z0) ∣ E, x0] - L(U, P, x0)∣
≤T 2
cmax
E
‘r T-1	C
于 X DTV(P(∙∣xt,ut)∣∣P(∙∣xt,ut)) + Dtv( /	dP(xt∣xt
T t=0	xt∈X
,ut)E(∙∣xt)∣∣(F。E)(∙∣xt,ut)) ∣ P,x0
≤√2T2 ∙ Cmax E
≤√2T2 ∙ Cmax E
T X JkL(P(∙∣xt,ut)∣∣P(∙∣xt,ut)) + ：KL(/,dP(xt∣xt,ut)E(∙∣xt)∣∣(F:E)(∙∣xt,ut)) ∣ P,x0
1 t-1	]----------------------
T £ JKL(P(∙∣χt,ut)∣∣p(∙∣χt,Ut))
t=0
+ JKL(P(∙∣xt,ut)∣∣P(∙∣xt,ut)) + KL(E(∙∣xt+ι)∣∣(F ◦ E)(∙∣xt,ut)) ∣ P,x0
≤2T 2
Cmax
\
E
1 T-1	ʌ	-
T £ 3KL(P(∙∣xt,ut)∣∣P(∙∣xt,ut))+2KL(E(∙∣xt+ι)∣∣(F。E)(∙∣xt,ut)) ∣ P,x0
t=0
where the last inequality is based on the fact that √α + Vb ≤ √2√α + b and is based on Jensen’s
inequality of -∖∕(∙) function.
For the second part of the proof, following similar arguments from Lemma 2, one can show the
following inequality for the solution of (SOC3) and (SOC2):
L(U1 ,P,C, x0) ≥e [L(U:, F2 ,c,z0) 1 E2 , x0] - √2T2 , cmaxU , ʌ/2RS(P2 ) + 3R3(P2 )
=E [L(U,F2,c, Z0)| E2,X0] + √2T2 ∙ CmaXU ∙ ,2R3(P2) + 3R3(P2)
-2√2T2 ∙ CmaxU ∙ ,2R2'(P2 ) + 3R3(P2)
≥E[L(U2 ,F2 ,c, z0) | E2 ,X0] + √2T2 ∙ CmaxU ∙ ,2R2,(P2) + 3R3(Pf) (19)
-2√2T2 ∙ CmaxU ∙ ,2R2'(P2) + 3R3(P2)
≥L(U2,P,C,X0)- 2 √2T2 ∙ CmaXU ∙ ,2R2'(P2 )+3R3(P2 ),
'
入2
15
Published as a conference paper at ICLR 2020
where the first and third inequalities are based on the first part of this Lemma, and the second
inequality is based on the optimality condition of problem (SOC2). This completes the proof.
A.5 Proof of Lemma 4
A Recap of the Result: Let (ULLc, PLLC) be a LLC solution to (SOC-LLC) and U； be a So-
lution to (SOC1). Suppose the nominal latent state-action pair {(zt, ut)}tT=-01 satisfies the con-
dition: (zt,ut)〜 N ((z2；,t, u2；,t), δ2I), where {(z2；,t,u；2,t}tT=-01 is the optimal trajectory of prob-
lem (SOC2). Then with probability 1 - η, we have L(U；,P,c, xo) ≥ L(ULLC, P, c, xo)-
q
R2(PbL；LC)+RLLC(PbL；LC) .
Discussions of the effect of δ on LLC Performance: The result of this lemma shows that when
the nominal state and actions are δ-close to the optimal trajectory of (SOC2), i.e., at each time step
(zt, ut) is a sample from the Gaussian distribution centered at (z2；,t, u2；,t) with standard deviation δ,
then one can obtain a performance bound of LLC algorithm that is in terms of the regularization loss
RLLC. To quantify the above condition, one can use Mahalanobis distance (De Maesschalck et al.,
2000) to measure the distance of (zt, ut) to distribution N((z2；,t, u2；,t), δ2I), i.e., we want to check
for the condition:
Il(Zt,Ut) - (z2,t,u2,t)k V 0 ∀t
δ
for any arbitrary error tolerance 0 > 0. While we cannot verify the condition without knowing the
optimal trajectory {(z22,t, u22,t)}tT=-o1, the above condition still offers some insights in choosing the
parameter δ based on the trade-off of designing nominal trajectory {(zt, ut)}tT=-o1 and optimizing
RLLC. When δ is large, the low-curvature regularization imposed by the RLLC regularizer will cover a
large portion of the state-action space. In the extreme case when δ → ∞, RLLC can be viewed as a
regularizer that enforces global linearity. Here the trade-off is that the loss RLLC is generally higher,
which in turn degrades the performance bound of the LLC control algorithm in Lemma 4. On the
other hand, when δ is small the low-curvature regularization in RLLC only covers a smaller region of
the latent state-action space, and thus the loss associated with this term is generally lower (which
provides a tighter performance bound in Lemma 4). However the performance result will only hold
when (zt,ut) happens to be close to (z22,t, u22,t) at each time-step t ∈ {0, . . . , T - 1}.
Proof: For simplicity, we will focus on analyzing the noiseless case when the dynamics is deter-
ministic (i.e., Σw = 0). Extending the following analysis for the case of non-deterministic dynamics
should be straight-forward.
First, consider any arbitrary latent state-action pair (z, u), such that the corresponding nominal
state-action pair (z, u) is constructed by z = z - δz, u = u - δu, where (δz, δu) is sampled from the
Gaussian distribution N (0, δ2I). (The random vectors are denoted as (δz0, δu0)) By the two-tailed
Bernstein’s inequality (Murphy, 2012), for any arbitrarily given η ∈ (0, 1] one has the following
inequality with probability 1 - η :
|fZ(z,u) + A(z, u)δz + B(z, u)δu - fZ(z, u)|
≤P2log(2∕η) JV(δz0,δu0)〜N(0,δ2i)[fz(z,u) + A(z,u)δz0 + B(z,u)δu0 - fz(z,u)]
+ ∣E(δz0,δu0)〜N(o,δ2i)[fz(z,u) + A(z,u)δz0 + B(z,u)δu0 - fz(z,u)]∣
1/2
≤(1 + √2log(2∕η)) (E(δz0,δu0)〜N(0,δ2i) [kfz(z,u) + A(z,u)δz0 + B(z,u)δu0 - fz(z,u)∣2] J
RLLC(Pb|z,u)
The second inequality is due to the basic fact that variance is less than second-order moment of a
random variable. On the other hand, at each time step t ∈ {0, . . . , T - 1} by the Lipschitz property of
the immediate cost, the value function Vt (z)
minUt：T-i E 卜T(ZT) + PT-t1 cτ(zτ, UT) | Zt = z]
is also Lipchitz with constant (T - t + 1)clip. Using the Lipschitz property of Vt+1, for any (z, u)
16
Published as a conference paper at ICLR 2020
and (δz, δu), such that (z, u) = (z - δz, u - δu), one has the following property:
|Vt+1(z0 + A(z, u)δz + B(z, u)δu) - Vt+1(fZ(z,u))|
≤(T - t)ciip ∙ |fz(z,u) + A(z,u)δz + B(z,u)δu - fz(z,u)∣,
(20)
Therefore, at any arbitrary state-action pair (Z, U), for Z = Z - δz, and U = U - δu with Gaussian
sample (δz, δu)〜N(0, δ2I), the following inequality on the value function holds w.p. 1 - η:
Vt+ι(fz(Z,U)) ≥ Vt+ι(z0 + A(z,u)δz + B(z,u)δu) - (T - t)chp(1 + p2log(2∕η)) ∙ JRLLC(P|Z, U),
which further implies
ct(Z, U) + Vt+ι(fz(Z, U))
≥ct(Z, U) + Vt+ι(Z0 + A(Z,u)δZ + B(Z,u)δu) - (T - t)chp(1 +，2 log(2∕η)) ∙ JRLLC(P|Z,U),
Now let U* be the optimal control w.r.t. Bellman operator Tt[Vt+ι](Z) at any latent state Z. Based on
the assumption of this lemma, at each state Z the nominal latent state-action pair (z, u) is generated
by perturbing (Z, U*) with Gaussian sample (δZ, δu)〜N(0, δ2I) that is in form of Z = Z - δZ,
U = U - δu. Then by the above arguments the following chain of inequalities holds w.p. 1 - η:
Tt[Vt+ι](Z) ：=minCt(Z,U) + Vt+ι(fz(Z,U))
U
=ct(Z,U*) + Vt+ι(fz (Z,U*))
≥ct(Z, U*) + Vt+ι(fz(z,u) + A(z,u)δz + B(z,u)δu)
-	lVt+1(z0 + A(Z,u)δz + B(Z,u)δU)- Vt+1(fZ(z, u*))1
≥Ct(Z,u + δu) + Vt+ι(fz (z,u) + A(z,u)δz + B(z,u)δu)	(
-	(T - t)clip(1 + P2 log(2∕η)) ,max RLLC(P|z, U)
≥ minCt(Z,u + δu) + Vt+ι(fz(z, u) + A(z,u)δz + B(z,u)δu)
δu
(21)
-	(T - t)ciip(1 + P2 log(2∕η)) ,max RLLC(P|z, U)
Recall the LLC loss function is given by
RLLC(Pb) = Ex,u hE hRLLC(Pb|Z, U) | Zi | Ei .
Also consider the Bellman operator w.r.t. latent SOC: Tt[V](z) = min。Ct(z, u) + V(fz(z, u)), and
the Bellman operator w.r.t. LLC: T⅛,llc[V](z) = minδuCt(z, δu + u) + V(fz(z,u) + A(z,u)δz +
B(Z, U)δU). Utilizing these definitions, the inequality in (21) can be further expressed as
Tt [Vt+1] (Z) ≥Tt,LLCM+l](Z) - (T - t)ClipCmaχ(1 + P2log(2∕η)) pUX JRLLC(P),	(22)
This inequality is due to the fact that all latent states are generated by the encoding observations, i.e.,
z 〜E (∙∣x), and thus by following analogous arguments as in the proof of Lemma 1, one has
maxRLLC(P|z,u) ≤ UXEEχ,u ∣E [Rllc(P∣z,u) [ z] | Ei = UXRLLC(P).
Therefore, based on the dynamic programming result that bounds the difference of value function w.r.t.
different Bellman operators in finite-horizon problems (for example see Theorem 1.3 in Bertsekas
(1995)), the above inequality implies the following bound in the value function, w.p. 1 - η:
min L(U, F, c, z0)
U,Pb
T-1	__________
≥L(ULLC, PLC,c, Z0) - X(T - t) ∙ ClipCmax ∙ T ∙ (1 + /2log(2T∕η)) ∙ √UX ∙ ,Rllc(P*lc)
t=1
≥L(ULlc, PLC,C, zo) - T2 ∙ CIipCmax ∙ (1 + /2log(2T∕η)) ∙ pUX ∙ JRLLC(PLLC).
(23)
17
Published as a conference paper at ICLR 2020
Notice that here We replace η in the result in (22) with n/T. In order to prove (23), We utilize (22) for
each t ∈ {0, . . . , T - 1}, and this replacement is the result of applying the Union Probability bound
(Murphy, 2012) (to ensure (23) holds With probability 1 - η).
Therefore the proof is completed by combining the above result With that in Lemma 3.
18
Published as a conference paper at ICLR 2020
B	The Latent Space iLQR Algorithm
B.1	Planning in the Latent Space (High-Level Description)
We follow the same control scheme as in Banijamali et al. (2018). Namely, we use the iLQR (Li &
Todorov, 2004) solver to plan in the latent space. Given a start observation xstart and a goal observation
xgoal, corresponding to underlying states {sstart, sgoal}, we encode the observations to retrieve zstart
and zgoal. Then, the procedure goes as follows: we initialize a random trajectory (sequence of actions),
feed it to the iLQR solver and apply the first action from the trajectory the solver outputs. We observe
the next observation returned from the system (closed-loop control), and feed the updated trajectory
to the iLQR solver. This procedure continues until the it reaches the end of the problem horizon. We
use a receding window approach, where at every planning step the solver only optimizes for a fixed
length of actions sequence, independent of the problem horizon.
B.2	Details about iLQR in the Latent Space
Consider the latent state SOC problem
T-1
minE CT(ZT) + Ect(Zt,ut) | zo
U	t=0
At each time instance t ∈ {0, . . . , T} the value function of this problem is given by
T-1
VT(z) = cτ(z),	Vt(Z)	= min E	cτ(ZT)	+ Xcτ(zτ,Uτ)	|	Zt	= Z ,	∀t	<	T.	(24)
Ut:T -1
τ=t
Recall that the nonlinear latent space dynamics model is given by:
Zt+1 = F(Zt,ut, Wt)= Fμ (Zt ,ut) + Fσ ∙ wt, wt 〜N (0,I), Vt ≥ 0,	(25)
where Fμ(Zt, Ut) is the deterministic dynamics model and F>F。is the covariance of the latent
dynamics system noise. Notice that the deterministic dynamics model Fμ(Zt, Ut) is smooth, and
therefore the following Jacobian terms are well-posed:
A(z,u):="嚏 U), B(z,u) :=	U), Vz ∈ Rnz, Vu ∈ Rnu.
By the Bellman’s principle of optimality, at each time instance t ∈ {0, . . . , T - 1} the value function
is a solution of the recursive fixed point equation
Vt(Z) = min Qt(Z, U),	(26)
u
where the state-action value function at time-instance t w.r.t. state-action pair (Zt, Ut) = (Z, U) is
given by
Qt(Z, u) = ct(z, u) + Ewt [Vt+1(F(Zt, ut, Wt)) | zt = z, ut = u] .
In the setting of the iLQR algorithm, assume we have access to a trajectory of latent states and actions
that is in form of {(Zt, ut, Zt+1)}tT=-01. At each iteration, the iLQR algorithm has the following steps:
1.	Given a nominal trajectory, find an optimal policy w.r.t. the perturbed latent states
2.	Generate a sequence of optimal perturbed actions that locally improves the cumulative cost
of the given trajectory
3.	Apply the above sequence of actions to the environment and update the nominal trajectory
4.	Repeat the above steps with new nominal trajectory
Denote by δZt = Zt - Zt and δut = ut - ut the deviations of state and control action at time step t
respectively. Assuming that the nominal next state Zt+1 is generated by the deterministic transition
Fμ(zt,ut) at the nominal state and action pair (zt,ut), the first-order Taylor series approximation of
the latent space transition is given by
δZt+ι ：= Zt+1 — Zt+1 = A(Zt,u)δZt + B(Zt,ut)δut + F。∙wt + O(k (δZt, δut)k2), Wt 〜N(0, I).
(27)
19
Published as a conference paper at ICLR 2020
To find a locally optimal control action sequence UJ= = ∏lz t(δ,t) + Ut, ∀t, that improves the
cumulative cost of the trajectory, we compute the locally optimal perturbed policy (policy w.r.t.
perturbed latent state) {πδ=z,t (δZt)}tT=-01 that minimizes the following second-order Taylor series
approximation ofQt around nominal state-action pair (Zt, Ut), ∀t ∈ {0, . . . , T - 1}:
Qt(zt, ut) = Qt(zt, ut) + 2
1>
δzt
δut
Fσ
>Fσ
) tt
Ut,U,U
tt
,tZZ
(Z,z(,zz(,uz
QtQtQt
t, ,u t,
QtQtQ
#
tt
1ZU
δδ
where the first and second order derivatives of the Q-function are given by
Qt,z(zt,Ut) = dct(∂Z,ut) + A(zt,Ut)>Vt+ι,z(zt,Ut),
Qt,u(zt,ut) = dct(∂U,ut) + B(zt,Ut)>Vt+ι,z(zt,Ut),
Qt,zz(zt,Ut) = d Ctdz2,ut) + A(zt,Ut)>Vt+ι,zz(zt,Ut)A(zt,Ut)
Qt,uz(zt,Ut) = d Cdzdzut) + B(zt,ut)>Vt+ι,zz(zt,Ut)A(zt,Ut)
Qt,uu(zt,ut) = d 年2,ut) + B(ζt,ut)>Vt+1,ζζ(zt,ut)B(Zt,ut)
du2
and the first and second order derivatives of the value functions are given by
Vt+1,z (zt, ut) = Ew
Vt+1,zz(zt,Ut)= Ew	2— (F (Zt,ut,W))
Notice that the Q-function approximation Qt in (28) is quadratic and the matrix
Qt,uz (zt , ut )
Qt,uu (zt, ut)
Qt,zz (zt, ut)
Qt,uz(zt, ut)
is positive semi-definite. Therefore the optimal perturbed policy
∏δz,t has the following closed-form solution:
∏δz,t(∙) ∈ arg min Qt(zt,ut) =⇒ ∏δz,t(δzt) = kt(zt,ut) + Kt(zt, ut)δzt,	(29)
where the controller weights are given by
kt (zt, ut ) = - (Qt,uu (zt, ut )) Qt,u (zt , ut) and Kt (zt , ut ) = - (Qt,uu (zt , ut )) Qt,uz (zt , ut).
Furthermore, by putting the optimal solution into the Taylor expansion of the Q-function Qt, we get
Q (Z U ) - Q (Z u ) = 1 1	QiII(Zt,ut) (Qi21(Zt,ut))	1
QAttt)	Qt (Zt,ut)=2 ∣¼]	*21(Zt,Ut)	Q[22(Zt,Ut)]闷卜
where the closed-loop first and second order approximations of the Q-function are given by
Q；,1l(Zt,Ut) = CwCw - Qt,u(Zt,Ut)>Qt,uu(Zt,Ut)-1Qt,u(Zt,Ut),
Qt,21(Zt,Ut) = Qt,z(Zt,Ut)› - kt(Zt,Ut)>Qt,uu(Zt,Ut)Kt(Zt,Ut),
Qt,22 (Zt, Ut ) = Qt,zz (Zt , Ut ) - Kt (Zt, Ut) Qt,uu (Zt , Ut )Kt (Zt, Ut).
Notice that at time step t the optimal value function also has the following form:
Vt (Zt ) = min Qt (Zt , Ut ) =δQt (Zt , Ut , δZt, πδz t (δZt )) + Qt (Zt, Ut)
δut
=1 [δZ 1> [Q加(Zt,Ut) %1(Zt,Ut)『1 [δZ 1 + Qt(Zt,ut).
2 t	Qt,21(Zt, Ut)	Qt,22 (Zt, Ut)	t
(30)
Therefore, the first and second order differential value functions can be
Vt,z (Zt,Ut) = Qt,2l(Zt,Ut), Vt,zz(Zt,Ut) = Q；,22 QttUt),
and the value improvement at the nominal state Zt at time step t is given by
Vt(Zt) = 1 Qt,1l(Zt,Ut) + Qt(ZttUt).
20
Published as a conference paper at ICLR 2020
B.3 Incorporating Receding-Horizon to iLQR
While iLQR provides an effective way of computing a sequence of (locally) optimal actions, it has
two limitations. First, unlike RL in which an optimal Markov policy is computed, this algorithm
only finds a sequence of open-loop optimal control actions under a given initial observation. Second,
the iLQR algorithm requires the knowledge of a nominal (latent state and action) trajectory at every
iteration, which restricts its application to cases only when real-time interactions with environment
are possible. In order to extend the iLQR paradigm into the closed-loop RL setting, we utilize the
concept of model predictive control (MPC) (Rawlings & Mayne, 2009; Borrelli et al., 2017) and
propose the following iLQR-MPC procedure. Initially, given an initial latent state z0 we generate a
single nominal trajectory: {(zt, ut, zt+1)}tT=-01, whose sequence of actions is randomly sampled, and
the latent states are updated by forward propagation of latent state dynamics (instead of interacting
with environment), i.e., zo = zo, zt+ι 〜F(zt, ut, wt), ∀t. Then at each time-step k ≥ 0, starting
at latent state Zk we compute the optimal perturbed policy {∏δz t(∙)}T-1 using the iLQR algorithm
with T - k lookahead steps. Having access to the perturbed latent state δzk = zk - zk, we only
deploy the first action Ul = 得Z k (δzk) + Uk in the environment and observe the next latent state
Zk+1. Then, using the subsequent optimal perturbed policy {得Z t(∙)}T-+1, we generate both the
estimated latent state sequence {zbt }tT=k+1 by forward propagation with initial state zbk+1 = zk+1
and action sequence {Utl }tT=-k1+1, where Utl = πδlZ,t(δzbt) + Ut, and δzbt = zbt - zt. Then one updates
the subsequent nominal trajectory as follows: {(zt, Ut, zt+1)}tT=-k1+1 = {(zbt, Utl, zbt+1)}tT=-k1+1, and
repeats the above procedure.
Consider the finite-horizon MDP problem minπt ,∀t E cT(xT) + PtT=-o1 ct (xt, Ut) | πt, P, xo
where the optimizer π is over the class of Markovian policies. (Notice this problem is the closed-loo
p
version of (SOC1).) Using the above iLQR-MPC procedure, at each time step t ∈ {0, . . . , T - 1}
one can construct a Markov policy that is in form of
∏t,iLQR-MPC(∙∣xt):= UtLQR s.t. {utLQR,…，UTQr}-iLQR(L(U,P*, C,zt)), with Zt 〜E(∙∣xt),
where LLQR('Control(U； P*,z)) denotes the iLQR algorithm with initial latent state z. To understand
the performance of this policy w.r.t. the MDP problem, we refer to the sub-optimality bound of iLQR
(w.r.t. open-loop control problem in (SOC1)) in Section 3.3, as well as that for MPC policy, whose
details can be found in Borrelli et al. (2017).
21
Published as a conference paper at ICLR 2020
Figure 3: PCC graphical model. Left: generative links, right: recognition links.
C Technical Proofs of Section 4
C.1 DERIVATION OF R30 ,NLE-BOUND(P, Q) DECOMPOSITION
5T 1	,1 1	If ,1	T , ∙	1 1 1 ∙1 FT F 1	4/	I	∖
We derive the bound for the conditional log-likelihood log P (xt+1 |xt, ut).
1—6/ I	∖	、— f	R/	I	ʌ 7 lʌ
log P(xt+l ∣Xt,Ut) = log /	P(xt+l,Zt,^t+l∣Xt,Ut)dztd^t+l
J zt,zt+ι
log EQ(zt,Zt+ι ∣Xt,Xt+ι,Ut)
ʌ I	∖
P (xt+1,Zt,Zt+1∣Xt,Ut)
Q(zt,^t+ι∣xt ,xt+ι,ut)
(a)
≥ EQ(zt,zt+ι ∣xt,xt+ι,ut)
R/	ʌ I ∖
log P(xt+1,Zt,Zt+1∣Xt,Ut)
Q(zt,Zt+ι∣xt,xt+ι,ut)
= E Q(Zt+ι∣χt+ι)
Q(zt∖zt+I,xt,ut)
log
公， ,	、公，.	.	、公，	.. J
P(zt∣xt)P(^t+ι∣zt,ut)P(xt+ι∣^t+ι)
Q(Zt+ι∣χt+ι)Q(zt∣^t+ι ,χt,ut)
-EQ(Zt+ι∣xt+ι) [logP(Xt+1岛+1)]
-EQ(Zt+ι∣xt+ι) [DKL (Q(Ztlzt+1,xt,ut)11P(Zt |xt))]
+ H(Q(2t+Jχt+ι))
+ E Q(Zt+ι∣xt+ι)卜ogP(Zt+1|zt,ut)]
Q(zt∖zt+1,xt,ut) L
0b
= R3,NLE-Bound(P, Q)
Where (a) holds from the log function concavity, (b) holds by the factorization
Q(zt,Zt+ι∣xt,xt+ι,ut) = Q(^t+ι∣χt+ι)Q(zt∣Zt+ι,χt,ut), and (C) holds by a simple decompo-
sition to the different components.
22
Published as a conference paper at ICLR 2020
_________ c，，	/ α 八 一
C.2 DERIVATION OF R200,BOUND(P, Q) DECOMPOSITION
I-V T 1 ∙ .t -I	-IC .t	∙ .	1	C	/ A∖
We derive the bound for the consistency loss 'consistency(P).
R020(Pb) = DKL
Pb(zt+1 |xt+1)k
P P(Zt+1|zt
zt
∖ 4/ I
,ut)P(zt|xt
(a)
=-H (Q(Zt+1|xt+1)) - EQ(Zt+ι∣xt+ι) log
Pb(
zt
Zt+ι∣Zt,ut)P(zt∣xt)dzt
-H (Q(zt+1lxt+1)) - EQ(Zt+ι∣χt+ι) logEQ(zt∣Zt+ι,χt,ut)
P(^t+ι ∣zt,ut)P(zt∣χt)] ^1
Q(ZtIZt+ι,xt,ut)]]
(b)
≤ -H (Q(2t+1|xt+1)) - E Q(Zt+1∣xt+1)
Q(zt∖Zt+ι,xt,ut)
log
ri∕ ʌ I ∖ 4/	∖
P (^t+1∣Zt,Ut)P (zt,xt)
Q(zt∣^t+ι,xt,ut)
-EQ(Zt+1∖xt+1) [DKL (Q(ZtI2t+1,xt,ut)IIP(ZtIxt))]
+ H (Q(Zt+11xt+1)) + E Q(Zt+ι∖χt+ι)
Q(zt∖Zt+ι,xt,ut)
ri / ʌ I
P (^t+lIZt,Ut
R020,Bound(Pb, Q)
Where (a) holds by the assumption that Q(Zt+ι I xt+ι) = Pb(Zt+ι I xt+ι), (b) holds from the log
function concavity, and (c) holds by a simple decomposition to the different components.
23
Published as a conference paper at ICLR 2020
D	Experimental Details
In the following sections we will provide the description of the data collection process, domains, and
implementation details used in the experiments.
D. 1 Data Collection Process
To generate our training and test sets, each consists of triples (xt, ut, xt+1), we: (1) sample an
underlying state st and generate its corresponding observation xt, (2) sample an action ut , and (3)
obtain the next state st+1 according to the state transition dynamics, add it a zero-mean Gaussian
noise with variance σ2Ins, and generate it’s corresponding observation xt+1.To ensure that the
observation-action data is uniformly distributed (see Section 3), we sample the state-action pair
(st , ut ) uniformly from the state-action space. To understand the robustness of each model, we
consider both deterministic (σ = 0) and stochastic scenarios. In the stochastic case, we add noise to
the system with different values of σ and evaluate the models’ performance under various degree of
noise.
D.2 Description of the Domains
Planar System In this task the main goal is to navigate an agent in a surrounded area on a 2D plane
(Breivik & Fossen, 2005), whose goal is to navigate from a corner to the opposite one, while avoiding
the six obstacles in this area. The system is observed through a set of 40 × 40 pixel images taken
from the top view, which specifies the agent’s location in the area. Actions are two-dimensional and
specify the x - y direction of the agent’s movement, and given these actions the next position of the
agent is generated by a deterministic underlying (unobservable) state evolution function. Start State:
one of three corners (excluding bottom-right). Goal State: bottom-right corner. Agent’s Objective:
agent is within Euclidean distance of 2 from the goal state.
Inverted Pendulum — SwingUp & Balance This is the classic problem of controlling an inverted
pendulum (Furuta et al., 1991) from 48 × 48 pixel images. The goal of this task is to swing up an
under-actuated pendulum from the downward resting position (pendulum hanging down) to the top
position and to balance it. The underlying state st of the system has two dimensions: angle and
angular velocity, which is unobservable. The control (action) is 1-dimensional, which is the torque
applied to the joint of the pendulum. To keep the Markovian property in the observation (image)
space, similar to the setting in E2C and RCE, each observation xt contains two images generated
from consecutive time-frames (from current time and previous time). This is because each image only
shows the position of the pendulum and does not contain any information about the velocity. Start
State: Pole is resting down (SWingUp), or randomly sampled in ±π∕6 (Balance). Agent's Objective:
pole's angle is within ±π∕6 from an upright position.
CartPole This is the visual version of the classic task of controlling a cart-pole system (Geva &
Sitte, 1993). The goal in this task is to balance a pole on a moving cart, while the cart avoids hitting
the left and right boundaries. The control (action) is 1-dimensional, which is the force applied to the
cart. The underlying state of the system st is 4- dimensional, which indicates the angle and angular
velocity of the pole, as well as the position and velocity of the cart. Similar to the inverted pendulum,
in order to maintain the Markovian property the observation xt is a stack of two 80 × 80 pixel images
generated from consecutive time-frames. Start State: Pole is randomly sampled in ±∏∕6. Agent's
Objective: pole,s angle is within ±π∕10 from an upright position.
3-link Manipulator — SwingUp & Balance The goal in this task is to move a 3-link manipulator
from the initial position (which is the downward resting position) to a final position (which is the top
position) and balance it. In the 1-link case, this experiment is reduced to inverted pendulum. In the
2-link case the setup is similar to that of arcobot (Spong, 1995), except that we have torques applied
to all intermediate joints, and in the 3-link case the setup is similar to that of the 3-link planar robot
arm domain that was used in the E2C paper, except that the robotic arms are modeled by simple
rectangular rods (instead of real images of robot arms), and our task success criterion requires both
swing-up (manipulate to final position) and balance.12 The underlying (unobservable) state st of
the system is 2N -dimensional, which indicates the relative angle and angular velocity at each link,
and the actions are N -dimensional, representing the force applied to each joint of the arm. The
12Unfortunately due to copyright issues, we cannot test our algorithms on the original 3-link planar robot arm
domain.
24
Published as a conference paper at ICLR 2020
state evolution is modeled by the standard Euler-Lagrange equations (Spong, 1995; Lai et al., 2015).
Similar to the inverted pendulum and cartpole, in order to maintain the Markovian property, the
observation state xt is a stack of two 80 × 80 pixel images of the N -link manipulator generated from
consecutive time-frames. In the experiments we will evaluate the models based on the case of N = 2
(2-link manipulator) and N = 3 (3-link manipulator). Start State: 1st pole with angle π, 2nd pole
with angle 2π∕3, and 3rd pole with angle π∕3, where angle π is a resting position. Agent's Objective:
the sum of all poles, angles is within ±π∕6 from an upright position.
TORCS Simulaotr This task takes place in the TORCS simulator (Wymann et al., 2000) (specifi-
cally in michegan f1 race track, only straight lane). The goal of this task is to control a car so it would
remain in the middle of the lane. We restricted the task to only consider steering actions (left / right
in the range of [-1, 1]), and applied a simple procedure to ensure the velocity of the car is always
around 10. We pre-processed the observations given by the simulator (240 × 320 RGB images) to
receive 80 × 80 binary images (white pixels represent the road). In order to maintain the Markovian
property, the observation state xt is a stack of two 80 × 80 images (where the two images are 7
frames apart - chosen so that consecutive observation would be somewhat different). The task goes
as follows: the car is forced to steer strongly left (action=1), or strongly right (action=-1) for the
initial 20 steps of the simulation (direction chosen randomly), which causes it to drift away from the
center of the lane. Then, for the remaining horizon of the task, the car needs to recover from the drift,
return to the middle of the lane, and stay there. Start State: 20 steps of drifting from the middle of
the lane by steering strongly left, or right (chosen randomly). Agent's Objective: agent (car) is within
Euclidean distance of 1 from the middle of the lane (full width of the lane is about 18).
D.3 Implementation
In the following we describe architectures and hyper-parameters that were used for training the
different algorithms.
D.3.1 Training Hyper-Parameters and Regulizers
All the algorithms were trained using:
•	Batch size of 128.
•	ADAM (Goodfellow et al., 2016) with α = 5 ∙ 10-4, βι = 0.9, β2 = 0.999, and e = 10-8.
•	L2 regularization with a coefficient of 10-3.
•	Additional VAE (Kingma & Welling, 2013) loss term given by 'VAE	=
-	Eq(z∣x) [logp(x∣z)] + DKL (q(z∣x)kp(z)), where p(z)〜N(0,1). The term was added
with a very small coefficient of 0.01. We found this term to be important to stabilize the
training process, as there is no explicit term that governs the scale of the latent space.
E2C training specifics:
•	λ from the loss term of E2C was tuned using a parameter sweep in {0.25, 0.5, 1}, and
was chosen to be 0.25 across all domains, as it performed the best independently for each
domain.
PCC training specifics:
•	λp was set to 1 across all domains.
•	λc was set to be 7 across all domains, after it was tuned using a parameter sweep in
{1, 3, 7, 10} on the Planar system.
•	λcur was set to be 1 across all domains without performing any tuning.
•	{z, u}, for the curvature loss, were generated from {z,u} by adding Gaussian noise
N(0, 0.12), where σ = 0.1 was set across all domains without performing any tuning.
•	Motivated by Hafner et al. (2018), we added a deterministic loss term in the form of cross
entropy between the output of the generative path given the current observation and action
(while taking the means of the encoder output and the dynamics model output) and the
observation of the next state. This loss term was added with a coefficient of 0.3 across
all domains after it was tuned using a parameter sweep over {0.1, 0.3, 0.5} on the Planar
system.
25
Published as a conference paper at ICLR 2020
D.3.2 Network Architectures
We next present the specific architecture choices for each domain. For fair comparison, The numbers
of layers and neurons of each component were shared across all algorithms. ReLU non-linearities
were used between each two layers.
Encoder: composed of a backbone (either a MLP or a CNN, depending on the domain) and an
additional fully-connected layer that outputs mean variance vectors that induce a diagonal Gaussian
distribution.
Decoder: composed of a backbone (either a MLP or a CNN, depending on the domain) and an
additional fully-connected layer that outputs logits that induce a Bernoulli distribution.
Dynamical model: the path that leads from {zt, ut} to ^t+ι. Composed of a MLP backbone and
an additional fully-connected layer that outputs mean and variance vectors that induce a diagonal
Gaussian distribution. We further added a skip connection from zt and summed it with the output of
the mean vector. When using the amortized version, there are two additional outputs A and B .
Backwards dynamical model: the path that leads from {^t+ι,ut,xt} to zt. each of the inputs
goes through a fully-connected layer with {Nz , Nu, Nx } neurons, respectively. The outputs are then
concatenated and pass though another fully-connected layer with Njoint neurons, and finally with an
additional fully-connected layer that outputs the mean and variance vectors that induce a diagonal
Gaussian distribution.
Planar system
•	Input: 40 × 40 images. 5000 training samples of the form (xt, ut, xt+1)
•	Actions space: 2-dimensional
•	Latent space: 2-dimensional
•	Encoder: 3 Layers: 300 units - 300 units - 4 units (2 for mean and 2 for variance)
•	Decoder: 3 Layers: 300 units - 300 units - 1600 units (logits)
•	Dynamics: 3 Layers: 20 units - 20 units - 4 units
•	Backwards dynamics: Nz = 5, Nu = 5, Nx = 100 - Njoint = 100 - 4 units
•	Number of control actions: or the planning horizon T = 40
Inverted Pendulum — Swing Up & Balance
•	Input: Two 48 × 48 images. 20000 training samples of the form (xt, ut, xt+1)
•	Actions space: 1-dimensional
•	Latent space: 3-dimensional
•	Encoder: 3 Layers: 500 units - 500 units - 6 units (3 for mean and 3 for variance)
•	Decoder: 3 Layers: 500 units - 500 units - 4608 units (logits)
•	Dynamics: 3 Layers: 30 units - 30 units - 6 units
•	Backwards dynamics: Nz = 10, Nu = 10, Nx = 200 - Njoint = 200 - 6 units
•	Number of control actions: or the planning horizon T = 400
Cart-pole Balancing
•	Input: Two 80 × 80 images. 15000 training samples of the form (xt, ut, xt+1)
•	Actions space: 1-dimensional
•	Latent space: 8-dimensional
•	Encoder: 6 Layers: Convolutional layer: 32 × 5 × 5; stride (1, 1) - Convolutional layer:
32 × 5 × 5; stride (2, 2) - Convolutional layer: 32 × 5 × 5; stride (2, 2) - Convolutional
layer: 10 × 5 × 5; stride (2, 2) - 200 units - 16 units (8 for mean and 8 for variance)
•	Decoder: 6 Layers: 200 units - 1000 units - 100 units - Convolutional layer: 32 × 5 × 5;
stride (1, 1) - Upsampling (2, 2) - convolutional layer: 32 × 5 × 5; stride (1, 1) - Upsampling
(2, 2) - Convolutional layer: 32 × 5 × 5; stride (1, 1) - Upsampling (2, 2) - Convolutional
layer: 2 × 5 × 5; stride (1, 1)
26
Published as a conference paper at ICLR 2020
•	Dynamics: 3 Layers: 40 units - 40 units - 16 units
•	Backwards dynamics: Nz = 10, Nu = 10, Nx = 300 - Njoint = 300 - 16 units
•	Number of control actions: or the planning horizon T = 200
3-link Manipulator — Swing Up & Balance
•	Input: Two 80 × 80 images. 30000 training samples of the form (xt, ut, xt+1)
•	Actions space: 3-dimensional
•	Latent space: 8-dimensional
•	Encoder: 6 Layers: Convolutional layer: 62 × 5 × 5; stride (1, 1) - Convolutional layer:
32 × 5 × 5; stride (2, 2) - Convolutional layer: 32 × 5 × 5; stride (2, 2) - Convolutional
layer: 10 × 5 × 5; stride (2, 2) - 500 units - 16 units (8 for mean and 8 for variance)
•	Decoder: 6 Layers: 500 units - 2560 units - 100 units - Convolutional layer: 32 × 5 × 5;
stride (1, 1) - Upsampling (2, 2) - convolutional layer: 32 × 5 × 5; stride (1, 1) - Upsampling
(2, 2) - Convolutional layer: 32 × 5 × 5; stride (1, 1) - Upsampling (2, 2) - Convolutional
layer: 2 × 5 × 5; stride (1, 1)
•	Dynamics: 3 Layers: 40 units - 40 units - 16 units
•	Backwards dynamics: Nz = 10, Nu = 10, Nx = 400 - Njoint = 400 - 16 units
•	Number of control actions: or the planning horizon T = 400
TORCS
•	Input: Two 80 × 80 images. 30000 training samples of the form (xt, ut, xt+1)
•	Actions space: 1-dimensional
•	Latent space: 8-dimensional
•	Encoder: 6 Layers: Convolutional layer: 32 × 5 × 5; stride (1, 1) - Convolutional layer:
32 × 5 × 5; stride (2, 2) - Convolutional layer: 32 × 5 × 5; stride (2, 2) - Convolutional
layer: 10 × 5 × 5; stride (2, 2) - 200 units - 16 units (8 for mean and 8 for variance)
•	Decoder: 6 Layers: 200 units - 1000 units - 100 units - Convolutional layer: 32 × 5 × 5;
stride (1, 1) - Upsampling (2, 2) - convolutional layer: 32 × 5 × 5; stride (1, 1) - Upsampling
(2, 2) - Convolutional layer: 32 × 5 × 5; stride (1, 1) - Upsampling (2, 2) - Convolutional
layer: 2 × 5 × 5; stride (1, 1)
•	Dynamics: 3 Layers: 40 units - 40 units - 16 units
•	Backwards dynamics: Nz = 10, Nu = 10, Nx = 300 - Njoint = 300 - 16 units
•	Number of control actions: or the planning horizon T = 200
E	Additional Results
E.1 Performance on Noisy Dynamics
Table 3 shows results for the noisy cases.
Table 3: Percentage of steps in goal state. Averaged over all models (left), and over the best model
(right). Subscript under the domain name is the variance of the noise that was added.
Domain	RCE (all)	E2C (all)	PCC (all)	RCE (top 1)	E2C (top 1)	PCC (top 1)
PlanarI	1.2 ± 0.6 =	0.6 ± 0.3 =	17.9 ± 3.1 二	5.5 ± 1.2 二	6.1 ± 0.9 =	44.7 ± 3.6 二
Planar2	0.4 ± 0.2	-1.5 ± 0.9	14.5 ± 2.3	-1.7 ± 0.5	15.5 ± 2.6	29.7 ± 2.9
PendulumI	-6.4 ± 0.3-	23.8 ± 1.2	16.4 ± 0.8	-8.1 ± 0.4	36.1 ± 0.3	29.5 ± 0.2
CartPoleI	-8.1 ± 0.6	6.6 ± 0.4	9.8 ± 0.7	20.3 ± 11	16.5 ± 0.4	17.9 ± 0.8
3-linkι	0.3 ± 0.1 一	0 ± 0 一	0.5 ± 0.1 一	1.3 ± 0.2 一	0 ± 0 一	1.8 ± 0.3 一
27
Published as a conference paper at ICLR 2020
E.2 Latent Space Representation for the Planar System
The following figures depicts 5 instances (randomly chosen from the 10 trained models) of the learned
latent space representations for both the noiseless and the noisy planar system from PCC, RCE, and
E2C models.
Figure 4: Noiseless Planar latent space representations using PCC.
Figure 5: Noisy (σ2 = 1) Planar latent space representations using PCC.
Figure 6: Noiseless Planar latent space representations using RCE.
Figure 7: Noisy (σ2 = 1) Planar latent space representations using RCE.
Figure 8: Noiseless Planar latent space representations using E2C.
Figure 9: Noisy (σ2 = 1) Planar latent space representations using E2C.
28