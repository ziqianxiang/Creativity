Published as a conference paper at ICLR 2020
Simple and Effective Regularization Methods
for Training on Noisily Labeled Data with
Generalization Guarantee
Wei Hu Zhiyuan Li Dingli Yu
Princeton University
{huwei,zhiyuanli,dingliy}@cs.princeton.edu
Ab stract
Over-parameterized deep neural networks trained by simple first-order methods are
known to be able to fit any labeling of data. Such over-fitting ability hinders gener-
alization when mislabeled training examples are present. On the other hand, simple
regularization methods like early-stopping can often achieve highly nontrivial
performance on clean test data in these scenarios, a phenomenon not theoretically
understood. This paper proposes and analyzes two simple and intuitive regulariza-
tion methods: (i) regularization by the distance between the network parameters to
initialization, and (ii) adding a trainable auxiliary variable to the network output for
each training example. Theoretically, we prove that gradient descent training with
either of these two methods leads to a generalization guarantee on the clean data
distribution despite being trained using noisy labels. Our generalization analysis
relies on the connection between wide neural network and neural tangent kernel
(NTK). The generalization bound is independent of the network size, and is com-
parable to the bound one can get when there is no label noise. Experimental results
verify the effectiveness of these methods on noisily labeled datasets.
1	Introduction
Modern deep neural networks are trained in a highly over-parameterized regime, with many more
trainable parameters than training examples. It is well-known that these networks trained with
simple first-order methods can fit any labels, even completely random ones (Zhang et al., 2017).
Although training on properly labeled data usually leads to good generalization performance, the
ability to over-fit the entire training dataset is undesirable for generalization when noisy labels are
present. Therefore preventing over-fitting is crucial for robust performance since mislabeled data are
ubiquitous in very large datasets (Krishna et al., 2016).
In order to prevent over-fitting to mislabeled data, some form of regularization is necessary. A simple
such example is early stopping, which has been observed to be surprisingly effective for this purpose
(Rolnick et al., 2017; Guan et al., 2018; Li et al., 2019). For instance, training ResNet-34 with
early stopping can achieve 84% test accuracy on CIFAR-10 even when 60% of the training labels
are corrupted (Table 1). This is nontrivial since the test error is much smaller than the error rate in
training data. How to explain such generalization phenomenon is an intriguing theoretical question.
As a step towards a theoretical understanding of the generalization phenomenon for over-
parameterized neural networks when noisy labels are present, this paper proposes and analyzes
two simple regularization methods as alternatives of early stopping:
1.	Regularization by distance to initialization. Denote by θ the network parameters and by
θp0q its random initialization. This method adds a regularizer λ }θ ´ θp0q}2 to the training
objective.
2. Adding an auxiliary variable for each training example. Let xi be the i-th training example
and f (θ, ∙) represent the neural net. This method adds a trainable variable b and tries to fit
the i-th label using f (θ, Xi) ` λbi. At test time, only the neural net f (θ, ∙) is used and the
auxiliary variables are discarded.
1
Published as a conference paper at ICLR 2020
These two choices of regularization are well motivated with clear intuitions. First, distance to
initialization has been observed to be very related to generalization in deep learning (Neyshabur et al.,
2019; Nagarajan and Kolter, 2019), so regularizing by distance to initialization can potentially help
generalization. Second, the effectiveness of early stopping indicates that clean labels are somewhat
easier to fit than wrong labels; therefore, adding an auxiliary variable could help “absorb” the noise
in the labels, thus making the neural net itself not over-fitting.
We provide theoretical analysis of the above two regularization methods for a class of sufficiently
wide neural networks by proving a generalization bound for the trained network on clean data
distribution when the training dataset contains noisy labels. Our generalization bound depends on the
(unobserved) clean labels, and is comparable to the bound one can get when there is no label noise,
therefore indicating that the proposed regularization methods are robust to noisy labels.
Our theoretical analysis is based on the recently established connection between wide neural net and
neural tangent kernel (Jacot et al., 2018; Lee et al., 2019; Arora et al., 2019a). In this line of work,
parameters in a wide neural net are shown to stay close to their initialization during gradient descent
training, and as a consequence, the neural net can be effectively approximated by its first-order Taylor
expansion with respect to its parameters at initialization. This leads to tractable linear dynamics
under `2 loss, and the final solution can be characterized by kernel regression using a particular
kernel named neural tangent kernel (NTK). In fact, we show that for wide neural nets, both of our
regularization methods, when trained with gradient descent to convergence, correspond to kernel
ridge regression using the NTK, which is often regarded as an alternative to early stopping in kernel
literature. This viewpoint makes explicit the connection between our methods and early stopping.
The effectiveness of these two regularization methods is verified empirically - on MNIST and
CIFAR-10, they are able to achieve highly nontrivial test accuracy, on a par with or even better than
early stopping. Furthermore, with our regularization, the validation accuracy is almost monotone
increasing throughout the entire training process, indicating their resistance to over-fitting.
2	Related Work
Neural tangent kernel was first explicitly studied and named by Jacot et al. (2018), with several
further refinements and extensions by Lee et al. (2019); Yang (2019); Arora et al. (2019a). Using
the similar idea that weights stay close to initialization and that the neural network is approximated
by a linear model, a series of theoretical papers studied the optimization and generalization issues
of very wide deep neural nets trained by (stochastic) gradient descent (Du et al., 2019b; 2018b;
Li and Liang, 2018; Allen-Zhu et al., 2018a;b; Zou et al., 2018; Arora et al., 2019b; Cao and Gu,
2019). Empirically, variants of NTK on convolutional neural nets and graph neural nets exhibit strong
practical performance (Arora et al., 2019a; Du et al., 2019a), thus suggesting that ultra-wide (or
infinitely wide) neural nets are at least not irrelevant.
Our methods are closely related to kernel ridge regression, which is one of the most common kernel
methods and has been widely studied. It was shown to perform comparably to early-stopped gradient
descent (Bauer et al., 2007; Gerfo et al., 2008; Raskutti et al., 2014; Wei et al., 2017). Accordingly,
we indeed observe in our experiments that our regularization methods perform similarly to gradient
descent with early stopping in neural net training.
In another theoretical study relevant to ours, Li et al. (2019) proved that gradient descent with early
stopping is robust to label noise for an over-parameterized two-layer neural net. Under a clustering
assumption on data, they showed that gradient descent fits the correct labels before starting to over-fit
wrong labels. Their result is different from ours from several aspects: they only considered two-layer
nets while we allow arbitrarily deep nets; they required a clustering assumption on data while our
generalization bound is general and data-dependent; furthermore, they did not address the question of
generalization, but only provided guarantees on the training data.
A large body of work proposed various methods for training with mislabeled examples, such as
estimating noise distribution (Liu and Tao, 2015) or confusion matrix (Sukhbaatar et al., 2014),
using surrogate loss functions (Ghosh et al., 2017; Zhang and Sabuncu, 2018), meta-learning (Ren
et al., 2018), using a pre-trained network (Jiang et al., 2017), and training two networks simultane-
ously (Malach and Shalev-Shwartz, 2017; Han et al., 2018; Yu et al., 2019). While our methods are
not necessarily superior to these methods in terms of performance, our methods are arguably simpler
(with minimal change to normal training procedure) and come with formal generalization guarantee.
2
Published as a conference paper at ICLR 2020
3	Preliminaries
Notation. We use bold-faced letters for vectors and matrices. We use }∙} to denote the Euclidean
norm of a vector or the spectral norm of a matrix, and 卜卜 to denote the Frobenius norm of a matrix.
X, •〉represents the standard inner product. Let I be the identity matrix of appropriate dimension.
Let rns “ t1, 2, . . . , nu. Let IrAs be the indicator of event A.
3.1	Setting: Learning from Noisily Labeled Data
Now we formally describe the setting considered in this paper. We first describe the binary classifica-
tion setting as a warm-up, and then describe the more general setting of multi-class classification.
Binary classification. Suppose that there is an underlying data distribution D over Rd X {±1},
where 1 and ´1 are labels corresponding to two classes. However, we only have access to samples
from a noisily labeled version of D. Formally, the data generation process is: draw px, yq „ D, and
flip the sign of label y with probability P (0 ≤ P < 1); let y p {± 1} be the resulting noisy label.
Let {(xi, yi)}n“i be i.i.d. samples generated from the above process. Although we only have access
to these noisily labeled data, the goal is still to learn a function (in the form of a neural net) that can
predict the true label well on the clean distribution D. For binary classification, it suffices to learn a
single-output function f : Rd → R whose sign is used to predict the class, and thus the classification
error of f on D is defined as Prpx,yq„D rsgnpf pxqq -“ ys.
Multi-class classification. When there are K classes (K > 2), let the underlying data distribution
D be over Rd X [K]. We describe the noise generation process as a matrix P P RKXK, whose
entry Pc1,c is the probability that the label c is transformed into c1 (@c, c1 P rKs). Therefore the
data generation process is: draw (x,c)〜D, and replace the label C with C from the distribution
Prrc = c1∣cS = pcι,c (@C1 P [KS).
Let tpxi, cCiq}in“1 be i.i.d. samples from the above process. Again we would like to learn a neural net
with low classification error on the clean distribution D. For K-way classification, it is common to
use a neural net with K outputs, and the index of the maximum output is used to predict the class.
Thus for f : Rd → RK, its (top-1) classification error on D is Prpx,cq„D[c R argmaxhPrKsfphqpxqs,
where fphq : Rd → R is the function computed by the h-th output of f.
As standard practice, a class label c P [Ks is also treated as its one-hot encoding epcq “
PO, 0, ∙∙∙ , 0,1,0,…,0) p RK (the c-th coordinate being 1), which can be paired with the K
outputs of the network and fed into a loss function during training.
Note that it is necessary to assume pc,c > Pcf,c for all c “ c1, i.e., the probability that a class label c
is transformed into another particular label must be smaller than the label c being correct - otherwise
it is impossible to identify class c correctly from noisily labeled data.
3.2	Recap of Neural Tangent Kernel
Now we briefly and informally recap the theory of neural tangent kernel (NTK) (Jacot et al., 2018;
Lee et al., 2019; Arora et al., 2019a), which establishes the equivalence between training a wide
neural net and a kernel method.
We first consider a neural net with a scalar output, defined as fpθ, xq P R, where θ P RN is all the
parameters in the net and x P Rd is the input. Suppose that the net is trained by minimizing the
'2 loss over a training dataset {(xi, yi)}L U Rd X R: L(θ) “ 1 Xn“1 Pf (θ, Xiq — yi)2. Let the
random initial parameters be θp0q, and the parameters be updated according to gradient descent on
L(θq. It is shown that if the network is sufficiently wide1, the parameters θ will stay close to the
initialization θ(0q during training so that the following first-order approximation is accurate:
f(θ, x)« f (θ(0), x) ' XVθ f (θ(0), x), θ — θ(0)y.	(1)
This approximation is exact in the infinite width limit, but can also be shown when the width is finite
but sufficiently large. When approximation (1) holds, we say that we are in the NTK regime.
Define φ(x) “ Vθf(θ(0), x) for any x P Rd. The right hand side in (1) is linear in θ. As a
consequence, training on the `2 loss with gradient descent leads to the kernel regression solution
1“Width” refers to number of nodes in a fully connected layer or number of channels in a convolutional layer.
3
Published as a conference paper at ICLR 2020
with respect to the kernel induced by (random) features φpxq, which is defined as kpx, x1q “
xφpxq, φpx1qy for x, x1 P Rd. This kernel was named the neural tangent kernel (NTK) by Jacot
et al. (2018). Although this kernel is random, it is shown that when the network is sufficiently wide,
this random kernel converges to a deterministic limit in probability (Arora et al., 2019a). If we
additionally let the neural net and its initialization be defined so that the initial output is small, i.e.,
fpθp0q, xq « 0,2 then the network at the end of training approximately computes the following
function:
χ→ k(x, X )J(k(X, X ))Ty,
(2)
where X “ (x1, . . . , xnq is the training inputs, y “ (y1, . . . , ynqJ is the training targets, k(x, Xq “
(k(x, xι),...,k(x, Xn))J P Rn, and k(X, X) P RnXn With (i,jɔ-th entry being k(x%, Xj).
Multiple outputs. The NTK theory above can also be generalized straightforwardly to the case
of multiple outputs (Jacot et al., 2018; Lee et al., 2019). Suppose We train a neural net With K
outputs, f (θ, x), by minimizing the '2 loss over a training dataset {(xp yi)}∖ι U Rd X RK:
L(θ) “ 1 Xn“1 }f (θ, Xi) ´ yi}2. When the hidden layers are sufficiently wide such that we are in
the NTK regime, at the end of gradient descent, each output of f also attains the kernel regression
solution with respect to the same NTK as before, using the corresponding dimension in the training
targets yi . Namely, the h-th output of the network computes the function
f Phq(X) = k(χ, X )J(k(x, X))Typhq,
where yphq P Rn whose i-th coordinate is the h-th coordinate of yi .
4 Regularization Methods
In this section we describe two simple regularization methods for training with noisy labels, and show
that if the network is sufficiently wide, both methods lead to kernel ridge regression using the NTK.3
We first consider the case of scalar target and single-output network. The generalization to multiple
outputs is straightforward and is treated at the end of this section. Given a noisily labeled training
dataset {(xi, yi[}i“i U Rd X R, let f (θ, ∙) be a neural net to be trained. A direct, unregularized train-
ing method would involve minimizing an objective function like L(θ) = 21 Xn“1 (f (θ, Xi) — yi)2.
To prevent over-fitting, we suggest the following simple regularization methods that slightly modify
this objective:
•	Method 1: Regularization using Distance to Initialization (RDI). We let the initial pa-
rameters θ(0) be randomly generated, and minimize the following regularized objective:
1 n	λ2
LλDI(θ) “ 2∑ (f(θ,Xi) ´ yi)2 + H }θ ´ θ(0)}2.
i“1
(3)
•	Method 2: adding an AUXiliary variable for each training example (AUX). We add an
auxiliary trainable parameter bi P R for each i P rns, and minimize the following objective:
1n
LλUX(θ, b) “ 2 ∑ PfPθ, Xi) + λbi ´ %)2,
2 i“1
where b = (b1, . . . , bn)J P Rn is initialized to be 0.
(4)
Equivalence to kernel ridge regression in wide neural nets. Now we assume that we are in
the NTK regime described in Section 3.2, where the neural net architecture is sufficiently wide so
that the first-order approximation (1) is accurate during gradient descent: f(θ, X) « f (θ(0), X) `
Φ(x)j(Θ — θ(0)). Recall that we have Φ(x) = Vθf (θ(0), x) which induces the NTK k(X, x1)“
xφ(X), φ(X1)y. Also recall that we can assume near-zero initial output: f(θ(0), X) « 0 (see
Footnote 2). Therefore we have the approximation:
f(θ, X) « Φ(x)j(Θ — θ(0)).	(5)
2We can ensure small or even zero output at initialization by either multiplying a small factor (Arora et al.,
2019a;b), or using the following “difference trick”: define the network to be the difference between two networks
with the same architecture, i.e., f (θ, x) “ 殍g(θι, x)—警g(θz, x); then initialize θι and θ2 to be the
same (and still random); this ensures f (θ(0), X) “ 0 (@X) at initialization, while keeping the same value of
xφ(x), φ(x1)y for both f and g. See more details in Appendix A.
3For the theoretical results in this paper we use `2 loss. In Section 6 we will present experimental results of
both `2 loss and cross-entropy loss for classification.
4
Published as a conference paper at ICLR 2020
Under the approximation (5), it suffices to consider gradient descent on the objectives (3) and (4)
using the linearized model instead:
1 n	2	λ2
LλDI(θ) = 2 ∑ (φ(xi)J(θ ´ θ(0)) ´ y) + -2 }θ ´ θ(0)}2 ,
i“1
1n	2
LλUX(θ, b) = 2 ∑ (φ(xi)J(θ ´ θ(0)) + -bi ´ y).
i“1
The following theorem shows that in either case, gradient descent leads to the same dynamics and
converges to the kernel ridge regression solution using the NTK.
Theorem 4.1. Fix a learning rate η > 0. Consider gradient descent on Lrdi with initialization θ(0):
θ(t + 1) = θ(tq — ηVθLλD1(θ(t)), t = 0,1,2,...
and gradient descent on LAUX (θ, b) with initialization θ(0) and b(0) = 0:
θ(0) = θ(0),	θ(t +1)	= θ(t) ´	ηVθ LAUX(θ(t), b(t)),	t =	0,2,2,...
b(0) = 0,	b(t + 1)	= b(t)´	ηVbLAX(θ(t), b(t)),	t =	0,1, 2,...
Then we must have θ(t) = θ(t) for all t. Furthermore, ifthe learning rate satisfies η ≤
then tθ(t)u converges linearly to a limit solution θ* such that:
φ(x)J(θ* — θ(0)) = k(x, X)J 'k(X, X) + λ2I厂1 y, @x,
Where y = (yι,... ,yn)J P Rn.
(6)
(7)
∣∣kpx,xq∣∣'λ2，
1
Proof sketch. The proof is given in Appendix B. A key step is to observe θ(t) “ θ(0)'Xn“1 λbi(t)'
□
φ(xi), from which We can show that {θ(t)} and {θ(t)} follow the same update rule.
Theorem 4.1 indicates that gradient descent on the regularized objectives (3) and (4) both learn
approximately the following function at the end of training when the neural net is sufficiently wide:
f *(x) “ k(x, X)J (k(X, X) + λ2I)´1 y.	(8)
If no regularization were used, the labels y would be fitted perfectly and the learned function would
be k(x, X)J (k(X, X))´1 ry (c.f. (2)). Therefore the effect of regularization is to add λ2I to the
kernel matrix, and (8) is known as the solution to kernel ridge regression in kernel literature. In
Section 5, we give a generalization bound of this solution on the clean data distribution, which is
comparable to the bound one can obtain even when clean labels are used in training.
Extension to multiple outputs. Suppose that the training dataset is t(xi,。。}之1 U Rd X RK, and
the neural net f (θ, x) has K outputs. On top of the vanilla loss L(θ) “ 2 Xn“1 }f (θ, Xiq — τ∕i}2,
the two regularization methods RDI and AUX give the following objectives similar to (3) and (4):
1	n	λ2
LλDI(θ) “ 2 ∑ }f (θ,Xi) — yi}2 ' E }θ ´ θ(0)}2,
i“1
2n
LλUx(θ, B) =	}f (θ, Xi) + λbi ´ yi}2 , B =(bl,..., bn) P RKxn.
2	i“1
In the NTK regime, both methods lead to the kernel ridge regression solution at each output. Namely,
letting Y “(yι,..., yn) P RKXn be the training target matrix and yphq P Rn be the h-th row of Y,
at the end of training the h-th output of the network learns the following function:
f Phq(X) “ k(x, X)J 'k(X, Xq + λ2I)T yphq,	h p [K].
(9)
5 Generalization Guarantee on Clean Data Distribution
We show that gradient descent training on noisily labeled data with our regularization methods RDI or
AUX leads to a generalization guarantee on the clean data distribution. As in Section 4, we consider
the NTK regime and let k(∙, ∙) be the NTK corresponding to the neural net. It suffices to analyze the
kernel ridge regression predictor, i.e., (8) for single output and (9) for multiple outputs.
We start with a regression setting where labels are real numbers and the noisy label is the true label
plus an additive noise (Theorem 5.1). Built on this result, we then provide generalization bounds for
the classification settings described in Section 3.1. Omitted proofs in this section are in Appendix C.
5
Published as a conference paper at ICLR 2020
Theorem 5.1 (Additive label noise). Let D be a distribution over Rd X [—1,1S. Consider the
following data generation process: (i) draw px, yq „ D, (ii) conditioned on px, yq, let ε be drawn
from a noise distribution Eχ,y over R that may depend on X and y, and (iii) let y “ y ' ε. Suppose
that Eχ,y has mean 0 and is subgaussian with parameter σ > 0, for any (x,y).
Let {(xi, yi, yi)}n“i be i.i.d. samples from the above process. Denote X “ (xι,..., Xn), y “
(yι,..., yn)J andry “ (yι,..., yn)J. Consider the kernel ridge regression solution in (8): f * (x)=
k(x, X)J (k(X, X) ' λ2l) y. Suppose that the kernel matrix satisfies tr[k(X, X)S = O(n).
Thenfor any loss function ' : R X R → [0,1] that is 1 -Lipschitz in the first argument such that
'(y, y) = 0, with probability at least 1 — δ we have_
E(χ,y)~D ['(f*(χ),y)‰ W λ 'Op1q JyJpkpX,Xqq≡y + O ´σ) + ∆,	(10)
2nλ
where ∆ = O (σjlo⅛^δq + λ Jlog^ +
Remark 5.1. As the number of samples n → 8, we have ∆ → 0. In order for the second term
O(λ) in (10) to go to 0, we need to choose λ to grow with n, e.g., λ = nc for some small constant
C > 0. Then, the only remaining term in (10) to worry about is ʌʌ/-
yJPk(X,X))Iy. Notice that it
n
depends on the (unobserved) clean labels y, instead ofthe noisy labels y. By a very similar proof,
one can show that training on the clean labels y (without regularization) leads to a population loss
bound O(a∕yJ(MXJX))Ty
n
. In comparison, we can see that even when there is label noise, we
only lose a factor of O(λ) in the population loss on the clean distribution, which can be chosen
as any slow-growing function of n. If yJ(k(X, X ))-1y grows much slower than n, by choosing
an appropriate λ, our result indicates that the underlying distribution is learnable in presence of
additive label noise. See Remark 5.2 for an example.
Remark 5.2. Arora et al. (2019b) proved that two-layer ReLU neural nets trained with gradi-
ent descent can learn a class of smooth functions on the unit sphere. Their proof is by showing
yJ(k(X, X)) Ty = O(1) if yi = g(xi) (@i P [n]) for certain function g, where k(∙, ∙) is the NTK
corresponding to two-layer ReLU nets. Combined with their result, Theorem 5.1 implies that the
same class of functions can be learned by the same network even if the labels are noisy.
Next we use Theorem 5.1 to provide generalization bounds for the classification settings described in
Section 3.1. For binary classification, We treat the labels as +1 and consider a single-output neural
net; for K-class classification, we treat the labels as their one-hot encodings (which are K standard
unit vectors in RK) and consider a K-output neural net. Again, We use `2 loss and Wide neural nets
so that it suffices to consider the kernel ridge regression solution ((8) or (9)).
Theorem 5.2 (Binary classification). Consider the binary classification setting stated in Section 3.1.
Let {(xi, yi, yi)Un“i U Rd X {士1} X {±1} be i.i.d. samples from that process. Recall that Prryi =
Vi∖yi] = P (0 ≤ P < 1). Denote X = (xι,..., Xn), y = (yι,...,yn)J and y = (yi,.. .,yn)J.
Consider the kernel ridge regression solution in (8): f *(x) = k(x, X)J (k(X, X) + λ2l) y.
Suppose that the kernel matrix satisfies trrk(X, X)s = O(n). Then with probability at least 1 — δ,
the classification error of f * on the clean distribution D satisfies
PxPqrLDrsgnpf*px))‰ ySW λ1011i
yJ (k(X, X))Ty
+1⅛o
P log δ
n
n
Theorem 5.3 (Multi-class classification). Consider the K -class classification setting stated in
SeCtion 3.1. Let {(xi,Ci,Ci)}i“i U Rd X [K S X [K S be i.i.d. samples from that process. Recall
that Pr[ci = C∖ci = CS= pc ,c (Vc,c1 P [K]), where the transition probabilities form a matrix
P P RKXK. Let gap = minc,cip[κ],c/ci (Pc,c — Pc,c).
Let X = (xi,..., Xn), and let yi = epciq P RK, y% = epciq P RK be one-hot label encodings.
Denote Y = (yi,..., yn) P RKXn, Y = (ɪ/i,..., y。)P RKXn, and let y(h) P Rn be the h-th row
of Y. Define a matrix Q = P ∙ Y P RKxn, and let q(h) P Rn be the h-th row of Q.
Consider the kernel ridge regression solution in (9): f(h) (x) = k(x, X)J 'k(X, X) + λ2I) 1 y(h).
Suppose that the kernel matrix satisfies tr[k(X, X)S = O(n). Then with probability at least 1 — δ,
the classification error off = (f (h) )kK“i on the clean data distribution D is bounded as
6
Published as a conference paper at ICLR 2020
(a) Test error vs. noise level for Setting 1. For each
noise level, we do a grid search for λ and report
the best accuracy.
(b) Training (dashed) & test (solid) errors vs. epoch
for Setting 2. Noise rate “ 20%, λ “ 4. Training
error of AUX is measured with auxiliary variables.
Figure 1: Performance on binary classification using `2 loss. Setting 1: MNIST; Setting 2: CIFAR.
px,Pcqr„D c R argmaxhPrKsf phqpxq
W 二 λ±≡ W C
gaP ∖	2 WJIY
(q(h))J(k(X, X))Tq(h) + K . O
n
Note that the bounds in Theorems 5.2 and 5.3 only depend on the clean labels instead of the noisy
labels, similar to Theorem 5.1.
6	Experiments
In this section, we empirically verify the effectiveness of our regularization methods RDI and AUX,
and compare them against gradient descent or stochastic gradient descent (GD/SGD) with or without
early stopping. We experiment with three settings of increasing complexities:
Setting 1: Binary classification on MNIST (“5” vs. “8”) using a two-layer wide fully-connected net.
Setting 2: Binary classification on CIFAR (“airplanes” vs. “automobiles”) using a 11-layer convolu-
tional neural net (CNN).
Setting 3: CIFAR-10 classification (10 classes) using standard ResNet-34.
For detailed description see Appendix D. We obtain noisy labels by randomly corrupting correct
labels, where noise rate/level is the fraction of corrupted labels (for CIFAR-10, a corrupted label is
chosen uniformly from the other 9 classes.)
6.1	Performance of Regularization Methods
For Setting 1 (binary MNIST), we plot the test errors of different methods under different noise rates
in Figure 1a. We observe that both methods GD+AUX and GD+RDI consistently achieve much lower
test error than vanilla GD which over-fits the noisy dataset, and they achieve similar test error to
GD with early stopping. We see that GD+AUX and GD+RDI have essentially the same performance,
which verifies our theory of their equivalence in wide networks (Theorem 4.1).
For Setting 2 (binary CIFAR), Figure 1b shows the learning curves (training and test errors) of SGD,
SGD+AUX and SGD+RDI for noise rate 20% and λ “ 4. Additional figures for other choices of λ
are in Figure 5. We again observe that both SGD+AUX and SGD+RDI outperform vanilla SGD and
are comparable to SGD with early stopping. We also observe a discrepancy between SGD+AUX and
SGD+RDI, possibly due to the noise in SGD or the finite width.
Finally, for Setting 3 (CIFAR-10), Table 1 shows the test accuracies of training with and without AUX.
We train with both mean square error (MSE∕'2 loss) and categorical cross entropy (CCE) loss. For
normal training without AUX, we report the test accuracy at the epoch where validation accuracy is
maximum (early stopping). For training with AUX, we report the test accuracy at the last epoch as
well as the best epoch. Figure 2 shows the training curves for noise rate 0.4. We observe that training
with AUX achieves very good test accuracy - even better than the best accuracy of normal training
with early stopping, and better than the recent method of Zhang and Sabuncu (2018) using the same
7
Published as a conference paper at ICLR 2020
Noise rate	0	0.2	0.4	0.6
Normal CCE (early stop)	94.05±0.07	89.73±0.43	86.35±0.47	79.13±0.41
Normal MSE (early stop)	93.88±0.37	89.96±0.13	85.92±0.32	78.68±0.56
CCE+AUX (IaSt)	94.22±0.10	92.07±0.10	87.81 ±0.37	82.60±0.29
CCE+AUX (best)	94.30±0.09	92.16±0.08	88.61±0.14	82.91±0.22
MSE+AUX (IaSt)	94.25±0.10	92.31±0.18	88.92±0.30	83.90±0.30
MSE+AUX (best)一	94.32±0.06^~	92.40±0.18	88.95±0.31	83.95±0.30
(Zhang and Sabuncu, 2018)	-	89.83±0.20~~	87.62±0.26^~	82.70±0.23~~
Table 1: CIFAR-10 test accuracies of different methods under
different noise rates.
Figure 2: Test accuracy during
CIFAR-10 training (noise 0.4).
∙0∙8f 4
578577577577
UUoN snglθqα!h
Figure 3: Setting 2, }W p4q }F and }Wp4q ´ W p4q p0q}F during training. Noise “ 20%, λ “ 4.
architecture (ResNet-34). Furthermore, AUX does not over-fit (the last epoch performs similarly to
the best epoch). In addition, we find that in this setting classification performance is insensitive of
whether MSE or CCE is used as the loss function.
6.2	Distance of Weights to Initialization, Verification of the NTK Regime
We also track how much the weights move during training as a way to see whether the neural net is in
the NTK regime. For Settings 1 and 2, we find that the neural nets are likely in or close to the NTK
regime because the weight movements are small during training. Figure 3 shows in Setting 2 how
much the 4-th layer weights move during training. Additional figures are provided as Figures 6 to 8.
Table 2 summarizes the relationship between the distance to initialization and other hyper-parameters
that we observe from various experiments. Note that the weights tend to move more with larger noise
level, and AUX and RDI can reduce the moving distance as expected (as shown in Figure 3).
The ResNet-34 in Setting 3 is likely not operating in the NTK regime, so its effectiveness cannot yet
be explained by our theory. This is an intriguing direction of future work.
	# samples	noise level	width	regularization strength λ	learning rate
Distance	/	/	—	∖	—
Table 2: Relationship between distance to initialization at convergence and other hyper-parameters. “/”:
positive correlation; “\”: negative correlation; '—': no correlation as long as width is sufficiently large and
learning rate is sufficiently small.
7	Conclusion
Towards understanding generalization of deep neural networks in presence of noisy labels, this paper
presents two simple regularization methods and shows that they are theoretically and empirically
effective. The theoretical analysis relies on the correspondence between neural networks and NTKs.
We believe that a better understanding of such correspondence could help the design of other
principled methods in practice. We also observe that our methods can be effective outside the NTK
regime. Explaining this theoretically is left for future work.
Acknowledgments
This work is supported by NSF, ONR, Simons Foundation, Schmidt Foundation, Mozilla Research,
Amazon Research, DARPA and SRC. The authors thank Sanjeev Arora for helpful discussions and
suggestions. The authors thank Amazon Web Services for cloud computing time.
8
Published as a conference paper at ICLR 2020
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized
neural networks, going beyond two layers. arXiv preprint arXiv:1811.04918, 2018a.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. arXiv preprint arXiv:1811.03962, 2018b.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. On
exact computation with an infinitely wide neural net. arXiv preprint arXiv:1904.11955, 2019a.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. arXiv preprint
arXiv:1901.08584, 2019b.
Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. Journal of Machine Learning Research, 3(Nov):463-482, 2002.
Frank Bauer, Sergei Pereverzev, and Lorenzo Rosasco. On regularization algorithms in learning
theory. Journal ofcomplexity, 23(1):52-72, 2007.
Yuan Cao and Quanquan Gu. A generalization theory of gradient descent for learning over-
parameterized deep relu networks. arXiv preprint arXiv:1902.01384, 2019.
Lenaic Chizat and Francis Bach. A note on lazy training in supervised differentiable programming.
arXiv preprint arXiv:1812.07956, 2018.
Simon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning deep homogeneous
models: Layers are automatically balanced. In Advances in Neural Information Processing Systems
31, pages 382-393. 2018a.
Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. arXiv preprint arXiv:1811.03804, 2018b.
Simon S Du, Kangcheng Hou, Barnabas P6czos, Ruslan Salakhutdinov, Ruosong Wang, and Keyulu
Xu. Graph neural tangent kernel: Fusing graph neural networks with graph kernels. arXiv preprint
arXiv:1905.13192, 2019a.
Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In International Conference on Learning Representations,
2019b.
L Lo Gerfo, Lorenzo Rosasco, Francesca Odone, E De Vito, and Alessandro Verri. Spectral algorithms
for supervised learning. Neural Computation, 20(7):1873-1897, 2008.
Aritra Ghosh, Himanshu Kumar, and PS Sastry. Robust loss functions under label noise for deep
neural networks. In Thirty-First AAAI Conference on Artificial Intelligence, 2017.
Melody Y Guan, Varun Gulshan, Andrew M Dai, and Geoffrey E Hinton. Who said what: Modeling
individual labelers improves classification. In Thirty-Second AAAI Conference on Artificial
Intelligence, 2018.
Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi
Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. In
NeurIPS, pages 8527-8537, 2018.
Daniel Hsu, Sham Kakade, Tong Zhang, et al. A tail inequality for quadratic forms of subgaussian
random vectors. Electronic Communications in Probability, 17, 2012.
Arthur Jacot, Franck Gabriel, and CIement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. arXiv preprint arXiv:1806.07572, 2018.
Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. Mentornet: Learning
data-driven curriculum for very deep neural networks on corrupted labels. arXiv preprint
arXiv:1712.05055, 2017.
9
Published as a conference paper at ICLR 2020
Ranjay A Krishna, Kenji Hata, Stephanie Chen, Joshua Kravitz, David A Shamma, Li Fei-Fei, and
Michael S Bernstein. Embracing error to enable rapid crowdsourcing. In Proceedings of the 2016
CHI conference on human factors in computing Systems, pages 3167-3179. ACM, 2016.
Jaehoon Lee, Lechao Xiao, Samuel S Schoenholz, Yasaman Bahri, Jascha Sohl-Dickstein, and Jeffrey
Pennington. Wide neural networks of any depth evolve as linear models under gradient descent.
arXiv preprint arXiv:1902.06720, 2019.
Mingchen Li, Mahdi Soltanolkotabi, and Samet Oymak. Gradient descent with early stopping is prov-
ably robust to label noise for overparameterized neural networks. arXiv preprint arXiv:1903.11680,
2019.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. arXiv preprint arXiv:1808.01204, 2018.
Tongliang Liu and Dacheng Tao. Classification with noisy labels by importance reweighting. IEEE
Transactions on pattern analysis and machine intelligence, 38(3):447-461, 2015.
Eran Malach and Shai Shalev-Shwartz. Decoupling" when to update" from" how to update". In
Advances in Neural Information Processing Systems, pages 960-970, 2017.
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning. MIT
Press, 2012.
Vaishnavh Nagarajan and J Zico Kolter. Generalization in deep networks: The role of distance from
initialization. arXiv preprint arXiv:1901.01672, 2019.
Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan Srebro. The role
of over-parametrization in generalization of neural networks. In International Conference on
Learning Representations, 2019.
Garvesh Raskutti, Martin J Wainwright, and Bin Yu. Early stopping and non-parametric regression:
an optimal data-dependent stopping rule. The Journal of Machine Learning Research, 15(1):
335-366, 2014.
Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning to reweight examples for
robust deep learning. arXiv preprint arXiv:1803.09050, 2018.
David Rolnick, Andreas Veit, Serge Belongie, and Nir Shavit. Deep learning is robust to massive
label noise. arXiv preprint arXiv:1705.10694, 2017.
Sainbayar Sukhbaatar, Joan Bruna, Manohar Paluri, Lubomir Bourdev, and Rob Fergus. Training
convolutional networks with noisy labels. arXiv preprint arXiv:1406.2080, 2014.
Yuting Wei, Fanny Yang, and Martin J Wainwright. Early stopping for kernel boosting algorithms:
A general analysis with localized complexities. In Advances in Neural Information Processing
Systems, pages 6065-6075, 2017.
Greg Yang. Scaling limits of wide neural networks with weight sharing: Gaussian process behavior,
gradient independence, and neural tangent kernel derivation. arXiv preprint arXiv:1902.04760,
2019.
Xingrui Yu, Bo Han, Jiangchao Yao, Gang Niu, Ivor Tsang, and Masashi Sugiyama. How does
disagreement help generalization against label corruption? In International Conference on Machine
Learning, pages 7164-7173, 2019.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In Proceedings of the International Conference
on Learning Representations (ICLR), 2017, 2017.
Zhilu Zhang and Mert Sabuncu. Generalized cross entropy loss for training deep neural networks
with noisy labels. In NeurIPS, pages 8778-8788, 2018.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes
over-parameterized deep ReLU networks. arXiv preprint arXiv:1811.08888, 2018.
10
Published as a conference paper at ICLR 2020
A	Difference Trick
We give a quick analysis of the “difference trick” described in Footnote 2, i.e., let f (θ, x) “
警g(θι, x)—警g(θ2, x) where θ “ (θι, θ2), and initialize θι and θ2 to be the same (and still
random). The following lemma implies that the NTKs for f andg are the same.
Lemma A.1. If θ1(0) “ θ2 (0), then
(i)	f (θ(0), x) “ 0, @x;
(ii)	<Vθf (θ(0), x), Vθf (θ(0), x1)y “ <Vθιg(θι(0), x), ▽/g(θι(0), x1)〉, @x, x1.
Proof. (i) holds by definition. For (ii), we can calculate that
@Vθ f (θ(0), x), Vθ f (θ(0), x1)D
“〈Vo"(θ(0), x), Vθιf (θ(0), x1)D +〈Vo2/(θ(0), x), Vθ2f (θ(0), x1)D
“ 1 <Vθιg(θι(0), x), Vθιg(θι(0), x)〉+ 2〈Ve,g(θ2(0), x), Ngg(θ2(0), x1)〉
“ @Veig(θι(0),x), vθig(θι(0), x1)〉.
□
The above lemma allows us to ensure zero output at initialization while preserving NTK. As a
comparison, Chizat and Bach (2018) proposed the following "doubling trick": neurons in the last
layer are duplicated, with the new neurons having the same input weights and opposite output weights.
This satisfies zero output at initialization, but destroys the NTK. To see why, note that with the
“doubling trick", the network will output 0 at initialization no matter what the input to its second to
last layer is. Thus the gradients with respect to all parameters that are not in the last two layers are 0.
In our experiments, we observe that the performance of the neural net improves with the “difference
trick.” See Figure 4. This intuitively makes sense, since the initial network output is independent of
the label (only depends on the input) and thus can be viewed as noise. When the width of the neural
net is infinity, the initial network output is actually a zero-mean Gaussian process, whose covariance
matrix is equal to the NTK contributed by the gradients of parameters in its last layer. Therefore,
learning an infinitely wide neural network with nonzero initial output is equivalent to doing kernel
regression with an additive correlated Gaussian noise on training and testing labels.
4 3 2
(％)JOXW ⅛ΦH
0.0 0.1 0.2 0.3 0.4 0.5
a
Figure 4: Plot of test error for fully connected two-layer network on MNIST (binary classification
between “5” and “8”) with difference trick and different mixing coefficients α, where f(θ1, θ2, x) “
aαg(θι, x) — ʌ/ɪ´ɪg(θι, x). Note that this parametrization preserves the NTK. The network has
10,000 hidden neurons and we train both layers with gradient descent with fixed learning rate for
5,000 steps. The training loss is less than 0.0001 at the time of stopping. We observe that when α
increases, the test error drops because the scale of the initial output of the network goes down.
11
Published as a conference paper at ICLR 2020
B Missing Proof in Section 4
Proofof Theorem 4.1. The gradient of LAUX(θ, b) can be written as
Vθ L Aux (θ, bq = 2n,aiφ(xi),	VbiL Aux(Θ, bq = X@, i = 1,...,n,
i“1
where ai	=	φ(xi)J(θ	—	θ(0)) ' λb —	y%(i	P	[n]).	Therefore We have VeLAUX(θ,b)=
Xn“i 1 Vbi LAUX (θ, bq ∙ φ(x). Then, according to the gradient descent update rule (7), we know that
θ(t) and b(tq can always be related by θ(t) = θ(0) ' Xn“11 bi(t)∙ φ(xi). It follows that
θ(t + ιq = θ(tq — η Xn=1 (Φ(χi)J(θ(t) — θ(oqq + 》仇⑴—命)。出)
=θ(t)	— ηXn=1	(Φ(χi)J(θ(t) —	θ(oqq	—	yi) φ(χiq	— ηλ£n=1 bi(t)φ(χi)
“θ(tq	— η Xn=1	(Φ(χi)J(θ(t) —	θ(oqq	—2 eg	— ηλ2(θ(tq — θ(oqq.
On the other hand, from (6) we have
θ(t + ιq = θ(tq — η Xi=1 (Φ(χi)J(θ(t) — θ(oqq —房)。(©)— ηλ2(θ(tq — θ(oqq.
Comparing the above two equations, we find that {θ(t)} and {θ(t)} have the same update rule. Since
θ(0q = θ(0),this proves θ(t) = θ(t) for all t.
Now we prove the second part of the theorem. Notice that LRDI (θ) is a strongly convex quadratic
function with Hessian VθLRDI (θ) = Xn= ι φ(xi)φ(xi)J + λ2I = ZZJ + λ2I, where Z =
(φ(xι),..., φ(xnqq. From the classical convex optimization theory, as long as η ≤ }zzj'xi}=
}zJz'λ2i} = }k(x X)}'『,gradient descent converges linearly to the unique optimum θ* of
LλDI(θ), which can be easily obtained:
θ* “ θ(0) + z(k(x, x q + λ2ι )Ty.
Then we have
Φ(χ)J(θ* — θ(0qq = φ(xqJz (k(x, x q + λ2ι q—1y = k(x, X qJ 'k(x, X q + λ2ι )´1 y,
finishing the proof.	□
C Missing Proofs in Section 5
C.1 Proof of Theorem 5.1
Define εi = yi — yi (i P [n]), and ε = (ει,..., εn)J = y — y.
We first prove two lemmas.
Lemma C.1. With probability at least 1 — δ, we have
∖
n	λ ,---------------
(f*(xi) — yi)2 ≤ 5A∕yJ(k(X, X))Ty + Iatrrk(X, X)] + σ'2 log(1∕δ).
2	2λ
i“1
Proof. In this proof we are conditioned on X and y, and only consider the randomness in y given
X and y.
First of all, we can write
(f *(χιq,...,f *(χnqqJ = k(X, X q 'k(X, X q + λ2ι )´1 y,
12
Published as a conference paper at ICLR 2020
so we can write the training loss on true labels y as
∖
n
∑ (f*(χi) ´ yiq2 “ >k(x, Xq 'k(x,Xq + λI)´1 y ´ y
i“1
“ >k(X, Xq 'k(X, Xq + λI)´1 (y + e)´ y>
“ >k(X, Xq 'k(X, Xq + λI)´1 ε ´ λ 'k(X, Xq + λι)´1 y>
W >k(X, Xq 'k(X, Xq + λI)´1 ε> + λ2 > 'k(X, Xq + λl)´1 y> .
(11)
Next, since ε, conditioned on X and y, has independent and subgaussian entries (with parameter σ),
by (Hsu et al., 2012), for any symmetric matrix A, with probability at least 1 ´ δ,
}Aε} ≤ σ JtrrA2S + 2atT[A4S log(1∕δ) + 2}A2} log(1∕δ).
(12)
Let A “ k(X, X q 'k(X, X q + λ2I )T and let λ1,..
We have
n2
trrA2 S = ∑1 (λ"+λψ
n4
trrA4 S= ∑1 (λ"+iλψ
}A2} W 1.
n
W ∑
i“1
n
W∑
i“1
.,λn > 0 be the eigenvalues of k(X, Xq.
λ2	“ tr[k(X, X qs
4λi ∙ λ2 —	4λ2	,
λ4 Ltrrk(X, X qs
3 W
44λ2 '⅜)3
(13)
9λ2
Therefore,
k(x, x q (k(x, x q ` λ2ι)
4λ2
trrkpx, xqs
trrkpX, Xqs
4λ2
+2
trrkpX, Xqs logp1{δq
9λ2
` 2 logp1{δq
+ a2log(1∕δ)).
Finally, since 'k(X, X) + λ2I)´2 M +(k(X, X))´1 (note (% + λ2)2 2 4% ∙ λ2), we have
λ2 >'k(X,Xq + λ2ι)´1 y> = λ2JyJ (k(X,Xq + λ2ιq∙ y W 2bJk(XXqq´y. (14)
The proof is finished by combining (11), (13) and (14).	□
Let H be the reproducing kernel Hilbert space (RKHS) corresponding to the kernel k(∙, ∙q. Recall
that the RKHS norm of a function f(xq = αJk(x, Xq is
}f }h = baJk(X, X qa.
Lemma C.2. With probability at least 1 ´ δ, we have
}f *}h WbyJ (k(X, X q + λ21 )´z + λ (？n + a2i0g(i∕δq)
Proof. In this proof we are still conditioned on X and y, and only consider the randomness in
y given X and y. Note that f *(x) = αjk(x, Xq with a = 'k(X, Xq + λ2I) 1 y. Since
'k(X, Xq + λ2I)´1 M (k(X, Xqq´1 and 'k(X, Xq + λ2I)´1 W λ∑ I, we can bound
}f *}h = JyJ (k(X, X q + λ2I q-1 k(X, X q(k(X, X q + λ2I q-1 y
W J(y + εqj (k(X, Xq + λ2Iq-1 (y + εq
13
Published as a conference paper at ICLR 2020
≤ JyJ PkpX, X) + λ2I)Ty + JεJ PkpX, X) + λ21)-％
& bJ PkpX, X) + λ2I)´1 y + ^εJε.
λ
Since εhas independent and subgaussian (with parameter σ) coordinates, using (12) with A “ I,
with probability at least 1 ´ δ we have
?EJE ≤ σ (?n + a% log(1∕δ)) .	□
Now we prove Theorem 5.1.
Proof of Theorem 5.1. First, by Lemma C.1, with probability 1 ´ δ∕3,
∖
n	λ /----------------
∑ pf*pχi) ´ yi)2 ≤ 5 JyJpkpX, X))´1 y + A/平(X, X)S + σθ2∖ogp3∕δ),
2	2λ
i“1
which implies that the training error on the true labels under loss function ` is bounded as
1n
n ∑
i“1
1n
'pf*pχi),y) “ 一	p'pf*3")—'@,仍))
n i“1
1n
≤ — ∑lf ,g)´ yil
n i“1
n
1
≤十
?、∑ lf*(xi) — y，l
√n∖ i“1
2
≤ λ JJpkpX, X))Ty
trrkpX, X)s
+ σλ ∕2log≡
n
n
+ K
n
Next, for function class FB “ tf px) “ αJkpx, X) : }f}H ≤ Bu, Bartlett and Mendelson (2002)
showed that its empirical Rademacher complexity can be bounded as
n
1
R S pF B ) fi — E
SUp y f pχi)γi
n γ~{±1}n f PFB i“11
B	tr[k(X, X)S
≤
n
By Lemma C.2, with probability at least 1 ´ δ∕3 we have
}f *}h ≤ JyJ pkpX, X) + λ2I)Ty + λ (？n + a2logp3∕δ)) fi B1.
We also recall the standard generalization bound from Rademacher complexity (see e.g. (Mohri et al.,
2012)): with probability at least 1 ´ δ, we have
sup {E(x,y)~Dr`pf px),y)S ´ n W'pf pXi),yi)∣ ≤ 2RSpF) + 3↑l^0pnδ.	(15)
Then it is tempting to apply the above bound on the function class FB1 which contains f*. However,
We are not yet able to do so, because B1 depends on the data X and y. To deal with this, We use a
standard e-net argument on the interval that B1 must lie in: (note that }y} “ Op?n))
B‘ P ”λ" + a2log≡) ,λ" + a2log≡)+O ´n)].
The above interval has length O (λ), so its e-net N has size O (3).Using a union bound, we apply
the generalization bound (15) simultaneously on FB for all B P N: With probability at least 1 ´ δ∕3
we have
SUp E(χ,y)~Dr`pf px),y)S — 1 £ 'pf pXi),yi)	≤ 2RRs(FB)
fBPF	ni“1
@B P N .
14
Published as a conference paper at ICLR 2020
By definition there exists B P N such that B1 ≤ B ≤ B1 ` e. Then We also have f * P FB. Using
the above bound on this particular B , and putting all parts together, we know that with probability at
least 1 ´ δ,
E(x,y)~D r`(f *(χ),y)S
W λ{yT(k(X, X))Ty
tr[k(X, X)] `
+葭
∕2J⅛Zδ)
n
n
n
+ 2AZtrrknX, Xqs (JyT (k(X, X))Ty + λ " + a2log(3∕δ)
“ λ ∕yJ(k(X, X))-1y ` 5σ 优[k(X, X)]
“2 V	n	' 2λ V
+ σ∖ ∕2J⅛zδ)
n
n
n
+ 2√tr[k(X, X)]
n
(JyT (k(X, X))Ty + λa2log(3∕δ)
Then, using trrk(X, X)] “ O(n) and choosing e “ 1, We obtain
E(x,y)~D r`(f *(x),y)]
W
+
W λ{yJ(k(X, X))Ty
yτ (k(X, X ))Ty
2log(3∕δ)
n
)+O (λ⅛ a2"。))+O ^ W)+O
λ +。⑴(yτ(k(X, X ))Ty
+O
log(1∕δ) + σ ∕log(1∕δ) +
n λn
n
n
n
□
C.2 Proof of Theorem 5.2
Proof of Theorem 5.2. We Will apply Theorem 5.1 in this proof. Note that We cannot directly apply
it on t(xi, yi, yj} because the mean of Qi — yi is non-zero (conditioned on (xi, yi)). Nevertheless,
this issue can be resolved by considering {(xi, (1 — 2p)yi, y∕} instead.4 Then we can easily check
that conditioned on y%, y% — (1 — 2p)yi has mean 0 and is subgaussian with parameter σ “ O(?p).
With this change, we are ready to apply Theorem 5.1.
Define the following ramp loss for U P R, y p {±(1 — 2p)}:
$ (1 — 2p),	Uy W 0,
'ramp(u, y) “ ∖ (1 — 2p) ― ɪuy, 0 < Uy < (1 — 2p)2,
、0,	Uy > (1 — 2p)2.
It is easy to see that 'ramp(u, y) is 1-Lipschitz in U for y P {±(1 — 2p)}, and satisfies 'ramp(y, y) “ 0
for y p t±(1 — 2p)}. Then by Theorem 5.1, with probability at least 1 — δ,
E(x,y)„D r`ramp(f *(x), (1 — 2p)y)]
4For binary classification, only the sign of the label matters, so we can imagine that the true labels are from
{±(1 — 2p)} instead of {±1}, without changing the classification problem.
15
Published as a conference paper at ICLR 2020
≤
V
λ ` opιq
2
(1 — 2p)yJ(k(X, X))T∙(1 — 2p)y
n
` O
Plog(1∕δ)
n
(1 —2p)(λ + O(1))
2
yJ(k(x, X))Ty
n
` O
P log(1∕δ)
n

Note that `ramp also bounds the 0-1 loss (classification error) as
'ramp(u, (1 — 2p)y) > (1 — 2p)I[sgn(u) ‰ sgn(y)S = (1 — 2p)I[sgn(u) ‰ y], @u P R, y P {±1}.
Then the conclusion follows.	□
C.3 Proof of Theorem 5.3
ProofofTheorem 5.3. By definition, we have E^i a]=E[ep3∣CiS = Pci (the ci-th column of
P). This means ErY |Qs = Q. Therefore we can view Q as an encoding of clean labels, and then
the observed noisy labels Y is Q plus a zero-mean noise. (The noise is always bounded by 1 so is
subgaussian with parameter 1.) This enables us to apply Theorem 5.1 to each fphq, which says that
with probability at least 1 — δ1 ,
E(x,c)~D IfPhq(X) — Ph,cl ≤
λ + O(1) ((qphq)J(k(X, X))Tqph)
' θ I 1 +
n
Letting δ1 = K and taking a union bound over h P [K], we know that the above bound holds for
every h simultaneously with probability at least 1 — δ.
Now we proceed to bound the classification error. Note that c R argmaxhPrK s f(hq (x) implies
Xh“1 ∣fPh)(x) — Ph,c∣ > gap. Therefore the classification error can be bounded as
Px,Pc)r„D cRargmaxhPrKsfPh)(X) ≤ Px,Pc)r„D
≤
KI	I
S fPh)(X) — Ph,cl > gap
h“1
K
—E(x,c)~D Σ |fPh)(x)-
gap
h“1
ph,c
h“1
Px,c)„D If Ph) (X) — Ph,cI
K
K
I =嬴 S E
K
≤λɪ≡SJ
gaP ∖	2	h⅛'
(qphq)J(k(X, X))TqPh)
-------------------------H K ∙ O
n
completing the proof.
D Experiment Details and Additional Figures
In Setting 1, we train a two-layer neural network with 10,000 hidden neurons on MNIST (“5” vs “8”).
In Setting 2, we train a CNN, which has 192 channels for each of its 11 layers, on CIFAR (“airplanes”
vs “automobiles”). We do not have biases in these two networks. In Setting 3, we use the standard
ResNet-34.
In Settings 1 and 2, we use a fixed learning rate for GD or SGD, and we do not use tricks like batch
normalization, data augmentation, dropout, etc., except the difference trick in Appendix A. We also
freeze the first and the last layer of the CNN and the second layer of the fully-connected net.5
In Setting 3, we use SGD with 0.9 momentum, weight decay of 5 X 10-4, and batch size 128. The
learning rate is 0.1 initially and is divided by 10 after 82 and 123 epochs (164 in total). Since we
5This is because the NTK scaling at initialization balances the norm of the gradient in each layer, and the
first and the last layer weights have smaller norm than intermediate layers. Since different weights are expected
to move the same amount during training (Du et al., 2018a), the first and last layers move relatively further from
their initializations. By freezing them during training, we make the network closer to the NTK regime.
16
Published as a conference paper at ICLR 2020
observe little over-fitting to noise in the first stage of training (before learning rate decay), we restrict
the regularization power of AUX by applying weight decay on auxiliary variables, and dividing their
weight decay factor by 10 after each learning rate decay.
See Figures 5to8 for additional results for Setting 2 with different λ's.
(a) λ “ 0.25
(b) λ “ 0.5
(c) λ “ 1
(d) λ “ 2
(e) λ “ 4
Figure 5: Training (dashed) & test (solid) errors vs. epoch for Setting 2. Noise rate “ 20%,
λ P t0.25, 0.5, 1, 2, 4, 8u. Training error of AUX is measured with auxiliary variables.
(f) λ “ 8
17
Published as a conference paper at ICLR 2020
∙0
∙5
Z
∙0∙5∙0
520
00
Ch
O
P
OE
uo-toZI{esUI 0*jφouto*j~α
575.4
575.2
575.0
574.8
00
6
O
40h
Poc
OoE
2
UuoN s∙≡uωqoJH
6
O
Figure 6:	Setting 2, }W p7q }F and }W p7q ´ Wp7qp0q}F during training. Noise rate “ 20%, λ “ 4.
601
5
4
3
O
1
O
UOnENfenvul Oa 8ue4Q
πuoN snπlα,qojj
Figure 7:	Setting 2, }W p4q }F and }W p4q ´ Wp4qp0q}F during training. Noise rate “ 0, λ “ 2.
∏uoN snEaqalH
Figure 8: Setting 2, }W p7q }F and }W p7q ´ Wp7qp0q}F during training. Noise rate “ 0, λ “ 2.
0-
0	100 200 300 400 500 600
Epoch
18