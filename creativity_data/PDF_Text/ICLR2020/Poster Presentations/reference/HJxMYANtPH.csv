title,year,conference
 A convergence theory for deep learning via over-parameterization,2019, In ICML
 Implicit regularization in deep matrixfactorization,2019, arXiv preprint arXiv:1905
 Onexact computation with an infinitely wide neural net,2019, arXiv preprint arXiv:1904
 On deep learning as a remedy for the curse of dimensionalityin nonparametric regression,2019, The Annals of Statistics
 Stability and generalization,2002, JMLR
 Globally optimal gradient descent fora ConvNet with GaussianinpUts,2017, In ICML
 Efficient approximation of deep relUnetworks for fUnctions on low dimensional manifolds,2019, arXiv preprint arXiv:1908
 On the global convergence of gradient descent for over-parameterized models Using optimal transport,2018, In NIPS
 Theloss sUrfaces of mUltilayer networks,2015, JMLR
 SGD learns the conjUgate kernel class of the network,2017, In NIPS
 ImageNet: A large-scalehierarchical image database,2009, In CVPR
 Training neUral networks as learning data-adaptive kernels:Provable representation and approximation benefits,2019, arXiv preprint arXiv:1901
 Gradient descent finds globalminima of deep neUral networks,2019, In ICML
 Gradient descent provably optimizesover-parameterized neUral networks,2018, arXiv preprint arXiv:1810
 Local polynomial modelling and its applications: monographs on statistics and ap-plied probability 66,2018, RoUtledge
 Classifi-cation regions of deep neUral networks,2017, arXiv preprint arXiv:1705
 EmpiricalstUdy of the topology and geometry of deep networks,2018, In CVPR
 Stiffness: Anew perspective on generalization in neural networks,2019, arXiv preprint arXiv:1901
 Linearized two-layersneural networks in high dimension,2019, arXiv preprint arXiv:1904
 Modelling the influence ofdata structure on learning in neural networks,2019, arXiv preprint arXiv:1909
 Deep residual learning for image recog-nition,2016, In CVPR
 Multilayer feedforward networks are uni-versal approximators,1989, Neural Networks
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, In ICML
 Neural tangent kernel: Convergence and gen-eralization in neural networks,2018, In NIPS
 Approximation by combinations of relu and squaredrelu ridge functions with `1 and `0 controls,2018, IEEE Transactions on Information Theory
 Understanding black-box predictions via influence functions,2017, InICML
 Learning multiple layers of features from tiny images,2009, Masterâ€™s thesis
 ImageNet classification with deep convo-lutional neural networks,2012, In NIPS
 Data-dependent stability of stochastic gradient descent,2018, InICML
 Wide neural networks of any depth evolve as linear models under gradientdescent,2019, arXiv preprint arXiv:1902
 Learning overparameterized neural networks via stochastic gradientdescent on structured data,2018, In NIPS
 Adding one neuron can eliminateall bad local minima,2018, In NIPS
 Deep vs,2016, shallow networks: An approximation theoryperspective
 On the number of linearregions of deep neural networks,2014, In NIPS
 Geometry of opti-mization and implicit regularization in deep learning,2017, arXiv preprint arXiv:1705
 Towards moderate overparameterization: global conver-gence guarantees for training shallow neural networks,2019, arXiv preprint arXiv:1902
 Nonlinear dimensionality reduction by locally linear embed-ding,2000, Science
 Nonparametric regression using deep neural networks with relu activationfunction,2017, arXiv preprint arXiv:1708
 Mean field analysis of neural networks: A centrallimit theorem,2019, Stochastic Processes and their Applications
 Learning ReLUs via gradient descent,2017, In NIPS
 Theoretical insights into the optimizationlandscape of over-parameterized shallow neural networks,2018, IEEE Transactions on InformationTheory
 A mean field view of the landscape of two-layersneural networks,2018, Proceedings of the National Academy of Sciences
 Exponentially vanishing sub-optimal local minima in multilayerneural networks,2017, arXiv preprint arXiv:1702
 Adaptivity of deep relu network for learning in Besov and mixed smooth Besov spaces:optimal rate and curse of dimensionality,2018, arXiv preprint arXiv:1810
 benefits of depth in neural networks,2016, In COLT
 An analytical formula of population gradient for two-layered ReLu network andits applications in convergence and critical point analysis,2017, In ICML
 Regularization matters: Generalization andoptimization of neural nets v,2018,s
 Error bounds for approximations with deep relu networks,2017, Neural Networks
 Understandingdeep learning requires rethinking generalization,2017, ICLR
 Critical points of neural networks: Analytical forms and landscapeproperties,2017, arXiv preprint arXiv:1710
 Stochastic gradient descent optimizesover-parameterized deep ReLU networks,2018, arXiv preprint arXiv:1811
