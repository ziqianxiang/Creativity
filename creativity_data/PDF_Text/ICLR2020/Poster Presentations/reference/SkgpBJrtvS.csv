title,year,conference
 Variationalinformation distillation for knowledge transfer,2019, In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition
 A theoretical analysis of contrastive unsupervised representation learning,2019, arXiv preprintarXiv:1902
 Soundnet: Learning sound representations fromunlabeled video,2016, In Advances in neural information processing systems
 Imagenet: A large-scalehierarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 On distinguishability criteria for estimating generative models,2014, arXiv preprintarXiv:1412
 Noise-contrastive estimation: A new estimation principlefor unnormalized statistical models,2010, In Proceedings of the Thirteenth International Conference onArtificial Intelligence and Statistics
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Knowledge transfer via distillationof activation boundaries formed by hidden neurons,2019, In Proceedings of the AAAI Conference onArtificial Intelligence
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Learning with side information through modalityhallucination,2016, In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
 Cross-modaladaptation for rgb-d detection,2016, In 2016 IEEE International Conference on Robotics and Automation(ICRA)
 Paraphrasing complex network: Network compressionvia factor transfer,2018, In Advances in Neural Information Processing Systems
 Lit: Learned intermediaterepresentation training for model compression,2019, In International Conference on Machine Learning
 Knowledge distillation by on-the-fly native ensemble,2018, InAdvances in neural information processing systems
 Representation learning with contrastive predictivecoding,2018, arXiv preprint arXiv:1807
 Learning deep representations with probabilistic knowledgetransfer,2018, In Proceedings of the European Conference on Computer Vision (ECCV)
 Correlation congruence for knowledge distillation,2019, arXiv preprint arXiv:1904
 Fitnets: Hints for thin deep nets,2014, arXiv preprint arXiv:1412
 Mo-bilenetv2: Inverted residuals and linear bottlenecks,2012, In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition
 Very deep convolutional networks for large-scale imagerecognition,2019, arXiv preprint arXiv:1409
 Similarity-preserving knowledge distillation,2019, arXiv preprintarXiv:1907
 Unsupervised feature learning via non-parametric instance discrimination,2018, In Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition
 Paying more attention to attention: Improving the perfor-mance of convolutional neural networks via attention transfer,2016, arXiv preprint arXiv:1612
 Wide residual networks,2016, arXiv preprint arXiv:1605
 Split-brain autoencoders: Unsupervised learningby cross-channel prediction,2017, In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition
 Shufflenet: An extremely efficientconvolutional neural network for mobile devices,2018, In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition
