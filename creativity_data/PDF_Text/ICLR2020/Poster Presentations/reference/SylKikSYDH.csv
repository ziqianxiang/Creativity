title,year,conference
 Character-level language modeling withdeeper self-attention,2019, In Proceedings of the AAAI Conference on Artificial Intelligence
 Adaptive input representations for neural language modeling,2019, arXivpreprint arXiv:1809
 Neural machine translation by jointly learning to align andtranslate,2014, arXiv preprint arXiv:1409
 Trellis networks for sequence modeling,2018, arXiv preprintarXiv:1810
 DeePmind lab,2016, CoRR
 Latent dirichlet allocation,1532, J
 Quasi-recurrent neural networks,2016, arXiv preprintarXiv:1611
 One bil-lion word benchmark for measuring Progress in statistical language modeling,2013, arXiv preprintarXiv:1312
 Hierarchical multiscale recurrent neural networks,2016, arXiv preprintarXiv:1609
 Transformer-xl: Attentive language models beyond a fixed-length context,2019, arXiv preprint arXiv:1901
 Language modeling with gated convolutionalnetworks,2016, arXiv preprint arXiv:1612
 Bert: Pre-training of deeP bidirectional trans-formers for language understanding,2018, arXiv preprint arXiv:1810
 Generating sequences with recurrent neural networks,2013, arXiv preprint arXiv:1308
 Neural turing machines,2014, arXiv preprint arXiv:1410
 Hybrid comPuting using a neural networkwith dynamic external memory,2016, Nature
 The goldilocks PrinciPle: Reading childrenâ€™s bookswith exPlicit memory rePresentations,2015, arXiv preprint arXiv:1511
 Long short-term memory,1997, Neural computation
 The curious case of neural text degeneration,2019, arXivpreprint arXiv:1904
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Thenarrativeqa reading comprehension challenge,2018, Transactions of the Association for ComputationalLinguistics
 Multiplicative lstm for sequence modelling,2016, arXivpreprint arXiv:1609
 Dynamic evaluation of transformer languagemodels,2019, CoRR
 Large memory layers withproduct keys,2019, arXiv preprint arXiv:1907
 Pointer sentinel mixture models,2016, arXiv preprintarXiv:1609
 Recurrent neural networkbased language model,2010, In Eleventh Annual Conference of the International Speech CommunicationAssociation
 Parallel wavenet: Fast high-fidelity speech synthesis,2018, In InternationalConference on Machine Learning
 Fast parametric learning with activation memo-rization,2018, arXiv preprint arXiv:1803
 The persistence and transience of memory,2017, Neuron
 Learning representations by back-propagatingerrors,1986, Nature
 Attention is all you need,2017, In Advances in neural information processing systems
 Xlnet: Generalized autore-gressive pretraining for language understanding,2019, arXiv preprint arXiv:1906
