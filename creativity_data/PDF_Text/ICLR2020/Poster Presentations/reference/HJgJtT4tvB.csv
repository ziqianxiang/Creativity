title,year,conference
 Recognising textual entailment with logical inference,2005, In Proceedingsof the conference on Human Language Technology and Empirical Methods in Natural LanguageProcessing
 A large anno-tated corpus for learning natural language inference,2015, In Proceedings of the 2015 Conference onEmpirical Methods in Natural Language Processing
 The pascal recognising textual entailmentchallenge,2005, In Machine Learning Challenges Workshop
 Bert: Pre-training of deepbidirectional transformers for language understanding,2019, In Proceedings of the 2019 Conference ofthe North American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies
 Annotation artifacts in natural language inference data,2018, arXiv preprintarXiv:1803
 Mmm: Multi-stagemulti-task learning for multi-choice reading comprehension,2019, arXiv preprint arXiv:1910
 The narrativeqa reading comprehension challenge,2018, Transactions of theAssociation for Computational Linguistics
 Race: Large-scale readingcomprehension dataset from examinations,2017, arXiv preprint arXiv:1704
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 An extended model of natural logic,2009, In Proceedingsof the eighth international conference on computational semantics
 Probing neural network comprehension of natural languagearguments,2019, In Proceedings of the 57th Annual Meeting of the Association for ComputationalLinguistics
 Glove: Global vectors for wordrepresentation,2014, In Proceedings of the 2014 conference on empirical methods in natural languageprocessing (EMNLP)
 Languagemodels are unsupervised multitask learners,2019, OpenAI Blog
 Mctest: A challenge dataset forthe open-domain machine comprehension of text,2013, In Proceedings of the 2013 Conference onEmpirical Methods in Natural Language Processing
 Storycloze task: Uw nlp system,2017, In Proceedings of the 2nd Workshop on Linking Models of Lexical
 Overview of the ntcir-11 qa-labtask,2014, In Ntcir
 Dream: A challenge dataset and models for dialogue-based reading comprehension,2019, Transactions of the Association forComputational Linguistics
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Glue:A multi-task benchmark and analysis platform for natural language understanding,2018, In Proceedingsof the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks forNLP
 Evalai: Towards better evaluation systems for aiagents,2019, arXiv preprint arXiv:1902
 Xlnet: Generalized autoregressive pretraining for language understanding,2019, arXiv preprintarXiv:1906
 Aligning books and movies: Towards story-like visual explanations by watchingmovies and reading books,2015, In Proceedings of the IEEE international conference on computervision
