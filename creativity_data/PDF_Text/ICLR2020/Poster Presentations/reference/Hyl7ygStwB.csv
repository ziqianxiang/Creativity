title,year,conference
 Neural machine translation by jointlylearning to align and translate,2015, In 6th International Conference on Learning Representations
 Distilling the knowledge ofbert for text generation,2019, arXiv preprint arXiv:1911
 Latent alignment andvariational attention,2018, In Advances in Neural Inf)rmati)n Pr)cessing Systems
 Bert: Pre-training of deepbidirectional transformers for language understanding,2019, NAACL
 Classical struc-tured prediction losses for sequence to sequence learning,2018, NAACL
 Thedifficulty of training deep architectures and the effect of unsupervised pre-training,2009, In ArtificialIntelligence and Statistics
 Long short-term memory,0899, Neural Comput
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Cross-lingual language model pretraining,2019, NeurIPS
 Fractalnet: Ultra-deep neural net-works without residuals,2017, ICLR
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Document-level neuralmachine translation with hierarchical attention networks,2018, arXiv preprint arXiv:1809
 Bleu: a method for automaticevaluation of machine translation,2002, In Proceedings of the 40th annual meeting on association forcomputational linguistics
 Glove: Global vectors for wordrepresentation,2014, In Proceedings of the 2014 conference on empirical methods in natural languageprocessing (EMNLP)
 Deep contextualized word representations,2018, arXiv preprint arXiv:1802
 Languagemodels are unsupervised multitask learners,2019, OpenAI Blog
 Improving neural machine translation mod-els With monolingual data,2016, ACL
 Neural machine translation of rare Words WithsubWord units,2016, ACL
 The evolved transformer,2019, In Kamalika Chaudhuri and RuslanSalakhutdinov (eds
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Multi-agent dual learning,2019, ICLR
 Pay less attention WithlightWeight and dynamic convolutions,2019, In International Conference on Learning Representations
 Googleâ€™s neural machine trans-lation system: Bridging the gap betWeen human and machine translation,2016, arXiv preprintarXiv:1609
 Tied transformers: Neural machinetranslation With shared encoder and decoder,2019, In Proceedings of the AAAI Conference on ArtificialIntelligence
 Xlnet: Generalized autoregressive pretraining for language understanding,2019, arXiv preprintarXiv:1906
