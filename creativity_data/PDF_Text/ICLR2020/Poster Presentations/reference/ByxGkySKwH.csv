title,year,conference
 Certified adversarial robustness via randomizedsmoothing,2019, arXiv:1902
 Dropout as a bayesian approximation: Representing model uncertainty indeep learning,2016, In ICML
 Explaining and harnessing adversarial examples,2015, InICLR
 Generative adversarial nets,2014, In NeurIPS
 On calibration of modern neural networks,2017, In ICML
 Formal guarantees on the robustness of a classifier againstadversarial manipulation,2017, In NeurIPS
 Why ReLU networks yield high-confidencepredictions far away from the training data and how to mitigate the problem,2019, In CVPR
 A baseline for detecting misclassified and out-of-distribution examplesin neural networks,2017, In ICLR
 Deep anomaly detection with outlier exposure,2019, InICLR
 Early methods for detecting adversarial images,2017, In ICLRWorkshop Track Proceedings
 A baseline for detecting misclassified and out-of-distributionexamples in neural networks,2017, 2017c
 Auto-encoding variational bayes,2014, In ICLR
 Glow: Generative flow with invertible 1x1 convolutions,2018, InNeurIPS
 Adversarial examples for generative models,2017, In ICLR Workshop
 Simple and scalable predictive uncertaintyestimation using deep ensembles,2017, In NeurIPS
 Simple and scalable predictiveuncertainty estimation using deep ensembles,2017, In NeurIPS
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Training confidence-calibrated classifiers for detecting out-of-distribution samples,2018, In ICLR
 A simple unified framework for detecting out-of-distributionsamples and adversarial attacks,2018, In NeurIPS
 Training confidence-calibrated classifiers fordetecting out-of-distribution samples,2017, arXiv:1711
 A simple unified framework for detectingout-of-distribution samples and adversarial attacks,2018, In NeurIPS
 Leveraging uncertainty information fromdeep neural networks for disease detection,2017, Scientific Reports
 Enhancing the reliability of out-of-distribution image detection inneural networks,2018, In ICLR
 Enhancing the reliability of out-of-distribution imagedetection in neural networks,2017, arXiv:1706
 Open categorydetection with PAC guarantees,2018, In PMLR
 Towards deep learning models resistantto adversarial attacks,2018, In ICLR
 Differentiable abstract interpretation for provably robust neuralnetworks,2018, In ICML
 Readingdigits in natural images with unsupervised feature learning,2011, In AISTATS
 Deep neural networks are easily fooled: High confidencepredictions for unrecognizable images,2015, In CVPR
 Certified defenses against adversarial examples,2018, InICLR
 Likelihood ratios for out-of-distribution detection,2019, In NeurIPS
 Stochastic backpropagation andapproximate inference in deep generative models,2014, 2014
 Better the devil you know: An analysis of evasion attacks using out-of-distribution adversarial examples,2019, preprint
 Evidential deep learning to quantify classificationuncertainty,2018, In NeurIPS
 80 million tiny images: A large data set fornonparametric object and scene recognition,2008, IEEE transactions on pattern analysis and machineintelligence
 Safer classification by synthesis,2018, preprint
 Provable defenses against adversarial examples via the convex outeradversarial polytope,2018, In ICML
 Fashion-MNIST: a novel image dataset for benchmarkingmachine learning algorithms,2017, preprint
