title,year,conference
 What does BERT lookat? an analysis of bertâ€™s attention,2020, CoRR
 XNLI: evaluating cross-lingual sentence representations,2018, InEllen Riloff
 Recognizing Textual Entail-ment: Models and Applications,2013, Morgan and Claypool
 BERT: pre-training of deepbidirectional transformers for language understanding,2019, In Jill Burstein
 Span-bert: Improving pre-training by representing and predicting spans,2019, CoRR
 Roberta: A robustly optimized BERT pretrainingapproach,2019, CoRR
 Cross-lingual models of word em-beddings: An empirical comparison,2016, In Proc
 Attention is all you need,2017, In Isabelle Guyon
 A broad-coverage challenge corpusfor sentence understanding through inference,2018, In Marilyn A
