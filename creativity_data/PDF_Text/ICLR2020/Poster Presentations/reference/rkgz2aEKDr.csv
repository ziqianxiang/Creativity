title,year,conference
 Layer normalization,2016, arXiv preprintarXiv:1607
 Advances in optimizingrecurrent networks,2013, In 2013 IEEE International Conference on Acoustics
 signsgd:Compressed optimisation for non-convex problems,2018, In ICML
 One billion word benchmark for measuring progress in statistical language mod-eling,2013, In INTERSPEECH
 Closing the generalization gap of adaptive gradient methods intraining deep neural networks,2018, arXiv preprint arXiv:1806
 Imagenet: A large-scalehierarchical image database,2009, In ICML
 Incorporating nesterov momentum into adam,2016, 2016
 Adaptive subgradient methods for online learning andstochastic optimization,2010, In COLT
 DeeP residual learning for image recog-nition,2016, In CVPR
 Neural networks for machine learninglecture 6a overview of mini-batch gradient descent,2012, Cited on
 Batch normalization: Accelerating deeP network training byreducing internal covariate shift,2015, In ICML
 Adam: A method for stochastic oPtimization,2014, In ICLR
 Efficient contextualized rePre-sentation: Language model Pruning for sequence labeling,2018, EMNLP
 Roberta: A robustly oPtimized bert PretrainingaPProach,2019, arXiv preprint arXiv:1907
 AdaPtive gradient methods with dynamicbound of learning rate,2019, In ICLR
 Forecasting with moving averages,2014, 2014
 Training tips for the transformer model,2018, The Prague Bulletin ofMathematical Linguistics
 On the convergence of adam and beyond,2018, InICLR
 Rethinkingthe inception architecture for computer vision,2016, In CVPR
 Attention is all you need,2017, In NIPS
 Taylor series methods,2007, In Introduction to variance estimation
 Dscovr: Randomized primal-dual blockcoordinate algorithms for asynchronous distributed optimization,2017, J
 Adadelta: an adaptive learning rate method,2012, arXiv preprint arXiv:1212
 Fixup initialization: Residual learning withoutnormalization,2019, In ICLR
