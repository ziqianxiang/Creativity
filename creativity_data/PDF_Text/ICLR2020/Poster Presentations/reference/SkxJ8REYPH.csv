title,year,conference
 Qsgd:Communication-efficient sgd via gradient quantization and encoding,2007, In Advances in NeuralInformation Processing Systems
 Stochastic gradient pushfor distributed deep learning,2019, In International Conference on Machine Learning
 Accelerated linear convergence of stochasticmomentum methods in Wasserstein distances,2019, arXiv preprint arxiv:1901
 Revisiting distributed syn-chronous SGD,2016, In International Conference on Learning Representations Workshop Track
 Scalable training of deep learning machines by incremental block train-ing with intra-block parallel optimization and blockwise model-update filtering,2016, In 2016 IEEEInternational Conference on Acoustics
 Slow andstale gradients can win the race: Error-runtime trade-offs in distributed SGD,2018, In InternationalConference on Artificial Intelligence and Statistics
 Anytime minibatch:Exploiting stragglers in online distributed optimization,2019, In International Conference on LearningRepresentations
 Understanding the role of momentumin stochastic gradient methods,2019, In Advances in Neural Information Processing Systems
 Collaborative deep learning infixed topology networks,2017, In Advances in Neural Information Processing Systems
 Error feedbackfixes SignSGD and other gradient compression schemes,2019, In International Conference on MachineLearning
 Adam: A method for stochastic optimization,2015, In InternationalConference on Learning Representations
 Decentralized deep learningwith arbitrary communication compression,2019, arXiv preprint arXiv:1907
 Decentralized stochastic optimization andgossip algorithms with compressed communication,2019, In International Conference on MachineLearning
 Imagenet classification with deep convo-lutional neural networks,2012, In Advances in Neural Information Processing Systems
 Can decentralizedalgorithms outperform centralized algorithms? a case study for decentralized parallel stochasticgradient descent,2017, In Advances in Neural Information Processing Systems
 RoBERTa: A robustly optimized BERT pre-training approach,2019, arXiv preprint arXiv:1907
 Stochastic gradient-push for strongly convex functions on time-varying directed graphs,2016, IEEE Trans
 Languagemodels are unsupervised multi-task learners,2019, Open AI tech
 Optimal statistical rates for decentralised non-parametricregression with linear speed-up,2019, In Advances in Neural Information Processing Systems
 Local SGD converges fast and communicates little,2019, In International Conferenceon Learning Representations
 Cooperative SGD: A unified framework for the design and analysisof communication-efficient SGD algorithms,2018, arXiv preprint arXiv:1808
 MATCHA: Speedingup decentralized SGD via matching decomposition sampling,2019, arXiv preprint arXiv:1905
 TernGrad:Ternary gradients to reduce communication in distributed deep learning,2007, In Advances in NeuralInformation Processing Systems
 On the linear speedup analysis of communication efficient mo-mentum SGD for distributed non-convex optimization,2019, In International Conference on MachineLearning
 Parallel restarted SGD with faster convergence and lesscommunication: Demystifying why model averaging works for deep learning,2019, In Proceedings ofthe AAAI Conference on Artificial Intelligence
 Deep learning with elastic averaged SGD,2015, In Advancesin Neural Information Processing Systems
 On the convergence properties of a k-step averaging stochastic gradi-ent descent algorithm for nonconvex optimization,2018, In International Joint Conference on ArtificialIntelligence
