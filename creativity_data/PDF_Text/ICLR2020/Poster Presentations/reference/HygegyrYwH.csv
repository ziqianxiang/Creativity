title,year,conference
 A convergence theory for deep learning via over-parameterization,2018, arXiv preprint arXiv:1811
 On the convergence rate of training recurrent neuralnetworks,2018, arXiv preprint arXiv:1810
 Fine-grained analysis ofoptimization and generalization for overparameterized two-layer neural networks,2019, arXiv preprintarXiv:1901
 Rademacher and gaussian complexities: Risk bounds andstructural results,2002, JMLR
 Generalization error bounds of gradient descent for learning over-parameterized deep relu networks,2019, arXiv preprint arXiv:1902
 Generalization bounds of stochastic gradient descent for wide anddeep neural networks,2019, arXiv preprint arXiv:1905
 A Note on Lazy Training in Supervised Differentiable Program-ming,2019, arXiv:1812
 Gradient descent finds globalminima of deep neural networks,2018, arXiv preprint arXiv:1811
 Gradient descent provably optimizesover-parameterized neural networks,2018, arXiv preprint arXiv:1810
 Neural tangent kernel: Convergence and gen-eralization in neural networks,2018, In Advances in neural information processing systems
 Risk and parameter convergence of logistic regression,2018, arXiv preprintarXiv:1803
 Learning overparameterized neural networks via stochastic gradientdescent on structured data,2018, In Advances in Neural Information Processing Systems
 On convergence proofs on perceptrons,1962, In Proceedings of the Symposium onthe Mathematical Theory ofAutomata
 Towards moderate overparameterization: global conver-gence guarantees for training shallow neural networks,2019, arXiv preprint arXiv:1902
 Understanding Machine Learning: From Theory toAlgorithms,2014, Cambridge University Press
 Regularization matters: Generalization andoptimization of neural nets vs their induced kernel,2018, arXiv preprint arXiv:1810
 An improved analysis of training over-parameterized deep neuralnetworks,2019, arXiv preprint arXiv:1906
 Stochastic gradient descent optimizesover-parameterized deep relu networks,2018, arXiv preprint arXiv:1811
	â–¡Proof of Lemma 2,2020,6
