title,year,conference
 A convergence theory for deep learning via over-parameterization,2018, arXiv preprint arXiv:1811
 Stronger generalization bounds fordeep nets via a compression approach,2018, arXiv preprint arXiv:1802
 Onexact computation with an infinitely wide neural net,2019, arXiv preprint arXiv:1904
 Fine-grained analysis ofoptimization and generalization for overparameterized two-layer neural networks,2019, arXiv preprintarXiv:1901
 Spectrally-normalized margin bounds forneural networks,2017, In Advances in Neural Information Processing Systems
 On the inductive bias of neural tangent kernels,2019, arXiv preprintarXiv:1905
 Generalization error bounds of gradient descent for learning over-parameterized deep relu networks,2019, arXiv preprint arXiv:1902
 Generalization bounds of stochastic gradient descent for wide anddeep neural networks,2019, arXiv preprint arXiv:1905
 On the global convergence of gradient descent for over-parameterized models using optimal transport,2018, In Advances in neural information processingsystems
 Gradient descent finds globalminima of deep neural networks,2018, arXiv preprint arXiv:1811
 Gradient descent provably optimizesover-parameterized neural networks,2018, arXiv preprint arXiv:1810
 Escaping from saddle points - online stochasticgradient for tensor decomposition,2015, In Proceedings of The 28th Conference on Learning Theory
 Learning one-hidden-layer neural networks with landscapedesign,2017, arXiv preprint arXiv:1711
 Limitations of lazytraining of two-layers neural networks,2019, arXiv preprint arXiv:1906
 Linearized two-layersneural networks in high dimension,2019, arXiv preprint arXiv:1904
 Size-independent sample complexity ofneural networks,2017, arXiv preprint arXiv:1712
 Characterizing implicit bias interms of optimization geometry,2018, arXiv preprint arXiv:1802
 Identity matters in deep learning,2016, arXiv preprint arXiv:1611
 Understanding generalization of deep neural networks trainedwith noisy labels,2019, arXiv preprint arXiv:1905
 Neural tangent kernel: Convergence and gen-eralization in neural networks,2018, In Advances in neural information processing systems
 How to escape sad-dle points efficiently,2017, In Proceedings of the 34th International Conference on Machine Learning
 Stochastic gradientdescent escapes saddle points efficiently,2019, arXiv preprint arXiv:1902
 Wide neural networks of any depth evolve as linear models under gradientdescent,2019, arXiv preprint arXiv:1902
 Gradient descent onlyconverges to minimizers,2016, In Conference on Learning Theory
 Learning overparameterized neural networks via stochastic gradientdescent on structured data,2018, arXiv preprint arXiv:1808
 Algorithmic regularization in over-parameterizedmatrix sensing and neural networks with quadratic activations,2017, arXiv preprint arXiv:1712
 Towards explaining the regularization effect of initial largelearning rate in training neural networks,2019, arXiv preprint arXiv:1907
 On the computational efficiency of trainingneural networks,2014, In Advances in neural information processing Systems
 Lexico-graphic and depth-sensitive margins in homogeneous and non-homogeneous deep models,2019, arXivpreprint arXiv:1905
 Norm-based capacity control in neuralnetworks,2015, In Conference on Learning Theory
 A PAC-Bayesian approach to spectrally-normalized margin bounds for neural networks,2017, arXiv preprintarXiv:1707
 The loss surface of deep and wide neural networks,2017, arXivpreprint arXiv:1704
 The loss surface and expressivity of deep convolutional neuralnetworks,2017, arXiv preprint arXiv:1710
 Bayesian deep convolutional networks with many chan-nels are gaussian processes,2019, In International Conference on Learning Representations
 Mean field analysis of neural networks,2018, arXivpreprint arXiv:1805
 Theoretical insights into the optimizationlandscape of over-parameterized shallow neural networks,2018, IEEE Transactions on InformationTheory
 An introduction to matrix concentration inequalities,2015, Foundations and TrendsRin Machine Learning
 Neural networks with finite intrinsic dimensionhave no spurious valleys,2018, arXiv preprint arXiv:1802
 On the margin theory of feedforward neuralnetworks,2018, arXiv preprint arXiv:1810
 Regularization matters: Generalization andoptimization of neural nets vs their induced kernel,2019, arXiv preprint arXiv:1810
 Computing with infinite networks,1997, In Advances in neural informationprocessing Systems
 Kernel anddeep regimes in overparametrized models,2019, arXiv preprint arXiv:1906
 Stochastic gradient descent optimizesover-parameterized deep ReLU networks,2018, arXiv preprint arXiv:1811
 The proof is analogous to that of Lemma 5,2019, As the loss '(y
