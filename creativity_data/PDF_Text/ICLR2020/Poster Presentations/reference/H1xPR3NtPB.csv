title,year,conference
 What does BERT lookat? an analysis of BERT’s attention,2019, In Proceedings of the 2019 ACL Workshop BlackboxNLP:Analyzing and Interpreting Neural Networks for NLP
 Unsupervisedlatent tree induction with deep inside-outside recursive auto-encoders,2019, In Proceedings of the 2019Conference of the North American Chapter of the Association for Computational Linguistics:Human Language Technologies
 A critical analysis of biased parsers in unsupervisedparsing,2019, arXiv preprint arXiv:1909
 Assessing bert’s syntactic abilities,2019, arXiv preprint arXiv:1901
 Cooperative learning of disjoint syntaxand semantics,2019, In Proceedings of the 2019 Conference of the North American Chapter of theAssociation for Computational Linguistics: Human Language Technologies
 A structural probe for finding syntax in word representa-tions,2019, In Proceedings of the 2019 Conference of the North American Chapter of the Associationfor Computational Linguistics: Human Language Technologies
 Long short-term memory,1997, Neural computation
 Grammar induction with neural languagemodels: An unusual replication,2018, In Proceedings of the 2018 EMNLP Workshop BlackboxNLP:Analyzing and Interpreting Neural Networks for NLP
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Constituency parsing with a self-attentive encoder,2018, In Proceedingsof the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: LongPapers)
 A generative constituent-context model for improved gram-mar induction,2002, In Proceedings of the 40th Annual Meeting of the Association for ComputationalLinguistics
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Jointly learning sentence embeddings and syntaxwith unsupervised tree-lstms,2019, Natural Language Engineering
 Building a large annotatedcorpus of English: The Penn Treebank,1993, Computational Linguistics
 From balustrades to pierre Vinken: Looking for syntax in trans-former self-attentions,2019, In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing andInterpreting Neural Networks for NLP
 Languagemodels are unsupervised multitask learners,2019, 2019
 Inducing syntactic trees from bert representations,2019, arXiv preprintarXiv:1906
 Straight to the tree: Constituency parsing with neural syntactic distance,2018, In Proceedingsof the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: LongPapers)
 Neural language modeling byjointly learning syntax and lexicon,2018, In International Conference on Learning Representations
 Visually grounded neural syntaxacquisition,2019, In Proceedings of the 57th Annual Meeting of the Association for ComputationalLinguistics
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Learning to com-pose words into sentences with reinforcement learning,2017, In International Conference on LearningRepresentations
 Bold numbers correspond to the top 3 results for each column,2014,										LTayer number
