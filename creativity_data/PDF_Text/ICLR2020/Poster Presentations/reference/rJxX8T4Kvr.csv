title,year,conference
 Stochastic gradient descent tricks,2012, In Neural networks: Tricks of the trade
 Revisiting dis-tributed synchronous sgd,2016, arXiv preprint:1604
 Geeps: Scalabledeep learning on distributed gpus with a gpu-specialized parameter server,2016, In Proceedings of the11th European Conference on Computer Systems
 Toward understanding the im-pact of staleness in distributed machine learning,2019, In 7th International Conference on LearningRepresentations
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint:1810
 Benchmarking deepreinforcement learning for continuous control,2016, In International Conference on Machine Learning
 Slow andstale gradients can win the race: error-runtime trade-offs in distributed sgd,2018, In InternationalConference on Artificial Intelligence and Statistics
 Adap-tive asynchronous parallelization of graph algorithms,2018, In Proceedings of the 2018 InternationalConference on Management of Data
 Q-prop:Sample-efficient policy gradient with an off-policy critic,2016, arXiv preprint:1611
 Omnivore: An optimizerfor multi-device deep learning on cpus and gpus,2016, arXiv preprint:1606
 More effective distributed ml via a stale synchronousparallel parameter server,2013, In Advances in neural information processing systems
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Imagenet classification with deep convo-Iutional neural networks,2012, In Advances in neural information processing Systems
 Learning to optimize,2016, arXiv preprint arXiv:1606
 Learning scheduling algorithms for data processing clusters,2019, In Proceedings of theACM Special Interest Group on Data Communication
 Neo: A learned query optimizer,2019, arXiv preprintarXiv:1904
 Playing atari with deep reinforcement learning,2013, arXivpreprint:1312
 Masteringthe game of go with deep neural networks and tree search,2016, nature
 A bridging model for parallel computation,1990, Communications of the ACM
 Deep reinforcement learning with double q-learning,2016, In Thirtieth AAAI conference on artificial intelligence
 Dynamic stale synchronous parallel dis-tributed training for deep learning,2019, arXiv preprint:1908
 Slow learners are fast,2009, In Advances in neuralinformation processing systems
