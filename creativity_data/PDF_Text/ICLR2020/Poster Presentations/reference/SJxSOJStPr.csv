title,year,conference
 Expert gate: Lifelong learning with anetwork of experts,2017, In CVPR
 Task-Free continual learning,2019, In CVPR
 Gradient based sample selectionfor online continual learning,2019, In NeurIPS
 Selfless sequential learning,2019, In ICLR
 Mixtures of dirichlet processes with applications to bayesian nonparametricproblems,1974, Ann
 Variational inference for dirichlet process mixtures,2006, Bayesian Anal
 Efficientlifelong learning with a-gem,2019, In ICLR
 On tiny episodic memories in continuallearning,2019, arXiv
 Bayesian density estimation by mixtures of normal distributions,1983, In Recentadvances in statistics
 Model-agnostic meta-learning for fast adaptationof deep networks,2017, In ICML
 Deep residual learning for image recog-nition,2016, In CVPR
 Re-evaluating continual learn-ing scenarios: A categorization and case for strong baselines,2018, In NeurIPS
 Adaptive mixturesof local experts,1991, Neural Comput
 Reconciling meta-learning andcontinual learning with online mixtures of tasks,2019, In NeurIPS
 Auto-Encoding variational bayes,2014, In ICLR
 Overcoming catastrophic forgettingin neural networks,2017, PNAS
 Learning multiple layers of features from tiny images,2009, Technicalreport
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Learn to grow: A continualstructure learning framework for overcoming catastrophic forgetting,2019, In ICML
 Learning without forgetting,2017, IEEE TPAMI
 Online learning of nonparametric mixture models via sequential variational approxima-tion,2013, In NeurIPS
 Gradient episodic memory for continual learning,2017, InNeurIPS
 Deep online learning via Meta-Learning:continual adaptation for Model-Based RL,2019, In ICLR
 Readingdigits in natural images with unsupervised feature learning,2011, In NeurIPS
 Pixel recurrent neural networks,2016, In ICML
 Continual lifelong learningwith neural networks: A review,2019, Neural Networks
 Infinite mixtures of gaussian process experts,2002, InNeurIPS
 iCaRL: in-cremental classifier and representation learning,2017, In CVPR
 Progressive neural networks,2016, In NeurIPS
 Outrageously large neural networks: The Sparsely-Gated Mixture-of-Experts layer,2017, InICLR
 Continual learning with deep generativereplay,2017, In NeurIPS
 Dirichlet process,2010, Springer
 Generative replay with feedback connections as a generalstrategy for continual learning,2018, arXiv
 Reinforced continual learning,2018, In NeurIPS
 Lifelong learning with dynamicallyexpandable networks,2018, In ICLR
17 Â± 0,1000,03	11
