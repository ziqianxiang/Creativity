title,year,conference
 Extremely large minibatch sgd: Training resnet-50on imagenet in 15 minutes,2017, arXiv preprint arXiv:1711
 Practical recommendations for gradient-based training of deep architectures,2012, InNeural networks: Tricks ofthe trade
 signsgd:compressed optimisation for non-convex problems,2018, CoRR
 Scale out for large minibatch sgd:Residual network training on imagenet-1k with improved accuracy and reduced time to train,2017, arXivpreprint arXiv:1711
 Dawnbench: An end-to-end deep learning bench-mark and competition,2017, Training
 Large scale distributed deep networks,2012, In Advances inneural information processing systems
 Adabatch: Adaptive batch sizes fortraining deep neural networks,2017, arXiv preprint arXiv:1712
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Incorporating nesterov momentum into adam,2016, 2016
 Mini-batch stochastic approximation methodsfor nonconvex stochastic composite optimization,2014, Mathematical Programming
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Firecaffe: near-linearacceleration of deep neural network training on compute clusters,2016, In Proceedings of the IEEEConference on Computer Vision and Pattern Recognition
 On large-batch training for deep learning: Generalization gap and sharp minima,2016, arXivpreprint arXiv:1609
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Scaling Distributed Machine Learning with System and Algorithm Co-design,2017, PhD thesis
 Optimizing neural networks with kronecker-factored approximatecurvature,2015, In International conference on machine learning
 Hogwild: A lock-free approach toparallelizing stochastic gradient descent,2011, In Advances in neural information processing systems
 Measuring the effects of data parallelism on neural network training,2018, arXiv preprintarXiv:1811
 On the importance of initializationand momentum in deep learning,2013, In International conference on machine learning
 Yet another accelerated sgd:Resnet-50 training on imagenet in 74,2019,7 seconds
 Image classification atsupercomputer scale,2018, arXiv preprint arXiv:1811
 Imagenet training inminutes,2018, In Proceedings of the 47th International Conference on Parallel Processing
 Large-batchtraining for lstm and beyond,2019, arXiv preprint arXiv:1901
 Why ADAM beats SGD for attention models,2019, CoRR
