title,year,conference
 Adversarial contrastive estimation,2018, In ACL
 Language GANs falling short,2018, arXiv preprint arXiv:1811
 Semi-sUpervised seqUence learning,2015, In NeurIPS
 Unified langUage model pre-training for natUral langUage Understandingand generation,2019, In NeurIPS
 Generative adversarial nets,2014, In NeurIPS
 Noise-contrastive estimation: A new estimation principlefor Unnormalized statistical models,2010, In AISTATS
 ALBERT: A lite bert for self-supervised learning of language representations,2019, arXiv preprintarXiv:1909
 RoBERTa: A robustly optimized BERT pre-training approach,2019, arXiv preprint arXiv:1907
 Efficient estimation of wordrepresentations in vector space,2013, In ICLR Workshop Papers
 Glove: Global vectors for wordrepresentation,2014, In EMNLP
 Sentence encoders on STILTs: SUPPlemen-tary training on intermediate labeled-data tasks,2018, arXiv preprint arXiv:1811
 UnsuPervised rePresentation learning with deePconvolutional generative adversarial networks,2016, In ICLR
 Time-contrastive networks: Self-suPervised learning from video,2017, ICRA
 Contrastive estimation: Training log-linear models on unlabeleddata,2005, In ACL
 Recursive deeP models for semantic comPositionality over a sentimenttreebank,2013, In EMNLP
 MASS: Masked sequence to sequencePre-training for language generation,2019, In ICML
 Ernie: Enhanced rePresentation through knowledge integration,2019, arXivpreprint arXiv:1904
 Attention is all you need,2017, In NeurIPS
 Extracting andcomPosing robust features with denoising autoencoders,2008, In ICML
 Simple statistical gradient-following algorithms for connectionist reinforcementlearning,1992, Machine Learning
 SeqGAN: Sequence generative adversarialnets with policy gradient,2017, In AAAI
 Aligning books and movies: Towards story-like visual explanations bywatching movies and reading books,2015, ICCV
