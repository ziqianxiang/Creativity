title,year,conference
 SMASH: One-shot model architecturesearch through hypernetworks,2018, In International Conference on Learning Representations
 Proxylessnas: Direct neural architecture search on target taskand hardware,2019, In International Conference on Learning Representations
 Improved regularization of convolutional neural networkswith cutout,2017, arXiv preprint arXiv:1708
 Nas-bench-102: Extending the scope of reproducible neural architec-ture search,2020, In International Conference on Learning Representations
 Efficient multi-objective neural architecturesearch via lamarckian evolution,2019, In International Conference on Learning Representations
 Categorical reparameterization with gumbel-softmax,2017, InInternational Conference on Learning Representations
 BOHB: Robust and efficient hyperparameter op-timization at scale,2018, In Jennifer Dy and Andreas Krause (eds
 An efficient approach for assessing hyperparameterimportance,2014, In E
 Non-stochastic best arm identification and hyperparameter optimiza-tion,2016, In Proceedings of the Seventeenth International Conference on Artificial Intelligence andStatistics (AISTATS)
 Learning multiple layers of features from tiny images,2009, Technical report
 Hyperband: Bandit-based con-figuration evaluation for hyperparameter optimization,2017, In International Conference on LearningRepresentations
 Ray rllib: A composable and scalable reinforcement learning library,2017, CoRR
 Best practices for scientific research on neural architecturesearch,2019, arXiv preprint arXiv:1909
 DARTS: Differentiable architecture search,2019, InInternational Conference on Learning Representations
 Neural architecture optimization,2018, In NeurIPS
 Efficient neural architecturesearch via parameter sharing,2018, In International Conference on Machine Learning
 Large-scale evolution of image classifiers,2017, In Doina Precup andYee Whye Teh (eds
 Aging Evolution for Image Classi-fier Architecture Search,2019, In AAAI
 Sequential design of computer experiments to minimizeintegrated response functions,2000, Statistica Sinica
 Nas evaluation is frustratingly hard,2020, InInternational Conference on Learning Representations
 NAS-bench-101: Towards reproducible neural architecture search,2019, In Kamalika Chaudhuri and RuslanSalakhutdinov (eds
 Learning transferable architecturesfor scalable image recognition,2018, In Conference on Computer Vision and Pattern Recognition
