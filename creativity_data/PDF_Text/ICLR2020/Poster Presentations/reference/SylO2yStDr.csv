title,year,conference
 Layer normalization,2016, arXiv preprintarXiv:1607
 Neural machine translation by jointlylearning to align and translate,2014, arXiv preprint arXiv:1409
 Adaptively sparse transformers,2019, arXivpreprint arXiv:1909
 Transformer-xl: Attentive language models beyond a fixed-length context,2019, arXivpreprint arXiv:1901
 Language modeling with gatedconvolutional networks,2017, In Proc
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Pre-trained language model representations forlanguage generation,2019, In Proceedings of the 2019 Conference of the North American Chapter ofthe Association for Computational Linguistics: Human Language Technologies
 Controllable abstractive summarization,2017, arXiv
 Efficientsoftmax approximation for gpus,2016, arXiv
 Deep Residual Learning for ImageRecognition,2015, In Proc
 Channel pruning for accelerating very deep neural net-works,2017, In Proceedings of the IEEE International Conference on ComPuter Vision
 Teaching machines to read and comprehend,2015, In Proc
 Improving neural networks by preventing co-adaptation of feature detectors,2012, arXiv preprintarXiv:1207
 Deep networks withstochastic depth,2016, In European conference on computer vision
 Condensenet: Anefficient densenet using learned group convolutions,2018, In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition
 Data-driven sparse structure selection for deep neural networks,2018, InProceedings of the European Conference on Computer Vision (ECCV)
 Variable computation in recur-rent neural networks,2016, arXiv preprint arXiv:1611
 Cross-lingual language model pretraining,2019, arXiv preprintarXiv:1901
 Optimal brain damage,1990, In Advances in neuralinformation processing systems
 Pruning filters forefficient convnets,2016, arXiv preprint arXiv:1608
 Rouge: A package for automatic evaluation of summaries,2004, In Workshop on TextSummarization Branches Out
 Efficient contextualized rep-resentation: Language model pruning for sequence labeling,2018, arXiv preprint arXiv:1804
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Rethinking the value ofnetwork pruning,2018, arXiv preprint arXiv:1810
 Sgdr: Stochastic gradient descent with warm restarts,2016, arXivpreprint arXiv:1608
 Pointer Sentinel MixtureModels,2016, arXiv
 Recovering fromrandom pruning: On the plasticity of deep convolutional neural networks,2018, In 2018 IEEE WinterConference on Applications of Computer Vision (WACV)
 Auto-sizing neural networks: With applications to n-gram lan-guage models,2015, arXiv preprint arXiv:1508
 How to construct deeprecurrent neural networks,2014, In Proceedings of the Second International Conference on LearningRepresentations (ICLR 2014)
 Very deep self-attention networks for end-to-end speech recognition,2019, arXiv preprint arXiv:1904
 Languagemodels are unsupervised multitask learners,2019, 2019
 Compression of neural machinetranslation models via pruning,2016, arXiv preprint arXiv:1606
 Get to the point: Summarization with pointer-generator networks,2017, In Proc
 Neural machine translation of rare words withsubword units,2015, arXiv preprint arXiv:1508
 Neural machine translation of rare words withsubword units,2016, In Proc
 Recursive deep models for semantic compositionality over a sentimenttreebank,2013, In Proceedings of EMNLP
 Adaptive attentionspan in transformers,2019, arXiv preprint arXiv:1905
 Distilling task-specific knowledge from bert into simple neural networks,2019, arXiv preprint arXiv:1903
 Well-read students learn better:The impact of student initialization on knowledge distillation,2019, arXiv preprint arXiv:1908
 Attention is all you need,2017, In Advances in neural informationprocessing Systems
 Regularization of neuralnetworks using dropconnect,2013, In International conference on machine learning
 Learning structured sparsity indeep neural networks,2016, In Advances in neural information processing systems
 Blockdrop: Dynamic inference paths in residual networks,2018, In Proceedings ofthe IEEE Conference on Computer Vision and Pattern Recognition
 Xlnet: Generalized autoregressive pretraining for language understanding,2019, arXiv preprintarXiv:1906
 Fixup initialization: Residual learning withoutnormalization,2019, arXiv preprint arXiv:1901
 Following Fan et al,2016, (2017)
