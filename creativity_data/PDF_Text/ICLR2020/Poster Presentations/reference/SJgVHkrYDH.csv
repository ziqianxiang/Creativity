title,year,conference
 Layer normalization,2016, arXiv:1607
 BERT: Pre-training of deepbidirectional transformers for language understanding,2019, In NAACL
 Cognitive graphfor multi-hop reading comprehension at scale,2019, In ACL
 Tying word vectors and word classifiers: Aloss framework for language modeling,2017, In ICLR
 Adam: A Method for Stochastic Optimization,2015, In ICLR
 RoBERTa: A robustly optimized bert pretrainingapproach,2019, arXiv:1907
 Compositional questions do not necessitate multi-hop reasoning,2019, In ACL
 Multi-hop reading compre-hension through question decomposition and rescoring,2019, In ACL
 Revealing the importance of semantic retrieval formachine reading at scale,2019, In EMNLP
 Passage re-ranking with BERT,2019, arXiv:1901
 Using the output embedding to improve language models,2017, In EACL
 Weaver: Deep Co-encoding of questions and documents for machine reading,2018, arXiv:1804
 Weight normalization: A simple reparameterization to acceleratetraining of deep neUral networks,2016, In NeurIPS
 Bidirectional attentionflow for machine comprehension,2017, In ICLR
