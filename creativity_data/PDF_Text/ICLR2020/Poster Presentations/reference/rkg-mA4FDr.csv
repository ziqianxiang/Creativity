title,year,conference
 Adaptive importance sampling to accelerate training ofa neural probabilistic language model,2008, IEEE Transactions on Neural Networks
 A large anno-tated corpus for learning natural language inference,2015, In Proceedings of the 2015 Conference onEmpirical Methods in Natural Language Processing (EMNLP)
 Universal sentence encoder,2018, InACL
 X-BERT: eX-treme multi-label text with BERT,2019, arXiv preprint arXiv:1905
 Strategies for training large vocabulary neural lan-guage models,2016, In Proceedings of the 54th Annual Meeting of the Association for ComputationalLinguistics (ACL)
 Bert: Pre-training of deepbidirectional transformers for language understanding,2019, In Proceedings of the 2019 Conference ofthe North American Chapter of the Association for Computational Linguistics (NAACL)
 Efficient Softmax approXima-tion for gpus,2017, In Proceedings of the 34th International Conference on Machine Learning (ICML)
 Quantization based fast innerproduct search,2016, In Proceedings of the 19th International Conference on Artificial Intelligence andStatistics (AISTATS)
 Efficient natural language response suggestionfor smart reply,2017, arXiv preprint arXiv:1705
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Poly-encoders: Trans-former architectures and pre-training strategies for fast and accurate multi-sentence scoring,2019, arXivpreprint arXiv:1905
 Skip-thought vectors,2015, In Advances in Neural Information ProcessingSystems (NIPS)
 Efficient training on very large corpora via gramian estimation,2019, In Proceedingsof the International Conference on Learning Representations (ICLR)
 RoBERTa: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 An efficient framework for learning sentence represen-tations,2018, In Proceedings of the International Conference on Learning Representations (ICLR)
 Training millionsof personalized dialogue agents,2018, In EMNLP
 MS MARCO: A human-generated machine reading comprehension dataset,2016, 2016
 Sentence-BERT: Sentence embeddings using siamese BERT-networks,2019, In Proceedings of the 2019 Conference on Empirical Methods in Natural LanguageProcessing (EMNLP)
 The probabilistic relevance framework: BM25 and be-yond,2009, Foundations and TrendsR in Information Retrieval
 Distilling task-specific knowledge from BERT into simple neural networks,2019, arXiv preprint arXiv:1903
 Few-shot learning through an informationretrieval lens,2017, In Advances in Neural Information Processing Systems
 Attention is all you need,2017, In NIPS
 Anserini: Enabling the use of lucene for information retrievalresearch,2017, In Proceedings of the 40th International ACM SIGIR Conference on Research andDevelopment in Information Retrieval
 Selection of negative samples for one-classmatrix factorization,2017, In Proceedings of the 2017 SIAM International Conference on Data Mining
