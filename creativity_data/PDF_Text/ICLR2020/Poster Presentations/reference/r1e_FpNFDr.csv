title,year,conference
 A convergence theory for deep learning via over-parameterization,2019, ICML
 Stronger generalization bounds fordeep nets via a compression approach,2018, arXiv preprint arXiv:1802
 Spectrally-normalized margin bounds forneural networks,2017, In Advances in Neural Information Processing Systems
 Generalization bounds of stochastic gradient descent for wide and deepneural networks,2019, arXiv preprint arXiv:1905
 Generalization bounds for neural networks via approximate de-scription length,2019, In Advances in Neural Information Processing Systems
 Gradient descent finds globalminima of deep neural networks,2019, ICML
 Gradient descent provably optimizesover-parameterized neural networks,2019, ICLR
 On consistency of kernel density estimators for randomlycensored data: rates holding uniformly over adaptive intervals,2001, In AnnaIeS de l'IHP Probabilites etstatistiques
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Proceedings of the thirteenth international conference on artificial intelligence andstatistics
 Size-independent sample complexity ofneural networks,2017, arXiv preprint arXiv:1712
 Size-independent sample complexity ofneural networks,2018, In Conference On Learning Theory
 Deep learning,2016, MIT press
 Characterizing implicit bias interms of optimization geometry,2018, arXiv preprint arXiv:1802
 Implicit bias of gradient descenton linear convolutional networks,2018, In Advances in Neural Information Processing Systems
 Decision theoretic generalizations of the PAC model for neural net and other learningapplications,1992, Information and Computation
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, In Proceedings of the IEEE internationalconference on computer vision
 Fantasticgeneralization measures and where to find them,2019, ICLR
 Efficient backprop,2012, InNeural networks: Tricks of the trade
 Norm-based generalisation bounds for multi-classconvolutional neural networks,2019, arXiv 1905
 Learning finite-dimensional coding schemes with nonlinear recon-struction maps,2018, arXiv preprint arXiv:1812
 Wide neural networks of any depth evolve as linear modelsunder gradient descent,2019, In Advances in neural information processing Systems
 Foundations of machine learning,2018, MITpress
 Generalization in deep networks: The role of distance frominitialization,2019, arXiv preprint arXiv:1901
 Norm-based capacity control in neuralnetworks,2015, In Conference on Learning Theory
 Exploring generaliza-tion in deep learning,2017, In Advances in Neural Information Processing Systems
 A pac-bayesian approach to spectrally-normalized margin bounds for neural networks,2018, ICLR
 Towardsunderstanding the role of over-parametrization in generalization of neural networks,2019, ICLR
 Convergence of Stochastic Processes,1984, Springer Verlag
 Exact solutions to the nonlinear dynamicsof learning in deep linear neural networks,2013, arXiv preprint arXiv:1312
 The singular values of convolutional layers,2018, arXivpreprint arXiv:1805
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Sharper bounds for Gaussian and empirical processes,1994, Annals of Probability
 New concentration inequalities in product spaces,1996, Inventiones mathematicae
 Estimation of Dependencies based on Empirical Data,1982, Springer Verlag
 On the uniform convergence of relative frequencies of eventsto their probabilities,1971, Theory of Probability and its Applications
 Data-dependent sample complexity of deep neural networks via lipschitzaugmentation,2019, arXiv preprint arXiv:1905
 Improved sample complexities for deep networks and robust classificationvia an all-layer margin,2019, ICLR
 Understandingdeep learning requires rethinking generalization,2016, arXiv preprint arXiv:1611
 Stochastic gradient descent optimizesover-parameterized deep relu networks,2018, arXiv preprint arXiv:1811
