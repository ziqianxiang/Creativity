title,year,conference
 Deep learning with differential privacy,2016, In CCS
 Adding robustness to support vectormachines against adversarial reverse engineering,2014, In CIKM
 Evasion attacks against machine learning at test time,2013, In ECMLPKDD
 Copycat cnn: Stealing knowledge by persuading confession with random non-labeled data,2018, In IJCNN
 Efficient projections ontothe l 1-ball for learning in high dimensions,2008, In ICML
 The algorithmic foundations of differential privacy,2014, Foundationsand TrendsR in Theoretical Computer Science
 Explaining and harnessing adversarialexamples,2014, arXiv preprint arXiv:1412
 High-fidelity extraction of neural network models,2019, arXiv preprint arXiv:1909
 Model extraction warningin mlaas paradigm,2018, In ACSAC
 Adam: A method for stochastic optimization,2014, In ICLR
 Adversarial learning,2005, In KDD
 Model reconstruction frommodel explanations,2018, arXiv preprint arXiv:1807
 Towards reverse-engineering black-box neural networks,2018, In ICLR
 Knockoff nets: Stealing functionality ofblack-box models,2019, In CVPR
 Aframework for the extraction of deep neural networks by leveraging public data,2019, arXiv preprintarXiv:1905
 Transferability in machine learning: fromphenomena to black-box attacks using adversarial samples,2016, arXiv preprint arXiv:1605
 Practical black-box attacks against machine learning,2017, In Asia CCS
 Stealing machinelearning models via prediction apis,2016, In USENIX Security
 Bdpl: A boundary differen-tially private layer against machine learning model extraction attacks,2019, In ESORICS
