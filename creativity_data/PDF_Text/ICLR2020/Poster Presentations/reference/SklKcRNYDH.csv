title,year,conference
 Disentangling adaptivegradient methods from learning rates,2020, arXiv preprint arXiv:2002
 Neural optimizer search with rein-forcement learning,2017, arXiv preprint arXiv:1709
 signsgd:compressed optimisation for non-convex problems,2018, arXiv preprint arXiv:1802
 Closing the generalization gap of adaptive gradient methods intraining deep neural networks,2018, arXiv preprint arXiv:1806
 A unified approach to adaptive regularization inonline and stochastic optimization,2017, arXiv preprint arXiv:1706
 Shampoo: Preconditioned stochastic tensor opti-mization,2018, In International Conference on Machine Learning
 Introduction to online convex optimization,2016, Foundations and TrendsR in Optimization
 Deep residual learning for image recog-nition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Learning multiple layers of features from tiny images,2009, 2009
 Measuring the intrinsic dimensionof objective landscapes,2018, arXiv preprint arXiv:1804
 Visualizing the loss land-scape of neural nets,2018, In Advances in Neural Information Processing Systems
 Optimizing neural networks with kronecker-factored approximatecurvature,2015, In International Conference on Machine Learning
 Kronecker-factored curvature approximations forrecurrent neural networks,2018, 2018
 Tensor2tensor for neural machine translation,2018, CoRR
 Adadelta: an adaptive learning rate method,2012, arXiv preprint arXiv:1212
