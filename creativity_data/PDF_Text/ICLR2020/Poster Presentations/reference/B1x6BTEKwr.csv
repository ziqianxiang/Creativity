title,year,conference
 A convergence theory for deep learning via over-parameterization,2019, In International Conference on Machine Learning
 Learning to learn by gradient descent by gradientdescent,2016, In Advances in Neural Information Processing Systems
 On the optimization of deep networks: Implicitacceleration by overparameterization,2018, In International Conference on Machine Learning
 Globally optimal gradient descent for a convnet with gaussianinputs,2017, In International Conference on Machine Learning
 SGD learns over-parameterized networks that provably generalize on linearly separable data,2018, In InternationalConference on Learning Representations
 Theloss surfaces of multilayer networks,2015, In International Conference on Artificial Intelligence andStatistics
 Essentially no barriersin neural network energy landscape,2018, In International Conference on Machine Learning
 Gradient descent finds globalminima of deep neural networks,2018, In International Conference on Machine Learning
 Gradient descent provably optimizesover-parameterized neural networks,2019, In International Conference on Learning Representations
 Truth orbackpropaganda? an empirical investigation of deep learning theory,2020, In International Conferenceon Learning Representations
 Global optimality in neural network training,2017, In IEEEConference on Computer Vision and Pattern Recognition
 Complexity of linear regions in deep networks,2019, In InternationalConference on Machine Learning
 Deep residual learning for image recog-nition,2016, In IEEE Conference on Computer Vision and Pattern Recognition
 Deep learning without poor local minima,2016, In Advances in Neural InformationProcessing Systems
 The multilinear structure of relu networks,2018, In InternationalConference on Machine Learning
 Deep learning,2015, Nature
 Over-parameterized deep neural networks have no strict localminima for any continuous activations,2018, arXiv preprint arXiv:1812
 Learning overparameterized neural networks via stochastic gradientdescent on structured data,2018, In Advances in Neural Information Processing Systems 31
 Understanding the loss surfaceof neural networks for binary classification,2018, In International Conference on Machine Learning
 A survey on deep learning in medical image analysis,2017, Medical Image Analysis
 Depth creates no bad local minima,2017, arXiv preprintarXiv:1702
 On connected sublevel sets in deep learning,2019, In International Conference onMachine Learning
 Optimization landscape and expressivity of deep cnns,2018, InInternational Conference on Machine Learning
 On the loss landscape of a classof deep neural networks with no bad local valleys,2019, In International Conference on LearningRepresentations
 Singularity of the hessian in deep learning,2016, arXivpreprint arXiv:1611
 Empirical analysisof the hessian of over-parametrized neural networks,2018, In International Conference on LearningRepresentations Workshop
 Masteringthe game of go with deep neural networks and tree search,2016, Nature
 Learning relus via gradient descent,2017, In Advances in Neural InformationProcessing Systems 30
 Theoretical insights into the optimizationlandscape of over-parameterized shallow neural networks,2018, IEEE Transactions on InformationTheory
 Exponentially vanishing sub-optimal local minima in multilayerneural networks,2018, In International Conference on Learning Representations Workshop
 Local minima in training ofdeep networks,2016, arXiv preprint arXiv:1611:06310
 An analytical formula of population gradient for two-layered relu network and itsapplications in convergence and critical point analysis,2017, In International Conference on MachineLearning
 Data Mining: Practical machinelearning tools and techniques,2016, Morgan Kaufmann
 No spurious local minima in a two hidden unit relunetwork,2018, In International Conference on Learning Representation Workshop
 Efficiently testing local optimality and escaping saddlesfor reLU networks,2019, In International Conference on Learning Representations
 Small nonlinearities in activation functions createbad local minima in neural networks,2019, In International Conference on Learning Representations
 Learning one-hidden-layer relu net-works via gradient descent,2019, In International Conference on Artificial Intelligence and Statistics
 Recovery guaranteesfor one-hidden-layer neural networks,2017, In International Conference on Machine Learning
 Empirical risk landscape analysis for understanding deep neural net-works,2018, In International Conference on Learning Representations
 Critical points of neural networks: Analytical forms and landscapeproperties,2018, In International Conference on Learning Representations
 Stochastic gradient descent optimizesover-parameterized deep relu networks,2019, Machine Learning
