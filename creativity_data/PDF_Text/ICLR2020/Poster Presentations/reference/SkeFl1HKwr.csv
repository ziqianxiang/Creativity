title,year,conference
 Understanding deep neuralnetworks with rectified linear units,2018, In Proc
 Learning long-term dependencies with gradi-ent descent is difficult,1994, IEEE Trans
 Evasion attacks against machine learning at test time,2013, In Proc
 Understanding batch normal-ization,2018, In Proc
 BERT: Pre-training of deepbidirectional transformers for language understanding,2019, In Proc
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Proc
 Generative adversarial nets,2014, In Proc
 Explaining and harnessing adversarialexamples,2015, In Proc
 Complexity of linear regions in deep networks,2019, In Proc
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, In Proc
 Deep residual learning for image recog-nition,2016, In Proc
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, In Proc
 Adam: A method for stochastic optimization,2014, In Proc
 In Proc,2018, Int’l Conf
 On the number of linearregions of deep neural networks,2014, In Proc
 Deepfool: A simple andaccurate method to fool deep neural networks,2016, In Proc
 In Proc,2018, Int’l Conf
 On the number of response regions of deepfeed forward networks with piece-wise linear activations,2014, CoRR
 Exponen-tial expressivity in deep neural networks through transient chaos,2016, In Proc
 Adversarial robustness through locallinearization,2019, In Proc
 Exact solutions to the nonlinear dynam-ics of learning in deep linear neural networks,2014, In Proc
 Bounding and counting linearregions of deep neural networks,2018, In Proc
 Mastering the game of Gowithout human knowledge,2017, Nature
 Intriguing properties of neural networks,2014, In Proc
