title,year,conference
 Fixinga broken ELBO,2018, In ICML
 The effects of adding noise during backpropagation training on a generalizationperformance,1996, In Neural computation
 Resampled priors for variational autoencoders,2019, In AISTATS
 Generalized denoising auto-encodersas generative models,2013, In NeurIPS
 Pattern recognition and machine learning,2006, Springer
 Generating sentences from a continuous space,2016, In CoNLL
 Large scale gan training for high fidelity naturalimage synthesis,2019, In ICLR
 Variational lossy autoencoder,2017, In ICLR
 Diagnosing and enhancing VAE models,2019, In ICLR
 Made: Masked autoencoder fordistribution estimation,2015, In International Conference on Machine Learning
 Resisting adversarial attacks using Gaussianmixture variational autoencoders,2019, In AAAI
 Automatic chemical design using a data-driven contin-uous representation of molecules,2018, In ACS central science
 Generative adversarial nets,2014, In NeurIPS
 Im-proved training of Wasserstein GANs,2017, In NeurIPS
 Beta-VAE: Learning basic visual concepts with aconstrained variational framework,2017, In ICLR
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, In ICML
 Junction tree variational autoencoder formolecular graph generation,2018, arXiv preprint arXiv:1802
 Auto-encoding variational Bayes,2014, In ICLR
 Grammar variational autoen-coder,2017, In ICML
 The neural autoregressive distribution estimator,2011, In AISTATS
 Gradient-based learning applied todocument recognition,1998, In IEEE
 Are GANscreated equal? A large-scale study,2018, In NeurIPS
 Spectral normalizationfor generative adversarial networks,2018, In ICLR
 Geometry of opti-mization and implicit regularization in deep learning,2017, arXiv preprint arXiv:1705
 What is high-throughput virtual screening? A perspective from organicmaterials discovery,2015, Annual Review of Materials Research
 Generating diverse high-fidelity images withVQ-VAE-2,2019, arXiv preprint arXiv:1906
 Taming VAEs,2018, arXiv preprint arXiv:1810
 Stochastic backpropagation andapproximate inference in deep generative models,2014, In ICML
 Contractive auto-encoders: Explicit invariance during feature extraction,2011, In ICML
 A generative process for samplingcontractive auto-encoders,2012, In ICML
 Distribution matching in varia-tional inference,2018, arXiv preprint arXiv:1802
 Enhancenet: Single image SUPer-resolution through automated texture synthesis,2017, In ICCV
 Assessinggenerative models via precision and recall,2018, In NeurIPS
 A hybrid convolUtional variational aU-toencoder for text generation,2017, In Empirical Methods in Natural Language Processing
 Creating artificial neUral networks that generalize,1991, In Neuralnetworks
 Learning strUctUred oUtpUt representation Using deepconditional generative models,2015, In NeurIPS
 AmortisedMAP Inference for Image SUper-resolUtion,2017, In ICLR
 A note on the evaluation of generativemodels,2016, In ICLR
 Wasserstein auto-encoders,2017, In ICLR
 VAE with a VampPrior,2018, In AISTATS
 Neural discrete representation learning,2017, In NeurIPS
 Extracting andcomposing robust features with denoising autoencoders,2008, In ICML
 Wide Residual Networks,2016, In BMVC
 Understandingdeep learning requires rethinking generalization,2017, In ICLR
 Towards deeper understanding of variationalautoencoding models,2017, arXiv preprint arXiv:1702
 All convolutions Convn and transposed con-volutions ConvTn have a filter size of 4×4 for MNIST and CIFAR-10 and 5×5 for CELEBA,2020, They14Published as a conference paper at ICLR 2020all have a stride of size 2 except for the last convolutional layer in the decoder
