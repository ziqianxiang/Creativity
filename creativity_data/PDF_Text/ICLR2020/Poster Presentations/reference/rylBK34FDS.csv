title,year,conference
 Learning the number of neurons in deep networks,2016, InAdvances in Neural Information Processing Systems
 Exponentially many local minima for singleneurons,1996, In Advances in neural information processing systems
 Nest: A neural network synthesis tool based on agrow-and-prune paradigm,2017, CoRR
 Centripetal sgd for pruning verydeep convolutional networks with complicated structure,2019, In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition
 Dynamic network surgery for efficient dnns,2016, In AdvancesIn Neural Information Processing Systems
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Soft filter pruning for acceleratingdeep convolutional neural networks,2018, arXiv preprint arXiv:1808
 Amc: Automl for modelcompression and acceleration on mobile devices,2018, In Proceedings of the European Conference onComputer Vision (ECCV)
 Data-driven sparse structure selection for deep neural networks,2018, InProceedings of the European Conference on Computer Vision (ECCV)
 Comparing measures of sparsity,2009, IEEE Transactions on InformationTheory
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Blind deconvolution using a normalized sparsitymeasure,2011, In CVPR 2011 
 Imagenet classification with deep convolu-tional neural networks,2012, In Advances in neural information processing systems
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 SNIP: SINGLE-SHOT NETWORKPRUNING BASED ON CONNECTION SENSITIVITY,2019, In International Conference on LearningRepresentations
 Pruning filters forefficient convnets,2016, arXiv preprint arXiv:1608
 Compressingconvolutional neural networks via factorized convolutional filters,2019, In Proceedings of the IEEEConference on Computer Vision and Pattern Recognition
 Toward compact convnets viastructure-sparsity regularized filter pruning,2019, IEEE transactions on neural networks and learningsystems
 Sparse convolu-tional neural networks,2015, In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition
 Learn-ing efficient convolutional networks through network slimming,2017, In Proceedings of the IEEEInternational Conference on Computer Vision
 Bayesian compression for deep learning,2017, InAdvances in Neural Information Processing Systems
 Learning sparse neural networks throughl_0 regularization,2017, arXiv preprint arXiv:1712
 Autopruner: An end-to-end trainable filter pruning method for efficientdeep model inference,2018, arXiv preprint arXiv:1805
 Thinet: A filter level pruning method for deep neuralnetwork compression,2017, In Proceedings of the IEEE international conference on computer vision
 Transformed `1 regularization forlearning sparse deep neural networks,2019, Neural Networks
 Playing atari with deep reinforcement learning,2013, arXiv preprintarXiv:1312
 Structured bayesianpruning via log-normal multiplicative noise,2017, In Advances in Neural Information ProcessingSystems
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Leveraging filtercorrelations for deep model compression,2018, arXiv preprint arXiv:1811
 On the importance of initializationand momentum in deep learning,2013, In International conference on machine learning
 Reconstruction of jointly sparse vectors via manifoldoptimization,2018, arXiv preprint arXiv:1811
 Learning structured sparsity in deepneural networks,2016, In Advances in neural information processing systems
 Learning intrinsic sparse structures within long short-term memory,2017, arXivpreprint arXiv:1709
 Ratio and difference of l1 and l2 norms and sparserepresentation with coherent dictionaries,2014, Commun
 Nisp: Pruning networks using neuron importance scorepropagation,2018, In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
 Nearly unbiased variable selection under minimax concave penalty,2010, The Annalsof statistics
 Discrimination-aware channel pruning for deep neural networks,2018, In Advances inNeural Information Processing Systems
 The CIFAR-10 dataset can be directly accessedthrough the dataset API provided in the “torchvision” python package,2020, Standard preprocessing
