title,year,conference
 Fine-grained analysis ofoptimization and generalization for overparameterized tWo-layer neural netWorks,2019, In Proceedingsofthe 36th International Conference on Machine Learning
 Deep reWiring: Trainingvery sparse deep netWorks,2018, In International Conference on Learning Representations
 Imagenet: A large-scalehierarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 Sparse netWorks from scratch: Faster training Without losingperformance,2019, arXiv preprint arXiv:1907
 Learning to prune deep neural netWorks via layer-Wiseoptimal brain surgeon,2017, In Advances in Neural Information Processing Systems
 The difficulty of training sparse neuralnetWorks,2019, arXiv preprint arXiv:1906
 The lottery tickethypothesis at scale,2019, arXiv preprint arXiv:1903
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Proceedings of the thirteenth international conference on artificial intelligence andstatistics
 Optimal brain surgeon and general networkpruning,1993, In IEEE international conference on neural networks
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, In Proceedings of the IEEE internationalconference on computer vision
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Learning multiple layers of features from tiny images,2009, Technical report
 Optimal brain damage,1990, In Advances in neuralinformation processing systems
 Snip: Single-shot network pruningbased on connection sensitivity,2018, International Conference on Learning Representations
 A signal propagationperspective for pruning neural networks at initialization,2019, arXiv preprint arXiv:1906
 Pruning filters forefficient convnets,2016, International Conference on Learning Representations
 Rethinking the value ofnetwork pruning,2019, International Conference on Learning Representations
 Learning sparse neural networks throughl_0 regularization,2018, International Conference on Learning Representations
 Scalable training of artificial neural networks with adaptive sparse connectivityinspired by network science,2018, Nature communications
 Parameter efficient training of deep convolutional neural networksby dynamic sparse reparameterization,2019, International Conference on Machine Learning
 The role ofover-parametrization in generalization of neural networks,2019, In International Conference on LearningRepresentations
 Automatic differentiation inpytorch,2017, 2017
 Fast exact multiplication by the Hessian,1994, Neural computation
 Exponentialexpressivity in deep neural networks through transient chaos,2016, In Advances in neural informationprocessing systems
 Exact solutions to the nonlinear dynamicsof learning in deep linear neural networks,2013, arXiv preprint arXiv:1312
 Very deep convolutional networks for large-scale imagerecognition,2014, International Conference on Learning Representations
 Generalized dropout,2016, arXiv preprint arXiv:1611
 EigenDamage: Structured pruningin the Kronecker-factored eigenbasis,2019, In Proceedings of the 36th International Conference onMachine Learning
 Mean field residual networks: On the edge of chaos,2017, In Advancesin neural information processing systems
 Wide residual networks,2016, In BMVC
 Understanding deeplearning requires rethinking generalization,2016, International Conference on Learning Representations
 Fast convergence of natural gradient descent foroverparameterized neural networks,2019, arXiv preprint arXiv:1905
