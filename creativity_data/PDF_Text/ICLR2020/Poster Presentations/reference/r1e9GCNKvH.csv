title,year,conference
 Stronger generalization bounds fordeep nets via a compression approach,2018, ICML
 A back-propagation algorithm with optimal use of hidden units,1989, In Advances inneural information processing systems
 Dynamical isometry and a mean fieldtheory of rnns: Gating enables signal propagation in recurrent neural networks,2018, arXiv preprintarXiv:1806
 Groupreduce: Block-wise low-rankapproximation for neural language model shrinking,2018, In Advances in Neural Information ProcessingSystems
 Mean replacement pruning,2018, 2018
 The state of sparsity in deep neural networks,2019, arXivpreprint arXiv:1902
 Dynamical isometry and a mean field theory of lstms and grus,2019, arXiv preprintarXiv:1901
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Proceedings of the thirteenth international conference on artificial intelligence andstatistics
 Compressing deep convolutionalnetworks using vector quantization,2014, arXiv preprint arXiv:1412
 Dynamic network surgery for efficient dnns,2016, In AdvancesIn Neural Information Processing Systems
 Deep learning withlimited numerical precision,2015, In International Conference on Machine Learning
 Optimal brain surgeon and general networkpruning,1993, In IEEE international conference on neural networks
 Mobilenets: Efficient convolutional neural networks formobile vision applications,2017, arXiv preprint arXiv:1704
 Structural learning with forgetting,1996, Neural networks
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Tracenorm regularization and faster inference for embedded speech recognition rnns,2017, arXiv preprintarXiv:1710
 Optimal brain damage,1990, In Advances in neuralinformation processing systems
 Snip: Single-shot network pruningbased on connection sensitivity,2018, arXiv preprint arXiv:1810
 Rethinking the value ofnetwork pruning,2018, arXiv preprint arXiv:1810
 Bayesian sparsification of recurrentneural networks,2017, arXiv preprint arXiv:1708
 Exploring sparsity in recurrentneural networks,2017, arXiv preprint arXiv:1704
 On the difficulty of training recurrent neuralnetworks,2013, In International conference on machine learning
 Pruning algorithms-a survey,1993, IEEE transactions on Neural Networks
 Compression of neural machinetranslation models via pruning,2016, arXiv preprint arXiv:1606
 Picking winning tickets before training bypreserving gradient flow,2020, In International Conference on Learning Representations
 Hardware-oriented compression of long short-termmemory for efficient inference,2018, IEEE Signal Processing Letters
 Learning intrinsic sparse structures within long short-term memory,2017, arXivpreprint arXiv:1709
 Learningcompact recurrent neural networks with block-term tensor decomposition,2018, In Proceedings of theIEEE Conference on Computer Vision and Pattern Recognition
