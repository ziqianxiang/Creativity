title,year,conference
 High-dimensional dynamics of generalization error in neuralnetworks,2017, arXiv preprint arXiv:1710
 A convergence analysis of gradientdescent for deep linear neural networks,2018, arXiv preprint arXiv:1810
 Gradient descent with identity initialization effi-ciently learns positive definite linear transformations,2018, In International Conference on MachineLearning
 Dynamical isometry and a mean fieldtheory of rnns: Gating enables signal propagation in recurrent neural networks,2018, arXiv preprintarXiv:1806
 Width provably matters in optimization for deep linear neural networks,2019, InInternational Conference on Machine Learning
 Dynamical isometry and a mean field theory of lstms and grus,2019, arXiv preprintarXiv:1901
 Stable architectures for deep neural networks,2017, Inverse Problems
 Identity matters in deep learning,2016, International Conference onLearning Representations
 Recurrent orthogonal networks and long-memorytasks,2016, arXiv preprint arXiv:1602
 Deep learning without poor local minima,2016, In Advances in Neural InformationProcessing Systems
 Imagenet classification with deep convo-lutional neural networks,2012, In Advances in neural information processing systems
 A recurrent neural network without chaos,2016, arXiv preprintarXiv:1612
 Deep linear networks with arbitrary loss: All local minimaare global,2018, In International Conference on Machine Learning
 A simple way to initialize recurrent networksof rectified linear units,2015, arXiv preprint arXiv:1504
 Spectrum concentration in deep residual learning: a free probabilityapproach,2019, IEEE Access
 Depth creates no bad local minima,2017, arXiv preprintarXiv:1702
 Resurrecting the sigmoid in deeplearning through dynamical isometry: theory and practice,2017, In Advances in neural informationprocessing systems
 The emergence of spectral univer-sality in deep networks,2018, arXiv preprint arXiv:1802
 Exponential convergence time of gradient descent for one-dimensional deep linearneural networks,2018, arXiv preprint arXiv:1809
 Dynam-ical isometry is achieved in residual networks in a universal way for any activation function,2019, InThe 22nd International Conference on Artificial Intelligence and Statistics
 Critical points of linear neural networks: Analytical forms and land-scape properties,2018, 2018
