title,year,conference
 High-dimensional dynamics of generalization error in neuralnetworks,2017, arXiv preprint arXiv:1710
 Spectrally-normalized margin bounds forneural networks,2017, In Advances in Neural Information Processing Systems
 On fast dropout and its applicability to recurrent networks,2013, arXiv preprintarXiv:1311
 Learning long-term dependencies with gradientdescent is difficult,1994, IEEE transactions on neural networks
 State-of-the-artspeech recognition with sequence-to-sequence models,2018, In 2018 IEEE International Conferenceon Acoustics
 A theoretically grounded application of dropout in recurrentneural networks,2016, In Advances in neural information processing systems
 Size-independent sample complexity ofneural networks,2018, In Proceedings of the 31st Conference On Learning Theory
 Generating sequences with recurrent neural networks,2013, arXiv preprintarXiv:1308
 Nearly-tight vc-dimension bounds for piece-wise linear neural networks,2017, In Conference on Learning Theory
 Recurrent continuous translation models,2013, In Proceedings ofthe 2013 Conference on Empirical Methods in Natural Language Processing
 Rademacher complexity margin bounds for learningwith a large number of classes,2015, In ICML Workshop on Extreme Classification: Learning with aVery Large Number of Labels
 Distribution of eigenvalues forsome sets of random matrices,1967, Matematicheskii Sbornik
 Stable recurrent models,2019, In International Conference on LearningRepresentations
 Norm-based capacity control in neuralnetworks,2015, In Conference on Learning Theory
 Stochastic gradient descent learns state equations with nonlinear activations,2018, arXivpreprint arXiv:1809
 On the difficulty of training recurrent neuralnetworks,2013, In International Conference on Machine Learning
 A matrix inequality associated with bounds on solutions of algebraic riccatiand lyapunov equations,1987, IEEE Transactions on Automatic Control
 Understandingdeep learning requires rethinking generalization,2017, 2017
 Stabilizing gradients for deep neural networks via efficientsvd parameterization,2018, In International Conference on Machine Learning
