Table 1: Comparison with WSNet on ESC-50 dataset. We use the same configuration but change the samplingstride to be learnable. ‘S’ denotes the stride and ‘C’ denotes the repetition times along the input channeldimension. We use the ‘S’ in WSNet as initial values and learns the offsets.
Table 2: Results of ImageNet classification. Our method uses vanilla MobileVetV2 as backbone. For a faircomparison, we evaluate multiple width multiplier values of 0.75, 0.5, 0.35 and 0.18 and only apply it on thefilter dimension of the first 1 × 1 convolution. We apply the proposed method on all the invert residual blocksequally to disentangle the architecture affects on the performance. MAdd are calculated based on all convolutionblocks with an assumption that the batch normalization layers are merged. '*' denotes our own implementation.
Table 3: Comparison of our method with other state-of-the-art models on ImageNet where our method showssuperior performance over all other methods. MAdd are calculated based on all convolution blocks with anassumption that the batch normalization layers are merged. Suffix ‘-A’ means we use larger compression ratio forfront layers. Our method does not modify the backbone model architecture and applies a uniform compressionratio, unless specified with suffix ‘-A’. All experiments are using MobileNetV2 as backbone unless labeled withEfficientNet as suffix.
Table 4: Epitome dimensions for the inverted residual blocks of the MobileNetV2 backbone. Here w, h, k, k0denotes the spatial size, input channels and output channels of the input feature map respectively. Variableswc and hc denote the spatial size of the epitome and are set to 1 for 1 X 1 convolutional layer. c is usedto set the compression ratio for each layer and is similar to the concept of width multiplier as defined inMobileNet (Howard et al., 2017). t is the expansion ratio as defined in MobileNetV2.
Table 5: Comparison with WSNet on ESC-50 dataset. We choose the compressed model by WSNet method asour baseline. By decreasing the size of the epitome, we can achieve higher compression ratio. We apply uniformcompression ratio for all layers. It is observed that the highest compression ratio we can achieve before ourmethod’s performance become smaller than WSNet is 3.16×.
Table 6: Comparison with other state-of-the-art models in classification accuracy on CIFAR-10. Our methodoutperforms the recently proposed AUTO-SLIM (Yu & Huang, 2019a) which is a compression method usingAutoML method.
Table 7: Experiment results of our method applied on fully connected layer of MobileNetV2.
