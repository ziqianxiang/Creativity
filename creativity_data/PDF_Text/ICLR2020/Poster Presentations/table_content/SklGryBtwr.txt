Table 1: Accuracy extending a negation predicate to novel arguments (test accuracy) when agents are trained to negate different numbers of words/objects.			that adult humans are far from perfectly sys-tematic (Lake et al., 2019))), given experience of 100 objects (again, not orders of magnitude dif-ferent from typical human experience), to be notable, particularly given the history of researchinto learning logical operators in connectionist models and the importance of negation in languageprocessing (Steedman, 1999; 2011). In what follows, we complement these observations by estab-lishing that not only the amount, but also the type of training data (or agent experience) can have asignificant impact on emergent generalisation.
Table 2: Tests of systematic generalisation in 2D and 3D environ-ments; five randomly-initialized agent replicas in each condition.
Table 3: generalisation accuracy achieved by a vision-and-language classifier trained on single screenshots versus a sit-uated agent trained in the DMLab environment.
Table 4: Action space of the agent in the 3D room.
Table 5:	Object/Color sets in the Lifting experiment.
Table 6:	Object/Color sets in the Putting experiment.
Table 7:	Object/Color sets in the Negation experiment.
Table 8: Object sets in the 2D x 3D experiment. The same sets of objects were used for DeepMind Lab andour 2D environment.
