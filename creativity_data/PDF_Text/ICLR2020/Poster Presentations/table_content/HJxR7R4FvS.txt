Table 1: Comparison on three large datasets. The best testing set performance is reported. The resultsbelow the line are from Liang et al. (2018), and VAE本 shows the VAE results based on our runs. Blueindicates improvement over the VAE baseline, and bold indicates overall best.
Table 2: Performance gain (×10-3) for various actors.
Table 3: Summary Statistics of datasets after all pre-processing steps. Interactions# is the number ofnon-zero entries. Sparsity% refers to the percentage of zero entries in the user-item interaction matrixX . Items# is the number of total items. HO# is the number of validation/test users held out of thetotal number of users in the 5th column Users#.
Table 4: Network architectures. The arrow indicates the flow between two layers. For each layer, weshow the number of units on top of its following activation function. BN indicates Batch Normalization.
Table 5: Summary of training schedule hyper-parameters. βmax indicates the maximum value of β .
Table 6: Performance trained with different metrics. (ML-20M)Training	Testing			Recall@20	Recall@50	NDCG@100RaCT (Recall@100)	0.40316	0.54317	0.43392RaCT (NDCG@100)	0.40269	0.54304	0.43395VAE	0.39623	0.53632	0.42586A fully-connected (FC) architecture is used for all networks, as detailed in Table 4. Please referto Goodfellow et al. (2016) for the activation functions. Batch Normalization (Ioffe & Szegedy, 2015)is used to normalize the input features, because the magnitude of the inputs (NLL) change as trainingprogresses. The encoder outputs the mean and variance of the varational distribution; the variance isimplemented via an exponential function.
Table 7: Comparison between our RaCT with NCF on two small datasets. NCF results are from Lianget al. (2018).
