Table 1: Success rates for all tasks, comparing our method against baselines. Each entry in the tablerepresents average success rate and standard deviation over 100 runs. The baselines learning fromscratch fail to learn complex tasks with multiple agents.
Table 2: Environment detailsA.1 Environment descriptionsIn both Jaco environments, the robot works on a table with size (1.6, 1.6) and top center position(0, 0, 0.82). The two Jaco arms are initialized at positions (-0.16, -0.16, 1.2) and (-0.16, 0.24, 1.2).
Table 3: HyperparametersB.2	Network architecturesActor Networks: In all experiments, we model our actor network for each primitive skill as a 3-layerMLP with hidden layer size 64. The last layer of the MLP is two-headed - one for the mean of theaction distribution and the other for the standard deviation of it. We use ReLU as activation functionin hidden layers. We do not apply any activation function for the final output layer. The actiondistribution output represents per-dimension normal distribution, from which single actions can besampled and executed in the environment.
