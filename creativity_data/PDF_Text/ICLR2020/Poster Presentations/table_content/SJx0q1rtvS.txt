Table 1: AUPR scores for outlier detection (OD) and novelty detec-tion (ND). σ = 0 indicates applying clipping bound only.
Table 2: Backdoor attack and detection results with varying poisoning ratio rp (clipping bound C = 1).
Table 3: Backdoor attack and detection results with varying poisoning ratio rp (noise scale σ = 0.5).
Table 4: AUPR scores for autoencoder outlier detection (OD) and novelty detection (ND).
Table 5: AUROC scores for autoencoder outlier detection (OD) and novelty detection (ND).
Table 6: Probability-based anomaly detection results.
Table 7: AUROC score comparison and privacy bound .
Table 8: Benign accuracy of models trained on datasets with different poisoning ratio rp . The more noise beingadded, the more utility is affected.
Table 9: Backdoor attack success rate of models trained on datasets with different poisoning ratio rp . Thesuccess rate is significantly reduced for models trained with differential privacy.
Table 10: AUPR scores for backdoor attack detection. Applying differential privacy significantly improves theresults.
Table 11: AUROC scores for backdoor attack detection. It shows that measuring model loss for poisoning samplesdetection could be effective when the poisoning ratio is low. Differential privacy improves the performance in allcases, except when the noise scale is too big to ruin the model parameters.
