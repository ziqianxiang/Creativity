Table 1: The Effect of Word-piece Overlap and of Structural Similarity For different pairs of B-BERTlanguages, and for two tasks (XNLI and NER), we show the contribution of word-pieces to the success ofthe model. In every two consecutive rows, we show results for a pair (e.g., English-Spanish) and then for thecorresponding pair after mapping English to a disjoint set of word-pieces. The gap between the performance ineach group of two rows indicates the loss due to completely eliminating the word-piece contribution. We addan asterisk to the number for NER when the results are statistically significant at the 0.05 level.
Table 2: Contribution of Word-Ordering similarity: We study the importance of word-order similarity byanalysing the performance of XNLI and NER when some percent of word-order similarity is reduced. Thepercent p controls the amount of similarity (how random each sentence is permuted). We can see that word-order similarity is quite important, however there must be other components of structural similarity that couldcontribute for the cross-lingual ability, as the performance of almost random is still passable.
Table 3: Cross-lingual ability from only unigram frequency: We study if only unigram frequency is usefulfor cross-lingual transfer. enfake-es indicates B-BERT trained with Fake-English and the new created corpus,where each sentence is a set of random words sampled from the same unigram distribution as es. The resultsshow that only unigram frequency is not enough for a reasonable cross-lingual performance.
Table 4: The Effect of Depth of B-BERT Architecture: We use Fake-English and Russian B-BERT andstudy the effect of depth of B-BERT towards XNLI. We vary the depth and fix both the number of attentionheads and the number of parameters - the size of hidden and intermediate units are changed so that the totalnumber of parameters remains almost the same. We train only on Fake-English and test on both Fake-Englishand Russian and report their test accuracy. The difference between the performance on Fake-English andRussian(∆) is our measure of cross-lingual ability (lesser the difference, better the cross-lingual ability).
Table 5: The Effect of Multi-head Attention: We study the effect of the number of attention heads ofB-BERT on the performance of Fake-English and Russian language on XNLI data. We fix both the number ofdepth and number of parameters of B-BERT and vary the number of attention heads. The difference betweenthe performance on Fake-English and Russian (∆) is our measure of cross-lingual ability.
Table 6: The Effect of Total Number of Parameters: We study the effect of the total number of parametersof B-BERT on the performance of Fake-English and Russian language on XNLI data. We fix both the numberof depth and number of attention heads of B-BERT and vary the total number of parameters by changing thesize of hidden and intermediate units. The difference between the performance on Fake-English and Russian(∆) is our measure of cross-lingual ability.
Table 7: Similar results on M-BERT with four languages: We show that the insights derived from bilingualBERT is also valid in the case of multilingual BERT (4 language BERT). Further, we also show that withenough depth, we only need a fewer number of parameters and attention heads to get comparable results.
Table 8: Effect of Next Sentence Prediction Objective: We study the effect of NSP objective on XNLI andNER. Column NSP and No-NSP show the performance (accuracy for XNLI and average (stdev) F1-score forNER) when B-BERT is trained with and without NSP objective respectively. The difference between the NSPand No-NSP shows that NSP objective hurts performance.
Table 9: Effect of Language Identity Marker in the Input: We study the effect of adding a languageidentifier in the input data. We use different end of string ([SEP]) tokens for different languages serving aslanguage identity marker. Column “With Lang-id” and “No Lang-id” show the performance when B-BERT istrained with and without language identity marker in the input.
Table 10: Effect of Character vs Word-Piece vs Word tokenization. We compare the performance ofB-BERT with different tokenized input on XNLI and NER data. Column Char, Word-Piece, Word reports theperformance of B-BERT with character, word-piece and work tokenized input respectively. We use 2k batch sizeand 500k epochs.
Table 11: Premise and Hypothesis in different language: Using XNLI test set, we construct textual en-tailment data with premise and hypothesis in different languages. The column A-B (e.g. enfake-target) refersto test data with premise in language A (enfake) and hypothesis in language B (target). We always train onFake-English and report test accuracy.
