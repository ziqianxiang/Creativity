Table 1: Precision gating (PG) on CNN - models tested are ShiftNet-20 and ResNet-20 on CIFAR-10, and ShuffleNet V2 0.5× on ImageNet. We compare PG against uniform quantization (UQ),PACT, and Fix-Threshold. Bavg is the average bitwidth. “fp” denotes floating-point accuracy. “Sp”denotes sparsity.
Table 2: Comparison with SeerNet on CNN - compare PG against SeerNet under similar modelprediction accuracy. In SeerNet the average bitwidth Bavg = Bhb + (1 - Sp) × B.
Table 3: Sweeping manually set thresholds - We sweep a series of manually set thresholds forResNet-20 on CIFAR-10. Compared to manually setting thresholds, PG achieves a better modelaccuracy (91.2%) with a larger sparsity (90.1%).
Table 4: PG with and without sparse back-propagation (SpBP) on CNNs.
Table 5: PG on LSTM - the dataset used is Penn Tree Bank (PTB). The metric is perplexity perword (PPW) and lower is better. Floating-point PPW is 110.1.
Table 6: SDDMM kernel sparsity and speedup - We report optimized kernel execution time andwall-clock speedup of each layer in ResNet-20 for CIFAR-10.
Table 7: Precision gating (PG) on CNN - additional models tested are ResNet-32 and ResNet-56on CIFAR-10. We compare PG against uniform quantization (UQ), PACT, and Fix-Threshold. “fp”is floating-point accuracy. “Sp” is sparsity.
