Table 1: Performance comparison of our model against SAC (Haarnoja et al., 2018b), TRPO (Schul-man et al., 2015), and PPO (Schulman et al., 2017) on benchmark control tasks in terms of distance(lower the better) of an agent from the given target. The mean final distances with standard devia-tions over ten trials are reported. We also normalize the reported values by the agent initial distancefrom the goal so values close to 1 or higher show failure. It can be seen that our method (shown inbold) accomplishes the tasks by reaching goals whereas other methods fail except for SAC in simplePusher and Ant Random Goal environments.
Table 2: HyperparametersC.2.2 PusherIn pUsher environment, a simple manipUlator has to move an object to the target location. Theprimitive policies were to pUsh the object to the bottom and left. These low-level policies wereobtained via SAC after 0.1 million training steps. In this environment, the state information for bothprimitive policies and the composite policy inclUde the goal location. Therefore, G, in this case, isnUll. The reward fUnction is given as:-λg ||oxy - gxy ||2 - λoMrxy - oxy ||2 - λct ||u||2	(II)14Published as a conference paper at ICLR 2020	Model Architectures	Hidden units	High-level Policy: Three layer feed forward network	300Composition-HIRO	Encoder Network: Bidirectional RNN with LSTMs	128	Decoder Network (Single layer feed forward network)	128	Attention Network: Wf, Wb, Wd ∈ Rd×d; W ∈ Rd	128	Encoder Network: Bidirectional RNN with LSTMs	128Composition-SAC	Decoder Network (Single layer feed forward network)	128	Attention Network: Wf, Wb, Wd ∈ Rd×d; W ∈ Rd	128HIRO	High-level Policy: Three layer feed forward network	300	Low-level Policy: Three layer feed forward network	300
Table 3: Network Architectures.The right most column shows the hidden units per layer.
