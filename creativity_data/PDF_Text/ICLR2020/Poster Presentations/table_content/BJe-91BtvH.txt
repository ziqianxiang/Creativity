Table 1: FID and KID scores (lower is better) for generatedimages using the common part of a ∈ A and the separatepart of b ∈ B . As real images, we consider the images in A.
Table 2: The accuracy of generatedimages according to a pretrained clas-sifier distinguishing between A and B.
Table 3: An evaluation of the cosine similarity (higher is better) before and after translation betweenthe VGG-face descriptors. Shown are average results over 100 random images created by sampling aand b from the test sets.
Table 4: User study (questions (1), (2) and (3)) showing preference to our method vs. Press et al.		(2019), see text.		A to B mapping	A’ to B’ shift	Facial hair male to male	Glasses all genders	Smile all genders	Hand- bags	Two Attrs	Remove Smile Add Glasses	Facial hair swap	Facial hair female A0, B0	Glasses train women test men(1)	96%	95%	55%	87%	91%	83%	93%	91%	93%(2)	84%	82%	43%	72%	93%	90%	83%	70%	86%(3)	97%	95 %	95%	90%	95%	91%	91%	91%	97%To evaluate the interpretability of the latent space, we interpolate between the latent code of theseparate parts of images b1 ∈ B and b2 ∈ B with the common latent code of an image a ∈ A. Thisis shown in Fig. 4 and appendix C. Note the mask changes throughout the interpolation.
Table 5: Mean and SD IoU for the two hair seg-mentation benchmarks.
Table 6: Attribute removal for the task of Smile, Facial hair and Glasses.
Table 7: The effect of removing losses. Shownare classifier accuracy, cosine similarity, KID, andpercentage of mask from the total size of the face.
