Table 1: BLEU scores for the base models on IWSLT'14 EnglishrGerman, IWSLT’13EnglishrFrench, and the base and big models on WMT'14 EngliSh→German task. Refer to Table5 in the Appendix for parameter comparisons.
Table 4: Attention distributions (%) between phrases (nodes) and tokens (leaves) across differenttranslation tasks. Statistics are derived from IWSLT’14 En-De and IWSLT’13 En-Fr test sets.
Table 2: Classification results in accuracy (%) onStanford Sentiment Analysis fine-grained (SST-5) and binary (SST-2), IMDB sentiment analysis,and Subject-Verb Agreement (SVA) tasks.
Table 3: Performances of different modelvariants on IWSLT’14 En-De, IWSLT’13 En-Frand Stanford Sentiment Analysis (fine-grained)tasks. '-HierEmb': no hierarchical embeddings,'-SubMask'： no subtree masking.
Table 5: Exact number of parameters for Transformer and our model, both used for WMT’14English-German task.
