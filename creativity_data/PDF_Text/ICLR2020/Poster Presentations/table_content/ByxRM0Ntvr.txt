Table 1: Performance of bi-linear projection and separable convolution layers on masked LM pre-training task and MNLI. Note that we expect these computationally cheaper models to have lowerperformance than the expensive Transformers as they do not compute input dependent attentionweights and have weaker representation power. Our goal in studying them is to see if they cansubstitute some of the expensive attention layers for computing the contextual mappings. Thesemodels are trained in a large batch setting, with a batch size of 8192 for 60k steps, unlike the otherset of experiments reported in Fig. 1. Note that average attention has clearly worse performance,showing that theses tasks indeed require an advanced architecture.
