Table 1: Anomaly detection benchmarks.
Table 2: Results on classic AD benchmark datasets in the setting with no pollution γp = 0 and a ratioof labeled anomalies of γl = 0.01 in the training set. We report avg. AUC with st. dev. computedover 10 seeds. A “?” indicates a statistically significant (α = 0.05) difference between 1st and 2nd.
Table 3: Complete results of experimental scenario (i), where we increase the ratio of labeled anomalies γl in the training set. We report the avg. AUC withst. dev. computed over 90 experiments at various ratios γl .
Table 4: Complete results of experimental scenario (ii), where we pollute the unlabeled part of the training set with (unknown) anomalies. We report the avg. AUCwith st. dev. computed over 90 experiments at various ratios γp .
Table 5: Complete results of experimental scenario (iii), where we increase the number of anomaly classes kl included in the labeled training data. We report theavg. AUC with st. dev. computed over 100 experiments at various numbers kl .
Table 6: Complete results on classic AD benchmark datasets in the setting with no pollution γp = 0 and a ratio of labeled anomalies of γlset. We report the avg. AUC with st. dev. computed over 10 seeds.
