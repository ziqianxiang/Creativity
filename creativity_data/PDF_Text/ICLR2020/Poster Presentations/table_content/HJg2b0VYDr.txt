Table 1: SVP performance on active learning. Average (± 1 std.) data selection speed-ups from 3runs of active learning using least confidence uncertainty sampling with varying proxies and labelingbudgets on four datasets. Bold speed-ups indicate settings that either achieve lower error or are within1 std. of the mean top-1 error for the baseline approach of using the same model for selection and thefinal predictions. Across datasets, SVP sped up selection without significantly increasing the error ofthe final target. Additional results and details are in Table 3.
Table 2: Number of parameters in each model.
Table 3: SVP performance on active learning. Average (± 1 std.) top-1 error and data selection speed-ups from 3 runs of active learning with varying proxies,methods, and labeling budgets on five datasets. Bold speed-ups indicate settings that either achieve lower error or are within 1 std. of the mean top-1 error forthe baseline approach of using the same model for selection and the final predictions. Across datasets and methods, SVP sped up selection without significantlyincreasing the error of the final target.
Table 4: Performance of training for fewer epochs on active learning. Average (± 1 std.) top-1 error and data selection speed-ups from 3 runs of active learningwith varying proxies trained for a varying number of epochs on CIFAR10, CIFAR100, and ImageNet. Bold speed-ups indicate settings that either achieve lower erroror are within 1 std. of the mean top-1 error for the baseline approach of using the same model for selection and the final predictions. Training for fewer epochs canprovide a significant improvement over random sampling but is not quite as good as training for the full schedule.
Table 5: Average top-1 error (± 1 std.) from 3 runs of core-set selection with varying selectionmethods on ImageNet, Amazon Review Polarity, and Amazon Review Full.
Table 6: Average (± 1 std.) top-1 error and runtime in minutes from 5 runs of core-set selection with varying proxies, selection methods, and subset sizes on CIFAR10and CIFAR100.
Table 7: Average top-1 error (± 1 std.) and runtime in minutes from 5 runs of core-set selection with varying selection methods calculated from ResNet20 modelstrained for a varying number of epochs on CIFAR10 and CIFAR100.
