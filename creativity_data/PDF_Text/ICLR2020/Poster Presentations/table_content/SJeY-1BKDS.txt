Table 1: Similarities among fixed-point algorithms for Power Iteration, FastICA, and MSP.
Table 2: Comparison of the MSP algorithm (Zhai et al., 2019b) with prior complete dictionarylearning algorithms: Subgradient method (Bai et al., 2018) and Riemannian Trust Region (Sunet al., 2015) methods in different models with fixed ground truth sparsity Î¸ = 0.3. Note that SGonly learns a unit vector each time and does not guarantee orthogonality; we therefore project thedictionary learned to O(n; R) for fair comparison.
Table 3: Statistics about the inner products between the top 20 noisy bases and their correspondingclosest top-20 clean bases.
