Table 1: Results of published models on the GLUE test set, which are scored by the GLUE evaluationserver. The number below each task denotes the number of training examples. The state-of-the-art results are in bold. All the results are obtained from https://gluebenchmark.com/leaderboard (StructBERT submitted under a different model name ALICE). * indicates theensemble model. Model references: 1: (Devlin et al., 2018); 2: (Phang et al., 2018); 3: (Joshi et al.,2019); 4: (Ratner et al., 2017); 5: (Liu et al., 2019a); 6: (Yang et al., 2019b); 7: (Liu et al., 2019b).
Table 2: Accuracy (%) on the SNLI dataset.
Table 3: SQuAD results. The StructBERTLarge ensemble is 10x systems which use different pre-training checkpoints and fine-tuning seeds.
Table 4: Ablation over the pre-training objectives using StructBERTBase architecture. Every result isthe average score of 8 runs with different random seeds (the MNLI accuracy is the average score ofthe matched and mis-matched settings).
