Table 1: Results on the PTB test set. Bold numbers correspond to the top 3 results for each column.
Table 2: Results of training a pseudo-optimum fideal with PTB and XLNet-base model.
Table 3: The definitions of distance measure functions for computing syntactic distances betweentwo adjacent words in a sentence. Note that r = gv(wi), s = gv(wi+1), P = gd(wi), and Q =gd(wi+1), respectively. d: hidden embedding size, n: the number of words (w) in a sentence (S).
Table 4: Results on the MNLI test set. Bold numbers correspond to the top 3 results for each column.										LTayer number, A: attention head number (AVG: the average of all attentions). f									: Results reported	by Htut et al. (2018) and Drozdov et al. (2019).点 Approaches in which COO parser is utilized. *:										These results are not strictly comparable to			ours	, due to the difference in data preprocessing.						Model	f	L	A	S-F1	SBAR	NP	VP	PP	ADJP	ADVPBaselines Random Trees	-	-	-	21.4	11%	25%	16%	22%	22%	27%Balanced Trees	-	-	-	20.0	8%	29%	11%	20%	22%	32%Left Branching Trees	-	-	-	8.4	6%	13%	1%	4%	1%	8%Right Branching Trees	-	-	-	51.9	65%	28%	75%	47%	45%	30%Random XLNet-base (F v )	-	-	-	22.0	12%	26%	15%	22%	22%	25%Random XLNet-base (F d)	-	-	-	23.5	14%	26%	18%	22%	22%	25%Pre-trained LMs (w/o bias) BERT-base	HEL	9	10	36.1	36%	37%	34%	45%	26%	42%BERT-large	JSD	17	10	37.0	38%	32%	34%	50%	22%	39%GPT2	JSD	1	10	44.0	43%	53%	31%	60%	24%	40%GPT2-medium	JSD	3	12	49.1	57%	32%	61%	44%	35%	37%RoBERTa-base	JSD	10	9	36.2	26%	35%	34%	50%	23%	44%RoBERTa-large	JSD	3	6	39.8	20%	28%	35%	30%	28%	27%XLNet-base	HEL	1	6	39.0	25%	39%	28%	59%	35%	44%XLNet-large	HEL	1	15	42.2	32%	49%	27%	62%	32%	49%Pre-trained LMs (w/ bias λ=1.5) BERT-base	HEL	2	12	52.7	64%	35%	70%	50%	46%	30%
