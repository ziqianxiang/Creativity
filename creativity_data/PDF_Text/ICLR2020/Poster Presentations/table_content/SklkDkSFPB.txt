Table 1: Spearman Rank correlation scores for several ranking metrics when ranking 100 randomarchitectures at three parameter budgets. Negative correlation implies that as the metric score goesup, final error goes down. Summing gradient norms follows a similar pattern to Fisher potential butappears to be less robust and moderately less accurate. '2-norms are extremely volatile and largelyuninformative. Crucially, adding more than 1 minibatch has diminishing returns.
Table 2: CIFAR-10 top-1 test error for student nets, with parameter count (in thousands, as P.(K))and total MAC operations (in millions, as Ops(M)). D-W specifies the number of layers and thewidth multiplier of the student. We compare BlockSwap to reductions that rely on reducing depth(D) and width (W), single-blocktype networks (MBConv6, DARTS, CondenseNet), and pruningvia SNIP (Lee et al., 2019). Comparisons to random configurations and '1-pruning are givenin Appendix C. We do not report Ops for SNIP since this is dependent on the choice of sparserepresentation format. BlockSwap is able to choose the networks with the lowest mean error for allparameter budgets considered.
Table 3: Top-1 and Top-5 classification errors (%) on the validation set of ImageNet for studentstrained with attention transfer from a ResNet-34. We can see that for a similar number of parameters,the student found from BlockSwap outperforms its counterparts, and in one instance, the teacher.
Table 4: Average Precisions (%) for COCO-2017 val detection for Mask R-CNNs using a ResNet-34-G(N) and BlockSwap backbone (each using 3M parameters).
Table 5: Block substitutions used in this paper: Conv refers to a k × k convolution. GConv is agrouped k × k convolution and Conv1 × 1 is a pointwise convolution. We assume that the input toeach block has N channels and that channel size doesn’t change unless written explicitly as (x → y).
Table 6: Comparing BlockSwap to randomly configured mixed-blocktype networks and '1-prunedversions of the original teacher on the CIFAR-10 dataset. Total MAC operations are not reported for'1-pruning because calculating this is highly dependent on choice of sparse tensor representationformat.__________________________________________________________________________D-W	Method	Parameters (K)	MAC Ops (M)	Error (μ ± σ)40-2	BlockSwap	811.4	132.5	3.79 ± 0.0140-2	BlockSwap	556.0	89.5	4.17 ± 0.2240-2	BlockSwap	404.2	92.8	4.21 ± 0.1340-2	BlockSwap	289.2	65.9	4.45 ± 0.1840-2	BlockSwap	217.0	38.8	4.68 ± 0.3740-2	BlockSwap	162.2	33.9	5.17 ± 0.0040-2	Random	795.7	81.6	4.26 ± 0.0440-2	Random	551.1	76.7	4.54 ± 0.1840-2	Random	397.4	59.5	4.91 ± 0.1240-2	Random	285.1	56.7	4.80 ± 0.1240-2	Random	217.6	46.9	5.13 ± 0.2440-2	Random	168.3	33.6	5.75 ± 0.0940-2	'1-pruning	894.7	-	4.12 ± 0.0340-2	'1-pruning	760.5	-	4.32 ± 0.0640-2	'1-pruning	671.1	-	4.51 ± 0.13
