Table 1: The setup of few-shot learning. If task t is used for meta-testing, yt is not given to the model.
Table 2: Average classification accuracies (with 95% confidence intervals) on the test-set of Mini-ImageNet and CIFAR-FS. For evaluation, we sample 2000 and 5000 episodes respectively forMiniImageNet and CIFAR-FS and test three different architectures as the feature extractor: Conv-4-64, Conv-4-128 and WRN-28-10. We train SIB with learning rate 0.001 and try different numbers ofsynthetic gradient steps K .
Table 3: Multi-source domain adaptation results on PACS with ResNet-18 features. Three domainsare used as the source domains keeping the fourth one as target.
Table 4: Average 5-way classification accuracies (with 95% confidence intervals) with Conv-4-64 onthe test set of CIFAR-FS. For each test, we sample 5000 episodes containing 5 categories (5-way)and 15 queries in each category. We report the results with using different learning rate Î· as wellas different number of updates K . Note that K = 0 is the performance only using the pre-trainedfeature.
Table 5: Average classification accuracies on the validation set and the test set of Mini-ImageNetwith backbone Conv-4-128. We modify the number of query images, i.e., n, for each episode to studythe effect on generalization.
