Table 1: Effectiveness of budget-aware conversion (BAC) on CIFAR-10 for image classificationwith ResNet-18 (He et al., 2016). The numbers are classification accuracy on the validation set.
Table 2: Comparison of learning rate schedules on CIFAR-10. The 1st, 2nd and the 3rd place undereach budget are color coded. The number here is the classification accuracy and each one is theaverage of 3 independent runs. “step-dx” denotes decay x times at even intervals with γ = 0.1. For“exp” and “step” schedules, BAC (Sec 3.1) is applied in place of early stopping. We can see linearschedule surpasses other schedules under almost all budgets.
Table 3: Robustness of linear schedule across budgets, tasks and architectures. Linear schedulesignificantly outperforms step decay given limited budgets. Note that the off-the-shelf decay foreach dataset has different parameters optimized for the specific dataset. For all step decay schedules,BAC (Sec 3.1) is applied to boost their budgeted performance. To reduce stochastic noise, we reportthe average and the standard deviation of 3 independent runs. See Sec 4.2 for the metrics of eachtask (the higher the better for all tasks).
Table 4: Where does one expect to find the model with the highest validation accuracy within thetraining progress? Here we show the best checkpoint location measured in training progress p andaveraged for each schedule across budgets greater or equal than 10% and 3 different runs.
Table A: Small-budget and full-budget model rank correlation measured in Kendall’s tau. Smooth-decaying schedules like linear and cosine can more accurately Predict the true rank of differentarchitectures given limited budget.
Table B: Small-budget validation accuracy averaged across random architectures. Linear scheduleis the most robust under small budgets.
Table C: Tab B normalized by the full-budget accuracy and then averaged across architectures.
Table D: Comparison with offline data subsampling. “Subset” meets the budget constraint by ran-domly subsample the dataset prior to training, while “full” uses all the data, but restricting thenumber of iterations. Note that budget-aware schedule is used for “full”.
Table E: Comparison with off-the-shelf poly schedule on Cityscapes Cordts et al. (2016) using PSP-Net Zhao et al. (2017). Poly and linear are similar smooth-decaying schedules (Fig 2) and thus havesimilar performance. The exact rank differs from task to task.
Table F: Comparison with off-the-shelf SGDR at the end of each period after the first restart.
Table G: Comparison with SGDR under budget-aware setting. “SGDR-r1” refers to restarting learn-ing rate once at midpoint of the training progress, and “SGDR-r2” refers to restarting twice at eveninterval.
Table H: Comparison between linear and step decay with different batch sizes. We can see that evenwhen we vary the batch size, linear schedule outperforms step decay.
Table I: Comparison between linear and step decay with different initial learning rate under fullbudget setting. On one hand, we see that linear schedule outperforms step decay under variousinitial learning rate. On the other hand, we see that initial learning rate is still a very importanthyper-parameter that needs to be tuned even with budget-aware, smooth-decaying schedules.
