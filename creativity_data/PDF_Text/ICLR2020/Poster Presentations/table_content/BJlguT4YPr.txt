Table 1: Summary of notation used in the paper. (This excludes notation used in defining models forthe KB completion and QA tasks of Section 3.)reinforcement learning (e.g., (Misra et al., 2018)). Some systems have also “neuralized” KB reasoning,but to date only over small KBs: this approach is natural when answers are naturally constrained todepend on a small set of facts (e.g., a single table (Zhong et al., 2017; Gupta & Lewis, 2018)), butmore generally requires coupling a learner with some (non-differentiable) mechanism to retrieve anappropriate small question-dependent subset of the KB as in (Sun et al., 2018; 2019).
Table 2: Complexity of implementations of relation-set following, where NT is the number of KBtriples, NE the number of entities, NR the number of relations, and b is batch size.
Table 3: Hits@1 on the KBQA datasets. Results for KV-Mem and VRN on MetaQA are from (Zhanget al., 2018); results for GRAFT-Net, PullNet and KV-Mem on WebQSP are from (Sun et al., 2018)and (Sun et al., 2019).
Table 4: Left: Hits@1 and Hits@10 for KB completion on NELL 995. Starred KB completionmethods are transductive, and do not generalize to entities not seen in training. Right: Comparison toMINERVA on several tasks for Hits@1.
Table 5: Left, time to run 10K examples for KBs of different size. Right, time for 10k examplesvs Hits@1 performance for ReifKB compared to three baselines on MetaQA-3hop questions.
