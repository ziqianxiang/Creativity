Table 1: Examples of generated questions at varying levels (L) of complexity.
Table 2: (a) CFQ dataset statistics. (b) CFQ complexity statistics in comparison to other semanticparsing datasets. Datasets in the first section map text to SQL for various DBs, with numbers asreported by Finegan-Dollak et al. (2018). Datasets in the second section map text to sparql forFreebase. The number of query patterns is determined by anonymizing entities and properties.
Table 3: Comparison of relevant measurements for different split methods on CFQ / scan.
Table 4: Mean accuracies of the three baseline systems on CFQ and SCAN (in %).
Table 5: Most frequent answers in CFQ.
Table 6: Summary of hyperparameters that deviate from the defaults. Default hyperparametersets are: lstm_bahdanau_attention_multi, transformer_base, and universal_transformer_tiny, respec-tively.
Table 7: Examples with a given error (in %) of total test set size. See text for details.
Table 8: Subqueries of “What sibling of M0 was M1’ s parent?” and their occurrences in training.
Table 9: Subqueries of “Did a male film director edit and direct M0 and M1?” and their occurrencesin training.
