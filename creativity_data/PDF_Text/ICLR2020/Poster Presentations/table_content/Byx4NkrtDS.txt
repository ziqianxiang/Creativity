Table 1: Pretraining protocolshyperparameters			protocol name	loss(α,β,γ)	Weights adjusted	P_POSNetI	TToTo	W, Wa, Wi	0.2PosNet2	1,0,0	W	0.2PosNet3	1,0,0	W	1PosNet4	1,1,0	W, Wa, Wi	0.2PosNet5	1,0.1,0	W, Wa, Wi	0.2MemNetI	0, 0.8, 0.2	W	1MemNet2	0, 0.9, 0.1	W	1MemNet3	0, 0.7, 0.3	W	1MemNet4	0, 0.8, 0.2	W, Wa, Wi	1MemNet5	0, 0.8, 0.2	W		0.2Different hyper parameter for RandNets:ht+1	=	(1---)ht +—(tanh(Wht + Wif (zt) +	WaAt	+ WcCt)	(18)ττQ(ht)	=	Woht + bo	(19)Number of neurons used 512, time constant τ is taken to be 2, the choice of hyper parameters isaccording to standard reservoir computing Iitterature (Jaeger, 2010; LUkoseviciUs & Jaeger). Theweights are taken from a standard Normal distribution. It is crucial to choose an appropriate stan-
Table 2: Random Networkshyperparameters			name	W	Wa	WiRandNetI	~L-	1	10RandNet2	1	5	10RandNet3	0.5	1	10RandNet4	0.5	5	10RandNet5	1.2	1	10RandNet6	1.2	5	105.6	Modular Network protocolThe results of modular network are obtained from combining the PosNet 1 and MemNet 25 fromtable . Both the learning ofQ function and V function is learned in the the same way as main resultsequ (7,8,9, 10).
Table 3: Performance of all networks on all tasksprotocol	BSC	HO	BAR	SC	SX	SY	IMLPosNetI	0.91	0.79	0.73	-0.36	0.90	0.19	0.782.PosNet1	0.97	0.78	0.79	-0.47	0.89	0.53	0.113.PosNet1	0.95	0.74	-0.04	-0.16	0.70	0.70	0.664.PosNet1	0.97	0.82	0.68	0.03	0.89	0.21	0.695.PosNet1	0.88	0.74	0.73	-0.36	0.88	0.20	0.786.PosNet1	0.91	0.63	0.48	-0.41	0.62	0.04	-0.217.PosNet1	0.94	0.64	0.51	0.26	0.64	0.15	-0.318.PosNet2	0.89	0.62	0.14	-0.25	0.76	0.24	0.849.PosNet3	0.93	0.48	0.41	0.04	0.22	-0.24	0.7310.PosNet4	0.83	0.58	0.72	-0.07	0.16	0.58	-0.2211.PosNet5	0.86	0.69	0.48	-0.19	0.21	0.65	0.0112.PosNet6	0.80	0.17	0.11	-0.33	0.07	0.36	-0.1013.RandNet1	0.89	0.52	-0.15	0.26	0.79	-0.17	-0.1414.RandNet2	0.84	0.25	-0.89	-0.03	0.62	-0.29	-0.1015.RandNet3	0.51	0.05	-0.93	-0.39	0.30	-0.35	-0.0916.RandNet4	0.65	-0.31	-0.94	-0.37	-0.23	-0.04	-0.1717.RandNet5	0.91	0.22	0.15	-0.16	0.62	0.06	-0.4718.RandNet6	0.88	0.70	-0.36	-0.44	0.63	0.11	-0.33
