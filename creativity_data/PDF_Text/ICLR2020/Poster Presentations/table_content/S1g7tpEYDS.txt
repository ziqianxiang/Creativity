Table 1: Evaluation of all models by FID (lower is better, best models in bold). We evaluate eachmodel by REC.: test sample reconstruction; N : random samples generated according to the priordistribution p(z) (isotropic Gaussian for VAE / WAE, another VAE for 2SVAE) orby fitting a Gaus-sian to qÎ´(z) (for the remaining models); GMM: random samples generated by fitting a mixtureof 10 Gaussians in the latent space; Interp.: mid-point interpolation between random pairs of testreconstructions. The RAE models are competitive with or outperform previous models throughoutthe evaluation. Interestingly, interpolations do not suffer from the lack of explicit priors on the latentspace in our models.
Table 2: Evaluation of random sample quality by precision / recall (Sajjadi et al., 2018) (highernumbers are better, best value for each dataset in bold). Itis notable that the proposed ex-post densityestimation improves not only precision, but also recall throughout the experiment. For example,WAE seems to have a comparably low recall of only 0.88 on MNIST which is raised considerablyto 0.95 by fitting a GMM. In all cases, GMM gives the best results. Another interesting point is thelow precision but high recall of all models on CIFAR-10 - this is also visible upon inspection of thesamples in Fig. 9.
Table 3: Comparing multiple regularization schemes for RAE models. The improvement in recon-struction, random sample quality and interpolated test samples is generally comparable, but hardlymuch better. This can be explained with the fact that the additional regularization losses make tuningtheir hyperparameters more difficult, in practice.
