Table 1: Values for common V-MPO parameters.
Table 2: Native action space for DMLab. See https://github.com/deepmind/lab/blob/master/docs/users/actions.md for more details.
Table 3: Reduced action set for DMLab from Hessel et al. (2018).
Table 4: Multi-task Atari-57 scores by level after 11.4B total (200M per level) environment frames. Allentries show mean ± standard deviation. Data for IMPALA (“PopArt-IMPALA”) was obtained fromthe authors of Hessel et al. (2018). Human-normalized scores are calculated as (E-R)/(H -R)×100,where E is the episode reward, R the episode reward obtained by a random agent, and H is theepisode reward obtained by a human.	18Published as a conference paper at ICLR 2020Setting	Single-task	Multi-taskAgent discount	0.99	Image height	72	Image width	96	Number of action repeats	4	Number of LSTM layers	2	3Pixel-control cost	2×10	-3Ttarget	10	η	0.1	0.5α (log-uniform)	[0.001, 0.01)	[0.01, 0.1)Table 5: Settings for DMLab.
Table 5: Settings for DMLab.
Table 6: Settings for Atari. TrXL: Transformer-XL.
Table 7: Settings for continuous control. For the humanoid gaps task from pixels the physics timestep was 5 ms and the control time step 30 ms.
