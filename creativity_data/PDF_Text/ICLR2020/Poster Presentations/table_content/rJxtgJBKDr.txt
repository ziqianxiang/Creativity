Table 1: Comparison of various transfer/lifelong schemes and SNOW.
Table 2: Overhead of each channel subscription approach for a feature map of N × h × w size.
Table 3: Datasets for experiments.
Table 4: Accuracy changes over varying hyper-parameter settings.
Table 5: Computation costs for inference for the ImageNet (source model), Food, DTD, Action, Car,and CUB. We run all tasks together on a single Tesla V100.
Table 6: Parameter overhead and Top-1 accuracy (%) of SNOW for each feature subscription method.
Table 7: Test accuracy of each approaches with 5 datasets.
Table 8: Computational performance for training on a single GPU12Published as a conference paper at ICLR 202040050Topl validation accuracy (Food)90Ooo8 7 6AQRrnOOE IdOl30 60 90 120 150 180Ooo8 7 6AQRrnOOE IdOl50Topl validation accuracy (DTD)90400 30 60 90 120 150 180
Table 9: Top-1 accuracy of Learning without Forgetting (LF)LF overall showed the training throughput of 272.69 images/sec with 13.99GB memory footprint.
