Table 1:	Corpus statistics and the baselineperformance (% accuracy) of our BiLSTMmodel (without domain adaptation) and re-sults reported in previous work.
Table 2:	Results on two-domain adapta-tion. F: Fine-tuning. V: Expanding vocab-ulary. H: Expanding RNN hidden states. M:Our proposed method of expanding memory.
Table 3: Dynamics of the progressive memory network for IDA with 5 domains. Upper-triangularvalues in gray are out-of-domain (zero-shot) performance.
Table 4: Comparing our approach with variants and previous work in the multi-domain setting. Inthis experiment, we use the memory-augmented RNN as the neural architecture. Italics representbest results in the IDA group. ↑, ]: p < 0.05 and 介，,：p < 0.01 (compared with F+V+M).
Table 5: Results on two-domain adaptation for dialogue response generation. F: Fine-tuning. V:Expanding vocabulary. H: Expanding RNN hidden states. M: Our proposed method of expand-ing memory. We also compare with EWC (Kirkpatrick et al., 2017) and progressive neural net-work (RUsU et al., 2016). ↑,，p < 0.05 and 介，养：p < 0.01 (compared with Line 8).
Table 6: Sample outputs of our IDA model S→T (F+M+V) from Table 5.
