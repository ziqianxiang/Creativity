Table 2: Ablation study on WMT100K data. For ST and noisy ST, we initialize the model withthe baseline and results are from one single iteration. Dropout is varied only in the PT step, whiledropout is always applied in FT step. Different decoding methods refer to the strategy used to createthe pseudo target. At test time we use beam search decoding for all models.
Table 3: Results on the toy sum dataset. For STand noisy ST, smoothness (J) and symmetric Q)results are from the pseudo-training step, whiletest errors (J) are from fine-tuning, all at the firstiteration.
Table 4: Results on two machine translation datasets. For WMT100K, we use the remaining 3.8MEnglish and German sentences from training data as unlabeled data for noisy ST and BT, respec-tively.
Table 5: Rouge scores on Gigaword datasets. For the 100K setting we use the remaining 3.7Mtraining data as unlabeled instances for noisy ST and BT. In the 3.8M setting we use 4M unlabeleddata for noisy ST. Stared entry (*) denotes that the system uses a much larger dataset for pretraining.
Table 6: Results on WMT100K data. All results are from one single iteration. “Parallel + real/faketarget” denotes the noise process applied on parallel data but using real/fake target in the “pseudo-training” step. “Mono + fake target” is the normal noisy self-training process described in previoussections.
Table 7: Ablation analysis on WMT100K dataset.
