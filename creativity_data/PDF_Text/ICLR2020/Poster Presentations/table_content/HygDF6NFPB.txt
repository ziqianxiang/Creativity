Table 1: Criteria for reproducibility considered in this work and their compliance among consideredmodels. (Y) indicates that the criterion is met, (N) indicates that the criterion is not satisfied, (A)indicates ambiguity (i.e. it is unclear whether the criteria is met or not), (-) indicates lack of infor-mation (i.e. no details are provided about the criteria). Note that GraphSAGE is excluded from thiscomparison, as it was not directly applied by authors to graph classification tasks.
Table 2: Pseudo-code for model assessment (left) and model selection (right). In Algorithm 1,“Select” refers to Algorithm 2, whereas “Train” and “Eval” represent training and inference phases,respectively. After each model selection, the best configuration bestk is used to evaluate the externaltest fold. Performances are averaged across R training runs, where R in our case is set to 3.
Table 3: Results on chemical datasets with mean accuracy and standard deviation are reported. Bestperformances are highlighted in bold.
Table 4: Results on social datasets with mean accuracy and standard deviation are reported. Bestperformances are highlighted in bold. OOR means Out of Resources, either time (> 72 hours for asingle training) or GPU memory.
Table 5:	Dataset Statistics. Note that, when node labels are not present, we either assigned the samefeature of 1 or the degree to all nodes in the dataset. Moreover, following the literature, we use the18 additional node attributes for ENZYMES.
Table 6:	The table displays the median number of selcted layers in relation to the addition of nodedegrees as input features on all social datasets. 1 indicates that an uninformative feature is used asnode label.
Table 7: Hyper-parameters used for model selection.
