Table 1: We use the F1 score on SQuAD-v1 as the accuracy metric. The baseline F1 score is thescore obtained by the pre-trained model (B ert-Large) provided on Bert’s public repository (as ofFebruary 1st, 2019). We use TPUv3s in our experiments. We use the same setting as the baseline: thefirst 9/10 of the total epochs used a sequence length of 128 and the last 1/10 of the total epochs useda sequence length of 512. All the experiments run the same number of epochs. Dev set means the testdata. It is worth noting that we can achieve better results by manually tuning the hyperparameters.
Table 2: Lamb achieves a higher performance (F1 score) than Lars for all the batch sizes. Thebaseline achieves a F1 score of 90.390. Thus, LARS stops scaling at the batch size of 16K.
Table 3: Top-1 validation accuracy of ImageNet/ResNet-50 training at the batch size of 16K (90epochs). The performance of momentum was reported by (Goyal et al., 2017). + means adding thelearning rate scheme of Goyal et al. (2017) to the optimizer: (1) 5-epoch warmup to stablize the initialstage; and (2) multiply the learning rate by 0.1 at 30th, 60th, and 80th epoch. The target accuracy isaround 0.763 (Goyal et al., 2017). All the adaptive solvers were comprehensively tuned. The tuninginformation was in the Appendix.
Table 4: Untuned Lamb for Bert training across different batch sizes (fixed #epochs). We usesquare root LR scaling and linear-epoch warmup. For example, batch size 32K needs to finish 15625iterations. It uses 0.2×15625 = 3125 iterations for learning rate warmup. BERT’s baseline achieved aF1 score of 90.395. We can achieve an even higher F1 score if we manually tune the hyperparameters.
Table 5: Untuned Lamb for ImageNet training with ResNet-50 for different batch sizes (90 epochs).
Table 6: CIFAR-10 training with DavidNet (batch size = 512). All of them run 24 epochs and finishthe training under one minute on one cloud TPU. We make sure all the solvers are carefully tuned.
Table 7: Test Accuracy by MNIST training with LeNet (30 epochs for Batch Size = 1024). Thetuning space of learning rate for all the optimizers is {0.0001, 0.001, 0.01, 0.1}. We use the samelearning rate warmup and decay schedule for all of them.
Table 8: AdamW stops scaling at the batch size of 16K. The target F1 score is 90.5. Lamb achievesa F1 score of 91.345. The table shows the tuning information of AdamW. In this table, we report thebest F1 score We observed from our experiments.
Table 9: The accuracy information of tuning default AdaGrad optimizer for ImageNet training withResNet-50 (batch size = 16384, 90 epochs, 7038 iterations).
Table 10: The accuracy information of tuning AdaGrad optimizer for ImageNet training with ResNet-50 (batch size = 16384, 90 epochs, 7038 iterations). We use the learning rate recipe of (Goyal et al.,2017): (1) 5-epoch warmup to stablize the initial stage; and (2) multiply the learning rate by 0.1 at30th, 60th, and 80th epoch. The target accuracy is around 0.763 (Goyal et al., 2017).
Table 11: The accuracy information of tuning default Adam optimizer for ImageNet training withResNet-50 (batch size = 16384, 90 epochs, 7038 iterations). The target accuracy is around 0.763(Goyal et al., 2017).	____________________________________________________Learning Rate	Top-1 Validation Accuracy0.0001	0.55210.0002	0.60890.0004	0.64320.0006	0.64650.0008	0.64790.001	0.66040.002	0.64080.004	0.56870.006	0.51650.008	0.48120.010	0.3673Table 12: The accuracy information of tuning Adam optimizer for ImageNet training with ResNet-50(batch size = 16384, 90 epochs, 7038 iterations). We use the learning rate recipe of (Goyal et al.,2017): (1) 5-epoch warmup to stablize the initial stage; and (2) multiply the learning rate by 0.1 at30th, 60th, and 80th epoch. The target accuracy is around 0.763 (Goyal et al., 2017).
Table 12: The accuracy information of tuning Adam optimizer for ImageNet training with ResNet-50(batch size = 16384, 90 epochs, 7038 iterations). We use the learning rate recipe of (Goyal et al.,2017): (1) 5-epoch warmup to stablize the initial stage; and (2) multiply the learning rate by 0.1 at30th, 60th, and 80th epoch. The target accuracy is around 0.763 (Goyal et al., 2017).
Table 13: The accuracy information of tuning default AdamW optimizer for ImageNet training withResNet-50 (batch size = 16384, 90 epochs, 7038 iterations). The target accuracy is around 0.763(Goyal et al., 2017)_________________________________________________________________________________learning rate	weight decay	L2 regularization	Top-1 Validation Accuracy0.0001	0.00001	default (0.01)	0.533121760.0002	0.00001	default (0.01)	0.55428060.0004	0.00001	default (0.01)	0.487691250.0006	0.00001	default (0.01)	0.463175450.0008	0.00001	default (0.01)	0.409037260.001	0.00001	default (0.01)	0.424011230.002	0.00001	default (0.01)	0.338704440.004	0.00001	default (0.01)	0.123392740.006	0.00001	default (0.01)	0.1229248050.008	0.00001	default (0.01)	0.080993650.010	0.00001	default (0.01)	0.0167643220.012	0.00001	default (0.01)	0.0327148440.014	0.00001	default (0.01)	0.0181477870.016	0.00001	default (0.01)	0.00667317720.018	0.00001	default (0.01)	0.0102945970.020	0.00001	default (0.01)	0.008260091
Table 14: The accuracy information of tuning default AdamW optimizer for ImageNet training withResNet-50 (batch size = 16384, 90 epochs, 7038 iterations). The target accuracy is around 0.763(Goyal et al., 2017)_________________________________________________________________________________learning rate	weight decay	L2 regularization	Top-1 Validation Accuracy0.0001	0.0001	default (0.01)	0.554890930.0002	0.0001	default (0.01)	0.565144840.0004	0.0001	default (0.01)	0.49869790.0006	0.0001	default (0.01)	0.475952150.0008	0.0001	default (0.01)	0.446858730.001	0.0001	default (0.01)	0.410298680.002	0.0001	default (0.01)	0.28080240.004	0.0001	default (0.01)	0.081115720.006	0.0001	default (0.01)	0.0681152340.008	0.0001	default (0.01)	0.0579223630.010	0.0001	default (0.01)	0.052225750.012	0.0001	default (0.01)	0.0173136390.014	0.0001	default (0.01)	0.0297851560.016	0.0001	default (0.01)	0.0165405270.018	0.0001	default (0.01)	0.005757650.020	0.0001	default (0.01)	0.0102335615
Table 15: The accuracy information of tuning default AdamW optimizer for ImageNet training withResNet-50 (batch size = 16384, 90 epochs, 7038 iterations). The target accuracy is around 0.763(Goyal et al., 2017)_________________________________________________________________________________learning rate	weight decay	L2 regularization	Top-1 Validation Accuracy0.0001	0.001	default (0.01)	0.211425780.0002	0.001	default (0.01)	0.42891440.0004	0.001	default (0.01)	0.135375980.0006	0.001	default (0.01)	0.338033050.0008	0.001	default (0.01)	0.326110840.001	0.001	default (0.01)	0.221944170.002	0.001	default (0.01)	0.18339030.004	0.001	default (0.01)	0.082560220.006	0.001	default (0.01)	0.0205078120.008	0.001	default (0.01)	0.0182698570.010	0.001	default (0.01)	0.0075073240.012	0.001	default (0.01)	0.0200805660.014	0.001	default (0.01)	0.0107625320.016	0.001	default (0.01)	0.00213623050.018	0.001	default (0.01)	0.0079549150.020	0.001	default (0.01)	0.005859375
Table 16: The accuracy information of tuning default AdamW optimizer for ImageNet training withResNet-50 (batch size = 16384, 90 epochs, 7038 iterations). The target accuracy is around 0.763(Goyal et al., 2017)_________________________________________________________________________________learning rate	weight decay	L2 regularization	Top-1 Validation Accuracy0.0001	001	default (0.01)	0.00097656250.0002	001	-default (0.01)	0.00099690760.0004	001	default (0.01)	0.00101725260.0006	001	default (0.01)	0.00093587240.0008	001	default (0.01)	0.00223795580.001	001	default (0.01)	0.0015665690.002	001	default (0.01)	0.0094807940.004	001	default (0.01)	0.00335693360.006	001	default (0.01)	0.00299072270.008	001	default (0.01)	0.00185139980.010	001	default (0.01)	0.0091349290.012	001	default (0.01)	0.00221761060.014	001	default (0.01)	0.00406901030.016	001	default (0.01)	0.00172932950.018	001	default (0.01)	0.000610351560.020	001	default (0.01)	0.0022379558
Table 17: The accuracy information of tuning default AdamW optimizer for ImageNet training withResNet-50 (batch size = 16384, 90 epochs, 7038 iterations). The target accuracy is around 0.763(Goyal et al., 2017)_________________________________________________________________________________learning rate	weight decay	L2 regularization	Top-1 Validation Accuracy0.0001	0.00001	disable	0.489176420.0002	0.00001	disable	0.581522640.0004	0.00001	disable	0.634602840.0006	0.00001	disable	0.648498540.0008	0.00001	disable	0.65989180.001	0.00001	disable	0.66628010.002	0.00001	disable	0.672668460.004	0.00001	disable	0.66927080.006	0.00001	disable	0.65730790.008	0.00001	disable	0.66394040.010	0.00001	disable	0.652303040.012	0.00001	disable	0.65055340.014	0.00001	disable	0.649902340.016	0.00001	disable	0.653238950.018	0.00001	disable	0.670267760.020	0.00001	disable	0.66086835
Table 18: The accuracy information of tuning default AdamW optimizer for ImageNet training withResNet-50 (batch size = 16384, 90 epochs, 7038 iterations). The target accuracy is around 0.763(Goyal et al., 2017)_________________________________________________________________________________learning rate	weight decay	L2 regularization	Top-1 Validation Accuracy0.0001	0.0001	disable	0.50333660.0002	0.0001	disable	0.59497070.0004	0.0001	disable	0.625610350.0006	0.0001	disable	0.65452070.0008	0.0001	disable	0.663269040.001	0.0001	disable	0.66770430.002	0.0001	disable	0.672444640.004	0.0001	disable	0.67028810.006	0.0001	disable	0.660339360.008	0.0001	disable	0.664265930.010	0.0001	disable	0.661519350.012	0.0001	disable	0.65458170.014	0.0001	disable	0.655090330.016	0.0001	disable	0.65293380.018	0.0001	disable	0.656514470.020	0.0001	disable	0.65334064
Table 19: The accuracy information of tuning default AdamW optimizer for ImageNet training withResNet-50 (batch size = 16384, 90 epochs, 7038 iterations). The target accuracy is around 0.763(Goyal et al., 2017)_________________________________________________________________________________learning rate	weight decay	L2 regularization	Top-1 Validation Accuracy0.0001	0.001	disable	0.46112060.0002	0.001	disable	0.00762939450.0004	0.001	disable	0.292338040.0006	0.001	disable	0.572957340.0008	0.001	disable	0.55747480.001	0.001	disable	0.59885660.002	0.001	disable	0.5862630.004	0.001	disable	0.620768250.006	0.001	disable	0.615030940.008	0.001	disable	0.46978760.010	0.001	disable	0.6197510.012	0.001	disable	0.542439760.014	0.001	disable	0.54290770.016	0.001	disable	0.552815740.018	0.001	disable	0.58192950.020	0.001	disable	0.5938924
Table 20: The accuracy information of tuning default AdamW optimizer for ImageNet training withResNet-50 (batch size = 16384, 90 epochs, 7038 iterations). The target accuracy is around 0.763(Goyal et al., 2017)_________________________________________________________________________________learning rate	weight decay	L2 regularization	Top-1 Validation Accuracy0.0001	001	disable	0.00099690760.0002	001	disable	0.00089518230.0004	001	disable	0.000956217470.0006	001	disable	0.00128173830.0008	001	disable	0.0168863930.001	001	disable	0.0381469730.002	001	disable	0.00152587890.004	001	disable	0.00142415370.006	001	disable	0.0814412460.008	001	disable	0.0281168610.010	001	disable	0.0118204760.012	001	disable	0.081380210.014	001	disable	0.0101114910.016	001	disable	0.00419108060.018	001	disable	0.00382486990.020	001	disable	0.002746582
Table 21: The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet-50 (batch size = 16384, 90 epochs, 7038 iterations). We use the learning rate recipe of (Goyal et al.,2017): (1) 5-epoch warmup to stablize the initial stage; and (2) multiply the learning rate by 0.1 at30th, 60th, and 80th epoch. The target accuracy is around 0.763 (Goyal et al., 2017).
Table 22: The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet-50 (batch size = 16384, 90 epochs, 7038 iterations). We use the learning rate recipe of (Goyal et al.,2017): (1) 5-epoch warmup to stablize the initial stage; and (2) multiply the learning rate by 0.1 at30th, 60th, and 80th epoch. The target accuracy is around 0.763 (Goyal et al., 2017).
Table 23: The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet-50 (batch size = 16384, 90 epochs, 7038 iterations). We use the learning rate recipe of (Goyal et al.,2017): (1) 5-epoch warmup to stablize the initial stage; and (2) multiply the learning rate by 0.1 at30th, 60th, and 80th epoch. The target accuracy is around 0.763 (Goyal et al., 2017).
Table 24: The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet-50 (batch size = 16384, 90 epochs, 7038 iterations). We use the learning rate recipe of (Goyal et al.,2017): (1) 5-epoch warmup to stablize the initial stage; and (2) multiply the learning rate by 0.1 at30th, 60th, and 80th epoch. The target accuracy is around 0.763 (Goyal et al., 2017).
Table 25: The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet-50 (batch size = 16384, 90 epochs, 7038 iterations). We use the learning rate recipe of (Goyal et al.,2017): (1) 5-epoch warmup to stablize the initial stage; and (2) multiply the learning rate by 0.1 at30th, 60th, and 80th epoch. The target accuracy is around 0.763 (Goyal et al., 2017).
