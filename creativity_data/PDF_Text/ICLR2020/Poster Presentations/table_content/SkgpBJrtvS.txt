Table 1: Test accuracy (%) of student networks on CIFAR100 of a number of distillation methods (oursis CRD); see Appendix for citations of other methods. ↑ denotes outperformance over KD and ] denotesunderperformance. We note that CRD is the only method to always outperform KD (and also outperforms allother methods). We denote by * methods where we used our reimplementation based on the paper; for all othermethods we used author-provided or author-verified code. Average over 5 runs.
Table 2: Top-1 test accuracy (%) of student networks on CIFAR100 of a number of distillation methods (oursis CRD) for transfer across very different teacher and student architectures. CRD outperforms KD and all othermethods. Importantly, some methods that require very similar student and teacher architectures perform quitepoorly. E.g. FSP (Yim et al., 2017) cannot even be applied; AT (Ba & Caruana, 2014) and FitNet (Zagoruyko &Komodakis, 2016a) perform very poorly etc. We denote by * methods where we used our reimplementationbased on the paper; for all other methods we used author-provided or author-verified code. Average over 3 runs.
Table 3: Top-1 and Top-5 error rates (%) of student network ResNet-18 on ImageNet validation set. Weuse ResNet-34 released by PyTorch team as our teacher network, and follow the standard training practice ofImageNet on PyTorch except that we train for 10 more epochs. We compare our CRD with KD (Hinton et al.,2015), AT (Zagoruyko & Komodakis, 2016a) and Online-KD (Lan et al., 2018). “*” reported by the originalpaper Lan et al. (2018) using an ensemble of online ResNets as teacher, no pretrained ResNet-34 was used.
Table 5: Performance on the task of using depth to predict semantic segmentation labels. We initialize thedepth network either randomly or by distilling from a ImageNet pre-pretrained ResNet-18 teacher.
Table 6: Ablative study of different contrastive objectives and negative sampling policies on CIFAR100. Forcontrastive objectives, we compare our objective with InfoNCE (Oord et al., 2018); For negative sampling policy,when given an anchor image xi from the dataset, we consider either randomly sample negative xj such that (a)i 6= j , or (b) yi 6= yj where y represents the class label. Average over 5 runs.
Table 7: Test accuracy (%) of student networks on CIFAR100 of combining distillation methods with KD;We check the compatibility of our objective with KD as well as PKT. ↑ denotes outperformance over KD and ]denotes underperformance.
Table 8: We measure the transferability of the student network, by evaluating a linear classifier on top of itsfrozen representations on STL10 (abbreviated as “STL”) and TinyImageNet (abbreviated as “TI”). The bestaccuracy is bolded and the second best is underlined.
Table 9: Test accuracy (%) of student and teacher networks on CIFAR100 with the Deep Mutual Train-ing Zhang et al. (2018b) setting, where the teacher and student networks are trained simultaneously rather thansequentially. We use “T” and “S” to denote the teacher and student models, respectively.
Table 10: Test accuracy (%) of student networks on CIFAR100 of a number of distillation methods (ours isCRD). Standard deviation is provided.
Table 11: Top-1 test accuracy (%) of student networks on CIFAR100 of a number of distillation methods(ours is CRD) for transfer across very different teacher and student architectures. Standard deviation is provided.
