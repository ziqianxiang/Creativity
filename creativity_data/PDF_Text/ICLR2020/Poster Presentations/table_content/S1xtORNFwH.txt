Table 1: Performance of FSNet on the CIFAR-10 dataset	""ModP-P^P			Before Compression			FSNet			CR	FSNet-WQ		CR		# Param 1.74M	Accuracy	# Param	Accuracy		# Param	Accuracy	ResNet	ResNet-110	1.73M	93.91%	0.44M	93.81%	3.95	0.12M	93.81%	14.50	ReSNet-164	―		94.39%	0.47M	94.59%	3.68	0.16M	94.65%	10.81DenseNet	DenseNet-100 (k = 12)	1.25M 4.83M	95.31%	0.37M	94.46%	3.38	0.15M	94.40%	8.33	DenSeNet-100 (k = 24广		95.71%	1.33M	95.40%	3.63	0.44M	95.43%	10.98comparable with that of different baselines including ResNet-110 and ResNet-164, DenseNet-100with growth rate k = 12 and k = 24. We use the idea of cyclical learning rates (Smith, 2015) fortraining FSNet. 4 cycles are used for training FSNet, and each cycle uses the same schedule oflearning rate and same number of epochs as that of the baseline. A new cycle starts with the initial6Published as a conference paper at ICLR 2020learning rate of 0.1 after the previous cycle ends. The cyclical learning method is only used forFSNet on the CIFAR-10 dataset, and the training procedure of FSNet is exactly the same as that of itsbaseline throughout all the other experiments.
Table 2: Comparison between FSNet and the filter pruning method in (Li et al., 2017) on theCIFAR-10 dataset	__________________________________f	JPerformance Model ^^^^^-^^	# Params	AccuracyResNet-110	-1.74M	93.91%Filter Pruning (Li et al., 2017)	-1.16M	93.30%FSNet	0.44M	93.81%FSNet-WQ	0.12M	93.81%65554 5_l 4S)35U 3m 25I-2151一%- -OJ,lmsu-u"-1-
Table 3: Performance of FSNet on ImageNet^^^PerformanCe Model ^^^^^^^	# Params	Top-1	Top-5ReSNet-50		 25.61M	75.11%	92.61%FSNet-I	-13.9M	73.11%	91.37%FSNet-I-WQ	3.55M	72.59%	91.20%FSNet-2	8.49M	70.36%	89.79%FSNet-2-WQ	.	2.20M	69.87%	89.61%Table 4: Comparative results of FSNet on ImageNet			 Model ^^^^^_	# ParamS	Top-1	GFLOPsThiNet (Luo et al.,2017)	12.38M	71.01%	-341-FSNet-I-WQ	-	3.55M	72.59%	2.47parameter space while its mAP is much better. Note that while the reported number of parameters ofTiny SSD is 1.13M, its number of effective parameters is only half of this number. i.e. 0.565M, asthe parameters are stored in half precision floating-point. In addition, the model size of FSNet-1-WQis 1.85MB, around 20% smaller than that of Tiny SSD, 2.3MB. It is also interesting to observe thatour FSNet-2 and FSNet-2-WQ are both smaller than MobileNetV2 SSD-Lite (Sandler et al., 2018)with better MAP. Since MobileNetV2 SSD-Lite is believed to be newer than MobileNetV1 SSD, thelatter is not reported in this experiment.
Table 4: Comparative results of FSNet on ImageNet			 Model ^^^^^_	# ParamS	Top-1	GFLOPsThiNet (Luo et al.,2017)	12.38M	71.01%	-341-FSNet-I-WQ	-	3.55M	72.59%	2.47parameter space while its mAP is much better. Note that while the reported number of parameters ofTiny SSD is 1.13M, its number of effective parameters is only half of this number. i.e. 0.565M, asthe parameters are stored in half precision floating-point. In addition, the model size of FSNet-1-WQis 1.85MB, around 20% smaller than that of Tiny SSD, 2.3MB. It is also interesting to observe thatour FSNet-2 and FSNet-2-WQ are both smaller than MobileNetV2 SSD-Lite (Sandler et al., 2018)with better MAP. Since MobileNetV2 SSD-Lite is believed to be newer than MobileNetV1 SSD, thelatter is not reported in this experiment.
Table 5: Performance of FSNet for object detectionModer''PP	# Params	-APSSD300	26.32M	77.31%TinySSD(Wong et al.,2018)	0.56M	61.3%MobileNetV2 SSD-Lite (Sandler et al., 2018)	3.46M	68.60%FSNet-I	-1.67M	67.60%FSNet-I-WQ	0.45M	67.63%FSNet-2	2.59M	70.14%FSNet-2-WQ	0.68M	70.00%3.3	Using DFSNet for Neural Architecture SearchWe also study the performance of integrating FS into Neural Architecture Search (NAS). The goal ofNAS is to automatically search for relatively optimal network architecture for the sake of obtainingbetter performance than that of manually designed neural architecture. We adopt DifferentiableArchitecture Search (DARTS) (Liu et al., 2019) as our NAS method due to its effective and efficientsearching scheme, where the choice for different architectures is encoded as learnable parameterswhich can be trained in an end-to-end manner. Since DFSNet is also a differentiable frameworkfor model compression, we combine DFSNet and DARTS so as to search for a compact neuralarchitecture aiming at great performance. We design a DFSNet-DARTS model by replacing all the1 × 1 convolution layers, including those for the depthwise separable convolution, in the DARTSsearch space with FS convolution layers. We perform NAS on the CIFAR-10 dataset using the
Table 6: PerformanCe of DFSNet-DARTS on the CIFAR-10 dataset^ModeT'-P^r	# Params	AccuracyDARTS	3.13M	97.50%DFSNet-DARTS"	1.88M	97.19%4 ConclusionWe present a novel method for Compression of CNNs through learning weight sharing by Filter Sum-mary (FS). EaCh Convolution layer of the proposed FSNet learns a FS wherein the Convolution filtersare overlapping 1D segments, and nearby filters share weights naturally in their overlapping regions.
Table 7: Performance of FSNet on the CIFAR-10 dataset, using larger ResNet and DenseNet as baselines	"""ModP-P^P			Before Compression			FSNet			CR	FSNet-WQ		CR		# Param	Accuracy	# Param	Accuracy		# Param	Accuracy	ResNet	ResNet-18	11.18M	94.18%	0.81M	93.93%	13.80	0.22M	93.91%	50.82	ResNet-34	21.30M	94.72%	1.68M	94.29%	12.68	0.45M	94.32%	47.33	ResNet-50	23.57M	95.16%	2.51M	94.91%	9.39	0.72M	94.92%	32.73	ResNet-101	42.61M	95.62%	4.84M	95.23%	8.80	1.38M	95.23%	30.88DenseNet	DenseNet-121	7.04M	95.13%	1.24M	95.11%	5.68	0.44M	95.13%	16Because the results of FSNet in Table 1 are obtained by cyclic training with four cycles, we show the performanceof FSNet using the same training procedure (with only one training cycle) as its baseline in Table 8. We can seethat the accuracy loss of FSNet is still less than 1% compared to its baseline, and FSNet with one cycle has evenhigher accuracy (95.46%) than that with four cycles (95.40%) for DenseNet-100 with a growth rate of 24.
Table 8: Performance of FSNet with one-cycle (the same training procedure as the baseline) on the CIFAR-10dataset	"^"ModP-P^P			Before Compression			FSNet			CR	FSNet-WQ		CR		# Param 1.74M	Accuracy	# Param	Accuracy		# Param	Accuracy	ResNet	ResNet-110	1.73M	93.91%	0.44M	93.02%	3.95	0.12M	93.03%	14.50	ReSNet-164	―		94.39%	0.47M	93.75%	3.68	0.16M	93.84%	10.81DenseNet	DenseNet-100 (k = 12)	1.25M 4.83M	95.31%	0.37M	94.36%	3.38	0.15M	94.31%	8.33	DenSeNet-100 (k = 24广		95.71%	1.33M	95.46%	3.63	0.44M	95.47%	10.98Table 9: Comparison between FSNet and the Discrimination-aware Channel Pruning (DCP) method(Zhuang et al., 2018) on the CIFAR-10 dataset, where the X J shows the compression ratio (the ratioof the number of parameters of the original model to that of the compressed model).
Table 9: Comparison between FSNet and the Discrimination-aware Channel Pruning (DCP) method(Zhuang et al., 2018) on the CIFAR-10 dataset, where the X J shows the compression ratio (the ratioof the number of parameters of the original model to that of the compressed model).
Table 10: Performance of FSNet-WQ and Linear Quantization on CIFAR-10^PdP-PP	# Params	AccuracyResNet-164	-1.73M	94.39%FSNet-WQ	0.16M	94.64%Linear Quantization	0.50M	94.36%Table 11: Performance of FSNet-WQ and Linear Quantization on ImageNetModePPP	# Params	Top-1	Top-5ResNet-50	25.61M	75.11%	92.61%FSNet-WQ	-	3.55M	72.59%	91.20%Linear Quantization (8-bit)	8.56M	74.88%	92.48%Linear Quantization (4-bit)	5.71M	0.086%	0.578%We are also interested in the visualization of the learned Filter Summary. For the visualization purpose, wechange the representation of a Filter Summary from a 1D vector to a 3D tensor. We design a 3D Filter Summary(FS) for the first convolution layer (conv1) of Network In Network (NIN) (Lin et al., 2014) on the CIFAR-10dataset. The conv1 of the original NIN has 192 filters of spatial size 5 × 5 and channel size 3. The 3D FilterSummary for conv1 of the FSNet version of NIN is a 3D tensor of size 24 × 8 × 3, with a spatial size of 24 × 8and a channel size of 3. In the forward process, 192 filters of size 5 × 5 × 3 are extracted as overlapping 3Dtensors from the 3D FS. The illustration of the learned 3D FS for conv1 of the FSNet version of NIN andthe independent 192 filters learned in conv1 of the original NIN are shown in Figure 5. We can observe that,compared to the independent 192 filters, the filters in the 3D FS are “smoothed” by the weight sharing scheme
Table 11: Performance of FSNet-WQ and Linear Quantization on ImageNetModePPP	# Params	Top-1	Top-5ResNet-50	25.61M	75.11%	92.61%FSNet-WQ	-	3.55M	72.59%	91.20%Linear Quantization (8-bit)	8.56M	74.88%	92.48%Linear Quantization (4-bit)	5.71M	0.086%	0.578%We are also interested in the visualization of the learned Filter Summary. For the visualization purpose, wechange the representation of a Filter Summary from a 1D vector to a 3D tensor. We design a 3D Filter Summary(FS) for the first convolution layer (conv1) of Network In Network (NIN) (Lin et al., 2014) on the CIFAR-10dataset. The conv1 of the original NIN has 192 filters of spatial size 5 × 5 and channel size 3. The 3D FilterSummary for conv1 of the FSNet version of NIN is a 3D tensor of size 24 × 8 × 3, with a spatial size of 24 × 8and a channel size of 3. In the forward process, 192 filters of size 5 × 5 × 3 are extracted as overlapping 3Dtensors from the 3D FS. The illustration of the learned 3D FS for conv1 of the FSNet version of NIN andthe independent 192 filters learned in conv1 of the original NIN are shown in Figure 5. We can observe that,compared to the independent 192 filters, the filters in the 3D FS are “smoothed” by the weight sharing schemebecause patterns of one filter are shared across its neighboring filters in the 3D FS.
