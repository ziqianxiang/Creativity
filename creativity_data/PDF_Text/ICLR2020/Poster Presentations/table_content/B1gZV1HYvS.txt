Table 1: Average reward gaps between demonstrators and learned agents in 2 cooperative tasks. Means andstandard deviations are taken across different random seeds.
Table 2: Average reward gaps between demonstrators and learned agents in 2 competitive tasks, where ‘agent+’and ‘agent-’ represent 2 teams of agents and ‘total’ is their sum. Means and standard deviations are taken acrossdifferent random seeds.
Table 3: KL divergence of learned agents position distribution and demonstrators position distribution from anindividual perspective in different scenarios. ‘Total’ is the KL divergence for state-action pairs of all agents,and ‘Per’ is the averaged KL divergence of each agent. Experiments are conducted under the same randomseed. Note that unmovable agents are not recorded since they never move from the start point, and there is onlyone movable agent in Cooperative-communication.
Table 4: Raw average total rewards in 2 comparative tasks. Means and standard deviations are taken acrossdifferent random seeds.
Table 5: Raw average rewards of each agent in 2 competitive tasks, where agent+ and agent- represent 2 teamsof agents and total is their sum. Means and standard deviations are taken across different random seeds.
Table 6: Results of different training frequency (1:4, 1:2, 1:1, 2:1, 4:1) of D and G on Communication-navigation. Means and standard deviations are taken across different random seeds.
