Table 1: Comparison of NAS algorithms withrandom sampling. We report results on PTB us-ing mean validation perplexity (the loWer, the bet-ter) and on CIFAR-10 using mean top 1 accuracy.
Table 2: Top 1 accuracy in the 7-node DARTSSearch space. We report the mean and best top-1accuracy on the test sets of architectures found byDARTS, NAO, ENAS, BayesNAS, and our randompolicy. As sanity check, we also train from scratchthe architectures reported in original papers, aswell as their reported performance.
Table 3: Results in reduced search spaces. For RNNs (A), We report the mean and best perplexityon the validation and test sets at the end of training the architectures found using DARTS, NAO,ENAS. For CNNs (B), we show the mean and best top-1 accuracy on the test set. Instead of runningrandom sampling in the reduced space, we compute the probability of the best model found by eachmethod to surpass the random one (details in Appendix A.2). The mean and best statistics of theentire search space are reported as Space.
Table 4: Search results w/o weight sharing. Wereport results from ENAS ans NAO on NASBenchwith 7 nodes over 10 runs.
Table 5: Ranking disorder of weightsharing in CNN.
Table 6: Comparison of state-of-the-art methods on NASBench-101 search space.
