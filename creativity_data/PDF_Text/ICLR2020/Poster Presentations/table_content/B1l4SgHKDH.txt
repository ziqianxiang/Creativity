Table 1: Validation and test perplexity on CC-News and Toronto Book Corpus. * denotes models initializedwith RoBERTa trained on additional data. The joint model perplexity ranges are estimated using 100,000samples, see Eq. 4. The number of parameters of each model is shown in parentheses.
Table 2: Human evaluation results on a subset of 333 sentences on the CC-News test set. The rate is computedas the percentage of sentences where the number of turkers preferring Model1 is strictly less than (denoted with<) or not greater than (denoted with â‰¤) those preferring Model2. Attention check is used to drop some votes,so there might exist ties. p-value is based on single-sided binomial test.
Table 3: Comparison of P (xt |x<t) between BASE LM and BIT-BASE on a few examples. Repetitions aremarked with red. Only the top 5 probabilities are shown.
Table 4: Optimization settings. We use the same setting for CC-News and Toronto Book Corpus.
Table 5: Example generations when BIT-BASE outperforms BALM according to human evaluation. BiT-Base-Worst shows the LM sample with the highest energy score.
Table 6: Example generations when BIT-BASE underperforms BALM according to human evaluation. BiT-Base-Worst shows the LM sample with the highest energy score.
