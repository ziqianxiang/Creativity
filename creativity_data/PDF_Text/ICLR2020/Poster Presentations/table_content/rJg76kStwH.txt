Table 1: Inference accuracy (AUC-PR) of different methods on three benchmark datasets.
Table 2: AUC-PR for different combi-nations of GNN and tunable embeddings.
Table 3: Performance on FB15K-237 with varied training set size.	Table 4: ae :	74rc-oh Ct				M , 1	MRR	Hits@10 Model		 		performance on FB15K-237.		0% 5% 10% 20% 100% 0% 5% 10% 20% 100%	Model	MRR	Hits@10			MLN	-	-	-	-	0.10	-	-	-	-	16.0			NTN	0.09 0.10 0.10 0.11 0.13 17.9 19.3 19.1 19.6 23.9	NTN	0.001	0.0Neural LP	0.01 0.13 0.15 0.16 0.24 1.5 23.2 24.7 26.4 36.2	Neural LP	0.010	2.7DistMult	0.23 0.24 0.24 0.24 0.31 40.0 40.4 40.7 41.4 48.5	DistMult	0.004	0.8ComplEx	0.24 0.24 0.24 0.25 0.32 41.1 41.3 41.9 42.5 51.1	ComplEx	0.013	2.2TransE	0.24 0.25 0.25 0.25 0.33 42.7 43.1 43.4 43.9 52.7	TransE	0.003	0.5RotatE	0.25 0.25 0.25 0.26 0.34 42.6 43.0 43.5 44.1 53.1	RotatE	0.006	1.5pLogicNet	-	-	-	-	0.33	-	-	-	-	52.8	ExpressGNN-E 0.181		29.3ExpressGNN-E^^0.42 0.42 0.42 0.44 0.45 53.1 53.1 53.3 55.2 57.3	ExpressGNN-EM 0.185		29.6ExpressGNN-EM 0.42 0.42 0.43 0.45 0.49 53.8 54.6 55.3 55.6 60.8			Performance analysis. The experimental results on the full training data are reported in Table 3(100% columns). Both ExpressGNN-E and ExpressGNN-EM significantly outperform all the baselinemethods. With learning the weights of logic rules, ExpressGNN-EM achieves the best performance.
Table 5: Complete statistics of the benchmark datasets.
Table 6: Inference performance of competitors and our method under the closed-world semantics.
