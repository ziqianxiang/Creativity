Table 1: Variants of learning rate regularization and importance measurement on 2-Split MNISTMethod	μ	ρ	Importance Ω	BWT (%)	ACC (%)UCB	x	-	1σ	0.00	99.2UCB	-	x	1σ	-0.04	98.7UCB	x	x	1σ	-0.02	98.0UCB	x	-	打加	-0.03	98.4UCB	-	x	打加	-0.52	98.7UCB	x	x	打加	-0.32	98.8UCB-P	x	x	∖μ∖Γσ	-0.01	99.0UCB-P	x	x	17σ	-0.01	98.9Table 2: Continually learning on different datasets. BWT and ACC in %. (*) denotes that methods do notadhere to the continual learning setup: BBB-JT and ORD-JT serve as the upper bound for ACC for BBB/ORDnetworks, respectively. ∣ denotes results reported by (Serra et al., 2018). f denotes the result reported fromoriginal work. BWT was not reported in ∣ and f. All others results are (re)produced by us and are averaged over3 runs with standard deviations given in Section A.3 of the appendix.
Table 2: Continually learning on different datasets. BWT and ACC in %. (*) denotes that methods do notadhere to the continual learning setup: BBB-JT and ORD-JT serve as the upper bound for ACC for BBB/ORDnetworks, respectively. ∣ denotes results reported by (Serra et al., 2018). f denotes the result reported fromoriginal work. BWT was not reported in ∣ and f. All others results are (re)produced by us and are averaged over3 runs with standard deviations given in Section A.3 of the appendix.
Table 3: Single Head vs. Multi-Head architecture and Generalized vs. Standard Accuracy. Generalized accuracymeans that task information is not available at test time. SM, PM, CF, and 8T denote the 5-Split MNIST,Permuted MNIST, Alternating CIFAR10/100, and sequence of 8 tasks, respectively.
Table 4: Utilized datasets summaryNames	#Classes	Train	TestFaCeSCrub (Ng & Winkler, 2014)	100	20,600	2,289MNIST (LeCun et al., 1998)	10	60,000	10,000CIFAR100 (Krizhevsky & Hinton, 2009)	100	50,000	10,000NotMNIST (Bulatov, 2011)	10	16,853	1,873SVHN (Netzer et al., 2011)	10	73,257	26,032CIFAR10 (Krizhevsky & Hinton, 2009)	10	39,209	12,630TraffiCSigns (Stallkamp et al., 2011)	43	39,209	12,630FashionMNIST (Xiao et al., 2017)	10	60,000	10,000A.2 Implementation DetailsIn this section we take a closer look at elements of our UCB model on MNIST and evaluate variantsof parameter regularization, importance measurement, as well as the effect of the number of samplesdrawn from the posited posterior.
Table 5: Search space for hyperparamters in BBB given by Blundell et al. (2015)BBB hyperparamters	-log σι	-log σ	πSearCh spaCe	{0,1,2}	{6,7, 8}	{0.25,0.5,0.75}Network architecture: For Split MNIST and Permuted MNIST experiments, we have used a two-layer perceptron which has 1200 units. Because there is more number of parameters in our Bayesianneural network compared to its equivalent regular neural net, we ensured fair comparison by matchingthe total number of parameters between the two to be 1.9M unless otherwise is stated. For themultiple datasets learning scenario, as well as alternating incremental CIFAR10/100 datasets, we haveused a ResNet18 Bayesian neural network with 7.1-11.3M parameters depending on the experiment.
Table 6: Continually learning on CIFAR10/100 using AlexNet and ResNet18 for UCB (our method)and HAT (Serra et al., 2018). BWT and ACC in %. All results are (re)produced by us.
Table 7: Number of Monte Carlo samples (N) in 2-Split MNISTMethod	N	BWT (%)	ACC (%)UCB	1	0.00	98.0UCB	2	0.00	98.3UCB	5	-0.15	99.0UCB	10	0.00	99.2UCB	15	-0.01	98.3A.3 Additional resultsHere we include some additional results such as Table 8 for 2-split MNIST and some complementaryresults for tables in the main text as follows: 9, 10, and 11 include standard deviation for resultsshown in Table 2a, 2b, 2c, respectively.
Table 8: Continually learning on 2-Split MNIST. BWT and ACC in %. (*) denotes that methods donot adhere to the continual learning setup: BBB-JT and ORD-JT serve as the upper bound for ACCfor BBB/ORD networks, respectively. All results are (re)produced by us.
Table 9: Continually learning on 5-Split MNIST. BWT and ACC in %. (*) denotes that methods do not adhereto the continual learning setup: BBB-JT and ORD-JT serve as the upper bound for ACC for BBB/ORD networks,respectively. All results are (re)produced by us.
Table 10: Continually learning on Permuted MNIST. BWT and ACC in %. (*) denotes that methoddoes not adhere to the continual learning setup: BBB-JT serves as the upper bound for ACC for BBBnetwork. ∣ denotes results reported by (Serra et al., 2018). f denotes the result reported from originalwork. BWT was not reported in 去 and f. All others results are (re)produced by us.
Table 11: Continually learning on CIFAR10/100. BWT and ACC in %. (*) denotes that method doesnot adhere to the continual learning setup: BBB-JT serves as the upper bound for ACC for BBBnetwork. All results are (re)produced by us.
