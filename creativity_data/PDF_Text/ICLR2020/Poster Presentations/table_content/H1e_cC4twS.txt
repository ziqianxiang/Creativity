Table 1: A sample task-oriented dialogue with annotated dialogue states after each user turn. Thedialogue states in red and blue denote slots from the attraction domain and train domain respectively.
Table 2: DST Joint Accuracy metric on MUltiWOZ 2.1 and 2.0. *: results reported on MUltiWOZ2.0leaderboard. ? : results reported by Eric et al. (2019). Best results are highlighted in bold.
Table 3: DST joint accuracy and slot accuracy on			Table 4: Latency analysis on MultiWOZ2.1.			MultiWOZ2.0 restaurant domain. Baseline re-			Latency is reported in terms of wall-clock			sults (except TSCP) were from Wu et al. (2019).			time in ms per prediction state.			Ablation Analysis. We conduct an extensive ablation analysis with several variants of our mod-els in Table 5. Besides the results of DST metrics, Joint Slot Accuracy and Slot Accuracy, wereported the performance of the fertility decoder in Joint Gate Accuracy and Joint Fertility Accu-racy. These metrics are computed similarly as Joint Slot Accuracy in which the metrics are based onwhether all predictions of gates or fertilities match the corresponding ground truth labels. We alsoreported the Oracle Joint Slot Accuracy and Slot Accuracy when the models are fed with groundtruth Xds×fert and Xdel labels instead of the model predictions. We noted that the model fails whenpositional encoding of Xds×fert is removed before being passed to the state decoder. The perfor-mance drop can be explained because PE is responsible for injecting sequential attributes to enablenon-autoregressive decoding. Second, we also note a slight drop of performance when slot gating isremoved as the models have to learn to predict a fertility of 1 for “none” and “dontcare” slots as well.
Table 5: Ablation analysis on MultiWOZ 2.1 on 4 components: partially delexicalized dialoguehistory Xdel, slot gating, positional encoding P E(Xds×fert), and pointer network.
Table 6: Performance of auto-regressive model variants on MultiWOZ2.0 and 2.1. Fertility predic-tion is removed as fertility becomes redudant in auto-regressive models.
Table 7: Summary of MultiWOZ dataset 2.1A.2 Model Hyper-ParametersWe employed dropout (Srivastava et al., 2014) of 0.2 at all network layers except the linear layersof generation network components and pointer attention components. We used a batch size of 32,embedding dimension d = 256 in all experiments. We also fixed the number of attention heads to 16in all attention layers. We shared the embedding weights to embed domain and slot tokens as input tofertility decoder and state decoder. We also shared the embedding weights between dialogue historyencoder and state generator. We varied our models for different values of T = Tfert = Tstate ∈{1, 2, 3}. In all experiments, the warmup steps are fine-tuned from a range from 13K to 20K trainingsteps.
Table 8: Additional domain-specific results of our model in MultiWOZ2.0 and MultiWOZ2.1. Themodel performs best with the restaurant domain and worst with the taxi domain.
Table 9: Additional results of our model in MultiWOZ2.1 when we assume access to the ground-truth labels of Xdel and Xds×fert (oracle prediction). We vary the the percentage of using the modelprediction Xdel and Xds×fert from 100% (true prediction) to 0% (oracle prediction).
Table 10: Full set of predicted dialogue states for dialogue ID MUL0536 in MultiWOZ2.1.
Table 11: Full set of predicted dialogue states for dialogue ID PMUL3759 in MultiWOZ2.1.
