Table 1: Comparison between black-box verification methods. * Technically, Lee et al. (2019) can handlesmoothing measures such that the likelihood ratios take a finite set of values, but most practical instances of thiscorrespond to finite-support distributions.
Table 2: Proportion of the examples With verified '0-robustness for different accuracy (e) parameters.
Table 3: `0 robustness results for Librispeech (Panayotov et al., 2015). From the Librispeech dataset, Wecreated a corpus of sentence utterances from ten different speakers. The classification task is, given an audiosample, to predict Whom is speaking. The test set consisted of 30 audio samples for each of the ten speakers.
Table 4: Certificates for various f -divergences for the information-limited setting. Note that the Renyidivergences are not proper f-divergences, but are defined as Ra(V∣∣ρ) = α⅛ι log(1 + Df(Vkρ)).
Table 5: Bounds on f -divergences: e0 is the vector with 1 in the first coordinate and zeros in all othercoordinates and 1 is the vector with all coordinates equal to 1. μp refers to the smoothing measureinduced by the `p norm, U(S) refers to the uniform measure over the set S, O is the set of orthogonalmatrices and Bp = {kzkp ≤ 1} is the unit ball in the `p norm.
Table 6: `1 comparison with Lecuyer et al. (2019) on MNIST.
