Table 1: Top-1 test accuracy for decentralized DCD, ECD, DeepSqueeze and CHOCO-SGD with differentcompression schemes. Reported top-1 test accuracies are averaged over three runs with fine-tuned hyper-parameters (learning rate, weight decay, consensus stepsize). The fine-tuned all-reduce baseline reaches accuracy92.64, with 1.04 MB gradient transmission per iteration. (? indicates that 2 out of 3 runs diverged).
Table 3: Summary of performance when training with the same epoch budget (as centralized SGD).
Table 4: Tuned hyper-parameters of CHOCO-SGD for training ResNet-20 on Cifar10, corresponding tothe ring topology with 8 nodes in Table 1. We randomly split the training data between nodes and shuffle it afterevery epoch. The per node mini-batch size is 128 and the degree of each node is 3.
Table 5: Tuned hyper-parameters of CHOCO-SGD, corresponding to the social network topology with 32nodes in Table 3. We randomly split the training data between the nodes and keep this partition fixed during theentire training (no shuffling). The per node mini-batch size is 32 and the maximum degree of the node is 14.
Table 6: Tuned hyper-parameters of DCD, ECD, and DeepSqueeze for training ResNet-20 on Cifar10,corresponding to the ring topology with 8 nodes in Table 1. We randomly split the training data between nodesand shuffle it after every epoch. The per node mini-batch size is 128 and the degree of each node is 3. We onlyreport the hpyerparameters corresponding to results that can reach to reasonable performance in our experiments.
Table 7: The exact epoch for the same bits budget in Fig. 1.
Table 8: The exact transmitted bits (in MB) for the same epoch budget in Fig. 1.
