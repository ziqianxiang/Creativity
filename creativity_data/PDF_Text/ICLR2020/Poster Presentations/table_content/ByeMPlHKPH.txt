Table 1: Results on IWSLT’14 De-En. Our Lite Transformer outperforms the transformer (Vaswaniet al., 2017) and the Lightweight convolution network (Wu et al., 2019b) especially in mobile settings.
Table 2: Results on WMT’14 En-De and WMT’14 En-Fr. Our Lite Transformer improves the BLEUscore over the transformer under similar Mult-Adds constraints.
Table 3: Performance and training cost of an NMT model in terms of CO2 emissions (lbs) and cloudcompute cost (USD). The training cost estimation is adapted from Strubell et al. (2019). The trainingtime for transformer and our Lite Transformer is measured on NVIDIA V100 GPU. The cloudcomputing cost is priced by AWS (lower price: spot instance; higher price: on-demand instance).
Table 4: Results on CNN-DailyMail dataset for abstractive summarization. Our Lite Transformerachieves similar F1-Rouge (R-1, R-2 and R-L) to the transformer (Vaswani et al., 2017) with morethan 2.4× less computation and 2.5× less model size. “#MAdds (x)” indicates the #Mult-Addsrequired by the model with the input length of x.
Table 5: Results on WIKITEXT-103 dataset for language modeling. We apply our Lite Transformerarchitecture on transformer base model with adaptive inputs (Baevski & Auli, 2019) and achieve 1.8lower test perplexity under similar resource constraint.
