Table 1: Inception score (IS, larger is better) and Frechet inception distance (FID, smaller is better) ofStackGAN++ (Zhang et al., 2017b), HDGAN (Zhang et al., 2018b), AttGAN (Xu et al., 2018), Obj-GAN (Liet al., 2019), and the proposed VHE-raster-scan-GAN; the values labeled with * are calculated by the providedwell-trained models and the others are quoted from the original publications; see Tab. 5 in Appendix C.1 for theerror bars of IS. Note that while the FID of Obj-GAN is the lowest, it does not necessarily imply it produceshigh-quality images, as shown in Figs. 13 and 27; this is because FID only measures the similarity on the imagefeature space, but ignores the shapes of objects and diversity of generated images. More discussions can befound in Section 3.1 and Appendix G.
Table 2: Ablation study for image-to-text learning, where the structures of different variations of raster-scan-GAN are illustrated in Figs. 1(b), 1(d), and 1(e).
Table 3: Accuracy (%) of ZSL on CUB and Flower. Note that some of them are attribute-based methods butapplicable in our setting by replacing attribute vectors with text features (labeled by *), as discussed in (Elhoseinyet al., 2017b).
Table 4: Comparison of the image-to-text retrieval performance, measured by Top-1 accuracy, and text-to-imageretrieval performance, measured by AP@50, between different methods on CUB-E.
Table 5: Inception score (IS) results in Table 1 with error bars.
Table 6: Inception score (IS) results in Table 2 with error bars.
