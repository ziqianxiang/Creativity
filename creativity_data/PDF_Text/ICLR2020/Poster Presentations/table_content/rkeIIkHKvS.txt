Table 1: Neighborhood aggregation schemesModels	I Aggregation and combination functions for round k (1 ≤ k ≤ K)General GNN framework	hVk) = COMBINE(k) n<	hvk-1∖ AGGREGATE(k)({hv,k-1) : v0 ∈ N^})θJ	GCN	hk = A(Pv0∈Nv∪{v} √Nv∣+ι)∙(Nv0|+1) ∙ W(I) ∙…		GraphSAGE	hVk) = A(W(k-1) ∙	・册T) IIAGGREGATE({hVkT), V ∈ Nv})	GAT	hvi=A( Pvj ∈Nv, ∪{v",k-1) ∙W (I) ∙ hvkτ)		GNNs are inspired by the Weisfeiler-Lehman test (Weisfeiler & Lehman, 1968; Shervashidze et al.,2011), which is an effective method for graph isomorphism. Similarly, GNNs utilize a neighborhoodaggregation scheme to learn a representation vector hv for each node v, and then use neural networksto learn a mapping function f (∙). Formally, consider the general GNN framework (Hamilton et al.,2017; Zhou et al., 2018; Xu et al., 2019) in Table 1 with K rounds of neighbor aggregation. Ineach round, only the features of 1-hop neighbors are aggregated, and the framework consists of twofunctions, AGGREGATE and COMBINE. We initialize h(v0) = xv. After K rounds of aggregation,each node V ∈ V obtains its representation vector hVK). We use hVK) and a mapping function f (∙),e.g., a fully connected layer, to obtain the final results for a specific task such as node classification.
Table 2: Smoothness valuesSmoothness value	Dataset MetriCS				Citeseer	Cora	PubMed	Amazon	BGP (small)	BGP (full)Feature Smoothness λf (10-2)	2.76	4.26	0.91	89.67	7.46	5.90Label Smoothness λl	0.26	0.19	0.25	0.22	0.71	≈0.71F1-Micro scores. Table 3 reports the F1-Micro scores of the different methods for the task ofnode classification. The F1-Micro scores are further divided into three groups. For the topology-based methods, Label Propagation has relatively good performance for the citation networks and theco-purchasing Amazon network, which is explained as follows. Label Propagation is effective incommunity detection and these graphs contain many community structures, which can be inferredfrom their small λl values. This is because a small λl value means that many nodes have the sameclass label as their neighbors, while nodes that are connected together and in the same class tend toform a community. In contrast, for the BGP graph in which the role (class) of the nodes is mainlydecided by topology features, struc2vec and GraphWave give better performance. GraphWave ranout of memory (512 GB) on the larger graphs as itis a spectrum-based method. For the feature-basedmethods, Logistic Regression and MLP have comparable performance on all the graphs.
Table 3: Node classification resultsFI-MiCro(%)^^^DataSet ¾			Citeseer	Cora	PubMed	Amazon	BGP (small)	BGP (full)struC2veC	30.98	41.34	47.60	39.86	48.40	49.66GraphWave	28.12	31.66	OOM	37.33	50.26	OOMLabel Propagation	71.07	86.26	78.52	88.90	34.05	36.82LogistiC Regression	69.96	76.62	87.97	85.89	65:34	-62.41-MLP	70.51	73.40	87.94	86.46	67.08	67.00GCN	71.27	80.92	80.31	91.17	51:26	-54.46-GraphSAGE	69.47	83.61	87.57	90.78	65.29	64.67GAT	74.69	90.68	81.65	91.75	47.44	58.87CS-GNN (w/o LTF)	73.58	90.38	89.42	92.48	66.20	68.83CS-GNN	75.71	91.26	89.53	92.77	66.39	68.76We further examine the effects of local topology features (LTF) on the performance. We report theresults of CS-GNN without using LTF, denoted as CS-GNN (w/o LTF) in Table 3. The results showthat using LTF does not significantly improve the performance of CS-GNN. However, the resultsdo reveal the effectiveness of the smoothness metrics in CS-GNN, because the difference between7Published as a conference paper at ICLR 2020CS-GNN (w/o LTF) and GAT is mainly in the use of the smoothness metrics in CS-GNN’s attentionmechanism. As shown in Table 3, CS-GNN (w/o LTF) still achieves significant improvements over
Table 4: Improvements of GNNs over non-GNN methodsImprovement (%)	Dataset ⅜	二	Citeseer	Cora	PubMed	Amazon	BGP (small)	BGP (full)Existing GNNs vs. topology-based methods	65%	60%	32%	65%	23%	37%CS-GNN vs. topology-based methods	74%	72%	42%	68%	50%	59%Existing GNNs vs. feature-based methods	2%	13%	-5%	6%	-17%	-9%CS-GNN vs. feature-based methods	8%	22%	2%	8%	0%	6%4.3	Smoothness AnalysisThe results in Section 4.2 show that GNNs can achieve good performance by gaining surroundinginformation in graphs with large λf and small λl . However, the experiments were conducted ondifferent graphs and there could be other factors than just the smoothness values of the graphs.
Table 5: Notations and their descriptions	Notations	DescriptionsG	A graphV	The set of nodes in a graphv/vi	Node v/vi in VE	The set of edges in a grapheVi ,Vj	An edge that connects nodes Vi and VjX	The node feature spacexV	The feature vector of node VyV/yV	The ground-truth / predicted class label of node VNV	The set of neighbors of node VhV/h(Vk)	The representation vector of node V (in round k)f (•)	A mapping functionW	A parameter matrixAn	An activation function(k) ai,j	The coefficient of node Vj to node Vi (in round k)(k) cV	The context vector of node V (in round k)(k) cV	The ground-truth context vector of node V (in round k)-nk-	The noise on the context vector of node V (in round k)(k) sV	The surrounding vector of node V (in round k)
Table 6: Dataset statisticsDataset	Citeseer	Cora	PubMed	Amazon (computer)	BGP (small)	BGP (full)|V|	3,312	2,708	19,717	13,752	10,176	-63,977|E|	4,715	5,429	44,327	245,861	206,799	349,606Average degree	-142-	2.00	-225-	17:88	20.32	5.46~~feature dim.	3,703	1,433	-500-	767	287	287classes num.	6	-7-	3	10	7	7λf (10-2)	2.7593	4.2564	0.9078	89.6716	7.4620	-5.8970λl	0.2554	0.1900	0.2455	0.2228	0.7131	≈0.7131Table 6 presents some statistics of the datasets used in our experiments, including the number ofnodes |V|, the number of edges |E|, the average degree, the dimension of feature vectors, the number14Published as a conference paper at ICLR 2020of classes, the feature smoothness λf and the label smoothness λl . The BGP (small) dataset wasobtained from the original BGP dataset, i.e., BGP (full) in Table 6, by removing all unlabeled nodesand edges connected to them. Since BGP (full) contains many unlabeled nodes, we used BGP(small)’s λl as an estimation. As we can see in Table 6, the three citation networks are quite sparse(with small average degree), while the other two networks are denser. According to the featuresmoothness, Amazon (computer) has much larger λf than that of the others, which means that nodeswith dissimilar features tend to be connected. As for the label smoothness λl , the BGP network has
Table 7: The F1-Micro scores of GraphSAGE with different aggregatorsF1-Micro (%)	Dataset Aggregators'	Citeseer	Cora	PUbMed	Amazon	BGP (small)	BGP (full)GCN	一	71.27	80.92	80.31	91.17	51.26	54.46GraPhSAGE-GCN =	68.57	83.61	81.76	88.14	49.64	48.54GraPhSAGE-mean	69.02	82.31	87.42	90.78	64.96	-63.76-GraPhSAGE-LSTM —	69.17	82.50	87.08	90.09	65.29	-64.67-GraphSAGE-pool (max)	69.47	82.87	87.57	87.39	65.06	-64.24-F	The Performance of GraphSAGE with Different AggregatorsTable 7 reports the F1-Micro scores of GCN and GraphSAGE with four different aggregators: GCN,mean, LSTM and max-pooling. The results show that for GraphSAGE, except on PubMed and BGP,the four aggregators achieve comparable performance. The results of GCN and GraphSAGE-GCNare worse than the others for PubMed and the BGP graphs because of the small information gain(i.e., small λf) of PubMed and the large negative disturbance (i.e., large λl) of BGP as reported inTable 3 and Section 4.2. As explained in Section 3.3, GCN uses additive combination merged withaggregation, where the features of each node are aggregated with the features of its neighbors. As aresult, GCN has poor performance for graphs with small λf and large λl, because it merges the con-text with the surrounding with negative information for a given task. This is further verified by theresults of GraphSAGE using the other three aggregators, which still have comparable performanceon all the datasets. In Section 4, we report the best F1-Micro score among these four aggregators foreach dataset as the performance of GraphSAGE.
