Table 1: Datasets statistics. For the Caltech-256 dataset, we randomly sampled 60 images for each classfollowing the procedure used in (Li et al., 2018). For the Aircraft and Flower dataset, we combined the originaltraining set and validation set and evaluated on the test set. For iNat 2017, we combined the original training setand 90% of the validation set following (Cui et al., 2018).
Table 2: Top-1 validation errors on seven datasets by fine-tuning ImageNet pre-trained ResNet-101 withdifferent hyperparmeters. Each row represents a network fine-tuned by a set of hyperparameters (left fourcolumns). The datasets are ranked by the relative improvement by disabling momentum. The lowest errorrates with the same momentum are marked as bold. Note that the performance difference for Birds is not verysignificant.
Table 3: Verification of the effect of momentum on other source domains rather than ImageNet. The hyperpa-rameters are n = 256, η = 0.01, and λ = 0.0001. Momentum 0 works better for transferring from iNat-2017to Birds and transferring from PlaCes-365 to Indoor comparing to momentum 0.9 counterparts.
Table 4: The ConneCtion between domain similarity and optimal ELR. The values in the seCond Column isprovided by Cui et al. (2018), in whiCh JFT pretrained ResNet-101 was used as the feature extraCtor. Note thatneither the pre-trained model or the dataset is released and we Cannot CalCulate the metriC for other datasets. Inother Columns, we CalCulate domain similarity using ImageNet pre-trained model as the feature extraCtor. The1st, 2nd, 3rd and 4th highest sCores are Color Coded. The optimal ELRs are also listed, whiCh Corresponds to thevalues in Fig 4.
Table 5: The average class error of (Li et al., 2018) and the extension of their experiments of on “dissimilar”datasets. The italic datasets and numbers are our experimental results. Note that the original Indoor result isfine-tuned from Places-365, while we fine-tune just from ImageNet pre-trained models.
Table 6: Comparison of data augmentation methods with different momentum values. The rest of the hyperpa-rameters are: n = 256 and λ = 0.0001.
Table 7: The Top-1 error of ResNet-101 pre-trained on different source dataset.
Table 8: Comparison of default hyperparameters and HPO for both fine-tuning (FT) and training from scratch(ST) tasks. FT Default and ST Default use their default hyperparameters, respectively. HPO refers to the findingthe best hyperparameters with grid search.
