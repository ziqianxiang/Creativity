Table 1: Standard and robust performance of various adversarial training methods on CIFAR10 for= 8/255 and their corresponding training timesMethod	Standard accuracy	PGD ( = 8/255)	Time (min)FGSM + DAWNBench			+ zero init	85.18%	0.00%	12.37+ early stopping	71.14%	38.86%	7.89+ previous init	86.02%	42.37%	12.21+ random init	85.32%	44.01%	12.33+ α = 10/255 step size	83.81%	46.06%	12.17+ α = 16/255 step size	86.05%	0.00%	12.06+ early stopping	70.93%	40.38%	8.81“Free” (m = 8) (Shafahi et al., 2019)1	85.96%	46.33%	785+ DAWNBench	78.38%	46.18%	20.91PGD-7 (Madry et al., 2017)2	87.30%	45.80%	4965.71+ DAWNBench	82.46%	50.69%	68.8training combined with random initialization is just as effective a defense as PGD-based training.
Table 2: Robustness of FGSM and PGD adversarial training on MNISTMethod	Standard accuracy	PGD	(e = 0.1)	PGD (e = 0.3)	Verified (e	= 0.1)PGD	99.20%	97.66%	89.90%	96.7%FGSM	99.20%	97.53%	88.77%	96.8%can drastically reduce the memory utilization, and when tensor cores are available, also reduce run-time. In some DAWNBench submissions, switching to mixed-precision computations was key toachieving fast training while keeping costs low.
Table 3: Time to train a robust CIFAR10 classifier to 45% robust accuracy using various adversarialtraining methods with the DAWNBench techniques of cyclic learning rates and mixed-precisionarithmetic, showing significant speedups for all forms of adversarial training.
Table 4: Imagenet classifiers trained with adversarial training methods at = 2/255 and = 4/255.
Table 5: Time to train a robust ImageNet classifier using various fast adversarial training methodsMethod	Precision	Epochs	Min/epoch	Total time (hrs)FGSM (phase 1)	single	6	22.65	2.27FGSM (phase 2)	single	6	65.97	6.60FGSM (phase 3)	single	3	114.45	5.72FGSM	single	15	-	14.59Free (m = 4)	single	92	34.04	52.20FGSM (phase 1)	mixed	6	20.07	2.01FGSM (phase 2)	mixed	6	53.39	5.34FGSM (phase 3)	mixed	3	95.93	4.80FGSM	mixed	15	-	12.14Free (m = 4)	mixed	92	25.28	38.76Using the minimum number of epochs needed for each training method to reach a baseline of 45%robust accuracy, we report the total training time in Table 3. We find that while all adversarial train-ing methods benefit from the DAWNBench improvements, FGSM adversarial training is the fastest,capable of learning a robust CIFAR10 classifier in 6 minutes using only 15 epochs. Interestingly, wealso find that PGD and free adversarial training take comparable amounts of time, largely becausefree adversarial training does not benefit from the cyclic learning rate as much as PGD or FGSMadversarial training.
Table 6: Ablation study showing the performance of R+FGSM from Tramer et al. (2017) and thevarious changes for the version of FGSM adversarial training done in this paper, over 10 randomseeds.
Table 7: Training parameters used for the DAWNBench experiments of Table 1Parameter	FGSM	PGD	FreeEpochs	30	40 Max learning rate	0.2	0.2	96 0.04A A DIRECT COMPARISON TO R+FGSM FROM TRAMEr ET AL. (2017)While a randomized version of FGSM adversarial training was proposed by Tramer et al. (2017), itwas not shown to be as effective as adversarial training against a PGD adversary. Here, we note thetwo main differences between our approach and that of Tramer et al. (2017).
Table 8: Training parameters used for Figure 2ParameterFGSM	PGD	FreeMax learning rate	0.20.2	0.04Table 9: ImageNet classifiers trained with free adversarial training methods at m = 3 minibatchreplay when augmented with DAWNBench optimizations, against '∞ perturbations of radius E =4/255, where 30 epochs of free training is equivalent to 15 epochs of FGSM trainingMethod	Step size	Epochs	Standard acc.	PGD+1	PGD+10Free+DAWNBench	4/255	15	49.87%	22.78%	22.18%Free+DAWNBench	5/255	15	50.48%	22.88%	22.25%Free+DAWNBench	4/255	30	49.87%	28.17%	27.08%Free+DAWNBench	5/255	30	50.48%	28.73%	27.81%Free (m = 4)	4/255	92	60.42%	31.22%	31.08%FGSM	5/255	15	55.45%	30.28%	30.18%E Training parameters for Figure 2For all methods, we use a batch size of 128, and SGD optimizer with momentum 0.9 and weightdecay 5 * 10-4. We report the average results over 3 random seeds. Maximum learning rates usedfor the cyclic learning rate schedule are shown in Table 8.
Table 9: ImageNet classifiers trained with free adversarial training methods at m = 3 minibatchreplay when augmented with DAWNBench optimizations, against '∞ perturbations of radius E =4/255, where 30 epochs of free training is equivalent to 15 epochs of FGSM trainingMethod	Step size	Epochs	Standard acc.	PGD+1	PGD+10Free+DAWNBench	4/255	15	49.87%	22.78%	22.18%Free+DAWNBench	5/255	15	50.48%	22.88%	22.25%Free+DAWNBench	4/255	30	49.87%	28.17%	27.08%Free+DAWNBench	5/255	30	50.48%	28.73%	27.81%Free (m = 4)	4/255	92	60.42%	31.22%	31.08%FGSM	5/255	15	55.45%	30.28%	30.18%E Training parameters for Figure 2For all methods, we use a batch size of 128, and SGD optimizer with momentum 0.9 and weightdecay 5 * 10-4. We report the average results over 3 random seeds. Maximum learning rates usedfor the cyclic learning rate schedule are shown in Table 8.
