Table 1: Statistics of the test accuracy distribution for q-FFL. By setting q > 0, the accuracy of theworst 10% devices is increased at the cost of possibly decreasing the accuracy of the best 10% devices.
Table 2: Our objective compared with weighing devices adversarially (AFL (Mohri et al., 2019)). Inorder to be favorable to AFL, we use the two small, well-behaved datasets studied in (Mohri et al.,2019). q-FFL (q > 0) outperforms AFL on the worst testing accuracy of both datasets. The tunableparameter q controls how much fairness we would like to achieve—larger q induces less variance.
Table 3: Statistics of the accuracy distribution of personalizedmodels using q-MAML. The method with q = 0 corresponds toMAML. Similarly, we see that the variance is reduced while theaccuracy of the worst 10% devices is increased.
Table 4: Statistics of federated datasetsDataset	Devices Samples Samples/device				mean			stdevSynthetic	100	12,697	127	73Vehicle	23	43,695	1,899	349Sent140	1,101	58,170	53	32Shakespeare	31	116,214	3,749	6,912D.2 Implementation DetailsD.2.1 MachinesWe simulate the federated setting (one server and m devices) on a server with 2 IntelR XeonRE5-2650 v4 CPUs and 8 NVidiaR 1080Ti GPUs.
Table 5: Full statistics of the test accuracy distribution for q-FFL. q-FFL increases the accuracy ofthe worst 10% devices without decreasing the average accuracies. We see that q-FFL encouragesmore uniform distributions under all uniformity metrics defined in Appendix A.2: (1) the varianceof the accuracy distribution (Definition 4), (2) the cosine similarity/geometric angle between theaccuracy distribution and the all-ones vector 1 (Definition 5), and (3) the KL-divergence between thenormalized accuracy vector a and the uniform distribution u, which can be directly translated to theentropy of a (Definition 6) .
Table 6: q-FFL results in more fair training accuracy distributions in terms of all uniformitymeasurements—(a) the accuracy variance, (b) the cosine similarity (i.e., angle) between the ac-curacy distribution and the all-ones vector 1, and (c) the KL divergence between the normalizedaccuracy a and uniform distribution u.
Table 7: Average testing accuracy under q-FFL objectives. We show that the resulting solutions ofq = 0 and q > 0 objectives have approximately the same average accuracies both with respect to alldata points and with respect to all devices.
Table 8: More statistics comparing the uniform sampling objective with q-FFL in terms of trainingaccuracies. We observe that uniform sampling could result in more fair training accuracy distributionswith smaller variance in some cases.
Table 9: More statistics showing more fair solutions induced by q-FFL compared with the uniformsampling baseline in terms of test accuracies. Again, we observe that under q-FFL, the testingaccuracy of the worst 10% devices tends to increase compared with uniform sampling, and thevariance of the final testing accuracies is smaller. Similarly, q-FFL is also more fair than uniformsampling in terms of other uniformity metrics.
Table 10: Effects of data heterogeneity and the number of devices on unfairness. For a fixed numberof devices, as data heterogeneity increases from top to bottom, the accuracy distributions becomeless uniform (with larger variance) for both q = 0 and q > 0. Within each dataset, the decreasingnumber of devices results in a more uniform accuracy distribution. In all scenarios (except on IIDdata), setting q > 0 helps to encourage more fair solutions.
Table 11: Test accuracy statistics of using a family of q’s on synthetic data. We show results with q’sselected from our candidate set {0.001, 0.01, 0.1, 1, 2, 5, 10, 15}. q-FFL allows for a more flexibletrade-off between fairness and accuracy. A larger q results in more fairness (smaller variance),but potentially lower accuracy. Similarly, a larger q imposes more uniformity in terms of othermetrics—(a) the cosine similarity/angle between the accuracy distribution and the all-ones vector 1,and (b) the KL divergence between the normalized accuracy a and a uniform distribution u.
Table 12: Effects of running q-FFL with several q’s in parallel. We train multiple global models(corresponding to different q’s) independently in the network. After the training finishes, eachdevice picks up a best, device-specific model based on the performance (accuracy) on the validationdata. While this adds additional local computation and more communication load per round, thedevice-specific strategy has the added benefit of increasing the accuracies of devices with the worst10% accuracies and devices with the best 10% accuracies simultaneously. This strategy is built uponthe proposed primitive Algorithm 2, and in practice, people can develop other heuristics to improvethe performance (similar to what we explore here), based on the method of adaptively averagingmodel updates proposed in Algorithm 2.
