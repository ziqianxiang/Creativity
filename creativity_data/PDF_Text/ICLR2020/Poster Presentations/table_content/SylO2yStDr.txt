Table 1: Results on WMT en-de Machine Translation (newstest2014 test set)Model	Layers	Params	PPLAdaptive Inputs (Baevski & Auli, 2018)	16	247M	18.7Transformer XL Large (Dai et al., 2019)	18	257M	18.3Adaptive Inputs + LayerDrop	16	247M	18.3Adaptive Inputs + LayerDrop	40	423M	17.7Table 2: Results on Wikitext-103 language modeling benchmark (test set).
Table 2: Results on Wikitext-103 language modeling benchmark (test set).
Table 3: Results for CNN-Dailymail Summarization and ELI5 QA (test set).
Table 4: Results on Various NLU Tasks for RoBERTa Large trained for 500K updates (dev set).
Table 5: Hyperparameters for RoBERTa PretrainingModel	BLEUTransformer (Wu et al., 2019a)	34.4Dynamic Conv (Wu et al., 2019a)	35.2Transformer + LayerDrop	34.5Table 6: BLEU for IWSLT (test set).
Table 6: BLEU for IWSLT (test set).
Table 7: Comparison between BERT base with and without distillation with our RoBERTa basetrained with LayerDrop. Our models are pruned before finetuning on each individual task. Thenumbers from BERT are taken from Devlin et al. (2018).
Table 8: Impact of additional finetuning on a16 layer language model pruned to 8 layers.
Table 9: Performance Varying Dropout withFixed LayerDrop on a 16 layer languagemodel trained on Wikitext-103 (Valid).
Table 10: Random v. Linear Decay Layer-Drop on a 16 layer language model trained onWikitext-103 (Valid). * result is from Baevski& Auli (2018)Structured Dropout	Valid PPLHalfFFN	29.6Baseline	28.3Head	28.1Sublayer	19.9Head + Sublayer	19.8Layer	19.7Head + Layer	19.7Table 11: Performance Varying StructuredDropout and Pruning to an 8 layer languagemodel trained on Wikitext-103 (Valid). Prun-ing is done by removing every other layer tohalf the model size.
Table 11: Performance Varying StructuredDropout and Pruning to an 8 layer languagemodel trained on Wikitext-103 (Valid). Prun-ing is done by removing every other layer tohalf the model size.
