Table 1: (Left) The intuitive assumptions activation functions (AFs) should ultimately fulfill. (Right)Existing ELU, CELU and ReLU like AFS do not fulfill them. Only learnable AFs alloW one totune their shape at training time, ignoring hyper-parameters such as α for LReLU. For SWish, ourexperimental results do not indicate problems With vanishing gradients. SLAF (Goyal et al., 2019)shoWed undefined values (iii), and We could not judge their performance (v).
Table 2: Performance comparison of activation functions on MNIST and Fashion-MNIST (the higher,the better) on two common deep architectures. Shown are the results averaged over 5 reruns as wellas the top result among these 5 runs. The best (“•”) and runner-up (“◦”) results per architecture arebold. As one can see, PAUs consistently outperform the other activation functions on average andyields the top performance on each dataset.
Table 3: Performance comparison of activation functions on CIFAR-10 (the higher, the better) onfour state-of-the-art deep neural architectures. Shown are the results averaged over 5 reruns as wellas the top result among these 5 runs. The best (“•”) and runner-up (“◦”) results per architecture arebold. As one can see, PAUs are either in the lead or close to the best. (“***”) are experiments thatdid not finish on time.
Table 4: MobileNetV2 top-1 and top-5 accuracies in ImageNet (higher is better) for differentactivations. Best (“•”) and runner-up (“◦”) are shown in bold. PAU is the best in top-1 accuracy andrunner-up for top-5.
Table 5: The number of models on which PAU and RPAU outperforms or underperforms each baselineactivation function we compared against in our experiments.
Table 6: Initial coefficients to approximate different activation functions.
Table 7: Architecture of Simple Convolutional Neural NetworksA.3.2 Learning ParametersThe parameters of the networks, both the layer weights and the coefficients of the PAUs, were trainedover 100 epochs using Adam (Kingma and Ba, 2015) with a learning rate of 0.002 or SGD (Qian,1999) with a learning rate of 0.01, momentum set to 0.5, and without weight decay. In all experimentswe used a batch size of 256 samples. The weights of the networks were initialized randomly andthe coefficients of the PAUs were initialized with the initialization constants of Leaky ReLU, seeTab. 6. We report the mean of 5 different runs for both the accuracy on the test-set and the loss on thetrain-set after each training epoch.
