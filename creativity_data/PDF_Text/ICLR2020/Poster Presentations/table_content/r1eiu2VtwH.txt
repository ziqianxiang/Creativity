Table 1: The comparison of NODE with the shallow state-of-the-art counterparts with default hy-perparameters. The results are computed over ten runs with different random seeds.
Table 2: The comparison of NODE with both shallow and deep counterparts with hyperparameterstuned for optimal performance. The results are computed over ten runs with different random seeds.
Table 3: The experimental comparison of various choice functions and architecture depths. TheFigure 3: NODE on UCI Higgs dataset: Left-Top: individual feature importance distributions forboth original and learned features. Left-Bottom: mean absolute contribution of individual trees tothe final response. Right: responses dependence on feature importances. See details in the text.
Table 4: Training and inference runtime for models with 1024 trees of depth six on the YearPredic-tion dataset, averaged over five runs. Both training and inference of eight-layer NODE architectureon GPU are on par with shallow counterparts of the same total number of trees in an ensemble.
Table 5: The datasets used in our experiments.
