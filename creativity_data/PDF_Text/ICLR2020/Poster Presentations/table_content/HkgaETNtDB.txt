Table 1: Mean (max) dev scores across 20 random restarts when finetuning BERTLARGE withvarious regularization strategies on each task. We show the following baseline results on the first andsecond cells: Devlin et al. (2018)’s regularization strategy (both dropout(p) and wdecay(0, 0.01))and Wiese et al. (2017)’s regularization strategy (wdecay(wpre, {0.01, 0.04, 0.07, 0.10})). In thethird cell, we demonstrate finetuning results with only mixout(wpre, {0.7, 0.8, 0.9}). The resultswith both mixout(wpre, {0.7, 0.8, 0.9}) and wdecay(wpre, 0.01) are also presented in the fourthcell. Bold marks the best of each statistics within each column. The mean dev scores greatly increasefor all the tasks when we use mixout(wpre, {0.7, 0.8, 0.9}).
Table 2: Mean (max) SST-2 dev scores across 20 random restarts when finetuning BERTLARGEwith each regularization strategy. Bold marks the best of each statistics within each column. For alarge training set, both mean and maximum dev scores are similar to each other.
Table 3: We present mean (max) dev scores across 20 random restarts with various regularizationtechniques for the additional output layers (ADDITIONAL) when finetuning BERTLARGE on eachtask. For all cases, we apply mixout(wpre, 0.7) to the pretrained layers (PRETRAINED). Thefirst row corresponds to the setup in Section 5. In the second row, we apply mixout(w0, 0.7) tothe additional output layer where w0 is its randomly initialized parameter. The third row showsthe results obtained by applying dropout(0.7) to the additional output layer. In the fourth row, wedemonstrate the best of each result from all the regularization strategies shown in Table 1. Boldmarks the best of each statistics within each column. We obtain additional gains in dev scores byvarying the regularization technique for the additional output layer.
Table 4: We present the test score when finetuning BERTLARGE with each regularization strat-egy on each task. The first row shows the test scores obtained by using both dropout(p) andwdecay(0, 0.01). These results in the first row are reported by Devlin et al. (2018). They usedthe learning rate of {2 × 10-5, 3 × 10-5, 4 × 10-5, 5 × 10-5} and a batch size of 32 for 3epochs with multiple random restarts. They selected the best model on each dev set. In the secondrow, we demonstrate the test scores obtained by using the proposed mixout in Section 6.2: usingmixout(wpre, 0.7) for the pretrained layers and mixout(w0, 0.7) for the additional output layerwhere w0 is its randomly initialized weight parameter. We used the learning rate of 2 × 10-5 and abatch size of 32 for 3 epochs with 20 random restarts. We submitted the best model on each dev set.
