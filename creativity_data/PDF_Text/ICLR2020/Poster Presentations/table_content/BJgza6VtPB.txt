Table 1: The effect of temperature on samples from an language model trained via MLE on theEMNLP17 News dataset. At a temperature of α = 1.0 the samples are syntactically correct butoften lack in global coherence. The sample quality varies predictably with temperature. At α > 1.0,the syntax breaks down and at α = 0.0 the model always outputs the same sequence. At α = 0.7the samples are both of high quality and of sufficient diversity.
Table 2: NLLoracle measured on the synthetictask (lower is better). All results are taken fromtheir respective papers. An MLE-trained modelwith reduced temperature easily improves uponthese GAN variants, producing the highest qual-ity sample.
Table 3: Samples from the different models on Image COCO and EMNLP2017 WMT News. ForSeqGAN and LeakGAN, samples were taken from (Guo et al., 2017). It’s the first two samplesfound in their appendix. For our samples, we reduced the temperature of the model till we achievedsimilar BLEU scores to the ones reported in (Guo et al., 2017) in order to keep comparison fair.
Table 4: BLEU (left) and Self-BLEU (right) on test data of EMNLPNEWS 2017. (Higher BLEUand lower Self-BLEU is better).
Table 5: BLEU (left) and Self-BLEU (right) on test data of Image COCO. (Higher BLEU and lowerSelf-BLEU is better).
Table 6:	Three randomly sampled sentences from our model with closest BLEU scores to the trainingset’s. The sentences have poor semantics or global coherence. They are also not perfect grammati-cally speaking.
Table 7:	Samples from SeqGAN taken from Guo et al. (2017).
Table 8:	Samples from LeakGAN taken from Guo et al. (2017).
Table 9:	Samples from our MLE with temperature to match BLEU scores reported in Guo et al.
