Table 1: Description of the datasets used for evaluation	Synthetic	Image COCO	EMNLP2017 WMT NEWSCategory	simulated data	image description	news articleVocabulary size	5000	4682	5255Sequence length	20/40	<37	<51Sentence number (training)	10000	10000	270000Sentence number (test)	10000	10000	10000As SeqGAN (Yu et al., 2017), our generator is a single-layer LSTM (Hochreiter & Schmidhuber,1997) and our discriminator is almost based on TextCNN (Kim, 2014) except that it concatenates thefeature representation of two compared samples and outputs the probability for their comparativerelations (i.e., >, <, ≈). We keep the most of the hyperparameters same with the SeqGAN exceptthe hyperparameters introduced by our models (i.e., w(>), w(<), w(≈)) which are tuned based on thesynthetic experiment and kept the same for the real data experiments.
Table 2: Performance comparison of different models in synthetic tests where sequence length is setto 20 and 40 respectively. For all the metrics presented, the lower, the better.
Table 3: Performance comparison of different models in the COCO caption generation task. Metricsfrom top to bottom represent respectively the generation quality, the generation diversity, and thedivergence between real data of generated sentences. For all the BLEU metrics, the higher, the better;for NLLgen and FD, the lower, the better.
Table 4: Performance comparison of different models in the EMNLP2017 WMT news generationtask. Metrics from top to bottom represent respectively the generation quality, the generation diversity,and the divergence between real and generated data. For all the BLEU metrics, the higher, the better.
Table 5: Human evaluation results of different models in both datasets. Scores are between 1-5,higher score indicates better quality.
Table 6: ResUlts of the ablation tests in the Synthetic data and the COCO dataset.
Table 7:	Samples generated by SAL in Image COCO dataseta picture of a person ’s umbrella in a cell phone .
Table 8:	Samples generated by CAL in Image COCO dataseta man is on a towel on a table outside of a real kitchen .
Table 9:	Samples generated by SeqGAN in Image COCO dataseta large image of a herd of racing train .
Table 10:	Samples generated by MLE in Image COCO dataseta jet airplane flies flying through front from an airplane .
Table 11:	Samples generated by SAL in EMNLP2017 WMT dataset(1)	it ’ s likely to be egyptian and many of the canadian refugees , but for a decade .
Table 12:	Samples generated by CAL in EMNLP2017 WMT dataset(1)	i didn ’ t put relatively quiet , we have , ’ his work right in the particular heat rate , takesteps traditionally clean .
Table 13:	Samples generated by SeqGAN in EMNLP2017 WMT dataset(1)	his missed 4 , 000 the first 95 really 69 - year - olds .
Table 14:	Samples generated by MLE in EMNLP2017 WMT dataset(1)	you know that that is great for our ability to make thinking about how you know and you ?(2)	when it ’ s a real thing possible , is if you the first time in a time here and get .
Table 15: Case StUdy of ComParative discrimination and self-adversarial learning.
Table 16: The human evaluation scale from 1to5 with corresponding criteria and example sentences.
Table 17: Performance comparison of different models in synthetic tests where sequence length is setto 20 and 40 respectively. For all metrics presented, lower value is better.
Table 18: Performance comparison of different models in the COCO caption generation task. Metricsfrom top to bottom represent respectively the generation quality, the generation diversity, and thedivergence between real data of generated sentences. For all BLEU metrics, higher value is better, forNLLgen and FD, lower is better.
