Table 1: Comparison to existing popular language modelling benchmarks.
Table 2: PG-19 statistics split by subsets.	Table 3: Eval. perplexities on PG-19.
Table 4: State-of-the-art results on EnWik8.
Table 5: Compression approaches on Enwik8.
Table 6: Validation and test perplexities on WikiText-103.
Table 7: WikiText-103 test perplexity broken down by word frequency buckets. The most frequentbucket is words which appear in the training set more than 10, 000 times, displayed on the left. Forreference, a uniform model would have perplexity |V | = 2.6e5 for all frequency buckets. *LSTMcomparison from Rae et al. (2018)	> 10K	1K-10K	100 - 1K	< 100	AllLSTM*	12.1	219	1,197	9,725	36.4TransformerXL (ours)	7.8	61.2	188	1,123	18.1Compressive Transformer	7.6	55.9	158	937	17.1Relative gain over TXL	2.6%	9.5%	21%	19.9%	5.8%Figure 2: Attention weight on Enwik8. Av-erage attention weight from the sequence overthe compressed memory (oldest), memory, andsequence (newest) respectively. The sequenceself-attention is causally masked, so more at-tention is placed on earlier elements in the se-quence. There is an increase in attention at thetransition from memory to compressed memory.
Table 8: Compressed memory size vs test performance for Enwik814Compressed Memory Size	256	512	1024	1536	2048WikiText-103 Perplexity	18.2	17.9	17.6	17.1	17.7Table 9: Compressed memory size vs test performance for WikiText-103D PG-19 PreprocessingThe raw texts from the Gutenberg project were minimally pre-processed by removing boilerplatelicense text. We then also replaced discriminatory words with a unique hDWxi token using theOfcom list of discriminatory words 4.
Table 9: Compressed memory size vs test performance for WikiText-103D PG-19 PreprocessingThe raw texts from the Gutenberg project were minimally pre-processed by removing boilerplatelicense text. We then also replaced discriminatory words with a unique hDWxi token using theOfcom list of discriminatory words 4.
Table 10: Examples of top topics on PG-19 corpus.
