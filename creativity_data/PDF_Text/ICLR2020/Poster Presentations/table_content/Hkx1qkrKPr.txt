Table 1: Testing accuracy (%) comparisons on different backbones w and w/o DropEdge.
Table 2: Accuracy (%) comparisons with SOTAs. The number in parenthesis denotes the networkdepth for the models with DropEdge.
Table 3: Dataset StatisticsDatasets	Nodes	Edges	Classes	Features	Traing/Validation/Testing	TypeCora	2,708	5,429	7	1,433	1,208/500/1,000	TransductiveCiteseer	3,327	4,732	6	3,703	1,812/500/1,000	TransductivePubmed	19,717	44,338	3	500	18,217/500/1,000	TransductiveReddit	232,965	11,606,919	41	602	152,410/23,699/55,334	InductiveB.2	Models and BackbonesBackbones Other than the multi-layer GCN, we replace the CNN layer with graph convolutionlayer to implement three popular backbones recasted from image classification. They are residualnetwork (ResGCN)(He et al., 2016; Li et al., 2019), inception network (IncepGCN)(Szegedy et al.,2016) and dense network (JKNet) (Huang et al., 2017; Xu et al., 2018b). Figure 5 shows the detailedarchitectures of four backbones. Furthermore, we employ one input GCL and one output GCL onthese four backbones. Therefore, the layers in ResGCN, JKNet and InceptGCN are at least 3 layers.
Table 4: Hyper-parameter DescriptionHyper-parameter	Descriptionlr weight-decay sampling-percent dropout normalization withloop withbn	learning rate L2 regulation weight edge preserving percent (1 — P) dropout rate the propagation models (KiPf & Welling, 2017) using self feature modeling using batch normalization15Published as a conference paper at ICLR 2020Table 5: The normalization / propagation modelsDescription	Notation	A0First-order GCN Augmented Normalized Adjacency Augmented Normalized Adjacency with Self-loop Augmented Random Walk	FirstOrderGCN- AugNormAdj BingGeNormAdj AUgRWalk	I + D-1/2AD-1/2 (D + I)-1/2 (A + I )(D + I)-1/2 I + (D + I)T∕2(A + I )(D + I)-1/2 (D + I)T(A + I)	Table 6: The hyper-parameters of best accuracy for each backbone on all datasets.
Table 5: The normalization / propagation modelsDescription	Notation	A0First-order GCN Augmented Normalized Adjacency Augmented Normalized Adjacency with Self-loop Augmented Random Walk	FirstOrderGCN- AugNormAdj BingGeNormAdj AUgRWalk	I + D-1/2AD-1/2 (D + I)-1/2 (A + I )(D + I)-1/2 I + (D + I)T∕2(A + I )(D + I)-1/2 (D + I)T(A + I)	Table 6: The hyper-parameters of best accuracy for each backbone on all datasets.
Table 6: The hyper-parameters of best accuracy for each backbone on all datasets.
Table 7: Accuracy (%) comparisons on different backbones with and without DropEdgeDataSet	Backbone		2				4					Orignal	DropEdge	Orignal	DropEdge	Origna]	GCN	-86.10	86.50	-85.50	87.60	-7870	ResGCN	-	-	86.00	87.00	85.40Cora	JKNet	-	-	86.90	87.70	86.70	IncepGCN	-	-	85.60	87.90	86.70	GraphSAGE	87.80	88.10	87.10	88.10	84.30	GCN	-75.90	78.70	-76.70	79.20	-7460	ResGCN	-	-	78.90	78.80	77.80Citeseer	JKNet	-	-	79.10	80.20	79.20	IncepGCN	-	-	79.50	79.90	79.60	GraphSAGE	78.40	80.00	77.30	79.20	74.10	GCN	-90.20	91.20	-88.70	91.30	-90^	ResGCN	-	-	90.70	90.70	89.60Pubmed	JKNet	-	-	90.50	91.30	90.60	IncepGCN	-	-	89.90	91.60	90.20	GraphSAGE	90.10	90.70	89.40	91.20	90.20	GCN	-96.11	96.13	-96.62	96.71		ResGCN	-	-	96.13	96.33	96.37
