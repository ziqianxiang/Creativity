Table 1: Large-Scale Simulated Robotic Grasping ResultsBlocking Actions	Timestep VTG Penalty		Previous Action	Grasp Success	Episode Duration	Action CompletionYes	No	No	No	92.72% ± 1.10%	132.09s ±5.70s	92.33% ± 1.476%Yes	Yes	No	No	91.53% ± 1.04%	120.81s ±9.13s	89.53% ± 2.267%No	No	No	No	84.11% ± 7.61%	122.15s ±14.6s	43.4% ± 22.41%No	Yes	No	No	83.77% ± 9.27%	97.16s ±6.28s	34.69% ± 16.80%No	Yes	Yes	No	92.55% ± 4.39%	82.98s ± 5.74s	47.28% ± 14.25%No	Yes	No	Yes	92.70% ± 1.42%	87.15s ±4.80s	50.09% ± 14.25%No	Yes	Yes	Yes	93.49% ± 1.04%	90.75s ±4.15s	49.19% ± 14.98%4.2	Concurrent QT-Opt on Large-Scale Robotic GraspingNext, we evaluate scalability of our approach to a practical robotic grasping task. We simulatea 7 DoF arm with an over-the-shoulder camera, where a bin in front of the robot is filled withprocedurally generated objects to be picked up by the robot. A binary reward is assigned if an objectis lifted off a bin at the end of an episode. We train a policy with QT-Opt (Kalashnikov et al., 2018),a deep Q-Learning method that utilizes the cross-entropy method (CEM) to support continuousactions. In the blocking mode, a displacement action is executed until completion: the robot uses aclosed-loop controller to fully execute an action, decelerating and coming to rest before observingthe next state. In the concurrent mode, an action is triggered and executed without waiting, whichmeans that the next state is observed while the robot remains in motion. Further details of thealgorithm and experimental setup are shown in Figure 3 and explained in Appendix A.4.2.
Table 2: Real-World Robotic Grasping Results.
