Table 1: Quantitative performance comparison of our proposed architecture on KITTI fordepths up to 80m. M refers to methods that train using monocular images, S refers to methodsthat train using stereo pairs, D refers to methods that use ground-truth depth supervision, Sem refersto methods that include semantic information, and Inst refers to methods that include semanticand instance information. MR indicates 640 x 192 input images, and HR indicates 1280 x 384input images. Our proposed architecture is able to further improve the current state of the art inself-supervised monocular depth estimation, and outperforms other methods that exploit semanticinformation (including ground truth labels) by a substantial margin.
Table 2: Ablative analysis of our semantic guidance (SEM) and two-stage-training (TST) contri-butions. The last column indicates class-average Abs. Rel. obtained by averaging all class-specificdepth errors in Figure 4, while other columns indicate pixel-average metrics.
Table 3: Analysis of the impact of pre-training the semantic segmentation network. On the Pre-Train column, I indicates ImageNet (Deng et al., 2009) pretraining and CS indicates CityScapes(Cordts et al., 2016) pretraining, with 1/2 indicating the use of only half the dataset (samples chosenrandomly). In the Fine-Tune column, D indicates fine-tuning the depth network and S indicates fine-tuning the semantic network (note that this is a self-supervised fine-tuning for the depth task, usingthe objective described in Section 3.1).
Table 4: Generalization capability of different networks, trained on both KITTI and CityScapesdatasets and evaluated on the NuScenes (Caesar et al., 2019) dataset. Our proposed semantically-guided architecture is able to further improve upon the baseline from Guizilini et al. (2019), whichonly used unlabeled image sequences for self-supervised depth training.
