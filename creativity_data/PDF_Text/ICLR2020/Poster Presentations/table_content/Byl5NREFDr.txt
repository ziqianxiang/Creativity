Table 1: Representative examples from the extraction datasets, highlighting the effect of task-specific heuristics in MNLI and SQuAD. More examples in Appendix A.5.
Table 2: A comparison of the original API (victim) with extracted models (random and wiki) interms of Accuracy on the original development set and Agreement between the extracted and victimmodel on the development set inputs. Notice high accuracies for extracted models. Unless specified,all extraction attacks were conducted use the same number of queries as the original training dataset.
Table 3: Development set accuracy of various extracted models on the original development set atdifferent query budgets expressed as fractions of the original dataset size. Note the high accuraciesfor some tasks even at low query budgets, and diminishing accuracy gains at higher budgets.
Table 4: Development set accuracy using wiki queries on MNLI and SQuAD with mismatchedBERT architectures between the victim and attacker. Note the trend: (large, large) > (base, large) >(base, base) > (large, base) where the (∙, ∙) refers to (victim, attacker) pretraining.
Table 5: SQuAD dev set results comparing BERT-large and XLNet-large attacker architectures.
Table 6: SQuAD dev set results on QANet, with and without GloVE (Pennington et al., 2014).
Table 7: Accuracy of membership classi-fiers on an identically distributed develop-ment set (WIKI) and differently distributedtest sets (RANDOM, SHUFFLE).
Table 8: Limited model extraction success on SQuAD 2.0 which includes unanswerable questions.
Table 9: Results on watermarked models. Dev Acc represents the overall development set accuracy,WM Label Acc denotes the accuracy of predicting the watermarked output on the watermarkedqueries and Victim Label Acc denotes the accuracy of predicting the original labels on the water-marked queries. A watermarked WIKI has high WM Label Acc and low Victim Label Acc.
Table 10: Ablation study of the membership classifiers. We measure accuracy on an identicallydistributed development set (WIKI) and differently distributed test sets (RANDOM, SHUFFLE). Notethe last layer representations tend to be more effective in classifying points as real or fake.
Table 11: Agreement between annotators Note that the agreement follows the expected intuitivetrend — original SQUAD >> wiki, highest agreement > random, highest agreement 〜 wiki,lowest agreement > RANDOM, lowest agreement.
Table 12: Development set F1 using different kinds of extraction datasets on SQuAD 1.1. The finalrandom and wiki schemes have also been indicated in the table.
Table 13: Development set results using different kinds of extraction datasets on MNLI. The finalrandom and wiki schemes have also been indicated in the table.
Table 14: More example queries from our datasets and their outputs from the victim model.
