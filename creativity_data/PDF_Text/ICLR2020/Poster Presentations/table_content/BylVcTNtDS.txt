Table 1: Attack performance on balanced re-training dataset. Acc, NABAC, and Eff stands foraccuracy, number of attempts to break all classes, and effectiveness, respectively.
Table 2: Attack performance on imbalanced re-training dataset. Acc, NABAC, and Eff stands foraccuracy, number of attempts to break all classes, and effectiveness, respectively.
Table 3: Effect of number of new layers in the re-trained model# of new layers	Accuracy	NABAC	Effectiveness(95%)	Effectiveness(99%)1	99.12% ± .27	-^48.25 ± 42.5^^	91.68% ± 5.69	87.82% ± 6.982	98.24% ± 2.10	51.87 ± 39.94	91.57% ± 4.87	86.45% ± 5.353	95.46% ± 4.2	257.26 ± 38716	89.45% ± 8.20	85.67% ± 8.88Figure 4: Number of reject training samples vs effectiveness/accuracysmooths out the single large value and distributes it to more neurons in activation vector. That is thereason the attack becomes less effective when more new layers are added.
Table 4: Attack Comparison. NQT, NQS, and Ef stands for number of query to the teacher model,number of query to the student model, and effectiveness, respectively.
Table 5: Impact of initial input on the attackInitial input	NABAC	EffeCtiVeness(95%)	Effectiveness(99%)Blank	18	98.37%	98.37%Random	19	98.37%	97.22%A face image	18	99.83%	99.19%Distribution of Target Classes. Fig. 6(a) illustrates a typical distribution of target classes triggeredby crafted images of the proposed method. It is clear that the distribution is far from Uniform. Itbasically means that more neurons in layer n- 1 are associated with class 1 and, hence, during bruteforce attack, more crafted images will trigger that class.
Table 6: Effect of number of target classes on the proposed attack# of target classes	Accuracy	NABAC	Effectiveness(95%)	Effectiveness(99%)5	97.38%	37	100.00%	98.21%10	93.30%	~~∏4^^	95.80%	93.75%15	85.72%	812	92.22%	84.17%Table 7: Effect of re-training set size# of samples per class	Accuracy	NABAC	Effectiveness(95%)	Effectiveness(99%)50	77.56%	13	97.48%	95.00%100	82.46%	17	97.21%	95.23%200	85.51%	21	98.25%	96.82%1000	89.89%	17	98.60%	97.64%2000	92.04%	17	98.60%	97.81%com (2017) to re-train the model. 80% of the dataset is used for fine-tuning and 20% for inference.
Table 7: Effect of re-training set size# of samples per class	Accuracy	NABAC	Effectiveness(95%)	Effectiveness(99%)50	77.56%	13	97.48%	95.00%100	82.46%	17	97.21%	95.23%200	85.51%	21	98.25%	96.82%1000	89.89%	17	98.60%	97.64%2000	92.04%	17	98.60%	97.81%com (2017) to re-train the model. 80% of the dataset is used for fine-tuning and 20% for inference.
