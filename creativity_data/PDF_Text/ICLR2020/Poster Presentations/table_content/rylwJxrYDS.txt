Table 1: WSJ accuracy of vq-wav2vec on the development (nov93dev) and test set (nov92) in termsof letter error rate (LER) and word error rate (WER) without language modeling (No LM), a 4-gramLM and a character convolutional LM. vq-wav2vec with BERT pre-training improves over the bestwav2vec model (Schneider et al., 2019).
Table 2: Comparison of Gumbel-Softmax and k-means vector quantization on WSJ (cf. Table 1).
Table 3: TIMIT phoneme recognition in terms of phoneme error rate (PER). All our models use theCNN-8L-PReLU-do0.7 architecture (Zeghidour et al., 2018).
Table 4: Librispeech results for a standard sequence to sequence model trained on discretized audiowithout BERT pre-training and results from the literature. All results are without a language model.
Table 5: TIMIT PER for (a) different mask sizes M with pM = 0.15 in BERT training and (b) maskprobabilities p for a fixed mask length M = 10.
Table 6: PER on TIMIT dev set for vq-wav2vec models trained on Libri100. Results are based onthree random seeds.
Table 7: Fraction of used codewords vs. number of theoretically possible codewords V G in brackets;39.9M is the number of tokens in Librispeech 100h .
