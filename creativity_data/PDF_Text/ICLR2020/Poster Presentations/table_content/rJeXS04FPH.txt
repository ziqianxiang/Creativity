Table 1: Performance of RNN-based language models on WT-103 and PTB dataset. In (a), standardrefers to standard (linear) embedding and classification layers while adaptive refers to adaptive inputand adaptive softmax for the input and the output layers, respectively.
Table 2: Transformer-XL performance on Wikitext-103 dataset. We use DeFINE with N = 3,k = 4096, and m = 384. For models without DeFINE, we use projective embeddings (Dai et al.,2019) that linearly projects the vector ei to a dimension of m = 384. Except the row marked with? that uses inner model dimension of 2100, all other rows uses an inner model dimension of 1920.
Table 3: Results of Transformer-based model (with and without DeFINE) on the task of neuralmachine translation. DeFINE attains similar performance to checkpoint averaging, but with fewerparameters.
Table 4: Performance comparison of different sequence models with different factorization methods.
Table 5: The performance of Transformer-XL with different factorization methods, with and withoutcompression method of Shu & Nakayama (2017). For compression, we used a 32 x 16 codingdescribed in Shu & Nakayama (2017).
Table 6: Comparison between different transformations on the WikiText-103 dataset.
Table 8: Different settings on WT-103: (a) Impact of different skip-connections. See Figure 5b andFigure 5c in Section A.2 for block level diagrams. (b) Impact of reduce operation in MER (Sec-tion 3.1).
Table 7: Impact of scaling depth and width on WT-103.
Table 9: Hyper-parameters for training word-level LSTM-based language model on WikiText-103.
