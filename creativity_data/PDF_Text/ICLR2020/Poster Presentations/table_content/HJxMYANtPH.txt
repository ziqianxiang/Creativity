Table 1: Classification accuracy of Algorithm 1 and other methods on the MNIST dataset. BCEstands for the binary cross-entropy loss. We use K-means for relative similarity based clusteringalgorithms, and kernel K-means for kernelized similarity based clustering algorithms. For example,BCE-kernelized (linear) denotes the setting that uses the BCE loss, kernelized similarity and thelinear activation function. The highest accuracy score in each study is in boldface.
Table 2: Classification accuracy of Algorithm 1 and other methods on the CIFAR-10 dataset.
Table 3: Comparison of different architectures for the local elasticity based clustering algorithm.
Table 4: Comparison with other feature extraction methods. For simplicity, we only consider CNNwith `2 loss on MNIST. The features from autoencoder and ResNet-152 are clustered by K-means.
Table 5: Comparison of two different settings, random and optimal (default), for parameter initial-ization. For simplicity, we only consider `2 loss on MNIST here.
Table 6: Comparison of different auxiliary examples for 5 vs 8 on MNIST.
Table 7: Comparison between the kernelized similarity based methods and the normalized kernel-ized similarity based methods on MNIST.
Table 8: Comparison of the clustering algorithms without data normalization on MNIST. Note thatwe use data normalization by default.
