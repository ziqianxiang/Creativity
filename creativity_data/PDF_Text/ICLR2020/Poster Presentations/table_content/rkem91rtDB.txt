Table 1: Evaluating graph representation quality by classification and clustering tasksTwo downstream tasks, classification and clustering, are deployed to evaluate the quality of thelearned graph representations.
Table 3: Representation quality with differ-Table 2: Representation quality with differ-	ent walk lengthsent sampling numbers1050-5-10-10	-5	0	5(a) Samples = 501050-5-10-10	0	10(b) Samples = 100(c) Samples = 150Figure 3: t-SNE VisUalziation of the MUTAG representations with different sampling numbers(a) Walk length = 5	(b) Walk length = 10	(C) Walk length = 15	(d) Walk length = 20	(e) Walk length = 25
Table 2: Representation quality with differ-	ent walk lengthsent sampling numbers1050-5-10-10	-5	0	5(a) Samples = 501050-5-10-10	0	10(b) Samples = 100(c) Samples = 150Figure 3: t-SNE VisUalziation of the MUTAG representations with different sampling numbers(a) Walk length = 5	(b) Walk length = 10	(C) Walk length = 15	(d) Walk length = 20	(e) Walk length = 25Figure 4: t-SNE Visualziation of MUTAG representations with different walk lengths
Table 4: Graph representation quality comparison between identity and RBF kernel on MUTAGidentity kernels or commonly adopted kernels could be deployed in the component of embeddingsubgraph distributions. in our implementation, we utilize a multi-layer deep neural network toapproximate a feature mapping function, for kernels whose feature mapping function is difficult toobtain. Figure 5 shows the t-SNE visualization of learned graph representations based on identitykernel and RBF kernel. As shown in Table 4, SEED variants with different kernels for distributionembedding could distinguish different classes with similar performance on the MUTAG dataset.
Table 5: Representation evaluation based on classification and clustering down-stream tasksF DeepSet in the Component of Embedding DistributionsIn this section, we investigate whether DeepSet (Zaheer et al., 2017) is an effective technique fordistribution embedding. In particular, we employ DeepSet to replace the multi-layer neural net-work for feature mapping function approximation, and similarity values generated by MMD serve18Published as a conference paper at ICLR 2020as supervision signals to guide DeepSet training. In our experiments, we compare the SEED imple-mentation based on DeepSet with MMD (DeepSet in Table 5) with the SEED implementation basedon the identity kernel (Identity Kernel in Table 5). We also observe that the MMD does not havesignificant performance difference. The result confirms that DeepSet could be a strong candidate forthe component of Embedding subgraph distributions.
Table 6: The impact of node feature and earliest visit time in WEAVE based on MUTAG datasetIn this section, we investigate the impact of node features and earliest visit time in WEAVE. InTable 6, Only node feature means only node features in WEAVE are utilized for subgraph encoding(which is equivalent to vanilla random walks), only earliest visit time means only earliest visit timeinformation in WEAVE is used for subgraph encoding, and Node feature + earliest visit time meansboth information is employed. We evaluate the impact on the MUTAG dataset. As shown above,it is crucial to use both node feature and earliest visit time information in order to achieve the bestperformance. Interestingly, on the MUTAG dataset, We observe that clustering could be easier ifWe only consider earliest visit time information. On the MUTAG dataset, node features seem tobe noisy for the clustering task. As the clustering task is unsupervised, noisy node features couldnegatively impact its performance when both node features and earliest visit time information areconsidered.
Table 7: Representation evaluation based on classification and clustering down-stream tasksapproximation. As shown in Figure 8, when we range the number of WEAVE samples from 100 to2000, the Nystrom approximation scales better than the exact MMD evaluation.
