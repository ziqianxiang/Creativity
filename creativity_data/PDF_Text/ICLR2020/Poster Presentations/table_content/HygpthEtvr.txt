Table 2: CNN Settingsparameter	valuedata set number of convolution layers size of convolution kernels number of output filters in convolution layers 1-2, 3-4, 5-6 operations after convolution layers 1-2, 3-4, 5-6 kernel size, stride, padding of maxing pooling activation function for convolution/output layer loss function and regularization function	CIFAR-10 6 3×3 32, 64,128 max pooling, dropout (rate=0.2) 2×2, 2, valid elu/softmax cross entropy and 'ι-normFollowing the parameter configurations of ADAM in Kingma & Ba (2015), AMSGrad in Reddiet al. (2018), and ADABound in Luo et al. (2019), we set ρ = 0.1, β = 0.999 and = 0.001 (seeTable 1), which are uniform for all the algorithms and commonly used in practice. Note that we havealso activated '1-regularization for these algorithms in the built-in function in TensorFlow/PyTorch,which amounts to adding the subgradient of the 'ι-norm to the gradient of the loss function. For theproposed ProxSGD, (t) and ρ(t) decrease over the iterations as follows,0.06	0.9e⑴=(t+4)O5，P⑴=(t+4)O5.	(20)Recall that the `1 -norm in the approximation subproblem naturally leads to the soft-thresholdingproximal mapping, see (10). The regularization parameter μ in the soft-thresholding then permitscontrolling the sparsity of the parameter variable x; in this experiment we set μ = 5 ∙ 10-5.
Table 3: DNN Settingsparameter	Valuedataset	MNISTnumber of hidden layers	6number of nodes per hidden layer	200activation function in hidden/output layer	tanh/softmaxloss function	cross entropyAfter customizing Algorithm 1 to problem (3), the approximation subproblem is(X - X(U)Tvx⑴ + 2(X - Mt))Tdiag(Tx(U)(X - x(t))	[x ,a	蕾a≤1 j +(a - a(t))Tva(t) + 1 (a - a(t))Tdiag(τa(t))(a - a(t)) ʃ .
