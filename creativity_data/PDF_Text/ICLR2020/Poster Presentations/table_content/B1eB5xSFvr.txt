Table 1: diffmPm performance comparison on an NVIDIA GTX 1080 Ti GPU. We benchmark in 2Dusing 6.4K particles. For the lines of code, we only include the essential implementation, excludingboilerplate code. [Reproduce: python3 diffmpm_benchmark.py]Approach	Forward Time	Backward Time	Total Time	# Lines of CodeTensorFlow	13.20 ms	35.70 ms	48.90 ms (188.×)	190CUDA	0.10 ms	0.14 ms	0.24 ms (0.92×)	460DiffTaichi	0.11 ms	0.15 ms	0.26 ms (1.00×)	1104.2	Differentiable Incompressible Fluid Simulator [smoke]We implemented a smoke simulator (Fig. 1, smoke) with semi-Lagrangian advection (Stam, 1999)and implicit pressure projection, following the example in Autograd (Maclaurin et al., 2015). Usinggradient descent optimization on the initial velocity field, we are able to find a velocity field thatchanges the pattern of the fluid to a target image (Fig. 7a in Appendix). We compare the performanceof our system against PyTorch, Autograd, and JAX in Table 2. Note that as an example from the6Published as a conference paper at ICLR 2020Table 2: smoke benchmark against Autograd, PyTorch, and JAX. We used a 110 × 110grid and 100 time steps, each with 6 Jacobi pressure projections. [Reproduce: python3smoke_[autograd/pytorch/jax/taichi_cpu/taichi_gpu].py]. Note that the Autograd program usesfloat64 precision, which approximately doubles the run time.
Table 2: smoke benchmark against Autograd, PyTorch, and JAX. We used a 110 × 110grid and 100 time steps, each with 6 Jacobi pressure projections. [Reproduce: python3smoke_[autograd/pytorch/jax/taichi_cpu/taichi_gpu].py]. Note that the Autograd program usesfloat64 precision, which approximately doubles the run time.
Table 3: Comparisons between DiffTaichi and other differentiable programming tools. Note thatthis table only discusses features related to differentiable physical simulation, and the other toolsmay not have been designed for this purpose. For example, PyTorch and TensorFlow are designedfor classical deep learning tasks and have proven successful in their target domains. Also note thatthe XLA backend of TensorFlow and JIT feature of PyTorch allow them to fuse operators to someextent, but for simulation we want complete operator fusion within megakernels. “Swift” AD (Weiet al., 2019) is partially implemented as of November 2019. “Julia” refers to Innes et al. (2019).
