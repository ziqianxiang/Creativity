Table 1: Benchmarking results of adverserial training of three networks. PGD-40 denotes runningthe projected gradient descent attacker (Madry et al., 2018) for 40 iterations. We set the perturbationsize as 0.3 for MNIST and 8/255 for CIFAR-10 in '∞ norm (adopted by all following experiments).
Table 2: The performance of RDI-SmallCNN. The”Average MFlops” is calculated by averaging thetotal flop costs consumed over the inference of the entire set (different samples take different FLOPsdue to input-adaptive inference). The perturbation size and step size are 0.3 and 0.01, respectively.
Table 3: The performance evaluation on RDI-ResNet38. The perturbation size and step size are8/255 and 2/255, respectively.
Table 4: The performance evaluation on RDI-MobilenetV2. The perturbation size and step size are8/255 and 2/255, respectively.
Table 5: Performance comparison between RDI-ResNet38 and ATMC.
Table 6: Performance on RDI-ResNet38 against random attack. The pertUrbation size and step sizeare 8/255 and 2/255, respectively. More details of random attack can be referenced in Appendix D.
Table 7: Performance on RDI-ResNet38 (defended with PGD) against FGSM attack (pertUrbationsize is 8/255). The original defended ResNet38 by PGD Under the same attack has ATA 51.11%.
Table 8: Performance on RDI-ResNet38 (defended with PGD) against WRM attack (pertUrbationsize is 0.3). The original defended ResNet38 by PGD Under the same attack has ATA 83.35%.
Table 9: The performance evaluation on RDI-ResNet38 (defended with PGD) against FGSM attack.
Table 10: The performance evaluation on RDI-ResNet38 (defended with PGD) against WRM attack.
