Published as a conference paper at ICLR 2020
Ridge	Regression:	Structure,	Cross-
Validation, and Sketching
Sifan Liu
Department of Statistics
Stanford University
Stanford, CA 94305, USA
sfliu@stanford.edu
Edgar Dobriban
Department of Statistics
University of Pennsylvania
Philadelphia, PA 19104, USA
dobriban@wharton.upenn.edu
Ab stract
We study the following three fundamental problems about ridge regression: (1)
what is the structure of the estimator? (2) how to correctly use cross-validation
to choose the regularization parameter? and (3) how to accelerate computation
without losing too much accuracy? We consider the three problems in a unified
large-data linear model. We give a precise representation of ridge regression as
a covariance matrix-dependent linear combination of the true parameter and the
noise. We study the bias of K-fold cross-validation for choosing the regulariza-
tion parameter, and propose a simple bias-correction. We analyze the accuracy
of primal and dual sketching for ridge regression, showing they are surprisingly
accurate. Our results are illustrated by simulations and by analyzing empirical
data.
1	Introduction
Ridge or '2-regularized regression is a widely used method for prediction and estimation when the
data dimension p is large compared to the number of datapoints n. This is especially so in problems
with many good features, where sparsity assumptions may not be justified. A great deal is known
about ridge regression. It is Bayes optimal for any quadratic loss in a Bayesian linear model where
the parameters and noise are Gaussian. The asymptotic properties of ridge have been widely studied
(e.g., Tulino & Verdu, 2004; Serdobolskii, 2007; Couillet & Debbah, 2011; Dicker, 2016; Dobriban
& Wager, 2018, etc). For choosing the regularization parameter in practice, cross-validation (CV)
is widely used. In addition, there is an exact shortcut (e.g., Hastie et al., 2009, p. 243), which has
good consistency properties (Hastie et al., 2019). There is also a lot of work on fast approximate
algorithms for ridge, e.g., using sketching methods (e.g., el Alaoui & Mahoney, 2015; Chen et al.,
2015; Wang et al., 2018; Chowdhury et al., 2018, among others).
Here we seek to develop a deeper understanding of ridge regression, going beyond existing work in
multiple aspects. We work in linear models under a popular asymptotic regime where n, p → ∞ at
the same rate (Marchenko & Pastur, 1967; Serdobolskii, 2007; Couillet & Debbah, 2011; Yao et al.,
2015). In this framework, we develop a fundamental representation for ridge regression, which
shows that it is well approximated by a linear scaling of the true parameters perturbed by noise. The
scaling matrices are functions of the population-level covariance of the features. As a consequence,
we derive formulas for the training error and bias-variance tradeoff of ridge.
Second, we study commonly used methods for choosing the regularization parameter. Inspired by
the observation that CV has a bias for estimating the error rate (e.g., Hastie et al., 2009, p. 243), we
study the bias of CV for selecting the regularization parameter. We discover a surprisingly simple
form for the bias, and propose a downward scaling bias correction procedure. Third, we study the
accuracy loss of a class of randomized sketching algorithms for ridge regression. These algorithms
approximate the sample covariance matrix by sketching or random projection. We show they can
be surprisingly accurate, e.g., they can sometimes cut computational cost in half, only incurring
5% extra error. Even more, they can sometimes improve the MSE if a suboptimal regularization
parameter is originally used.
1
Published as a conference paper at ICLR 2020
Our work leverages recent results from asymptotic random matrix theory and free probability theory.
One challenge in our analysis is to find the limit of the trace tr (∑ι + Σ-1)-1∕p, where ∑ι and ∑2
are p × p independent sample covariance matrices of Gaussian random vectors. The calculation
requires nontrivial aspects of freely additive convolutions (e.g., Voiculescu et al., 1992; Nica &
Speicher, 2006).
Our work is connected to prior works on ridge regression in high-dimensional statistics (Serdobol-
skii, 2007) and wireless communications (TUIino & Verdu, 2004; Couillet & Debbah, 2011). Among
other related works, El Karoui & Kosters (2011) discuss the implications of the geometric sensitivity
of random matrix theory for ridge regression, without considering our problems. El Karoui (2018)
and Dicker (2016) study ridge regression estimators, but focus only on the risk for identity covari-
ance. Hastie et al. (2019) study “ridgeless” regression, where the regularization parameter tends to
zero.
Sketching is an increasingly popular research topic, see Vempala (2005); Halko et al. (2011); Ma-
honey (2011); Woodruff (2014); Drineas & Mahoney (2017) and references therein. For sketched
ridge regression, Zhang et al. (2013a;b) study the dual problem in a complementary finite-sample
setting, and their results are hard to compare. Chen et al. (2015) propose an algorithm combining
sparse embedding and the subsampled randomized Hadamard transform (SRHT), proving relative
approximation bounds. Wang et al. (2017) study iterative sketching algorithms from an optimization
point of view, for both the primal and the dual problems. Dobriban & Liu (2018) study sketching
using asymptotic random matrix theory, but only for unregularized linear regression. Chowdhury
et al. (2018) propose a data-dependent algorithm in light of the ridge leverage scores. Other related
works include Sarlos (2006); Ailon & Chazelle (2006); Drineas et al. (2006; 2011); Dhillon et al.
(2013); Ma et al. (2015); Raskutti & Mahoney (2016); Gonen et al. (2016); Thanei et al. (2017);
Ahfock et al. (2017); Lopes et al. (2018); Huang (2018).
The structure of the paper is as follows: We state our results on representation, risk, and bias-
variance tradeoff in Section 2. We study the bias of cross-validation for choosing the regularization
parameter in Section 3. We study the accuracy of randomized primal and dual sketching for both
orthogonal and Gaussian sketches in Section 4. We provide proofs and additional simulations in the
Appendix. Code reproducing the experiments in the paper are available at https://github.
com/liusf15/RidgeRegression.
2	Ridge regression
We work in the usual linear regression model Y = Xβ + ε, where each row xi of X ∈ Rn×p is a
datapoint in p dimensions, and so there are p features. The corresponding element yi of Y ∈ Rn
is its continous response (or outcome). We assume mean zero uncorrelated noise, so Eε = 0, and
Cov [ε] = σ2In. We estimate the coefficient β ∈ Rp by ridge regression, solving the optimization
problem
β = arg β∈∈Rp11IY - Xβk2 + λkβ k2,
where λ > 0 is a regularization parameter. The solution has the closed form
β = (X >X∕n + λIp)-1 X >Y∕n.	(1)
We work in a ”big data” asymptotic limit, where both the dimension p and the sample size n tend
to infinity, and their aspect ratio converges to a constant, p∕n → γ ∈ (0, ∞). Our results can be
interpreted for any n and p, using γ = p∕n as an approximation.
We recall that the empirical spectral distribution (ESD) ofap ×p symmetric matrix Σ is the distribu-
tion P PP=ι δλi where λi,i = 1,...,p are the eigenvalues of Σ, and δχ is the point mass at x. This
is a standard notion in random matrix theory, see e.g., Marchenko & Pastur (1967); Tulino & Verdu
(2004); Couillet & Debbah (2011); Yao et al. (2015). The ESD is a convenient tool to summarize all
information obtainable from the eigenvalues of a matrix. For instance, the trace of Σ is proportional
to the mean of the distribution, while the condition number is related to the range of the support.
As is common, we will work in models where there is a sequence of covariance matrices Σ = Σp ,
and their ESDs converges in distribution to a limiting probability distribution. The results become
simpler, because they depend only on the limit.
2
Published as a conference paper at ICLR 2020
By extension, we say that the ESD of the n × p matrix X is the ESD of X>X/n. We will consider
some very specific models for the data, assuming it is of the form X = UΣ1/2, where U has iid
entries of zero mean and unit variance. This means that the datapoints, i.e., the rows ofX, have the
form Xi = Σ^2ui, i = 1,...,p, where Ui have iid entries. Then Σ is the ”true” covariance matrix
of the features, which is typically not observed. These types of models for the data are very common
in random matrix theory, see the references mentioned above.
Under these models, it is possible to characterize precisely the deviations between the empirical
covariance matrix Σb = n-1X>X and the population covariance matrix Σ, dating back to the well
known classical Marchenko-Pastur law for eigenvectors (Marchenko & Pastur, 1967), extended to
more general models and made more precise, including results for eigenvectors (see e.g., Tulino &
Verdu, 2004; Couillet & Debbah, 2011; Yao et al., 2015, and references therein). This has been
used to study methods for estimating the true covariance matrix, with several applications (e.g.,
Paul & Aue, 2014; Bun et al., 2017). More recently, such models have been used to study high
dimensional statistical learning problems, including classification and regression (e.g., Zollanvari &
Genton, 2013; Dobriban & Wager, 2018). Our work falls in this line.
We start by finding a precise representation of the ridge estimator. For random vectors un , vn of
growing dimension, we say un and vn are deterministic equivalents, if for any sequence of fixed
(or random and independent ofun, vn) vectors wn such that lim sup kwnk2 < ∞ almost surely, we
have |wn>(un - vn)| → 0 almost surely. We denote this by un vn. Thus linear combinations of
un are well approximated by those of vn . This is a somewhat non-standard definition, but it turns
out that it is precisely the one we need to use prior results from random matrix theory such as from
(Rubio & Mestre, 2011).
We extend scalar functions f : R → R to matrices in the usual way by functional calculus, applying
them to the eigenvalues and keeping the eigenvectors. If M = V ΛV > is a spectral decomposition
ofM, then we define f(M) := Vf(Λ)V >, where f(Λ) is the diagonal matrix with entries f (Λii).
For a fixed design matrix X, we can write the estimator as
β=(∑+λip)-1∑ β+(∑+λip)-1 Xn>ε.
However, for a random design, we can find a representation that depends on the true covariance Σ,
which may be simpler when Σ is simple, e.g., when Σ = Ip is isotropic.
Theorem 2.1 (Representation of ridge estimator). Suppose the data matrix has the form X =
U Σ1/2, where U ∈ Rn×p has iid entries ofzero mean, unit variance and finite 8 + C -th moment for
some c > 0, and Σ = Σp ∈ Rp×p is a deterministic positive definite matrix. Suppose that n, p → ∞
with p/n → γ > 0. Suppose the ESD ofthe sequence ofΣs converges in distribution to a probability
measure with compact support bounded away from the origin. Suppose that the noise is Gaussian,
and that β = βp is an arbitrary sequence of deterministic vectors, such that lim sup kβ k2 < ∞.
Then the ridge regression estimator is asymptotically equivalent to a random vector with the follow-
ing representation:
Z
β(λ) X A(Σ,λ) ∙ β + B(Σ,λ) ∙ σ ∙ i/?.
Here Z 〜 N(0, Ip) is a random vector that is stochastically dependent only on the noise ε, and
A, B are deterministic matrices defined by applying the scalar functions below to Σ:
A(x, λ) = (cpx + λ)-2 (cp + c0p)x,	B(x, λ) = (cpx + λ)-1 cp x.
Here cp := c(n, p, Σ, λ) is the unique positive solution of the fixed point equation
1	- Cp = Cp tr [Σ(cp∑ + λI)-1] .	(2)
This result gives a precise representation of the ridge regression estimator. It is a sum of two terms:
the true coefficient vector β scaled by the matrix A(Σ, λ), and the noise vector Z scaled by the matrix
B(Σ, λ). The first term captures to what extent ridge regression recovers the ”signal”. Morever, the
3
Published as a conference paper at ICLR 2020
Figure 1: Ridge regression bias-variance tradeoff. Left: γ = p/n = 0.2; right: γ = 2. The data
matrix X has iid Gaussian entries. The coefficient β has distribution β 〜 N(0,Ip∕p), while the
noise ε 〜N(0,Ip).
noise term Z is directly coupled with the noise in the original regression problem, and thus also the
estimator. The result would not hold for an independent noise vector Z.
However, the coefficients are not fully explicit, as they depend on the unknown population covari-
ance matrix Σ, as well as on the fixed-point variable cp .
Some comments are in order:
1.	Structure of the proof. The proof is quite non-elementary and relies on random matrix
theory. Specifically, it uses the language of the recently developed ”calculus of determin-
istic equivalents” (Dobriban & Sheng, 2018), and results by (Rubio & Mestre, 2011). A
general takeaway is that for n not much larger than p, the empirical covariance matrix Σ
is not a good estimator of the true covariance matrix Σ. However, the deviation of linear
functionals of Σ, can be quantified. In particular, we have
(Σb + λI)-1	(cpΣ + λI)-1,
in the sense that linear combinations of the entries of the two matrices are close (see the
proof for more details).
2.	Understanding the resolvent bias factor cp . Thus, cp can be viewed as a resolvent bias
factor, which tells us by what factor Σ is multiplied when evaluating the resolvent (Σ +
λI)-1, and comparing it to its naive counterpart (Σ + λI)-1. It is known that cp is well
defined, and this follows by a simple monotonicity argument, see Hachem et al. (2007);
Rubio & Mestre (2011). Specifically, the left hand side of (2) is decreasing in cp, while the
right hand size is increasing in
Also c0p is the derivative of cp, when viewing it as a function of z := -λ. An explicit
expression is provided in the proof in Section A.1, but is not crucial right now.
Here we discuss some implications of this representation.
For uncorrelated features, Σ = Ip , A, B reduce to multiplication by scalars. Hence, each coor-
dinate of the ridge regression estimator is simply a scalar multiple of the corresponding coordinate
of β . One can use this to find the bias in each individual coordinate.
Training error and optimal regularization parameter. This theorem has implications for under-
standing the training error, and optimal regularization parameter of ridge regression. As it stands,
the theorem itself only characterizes the behavior og linear combinations of the coordinates of the
estimator. Thus, it can be directly applied to study the bias Eβ(λ) - β of the estimator. How-
ever, it cannot directly be used to study the variance; as that would require understanding quadratic
functionals of the estimator. This seems to require significant advances in random matrix theory,
going beyond the results of Rubio & Mestre (2011). However, we show below that with additional
assumptions on the structure of the parameter β, we can derive the MSE of the estimator in other
ways.
4
Published as a conference paper at ICLR 2020
We work in a random-effects model, where the p-dimensional regression parameter β is random,
each coefficient has zero mean Eβi = 0, and is normalized so that Varβi = α2 /p. This ensures that
the signal strength Ekβ k2 = α2 is fixed for any p. The asymptotically optimal λ in this setting is
always λ* = γσ2/α2 see e.g., TUIino & Verdu (2θ04); Dicker (2016); Dobriban & Wager (2018).
The ridge regression estimator with λ = pσ2/(nɑ2) is the posterior mean of β, when β and ε are
normal random variables.
For a distribution F, we define the quantities
θi ㈤=/ (x⅛y dFγ(X)，
(i = 1, 2, . . .). These are the moments of the resolvent and its derivatives (up to constants). We use
the following loss functions: mean squared estimation error: M(β) = Ekβ - βk22, and residual or
training error: R(β) = E [k] Y - Xβk2.
Theorem 2.2 (MSE and training error of ridge). Suppose β has iid entries with Eβi = 0, Var [βi] =
α2∕p, i = 1,...,p and β is independent of X and ε. Suppose X is an arbitrary n X P matrix
depending on n and p, and the ESD of X converges weakly to a deterministic distribution F as
n, p → ∞ and p/n → γ. Then the asymptotic MSE and residual error of the ridge regression
estimator β(λ) has the form
lim M(β(λ)) = α2λ2θ2 + γσ2[θ1 - λθ2],
n→∞
lim R(β(λ)) = α2λ2[θ1 - λθ2] + σ211 - γ(1 + λθ1 - λ2θ2)],
n→∞
(3)
(4)
Bias-variance tradeoff. Building on this, we can also study the bias-variance tradeoff of ridge
regression. Qualitatively, large λ leads to more regularization, and decreases the variance. However,
it also increases the bias. Our theory allows us to find the explicit formulas for the bias and variance
as a function ofλ. See Figure 1 for a plot and Sec. A.3 for the details. As far as we know, this is one
of the few examples of high-dimensional asymptotic problems where the precise form of the bias
and variance can be evaluated.
Bias-variance tradeoff at optimal λ* = γσ2∕α2. (see Figure 6) This can be viewed as the “pure”
effect of dimensionality on the problem, keeping all other parameters fixed, and has intriguing prop-
erties. The variance first increases, then decreases with γ. In the ”classical” low-dimensional case,
most of the risk is due to variance, while in the ”modern” high-dimensional case, most ofit is due to
bias. This is consistent with other phenomena in proportional-limit asymptotics, e.g., that the map
between population and sample eigenvalue distributions is asymptotically deterministic (Marchenko
& Pastur, 1967).
Future applications. This fundamental representation may have applications to important statistical
inference questions. For instance, inference on the regression coefficient β and the noise variance
σ2 are important and challenging problems. Can we use our representation to develop debiasing
techniques for this task? This will be interesting to explore in future work.
3 Cross-validation
How can we choose the regularization parameter? In practice, cross-validation (CV) is the most
popular approach. However, it is well known that CV has a bias for estimating the error rate,
because it uses a smaller number of samples than the full data size (e.g., Hastie et al., 2009, p.
243). In this section, we study related questions, proposing a bias-correction method for the optimal
regularization parameter. This is closely connected to the previous section, because it relies on the
same random-effects theoretical framework. In fact, our conclusions here are a direct consequence
of the properties of that framework.
Setup. Suppose we split the n datapoints (samples) into K equal-sized subsets, each containing
n0 = n/K samples. We use the k-th subset (Xk, Yk) as the validation set and the other K - 1
subsets (X-k, Y-k), with total sample size n1 = (K - 1)n/K as the training set. We find the ridge
5
Published as a conference paper at ICLR 2020
Figure 2: Left: Cross-validation on the Million Song Dataset (MSD, Bertin-Mahieux et al., 2011).
For the error bar, we take n = 1000, p = 90, K = 5, and average over 90 different sub-datasets.
For the test error, we train on 1000 training datapoints and fit on 9000 test datapoints. The debiased
λ reduces the test error by 0.00024, and the minimal test error is 0.8480. Right: Cross-validation on
the flights dataset Wickham (2018). For the error bar, we take n = 300, p = 21, K = 5, and average
over 180 different sub-datasets. For the test error, we train on 300 datapoints and fit on 27000 test
datapoints. The debiased λ reduces the test error by 0.0022, and the minimal test error is 0.1353.
•	, ∙	， A ∙
regression estimator β-k, i.e.
β-k (λ) = (X>k X-k + nιλIp)-1 X>k Y-k.
The expected cross-validation error is, for isotropic covariance, i.e., Σ = I,
CV (λ) = ECdV (λ) = E
1K
K E kYk - Xke-k (X)k2/n0
σ2 + E hkβ-k - βk2i .
Bias in CV. When n, p tend to infinity so that p/n → γ > 0, and in the random effects model with
Eβi = 0, Varei = α2∕p described above, the minimizer of CV(λ) tends to λk = γσ2∕α2, where
Y is the limiting aspect ratio of X-k, i.e. Y = γK∕(K 一 1). Since the aspect ratios of X-k and
X differ, the limiting minimizer of the cross-validation estimator of the test error is biased for the
limiting minimizer of the actual test error, which is λ* = γσ2∕α2.
Bias-correction. Suppose We have found λk, the minimizer of CV(λ). Afterwards, We usually refit
ridge regression on the entire dataset, i.e., find
^	^ . .	~Γ	^ ,	-i ~Γ
β(^*) = (X τX + λ*nI )-1X τY.
Based on our bias calculation, we propose to use a bias-corrected parameter
^ , ^
λ* := ^
*
k
K 一 1
K
So if we use 5 folds, we should multiply the CV-optimal λ by 0.8. We find it surprising that
this theoretically justified bias-correction does not depend on any unknown parameters, such as
β, α2, σ2.While the bias of CV is widely known, we are not aware that this bias-correction for the
regularization parameter has been proposed before.
Numerical examples. Figure 2 shows on two empirical data examples that the debiased estima-
tor gets closer to the optimal λ than the original minimizer of the CV. However, in this case it
does not significantly improve the test error. Simulation results in Section A.4 also show that the
bias-correction correctly shrinks the regularization parameter and decreases the test error. We also
consider examples where p n (i.e., γ 1), because this is a setting where it is known that the
bias of CV can be large (Tibshirani & Tibshirani, 2009). However, in this case, we do not see a
significant improvement.
Extensions. The same bias-correction idea also applies to train-test validation. In addition, there
is a special fast “short-cut” for leave-one-out cross-validation in ridge regression (e.g., Hastie et al.,
6
Published as a conference paper at ICLR 2020
2009, p. 243), which has the same cost as one ridge regression. The minimizer converges to λ*
(Hastie et al., 2019). However, we think that the bias-correction idea is still valuable, as the idea
applies beyond ridge regression: CV selects regularization parameters that are too large. See Sec-
tion A.5 for more details and experiments comparing different ways of choosing the regularization
parameter.
4 S ketching
A final important question about ridge regression is how to compute it in practice. In this section, we
study that problem in the same high-dimensional model used throughout our paper. The computa-
tion complexity of ridge regression, O(np min(n, p)), can be intractable in modern large-scale data
analysis. Sketching is a popular approach to reducing the time complexity by reducing the sample
size and/or dimension, usually by random projection or sampling (e.g. Mahoney, 2011; Woodruff,
2014; Drineas & Mahoney, 2016). Specifically, primal sketching approximates the sample covari-
ance matrix X>X/n by X>L>LX/n, where L is an m × n sketching matrix, and m < n. If L is
chosen as a suitable random matrix, then this can still approximate the original sample covariance
matrix. Then the primal sketched ridge regression estimator is
βp = (X >L>LX∕n + λIp)-1 X >Y/n.	(5)
Dual sketching reduces p instead. An equivalent expression for ridge regression is β =
n-1X> XX>/n + λIn - Y. Dual sketched ridge regression reduces the computation cost of
the Gram matrix XX>, approximating it by XRR>X> for another sketching matrix R ∈ Rp×d
(d < p), so
βd = X> (XRRTX>∕n + λIn)-1 Y∕n.	(6)
The sketching matrices R and L are usually chosen as random matrices with iid entries (e.g., Gaus-
sian ones) or as orthogonal matrices. In this section, we study the asymptotic MSE for both or-
thogonal (Section 4.1) and Gaussian sketching (Section 4.2). We also mention full sketching, which
performs ridge after projecting down both X and Y. In section A.11, we find its MSE. However,
the other two methods have better tradeoffs, and we can empirically get better results for the same
computational cost.
4.1	Orthogonal sketching
First we consider primal sketching with orthogonal projections. These can be implemented by
subsampling, Haar distributed matrices, or subsampled randomized Hadamard transforms (Sarlos,
2006). We recall that the standard Marchenko-Pastur (MP) law is the probability distribution which
is the limit of the ESD of X T X∕n, when the n × p matrix X has iid standard Gaussian entries, and
n, p → ∞ so that p∕n → γ > 0, which has an explicit density (Marchenko & Pastur, 1967; Bai &
Silverstein, 2010).
Theorem 4.1 (Primal orthogonal sketching). Suppose β has iid entries with Eβi = 0, Var [βi]
= α2 ∕p, i = 1, . . . ,p and β is independent ofX and ε. Suppose X has iid standard normal entries.
We compute primal sketched ridge regression (5) with an m × n orthogonal matrix L (m < n,
LLT = Im). Let n,p and m tend to infinity with p∕n → γ ∈ (0, ∞) and m∕n → ξ ∈ (0, 1). Then
the MSE of βp (λ) has the limit
J(λ + ξ - 1)2 + γ(1 - ξ)] θ2 (ξ, ξ)	9ξθl (ξ, ξ) - (λ + ξ - 1)θ2 (ξ, ξ)
M (λ) = α2-2+ γσ2	-2,
ξ2	ξ2
(7)
where θi(γ, λ) =	(x + λ)-i dFγ (x) and Fγ is the standard Marchenko-Pastur law with aspect
ratio γ.
Structure of the proof. The proof is in Section A.6, with explicit formulas in Section A.6.1. The
θi are related to the resolvent of the MP law and its derivatives. In the proof, we decompose the
7
Published as a conference paper at ICLR 2020
≡⅛ssnslJJsw
Primal sketching MSE
Primal sketching bias and variance
Figure 3: Primal orthogonal sketching with n = 500, γ = 5, λ = 1.5, α = 3, σ = 1. Left:
MSE of primal sketching normalized by the MSE of ridge regression. The error bar is the standard
deviation over 10 repetitions. Right: Bias and variance of primal sketching normalized by the bias
and variance of ridge regression, respectively.
MSE as the sum of variance and squared bias, both of which further reduce to the traces of certain
random matrices, whose limits are determined by the MP law Fγ and λ. The two terms on the
RHS of Equation (7) are the limits of squared bias and variance, respectively. There is an additional
key step in the proof, which introduces the orthogonal complement L1 of the matrix L such that
L>L + L1>L1 = In, which leads to some Gaussian random variables appearing in the proof, and
simplifies calculations.
Simulations. A simulation in Figure 3 (left) shows a good match with our theory. It also shows
that sketching does not increase the MSE too much. In this case, by reducing the sample size to half
the original one, we only increase the MSE by a factor of 1.05. This shows sketching can be very
effective. We also see in Figure 3 (right) that variance is compromised much more than bias.
Robustness to tuning parameter. The reader may wonder how strongly this depends on the choice
of the regularization parameter λ. Perhaps ridge regression works poorly with this λ, so sketching
cannot worsen it too much? What happens if we take the optimal λ instead of a fixed one? In
experiments in Section A.12 we show that the behavior is quite robust to the choice of regularization
parameter.
The next theorem states a result for dual orthogonal sketching.
Theorem 4.2 (Dual orthogonal sketching). Under the conditions of Theorem 4.1, we compute the
dual sketched ridge regression with an orthogonal p × d sketching matrix R (d 6 p, R>R = Id).
Let n,p and d go to infinity with p/n → γ ∈ (0, ∞) and d/n → ζ ∈ (0, γ). Then the MSE of βd (λ)
has the limit
2
^γ [γ - 1 + (λ - Y + Z)2θ2(Z, λ) + (γ - ζW2(Z, λ)] + σ2 ]&I(Z, λ) - (λ + Z - Y]θ2(ζ, λ)] ,
where &(Z, λ) = (1 一 Z)∕λi + Z J(X + λ)-ldFζ(x), and FZ is the standardMarchenko-Pastur law.
Proof structure and simulations. The proof in Section A.7 follows similar path to the previous one.
Here Gi comes in because of the companion Stieltjes transform of MP law. The simulation results
shown in Figure 11 agrees well with our theory. They are similar to the ones before: sketching has
favorable properties, and the bias increases less than the variance.
Optimal tuning parameters. For both primal and dual sketching, the optimal regularization pa-
rameter minimizing the MSE seems analytically intractable. Instead, we use a numerical approach
in our experiments, based on a binary search. Since this is one-dimensional problem, there are no
numerical issues. See Figure 13 in Section A.12.3.
8
Published as a conference paper at ICLR 2020
Auu ① Pte①①.≥4Ea0:
Figure 4: Left: Ratio of optimal MSE of marginal regression to that of optimally tuned ridge regres-
sion, for three values of Y = p/n, as a function of the SNR ɑ2∕σ2. Right: Gaussian dual sketch
when there is no noise. γ = 0.4, α= 1, λ = 1 (both for original and sketching). Standard error
over 50 experiments.
4.1.1 Extreme projection — marginal regression
It is of special interest to investigate extreme projections, where the sketching dimension is much
reduced compared to the sample size, so m n. This corresponds to ξ = 0. This can also be
viewed as a scaled marginal regression estimator, i.e., β 叱 X > Y. For dual sketching, the same case
can be recovered with ζ = 0. Another interest of studying this special case is that the formula for
MSE simplifies a lot.
Theorem 4.3 (Marginal regression). Under the same assumption as Theorem 4.1, let ξ = 0.
Then the form of the MSE is M(λ) = [α2 [(λ - 1)2 + γ] + σ2γ]∕λ2. MoreoVer the op-
timal λ* that minimizes this equals γσ2∕a2 + 1 + Y and the optimal MSE is M(λ*) =
α2(1 - α2∕[α2(1 + γ) + γσ2]).
The proof is in Section A.8. When is the optimal MSE of marginal regression small? Compared to
the MSE of the zero estimator α2, it is small when Y(σ2∕α2 + 1) + 1 is large. In Figure 4 (left),
we compare marginal and ridge regression for different aspect ratios and SNR. When the signal to
noise ratio (SNR) α2∕σ2 is small or the aspect ratio Y is large, marginal regression does not increase
the MSE much. As a concrete example, if we take α2 = σ 2 = 1 and Y = 0.7, the marginal MSE is
1 - 1∕2.4 ≈ 0.58. The optimal ridge MSE is about 0.52, so their ratio is only ca. 0.58∕0.52 ≈ 1.1.
It seems quite surprising that a simple-minded method like marginal regression can work so well.
However, the reason is that when the SNR is small, we cannot expect ridge regression to have good
performance. Large Y can also be interpreted as small SNR, where ridge regression works poorly
and sketching does not harm performance too much.
4.2 Gaussian sketching
In this section, we study Gaussian sketching. The following theorem states the bias of dual Gaussian
sketching. The bias is enough to characterize the performance in the high SNR regime where α∕σ →
∞, and we discuss the extension to low SNR after the proof.
Theorem 4.4 (Bias of dual Gaussian sketch). Suppose X is an n × p standard Gaussian random
matrix. Suppose also that R is a p × d matrix with i.i.d. N(0, 1∕d) entries. Then the bias of dual
sketch has the expression Bias2 (βd) = ɑ2 + α2 ∕γ ∙ [m0(z) — 2m(z)] ∣z=o, where m is a function
described below, and m0(z) denotes the derivative of m w.r.t. z. Below, we use the branch of the
square root with positive imaginary part.
The function m is characterized by its inverse function, which has the explicit formula m-1(z) =
1∕[1 + z∕Z] — [γ + 1 —，(Y — 1)2 + 4λz]∕(2z) for COmPleX Z with positive imaginary part.
9
Published as a conference paper at ICLR 2020
About the proof. The proof is in Section A.9.We mention that the same result holds when the
matrices involved have iid non-Gaussian entries, but the proof is more technical. The current proof
is based on free probability theory (e.g., Voiculescu et al., 1992; Hiai & Petz, 2006; Couillet &
Debbah, 2011). The function m is the Stieltjes transform of the free additive convolution of a
standard MP law F∖∕ξ and a scaled inverse MP law λ∕γ ∙ F-Y (See the proof).
Numerics. To evaluate the formula, we note that m-1(m(0)) = 0, so m(0) is a root of m-1.
Also, dm(0)/dz equals 1/(dm-1(y)/dy|y=m(0)), the reciprocal of the derivative of m-1 evaluated
at m(0). We use binary search to find the numerical solution. The theoretical result agrees with the
simulation quite well, see Figure 4.
Somewhat unexpectedly, the MSE of dual sketching can be below the MSE of ridge regression, see
Figure 4. This can happen when the original regularization parameter is suboptimal. As d grows,
the MSE of Gaussian dual sketching converges to that of ridge regression.
We have also found the bias of primal Gaussian sketching. However, stating the result requires free
probability theory, and so we present it in the Appendix, see Theorem A.1. To further validate our
results, we present additional simulations in Sec. A.12, for both fixed and optimal regularization
parameters after sketching. A detailed study of the computational cost for sketching in Sec. A.13
concludes, as expected, that primal sketching can reduce cost when p < n, while dual sketching can
reduce it when p > n; and also provides a more detailed analysis.
Acknowledgments
The authors thank Ken Clarkson for helpful discussions and for providing the reference Chen et al.
(2015). ED was partially supported by NSF BIGDATA grant IIS 1837992. SL was partially sup-
ported by a Tsinghua University Summer Research award. A version of our manuscript is available
on arxiv at https://arxiv.org/abs/1910.02373.
References
Daniel Ahfock, William J Astle, and Sylvia Richardson. Statistical properties of sketching algorithms. arXiv
preprint arXiv:1706.03665, 2017.
Nir Ailon and Bernard Chazelle. Approximate nearest neighbors and the fast johnson-lindenstrauss transform.
In Proceedings of the thirty-eighth annual ACM symposium on Theory of computing, pp. 557-563. ACM,
2006.
Greg W Anderson, Alice Guionnet, and Ofer Zeitouni. An Introduction to Random Matrices. Number 118.
Cambridge University Press, 2010.
Theodore W Anderson. An Introduction to Multivariate Statistical Analysis. Wiley New York, 2003.
Zhidong Bai and Jack W Silverstein. Spectral analysis of large dimensional random matrices. Springer Series
in Statistics. Springer, New York, 2nd edition, 2010.
Thierry Bertin-Mahieux, Daniel P.W. Ellis, Brian Whitman, and Paul Lamere. The million song dataset. In
Proceedings of the 12th International Conference on Music Information Retrieval (ISMIR 2011), 2011.
Joel Bun, Jean-Philippe Bouchaud, and Marc Potters. Cleaning large correlation matrices: tools from random
matrix theory. Physics Reports, 666:1-109, 2017.
Shouyuan Chen, Yang Liu, Michael R Lyu, Irwin King, and Shengyu Zhang. Fast relative-error approximation
algorithm for ridge regression. In UAI, pp. 201-210, 2015.
Agniva Chowdhury, Jiasen Yang, and Petros Drineas. An iterative, sketching-based framework for ridge re-
gression. In International Conference on Machine Learning, pp. 988-997, 2018.
Romain Couillet and Merouane Debbah. Random Matrix Methods for Wireless Communications. Cambridge
University Press, 2011.
Paramveer Dhillon, Yichao Lu, Dean P Foster, and Lyle Ungar. New subsampling algorithms for fast least
squares regression. In Advances in neural information processing systems, pp. 360-368, 2013.
Lee H Dicker. Ridge regression and asymptotic minimax estimation over spheres of growing dimension.
Bernoulli, 22(1):1-37, 2016.
Edgar Dobriban and Sifan Liu. A new theory for sketching in linear regression. arXiv preprint
arXiv:1810.06089, NeurIPS 2019, 2018.
Edgar Dobriban and Yue Sheng. Distributed linear regression by averaging. arXiv preprint arxiv:1810.00412,
2018.
Edgar Dobriban and Yue Sheng. One-shot distributed ridge regression in high dimensions. arXiv preprint
arXiv:1903.09321, 2019.
Edgar Dobriban and Stefan Wager. High-dimensional asymptotics of prediction: Ridge regression and classifi-
cation. The Annals of Statistics, 46(1):247-279, 2018.
10
Published as a conference paper at ICLR 2020
Petros Drineas and Michael W Mahoney. RandNLA: randomized numerical linear algebra. Communications
oftheACM, 59(6):80-90, 2016.
Petros Drineas and Michael W Mahoney. Lectures on randomized numerical linear algebra. arXiv preprint
arXiv:1712.08880, 2017.
Petros Drineas, Michael W Mahoney, and S Muthukrishnan. Sampling algorithms for l 2 regression and ap-
plications. In Proceedings of the seventeenth annual ACM-SIAM symposium on Discrete algorithm, pp.
1127-1136. Society for Industrial and Applied Mathematics, 2006.
Petros Drineas, Michael W Mahoney, S Muthukrishnan, and Tamas SarlOs. Faster least squares approximation.
Numerische mathematik, 117(2):219-249, 2011.
Ahmed el Alaoui and Michael W Mahoney. Fast randomized kernel ridge regression with statistical guarantees.
In Advances in Neural Information Processing Systems, pp. 775-783, 2015.
Noureddine El Karoui. On the impact of predictor geometry on the performance on high-dimensional ridge-
regularized generalized robust regression estimators. Probability Theory and Related Fields, 170(1-2):95-
175, 2018.
Noureddine El Karoui and Holger Kosters. Geometric sensitivity of random matrix results: consequences for
shrinkage estimators of covariance and related statistical methods. arXiv preprint arXiv:1105.1404, 2011.
Alon Gonen, Francesco Orabona, and Shai Shalev-Shwartz. Solving ridge regression using sketched precondi-
tioned svrg. In International Conference on Machine Learning, pp. 1397-1405, 2016.
Walid Hachem, Philippe Loubaton, and Jamal Najim. Deterministic equivalents for certain functionals of large
random matrices. The Annals of Applied Probability, 17(3):875-930, 2007.
Nathan Halko, Per-Gunnar Martinsson, and Joel A Tropp. Finding structure with randomness: Probabilistic
algorithms for constructing approximate matrix decompositions. SIAM review, 53(2):217-288, 2011.
Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The elements of statistical learning. Springer series in
statistics, 2009.
Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani. Surprises in high-dimensional ridge-
less least squares interpolation. arXiv preprint arXiv:1903.08560, 2019.
Fumio Hiai and DeneS Petz. The semicircle law, free random variables and entropy. Number 77. American
Mathematical Soc., 2006.
Zengfeng Huang. Near optimal frequent directions for sketching dense and sparse matrices. In International
Conference on Machine Learning, pp. 2053-2062, 2018.
Miles E Lopes, Shusen Wang, and Michael W Mahoney. Error estimation for randomized least-squares algo-
rithms via the bootstrap. arXiv preprint arXiv:1803.08021, 2018.
Ping Ma, Michael W Mahoney, and Bin Yu. A statistical perspective on algorithmic leveraging. The Journal
of Machine Learning Research, 16(1):861-911, 2015.
Michael W Mahoney. Randomized algorithms for matrices and data. Foundations and TrendsR in Machine
Learning, 3(2):123-224, 2011.
Vladimir A Marchenko and Leonid A Pastur. Distribution of eigenvalues for some sets of random matrices.
Mat. Sb., 114(4):507-536, 1967.
Robb J Muirhead. Aspects of multivariate statistical theory, volume 197. John Wiley & Sons, 2009.
Alexandru Nica and Roland Speicher. Lectures on the combinatorics of free probability, volume 13. Cambridge
University Press, 2006.
Debashis Paul and Alexander Aue. Random matrix theory in statistics: A review. Journal of Statistical Planning
and Inference, 150:1-29, 2014.
Garvesh Raskutti and Michael W Mahoney. A statistical perspective on randomized sketching for ordinary
least-squares. The Journal of Machine Learning Research, 17(1):7508-7538, 2016.
Francisco Rubio and Xavier Mestre. Spectral convergence for a general class of random matrices. Statistics &
Probability Letters, 81(5):592-602, 2011.
Tamas Sarlos. Improved approximation algorithms for large matrices via random projections. In Foundations
of Computer Science, 2006. FOCS’06. 47th Annual IEEE Symposium on, pp. 143-152. IEEE, 2006.
Vadim Ivanovich Serdobolskii. Multiparametric Statistics. Elsevier, 2007.
Gian-Andrea Thanei, Christina Heinze, and Nicolai Meinshausen. Random projections for large-scale regres-
sion. In Big and complex data analysis, pp. 51-68. Springer, 2017.
Ryan J Tibshirani and Robert Tibshirani. A bias correction for the minimum error rate in cross-validation. The
Annals of Applied Statistics, pp. 822-829, 2009.
Antonio M Tulino and Sergio Verdu. Random matrix theory and wireless communications. Communications
and Information theory, 1(1):1-182, 2004.
Santosh S Vempala. The random projection method, volume 65. American Mathematical Soc., 2005.
Dan V Voiculescu, Ken J Dykema, and Alexandru Nica. Free random variables. Number 1. American Mathe-
matical Soc., 1992.
Jialei Wang, Jason D Lee, Mehrdad Mahdavi, Mladen Kolar, and Nathan Srebro. Sketching meets random
projection in the dual: A provable recovery algorithm for big and high-dimensional data. Electronic Journal
of Statistics, 11(2):4896-4944, 2017.
Shusen Wang, Alex Gittens, and Michael W Mahoney. Sketched ridge regression: Optimization perspective,
statistical perspective, and model averaging. Journal of Machine Learning Research, 18:1-50, 2018.
11
Published as a conference paper at ICLR 2020
Hadley Wickham. nycflights13: Flights that Departed NYC in 2013, 2018. URL https://CRAN.
R-project.org/package=nycflights13. R package version 1.0.0.
David P Woodruff. Sketching as a tool for numerical linear algebra. Foundations and TrendsR in Theoretical
Computer Science, 10(1-2):1-157, 2014.
Jianfeng Yao, Zhidong Bai, and Shurong Zheng. Large Sample Covariance Matrices and High-Dimensional
Data Analysis. Cambridge University Press, New York, 2015.
Lijun Zhang, Mehrdad Mahdavi, and Rong Jin. Linear convergence with condition number independent access
of full gradients. In Advances in Neural Information Processing Systems, pp. 980-988, 2013a.
Lijun Zhang, Mehrdad Mahdavi, Rong Jin, Tianbao Yang, and Shenghuo Zhu. Recovering the optimal solution
by dual random projection. In Conference on Learning Theory, pp. 135-157, 2013b.
Amin Zollanvari and Marc G Genton. On Kolmogorov asymptotics of estimators of the misclassification error
rate in linear discriminant analysis. Sankhya A, 75(2):300-326, 2013.
A Appendix
A.1 Proof of Theorem 2.1
If p/n → γ and the spectral distribution of Σ converges to H, we have by the general Marchenko-
Pastur (MP) theorem of Rubio and Mestre (Rubio & Mestre, 2011), that
(Σb + λI)-1	(cpΣ + λI)-1,
where cp := c(n, p, Σ, λ) is the unique positive solution of the fixed point equation
1 - Cp = cp tr [Σ(cpΣ + λI)-1].
Here, using the terminology of the calculus of deterministic equivalents (Dobriban & Sheng, 2018),
two sequences of (not necessarily symmetric) n × n matrices An , Bn of growing dimensions are
equivalent, and we write
An	Bn
if limn→∞ tr [Cn(An - Bn)] = 0 almost surely, for any sequence Cn of (not necessarily symmet-
ric) n × n deterministic matrices with bounded trace norm, i.e., such that lim sup kCnktr < ∞
(Dobriban & Sheng, 2018). Informally, linear combinations of the entries of An can be approxi-
mated by the entries of Bn .
We start with
β = (X IXln + λIp)-1 X IYln = (X IXln + λIp)-1 X T(Xle + ε)
X>ε
=(Σ + λIp ) Σ β + (Σ + λIp ) -------.
Then, by the general MP law written in the language of the calculus of deterministic equivalents
(Σb +λIp)-1Σb = Ip - λ(Σb +λIp)-1	Ip - λ(cpΣ + λI)-1 = cpΣ(cpΣ+λI)-1.
By the definition of equivalence for vectors,
(Σb +λIp)-1Σbβ	cpΣ(cpΣ+ λI)-1β.
1
We note a subtle point here. The rank of the matrix M := (Σ + λIp)-1Σ is at most n, and so it
is not a full rank matrix when n < p. In contrast, cpΣ(cpΣ + λI)-1 can be a full rank matrix.
Therefore, for the vectors β in the null space of Σ, which is also the null space of X, we certainly
have that the two sides are not equal. However, here we assumed that the matrix X is random, and
so its null space is a random max(p - n, 0) dimensional linear space. Therefore, for any fixed vector
β , the random matrix M will not contain it in its null space with high probability, and so there is no
contradiction.
We should also derive an asymptotic equivalent for
(Σ + λIp)-1 X>ε.
n
12
Published as a conference paper at ICLR 2020
Figure 5: Simulation for ridge regression. We take n = 1000, λ = 0.3. Also, X has iid N (0, 1)
entries, βi 〜俶 N(0, α2∕p), ε,〜血 N(0, σ2), with ɑ = 3,σ = 1. The standard deviations are over
50 repetitions. The theoretical lines are plotted according to Theorem 2.2. The MSE is normalized
by the norm of β .
Suppose we have Gaussian noise, and let Z 〜N(0, Ip). Then we can write
(ς+λIp)-1 W =d (bb+独厂无1/2 nZ2.
So the question reduces to finding a deterministic equivalent for h(Σ), where h(x) = (x + λ)-2x.
Note that
h(x) = (x + λ)-2x = (x + λ)-2(x + λ - λ) = (x + λ)-1 - λ(x + λ)-2.
By the calculus of determinstic equivalents: (Σb + λ)-1 (cpΣ + λI)-1. Moreover, fortunately
the limit of the second part was recently calculated in (Dobriban & Sheng, 2019). This used the
so-called ”differentiation rule” of the calculus of deterministic equivalents to find
(Σb + λ)-2	(cpΣ+λI)-2(I-c0pΣ).
The derivative c0p = dcp /dz has been found in Dobriban & Sheng (2019), in the proof of Theorem
3.1, part 2b. The result is (with γp = p/n, Hp the spectral distribution of Σ, and T a random variable
distributed according to Hp)
Cp =	YpEH ^T-ZT	.	(8)
-1 + YpzEHp (cpT-z)2
So, we find the final answer
(Σb +λIp)-1Σb 1/2	A(Σ, λ) := (cpΣ + λI)-1 - λ(cpΣ + λI)-2(I- c0pΣ).
A.2 Risk analysis
Figure 5 shows a simulation result. We see a good match between theory and simulation.
A.2.1 Proof of Theorem 2.2
_ ________ _ ____ ^ , - 一
Proof. The MSE of β has the form
Eke — βk2 = bias2 + δ2,
where
bias2 = E
δ2 = σ2E
(X>X/n + λIp)-1 X>X∕nβ - β^2
Il(X>X/n + λIp)-1 n-1X/；］.
13
Published as a conference paper at ICLR 2020
We assume that X has iid entries of zero mean and unit variance, and that Eβ = 0, Var [β] =
α2 *∕pIp. As p/n → Y as n goes to infinity, the ESD of 1X>X converges to the MP law FY. So We
have
bias2
→ α2 λ2∕ (X⅛ dFγ (X)
and
δ2 = σ2 E
n2
=σ2 E
n
→ σ2 γ
[tr[(X>X∕n + λIp)-2 X>X]]
[tr[(X>X∕n + λIp)-1 - λ (X>X∕n + XIp)-2]]
J'+lddF(X)- λ/(X⅛dFγ (X).
Denoting θi(γ, λ) = R (χ+1λ)idFY(χ),then
AMSE⑻=α2λ2θ2 + γσ2 [θι - λθ2].
(9)
For the standard Marchenko-Pastur law (i.e., when Σ = Ip), we have the explicit forms of θ1 and
θ2 . Specifically,
11
θ1 = J x+λ dFY (X) = - 2
2(1+λ)	2
λY	√ √γλz2
where
1
Z2 =------
2	2
(√γ +1√Yλ ) + ∕√γ +1√γλ )2 - 4
τ. .	1	..	. ..	1∙	..	r1.∙	1.∙	C	C 含 I J	c	z,,
It is known that the limiting Stieltjes transform mFγ := mY of Σ has the explicit form (Marchenko
& PaStUr,1967):	“____________________
(z + Y - 1) + p(z + Y - 1)2 - 4zγ
.
-2zY
As UsUal in the area, we Use the principal branch of the sqUare root of complex nUmbers. Hence
θ1
(-λ+γ-1) + ,(-λ+γ-1)2+4λγ
2λY
. Also
θ2(γ,χ) = j (X⅛ dFY (X) = -/ dλx⅛ dFγ(X)
d1
-dλθ1 =-铲 +
1 d z2
√γdλ λ
2
1 Y + 1 _ ɪ [	λ + Y + 1	_ J(√γ + √λ)2 - 4
铲 R - 2√y [Yλq(√Y + 告)2- 4	λ ]
For the residUal,
E 1kY - Xβk2∣X = α2λ21tr[(X>X∕n + λIp)-1 - λ (X>X∕n + λIp)-2]
+ σ21[tr(In) - 2tr (X>X∕n + λIp)-1 X>X∕n + tr ((X>X∕n + λIp)-1 X>X∕n)].
14
Published as a conference paper at ICLR 2020
Next,
E 1tr[((X>X/n + λIp)-1 X>X∕n)2] = E 1 tr[(lp - λ (X>X/n + λIp)-1)2]
→ 1 - 2λθ1 + λ2 3θ2.
Therefore
E 1 kY - Xβk2 →ɑ2λ2[θ1 - λθ2] + σ211 - 2γ(1 - λθ1) + γ(1 - 2λθ1 + λ2θ2)]
n2
=α2λ2[θ1 - λθ2] +σ2 11 -γ(1+λθ1 -λ2θ2) .
□
A.3 Bias-variance tradeoff
The limiting MSE decomposes into a limiting squared bias and variance. The specific forms of these
are
bias2 = α2∕ (X + λ)2 dFY(X),	Var = γσ2∕ (χ +xλ)2 dFY(X).
See Figure 1 for a plot. We can make several observations.
1.	The bias increases with λ, starting out at zero for λ = 0 (linear regression), and increasing
to α2 as λ → ∞ (zero estimator).
2.	The variance decreases with λ, from γσ2 X-1dFY(X) to zero.
3.	In the setting plotted in the figure, when α2 and σ 2 are roughly comparable, there are
additional qualitative properties we can investigate. When γ is small, the regularization
parameter λ influences the bias more strongly than the variance (i.e., the derivative of the
normalized quantities in the range plotted is generally larger for the normalized squared
bias). In contrast when γ is large, the variance is influenced more.
Next We consider how bias and variance change with Y at the optimal λ* = γσ2∕α2. This can be
viewed as the ”pure” effects of dimensionality on the problem, keeping all other parameters fixed.
Ineed, α2∕σ2 can be viewed as the Signal-to-noise ratio (SNR), and is fixed. This analysis allows
us to study for the best possible estimator (ridge regression, a Bayes estimator), behaves with the
dimension. We refer to Figure 6, where we make some specific choices of α and σ .
1. Clearly the overall risk increases, as the problem becomes harder with increasing dimen-
sion. This is in line with our intuition.
2. The classical bias-variance tradeoff can be summarized by the equation
bias2(λ) + var(λ) > M*(α,γ),
where we made explicit the dependence of the bias and variance on λ, and where M *(α,γ)
is the minimum MSE achievable, also known as the Bayes error, for which there are explicit
formulas available (Tulino & Verdu, 2004; Dobriban & Wager, 2018).
3. The variance first increases, then decreases with γ. This shows that in the ”classical”
low-dimensional case, most of the risk is due to variance, while in the ”modern” high-
dimensional case, most of it is due to bias. This observation is consistent with other phe-
nomena in proportional-limit asymptotics, for instance that the map between population
and sample eigenvalue distributions is asymptotically deterministic (Marchenko & Pastur,
1967; Bai & Silverstein, 2010).
A.4 Simulations with cross-validation
See Figure 7. We consider both small and large γ. Our bias-correction procedure shrinks the λ to
the correct direction and decreases the test error. It is also shown that the one-standard-error rule
(e.g., Hastie et al., 2009) does not perform well here.
15
Published as a conference paper at ICLR 2020
Figure 6: Bias-variance tradeoff at optimal λl = γσ2∕α2, when ɑ = 3,σ = 1.
」0」」a)S
」0」」①Ml S
Figure 7: Left: we generate a training set (n = 1000, p = 700, γ = 0.7, α = σ = 1) and a test set
(ntest = 500) from the same distribution. We split the training set into K = 5 equally sized folds
and do cross-validation. The blue error bars plot the mean and standard error of the K test errors.
The red dotted line indicates the ”one-standard-error” location. The green dashed line indicates
the optimal λCv obtained by k-fold cross-validation, while the red dashed-dotted line indicates the
debiased version KKI λCv. The orange line plots the test error when training on the whole training
set and fit on the whole test set, and the purple dashed-dotted line indicates the minimal Xlest. The
test error is 1.513 at XCv and 1.510 at KK1 λ∖v. So the bias-correction decreases the test error
by about 0.003. Right: we take n = 200, p = 1000, γ = 5, α = 3, σ = 1. The bias-correction
decreases the test error from 8.92 to 8.89, so it decreases by 0.03.
16
Published as a conference paper at ICLR 2020
A.5 Choosing the regularization parameter- additional details
Another possible prediction method is to use the average of the ridge estimators computed during
cross-validation. Here it is also natural to use the CV-optimal regularization parameters, averaging
O ^ ..
β-k (λk ),i∙e.
1K
βavg (Xk) = KEe-k O
k=1
This has the advantage that it does not require refitting the ridge regression estimator, and also that
we use the optimal regularization parameter.
A. 5.1 Train-test validation
The same bias in the regularization parameter also applies to train-test validation. Since the num-
ber of samples is changed when restricting to the training set, the optimal λ chosen by train-test
validation is also biased for the true regularization parameter minimizing the test error. We will
later see in simulations (Figure 8) that retraining the ridge regression estimator on the whole data
will still significantly improve the performance (this is expected based on our results on CV). For
prediction, here we can also use ridge regression on the training set. This effectively reduces sample
size n → ntrain, where ntrain is the sample size of the training set. However, if the training set
grows such that n/ntrain → 1 while ntrain → ∞, the train-test split has asymptotically optimal
performance.
A. 5.2 Leave-one-out
There is a special “short-cut” for leave-one-out in ridge regression, which saves us from burdensome
computation. Write loo(λ) for the leave-one-out estimator of prediction error with parameter λ.
Instead of doing ridge regression n times, we can calculate the error explicitly as
1n
loo(λ) = n X
Yi - X>β(λ)
1 - Sii(λ)
2
where S(λ) = X(X>X + nλI)-1X>. The minimizer of loo(λ) is asymptotically optimal, i.e.,
it converges to λ* (Hastie et al., 2019). However, the computational cost of this shortcut is the
same as that of a train-test split. Therefore, the method described above has the same asymptotic
performance.
Figure 8: Comparing different ways of doing cross-validation. We take n = 500, p = 550, α = 20,
σ = 1, K = 5. As for train-test validation, we take 80% of samples to be training set and the rest
20% be test set. The error bars are the mean and standard deviation over 20 repetitions.
Simulations: Figure 8 shows simulation results comparing different cross-validation methods:
1.	kf — k-fold cross-validation by taking the average of the ridge estimators at the CV-optimal
regularization parameter.
17
Published as a conference paper at ICLR 2020
2.	kf refit — k-fold cross-validation by refitting ridge regression on the whole dataset using
the CV-optimal regularization parameter.
3.	kf bic — k-fold cross-validation by refitting ridge regression on the whole dataset using the
CV-optimal regularization parameter, with bias correction.
4.	tt — train-test validation, by using the ridge estimator computed on the train data, at the
validation-optimal regularization parameter. Note: we expect this to be similar, but worse
than the ”kf” estimator.
5.	tt refit — train-test validation by refitting ridge regression on the whole dataset, using the
validation-optimal regularization parameter. Note: we expect this to be similar, but slightly
worse than the ”kf refit” estimator.
6.	tt bic — train-test validation by refitting ridge regression on the whole dataset using the
CV-optimal regularization parameter, with bias correction.
7.	loo — leave-one-out
Figure 8 shows that the naive estimators (kf and tt) can be quite inaccurate without refitting or bias
correction. However, if we either refit or bias-correct, the accuracy improves. In this case, there
seems to be no significant difference between the various methods.
A.6 Proof of Theorem 4.1
>λ /` n	/	ʌ	.	∙ r∙ ∙ . ι - A	1
Proof. Suppose m/n → ξ as n goes to infinity. For βp , we have
bias2 = E
Il(X>LLXln + λIp)-1 X>X∕nβ - β^22
δ2 = σ2E
(X >L>LX∕n + λIp)-1 n-1X >
Denote M = (X>L>LX∕n + λIp) 1, the resolvent of the sketched matrix. We further assume
that X has iid N(0, 1) entries and LL> = Im . Let L1 be an orthogonal complementary matrix of
L, SUCh that L>L + L>L1 = In. We also denote N = X Ln LIX. Then
MX >X∕n =MX >L>LX + X >L>L1X = ip — λM + MN.
n
Therefore, using that Cov [β] = α2∕p ∙ Ip, We find the bias as
bias2 = CaE [tr(M - Ip)(M> - Ip)]
2
—{λ2E [tr[M2]] + E tr M
(X>L1>L1X)
- 2λE trMN .
n
By the properties of Wishart matrices (e.g., Anderson, 2003; Muirhead, 2009), We have
n-m
E [N] =------Ip,
n
E [(N)2] = -2EE [Wish^a,rt(Ip,.n - m)2] = <2 [n - m + p(n - m) + (n - m)2]Ip .
Recalling that m, n → ∞ such that m∕n → ξ, and that θi(γ, λ) = (x + λ)-idFγ(x),
bias2 = Q [λ2 + n - m + PIn - m) + (n - m - 2λn-m] E [tr[M2]]
p	n2	n
→α[(λ+ξ-1)+γ(1 - ξ)]θ(γ, ξ, λ).
Moreover,
δ2 = σ2E MM2X>X]]
=σ2 ∙ {e [tr[M]] - λE [tr[M2]] + E [tr[M2N]] }
→γσ[θ1(γ,ξ,λ)-λθ(γ,ξ,λ)+(1 -ξ)θ(γ,ξ,λ)].
18
Published as a conference paper at ICLR 2020
Here we used the additional definitions
θi(γ,ξ,λ) = / (ξx + λ)i dFγ∕ξ(x)
θi(γ,λ) = θi(γ,ξ= 1, λ).
Note that these can be connected to the previous definitions by
θ1(γ, ξ, λ) =
θ2 (γ, ξ, λ) =
Therefore the AMSE of βp is
dFγ∕ξ(X) = ξθ1 (ξ, ξ
AMSE(βp) = α2[(λ + ξ - 1)2 + γ(1 - ξ)]θ2(γ, ξ, λ) + γσ2[θ1(γ, ξ,λ) - (λ + ξ - I)θ2(γ, ξ, λ)]
+γσ2 [I θ1 (ξ,ξ)- (λ+ξ - 1) ξ2 θ2 (γ,ξ
(10)
□
A.6.1 Isotropic case
Consider the special case where Γ = I, that is, X has iid N (0, 1) entries. Then Fγ is the standard
MP law, and We have the explicit forms for θi = θi(γ, λ)=((方/工产 dFγ:
θι(γ,λ) = - 1+λ + $[√γ + 1+λ + ι∕(√7 +1√=λ)2 -4],
Yλ	2 √γλ	√γ	V	√γ
1 Y +1	1 f λ +1	1	1 L L ^^1 + λ 2	^ 1
θ2(γ,λ) =-铲 + K-2√Y(~Γ + 1)λq(√ +詈)2-4 + 2√YV(√γ + 中) -42,
1-ζ
θ1 (Z, λ) = ζθ1 (Z, λ) +-ʌ-,
λ
1-Z
θ2(ζ, λ) = ζθ2(Z, λ) +—λ2~,
The results are obtained by the contour integral formula
∕f(x)dFγ (x) = -卷/
|z|=1
f(∣1+ γz∣2)(1-z2)2 d
z2(1 + √Yz)(z + √Y)
See Proposition 2.10 of Yao et al. (2015).
A.7 Proof of Theorem 4.2
Proof. Suppose d/p → Z as n goes to infinity. For βd, we have
bias2 = E ∣∣n-1X> (XRRTX>/n + λIn)-1 Xβ - β∣∣2] ,
2 XX>
δ2 = σ2 tr[(XRR>X>/n + λIn)	--2-].
Denote M = (XRRTX>/n + λIn 1. Note that, using that Cov[β] = α2∕p ∙ Ip
bias2 = QE [tr[MXX>∕n]2] - 2QE [tr[MXX>∕n]] + Q tr(Ip).
19
Published as a conference paper at ICLR 2020
Moreover, letting R1 to be an orthogonal complementary matrix ofR, such that RR>+R1R1> = In,
and N = xrir>x> ,
n
E 1 tr[MXX>/n] = 1 tr[In - λE [tr[M]] + E [MN]]
pp
→ Y- λ Z x+λ 识⑺+γ-ζ Z x+λ dFx
where FZ is the companion MP law, that is, FZ = (1 - γ)δ0 + YFZ. The third term calculated
by using that XR and XR1 are independent for a Gaussian random matrix X , so that M, N are
independent, and that E [N] = PndIn. Thus
E
1tr[MXX>∕nfl → 1 -入 + Z - Yθι(ζ, λ)
p	γγ
1 - λ + Z - Y
Y Y
1-Z
—ʌ---+ ζθ1(ζ, λ) .
λ
Then
E 1tr[MXX>∕n]2 = 1 E [tr[In + λ2M2 + MNMN - 2λM + 2MN - λM2N - λMNM].
Note that
E [MNMN|M] = M[(p - d)(M> + tr(M)In) + (p - d)2M]/n2
=P-d +(p-d产 M 2 + 一 tr(M)M,
n2	n2
so
E 1-tMMXX>∕n]2l → 1[1 + (λ2 - 2λ(Y - Z) + (y - Z)2)&(Z, λ)
pY
+ 2(Y - Z - λ)θθι(ζ, λ) + (Y - ζW2(Z, λ)].
Thus we find the following exprssion for the limiting squared bias:
2
bias2 → 一 [γ - 1 + (λ - γ + Z)2& + (γ - ZW2].
Y
With similar calculations (that we omit for brevity), we can find
δ2 → σ2(R(Z, λ) - (λ + Z - Y)&(Z, λ)).
_ _ _ _ ʌ
Therefore the AMSE of βd is
2
AMSE = 一 [Y - 1 + (λ - Y + Z)2& + (Y - ZW2]+ σ2[θ1(Z, λ) - (λ + Z - Y)&(Z, λ)].
Y1
(11)
□
A.8 Proof of Theorem 4.3
Proof. Recall that we have m, n → ∞, such that m∕n → ξ. Then we need to take ξ → 0.
However, we find it more convenient to do the calculation directly from the finite sample results as
m, n, p → ∞ with m∕n → 0, p∕n → Y, It is not hard to check that computing the results in the
other way (i.e., interchanging the limits), leads to the same results. Starting from our bias formula
for primal sketching, we first get
bias2 = Q [λ2 + n - m + Ppn - m) + (n - m - 2λ工[E [tr[(X>L>LX∕n + λIp)-2]i
p	n2	n
→ α2 [(λ - 1)2 + Y]∕λ2 .
20
Published as a conference paper at ICLR 2020
The limit of the trace term is not entirely trivial, but it can be calculated by (1) observing that the
m × p sketched data matrix P = LX has iid normal entries (2) thus the operator norm of P >P/n
vanishes, (3) and so by a simple matrix perturbation argument the trace concentrates around p∕λ2 *.
This gives the rough steps of finding the above limit. Moreover,
2
δ2 =荔E [tr[(X>L>LX/n + λIp)-2 X>X]] → γσ2∕λ2 ∙ EFYX2 = γσ2∕λ2
So the MSE is M(λ) = α2[(λ - 1)2 + Y]∕λ2 + σ2 ∙ γ∕λ2. From this it is elementary to find the
optimal λ and its objective value.	□
A.9 Proof of Theorem 4.4
Proof. Note that the bias can be written as
bias2
α2
一E tr[
p
-2 Q E
p
XRRTX T	-1 XX T 2
+λIn	F ]2
[tr[(XRR>X >∕n + λIn)-1 XX >∕n]] + α2.
Write G = XX>. Since RRT 〜Wp(Ip, d), We have XRRTX> 〜Wn(G, d). So XRRTX> =
G1/2WG1/2, where W 〜Wn(In, d).
E tr[
XRRTX T	-1	T
—nd — + λIn	xx /n]
E 卜“91/2WG1^∕d + nλIn)-1G]]
E tr[(~d + λ(n)-1)-1].
So we need to find the law of 乎 + λ(G)-1. Suppose first that G = XX> 〜Wn(In,p) Then
W and G-1 are asymptotically freely independent. The l.s.d. of W∕d is the MP law F∖∕ξ while
the l.s.d. of G∕p is the MP law Fι∕γ. We need to find the additive free convolution W 田 G, where
G= λ G-1.
Recall that the R-transform of a distribution F is defined by
RF(Z) = m-1(-z) - Z,
where mF-1(z) is the inverse function of the Stieltjes transform of F (e.g., Voiculescu et al., 1992;
Hiai & Petz, 2006; Couillet & Debbah, 2011). We can find the R-transform by solving
m F (RF (Z) + _) = —z.
Note that the R-transform of W∕d is
RW (z)
1
1-z∕ξ.
The Stieltjes transform of G-1 is
mG-1(z)
/十匹/y(X
-1 - -4m∖∕γ(1)
Z	Z2	Z
1	1 - 1 - 1 + √(1 + Y + 1 )2 - 4
---
2 Z
—
1 + 1 - 1 + λ∕(1 + 1 - 1 )2 - 4
γz	γz γ
2z
γ
21
Published as a conference paper at ICLR 2020
Figure 9: Dual Gaussian sketch improves MSE.
Then the R-transform of G-1 is
RG-1(z)
1 , Y + 1 - VZ(Y + 1)2 — 4γ(z + 1)
Z+	2Z
Y - 1 - VZ(Y - 1)2 -年
2Z
Since We have the property that Raμ(z) = aRμ(az),
RG = Rλg-i (Z)
γ
γ — 1 — ʌ/(Y — 1)2 — 4λz
2Z
Hence We have
RW 田 G = RW + RG
1 Y — 1 — ʌ/(Y — 1)2 — 4λz
1 — z/g	2z
Moreover, the Stieltjes transform of μ = W 田 GG satisfies
1	-1	1
mμ (Z) = mW田G (Z) = RF (-Z) — Z
1 Y — 1 — ʌ/(Y — 1)2 + 4λz	1
-------+ --------二—---------------
1 + z/ξ	—2z	z
Note that
2QE tr[f XRR[X> + λIn)	XX>/n] → 2α2E“ [11 = 2Q lim m(z),
p	nd	Y x Y z→0
α2
—E
tr[
XRR>X >
nd
+ λIn )
XX >
nd
→ — Eμ ɪ = — lim -dm(z).
Y μ Lx2J	Y z→0 dz	/
p
So it suffices to find m(z) and 关m(z) evaluated at zero.
□
XRR>X >	-2	>
nd + λIn) xx /n]
This result can characterize the performance of sketching in the high SNR regime, Where α σ.
To understand the loWer SNR regime, We need to study the variance, and thus We need to calculate
σ2E [tr[(W + λ(G)T)-2G-1]
Yp
where G = XX> 〜 Wn(In,p) is a Wishart distribution, and XRR>X> =d G1/2WG1/2, with
W 〜Wn(In, r). This seems to be quite challenging, and we leave it to future work.
Var = σ2 JE tr[
22
Published as a conference paper at ICLR 2020
A.10 Results for primal Gaussian sketching
The statement requires some notions from free probability, see e.g., Voiculescu et al. (1992); Hiai
& Petz (2006); Nica & Speicher (2006); Anderson et al. (2010); Couillet & Debbah (2011) for
references .
Theorem A.1 (Bias of primal Gaussian sketch). Suppose X is an n × p standard Gaussian random
matrix. Suppose also that L is a d × n matrix with i.i.d. N (0, 1/d) entries. Then the bias of primal
sketch has the expression MSE(∕3p) = α2 + αγ2 [τ((a + b)-1b(a+b)-1b-1) — 2τ((a + b)-1)], where
a and b two free random variables, that are freely independent in a non-commutative probability
space, and T is their trace. Specifically, the law of a is the MP law Fι∕ξ and b = λb, where the law
of b is the MP law Fι∕γ.
Proof of Theorem A.1. Note that
bias2 = E Il(X>L>LX/(nd) + λIp)-1 (X>X∕n)β — β^
and (X>L>LX∕(nd) + λIp)-1 X> = X>(L>LXX>∕(nd) + λIn)-1. Thus
bias2
E IIX> (
L^LXX> + λIn)-1 Xβ — β
nd	n
2
2
=ɑ2 + Q E[tr[( XX;LzL + λIn)-i XX> (X> + 八乙尸 XX1 ]
p	nd	n nd	n
-2tr[(L>LXX> + λIn)-iXX1]].
nd	n
First We find the l.s.d. of (LTLndXT + λIn)-1 XX>. Write W = L>L, G = XX>. Then
(ULXXI + "n尸 XXΓ = (WG + "n尸 G = G-1( W∙ + λ( G )-1 )-1G,
n	nn	n	n
which is similar to (W + λ(GG)-1)-1. So it suffices to find the Lsd of (} + λ(G)-1)-1.
By the definition, W 〜Wn(In, d), G 〜Wn(In,p), therefore the l.s.d. of W/d converges to the
MP law F∖∕ξ and the LSd of G/p converges to the MP law F∖∕γ.
Also note that
(XX>H + λIn)-1 XX> (L>LXX> + λIn)-1 XX> = (W + λnG-1)-1G-1( W + λnG-1)
nd	n nd	n d	d
We write A =号,B = λ (G)-1. Then it suffices to find
2
—E [tr[(A + B)TB(A + B)TB-1]].
We will find an expression for this using free probability. For this we will need to use some series
expansions. There are two cases, depending on whether the operator norm of BA-1 is less than or
greater than unity, leading to different series expansions. We will work out below the first case, but
the second case is similar and leads to the same answer.
1G.
tr[(A + B)-1B(A + B)-1B-1] =tr[A-1(I+BA-1)-1BA-1(I+BA-1)-1B-1]
Since the operator norm of BA-1 is less unity, we have the von Neumann series expansion
∞
[I+BA-1]-1 = X(—BA-1)i,
i=0
23
Published as a conference paper at ICLR 2020
then we have
tr[(A + B)-1B(A + B)-1B-1] = X (-1)i+j tr[(BA-1)i+j+1B-1A-1]
i,j≥0
= X (-1)i+j tr[(A-1B)i+j+1A-1B-1].
i,j≥0
Since A and B are asymptotically freely independent in the free probability space arising in the limit
(e.g., Voiculescu et al., 1992; Hiai & Petz, 2006; Couillet & Debbah, 2011), and the polynomial
(a-1b)i+j+1a-1b-1 involves an alternating sequence of a, b, we have
1 tr[(A-1B)i+j+1A-1B-1] → T[(a-1b)i+j+1a-1b-1],
n
where a and the b are free random variables and τ is their law. Specifically, a is a free random
variable with the MP law F∖∕ξ and b is λb-1, where b is a free r.v. with MP law Fι∕γ. Moreover,
they are freely independent.
Hence, we have
1 tr[(A + B)TB(A + B)TB-1] → T[χ(-1)i(a-1b)i+1 X(-1)j(a-1b)ja-1b-1]
n	i≥0	j≥0
= T[(a-1b)(1+ a-1b)-1(1+ a-1b)-1a-1b-1]
= T[(a + b)-1b(a + b)-1b-1].
Therefore,
α2 1
bias2 → α2 + - - tr[(A + B)TB(A + B)TBT - 2tr[A + B]-1]
γn
2
=α2 +----[τ ((a + b)-1b(a + b)-1b-1) — 2τ ((a + b)-1)].
γ
□
A.11 Results for full sketching
The full sketch estimator projects down the entire data, and then does ridge regression on the
sketched data. It has the form
βf = (X >L>LX∕n + λIp)-1 X >L>LY
We have
bias2
a"2/ ξX+λ dFγ7ξ (X)
var = σ2γ
σ2γ
J ξχ~+λdFγ6(X)- λ / (ξx + λ)2 dFγ7ξ(X)
therefore
,ʌ .
AMSE(βf)
+ σ2γ
The optimal λ for full sketch is always λ* = IO2, the same as ridge regression. Some simulation
results are shown in Figure 10, and they show the expected shape (e.g., they decrease with ξ).
24
Published as a conference paper at ICLR 2020
Figure 10: Simulation results for full sketch, with n
averaged over 30 independent experiments.
1000, γ = 0.1. The simulation results are
Figure 11: Dual orthogonal sketching with γ = 1.5, λ = 1, α = 3, σ = 1. Left: MSE of dual
sketching normalized by the MSE of ridge regression. The standard deviation is over 50 repetitions.
Right: Bias and variance of dual sketching normalized by the bias and variance of ridge regression,
respectively.
A.12 Numerical results
A.12.1 Dual orthogonal sketching
See Figure 11 for additional simulation results for dual orthogonal sketching.
A.12.2 Performance at a fixed regularization parameter
First we fix the regularization parameter at the optimal value for original ridge regression. The
results are visualized in Figure 12. On the x axis, we plot the reduction in sample size m/n for
primal sketch, and the reduction in dimension d/p for dual sketch. In this case, primal and dual
sketch will increase both bias and variance, and empirically in the current case, dual sketch increases
them more. So in this particular case, primal sketch is preferred.
A.12.3 Performance at the optimal regularization parameter
We find the optimal regularization parameter λ for primal and dual orthogonal sketching. Then we
use the optimal regularization parameter for all settings, see Figure 13. Both primal and dual sketch
increase the bias, but decrease the variance. It is interesting to note that, for equal parameters ξ and
25
Published as a conference paper at ICLR 2020
Figure 12: Fixed regularization parameter λ = 0.7, optimal for original ridge, in a setting where
γ = 0.7, and α2 = σ2.
Figure 13: Primal and dual sketch at optimal λ. We take γ = 0.7 and let ξ range between 0.001 and
1, where for primal sketch ξ = r/n while for dual sketch ξ = d/p.
26
Published as a conference paper at ICLR 2020
ζ, and in our particular case, dual sketch has smaller variance, but larger bias. So primal sketch is
preferred bias or MSE is important, but dual sketch is more desired when one wants smaller variance.
All in all, dual sketch has larger MSE than primal sketch in the current setting. It can also be seen
that in this specific example, the optimal λ for primal sketch is smaller than that of dual sketch.
However these results are hard to interpret, because there is no natural correspondence between the
two parameters ξ and ζ .
A.13 Computational complexity
Since sketching is a method to reduce computational complexity, itis important to discuss how much
computational efficiency we gain. Recall our three estimators
β = (X>X/n + λIp)-1 X>Y/n = n-1X> (XX>/n + λIn)-1 Y,
βp = (X >L>LX∕n + λIp)-1 X >Y/n,
βd = n-1X> (XRR>X>∕n + λIn)-1 Y,
Their computational complexity, when computed in the usual way, is:
•	No sketch (Standard ridge): if p < n, computing X>Y and X>X requires O(np) and
O(np2) flops, then solving the linear equation (X>X∕n + λIp)β = X>Y∕n requires
O(p3) flops by the LU decomposition. It is O(np2) flops in total.
If p > n, we use the second formula for β, and the total flops is O (pn2).
•	Primal sketch: for the Hadamard sketch (and other sketches based on the FFT), computing
LX by FFT requires mp log n, computing (LX)>LX requires mp2, so the total flops is
O(p3 + mp(log n + p)). So the primal sketch can reduce the computation cost only when
p < n.
•	Dual sketch: computing XRR>X> requires nd (log p + n) flops by FFT, solving
(XRR>X>/n + λIn)-1 Y requires O(n3) flops, the matrix-vector multiplication ofX>
and (XRR>X>∕n+λIn)-1Y requires O(np) flops, so the total flops is O(n3 +nd(logp+
n)). Dual sketching can reduce the computation cost only when p > n.
27