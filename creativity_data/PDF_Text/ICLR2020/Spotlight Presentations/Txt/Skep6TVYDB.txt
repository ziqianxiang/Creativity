Published as a conference paper at ICLR 2020
Gradientless	Descent:	High-Dimensional
Zeroth-Order Optimization
Daniel Golovin, John Karro, Greg Kochanski, Chansoo Lee
Xingyou Song, Qiuyi(Richard) Zhang*
Google Brain
{dgg,karro,gpk,chansoo,xingyousong,qiuyiz}@google.com
Ab stract
Zeroth-order optimization is the process of minimizing an objective f (x), given
oracle access to evaluations at adaptively chosen inputs x. In this paper, we present
two simple yet powerful GradientLess Descent (GLD) algorithms that do not rely
on an underlying gradient estimate and are numerically stable. We analyze our
algorithm from a novel geometric perspective and present a novel analysis that
shows convergence within an -ball of the optimum in O(kQ log(n) log(R/))
evaluations, for any monotone transform of a smooth and strongly convex ob-
jective with latent dimension k < n, where the input dimension is n, R is the
diameter of the input space and Q is the condition number. Our rates are the first
of its kind to be both 1) poly-logarithmically dependent on dimensionality and
2) invariant under monotone transformations. We further leverage our geometric
perspective to show that our analysis is optimal. Both monotone invariance and
its ability to utilize a low latent dimensionality are key to the empirical success of
our algorithms, as demonstrated on BBOB and MuJoCo benchmarks.
1	Introduction
We consider the problem of zeroth-order optimization (also known as gradient-free optimization, or
bandit optimization), where our goal is to minimize an objective function f : Rn → R with as few
evaluations of f(x) as possible. For many practical and interesting objective functions, gradients
are difficult to compute and there is still a need for zeroth-order optimization in applications such
as reinforcement learning (Mania et al., 2018; Salimans et al., 2017; Choromanski et al., 2018),
attacking neural networks (Chen et al., 2017; Papernot et al., 2017), hyperparameter tuning of deep
networks (Snoek et al., 2012), and network control (Liu et al., 2017).
The standard approach to zeroth-order optimization is, ironically, to estimate the gradients from
function values and apply a first-order optimization algorithm (Flaxman et al., 2005). Nesterov &
SPokoiny (2011) analyze this class of algorithms as gradient descent on a Gaussian smoothing of
the objective and gives an accelerated θ(n√Qlog((LR2 + F)/e)) iteration complexity for an L-
LiPschitz convex function with condition number Q and R = kx0 - x* k and F = f(x0) - f(x*).
They propose a two-point evaluation scheme that constructs gradient estimates from the difference
between function values at two points that are close to each other. This scheme was extended by
(Duchi et al., 2015) for stochastic settings, by (Ghadimi & Lan, 2013) for nonconvex settings, and by
(Shamir, 2017) for non-smooth and non-Euclidean norm settings. Since then, first-order techniques
such as variance reduction (Liu et al., 2018), conditional gradients (Balasubramanian & Ghadimi,
2018), and diagonal preconditioning (Mania et al., 2018) have been successfully adopted in this
setting. This class of algorithms are also known as stochastic search, random search, or (natural)
evolutionary strategies and have been augmented with a variety of heuristics, such as the popular
CMA-ES (Auger & Hansen, 2005).
These algorithms, however, suffer from high variance due to non-robust local minima or highly
non-smooth objectives, which are common in the fields of deep learning and reinforcement learn-
* Author list in alphabetical order.
1
Published as a conference paper at ICLR 2020
ing. Mania et al. (2018) notes that gradient variance increases as training progresses due to higher
variance in the objective functions, since often parameters must be tuned precisely to achieve rea-
sonable models. Therefore, some attention has shifted into direct search algorithms that usually
finds a descent direction u and moves to x + δu, where the step size is not scaled by the function
difference.
The first approaches for direct search were based on deterministic approaches with a positive span-
ning set and date back to the 1950s (Brooks, 1958). Only recently have theoretical bounds surfaced,
with Gratton et al. (2015) giving an iteration complexity that is a large polynomial of n and Dodan-
geh & Vicente (2016) giving an improved O(n2L2/). Stochastic approaches tend to have better
complexities: Stich et al. (2013) uses line search to give a O(nQ log(F /)) iteration complexity for
convex functions with condition number Q and most recently, Gorbunov et al. (2019) uses impor-
tance sampling to give a O(nQ log(F∕e)) complexity for convex functions with average condition
number Q, assuming access to sampling probabilities. StiCh et al. (2013) notes that direct search
algorithms are invariant under monotone transforms of the objective, a property that might explain
their robustness in high-variance settings.
In general, zeroth order optimization suffers an at least linear dependence on input dimension n and
recent works have tried to address this limitation when n is large but f(x) admits a low-dimensional
structure. Some papers assume that f(x) depends only on k coordinates and Wang et al. (2017)
applies Lasso to find the important set of coordinates, whereas Balasubramanian & Ghadimi (2018)
simply change the step size to achieve an O(k(log(n)/)2) iteration complexity. Other papers as-
sume more generally that f(x) = g(PAx) only depends on a k-dimensional subspace given by the
range of PA and Djolonga et al. (2013) apply low-rank approximation to find the low-dimensional
subspace while Wang et al. (2013) use random embeddings. Hazan et al. (2017) assume that f(x)
is a sparse collection of k-degree monomials on the Boolean hypercube and apply sparse recov-
ery to achieve a O(nk) runtime bound. We will show that under the case that f(x) = g(PAx),
our algorithm will inherently pick up any low-dimensional structure in f(x) and achieve a con-
vergence rate that depends on k log(n). This initial convergence rate survives, even if we perturb
f(x) = g(PAx) + h(x), so long as h(x) is sufficiently small.
We will not cover the whole variety of black-box optimization methods, such as Bayesian opti-
mization or genetic algorithms. In general, these methods attempt to solve a broader problem (e.g.
multiple optima), have weaker theoretical guarantees and may require substantial computation at
each step: e.g. Bayesian optimization generally has theoretical iteration complexities that grow ex-
ponentially in dimension, and CMA-ES lacks provable complexity bounds beyond convex quadratic
functions. In addition to the slow runtime and weaker guarantees, Bayesian optimization assumes
the success of an inner optimization loop of the acquisition function. This inner optimization is
often implemented with many iterations of a simpler zeroth-order methods, justifying the need to
understand gradient-less descent algorithms within its own context.
1.1	Our contributions
In this paper, we present GradientLess Descent (GLD), a class of truly gradient-free algorithms
(also known as direct search algorithms) that are parameter free and provably fast. Our algorithms
are based on a simple intuition: for well-conditioned functions, if we start from a point and take
a small step in a randomly chosen direction, there is a significant probability that we will reduce
the objective function value. We present a novel analysis that relies on facts in high dimensional
geometry and can thus be viewed as a geometric analysis of gradient-free algorithms, recovering
the standard convergence rates and step sizes. Specifically, we show that if the step size is on the
order of O(√1n), we can guarantee an expected decrease of 1 - Ω(ɪ) in the optimality gap, based
on geometric properties of the sublevel sets of a smooth and strongly convex function.
Our results are invariant under monotone transformations of the objective function, thus our conver-
gence results also hold for a large class of non-convex functions that are a subclass of quasi-convex
functions. Specifically, note that monotone transformations of convex functions are not necessarily
convex. However, a monotone transformation of a convex function is always quasi-convex. The
maximization of quasi-concave utility functions, which is equivalent to the minimization of quasi-
convex functions, is an important topic of study in economics (e.g. Arrow & Enthoven (1961)).
2
Published as a conference paper at ICLR 2020
Table 1: Comparison of zeroth order optimization for well-conditioned convex functions where
R = ∣∣χo - x*k and F = f(xo) - f(x*). ’Monotone' column indicates the invariance under
monotone transformations (Definition 4). ‘k-Sparse’ and ‘k-Affine’ columns indicate that iteration
complexity is poly(k, log(n)) when f(x) depends only on a k-sparse subset of coordinates or on a
rank-k affine subspace.
Algorithm	Iteration Complexity	Monotone	k-Sparse	k-Afine
NeSterOV & Spokoiny (2011)	n log((R2 + FW~	No	-No-	-No-
Balasubramanian & Ghadimi (2018)	n log(F∕e)	No	-Yes-	-No-
Stich etal. (2013)	n log(F∕e)	Yes	-No-	-No-
GOrbUnOV et al.(2019)	n log(F∕e)	Yes	-No-	-No-
This paper (GLD)	n log(R∕e)	Yes	YeS	YeS
Intuition suggests that the step-size dependence on dimensionality can be improved when f(x)
admits a low-dimensional structure. With a careful choice of sampling distribution we can show that
if f (x) = g(PAx), where PA is a rank k matrix, then our step size can be on the order of O(我)as
our optimization behavior is preserved under projections. We call this property affine-invariance and
show that the number of function evaluations needed for convergence depends logarithmically on
n. Unlike most previous algorithms in the high-dimensional setting, no expensive sparse recovery
or subspace finding methods are needed. Furthermore, by novel perturbation arguments, we show
that our fast convergence rates are robust and holds even under the more realistic assumption when
f(x) = g(PAx) + h(x) with h(x) being sufficiently small.
Theorem 1 (Convergence of GLD: Informal Restatement of Theorem 7 and Theorem 14). Let f(x)
be any monotone transform of a convex function with condition number Q and R = ∣x0 - x* ∣. Let
y be a sample from an appropriate distribution centered at x. Then, with constant probability,
f(y)- f (χ*) ≤ (f(χ)- f(χ*))(1- 5n⅛)
Therefore, we can find xT such that ∣xT -x* ∣ ≤ after T = O(nQ log(R/)) function evaluations.
Furthermore, for functions f(x) = g(PAx) + h(x) with rank k matrix PA and sufficiently small
h(x), we only require O(kQ log(n) log(R/)) evaluations.
Another advantage of our non-standard geometric analysis is that it allows us to deduce that our
rates are optimal with a matching lower bound (up to logarithmic factors), presenting theoretical
evidence that gradient-free inherently requires Ω(nQ) function evaluations to converge. While
gradient-estimation algorithms can achieve a better theoretical iteration complexity of O(n√Q),
they lack the monotone and affine invariance properties. Empirically, we see that invariance prop-
erties are important to successful optimization, as validated by experiments on synthetic BBOB and
MuJoCo benchmarks that show the competitiveness of GLD against standard optimization proce-
dures.
2	Preliminaries
We first define a few notations for the rest of the paper. Let X be a compact subset of Rn and let
k ∙ k denote the Euclidean norm. The diameter of X, denoted ∣∣X∣ = maxχ,χo∈χ ||x - χ0∣, is the
maximum distance between elements in X . Let f : X → R be a real-valued function which attains
its minimum at x*. We use f(X) = {f (x) : x ∈ X} to denote the image of f on a subset X ofRn ,
and B(c, r) = {x ∈ Rn : kc - xk ≤ r} to denote the ball of radius r centered at c.
Definition 2. The level set of f at point x ∈ X is Lc (f) = {y ∈ X : f(y) = f (x)}. The sub-level
set of f at point X ∈ X is L⅛(f) = {y ∈ X : f (y) ≤ f (x)}. When the function f is clear from the
context, we omit it.
Definition 3. We s^y that f is a-strongly convex for α > 0 if f (y) ≥ f(x) + Wf(X) y — Xi +
ɑ2 ∣∣y — x∣2 forall Xjy ∈ X and β-smoothfor β > 0 if f (y) ≤ f (x) + Ef (x), y — Xi + β ∣∣y — x∣2
for all X, y ∈ X.
3
Published as a conference paper at ICLR 2020
Definition 4. We say that gof is a monotone transformation of f if g : f (X) → R isa monotonically
(and strictly) increasing function.
Monotone transformations preserve the level sets of a function in the sense that Lx (f) = Lx (g ◦
f). Because our algorithms depend only on the level set properties, our results generalize to any
monotone transformation of a strongly convex and strongly smooth function. This leads to our
extended notion of condition number.
Definition 5. A function f has condition number Q ≥ 1 if it is the minimum ratio β∕a over all
functions g such that f is a monotone transformation of g and g is α-strongly convex and β smooth.
When we work with low rank extensions of f, we only care about the condition number of f within
a rank k subspace. Indeed, if f only varies along a rank k subspace, then it has a strong convexity
value of 0, making its condition number undefined. If f is α-strongly convex and β-smooth, then its
Hessian matrix always has eigenvalues bounded between α and β. Therefore, we need a notion of
a projected condition number. Let A ∈ Rd×k be some orthonormal matrix and let PA = AA> be
the projection matrix onto the column space of A.
Definition 6. For some orthonormal A ∈ Rd×k with d > k, a function f has condition number
restricted to A, Q(A) ≥ 1, if it is the minimum ratio β∕α over all functions g such that f is a
monotone transformation ofg and h(y) = g(Ay) is α-strongly convex and β smooth.
3	Analysis of Descent Steps
The GLD template can be summarized as follows: given a sampling distribution D, we start at x0
and in iteration t, we choose a scalar radii rt and we sample yt from a distribution rtD centered
around xt, where rt provides the scaling of D. Then, if f(yt) < f(xt), we update xt+1 = yt;
otherwise, we set xt+1 = xt . The analysis of GLD follows from the main observation that the sub-
level set of a monotone transformation of a strongly convex and strongly smooth function contains
a ball of sufficiently large radius tangent to the level set (Lemma 15). In this section, we show that
this property, combined with facts of high-dimensional geometry, implies that moving in a random
direction from any point has a good chance of significantly improving the objective.
As we mentioned before, the key to fast convergence is the careful choice of step sizes, which we
describe in Theorem 7. The intuition here is that we would like to take as large steps as possible
while keeping the probability of improving the objective function reasonably high, so by insights in
high-dimensional geometry, We choose a step size of Θ(1∕√n). Also, We show that if f (x) admits
a latent rank-k structure, then this step size can be increased to Θ(1∕√k) and is therefore only
dependent on the latent dimensionality of f (x), allowing for fast high-dimensional optimization.
Lastly, our geometric understanding allows us to show that our convergence rates are optimal with a
matching lower bound. Without loss of generality, this section assumes that f(x) is strongly convex
and smooth with condition number Q.
3.1	Step Size
Theorem 7. Forany X such that 5Q ∣∣x 一 x*k ∈ [C1,C2], we can find integers 0 ≤ k1,k2 < log C2
such that if r = 2k1 C1 or r = 2-k2 C2, then a random sample y from uniform distribution over
Bx = B(x, √n) satisfies
f(y)- f(χ*) ≤ (f(χ)- f(χ*)) (1 - 5⅛)
with probability at least 4.
Proving the above theorem requires the following lemma about the intersection of balls in high
dimensions and it is proved in the appendix.
Lemma 8. Let B1 and B2 be two balls in Rn of radii r1 and r2 respectively. Let ` be the distance
between the centers. If ri ∈ [ 2√n, √n ] and r ≥ ' 一看,then
vol (B1 ∩ B2) ≥ cnvol(B1) ,
where Cn is a dimension-dependent constant that is lower bounded by 4 at n = 1.
4
Published as a conference paper at ICLR 2020
3.2	Gaussian Sampling and Low Rank Structure
A direct application of Lemma 8 seems to imply that uniform sampling of a high-dimensional ball
is necessary. Upon further inspection, this can be easily replaced with a much simpler Gaussian
sampling procedure that concentrates the mass close to the surface to the ball. This procedure
lends itself to better analysis when f (x) admits a latent low-dimensional structure since any affine
projection of a Gaussian is still Gaussian.
Lemma 9. Let B1 and B2 be two balls in Rn of radii r1 and r2 respectively. Let ` be the distance
between the centers. If ri ∈ [2√n, √n] and r ≥ ' 一 ' and X = (Xi,…,Xn) are independent
Gaussians with mean centered at the center of Bi and variance r1, then
Pr[X ∈ B2] > c,
where c is a dimension-independent constant.
Assume that there exists some rank k projection matrix PA such that f(x) = g(PAx), where k is
much smaller than n. Because Gaussians projected on a k-dimensional subspace are still Gaussians,
we show that our algorithm has a dimension dependence on k. We let Qg(A) be the condition
number of g restricted to the subspace A that drives the dominant changes in f (x).
Theorem 10. Let f(x) = g(PAx) for some unknown rank k matrix PA with k < n and suppose
5QkPA(X —x*)k ∈ [C1,C2] for some numbers C1,C2 ∈ R+. Then,, there exist integers 0 ≤
ki, k2 < log C2 such that if r = 2k1 Ci or r = 2-k2C2, then a random sample y from a Gaussian
distribution N(x, r21) satisfies
f(y) — f(χ*) ≤ (f(X)- f(χ*)) (1 - KkQg(A))
with constant probability.
Note that the speed-up in progress is due to the fact that We can now tolerate the larger sampling
radius of Ω(1/√k), while maintaining a high probability of making progress. If k is unknown, we
can simply use binary search to find the correct radius with an extra factor of log(n) in our runtime.
The low-rank assumption is too restrictive to be realistic; however, our fast rates still hold, at least for
the early stages of the optimization, even ifwe assume that f(X) = g(PAX) + h(X) and |h(X)| ≤ δ
is a full-rank function that is bounded by δ. In this setting, we can show that convergence remains
fast, at least until the optimality gap approaches δ.
Theorem 11. Let f(X) = g(PAX) + h(X) for some unknown rank k matrix PA with k < n where
g, h are convex and |h| ≤ δ. Suppose 5Q ∣∣Pax — z*k ∈ [Ci, C2] for some numbers Ci,C2 ∈ R+
where z* minimizes g(z). Then, there exist integers 0 ≤ ki,k2 < log C2 such that if r = 2k1 Ci or
r = 2-k2C2, then a random sample y from a Gaussian distribution N(x, r21) satisfies
f(y) — f(χ*) ≤ (f(χ) — f(χ*)) (1 — i0⅛7)
with constant probability whenever f(X) — f(X*) ≥ 60δkQg (A).
3.3	Lower Bounds
We show that our upper bounds given in the previous section are tight up to logarithmic factors for
any symmetric sampling distribution D. These lower bounds are easily derived from our geometric
perspective as we show that a sampling distribution with a large radius gives an extremely low
probability of intersection with the desired sub-level set. Therefore, while gradient-approximation
algorithms can be accelerated to achieve a runtime that depends on the square-root of the condition
number Q, gradient-less methods that rely on random sampling are likely unable to be accelerated
according to our lower bound. However, we emphasize that monotone invariance allows these results
to apply to a broader class of objective functions, beyond smooth and convex, so the results can be
useful in practice despite the seemingly worse theoretical bounds.
5
Published as a conference paper at ICLR 2020
1
2
3
4
5
6
7
8
9
10
Algorithm 1: Gradientless Descent with Binary Search (GLD-Search)
Input: function: f : Rn → R, T ∈ Z+: number of iterations, x0: starting point,
D : sampling distribution, R: maximum search radius, r: minimum search radius
Set K = log(R/r)
for t = 0, . . . , T do
Ball Sampling Trial i:
for k = 0, . . . , K do
Set ri,k =2-kR.
Sample "i,k 〜ri,kD.
end
Update: xt+1 = arg mink f (y) y = xt, y = xt + vi,k
end
return xt
Theorem 12. Let y = x + v, where v is a random sample from rD for some radius r > 0 and D
is standard Gaussian or any rotationally symmetric distribution. Then, there exist a region X with
positive measure such that for any x ∈ X,
f (y) - f (χ*) ≥ (f (χ) - f (χ*))(1 - √5⅞nQ))
with probability at least 1 一。。仅另)∙
4	Gradientles s Algorithms
In this section, we present two algorithms that follow the same Gradientless Descent (GLD) tem-
plate: GLD-Search and GLD-Fast, with the latter being an optimized version of the former when
an upper bound on the condition number of a function is known. For both algorithms, since they
are monotone-invariant, we appeal to the previous section to derive fast convergence rates for any
monotone transform of convex f(x) with good condition number. We show the efficacy of both
algorithms experimentally in the Experiments section.
4.1	Gradientless Descent with B inary Search
Although the sampling distribution D is fixed, we have a choice of radii for each iteration of the
algorithm. We can apply a binary search procedure to ensure progress. The most straightforward
version of our algorithm is thus with a naive binary sweep across an interval in [r, R] that is un-
changed throughout the algorithm. This allows us to give convergence guarantees without previous
knowledge of the condition number at a cost of an extra factor of log(n/).
Theorem 13. Let x0 be any starting point and f a blackbox function with condition number Q.
Running Algorithm 1 with r = √n, R = ∣∣Xk and D = N(0, I) as a standard Gaussian returns a
point XT such that ∣xτ 一 x*∣ ≤ 2Q3/26 after O(nQ log(n∣X∣∕e)2) function evaluations with high
probability.
Furthermore, if f(x) = g(PAx) admits a low-rank structure with PA a rank k matrix, then we
only require O(kQg(A) log(n∣X∣∕e)2) function evaluations to guarantee ∣Pa(xt - X*)k ≤ e.
This holds analogously even if f(x) = g(PAx) + h(x) is almost low-rank where |h| ≤ δ and
>60δkQg(A).
4.2	Gradientless Descent with Fast Binary Search
GLD-Search (Algorithm 1) uses a naive lower and upper bound for the search radius ∣∣χt 一 χ*∣,
which incurs an extra factor of log(1/) in the runtime bound. In GLD-Fast, we remove this extra
factor dependence on log(1/) by drastically reducing the range of the binary search. This is done
by exploiting the assumption that f has a good condition number upper bound Q and by slowly
halfing the diameter of the search space every few iterations since We expect Xt → x* as t → ∞.
6
Published as a conference paper at ICLR 2020
1
2
3
4
5
6
7
8
9
10
11
Algorithm 2: Gradientless Descent with Fast Binary Search (GLD-Fast)
Input: function f : Rn → R, T ∈ Z+: number of iterations, x0: starting point,
D: sampling distribution, R: diameter of search space, Q: condition number bound
Set K = log(4√Q), H = nQ log(Q)
for t = 1, . . . , T do
Set R = R/2 when T ≡ 0 mod H (every H iterations).
Ball Sampling Trial i:
fork = -K, ..., 0, ..., Kdo
Set /,k =2-kR.
Sample vi,k 〜ri,kD.
end
Update: xt+1 = arg mini f (y) y = xt, y = xt + vi
end
return xt
Theorem 14. Let x0 be any starting point and f a blackbox function with condition number upper
bounded by Q. Running Algorithm 2 with suitable parameters returns a point xT such that f (xT ) -
f (x*) ≤ E after O(nQ Iog2(Q) log(kXk∕e)) function evaluations with high probability.
Furthermore, iff(x) = g(PAx) admits a low-rank structure with PA a rank k matrix, then we only
require O(kQg (A) log(n) log2 (Qg (A)) log(kXk/E))function evaluations to guarantee kPA(xT -
x*)k ≤ 匕 This holds analogously even if f (x) = g(PAx) + h(x) is almost low-rank where |h| ≤ δ
and E > 60δkQg (A).
5 Experiments
We tested GLD algorithms on a simple class of objective functions and compare it to Accelerated
Random Search (ARS) by Nesterov & Spokoiny (2011), which has linear convergence guarantees
on strongly convex and strongly smooth functions. To our knowledge, ARS makes the weakest as-
sumption among the zeroth-order algorithms that have linear convergence guarantees and perform
only a constant order of operations per iteration. Our main conclusion is that GLD-Fast is compa-
rable to ARS and tends to achieve a reasonably low error much faster than ARS in high dimensions
(≥ 50). In low dimensions, GLD-Search is competitive with GLD-Fast and ARS though it requires
no information about the function.
We let Hα,β,n ∈ Rn×n be a diagonal matrix with its i-th diagonal equal to α + (β - α) n-1. In
simple words, its diagonal elements form an evenly space sequence of numbers from α to β . Our
objective function is then fα,β,n : Rn → R as fα,β,n(x) = 2x>Hα,β,nx, which is α-strongly
convex and β-strongly smooth. We always use the same starting point X = √n (1,∙∙∙, 1), which
requires ∣∣X∣∣ = √Q for our algorithms. We plot the optimality gap f (bt) - f (x*) against the num-
ber of function evaluations, where bt is the best point observed so far after t evaluations. Although
all tested algorithms are stochastic, they have a low variance on the objective functions that we use;
hence we average the results over 10 runs and omit the error bars in the plots.
We ran experiments on fι,8,n with imperfect curvature information a and β (see Figure 3 in ap-
pendix). GLD-Search is independent of the condition number. GLD-Fast takes only one parameter,
which is the upper bound on the condition number; if approximation factor is z, then we pass 8z as
the upper bound. ARS requires both strong convexity and smoothness parameters. We test three dif-
ferent distributions of the approximation error; when the approximation factor is z, then ARS-alpha
gets (α∕z, β), ARS-beta gets (α, zβ), and ARS-even gets (α/√z, √zβ) as input. GLD-Fast is more
robust and faster than ARS when the condition number is over-approximated. When the condition
number is underestimated, GLD-Fast still steadily converges.
7
Published as a conference paper at ICLR 2020
ʤ⅛-raEijdo 60—1
Evaluations (ThOUSandS)
Evaluations (ThOUSandS)
Dimension = 50
Dimension = 10
Dimension = 20
ʤ⅛⅛E-⅛0 60—1
0123456789 10	0123456789 10	0123456789 10	0123456789 10
Evaluations (Thousands) Evaluations (Thousands) Evaluations (Thousands) Evaluations (Thousands)
Figure 1: The average optimality gap on a quadratic objective function that is strongly convex and
smooth objective (top); and its monotone transformation (bottom). Further experiments on non-
convex BBOB functions show similar behavior and are in the appendix.
→- GLD-Search
GLD-Fast
-*- ARS
十ARS
^∙- GLD~Fast
■«- GLD-Seareh
5.1	Monotone Transformations
In Figure 1, we ran experiments on f1,8,n for different settings of dimensionality n, and its monotone
transformation with g(y) = - exp(-√y). For this experiment, We assume a perfect oracle for the
strong convexity and smoothness parameters of f . The convergence of GLD is totally unaffected
by the monotone transformation. For the low-dimension cases of a transformed function (bottom
half of the figure), we note that there are inflection points in the convergence curve of ARS. This
means that ARS initially struggles to gain momentum and then struggles to stop the momentum
when it gets close to the optimum. Another observation is that unlike ARS that needs to build up
momentum, GLD-Fast starts from a large radius and therefore achieves a reasonably low error much
faster than ARS, especially in higher dimensions.
5.2	BBOB Benchmarks
To show that practicality of GLD on practical and non-convex settings, we also test GLD algorithms
on a variety of BlackBox Optimization Benchmarking (BBOB) functions (Hansen et al., 2009). For
each function, the optima is known and we use the log optimality gap as a measure of competance.
Because each function can exhibit varying forms of non-smoothness and convexity, all algorithms
are ran with a smoothness constant of 10 and a strong convexity constant of 0.1. All other setup
details are same as before, such as using a fixed starting point.
The plots, given in Appendix C, underscore the superior performance of GLD algorithms on various
BBOB functions, demonstrating that GLD can successfully optimize a diverse set of functions even
without explicit knowledge of condition number. We note that BBOB functions are far from convex
and smooth, many exhibiting high conditioning, multi-modal valleys, and weak global structure.
Due to our radius search produce, our algorithm appears more robust to non-ideal settings with
non-convexity and ill conditioning. As expected, we note that GLD-Fast tend to outperform GLD-
Search, especially as the dimension increases, matching our theoretical understanding of GLD.
5.3	Mujoco Control Benchmarks and Affine Transformations
We also ran experiments on the Mujoco benchmarks with varying architectures, both linear and
nonlinear. This demonstrates the viability of our approach even in the non-convex, high dimensional
setting. We note that however, unlike e.g. ES which uses all queries to form a gradient direction,
our algorithm removes queries which produce less reward than using the current arg-max, which
can be an information handicap. Nevertheless, we see that our algorithm still achieves competitive
performance on the maximum reward. We used a horizon of 1000 for all experiments.
8
Published as a conference paper at ICLR 2020
We further tested the affine invariance of GLD on the policy parameters from using Gaussian ball
sampling, under the HalfCheetah benchmark by projecting the state s of the MDP with linear policy
to a higher dimensional state Ws, using a matrix multiplication with an orthonormal W . Specif-
ically, in this setting, for a linear policy parametrized by matrix K, the objective function is thus
J(KW) where πK(Ws) = KWs. Note that when projecting into a high dimension, there is a
slowdown factor of log dnew where dnew, doid are the new high dimension and previous base di-
dold
mension, respectively, due to the binary search in our algorithm on a higher dimensional space.
For our HalfCheetah case, we projected the 17 base dimension to a 200-length dimension, which
SUggeStS that the slowdown factor is a factor log 2070 ≈ 3.5. This can be shown in our plots in the
appendix (Figure 15).
Table 2: Final rewards by GLD with linear (L) and deep (H41) policies on Mujoco Benchmarks show
that GLD is competitive. We apply an affine projection on HalfCheetah to test affine invariance. We
use the reward threshold found from (Mania et al., 2018) with Reacher’s threshold (Schulman et al.,
2017) for a reasonable baseline.
Env.	Arch	Rew. at (104, 105, Max) Queries	Rew. Thresh.
HalfCheetah-v1	L	3799, 3903, 4064	3430	=
HalfCheetah-Vl, Proj-200「	T	1594,3509,4342	1430
HalfCheetah-Vl	H41	2741, 3074, 3392	1430
Hopper-v1	-L	1017, 3359, 3375	"3120
Hopper-v1	H41	2708, 3370, 3566	"3120
ReaCher-VI	^L	-70,-5,-4	10Q
ReaCher-VI	H41	-231,-17,-15	"^10
Swimmer-v1	T	365, 369,369	-325
Swimmer-v1	H41	353, 369, 369	125
Walker2d-v1	T	1027,2201, 2201	"4390
Walker2d-v1	一	H41	1630,1963,2146	4390	—
6 Conclusion
We introduced GLD, a robust zeroth-order optimization algorithm that is simple, efficient, and we
show strong theoretical convergence bounds via our novel geometric analysis. As demonstrated
by our experiments on BBOB and MuJoCo benchmarks, GLD performs very robustly even in the
non-convex setting and its monotone and affine invariance properties give theoretical insight on its
practical efficiency.
GLD is very flexible and allows easy modifications. For example, it could use momentum terms
to keep moving in the same direction that improved the objective, or sample from adaptively cho-
sen ellipsoids similarly to adaptive gradient methods. (Duchi et al., 2011; McMahan & Streeter,
2010). Just as one may decay or adaptively vary learning rates for gradient descent, one might use
a similar change the distribution from which the ball-sampling radii are chosen, perhaps shrinking
the minimum radius as the algorithm progresses, or concentrating more probability mass on smaller
radii.
Likewise, GLD could be combined with random restarts or other restart policies developed for gra-
dient descent. Analogously to adaptive per-coordinate learning rates Duchi et al. (2011); McMahan
& Streeter (2010), one could adaptively change the shape of the balls being sampled into ellipsoids
with various length-scale factors. Arbitrary combinations of the above variants are also possible.
9
Published as a conference paper at ICLR 2020
References
Kenneth J Arrow and Alain C Enthoven. Quasi-concave programming. Econometrica: Journal of
the Econometric Society, pp. 779-800,1961.
Anne Auger and Nikolaus Hansen. A restart cma evolution strategy with increasing population size.
In Evolutionary Computation, 2005. The 2005 IEEE Congress on, volume 2, pp. 1769-1776.
IEEE, 2005.
Krishnakumar Balasubramanian and Saeed Ghadimi. Zeroth-order (non)-convex stochastic opti-
mization via conditional gradient and gradient updates. In Advances in Neural Information Pro-
cessing Systems, pp. 3455-3464, 2018.
Samuel H Brooks. A discussion of random methods for seeking maxima. Operations research, 6
(2):244-251, 1958.
Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. Zoo: Zeroth order opti-
mization based black-box attacks to deep neural networks without training substitute models. In
Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, pp. 15-26. ACM,
2017.
Krzysztof Choromanski, Mark Rowland, Vikas Sindhwani, Richard E Turner, and Adrian Weller.
Structured evolution with compact architectures for scalable policy optimization. arXiv preprint
arXiv:1804.02395, 2018.
Josip Djolonga, Andreas Krause, and Volkan Cevher. High-dimensional gaussian process bandits.
In Advances in Neural Information Processing Systems, pp. 1025-1033, 2013.
Mahdi Dodangeh and LUis N Vicente. Worst case complexity of direct search under convexity.
Mathematical Programming, 155(1-2):307-332, 2016.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.
John C Duchi, Michael I Jordan, Martin J Wainwright, and Andre Wibisono. Optimal rates for
zero-order convex optimization: The power of two function evaluations. IEEE Transactions on
Information Theory, 61(5):2788-2806, 2015.
Abraham D Flaxman, Adam Tauman Kalai, and H Brendan McMahan. Online convex optimization
in the bandit setting: gradient descent without a gradient. In Proceedings of the sixteenth annual
ACM-SIAM symposium on Discrete algorithms, pp. 385-394. Society for Industrial and Applied
Mathematics, 2005.
Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochas-
tic programming. SIAM Journal on Optimization, 23(4):2341-2368, 2013.
Eduard Gorbunov, Adel Bibi, Ozan Sener, El Houcine Bergou, and Peter Richtdrik. A stochastic
derivative free optimization method with momentum. arXiv preprint arXiv:1905.13278, 2019.
Serge Gratton, Clement W Royer, LUiS Nunes Vicente, and Zaikun Zhang. Direct search based on
probabilistic descent. SIAM Journal on Optimization, 25(3):1515-1541, 2015.
Nikolaus Hansen, Steffen Finck, Raymond Ros, and Anne Auger. Real-Parameter Black-Box Op-
timization Benchmarking 2009: Noiseless Functions Definitions. Research Report RR-6829,
INRIA, 2009.
Elad Hazan, Adam Klivans, and Yang Yuan. Hyperparameter optimization: A spectral approach.
arXiv preprint arXiv:1706.00764, 2017.
Shengqiao Li. Concise formulas for the area and volume of a hyperspherical cap. Asian Journal of
Mathematics and Statistics, 4(1):66-70, 2011.
Sijia Liu, Jie Chen, Pin-Yu Chen, and Alfred O Hero. Zeroth-order online alternating direction
method of multipliers: Convergence analysis and applications. arXiv preprint arXiv:1710.07804,
2017.
10
Published as a conference paper at ICLR 2020
Sijia Liu, Bhavya Kailkhura, Pin-Yu Chen, Paishun Ting, Shiyu Chang, and Lisa Amini. Zeroth-
order stochastic variance reduction for nonconvex optimization. In Advances in Neural Informa-
tion Processing Systems, pp. 3727-3737, 2018.
Horia Mania, Aurelia Guy, and Benjamin Recht. Simple random search provides a competitive
approach to reinforcement learning. arXiv preprint arXiv:1803.07055, 2018.
H. Brendan McMahan and Matthew J. Streeter. Adaptive bound optimization for online convex
optimization. In COLT 2010 - The 23rd Conference on Learning Theory, Haifa, Israel, June
27-29, 2010, pp. 244-256, 2010.
Yurii Nesterov and Vladimir Spokoiny. Random gradient-free minimization of convex functions.
Technical report, Universite CatholiqUe de Louvain, Center for Operations Research and Econo-
metrics (CORE), 2011.
Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram
Swami. Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM
on Asia conference on computer and communications security, pp. 506-519. ACM, 2017.
Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution strategies as a
scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864, 2017.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. CoRR, abs/1707.06347, 2017. URL http://arxiv.org/abs/
1707.06347.
Ohad Shamir. An optimal algorithm for bandit and zero-order convex optimization with two-point
feedback. Journal of Machine Learning Research, 18(52):1-11, 2017.
Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical bayesian optimization of machine
learning algorithms. In Advances in neural information processing systems, pp. 2951-2959, 2012.
Sebastian U Stich, Christian L Muller, and Bernd Gartner. Optimization of convex functions with
random pursuit. SIAM Journal on Optimization, 23(2):1284-1309, 2013.
Yining Wang, Simon Du, Sivaraman Balakrishnan, and Aarti Singh. Stochastic zeroth-order opti-
mization in high dimensions. arXiv preprint arXiv:1710.10551, 2017.
Ziyu Wang, Masrour Zoghi, Frank Hutter, David Matheson, and Nando De Freitas. Bayesian opti-
mization in high dimensions via random embeddings. In Twenty-Third International Joint Con-
ference on Artificial Intelligence, 2013.
11
Published as a conference paper at ICLR 2020
A Proofs of Section 3
Lemma 15. If h has condition number Q, thenfor allX ∈ X, there is a ballOfradius QT∣∣x 一x*k
that is tangent at X and inside the sublevel Set LX(h).
Proof. Write h = g ◦ f such that f is α-strongly convex and β-smooth for some β = Qα and g is
monotonically increasing. From the smoothness assumption, we have for any s,
f (x — 1 Vf(X) + S)
≤ f(X)+DVf(X), s -1 Vf(X)E+2 us -1 Vf(Xu
=f (X) + 2(ksk2- β ∣Vf(X)k2).
Consider the ball B = B(X - ɪVf (x), β∣∣Vf (X)I∣). For any y ∈ B, the above inequality implies
f(y) ≤ f (X). Hence, when we apply g on both sides, we still have h(y) ≤ h(X) for all y ∈ B.
Therefore, B ⊆ £：(#).
By strong convexity, ∣Vf (x)∣ ≥ α ∣ x - x * ∣. It follows that the radius of B is at leastα ∣∣X-X*∣. □
Proof of Lemma 8. Without loss of generality, consider the unit distance case where ` = 1. Further-
more, it suffices to prove for the smallest possible radius r2 = 1 - a.
Since |r1 -r2| ≤ ` ≤ r1+r2, the intersection B1∩B2is composed of two hyperspherical caps glued
end to end. We lower bound vol (B1 ∩ B2) by the volume of the cap C1 of B1 that is contained in
the intersection. Consider the triangle with sides r1, r2 and `. From classic geometry, the height of
C1 is
Cl = 1 (1 + r2 - r2) > 0.
The volume ofa spherical cap is Li (2011),
(1)
Vol(CI ) = 2vol(Bι) I1 c2(n+1,1).
2	1	2	2	2
where I is the regularized incomplete beta function defined as
Ix (a, b)
R0 ta-1 (1- t)b-1 dt
R01 ta-1(1 - t)b-1 dt
where X ∈ [0, 1] and a, b ∈ (0, ∞). Note that for any fixed a and b, Ix(a, b) is increasing in X.
Hence, in order to obtain a lower bound on Vol(Cι),we want to lower bound 1 - 患 or equivalently,
c2
upper bound -⅛.
r1
Write ri = 2√n for some α ∈ [1,2]. From Eq.(1),
1	ɑ2	1
c1	4n + 8n 32n2 .
Hence,
J =	(8 +4α - 1)
r1	16 n α	n
Since g(α) = 8 + 4ɑ is convex in [1, 2], g(a) ≤ max(g(1),g(2)) = 12. It follows that
ci ≤ i6√∕n (12 - n) ≤ 4√√n. So, 1 - ∣2 ≥ 1 - 高.To complete the proof, note that
Vn := Ii-ɪ (n+1,1) is increasing in n, and Vi = 1. As n goes to infinity, this value converges to
16n	2	2	4
1 as Bi ⊂ B2.	□
12
Published as a conference paper at ICLR 2020
ProofofLemma 7. Let V = 51Q. Let q =(1 一 V)x + νx*. Let Bq = B(Cq, rq) be a ball that has q
on its surface, lies inside Lq, and has radius rq = QTkx — χ*∣∣. Lemma 15 guarantees its existence.
Suppose that
vol (Bx ∩ Bq) ≥ 1 vol (Bx)	(2)
and that a random sample y from Bx belongs to Bq, which happens with probability at least 11.
Then, our guarantee follows by
f(y)- f(x*) ≤ f(q)- f(x*)
≤ (1 一 ν)f (x) + νf (x*) — f (x*)
≤ (1-ν)(f(x)-f(x*))
where the first line follows from Lemma 15 and second line from convexity of f.
Therefore, it now suffices to prove Eq. 2. To do so, we will apply Lemma 8 after showing that the
radius of Bx and Bq are in the proper ranges. Let ` = kx 一 cq k and note that
` ≤ kx 一 qk +rq	(3)
≤ V kx — x*k + rq = V kx — x*k + QTkq — x*k
≤ (ν + Q-1(1 — V)) kx — x*k	(4)
≤ 5Qkbx - x*k∙
Since x is outside of Bq, we also have
' ≥ Irq = QTkq — x*k = Q-1(1 — V) kx — x*k
≥ 5Q kbx - x*k.	(5)
It follows that
`3
2 ≤ 5Qkbx - x k ≤'.
In the log2 space, our choice of k1 is equivalent to starting from log2 C1 and sweeping through the
range [log2 Ci, log2 C2] at the interval of size 1. This is guaranteed to find a point between ` and ',
which is also an interval of size 1. Therefore, there exists a k1 satisfying the theorem statement, and
similarly, we can prove the existence of k2 .
Finally, it remains to show that rq ≥ (1 — 1∕(4n))'. From Eq. (3), it suffices to show that kx — qk ≤
言 or equivalently V kx — x*k ≤ 看.From Eq. (4),
kx — qk = V kx — x* k ≤ vQ(1 — v)-1'.
For any Q,n ≥ 1, 1 — V ≥ 4. So,
VQ(I- V)-i = 5n(I- V)-i ≤ 41n	⑹
and the proof is complete.	□
Proof of Lemma 9. Without loss of generality, let ` = 1 and B2 is centered at the origin with radius
r2 and B1 is centered at e1 = (1, 0, ..., 0). Then, we simply want to show that
n
Pr (1+X1)2+XXi2 ≤r22 >cn
By Markov’s inequality, we see that Pin=2 Xi2 ≤ 2r12 = 2/n with probability at most 1/2. And
since X1 is independent and r2 ≥ 1 — 1/n, it suffices to show that
Pr[(1+ Xi)2 ≤ 1 — 4/n] > Ω(1)
Since Xi has standard deviation at least rι∕√n ≥ 1∕(2n), We see that the probability of deviating
at least a few standard deviation below is at least a constant.	□
13
Published as a conference paper at ICLR 2020
Proof of Theorem 10. We can consider the projection of all points onto the column space of A and
since the Gaussian sampling process is preserved, our proof follows from applying Theorem 7 re-
Stricted onto the k-dimensional subspace and using Lemma 9 in place of Lemma 8.	□
Proofof Theorem 11. By the boundedness of h, since f(x) - f (x*) ≥ 60δkQg (A), we see that
g(PAx) - g(χ*) ≥ 60δkQg (A) - 2δ > 0. By Lemma 9, We see that if we sample from a Gaussian
2
distribution y 〜 N(x, LkI), then if z* is the minimum of g(x) restricted to the column space of A,
then
g(PAy) - g(z*) ≤ (g(PAx) - g(Z*)) (1 - 5kA )
5kQg (A)
with constant probability. By boundedness on h, we know that h(y) ≤ h(x) + 2δ. Furthermore, this
also implies that g(PAx*) ≤ g(z*) + 2δ. Therefore, we know that the decrease is at least
f(y) - f(x*) = g(PAy) - g(PAx*) + h(y) - h(x*)
≤ g(PAy) - g(z*) +2δ
≤ (g(PAx)- g(Z*)) (1- 5QA)+2δ
≤ (g(PAx)- g(PAx*) + 2δ) (1- 5kQ1(A))+2δ
≤ Sx)-f(x*)+4δ) (1- 5kQg(A))+2δ
≤ Sx)-f(X*))(1-忌A))+6δ
Since f (x) - f (x*) ≥ 10δkQg(A), we conclude that (f (x) - f (x*)) (1 - 5k@：(A)) + 6δ ≤
(f (x) - f (x*)) (1 - 10kQg(A)) and our proof is complete.	□
Proofof Theorem 12. Our main proof strategy is to show that progress can only be made with a
radius size of O(,log(nQ)/(nQ)); larger radii cannot find descent directions with high probability.
Consider a simple ellipsoid function f(x) = x>Dx, where D is a diagonal matrix and D11 ≤
D22 ≤ ... ≤ Dnn, where WLOG we let D11 = 1 and Dii = Q for i > 1. The optima is x* = 0
with f(x*) = 0.
Consider the region X = {x = (x1,x2,…,xn)∣1 ≥ xι ≥ 0.9, |xi| ≤ 0.1∕(Q√n)}. Then, if we let
V 〜 N(0, I) be a standard Gaussian vector, then for some radius r, we see that the probability of
finding a descent direction is:
Pr[f (x + rv) ≤ f (x)] = Pr (x1 + rv1)2 + X Dii (xi + rvi)2 ≤ x12 + XDiixi2
i>1	i>1
= Pr 2rx1v1 + r2v12 + Q	(2rxi vi + r2vi2 ) ≤ 0
i>1
≤ Pr 2rx1v1 ≤ -Q	2rxi vi - Qr2	vi2
i>1	i>1
=Pr v1X1 ≤ -Q X xiVi - 1 Qr X v2
i>1	i>1
14
Published as a conference paper at ICLR 2020
By standard concentration bounds for sub-exponential variables, we have
Pr | _1_ X Vi- 1| ≥ t ≤ 2e-(nT)t2/8
n -1i>1
Therefore, with exponentially high probability, Pi>1 Xii ≥ n/2. Also, since ∣x∕ ≤ 0.1∕(Q√n),
Chernoff bounds give:
Pr X xivi ≥ t ≤ 2e-50(Qt)2
i>1
Therefore, with probability at least 1 - 1∕(nQ)3, | Pi>1 ViXi | ≤ dlog(nQ)∕Q.
If Qrn ≥ Ω(,log(nQ)), then We have
-Q XViXi- 2Qr Xχ2 ≤ -Ω(Pl0g(nQ))
We conclude that the probability of descent is upper bounded by Pr V1X1≤ -Ω(Plog(nQ))].
This probability is exactly Φ(-l), where Φ is the cumulative density of a standard normal and
l = Ω(,log(nQ)). By a naive upper bound, we see that
Φ(-l) = -L= Z e-x2/2 dx
√2∏ Ji
≤ C 广 xe"∕2 dx
-l Ji
=Ce-i2∕2
l
Since l = Ω(,log(nQ)), we conclude that with probability at least 1 - 1∕poly(nQ), we have
f(y)- f (χ*) ≥ f(χ)- f(χ*).
Otherwise, we are in the case that Qrn ≤ O(,log(nQ)). Arguing simiarly as before, with high
probability, our objective function and each coordinate can change by at most O( ,log(nQ)/(Qn)).
Next, we extend our proof to any symmetric distribution D. Since D is rotationally symmetric, ifwe
parametrize V = (r, θ) is polar-coordinates, then the p.d.f. of any scaling of D must take the form
p(V) = pr(r)u(θ), where u(θ) induces the uniform distribution over the unit sphere. Therefore, if
Y is a random variable that follows D, then we may write Y = RV∕kVk, where R is a random scalar
with p.d.f pr (r) and V is a standard Gaussian vector and R, X are independent.
As previously argued, kv∣∣ ∈ [0.5n, 1.5n] with exponentially high probability. Therefore, if R ≥
Ω(∙∖∕log(nQ)∕Q), the same arguments will imply that Y is a descent direction with polynomially
small probability. Thus, when Y is a descent direction, it must be that R ≤ Ω(,log(nQ)∕Q) and
as argued previously, our lower bound follows similarly.
□
B	Proofs of Section 4
Proof of Theorem 13. By the Gaussian version of Theorem 7 (full rank version of Theorem 10),
as long as our binary search sweeps between minimum search radius r ≤ WTn∣∣x - x*k and
15
Published as a conference paper at ICLR 2020
maximum search radius of the diameter of the whole space R = kX k, the objective value will
decrease multiplicatively by 1 - 5Iq in each iteration with constant probability. Therefore, if kxt -
x*k ≥ 2Qe and We set r =言 and R = ∣∣Xk, then with high probability, We expect f (XT)-
f (x*) ≤ βQ2e2 after T = O(nQ log(∣X∣/(QE))) iterations, where we note that F = f (xo)-
f (x*) ≤ β∣∣X∣∣2 by smoothness.
Otherwise, if there exists some Xt such that	∣∣χt	-	χ*∣	≤	2Qe, then	f(XT)	-	f(x*)	≤
f(xt) - f(x*) ≤ 4βQ2E2. Therefore, by strong convexity, we conclude that in either case,
∣XT - X* ∣ ≤ 2Q3/2E. Finally note that each iteration uses a binary search that requires
O(log(R∕r)) = O(log(n∣X∣∕e)) function evaluations.
Therefore, by combining these bounds, we derive our result. The low-rank result follows from
applying Theorem 10 and Theorem 11 instead.	□
Proof of Theorem 14. Let H = O(nQ log(Q)) be the number of iterations between successive ra-
dius halving and we initialize R = ∣X ∣ and half R every H iterations. We call the iterations
between two halving steps an epoch. We claim that ∣Xi - X0 ∣ ≤ R for all iterations and proceed
with induction on the epoch number. The base case is trivial.
Assume that ∣Xi - X0 ∣ ≤ R for all iterations in the previous epoch and let iteration is be the start
of the epoch and iteration is + H be the end of the epoch. Then, since ∣Xis - X* ∣ ≤ R, we see that
f(xis) - f(x*) ≤ βR2 by smoothness. If z⅛ ≤ ∣∣Xi - x*∣ ≤ 4√QR for all i in the previous
s	4Q
epoch, then by the Gaussian version of Theorem 7 (Theorem 10), since we do a binary sweep from
4r to 4√qR, we can choose D accordingly so that we are guaranteed that our objective value
4Q
will decrease multiplicatively by 1 - 5niQ with constant probability at a cost of O(log(Q)) function
evaluations per iteration. This implies that with high probability, after O(nQ log(Q)) iterations, we
conclude
f (Xis+H)- f (X*) ≤ 4Q (f (Xis)- f (X*)) ≤ 4kxis - x*k2 ≤ 4 R2
Otherwise, there exists some 1 ≤ j ≤ H such that
llXis+j - X*k ≥ 4√QR or ||Xis+j - x*∣ ≤ 4rQ .
If it is the former, then by strong convexity, f(Xis+j) - f(X*) ≥ α∣Xis+j - X* ∣2 ≥ 2βR2, which
contradicts the fact that f (Xis ) -f(X*) ≤ βR2 by smoothness. Ifit is the latter, then by smoothness,
we reach the same conclusion:
f (Xis+亩-f (x*) ≤ f (Xis+j) - /(x*) ≤ β∣Es+j - x*II2 ≤ 4R2
Therefore, by strong convexity, we have
ll	*ll . fl X (Xis+H) - f (x* )
i"-, k≤v--------a-----
R
≤ —
-2
And our induction is complete. Therefore, we conclude that after log(∣X∣∕e) epochs, we have
∣XT - X* ∣ ≤ E. Each epoch has H iterations, each with O(log(Q)) function evaluations and so our
result follows.
The low-rank result follows from applying Theorem 10 and Theorem 11 instead. However, note that
since we do not know the latent dimension k, we must extend the binary search to incur an extra
log(n) factor in the binary search cost.	□
16
Published as a conference paper at ICLR 2020
C Figures
ʤ⅛--eE-⅛O 6O—I
Figure 2: The average optimality gap by the condition number of the objective function.
Evaluations (Thousands)
■«- GLD-Search
^∙- GLD~Fast
十ARS
dec⅛⅛E⅛O raol-
Evaluations (Thousands)
—GLD-Search
■— GLD-Fast
-毕 ARS-alpha
T ARS-beta
—t— ARS-even
Figure 3:	The average optimality gap by the accuracy of the condition number estimate, where
approximation factor is the ratio of estimated to true condition number. The dimension n = 20.
C.1 BBOB Function Plots
Figure 4:	Convergence plot for the BBOB Rastrigin Function.
Convergence Plots for Rastrigin Function
Dimension = 5
0123456789 10
Evaluations (Thousands)
Dimension = 10
0123456789 10
Evaluations (Thousands)
Dimension = 20
0123456789 10
Evaluations (Thousands)
Dimension = 40
0123456789 10
Evaluations (Thousands)
→- ARS
—∙— GLD-Fast
-B- GLD-Search
Figure 5:	Convergence plot for the BBOB BentCigar Function.
Convergence Plots for BentCigar Function
de°X4=eluao 6o-∣
0123456789 10
Evaluations (Thousands)
Dimension = 20
Evaluations (Thousands)
→- ARS
—∙— GLD-Fast
GLD-Search
17
Published as a conference paper at ICLR 2020
Figure 6:	Convergence plot for the BBOB BuecheRastrigin Function.
Convergence Plots for BuecheRastrigin Function
Dimension = 5	Dimension — 10
Dimension = 20	Dimension = 40
→- ARS
.GLD-Fast
GLD-Search
Evaluations ΓΓhousaπds) Evaluations ΓΓhousa∏ds)
Evaluations (Thousands) Evaluations ΓΓhousaπds)
Figure 7:	Convergence plot for the BBOB DifferentPowers Function.
Convergence Plots for DifferentPowers Function
de°X4=eEQdo 60-j
→- ARS
—∙— GLD-Fast
GLD-Search
Evaluations (Thousands) Evaluations (Thousands) Evaluations (Thousands) Evaluations (Thousands)
Figure 8:	Convergence plot for the BBOB Discus Function.
Convergence Plots for Discus Function
Dimension — 5
129 6 3
deX4=eEQdo 60
12
Dimension - 10
Dimension - 20
Dimension = 40
→- ARS
—GLD-Fast
GLD-Search
9
9
9
6
6
6
3
3
3
O
O
O
0123456789 10
Evaluations (ThOUSandS)
0123456789 10
Evaluations (ThOUSandS)
0123456789 10
Evaluations (Thousands)
0123456789 10
Evaluations (Thousands)
Figure 9:	Convergence plot for the BBOB Ellipsoidal Function.
Convergence Plotsfbr Ellipsoidal Function
Dimension — 5
Dimension - 10
Dimension - 20
Dimension = 40
de°X4=eluao 6o-∣
0123456789 10
Evaluations (Thousands)
0123456789 10
Evaluations (Thousands)
0123456789 10
Evaluations (Thousands)
0123456789 10
Evaluations (Thousands)
→- ARS
—∙— GLD-Fast
GLD-Search
18
Published as a conference paper at ICLR 2020
Figure 10: Convergence plot for the BBOB Katsuura Function.
Convergence Plots for Katsuura Function
deg X4=elu=do 6。一
0123456789 10
Evaluations (ThOUSandS)
Dimension = 10
Evaluations (Thousands)
Dimension = 20	Dimension = 40
→- ARS
-∙— GLD-Fast
-B- GLD-Search
Figure 11:	Convergence plot for the BBOB SchaffersF7 Function.
Convergence Plots for SchaffersF7 Function
deg X4=euφdo 6o-∣
→- ARS
-∙- GLD-FSSt
GLD-Search
Figure 12:	Convergence plot for the BBOB Ill-Conditioned SchaffersF7 Function.
Convergence Plots for SchaffersF7IIIConditioned Function
Dimension = 5	Dimension - 10	Dimension = 20	Dimension = 40
deg X4=elu=do 6。一
→- ARS
—GLD-Fast
GLD-Search
Evaluations CThousands) Evaluations CThousands) Evaluations (Thousands) Evaluations (ThOUSandS)
Figure 13:	Convergence plot for the BBOB SharpRidge Function.
Convergence Plots for SharpRidge Function
Dimension = 40
Evaluations (Thousands)
Dimension = 20
0123456789 10
Evaluations (Thousands)
Dimension = 10
0123456789 10
Evaluations (Thousands)
Dimension = 5
0123456789 10
Evaluations (Thousands)
deg X4=elu=do 6oη
→- ARS
—∙— GLD-Fast
GLD-Search
19
Published as a conference paper at ICLR 2020
Figure 14:	Convergence plot for the BBOB Weierstass Function.
Convergence Plots for Weierstass Function
Dimension = 20	Dimension = 40
Dimension = 5
4 2 0
de°x4-Ecdo 60-
0123456789 10
Evaluations (Thousands)
Dimension - 10
Evaluations (Thousands)
→- ARS
.GLD-Fast
GLD-Search
Evaluations (Thousands) Evaluations ΓΓhousaπds)
20
Published as a conference paper at ICLR 2020
C.2 Mujoco Control Plots
p」PM ①= PJfuM ①c≤
Figure 15: Plot of maximum reward so far found by algorithm. Main line is the median trajectory
across 3 runs.
2000	4000	6000	8000	10000
IOOOOO
10000
20000 40000 60000 S0000
2000	4000	6000 SOOO
Linear
Hidden Layer
Linear Projected-200
-3OOO -.
Figure 16: Example of rewards found by all samples by algorithm.
Queries
21