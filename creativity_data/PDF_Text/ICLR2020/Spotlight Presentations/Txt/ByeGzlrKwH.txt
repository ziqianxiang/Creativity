Published as a conference paper at ICLR 2020
Compression based bound for non-compressed
network: unified generalization error anal-
ysis of large compressible deep neural net-
WORK
Taiji Suzuki
Graduate School of Information Science and Technology, The University of Tokyo, Japan
Center for Advanced Intelligence Project, RIKEN, Japan
Japan Digital Design
taiji@mist.i.u-tokyo.ac.jp
Hiroshi Abe	Tomoaki Nishimura
iPride Co., Ltd., Japan,	NTT Data Corporation, Japan,
abe@ipride.co.jp,	Tomoaki.Nishimura@nttdata.com
Ab stract
One of the biggest issues in deep learning theory is the generalization ability
of networks with huge model size. The classical learning theory suggests that
overparameterized models cause overfitting. However, practically used large deep
models avoid overfitting, which is not well explained by the classical approaches.
To resolve this issue, several attempts have been made. Among them, the compres-
sion based bound is one of the promising approaches. However, the compression
based bound can be applied only to a compressed network, and it is not applicable
to the non-compressed original network. In this paper, we give a unified frame-
work that can convert compression based bounds to those for non-compressed
original networks. The bound gives even better rate than the one for the com-
pressed network by improving the bias term. By establishing the unified frame-
work, we can obtain a data dependent generalization error bound which gives a
tighter evaluation than the data independent ones.
1	Introduction
Deep learning has shown quite successful results in wide range of machine learning applications.
such as image recognition (Krizhevsky et al., 2012), natural language processing (Devlin et al., 2018)
and image synthesis tasks (Radford et al., 2015). The success of deep learning methods is mainly due
to its flexibility, expression power and computational efficiency for large dataset training. Due to its
significant importance in wide range of application areas, its theoretical analysis is also getting much
important. For example, it has been known that the deep neural network has universal approximation
capability (Cybenko, 1989; Hornik, 1991; Sonoda & Murata, 2015) and its expressive power grows
up in an exponential order against the number of layers (Montufar et al., 2014; Bianchini & Scarselli,
2014; Cohen et al., 2016; Cohen & Shashua, 2016; Poole et al., 2016; Suzuki, 2019). However,
theoretical understandings are still lacking in several important issues.
Among several topics of deep learning theories, a generalization error analysis is one of the biggest
issues in the machine learning literature. An important property of deep learning is that it general-
izes well even though its parameter size is quite large compared with the sample size (Neyshabur
et al., 2019). This can not be well explained by a classical VC-dimension type theory (Harvey
et al., 2017) which suggests that overparameterized models cause overfitting and thus result in poor
generalization ability.
For this purpose, norm based bounds have been extensively studied so far (Neyshabur et al., 2015;
Bartlett et al., 2017b; Neyshabur et al., 2017; Golowich et al., 2018). These bounds are beneficial
1
Published as a conference paper at ICLR 2020
because the bounds are not explicitly dependent on the number of parameters and thus are useful
to explain the generalization error of overparameterized network (Bartlett, 1998; Neyshabur et al.,
2015; 2019). However, these bounds are typically exponentially dependent on the number of layers
and thus tends to be loose for deep network situations (Dziugaite & Roy, 2017; Arora et al., 2018;
Nagarajan & Kolter, 2019). As a result, Arora et al. (2018) reported that a simple VC-dimension
bound (Li et al., 2018; Harvey et al., 2017) can still give sharper evaluations than these norm based
bounds in some practically used deep networks. Wei & Ma (2019) improved this issue by involving
a data dependent Lipschitz constant as performed in Arora et al. (2018); Nagarajan & Kolter (2019).
On the other hand, compression based bound is another promising approach for tight generalization
error evaluation which can avoid the exponential dependence on the depth. The complexity of deep
neural network model is regulated from several aspects. For example, we usually impose explicit
regularization such as weight decay (Krogh & Hertz, 1992), dropout (Srivastava et al., 2014; Wager
et al., 2013), batch-normalization (Ioffe & Szegedy, 2015), and mix-up (Zhang et al., 2018; Verma
et al., 2018). Zhang et al. (2016) reported that such explicit regularization does not have much
effect but implicit regularization induced by SGD (Hardt et al., 2016; Gunasekar et al., 2018; Ji &
Telgarsky, 2019) is important. Through these explicit and implicit regularizations, deep learning
tends to produce a simpler model than its full expression ability (Valle-Perez et al., 2019; Verma
et al., 2018). To measure how “simple” the trained model is, one of the most promising approaches
currently investigated is the compression bounds (Arora et al., 2018; Baykal et al., 2019; Suzuki
et al., 2018). These bounds measure how much the network can be compressed and characterize
the size of the compressed network as the implicit effective dimensionality. Arora et al. (2018)
characterized the implicit dimensionality based on so called layer-cushion quantity and suggested
to perform random projection to obtain a compressed network. Along with a similar direction,
Baykal et al. (2019) proposed a pruning scheme called Corenet and derived a bound of the size
of the compressed network. Suzuki et al. (2018) has developed a spectrum based bound for their
compression scheme. Unfortunately, all of these bounds guarantee the generalization error of only
the compressed network, not the original network. Hence, it does not give precise explanations
about why large network can avoid overfitting.
In this paper, we derive a unified framework to obtain a compression based bound for a non-
compressed network. Unlike the existing researches, our bound is valid to evaluate the original
network before compression, and thus gives a direct explanation about why deep learning general-
izes despite its large network size. The difficulty to apply the compression bound to the original
network lies in evaluation of the population L2-bound between the compression network and the
original network. A naive evaluation results in the VC-bound which is not preferable. This difficulty
is overcome by developing novel data dependent capacity control technique using local Rademacher
complexity bounds (Mendelson, 2002; Bartlett et al., 2005; Koltchinskii, 2006; Gine & Koltchinskii,
2006). Then, the bound is applied to some typical situations where the network is well compressed.
Our analysis stands on the implicit bias hypothesis (Gunasekar et al., 2018; Ji & Telgarsky, 2019)
that claims deep learning tends to produce rather simple models. Actually, Gunasekar et al. (2018);
Ji & Telgarsky (2019) showed gradient descent results in (near) low rank parameter matrices in each
layer in linear network settings. Martin & Mahoney (2018) evaluated the eigenvalue decays of the
weight matrix through random matrix theories and several numerical experiments. These observa-
tions are also supported by the flat minimum analysis (Hochreiter & Schmidhuber, 1997; Wu et al.,
2017; Langford & Caruana, 2002), that is, the product of the eigenvalues of the Hessian around the
SGD solution tends to be small, which means SGD converges to a flat minimum and possess sta-
bility against small perturbations leading to good generalization. Based on these observations, we
make use of the eigenvalue decay of the weight matrix and the covariance matrix among the nodes
in each layer (this assumption is actually verified by numerical experiments in Appendix D). The
eigenvalue decay speed characterizes the redundancy in each layer and thus is directly relevant to
compression ability. Our contributions in this paper are summarized as follows:
•	We give a unified framework to obtain a compression based bound for non-compressed net-
work which properly explains that a compressible network can generalizes well. The bound
can convert several existing compression based bounds to that for non-compressed one in
a unifying manner. The bound is applied to near low rank models as concrete examples.
•	We develop a data dependent capacity control technique to bound the discrepancy between
the original network and compressed network. As a result, we obtain a sharp generalization
2
Published as a conference paper at ICLR 2020
Table 1: Comparison of each generalization error to our bound. RF is the Frobenius norm of the
weight matrix, R2 is the operator norm of the weight matrix, Rp→q is the (p, q) matrix norm, L is the
depth, m is the maximum of the width, n is the sample size. Rn and Rr represent the Rademacher
complexity and local Rademacher complexity respectively. κ is a Lipschitz constant between layers.
α represents the eigenvalue drop rate of the weight matrix, and β represents that of the covariance
matrix among the nodes in each internal layer. r is the bias induced by compression. “Original”
indicates whether the bound is about the original network or not.
Authors	Rate	Bound type	Original
NeyShabur et al. (2015) Bartlett etal. (2017b)	2l RF √n RL (LR2→1 Y/2 ~√ ILR/3 2	Norm base Norm base	Yes Yes
Wei & Ma (2019)	(l+Lκ 3 R2→ι+Lκ 3 r2→3J	Norm base	Yes
	√n			
Neyshabur et al. (2017)	RL √L3m RI √n V	R2	Norm base	Yes
Golowich et al. (2018)	RF min{n⅛, ∖∕~ζ}	Norm base	Yes
Li etal. (2018) Harvey et al. (2017)	R∣√L2m √n	VC-dim.	Yes
Arora etal. (2018)	I L2 ]max Ib(Xi)|2 PL=I M2：2 1 ∖ n	1≤i≤n	μ'μ'― r + V	nr2 一	Compression	No
Suzuki et al. (2018)		r+qPL=1 m'+1m]`	Compression	No
Ours (Thm. 1)	rjn + Rr(F -G) + Rn(G)	General	Yes
Ours (Cor. 1)	,L(Lκ2)ι∕“ Lm	Low rank weight	Yes
	Γ~Γ	β	4∕β	Low rank weight Low rank cov.	
Ours (Thm. 4)	l∕L1+(2α-) + β (Lm)4∕β+2(IT/2a) V	n		Yes
error bound which is even better than that of the compressed network. All derived bounds
are characterized by data dependent quantities.
Other related work Recently, the role of over-parameterization for two layer networks has been
extensively studied (Neyshabur et al., 2019; Arora et al., 2019). These are for the shallow network
and the generalization error is essentially given by the norm based bounds. It is not obvious that
these bounds also give sharp bounds for deep models.
PAC-Bayes bound is also applied to obtain a non-vacuous compression based bound (Zhou et al.,
2019). However, the bound is still for the compressed (quantized) models and it is not obvious that
that bound can be converted to that for the original network.
Relation between compression and learnability was traditionally studied in a different framework as
in Littlestone & Warmuth (1986) and minimum description code length (Hinton & Van Camp, 1993).
Our bound would share the same spirits with these studies but give a new analysis by incorporating
recent observations in deep learning researches.
2 Preliminaries: Problem formulation and notations
In this section, we give the problem setting and notations that will be used in the theoretical analysis.
We consider the standard supervised leaning formulation where data consists of input x ∈ Rd and
output (or label) y ∈ R. We consider a single output setting, i.e., the output y is a 1-dimensional real
value, but it is straight forward to generalize the result to a multiple output case. Suppose that we
are given n i.i.d. observations Dn = (xi, yi)in=1 distributed from a probability distribution P. To
measure the performance of a trained function f, we use a loss function ψ : R × R → R and define
3
Published as a conference paper at ICLR 2020
a training error and its expected one as
1n
ψ(f) ：= n^ψ(yi,f(χi)), "(f)：= E[ψ(γ,f(χ))],
where the expectation is taken with respect to (X, Y)〜P. Basically, We are interested in the
generalization error Ψ(f) - Ψ(f) for an estimator f. We denote the empirical L2-norm by kfkn :=
VPn=ι f (zi)2∕n for an empirical observation Zi = (Xi,yi) (i = 1,... ,n). The population L?-
norm is denoted by ||f|匕:= PEZ〜p[f (Z)2].
This paper deals with deep neural networks as a model. The activation function is denoted by η
which will be assumed to be 1-Lipschitz as satisfied by ReLU (Assumption 1). Let the depth of
the network be L and the width of the `-th internal layer be m` (` = 1, . . . , L + 1) where we set
m1 = d (dimension of input) and mL+1 = 1 (dimension of output) for convention. Then, the set of
networks having depth L and width m = (m1 , . . . , mL ) with norm constraint as
NN(m, R2,RF) := {f (x) = G ◦ (W(L)η(∙)) ◦ (W(LT)η(∙)) ◦…◦ (W⑴x) |
W(') ∈ Rm'×m'+1, kW(')∣∣2 ≤ R, kW(')∣∣F ≤ RF 0∙
where kWk2 := supu:kW uk6=0 kW uk/kuk1 is the operator norm (the maximum singular value),
kWkF := yPi,j Wi,j
is the Frobenius norm, and G is the “clipping” operator that is defined by
G(x) = max{-M, min{x, M}}for a constant M. The reason why we put the clipping operator G
on top of the last layer is because the clipping operator restricts the L∞-norm by a constant M and
then we can avoid unrealistically loose generalization error. Note that the clipping operator does not
change the classification error for binary classification. We express F to represent the “full model”:
F = NN(m, R2, RF) for a given R2, RF > 0. Here, we implicitly suppose that R2 is close to 1
so that the norm of the output from internal layers is not too much amplified, while RF could be
moderately large.
The Rademacher complexity is the typical tool to evaluate the generalization error on a function class
F0, which is denoted by Rn(F0) := Ee [supf∈fo 1 Pn=I Gf(Zi) | Dn] where Dn = (zi)‰ι =
(xi, yi)in=1, and i (i = 1, ...n) is an i.i.d. Rademacher sequence (P (i = 1) = P(i = -1) =
1/2). This is also called conditional Rademacher complexity because the expectation is taken con-
ditioned on fixed Dn. Its expectation with respect to Dn is denoted by Rn(F0) := EDn[Rn(F0)].
Roughly speaking the Rademacher complexity measures the size of the model and it gives an upper
bound of the generalization error (Vapnik, 1998; Mohri et al., 2012).
The main difficulty in generalization error analysis of deep learning is that the Rademacher com-
plexity of the full model F is quite large. One of the successful approaches to avoid this difficulty
is the compression based bound (Arora et al., 2018; Baykal et al., 2019; Suzuki et al., 2018) which
measures how much the trained network f can be compressed. If the network can be compressed
to much smaller one, then its intrinsic dimensionality can be regarded as small. To describe it more
^
precisely, suppose that the trained network f is included in a subset of the neural network model:
R 一手一 Th	I 手
f ∈ F ⊂ F. For example, F
can be a set of networks with weight matrices that have bounded
norms and are near low rank (Sec. 3.1 or Sec. 3.2). We do not assume a specific type of training
procedure, but we
give a uniform bound valid for any estimator f that falls into F and satisfies the
^ ^
ollowing compressibility condition. We suppose that the network f is easy to compress, that is, f
can be compressed to a smaller network gb which is included in a submodel: gb ∈ G . For example,
G can be a set of networks with a smaller size than f. How small the trained network f can be
compressed has been characterized by several notions such as “layer-cushion” (Arora et al., 2018).
Typical compression based bounds give generalization errors of the compressed model gb, not the
original network f. Our approach converts an error bound of gbto that of f and eventually obtains a
tighter evaluation.
IInthiSPaPer, k∙∣∣ denotes the Euclidean norm: ∣∣uk = ，Pg U.
4
Published as a conference paper at ICLR 2020
The biggest difficulty for transforming the compression bound to that of f lies in evaluation of the
population L2-norm between f and gb. Basically, the compression based bounds are given as
ʌ ^ ^ _ ʌ
Ψ(b) ≤ Ψ(f)+ kf - bkn + CRn(G),	(1)
for a constant C > 0 under some assumptions (Table 1). The term kf - gbkn appears to adapt the
empirical error of f to that of gb, that is called “compression error” which can be seen as a bias term.
We see that, in the right hand side, there appears the complexity of G which is assumed to be much
smaller than that of the full model F . However, the left hand side is not the expected error of f but
that of g. One way to transfer this bound to that of f is that We have ∣Ψ(b) - Ψ(f )| ≤ kg - f 口工？
by assuming Lipschitz continuity of the loss function and then convert the bound (1) to
^ ʌ ^ ^ ^ _ ʌ
Ψ(f) ≤ Ψ(f) + (kf - bkn + kf - bkL2 ) + Rn(G).
However, to bound the term kf - gbkn + kf - gbkL2 , there typically appears the complexity of the
model Fb which is larger than the compressed model G like ∣∣f - bkn, ≤ Jkf- bkL? + Op(R(F)),
which results in slow convergence rate. To overcome this difficulty, we need to carefully control
the difference between the training and test error off and gbby utilizing the local Rademacher com-
plexity technique (Mendelson, 2002; Bartlett et al., 2005; Koltchinskii, 2006; Gine & Koltchinskii,
2006). The local Rademacher complexity of a model F0 with radius r > 0 is defined as
• .. —.- ....	-`
Rr (F0):= Rn ({f ∈F0∣kfkL2 ≤r}).
The main difference from the standard Rademacher complexity is that the model is localized to a set
of functions satisfying kf kL2 ≤ r. As a result, we obtain a tighter error bound.
Throughout this paper, we always assume the following assumptions. Let PX and PY denote the
marginal distribution ofx and that ofy respectively.
Assumption 1 (Lipschitz continuity of loss and activation functions). The loss function ψ is 1-
Lipschitz continuous with respect to the function output:
∣ψ(y,u) - Ψ(y,u0)∣ ≤ |u - u0∣ (∀y ∈ SuPP(PY), u,u0 ∈ R).
The activation function η is also 1-Lipschitz continuous: kη(u) - η(u0)k ≤ ku - u0k (∀u ∈ Rd0)
where d0 is any positive integer.
Assumption 2. The norm of input is bounded by Bx > 0: kxk ≤ Bx (∀x ∈ suPP(PX )).
Assumption 3. The L∞-norms of all elements in F and G are bounded by M ≥ 1: kfk∞, kgk∞ ≤
M for all f ∈ Fb and g ∈ Gb.
This assumption can be ensured by applying the clipping operator G to the output of the functions.
In this paper, all the variables L, m`, R2 , RF, M, Bx are supposed to be o(n). What we will derive
in the following is a bound which has mild dependency on the depth L and depends on the width
(m`)L=ι in a sub-linear order by using the compression based approach.
Existing bounds for no-compressed network Here we give a brief review of the generaliza-
tion error bound for non-compressed models. (i) VC-bound: The Rademacher complexity of the
full model F can be bounded by a naive VC-dimension bound (Harvey et al., 2017) which is
Rn(F) = O (JLPg=Inm'm'+1 log(n)) . In this bound, there appears the number of parame-
ters Pn=ι m`m`+1 in the numerator. However, the number of parameters is often larger than
the sample size n in practical use. Hence, this bound is not appropriate to evaluate generaliza-
tion ability of overparameterized networks. (ii) Norm-based bound: Golowich et al. (2018) showed
the norm based bound which is given as Rn (F) = O (J LRL). However, this is exponentially
dependent on the depth as RFL resulting in quite loose bound. Neyshabur et al. (2017) showed
a norm based bound of Rn(F) = O ( JL3(maχ'mQRF/ Rjr) which avoids the exponential de-
pendency. However, there is still dependency on the width, p` m'RF, which is larger than the
5
Published as a conference paper at ICLR 2020
linear order of the width since RF could be moderately large. Bartlett et al. (2017b) showed
Rn(F )
O
R2/3
L R2→1
L r2/3
R2
3/2
The norm constraint on R2→1 implicitly assumes sparsity on
the weight matrix and R2→1 typically depends on the width linearly. Wei & Ma (2019) improved
the exponential dependency R2L appearing in this bound (Bartlett et al., 2017b) to obtained a bound
O (√n(1 + Lκ3 R2→3ι + Lκ3R2→ι) / ) where K is the LiPschitz continuity between layers.
We can see that R22 *→1 and R12→1 can depend on the width linearly and quadratically respectively
even though R2 is bounded.
3 Compression bound for noncompressed network
Here, we give a general theoretical tool that converts a comPression based bound to that for the
original network f . We suPPose the model classes F and G are fixed indePendently on each data
observation2. For sets of functions, F0 and G0 , we denote the Minkowski difference of them by
F0 - G0 := {f - g | f ∈ F0, g ∈ G0}. We assume that the local Rademacher comPlexity of F - G
has a concave shaPe with resPect to r > 0: SuPPose that there exists a function φ : [0, ∞) → [0, ∞)
such that
• , ^ ^. .,. ., ........................... 、
Rr(F -G) ≤ φ(r) and φ(2r) ≤ 2φ(r) (∀r > 0).
This condition is not restrictive, and usual bounds for the local Rademacher comPlexity satisfy this
condition (Mendelson, 2002; Bartlett et al., 2005). Using this notation, we define r* = r* (t) as
r*(t) := inf
(r> 0∣8 * + M r4- + M2 ɪ ≤ 1).
r2	r2 n	r2 n	2
(2)
This is roughly given by the fixed Point of a function r2 7→ φ(r), and itis useful to bound the ratio of
the emPirical L2-norm and the PoPulation L2-norm of an element h in Fb-Gb: khk2L /(khk2n + r*2) ≤
1/2 with high probability. Finally, We denote ψ(F0) := {ψ(∙,f(∙)) | f ∈ F0} for a set F0 of
functions. Then, we obtain the following theorem that gives the comPression based bound for non-
compressed networks.
Theorem 1. Suppose that the empirical L?-distance between f and g is bounded by ∣∣f — g∣∣n ≤ r2
for a fixed r > 0 almost surely. Let Ir := <2(r2 + r2), then, under Assumptions 1, 2, 3, there exists
a universal constant C > 0 such that
^
Ψ(fb) ≤
ψ⑺ + 2Rn(G) + M Mn +C
'--------------------}
{^^"∖lf^^^
main term
t 1 + tM
Rr (Ψ(F)- Ψ(G))+ " - + ——
nn
{^^^^^^^^^^^^^^^^^^^^^
bias term
|
}
with probability at least 1 - 3e-t for all t ≥ 1.
The proof is given in Appendix A. The bound consists of two terms: “main term” and “bias term.”
The main term represents the complexity of the compressed model G which could be much smaller
than F. The bias term represents a sample complexity to bridge the original model and the com-
pressed model. Typically We have r2 = o(1∕√n), and if We set r = Op(1), then the bias term can
be faster than the main term which is O(1∕√n). The term Rr (ψ(F) - ψ(G)) can be refined a little
bit and the refined term can be evaluated by using the covering number of the model. The refined
version is given in Appendix A (Theorem 5). This bound is general, and can be combined with the
compression bounds derived so far such as Arora et al. (2018); Baykal et al. (2019); Suzuki et al.
(2018) where the complexity of G and the bias r are analyzed for their generalization error bounds.
The main difference from the compression bound (1) for g is that the bias term r = ∣∣f - g∖∣n is
replaced by √1n ∣∣f - g∖∣n which is √n times smaller. Since r2 is typically o(1∕√n) and Rr (ψ(F)-
2 We can extend the result to data dependent models Fb and Gb by taking uniform bound for all possible
choice of the pair F and G . However, we omit explicit presentation of this uniform bound for simplicity.
6
Published as a conference paper at ICLR 2020
I /分 ∖ ∖	F	1 ∙ .1	1	.1	∙	∕' . -I	. . ∙	ʌ	∙ . 1
Ψ(G)) can be made in the same order as the main term or even faster by setting r appropriately, We
may neglect these terms. Then, the bound is informally written as
ψ(b) ≤ Wf) + Op (Rn(G) + √n kf - bkn + VnnJ .
This allows us to obtain tighter bound than the compression bound for g because the bias term r/√n
is much smaller than r and eventually we can let the variance term Rn(G) much smaller by taking
small compressed model G when we balance the bias and variance trade-off. This is an advantageous
point of directly bounding the generalization error of f instead of gb.
Finally, we note that some existing bounds such as Arora et al. (2018); Bartlett et al. (2017b); Wei
& Ma (2019) assumes a constant margin so that the bias term can be a sufficiently small constant
(which does not need to converge to 0). On the other hand, our bound does not assume it and the
bias term should converge to 0 so that the bias is balanced with the variance term, which is a more
difficult problem setting.
Example 1. In practice, a trained network can be usually compressed to one with sparse weight
matrix via pruning techniques (Denil et al., 2013; Denton et al., 2014). Based on this observation,
Baykal et al. (2019) derived a compression based bound based on a pruning procedure. In this
situation, we may suppose that G is the set of networks with S non-zero parameters where S is much
smaller than the total number of parameters: G = {f ∈ NN(m, R2,Rf) | PL=I ∣∣W (')ko ≤ S}
where ∣∣ W(')∣o is the number of nonzero parameters of the weight matrix W('). In this situ-
ation, its Rademacher complexity is bounded by
R(G) ≤ CMdLS log(n) (see Appendix B.2
for the proof). This is much smaller than the VC-dimension bound
S《Pn=I m`m'+ι.
JL Pn=『me+1 iog(n if
Although our bound can be adopted to several compression based bounds, we are going to demon-
strate how small the obtained bound can be through some typical situations in the following.
3.1 Compression bound with near low rank weight matrix
Here, we analyze the situation where the trained network has near low rank weight matrices
(W"ι. It has been reported that the trained network tends to have near low rank weight matri-
ces experimentally (Gunasekar et al., 2018; Ji & Telgarsky, 2019) (see Appendix D for the empirical
verification). This situation has been analyzed in Arora et al. (2018) where the low rank property
is characterized by their original quantities such as layer cushion. However, we employ a much
simpler and intuitive condition to highlight how the low rank property affects the generalization.
Assumption 4. Assume that each ofweight matrices W (') (' = 1,...,L) ofany f ∈ F is near low
rank, that is, there exists α > 1/2 and V0 > 0 such that
σj(W⑶)≤ V0j-α,
where σj (W) is the j -th largest singular value of a matrix W (σι(W) ≥ σ2(W) ≥ ∙∙∙ ≥ 0).
In this situation, we can see that for any 1 ≤ S ≤ min{m', m'+ι}, we can approxi-
mate W(') by a rank S matrix W0 as ∣∣W(') 一 W0∣∣2 ≤ V0s-α. Let the set of networks
with exactly low rank weight matrices be NN(m, s, R2, RF) := {f ∈ NN(m, R2, RF) |
the weight matrix W (') of f has rank sg} for S = (si,..., sl). If we set G = NN(m, s, R2,Rf),
then we have the following theorem.
Theorem 2. The compressed model G = NN(m, s, R2, RF) has the following complexity:
Rn(G) ≤CM∖∣LP=^m' + m'+i)log(n).
n
If JF satisfies Assumption 4, we can set r = VqRLTBx PL=I s-α: for any f ∈ Fb, there ex-
ists b ∈ G such that ∣∣f 一 b∣n ≤ r. Then, letting Ai = LP'=1 s'(m'+m'+1) log(n) and
7
Published as a conference paper at ICLR 2020
A2 = L (P'=1 m`)(2LVOR2—Bx) / , the overall generalization error is bounded by
^ ʌ ^	2 α — 1 —2α—	/~^^τ- 二—r-	i--	1 + t^M
Ψ(f) ≤ Ψ(f) + C MA1 + M2O+TA2+2α + WQaA2 + (r + M) √A? + —— ,
2n
with probability 1 - 3e-t for any t > 1 where C > 0 is a constant depending on α.
See Appendix B.3 for the proof. This indicates that, if α > 1/2 is large (in other words, each
weight matrix is close to rank 1), then we have a better generalization error bound. Note that the
rank s` can be arbitrary chosen and r and Ai are in a trade-off relation. Hence, by selecting the
rank appropriately so that this trade-off is balanced, then we obtain the optimal upper bound as in
the following corollary.
Corollary 1. Under Assumption 4, using the same notation as Theorem 2, it holds that
Ψ(f) ≤ Ψ(f) + C [m 1T2aTLPL=I m')(2LVORLTBx)T/。Iog(n) + M2α+τA2α+τ + 1+1M
n	2n
with probability 1 - 3e-t for any t > 1 where C is a constant depending on α.
An important point here is that the bound is O( JL P'=τ m') which has linear dependency on
the width m` in the square root, but the naive VC-dimension bound has quadratic dependency
O( JL PgTnm'm'). In other words, the term in the square root has linear dependency to the number
of nodes instead of the number of parameters. This is huge gap because the width can be quite large
in practice. This result implies that a compressible model achieves much better generalization than
the naive VC-bound.
In the generalization error bound, there appears R2L . Even though R2 can be much smaller than
RF, the exponential dependency R2L can give loose bound as pointed out in Arora et al. (2018).
This is due to a rough evaluation of the Lipschitz continuity between layers, but the practically
observed Lipschitz constant is usually much smaller. To fix this issue, we give a refined version of
Corollary 1 in Appendix B.4 by using data dependent Lipschitz constants such as interlayer cushion
and interlayer smoothness introduced by Arora et al. (2018). The refined bound does not involve the
exponential term R2L, but instead κ2 (κ: Lipschitz continuity) appears.
3.2 Compression bound with near low rank covariance matrix
Strictly speaking, the near low rank condition on the weight matrix in the previous section can be
dealt with a standard Rademacher complexity argument. Here, we consider more data dependent
bound: We assume the near low rank property of the covariance matrix among the nodes in an
internal layer (see Appendix D for the empirical verification). A compression based bound for gb
using the low rank property of the covariance has been studied by Suzuki et al. (2018), but their
analysis requires a bit strong condition on the weight matrix. In this paper, we employ a weaker
assumption.
Let Σ(`) = 1 pn=ι φ'(χi)φ'(χi)> be the covariance matrix of the nodes in the '-th layer where
φ'(x) = η ◦ (W('-1)η(∙)) ◦•••◦ (W⑴x).
Assumption 5. Suppose that the trained network f satisfies the following conditions:
σj(∑(`)) ≤ μj') =： Uoj-β,
for a fixed β > 1 and U0 > 0.
(3)
]
If f satisfies this assumption, then we can show that f can be compressed to a smaller one f] that
has width (m')L=2 with compression error roughly evaluated as ∣∣f - f ]kn . p`(m')-β/2. More
precisely, for given r` > 0 (' = 1,...,L) which corresponds to the compression error in the '-
th layer, let m` := max{1 ≤ j ≤ m` | "j') ≥ r2∕4}. Then, We define N'(r) = β+ m` +
8(PkT R2 彳2——RFrkL, for r = (ri,... ,txl). Correspondingly We set
m' := 5N'(r) log(80Ne(r)).
Then, we obtain the following theorem.
8
Published as a conference paper at ICLR 2020
Theorem 3. Let r := PL=I R"-kRf^. Then, under Assumption 5, there exists b with width
m]
(m1, m]2, . . . , m]L) that satisfies gb ∈
NN(m], J230 max' m`R2 yβ2f
max' m'Rp) and
..^ ,..
kf - gkn ≤ r.
In particular, we may set
b = NN(m],信max'm'R2, q20
max' m'RF),
and then it holds that
Rn(G) ≤ CyL PL= m'+1m' log(n) for a constant C > 0.
See Appendix B.5 for the proof. Here, we again observe that there appears a trade-off between
r and m' because as r` becomes small, then m` becomes large and thus m' becomes large. The
evaluation given in Theorem 3 can be substituted to the general bound (Theorem 1). If F is the full
model F, then there appears the number P3 m`m`+1 of parameters which could be larger than
n, which is unavoidable. This dependency on the number of parameters becomes much milder if
both of Assumptions 4 and 5 are satisfied.
Theorem 4. Under Assumptions 4 and 5, it holds that
Ψ(fb) ≤ Ψb(fb) +C
-	1+—Y-----
1	4α__I o
M	[Pl V Ql] L	(2α-1)+β
n
4∕β
∖ 4∕β + 2(1-1∕2α)
m'	log(n)3
2α	-----
+ M2α+1 (LPLp'=1m' log(n)) K + M等 Jlo⅛n3 + 1+Mt
for PL = (2LKRLTBx)1/a and QL
4UoRF(1VR2)l exp(4(2√L-1))
(0.25)4 (1∧R2)2l
2∕β
with probability 1 -
3e-t (t > 1), where C is a constant depending on α, β.
Ifwe omit L and log(n) terms for simplicity of presentation, then the bound can be written as
where the O(∙) symbol hides the poly-log order. This is tighter than that of Corollary 1. We can see
that as β and a get large, the bound becomes tighter. Actually, by taking the limit of α,β → ∞, then
the bound goes to L2Jlonn) + L P'=n m'. Moreover, the term dependent on the width is O(1∕n)
with respect to the sample size n which is faster than the rate
O( √p'⅛m')
which was presented in
Corollary 1. Hence, the low rank property of both the covariance matrix and the weight matrix helps
to obtain better generalization. Although the bound contains the exponential term R2L, we can give
a refined version that does not contain the exponential term by assuming interlayer cushion (Arora
et al., 2018). See Appendix B.6 for the refined version.
There appears exp( 1 (2√L - 1)) which is exponentially dependent on L. However, this term is
moderately small for realistic settings of the depth L. Actually, it is 7.27 for L = 20 and 26.7 for
L = 50 (We can replace this term in exchange for larger polynomial dependency on L). The bound is
not optimized with respect to the dependency on the depth L. In particular, the term L2 ʌ/log(n)/n
could be an artifact of the proof technique and the L2 term would be improved.
Finally, we compare our bound with the following norm based bounds; O
by Bartlett et al. (2017b) and O (√n(1 + Lκ4 R2→ι + K2LR2→, / ) by Wei & Ma (2019).
Since our bound and their bounds are derived from different conditions, we cannot tell which is
better. Here, we consider a special case where m` = m (∀') and W(')=* 11> ∈ Rm×m which
is an extreme case of low rank settings (note that W(`)has rank 1). Then, R2 = 1, RF = 1,
9
Published as a conference paper at ICLR 2020
R2→1 = √m and R1→1 = m, and thus their bounds are O(，m/n) and O(，(m + m2)∕n)
respectively. However, β and α in our bound
4~	4∕β	r~~
m m 4∕β + 2(1-1∕2α) /n
m mi+e(i-i1/2a)/2 /n can be
arbitrary large in this situation, so that our bound has much milder dependency on the width m. On
the other hand, if the weight matrix has small norm and has no spectral decay (corresponding to
small α and β), then our bound can be looser than theirs. Combining compression based bounds
and norm based bounds would be interesting future work.
4 Conclusion
In this paper, we derived a compression based error bound for non-compressed network. The bound
is general and it can be adopted to several compression based bound derived so far. The main dif-
ficulty lies in evaluating the population L2-norm between the original network and the compressed
network, but it can be overcome by utilizing the data dependent bound by the local Rademacher
complexity technique. We have applied the derived bound to a situation where low rank properties
of the weight matrices and the covariance matrices are assumed. The obtained bound gives much
better dependency on the parameter size than ever obtained compression based ones.
Acknowledgment We thank the anonymous reviewers for their valuable comments. TS was par-
tially supported by MEXT Kakenhi (15H05707, 18K19793 and 18H03201), Japan Digital Design,
and JST-CREST, Japan .
5 Proof outline of Theorem 1
C	.	.	.	.	1	ʌ ∙	..	1	.....	1	...	1	一
Remember that for a trained network f, gb is its compressed version that is included in a submodel
G . First, we decompose the generalization gap as
^ ʌ ^.	^ ʌ ^ ʌ ʌ
ψ(f)- ψ(f) = [(ψ(f) - ψ(b)) - (ψ(f) - ψ(b))] + (ψ(b) - ψ(b)).	(4)
The second block in the right hand side is easy to bound, i.e., by applying the standard Rademacher
complexity bound (Theorem 3.1 of Mohri et al. (2012)) with the contraction inequality (Theorem
11.6 of Boucheron et al. (2013) or Theorem 4.12 of Ledoux & Talagrand (1991)), it holds that
ψ(b)- ψ(g) ≤ 2Rn(Q)+qw,
t
with probability 1 - e-t. Since G is a small model, this bound could be much smaller than a naive
VC-dimension bound. The first block “bridges” the generalization gap ofgbto thatoff, but bounding
the first block is more involved. We make use of the local Rademacher complexity to bound the term.
Suppose that the event in which ∣∣f - g∣∣L2 ≤ r holds has high probability (which should be proven
later), then it is also expected that ψ(y, f) - ψ(y, g) has small L2-norm. This is true because of the
Lipschitz continuity assumption (Assumption 1). Actually, the Talagrand’s concentration inequality
yields that
ψ(f) - ψ(b) - (ψ(f) - ψ(b)) ≤ c [Φ(r)+qrn+1+M i,
for Φ(r) := Rn({ψ(f) - ψ(g) | f ∈ F,b ∈ b, kf - g∣L2 ≤ r}) with probability 1 - e-t.
Since Φ(r) requires the restriction ∣∣f - g∣∣L2 ≤ r, this quantity is much smaller than the standard
Rademacher complexity Rn (ψ(F) - ψ(G )), which yields fast convergence rate.
τ-,∙ 11	1 111	1 . 1	FF ∙ 1 ∙ . CUR ^ll	,
Finally, we should bound the probability of ∣f - gb∣L2 ≤
ratio type empirical process. Actually, we can show that
r. This can be bounded by utilizing the
P
(P - Pn )(h2)
Ph + r2
≥
2	≤ e-t
for r* defined in Eq. (2), where Pnf := 1 Pn=ι f(zi) and Pf = E[f (Z)]. This yields that
kf- bkL2 -kf'- b∣n ≤ Iqf-bkL2 + r2) with probability 1 - e-t and equivalently ∣f-b∣L2 ≤
2(kf- bl∣n + r2). Since ∣f - bkn ≤ ^ a.s., we have ∣f - b∣L2 ≤ 2(r2 + r2) = r2.
5 T	1	.1 . ɪ / ∙ ∖ ∕∙A// / T^∖	ʃ / 分、∖ 1	♦	.< ɪ ∙	< ∙ .	. ∙	J Z *
We can show that Φ(r) ≤ Rr(ψ(F) - ψ(G)) by using the Lipschitz continuity assumption (AS-
sumption 1). Then, we obtain the assertion.
10
Published as a conference paper at ICLR 2020
References
S. Arora, R. Ge, B. Neyshabur, and Y. Zhang. Stronger generalization bounds for deep nets via
a compression approach. In J. Dy and A. Krause (eds.), Proceedings of the 35th International
Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp.
254-263, Stockholmsmassan, Stockholm Sweden, 10-15 Jul 2018. PMLR.
S. Arora, S. S. Du, W. Hu, Z. Li, and R. Wang. Fine-Grained Analysis of Optimization
and Generalization for Overparameterized Two-Layer Neural Networks. arXiv e-prints, art.
arXiv:1901.08584, Jan 2019.
F. Bach. On the equivalence between kernel quadrature rules and random feature expansions. Jour-
nal of Machine Learning Research, 18(21):1-38, 2017.
P. Bartlett, O. Bousquet, and S. Mendelson. Local Rademacher complexities. The Annals of Statis-
tics, 33:1487-1537, 2005.
P. L. Bartlett. The sample complexity of pattern classification with neural networks: the size of
the weights is more important than the size of the network. IEEE Transactions on Information
Theory, 44(2):525-536, March 1998.
P. Bartlett, D. J. Foster, and M. Telgarsky. Spectrally-normalized margin bounds for neural networks.
arXiv preprint arXiv:1706.08498, 2017a.
P. L. Bartlett, D. J. Foster, and M. J. Telgarsky. Spectrally-normalized margin bounds for neural
networks. In Advances in Neural Information Processing Systems, pp. 6241-6250, 2017b.
C. Baykal, L. Liebenwein, I. Gilitschenski, D. Feldman, and D. Rus. Data-dependent coresets
for compressing neural networks with applications to generalization bounds. In International
Conference on Learning Representations, 2019.
M.	Bianchini and F. Scarselli. On the complexity of neural network classifiers: A comparison
between shallow and deep architectures. IEEE transactions on neural networks and learning
systems, 25(8):1553-1565, 2014.
S. Boucheron, G. Lugosi, and P. Massart. Concentration Inequalities: A Nonasymptotic Theory of
Independence. OUP Oxford, 2013.
O. Bousquet. A Bennett concentration inequality and its application to suprema of empirical process.
C. R. Acad. Sci. Paris Ser. I Math., 334:495-500, 2002.
N.	Cohen and A. Shashua. Convolutional rectifier networks as generalized tensor decompositions.
In Proceedings of the 33th International Conference on Machine Learning, volume 48 of JMLR
Workshop and Conference Proceedings, pp. 955-963, 2016.
N.	Cohen, O. Sharir, and A. Shashua. On the expressive power of deep learning: A tensor analysis.
In The 29th Annual Conference on Learning Theory, pp. 698-728, 2016.
G. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control,
Signals, and Systems (MCSS), 2(4):303-314, 1989.
M. Denil, B. Shakibi, L. Dinh, M. A. Ranzato, and N. de Freitas. Predicting parameters in deep
learning. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger (eds.),
Advances in Neural Information Processing Systems 26, pp. 2148-2156. Curran Associates, Inc.,
2013.
E. L. Denton, W. Zaremba, J. Bruna, Y. LeCun, and R. Fergus. Exploiting linear structure within
convolutional networks for efficient evaluation. In Z. Ghahramani, M. Welling, C. Cortes, N. D.
Lawrence, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 27,
pp. 1269-1277. Curran Associates, Inc., 2014.
J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of Deep Bidirectional
Transformers for Language Understanding. arXiv e-prints, art. arXiv:1810.04805, Oct 2018.
11
Published as a conference paper at ICLR 2020
G. K. Dziugaite and D. M. Roy. Computing nonvacuous generalization bounds for deep (stochastic)
neural networks with many more parameters than training data. In Proceedings of the Thirty-Third
Conference on Uncertainty in Artificial Intelligence, 2017.
E. Gine and V. Koltchinskii. Concentration inequalities and asymptotic results for ratio type empir-
ical processes. TheAnnalsofProbabilily, 34(3):1143-1216, 2006.
N. Golowich, A. Rakhlin, and O. Shamir. Size-independent sample complexity of neural networks.
In S. Bubeck, V. Perchet, and P. Rigollet (eds.), Proceedings of the 31st Conference On Learning
Theory, volume 75 of Proceedings of Machine Learning Research, pp. 297-299. PMLR, 06-09
Jul 2018.
S. Gunasekar, J. D. Lee, D. Soudry, and N. Srebro. Implicit bias of gradient descent on linear
convolutional networks. In Advances in Neural Information Processing Systems, pp. 9482-9491,
2018.
M.	Hardt, B. Recht, and Y. Singer. Train faster, generalize better: Stability of stochastic gradient
descent. In M. F. Balcan and K. Q. Weinberger (eds.), Proceedings of The 33rd International
Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pp.
1225-1234, New York, New York, USA, 20-22 Jun 2016. PMLR.
N.	Harvey, C. Liaw, and A. Mehrabian. Nearly-tight VC-dimension bounds for piecewise linear neu-
ral networks. In S. Kale and O. Shamir (eds.), Proceedings of the 2017 Conference on Learning
Theory, volume 65 of Proceedings of Machine Learning Research, pp. 1064-1068, Amsterdam,
Netherlands, 07-10 Jul 2017. PMLR.
G.	Hinton and D. Van Camp. Keeping neural networks simple by minimizing the description length
of the weights. In in Proc. of the 6th Ann. ACM Conf. on Computational Learning Theory.
Citeseer, 1993.
S. Hochreiter and J. Schmidhuber. Flat minima. Neural Computation, 9(1):1-42, 1997.
K. Hornik. Approximation capabilities of multilayer feedforward networks. Neural Networks, 4(2):
251-257, 1991.
S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing
internal covariate shift. In F. Bach and D. Blei (eds.), Proceedings of the 32nd International
Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp.
448-456, Lille, France, 07-09 Jul 2015. PMLR.
Z. Ji and M. Telgarsky. Gradient descent aligns the layers of deep linear networks. In International
Conference on Learning Representations, 2019.
V. Koltchinskii. Local Rademacher complexities and oracle inequalities in risk minimization. The
Annals of Statistics, 34:2593-2656, 2006.
A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional
neural networks. In Advances in neural information processing systems, pp. 1097-1105, 2012.
A. Krogh and J. A. Hertz. A simple weight decay can improve generalization. In Advances in neural
information processing systems, pp. 950-957, 1992.
J. Langford and R. Caruana. (not) bounding the true error. In T. G. Dietterich, S. Becker, and
Z. Ghahramani (eds.), Advances in Neural Information Processing Systems 14, pp. 809-816. MIT
Press, 2002.
M.	Ledoux and M. Talagrand. Probability in Banach Spaces. Isoperimetry and Processes. Springer,
New York, 1991. MR1102015.
X. Li, J. Lu, Z. Wang, J. Haupt, and T. Zhao. On tighter generalization bound for deep neural
networks: Cnns, resnets, and beyond. arXiv preprint arXiv:1806.05159, 2018.
N.	Littlestone and M. K. Warmuth. Relating data compression and learnability. Technical report,
University of California, Santa Cruz, 1986.
12
Published as a conference paper at ICLR 2020
C. H. Martin and M. W. Mahoney. Implicit self-regularization in deep neural networks: Evidence
from random matrix theory and implications for learning. arXiv preprint arXiv:1810.01075, 2018.
S. Mendelson. Improving the sample complexity using global data. IEEE Transactions on Informa-
tion Theory, 48:1977-1991, 2002.
M. Mohri, A. Rostamizadeh, and A. Talwalkar. Foundations of machine learning. 2012.
G. F. Montufar, R. Pascanu, K. Cho, and Y. Bengio. On the number of linear regions of deep neural
networks. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Weinberger (eds.),
Advances in Neural Information Processing Systems 27, pp. 2924-2932. Curran Associates, Inc.,
2014.
V. Nagarajan and Z. Kolter. Deterministic PAC-Bayesian generalization bounds for deep net-
works via generalizing noise-resilience. In International Conference on Learning Representations
(ICLR2019), 2019.
B. Neyshabur, R. Tomioka, and N. Srebro. Norm-based capacity control in neural networks. In
Proceedings of The 28th Conference on Learning Theory, pp. 1376-1401, Montreal Quebec,
2015.
B. Neyshabur, S. Bhojanapalli, D. McAllester, and N. Srebro. A PAC-Bayesian approach to
spectrally-normalized margin bounds for neural networks. arXiv preprint arXiv:1707.09564,
2017.
B. Neyshabur, Z. Li, S. Bhojanapalli, Y. LeCun, and N. Srebro. The role of over-parametrization
in generalization of neural networks. In International Conference on Learning Representations,
2019.
B. Poole, S. Lahiri, M. Raghu, J. Sohl-Dickstein, and S. Ganguli. Exponential expressivity in deep
neural networks through transient chaos. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon,
and R. Garnett (eds.), Advances in Neural Information Processing Systems 29, pp. 3360-3368.
Curran Associates, Inc., 2016.
A. Radford, L. Metz, and S. Chintala. Unsupervised Representation Learning with Deep Convolu-
tional Generative Adversarial Networks. arXiv e-prints, art. arXiv:1511.06434, Nov 2015.
J. Schmidt-Hieber. Nonparametric regression using deep neural networks with ReLU activation
function. The Annals of Statistics, pp. to appear, 2019.
S.	Sonoda and N. Murata. Neural network with unbounded activation functions is universal approx-
imator. Applied and Computational Harmonic Analysis, 2015.
N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: a simple way
to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):
1929-1958, 2014.
I. Steinwart and A. Christmann. Support Vector Machines. Springer, 2008.
T.	Suzuki. Adaptivity of deep ReLU network for learning in Besov and mixed smooth Besov spaces:
optimal rate and curse of dimensionality. In International Conference on Learning Representa-
tions (ICLR2019), 2019.
T. Suzuki, H. Abe, T. Murata, S. Horiuchi, K. Ito, T. Wachi, S. Hirai, M. Yukishima, and
T. Nishimura. Spectral-Pruning: Compressing deep neural network via spectral analysis. arXiv
e-prints, art. arXiv:1808.08558, Aug 2018.
M. Talagrand. New concentration inequalities in product spaces. Inventiones Mathematicae, 126:
505-563, 1996.
G. Valle-Perez, C. Q. Camargo, and A. A. Louis. Deep learning generalizes because the parameter-
function map is biased towards simple functions. In International Conference on Learning Rep-
resentations, 2019.
13
Published as a conference paper at ICLR 2020
A. W. van der Vaart and J. A. Wellner. Weak Convergence and Empirical Processes: With Applica-
tions to Statistics. Springer, New York, 1996.
V. N. Vapnik. Statistical Learning Theory. Wiley, New York, 1998.
V. Verma, A. Lamb, C. Beckham, A. Najafi, I. Mitliagkas, A. Courville, D. Lopez-Paz, and Y. Ben-
gio. Manifold mixup: Better representations by interpolating hidden states. arXiv preprint
arXiv:1806.05236, 2018.
S. Wager, S. Wang, and P. S. Liang. Dropout training as adaptive regularization. In C. J. C. Burges,
L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger (eds.), Advances in Neural Infor-
mation Processing Systems 26, pp. 351-359. Curran Associates, Inc., 2013.
M. Wainwright. High-Dimensional Statistics: A Non-Asymptotic Viewpoint. Cambridge Series in
Statistical and Probabilistic Mathematics. Cambridge University Press, 2019.
C. Wei and T. Ma. Data-dependent sample complexity of deep neural networks via lipschitz aug-
mentation. In Advances in neural information processing systems, pp. to appear, 2019.
L. Wu, Z. Zhu, et al. Towards understanding generalization of deep learning: Perspective of loss
landscapes. arXiv preprint arXiv:1706.10239, 2017.
C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning requires
rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz. mixup: Beyond empirical risk minimization.
In International Conference on Learning Representations, 2018.
W. Zhou, V. Veitch, M. Austern, R. P. Adams, and P. Orbanz. Non-vacuous generalization bounds
at the imagenet scale: a PAC-Bayesian compression approach. In International Conference on
Learning Representations (ICLR2019), 2019.
Notation lists
Since we use plenty of notations, we give the notation list in Table 2.
Appendix
In the appendix, we give the proofs of the main text. We use the following notation throughout the
appendix:
1n
Pnf = n Ef (Zi), Pf = E[f(Z)]∙
To evaluate it, the covering number is useful (van der Vaart & Wellner, 1996).
Definition 1 (Covering number). For a metric space F equipped with a metric d, the -covering
number N (F, d, ) is defined as the minimum number of balls with radius (measured by the metric
d) to cover the metric space F.
Hereafter, C denotes a constant which will be dependent on the context. We let a ∧ b := min{a, b}
and a ∨ b := max{a, b} for a, b ∈ R.
A Proof of Theorem 1
1 ʌ	. . 1 1	ICl	1	1	c(^∕ / 八	/∕∖l ，一 手∙^■一分 II，	Il
Denote the local Rademacher complexity of {ψ(f) - ψ(g) | f ∈ F, gb ∈ G, kf - gkL2 ≤ r} by
.. — ... .... ^ , ^ .. .. -.
φ(r) := Rn({ψ(f) - ψ(g) | f ∈Fb,g ∈ G, kf - gkL2 ≤ r}).
Here, we restate Theorem 1 in the following in more complete form. Remember that
gb ∈ G are the trained original network and the compressed network respectively.
^ ^
fb ∈ Fb and
14
Published as a conference paper at ICLR 2020
Table 2: Notation list
notation	definition
n Zi = (xi, yi) Dn = (Zi)n=1 ψ(y,f (χ)) M Bx k∙kn INE ^ Ψ(f) Ψ(f) (ei)n=1 Rn(F0) Rn(F0) Rr (F0) r r* r L W⑶ ^ ς (') R2 RF m = (mi,. .. ,mL) m] = (mi, m2,. .., m£) S = (s1, . . . , sL) F =NN(m, R2,Rf) ^ G ^ ^ f ∈F 一 ^ b ∈g Λ (`) % ɑ β		sample size i-th observation (x^ input, yi. output) training data loss function L∞ -norm bound of models norm bound of input empirical L2-norm (∣f ∣n := PPn=I f (zi)2∕n) population L2-norm (∣f "2 := √Ez〜Pf(Z)2]) training error (empirical risk) generalization error (expected risk) Rademacher random variable conditional Rademacher complexity Rademacher complexity local Rademacher complexity 1	1 ell*	^ll upper bound of ∣f - g∣n fixed point of the local Rademacher complexity (Eq. (2)) √2(r2 + r*) depth of networks weight matrix of the '-th layer covariance matrix of the '-th layer operator norm bound of W(') Frobenius norm bound of W (') list of widths of networks list of widths of compressed networks list of ranks of the weight matrices of compressed networks the whole set of networks with width m set of trained networks set of compressed networks trained network compressed network an upper bound of the j-th largest eigenvalue of the covariance ma- trix in the '-th layer of f decreasing rate of the eigenvalues of W(') decreasing rate of the eigenValUeS of Σ(')	
EI. . .	.	∏	.1	. .1	■ ■	1 T 1∙ .	1	G J ^ ■	1	777 UG ^ll ，入 9
Theorem 5. Suppose that the empirical L?-distance between f and g is bounded by ∣∣f 一 g∣∣n ≤ r2
for a fixed r > 0 almost surely. Let r :=，2(r2 + r2), then, under Assumptions 1, 2, 3, there exists
a universal constant C > 0 such that
,^	ʌ , ^ — ,
Ψ(f) ≤ Ψ(f) + 2Rn(G) +
t 1 + tM
φ(r)+rV n +
with probability at least 1 一 3e-t for all t ≥ 1.
TL ɪ . .1 . A / ʃ / T^∖ ʃ / 分、∖ ∙ .1 . .	∕' EI	1 i' .Λ	Λ t	1 1 1 C 1
Note that R(ψ(F) 一 ψ(G)) in the statement of Theorem 1 of the main body is replaced by refined
quantity Φ(r) (We can show Φ(r) ≤ R(ψ(F) 一 ψ(G)) from the LiPschitz continuity of ψ).
Proof of Theorems 1 and 5. First, by the standard Rademacher complexity analysis, we have that
1n
ψ(b) — Ψ(b) = — £侬(%讥“))一 E[ψ(z,b(x))])
n
i=1
15
Published as a conference paper at ICLR 2020
1n
≤ SUp — E(ψ(zi, g(xi)) - E[ψ(Z, g(X))])
g∈Gb n i=1
≤ 2EDn
1n
sup -废道ψ(zi,g(xi))
g∈Gb n i=1
+JMit ≤ 2Rn© +yM2t
(5)
with probability 1 - e-t, where we used the Rademacher concentration inequality (Theorem 3.1
of Mohri et al. (2012)) in the third line and the contraction inequality (Theorem 11.6 of Boucheron
et al. (2013) or Theorem 4.12 of Ledoux & Talagrand (1991) and its proof) in the last line. We let
this event be E0(t).
Next, we observe that
,^	ʌ , ^	, ^	一.	一.	八,一.	八.一.	ʌ , ^
Ψ(f) -	Ψ(f)	= Ψ(f) -	Ψ(b) +	Ψ(b)	- Ψ(b)	+ Ψ(b) -	Ψ(f)
^ ʌ ^ ʌ ʌ
= (ψ(f)	- ψ(b) - (ψ(f) - ψ(b))) + ψ(b) - ψ(b)
≤ [ψ(f)	- ψ(b) - (ψ(f) - ψ(b))]+ 2Rn(G) + MM—	⑹
where we used	Eq.	(5) in the last line. Here, it should be noticed that it is not a good strategy to
bound the first term	Ψ(f)	- Ψ(g) -	(Ψ(f) - Ψ(g)) by bounding Ψ(f) - Ψ(f) and Ψ(g)	- Ψ(b)
independently. Instead, we should bound them simultaneously to obtain tighter bound. This can be
accomplished by using the local Rademacher complexity technique.
Note that f and gb are date dependent and we can only bound the empirical L2-distance between
them. On the other hand, the local Rademacher complexity is characterized by the population L2 -
norm. To bridge this gap, we need to bound the population L2-distance kf -gbkL2 between f and gb in
.	CJ	.	.	1 T	1	ill* ^ll ，入 El	IF	1.1	1	ICl	1
terms of the empirical L? -norm bound ∣∣f - g k n ≤ r. Todo so, we also bound the local Rademacher
complexity of F - G : Rr(F - G ). Suppose that there exists a function φ : [0, ∞) → [0, ∞) such
that the the following conditions are satisfied:
♦ , ^ ^. ,.
Rr (F -G) ≤ φ(r)
and
φ(2r) ≤ 2φ(r).
Note that Eq. (11) gives one example of φ(r). Then, by the so-called peeling device, we can show
that for any r > 0,
P sup
t ʌ ʌ
h∈Fb-Gb
(P -Pn)(h2)
Ph2 + r2
φ(r)	4t	2 2t	t
≥8 卞 + Mrn + M 两 ≤e
for all t > 0 (Theorem 7.7 and Eq. (7.17) of Steinwart & Christmann (2008)). Hence, if we choose
r* = r*(t) so that
φ(r*) V 4t -2 2t
8 7τ+Mqrn+M 不
1
≤ 2,
then it holds that
P(h2) ≤ 2Pn(h2) + 2r*2
t
uniformly over all h ∈ F - G with probability greater than 1 - e-t. We let this event as E1 (t). In
this event, if ∣f - b∣n ≤ r2, then it holds that
Ilfb- bkL2 ≤ 2(r2 + r2) =r2.
TL T .	Λ	Λ ,ʃ, / Q^∖	,ʃ, ∕^∖	/ ,ʃ, / ^r∖	,ʃ, / ^∖ ∖ EF	1 . 1 ∙	1 ,IEl	1，
Next, we bound Ψ(f) - Ψ(gb) - (Ψ(f) - Ψ(gb)). To bound this term, we apply the Talagrand’s con-
centration inequality (Proposition 2 and Talagrand (1996); Bousquet (2002)). To apply it, we should
bound the variance and L∞-norm of ψ(y, f (x)) -ψ (y,g (x)) - (E[ψ(Y, f(X))] - E[ψ (Y,g(X))])
for any f ∈ F,g ∈ G with ∣∣f -引匕 ≤ r (where r will be set 2(r2 + r2)). Due to the Lipschitz
continuity ofψ, we have that
Var[ψ (Y, f(X))-ψ(Y, g(X))] ≤ Var[f(X) - g(X)] ≤ ∣f - g∣2L2 ≤r2.
16
Published as a conference paper at ICLR 2020
Similarly, it holds that
IΨ(y,f(χ)) - ψ(y,g(χ)) - (E[ψ(Y,f(X))] - E[Ψ(Y,g(X))])∣
≤ ∣ψ(y, f (x)) - E[ψ(Y,f (X))]| + ∣ψ(y, g(x)) - E[ψ(Y,g(X))])∣ ≤ 2M.
Hence, by the Talagrand’s concentration inequalit (Proposition 2 and Talagrand (1996); Bousquet
(2002)), it holds that
sup (P - Pn)(ψ(Y, f(X)) - ψ(Y, g(X)))
f,g:kf -gkL2 ≤r
≤2E sup	(P - Pn)(ψ(Y, f(X)) - ψ(Y, g(X)))
f,g:kf -gkL2 ≤r	.
+ r∖R + 4tM
nn
with probability at least 1 - e-t for anyt > 0. The first term in the right hand side can be bounded
as
E sup (P - Pn)(ψ(Y, f(X)) -ψ(Y,g(X)))
f,g:kf -gkL2 ≤r
1n
≤ 2E0n,e	sup	— T∕[ψ(yi,f(Xi))- Ψ(yi,g(χi))]
f,g:kf -gkL2 ≤r n i=1
= 2Φ(n),
where we used the standard symmetrization argument (see Lemma 11.4 of Boucheron et al. (2013)
for example).
Combining these inequalities, it holds that
sup (P - Pn)(Ψ(Y,f (X))- Ψ(Y,g(X))) ≤ C Φ(r) + rʌ/ɪ + 1+Mt .
f,g:kf -gkL2 ≤r	n	n
for a universal constant C > 0 with probability at least 1 - e-t for allt > 0. We denote by this
event as E2 (t, r).
We define an event E3(t) = Eι(t) ∩ E2(t, r). Then P(E3(t)) ≥ 1 - 2e-t for all t > 0. In this event,
Ilf- bkL2 ≤ r2 and thus it holds that
Ψ(f)- Ψ(b)-(Ψ(f)- Ψ(b)) ≤ C Φ(r) + 宁∖D +1+m ,
nn
for a universal constant C > 0. Combining this and Eq. (6), we obtain the assertion on the event
E0(t) ∩ E3(t). This gives the proof of Theorem 5.
To show Theorem 1, note that Iψ(f) - ψ(g)IL2 ≤ If - gIL2 by the Lipschitz continuity of ψ
and this yields {ψ(f) - ψ(g) | f ∈ Fb, g ∈ Gb, If - gIL2 ≤ r} ⊂ {ψ(f) - ψ(g) | f ∈ Fb, g ∈
Gb, Iψ(f) - ψ(g)IL2 ≤ r}. Therefore, we have
Φ(r) ≤ Rr(Ψ(F)- Ψ(G))∙
□
Hereafter, we derive some upper bounds of the (local) Rademacher complexities under some cover-
ing number conditions.
Lemma 1. Foragiven r > 0, let ^n = Yn(Dn) := sup{kf-g∣n | kf - g"? ≤ r, f ∈ F ,g ∈ G}.
Then, it holds that
_________ . ∙ , ^ ^,,
max{Φ(r), Rr (F - G)}
≤C
{n+EDn IcJ
log(N(F,|H|n,e/2))也
log(N(G,k」n,e/2))也
n
n
(7)
17
Published as a conference paper at ICLR 2020
and
Φ(r) ≤ C0RRr(Ψ(Fb) - ψ(G))Plog(n)log(2nM),	(8)
where C, C0 are universal constants.
Proof. The conditional Rademacher complexity of the set {ψ(y, f(x)) - ψ(y, g(x)) | f ∈ Fb, g ∈
G, kf - g kL2 ≤ r} can be bounded by a constant times the following Dudley integral (see Theorem
5.22 of Wainwright (2019) or Lemma A.5 of Bartlett et al. (2017a) for example):
log(N({ψ(f) - ψ(g) | f ∈
^ ^ .. .. , 一
F,g ∈ G, kf - gkL2 ≤ r}, k ∙ kn,e^ de
n
inf
α>0
α
≤
≤
≤
≤
, , , ... ^ ^ ..	.. .
log(N ({ψ(f) - ψ(g) If ∈ F,g ∈ G, kf - g∣∣L2 ≤ r}, k ∙ kn, e)) &
n
lθg(N(ψ(F), k∙ kn, 〃2)) + lθg(N(ψ(G), k∙ kn, "2)) de
log(N(Fb, k∙kn, e/2)) + log(N(G, k∙kn, e/2)) d^
-,^ ・―
log(N(Fb, k∙kn,e∕2))dc
log(N(G,k∙kn,e∕2))dq
(9)
(10)
^

^
n
n
n
O
n
where we used kψ(f) - ψ(g) - (ψ(f 0) - ψ(g0))kn ≤ kψ(f) -ψ(f0)kn+ kψ(g) - ψ(g0)kn ≤ for
f,f0 ∈ Fb and g, g0 ∈ Gb with kf -f0kn ≤ /2 and kg-g0kn ≤ /2 in the third line, and 1-Lipschitz
continuity of the loss function ψ in the fourth line (i.e., ∣ψ(y, f (x)) - ψ(y, g(x))∣ ≤ ∣f (x) - g(x)∣
which yields kψ(f) - ψ(g)kn ≤ kf - gkn),
In the same way, we can see that Rr (F - G) is bounded by the Dudley integral as
• ^ ^
Rr (Fb -G)
， ,,^ - . ^	^ , ^ .. ^ ,.. - 」
log(N ({f - b If ∈ Fb, b ∈ G, kf - bkL2 ≤ r}, k∙kn, e)) de
n
C	rγn /
≤n + CEDn //nV
Γ ʌ I I	ʌ I I	1
≤ C + CEd	ZYnl∕UW3de 十 /^n ʌ，㈣NG皿』3a ,(“)
n	n 1/n	n	1/n	n
where we used the same argument as Eq. (10) and C > 0 is a universal constant. Then, we conclude
Eq. (7).
Next, we show Eq. (8). The term Eq. (9) can be evaluated by using the local Rademacher complexity
of ψ(F) - ψ(G). Note that kψ(f) - ψ(g)kL2 ≤ kf - gkL2 by the Lipschitz continuity of ψ and
this yields {ψ(f) - ψ(g) I f ∈ Fb,g ∈ Gb, kf - gkL2 ≤ r} ⊂ {ψ(f) - ψ(g) I f ∈ Fb,g ∈
G, kψ(f) - ψ(g)kL2 ≤ r}. Then, the Sudakov’s minoration (Corollary 4.14 of Ledoux & Talagrand
(1991)) gives an upper bound of the right hand side of Eq. (9):
log(N ({ψ(f) - ψ(g) I f ∈
^ ^ . .. , 一
Fb,g ∈ G, kf - gkL2 ≤ r}, k ∙ kn,e^de
n
log(N({ψ(f) - ψ(g) If∈
^	^ .. ,, .... - ..
F,g ∈ G, kψ(f) - ψ(g)∣L2 ≤ r}, k ∙ kn,e)) de
n
Yn ʌ 一 4、
≤ /	Rn,r (ψ (Fb)-
1/n
≤ Rn,r(Ψ(Fb) - ψ(G))√log(n) log(nγn),
18
Published as a conference paper at ICLR 2020
whereRn,r(Ψ(F)-ψ(G)) := Ee [sup{ 1 Pn=I ei(ψ(yi, f(χiY) - ψ(yi,g(χi))) | f ∈F,g ∈ b,kψ(f) - Ψ(g)∣∣L2 ≤ r}].
Since Yn	≤	2M, the expectation of the right hand side with respect to Dn is
Rr (ψ(F) - ψ(G)) log(n) log(2nM). This gives an upper bound of the right hand side of
Eq.(9) and yields Eq. (8).	□
Lemma 2.
E sup{Pnh2 | h ∈ Fb-Gb: khkL2 ≤ r} ≤ r2 + 2M φ(r).
Proof. By the contraction inequality of the Rademacher complexity (Theorem 4.12 of Ledoux &
Talagrand (1991) and its proof), we have
E sup{Pnh2 | h ∈ Fb - Gb : khkL2 ≤ r}
≤E hsup{(Pn -P)h2 | h∈ Fb-Gb: khkL2 ≤r}i +r2
1n
≤ 2E sup{- EEih(Xi)2 | h ∈ F - C : |向区 ≤ r} + r2
n i=1
(symmetrization; Lemma 11.4 of Boucheron et al. (2013))
1n
≤ 2ME sup{— J2eih(xi) I h ∈F - C ： 1向匕 ≤ r}
n i=1
+ r2
(∙.∙ contraction inequality)
2MR (F -G)+ r2 ≤ 2Mφ(r) + r2.
□
Lemma 3. Suppose that
SuPlog(N(F, k∙ kn,"2)) + suplog(N(G, k∙ ∣∣n,e∕2)) ≤ S1 + S2 log(1∕e) + S3e-2q
Dn	Dn
for q < 1. Then, for a universal constant C > 0 and a constant Cq > 0 which depends on q < 1, it
holds that
Φ(r) ≤ C max/1+ MS1 + S2 log(n) + rJ S1 + S2 log(n),
nn	n
In particular,
I-	.	.	, T	、	1
r2 ≤ C MSI + S2 log(n) + (M 1-qS3 γ+q + 1 + Mt
* -	n	∖ n J	n
>Λ	/-	1 ʌ	F	.1	. ʌ	r Il f	Il	Il f	Il	/	，一 券	_ 分、C	♦	-C
Proof. Remember that Yn := sup{∣∣f - g∣∣n : ∣∣f - g∣∣L2 ≤ r, f ∈ F ,g ∈ G} fora given r > 0.
Under the assumption, we may take φ(r) (defined below) as an upper bound of Φ(r) by Lemma 1:
°(r) = C 1 + √1n E
/ Yn PS1 + S2 log(e-1) + S3「2q d(),
where C > 0 is a universal constant. The right hand side can be evaluated as
γn
，S1 + S2 log(1∕e)+ S3「2qde
1/n
E
≤ E hγnpS1 + S2 log(n)i + 1-q PSEBn-q]
≤ jE[{2h2 I h ∈F -G,kh∣L2 ≤ r}] PS1 + S2 log(n)
+√3qE[{Pnh2 1h ∈f -c, khkL2 ≤r}产
19
Published as a conference paper at ICLR 2020
≤ P + 2Mφ(r)PSι + S2 log(n) + ^S3(r2 + 2Mφ(r)) 1-q,
1-q
(12)
where we used Lemma 2. Hence, if the first term is larger than the second term, we have that
φ(r) ≤ C
SI +S2 IogS) P + 2Mφ(r)
n
S1 +S2 IogS) + Cr
≤ C + C2M
n
Therefore, we obtain that

n
S1 + S2 log(n)	φ(r)
十亍.
n
φ(r) ≤ 2C + 2C2M
n
S1 +S2 IogS) +2Cr
n

S1 + S2 log(n)
n
(13)
On the other hand, if the second term in Eq. (12) is larger than the first one, then Young’s inequality
gives that
φ(r)
≤C
S3
n(1 - q)2
(r2 + 2Mφ(r)) 1-q
≤ C + C
n
■	1
(Cj C 2S3Y11 ∖2Mφ(r)
q(jLF ) +(1-q) 丁一 + V'
n(1 - q)2
r2(1-q)
where c0 >
conclude that
0 is any positive real. Thus taking c0 sufficiently large (which depends on M, q), we
φ(r) ≤ Cq 1 +
M1-qS3
+ ʌISr2(1-q),
n
(14)
1
1 + q
n
where Cq is a constant depending only on q < 1. These two inequalities (Eq. (13) and Eq. (14)) give
the first assertion. By noticing the assumption M ≥ 1, r* can be derived from a simple calculation.
□
B Derivation of compression based bound for non-compressed
NETWORKS
B.1	Full model bound
Here, we assume that the model F of the trained network is the full model F = F =
NN(m, R2 , RF) and G is included in NN(m, R2, RF). Then, their covering entropy is bounded
by
LL
log(N(Fb, k ∙ k∞,e)) ≤ (E m'm'+ι)log(e-1) + L(E m'm'+1)log(L(R2 ∨ 1)(maxm` + 1)),
LL
log(N(Gb) k∙ ∣∣∞,e)) ≤ (Xm'm'+ι)log(e-1) + L(Xm'm'+1)log(L(R2 ∨ 1)(maxm` + 1)).
Hence, the condition in Lemma 3 holds for Si = PL=I m`m`+1, S2 = LSi log(L(R2 ∨ R2 ∨
1)(max' m` + 1)) and S3 = 0. In this case, We can set
r2 = C(M + 1)(Sι + 1 + S2 log(n)) + Mt
n
for a constant C > 0.
20
Published as a conference paper at ICLR 2020
B.2	Complexity of a sparse model (Proof of Example 1)
Suppose that G is the model With sparse weight matrices given in Example 1. Let m = max' m`
and B = R2 , then we can see that
Gb⊂ Φ(L,m,S,B)
where the definition of Φ(L, m, S, B) is given in Appendix C.1. Therefore, its covering number is
bounded by
log(N(G, k ∙ ∣∣∞,e))
≤ S log(-1) + LS log(L(R2 ∨ 1)(max m` + 1)) ≤ O(SL log(n) + Slog(-1)),
by Lemma 4. Hence, the Rademacher complexity is bounded as
G ≤ CMrL— log(nL(R2 V 1)(max m` + 1)) = O (M{Lg log(n)
by the Dudley integral, R(G).
--.ʃʌ`....
Iog(N (GJHI∞,C))
n
d, where C is a universal constant.
B.3	Near low rank condition on the weight matrix (Proof of Theorem 2 and
Corollary 1 )
Here, we give proofs of Theorem 2 and Corollary 1 which give a generalization error bound when
the trained network has near low rank weight matrices (W('))L=ι (Assumption 4).
Under Assumption 4, we can see that for any 1 ≤ S ≤ min{m', m'+ι}, we can approximate W(')
by a rank s matrix W0 as
∣W (`) - W0∣2 ≤ V0s-α, ∣W0∣2 ≤ ∣W (`) ∣2	(15)
∣W⑶-W0∣F ≤ -ɪ= Vo(s - I)(I-α”2.	(16)
2α - 1
This can be checked by discarding the singular vectors corresponding to the singular values smaller
than the s-th largest one. This ensures that, for any f ∈ F, there exists f0 ∈ F such that it has width
S = (si,..., sl), weight matrix W ](')with ∣∣ W ](')∣2 ≤ R2, ∣W ](')∣F ≤ RF and
L
kf - f0k∞ ≤ X V0RΓ1s-αBχ.	(17)
'=1
This can be proved as follows. Let f0(χ) = G ◦ (W](L)η(∙)) ◦…。(W](I)X) where W](')
is a rank s` matrix that satisfies Eqs. (15) and (16) for W0 = W]('). Let f'(x) = G ◦
(W ](L)η(∙))。…。(W ]('+1)η(∙)) ◦ (W (')η(∙)) ◦•••◦ (W ⑴ x) and fo (X) = f0.Then, ∣∣f — f0∣∣∞ ≤
P3 kf'-f'-ik∞. WeCanSeethatk(W(')η(∙))◦•••◦ (W(1)x)| ≤ Qk=IkW(叫b∣x∣ ≤ R2Bx,
k(W](')η(∙)) ◦ (W('-1即(・))◦…。(W(I)X) - (W(')η(∙)) ◦ (W('-1)η(∙)) ◦…。(W(I)X)∣ ≤
kW(')- W](')∣2∣(W('-1)η(∙))。…。(W(1)x)k ≤ V0s-αR2-1Bχ. This gives kf` - f'∣∞ ≤
QL='+1 k W(k)k2Vos-αR2-1Bx ≤ RLTV0s-αBx. Finally, by summing up this from ' = 1 to
` = L, we obtain Eq. (17).
In particular, for any > 0, by setting s' = s'(e) = min{d("(LV0RLTBx))T∕α], m` ∧ m'+i}
for all `, then kf - f0 k∞ ≤ . This indicates that, by Lemma 5, the covering entropy of Fb is
bounded by
log(N(F, k∙ k∞, E)) ≤ (χ(m' + m'+i)s'("2)) [log(ET) + 2Llog(2L(R2 V 1)(maxm` + 1))]
'=i	(18)
L
≤ 2(Xm`)(2LV0RL-1Bx)1/ae-1/a[log(e-1) + 2Llog(2L(R2 V 1)(maxm` + 1))].
'=i
21
Published as a conference paper at ICLR 2020
A ..	I	I 6	、公 RTRT/	C C、C
As the compressed network G, we may choose G = NN(m, s, R2, RF) for s
for all f ∈ F, there exists g ∈ G satisfying
(s1, . . . , sL) so that,
L
kf -gk∞ ≤ (WRLTBx) X Sia
'=1
Hence, We may set r2 = [(VoRL-1Bx) PL= 1 s-α]2. In this case, the covering number of Gb is
bounded as (18) by replacing s' with s`.
Therefore, Lemma 3 gives that
n
r2 ≤ C(M + I)(SI + 1 + S2 IogS)) + Mt ∨ M2α+1
2α
S3 A1+2α
n
where
L
S1 =	s'(m' + m'+1),
'=1
S2 = LS1 log(L(R2 ∨ 1)(max m' + 1)),
L
S3 =(Xm`)(2LKRLTBx)1∕α[log(n) + 2Llog(2L(R2 ∨ 1)(maχm` + 1))],
'=1
where q = 1∕2a was used. This indicates that, if a > 1/2 is large (in other words, each weight
matrix is close to rank 1), then the local Rademacher complexity can be small. Actually, the bound
is smaller than P'=1 ；'m'+1 because each rank s` must satisfy s` ≤ min{m', m'+ι}.
Finally, we observe that
Rn(Gb) ≤CM∖∣LPL=I s'(m' + m'+l)
n
log(nL(R2 ∨ 1)(max m' + 1)),
by Lemma 5 for G = NN(m, s, R2, Rf) and the Dudley integral (van der Vaart & Wellner, 1996):
R(G) . RM ,lOg(N(Gnk'k∞≡)de. This gives Theorem 2.
Corollary 1 can be obtained by substituting s' = min{m', m'+1, dLV0R2L-1Bxe1∕α}.
B.4 Improved bound with Lipschitz continuity constraint
In the generalization error bound of Theorem 2 and Corollary 1, there appears R2L . Even though
R2 can be much smaller than RF, the exponential dependency R2L could give lose bound as pointed
out in Arora et al. (2018). We improve this exponential dependency by assuming the following
condition.
Assumption 6 (Lipschitz continuity between layers: Interlayer cushion, interlayer smoothness
(Arora et al., 2018)). For the trained network f = G ◦ (W (L)η(∙)) ◦•••◦ (W ⑴ x) ,let φ'(x) = η ◦
(W('-1)η(∙))◦…o(W(I)X) be the input to the '-th Iayerand m`,`o (x) = (W('O)η(∙))◦…o(W⑶x)
be the transformation from the '-th layer to '0-th layer. Then, we assume that there exists κ,τ > 0
such that τ ≤ 1∕(2κ2L) andforany ', '0 ∈ [L],
n2
X[M','0 (φ'(xi) + ξi(1)) - M','0 (φ'(xi) + ξi(1) + ξi(2))]2 ≤κ2 kξ(1)k+τkξ(2)k	,
i=1
for all ξ(1) = (ξ1(1),...,ξn(1))> ∈ Rn and ξ(2) = (ξ1(2),...,ξn(2))> ∈Rn.
22
Published as a conference paper at ICLR 2020
This assumption is a simplified version of the interlayer cushion and the interlayer smoothness in-
troduced in Arora et al. (2018). Although a trivial bound of κ is κ ≤ R2L, the practically observed
Lipschitz constant is usually much smaller. Assumption 6 captures this point and gives better de-
pendency on the depth L. Actually, we can remove the exponential dependency on R2 as in the
following corollary.
Corollary 2. Under Assumptions 4 and 6, it holds that
Ψ(f) ≤ Ψ(f) + C [m 1-V2αTL(PL=I m'X2LV0κ2Bx)1/a iog(n) + M2α--A22α+ι + 1 + tM
n	2n
for A2 = L (P'=1 m`)(2LVOK Bx) / with probability 1 一 3e-t for any t > 1 where C is a constant
depending on α.
This is almost same as Corollary 1, but the exponential dependency on R2L is replaced by the Lips-
chitz continuity κ2 .
Proof of Corollary 2. Suppose that
s` ≥ min {mg,mg+ι J(4κV0L)Vα]},
then we show that Eq. (17) can be replaced by
L
kf 一 f0kn ≤ 4 X Voκ2s-αBχ,
'=1
(19)
(20)
where if s` = min{mg, m'+ι}, then s-α term can be replaced by 0 (which means no-compression
in the layer `). Once we obtain this evaluations, then the following argument is same as the proof of
Theorem 2 and Corollary 1 (Sec. B.3).
Let φ'(χ) = η ◦ (W⑶η(∙)) ◦ .・・ ◦ (W⑴x) and φ'(x) = η ◦ (W](')η(∙)) ◦…。(WMI)X) for
' =1,...,L 一 1, and let Φl(x) = G ◦ (W(L) η(∙)) ◦•••◦ (W ⑴ x) and φL(χ) = G ◦ (W ](L)η(∙)) ◦
•••◦ (W](I)X). Let CB := 2κBχ. We will show that
kφk- Φk kn ≤ 2kV0Cb (X s-α) , kφk kn ≤ CB ,
for all k = 1, . . . , L. We show this by inductive reasoning. To do so, we assume that, for k =
1, . . . , ` 一 1, this is satisfied, and then we show this for k = `. Note that, for all k with k < `, it
holds that, for any `0 > k,
kMk,'0 ◦ φk - Mk-1,'0 φk-1kn ≤ κ(kφk - n(W (k)φk-1 )kn + T kη(W ⑻次-I)- φk Iln)
≤ κ(V0sk-αkφ]k-1kn + τ κkφ]k-1 一 φk-1kn)
≤ κ V0sk-αCB + Tκ2κV0CB X sj-α	(by induction)
j≤k-1
(	1 k-1	∖
≤ kV0Cb I s-α + — jɑ s-α I (by the assumption of T).
j=1
Note that the term sk-α can be replaced by 0 if sk = min{mk, mk+1} which corresponds to the full
rank setting (W](k) = W(k)). Therefore, we have that
kφ' - φ]kn ≤ X kMj,' ◦ φj - Mj-1,' ◦ φ]-1kn ≤ 2κV0CB (X s-。) ∙
j =1	k=1
Under the setting (19), this gives that
kφ' 一 φ'kn ≤ CB/2.
23
Published as a conference paper at ICLR 2020
Finally, noting that kφ'kn ≤ CB/2,we have
kφ'kn ≤ CB .
This concludes the inductive reasoning.
Finally, noting that f = Φl and f = φ%, We have Eq. (20).	□
B.5 Near low rank condition on the covariance matrix (Proof of Theorem 3
and Theorem 4)
Under Assumption 5, f can be compressed as follows. Suppose that the network is compressed
to smaller one upto the ` - 1-th layer and the weight matrix of the compressed one is denoted
by (W](k))k=1 where each W](k) has size mk+1 X m£ (here, mk ≤ mk is assumed), and, in
the ' 一 1-th layer, W](`-1) has size m` X m'-1. The input to the '-th layer of the compressed
networkis denoted by φ'(χ) = η(W ]('-1)η(∙ ∙ ∙ W ](1)x) ∙ ∙ ∙). Let r' = ∣∣kφ' 一 φ'k∣∣n and ∑]')：=
n pn=ιφ'(Xi)(。'(Xi))>.
For a given matrix Σ and a precision r2 > 0, the degrees of freedom3 are defined as
N'(r2, ∑)：
m`
X σj (ς)
j=1 σj (Σ)+ r2 .
(21)
Since the degrees of freedom are monotonically increasing with respect to each σj (Σ), we can see
that N'(r2, Σ) ≥ N'(r2, Σ0) if Σ 上 Σ0. Let4
m' = d5N'(r2, ∑]'))log(80N'(r2, *]')))],
then Proposition 1 tells that there exits a matrix A' ∈ m` X m' and j` ⊂ {1,..., m`)m' such that
l∣w>φ' - w>A^'φ',j' ∣n ≤ 4r2w>∑]')(∑]') + r2I)-1w ≤ 4r2 ∣∣w∣2,	(22)
for any W ∈ Rm'5, and the norm of A' is bounded as
∣A'∣2 ≤ J
20
行m'.
Next, we evaluate the degrees of freedom of ∑]'). We bound this by using the degrees of freedom
of Σ(`). First note that r2 = ∣∣∣φ' 一 φ'∣∣n. Let S ≤ m. For any matrix U ∈ Rm'×s such that
U>u = Is, Tr[U>∑]')U] = Pn[φ]>UU>φ'] ≤ 2{Pn[φ>UU>φ']+ Pn[(φ' 一 φ')>UU>(φ' 一
φ')]} by the Cauchy-Schwartz inequality. Here, let U be the matrix that gives Pn[φ>UU>φ']=
Pm=m'-s+ι σj (∑ (')) = inf U ：u >u=Is Pn就 UU >φ'],thenby noticing Pn[(φ' - φ')>UU >(φ' -
φ')]	≤	PnkΦ'	-	Φ]k2	≤	r2,	we obtain that	Pn[φ'>UU>φ']	≤	2[pm^m'-s+1 σj(Σ('))+ r2].
Finally, by minimizing the left hand side with respect to U, we obtain that
m`	m`
X	σj(∑]')) ≤ 2 ( X	σj(Σ(`))+ r2).
j=m'-s + 1	ξj=m'-s+1	)
By setting s = m` 一 m + 1 for 1 ≤ m ≤ m` , this indicates that
m`	m`	m`
X σj (∑(')) ≤ 2 (X σj (Σ (`)) + rij ≤ 2 (X μ j') + r2 j .
j=m	j=m	j=m
3The definition is not dependent on `, but to make it clear that we are dealing with the `-th layer, we use the
notation n`.
4 dxe is the smallest integer that is not less than x ∈ R.
5For a vector x ∈ Rm and index set J ∈ {1, . . . , m}l, xJ is the vector corresponding to the index set J,
that is, xJ = (xj )j∈J .
24
Published as a conference paper at ICLR 2020
Now, let rm`	:= min{j	∈ {1,..., mf} | μjf)	≤	r2} (if μ息 >	r2, then We set	rm` = m`). Then,
N (2	ς]	) =	X σj H))	≤.	+ X	σj (ς('□
' γ ,⑶-ɪʧ σj(Σ%)) +	r2	-	Ime 二	σj(Σ%)) + r2
j = 1 八(f" 1	j>m `	八(f” 1
m ` + £
j〉m `
r2
≤
≤ m`+ r I r2+ X σj(ς(`))
∖ j>m `
≤
≤
.	2 ( 2 TT m 1-βʌ	2 2 2 m`r2
mf + ~	rf	+ UO^--≤	≤ mf + -	( rf	+ N-7
r2∖	p	— 1	I	r2	∖ P	— 1
2 ( 2 mfr2、	β + 1	rf
m f + F r2 + 飞一r =飞一^m f + 2 Jf.
r2 β - 1	β - 1	r2
Now, let
r2
then
Nf(r2, %)≤ β+4 m f + 8 r2.
We define the right hand side as mf:
]β +1 ∙ _LQrr
mf:= 口 m f+8 泮
We have, by Eq. (22),
m'+ι
X MWjf)φ]) -n(wj?余。"∣∣n ≤ 4
j=i
≤ 4瑶× 4rf =瑶%
m'+ι
X ∣W*∣2r2
j=i
By the induction assumption, we also have that
IlIIn(W⑶必-φ'+ι∣∣∣∣n = IlIln(W⑶必-η(w⑶〃)IlIln ≤ IlW⑶向2 ≤ Rrr2.
Combining these inequalities, if we define
φf+ι = n(W(f) 4或,&(X)),
and set WMf) = W(f)√lf and reset WMfT) - Wj(f-1), then it holds that
j `, -
∣∣∣∣φf+1 - φf+ι Illln ≤ rf+1
where we let
rf+1 = R2rf + RpΓf.
Letting r0 = 0, by an induction argument, we obtain
f
rf+1 ≤ X RtkRF亍k.
k=1
Finally, we obtain
Γ L	2 2
If - f 嚅 ≤ rL ≤ X RrLikR*	,
k=1
for a compressed network f ] that has width m] = (m；,..., mɪ) with parameters WMf)二
WJf)I-Af. Note that
IW"(f)∣∣2 ≤ IlWJf+ 1,-∣∣2∣∣Af∣∣2 ≤ R2y20∑f, IWMf)IIF ≤ IWjf+1,-IFkAfk2 ≤ Rf^mf.
25
Published as a conference paper at ICLR 2020
Therefore, if We set C = NN(m], J20max' mgR?, J20 max' m'Rp), then there exists C ∈ Gb
such that
^
kf - Ckn ≤ r
Where
r2 = rL.
一	1 .	L .分…C ”	. . I .一
Moreover, applying Lemma 5 to G and the Dudley integral yields
R(G) ≤ CMyZ
P'=1 m'm'+ι
n
log(nL(R2 V 1)(maxm` + 1)2).
This gives the assertion of Theorem 3.
γ2r2
Here, we consider a situation where RjFrj = c2 -l-2l for some constant c0 > 0. Then it holds that
屋+1 = R2
`
r' = R2 Y
k = 1
ri ≤ Rj exp (c0(2√' — 1)) rɪ.
Therefore, by setting CL := (1 V Rj)L exp ^c0(2√L - 1)), it holds that r` ≤ CLrI for '
1,...,L,in particular, we have
r ≤ CLri.
In this situation, the degrees of freedom are bounded by
N'(r2, ∑]')) ≤ m' = β-4m` + 8'黑.
Next, we bound m`. To do so, we should bound r` from below. Note that
需M= cRF2 W- Π (1 + 用 ri ≥
c0R 1
Rf √'
2-1
k=1
r`
1 + X 用 ri ≥
k = 1
c0R2 1
+ 2c0(1 -
m2 ≤ 阳/(4U0))-i/e ≤ (4U0)i/e
+ 2c0(1 - -⅛)1 ri)"'
≤ (4U0)i∕β
2∕β ∕1	∖-2∕β
(1 ∧ c0)	L
4U0RF	]i/e
一(0.5 八 C0)2c0R22
r-2/e
力ri
By Lemma 3, we can evaluate r2 for F satisfying Assumption 4 as
2α
r2 ≤ C(M + DB + 1 + s2 log(n))+ Mt v M2⅛ (也ʌ ɪ^
* -	n	n n J
where
L
Si = E m2m2+i,
2=i
S2 = LSi log(L(R2 V 1)(maxm` + 1)2) = O (L E m2m2+i log(n)
26
Published as a conference paper at ICLR 2020
L
S3 = (Xm')(2LVoRL-1Bx)1/a[log(n) + 2Llog(2L(R2 ∨ 1)(maxm` + 1)2)]
'=1
L(X m')(2LVoRL-1Bχ)1α log(n)),
'=1	)
Then, the overall generalization error is upper bounded by
Ψ(f) ≤ Ψ(f) + C r2 + rSr2(IT/2a) + (M(M2 + r2)LP'=1 m'm'+1 log(n) + 1 + Mt
n	nn
with probability 1 - 3e-t for all t ≥ 1. By letting QL,α,n ：= L (2LV0R2-Bx / log(n) and assuming
r ≤ 1, the second and third terms in C[∙] is bounded by
∖
L
QLan(X m')(CLrι)2(1-1^2α) + C0M
'=1
PL=ι[m ` + 'RF∕(c2R2)]2
n
log(n)3
L
uL
≤t QL,ɑ,n(Xm')(CLri)2(1-1/2a) + 2C0Mt
LL h (0.5∧C04u⅛R2)2Li	r-4/e + L3RF/R4 log(n)3.
n
Hence, by setting CLri = (P'=1m ) 4/e+2(1 1/2a) which balances the first and the second terms,
then r ≤ CLri ≤ 1 and the right hand side is bounded by
L
4∕β
4∕β + 2(1-1∕2α)
'=1
2(1-1∕2a)
L 4∕β + 2(1-1∕2α)
+ CM
2(1-1∕2α)	τ	4∕β	, 「	4U r2	12∕β
L 4/e + 2(1-1/2a) (P'=1 m`) …1…(CL)4/e (0.5∧co)2C0(F∧R2)2L
L-----------------------------------------------------------------------------log (n)3
n
+cMRJ JL4 l°g(n)3.
Finally, by setting c0 = 1/4, we obtain the assertion for
2/e
4UoRF(1 ∨ R2)L exp (co(2√L - 1))
QL =	(0.5 ∧ co)2c0(1 ∧ R2)2L	≤
This gives Theorem 4.
/	、-1 2/e
4U0RF(1 ∨ R2)L exp (4(2√L - 1))
(0.25)4(1 ∧ R2)2L
B.6 Improved bound of Theorem 4 with Lipschitz continuity constraint
Here, we again note that there appears R2L in PL and QL in the bound of Theorem 4. This is due to a
rough evaluation of the interlayer Lipschitz continuity. We can reduce this exponential dependency
under Assumption 6.
Corollary 3. Assume Assumption 6 in addition to Assumptions 4 and 5, then the bound in Theorem
4 holds for the following redefined PL and QL:
「	D /1	7—	x -∣ 2/e
_	4UoRF exp(4(2√L-1))
QL =	(0.25)4
except that the term M
is replaced by Mκ2
∖
Ψ(fb) ≤Ψb(fb)+C
■ Ir	η 1+ 4αβ +β
M U [Pl V Ql]L	(2α-1)+β
4∕β
∖ 4∕β + 2(1-1∕2α)
m`	log(n)3
27
Published as a conference paper at ICLR 2020
2α	/
+ M2α+1 (LPL p=1m' log(n))2α+1 + Mκ2RF呼产咨 + 1+Mt
Proof of Corollary 3. To show Corollary 3, we set
where c0 is a constant, and by the same argument as in the proof of Corollary 2, we can show that
`	k-1
r' ≤ 2κX √k Y
k=1	j=1
Then, through a cumbersome calculation, we have that
上≤ C也√'
r`	co
for a universal constant C. Moreover, we can show that r` can be bounded as
eco(2√'+l-1) - eco
r` ≤ 2κ--------------------ri.
c0
This also gives
IrL ≤ C0 — exp(co(2√L - 1))rι,
c0
for a universal constant C0. Then, redefining CL = Kexp(c0(2√L - 1)), We can apply the same
argument as in the proof of Corollary 2. Indeed, we can show
,	4UoRF	广β
(0.5 ∧ co)2c0κ2
]
m`
β + 1 ∙ l
口 m'+
From the above argument, if We set G
exists gb ∈ G such that
NN(m],后max' m'R2, J20 max' mgRp), then there
^
kf - bkn ≤ r
Where
Moreover, We can shoW
r2 = rL.
,	、	2α
r2 ≤ C(M + 1)(SI + 1+ S2 log(n)) + Mt ∨ M2α-⅛ (S3)1+2α
* -	n	n n J
Where
L
S1 = X m'] m]'+1,
'=1
S2 = LS1 log(L(R2 ∨ 1)(max m' + 1)2) = O L X m]' m]'+1 log(n) ,
L
S3 = (Xm')(2LVoK2Bx)1/a[log(n) + 2Llog(2L(R2 ∨ 1)(maxm` + 1)2)]
'=1	'
=O (L(Xm')(2LV¾κ2BχWαlog(n)).
Here, to evaluate S3, We used the argument in Sec. B.4 (proof of Corollary 2).
The remaining argument is the same as the proof of Theorem 3 and Theorem 4 (Sec. B.5).	□
28
Published as a conference paper at ICLR 2020
C Auxiliary lemmas
In this section, we give several auxiliary lemmas that are used in the proof of the theorems. These
results are not new at all, but we explicitly present them for completeness.
C.1 Covering number of deep network models
Define the neural network with height L, width m, sparsity constraint S and norm constraint B as
Φ(L, m, S, B)= {G ◦ (W(L)η(∙) + b(L)) ◦…。(W⑴X + b⑴)| W(L) ∈ R1×m, b(L) ∈ R,
W ⑴ ∈ Rm×d, b ⑴ ∈ Rm, W ⑶ ∈ Rm×m, b⑶ ∈ Rm(1 <'<L),
XL= 1(kW(')ko + kb(')ko) ≤ S, maxkW(')k∞ ∨kb(')k∞ ≤ B},
where ∣∣ ∙ ko is the 'o-norm of the matrix (the number of non-zero elements of the matrix) and ∣∣∙∣∣∞
is the '∞-norm of the matrix (maximum of the absolute values of the elements).
The following evaluation of the covering number of the model Φ(L, m, S, B) is shown by Schmidt-
Hieber (2019); Suzuki (2019).
Lemma 4 (Covering number evaluation (Schmidt-Hieber, 2019; Suzuki, 2019)). The covering num-
ber of Φ(L, m, S, B) can be bounded by
log N(Φ(L, m,S, B), k∙∣∞ ,δ) ≤ S log(δ-1 L(B ∨ 1)L-1(m + 1)2L)
≤ 2SL log((B ∨ 1)(m + 1)) + S log(δ-1L).
Proof of Lemma 4. Given a network f ∈ Φ(L, m, S, B) expressed as
f(x) = G。(W(L)η(∙) + b(L))。…。(W(I)X + b(1)),
let
Ak(f )(x) = η ◦ (W(k-1)η(∙) + b(k-1))。…。(W⑴X + b(1)),
and
Bk(f )(x) = G。(W(L)η(∙) + b(L))。…。(W(k)η(x) + b(k)),
for k = 2, . . . , L. Corresponding to the last and first layer, we define BL+1 (f)(X) = X and
Aι(f )(x) = x. Then, it is easy to see that f (x) = Bk+ι(f)。(W(k) ∙ +b(k))。Ak(f )(x). Now,
suppose that a pair of different two networks f, g ∈ Φ(L, m, S, B) given by
f(x) = G。(W(L)η(∙)+b(L))。…。(W(I)X+b(1)), g(x) = G。(W(L)'η(∙)+b(L)0)。…。(W(I)0χ+b(1)0),
has a parameters with distance δ: ∣ W(') 一 W(')0∣∞ ≤ δ and ∣∣b(') 一 b(')0∣∞ ≤ δ. Now, not that
∣Ak(f)k∞ ≤ maxjkW,kT)∣ι∣Ak-ι(f)k∞ + ∣b(k-1)k∞ ≤ mB∣Ak-ι(f)k∞ + B ≤ (B ∨
1)(m + 1)∣Ak-1(f)∣∞ ≤ (B ∨ 1)k-1(m + 1)k-1, and similarly the Lipshitz continuity of Bk (f)
with respect to ∣∣ ∙ ∣∣∞-norm is bounded as (Bm)L-k+1. Then, it holds that
|f(X) 一 g(X)|
L
=XBk+1 (g)。(W(k) ∙ +b(k))。Ak(f)(x) - Bk+ι(g)。(W(k)0 ∙ +b(k)0)。Ak(f)(x)
k=1
L
≤ X(Bm)L-kk(W(k) ∙ +b(k))。Ak(f )(x) - (W(k)0 ∙ +b(k)0)。Ak(f )(x)∣∞
k=1
L
≤ X(Bm)L-kδ[m(B ∨ 1)k-1mk-1 + 1]
k=1
L
≤ X(Bm)L-kδ(B ∨ 1)k-1mk ≤ δL(B ∨ 1)L-1(m + 1)L.
k=1
29
Published as a conference paper at ICLR 2020
Thus, for a fixed sparsity pattern (the locations of non-zero parameters), the covering number is
bounded by (δ∕[L(B ∨ 1)L-1(m + 1)l]) S. There are the number of configurations of the sparsity
pattern is bounded by ((呢1产)≤ (m + I)LS. Thus, the covering number of the whole space Φ is
bounded as
(m + 1)LS {δ∕[L(B ∨ 1)L-1(m + 1)L]}-S = [δ-1L(B ∨ 1)L-1(m + 1)2L]S,
which gives the assertion.
□
Lemma 5 (Covering number evaluation). Let NN(m, R2 , RF) be the set of neural networks
with depth m(m1, . . . , mL), kW (`) k∞ ≤ R2 and kW (`) kF ≤ RF. The covering number of
NN(m, R2, RF) can be bounded by
log N (NN(m,R2 ,Rf), k∙k∞,δ)
L
≤ (X m'm'+1)log(δ-lL(R2 ∨ 1)L-1(maxm` + I)L)
'=1
LL
≤ (E m'm'+1)log(δ-1) + L(E m'm'+1)log(L(R2 ∨ 1)(maxm` + 1)).
'=1	'=1	'
Moreover, the set of networks with low rank weight matrices, NN(m, s, R2 , RF), has the following
covering number bound:
log N (NN(m, s, R2, RF), k ∙ k∞, δ)
L
≤ Xs`(m` + m'+1)log(δ-lL(R2 ∨ 1)2L-1(maxm` + 1)2L.
'=1
ProofofLemma 5. Let B = R2, m = max' m`, and S = PL=I m`m`+i, then We can see
that NN(m, R2, RF) is a subset of Φ(L, m, S, B) because kW k∞ ≤ kW k2. Hence Lemma
4 gives the first assertion. As for the second one, we can easily check that the covering num-
ber of NN(m, s, R2 , RF) can be bounded by the one given in Lemma 5 for Φ(2L, m, S, B) with
S = PL=ι s`(m` + m'+ι). Then, we obtain the second assertion.
□
C.2 Compression error bound for one layer
The following proposition was shown by Bach (2017); Suzuki et al. (2018). Let ΣbI,I0 ∈ RK×H
for integers K, H ∈ N and a matrix ΣbI,I0 ∈ RK×H be a matrix (Σbi,j)i∈I,j∈I0 for the index sets
I ∈ [m']K and 10 ∈ [m']H. Let F = {1,...,m'} be the full index set. Let the degrees of freedom
corresponding to Σ be N(λ) := N'(λ, Σ) (see Eq. (21)) for λ > 0.
Proposition 1. Let U = (Uj,l)j,l is the orthogonal matrix that diagonalizes Σ, that is, Σ =
U diag(μι,...,μm') U > for μj ≥ 0( j = 1,..., m`). Define
，	1 m C	μ(')	1 ʌ ʌ ,1.,	一
TT = Nλ X Ujl μp7I = N^[*(* + XI) ]j,j (j ∈{1,...,m'}).	(23)
For λ > 0, if
m ≥ 5N(λ)log(80N(λ)),
then there exist vι,...,vm ∈ {1,..., m'} such that, for every α ∈ Rm',
30
Published as a conference paper at ICLR 2020
and Pm=I TjT ≤ 5m X m`, where ||e|艮：=Pm=I βjτj. Let τ := mλτ0 and IT = diag(τ).
Then, A := Σf,j(Σj,j + Iτ)-1 for J = {vι,..., Vm} satisfies
kAk2 ≤ 总',
1 .1	. ∙	1 r) .ι	ι ∙	.ι ■ «-	-	■ ι r> Λ ~T r	- EIm〃
and the optimal β that achieves the infimum is given by β = A > α for any α ∈ Rm'.
C.3 Concentration inequality
Proposition 2 (Talagrand’s Concentration Inequality (Talagrand, 1996; Bousquet, 2002)). Let G
be a function class on X that is separable with respect to ∞-norm, and {xi}in=1 be i.i.d. random
variables with values in X. Furthermore, let B ≥ 0and U ≥ 0be B := supg∈G E[(g - E[g])2] and
U ：= suPg∈G kgk∞, then for Z ：= suPg∈G ∣ n Pn=1 g(xi) — E[g] ∣, we have
P(Z ≥ 2E[Z ] + r2Bt + 2Ut! ≤ e-t,	(25)
nn
for allt > 0.
D Numerical experiments
In this section, we experimentally validate the assumptions we made in the theoretical analysis
and investigate how large the intrinsic dimensionality becomes. We use VGG-19 network trained
on CIFAR-10. The VGG-19 network have 16 convolution layers (named c0, . . . , c15) and 3 fully
connected layers (named l16, . . . , l18). The size of each filter in each convolution layer is 3 × 3.
Our theory does not support a convolution layer in a strict sense, but we adopt it as follows. If the
convolution layer in the '-th layer has the input channel size m` and the output channel size m'+1
with the filter size k × k (in our case k = 3), then the weight matrix is given as a 4-way tensor with
the size m'+1 × m` × k × k: W(') ∈ Rm'+1×m'×k×k. Although a singular value of a 4-way tensor
is not well-defined, we can perform a low rank approximation of the weight matrix by folding out
the tensor to a large matrix. Actually, considering “similarity” between the filters as
K(')
k,k	∖ m'+1
X W (') W (')	)
i,c,κ1,κ2	j,c,κ1,κ2
κ1,κ2=1	i,j =1
∈ Rm'+ι ×m'+ι
then We can easily see that, for s` ∈ [m'+1], it holds that
m'+ι
kW⑶-PTW0kF X σjo(K(')),
∖ j0=m
where P ∈ Rs'×m'+1 is a projection matrix to the eigen-space corresponding to the s` largest
singular values of K('), W0 := PW(') = (Pm=11 Pi,ioWi(o'j,k,ko)i,j,k,k0∈[s`] × [m`] × [k] × [k] and the
Frobenius norm ∣∣ ∙ |山 of a tensor is the Euclidean norm as a vector. Therefore, we can use the
eigenvalues of K(')to evaluate the redundancy of parameters among filters.
As for the covariance matrix in a convolution layer, we also apply the same argument. That is, the
input to the '-th layer (which is a convolution layer) is given by φ'(χ) ∈ Rm'×I×J where I and J
are the width and height of the input, and we define the following “covariance” matrix as a similarity
measure between the channels:
(∖ m`,m`
n ^X	^X	^X	φ',(i,c1,c2)(XiO)φ',(j,c1,c2)(XiO))	∈ Rm'×m'
i0 =1 1≤c1 ≤I 1≤c2≤J	i,j=1
This also serve the redundancy measure and analogous argument to the main text can be applied.
31
Published as a conference paper at ICLR 2020
Near low rank properties of the covariance matrix and weight matrix Here, We see plausibil-
ity of the near loW rank assumptions We made in the analysis. Figure 1 presents the eigenvalues of
covariance matrix Σ(') in each of layer c2, c7, c12 and l16. The eigenvalues are sorted m decreas-
ing order. We can see that the eigenvalue distributions are highly concentrated around 0 and the
eigenvalues decrease quickly, which indicates the near low rank property of 夕⑷.
(a) c2
Iayerxl2 (cov)
9n"> .J-n6u"
O 1∞	200	300	400	500
index
(c) c12
。-e> ,JeInSU"9n"> .J-n6u-s
(b) c7
layer： Il 6 (cov)
20
O
O IOOO 2000	3000	4000
index
(d) l16
Figure 1: Eigenvalue distribution of the covariance matrices
Next, We plot the eigenvalues of K(')in Figure 2 for layer c2, c7, c12 and 116. We again observe a
rapid decrease of the eigenvalues. These results justify our theoretical assumptions.
。-eΛ∙J-nβu"
Iayenc2 (weight mat)
9n"> .J-n6u"
Iayerχ7 (weight mat)
2.5
9n"> .J-n6u"
(c) c12
Figure 2: Singular-value distribution of the weight matrices
(d) l16
Intrinsic dimensionality Here, we calculate the intrinsic dimensionalities of the VGG-19 net-
work. For that purpose, we set a threshold parameter ν ∈ {10-1 , 10-2, 10-3} and compute
32
Published as a conference paper at ICLR 2020
Table 3: The effective ranks rm` and s` in each layer for each threshold ν. “In/Out” indicates the
channel sizes of the input and output.
layer	In/Out	Cov (m `)			Weight (s`)			
		ν : 10-1	10-2	10-3	v:10-1	10-2	10-3
-Co-	3/6^	3	3-	3-	9	16	19^
c1	64/64	5	21	51	26	62	64
c2	64/128	5	33	64	37	110	128
c3	128/128	9	76	128	50	128	128
c4	128/256	11	110	128	102	255	256
c5	256/256	13	200	256	93	256	256
c6	256/256	17	219	256	55	230	256
c7	256/256	11	110	253	37	175	252
c8	256/512	10	35	129	30	122	349
c9	512/512	6	33	79	16	60	217
c10	512/512	3	15	41	16	42	127
c11	512/512	2	12	23	11	39	129
c12	512/512	2	7	16	7	18	46
c13	512/512	3	6	16	9	22	46
c14	512/512	3	7	18	16	38	59
c15	512/512	4	14	28	36	51	92
l16	512/4096	5	10	11	11	21	49
l17	4096/4096	7	10	11	10	49	990
l18	4096/10	6	10	10	9	9	9
U C ■	_ Γ	1 I / T/ ∖ 、	/ T/ ∖ 1	_ _ 4	♦	U C 	_ Γ 1 I /R ∖ 、
s` := #{j ∈ [m'+ι] | σj(K(')) ≥ V × maxj0 σj∙o(K('))} and m` := #{j ∈ [m`] | σj(Σ(')) ≥
v X maxjo σjo(∑('))} (which corresponds to setting r； = 4ν X maxjo σjo(∑('))). Table 4 sum-
marizes the effective ranks rm`, s` of all layers. We can see that the effective ranks can be much
smaller than the channel sizes in several layers (especially layers from c8 to l18,), which indicates
the network has high redundancy and its intrinsic dimensionality could be much smaller than the
actual number of parameters.
Next, We compute the intrinsic dimensionality of each layer based on the effective ranks calculated
above. Basically, the main term of our bound (Theorem 4) is given by Jp3 m'+1m' (note that
m' is essentially controlled by rm`). Hence, we employ rm'+ι X rm` as the intrinsic dimensionality
of each fully connected layer. As for a convolution layer, we employ rm'+γτm`k2 as the intrinsic
dimensionality which is the number of parameters of compressed network. We also calculate the
intrinsic dimensionality obtained by compressing only the weight matrix. That is given by s` m`k2 +
m'+ιs'. Both of them are summarized in Table 4. We can see that the intrinsic dimensionality is
smaller than the actual number of parameters. In particular, it is much smaller for higher layers
such as c8 to l18. This indicates that the information required for classification is almost distilled
in the first few layers and the contribution of the subsequent layers would be much smaller than the
earlier layers. Moreover, we see that compressing the network using the covariance matrix gives
smaller intrinsic dimensionalities than the weight matrix. This is because the improvement induced
by decreasing Tm` is a quadratic order but that by s` is just a linear order. Another reason is that the
effective rank of the covariance matrix is more data dependent in a sense that it is strongly dependent
on the distribution of the data, and thus it can capture data dependent redundancy more efficiently
(see also Figures 1 and 2). Since the intrinsic dimensionality is much smaller than the actual number
of parameters, the VC-dimension bound is too pessimistic and a compression based bound like ours
gives a better generalization error bound.
Finally, we give a comparison of intrinsic dimensionalities calculated by Arora et al. (2018) and
ours. We borrowed the values presented in the paper (Arora et al., 2018). We would like to note
that the comparison is not completely fair because the intrinsic dimensionality of both our analysis
and that of Arora et al. (2018) neglect constant functors (such as depth), and thus the final gener-
alization error is not merely determined by the raw values. However, the comparison offers better
understanding of our analysis by observing difference and similarity between them. We can see
33
Published as a conference paper at ICLR 2020
Table 4: The intrinsic dimensionality in each layer for each threshold ν . Here again, “In/Out”
indicates the channel sizes of the input and output. “Orig” indicates the number of parameters
(m'+ιm' X filter size) in each layer.
layer	In/Out	Orig	Cov			Weight		
			V :10T	10-2	10-3	v :10T	10-2	10-3
^^C0-	3/6^	∏728^	135^	567^	1,377	9T0^	910	9Γ0-
cl	64/64	36,864	225	6,237	29,376	1,920	1,920	1,920
c2	64/128	73,728	405	22,572	73,728	6,336	11,264	13,376
c3	128/128	147,456	891	75,240	147,456	33,280	79,360	81,920
c4	128/256	294,912	1,287	198,000	294,912	52,096	154,880	180,224
c5	256/256	589,824	1,989	394,200	589,824	128,000	327,680	327,680
c6	256/256	589,824	1,683	216,810	582,912	261,120	652,800	655,360
c7	256/256	589,824	990	34,650	293,733	238,080	655,360	655,360
c8	256/512	1,179,648	540	10,395	91,719	154,880	647,680	720,896
c9	512/512	2,359,296	162	4,455	29,151	189,440	896,000	1,290,240
c10	512/512	2,359,296	54	1,620	8,487	153,600	624,640	1,786,880
c11	512/512	2,359,296	36	756	3,312	81,920	307,200	1,111,040
c12	512/512	2,359,296	54	378	2,304	81,920	215,040	650,240
c13	512/512	2,359,296	81	378	2,592	56,320	199,680	660,480
c14	512/512	2,359,296	108	882	4,536	35,840	92,160	235,520
c15	512/512	2,359,296	180	1,260	2,772	46,080	112,640	235,520
116	512/4096	2,097,152	35	100	121	73,728	175,104	271,872
117	4096/4096	16,777,216	42	100	110	294,912	417,792	753,664
118	4096/10	40,960	42	90	90	45,166	86,226	201,194
Table 5: Comparison of the intrinsic dimensionality of our analysis and that in Arora et al. (2018).
layer	In/Out	Orig	Arora etal. (2018)	Cov			
				v:10-1	10-2	10-3
-c0-	3/6^	1,728^	164^	135^	567	-~1377-
c3	128/128	147,456	644,654	891	75,240	147,456
c5	256/256	589,824	3,457,882	1,989	394,200	589,824
c8	256/512	1,179,648	36,920	540	10,395	91,719
c11	512/512	2,359,296	22,735	36	756	3,312
c14	512/512	2,359,296		26,584	108	882	4,536
that our intrinsic dimensionality gives a smaller number than theirs. This is because compression
through the covariance matrix gives quadratic factor improvement while compression through low
rank property of the weight matrices gives linear order improvement. This indicates considering
near low rank properties of both of weight matrices and covariance matrices yields sharper bounds.
It can be realized by our unified theoretical frame-work.
34