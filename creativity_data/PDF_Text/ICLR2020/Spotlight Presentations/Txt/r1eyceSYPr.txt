Published as a conference paper at ICLR 2020
Unbiased Contrastive Divergence Algorithm
for Training Energy-Based Latent Variable
Models
Yixuan Qiu
Department of Statistics and Data Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
yixuanq@andrew.cmu.edu
Lingsong Zhang & Xiao Wang
Department of Statistics
Purdue University
West Lafayette, IN 47907, USA
{lingsong, wangxiao}@purdue.edu
Ab stract
The contrastive divergence algorithm is a popular approach to training energy-
based latent variable models, which has been widely used in many machine learn-
ing models such as the restricted Boltzmann machines and deep belief nets. De-
spite its empirical success, the contrastive divergence algorithm is also known to
have biases that severely affect its convergence. In this article we propose an un-
biased version of the contrastive divergence algorithm that completely removes its
bias in stochastic gradient methods, based on recent advances on unbiased Markov
chain Monte Carlo methods. Rigorous theoretical analysis is developed to justify
the proposed algorithm, and numerical experiments show that it significantly im-
proves the existing method. Our findings suggest that the unbiased contrastive
divergence algorithm is a promising approach to training general energy-based
latent variable models.
1	Introduction
Energy-based latent variable models cover a broad class of generative models that are frequently
used to characterize sophisticated distributions of high-dimensional data. Popular examples of this
class include the restricted Boltzmann machines (RBM, Smolensky, 1986; Hinton, 2012), deep be-
lief nets (Hinton et al., 2006), and exponential family harmoniums (Welling et al., 2005), among
many others. Energy-based models are complementary to directed generative models such as the
variational autoencoders (Kingma & Welling, 2014), and have gained great success in synthesizing
realistic data samples (Xie et al., 2016; 2018b). They can also be combined with directed models to
build more sophisticated structures (Xie et al., 2018a). In this article we focus on the energy-based
latent variable model, whose general form can be expressed in terms of the joint distribution of a
visible random vector, v ∈ V ⊂ Rp , and a hidden or latent random vector, h ∈ H ⊂ Rr , with the
density function
p(v, h； θ) =	exp{-E(v, h; θ)},	(1)
Z(θ)
where θ ∈ Θ is the unknown parameter vector, E(v, h; θ) is the energy function, and Z(θ) is a
normalizing constant to ensure that p(v, h; θ) is a legitimate probability density or mass function.
The model distribution, pv(v; θ), is defined to be the marginal distribution of p(v, h; θ).
Similar to many other machine learning models, the standard approach to estimating the parameter
vector θ is the maximum likelihood method. It can be shown that the derivative of the log-likelihood
function can be expressed as the difference of two expectations, and hence Monte Carlo methods,
especially the Markov chain Monte Carlo (MCMC, Gilks et al., 1995), can be used to approximate
the gradient. Various optimization techniques, such as the stochastic gradient method (SG, Robbins
& Monro, 1951; Bottou, 2010), can then proceed to iteratively update the parameter estimate. This
strategy, though elegant in theory, is not without limitations. In particular, MCMC estimators are
typically consistent in the limiting case, but biased on finite steps, so one needs to run MCMC for a
long time to obtain an accurate gradient, which would take tremendous amount of computing time.
1
Published as a conference paper at ICLR 2020
To reduce the computational complexity, Hinton (2002) proposed a simple and fast algorithm, called
the contrastive divergence (CD) algorithm. The basic idea of CD is to truncate MCMC at the k-th
step, and use the resulting approximate gradient to update θ, where k is a fixed integer as small as
one. Such an approach is usually referred to as the CD-k algorithm. The simplicity and computa-
tional efficiency of CD makes it widely used in many popular energy-based models, and there was
also numerous empirical evidence to illustrate the effectiveness of CD. More recently, the CD idea
is applied to directed generative models (Ruiz & Titsias, 2019), where CD is used to define a loss
function that combines variational inference and MCMC.
However, the success of CD also raised a lot of questions regarding its convergence properties. Both
theoretical and empirical results show that CD in general does not converge to a local minimum
of the likelihood function (Carreira-Perpinan & Hinton, 2005), and diverges even in some simple
models (Schulz et al., 2010; Fischer & Igel, 2010). The main issue of CD is that the truncation of
MCMC produces a biased stochastic gradient for the log-likelihood function in every iteration, and
such uncontrolled biases may be accumulated to distort the true ascent direction. Due to this reason,
the training of energy-based models has been a longstanding challenge in machine learning research.
In this article, we propose a new unbiased contrastive divergence (UCD) algorithm based on recent
advances in unbiased MCMC theory, which offers new possibilities for solving the model training
problem. In the seminal work Glynn & Rhee (2014), the authors developed an unbiased estimator
for the expectation with respect to the invariant distribution of a Markov chain. More recently,
this estimator was further extended to the MCMC setting by Jacob et al. (2017), using a technique
called coupling. At a high level, by carefully designing the MCMC algorithm, one is able to get an
unbiased MCMC estimator with only finite number of Markov transitions.
Under the framework of Glynn & Rhee (2014) and Jacob et al. (2017), the proposed UCD algorithm
is a Gibbs-sampler-based training method for energy-based latent variable models. We prove that the
stochastic gradient generated by UCD is unbiased with a finite variance, which implies the conver-
gence of SG based on it. Similar to CD-k, UCD generates Markov chains to compute the gradient,
but the chain in UCD stops at a random time, instead of a fixed one as in CD-k . The theoretical
analysis indicates that the stopping time has a finite expectation, so on average the computation
can be completed in finite time. Besides theoretical justifications, our numerical experiments show
that UCD significantly improves existing training algorithms, suggesting that it is a promising ap-
proach with a solid convergence guarantee. The implementation of the UCD algorithm is available
at https://github.com/yixuan/cdtau. The highlights of this article are as follows:
•	We develop a new training algorithm for general energy-based latent variable models that
include many popular models (e.g. RBM) as special cases. To our best knowledge, this is
the first algorithm that has a solid convergence guarantee for such models.
•	The proposed algorithm resolves a longstanding problem of the CD algorithm, the bias in
approximating the gradient. In particular, our method is completely unbiased, and theoret-
ical justifications are developed to guarantee its convergence.
•	We have tailored a specialized algorithm for RBM, which is shown to significantly reduce
the computational cost.
2	A B rief Review of Contrastive Divergence
In this section we briefly review the CD algorithm, and point out some of its weaknesses that have
been studied in the existing literature. For a single observation v , the marginal data log-likelihood
function is `(θ; v) = log{pv(v; θ)} = log{ p(v, h; θ)dh}. Assume that E(v, h; θ) is contin-
uously differentiable for θ, and then with n data points D = (v1, . . . , vn), the derivative of the
log-likelihood function `(θ; D) = P `(θ; vi), also known as the score function, can be written as
∂'(θ; D)
—∂θ — = -n
E(v,h)~P(D)p(hMθ) {dE⅞θ^} - E(v,h)~p("θ) {dE⅞θ^}1, (2)
whereP(D) stands for the empirical distribution of D, andp(h∣v; θ) is the conditional distribution of
the latent variable h given v = v. A simple derivation of (2) can be found in Fischer & Igel (2014).
Throughout this article We denote X = (v, h) ∈ X := V X H and f(x; θ) = ∂E(v, h; θ)∕∂θ. Then
2
Published as a conference paper at ICLR 2020
the two expectations in (2) can be abbreviated as ED{f (x; θ)} and EM {f (x; θ)}, respectively,
where M := p(v, h; θ) is the complete model distribution.
In many cases, for example the RBM model, ED {f (x; θ)} has a closed form, so the major com-
putational difficulty comes from the EM {f (x; θ)} term. A common scheme to approximate this
expectation is to run a Markov chain ξo → ξι → ∙∙∙ with M as the invariant distribution, and
then under mild conditions we have limt→∞ E{f(ξt; θ)} = EM {f (x; θ)}. Of course, such a limit
cannot be reached in finite steps, so the CD-k algorithm truncates the Markov chain at the k-th step,
resulting in the following approximation:
∆(θ) ：= - [Ed{f(x; θ)}- f(ξk; θ)].	(3)
It is easy to see that ∆(θ) ≈ n-1∂'(θ; D)∕∂θ is a stochastic approximation to the true gradient,
so one can use SG to update θ via the iteration θi+1 = θi + αi∆(θi), where θi is the parameter
estimate in the i-th iteration, and αi is the step size.
Despite its simplicity, various research articles have pointed out the weaknesses of the CD-k algo-
rithm. For instance, Sutskever & Tieleman (2010) gave an example to show that E{∆(θ)} is not the
gradient of any objective function, and Schulz et al. (2010); Fischer & Igel (2014) studied numerical
experiments in which CD-k does not converge at all for small k values. Carreira-Perpinan & Hin-
ton (2005) considered the fixed points of ∆(θ), the θ values such that E{∆(θ)} = 0, and showed
that they do not match the fixed points of ∂'(θ; D)∕∂θ in general. This implies that even if CD-k
converges, the resulting parameter estimate may not be a local minimum of the likelihood function.
Another variant of CD is the persistent contrastive divergence (PCD, Tieleman, 2008; Tieleman &
Hinton, 2009), which has been reported to improve CD in many numerical experiments. However,
it is still an approximation method, and its convergence properties are more difficult to analyze, as
the stochastic gradients generated by PCD become correlated across iterations. In fact, Schulz et al.
(2010); Fischer & Igel (2010) also gave examples in which PCD failed to converge. There are also
some other training methods as extensions to CD, such as the multi-grid method (Gao et al., 2018)
and the short-run MCMC (Nijkamp et al., 2019), but all these methods inherit the bias of CD.
To summarize, it is surprising that virtually none of the popular training methods for energy-based
models, including CD and PCD, provide a solid convergence guarantee. The major defects of CD
stem from the fact that ∆(θ) is a biased estimator for the true log-likelihood gradient, and SG may
fail with uncontrolled bias accumulation. To this end, the ultimate solution is to design a training
algorithm that completely removes the bias of CD.
3 The Unbiased Contrastive Divergence Algorithm
3.1	Unbiased MCMC Estimators
Since CD highly relies on the MCMC method, the main ingredient of the proposed UCD algorithm is
the theory of unbiased MCMC developed by Glynn & Rhee (2014) and Jacob et al. (2017). Consider
the second term in (2), namely, EM{f(x; θ)}. In what follows we omit the dependence on θ for
brevity if no confusion is caused. If a Markov chain {ξt} satisfies E{f(ξt)} → EM{f(x)} as
t → ∞, then under some regularity conditions, we can express the limit as a telescoping sum,
∞
EM{f(x)} = E{f(ξk)} + X [E{f(ξt)} - E{f(ξt-1)}]
t=k+1
for any fixed k ≥ 0. Now assume that there exists another Markov chain {ηt } such that ξt and ηt
have the same marginal distributions for all t ≥ 0, and ξt = ηt-1 for all t ≥ τ, where τ is some
random time. If we allow the exchange of expectation and summation, then we would get
∞
EM{f(x)} =E f(ξk)+ X {f(ξt) -f(ηt-1)}
t=k+1
τ-1
E f(ξk)+ X {f(ξt) -f(ηt-1)}
t=k+1
where the first identity holds since E{f(ξt)} = E{f(ηt)} for all t ≥ 0, and the second one is due to
the fact that ξt = ηt-1 fort ≥ τ. As a consequence, the quantity f(ξk)+Ptτ=-k1+1{f(ξt)-f(ηt-1)}
is an unbiased estimator for EM {f (x)}. Such an idea seems rather simple, but the construction of
the chain {ηt}, which we describe in the next section, is a highly non-trivial task.
3
Published as a conference paper at ICLR 2020
3.2	Coupling of Markov Chains
Let Mt denote the marginal distribution of a Markov chain {ξt} at the t-th step. By construction,
Mt converges to M as t → ∞. To develop the unbiased estimator Hk (ξ, η), the second chain {ηt}
must satisfy two conditions: (1) marginally η 〜Mt； (2) {ξt} and the lag-one sequence {ηt-ι} will
meet and stay identical after some random time τ. Condition (1) can be trivially met if {ξt} and {ηt}
are sampled independently. However, in this way the probability that ξt = ηt-1 may be extremely
small, or even be zero for continuous random variables. Therefore, a special joint distribution for
(ξt,ηt-ι) needs to be assigned subject to ξt 〜 Mt and m 〜 Mt-ι. Such a pair of random
variables under the marginal distribution constraints is called a coupling, and for our purpose we
attempt to seek a coupling scheme such that P(ξt = ηt-1) > 0. Figure 1 illustrates the coupling
process of two Markov chains {ξt} and {ηt-1}.
Figure 1: An illustration of the coupling
process. {ξt} and {ηt} start from the
same value, and have the same marginal
distribution Mt at each step. The two
chains are correlated in such a way that
the event ξt = ηt-1 occurs with a posi-
tive probability for each t. After a ran-
dom time τ (τ = 5 in the illustration),
{ξt} meets {ηt-1} and they stay identi-
cal afterwards.
To implement such a coupling, first let {ξt} and {ηt } start from the same initial value ξ0 = η0,
and additionally draw ξι 〜T(∙∣ξo), where T(y|x) stands for the transition density function from
state X to state y. Next, we need to draw (ξ2, ηι) such that marginally ξ2 〜M2 and ηι 〜Mi,
which can be a difficult task as Mt may not have closed forms. Fortunately, it is much simplified for
Markov chains: due to the Markov property, ξ2 and η1 will have the requested marginal distributions
if we sample ξ2∣ξ1 〜T(∙∣ξι) and η1∣η0 〜T(∙∣ηo) conditional on ξι and η0. That is, the coupling
of Markov chains can be achieved by the coupling of one-step transitions, which is a much simpler
task. Define two density functions p(∙) = T(∙∣ξι) and q(∙) = T(∙∣no), and then the problem reduces
to drawing a coupling (ξ,n) such that ξ 〜 p(∙), n 〜 q(∙), and P(ξ = n) > 0, which can be
accomplished via the maximal coupling technique (Appendix A.1).
Specific to our problem (2), we need to sample x = (v, h) from p(v, h; θ). In energy-based latent
variable models, the most widely-used MCMC method is the Gibbs sampler (Geman & Geman,
1984), which sequentially updates one block of x based on the conditional distribution of this block
given the rest. As an example, in RBM models v|{h = h} and h|{v = v} follow multivariate
Bernoulli distributions with independent components, which are very easy to sample from. The
coupling for Gibbs samplers was briefly mentioned in Jacob et al. (2017) as a special case of the
Metropolis-Hastings scheme (Metropolis et al., 1953; Hastings, 1970), but next we show that some
specific structure of Gibbs samplers can be utilized to simplify the process.
For simplicity and clarity, we assume that the Gibbs sampler for M follow the natural division of
blocks x = (v, h). That is, one can easily sample from the two transition distributions Tv(v|h) :=
p(v|h; θ) and Th(h|v) := p(h|v; θ). The more sophisticated cases, for example h consists of
multiple layers h = (h1, . . . , hL), can be dealt with similarly. In Algorithm 1, we describe the steps
to sample two coupled chains {ξt = (vt, ht)} and {ηt = (v0t, h0t)} based on the Gibbs sampler.
Three remarks are made for Algorithm 1: (1) The meeting event (line 4) only depends on the Tv
transition density. To verify this, note that at the t-th step, we need to draw ξt∣ξt-ι ~ T(∙∣ξt-ι) and
nt-1∣nt-2 〜T(∙∣m), where T(v, h|v, h) = T(v∣h)Th(h∣v) is the transition density for a full
00
update cycle. It is easy to show that T(v, h∣v0, h0)∕T(v, h|v, h) = Tv(v∣h0)∕TV(v|h), so the Th
part cancels in the ratio. (2) Once ξt and ηt-1 meet, they stay identical afterwards, because by then
Tv (∙∣ht-2) = Tv (∙∣ht-ι), and the event in line 4 always happens. (3) Line 7 is a rejection sampling
step. In our numerical experiments we find that very few samples are rejected, so its cost is tiny.
4
Published as a conference paper at ICLR 2020
Algorithm 1 Coupling method for the Gibbs sampler
Input: Densities Tv(v|h) and Th(h|v), initial values ξ0 = (v0, h0) = η0 = (v00, h00), Tmax
Output: Coupled chains {ξt } and {ηt}
1:	Sample vι 〜Tv(∙∣ho) and hi 〜Th(∙∣vι). Set ξι = (vι, hi)
2:	for t = 2, 3, . . . do
3:	Sample Vt 〜Tv(∙∣ht-ι), ht 〜Th(∙∣vt), and U 〜Uniform(0,1)
4:	if U ≤ Tv(vt|h0t-2)/Tv(vt|ht-i) or t ≥ Tmax (maximum stopping time) then
5:	Set ξt = (vt, ht), ηt-i = ξt
6:	else
7:	Sample vt-i 〜Tv(∙∣ht-2), h0t-ι 〜Th(∙∣vt-ι), and U0 〜Uniform(0,1)
untilU0>Tv(v0t-i|ht-i)/Tv(v0t-i|h0t-2)
8:	Set ξt = (vt, ht), ηt-i = (v0t-i , ht-i)
9:	end if
10:	end for
3.3 Unbiased Contrastive Divergence
The technical tools introduced in Sections 3.1 and 3.2 enable us to develop a new algorithm to train
model (1). Recall that the true gradient of the log-likelihood function is given by (2). The first term,
ED{f(x; θ)}, can be computed exactly, and the second term, EM{f(x; θ)}, is approximated by an
unbiased estimator g2(θ) ：= f (ξk) + PT=k+ι{f (ξt) - f (ηt-1)}, where the coupled Markov chains
{ξt} and {ηt} are generated by Algorithm 1. Assume that the parameter vector θ lies in a closed
convex set Θ, and let Pθ(∙) denote the projection onto Θ. Putting the pieces together, Algorithm
2 illustrates the UCD algorithm for training energy-based latent variable models. The initial chain
length k can be any fixed number, and in this article we take k = 1 for all the numerical experiments.
Algorithm 2 UCD Algorithm for estimating θ
Input: T , {αi }, k, initial value θ0
Output: Parameter estimate for θ
1:	for i = 0, 1, . . . , T - 1 do
2:	Draw one data point V 〜p(D), and sample h 〜p(h∣v; θi)
3:	Set ξ0 = η0 = (v, h), and run Algorithm 1 with θ = θi until ξτi = ητi-i
4:	g(θ) 一 一Eh〜p(h∣va{f (v, h; θ)} + f (ξk) + PT=k+i{f (ξt) - f (ηt-i)}
5:	θi+i - Pθ (θi + αi ∙ g(θi))
6:	end for
7:	return θ = TT PT=I θi
Next, we analyze the theoretical property of Algorithm 2 and state the conditions for it to converge.
As a standard setting, We assume that the Markov chains generated by the GibbS sampler are 夕-
irreducible and aperiodic (Meyn & Tweedie, 2012). This is a very mild assumption that every
practical Gibbs sampler should satisfy. Then we make the following two assumptions that guarantee
the convergence of Gibbs samplers.
Assumption 1. (Drift condition) There exist a pair of functions r : V → [1, +∞), l : H → [1, +∞)
and constants γi , γ2 , Li , L2 > 0 such that γi γ2 < 1 and
Ev〜p(v|h；e)r(v) ≤ γιl(h) + Li, Eh〜p(h|v；e)l(h) ≤ Yr(v + L?, ∀v ∈ V, h ∈ H, θ ∈ Θ.
Also, there exist constants C > 0 andD > 0 such that |f (x; θ)∣2+c ≤ l(h) andEh〜ph(h；e)l(h) ≤ D
for all x = (V, h) ∈ X and θ ∈ Θ.
Assumption 2. (Minorization condition) There exist constants d > 2(γ2Li + L2)/(1 - γiγ2),
ε > 0, and a density function q(∙) such that p(v∣h; θ) ≥ εq(v) for all h ∈ D, V ∈ V,and θ ∈ Θ,
where D = {h ∈ H : l(h) ≤ d}.
In the following theorem we show three important facts about the proposed stochastic gradient g(θ):
(1) g(θ) is unbiased for the true score function; (2) it has a bounded second moment uniformly in θ;
(3) in expectation it can be computed in finite time.
5
Published as a conference paper at ICLR 2020
Theorem 1. Under Assumptions 1 and 2, there exist constants D1,D2 > 0 such that E{0(θ)}=
∂'(θ; v)∕∂θ, E [{g2(θ)}2] ≤ Di, and E(Ti) ≤ D2 forall θ ∈ Θ and i = 1,2,...,T — L
Theorem 1 provides the building blocks for the convergence analysis of Algorithm 2. With the
unbiased gradient estimator and the bounded second moment, we establish a solid convergence
guarantee for the proposed algorithm. As a typical setting, in the following corollary we consider a
convex log-likelihood function.
Corollary 1. Assume that `(θ; v) is convex and L-Lipschitz continuous in θ ∈ Θ, and Θ is a
closed and bounded convex set. Then by choosing ai = αo∕√7 for some constant ɑ0 > 0, we have
'* — '(0; V) ≤ O(1∕√T), where '* is the maximum value of '(θ; V).
The proof Corollary 1 is standard, see for example Bottou (2010); Bottou et al. (2018). When the
log-likelihood function is nonconvex as in the RBM model, there are also other versions of the
convergence result for SG, for example Theorem 4.10 of Bottou et al. (2018). Such directions can
be studied separately and are omitted here.
Finally, we shall point out an important special case of Theorem 1, i.e., if the Markov chain {ξt} has
finite states, then the two assumptions are automatically satisfied. This shows that many widely-used
models, for example RBM, can directly use the UCD algorithm without the need to find such r(∙)
and l(∙) functions. We summarize this useful fact in the following corollary.
Corollary 2. If X is a finite state space and Θ is compact, then Assumptions 1 and 2 hold, and
Theorem 1 applies.
4 Training Restricted B oltzmann Machines
RBM is one of the most popular and widely-used energy models in machine learning, defined by
the energy function E(V, h; θ) = —VTb — VTWh — hTc, where V ∈ {0, 1}m, h ∈ {0, 1}n,
and θ = (W , b, c) are model parameters. The Gibbs sampler for RBM has a nice structure: let
σ(x) = 1/(1+ exp(—x)) be the sigmoid function, and then v∣{h = h}〜BernoUlli(σ( Wh + b))
and h∣{v = v} 〜 BernoUlli(σ(WTV + c)). The coupling method in Algorithm 1 directly works
for RBM, but here we show an improved version that is tailored for RBM and is more efficient.
Let u, p ∈ Rr, and the notation y = 1{u ≤ p} stands for a binary vector such that yi = 1 ifui ≤ pi
and yi = 0 otherwise. Also let Tv(V|h) = Qim=1 pivi (1 — pi)1-vi denote the transition density from
h to v, where p = (p1, . . . ,pm)T = σ(Wh + b). Then the specialized coupling method for RBM
is given in Algorithm 3.
Algorithm 3 Coupling method for RBM
Input: Model parameters W, b, c, step-t states ξt = (Vt, ht), ηt-1 = (V0t-1, h0t-1)
Output: New states ξt+1 = (Vt+1, ht+1), ηt = (V0t, h0t)
1:	Sample Ui 〜Uniform(0,1), Z1 〜Uniform([0,1]m), and set vt+i = 1{Zi ≤ σ(Wht + b)}
2:	if U1 ≤ Tv(Vt+1|h0t-1)/Tv(Vt+1|ht) then
3:	Set V0t = Vt+i
4:	else
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
repeat
Sample U2 〜Uniform(0,1), U0 〜Uniform(0,1), Z2 〜Uniform([0,1]m)
if Vt+i has not been accepted then
Propose Vt+i = 1{Z2 ≤ σ(Wht + b)}, accept if U2 > Tv(Vt+i|h0t-i)/Tv(Vt+i|ht)
end if
if V0t has not been accepted then
Propose V0t = 1{Z2 ≤ σ(W h0t-i + b)}, accept if U20 > Tv(V0t|ht)/Tv(V0t|h0t-i)
end if
until Vt+i and V0t are both accepted
end if
Sample Z3 〜Uniform([0,1]n)
Set ht+i = 1{Z3 ≤ σ(W TVt+i + c)}, h0t = 1{Z3 ≤ σ(W TV0t + c)}
6
Published as a conference paper at ICLR 2020
The intuition behind Algorithm 3 is the following: line 2 indicates that it is also a maximal coupling
method, so the probability P (ξt+1 = ηt) is the same as Algorithm 1. However, in the event {ξt+1 6=
ηt}, ξt+1 and ηt are independent in Algorithm 1 but correlated in Algorithm 3, achieved by the use of
common random variates Z2 and Z3. The correlation between ξt+1 and ηt helps to make P (ξt+2 =
ηt+1) larger, thus accelerating the meeting of {ξt} and {ηt-1}. A more rigorous justification of this
algorithm is given in Appendix A.2.
Finally, it is known that in the gradient expression (2), f(x; θ) = (σ(W h + b)hT, σ(Wh + b), h)
for RBM, corresponding to the parameters θ = (W , b, c). With the coupled chains {ξt} and {ηt-1},
RBM can then be trained using UCD given by Algorithm 2.
5	Related Work
In this section we highlight the novelty of our article and clarify its overlap with prior art. In literature
there were several attempts to prove the convergence of CD in special cases, or to reduce the bias of
CD using other sampling techniques, all with undesirable results. For example, Yuille (2005) gave
conditions for CD to converge, which unfortunately can hardly be satisfied in any realistic models.
Jiang et al. (2018) showed a convergence result of CD for the exponential families, but consequently
the model is restrictive and does not include the latent variable model. Krause et al. (2018) used
importance sampling to estimate the normalizing constant, which is consistent with a large sample.
However, it still induces a bias in the finite case, and the bias heavily depends on the choice of the
importance weights. In contrast, the UCD method proposed in this article directly fixes the bias of
CD, and hence bypasses the challenges in algorithm convergence.
Unbiased MCMC is a relatively new topic in statistics and machine learning. Some background
knowledge in this article, for example Section 3.1, is taken from Jacob et al. (2017), which estab-
lished a general framework for unbiased MCMC. Our new contributions are in the following aspects.
First, we have developed Algorithm 1 and Theorem 1 exclusively for the Gibbs sampler, taking into
account the special structure of Gibbs MCMC. Second, our theoretical results, including Theorem
1 and Corollary 2, have more practical assumptions than the ones in Jacob et al. (2017). For exam-
ple, one of their key assumptions is that E{∣f (ξt)∣2+c} is uniformly bounded for every finite step,
which is quite abstract and hard to verify in practice compared with our Assumption 1. Third, Jacob
et al. (2017) studied MCMC with a fixed target distribution, whereas we need to control the variance
of estimators that evolve with parameter updates. Finally, in Section 4 we develop a specialized
coupling algorithm for RBM, which is shown to be more efficient than the general one.
Another related work is the Markov chain Las Vegas method1 (MCLV, Savarese et al., 2018), which
also constructs an unbiased estimator for the score function of RBM. The main differences bewteen
our method and MCLV are as follows: (1) MCLV is based on the regeneration theory of Markov
chains, whereas UCD is built upon the coupling technique. (2) MCLV is exclusively designed for
RBM, but UCD applies to a broader range of models. (3) In its current state, MCLV cannot handle
continuous random variables, but UCD can. More comprehensive comparisons between MCLV and
UCD are left for future exploration.
6	Numerical Experiments
6.1	Bars-and-Stripes Data
We compare CD-k, PCD, and the proposed UCD algorithm for training RBM models on different
data sets. In the first experiment we reproduce the results for the bars-and-stripes (BAS) data that
have been studied by Schulz et al. (2010); Fischer & Igel (2010; 2014). It is a small data set with
36 data points and 16 binary variables, and is fit by a small model with 16 hidden units. However,
it is one of the most important benchmark data sets for RBM since its log-likelihood value can be
evaluated exactly, and it demonstrates the divergence of CD-based training algorithms. In our study,
k is set to 1 for CD (more experiments with larger k are given in Appendix B.1), and each algorithm
is run for 100 times, accounting for the randomness in the training process. A common learning
1We were informed of this method by a public comment on https://openreview.net/forum?id=
r1eyceSYPr, after the current paper was accepted.
7
Published as a conference paper at ICLR 2020
rate α = 0.01 is set, and 1000 parallel Markov chains are used to approximate the gradient in each
iteration. The results are shown in Figure 2.
O 2500	50∞	75∞	10∞0 O 25∞	50∞	75∞ 100∞ O 2500 50∞ 75∞ 100∞
Iteration	Iteration	Iteration
Figure 2: Left: exact log-likelihood values in each iteration. The shaded bands stand for the 2.5%
and 97.5% quantiles across 100 runs, and the three trajectories in darker colors are sample learning
curves in one run. Middle: average stopping time τ for UCD in each iteration. Right: average
number of rejected samples in the coupling algorithm for UCD.
Figure 2 shows the following findings. First, we reproduce the results in Fischer & Igel (2014)
that CD and PCD fail to converge to the true maximum likelihood value. In contrast, UCD does
converge. Second, UCD has an adaptive choice of the stopping time in the Markov chain, compared
to the fixed k in CD. In the BAS data, the stopping time has a steep increase around the 1200th
iteration. Interestingly, this is exactly where CD begins to fail. An interpretation of this phenomenon
is that UCD automatically uses a large MCMC sample for parameter values that result in a “hard”
distribution. An even more surprising fact is that the average stopping time τ for UCD is 2.40,
making it computationally more efficient than the CD-20 algorithm, where 20 is the smallest k such
that CD-k training is comparable to UCD (see Appendix B.1 for more discussions). Third, the cost
of the rejection sampling step in UCD (line 7 of Algorithm 1) is tiny, as the number of rejected
samples rarely goes above two. Finally, UCD does not see a massive increase in the variance. In
fact, at the end of training the quantile band for UCD is much narrower than those of CD and PCD.
All these findings further highlight the advantages of UCD.
6.2	Simulated RBM Data
In the second example we show that the findings for the BAS data can be observed in other model
settings. We simulate a data set from an RBM model with 200 visible units and 20 hidden units,
where the entries of weight and bias parameters are all generated from a N (0, 1) distribution. The
sample size of the simulated data set is 1000, and we fit an RBM model using 100 hidden units,
which is larger than the true model since we intend to mimic the common practice of overparame-
terization in RBM training. We use a common learning rate α = 0.2 and 1000 Markov chains in
each iteration for all three algorithms. The log-likelihood values are approximated by Monte Carlo
averages. The result is given in Figure 3, which shows similar patterns to the BAS data: CD and
PCD eventually diverge, whereas UCD follows the typical behavior of SG. Additional experiments
and discussions are given in Appendix B.2.
φ-e> Poof-φ--lωo-j
Figure 3: Approximate log-likelihood values for each algorithm on the simulated RBM data set.
8
Published as a conference paper at ICLR 2020
6.3	Fashion-MNIST Data
Next we consider the Fashion-MNIST data set2, a replacement for the well-known but overused
MNIST data set of handwritten digits (LeCun et al., 1990). Each data point in Fashion-MNIST
contains 784 values within [0, 1], representing a 28 × 28 greyscale image. The whole data set
contains 60000 images, and we binarize the data by treating original values as probabilities and
sampling from a Bernoulli distribution for each element. On the binarized data, we fit an RBM with
1000 hidden units, and train the model with different algorithms using a mini-batch size of 1000
and a learning rate α = 0.1. For each training algorithm, 1000 parallel Markov chains are used to
compute the gradient. Figure 4 demonstrates the training trajectories of the three algorithms.
8-B> Pooq--ωw-lωo~l
Figure 4: Approximate log-likelihood values for each algorithm on the Fashion-MNIST data set.
The training patterns for CD and PCD are surprising: the log-likelihood values ofCD are decreasing,
and PCD seems to bounce back and forth among three different paths. In fact, in Appendix B.3 we
show that CD-k does not converge even if k is as large as 30, whereas the average stopping time for
UCD is about 27. These results further demonstrate the superior performance of UCD.
7	Discussion
In this article we use the unbiased MCMC technique to estimate the score function of energy-based
latent variable models, which effectively fixes the bias of CD algorithms. Both the theoretical analy-
sis and the numerical experiments at different scales demonstrate that the impact of bias elimination
is huge.
It is expected that UCD may have slightly larger variance compared with CD and PCD, but we
emphasize that the value of UCD is not a simple question of bias and variance trade-off. This is be-
cause for MCMC-based methods, the variance can always be reduced by running more independent
Markov chains and taking the average, while removing the bias is highly non-trivial. Moreover, in
the context of SG methods, bias is typically more harmful than variance, as the former may lead to
divergence of the algorithm.
In terms of computational cost, it is true that UCD is in general slower than CD-1 or PCD, but it is
comparable to or even faster than CD-k that achieves a similar log-likelihood level. Therefore, UCD
is not meant to completely replace CD or PCD, but rather to serve as an important addition to the
existing training algorithms. In practice, a very useful technique is to first run the fast CD-1 or PCD
to the near-optimum, and then proceed with UCD for guaranteed convergence.
References
Leon Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of
COMPSTAT‘2010, pp. 177-186. Springer, 2010. 1, 3.3
Leon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine
learning. SIAM Review, 60(2):223-311, 2018. 3.3
2https://research.zalando.com/welcome/mission/research-projects/
fashion-mnist/
9
Published as a conference paper at ICLR 2020
MigUel A. Carreira-Perpinan and Geoffrey E. Hinton. On contrastive divergence learning. In Pro-
ceedings of the Tenth International Workshop on Artificial Intelligence and Statistics, 2005. 1,
2
Asja Fischer and Christian Igel. Empirical analysis of the divergence of gibbs sampling based
learning algorithms for restricted boltzmann machines. In International Conference on Artificial
NeuralNetworks,pp. 208-217. Springer, 2010. 1, 2, 6.1
Asja Fischer and Christian Igel. Training restricted boltzmann machines: An introdUction. Pattern
Recognition, 47(1):25-39, 2014. 2,2, 6.1, 6.1
RUiqi Gao, Yang LU, JUnpei ZhoU, Song-ChUn ZhU, and Ying Nian WU. Learning generative con-
vnets via mUlti-grid modeling and sampling. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 9155-9164, 2018. 2
StUart Geman and Donald Geman. Stochastic relaxation, gibbs distribUtions, and the bayesian
restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelligence, PAMI-6
(6):721-741, 1984. 3.2
Walter R Gilks, Sylvia Richardson, and David Spiegelhalter. Markov Chain Monte Carlo in Prac-
tice. Chapman and Hall/CRC, 1995. 1
Peter W Glynn and Chang-han Rhee. Exact estimation for markov chain eqUilibriUm expectations.
Journal of Applied Probability, 51(A):377-389, 2014. 1, 3.1
W. Keith Hastings. Monte carlo sampling methods Using markov chains and their applications.
Biometrika, 57(1):97-109, 1970. 3.2
Geoffrey E Hinton. Training prodUcts of experts by minimizing contrastive divergence. Neural
computation, 14(8):1771-1800, 2002. 1
Geoffrey E Hinton. A practical gUide to training restricted boltzmann machines. In Neural networks:
Tricks of the trade, pp. 599-619. Springer, 2012. 1
Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm for deep belief
nets. Neural computation, 18(7):1527-1554, 2006. 1
Pierre E Jacob, John O'Leary, and Yves F Atchade. Unbiased markov chain monte carlo with
coUplings. arXiv preprint arXiv:1708.03625, 2017. 1, 3.1, 3.2, 5,4, D.1, D.1, D.1
Bai Jiang, TUng-YU WU, Yifan Jin, Wing H Wong, et al. Convergence of contrastive divergence
algorithm in exponential family. The Annals of Statistics, 46(6A):3067-3098, 2018. 5
Alicia A Johnson and Owen BUrbank. Geometric ergodicity and scanning strategies for two-
component gibbs samplers. Communications in Statistics-Theory and Methods, 44(15):3125-
3145, 2015. D.1
Diederik P Kingma and Max Welling. Stochastic gradient vb and the variational aUto-encoder. In
Proceedings of the 2nd International Conference on Learning Representations, 2014. 1
Oswin KraUse, Asja Fischer, and Christian Igel. PopUlation-contrastive-divergence: Does consis-
tency help with rbm training? Pattern Recognition Letters, 102:1-7, 2018. 5
Yann LeCUn, Bernhard E Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne E
HUbbard, and Lawrence D Jackel. Handwritten digit recognition with a back-propagation net-
work. In Advances in Neural Information Processing Systems 2, pp. 396-404, 1990. 6.3
David A Levin and YUval Peres. Markov Chains and Mixing Times, volUme 107. American Mathe-
matical Soc., 2017. A.1
Nicholas Metropolis, Arianna W RosenblUth, Marshall N RosenblUth, AUgUsta H Teller, and Edward
Teller. EqUation of state calcUlations by fast compUting machines. The Journal of Chemical
Physics, 21(6):1087-1092, 1953. 3.2
10
Published as a conference paper at ICLR 2020
Sean P Meyn and Richard L Tweedie. Markov Chains and Stochastic Stability. Springer Science &
Business Media, 2012. 3.3
Erik Nijkamp, Mitch Hill, Song-Chun Zhu, and Ying Nian Wu. Learning non-convergent non-
persistent short-run mcmc toward energy-based model. In Advances in Neural Information Pro-
cessing Systems 32,pp. 5233-5243, 2019. 2
Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathemat-
ical Statistics, 22(3):400-407, 1951. 1
Jeffrey S Rosenthal. Minorization conditions and convergence rates for markov chain monte carlo.
Journal of the American Statistical Association, 90(430):558-566, 1995. D.1
Francisco JR Ruiz and Michalis K Titsias. A contrastive divergence for combining variational infer-
ence and mcmc. arXiv preprint arXiv:1905.04062, 2019. 1
Pedro HP Savarese, Mayank Kakodkar, and Bruno Ribeiro. From monte carlo to las vegas: Im-
proving restricted boltzmann machine training through stopping sets. In Thirty-Second AAAI
Conference on Artificial Intelligence, 2018. 5
Hannes Schulz, Andreas Muller, and Sven Behnke. Investigating convergence of restricted boltz-
mann machine learning. In NIPS 2010 Workshop on Deep Learning and Unsupervised Feature
Learning, 2010. 1,2, 6.1
Paul Smolensky. Information processing in dynamical systems: Foundations of harmony theory.
Technical report, Colorado Univ at Boulder Dept of Computer Science, 1986. 1
Ilya Sutskever and Tijmen Tieleman. On the convergence properties of contrastive divergence. In
Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,
pp. 789-795, 2010. 2
Tijmen Tieleman. Training restricted boltzmann machines using approximations to the likelihood
gradient. In Proceedings of the 25th International Conference on Machine Learning, pp. 1064-
1071. ACM, 2008. 2
Tijmen Tieleman and Geoffrey Hinton. Using fast weights to improve persistent contrastive diver-
gence. In Proceedings of the 26th International Conference on Machine Learning, pp. 1033-1040.
ACM, 2009. 2
Max Welling, Michal Rosen-Zvi, and Geoffrey E Hinton. Exponential family harmoniums with an
application to information retrieval. In Advances in Neural Information Processing Systems 17,
pp. 1481-1488, 2005. 1
Jianwen Xie, Yang Lu, Song-Chun Zhu, and Yingnian Wu. A theory of generative convnet. In
Proceedings of the 33rd International Conference on Machine Learning, pp. 2635-2644, 2016. 1
Jianwen Xie, Yang Lu, Ruiqi Gao, and Ying Nian Wu. Cooperative learning of energy-based model
and latent variable model via mcmc teaching. In Thirty-Second AAAI Conference on Artificial
Intelligence, 2018a. 1
Jianwen Xie, Zilong Zheng, Ruiqi Gao, Wenguan Wang, Song-Chun Zhu, and Ying Nian Wu.
Learning descriptor networks for 3d shape synthesis and analysis. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pp. 8629-8638, 2018b. 1
Alan L Yuille. The convergence of contrastive divergences. In Advances in Neural Information
Processing Systems 17, pp. 1593-1600, 2005. 5
11
Published as a conference paper at ICLR 2020
A Distribution Coupling Methods
A.1 The Maximal Coupling Algorithm
Let (ξ, η) be a pair of random variables defined on the same probability space, and p and q be two
distributions. (ξ, η) is called a coupling of P and q if marginally ξ 〜P and η 〜q. It is well known
(see e.g. Proposition 4.7 of Levin & Peres, 2017) that among all possible joint distributions of (ξ, η),
P(ξ = η) ≤ 1 - kP - qkTV
min{P(x),q(x)}dx,
(4)
where kP - qkTV is the total variation distance between P and q. The maximal coupling algorithm,
given in Algorithm 4, generates a coupling (ξ, η) that achieves the bound in (4). That is, it maximizes
the probability that two random variables are equal subject to their marginal distributions.
Algorithm 4 Maximal coupling of two distributions p(∙) and q(∙), from Jacob et al. (2017).
Input: Density functions p(∙) and q(∙)
Output: A coupling (ξ, η) with ξ 〜p(∙) and η 〜q(∙)
1:	Sample ξ 〜p(∙) and U 〜 Uniform(0,1) independently
2:	if U ≤ q(ξ)∕p(ξ) then
3:	return (ξ, ξ)
4:	else
5:	Sample η 〜q(∙) and U0 〜 Uniform(0,1) independently until U0 > p(η)∕q(η)
6:	return (ξ, η )
7:	end if
A.2 Coupling Method for RBM
In the general maximal coupling method (Algorithm 4), letp(∙) = T(∙∣ξt) and q(∙) = T(∙∣ηt-ι) be
the transition densities of RBM, and then it generates the new states ξt+1 and ηt with the probability
P(ξt+ι = ηt ∣ξt, ηt-ι) maximized. If the event {ξt+ι = ηt} does not happen, then ξt+ι and η are
sampled independently.
However, if ξt+1 and ηt are close to each other, then in the next iteration the probability P (ξt+2 =
ηt+1∣ξt+1,ηt) would be large, which helps to shorten the stopping time. Therefore, we are motivated
to minimize some type of distance between ξt+1 and ηt. For RBM, we characterize it by E(kvt+1 -
vt0 k2|vt+1 6= vt0) and E(kht+1 - h0tk2|vt+1, v0t). Since all these variables are binary vectors, the
norm of difference is basically the number of unequal components, and the problem reduces to the
maximal coupling of Bernoulli variables.
First consider E(kht+1 - h0t k2|vt+1, v0t). Given the v variables, ht+1 and h0t follow Bernoulli
distributions elementwisely, with individual mean vectors denoted as μι and μ2, respectively. It
can be shown that the maximal coupling of two Bernoulli variables is achieved by using the same
random variate, so E(∣∣ht+ι - htk2∣vt+1, Vt) is minimized by setting ht+1 = 1{Z ≤ μι} and
h0t = 1{Z ≤ μ2}, where Z 〜Uniform([0,1]n). This leads to lines 15-16 of Algorithm 3.
Next, conditional on {vt+1 6= vt0 }, the maximal coupling algorithm generates vt+1 and vt0 with
marginal distributions proportional toP1(v) - min{P1(v), P2(v)} and P2(v) - min{P1(v), P2(v)},
respectively, where pι(∙) = TV(∙∖ht) and p2(∙) = TV(∙∣ht-ι). Algorithm 3 implements this by
two rejection sampling steps (line 8 and line 11). If we use the common random variates Z2 〜
Uniform([0, 1]n) for the proposals, then vt+1 and vt0 are maximally coupled in the event that they
are accepted in the same iteration. This property helps to reduce E(kvt+1 - vt0 k2 ∖vt+1 6= v0t).
To demonstrate that the specialized Algorithm 3 improves the general Algorithm 1, we sample
coupled Markov chains from an RBM model with m = 500, n = 100, and the elements of (W, b, c)
are all generated from aN(0, 0.12) distribution. For both algorithms, the elements of the initial state
v0 are sampled from independent Bernoulli(0.5) distributions, and we generate coupled Markov
chains until the stopping time τ is reached or the chain length exceeds 1000. Figure 5 shows the
distributions ofτ for both algorithms based on 1000 replications.
12
Published as a conference paper at ICLR 2020
0
6
250
Oooo
0 5 0 5
2 11
A0u8nbωlt
250	500	750 1000 6	10	20	30	40	50
Stopping Time τ
Figure 5: Distribution of the stopping time τ for the general (Algorithm 1) and specialized (Algo-
rithm 3) coupling methods.
It is very clear that the general algorithm leads to a surprisingly long stopping time, and even has
a long tail beyond 1000. In contrast, 65.4% of the stopping times in the specialized algorithm are
smaller than or equal to 10. In this sense the improvement brought about by Algorithm 3 is huge.
B Additional Results for Numerical Experiments
All experiments in this article were run on an Intelr Xeonr Gold 6126 processor with 12 cores and
24 threads. CD and PCD algorithms used the OpenBLAS library3 for parallel matrix computations,
and UCD used OpenMP4 to generate Markov chains in parallel.
B.1 BAS Data
For the BAS data, we gradually increase the value of k in the CD-k algorithm, and plot their training
trajectories for the log-likelihood values (Figure 6). It can be seen that the smallest k to make the
result comparable to UCD is about 20. However, the stopping time for UCD has an average of 2.40
across all the iterations, which means that it is more efficient than a fixed-k CD algorithm with a
similar performance. The running time for each training algorithm, which is given in Table 1, also
supports this claim.
Ooooo
5 0 5 0 5
-1-2-2-3-3
① nra>uorpunLL.PoolI=3'=do0—1
Method
CD-I
----CD-2
----CD-5
----CD-IO
----CD-20
25∞	50∞	7500	100∞
Iteration
■o
Figure 6: CD-k algorithms for training the BAS data with different k.
B.2 Simulated RBM Data
For the experiment in Section 6.2, Figure 7 shows the training processes of CD-k algorithms with
larger k . Figure 8 gives the average stopping time for UCD in each iteration, and the number of
3https://www.openblas.net/
4https://www.openmp.org/
13
Published as a conference paper at ICLR 2020
Table 1: Running time (in seconds) for each training algorithm on the BAS experiment.
CD-1 w/ log-likelihood values	CD-1	CD-2	CD-5	CD-10	CD-20	PCD	UCD
65.01	5.38	8.44	17.89	33.17	63.51	5.18	15.99
discarded samples in the rejection sampling step. Table 2 illustrates the computational time for each
algorithm. We can find that the computational cost of UCD is slightly larger than but very close to
CD-1 and PCD.
Figure 7: Approximate log-likelihood values for each algorithm on the simulated RBM data set.
1.44
0	500	1000	1500	2000
Iteration
Iteration
Figure 8: Left: average stopping time τ for UCD in training the model on simulated RBM data.
Right: average number of rejected samples in the coupling algorithm for UCD.
10∞	15∞	20∞
Table 2: Running time (in seconds) for each training algorithm on the experiment in Section 6.2.
CD-1 w/ log-likelihood values	CD-1	CD-2	CD-5	CD-10	PCD	UCD
216.46	16.52	24.44	48.54	89.09	16.18	28.47
One useful technique to combine the computational efficiency of CD/PCD and the convergence of
UCD is to use CD/PCD in the early stage of the optimization, and then take the resulting parameter
values as initial starts for the UCD algorithm. Figure 9 demonstrates this idea: in the first 500
iterations the model is trained using PCD, and then the parameter values are fine-tuned by UCD
with a small number of iterations and a reduced learning rate.
B.3 FASHION-MNIST DATA
For the Fashion-MNIST data, Figure 10 gives the training trajectories of CD-10, CD-20, and CD-30
algorithms, but unfortunately none of them have a convergent pattern. Figure 11 shows the average
stopping time and the number of discarded samples in the UCD algorithm. Again, the cost for
rejection sampling can be ignored, and the value of τ gets stable around 30 after 2000 iterations.
14
Published as a conference paper at ICLR 2020
D D
e C C
8 Pu
a
:
0
200	400	600
Iteration
Figure 9: Combine PCD and UCD for efficient and convergent training.
Finally in Table 3 We demonstrate the running time for each training algorithm. Although UCD
takes more time than other methods, the additional cost seems to be the necessary price to achieve a
convergent result, as all other methods compared have a divergent log-likelihood.
φ-e> Pooil--φw'M0~l
Figure 10: Approximate log-likelihood values for CD algorithms on the Fashion-MNIST data set.
5040302010
0)E 匚 MUQ,doaS ½2φ><
u5qU ∙lod ss-dujes p-ɔ?00;⅜
1O∞	2O∞
Iteration
Figure 11: Left: average stopping time T for UCD in training the model on Fashion-MNIST data.
Right: average number of rejected samples in the coupling algorithm for UCD.
C Variance of Gradient Estimates
To compare the variances of stochastic gradients generated by different algorithms, in each opti-
mization iteration We estimate the folloWing gradient variance defined by
V(f(θ))= e[∣∕(Θ)- E(f(θ))[
15
Published as a conference paper at ICLR 2020
Table 3: Running time (in minutes) for each training algorithm on the FaShion-MNIST experiment.
CD-1 w/ log-likelihood values	CD-1	CD-10	CD-20	CD-30	PCD	UCD
65.14	25.63	114.85	212.22	311.89	25.53	494.01
where f (θ) is an estimator for the second term of (2), and IlTlF is the FrobeniUS norm. Among the
training methods considered in this article, f(θ) is biased in CD and PCD, and is unbiased in UCD.
We consider the example in Section 6.2, and estimate V (f(θ)) using the 1000 parallel Markov
chains in each iteration. Figure 12 shows the estimated gradient variances for b, c, and W along the
training process.
8cπcπ> ^cv-pα,leuji3sw
50-
40-
30-
20-
10-
30-
20-
10-
0-
4000-
30∞-
20∞-
10∞-
δ iobo	2∞o δ	ι∞o 2∞oδ ιd∞ 2δb0δ ιδbo 20∞δ	1000 2δb0δ 1∞0	2∞0
Iteration
Figure 12: Estimated gradient variance in each optimization iteration for different methods.
It can be observed from Figure 12 that the gradient variance of UCD is generally comparable to CD
and PCD, with only a few extreme cases. This finding suggests that the eliminated bias does not
significantly increase the variance, which is important for the convergence speed of SG.
D Proof of Theorems
D.1 Theorem 1
Omitting the iteration index i for brevity, we first prove that the stopping time τ has a finite expecta-
tion. Let Tθ(x∣x0) := Tθ(v, h∣v0, h0) = p(v∣h0; θ)p(h∣v; θ) denote the transition density for a full
update cycle. Under Assumptions 1 and 2, Lemma 2 of Johnson & Burbank (2015) shows that there
exists a constant γ ∈ [γ1γ2, 1) such that
Ex~Tθ(x|x0)l(h) ≤ Yl(h0) + Y2L1 + L2
(5)
for all x0 ∈ X. Therefore, the drift condition holds for the transition density Tθ(x∣x0).
Next, since Algorithm 1 is a maximal coupling algorithm, we have (see for example Jacob et al.,
2017)
P (ξt+ι = η∕ξt = x, ηt-1
x0)
/ min{Tθ (y|x), Tθ (y∣x0)}dy.
By Assumption 2, p(v|h; θ) ≥ εq(v) for all h ∈ D, v ∈ V, and θ ∈ Θ, so
P (ξt+1
ηtlξt
x, ηt-1 = x0) ≥
/ εq(v)p(h|v; θ)dy
f	_	r r 1	∕~ ι ∖ ʌ τ ,	. 1	,	∕~∖	/7 I ~ λ∖ ♦	♦ ♦ ,	1	∙ . r∙ . ∙
on x, x0	∈	D X V,	where	y = (v, h). Note	that	q(v)p(h∣v; θ) is a joint density function,	so
J q(v)p(h|v； θ)dy = 1, and hence P(ξt+ι = ηt∣ξt = x, ηt-ι = x0) ≥ ε for all h ∈ D and θ ∈ Θ.
16
Published as a conference paper at ICLR 2020
Then by Proposition 3.4 of Jacob et al. (2017), there exist constants κ1 > 0 and ρ1 ∈ (0, 1) such
that for all t > 0, P(τ > t) ≤ κ1l(h0)ρt1. As a result, we obtain
E(T) = XX P(τ>t) ≤ K1l(h0)
t=0	1 - ρ1
(6)
The right hand side of (6) does not depend on the value of θ, so E(τ) is uniform in the iteration
index i.
To make sure that g(θ) is well defined, a few regularity conditions, formulated as Assumption 2.1
of Jacob et al. (2017), need to be verified. Under the drift condition (5) and Assumption 2, Theorem
12 of Rosenthal (1995) shows that {ξt} is a geometrically ergodic Markov chain. Therefore, there
exist constants κ2 > 0 and ρ2 ∈ (0, 1) such that
∣Eφ(ξt) - EmΦ∣≤ κ2l(h0)ρ2	⑺
for all φ : ∣φ(v, h)| ≤ l(h). Therefore, there exists a constant M > 0 such that ∣Eφ(ξt)∣ ≤
EMl + κ2l(h0)ρt2 ≤ D + κ2l(h0) ≤ M < ∞.
Since |f(x; θ)∣2+c ≤ l(h) by Assumption 1, we get E{∣f (ξt)∣2+c} ≤ M for all t > 0. Moreover,
(7) implies that E{f(ξt)} → EMf. These two results verify Assumption 2.1 of Jacob et al. (2017),
and hence the unbiasedness of g(θ) is true by design.
Finally, we need to show that the second moment of the stochastic gradient is bounded uniformly in
θ. Let ∆k = f(ξk) and ∆t = f(ξt)- f (ηt-ι) for t ≥ k + 1, and then g2(θ) = Pt∞=k ∆t. From
the ergodicity property (7) we immediately get E(∆2k) ≤ M. For t ≥ k + 1,
E(∣∆t∣2+c)= E(|f(ξt) - f(ηt-1)∣2+c) ≤ 21+cE(∣f(ξt)∣2+c + If(ηt-ι)∣2+c) ≤ 22+cM.
Then by Holder,s inequality,
E(∆2) = E(∆2l{τ > t}) ≤ {E(∣∆t∣2+c)}2∕(c+2) {P(τ > t)}c/(C+2)
≤ 4M2/(C+2) {κ1l(h0)p1}c/(c+2).
Since g2(θ) = P∞=k ∆t, we get
{/ ∞	∖ 2 I ∞	∞
X ∆t	≤ ≤ X X E∣∆t∆s1{τ>t}1{τ>s})∣.
t=k+1		t=k+1 s=k+1
We also have
E ∣∆t∆s1{τ > t}1{τ >s})∣ ≤ ,E(∆2l{τ>t})E(∆2l{τ>s}) ≤ Coρ3ρ3
for some constants C0 > 0 and ρ3 ∈ (0, 1), so finally,
∞∞
E [{g2(θ)}2] ≤ 2E(∆k) + 2E [{g2(θ) - ∆k}2] ≤ 2M + 2 X X Cop3p3 < ∞,
t=k+1 s=k+1
and the bound does not depend on θ.
D.2 Corollary 2
Since E(v, h; θ) is continuous in θ, every conditional distribution of p(v, h; θ) is also continuous
in θ. This implies that Tθ(x∣x0) is continuous in θ as well. We have assumed that Tθ is irreducible
and aperiodic, so for each θ, Tθ(x∣x0) > 0 for every x, x0 ∈ X. By the compactness of Θ, there
exist a constant ε > 0 such that Tθ(x∣x0) ≥ ε for all x, x0 ∈ X and θ ∈ Θ.
Similarly, f (x; θ) = ∂E(v, h; θ)∕∂θ is continuous in θ, so for any c > 0, there is a constant
M > 1 such that ∣f (x; θ)∣2+c ≤ M for all X ∈ X and θ ∈ Θ. Then by choosing constant functions
l(h) = M, r(v) = M and constants γ1 = γ2 = 1/2 and L1 = L2 = M/2, we make Assumption 1
hold.
For Assumption 2, the density function q(∙) can be chosen as a uniform distribution over the finite
space X. Then the proof is complete.
17