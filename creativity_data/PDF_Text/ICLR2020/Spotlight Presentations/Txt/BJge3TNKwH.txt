Published as a conference paper at ICLR 2020
Sliced CRAMER SYNAPTIC Consolidation for
Preserving Deeply Learned Representations
Soheil Kolouri, Nicholas A. Ketz, & Praveen K. Pilly
HRL Laboratories, LLC
Malibu, CA, 91301, USA
{skolouri, naketz, pkpilly}@hrl.com
Andrea Soltoggio
School of Computer Science,
Loughborough University,
Leicestershire, UK
a.soltoggio@lboro.ac.uk
Ab stract
Deep neural networks suffer from the inability to preserve the learned data repre-
sentation (i.e., catastrophic forgetting) in domains where the input data distribu-
tion is non-stationary, and it changes during training. Various selective synaptic
plasticity approaches have been recently proposed to preserve network param-
eters, which are crucial for previously learned tasks while learning new tasks.
We explore such selective synaptic plasticity approaches through a unifying lens
of memory replay and show the close relationship between methods like Elastic
Weight Consolidation (EWC) and Memory-Aware-Synapses (MAS). We then pro-
pose a fundamentally different class of preservation methods that aim at preserv-
ing the distribution of the network’s output at an arbitrary layer for previous tasks
while learning a new one. We propose the sliced Cramer distance as a suitable
choice for such preservation and evaluate our Sliced Cramer Preservation (SCP)
algorithm through extensive empirical investigations on various network architec-
tures in both supervised and unsupervised learning settings. We show that SCP
consistently utilizes the learning capacity of the network better than online-EWC
and MAS methods on various incremental learning tasks.
1	Introduction
incremental learning without catastrophic forgetting is one of the core characteristics of a lifelong
learning machine (L2M) and has recently gained renewed attention from the machine learning com-
munity. in real-world applications, the input distribution of the data (e.g., sensory inputs) is prone
to constant changes due to environmental variations (e.g., seasonal changes), exposure to new situ-
ations (e.g., change in the surface friction), sensory malfunction (e.g., water droplets on a camera),
among others. it is therefore desirable to continue to train the base computational model only on the
new data/task and incrementally accumulate knowledge to improve the performance of the system
over time, as opposed to retraining the model on the composition of old and new data.
The existing computational models, for instance deep convolutional neural networks (CNNs), face
two fundamental issues regarding incremental learning, 1) catastrophic forgetting (McCloskey &
Cohen, 1989), which refers to the forgetting of previously acquired knowledge when learning new
tasks as a result of interference between the old and new tasks, and 2) intransigence, which refers
to the inability to acquire new knowledge while trying to preserve old knowledge (e.g., reducing
the learning rate) Chaudhry et al. (2018). Note that we use the term ‘knowledge’ here to indicate
the input/output behavior of the computational model as in (Hinton et al., 2015). A successful
incremental learner should be able to overcome both forgetting and intransigence. The commonly
used strategies to overcome catastrophic forgetting include:
1
Published as a conference paper at ICLR 2020
1.	selective synaptic plasticity to preserve learned knowledge (Kirkpatrick et al., 2017; Zenke
et al., 2017; Aljundi et al., 2018; Chaudhry et al., 2018), which is rooted in the idea of
homeostatic plasticity in neuroscience,
2.	additional neural resource allocation to learn new knowledge and preserve old knowledge,
(Rusu et al., 2016; Lee et al., 2017; Li & Hoiem, 2017; Rannen et al., 2017; Schwarz et al.,
2018; Li et al., 2019), which is similar to neurogenesis in the hippocampus,
3.	memory and experience replay (Rebuffi et al., 2017; Shin et al., 2017; Wu et al., 2018;
Hu et al., 2019; Rostami et al., 2019), which is based on the well-established theory of
complementary learning system (CLS) (McClelland et al., 1995).
Each framework has its advantages and disadvantages, and Parisi et al. (2019) provide an excellent
survey of these methods. We note that the term “synaptic weights” refers to the strength of a con-
nection between two nodes. The term ‘plasticity’ is used analogously to ‘neural plasticity’ in the
human brain, which refers to the ability of the neurons to change their synaptic weights. The term
“selective plasticity” refers to the desired capability of a network to selectively increase or decrease
the plasticity of individual synapses throughout the neural architecture.
Our focus in this paper is on selective synaptic plasticity. The standard deep neural network ar-
chitectures are uniformly plastic; hence, all neurons are prone to changes during training, and this
powerful capability is also the demise of these networks and leads to catastrophic forgetting. The
idea of selective synaptic plasticity is to partially preserve synapses that are critical for previously
learned tasks by rigidifying those synapses (i.e., to enforce critical synapses to change less). Rigid-
ifying the network over time leads to a loss of learning capability for future tasks, which is known
as ‘intransigence’ in the literature. Selective synaptic plasticity, by itself, could not fully overcome
intransigence. A combination of strategies like efficient memory replay for reconsolidation, neuro-
genesis, and selective synaptic plasticity could lead to superior methods that defeat both catastrophic
forgetting and intransigence. Chaudhry et al. (2018), for instance, provide such a combination of
memory replay and selective synaptic plasticity. Also, there have been various efforts toward mak-
ing the idea of neural resource allocation scalable, the progress and compress work Schwarz et al.
(2018), and the incremental moment matching Lee et al. (2017) work fall under this category.
In this paper, we focus on selective synaptic plasticity to preserve learned representations in a deep
neural network. Inspired by (Chaudhry et al., 2018), we take a geometric view and devise a new
method for selective synaptic plasticity. The proposed method is fundamentally different from the
previous approaches like Elastic Weight Consolidation (EWC) (Kirkpatrick et al., 2017) and Mem-
ory Aware Synapses (MAS) (Aljundi et al., 2018), which we indicate as sample-based approaches.
Instead, we focus on identifying synaptic importance parameters that preserve the ‘distribution’ of
the latent representation of a task. Focusing on preserving the distribution of the latent represen-
tation of a neural network at an arbitrary layer, as opposed to the expected change in network’s
response for individual samples, enforces a less restrict regularization on the network, and enables
a better utilization of the network learning capacity. The primary concept of sample-based versus
distribution-based regularizations are visualized in Figure 1. Similar to the MAS framework, our
proposed method denoted as Sliced Cramer Preservation (SCP) is also able to preserve a task rep-
resentation in any layer of a neural network, hence, enabling its application to various unsupervised
or self-supervised learning settings.
Our specific contributions in this paper are:
•	Introducing a distribution-based regularization using the sliced Cramer distance (aka,
Cramer Wold distance (Tabor et al., 2018)) for selective synaptic plasticity that preserves
the distributions of the representations of previously seen tasks at an arbitrary layer of a
deep neural network while learning a new task.
•	Providing a geometric interpretation for the MAS algorithm (Aljundi et al., 2018) that
further gives insight into the otherwise heuristic choices made in the approach.
•	Comparing the proposed method to online-EWC and MAS on the benchmark permuted
MNIST dataset, sequential unsupervised learning with auto-encoders, and the more inter-
esting problem of semantic segmentation of driving scenes, and demonstrating significant
improvements in overcoming catastrophic forgetting and intransigence over these methods.
2
Published as a conference paper at ICLR 2020
Figure 1: Sample-based approaches regularize the learning by the expected change, as measured
by a dissimilarity measure, of the response of the network for individual samples from Task A,
after learning Task A and during learning Task B, Ex〜PA [d(φ(x; θ), φ(x; θA))], where d(∙, ∙) is
a dissimilarity measure between two K-dimensional vectors. Therefore, the regularization is an
empirical expected change of the response for samples. EWC and MAS fall under the sample-based
category. The proposed distribution-based approach, on the other hand, regularizes the change in
the overall distribution of the network’s output for input samples from Task A, after learning Task
A and during learning Task B, dp (PA(∙∣θ),ρA(∙∣ΘA)), where PA(∙∣θ) is defined in equation 8 and
dp(∙, ∙) is a distance measure between two probability distributions defined on Z ⊆ RK.
2	Problem Set-up and Preliminaries
Consider data from a stream of tasks Xt = {χt 〜PX}n= 1, where PX is the probability density
function (PDF) for task t defined on X ⊂ Rd . We consider both supervised and unsupervised tasks,
where in the supervised case the input sample, xit , is accompanied with the corresponding label
yt ∈ RK. Let φ(∙; θ) : Rd → RK denote a parametric function (e.g., a neural network) that is to
be optimized to solve the stream of tasks. In the supervised learning setting, we consider φ to be the
mapping to the logits prior to applying the softmax layer.
2.1	A Geometric View of Elastic Weight Consolidation
In their seminal work, Kirkpatrick et al. (2017) considered the problem of overcoming catastrophic
forgetting in supervised and also reinforcement learning scenarios where a supervisory signal y
exists, whether in the form of labels/annotations, or environmental rewards, respectively. Here we
reiterate the geometric interpretation of the EWC framework following the work of (Chaudhry et al.,
2018), which we will then adapt to define our generic consolidation framework. Let pθ(y|x) =
SoftmaX(φ(x; θ)), where [pθ (y |x)j is the softmax probability of the j-th class. For simplicity,
let us consider the case where we want to learn only two tasks consecutively, i.e., tasks ‘A’ and
‘B.’ Then, EWC ensures that while learning task 'B,' the conditional likelihood pθ(y IxA) does not
drift far from the optimal conditional likelihood pθɪ (y|xA), where θA are the parameters initially
optimized for task A:
arg min LB (θ)
θ
argθminLB⑻ + λEx〜pχ [dκl(PθA(yIx) || Pθ(yIx))]
arg min
θ
LB(θ) + λEx~pχ
；/Pθa(y|x)、]]
°9 pθ(y|x) U
(1)
where λ is the regularization coefficient. Note that equation 1 is essentially an optimization with a
memory replay regularizer. Here, while learning task B, samples from task A (i.e., a memory buffer
from this task) are fed through the network and the conditional likelihood is constantly checked
against the optimal conditional likelihood for task A, pθɪ (y|xA), to ensure a minimal deviation
from those parameters.
The key question answered by the EWC framework is on how to avoid memory replay and yet
achieve a similar result, i.e., not forget the knowledge from old task (task A). The answer lies in the
second-order Taylor expansion of the regularizer around the parameters optimized for the old task,
θA. It is straightforward to show (Chaudhry et al., 2018) that the second-order Taylor expansion of
3
Published as a conference paper at ICLR 2020
the regularizer is of the form:
Ex〜PX [DKL(Pθ⑻X) || Pθ+δθ⑻X))] ≈ 1 δθTFθδθ = kδθkFθ	⑵
where Fθ is the Fisher Information Matrix (FIM) and is defined as:
Fθ = Ex 〜PX
E	l^∂∂iog(pθ(y∣χ))∖ ∂∂iog(pθ(y∣χ))∖T
Ey〜pθA [[	∂θ	) [	∂θ	)
(3)
see supplementary material for complete derivations. Therefore, when δθ → 0 the KL-divergence
regularize] enforces closeness of θ to θA in a Riemmanian pseudo-manifold induced by the FIM.
Given that the number of parameters could easily reach several million in standard deep neural
networks, it is practically infeasible to store and use the FIM matrix, Fθ . Therefore, Kirkpatrick
et al. (2017) assume that Fθ is diagonal and further approximate the KL-divergence with:
1M
Ex〜PX [DKL(Pθ(y|X) || Pθ+δθ(y|X))] ≈ 2 £ [Fθ]m,m [δθ]m	(4)
m=1
where M is the total number of parameters in the neural network. This leads to the main equation
in the EWC framework:
λM
argminLB(θ) = argminLB(θ) + 2	[FθA]mm [θ - θA]m	(5)
θ	θ	2 m=1
From our point of view, the critical aspect of these derivations is the connection between memory
replay and structural plasticity. Note that we started with equation 1, which uses the idea of a
memory replay regularizer. Then by assuming δθ → 0, using the second-order Taylor expansion of
the KL-divergence around θA, and assuming that FIM is a diagonal matrix We arrived at equation 5,
which provides the idea of synaptic importance parameters and is an embodiment of the selective
synaptic plasticity frameWork. Next, We use this critical aspect and develop an analogous regularizer
(i.e., based on memory replay) for the MAS algorithm.
2.2	Generalizing to Unsupervised Learning
The EWC frameWork as explained in the previous section preserves the softmax probability of sam-
ples from Task A While learning Task B. This limits the applicability of the method to netWorks
With outputs living on a K-dimensional simplex, e.g., supervised learning and reinforcement learn-
ing Where the netWork outputs a probability over a finite set of actions. More recently, Aljundi et al.
(2018) presented their Memory-AWare-Synapses (MAS) frameWork, Which lifts the requirement for
EWC outputs to live on a simplex, and enables calculation of the synaptic importance parameters
even in unsupervised learning and also during testing. While the method is exciting and practically
very useful, there is no geometric motivation behind the algorithm. Here We reverse engineer the
importance term used in MAS and shoW a simple regularizer that leads to the MAS algorithm and
more importantly provides a geometric interpretation for the algorithm.
Let the regularizer for Task B, be the expected absolute difference betWeen squared `2 norms of the
output of the netWork for samples from Task A, i.e.:
argθminLS argθminLB(θ) + λEx~pX Βkφ(X; θ)k2 -kφ(X;θA)k2)2]⑹
It is straightforWard (see Supplementary material) to shoW that using the second-order Taylor ex-
pansion of the above regularizer leads to:
arg min LB (θ)
θ
M
arg min LB (θ) + λ X [Ω]m,m [θ - θA]；
θ	m=1
(7)
where [Ω]m,n = Ex〜PA [(dk∂[xfk )(dkφ(θ)],which is the importance parameter used by
Aljundi et al. (2018). From a geometric perspective, MAS preserves the norms of the samples from
4
Published as a conference paper at ICLR 2020
Task A while learning Task B. In other words; MAS enforces closeness of θ to θA in a Riemmanian
pseudo-manifold induced by matrix Ω.
The general idea of using the expected value of a “suitable” distance/divergence between samples,
Ex〜PAd(Φ(x; θ),Φ(x; θA)) and leveraging its second-order Taylor expansion to obtain synaptic
importance values is crucial here and could lead to various undiscovered algorithms based on new
distances/divergences. We refer to these approaches as sample-based regularization methods.
2.3	Preserving Distribution of an Arbitrary Layer
We approach the problem of overcoming catastrophic forgetting from the angle of preserving
Task A’s distribution at an arbitrary layer of the neural network, when learning Task B. Let
ziA = φ(xiA ; θ) ∈ RK be the output of the network, for a sample from Task A, at the target layer
(e.g., output logits in a NN classifier, or reconstructed image of an autoencoder). The distribution
of the random variable zA in the target layer follows from the Random Variable Transform (RVT)
theorem (Gillespie, 1983):
pA(z∣θ) = I
X
pAX(x)δ(z-
φ(x; θ))dx
(8)
Then we propose the following general regularization to overcome forgetting when learning task B:
arg min LB (θ) = arg min LB (θ) + λd(pA(∙∣θ),pA(∙∣θA))	(9)
θθ
where d(∙, ∙) is a discrepancy measure between the two probability distributions defined in RK. Note
that equation 9 could be performed with any discrepancy measure or distance, e.g., the Wasserstein
distance (Villani, 2008; Kolouri et al., 2017), using a memory replay strategy. In what follows, we
describe a ‘suitable, distance d(∙, ∙) that: 1) respects the underlying geometry of the space, and 2)
enables a similar strategy to that of the EWC framework to provide importance parameters.
3	SLICED-CRAMER DISTANCE FOR STRUCTURAL PLASTICITY
3.1	CRAMER DISTANCE
The P-Cramer distance (Cramer, 1928; Szekely & Rizzo, 2013) between two one-dimensional
probability density functions p0 and p1 is defined as the `p -norm between their cumulative dis-
tribution functions. Note that here we avoid any measure theoretic notations for simplicity. Let
qi(t) = R-∞pi(τ)dτ denote the cumulative distribution function for Pi, then the P-Cramer distance
is defined as:
1
Cp(po,pι) = (/ ∣qo(t) - qι(t)lpdt)	(10)
for p ≥ 1. Similar to the Wasserstein distance and unlike the KL-divergence and its symmetric
form Jensen-Shannon distance (i.e., the square root of the Jensen-Shannon divergence), the Cramer
distance respects the underlying geometry of the space. Moreover, the Cramer distance provides
unbiased sample gradients (Bellemare et al., 2017), and for p = 1 , C1 is equivalent to the 1-
Wasserstein distance, Wι. In addition, and similar to the Wasserstein metric, the dual of the Cramer
distance is of the form of an integral probability metric (IPM) (Dedecker & Merlevede, 2007).
To further demonstrate the favorable characteristics of this distance, consider the following para-
metric distribution matching in one-dimension, where the target distribution, p, is a box distribution
defined onR and pτ (t) = p(t-τ) is the shifted version ofp and the goal is to optimize τ to minimize
the distance between p and P (τ* = 0). For this simple setting, we calculate the energy landscape
(i.e., the distance between pτ and p) as a function of τ for the Jensen-Shannon distance, Wp, and
Cp for P = 1, 2. Figure 2 shows the distributions P and Pτ on the left and the energy landscape as a
function of T on the right. It can be clearly seen that the Wasserstein and Cramer distances respect
the underlying geometry of the problem, while the Jensen-Shannon (JS) distance fails to do so.
To extend the Cramer distance to higher-dimensional distributions, we utilize the idea of distribution
slicing used in various recent publications (Kolouri et al., 2016; 2019; 2018). We note that the sliced-
Cramer distance (also known as the Cramer-Wold distance) was recently used in Tabor et al. (2018)
for generative modeling. We briefly describe the idea in the following section.
5
Published as a conference paper at ICLR 2020
Figure 2: The energy landscape of various distances as a function of the translation parameter. It can
be seen that both Wasserstein and Cramer distances respect the underlying geometry of the problem
while the Jensen-Shannon distance fails to do so.
3.2	SLICED-CRAMER DISTANCE
The idea of slicing a higher-dimensional distribution has roots in the Radon transform that is com-
monly used in computational tomography. The idea is to represent a high-dimensional distribution
via the infinite set of its marginal distributions. In short, let p0 and p1 be d-dimensional probability
density functions defined on X ⊂ Rd, then their Radon transform is defined as:
Rpi
(t,ξ)=
X
pi(x)δ(t-
X ∙ ξ)dx
(11)
for ∀t ∈ R and ∀ξ ∈ Sd-1 where Sd-1 denotes the d-dimensional unit sphere. Note that Rpi(∙, ξ)
is a so called slice of Pi, which is a one-dimensional marginal distribution of pi. Let Rqi (∙, ξ) be the
corresponding cumulative distribution function of Rpi(∙, ξ):
Rqi(t, ξ) = -t∞Rpi(τ,ξ)dτ
(12)
then the SIiCed-Cramer distance between po and pi is the expected value of the Cramer distance
between their one-dimensional slices, i.e., Rpi(∙, ξ) when ξ 〜Usd-ι for Ud-I being the uniform
distribution on the d-dimensional unit sphere. In other words, the sliced Cramer distance is defined
as:
SCp(p0,p1)
1
ILd ICp(Rp0(∙,ξ),Rpι(∙,ξ))dξ)p
(Zmd J巡 ∣Rqo(t, ξ) -Rqi(t, ξ)∣pdtdξ)p
(13)
Now we are ready to propose our method for overcoming representation forgetting.
3.3	Overcoming representation forgetting
The critical point here is that the sample-based regularizers could over-estimate the importance
of synapses, leading to intransigence faster. More importantly, a substantial expected change in
the network’s output (over individual samples) would not necessarily mean catastrophic forgetting.
Changes in the network’s output within a mode (e.g., in supervised classification within the distribu-
tion of a particular class) are harmless, so long as the representation of the data is not significantly
changing. Our proposed regularizer tolerates such changes and therefore has the potential for better
utilization of the network’s learning capacity. We propose the following regularization for overcom-
ing catastrophic forgetting:
arg min LB(θ) = arg min LB(θ) + λSC2(pA(∙∣ΘA),pA(∙∣θ))	(14)
θθ
equation 14 requires memory replay from the old task(s) while learning the new one. To transition
from memory replay to selective synaptic plasticity, we derive the second-order Taylor expansion
of the regularizer SC2 around the optimal parameters for the previous tasks, θ%. Assuming that
θ = θA + δθ where δθ → 0, it is straightforward to show that:
SC2(pA(∙∣ΘA),pA(∙∣Θ)) ≈ (δθ)TΓθA(δθ) = kδθkrθ1	(15)
6
Published as a conference paper at ICLR 2020
where 展为 is defined as:
Γθa ：= /	/(
Sd-1	R
d RqA(t, ξ∣θA) ʌ dd RqA(t, ξ∣θA)
dθ
dθ
T
dtdξ
(16)
See supplementary materials for the detailed derivations. Note that similar to Fθ却 Γθɪ is also
positive-semi-definite (PSD) and therefore our SIiced-Cramer regularizer enforces closeness of θ
and θ* in a Riemannian pseudo-manifold induced by the PSD matrix Γθ.
While the definition of Γ in equation 16, regardless of its similarity to equation 3, may seem in-
timidating, it leads to a straightforward empirical algorithm, which we discuss in the next section.
Before that, we point out that similar to the FIM, calculating Γ is also practically infeasible, and we
approximate Γ with a diagonal matrix that simplifies the regularization into:
arg min LB(θ)
θ
M
argminLB(θ) + λ X [Γ]m,m [δθ]2m
θ	m=1
(17)
Finally, we emphasize that while equation 17 is similar to equation 5, it enforces a very different
constraint on the neural network. equation 5 enforces conditional class likelihoods to be preserved,
which is sample-based; however, our proposed formulation in equation 17 preserves the distribu-
tion of network’s outputs at a particular layer for old tasks, i.e., it preserves the distribution of the
previously learned representations.
4 Proposed Algorithm
Here we derive the algorithmic steps required to calculate Γ empirically, as shown in Algorithm 1.
The empirical distribution at the network,s output can be written as, pA(z∣θ) ≈ 得 PN=I δ(z -
φ(xnA; θ)), and the slices of this empirical distribution are defined as:
1N
RpA(t, ξlθ) ≈ N £ δ(t - ξ ∙ φ(xA； θ))
n=1
Let u(∙) denote the step function, which is the cumulative distribution of the Dirac delta function.
Then the cumulative distribution of RpZ(t, ξ∣θ) can be written as, RqZ(t, ξ∣θ) ≈ 得 PN=I u(t -
ξ φ(xA; θ)). Therefore We have:
d RqZ也 ξ∣θ)
dθ
≈ N X (-⅛^) δ(τ ∙ Φ(χZ; θ))
n=1
(18)
substituting equation 18 into equation 16 and using a Monte-Carlo approximation of the integration
of Sd-1, with L samples, leads to:
(19)
where ξιs are randomly drawn from SK-1, and Z = N PN=I Φ(xA; θA). These derivations give
birth to our proposed algorithm shown in Algorithm 1. Given that calculation of matrix Γ is not
practically feasible (due to the large number of parameters ofa deep neural network), we follow the
work of (Kirkpatrick et al., 2017) and approximate Γ to be a diagonal matrix, which simplifies to:
1L 1N
[%A-L X (N X
dξι∙ Φ(xA; θA)!2 = 1 X (dξι∙ ΦA Y
dθi = = L = k dθi )
where φA = N PN=I φ(xA; θA).
7
Published as a conference paper at ICLR 2020
Algorithm 1: Sliced Cramer Preservation (SCP)
Input: Data, XA = {xA 〜PX}N=ι, and the optimized neural network, φ(∙; θΑ), for Task A.
Parameters: Number of random projections, L.
Output: Synaptic importance matrix Γ
ι Calculate the mean response of the network at the targeted layer: φA = N PN=I φ(xA; θA).
2	Initialize the synaptic importance matrix, [Γ]i,j = 0.
3	for l - 1 to L do
4	Sample ξl from SK-1
___
5	Slice the mean response: P = ξι ∙ φA
6	Calculate VθP using auto-differentiation
7	Update Γ: Γ += L(Vθρ)(Vθρ)T
Figure 3: Comparison among online-EWC, MAS, and SCP on learning ten permuted MNIST tasks
(a), and the histogram of Log(λ × ∙) of the synaptic importances (i.e., the diagonal values of F, Ω,
and Γ, for EWC, MAS, and SCP, respectively) for each method (b).
4.1	Online extension of the algorithm
To extend the framework into sequential learning of multiple tasks and to avoid memorizing task-
specific Γs (or their diagonals), we follow the EWC++ framework proposed by Chaudhry et al.
(2018), which is, in essence, identical to the online-EWC proposed by Schwarz et al. (2018). The
EWC++ (and online-EWC) methods ameliorate the need for predicting task identities and calculat-
ing task-specific FIMs and keep the memory requirement of the method constant (EWC requires
linear growth of memory as a function of number of tasks). In EWC++, given Fθ(t-1) at task (t - 1),
the accumulated FIM after learning task t is calculated as 琮，= αFθ* +(1 - α)Fi(-γ∖ where Fθ*
is the task specific FIM for task t, and α ∈ [0, 1) is a hyperparameter that indicates the importance
of preserving the most recent task over the older ones. Similarly, we use,
Γθt) = αΓθ% +(1 - α)rθt-1)	(20)
to obtain the SIiCed-Cramer regularizer for task (t + 1).
5	Numerical Experiments
5.1	Permuted MNIST
We first test our proposed algorithm on the benchmark permuted MNIST task and compare the
performance with online-EWC and our implementation of the online-MAS algorithm. For this ex-
periment, we used a single head model that learns ten tasks, where each task contains a permuted
version of the original MNIST dataset. We note that the reported results are a function of the archi-
tecture of the underlying network. Meaning that, while all three methods perform well on this task
for larger networks, the true competitiveness of the proposed method emerges for smaller networks
where “over-estimation” of the synaptic importances significantly hinders learning of the subse-
quent tasks and leads to intransigence. For this experiment, we used a fully-connected network (i.e.,
8
Published as a conference paper at ICLR 2020
Figure 4: Qualitative and quantitative comparison between MAS and SCP on sequential learning of
auto-encoders. Columns in Panel (a) show the reconstruction of data after learning consequent tasks
in a random permutation of MNIST sequence. Panel (b) shows the average '1 -reconstruction loss
for each method over all tasks and over 10 runs.
Figure 5: A qualitative comparison of the EWC, MAS, and SCP algorithms on semantic segmenta-
tion of the SYNTHIA Dataset (Ros et al., 2016). Task 1 (T1) is semantic segmentation in summer,
and Task 2 (T2) is semantic segmentation in winter. The first row shows performance after learning
T1 on input from T1. Second row, shows performance after learning T2 on input from T2. The third
row shows performance on input from T1 after learning T2. The last row, magnifies the last three
images in the third row for the ease of comparison.
a multi-layer perceptron) with the following architecture, 784 → 1024 → 512 → 256 → 10 neu-
rons, and for all optimizations we used the ADAM optimizer with learning rate, lr = 1e - 4. For
our proposed method, SCP, we used L = 100 slices. We repeated each experiment 10 times (with
different permutations), and reported the average accuracy over all tasks in Figure 3 (a). We can
see that SCP is capable of utilizing the capacity of the network better, which points to the fact that
regularizing the distribution as opposed to the samples provides a less restricted regularization for
the network and still enables the network to freely move individual samples so long as the overall
latent distribution of the data is consistent.
The regularization coefficients for each algorithm was cross-validated on the following grid, λ ∈
{1e + i | i ∈ [-3, -2, ..., 9]} and the optimal value was used to report the results in Figure 3 (a).
Moreover, in Figure 3 (b) we show the histogram of the logarithm of the product of the regulariza-
tion coefficients with importance parameters for each method (i.e., λ times the diagonal values of
Fθ, Ω, and Γ for EWC, MAS, and SCP, respectively). One can see that the optimal values of the reg-
ularization coefficients provide, more or less, scale-consistent synaptic importances for all methods
and distinguishing factor between the methods is on the difference between these distributions. An-
other interesting observation is that the distribution of the synaptic importances are more similar for
sample-based methods (i.e., EWC and MAS) compared to the proposed distribution-based method.
9
Published as a conference paper at ICLR 2020
5.2	Sequential Learning of Auto-Encoders
Next, we consider an experiment consisting of unsupervised/self-supervised sequential learning. To
that end, we learn an auto-encoder on single digits of the MNIST dataset sequentially. The model is
chosen to be a fully connected auto-encoder, with the following encoder 728 → 1024 → 1024 →
1024 → 256, a mirrored decoder 256 → 1024 → 1024 → 1024 → 784, and Rectified Linear
Unit (ReLU) activations. For the loss function, We used cross-entropy plus the '1 -norm of the
reconstruction error. Similar to the previous experiment, we used the ADAM optimizer (Kingma &
Ba, 2014) for training the netWork With lr = 1e-4. We perform 50 epochs of learning on each digit,
before sWitching to the next digit. For consolidation, We used L = 100 slices for SCP. Finally, We
permute the order of the digits and run our experiments 10 times, and compare ‘No Consolidation,’
With MAS, and SCP. Due to the unsupervised nature of the experiment, the EWC frameWork does
not apply here.
Figure 4 demonstrate the results of this experiment. Panel (a) shoWs the reconstruction of a sample
digit from one of our runs With the input digit sequence of [2, 7, 4, 0, 9, 1, 6, 5, 8, 3]. It can be seen
that both MAS and SCP can successfully retain the learned knoWledge While acquiring neW knoWl-
edge, While Without consolidation the auto-encoder suffers from catastrophic forgetting. Panel (b)
shows the average '1 -norm of the reconstruction error for all seen tasks over the 10 runs. We can see
that SCP and MAS are qualitatively on par, and SCP provides a modest yet statistically significant
improvement over MAS for this task.
5.3	Semantic Segmentation of SYNTHIA Dataset
Figure 6: Testing Dice score (Zou et al., 2004) of
online-EWC, MAS, and SCP on sequential learn-
ing for semantic segmentation of the summer im-
ages (Task 1) and winter images (Task 2) from
SYNTHIA dataset (Ros et al., 2016). The blue
and red shadings on the plots indicate the dura-
tions in which the models were trained on summer
and winter data, respectively
Lastly, we go beyond the benchmark yet less practical MNIST dataset and address catastrophic for-
getting in a more interesting/critical application of autonomous vehicles. We specifically consider
the problem of learning semantic segmentation of road scenes in a sequential manner, where the in-
put distribution of the data changes over time. Semantic segmentation is the task of assigning a class
label to every pixel of an input image. To that end, we use two sequences of the SYNTHIA dataset
(Ros et al., 2016), namely ‘SYNTHIA-SEQS-01-SUMMER’ and ‘SYNTHIA-SEQS-01-WINTER’
as Task 1 and Task 2, respectively. There are 13 classes in the dataset namely: Miscellaneous, Sky,
Building, Road, Sidewalk, Fence, Vegetation, Pole, Car, Sign, Pedestrian, Cyclist, and Lane Mark-
ing. We keep the last 100 frames of each sequence as the testing-set and train a deep convolutional
U-Net architecture (Ronneberger et al., 2015) on the tasks mentioned above. For the loss function,
we used (1 - Dice) (Zou et al., 2004), and each task was learned over 100 epochs. For the opti-
mizer, we used the ADAM optimizer (Kingma & Ba, 2014) with learning rate, lr = 1e - 4. For
consolidation, we used L = 100 slices for SCP.
We learn the tasks sequentially (summer first
and then winter), and a qualitative compari-
son of online-EWC, MAS, and SCP on a sam-
ple test frame is in Figure 5. The first row
shows the performance on T1 after learning T1
(base model), the second row shows perfor-
mance on T2 after learning on T1 and then T2
(indicator of intransigence), and the third row
shows performance on T1 after learning on T1
and then T2 (indicator of the catastrophic for-
getting). We can see that catastrophic forget-
ting happens when no synaptic consolidation is
leveraged. Moreover, compared to online-EWC
and MAS, SCP suffers less from intransigence
as fewer artifacts are present in the second row.
Lastly, SCP overcomes catastrophic-forgetting
more successfully compared to EWC and MAS
as it is apparent from the lack of artifacts in
the last row. Finally, we provide a quantita-
tive comparison between the methods in Figure
6, where we report the Dice score (Zou et al.,
2004), averaged over ten runs, for each task and each method during the sequential training. As
10
Published as a conference paper at ICLR 2020
can be seen, SCP significantly outperforms online-EWC and MAS on this task both in overcoming
catastrophic forgetting (left plot) and overcoming intransigence (right plot).
6	Conclusion
We introduced a new generic approach towards selective synaptic plasticity for preserving the dis-
tribution of the network’s output, at an arbitrary layer, for previously learned tasks. We started
from a memory-replay-based regularization that penalized the change in the distribution of the net-
work’s output and showed that a second-order Taylor expansion of such regularization would lead
to a selective synaptic plasticity approach that does not require memory of samples from previously
seen tasks. Furthermore, We proposed the Sliced-Cramer distance as a suitable metric for preserving
these distributions, which leads to a straightforward algorithm for selective plasticity. Also, using a
similar approach, We reverse-engineered the Memory AWare Synapses (MAS) frameWork and pro-
vided a geometrically meaningful regularization that leads to this algorithm. We then compared
the online-EWC, MAS, and SCP methods on a variety of learning tasks, including supervised and
unsupervised/self-supervised learning, and consistently shoWed competitive performance.
7	Acknowledgments
This material is based upon Work supported by the United States Air Force and DARPA under
Contract No. FA8750-18-C-0103. Any opinions, findings and conclusions or recommendations
expressed in this material are those of the author(s) and do not necessarily reflect the vieWs of the
United States Air Force and DARPA.
References
Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars.
Memory aWare synapses: Learning What (not) to forget. In Proceedings of the European Confer-
ence on Computer Vision (ECCV), pp. 139-l54, 2018.
Marc G Bellemare, Ivo Danihelka, Will Dabney, Shakir Mohamed, Balaji Lakshminarayanan,
Stephan Hoyer, and Remi Munos. The Cramer distance as a solution to biased Wasserstein gra-
dients. arXiv preprint arXiv:1705.10743, 2017.
Arslan Chaudhry, Puneet K Dokania, Thalaiyasingam Ajanthan, and Philip HS Torr. Riemannian
Walk for incremental learning: Understanding forgetting and intransigence. In Proceedings of the
European Conference on Computer Vision (ECCV), pp. 532-547, 2018.
Harald Cramer. On the composition of elementary errors: First paper: Mathematical deductions.
Scandinavian Actuarial Journal, 1928(1):13-74, 1928.
Jerome Dedecker and Florence Merlevede. The empirical distribution function for dependent vari-
ables: asymptotic and nonasymptotic results in Lp. ESAIM: Probability and Statistics, 11:102-
114, 2007.
Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural netWorks
With cutout. arXiv preprint arXiv:1708.04552, 2017.
Daniel T Gillespie. A theorem for physicists in the theory of random variables. American Journal
of Physics, 51(6):520-533, 1983.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knoWledge in a neural netWork. arXiv
preprint arXiv:1503.02531, 2015.
Wenpeng Hu, Zhou Lin, Bing Liu, Chongyang Tao, ZhengWei Tao, JinWen Ma, Dongyan Zhao,
and Rui Yan. Overcoming catastrophic forgetting via model adaptation. In International Confer-
ence on Learning Representations, 2019. URL https://openreview.net/forum?id=
ryGvcoA5YX.
11
Published as a conference paper at ICLR 2020
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A.
Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hass-
abis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forget-
ting in neural networks. Proceedings of the National Academy of Sciences, 114(13):3521-3526,
2017. ISSN 0027-8424. doi: 10.1073/pnas.1611835114. URL https://www.pnas.org/
content/114/13/3521.
Soheil Kolouri, Yang Zou, and Gustavo K Rohde. Sliced Wasserstein kernels for probability distri-
butions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 5258-5267, 2016.
Soheil Kolouri, Se Rim Park, Matthew Thorpe, Dejan Slepcev, and Gustavo K Rohde. Optimal
mass transport: Signal processing and machine-learning applications. IEEE signal processing
magazine, 34(4):43-59, 2017.
Soheil Kolouri, Gustavo K Rohde, and Heiko Hoffmann. Sliced Wasserstein distance for learning
gaussian mixture models. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 3427-3436, 2018.
Soheil Kolouri, Phillip E. Pope, Charles E. Martin, and Gustavo K. Rohde. Sliced Wasserstein
auto-encoders. In International Conference on Learning Representations, 2019. URL https:
//openreview.net/forum?id=H1xaJn05FQ.
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 and cifar-100 datasets. URl:
https://www. cs. toronto. edu/kriz/cifar. html, 6, 2009.
Sang-Woo Lee, Jin-Hwa Kim, Jaehyun Jun, Jung-Woo Ha, and Byoung-Tak Zhang. Overcoming
catastrophic forgetting by incremental moment matching. In Advances in neural information
processing systems, pp. 4652-4662, 2017.
Xilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, and Caiming Xiong. Learn to grow: A
continual structure learning framework for overcoming catastrophic forgetting. arXiv preprint
arXiv:1904.00310, 2019.
Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE transactions on pattern analysis
and machine intelligence, 40(12):2935-2947, 2017.
James L McClelland, Bruce L McNaughton, and Randall C O’Reilly. Why there are complementary
learning systems in the hippocampus and neocortex: insights from the successes and failures of
connectionist models of learning and memory. Psychological review, 102(3):419, 1995.
Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The
sequential learning problem. In Psychology of learning and motivation, volume 24, pp. 109-165.
Elsevier, 1989.
German I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan Wermter. Continual
lifelong learning with neural networks: A review. Neural Networks, 2019.
Amal Rannen, Rahaf Aljundi, Matthew B Blaschko, and Tinne Tuytelaars. Encoder based lifelong
learning. In Proceedings of the IEEE International Conference on Computer Vision, pp. 1320-
1328, 2017.
Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with
residual adapters. In Advances in Neural Information Processing Systems, pp. 506-516, 2017.
Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedi-
cal image segmentation. In International Conference on Medical image computing and computer-
assisted intervention, pp. 234-241. Springer, 2015.
12
Published as a conference paper at ICLR 2020
German Ros, Laura Sellart, Joanna Materzynska, David Vazquez, and Antonio M Lopez. The
synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes.
In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3234-
3243, 2016.
Mohammad Rostami, Soheil Kolouri, and Praveen K Pilly. Complementary learning for overcoming
catastrophic forgetting using experience replay. In Proceedings of the Twenty-Seventh Interna-
tional Joint Conference on Artificial Intelligence, IJCAI-18. International Joint Conferences on
Artificial Intelligence Organization, 2019.
Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray
Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint
arXiv:1606.04671, 2016.
Jonathan Schwarz, Wojciech Czarnecki, Jelena Luketina, Agnieszka Grabska-Barwinska, Yee Whye
Teh, Razvan Pascanu, and Raia Hadsell. Progress & compress: A scalable framework for contin-
ual learning. In International Conference on Machine Learning, pp. 4535-4544, 2018.
Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. Continual learning with deep generative
replay. In Advances in Neural Information Processing Systems, pp. 2990-2999, 2017.
Gabor J Szekely and Maria L Rizzo. Energy statistics: A class of statistics based on distances.
Journal of statistical planning and inference, 143(8):1249-1272, 2013.
Jacek Tabor, Szymon Knop, PrZemySIaW Spurek, Igor Podolak, Marcin Mazur, and Stanislaw Jas-
trzebski. Cramer-Wold autoencoder. arXivpreprint arXiv:1805.09235, 2018.
Cedric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media,
2008.
Chenshen Wu, Luis Herranz, Xialei Liu, Joost van de Weijer, Bogdan Raducanu, et al. Memory
replay gans: Learning to generate new categories without forgetting. In Advances In Neural
Information Processing Systems, pp. 5962-5972, 2018.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint
arXiv:1605.07146, 2016.
Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence.
In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 3987-
3995. JMLR. org, 2017.
Kelly H Zou, Simon K Warfield, Aditya Bharatha, Clare MC Tempany, Michael R Kaus, Steven J
Haker, William M Wells III, Ferenc A Jolesz, and Ron Kikinis. Statistical validation of image
segmentation quality based on a spatial overlap index1: scientific reports. Academic radiology,
11(2):178-189, 2004.
13
Published as a conference paper at ICLR 2020
8	Supplementary Materials
8.1	Taylor Expansion of the KL-Divergence
For the sake of completion, here we derive the second-order Taylor expansion of DKL,
DKL
(Pθo ||pe) = /
X
pθ0 (x)log
p Pθo (X) ∖
(pθ (X) J
dX
around θ0 where we can write θ = θ0 + δθ. The second-order Taylor expansion is:
Dkl(pθoIIpθ) ≈ DKL(PΘ0∣∣PΘ0) + δθT (dDKL(pθ0llpθ) ∣θo) + 1 δθT (d2DKLy lpθ) ∣θo) δθ
dθ	2	dθ
1.	Where for the first-order term we have:
dDKL(Pθo ||pe)
dθ
-	pθ0 (X)
X
dlog(pθ (x)) dx
/ Pθ0 (X) dPθ (X) dx
JX Pθ(x)	dθ
Therefore at θ = θ0 we have:
3叩 I%=TX 陪 dχ=dθ( IX…=Q
2.	and for the second-order term:
d2 dkl(pθo ||Pe)
dθ2
Pθ0 (x) dpθ(x)
pθ (X)	dθ
p Pθo (x)
JX Pθ(x)
d dpθ (x)、d dpθ (x)、T d
V dθ ) V dθ d X
P U d⅛) dX
JX Pθ (X) dθ2
Therefore at θ = θ0 we have:
T
d2DκL(pθ0 ∣∣pθ )
dθ2
pθ0 (X)
X
1	dpθo (x) ∖ ( 1	dpθo (x)
Pθo (x)	dθ	P ∖Pθo (x)	dθ
dX -
d2 Pθ0 (X)
JX	dθ2	X
(diog(Pθo(Xy)∖ (dlog(pθo(X))ATd
V	dθ	) V	dθ	d X
Pθ0(X)
X
E	∣^ d d log(Pθ0 (X)) ∖ d d log(Pθ0 (X)) ∖ T
Ex〜pθ0 [I	dθ	)I	dθ	)
F
which is the Fisher Information Matrix (FIM).
Finally, putting everything together we have:
Dkl(pΘo ∣∣Pθ) ≈ 2 δθT Fδθ
which concludes the derivation.
8.2	Taylor Expansion of the MAS Regularizer
Here we drive the second-order Taylor expansion of the MAS regularizer in equation 6,
MASreg
= EX 〜PX [2(kΦ(χ; θ)k2-kΦ(χ; θ0)k2)2
=2IX Px (χ)(kφ(x; θ)k2 - kφ(x; θ0)k2)2dχ
around θ0. The second-order Taylor expansion is:
MASreg ≈ δθT fdMASreg Q +1 δθT
dθ	2
d2MASreg
lθ0 δθ
14
Published as a conference paper at ICLR 2020
1.	Where for the first-order term we have:
-MSreg 1	= Z PX(x)d kφ(X;θ)k2 (kφ(x; θ)k1 2-kφ(x;θo)k2)
dθ	m X	d [θ]m
which for θ = θ0 is zero.
2.	For the second-order term we have:
^d2MASreg]	d d d kφ(x; θ)k2 d kφ(x; θ)k2
L = JXPX(X)Tkdx+
Z Px(X) d kφ(x2θ"	(kφ(x; θ)k2 -kφ(x; θ0)k2)dx
X	d θ2	m,n
where evaluated at θ = θ0 the second term on the right-hand-side vanishes.
Finally, putting everything together we have:
MAS F ≈ δθT Ex~Px] (Y ) (Y )[δθ
|-------------------------{z--------------}
Ω
which is the term reported in the paper.
8.3	TAYLOR EXPANSION OF THE SLICED-CRAMER DISTANCE
Here We derive the second-order Taylor expansion of the squared 2-Sliced-Cramer distance,
sc‰0,pθ) = L Jr I" (t，ξ) - Rqθ(t，ξ)l2dtdξ
around θ0, as reported in equation 16. The second-order Taylor expansion is:
SC22(Pθ0,Pθ)≈SC22(Pθ0,Pθ0)+δθT
d SC22(Pθ0 , Pθ)
dθ
院)+2 δθτ
d2SC22 (Pθ0 ,Pθ)
dθ2
Iθ0	δθ
1. Where for the first-order term We have:
dsC2Pθ0 ,Pθ) = 2
dθ
d	/竺绊& (Rqθ (t, ξ)-Rqθo (t, ξ))dtdξ
Sd-1 R dθ
Which for θ = θ0 is equal to zero.
2. For the second-order term We have:
d2 SC22(Pθ0 ,Pθ)
d2 Rqθ(t,ξ)
dθ2
Sd-1	R
dθ2
(Rqθ(t,ξ)-Rqθ0(t,ξ))dtdξ+
2 Sd-1 R
Which evaluated at θ = θ0 is:
dRqθ(t,ξ)	dRqθ(t,ξ)
dθ
dθ
T
dtdξ
d2 SC22 (Pθ0 ,Pθ)
dθ2
尿
2 Sd-1 R
2Γ
dRqθ0(t,ξ)	dRqθ0(t,ξ)
dθ
dθ
T
dtdξ
Finally, putting everything together We have:
SC22(Pθ0,Pθ) ≈ δθTΓδθ
15
Published as a conference paper at ICLR 2020
Task1: MNIST
3 7/0
I /trx9 夕 /«■/
Y 70>634o
7/5, 0 T ∕√∙
7λmoo∕^2^7
Task2: SVHN
Oo
95
0 5 0 5
.9.8.87
^us⊃ou<
1.00
0.95
0.90
0.85
0.80
0.75
0.75
1.00
0.95
0.90
0.85
0.80
O IO 20	30	40
Epochs
夕.3-813〃S 夕
sooa > Ol √- 85
Figure 7:	Sample images from MNIST and SVHN dataset (a), and the testing performance of the
Online-EWC, MAS, and SCP on sequential learning starting from MNIST (Task 1) and then SVHN
(Task 2) (b). The blue and red shadings on the plots indicate the durations in which the models were
trained on the MNIST and SVHN datasets, respectively.
VGGlike(
(activation): ReLU(inplace)
(features): Sequential (
(0)：
(1)：
(2)：
(3):
(4):
(5)：
(6):
(7)：
⑻：
(θ):
Conv2d(1, 64f kernel_size=(3, 3)r stride=(lr 1)r Padding=(2, 2))
BatchNormZd(64, eps=le-05, momentum=0.1, affine=True, track_running_stats=True)
ReLU(inplace)
ConV2d(64, 64r kernel-size=(3, 3)r stride=(l, 1)l padding=(2r 2))
BatchNormZd(64, eps=le-05, momentum=0.1, affine=True, track_running_stats=True)
ReLU(inplace)
MaxPoolZd(kernel_size=2 r stride=2 r padding=O, dilation=lr CeilAode=FaISe)
Conv2d(64, 128r kernel_size=(3r 3), stride=(1, ɪ)f padding=(2r 2))
BatchNormZd(128, eps=le-05 r momentum=。.1, affine=Truer track_running_statS=True)
ReLU(inplace)
(10): Conv2d(128, 128r
(11): BatchNorm2d(128r
(12): ReLU(inplace)
(13): MaxPool2d(kernel
(14): Conv2d(128, 256r
(15): BatchNormZd(256 r
(16): ReLU(inplace)
(17): ConV2d(256, 256r
(18): BatChNOnn2d(256,
(19): ReLU(inplace)
(20): MaxPool2d(kernel
kernel_size=(3r 3)r stride=(lr 1)r padding=(2r 2))
eps=le-05r InQInentUm=O. 1, affine=True, track_running_stats=True)
_SiZe=2, Stride=2, padding=。， diLation=Ir ceil_mode=False)
kernel_size=(3, 3)r stride=(lr 1)r padding=(2r 2))
eps=le-05 r moιπentuπι=0.1 r af f ine=True, track_running_stats=True)
kernel_size=(3r 3)r stride=(lr 1)r padding=(2r 2))
eps=le-05r InQInentUm=O. 1, affine=True, track_running_stats=True)
SiZe=2, stride=2f padding=。, dilation=lr ceil_mode=False)
(fc): Sequential(
(O): Linear (in_features=l2544, out_features=2048 r Lias=True)
(1): BatchNormld(2048 r eps=le-05, moιπentuπι=0.1r affine=True, track_running_stats=True)
(2): ReLU(inplace)
(3): Dropout(p=0.2)
(4): Linear(in_features=204Bt out_features=1024r bias=True)
(5): BatchNormld(1024r eps=le-05r m□meπtum=0.1r affine=True, track_running_stats=True)
(6): ReLU(inplace)
(7): Dropout(p=0.2)
)
(classify): Sequential(
(O): Linear(in_features=1024 r out_features=10 r bias=True)
Figure 8:	The model we used in the MNIST-to-SVHN experiment.
8.4	MNIST-TO-SVHN EXPERIMENT
Given the space constraint of the conference, we include our results on the MNIST-to-SVHN exper-
iment in the supplementary material. In this experiment, Task 1 is learning the MNIST digits, and
Task 2 is learning the SVHN digits. We show sample images from these two datasets in Figure 7a.
In this experiment, we used a VGG-like architecture, for which we include the details in 8.
16
Published as a conference paper at ICLR 2020
Figure 9: Sample super-classes from the CIFAR100 dataset and visualization of the data augmenta-
tion procedure we used in this experiment (a), and the average testing accuracy of the Online-EWC,
MAS, and SCP on sequential learning of the tasks (b). We note that we were, unfortunately, only
able to run the experiment once, and will update the Figure with the results from multiple runs as
soon as they are available.
We performed incremental learning with no consolidation, Online-EWC, MAS, and SCP. The im-
portance parameters were cross-validated on a coarse grid (due to the computational and time re-
strictions) of {1e + i|i ∈ [2, 3, 4, 5, 6]}, where the optimal parameters for the methods where 1e + 3
for SCP, 1e+5 for MAS, and 1e+4 for Online-EWC. We chose 20 epochs per task. Figure 7b shows
the results of the experiment. Note that the first plot shows the testing performance of the methods
on Task1 (i.e., MNIST dataset), where the blue shade indicates the first 20 epochs (training on the
MNIST dataset), and the red shade indicates the second 20 epochs (training on the SVHN dataset).
As can be seen, all methods can address catastrophic forgetting, while Online-EWC and SCP out-
perform MAS (However, a one should perform a finer grid search on the importance-parameters for
a definitive evaluation). The middle plot shows the testing performance of the model on the SVHN
dataset (Task 2). The MAS and SCP methods outperform online-EWC, and SCP performs slightly
better than MAS (i.e., overcomes intransigence better). The third plot shows the average testing
performance of the methods on both tasks. Similarly, we can see that SCP outperforms MAS and
Online-EWC. We repeated the experiments ten times, and the plots show the average performance.
8.5	CIFAR 1 00 Experiments
The CIFAR100 (Krizhevsky et al., 2009) dataset contains images from 20 super-classes, where each
super-class contains five sub-classes. For this dataset, we considered an experiment in which the
tasks are supervised classification of the super-classes. In short, we split the data into five sequential
tasks, where each task contains a sub-class from all the 20 super-classes. We used a Wide Residual
Network (Wide-ResNet) Zagoruyko & Komodakis (2016) network as our model and utilized the
data augmentation suggested by (DeVries & Taylor, 2017). Figure 9a shows sample super-classes
together with data augmentation. For each method, we used a coarse grid search for the importance
parameters {1e + i | i ∈ [2, 3, 4, 5, 6]}. Unfortunately, and due to the lack of time, we were able to
only run the experiment for each method once. We report the average classification results in Figure
9b. For the final version of the paper, we will complete this experiment with 1) a finer grid search
over the importance parameters and 2) multiple runs of the experiments and reporting the average.
17