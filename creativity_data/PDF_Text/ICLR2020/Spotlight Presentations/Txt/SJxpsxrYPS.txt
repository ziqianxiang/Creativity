Published as a conference paper at ICLR 2020
Progressive learning and disentanglement of
HIERARCHICAL REPRESENTATIONS
Zhiyuan Li, Jaideep Vitthal Murkute, Prashnna Kumar Gyawali & Linwei Wang
Golisano College of Computing and Information Sciences
Rochester Institute of Technology
Rochester, NY 14623, USA
{zl7904,jvm6526,pkg2182,Linwei.Wang}@rit.edu
Ab stract
Learning rich representation from data is an important task for deep generative
models such as variational auto-encoder (VAE). However, by extracting high-level
abstractions in the bottom-up inference process, the goal of preserving all factors
of variations for top-down generation is compromised. Motivated by the concept
of “starting small”, we present a strategy to progressively learn independent hi-
erarchical representations from high- to low-levels of abstractions. The model
starts with learning the most abstract representation, and then progressively grow
the network architecture to introduce new representations at different levels of
abstraction. We quantitatively demonstrate the ability of the presented model to
improve disentanglement in comparison to existing works on two benchmark data
sets using three disentanglement metrics, including a new metric we proposed to
complement the previously-presented metric of mutual information gap. We fur-
ther present both qualitative and quantitative evidence on how the progression of
learning improves disentangling of hierarchical representations. By drawing on
the respective advantage of hierarchical representation learning and progressive
learning, this is to our knowledge the first attempt to improve disentanglement by
progressively growing the capacity of VAE to learn hierarchical representations1.
1	Introduction
Variational auto-encoder (VAE), a popular deep generative model (DGM), has shown great promise
in learning interpretable and semantically meaningful representations of data (Higgins et al. (2017);
Chen et al. (2018); Kim & Mnih (2018); Gyawali et al. (2019)). However, VAE has not been able to
fully utilize the depth of neural networks like its supervised counterparts, for which a fundamental
cause lies in the inherent conflict between the bottom-up inference and top-down generation process
(Zhao et al. (2017); Li et al. (2016)): while the bottom-up abstraction is able to extract high-level
representations helpful for discriminative tasks, the goal of generation requires the preservation of all
generative factors that are likely at different abstraction levels. This issue was addressed in recent
works by allowing VAEs to generate from details added at different depths of the network, using
either memory modules between top-down generation layers (Li et al. (2016)), or hierarchical latent
representations extracted at different depths via a variational ladder autoencoder (VLAE, Zhao et al.
(2017)).
However, it is difficult to learn to extract and disentangle all generative factors at once, especially at
different abstraction levels. Inspired by human cognition system, Elman (1993) suggested the im-
portance of “starting small” in two aspects of the learning process of neural networks: incremental
input in which a network is trained with data and tasks of increasing complexity, and incremental
memory in which the network capacity undergoes developmental changes given fixed external data
and tasks — both pointing to an incremental learning strategy for simplifying a complex final task.
Indeed, the former concept of incremental input has underpinned the success of curriculum learning
(Bengio et al. (2015)). In the context of DGMs, various stacked versions of generative adversarial
networks (GANs) have been proposed to decompose the final task of high-resolution image gener-
ation into progressive sub-tasks of generating small to large images (Denton et al. (2015); Zhang
1Source code available at https://github.com/Zhiyuan1991/proVLAE.
1
Published as a conference paper at ICLR 2020
et al. (2018)). The latter aspect of “starting small” with incremental growth of network capacity
is less explored, although recent works have demonstrated the advantage of progressively growing
the depth of GANs for generating high-resolution images (Karras et al. (2018); Wang et al. (2018)).
These works, so far, have focused on progressive learning as a strategy to improve image generation.
We are motivated to investigate the possibility to use progressive learning strategies to improve learn-
ing and disentangling of hierarchical representations. At a high level, the idea of progressively or
sequentially learning latent representations has been previously considered in VAE. In Gregor et al.
(2015), the network learned to sequentially refine generated images through recurrent networks. In
Lezama (2019), a teacher-student training strategy was used to progressively increase the number of
latent dimensions in VAE to improve the generation of images while preserving the disentangling
ability of the teacher model. However, these works primarily focus on progressively growing the
capacity of VAE to generate, rather than to extract and disentangle hierarchical representations.
In comparison, in this work, we focus on 1) progressively growing the capacity of the network to
extract hierarchical representations, and 2) these hierarchical representations are extracted and used
in generation from different abstraction levels. We present a simple progressive training strategy that
grows the hierarchical latent representations from different depths of the inference and generation
model, learning from high- to low-levels of abstractions as the capacity of the model architecture
grows. Because it can be viewed as a progressive strategy to train the VLAE presented in Zhao
et al. (2017), we term the presented model pro-VLAE. We quantitatively demonstrate the ability
of pro-VLAE to improve disentanglement on two benchmark data sets using three disentanglement
metrics, including a new metric we proposed to complement the metric of mutual information gap
(MIG) previously presented in Chen et al. (2018). These quantitative studies include comprehensive
comparisons to β-VAE (Higgins et al. (2017)), VLAE (Zhao et al. (2017)), and the teacher-student
strategy as presented in (Lezama (2019)) at different values of the hyperparameter β. We further
present both qualitative and quantitative evidence that pro-VLAE is able to first learn the most
abstract representations and then progressively disentangle existing factors or learn new factors at
lower levels of abstraction, improving disentangling of hierarhical representations in the process.
2	Related works
A hierarchy of feature maps can be naturally formed in stacked discriminative models (Zeiler &
Fergus (2014)). Similarly, in DGM, many works have proposed stacked-VAEs as a common way to
learn a hierarchy of latent variables and thereby improve image generation (S0nderby et al. (2θl6);
Bachman (2016); Kingma et al. (2016)). However, this stacked hierarchy is not only difficult to train
as the depths increases (S0nderby et al. (2016); Bachman (2016)), but also has an unclear benefit for
learning either hierarchical or disentangled representations: as shown in Zhao et al. (2017), when
fully optimized, it is equivalent to a model with a single layer of latent variables. Alternatively, in-
stead ofa hierarchy of latent variables, independent hierarchical representations at different abstrac-
tion levels can be extracted and used in generation from different depths of the network (Rezende
et al. (2014); Zhao et al. (2017)). A similar idea was presented in Li et al. (2016) to generate lost
details from memory and attention modules at different depths of the top-down generation process.
The presented work aligns with existing works (Rezende et al. (2014); Zhao et al. (2017)) in learning
independent hierarchical representation from different levels of abstraction, and we look to facilitate
this learning by progressively learning the representations from high- to low-levels.
Progressive learning has been successful for high-quality image generation, mostly in the setting of
GANs. Following the seminar work of Elman (1993), these progressive strategies can be loosely
grouped into two categories. Mostly, in line with incremental input, several works have proposed
to divide the final task of image generation into progressive tasks of generating low-resolution to
high-resolution images with multi-scale supervision (Denton et al. (2015); Zhang et al. (2018)).
Alternatively, in line with incremental memory, a small number of works have demonstrated the
ability to simply grow the architecture of GANs from a shallow network with limited capacity for
generating low-resolution images, to a deep network capable of generating super-resolution images
(Karras et al. (2018); Wang et al. (2018)). This approach was also shown to be time-efficient since
the early-stage small networks require less time to converge comparing to training a full network
from the beginning. This latter group of works provided compelling evidence for the benefit of pro-
gressively growing the capacity of a network to generate images, although its extension for growing
the capacity of a network to learn hierarchical representations has not been explored.
2
Published as a conference paper at ICLR 2020
Limited work has considered incremental learning of representations in VAE. In Gregor et al. (2015),
recurrent networks with attention mechanisms were used to sequentially refines the details in gen-
erated images. It however focused on the generation performance of VAE without considering the
learned representations. In Lezama (2019), a teacher-student strategy was used to progressively
grow the dimension of the latent representations in VAE. Its fundamental motivation was that, given
a teacher model that has learned to effectively disentangle major factors of variations, progressively
learning additional nuisance variables will improve generation without compromising the disentan-
gling ability of the teacher - the latter accomplished via a newly-proposed Jacobian supervision.
The capacity of this model to grow, thus, is by design limited to the extraction of nuisance variables.
In comparison, we are interested in a more significant growth of the VAE capacity to progressively
learn and improve disentangling of important factors of variations which, as we will later demon-
strate, is not what the model in Lezama (2019) is intended for. In addition, neither of these works
considered learning different levels of abstractions at different depths of the network, and the pre-
sented pro-VLAE provides a simpler training strategy to achieve progressive representation learning.
Learning disentangled representation is a primary motivation of our work, and an important topic
in VAE. Existing works mainly tackle this by promoting the independence among the learned latent
factors in VAE (Higgins et al. (2017); Kim & Mnih (2018); Chen et al. (2018)). The presented
progressive learning strategy provides a novel approach to improve disentangling that is different to
these existing methods and a possibility to augment them in the future.
3	Methods
3.1	model: VAE with hierarchical representations
We assume a generative model p(x, z) = p(x|z)p(z) for observed x and its latent variable z.
To learn hierarchical representations of x, we decompose z into {z1 , z2, ..., zL} with zl(l =
1, 2, 3, ..., L) from different abstraction levels that are loosely guided by the depth of neural net-
work as in Zhao et al. (2017). We define the hierarchical generative model pθ as:
p(x, z) = p(x|z1, z2, ..., zL)	p(zl).	(1)
l=1
Note that there is no hierarchical dependence among the latent variables as in common hierarchical
latent variable models. Rather, similar to that in Rezende et al. (2014) and Zhao et al. (2017), zl’s
are independent and each represents generative factors at an abstraction level not captured in other
levels. We then define an inference model qφ to approximate the posterior as:
L
q(z1, z2,..., zL|x) =	q(zl|hl(x)),	(2)
l=1
where hl(x) represents a particular level of bottom-up abstraction ofx. We parameterize pθ and qφ
with an encoding-decoding structure and, as in Zhao et al. (2017), we approximate the abstraction
level with the network depth. The full model is illustrated in Fig. 1(c), with a final goal to maximize
a modified evidence lower bound (ELBO) of the marginal likelihood of data x:
logp(x) ≥ L = Eq(z∣x)[logp(x∣z)] - βKL(q(z∖x)∖∖p(z)),	(3)
where KL denotes the Kullback-Leibler divergence, priorp(z) is set to isotropic Gaussian N(0, I)
according to standard practice, and β is a hyperparameter introduced in Higgins et al. (2017) to
promote disentangling, defaulting to the standrd ELBO objective when β = 1.
3.2	progressive learning of hierarchical representation
We present a progressive learning strategy, as illustrated in Fig. 1, to achieve the final goal in equa-
tion (3) by learning the latent variables zl progressively from the highest (l = L) to the lowest
l = 1) level of abstractions. We start by learning the most abstraction representations at layer L as
show in Fig. 1(a). In this case, our model degenerates to a vanilla VAE with latent variables zL at
the deepest layer. We keep the dimension of zL small to start small in terms of the capacity to learn
latent representations, where we define the inference model at progressive step s = 0 as:
ZL 〜N(ML(hL),GL(hL)), hl = f (hi-ι), for I = 1, 2,...,L, and ho ≡ x,	(4)
3
Published as a conference paper at ICLR 2020
Figure 1: Progressive learning of hierarchical representations. White blocks and solid lines are VAE
models at the current progression. α is a fade-in coefficient for blending in the new network com-
ponent. Gray circles and dash line represents (optional) constraining of the future latent variables.
and the generative model as:
gL = fLd(zL), gl = fld(gl+1), x = D(x; f0d(g0)),	(5)
where fe, μL, and ©l are parts of the encoder architecture, fd are parts of the decoder architecture,
and D is the distribution of x parametrized by f0d(g0), which can be either Bernoulli or Gaus-
sian depending on the data. Next, as shown in Fig. 1, we progressively grow the model to learn
zL-1, ..., z2, z1 from high to low abstraction levels. At each progressive step s = 1, 2, ..., L - 1, we
move down one abstraction level, and grow the inference model by introducing new latent code:
zi ~N(μι(hι),σι(hι)), l = L - s.	(6)
Simultaneously, we grow the decoder such that it can generate with the new latent code as:
gι = fιd([mι(zι); gι+1]), l = L - s,	(7)
where mι includes transposed convolution layers outputting a feature map in the same shape as gι+1,
and [∙; ∙] denotes a concatenation operation. The training objective at progressive step S is then:
L
Lpro = Eq(zL,zL-1,...,zL-Sx) [log p(x∣ZL, ZL-I，…,ZL-s)] - β ɪ2 KL(q(zι∣x)l∣P(zι)),	(8)
L-s
By replacing the full objective in equation (3) with a sequence of the objectives in equation (8)
as the training progresses, we incrementally learn to extract and generate with hierarchical latent
representations zι ’s from high to low levels of abstractions. Once trained, the full model as shown
in Fig. 1(c) will be used for inference and generation, and progressive processes are no loner needed.
3.3	implementation strategies
Two important strategies are utilized to implement the proposed progressive representation learning.
First, directly adding new components to a trained network often introduce a sudden shock to the
gradient: in VAEs, this often leads to the explosion of the variance in the latent distributions. To
avoid this shock, we adopt the popular method of “fade-in” (Karras et al. (2018)) to smoothly blend
the new and existing network components. In specific, we introduce a “fade-in” coefficient α to
equations (6) and (7) when growing new components in the encoder and the decoder:
Zι ~N(μι(αhι)，σι(αhι)), gι = fd([αmι(zι); gι+ι]),	(9)
where α increases from 0 to 1 within a certain number of iterations (5000 in our experiments) since
the addition of the new network components μι ,σι, and mι.
Second, we further stabilize the training by weakly constraining the distribution of zι ’s before they
are added to the network. This can be achieved by a applying a KL penalty, modulated by a small
coefficient γ, to all latent variables that have not been used in the generation at progressive step s:
L-s-1
Lpre-trained = γ X XX [ - KL(q(ZI Ix)IIp(ZI))]，	(IO)
ι=1
4
Published as a conference paper at ICLR 2020
where γ is set to 0.5 in our experiments. The final training objective at step s then becomes:
L = LprO + Lpre-trained	(11)
Note that the latent variables at the hierarchy lower than L - s are neither meaningfully inferred
nor used in generation at progressive step s, and Lpre-trained merely intends to regularize the
distribution of these latent variables before they are added to the network. In the experiments below,
we use both “fade-in” and Lpre-trained when implementing the progressive training strategy.
3.4	disentanglement metric
Various quantitative metrics for measuring disentanglement have been proposed (Higgins et al.
(2017); Kim & Mnih (2018); Chen et al. (2018)). For instance, the recently proposed MIG metrics
(Chen et al. (2018)) measures the gap of mutual information between the top two latent dimensions
that have the highest mutual information with a given generative factor. A low MIG score, therefore,
suggests an undesired outcome that the same factor is split into multiple dimensions. However, if
different generative factors are entangled into the same latent dimension, the MIG score will not be
affected.
Therefore, we propose a new disentanglement metric to supplement MIG by recognizing the entan-
glement of multiple generative factors into the same latent dimension. We define MIG-sup as:
1J
~I〉: (InOrm (Zj ; vk(j) ) - maχ InOrm (Zj; Vk )) ,
J 1	k6=k(j)
(12)
where z is the latent variables and v is the ground truth factors, k(j) = argmaxk Inorm (zj ; vk), J is
the number of meaningful latent dimensions, and Inorm (zj ; vk) is normalized mutual information
I(zj ; vk)/H(vk). Considering MIG and MIG-sup together will provide a more complete measure
of disentanglement, accounting for both the splitting of one factor into multiple dimensions and
the encoding of multiple factors into the same dimension. In an ideal disentanglement, both MIG
and MIG-sup should be 1, recognizing a one-to-one relationship between a generative factor and a
latent dimension. This would have a similar effect to the metric that was proposed in Eastwood &
Williams (2018), although MIG-based metrics do not rely on training extra classifiers or regressors
and are unbiased for hyperparameter settings. The factor metric (Kim & Mnih (2018)) also has
similar properties with MIG-sup, although MIG-sup is stricter on penalizing any amount of other
minor factors in the same dimension.
4	Experiment
We tested the presented pro-VLAE on four benchmark data sets: dSprites (Matthey et al. (2017)),
3DShapes (Burgess & Kim (2018)), MNIST (LeCun et al. (1998)), and CelebA (Liu et al. (2015)),
where the first two include ground-truth generative factors that allow us to carry out comprehen-
sive quantitative comparisons of disentangling metrics with existing models. In the following, we
first quantitatively compare the disentangling ability of pro-VLAE in comparison to three existing
models using three disentanglement metrics. We then analyze pro-VLAE from the aspects of how it
learns progressively, its ability to disentangle, and its ability to learn abstractions at different levels.
Comparisons in quantitative disentanglement metrics: For quantitative comparisons, we consid-
ered the factor metric in Kim & Mnih (2018), the MIG in Chen et al. (2018), and the MIG-sup pre-
sented in this work. We compared pro-VLAE (changing β) with beta-VAE (Higgins et al. (2017)),
VLAE (Zhao et al. (2017)) as a hierarchical baseline without progressive training, and the teacher-
student model (Lezama (2019)) as the most related progressive VAE without hierarchical represen-
tations. All models were considered at different values of β except the teacher-student model: the
comparison of β-VAE, VLAE, and the presented pro-VLAE thus also provides an ablation study on
the effect of learning hierarchical representations and doing so in a progressive manner.
For fair comparisons, we strictly required all models to have the same number of latent variables
and the same number of training iterations. For instance, if a hierarchical model has three layers
that each has three latent dimensions, a non-hierarchical model will have nine latent dimensions; if
a progressive method has three progressive steps with 15 epochs of training each, a non-progressive
method will be trained for 45 epochs. Three to five experiments were conducted for each model at
each β value, and the average of the top three is used for reporting the quantitative results in Fig. 2.
5
Published as a conference paper at ICLR 2020
Figure 2: Quantitative comparison of disentanglement metrics. Each point is annotated by the
β value and averaged over top three best random seeds for the given β on the give model. Left
to right: reconstruction errors vs. disentanglement metrics of factor, MIG, and MIG-sup, a higher
value indicating a better disentanglement in each metric.
Figure 3: MIG vs. MIG-sup following a similar presentation in Fig. 2. A better disentanglement
should have higher MIG and higher MIG-sup, locating at the top-right quadrant of the plot.
As shown, for MIG and MIG-sup, VLAE generally outperformed β-VAE at most β values, while
pro-VLAE showed a clear margin of improvement over both methods. With the factor metric, pro-
VLAE was still among the top performers, although with a smaller margin and a larger overlap with
VLAE on 3DShapes, and with β-VAE (β = 10) on dSprites. The teacher-student strategy with
Jacobian supervision in general had a low to moderate disentangling score, especially on 3DShapes.
This is consistent with the original motivation of the method for progressively learning nuisance
variables after the teacher learns to disentangle effectively, rather than progressively disentangling
hierarchical factors of variations as intended by pro-VLAE. Note that pro-VLAE in general per-
formed better with a smaller value of β (β < 20), suggesting that progressive learning already had
an effect of promoting disentangling and a high value of β may over-promote disentangling at the
expense of reconstruction quality.
Fig. 3	shows MIG vs. MIG-sup scores among the tested models. As shown, results from pro-VLAE
were well separated from the other three models at the right top quadrant of the plots, obtaining
simultaneously high MIG and MIG-sup scores as a clear evidence for improved disentangling ability.
Fig. 4	provides images generated by traversing each latent dimension using the best pro-VLAE
(β = 8), the best VLAE (β = 10), and the teacher-student model on 3DShapes data. As shown,
pro-VLAE learned to disentangle the object, wall, and floor color in the deepest layer; the following
hierarchy of representations then disentangled objective scale, orientation, and shape, while the
lowest-level of abstractions ran out of meaningful generative factors to learn. In comparison, the
VLAE distributed six generative factors over the nine latent dimensions, where color was split across
6
Published as a conference paper at ICLR 2020
VLAE
Teacher-student model
MIG:0.3091, MIG-SUP:0.3292,
Factor-metric:0.4927
Figure 4: Traversing each latent dimension in pro-VLAE (β = 8), VLAE (β = 10), and teacher-
student model. The hierarchy of the latent variables is noted by brackets on the side.
Generatated images from random sample
of latent variable
Figure 5: Progressive learning of hierarchical representations. At each progression and for each
zl , the row of images are generated by randomly sampling from its prior distributions while fixing
the other latent variables (this is NOT traversing). The green bar at each row tracks the mutual
information I(x; zl), while the total mutual information I(x; z) is labeled on top.
the hierarchy and sometimes entangled with the object scale (in z2). The teacher-student model was
much less disentangled, which we will delve into further in the following section.
Information flow during progressive learning: To further understand what happened during pro-
gressive learning, we use mutual information I(x, zl) as a surrogate to track the amount of informa-
tion learned in each hierarchy of latent variables zl during the progressive learning. We adopted the
approach in Chen et al. (2018) to empirically estimate the mutual information by stratified sampling.
Fig. 5	shows an example from 3DShapes. At progressive step 0, pro-VAE was only learning the
deepest latent variables in z3 , discovering most of the generative factors including color, objective
shape, and orientation entangled within z3. At progressive step 1, interestingly, the model was able
to “drag” out shape and rotation factors from z3 and disentangle them into z2 along with a new
scale factor. Thus I(x; z3) decreased from 10.59 to 6.94 while I(x; z2) increased from 0.02 to
5.98 in this progression, while the total mutual information I(x; z) increased from 10.61 to 12.84,
suggesting the overall learning of more detailed information. Since 3DShapes only has 6 factors,
the lowest-level representation z1 had nothing to learn in progressive step 2, and the allocation of
mutual information remained nearly unchanged. Note that the sum of I(x, zl)’s does not equal to
I(x, z) and Iover = P1L I(x, zl) - I(x, z) suggests the amount of information that is entangled.
In comparison, the teacher-student model was less effective in progressively dragging entangled
representations to newly added latent dimensions, as suggested by the slowing changing of I(x, zl)’s
7
Published as a conference paper at ICLR 2020
夕7*A夕夕74
C6AA330
4sl42F2q
e / q，0 76 4
^7 If a 2 ∙** 4 3 3
ə ⅛ 6 A— 6。4 夕
Cg6。/。夕,
/3941033
。。。。0。。。
9d>O。。。7。
。0。C。0<7o
。。rsoo。。。
。0。。。04 0
Ooo0。。。。
。。Ooo00。

Shallow
Figure 6:	Visualization of hierarchical features learnt for MNIST data. Each sub-figure is generated
by randomly sampling from the prior distribution of zl at one abstraction level while fixing the
others. The original latent code is inferred from a image with digit “0”. From left to right: z3
encodes the highest abstraction: digit identity; z2 encodes stroke width; and z1 encodes other digit
styles.
latent vaπablez2

ififA f)f)f)HHHΠΠf)
_	- J∣A ---	pale face
陛At ∙t,^k∕∙k∕4kt.

-shadow
►Cb CbCXQbCXO Q Q Q ———— ,	———
slight open mouth
Figure 7:	Visualization of hierarchical features learnt for CelebA data. Each subfigure is generated
by traversing along a selected latent dimension in each row within each hierarchy of zl ’s. From left
to right: latent variables z4 to z1 progressively learn major (e.g., gender in z4 and smile in z3) to
minor representations (e.g. wavy-hair in z2 and eye-shadow in z1) in a disentangled manner.
during progression and the larger value of Iover . This suggests that, since the teacher-student model
was motivated for progressively learning nuisance variables, the extent to which its capacity can
grow for learning new representations is limited by two fundamental causes: 1) because it increases
the dimension of the same latent vectors at the same depth, the growth of the network capacity is
limited in comparison to pro-VLAE, and 2) the Jacobian supervision further restricts the student
model to maintain the same disentangling ability of the teacher model.
Disentangling hierarchical representations: We also qualitatively examined pro-VLAE on data
with both relatively simple (MNIST) and complex (CelebA) factors of variations, all done in un-
supervised training. On MNIST (Figure 6), while the deepest latent representations encoded the
highest-level features in terms of digit identity, the representations learned at shallower levels en-
coded changes in writing styles. In Figure 7, we show the latent representation progressively learned
in CelebA from the highest to lowest levels of abstractions, along with disentangling within each
level demonstrated by traversing one selected dimension at a time. These dimensions are selected
as examples associated with clear semantic meanings. As shown, while the deepest latent rep-
resentation z4 learned to disentangle high-level features such as gender and race, the shallowest
representation z1 learned to disentangle low-level features such as eye-shadow. Moreover, the num-
ber of distinct representations learned decreased from deep to shallow layers. While demonstrating
disentangling by traversing each individual latent dimension or by hierarchically-learned represen-
tations has been separately reported in previous works (Higgins et al. (2017); Zhao et al. (2017)), to
our knowledge this is the first time the ability of a model to disentangle individual latent factors in a
hierarchical manner has been demonstrated. This provides evidence that the presented progressive
strategy of learning can improve the disentangling of first the most abstract representations followed
by progressively lower levels of abstractions.
8
Published as a conference paper at ICLR 2020
5	Conclusion
In this work, we present a progressive strategy for learning and disentangling hierarchical represen-
tations. Starting from a simple VAE, the model first learn the most abstract representation. Next,
the model learn independent representations from high- to low-levels of abstraction by progres-
sively growing the capacity of the VAE deep to shallow. Experiments on several benchmark data
sets demonstrated the advantages of the presented method. An immediate future work is to include
stronger guidance for allocating information across the hierarchy of abstraction levels, either through
external multi-scale image supervision or internal information-theoretic regularization strategies.
References
Philip Bachman. An architecture for deep, hierarchical generative models. In Advances in Neural
Information Processing Systems, pp. 4*26-4834, 2016.
Yoshua Bengio, Jerome Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In
International ConferenCeS on MaChine Learning, 2015.
Chris Burgess and Hyunjik Kim. 3d shapes dataset. https://github.com/deepmind/3dshapes-dataset/,
2018.
Tian Qi Chen, Xuechen Li, Roger B Grosse, and David K Duvenaud. Isolating sources of disentan-
glement in variational autoencoders. In AdvanCeS in Neural Information ProCeSSing Systems, pp.
2610-2620, 2018.
Emily L Denton, Soumith Chintala, Arthur Szlam, and Rob Fergus. Deep generative image models
using a laplacian pyramid of adversarial networks. In AdvanceS in Neural Information Processing
Systems, pp.1486-1494, 2015.
Cian Eastwood and Christopher KI Williams. A framework for the quantitative evaluation of disen-
tangled representations. In InternatiOnal COnferenCe on Learning RePreSentations, 2018.
Jeffrey L Elman. Learning and development in neural networks: The importance of starting small.
COgnition, 48(1):71-99, 1993.
Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Jimenez Rezende, and Daan Wierstra. Draw: A
recurrent neural network for image generation. In InternatiOnal COnference on MaChine Learning,
2015.
Prashnna Kumar Gyawali, Zhiyuan Li, Cameron Knight, Sandesh Ghimire, B Milan Horacek, John
Sapp, and Linwei Wang. Improving disentangled representation learning with the beta bernoulli
process. In IEEE InternatiOnal COnferenCe on Data Mining, 2019.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a
constrained variational framework. In InternatiOnal COnferenCe on Learning RePreSentations,
2017.
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for im-
proved quality, stability, and variation. In International COnferenCe on Learning RePreSentations,
2018.
Hyunjik Kim and Andriy Mnih. Disentangling by factorising. In InternatiOnal COnference on
MaChine Learning, 2018.
Durk P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Im-
proved variational inference with inverse autoregressive flow. In AdVanCeS in neural information
processing systems, pp. 4743T751, 2016.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
9
Published as a conference paper at ICLR 2020
Jose Lezama. Overcoming the disentanglement Vs reconstruction trade-off Viajacobian supervision.
In International Conference on Learning Representations, 2019.
Chongxuan Li, Jun Zhu, and Bo Zhang. Learning to generate with memory. In International
Conference on Machine Learning, pp. 1177-1186, 20l6.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.
In ProCeedingS of the IEEE International ConferenCe on ComPuter ViSion, pp. 3730-3738, 2015.
Loic Matthey, Irina Higgins, Demis Hassabis, and Alexander Lerchner. dsprites: Disentanglement
testing sprites dataset. https://github.com/deepmind/dsprites-dataset/, 2017.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and ap-
proximate inference in deep generative models. In Intemational ConferenCe on MaChine Learning,
2014.
Casper Kaae S0nderby, Tapani Raiko, Lars Maal0e, S0ren Kaae S0nderby, and Ole Winther. Ladder
variational autoencoders. In Advances in neuralinfOrmation processing systems, pp. 3738-3746,
2016.
Yifan Wang, Federico Perazzi, Brian McWilliams, Alexander Sorkine-Hornung, Olga Sorkine-
Hornung, and Christopher Schroers. A fully progressive approach to single-image super-
resolution. In ProCeedingS of the IEEE ConferenCe on ComPUter ViSion and Pattern ReCognition
WorkShops, pp. 864-873, 2018.
Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In
EUroPean ConferenCe on ComPUter Vision, pp. 818-833. Springer, 2014.
Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dim-
itris N Metaxas. Stackgan++: Realistic image synthesis with stacked generative adversarial net-
works. IEEE TranSaCtionS on Pattern AnalySiS andMaChine Intelligence, 41(8):1947-1962, 2018.
Shengjia Zhao, Jiaming Song, and Stefano Ermon. Learning hierarchical features from deep gener-
ative models. In Intemational ConferenCe on MaChine Learning, pp. 4091U099, 2017.
10
Published as a conference paper at ICLR 2020
Appendix
A Examples of performance of different metrics
MIG: 0.1295, MIG-sup:0.4225, factor: 0.6653
Figure 8: An example of one factor being encoded in multiple dimensions. Each row is a traverse for
one dimension (dimension order adjusted for better visualization). Notice that both dim1 and dim2
are encoding floor-color, both dim3 and dim4 are encoding wall-color, and both dim5 and dim6
are encoding object color. Therefore, the MIG is very low since it penalizes splitting one factor to
multiple dimensions. On the other hand, the MIG-sup and factor-metric is not too bad since one
dimension mainly encodes one factor, even though there are some entanglement of color-vs-shape
and color-vs-scale.
Figure 9: An example of one dimension containing multiple factors. Each row is a traverse for one
dimension (dimension order adjusted for better visualization). Notice that both models achieve high
and similar MIG because all 6 factors are encoded and no splitting to multiple dimensions. However,
the right-hand side model has much lower MIG-sup and factor-metric than the left-hand side model.
Because both scale and shape are encoded in dim5, while dim6 has no factor. Both MIG-sup and
factor-metric penalize encoding multiple factors in one dimension. Besides, our MIG-sup is lower
and drops more than factor-metric because MIG-sup is stricter in this case.
11
Published as a conference paper at ICLR 2020
B A closer comparison with VLAE
B.1 Two dimensional traversing on MNIST dataset
4∙8J√4 76 / 7N 夕 4
8，s<，g06z8e6,3*x 4424ɪ8
8<8∕34f34m，-// 46 374 WCOd
3464464*0Nqg2。0/*98夕686
Liβc⅛y40oooβʃ4x^0oσo^oJ∙343
84 7・*-s3£22 9/ 3/1£4 3p^“
2.¥/8008800, QgXo3g3,346&5
♦27,3T* / 3∙1N66g∕6z47J35
5≠,3g3gg,∙q ⅜-34 H4e3N 1，oo
β∙1228rg∙3-74 q3∕48N 63334
，+ 33g£.J8o3qg3 93g2/Jʃ 5
，533,33818354,35f，L 3 f Oo
，，3J/5/4,633/3Mq732/8
38G3g82∕53z8,3 78q4 夕45
JO93 7/ 36*4NJ//s〃g73q8。
W /V 3 00 F ? ‰ 9rzJ39q0G 7∕5f3
3703。，733，~。55900237夕2ʃ
A0 3o∙5yg‰夕3g3z?，2725，^
33333727α∕7∕3∕-∕627,∙*7q
qo∙夕, 733037$ 75 2 33334 Qz7
33313333/337 O32q√j 7 y7
VSV— < V-r9 c∙ 6 ʃ G Zb G 5 ʃ 6 5 c> (M r) C
«1 5 c∙ S S Xte Olzto SCSSLrS 1-L VJ t L i
6 Tv∙q*∙4*-f5c 4∙J 165135q1*Jl
qqsc ,466655^164 19551-9
5555∙ls∕6s 9 夕 5σ-q5Lo-3383
rTr5q50q5455*e33oc0l.633
年 T45 乙 ⅞sz∙3i 3%ΛDO¾4 Q∙ul∕
5 7s≤459 9 I6388Z33Z,3ZI
Γ¾∕^5Λ∙f9∙3f≠ro∕L 8,42 夕6 q
“66夕>32.8，/3，，，141/8337
S 66，rq88/az/2，/g，，2o?
via rgo∕8J 夕 73322V8 ? 21』/
夕夕 08qN37∕√g∕/Uq4,夕；夕
6s‹34 8/3,9 232,4 4 夕/ 3 9 32
3Z7,3CD63J/2007/OOrn
/yrgFJ 夕 OO7，/，，OOJ 4F∕g 夕,
夕夕2gs,夕 37JN2g∕b1og"ga3
d 7gr夕。夕/7夕夕夕?O07Λ*J
WF，夕 4 α92003gyg”,∙⅛<Fe"3”
aFg3<72qα, ，夕 a2〃，，2a
夕 2夕”∕∙3 fp,f0yFy》 & QJF3 ~ 4
-----------!77777777τ7π7m
-----------)77777777Π∙7Λ∕T7ΠΠ
-----------lllz77177777ΠΛT7n∙π∙nl
-----------(ll)777777777AlmMIHI
—----------)l - 777777y7«/ol«/Qlel^lo—
— 7777 夕夕夕夕夕 qσ-1-σ-qQβ~
////∕f∕fffy^∕o<9∙ulο∙QIQiHαiQI
/////，7 fF夕彳。049qqq44q
//////r∕rfpg4gqgqqqqq
//////,⅛ogggoo8gooq4qquq
//∕r∕∕∕y3383880D5SSQQq
/////，zJJ35533333e<p«sq
//，/34，//33 5 33333220q
，/ ///∕444*4-22z2522z2τ!P
/∕,4∕444464JazΛ22L 巳 22N
d”/4d，6666A<i2.a2-2-Nɪ,lZ.
，“〃〃。。。666666>4，3λ‰ɪlɪ-a
〃，〃o，0 60 6666GGb(>U U X ‰ X
〃夕〃。doooOOOGSGGG>G>U>U』3
。夕夕。。。。。0。006GSΘb(o5,3u
OcCCOOOO OooOOGG6(0SSS
Figure 10: MNIST traversing results following the same generation strategy and network hierarchy
as those presented in Figure 5 of (Zhao et al. (2017)). The network has 3 layers and 2 dimensional
latent code at each layer. Each image is generated by traversing each of the two-dimensional latent
code in one layer, while randomly sampling from the other layers. From left to right: The top layer
z3 encodes the digit identity and tilt; z2 encodes digit width (digits around top-left are thicker than
digits around bottom-right); and the bottom layer z1 encodes stroke width. Compared to VLAE,
the representation learnt in the presented method suggests smoother traversing on digits and similar
results for digit width and stroke width.
B.2 Information allocation in each layer
Table 1: Mutual information I(x; zl) between data x and latent codes zl at each l-th depth of the
network, corresponding to the qualitative results presented in Fig. 4 and Fig. 6 on 3Dshapes and
MNIST data sets. Both VLAE and the presented pro-VLAE models have the same hierarchical
architecture with 3 layers and 3 latent dimensions for each layer. Compared to VLAE, the presented
method allocates information in a more clear descending order owing to the progressive learning.
3DShapes	I(x； Z3)	I(x; Z2)	I(x; z1)	total I(x; z)
VLAE	441 ^^	-^4.69^^	5.01	12.75
pro-VLAE	6.94	6.07	0.00	13.02
MNIST	I(x; Z3)	I(x; Z2)	I(x; z1)	total I(x; z)
VLAE	-8.28^^	-^8.89^^	7.86	11.04
pro-VLAE	9.83	8.24	6.28	10.93
12
Published as a conference paper at ICLR 2020
C Ablation study on implementation strategies
Table 2: The effect of progressive implementation strategies, i.e., “fade-in” and pre-trained KL
penalty, on successful training rates. We conducted 15 ablations experiments that each has 4 sub-
experiments. As shown, the pro-VLAE cannot be trained successfully without the presented imple-
mentation strategies, while each of the strategies helps stabilize the progressive training.
no strategies	pre-trained KL only	fade-in only	Both
0.0	0667	0.733	0.867
D Closer investigation of Information flow over latent
VARIABLES
In this section, we present additional quantitative results on how information flow among the latent
variables during progressive training. We conducted experiments on both 3DShapes and MNIST
data sets, considering different hierarchical architectures including a combination of different num-
ber of latent layers L and different number of latent dimensions zdim for each layer. Each experi-
ment was repeated three times with random initializations, from which the mean and the standard
deviation of mutual information I(x; zl) were computed.
As shown in Tables 3-8, for all hierarchical architectures, the information amount in each layer is
captured in a clear descending order, which aligns with the motivation of the presented progressive
learning strategy. Generally, the information also tends to flow from previous layers to new layers,
suggesting a disentanglement of latent factors as new latent layers are added. This is especially
obvious for 3DShapes data where the generative factors are better defined.
In addition, models with small latent codes (zdim = 1) are not able to learn the same amount of
information (total I(x, z)) as those with larger latent codes (zdim = 3). The variance of information
in each layer in the former also appears to be high. We reason that it may be because that the model
is trying to squeeze too much information into a small code, resulting in large vibrations during
progressive learning. On the other hand, while a model has large latent codes (L = 4, zdim = 3),
the information flow becomes less clear after the addition of certain layers. Overall, assuming there
are K generative factors and there are D dimensions in total available in model, ideally we would
like to design the model such that D = K. However, since K is unknown in most data, L and zdim
become hyperparameters that need to be tuned for different data sets.
Table 3: 3DShapes, L = 2
, zdim = 3
progressive step	I(X； Z2 )	I(x； zι)	total I(x; Z)
0	10.68 ± 0.19		10.68 ± 0.19
1	7.22 ± 0.30	5.94 ± 0.26	12.88 ± 0.20
Table 4: 3DShapes, L = 3
, zdim = 2
progressive step	I(x； Z3)	I（X； Z2）	I(x； zι)	total I(x； z)
0	10.16 ± 0.13			10.16 ± 0.13
1	9.76 ± 0.05	7.36 ± 0.10		13.00 ± 0.02
2	6.83 ± 1.37	6.66 ± 0.17	5.80 ± 0.41	13.07 ± 0.02
13
Published as a conference paper at ICLR 2020
Table 5: 3DShapes, L = 4, zdim = 1
progressive step	I（x; z4）	I（x; Z3 ）	I（x; Z2）	I（x； zι）	total I（x; Z）
0	4.89 ± 0.03				4.89 ± 0.03
1	4.77 ± 0.04	3.55 ± 0.04			8.14 ± 0.09
2	4.66 ± 0.04	3.75 ± 0.04	2.70 ± 0.10		10.67 ± 0.09
3	4.55 ± 0.11	3.53 ± 0.35	2.80 ± 0.19	2.17 ± 0.14	11.72 ± 0.03
Table 6: MNIST, L = 3, zdim = 1
progressive step	I（x； Z3 ）	I（x； Z2）	I（x； zι）	total I（x; Z）
0	5.86 ± 1.19			5.86 ± 1.19
1	3.62 ± 1.04	4.64 ± 2.83		7.62 ± 2.63
2	3.88 ± 0.75	4.99 ± 0.98	2.37 ± 0.77	8.25 ± 1.65
Table 7: MNIST, L = 3
, zdim = 3
progressive step	I（x； Z3）	I（x； Z2）	I（x； zι）	total I（x； Z）
0	10.08 ± 0.10			10.08 ± 0.10
1	9, 97 ± 0.05	8.03 ± 0.17		11.01 ± 0.04
2	9.91 ± 0.04	8.09 ± 0.07	6.27 ± 0.02	11.02 ± 0.02
Table 8: MNIST, L = 4, zdim = 3
progressive step	I（x； Z4）	I（x； Z3）	I（x； Z2）	I（x； zι）	total I（x； Z）
0	10.06 ± 0.22				10.06 ± 0.22
1	10.11 ± 0.06	7.95 ± 0.08			10.98 ± 0.02
2	10.06 ± 0.08	8.1 ± 0.04	6.39 ± 0.12		10.98 ± 0.06
3	9.99 ± 0.09	8.11 ± 0.03	6.45 ± 0.15	3.52 ± 0.07	11.03 ± 0.03
14