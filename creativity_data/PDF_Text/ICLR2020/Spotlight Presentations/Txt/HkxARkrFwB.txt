Published as a conference paper at ICLR 2020
word2ket: SPACE-EFFICIENT WORD EMBEDDINGS IN-
spired by Quantum Entanglement
Aliakbar Panahi
Department of Computer Science
Virginia Commonwealth University
panahia@vcu.edu
Seyran Saeedi
Department of Computer Science
Virginia Commonwealth University
saeedis@vcu.edu
Tom Arodz*
Department of Computer Science
Virginia Commonwealth University
Richmond, VA 23284, USA
tarodz@vcu.edu
Ab stract
Deep learning natural language processing models often use vector word embed-
dings, such as word2vec or GloVe, to represent words. A discrete sequence of
words can be much more easily integrated with downstream neural layers if it
is represented as a sequence of continuous vectors. Also, semantic relationships
between words, learned from a text corpus, can be encoded in the relative con-
figurations of the embedding vectors. However, storing and accessing embedding
vectors for all words in a dictionary requires large amount of space, and may
strain systems with limited GPU memory. Here, we used approaches inspired by
quantum computing to propose two related methods* 1, word2ket and word2ketXS,
for storing word embedding matrix during training and inference in a highly effi-
cient way. Our approach achieves a hundred-fold or more reduction in the space
required to store the embeddings with almost no relative drop in accuracy in prac-
tical natural language processing tasks.
1	Introduction
Modern deep learning approaches for natural language processing (NLP) often rely on vector rep-
resentation of words to convert discrete space of human language into continuous space best suited
for further processing through a neural network. For a language with vocabulary of size d, a simple
way to achieve this mapping is to use one-hot representation - each word is mapped to its own row
of a d × d identity matrix. There is no need to actually store the identity matrix in memory, it is triv-
ial to reconstruct the row from the word identifier. Word embedding approaches such as word2vec
(Mikolov et al., 2013) or GloVe (Pennington et al., 2014) use instead vectors of dimensionality p
much smaller than d to represent words, but the vectors are not necessarily extremely sparse nor
mutually orthogonal. This has two benefits: the embeddings can be trained on large text corpora to
capture the semantic relationship between words, and the downstream neural network layers only
need to be of width proportional to p, not d, to accept a word or a sentence. We do, however, need to
explicitly store the d × p embedding matrix in GPU memory for efficient access during training and
inference. Vocabulary sizes can reach d = 105 or 106 (Pennington et al., 2014), and dimensionality
of the embeddings used in current systems ranges from p = 300 (Mikolov et al., 2013; Penning-
ton et al., 2014) to p = 1024 (Devlin et al., 2018). The d × p embedding matrix thus becomes a
substantial, often dominating, part of the parameter space of a learning model.
In classical computing, information is stored in bits - a single bit represents an element from the
set B = {0, 1}, it can be in one of two possible states. A quantum equivalent of a bit, a qubit, is
* Corresponding author.
1PyTorch implementation available at: https://github.com/panaali/word2ket
1
Published as a conference paper at ICLR 2020
fully described by a single two-dimensional complex unit-norm vector, that is, an element from the
set C2. A state of an n-qubit quantum register corresponds to a vector in C2n . To have exponential
dimensionality of the state space, though, the qubits in the register have to be interconnected so that
their states can become entangled; a set of all possible states of n completely separated, independent
qubits can be fully represented by C2n instead of C2n . Entanglement is a purely quantum phe-
nomenon - We can make quantum bits interconnected, so that a state of a two-qubit system cannot
be decomposed into states of individual qubits. We do not see entanglement in classical bits, which
are always independent - we can describe a byte by separately listing the state of each of the eight
bits. We can, however, approximate quantum register classically - store vectors of size m using
O (log m) space, at the cost of losing the ability to express all possible m-dimensional vectors that
an actual O (log m)-qubit quantum register would be able to represent. As we show in this paper,
the loss of representation power does not have a significant impact on NLP machine learning algo-
rithms that use the approximation approaches to store and manipulate the high-dimensional word
embedding matrix.
1.1	Our Contribution
Here, we used approaches inspired by quantum computing to propose two related methods, word2ket
and word2ketXS, for storing word embedding matrix during training and inference in a highly effi-
cient way2. The first method operates independently on the embedding of each word, allowing for
more efficient processing, while the second method operates jointly on all word embeddings, offer-
ing even higher efficiency in storing the embedding matrix, at the cost of more complex processing.
Empirical evidence from three NLP tasks shows that the new word2ket embeddings offer high space
saving rate at little cost in terms of accuracy of the downstream NLP model.
2	FROM TENSOR PRODUCT SPACES TO word2ket EMBEDDINGS
2.1	Tensor Product Space
Consider two separable3 Hilbert spaces V and W. A tensor product space of V and W, denoted
as V 0 W, is a separable Hilbert space H constructed using ordered pairs V 0 w, where V ∈ V
and w ∈ W. In the tensor product space, the addition and multiplication in H have the following
properties
c{V 0 w} = {cV} 0 w = V 0 {cw} ,	(1)
V 0 w + V0 0 w = {V + V0} 0 w,
V 0 w + V 0 w0 = V 0 {w + w0} .
The inner product between V 0 w and V0 0 w0 is defined as a product of individual inner products
hV 0 w,V0 0 w0i = hV, V0i hw, w0i .	(2)
It immediately follows that ||V 0 w|| = ||V|| ||w||; in particular, a tensor product of two unit-norm
vectors, from V and W, respectively, is a unit norm vector in V 0 W. The Hilbert space V 0 W is
a space of equivalence classes of pairs V 0 w; for example {cV} 0 w and V 0 {cw} are equivalent
ways to write the same vector. A vector in a tensor product space is often simply called a tensor.
Let {ψj } and {φk } be orthonormal basis sets in V and W, respectively. From eq. 1 and 2 we can
see that
{χ Cjψj} 0 (X dkφk) = XX Cjdkψj 0 φk,
hψj 0 φk, ψj0 0 φk0i = δj-j0 δk-k0,
where δz is the Kronecker delta, equal to one at z = 0 and to null elsewhere. That is, the set
{ψj 0 φk}jk forms an orthonormal basis in V 0 W, with coefficients indexed by pairs jk and
2In Dirac notation popular in quantum mechanics and quantum computing, a vector u ∈ C2n is written as
|ui, and called a ket.
3That is, with countable orthonormal basis.
2
Published as a conference paper at ICLR 2020
1
√
numerically equal to the products of the corresponding coefficients in V and W . We can add any
pairs of vectors in the new spaces by adding the coefficients. The dimensionality of VXW is the
product of dimensionalities of V and W .
We can create tensor product spaces by more than one application of tensor product, H = UXVXW,
with arbitrary bracketing, since tensor product is associative. Tensor product space of the form
n
O Hj = H1 X H2 X . . . X Hn
j=1
is said to have tensor order4 of n.
2.2	Entangled Tensors
Consider H = V X W. We have seen the addition property v X w + v0 X w = {v + v0} X w
and similar property with linearity in the first argument - tensor product is bilinear. We have not,
however, seen how to express v X w + v0 X w0 as φ X ψ for some φ ∈ V, ψ ∈ W. In many cases,
while the left side is a proper vector from the tensor product space, it is not possible to find such φ
and ψ. The tensor product space contains not only vectors of the form v X w, but also their linear
combinations, some of which cannot be expressed as φ 0 ψ. For example, P；=。Pk=ι ψj√φk can
be decomposed as {P；=。√ψj} 0 {Pk=ι √2φk }. On the other hand, ψ0l0φo√ψ1l0φl cannot; no
matter what we choose as coefficients a, b, c, d, we have
ψo 0 φo + √ψι 0 φι = (aψo + bψι) 0 (cφo + dφι)
=acψo 0 φo + bdψι 0 φι + adψo 0 φι + bcψι 0 φo,
since we require ac = 1 / √2, that is, a = 0, C = 0, and similarly bd = 1/√2, that is, b = 0, C = 0,
yet we also require bd = ad = 0, which is incompatible with a, b, c, d 6= 0.
For tensor product spaces of order n, that is, Njn=； Hj , tensors of the form v = Njn=； vj , where
vj ∈ Hj, are called simple. Tensor rank5 ofa tensor v is the smallest number of simple tensors that
sum up to v; for example, ψ0l0φ0√ψ1l0φ1 is a tensor of rank 2. Tensors with rank greater than one
are called entangled. Maximum rank of a tensor in a tensor product space of order higher than two
is not known in general (BUCzynSki & Landsberg, 2013).
2.3	THE word2ket EMBEDDINGS
A p-dimensional word embedding model involving a d-token vocabulary is6 a mapping f : [d] →
Rp, that is, it maps word identifiers into a p-dimensional real Hilbert space, an inner product space
with the standard inner producth∙,)leading to the L? norm. Function f is trained to capture se-
mantic information from the language corpus it is trained on, for example, two words i, j with
hf(i), f(j)i 〜0 are expected to be semantically unrelated. In practical implementations, we repre-
sent f as a collection of vectors fi ∈ Rp indexed by i, typically in the form of d × p matrix M, with
embeddings of individual words as rows.
We propose to represent an embedding v ∈ Rp of each a single word as an entangled tensor. Specif-
ically, in word2ket, we use tensor of rank r and order n of the form
rn
v = XOvjk,	(3)
4 Note that some sources alternatively call n a degree or a rank of a tensor. Here, we use tensor rank to refer
to a property similar to matrix rank, see below.
5 Note that some authors use rank to denote what we above called order. In the nomenclature used here, a
vector space of n × m matrices is isomorphic to a tensor product space of order 2 and dimensionality mn, and
individual tensors in that space can have rank of up to min(m, n).
6We write [d] = {0, ..., d}.
3
Published as a conference paper at ICLR 2020
where vjk ∈ Rq. The resulting vector v has dimension p = qn, but takes rnq = O (rq log p/q)
space. We use q ≥ 4; it does not make sense to reduce it to q = 2 since a tensor product of two
vectors in R2 takes the same space as a vector in R4 , but not every vector in R4 can be expressed as
a rank-one tensor in R2 0 R2.
If the downstream computation involving the word embedding vectors is limited to inner products
of embedding vectors, there is no need to explicitly calculate the qn -dimensional vectors. Indeed,
we have (see eq. 2)
hv, wi
rn
vjk,	wjk0
k0=1 j=1
r,r n
hvjk,wjk0i.
k,k0=1 j=1
Thus, the calculation of inner product between two p-dimensional word embeddings, v and w, rep-
resented via word2ket takes O r2q log p/q time and O (1) additional space.
In most applications, a small number of embedding vectors do need to be made available for
processing through subsequent neural network layers - for example, embeddings of all words
in all sentences in a batch. For a batch consisting of b words, the total space requirement is
O (bp + rq log p/q), instead of O (dp) in traditional word embeddings.
Reconstructing a b-word batch of p-dimensional word embedding vectors from tensors of rank r and
order n takes O (brpn) arithmetic operations. To facilitate parallel processing, we arrange the order-
n tensor product space into a balanced tensor product tree (see Figure 1), with the underlying vectors
vjk as leaves, and v as root. For example, for n = 4, instead of v = Pk ((v1k 0 v2k) 0 v3k) 0 v4k
we use v = Pk(v1k 0 v2k) 0 (v3k 0 v4k). Instead of performing n multiplications sequentially,
we can perform them in parallel along branches of the tree, reducing the length of the sequential
processing to O (log n).
Typically, word embeddings are trained using gradient descent. The proposed embedding repre-
sentation involves only differentiable arithmetic operations, so gradients with respect to individual
elements of vectors vjk can always be defined. With the balanced tree structure, word2ket represen-
tation can be seen as a sequence of O (log n) linear layers with linear activation functions, where n
is already small. Still, the gradient of the embedding vector v with respect to an underlying tunable
parameters vlk involves products ∂ Pk Qjn=1 vjk /∂vlk = Qj 6=l vjk, leading to potentially high
Lipschitz constant of the gradient, which may harm training. To alleviate this problem, at each node
in the balanced tensor product tree we use LayerNorm (Ba et al., 2016).
3	LINEAR OPERATORS IN TENSOR PRODUCT SPACES AND word2ketXS
3.1	Linear Operators in Tensor Product Spaces
Let A : V → U be a linear operator that maps vectors from Hilbert space V into vector in Hilbert
space U; that is, for v, v0, ∈ V, α, β ∈ R, the vector A(αv + βv0) = αAv + βAv0 is a member of
U . Let us also define a linear operator B : W → Y .
A mapping A 0 B is a linear operator that maps vectors from V 0 W into vectors in U 0 Y . We
define A 0 B : V 0 W → U 0 Y through its action on simple vectors and through linearity
(A 0 B) (X ψj 0 φ) = X(Aψj) 0 (Bφk),
for ψj ∈ V and φk ∈ U. Same as for vectors, tensor product of linear operators is bilinear
(X ajAJ 0 (X bkBk! = X ajbk (Aj 0 Bk).
In finite-dimensional case, for n × n0 matrix representation of linear operator A and m × m0 matrix
representing B, we can represent A 0 B as an mn × m0n0 matrix composed of blocks ajkB.
4
Published as a conference paper at ICLR 2020
3.2	THE word2ketXS EMBEDDINGS
We can see a p-dimensional word embedding model involving a d-token vocabulary as a linear
operator F : Rd → Rp that maps the one-hot vector corresponding to a word into the corresponding
word embedding vector. Specifically, if ei is the i-th basis vector in Rd representing i-th word in
the vocabulary, and vi is the embedding vector for that word in Rp , then the word embedding linear
operator is F = Pid=1 vieiT . If we store the word embeddings a d × p matrix M, we can then
interpret that matrix’s transpose, MT, as the matrix representation of the linear operator F.
Consider q and t such that qn = p and tn = d, and a series of n linear operators Fj : Rt → Rq . A
tensor product jn=1 Fj is a Rd → Rp linear operator. In word2ketXS, we represent the d × p word
embedding matrix as
rn
F = X O Fjk,	(4)
where Fjk can be represented by a q × t matrix. The resulting matrix F has dimension p × d, but takes
rnqt = O (rqt max(log p/q, log d/t)) space. Intuitively, the additional space efficiency comes from
applying tensor product-based exponential compression not only horizontally, individually to each
row, but horizontally and vertically at the same time, to the whole embedding matrix.
We use the same balanced binary tree structure as in word2ket. To avoid reconstructing the full
embedding matrix each time a small number of rows is needed for a multiplication by a weight
matrix in the downstream layer of the neural NLP model, which would eliminate any space saving,
we use lazy tensors (Gardner et al., 2018; Charlier et al., 2018). If A is an m × n matrix and matrix
B is p X q, then ijth entry of A 0 B is equal to
(A 0 B)jj = ab(i-i)∕pC+ι,b(j-i)∕qC+1 bi-b(i-i)/p」p,j-b(j-i)/q」q.
As we can see, reconstructing a row of the full embedding matrix involves only single rows of the
underlying matrices, and can be done efficiently using lazy tensors.
Figure 1: Architecture of the word2ket (left) and word2ketXS (right) embeddings. The word2ket
example depicts a representation of a single-word 256-dimensional embedding vector using rank 5,
order 4 tensor P5k=1 Nj4=1 vjk that uses twenty 4-dimensional vectors vjk as the underlying train-
able parameters. The word2ketXS example depicts representation ofa full 81-word, 16-dimensional
embedding matrix as P5k=1 Nj4=1 Fjk that uses twenty 3 × 2 matrices Fjk as trainable parameters.
5
Published as a conference paper at ICLR 2020
4	EXPERIMENTAL EVALUATION OF word2ket AND word2ketXS IN
Downstream NLP Tasks
In order to evaluate the ability of the proposed space-efficient word embeddings in capturing seman-
tic information about words, we used them in three different downstream NLP tasks: text summa-
rization, language translation, and question answering. In all three cases, we compared the accuracy
in the downstream task for the proposed space-efficient embeddings with the accuracy achieved by
regular embeddings, that is, embeddings that store p-dimensional vectors for d-word vocabulary
using a single d × p matrix.
Table 1: Results for the GIGAWORD text summarization task using Rouge-1, Rouge-2, and Rouge-
L metrics. The space saving rate is defined as the total number of parameters for the embedding
divided by the total number of parameters in the corresponding regular embedding.
Embedding	Order/Rank	Dim	RG-1	RG-2	RG-L	#Params	Space Saving Rate
Regular	1/1	256	35.80	16.40	32.47	7,789,568	1
word2ket	4/1	256	33.65	14.87	30.47	486,848	16
word2ketXS	2/10	400	35.19	16.21	31.76	70,000	111
word2ketXS	4/1	256	34.05	15.39	30.75	224	34,775
Regular	1/1	8,000	36.71	17.48	33.37	243,424,000	1
word2ketXS	2/10	8000	35.17	16.35	31.72	19,200	12,678
In text summarization experiments, we used the GIGAWORD text summarization dataset (Graff
et al., 2003) using the same preprocessing as (Chen et al., 2019), that is, using 200K examples in
training. We used an encoder-decoder sequence-to-sequence architecture with bidirectional forward-
backward RNN encoder and an attention-based RNN decoder (Luong et al., 2015), as implemented
in PyTorch-Texar Hu et al. (2018). In both the encoder and the decoder we used internal layers
with dimensionality of 256 and dropout rate of 0.2, and trained the models, starting from random
weights and embeddings, for 20 epochs. We used the validation set to select the best model epoch,
and reported results on a separate test set. We used Rouge 1, 2, and L scores (Lin, 2004). In
addition to testing the regular dimensionality of 256, we also explored 400, and 8000, but kept the
dimensionality of other layers constant.
The results in Table 1 show that word2ket can achieve 16-fold reduction in trainable parameters at
the cost of a drop of Rouge scores by about 2 points. As expected, word2ketXS is much more space-
efficient, matching the scores of word2ket while allowing for 34,000 fold reduction in trainable
parameters. More importantly, it offers over 100-fold space reduction while reducing the Rouge
scores by only about 0.5. Thus, in the evaluation on the remaining two NLP tasks we focused on
word2ketXS.
The second task we explored is German-English machine translation, using the IWSLT2014 (DE-
EN) dataset of TED and TEDx talks as preprocessed in (Ranzato et al., 2016). We used the same
sequence-to-sequence model as in GIGAWORD summarization task above. We used BLEU score
Table 2: Results for the IWSLT2014 German-to-English machine translation task. The space saving
rate is defined as the total number of parameters for the embedding divided by the total number of
parameters in the corresponding regular embedding.
Embedding	Order/Rank	Dimensionality	BLEU	#Params	Space Saving Rate
Regular	1/1	256	26.44	8,194,816	1
word2ketXS	2/30	400	25.97	214,800	38
word2ketXS	2/10	400	25.33	71,600	114
word2ketXS	3/10	1000	25.02	9,600	853
6
Published as a conference paper at ICLR 2020
Table 3: Results for the Stanford Question Answering task using DrQA model. The space saving
rate is defined as the total number of parameters for the embedding divided by the total number of
parameters in the corresponding regular embedding.
Embedding	Order/Rank	F1	#Params	Space Saving Rate
Regular	1	72.73	35,596,500	1
word2ketXS	2/2	72.23	24,840	1,433
word2ketXS	4/1	70.65	380	93,675
to measure test set performance. We explored embedding dimensions of 100, 256, 400, 1000, and
8000 by using different values for the tensor order and the dimensions of the underlying matrices
Fjk. The results in Table 2 show a drop of about 1 point on the BLEU scale for 100-fold reduction in
the parameter space, with drops of 0.5 and 1.5 for lower and higher space saving rates, respectively.
The third task we used involves the Stanford Question Answering Dataset (SQuAD) dataset. We
used the DrQA’s model (Chen et al., 2017), a 3-layer bidirectional LSTMs with 128 hidden units for
both paragraph and question encoding. We trained the model for 40 epochs, starting from random
weights and embeddings, and reported the test set F1 score. DrQA uses an embedding with vocab-
ulary size of 118,655 and embedding dimensionality of 300. As the embedding matrix is larger, we
can increase the tensor order in word2ketXS to four, which allows for much higher space savings.
Results in Table 3 show a 0.5 point drop in F1 score with 1000-fold saving of the parameter space
required to store the embeddings. For order-4 tensor word2ketXS, we see almost 105-fold space
saving rate, at the cost of a drop of F1 by less than two points, that is, by a relative drop of less than
3%. We also investigated the computational overhead introduced by the word2ketXS embeddings.
For tensors order 2, the training time for 40 epochs increased from 5.8 for the model using regular
embedding to 7.4 hours for the word2ketXS-based model. Using tensors of order 4, to gain addi-
tional space savings, increased the time to 9 hours. Each run was executed on a single NVIDIA Tesla
V100 GPU card, on a 2 Intel Xeon Gold 6146 CPUs, 384 GB RAM machine. While the training
time increased, as shown in Fig. 3, the dynamics of model training remains largely unchanged.
The results of the experiments show substantial decreases in the memory footprint of the word
embedding part of the model, used in the input layers of the encoder and decoder of sequence-to-
sequence models. These also have other parameters, including weight matrices in the intermediate
layers, as well as the matrix of word probabilities prior to the last, softmax activation, that are not
compressed by our method. During inference, embedding and other layers dominate the memory
Figure 2: Dynamics of the test-set F1 score on SQuAD dataset using DrQA model with different
embeddings: rank-2 order-2 word2ketXS, rank-1 order-4 word2ketXS, and regular embedding.
7
Published as a conference paper at ICLR 2020
Figure 3: Test set questions and answers from DrQA model trained using rank-1 order-4
word2ketXS embedding that utilizes only 380 parameters (four 19 × 5 matrices Fjk, see eq. 4)
to encode the full, 118,655-word embedding matrix.
footprint of the model. Recent successful transformer models like BERT by (Devlin et al., 2018),
GPT-2 by (Radford et al., 2019), RoBERTa by (Liu et al., 2019) and Sparse Transformers by (Child
et al., 2019) require hundreds of millions of parameters to work. In RoBERTaBASE, 30% of the
parameters belong to the word embeddings.
During training, there is an additional memory need to store activations in the forward phase in
all layers, to make them available for calculating the gradients in the backwards phase. These
often dominate the memory footprint during training, but one can decrease the memory required for
storing them with e.g. gradient checkpointing Chen et al. (2016) used recently in Child et al. (2019).
4.1	Related Work
Given the current hardware limitation for training and inference, it is crucial to be able to decrease
the amount of memory these networks requires to work. A number of approaches have been used
in lowering the space requirements for word embeddings. Dictionary learning (Shu & Nakayama,
2018) and word embedding clustering (Andrews, 2016) approaches have been proposed. Bit en-
coding has been also proposed Gupta et al. (2015). An optimized method for uniform quantization
of floating point numbers in the embedding matrix has been proposed recently (May et al., 2019).
To compress a model for low-memory inference, (Han et al., 2015) used pruning and quantization
for lowering the number of parameters. For low-memory training sparsity (Mostafa & Wang, 2019)
(Gale et al., 2019) (Sohoni et al., 2019) and low numerical precision (De Sa et al., 2018) (Micike-
vicius et al., 2017) approaches were proposed. In approximating matrices in general, Fourier-based
approximation methods have also been used (Zhang et al., 2018; Avron et al., 2017). None of these
approaches can mach space saving rates achieved by word2ketXS. The methods based on bit en-
coding, such as Andrews (2016); Gupta et al. (2015); May et al. (2019) are limited to space saving
rate of at most 32 for 32-bit architectures. Other methods, for example based on parameter sharing
Suzuki & Nagata (2016) or on PCA, can offer higher saving rates, but their storage requirement is
limited by d + p, the vocabulary size and embedding dimensionality. In more distantly related work,
tensor product spaces have been used in studying document embeddings, by using sketching of a
tensor representing n-grams in the document Arora et al. (2018).
Acknowledgments
T.A. is funded by NSF grant IIS-1453658.
8
Published as a conference paper at ICLR 2020
References
Martin Andrews. Compressing word embeddings. In International Conference on Neural Informa-
tion Processing, pp. 413-422. Springer, 2016.
Sanjeev Arora, Mikhail Khodak, Nikunj Saunshi, and Kiran Vodrahalli. A compressed sensing view
of unsupervised text embeddings, bag-of-n-grams, and LSTMs. In International Conference on
Learning Representations, 2018.
Haim Avron, Michael Kapralov, Cameron Musco, Christopher Musco, Ameya Velingker, and Amir
Zandieh. Random Fourier features for kernel ridge regression: Approximation bounds and statis-
tical guarantees. In Proceedings of the 34th International Conference on Machine Learning, pp.
253-262, 2017.
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450, 2016.
JarosIaW BUczynski and Joseph M Landsberg. Ranks of tensors and a generalization of secant
varieties. Linear Algebra and its Applications, 438(2):668-689, 2013.
Benjamin Charlier, Jean Feydy, and Joan GlaUnes. KeOps: CalcUl rapide sUr GPU dans les espaces
a` noyaUx, 2018.
Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to ansWer open-
domain qUestions. arXiv preprint arXiv:1704.00051, 2017.
LiqUn Chen, Yizhe Zhang, RUiyi Zhang, Chenyang Tao, Zhe Gan, Haichao Zhang, Bai Li, Ding-
han Shen, ChangyoU Chen, and LaWrence Carin. Improving seqUence-to-seqUence learning via
optimal transport. arXiv preprint arXiv:1901.06283, 2019.
Tianqi Chen, Bing XU, ChiyUan Zhang, and Carlos GUestrin. Training deep nets With sUblinear
memory cost. arXiv preprint arXiv:1604.06174, 2016.
ReWon Child, Scott Gray, Alec Radford, and Ilya SUtskever. Generating long seqUences With sparse
transformers. arXiv preprint arXiv:1904.10509, 2019.
Christopher De Sa, Megan Leszczynski, Jian Zhang, Alana Marzoev, Christopher R Aberger,
KUnle Olukotun, and Christopher Re. High-accuracy low-precision training. arXiv preprint
arXiv:1803.03383, 2018.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Trevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity in deep neural networks. CoRR,
abs/1902.09574, 2019.
Jacob Gardner, Geoff Pleiss, Kilian Q Weinberger, David Bindel, and Andrew G Wilson. GPyTorch:
Blackbox matrix-matrix gaussian process inference with GPU acceleration. In Advances in Neural
Information Processing Systems, pp. 7576-7586, 2018.
David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. English gigaword. Linguistic Data Con-
sortium, Philadelphia, 4(1):34, 2003.
Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with
limited numerical precision. In Proceedings of the 32nd International Conference on Machine
Learning, pp. 1737-1746, 2015.
Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks
with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015.
Zhiting Hu, Haoran Shi, Bowen Tan, Wentao Wang, Zichao Yang, Tiancheng Zhao, Junxian He,
Lianhui Qin, Di Wang, et al. Texar: A modularized, versatile, and extensible toolkit for text
generation. arXiv preprint arXiv:1809.00794, 2018.
9
Published as a conference paper at ICLR 2020
Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization
branches out, pp. 74-81, 2004.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
approach. arXiv preprint arXiv:1907.11692, 2019.
Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-
based neural machine translation. In Proceedings of the 2015 Conference on Empirical Methods
in Natural Language Processing, pp. 1412--1421, 2015.
Avner May, Jian Zhang, Tri Dao, and Christopher Re. On the downstream performance of
compressed word embeddings. In Advances in Neural Information Processing Systems, pp.
arXiv:1909.01264, 2019.
Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia,
Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision
training. arXiv preprint arXiv:1710.03740, 2017.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed represen-
tations of words and phrases and their compositionality. In Advances in Neural Information
Processing Systems, pp. 3111-3119, 2013.
Hesham Mostafa and Xin Wang. Parameter efficient training of deep convolutional neural networks
by dynamic sparse reparameterization. In International Conference on Machine Learning, pp.
4646-4655, 2019.
Jeffrey Pennington, Richard Socher, and Christopher Manning. GloVe: Global vectors for word rep-
resentation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language
Processing, pp. 1532-1543, 2014.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
models are unsupervised multitask learners. OpenAI Blog, 1(8), 2019.
Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. Sequence level train-
ing with recurrent neural networks. In International Conference on Learning Representations,
pp. arXiv:1511.06732, 2016.
Raphael Shu and Hideki Nakayama. Compressing word embeddings via deep compositional code
learning. In International Conference on Learning Representations, pp. arXiv:1711.01068, 2018.
Nimit Sharad Sohoni, Christopher Richard Aberger, Megan Leszczynski, Jian Zhang, and Christo-
pher Re. Low-memory neural network training: A technical report. CoRR, abs/1904.10631,
2019.
Jun Suzuki and Masaaki Nagata. Learning compact neural word embeddings by parameter space
sharing. In International Joint Conference on Artificial Intelligence, pp. 2046-2051, 2016.
Jian Zhang, Avner May, Tri Dao, and Christopher Re. Low-precision random Fourier features for
memory-constrained kernel approximation. arXiv preprint arXiv:1811.00155, 2018.
10