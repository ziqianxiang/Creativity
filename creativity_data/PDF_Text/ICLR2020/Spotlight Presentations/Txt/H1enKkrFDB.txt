Published as a conference paper at ICLR 2020
Stable Rank Normalization for Improved Gen-
eralization in Neural Networks and GANs
Amartya Sanyal
Department of Computer Science
University of Oxford,
The Alan Turing Institute
amartya.sanyal@cs.ox.ac.uk
Philip H. Torr
Department of Engineering Science
University of Oxford
philip.torr@eng.ox.ac.uk
Puneet K. Dokania
Department of Engineering Science
University of Oxford
puneet@robots.ox.ac.uk
Ab stract
Exciting new work on generalization bounds for neural networks (NN) given
by Bartlett et al. (2017); Neyshabur et al. (2018) closely depend on two parameter-
dependant quantities a) the Lipschitz constant upper bound and b) the stable rank
(a softer version of rank). Even though these bounds typically have minimal
practical utility, they facilitate questions on whether controlling such quantities
together could improve the generalization behaviour of NNs in practice. To this
end, we propose stable rank normalization (SRN), a novel, provably optimal,
and computationally efficient weight-normalization scheme which minimizes the
stable rank of a linear operator. Surprisingly we find that SRN, despite being
non-convex, can be shown to have a unique optimal solution. We provide extensive
analyses across a wide variety of NNs (DenseNet, WideResNet, ResNet, Alexnet,
VGG), where applying SRN to their linear layers leads to improved classification
accuracy, while simultaneously showing improvements in generalization, evaluated
empirically using shattering experiments (Zhang et al., 2016); and three measures
of sample complexity by Bartlett et al. (2017), Neyshabur et al. (2018), & Wei &
Ma. Additionally, we show that, when applied to the discriminator of GANs, it
improves Inception, FID, and Neural divergence scores, while learning mappings
with a low empirical Lipschitz constant.
1 Introduction
Deep neural networks have shown astonishing ability in tackling a wide variety of machine learning
problems including a great ability to generalize under extreme over-parameterization. Within this
work we leverage very recent, and important, theoretical results on the generalization bounds of
deep networks to yield a practical low cost method to normalize the weights within a network
using a scheme - which we call Stable Rank Normalization (SRN). The motivation for SRN comes
from the generalization bound for NNs given by NeyshabUr et al. (2018) and Bartlett et al. (2017),
O Qid kWi k22 Pid=1 srank(Wi) 1 , that depend on two parameter-dependent quantities: a) the
scale-dependent Lipschitz constant upper-bound Qid kWi k2 (product of spectral norms) and b) the
sum of scale-independent stable ranks (srank(W)). Stable rank is a softer version of the rank operator
and is defined as the squared ratio of the Frobenius norm to the spectral norm. Although these two
terms appear frequently in these bounds, the empirical impact of simultaneously controlling them on
the generalization behaviour of NNs has not been explored yet possibly because of the difficulties
associated with optimizing stable rank. This is precisely the goal of this work and based on extensive
experiments across a wide variety of NN architectures, we show that, indeed, controlling them
1d and kWik2 represents the number of layers and the spectral norm of the i-th linear layer Wi, respectively.
1
Published as a conference paper at ICLR 2020
simultaneously improves the generalization behaviour, while improving the classification performance
of NNs. We observe improved training of Generative Adversarial Networks (GAN) Goodfellow et al.
(2014) as well.
To this end, we propose Stable Rank Normalization (SRN) which allows us to simultaneously control
the Lipschitz constant and the stable rank of a linear operator. Note that the widely used Spectral
Normalization (SN) (Miyato et al., 2018) allows explicit control over the Lipschitz constant, however,
as will be discussed in the paper, it does not have any impact on the stable rank. We would like to
emphasize that, as opposed to SN, the SRN solution is optimal and unique even in situations when
it is non-convex. It is one of those rare cases where an optimal solution to a provably non-convex
problem could be obtained. Computationally, our proposed SRN for NNs is no more complicated
than SN, just requiring computation of the largest singular value which can be efficiently obtained
using the power iteration method (Mises & Pollaczek-Geiringer, 1929).
Experiments Although SRN is in principle applicable to any problem involving a sequence of
affine transformations, considering recent interests, we show its effectiveness when applied to the
linear layers of deep neural networks. We perform extensive experiments on a wide variety of NN
architectures (DenseNet, WideResNet, ResNet, Alexnet, VGG) for the analyses and show that, SRN,
while providing the best classification accuracy (compared against standard, or vanilla, training
and SN), consistently shows improvement on the generalization behaviour. We also experiment
with GANs and show that, SRN prefers learning discriminators with low empirical Lipschitz while
providing improved Inception, FID and Neural Divergence scores (Gulrajani et al., 2019).
We would like to note that although SN is being widely used for the training of GANs, its effect on
the generalization behaviour over a wide variety of NNs has not yet been explored. To the best of our
knowledge, we are the first to do so.
Contributions
•	We propose SRN— a novel normalization scheme for simultaneously controlling the Lips-
chitz constant and the stable rank of a linear operator.
•	Optimal and unique solution to the provably non-convex stable rank normalization problem.
•	Efficient and easy to implement SRN algorithm for NNs.
2	Background and Intuitions
Neural Networks Consider fθ : Rm → Rk to be a feed-forward multilayer NN parameterized
by θ ∈ Rn, each layer of which consists of a linear followed by a non-linear2 mapping. Let
al-1 ∈ Rnl-1 be the input (or pre-activations) to the l-th layer, then the output (or activations)
of this layer is represented as al = φl (zl), where zl = Wlal-1 + bl is the output of the linear
(affine) layer parameterized by the weights Wl ∈ Rnl-1 ×nl and biases bl ∈ Rnl, and φl(.) is
the element-wise non-linear function applied to zl . For classification tasks, given a dataset with
input-output pairs denoted as (x ∈ Rm, y ∈ {0, 1}k; Pj yj = 1) 3, the parameter vector θ is learned
using back-propagation to optimize the classification loss (e.g., cross-entropy).
Singular Value Decomposition (SVD) Given W ∈ Rs×r with rank k ≤ min(s, r), we denote
{σi}ik=1, {ui}ik=1, and {vi}ik=1 as its singular values, left singular vectors, and right singular vectors,
respectively. Throughout this work, a set of singular values is assumed to be sorted σι ≥ ∙∙∙ ≥ σk.
σi(W) denotes the i-th singular value of the matrix W. Using singular values, the matrix 2-norm
∣Wk2 and the Frobenius norm IlWkF Can be ComPUted as σι and ∖∕P~σ2, respectively.
Stable Rank Below we provide the formal definition and some properties of stable rank.
Definition 2.1. The Stable Rank (Rudelson & Vershynin, 2007) of an arbitrary matrix W is defined
kWk2	Pk σ2 (W)
as Srank(W) = ^^2 = -i=2(W)--, where k is the rank ofthe matrix. Stable rank Is
•	a soft version of the rank operator and, unlike rank, is less sensitive to small perturbations.
2e.g. ReLU, tanh, sigmoid, and maxout.
3 yj is the j-th element of vector y. Only one class is assigned as the ground-truth label to each x.
2
Published as a conference paper at ICLR 2020
•	differentiable as both Frobenius and Spectral norms are almost always differentiable.
•	upper-bounded by the rank: Srank(W) = Pi=j(W(W) ≤ Pi=I(W(W) = k.
•	invariant to scaling, implying, Srank(W) = srank( (W), for any η ∈ R \ {0}.
Lipschitz Constant Here we describe the global and the local Lipschitz constants. Briefly, the Lip-
schitz constant is a quantification of the sensitivity of the output with respect to the change in the input.
A function f : Rm 7→ Rk is globally L-Lipschitz continuous if ∃L ∈ R+ : kf(xi) - f (xj)kq ≤
L ∣∣Xi - Xjkp , ∀(xi, Xj) ∈ Rm, where ∣∣∙kp and |卜|[ represents the norms in the input and the output
metric spaces, respectively. The global Lipschitz constant Lg is:
Lg
max
xi,xj∈Rm
xi 6=xj
kf(Xi)- f(xj)kq
kxi - Xjkp
(1)
The above definition of the Lipschitz constant depends on all pairs of inputs in the domain
Rm × Rm , (thus, global). However, one can define the local Lipschitz constant based on the
sensitivity off in the vicinity of a given point X. Precisely, at X, for an arbitrarily small
δ > 0, the local Lipschitz constant is computed on the open ball of radius δ centered at X. Let
h ∈ Rm, khkp < δ, then, similar to Lg, the local Lipschitz constant off at X, Ll (X), is greater
than or equal to Suph6=0,khk <δ
kf(X + h)-f(X)kq
Mp
Assuming f to be Frechet differentiable, as
h → 0, usingf (X + h) -f(X) ≈ Jf (X)h, Ll(X) is the matrix (operator) norm of the Jacobian
(Jf (x)=娶 |x ∈ Rk×m) as follows:4
Ll(X)
(=a)
lim Sup
δ→0 h6=0
khkp<δ
kJf (x)hkq
khkp
(b)
Sup
h6=0
h∈Rm
Jf(x)hkq
khkp
kJf (x)kp,q.
(2)
A function is said to be locally Lipschitz with local Lipschitz constant Ll if for all x ∈ Rm he function
is Ll locally-Lipschitz at x. Thus, Ll = Supx∈Rm Ll (x). Notice that the Lipschitz constant (global
or local), greatly depends on the chosen norms. When p = q = 2, the upperbound on the local
Lipschitz constant at x boils down to the 2-matrix norm (maximum singular value) of the Jacobian
Jf (x) (see last equality of (2)). With these preliminary definitions, in Section 3, we discuss more
optimistic (or empirical) estimates of Ll and Lg , its link with generalization and then in Section 5,we
show empirically the effect of SRN on them and on generalization.
The local Lipschitz upper-bound for Neural Networks As mentioned earlier (2), Ll (x) =
kJf (x)kp,q, where, in the case of NNs (proof along with why it is loose in Appendix C)
Ll (x) = kJf (x)kp,q ≤kWι kp,q …kWιkp,q and Ll = Ll(X)	(3)
3	Why Stable Rank Normalization
Lipschitz alone is not sufficient Although learning low Lipschitz functions has been shown to
provide better generalization (Anthony & Bartlett, 2009; Bartlett et al., 2017; Neyshabur et al., 2018;
2015; Yoshida & Miyato, 2017; Gouk et al., 2018), enable stable training of GANs (Arjovsky et al.,
2017; Gulrajani et al., 2017; Miyato et al., 2018) and help provide robustness against adversarial
attacks (Cisse et al., 2017), controlling Lipschitz upper bound alone is not sufficient to provide
assurance on the generalization error. One of the reasons is that it is scale-dependent, implying, for
example, even though scaling an entire ReLU network would not alter the classification behaviour, it
can massively increase the Lipschitz constant and thus the theoretical generalization bounds. This
suggests that either the bound is of no practical utility, or at least one should regulate both—the
Lipschitz constant, and the stable rank (scale-independent)—in a hope to see improved generalization
in practice.
4Here, (a) is by definition of local lipschitzness and (b) is due to the property of norms that for any non-
negative scalar c, kcxk = c kxk.
3
Published as a conference paper at ICLR 2020
Stable rank controls the noise-sensitivity As shown by Arora et al. (2018), one of the critical
properties of generalizable NNs is low noise sensitivity— the ability of a network to preferentially
carry over the true signal in the data. For a given noise distribution N , it can be quantified as
Φfθ,N = maxΦfθ,N (x),
where Φfθ,n (x) := En〜N
kfθ (X + η kχk)-fθ (x)『
kfθ (χ)k2
For a linear mapping with parameters W and the noise distribution being normal-N (0, I), it can be
shown that ΦfW,N ≥ srank(W) (c.f. Proposition 3.1 in Arora et al. (2018)). Thus, decreasing the
stable rank directly decreases the lower bound of the noise sensitivity. In Figure 1, we show Φfθ,N of
a ResNet110 trained on CIFAR100. Note that although the Lipschitz upper bound of SRN and SN
are the same, SRN (algorithmic details in Section 4) is much less sensitive to noise as the constraints
imposed enforce the stable rank to decrease to 30% of its original value, which in effect reduces the
noise sensitivity.
Stable rank impacts empirical Lipschitz constant It is apparent
that the Lipschitz constant upper bound (Qid kWi k2), along with
being scale-dependent, also is data-independant and hence, provides
a pessimistic estimate of the behaviour of a model on a particular task
or dataset. Considering this, a relatively optimistic estimate of the
model’s behaviour would be an empirical estimate of the Lipschitz
constant (Le) on a task-specific dataset 5. Note that local Le is just
the norm of the Jacobian at a given point 6. Empirically, Novak et al.
(2018) provided results showing how local Le (in the vicinity of
train data) is correlated with the generalization error of NNs. This
observation is further supported by the work of Wei & Ma; Nagarajan
& Kolter (2019); Arora et al. (2018) whereby the variants of Le are
used to derive generalization bounds. Thus, a tool that favours low
Le is likely to provide better generalization behaviour in practice.
To this end, we first consider a simple two layer linear-NN example
in Appendix B.2 and show that low rank mappings do favour low
Figure 1: Noise Sensitivity
(lower the better). Test ac-
curacy: SRN (73.1%), SN
(71.5%), and Vanilla (72.4%).
Le . Since direct minimization of rank for NNs is non-trivial, the expectation is that learning low
stable rank (softer version of rank) might induce similar behaviour. We experimentally validate this
hypothesis by showing that, as we decrease the stable rank, the empirical Lipschitz decreases. This
shows SRN indeed prefers mappings with a low empirical Lipschitz constant.
4	Stable Rank Normalization
Here we provide a theoretically sound procedure to do SRN. A big challenge in stable rank normal-
ization comes from the fact that it is scale-invariant (refer Definition 2.1), thus, any normalization
scheme that modifies W = Pi σiUiV> to W = Pi σ^ UiV> Will have no effect on the stable rank,
making SRN non-trivial. Examples of such schemes are SN (Miyato et al., 2018) where η = σ1,
and Frobenius normalization Where η = kWkF . As Will be shoWn, our approach to stable rank
normalization is optimal and efficient. Note, the Widely used SN (Miyato et al., 2018) is not optimal
(proof in Appendix A.2).
The SRN Problem Statement Given a matrix W ∈ Rm×n With rank p and spectral partitioning
index k (0 ≤ k < p), We formulate the SRN problem as:
arg min Il W 一 W k∣∣	s.t.	Srank(W k) = r, λi = σi, ∀i ∈{1,…，k} .	(4)
Wk ∈Rm×n	F	、	{Z	'、/ V	/
stable rank constraint spectrum preservation constraints
Where, 1 ≤ r < srank(W) is the desired stable rank, λis and σis are the singular values of Wc k and
W, respectively. The partitioning index k is used for the singular value (or the spectrum) preservation
5It can be the train-/test- data, the generated data (e.g., in GANs), or some interpolated data points.
6For completeness, We provide the relationship betWeen the global and the local Le in Proposition B.1.
4
Published as a conference paper at ICLR 2020
constraint. It gives us the flexibility to obtain Wc k such that its top k singular values are exactly the
same as that of the original matrix. Note, the problem statement is more general in the sense that
putting k = 0 removes the spectrum preservation constraint.
The Solution to SRN The optimal unique solution to the above problem is provided in Theorem 1
and proved in Appendix A.1. Note, at k = 0, the problem (4) is non-convex, otherwise convex.
Theorem 1. Given a real matrix W ∈ Rm×n with rank p, a target spectrum (or singular value)
preservation index k (0 ≤ k < p), and a target stable rank of r (1 ≤ r < srank(W)), the
optimal solution Wc k to problem (4) is Wc k = γ1 S1 + γ2 S2, where S1 = Pim=a1x(1,k) σiuivi> and
S2 = W - S1. {σi}ik=1, {ui}ik=1 and {vi}ik=1 are the top k singular values and vectors of W, and,
depending on k, γι and γ2 are defined below. For simplicity, we first define Y = ""SilSIkF，then
a)	If k = 0 (no spectrum preservation), the problem becomes non-convex, the optimal solution
to which is obtainedfor γ2	= Y +	r—	and γι	=	γ2,	when r >	1.	If r = 1, then γ2	= 0
rγ
and Y1 = 1. Since, in this case,
kSιkF = σ2, Y = √kr-1
kS k2
b)	If k ≥ 1, the problem IS convex. If r ≥ 11 121f the optimal solution IS obtainedfor γι
σ1
and Y2 = Y and if not, the problem is not feasible.
c)	Also, Wc k - W monotonically increases with k for k ≥ 1.
1,
Intuitively, Theorem 1 partitions the given matrix into two parts, depending on k, and then scales
them differently in order to obtain the optimal solution. The value of the partitioning index k is a
design choice. If there is no particular preference to k, then k = 0 provides the most optimal solution.
We provide a simple example to better understand this. Given W = I3 (rank = srank(W) = 3), the
objective is to project it to a new matrix with stable rank of 2. Solutions to this problem are:
00	1
1 0 , Wc 2 = 0
00	0
0
1
√2
0
0
0
1
√2.
√2+ι
2
0
0
0	0
√2+ι	0
2√2	0
O	√2+ι
0	2√2
(5)
Here, Wc 1 is obtained using the standard rank minimization (Eckart-Young-Mirsky (Eckart & Young,
1936)) while W2 and W2 are the solutions of Theorem 1 with k = 1 and k = 0, respectively. It is
easy to verify that the stable rank of all the above solutions is 2. However, the Frobenius distance
(lower the better) of these solutions from the original matrix follows the order W - Wc1	>
W - Wc 2	> W - Wc 3 . As evident from the example, the solution to SRN, instead of
completely removing a particular singular value, scales them (depending on k) such that the new
matrix has the desired stable rank. Note that for Wc1 (true for any k ≥ 1), the spectral norm of
the original and the normalized matrices are the same, implying, Y1 = 1. However, for k = 0, the
spectral norm of the optimal solution is greater than that of the original matrix. It is easy to verify
from Theorem 1 that as k increases, Y2 decreases. Thus, the amount of scaling required for the second
partition S2 is more aggressive. In all situations, the following inequality holds: Y2 ≤ 1 ≤ Y1.
5
Published as a conference paper at ICLR 2020
Algorithm 2 SRN for a Linear Layer in NN
Algorithm 1 Stable Rank Normalization
Require: W ∈ Rm×n, r, k ≥ 1
2
1	Si <- 0, β <— I∣w∣∣F, η <- 0, ι <- 0
2	for i ∈ {1,…，k} do
3	{ui, vi,σi}^ SVD(W,i)
4	:	. Power method to get i-th singular value
5	if r ≥ (σ2 + η) /σ2 then
6	Si — Si + σiUiV>
7	η - η + σ2,β - β - σi
8	ι 一 ι +1
9	:	else
10	:	break
11	:	end if
12	: end for
13	2 η — rσ1 — η
14	return Wι — Si + q(W - Si), l
Require: W ∈ Rm×n , r, learning rate α, mini-
batch dataset D
1	Initialize u ∈ Rm with a random vector.
2	V — w>u U — W>v J kw>uk, U ・ kwF
3	. Perform power iteration
4	σ(W) = U>Wv
5	W f = W/σ(W) . Spectral Normalization
6	Wc = Wf - Uv>
7	if IlWll ≤ √r-1 then F
8	return Wf
9	end if
10	Wf = uv> + W 1√r-i	. Stable Rank f	+	kw IIf
	Normalization
11	return W — W — αVwL(Wf, D)
Algorithm for Stable Rank Normalization
We provide a general procedure in Algorithm 1 to
solve the stable rank normalization problem for k ≥ 1 (the solution for k = 0 is straightforward
from Theorem 1). Claim 2 provides the properties of the algorithm. The algorithm is constructed so
that the prior knowledge of the rank of the matrix is not necessary.
Claim 2. Given a matrix W, the desired stable rank r, and the partitioning index k ≥ 1, Algorithm 1
requires computing the top l (l ≤ k) singular values and vectors of W. It returns Wl and the scalar
l such that srank(Wl ) = r, and the top l singular values of W and Wl are the same. If l = k,
then the solution provided is the optimal solution to the problem (4) with all the constraints satisfied,
otherwise, it returns the largest l up to which the spectrum is preserved.
Combining Stable Rank and Spectral Normalization for NNs Following the arguments pro-
vided in Section 1 and 3, for better generalizability, we propose to normalize both the stable rank and
the spectral norm of each linear layer of a NN simultaneously. To do so, we first perform approximate
SN (Miyato et al., 2018), and then perform optimal SRN (using Algorithm 1). We use k = 1 to
ensure that the first singular value (which is now normalized) is preserved. Algorithm 2 provides a
simplified procedure for the same for a given linear layer of a NN. Note, the computational cost of
this algorithm is exactly the same as that of SN, which is to compute the top singular value using the
power iteration method.
5	Experiments
Dataset and Architectures For classification, we perform experiments on ResNet-110 (He et al.,
2016), WideResNet-28-10 (Zagoruyko & Komodakis, 2016), DenseNet-100 (Huang et al., 2017),
VGG-19 (Simonyan & Zisserman, 2014), and AlexNet (Krizhevsky & Hinton, 2009) using the
CIFAR100 (Krizhevsky & Hinton, 2009) dataset. We present further experiments with CIFAR10
in Appendix D.1 in Figure 10 and 11. We train them using standard training recipes with SGD, using
a learning rate of 0.1 (except AlexNet where we use a learning rate of 0.01), and a momentum of 0.9
with a batch size of 128 (further details in Appendix D). In addition to training for a fixed number of
epochs, we also present results in the Appendix in Figure 8 and 9 where the training accuracy (as
opposed to number of iterations) is used as a stopping criterion to show that our regularizor performs
well with a range of stopping criterions.
For GAN experiments, we use CIFAR100, CIFAR10, and CelebA (Liu et al., 2015) datasets. We
show results on both, conditional and unconditional GANs. Please refer to Appendix E.1 for further
details about the training setup.
Choosing stable rank Given a matrix W ∈ Rm×n , the desired stable rank r is controlled using a
single hyperparameter c as r = c min(m, n), where c ∈ (0, 1]. For simplicity, we use the same c for
6
Published as a conference paper at ICLR 2020
Figure 2: Test accuracies on CIFAR100 for clean data. Higher is better.
(a) Resnet110 (b) WideResnet-28 (c) Alexnet (d) Densenet-100 (e) VGG-19
all the linear layers. Itis trivial to note that if c = 1, or for a given c, if srank(W) ≤ r, then SRN boils
down to SN. For classification, we choose c = {0.3, 0.5}, and compare SRN against standard training
(Vanilla) and training with SN. For GAN experiments, we choose c = {0.1, 0.3, 0.5, 0.7, 0.9}, and
compare SRN-GAN against SN-GAN (Miyato et al., 2018), WGAN-GP (Gulrajani et al., 2017), and
orthonormal regularization GAN (Ortho-GAN) (Brock et al., 2016).
Result Overview
•	SRN improves classification accuracy on a wide variety of architectures.
•	Normalizing stable rank improves the learning capacity of spectrally normalized networks.
• SRN shows remarkably less memorization, even on settings very hard to generalize.
•	SRN shows much improved generalization behaviour evaluated using recently proposed
sample complexity measures.
•	As we decrease the stable rank, the empirical Lipschitz of SRN-GAN decreases. Proving
our arguments provided in Section 3.
•	SRN-GAN provides much improved Neural divergence score (ND) (Gulrajani et al., 2019)
compared to SN-GAN, proving that it is robust to memorization in GANs as well.
•	SRN-GAN also provide improved Inception and FID scores in all our experiments (except
one where SN-GAN is better).
5.1	Classification Experiments
We perform each experiment 5 times using a new random seed each time and report the mean, and
the 75% confidence interval for the test error in Figure 2. These experiments show that the test
accuracy of SRN, on a wide variety NNs, is always higher than the Vanilla and SN (except for
SRN-50 on Alexnet where SRN and SN are almost equal). However, SN performs slightly worse
than Vanilla for WideResNet-28 and ResNet110. The fact that SRN does involve SN, combined with
the above statement, indicate that even though SN reduced the learning capability of these networks,
normalizing stable rank must have improved it significantly in order for SRN to outperform Vanilla.
For example, in the case of ResNet110, SN is 71.5% accurate whereas SRN provides an accuracy of
73.2%. In addition to this, we would like to note that even though SN is being used extensively for the
training of GANs, it is not a popular choice when it comes to training standard NNs for classification.
We suspect that this is because of the decrease in the capacity, which we have shown to be increased
by the stable rank normalization, proving the worth of SRN for classification tasks as well.
5.2	Study of Generalization Behaviour
Our last set of experiments established that SRN provides improved classification accuracies on
various NNs. Here we study the generalization behaviour of these models. Quantifying generalization
behaviour is non-trivial and there is no clear answer to it. However, we utilize recent efforts that
explore the theoretical understanding of generalization and use them to study it in practice.
Shattering Experiments To inspect the generalization behaviour in NNs we begin with the shat-
tering experiment (Zhang et al., 2016). It is a test of whether the network can fit the training data well
but not a label-randomized version of it (each image of the dataset is assigned a random label). As
there is no correlation of the labels with the data points P (y|x) is essentially uninformative because
it is uniformly random. Thus, the test accuracy on this task is almost 1%. A high training accuracy —
which indicates a high generalization gap (difference between train and test accuracy) can be achieved
7
Published as a conference paper at ICLR 2020
(a) Resnet110 (b) WideResNet-28 (c) Alexnet
(d) Densenet-100 (e) VGG-19
Figure 3: Train accuracies on CIFAR100 for shattering experiment. Lower indicate less memorization,
thus, better.
至SUeα
(g) D100-Jac-Norm	(h) D100-Spec-L1	(i) D100-Spec-Fro
Figure 4: (log) Sample complexity (Calg) of ResNet-110 (Figure 4a to 4c), WideResNet-28-10 (Fig-
ure 4d to 4f), and Densenet-100 (Figure 4g to 4i) quantified using the three measures discussed in the
paper. Left is better. Vanilla is omitted from Figure 4b, 4c, 4h and 4i as it is too far to the right. Also,
in situations where SRN-50 and SN performed the same, we removed the histogram to avoid clutter.
only by memorizing the train data 7. Figure 3 shows that SRN reduces memorization on random
labels (thus, reduces the estimate of the Rademacher complexity (Zhang et al., 2016)). Note, as
shown in the classification experiments, the same model was able to achieve the highest training
accuracy when the labels were not randomized.
	SRN-50	SRN-30	Spectral (SN)	Vanilla
WD	12.02 ± 1.77	11.87 ± 0.57	11.13 ± 2.56	10.56 ± 2.32
w/o WD	17.71 ± 2.30	19.04 ± 4.53	17.22 ± 1.94	13.49 ± 1.93
Table 1: Highly non-generalizable setting. Training error for ResNet-110 on CIFAR100 with
randomized labels, low lr= 0.01, and with and without weight decay. (Higher is better.) The clean
test accuracy for this setting is shown in Appendix D.1.
We also look specifically at highly non-generalizable settings — low learning rate and without weight
decay. As shown in Table 1, SRN consistently achieves lower generalization error (by achieving a
low train error) both in the presence and the absence of weight decay 8. Similar results are reported
for Alexnet and WideResNet in Appendix D.1.
7The training of all the models of one architecture were stopped after the same number of epochs - double
the number of epochs the model were trained on the clean dataset.
8These result are reported after 200 epochs. It can be looked on as combined with early stopping, which is a
powerful way of avoiding memorizing random labels (Li et al.).
8
Published as a conference paper at ICLR 2020
(a) Varying stable rank constraints
Figure 5: eLhist for unconditional GAN on CIFAR10. Dashed vertical lines represent 95th percentile.
Solid circles and crosses represent the inception score for each histogram. Figure 5a shows SRN-GAN
for different stable rank constraints (e.g. 90 implies c = 0.9). Figure 5b compares various approaches.
Random-GAN represents random initialization (no training). For SRN-GAN, we use c = 0.7.
(b) Comparison against different approaches
Empirical Evaluation of Generalization Behaviour When all the factors in training (eg. ar-
chitecture, dataset, optimizer, among other) as in SRN vs SN vs Vanilla, are fixed, and the only
variability is in the normalization, the generalization error can be written as |Train Err - Test Err| ≤
O (,Calg/m
where O (∙) ignores the logarithmic terms, m is the number of samples in the dataset,
and Calg denotes a measure of sample complexity for a given algorithm. Lower the value of Calg , the
better is the generalization. Before we give various expressions for Calg , we first define a common
quantity in all these expressions, called the margin γ = fθ (x) [y] - maxj 6=y fθ (x) [j]. It measures
the gap between the output of the network on the correct label and the other labels. Now we define
three recently proposed sample complexity measures useful to quantify the generalization behaviour
with further descriptions in Appendix D.1:
• Spec-Fro: QL=I IIWi 112 PL=I Srank(Wi)∕γ2 ( yshabur et a] , 201 ).
• Spec-L1: QiL=1 kWik22 PiL=1
∣Wi∣/3! /γ2 (BartlettetaL,2017), ∣∣.∣∣2,ι
is the matrix 2-1 norm.
• Jac-Norm: PL=I kh∣∣2 kJi∣∣2∕γ (W	i & Ma), where hi is the ith hidden layer and Ji =第
Histogram of the Empirical Lipschitz Constant (eLhist) We evaluate above mentioned sample
complexity measures on 10, 000 points from the dataset and plot the distribution of the log using
a histogram shown in Figure 4. The more to the left the histogram, the better is the generalization
capacity of the network.
For better clarity, we provide the 90 percentile for each of these histograms in Table 5 in Appendix D.1.
As the plots and the table show, both SRN and SN produces a much smaller quantity than a Vanilla
network and in 7 out of the 9 cases, SRN is better than SN. The difference between SRN and SN
is much more significant in the case of Jac-Norm. As this depend on the empirical lipschitzness, it
provides the empirical validation of our arguments in Section 3.
Above experiments indicate that SRN, while providing enough capacity for the standard classification
task, is remarkably less prone to memorization and provides improved generalization.
5.3 Training of Generative Adversarial Networks (SRN-GAN)
In GANs, there is a natural tension between the capacity and the generalizability of the discriminator.
The capacity ensures that if the generated distribution and the data distribution are different, the
discriminator has the ability to distinguish them. At the same time, the discriminator has to be
generalizable, implying, the class of hypothesis should be small enough to ensure that it is not just
memorizing the dataset. Based on these arguments, we use SRN in the discriminator of GAN which
we call SRN-GAN, and compare it against SN-GAN, WGAN-GP, and orthonormal regularization
based GAN (Ortho-GAN).
Along with providing results using evaluation metrics such as Inception score (IS) (Salimans et al.,
2016) , FID (Heusel et al., 2017), and Neural divergence score (ND) (Gulrajani et al., 2019),
we use histograms of the empirical Lipschitz constant, refered to as eLhist from now onwards,
1Results are taken from Miyato et al. (2018). The rest of the results in the tables are generated by us.
9
Published as a conference paper at ICLR 2020
for the purpose of analyses. For a given trained GAN (unconditional), we create 2, 000 pairs of
samples, where each pair (xi, xj) consists of xi (randomly sampled from the ‘real’ dataset) and
xj (randomly sampled from the generator). Each pair is then passed through the discriminator to
compute kf(xi)-f(xj)k2/kxi-xjk2, which we then use to create the histogram. In the conditional
setting, we sample a class from a discrete uniform distribution, and then follow the same approach as
described for the unconditional setting.
Effect of Stable Rank on eLhist and
Inception Score As shown in Fig-
ure 5a, lowering the value of c (ag-
gressive reduction in the stable rank)
moves the histogram towards zero, im-
plying, lower empirical Lipschitz con-
stant. This validates our arguments
provided in Section 3. Lowering c
also improves inception score, how-
ever, extreme reduction in the stable
rank (c = 0.1) dramatically collapses
the histogram to zero and also drops
the inception score significantly. This
is due to the fact that at c = 0.1, the
	Algorithm	Inception Score	FID	Intra-FID
	Orthonormal1	7.92 ± .04	23.8	-
	WGAN-GP	7.86 ±.07	21.7	-
	SN-GAN1	8.22 ±.04	20.67	-
	SRN-70-GAN	8.53 ± 0.04	19.83	-
	SRN-50-GAN	8.33 ± 0.06	19.57	-
	SN-GAN	8.71 ± .04	16.049	26.24
	SRN-70-GAN	8.93 ± 0.12	15.92	24.01
	SRN-50-GAN	8.76 ± 0.09	16.89	27.3
Table 2: Inception and FID score on CIFAR10.
capacity of the discriminator is reduced to the point that it is not able to learn to differentiate between
the real and the fake samples anymore.
Table 2 and 3 show that SRN-GAN consistently provide better FID score and an extremely competi-
tive inception score on CIFAR10 (both conditional and unconditional setting) and CIFAR100 (uncon-
ditional setting). In Table 4, we compare the ND loss on CIFAR10 and CelebA datasets. Note, ND
has been looked as a metric more robust to memorization than FID and IS in recent works (Gulrajani
et al., 2019; Arora & Zhang, 2017). We report our exact setting to compute ND in Appendix E.1.
We essentially report the loss incurred by a fresh classifier trained to discriminate the generator
distribution and the data distribution. Thus higher the loss, the better the generated images. As
evident, SRN-GAN provides better ND scores on both datasets. For a qualitative analysis of the
images, we compare generations in both conditional and unconditional setting in Appendix F.
Comparing different approaches In addition, in Fig-
ure 5b, we provide eLhist for comparing different ap-
proaches. Random-GAN, as expected, has a low empirical
Lipschitz constant and extremely poor inception score. Un-
surprisingly, WGAN-GP has a lower Le than Random-GAN,
due to its explicit constraint on the Lipschitz constant, while
providing a higher inception score. On the other hand, SRN-
GAN, by virtue of its softer constraints on the Lipschitz
constant, trades off a higher Lipschitz constant for a bet-
ter inception score—highlighting the flexibility provided by
SRN. Additional experiments in Appendix E.2 show more
detailed behaviour of GANs in regards to empirical lipschitz
in a variety of settings.
6 Conclusion
Model IS FID
SN-GAN 9.04	23.2
SRN-GAN (Our) 8.85	19.55
Table 3:	CIFAR100 experiments.
Model CIFAR10	CelebA
SN-GAN 10.69	0.36
SRN-GAN (Our) 11.97	0.64
Table 4:	Neural Discriminator
Loss (Higher the better).
We proposed a new normalization (SRN) that allows us to constrain the stable rank of each affine
layer of a NN, which in turn learns a mapping with low empirical Lipschitz constant. We also provide
optimality guarantees of SRN. On a variety of neural network architectures, we showed that SRN
improves the generalization and memorization properties of a standard classifier. In addition, we
show that SRN improves the training of GANs and provide better inception, FID, and ND scores.
7 Acknowledgements
The authors would like to thank Leonard Berrada and Pawan Kumar for helpful discussions. AS
acknowledges support from The Alan Turing Institute under the Turing Doctoral Studentship grant
TU/C/000023. PHS and PD are supported by the ERC grant ERC-2012-AdG 321162-HELIOS,
EPSRC grant Seebibyte EP/M013774/1 and EPSRC/MURI grant EP/N019474/1. PHS and PD also
acknowledges the Royal Academy of Engineering and FiveAI.
10
Published as a conference paper at ICLR 2020
References
Martin Anthony and Peter L Bartlett. Neural network learning: Theoretical foundations. cambridge
university press, 2009.
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein gan. 2017.
Sanjeev Arora and Yi Zhang. Do gans actually learn the distribution? an empirical study. arXiv
preprint arXiv:1706.08224, 2017.
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for
deep nets via a compression approach. In Jennifer Dy and Andreas Krause (eds.), Proceedings of
the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine
Learning Research, pp. 254-263, Stockholmsmassan, Stockholm Sweden, 10-15 Jul 2018. PMLR.
URL http://proceedings.mlr.press/v80/arora18b.html.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Advances in Neural Information Processing Systems, pp. 6240-6249, 2017.
Andrew Brock, Theodore Lim, J. M. Ritchie, and Nick Weston. Neural photo editing with introspec-
tive adversarial networks. International Conference on Learning Representations, 2016.
Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier. Parseval
networks: Improving robustness to adversarial examples. In Doina Precup and Yee Whye Teh (eds.),
Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings
of Machine Learning Research, pp. 854-863, International Convention Centre, Sydney, Australia,
06-11 Aug 2017. PMLR. URL http://proceedings.mlr.press/v70/cisse17a.html.
Vincent Dumoulin, Jonathon Shlens, and Manjunath Kudlur. A learned representation for artistic
style. Proc. of ICLR, 2017.
Carl Eckart and Gale Young. The approximation of one matrix by another of lower rank. Psy-
chometrika, 1(3):211-218, sep 1936. ISSN 0033-3123. doi: 10.1007/BF02288367. URL
http://link.springer.com/10.1007/BF02288367.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural informa-
tion processing systems, pp. 2672-2680, 2014.
Henry Gouk, Eibe Frank, Bernhard Pfahringer, and Michael Cree. Regularisation of neural networks
by enforcing lipschitz continuity. arXiv preprint arXiv:1804.04368, 2018.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Improved
training of wasserstein gans. 2017.
Ishaan Gulrajani, Colin Raffel, and Luke Metz. Towards GAN benchmarks which require gen-
eralization. In International Conference on Learning Representations, 2019. URL https:
//openreview.net/forum?id=HkxKH2AcFm.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image
Recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
pp. 770-778. IEEE, jun 2016. ISBN 978-1-4673-8851-1. doi: 10.1109/CVPR.2016.90. URL
http://ieeexplore.ieee.org/document/7780459/.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans
trained by a two time-scale update rule converge to a local nash equilibrium. In Advances in Neural
Information Processing Systems, pp. 6626-6637, 2017.
Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected
convolutional networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR). IEEE, jul 2017. doi: 10.1109/cvpr.2017.243.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
11
Published as a conference paper at ICLR 2020
Naveen Kodali, James Hays, Jacob Abernethy, and Zsolt Kira. On convergence and stability of
GANs, 2018. URL https://openreview.net/forum?id=ryepFJbA-.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.
Technical report, Citeseer, 2009.
Mingchen Li, Mahdi Soltanolkotabi, and Samet Oymak. Gradient descent with early stopping is
provably robust to label noise for overparameterized neural networks.
Jae Hyun Lim and Jong Chul Ye. Geometric gan. 2017.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In
Proceedings of International Conference on Computer Vision (ICCV), 2015.
Leon Mirsky. Symmetric gauge functions and unitarily invariant norms. The quarterly journal of
mathematics ,11(1):50-59,1960.
Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint
arXiv:1411.1784, 2014.
R. V. Mises and H. Pollaczek-Geiringer. Praktische verfahren der gleichungsauflosung . ZAMM -
ZeitsChriftfUrAngewandteMathematikundMeChanik, 9(2):152-164, 1929. doi: 10.1002/zamm.
19290090206.
Takeru Miyato and Masanori Koyama. cgans with projection discriminator. International ConferenCe
on learning Representations, 2018.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for
generative adversarial networks. In International ConferenCe on Learning Representations, 2018.
URL https://openreview.net/forum?id=B1QRgziT-.
Vaishnavh Nagarajan and Zico Kolter. Deterministic PAC-bayesian generalization bounds for deep
networks via generalizing noise-resilience. In International ConferenCe on Learning Representa-
tions, 2019. URL https://openreview.net/forum?id=Hygn2o0qKX.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural
networks. In ConferenCe on Learning Theory, pp. 1376-1401, 2015.
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A PAC-bayesian approach to
spectrally-normalized margin bounds for neural networks. In International ConferenCe on Learning
Representations, 2018. URL https://openreview.net/forum?id=Skz_WfbCZ.
Roman Novak, Yasaman Bahri, Daniel A Abolafia, Jeffrey Pennington, and Jascha Sohl-
Dickstein. Sensitivity and generalization in neural networks: an empirical study. arXiv preprint
arXiv:1802.08760, 2018.
Henning Petzka, Asja Fischer, and Denis Lukovnikov. On the regularization of wasserstein GANs.
In International ConferenCe on Learning Representations, 2018. URL https://openreview.
net/forum?id=B1hYRMbCW.
Prajit Ramachandran, Barret Zoph, and Quoc V. Le. Searching for activation functions, 2018. URL
https://openreview.net/forum?id=SkBYYyZRZ.
Mark Rudelson and Roman Vershynin. Sampling from large matrices. Journal of the ACM, 54(4):
21-es, jul 2007. doi: 10.1145/1255443.1255449.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans. In AdvanCes in Neural Information ProCessing Systems, pp.
2234-2242, 2016.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Dustin Tran, Rajesh Ranganath, and David M. Blei. Hierarchical implicit models and likelihood-free
variational inference. 2017.
12
Published as a conference paper at ICLR 2020
Colin Wei and Tengyu Ma. Data-dependent sample complexity of deep neural networks via lipschitz
augmentation.
Helmut Wielandt. An extremum property of sums of eigenvalues. Proceedings of the American
Mathematical Society,6(1):106-106,jan 1955. doi: 10.1090∕s0002-9939-1955-0067842-9.
Yuichi Yoshida and Takeru Miyato. Spectral norm regularization for improving the generalizability
of deep learning. arXiv preprint arXiv:1705.10941, 2017., 2017.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In Procedings of the British
Machine Vision Conference 2016. British Machine Vision Association, 2016. doi: 10.5244/c.30.87.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep
learning requires rethinking generalization. International Conference on Learning Representations
(ICLR), nov 2016. URL http://arxiv.org/abs/1611.03530.
Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena. Self-attention generative
adversarial networks. 2018.
13
Published as a conference paper at ICLR 2020
A Technical Proofs
Here we provide an extensive proof of Theorem 1 (Appendix A.1). We also provide the optimal
solution to the spectral norm problem in Appendix A.2. Auxiliary lemmas on which our proof
depends are provided in Appendix A.3.
A. 1 Proof for Optimal Stable Rank Normalization. (Main Theorem)
Theorem 1. Given a real matrix W ∈ Rm×n with rank p, a target spectrum (or singular value)
preservation index k (0 ≤ k < p), and a target stable rank of r (1 ≤ r < srank(W)), the
optimal solution Wc k to problem (4) is Wc k = γ1S1 + γ2 S2, where S1 = Pim=a1x(1,k) σiuivi> and
S2 = W - S1. {σi}ik=1, {ui}ik=1 and {vi}ik=1 are the top k singular values and vectors of W, and,
depending on k, γ1 and γ2 are defined below. For simplicity, we first define γ
好着PkF, then
kS2 kF
a)	If k = 0 (no spectrum preservation), the problem becomes non-convex, the optimal solution
to which is obtainedfor γ2 = Y——r------and γι = γ2, when r > 1. If r = 1, then γ2 = 0
r
and γ1 = 1. Since, in this case, kS1 k2F = σ12, γ
kS2kF
kS k2
b)	If k ≥ 1, the problem IS convex. If r ≥ ；2 F the optimal solution IS obtainedfor γι = 1,
and γ2 = γ and if not, the problem is not feasible.
c) Also,
- W monotonically increases with k for k ≥ 1.
Proof. Here we provide the proof of Theorem 1 (in the main paper) for all the three cases with
optimality and uniqueness guarantees. Let Wc k be the optimal solution to the problem for any of
>
the two cases. From Lemma 5, the SVD of W and Wk can be written as W = UΣV> and
Wck = UΛV>, respectively. Then, L = W -= hΣ - Λ, Σ - ΛiF . From now onwards,
we denote Σ and Λ as vectors consisting of the diagonal entries, and h., .i as the vector inner product
10
Proof for Case (a): In this case, there is no constraint enforced to preserve any of the singular values
of the given matrix while obtaining the new one. The only constraint is that the new matrix should
have the stable rank of r. Let Us assume Σ = (σι, .一，σp), ∑2 = (σ2,…,σp), A = (λι, ∙∙∙ , λp)
and A2 = (λ2, ∙∙∙ ,λp). Using these notations, We can write L as:
L=hΣ,Σi+hA,Ai-2hΣ,Ai
= hΣ, Σi + λ21 + hA2, A2i - 2σ1λ1 - 2 hΣ2, A2i	(6)
Pjp=2 λj2
Using the stable rank constraint Srank(Wk) = r, which is r = 1+-飞--.
λ1
Case for r > 1 If r > 1 we obtain the following equality constraint, making the problem non-
convex.
λ2 = ^Ai	⑺
r-1
However, we will show that the solution we obtain is optimal and unique. Substituting (7) into (6) we
get
L = h∑, ∑ + ^AiAL + hA2, A2 - 2σι∕也2' AA) - 2 h∑2, A/	(8)
r-1	r-1
10h., .iF represents the Frobenius inner product of two matrices, which in the case of diagonal matrices is the
same as the inner product of the diagonal vectors.
14
Published as a conference paper at ICLR 2020
∂L
Setting 用一
∂Λ2
0 to get the family of critical points
含十丛2 - 2√Jσi1)∖,A2
- 2Σ2 = 0
∑2 =A2	+1------/	1	=
T - 1	1√(r - 1)hA2, A2i)
∑21i] = (，+1- L 。1	! =1
λ2 [i]	T - 1	1√(r - 1) hA2, A2i	Y2
∀1 ≤i≤p
(9)
(10)
As the R.H.S. of 10 is independant of i, the above equality implies that all the critical points of (8)
are a scalar multiple of Σ2, implying, A2 = γ2Σ2. Note that the domain of A2 are all strictly positive
vectors and thus, we can ignore the critical point at A2 = 0. Substituting this into (9) we obtain
∑2 = Y2∑2 I---------+ 1-------1	=)
r - 1	γ2√(r -1)h∑2, ∑2i )
Using the fact that hΣ2, Σ2i = kS2 k2F in the above equality and with some algebraic manipulations,
We obtain γ2 = γ+r-1 where, Y = √√S-kσ1. Note, r ≥ 1, Y ≥ 0, and Σ ≥ 0, implying, A2 =
γ2 Σ2 ≥ 0.
Local minima: Now, we will show that A2 is indeed a minima of (8). To show this, we compute
the hessian of L. Recall that
∂L
∂A
2r A	2σ1Λ
r - 1	√(r - 1) kΛk2
- 2Σ2
∂2l
∂2Λ
2r-1 -	,	2σ1——J fkΛk21 - 1∏-ΛΛ>)
r - 1	√F-^ kΛk2	2 Mk2
r___σ1 Mk2 II +	2σ1
rɪɪ 一 √FτWl	√r-1 kΛk3
Now we need to show that H at the solution Λ2 is PSD i.e. ∀x ∈ Rp-1, x>H(Λ2)x ≥ 0
x>Hx=2 (rɪɪ- √F=km⅛
x22+
2σ1
,	----qX
√- kΛ∣∣2
(ΛΛ>) X
H
(a)	r
≥ 2	r-1
σ1 Mk2	ʌ kχk2
√F-υ 川2	k k2
—
(b)	r
≥ 2 --------
_ r- -1
(d)	2r
≥ r - 1 (1
σ1
(r-1)λ1
1
(1 + r - 1)
kχk2(=) 之(1- ； )四2
kXk22 = 2 kXk22 ≥0
—
—
Here (a) is due to the fact that the matrix ΛΛ> is an outer product matrix and is hence PSD. (b)
follows due to (7) and (c) follows by substituting λ1 = Y1 σ1 and then the value of Y1. Finally (d)
follows as ( 1 - ---Y------ ) is decreasing with respect to Y and we know that γ < 1 due to the
(Y+r-1)
assumption that srank(W) < r. Thus, we can substitute Y = 1 to find the minimum value of the
expression. This concludes our proof that Λ2 is indeed a local minima of L.
Uniqueness: The uniqueness of Λ2 as a solution to (8) is shown in Lemma 6 and is also guaranteed
by the fact that Y2 has a unique value. Using Λ2 = Y2Σ2 and λ1 = Y1 σ1 in (7), we obtain a unique
solution γ1 = γ2.
15
Published as a conference paper at ICLR 2020
Now, we need to show that it is also an unique solution to Theorem 1.
For all solutions to Theorem 1 that have singular vectors which are different than that of W,
by Lemma 5, the matrix formed by replacing the singular vectors of the solution with that of W is
also a solution. Thus, if there were a solution with different singular values than Wck , it should have
appeared as a solution to (8). However, we have shown that (8) has a unique solution.
Now, we need to show that among all matrices with the same singular values as that of Wk, Wk
is strictly better in terms of W - Wck . This requires a further assumption that every non-zero
singular value of Λ2 has a multiplicity of 1 i.e. they are all distinctly unique. Intuitively, this doesn’t
allow to create a different matrix by simply interchanging the singular vectors associated with the
equal singular values. As the elements of Σ2 are distinct, the elements of Λ2 = γ2Σ2 are also distinct
and thus by the second part of Lemma 5, Wck is strictly better, in terms of W -, than all
matrices which have the same singular values as that of Wdk. This concludes our discussion on the
uniqueness of the solution.
Case for r = 1: Substituting r = 1 in the constraint r = 1 + —j=2T j Weget
As it is a sum of squares, each of the individual elements is also zero i.e. λj = 0 ∀2 ≤ j ≤ p.
Substituting this into (6), We get the folloWing quadratic equation in λ1
L = hΣ, Σi +λ12 -2σ1λ1	(11)
Which is minimized at λ1 = σ1, thus proving that γ1 = 1 and γ2 = 0.
Proof for Case (b): In this case, the constraints are meant to preserve the top k singular values of the
given matrix while obtaining the new one. Let ∑ι = (σι, ∙∙∙ , σk), ∑2 = (σk+ι,…,σp), Λι =
(λι,…，λk), Λ2 = (λk+ι,…，λp). Since satisfying all the constraints imply ∑ι = Λι, thus,
L := W -	Wck	= hΣ2	-	Λ2, Σ2	- Λ2i.	From the stable rank constraint srank(Wc k)	=	r,	we
have
hΛ1,Λ1i+hΛ2,Λ2i
r =	λ2
.∙. hΛ2, Λ2i = rλ2 -hΛ1, Λιi = rσ2 -{∑ι, ∑ιi	(12)
The above equality constraint makes the problem non-convex. Thus, we relax itto srank(Wk) ≤ r to
make it a convex problem and show that the optimality is achieved with equality. Let rσ12-hΣ1, Σ1i =
η. Then, the relaxed problem can be written as
min L := hΣ2 - Λ2, Σ2 - Λ2i
Λ2∈Rp-k
s.t. Λ2 ≥0,hΛ2,Λ2i ≤η.
We introduce the Lagrangian dual variables Γ ∈ Rp-k and μ corresponding to the positivity and the
stable rank constraints, respectively. The Lagrangian can then be written as
L (A2, r,μ)Γ≥O,μ≥0 = h∑2 - λ2, ς2 -卜。+ μ (〈-2, λ2i - η) -hr, λ2)
∂L
Using the primal optimality condition —— = 0, we obtain
∂Λ2
2A2 - 2∑2 + 2μA2 - Γ = O
Γ + 2∑2
2(1+ μ)
(13)
(14)
16
Published as a conference paper at ICLR 2020
Using the above condition on Λ2 with the constraint hΛ2 , Λ2i ≤ η, combined with the stable rank
constraint of the given matrix W that comes with the problem definition, srank(W) > r (which
implies hΣ2, Σ2i > η), the following inequality must be satisfied for any Γ ≥ 0
1 < h∑2,∑2i ≤hΓ + ∑2,Γ + ∑2i ≤ (1 + μ)2
(15)
For the above inequality to satisfy, the dual variable μ must be greater than zero, implying,hA2, Λ2)-η
must be zero for the complementary slackness to satisfy. Using this with the optimality condition (14)
we obtain
(1 + μ)2
hΓ+ 2∑2,Γ+2∑2i
4η
Substituting the above solution back into the primal optimality condition we get
A2 = (Γ + 2∑2)
η
p∕hr + 2∑2, γ + 2∑2i
(16)
η
η
Finally, we use the complimentary slackness condition Γ	A2 = 011 to get rid of the dual variable Γ
as follows
Γ Θ (Γ + 2∑2) /,	Vzn -------r = 0
√(Γ + 2∑2, Γ + 2∑2i
It is easy to see that the above condition is satisfied only when Γ = 0 as ∑2 ≥ 0 and η > 0. Therefore,
using Γ = 0 in (16) we obtain the optimal solution of A2 as
ψσ- - kSιkF	_
Fl ∑2 = γ∑2
(17)
Proof for Case (c): The monotonicity of Wk - W for k ≥ 1 is shown in Lemma 3.
□
Note that by the assumption that srank(W) < r, we can say that γ < 1. Therefore in all the cases
γ2 < 1. Let us look at the required conditions for γ1 ≥ 1 to hold. When k ≥ 1, γ1 = 1 holds. When
k = 0, for γ1 > 1 to be true, γ2 < γ should hold, implying, (γ - 1) < r(γ - 1), which is always
true as r> 1 (by the definition of stable rank).
Lemma 3. For k ≥ 1, the solution to the optimization problem (4) obtained using Theorem 1 is
closest to the original matrix W in terms of Frobenius norm when only the spectral norm is preserved,
implying, k = 1.
Proof. For a given matrix W and a partitioning index k ∈ {1, ∙ ∙ ∙ ,p},let Wk = Sk + YSk be the
matrix obtained using Theorem 1. We use the superscript k along with S1 and S2 to denote that this
refers to the particular solution of Wck. Plugging the value of γ and using the fact that S2k F 6= 0,
we can write
IW -WIF=——
=网F- Jrσ2 -IISkIIF
=IISkHF- Jrσ2 -IWkF + IISkHF.
Thus, (w — W,I can be written in a simplified form as f (x) = X — √a + x2, where X = IISk。
and a = rσ12 - kWk2F. Note, a ≤ 0 as 1 ≤ r≤ srank(W), and a+x2 ≥ 0 because of the condition
in Theorem 1. Under these settings, it is trivial to verify that f is a monotonically decreasing function
of X. Using the fact that as the partition index k increases, X decreases, it is straightforward to
conclude that the minimum of f (x) is obtained at k = 1.	□
11 is the hadamard product
17
Published as a conference paper at ICLR 2020
A.2 Proof for Optimal Spectral Normalization
The widely used spectral normalization (Miyato et al., 2018) where the given matrix W ∈ Rm×n is
divided by the maximum singular value is an approximation to the optimal solution of the spectral
normalization problem defined as
arg min W -Wc
2
(18)
F
∕G∖ ,
s.t. σ(W) ≤ s,
where σ(W) denotes the maximum singular value and s > 0 is a hyperparameter. The optimal
solution to this problem is shown in Algorithm 3. In what follows we provide the optimality proof
Algorithm 3 Spectral Normalization
Require: W ∈ Rm ×n, S
1:	Wi - 0, P — min(m, n)
2:	for k ∈ {1,…，p} do
3:	{uk, Vk ,σk} J SVD(W, k)	. perform power method to get k-th singular value
4:	if σk ≥ s then
5:	W1 J W1 + s ukvk>
6:	W J W - σk ukvk>
7:	else
8:	break	. exit for loop
9:	end if
10:	end for
11:	return W J W1 + W
of Algorithm 3 for the sake of completeness. Let SVD (W) = UΣV> and let us assume that
Z = SΛT> is a solution to the problem 18. Trivially, X = UΛV> also satisfies σ (X) ≤ s. Now,
kW - Xk2F = U (Σ - Λ) V> 2F = k(Σ - Λ)k2F ≤ kW - Zk2F, where the last inequality directly
comes from Lemma 4. Thus the singular vectors of the optimal solution must be the same as that of
W. This boils down to solving the following problem
arg min kΛ - Σk2F s.t. Λ [i] ≤ s ∀i ∈ {0, min (m, n)} .
Λ∈R+min(m,n)
(19)
Here, without loss of generality, we abuse notations by considering Λ and Σ to represent the diagonal
vectors of the original diagonal matrices Λ and Σ, and Λ [i] as its i-th index. It is trivial to see that the
optimal solution with minimum Frobenius norm is achieved when
Σ [i] , if Σ [i] ≤ s
s,	otherwise.
This is exactly what Algorithm 3 implements.
Λ[i]=
A.3 Auxiliary Lemmas
Lemma 4. [Reproduced from Theorem 5 in Mirsky (1960)] For any two matrices A, B ∈ Rm×n
with singular values as σι ≥ •一≥ σn and pi ≥ ∙∙∙ ≥ Pn, respectively
n
kA-Bk2F≥X(σi-pi)2
i=i
Proof. Consider the following symmetric matrices
0
A>
,Y
0
B>
,Z
0	A-B
(A-B)>	0
A
0
B
0
X
18
Published as a conference paper at ICLR 2020
Let τι ≥ ∙∙∙ ≥ Tn be the singular values of Z. Then the set of characteristic roots of X, Y
and Z in descending order are {ρι,…，Pn, -Pn,…，-pι}, {σι,…，σn, —Qn …，-σι}, and
{τι,…,Tn, —Tn,…,τι}, respectively. By Lemma 2 in Wielandt (1955)
[σ1 - P1,…，σn - Pn,Pn - σn, ∙ ∙ ∙ ,Pl - σ1] W [τ1, ' ' ' τn, -τn, -τ1] ,
which implies that
nn
X(σi-Pi)2 ≤Xτi2 = kA - Bk2F	(20)
i=1	i=1
□
Lemma 5. Let A, B ∈ Rm×n where SVD(A) = UΣV> and B is the solution to the following
problem
B = arg min kW - Ak2F .	(21)
srank(W)=r
Then, SVD (B) = UΛV> where Λ is a diagonal matrix with non-negative entries. Implying, A and
B will have the same singular vectors.
Proof. Let us assume that Z = SΛT> is a solution to the problem 21 where S 6= U and T 6= V.
Trivially, X = UΛV> also lies in the feasible set as it satisfies srank(X) = r (note stable rank
only depends on the singular values). Using the fact that the Frobenius norm is invariant to unitary
transformations, we can write kA - Xk2F = U (Σ - Λ) V> 2F = k(Σ - Λ)k2F. Combining this
with Lemma 4, we obtain kA - Xk2F = k(Σ - Λ)k2F ≤ kA - Zk2F. Since, S 6= U and T 6= V, we
can further change ≤ to a strict inequality <. This completes the proof.
Generally speaking, the optimal solution to problem 21 with constraints depending only on the
singular values (e.g. stable rank in this case) will have the same singular vectors as that of the original
matrix.
Further the inequality in (20) can be converted into a strict inequality if neither of A and B have
repeated singular values. Using that strict inequality, if both Σ and Λ have no repeated values, then B
is the only solution to (21) that has the singular values of Λ.
□
Lemma 6. Let yι = axι + b^ι and y = ax2 + b^2, where ^ι and X2 denotes the unit vectors.
Then, y1 = y2 if x1 = x2.
B Empirical Lipschitz constant
B.1	Relating empirical local and global Lipschitz constants
Proposition B.1. Let f : Rm → R be a Frechet differentiable function, D the dataset, and
Conv (xi, xj) denotes the convex combination of a pair of samples xi and xj, then ∀p, q ∈ [1, ∞]
such that P + 1 = 1
max
xi,xj ∈D
If(Xi) - f (Xj)I
kxi - xj Ilp
max
xi ,xj ∈D
x∈Conv (xi ,xj )
IJf(X)Iq
≤
19
Published as a conference paper at ICLR 2020
Proof. Let f : Rm → R be a differentiable function on an open set containing xi and xj such that
xi 6= xj . By applying fundamental theorem of calculus
|f (Xi) - f (xj)| = ∣/ Vf (Xi + θ (Xj- Xi))> (xj - Xi) ∂θ
≤	Vf (Xi + θ (Xj - Xi))> (Xj - Xi) ∂θ
(a)	1
≤	kVf(Xi+θ(Xj - Xi))kq k(Xj - Xi)kp ∂θ
≤ Z max kVf (Xi +θ(Xj -Xi))kq k(Xj -Xi)kp∂θ
= max kVf (Xi +θ(Xj -Xi))kq k(Xj - Xi)kp Z ∂θ
O If((Xi)- f (Xj )l ≤ ©max。Vf(Xi + θ (Xj- Xi))kq = “maX	JVf(X)kq.
k(Xj - Xi)kp	θ∈(0,1)	x∈Conv (xi,xj)
The inequality (a) is due to Holder,s inequality.
□
B.2	Effect of Rank on the Empirical Lipschitz Constants
Let f(X) = W2W1X be a two-layer linear NN with weights W1 and W2. The Jacobian in this case
is independent of X. ThUs, the local Lipschitz constant is the same for all X ∈ Rm, implying, local
Le = Ll(X) = Ll = kW2W1 k ≤ kW2k kW1 k. Note, in the case of 2-matrix norm redUcing the
rank will not affect the UpperboUnd. HoweVer, as will be discUssed below, rank redUction greatly
inflUences the global Le .
Let Xi and Xj be random pairs from D and ∆X 6= 0 be the difference Xi - Xj , then, the global Le
is max{χi,χj}∈D kWW^x. Let kι and k2 be the ranks, and σι ≥ ∙∙∙ ≥ σkl and λι ≥ ∙∙∙ ≥ λk2
the singular values of the matrices W1 and W2, respectively. Let Pi = UiU> be the orthogonal
projection matrix corresponding to Ui and Ui, the left and the right singular vectors of Wι. Similarly,
We define Qi for W2 corresponding to Vi and Vi. Then, W2W1 = Pk= 1 P：=i 入iσjQiPj. The
upperbound, λ1σ1, can be achieved if and only if ∆x = Ui ∣∣∆x∣∣ and uι = Vi (a perfect alignment),
Which is highly unlikely. In practice, not just the maximum singular values, as is the case With the
Lipschitz upper-bound, rather the combination of the projection matrices and the singular values play
a crucial role in providing an estimate of global Le . Thus, reducing the singular values, Which is
equivalent to minimizing the rank (or stable rank), Will directly affect Le . For example, assigning
σj = 0, Which in effect Will reduce the rank of W1 by one, Will nullify its influence on all projections
associated With Pj. Implying, all the k2 projections σj (Pik=2 1 λiQi)Pj that Would propagate the
input via Pj Will be blocked. This, in effect, Will influence kW2W2∆Xk; hence the global Le. In
a more general setting, let ki be the rank of the i-th linear layer, then, each singular value of a j -th
layer can influence the maximum of Qij=-11 ki Qli=j+1 ki many paths through Which an input can
be propagated. Thus, mappings With loW rank (stable) Will greatly reduce the global Le . Similar
arguments can be draWn for local Le in the case of NN With non-linearity.
C The local Lipschitz upper-bound for Neural Networks
As mentioned in Section 2, Ll(x) = kJf (x)kp,q, where, in the case of NN, the Jacobian is:
Jf (x)
∂f (x)	∂z1 ∂φ1(z1)	∂zl ∂φl(zl)
∂x
∂x	∂z1
∂al-1	∂zl .
(22)
Using ?1 = Wl (affine transformation), and applying SUbmUltiPlicatiVity of the matrix norms:
∂ al-1
kJf(x)kp,q≤kW1kp,q
∂φ1(z1)
∂zι
…kWl kp,q
∂φι (Zl)
∂zι
(23)
20
Published as a conference paper at ICLR 2020
Note, most commonly used activation functions φ(.) such as ReLU, sigmoid, tanh and maxout are
known to have Lipschitz constant of 1 (if scaled appropriately)12, thus, the upper bound can further
be written only using the operator norms of the intermediate matrices as
Ll(X) ≤ k Jf (x)kp,q ≤ IWl kp,q …kWlkp,q .	(24)
Furthermore Ll (x) can be substituted by Ll, the local Lipschitz constant, as the upper
bound (Eq. (24)) is independent of x. Note that this is one of the main reasons why we con-
sider the empirical Lipschitz to better reflect the true behaviour of the function as the NN is never
exposed to the entire domain Rm but only a small subset dependant on the data distribution.
The other reason why this upper bound is a bad estimate is that the inequality in Eq (23) is tight only
when the partial derivatives are aligned, implying」∣ 1dz' dZ+1 Il = Il ndz' Il Il dZ+1 Il ∀l -2 ≤
,	‘U dz'-ι dz' 12 Il dz'-ι l∣2 Il dz' l∣2	一
` ≤ l. This problem has been referred to as the problem of mis-alignment and is similar to quantities
like layer cushion in Arora et al. (2018).
D	Experimental details
WideResNet-28-10 We use a standard WideResNet with 28 layers and a growth factor of 10. In total,
the network has 36,539,124 trainable parameters. The network is the standard configuration with
batchnorm and ReLU activations and is trained with a weight decay of 1e - 4. The learning rate was
multiplied by 0.2 after 60, 120, and 160 epochs respectively.
ResNet-110 The ResNet-110 is a standard 110 layered ResNet with batch Norm and ReLU and has
1, 973, 236 parameters. The network is trained with SGD, an initial learning rate of 0.1, which is
multiplied 0.1 after 150 and 250 epochs respectively, a weight decay of 5e - 4 and a momentum of
0.9.
Densenet-100 The DenseNet-100 is a standard 100-layered densenet with Batchnorm and ReLU and
has a total of 800, 032 trainable parameters. The network is trained with SGD, an initial learning rate
of 0.1, which is multiplied by 0.1 after 150 and 250 epochs respectively, a weight decay of 1e - 4,
and a momentum of 0.9.
VGG19 The VGG19 model is the standard 19-layered VGG model with Batchnorm and ReLU. It
has a total of 20, 548, 392 trainable parameters and is trained with SGD with a momentum of 0.9
and a weight decay of 5e - 4. The initial learning rate is 0.1 and is multiplied by 0.1 after 150 and
250 epochs respectively. For the shattering experiments, we used the same architecture and the same
training recipe except the initial learning rate, which was deceased to 0.01 as the model failed to learn
the random labels with a large learning rate.
AlexNet The Alexnet model is the standard ALexNet model with 4, 965, 092 trainable parameters.
It was trained with SGD, with a momentum of 0.9, with an initial learning rate is 0.01, which is
multiplied by 0.1 after 150 and 250 epochs respectively. The optimizer was further augmented with a
weight decay rate of 5e - 4. Please refer to the next section for results on different learning rates and
with and without weight decay.
D.1 Additional Experiments on Generalization
Complexity measures In this section, we provide more details about the various complexity
measures we used in Figure 4.
•	Spec-Fro: Qi=IkWik2 /=Srank(Wi) (NeyShabUr et al., 2018). This bound is the
γ2
main motivation of this paper; the two quantities used to normalize the margin (γ) is the
product of spectral norm i.e. QiL=1 kWik22 (or worst case lipschitzness) and sum of stable
rank i.e., PiL=1 Srank(Wi) (or an approximate parameter count like rank of a matrix).
12implying, maxz Ildφ(Z)ll = 1.
p
21
Published as a conference paper at ICLR 2020
QL=1 kWik2 (pL=1 kWk2/3)3
•	SpeC-L1: ---------------1----------i 2	, where kJ. ι is the matrix 2-1 norm. As
γ2	2,1
showed by Bartlett et al. (2017), Spec-L1 is the spectrally normalized margin, and unlike
just the margin, is a good indicator of the generalization properties of a network.
•	Jac-Norm: PL=I khik2 kJik2 ( ei&M ), where hi is the ith hidden layer and Ji = IhY
i.e., the Jacobian of the margin with respect to the ith hidden layer (thus, a vector). Note,
Jac-Norm depends on the norm of the Jacobian (local empirical Lipschitz) and norm of the
hidden layers - additional data-dependent terms compared to Spec-Fro and Spec-L1, thus
captures a more realistic (and optimistic) generalization behaviour.
For better clarity regarding Figure 4, we provide the 90 percentile for each of these histograms in
Table 5. As the plots and the table show, both SRN and SN produces a much smaller quantity than a
Vanilla network and in 7 out of the 9 cases, SRN is better than SN. The difference between SRN and
SN is much more significant in the case of Jac-Norm. As this depend on the empirical lipschitzness,
it provides the empirical validation of our arguments in Section 3.
Model	Algorithm	Jac-Norm	Spec-L1	Spec-Fro
	Vanilla	17.7	∞	∞
ResNet-110	Spectral (SN)	17.8	10.8	7.4
	SRN-30	17.2	10.7	7.2
	Vanilla	16.2	14.60	11.18
WideResNet-28-10	Spectral (SN)	16.13	7.23	4.5
	SRN-50	15.8	7.3	4.5
	SRN-30	15.7	7.20	4.4
	Vanilla	19.2	∞	∞
Densenet-100	Spectral (SN)	17.8	12.2	9.4
	SRN-50	17.6	12	9.2
	SRN-30	17.7	11.8	9.0
Table 5: Values of 90 percentile of log complexity measures from Figure 4. Here ∞ refers to the
situations where the product of spectral norm blows up. This is the case in deep networks like
ResNet-110 and Densenet-100 where the absence of spectral normalization (Vanilla) allows the
product of spectral norm to grow arbitrarily large with increasing number of layers. Lower is better.
Alexnet experiments: Figure 6 shows the test error and generalization error of Alexnet trained
with a large learning rate of 0.1. Note that, the model fails to learn completely without weight decay.
Generalisation Error decreases monotonically with decreasing c in the stable rank constraint. Test
error is the lowest for c = 0.5. The constraint becomes too aggressive for even c lower than that. The
slightly more interesting observation is that having a weight decay actually hurts generalization error
while it has a slightly positive effect on test error.
Low Learning Rate Here, we train a WideResnet-28-10 with SRN, SN, and vanilla methods with
an lr = 0.01 and weight decay of 5 × 10-4 on randomly labelled CIFAR100. for 50 epochs. The
results are shown in Table 6 and it further supports that SRN is more robust to random noise than SN
or vanilla methods.
Stable-30	Spectral	Vanilla
.29.04	17.24	1.22	∙
Table 6: Training Error for WideResNet-28-10 on CIFAR100 with randomized labels, low lr= 0.01,
and with weight decay. (Higher is better.)
With and without weight decay In Figure 7a, we show the training error of Alexnet trained with
SGD with and without weight decay (= 5e - 4) with a learning rate of 0.01. Again, we see that a
22
Published as a conference paper at ICLR 2020
JojJ 山 UoQEZ=E jəuəo
(a) Generalization Error
Figure 6: Test Error and Generalization Error of AlexNet trained with SGD with lr = 0.1 on (clean)
CIFAR-100. (Lower is better
w∕o WD
WD
(b) Test Error
more aggressive stable rank constraint decreases fitting the random data . Similar results are seen for
ResNet-110 in Figure 7b.
Ooo
2 1
」0」」山6u⊂ωJl
(a) AlexNet	(b) ResNet110
Figure 7: Training error on randomly labelled CIFAR-100 with a learning rate of 0.01 and with/ with
out weight decay. (Higher is better.
Low Learning Rate, with and without weight decay on clean CIFAR100 In Appendix D.1, we
show the test accuracies for the clean data with the same confifuration as in Table 1. This corresponds
to the hihgly non-generelizable learning setting.
	Vanilla	Spectral	Stable-50	Stable-30
W/o WD	69.2 ± 0.5	69±0.1	69.1 ±0.85	69.3 ±0.4
With WD	70.4± 0.3	71.35 ±0.25	70.6 ±0.1	70.6 ±0.1
Table 7: Clean Test Accuracy on CIFAR10. The learning configuration corresponds to the non-
generelizable settings with high learning rate. The corresponding shattering experiments for this
setting are shown in Table 1.
Training Accuracy as Stopping Criterion In this section we show that our regularizor performs
consistently for a different stopping criterion. In particular, we use the train accuracy as a stopping
criterion. For Resnet110, WideResnet-28,Densenet-100, and VGG-19 we use a train accuracy of
99% as a stopping criterion and report the test accuracy when that train accuracy was achieved for
the first time. For Alexnet, as SRN-30 never achieves a train accuracy higher than 55%, we use 55%
23
Published as a conference paper at ICLR 2020
谢;! ⅛∕∕J
sn
(a) Resnet110 (b) WideResnet-28
76.5
⅛l l fM I
(c) Alexnet
75.0
(d) Densenet-100
(e) VGG-19
Figure 8: Test accuracies on CIFAR100 for clean data using a stopping criterion based on train
accuracy. Higher is better.
(a) Resnet110	(b) WideResnet-28
(c) Alexnet	(d) Densenet-100
Figure 9: Test accuracies on CIFAR10 for clean data using a stopping criterion based on train
accuracy. Higher is better.
as the stopping criterion and plot the test accuracies in Figure 8. Our results show that SRN-30 and
SRN-50 outperform SN and vanilla consistently. In Figure 9, we show similar plots for CIFAR10.
CIFAR10 experiments In this section, we plot results on CIFAR10 trained using ResNet-110,
Desnenet100, WideResNet-28, and Alexnet. In Figure 9, we plot the test accuracy on clean CIFAR-10
with the training accuracy as the stopping criterion. For all models other than Alexnet, we use 99%
training accuracy as the criterion and for Alexnet we use 85%. In Figure 10, we plot the test accuracy
on clean CIFAR10 using the number of epochs as the stopping criterion.The results here are consistent
with those in the main paper in that SRN outperforms the vanilla and SN.
In Figure 11, we plot the training accuracy on CIFAR10 when the labels are randomized for Resnet100,
and Alexnet. SRN-50 and SRN-30 are much better than Vanilla and SN in this case.
Figure 11: Training accuracy on randomly labelled CIFAR-10 (Lower is better).
E Additional Experiments on GANs
E.1 GAN experimental setup
Datasets and Network Architectures Each of the CIFAR datasets contain a total of 50, 000 RGB
images in the training set, where each image is of size 32 × 32, and a further 10, 000 RGB images of
the same dimension in the test set. The CelebA dataset contains more than 200K images scaled to a
size of 64 × 64. The model architecture for both the generator and the discriminator was chosen to be
a 32 layered ResNet (He et al., 2016) due to its previous superior performance in other works (Miyato
et al., 2018). We use Adam optimizer (Kingma & Ba, 2014) which depends on three main hyper-
parameters α- the initial learning rate, β1 - the first order moment decay rate and β2- the second
24
Published as a conference paper at ICLR 2020
Figure 10: Test accuracies on CIFAR10 for clean data using the number of epochs as a stopping
criterion. Higher is better.
order moment decay rate. We cross-validate these parameters in the set α ∈ {0.0002, 0.0005}, β1 ∈
{0, 0.5}, β2 ∈ {0.9, 0.999} and chose α = 0.0002, β1 = 0.0 and β2 = 0.999 which performed
consistently well in all of the experiments.
GAN objective functions In the case of conditional GANs (Mirza & Osindero, 2014), we used the
conditional batch normalization (Dumoulin et al., 2017) to condition the generator and the projection
discriminator (Miyato & Koyama, 2018) to condition the discriminator. The dimension of the latent
variable for the generator was set to 128 and was sampled from a zero mean and unit variance
Gaussian distribution. For training the model, we used the hinge loss version of the adversarial
loss (Lim & Ye, 2017; Tran et al., 2017) in all experiments except the experiments with WGAN-GP.
The hinge loss version was chosen as it has been shown to give consistently better performance in
previous works (Zhang et al., 2018; Miyato et al., 2018). For training the WGAN-GP model, we used
the original loss function as described in Gulrajani et al. (2017).
Evaluation Metrics We use Inception (Salimans et al., 2016) and Frechet Inception Dis-
tance (FID) (Heusel et al., 2017) scores for the evaluation of the generated samples. For measuring
the inception score, we generate 50, 000 samples, as was recommended in Salimans et al. (2016). For
measuring FID, we use the same setting as Miyato et al. (2018) where we sample 10, 000 data points
from the training set and compare its statistics with that of 5, 000 generated samples. In addition,
we use a recent evaluation metric called Neural divergence score Gulrajani et al. (2019) which is
more robust to memorization. The exact set-up for the same is discussed below. In the case of
conditional image generation, we also measure Intra-FID (Miyato et al., 2018), which is the mean
of the FID of the generator, when it is conditioned over different classes. Let FID(G, c) be the FID
of the generator G when it is conditioned on the class c ∈ C (where C is the set of classes), then,
IntraFID(G) = IC FID(G,c)
Neural Divergence Setup We train a new classifier inline with the architecture in Gulrajani et al.
(2019). It includes three convolution layers with 16, 32 and 64 channels, a kernel size of 5 × 5
and a stride of 2. Each of these layers are followed by a Swish activation (Ramachandran et al.,
2018) and then finally a linear layer that gives a single output. The network is initialized using
normal distribution with zero mean and the standard deviation of 0.02, and trained using Adam
optimizer with α = 0.0002, β1 = 0., β2 = 0.9 for a total of 100, 000 iterations with minibatch
of 128 generated samples and 128 samples from the test set13. We use the standard WGAN-GP
loss function, log (1 + exp (f (xfake))) + log (1 + exp (-xreal)), where f represents the network
described above. Finally, we generate 1 Million samples from the generator and report the average
log (1 + exp (f (xfake))) over these samples. Higher average value implies better generation as the
network in this case is unable to distinguish the generated and the real samples.
E.2 More Empirical Lipschitz plots
For the purpose of analysis, Figure 13b and 14b shows eLhist for pairs where each sample either comes
from the true data or from the generator, and we observe a similar trend. To verify that same results
hold in the conditional setup, we show comparisons for GANs with projection discriminator (Miyato
& Koyama, 2018) in Figure 12, 13a and 14a, and observe a similar trend. Further, to see the value
of the local Lipschitzness in the vicinity of real and generated samples we also plot the norm of the
13For CelebA, we used the training set.
25
Published as a conference paper at ICLR 2020
Jacobian in Figure 15 and 16 in Appendix E.2 and observe mostly a similar trend. In Appendix E.3
(Figure 17), we also show that the discriminator training of SRN-GAN is more stable than SN-GAN.
Conditional GANs Figure 12 shows the eLihst of conditional GANs with projection discrimina-
tor (Miyato & Koyama, 2018).
Figure 12: Comparison: eLhist of the discriminator in the conditional GAN setting with projection discriminator
on CIFAR100.
Empirical Lipschitzness between real samples and between fake samples. Figure 13 shows the
histogram of eLhist of the discriminator for pairs of fake samples i.e. samples generated by the
generator. Figure 14 shows eLhist of the discriminator when samples came from the dataset.
-0.10 -0.05 0.00 0.05 0.10 0.15 0.20
Magnitude of Effective Lipschitz variable
(a) Conditional GAN with projection discriminator.
ωl-oo∞ UO¾①。U-
5 5 5 5 5 5 5
0∂7.65.4.3.N
A。U ①nb①」LL.
-0.1 0.0	0.1	0.2	0.3	0.4	0.5
Magnitude of Effective Lipschitz variable
(b) Unconditional GAN setting.
(a) Conditional GAN with projection discriminator
Figure 14: Comparison: eLhist of the discriminator for pairs of samples from the real distribution on CIFAR10.
IWndom GAN
Stable
Spectral
Figure 13: Comparison: eLhist of the discriminator for pairs of samples selected from the generator on
CIFAR10
(b) Unconditional GAN setting.
Jacobian norm in the vicinity of the points Here we compare the Jacobian of the discriminator
of the trained models in the vicinity of the samples from the generator and the real dataset. This
is a penalized measure in various algorithms Gulrajani et al. (2017); Petzka et al. (2018) (often
referred to as local perturbations) and was independently proposed by Kodali et al. (2018). Figure 15
26
Published as a conference paper at ICLR 2020
and Figure 16 show the histogram of the norm of the Jacobian of the discriminator in the vicinity of
the generated and the real samples, respectively. To generate these plots, 2, 000 samples were used
from the respective distributions. It is interesting to note that the norm is the same for the points in the
vicinity of the real data points and the generated data points for the Stable Rank Normalization GAN
(SRN-GAN) as well for WGAN-GP whereas it varies between fake and real samples for Spectral
Normalization GAN (SN-GAN).
Magnitude of Effective Lipschitz variable
(a) Conditional GAN with projection discriminator.
Magnitude of Effective Lipschitz variable
(b) Unconditional GAN setting.
Figure 15: Jacobian norm of the discriminator in the neighbourhood of the samples from the generator trained
on CIFAR10.
Random
Stable
Spectral
*
0	2	4	6	8	10
Magnitude of Effective Lipschitz variable
(a) Conditional GAN with projection discriminator
①」0。S Uo¾①。U-
5 5 5 5 5 5 5
C67.6.5.4,3.N
Auu①nb①」Ll-
0	1	2	3	4	5	6
Magnitude of Effective Lipschitz variable
(b) Unconditional GAN setting.
Figure 16: Jacobian norm of the discriminator in the neighbourhood of the samples from the real dataset
(CIFAR10).
E.3 Training Stability
-Stable - 50
Stable - 70
-Spectral
Unconditional GAN
(pφ⅛qs) SSol .loleuE..DSQ
IOO	150
Iteration
Figure 17: Loss incurred by the discriminator. The loss of SRN-GAN with the stable rank constraint of 70 is
shifted upwards by 0.2 so that we can compare the change of the loss during training as opposed to the absolute
magnitude of the loss.
(P 2ΞS) SSo-1」0%UE tsQ
Conditional GAN
100	150
Iteration
27
Published as a conference paper at ICLR 2020
Training Stability In Figure 17 we show the discriminator loss during the course of the training
as an indicator of whether the generator gets sufficient gradient during training or not. These plots
clearly suggest that the discriminator loss is more consistent for SRN than the SN.
F Examples of Generated Images
F.1 CelebA images
For these images, we generated 100 images from the respective models and hand-picked the 10 best
images in terms of visual quality.
Figure 18: Image samples generated from the unconditional SRN-GAN.
Figure 19: Image samples generated from the unconditional SN-GAN.
28
Published as a conference paper at ICLR 2020
F.2 CIFAR10-UNCONDITIONAL GAN
(a) SRN-70-GAN
(b) SRN-50-GAN
(c) SN-GAN
(d) WGAN-GP
Figure 20: Image samples generated from the unconditional SRN-GAN, SN-GAN, and WGAN-GP.
29
Published as a conference paper at ICLR 2020
30