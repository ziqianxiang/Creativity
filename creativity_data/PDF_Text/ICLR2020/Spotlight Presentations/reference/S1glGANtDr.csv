title,year,conference
 Dynamic Programming and Optimal Control,1886, Athena Scientific
 Openai gym,2016, arXiv preprint arXiv:1606
 Stochastic primal-dual methods and sample complexity of rein-forcement learning,2016, arXiv preprint arXiv:1612
 Learning from conditional distributionsvia dual kernel embeddings,2017, In Proceedings of the 20th International Conference on ArtificialIntelligence and Statistics (AISTATS)
 The linear programming approach to approximatedynamic programming,2003, Operations Research
 Doubly robust policy evaluation and learning,2011, InProceedings of the 28th International Conference on Machine Learning (ICML)
 More robust doubly robustoff-policy evaluation,2018, In Proceedings of the 35th International Conference on Machine Learning(ICML)
 A kernel loss for solving the bellman equation,2019, NeuralInformation Processing Systems (NeurIPS)
 Batch mode reinforcementlearning based on the synthesis of artificial trajectories,2013, Annals of Operations Research
 Using options and covariance testing for longhorizon off-policy policy evaluation,2017, In Advances in Neural Information Processing Systems 30(NIPS)
 Consistent on-line off-policy evaluation,2017, In Proceedings of the 34thInternational Conference on Machine Learning (ICML)
 Importance sampling policy evaluation with anestimated behavior policy,2019, In Proceedings of the 36th International Conference on MachineLearning
 Doubly robust off-policy evaluation for reinforcement learning,2016, InProceedings of the 23rd International Conference on Machine Learning (ICML)
 Double reinforcement learning for efficient off-policy evalua-tion in markov decision processes,2019, arXiv preprint arXiv:1908
 Efficiently breaking the curse of horizon: Double reinforcementlearning in infinite-horizon processes,2019, arXiv preprint arXiv:1909
 Unbiased offline evaluation of contextual-bandit-based news article recommendation algorithms,2011, In Proceedings of the 4th InternationalConference on Web Search and Data Mining (WSDM)
 Toward minimax off-policy value estimation,2015, InProceedings of the 18th International Conference on Artificial Intelligence and Statistics (AISTATS)
 Finite-sampleanalysis of proximal gradient td algorithms,2015, In UAI
 Monte Carlo Strategies in Scientific Computing,0387, Springer Series in Statistics
 Breaking the curse of horizon: Infinite-horizon off-policy estimation,2018, In Advances in Neural Information Processing Systems
 Representation balancing mdps for off-policy policy evaluation,2018, InAdvances in Neural Information Processing Systems
 Understanding the curse of horizon in off-policyevaluation via conditional importance sampling,2019, arXiv preprint arXiv:1910
 Off-policy policy gradient withstate distribution correction,2019, arXiv preprint arXiv:1904
 Dualdice: Behavior-agnostic estimation ofdiscounted stationary distribution corrections,2019, Neural Information Processing Systems (NeurIPS)
 Markov Decision Processes,2014,: Discrete Stochastic Dynamic Programming
 Conditional importance sampling for off-policy learning,2019, arXiv preprintarXiv:1910
 Reinforcement Learning: An Introduction,1998, MIT Press
 Optimal off-policy evaluation for reinforcement learn-ing with marginalized importance sampling,2019, Neural Information Processing Systems (NeurIPS)
