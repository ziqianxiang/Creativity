title,year,conference
 Reconciling modern machine learningand the bias-variance trade-off,2018, arXiv preprint arXiv:1812
 On the inductive bias of neural tangent kernels,2019, arXiv preprintarXiv:1905
 On the global convergence of gradient descent for over-parameterized models using optimal transport,2018, In Advances in neural information processingsystems
 Asymptotics of wide networks from feynman diagrams,2019, In ICMLWorkshop on Physics for Deep Learning
 Products of many large random matrices and gradients in deep neuralnetworks,2018, arXiv preprint arXiv:1812
 Deep relu networks have surprisingly few activation patterns,2019, InAdvances in Neural Information Processing Systems
 Multilayer feedforward networks are uni-versal approximators,1989, Neural networks
 Neural tangent kernel: Convergence and gen-eralization in neural networks,2018, In Advances in neural information processing Systems
 Wide neural networks of any depth evolve as linear models under gradientdescent,2019, arXiv preprint arXiv:1902
 Towards explaining the regularization effect of initial largelearning rate in training neural networks,2019, arXiv preprint arXiv:1907
 A mean field view of the landscape oftwo-layers neural networks,2018, arXiv preprint arXiv:1804
 Mean-field theory of two-layers neuralnetworks: dimension-free bounds and kernel limit,2019, arXiv preprint arXiv:1902
 Resurrecting the sigmoid in deeplearning through dynamical isometry: theory and practice,2017, In Advances in neural informationprocessing systems
 Parameters as interacting particles: long time convergenceand asymptotic error scaling of neural networks,2018, In Advances in neural information processingsystems
 Regularization matters: Generalization andoptimization of neural nets v,2018,s
