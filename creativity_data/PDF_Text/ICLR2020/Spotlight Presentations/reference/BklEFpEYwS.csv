title,year,conference
 Deep variational informationbottleneck,2016, arXiv preprint arXiv:1612
 Meta-learning by adjusting priors based on extended pac-bayes theory,2018, InInternational Conference on Machine Learning
 Weight uncertainty inneural networks,2015, arXiv preprint arXiv:1505
 Towards a neural statistician,2016, arXiv preprintarXiv:1606
 A bayesian approach to unsupervised one-shot learning of object categories,2003, InProceedings Ninth IEEE International Conference on Computer Vision
 Conditional neural processes,2018, arXivpreprint arXiv:1807
 Neural processes,2018, arXiv preprint
 Meta-learning probabilistic inference for prediction,2018, arXiv preprint arXiv:1805
 Recasting gradient-based meta-learning as hierarchical bayes,2018, arXiv preprint arXiv:1801
 Towards understanding generalization in gradient-based meta-learning,2019, arXiv preprint arXiv:1907
 Meta-learning priors for efficient onlinebayesian regression,2018, arXiv preprint arXiv:1807
 Task agnostic meta-learning for few-shot learning,2019, InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition
 Siamese neural networks for one-shotimage recognition,2015, In ICML deep learning workshop
 One shot learning ofsimple visual concepts,2011, In Proceedings of the annual meeting of the cognitive science society
 Discrete infomax codes for meta-learning,2019, arXivpreprint arXiv:1905
 Pac-bayesian model averaging,1999, In COLT
 Efficient off-policymeta-reinforcement learning via probabilistic context variables,2019, arXiv preprint arXiv:1903
 Optimization as a model for few-shot learning,2016, In ICLR 2016
 A Bayesian framework for concept learning,1999, PhD thesis
 Deep learning and the information bottleneck principle,2015, In2015 IEEE Information Theory Workshop (ITW)
 Matching networks for oneshot learning,2016, In Advances in neural information processing systems
 Beyond pascal: A benchmark for 3d objectdetection in the wild,2014, In IEEE Winter Conference on Applications of Computer Vision (WACV)
 Metagan:An adversarial approach to few-shot learning,2018, In Advances in Neural Information ProcessingSystems
 The goal of the meta-learner is to extractinformation about the meta-training tasks and the test task training data to serve as a prior for testexamples from the novel task,2020, This information will be in terms of a distribution Q over possiblemodels
