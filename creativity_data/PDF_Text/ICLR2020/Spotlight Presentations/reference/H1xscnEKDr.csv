title,year,conference
 Obfuscated gradients give a false sense of se-curity: Circumventing defenses to adversarial examples,2018, In International Conference on MachineLearning
 Synthesizing robust adversarialexamples,2018, In ICML
 Unrestricted adversarialexamples via semantic manipulation,2020, In International Conference on Learning Representations
 Simple physi-cal adversarial examples against end-to-end autonomous driving models,2019, In IEEE InternationalConference on Embedded Software and Systems
 Curriculum adversarial training,2018, In International JointConference on Artificial Intelligence
 Adversarial examples are not easily detected: Bypassing ten detectionmethods,2017, In International Workshop on AI and Security
 Towards evaluating the robustness of neural networks,2017, IEEESymposium on Security and Privacy
 Certified adversarial robustness via random-ized smoothing,2019, In International Conference on Machine Learning
 Ex-ploring the landscape of spatial robustness,2019, In International Conference on Machine Learning
 Explaining and harnessing adversarialexamples,2015, In International Conference on Learning Representations
 Deep residual learning for image recog-nition,2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
 Benchmarking neural network robustness to common cor-ruptions and perturbations,2019, In International Conference on Learning Representations
 Certified robustness to adversarialexamples with differential privacy,2019, In IEEE Symposium on Security and Privacy
 On detecting adversarialperturbations,2017, In International Conference on Learning Representations
 Deep face recognition,2015, In BMVC
 Certified defenses against adversarial exam-ples,2018, In International Conference on Learning Representations
 Semidefinite relaxations for certifying ro-bustness to adversarial examples,2018, In Neural Information Processing Systems
 Accessorize to a crime:Real and stealthy attacks on state-of-the-art face recognition,2016, In ACM SIGSAC Conference onComputer and Communications Security
 Intriguing properties of neural networks,2014, In International Conference onLearning Representations
 Improv-ing robustness of ml classifiers against realizable evasion attacks using conserved features,2019, InUSENIX Security Symposium
 Adversarial Machine Learning,2018, Morgan Claypool
 Efficient formal safety analysis of neuralnetworks,2018, In Neural Information Processing Systems
 Provable defenses against adversarial examples via the convex outeradversarial polytope,2018, In International Conference on Machine Learning
 Scaling provable adversarialdefenses,2018, In Neural Information Processing Systems
 Feature squeezing: Detecting adversarial examples in deepneural networks,2018, In Network and Distributed Systems Security Symposium
