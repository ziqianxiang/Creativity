title,year,conference
 Deep equilibrium models,2019, In Neural InformationProcessing Systems (NeurIPS)
 The fifth PASCAL recognizingtextual entailment challenge,2009, In TAC
 Training deep nets with sublinearmemory cost,2016, arXiv preprint arXiv:1604
 Generating long sequences with sparsetransformers,2019, arXiv preprint arXiv:1904
 The PASCAL recognising textual entailmentchallenge,2005, In Machine Learning Challenges Workshop
 Transformer-xl: Attentive language models beyond a fixed-length context,2019, arXivpreprint arXiv:1901
 Learn-ing generic sentence representations using convolutional neural networks,1254, In Proceedings ofthe 2017 Conference on Empirical Methods in Natural Language Processing
 The reversible residual net-work: Backpropagation without storing activations,2017, In Advances in neural information processingsystems
 Efficient training of bertby progressively stacking,2019, In International Conference on Machine Learning
 Cohesion in English,1976, Routledge
 Modelingrecurrence for transformer,2019, Proceedings of the 2019 Conference of the North
 Learning distributed representations of sentencesfrom unlabelled data,1367, In Proceedings of the 2016 Conference of the North American Chapter ofthe Association for Computational Linguistics: Human Language Technologies
 Coherence and coreference,1979, Cognitive Science
 Association for Computational Linguistics,2012, doi:10
 RACE: Large-scale ReAdingcomprehension dataset from examinations,2017, In Proceedings of the 2017 Conference on EmpiricalMethods in Natural Language Processing
 RoBERTa: A robustly optimized BERT pre-training approach,2019, arXiv preprint arXiv:1907
 Learned in translation:Contextualized word vectors,2017, In I
 Glove: Global vectors for word rep-resentation,2014, In Proceedings of the 2014 Conference on Empirical Methods in Natural LanguageProcessing (EMNLP)
 Languagemodels are unsupervised multitask learners,2019, OpenAI Blog
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, arXiv preprint arXiv:1910
 Mesh-tensorflow: Deeplearning for supercomputers,2018, In Advances in Neural Information Processing Systems
 Bi-directional block self-attention for fast and memory-efficient sequence modeling,2018, arXiv preprint arXiv:1804
 Recursive deep models for semantic compositionality over a sentimenttreebank,2013, In Proceedings of the 2013 Conference on Empirical Methods in Natural LanguageProcessing
 Patient knowledge distillation for BERT modelcompression,2019, arXiv preprint arXiv:1908
 Well-read students learn better:The impact of student initialization on knowledge distillation,2019, arXiv preprint arXiv:1908
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 StructBERT: Incor-porating language structures into pre-training for deep language understanding,2019, arXiv preprintarXiv:1908
 XLNet: Generalized autoregressive pretraining for language understanding,2019, arXiv preprintarXiv:1906
 ReducingBERT pre-training time from 3 days to 76 minutes,2019, arXiv preprint arXiv:1904
 Aligning books and movies: Towards story-like visual explanations by watchingmovies and reading books,2015, In Proceedings of the IEEE international conference on computervision
