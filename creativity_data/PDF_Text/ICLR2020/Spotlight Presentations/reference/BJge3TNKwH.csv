title,year,conference
 The Cramer distance as a solution to biased Wasserstein gra-dients,2017, arXiv preprint arXiv:1705
 RiemannianWalk for incremental learning: Understanding forgetting and intransigence,2018, In Proceedings of theEuropean Conference on Computer Vision (ECCV)
 The empirical distribution function for dependent vari-ables: asymptotic and nonasymptotic results in Lp,2007, ESAIM: Probability and Statistics
 Improved regularization of convolutional neural netWorksWith cutout,2017, arXiv preprint arXiv:1708
 Distilling the knoWledge in a neural netWork,2015, arXivpreprint arXiv:1503
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Overcoming catastrophic forget-ting in neural networks,0027, Proceedings of the National Academy of Sciences
 Sliced Wasserstein kernels for probability distri-butions,2016, In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
 Optimalmass transport: Signal processing and machine-learning applications,2017, IEEE signal processingmagazine
 Sliced Wasserstein distance for learninggaussian mixture models,2018, In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition
 Sliced Wassersteinauto-encoders,2019, In International Conference on Learning Representations
 Overcomingcatastrophic forgetting by incremental moment matching,2017, In Advances in neural informationprocessing systems
 Learn to grow: Acontinual structure learning framework for overcoming catastrophic forgetting,2019, arXiv preprintarXiv:1904
 Learning without forgetting,2017, IEEE transactions on pattern analysisand machine intelligence
 Why there are complementarylearning systems in the hippocampus and neocortex: insights from the successes and failures ofconnectionist models of learning and memory,1995, Psychological review
 Continuallifelong learning with neural networks: A review,2019, Neural Networks
 Encoder based lifelonglearning,2017, In Proceedings of the IEEE International Conference on Computer Vision
 Learning multiple visual domains withresidual adapters,2017, In Advances in Neural Information Processing Systems
 Progressive neural networks,2016, arXiv preprintarXiv:1606
 Continual learning with deep generativereplay,2017, In Advances in Neural Information Processing Systems
 Cramer-Wold autoencoder,2018, arXivpreprint arXiv:1805
 Memoryreplay gans: Learning to generate new categories without forgetting,2018, In Advances In NeuralInformation Processing Systems
 Wide residual networks,2016, arXiv preprintarXiv:1605
 Statistical validation of imagesegmentation quality based on a spatial overlap index1: scientific reports,2004, Academic radiology
