title,year,conference
 A Kernel Perspective forRegularizing Deep Neural Networks,2018, arXiv:1810
 TheLoss Surfaces of Multilayer Networks,2014, arXiv:1412
 Support-vector networks,1995, Machine learning
 Sharp Minima Can General-ize For Deep Nets,2017, arXiv:1703
 Gradient Descent Finds GlobalMinima of Deep Neural Networks,2019, In International Conference on Machine Learning
 Qualitatively characterizing neural networkoptimization problems,2014, arXiv:1412
 Nearly-tight vc-dimension bounds for piece-wise linear neural networks,2017, CoRR
 Surprises in high-dimensional ridgeless least squares interpolation,2019, arXiv preprint arXiv:1903
 Piecewise linear activations substantially shape theloss surfaces of neural networks,2020, International Conference of Learning Representations
 Deep Residual Learning for ImageRecognition,2015, arXiv:1512
 Delving Deep into Rectifiers: SurpassingHuman-Level Performance on ImageNet Classification,2015, arXiv:1502
 Norm matters: Efficient and accuratenormalization schemes in deep networks,2018, arXiv:1803
 Densely connectedconvolutional networks,2017, In Proceedings of the IEEE conference on computer vision and patternrecognition
 Understanding generalization through visualizations,2019, arXiv preprintarXiv:1906
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, arXiv preprint arXiv:1502
 Neural Tangent Kernel: Convergence andGeneralization in Neural Networks,2018, arXiv:1806
 Gradient descent aligns the layers of deep linear networks,2018, arXivpreprint arXiv:1810
 Deep Learning without Poor Local Minima,2016, arXiv:1605
 On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Min-ima,2016, arXiv:1609
 Deep linear networks with arbitrary loss: All local minima areglobal,2018, In International Conference on Machine Learning
 Deep learning,1476, Nature
 Deep Neural Networks as Gaussian Processes,2017, arXiv:1711
 Wide Neural Networks of Any Depth Evolve as Linear ModelsUnder Gradient Descent,2019, arXiv:1902
 Understanding the Loss Surface of NeuralNetworks for Binary Classification,2018, arXiv:1803
 A pac-bayesianapproach to spectrally-normalized margin bounds for neural networks,2017, CoRR
 On the loss landscape of a classof deep neural networks with no bad local valleys,2018, arXiv:1809
 Spurious Local Minima are Common in Two-Layer ReLU NeuralNetworks,2017, arXiv:1712
 Mo-bilenetv2: Inverted residuals and linear bottlenecks,2018, In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition
 Kernel principal component anal-ysis,1997, In International conference on artificial neural networks
 The singular values of convolutional layers,2018, arXivpreprint arXiv:1805
 Least squares support vector machine classifiers,1999, Neuralprocessing letters
 Local minima in training ofneural networks,2016, arXiv:1611
 L2 Regularization versus Batch and Weight Normalization,2017, arXiv:1706
 Small nonlinearities in activation functions createbad local minima in neural networks,2018, arXiv:1802
 Small nonlinearities in activation functions create badlocal minima in neural networks,2019, In International Conference on Learning Representations
 Understandingdeep learning requires rethinking generalization,2016, arXiv:1611
 Three Mechanisms of Weight DecayRegularization,2018, arXiv:1810
 Fixup Initialization: Residual Learning WithoutNormalization,2019, arXiv:1901
