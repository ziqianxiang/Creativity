title,year,conference
 A convergence theory for deep learning via over-parameterization,2018, arXiv preprint arXiv:1811
 Onexact computation with an infinitely wide neural net,2019, arXiv preprint arXiv:1904
 Fine-grained analysis ofoptimization and generalization for overparameterized two-layer neural networks,2019, arXiv preprintarXiv:1901
 On the inductive bias of neural tangent kernels,2019, arXiv preprintarXiv:1905
 Kernel methods for deep learning,2009, In Advances in neuralinformation processing systems
 SGD learns the conjugate kernel class of the network,2017, arXiv preprintarXiv:1702
 Imagenet: A large-scale hi-erarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 Gradient descent finds globalminima of deep neural networks,2018, arXiv preprint arXiv:1811
 Gradient descent provably optimizesover-parameterized neural networks,2018, arXiv preprint arXiv:1810
 Graph neural tangent kernel: Fusing graph neural networks with graph kernels,2019, ArXiv
 Deep convolutional net-works as shallow gaussian processes,2019, In International Conference on Learning Representations
 Scaling and benchmarking self-supervised visual representation learning,2019, arXiv preprint arXiv:1905
 Neural tangent kernel: Convergence and gen-eralization in neural networks,2018, In Advances in neural information processing systems
 Wide neural networks of any depth evolve as linear models under gradientdescent,2019, arXiv preprint arXiv:1902
 Learning overparameterized neural networks via stochastic gradientdescent on structured data,2018, arXiv preprint arXiv:1808
 Bayesian deep convolutional networks with many chan-nels are gaussian processes,2019, In International Conference on Learning Representations
 Modern neural networks generalize on smalldata sets,2018, In Advances in Neural Information Processing Systems
 The effect of networkwidth on stochastic gradient descent and generalization: an empirical study,2019, arXiv preprintarXiv:1905
 Continuous neural networks,2007, In Proceedings of the EleventhInternational Conference on Artificial Intelligence and Statistics
 Computing with infinite networks,1997, In Advances in neural informationprocessing systems
 Understandingdeep learning requires rethinking generalization,2016, arXiv preprint arXiv:1611
 Split-brain autoencoders: Unsupervised learningby cross-channel prediction,2017, In Proceedings of the IEEE Conference on Computer Vision andPatternRecognition
 Stochastic gradient descent optimizesover-parameterized deep ReLU networks,2018, arXiv preprint arXiv:1811
