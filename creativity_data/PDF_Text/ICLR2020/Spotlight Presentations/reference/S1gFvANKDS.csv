title,year,conference
 A convergence theory for deep learning via over-parameterization,2018, arXiv preprint arXiv:1811
 Fine-grained analysis ofoptimization and generalization for overparameterized two-layer neural networks,2019, arXiv preprintarXiv:1901
 Dynamical isometry and a mean fieldtheory of rnns: Gating enables signal propagation in recurrent neural networks,2018, arXiv preprintarXiv:1806
 Sgd learns the conjugate kernel class of the network,2017, In Advances in Neural InformationProcessing Systems
 Gradient descent finds globalminima of deep neural networks,2018, arXiv preprint arXiv:1811
 Gradient descent provably optimizesover-parameterized neural networks,2018, arXiv preprint arXiv:1810
 Asymptotics of wide networks from feynman diagrams,2019, In TheoreticalPhysics for Deep Learning
 Space - time approach to quantum electrodynamics,1949, Phys
 Deep convolutionalnetworks as shallow gaussian processes,2019, In International Conference on Learning Representations
 Scaling description of generalization withnumber of parameters in deep learning,2019, arXiv preprint arXiv:1901
 Disentangling feature and lazylearning in deep neural networks: an empirical study,2019, CoRR
 An inVestigation into neural net optimizationVia hessian eigenValue density,2019, arXiv preprint arXiv:1901
 Gradient descent happens in a tiny subspace,2018, arXivpreprint arXiv:1812
 Products of many large random matrices and gradients in deep neuralnetworks,2018, arXiv preprint arXiv:1812
 Bayesian deep convolutional networks with many channelsare gaussian processes,2019, In International Conference on Learning Representations
 Measurements of three-level hierarchical structure in the outliers in the spectrum ofdeepnet hessians,2019, arXiv preprint arXiv:1901
 Geometry of neural network loss surfaces via random matrixtheory,2017, In International Conference on Machine Learning
 Nonlinear random matrix theory for deep learn-ing,2017, In I
 The spectrum of the fisher information matrix ofa single-hidden-layer neural network,2018, In Advances in Neural Information Processing Systems
 Resurrecting the sigmoid in deeplearning through dynamical isometry: theory and practice,2017, CoRR
 Eigenvalues of the hessian in deep learning: Singularityand beyond,2016, arXiv preprint arXiv:1611
 Empirical analysis of thehessian of over-parametrized neural networks,2017, arXiv preprint arXiv:1706
 A Planar Diagram Theory for Strong Interactions,1974, Nucl
 Numerical Linear Algebra,0898, SIAM
