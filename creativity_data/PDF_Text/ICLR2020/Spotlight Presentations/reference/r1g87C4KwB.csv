title,year,conference
 Critical learning periods in deep neuralnetworks,2017, CoRR
 Negative eigenvalues of thehessian in deep neural networks,2019, CoRR
 A convergence analysis of gradient de-scent for deep linear neural networks,2019, In International Conference on Learning Representations
 Spectrally-normalized margin bounds forneural networks,2017, In Advances in Neural Information Processing Systems
 Understanding batch normal-ization,2018, In Advances in Neural Information Processing Systems
 Identifying and attacking the saddle point problem in high-dimensional non-convex op-timization,2014, CoRR
 ImageNet: A Large-Scale HierarchicalImage Database,2009, In CVPR09
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Why does unsupervised pre-training help deep learning? J,2010, Mach
 Emergent properties of the local geometry of neural loss land-scapes,2019, arXiv preprint arXiv:1910
 The goldilocks zone: Towards better understanding of neuralnetwork loss landscapes,2019, In Proceedings of the AAAI Conference on Artificial Intelligence
 Stiffness: Anew perspective on generalization in neural networks,2019, arXiv preprint arXiv:1901
 An investigation into neural net optimizationvia hessian eigenvalue density,2019, In Proceedings of the 36th International Conference on MachineLearning
 Deep Learning,2016, 2016
 Qualitatively characterizing neural netWorkoptimization problems,2014, arXiv preprint arXiv:1412
 The local elasticity of neural netWorks,2020, In International Conferenceon Learning Representations
 Deep residual learning for image recog-nition,2015, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
 Long short-term memory,1997, Neural Comput
 Three factors influencing minima in SGD,2017, CoRR
 Fantas-tic generalization measures and where to find them,2020, In International Conference on LearningRepresentations
 Accelerating stochastic gradient descent using predictive variancereduction,2013, In Advances in Neural Information Processing Systems 26
 On large-batch training for deep learning: Generalization gap and sharp minima,2017, In 5thInternational Conference on Learning Representations
 Learning multiple layers of features from tiny images,2009, Technical report
 LCA: Loss Change Allocation for NeuralNetwork Training,2019, arXiv preprint arXiv:1909
 An iteration method for the solution of the eigenvalue problem of linear differ-ential and integral operators,1950, J
 Efficient backprop,2012, InNeural Networks: Tricks of the Trade (2nd ed
 Towards understanding regularizationin batch normalization,2019, In International Conference on Learning Representations
 Learning word vectors for sentiment analysis,2011, In Proceedings of the 49th Annual Meetingof the Association for Computational Linguistics: Human Language Technologies
 Second-order optimization for neural networks,2016, University of Toronto (Canada)
 Revisiting small batch training for deep neural networks,2018, CoRR
 Implicit regularization in deep learning,2017, CoRR
 Measurements of three-level hierarchical structure in the outliers in the spectrum ofdeepnet hessians,2019, CoRR
 Topmoumoute online natural gra-dient algorithm,2008, In Advances in Neural Information Processing Systems
 Empirical analysisof the hessian of over-parametrized neural networks,2017, CoRR
 Fashion-mnist: a novel image dataset for benchmark-ing machine learning algorithms,2017, CoRR
 A Walk with SGD,2018, arXivpreprint arXiv:1802
 Amean field theory of batch normalization,2019, CoRR
 Hessian-based analysisof large batch training and robustness to adversaries,2018, CoRR
 Which algorithmic choices matter at which batch sizes? insightsfrom a noisy quadratic model,2019, In H
 The Anisotropic Noise in StochasticGradient Descent: Its Behavior of Escaping from Minima and Regularization Effects,2018, arXivpreprint arXiv:1803
 The network used is the DenseNet-121 from Huang et al,2015, (2016)
