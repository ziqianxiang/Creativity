title,year,conference
 High-dimensional dynamics of generalization error in neuralnetworks,2017, arXiv preprint arXiv:1710
 A convergence theory for deep learning via over-parameterization,2018, arXiv preprint arXiv:1811
 Stronger generalization bounds fordeep nets via a compression approach,2018, arXiv preprint arXiv:1802
 Onexact computation with an infinitely wide neural net,2019, arXiv preprint arXiv:1904
 Fine-grained analysis ofoptimization and generalization for overparameterized two-layer neural networks,2019, arXiv preprintarXiv:1901
 Spectrally-normalized margin bounds forneural networks,2017, In Advances in Neural Information Processing Systems
 Benign overfitting in linearregression,2019, arXiv preprint arXiv:1906
 Reconciling modern machine learningand the bias-variance trade-off,2018, arXiv preprint arXiv:1812
 A generalization theory of gradient descent for learning over-parameterized deep relu networks,2019, arXiv preprint arXiv:1902
 The spectrum of random inner-product kernel matrices,2013, RandomMatrices: Theory and Applications
 On the global convergence of gradient descent for over-parameterizedmodels using optimal transport,2018, In Advances in neural information processing systems
 Distillation â‰ˆ early stopping? harvesting darkknowledge utilizing anisotropic information retrieval for overparameterized neural network,2019, arXivpreprint arXiv:1910
 Gradient descent provably optimizesover-parameterized neural networks,2018, arXiv preprint arXiv:1810
 The spectrum of kernel random matrices,2010, The Annals of Statistics
 The jamming transition as a paradigm to understand the loss landscape ofdeep neural networks,2018, arXiv preprint arXiv:1809
 Limitations of lazytraining of two-layers neural networks,2019, arXiv preprint arXiv:1906
 Linearized two-layersneural networks in high dimension,2019, arXiv preprint arXiv:1904
 Implicit bias of gradient descenton linear convolutional networks,2018, In Advances in Neural Information Processing Systems
 Surprises in high-dimensional ridgeless least squares interpolation,2019, arXiv preprint arXiv:1903
 Neural tangent kernel: Convergence andgeneralization in neural networks,2018, In Advances in neural information processing systems
 Gradient descent aligns the layers of deep linear networks,2018, arXivpreprint arXiv:1810
 Universal statistics of fisher information indeep neural networks: Mean field approach,2018, arXiv preprint arXiv:1806
 Gradient descent with early stopping is prov-ably robust to label noise for overparameterized neural networks,2019, arXiv preprint arXiv:1903
 Learning overparameterized neural networks via stochastic gradientdescent on structured data,2018, In Advances in Neural Information Processing Systems
 Algorithmic regularization in over-parameterizedmatrix sensing and neural networks with quadratic activations,2017, arXiv preprint arXiv:1712
 Towards explaining the regularization effect of initial largelearning rate in training neural networks,2019, arXiv preprint arXiv:1907
 On the spectrum of random features maps of high dimensionaldata,2018, arXiv preprint arXiv:1805
 The generalization error of random features regression: Preciseasymptotics and double descent curve,2019, arXiv preprint arXiv:1908
 A mean field view of the landscape of two-layer neural networks,2018, Proceedings of the National Academy of Sciences
 Mean-field theory of two-layers neuralnetworks: dimension-free bounds and kernel limit,2019, arXiv preprint arXiv:1902
 Generalization in deep networks: The role of distance frominitialization,2019, arXiv preprint arXiv:1901
 In search of the real inductive bias: On therole of implicit regularization in deep learning,2014, arXiv preprint arXiv:1412
 Towardsunderstanding the role of over-parametrization in generalization of neural networks,2018, arXiv preprintarXiv:1805
 Stochastic particle gradient descent for infinite ensembles,2017, arXivpreprint arXiv:1712
 Towards moderate overparameterization: global con-vergence guarantees for training shallow neural networks,2019, arXiv preprint arXiv:1902
 Nonlinear random matrix theory for deep learning,2017, In Advancesin Neural Information Processing Systems
 The spectrum of the fisher information matrix of a single-hidden-layer neural network,2018, In Advances in Neural Information Processing Systems
 Neural networks as interacting particle systems:Asymptotic convexity of the loss landscape and universal scaling of the approximation error,2018, arXivpreprint arXiv:1805
 Mean field analysis of neural networks: A centrallimit theorem,2018, arXiv preprint arXiv:1808
 A bayesian perspective on generalization and stochastic gradientdescent,2017, arXiv preprint arXiv:1710
 Adaptivity of deep relu network for learning in besov and mixed smooth besov spaces:optimal rate and curse of dimensionality,2018, arXiv preprint arXiv:1810
 On the margin theory of feedforward neuralnetworks,2018, arXiv preprint arXiv:1810
 Kernel anddeep regimes in overparametrized models,2019, arXiv preprint arXiv:1906
 On the number of variables to use in principal component regression,2019, 2019
 Understandingdeep learning requires rethinking generalization,2016, arXiv preprint arXiv:1611
