title,year,conference
 High-dimensional dynamics of generalization error in neuralnetworks,2017, arXiv preprint arXiv:1710
 On large-batch training for deep learning: Generalization gap and sharp minima,2016, arXivpreprint arXiv:1609
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Exploring general-ization in deep learning,2017, In Advances in Neural Information Processing Systems
 Tighter variational bounds are not necessarily better,2018, arXiv preprintarXiv:1802
 The necessary and sufficient conditions for consistencyof the method of empirical risk,1991, Pattern Recognition and Image Analysis
 Understandingdeep learning requires rethinking generalization,2016, arXiv preprint arXiv:1611
