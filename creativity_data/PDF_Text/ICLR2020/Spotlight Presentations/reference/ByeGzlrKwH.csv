title,year,conference
 Spectrally-normalized margin bounds for neuralnetworks,2017, In Advances in Neural Information Processing Systems
 Data-dependent coresetsfor compressing neural networks with applications to generalization bounds,2019, In InternationalConference on Learning Representations
 Concentration Inequalities: A Nonasymptotic Theory ofIndependence,2013, OUP Oxford
 Approximation by superpositions of a sigmoidal function,1989, Mathematics of Control
 Predicting parameters in deeplearning,2013, In C
 Concentration inequalities and asymptotic results for ratio type empir-ical processes,2006, TheAnnalsofProbabilily
 Implicit bias of gradient descent on linearconvolutional networks,2018, In Advances in Neural Information Processing Systems
 Nearly-tight VC-dimension bounds for piecewise linear neu-ral networks,2017, In S
 Flat minima,1997, Neural Computation
 Approximation capabilities of multilayer feedforward networks,1991, Neural Networks
 Gradient descent aligns the layers of deep linear networks,2019, In InternationalConference on Learning Representations
 Local Rademacher complexities and oracle inequalities in risk minimization,2006, TheAnnals of Statistics
 Imagenet classification with deep convolutionalneural networks,2012, In Advances in neural information processing systems
 Probability in Banach Spaces,1991, Isoperimetry and Processes
 Relating data compression and learnability,1986, Technical report
 Implicit self-regularization in deep neural networks: Evidencefrom random matrix theory and implications for learning,2018, arXiv preprint arXiv:1810
 Foundations of machine learning,2012, 2012
 On the number of linear regions of deep neuralnetworks,2014, In Z
 Deterministic PAC-Bayesian generalization bounds for deep net-works via generalizing noise-resilience,2019, In International Conference on Learning Representations(ICLR2019)
 Norm-based capacity control in neural networks,2015, InProceedings of The 28th Conference on Learning Theory
 A PAC-Bayesian approach tospectrally-normalized margin bounds for neural networks,2017, arXiv preprint arXiv:1707
 The role of over-parametrizationin generalization of neural networks,2019, In International Conference on Learning Representations
 Nonparametric regression using deep neural networks with ReLU activationfunction,2019, The Annals of Statistics
 Neural network with unbounded activation functions is universal approx-imator,2015, Applied and Computational Harmonic Analysis
 Support Vector Machines,2008, Springer
 New concentration inequalities in product spaces,1996, Inventiones Mathematicae
 Weak Convergence and Empirical Processes: With Applica-tions to Statistics,1996, Springer
 Statistical Learning Theory,1998, Wiley
 Manifold mixup: Better representations by interpolating hidden states,2018, arXiv preprintarXiv:1806
 Dropout training as adaptive regularization,2013, In C
 High-Dimensional Statistics: A Non-Asymptotic Viewpoint,2019, Cambridge Series inStatistical and Probabilistic Mathematics
 Data-dependent sample complexity of deep neural networks via lipschitz aug-mentation,2019, In Advances in neural information processing systems
 Towards understanding generalization of deep learning: Perspective of losslandscapes,2017, arXiv preprint arXiv:1706
 Understanding deep learning requiresrethinking generalization,2016, arXiv preprint arXiv:1611
 Non-vacuous generalization boundsat the imagenet scale: a PAC-Bayesian compression approach,2019, In International Conference onLearning Representations (ICLR2019)
