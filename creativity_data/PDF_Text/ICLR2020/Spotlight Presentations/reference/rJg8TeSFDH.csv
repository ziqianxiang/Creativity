title,year,conference
 Theoretical analysis of auto rate-tuning by batchnormalization,2019, In International Conference on Learning Representations
 Layer normalization,2016, arXiv preprintarXiv:1607
 Understanding batch normal-ization,2018, In S
 Sgd: General analysis and improved rates,2019, arXiv preprint arXiv:1901
 Identity mappings in deep residualnetworks,2016, In European conference on computer vision
 Norm matters: efficient and accuratenormalization schemes in deep networks,2018, In S
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, In International Conference on Machine Learning
 Automatic differentiation inpytorch,2017, 2017
 How does batch nor-malization help optimization? In S,2018, Bengio
 Instance normalization: The missing in-gredient for fast stylization,2016, arXiv preprint arXiv:1607
 WNGrad: Learn the Learning Rate in Gradient De-scent,2018, arXiv preprint arXiv:1803
 Group normalization,2018, In The European Conference on Computer Vision(ECCV)
 By Lemma 1,2020,3 and the update rule of GD with WD
 The blue line is TEXP-- corresponding to Step Decay schedule which divides LR by 10 atepoch 80 and 120,2019, They have similar trajectories and performances by a similar argument to Theorem 2
