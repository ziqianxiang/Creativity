Figure 1: Restricting over-generalized rulesSabunCu (2018), the classifier Pθ (y|x) might be misled. In contrast, What We hope to achieve is tolearn the Pjφ(rj |x) distribution using the limited labeled data and the overlap among the rules suchthat Pr(rj |x) predicts a value of 0 for examples Wrongly covered. Such examples are then excludedfrom training Pθ . The dashed boundaries indicate the revised boundaries of Rj s that We can hopeto learn based on consensus on the labeled data and the set of rules. Even after such restriction, Rj sare useful for training the classifier because of the unlabeled points inside the dashed regions thatget added to the labeled set.
Figure 2: Negative implication losstwo rules Rj and Rk of differing labels (`j 6=`k), then both rji and rki cannot be 1. This isbecause the constraint among pairs (yi, rji) and (yi, rki) as stated in Equation 3 subsumes this one.
Figure 3: Rule-specific denoising by our method.
Figure 4: Effect of rule precisionRole of Exemplars in Rules We next evaluate the importance of the exemplar-rule pairs inlearning the Pjφ and Pθ networks. The exemplars of a rule give an interesting new form ofsupervision about an instance where a labeling rule must fire. To evaluate the importance of thissupervision, we exclude the rj = 1 likelihood on rule-exemplar pairs from LL(φ), that is, thefirst term in Equation 2 is dropped. In the table below we see that performance of ImplyLossusually drops when the exemplar-rule supervision is removed. Interestingly, even after this drop, theperformance of ImplyLoss surpasses most of the methods in Table 2 indicating that even withoutexemplar-rule pairs our training objective is effective in learning from rules and labeled instances.
Figure 5: Effect of increasing labeled dataLearning from noisily labeled data has beenextensively studied in settings like crowd-sourcing. One category of these algorithmsupper-bound the loss function to make it robust to noise. These include methods like MAE (Ghosh8Published as a conference paper at ICLR 2020et al., 2017), Generalized Cross Entropy (CE)(Zhang & Sabuncu, 2018), and Ramp loss (Collobertet al., 2006). Most of these assume that noise is independent of the input given the true label. In ourmodel noise is systematic and instance-dependent.
