Figure 1: Problem formulation and motivation. Metric-based meta-learning models usually con-sist of a feature encoder E and metric function M. We aim to improve the generalization abilityof the models training from seen domains to arbitrary unseen domains. The key observation is thatthe distributions of the image features extracted from tasks in the unseen domains are significantlydifferent from those in the seen domains.
Figure 2: Method overview. (a) We propose a feature-wise transformation layer to modulate in-termediate feature activation z in the feature encoder E with the scaling and bias terms sampledfrom the Gaussian distributions parameterized by the hyper-parameters θγ and θβ . During the train-ing phase, we insert a collection of feature-wise transformation layers into the feature encoder tosimulate feature distributions extracted from the tasks in various domains. (b) We design a learning-to-learn algorithm to optimize the hyper-parameters θγ and θβ of feature-wise transformation layersby maximizing the performance of the applied metric-based model on the pseudo-unseen domain(bottom) after it is optimized on the pseudo-seen domain (top).
Figure 3: T-SNE visualization of the image features extracted from tasks in different domains.
Figure 4: Visualization of the feature-wise transformation layers. We show the quartile visual-ization of the activations SoftPlus(θγ) and SoftPlus(θβ) from each feature-wise transformation layerthat are optimized by the proposed learning-to-learn algorithm.
