Figure 1: Illustration of NEXT. In each epoch, NEXT is executed on a randomly generated planning problem.
Figure 2: Our neural network model mapsa N-link robot from the original planningspace (a (N + 2)-d configuration space) to a3d discrete latent planning space in which Weplan a path using value iteration. The resultof value iteration is then used as features fordefining V* (s|U) and π* (s0∣s,U).
Figure 3: Attention-based state embedding module. sw = (x, y) and z = sh. The upper part is spatial attention,with the first two channels being x and y, and the last two channels being constant templates with the row andcolumn coordinates, as shown with a d set to 3. The bottom module learns the representation for z. The finalembedding is obtained by outer-product of these two attention parts.
Figure 4: Overall model architecture. Current and goal states are embedded through attention module. Then theembedding of the goal state is concatenated With the map to produce ν*(0) and RR as the input to the planningmodule. The output of the planning module is aggregated with the embedding of the current state to producefeature ψ(s) for defining Vz and π .
Figure 5: Search trees and the learned V* and π* produced by NEXT. Obstacles are colored in blue. The startand goal locations are denoted by orange and brown dots. In (a) to (c), samples are represented with yellow*circles. In (d), the level of redness denotes the value of the cost-to-go estimate V . The cyan arrows point froma given state S to the mean of the learned policy π*(s0∣s, U).
Figure 6: Search trees and a solution path produced in an instance of spacecraft planning. The 7 DOF spacecrafthas a yellow body and two 2 DOF red arms. NEXT-KS produced a nearly minimum viable search tree whileRRT* failed to find a path within limited trials.
Figure 7: First row: histograms of results, in terms of success rate, average collision checks, and average costof the solution paths; Second row: NEXT improvement curves in the 5d experiments. All algorithms are set touse UP to 500 samples, except RRT*-10k, which uses 10,000 samples. The value of collision checks and pathcosts are normalized w.r.t. the performance of RRT*.
Figure 8: The success rate and average path cost of the different planners under varying time limits. RunningNEXT for 1 second achieves the same success rate as running BIT* for 50 seconds.
Figure 9: The collision-free Path producedby NEXT for robot arm planning. The startand goal configurations have end-effectors indifferent bins of the shelf.
Figure 10: Different views of the same planning problem. In (a) we color the obstacles, the startingand the goal position of the robot in deep blue, orange and brown, respectively. The stick robot canmove and rotate. The corresponding configuration space is 3d, as visualized in (b), with the extradimension being the rotation angle w.r.t. the x-axis. The blue region indicates the feasible state space,i.e., the set of collision-free states. The starting and the goal position are denoted with an orange anda brown dot, respectively. Although the workspace looks trivial, the configuration space is irregular,which makes the planning difficult.
Figure 11: Illustration for one iteration of Algorithm 1. The left and right figures illustrate twodifferent cases where the sample returned by the Expand operator is unreachable and reachablefrom the search tree.
Figure 12: Left: attention module, instantiating the Figure 3; Right: policy/value network, instantiat-ing the Figure 4.
Figure 13: planning moduleE Experiment DetailsE.1	Benchmark EnvironmentsWe used four benchmark environments in our experiment. For the first three, the workspace dimensionis 2d. We generated the maze maps with the recursive backtracker algorithm using the following imple-mentation: https://github.com/lileee/gated-path-planning-networks/blob/master/generate_dataset.py.
Figure 14: Improved GPPN architectureworkspace locations. Its output and the full robot configurations are processed together by the MLP,which then produces the current value and action estimates. The improved GPPN is trained usingsupervisions from the near-optimal paths produced by RRT*.
Figure 15: The solution path produced by NEXT in a workspace planning task (2d), rigid bodynavigation task (3d), 3-link snake task (5d) from left to right. The orange dot and the brown dot arestarting and goal locations, respectively.
Figure 16: The solution paths produced by NEXT in spacecraft planning task (7d). Spacecraft has ayellow body and two 2 DoF (red) arms. Blue cuboids are obstacles.
Figure 17: The first and second rows display the improvement curves of our algorithms on all 3000problems of the 2d workspace planning and 3d rigid body navigation problems. We compare ouralgorithms with RRT*. Three columns correspond to the success rate, the average collision checks,and the average cost of the solution paths for each algorithm.
Figure 18: Column (a) to (c) are the search trees produced by the RRT*, NEXT-KS, and NEXT-GPon the same workspace planning task (2d). The learned V * and ∏* from NEXT-KS are plotted incolumn (d). In the figures, obstacles are colored in deep blue, the starting and goal locations aredenoted by orange and brown dots, respectively. In column (a) to (c), samples are represented withhollow yellow circles, and edges are colored in green. In column (d), the level of redness denotes thevalue of the cost-to-go estimate V*, and the cyan arrows point from a given state S to the center ofthe proposal distribution ∏ *(s0∣s,U). We set the maximum number of samples to be 500.
Figure 19: Each column corresponds to one example from the rigid body navigation problem (3d).
Figure 20: Each column corresponds to one example from the 3-link snake problem (5d). The topand the bottom rows are the search trees produced by the RRT* and NEXT-KS, respectively. In thefigures, obstacles are colored in deep blue, and the rigid bodies are represented with matchsticks.
Figure 21: Each column corresponds to one example from the spacecraft planning problem (7d).
Figure 22: Examples of different shelves sampled from the distribution.
Figure 23: First row: robot arm solution trajectories produced by NEXT(-KS) in four planningproblems; Second row: BIT* solutions on the same planning problems. NEXT only takes 5 secondsto complete each problem while BIT* needs 250 seconds to find a solution for the hardest problems(last two columns).
