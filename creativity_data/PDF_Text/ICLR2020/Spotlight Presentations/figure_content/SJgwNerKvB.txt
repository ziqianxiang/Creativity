Figure 1: Task-conditioned hypernetworks for continual learning. (a) Commonly, the parame-ters of a neural network are directly adjusted from data to solve a task. Here, a weight generatortermed hypernetwork is learned instead. Hypernetworks map embedding vectors to weights, whichparameterize a target neural network. In a continual learning scenario, a set of task-specific em-beddings is learned via backpropagation. Embedding vectors provide task-dependent context andbias the hypernetwork to particular solutions. (b) A smaller, chunked hypernetwork can be usediteratively, producing a chunk of target network weights at a time (e.g., one layer at a time). Chunkedhypernetworks can achieve model compression: the effective number of trainable parameters can besmaller than the number of target network weights.
Figure 2: 1D nonlinear regression. (a) Task-conditioned hypernetworks with output regularizationcan easily model a sequence of polynomials of increasing degree, while learning in a continual fashion.
Figure 3: Experiments on the permuted MNIST benchmark. (a) Final test set classificationaccuracy on the t-th task after learning one hundred permutations (PermutedMNIST-100). Task-conditioned hypernetworks (hnet, in red) achieve very large memory lifetimes on the permutedMNIST benchmark. Synaptic intelligence (SI, in blue; Zenke et al., 2017), online EWC (in orange;Schwarz et al., 2018) and deep generative replay (DGR+distill, in green; Shin et al., 2017) methods areshown for comparison. Memory retention in SI and DGR+distill degrade gracefully, whereas EWCsuffers from rigidity and can never reach very high accuracy, even though memories persist for theentire experiment duration. (b) Compression ratio lθh∪{e:)}| versus task-averaged test set accuracyafter learning all tasks (labelled ‘final’, in red) and immediately after learning a task (labelled ‘during’,in purple) for the PermutedMNIST-10 benchmark. Hypernetworks allow for model compression andperform well even when the number of target model parameters exceeds their own. Performancedecays nonlinearly: accuracies stay approximately constant for a wide range of compression ratiosbelow unity. Hyperparameters were tuned once for compression ratio ≈ 1 and were then used for allcompression ratios. Shaded areas denote STD (a) resp. SEM (b) across 5 random seeds.
Figure 4: Two-dimensional task embedding space for the split MNIST benchmark. Color-coded test set classification accuracies after learning the five splits, shown as the embedding vectorcomponents are varied. Markers denote the position of final task embeddings. (a) High classificationperformance with virtually no forgetting is achieved even when e-space is low-dimensional. Themodel shows information transfer in embedding space: the first task is solved in a large volume thatincludes embeddings for subsequently learned tasks. (b) Competition in embedding space: the lasttask occupies a finite high performance region, with graceful degradation away from the embeddingvector. Previously learned task embeddings still lead to moderate, above-chance performance.
Figure 5: Split CIFAR-10/100 CLbenchmark. Test set accuracies (mean± STD, n = 5) on the entire CIFAR-10 dataset and subsequent CIFAR-100splits of ten classes. Our hypernetwork-protected ResNet-32 displays virtuallyno forgetting; final averaged perfor-mance (hnet, in red) matches the imme-diate one (hnet-during, in blue). Further-more, information is transferred acrosstasks, as performance is higher thanwhen training each task from scratch(purple). Disabling our regularizer leadsto strong forgetting (in yellow).
Figure A1: Hypernetwork-protected replay model setups. (a) A hypernetwork-protected VAE,that we used for HNET+R and HNET+TIR main text experiments. (b) A hypernetwork-protectedGAN, that we used for our class-incremental learning Appendix F experiments. (c) A task inferenceclassifier protected with synthetic replay data, used on HNET+TIR experiments.
Figure A2: Additional experiments on the PermutedMNIST-100 benchmark. (a) Final test setclassification accuracy on the t-th task after learning one hundred permutations (PermutedMNIST-100). All runs use exactly the same hyperparameter configuration except for varying values ofβoutput. The final accuracies are robust for a wide range of regularization strengths. If βoutput is tooweak, forgetting will occur. However, there is no severe disadvantage of choosing βoutput too high(cmp. (c)). A too high βoutput simply shifts the attention of the optimizer away from the current task,leading to lower baseline accuracies when the training time is not increased. (b) Due to an increasednumber of output neurons, the target network for PermutedMNIST-100 has more weights than forPermutedMNIST-10 (this is only the case for CL1 and CL3). This plot shows that the performancedrop is minor when choosing a hypernetwork with a comparable number of weights as the targetnetwork in CL2 (orange) compared to one that has a similar number of weights as the target networkfor CL1 in PermutedMNIST-100 (red). (c) Task-averaged test set accuracy after learning all tasks(labelled ‘final’, in red) and immediately after learning a task (labelled ‘during’, in purple) for the runsdepicted in (a). For low values of βoutput final accuracies are worse than immediate once (forgettingoccurs). If βoutput is too high, baseline accuracies decrease since the optimizer puts less emphasison the current task (note that the training time per task is not increased). Shaded areas in (a) and (b)denote STD, whereas error bars in (c) denote SEM (always across 5 random seeds).
Figure A3: Replication of the split CIFAR-10/100 experiment of Zenke et al. (2017). Test setaccuracies on the entire CIFAR-10 dataset and subsequent CIFAR-100 splits. Both task-conditionedhypernetworks (hnet, in red) and synaptic intelligence (SI, in green) transfer information forward andare protected from catastrophic forgetting. The performance of the two methods is comparable. Forcompleteness, we report our test set accuracies achieved immediately after training (hnet-during, inblue), when training from scratch (purple), and with our regularizer turned off (fine-tuning, yellow).
Figure A4: Context-free inference using hypernetwork-protected replay (HNET+TIR) on longtask sequences. Final test set classification accuracy on the t-th task after learning one hundredpermutations of the MNIST dataset (PermutedMNIST-100) for the CL2 (a) and CL3 (b) scenarios,where task identity is not explicitly provided to the system. As before, the number of hypernetworkparameters is not larger than that of the related work we compare to. (a) HNET+TIR displays almostperfect memory retention. We used a stochastic regularizer (cf. Appendix D note below) whichevaluates the output regularizer in Eq. 2 only for a random subset of previous tasks (here, twenty).
Figure A5: Additional experiments with online EWC and fine-tuning on the PermutedMNIST-100 benchmark. (a) Final test set classification accuracy on the t-th task after learning one hundredpermutations (PermutedMNIST-100) using the online EWC algorithm (Schwarz et al., 2018) toprevent forgetting. All runs use exactly the same hyperparameter configuration except for varyingvalues of the regularization strength λ. Our method (hnet, in red) and the online EWC run (λ = 100,in orange) from Fig. 3a are shown for comparison. It can be seen that even when tuning theregularization strength one cannot attain similar performance as with our approach (cmp. Fig. A2a).
Figure A6: Hyperparameter search for online EWC and SI on the PermutedMNIST-100 bench-mark. We conduct the same hyperparameter search as performed in van de Ven & Tolias (2018). Wedid not compute different random seeds for this search. (a) Hyperparameter search on the regular-isation strength c for the SI algorithm. Accuracies during and after the experiment are shown. (b)Hyperparameter search for parameters λ and γ of the online EWC algorithm. Only accuracies afterthe experiment are shown.
Figure A7: Robustness to hypernetwork architecture choice for a large range of compressionratios. Performance vs. compression for random hypernetwork architecture choices, for split MNISTand PermutedMNIST-10 (mean ± STD, n = 500 architectures per bin). Every model was trainedwith the same setup (including all hyperparameters) used to obtain results reported in Table 1 (CL1).
Figure A8: Image samples from hypernetwork-protected replay models. The left column of bothof the subfigures display images directly after training the replay model on the corresponding class,compared to the right column(s) where samples are obtained after training on eights and nines i.e. allclasses. (a) Image samples from a class-incrementally trained VAE. Here the exact same trainingconfiguration to obtain results for split MNIST with the HNET+R setup are used, see AppendixC. (b) Image samples from a class-incrementally trained GAN. For the training configurations, seeAppendix B. In both cases the weights of the generative part, i.e., the decoder or the generator, areproduced and protected by a hypernetwork.
